{"doc_id": "arxiv:2601.13178", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.13178.pdf", "meta": {"doc_id": "arxiv:2601.13178", "source": "arxiv", "arxiv_id": "2601.13178", "title": "Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient Portal Messages", "authors": ["Joseph Gatto", "Parker Seegmiller", "Timothy Burdick", "Philip Resnik", "Roshnik Rahat", "Sarah DeLozier", "Sarah M. Preum"], "published": "2026-01-19T16:05:31Z", "updated": "2026-01-19T16:05:31Z", "summary": "Medical triage is the task of allocating medical resources and prioritizing patients based on medical need. This paper introduces the first large-scale public dataset for studying medical triage in the context of asynchronous outpatient portal messages. Our novel task formulation views patient message triage as a pairwise inference problem, where we train LLMs to choose `\"which message is more medically urgent\" in a head-to-head tournament-style re-sort of a physician's inbox. Our novel benchmark PMR-Bench contains 1569 unique messages and 2,000+ high-quality test pairs for pairwise medical urgency assessment alongside a scalable training data generation pipeline. PMR-Bench includes samples that contain both unstructured patient-written messages alongside real electronic health record (EHR) data, emulating a real-world medical triage scenario.   We develop a novel automated data annotation strategy to provide LLMs with in-domain guidance on this task. The resulting data is used to train two model classes, UrgentReward and UrgentSFT, leveraging Bradley-Terry and next token prediction objective, respectively to perform pairwise urgency classification. We find that UrgentSFT achieves top performance on PMR-Bench, with UrgentReward showing distinct advantages in low-resource settings. For example, UrgentSFT-8B and UrgentReward-8B provide a 15- and 16-point boost, respectively, on inbox sorting metrics over off-the-shelf 8B models. Paper resources can be found at https://tinyurl.com/Patient-Message-Triage", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.13178v1", "url_pdf": "https://arxiv.org/pdf/2601.13178.pdf", "meta_path": "data/raw/arxiv/meta/2601.13178.json", "sha256": "5f57178843cebea410db10882804ed614e4f43ab5865565facc6d3d903fd708f", "status": "ok", "fetched_at": "2026-02-18T02:21:08.746987+00:00"}, "pages": [{"page": 1, "text": "Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient\nPortal Messages\nJoseph Gatto1, Parker Seegmiller1, Timothy Burdick2,\nPhilip Resnik3, Roshnik Rahat1, Sarah DeLozier2, Sarah M. Preum1\n1 Dartmouth College, Hanover NH\n2 Dartmouth Health, Hanover NH\n3 University of Maryland, College Park\n{joseph.m.gatto.gr}@dartmouth.edu\nAbstract\nMedical triage is the task of allocating medi-\ncal resources and prioritizing patients based on\nmedical need. This paper introduces the first\nlarge-scale public dataset for studying medical\ntriage in the context of asynchronous outpa-\ntient portal messages. Our novel task formu-\nlation views patient message triage as a pair-\nwise inference problem, where we train LLMs\nto choose “which message is more medically\nurgent\" in a head-to-head tournament-style re-\nsort of a physician’s inbox. Our novel bench-\nmark PMR-Bench contains 1569 unique mes-\nsages and 2,000+ high-quality test pairs for\npairwise medical urgency assessment along-\nside a scalable training data generation pipeline.\nPMR-Bench includes samples that contain both\nunstructured patient-written messages along-\nside real electronic health record (EHR) data,\nemulating a real-world medical triage scenario.\nWe develop a novel automated data annota-\ntion strategy to provide LLMs with in-domain\nguidance on this task. The resulting data is\nused to train two model classes, UrgentRe-\nward and UrgentSFT, leveraging Bradley-\nTerry and next token prediction objective, re-\nspectively to perform pairwise urgency clas-\nsification. We find that UrgentSFT achieves\ntop performance on PMR-Bench, with Urgen-\ntReward showing distinct advantages in low-\nresource settings. For example, UrgentSFT-\n8B and UrgentReward-8B provide a 15- and\n16-point boost, respectively, on inbox sorting\nmetrics over off-the-shelf 8B models. Paper\nresources can be found at https://tinyurl.\ncom/Patient-Message-Triage\n1\nIntroduction\nRecently, there has been great focus on integrat-\ning Large Language Models (LLMs) into clinical\nworkflows (Liu et al., 2024; Artsi et al., 2025; Tu\net al., 2025; Chen et al., 2024; Hu et al., 2025;\nWang et al., 2025). An emerging use case of LLMs\nFigure 1: In this study, we introduce PMR-Bench, a\nnovel dataset for evaluating LLM capacity to produce\n“Urgency Aware\" inboxes, where patient messages in\nclinicians’ inboxes are sorted by medical urgency. Note\nthat in a categorical setup, multiple messages can have\na similar level of urgency.\nin medicine has been their ability to help physi-\ncians manage and respond to the surging number of\nmessages patients send their doctor through EHR-\nintegrated web portals (also known as “patient por-\ntals\") (Holmgren et al., 2025; Garcia et al., 2024).\nThis surge has been correlated to increases in physi-\ncian burnout (Apaydin et al., 2025; Stillman, 2023)\nand prior works have highlighted multiple direc-\ntions in which NLP can be used to alleviate patient\nportal workloads via (i) patient message catego-\nrization (Harzand et al., 2023), (ii) LLM-drafted\nresponses to patient messages (Nov et al., 2023;\nAthavale et al., 2023; Hu et al., 2025; Garcia et al.,\n2024), and (iii) eliciting missing information in\npatient messages through follow-up question gen-\neration (Gatto et al., 2025b).\nAn under-explored problem in this space is the\ntask of Patient Message Ranking (PMR), whose\ngoal is to help doctors prioritize/rank patient mes-\nsages with higher degrees of medical urgency.\nPMR optimizes the ordering of messages in a doc-\ntor’s inbox, directly influencing the patients that\nare responded to sooner (Apathy et al., 2024), lead-\ning to preventing care escalations for patients with\n1\narXiv:2601.13178v1  [cs.CL]  19 Jan 2026\n"}, {"page": 2, "text": "more time-sensitive/urgent needs (Mermin-Bunnell\net al., 2023). PMR is similar to prior works in pa-\ntient message triage such as (Gatto et al., 2022;\nHarzand et al., 2023; Si et al., 2020; Liu et al.,\n2025b) who solve the problem of portal message\nranking by mapping messages into a discrete label\nspace (e.g. urgent vs non-urgent).\nHowever, this paper aims to address the fol-\nlowing three limitations of triage-related prior\nwork. (i) Categorical definitions of urgency vary\nat a clinician- and organization-level, making prior\nwork difficult to generalize to another setting. For\nexample, clinicians of different specialties, training\nlevels, years of experience, or geo-location (e.g., ru-\nral vs urban) usually have differing notions of what\nqualifies as an “urgent\" message (Quan et al., 2013).\nThe phenomenon of low inter-rater agreement can\nbe found throughout clinical NLP (Wornow et al.,\n2024; Brake and Schaaf, 2024) and prior works\nhave discussed this challenge as it pertains to medi-\ncal triage (Naved and Luo, 2024). (ii) Ranking mes-\nsages based on discrete class labels provides only\na weak ordering of messages, with no intra-class\nprioritization. (iii) Most prior work focuses on only\nmessages specific to a single medical condition\n(COVID-19 (Gatto et al., 2022; Mermin-Bunnell\net al., 2023)), organ system (cardiology (Si et al.,\n2020)), or medical emergencies (Liu et al., 2025b).\nIn this study, we address the gaps in prior work\nby posing message triage as a pairwise ranking\nproblem instead of a classification problem.\nSpecifically, we introduce a novel benchmark,\nPatient Message Ranking (PMR)-Bench, a pair-\nwise text classification task covering a diverse array\nof medical conditions in primary care where the\ngoal is to decide which of the two messages is\nmore medically urgent. Unlike classification, this\ntask formulation is more directly connected to the\nreal problem of deciding which messages should\nbe treated as having higher priority. In addition,\nintuitively, a binary higher- versus lower-urgency\ncomparison involves simpler comparison seman-\ntics than an ordinal set of three or more urgency\ncategories. The ability to compute these compar-\nisons inherently solves the ranking problem, as a\nPMR model can be deployed as the comparator in\na sorting algorithm (e.g., bubble sort/quick sort) to\nrank a doctor’s inbox based on medical urgency\n(Qin et al., 2024; Zhuang et al., 2024).\nPMR-Bench contains 1,569 unique patient mes-\nsages with clinicians’ ordinal annotations. This\nenables large-scale generation of data pairs for pair-\nwise urgency detection (i.e. which of two patients\nis more medically urgent) from two different medi-\ncal communication platforms. First, PMR-Reddit\nis a publicly available, curated set of patient mes-\nsages from r/AskDocs — an online forum where\nmedical experts respond to patient queries. Sec-\nond, PMR-Synth is a publicly available set of\npairwise message comparisons using high-quality,\nsynthetic patient portal messages, paired with real\nEHR data to emulate a real patient-portal environ-\nment in which urgency is determined using both\npatient message and structured EHR data. Finally,\nPMR-Real is a proprietary set of real-patient mes-\nsages and corresponding EHR data sourced from a\nlarge regional hospital in the US.\nWe explore two fine-tuning strategies for LLMs.\nThe first is UrgentSFT, which uses Supevised\nFine-Tuning (SFT) to adapt LLMs to our novel\ntask. Furthermore, we introduce UrgentReward,\nwhich frames pairwise inference training in a re-\nward modeling context. We show that UrgentRe-\nward achieves high performance with only an 8B\nparameter LLM. UrgentReward-8B outperforms\nGPT-OSS (120B) on this task and achieves 95%\nof the performance of larger finetuned LLMs. We\nalso define a set of task-specific metrics to evaluate\nmodel performance in this task. We summarize our\ncontributions as follows:\n• We introduce PMR-Bench, the first large-scale\nbenchmark for pairwise medical urgency as-\nsessment. PMR-Bench includes patient mes-\nsages paired with real structured EHR data —\nemulating a realistic patient message triage\nenvironment. We benchmark 8 LLMs on our\nnovel task. We will make our dataset available\non Huggingface.\n• We develop two models, UrgentReward and\nUrgentSFT, which are pairwise inference ap-\nproaches for determining which of two pa-\ntient messages should be attended to first. Our\nmethods optimize for accuracy and efficiency\nwith strong performance across 4B, 8B, 27B,\nand 32B parameter model variants — making\nour methods more suitable for low-resource\nsettings. For example, UrgentSFT-8B and\nUrgentReward-8B provide a 15- and 16-point\nboost, respectively, on inbox sorting metrics\nover off-the-shelf 8B models.\n2\n"}, {"page": 3, "text": "2\nRelated Work\nLLMs for Document Ranking:\nDocument\nranking is a common Information Retrieval task\nthat aims to rank document relevance to a search\nquery (Xu et al., 2025). In recent years, it has be-\ncome common to employ computationally expen-\nsive document ranking models, including LLMs,\non small lists of documents, refining the outputs of\nmore efficient methods which have been applied to\nlarger document sets (Robertson et al., 2009). For\nexample, (Sun et al., 2023; Qin et al., 2024; Zhuang\net al., 2024) introduce state-of-the-art re-ranking\nmethods in the context of LLMs, with recent works\n(Zhuang et al., 2025) using reasoning models for\ndocument ranking.\nGiven a model that can perfectly determine\nwhich of two documents is most relevant to a query,\nwe can leverage the theoretical guarantees of sort-\ning algorithms to re-rank a set of documents using\npairwise comparisons (Zhuang et al., 2024). In\npractice, LLMs may make mistakes and thus be-\ncome sensitive to the initial ordering of the docu-\nments. One can avoid order sensitivity by com-\nputing all\n\u0000n\n2\n\u0001\ncomparisons and sorting by win\nrate. This method is highly accurate, but limited\nby higher inference cost when compared to more\nefficient approaches such as pointwise or listwise\nre-ranking (Qin et al., 2024; Zhuang et al., 2024).\nIn this study, we focus exclusively on pairwise\nre-ranking strategies when sorting patient portal\nmessages. This design choice is motivated by the\nfollowing three task-specific constraints. (i) PMR\nis a safety-critical task, demanding that we choose\na sorting method that provides stronger guarantees\nand is less sensitive to the initial document order.\n(ii) PMR can afford higher latency, e.g., a clini-\ncian’s inbox can be re-sorted during the hours the\nclinician is away / seeing patients. Also, unlike\nother IR tasks such as web page ranking, where\nusers only visit a handful of search results, in PMR,\nclinicians eventually attend to all patient messages\nwithin a time window (e.g., 2-3 business days).\n(iii) In production, a PMR system can reuse past\ncomparisons, precluding the need to re-sort the\nfull inbox with\n\u0000n\n2\n\u0001\ncomparisons every time a new\npatient message arrives.\nPatient Messages Triaging\nPrior works have ex-\nplored employing NLP methods to classify patient\nmessages based on their urgency. For example, (Si\net al., 2020) employ transformer-based classifiers to\ncategorize patient messages into [urgent, medium,\nFigure 2: Example data format for PMR-Reddit and\nPMR-Synth/Real. Data from Reddit is unstructured text,\nwith responses from moderator-verified clinical experts.\nData from PMR-Synth and PMR-Real include linked\nEHR data, as the EHR data impact triage decisions.\nnon-urgent] categories. Similarly, (Harzand et al.,\n2023) study how to route patient messages using\nfive categories: [urgent, clinician, prescription re-\nfill, schedule, form]. Yang et al. (2024) study the\ncapacity of a BERT-based model (Devlin et al.,\n2019) to flag patient messages with high acuity.\nMermin-Bunnell et al. (2023) use NLP to automati-\ncally detect patients with COVID while Gatto et al.\n(2022) use BERT-based methods to detect the sever-\nity of messages from online medical Q&A forums.\nOther related works, such as (Lu et al., 2024) have\nused LLMs to determine the urgency of synthetic\nclinical vignettes from the Emergency Department\n(ED) into 5 categorical rankings determined by the\nEmergency Severity Index (ESI).1 In this study, we\nextend prior work by developing an urgency scale\nthat extends to domains such as primary care while\nre-framing the urgency classification task as a pair-\nwise inference problem for enhanced accuracy and\ngeneralizability.\nAdditionally, different NLP approaches, and\nmore recently LLMs, have been applied broadly to\npatient messages ranging from writing responses\nto patient messages (Nov et al., 2023; Athavale\net al., 2023; Garcia et al., 2024) to asking patients\nfollow-up questions to elicit missing information\nGatto et al. (2025a).\n3\n"}, {"page": 4, "text": "Reddit\nSynth\nReal\nAvg Tokens\n228 ±153\n457 ±100\n511 ±147\nUnique Msg\n1121\n60\n388\nLevel 1\n126\n10\n34\nLevel 2\n45\n10\n68\nLevel 3\n59\n10\n79\nLevel 4\n384\n10\n100\nLevel 5\n283\n10\n100\nLevel 6\n324\n10\n7\nHas EHR\n✗\n✓\n✓\nTable 1: PMR-Bench dataset statistics. Token numbers\nestimated using the Qwen3 tokenizer.\n3\nMethods\n3.1\nPMR as a Pairwise Ranking Task\nJust as we are biased towards reading emails in\nthe order they appear in our inbox, physicians\nare biased toward reviewing patient messages/-\nqueries in the order they are presented (Apathy\net al., 2024). Thus, for a set of patient messages\nP = {p1, . . . , pn}, the goal of PMR is to learn\nthe optimal ordering P ′ such that messages with a\nhigher degree of medical urgency are ranked higher\nin P ′. The resulting sort enables patients with\ngreater medical needs to receive clinicians’ atten-\ntion sooner.\nEach pi ∈P may contain an associated struc-\ntured EHR record ei. EHR records contain relevant\npatient details such as age, gender, medication list,\ndiagnosis history, and active problem list. Such\ncontext is often taken into consideration when pa-\ntient messages are reviewed (Ozkaynak et al., 2014)\n— making PMR a multi-modal inference problem.\nFollowing (Qin et al., 2024), we sort P via pair-\nwise comparisons across patients. For example,\nfor any two patient messages (pi, pj), we can feed\nthem to a model f whose job is to determine which\nof the two patient messages should be attended to\nfirst. We can then re-sort a clinician’s inbox using\nf as the comparator in a sorting algorithm or to\ncompute all\n\u0000n\n2\n\u0001\ncomparisons and sort messages\nbased on their \"win-rate\" (Shah and Wainwright,\n2018). In this study, we focus on the later, as |P| is\noften small and sorting by win-rate gives a better\nupper-bound on performance as methods are often\nsensitive to the initial ordering of samples.\n1Although relevant, ESI rankings do not apply to primary\ncare / outpatient clinics due to significant differences in medi-\ncal content (e.g. portal messages have far-fewer critical emer-\ngencies).\n3.2\nPMR-Bench Dataset Overview\nIn Table 1 we provide an overview of PMR-\nBench, which contains 1,569 unique healthcare\nmessages from multiple data sources. (i) PMR-\nReddit contains messages sourced from the sub-\nreddit r/AskDocs, (ii) PMR-Real leverages a pro-\nprietary corpus of patient messages from a large\nregional hospital in the US, and (iii) PMR-Synth\nuses expert-written messages which aim to emulate\nthe style and prose of PMR-Real while enabling us\nto share high-quality data publicly for reproducibil-\nity. Example data from each source is shown in\nFigure 2. Due to the high cost and challenges of\ncollecting reliable annotation at scale, we devel-\noped a reproducible annotation method that infers\nlabels from clinician responses to messages that\nare readily available in real responses to patient\nmessages on r/AskDocs and from real physicians\nin PMR-Real (more details in Section 3.2.1). For\nPMR-Synth, we had clinical experts directly anno-\ntate pairs of messages.\nIn the remainder of this section, we first describe\nin detail our automated urgency annotation process\nfollowed by additional details for each dataset.\n3.2.1\nUrgency Annotation\nWe curate pairwise urgency labels by leveraging\nexisting expert responses to patient messages. To\naccomplish this, we aim to classify each response\ninto an ordinal set of urgency categories. To the\nbest of our knowledge, there does not exist any\naccepted standard for medical urgency classifica-\ntion outside of emergency medicine (Tanabe et al.,\n2004) — which has limited applicability to patient\nportal messages. We have thus developed a 6-tier\nordinal scale for labeling urgency in patient portal\nmessages in collaboration with a team of clinicians\nat a large regional medical center. Our scale goes\nfrom Level 1 (Most Urgent — Emergency Atten-\ntion Needed) to Level 6 (Least Urgent — No Med-\nical Attention Needed). The full label space with\nexamples is shown in Table 6 in the Appendix.\nNow, for a sample (q, r) where q is a patient\nmessage and r is a response from a clinical expert,\nwe use an LLM g to classify the response r into\nthe above scale to determine the degree of urgency\nof the message.2 For example, patients instructed\nto go to the ED are classified as “Level 1\" while\npatients given self-care strategies are classified as\n2We used GPT-5 for public datasets. For sensitive data, we\nused GPT-OSS 20B on our secure computing server.\n4\n"}, {"page": 5, "text": "\"Level 5\". We then create pairwise annotation for\ntwo messages (qi, qj) based on the relative rank-\ning of their respective responses (g(ri), g(rj)). In\nAppendix A.4 we demonstrate each of the six lev-\nels of urgency, and outline additional steps taken\nto ensure reliability and quality of annotation (e.g.\nsample quality filtering and judge models to vali-\ndate the label accuracy of each pair of messages).\nWe apply the above strategy to create PMR-\nReddit and PMR-Real. As discussed in Section\n3.1, samples in PMR-Real each have an associ-\nated EHR record, while PMR-Reddit samples do\nnot. To create a publicly available multi-modal ver-\nsion of the task, we also construct PMR-Synth. As\nsynthetic patient messages will not have a corre-\nsponding expert response to infer the urgency label\nlike the other two datasets, we had a team of clin-\nicians (i.e., triage nurses and physicians) provide\npairwise annotation directly to pairs of synthetic\npatient messages. Each patient message in PMR-\nSynth is paired with a de-identified EHR record\nfrom a real patient at our partner hospital and is\nmade publicly available for reproducibility. We\nprovide sample data as a supplement to this sub-\nmission for reference. More details for PMR-Synth\nannotation can be found in Appendix A.5.\nWe note that the design choice to go from an\nordinal urgency scale to a pair-wise urgency\ncomparison has many advantages. First, having\na quantitative measure of gap in urgency between\ntwo samples allows us to define the difficulty of a\npair, where samples far apart on the scale should\nbe easier to classify compared to samples which\nare close-by. In Section 5 we show that all model\nperformances go down as difficulty goes up, vali-\ndating the quality of our annotations. Furthermore,\nhaving a label for a single message allows us to\nrun an experiment where we test model capacity to\npredict the label directly.\n3.2.2\nDataset Details\nPMR-Reddit:\nWe\nsource\nmessages\nfrom\nr/AskDocs, a forum where patients can request\nfeedback from verified clinical experts on Reddit.\nUsing an LLM, we filter for those patients who are\nlooking for feedback on acutely onset symptoms.\nWe then classify each comment from a verified\nexpert clinician using GPT-5. Note that we only\nconsider posts with expert comments which (i) re-\nceived at least 5 upvotes, or (ii) had comment sec-\ntions with agreement across multiple expert com-\nments. We then classify each comment into our\n6-level urgency scale (see Table 6). Our resulting\ndataset has 1,121 unique posts with the label dis-\ntribution shown in Table 3.2.2. The ordinal label\nassigned to the expert comment is used to sample\nposts for pairwise comparisons. The PMR-Reddit\ntest set has a total of 1502 pairwise comparisons\nderived from 362 posts.\nPMR-Real: We source a set of real patient por-\ntal messages from a large regional medical center\nin the United States. Similar to the preprocessing\nof PMR-Reddit, we filter for patient messages de-\nscribing acutely onset symptoms and classify the\nresponses to the resulting messages into our ordi-\nnal urgency scale. A key difference between PMR-\nReal and PMR-Reddit is the inclusion of structured\nEHR data. Each PMR-Real message has as its\nlinked EHR data, including the patient’s (i) active\nproblem list, (ii) recent diagnoses, (iii) active med-\nications, and (iv) demographic information. Note\nthat we are unable to publicly release PMR-Real\ndue to the sensitive nature of the dataset.\nPMR-Synth: Messages in PMR-Synth were writ-\nten and curated by expert members of the study\nteam and subsequently reviewed by additional clin-\nical experts to ensure high-quality, realistic samples\ncovering a wide array of medical topics. Note, we\nintentionally opted to hand-craft each message as\nLLM-generated content struggled to match realistic\npatient tone and style. The EHR record associated\nwith each message is the de-identified EHR of a\nreal patient from the same pool of patients used to\ncreate PMR-Real. Unlike PMR-Reddit and PMR-\nSynth, we had a team of medical experts directly\nclassify all\n\u0000n\n2\n\u0001\nmessage pairs for two separate in-\nboxes of size 30, annotating which of two patients\nshould receive priority medical care. We treat one\ninbox as a training and the other as a testing set. We\nreport inter-annotator agreement for PMR-Synth\nin Table 4 with additional annotation details in Ap-\npendix A.5.\nSample Difficulty Quantification: Each message\nnow has a label 1-6 describing how urgent the mes-\nsage is. We consider the difference between the two\nlabels to be a proxy for pairwise sample difficulty.\nEasy: Samples with a difficulty level of at least 4\n(e.g. Level 1 vs Level 5/6). Medium: Samples\nwhich are 2-3 difficulty levels apart (e.g. Level 1\nvs Level 3/4). Hard: Samples which are less than\n2 difficulty levels apart (e.g. Level 2 and Level 3).\nIt should be noted that this was an empirical design\nchoice and can be explored further in future work.\n5\n"}, {"page": 6, "text": "3.3\nUrgentSFT\nTo solve the pairwise urgency task, we develop Ur-\ngentSFT, a two-step inference procedure for classi-\nfying pairwise medical urgency. Consider an LLM\nf which processes a pair of texts f(a, b) where a\nand b denote two different patient messages (our\nUrgentSFT prompt can be found in Figure 4). Ur-\ngentSFT essentially asks the model to output a\nprobability (via the ‘YES’ token as in other IR\ntasks (Liang et al., 2023)) deciding if message b\nshould be attended to before message a based on\ntheir respective medical urgency. For a given pair\nof patient messages (a, b) where b was annotated\nas the more urgent message, we evaluate model cor-\nrectness using the following probability difference\nη = P(YES|f(a, b)) −P(YES|f(b, a))\nIf η > 0, then the model is correct, as it has success-\nfully attributed a higher probability to the more ur-\ngent patient message. We formulate UrgentSFT as\na two-step inference leveraging probability scores\nto help prevent ties and improve models’ sensitivity\nto input order. Using patient messages which have\nbeen classified into our ordinal urgency scale, we\ncreate training pairs for UrgentSFT fine-tuning.\n3.4\nUrgentReward\nUrgentReward frames pairwise classification us-\ning a Bradley-Terry (BT) loss (Bradley and Terry,\n1952) to help the LLM better internalize a rela-\ntive urgency ranking among patient messages. We\nnote that this is in contrast to UrgentSFT which\nis trained using the standard next token prediction\nobjective.\nFor a given patient message t, we create Urgen-\ntReward training triplets (t, tm, tl) where tm and tl\nare patient messages that are more and less urgent\nthan t, respectively. Let tp denote a prompt which\ncontains within it the message t. Specifically, the\nUrgentReward prompt template (shown in Figure 5)\nasks the model to write a message more medically\nurgent than the one provided. We thus frame the\ntask of quantifying how much more urgent one mes-\nsage is when compared to another as the scoring\nof a prompt completion — aligning our work with\nprior studies on reward modeling. We fine-tune a re-\nward model using Ur using the TRL package (von\nWerra et al., 2020), which uses a Bradley-Terry\nobjective to maximize Ur(tp, tm) −Ur(tp, tl).\nAt test time, we apply UrgentReward similarly\nto UrgentSFT. For a given pair of patients (a, b), we\nrun two inferences f(a, b) = s1 and f(b, a) = s2.\nIf s1 > s2, this means that b is more urgent than a.\nIt is crucial to note that the pairwise application of\nthe BT model makes it distinct from prior works\nin IR that apply BT models in a pointwise re-rank\nsetting. We perform two BT inferences per pair,\nand use those scores only to produce a pairwise\nclassification label.\nTo leverage existing knowledge on scoring\nprompt completions, all UrgentReward models are\nfine-tuned on Qwen-based SkyWork-Reward-v2\nmodels (Liu et al., 2025a), which are state-of-the-\nart LLM-based sequence classifiers pre-trained on\n26 million preference pairs.\n4\nExperimental Setup\nIn this section, we describe the baselines and met-\nrics used to evaluate our methods in this study. We\nconsider two evaluation settings. We first consider\nan intrinsic evaluation where we directly assess\nthe binary classification accuracy of each model on\nPMR-Bench pairs. We then conduct an extrinsic\nevaluation, directly assessing how well each model\ncan sort a clinician’s inbox.\n4.1\nIntrinsic Evaluation\nFollowing (Lambert et al., 2025) we report the clas-\nsification accuracy in Table 2 as our evaluation\nsetup is structurally similar to that of a reward mod-\neling evaluation. We report overall accuracy as\nwell as per-difficulty accuracy, where difficulty is\ndefined by the difference in ordinal triage rankings\nas defined in Section 3.2.2.\n4.2\nExtrinsic Evaluation\nData Preparation:\nWe sample messages of\nvarying urgency levels to create a diverse inbox\nof approximately 30 messages per corpus. This\nis motivated by (Adler-Milstein et al., 2020), who\nfound that clinicians received 229 messages per\nweek, which is approximately 32 messages per day.\nPlease see Appendix Table A.2 for the urgency\nlevel distribution of each inbox.\nInference:\nFor an inbox with n messages in it,\nwe compute\n\u0000n\n2\n\u0001\npairwise comparisons. Each time\na sample is deemed more urgent than another, we\nincrement it’s score by (1 + η), where η is the dif-\nference in normalized probabilities / reward scores.\nThe inbox is then sorted based on the total score of\neach sample.\nEvaluation Metrics: We convert the urgency la-\nbels from our ordinal urgency scale into relevancy\n6\n"}, {"page": 7, "text": "PMR-Reddit\nPMR-Synth\nPMR-Real\nEasy\nMed\nHard\nTotal\nEasy\nMed\nHard\nTotal\nEasy\nMed\nHard\nTotal\nNumber of Test Pairs\n318\n736\n448\n1502\n75\n175\n185\n435\n51\n274\n241\n566\nInstruct Models\nQwen3-4B\n0.85\n0.72\n0.66\n0.73\n0.76\n0.66\n0.50\n0.61\n0.86\n0.74\n0.51\n0.66\nQwen3-8B\n0.85\n0.73\n0.68\n0.74\n0.80\n0.74\n0.55\n0.67\n0.88\n0.72\n0.60\n0.68\nQwen3-32B\n0.89\n0.76\n0.69\n0.77\n0.77\n0.70\n0.51\n0.63\n0.90\n0.76\n0.58\n0.70\nMedGemma-27b\n0.89\n0.74\n0.70\n0.76\n0.95\n0.73\n0.63\n0.72\n0.80\n0.74\n0.60\n0.68\nDeep Reasoning Models\nQwen3-32B-R\n0.81\n0.64\n0.56\n0.65\n0.68\n0.58\n0.38\n0.51\n0.78\n0.58\n0.41\n0.53\nGPT-OSS*\n0.93\n0.76\n0.67\n0.77\n0.79\n0.60\n0.50\n0.59\n0.86\n0.64\n0.56\n0.63\nUrgentSFT\nQwen3-4BSF T\n0.84\n0.69\n0.63\n0.70\n0.81\n0.75\n0.50\n0.65\n0.90\n0.79\n0.67\n0.75\nQwen3-8BSF T\n0.90\n0.75\n0.69\n0.76\n0.83\n0.68\n0.52\n0.64\n0.92\n0.80\n0.64\n0.74\nQwen3-32BSF T\n0.96\n0.86\n0.84\n0.87\n0.92\n0.70\n0.60\n0.69\n†\n†\n†\n†\nMedGemma-27BSF T\n0.98\n0.85\n0.87\n0.88\n0.93\n0.77\n0.60\n0.73\n0.92\n0.78\n0.72\n0.77\nUrgentReward\nReward-4Bbase\n0.79\n0.67\n0.55\n0.66\n0.75\n0.64\n0.53\n0.61\n0.82\n0.75\n0.61\n0.70\nReward-4Burgent\n0.91\n0.80\n0.80\n0.82\n0.80\n0.66\n0.56\n0.64\n0.92\n0.82\n0.63\n0.75\nReward-8Bbase\n0.80\n0.67\n0.55\n0.66\n0.84\n0.75\n0.54\n0.68\n0.86\n0.72\n0.57\n0.67\nReward-8Burgent\n0.93\n0.82\n0.85\n0.85\n0.91\n0.73\n0.62\n0.71\n0.92\n0.81\n0.63\n0.74\nTable 2: Pairwise classification accuracy on each dataset, reported by each difficulty level (easy, medium (med),\nhard). Top performing models are bold. 2nd highest performing models are underlined. We find that UrgentSFT-\nMedGemma shows the highest performance with UrgentReward showing comparable scores with a much smaller\nmodel. * GPT-OSS experiments use the 120B model for PMR-Reddit / Synth and 20B for real. † We do not have\nthe computational capacity to fine-tune Qwen3-32B on our secure server.\nscores to map our problem into an information\nretrieval setting. E.g., Level 1 samples have the\nhighest relevancy scores as these patient messages\nshould be attended to before lower urgency pa-\ntient messages. We can then compute classic IR\nmetrics such as NDCG@K (Järvelin and Kekäläi-\nnen, 2002) directly from our relevancy-mapped\nsamples. One drawback of NDCG is it will not\nproperly penalize the model for sorting a highly\nurgent message to the bottom of the inbox which\nis a safety-critical error in this task. Inspired by\nprior works with similar motivations to address\ncontent sorted at the bottom of a list (Gienapp\net al., 2020; CrowdCent, 2025) we report a tail-\nnormalized NDCG (T-NDCG) which penalizes a\nmodel for sorting urgent information to the bot-\ntom. Specifically, for an inbox I which has been\nsorted by a ranking model, the T-NDCG@k is\nNDCG@K(I) −NDCG@K(r(I)), where r(I)\nis the reverse sorting of I — penalizing the model\nfor placing urgent messages at the bottom of the\nlist. We use the ranx (Bassani, 2022) package to\ncompute IR metrics in this study.\n4.3\nBaseline Models\nInstruct Models: We explore four non-reasoning\nmodels, Qwen3-4/8/32B (Team, 2025) with think-\ning disabled, and a medical LLM, Medgemma-27b-\ntext-it (Sellergren et al., 2025).\nReasoning Models:\nWe explore two reasoning\nmodels, Qwen3-32B and GPT-OSS (OpenAI et al.,\n2025). Unlike instruct models, which use the proba-\nbility of the “YES\" token, we attribute a probability\nof 1.0 when the model predicts “YES\" as the rea-\nsoning process makes use of token probabilities\nless meaningful.\nTraining Data: The training data for UrgentSFT\nand UrgentReward are exactly the same for each\nof the three datasets. We curate training triplets\nfrom the pool of samples we classified into our 6-\nlabel scale. For UrgentReward, this translates into\nan anchor sample and then a chosen and rejected\ncompletion used for training e.g. (Anchor, More\nUrgent, Less Urgent). The same triplet is converted\ninto multiple SFT samples (e.g. (Anchor, More\nUrgent, YES) and (Anchor, Less Urgent, NO)).\nAdditional training details, including training data,\ndata distributions, fine-tuning strategy, and model\nselection is shown in Appendix B.\nMulti-Class Baseline:\nFor our extrinsic evalu-\nation, we also evaluate LLM capacity to predict\nthe class label directly. We explore GPT-OSS as\nwell as MedGemma-27B and Qwen3-32B with and\nwithout SFT on this task. As multi-class models\n7\n"}, {"page": 8, "text": "will produce rankings with many ties, we report an\nexpected T-NDCG (McSherry and Najork, 2008) in\nTable 3, which in our implementation is the average\nT-NDCG given numerous intra-class shuffles.\n5\nResults\n5.1\nIntrinsic Evaluation: Pairwise\nClassification\nTable 2 displays our pairwise classification results.\nWe find that on PMR-Reddit and PMR-Synth, Ur-\ngentSFT with MedGemma-27b achieves the high-\nest overall performance. In general, PMR-Reddit\nscores are higher than PMR-Synth and PMR-Real.\nThis result is intuitive as models do not need to pro-\ncess structured EHR information in PMR-Reddit.\nAlso noteworthy is that PMR-Reddit has a much\nlarger training set, likely contributing to higher per-\nformance. However, our ablation study in Table\n8 shows that UrgentReward has the capacity to\nget more out of less training data when applied to\nsmaller models, making it a viable option when\nfewer resources are accessible.\nNoteworthy is the comparison between baseline\ninstruct models and reasoning models. We find\nthat Qwen3-32B without reasoning out-performs\nQwen3-32B with reasoning. We believe this may\nbe due to input order biases being exaggerated by\nreasoning as well as the Instruct Models having\nthe advantage of tie-breaking via token probabil-\nities. Overall, our methods substantially reduce\npairwise triage error on all real datasets, demon-\nstrating that UrgentSFT and UrgentReward can de-\nliver meaningful real-world triage improvements\nwhile remaining lightweight and deployable with\nsmaller language models\n5.2\nExtrinsic Evaluation: Inbox Sorting\nThe results of extrinsic evaluation are presented\nin Table 3. The T-NDCG@30 metric considers\nthe full inbox, reflecting broad-scale sorting qual-\nity for a given model. In contrast, T-NDCG@10\nmetrics emphasizes the top and bottom of the list,\nmore-heavily rewarding correct placement—and\npenalizing misplacement—of highly urgent mes-\nsages. The larger performance gap at @10 suggests\nour models are particularly effective at handling\nmore urgent samples. On PMR-Reddit we find\nthat UrgentSFT with Qwen3-32B is our highest per-\nforming model. Notably, this model outperforms\nthe multi-class baseline, supporting our hypothesis\nthat pairwise inference is more effective. On PMR-\nReddit\nSynth\nReal\n@10 @30 @10 @30 @10 @30\nMulti-Class\nMedGem27B\n0.49\n0.25\n0.54\n0.24\n0.64\n0.32\nMedGem27BSF T\n0.62\n0.32\n0.27\n0.18\n0.66\n0.35\nQwen32B\n0.59\n0.28\n0.50\n0.26\n0.62\n0.34\nQwen32BSF T\n0.50\n0.24\n0.57\n0.27\n†\n†\nGPT-OSS\n0.40\n0.18\n0.48\n0.23\n0.48\n0.23\n0-Shot Pairs\nQwen8B\n0.41\n0.20\n0.52\n0.27\n0.52\n0.18\nQwen32B\n0.54\n0.25\n0.62\n0.28\n0.42\n0.17\nMedGem27B\n0.38\n0.16\n0.66\n0.32\n0.70\n0.31\nGPT-OSS*\n0.44\n0.24\n0.58\n0.28\n0.59\n0.32\nUrgentSFT Pairs\nQwen8BSF T\n0.51\n0.23\n0.62\n0.30\n0.77\n0.39\nQwen32BSF T\n0.77\n0.36\n0.73\n0.34\n†\n†\nMedGem27BSF T\n0.61\n0.29\n0.66\n0.35\n0.75\n0.39\nUrgentReward Pairs\nReward-4BBase\n0.22\n0.10\n0.37\n0.18\n0.48\n0.18\nReward-4BUrgent\n0.69\n0.30\n0.55\n0.26\n0.65\n0.37\nReward-8BBase\n0.33\n0.14\n0.57\n0.26\n0.54\n0.20\nReward-8BUrgent\n0.58\n0.26\n0.64\n0.29\n0.71\n0.38\nTable 3: Results of our extrinsic evaluation where each\nmodel is tasked with re-ranking a clinician’s inbox. We\nreport the T-NDCG metric (as described in section 4.2)\nat k = 10 and k = 30. *GPT-OSS 120B used for Red-\ndit/Synth and 20B used for Real due to resource con-\nstraints. † Experiment cannot be run due to resource\nconstraints.\nSynth we similarly find that our top-performing\nmodel is UrgentSFT with Qwen3-32B. Finally, on\nPMR-Real we see that UrgentSFT-8B is the top-\nperforming models, with a T-NDCG of 0.77 and\n0.39, @ 10 and @ 30, respectively.\nAlso, while some multi-class baselines appear\ncompetitive, their T-NDCG scores can vary greatly\ngiven different shuffles of the discrete class labels\ngenerated by the model, as demonstrated by high\nstandard deviations of these models (in Appendix\nC.3). This makes pairwise inference not only more\neffective, but a more stable ranking mechanism.\nDue to space constraints, we refer the reader to\nthe Appendix for additional ablation experiments\nand discussions. For example, in Appendix C.1 we\nshow how performance changes with varying train-\ning set sizes, with UrgentReward showing strong\nsample efficiency. In Appendix C.2 we analyze\nPMR-Synth performance with and without EHR\ndata. Finally, Appendix D performs a brief analysis\nof model biases on patient demographics, demon-\nstrating how model performance varies based on\nthe age and gender of a patient.\n8\n"}, {"page": 9, "text": "6\nConclusion\nIn this study, we re-define patient portal message\ntriage via a novel benchmark PMR-Bench, which\nframes triage as a pairwise inference problem. Our\ndata leverages expert annotation and is first-of-\nits-kind to include structured EHR data alongside\npatient-written queries for medical triage. Our re-\nsults demonstrate that, on average, our two models,\nUrgentReward and UrgentSFT improve ranking\nperformance over all baseline approaches, produc-\ning SOTA inbox sorting models.\n7\nLimitations\nWhile this work takes a strong step towards solving\nthe issue of pairwise urgency classification, other\npractical considerations must be taken before de-\nployment. For example, one may want to avoid the\ncase where a low-urgency message is continually\nplaced on the bottom of the inbox, causing longer-\nthan-usual response delays. A real-world system\nmay want to include time-in-inbox as a factor to\nabide by policies that pertain to response times\n(e.g. some healthcare systems require clinicians\nto respond within 72 hours regardless of message\nurgency).\nOur PMR-Real results are limited in that we\nare restricted to experimentation on a secure ma-\nchine that cannot access the internet and only has\na single 40GB GPU for experimentation. This\nmakes benchmarking of proprietary LLMs and\nlarger open-source models infeasible. Furthermore,\nwe are unable to release the PMR-Real dataset due\nto institutional IRB policies. We release PMR-\nSynth dataset to mitigate this issue, which aims to\nmimic the style and format of samples in PMR-\nReal.\nOutside the scope of this submission was an in-\ndepth analysis of model biases to different demo-\ngraphic and/or medical background traits. It may\nbe the case that LLMs over or under triage cer-\ntain subpopulations and this should be investigated\nbefore deployment.\nFinally, one data limitation is that while a subset\nof PMR-Bench has gone under expert review, some\nlabels are extracted from expert responses using\nLLMs. While ideally all samples could have under-\ngone human review, we believe the classification of\nexpert responses alongside extensive postprocess-\ning (as discussed in Appendix A.4.2) ensures the\nreliability and quality of the data while permitting\nus to study this problem at a greater scale. This par-\nticularly applies to our test sets, which underwent\nmultiple rounds of filtering to capture high-quality\ntriage data, i.e., it was clear from the clinician’s\nresponse that a given patient was more urgent than\nanother.\n8\nEthical Considerations\nThis study was conducted under IRB approval from\nthe submitting author’s institution. All publicly\nreleased EHR data has been de-identified and ap-\nproved for release. All processing of any sensi-\ntive patient information was performed on a secure\ncomputing server with no internet access, hosted\nby the submitting authors institution. The PMR-\nReddit samples are IRB-exempt and can be shared\nfollowing Reddit’s data usage policy.\nWe further wish to highlight that while this work\naims to sort patient messages by their medical ur-\ngency, it is generally the case that all messages\nare addressed/processed by clinicians. All patient\nmessages must be responded to as all users are de-\nserving of the medical attention they are requesting.\nIn this study, we aim to address preventable care es-\ncalation, where patients with more urgent issues are\nnot addressed in a timely manner, which can lead to\ncare escalations, e.g., hospitalization, admission to\nemergency room, delayed care, worsening medical\nsymptoms, and other care-related inefficiencies.\nReferences\nJulia Adler-Milstein, Wendi Zhao, Rachel Willard-\nGrace, Margae Knox, and Kevin Grumbach. 2020.\nElectronic health records and burnout: time spent\non the electronic health record after hours and mes-\nsage volume associated with exhaustion but not with\ncynicism among primary care clinicians. Journal\nof the American Medical Informatics Association,\n27(4):531–538.\nNate C Apathy, Katelyn Hicks, Lucy Bocknek, Garrett\nZabala, Katharine Adams, Kylie M Gomes, and Tara\nSaggar. 2024. Inbox message prioritization and man-\nagement approaches in primary care. JAMIA open,\n7(4):ooae135.\nEric A Apaydin, Claudia Der-Martirosian, Caroline\nYoo, Danielle E Rose, Nicholas J Jackson, Susan E\nStockdale, and Lucinda B Leung. 2025. Secure mes-\nsages, video visits, and burnout among primary care\nproviders in the veterans health administration: Na-\ntional survey study. Journal of Medical Internet Re-\nsearch, 27:e68858.\nYaara Artsi, Vera Sorin, Benjamin S Glicksberg, Pana-\ngiotis Korfiatis, Girish N Nadkarni, and Eyal Klang.\n9\n"}, {"page": 10, "text": "2025. Large language models in real-world clini-\ncal workflows: a systematic review of applications\nand implementation. Frontiers in Digital Health,\n7:1659134.\nAnand Athavale, Jonathan Baier, Elsie Ross, and Eri\nFukaya. 2023. The potential of chatbots in chronic\nvenous disease patient management. JVS-vascular\ninsights, 1:100019.\nElias Bassani. 2022. ranx: A blazing-fast python library\nfor ranking evaluation and comparison. In ECIR (2),\nvolume 13186 of Lecture Notes in Computer Science,\npages 259–264. Springer.\nRalph Allan Bradley and Milton E Terry. 1952. Rank\nanalysis of incomplete block designs: I. the method\nof paired comparisons.\nBiometrika, 39(3/4):324–\n345.\nNathan Brake and Thomas Schaaf. 2024. Comparing\ntwo model designs for clinical note generation; is\nan LLM a useful evaluator of consistency? In Find-\nings of the Association for Computational Linguistics:\nNAACL 2024, pages 352–363, Mexico City, Mexico.\nAssociation for Computational Linguistics.\nJunying Chen, Zhenyang Cai, Ke Ji, Xidong Wang,\nWanlong Liu, Rongsheng Wang, Jianye Hou, and\nBenyou Wang. 2024. Huatuogpt-o1, towards med-\nical complex reasoning with llms. arXiv preprint\narXiv:2412.18925.\nCrowdCent. 2025.\nScoring:\nSymmetric ndcg@k.\nCrowdCent Challenge Docs. Accessed: 2026-01-02.\nMichael Han Daniel Han and Unsloth team. 2023. Un-\nsloth.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: efficient finetuning\nof quantized llms. In Proceedings of the 37th Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS ’23, Red Hook, NY, USA. Curran\nAssociates Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nPatricia Garcia, Stephen P Ma, Shreya Shah, Margaret\nSmith, Yejin Jeong, Anna Devon-Sand, Ming Tai-\nSeale, Kevin Takazawa, Danyelle Clutter, Kyle Vogt,\net al. 2024. Artificial intelligence–generated draft\nreplies to patient inbox messages. JAMA Network\nOpen, 7(3):e243201–e243201.\nJoseph Gatto, Parker Seegmiller, Timothy Burdick,\nInas S. Khayal, Sarah DeLozier, and Sarah M.\nPreum. 2025a. Follow-up question generation for\nenhanced patient-provider conversations. Preprint,\narXiv:2503.17509.\nJoseph Gatto, Parker Seegmiller, Timothy E. Burdick,\nInas S. Khayal, Sarah DeLozier, and Sarah M. Preum.\n2025b. Follow-up question generation for enhanced\npatient-provider conversations. In Proceedings of the\n63rd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n25222–25240, Vienna, Austria. Association for Com-\nputational Linguistics.\nJoseph Gatto, Parker Seegmiller, Garrett Johnston, and\nSarah Masud Preum. 2022. Identifying the perceived\nseverity of patient-generated telemedical queries re-\ngarding covid: Developing and evaluating a transfer\nlearning–based solution. JMIR Medical Informatics,\n10(9):e37770.\nLukas Gienapp, Maik Fröbe, Matthias Hagen, and Mar-\ntin Potthast. 2020. The impact of negative relevance\njudgments on ndcg. In Proceedings of the 29th ACM\nInternational Conference on Information & Knowl-\nedge Management, CIKM ’20, page 2037–2040, New\nYork, NY, USA. Association for Computing Machin-\nery.\nArash Harzand, Muhammad Zia ul Haq, Andrew M\nHornback, Alison D Cowan, and Blake Anderson.\n2023. Clinician-trained artificial intelligence for en-\nhanced routing of patient portal messages in the elec-\ntronic health record. medRxiv, pages 2023–11.\nA Jay Holmgren, Nate C Apathy, Christine A Sinsky,\nJulia Adler-Milstein, David W Bates, and Lisa Roten-\nstein. 2025. Trends in physician electronic health\nrecord time and message volume. JAMA Internal\nMedicine, 185(4):461–463.\nDi Hu, Yawen Guo, Yiliang Zhou, Lidia Flores, and Kai\nZheng. 2025. A systematic review of early evidence\non generative ai for drafting responses to patient mes-\nsages. npj Health Systems, 2(1):27.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models. Preprint, arXiv:2106.09685.\nKalervo Järvelin and Jaana Kekäläinen. 2002. Cumu-\nlated gain-based evaluation of IR techniques. ACM\nTrans. Inf. Syst., 20(4):422–446.\nNathan Lambert, Valentina Pyatkin, Jacob Morrison,\nLester James Validad Miranda, Bill Yuchen Lin, Khy-\nathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick,\nYejin Choi, et al. 2025. Rewardbench: Evaluating re-\nward models for language modeling. In Findings\nof the Association for Computational Linguistics:\nNAACL 2025, pages 1755–1797.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya\nKumar, Benjamin Newman, Binhang Yuan, Bobby\nYan, Ce Zhang, Christian Cosgrove, Christopher D.\n10\n"}, {"page": 11, "text": "Manning, Christopher Ré, Diana Acosta-Navas,\nDrew A. Hudson, Eric Zelikman, Esin Durmus,\nFaisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu\nYao, Jue Wang, Keshav Santhanam, Laurel Orr,\nLucia Zheng, Mert Yuksekgonul, Mirac Suzgun,\nNathan Kim, Neel Guha, Niladri Chatterji, Omar\nKhattab, Peter Henderson, Qian Huang, Ryan Chi,\nSang Michael Xie, Shibani Santurkar, Surya Gan-\nguli, Tatsunori Hashimoto, Thomas Icard, Tianyi\nZhang, Vishrav Chaudhary, William Wang, Xuechen\nLi, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023.\nHolistic evaluation of language models. Preprint,\narXiv:2211.09110.\nChris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He,\nJiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxi-\nang Zhang, Jiacheng Xu, Yang Liu, and Yahui Zhou.\n2025a.\nSkywork-reward-v2: Scaling preference\ndata curation via human-ai synergy. arXiv preprint\narXiv:2507.01352.\nLei Liu, Xiaoyan Yang, Junchi Lei, Yue Shen, Jian\nWang, Peng Wei, Zhixuan Chu, Zhan Qin, and Kui\nRen. 2024. A survey on medical large language mod-\nels: Technology, application, trustworthiness, and\nfuture directions. arXiv preprint arXiv:2406.03712.\nSiru Liu, Aileen P Wright, Allison B McCoy, Sean S\nHuang, Bryan Steitz, and Adam Wright. 2025b. De-\ntecting emergencies in patient portal messages using\nlarge language models and knowledge graph-based\nretrieval-augmented generation. Journal of the Amer-\nican Medical Informatics Association, 32(6):1032–\n1039.\nMeng Lu, Brandon Ho, Dennis Ren, and Xuan Wang.\n2024. TriageAgent: Towards better multi-agents col-\nlaborations for large language model-based clinical\ntriage. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2024, pages 5747–5764,\nMiami, Florida, USA. Association for Computational\nLinguistics.\nFrank McSherry and Marc Najork. 2008.\nComput-\ning information retrieval performance measures effi-\nciently in the presence of tied scores. In European\nconference on information retrieval, pages 414–421.\nSpringer.\nKellen Mermin-Bunnell, Yuanda Zhu, Andrew Horn-\nback, Gregory Damhorst, Tiffany Walker, et al.\n2023. Use of natural language processing of patient-\ninitiated electronic health record messages to identify\npatients with covid-19 infection. jama network open\n6, 7 (07 2023), e2322299–e2322299.\nBilal A Naved and Yuan Luo. 2024. Contrasting rule\nand machine learning based digital self triage systems\nin the usa. NPJ digital medicine, 7(1):381.\nOded Nov, Nina Singh, and Devin Mann. 2023. Putting\nchatgpt’s medical advice to the (turing) test: survey\nstudy. JMIR Medical Education, 9:e46939.\nOpenAI, :, Sandhini Agarwal, Lama Ahmad, Jason\nAi, Sam Altman, Andy Applebaum, Edwin Ar-\nbus, Rahul K. Arora, Yu Bai, Bowen Baker, Haim-\ning Bao, Boaz Barak, Ally Bennett, Tyler Bertao,\nNivedita Brett, Eugene Brevdo, Greg Brockman,\nSebastien Bubeck, Che Chang, Kai Chen, Mark\nChen, Enoch Cheung, Aidan Clark, Dan Cook,\nMarat Dukhan, Casey Dvorak, Kevin Fives, Vlad\nFomenko, Timur Garipov, Kristian Georgiev, Mia\nGlaese, Tarun Gogineni, Adam Goucher, Lukas\nGross, Katia Gil Guzman, John Hallman, Jackie\nHehir, Johannes Heidecke, Alec Helyar, Haitang Hu,\nRomain Huet, Jacob Huh, Saachi Jain, Zach John-\nson, Chris Koch, Irina Kofman, Dominik Kundel,\nJason Kwon, Volodymyr Kyrylov, Elaine Ya Le, Guil-\nlaume Leclerc, James Park Lennon, Scott Lessans,\nMario Lezcano-Casado, Yuanzhi Li, Zhuohan Li,\nJi Lin, Jordan Liss, Lily, Liu, Jiancheng Liu, Kevin\nLu, Chris Lu, Zoran Martinovic, Lindsay McCallum,\nJosh McGrath, Scott McKinney, Aidan McLaughlin,\nSong Mei, Steve Mostovoy, Tong Mu, Gideon Myles,\nAlexander Neitz, Alex Nichol, Jakub Pachocki, Alex\nPaino, Dana Palmie, Ashley Pantuliano, Giambat-\ntista Parascandolo, Jongsoo Park, Leher Pathak, Car-\nolina Paz, Ludovic Peran, Dmitry Pimenov, Michelle\nPokrass, Elizabeth Proehl, Huida Qiu, Gaby Raila,\nFilippo Raso, Hongyu Ren, Kimmy Richardson,\nDavid Robinson, Bob Rotsted, Hadi Salman, Su-\nvansh Sanjeev, Max Schwarzer, D. Sculley, Harshit\nSikchi, Kendal Simon, Karan Singhal, Yang Song,\nDane Stuckey, Zhiqing Sun, Philippe Tillet, Sam\nToizer, Foivos Tsimpourlas, Nikhil Vyas, Eric Wal-\nlace, Xin Wang, Miles Wang, Olivia Watkins, Kevin\nWeil, Amy Wendling, Kevin Whinnery, Cedric Whit-\nney, Hannah Wong, Lin Yang, Yu Yang, Michihiro\nYasunaga, Kristen Ying, Wojciech Zaremba, Wenting\nZhan, Cyril Zhang, Brian Zhang, Eddie Zhang, and\nShengjia Zhao. 2025. gpt-oss-120b & gpt-oss-20b\nmodel card. Preprint, arXiv:2508.10925.\nMustafa Ozkaynak, Sharon Johnson, Stephanie Shi-\nmada, Beth Ann Petrakis, Bengisu Tulu, Cliona Ar-\nchambeault, Gemmae Fix, Erin Schwartz, and Susan\nWoods. 2014. Examining the multi-level fit between\nwork and technology in a secure messaging imple-\nmentation. In AMIA Annual Symposium Proceedings,\nvolume 2014, page 954.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,\nJunru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu\nLiu, Donald Metzler, Xuanhui Wang, and Michael\nBendersky. 2024. Large language models are effec-\ntive text rankers with pairwise ranking prompting. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2024, pages 1504–1518, Mexico\nCity, Mexico. Association for Computational Lin-\nguistics.\nSherman D Quan, Dante Morra, Francis Y Lau, W Coke,\nBrian M Wong, Robert C Wu, and Peter G Rossos.\n2013. Perceptions of urgency: defining the gap be-\ntween what physicians and nurses perceive to be an\nurgent issue. International Journal of Medical Infor-\nmatics, 82(5):378–386.\n11\n"}, {"page": 12, "text": "Stephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri,\nAtilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, Cían Hughes, Charles\nLau, Justin Chen, Fereshteh Mahvar, Liron Yatziv,\nTiffany Chen, Bram Sterling, Stefanie Anna Baby, Su-\nsanna Maria Baby, Jeremy Lai, Samuel Schmidgall,\nLu Yang, Kejia Chen, Per Bjornsson, Shashir Reddy,\nRyan Brush, Kenneth Philbrick, Mercy Asiedu, Ines\nMezerreg, Howard Hu, Howard Yang, Richa Tiwari,\nSunny Jansen, Preeti Singh, Yun Liu, Shekoofeh Az-\nizi, Aishwarya Kamath, Johan Ferret, Shreya Pathak,\nNino Vieillard, Ramona Merhej, Sarah Perrin, Ta-\ntiana Matejovicova, Alexandre Ramé, Morgane Riv-\niere, Louis Rouillard, Thomas Mesnard, Geoffrey\nCideron, Jean bastien Grill, Sabela Ramos, Edouard\nYvinec, Michelle Casbon, Elena Buchatskaya, Jean-\nBaptiste Alayrac, Dmitry Lepikhin, Vlad Feinberg,\nSebastian Borgeaud, Alek Andreev, Cassidy Hardin,\nRobert Dadashi, Léonard Hussenot, Armand Joulin,\nOlivier Bachem, Yossi Matias, Katherine Chou,\nAvinatan Hassidim, Kavi Goel, Clement Farabet,\nJoelle Barral, Tris Warkentin, Jonathon Shlens, David\nFleet, Victor Cotruta, Omar Sanseviero, Gus Martins,\nPhoebe Kirk, Anand Rao, Shravya Shetty, David F.\nSteiner, Can Kirmizibayrak, Rory Pilgrim, Daniel\nGolden, and Lin Yang. 2025. Medgemma technical\nreport. Preprint, arXiv:2507.05201.\nNihar B Shah and Martin J Wainwright. 2018. Sim-\nple, robust and optimal ranking from pairwise com-\nparisons.\nJournal of machine learning research,\n18(199):1–38.\nShijing Si, Rui Wang, Jedrek Wosik, Hao Zhang, David\nDov, Guoyin Wang, and Lawrence Carin. 2020.\nStudents need more attention: Bert-based attention\nmodel for small data with application to automatic\npatient message triage. In Proceedings of the 5th Ma-\nchine Learning for Healthcare Conference, volume\n126 of Proceedings of Machine Learning Research,\npages 436–456. PMLR.\nMichael Stillman. 2023. Death by patient portal. JAMA,\n330(3):223–224.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang\nWang, Pengjie Ren, Zhumin Chen, Dawei Yin, and\nZhaochun Ren. 2023. Is ChatGPT good at search?\ninvestigating large language models as re-ranking\nagents. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 14918–14937, Singapore. Association for\nComputational Linguistics.\nPaula Tanabe, Rick Gimbel, Paul R Yarnold, and\nJames G Adams. 2004. The emergency severity in-\ndex (version 3) 5-level triage system scores predict\ned resource consumption.\nJournal of Emergency\nNursing, 30(1):22–29.\nQwen Team. 2025. Qwen3 technical report. Preprint,\narXiv:2505.09388.\nTao Tu, Mike Schaekermann, Anil Palepu, Khaled Saab,\nJan Freyberg, Ryutaro Tanno, Amy Wang, Brenna\nLi, Mohamed Amin, Yong Cheng, et al. 2025. To-\nwards conversational diagnostic artificial intelligence.\nNature, pages 1–9.\nLeandro von Werra, Younes Belkada, Lewis Tunstall,\nEdward Beeching, Tristan Thrush, Nathan Lambert,\nShengyi Huang, Kashif Rasul, and Quentin Gal-\nlouédec. 2020. Trl: Transformer reinforcement learn-\ning. https://github.com/huggingface/trl.\nHanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu,\nGuleid Hussein, Mohamad El Labban, Kingsley\nIheasirim, Hariprasad Reddy Korsapati, Chuck Out-\ncalt, and Jimeng Sun. 2025. Towards adapting open-\nsource large language models for expert-level clinical\nnote generation. In Findings of the Association for\nComputational Linguistics: ACL 2025, pages 12084–\n12117, Vienna, Austria. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nMichael Wornow, Alejandro Lozano, Dev Dash, Jenelle\nJindal, Kenneth W. Mahaffey, and Nigam H. Shah.\n2024. Zero-shot clinical trial patient matching with\nllms. Preprint, arXiv:2402.05125.\nZhichao Xu, Fengran Mo, Zhiqi Huang, Crystina Zhang,\nPuxuan Yu, Bei Wang, Jimmy Lin, and Vivek Sriku-\nmar. 2025. A survey of model architectures in infor-\nmation retrieval. Preprint, arXiv:2502.14822.\nJie Yang, Jonathan So, Hao Zhang, Simon Jones,\nDenise M Connolly, Claudia Golding, Esmelin\nGriffes, Adam C Szerencsy, Tzer (Jason) Wu,\nYindalon Aphinyanaphongs, and Vincent J Major.\n2024. Development and evaluation of an artificial\nintelligence-based workflow for the prioritization of\npatient portal messages. JAMIA Open, 7(3):ooae078.\nShengyao Zhuang, Xueguang Ma, Bevan Koopman,\nJimmy Lin, and Guido Zuccon. 2025.\nRank-\nr1: Enhancing reasoning in llm-based document\nrerankers via reinforcement learning. arXiv preprint\narXiv:2503.06034.\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman,\nand Guido Zuccon. 2024. A setwise approach for\neffective and highly efficient zero-shot ranking with\nlarge language models. In Proceedings of the 47th\n12\n"}, {"page": 13, "text": "International ACM SIGIR Conference on Research\nand Development in Information Retrieval, pages\n38–47.\nA\nAdditional Data Details\nA.1\nInter-Annotator Agreement\nIn Table 4 we show the inter-annotator agreement\nmetrics for PMR-Synth.\nMetric\nValue\nN (item pairs)\nPercent agreement\n0.85\n20\nKrippendorff’s α\n0.63\n20\nTable 4: Inter-annotator agreement metrics (two anno-\ntators, binary \"Which patient is more urgent\" labels).\nAcross all 20 samples, there were only three disagree-\nments between the two annotators, showing strong align-\nment and annotation quality.\nA.2\nExtrinsic Eval Data Distribution\nLevel\nReddit\nSynth\nReal\n1\n5\n5\n5\n2\n5\n5\n3\n3\n5\n5\n5\n4\n5\n5\n7\n5\n5\n5\n7\n6\n5\n5\n4\nTable 5: Extrinsic Evaluation Inbox Distribution by\nLevel.\nIn Table 5 we show the number of samples\nper-level in the extrinsic inbox evaluations. We\nnote that to ensure data quality in PMR-Real, the\nauthors manually selected each sample included\nin the inbox, verifying that each extracted label\nclosely aligns with the label definition. This was\nperformed as (i) our secure computing server only\nhas access to smaller LLMs, making automated\nlabel extraction more challenging and (ii) unlike\npairwise comparisons, we are not able to do LLM\npost-processing where we confirm, using the clin-\nicians response, that one sample is more urgent\nthan another. In general, we tried to keep the label\ndistribution even, but given the manual efforts it\nwas challenging to maintain the uniformity without\nextending manual data review beyond capacity.\nA.3\nDataset Examples\nIn Table 6 we show each class label, it’s defini-\ntion, and an example to help better understand our\ndataset.\nA.4\nUrgency Annotation Details\nIn this section, we detail data filtration steps taken\nto curate high-quality patient messages for PMR-\nReal and PMR-Reddit. Note that these steps do\n13\n"}, {"page": 14, "text": "Level\nDefinition Summary\nExample Message\nExample Response\n1\nPatient is facing life-threatening\nsymptoms and must seek\nemergency care.\nHey doc, having really bad\nshortness of breath today\nYou should be seen at the ED\nimmediately!\n2\nPatient is experiencing urgent\nyet non-life-threatening\nsymptoms worthy of same-day\ncare, such as urgent care or a\nwalk-in clinic.\nHi doc, have a fever of 102 and\nsome spots in my throat.\nYou may have strep and should\nbe seen at an urgent care ASAP.\n3\nPatient is experiencing\nsemi-urgent symptoms worthy\nof an in-person healthcare visit.\nThe visit must occur in the next\n1-3 days.\nHi Doc, I fell down a few days\nago and have been experiencing\nthis lingering back pain.\nI can see you tomorrow if you\nare available. Thanks.\n4\nPatient is experiencing\nnon-urgent symptoms worthy of\nan in-person healthcare visit.\nThe visit is not time-sensitive.\nBeen struggling with this cough\noff-and-on for a month now.\nWondering if we can meet to\nchat.\nSure, we have opening for next\nweek. Please call the office.\n5\nPatient is experiencing\nsymptoms that may be\nalleviated by self-treatment\noptions. They do not require an\nin-person visit but do require\nremote guidance.\nHi doc, got a stuffy nose,\nthought I would let you know.\nAny meds you suggest?\nYou can take sudafed for the\ncongestion. Let me know if it\nprogresses.\n6\nPatient is experiencing\nsymptoms that require no\nmedical attention and are not of\nconcern. No further action is\nneeded.\nWondering where I can get a\nCOVID test these days? Been\nfeeling a little sniffly.\nYou can get a COVID test from\na local drugstore.\nTable 6: Label definitions with examples. Due to space constraints, we use manually-written examples to communi-\ncate the label space. In practice, messages and responses are much more complex.\nnot apply to PMR-Synth as those messages are\nhand-crafted and not sourced from a larger corpus.\nA.4.1\nClinician’s Response Classification\nWhen classifying clinicians’ responses to messages\ninto our 6-level ordinal scale, we allow the model\nto predict two additional classes which we use for\ndata filtering. The first is “UNCLEAR\", which the\nmodel can select if the response does not cleanly fit\ninto one of our categories. The next is “SUPPORT-\nIVE CARE\" which captures suggestions for things\nlike physical therapy or non-urgent mental health\nsessions. We found that filtering these two sample\ntypes was necessary for data quality as (i) any sam-\nple which is unclear should not be included and\n(ii) suggestions to seek supportive care are often of\na fundamentally different nature compared to sug-\ngestions to seek to acute care. Without this label,\nthe model may conflate suggestions to visit one’s\nPCP and one’s physical therapist, for example, as\nthe same urgency tier, which is not correct given\nour task definition.\nA.4.2\nData Quality Checks\nFor a given data source with paired (message, clin-\nician’s response) data tuples, we can automatically\nextract an urgency label from the clinician’s re-\nsponse based on urgency hierarchy shown in Table\n6. We then create pairwise classification pairs by\nsampling messages with different urgency labels.\nFor example, we may sample a Level 1 message\nand a Level 3 messages. Since level 1 is the highest\nurgency, we can create a pair where we know the\ncorrect label for “which message is more medically\nurgent\".\nTo ensure the quality of the pairwise annota-\ntions, we perform additional data quality checks\nbefore creating our test sets. For PMR-Reddit and\nPMR-Real, we pass each sampled data pair through\nan LLM — providing both the message and clini-\ncian’s response for each sample. The prompt asks\nthe model to review which of the two patients is\nmore urgent based on the clinician’s response. Im-\nportantly, the model can choose one of the patients\nor decide that it is unclear which patient is more\nurgent. We only retain pairs for which the LLM (i)\n14\n"}, {"page": 15, "text": "agrees with our auto-label and (ii) does not find the\ndecision to be unclear. We perform this filtration\ntwice with two different prompt variations, only\nkeeping pairs where both filters find it clear from\nthe clinician’s response which of the two patients\nis more urgent.\nA.4.3\nSample Inclusion Criteria\nPMR-Reddit:\nUsing LLMs as filtration mecha-\nnisms, we run a series of filters over the raw PMR-\nReddit samples to identify patient messages that\nmeet desirable criteria. Specifically, we ensure that\neach patient is over 18 years old and that they are\nsuffering from an acutely onset issue.\nPMR-Real:\nWe similarly filter for patients who\nare 18 years or older, but without use of an LLM\nas we can leverage structured EHR data. Then, we\nfilter for patients suffering acutely onset issues.\nA.5\nAdditional PMR-Synth Details\nA.5.1\nAnnotation\nWe constructed two separate inboxes of synthetic\npatient portal messages of size n=30 to create PMR-\nSynth. We can build all possible\n\u0000n\n2\n\u0001\npairs of mes-\nsages and gave them to a team of triage nurses and\nphysicians at our collaborating hospital for anno-\ntation. Annotators were instructed to select which\nof two patients were more medically urgent based\non the message and structured EHR data presented\nto them. If the annotators found any samples to\nbe challenging, a 2nd annotator could be requested\nfor additional review. Due to the significant anno-\ntation expense, we were unable to do a full-scale\n2-annotations per pair. However, we did find strong\ninter-annotator agreement (see Table 4) on a subset\nof the samples between two expert annotators.\nEach annotator was recruited through our col-\nlaboration with a partner hospital and was com-\npensated $50-per-hour for their annotation efforts.\nEach annotator was aware of the data’s intended\nuse. Each annotator was verbally instructed on how\nto perform the annotation task, which as mentioned\nabove entailed choosing which of two patients they\nwould provide priority medical care to. They were\nfurther instructed not to assign any ties and to use\nthe corresponding EHR data in whichever way they\nsee fit. From our discussions with annotators, we\ncan note that in the case where the degree of medi-\ncal urgency between two patients was very similar,\nthe EHR data often served as a useful tie-breaking\nmechanism to help decide which patient was gen-\nerally more high-risk.\nA.5.2\nInbox Creation\nOur extrinsic evaluation metric requires relevancy\nlabels which for PMR-Reddit and PMR-Real were\nextracted from ground-truth responses. While we\ndo not have ground-truth responses for PMR-Synth,\nwe do have the ability to sort the 30 message in-\nbox by win-rate and assign a discrete label to each\nelement in the inbox based on the position it was\nsorted to (e.g. Top 1/6 most urgent message receive\nLevel 1, bottom 1/6 receive Level 6). We use this\nstrategy to align per-difficulty PMR-Synth results\nwith PMR-Reddit and PMR-Real.\nB\nAdditional Training Details\nAll fine-tuned models are trained using Lo-\nRA/QLoRA (Hu et al., 2021; Dettmers et al., 2023)\nwith rank = 64 and alpha = 64. For all Qwen and\nMedGemma baselines we use Unsloth (Daniel Han\nand team, 2023) 4-bit BitsAndBytes 3 quantiza-\ntions. As the SkyWork (Liu et al., 2025a) reward\nmodels could fit on our secure computing platform\nwe used the original model for such experiments.\nWe use TRL (von Werra et al., 2020) and Hugging-\nface Transformeres (Wolf et al., 2020) for model\ndevelopment.\nAll PMR-Reddit and Real models are trained for\n1 epoch with learning rate 1e-5 with the best model\nchosen from the development set. For PMR-Synth,\nsince our annotation was for all\n\u0000n\n2\n\u0001\nsamples in an\ninbox, we cannot create a development set with dis-\njoint patient messages. Due to this dataset having\nfar fewer unique queries, we ran two experiments,\none with one epoch and another with three epochs.\nWe found training longer to be more effective and\nreported the 3 epoch result for all trained models\nwith no other parameter search.\nIn Table 7 we show the distribution of training\nsamples for PMR-Reddit and PMR-Synth. Specif-\nically, we take the following steps when creating\nSFT and Reward training samples:\n1. We random sample an anchor sample. This is\nany sample with an urgency level between 2-5\n(as Level 1 and Level 6 samples cannot have\nanything reliably considered to be “more\" or\n“less\" urgent respectively).\n3https://github.com/bitsandbytes-foundation/bitsandbytes\n15\n"}, {"page": 16, "text": "2. We then sample two messages, one which is\nmore and one which is less urgent than the an-\nchor sample. This creates a triplet of (anchor,\nmore urgent, less urgent) messages.\n(a) Note that we cap how many times a sam-\nple can be included as a more or less\nurgent training instance, as performing\nthe full\n\u0000n\n2\n\u0001\nwould be very computation-\nally expensive for our larger training sets\n(i.e. PMR-Reddit, PMR-Real).\n3. Now that we have this triplet, we use it to\nbuild SFT and Reward training samples.\n(a) SFT: We create four samples from each\ntriplet.\nRecall that for SFT, we are\nprompting for a pair of patients (a,b),\nto determine if b is more urgent than\na. Thus, we create the training samples\n(anchor, more urgent, yes), (anchor, less\nurgent, no), (more urgent, anchor, no),\n(less urgent, anchor, yes).\n(b) Reward:\nRecall that UrgentReward\nasks the model to write a sample more\nurgent than the one provided. Thus, we\ncreate a training triplet (anchor, more ur-\ngent, less urgent) which corresponds to a\npositive and negative completion to the\nprompt. Because of the disproportion-\nate number of SFT samples, we also add\nthe inverse prompt (i.e. write a message\nwhich less urgent) to the training pool\nand provide the triplet (anchor, less ur-\ngent, more urgent). This design choice (i)\nhelps balance out the number of gradient\nupdates compared to SFT and (ii) pro-\nvides the same advantage SFT receives\nin learning from the inverse. Note we\nnever apply the inverse prompt during\ntest time.\nC\nAdditional Results\nC.1\nTraining Set Size Ablation\nIn Table 8 we demonstrate how number of training\nsamples affects pairwise classification performance\non PMR-Reddit.\nC.2\nEHR Ablation\nIn Table 9 we show an ablation where we train\nwith and without the EHR in the input for PMR-\nSynth. Interestingly, our Qwen-based models (i.e.\nPMR-Reddit\nPMR-Synth\nUnique Message Count\n772\n30\nUrgentSFT\n26,544\n2,352\nUrgentReward\n13,272\n1,164\nTable 7: Amount of training pairs/triplets used for Ur-\ngentSFT and UrgentReward respectively. Training pairs\nare built from the set of unique messages for which a\nmessages has been classified into our 6-level hierarchy.\nUrgentSFT has 2x the training data because for a given\ntriplet (Anchor, More Urgent, Less Urgent) used to train\nUrgentReward, we convert this into (Anchor, More Ur-\ngent) and (Anchor, Less Urgent) SFT pairs. However,\nthe data and number of comparisons between unique\nsamples is the same.\nModel\nEasy\nMed\nHard\nTotal\nUrgentSFT-Q4B-Small\n0.82\n0.66\n0.61\n0.68\nUrgentSFT-Q4B-Large\n0.84\n0.69\n0.63\n0.70\nUrgentReward-4B-Small\n0.91\n0.75\n0.69\n0.77\nUrgentReward-4B-Large\n0.91\n0.80\n0.80\n0.82\nUrgentSFT-Q8B-Small\n0.84\n0.68\n0.59\n0.69\nUrgentSFT-Q8B-Large\n0.90\n0.75\n0.69\n0.76\nUrgentReward-8B-Small\n0.93\n0.81\n0.81\n0.84\nUrgentReward-8B-Large\n0.93\n0.82\n0.85\n0.85\nUrgentSFT-Q32B-Small\n0.96\n0.82\n0.77\n0.84\nUrgentSFT-Q32B-Large\n0.96\n0.86\n0.84\n0.87\nUrgentSFT-M27B-Small\n0.95\n0.83\n0.82\n0.85\nUrgentSFT-M27B-Large\n0.98\n0.85\n0.87\n0.88\nTable 8: Comparing the performance of each model on\nPMR-Reddit when presented with a smaller training\nset (≈2, 300 triplets) vs a larger training set (≈6, 600\ntriplets). In this table, Q denotes a Qwen3 model and M\ndenotes a MedGemma model. We find that UrgentRe-\nward models are more sample efficient and can make 4B\nand 8B parameter models viable. Note that the results\nin Table 2 display the larger dataset.\nUrgentSFT-Qwen32B and UrgentReward-8B) find\nlittle to no benefit from the inclusion of the EHR\ndata. This likely suggests that the model is focusing\nits attention on the message, limiting it’s capacity to\nachieve higher performance by using all available\ninformation.\nHowever, we do see a clear boost in performance\nfrom the UrgentSFT-MedGemma model. This is\nintuitive as such models should be more naturally\npre-disposed to medical terminology, making the\nEHR data more useful.\nFinally, we notice that removing the EHR im-\nproves performance for GPT-OSS — specifically\non easy and medium samples. Given that easy\nsamples, for example, may be more likely to have\ntheir urgency label be independent of the EHR, it\n16\n"}, {"page": 17, "text": "Model\nEasy\nMed\nHard\nTotal\nUrgentSFT-Qwen3-32B\nWith EHR\n0.92\n0.70\n0.60\n0.69\nNo EHR\n0.88\n0.72\n0.62\n0.70\nUrgentSFT-MedGemma\nWith EHR\n0.93\n0.77\n0.60\n0.73\nNo EHR\n0.92\n0.74\n0.57\n0.70\nUrgentReward-8B\nWith EHR\n0.91\n0.73\n0.62\n0.71\nNo EHR\n0.93\n0.77\n0.57\n0.71\nGPT-OSS-120B\nWith EHR\n0.79\n0.60\n0.50\n0.59\nNo EHR\n0.85\n0.66\n0.44\n0.62\nTable 9: Showing model performance on PMR-Synth\nwith and without inclusion of the structured EHR data\nas part of the input.\nCategory\nCount (%)\nOverlapping Correct (Both correct)\n239 (54.9%)\nOverlapping Incorrect (Both incorrect)\n102 (23.4%)\nOSS correct, MedGemma incorrect\n17 (3.9%)\nOSS incorrect, MedGemma correct\n77 (17.7%)\nTotal\n435\nTable 10: Overlap analysis of prediction outcomes be-\ntween OSS and MedGemma-SFT models. The majority\nof predictions are either correctly classified by both mod-\nels or incorrectly classified by both, while MedGemma-\nSFT corrects a larger fraction of errors made by OSS\nthan vice versa.\nis not surprising that the model with no in-domain\ntraining found the structured EHR data distracting.\nFuture works may wish to explore extending medi-\ncal reasoning capacity to process structured EHR\ninformation before making a prediction.\nC.3\nMulti-Class Inference and Robustness to\nInitial Order\nThe NDCG metric will not break ties, which means\nthat a multi-class model’s performance will some-\nwhat depend on a meaningless order within each\ndiscrete class. This is a big issue for multiclass\nwhen the LLM decides to over-predict one of the\nclasses. One way to address this problem is to\nshuffle the order of the elements within a discrete\nclass multiple times and report the average met-\nric. This is exactly what we do in Table 2 of the\nmain paper. Here in Table 11 we show that while\nsome multi-class models can achieve reasonable\nT-NDCG scores, the performance can vary greatly\ndepending on the intra-class shuffling. Thus, our\nmethods not only have stronger performance, but\nmore consistent rankings.\nD\nError Analysis\nWe conduct a brief error analysis comparing two\nmodels, UrgentSFT-MedGemma and GPT-OSS-\n120B, on the publicly available PMR-Synth dataset.\nThese models were selected to represent, respec-\ntively, a top-performing model in this task and a\nwidely used open-source baseline. We focus our\nanalysis on PMR-Synth to ensure reproducibility:\nthe dataset includes structured EHR data with ex-\ntractable demographic attributes relevant to our\nanalysis and can be publicly released.\nWe begin our error analysis by examining the\noverlap in prediction outcomes between the two\nmodels, as shown in Table 10. While a majority of\ninstances are either correctly or incorrectly classi-\nfied by both models, MedGemma-SFT accurately\npredicts substantially more samples that are inaccu-\nrately predicted by the OSS model than vice versa.\nNext, we investigate whether prediction errors\nare systematically associated with demographic\nfactors. Specifically, we test for correlations be-\ntween gender and age. Table 12 presents predic-\ntion performance stratified by urgency role and\ngender for both models. Across both OSS and\nMedGemma-SFT, accuracy is consistently higher\nwhen the more urgent case is male compared to\nfemale. In contrast, when the less urgent case\nis male, accuracy drops substantially—most no-\ntably for OSS, where correctness declines to 38.2%.\nWhile MedGemma-SFT achieves higher accuracy\nacross all urgency–gender combinations and ex-\nhibits a reduced gender disparity relative to OSS,\nlower performance persists in scenarios where the\nless urgent patient is male, indicating that gender-\nassociated effects are not fully mitigated.\nNext, we examine whether prediction correct-\nness is associated with patient age ordering in the\nurgency comparison task. Table 13 summarizes pre-\ndiction performance when either the more urgent\nor less urgent patient is older (i.e., when patient A\nhas a higher age than patient B in a given pair). For\nOSS, a chi-square test of independence found no\nstatistically significant association between predic-\ntion correctness and age ordering (p = 0.565), with\na very small effect size (Cramér’s V = 0.03). A\nsimilar pattern was observed for MedGemma-SFT,\nwhere correctness was also independent of age or-\ndering (p = 0.164, Cramér’s V = 0.07). Overall,\nthese results indicate that MedGemma-SFT’s pre-\n17\n"}, {"page": 18, "text": "Avg T-NDCG @ 10\nSD @ 10\nAvg T-NDCG @ 30\nSD @ 30\nPMR-Reddit\nMedGemma-27B\n0.49\n0.08\n0.25\n0.05\nMedGemma-27B-SFT\n0.62\n0.06\n0.32\n0.01\nQwen3-32B\n0.59\n0.06\n0.28\n0.03\nQwen3-32B-SFT\n0.50\n0.08\n0.24\n0.04\nGPT-OSS\n0.40\n0.09\n0.18\n0.05\nPMR-Synth\nMedGemma-27B\n0.54\n0.08\n0.24\n0.06\nMedGemma-27B+SFT\n0.27\n0.08\n0.18\n0.03\nQwen3-32B\n0.5\n0.08\n0.26\n0.03\nQwen3-32B + SFT\n0.57\n0.05\n0.27\n0.03\nGPT-OSS\n0.48\n0.08\n0.23\n0.04\nTable 11: Average NDCG over 300 trials for the PMR-Synth MultiClass. These results correspond to those found in\nTable 2.\nModel\nUrgency Level\nGender\nIncorrect\nCorrect\nOSS\nMore Urgent\nFemale\n150 (41.9%)\n208 (58.1%)\nOSS\nMore Urgent\nMale\n29 (37.7%)\n48 (62.3%)\nOSS\nLess Urgent\nFemale\n133 (36.2%)\n234 (63.8%)\nOSS\nLess Urgent\nMale\n46 (67.6%)\n22 (32.4%)\nMedGemma-SFT\nMore Urgent\nFemale\n101 (28.2%)\n257 (71.8%)\nMedGemma-SFT\nMore Urgent\nMale\n18 (23.4%)\n59 (76.6%)\nMedGemma-SFT\nLess Urgent\nFemale\n92 (25.1%)\n275 (74.9%)\nMedGemma-SFT\nLess Urgent\nMale\n27 (39.7%)\n41 (60.3%)\nTable 12: Prediction performance stratified by urgency level and gender. In other words, when the more urgent\npatient was Male/Female, what was the performance? MedGemma-SFT consistently achieves higher accuracy\nthan OSS across all urgency–gender combinations, with particularly strong improvements for female cases. For\nexample, the row for \"OSS, More Urgent, Female\" shows that when the more urgent patient is female, OSS correctly\nclassified 208 cases (58.1%) and incorrectly classified 150 cases (41.9%).\nModel\nUrgency Level\nCorrect\nIncorrect\nTotal\nOSS\nLess Urgent (Older)\n119\n68\n187\nOSS\nMore Urgent (Older)\n123\n93\n216\nMedGemma-SFT\nLess Urgent (Older)\n130\n57\n187\nMedGemma-SFT\nMore Urgent (Older)\n152\n64\n216\nTable 13: Prediction performance stratified by urgency level for older patients. MedGemma-SFT consistently\noutperforms the OSS model across both less urgent and more urgent cases, with higher correct predictions and\nfewer incorrect classifications.\ndiction accuracy is largely unaffected by whether\nthe older patient appears in the more urgent or less\nurgent role, suggesting minimal age-related bias in\nurgency-based decision-making.\nE\nPrompts\nIn Figures 3, 5 and 4 we show the core prompts\nused in this study.\n18\n"}, {"page": 19, "text": "System Prompt\n### Role You are a medical expert. You specialize in understanding the urgency of medical queries.\n### Definitions: Triage nurses can categorize patients into 1 / 6 categories, where 1 is \"most urgent\" and 6 is \"least\nurgent\".\n- Level 1 –> Patient has life-threatening issue that needs immediate attention.\n- Level 2 –> Patient has non life-threatening issue that would benefit from same-day treatment (e.g. urgent care)\n- Level 3 –> Patient should make an appointment with a doctor soon (1-3 days).\n- Level 4 –> Patient should see their doctor sometime in the near future (could be more than 3 days)\n- Level 5 –> Patient has symptoms that can be treated at home, and would benefit from a message instructing them on\nwhat to do.\n- Level 6 –> Patient presents something that is a non-issue and no further steps are needed.\nYou can use this context to help think about which of two patients are more urgent.\n### Important Note: Patient messages may or may not be presented alongside structured EHR information such as\nmedications, diagnoses, problem list, or demographics. These fields when available may contribute to medical urgency.\n### Relevant Context: Use the following information about medical urgency to help guide your response:\n- Patients who are more medically urgent will benefit more from priority medical care.\n- Sometimes, a patient may be lower urgency because they have already been seen by a physician for this exact issue very\nrecently.\n- Sometimes, the patient writes a message in a tone which is more urgent than their actual medical issue. Medical urgency\nmust be based on objective medical content, not tone. E.g. health anxiety is not medically urgent.\n- Sometimes, a patient with a more severe chronic issue may be less urgent than a patient with a less severe acute issue, as\nthe chronic patient may not have a time-sensitive issue.\n- Consider the risk factors of each patient. Two patients with the same symptoms may have different needs based on\ndemographics or prior diagnosis that make them higher risk patients.\nFigure 3: Shown is the system prompt provided to all models in all experiments\nUrgentSFT/Baseline Prompt\n### CONTEXT ###\nYou are a triage nurse who excels at determining the medical urgency of patient messages.\nThere is a message already in your inbox from \"Existing Patient\".\nA new message from \"New Patient\" has just arrived.\n### Existing Patient: { message 1 }\n### New Patient: { message 2 }\n### Instruction: Your job is to answer the question \"Is the new message from \"New Patient\" *more* medically urgent\nthan the existing message from \"Existing Patient\"?\nIf the answer is YES, then \"New Patient\" will be seen by a doctor first. If the answer is NO, \"Existing Patient\" will be\nseen first.\n### Response Format: Output \"YES\" or \"NO\" and nothing else.\nFigure 4: The prompt used for UrgentSFT, Instruct, and Reasoning Baselines.\nUrgentReward\n### Instruction: You are provided with a patient message sent to a clinician.\nYour job is to generate a new patient message that is **more medically urgent** than the provided patient message.\nOutput the **more urgent** patient message and nothing else.\n### Patient Message: { message }\n### More Urgent Patient Message:\nFigure 5: The prompt used for UrgentReward. This prompt differs as we leverage pre-trained reward models which\nare trained to score completions. We thus re-formulate the task to better utilize existing knowledge.\n19\n"}]}