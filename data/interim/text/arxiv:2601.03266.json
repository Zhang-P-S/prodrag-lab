{"doc_id": "arxiv:2601.03266", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.03266.pdf", "meta": {"doc_id": "arxiv:2601.03266", "source": "arxiv", "arxiv_id": "2601.03266", "title": "Benchmarking and Adapting On-Device Large Language Models for Clinical Decision Support", "authors": ["Alif Munim", "Jun Ma", "Omar Ibrahim", "Alhusain Abdalla", "Shuolin Yin", "Leo Chen", "Bo Wang"], "published": "2025-12-18T22:29:45Z", "updated": "2025-12-18T22:29:45Z", "summary": "Large language models (LLMs) have rapidly advanced in clinical decision-making, yet the deployment of proprietary systems is hindered by privacy concerns and reliance on cloud-based infrastructure. Open-source alternatives allow local inference but often require large model sizes that limit their use in resource-constrained clinical settings. Here, we benchmark two on-device LLMs, gpt-oss-20b and gpt-oss-120b, across three representative clinical tasks: general disease diagnosis, specialty-specific (ophthalmology) diagnosis and management, and simulation of human expert grading and evaluation. We compare their performance with state-of-the-art proprietary models (GPT-5 and o4-mini) and a leading open-source model (DeepSeek-R1), and we further evaluate the adaptability of on-device systems by fine-tuning gpt-oss-20b on general diagnostic data. Across tasks, gpt-oss models achieve performance comparable to or exceeding DeepSeek-R1 and o4-mini despite being substantially smaller. In addition, fine-tuning remarkably improves the diagnostic accuracy of gpt-oss-20b, enabling it to approach the performance of GPT-5. These findings highlight the potential of on-device LLMs to deliver accurate, adaptable, and privacy-preserving clinical decision support, offering a practical pathway for broader integration of LLMs into routine clinical practice.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.03266v1", "url_pdf": "https://arxiv.org/pdf/2601.03266.pdf", "meta_path": "data/raw/arxiv/meta/2601.03266.json", "sha256": "6a8fc3afc4897003bb8b8b924fba41130ebad803efa11e535505821435c8e681", "status": "ok", "fetched_at": "2026-02-18T02:24:05.294118+00:00"}, "pages": [{"page": 1, "text": "1\nBenchmarking and Adapting On-Device Large\nLanguage Models for Clinical Decision Support\nAlif Munim∗, Jun Ma∗, Omar Ibrahim∗, Alhusain Abdalla∗, Shuolin Yin, Leo Chen, and Bo Wang\nAbstract\nLarge language models (LLMs) have rapidly advanced in clinical decision-making, yet the deployment of proprietary\nsystems is hindered by privacy concerns and reliance on cloud-based infrastructure. Open-source alternatives allow local\ninference but often require large model sizes that limit their use in resource-constrained clinical settings. Here, we benchmark\ntwo on-device LLMs, gpt-oss-20b and gpt-oss-120b, across three representative clinical tasks: general disease diagnosis,\nspecialty-speciﬁc (ophthalmology) diagnosis and management, and simulation of human expert grading and evaluation.\nWe compare their performance with state-of-the-art proprietary models (GPT-5 and o4-mini) and a leading open-source\nmodel (DeepSeek-R1), and we further evaluate the adaptability of on-device systems by ﬁne-tuning gpt-oss-20b on general\ndiagnostic data. Across tasks, gpt-oss models achieve performance comparable to or exceeding DeepSeek-R1 and o4-\nmini despite being substantially smaller. In addition, ﬁne-tuning remarkably improves the diagnostic accuracy of gpt-oss-20b,\nenabling it to approach the performance of GPT-5. These ﬁndings highlight the potential of on-device LLMs to deliver accurate,\nadaptable, and privacy-preserving clinical decision support, offering a practical pathway for broader integration of LLMs into\nroutine clinical practice.\n✦\nINTRODUCTION\nLarge language models (LLMs) are rapidly transforming the landscape of clinical medicine. Trained on massive corpora of\ngeneral-domain and biomedical text, these models have demonstrated emergent reasoning abilities that enable comprehen-\nsive summaries from medical dialogue [1], disease diagnosis, and treatment planning [2]. For example, Med-PaLM [3], [4]\nand AMIE [5], [6] achieved near-clinician performance in open-ended medical question answering, differential diagnosis,\nand patient consultation simulations. In real-world evaluations [7], LLM-assisted physicians exhibited improved diagnostic\naccuracy and management decisions across diverse clinical scenarios. Together, these advances illustrate the growing\npotential of LLMs to augment clinical expertise and support decision-making at the point of care.\nDespite these achievements, the translation of LLMs into real-world clinical workﬂows remains limited. Most frontier\nLLMs are proprietary, cloud-hosted, and trained on non-transparent datasets, creating challenges related to data privacy,\nregulatory compliance, reproducibility, and cost. The transmission of patient information to external servers conﬂicts with\ndata-governance policies in many health institutions. Moreover, the high cost associated with developing, training, and\ndeploying large-scale proprietary models can be prohibitive for many healthcare institutions.\nThe open-source community has developed increasingly capable models that approach the performance of proprietary\nsystems while allowing full local control. Models such as DeepSeek-R1 [8] have demonstrated competitive reasoning and\nclinical comprehension across diagnostic and treatment-related tasks [9], [10]. However, the adoption of DeepSeek-R1 in\nclinical environments is constrained because of the large model size of 671 billion parameters, which requires substantial\ncomputational costs. The models of this scale not only hinder deployment in clinics with limited access to computing\nresources, but also make ﬁne-tuning for adaptation in evolving clinical domains prohibitively expensive. These limitations\nunderscore the need for smaller, efﬁcient models that retain clinical reasoning ability while remaining feasible for on-\npremise use.\nThe recent advancements in on-device LLMs offer a promising solution to the challenges posed by both large proprietary\nand open-source models. In particular, the gpt-oss family [11] represents this new generation of efﬁcient and privacy-\npreserving architectures. The gpt-oss-20b model and its larger counterpart, gpt-oss-120b, are designed and optimized for\ndeployment on a single consumer GPU with 16GB and 80GB memory, respectively. This study aims to determine whether\non-device LLMs can be practically used for clinical decision support. We systematically benchmark gpt-oss-20b and gpt-\noss-120b models across three representative tasks: general disease diagnosis, (ophthalmology) specialty-speciﬁc disease\n•\nAlif Munim is with AI Collaborative Centre, University Health Network, Toronto, Canada.\n•\nJun Ma is with AI Collaborative Centre and Princess Margaret Cancer Centre, University Health Network, Toronto, Canada. (∗Equal Contribution)\n•\nOmar Ibrahim is with AI Collaborative Centre, University Health Network, Toronto, Canada. (∗Equal Contribution)\n•\nAlhusain Abdalla is with AI Collaborative Centre, University Health Network, Toronto, Canada. (∗Equal Contribution)\n•\nShuolin Yin is with Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada.\n•\nLeo Chen is with Division of Urology, Department of Surgery, St. Michael’s Hospital, Unity Health Toronto and University of Toronto, Toronto, Canada\n•\nBo Wang (Corresponding Author) is with Peter Munk Cardiac Centre, University Health Network; Department of Laboratory Medicine and Pathobiology\nand Department of Computer Science, University of Toronto; Vector Institute, Toronto, Canada. E-mail: bowang@vectorinstitute.ai\narXiv:2601.03266v1  [cs.CL]  18 Dec 2025\n"}, {"page": 2, "text": "2\ndiagnosis and management, and simulation of human clinician judgement and evaluation (Fig. 1). To establish a clear\nperformance landscape, we compare these on-device models against leading proprietary models (GPT-5 and o4-mini) and\na strong open-source model (DeepSeek-R1).\nFig. 1. Overview of the benchmark framework. This study compares the on-device LLMs with state-of-the-art open-source and proprietary LLMs\nacross general disease diagnosis, specialty diagnosis and treatment recommendations on ophthalmology multiple-choice questions, and judgment\nfor open-ended clinical decision questions.\nRESULTS\nDataset and evaluation methods\nWe mainly focus on assessing the performance of LLMs on disease diagnosis, treatment recommendations, and simulating\nexpert judgment for open-ended questions, as these are common tasks in clinical practice. We curated three datasets\nand benchmarked model performance on three scenarios: LLM-as-a-generalist, LLM-as-a-specialist, and LLM-as-a-clinical-\njudge (Figure 1).\nSpeciﬁcally, to assess the capability of LLMs for general disease diagnosis, we collected 207 case reports from the Euro-\nrad library where each case contains clinical history, ﬁndings from associated medical images (e.g., computed tomography\n(CT), magnetic resonance imaging (MRI), and ultrasound), and a ﬁne-grained differential diagnosis list (Methods). The\ntask is to select the diagnosis from the list based on patient history and imaging ﬁndings (Supplementary Prompt 1).\nThe second dataset was from a ophthalmology multiple-choice question dataset [12], aiming to evaluate the specialty-\nspeciﬁc performance of LLMs (Methods). The dataset contains 39 diagnosis questions and 91 management questions. Each\nquestion contains patient sex, age, and examinations ﬁndings, such as visual acuity, intraocular pressure, and fundus\nexaminations, and a list of ﬁve to nine answer options. The task is to select correct answers, and each question may have\nmultiple correct answers (Supplementary Prompt 2).\nThe third dataset was adapted from the existing benchmarking study of the DeepSeek-R1 model [10], including 1315\ncases with human expert scores from 125 patients across ﬁve specialties (internal medicine, neurology, surgery, gynecology,\nand pediatrics) (Methods). LLMs are tasked to simulate human experts to assess given diagnoses from existing LLMs and\nassign a score from 1 to 5 based on a predeﬁned rubric. We compare these LLM-generated scores against ground-truth\nhuman expert scores (Supplementary Prompt 3-4).\nFor the LLM-as-a-generalist and LLM-as-a-specialist tasks, the evaluation was based on exact match accuracy For the\nLLM-as-a-clinical-judge task, we computed the relative error between the clinicians’ scores and the LLM’s scores. To ensure\nrigorous zero-shot evaluation and prevent data leakage, all three benchmark datasets were curated from cases released after\nthe training data cutoff for all evaluated LLMs.\nOn-device LLMs show competitive zero-shot performance\nFig. 2a compares the diagnostic accuracy of the evaluated models for general disease diagnosis across radiological\ncase reports (Supplementary Table 1). While the proprietary frontier model GPT-5 achieved the highest overall average\nperformance with an accuracy of 88.9% (95% CI: 83.9–92.5%), the on-device models demonstrated strong competitiveness\nrelative to the established baselines. Speciﬁcally, the gpt-oss-120b (L) model achieved an average accuracy of 84.1% (95%\nCI: 78.5–88.4%), effectively matching the efﬁcient proprietary model o4-mini (84.1%; 95% CI: 78.5–86.3%; p > 0.05) and\n"}, {"page": 3, "text": "3\nFig. 2. Zero-shot and ﬁne-tuning performance of on-device LLMs. a, Results of LLM-as-a-generalist: diagnosis accuracy on a wide range of\nradiological cases (N=207). L, M, and H denote low, medium, and high reasoning efforts, respectively. b, Results of LLM-as-a-specialist: accuracy\non ophthalmology cases (N=130) with diagnosis and management multiple-choice questions. c, Results of LLM-as-a-clinical-judge: violin plots\ncomparing the relative error for disease diagnosis and treatment open-ended question assessment (N=1315). d, Fine-tuned gpt-oss-20b (M) model\noutperforms proprietary (o4-mini) and open-source LLMs (DeepSeek-R1) on the disease differential diagnosis task. e, Model performance across\n10 radiological sub-specialties.\n"}, {"page": 4, "text": "4\nsurpassing the leading open-source model DeepSeek-R1 (81.6%; 95% CI: 75.8–86.3%; p < 0.01). Furthermore, the highly\nefﬁcient gpt-oss-20b (H) model secured an accuracy of 80.7% (95% CI: 74.8–85.5%), approaching the performance of the\nsigniﬁcantly larger DeepSeek-R1 (p > 0.05).\nFig. 2b presents the model performance of the LLM-as-a-specialist task on the ophthalmology QA dataset (Supplemen-\ntary Table 2). The open-source model DeepSeek-R1 achieved the highest overall accuracy of 79.2% (95% CI: 71.5–85.3%),\ndemonstrating particularly strong capabilities in diagnosis (92.3%; 95% CI: 79.7–97.3%) and management (73.6%; 95% CI:\n63.7–81.6%). Remarkably, the on-device gpt-oss-120b (M) model secured the second-highest rank with an overall accuracy\nof 77.7% (95% CI: 69.8–84.0%), surpassing both the proprietary GPT-5 (73.8%; 95% CI: 65.7–80.6%) and o4-mini (71.5%;\n95% CI: 63.3–78.6%) baselines (p < 0.01). This performance underscores the capacity of locally deployed large models to\nrival state-of-the-art proprietary systems in specialty-speciﬁc medical reasoning. While the smaller gpt-oss-20b variants\nshowed a performance gap, with the best-performing gpt-oss-20b (H) achieving 66.2% (95% CI: 57.7–73.7%), they remained\ncompetitive in speciﬁc subtasks (e.g., Refractive Disorders or Strabismus). Across all evaluated architectures, models\nconsistently demonstrated higher performance in diagnosis compared to patient management, reﬂecting the increased\ncomplexity of treatment planning in specialty care.\nFig. 2c shows the evaluation results of models acting as clinical judges, measured by the median relative error\n(interquartile range) between the model’s score and human expert consensus. The proprietary GPT-5 achieved the lowest\nmedian error for treatment-related judgments (0.00, IQR: -0.33–0.50), demonstrating high alignment with human experts\nacross specialties such as Internal Medicine and Pediatrics. However, for diagnostic assessment, GPT-5 exhibited a tendency\nto underestimate performance (median error: -0.50, IQR: -1.00–0.00). In contrast, the on-device gpt-oss-120b variants\ndemonstrated high stability, achieving a median error of 0.00 (IQR: -0.50–0.25) across all diagnostic tasks, signiﬁcantly\noutperforming o4-mini (p < 0.01), which showed a larger deviation (-1.00, IQR: -1.83–0.17). Among the open-source\nmodels, DeepSeek-R1 and the gpt-oss-20b series generally aligned well with human scores but displayed slightly higher\nvariability in treatment evaluations compared to the larger 120b models. When stratiﬁed by disease frequency, the gpt-oss-\n120b models maintained consistent accuracy across “Rare”, “Less Frequent”, and “Frequent” conditions, suggesting that\nlarger on-device models can offer robust, human-aligned evaluation capabilities.\nModel reliability was further assessed using standard agreement and stability metrics. Inter-model agreement, measured\nusing Cohen’s κ, was moderate for diagnostic reasoning tasks (κ ≈0.45–0.87) and lower for treatment evaluation tasks\n(κ ≈0.20–0.43), reﬂecting the open-ended and subjective nature of clinical management decisions. In contrast, intra-model\nstability across repeated inference runs was high. For diagnostic judgment tasks, the Intraclass Correlation Coefﬁcient (ICC)\nindicated excellent reliability across all models (ICC ≈0.91–0.97), while treatment scoring showed greater variability, with\nproprietary models exhibiting higher stability than open and on-device models. Together, these results support the use of\nconsensus-based aggregation to mitigate stochastic variability and produce reliable clinical outputs.\nFine-tuning substantially improves the diagnostic capability of on-device LLMs\nTo assess the adaptability of on-device LLMs for clinical decision support, we ﬁne-tuned the gpt-oss-20b (M) model on\nthe general disease diagnosis dataset (Methods). As shown in Fig. 2d (Supplementary Fig. 1–3), the ﬁne-tuned model\nproduced a marked improvement in diagnostic accuracy, increasing from 77.3% (95% CI: 71.1-82.5%) in the base model to\n86.5% (95% CI: 81.1-90.5%) after adaptation. This performance not only exceeded the accuracy of DeepSeek-R1 (81.6%, 95%\nCI: 75.8-86.3%) and o4-mini (84.1%, 95% CI: 78.5-88.4%) but also approached that of the state-of-the-art proprietary GPT-5\nmodel (88.9%, 95% CI: 83.9-92.5%).\nTo illustrate the impact of ﬁne-tuning on both diagnostic accuracy and reasoning quality, we present three case studies\n(Supplementary Cases 1–3). Case 1 (Von Hippel-Lindau syndrome) shows ﬁne-tuning enhances reasoning quality even\nwhen base models reach correct diagnoses: the base model provided superﬁcial comparisons with uncertain statements,\nwhile the ﬁne-tuned model employed systematic multi-system integration with detailed syndrome comparison and deﬁni-\ntive synthesis. Cases 2–3 demonstrate error correction through recognition of critical clinical features. Furthermore, the\nsub-speciality-wise analysis (Fig. 2e, Supplementary Fig. 4) reveals that ﬁne-tuning effectively mitigates domain-speciﬁc\nweaknesses observed in base models, particularly in specialized areas such as cardiovascular and breast imaging.\nThe substantial performance gain demonstrates that targeted domain adaptation can effectively compensate for the\nsmaller scale of on-device models. Even with medium reasoning effort, the ﬁne-tuned gpt-oss-20b model exhibits diagnostic\naccuracy comparable to signiﬁcantly larger proprietary systems while retaining the advantages of local deployability and\nprivacy preservation. These ﬁndings highlight the strong potential of lightweight, ﬁne-tuned on-device LLMs to provide\nhigh-quality clinical decision support in settings with limited computational resources.\nDISCUSSION\nThis study systematically evaluates the capabilities of on-device large language models across three representative clinical\ntasks: general diagnosis, specialty-level reasoning, and simulation of expert judgment. Across all settings, the gpt-oss mod-\nels demonstrate clinically meaningful performance despite their substantially smaller scale relative to frontier proprietary\nmodels, underscoring the feasibility of deploying lightweight LLMs directly within healthcare institutions.\nIn the general disease diagnosis task, on-device models achieved strong zero-shot performance, showing that compact\narchitectures can capture the broad clinical reasoning patterns required for diagnosis. Moreover, in the ophthalmology\n"}, {"page": 5, "text": "5\nspecialty task, the on-device gpt-oss-120b model outperformed both GPT-5 and o4-mini while ranking second overall\nbehind DeepSeek-R1. This is noteworthy given that most existing clinical LLM evaluations have focused on large cloud-\nbased models, leaving open the question of whether smaller locally deployable systems can achieve similar levels of\ngeneralization. Our ﬁndings indicate that modern on-device architectures can support robust performance in common\ndiagnostic scenarios without requiring external computation or data transfer.\nThe ability of LLMs to act as ”clinical judges” is critical for scalable quality assurance and automated evaluation. Our\nresults indicate that on-device models can align closely with human expert consensus, often exhibiting greater stability\nthan efﬁcient proprietary alternatives. This suggests that compact models can support evaluative tasks that require not\nonly factual knowledge but also sensitivity to clinical reasoning standards and rubric-based assessment criteria. This\ncapability is essential for deploying LLMs as automated auditors or second-opinion systems in clinical workﬂows.\nBeyond zero-shot performance, the adaptability of on-device models represents a substantial advantage over closed-\nsource systems. Fine-tuning the gpt-oss-20b model on general diagnosis data substantially improved its performance,\neven exceeding the diagnostic accuracy of several much larger frontier models. This result suggests that domain-speciﬁc\noptimization can efﬁciently compensate for smaller model scale. Fine-tuning also enhanced robustness across radiology\nsubspecialties, yielding more uniform performance and mitigating weaknesses observed in the base model. These ﬁndings\nhighlight that clinics can deploy efﬁcient, low-cost, customized AI tools tailored to their speciﬁc patient demographics and\ndisease prevalences without compromising data privacy.\nTaken together, the experiments in this study highlight three key insights. First, compact on-device LLMs can provide\nstrong general diagnostic reasoning and domain-speciﬁc performance. Second, on-device LLMs can approximate expert\njudgment with surprising ﬁdelity, positioning them as valuable components of locally governed AI ecosystems that support\nboth clinical decision-making and meta-evaluative tasks. Third, ﬁne-tuning plays a critical role in achieving competitive\naccuracy across diverse subspecialties, allowing healthcare institutions to develop tailored high-performing models from\nrelatively small architectures.\nOur study also has limitations. First, while our benchmarks cover diagnosis, management, and evaluation, they rely\non retrospective patient cases and examination questions, which may not fully capture the complexity and noise of real-\ntime clinical environments. Local deployment mitigates privacy and data governance concerns but introduces operational\nchallenges, including hardware reliability, secure integration with clinical information systems, and ongoing monitoring of\nmodel behavior. Fine-tuning requires access to high-quality labeled data, which may be limited in certain specialties.\nIn summary, this work demonstrates that on-device LLMs offer a promising and practical alternative to large pro-\nprietary systems for clinical decision support. These compact models can achieve reliable diagnostic reasoning, robust\nsubspecialty performance, and alignment with expert judgment—–all while maintaining strict control over patient data\nand computational infrastructure. These features position on-device LLMs as strong candidates for safe, scalable, and\nequitable integration of AI into clinical practice.\nMETHODS\nDataset curation and pre-processing\nCase reports for the LLM-as-a-generalist task were curated from the European Society of Radiology (Eurorad) (https:\n//www.eurorad.org/) database across a broad spectrum of subspecialties, including musculoskeletal, cardiovascular,\nabdominal, uroradiology, neuroradiology, paediatric, head and neck, breast, and chest imaging. To strictly mitigate data\nleakage, the test set was restricted to cases published in 2025, postdating the training cut-off of the evaluated models.\nFrom an initial pool of 350 prospective test cases, we employed GPT-5 to screen and exclude reports containing explicit\ndiagnostic disclosures in the description. Historical cases published prior to 2025 were allocated for ﬁne-tuning, yielding a\nﬁnal dataset comprising 1,895 training cases and 207 independent test cases.\nCases for the LLM-as-a-specialist task were based on the recent ophthalmology multiple-choice question dataset [12].\nIt contains 130 questions covering six topics: anterior segment diseases, external eye/orbital diseases, glaucoma, ocular\ntrauma, refractive disorders/strabismus, and retinal diseases. Each question has one to six correct answers from ﬁve to\nnine answer choices.\nCases for the LLM-as-a-clinical-judge scoring task were curated from the benchmark data in [10], comprising 125 patient\ncases. Each case included a chief complaint and up to ﬁve diagnostic or treatment recommendations generated by distinct\nmodels, including GPT-4, GPT-4o, GPT-3.5, Gem2FTE, and DeepSeek-R1. In this task, the evaluated LLMs were required\nto audit these predictions by assigning quality scores rather than generating de novo diagnoses. Performance was assessed\nby measuring the concordance between model-generated scores and reference ratings provided by medical experts.\nTask-speciﬁc inference protocols\nTo ensure fair comparison, we developed standardized zero-shot inference pipelines for each clinical task. All protocols\nutilized a self-consistency framework where each case was queried three independent times (k = 3).\nGeneral Radiology Diagnosis. The LLM-as-a-generalist diagnostic task was formulated as a constrained single-label\nselection problem (Supplementary Prompt 1). Models were provided with the patient history and imaging ﬁndings and\ninstructed to select the most likely diagnosis strictly verbatim from a provided differential list. Post-processing utilized a\ndeterministic regex-based extractor to isolate the ﬁnal diagnosis from the generated reasoning stream.\n"}, {"page": 6, "text": "6\nOphthalmology Specialty QA. The LLM-as-a-specialist task involved complex multiple-choice questions (MCQs)\nrequiring multi-label classiﬁcation. Unlike the radiology task, models were instructed to “Select ALL correct answers”\nfrom options A–Z. The system prompt enforced a strict output format consisting only of concatenated capital letters (e.g.,\n“ABE” or “D”), prohibiting explanatory text in the ﬁnal output to facilitate automated parsing (Supplementary Prompt 2).\nClinical Judgment Simulation. The LLM-as-a-clinical-judge task evaluated the models’ ability to simulate expert\nclinical judgment. Models were presented with a clinical case adapted from [10], alongside a candidate diagnosis/treatment\nplan, and a reference standard. They were instructed to assign a quality score on a 5-point Likert scale (1 = Most relevant\noptions missing, 5 = All relevant options mentioned) based on a strict scoring rubric. Half-point scores (e.g., 4.5) were\npermitted to capture granular distinctions in quality (Supplementary Prompt 3).\nInference Implementation\nWe benchmarked three distinct categories of models to represent the current landscape of LLMs:\nProprietary Frontier Models (GPT-5 and o4-mini). We selected the latest snapshots of OpenAI’s premier reasoning\nmodels, gpt-5-2025-08-07 and o4-mini-2025-04-16. These models represent the current state-of-the-art in closed-\nsource reasoning, utilizing hidden chain-of-thought (CoT) processing to plan responses before output generation. Inference\nwas conducted via the OpenAI Responses API. To evaluate the impact of inference-time compute, we modulated the\nreasoning_effort parameter (low, medium, high). For all tasks, we utilized the default medium effort setting.\nOpen-Source State-of-the-Art Model (DeepSeek-R1). To represent the pinnacle of open-source capability, we evaluated\nthe 671-billion parameter DeepSeek-R1-0528. This model utilizes large-scale reinforcement learning to optimize reason-\ning paths and is currently the strongest non-proprietary baseline available. Inference was performed via the OpenRouter\nAPI. We utilized a context window of 8,192 tokens to strictly enforce the capture of the model’s native reinforcement-\nlearning-aligned reasoning traces.\nOn-Device Models (gpt-oss-20b and gpt-oss-120b). The primary focus of this study is the gpt-oss family, a new\ngeneration of efﬁcient, privacy-preserving architectures designed for local deployment. We evaluated both the 20-billion\nparameter variant (optimized for consumer GPUs) and the 120-billion parameter variant to assess the trade-off between\ncomputational efﬁciency and diagnostic reasoning capability. The gpt-oss models were evaluated using the Hugging Face\nInference Router, targeting the Fireworks AI provider for the 20b model and Cerebras for the 120b model to maximize\nthroughput. Similar to the proprietary OpenAI models, we modulated the reasoning_effort parameter (low, medium,\nhigh), in this case through the system prompt (e.g. by appending “Reasoning: high” for the highest level of reasoning) to\ntrigger extended chain-of-thought generation patterns. Output parsing relied on a custom regex extractor to identify valid\nanswer sequences within the generated text.\nTraining Dataset Preparation for Radiological Cases\ngpt-oss represents a family of reasoning-capable language models designed to generate structured Chain-of-Thought (CoT)\nreasoning during inference. To develop high-quality training data for ﬁne-tuning gpt-oss-20b on radiological diagnosis\ntasks, we employed gpt-oss-120b—a substantially larger model from the same architectural family—to curate systematic\ndiagnostic reasoning data for all 1,895 cases in the Eurorad dataset. The use of gpt-oss-120b for data curation was predicated\non three key advantages: (i) architectural consistency between the 120b and 20b variants ensures compatibility of reasoning\npatterns, (ii) the larger parameter count enables more sophisticated medical reasoning and systematic differential diagnosis\nevaluation, and (iii) automated generation provides scalable, consistent reasoning data across the entire dataset.\nFor each radiological case, gpt-oss-120b was provided with the clinical case presentation (comprising patient history and\nimaging ﬁndings), the original expert radiologist discussion from the Eurorad dataset as contextual grounding, and the list\nof differential diagnoses. The original discussion, which provided academic descriptions of case characteristics and imaging\nﬁndings, served as reference material to inform the generation of improved systematic reasoning. The model was instructed\nto generate Chain-of-Thought reasoning following a structured four-step diagnostic framework: (1) symptom-ﬁnding\ncorrelation—establishing connections between clinical presentation and imaging observations, (2) differential mapping—\nevaluating how imaging ﬁndings support or contradict each candidate diagnosis, (3) systematic elimination—providing\nexplicit reasoning for excluding less likely diagnostic options, and (4) diagnostic convergence—demonstrating the logical\npathway to the ﬁnal diagnosis. Reasoning generation employed the following parameters: temperature = 0.6, maximum\noutput tokens = 2000, target length = 200–400 words, accessed via the HuggingFace Inference API.\nGenerated reasoning samples underwent systematic validation to ensure data quality. We veriﬁed that all 1,895 gen-\nerated reasoning chains converged to the correct ground truth diagnosis, conﬁrming alignment between the model’s\nreasoning process and the clinically validated diagnoses. Automated quality metrics assessed each response for: (i) appro-\npriate length (200–600 words acceptable range), (ii) presence of all four required reasoning components (symptom-ﬁnding\ncorrelation, differential mapping, systematic elimination, and diagnostic convergence), and (iii) response completeness\n(absence of premature truncation). Additionally, a subset of cases was manually inspected to validate that the systematic\nelimination reasoning across all four reasoning phases logically progressed toward the correct diagnosis, and that the ﬁnal\ndiagnosis selection matched the ground truth. For this subset, the generated reasoning was also compared against the\noriginal discussions to ensure clinical accuracy and logical coherence. The complete dataset comprising all 1,895 cases was\nused for gpt-oss-20b ﬁne-tuning.\n"}, {"page": 7, "text": "7\nTraining Protocol\nFine-tuning of gpt-oss-20b (M) was performed using the curated reasoning dataset generated by gpt-oss-120b. The model\nwas loaded using the Unsloth framework with 4-bit quantization to enable efﬁcient training on limited computational\nresources while maintaining model performance. A maximum sequence length of 4,096 tokens was conﬁgured to accom-\nmodate the clinical case presentations and associated reasoning chains.\nParameter-efﬁcient ﬁne-tuning was implemented using Low-Rank Adaptation (LoRA) with rank r=32 and alpha=64,\ntargeting all linear layers in the model architecture, including query, key, value, and output projections, as well as the\nfeed-forward network components (gate, up, and down projections) across all 32 transformer layers. Additionally, expert\nlayers within the mixture-of-experts architecture were targeted at strategic depths: early layers (0-7) for initial processing,\nmiddle layers (8-15) for pattern recognition and reasoning, upper layers (16-23) for deep reasoning, and deep layers (24-31)\nfor ﬁnal reﬁnement and output generation. This comprehensive targeting strategy ensured that the model could effectively\nlearn diagnostic reasoning patterns at multiple levels of abstraction. A LoRA dropout rate of 0.05 was applied to prevent\noverﬁtting, and gradient checkpointing was enabled to reduce memory consumption during training.\nThe curated reasoning data was formatted using the gpt-oss chat template with medium reasoning effort, incorporating\nthe gpt-oss-120b-generated reasoning as structured thinking content. This approach enabled the model to learn from\nthe systematic diagnostic patterns demonstrated by the larger model while maintaining compatibility with the gpt-oss\nreasoning framework. Training was conducted over 3 epochs using the AdamW optimizer with a learning rate of 1×10−4,\ncosine learning rate scheduling, and a warmup ratio of 0.1. Mixed-precision training with bﬂoat16 was utilized to accelerate\ncomputation while maintaining numerical stability. All experiments were conducted with ﬁxed random seeds to ensure\nreproducibility.\nInference protocol of ﬁne-tuned model\nTo evaluate the ﬁne-tuned gpt-oss-20b model on radiological diagnosis, we designed a controlled inference pipeline\noptimized for the exploration of deterministic and diverse hypotheses. All predictions were generated using group\nbeam search, a decoding strategy that encourages exploration across diverse reasoning paths while also maintaining\nstability in high-stakes clinical settings. After systematic experimentation with different decoding settings, we found that\na conﬁguration of 13 beams, 13 beam groups, and a diversity penalty of 0.5 provided the strongest performance on the\nEurorad validation set. This setup enforced full beam-group separation, ensuring that each beam starts its own reasoning\ntrajectory and looks at the presented case from a different angle, which is particularly effective in reducing mode collapse\nand repetitive reasoning. Maximum generation length was set to 3,000 tokens to accommodate longer Chain-of-Thought\noutputs, and sampling was disabled to ensure reproducibility across runs.\nFor each case, the model produced 13 independent reasoning traces. Final predictions were obtained using a majority-\nvote aggregation over the extracted diagnostic answers, with ties resolved by selecting the earliest beam. All inputs were\nencoded with the gpt-oss chat template using left padding and a 4,096-token context window, and inference was executed\nusing the Unsloth runtime with 4-bit quantized weights and attached LoRA adapters. This protocol allowed the model to\nbalance the range of diagnostic reasoning with reliability, resulting in a stable exact-match performance while preserving a\nclinically interpretable diagnostic rationale.\nStatistical analysis\nTo account for the stochastic nature of Large Language Model (LLM) generation, we employed a self-consistency frame-\nwork wherein each model was queried three independent times (k = 3) for every case. All performance metrics and\nstatistical comparisons were derived from the consensus prediction of these three runs. For nominal tasks, including the\ngeneral diagnosis questions and ophthalmology specialty-speciﬁc questions, the consensus was determined via majority\nvoting, where a prediction was considered correct only if the correct answer was generated in at least two of the three runs.\nFor ordinal tasks evaluated on a 5-point Likert scale (LLM-as-a-judge for diagnosis and treatment scoring), the consensus\nwas deﬁned as the mean score of the three runs to obtain a stable per-case consensus score.\nModel performance for nominal tasks was reported as accuracy, with 95% Conﬁdence Intervals (CIs) calculated using\nthe Wilson Score Interval method to provide robust estimates for binomial proportions. For the clinical judge task,\nperformance was reported as the signed median error across cases between the model’s mean consensus score and the\nground-truth human expert score, summarized using the interquartile range (IQR). Statistical signiﬁcance between model\nperformances was determined using pairwise hypothesis tests on paired samples evaluated on the same test cases, with\na signiﬁcance threshold of P < 0.05. Differences in accuracy for nominal tasks were assessed using McNemar’s Test with\ncontinuity correction, while differences in the distribution of errors for ordinal clinical judgment tasks were assessed using\nthe Wilcoxon Signed-Rank Test.\nWe further evaluated both the internal stability of the models and agreement between models using metrics appropriate\nto the nominal and ordinal structure of the evaluation tasks. To measure generation consistency across the three inference\nruns (intra-model stability), we calculated Fleiss’ Kappa (κ) for nominal datasets and the Intraclass Correlation Coefﬁcient\n(ICC, form 3,k) for ordinal datasets. To assess the degree to which different models converged on identical predictions\nindependent of ground truth (inter-model agreement), we calculated standard Cohen’s Kappa for nominal tasks. For\nordinal LLM-as-a-judge scoring tasks, Linear Weighted Kappa was employed to penalize partial disagreements (e.g., scores\nof 4 vs. 5) less severely than complete disagreements.\n"}, {"page": 8, "text": "8\nData Availability\nThe benchmarking results and model outputs generated in this study are available in the Supplementary Information.\nThe raw input data for the general diagnosis task are available from the Eurorad library (https://www.eurorad.org/); a\nscript to retrieve the speciﬁc cases used in this study is provided in the code repository. The ophthalmology and clinical\njudge datasets are publicly available at https://github.com/bowang-lab/on-device-LLM. The training dataset with gpt-\noss-120b reasoning enhancement used to ﬁne-tune the model is available at https://huggingface.co/datasets/omareng/\neurorad-gpt-oss-training-data.\nCode Availability\nThe code for model inference, benchmarking, and evaluation is publicly available on GitHub at https://github.com/\nbowang-lab/on-device-LLM. The ﬁne-tuned model weights are available on HuggingFace at https://huggingface.co/\nomareng/on-device-LLM-gpt-oss-20b.\nAcknowledgements\nThis work was supported by the Natural Sciences and Engineering Research Council of Canada (RGPIN-2020-06189 and\nDGECR-2020-00294) and CIFAR AI Chair programs. This research was enabled, in part, by computing resources provided\nby the Digital Research Alliance of Canada.\nREFERENCES\n[1]\nD. Van Veen, C. Van Uden, L. Blankemeier, J.-B. Delbrouck, A. Aali, C. Bluethgen, A. Pareek, M. Polacin, E. P. Reis, A. Seehofnerov´a,\nN. Rohatgi, P. Hosamani, W. Collins, N. Ahuja, C. P. Langlotz, J. Hom, S. Gatidis, J. Pauly, and A. S. Chaudhari, “Adapted large language\nmodels can outperform medical experts in clinical text summarization,” Nature Medicine, vol. 30, no. 4, p. 1134–1142, 2024.\n[2]\nX. Liu, H. Liu, G. Yang, Z. Jiang, S. Cui, Z. Zhang, H. Wang, L. Tao, Y. Sun, Z. Song, T. Hong, J. Yang, T. Gao, J. Zhang, X. Li, J. Zhang, Y. Sang,\nZ. Yang, K. Xue, S. Wu, P. Zhang, J. Yang, C. Song, and G. Wang, “A generalist medical language model for disease diagnosis assistance,”\nNature Medicine, vol. 31, no. 3, pp. 932–942, 2025.\n[3]\nK. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, P. Payne, M. Seneviratne,\nP. Gamble, C. Kelly, A. Babiker, N. Sch¨arli, A. Chowdhery, P. Mansﬁeld, D. Demner-Fushman, B. Ag¨uera y Arcas, D. Webster, G. S. Corrado,\nY. Matias, K. Chou, J. Gottweis, N. Tomasev, Y. Liu, A. Rajkomar, J. Barral, C. Semturs, A. Karthikesalingam, and V. Natarajan, “Large\nlanguage models encode clinical knowledge,” Nature, vol. 620, no. 7972, p. 172–180, 2023.\n[4]\nK. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, M. Amin, L. Hou, K. Clark, S. R. Pfohl, H. Cole-Lewis, D. Neal, Q. M. Rashid,\nM. Schaekermann, A. Wang, D. Dash, J. H. Chen, N. H. Shah, S. Lachgar, P. A. Mansﬁeld, S. Prakash, B. Green, E. Dominowska,\nB. Ag¨uera y Arcas, N. Tomaˇsev, Y. Liu, R. Wong, C. Semturs, S. S. Mahdavi, J. K. Barral, D. R. Webster, G. S. Corrado, Y. Matias, S. Azizi,\nA. Karthikesalingam, and V. Natarajan, “Toward expert-level medical question answering with large language models,” Nature Medicine,\nvol. 31, no. 3, p. 943–950, 2025.\n[5]\nD. McDuff, M. Schaekermann, T. Tu, A. Palepu, A. Wang, J. Garrison, K. Singhal, Y. Sharma, S. Azizi, K. Kulkarni, L. Hou, Y. Cheng, Y. Liu,\nS. S. Mahdavi, S. Prakash, A. Pathak, C. Semturs, S. Patel, D. R. Webster, E. Dominowska, J. Gottweis, J. Barral, K. Chou, G. S. Corrado,\nY. Matias, J. Sunshine, A. Karthikesalingam, and V. Natarajan, “Towards accurate differential diagnosis with large language models,” Nature,\nvol. 642, no. 8067, p. 451–457, 2025.\n[6]\nT. Tu, M. Schaekermann, A. Palepu, K. Saab, J. Freyberg, R. Tanno, A. Wang, B. Li, M. Amin, Y. Cheng, E. Vedadi, N. Tomasev, S. Azizi,\nK. Singhal, L. Hou, A. Webson, K. Kulkarni, S. S. Mahdavi, C. Semturs, J. Gottweis, J. Barral, K. Chou, G. S. Corrado, Y. Matias,\nA. Karthikesalingam, and V. Natarajan, “Towards conversational diagnostic artiﬁcial intelligence,” Nature, vol. 642, no. 8067, p. 442–450,\nApr. 2025.\n[7]\nR. Korom, S. Kiptinness, N. Adan, K. Said, C. Ithuli, O. Rotich, B. Kimani, I. King’ori, S. Kamau, E. Atemba et al., “AI-based clinical decision\nsupport for primary care: A real-world study,” arXiv preprint arXiv:2507.16947, 2025.\n[8]\nD. Guo, D. Yang, H. Zhang, J. Song, P. Wang, Q. Zhu, R. Xu, R. Zhang, S. Ma, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao,\nZ. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo,\nG. Hao, G. Chen, G. Li, H. Zhang, H. Xu, H. Ding, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Chen, J. Yuan, J. Tu, J. Qiu, J. Li, J. L. Cai, J. Ni,\nJ. Liang, J. Chen, K. Dong, K. Hu, K. You, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia,\nM. Zhang, M. Zhang, M. Tang, M. Zhou, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang,\nR. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, T. Yun,\nT. Pei, T. Sun, T. Wang, W. Zeng, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng,\nX. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan,\nY. K. Li, Y. Q. Wang, Y. X. Wei, Y. Zhang, Y. Xu, Y. Li, Y. Zhao, Y. Sun, Y. Wang, Y. Yu, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Piao, Y. Wang,\nY. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Ou, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Xiong, Y. Luo, Y. You, Y. Liu, Y. Zhou, Y. X. Zhu, Y. Huang, Y. Li,\nY. Zheng, Y. Zhu, Y. Ma, Y. Tang, Y. Zha, Y. Yan, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu,\nZ. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang, “DeepSeek-R1 incentivizes reasoning in LLMs\nthrough reinforcement learning,” Nature, vol. 645, no. 8081, p. 633–638, 2025.\n[9]\nM. Tordjman, Z. Liu, M. Yuce, V. Fauveau, Y. Mei, J. Hadjadj, I. Bolger, H. Almansour, C. Horst, A. S. Parihar, A. Geahchan, A. Meribout,\nN. Yatim, N. Ng, P. Robson, A. Zhou, S. Lewis, M. Huang, T. Deyer, B. Taouli, H.-C. Lee, Z. A. Fayad, and X. Mei, “Comparative benchmarking\nof the deepseek large language model on medical tasks and clinical reasoning,” Nature Medicine, vol. 31, no. 8, p. 2550–2555, 2025.\n[10] S. Sandmann, S. Hegselmann, M. Fujarski, L. Bickmann, B. Wild, R. Eils, and J. Varghese, “Benchmark evaluation of deepseek large language\nmodels in clinical decision-making,” Nature Medicine, vol. 31, no. 8, p. 2546–2549, 2025.\n"}, {"page": 9, "text": "9\n[11] OpenAI, S. Agarwal, L. Ahmad, J. Ai, S. Altman, A. Applebaum, E. Arbus, R. K. Arora, Y. Bai, B. Baker, H. Bao, B. Barak, A. Bennett,\nT. Bertao, N. Brett, E. Brevdo, G. Brockman, S. Bubeck, C. Chang, K. Chen, M. Chen, E. Cheung, A. Clark, D. Cook, M. Dukhan, C. Dvorak,\nK. Fives, V. Fomenko, T. Garipov, K. Georgiev, M. Glaese, T. Gogineni, A. Goucher, L. Gross, K. G. Guzman, J. Hallman, J. Hehir, J. Heidecke,\nA. Helyar, H. Hu, R. Huet, J. Huh, S. Jain, Z. Johnson, C. Koch, I. Kofman, D. Kundel, J. Kwon, V. Kyrylov, E. Y. Le, G. Leclerc, J. P. Lennon,\nS. Lessans, M. Lezcano-Casado, Y. Li, Z. Li, J. Lin, J. Liss, L. X. Liu, J. Liu, K. Lu, C. Lu, Z. Martinovic, L. McCallum, J. McGrath, S. McKinney,\nA. McLaughlin, S. Mei, S. Mostovoy, T. Mu, G. Myles, A. Neitz, A. Nichol, J. Pachocki, A. Paino, D. Palmie, A. Pantuliano, G. Parascandolo,\nJ. Park, L. Pathak, C. Paz, L. Peran, D. Pimenov, M. Pokrass, E. Proehl, H. Qiu, G. Raila, F. Raso, H. Ren, K. Richardson, D. Robinson,\nB. Rotsted, H. Salman, S. Sanjeev, M. Schwarzer, D. Sculley, H. Sikchi, K. Simon, K. Singhal, Y. Song, D. Stuckey, Z. Sun, P. Tillet, S. Toizer,\nF. Tsimpourlas, N. Vyas, E. Wallace, X. Wang, M. Wang, O. Watkins, K. Weil, A. Wendling, K. Whinnery, C. Whitney, H. Wong, L. Yang,\nY. Yang, M. Yasunaga, K. Ying, W. Zaremba, W. Zhan, C. Zhang, B. Zhang, E. Zhang, and S. Zhao, “gpt-oss-120b & gpt-oss-20b model card,”\narXiv preprint arXiv:2508.10925, 2025.\n[12] P. Xu, Y. Wu, K. Jin, X. Chen, M. He, and D. Shi, “DeepSeek-R1 outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in bilingual complex\nophthalmology reasoning,” Advances in Ophthalmology Practice and Research, vol. 5, no. 3, pp. 189–195, 2025.\n"}, {"page": 10, "text": "1\nSupplementary\n✦\nPrompt\nPrompt 1. Prompt for General Radiology Diagnosis\nYou are a careful radiology diagnosis selector. Given a clinical case description and a ﬁnite list of candidate diagnoses,\nchoose the single most likely ﬁnal diagnosis FROM THE LIST.\nResponse rules:\n1) Output EXACTLY one option, copied VERBATIM from the list.\n2) Output ONLY the diagnosis text. No explanation. No punctuation. No quotes.\nPrompt\nPrompt 2. Prompts for Ophthalmology QA\nYou are a careful ophthalmology question-answering assistant. You will be given a multiple-choice case with options\nlabeled A–Z. Some questions have a single correct answer, while others have multiple correct answers. Select ALL\ncorrect answers. If only one answer is correct, return just that single letter. Respond with ONLY the capital letters\n(A–Z), concatenated together with no spaces or punctuation (e.g., ’ABE’ for multiple answers, or ’D’ if only one). Do\nnot explain.\nCase: [Case Data Inserted Here]\nTask: Choose the correct answer(s).\nReturn: One or more letters from A to Z, concatenated with no spaces (e.g., ABE or D).\n"}, {"page": 11, "text": "2\nPrompt\nPrompt 3. Prompt for LLM-as-a-Clinical-Judge on Diagnosis Tasks\nYou are asked to evaluate the quality of a model’s diagnostic output using the following rubric:\nScoring Rubric (Likert scale 1–5):\n1. Most relevant diagnoses not mentioned.\n2. Many relevant diagnoses missing or incorrectly identiﬁed.\n3. Some relevant diagnoses mentioned, but important omissions or inaccuracies present.\n4. Most relevant diagnoses correctly identiﬁed, with only minor omissions.\n5. All relevant diagnoses correctly identiﬁed.\nInstruction:\nGiven the following task description, the true disease, and the model output, assign a single integer score from 1 to\n5 according to the rubric. Half-point scores (e.g., 1.5, 2.5, 3.5, 4.5) are allowed if the quality falls between two rubric\nlevels. Output only the score, with no explanation or justiﬁcation.\nInputs:\n[Case Data Inserted Here]\nPrompt\nPrompt 4. Prompt for LLM-as-a-Clinical-Judge on Treatment Tasks\nYou are asked to evaluate the quality of a model’s treatment suggestion output using the following rubric:\nScoring Rubric (Likert scale 1–5):\n1. All or most suggested options redundant or unjustiﬁed.\n2. Some suggested options redundant or unjustiﬁed.\n3. Most suggested options appropriate, but minor redundancy or weak justiﬁcation present.\n4. Few suggested options redundant or unjustiﬁed.\n5. No suggested options redundant or unjustiﬁed.\nInstruction:\nGiven the following task description, the true disease, and the model output, assign a score from 1 to 5 according to\nthe rubric. Half-point scores (e.g., 1.5, 2.5, 3.5, 4.5) are allowed if the quality falls between two rubric levels. Output\nonly the score, with no explanation or justiﬁcation.\nInputs:\n[Case Data Inserted Here]\n"}, {"page": 12, "text": "3\nPrompt\nPrompt 5. Prompt for Reasoning Data Generation, Fine-tuning gpt-oss-20b, and Inference (Same prompt for all)\nYou are an expert radiologist demonstrating step-by-step diagnostic reasoning.\nCase presentation:\n[Case Data Inserted Here]\nDifferential diagnoses to consider:\n[Answer Options]\nGenerate systematic Chain-of-Thought reasoning that shows how clinicians think through cases:\n1. Connect symptoms to ﬁndings: Link clinical presentation with imaging observations\n2. Map to differentials: Show how ﬁndings support or contradict each differential diagnosis\n3. Systematic elimination: Explicitly rule out less likely options with reasoning\n4. Converge to answer: Demonstrate the logical path to the correct diagnosis\n"}, {"page": 13, "text": "4\nTABLE 1\nLLM-as-a-Generalist Task: Comparative diagnostic accuracy across radiological anatomical subgroups. Performance is evaluated using self-consistency majority voting accuracy (%, 95% CI).\nWhile the proprietary GPT-5 sets the highest overall average performance (88.9% average), the on-device gpt-oss-120b (L) model achieves an average accuracy of 82.3%, effectively matching the\nopen-source frontier DeepSeek-R1 (82.3%) and approaching the efﬁcient proprietary baseline o4-mini (85.2%).\nCategory\nGPT-5\no4-mini\nDeepSeek-R1\ngpt-oss-20b (L)\ngpt-oss-20b (M)\ngpt-oss-20b (H)\ngpt-oss-120b (L)\ngpt-oss-120b (M)\ngpt-oss-120b (H)\nMusculoskeletal system\n87.2 (74.8–94.0)\n85.1 (72.3–92.6)\n87.2 (74.8–94.0)\n80.9 (67.5–89.6)\n85.1 (72.3–92.6)\n87.2 (74.8–94.0)\n89.4 (77.4–95.4)\n83.0 (69.9–91.1)\n85.1 (72.3–92.6)\nCardiovascular\n83.3 (43.6–97.0)\n83.3 (43.6–97.0)\n66.7 (30.0–90.3)\n50.0 (18.8–81.2)\n50.0 (18.8–81.2)\n50.0 (18.8–81.2)\n66.7 (30.0–90.3)\n66.7 (30.0–90.3)\n66.7 (30.0–90.3)\nAbdominal imaging\n94.4 (81.9–98.5)\n83.3 (43.6–97.0)\n75.0 (58.9–86.2)\n63.9 (47.6–77.5)\n66.7 (50.3–79.8)\n80.6 (65.0–90.2)\n86.1 (71.3–93.9)\n83.3 (68.1–92.1)\n83.3 (68.1–92.1)\nUroradiology & genital male imaging\n100.0 (72.2–100.0)\n86.1 (71.3–93.9)\n100.0 (72.2–100.0)\n80.0 (49.0–94.3)\n90.0 (59.6–98.2)\n100.0 (72.2–100.0)\n90.0 (59.6–98.2)\n100.0 (72.2–100.0)\n90.0 (59.6–98.2)\nNeuroradiology\n85.4 (71.6–93.1)\n82.9 (68.7–91.5)\n85.4 (71.6–93.1)\n82.9 (68.7–91.5)\n82.9 (68.7–91.5)\n85.4 (71.6–93.1)\n82.9 (68.7–91.5)\n80.5 (66.0–89.8)\n85.4 (71.6–93.1)\nPaediatric radiology\n83.3 (66.4–92.7)\n76.7 (59.1–88.2)\n73.3 (55.6–85.8)\n73.3 (55.6–85.8)\n76.7 (59.1–88.2)\n70.0 (52.1–83.3)\n76.7 (59.1–88.2)\n73.3 (55.6–85.8)\n80.0 (62.7–90.5)\nHead & neck imaging\n100.0 (77.2–100.0)\n100.0 (77.2–100.0)\n84.6 (57.8–95.7)\n84.6 (57.8–95.7)\n84.6 (57.8–95.7)\n92.3 (66.7–98.6)\n100.0 (77.2–100.0)\n76.9 (49.7–91.8)\n92.3 (66.7–98.6)\nBreast imaging\n100.0 (56.6–100.0)\n80.0 (37.6–96.4)\n80.0 (37.6–96.4)\n40.0 (11.8–76.9)\n40.0 (11.8–76.9)\n40.0 (11.8–76.9)\n60.0 (23.1–88.2)\n60.0 (23.1–88.2)\n60.0 (23.1–88.2)\nChest imaging\n90.9 (62.3–98.4)\n81.8 (52.3–94.9)\n81.8 (52.3–94.9)\n72.7 (43.4–90.3)\n81.8 (52.3–94.9)\n81.8 (52.3–94.9)\n81.8 (52.3–94.9)\n81.8 (52.3–94.9)\n81.8 (52.3–94.9)\nOthers\n75.0 (40.9–92.9)\n75.0 (40.9–92.9)\n75.0 (40.9–92.9)\n62.5 (30.6–86.3)\n62.5 (30.6–86.3)\n62.5 (30.6–86.3)\n75.0 (40.9–92.9)\n75.0 (40.9–92.9)\n62.5 (30.6–86.3)\nAverage\n88.9 (83.9–92.5)\n84.1 (78.5–88.4)\n81.6 (75.8–86.3)\n74.4 (68.0–79.9)\n77.3 (71.1–82.5)\n80.7 (74.8–85.5)\n84.1 (78.5–88.4)\n80.2 (74.2–85.1)\n82.6 (76.9–87.2)\n"}, {"page": 14, "text": "5\nTABLE 2\nLLM-as-a-Specialist Task: Comparative accuracy on ophthalmology diagnosis and management tasks. Performance is evaluated across distinct clinical subspecialties and question types\n(Diagnosis vs. Management). Scores are presented as accuracy percentages (95% conﬁdence intervals). The open-source DeepSeek-R1 achieves the highest overall accuracy, while the on-device\ngpt-oss-120b (M) surpasses the proprietary baselines GPT-5 and o4-mini.\nType\nTopic\nGPT-5\no4-mini\nDeepSeek-R1\ngpt-oss-20b (L)\ngpt-oss-20b (M)\ngpt-oss-20b (H)\ngpt-oss-120b (L)\ngpt-oss-120b (M)\ngpt-oss-120b (H)\nDiagnosis\nGlaucoma\n100.0 (64.6–100.0)\n85.7 (48.7–97.4)\n100.0 (64.6–100.0)\n71.4 (35.9–91.8)\n85.7 (48.7–97.4)\n85.7 (48.7–97.4)\n85.7 (48.7–97.4)\n85.7 (48.7–97.4)\n85.7 (48.7–97.4)\nExternal Eye/Orbital Diseases\n80.0 (49.0–94.3)\n80.0 (49.0–94.3)\n90.0 (59.6–98.2)\n70.0 (39.7–89.2)\n70.0 (39.7–89.2)\n80.0 (49.0–94.3)\n70.0 (39.7–89.2)\n80.0 (49.0–94.3)\n80.0 (49.0–94.3)\nRetinal Diseases\n100.0 (43.8–100.0)\n100.0 (43.8–100.0)\n100.0 (43.8–100.0)\n66.7 (20.8–93.9)\n66.7 (20.8–93.9)\n66.7 (20.8–93.9)\n100.0 (43.8–100.0)\n66.7 (20.8–93.9)\n66.7 (20.8–93.9)\nAnterior Segment Diseases\n62.5 (30.6–86.3)\n75.0 (40.9–92.9)\n75.0 (40.9–92.9)\n62.5 (30.6–86.3)\n62.5 (30.6–86.3)\n62.5 (30.6–86.3)\n75.0 (40.9–92.9)\n75.0 (40.9–92.9)\n75.0 (40.9–92.9)\nOcular Trauma\n100.0 (61.0–100.0)\n83.3 (43.6–97.0)\n100.0 (61.0–100.0)\n83.3 (43.6–97.0)\n66.7 (30.0–90.3)\n83.3 (43.6–97.0)\n83.3 (43.6–97.0)\n100.0 (61.0–100.0)\n83.3 (43.6–97.0)\nRefractive Disorders/Strabismus\n100.0 (56.6–100.0)\n100.0 (56.6–100.0)\n100.0 (56.6–100.0)\n100.0 (56.6–100.0)\n100.0 (56.6–100.0)\n100.0 (56.6–100.0)\n100.0 (56.6–100.0)\n100.0 (56.6–100.0)\n100.0 (56.6–100.0)\nAverage\n87.2 (73.3–94.4)\n84.6 (70.3–92.8)\n92.3 (79.7–97.3)\n74.4 (58.9–85.4)\n74.4 (58.9–85.4)\n79.5 (64.5–89.2)\n82.1 (67.3–91.0)\n84.6 (70.3–92.8)\n82.1 (67.3–91.0)\nManagement\nGlaucoma\n78.6 (52.4–92.4)\n64.3 (38.8–83.7)\n71.4 (45.4–88.3)\n50.0 (26.8–73.2)\n64.3 (38.8–83.7)\n50.0 (26.8–73.2)\n71.4 (45.4–88.3)\n78.6 (52.4–92.4)\n64.3 (38.8–83.7)\nExternal Eye/Orbital Diseases\n78.6 (52.4–92.4)\n64.3 (38.8–83.7)\n78.6 (52.4–92.4)\n57.1 (32.6–78.6)\n71.4 (45.4–88.3)\n64.3 (38.8–83.7)\n71.4 (45.4–88.3)\n71.4 (45.4–88.3)\n71.4 (45.4–88.3)\nRetinal Diseases\n37.5 (13.7–69.4)\n37.5 (13.7–69.4)\n37.5 (13.7–69.4)\n12.5 (2.2–47.1)\n25.0 (7.1–59.1)\n37.5 (13.7–69.4)\n50.0 (21.5–78.5)\n37.5 (13.7–69.4)\n37.5 (13.7–69.4)\nAnterior Segment Diseases\n70.6 (46.9–86.7)\n64.7 (41.3–82.7)\n70.6 (46.9–86.7)\n76.5 (52.7–90.4)\n64.7 (41.3–82.7)\n76.5 (52.7–90.4)\n70.6 (46.9–86.7)\n76.5 (52.7–90.4)\n70.6 (46.9–86.7)\nOcular Trauma\n57.7 (38.9–74.5)\n69.2 (50.0–83.5)\n76.9 (57.9–89.0)\n42.3 (25.5–61.1)\n57.7 (38.9–74.5)\n50.0 (32.1–67.9)\n65.4 (46.2–80.6)\n76.9 (57.9–89.0)\n57.7 (38.9–74.5)\nRefractive Disorders/Strabismus\n83.3 (55.2–95.3)\n83.3 (55.2–95.3)\n91.7 (64.6–98.5)\n66.7 (39.1–86.2)\n58.3 (32.0–80.7)\n83.3 (55.2–95.3)\n83.3 (55.2–95.3)\n91.7 (64.6–98.5)\n83.3 (55.2–95.3)\nAverage\n68.1 (58.0–76.8)\n65.9 (55.7–74.8)\n73.6 (63.7–81.6)\n52.7 (42.6–62.7)\n59.3 (49.1–68.9)\n60.4 (50.2–69.9)\n69.2 (59.1–77.8)\n74.7 (64.9–82.5)\n64.8 (54.6–73.9)\nOverall\n73.8 (65.7–80.6)\n71.5 (63.3–78.6)\n79.2 (71.5–85.3)\n59.2 (50.6–67.3)\n63.8 (55.3–71.6)\n66.2 (57.7–73.7)\n73.1 (64.9–80.0)\n77.7 (69.8–84.0)\n70.0 (61.6–77.2)\n"}, {"page": 15, "text": "6\nTABLE 3\nLLM-as-a-Clinical-Judge Task: Comparative performance on treatment and diagnosis across ﬁve specialties. Scores are presented as the median error (25%-75% interquartile range, IQR)\nrelative to human expert scores. A value of 0 indicates perfect alignment, while negative values indicate the model underestimated the clinical score.\nType\nTopic\nGPT-5\no4-mini\nDeepSeek-R1\ngpt-oss-20b (L)\ngpt-oss-20b (M)\ngpt-oss-20b (H)\ngpt-oss-120b (L)\ngpt-oss-120b (M)\ngpt-oss-120b (H)\nTreatment\nGynecology\n-0.04 (-0.75–0.50)\n0.00 (0.00–0.50)\n0.38 (0.00–0.67)\n0.17 (-0.02–0.52)\n0.17 (-0.33–0.50)\n0.17 (-0.10–0.50)\n0.00 (-0.35–0.50)\n0.17 (-0.33–0.50)\n0.00 (-0.25–0.50)\nInternal Medicine\n0.00 (-0.25–0.50)\n0.08 (0.00–0.50)\n0.17 (0.00–0.50)\n0.00 (-0.33–0.50)\n0.00 (-0.08–0.50)\n0.00 (-0.17–0.50)\n0.00 (-0.08–0.50)\n0.00 (-0.08–0.50)\n0.00 (-0.17–0.50)\nNeurology\n0.50 (0.00–0.83)\n0.50 (0.00–1.00)\n0.21 (0.00–0.83)\n0.00 (-0.17–0.67)\n0.17 (-0.17–0.50)\n0.00 (-0.33–0.50)\n0.17 (-0.17–0.50)\n0.17 (-0.17–0.67)\n0.17 (0.00–0.67)\nPediatrics\n0.00 (-0.33–0.50)\n0.00 (0.00–0.50)\n0.17 (0.00–0.50)\n0.00 (-0.33–0.50)\n0.00 (-0.33–0.50)\n0.00 (-0.33–0.50)\n0.17 (-0.17–0.50)\n0.00 (-0.19–0.50)\n0.04 (-0.17–0.50)\nSurgery\n0.17 (-0.17–0.83)\n0.42 (0.00–1.00)\n0.50 (0.00–1.00)\n0.25 (-0.08–0.83)\n0.33 (0.00–1.00)\n0.50 (-0.08–1.00)\n0.33 (0.00–1.00)\n0.50 (0.00–1.00)\n0.50 (0.00–1.00)\nOverall\n0.00 (-0.33–0.50)\n0.25 (0.00–0.63)\n0.29 (0.00–0.67)\n0.08 (-0.17–0.50)\n0.17 (-0.17–0.50)\n0.17 (-0.29–0.50)\n0.17 (-0.17–0.50)\n0.17 (-0.17–0.58)\n0.17 (-0.17–0.50)\nDiagnosis\nGynecology\n-0.33 (-0.83–0.00)\n-0.50 (-1.33–0.00)\n-0.17 (-0.50–0.00)\n-0.17 (-0.50–0.00)\n-0.17 (-0.67–0.00)\n-0.33 (-0.67–0.00)\n0.00 (-0.33–0.00)\n0.00 (-0.33–0.00)\n0.00 (-0.33–0.08)\nInternal Medicine\n-0.33 (-1.00–0.00)\n-1.25 (-2.00–0.33)\n-0.33 (-0.75–0.00)\n-0.33 (-1.00–0.00)\n-0.33 (-0.69–0.00)\n-0.33 (-0.94–0.00)\n0.00 (-0.50–0.25)\n0.00 (-0.50–0.33)\n0.00 (-0.54–0.33)\nNeurology\n-0.71 (-1.50–0.17)\n-1.00 (-1.92–0.33)\n-0.33 (-0.67–0.00)\n-0.33 (-1.02–0.00)\n-0.33 (-1.00–0.00)\n-0.17 (-1.17–0.00)\n-0.17 (-1.50–0.17)\n-0.17 (-1.54–0.17)\n-0.17 (-1.67–0.17)\nPediatrics\n-0.50 (-1.08–0.00)\n-1.00 (-2.00–0.33)\n-0.33 (-0.67–0.00)\n-0.33 (-0.83–0.00)\n-0.33 (-0.79–0.00)\n-0.33 (-0.67–0.00)\n0.00 (-0.50–0.23)\n0.00 (-0.50–0.17)\n0.00 (-0.33–0.25)\nSurgery\n-0.33 (-0.83–0.00)\n-0.67 (-1.69–0.00)\n-0.17 (-0.50–0.17)\n0.00 (-0.50–0.33)\n0.00 (-0.58–0.33)\n0.00 (-0.58–0.27)\n0.00 (-0.50–0.37)\n0.00 (-0.33–0.50)\n0.00 (-0.35–0.50)\nOverall\n-0.50 (-1.00–0.00)\n-1.00 (-1.83–0.17)\n-0.33 (-0.67–0.00)\n-0.33 (-0.67–0.00)\n-0.25 (-0.67–0.00)\n-0.25 (-0.67–0.00)\n0.00 (-0.50–0.25)\n0.00 (-0.50–0.25)\n0.00 (-0.50–0.25)\n"}, {"page": 16, "text": "7\nTABLE 4\nLLM-as-a-Clinical-Judge Task: Impact of disease prevalence on clinical judgment accuracy. Model alignment with human expert scoring is stratiﬁed by disease frequency (Rare, Less Frequent,\nFrequent). Scores are presented as the median error (25%-75% interquartile range, IQR) relative to human expert scores.\nType\nFrequency\nGPT-5\no4-mini\nDeepSeek-R1\ngpt-oss-20b (L)\ngpt-oss-20b (M)\ngpt-oss-20b (H)\ngpt-oss-120b (L)\ngpt-oss-120b (M)\ngpt-oss-120b (H)\nTreatment\nRare\n0.00 (0.00–0.50)\n0.25 (0.00–0.50)\n0.25 (0.00–0.67)\n0.00 (-0.17–0.50)\n0.08 (-0.17–0.50)\n0.00 (-0.33–0.50)\n0.17 (0.00–0.50)\n0.00 (-0.17–0.50)\n0.00 (-0.17–0.50)\nLess Frequent\n0.00 (-0.33–0.50)\n0.29 (0.00–0.75)\n0.42 (0.00–0.50)\n0.17 (-0.17–0.50)\n0.17 (-0.17–0.50)\n0.17 (-0.31–0.50)\n0.17 (0.00–0.50)\n0.25 (0.00–0.63)\n0.25 (0.00–0.65)\nFrequent\n0.00 (-0.50–0.50)\n0.17 (0.00–0.50)\n0.17 (0.00–0.71)\n0.00 (-0.27–0.50)\n0.00 (-0.17–0.67)\n0.17 (-0.17–0.50)\n0.17 (-0.17–0.50)\n0.17 (-0.17–0.58)\n0.17 (-0.17–0.50)\nOverall\n0.00 (-0.33–0.50)\n0.25 (0.00–0.63)\n0.29 (0.00–0.67)\n0.08 (-0.17–0.50)\n0.17 (-0.17–0.50)\n0.17 (-0.29–0.50)\n0.17 (-0.17–0.50)\n0.17 (-0.17–0.58)\n0.17 (-0.17–0.50)\nDiagnosis\nRare\n-0.67 (-1.25–0.00)\n-1.25 (-2.00–0.50)\n-0.33 (-0.92–0.08)\n-0.33 (-1.17–0.00)\n-0.33 (-1.08–0.08)\n-0.33 (-1.17–0.13)\n0.00 (-1.29–0.42)\n0.00 (-1.38–0.33)\n0.00 (-1.25–0.33)\nLess Frequent\n-0.33 (-1.08–0.00)\n-1.00 (-1.83–0.33)\n-0.29 (-0.67–0.00)\n-0.33 (-0.83–0.00)\n-0.33 (-0.67–0.00)\n-0.33 (-0.67–0.00)\n0.00 (-0.67–0.17)\n0.00 (-0.67–0.23)\n0.00 (-0.67–0.25)\nFrequent\n-0.33 (-0.83–0.00)\n-0.63 (-1.50–0.00)\n-0.25 (-0.50–0.00)\n0.00 (-0.33–0.00)\n0.00 (-0.50–0.00)\n-0.04 (-0.50–0.00)\n0.00 (-0.33–0.17)\n0.00 (-0.23–0.25)\n0.00 (-0.33–0.25)\nOverall\n-0.50 (-1.00–0.00)\n-1.00 (-1.83–0.17)\n-0.33 (-0.67–0.00)\n-0.33 (-0.67–0.00)\n-0.25 (-0.67–0.00)\n-0.25 (-0.67–0.00)\n0.00 (-0.50–0.25)\n0.00 (-0.50–0.25)\n0.00 (-0.50–0.25)\n"}, {"page": 17, "text": "8\nTABLE 5\nLLM-as-a-Generalist Task: Comparative diagnostic accuracy of ﬁne-tuned gpt-oss-20b model. Diagnostic accuracy (95% conﬁdence interval, CI) is compared between the base gpt-oss-20b\nmodel and its ﬁne-tuned counterpart, both evaluated at medium (M) reasoning effort.\nCategory\nGPT-5\no4-mini\nDeepSeek-R1\ngpt-oss-20b (M)\nFine-tuned gpt-oss-20b (M)\nMusculoskeletal system\n87.2 (74.8–94.0)\n85.1 (72.3–92.6)\n87.2 (74.8–94.0)\n85.1 (72.3–92.6)\n91.5 (80.1–96.6)\nCardiovascular\n83.3 (43.6–97.0)\n83.3 (43.6–97.0)\n66.7 (30.0–90.3)\n50.0 (18.8–81.2)\n83.3 (43.6–97.0)\nAbdominal imaging\n94.4 (81.9–98.5)\n83.3 (43.6–97.0)\n75.0 (58.9–86.2)\n66.7 (50.3–79.8)\n86.1 (71.3–93.9)\nUroradiology & genital male\n100.0 (72.2–100.0)\n86.1 (71.3–93.9)\n100.0 (72.2–100.0)\n90.0 (59.6–98.2)\n100.0 (72.2–100.0)\nNeuroradiology\n85.4 (71.6–93.1)\n82.9 (68.7–91.5)\n85.4 (71.6–93.1)\n82.9 (68.7–91.5)\n87.8 (74.5–94.7)\nPaediatric radiology\n83.3 (66.4–92.7)\n76.7 (59.1–88.2)\n73.3 (55.6–85.8)\n76.7 (59.1–88.2)\n73.3 (55.6–85.8)\nHead & neck imaging\n100.0 (77.2–100.0)\n100.0 (77.2–100.0)\n84.6 (57.8–95.7)\n84.6 (57.8–95.7)\n92.3 (66.7–98.6)\nBreast imaging\n100.0 (56.6–100.0)\n80.0 (37.6–96.4)\n80.0 (37.6–96.4)\n40.0 (11.8–76.9)\n80.0 (37.6–96.4)\nChest imaging\n90.9 (62.3–98.4)\n81.8 (52.3–94.9)\n81.8 (52.3–94.9)\n81.8 (52.3–94.9)\n90.9 (62.3–98.4)\nOthers\n75.0 (40.9–92.9)\n75.0 (40.9–92.9)\n75.0 (40.9–92.9)\n62.5 (30.6–86.3)\n75.0 (40.9–92.9)\nAverage\n88.9 (83.9–92.5)\n84.1 (78.5–88.4)\n81.6 (75.8–86.3)\n77.3 (71.7–82.5)\n86.5 (81.1–90.5)\n"}, {"page": 18, "text": "9\nAlgorithm 1 Inference protocol for robust diagnostic reasoning. The algorithm employs diverse group beam search to\ngenerate multiple independent reasoning paths (B beams, G groups), followed by majority-vote aggregation to mitigate\nhallucination and select the most consensus-driven diagnosis.\nRequire:\n1: C: Patient Case (History and Imaging Findings)\n2: L: List of Differential Diagnoses {l1, l2, ..., ln}\n3: M: Fine-tuned On-device Model\n4: Parameters: Number of beams B, Number of groups G, Diversity penalty λ\nEnsure:\n5: ˆy: Final Selected Diagnosis\n6: R: Generated Reasoning Trace associated with ˆy\n7: Prompt Construction\n8: P ←FORMATPROMPT(C, L)\n▷Inject case and constraints into template\n9: Tin ←TOKENIZE(P)\n10: Diverse Generation (Exploration)\n11: Generate B distinct reasoning paths using Group Beam Search\n12: S ←GENERATE(Tin, M, beams = B, groups = G, penalty = λ)\n13:\n▷Return sequences {s1, s2, .., sB} sorted by beam index\n14: Answer Extraction and Normalization\n15: Draw ←∅\n▷List to store extracted answers\n16: for i ←1 to B do\n17:\nri, di ←PARSEOUTPUT(si)\n▷Split reasoning (r) and diagnosis (d) via delimiter\n18:\ndnorm\ni\n←NORMALIZETEXT(di)\n▷Lowercase, strip punctuation/whitespace\n19:\nDraw ←Draw ∪{dnorm\ni\n}\n20: end for\n21: Majority Vote Aggregation\n22: V ←COUNTFREQUENCIES(Draw)\n▷Map unique diagnoses to vote counts\n23: vmax ←max(V )\n24: T ←{d | V [d] = vmax}\n▷Set of diagnoses with the highest vote count\n25: Tie-Breaking and Final Selection\n26: if |T | = 1 then\n27:\nˆy ←element in T\n28: else\n29:\n▷Tie-breaker: Select the diagnosis appearing in the earliest beam index\n30:\nˆy ←ﬁrst dnorm\ni\n∈Draw such that dnorm\ni\n∈T\n31: end if\n32: idx ←index of ﬁrst occurrence of ˆy in Draw\n33: return ˆy, sidx\n"}, {"page": 19, "text": "10\nFig. 1. Parameter efﬁciency: Fine-tuned gpt-oss-20b model outperforms the 671B DeepSeek-R1. Comparative diagnostic accuracy of the\nﬁne-tuned gpt-oss-20b model (green) versus the open-source frontier DeepSeek-R1 (gray). Despite being signiﬁcantly smaller, the ﬁne-tuned on-\ndevice model achieves a higher overall micro-average accuracy (86.5% vs 81.6%) and demonstrates superior performance in 7 out of 10 anatomical\nsubgroups. Error bars represent 95% Wilson score intervals.\n"}, {"page": 20, "text": "11\nFig. 2. On-device versus cloud-based efﬁciency: Fine-tuned model surpasses o4-mini. Performance comparison between the ﬁne-tuned\ngpt-oss-20b (red) and OpenAI’s efﬁcient proprietary model, o4-mini (gray). The on-device model demonstrates robust generalization, exceeding\nthe cloud-based baseline in overall accuracy (86.5% vs 84.1%) and achieving competitive results across diverse radiological specialties. Vertical\ndashed lines indicate the overall micro-average accuracy for each model.\n"}, {"page": 21, "text": "12\nFig. 3. Approaching the frontier: On-device model demonstrates competitive performance with GPT-5. The forest plot illustrates the\ndiagnostic accuracy of the ﬁne-tuned gpt-oss-20b (blue) relative to the state-of-the-art GPT-5 (gray). While GPT-5 maintains a slight lead in overall\naccuracy (88.9% vs 86.5%), the conﬁdence intervals overlap signiﬁcantly across the majority of subgroups (e.g., Musculoskeletal, Abdominal,\nNeuroradiology), indicating statistical parity in these domains.\n"}, {"page": 22, "text": "13\nFig. 4. Heatmap of diagnostic accuracy across anatomical subgroups and model architectures. Color intensity represents accuracy\n(Green=High, Red=Low). Base on-device models (middle columns) exhibit signiﬁcant performance degradation in specialized domains such as\nCardiovascular and Breast imaging. Fine-tuning the 20b model (far right column) effectively mitigates these domain-speciﬁc weaknesses, restoring\nperformance levels comparable to the 120b parameter models and proprietary baselines.\nCASE STUDIES\nThe following case studies illustrate the chain-of-thought reasoning capabilities of gptoss 20b ﬁne-tuned on the Eurorad\nradiology dataset. Each case demonstrates the model’s systematic approach to differential diagnosis: (1) identifying core\nclinical-radiologic patterns, (2) mapping each differential to observed features, (3) systematically eliminating unlikely\ndiagnoses, and (4) converging on a ﬁnal answer with supporting rationale.\nCase Study\nCase Study 1: Von Hippel-Lindau Syndrome\nCase Presentation\nClinical History: A 41-year-old male presents with a history of microscopic haematuria, a palpable right ﬂank mass,\nand recurrent headaches. A CT scan was performed, followed by a brain MRI to rule out tumoural pathology. The\npatient had no signiﬁcant medical history.\nImaging Findings: In the abdominal CT scan, multiple bilateral renal cystic lesions with septum formation were\nobserved. The right kidney had multiple, irregular, heterogeneous lesions with signiﬁcant enhancement post-contrast,\nand hypodense central areas related to necrosis, the biggest located in the superior pole. Similar characteristics were\nnoted in the left kidney lesions. Also, multiple simple pancreatic cysts and splenic vein dilation were observed,\nalong with pancreaticosplenic collaterals. There is a lesion in the splenic hilum that enhances and follows contrast\nin different phases, suggestive of a splenic hamartoma. The brain MRI revealed multiple heterogeneous lesions with\na cystic and a solid component in the posterior fossa, showing avid and homogeneous enhancement of the solid\nportion, and causing vasogenic oedema and mass effect on the pons.\n"}, {"page": 23, "text": "14\nDifferential diagnoses to consider: Von Hippel-Lindau syndrome (VHL), Sporadic haemangioblastomas, Multiple\nendocrine neoplasia type 1 (MEN-1), Tuberous sclerosis complex (TSC), Polycystic kidney disease (PKD), Hereditary\nleiomyomatosis and renal cell cancer (HLRCC)\nGround Truth: Von Hippel-Lindau syndrome (VHL)\nBase Model Response (Before Fine-Tuning)\nWe need to consider the constellation: renal cystic lesions with septations, bilateral; pancreatic cysts; splenic\nhamartoma; brain hemangioblastomas. Also hematuria, ﬂank mass. [Issue: Lists features but no systematic framework]\nVHL is classic: bilateral clear cell RCC, hemangioblastomas of CNS, pancreatic cysts, renal cysts, etc. [Issue: Brief\nstatement, no detailed reasoning]\nMEN1: pituitary, parathyroid, pancreatic endocrine tumors, but not renal cysts or hemangioblastomas. [Issue:\nSuperﬁcial elimination - just ”but not X”]\nTSC: cortical tubers, subependymal nodules, renal angiomyolipomas, but not hemangioblastomas. [Issue: Lists features\nwithout explaining why they exclude diagnosis]\nPKD: bilateral cystic kidneys, but not brain lesions. [Issue: One-sentence dismissal]\nHLRCC: renal cell carcinoma (papillary type 2), cutaneous leiomyomas, uterine leiomyomas, but not brain heman-\ngioblastomas. [Issue: No detailed analysis]\nThus VHL ﬁts best. The presence of multiple bilateral renal cystic lesions with septations and enhancing lesions\nsuggests clear cell RCC. The brain lesions are hemangioblastomas. Splenic hamartoma is also associated with VHL?\n[Issue: Shows uncertainty - questioning itself] Splenic hamartoma is not classic but can be seen. Pancreatic cysts also\nseen. So VHL is the best. [Issue: Lacks conﬁdence and systematic synthesis]\nBase Model Final Answer: Von Hippel-Lindau syndrome (VHL) – CORRECT but with uncertain, superﬁcial\nreasoning\nFine-Tuned Model Response\nStep 1 – Relate the clinical picture to the imaging\nThe patient is a 41-year-old man with microscopic haematuria, a palpable right-ﬂank mass and recurrent headaches.\nThe ﬂank mass corresponds to the enlarged, heterogeneous right kidney seen on CT . The headaches are explained\nby the posterior-fossa lesions on brain MRI, which are cystic-solid, avidly enhancing and causing mass effect on the\npons. The combination of renal cystic-solid lesions, pancreatic cysts, splenic vascular anomalies (splenic-vein dilation,\npancreatic-splenic collaterals) and multiple posterior-fossa haemangioblastomas is highly suggestive of a hereditary\ntumour-spectrum disorder.\nStep 2 – Map ﬁndings to each differential\n"}, {"page": 24, "text": "15\nDifferential\nTypical imaging/clinical\nhallmarks\nConcordance with this\ncase\nReason for\nexclusion\nVon\nHippel-Lindau\n(VHL)\nBilateral renal cystic-solid\nlesions (cystic-solid RCC),\npancreatic cysts, splenic\nvascular malformations,\nmultiple cerebellar/brain-stem\nhaemangioblastomas,\nhaematuria, ﬂank mass,\nheadaches.\nAll key features\npresent; age ﬁts (40-50\nyr).\n–\nSporadic haeman-\ngioblastomas\nUsually solitary, no systemic\nrenal or pancreatic lesions.\nMultiple\nhaemangioblastomas\npresent; renal and\npancreatic ﬁndings are\nabsent in sporadic\ndisease.\nMultiple haeman-\ngioblastomas with\nsystemic ﬁndings\nexclude sporadic\nform.\nMEN-1\nPancreatic neuroendocrine\ntumours, parathyroid\nhyperplasia, pituitary\nadenomas; renal cysts are\nuncommon,\nhaemangioblastomas are not\ntypical.\nPancreatic cysts are\nsimple, not\nneuroendocrine; no\nendocrine lesions\nreported; renal lesions\nare cystic-solid RCC,\nnot typical MEN-1.\nAbsence of\nneuroendocrine\ntumours and\nendocrine\ndysfunction.\nTSC\nMultiple cortical tubers,\nsubependymal nodules, renal\nangiomyolipomas (fat-dense),\ncardiac rhabdomyomas, retinal\nhamartomas.\nNo cortical tubers, no\nfat-dense renal lesions,\nno cardiac or retinal\nﬁndings.\nPathognomonic\nTSC features\n(cortical tubers,\nangiomyolipomas)\nare absent.\nPKD\nBilateral, large, non-enhancing\ncysts; no solid enhancing\ncomponents; no pancreatic or\nsplenic vascular lesions; no\nbrain haemangioblastomas.\nRenal lesions are\ncystic-solid with\nnecrotic centres, not\nsimple cysts;\nextra-renal lesions\npresent.\nPresence of solid\nenhancing\ncomponents and\nextra-renal\nmanifestations.\nHLRCC\nRenal cell carcinoma (often\npapillary type II) with\naggressive behaviour; no\npancreatic cysts or splenic\nvascular anomalies; no\nhaemangioblastomas.\nRenal lesions are\ncystic-solid but lack the\nclassic papillary-type II\nmorphology;\nextra-renal ﬁndings are\nabsent in HLRCC.\nMissing\ncharacteristic\nHLRCC features\n(aggressive\npapillary RCC,\ncutaneous\nleiomyomas).\nStep 3 – Systematic elimination\n• Sporadic haemangioblastomas are excluded because the patient has multiple CNS lesions accompanied by systemic\nrenal and pancreatic manifestations, which are not seen in sporadic cases.\n• MEN-1 is ruled out by the absence of neuroendocrine pancreatic tumours, parathyroid disease, or pituitary\nadenomas.\n• TSC is eliminated by the absence of cortical tubers, subependymal nodules, and fat-dense angiomyolipomas.\n• PKD is excluded because the renal lesions are cystic-solid with enhancing components and necrotic centres, not\nsimple non-enhancing cysts.\n• HLRCC is ruled out by the lack of aggressive papillary-type II RCC morphology and the presence of CNS\nhaemangioblastomas, which are not part of HLRCC.\n•\nOnly VHL accounts for the complete constellation of ﬁndings : bilateral cystic-solid renal lesions, pancreatic\ncysts, splenic vascular anomalies, and multiple CNS haemangioblastomas.\nStep 4 – Converge on the ﬁnal diagnosis\nThe presence of bilateral cystic-solid renal lesions with necrotic centres, multiple simple pancreatic cysts, splenic\nvascular dilatation, and multiple posterior-fossa haemangioblastomas together with haematuria and a ﬂank mass\nﬁts the classic VHL phenotype.\nNone of the other entities account for the full spectrum of ﬁndings . The logical\nsynthesis of clinical presentation and imaging demonstrates that the patient’s ﬁndings are most consistent with\nVon Hippel-Lindau syndrome. Genetic conﬁrmation (VHL gene mutation) would be the next step, followed by\nmultidisciplinary surveillance for renal, pancreatic, splenic, and CNS manifestations.\nFine-Tuned Model Final Answer: Von Hippel-Lindau syndrome (VHL) – CORRECT with systematic, conﬁdent\n"}, {"page": 25, "text": "16\nreasoning\nKey Improvements from Fine-Tuning:\n• Structured framework: Transformed unstructured list into systematic 4-step analysis with explicit clinical-imaging\ncorrelation\n• Comprehensive comparison: Added detailed 6-syndrome table with concordance/discordance columns vs. brief\ntext dismissals\n• Multi-system integration: Step 1 explicitly synthesizes renal, pancreatic, splenic, and CNS ﬁndings into hereditary\ntumor syndrome pattern\n• Explicit elimination logic: ”Reason for exclusion” column with pathophysiologic justiﬁcation vs. superﬁcial ”but\nnot X” statements\n• Eliminated uncertainty: Deﬁnitive statements about splenic ﬁndings vs. questioning (”Splenic hamartoma is also\nassociated with VHL?”)\n• Clinical conﬁdence: Comprehensive synthesis with next-step recommendations vs. tentative conclusion\nCase Study\nCase Study 2: Primary Cardiac Lymphoma\nCase Presentation\nClinical History: An 81-year-old male with a history of paroxysmal atrial ﬁbrillation presents to the emergency\ndepartment with a one-week onset of orthopnoea and exertional dyspnoea. Laboratory tests demonstrate elevated\ntroponins, the electrocardiogram reveals a third-degree atrioventricular block, and the echocardiogram shows right\nventricular thickening and pericardial effusion.\nImaging Findings: A PET-CT scan was carried out revealing a large hypermetabolic myocardial tissue. It extended\nthrough the right chambers and around the large vessels, sparing the left ventricle. Additionally, a hypermetabolic\nnodular image was identiﬁed at the left lung base. Therefore, a contrast-enhanced chest CT scan was performed.\nIt conﬁrmed the presence of an inﬁltrative hypodense mass in the myocardium. It involved the right atrium, right\nventricle, interatrial septum, and atrioventricular groove. The mass caused mural thickening of the right cavities. A\ncuff encircled the right coronary artery along its course. A moderate pericardial effusion was also observed. Finally,\nthe CT scan conﬁrmed the solid nodule in the posterior segment of the left lower lobe. A CT-guided percutaneous\nbiopsy of the lung nodule was made for histological examination.\nDifferential diagnoses to consider: Angiosarcoma, IgG4-related disease, Primary cardiac lymphoma, Erdheim-\nChester disease, Undifferentiated pleomorphic sarcoma\nGround Truth: Primary cardiac lymphoma\nBase Model Response (Before Fine-Tuning)\nWe need to reason step by step using only provided info. The case: 81-year-old male, paroxysmal atrial ﬁbrillation,\northopnea, exertional dyspnea, elevated troponins, ECG shows third-degree AV block, echo shows right ventricular\nthickening and pericardial effusion. PET-CT shows large hypermetabolic myocardial tissue extending through right\nchambers and around large vessels, sparing left ventricle. Hypermetabolic nodular image at left lung base. Contrast\nCT conﬁrms inﬁltrative hypodense mass in myocardium involving right atrium, right ventricle, interatrial septum,\nAV groove. Mural thickening of right cavities. Cuff around right coronary artery. Moderate pericardial effusion. Solid\nnodule in posterior segment left lower lobe. [Issue: Just lists ﬁndings without synthesis]\nLet’s analyze each:\nAngiosarcoma: Primary cardiac sarcoma, most common primary malignant tumor of heart. Usually involves\nright atrium, can extend to right ventricle, pericardial effusion, can cause arrhythmias, conduction block. PET\nhypermetabolic. Age: typically younger (median 40-50). But can occur in older. [Issue: Makes excuses instead of using\nage as exclusion criterion] Lung metastasis possible. So ﬁts. [Issue: Prematurely accepts this diagnosis]\n"}, {"page": 26, "text": "17\nIgG4-related disease: Inﬁltrative disease, can involve heart (IgG4-related pericarditis, myocarditis). Usually involves\npericardium, can cause thickening, but not typically hypermetabolic mass on PET? Could show uptake. But IgG4\ndisease often involves multiple organs, but lung involvement can be nodules. However, IgG4 disease rarely presents\nas a large inﬁltrative mass in myocardium. Also would have elevated IgG4 levels, histology with IgG4 plasma cells.\nNot typical. [Issue: Rambling, uncertain reasoning]\nPrimary cardiac lymphoma: Rare, often involves right atrium, can cause conduction block, pericardial effusion,\ninﬁltration. PET hypermetabolic. Age: can be older. Lung involvement? Usually no lung nodules. But could\nmetastasize to lung. But primary cardiac lymphoma often involves both atria and ventricles, but can be localized.\nCould present with AV block. So possible. [Issue: Doesn’t recognize AV block as KEY feature for lymphoma]\nErdheim-Chester disease: Non-Langerhans cell histiocytosis, systemic inﬁltration, can involve heart (pericardial\nthickening, inﬁltration of myocardium). Usually involves both ventricles, but can involve right side. Also can involve\nlungs (nodules). Age: older adults. PET hypermetabolic. But typical features: symmetric osteosclerosis of long bones,\nretroperitoneal ﬁbrosis, etc. Not mentioned. Also inﬁltration often involves both ventricles, not sparing left ventricle.\nSo less likely. [Issue: Superﬁcial elimination]\nUndifferentiated pleomorphic sarcoma: A type of sarcoma, but not typical for heart. Could be primary cardiac\nsarcoma but usually angiosarcoma is most common. Undifferentiated pleomorphic sarcoma can be in heart but less\ncommon. Could present similarly. But the mass is hypermetabolic, inﬁltration of right chambers. Could be. [Issue: No\nreal elimination logic]\nWe need to choose the most likely. The key features: right atrium/ventricle involvement, pericardial effusion,\nconduction block, hypermetabolic mass, lung nodule. Angiosarcoma is classic for right atrium, pericardial effusion,\nconduction block, metastasis to lung. [Issue: Misses that AV block is MORE classic for lymphoma] Primary cardiac\nlymphoma also can present similarly. But which is more likely? Let’s consider age: 81. Angiosarcoma usually younger.\nPrimary cardiac lymphoma often in immunocompromised or older? Primary cardiac lymphoma can occur in older\nadults. But the presence of a lung nodule that was biopsied: likely metastasis. In angiosarcoma, lung metastases\ncommon. In primary cardiac lymphoma, metastasis to lung less common? Actually lymphoma can metastasize to\nlung. But the lung nodule is hypermetabolic. The biopsy would reveal histology. But we don’t have histology results.\n[Issue: Circular reasoning without clear conclusion]\nAngiosarcoma often shows inﬁltrative mass in right atrium, can extend to right ventricle, pericardial effusion,\ncan encircle coronary arteries. The mass can be hypermetabolic. The left ventricle is spared. That ﬁts. Primary\ncardiac lymphoma often involves both atria and ventricles, but can be localized. But the mass in this case is large,\ninﬁltrative, hypermetabolic, involving right atrium, right ventricle, interatrial septum, AV groove. That is typical\nfor angiosarcoma. Also the presence of a lung nodule suggests metastasis. So angiosarcoma is likely. [Issue: Wrong\nconclusion - prioritized lung nodule over AV block]\nBase Model Final Answer: Angiosarcoma – INCORRECT\nFine-Tuned Model Response\nStep 1 – Relate the clinical picture to the imaging\nThe\npatient\nis\nan\nelderly\nman\nwith\nnew-onset\northopnoea,\nexertional\ndyspnoea,\na\nthird-\ndegree\nAV\nblock,\nelevated\ntroponin\nand\na\nright-ventricular-predominant\nthickening\non\necho.\nThese ﬁndings point to a process that involves the right-sided myocardium, the conduction system (AV node)\nand produces a pericardial effusion. The PET-CT shows a large, intensely FDG-avid mass that surrounds the right\natrium, right ventricle, inter-atrial septum and the atrioventricular groove, sparing the left ventricle. A second\nhypermetabolic nodule is seen in the left lower-lobe lung, later conﬁrmed as a metastatic lesion. Thus we have a\nright-sided cardiac inﬁltrative tumour with extra-cardiac metastasis.\nStep 2 – Map ﬁndings to each differential\n"}, {"page": 27, "text": "18\nDifferential\nTypical imaging & clinical\nfeatures\nConcordance with this\ncase\nDiscordance\nPrimary cardiac\nlymphoma (PCL)\nUsually involves right-sided\nchambers, encases AV node\n→AV block; often presents\nwith pericardial effusion;\nmay be FDG-avid; can\nmetastasize to lung; left\nventricle usually spared.\nAll three hallmarks\npresent (right-sided\ninﬁltrate, AV block,\npericardial effusion,\nlung metastasis, FDG\navidity).\nNone.\nAngiosarcoma\nMost common primary\ncardiac sarcoma; tends to\ninvolve right atrium, may\ncause hemopericardium,\npericardial effusion; often\nshows heterogeneous\nenhancement, necrosis, and\nmay invade coronary arteries.\nRight-sided location and\npericardial effusion ﬁt.\nAngiosarcoma\nusually shows a\nmore heterogeneous,\nnecrotic mass with\nprominent vascular\nchannels; the PET-CT\nshows a uniformly\nhypermetabolic,\nwell-circumscribed\ninﬁltrate, and the\nmass encircles the\nright coronary artery\nwithout obvious\nnecrosis. Metastasis\nto lung is possible\nbut less common\nthan lymphoma.\nIgG4-related disease\nCan cause ”tumefactive”\ncardiac lesions, but typically\npresents with systemic\nfeatures (pancreatitis,\nretro-orbital disease), and\nimaging shows diffuse\nmyocardial thickening rather\nthan a discrete mass; PET\nmay be mildly FDG-avid.\nNone of the systemic\nIgG4 manifestations are\npresent.\nLack of extra-cardiac\nIgG4-related lesions\nand the aggressive,\nFDG-avid nature\nargue against it.\nErdheim-Chester\ndisease\nBilateral symmetric\nosteosclerosis of long bones,\npericardial thickening,\nretro-orbital pseudotumor;\ncardiac involvement is\nusually inﬁltrative but not as\nfocal; PET may be positive.\nNo bone lesions or other\nclassic sites.\nUnlikely.\nUndifferentiated\npleomorphic sarcoma\nRare, often involves left\nventricle, may be large,\nheterogeneous, necrotic, and\ncan cause hemopericardium.\nRight-sided location is\natypical; mass is\nuniformly\nhypermetabolic without\nnecrosis.\nUnlikely.\nStep 3 – Systematic elimination\n• Angiosarcoma is less likely because the lesion is uniformly FDG-avid, lacks necrotic areas, and the patient’s age\n(81) is above the typical 30-50 yr range.\n• IgG4-related disease and Erdheim-Chester disease are excluded by the absence of systemic manifestations and\ncharacteristic extra-cardiac ﬁndings.\n• Undifferentiated pleomorphic sarcoma is improbable given the right-sided predominance and lack of necrosis.\nStep 4 – Converge on the most plausible diagnosis\nThe combination of (1) right-sided cardiac inﬁltration with AV node encasement causing complete heart block , (2)\nmoderate pericardial effusion, (3) intense FDG uptake, (4) a solitary lung metastasis, and (5) the patient’s age and\nclinical course, aligns perfectly with primary cardiac lymphoma. The subsequent CT-guided lung biopsy conﬁrming\nmetastatic lymphoma clinches the diagnosis.\nFine-Tuned Model Final Answer: Primary cardiac lymphoma – CORRECT\nKey Improvements from Fine-Tuning:\n• Recognized key clinical feature: Immediately identiﬁed AV node encasement causing third-degree AV block as\n"}, {"page": 28, "text": "19\npathognomonic for primary cardiac lymphoma\n• Systematic framework: Transformed rambling analysis into structured 4-step approach with clear clinical synthesis\n• Detailed differential table: Added comprehensive 5-entity comparison with concordance/discordance analysis\nvs. unstructured text\n• Proper age weighting: Used patient age (81 years) to deﬁnitively exclude angiosarcoma (typical 30-50 yr) rather\nthan making excuses\n• Imaging pattern recognition: Identiﬁed ”uniformly hypermetabolic, well-circumscribed” pattern as distinguishing\nfrom heterogeneous, necrotic angiosarcoma\n• Clinical correlation: Step 4 synthesizes all ﬁndings (AV block + location + effusion + lung met + age) to converge\non correct diagnosis\nCase Study\nCase Study 3: Non-Ischemic Cerebral Enhancing (NICE) Lesions\nCase Presentation\nClinical History: Five days after an uncomplicated embolisation of an asymptomatic, growing right middle cerebral\nartery (MCA) aneurysm, a 56-year-old woman comes to the ER due to headache, a seizure and ﬂuctuant left upper\nextremity weakness and left campimetric defects. A Todd’s paralysis is suspected, and the patient is admitted for a\nworkup.\nImaging Findings: Initially, a brain CT is performed, which shows no signiﬁcant abnormalities apart from the\nembolisation material. Then a brain MRI is performed, including the following sequences: DWI, SWI, T2, FLAIR,\n3D TOF and pre and post-gadolinium T1. The MRI discloses multiple round lesions varying in size in the right\nhemisphere. The lesions are spontaneously T1 hypointense and T2 hyperintense, the largest ones showing a small,\ncentral T2 hypointensity. Post-gadolinium enhancement is demonstrated in all the lesions. No restricted diffusion or\nsusceptibility artefacts are present. Follow-up imaging after 6 months shows overall improvement with resolution\nof the enhancing foci. However, T2 hyperintensity is still present, and central punctate susceptibility artefacts have\nappeared, interpreted as microhaemorrhagic foci.\nDifferential diagnoses to consider: Non-ischemic cerebral enhancing (NICE) lesions, Subacute cerebral infarcts,\nCerebral microabscesses, Brain metastases\nGround Truth: Non-ischemic cerebral enhancing (NICE) lesions\nBase Model Response (Before Fine-Tuning)\nWe need to consider the imaging features: multiple round lesions, T1 hypointense, T2 hyperintense, central T2 hy-\npointensity, post-gadolinium enhancement, no diffusion restriction, no susceptibility artifacts initially. After 6 months,\nresolution of enhancement, persistent T2 hyperintensity, central punctate susceptibility artifacts (microhemorrhages).\n[Issue: Lists features without prioritizing temporal context]\nThis suggests that the lesions were likely embolization-related changes: e.g., embolic microinfarcts? But no diffusion\nrestriction. [Issue: Immediately questions own reasoning] Subacute infarcts would show diffusion restriction early, then\nT2 hyperintensity, then resolution of enhancement? But subacute infarcts usually show restricted diffusion early, then\nT2 hyperintensity, then no enhancement. But here no diffusion restriction. [Issue: Circular reasoning] Also multiple\nlesions after embolization could be due to embolic microinfarcts from procedure. But the lesions are enhancing,\nwhich is not typical for infarcts. However, early subacute infarcts can show enhancement due to breakdown of BBB.\nBut they usually show diffusion restriction early. But maybe small lesions might not show diffusion restriction?\n[Issue: Making excuses for contradictory evidence instead of using it to exclude] But central T2 hypointensity suggests\nhemorrhagic component? But no susceptibility artifacts initially. After 6 months, microhemorrhages appear. Could\nbe due to microinfarcts with hemorrhagic transformation? But microhemorrhages appear later. [Issue: Continued\ncircular reasoning]\nAlternatively, could be cerebral microabscesses? They would show diffusion restriction, rim enhancement, central T2\nhyperintensity. But no diffusion restriction. So unlikely.\n"}, {"page": 29, "text": "20\nBrain metastases: multiple enhancing lesions, T1 hypointense, T2 hyperintense, central T2 hypointensity? Usually\nmetastases have ring enhancement, central necrosis, sometimes hemorrhage. But metastases would show diffusion\nrestriction? Not necessarily. But metastases would not spontaneously resolve after 6 months unless treated. Also\npatient had no known primary cancer. So less likely.\nNon-ischemic cerebral enhancing (NICE) lesions: This is a broad category. [Issue: Dismisses correct answer as ”too\nbroad”] Could include inﬂammatory lesions, demyelinating lesions, etc. But the context of embolization suggests\nprocedure-related changes. Could be ”post-embolization changes” like ”cerebral embolic infarcts” or ”cerebral\nmicroinfarcts” that enhance due to BBB breakdown. The resolution of enhancement over 6 months ﬁts with healing\nof infarcts. The persistent T2 hyperintensity indicates gliosis or chronic changes. The microhemorrhages appear later\ndue to microinfarcts with hemorrhagic transformation. [Issue: Talks around NICE but doesn’t recognize it as speciﬁc\ndiagnosis]\nThus the most likely is Subacute cerebral infarcts. [Issue: Wrong conclusion - failed to use absence of DWI restriction as\ndeﬁnitive exclusion criterion]\nBase Model Final Answer: Subacute cerebral infarcts – INCORRECT\nFine-Tuned Model Response\nStep 1 – Relate the clinical picture to the imaging problem\nThe patient had a recent endovascular embolisation of a right MCA aneurysm. Five days later she develops a new\nheadache, a focal seizure and transient left-sided weakness with visual ﬁeld loss. This post-intervention presentation\nwith focal neurological deﬁcits immediately suggests a complication rather than a primary neoplasm or infection.\nThe transient weakness is classic for Todd’s paralysis, which is a post-ictal phenomenon that can mimic a new infarct.\nStep 2 – Describe the MRI pattern and map to differentials\nMRI Pattern:\n• Multiple round foci conﬁned to the right cerebral hemisphere (the side of the intervention).\n• Spontaneous T1-hypointensity, T2-hyperintensity, with a small central T2-hypointense core.\n• Uniform, homogeneous post-gadolinium enhancement in all lesions.\n• No diffusion restriction on DWI, no blooming on SWI at the initial scan.\n• Follow-up (6 mo) shows resolution of enhancement, persistent T2 hyperintensity, and new tiny susceptibility foci\n(micro-haemorrhages).\nThis pattern is characteristic of NICE lesions: they are multiple, well-circumscribed, enhance avidly, lack diffusion\nrestriction, and evolve to a non-enhancing T2-hyperintense state with occasional micro-bleeds.\n"}, {"page": 30, "text": "21\nDifferential\nExpected imaging features\nConcordance/discordance with case\nNICE\n(post-embolisation)\nMultiple, round, enhancing\nlesions conﬁned to the\nipsilateral hemisphere; no\ndiffusion restriction; may show\ncentral T2 hypointensity;\nevolution to non-enhancing T2\nhyperintensity with\nmicro-bleeds.\nAll criteria met: multiple\nright-hemisphere lesions,\nhomogeneous enhancement, no DWI\nrestriction, central T2 core, evolution\npattern.\nSubacute cerebral\ninfarcts\nUsually wedge-shaped, follow\nvascular territories; diffusion\nrestriction present in the\nacute/subacute phase; may show\nblooming on SWI due to\nhaemorrhagic conversion;\nenhancement is often peripheral\nand delayed.\nLesions are round, not territorial; no\ndiffusion restriction; no SWI blooming\ninitially; enhancement is uniform, not\nperipheral.\nCerebral\nmicroabscesses\nOften solitary or few,\nring-enhancing with central\ndiffusion restriction; may have\nsurrounding oedema; SWI may\nshow central susceptibility.\nMultiple lesions, no diffusion\nrestriction, no oedema, uniform rather\nthan ring enhancement.\nBrain metastases\nUsually multiple, often\nperipherally located, may have\nsurrounding oedema, variable\nenhancement (often\nheterogeneous), and frequent\nsusceptibility artefacts from\nhaemorrhage.\nLesions are conﬁned to one\nhemisphere, lack surrounding oedema,\nenhancement is homogeneous, and\ninitial SWI is negative.\nStep 3 – Systematic elimination\n• Subacute infarcts are excluded by the absence of diffusion restriction and the non-territorial, round morphology.\nAcute ischemic lesions would show restricted diffusion , which is not present here.\n• Microabscesses are unlikely because there is no diffusion restriction, no oedema, and the enhancement is not\nring-like. Abscesses typically show a hyperintense DWI core.\n• Metastases are improbable given the unilateral distribution, lack of perilesional oedema, and the temporal\nrelationship to a recent endovascular procedure. The patient also has no known primary malignancy.\n• The combination of a recent aneurysm embolisation, unilateral multiple enhancing lesions without diffusion\nrestriction, central T2 hypointensity, and the characteristic evolution (resolution of enhancement, persistence of\nT2 hyperintensity, and later micro-bleeds) ﬁts the classic proﬁle of NICE lesions.\nStep 4 – Converge on the ﬁnal diagnosis\nThe temporal relationship to endovascular intervention is the key clinical clue . The combination of multiple ipsi-\nlateral round enhancing lesions without diffusion restriction, the presence of central T2 hypointensity, and the\ncharacteristic 6-month evolution (resolution of enhancement with persistent T2 signal and development of micro-\nhaemorrhages) is pathognomonic for non-ischemic cerebral enhancing (NICE) lesions. Recognizing this pattern\nprevents unnecessary invasive workup and guides appropriate follow-up imaging.\nFine-Tuned Model Final Answer: Non-ischemic cerebral enhancing (NICE) lesions – CORRECT\nKey Improvements from Fine-Tuning:\n• Recognized temporal context: Immediately identiﬁed ”5 days post-embolization” as key diagnostic clue for\nprocedural complication (NICE lesions)\n• Used absence of DWI restriction deﬁnitively: Applied lack of diffusion restriction to systematically exclude\ninfarcts rather than making excuses\n• Eliminated circular reasoning: Replaced ”But maybe...” contradictions with clear exclusion logic based on imaging\nfeatures\n• Recognized NICE as speciﬁc diagnosis: Understood NICE lesions as established post-procedural entity, not just\n”broad category”\n• Applied evolution pattern: Used 6-month follow-up (resolution of enhancement →persistent T2 →microbleeds)\nas pathognomonic feature\n• Systematic framework: Transformed rambling analysis into structured 4-step approach with clear MRI pattern\ndescription and differential table\n"}]}