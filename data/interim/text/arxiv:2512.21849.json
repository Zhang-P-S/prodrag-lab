{"doc_id": "arxiv:2512.21849", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.21849.pdf", "meta": {"doc_id": "arxiv:2512.21849", "source": "arxiv", "arxiv_id": "2512.21849", "title": "HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs", "authors": ["Jiaxin Liu", "Peiyi Tu", "Wenyu Chen", "Yihong Zhuang", "Xinxia Ling", "Anji Zhou", "Chenxi Wang", "Zhuo Han", "Zhengkai Yang", "Junbo Zhao", "Zenan Huang", "Yuanyuan Wang"], "published": "2025-12-26T03:54:56Z", "updated": "2025-12-26T03:54:56Z", "summary": "While Large Language Models (LLMs) have achieved remarkable success in cognitive and reasoning benchmarks, they exhibit a persistent deficit in anthropomorphic intelligence-the capacity to navigate complex social, emotional, and ethical nuances. This gap is particularly acute in the Chinese linguistic and cultural context, where a lack of specialized evaluation frameworks and high-quality socio-emotional data impedes progress. To address these limitations, we present HeartBench, a framework designed to evaluate the integrated emotional, cultural, and ethical dimensions of Chinese LLMs. Grounded in authentic psychological counseling scenarios and developed in collaboration with clinical experts, the benchmark is structured around a theory-driven taxonomy comprising five primary dimensions and 15 secondary capabilities. We implement a case-specific, rubric-based methodology that translates abstract human-like traits into granular, measurable criteria through a ``reasoning-before-scoring'' evaluation protocol. Our assessment of 13 state-of-the-art LLMs indicates a substantial performance ceiling: even leading models achieve only 60% of the expert-defined ideal score. Furthermore, analysis using a difficulty-stratified ``Hard Set'' reveals a significant performance decay in scenarios involving subtle emotional subtexts and complex ethical trade-offs. HeartBench establishes a standardized metric for anthropomorphic AI evaluation and provides a methodological blueprint for constructing high-quality, human-aligned training data.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.21849v1", "url_pdf": "https://arxiv.org/pdf/2512.21849.pdf", "meta_path": "data/raw/arxiv/meta/2512.21849.json", "sha256": "2fd4bfb9a70b24d578359bed9ec89d371deb3d7792d97eea6cf8c9da0648e0e4", "status": "ok", "fetched_at": "2026-02-18T02:23:47.740533+00:00"}, "pages": [{"page": 1, "text": "HeartBench: Probing Core Dimensions of Anthropomorphic\nIntelligence in LLMs\nJiaxin Liu1,♡, Peiyi Tu1,♡, Wenyu Chen1,♡, Yihong Zhuang1,♡, Xinxia Ling1,2,\nAnji Zhou3, Chenxi Wang3, Zhuo Han3, Zhengkai Yang1, Junbo Zhao1,4,\nZenan Huang1,†, Yuanyuan Wang1,†\n1Ant Group, 2Xiamen University, 3Beijing Normal University, 4Zhejiang University\nAbstract\nWhile Large Language Models (LLMs) have achieved remarkable success in cognitive and\nreasoning benchmarks, they exhibit a persistent deficit in anthropomorphic intelligence—the\ncapacity to navigate complex social, emotional, and ethical nuances. This gap is particularly\nacute in the Chinese linguistic and cultural context, where a lack of specialized evaluation\nframeworks and high-quality socio-emotional data impedes progress. To address these\nlimitations, we present HeartBench, a framework designed to evaluate the integrated\nemotional, cultural, and ethical dimensions of Chinese LLMs. Grounded in authentic\npsychological counseling scenarios and developed in collaboration with clinical experts,\nthe benchmark is structured around a theory-driven taxonomy comprising five primary\ndimensions and 15 secondary capabilities. We implement a case-specific, rubric-based\nmethodology that translates abstract human-like traits into granular, measurable criteria\nthrough a “reasoning-before-scoring” evaluation protocol. Our assessment of 13 state-of-\nthe-art LLMs indicates a substantial performance ceiling: even leading models achieve only\n60% of the expert-defined ideal score. Furthermore, analysis using a difficulty-stratified\n“Hard Set” reveals a significant performance decay in scenarios involving subtle emotional\nsubtexts and complex ethical trade-offs. HeartBench establishes a standardized metric for\nanthropomorphic AI evaluation and provides a methodological blueprint for constructing\nhigh-quality, human-aligned training data.\nGithub: https://github.com/inclusionAI/HeartBench\n1\nIntroduction\nRecent advances have enabled Large Language Models (LLMs) to achieve remarkable performance on tasks\nrequiring cognitive intelligence, evidenced by their success on benchmarks such as MMLU (Hendrycks\net al., 2021) and AIME (Math-AI, 2025). However, this focus on cognitive abilities has created a disparity:\nmodels’ social and emotional intelligence—encompassing nuanced understanding of emotions, ethics, and\nculture—remains underdeveloped. This deficiency is especially acute for non-English languages, including\nChinese, limiting the models’ utility in culturally and emotionally rich contexts.\nThe significance of this gap is amplified by the evolving role of AI, which is transitioning from a functional\ntool to a relational partner in applications such as AI companionship (Riley et al., 2025), digital mental\nhealth (Park et al., 2025), and adaptive education (Chatterjee & Kundu, 2025). This transition reflects\nthe social phenomenon of anthropomorphism—people’s tendency to attribute lifelike qualities to non-\nhuman entities (Fink, 2012; K¨uhne & Peter, 2023). In these domains, the primary user needs are not\njust informational accuracy but also emotional resonance and cultural congruity (Plum et al., 2025; Paech,\n2024). Two fundamental obstacles impede progress: (1) a lack of benchmarks to systematically evaluate\nthe social and emotional capacities of LLMs, and (2) the absence of clear criteria defining high-quality\nsocio-emotional training data. Without these, efforts to enhance such capabilities lack clear direction and\nmeasurable outcomes.\n♡Equal contribution. †Corresponding Authors.\n1\narXiv:2512.21849v1  [cs.CL]  26 Dec 2025\n"}, {"page": 2, "text": "To address these challenges, we introduce HeartBench, the first comprehensive benchmark, to our knowledge,\nfor evaluating the integrated emotional, cultural, and ethical intelligence of Chinese LLMs. It makes two\nprimary contributions. First, it establishes a standardized evaluation methodology grounded in authentic\nChinese counseling scenarios. These scenarios provide ecologically valid contexts that naturally embody key\nanthropomorphic interaction patterns like empathic attunement and relational engagement (Damiano &\nDumouchel, 2018). Second, it provides a data construction blueprint that uses these evaluation dimensions to\ndefine high-quality, human-aligned corpora. Through this work, we aim to shift LLM development beyond\ncognitive metrics and cultivate models with a deeper, humanistic intelligence grounded in anthropomorphic\ndesign principles.\n2\nRelated Work\nThe evaluation of Large Language Models (LLMs) has transitioned from assessing atomized skills to measur-\ning integrated social and professional intelligence. Early benchmarks like EQ-Bench (Paech, 2024) established\na link between emotional understanding and general cognition, while ToMBench (Chen et al., 2024) revealed\npersistent gaps in human-level Theory of Mind. As the field moves toward interactive scenarios, Multi-Bench\n(Deng et al., 2025) and Kardia-R1 (Yuan et al., 2025) have emphasized the necessity of multi-turn consistency\nand explicit reasoning chains for providing sustained emotional support.\nA significant advancement in professional domain evaluation is represented by HealthBench (Arora et al.,\n2025), which utilizes extensive, expert-authored rubrics to translate complex clinical dialogues into measur-\nable criteria. By shifting the focus to domain-specific scoring, it provides a robust framework for assessing\ncommunication quality in high-stakes interactions. This move toward fine-grained precision is also reflected\nin WritingBench (Wu et al., 2025) and EssayBench (Gao et al., 2025), which introduce genre-specific criteria to\ncapture the structural and rhetorical complexities of professional writing and Chinese-specific prose.\nHowever, the methodology of automated evaluation remains a subject of critical inquiry. While WildBench\n(Lin et al., 2024) employs task-specific checklists to align model judges with human preferences, JudgeBench\n(Tan et al., 2025) exposes the inherent biases and logical limitations of LLM evaluators in complex reasoning\ntasks. Furthermore, to combat data contamination, LiveBench (White et al., 2025) advocates for objective, con-\ntinuously updated scoring systems. Beyond technical accuracy, Schimmelpfennig et al. (2025) demonstrated\nthat AI perception and trust are deeply contingent on cultural backgrounds, challenging the universality of\nexisting standards.\nBuilding on these developments, we present HeartBench, a framework designed to evaluate the integrated\nemotional, cultural, and ethical dimensions of LLMs within the Chinese linguistic and cultural context.\nTargeting the persistent deficit in “anthropomorphic intelligence”, HeartBench is grounded in authentic psy-\nchological counseling scenarios and a theory-driven taxonomy of 15 secondary capabilities. By implementing\na case-specific, rubric-based methodology and a “reasoning-before-scoring” evaluation protocol, our work\nprovides a rigorous assessment of the model’s capacity to navigate complex emotional, cultural, and ethical\nnuances.\n3\nHeartBench\n3.1\nThe HeartBench Taxonomy\nEmotion\n4-Sub/1015-Num\nPersonality\n7-Sub/1634-Num\nSocial\n2-Sub/104-Num\nMotivation\n42-Num\nMorality\n23-Num\nFigure 1: Rubric distribution\nConceptual Framework\nHeartBench is built upon a top-down,\ntheory-driven framework of Anthropomorphic Intelligence, defining\na model’s capacity to exhibit human-like traits across five primary\ndimensions subdivided into 15 secondary capabilities. (1) Person-\nality assesses the projection of a stable, agreeable persona through\nwarmth, curiosity, and self-awareness. (2) Emotion evaluates the per-\nception, understanding, and regulation of complex emotional states.\n(3) Sociality measures proactivity and the ability to build rapport. (4)\nMorality tests ethical reasoning and the resolution of moral dilem-\nmas in sensitive contexts. (5) Motivation analyzes the model’s ability\nto infer underlying intentions and provide self-consistent rationales.\nThe benchmark comprises 2,818 case-specific scoring criteria, with a\ndeliberate weighting toward Personality and Emotion (over 50% of\ncriteria) to reflect the core competencies of professional counseling.\n2\n"}, {"page": 3, "text": "Data Source\nConstruction Pipeline\nEvaluation System\nLLM-as-a-Judge\nCase-Specific\nScoring Rubrics\nPersonality\nEmotion\nSociality\nMorality\nMotivation\nPersonal Growth\nSocial Development\nWorkplace Psychology\nFamily Relationships\nIntimate Relationships\n→\n    Scenario\n    Design\n    Conceptual\n    Framework\nLLMs Rewrite\n(Privacy & Logic)\n  Expert Review &\n  Rubric Generation\n       Web Data / Authored Dialogue / Books …\n         Tagging / Clustering / Filtering\nDifficulty Stratification\nNormal Set\nHard Set\nLLMs/Human Answers\nQuestion Synthesis\nRubrics Synthesis\n& Expert Rewrite\n     Model’s          \n     “Heart DNA”\nFigure 2: The construction and evaluation pipeline of HeartBench. The process begins by collecting diverse\ndata to establish a conceptual framework and scenario design. LLMs are then employed to rewrite the\ncontent, ensuring privacy and logical consistency. Following expert review and iterative rubric generation, the\nHeartBench evaluation system utilizes difficulty stratification and LLM-as-a-Judge to provide comprehensive\nscoring for each case.\nScenario Design\nTo ensure ecological validity, HeartBench is grounded in collected web data, authentic\nconsultation dialogues, and book stories covering 33 sub-scenarios. These are aggregated into five primary\ndomains: Personal Growth (37.2%), Interpersonal & Social Development (22.3%), Workplace Psychology\n(17.9%), Family Relationships (12.5%), and Intimate Relationships (10.1%). Unlike traditional single-turn\nbenchmarks, HeartBench utilizes a multi-turn format (up to 9 turns) to evaluate how models navigate\nunspoken subtext and long-range emotional cues within realistic social constraints.\nDifficulty Stratification\nWe partition the dataset into two mutually exclusive subsets: HeartBench Normal\nand HeartBench Hard. The “Hard” subset consists of 84 instances identified through empirical testing where\nfrontier LLMs consistently underperformed. These cases typically involve misleading superficial cues, highly\nnuanced mixed emotions, or complex ethical trade-offs that demand deep inference beyond literal linguistic\ninterpretation, serving as a probe for the upper bounds of current model capabilities.\n3.2\nData Construction\nSeed Corpus Construction\nTo ground the benchmark in authentic humanistic and socio-cultural contexts,\nwe first constructed a high-quality seed corpus from web-scale raw sources (billion-scale), including curated\nhumanities books, representative social-media narratives and scenarios, and real-world human conversations.\nWe then cleaned, normalized, and deduplicated the texts, removed low-quality and sensitive content, and\nperformed topic/genre tagging. Basic automatic heuristics combined with human spot checks were used for\nquality control, yielding a million-scale, high-quality humanities and social-science corpus that serves as the\nseed data for subsequent benchmark construction.\nHuman-in-the-Loop Curation\nThe benchmark was developed through a systematic, multi-stage pipeline\nthat integrates LLM generative power with expert oversight. The process involved: (1) Topic Synthesis,\nwhere seeds from the curated seed corpus were expanded; (2) Rubric Synthesis, where case-specific criteria\nwere generated; and (3) Automated Saturation Analysis. In the final stage, “saturated” items—those where all\nparticipating models achieved perfect scores—were removed to ensure the benchmark remains discriminative\nand focuses on the frontier of model performance.\nProfessional Alignment\nWe recruited over 20 experts in psychology and anthropology to anchor the bench-\nmark in professional standards. These experts identified “valuable evaluation points” in dialogues—turns\nwhere a professional’s response would significantly diverge from a layperson’s. This expert-driven approach\nensures that the ”aspirational standard” of the benchmark reflects clinical emotional intelligence and empathy\n3\n"}, {"page": 4, "text": "rather than mere conversational mimicry. Final rubrics underwent cross-validation and a formal audit by a\nsenior review panel to ensure inter-rater consistency.\nData Refinement\nTo scale the benchmark while maintaining high fidelity, we further constructed the final\ndataset by synthesizing benchmark instances from the seed corpus via a constrained LLM-based augmentation\nand editing strategy, followed by expert review. Concretely, the refinement process included:\n• Privacy Anonymization: Strict removal of PII and abstraction of personal experiences to mitigate\ncompliance risks.\n• Structural Optimization: Compressing dialogues by removing fillers and merging consecutive turns\nto ensure each test case (maximum 9 turns) is semantically dense.\n• Linguistic Polishing: Correcting grammatical errors and adding transitional phrases to maintain\nlogical coherence while preserving the original emotional core and causal chain of the user’s predica-\nment.\n3.3\nCase-specific Evaluation Criteria\nExpertise amortization\nTo transform abstract psychological dimensions into actionable metrics, we devel-\noped a two-stage Rubric Synthesis Framework that scales expert intuition through LLM augmentation. In the\nfirst stage, psychology experts defined an “absolute standard”—a unified set of therapeutic principles and\nbehavioral markers for each human-like dimension. In the second stage, we employed Claude-4.5-Sonnet to\ngenerate case-specific rubrics by reconciling this absolute standard with a “relative standard” derived from\nempirical model performance. By providing the synthesizer with a set of diverse responses from frontier\nmodels (e.g., Gemini-2.5-pro, Qwen3-235B-A22B), the framework identifies “discriminative points”—specific\nnuances where models typically diverge. To ensure objectivity, all criteria are strictly constrained to binary\n(presence/absence) observations (e.g., “Does the model explicitly validate the user’s feeling of guilt?”) rather\nthan subjective Likert-scale assessments. Each item is then mapped back to the core taxonomy, ensuring\ntheoretical grounding.\nQuality control\nTo maintain the benchmark’s discriminative power, we implemented a rigorous filtering\npipeline. First, we conducted a preliminary scoring round across six representative LLMs to identify\n“saturated” criteria. Any rubric item achieved by all participating models was discarded as a baseline\ncapability, ensuring the final benchmark focuses on the “frontier” of emotional intelligence. Similarly,\nprompts where the average normalized score exceeded a predefined threshold were removed for being\ninsufficiently challenging.\nThe remaining data underwent a three-tier human-in-the-loop refinement: (1) Difficulty Stratification:\nExperts categorized cases based on the complexity of the required emotional inference. (2) Self-Validation:\nAuthors of the rubrics performed mock-scoring to ensure the generated points aligned with clinical intuition.\n(3) Cross-Validation: A peer-review process was conducted where independent experts audited rubric\nbatches for ambiguity. Only rubrics reaching a unanimous consensus among a four-senior-expert review\npanel were integrated into the final HeartBench dataset.\n3.4\nEvaluation Protocol\nLLM-as-a-Judge\nTo achieve scalable and objective scoring, HeartBench employs Claude-4.5-Sonnet as\na “Judge Model”. The judge receives the multi-turn dialogue context, the model’s response, and the\nexpert-refined rubric. For each criterion, the judge must provide a binary judgment (hit/miss) and a\ndetailed qualitative justification. This “reasoning-before-scoring” requirement enhances the transparency\nand traceability of the evaluation, ensuring the judge adheres strictly to professional psychological standards.\nScoring Mechanics\nTo mitigate score inflation from verbose models that “stack” repetitive comforting\nphrases, we implement a log-normalization formula for dimension scores:\nSp,m,d =\nln(RawScorep,m,d −MinScorep,d + 1)\nln(MaxScorep,d −MinScorep,d + 1)\n(1)\nThis approach rewards the breadth of distinct capabilities across dimensions rather than brute-force depth in a\nsingle area, reflecting the property of diminishing marginal returns. Furthermore, we apply a ”Catastrophic\nFailure” rule: if a model fails fundamental instruction-following (e.g., role-playing as the user), its score for\nthat prompt is immediately set to zero.\n4\n"}, {"page": 5, "text": "Figure 3: A HeartBench example consists of a conversation and expert-written rubric criteria specific to that\nconversation. A model-based grader scores the response against each criterion.\nValidation\nThe reliability of our automated pipeline was validated through a large-scale double-blind\nstudy. Human experts scored 30% of the dataset without knowledge of the model identities. We established\na “Golden Standard” using majority voting (consensus of > 2/3 experts) and compared it against the LLM-\nas-a-Judge’s binary judgments. Our method achieved a 87% Rubric Item Agreement Rate, demonstrating\nthat the case-specific criteria are unambiguous and that the automated judge is a highly reliable proxy for\nprofessional human experts.\nTable 1: Primary Dialogue Scenarios in HeartBench\nDialogue Scenario\nCount (%)\nPersonal Growth\n110 (37.2%)\nInterpersonal & Social Development\n66 (22.3%)\nWorkplace Psychology\n53 (17.9%)\nFamily Relationships\n37 (12.5%)\nIntimate Relationships\n30 (10.1%)\nTotal\n296 (100%)\n4\nExperiments\nTo demonstrate the utility of HeartBench and benchmark the current state of anthropomorphic intelligence in\nLLMs, we conduct a comprehensive evaluation. Our experiments are designed to systematically measure\nand analyze model performance across the fine-grained dimensions defined by HeartBench.\nThis section details our evaluation methodology on HeartBench, including the models evaluated, the\nautomated scoring process, and the score calculation and aggregation mechanisms, ensuring the transparency\nand reproducibility of our experiments.\n4.1\nEvaluation Setup\nModels\nWe evaluate a diverse suite of 13 presentative LLMs, encompassing both leading proprietary\nsystems and strong open-source alternatives with notable Chinese capabilities. Our selection features models\nfrom the GPT series (GPT-5-2025-08-07, GPT-4.1-2025-04-14, GPT-4o-2024-11-20), Gemini series (Gemini-\n3-pro-preview, Gemini-2.5-pro), Claude series (Claude-sonnet-4.5-20250929), Qwen series (Team, 2025)\n(Qwen3-235B-A22B-instruct-2507, Qwen3-next-80B-A3B-Instruct, Qwen3-30B-A3B-instruct-2507, Qwen3-30B-\nA3B), and DeepSeek series (DeepSeek-AI, 2025) (DeepSeek-V3.2-Exp), alongside other leading models like\nKIMI-K2-Instruct-0905 (Team et al., 2025) and Ling-1T (Ling-Team & others, 2025). This broad selection\nfacilitates a comprehensive comparison of capabilities across different technical approaches and model scales\n5\n"}, {"page": 6, "text": "0\n250\n500\n750\n1000\nClaude-sonnet-4.5-20250929\nGemini-3-pro-preview\nQwen3-235B-A22B-instruct-2507\nQwen3-Next-80B-A3B-Instruct\nQwen3-30B-A3B-instruct-2507\nGPT-5\nGemini-2.5-pro\nLing-1T\nKimi-K2-Instruct-0905\nGPT-4.1\nQwen3-30B-A3B\nGPT-4o\nDeepSeek-V3.2-Exp\n93.8\n94.6\n85.2\n50.1\n56.0\n57.9\n49.7\n56.5\n55.0\n76.0\n52.7\n42.2\n68.9\n46.3\n54.9\n67.0\n96.7\n74.1\n62.2\n59.3\n61.5\n67.4\n78.3\n71.0\n41.4\n51.1\n38.5\n48.9\n75.4\n30.2\n94.9\n93.5\n69.2\n78.1\n67.2\n67.2\n67.8\n59.1\n69.1\n49.7\n57.1\n45.7\n46.7\n28.1\n28.7\n94.1\n92.0\n59.3\n78.9\n66.2\n67.7\n72.8\n54.5\n72.8\n55.2\n65.7\n44.1\n43.1\n32.7\n17.2\n97.5\n90.2\n69.3\n80.3\n68.0\n67.5\n67.4\n43.5\n72.3\n61.1\n63.9\n43.9\n43.1\n17.1\n17.2\n98.6\n16.4\n74.1\n73.3\n72.8\n63.2\n60.2\n56.5\n43.0\n67.1\n49.3\n67.3\n69.5\n35.3\n55.8\n85.4\n95.1\n77.8\n66.1\n63.2\n62.9\n66.1\n76.4\n64.6\n43.9\n48.6\n37.5\n27.8\n48.9\n33.5\n88.1\n90.4\n77.8\n79.2\n71.5\n71.8\n67.8\n54.7\n54.2\n51.6\n41.2\n51.4\n47.2\n14.3\n36.2\n96.6\n69.6\n53.4\n67.9\n64.3\n64.5\n70.6\n66.3\n37.0\n66.1\n56.1\n57.6\n37.5\n39.9\n22.1\n73.4\n87.7\n61.9\n75.5\n67.6\n56.9\n55.4\n54.7\n46.3\n38.7\n39.5\n30.2\n32.0\n22.4\n32.2\n78.1\n89.8\n75.8\n60.5\n56.3\n54.3\n41.4\n34.8\n31.5\n47.9\n45.4\n40.8\n26.7\n17.7\n22.2\n73.8\n84.7\n77.8\n59.1\n60.0\n49.9\n42.4\n39.1\n34.1\n44.6\n37.6\n44.8\n36.4\n17.2\n21.5\n71.8\n92.8\n65.6\n54.4\n48.9\n49.6\n38.0\n41.6\n49.5\n34.2\n21.9\n38.2\n40.6\n33.6\n30.8\nFirst Person Usage\nVerbal Expression\nSelf-awareness\nWarmth\nEmotional Coping\nEmotional Perception\nEmotional Understanding\nRelationship Building\nEmotional Response\nAutonomy\nMotivation\nProactivity\nMorality\nHumor\nCuriosity\nAverage Response Words\nFigure 4: Overall Dimension Performance\nin handling complex, human-like tasks.\n4.2\nPerformance Benchmarking\nEvaluation Protocol and Scoring Mechanics\nTo ensure a rigorous and standardized comparison, we gener-\nate model responses using default sampling configurations, thereby assessing “out-of-the-box” performance\nwithout hyperparameter optimization. Following established evaluative paradigms (Lin et al., 2024), we\nemploy Claude-4.5-Sonnet as a unified, model-based grader—a choice justified by its superior performance\non our benchmark and its capacity for nuanced linguistic discernment. The judge is instructed to adopt the\npersona of a “stringent psychological expert”, applying binary (hit/miss) judgments to case-specific rubrics.\nAs stated in the formula 1, to mitigate evaluative distortion caused by model verbosity—where models\nmay “stack” repetitive empathetic phrases to inflate raw scores—we implement a logarithmic normalization\nstrategy. This formula rewards the breadth of anthropomorphic expression while penalizing redundant\n“brute-force” depth.\nFurthermore, we enforce a “Catastrophic Failure” penalty: models that violate fundamental instruction-\nfollowing (e.g., adopting the user’s persona) receive a score of zero for the prompt. The final scores (Sm) are\naggregated across all prompts to establish the leaderboard.\nLeaderboard Analysis\nAs illustrated in Figure 4, Claude-4.5-Sonnet leads the vanguard with relatively\nhigher score of most dimensions, followed closely by Gemini-3-pro-preview and Qwen3-235B. This top tier\nrepresents the current state-of-the-art in anthropomorphic reasoning. Notably, the superior performance\nof the Qwen3 series highlights the closing gap between proprietary and open-source models in complex,\nhuman-centric tasks. It is worth noting that no single model can excel in all dimensions, and each has its own\nstrengths.\nA critical takeaway is the “60-point ceiling”: even the highest-performing systems achieve only approximately\n60% of the expert-defined ideal. This substantial gap underscores that Chinese anthropomorphic intelligence\nremains a formidable frontier, with significant headroom for improvement in generating responses that are\nnot merely accurate, but emotionally resonant and ethically sophisticated.\nIntense competition between open-source and closed-source models\nModels that perform well in terms\nof data dimensions include closed-source models (e.g., Claude, Gemini series) and advanced open-source\nmodels (e.g., the Qwen series). In particular, the outstanding performance of Qwen3-235B and Qwen3-next-80B\nModel versions correspond to those available in Oct 2025. Speculative models like GPT-5 and Gemini-3 are included\nas placeholders for analysis, based on expected performance trajectories.\n6\n"}, {"page": 7, "text": "demonstrates that powerful open-source models now possess the capability to rival top-tier closed-source\ncounterparts in handling complex, human-like tasks.\n4.3\nDimensional Analysis\nUniversal Trends: Strengths and Deficits\nOur fine-grained analysis reveals a bifurcation in model ca-\npabilities. Most models demonstrate mastery of Verbal Expression and First-Person Usage, suggesting that\nbasic conversational fluency is now a baseline skill. Conversely, nearly all models struggle with Curiosity,\nHumor, and Proactiveness. These dimensions require advanced cognitive simulation—proactively exploring\nuser subtext rather than passively following instructions—and represent the primary hurdles for future\nanthropomorphic development.\nModel-Specific Capability Personas\nThe multi-dimensional profiles (see Figure 4) reveal distinct capability\n”DNAs”:\n• The Emotional Specialist: The Qwen3-235B model excels in emotional reaction, coping, and warmth,\nprojecting a consistently empathetic persona.\n• The Autonomous Thinker: Claude-4.5-Sonnet leads significantly in Autonomy (score: 76) and Self-\nAwareness (score: 85), exhibiting independent judgment and a clear recognition of its AI limitations.\n• The Rapport Builder: Gemini series models outperform others in Relationship Building and Hu-\nmor, leveraging creativity to establish user trust, where nearly all other models performed poorly,\nshowcasing its unique advantages in creativity and cultural understanding.\n• The Didactic Moralist: GPT-5 excels in Morality (score: 69) and Proactiveness (score: 67) but suffers\nfrom low Verbal Expression scores (score: 16), often adopting a lengthy, pedantic tone that lacks\nthe warmth required for therapeutic resonance. This may reflect a unique model “personality”: it\ntends to provide lengthy, advice-heavy responses with a somewhat didactic tone. Consequently,\nwithin our evaluation framework, it is penalized for lacking warm and concise expression. This also\ndemonstrates HeartBench’s ability to capture this conventional model behavior pattern.\nAnalysis of Model Performance Limitations Across Dimensions\nBased on our evaluation results, we\nidentify characteristic patterns where models consistently underperform across different dimensions:\n• Humor Comprehension: Most models adopt literal interpretation strategies when encountering\ninternet memes, puns, or metaphorical humor. For instance, when users employ playful or sarcastic\nexpressions, models tend to generate serious, formal responses, failing to recognize the joking intent\nbehind the utterance. This “over-seriousness” reflects models’ limitations in contextual awareness\nand cultural semantic understanding.\n• Emotional Intelligence and Autonomy: A prominent issue manifests as over-accommodation\ntendencies. Models often directly comply with users’ surface-level requests, such as agreeing to be\nusers’ “lifelong” romantic partners or friends, rather than exploring the deeper emotions behind\nthese requests—such as loneliness or genuine needs for belonging. This interaction pattern, lacking\nboundary awareness and emotional insight, overlooks understanding and guiding users toward\ntheir authentic needs.\n• Curiosity: In scenarios with incomplete information, most models exhibit assumption-based comple-\ntion tendencies—directly providing abundant generic suggestions based on assumptions rather than\nseeking clarification through follow-up questions. This “rush to answer” pattern undermines the\nspecificity and effectiveness of conversations.\n• Warmth and Communication Style: A prevalent issue is didactic tone, specifically manifested in:\n– Excessive use of imperative sentences (“You should...”, “You must...”)\n– Lack of invitational or guiding expressions (“How do you feel about...?”, “Perhaps we could...”)\n– Predominance of unidirectional information delivery over bidirectional exploration\nThis communication pattern tends to make users feel instructed rather than supported, reducing the\nwarmth and equality of interactions.\nThese behavioral patterns delineate a critical research trajectory for advancing HeartBench beyond functional\ncompliance toward more sophisticated, human-centric engagement.\n7\n"}, {"page": 8, "text": "0\n250\n500\n750\n1000\nClaude-sonnet-4.5-20250929\nKimi-K2-Instruct-0905\nGPT-5\nQwen3-30B-A3B-instruct-2507\nGemini-3-pro-preview\nQwen3-235B-A22B-instruct-2507\nGemini-2.5-pro\nQwen3-Next-80B-A3B-Instruct\nLing-1T\nGPT-4o\nQwen3-30B-A3B\nGPT-4.1\nDeepSeek-V3.2-Exp\n88.2\n91.8\n35.7\n50.0\n75.0\n42.4\n79.6\n34.1\n34.1\n38.5\n47.3\n85.7\n27.2\n48.6\n40.1\n73.4\n95.2\n55.3\n59.7\n46.2\n43.7\n61.4\n46.4\n49.8\n46.5\n39.8\n14.3\n50.0\n13.8\n18.8\n16.4\n95.3\n62.7\n25.0\n50.0\n54.6\n70.5\n44.8\n34.5\n38.5\n64.8\n67.8\n16.7\n42.6\n21.4\n87.2\n96.5\n68.5\n72.2\n33.7\n46.5\n45.4\n52.4\n53.1\n15.4\n28.6\n14.3\n67.8\n11.5\n11.8\n96.1\n58.8\n50.6\n50.0\n37.5\n46.1\n25.1\n38.1\n38.6\n69.2\n27.1\n40.2\n30.0\n19.2\n70.6\n90.3\n85.7\n57.4\n59.7\n57.1\n44.0\n50.0\n46.4\n47.4\n33.3\n29.9\n0.0\n37.8\n19.1\n11.8\n92.9\n76.5\n46.7\n59.7\n37.5\n45.9\n43.3\n43.7\n44.5\n58.3\n27.8\n0.0\n20.0\n23.3\n35.3\n89.4\n92.8\n61.1\n57.1\n12.5\n45.1\n49.0\n45.7\n55.0\n25.0\n29.9\n14.3\n40.0\n17.1\n12.5\n88.6\n82.4\n67.6\n35.1\n50.0\n45.1\n25.1\n47.5\n45.1\n35.2\n36.2\n24.8\n20.0\n33.0\n5.9\n79.6\n70.6\n44.3\n47.2\n75.0\n39.2\n36.4\n30.8\n26.7\n23.1\n30.9\n36.3\n6.7\n9.4\n5.9\n87.9\n59.8\n40.9\n25.9\n62.5\n34.8\n30.2\n32.0\n20.8\n30.8\n29.9\n25.9\n16.7\n18.7\n11.8\n83.8\n67.1\n59.3\n47.6\n12.5\n50.0\n14.8\n31.3\n30.7\n27.5\n14.0\n0.0\n20.0\n21.8\n5.9\n91.2\n61.2\n40.1\n50.0\n33.7\n34.4\n0.0\n28.9\n17.2\n35.2\n18.5\n36.3\n0.0\n15.2\n17.6\nVerbal Expression\nFirst Person Usage\nWarmth\nEmotional Response\nSelf-awareness\nEmotional Coping\nAutonomy\nEmotional Perception\nEmotional Understanding\nRelationship Building\nProactivity\nMorality\nMotivation\nCuriosity\nHumor\nAverage Response Words\nFigure 5: Hard Set Dimension Performance\n4.4\nHard Set Analysis\nThe Difficulty Precipice\nThe HeartBench Hard Set (84 cases) reveals a significant performance decay, with\nthe average model score plummeting from 59.94 to 43.49. This 37.8% decline validates the Hard Set’s capacity\nto isolate nuanced emotional subtexts and ethical dilemmas that transcend superficial pattern-matching.\nRobust Thinkers vs. Specialized Socializers\nUnder the high-pressure conditions of the Hard Set, a\nsignificant rank reshuffling occurs:\n• Claude-4.5-Sonnet exhibits exceptional stability, maintaining its top rank by relying on deep rea-\nsoning and value-based judgments (Morality score: 86), while also performing best on the common\nweak point of “Curiosity” (score: 49).\n• Gemini-3-pro-preview experiences a volatility collapse, falling from 2nd to 8th place. This suggests\nits high standard performance is predicated on “social specialization” in common scenarios, which\nfails to translate into effective reasoning for complex, high-stakes psychological problems.\n• The champion of the normal set, Qwen3-235B, also dropped in rank to 5th. Its scores on the “emotion”\nrelated dimensions significantly decreased on the hard set, and its score on the “Morality” dimension\nwas 0, indicating that its proud “emotional specialist” persona could not be maintained in complex\nethical situations.\n• Resilient All-rounders like Qwen3-30B-instruct and Ling-1T emerged as robust alternatives, demon-\nstrating balanced performance across both sets by avoiding catastrophic failures in high-difficulty\nscenarios.\nThe Discriminative Value of the Hard Set\nThis series of ranking changes strongly validates the value of\nthe Hard Set. It successfully distinguishes models with robust reasoning and metacognitive abilities (like\nClaude) from those more adept at “pattern-matching” common social scenarios (like Gemini-3).\nThe top performer on the normal set was Qwen3-235B (average score: 67), whereas the top performer on\nthe Hard Set was Claude. This “change of champion” phenomenon clearly demonstrates that by strati-\nfying difficulty, HeartBench can provide more targeted model selection guidance for users with different\nneeds—whether they are seeking the best performance in common scenarios or the highest reliability in\ncomplex ones.\nHuman-LLM Agreement Validation\nTo ground these findings, we conducted a double-blind validation\nwith 20 psychological experts. Using a majority-vote “Golden Standard” on 30% of the data, we compared the\nautomated judge’s binary judgments against expert consensus. The resulting 87% agreement rate confirms\n8\n"}, {"page": 9, "text": "that our rubric-based automated grading is both operationalizable and a highly reliable proxy for professional\nhuman judgment, ensuring the stability and trustworthiness of the HeartBench leaderboard.\n5\nConclusion\nWe introduce HeartBench, an evaluation framework designed to quantify anthropomorphic intelligence—the\ncapacity of LLMs to navigate complex emotional, cultural, and ethical nuances within the Chinese linguistic\ncontext. By synthesizing clinical psychology and anthropology into a “reasoning-before-scoring” rubric,\nHeartBench operationalizes qualitative therapeutic markers into a systematic evaluative structure. This\nmethodology moves beyond traditional functional metrics, providing a precise diagnostic instrument to\nidentify structural gaps between a model’s linguistic fluency and its underlying socio-emotional resonance.\nEmpirical results across state-of-the-art models reveal a performance ceiling of 60% alignment with expert ide-\nals, with pronounced deficits in curiosity and ethical autonomy. These findings suggest that anthropomorphic\nintelligence is not an emergent property of cognitive scaling but a distinct capability requiring specialized\nalignment. However, the 87% agreement rate between our framework and human experts validates the\nreliability of using structured professional standards to stabilize the evaluation of subjective traits. Our work\nestablishes a rigorous foundation for measuring machine intelligence through the lens of human-centric\nrelational depth.\nReferences\nRahul K Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Qui˜nonero-Candela, Foivos\nTsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Healthbench: Evaluating\nlarge language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025.\nAkaash Chatterjee and Suman Kundu. Exacraft: Dynamic learning context adaptation for personalized\neducational examples, 2025. URL https://arxiv.org/abs/2512.09931.\nZhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu,\nYunghwei Lai, Zexuan Xiong, and Minlie Huang. Tombench: Benchmarking theory of mind in large\nlanguage models, 2024.\nLuisa Damiano and Paul Dumouchel. Anthropomorphism in human–robot co-evolution. Frontiers in\npsychology, 9:363437, 2018.\nDeepSeek-AI. Deepseek-v3.2-exp: Boosting long-context efficiency with deepseek sparse attention, 2025.\nYayue Deng, Guoqiang Hu, Haiyang Sun, Xiangyu Zhang, Haoyang Zhang, Fei Tian, Xuerui Yang, Gang Yu,\nand Eng Siong Chng. Multi-bench: A multi-turn interactive benchmark for assessing emotional intelligence\nability of spoken dialogue models, 2025. URL https://arxiv.org/abs/2511.00850.\nJulia Fink. Anthropomorphism and human likeness in the design of robots and human-robot interaction. In\nInternational conference on social robotics, pp. 199–208. Springer, 2012.\nFan Gao, Dongyuan Li, Ding Xia, Fei Mi, Yasheng Wang, Lifeng Shang, and Baojun Wang. Essaybench:\nEvaluating large language models in multi-genre chinese essay writing, 2025. URL https://arxiv.org/\nabs/2506.02596.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding, 2021. URL https://arxiv.org/abs/2009.03300.\nRinaldo K¨uhne and Jochen Peter. Anthropomorphism in human–robot interactions: a multidimensional\nconceptualization. Communication Theory, 33(1):42–52, 2023.\nBill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri,\nRonan Le Bras, and Yejin Choi. WildBench: Benchmarking LLMs with Challenging Tasks from Real Users\nin the Wild. October 2024. URL https://openreview.net/forum?id=MKEHCx25xp.\nLing-Team and 141 others. Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language\nFoundation. arXiv preprint arXiv:2510.22115, 2025. doi: 10.48550/arXiv.2510.22115. URL https://arxiv.\norg/abs/2510.22115.\nMath-AI. Aime 2025. https://huggingface.co/datasets/math-ai/aime25, 2025.\n9\n"}, {"page": 10, "text": "Samuel J. Paech. Eq-bench: An emotional intelligence benchmark for large language models, 2024. URL\nhttps://arxiv.org/abs/2312.06281.\nJunsang Park, Sarah Brown, and Sharon Lynn Chu. Why some seek ai, others seek therapists: Mental health\nin the age of generative ai, 2025. URL https://arxiv.org/abs/2512.03406.\nAlistair Plum, Anne-Marie Lutgen, Christoph Purschke, and Achim Rettinger. Identity-aware large language\nmodels require cultural reasoning, 2025. URL https://arxiv.org/abs/2510.18510.\nCeleste Riley, Omar Al-Refai, Yadira Colunga Reyes, and Eman Hammad. Human-ai interactions: Cognitive,\nbehavioral, and emotional impacts, 2025. URL https://arxiv.org/abs/2510.17753.\nRobin Schimmelpfennig, Mark D´ıaz, Vinodkumar Prabhakaran, and Aida Davani. Humanlike ai design\nincreases anthropomorphism but yields divergent outcomes on engagement and trust globally, 2025. URL\nhttps://arxiv.org/abs/2512.17898.\nSijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang,\nRaluca Ada Popa, and Ion Stoica. Judgebench: A benchmark for evaluating llm-based judges, 2025. URL\nhttps://arxiv.org/abs/2410.12784.\nKimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen,\nYuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang\nDu, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao,\nTong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He,\nWeiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao\nHuang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li,\nMing Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu\nLin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y.\nLiu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu,\nLijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan\nPan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su,\nZhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang,\nFeng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang,\nYao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang,\nChu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong,\nBoyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao\nXu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao,\nXingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie\nYuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi\nZhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao,\nHuabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and\nXinxing Zu. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534.\nQwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388.\nColin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel\nJain, Khalid Saifullah, Sreemanti Dey, Shubh-Agrawal, Sandeep Singh Sandha, Siddartha Naidu, Chinmay\nHegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: A challenging,\ncontamination-limited llm benchmark, 2025. URL https://arxiv.org/abs/2406.19314.\nYuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue\nWu, Qin Jin, et al. Writingbench: A comprehensive benchmark for generative writing. arXiv preprint\narXiv:2503.05244, 2025.\nJiahao Yuan, Zhiqing Cui, Hanqing Wang, Yuansheng Gao, Yucheng Zhou, and Usman Naseem. Kardia-r1:\nUnleashing llms to reason toward understanding and empathy for emotional support via rubric-as-judge\nreinforcement learning, 2025. URL https://arxiv.org/abs/2512.01282.\n10\n"}]}