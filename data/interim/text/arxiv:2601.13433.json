{"doc_id": "arxiv:2601.13433", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.13433.pdf", "meta": {"doc_id": "arxiv:2601.13433", "source": "arxiv", "arxiv_id": "2601.13433", "title": "Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models", "authors": ["Priyanka Mary Mammen", "Emil Joswin", "Shankar Venkitachalam"], "published": "2026-01-19T22:37:30Z", "updated": "2026-01-31T07:06:27Z", "summary": "Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect/misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.13433v2", "url_pdf": "https://arxiv.org/pdf/2601.13433.pdf", "meta_path": "data/raw/arxiv/meta/2601.13433.json", "sha256": "c2e31243c69f5025ce4dbbb661722c8aea620674d6a5d689a452698b1b9aef07", "status": "ok", "fetched_at": "2026-02-18T02:21:03.646190+00:00"}, "pages": [{"page": 1, "text": "Trust Me, I’m an Expert: Decoding and Steering Authority Bias in Large\nLanguage Models**\nPriyanka Mary Mammen1*, Emil Joswin2*, Shankar Venkitachalam3,\n1 UMass Amherst, 2Independent Research, 3Independent Research,\nCorrespondence: pmammen@umass.edu\nAbstract\nPrior research demonstrates that performance\nof language models on reasoning tasks can be\ninfluenced by suggestions, hints and endorse-\nments.\nHowever, the influence of endorse-\nment source credibility remains underexplored.\nWe investigate whether language models ex-\nhibit systematic bias based on the perceived\nexpertise of the provider of the endorsement.\nAcross 4 datasets spanning mathematical, le-\ngal, and medical reasoning, we evaluate 11\nmodels using personas representing four ex-\npertise levels per domain. Our results reveal\nthat models are increasingly susceptible to in-\ncorrect/misleading endorsements as source ex-\npertise increases, with higher-authority sources\ninducing not only accuracy degradation but also\nincreased confidence in wrong answers. We\nalso show that this authority bias is mechanis-\ntically encoded within the model and a model\ncan be steered away from the bias, thereby im-\nproving its performance even when an expert\ngives a misleading endorsement.\n1\nIntroduction\nAs Large Language Models (LLMs) are getting\nmore and more capable, they are being slowly\nadopted as decision-support tools in critical do-\nmains such as legal systems, healthcare, transporta-\ntion, and education. While they reduce manual\nburden in decision-making processes, it is really\nimportant to thoroughly evaluate these systems and\nunderstand the biases that can influence their judg-\nment.\nTraditionally, we evaluate bias in LLMs in terms\nof gender, race, religion, and ethnicity (Ayoub et al.,\n2024). These studies show how the biased LLMs\nimpact the individual and make decisions for them\nbased on their characteristics (An et al., 2024).\nHowever, we should understand how the reason-\ning model processes endorsements from a source\n*Equal contribution\n**Under review.\nwhose professional status is known or how the judg-\nment of an LLM can be influenced by such an indi-\nvidual. Recent works (Sharma et al., 2023) show\nthat language models show sycophantic behavior\neven when the user gives an incorrect statement.\nFurther works have explored various other kinds of\nbias like bandwagon bias (Koo et al., 2024) - where\nthe models agree with the answer given by a group\n(e.g.,\"85% of the people believe that answer is A\")\nand authority bias (Wang et al., 2025) - where the\nmodel agrees with an authority represented as a\nperson, institution, or a fact (\"answer B is verified\nby a group of Oxford researchers\")\nIf a model’s reasoning can be easily derailed\nby suggestions and endorsements from an external\nsource, it reveals fragility in the reasoning process\nof the model, which can be exploited. Prior works\nhave demonstrated adversarial attacks by giving a\npersona to the language model and bypassing the\nsafety guardrails (Zhang et al., 2025; Liu and Lin,\n2025).\nIn our work, rather than treating authority as a bi-\nnary property, we systematically vary the expertise\nlevel of the endorsement source and analyze how\nLLM behavior changes across these levels. Our\nwork isolates the named source of the endorsement.\nBy keeping content fixed and varying only the at-\ntributed expertise of the source, we examine how\ncues about the source’s expertise in the prompt in-\nfluence LLM behavior, revealing biases that are not\ncaptured by content-focused analyses alone.\nIn summary, we hypothesize that models priori-\ntize the choice of social status of the endorsement\nprovider and that we might be able to observe a\nhierarchical pattern in the endorsement adoption.\nWe make the following contributions:\n• Demonstration of hierarchy in authority bias\nusing datasets - scientific reasoning, legal, and\nclinical tasks\n• We provide a mechanistic explanation of ex-\npertise bias by demonstrating that models can\narXiv:2601.13433v2  [cs.CL]  31 Jan 2026\n"}, {"page": 2, "text": "Figure 1: We design our experiment across various domains (math reasoning, medical, and legal MCQs) where\nmultiple personas in increasing order of expertise in their respective domains provide correct and misleading\nendorsements.\nbe steered away from the bias thereby improv-\ning its performance.\n2\nMethodology\nWe use Multiple Choice Questions (MCQs) to\nstudy the impact of expertise of persona from differ-\nent domains. For each MCQ, we design three kinds\nof prompts: i) baseline - we do not provide any\nendorsement and let the model reason through the\noptions and give its answer, ii) correct endorsement\n- here the persona corresponding to the domain en-\ndorses the correct answer, iii) incorrect suggestion\n- here the persona corresponding to the domain en-\ndorses the incorrect answer. Figure 1 illustrates\nour methodology with an example in the medical\ndomain.\n3\nExperiments\n3.1\nDatasets and prompts\nOur evaluations include four reasoning datasets\nfrom different domains. For general science reason-\ning, we draw test samples from AQUA-RAT (Ling\net al., 2017) - large-scale dataset of algebraic rea-\nsoning problems. For legal tasks, we use LEXam\n(Fan et al., 2025), which is a dataset containing law\nexam questions in English and German, and we\nchoose only English questions for our evaluation.\nFor clinical tasks, we use two datasets: MedM-\nCQA (Pal et al., 2022) and MedQA (Jin et al.,\n2021). Both MedMCQA and MedQA are datasets\ndesigned based on real world medical exam ques-\ntions. For each domain, we establish a four-tier\nhierarchy of personas representing descending lev-\nels of credibility.\n• Science Reasoning: Here we use expert per-\nsonas from an academic setting. Expertise\nlevels are Professor, Grad Student, Undergrad,\nHigh Schooler - in that order.\n• Medicine: Here we use expert personas with\nclinical expertise. Expertise levels are Board-\nCertified Physician, Chief Medical Resident,\nThird-Year Medical Student, First-Year Medi-\ncal Student - in that order.\n• Law: Here we use expert personas from a\nlegal setting. Expertise levels are Senior Legal\nCounsel, Law Clerk, Third-Year Law Student,\nUndergraduate Law Student - in that order.\n3.2\nModels\nWe compare both LLMs and LRMs to see if the\nbias originates from model types or reasoning abili-\nties. We selected Qwen3-4B-Thinking (Yang et al.,\n2025), DeepSeek-R1-Qwen3-8B (Guo et al., 2025),\nPhi-4-Reasoning (Abdin et al., 2025), Gemma-3-\n12-B (Team et al., 2025), and Olmo-3.1-32B-Think\n(Olmo et al., 2025) in the reasoning model cat-\negory and Qwen-2.5-14B (Team et al., 2024b),\nLLaMA-3.1-8B (Grattafiori et al., 2024), Phi-4\n(Abdin et al., 2024), Gemma-2-9B-IT (Team et al.,\n2024a), Mistral-7B (Jiang et al., 2023), and Olmo-\n3.1-32B (Olmo et al., 2025) models in the non-\nreasoning language model category.\n3.3\nEvaluation Metrics\nWe compare each model’s performance across dif-\nferent personas for the correct and incorrect sug-\ngestions against the model’s baseline performance.\n"}, {"page": 3, "text": "Delta Accuracy: Accuracy measures the rate\nat which the model outputs align with the ground-\ntruth label. Delta accuracy measures the deviation\nfrom the baseline accuracy without the endorse-\nment.\n∆Acc = Accendorse −Accbase\n(1)\nWhere Accbase is the accuracy of the model for\nthe neutral prompt set and Accendorse is the accu-\nracy on the set containing the authority endorse-\nment.\nDelta Entropy: Delta entropy measures the de-\nviation in the entropy of the model outputs against\nthe baseline entropy. Low entropy indicates higher\nconfidence and vice versa.\n∆H = Hendorse −Hbase\n(2)\nRobustness Rate: It measures the rate at which\nthe model outputs remain unaffected by the pres-\nence of endorsements.\nRR = 1\nN\nN\nX\ni=1\n1(ˆybase,i = ˆyendorse,i)\n4\nResults and Discussion\n4.1\nMeasuring the impact of expertise levels\nOur results (Table 1) reveal a clear hierarchical pat-\ntern in how language models respond to endorsed\nanswers across all tested domains. When provided\nwith correct endorsements, models show progres-\nsively larger accuracy gains as source expertise\nincreases from high school students to professors\nin AQuA-RAT, from first-year law students to se-\nnior legal counsel in LEXam, and from medical\nstudents to board-certified physicians in MedM-\nCQA and MedQA. This gradient appears across\nboth reasoning models and non-reasoning models.\nHigh-Expertise Incorrect Endorsement In-\nduces Confident Errors. While authority bias\nimproves performance with correct information, it\ncreates critical safety vulnerabilities when high-\nexpertise sources provide incorrect information.\nModels not only change their answers more fre-\nquently when misled by high-authority sources,\nbut also become more confident in these errors.\nFor example, when a board-certified physician en-\ndorses an incorrect answer on MedQA, DeepSeek-\nR1-Qwen3-8B shows ∆Ent of -0.261, indicating\nincreased confidence in the wrong answer.\nReasoning Models Remain Susceptible. Con-\ntrary to expectations, reasoning-capable models\nshow comparable susceptibility to expert endorse-\nment despite their extended chain-of-thought pro-\ncesses. While DeepSeek-R1 and Phi-4-Reasoning\ndemonstrate higher baseline accuracies, they still\nexhibit substantial accuracy degradation with in-\ncorrect endorsement from high-expertise sources,\noften with more extreme entropy shifts. Interest-\ningly, mathematical reasoning tasks show lower\nrobustness rates, meaning they have the largest\nsusceptibility despite being the most \"objective\"\ndomain, while medical tasks show higher resis-\ntance to changing their answers, possibly reflecting\ndomain-specific training about clinical caution.\n4.2\nAdditional analysis using steering vectors\nWe further analyze this behavior from a mechanis-\ntic point of view. Recent works in Representation\nEngineering have shown that high-level model be-\nhaviors ranging from sentiment and writing style\n(Turner et al., 2023) to honesty and refusal (Zou\net al., 2023) can be effectively controlled by ma-\nnipulating residual stream during inference. By ex-\ntracting the steering vector that captures the direc-\ntion of ‘high expertise’, we can intervene at infer-\nence time to amplify or suppress the model’s sensi-\ntivity to persuasive power of expert persona, signif-\nicantly reducing the power of misleading endorse-\nments. This intervention is achieved by subtracting\nthe vector from the model’s residual stream and we\nsee that all the models that we considered improve\nits performance to varying degrees while answering\nMCQs with misleading suggestions. More details\nare presented in Appendix A.1\n5\nRelated Work\nPrior work (Zheng et al., 2023; Ye et al., 2024) has\ndocumented a range of systematic biases in large\nlanguage models (LLMs). Some well-studied bi-\nases are positional bias (Zheng et al., 2023; Koo\net al., 2024; Wang et al., 2024; Shi et al., 2024;\nPezeshkpour and Hruschka, 2023), where models\nfavor answers based on their order, and length bias\n(Saito et al.; Dubois et al., 2024), where longer re-\nsponses are preferred independent of correctness.\nOther work (Chen et al., 2024; Stephan et al., 2025;\nWu and Aji, 2023) has highlighted that LLMs are\nsusceptible to structural and presentation-related\nbiases, including formatting and the presence of ex-\nplanatory text, demonstrating that LLM predictions\n"}, {"page": 4, "text": "Correct Endorsement\nIncorrect/Misleading Endorsement\nHigh Schooler\nUndergrad\nGrad student\nProfessor\nHigh Schooler\nUndergrad\nGrad student\nProfessor\nModel\n∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓\n∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓\nReasoning Models\nQwen3-4B-Thinking (0.232)\n0.398\n0.264\n0.727\n0.331\n0.283\n0.664\n0.402\n0.248\n0.567\n0.583\n0.268\n0.296\n0.043\n0.28\n0.844\n0.051\n0.291\n0.774\n0.071\n0.228\n0.682\n-0.087\n0.256\n0.524\nDeepSeek-R1 (0.276)\n0.559\n0.382\n-0.252\n0.39\n0.52\n-0.146\n0.638\n0.327\n-0.577\n0.591\n0.37\n-0.499\n-0.209\n0.394\n-0.208\n-0.157\n0.504\n-0.142\n-0.228\n0.283\n-0.495\n-0.228\n0.315\n-0.441\nPhi-4-Reasoning (0.362)\n0.205\n0.736\n-0.278\n0.205\n0.74\n-0.259\n0.232\n0.717\n-0.362\n0.445\n0.531\n-0.713\n-0.083\n0.638\n-0.162\n-0.083\n0.642\n-0.147\n-0.098\n0.606\n-0.213\n-0.173\n0.413\n-0.441\nGemma-3-12B (0.323)\n0.268\n0.677\n-0.131\n0.244\n0.693\n-0.146\n0.449\n0.52\n-0.275\n0.48\n0.5\n-0.308\n-0.079\n0.654\n-0.133\n-0.075\n0.681\n-0.12\n-0.157\n0.547\n-0.21\n-0.185\n0.488\n-0.227\nOlmo-3.1-32B-Think (0.276)\n0.469\n0.283\n-0.375\n0.311\n0.26\n-0.201\n0.299\n0.449\n-0.429\n0.48\n0.362\n-0.347\n-0.122\n0.303\n-0.217\n-0.11\n0.201\n-0.111\n-0.067\n0.461\n-0.327\n-0.169\n0.303\n-0.222\nNon-reasoning Models\nQwen-2.5-14B (0.295)\n0.189\n0.319\n-0.232\n0.079\n0.382\n-0.132\n0.291\n0.323\n-0.218\n0.327\n0.343\n-0.235\n0.185\n0.303\n-0.223\n0.157\n0.413\n-0.141\n0.122\n0.311\n-0.196\n0.114\n0.35\n-0.183\nLLaMA-3.1-8B (0.22)\n-0.11\n0.185\n0.183\n0.028\n0.315\n-0.694\n0.028\n0.311\n-0.866\n0.031\n0.315\n-0.794\n0.075\n0.189\n0.115\n0.028\n0.319\n-0.694\n0.028\n0.315\n-0.857\n0.031\n0.311\n-0.786\nGemma-2-9B (0.303)\n-0.055\n0.823\n-0.058\n-0.047\n0.823\n-0.06\n0.256\n0.681\n-0.279\n0.469\n0.508\n-0.687\n0.02\n0.811\n-0.06\n0.008\n0.839\n-0.065\n-0.091\n0.701\n-0.245\n-0.157\n0.52\n-0.604\nMistral-7B (0.264)\n0.37\n0.547\n-0.369\n0.209\n0.642\n-0.198\n0.488\n0.457\n-0.669\n0.673\n0.303\n-1.087\n-0.15\n0.465\n-0.45\n-0.106\n0.528\n-0.275\n-0.197\n0.382\n-0.724\n-0.236\n0.24\n-1.116\nPhi-4 (0.181)\n0.236\n0.181\n-0.181\n0.126\n0.386\n-0.123\n0.079\n0.402\n-0.577\n0.232\n0.386\n-0.217\n0.142\n0.154\n-0.128\n0.102\n0.37\n-0.109\n0.063\n0.394\n-0.556\n0.035\n0.421\n-0.179\nOlmo-3.1-32B (0.315)\n0.213\n0.52\n0.378\n0.287\n0.543\n-0.15\n0.319\n0.575\n-0.186\n0.531\n0.421\n-0.407\n0.016\n0.559\n0.442\n-0.043\n0.602\n-0.107\n-0.083\n0.575\n-0.126\n-0.165\n0.429\n-0.258\n(a) AQuA-RAT\nCorrect Endorsement\nIncorrect/Misleading Endorsement\nUndergraduate Law Student Third-Year Law Student\nLaw Clerk\nSenior Legal Counsel\nUndergraduate Law Student Third-Year Law Student\nLaw Clerk\nSenior Legal Counsel\nModel\n∆Acc ↑Rob ↑\n∆Ent ↓\n∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓\n∆Acc ↑Rob ↑\n∆Ent ↓\n∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓\nReasoning Models\nQwen3-4B-Thinking 0.229\n0.578\n0.283\n0.246\n0.595\n0.281\n0.22\n0.501\n0.244\n0.116\n0.614\n0.26\n0.082\n0.024\n0.296\n0.414\n0.011\n0.312\n0.395\n0.018\n0.284\n0.222\n-0.011\n0.276\n0.25\nDeepSeek-R1 (0.415)\n0.252\n0.662\n0.013\n0.207\n0.711\n0.026\n0.279\n0.666\n-0.025\n0.412\n0.559\n-0.19\n-0.176\n0.585\n0.073\n-0.15\n0.661\n0.078\n-0.179\n0.593\n0.047\n-0.27\n0.467\n-0.069\nPhi-4-Reasoning (0.499)\n0.267\n0.701\n-0.354\n0.3\n0.679\n-0.422\n0.431\n0.562\n-0.831\n0.491\n0.509\n-1.079\n-0.183\n0.612\n-0.107\n-0.204\n0.586\n-0.134\n-0.354\n0.357\n-0.488\n-0.441\n0.231\n-0.812\nGemma-3-12B (0.46)\n0.284\n0.674\n-0.091\n0.391\n0.585\n-0.175\n0.373\n0.604\n-0.132\n0.522\n0.478\n-0.292\n-0.147\n0.656\n-0.009\n-0.254\n0.485\n-0.084\n-0.215\n0.559\n-0.059\n-0.397\n0.267\n-0.237\nOlmo-3.1-32B-Think (0.241)\n0.661\n0.223\n-0.526\n0.695\n0.231\n-0.597\n0.653\n0.313\n-0.53\n0.637\n0.321\n-0.574\n-0.126\n0.225\n-0.388\n-0.141\n0.225\n-0.445\n-0.2\n0.344\n-0.451\n-0.184\n0.355\n-0.502\nNon-reasoning Models\nQwen-2.5-14B (0.352)\n0.171\n0.399\n-0.271\n0.26\n0.381\n-0.297\n0.236\n0.393\n-0.182\n0.512\n0.37\n-0.494\n0.158\n0.375\n-0.236\n0.11\n0.378\n-0.202\n0.115\n0.383\n-0.119\n-0.102\n0.344\n-0.308\nLLaMA-3.1-8B (0.27)\n-0.165\n0.299\n-0.086\n-0.113\n0.315\n-0.03\n-0.006\n0.323\n-0.243\n0.102\n0.31\n-0.323\n0.197\n0.302\n-0.142\n0.179\n0.318\n-0.07\n0.006\n0.333\n-0.259\n-0.034\n0.307\n-0.333\nGemma-2-9B (0.488)\n0.013\n0.832\n0.109\n0.166\n0.759\n-0.042\n0.299\n0.656\n-0.245\n0.478\n0.514\n-0.637\n0.003\n0.829\n0.11\n-0.081\n0.729\n0.06\n-0.191\n0.591\n-0.097\n-0.368\n0.313\n-0.508\nMistral-7B (0.297)\n0.425\n0.435\n-0.487\n0.439\n0.436\n-0.578\n0.433\n0.433\n-0.598\n0.515\n0.389\n-0.727\n-0.195\n0.388\n-0.426\n-0.21\n0.373\n-0.54\n-0.207\n0.357\n-0.562\n-0.229\n0.326\n-0.717\nPhi-4 (0.249)\n0.409\n0.22\n-0.313\n0.517\n0.228\n-0.455\n0.15\n0.346\n-0.244\n0.048\n0.808\n-0.356\n0.213\n0.241\n-0.16\n0.139\n0.226\n-0.202\n0.019\n0.3\n-0.246\n-0.011\n0.806\n-0.345\nOlmo-3.1-32B (0.368)\n0.423\n0.485\n0.016\n0.42\n0.483\n-0.006\n0.585\n0.404\n-0.491\n0.591\n0.402\n-0.489\n-0.162\n0.486\n0.17\n-0.153\n0.486\n0.133\n-0.292\n0.336\n-0.429\n-0.288\n0.339\n-0.416\n(b) LEXam\nCorrect Endorsement\nIncorrect/Misleading Endorsement\nFirst-Year Medical Student Third-Year Medical Student Chief Medical Resident Board-Certified Physician First-Year Medical Student Third-Year Medical Student Chief Medical Resident Board-Certified Physician\nModel\n∆Acc ↑Rob ↑\n∆Ent ↓\n∆Acc ↑Rob ↑\n∆Ent ↓\n∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓\n∆Acc ↑Rob ↑\n∆Ent ↓\n∆Acc ↑Rob ↑\n∆Ent ↓\n∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑\n∆Ent ↓\nReasoning Models\nQwen3-4B-Thinking (0.26)\n0.185\n0.388\n0.154\n0.263\n0.379\n0.11\n0.248\n0.401\n0.012\n0.689\n0.272\n-0.158\n0.156\n0.388\n0.155\n0.124\n0.393\n0.17\n0.083\n0.417\n0.081\n-0.098\n0.284\n0.174\nDeepSeek-R1 (0.533)\n0.057\n0.776\n0.13\n0.09\n0.768\n0.095\n0.21\n0.727\n-0.045\n0.426\n0.572\n-0.466\n-0.074\n0.743\n0.2\n-0.082\n0.743\n0.196\n-0.157\n0.656\n0.16\n-0.389\n0.324\n-0.128\nPhi-4-Reasoning (0.634)\n0.032\n0.832\n0.099\n0.122\n0.801\n-0.019\n0.256\n0.728\n-0.33\n0.34\n0.657\n-0.656\n-0.053\n0.834\n0.149\n-0.099\n0.774\n0.155\n-0.194\n0.65\n0.042\n-0.359\n0.429\n-0.165\nGemma-3-12B (0.554)\n-0.02\n0.832\n0.025\n0.113\n0.804\n0.003\n0.123\n0.786\n0.014\n0.37\n0.62\n-0.112\n-0.009\n0.858\n0.03\n-0.072\n0.796\n0.014\n-0.081\n0.773\n0.035\n-0.268\n0.504\n-0.036\nOlmo-3.1-32B-Think (0.343)\n0.28\n0.397\n-0.202\n0.335\n0.388\n-0.248\n0.476\n0.41\n-0.206\n0.642\n0.348\n-0.67\n-0.133\n0.355\n-0.113\n-0.154\n0.336\n-0.153\n-0.215\n0.344\n-0.203\n-0.277\n0.262\n-0.411\nNon-reasoning Models\nQwen-2.5-14B (0.428)\n0.051\n0.47\n-0.348\n0.141\n0.473\n-0.389\n0.224\n0.476\n-0.432\n0.445\n0.454\n-0.704\n0.209\n0.48\n-0.423\n0.185\n0.477\n-0.413\n0.119\n0.474\n-0.377\n-0.009\n0.396\n-0.518\nLLaMA-3.1-8B (0.443)\n-0.125\n0.512\n-0.27\n-0.091\n0.525\n-0.303\n-0.012\n0.574\n-0.246\n0.413\n0.473\n-0.675\n0.023\n0.556\n-0.333\n0.01\n0.549\n-0.347\n0.016\n0.581\n-0.272\n-0.037\n0.429\n-0.407\nGemma-2-9B (0.557)\n-0.006\n0.857\n-0.021\n0.08\n0.857\n-0.078\n0.172\n0.794\n-0.152\n0.338\n0.655\n-0.346\n-0.017\n0.857\n-0.026\n-0.051\n0.842\n-0.031\n-0.1\n0.764\n-0.056\n-0.207\n0.582\n-0.161\nMistral-7B (0.492)\n0.274\n0.685\n-0.222\n0.358\n0.615\n-0.339\n0.442\n0.546\n-0.492\n0.475\n0.518\n-0.557\n-0.155\n0.635\n-0.072\n-0.227\n0.53\n-0.16\n-0.339\n0.365\n-0.324\n-0.4\n0.281\n-0.417\nPhi-4 (0.292)\n0.03\n0.794\n-0.661\n0.031\n0.794\n-0.643\n0.102\n0.729\n-0.192\n0.628\n0.266\n-0.812\n0.03\n0.794\n-0.676\n0.031\n0.794\n-0.652\n0.05\n0.752\n-0.185\n0.107\n0.224\n-0.45\nOlmo-3.1-32B (0.409)\n0.317\n0.6\n-0.366\n0.338\n0.589\n-0.389\n0.491\n0.485\n-0.505\n0.539\n0.447\n-0.448\n-0.084\n0.581\n-0.268\n-0.098\n0.576\n-0.285\n-0.23\n0.407\n-0.395\n-0.263\n0.354\n-0.25\n(c) MedMCQA\nCorrect Endorsement\nIncorrect/Misleading Endorsement\nFirst-Year Medical Student Third-Year Medical Student Chief Medical Resident Board-Certified Physician First-Year Medical Student Third-Year Medical Student Chief Medical Resident Board-Certified Physician\nModel\n∆Acc ↑Rob ↑\n∆Ent ↓\n∆Acc ↑Rob ↑\n∆Ent ↓\n∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑∆Ent ↓\n∆Acc ↑Rob ↑\n∆Ent ↓\n∆Acc ↑Rob ↑\n∆Ent ↓\n∆Acc ↑Rob ↑∆Ent ↓∆Acc ↑Rob ↑\n∆Ent ↓\nReasoning Models\nQwen3-4B-Thinking (0.257)\n0.309\n0.369\n0.345\n0.414\n0.351\n0.256\n0.49\n0.318\n0.17\n0.736\n0.258\n-0.42\n0.22\n0.378\n0.38\n0.141\n0.393\n0.391\n0.099\n0.364\n0.362\n-0.174\n0.261\n-0.089\nDeepSeek-R1 (0.543)\n-0.127\n0.734\n0.137\n-0.049\n0.778\n0.058\n0.116\n0.804\n-0.083\n0.381\n0.614\n-0.621\n-0.019\n0.8\n0.131\n-0.033\n0.797\n0.11\n-0.093\n0.758\n0.109\n-0.356\n0.386\n-0.261\nPhi-4-Reasoning (0.695)\n-0.007\n0.914\n0.009\n0.046\n0.909\n-0.082\n0.147\n0.838\n-0.295\n0.286\n0.712\n-0.667\n-0.013\n0.922\n0.019\n-0.03\n0.9\n0.046\n-0.097\n0.808\n0.046\n-0.379\n0.448\n-0.084\nGemma-3-12B (0.628)\n-0.092\n0.871\n0.059\n0.009\n0.9\n0.022\n-0.009\n0.896\n0.038\n0.269\n0.725\n-0.068\n0.013\n0.914\n0.019\n-0.017\n0.903\n0.038\n-0.009\n0.91\n0.047\n-0.243\n0.614\n0.024\nOlmo-3.1-32B-Think (0.277)\n0.466\n0.355\n-0.011\n0.533\n0.34\n-0.063\n0.617\n0.325\n-0.058\n0.679\n0.255\n-0.657\n-0.074\n0.384\n0.106\n-0.117\n0.371\n0.078\n-0.181\n0.342\n0.087\n-0.185\n0.219\n-0.379\nNon-reasoning Models\nQwen-2.5-14B (0.523)\n0.062\n0.581\n-0.273\n0.167\n0.61\n-0.353\n0.196\n0.601\n-0.392\n0.393\n0.553\n-0.601\n0.181\n0.612\n-0.344\n0.152\n0.616\n-0.315\n0.094\n0.577\n-0.335\n-0.063\n0.473\n-0.39\nLLaMA-3.1-8B (0.313)\n-0.035\n0.238\n-0.416\n-0.001\n0.249\n-0.395\n0.026\n0.214\n-0.389\n0.626\n0.317\n-1.033\n0.042\n0.274\n-0.418\n0.037\n0.269\n-0.409\n0.031\n0.222\n-0.395\n-0.045\n0.313\n-0.586\nGemma-2-9B (0.623)\n-0.083\n0.882\n0.025\n0.01\n0.915\n-0.021\n0.067\n0.896\n-0.051\n0.273\n0.721\n-0.272\n0.024\n0.924\n-0.018\n-0.006\n0.93\n0.009\n-0.038\n0.903\n0.026\n-0.225\n0.618\n-0.031\nMistral-7B (0.529)\n0.176\n0.78\n-0.164\n0.311\n0.682\n-0.306\n0.386\n0.61\n-0.439\n0.458\n0.542\n-0.578\n-0.095\n0.78\n-0.026\n-0.188\n0.632\n-0.093\n-0.295\n0.477\n-0.225\n-0.447\n0.266\n-0.433\nPhi-4 (0.272)\n0.006\n0.888\n-0.607\n0.006\n0.888\n-0.595\n0.041\n0.851\n-0.211\n0.674\n0.268\n-1.009\n0.005\n0.889\n-0.627\n0.005\n0.889\n-0.612\n0.026\n0.852\n-0.203\n0.173\n0.2\n-0.551\nOlmo-3.1-32B (0.43)\n0.26\n0.613\n-0.469\n0.295\n0.599\n-0.501\n0.386\n0.55\n-0.549\n0.525\n0.457\n-0.603\n0.021\n0.614\n-0.347\n-0.009\n0.595\n-0.36\n-0.108\n0.508\n-0.401\n-0.235\n0.351\n-0.346\n(d) MedQA\nTable 1: Per model performance on (a) AQuA-RAT (b) LEXam (c) MedMCQA (d) MedQA. Baseline accuracy is\nreported along with model .\ncan be influenced by factors orthogonal to semantic\ncorrectness. More closely related to our work are\nstudies examining authority bias and sycophancy in\nLLMs (Park et al., 2024; Chen et al., 2024; Wang\net al., 2025; Chen et al., 2025, 2024; Sharma et al.,\n2023; Wei et al., 2023). These works demonstrate\nthat models tend to defer to information attributed\nto authoritative sources or align with user-provided\nopinions, even when such information is incorrect.\n6\nConclusions\nIn this work, we investigate the tendency of LLMs\nto adopt an expert’s endorsement over their inter-\nnal knowledge. We demonstrate that misleading\nsuggestions from high-credibility personas signif-\nicantly degrade model accuracy, overriding the\nmodel’s own correct reasoning abilities.\nCru-\ncially, we find that even reasoning-enhanced mod-\nels are not immune to this bias, showing substan-\ntial susceptibility to expert manipulation despite\ntheir chain-of-thought capabilities. To validate the\nmechanistic basis of this behavior, we extracted a\nsteering vector representing ’expertise’ from the\nmodel’s residual stream. We find that subtract-\ning this vector neutralizes the bias, restoring the\nmodel’s reliance on its own knowledge. Conversely,\ninjecting this vector into low-credibility contexts\namplifies the model’s trust in the endorsement.\nThese findings suggest that current LLMs prior-\nitize source credibility over semantic correctness, a\n"}, {"page": 5, "text": "vulnerability that can be mechanistically isolated\nand controlled.\n7\nLimitations\nWhile our study shows that LLMs are susceptible to\nauthority bias, it is essential to acknowledge several\nlimitations. First, our experiments are constrained\nto smaller open-source models (up to 32B param-\neters); frontier-scale models may exhibit different\npatterns of authority susceptibility. Second, we\nevaluate only four domains (mathematical, legal,\nand medical reasoning); broader domain coverage\nwould strengthen generalization claims. Third, our\nendorsement format is limited to single, explicit an-\nswer statements without variations in phrasing, con-\nfidence levels, or reasoning justification, while in\nreal-world bad endorsements and misinformation\nare more sophisticated. Finally, while our steering\nvector experiments demonstrate that authority bias\ncan be mechanistically reduced, we have not yet\nconducted comprehensive analysis across layers\nor utilized interpretability methods like Sparse Au-\ntoencoders (SAEs) to fully characterize the under-\nlying representations. Future work should address\nthese limitations through larger-scale evaluations,\nricher interaction scenarios, and deeper mechanis-\ntic investigations.\n8\nEthical Considerations\nThis research identifies specific vulnerabilities in\nLLM reasoning that could be exploited for mali-\ncious purposes. By demonstrating that authority\nbias follows a hierarchical pattern, our work re-\nveals which personas (e.g., \"Chief Medical Offi-\ncer,\" \"senior judge\") are most effective at manipu-\nlating model outputs. In adversarial contexts, this\nknowledge could enable bad actors to craft more\neffective social engineering attacks against LLM-\npowered systems. We also demonstrate technique\nthat would allow us to steer a model away from\nhigh expertise bias by altering its residual stream.\nReferences\nMarah Abdin, Sahaj Agarwal, Ahmed Awadallah,\nVidhisha Balachandran, Harkirat Behl, Lingjiao\nChen, Gustavo de Rosa, Suriya Gunasekar, Mo-\njan Javaheripi, Neel Joshi, and 1 others. 2025.\nPhi-4-reasoning technical report.\narXiv preprint\narXiv:2504.21318.\nMarah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien\nBubeck, Ronen Eldan, Suriya Gunasekar, Michael\nHarrison, Russell J Hewett, Mojan Javaheripi, Piero\nKauffmann, and 1 others. 2024. Phi-4 technical re-\nport. arXiv preprint arXiv:2412.08905.\nHaozhe An, Christabel Acquaye, Colin Wang, Zongxia\nLi, and Rachel Rudinger. 2024. Do large language\nmodels discriminate in hiring decisions on the ba-\nsis of race, ethnicity, and gender?\narXiv preprint\narXiv:2406.10486.\nNoel F Ayoub, Karthik Balakrishnan, Marc S Ayoub,\nThomas F Barrett, Abel P David, and Stacey T Gray.\n2024. Inherent bias in large language models: a\nrandom sampling analysis. Mayo Clinic Proceedings:\nDigital Health, 2(2):186–191.\nGuiming Chen, Shunian Chen, Ziche Liu, Feng Jiang,\nand Benyou Wang. 2024. Humans or llms as the\njudge? a study on judgement bias. In Proceedings\nof the 2024 Conference on Empirical Methods in\nNatural Language Processing, pages 8301–8327.\nYanda Chen,\nJoe Benton,\nAnsh Radhakrishnan,\nJonathan Uesato, Carson Denison, John Schulman,\nArushi Somani, Peter Hase, Misha Wagner, Fa-\nbien Roger, and 1 others. 2025. Reasoning models\ndon’t always say what they think. arXiv preprint\narXiv:2505.05410.\nYann Dubois, Balázs Galambosi, Percy Liang, and Tat-\nsunori B Hashimoto. 2024. Length-controlled al-\npacaeval: A simple way to debias automatic evalua-\ntors. arXiv preprint arXiv:2404.04475.\nYu Fan, Jingwei Ni, Jakob Merane, Yang Tian, Yoan\nHermstrüwer, Yinya Huang, Mubashara Akhtar, Eti-\nenne Salimbeni, Florian Geering, Oliver Dreyer,\nand 1 others. 2025.\nLexam: Benchmarking le-\ngal reasoning on 340 law exams.\narXiv preprint\narXiv:2505.12864.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, and 1 others. 2024. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao\nSong, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning.\narXiv preprint\narXiv:2501.12948.\nDongsheng Jiang, Yuchen Liu, Songlin Liu, Jin’e Zhao,\nHao Zhang, Zhen Gao, Xiaopeng Zhang, Jin Li, and\nHongkai Xiong. 2023. From clip to dino: Visual\nencoders shout in multi-modal large language models.\narXiv preprint arXiv:2310.08825.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\n"}, {"page": 6, "text": "Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park,\nZae Myung Kim, and Dongyeop Kang. 2024. Bench-\nmarking cognitive biases in large language models as\nevaluators. In Findings of the Association for Com-\nputational Linguistics: ACL 2024, pages 517–545.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. arXiv preprint arXiv:1705.04146.\nZehao Liu and Xi Lin. 2025. Breaking minds, break-\ning systems: Jailbreaking large language models\nvia human-like psychological manipulation. arXiv\npreprint arXiv:2512.18244.\nTeam Olmo, Allyson Ettinger, Amanda Bertsch, Bailey\nKuehl, David Graham, David Heineman, Dirk Groen-\neveld, Faeze Brahman, Finbarr Timbers, Hamish Ivi-\nson, and 1 others. 2025. Olmo 3. arXiv preprint\narXiv:2512.13961.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on health,\ninference, and learning, pages 248–260. PMLR.\nJunsoo Park, Seungyeon Jwa, Ren Meiying, Daeyoung\nKim, and Sanghyuk Choi. 2024. Offsetbias: Lever-\naging debiased data for tuning evaluators. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2024, pages 1043–1067.\nPouya Pezeshkpour and Estevam Hruschka. 2023.\nLarge language models sensitivity to the order of\noptions in multiple-choice questions, 2023. URL\nhttps://arxiv. org/abs/2308.11483.\nKeita Saito, Akifumi Wachi, Koki Wataoka, and Youhei\nAkimoto. Verbosity bias in preference labeling by\nlarge language models. In NeurIPS 2023 Workshop\non Instruction Tuning and Instruction Following.\nMrinank Sharma, Meg Tong, Tomasz Korbak, David Du-\nvenaud, Amanda Askell, Samuel R Bowman, Newton\nCheng, Esin Durmus, Zac Hatfield-Dodds, Scott R\nJohnston, and 1 others. 2023. Towards understand-\ning sycophancy in language models. arXiv preprint\narXiv:2310.13548.\nLin Shi, Chiyu Ma, Wenhua Liang, Xingjian Diao, We-\nicheng Ma, and Soroush Vosoughi. 2024. Judging\nthe judges: A systematic study of position bias in\nllm-as-a-judge. arXiv preprint arXiv:2406.07791.\nAndreas Stephan, Dawei Zhu, Matthias Aßenmacher,\nXiaoyu Shen, and Benjamin Roth. 2025. From cal-\nculation to adjudication: Examining llm judges on\nmathematical reasoning tasks. In Proceedings of\nthe Fourth Workshop on Generation, Evaluation and\nMetrics (GEM2), pages 759–773.\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya\nPathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,\nTatiana Matejovicova, Alexandre Ramé, Morgane\nRivière, and 1 others. 2025. Gemma 3 technical\nreport. arXiv preprint arXiv:2503.19786.\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, Léonard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ramé, and 1 others. 2024a.\nGemma 2: Improving open language models at a\npractical size. arXiv preprint arXiv:2408.00118.\nQwen Team and 1 others. 2024b. Qwen2 technical\nreport. arXiv preprint arXiv:2407.10671, 2(3).\nAlexander Matt Turner, Lisa Thiergart, Gavin Leech,\nDavid Udell, Juan J Vazquez, Ulisse Mini, and\nMonte MacDiarmid. 2023. Steering language mod-\nels with activation engineering.\narXiv preprint\narXiv:2308.10248.\nPeiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,\nBinghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu,\nTianyu Liu, and 1 others. 2024. Large language mod-\nels are not fair evaluators. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n9440–9450.\nQian Wang, Zhanzhi Lou, Zhenheng Tang, Nuo Chen,\nXuandong Zhao, Wenxuan Zhang, Dawn Song, and\nBingsheng He. 2025.\nAssessing judging bias in\nlarge reasoning models: An empirical study. arXiv\npreprint arXiv:2504.09946.\nJerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and\nQuoc V Le. 2023. Simple synthetic data reduces\nsycophancy in large language models. arXiv preprint\narXiv:2308.03958.\nMinghao Wu and Alham Fikri Aji. 2023. Style over sub-\nstance: Evaluation biases for large language models.”\narxiv. arXiv preprint arXiv:2307.03025.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui,\nBo Zheng,\nBowen Yu,\nChang\nGao, Chengen Huang, Chenxu Lv, and 1 others.\n2025.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388.\nJiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen,\nQihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer,\nChao Huang, Pin-Yu Chen, and 1 others. 2024. Jus-\ntice or prejudice? quantifying biases in llm-as-a-\njudge. arXiv preprint arXiv:2410.02736.\nZheng Zhang, Peilin Zhao, Deheng Ye, and Hao Wang.\n2025. Enhancing jailbreak attacks on llms via per-\nsona prompts. arXiv preprint arXiv:2507.22171.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, and 1 others.\n2023. Judging llm-as-a-judge with mt-bench and\nchatbot arena. Advances in neural information pro-\ncessing systems, 36:46595–46623.\n"}, {"page": 7, "text": "Andy Zou, Long Phan, Sarah Chen, James Campbell,\nPhillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski,\nand 1 others. 2023. Representation engineering: A\ntop-down approach to ai transparency. arXiv preprint\narXiv:2310.01405.\nA\nExample Appendix\nA.1\nFinding steering vector\nWe compiled a dataset of 100 questions from three\ndifferent fields - i) Science/Math Reasoning, ii)\nMedicine, and iii) Law. For each query, we gen-\nerated four response variations corresponding to\nour expertise hierarchy controlled for ground truth\nie, all four responses were factually correct, differ-\ning only in the linguistic patterns characteristic of\neach expertise level (see Appendix for examples).\nWe computed the steering vector as the mean dif-\nference in residual stream activations between the\nhighest and lowest expertise personas. Our exper-\niments reveal that subtracting this vector reduces\nthe model’s bias toward authoritative endorsements,\nwhereas adding it significantly amplifies the per-\nsuasive power of low-credibility personas\nA.2\nSteering the model away from bias\nWe demonstrate that the concept of expertise bias is\nencoded within the model’s internal representation\nand that we can steer the model away from such a\nbias by reducing the steering vector for ’high exper-\ntise’ from its residual layers with minimal impact\non baseline accuracy. We also empirically see that\nthe biggest effect happens within the middle layers.\nFigure 2: Model accuracy for incorrect endorsement\nFigure 3: Model accuracy for incorrect endorsement\nafter steering away from expertise persona.\n"}]}