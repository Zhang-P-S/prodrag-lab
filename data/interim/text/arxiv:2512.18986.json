{"doc_id": "arxiv:2512.18986", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.18986.pdf", "meta": {"doc_id": "arxiv:2512.18986", "source": "arxiv", "arxiv_id": "2512.18986", "title": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression", "authors": ["Kun Zhao", "Siyuan Dai", "Yingying Zhang", "Guodong Liu", "Pengfei Gu", "Chenghua Lin", "Paul M. Thompson", "Alex Leow", "Heng Huang", "Lifang He", "Liang Zhan", "Haoteng Tang"], "published": "2025-12-22T02:54:10Z", "updated": "2025-12-22T02:54:10Z", "summary": "Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.18986v1", "url_pdf": "https://arxiv.org/pdf/2512.18986.pdf", "meta_path": "data/raw/arxiv/meta/2512.18986.json", "sha256": "51d2baf478550ce1ce62bcd166cfeb6956594729b6d133c658f922fb281457bd", "status": "ok", "fetched_at": "2026-02-18T02:23:59.261499+00:00"}, "pages": [{"page": 1, "text": "R-GenIMA: Integrating Neuroimaging and\nGenetics with Interpretable Multimodal AI for\nAlzheimer’s Disease Progression\nKun Zhao1, Siyuan Dai1, Yingying Zhang2, Guodong Liu3,\nPengfei Gu2, Chenghua Lin4, Paul M. Thompson5, Alex Leow6,\nHeng Huang7, Lifang He8, Liang Zhan1*, Haoteng Tang2*, for\nthe Alzheimer’s Disease Neuroimaging Initiative (ADNI) Project9\n1*Electrical & Computer Engineering, University of Pittsburgh, 4200\nFifth Avenue, Pittsburgh, 15260, PA, USA.\n2*Computer Science, University of Texas Rio Grande Valley, 1201 West\nUniversity Drive, Edinburg, 78539, TX, USA.\n3Eli and Lilly company, 893 S Delaware St, Indianapolis, 46285, IN, USA.\n4Computer Science, The University of Manchester, Kilburn Building,\nOxford Road, Manchester, M13 9PL, UK.\n5Imaging Genetics Center, University of Southern California, 4676\nAdmiralty Way, Marina del Rey, 90292, CA, USA.\n6Psychiatry, University of Illinois Chicago, 1601 W. Taylor St., Chicago,\n60612, IL, USA.\n7Computer Science, University of Maryland College Park, 8125 Paint\nBranch Drive, College Park, 20742, MD, USA.\n8Computer Science & Engineering, Lehigh University, 113 Research\nDrive, Bethlehem, 18015, PA, USA.\n*Corresponding author(s). E-mail(s): liang.zhan@pitt.edu;\nhaoteng.tang@utrgv.edu;\nContributing authors: kun.zhao@pitt.edu; siyuan.dai@pitt.edu;\nyingying.zhang01@utrgv.edu; Guodong.liu@lilly.com;\npengfei.gu01@utrgv.edu; chenghua.lin@manchester.ac.uk;\npthomp@usc.edu; alexfeuillet@gmail.com; heng@umd.edu;\nlih319@lehigh.edu;\n1\narXiv:2512.18986v1  [cs.LG]  22 Dec 2025\n"}, {"page": 2, "text": "Abstract\nEarly detection of Alzheimer’s disease (AD) requires models capable of integrat-\ning macro-scale neuroanatomical alterations with micro-scale genetic suscepti-\nbility, yet existing multimodal approaches struggle to align these heterogeneous\nsignals. We introduce R-GenIMA, an interpretable multimodal large language\nmodel that couples a novel ROI-wise vision transformer with genetic prompting\nto jointly model structural MRI and single nucleotide polymorphisms (SNPs)\nvariations. By representing each anatomically parcellated brain region as a visual\ntoken and encoding SNP profiles as structured text, the framework enables\ncross-modal attention that links regional atrophy patterns to underlying genetic\nfactors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art per-\nformance in four-way classification across normal cognition (NC), subjective\nmemory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond\npredictive accuracy, the model yields biologically meaningful explanations by\nidentifying stage-specific brain regions and gene signatures, as well as coherent\nROI–Gene association patterns across the disease continuum. Attention-based\nattribution revealed genes consistently enriched for established GWAS-supported\nAD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved\nneuroanatomical signatures identified shared vulnerability hubs across disease\nstages alongside stage-specific patterns: striatal involvement in subjective decline,\nfrontotemporal engagement during prodromal impairment, and consolidated mul-\ntimodal network disruption in AD. These results demonstrate that interpretable\nmultimodal AI can synthesize imaging and genetics to reveal mechanistic insights,\nproviding a foundation for clinically deployable tools that enable earlier risk\nstratification and inform precision therapeutic strategies in Alzheimer’s disease.\nKeywords: Prodromal Alzheimer’s disease, Neuroimaging–genetics integration,\nROI-wise vision transformer, Interpretable multimodal large language models, SNPs,\nT1-weighted MRI\n1 Introduction\nNeurodegenerative disorders such as Alzheimer’s disease (AD) pose an escalating\nchallenge to global health, with prevalence rising rapidly alongside population aging\nand no curative therapies currently available [1, 2]. Despite decades of research, the\nbiological processes that initiate cognitive decline remain incompletely understood,\nparticularly during the earliest preclinical phases of the disease. AD unfolds along\na prolonged continuum characterized by subtle yet systematic disturbances in brain\nstructure, neuronal connectivity, and molecular homeostasis that emerge years before\novert symptoms. Across this trajectory—from normal cognition (NC) through subjec-\ntive memory concerns (SMC) and mild cognitive impairment (MCI)—neural systems\nundergo progressive reconfiguration driven by amyloid accumulation, tau propagation,\nsynaptic dysfunction, and neuroinflammatory processes [3]. Mounting evidence indi-\ncates that these early physiological and circuit-level disruptions can be detected using\nsensitive neuroimaging markers and computational models, offering a crucial oppor-\ntunity to identify individuals at heightened risk before irreversible neurodegeneration\n2\n"}, {"page": 3, "text": "occurs [4]. Consequently, characterizing the brain alterations that precede clinical\ndementia is essential not only for improving early diagnosis and risk stratification,\nbut also for advancing mechanistic insight into AD progression and enabling timely,\nprecision therapeutic interventions.\nStructural MRI has been widely used to characterize AD–related neurodegener-\nation, revealing hallmark patterns of hippocampal atrophy, cortical thinning, and\nlarge-scale network disruption that track disease severity and clinical progression [5, 6].\nAs a macro-scale modality, T1-weighted imaging captures the anatomical consequences\nof accumulating pathology, yet these morphological alterations often emerge only after\nsubstantial molecular injury has occurred. In the earliest stages—particularly the\nsubtle transition from normal cognition to subjective memory concerns—structural\nchanges may be minimal, highly variable across individuals, and confounded by nor-\nmative aging, limiting the sensitivity of MRI-only approaches for early detection\n[7, 8]. Conversely, genetic variation reflects micro-scale, lifelong biological suscepti-\nbility, with single nucleotide polymorphisms (SNPs) influencing amyloid processing,\ntau propagation, synaptic maintenance, lipid metabolism, and neuroinflammatory\npathways that precede and ultimately shape regional brain vulnerability [9, 10]. How-\never, SNPs models may provide weak disease predictive power because molecular risk\ndoes not directly map to clinical symptoms or macro-scale atrophy patterns. These\ncomplementary limitations underscore a central challenge in AD research: macro-\nscale imaging captures downstream structural consequences, while micro-scale genetics\nencodes upstream drivers of vulnerability, yet neither modality alone is sufficient to\nfully capture early disease processes. Integrating T1 MRI with genetic SNPs therefore\noffers a crucial opportunity to bridge molecular susceptibility and structural expres-\nsion, enhance early-stage detection, and reveal biologically grounded pathways through\nwhich genetic risk becomes instantiated in the human brain. Despite growing interest\nin imaging–genetic studies of Alzheimer’s disease, several major challenges continue\nto limit progress toward biologically grounded and clinically actionable models. Struc-\ntural MRI and genetic data operate at fundamentally different biological scales: MRI\ncaptures macro-scale anatomical consequences of neurodegeneration, whereas SNPs\nreflect micro-scale molecular susceptibility that precedes structural injury. Traditional\nstatistical approaches—such as sparse regression [6, 11, 12], canonical correlation anal-\nysis [13–15], and GWAS-based association [16–23]—are limited in their ability to model\nhigh-dimensional SNPs, nonlinear gene–brain interactions, and subtle cross-modal cor-\nrespondences [24]. As a result, they may yield suboptimal or unstable estimates of\nhow genetic risk manifests in regional brain vulnerability.\nRecent deep learning and transformer-based multimodal architectures[25–28] have\nsubstantially improved the capacity to integrate heterogeneous biomedical data. How-\never, traditional multimodal deep learning approaches employ independent encoders\nfor MRI and genetic data (e.g., 3D CNNs for neuroimaging and MLPs for SNPs), fol-\nlowed by a simple late-fusion step in which high-level features are concatenated. This\nlate-fusion paradigm introduces a fundamental bottleneck: concatenation collapses the\ninherent spatial structure of neuroimaging representations, preventing the model from\naligning localized anatomical alterations—such as hippocampal or temporoparietal\n3\n"}, {"page": 4, "text": "degeneration—with their potential genetic determinants[29]. As a result, the fused rep-\nresentation becomes biologically ambiguous, limiting interpretability and weakening\nthe ability to infer fine-grained gene–brain associations. Furthermore, most exist-\ning deep learning approaches address only coarse diagnostic categories, rather than\nmodeling the continuous trajectory of early disease states in which genetic and struc-\ntural signals evolve jointly [26–28]. Collectively, these limitations highlight the need\nfor a unified, scalable, and interpretable framework capable of bridging micro-scale\nmolecular risk with macro-scale neuroanatomical expression across the AD trajectory.\nMultimodal large language models (MLLMs) offer a promising direction for\naddressing these challenges by enabling joint representation learning across hetero-\ngeneous modalities—including structural MRI and SNP sequences—within a single\npretrained, high-capacity architecture. Unlike conventional deep learning models,\nMLLMs treat imaging features and genetic variants as structured “tokens,” allow-\ning the model to flexibly encode nonlinear cross-modal interactions and to capture\nlong-range dependencies intrinsic to genomic data [30, 31]. Despite recent progress\nin medical MLLMs, current state-of-the-art systems (e.g., Med-LLaVA [32], RadFM\n[33]) remain fundamentally limited for neuroimaging–genetics integration because\nthey are architecturally constrained to 2-dimensional visual backbones. Even when\nadapted to accommodate 3D volumetric inputs, these models typically rely on aggres-\nsive global pooling to control token complexity, which collapses spatial specificity\nand suppresses subtle regional variations [34]. As a result, fine-grained neuroanatom-\nical signatures—especially those localized to disease-sensitive cortical or subcorti-\ncal regions—are diluted, hindering any attempt to resolve biologically meaningful\ngene–brain associations. This limitation motivates the need for a more anatomically\nstructured visual representation so that regional structural variations can be preserved\nand biologically meaningful gene–brain interactions can be captured by the MLLM.\nTo address these limitations, we introduce a new ROI-wise vision encoder—Region-\nwise Vision Transformer (RiT)—together with a multimodal framework, R-GenIMA\n(ROI–Genome fused Interpretable Multimodal AI), that integrates anatomically\ngrounded visual tokens with genetic prompting. Instead of operating on whole-brain\nMRI volumes, each T1-weighted scan is parcellated into anatomically defined 3D\nregion-of-interests (ROIs) using a standard atlas, and each ROI is represented as an\nindividual visual token. This preserves regional structural detail and enables the model\nto attend to localized patterns of neurodegeneration that are relevant to disease stag-\ning. In parallel, SNP sequences are serialized into textual descriptions and encoded as\ndiscrete genetic tokens, allowing the LLM to exploit its pre-trained biomedical priors\nand capture long-range, nonlinear dependencies across variants. By projecting both\nROI tokens and SNP tokens into a shared semantic embedding space, R-GenIMA\nenables cross-modal self-attention that explicitly models the relationships between\ngenetic variation and spatially specific brain structures. This design simultaneously\nbridges the semantic gap between genotype and phenotype and retains the spa-\ntial granularity essential for interpreting gene–brain interactions in early Alzheimer’s\ndisease.\n4\n"}, {"page": 5, "text": "We evaluate R-GenIMA on the Alzheimer’s Disease Neuroimaging Initiative\n(ADNI) dataset[35, 36], demonstrating state-of-the-art performance in four-way clas-\nsification across NC, SMC, MCI, and AD. To the best of our knowledge, this\nrepresents the first unified imaging–genetic framework capable of fine-grained disease-\nstage prediction while simultaneously providing mechanistically interpretable outputs\nat multiple biological scales. Specifically, the model identifies: (i) stage-specific\nneuroanatomical signatures that evolve systematically across the AD continuum,\n(ii) disease-associated genetic markers enriched for established risk loci, and (iii)\nreproducible ROI–gene associations that reveal coordinated patterns of molecular\nsusceptibility and regional vulnerability. Together, these findings demonstrate that\nmultimodal large language models can bridge micro-scale genetic variation with macro-\nscale structural brain alterations in a biologically principled and clinically interpretable\nmanner, offering new avenues for early detection, risk stratification, and mechanistic\ninvestigation of Alzheimer’s disease.\n2 Results\nTable 1 Overall classification performance across the\nthree data configurations. We report both Accuracy\nand Macro-F1 scores. Methods marked with † denote\nsimple feature-concatenation baselines, whereas\nmethods marked with ‡ refer to cross-modal\nattention–based fusion models (see Section 4.5.1).\nMethod\nAccuracy\nMacro-F1\nSNPs-only\nMLP\n38.82%\n26.44%\nBERT[37]\n67.08%\n43.36%\nLlama[38]\n81.08%\n65.93%\nQwen[39]\n80.84%\n66.14%\nImage-Gene\nBERT[37]+Med3D[40]†\n94.59%\n87.16%\nBERT[37]+Med3D[40]‡\n90.42%\n82.45%\nBERT[37]+RiT†\n95.09%\n89.97%\nBERT[37]+RiT‡\n92.87%\n85.67%\nLlama[38]+Med3D[40]\n93.36%\n85.71%\nQwen[39]+Med3D[40]\n93.61%\n85.87%\nR-GenIMA (Llama)\n95.09%\n90.37%\nR-GenIMA (Qwen)\n95.58%\n90.42%\nMixture Dataset\nLlama[38]+Med3D[40]\n97.54%\n92.20%\nQwen[39]+Med3D[40]\n96.80%\n90.23%\nR-GenIMA (Llama)\n98.03%\n97.90%\nR-GenIMA (Qwen)\n99.01%\n98.50%\n5\n"}, {"page": 6, "text": "2.1 Multimodal Classification Performance Across\nAlzheimer’s Disease Stages\nA primary objective of this study is to assess the effectiveness of our proposed mul-\ntimodal framework for fine-grained Alzheimer’s disease stage classification within the\nADNI cohort. Specifically, we evaluated: (i) a SNPs-only setting to establish the\ndiscriminative capacity of genomic variation; (ii) a paired Image–Gene setting to deter-\nmine how structural MRI enhances disease-stage representation; and (iii) a Mixture\nDataset setting that augments paired samples with additional SNPs-only subjects\nto examine whether heterogeneous training data can further improve multimodal\nlearning. Tables 1 summarizes the full set of experiments and corresponding findings.\nThe results in Table 1 first highlight the behavior of genetic predictors in the SNPs-\nonly setting. Large language models such as Llama and Qwen achieve accuracies of\n81.08% and 80.84%, substantially outperforming the two baseline architectures—MLP\n(38.82%) and BERT (67.08%)—demonstrating that LLMs provide a markedly more\nexpressive framework for modeling high-dimensional genomic variation. Despite this\nadvantage, SNPs-only prediction remains moderate in absolute terms, reflecting a\nfundamental limitation: genetic variation encodes latent susceptibility rather than\nexpressed pathology, and therefore provides an incomplete view of disease-stage\ndifferentiation when used in isolation. To address this limitation, we introduced struc-\ntural MRI as a complementary modality and evaluated multimodal imaging–genetics\nmodels. Across architectures, integrating imaging features with SNPs embeddings\nconsistently improved performance over the SNPs-only models, confirming that macro-\nscopic neuroanatomical signatures supply critical downstream information that is not\npresent in genomic data alone. For example, the Qwen+Med3D fusion model achieves\n93.61% accuracy, a substantial increase over the 80.84% achieved by Qwen on genetics\nalone.\nBuilding on this, we further incorporated our proposed ROI-based visual represen-\ntation, RiT(ROI-wised Transformer), designed to enhance the multimodal integration\nmechanism. Replacing whole-brain volumes with 3D ROI patches yielded additional\nperformance gains across all fusion architectures. For example, the R-GenIMA (Qwen)\nand R-GenIMA (Llama) models reach 95.58% and 95.09% accuracies, consistently\noutperforming their whole-brain counterparts. This improvement suggests that region-\nlevel decomposition provides a more discriminative and biologically aligned encoding of\nstructural heterogeneity, enabling the model to more effectively map imaging signals to\nunderlying genetic susceptibility profiles. Collectively, these results demonstrate that\nwhile genetic predictors alone carry meaningful disease-related information, the com-\nbination of macro-scale imaging and micro-scale genomic features—especially under\nROI-guided encoding—offers a more powerful and fine-grained representation of early\nAlzheimer’s disease progression.\nWe further examined whether enlarging the genetic sample space could enhance\nmultimodal learning by training on a Mixture Dataset that augments paired imag-\ning–genetic examples with additional SNPs-only subjects. This heterogeneous training\nregime yielded the strongest performance gains observed in our study. The qwen+3droi\nmodel reached an accuracy of 99.01% and a Macro-F1 of 98.50%, substantially exceed-\ning its performance on the paired Image–Gene dataset (95.58%). This improvement\n6\n"}, {"page": 7, "text": "suggests that additional unimodal genetic data refine the structure of the genomic\nembedding space and strengthen its alignment with imaging-derived features, even\nthough these added subjects lack MRI scans. In effect, the model leverages the broader\ngenetic distribution to stabilize disease-stage boundaries and improve cross-modal rep-\nresentation learning, enabling near-perfect discrimination across AD, MCI, SMC, and\nNC. These findings indicate that multimodal imaging–genetics models can benefit\nmeaningfully from heterogeneous datasets and do not require complete modality pairs\nto achieve state-of-the-art performance.\nTo further characterize the model’s behavior across disease stages, we additionally\nreport class-specific Precision, Recall, F1-score, and Specificity for NC (see Table 2),\nSMC (see Table 3), MCI (see Table 4) and AD (see Table 5). Multi-class neurodegener-\nation classification is intrinsically imbalanced and clinically heterogeneous; therefore,\nper-class evaluation is essential for determining whether the model maintains consis-\ntent performance across heterogeneous subgroups. Overall, the model demonstrates\nstable predictive performance across classes, indicating that it does not rely dispro-\nportionately on any single category. Our results reveal clear performance differences\nacross disease stages: predictions for AD and NC achieve higher precision and speci-\nficity, reflecting their more distinct imaging signatures, whereas SMC and MCI exhibit\ncomparatively lower recall, consistent with their subtle phenotypes.\nTable 2 NC classification performance across different data\nconfigurations. We report Precision, Recall, Macro-F1, and Specificity\nscores. Methods marked with † denote simple feature-concatenation\nbaselines, while those marked with ‡ indicate cross-modal\nattention–based fusion models (see Section 4.5.1).\nMethod\nP\nR\nF1\nSpecificity\nSNPs-only\nMLP\n29.90%\n25.22%\n27.38%\n76.71%\nBERT[37]\n53.33%\n76.52%\n62.86%\n73.63%\nLlama[38]\n84.50%\n86.51%\n85.49%\n92.88%\nQwen[39]\n86.89%\n84.13%\n85.48%\n94.31%\nImage-Gene\nBERT[37]+Med3D[40]†\n96.83%\n96.83%\n96.83%\n98.58%\nBERT[37]+Med3D[40]‡\n91.41%\n92.86%\n92.13%\n96.09%\nBERT[37]+RiT†\n95.35%\n97.62%\n96.47%\n97.86%\nBERT[37]+RiT‡\n94.49%\n95.24%\n94.86%\n97.51%\nLlama[38]+Med3D[40]\n94.74%\n93.91%\n94.32%\n97.94%\nQwen[39]+Med3D[40]\n94.74%\n93.91%\n94.32%\n97.94%\nR-GenIMA (Llama)\n96.75%\n94.44%\n95.58%\n98.85%\nR-GenIMA (Qwen)\n96.85%\n97.62%\n97.23%\n98.58%\nMixture Dataset\nLlama[38]+Med3D[40]\n96.64%\n100%\n98.29%\n98.63%\nQwen[39]+Med3D[40]\n99.13%\n99.13%\n99.13%\n99.65%\nR-GenIMA (Llama)\n96.58%\n98.26%\n97.41%\n98.63%\nR-GenIMA (Qwen)\n98.28%\n99.13%\n98.70%\n99.31%\n7\n"}, {"page": 8, "text": "Table 3 SMC classification performance across different data\nconfigurations. We report Precision, Recall, Macro-F1, and Specificity\nscores. Methods marked with † denote simple feature-concatenation\nbaselines, while those marked with ‡ indicate cross-modal\nattention–based fusion models (see Section 4.5.1).\nMethod\nP\nR\nF1\nSpecificity\nSNPs-only\nMLP\n7.69%\n11.76%\n9.30%\n93.84%\nBERT[37]\n100%\n5.88%\n11.11%\n100%\nLlama[38]\n13.64%\n50.00%\n21.43%\n95.26%\nQwen[39]\n14.81%\n66.67%\n24.24%\n94.23%\nImage-Gene\nBERT[37]+Med3D[40]†\n66.67%\n66.67%\n66.67%\n99.50%\nBERT[37]+Med3D[40]‡\n57.14%\n66.67%\n61.54%\n99.25%\nBERT[37]+RiT†\n71.43%\n83.33%\n76.92%\n99.50%\nBERT[37]+RiT‡\n66.67%\n66.67%\n66.67%\n99.50%\nLlama[38]+Med3D[40]\n100%\n47.06%\n64.00%\n99.99%\nQwen[39]+Med3D[40]\n100%\n47.06%\n64.00%\n99.99%\nR-GenIMA (Llama)\n71.43%\n83.33%\n76.92%\n99.50%\nR-GenIMA (Qwen)\n71.43%\n83.33%\n76.92%\n99.50%\nMixture Dataset\nLlama[38]+Med3D[40]\n100%\n58.82%\n74.07%\n99.99%\nQwen[39]+Med3D[40]\n90.91%\n58.82%\n71.43%\n99.74%\nR-GenIMA (Llama)\n100%\n94.12%\n96.97%\n99.99%\nR-GenIMA (Qwen)\n100%\n94.12%\n96.97%\n99.99%\n2.2 Model-Prioritized Genes Show Significant Enrichment for\nEstablished AD Risk Loci\nWe evaluated whether the genes prioritized by the model’s attention rollout–based\nattribution [41] reflect biologically meaningful AD signals rather than arbitrary inter-\nnal weights. Because our framework operates on a predefined panel of 105 candidate\ngenes, we used this same panel as the background gene universe to ensure a model-\nconsistent enrichment analysis. Within this universe, 45 genes have been previously\nimplicated in AD by genome-wide association studies (GWAS)1 based on publicly\navailable summary statistics retrieved from the NHGRI–EBI GWAS Catalog2, pro-\nviding a biologically grounded reference set against which to assess enrichment. To\nderive a robust set of salient genes from the model, we ranked genes by attention-based\nimportance and applied a bootstrap stability procedure involving 1,000 resampling\niterations (1,000 samples per iteration), retaining the top 45 genes in each run. Genes\nappearing in more than 50% of the iterations were designated as stable high-attention\ngenes, a criterion designed to minimize spurious selections driven by sampling noise\nor idiosyncratic attention fluctuations. This procedure yielded nine consistently pri-\noritized genes—RBFOX1, TEAM, CHAT, IGH, APOE, BIN1, CLU, CNTNAP2, and\nNECTIN2—all of which have prior evidence linking them to AD-related pathways.\n1Defined as loci reported with genome-wide significance (P < 5 × 10−8).\n2Summary statistics downloaded from the NHGRI–EBI GWAS Catalog (https://www.ebi.ac.uk/gwas/)\n[42] on 10/02/2025.\n8\n"}, {"page": 9, "text": "Table 4 MCI classification performance across different data\nconfigurations. We report Precision, Recall, Macro-F1, and Specificity\nscores. Methods marked with † denote simple feature-concatenation\nbaselines, while those marked with ‡ indicate cross-modal\nattention–based fusion models (see Section 4.5.1).\nMethod\nP\nR\nF1\nSpecificity\nSNPs-only\nMLP\n57.21%\n53.12%\n55.09%\n51.36%\nBERT[37]\n79.37%\n79.02%\n79.19%\n74.86%\nLlama[38]\n89.34%\n81.11%\n85.02%\n88.95%\nQwen[39]\n89.50%\n82.49%\n85.85%\n88.95%\nImage-Gene\nBERT[37]+Med3D[40]†\n93.75%\n96.77%\n95.24%\n92.63%\nBERT[37]+Med3D[40]‡\n92.13%\n91.71%\n91.92%\n91.05%\nBERT[37]+RiT†\n96.73%\n95.39%\n96.06%\n96.32%\nBERT[37]+RiT‡\n94.42%\n93.55%\n93.98%\n93.68%\nLlama[38]+Med3D[40]\n93.19%\n97.77%\n95.42%\n91.25%\nQwen[39]+Med3D[40]\n93.99%\n97.77%\n95.84%\n92.34%\nR-GenIMA (Llama)\n95.43%\n96.31%\n95.87%\n94.74%\nR-GenIMA (Qwen)\n96.31%\n96.31%\n96.31%\n95.79%\nMixture Dataset\nLlama[38]+Med3D[40]\n97.38%\n99.55%\n98.45%\n96.72%\nQwen[39]+Med3D[40]\n96.96%\n95.55%\n98.24%\n96.17%\nR-GenIMA (Llama)\n98.65%\n97.77%\n98.21%\n98.36%\nR-GenIMA (Qwen)\n99.55%\n99.11%\n99.33%\n99.45%\nTo quantify whether these model-prioritized genes were disproportionately\nenriched for established AD risk loci, we constructed a 2 × 2 contingency table within\nthe same 105-gene universe and performed a one-sided Fisher’s Exact Test (“greater”),\nan approach appropriate for small sample sizes and targeted enrichment hypotheses\n[43]. The analysis yielded a statistically significant enrichment (P = 0.027; odds ratio\n= 5.58). The P-value indicates that, under the null hypothesis that the model’s priori-\ntization is unrelated to GWAS evidence, an overlap as large as the one observed would\nbe expected with probability only ∼2.7%. The odds ratio provides a complementary\neffect-size interpretation, showing that attention-selected genes are approximately 5.6\ntimes more likely to be GWAS-supported AD genes than non-selected genes within the\nsame feature space. Because the enrichment is tested against the model’s restricted\ngene universe rather than the whole genome, the analysis is conservative with respect\nto genome-wide claims; nevertheless, the results demonstrate that the model’s inter-\nnal attribution aligns strongly with established AD genetics, supporting the biological\nvalidity and interpretability of our multimodal imaging–genetics framework.\nThe enrichment serves as an external validation step, demonstrating that the genes\nhighlighted by the model coincide with independently established GWAS risk loci at a\nrate significantly above chance. This provides a principled and quantitative indicator\nof interpretability quality for the attention-based gene ranking within our multimodal\nAD prediction framework.\n9\n"}, {"page": 10, "text": "Table 5 AD classification performance across different data\nconfigurations. We report Precision, Recall, Macro-F1, and Specificity\nscores. Methods marked with † denote simple feature-concatenation\nbaselines, while those marked with ‡ indicate cross-modal\nattention–based fusion models (see Section 4.5.1).\nMethod\nP\nR\nF1\nSpecificity\nSNPs-only\nMLP\n10.53%\n15.69%\n12.60%\n80.89%\nBERT[37]\n38.89%\n13.73%\n20.29%\n96.91%\nLlama[38]\n71.19%\n72.41%\n71.79%\n95.13%\nQwen[39]\n68.97%\n68.97%\n68.97%\n94.84%\nImage-Gene\nBERT[37]+Med3D[40]†\n96.08%\n84.48%\n89.91%\n99.43%\nBERT[37]+Med3D[40]‡\n85.71%\n82.76%\n84.21%\n97.71%\nBERT[37]+RiT†\n91.23%\n89.66%\n90.43%\n98.57%\nBERT[37]+RiT‡\n86.44%\n87.93%\n87.18%\n97.71%\nLlama[38]+Med3D[40]\n90.00%\n88.24%\n89.11%\n98.59%\nQwen[39]+Med3D[40]\n88.46%\n90.20%\n89.32%\n98.31%\nR-GenIMA (Llama)\n93.10%\n93.10%\n93.10%\n98.85%\nR-GenIMA (Qwen)\n92.86%\n89.66%\n91.23%\n98.85%\nMixture Dataset\nLlama[38]+Med3D[40]\n100%\n96.08%\n98.00%\n99.99%\nQwen[39]+Med3D[40]\n92.16%\n92.16%\n92.16%\n98.87%\nR-GenIMA (Llama)\n98.08%\n100%\n99.03%\n99.71%\nR-GenIMA (Qwen)\n98.08%\n100%\n99.03%\n99.71%\n2.3 Bootstrap-Validated Brain Regions Reveal Stage-Specific\nVulnerability Patterns\nWe next applied attention-rollout analysis combined with permutation testing to char-\nacterize the neuroanatomical substrates most strongly implicated across disease stages.\nAcross 1,000 bootstrap iterations, we quantified the reproducibility of regional impor-\ntance and retained only those ROIs that appeared in more than 50This procedure\nyielded a compact set of stage-specific loci: six ROIs with consistent relevance in SMC\nand MCI, and five ROIs prominently associated with AD. The resulting saliency maps,\nas shown in Figure 1, reveal a structured pattern of shared and divergent neuroanatom-\nical involvement along the Alzheimer’s disease continuum. Across all three diagnostic\ngroups, the model consistently highlighted a common set of regions—left cerebellar\ncortex, right insula, left thalamus, and right transverse temporal cortex—suggesting\na core vulnerability architecture detectable even at the earliest symptomatic stages.\nBeyond this shared foundation, AD and SMC jointly highlighted the left caudate,\nwhereas SMC alone recruited the left putamen, indicating selective engagement of\nstriatal circuits during subjective decline. In contrast, MCI exhibited two additional\nstage-specific cortical loci—the right temporal pole and right frontal pole—consistent\nwith emerging frontotemporal network disruption during prodromal impairment [44–\n46]. Together, these stage-resolved signatures illustrate a progressive reorganization\nfrom focal cortical perturbations in SMC, to broader frontostriatal involvement in\nMCI, and consolidated multimodal network disruption in AD.\n10\n"}, {"page": 11, "text": "SMC\nMCI\nAD\nFig. 1 Identified brain ROIs associated with disease stages. SMC associated brain ROIs are\nLeft-Cerebellum-Cortex, Left-Thalamus, ctx-rh-insula, Left-Caudate, Left-Putamen and ctx-rh-\ntransversetemporal. MCI associated brain ROIs are Left-Cerebellum-Cortex, ctx-rh-insula, ctx-rh-\ntransversetemporal, ctx-rh-temporalpole, ctx-rh-frontalpole and Left-Thalamus. AD associated brain\nROIs are Left-Cerebellum-Cortex, ctx-rh-insula, Left-Thalamus, ctx-rh-transversetemporal and Left-\nCaudate.\n2.4 Reproducible ROI–Gene Associations Across Disease\nProgression\nROI–gene associations are visualized using a Manhattan-style plot in which ROIs are\narranged as contiguous blocks along the x-axis, and each point represents a ROI–gene\ninteraction, colored according to its ROI of origin. For each ROI, the two most sta-\nble gene associations—as determined by the bootstrap-derived stability score—are\nannotated. Figure 2 illustrate the resulting association landscapes for the AD, MCI,\nand SMC cohorts, respectively. To facilitate clearer interpretation, the visualization\nis restricted to the disease stage-specific ROIs identified in Section 2.3. This repre-\nsentation provides an intuitive summary of the multiscale organization of ROI–gene\nrelationships and highlights distinct, stage-dependent peaks of reproducible genetic\ninvolvement.\nLeft-Cerebellum-Cortex\nctx-rh-insula\nLeft-Thalamus\nctx-rh-transversetemporal\nLeft-Caudate\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\nStability Score\nRBFOX1\nADAM10\nRBFOX1\nCNTNAP2\nRBFOX1\nAPP\nRBFOX1\nDAB1\nRBFOX1\nAPP\nLeft-Cerebellum-Cortex\nctx-rh-insula\nLeft-Thalamus\nctx-rh-transversetemporal\nctx-rh-frontalpole\nctx-rh-temporalpole\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\nStability Score\nTOMM40\nRBFOX1\nRBFOX1\nRELN\nRBFOX1\nAPP\nRBFOX1\nJCAD\nRBFOX1\nJCAD\nRBFOX1\nAPP\nLeft-Cerebellum-Cortex\nctx-rh-insula\nLeft-Thalamus\nctx-rh-transversetemporal\nLeft-Putamen\nLeft-Caudate\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\nStability Score\nRBFOX1\nTEAM\nRBFOX1\nTOMM40\nRBFOX1\nTEAM\nRBFOX1\nTOMM40\nRBFOX1\nBIN1\nRBFOX1\nTOMM40\nSMC\nMCI\nAD\nFig. 2 Identified ROI–Gene association patterns in the SMC, MCI, and AD cohorts. For each cohort,\nthe two genes with the highest stability scores per ROI are annotated.\n11\n"}, {"page": 12, "text": "3 Discussion\n3.1 ROI-Level Decomposition and Semantic Alignment Enable\nEffective Multimodal Integration\nThe superior performance of our multimodal framework, particularly the R-GenIMA\n(Qwen) variant, reflects its ability to overcome two longstanding challenges in imag-\ning–genetics modeling for early Alzheimer’s disease: the loss of fine-scale neuroanatom-\nical information in conventional 3D imaging pipelines and the representational\nmismatch between genetic and structural modalities.\nFirst, the ROI-patched strategy offers a principled solution to the resolu-\ntion–context trade-off inherent to whole-brain 3D CNNs. Standard volumetric\narchitectures require substantial spatial downsampling to accommodate memory\nconstraints, which can obscure subtle, spatially localized morphological deviations\ncharacteristic of prodromal AD. By decomposing T1 MRI into 109 high-resolution\nROI patches (963 voxels each), our framework preserves regional detail while enabling\nthe Vision Transformer to model distributed inter-regional relationships. This hier-\narchical representation retains local structural fidelity yet still captures global brain\norganization, helping explain the consistent performance improvements over whole-\nbrain approaches in the Image–Gene setting (e.g., 95.58% vs. 93.61% for R-GenIMA\n(Qwen) vs. Qwen+Med3D).\nSecond, the MLLM architecture mitigates the semantic and statistical mis-\nmatch that complicates traditional fusion approaches. Conventional methods typically\nemploy late or shallow fusion, treating genetic and imaging features as independent\nnumerical descriptors, thereby limiting their ability to capture non-linear geno-\ntype–phenotype interactions. In contrast, our framework embeds SNPs and ROI\npatches into a unified tokenized representation, enabling cross-modal attention mecha-\nnisms to learn how genetic susceptibility patterns modulate regional brain alterations.\nThe marked performance gap between LLM-based genetic encoders (≈81% accuracy)\nand MLP baselines (38.82%) underscores the value of treating SNPs sequences as\nstructured semantic inputs rather than unordered feature vectors. This shared repre-\nsentation space allows the model to integrate complementary micro-scale (genetic) and\nmacro-scale (neuroanatomical) information, thereby facilitating biologically meaning-\nful cross-modal reasoning—for example, allowing susceptibility genes such as APOE,\nBIN1, or RBFOX1 to guide attention toward structurally vulnerable ROIs.\n3.2 Heterogeneous Training Data Enhances Multimodal\nLearning and Cross-Category Robustness\nThe empirical findings underscore the value of integrating heterogeneous data sources\nwithin the MLLM framework. The substantial increase in accuracy when training on\nthe Mixture Dataset—where paired imaging–genetic samples are supplemented with\nadditional unimodal genetic data—indicates that the model benefits from broader\nexposure to genomic variability. Rather than relying solely on rigid, modality-specific\nrepresentations typical of CNN-based fusion, the MLLM architecture leverages abun-\ndant SNPs-only examples to refine the structure of the genetic embedding space,\n12\n"}, {"page": 13, "text": "which in turn stabilizes downstream multimodal learning. This process effectively\ntransfers population-level susceptibility information from the genetic domain to the\nimaging domain, enabling the model to interpret structurally ambiguous or subtle\nneuroanatomical patterns with improved consistency. Such cross-modal reinforcement\nappears to be a key contributor to the model’s robustness and its uniformly high\nperformance across diagnostic categories spanning AD, SMC, MCI, and NC.\n3.3 Stage-Resolved Neuroanatomical Patterns Align with\nEstablished Neurobiology of Early AD\nThe stage-specific ROI patterns uncovered by our multimodal framework exhibit\nstrong concordance with contemporary neuroscientific evidence on early Alzheimer’s\ndisease, supporting the biological plausibility of the model’s spatial attributions.\nNotably, four ROIs—left cerebellar cortex, right insula, left thalamus, and right trans-\nverse temporal gyrus—are associated with SMC, MCI, and AD, suggesting their role\nas shared hubs of early vulnerability. The insula is reported as one of the earliest\nregions exhibiting functional disconnection and cortical thinning in both SMC and\nMCI, particularly within salience and interoceptive networks that bridge attention,\nemotion, and memory processing. Multiple studies demonstrate insular atrophy and\nnetwork disruption even before objective cognitive impairment manifests [47, 48]. The\nthalamus, a critical relay hub linking subcortical nuclei with cortico-hippocampal\nmemory systems, exhibits early structural degeneration and impaired connectivity in\nAD and prodromal stages [49]. The transverse temporal gyrus (Heschl’s gyrus) also\ndemonstrates early cortical thinning in AD progression, particularly in SMC and MCI,\nreflecting its role in multimodal integration and auditory–memory coupling [26, 50].\nGrowing evidence contradicts the traditional view that the cerebellum is largely spared\nin Alzheimer’s disease and demonstrates cerebellar atrophy and morphological changes\nassociated with cognitive decline in AD and aging [51–53]\nBeyond these shared ROIs, the model also highlights stage-specific distinctions\nwith strong biological interpretability. Both AD and SMC exhibit involvement of the\nleft caudate, consistent with evidence of early striatal disruptions affecting attention,\nmotivation, and executive function—even before measurable memory deficits arise\n[54, 55] This suggests that striatal vulnerability may precede and potentially con-\ntribute to subjective cognitive concerns. The left putamen, unique to SMC in our\nfindings, has been implicated in very early alterations of motor–cognitive integration\nand procedural memory, often detectable only in subtle functional changes rather than\ngross atrophy[56]. Its appearance in SMC but not MCI or AD supports the notion\nthat SMC is associated with early basal ganglia involvement before broader corti-\ncal deterioration emerges. The MCI-associated temporal pole and frontal pole align\nwith well-established cortical biomarkers of prodromal AD. Temporal pole thinning\nmarks progression in semantic memory and limbic–default mode network dysfunction\n[57, 58], while frontal pole deterioration reflects impaired executive control networks\noften emerging in MCI [59, 60].\nTaken together, the identified ROIs outline a stage-dependent pattern of neural\ninvolvement that is consistent with current understanding of early Alzheimer’s disease\n13\n"}, {"page": 14, "text": "while also highlighting a distinct organization of regions emerging from our multi-\nmodal analysis. The ROIs shared across SMC, MCI, and AD correspond to systems\ninvolved in salience processing, multimodal integration, and thalamo–cortical com-\nmunication—domains known to exhibit early physiological vulnerability—whereas the\nstage-specific ROIs delineate shifts in the dominant neural processes affected across\nthe continuum, from striatal involvement in subjective concerns to association cortical\nalterations during prodromal impairment. Notably, the appearance of cerebellar and\nstriatal regions, supported by recent evidence of their involvement in early AD, sug-\ngests that the model is sensitive to broader network-level perturbations beyond the\nclassical limbic focus. The combination and staging of these regions thus constitute a\ncoherent spatial signature emerging from the model, offering additional perspectives\non the distributed neural alterations that characterize early Alzheimer’s disease.\n3.4 Model-Prioritized Genes Span Classical Risk Loci,\nSynaptic Regulators, and Neuroimmune Pathways\nThe\nnine\ngenes\nthat\nconsistently\nreceived\nhigh\nattribution\nin\nthe\nAD\ncohort—RBFOX1, TEAM, CHAT, IGH, APOE, BIN1, CLU, CNTNAP2, and\nNECTIN2—form a biologically coherent set that spans several molecular pathways\nimplicated in Alzheimer’s disease, while also suggesting interactions that extend\nbeyond the canonical APOE-centered framework. Several of these genes (APOE,\nBIN1, CLU, NECTIN2) have long been recognized as major susceptibility loci through\nlarge-scale GWAS, reflecting contributions to lipid metabolism, endocytosis, and\namyloid-associated processes [10, 19]. Their prominence in the model aligns with their\nestablished relevance to late-onset AD and demonstrates that the multimodal archi-\ntecture can recover high-confidence risk factors even when operating on a restricted\ncandidate gene set.\nMeanwhile, the model highlights a subset of genes—such as RBFOX1, CNTNAP2,\nand CHAT—that participate in neuronal connectivity, synaptic function, and cholin-\nergic signaling, domains increasingly understood to deteriorate early in AD [61]. The\njoint appearance of synaptic-regulatory genes (RBFOX1, CNTNAP2) and classical\nrisk loci (APOE, BIN1) is notable, as it reflects a shift from single-gene interpre-\ntation toward multigene network involvement, consistent with the emerging view of\nAD as a disorder of distributed neural circuitry rather than isolated molecular lesions\n[62]. The emergence of immune-related loci such as IGH further supports a broader\npathophysiological profile, resonating with recent evidence implicating neuroimmune\ndysregulation in AD progression [63, 64]. Importantly, the overall pattern does not\nmerely recapitulate known risk genes but instead reveals a clustered constellation of\nsusceptibility, synaptic, and immune pathways that collectively characterize the AD-\nstage embeddings learned by the model. This organization suggests that the attention\nmechanism could capture coordinated genetic programs that underlie the multiscale\nimaging signatures observed in the AD cohort, offering a complementary molecular\nperspective on the disease stage identified through imaging–genetic fusion.\n14\n"}, {"page": 15, "text": "3.5 Brain ROI–Gene Associations Evolve Systematically from\nSynaptic Vulnerability to Network Disruption\nThe stage-dependent organization of Gene–ROI associations uncovered by our model\nreveals a coherent biological trajectory that aligns with contemporary views of AD as\na disorder of progressively disrupted neural circuits. Rather than reflecting isolated\neffects, the associations form structured molecular–anatomical patterns that evolve\nsystematically from SMC through MCI to clinically manifest AD. This progression is\nreflected both in the sets of implicated genes—including APP, RBFOX1, TOMM40,\nCNTNAP2, and BIN1—and in the anatomical loci to which they are linked, spanning\nstriatal hubs, thalamic relay nuclei, cerebellar cortices, and multimodal association\nareas.\nIn the SMC cohort, stable associations emerged between striatal structures (i.e.,\ncaudate, putamen), cerebellar cortex, thalamus, and insula with genetic factors\nsuch as RBFOX1, TOMM40, TEAM, and BIN1. This configuration is biologically\nreasonable and consistent with recent imaging evidence showing that SMC is accom-\npanied by subtle disruptions in cortico-striatal, salience, and cerebellar networks prior\nto overt hippocampal degeneration [65, 66]. RBFOX1 regulates synaptic splicing\nand neuronal excitability [67], while BIN1 modulates endocytic trafficking and tau\nspreading [68]. TOMM40 contributes to mitochondrial integrity, an early-vulnerable\nprocess in preclinical AD [69–71]. Their appearance in SMC thus suggests that\nearly subjective decline may reflect microcircuit-level vulnerabilities involving synaptic\nhomeostasis, metabolic stress, and endocytic regulation, which manifest preferentially\nin striatal–cerebellar–salience circuits.\nIn\nMCI,\nthe\ndistribution\nof\nassociations\nshifts\ntoward\nassociation\ncortex\nregions—including frontal pole, temporal pole, insula, and transverse temporal\ngyrus—paired with genes such as APP, RBFOX1, TOMM40, RELN, and JCAD.\nAPP’s emergence indicates that amyloidogenic processes accelerate during the pro-\ndromal stage [72, 73]. RBFOX1’s continued prominence reflects the well-established\nrole of synaptic dysfunction as an early and central mechanism of cognitive impair-\nment. TOMM40’s strong associations further support the hypothesis that mitochon-\ndrial dysfunction contributes to early cortical vulnerability in high-energy-demand\nregions[71, 74]. RELN and JCAD are involved in cortical laminar development and\nendothelial stability, respectively, both implicated in early AD-related connectivity\nand microvascular dysfunction [75–77]. This pattern aligns with neuroimaging studies\nshowing that temporal and frontal association cortices undergo measurable atrophy\nand connectivity loss during MCI, reflecting emerging cognitive deficits [78]. The con-\njoint appearance of synaptic-regulatory genes and classical AD risk loci indicates that\nMCI represents a convergence point where molecular susceptibility transitions toward\nregionally specific cortical degeneration.\nIn the AD cohort, associations become highly reproducible and anatomically con-\nsolidated, involving subcortical hubs (i.e., caudate, thalamus), cerebellar cortex, insula,\nand transverse temporal cortex. These regions are linked predominantly with APP,\nRBFOX1, CNTNAP2, DAB1, and NECTIN2. APP and NECTIN2 reflect classical\namyloid and lipid transport pathways, while BIN1/CLU family processes continue\nto underpin endocytic and immune components of pathology [79, 80]. CNTNAP2\n15\n"}, {"page": 16, "text": "contributes to neuronal connectivity and axonal signaling, with mounting evidence\nindicating early disruption of large-scale communication networks in AD [81]. DAB1\nparticipates in Reelin signaling, affecting cortical lamination and synaptic stability.\nAll these processes have been shown to deteriorate notably in AD progression [82].\nThese\nstage-specific\nGene–ROI\nassociations\noutline\na\ncoherent\nmolecu-\nlar–anatomical\nprogression\nacross\nthe\nAlzheimer’s\ndisease\ncontinuum.\nThe\nearly\ninvolvement\nof\nsynaptic,\nmetabolic,\nand\nendocytic\npathways\nin\nstri-\natal–cerebellar–salience circuits during SMC, the emergence of amyloidogenic and\ncortical vulnerability signatures in MCI, and the consolidation of multimodal cor-\ntical–subcortical disruptions in AD together suggest that the model captures a\nbiologically structured cascade rather than isolated correlations. This organized\npattern offers a complementary view of disease progression—one that links genetic sus-\nceptibility to regionally specific network vulnerability—and underscores the potential\nof multimodal learning frameworks to reveal interpretable, mechanistically informed\nsignatures of neurodegeneration.\n3.6 Translational Potential and Current Limitations of the\nMultimodal Framework\nOur proposed R-GenIMA framework has several potential clinical impacts. By inte-\ngrating structural MRI with genetic variation in an interpretable architecture, the\nmodel identifies stage-dependent molecular–anatomical signatures that may enable\nearlier and more biologically informed detection of individuals at risk for cognitive\ndecline. Such signatures could support precision stratification in clinical trials, guide\nhypotheses about pathway-specific interventions, and help link genetic susceptibility\nto regionally specific patterns of neurodegeneration. The ability to generate consistent,\nmechanistically plausible Gene–ROI associations across SMC, MCI, and AD further\nsuggests that multimodal learning may provide clinically meaningful insight into how\nheterogeneous biological processes shape disease trajectories.\nDespite these promising findings, several important limitations warrant consider-\nation. Most fundamentally, our analyses are constrained to the ADNI cohort, which,\nwhile well-characterized and widely used in AD research, represents a research-\nrecruited population that may not fully capture the demographic, genetic, and clinical\nheterogeneity encountered in real-world healthcare settings. ADNI participants are\npredominantly non-Hispanic White individuals with relatively high educational attain-\nment, potentially limiting generalizability across diverse populations with different\ngenetic architectures, environmental exposures, and patterns of brain aging. Exter-\nnal validation in independent cohorts—particularly those with broader representation\nacross ancestry groups, socioeconomic backgrounds, and geographic regions—will be\nessential to assess whether the model’s learned associations generalize beyond this spe-\ncific dataset. Additionally, because ADNI remains one of the few large-scale studies\nproviding paired structural MRI and comprehensive genetic data, opportunities for\nsuch validation are currently limited, underscoring the need for continued investment\nin multimodal data collection efforts.\nFrom a methodological perspective, our framework operates on a predefined can-\ndidate gene panel (105 genes, yielding 1,079 SNPs after quality control) selected based\n16\n"}, {"page": 17, "text": "on prior AD literature. While this targeted approach enhances interpretability and\ncomputational efficiency, it inherently restricts the model’s ability to discover novel\ngenetic associations outside this curated set. Future work incorporating genome-wide\nSNP data or whole-genome sequencing could reveal additional susceptibility loci not\ncaptured by our current gene panel, though such expansion would require careful con-\nsideration of increased dimensionality and multiple testing concerns. Similarly, our\nROI-based parcellation strategy, while preserving regional anatomical detail, relies on a\nfixed atlas (Desikan-Killiany) that may not optimally capture disease-relevant bound-\naries, particularly in subcortical structures or regions undergoing subtle morphological\nchanges during preclinical stages.\nFinally, while our study demonstrates that interpretable multimodal AI can syn-\nthesize imaging and genetic data to produce biologically coherent disease signatures,\ntranslation from research findings to clinical implementation requires addressing addi-\ntional challenges beyond model performance. These include establishing standardized\npreprocessing pipelines compatible with clinical imaging protocols, developing user\ninterfaces that convey probabilistic predictions and uncertainty estimates appropri-\nately, ensuring robustness to variation in scanner hardware and acquisition parameters,\nand conducting prospective validation studies to evaluate real-world utility and\npotential impact on patient outcomes. The current work provides a methodologi-\ncal foundation and proof-of-concept demonstration, but the pathway from research\ntool to clinically actionable system will require sustained collaborative effort among\ncomputational scientists, neuroimaging researchers, geneticists, and clinicians.\nOverall, these findings illustrate both the promise and current boundaries of mul-\ntimodal interpretable learning in Alzheimer’s research. The R-GenIMA framework\nsuccessfully bridges micro-scale genetic variation with macro-scale structural brain\nalterations in a biologically principled manner, offering new opportunities for early\ndetection, mechanistic investigation, and precision medicine approaches to AD. As\nmultimodal datasets expand and validation efforts progress, such approaches may ulti-\nmately contribute to more personalized, biology-informed strategies for identifying\nat-risk individuals and guiding therapeutic development.\n4 Methods\n4.1 Preliminary Konwledge\nSelf-Attention Mechanism. Self-attention is the core component of Transformer\narchitectures [83], designed to model long-range dependencies by relating different\npositions within a sequence. Unlike recurrent neural networks (RNNs), which process\ntokens sequentially, self-attention operates on all tokens in parallel.\nGiven an input sequence X ∈Rn×d, three learnable linear projections are applied\nto obtain the Query (Q), Key (K), and Value (V) matrices:\nQ = XWQ,\nK = XWK,\nV = XWV .\n(1)\n17\n"}, {"page": 18, "text": "Attention weights are computed using scaled dot-product attention:\nAttention(Q, K, V) = softmax\n\u0012QK⊤\n√dk\n\u0013\nV,\n(2)\nwhere dk denotes the key dimension, and the scaling factor stabilizes training by\npreventing large dot-product values from saturating the softmax.\nTransformers. Building upon self-attention, Transformers [83] consist of stacked\nlayers combining multi-head self-attention (MSA) and feed-forward networks (FFN),\ninterconnected via residual connections and layer normalization. Depending on\nthe task, Transformers can adopt encoder-only, decoder-only, or encoder–decoder\narchitectures.\nBERT [37] is an encoder-only Transformer that learns bidirectional representations\nthrough a masked language modeling objective, where randomly masked tokens are\npredicted from surrounding context. In bioinformatics, BERT-style models have been\nadapted to genetic sequences [84] by treating DNA segments as sentences and variants\n(e.g., SNPs) as tokens, enabling the modeling of long-range epistatic interactions across\ngenetic loci.\nThe Vision Transformer (ViT) [85] extends Transformers to images by partition-\ning an image into fixed-size patches that are linearly embedded as tokens. Learnable\npositional embeddings preserve spatial structure, allowing global context modeling\nacross the image. This property is particularly beneficial for medical imaging, where\nlong-range spatial dependencies (e.g., inter-hemispheric brain symmetry) are clinically\nrelevant and difficult for CNNs with local receptive fields to capture.\nCross-Modal Attention Mechanism. While self-attention models relationships\nwithin a single modality, cross-modal attention (CMA) [86] enables information fusion\nacross different modalities (e.g., imaging and genetics). In CMA, one modality provides\nQueries (Q), while the other supplies Keys (K) and Values (V).\nFor example, aligning genetic features Fgene with image features Fimg can be\nformulated as:\nCrossAttention(Fgene, Fimg) = softmax\n\u0012(FgeneWQ)(FimgWK)⊤\n√dk\n\u0013\n(FimgWV ).\n(3)\nThis operation computes a weighted aggregation of image features conditioned on\ngenetic information, allowing genetic risk factors to selectively attend to brain regions\nmost relevant to disease pathology.\nConvolutional Neural Networks. Convolutional Neural Networks (CNNs) are\nspecialized architectures for grid-structured data such as images. They extract hierar-\nchical representations through convolutional filters that capture local patterns (e.g.,\nedges and textures), followed by pooling operations for spatial downsampling.\nMed3D [40] is a 3D CNN pre-trained on large-scale, heterogeneous medical volumes\nto address data scarcity in medical imaging. By learning domain-invariant volumet-\nric representations across modalities (e.g., MRI and CT), Med3D serves as a strong\nbackbone for encoding 3D brain MRI, outperforming models trained from scratch or\ntransferred from natural images.\n18\n"}, {"page": 19, "text": "4.2 R-GenIMA Architecture\nFig. 3 (1) The R-GeneIMA framework integrates ROI-based neuroimaging features with SNPs pro-\nfiles. ROI patches are embedded using the proposed RiT model, and a structured prompt couples\nthese embeddings with the SNPs profile before being passed to the LLM for stage-specific reasoning.\n(2) Illustration of the 3D ROI parcellation process. (3) Illustration of the RiT model.\nOur proposed ROI–Gene fused Interpretable Multimodal AI framework (R-\nGenIMA) is illustrated in Figure 3. Given a subject with a T1-weighted 3D brain\nMRI volume denoted as I ∈RH×W ×D×C, and a corresponding genetic SNPs pro-\nfile denoted by G, R-GenIMA encodes the 3D parcellated anatomical regions using\nour designed ROI-wise vision Transformer (RiT) model and embeds the SNPs pro-\nfile using a structured prompting strategy. To enable controlled comparisons, the RiT\n19\n"}, {"page": 20, "text": "model can be substituted with a whole-brain 3D MRI encoder to process the unseg-\nmented MRI volume, serving as a baseline for evaluating the contribution of ROI-level\ndecomposition.\n4.2.1 RiT Model and Image Embedding\nTo preserve fine-grained anatomical information while avoiding the memory con-\nstraints inherent in whole-brain volumetric processing, we introduce an ROI-wise\nVision Transformer (RiT) that operates on anatomically parcellated 3D patches. Each\nT1-weighted MRI volume I is first segmented into atlas-defined ROIs:\n{I(i)\nr }N\ni=1 = Segment(I),\n(4)\nwhere I(i)\nr\ndenotes the ith ROI volume. For consistency across subjects, we fix the\nnumber of ROIs to N = 109 (the maximal count in the dataset) and resample each\nROI to a standardized spatial dimension of 96 × 96 × 96. Missing or anatomically\nabsent ROIs are filled with zero-valued patches, ensuring a uniform input sequence.\nEach 963 ROI patch is embedded using a 3D convolutional patch encoder whose\nkernel spans the entire spatial extent of the ROI and is applied with a stride of 1 along\nthe ROI sequence dimension, producing a single visual token per region:\nz(i)\nr\n= Conv3D(I(i)\nr ) ∈R1×768.\n(5)\nThese token embeddings are subsequently refined by a lightweight Vision Trans-\nformer, which models inter-ROI dependencies and extracts region-specific structural\nsignatures:\nh(i)\nr\n= ViT(z(i)\nr ).\n(6)\nFinally, the resulting sequence of ROI embeddings is aligned to the large language\nmodel (LLM) embedding space through a linear connector:\nHimage = Linear\n\u0000{h(i)\nr }N\ni=1\n\u0001\n.\n(7)\nThis produces a set of visual tokens compatible with the LLM’s multimodal atten-\ntion mechanism, enabling direct cross-modal interactions with SNPs-derived genetic\ntokens.\nAs a comparative reference, we also include a whole-brain MRI encoder that cap-\ntures global patterns of structural atrophy. In this branch, the full 3D T1 volume I\nis processed using a pre-trained Med3D architecture, a 3D convolutional neural net-\nwork optimized for medical volumetric feature extraction. The model produces a single\nglobal representation summarizing large-scale anatomical structure:\nhwb = Med3D(I) ∈R1×256.\n(8)\nThis global feature vector is subsequently projected into the LLM embedding space\nthrough a linear transformation:\nHimage = Linear(hwb).\n(9)\n20\n"}, {"page": 21, "text": "This baseline provides a contrast to the region-wise RiT encoder by emphasizing coarse\nwhole-brain morphology rather than localized, high-resolution anatomical patterns.\n4.2.2 Prompt Design for Imaging–SNPs Coupling\nTo leverage the cross-modal reasoning capabilities of the MLLM, we design\na structured instruction prompt that jointly incorporates genetic information\nand MRI-derived visual embeddings within a unified semantic context. The\nprompt\nenables\nthe\nmodel\nto\nalign\ntextual\nrepresentations\nof\ngenetic\nvari-\nation\nwith\nspatially\nlocalized\nneuroanatomical\nfeatures,\nthereby\nsupporting\ntoken-wise cross-attention across modalities. The template is defined as follows:\nA chat between a curious user and an artificial intelligence assistant. The\nassistant gives helpful, detailed, and polite answers. Genome Information:\n<Genetic Content> Brain Image: <Image Content>. Your task is to classify\nthe disease of the subject based on their Brain Image and Genome Information.\nChoose one of the following labels: [NC, SMC, MCI, AD].\nMultimodal coupling is enabled through a two-level insertion mechanism:\nText-level genetic insertion. The placeholder <Genetic Content> is replaced with a\nserialized textual representation of the subject’s SNPs profile. This converts discrete\nallelic values into natural-language sequences that the LLM can interpret using its\nbiomedical pretraining, enabling contextual reasoning over genetic patterns.\nEmbedding-level\nvisual\ninsertion.\nThe\nplaceholder\n<Image Content>\nacts\nas\na dedicated anchor token. During the forward pass, it is replaced with the\nsequence of visual embeddings produced by the ROI-wise 3D vision encoder. These\nembeddings are mapped into the LLM token space by the Connector module,\nallowing the model to perform cross-attention between specific genetic variants\nand anatomically localized brain features. The MLLM receives this combined\nprompt as input and generates a natural-language diagnostic output of the form:\nThis subject is <label>.\n4.2.3 Generative Reasoning via the MLLM\nFor the genetic modality, the SNPs sequence G is serialized into natural-language\ntext and incorporated into the instruction prompt as explained in Section 4.2.2. The\ntokenizer maps this text into an input token sequence X = {x1, x2, . . . , xn} containing\nboth the instructional template and the genetic content, together with an image-anchor\nplaceholder that will later be substituted by visual embeddings. The diagnostic output\nis expressed as a target token sequence Y = {y1, y2, . . . , ym}. The MLLM models\nthe conditional probability of Y given both the textual input and the image-derived\nembeddings Himage via an autoregressively factorized joint probability:\nPθ(Y | X, Himage) =\nm\nY\nt=1\nPθ\n\u0000yt | y<t, X, Himage\n\u0001\n,\n(10)\nwhere y<t denotes previously generated tokens. The term Himage represents the fused\nROI-level visual tokens projected into the LLM embedding space, enabling multimodal\ncross-attention during generation.\n21\n"}, {"page": 22, "text": "Optimization. Model parameters θ are optimized by minimizing the negative log-\nlikelihood (NLL) over a batch of size B:\nL(θ) = −1\nB\nB\nX\nj=1\nmj\nX\nt=1\nlog Pθ\n\u0010\ny(j)\nt\n| y(j)\n<t , X(j), H(j)\nimage\n\u0011\n.\n(11)\nInference. During inference, the fine-tuned model LLMθ∗autoregressively predicts\nthe diagnosis token by token:\nˆyt = arg max\nv∈V Pθ∗(v | ˆy<t, X, Himage) ,\n(12)\nwhere V denotes the vocabulary. This generative formulation allows the model to inte-\ngrate textual SNPs patterns with anatomically localized MRI embeddings through\ncross-modal attention, enabling semantically aligned reasoning over multimodal\nbiological inputs.\n4.3 Cluster-based Attention Stability Analysis for ROI–Gene\nAssociations\nTo identify robust and biologically meaningful interactions between genetic factors\nand regional neuroanatomy across disease stages, we developed a bootstrap-based\nattention stability framework. For each subject s, the multimodal model produces an\nattention weight as,r,g that reflects the contribution of gene g to the representation of\nregion of interest (ROI) r. For every ROI–gene pair (r, g), these values are aggregated\nacross subjects within a diagnostic group (SMC, MCI, or AD) to form\nXr,g = {a1,r,g, a2,r,g, . . . , aN,r,g},\n(13)\nwhich represents the empirical distribution of interaction strengths for that group.\nBecause raw attention estimates may be sensitive to inter-subject variability and occa-\nsional outliers, we quantified the reproducibility of each ROI–gene association via\nnonparametric bootstrap resampling. For each pair (r, g), we generated B = 1000\nbootstrap replicates by sampling N values from Xr,g with replacement, and computed\na mean attention value for each replicate:\nµ(b)\nr,g = 1\nN\nN\nX\ni=1\nx(b)\ni .\n(14)\nThis process yields an empirical distribution\n{µ(1)\nr,g, µ(2)\nr,g, . . . , µ(B)\nr,g },\n(15)\n22\n"}, {"page": 23, "text": "Table 6 Simplified summary of ADNI diagnostic criteria for NC, SMC, MCI, and AD.\nCategory Cognitive Tests\nFunctional Status\nKey Features\nNC\nMMSE 24–30; CDR = 0;\nLM-II normal\nNo\nfunctional\nimpairment\nNo subjective complaints;\ncognitively normal.\nSMC\nMMSE 24–30; CDR = 0;\nnormal scores\nNo impairment\nSubjective\nmemory\ncon-\ncerns\nwithout\nobjective\ndeficits.\nMCI\nMMSE 24–30; CDR = 0.5;\nLM-II below cutoff\nMinimal\nimpair-\nment;\nADLs\npreserved\nObjective memory impair-\nment but not dementia.\nAD\nMMSE 20–26; CDR 0.5–1\nFunctional decline\nMeets\nNINCDS–ADRDA\ncriteria for probable AD.\ncapturing both the central tendency and variability of the ROI–gene interaction. From\nthis distribution, we compute the bootstrap mean\n¯µr,g = 1\nB\nB\nX\nb=1\nµ(b)\nr,g\n(16)\nand derive a 95% confidence interval (CI) from the 2.5th and 97.5th percentiles. To\nsummarize association robustness, we define a stability metric:\nStabilityr,g =\n¯µr,g\nCI97.5% −CI2.5%\n,\n(17)\nwhich normalizes the average attention strength by its uncertainty. Higher stability\nvalues correspond to associations that are both strong and consistently recovered\nacross subjects, allowing prioritization of ROI–gene pairs most likely to reflect disease-\nrelevant biological pathways.\n4.4 Dataset\n4.4.1 Data Description and Preprocessing\nWe utilize 3D T1 MRI and corresponding SNPs data from the publicly available\nAlzheimer’s Disease Neuroimaging Initiative (ADNI) study [87]. The dataset includes\n1998 subjects, consisting of 644 NC (mean age = 73.29 ± 6.16, 349 female), 100 SMC\n(mean age = 72.33 ± 5.67, 59 female), 916 MCI (mean age = 73.06 ± 7.57, 370\nfemale), and 338 AD (mean age = 74.99 ± 7.92, 149 female). Brief diagnostic criteria\nfor NC, SMC, MCI, and AD used in this study are summarized in Table 6. Full clinical\ndefinitions and assessment protocols are provided in the ADNI Procedures Manual\nand related diagnostic guidelines [6, 88].\nProcessing of 3D T1-weighted MRI data was performed using the standard\nFreeSurfer pipeline (recon-all, version 6.0)[89]. For each subject, recon-all was applied\nto carry out skull stripping, intensity normalization, cortical surface reconstruc-\ntion, and automated segmentation of cortical and subcortical structures. Regional\n23\n"}, {"page": 24, "text": "definitions were obtained from the Desikan–Killiany cortical parcellation[90] and\nFreeSurfer’s automated subcortical segmentation. These atlas-based labels were used\nto define anatomically consistent ROIs across subjects for subsequent analysis.\nThe SNPs quality-control procedure consisted of three standard filtering steps: han-\ndling missingness, filtering by minor allele frequency (MAF), and testing for deviation\nfrom Hardy–Weinberg equilibrium (HWE). First, SNPs with more than 95% missing\ngenotypes were removed; for the remaining loci, missing values were imputed using\nthe empirical allele-frequency–based expected genotype. Second, variants with a MAF\nbelow 0.05 were excluded to avoid instability in downstream modeling. Third, SNPs\nexhibiting significant deviation from HWE (p-value < 1×10−6) were removed, as such\ndeviations may reflect technical artifacts or population substructure. After applying\nthese quality-control criteria, 1,079 SNPs remained from the original 76K variants.\n4.4.2 Genetic Data Augmentation via Gene-Order Permutation\nSince individual genes do not possess an inherently meaningful sequential ordering\nin the context of variant–disease prediction, the model should, in principle, exhibit\npermutation invariance to the arrangement of gene blocks. To enforce this inductive\nbias and simultaneously expand the effective training distribution, we implemented a\ngene-order permutation augmentation strategy.\nFor\neach\nsubject,\nthe\nSNPs\nsequences\nbelonging\nto\na\ngiven\ngene\n(e.g.,\nSNPi1, . . . , SNPiK for Genei) were preserved as an intact block, while the ordering of\nthese gene-level blocks (e.g., Genei, Genej, . . . ) was randomly permuted. Critically,\nthe internal SNPs order within each gene was not altered, ensuring that biologically\nmeaningful within-gene structure was retained. This augmentation yields alternate yet\nsemantically equivalent genetic representations, promoting robustness to superficial\ninput permutations.\nThrough repeated random permutations, the dataset was substantially expanded\nto 50,000 training samples and 10,000 testing samples. This strategy serves two com-\nplementary purposes: (i) reinforcing the model’s invariance to arbitrary gene ordering,\nthereby preventing spurious sequence-dependent learning, and (ii) increasing genetic\nsample diversity to improve generalization in downstream multimodal classification.\n4.4.3 Experimental Data Configurations\nFollowing genetic augmentation and ROI-based image processing, we constructed\nthree complementary data configurations to systematically evaluate the contributions\nof unimodal and multimodal information, as well as the impact of incorporating\nheterogeneous samples during model training:\n1. Gene-only\nConfiguration:\nThis\nconfiguration\ncontains\nexclusively\nSNPs\nsequences (after gene-order permutation augmentation). It isolates the predictive\nvalue of genetic variation and serves as a baseline for evaluating the MLLM’s\ncapacity to model high-dimensional genomic signals without any imaging input.\n2. Image–Gene Configuration: Each sample includes both the subject’s SNPs\nsequence and their corresponding 3D T1-weighted MRI scan. This paired mul-\ntimodal dataset enables evaluation of cross-modal reasoning and quantifies the\n24\n"}, {"page": 25, "text": "benefits of joint imaging–genetic representation learning. Only subjects with\ncomplete genetic and imaging data are included.\n3. Mixture Data Configuration: To examine whether unimodal genetic samples\ncan enhance multimodal learning, we constructed a hybrid dataset in which 50%\nof samples contain paired SNPs–MRI data, while the remaining 50% contain SNPs\nsequences only. This configuration increases genetic sample diversity and allows\nthe model to refine genomic embeddings even when imaging data are unavailable,\nthereby testing the hypothesis that additional genetic-only samples can improve\ndownstream multimodal classification.\nTogether, these three data configurations enable a structured evaluation of (i) the\nstand-alone utility of genetic predictors, (ii) the advantages of multimodal fusion, and\n(iii) the effect of integrating heterogeneous genetic-only samples into the training of a\nunified multimodal MLLM framework.\n4.5 Implementation Details\n4.5.1 Baseline Models\nTo comprehensively evaluate the performance of our proposed framework, we com-\npared it against a set of representative unimodal and multimodal baselines spanning\nclassical neural architectures, pretrained language models, and standard fusion\nstrategies.\nGene-only Baselines. These models isolate the predictive contribution of genetic\ninformation. Three families of text-based encoders were considered:\nMLP [91]: A conventional feedforward neural network that treats SNPs vectors as\nfixed-length numerical inputs, serving as a non-sequential learning baseline.\nBERT [37]: A bidirectional transformer encoder applied to serialized SNPs\nsequences to capture local contextual dependencies, representing a discriminative\nlanguage-modeling approach.\nLarge Language Models (Llama [38] and Qwen [39]): Two state-of-the-art gen-\nerative LLMs—Llama3-8B-Instruct and Qwen2.5-7B—were employed to assess the\nbenefit of large-scale pretraining for interpreting genetic descriptions in the absence\nof imaging information.\nMultimodal (Image–Gene) Baselines. To benchmark our ROI-guided fusion\napproach against traditional multimodal pipelines, we paired a 3D medical imaging\nencoder with a genetic text encoder.\nBERT + Med3D [40] (concat): A late-fusion architecture in which global whole-\nbrain features extracted by Med3D (a 3D ResNet-based backbone) are concatenated\nwith BERT-derived genetic embeddings prior to classification.\nBERT + Med3D (crossmodal): A cross-attention baseline that models interac-\ntions between whole-brain image embeddings and textual SNPs representations using\nstandard multimodal attention layers.\nTogether, these baselines allow us to disentangle the individual contributions of\ngenetic feature modeling (MLP vs. BERT vs. LLMs) and to directly compare con-\nventional fusion strategies against the proposed ROI-guided, semantically aligned\nmultimodal integration.\n25\n"}, {"page": 26, "text": "4.5.2 Experimental Setting.\nWe adopt a two-stage training strategy. First, the vision encoders—Med3D and the\nproposed RiT—are trained on a four-way brain MRI classification task (NC, SMC,\nMCI, AD). After convergence, the encoder weights are frozen. In the second stage, the\nConnector and the LLM are jointly fine-tuned while receiving fixed visual embeddings\nfrom the pretrained encoders.\nFine-tuning is conducted on 8×A100 GPUs using full-parameter training with\nthe AdamW optimizer. We employ DeepSpeed ZeRO-3 to shard model parameters,\ngradients, and optimizer states across devices. Each GPU processes a batch of 16\nsamples. All parameters are trained in bfloat16 precision with a learning rate of 2 ×\n10−6. Models are trained for 3 epochs, and each fine-tuning run requires approximately\n48 hours.\nData availability.\nAccess to raw ADNI data requires user registration and adher-\nence to the ADNI Data Use Agreement 3. The authors do not have the right to\nredistribute the original ADNI data. The processed data are available from the\ncorresponding author upon reasonable request, subject to ADNI data-use regulations.\nCode availability.\nCode is available at https://github.com/hegehongcha/genetic\nimaging\nAcknowledgment.\nThis study is partially supported by the National Institutes of\nHealth (R21AG087888, U01AG068057) and the National Science Foundation (CCF\n2523787, IIS 2319450, IIS 2045848). Part of this work used the Bridges-2 system,\nwhich is supported by NSF OAC-1928147 at the Pittsburgh Supercomputing Center\n(PSC). We also acknowledge the UTRGV High Performance Computing Resource,\nsupported by NSF grants 2018900 and IIS-2334389, and DoD grant W911NF2110169.\nData used in preparation of this article were obtained from the Alzheimer’s Disease\nNeuroimaging Initiative (ADNI) database4 funded by NIH grant U19AG024904. As\nsuch, the investigators within the ADNI contributed to the design and implementation\nof ADNI and/or provided data but did not participate in analysis or writing of this\nreport. A complete listing of ADNI investigators can be found at: 5\nAuthor contributions.\nK.Z. contributed to conceptualization, formal analysis,\ninvestigation, methodology, validation, visualization, writing—original draft, and writ-\ning—review & editing. S.D., Y.Z., G.L., and P.G. contributed to conceptualization,\nmethodology, and writing—review & editing. C.L. contributed to methodology, for-\nmal analysis, validation and investigation. P.M.T., A.L., H.H. and L.H. contributed\nto writing—review & editing, funding acquisition and validation. L.Z. and H.T. con-\ntributed to data curation, formal analysis, funding acquisition, investigation, resources,\nsupervision, writing—review & editing and project administration.\nCompeting interests.\nThe authors declare no financial or non-financial competing\ninterests related to this work.\n3ADNI\nDUA:\nhttps://adni.loni.usc.edu/wp-content/themes/adni 2023/documents/ADNI Data Use\nAgreement.pdf\n4http://adni.loni.usc.edu\n5http://adni.loni.usc.edu/wp-content/uploads/how to apply/ADNI Acknowledgement List.pdf.\n26\n"}, {"page": 27, "text": "References\n[1] World Health Organization: Dementia — Fact sheet. https://www.who.int/\nnews-room/fact-sheets/detail/dementia. Accessed: 2025-12-02 (2025)\n[2] Xiaopeng, Z., Jing, Y., Xia, L., Xingsheng, W., Juan, D., Yan, L., Baoshan, L.:\nGlobal burden of alzheimer’s disease and other dementias in adults aged 65 years\nand older, 1991–2021: population-based study. Frontiers in Public Health 13,\n1585711 (2025)\n[3] Zhang, J., Zhou, W., Cassidy, R.M., Su, H., Su, Y., Zhang, X., Initiative, A.D.N.,\net al.: Risk factors for amyloid positivity in older people reporting significant\nmemory concern. Comprehensive Psychiatry 80, 126–131 (2018)\n[4] Jack Jr, C.R., Bennett, D.A., Blennow, K., Carrillo, M.C., Feldman, H.H., Frisoni,\nG.B., Hampel, H., Jagust, W.J., Johnson, K.A., Knopman, D.S., et al.: A/t/n:\nAn unbiased descriptive classification scheme for alzheimer disease biomarkers.\nNeurology 87(5), 539–547 (2016)\n[5] Jack Jr, C.R., Bennett, D.A., Blennow, K., Carrillo, M.C., Dunn, B., Haeberlein,\nS.B., Holtzman, D.M., Jagust, W., Jessen, F., Karlawish, J., et al.: Nia-aa research\nframework: toward a biological definition of alzheimer’s disease. Alzheimer’s &\ndementia 14(4), 535–562 (2018)\n[6] Weiner, M.W., Veitch, D.P., Aisen, P.S., Beckett, L.A., Cairns, N.J., Green, R.C.,\nHarvey, D., Jack, C.R., Jagust, W., Liu, E., et al.: The alzheimer’s disease neu-\nroimaging initiative: a review of papers published since its inception. Alzheimer’s\n& Dementia 9(5), 111–194 (2013)\n[7] Sperling, R.A., Aisen, P.S., Beckett, L.A., Bennett, D.A., Craft, S., Fagan, A.M.,\nIwatsubo, T., Jack Jr, C.R., Kaye, J., Montine, T.J., et al.: Toward defining\nthe preclinical stages of alzheimer’s disease: Recommendations from the national\ninstitute on aging-alzheimer’s association workgroups on diagnostic guidelines for\nalzheimer’s disease. Alzheimer’s & dementia 7(3), 280–292 (2011)\n[8] Jessen, F., Amariglio, R.E., Van Boxtel, M., Breteler, M., Ceccaldi, M., Ch´etelat,\nG., Dubois, B., Dufouil, C., Ellis, K.A., Van Der Flier, W.M., et al.: A conceptual\nframework for research on subjective cognitive decline in preclinical alzheimer’s\ndisease. Alzheimer’s & dementia 10(6), 844–852 (2014)\n[9] Bateman, R.J., Xiong, C., Benzinger, T.L., Fagan, A.M., Goate, A., Fox, N.C.,\nMarcus, D.S., Cairns, N.J., Xie, X., Blazey, T.M., et al.: Clinical and biomarker\nchanges in dominantly inherited alzheimer’s disease. New England Journal of\nMedicine 367(9), 795–804 (2012)\n[10] Kunkle, B.W., Grenier-Boley, B., Sims, R., Bis, J.C., Damotte, V., Naj, A.C.,\nBoland, A., Vronskaya, M., Van Der Lee, S.J., Amlie-Wolf, A., et al.: Genetic\n27\n"}, {"page": 28, "text": "meta-analysis of diagnosed alzheimer’s disease identifies new risk loci and impli-\ncates aβ, tau, immunity and lipid processing. Nature genetics 51(3), 414–430\n(2019)\n[11] Bertsimas, D., Pauphilet, J., Van Parys, B.: Sparse regression. Statistical Science\n35(4), 555–578 (2020)\n[12] Stein, J.L., Hua, X., Lee, S., Ho, A.J., Leow, A.D., Toga, A.W., Saykin, A.J.,\nShen, L., Foroud, T., Pankratz, N., et al.: Voxelwise genome-wide association\nstudy (vgwas). neuroimage 53(3), 1160–1174 (2010)\n[13] Weenink, D.: Canonical correlation analysis. In: Proceedings of the Institute of\nPhonetic Sciences of the University of Amsterdam, vol. 25, pp. 81–99 (2003).\nUniversity of Amsterdam Amsterdam\n[14] Chi, E.C., Allen, G.I., Zhou, H., Kohannim, O., Lange, K., Thompson, P.M.:\nImaging genetics via sparse canonical correlation analysis. In: 2013 IEEE 10th\nInternational Symposium on Biomedical Imaging, pp. 740–743 (2013). IEEE\n[15] Du, L., Liu, K., Yao, X., Risacher, S.L., Han, J., Saykin, A.J., Guo, L., Shen,\nL.: Detecting genetic associations with brain imaging phenotypes in alzheimer’s\ndisease via a novel structured scca approach. Medical image analysis 61, 101656\n(2020)\n[16] Uffelmann, E., Huang, Q.Q., Munung, N.S., De Vries, J., Okada, Y., Martin, A.R.,\nMartin, H.C., Lappalainen, T., Posthuma, D.: Genome-wide association studies.\nNature Reviews Methods Primers 1(1), 59 (2021)\n[17] Shen, L., Kim, S., Risacher, S.L., Nho, K., Swaminathan, S., West, J.D., Foroud,\nT., Pankratz, N., Moore, J.H., Sloan, C.D., et al.: Whole genome association\nstudy of brain-wide imaging phenotypes for identifying quantitative trait loci in\nmci and ad: A study of the adni cohort. Neuroimage 53(3), 1051–1063 (2010)\n[18] Shen, L., Thompson, P.M., Potkin, S.G., Bertram, L., Farrer, L.A., Foroud, T.M.,\nGreen, R.C., Hu, X., Huentelman, M.J., Kim, S., et al.: Genetic analysis of quan-\ntitative phenotypes in ad and mci: imaging, cognition and biomarkers. Brain\nimaging and behavior 8(2), 183–207 (2014)\n[19] Lambert, J.-C., Ibrahim-Verbaas, C.A., Harold, D., Naj, A.C., Sims, R., Bel-\nlenguez, C., Jun, G., DeStefano, A.L., Bis, J.C., Beecham, G.W., et al.: Meta-\nanalysis of 74,046 individuals identifies 11 new susceptibility loci for alzheimer’s\ndisease. Nature genetics 45(12), 1452–1458 (2013)\n[20] Thompson, P.M., Stein, J.L., Medland, S.E., Hibar, D.P., Vasquez, A.A., Rente-\nria, M.E., Toro, R., Jahanshad, N., Schumann, G., Franke, B., et al.: The enigma\nconsortium: large-scale collaborative analyses of neuroimaging and genetic data.\nBrain imaging and behavior 8(2), 153–182 (2014)\n28\n"}, {"page": 29, "text": "[21] Xu, Q., Guo, L., Cheng, J., Wang, M., Geng, Z., Zhu, W., Zhang, B., Liao,\nW., Qiu, S., Zhang, H., et al.: Chimgen: a chinese imaging genetics cohort to\nenhance cross-ethnic and cross-geographic brain research. Molecular psychiatry\n25(3), 517–529 (2020)\n[22] Fu, J., Zhang, Q., Wang, J., Wang, M., Zhang, B., Zhu, W., Qiu, S., Geng, Z.,\nCui, G., Yu, Y., et al.: Cross-ancestry genome-wide association studies of brain\nimaging phenotypes. Nature Genetics 56(6), 1110–1120 (2024)\n[23] Elliott, L.T., Sharp, K., Alfaro-Almagro, F., Shi, S., Miller, K.L., Douaud, G.,\nMarchini, J., Smith, S.M.: Genome-wide association studies of brain imaging\nphenotypes in uk biobank. Nature 562(7726), 210–216 (2018)\n[24] Xin, Y., Sheng, J., Miao, M., Wang, L., Yang, Z., Huang, H.: A review of imag-\ning genetics in alzheimer’s disease. Journal of clinical neuroscience 100, 155–163\n(2022)\n[25] Shen, L., Thompson, P.M.: Brain imaging genomics: integrated analysis and\nmachine learning. Proceedings of the IEEE 108(1), 125–162 (2019)\n[26] Li, Y., Niu, D., Qi, K., Liang, D., Long, X.: An imaging and genetic-based\ndeep learning network for alzheimer’s disease diagnosis. Frontiers in Aging\nNeuroscience 17, 1532470 (2025)\n[27] Wang, J.X., Li, Y., Li, X., Lu, Z.-H.: Alzheimer’s disease classification through\nimaging genetic data with ignet. Frontiers in Neuroscience 16, 846638 (2022)\n[28] Venugopalan, J., Tong, L., Hassanzadeh, H.R., Wang, M.D.: Multimodal deep\nlearning models for early detection of alzheimer’s disease stage. Scientific reports\n11(1), 3254 (2021)\n[29] Golovanevsky, G., Eickhoff, S.B., Patil, K.R.: Multimodal attention-based deep\nlearning for alzheimer’s disease diagnosis. Journal of the American Medical\nInformatics Association (JAMIA Open) 5(4), 083 (2022). Criticizes simple\nconcatenation and proposes attention mechanisms\n[30] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models\nare few-shot learners. Advances in neural information processing systems 33,\n1877–1901 (2020)\n[31] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\nAskell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from\nnatural language supervision. In: International Conference on Machine Learning,\npp. 8748–8763 (2021). PmLR\n[32] Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T.,\n29\n"}, {"page": 30, "text": "Poon, H., Gao, J.: Llava-med: Training a large language-and-vision assistant for\nbiomedicine in one day. Advances in Neural Information Processing Systems 36,\n28541–28564 (2023)\n[33] Wu, C., Zhang, X., Zhang, Y., Hui, H., Wang, Y., Xie, W.: Towards general-\nist foundation model for radiology by leveraging web-scale 2d&3d medical data.\nNature Communications 16(1), 7866 (2025)\n[34] Xin, Y., Ates, G.C., Gong, K., Shao, W.: Med3dvlm: An efficient vision-language\nmodel for 3d medical image analysis. arXiv preprint arXiv:2503.20047 (2025)\n[35] Wang, H., Nie, F., Huang, H., Kim, S., Nho, K., Risacher, S.L., Saykin, A.J.,\nShen, L., Initiative, A.D.N.: Identifying quantitative trait loci via group-sparse\nmultitask regression and feature selection: an imaging genetics study of the adni\ncohort. Bioinformatics 28(2), 229–237 (2012)\n[36] Mueller, S.G., Weiner, M.W., Thal, L.J., Petersen, R.C., Jack, C., Jagust, W.,\nTrojanowski, J.Q., Toga, A.W., Beckett, L.: The alzheimer’s disease neuroimaging\ninitiative. Neuroimaging Clinics 15(4), 869–877 (2005)\n[37] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert: Pre-training of deep\nbidirectional transformers for language understanding. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (long and Short Papers),\npp. 4171–4186 (2019)\n[38] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T.,\nRozi`ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave,\nE., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (2023).\nhttps://arxiv.org/abs/2302.13971\n[39] Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang,\nF., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin,\nJ., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu,\nQ., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang,\nY., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., Qiu, Z.: Qwen2.5 technical report. arXiv\npreprint arXiv:2412.15115 (2024)\n[40] Chen, S., Ma, K., Zheng, Y.: Med3d: Transfer learning for 3d medical image\nanalysis. arXiv preprint arXiv:1904.00625 (2019)\n[41] Abnar, S., Zuidema, W.: Quantifying attention flow in transformers. In: Proceed-\nings of the 58th Annual Meeting of the Association for Computational Linguistics,\npp. 4190–4197 (2020)\n[42] Cerezo, M., Sollis, E., Ji, Y., Lewis, E., Abid, A., Bircan, K., Hall, P.,\nHayhurst,\nJ.,\nJohn,\nS.,\nMosaku,\nA.,\nRamachandran,\nS.,\nForeman,\nA.,\n30\n"}, {"page": 31, "text": "Ibrahim,\nA.,\nMcLaughlin,\nJ.,\nPendlington,\nZ.,\nStefancsik,\nR.,\nLambert,\nS.A., McMahon, A., Morales, J., Keane, T., Inouye, M., Parkinson, H.,\nHarris, L.W.: The nhgri-ebi gwas catalog: standards for reusability, sus-\ntainability and diversity. Nucleic Acids Research 53(D1), 998–1005 (2024)\nhttps://doi.org/10.1093/nar/gkae1070\nhttps://academic.oup.com/nar/article-\npdf/53/D1/D998/60618753/gkae1070.pdf\n[43] Rivals, I., Personnaz, L., Taing, L., Potier, M.-C.: Enrichment or depletion of a\ngo category within a class of genes: which test? Bioinformatics 23(4), 401–407\n(2007)\n[44] Yao, Z., Zhang, Y., Lin, L., Zhou, Y., Xu, C., Jiang, T., Initiative, A.D.N.: Abnor-\nmal cortical networks in mild cognitive impairment and alzheimer’s disease. PLoS\ncomputational biology 6(11), 1001006 (2010)\n[45] Karas, G., Sluimer, J., Goekoop, R., Van Der Flier, W., Rombouts, S., Vrenken,\nH., Scheltens, P., Fox, N., Barkhof, F.: Amnestic mild cognitive impairment: struc-\ntural mr imaging findings predictive of conversion to alzheimer disease. American\nJournal of Neuroradiology 29(5), 944–949 (2008)\n[46] Machulda, M.M., Lundt, E.S., Albertson, S.M., Spychalla, A.J., Schwarz, C.G.,\nMielke, M.M., Jack Jr, C.R., Kremers, W.K., Vemuri, P., Knopman, D.S., et al.:\nCortical atrophy patterns of incident mci subtypes in the mayo clinic study of\naging. Alzheimer’s & dementia 16(7), 1013–1022 (2020)\n[47] Schindler, L.S., Subramaniapillai, S., Barth, C., Meer, D., Pedersen, M.L., Kauf-\nmann, T., Maximov, I.I., Linge, J., Leinhard, O.D., Beck, D., et al.: Associations\nbetween abdominal adipose tissue, reproductive span, and brain characteristics\nin post-menopausal women. NeuroImage: Clinical 36, 103239 (2022)\n[48] Hughes, L.E., Rittman, T., Regenthal, R., Robbins, T.W., Rowe, J.B.: Improving\nresponse inhibition systems in frontotemporal dementia with citalopram. Brain\n138(7), 1961–1975 (2015)\n[49] Zhou, B., Liu, Y., Zhang, Z., An, N., Yao, H., Wang, P., Wang, L., Zhang, X.,\nJiang, T.: Impaired functional connectivity of the thalamus in alzheimer’s disease\nand mild cognitive impairment: a resting-state fmri study. Current Alzheimer\nResearch 10(7), 754–766 (2013)\n[50] Wang, D., Belden, A., Hanser, S.B., Geddes, M.R., Loui, P.: Resting-state con-\nnectivity of auditory and reward systems in alzheimer’s disease and mild cognitive\nimpairment. Frontiers in human neuroscience 14, 280 (2020)\n[51] Paitel, E.R., Pettigrew, C., Callow, D.D., Moghekar, A., Miller, M.I., Faria,\nA.V., Oishi, K., Albert, M., Soldan, A.: Cerebellar white matter microstructure\nis associated with age, cerebrospinal fluid amyloid beta levels, and cognition in\ncognitively unimpaired older adults. Human Brain Mapping 46(16), 70398 (2025)\n31\n"}, {"page": 32, "text": "[52] Tang, F., Zhu, D., Ma, W., Yao, Q., Li, Q., Shi, J.: Differences changes in cere-\nbellar functional connectivity between mild cognitive impairment and alzheimer’s\ndisease: a seed-based approach. Frontiers in Neurology 12, 645171 (2021)\n[53] Elyan, R., Ahmed, B., Karunanayaka, P.: Hyperactive cerebellum in alzheimer’s\ndisease. In: Proceedings of the International Society for Magnetic Resonance\nin Medicine (ISMRM 2024) (2024). Abstract 1122, Conference Proceedings.\nhttps://archive.ismrm.org/2024/1122.html\n[54] Xiong, Y., Ye, C., Chen, Y., Zhong, X., Chen, H., Sun, R., Zhang, J., Zhong,\nZ., Huang, M.: Altered functional connectivity of basal ganglia in mild cognitive\nimpairment and alzheimer’s disease. Brain sciences 12(11), 1555 (2022)\n[55] Madsen, S.K., Ho, A.J., Hua, X., Saharan, P.S., Toga, A.W., Jack Jr, C.R.,\nWeiner, M.W., Thompson, P.M., Initiative, A.D.N., et al.: 3d maps localize cau-\ndate nucleus atrophy in 400 alzheimer’s disease, mild cognitive impairment, and\nhealthy elderly subjects. Neurobiology of aging 31(8), 1312–1325 (2010)\n[56] Wang, X., Huang, W., Su, L., Xing, Y., Jessen, F., Sun, Y., Shu, N., Han,\nY.: Neuroimaging advances regarding subjective cognitive decline in preclinical\nalzheimer’s disease. Molecular Neurodegeneration 15(1), 55 (2020)\n[57] Ereira, S., Waters, S., Razi, A., Marshall, C.R.: Early detection of dementia with\ndefault-mode network effective connectivity. Nature Mental Health 2(7), 787–800\n(2024)\n[58] Dickerson, B.C., Brickhouse, M., McGinnis, S., Wolk, D.A., Initiative, A.D.N.,\net al.: Alzheimer’s disease: The influence of age on clinical heterogeneity through\nthe human brain connectome. Alzheimer’s & Dementia: Diagnosis, Assessment &\nDisease Monitoring 6, 122–135 (2017)\n[59] Friedman, N.P., Robbins, T.W.: The role of prefrontal cortex in cognitive control\nand executive function. Neuropsychopharmacology 47(1), 72–89 (2022)\n[60] Rashidi-Ranjbar, N., Rajji, T.K., Kumar, S., Herrmann, N., Mah, L., Flint, A.J.,\nFischer, C.E., Butters, M.A., Pollock, B.G., Dickie, E.W., et al.: Frontal-executive\nand corticolimbic structural brain circuitry in older people with remitted depres-\nsion, mild cognitive impairment, alzheimer’s dementia, and normal cognition.\nNeuropsychopharmacology 45(9), 1567–1578 (2020)\n[61] Chen, Z.-R., Huang, J.-B., Yang, S.-L., Hong, F.-F.: Role of cholinergic signaling\nin alzheimer’s disease. Molecules 27(6), 1816 (2022)\n[62] Kim, S., Nam, Y., Kim, H.S., Jung, H., Jeon, S.G., Hong, S.B., Moon, M.: Alter-\nation of neural pathways and its implications in alzheimer’s disease. Biomedicines\n10(4), 845 (2022)\n32\n"}, {"page": 33, "text": "[63] Leng, F., Edison, P.: Neuroinflammation and microglial activation in alzheimer\ndisease: where do we go from here? Nature Reviews Neurology 17(3), 157–172\n(2021)\n[64] Hansen, D.V., Hanson, J.E., Sheng, M.: Microglia in alzheimer’s disease. Journal\nof Cell Biology 217(2), 459–472 (2018)\n[65] Tsai, C.-L., Lee, P.-L., Liang, C.-S., Lin, Y.-K., Lin, G.-Y., Tsai, C.-K., Hsu, Y.-\nC., Lin, C.-P., Chou, K.-H., Yang, F.-C.: Disrupted brain connectivity patterns\nin early-to-middle-aged adults with subjective memory complaints: a longitudinal\nresting-state functional magnetic resonance imaging study. Psychiatry Research,\n116732 (2025)\n[66] Cai, C., Huang, C., Yang, C., Lu, H., Hong, X., Ren, F., Hong, D., Ng, E.: Altered\npatterns of functional connectivity and causal connectivity in salience subnetwork\nof subjective cognitive decline and amnestic mild cognitive impairment. Frontiers\nin neuroscience 14, 288 (2020)\n[67] Gehman, L.T., Stoilov, P., Maguire, J., Damianov, A., Lin, C.-H., Shiue, L.,\nAres Jr, M., Mody, I., Black, D.L.: The splicing regulator rbfox1 (a2bp1) controls\nneuronal excitation in the mammalian brain. Nature genetics 43(7), 706–711\n(2011)\n[68] Crotti, A., Sait, H.R., McAvoy, K.M., Estrada, K., Ergun, A., Szak, S., Marsh,\nG., Jandreski, L., Peterson, M., Reynolds, T.L., et al.: Bin1 favors the spreading\nof tau via extracellular vesicles. Scientific reports 9(1), 9477 (2019)\n[69] Wang, W., Zhao, F., Ma, X., Perry, G., Zhu, X.: Mitochondria dysfunction in the\npathogenesis of alzheimer’s disease: recent advances. Molecular neurodegeneration\n15(1), 30 (2020)\n[70] Chen, Y.-C., Chang, S.-C., Lee, Y.-S., Ho, W.-M., Huang, Y.-H., Wu, Y.-Y.,\nChu, Y.-C., Wu, K.-H., Wei, L.-S., Wang, H.-L., et al.: Tomm40 genetic vari-\nants cause neuroinflammation in alzheimer’s disease. International Journal of\nMolecular Sciences 24(4), 4085 (2023)\n[71] Lee, E.-G., Chen, S., Leong, L., Tulloch, J., Yu, C.-E.: Tomm40 rna transcription\nin alzheimer’s disease brain and its implication in mitochondrial dysfunction.\nGenes 12(6), 871 (2021)\n[72] Hampel, H., Hardy, J., Blennow, K., Chen, C., Perry, G., Kim, S.H., Villemagne,\nV.L., Aisen, P., Vendruscolo, M., Iwatsubo, T., et al.: The amyloid-β pathway in\nalzheimer’s disease. Molecular psychiatry 26(10), 5481–5503 (2021)\n[73] Orobets, K.S., Karamyshev, A.L.: Amyloid precursor protein and alzheimer’s\ndisease. International journal of molecular sciences 24(19), 14794 (2023)\n33\n"}, {"page": 34, "text": "[74] Ferencz, B., Laukka, E.J., L¨ovd´en, M., Kalpouzos, G., Keller, L., Graff, C.,\nWahlund, L.-O., Fratiglioni, L., B¨ackman, L.: The influence of apoe and tomm40\npolymorphisms on hippocampal volume and episodic memory in old age. Frontiers\nin human neuroscience 7, 198 (2013)\n[75] Alexander, A., Herz, J., Calvier, L.: Reelin through the years: From brain\ndevelopment to inflammation. Cell reports 42(6) (2023)\n[76] Xu, S., Xu, Y., Liu, P., Zhang, S., Liu, H., Slavin, S., Kumar, S., Koroleva, M.,\nLuo, J., Wu, X., et al.: The novel coronary artery disease risk gene jcad/kiaa1462\npromotes endothelial dysfunction and atherosclerosis. European heart journal\n40(29), 2398–2408 (2019)\n[77] Jones, P.D., Kaiser, M.A., Najafabadi, M.G., Koplev, S., Zhao, Y., Dou-\nglas, G., Kyriakou, T., Andrews, S., Rajmohan, R., Watkins, H., Chan-\nnon, K.M., Ye, S., Yang, X., Bj¨orkegren, J.L.M., Samani, N.J., Webb, T.R.:\n¡i¿jcad¡/i¿, a gene at the 10p11 coronary artery disease locus, regulates hippo\nsignaling in endothelial cells. Arteriosclerosis, Thrombosis, and Vascular Biol-\nogy 38(8), 1711–1722 (2018) https://doi.org/10.1161/ATVBAHA.118.310976\nhttps://www.ahajournals.org/doi/pdf/10.1161/ATVBAHA.118.310976\n[78] Fan, Y., Batmanghelich, N., Clark, C.M., Davatzikos, C.: Spatial patterns of brain\natrophy in mci patients, identified via high-dimensional pattern classification,\npredict subsequent cognitive decline. NeuroImage 39(4), 1731–1743 (2008) https:\n//doi.org/10.1016/j.neuroimage.2007.10.031\n[79] Foster, E.M., Dangla-Valls, A., Lovestone, S., Ribe, E.M., Buckley, N.J.: Clusterin\nin alzheimer’s disease: Mechanisms, genetics, and lessons from other pathologies.\nFrontiers in Neuroscience Volume 13 - 2019 (2019) https://doi.org/10.3389/\nfnins.2019.00164\n[80] Zheng, Q., Wang, X.: Alzheimer’s disease: insights into pathology, molecu-\nlar mechanisms, and therapy. Protein & Cell 16(2), 83–120 (2024) https://\ndoi.org/10.1093/procel/pwae026\nhttps://academic.oup.com/proteincell/article-\npdf/16/2/83/58161180/pwae026.pdf\n[81] Pe˜nagarikano, O., Abrahams, B., Herman, E., Winden, K., Gdalyahu, A., Dong,\nH., Sonnenblick, L., Gruver, R., Almajano, J., Bragin, A., Golshani, P., Trachten-\nberg, J., Peles, E., Geschwind, D.: Absence of cntnap2 leads to epilepsy, neuronal\nmigration abnormalities, and core autism-related deficits. Cell 147(1), 235–246\n(2011) https://doi.org/10.1016/j.cell.2011.08.040\n[82] Iturria-Medina, Y., Sotero, R.C., Toussaint, P.J., Mateos-P´erez, J.M., Evans,\nA.C., Weiner, M.W., Jack, C.R., Trojanowski, J.Q., Toga, A.W., Green, R.C.,\nSaykin, A.J., Shaw, L.M., Thomas, R.G., Koeppe, R.A.: Early role of vascular\ndysregulation on late-onset alzheimer’s disease based on multifactorial data-\ndriven analysis. Nature Communications 7, 11934 (2016) https://doi.org/10.\n34\n"}, {"page": 35, "text": "1038/ncomms11934\n[83] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser,  L., Polosukhin, I.: Attention is all you need. Advances in neural\ninformation processing systems 30 (2017)\n[84] Le, N.Q.K., Ho, Q.-T., Nguyen, T.-T.-D., Ou, Y.-Y.: A transformer architecture\nbased on bert and 2d convolutional neural network to identify dna enhancers from\nsequence information. Briefings in bioinformatics 22(5), 005 (2021)\n[85] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby,\nN.: An image is worth 16x16 words: Transformers for image recognition\nat scale. In: International Conference on Learning Representations (2021).\nhttps://openreview.net/forum?id=YicbFdNTTy\n[86] Wei, X., Zhang, T., Li, Y., Zhang, Y., Wu, F.: Multi-modality cross attention\nnetwork for image and sentence matching. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 10941–10950\n(2020)\n[87] ADNI (Alzheimer’s Disease Neuroimaging Initiative) (2022). https://adni.loni.\nusc.edu/\n[88] Aisen, P.S., Petersen, R.C., Donohue, M.C., Gamst, A., Raman, R., Thomas,\nR.G., Walter, S., Trojanowski, J.Q., Shaw, L.M., Beckett, L.A., Jack Jr.,\nC.R., Jagust, W., Toga, A.W., Saykin, A.J., Morris, J.C., Green, R.C.,\nWeiner,\nM.W.,\nInitiative,\nA.D.N.:\nClinical\ncore\nof\nthe\nalzheimer’s\ndis-\nease neuroimaging initiative: Progress and plans. Alzheimer’s & Demen-\ntia 6(3), 239–246 (2010) https://doi.org/10.1016/j.jalz.2010.03.006 https://alz-\njournals.onlinelibrary.wiley.com/doi/pdf/10.1016/j.jalz.2010.03.006\n[89] Fischl, B.: Freesurfer. NeuroImage 62(2), 774–781 (2012) https://doi.org/10.\n1016/j.neuroimage.2012.01.021 . 20 YEARS OF fMRI\n[90] Desikan, R.S., S´egonne, F., Fischl, B., Quinn, B.T., Dickerson, B.C., Blacker, D.,\nBuckner, R.L., Dale, A.M., Maguire, R.P., Hyman, B.T., et al.: An automated\nlabeling system for subdividing the human cerebral cortex on mri scans into gyral\nbased regions of interest. Neuroimage 31(3), 968–980 (2006)\n[91] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by back-\npropagating errors. nature 323(6088), 533–536 (1986)\n35\n"}]}