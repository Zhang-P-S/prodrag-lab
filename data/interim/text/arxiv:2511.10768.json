{"doc_id": "arxiv:2511.10768", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.10768.pdf", "meta": {"doc_id": "arxiv:2511.10768", "source": "arxiv", "arxiv_id": "2511.10768", "title": "Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs", "authors": ["Ajwad Abrar", "Nafisa Tabassum Oeshy", "Prianka Maheru", "Farzana Tabassum", "Tareque Mohmud Chowdhury"], "published": "2025-11-13T19:42:11Z", "updated": "2025-11-13T19:42:11Z", "summary": "Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.10768v1", "url_pdf": "https://arxiv.org/pdf/2511.10768.pdf", "meta_path": "data/raw/arxiv/meta/2511.10768.json", "sha256": "bbf2e25b3b8a2c6c4d4f7e55adb1e87915f2d890d2cb52a95a8448be9969c91c", "status": "ok", "fetched_at": "2026-02-18T02:27:07.117572+00:00"}, "pages": [{"page": 1, "text": "Faithful Summarization of Consumer Health Queries:\nA Cross-Lingual Framework with LLMs\nAjwad Abrar ∗\nIslamic University of Technology\nDhaka, Bangladesh\najwadabrar@iut-dhaka.edu\nNafisa Tabassum Oeshy ∗\nIslamic University of Technology\nDhaka, Bangladesh\nnafisatabassum4@iut-dhaka.edu\nPrianka Maheru ∗\nIslamic University of Technology\nDhaka, Bangladesh\npriankamaheru@iut-dhaka.edu\nFarzana Tabassum\nIslamic University of Technology\nDhaka, Bangladesh\nfarzana@iut-dhaka.edu\nTareque Mohmud Chowdhury\nIslamic University of Technology\nDhaka, Bangladesh\ntareque@iut-dhaka.edu\nAbstract\nSummarizing consumer health questions (CHQs) can ease communication in\nhealthcare, but unfaithful summaries that misrepresent medical details pose serious\nrisks. We propose a framework that combines TextRank-based sentence extraction\nand medical named entity recognition with large language models (LLMs) to\nenhance faithfulness in medical text summarization. In our experiments, we fine-\ntuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ\n(Bangla) datasets, achieving consistent improvements across quality (ROUGE,\nBERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and\noutperforming zero-shot baselines and prior systems. Human evaluation further\nshows that over 80% of generated summaries preserve critical medical information.\nThese results highlight faithfulness as an essential dimension for reliable medical\nsummarization and demonstrate the potential of our approach for safer deployment\nof LLMs in healthcare contexts.\n1\nIntroduction\nThe rapid growth of online health consultations, especially consumer health questions (CHQs), has\ncreated both opportunities and challenges for healthcare delivery. These platforms, accelerated by\nthe pandemic, now serve as vital sources of medical information and support. However, the large\nvolume of verbose and sometimes redundant patient queries places a significant burden on healthcare\nprofessionals, who must spend time identifying the core concern before providing an appropriate\nresponse. Automatic medical text summarization has emerged as a potential solution to this problem\nby condensing lengthy questions into concise, focused forms.\nTraditional approaches to text summarization are primarily evaluated on general quality metrics such\nas ROUGE or BERTScore, which measure lexical or semantic similarity to reference summaries.\nWhile useful, these metrics often fail to capture faithfulness—the factual consistency of a summary\n∗These authors contributed equally to the work.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Muslims in ML.\narXiv:2511.10768v1  [cs.CL]  13 Nov 2025\n"}, {"page": 2, "text": "with its source. Prior studies show that abstractive models frequently produce intrinsic errors (e.g.,\nmisrepresenting entities or relationships) and extrinsic errors (e.g., introducing unsupported facts)\n[Maynez et al., 2020, Huang et al., 2021]. In the medical domain, even minor distortions can mislead\nprofessionals or patients, posing direct risks to health outcomes. Thus, ensuring faithful summaries is\nessential for practical deployment in healthcare contexts.\nDespite the progress of large language models (LLMs), current systems still struggle to balance\nfluency, conciseness, and factual consistency in medical summarization. Faithfulness remains\nunderexplored compared to readability or general accuracy, and most methods do not explicitly\naddress it. This gap motivates the need for specialized frameworks that preserve the integrity of\nmedical information while still providing concise and accessible summaries.\nTo address this, we propose a novel framework that combines TextRank-based sentence extraction\nwith medical named entity recognition (NER) to guide LLMs in generating summaries that are both\naccurate and faithful. We evaluate the framework on English (MeQSum) and Bangla (BanglaCHQ-\nSumm) datasets using LLaMA-2-7B fine-tuned with low-rank adaptation. Our main contributions\nare:\n• We integrate extractive and abstractive methods to improve both informativeness and relia-\nbility.\n• We incorporate medical NER to ensure critical entities are preserved in summaries.\n• We provide the first cross-lingual evaluation of faithfulness in medical CHQ summarization,\ndemonstrating improvements over zero-shot baselines and prior state-of-the-art systems.\n2\nRelated Work\nText summarization is a well-established task in NLP that aims to condense lengthy documents while\npreserving salient content [El-Kassas et al., 2021]. Techniques generally fall into extractive methods,\nwhich select important sentences, and abstractive methods, which generate new phrasing [Rush et al.,\n2017]. While both have advanced with neural models and pretrained transformers, applying them\nto biomedical text is especially challenging due to domain-specific terminology, the complexity of\nclinical narratives, and the high stakes of factual accuracy [Afantenos et al., 2005, Morozovskii and\nRamanna, 2023].\nMedical summarization has been studied for clinical notes, conversations, and consumer health\nquestions (CHQs). The MeQSum dataset [Ben Abacha and Demner-Fushman, 2019] introduced\n1,000 annotated CHQs and spurred research into neural abstractive approaches. More recently,\nBanglaCHQ-Summ [Khan et al., 2023] extended this task to Bangla, highlighting cross-lingual gaps.\nDespite promising results with transformer-based models [Michalopoulos et al., 2022], maintaining\nreliability across languages and medical domains remains an open challenge.\nA key limitation of existing work lies in faithfulness—the factual consistency of generated summaries.\nPrior studies show abstractive models often introduce intrinsic errors (contradictions) or extrinsic\nerrors (unsupported additions) [Maynez et al., 2020, Huang et al., 2021]. Recent solutions include\nconstrained decoding [Mao et al., 2020], fact-checking modules [Kryscinski et al., 2020], and\nspecialized evaluation metrics such as SummaC and AlignScore [Laban et al., 2021, Zha et al., 2023].\nHowever, ensuring factual alignment for consumer health queries, where even minor distortions can\nmislead patients or clinicians, remains underexplored. This gap motivates our focus on frameworks\nthat explicitly preserve medical entities and source-grounded content.\n3\nProposed Methodology\nWe propose a framework to improve the faithfulness of medical text summarization, with a focus\non Consumer Health Questions (CHQs) in English and Bangla. The framework combines Medical\nNamed Entity Recognition (NER) and the TextRank algorithm for extractive sentence selection,\nfollowed by fine-tuning a Large Language Model (LLM) to generate accurate and reliable summaries.\nAn overview is shown in Figure 1.\n2\n"}, {"page": 3, "text": "Figure 1: Proposed framework: TextRank extracts relevant sentences containing medical entities,\nwhich are used to fine-tune the LLM. The final summary is selected to maximize both accuracy and\nfaithfulness.\n3.1\nDatasets\nWe use two benchmark datasets. For English, the MeQSum dataset [Ben Abacha and Demner-\nFushman, 2019] contains 1,000 consumer health questions with expert-validated summaries, ensuring\nhigh-quality references. For Bangla, we use BanglaCHQ-Summ [Khan et al., 2023], which includes\n2,350 annotated pairs of questions and summaries, representing the first large-scale resource for\nBangla CHQ summarization. Together, these datasets enable evaluation across both high-resource\nand low-resource settings.\n3.2\nPreprocessing and Relevant Sentence Extraction\nPreprocessing standardizes the datasets into question and summary fields, while identifying over-\nlapping medical entities and negation terms to ensure critical information is retained. To reduce\nnoise, we apply the TextRank algorithm [Mihalcea and Tarau, 2004] to extract sentences containing\nmedical entities and query-related words. This guarantees that summaries remain faithful to medically\nimportant content before abstractive generation by the LLM.\n3.3\nEvaluation Metrics\nWe assess performance using both general quality and faithfulness metrics. General quality is mea-\nsured by ROUGE-1/2/L and BERTScore, which capture lexical and semantic overlap. Faithfulness is\nmeasured by SummaC [Laban et al., 2021] and AlignScore [Zha et al., 2023], which evaluate factual\nconsistency, along with Flesch Reading Ease (FRE) for readability. This combination ensures that\ngenerated summaries are not only fluent but also factually aligned with the source.\n4\nResults and Discussion\n4.1\nPerformance Analysis (MeQSum)\nWe evaluate LLaMA-2-7B in four settings: zero-shot, fine-tuning (FT) without TextRank, FT\nwith TextRank-selected sentences (FT+TR), and best-of-3 selection using either ROUGE-1 (R1)\nor SummaC as the selector. Zero-shot underperforms, as expected for task-specific summarization\n3\n"}, {"page": 4, "text": "Setting (MeQSum)\nR1\nR2\nRL\nBERT\nRead.\nSummaC\nAlignScore\nZero-shot (no FT)\n21.97\n6.48\n19.98\n0.60\n65.16\n0.28\n21.80\nFT (no TR)\n44.23\n27.36\n41.55\n0.71\n70.21\n0.31\n38.45\nFT + TR\n47.07\n29.44\n44.08\n0.72\n70.69\n0.37\n45.65\nBest-of-3 (R1 select)\n50.50\n34.38\n47.74\n0.74\n71.56\n0.40\n39.24\nBest-of-3 (SummaC)\n48.27\n31.38\n45.34\n0.73\n71.56\n0.57\n45.91\nTable 1: MeQSum results across settings. Best-of-3 improves both quality and faithfulness; SummaC-\nbased selection yields the highest factual consistency and strong alignment.\nModel\nR1\nR2\nRL\nBERT\nRead.\nSummaC\nAlignScore\nMixtral-8x7B-Inst. [Dada et al., 2024]\n32.47\n36.38\n16.86\n0.72\n–\n–\n–\nBioBART + FaMeSumm [Zhang et al., 2023]\n31.76\n11.71\n29.64\n0.74\n–\n0.46\n–\nOurs (Best-of-3)\n50.50\n34.38\n47.74\n0.74\n71.56\n0.57\n0.46\nTable 2: State-of-the-art comparison on MeQSum. Our method achieves the best performance across\nmost metrics, with strong factual consistency (SummaC) and alignment (AlignScore).\n[Abbasiantaeb et al., 2024]. Fine-tuning substantially boosts both general quality and faithfulness,\nand adding TextRank further improves alignment with source content. Selecting the best of three\ncandidates yields the strongest scores.\nTemperature. We sweep t ∈{0.1, 0.3, 0.5, 0.7, 0.9} and observe a trade-off: lower t favors ROUGE,\nhigher t favors SummaC. We adopt t=0.7 as a balanced choice, giving peak faithfulness with\ncompetitive ROUGE [Yu et al.].\nComparison to prior work. Our approach surpasses strong baselines on MeQSum, particularly in\nR1/RL and readability, while achieving competitive or superior faithfulness.\n4.2\nPerformance Analysis (BanglaCHQ-Summ)\nWe replicate the evaluation on BanglaCHQ-Summ to assess cross-lingual robustness. Zero-shot\nperformance is weak, reflecting linguistic/domain gaps. Fine-tuning improves all metrics, and FT+TR\nyields further gains. Best-of-3 selection again provides the strongest outcomes; SummaC-based\nselection maximizes faithfulness.\nSetting (Bangla)\nR1\nR2\nRL\nBERT SummaC\nZero-shot (no FT)\n19.10\n8.21\n18.97\n0.62 0.22\nFT (no TR)\n28.24\n14.22\n24.54\n0.71 0.26\nFT + TR\n30.71\n15.71\n28.95\n0.74 0.28\nBest-of-3 (R1 select)\n32.35\n16.32\n29.09\n0.76 0.29\nBest-of-3 (SummaC select)\n30.92\n15.74\n27.35\n0.73 0.32\nTable 3: BanglaCHQ-Summ results. Best-of-3 improves quality (R1/RL/BERT) and faithfulness\n(SummaC).\n4.3\nHuman Evaluation\nTo validate the reliability of generated summaries, we conducted a human evaluation on the MeQSum\ndataset with the help of a medical doctor. The evaluation focused on two questions: (1) whether\nthe summary retained all critical information from the source text, and (2) whether it was factually\nconsistent. A binary (yes/no) judgment was provided for each case. A summary was considered\nfaithful only if both conditions were met. Results showed that 82% of the summaries satisfied these\ncriteria, demonstrating strong factual alignment.\n4\n"}, {"page": 5, "text": "5\nConclusion\nWe proposed a framework that combines TextRank-based extraction, medical NER, and fine-tuned\nLLaMA-2-7B to enhance the faithfulness of medical text summarization. Experiments on English\n(MeQSum) and Bangla (BanglaCHQ-Summ) datasets show consistent gains over zero-shot and prior\nsystems in both quality and factual consistency. Human evaluation further confirmed the reliability\nof our approach, with over 80% of summaries judged as faithful. A limitation of this study is that\nexperiments were conducted on a single LLM (LLaMA-2-7B) and two languages, leaving scope for\nfuture work to explore multiple LLMs and broader multilingual settings. Future directions include\nadapting the framework to few-shot and multilingual settings, extending to other sensitive domains\n(e.g., legal and financial texts), and incorporating expert feedback for improved robustness and\npractical integration into clinical workflows.\nReferences\nZahra Abbasiantaeb, Yifei Yuan, Evangelos Kanoulas, and Mohammad Aliannejadi. Let the llms\ntalk: Simulating human-to-human conversational qa via zero-shot llm-to-llm interactions. In\nProceedings of the 17th ACM International Conference on Web Search and Data Mining, pages\n8–17, 2024.\nStergos Afantenos, Vangelis Karkaletsis, and Panagiotis Stamatopoulos. Summarization from medical\ndocuments: a survey. Artificial intelligence in medicine, 33(2):157–177, 2005.\nAsma Ben Abacha and Dina Demner-Fushman. On the summarization of consumer health questions.\nIn Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 2228–2234, Florence, Italy, July\n2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1215. URL https:\n//aclanthology.org/P19-1215.\nAmin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Kora¸s, Constantin Marc Seibold,\nKaleb E Smith, and Jens Kleesiek. Clue: A clinical language understanding evaluation for llms.\narXiv preprint arXiv:2404.04067, 2024.\nWafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed. Automatic text\nsummarization: A comprehensive survey. Expert systems with applications, 165:113679, 2021.\nYichong Huang, Xiachong Feng, Xiaocheng Feng, and Bing Qin. The factual inconsistency problem\nin abstractive text summarization: A survey. arXiv preprint arXiv:2104.14839, 2021.\nAlvi Khan, Fida Kamal, Mohammad Abrar Chowdhury, Tasnim Ahmed, Md Tahmid Rahman Laskar,\nand Sabbir Ahmed. BanglaCHQ-summ: An abstractive summarization dataset for medical queries\nin Bangla conversational speech. In Firoj Alam, Sudipta Kar, Shammur Absar Chowdhury, Farig\nSadeque, and Ruhul Amin, editors, Proceedings of the First Workshop on Bangla Language\nProcessing (BLP-2023), pages 85–93, Singapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.banglalp-1.10. URL https://aclanthology.org/2023.\nbanglalp-1.10.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual\nconsistency of abstractive text summarization. In Bonnie Webber, Trevor Cohn, Yulan He, and\nYang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 9332–9346, Online, November 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.emnlp-main.750. URL https://aclanthology.org/2020.\nemnlp-main.750.\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. Summac: Re-visiting nli-\nbased models for inconsistency detection in summarization. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing, pages 8425–8441, 2021.\nYuning Mao, Xiang Ren, Heng Ji, and Jiawei Han. Constrained abstractive summarization: Preserving\nfactual consistency with constrained generation. arXiv preprint arXiv:2010.12723, 2020.\n5\n"}, {"page": 6, "text": "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality\nin abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.\nGeorge Michalopoulos, Kyle Williams, Gagandeep Singh, and Thomas Lin. MedicalSum: A guided\nclinical abstractive summarization model for generating medical reports from patient-doctor\nconversations. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the\nAssociation for Computational Linguistics: EMNLP 2022, pages 4741–4749, Abu Dhabi, United\nArab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/\n2022.findings-emnlp.349. URL https://aclanthology.org/2022.findings-emnlp.349.\nRada Mihalcea and Paul Tarau. Textrank: Bringing order into text. In Proceedings of the 2004\nconference on empirical methods in natural language processing, pages 404–411, 2004.\nDanila Morozovskii and Sheela Ramanna. Rare words in text summarization. Natural Language\nProcessing Journal, 3:100014, 2023.\nAM Rush, SEAS Harvard, S Chopra, and J Weston. A neural attention model for sentence summa-\nrization. aclweb. In Proceedings of the 2015 conference on empirical methods in natural language\nprocessing, 2017.\nChan Xing Yu, Chan Si Yu James, and Poh Hui-Li Phyllis David. Can llms have a fever? investigating\nthe effects of temperature on llm security.\nYuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. Alignscore: Evaluating factual consistency\nwith a unified alignment function. arXiv preprint arXiv:2305.16739, 2023.\nNan Zhang, Yusen Zhang, Wu Guo, Prasenjit Mitra, and Rui Zhang. Famesumm: Investigating and\nimproving faithfulness of medical summarization, 2023.\n6\n"}]}