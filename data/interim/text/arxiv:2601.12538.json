{"doc_id": "arxiv:2601.12538", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.12538.pdf", "meta": {"doc_id": "arxiv:2601.12538", "source": "arxiv", "arxiv_id": "2601.12538", "title": "Agentic Reasoning for Large Language Models", "authors": ["Tianxin Wei", "Ting-Wei Li", "Zhining Liu", "Xuying Ning", "Ze Yang", "Jiaru Zou", "Zhichen Zeng", "Ruizhong Qiu", "Xiao Lin", "Dongqi Fu", "Zihao Li", "Mengting Ai", "Duo Zhou", "Wenxuan Bao", "Yunzhe Li", "Gaotang Li", "Cheng Qian", "Yu Wang", "Xiangru Tang", "Yin Xiao", "Liri Fang", "Hui Liu", "Xianfeng Tang", "Yuji Zhang", "Chi Wang", "Jiaxuan You", "Heng Ji", "Hanghang Tong", "Jingrui He"], "published": "2026-01-18T18:58:23Z", "updated": "2026-01-18T18:58:23Z", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.12538v1", "url_pdf": "https://arxiv.org/pdf/2601.12538.pdf", "meta_path": "data/raw/arxiv/meta/2601.12538.json", "sha256": "edc82b75fe0496cb13bf8288aac941e4437dc136a82b2dce4a7ab55d130db916", "status": "ok", "fetched_at": "2026-02-18T02:21:16.277027+00:00"}, "pages": [{"page": 1, "text": "Agentic Reasoning for Large Language Models\n♢Foundations · Evolution · Collaboration ♢\nTianxin Wei1†\nTing-Wei Li1†\nZhining Liu1†\nXuying Ning1\nZe Yang2\nJiaru Zou1\nZhichen Zeng1\nRuizhong Qiu1\nXiao Lin1\nDongqi Fu2\nZihao Li1\nMengting Ai1\nDuo Zhou1\nWenxuan Bao1\nYunzhe Li1\nGaotang Li1\nCheng Qian1\nYu Wang5\nXiangru Tang6\nYin Xiao1\nLiri Fang1\nHui Liu3\nXianfeng Tang3\nYuji Zhang1\nChi Wang4\nJiaxuan You1\nHeng Ji1\nHanghang Tong1\nJingrui He1\n1University of Illinois Urbana-Champaign\n2Meta\n3Amazon\n4Google Deepmind\n5UCSD\n6Yale\n† Equal contribution,\nCorresponding Author\nAbstract: Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-\nmaking. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world\nsettings, exemplified by standard benchmarks in mathematics and code, they struggle in open-ended and\ndynamic environments. The emergence of agentic reasoning marks a paradigm shift, bridging thought and\naction by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In\nthis survey, we provide a systematic roadmap by organizing agentic reasoning along three complementary\ndimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning\nestablishes core single-agent capabilities, including planning, tool use, and search, that operate in stable\nenvironments; self-evolving agentic reasoning examines how agents refine these capabilities through feedback,\nmemory, and adaptation in evolving settings; and collective multi-agent reasoning extends intelligence to\ncollaborative scenarios where multiple agents coordinate roles, share knowledge, and pursue shared goals.\nAcross all layers, we analyze system constraints and optimization settings by distinguishing in-context reasoning,\nwhich scales test-time interaction through structured orchestration and adaptive workflow design, from post-\ntraining reasoning, which optimizes behaviors through reinforcement learning and supervised fine-tuning. We\nfurther review and contextualize agentic reasoning frameworks in real-world applications and benchmarks\nspanning science, robotics, healthcare, autonomous research, and math, illustrating how different reasoning\nmechanisms are instantiated and evaluated across domains. This survey synthesizes agentic reasoning methods\ninto a unified roadmap that bridges thoughts and actions, offering actionable guidance for agentic systems\nacross environmental dynamics, optimization settings, and agent interaction settings. Finally, we outline\nopen challenges and future directions, situating how agentic reasoning has developed while identifying what\nremains ahead: personalization, long-horizon interaction, world modeling, scalable multi-agent training, and\ngovernance frameworks for real-world deployment.\nKeywords: Agentic AI, LLM Agent, Agentic Reasoning, Self-evolving\n§ Github: https://github.com/weitianxin/Awesome-Agentic-Reasoning\n1. Introduction\nReasoning lies at the core of intelligence, enabling logical inference, problem-solving, and decision-making\nacross interactive and dynamic settings. Large language models (LLMs) have achieved remarkable gains in\n1\narXiv:2601.12538v1  [cs.AI]  18 Jan 2026\n"}, {"page": 2, "text": "Agentic Reasoning for Large Language Models\nFigure 1: An overview of agentic reasoning.\nclosed-world domains such as mathematical problem solving and code generation. Empirically, techniques\nthat explicitize intermediate reasoning, such as Chain-of-Thought prompting, decomposition, and program-\naided solving, have significantly bolstered inference performance [1, 2, 3, 4]. Yet, these approaches often\nassume static contexts and short-horizon reasoning. Conventional LLMs lack mechanisms to act, adapt, or\nimprove in open-ended environments where information evolves over time.\nIn this survey, we systematize this evolution under the framework of Agentic Reasoning: rather than passively\ngenerating sequences, LLMs are reframed as autonomous reasoning agents that plan, act, and learn through\ncontinual interaction with their environment. This reframing unifies reasoning with acting, positioning\nreasoning as the organizing principle for perception, planning, decision, and verification. Systems such as\nReAct [5] interleave deliberation with environment interaction, tool-use frameworks enable self-directed\nAPI calling, and workflow-based agents dynamically orchestrate sub-tasks and verifiable actions [5, 6, 7].\nConceptually, this parallels the shift from static, one-shot inference to sequential decision-making under\nuncertainty. Unlike simple input-output mapping, this paradigm requires agents to plan over long horizons,\nnavigate partial observability, and actively improve through feedback [8, 9, 10].\n2\n"}, {"page": 3, "text": "Agentic Reasoning for Large Language Models\nDefinition of Agentic Reasoning\nAgentic reasoning positions reasoning as the central mechanism of intelligent agents, spanning founda-\ntional capabilities (planning, tool use, and search), self-evolving adaptation (feedback, and memory-driven\nadaptation), and collective coordination (multi-agent collaboration), realizable through either in-context\norchestration or post-training optimization.\nTo systematically characterize the environmental dynamics, we structure our survey around three comple-\nmentary scopes of agentic reasoning: foundational capabilities, self-evolution, and collective intelligence,\nspanning diverse interactive and dynamic settings. Foundational Agentic Reasoning establishes the bedrock\nof core single-agent capabilities, including planning, tool use, and search, that enable operations within stable,\nalbeit complex, environments. Here, agents act by decomposing goals, invoking external tools, and verifying\nresults through executable actions. For instance, program-aided reasoning [3] grounds logical derivations\nin code execution; repository-level systems such as OpenHands [11] integrate reasoning, planning, and\ntesting into unified loops; and structured memory modules [12, 13] transform factual recall into procedural\ncompetence by persisting intermediate reasoning traces for reuse.\nBuilding upon these foundations, Self-Evolving Agentic Reasoning enables agents to improve continually\nthrough cumulative experience. Encompassing task-specific self-improvement (e.g., via iterative critique), this\nparadigm extends adaptation to include persistent updates of internal states like memory and policy. Rather\nthan following fixed reasoning paths, agents develop mechanisms for feedback integration and memory-\ndriven adaptation to navigate evolving environments. Reflection-based frameworks such as Reflexion [14]\nallow agents to critique and refine their own reasoning processes, while reinforcement formulations such\nas RL-for-memory [15] formalize memory writing and retrieval as policy optimization. Through these\nmechanisms, agents dynamically integrate inference-time reasoning with learning, progressively updating\ninternal representations and decision policies without full retraining. This continual adaptation links\nreasoning with learning, enabling models to accumulate competence, and generalize across tasks.\nFinally, Collective Multi-Agent Reasoning scales intelligence from isolated solvers to collaborative ecosystems.\nRather than operating in isolation, multiple agents coordinate to achieve shared goals through explicit role\nassignment (e.g., manager–worker–critic), communication protocols, and shared memory systems [16,\n17]. As agents specialize in subtasks and refine each other’s outputs, collaboration amplifies reasoning\ndiversity, enabling systems to debate, resolve disagreements, and achieve consistency through natural\nlanguage-based multi-turn interactions [18, 19]. However, this complexity also introduces challenges in\nstability, communication efficiency, and trustworthiness, necessitating structured coordination frameworks\nand rigorous evaluation standards [20, 21].\nAcross all layers, we analyze system constraints and optimization settings by distinguishing two comple-\nmentary modes, corresponding to inference-time orchestration [5, 14, 22, 23, 24, 25] and training-based\ncapability optimization [26, 27, 28, 15]. In-context Reasoning focuses on scaling inference-time com-\npute: through structured orchestration, search-based planning, and adaptive workflow design, it enables\nagents to navigate complex problem spaces dynamically without modifying model parameters. Conversely,\nPost-training Reasoning targets capability internalization: it consolidates successful reasoning patterns\nor tool-use strategies into the model’s weights via reinforcement learning and fine-tuning. Together, they\nprovide an actionable roadmap for designing agents.\nBuilding on the three-layer taxonomy, agentic reasoning has begun to underpin a wide range of practical\napplications, from mathematical exploration [29, 30] and vibe coding [11, 31, 32] to scientific discovery\n3\n"}, {"page": 4, "text": "Agentic Reasoning for Large Language Models\nSurvey Scope\nThis survey reviews reasoning-empowered agentic systems where reasoning drives adaptive behavior. We\nanalyze these systems through two complementary optimization modes:\n• In-context Reasoning: scales inference-time interaction through structured orchestration and plan-\nning without parameter updates.\n• Post-training Reasoning: internalizes reasoning strategies into model parameters via reinforcement\nlearning and fine-tuning.\nOur scope covers methodologies embedding these modes into planning, memory, and self-improvement\nacross single-agent and multi-agent contexts. This survey summarizes progress up to 2025.\n[33, 34, 35], embodied robotics [36, 37, 38], healthcare [39, 40], and autonomous web exploration [41, 42].\nThese applications expose distinct reasoning demands shaped by domain-specific data modalities, interaction\nconstraints, and feedback loops, motivating diverse system designs [43, 44] that integrate planning, tool use,\nsearch, reflection, memory mechanisms, and multi-agent coordination. On the other hand, the benchmark\nlandscape has emerged to evaluate agentic reasoning, ranging from targeted tests that isolate individual\nagentic capabilities to application-specific benchmarks that assess end-to-end behavior in domain-specific\nenvironments and scenarios [45, 46, 47, 48, 20, 21, 49, 50].\nTogether, this survey synthesizes agentic reasoning methods into a unified roadmap that bridges reasoning\nand acting. We systematically characterize these methods across the complementary scopes of foundational,\nself-evolving, and collective reasoning, while distinguishing between in-context and post-training optimiza-\ntion modes. We further contextualize this roadmap through representative applications and evaluation\nbenchmarks, illustrating how different agentic reasoning mechanisms are instantiated and assessed across\nrealistic domains and task settings. Finally, we outline open challenges and future directions, identifying key\nfrontiers such as personalization, long-horizon interaction, world modeling, scalable multi-agent training,\nand governance frameworks for real-world deployment.\nContributions\nThis survey makes the following contributions:\n• Conceptual framing: We formalize the paradigm of Agentic Reasoning, spanning foundational,\nself-evolving, and collective reasoning layers.\n• Systematic review: We analyze single-agent, adaptive, and multi-agent systems, emphasizing\nreasoning-centered workflow orchestration across in-context and post-training dimensions.\n• Applications and evaluation: We review real-world applications and benchmarks to illustrate the\ninstantiation and evaluation of agentic reasoning mechanisms.\n• Future agenda: We identify emerging challenges in robustness, trustworthiness, and efficiency,\noutlining directions for the next generation of adaptive and collaborative agents.\n4\n"}, {"page": 5, "text": "Contents\n1\nIntroduction\n1\n2\nFrom LLM Reasoning to Agentic Reasoning\n7\n2.1\nPositioning Our Survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nPreliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3\nFoundational Agentic Reasoning\n10\n3.1\nPlanning Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.1.1\nIn-context Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.1.2\nPost-training Planning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.2\nTool-Use Optimization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.2.1\nIn-Context Tool-integration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.2.2\nPost-training Tool-integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.2.3\nOrchestration-based Tool-integration . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.3\nAgentic Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.3.1\nIn-Context Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.3.2\nPost-Training Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4\nSelf-evolving Agentic Reasoning\n20\n4.1\nAgentic Feedback Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.1.1\nReflective Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.1.2\nParametric Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.1.3\nValidator-Driven Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.2\nAgentic Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.2.1\nAgentic Use of Flat Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.2.2\nStructured Use of Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.2.3\nPost-training Memory Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.3\nEvolving Foundational Agentic Capabilities\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.3.1\nSelf-evolving Planning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.3.2\nSelf-evolving Tool-use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.3.3\nSelf-evolving Search\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n5\nCollective Multi-agent Reasoning\n29\n5.1\nRole Taxonomy of Multi-Agent Systems (MAS)\n. . . . . . . . . . . . . . . . . . . . . . . .\n30\n5.1.1\nGeneric Roles\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n5.1.2\nDomain-Specific Roles\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.2\nCollaboration and Division of Labor\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.2.1\nIn-context Collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n5.2.2\nPost-training Collaboration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.3\nMulti-Agent Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.3.1\nFrom Single-Agent Evolution to Multi-Agent Evolution . . . . . . . . . . . . . . . .\n38\n5.3.2\nMulti-agent Memory Management for Evolution . . . . . . . . . . . . . . . . . . .\n39\n5.3.3\nTraining Multi-agent to Evolve\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n6\nApplications\n43\n6.1\nMath Exploration & Vibe Coding Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n5\n"}, {"page": 6, "text": "Agentic Reasoning for Large Language Models\n6.2\nScientific Discovery Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n6.3\nEmbodied Agents\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n6.4\nHealthcare & Medicine Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n6.5\nAutonomous Web Exploration & Research Agents . . . . . . . . . . . . . . . . . . . . . . .\n57\n7\nBenchmarks\n64\n7.1\nCore Mechanisms of Agentic Reasoning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n7.1.1\nTool Use\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n7.1.2\nSearch\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n7.1.3\nMemory and Planning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n7.1.4\nMulti-Agent System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n7.2\nApplications of Agentic Reasoning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n7.2.1\nEmbodied Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n7.2.2\nScientific Discovery Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n7.2.3\nAutonomous Research Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n7.2.4\nMedical and Clinical Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n7.2.5\nWeb Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n7.2.6\nGeneral Tool-Use Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n8\nOpen Problems\n72\n8.1\nUser-centric Agentic Reasoning and Personalization . . . . . . . . . . . . . . . . . . . . .\n72\n8.2\nLong-horizon Agentic Reasoning from Extended Interaction\n. . . . . . . . . . . . . . . .\n73\n8.3\nAgentic Reasoning with World Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n8.4\nMulti-agent Collaborative Reasoning and Training . . . . . . . . . . . . . . . . . . . . . .\n73\n8.5\nLatent Agentic Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n8.6\nGovernance of Agentic Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\nSurvey Structure\nThis survey is organized as follows:\n• Sec. 2: Preliminaries. Key background on LLM and Agentic reasoning.\n• Sec. 3: Foundational Agentic Reasoning. Core single-agent capabilities including planning, tool\nuse, and search.\n• Sec. 4: Self-evolving Reasoning. Feedback, memory, and continual adaptation mechanisms that\nenhance reasoning over time.\n• Sec. 5: Collective Multi-agent Reasoning. Coordination, communication, and shared-memory\nstrategies for collaboration.\n• Sec. 6: Applications. Reasoning-empowered applications across science, robotics, healthcare,\nautonomous research and math/code.\n• Sec. 7: Benchmarks. Datasets, metrics, and evaluation protocols for assessing reasoning and agentic\nabilities.\n• Sec. 8: Open Problems. Challenges and future directions for AI Agent reasoning.\n6\n"}, {"page": 7, "text": "Agentic Reasoning for Large Language Models\n2. From LLM Reasoning to Agentic Reasoning\nTraditional reasoning with large language models (LLMs) is typically formulated as a one-shot or few-shot\nprediction task over static inputs. These models rely on scaling test-time computation, improving accuracy\nby increasing model size or inference budget, but without the ability to interact, remember, or adapt to\nchanging goals. Methods such as prompt engineering, in-context learning, and chain-of-thought prompting\nhave made reasoning more explicit, yet conventional LLMs remain passive sequence predictors that operate\nwithin fixed prompts.\nAgentic reasoning, in contrast, emphasizes scaling test-time interaction. Instead of depending solely on\ninternal parameters, agentic systems reason through action: invoking tools, exploring alternatives, updating\nmemory, and integrating feedback. This transforms inference into an iterative process that includes decision\nsteps, reflection, and learning from experience. Reasoning becomes a dynamic loop that connects the model,\nmemory, and environment.\nTable 1: Contrasting capabilities of LLM reasoning and agentic reasoning.\nDimension\nLLM Reasoning\n↔\nAgentic Reasoning\nParadigm\npassive\n↔\ninteractive\nstatic input\n↔\ndynamic context\nComputation\nsingle pass\n↔\nmulti step\ninternal compute\n↔\nwith feedback\nStatefulness\ncontext window\n↔\nexternal memory\nno persistence\n↔\nstate tracking\nLearning\noffline pretraining\n↔\ncontinual improvement\nfixed knowledge\n↔\nself evolving\nGoal Orientation\nprompt based\n↔\nexplicit goal\nreactive\n↔\nplanning\nThis transition marks a conceptual shift: reasoning no longer scales through static capacity, but through\nstructured interaction that enables planning, adaptation, and collaboration across time and tasks.\n2.1. Positioning Our Survey\nWhile several recent surveys have examined LLM reasoning or agent architectures [51, 52, 53, 54, 55, 56,\n57, 58, 59], our work focuses specifically on agentic reasoning as a unified paradigm for understanding\nreasoning as interaction. We position this survey at the intersection of model-centric reasoning and system-\nlevel intelligence, aiming to bridge prior discussions on reasoning mechanisms and agent architectures.\nRelation to LLM Reasoning Surveys. Existing surveys on LLM reasoning mainly investigate how to elicit\nor enhance reasoning within a model’s internal computation process. For example, Huang and Chang\n[51], Chen et al. [52], Xu et al. [53], Ke et al. [54] summarize prompting and scaling techniques such\nas chain-of-thought, reinforcement post-training, and long-context reasoning, emphasizing how LLMs can\n7\n"}, {"page": 8, "text": "Agentic Reasoning for Large Language Models\nlearn to reason better through inference-time supervision or post-training alignment. These works improve\nthe internal expressiveness of reasoning traces but typically remain within static inference settings, where\nreasoning unfolds in a single forward pass without external interaction. In contrast, our survey examines\nhow reasoning extends beyond text generation, encompassing dynamic planning, adaptive memory, and\nfeedback-driven behavior during deployment.\nRelation to AI Agent Surveys. Several contemporary surveys have begun to explore LLM-based agents from\narchitectural or system perspectives [56, 57, 58, 59]. These works analyze how agents employ reinforcement\nlearning, planning, and tool-use modules to operate in complex environments. For instance, Zhang et al.\n[56], Lin et al. [57] focus on reinforcement learning for agentic search and decision-making, while Fang\net al. [58], Gao et al. [59] emphasize self-evolving and lifelong agentic systems that continuously learn from\ninteraction. Our focus complements these perspectives by centering on the reasoning process that these\narchitectures enable, specifically how interaction, feedback, and collaboration transform static inference\ninto adaptive reasoning. Rather than viewing reasoning as an implicit by-product of architectural design,\nwe treat it as the unifying mechanism that links single-agent reinforcement, multi-agent coordination, and\nself-evolving intelligence.\nIn summary, our survey provides a reasoning-centric lens on intelligent agency. We examine how foundational\nreasoning mechanisms, post-training adaptation, and long-term self-evolution jointly constitute the basis of\nagentic reasoning, illustrating the transition from static prediction to interactive, adaptive, and continually\nimproving intelligence.\n2.2. Preliminaries\nThis subsection formalizes the transition from static language modeling to agentic reasoning. To align with\nthe three-layered dimensions (Foundational, Self-Evolving, Collaboration) outlined in the introduction, we\nunify these capabilities under a single control-theoretic framework.\nFormalizing Agentic Reasoning: A Latent-Space View.\nStandard approaches often conflate the agent’s\ncontext with the environment state. We model the environment as a Partially Observable Markov Decision\nProcess (POMDP) and introduce an internal reasoning variable to expose the “think–act” structure of agentic\npolicies. Concretely, we consider the tuple ⟨X , O, A, Z, M, T , Ω, R, γ⟩, where X is the latent environment\nstate space (unobservable to the agent), O is the observation space (e.g., user queries, API returns), A is the\nexternal action space (e.g., tool invocation, final answer), Z is a reasoning trace space (e.g., latent plans,\noptionally verbalized as chain-of-thought), and M is the agent’s internal memory/context space (e.g., a\nsufficient statistic of interaction history). T and Ωdenote the transition and observation kernels, R the\nreward, and γ ∈(0, 1) the discount factor.\nAt timestep t, the agent conditions on a history ht = (o≤t, z<t, a<t) (i.e., ot is observed before generating zt\nand then at). Equivalently, the history can be summarized by an internal memory state mt ∈M. Crucially,\nwe distinguish external actions from internal reasoning. We factorize the policy as\nπθ(zt, at | ht) = πreason(zt | ht)\n⏟\n ⏞\n \nInternal Thought\n· πexec(at | ht, zt)\n⏟\n ⏞\n \nExternal Action\n.\n(1)\nThis decomposition highlights the core shift in agentic systems: performing computation in Z (thinking) be-\nfore committing to A (acting). The objective remains maximizing the expected return J(θ) = Eτ\n[︀∑︀\nt≥0 γtrt\n]︀\n.\n8\n"}, {"page": 9, "text": "Agentic Reasoning for Large Language Models\nIn-Context Reasoning: Inference-Time Search.\nIn this regime, model parameters θ are frozen. The agent\noptimizes the reasoning trajectory by searching over Z to maximize a heuristic value function ˆv(ht, z). We\nmodel inference as selecting a trajectory τ = (h0, z0, a0, h1, z1, a1, . . .). Methods like ReAct [5] perform greedy\ndecoding over alternating thoughts z and actions a. Tree-of-Thoughts (ToT [4]) and related MCTS-style\napproaches treat partial thoughts as nodes u ∈U (e.g., a representation derived from (ht, zt)) and search for\nan optimal path:\nτ⋆∈arg max\nτ\n∑︁\nt\nˆvϕ(ut),\n(2)\nwhere ˆvϕ is a heuristic evaluator or verifier. This corresponds to planning in Z without updating the policy\nparameters.\nPost-Training: Policy Optimization.\nThis paradigm optimizes θ to align the policy with long-horizon\nrewards rt (e.g., correctness, safety), including reasoning models (e.g., DeepSeek-R1 [60]) and learning-to-\nsearch systems (e.g., Search-R1 [27], DeepRetrieval [61]) that train multi-turn reasoning or tool use with RL.\nWhile PPO [62] is standard, Group Relative Policy Optimization (GRPO) [63]-based methods are widely\nused for reasoning tasks. GRPO eliminates the value network by constructing advantages from group-relative\nrewards. For a group of G sampled outputs {yi}G\ni=1 from the same prompt q, a common GRPO objective is:\nLGRPO(θ) = Eq∼P(Q)\n[︃\n1\nG\nG\n∑︁\ni=1\n(︀\nmin\n(︀\nρi ˆAi, clip(ρi, 1 −ϵ, 1 + ϵ) ˆAi\n)︀−β DKL(πθ ∥πref)\n)︀\n]︃\n,\n(3)\nwhere ρi =\nπθ(yi|q)\nπθold(yi|q) and the group-normalized advantage is\nˆAi = ri −µ\nσ + δ ,\nµ = 1\nG\nG\n∑︁\nj=1\nrj,\nσ =\n⎯\n⎸\n⎸\n⎷1\nG\nG\n∑︁\nj=1\n(rj −µ)2,\n(4)\nwith δ > 0 a small constant for numerical stability. Advanced methods such as ARPO [64] and DAPO [65]\nextend this framework to handle sparse rewards and improve stability in complex tool-use environments\n(e.g., via replay/rollout strategies and decoupled clipping).\nCollective Intelligence: Multi-Agent Reasoning.\nWe extend the single-agent formulation to a decentralized\npartially observable multi-agent setting, commonly formalized as a Dec-POMDP. The core distinction lies\nin expanding each agent’s observation to include a communication channel C. For a system of N agents,\nthe joint policy π is composed of individual policies πi, where agent i’s observation oi\nt explicitly includes\ncommunicative messages c−i\nt−1 generated by peers. Crucially, in agentic MARL, communication is not merely\nsignal transmission but an extension of the reasoning process: one agent’s external action can act as a prompt\nthat triggers another agent’s internal reasoning chain. Existing frameworks like AutoGen [66] and CAMEL\n[67] represent static role-playing with fixed policies. Recent agentic RL advances (e.g., GPTSwarm [68],\nMaAS, agents trained via PPO/GRPO [69]) aim to optimize this joint reasoning distribution. The challenge\nshifts from single-agent planning to mechanism design: optimizing the communication topology and\nincentive structures to align decentralized reasoning processes πi\nreason toward a coherent global objective,\noften utilizing Centralized-Training/Decentralized-Execution (CTDE) paradigms to stabilize the emergence\nof cooperative behaviors.\n9\n"}, {"page": 10, "text": "Agentic Reasoning for Large Language Models\nSelf-Evolving Agents: The Meta-Learning Loop.\nWhile foundational agents optimize reasoning z within\nan episode, self-evolving agents optimize the agent system itself across episodes k = 1, . . . , K. Let Sk denote\nthe evolvable system state (e.g., explicit memories, tool libraries, or code). A generic meta-update rule is\nSk+1 ←U(Sk, τk, Fk),\n(5)\nwhere Fk represents environmental feedback (rewards, execution errors) and Sk represents the evolvable\nstate. We categorize self-evolution by the nature of S:\n• Verbal Evolution: S consists of textual reflections or guidelines. Methods like Reflexion [14] update S by\nsynthesizing error logs into linguistic cues that condition future reasoning policies.\n• Procedural Evolution: S consists of a library of executable tools or skills. Agents like Voyager [36] evolve\nby synthesizing new code-based skills, expanding the action space A permanently.\n• Structural Evolution: S consists of the agent’s source code or architecture itself. Advanced methods like\nAlphaEvolve [70] treat the agent’s code as a hypothesis space, using an LLM as a mutation operator to\nsearch for superior reasoning algorithms.\nThis framework unifies these diverse approaches as gradient-free or gradient-based optimization steps over\nthe agent’s explicit memories and artifacts (and optionally parameters), closing the loop between experience\nand competence.\n3. Foundational Agentic Reasoning\nAgentic reasoning originates from the behavior of a single agent. Before discussing adaptation and collabora-\ntion, we focus on how an individual agent translates reasoning into structured action through three core\ncomponents: planning, search, and tool use. In this setting, the agent is not a passive text generator but an\nautonomous problem solver that formulates plans, explores alternatives through retrieval or environment\nsearch, and leverages tools to execute grounded operations. Together, these mechanisms establish the\nfoundation of agentic reasoning, linking abstract deliberation with verifiable action.\nA canonical foundational workflow can be viewed as an iterative cycle that interleaves planning (goal\ndecomposition and task formulation), tool use (invoking external systems or APIs to act on the world) and\nsearch (retrieval and exploration for decision support), Reasoning serves as the organizing principle across\nthese stages, determining when to plan, what to retrieve, and how to act, transforming static inference into\ninteractive decision-making.\nBy analyzing these components, we clarify how structured reasoning elevates a static LLM into an autonomous,\ngoal-driven agent. The next section introduces self-evolving reasoning, where feedback and memory enable\ncontinual adaptation and extension of these foundational capabilities. Subsequently, we examine collective\nreasoning, in which multiple agents coordinate through roles, communication, and shared memory to\nachieve objectives beyond individuals.\n3.1. Planning Reasoning\nPlanning is a central component of intelligent behavior, enabling agents to decompose problems, sequence\ndecisions, and navigate complex environments with foresight. Recent research has increasingly explored\n10\n"}, {"page": 11, "text": "Agentic Reasoning for Large Language Models\nFigure 2: Overview of Planning Reasoning in LLM agents, categorized into in-context planning and post-\ntraining planning.\nplanning in the context of large language models (LLMs), either as autonomous agents or as components\nin broader systems. In this section, we categorize existing work in agent planning for reasoning into six\nmethodological styles, where each category highlights a distinct planning strategy that supports complex\nagentic reasoning.\n3.1.1. In-context Planning\nWorkflow Design.\nWorkflow-based approaches often emphasize structuring the overall planning process\ninto distinct stages (e.g., perception, reasoning, execution, verification), which are either explicitly scaffolded\nor learned implicitly. For example, [72, 73, 71, 92] design planning pipelines that decompose task solving\ninto subtasks, often leveraging a deliberate plan-and-act framework. Similarly, [2, 93, 75, 7] rely on\nstructured prompting to sequentialize tasks and guide reasoning progression. Methods like [94] use structured\ntransitions between diverse “X-of-Thought” strategies. PERIA [95] combines perception, imagination, and\naction in a unified multimodal workflow. Others such as [96] explicitly target long-horizon planning through\nstructured sequencing, while [97] build workflows for code-related planning.\nThese workflows are then grounded by a reactive controller that iteratively consumes the current state\nand interleaves reasoning with actions: in web automation, agents follow inspect-reason-act-observe loops\n[5, 49], with robustness improved by dynamically adapting in-context examples [98]; in code, agents decide\nimmediate executions/API calls, read outputs or errors, and refine step-by-step [99, 78, 14, 79, 100, 101, 102,\n103, 104]; in robotics, monitors trigger on-the-fly safety interventions and VLM-guided subgoal execution\nwith real-time adjustment [87, 105]. This reactive workflow view unifies scripted stage design with online\nadaptation: the workflow provides interpretable structure and interfaces (what is done when), while the\nreactive loop supplies closed-loop grounding and error recovery (how it is done in context). The approach\nis broadly effective yet can accumulate errors over long horizons, motivating incremental verification and\nmemory within the workflow to stabilize execution.\nTree Search / Algorithm Simulation.\nTree-based search strategies, especially BFS, DFS, A*, MCTS, and\nbeam search, have become prominent as interpretable and effective planning scaffolds. Several works\nsimulate tree traversal algorithms to mimic deliberative processes: [4, 106, 107, 108] apply breadth- or\ndepth-first strategies to explore structured thought trees. A*-like guided expansions appear in [109, 110, 111],\nproviding heuristic-driven planning with state evaluation. Besides that, MCTS is heavily explored in agentic\nresearch: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123] use MCTS or its variations for\ncontrolled exploration and improved reasoning fidelity. Beam search is leveraged in [124, 125, 126] to\nprune and prioritize reasoning trajectories efficiently. Other tree-search-inspired works include [127] which\n11\n"}, {"page": 12, "text": "Agentic Reasoning for Large Language Models\nTable 2: Representative Agentic Planning systems categorized by Modality, Structure, Format, and Tool.\nMethod\nStructure\nFormat\nTool\nModality I: Language Agents (e.g., Search Agents, Code Agents)\nReWOO [71]\nDecomposed\nNatural Language\nNone\nReflexion [14]\nSequential\nNatural Language\nNone\nLLM+P [72]\nSequential\nFormal Language\nNone\nIPC [73]\nSequential\nFormal Language\nNone\nToT [4]\nTree\nNatural Language\nNone\nGoT [74]\nGraph\nNatural Language\nNone\nAoT [75]\nGraph\nNatural Language\nNone\nHTP [76]\nHypertree\nNatural Language\nRetrieval\nRefPlan [77]\nTree\nConstrained Space\nNone\nGorilla [78]\nSequential\nProgramming Language\nRetrieval, API\nCodeNav [79]\nSequential\nProgramming Language\nCode Indexer, Code Search\nPoG [80]\nGraph\nNatural Language\nKnowledge Graph\nTool-Planner [81]\nSequential\nNatural Language\nTool Cluster\nModality II: Visual/Multimodal Agents (e.g., GUI Agents, Embodied Agents)\nVisualPredictor [82]\nTree\nFormal Language\nNone\nLLM-Planner [83]\nSequential\nFormal Language\nObject Detector, KNN\nAgent-E [84]\nSequential\nFormal Language\nDOM Grounder, Screenshot\nAgent S [85]\nHierarchical\nNatural Language\nAPI, Search, Memory\nExRAP [86]\nSequential\nNatural Language\nMemory\nAESOP [87]\nReactive\nNatural Language\nAnomaly Detector\nHRV [88]\nHierarchical\nFormal Language\nSymbolic Verifier\nBehaviorGPT [89]\nSequential\nVisual Features\nWorld Model\nDino-WM [90]\nTree\nVisual Features\nWorld Model\nFLIP [91]\nSequential\nVisual Features\nLanguage Model\nuses learned search policies and [128] which differentiates between fast (reactive) and slow (deliberative)\nplanning. These methods mirror traditional algorithmic planning, grounding LLMs’ search processes in\nclassical decision-making frameworks.\nThis search-over-hierarchy view maps cleanly onto domain systems. In the web setting, planner-executor\narchitectures generate high-level subtask trees in natural language and bind leaves to DOM-grounded actions,\noften with memory to persist context [84, 129, 85]. For code agents, hierarchical task trees and pseudo-code\nplans recursively break problems into compilable/editable units, while structured pipelines embed hierarchical\nRL or MCTS within the tree to choose promising edits and verification paths [76, 22, 130, 131, 132]. In\nrobotics, behavior trees and high-level goal decomposition translate language instructions into subgoal\nsequences executed by low-level controllers and skills [133, 134, 135, 136, 137].\nTaken together, hierarchical tree-search couples plan synthesis (node expansion, heuristic/evidence-based\n12\n"}, {"page": 13, "text": "Agentic Reasoning for Large Language Models\nselection) with plan realization (leaf grounding and feedback), yielding interpretable, long-horizon agents\nthat can backtrack, refine, and verify before committing to irreversible actions, while remaining flexible\nenough to incorporate learned policies and memory for efficiency and robustness.\nProcess Formalization.\nFormalizing planning through symbolic representations, programming languages,\nor logic frameworks ensures compositionality, interpretability, and generalization. Several works encode\nplans as code-like artifacts or PDDL programs: [138, 139, 140, 97, 141, 142] incorporate symbolic logic\nor procedural programming into LLM prompting or output generation. These representations enable\ndownstream tool execution and interface more cleanly with classical planners or robot controllers. PDDL-\nbased formulations explicitly bridge LLM planning with well-established planning ecosystems, as in [139, 140].\nCodePlan [97] highlights the use of program synthesis to scaffold long-horizon reasoning. Such formalization\nprovides structural scaffolds for agent behavior and often enhances explainability and robustness of the\ngenerated plans.\nDecoupling / Decomposition.\nDecoupling strategies aim to modularize complex planning into separa-\nble components such as goal recognition, memory retrieval, and plan refinement. Notably, ReWOO [71]\nexplicitly separates observation and reasoning modules to optimize for efficiency. Similarly, works like\n[143, 144, 145, 146, 147, 142, 148] break reasoning into reusable or hierarchical abstractions. [76] pro-\nmotes hierarchical thinking through hypertrees, while [82] abstracts the world with symbolic predicates to\nreduce planning burden. Others, such as [149] and [119], decompose via latent variables or state spaces.\nThese decompositions not only enhance tractability, but also align with neural-symbolic hybrid frameworks.\nThey are especially common in long-horizon or multi-agent planning scenarios, such as [150, 151].\nExternal Aid / Tool Use.\nMany systems leverage external structures or tools to aid planning, including\nretrieval-augmented generation (RAG), knowledge graphs, world models, and general-purpose tool use.\nKnowledge-augmented frameworks like [80, 88, 181, 182, 143] inject structured representations (e.g., graphs,\nscene layouts) into the LLM context. RAG-style systems [86, 183, 184] retrieve relevant knowledge to support\ncontinual instruction planning. World model-based agents such as [112, 138, 185, 89, 90, 91, 186, 187]\nlearn or leverage environment models for model-based planning. Tool-oriented frameworks like HuggingGPT\n[7], Tool-Planner [81], and RetroInText [148] use external APIs or modular toolchains to support planning\nexecution. These systems often reflect agent-environment interaction and capitalize on external resources to\nscaffold or augment LLM capabilities.\n3.1.2. Post-training Planning\nReward Design / Optimal Control.\nFinally, planning as optimization entails designing suitable reward\nstructures and solving for optimal behavior using RL or control-theoretic tools. Reflexion [14], Reflect-\nthen-Plan [77], and Rational Decision Agents [188] incorporate utility-based learning to guide planning\nbehavior. Reward modeling appears in works such as [189], while others like [190] emphasize reward\nshaping. Optimal control is tackled explicitly in [191, 192, 193, 194], and trajectory optimization via\ndiffusion models is seen in [195, 196, 197]. Offline RL methods like [119, 198, 147] leverage pretrained\ndynamics or cost models. The control-theoretic orientation in these works complements symbolic or heuristic\napproaches by optimizing over continuous, structured, or learned reward spaces.\n13\n"}, {"page": 14, "text": "Agentic Reasoning for Large Language Models\nTable 3: Representative Tool-Use Optimization systems categorized by Integration Stage, Learning Type, and\nTool Strategy.\nMethod\nStage\nLearning\nTool Strategy\nModality I: In-Context Integration\nReAct [5]\nInference\nPrompting\nInterleaved reasoning–action\nART [199]\nInference\nFew-shot\nRetrieved multi-step demos\nChatCoT [200]\nInference\nPrompting\nCoT with tool calls\nGEAR [201]\nInference\nDelegation\nLight model for tool selection\nAVATAR [202]\nInference\nContrastive\nIn-context tool reasoning\nModality II: Post-Training Integration\nToolformer [6]\nPost-train\nSelf-sup. + SFT\nSelf-generated API calls\nToolLLM [203]\nPost-train\nSFT\nLarge-scale API demos\nToolAlpaca [204]\nPost-train\nSFT\nSimulated dialogues\nReSearch [205]\nPost-train\nRL + Reflec.\nAdaptive retrieval reasoning\nReTool [206]\nPost-train\nRL\nReinforced code execution\nToolRL [207]\nPost-train\nRL\nMulti-tool policy learning\nModality III: Orchestration-based Integration\nHuggingGPT [7]\nSystem\nPlanner–Exec.\nMulti-tool coordination\nTaskMatrix.AI [208]\nSystem\nPlanner\nMassive API ecosystem\nToolPlanner [81]\nSystem\nRL\nPlan-before-act framework\nOctoTools [209]\nSystem\nRule-based\nHierarchical orchestration\nToolExpNet [210]\nSystem\nEmbedding\nExperience-based selection\nToolChain* [211]\nSystem\nSearch\nA* decision over tools\n3.2. Tool-Use Optimization\nTool use optimization is the capacity of an agent to augment its intrinsic capabilities by intelligently invoking\nexternal modules. This allows agents to overcome limitations such as outdated knowledge, inability to\nperform precise calculations, or lack of access to private information. The core challenge lies in the agent’s\nability to reason about when to use a tool, which tool to select from a library, and how to generate a valid call.\nIn this section, we examine existing approaches to tool use optimization, which can be broadly classified into\nthree styles: in-context tool-integration, post-training tool-integration, and orchestration-based tool-integration.\n3.2.1. In-Context Tool-integration\nThe in-context demonstration paradigm is a training-free approach to empowering LLMs with new capabilities\nat inference time. This method leverages the remarkable in-context learning ability of modern LLMs, guiding\na frozen, off-the-shelf model to perform complex tasks by providing carefully crafted instructions, examples,\nand contextual information directly in the prompt.\n14\n"}, {"page": 15, "text": "Agentic Reasoning for Large Language Models\nFigure 3: Comparison between traditional LLM and agentic tool-use systems. While traditional models op-\nerate in a closed world with fixed reasoning, agentic tool-use systems enable dynamic selection, orchestration,\nand integration of external tools, allowing agents to extend reasoning, improve precision, and dynamically\nadapt across domains.\nInterleaving Reasoning and Tool Use.\nThe foundation of in-context agentic reasoning lies in augmenting\nthe Chain-of-Thought (CoT) process with the ability to take action.[1]. ChatCoT [200] formalizes this\nparadigm by structuring reasoning traces as alternating \"thought-tool-observation\" steps in natural language,\nallowing LLMs to reflect on intermediate outputs and dynamically plan the next tool query. While CoT enables\nLLMs to break down problems into intermediate reasoning steps, it operates in a closed world, limited by the\nmodel’s internal knowledge. The key innovation in agentic tool use is to interleave these reasoning steps with\nactions (tool calls), creating a dynamic loop that allows the agent to interact with external environments to\ngather information and execute tasks [212, 213]. ReAct [5] introduced the \"Reasoning+Acting\" synergy.\nThis approach enables the model to use reasoning to create, track, and adjust its action plans, while the\nactions allow it to interface with and gather information from external environments like knowledge bases\nor the web. Similarly, ART [199] provides a structured approach by maintaining a library of successful task\ndemonstrations. For a new task, ART retrieves a relevant multi-step exemplar and uses it as a few-shot\nprompt, guiding the LLM to follow a proven reasoning and tool-use path.\nOptimizing Context for Tool Interaction.\nWhile the foundational interleaved loop is powerful, its per-\nformance degrades when agents must handle large or complex toolsets. A significant branch of research\naddresses this by optimizing the in-context information provided to the agent. Recent studies demonstrate\nthat well-written tool documentation enables LLMs to utilize new tools in a zero-shot manner [214, 215].\nThis finding aligns with the key insight that LLMs, much like humans, benefit from clear and concise in-\nstructions. Alternatively, GEAR [201] introduces a computationally efficient, training-free algorithm that\ndelegates the tool selection process to a small language model while reserving the more powerful LLM for\nthe final reasoning step to reduce costs. AVATAR [202] enhances the robustness of this choice by prompting\nthe agent to perform in-context \"contrastive reasoning\" before acting.\nWhile these in-context methods are flexible, their performance is ultimately bounded by the inherent\ncapabilities of the frozen LLM and the length of its context window. Consequently, subsequent research has\nfocused on post-training methods.\n15\n"}, {"page": 16, "text": "Agentic Reasoning for Large Language Models\n3.2.2. Post-training Tool-integration\nTool integration [5, 216, 217] with post-training techniques has emerged as a key strategy for addressing\nthe inherent limitations of LLMs or LRMs, such as outdated knowledge, limited computational precision, and\nshallow multi-step reasoning. By learning how to interact with external tools, reasoning models can dynamically\naccess up-to-date information, execute precise symbolic or numerical computations, and decompose complex\ntasks into grounded, tool-assisted reasoning steps [218, 219, 9, 220, 221]. With tools as intermediaries,\nmodels are enriched and augmented by external capabilities, enabling the generation of more accurate and\ngeneralizable agentic reasoning trajectories [222, 215, 223].\nBootstrapping of Tool Use via SFT.\nEarly works on tool-integration [5, 6, 203, 204, 224, 225, 226, 227,\n228] primarily apply supervised fine-tuning (SFT) over curated tool-use reasoning steps, where models were\ntrained to imitate demonstrations of search queries, code executions, or API calls. The SFT stage provided\nan initial competency in invoking tools, interpreting tool outputs, and integrating the results into coherent\nreasoning chains [225, 14]. For example, Toolformer [6] introduces a self-supervised framework in which\nlarge language models generate, validate, and retain useful API calls within unlabeled text, followed by\nfine-tuning on the filtered data to enhance factual accuracy and practical utility. ToolLLM [203] further\nscales SFT training to over 16,000 real-world APIs, applying supervised fine-tuning on massive curated\ndemonstrations to endow models with robust planning and invocation abilities. ToolAlpaca [204] extends\nthe idea to compact LLMs by automatically constructing a diverse toolset and generating multi-turn tool-\nuse dialogues via multi-agent simulation, followed by fine-tuning to enable generalized tool-use even for\npreviously unseen tools. While effective at bootstrapping tool-awareness, applying SFT along suffers from\noverfitting to the specific patterns in the training data [229, 230, 231, 172], leading to brittle tool-selection\nstrategies and limited adaptability in unseen downstream application scenarios [232, 207, 233].\nMastery of Tool Use via RL.\nRecent studies [234, 207, 235, 205, 236, 27, 237, 206] leverages reinforcement\nlearning (RL) during model post-training to go beyond imitation and achieve mastery in tool-integrated\nreasoning. With the integration of RL, models refine their tool-use strategies through outcome-driven rewards,\nlearning when, how, and which tools to invoke via trial and error [205, 238, 206, 239]. For instance, SWE-RL\n[235] optimizes code-editing policies on large-scale software evolution data, improving not only software\nissue resolution but also general reasoning skills. ReSearch [205] embeds search operations into multi-hop\nreasoning chains, enabling adaptive retrieval during complex QA. ReTool integrates real-time code execution\ninto reasoning rollouts, leading to optimal performance on advanced math reasoning benchmarks. ToolRL\n[207] generalizes this paradigm to diverse toolsets by introducing principled reward designs for stable\nand scalable multi-tool learning. Across these settings, RL has been shown to yield more robust, adaptive,\nand generalizable tool-use policies than SFT alone, often transferring effectively to out-of-domain tasks\n[240, 241, 242, 243, 244].\n3.2.3. Orchestration-based Tool-integration\nIn real-world applications, tool use within complex systems often extends beyond the single-model, single-tool\nsetting, requiring orchestration among multiple tools to complete complex tasks. This orchestration typically\ninvolves planning, sequencing, and managing dependencies across tools, i.e., ensuring that intermediate\noutputs are passed and transformed appropriately. Several early works [7, 208, 245] explore this direction\nby devising strategies for the coordinated use of multiple tools, enabling systems to solve multi-stage tasks\nthat no single tool can handle in isolation. Specifically, HuggingGPT [7] employs a centralized agent that\n16\n"}, {"page": 17, "text": "Agentic Reasoning for Large Language Models\nleverages a language interface to plan which tools to invoke and when, enabling the solution of complex\ntasks requiring multiple tools in sequence. TaskMatrix.AI [208] connects foundation models with millions\nof APIs, using the models to generate task-solution outlines and automatically matching certain sub-tasks\nto off-the-shelf models and systems with specialized functionalities. ToolkenGPT [209] augments frozen\nlanguage models with massive tool sets by encoding each tool as a special token during next-token prediction.\nAgentic Pipelines for Tool Orchestration.\nThere are many frameworks designed to enable LLMs to call\nand orchestrate tools effectively. Most of the current agentic paradigm follows a “plan before action” strategy,\nwhere the model first generates a structured plan for tool use and then executes it. ToolPlanner [81]\nintroduces a two-stage reinforcement learning framework with path planning and feedback, supported by\nMGToolBench, to bridge the gap between API-heavy training data and real-world user instructions. Tool-\nMVR [246] enhances reliability and reflection through meta-verification of tool calls and exploration-based\nreflection learning, achieving strong gains over GPT-4 and other baselines. More recently, OctoTools [209]\nprovides a training-free, extensible framework with standardized tool cards, a hierarchical planner, and an\nexecutor, showing broad improvements across multi-domain reasoning tasks. Chain-of-Tools [247] leverages\nfrozen LLMs’ semantic representations to dynamically compose unseen tools in chain-of-thought reasoning,\nenabling generalization to massive tool pools without fine-tuning. PyVision [248] introduces an interactive,\nmulti-turn framework that enables MLLMs to dynamically generate, execute, and refine Python-based tools,\nmoving beyond static toolsets in visual reasoning. ConAgents [228] makes an initial extension of tool use\nframeworks for interactive multi-agent settings. We are also glad to see emerging applications of such agentic\ntool orchestration frameworks in the chemistry domain [249].\nTool Representations for Orchestration.\nBeyond designing orchestration pipelines, another line of research\nfocuses on optimizing the tools themselves to facilitate more accurate selection, composition, and coordination\nduring orchestration. ToolExpNet [210] models tools and their usage experiences as a network that encodes\nsemantic similarity and dependency relations, allowing LLMs to distinguish between similar tools and account\nfor interdependencies during selection. T2Agent [250] addresses multimodal misinformation detection by\nrepresenting tools with standardized templates and using Bayesian optimization to select a task-relevant\nsubset. Coupled with Monte Carlo Tree Search over this reduced action space, T2Agent enables efficient\nmulti-source verification. ToolChain* [211] frames the entire tool action space as a decision tree and applies\nA* search with task-specific cost functions to guide navigation. This representation allows efficient pruning\nof high-cost branches and identification of optimal tool-use paths. ToolRerank [251] refines tool retrieval\nby introducing adaptive truncation for seen vs. unseen tools and hierarchy-aware reranking to balance\nconcentration (for single-tool queries) and diversity (for multi-tool queries).\n3.3. Agentic Search\nSingle-agent Agentic Retrieval-Augmented Generation (RAG) systems embed reasoning and control into a\ncentralized agent that governs the entire retrieval-generation loop. Unlike traditional RAG pipelines [252, 10,\n253] that perform fixed, one-shot retrieval before generation, agentic RAG agents dynamically control when,\nwhat, and how to retrieve based on real-time reasoning needs. This enables the model to adapt retrieval\nstrategies mid-inference, refine its queries, and better integrate evidence from multiple sources. Based on\nhow the agent selects, refines, and integrates retrieved content during reasoning, we categorize single-agent\nAgentic RAG systems into three distinct architectural styles: in-context, post-training, and structure-enhanced\nagentic RAG.\n17\n"}, {"page": 18, "text": "Agentic Reasoning for Large Language Models\nQuery\nUser\nData Sources\nData Embedding\nVector Database\nRetrieved\nDocument\n+\nUser\nQuery\n+\nSystem\nPrompt\nFinal Answer\nTraditional RAG System\nStatic Retrieval\nAgentic Search System\nAutonomous Agent\nDynamic Retrieval\nUser Query\nReasoning\nCritique\n& Adapt\nFinal Answer\nTool Use\nSynthesize & \nGenerate\nWHEN,WHAT, HOW \nto Retrieve\nØ Dynamic search\nØ In-context Search\nØ Search SFT/RL\nquery\nExperience from web\nEvolve\nFigure 4: Comparison between traditional RAG systems and agentic search systems. Traditional RAG relies\non static retrieval over a vector database, while agentic search introduces autonomous decision-making for\nwhen, what, and how to retrieve, enabling dynamic search, in-context retrieval, critique-and-adapt loops,\nand tool use.\n3.3.1. In-Context Search\nInterleaving Reasoning and Search.\nIn-context agentic RAG systems embed retrieval behavior directly\ninto the inference process of language models through carefully designed prompting strategies. Rather than\ntraining the model to learn retrieval behavior, these methods guide it to alternate between reasoning and\nsearch within a single forward pass, typically via few-shot exemplars or special tokens. A representative\nexample is ReAct [5], which interleaves Chain-of-Thought reasoning with tool-use commands such as\n<Search> to dynamically invoke external APIs or knowledge sources. Extensions such as Self-Ask [254] and\nIRCoT [213] go beyond sequential reasoning by prompting the model to recursively decompose questions and\nretrieve sub-evidence accordingly. More recent methods [255, 183, 256, 263] introduce reflective retrieval,\nwhere the model explicitly assesses whether it needs additional information at each step, deciding to retrieve\nonly when necessary. These approaches require no additional training, making them highly flexible and\ndeployable, but often rely on prompt engineering and may struggle with stability across diverse domains.\nStructure-Enhanced Search.\nStructure-enhanced agentic RAG systems enhance retrieval-augmented\ngeneration by enabling a single agent to reason over symbolic knowledge sources such as knowledge graphs\nthrough dynamic querying, tool invocation, and reflective self-monitoring. Unlike static KG retrievers or query\nexecutors, these agents decide when to access structured knowledge, how to formulate graph-based queries,\nand whether retrieved information suffices for continuing the reasoning trajectory. Agent-G [262] introduces\na modular agentic architecture that integrates unstructured document retrieval with structured graph\nreasoning, using feedback loops and specialized retriever modules to ensure accurate multi-hop responses.\nMC-Search [263] introduces five canonical reasoning topologies to model multimodal search-enhanced\nreasoning process, and proposes a end-to-end agentic RAG and step-wise evaluation pipeline to evaluate\nmodel’s planning and retrieval fidelity across heterogeneous sources. Similarly, GeAR [264] incorporates\ngraph expansion operations into an agentic controller to address challenges in complex multi-hop queries,\nenhancing coherence across structured and unstructured sources. Beyond retrieval orchestration, ARG [265]\nproposes a fully end-to-end agentic framework for reasoning over knowledge graphs via active self-reflection.\nThe model autonomously determines when to retrieve, performs iterative critique based on symbolic inputs,\n18\n"}, {"page": 19, "text": "Agentic Reasoning for Large Language Models\nTable 4: Representative Agentic Search systems categorized by Reasoning Structure, Format, and Tool Use.\nNL denotes natural language traces used during reasoning, Ops refers to symbolic or graph operations, and\nKG stands for knowledge graph. Tool use includes search APIs, browser actions, or KG-based retrieval.\nMethod\nStructure\nFormat\nTool\nModality I: In-Context Agentic Search\nReAct [5]\nInterleaved\nNL + Actions\nSearch API\nSelf-Ask [254]\nDecomposed\nNL Queries\nSearch API\nIRCoT [213]\nSequential\nNL + CoT\nSearch API\nSelf-RAG [255]\nReflective\nNL Self-check\nConditional Search\nDeepRAG [256]\nIterative\nNL Feedback\nSearch API\nModality II: Post-Training Agentic Search\nToolformer [6]\nSequential\nTool Tokens\nAPIs, Search\nINTERS [257]\nSequential\nInstructions\nSearch API\nWebGPT [258]\nSequential\nNL + Browser\nWeb Search\nRAG-RL [259]\nDecision\nNL Policy\nEvidence API\nSearch-R1 [27]\nIterative\nNL + Tokens\nLive Web\nDeep-\nResearcher [260]\nMulti-step\nNL Trajectories\nBrowser Tools\nReSearch [205]\nStep-wise\nNL Steps\nSearch + Verifier\nReARTeR [261]\nReflective\nNL Policy\nTool Cluster\nModality III: Structure-Enhanced Agentic Search\nAgent-G [262]\nModular\nNL + Graph Ops\nKG Query\nMC-Search [263]\nMulti-step\nNL\nMultimodal Search\nGeAR [264]\nGraph\nGraph Ops\nKG Expansion\nARG [265]\nReflective\nNL + Symbols\nKG Traversal\nand exhibits interpretable, step-wise reasoning behavior over graphs. Together, these systems represent a\nshift from passive graph access to active, feedback-driven symbolic reasoning, highlighting the potential of\nstructured agentic RAG to achieve both factual reliability and interpretability.\n3.3.2. Post-Training Search\nPost-training agentic RAG methods endow language models with retrieval-aware capabilities by fine-tuning\nthem to make informed decisions throughout multi-step reasoning. Unlike in-context prompting, these\napproaches train models, either via supervised fine-tuning (SFT) or reinforcement learning (RL), to determine\nwhen retrieval is necessary, how to formulate queries, and how to incorporate retrieved evidence.\nSFT-Based Agentic Search.\nThese methods construct curated or synthetic datasets that interleave retrieval\noperations with natural language reasoning, and subsequently apply supervised fine-tuning to instill retrieval-\naware capabilities into the model. Toolformer [6] introduces a self-supervised approach to annotate tool-use\n19\n"}, {"page": 20, "text": "Agentic Reasoning for Large Language Models\nbehaviors within model-generated text, enabling LLMs to learn when and how to invoke tools such as web\nsearch or calculators. INTERS [257] extends this direction by performing instruction-based fine-tuning over\na diverse, multi-task dataset compiled from over 40 sources, capturing a wide spectrum of retrieval-reasoning\npatterns. This class of methods benefits from scalable data generation pipelines [266, 267, 23], which\nminimize the need for human annotation. Instructional reformulation techniques [268, 257, 269] further\nenhance generalization by aligning tasks with human-preferred formats and reasoning.\nRL-Based Agentic Search.\nThese methods optimize retrieval-aware behaviors through reward signals\nthat reflect answer quality, factuality, or user preferences. WebGPT [258] introduces reward modeling to\nsupervise search-augmented chains aligned with human judgment, while RAG-RL [259] formulates retrieval\nas a sequential decision-making task over evidence access. More recent efforts such as Search-R1 [27] and\nDeep-Researcher [260] go further by training agents to dynamically issue retrieval actions (e.g., generating\n<Search> tokens mid-reasoning) and operate in open-ended environments such as the live web. These agents\nexhibit emergent capabilities such as iterative decomposition, re-verification, and evidence planning. Finally,\nsystems like ReSearch [205] and ReARTeR [261] pursue not only accurate answers but also interpretable\nand faithful reasoning trajectories, highlighting the potential of reinforcement-learned retrievers to act as\ncontrollable and reflective agents.\n4. Self-evolving Agentic Reasoning\nSelf-evolving agentic reasoning refers to an agent’s capacity to improve its own reasoning process through\nexperience. At the core of this evolution lie two fundamental mechanisms: feedback and memory. Feedback\nprovides evaluative signals for self-correction and refinement, allowing the agent to revise its reasoning\nstrategies based on outcomes or environmental responses. Memory, in turn, acts as a persistent substrate for\nstoring, organizing, and synthesizing past interactions, enabling knowledge accumulation and reuse across\ntasks. Together, these mechanisms transform reasoning from a static process into a dynamic, adaptive loop\ncapable of continual improvement.\nBuilding upon foundational capabilities such as planning, search, and tool use, self-evolving agents integrate\nfeedback and memory to refine their internal reasoning policies, adjust decision-making strategies, and\ngeneralize across diverse contexts, often without explicit external supervision. This continual adaptation\nmarks a critical step toward lifelong reasoning and lays the groundwork for the collective intelligence explored\nin the next section.\n4.1. Agentic Feedback Mechanisms\nAgentic feedback mechanisms enable models to iteratively refine their reasoning and actions rather than\nrelying on one-shot responses. By incorporating self-critique, verifier guidance, or validator-based resam-\npling, these methods emulate human trial-and-error learning and form the foundation for autonomous\nself-improvement. Broadly, they operate through three distinct feedback regimes: (1) reflective feedback,\nwhere models revise their reasoning through self-critique or verification; (2) parametric adaptation, where\nfeedback is consolidated into updated model parameters; and (3) validator-driven feedback, where binary\noutcome signals guide resampling without introspection.\nThese regimes define a continuum between dynamic, inference-time adaptability, durable learning through\nparameter updates, and efficient correction through external signals. Together, they highlight how modern\nagents leverage feedback to balance flexibility, reliability, and efficiency.\n20\n"}, {"page": 21, "text": "Agentic Reasoning for Large Language Models\nFigure 5: Illustration of three forms of agentic feedback mechanisms. Inference-time reflection enables\nreal-time self-critique and revision during reasoning; offline adaptation consolidates feedback into model\nparameters for long-term improvement; and outcome-based feedback relies on validator signals (success or\nfailure) to refine behavior through retry. Together, they represent a continuum from adaptive reflection to\nstable learning and efficient validation.\n4.1.1. Reflective Feedback\nReflective feedback methods improve model reliability by modifying the reasoning process during inference,\nwithout updating model parameters. These approaches expose intermediate reasoning outputs, such as\nchains of thought or partial solutions, and introduce additional assessment steps that directly influence how\nthe model continues its generation.\nEarly self-critique and rationale-refinement methods [14, 270] implement reflection through an explicit\ngenerate–critique–revise loop. A model first produces an answer together with its reasoning. The same\nmodel, or a separately prompted critic role, then analyzes this output to identify logical errors, unsupported\nassumptions, or missing steps. The critique is appended as context for a revised generation, and this\nprocess may be repeated multiple times or augmented with external evidence such as retrieval. More\nrecent self-improvement frameworks [271] extend reflective feedback beyond a single inference episode by\naccumulating critiques or failure cases across interactions. Instead of correcting only one response, these\nmethods reuse past feedback to guide future generations through prompt refinement or curated supervision\nsignals, while still operating without direct parameter updates at inference time. Search-based reasoning\nstrategies [272, 4, 74] improve reliability by generating and comparing multiple candidate reasoning\npaths. These methods explore the solution space through stochastic sampling or structured search, then\nselect or aggregate outputs using voting schemes, heuristic scores, or learned evaluators. Improvement\narises from comparison across alternatives rather than explicit revision of a single reasoning trajectory.\nDecomposition-based prompting methods [2, 273] reformulate complex problems into ordered sequences of\nsimpler subproblems. Intermediate results are reused in later steps, allowing partial inspection of reasoning\nprogress and reducing error propagation, even when no explicit critique step is introduced.\nOverall, reflective feedback alters inference-time reasoning trajectories by introducing additional reasoning\nor comparison steps. Feedback is used to guide generation within an episode, while the model’s parameters\nremain unchanged.\n21\n"}, {"page": 22, "text": "Agentic Reasoning for Large Language Models\n4.1.2. Parametric Adaptation\nParametric adaptation incorporates feedback into a model’s parameters through additional training, producing\npersistent behavioral changes that generalize beyond individual inference episodes. Unlike reflective feedback,\nthese methods transform feedback signals into supervised or preference-based training objectives that update\nthe model’s weights.\nTrajectory-level supervised fine-tuning approaches [274, 103] attach feedback to intermediate reasoning\ntraces rather than only final answers. Models first generate multi-step trajectories, which are then reviewed by\nhumans, auxiliary models, or automated verifiers. Incorrect steps are corrected or replaced, and the resulting\nfeedback-enriched trajectories are used as supervised training data, encouraging the model to internalize\nimproved reasoning patterns. Distillation-based methods [275] further leverage improved reasoning traces\nby training student models on high-quality chains of thought or self-corrected solutions generated by\nstronger teachers. This process transfers structured reasoning behaviors into more stable or efficient models,\nremoving the need for explicit reflection at inference time. Preference-alignment approaches [276, 277, 278]\nincorporate feedback in the form of comparative judgments that distinguish preferred from dispreferred\noutputs. Training objectives such as reward modeling or direct preference optimization adjust the model’s\nparameters so that preferred behaviors become more likely. Although feedback is often defined over final\noutputs, it implicitly shapes the internal reasoning strategies that produce them. Recent work shows that\nverification-augmented training data can further improve reasoning robustness across domains [279, 280].\nIn these settings, trajectories are filtered or revised based on correctness or consistency signals before training,\nyielding datasets that emphasize reliable reasoning patterns.\nIn summary, parametric adaptation embeds feedback directly into the model’s parameters, yielding durable\nimprovements across tasks. This durability comes at the cost of additional training and reduced flexibility\ncompared to inference-time methods.\n4.1.3. Validator-Driven Feedback\nValidator-driven feedback improves model outputs using external success or failure signals, without modifying\nthe model’s reasoning process or parameters. A validator, such as a unit test, constraint checker, simulator, or\nenvironment signal, evaluates candidate outputs and determines whether they satisfy predefined correctness\ncriteria.\nRetry-based systems [281, 282] implement this paradigm by repeatedly sampling candidate outputs until\none passes validation. The model generates a complete solution, submits it to the validator, and discards\nit if validation fails. Subsequent attempts are generated independently, without conditioning on explicit\ninformation about previous failures. This strategy is particularly effective in domains with reliable and\ninexpensive validation, such as program synthesis and software engineering [283, 284, 285]. Generated\ncode can be executed against unit tests, providing an unambiguous correctness signal. The model iterates\nuntil a solution satisfies all tests, even in the absence of explicit reasoning correction. Similar mechanisms\nappear in embodied and interactive agents [136, 286], where action sequences are repeatedly executed until\nthe environment signals task completion. Failed sequences are abandoned and new ones are attempted,\nbased solely on external success signals. Some hybrid methods introduce lightweight guidance within the\nretry loop, for example by assigning higher reward to behaviors that eventually lead to successful outcomes\n[287]. However, the dominant mechanism remains selection through external validation rather than revision\nof reasoning steps or parameter updates.\nOverall, validator-driven feedback offers an efficient and scalable way to improve output correctness when\n22\n"}, {"page": 23, "text": "Agentic Reasoning for Large Language Models\nTable 5: Representative Agentic Feedback Mechanisms categorized by Feedback Stage, Feedback Source, and\nUpdate Target.\nMethod / System\nFeedback Stage\nFeedback Source\nUpdate Target\nI. Reflective Feedback\nReflexion [14]\nInference\nSelf-generated critique\nTrajectory\nSelf-Refine [270]\nInference\nSelf-evaluation\nTrajectory\nConstitutional AI [278]\nInference\nNormative rules\nTrajectory\nRLAIF [288]\nInference\nAI verifier\nTrajectory\nSelfCheckGPT [289]\nInference\nCross-sample divergence\nTrajectory\nZero-Shot Verification-CoT [290]\nInference\nExternal verifier\nTrajectory\nASCoT [291]\nInference\nVulnerability detection\nTrajectory\nMM-Verify [292]\nInference\nMultimodal verifier\nTrajectory\nReAct [5]\nInference\nAction outcomes\nTrajectory\nPAL [3]\nInference\nCode execution\nTrajectory\nWebGPT [258]\nInference\nWeb evidence\nTrajectory\nMemGPT [293]\nInference\nRetrieved memory\nTrajectory\nVoyager [36]\nInference\nEnvironment + memory\nTrajectory\nII. Parametric Adaptation\nAgentTuning [274]\nTraining\nHigh-quality trajectories\nModel parameters\nReST [103]\nTraining\nCritique–revision pairs\nModel parameters\nReFT [294]\nTraining\nReflection-augmented data\nModel parameters\nDistill-CoT [275]\nTraining\nExpert CoT\nModel parameters\nReflectEvo [279]\nTraining\nReflection traces\nModel parameters\nReasoning-CV [280]\nTraining\nVerification signals\nModel parameters\nIII. Validator-Driven Feedback\nReZero [281]\nInference\nBinary validator\nOutput only\nRetrials [282]\nInference\nAcceptance signal\nOutput only\nCodeRL [283]\nInference\nUnit tests\nOutput only\nLEVER [284]\nInference\nExecution results\nOutput only\nSWE-bench [285]\nInference\nTest suite\nOutput only\nSayCan [136]\nInference\nEnvironment state\nOutput only\nPaLM-E [286]\nInference\nEnvironment feedback\nOutput only\nReflect–Retry–Reward [287]\nInference\nValidator + reflection signal\nOutput only\nreliable validators are available. Its limitation is that feedback is non-diagnostic, correcting individual outputs\nwithout explaining failures or altering the model’s reasoning behavior.\n23\n"}, {"page": 24, "text": "Agentic Reasoning for Large Language Models\nRaw conversation \nand history\nText\nSummarized or \nexpanded item\nSemantic\nExecution plan\nWorkflow\nReasoning path\nTrajectory\nConversation\nExperience\nIn-context Use\nConnecting entities, \nevents and facts\nStructured Representation\nGraph Memory\nMultimodal Memory\nMemory Reward\nPost-training Control\nControl\nUpdate\nFigure 6: Overview of Agentic Memory in LLM agents, showing three parallel dimensions: in-context use\n(text and experience), structured representation (graph and multimodal memory), and post-training control\n(reward-guided memory management).\n4.2. Agentic Memory\nRecent advances in memory-augmented LLM agents have shifted the focus from static memory storage to\nmore dynamic, interactive mechanisms that directly support agentic reasoning. Rather than merely extending\nthe context window or storing historical inputs, memory is increasingly treated as an integral component of\nthe reasoning loop, used for reflecting on past experiences, guiding future actions, and dynamically adapting\nto complex, long-horizon tasks. Formally, an agent maintains a memory module where each memory entry\nmay represent a raw observation, summarized trajectory, subgoal, tool invocation trace, or other structured\nelement depending on the system design.\nThe agent’s reasoning process then operates not only on its immediate context but also on this persistent\nmemory, enabling reflection, generalization, and long-term goal tracking. In this section, we organize prior\nwork along four emerging trends in the use of memory to support and enable agentic reasoning. Figure 6\nsummarizes how agentic memory progresses from contextual recall to adaptive control. In-context memory\ncaptures textual and semantic information from prior interactions; structured memory integrates these into\ngraph and multimodal representations; post-training control enables agents to evolve, update, and retrieve\nmemory through learned reward-based mechanisms.\n4.2.1. Agentic Use of Flat Memory\nFactual Memory.\nTraditional memory systems for LLM agents typically treat memory as a passive buffer,\nmainly used to store dialogue histories or recent observations to address the limited context window of\ntransformer models. Examples include dense retrieval methods [252, 319, 297], pre-defined modules in\nLangChain and LLamaIndex [296], and cache-inspired designs like MemGPT [293]. These approaches\nusually retrieve semantically similar past content to augment prompts, without influencing the agent’s\ninternal reasoning. Enhancements such as RET-LLM with differentiable memory [320], SCM with controller-\nbased mechanisms [321], as well as LOCOMO and LongMemEval benchmarks for long-term retention [322,\n323] further improve recall but remain largely static. These systems often rely on fixed heuristics and\n24\n"}, {"page": 25, "text": "Agentic Reasoning for Large Language Models\nTable 6: Representative Agentic Memory systems categorized by Setting, Format, and Memory Type.\nMethod / System\nSetting\nFormat\nMemory Type\nI. Agentic Use of Flat Memory (In-Context)\nLangMem [295]\nIn-Context\nText\nFactual\nLlamaIndex [296]\nIn-Context\nText\nFactual\nMemGPT [293]\nIn-Context\nText\nFactual\nMemoryBank [297]\nIn-Context\nSemantic\nFactual\nAmem [24]\nIn-Context\nSemantic\nFactual\nWorkflow Memory [298]\nIn-Context\nWorkflow\nExperience\nMemOS [13]\nIn-Context\nSemantic\nFactual\nLightMem [299]\nIn-Context\nSemantic\nFactual\nNemori [300]\nIn-Context\nSemantic\nFactual\nACE [301]\nIn-Context\nWorkflow\nExperience\nReasoning Bank [302]\nIn-Context\nWorkflow\nExperience\nDynamic Cheatsheet [303]\nIn-Context\nTrajectory\nExperience\nSleep-time Compute [304]\nIn-Context\nTrajectory\nExperience\nEvo-Memory [25]\nIn-Context\nSemantic\nExperience\nII. Structured Memory Representations\nGraphRAG [305]\nIn-Context\nGraph\nFactual\nMEM0 [12]\nIn-Context\nGraph\nFactual\nZep [306]\nIn-Context\nGraph\nFactual\nOptimus-1 [307]\nIn-Context\nMultimodal\nExperience\nRAP [308]\nIn-Context\nMultimodal\nExperience\nM3-Agent [309]\nIn-Context\nMultimodal\nFactual\nMem-Gallery [310]\nIn-Context\nMultimodal\nFactual\nAgent-ScanKit [311]\nIn-Context\nMultimodal\nExperience\nIII. Post-training Memory Control\nMem1 [312]\nPost-training\nSemantic\nFactual\nMemory-as-Action [313]\nPost-training\nSemantic\nFactual\nMemAgent [314]\nPost-training\nSemantic\nFactual\nMem-α [315]\nPost-training\nSemantic\nFactual\nMemory-R1 [15]\nPost-training\nSemantic\nFactual\nAgent Early Experience [316]\nPost-training\nImplicit\nExperience\nAgentic Memory [317]\nPost-training\nSemantic\nExperience\nMemRL [318]\nPost-training\nSemantic\nExperience\nunstructured token lists [297], limiting adaptability for tasks involving goal decomposition [324, 143],\nlong-term planning [150], or iterative self-improvement [325]. In contrast, emerging agentic memory treats\n25\n"}, {"page": 26, "text": "Agentic Reasoning for Large Language Models\nmemory as part of the reasoning loop, supporting reflection [326], and decision-making [327]. Amem [24]\nenables LLM agents to autonomously generate contextual memory descriptions, build dynamic links between\nrelated experiences, and evolve memory content in response to new information. Similarly, Zep [306], Mirix\n[328], MemOS [13], LightMem [299], and Nemori [300] leverage LLMs to automatically produce context-\naware memory representations. Beyond LLM-driven approaches, recent work has explored reinforcement\nlearning to explicitly train agents to acquire and organize factual memory, such as Mem-α [315] and\nMemory-R1 [15], which we discuss in detail in later sections.\nExperience Memory.\nWorkflow Memory [298] tracks procedural traces to enable plan recovery and consis-\ntent reasoning. Sleep-time Compute enables LLM agents to pre-compute and store anticipated reasoning\nsteps before user interaction, effectively “thinking offline” using memory as a preparatory resource [304].\nDynamic Cheatsheet (DC) [303] equips black-box models with external memory to store reusable strategies,\nreducing redundant reasoning. Other efforts explore complementary paradigms of agentic memory. In\nparallel, workflow memory has emerged as another structured approach, particularly suited for procedural\nand tool-augmented tasks. It explicitly tracks procedural traces during execution, supporting plan recovery,\nlong-term consistency, and interpretable chaining of actions. Atomic reasoning [143] proposes a structured\ntrace over a finite set of reusable atomic skills in a streamlined generation space to reduce spurious reasoning\npatterns. Context evolution (ACE) [301] treats contexts as evolving playbooks rather than building a static\nstructured store, whereas Reasoning Bank [302] focuses on reusing failed reasoning traces to enhance future\ntask performance. Evo-Memory [25] synthesizes these ideas by benchmarking self-evolving memory under\nstreaming task settings, highlighting experience reuse as a central capability for stateful, long-horizon agentic\nreasoning. In addition to factual memory, Mirix [328] further introduces a procedural memory component\nto capture reusable action patterns, while Agentic Memory [317] and MemRL [318] adopt reinforcement\nlearning to optimize the acquisition and management of experiential memory.\nThis marks a shift from static buffers toward structured, reasoning-centric memory architectures. In these\nagentic memory systems, memory serves as a dynamically growing context: agents not only record past\nactions but actively reflect, edit, and refine their strategy over time.\n4.2.2. Structured Use of Memory\nBeyond flat memory usage and control, the structure of memory plays a critical role in enabling complex\nreasoning. Recent work increasingly explores structured representations, such as semantic graphs, workflows,\nand hierarchical trees, often extended to multimodal settings, to better capture dependencies, and contextual\nrelationships.\nGraph-based representations provide a flexible substrate for organizing relational knowledge in agents [329].\nGraphRAG [305] serves as a foundational technique that augments retrieval with graph-structured reasoning,\nenabling more contextually coherent and multi-hop information integration. Building on this foundation,\nagent systems such as MEM0 [12] and Zep [306] organize memory explicitly as dynamic knowledge graphs,\nallowing agents to store, retrieve, and reason over entities, attributes, and their relations with improved\nefficiency and semantic grounding. Beyond graphs, structured memory has also been explored through\nalternative organizational forms. MemTree [330] leverages a dynamic tree-structured representation to\nhierarchically organize and integrate information, while workflow-oriented systems such as AutoFlow [331],\nAFLOW [332], and FlowMind [333] represent reasoning workflows explicitly in memory, capturing sequences\nof subgoals, tool invocations, and decision points.\nNew benchmarks have pushed reasoning memory into multimodal domains, where agents are required\n26\n"}, {"page": 27, "text": "Agentic Reasoning for Large Language Models\nto ground, retrieve, and reuse information across heterogeneous modalities. M3-Agent [309] evaluates\nvisual–audio–text reasoning through “see, listen, and reason,” while Agent-Scankit [311] proposes multimodal\nagents with integrated memory modules for adaptive retrieval and grounding. Optimus-1 [307] proposes\na hybrid multimodal memory architecture that represents world knowledge as a hierarchical directed\nknowledge graph and abstracts past interactions into a multimodal experience pool. RAP [308] retrieves\nrelevant experiences based on contextual similarity, enabling adaptive reuse of multimodal memory.\nThese structured memory formats align task semantics, temporal dependencies, and multimodal signals,\nenabling agents to reason compositionally and maintain coherent behavior over extended interactions. As\ntask complexity increases, the abstraction and organization of memory become increasingly critical for\nbuilding robust and generalist agents.\n4.2.3. Post-training Memory Control\nConversely, memory systems can also be controlled by the agent’s reasoning process itself. Rather than\nrelying on fixed heuristics for reading and writing memory, recent work has explored agent-controllable\nmemory operations, where the agent explicitly decides what to store, when to retrieve, and how to interact\nwith memory. This reframes memory as a policy target, no longer a passive buffer, but a resource that is\nactively shaped by reasoning.\nMemAgent [314] formulates memory overwrite as a reinforcement learning problem: the agent is rewarded\nfor preserving information that proves useful and for discarding irrelevant content. By using a newly\nproposed DAPO algorithm, the model learns to maintain a constant-sized memory across conversations\nwhile maximizing future utility. Mem1 [312] presents an end-to-end reinforcement learning framework\nwhere agents maintain a compact, shared internal state across turns, jointly supporting reasoning and\nmemory consolidation. Memory-R1 [15] further advances this line by introducing a dual-agent design: a\nMemory Manager that dynamically decides when to add, update, or delete entries in the memory store, and\nan Answer Agent that distills the most relevant retrieved memories to guide response generation. Recent\nwork such as Mem-α [315] also explores RL-based control of multi-component memory construction in\nagents, providing a unified perspective on adaptive memory construction and reasoning control. Memory-\nas-Action [313] integrates memory editing including insertions, deletions, and modifications directly into\nthe reasoning policy, proposing a Dynamic Context Policy Optimization algorithm to handle non-prefix\ntrajectory changes caused by memory operations. Agent Learning via Early Experience [316] further\nrelaxes reward dependence by enabling agents to learn from their own interaction traces through self-\nprediction and reflection, bridging imitation and reinforcement learning. Moreover, Agentic Memory [317]\nand MemRL [318] adopt reinforcement learning to optimize the acquisition and management of experiential\nmemory.\nTogether, these systems mark a shift toward learning-based memory control, where memory usage is\noptimized through reinforcement or imitation learning. By integrating memory management into the\nreasoning policy, agents become more adaptive, scalable, and capable of long-horizon decision-making in\ndynamic environments.\n4.3. Evolving Foundational Agentic Capabilities\n4.3.1. Self-evolving Planning\nRecent advances view planning not as a fixed reasoning routine but as an evolving capability. Instead of\nrelying on static datasets or human-designed curricula, agents can autonomously generate tasks, learn from\n27\n"}, {"page": 28, "text": "Agentic Reasoning for Large Language Models\nSelf-evolving Planning\nSelf-evolving Tool-Use\nSelf-evolving Search\nTask Generation\nStrategy Refinement\nTool Creation\nTool Synthesis\nDynamic Retrieval\nKnowledge Synthesis\nFigure 7: An overview of evolving foundational agentic capabilities along three key dimensions: planning\n(task generation and strategy refinement), tool-use (tool creation and synthesis), and search (dynamic\nretrieval and knowledge synthesis). These dimensions reflect how agentic systems autonomously enhance\ntheir reasoning and problem-solving capacity over time.\ntheir own feedback, and adapt strategies through iterative interaction with the environment. This enables\ncontinuous improvement without external supervision.\nA representative direction is self-generated task construction. For example, SCA enables agents to alternate\nbetween generating problems and solving them, reusing successful trajectories for fine-tuning [334]. Self-\nrewarding frameworks further allow agents to assess their own outputs, producing high-quality training\nsignals without human labels [335, 336]. Other works directly leverage execution feedback for online\nadaptation, such as SELF, SCoRe, PAG, TextGrad, and AutoRule, which transform natural-language critiques\nor traces into training rewards, enabling continual policy refinement [337, 338, 339, 340].\nBeyond internal feedback, agents can also evolve through environment shaping. AgentGen constructs adaptive\nenvironments to induce curriculum learning [341], while Reflexion and AdaPlanner use self-reflective or\nadaptive strategies to refine plans at runtime [14, 342]. Self-Refine iteratively critiques and improves outputs\n[270], and SICA allows self-modification of code and reasoning tools [343]. From an RL perspective, RAGEN\nand DYSTIL model planning as a Markov Decision Process and optimize strategies with dense feedback\n[344, 345].\nTogether, these methods establish a self-improving planning loop, where agents generate their own tasks,\nshape their environments, and refine strategies, laying the groundwork for autonomous, open-ended planning\nevolution.\n4.3.2. Self-evolving Tool-use\nCreating and Synthesizing Tools.\nThe culmination of in-context reasoning is the emergent capability of\nagents to autonomously create new tools. This is achieved not through training, but by prompting a frozen\nLLM to act as a programmer when it encounters a problem that its existing toolset cannot solve. The LATM\nframework [346] uses a powerful model as a one-time \"tool maker\" and a cheaper, lightweight model as\na frequent \"tool user,\" thus amortizing the cost of creation. To enable specialization beyond the limits of\ngeneral-purpose APIs, frameworks like CRAFT [347] and CREATOR [348] generate custom tools tailored for\nspecific domains. Taking this a step further, ToolMaker [349] can convert entire public code repositories into\nusable tools, allowing agents to leverage complex, human-written codebases on the fly.\n28\n"}, {"page": 29, "text": "Agentic Reasoning for Large Language Models\n4.3.3. Self-evolving Search\nSearch plays a central role in agentic reasoning, enabling models to retrieve, select, and synthesize relevant\nknowledge across large and evolving memory spaces. In early systems, search was typically static—built on\nfixed retrieval heuristics or similarity-based dense retrievers [252, 255, 297, 293]. These methods augmented\nprompts with retrieved information but lacked adaptive control over how memory evolves or how search\nstrategies are improved over time.\nRecent research increasingly links search and memory in a co-evolutionary loop: agents continuously update\ntheir memory base during task execution, while dynamically adjusting how search is performed over this\nevolving knowledge. Agentic memory systems such as MemGPT [293], MemoryBank [297], and Workflow\nMemory [298] already highlight how retrieved information can be synthesized and re-inserted into memory,\ngradually improving retrieval quality. Dynamic Cheatsheet (DC) [303] further demonstrates how reusable\nstrategies can be accumulated and leveraged across queries, effectively transforming static search into a\nliving retrieval substrate that evolves with agent experience.\nEvolving Memory Bases.\nUnlike static index-based retrieval, self-evolving agents actively refine their\nmemory base through reflection and post-execution updates. Reflexion [14] allows agents to critique their\nown reasoning traces and store distilled insights, improving future search relevance. Reasoning Bank [302]\nand context evolution methods [301] explicitly restructure memory representations to align retrieval results\nwith evolving problem-solving strategies, effectively making the retrieval target itself adaptive over time.\nDynamic Search and Synthesis.\nBeyond memory updates, search strategies themselves can evolve through\ndynamic prioritization and synthesis. Structured memory representations—such as workflows [331, 332, 333]\nand knowledge graphs [329, 305, 12, 306]—provide semantic scaffolding that enables multi-hop and\ncompositional search, supporting richer reasoning over longer horizons. Systems like MemOS [13] and\nMemory-as-Action [313] take this further by integrating search decisions directly into the reasoning policy,\nallowing retrieval targets, strategies, and sources to co-adapt as agents accumulate experience.\nOverall, self-evolving search transforms retrieval from a static utility into a continuously adapting component\nof the reasoning loop. By evolving memory bases, dynamically adjusting search strategies, and synthesizing\nretrieval results into structured knowledge, agents can maintain more relevant, structured, and actionable\ninformation over extended time horizons.\n5. Collective Multi-agent Reasoning\nBuilding upon the single-agent foundation, where reasoning supports planning, search, and tool use within\na unified perception–action loop, multi-agent reasoning extends these principles to collaborative settings.\nIn a multi-agent system (MAS), multiple reasoning agents interact to jointly solve complex tasks. Rather\nthan identical problem solvers, agents assume complementary roles, such as Manager for task decomposition,\nWorker for execution, and Verifier for evaluation, enabling specialization and division of cognitive labor. This\nrole differentiation marks the first step toward collective intelligence, where reasoning is distributed and\ncoordinated across multiple agents.\nBeyond role assignment, the essence of multi-agent reasoning lies in how these agents collaborate, communi-\ncate, and co-evolve. Collaboration schemas define how reasoning traces are exchanged, conflicts are resolved,\nand shared memory is maintained to achieve alignment. Through such interaction, reasoning transitions\n29\n"}, {"page": 30, "text": "Agentic Reasoning for Large Language Models\nfrom an individual process into a distributed, iterative loop, in which agents refine each other’s outputs and\ncollectively converge toward better solutions.\nCompared with single-agent systems, multi-agent reasoning introduces new challenges that require rethinking\nreasoning at the system level:\n• Role differentiation: how to design static or adaptive roles that align with task structure and expertise\ndistribution;\n• Collaboration and communication: how agents exchange intermediate reasoning, negotiate consensus,\nand divide labor efficiently;\n• Collective memory and evolution: how shared or distributed state supports long-term coordination and\ncontinual adaptation.\nThese challenges motivate the following structure of our analysis. Section 5.1 examines the role taxonomy of\nmulti-agent systems, from generic organizational roles to domain-specific specializations. Section 5.2 focuses\non collaboration and division of labor, including in-context and post-training coordination strategies. Finally,\nSection 5.3 explores how memory enables multi-agent systems to evolve over time and maintain collective\nconsistency. Together, these perspectives provide a unified view of how reasoning scales from individual\nagents to adaptive, collaborative intelligence.\n5.1. Role Taxonomy of Multi-Agent Systems (MAS)\nIn this subsection, we first summarize the generic roles that often appear in a multi-agent system (MAS).\nThen, we introduce the specific functions of different roles when an MAS is applied in different domains, such\nas software engineering, finance, legal activities, education, healthcare, biomedicine, and music applications.\nFigure 8: An overview of generic roles of agent and their specific domain adaptations in Section 5.1.\n5.1.1. Generic Roles\n• Leader/Coordinator: The leader, or coordinator, is responsible for maintaining high-level coherence\nwithin the system. This role involves setting global objectives, decomposing tasks into manageable\nsubgoals, and assigning them to appropriate agents. In addition, the leader arbitrates conflicts that emerge\n30\n"}, {"page": 31, "text": "Agentic Reasoning for Large Language Models\nbetween agents with overlapping or contradictory outputs. In practice, this role often manifests itself as\na meta-controller that monitors the progress of other agents and ensures that execution adheres to an\noverarching plan.\n• Worker/Executor: Executors, often called workers, are the operational backbone of MAS. They engage\nin concrete actions such as invoking external tools, writing or executing code, retrieving documents,\nor interfacing with the environment. Although they typically act under the directives of a leader, well-\ndesigned systems allow for adaptive autonomy, where executors can refine or optimize their assigned\ntasks when new local information becomes available.\n• Critic/Evaluator: The critic/evaluator role centers on quality assurance. This role includes verifying\ncorrectness, testing hypotheses, red-teaming responses, and surfacing potential risks. In LLM-based\nsystems, this often corresponds to LLM-as-a-judge setups, where dedicated evaluators assess the factuality,\nsafety, or stylistic alignment of output. Critic roles help introduce checks and balances into otherwise\ngenerative workflows, thereby mitigating error propagation.\n• Memory Keeper: Effective MAS requires persistent memory to accumulate context, prevent repetitive\nfailures, and enable learning across episodes. The memory keeper curates and maintains long-term\nknowledge structures such as episodic logs, semantic embeddings, retrieval indices, or knowledge graphs.\nBy abstracting memory management into a dedicated role, the system can better balance short-term\nreactivity with long-term continuity and adaptation.\n• Communication Facilitator: Communication overhead can easily undermine MAS efficiency. This role\ngoverns protocols for inter-agent exchange, including defining message schemas, managing communi-\ncation bandwidth, enforcing gating mechanisms, and orchestrating consensus-building. By reducing\nambiguity and ensuring structured information flow, the communication facilitator prevents bottlenecks\nand coordination failures in large-scale or heterogeneous agent populations.\n5.1.2. Domain-Specific Roles\nBeyond generic agent roles, domain-specific tasks often require specialized functions. These roles reflect\nprofessional practices in particular industries and map naturally onto MAS architectures.\nSoftware Engineering: In software engineering, MAS generally maps onto roles that mirror the software de-\nvelopment lifecycle: architects, developers, code reviewers/testers, CI orchestrators, and release managers [17,\n350]. The rationale is to distribute the responsibilities in a way that balances creativity, verification, automa-\ntion, and governance, just as in industrial software practice.\n• Architects define system-level design principles and establish structural blueprints.\n• Developers translate these abstractions into concrete implementations.\n• Code reviewers and testers safeguard reliability, checking correctness, maintainability, and functional\ncoverage.\n• CI orchestrators automate builds, testing, and artifact pipelines, reducing integration frictions.\n• Finally, release managers oversee deployment, aligning new versions with milestones and safety protocols.\n31\n"}, {"page": 32, "text": "Agentic Reasoning for Large Language Models\nPrevious work has demonstrated similar mappings, such as MetaGPT [17], which decomposes development\ninto Product Manager, Architect, and Engineer agents. ChatDev [350] further emphasizes communicative\ncollaboration among specialized agents to support requirement analysis, coding, and testing. More recently,\nself-evolving collaboration networks have expanded this paradigm by enabling MAS to dynamically reorganize\nand optimize their roles throughout the software lifecycle [351]. A variant of MAS is also applied to the\nHigh-Performance Computing (HPC) domain [352] By structuring MAS around these stages, the architecture\ngains the same robustness and scalability as professional engineering workflows.\nFinance: The financial domain can be roughly decomposed into four archetypal roles: analysts, risk managers,\ntraders/execution agents, and compliance officers [353, 354]. This division reflects the established institutional\ndesign of financial organizations, where the responsibilities are segmented to balance profit generation with\nsystemic stability.\n• Analysts operate at different levels (e.g., fundamental, sentiment, or technical), each extracting distinct\nsignals from raw market or textual data.\n• Risk Managers then monitor portfolio exposure, apply stress tests, and enforce safeguards to prevent\ncascading vulnerabilities.\n• Traders take responsibility for market interaction, while Execution agents ensure that orders are placed\nwith speed and efficiency under liquidity constraints.\n• Finally, Compliance roles ensure that activities remain aligned with regulatory requirements, enabling\ntraceable decision-making and proper oversight.\nTogether, this layered ecology mirrors real-world financial institutions, where specialization and checks-\nand-balances are indispensable. Recent advances in MAS for finance mirror this layered ecology. R&D-\nAgent-Quant [355] demonstrates how agents can specialize in factor discovery and joint optimization for\nquantitative strategies. FinRobot [356] provides an open source multi-agent platform tailored to financial\napplications, reflecting the practical need for modularity and scalability. PEER [357] introduces expertization\nand tuning methods to adapt MAS to domain-specific responsibilities, while FinCon [358] highlights the\nrole of conceptual verbal reinforcement to enhance decision-making and compliance. Together, these works\nunderscore how MAS can replicate the specialization, checks, and balances of real-world financial institutions.\nLegal Activities: Multi-agent systems are also designed to model the collaborative and adversarial processes\ninherent in legal practice, with roles assigned to manage consultation, reasoning, and argumentation.\n• For legal consultation, frameworks often simulate a law firm’s structure with a receptionist agent for\nclient intake, specialized lawyer agents for providing advice, a secretary agent for documentation, and a\nboss agent for quality control. In a consultation model, the receptionist agent first clarifies a user’s query\nbefore routing it to the appropriate lawyer agent. After the multi-turn consultation, the secretary agent\nsummarizes the interaction, and the boss agent provides an evaluation, ensuring a comprehensive and\nhigh-quality service [359].\n• For statutory reasoning, tasks are decomposed between knowledge acquisition agents that interpret legal\ntexts and knowledge application agents that apply formalized rules to case facts. To be specific, in reasoning\nsystems, the knowledge acquisition agent first builds a reusable ontology from legal statutes; then, the\nknowledge application agent uses this formal structure to analyze the specifics of a new case, ensuring\nconsistent and transparent logic [360].\n32\n"}, {"page": 33, "text": "Agentic Reasoning for Large Language Models\n• To simulate courtroom dynamics, roles such as judge, plaintiff, defendant, and adversarial lawyer agents\nare created [361]. In courtroom simulations, adversarial lawyer agents engage in debate before a judge\nagent, reflecting on their performance after each trial to iteratively improve their argumentation strategies\nby updating their internal knowledge bases [361].\nEducation: In education, MAS is being developed to provide personalized and adaptive learning experiences\nby distributing pedagogical functions among specialized agents.\n• For personalized tutoring, a central tutor agent might engage a student using Socratic dialogue, while\na memory dispatcher agent tracks the student’s progress and misconceptions to adapt the difficulty and\nfocus of the lesson in real-time [362].\n• For curriculum design, a pipeline of agents collaborates: a research agent gathers relevant information, a\nplanning agent structures it into a coherent course, and other agents generate specific learning activities\nor assessments. Also, it can be modeled by an adversarial process, where evaluator agent critiques a lesson\nplan created by generator agent, and optimizer agent refines it based on feedback [363].\nThese systems demonstrate a shift towards creating intelligent, adaptive platforms that can support educators\nand provide students with more effective, engaging, and individualized learning journeys.\nHealthcare: In the healthcare domain, multi-agent systems are structured to mirror clinical and research\nworkflows, distributing complex tasks among specialized AI agents.\n• For clinical diagnostics and consultation, these roles often include a triage agent (or moderator) for initial\ncase assessment, various specialist agents (e.g., pathologists, neurologists), a doctor agent for patient\ninteraction, and a measurement agent to provide test results [364, 365]. To be more specific, in the\ndiagnostic setting, a triage agent first assesses the complexity of a case and routes it to the appropriate\nspecialist agents for analysis. These specialists may then engage in multiround discussions, with a lead\nphysician agent synthesizing their opinions to reach a consensus. In addition to that, a doctor agent\nconducts a multi-turn dialogue with a patient agent, requesting specific data from a measurement agent to\ngather information dynamically.\n• For autonomous research, roles are modeled after the scientific process, featuring a meta agent for\nstrategic planning, an executor for running analyses, an evaluator for assessing outcomes, and a reflector\nfor synthesizing knowledge [366]. This division of labor allows for a systematic and comprehensive\napproach to multifaceted health challenges. Especially, the meta agent plans an experiment, the executor\ncarries it out, the evaluator provides immediate feedback, and the reflector distills successful strategies\ninto a persistent knowledge base, creating a self-improving cycle that enhances future planning.\n• For public health events, ShortageSim [367] models FDA regulators, manufacturers, and healthcare\nbuyers interacting under information asymmetry, enables counterfactual policy testing and evaluates how\nannouncements and disruptions shape investment, stockpiling, and resolution timing against historical\ntrajectories.\nOther frameworks such as MMedAgent and MedAgent-Pro focus on orchestrating specialized medical\ntools, using a central agent to plan actions and aggregate results from various tool-based agents to handle\nmultimodal data [39, 368].\n33\n"}, {"page": 34, "text": "Agentic Reasoning for Large Language Models\nFigure 9: An overview of Agentic Collaboration in the multi-agent system, containing two parallel dimen-\nsions: in-context collaboration (training-free task-specific coordination design) and post-training collaboration\n(optimization-based automated workflow generation).\nBiomedicine: In biomedicine, particularly in drug and material discovery, MAS is designed to automate\nand accelerate the scientific process by assigning roles that reflect the iterative cycle of design, testing, and\nrefinement.\nFor de novo molecule design, key roles include the actor (or reasoner) for generating novel structures, the\nevaluator for assessing chemical properties, and the self-reflector for refining future hypotheses based on\nresults. To be specific, the actor agent proposes new candidates, which are then passed to the evaluator agent.\nThe evaluator uses computational chemistry tools to calculate properties like binding affinity and synthetic\naccessibility, providing quantitative feedback [369]. This feedback is then analyzed by the self-reflector\nagent to update the system’s strategy for the next generation cycle, creating a feedback-driven process of\noptimization [370].\nSimilarly, LIDDIA acts as a \"digital chemist\" with a Reasoner, Executor, Evaluator, and Memory component to\nnavigate the drug discovery process and balance the exploration of new chemical spaces with the exploitation\nof promising candidates [369]. To streamline the creation of machine learning workflows, DrugAgent uses\nan LLM Planner and an LLM Instructor to automate programming for tasks like ADMET prediction [371]. In\ngenomics, GenoMAS orchestrates six specialized agents through a guided-planning framework to analyze\ncomplex gene expression data, integrating the reliability of structured workflows with the adaptability of\nautonomous agents [372].\nMusic: In the creative domain of music composition, MAS is being explored to decompose the intricate\nprocess of creating music into collaborative, specialized roles. A system like ComposerX might feature\na conductor agent that interprets a high-level user prompt and oversees the project, a melody agent that\ngenerates primary musical themes, a harmony agent that creates supporting chord progressions, and a\nrhythm agent that lays down the percussive and temporal foundation. These agents would interact iteratively,\nwith the conductor agent synthesizing their outputs and providing feedback to ensure the different musical\nlayers are coherent and aligned with the initial creative vision. This mirrors the collaborative process of a\nhuman orchestra or band, distributing creative responsibilities to achieve a complex and harmonious final\nproduct [373].\n5.2. Collaboration and Division of Labor\nCollaboration and division of labor constitute a central organizing principle in modern multi-agent systems.\nInstead of treating agents as homogeneous components, recent work emphasizes how responsibilities are\ndecomposed and coordinated across specialized agents to improve efficiency and robustness. From this\nperspective, existing approaches can be broadly organized along two dimensions. In-context collaboration\nfocuses on coordination strategies that are specified or induced at inference time without additional training.\n34\n"}, {"page": 35, "text": "Agentic Reasoning for Large Language Models\nPost-training collaboration instead optimizes agent roles, interaction structures, or routing policies through\nlearning or search. In addition, agentic routing can be viewed as a special case of this division of labor, where\nrouting decisions explicitly offload cognition and computation to different agents based on task demands.\n5.2.1. In-context Collaboration\nIn the design of multi-agent systems, several studies have observed that leveraging task-specific in-context\ninformation is often sufficient to build highly effective systems without the need for explicit training. Among\nthese works, one line of research relies on manually crafted pipelines, where researchers design the agent\ninteractions and workflows tailored to the target task. In contrast, another line explores LLM-driven automatic\npipeline generation, allowing the model itself to construct and adapt the system’s structure dynamically\nbased on the task context.\nManually Crafted Pipelines.\nThese approaches rely on predefined hierarchies or fixed collaboration\nworkflows, where agent roles, execution order, and communication rules are determined before execution.\nHierarchical systems such as AgentOrchestra [374], MetaGPT [17], and SurgRAW [375] feature a cen-\ntral planner or conductor directing subordinate agents through structured subgoals. Cascading pipelines\nlike Collab-RAG [376], MA-RAG [377], Chain of Agents [378], and AutoAgents [16] process information\nsequentially, passing intermediate outputs downstream with limited revision. Modular role-decomposed\nframeworks such as RAG-KG-IL [379], SMoA [380], and MDocAgent [381] define fixed functional roles (e.g.,\nretriever, reasoner, or vision agent) but allow minimal dynamic coordination. While these manually designed\npipelines offer interpretability, modularity, and low execution complexity, their rigidity restricts adaptability to\nambiguous or evolving reasoning tasks, motivating more flexible, reasoning-driven coordination mechanisms.\nLLM-Driven Pipelines.\nThis category leverages LLMs as orchestrators that decompose high-level goals\ninto subgoals, route them to role-specialized agents or tools, and iteratively refine workflows based on\nintermediate feedback until completion. AutoML-Agent [382] proposes a full-pipeline, orchestrator-led agent\nteam that plans, assigns, and coordinates web/API/code tools through role-specialized micro-agents (e.g.,\ncoder/tester/runner), enabling end-to-end software development workflows. Magentic-One [383] introduces\na generalist multi-agent system where a central Orchestrator plans, tracks progress, and performs ledger-based\nrouting over specialized agents (WebSurfer, FileSurfer, Coder, ComputerTerminal), achieving competitive\nresults on GAIA, AssistantBench, and WebArena. MAS-GPT [384] trains an LLM to emit executable MAS\ncode conditioned on a user query, so a single forward pass generates a query-specific multi-agent workflow.\nMetaAgent [385] presents a finite-state-machine (FSM) abstraction to declare states, transitions, and tools,\nfrom which a LLM designer automatically constructs the MAS pipeline. AOP [146] formalizes orchestrator\nresponsibilities and introduces three design principles, i.e., solvability, completeness, non-redundancy, and\nthen operationalizes them with fast decomposition/assignment plus a reward-model evaluator.\nAgent Routing. Closely related to LLM-driven orchestration, a line of work explicitly models agent routing as\na decision layer that selects appropriate specialists for each query or subtask. For example, AgentRouter [386]\nproposes a knowledge-graph-guided router that leverages structured task semantics to dispatch questions to\nrelevant agents, enabling effective collaborative question answering without modifying individual agents.\nSimilarly, Talk to Right Specialists [387] frames routing and planning as a unified inference-time process,\nwhere a controller dynamically assigns subtasks to domain-specialized agents based on intermediate reasoning\nstates. These approaches highlight that agentic routing itself can be viewed as an inference-time realization\nof division of labor, where cognition is selectively offloaded to specialized agents.\n35\n"}, {"page": 36, "text": "Agentic Reasoning for Large Language Models\nTheory-of-Mind-Augmented Collaboration.\nAnother interesting line of research is Theory of Mind (ToM),\nwhich refers to the ability of an agent to infer and reason about the beliefs, intentions, and mental states of\nother agents. Li et al. [388] first showed that equipping LLM agents with explicit belief-state representations\nin a cooperative text game improves both collaboration performance and the accuracy over ToM-free LLM\nbaselines. Building on this, Hypothetical Minds [389] scaffolds ToM as a modular hypothesis-generation\nand refinement loop for other agents’ strategies, while MindForge [390] extends ToM-aware reasoning to\nembodied collaborative learning. In parallel, Wu et al. [391] provides a mechanistic account of how LLMs\nencode ToM, identifying sparse parameter patterns whose perturbation selectively disrupts social reasoning.\nPushing toward, ToM-agent [392] augments LLM generative agents with counterfactual reflection over\ncounterparts’ beliefs and BeliefNet [393] offers a ToM-centric joint-action simulator where embodied agents\nact based on nested belief states.\n5.2.2. Post-training Collaboration\nIn multi-agent systems, the design of agent prompts (or personas) and the interaction topology plays a\ncritical role in determining the system’s ability to solve complex tasks. Recently, optimizing these components\nduring the post-training phase has emerged as an important research direction. Based on the optimization\nobjective, existing studies can be broadly categorized into two lines of work: prompt optimization and\ntopology optimization.\nMulti-agent Prompt Optimization. Prompt optimization in multi-agent systems focuses on how agent roles,\nworkflows, and feedback are encoded in prompts to yield reliable coordination and stronger task performance.\nFor example, AutoAgents [16] extends prompt optimization from single-agent contexts to multi-agent teams,\nrefining role specialization and execution plans through structured dialogue among meta-agents. SPP [18]\nintroduces a cognitive synergist that dynamically selects multiple personas during multi-agent collaboration\nfor knowledge-intensive and reasoning-intensive tasks, enabling complementary expertise to emerge. DSPy\nAssertions [394] introduces LM Assertions that can be either hard (Assert) or soft (Suggest). When violated,\nthese assertions trigger backtracking and prompt revision using erroneous outputs and error traces. During\ncompilation, the mechanism bootstraps examples and counterexamples to reinforce few-shot prompts, which\nimproves both recall and accuracy. MASS [395] demonstrates that prompts are often the dominant factor\nin MAS performance, and further applies automatic prompt optimization [396] by incorporating local and\nglobal topology information to refine each agent’s prompt in a fine-grained manner.\nAs for topology optimization, two categories of research have emerged, each pursuing relatively independent\noptimization pathways. The first category of work treats the multi-agent topology as a communication\ngraph, leveraging graph-based methods to identify an optimal structure that achieves strong performance\nunder constrained communication costs (i.e., limited graph size). The second category adopts a policy-based\nperspective, where variable training paradigms are employed to learn an agent-selection policy with specially\ndesigned rewards or supervision signals. Through iterative, policy-based selection of subsequent agents, these\napproaches aim to progressively construct topologies that yield optimal overall performance. We discuss\nthese two categories of approaches in greater detail in the following paragraphs.\nGraph-based Topology Generation.\nA large body of work models multi-agent systems (MAS) as graphs\nwhere agents are nodes, and inter-agent communication forms edges. Then MAS design becomes a problem\nof learning the communication/coordination topology. These works could be roughly divided into three\ngroups as follows.\nGraph generation. These methods aim to construct communication topologies from scratch by adaptively\n36\n"}, {"page": 37, "text": "Agentic Reasoning for Large Language Models\ngenerating task-conditioned graphs. GommFormer [397] uses an encoder-decoder framework to learn the\ncommunication graph via continuous relaxation of the graph representation, optimizing topology end-to-end\nunder bandwidth constraints. G-designer [398] starts from a task-anchored network with a virtual task\nnode, then uses a variational graph auto-encoder to decode a query-adaptive communication graph. MCGD\n[399] builds a sparse coordination graph with continuous node and discrete edge attributes, and performs\ncategorical diffusion on edges and anisotropic diffusion on actions to capture structure diversity.\nGraph pruning. These works start from dense collaboration graphs and aim to prune them into compact,\ntask-appropriate pipelines while preserving utility and lowering token and compute costs. For example,\nAgentPrune [400] first formulates the MAS problem as a spatial-temporal graph sparsification problem,\nand then applies one-shot magnitude pruning to learn a sparse and effective pipeline. AGP [401] learns a\ndual-pruning policy, i.e., soft-pruning on edges and hard-pruning on nodes, to acquire a per-query topology.\nG-Safeguard [402] introduces pruning as a security mechanism. It treats communication edges as the search\nspace, employs a graph neural network to identify risky nodes, and applies deterministic rules to prune their\noutward edges based on a model-driven threshold, thereby defending the system against adversarial attacks.\nTopology search. This line of research explores the graph space by searching over agentic operators and\ncommunication edges to identify effective pipelines. Specifically, AFlow [332] automates multi-agent workflow\ndesign with Monte-Carlo Tree Search over a fixed library of operators. MASS pre-defines some influential\ngraph motifs, such as debating and tool-using, and then implements topology search inside this pruned motif\nsubset. Then MASS [395] performs a prompt search on that topology to maximize performance. MaAS [403]\nreplaces single-graph search with a probabilistic “agentic supernet” over layered operator choices and uses a\ncontroller to sample a query-conditioned subgraph. DynaSwarm [404] broadens the design space from a\nsingle optimized communication graph to a portfolio of candidate structures. It employs Actor–Critic (A2C)\noptimization to refine this portfolio and introduces a lightweight graph selector that chooses the most suitable\ntopology for each instance. GPTSwarm [68] formulates the search space as inter-agent connections within\na computational graph. It relaxes the discrete topology into continuous edge probabilities and leverages\nreinforcement learning to optimize the resulting connection schemes, thereby enabling flexible and adaptive\ngraph structures.\nPolicy-based Topology Generation.\nA growing line of research strengthens multi-agent pipeline generation\nby learning the policy of selecting subsequent agents with advanced training paradigms such as supervised\nfine-tuning (SFT), and reinforcement learning (RL). These approaches embed auxiliary signals into the\noptimization process, enabling agents to acquire stronger reasoning skills and more reliable coordination.\nRouting can be viewed as a special case of collaboration, in which a router conditions on task state and system\ncontext to learn a policy for selecting agents that maximize efficiency and performance [405, 406, 407, 408].\nBroadly, these methods can be grouped into three categories based on the signal type they inject into learning.\nRelative-advantage policy learning. Several approaches rely on critic-free objectives to form advantages,\nthereby avoiding centralized value models and providing effective guidance to optimize policy. For example,\nMAGRPO [409] proposes a Dec-POMDP formulation for LLM collaboration and replaces centralized critics\nwith a group-relative advantage signal, enabling decentralized training/execution at dialog-turn granularity.\nMHGPO [410] extends GRPO-style signals to heterogeneous groups: it jointly optimizes different agent roles\nvia a shared group-relative objective, and introduces practical sampling/optimization tweaks. COPY [26]\nutilizes two-agent co-training framework with shared rewards and KL regularization (to a frozen ref and\ncross-agent policies), improving stability and transfer between pioneer/observer roles on reasoning tasks.\nLLM-generated prior guidance. Other methods leverage LLMs to generate rewards or priors for learning.\n37\n"}, {"page": 38, "text": "Agentic Reasoning for Large Language Models\nSpecifically, LGC-MARL [411] uses an LLM to propose a Reward Function Generator (RFG) that turns\nnatural-language objectives into structured reward terms. LAMARL [412] lets an LLM synthesize a prior\npolicy and a task-specific reward function, then fine-tunes agents with RL. MAPoRL [413] defines rewards as\nweighted sums of LLM verifier scores on current and future turns, then updates policies with multi-agent PPO.\nCOPPER [326] learns a shared reflector with a counterfactual-PPO pipeline in which a learned reward model\nscores each agent’s reflection by its marginal contribution to task improvement. SIRIUS [414] builds an\nexperience library by retaining trajectories that lead to successful outcomes and augmenting failures, while a\nJudgment–Critic–Actor triad supplies LLM-generated correctness signals that filter and supervise subsequent\nfine-tuning across reasoning tasks. Multiagent Finetuning [415] bootstraps reasoning by running multi-\nagent debates among generator LLMs and using LLM critics plus majority voting to produce self-generated\nsupervisory signals, then fine-tunes role-specialized agents on critic-selected trajectories to improve both\naccuracy and diversity.\nHuman preference signals. This line of research replaces or augments environment rewards with human-\nderived feedback to align behavior with human intent, in both online and offline regimes. For instance,\nM3HF [416] organizes human input into multi-phase feedback (e.g., scalar ratings, pairwise comparisons,\nand natural-language rationales) processed by LLMs into reward shaping signals. O-MAPL [417] introduces\nan end-to-end preference-based learning framework and directly learns Q-values from offline preference\ndata, bypassing the two-stage reward-model-then-RL pipeline.\n5.3. Multi-Agent Evolution\nWhile self-evolving agents enable individual models to continuously improve through interaction and feedback,\nmany real-world applications require collective intelligence supported by cooperation among multiple agents.\nTherefore, recent studies extend self-evolution from single-agent settings including planning, tool-use, and\nsearch evolution [14, 342, 270, 344, 36] to multi-agent co-evolution, where adaptation emerges across\ndistributed agents [332, 418, 419, 420, 421]. Beyond evolving model parameters, memory, prompts, and\ntools [12, 422, 423, 424], multi-agent evolution further targets shared memory, communication mechanisms,\nand collaboration protocols [395, 421, 298].\nAs a result, multi-agent memory must jointly evolve along architecture, topology, content, and management\ndimensions, supported by hierarchy-structured, role-aware architectures [425], governed and distributed\nstorage topologies [426, 427], modular and task-structured memory contents [328, 428], and active man-\nagement mechanisms for compression, verification, and continual updating [429, 430] to ensure coherent\nand scalable collaboration.\nThe goal thus shifts from optimizing a single agent’s capability to improving the collective performance of\nmultiple agents on complex, long-horizon tasks [332, 418, 419, 70].\n5.3.1. From Single-Agent Evolution to Multi-Agent Evolution\nWhile the shift from single-agent evolution to multi-agent co-evolution broadens the spatial dimension of\nadaptation from an individual model to a collective, the temporal dimension of evolution remains equally\ncrucial. Beyond determining who evolves (a single agent or a population), recent studies also investigate\nwhen and how fast agents should adapt during interaction. This perspective leads to a complementary\naxis of analysis that distinguishes short-horizon, within-episode updates from long-term, cross-episode\nimprovements, commonly referred to as intra-test-time evolution and inter-test-time evolution. We summarize\nthese temporal modes of self-evolving behavior.\n38\n"}, {"page": 39, "text": "Agentic Reasoning for Large Language Models\nIntra-test-time evolution refers to the ability of agents to adapt and improve during task execution, enabling\nthem to correct failures and refine strategies on the fly when facing unseen states or unexpected feedback.\nUnlike static inference pipelines, this paradigm embeds self-reflection, dynamic planning, memory rewriting,\nor even localized fine-tuning into the execution loop. Representative works leverage natural-language\nself-critique [14, 270] and runtime adaptive planning [342, 431] to generate corrective signals without\nexternal supervision. Reflexion [14] allows agents to store distilled reflective feedback for immediate\nbehavior improvement, while AdaPlanner [342] dynamically revises and replans mid-trajectory based on\nenvironmental mismatch detection. Beyond contextual adaptation, methods such as test-time supervised\nupdating [432] and test-time reinforcement learning (TTRL) [433, 434] directly modify model behavior\nwhen encountering difficult cases, often through problem-variant generation and targeted optimization.\nThese approaches demonstrate that performance at inference time can improve within a single episode,\nforming short-horizon adaptation loops where the model learns while solving, rather than merely executing\na fixed policy.\nInter-test-time evolution extends the self-improving process to across-task learning, where adaptations made\nin one task can be consolidated and transferred to future tasks. This enables the accumulation of persistent,\ngeneralizable capabilities over a lifelong interaction stream. A prominent paradigm involves offline self-\ndistillation, where the agent generates responses and then refines them via self-evaluation before using them\nfor supervised fine-tuning-such as in SELF [337], STaR [435], and Quiet-STaR [436]. These methods turn\nincorrect initial reasoning into high-quality labeled data for future performance gains. Additionally, online\nreinforcement learning frameworks such as RAGEN [344] and DYSTIL [345] continuously update policies\nbased on dense interaction feedback, allowing agents to gradually internalize complex decision-making\nstrategies over long horizons. Inter-test-time evolution can also incorporate curriculum mechanisms that\nautomatically adjust task difficulty and environment complexity [437, 438], as well as experience structuring\nvia memory evolution to preserve accumulated reasoning heuristics [439, 440, 298]. This temporal mode\nfocuses on stable long-term improvement, transforming short-lived corrections from individual tasks into\ncontinual competence growth across diverse task distributions.\nTo support these new capabilities, mechanisms evolve from individual reward-based or reflective adapta-\ntion [337, 338, 339] to multi-agent reinforcement learning and game-theoretic co-optimization [419, 420],\nenabling collaborative structures to self-organize under evolving task requirements. Moreover, memory-\ndriven multi-agent evolution (e.g., shared workflow memory or knowledge graphs) helps maintain accu-\nmulative group intelligence across episodes [298, 13]. Overall, multi-agent evolution transforms isolated\nself-improvement loops into adaptive intelligent ecosystems capable of self-correction, self-organization, and\nsocial learning. This transition marks a critical step toward artificial collective intelligence, where cooperative\ndynamics drive continuous progress beyond the capabilities of any individual agent [395, 332, 418, 441].\n5.3.2. Multi-agent Memory Management for Evolution\nMulti-agent LLM systems pose unique challenges for memory design compared with single-agent settings.\nBeyond maintaining an individual agent’s local context, they must capture inter-agent interactions, track\nroles and dependencies over time, and preserve both shared and private knowledge coherently. Memory must\nalso remain scalable as collaboration grows and interactions accumulate. To provide a clearer understanding\nof this landscape, we categorize existing approaches along four key dimensions: (1) architecture, how\nmemory is organized within and across agents; (2) topology, whether it is centralized, distributed, or hybrid;\n(3) content, the type and structure of stored knowledge; and (4) management, how memory is written,\nretrieved, and updated over time. Illustrations are shown in Figure 10.\n39\n"}, {"page": 40, "text": "Agentic Reasoning for Large Language Models\nHierarchical\nFlat\nCentralized\nDecentralized\nSemantic\nProcedural\nSummarize \nand Forget\nFilter and \nVerify\nArchitecture\nTopology\nContent\nManagement\nMulti-agent Memory\nFigure 10: Four dimensions of multi-agent memory design. The framework includes (1) Architecture,\nhow memory is structured; (2) Topology, where it is stored and shared; (3) Content, what type of knowledge\nis stored; and (4) Management, how it is maintained and updated.\nArchitecture Dimension: Hierarchical and Heterogeneous Designs.\nRecent work highlighted that\nprevailing multi-agent memory mechanisms were overly simplistic and lacked per-agent customization [425].\nTo address this, G-Memory constructs a three-tier graph hierarchy (insight, query, interaction graphs) that\nseparates high-level generalizable insights from fine-grained execution traces. This hierarchical approach\nenables bi-directional memory traversal for retrieving both abstract lessons and concrete precedents across\nepisodes. However, instead of global aggregation, Intrinsic Memory Agents adopts an opposing strategy by\nmaintaining dedicated role-aligned memory templates for each agent [442]. This heterogeneous approach\npreserves specialized perspectives on collaborative planning benchmarks by reducing irrelevant information\nper agent. Recent work further explores hybrid strategies, with some systems employing adaptive hierarchical\nknowledge graphs in decentralized architectures that allow agents to reason over past interactions and\nshare only relevant information rather than raw experiences [443]. These contrasting approaches reveal a\nfundamental trade-off: hierarchical designs optimize for global coherence and cross-episode learning, while\nheterogeneous designs optimize for role fidelity and computational efficiency.\nStorage Topology and Memory Governance.\nSystems employ different topologies to balance scalability,\nprivacy, and coherence, each reflecting different assumptions about trust and coordination. SEDM (Self-\nEvolving Distributed Memory) [426] tackles memory management by turning memory into an active,\nself-optimizing component through verifiable write admission (via reproducible replay) and utility-based\nconsolidation. This centralized approach with verification gates ensures that only factual or useful information\nenters the repository and performs cross-domain knowledge diffusion to enable transfer across heterogeneous\ntasks. In contrast, when privacy and organizational boundaries matter, Collaborative Memory\n[427]\ndistinguishes private versus shared memory fragments using bipartite graph policies. Every entry carries\nimmutable provenance (source agent, accessed resources, timestamp), enabling compliance auditing and\nsafe cross-agent knowledge transfer in federated systems. At the other end of the spectrum, some systems\nlike Memory Sharing [444] adopt uncontrolled pooling where all agents freely exchange experiences in\n40\n"}, {"page": 41, "text": "Agentic Reasoning for Large Language Models\na shared memory pool. Research shows that memory sharing among LLM agents leads to a more diverse\ncollective memory pool, which improved performance on open-ended tasks by creating emergent collective\nintelligence. These three topologies represent increasing levels of formality and control, reflecting different\npriorities for managing the trade-off between knowledge diversity and verification rigor.\nMemory Content: Semantic, Task, and Cognitive-Phase Decomposition.\nDifferent content decomposition\nstrategies suit different task characteristics, and the choice of content structure fundamentally shapes how\nagents interact with memory. MIRIX [328] pioneered semantic decomposition by defining six specialized\nmemory types (Core, Episodic, Semantic, Procedural, Resource, Knowledge Vault) managed by distinct\nagents, achieving a 35% accuracy gain on multimodal QA tasks while reducing storage through flexible\nrouting. Building on this modular principle, LEGOMem [428] instead employs task-based decomposition,\nbreaking execution traces into reusable memory units flexibly assigned to either central planners or specialist\ntask agents. This design shows that orchestrator memory improves task decomposition and delegation, while\nagent memory enhances subtask execution, effectively narrowing performance gaps between small and\nlarge LLM teams. Recently, MAPLE introduced Cognitive-phase Decomposition [445], using specialized\nagents (Solver, Checker, Reflector, Archiver) to enable systematic error detection and plan repair cycles.\nThe Reflector diagnoses errors after each episode, and the Archiver stores refined plans to avoid repeated\nmistakes, supporting feedback-driven learning. These three content decomposition strategies reveal that\nmemory design should align with task structure: semantic content for heterogeneous information, task-based\nfor workflow automation, and cognitive-phase for error-sensitive reasoning.\nMemory Management Strategies.\nEffective long-term memory requires active management balancing\nrelevance, efficiency, and coherence through different approaches that trade off simplicity against sophistica-\ntion. Lyfe Agents [429] pioneered the forgetting-based approach using Summarize-and-Forget mechanisms\nto regularly compress memory, retaining only critical context. This strategy is suitable when storage is\nseverely constrained, though it risks losing nuanced details for edge cases. To improve upon simple forgetting,\nAGENT-KB [430] introduced more sophisticated management by organizing procedural traces into structured\n(entity, action, observation) triples and learning pattern abstractions reusable across tasks. Agents collaborate\nto retrieve, update, and reason over memory segments, enabling generalization without explicit retraining\nwhile central coordination ensures long-term consistency for scalable embodied planning. The choice among\nthese strategies depends on system priorities: forgetting prioritizes storage efficiency, verification prioritizes\nreliability, and learning-based approaches prioritize adaptability. Production systems typically combine\nstrategies, e.g., verification for critical memories and forgetting for low-utility peripheral information, to\nbalance multiple objectives.\nDiscussions.\nDespite substantial progress, multi-agent memory systems remain largely unexplored with\nrespect to post-training and model adaptation. Current approaches focus primarily on memory organization\nand retrieval for pre-trained models, with little investigation into how multiple agents can jointly optimize\ntheir memories through post-training procedures such as reinforcement learning or supervised fine-tuning.\nThis represents a notable gap: while post-training techniques have been actively explored for single-agent\nmemory systems, extending them to enable multi-agent teams to co-evolve their memory structures and\nmanagement policies remains an open problem.\n41\n"}, {"page": 42, "text": "Agentic Reasoning for Large Language Models\n5.3.3. Training Multi-agent to Evolve\nRecent advancements have shifted multi-agent systems from fixed, hand-designed coordination toward train-\ning paradigms that enable agents to evolve over time [26, 446, 414]. Training multi-agent systems to evolve\nrepresents a critical step toward realizing adaptive, long-horizon intelligence beyond static coordination.\nIn this emerging paradigm, agents improve collectively through interaction, feedback, and shared memory,\nrather than isolated or independently optimized behaviors. By embedding reasoning into the learning loop,\nvia reinforcement learning [447], self-play [448], curriculum evolution [413], and verifier-driven feedback\n[449], multi-agent systems can internalize coordination strategies, address inter-agent credit assignment,\nand progressively refine divisions of labor. This evolution transforms multi-agent reasoning from a static\nensemble of cooperating LLMs into a self-improving organization that adapts its structure, communication\npatterns, and policies in response to task complexity and environmental change [450].\nCo-evolution via Interaction and Intrinsic Feedback.\nA growing body of work has operationalized multi-\nagent evolution through explicit training objectives that couple interaction, feedback, and role specialization.\nFor instance, Multi-Agent Evolve [446] instantiates a closed-loop co-evolution framework containing three\ninteracting roles (Proposer, Solver, and Judge), all of which are derived from a shared LLM backbone and\njointly optimized via reinforcement learning. This forms a self-improving curriculum that enables collective\nskill growth without external supervision. In a related spirit, CoMAS [451] emphasizes intrinsic interaction\nrewards, extracting learning signals directly from multi-agent discussion dynamics through an LLM-based\njudge, thereby enabling decentralized co-evolution driven purely by collaborative interaction.\nMulti-Agent Reinforcement Fine-Tuning for Collective Adaptation.\nAdditional works have focused on\nprincipled reinforcement fine-tuning frameworks tailored to LLM-absed multi-agent systems. For example,\nMARFT [447] formalizes multi-agent reinforcement fine-tuning by highlighting key mismatches between\nclassical MARL assumptions and LLM-based agent organizations, such as role heterogeneity, dynamic\ncoordination, and long-horizon dialogue, and provides a systematic framework for stabilizing collective\npost-training. Stronger-MAS [448] further adapts on-policy reinforcement learning to multi-role, multi-turn\nsettings by introducing agent- and turn-wise grouping strategies that extend GRPO-style optimization,\nenabling more effective coordination learning across complex agent workflows. Similarly, MAPoRL [413]\nproposes multi-agent post-co-training, where multiple LLMs are jointly optimized using a collaboration-aware\nverifier that rewards not only final outcomes but also the quality of intermediate discussions, encouraging\nthe emergence of transferable communication strategies.\nRole Specialization and Joint Credit Assignment.\nOther approaches have explored structured role\nspecialization and joint credit assignment. MALT [452] trains sequential pipelines of heterogeneous agents\nusing trajectory expansion and outcome-based reinforcement signals, allowing each agent to improve its\nspecialized function while optimizing end-to-end collaborative performance. MARS [453] extends this idea\nto long-horizon research settings by jointly training complementary System 1 (fast, intuitive) and System 2\n(deliberate, tool-using) agents via multi-agent reinforcement learning, enabling adaptive division of labor\nunder complex tool interactions.\nPreference- and Alignment-Driven Multi-Agent Evolution.\nFinally, another line of work has studied\nevolution under preference- and alignment-driven objectives. Preference-based multi-agent reinforcement\n42\n"}, {"page": 43, "text": "Agentic Reasoning for Large Language Models\nFoundational\nSelf-evolving\nCollective\nReasoning Layers\nAgentic Reasoning \nApplications \n(▷§Section 6)\nWeb Exploration and \nResearch Agents \n(▷§Section 6.5)\nMath Exploration \nand Vibe Coding \n(▷§Section 6.1)\nScientific Discovery \nAgents (▷§Section 6.2)\nEmbodied Agents \n(▷§Section 6.3)\nHealthcare & \nMedicine Agents \n(▷§Section 6.4)\nPlanning Tool Use\nSearch\nFeedback Memory Evolving\nRole \nTaxonomy\nMulti-agent \nCollaboration\nMemory\nCo-evolving\nFigure 11: An overview of the applications of agentic reasoning.\nlearning [449] studies how collective policies and equilibria can be learned from preference-only feedback,\naddressing data coverage and stability challenges inherent in multi-agent settings. From a safety perspective,\nAlignment Waltz [454] frames alignment as a cooperative co-evolution process between a generation agent\nand a feedback agent, where evolving guidance enables the system to iteratively refine unsafe or unhelpful\nbehaviors. Collectively, these methods demonstrate how embedding reinforcement learning, co-evolution,\nand verifier-driven feedback into multi-agent training enables LLM-based systems to evolve from static\ncollaborations into adaptive, self-improving organizations.\n6. Applications\nBuilding on the established three-layer taxonomy (i.e. foundational, self-evolving and collective reason-\ning) mentioned in previous sections, we now examine how these capabilities manifest across real-world\napplications. This section surveys representative reasoning-empowered agentic systems across several key\ndomains, as illustrated in Figure 11, including math exploration and vibe coding (Section 6.1), scientific\ndiscovery (Section 6.2), robotics (Section 6.3), healthcare (Section 6.4), and autonomous web exploration\nand research (Section 6.5). Specifically, each domain exhibits distinctive forms of reasoning, influenced by its\ndata modalities and environmental constraints. Accordingly, our discussion in each subsection is organized\naround three layers: (1) core abilities such as planning, tool use and search that span scientific hypothesis\ngeneration, embodied control, medical reasoning, automated experimentation and symbolic problem solving,\nfor example; (2) self-evolving abilities that integrate feedback, reflection and memory modules which\nrefine domain-specific competence through iterative experiment loops, lifelong skill learning and clinical\nadaptation; and (3) collective multi-agent reasoning that enables collaboration and specialization from\ncooperative scientific assistants to coordinated robotic teams, diagnostic ensembles or multi-aspect experts.\nThis section highlights how agentic reasoning frameworks adapt to domain-specific knowledge structures\nand tasks, illustrating the transition from traditional LLM reasoning to goal-directed, domain-aware and\nactive agentic intelligence.\n43\n"}, {"page": 44, "text": "Agentic Reasoning for Large Language Models\n6.1. Math Exploration & Vibe Coding Agents\nMathematics and code have traditionally served as two of the most widely used domains for evaluating\nreasoning in artificial intelligence, as both require structured symbolic manipulation and precise multi-step\ndeduction. Traditional benchmark-driven evaluation in these domains is showing clear limitations. Widely\nused math datasets such as GSM8K [455], MATH [456], and AIME [457] are increasingly saturated, which\nmakes it difficult to distinguish among modern high-performing models. The problems in these datasets\noften rely on a small set of recurring techniques and do not require the sustained and exploratory reasoning\nneeded to assess more advanced mathematical capabilities. Even recent evaluations such as FrontierMath\n[458] continue to emphasize final-answer accuracy, which offers only a partial view of an agent’s reasoning\nprocess and its ability to adjust strategies during problem solving.\nUnder the agentic reasoning paradigm, however, both areas are undergoing a substantial shift from static\nproblem solving to dynamic processes that emphasize exploration, adaptation, and collaboration. In mathe-\nmatics, recent systems [70, 459, 29, 30] demonstrate that agents can engage in competition-level reasoning,\nbuilding on the success of LLMs in coding tasks. Work in foundational mathematics [460, 461] further shows\nthat agents can search for new problems, propose conjectures, construct auxiliary lemmas, and explore\ndeeper structures in mathematical concepts. These developments position mathematics not merely as an\nevaluation benchmark but as a domain of active mathematical exploration.\nLarge Language Models have also reshaped coding through the emerging workflow known as agentic coding\nand vibe coding [32, 462]. In this paradigm, the model acts as an interactive collaborator that engages in\nmulti-turn natural-language dialogue. Users iteratively design and refine programs while the agent maintains\ncontext, adapts to evolving requirements, and continuously self-corrects. Modern tools such as Copilot1 and\nCursor2 have further popularized this collaborative workflow, making interactive programming a common\npractice in real-world software development.\nIn this section, we organize our discussion according to the three-layer framework introduced earlier. The\nfoundational layer (Section 6.1.1) concerns the core reasoning and execution skills: mathematical agents\nperform symbolic manipulations and step-by-step derivations across arithmetic, algebra, geometry, and\ncalculus, while code agents carry out syntax-aware generation, implement functions, and verify correctness\nthrough interpreter or compiler feedback. The self-evolving layer (Section 6.1.2) introduces mechanisms for\nreflection and adaptation. Mathematical agents learn from intermediate reasoning traces to correct missteps\nor explore alternative solution paths, and code agents iteratively debug, refine, and optimize implementations\nbased on runtime feedback or test results. The collective layer (Section 6.1.3) focuses on collaboration,\nwhere agents exchange intermediate results, share reusable modules, and jointly develop complex proofs or\ncodebases. Taken together, these layers reveal how mathematics and coding are becoming domains in which\nagentic reasoning enables increasingly creative and adaptive problem solving.\n6.1.1. Foundational agentic reasoning\nPlanning.\nExplicit planning is widely recognized as a core mechanism for enhancing the structured\nreasoning capabilities of LLMs. In the domain of mathematical discovery, several systems exhibit structures\nthat can be interpreted as forms of planning. In representation theory and knot theory, the system of Davies\net al. [463] guides human mathematicians by proposing intermediate objects and promising avenues of\nexploration, which function as high-level suggestions for organizing problem-solving workflows. In geometric\n1https://github.com/features/copilot\n2https://cursor.com\n44\n"}, {"page": 45, "text": "Agentic Reasoning for Large Language Models\nreasoning, Trinh et al. [29] solves Olympiad-level geometry problems by decomposing them into sequential\nstages of construction, lemma generation, and verification, yielding a structured multi-step process that\nresembles a planned reasoning trajectory. Program-search approaches [30] iteratively refine candidate\nprograms and mathematical structures, a procedure that naturally forms a coarse-to-fine exploration path.\nLarge-scale exploration frameworks [461, 460] also operate through cycles of proposing, testing, and\nmodifying conjectures or geometric objects, which collectively create a procedural structure aligned with\nplanning. Efforts toward more robust mathematical reasoning [459] similarly rely on stepwise reasoning\npatterns, further reinforcing the presence of implicit planning dynamics. of implicit planning dynamics\nacross mathematical agents.\nIn code agents, planning has likewise emerged as an essential component for organizing multi-step reasoning\nand enabling more structured decision-making. Early systems such as CodeChain [464] and CodeAct [99]\nintroduce explicit planning or action spaces to support modular code construction, while KareCoder [465]\nintegrate external knowledge sources or domain-specific information into the planning process. Subsequent\nworks explore more structured planning organizations, including multi-stage control flows [466, 467], tree-\nshaped planning structures [468, 22], and adaptive refinement mechanisms [469]. Planning has also been\nlinked to improved exploration breadth: GIF-MCTS [470] incorporates Monte Carlo Tree Search to explore\nmultiple code-generation trajectories. Recent extensions demonstrate applicability in specialized domains\nsuch as hardware design, where VerilogCoder [471] employs graph-structured planning and waveform-based\nverification. To address environments where state serialization is difficult, Guided Search [472] introduces\nlookahead and trajectory selection strategies for evaluating candidate actions without full environment\naccess.\nTool-Use.\nIntegrating external computational tools with LLMs has become a central mechanism for ex-\ntending the reasoning and generation capabilities of single-agent systems. A defining characteristic of many\nmathematical reasoning systems is their integration with external computational tools. Formal theorem-\nproving agents such as Thakur et al. [473] operate directly within the Lean proof assistant, selecting tactics\nand interacting with the underlying prover through in-context guidance. Position papers on formal math-\nematical reasoning [474] emphasize that progress in mathematical AI will depend on systems that can\ncall theorem provers, satisfiability solvers, and computer algebra systems as part of a broader reasoning\nloop. Program-search frameworks for discovery [30] rely on executing generated programs and employing\nsymbolic routines for verification. Generative modelling approaches [475] make use of computational\nnumber-theoretic tools to check and filter generated candidates. Geometry-focused systems [29, 476] inte-\ngrate automated geometric solvers and checkers to validate constructions and derived relations. Across these\nsystems, external computational resources play a central role in enabling correct and scalable mathematical\nreasoning.\nIn code agents, external tools have similarly become crucial for extending the capabilities of LLM-based\nagents beyond pure text generation. Early work such as Toolformer [6] and ToolCoder [477] explored how\nmodels can learn to invoke APIs or search tools to obtain missing information during generation. Subsequent\nsystems integrate increasingly rich toolchains: ToolGen [478] leverages automatic completion tools to resolve\nundefined dependencies, while CodeAgent [479] incorporates multiple programming utilities including\nsearch, documentation reading, symbol navigation, and code execution to support more realistic software\nworkflows. Several methods focus on improving tool-feedback loops, such as ROCODE [480], which combines\nreal-time error detection with adaptive backtracking, and CodeTool [481], which introduces process-level\nsupervision to improve the reliability of tool invocation. Collectively, these systems show that tool integration\nprovides essential external signals, via search results, documentation, static analysis, or execution feedback,\n45\n"}, {"page": 46, "text": "Agentic Reasoning for Large Language Models\nthat extend the reasoning and generation capabilities of single-agent LLMs.\nSearch and Retrieval.\nSearch and retrieval has emerged as a complementary mechanism that enriches\nmodel contexts through external information sources. Search is a recurring mechanism in mathematical\ndiscovery. Program-search based systems [30] treat mathematical discovery as navigating a program space\nin which candidate programs encode conjectures or structural hypotheses, with iterative filtering based on\nsymbolic or numerical checks. Generative modelling approaches [475] explore families of mathematical\nobjects by sampling from flexible distributions that capture structural regularities. Geometric systems such as\nTrinh et al. [29] and Swirszcz et al. [460] search over constructions, configurations, and high-dimensional\npolytopes, guided by learned heuristics or structural constraints. Large-scale discovery frameworks [461]\noperate through repeated propose–test–refine cycles across conjectures, supporting wide exploration over\nmathematical landscapes. All these systems rely on systematic search procedures that structure the exploration\nof mathematical ideas.\nIn code generation, repository-level retrieval systems such as RepoHyper [482] locate reusable code segments\nfrom large-scale code bases to provide more informative contexts for generation. CodeNav [79] dynamically\nindexes real repositories during generation, retrieving relevant functions and adjusting based on execution\nfeedback. AUTOPATCH [483] applies retrieval to performance optimization, combining historical code\nexamples with control flow graph analysis for context-aware improvements. Structure-aware retrieval has\nalso been explored: knowledge-graph-based repository representations [484] improve retrieval quality by\ncapturing symbolic and relational structure, while cAST [485] introduces AST-based chunking to enhance\nsyntactic coherence and retrieval granularity. These retrieval methods demonstrate how external knowledge\nsources can augment single-agent LLMs by providing high-quality, structured contexts that guide both\nunderstanding and generation.\n6.1.2. Self-evolving agentic reasoning\nAgentic Feedback and Reflection.\nAcross mathematical and code reasoning tasks, feedback operates as an\nexternal signal that highlights discrepancies, confirms correct inferences, and directs the agent toward more\nreliable subsequent computations. Feedback mechanisms appear prominently across mathematical discovery\nsystems. In program-search based discovery [30], executing candidate programs and evaluating their outputs\nagainst constraints yields counterexamples or confirmations, enabling iterative refinement of conjectures.\nIn geometry, automated checkers validate constructions and derived relationships [29, 476], providing\ncorrectness signals that guide subsequent revisions. Interactive evaluation frameworks [486] show that\nhuman clarifications and follow-up prompts expose reasoning errors and improve model responses. Position\nwork on formal reasoning [474] highlights verification, proof checking, and model checking as essential\nsources of structured feedback. In several systems involving multiple candidate hypotheses [30, 475, 461], the\nuse of verification signals to retain promising candidates functions analogously to a fitness-based evaluation\nstep, since these signals determine which hypotheses survive and which are discarded, thereby shaping the\ndirection of subsequent exploration without introducing an explicit learning signal.\nFor code agents, feedback and reflection are central to improving reliability over multi-step reasoning. Fault-\naware editing methods such as Self-Edit [487] incorporate execution-based signals to refine erroneous code,\nwhile Self-Repair [488] integrates code and feedback models to diagnose test failures and propose targeted\ncorrections. More structured systems like LeDeX [489] combine stepwise annotation, execution-driven\nverification, and automated repair into a closed-loop pipeline in which feedback continually informs the next\nrevision. Reflection also functions as a form of memory: iterative self-improvement frameworks such as Self-\n46\n"}, {"page": 47, "text": "Agentic Reasoning for Large Language Models\nRefine [270], Self-Iteration [490], and Self-Debug [491] reuse earlier drafts, analyses, and explanations to\nguide subsequent revisions, while artifact-level mechanisms such as CodeChain [464] and LeDeX [489] retain\nreusable components, corrected snippets, and execution traces as persistent representations. Together, these\napproaches demonstrate how feedback—whether symbolic, execution-based, or self-generated—interacts\nwith iterative memory to support structured refinement and long-horizon improvement in code-oriented\nagentic systems.\nMemory.\nMemory provides agents with a mechanism for retaining and leveraging information from earlier\nreasoning steps, allowing them to maintain consistency, improve intermediate states, and improve their\nperformance over extended problem-solving horizons. While few systems introduce an explicit memory\nmodule, many mathematical agents rely on forms of persistent state that can be viewed as implicit memory.\nInteractive evaluation frameworks [486] maintain conversational and problem-state context across multiple\nturns, allowing models to build upon earlier partial derivations. Formal-theorem-proving agents [473]\noperate over evolving proof states in Lean, which accumulate tactics, subgoals, and intermediate lemmas,\nfunctioning as structured persistent information. Program-search and discovery systems [30, 461] retain\nconjecture histories, counterexamples, and successful constructions as part of their iterative refinement\nprocesses. Their role in preserving and reusing information across reasoning steps aligns with the broader\nnotion of memory in agentic systems.\nIn code agents, memory increasingly takes the form of explicit structures that maintain coherence over\nlong-horizon generation. Several systems construct shared or structured workspaces: Self-Collaboration\n[492] introduces a blackboard memory for storing task descriptions, intermediate drafts, and revision\nrecords, enabling agents to coordinate through a common representation. Architectural approaches such as\nL2MAC [493] and Cogito [494] extend this idea by organizing context into dedicated registers, hierarchical\nmemory units, or long-term knowledge stores, overcoming context-window limits and supporting multi-file\nor large-function reasoning. Across these designs, the underlying insight is consistent: effective code agents\nrequire persistent, structured, and often domain-aware memory that preserves intermediate reasoning and\nenables self-improvement across extended development trajectories.\n6.1.3. Collective multi-agent reasoning\nTo address the growing complexity of tasks in mathematical discovery and code generation, recent systems\nincreasingly rely on multi-agent or modular designs that decompose problems into cooperating specialized\ncomponents. Mathematical discovery frameworks often organize reasoning into explicitly defined multi-agent\nor multi-component workflows that collaborate to explore and validate mathematical ideas. The polytope-\ngeneration system [460] uses multiple specialized components that generate, evaluate, and refine geometric\nobjects, forming a genuine collaborative workflow. Large-scale exploration frameworks [461] often divide\ndiscovery into modules for proposing conjectures, identifying counterexamples, and refining statements,\nwhich, although implemented within a unified system, mirror multi-agent role specialization. Early work\non AI-assisted mathematical research [463] and Olympiad-level systems [476] also involve human–AI\ncollaboration, where human mathematicians interact with AI systems in a complementary manner. These\ndevelopments indicate that mathematical discovery is an inherently collaborative process, and multi-agent\narchitectures provide a natural vehicle for expressing such collaboration in agentic systems.\nMulti-agent systems for code generation have progressed from simple role-based pipelines to adaptive,\ncollaborative frameworks capable of handling long-horizon software development. Early approaches such as\nSelf-Collaboration [492] and AgentCoder [495] decompose tasks into sequential roles, while hierarchical\n47\n"}, {"page": 48, "text": "Agentic Reasoning for Large Language Models\ndesigns like PairCoder [496] and FlowGen [497] introduce an architecture in which high-level agents handle\nplanning and lower-level agents carry out concrete implementation. Flexible systems such as SoA [498]\nfurther adjust the number and specialization of agents in response to task complexity. Other frameworks,\nincluding MapCoder [499], AutoSafeCoder [500], and QualityFlow [501], rely on repeated cycles in which\nmultiple agents generate, test, analyze, and repair code. Recent work explores self-evolving system structures,\nas in SEW [502], which reorganizes collaboration pathways based on runtime feedback, and EvoMAC [351],\nwhich adjusts agent strategies through an iterative text-based update mechanism. Collaborative optimization\nmethods such as Lingma SWE-GPT [503], CodeCoR [504], SyncMind [505], and CANDOR [506] explicitly\nimprove cross-agent coordination. Together, these systems show a clear shift toward multi-agent code\ngenerators that rely not only on role decomposition, but also on reflection, distributed evaluation, adaptive\nrestructuring, and team-level optimization, transforming code generation into an increasingly coordinated\nand resilient problem-solving process.\n6.2. Scientific Discovery Agents\nScientific-discovery agents aim to accelerate the entire life cycle for scientific research, from hypothesis\ngeneration through experimental execution, by coupling LLMs with domain-specific simulators, laboratory\nautomation and up-to-date literature. These systems ground decision in verifiable processes while handling\nheterogeneous data, safety constraints and long-horizon goals.\nIn this subsection, we begin with the foundational layer (Section 6.2.1), which encompasses planning under\nscientific context, tool-augmented interaction with scientific resources, search and retrieval mechanisms\nincluding RAG-based systems and execution-time integration with laboratory hardware. Building upon these\ncapabilities, the self-evolving layer (Section 6.2.2) introduces agentic memory, feedback and reflection, which\nenable scientific agents to refine hypotheses, adapt protocols and learn from experimental outcomes. Finally,\nthe collective layer (Section 6.2.3) explores multi-agent collaboration, where agents coordinate roles, share\nintermediate knowledge and jointly reason toward complex scientific goals.\n6.2.1. Foundational agentic reasoning\nPlanning.\nScientific agents utilize reasoning-enhanced planning ability to decompose a research goal into\nsteps, decides which tool or simulator to call next, then revises the plan as evidence arrives. In short, the chain\nof thought emerges from LLM reasoning that compiles instructions into rigorous executable plans [1]. For\nexample, ProtAgents [507] materializes a planner agent that utilizes LLM reasoning capability to formulate\na concrete plan for protein analysis and keep modifying it with feedback from another critic agent, and\nEunomia [508] uses ReAct-style [5] workflow to make in-context reasoning: after retrieving a top-k evidence\nset, the backbone LLM quote a warranting sentence, and that citation drives the next action choice. Other\nexamples include MatExpert [35], which deploys a chain-of-thought LLM to author a stepwise transition\npathway and then emits a structured crystal candidate from a feedback loop.\nPlanning can also act as reasoning constraint. For instance, Curie [509] utilizes a rigor engine to align, setup\nand do reproducibility check within planning steps proposed by the Architect LLM. Thus, the Architect’s free-\nform reasoning cannot advance unless these rigor gates are satisfied, which transforms planning into both a\nguide and a regulator of the reasoning process. In addition, a general purpose biomedical agent, Biomni [40]\nconstrains its reasoning within a dynamically constructed biomedical action space of comprehensive tools,\nsoftware packages and databases, requiring each hypothesis to be operationalized as executable code.\n48\n"}, {"page": 49, "text": "Agentic Reasoning for Large Language Models\nTool-Use.\nTool use is an important part of the reasoning loop for scientific agents nowadays. Specifically,\nrather than following rigid rules, these agents can decide which tool and when to call, how to fill parameters\nand verify or revise based on evidence. For example, SciAgent [510] formalizes tool-augmented reasoning as\na four-step procedure: planning, retrieval, tool-based action and execution. Agents are trained to decide\nwhen to call a tool, which one, and how to integrate it into solving scientific tasks. Through domain-specific\ntools, ChemCrow [33] chains various expert chemistry tools so intermediate calculations become premises\nin the next reasoning step, which enables end-to-end planning and autonomous syntheses. CACTUS [511]\nsimilarly grounds explanations in cheminformatics outputs, reducing reliance on free-form reasoning by\nlanguage models alone.\nOther notable examples include ChemToolAgent [512] and CheMatAgent [513]. In particular, ChemToolA-\ngent [512] employs a ReAct-like [5] architecture with multiple specialized chemistry tools, allowing the\nLLM to choose and parameterize tool calls while CheMatAgent [513] pushes further by learning tool use: it\nintegrates over 100 chemistry/materials tools, curates a tool-specific benchmark, and uses Monte Carlo Tree\nSearch with step-level fine-tuning to learn both which tool to pick and how to fill arguments.\nFor biomedical agents, TxAgent [514] scales therapeutic reasoning across 211 vetted tools and it carries\nout multi-step reasoning that reconciles drug labels, interactions, and patient context—turning clinical\njustification into an executable trace. On the other hand, AgentMD [515] builds a two-stage tool memory:\nit first mines thousands of clinical calculators from literature (i.e. making tools), then selects and applies\nthe right ones at inference (i.e. using tools), pinning predictions to concrete computations. Other recent\nsystems [516, 517, 518, 519, 520, 521] reinforce similar design: co-design tool-use with reasoning so each\nclaim is computable and auditable.\nAnother notable category of tool-use is the ability of agentic execution, which includes but not limited to run\ncodes and simulate environments. Execution layers bridge high-level plans to physical infrastructure, which\nenables scientific agents to autonomously operate laboratory hardware, orchestrate simulation pipelines,\nand manage large-scale data workflows. Recent works such as Organa [522] ties LLM reasoning to task-\nand-motion planning plus scheduling and perception, executing multi-step experiments with autonomous\nrobots; AtomAgents [523] exemplifies the simulation side of execution: a physics-aware system that plans\nand runs atomistic workflows, coordinating tools for code execution, analysis, and hypothesis checking; and\nChemist-x [524] shows wet-lab execution beyond a digital-only scenario, where agents generate control\nscripts and drive an automated platform to validate conditions without human intervention.\nSeveral other platforms couple execution with optimization or team-based autonomy. For instance, SGA [525]\nformalizes the workflow of LLM-as-proposer and simulator-as-optimizer while MatExpert [35] operates a\nretrieval, transition and generation workflow for material discovery tasks, and CellAgent [526] coordinates\nplanner, executor and evaluator roles to run full single-cell analysis pipelines.\nSearch and retrieval.\nBeyond simple context stuffing, recent scientific agentic systems elevate retrieval\ninto a deliberate reasoning step: agents decide when and what to fetch, and how to use the evidence before\ncommitting to a hypothesis. With retrieval ability, BioDiscoveryAgent [527] pulls literature and interim\nassay results inside a closed loop so the model’s next gene-perturbation choices are conditioned on what\nwas read and measured; while DrugAgent [528] coordinates knowledge graph queries, targeted literature\nsearch through web API and machine learning predictors. Its planner selects retrieval actions and then\nreconciles heterogeneous evidence into an explainable rationale. To facilitate scientific research, ARIA [529]\noperationalizes a search, filter then synthesis workflow as role-bound steps that carry citations forward,\nturning literature into actionable procedures. Similarly, AI Scientist-v2 [530] employs an agentic tree-search\n49\n"}, {"page": 50, "text": "Agentic Reasoning for Large Language Models\nframework in which the agent actively queries scientific literature database during hypothesis formulation\nand manuscript drafting, ensuring that analyses and writing are grounded in existing evidence. For research\nidea generation, another recent work [531] constrains the process with curated background packets, using\nretrieval as an experimental control.\nBuilding on these developments, retrieval-augmented generation (RAG) frameworks position external\nsources not merely as supporting references but as active components of the reasoning process. Specifically,\nRAG-enhanced scientific agents make external sources as primary inputs to LLM context and reasoning\nmaterial, mostly with explicit planning, passage extraction, citation and contradiction checks. For example,\nPaperQA [516] and PaperQA2 [517] treat retrieval as the main loop. By deciding which documents to\nread, attributing every claim, and detecting conflicts to steer synthesis, these works can yield expert-level\nliterature reviews that are inherently verifiable. In material science, LLaMP [518] extends RAG beyond text.\nSpecifically, it utilizes hierarchical ReAct [5] agents call material-specific APIs to fetch band gaps or elastic\ntensors, edit structures and then reason with computed properties.\n6.2.2. Self-evolving agentic reasoning\nScientific discovery agents can go beyond static reasoning and acquire the ability to self-evolve, which is to\nlearn from experience, refine their internal representations and improve decision quality over successive\ninteractions. This self-evolving layer equips agents with mechanisms to monitor and revise their own\nreasoning, retain and reuse intermediate hypotheses and adjust future plans based on external feedback or\nenvironmental signals. In the following paragraphs, we discuss how memory modules enable the accumulation\nof scientific knowledge and how feedback and reflection mechanisms support continual adaptation and\nreasoning consistency throughout long-horizon scientific workflows.\nMemory.\nChemAgent [532] implements a self-updating library. It decomposes chemistry problems into\nsub-tasks and writes reusable skills (ex: procedures, patterns, solutions) that later prompts can retrieve and\nadapt, stabilizing long multi-step reasoning without re-deriving everything from scratch. On the other hand,\nMatAgent [533] emphasizes interpretable generation for inorganic materials, where short-term memory\nrecalls recent compositions and feedback, long-term memory preserves successful designs together with their\nreasoning traces, and both are reused across iterations to guide proposal refinement and enable transparent\naudit.\nAgentic Feedback and Reflection.\nFirstly, Scientific Generative Agent [525] ties discrete LLM proposals\nto inner-loop simulations that optimize continuous parameters, advancing only when evidence improves.\nThe reflection ability is driven by measurable loss reductions. Next, ChemReasoner [534] performs heuristic\nsearch over the LLM’s idea space but scores and steers candidates with quantum-chemical feedback, turning\nelectronic-structure signals into a principled critique of linguistic hypotheses. Complementing these physics-\nbased signals, Curie [509] embeds rigor check directly into control flow via intra-agent checks, inter-agent\ngates and an experiment-knowledge module. In parallel, LLMatDesign [535] builds explicit self-reflection\ninto materials workflows, prompting the agent to surface and repair inconsistencies before they propagate\nto tool calls. Moreover, NovelSeek [536] utilizes reflection as a closed loop, updating code and plans with\nhuman-interactive feedback after each round. Finally, a recent study [537] regularizes the process up front\nwith explicit goals & constraints and afterwards with standardized scoring to provides an objective standard\nthat makes reflection repeatable.\n50\n"}, {"page": 51, "text": "Agentic Reasoning for Large Language Models\n6.2.3. Collective multi-agent reasoning\nMulti-agent frameworks for scientific discovery distribute labor across specialized LLM-driven roles, where\nadvanced LLM reasoning not only orchestrates coordination between scientific agents but also adjudicates\nconflicting evidence to maintain coherence in the process.\nTo illustrate, we introduce some important multi-agent frameworks as follows. Firstly, ProtAgents [507]\nexemplifies this pattern in protein design. The framework involve agents for literature retrieval, structure\nanalysis, physics simulation, and results analysis. Specifically, the backbone LLM directs reasoning over\nmulti-modal outputs, choosing when to iterate or convergence-check based on feedback signals. PiFlow [538],\non the other hand, instantiates reasoning as principle-aware uncertainty reduction with a multi-agent loop in\nwhich a Planner agent relays strategy to a Hypothesis agent and a validation loop, explicitly tying multi-agent\ncommunication to hypothesis–evidence alignment. AtomAgents [523] also brings similar role specialization\nto alloy discovery. In particular, the agent uses LLM-guided reasoning to control over when to trigger\nsimulations and how to evaluate multi-modal results, letting reasoning allocate computational resources and\nprune alloy candidates.\nWith a similar planner, executor and evaluator framework, CellAgent [526] instantiates researches on single-\ncell analysis, where the planner LLM reasoning selects tools or hyper-parameters and the evaluator LLM\ntriggers self-iterative re-runs when quality checks fail. Some other notable works include ARIA [529] that\nintroduces a four-agent framework (scout, filter, synthesizer and procedure-drafter), Curie [509] that embeds\nrigor into multi-agent planning, Team of AI-made Scientists (TAIS) [539] for gene-expression discovery and\nthe Virtual Lab [540] for nano-body design with role agents.\n6.3. Embodied Agents\nEmbodied agents extend reasoning beyond text, anchoring language in robotic perception, manipulation\nand navigation. By embedding LLMs within robotic and simulated bodies, these embodied agents tackle\nreal-world generalization, continual adaptation and multi-modal grounding.\nIn this subsection, we begin with the foundational layer (Section 6.3.1), which covers long-horizon embodied\nplanning, tool-assisted perception, manipulation and execution. Building upon these capabilities, the self-\nevolving layer (Section 6.3.2) introduces agentic memory, feedback and self-reflection capabilities enabling\nrobots to refine control policies, adapt to novel environments and improve performance through continual\ninteraction. Finally, the collective reasoning layer explores multi-robot collaboration (Section 6.3.3), where\nagents coordinate perception, share learned representations and jointly reason about tasks to achieve complex\nembodied goals.\n6.3.1. Foundational agentic reasoning\nPlanning.\nEarly work such as SayCan [136] established the template by mapping linguistic descriptions to\nskill affordance estimates and SayPlan [541] refined this grounding by leveraging 3D scene graphs to align\ngoal references with object-centric representations and spatial models. Beyond symbolic representations,\nEmbodiedGPT [542] use curated video CoT annotations of sub-goals to train models taht map multi-model\ninput to structured sequences for embodied planning, while context-aware planning system [543] adds\nsemantic spatial map and object location information to the planning pipeline, enabling dynamical planning\nduring execution. In addition, DEPS [544] introduces an interactive planning loop (i.e. describe, explain,\nplan and select) for open-world multi-task agents.\n51\n"}, {"page": 52, "text": "Agentic Reasoning for Large Language Models\nEmbodied agents also rely on multi-modal reasoning traces that explicitly align perception with action. For\nexample, Embodied CoT [545] trains vision-language-action models to generate reasoning steps incorporating\nvisual features before executing an action. Fast ECoT [546] accelerates this by caching and re-using reasoning\nsegments across time-steps, reducing inference latency while preserving task success. More recently, Cosmos-\nReason1 [547] establishes an ontology of space, time and dynamics that lets CoT sequences encode structured\nphysical priors. CoT-VLA [548] builds a visual chain-of-thought by predicting future image frames as\nintermediate sub-goals prior to action generation. Finally, Emma-X [549] integrates grounded chain-of-\nthought with look-ahead spatial reasoning, improving long-horizon embodied task performance.\nAnother line of works strengthen embodied planning through reinforcement learning, considering planning\nnot only as static decomposition but as a self-evolving process that adapts to environment feedback. Robot-\nR1 [550] trains large VLMs to predict keypoint transitions under visual context, turning RL into a mechanism\nfor learning physically grounded forward models. ManipLVM-R1 [551] exploits verifiable physical reward\nsignals (e.g., trajectory match and affordance correctness) to reduce reliance on dense expert annotation.\nEmbodied-R [38] presents a collaborative framework where VLMs handle perception and smaller LMs handle\nreasoning, and the whole is trained via RL for embodied spatial reasoning. VIKI-R [552] further extends\nthis direction into heterogeneous multi-agent cooperation with a two-stage design, employing a two-stage\npipeline of chain-of-thought fine-tuning followed by hierarchical RL across agents coordinating activation\nand planning.\nTool-use.\nEmbodied agents can also be strengthened to interact with external tools to enhance perception\nand compensate for incomplete observations. GSCE [553], for example, provides a prompt-framework that\nbinds skill APIs and constraints for safe LLM-driven drone control. MineDojo [554] links agents to internet-\nscale corpora and thus enabling richer affordance grounding. Physical AI Agents [34] further introduces a\nmodular architecture and a retrieval augmented generation design pattern for embedding real-world physical\ninteraction into LLM-driven agents. Beyond offline tool use, some systems treat the environment itself as an\nAPI. For example, Matcha agent [450] uses an LLM to issue queries about objects and scenes and thereby\nacquire perceptual information needed for task completion.\nOn the other hand, execution module is one of the most important tool type. It translates high-level\nlanguage instructions into continuous motor commands, enabling embodied agents to act reliably in physical\nenvironments. Early systems such as SayCan [136] uses language to invoke robot pick-and-place skills;\nwhile LEO [555] broaden execution to more general manipulation settings and Hi Robot [556] uses a VLM\nreasoner to process complex prompts and a low-level action policy executes the chosen step. More recent\nefforts broaden the execution space: Gemini Robotics [557] introduces a large-scale vision-language-action\nmodel for real-world robot control and Octopus [558] generates executable code in simulated environments\nthat bridges planning and manipulation.\nBeyond single-agent control, hybrid pipelines couple reactive reflexes with language-guided policies to\nsupport complex domains. For example, CaPo [559] incorporates an execution phase where agents carry\nout decomposed sub-tasks and adapt their meta-plan based on progress; COHERENT [560] embeds a robot\nexecutor module within its PEFA (i.e. proposal, execution, feedback and adjustment) loop, which ensures\neach assigned sub-task is acted and refined appropriately; and MP5 [561] integrates multi-modal perception\nto generate executable plans in open-ended Minecraft. At the perception–action interface, LLM-Planner [83]\ngenerates sub-goals and maps them into action sequences via a low-level controller and EmbodiedGPT [542]\nillustrate how LLM-generated plans can be translated into control policy for embodied control in physical\nenvironments.\n52\n"}, {"page": 53, "text": "Agentic Reasoning for Large Language Models\nSearch and retrieval.\nEmbodied agents can also use search and retrieval ability to ground language in\nspatial structure and past experience. Early navigation systems such as L3MVN [562] use LLMs to query\na semantic map and select promising frontiers as long-term goals during visual target navigation, while\nSayNav [563] and SayPlan [541] build 3D scene graphs and then search task-relevant subgraphs so language\ninstructions can be translated into grounded waypoints and sub-tasks in large environments. Long-horizon\nnavigation works like ReMEmbR [564] maintain a structured spatio-temporal memory that can be queried\nto answer “where” and “when” questions about past robot experience. Additionally, RAG-style systems make\nretrieval a first-class part of the planning loop: Embodied-RAG [565] and EmbodiedRAG [37] treat an agent’s\nexperience and 3D scene graphs as non-parametric memories from which task-relevant episodes or subgraphs\nare retrieved for navigation and task planning; Retrieval-Augmented Embodied Agents [566] retrieve policies\nfrom a shared memory bank and condition action generation on them; and MLLM-as-Retriever [567] trains\na multi-modal LLM retriever to rank past trajectories so each decision step can condition on the most useful\nprior experience rather than only the current observation.\n6.3.2. Self-evolving agentic reasoning\nEmbodied agents reliably achieve long-horizon autonomy when they can self-evolve over time: monitor their\nown internal states, store and update task-relevant knowledge and adjust behaviors when plans deviate. In\nthe following paragraphs, we examine how memory modules, feedback signals and agentic reflection enable\nembodied agents to turn planning from a one-shot process into a continually improving cycle of behavior.\nMemory.\nEffective memory mechanisms enable agents to reuse past experiences and maintain coherent\ntask execution over extended interactions. Many systems cache recent observations in episodic buffers while\nsummarizing long-term semantics in structured graphs, as in household planning [568] and long-horizon\nagents with hybrid multi-modal memory [307]. Skills and routines can be shared across tasks via indexed\nmemory stores. For example, HELPER-X [569] indexes discovered skills and action scripts, which aid future\ndialogue and can be shared across domains. Spatial navigation methods such as BrainNav [570] maintain\nbiologically inspired dual-map memories linked by a hippocampal hub to reduce hallucinations and drift.\nBroader contexts also benefit: CAPEAM [543] incorporates environment-aware memory modules that track\nobject states and spatial changes. Finally, lifelong episodic systems such as Ella [571] maintains long-term\nmulti-modal memory system to support social-robot interaction.\nAgentic Feedback and Reflection.\nDialogue-based critique, calibrated uncertainty and environment-aware\nreward shaping refine policies beyond binary success signals. For example, Matcha agent [450] treats\nobjects and scenes as interactive information sources before acting and FAMER [572] uses lightweight\npreference feedback to adapt embodied agents to user intentions in real time. Uncertainty-aware planners\nsuch as KnowNo [573], which proactively solicit guidance when confidence falls below guarantees, and\nOctopus [558], which exploits environmental feedback to improve generated executable programs over\ntime. At the multi-agent level, MindForge [390] introduces theory-of-mind style perspective feedback so\nheterogeneous robots adapt to each other’s reasoning strategies; while ReAd [574] introduces a advantage-\nbased feedback loop that enables an LLM planner to self-refine its collaboration strategies across embodied\nmulti-agent tasks.\nRobust reflection mechanisms help agents anticipate failures by monitoring their own reasoning and actions\nand then adjusting plans. Optimus-1 [307] couples a Knowledge-guided Planner with an Experience-Driven\nReflector to revise decisions using stored experience, while another recent study [575] defines structured\n53\n"}, {"page": 54, "text": "Agentic Reasoning for Large Language Models\nagentic workflows (including self-Reflection, multi-Agent reflection and LLM Ensemble) that enable robots to\nreflect on and refine LLM-generated object-centered plans, thus reducing reasoning errors. Systems such as\nEMAC+ [576] interleave perception, planning and verification steps to perform online plan refinement and\nearlier works such as Voyager [36] also embeds an iterative prompting loop that uses environment feedback\nand execution errors to refine its skill library over time.\n6.3.3. Collective multi-agent reasoning\nMulti-agent collaboration enables embodied systems to divide labor and coordinate complex tasks more\nefficiently, with language often serving as the primary medium for negotiation and role allocation. For\ninstance, SMART-LLM [577] decomposes high-level instructions and allocates sub-tasks across multiple\nrobots, while CaPo [559] optimizes cooperative plans to avoid redundant exploration. For heterogeneity\nand coordination mechanisms, COHERENT [560] deploys a propose-execution-feedback-adjust loop across\ndiverse robot types to enable seamless joint operation. In addition, Theory of Mind (ToM), which refers to an\nembodied agent’s ability to infer and reason about others’ beliefs and mental states, is also highly related to\nembodied multi-agent systems [388, 391, 389]. For example, MindForge [390] equips agents with explicit\ntheory-of-mind representations and natural inter-agent communication to coordinate collaboratively.\nFor multi-modal frameworks, EMAC+ [576] integrate vision and language modules and continuously refine\nplans via visual feedback, COMBO [578] integrates vision and language modules and continuously refine\nplans via visual feedback, and VIKI-R [552] demonstrates reinforcement learning as a scalable coordination\nmechanism among embodied agents. At larger scales, studies such as RoCo [579] show how role negotiation\nand flexible protocols support adaptable teamwork in dynamic environments.\n6.4. Healthcare & Medicine Agents\nHealthcare and medical agents seek to support the full clinical decision pipeline, from initial symptom triage\nto treatment planning and integrating LLMs with structured patient records, medical ontologies and expert\nguidelines. Unlike general assistants, these systems must operate under strict safety constraints, multi-modal\nevidence and legal justification.\nIn this subsection, we begin with the foundational layer (Section 6.4.1), which includes medical and diagnostic\nreasoning and tool-augmented access to various biomedical knowledge bases and APIs. Building on these\nprimitives, the self-evolving layer (Section 6.4.2) examines memory, feedback and reflective modules that\nallow these agents to accumulate patient-specific context, adapt to longitudinal trajectories and revise clinical\nplans over time. Finally, the collective layer (Section 6.4.3) highlights multi-agent collaboration, which\nincludes doctor–agent co-planning, human–AI shared autonomy and specialist model ensembles.\n6.4.1. Foundational agentic reasoning\nPlanning.\nPlanning is a core capability for healthcare agents, which enables them to structure long-horizon\nclinical pathways into diagnostic and treatment phases, refine workflows dynamically as patient conditions\nevolve and coordinate across teams and tools toward cohesive care delivery. We discuss several various recent\nadvancements as follows. For instance, a recent agentic clinical system [580] orchestrates specialized tools\nand guideline citations to support oncology decision-making, EHRAgent [581] decomposes multi-table EHR\ninference into code-execution steps with feedback learning and PathFinder [365] presents a multi-agent,\nmulti-modal histopathology workflow for diagnostic reasoning.\nOther frameworks model planning as an explicit orchestration layer across levels of abstraction. For example,\n54\n"}, {"page": 55, "text": "Agentic Reasoning for Large Language Models\nMedAgent-Pro [368] proposes a hierarchical workflow which first generates disease-level diagnostic plans\nfrom guideline criteria and then dispatches tool-agent modules for execution. MedOrch [582] treats tool\ninvocation itself as a planning primitive across modalities, orchestrating reasoning agents for multi-step\ndiagnostic execution. On the other hand, ClinicalAgent [583] coordinates multi-agent workflows for clinical\nplanning, leveraging LLM reasoning to allocate tools and synthesize evidence. In addition, planning in\nhealthcare agents is increasingly adaptive, responding to new information and evolving contexts. For example,\nDoctorAgent-RL [584] models clinical consultation as a dynamic decision-making process under uncertainty,\noptimizing questioning strategies and diagnostic paths via reinforcement learning; while DynamiCare [585]\nadjusts specialist-agent teams across multi-round interactions as new patient information emerges.\nTool-use.\nTool integration significantly expands a healthcare agent’s action space, enabling precise calcula-\ntions, medical image interpretation and access to specialized databases. Recent studies are summarized as\nfollows. Several systems explicitly foreground extensibility. MedOrch [582] introduces a modular architecture\nthat allows new diagnostic APIs to be incorporated without retraining, while TxAgent [514] integrates over\ntwo hundreds pharmacological tools to support therapeutic decision-making across drug–disease–treatment\nrelationships. AgentMD [515] similarly curates and leverages over two thousands executable clinical calcula-\ntors to learn risk-prediction pipelines.\nOther approaches focus on structured function calling for safe execution. For example, LLM-based agents\ncan reliably invoke bedside calculators when provided with explicit function signatures, ensuring arithmetic\ncorrectness in dosing and risk scoring [586]. MeNTi [587] goes further by enabling nested tool calls across\nmulti-step medical calculators. Complementing these text-based integrations, MMedAgent [39] demonstrates\nthat agents can learn to select among multi-modal tools.\nIn addition, execution is crucial for translating high-level clinical plans into concrete actions such as code\noperations, database queries or robotic procedures. VoxelPrompt [588] embed 3-D volumetric priors so\nthat language instructions drive spatial segmentation and analysis of medical image volumes. On the other\nhand, embodied ultrasound-robot controllers [589] translate LLM-generated plans into closed-loop robotic\nscanning via a “think-observe-execute” loop. Adaptive reasoning-and-acting systems [590] further refine\nboth the reasoning and actions over time in simulated clinical environments. In medical imaging, systems\nlike MedRAX [591] materializes multi-step reasoning by integrating specialized chest-X-ray tools and LLM\nreasoning into an end-to-end diagnostic agent. PathFinder [365] similarly executes multi-agent, multi-modal\ndiagnostic workflows in histopathology.\nAnother class of healthcare agents deploys code-level workflows. For example, Conversational Health\nAgents [592] compile dialogue actions into function calls and code execution for downstream processing,\nwhile EHRAgent [581] materializes EHR operations via executable code. MedAgentGym [593] trains agents\nto produce code that is directly executed and graded, enforcing reliability of reasoning traces. DoctorAgent-\nRL [584] validates multi-turn dialogue acts by executing reinforcement-learned strategies in simulated\nconsultations, while AIPatient [594] materializes realistic patient scenarios for execution-based evaluation\nand another recent study [595] demonstrates how self-evolving multi-agent simulations allow execution\nbehaviors themselves to improve over time.\nSearch and retrieval.\nSearch-based agents enhance clinical decision-making by linking LLM reasoning with\nexternal biomedical knowledge sources. For instance, MeNTi [587] supplements therapeutic reasoning by\nbridging LLM calls into multi-step medical calculators while EHRAgent [581] dynamically executes code op-\nerations over multi-table EHR data to support complex tabular inference. Conversational Health Agents [592]\n55\n"}, {"page": 56, "text": "Agentic Reasoning for Large Language Models\nenrich personalized dialogue by integrating developer-defined external sources and orchestrating action\nflows. Another line of work explicitly embeds retrieval-augmented generation (RAG) into healthcare agents.\nFor example, CLADD [596] retrieves molecular graphs and prior assay results before proposing compound\nhypotheses and MedReason [597] issues targeted knowledge-graph sub-queries to anchor each reasoning\nstep for clinical QA.\n6.4.2. Self-evolving agentic reasoning\nSelf-evolving capabilities enable healthcare agents to maintain longitudinal clinical coherence. Representative\nuse cases include accumulating relevant medical context across encounters, updating beliefs as new evidence\narrives and revising decisions when inconsistencies surface. In the following paragraphs, we examine how\nmemory, feedback and reflective mechanisms collectively turn clinical reasoning from a one-shot prediction\ninto a continually improving care process.\nMemory.\nPersistent memory is essential for tracking medical or patient history and maintaining context\nacross interactions. For instance, epidemic-modeling agents [598] maintain temporal contact histories to\ntrace infection chains over time; while MedAgentSim [595] stores experience histories and refine diagnostic\nstrategies over time. In structured data settings, EHRAgent [581] records intermediate computations\nover tabular EHRs so subsequent steps can reference prior results. EvoPatient [599] interleaves memory\nwith coevolution maintains evolving clinical state across dialogue phases while AIPatient [594] persists\nlongitudinal EHR-derived variables to drive consistent responses. Multi-agent systems such as MedOrch [582]\ncontain clinical knowledge graph agent which can be considered as external memory that can be queried to\nretrieve known relationships or diagnostic patterns.\nAgentic Feedback and Reflection.\nAgentic feedback and self-reflection are complementary mechanisms\nthat improve reliability and adaptability of healthcare agents. Feedback converts execution outcomes into\nlearning signals: DynamiCare [585] updates multi-agent treatment strategies when newly observed patient\nstate contradicts prior plans; DoctorAgent-RL [584] optimizes questioning policies from consultation rewards;\nand MedAgentGym [593] enforces correctness by executing and grading generated code. Tool-use pipelines\nalso propagate execution feedback. For example, the success/failure of table queries in EHRAgent [581] or\ncalculator calls in MeNTi [587] and clinical-calculation agents [586] to refine subsequent actions.\n6.4.3. Collective multi-agent reasoning\nMulti-agent collaboration is central to healthcare AI, since clinical decision-making often depends on consensus\namong specialists, negotiation of competing hypotheses and coordination across roles such as physicians,\npatients and trial designers. In the following, we discuss several strands of research centered around\nmulti-agent capabilities.\nFor collaborative decision-making, notable frameworks include MDAgents [364], which automatically assigns\ntailored collaboration structures to teams of LLMs depending on medical task complexity, and DoctorAgent-\nRL [584], which uses a multi-agent reinforcement-learning framework to optimize multi-turn doctor-patient\nconsultation dialogues. In addition, Agent-derived Multi-Specialist Consultation (AMSC) [600] explores\nstaged multi-specialist dialogues for differential diagnosis that mimics the medical scene of a patient consulting\nwith multiple specialists. Other notable works include ClinicalAgent [583], which organizes clinical trial\nworkflows via role-based agent collaboration / LLM reasoning and PathFinder [365], which integrates a\n56\n"}, {"page": 57, "text": "Agentic Reasoning for Large Language Models\ndiverse set of agents that can gather evidence and provide comprehensive diagnoses with natural language\nexplanations.\nOn the other hand, there are studies focusing on simulation-driven collaboration. These works highlight how\nmulti-agent setups enrich training and evaluation. MedAgentSim [595] co-evolves doctor and patient agents\nto simulate real-world multi-turn clinical interactions, and EvoPatient [599] uses co-evolution of patient and\ndoctor agents to generate diagnostic dialogue data and therefore gathers experience to improve the quality\nof both questions and answers to enable accurate human doctor training. In addition, DynamiCare [585]\ninitiates a team of specialist agents that iteratively queries the patient system to integrate new information and\nadapts the composition and strategy. Finally, medical agents can also collaborative to aid medical reasoning\nprocess. For example, MedAgents [601] demonstrates zero-shot cooperation among domain-specialist agents\nin medical reasoning tasks, CLADD [596] uses retrieval-augmented generation to support drug-discovery\nworkflows across agents and GMAI-VL-R1 [602] combines multi-modal reasoning and reinforcement learning\nin a multi-agent framework to support large-scale medical decision-making.\n6.5. Autonomous Web Exploration & Research Agents\nWeb agents, GUI agents and autonomous research agents constitute three interlinked but distinct trajectories\nof agentic reasoning systems. Firstly, web agents specialize in navigating online resources, issuing web API\ncalls or browser actions to retrieve dynamic evidence and steer research direction. GUI agents go further by\nmanipulating software interfaces and multi-modal dashboards directly (i.e. clicking, typing, navigating)\nto execute experiments, data workflows and interface-based tasks. Autonomous research agents sit at the\ntop of this hierarchy, pairing LLM reasoners with scientific workflows, tool-chains and meta-loops to drive\nhypothesis generation, data synthesis and paper writing. The core connection is a progression of autonomy:\nfirst web agents retrieve evidence from online resources, then GUI agents operationalize actions inside\nsoftware interfaces, and finally autonomous research agents orchestrate full scientific workflows end-to-end.\nIn this subsection we begin with the foundational layer (Section 6.5.1), which captures the core capabilities\nthat any autonomous agent must support: perceiving its environment, reasoning about goals, planning\nactions and grounding those into tool-augmented workflows. Building on these primitives, the self-evolving\nlayer (Section 6.5.2) examines how agents incorporate feedback, memory and reflection to iteratively refine\ntheir behaviors and improve methods over time. Finally, the collective layer (Section 6.5.3) highlights how\nagents move beyond individual competence into coordination, specialization and emergent collaboration.\nWhile web agents, GUI agents and autonomous agents share common themes of goal-directed autonomy,\ntool-use and iterative improvement, they differ in where they act on, how they manipulate their environment\nand what goal they aim to achieve.\n6.5.1. Foundational agentic reasoning\nPlanning.\nPlanning is essential for web agents because they must decompose long-horizon tasks into\nmanageable steps, adapt to dynamic pages and coordinate tool/invocation strategies. Early work such\nas WebGPT [258] fine-tuned GPT-3 [603] to answer open-ended questions via a text-based web-browser\ninterface. Then, various web-based methods deepened the planning paradigm: for example, SEEACT [604]\nexplored large multi-modal models as generalists that integrating visual and HTML grounding for web-based\ntasks, and AutoWebGLM [605] introduced HTML simplification and various learning techniques for open-\ndomain web task decomposition and navigation. These works paved the way for recent systems such as\nAgent Q [113] that integrate guided MCTS, self-critique and off-policy preference optimization on web-task\nbenchmarks, and set the stage for even more advanced long-horizon web planners such as WebExplorer [606]\n57\n"}, {"page": 58, "text": "Agentic Reasoning for Large Language Models\nand WebSailor [41].\nIn addition, reinforcement learning has become a core tool for improving the decision-making and planning\nbehavior of web-based LLM agents. WebRL [437] introduces a self-evolving online curriculum that generates\nnew tasks from unsuccessful attempts and trains an outcome-supervised reward model to guide policy\noptimization. WebAgent-R1 [28] performs end-to-end multi-turn RL, learning web interaction policies\ndirectly from online rollouts with binary success rewards. DeepResearcher [260] scales RL to real-world\nweb environments, using a multi-agent browsing architecture and exhibiting emergent behaviors such as\nplan formulation, cross-source corroboration, and self-reflection. Hybrid pipelines like AutoWebGLM [605]\ncombine supervised training with RL fine-tuning to strengthen task decomposition and structured navigation,\nwhile Navigating WebAI [607] combines supervised learning and RL techniques to improve web navigation\nperformance. Methods such as Pangu DeepDiver [608], EvolveSearch [609] and WebEvolver [610] use\nRL-based self-improvement, for example by adaptively scaling search depth or jointly training an agent and\na world-model-like simulator to improve long-horizon web decision-making. Hierarchical approaches like\nArCHer [611] optimize high-level and low-level policies with a multi-turn hierarchical RL framework, while\nPAE [612] combines a task proposer, an acting agent and an evaluator to support autonomous skill discovery\nvia RL in internet environments.\nPlanning is a core capability for GUI agents, enabling them to coordinate long, multi-step interactions across\napplications and operating environments. OS-Copilot [613] approaches this by treating the desktop as a\nunified control space in which a generalist agent continually refines its multi-step workflows. Agent S [85]\nbuilds an experience-augmented planning stack that decomposes tasks into sub-goals while retrieving past\ntrajectories and external knowledge to guide action sequencing. InfiGUIAgent [614] strengthens planning\nby integrating hierarchical task structuring into a multi-modal backbone, allowing agents to organize\nGUI procedures at multiple levels of abstraction. MobA [615] and PC Agent [616] employ hierarchical\narchitectures that separate high-level planning from low-level execution—on mobile and desktop respectively.\nGUI foundation models such as OS-ATLAS [617], OSCAR [618] and UItron [619] further emphasize robust\ncross-application planning: OS-ATLAS offers a platform-agnostic action model for consistent control, OSCAR\nmaintains state-aware plans that adapt as execution unfolds and UItron unifies offline and online planning\nwithin a single general-purpose GUI agent.\nLikewise, reinforcement learning has become a central way to endow GUI agents with planning over long\naction sequences. End-to-end frameworks such as ARPO [620] and ComputerRL [621] directly optimize\nmulti-step GUI trajectories with replay buffers or large-scale online interaction, replacing hand-crafted scripts\nwith learned policies for general desktop control. R1-style and semi-online methods, including UI-R1 [622],\nGUI-R1 [623], InfiGUI-R1 [624] and UI-S1 [625], start from strong vision–language backbones and then use\nRL to sharpen action prediction and long-horizon reasoning. A complementary line focuses on where to act\nby improving visual grounding: GUI-Bee [626], SE-GUI [627], UIShift/GUI-Shift [628] and UI-AGILE [629]\ndevelop RL-based grounding frameworks to help agents reliably localize target elements before executing\nactions. ZeroGUI [630] pushes toward fully automated online RL loops, where the agent generates its own\ntasks and trajectories and improves with zero human annotation, while ComputerRL [621] scales end-to-end\nonline RL in large distributed desktop environments. AgentCPM-GUI [631] couples supervised pre-training\nwith reinforcement fine-tuning to strengthen decision quality on mobile apps. Finally, foundation-style GUI\nagents such as AutoGLM [632] and Mobile-Agent-v3 [633] serve as general backbones that unify perception,\ngrounding and action, and are trained or fine-tuned with scalable RL frameworks to align long-horizon GUI\nplanning with real-world success signals.\nFor autonomous research agents, planning modules translate abstract goals into actionable research itineraries.\nFor example, Agent Laboratory [634] organizes work into three structured stages, namely literature review,\n58\n"}, {"page": 59, "text": "Agentic Reasoning for Large Language Models\nexperimentation and report writing, and supports the workflow with tool-hooks that automate code execution,\nexperiment runs and documentation. GPT Researcher [635] uses a plan →research →write cycle, where a\ndedicated planner drafts the outline, retrieval/analysis agents gather evidence and a writer compiles the\nfinal report. Chain of Ideas [636] retrieves literature into a chain structure to reflect domain progression\nand support ideation via experiment design, whereas IRIS [637] performs hypothesis exploration via Monte\nCarlo Tree Search to expand promising branches before committing to downstream tasks. Broader variants\ninclude ARIA [529] and NovelSeek [536] that automate the research workflow with a complete literature\nsearch, hypothesis generation and experiment planning cycle.\nTool-Use.\nFor web agents, tool-use abilities underpins execute plans in realistic, dynamic environments.\nFor example, WebVoyager [638] systematizes multi-modal execution by building an end-to-end agent\nthat operates on real websites. On the interaction side, BrowserAgent [639] makes the action space\nmore human-like, defining a compact set of browser primitives (e.g., click, scroll, type) and coupling\nthem with an explicit memory mechanism to maintain key conclusions across steps, yielding strong gains\non multi-hop QA benchmarks. Finally, methods like WALT [640] and pipeline-oriented systems such as\nWebDancer [641] and WebShaper [642] push tool use from mere execution toward tool discovery and\ndata-centric interaction. Specifically, WALT teaches agents to reverse-engineer reusable tools from website\nfunctionality, while WebDancer and WebShaper embed web actions inside multi-turn information-seeking\nand dataset-synthesis loops, respectively.\nTool use is another core capability for GUI agents, enabling them to invoke system functions and application\nfeatures as structured tools. As pioneering systems, AutoDroid [643] automatically analyzes Android apps\nto construct functionality-aware UI abstractions that LLM agents can reason over as capabilities rather\nthan raw layouts, while its successor AutoDroid-V2 [644] re-frames mobile UI automation as LLM-driven\ncode generation, with an on-device small language model emitting executable scripts for a local interpreter.\nMobileExperts [645] models each expert as a tool-capable specialist and uses a dual-layer controller to select\nwhich expert and its associated tool-set to invoke at different stages of a mobile workflow. AgentStore [646]\npushes this idea to the platform level by treating heterogeneous agents themselves as tools: a MetaAgent\nuses AgentTokens to route operating-system subtasks to the most suitable specialized “tool-agent” through a\nunified interface. OS-Copilot [613] and OSCAR [618] integrate rich system-level tools into unified computer-\ncontrol frameworks, so that complex desktop tasks are expressed as sequences of tool calls. OS-ATLAS [617]\ncomplements these systems with a foundation action model that offers robust cross-platform GUI grounding,\nserving as a reliable actuator layer for downstream tool-using agents. Finally, SeeClick [220] strengthens the\nexecution stack by pre-training a visual GUI agent for GUI grounding, improving the ability to locate the\ncorrect on-screen elements from instructions.\nSpecialized tools can expand an autonomous research agent’s capabilities beyond pure text, allowing more\nfine-grained ability. For instance, Agentic Reasoning [223] automatically routes queries to appropriate tool\nmodules like code execution, web search and structured memory agents when the main LLM detects a\ngap in reasoning; while Webthinker [647] empowers autonomous web exploration and page navigation\nduring long-horizon investigations, by interleaving reasoning, search and draft-writing with a web explorer\nmodule. PaperQA [516] and its follow-up synthesis agent [517] integrate PDF parsing and citation-level\ngrounding to produce verifiable answers and literature syntheses, while Scideator [648] provides an IDE-\nstyle tool-chain that combines paper facets with novelty checks for real-time brainstorming. In addition,\nDeepResearcher [260] shows that reinforcement learning over real-web interactions improves deep-research\nefficiency and quality, with emergent behaviors such as plan refinement and cross-source corroboration.\n59\n"}, {"page": 60, "text": "Agentic Reasoning for Large Language Models\nExecution components ground high-level reasoning in code, simulations or laboratory protocols to produce\nverifiable scientific outcomes. Agent Laboratory [634] executes experiments specified in declarative configu-\nration files by orchestrating external toolchains, while Agentic Reasoning [223] integrates a coding agent that\nexecutes Python alongside web search and structured memory, feeding the results back into the reasoning\nprocess. MLR-Copilot [649] turns research plans into runnable implementations via an ExperimentAgent that\nleverages retrieved prototype code, runs experiments, and iteratively debugs implementations. Dolphin [650]\ncloses the loop by generating ideas, implementing them through code templates with traceback-guided\ndebugging, executing experiments, and using the analyzed results to steer the next research cycle. The\nAI Scientist [651] automates end-to-end ML experiments, i.e. generating ideas, writing code, executing\nexperiments and visualizing results, so that observed outcomes can guide subsequent runs, while The AI\nScientist-v2 [530] adds a dedicated experiment manager and progressive agentic tree search to prioritize and\nschedule experiment branches. Most recently, NovelSeek [536] introduces a unified closed-loop multi-agent\nframework that spans hypothesis generation, idea-to-methodology construction, and multi-round automated\nexperiment execution with feedback across diverse scientific domains.\nSearch and Retrieval.\nSearch and retrieval lie at the heart of what differentiates web agents from static\nlanguage models: they must locate, synthesize and refactor web-scale information in dynamic environments.\nWebExplorer [606] tackles this by generating challenging information-seeking trajectories and training\nagents to interleave a search tool and a browse tool over many turns, resulting in improved multi-step\nretrieval policies on complex benchmarks. WebSailor [41] likewise focuses on information-seeking under\nextreme uncertainty, constructing high-uncertainty search tasks and using a two-stage post-training pipeline\nto instill uncertainty-reducing search strategies for long-horizon web tasks. INFOGENT [652] also performs\nmulti-query search across diverse web sources, enabling comprehensive information retrieval beyond task\ncompletion. For retrieval-augmented generation applications, RaDA [653] explicitly disentangles web-agent\nplanning into Retrieval-augmented Task Decomposition and Retrieval-augmented Action Generation, so\nthat each high-level subgoal and concrete action is conditioned on fresh search results while respecting\ncontext limits. In addition, GeAR [264] advances retrieval itself by augmenting a base retriever with graph\nexpansion and an agent framework, enabling multi-hop passage retrieval along graph-structured evidence\nchains. Finally, WebRAGent [654] exemplifies retrieval-augmented generation for web agents by retrieving\npast trajectories and external knowledge into a multi-modal RAG policy.\nSeveral GUI agents use retrieval capability to inject external experience or knowledge at inference time.\nSynapse [655] maintains an exemplar memory of abstracted trajectories and, for each new task, retrieves\nsimilar past trajectories as in-context plans, substantially improving multi-step decision-making. Learn-\nAct [656] builds a three-agent pipeline that mines human demonstrations into a knowledge store and retrieves\nthe most relevant instructions to guide mobile GUI execution on unseen and diverse tasks. MobileGPT’s\nExplore–Select–Derive–Recall framework [657] equips a phone agent with human-like app memory, storing\nmodular procedures that can be recalled and recomposed when similar tasks reappear. TongUI [658] turns\nlarge-scale multi-modal web tutorials into the GUI-Net trajectory corpus, effectively giving agents a large\noffline memory of how humans operate hundreds of apps across multiple operating systems. RAG-GUI [659]\nmakes retrieval explicit at inference time by querying web tutorials and generating textual guidelines that\nare fed into any VLM-based GUI agent as step-by-step hints. WebRAGent [654] shows a related pattern in\nweb automation, combining a multi-modal retriever with a web agent so that each action is conditioned on\nretrieved guidance.\nSearch modules probe the research landscape to surface relevant papers and passages, enriching context and\ngrounding subsequent reasoning. WebThinker [647] equips large reasoning models with a Deep Web Explorer\n60\n"}, {"page": 61, "text": "Agentic Reasoning for Large Language Models\nmodule for autonomous web search and page navigation, and uses an Autonomous Think–Search–and–Draft\nstrategy with RL-based training to decide when to browse and what to extract during long-horizon tasks.\nDeepResearcher [260] scales end-to-end training on the real web via reinforcement learning in live search\nenvironments, optimizing the iterative think–search loop and exhibiting emergent behaviors such as plan\nformulation, cross-source corroboration, and self-correction over multi-step research trajectories. Retrieval-\ncentric agents like PaperQA [516] and its successor PaperQA2 [517] demonstrate that tightly coupling\nfull-text retrieval with generation can substantially improve scientific QA accuracy while preserving cited\nprovenance for literature synthesis.\nIn research settings, retrieval-augmented generation (RAG) grounds idea generation and analysis in freshly\nretrieved and citable passages. For example, GPT Researcher [635] is an autonomous research-agent that\nretrieves sources and generates reports with citations, enabling traceability of claims to evidence. Chain of\nIdeas [636] organizes relevant literature into a chain-structured scaffold that mirrors a field’s progressive\ndevelopment, thereby guiding retrieval and ideation toward subsequent links in the argument. Meanwhile,\nScideator [648] extracts key paper facets (e.g., purpose, mechanism, evaluation) and leverages them to drive\ntargeted retrieval and recombination of ideas for identifying methodological or evidentiary gaps.\n6.5.2. Self-evolving agentic reasoning\nEffective self-evolving abilities enable these autonomous agents to adapt their behavior over time, retain\ncrucial task context across interaction cycles and incrementally refine planning and execution strategies. The\nfollowing paragraphs review how memory, feedback and self-reflection mechanisms support this continual\nimprovement across these agent families, turning interaction from a one-shot pipeline into an iterative\nlearning loop.\nMemory.\nMemory modules transform brittle, single-pass web interactions into reusable experience. For\nexample, Agent Workflow Memory (AWM) [298] induces reusable workflows from successful trajectories\nand retrieves them to guide future tasks, while ICAL [660] distills noisy trajectories into high-level verbal\nand visual abstractions that are stored as a memory of multimodal experience and later injected into\nprompts. Control-oriented designs such as BrowserAgent [639] maintain explicit histories of past actions and\nintermediate conclusions in the agent’s context, instead of only re-encoding the current page view. GLM-based\nagents like AutoWebGLM [605] and AgentOccam [661] emphasize compressed page representations, using\nHTML simplification and carefully tuned observation spaces so that the agent’s prompt contains a shorter,\nmore informative view of the state, with past steps preserved through the usual action–observation history.\nMore integrated frameworks like LiteWebAgent [662] expose planning, memory and tree search as modular\ncomponents, and can plug in workflow memories together with search traces for long-horizon reuse.\nRecent GUI agents adopt explicit memory modules that store and retrieve task-relevant information during\nlong-horizon execution. Earlier work such as MobileGPT [663] equips a mobile assistant with human-like\napp memory: it decomposes procedures into modular sub-tasks that are explored, selected, derived, and\nthen stored so they can be recalled and reused instead of being re-discovered from scratch. Chain-of-Memory\n(CoM) [664] incorporates short- and long-term memory by recording action descriptions and task-relevant\nscreen information in a dedicated memory module, enabling cross-application navigation to track task state.\nMore recent systems build increasingly structured memories: MobA’s multifaceted memory module [615]\nmaintains environment- and user-level traces that an adaptive planner retrieves when refining mobile\ntask plans, while MGA [665] represents each step as a triad of current screenshot, spatial layout, and a\ndynamically updated structured memory that summarizes past transitions, mitigating error accumulation in\n61\n"}, {"page": 62, "text": "Agentic Reasoning for Large Language Models\nlong chains of actions. Mobile-Agent-E [666] adds a persistent long-term store of tips and shortcuts distilled\nfrom prior trajectories, so later plans can call reusable guidance and subroutines instead of relearning them.\nMirage-1 [667] similarly organizes experience into a hierarchical skill memory that a planner can retrieve as\nreusable building blocks for new GUI tasks.\nLong-term memory is crucial for autonomous research agents because it enables accumulation and reuse of\nprior knowledge, fostering continuity across research cycles. For example, Agent Laboratory [634] retains\nprior experiment code, results, and interpretation across its multi-phase workflow, enabling later stages to\nbuild on earlier work. GPT Researcher [635] generates reports with embedded citations and provides context\nfor planning and extension of research topics. Chain of Ideas [636] structures relevant literature into a\nchain scaffold that reflects a field’s progression and can be revisited as new evidence arises. The AI Scientist-\nv2 [530] incorporates a progressive agentic tree-search approach that enables branching, backtracking and\nfollow-up experimentation across iterations.\nAgentic Feedback and Reflection.\nModern web agents treat interaction as a continual learning process,\nusing feedback signals and reflection modules to refine their reasoning and recover from failures over time.\nAgent Q [113] combines guided Monte Carlo tree search with a self-critique stage, so that rollouts provide\nnot only action sequences but also preference-style supervision. ReAP [668] makes reflection explicit by\ntreating it as a retrieval problem: it stores task–reflection key–value pairs summarizing what was learned\nfrom past trajectories, then, at inference time, retrieves the most relevant reflections and appends them to\nthe agent’s prompt to guide planning on new web-navigation tasks. Agent-E [669] introduces an automatic\nvalidation pipeline that detects execution errors across text and vision, and then triggers self-refinement,\nenabling agents to iteratively correct their own workflows. Recon-Act [670] uses a dual-team architecture\nin which a Reconnaissance team extracts generalized tools from successful and failed trajectories, and an\nAction team applies these tools to re-plan tasks, forming a closed feedback loop. INFOGENT [652], on\nthe other hand, leverages aggregator feedback to iteratively refine navigation and search strategies based\non identified information gaps. And WINELL [671], as a updating web agent, relies on feedback from the\naggregation process to adapt subsequent searches and update selection during continuous operation. Finally,\nself-reflective search agents such as WebSeer [672] integrate explicit self-reflection signals into reinforcement\nlearning, constructing reflection-annotated trajectories and a two-stage training framework so that mis-solved\nor uncertain cases become targeted feedback that deepens future search and reasoning.\nGUI agents also integrate explicit reflection so they can critique and repair their own plans. Early computer-\ncontrol systems with structured reflection, for example, a zero-shot desktop control agent with structured\nself-reflection loops [673], provides conceptual templates that later GUI agents adapt to visual, multi-\napplication settings. GUI-Reflection [674] instantiates this idea end-to-end: it builds a reflection-oriented\ntask suite, automatically synthesizes error scenarios from existing successful trajectories, and adds an online\nreflection-tuning stage so multi-modal GUI models learn to detect failures, reason about causes, and generate\ncorrective actions without human annotation. History-Aware Reasoning (HAR) [675] treats long-horizon GUI\nautomation as a reflective learning problem, constructing reflective learning scenarios, synthesizing tailored\ncorrection guidelines, and designing a hybrid RL reward so the agent acquires episodic reasoning knowledge\nfrom its own errors and shifts from history-agnostic to history-aware reasoning. MobileUse [676] introduces\nhierarchical reflection on mobile devices, where the agent self-monitors at the action, subtask, and task level\nand triggers reflection on demand, pausing only when needed to diagnose and recover. InfiGUIAgent [614]\nintegrates hierarchical and expectation–reflection reasoning in a second training stage, enabling the agent to\nrun expectation–reflection cycles that compare expected and actual outcomes and revise multi-step plans\nwhen they diverge. Mobile-Agent-E [666] embeds an Action Reflector and Notetaker that evaluate executed\n62\n"}, {"page": 63, "text": "Agentic Reasoning for Large Language Models\nsteps and write refined Tips and Shortcuts back into persistent long-term memory, forming a self-evolution\nloop where the agent’s behavior is progressively refined from accumulated experience.\nFor autonomous research agents, learning from outcomes is essential to improve reasoning and experimental\nreliability over time. CycleResearcher [677] couples a research agent with a reviewer agent that provides\nautomated peer-review feedback, and uses an iterative preference-training loop so the research agent can\nrefine future drafts and decisions. MLR-Copilot [649] monitors execution results and human comments\nduring experiment implementation and execution, using these signals to iteratively refine code, configurations\nand even upstream hypotheses. Dolphin [650] implements a closed-loop auto-research framework in which\ngenerated code is run on benchmarks and exception-guided debugging plus outcome analysis feed back\ninto idea generation and implementation, pruning unproductive paths. At the search–reasoning interface,\nDeepResearcher [260] optimizes query, browsing, and answering policies via reinforcement learning on\nreal-web trajectories, with outcome rewards inducing behaviors such as planning, cross-validation, and\nself-reflection. Agentic Deep Research [678] further emphasizes reward design for reasoning-driven search,\narguing that principled incentives over answer quality and reasoning traces provide structured signals that\nimprove downstream synthesis in deep-research agents.\n6.5.3. Collective multi-agent reasoning\nCollective multi-agent reasoning for web agents reframes browser use as cooperation among specialized roles\nrather than a single monolithic policy. WebPilot [679] models web task execution as a multi-agent system with\na global planning agent that decomposes tasks and local MCTS-based executors that solve subtasks, jointly\nsteering search in complex web environments. INFOGENT [652] organizes web information aggregation\ninto a Navigator, Extractor, and Aggregator, so exploration, evidence extraction, and synthesis are handled\nby distinct cooperating agents with feedback from the Aggregator to guide future navigation; WINELL [671]\nleverages agentic web search to plan and execute iterative information gathering for discovering timely\nfactual updates relevant to a target Wikipedia article.. Recon-Act [670] adopts a Reconnaissance–Action\nparadigm in which a Recon team analyzes successful and failed trajectories to derive generalized tools or\nhints, and an Action team re-plans and executes with this evolving toolset. PAE [612] uses three roles, namely\na task proposer, an acting web agent and a VLM-based evaluator, to autonomously generate vision-based\nweb tasks and feed success signals back into the policy via RL. Hierarchical web agents such as Agent-E [84]\nand Plan-and-Act [93] similarly separate a high-level planner from a browser-navigation agent, enabling\nstructured plan–execute cooperation. At a more conceptual level, Agentic Web [680] envisions the internet as\nan agentic web of interacting agents and analyzes how coordination, communication protocols and economic\nincentives shape such ecosystems, while Agentic Deep Research [678] frames information seeking as iterative\nfeedback loops of reasoning, retrieval, and synthesis that can be instantiated by single- or multi-agent web\nresearch systems.\nMulti-agent designs for GUI agents typically decompose “using a computer” into cooperating roles that\nplan, perceive, decide and execute. COLA [681] instantiates a scenario-aware task scheduler, a planner,\na decision-agent pool, an executor, and a reviewer, so UI tasks are split into basic capability units and\nrouted to domain-specialized agents rather than a single monolith. On mobile, Mobile-Agent-v2 [682]\nadopts a tri-role pattern with planner, decision, and reflection agents for progress navigation, local action\nselection, and error correction, while Mobile-Agent-E [666] further builds a hierarchical stack with a Manager\nand four subordinate agents (i.e. Perceptor, Operator, Action Reflector, Notetaker) plus a self-evolution\nmodule that learns long-term Tips and Shortcuts from experience. Mobile-Agent-V [683] similarly employs\na video agent, decision agent, and reflection agent to coordinate multi-modal perception and execution,\nand MobileExperts [645] dynamically forms teams of expert agents with a dual-layer planner that allocates\n63\n"}, {"page": 64, "text": "Agentic Reasoning for Large Language Models\nsubtasks to tool-specialized experts. SWIRL [684] makes this structure explicit for RL, training a Navigator\nthat converts language and screen context into structured plans and an Interactor that grounds those plans\ninto atomic GUI actions within a multi-agent RL workflow. PC Agent [616] uses separate planning and\ngrounding agents in a two-stage pipeline for desktop automation, illustrating how multi-agent decomposition\ncan improve long-horizon PC control.\nTo facilitate autonomous research agents, multi-agent collaboration enables a single model’s linear workflow\nto become a coordinated research group: specialized agents operate in parallel, exchange intermediate\nartifacts through explicit interfaces, and provide adversarial or complementary feedback to improve both\ncreativity and rigor. For example, AgentRxiv [685] coordinates author, reviewer, and editor agents that\niteratively refine manuscripts and share evolving artifacts across virtual “labs.” ARIA [529] instantiates a\nrole-structured multi-LLM team that searches, filters, and synthesizes scientific literature into actionable\nexperimental procedures. Earlier multi-agent designs such as CAMEL [531] demonstrate how cooperative\nrole-play with tool access can enhance hypothesis generation and task decomposition. In experimental\nsciences, Coscientist [686] integrates planning, robotic instrument control, and analysis into a multi-agent\nclosed loop that autonomously designs and executes wet-lab experiments. Finally, TAIS [539] defines\na hierarchical team, namely project manager, data engineer and domain expert, that jointly discovers\ndisease-predictive genes from expression data through coordinated division of labor.\n7. Benchmarks\nAgentic reasoning has been evaluated through a rapidly growing set of benchmarks, but existing suites often\ndiffer in what they treat as the core capability, such as tool invocation accuracy, memory retention under long\ncontexts, or coordination quality in multi-agent settings. To provide a coherent view, we organize benchmarks\nfrom two complementary perspectives. We first summarize benchmarks that isolate core mechanisms of\nagentic reasoning, which helps pinpoint where systems succeed or fail at the capability level. We then review\napplication-level benchmarks that evaluate end-to-end agent behavior in realistic domains, capturing the\ncombined effects of perception, planning, tool use, memory, and coordination.\n7.1. Core Mechanisms of Agentic Reasoning\nWe begin with benchmarks that target mechanism-level capabilities, aiming to evaluate agentic reasoning\nin a more controlled and interpretable manner. Concretely, these benchmarks decompose agentic behavior\ninto a small set of recurring primitives, including tool use, search, memory and planning, and multi-agent\ncoordination. Such mechanism-centric evaluations make it easier to attribute performance changes to specific\ncomponents, and they complement end-to-end benchmarks that may conflate multiple sources of errors.\n7.1.1. Tool Use\nEvaluating tool-using models remains an open challenge due to the diversity of tasks, tools, and usage\nscenarios involved [687]. The key difficulties arise from the wide range of available tools, varying levels of\nscenario complexity, and the prevalence requirements specifically for the task domain.\nSingle-Turn Tool Use.\nWhile agentic reasoning often focuses on multi-turn or long-horizon interactions,\nsingle-turn tool use remains a foundational capability for evaluating LLMs’ basic tool invocation skills. ToolQA\n[688] constructs a dataset of 1,530 dialogues involving 13 specialized tools, designed to assess LLMs’ ability to\n64\n"}, {"page": 65, "text": "Agentic Reasoning for Large Language Models\nFigure 12: An overview of the benchmarks on agentic reasoning.\ninterface with external knowledge sources in a question-answering context. APIBench [78] introduces a large-\nscale benchmark grounded in real-world APIs from HuggingFace, TorchHub, and TensorHub, comprising\n1,645 unique APIs and 16,450 instruction–API pairs. It is used to train and evaluate Gorilla, an LLM capable\nof invoking a broad range of APIs, emphasizing generalization across diverse tool interfaces. ToolLLM-\nToolBench [203] curates 16,464 real-world APIs across 49 categories from the RapidAPI Hub, and uses\nChatGPT to generate diverse, instruction-style prompts for these APIs. The benchmark is used to train\nToolLLaMA, a model that demonstrates strong tool-use capabilities and exhibits promising generalization\nto unseen APIs. MetaTool [689] introduces the TOOLE dataset, containing over 20,000 entries and a\nbenchmark comprising approximately 200 tools across diverse scenarios, including software engineering,\nfinance, and art design. It splits tool selection tasks to tool selection with similar choices, tool selection\nin specific scenarios , tool selection with possible reliability issues, and multi-tool selection. T-Eval [690]\ndecomposes tool utilization into a series of sub-processes: instruction following, planning, reasoning, retrieval,\nunderstanding, and review, and evaluates each step individually to provide a fine-grained assessment of\ntool-use capabilities. The benchmark includes a total of 23,305 test cases spanning 15 different tools. GTA\n(General Tool Agents) [691] targets realistic tool-use scenarios by emphasizing real user queries, real-world\ndeployed tools, and multimodal inputs. It introduces 229 challenging tasks grounded in practical applications,\nspanning 14 tools across diverse domains. ToolRet [692] focuses specifically on the task of tool retrieval,\nintroducing a heterogeneous benchmark consisting of 7.6K diverse retrieval tasks and a corpus of 43K tools.\nMulti-Turn Tool Use.\nMulti-turn tool use offers a more realistic simulation of real-world applications,\nwhere agents autonomously select and sequence tools to solve complex tasks. ToolAlpaca [204] is one of the\nearliest efforts in this direction, using multi-agent simulations to generate 3,938 tool-use instances from over\n400 real-world APIs across 50 distinct categories. SambaNova-ToolBench [693] introduces a benchmark\ncentered on software tool manipulation for real-world tasks, with varying levels of API complexity to test\nagent capabilities. API-Bank [694] provides a dataset of 1,888 tool-use dialogues from 2,138 APIs, along with\na runnable evaluation system containing 73 APIs and 314 tool-use test cases. UltraTool [695] evaluates tool-\nuse capabilities across six dimensions: planning awareness, planning ability, creation, tool-use awareness,\ntool selection, and tool usage. The benchmark spans 22 domains, includes 2,032 tools, and provides\n5,824 evaluation samples. ToolFlow distinguishes itself from prior benchmarks by emphasizing long-term\nplanning. It features 224 expert-curated tasks involving 107 real-world tools, highlighting challenges in goal\ndecomposition and multi-step decision-making. More recently, MTU-Bench [696] presents a multi-granularity\nbenchmark for multi-turn, multi-tool scenarios, and releases MTU-Instruct, a large-scale instruction dataset\ncontaining 54,798 dialogues involving 136 tools. m & m’s introduces a benchmark with over 4,000 multi-\nstep, multimodal tasks involving 33 tools, including multimodal models, public APIs, and image processing\nmodules. It also provides a high-quality subset of 1,565 task plans that are human-verified and executable\n65\n"}, {"page": 66, "text": "Agentic Reasoning for Large Language Models\nend-to-end.\n7.1.2. Search\nTo systematically assess an agent’s ability to acquire information through interaction, recent benchmarks\ncast search as a sequential reasoning problem and can be broadly categorized into unimodal and multimodal\nsettings, differing in the nature of evidence sources, interaction spaces, and grounding requirements.\nUnimodal Search.\nRecent benchmarks for single-modal agentic search increasingly frame information\nseeking as a sequential, decision-driven process, emphasizing planning, interaction, and evidence synthesis.\nFor example, WebWalker [697] emphasizes structured website traversal, explicitly modeling search as\ncoordinated horizontal exploration and vertical drilling across interconnected pages. To reflect realistic\nopen-world information seeking, InfoDeepSeek [698] introduces a dynamic Web setting with verifiable\nyet non-curated answers, highlighting robustness to noise and distributional shift. Several benchmarks\nscale search along temporal and informational dimensions: Mind2Web 2 [50] focuses on long-horizon\nbrowsing and citation-grounded synthesis, whereas RAVine [699] augments answer quality with process-\nlevel efficiency and interaction fidelity. Complementarily, WideSearch [700] and DeepWideSearch [701]\ndistinguish between breadth-oriented large-scale fact aggregation and depth-oriented multi-hop reasoning,\nrevealing the difficulty of jointly optimizing coverage and reasoning coherence. Domain-specific benchmarks\nfurther stress reliability under strict correctness constraints: MedBrowseComp [702] targets clinical decision\nsupport by requiring agents to integrate heterogeneous and potentially conflicting medical evidence, while\nFinAgentBench [703] evaluates retrieval-centric reasoning in financial analysis through document-type\nselection and fine-grained passage localization. Finally, LocalSearchBench [704] grounds agentic search in\nreal-world local services, evaluating multi-constraint, multi-entity reasoning over large structured databases.\nCollectively, these benchmarks redefine agentic search evaluation around planning depth, interaction quality,\nevidence integration, and real-world fidelity, providing a more holistic assessment of search-centric reasoning\nin language-based agents.\nMultimodal Search.\nRecent benchmarks on multimodal agentic search move beyond static multimodal\nquestion answering to systematically evaluate an agent’s ability to actively retrieve, browse, and reason\nover heterogeneous information sources under realistic constraints. Benchmarks such as MMSearch [705]\nand its extension MMSearch-Plus [706] frame multimodal search as an end-to-end process, where agents\nmust interpret multimodal queries and synthesize answers by jointly leveraging textual and visual evidence,\nexplicitly modeling different input–output modality configurations. Complementing this setting, MM-\nBrowseComp [707] adapts the “hard-to-find, easy-to-verify” paradigm to multimodal web environments,\nenforcing mandatory image dependence to prevent text-only shortcuts and to stress-test multimodal evidence\ngrounding during open-web browsing. BEARCUBS [708] further emphasizes computer-using agents in live\nweb scenarios, requiring explicit interaction trajectories and multimodal manipulation (e.g., videos or 3D\nnavigation), thereby evaluating not only retrieval accuracy but also procedural competence. Moving into\ndomain-specific and tool-augmented regimes, PaperArena [709] evaluates multimodal agentic search in\nscientific workflows, where agents must coordinate PDF parsing, figure understanding, database queries, and\nweb search to answer research-level questions. Finally, Video-BrowseComp [710] and VideoDR [711] extend\nagentic search to video-centric settings, requiring agents to extract visual-temporal cues from videos and\niteratively validate hypotheses via open-web evidence, with carefully designed constraints to ensure dual\ndependence on video and external retrieval. Together, these benchmarks delineate a clear evolution toward\n66\n"}, {"page": 67, "text": "Agentic Reasoning for Large Language Models\nevaluating multimodal agents as interactive researchers, highlighting planning, tool use, and multimodal\nevidence integration as first-class capabilities in agentic search.\n7.1.3. Memory and Planning\nA distinctive advantage of agents lies in their ability to leverage memory to achieve accurate long-term\nperformance and strong reasoning capabilities. This ability can be assessed from two complementary\nperspectives. The first concerns memory management, which reflects how effectively an agent integrates,\norganizes, and retrieves long-term memories. The second concerns memory utilization, which captures how\nwell an agent exploits historical information to support planning and informed feedback. In this section, we\nseparately discuss benchmarks from these two aspects.\nFrom the perspective of memory management, existing benchmarks can be broadly categorized into Long-\nHorizon Episodic Memory and Multi-session Recall, depending on whether the textual context consists of a\nsingle continuous long-form input or multiple discontinuous conversational sessions.\nLong-Horizon Episodic Memory.\nThis category targets single-episode tasks with partial observability and\ndelayed rewards, requiring agents to store and retrieve information over extended time spans. Benchmarks in\nthis space evaluate memory retention, retrieval, and reasoning across long contexts. PerLTQA [712] simulates\npersonalized dialogue, where agents answer questions using long-term persona and event memories. It\nincludes 8.5K QA pairs and evaluates memory classification, retrieval ranking, and synthesis fidelity. ELITR-\nBench [713] tests QA on noisy meeting transcripts, where relevant evidence may appear far earlier than the\nquery. Models are scored via GPT-4 across various ASR noise levels and dialogue settings. In the meanwhile,\nMulti-IF [714] and MultiChallenge [715] focus on multi-turn instruction following. Multi-IF [714] spans\n4.5K tri-turn conversations in 8 languages, with evaluation based on strict and relaxed instruction accuracy.\nMultiChallenge [715] tests four memory-intensive phenomena: retention, inference, editing, and coherence,\nusing 273 curated dialogues with binary pass/fail evaluation. TurnBench-MS [716] evaluates multi-step\nreasoning across 540 symbolic logic games, tracking win rate, round-level accuracy, and verifier usage.\nStoryBench [717] casts memory as decision-making in interactive narratives, where agents must remember\nprior choices to progress. It assesses decision accuracy, retry counts, and runtime efficiency. MemBench\n[718] tests factual and reflective memory across 60K episodes in participatory and observational settings,\nwith metrics for accuracy, recall, capacity, and retrieval speed. MMRC [719] develops a multimodal memory\nbenchmark focused on single-round multimodal conversations. Together, these benchmarks emphasize\nstructured memory demands, with metrics capturing not just task success but also memory precision,\nsynthesis quality, and robustness under long-context stress.\nMulti-session Recall.\nMulti-session Recall focuses on multi-episode tasks where agents must retain and\nintegrate knowledge across separate sessions, supporting lifelong adaptation and mitigating catastrophic\nforgetting. A range of recent benchmarks systematically probe this capability under realistic, long-term\ninteraction scenarios. LOCOMO [322] evaluates LLM agents on sustained conversational memory across\n19-session dialogues, using tasks such as multi-hop QA, event summarization, and multi-modal response\ngeneration. MemSim [720] introduces a simulator-based framework with over 2,900 synthetic trajectories\nin daily life domains, assessing fact retention across sessions via accuracy, diversity, and rationality scores.\nLONGMEMEVAL [323] benchmarks assistants on five sub-tasks: information extraction, multi-session rea-\nsoning, temporal inference, knowledge updating, and abstention, over dialogue histories spanning up to\n1.5M tokens, with GPT-4 judged accuracy and retrieval recall. REALTALK [721] presents 21-day real human\n67\n"}, {"page": 68, "text": "Agentic Reasoning for Large Language Models\nconversations with 17K tokens per dyad, enabling evaluation of memory probing and persona simulation\nthrough multi-hop QA and emotional grounding metrics. Furthermore, MemoryAgentBench [722] unifies\ndiverse memory tasks such as test-time learning, conflict resolution, and long-range understanding across\nmultiple datasets, with task-specific metrics including classification accuracy, partial-match F1, and ROUGE.\nMem-Gallery [310] introduces a multimodal long-term memory evaluation benchmark that systematically\ncovers a wide range of memory management and utilization scenarios. Lastly, Evo-Memory [25] introduces a\nbenchmark and a unified evaluation protocol for measuring experience reuse in test-time learning. Collec-\ntively, these benchmarks underscore the importance of dynamic memory integration across sessions and\nprovide comprehensive evaluations across factual recall, adaptation, and reasoning.\nFrom the perspective of memory utilization, we provide a detailed discussion of benchmarks that evaluate\nan agent’s ability to support planning and feedback using historical information.\nPlanning and Feedback.\nBenchmarks targeting planning and feedback primarily assess whether agents can\neffectively utilize memory to support multi-step planning based on environmental feedback, and maintain\ncoherent internal state over extended interactions. First, ALFWorld [48] employs interactive environments to\nevaluate the consistency of multi-step planning, requiring agents to accumulate observations across actions\nand maintain latent internal states throughout execution. Moreover, formal planning benchmarks such as\nPlanBench [723] and ACPBench [724] assess planning capabilities in explicitly defined dynamic environments,\ntesting whether agents can correctly reason about action preconditions, effects, reachability, and overall\nplan validity. TEXT2WORLD [725] integrate fragmented textual descriptions into a coherent and executable\nworld model, evaluating the capacity to continuously consolidate historical facts into structured planning\nrepresentations. More recent benchmarks place greater emphasis on feedback integration and planning\nunder non-stationary conditions. For example, REALM-Bench [726] introduces dynamic disturbances in\nreal-world manufacturing scenarios, requiring agents to remember prior commitments and replan when\nunderlying assumptions are violated, while TravelPlanner [727] focuses on accurate itinerary construction\nunder constrained and evolving information. Finally, FlowBench [728] and UrbanPlanBench [729] assess\nplanning performance in procedural and domain-specific settings, respectively, where agents must preserve\nconversational or policy context and apply it consistently across decision steps. Together, these benchmarks\ngo beyond one-shot plan generation and systematically investigate whether agents can leverage historical\ninformation to support sustained planning, adaptive feedback integration, and iterative decision revision\nover time.\n7.1.4. Multi-Agent System\nTo evaluate coordination, competition, and decision making beyond isolated reasoning, recent benchmarks\nsituate multi-agent systems in interactive environments. These works broadly span game-based evaluations,\nsimulation-centric real-world scenarios, and language-driven social reasoning tasks.\nGame-based reinforcement learning evaluation.\nGame-based reinforcement learning evaluation bench-\nmarks leverage classical and novel gaming environments to systematically compare the performance of\nmulti-agent RL algorithms under cooperative and adversarial settings. MAgent [730] facilitates massive-scale\nmulti-agent scenarios such as pursuit and resource competition within customizable grid-worlds, evaluating\nindividual cumulative rewards and competitive metrics like resource occupancy rates. Pommerman [731]\nadapts the classic Bomberman game for cooperative and adversarial interactions, quantifying performance\nthrough win rates, survival duration, and kill-to-suicide ratios. SMAC [732] centers on decentralized micro-\n68\n"}, {"page": 69, "text": "Agentic Reasoning for Large Language Models\nmanagement challenges in StarCraft II scenarios, evaluating team success via win rates, average damage\noutput, and formation dispersion. MineLand [733] utilizes Minecraft as a realistic ecological simulation for\nlarge-scale multi-agent coordination, with up to 64 agents cooperating to meet physical needs under partial\nobservability. TeamCraft [734] also employs Minecraft to benchmark embodied multi-modal agents tasked\nwith interpreting visual, textual, and environmental prompts to collaboratively achieve 55,000 procedurally\ngenerated task instances. Melting Pot [735] assesses agents’ zero-shot generalization capabilities in diverse\nsocial dilemma environments, utilizing metrics such as per-capita return, social welfare, and inequality\nindices. BenchMARL [736] provides standardized algorithm comparisons across multiple scenarios (e.g.,\nSMACv2, VMAS, MPE), measuring convergence rates, final performance, and hyperparameter sensitivity.\nFinally, Arena [737] encompasses a comprehensive suite of cooperative and adversarial games across various\ncomplexities, evaluating individual returns, collective social welfare, and emergent communication protocols.\nSimulation-centric real-world assessment.\nSimulation-centric real-world benchmarks simulate realistic\nor pseudo-realistic environments, emphasizing scalability, partial observability, and dynamic planning.\nSMARTS [738] offers a scalable multi-agent driving platform for real-world traffic scenarios like merges and\nintersections, with evaluation based on collision rates, task completion, and agent behavior distributions.\nNocturne [739] provides high-throughput, partially observable driving simulations using Waymo trajectories,\ntesting coordination and human-like behavior in tasks such as intersections and roundabouts. MABIM [740]\nbenchmarks multi-echelon inventory management, simulating cooperative and competitive retail dynamics,\nevaluated via profit metrics across diverse inventory settings. IMP-MARL [741] addresses infrastructure\ninspection and maintenance scheduling, measuring risk reduction and cost efficiency in large-scale systems.\nPOGEMA [742] focuses on decentralized multi-agent pathfinding in grids, tracking success rate, path\nefficiency, and large-scale coordination. INTERSECTIONZOO [743] studies contextual RL for cooperative\neco-driving at intersections, using traffic simulations to evaluate emissions and travel-time performance.\nREALM-Bench [726] introduces real-world planning tasks from logistics to disaster relief, with dynamic\ndisruptions, multi-threaded dependencies, and evaluation via planning quality, adaptability, and constraint\nsatisfaction. Together, these benchmarks reflect challenges in scaling, uncertainty, coordination, and dynamic\nadaptation, offering rigorous testbeds for real-world multi-agent systems.\nLanguage, Communication, and Social Reasoning.\nBenchmarks in Language, Communication, and\nSocial Reasoning explore multi-agent communication protocols, Theory-of-Mind reasoning, game-theoretic\ninteractions, and language-driven coordination. LLM-Coordination [744] examines collaborative reasoning\nand joint-planning abilities of LLM agents through cooperative gameplay (e.g., Hanabi, Overcooked-AI),\nmeasured by holistic scores and fine-grained coordination question accuracy. AVALONBENCH [745] leverages\nthe social deduction game Avalon to assess role-conditioned language-based reasoning, with datasets of\nthousands of five-player dialogues and metrics on win-rate, role accuracy, and voting dynamics. Welfare\nDiplomacy [746] extends the classic game Diplomacy to general-sum welfare negotiation, using 50-game\ndatasets to quantify coalition stability and welfare-oriented strategic reasoning. MAgIC [747] covers social\ndeduction and classic dilemmas (e.g., Chameleon, Prisoner’s Dilemma), employing handcrafted scenario\ndatasets to benchmark reasoning, deception, coordination, and rationality. BattleAgentBench [19] assesses\nlanguage-based cooperative and competitive dynamics in strategic gameplay environments, scoring navigation\naccuracy, agent interactions, and exploitability across diverse map datasets. COMMA [748] evaluates\nmultimodal communicative reasoning through collaborative puzzle-solving tasks involving visual-language\ncoordination, measured by grounding accuracy, privacy compliance, and dialogue effectiveness across\nthousands of scenarios. IntellAgent [749] introduces synthetic conversational AI tasks in retail and airline\n69\n"}, {"page": 70, "text": "Agentic Reasoning for Large Language Models\ndomains, generating extensive policy-constrained dialogue datasets evaluated by conversational success,\nmistake frequency, and policy adherence. Finally, MultiAgentBench [21] provides a comprehensive assessment\nacross tasks such as Minecraft building, coding, and bargaining, employing dynamic key-performance\nindicators and LLM-scored communication quality across various multi-agent topologies and scenarios.\n7.2. Applications of Agentic Reasoning\nWhile mechanism-centric benchmarks help isolate individual capabilities, real-world deployments require\nthese capabilities to work together under realistic constraints, such as partial observability, long-horizon\ndependencies, and safety-critical decisions. We therefore next review application-level benchmarks that\nevaluate end-to-end agent performance across representative environments, with tasks that jointly stress\nperception, reasoning, action execution, and coordination.\nIn this subsection, we review benchmarks designed to evaluate the application-level performance of agentic\nreasoning systems across various domains. These benchmarks assess agents’ ability to perceive, reason,\nand act in realistic or high-impact task settings. We organize the discussion into six categories based on\nthe application environment: Embodied Agents, Scientific Discovery Agents, Autonomous Research Agents,\nMedical and Clinical Agents, Web Agents, and Tool-Use Agents. Each subsubsection introduces representative\nbenchmarks and describes their design motivation, task format, and evaluation metrics.\n7.2.1. Embodied Agents\nBenchmarks under this category evaluate agents that interact with physical or simulated environments,\nrequiring grounding, perception, and action planning. AgentX [750] provides a diverse suite of vision-\nlanguage embodied tasks in driving and sports, where agents must make decisions using multimodal\ninformation from videos. It emphasizes reasoning across scenes with occlusions, temporal gaps, or distractors.\nBALROG [751] builds a reinforcement learning-centric framework for benchmarking agentic planning in game\nenvironments, focusing on instruction-following, temporal abstraction, and error correction. ALFWorld [48]\nlinks language instructions to object interactions in a text-based 3D environment, evaluating perception-\ngrounded execution. AndroidArena [752] targets GUI-based mobile tasks, where agents must perform actions\nlike form-filling and app navigation using vision-language understanding. StarDojo [753] leverages the\nopen-ended Stardew Valley game to study social planning and role-based coordination. MindAgent [754] and\nNetPlay [755] create multiplayer gaming testbeds to benchmark emergent social reasoning and negotiation\nunder uncertainty. OSWorld [756] offers a simulated desktop environment with diverse cross-app productivity\ntasks, such as opening files, converting formats, and modifying documents. These environments challenge\nagents to coordinate between perception, planning, and symbolic action in dynamic and often partially\nobservable scenarios.\n7.2.2. Scientific Discovery Agents\nScientific benchmarks aim to test agents’ capabilities in knowledge acquisition, hypothesis generation, and\nexperimental automation. DISCOVERYWORLD [757] introduces a virtual lab where agents explore scientific\nphenomena in biology, chemistry, and physics through simulated tools and instruments. ScienceWorld [758]\nfocuses on elementary science experiments using textual instructions and environment interactions, requiring\nstep-by-step hypothesis testing. ScienceAgentBench [759] builds a benchmark from real-world scientific\npapers, translating tasks like code implementation, figure generation, and variable extraction into executable\nsubtasks, assessing agents’ ability to automate the research process. The AI Scientist [651] simulates a\nfull end-to-end research pipeline, where agents perform literature review, method writing, experiment\n70\n"}, {"page": 71, "text": "Agentic Reasoning for Large Language Models\nexecution, and peer-review simulation. LAB-Bench [760] evaluates biology-specific agents on tasks involving\ngenetic sequence reasoning and experiment planning. MLAgentBench [761] benchmarks agents’ ability\nto autonomously train, evaluate, and tune machine learning models, offering realistic experimentation\nworkflows. These benchmarks collectively probe open-ended reasoning, long-horizon planning, and scientific\ngrounding in semi-structured data settings.\n7.2.3. Autonomous Research Agents\nThis category benchmarks agents designed for long-horizon workflows across general-purpose research,\noffice, or planning tasks. WorkArena [762] and its extension WorkArena++ [763] propose enterprise task\nbenchmarks where agents must complete ticket-based workflows involving retrieval, summarization, and\ncoordination across documents. OfficeBench [764] simulates a productivity software suite environment\nwith tasks such as creating meeting memos, modifying spreadsheets, and replying to emails, emphasizing\ngoal decomposition and tool selection. PlanBench [723] and FlowBench [728] test general workflow\nplanning skills with abstracted task graphs and structured dependencies. ACPBench [724] evaluates agents\nin assistant–collaborator–planner triads, tracking performance in a hybrid role hierarchy. TRAIL [765]\nfocuses on multi-agent trace debugging and error attribution [766] in LLM-based systems, providing dense\nannotations for reasoning chains. CLIN [767] introduces lifelong few-shot learning benchmarks where agents\nadapt to distribution shift and task evolution. Agent-as-a-Judge [768] studies peer-review style evaluation\nwith agents grading reasoning chains and correctness of other agents’ outputs. InfoDeepSeek [698] measures\ninformation-seeking abilities in open-domain QA and synthesis tasks. Together, these benchmarks capture the\ngrowing demand for agentic reasoning in complex knowledge workflows that involve abstraction, iteration,\nand evaluation.\n7.2.4. Medical and Clinical Agents\nThese benchmarks test agents’ abilities to reason with clinical knowledge, patient data, and multimodal\nbiomedical sources. AgentClinic [769] introduces a virtual hospital environment where agents make di-\nagnostic decisions based on patient symptoms and medical imaging. MedAgentBench [770] combines\nmedical QA, patient simulation, and retrieval tasks in a multi-format benchmark grounded in standardized\nexams. MedAgentsBench [771] evaluates multi-hop medical reasoning over structured and unstructured\ndata, scoring agents on correctness and evidence alignment. EHRAgent [581] benchmarks agents working\nover structured electronic health record (EHR) tables and clinical notes to complete tasks like diagnosis code\nprediction and medication reasoning. MedBrowseComp [702] focuses on browsing-based medical QA, where\nagents must retrieve and verify information across web pages. ACC [772] explores trustworthy medical\nagents with retrieval, hallucination detection, and citation-based support evaluation. MedAgents [601] uses\na collaborative multi-agent dialogue setup to simulate patient–doctor–nurse interactions, scoring fluency\nand factual accuracy. GuardAgent [773] proposes a clinical privacy safeguard agent with structured risk\ndetection benchmarks on EHR and website forms. These datasets emphasize correctness, trustworthiness,\nand safety in real-world clinical deployment contexts.\n7.2.5. Web Agents\nWeb agents operate in realistic browsing environments and are benchmarked on their ability to parse layouts,\nexecute actions, and handle dynamic content. WebArena [45] introduces a browser-based benchmark suite\ncontaining 90+ realistic websites across domains like shopping and booking, where agents complete tasks\nwith structured goals and click-based APIs. VisualWebArena [46] extends this with visual rendering, requir-\n71\n"}, {"page": 72, "text": "Agentic Reasoning for Large Language Models\ning agents to parse webpage images and align instructions with rendered components. WebVoyager [638]\nproposes goal-driven navigation with long-horizon tasks involving multi-page traversal and backtracking.\nMind2Web [50] targets cross-domain web automation with multi-task datasets and rich grounding annota-\ntions. WebCanvas [774] supports fine-grained layout manipulation, such as drag-drop and resize actions.\nWebLINX [775] simulates information gathering tasks with browsing, summarization, and answer synthesis.\nBrowseComp-ZH [776] brings language and infrastructure diversity with Chinese websites, challenging\nagents on multilingual understanding. LASER [777], WebWalker [697], and AutoWebBench [605] focus on\nstructured page representation, real-time action execution, and policy learning in web navigation. These\nbenchmarks highlight perception, grounding, and policy generalization challenges in web settings.\n7.2.6. General Tool-Use Agents\nThis group of benchmarks emphasizes LLM agents’ ability to invoke, coordinate, and reason over tools and\nAPIs. GTA [691] presents a realistic tool-use benchmark grounded in user queries and deployed software\ntools, spanning APIs from image generation to analytics dashboards. NESTFUL [778] evaluates nested API\ninvocation tasks requiring compositional planning across toolchains. CodeAct [99] simulates executable\nfunction calling and evaluates agents on parsing, composition, and runtime accuracy. RestGPT [225] connects\nLLMs with RESTful APIs via coarse-to-fine planning pipelines, tested on 60+ tool types. Search-o1 [23]\nframes tool use as sequential retrieval, with benchmarks spanning code search, PDF querying, and scientific\ntool usage. Agentic RL [779] proposes a reinforcement learning agent with access to tool interfaces and\nevaluation tasks such as calendar scheduling and translation. ActionReasoningBench [780] benchmarks\nagents’ ability to reason about action side effects and downstream consequences using a structured action\ngrammar. R-Judge [781] introduces safety judgment benchmarks where agents assess risky plans involving\ntools. These datasets jointly reflect the increasing complexity and compositionality of tool-augmented agent\nenvironments.\n8. Open Problems\nIn this section, we highlight open problems arising from user-centric personalization, long-horizon interaction\nand credit assignment, world-model-based reasoning, multi-agent collaboration and training, latent internal\nreasoning, and the governance of agentic systems operating autonomously in real-world environments.\n8.1. User-centric Agentic Reasoning and Personalization\nUser-centric agentic reasoning [782, 783] refers to an agent’s ability to tailor its reasoning and actions to\na specific individual user by modeling user characteristics, preferences, and interaction history over time.\nRather than optimizing a fixed, task-defined objective, a user-centric agent treats the user as part of the\nenvironment and continuously adapts its strategy through extended, multi-turn interaction. This requires\nthe agent to dynamically infer evolving user intent, accommodate changes in goals and behavior styles, and\nadjust decisions based on explicit or implicit user feedback as the dialogue progresses. Crucially, user-centric\nagentic reasoning involves balancing short-term task rewards with long-term user experience, satisfaction,\nand trust, which introduces non-stationary objectives and long-horizon credit assignment challenges beyond\nconventional agentic reasoning settings.\n72\n"}, {"page": 73, "text": "Agentic Reasoning for Large Language Models\n8.2. Long-horizon Agentic Reasoning from Extended Interaction\nA central open challenge in agentic reasoning is robust long-horizon planning and credit assignment across\nextended interactions. While methods such as ReAct and Tree of Thought improve short-horizon reasoning\n[5, 4], errors still compound rapidly in long tasks, as illustrated by embodied agents like Voyager [36].\nRL-trained agents such as WebRL and Agent-R1 improve performance in realistic environments but rely\non heavily engineered, domain-specific rewards and largely treat episodes independently [437, 28]. More\nrecent process-aware approaches attempt to construct finer-grained credit signals [784, 15, 785], yet remain\nenvironment-specific. A core open problem is how to assign credit across tokens, tool calls, skills, and memory\nupdates, and to generalize such learning across a long sequence of episodes and tasks.\n8.3. Agentic Reasoning with World Models\nWorld-model-based agents [786, 316] aim to mitigate myopic reasoning by enabling internal simulation\nand lookahead. Model-based RL systems such as DreamerV3 demonstrate the effectiveness of imagined\nrollouts for long-horizon control [787], while recent LLM-based agents adapt world models to web, code,\nand GUI environments [788, 786, 789, 790]. However, current designs rely on ad hoc representations\nand are typically trained on short-horizon or environment-specific data, raising concerns about calibration\nand generalization. Only a few works explore co-evolving world models and agents over long time scales\n[610, 791]. An open problem is how to jointly train, update, and evaluate world models in non-stationary\nenvironments, and how to assess their causal impact on downstream planning reliability.\n8.4. Multi-agent Collaborative Reasoning and Training\nMulti-agent collaboration has emerged as a powerful paradigm for scaling agentic reasoning through role\nspecialization and division of labor [67, 792, 66]. While debate- and role-based systems often outperform\nsingle agents, most collaboration structures are still manually designed. Recent multi-agent RL approaches\nbegin to treat collaboration itself as a trainable skill [409, 413, 26], but credit assignment at the group level\nremains poorly understood. Scaling to larger agent populations further introduces challenges in topology\nadaptation, coordination overhead, and safety [793, 794, 766]. A key open problem is how to learn adaptive,\ninterpretable collaboration policies that remain robust under partial observability and adversarial conditions.\n8.5. Latent Agentic Reasoning\nLatent agentic reasoning [795, 796, 441] explores performing planning, decision-making and collaboration\nin internal latent spaces rather than explicit natural language or symbolic traces. Recent work suggests\nthat latent reasoning can improve efficiency and scalability, but at the cost of reduced interpretability and\ncontrollability. In agentic settings, this raises additional challenges, including how to align latent reasoning\nwith external objectives, tools, agents and memory systems. Diagnosing failures becomes particularly difficult\nwhen intermediate reasoning steps are not externally observable. An open problem is how to design learning\nobjectives, probing methods, and evaluation benchmarks that make latent agentic reasoning both effective\nand auditable.\n8.6. Governance of Agentic Reasoning\nGovernance is a cross-cutting challenge for agentic reasoning systems that act autonomously over tools,\nenvironments, and other agents. Beyond standard LLM safety issues, agentic systems introduce new risks due\n73\n"}, {"page": 74, "text": "Agentic Reasoning for Large Language Models\nto long-horizon planning, persistent memory, and real-world action execution [797]. Failures may arise from\ninteractions across time and components, making attribution and auditing difficult. Existing benchmarks\nand guardrails mainly focus on short-horizon behaviors [773, 781], leaving planning-time failures and\nmulti-agent dynamics underexplored. A central open problem is to develop governance frameworks that\njointly address model-level alignment, agent-level policies, and ecosystem-level interactions under realistic\ndeployment conditions.\nReferences\n[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural\ninformation processing systems, 35:24824–24837, 2022.\n[2] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in\nlarge language models. arXiv preprint arXiv:2205.10625, 2022.\n[3] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning, pages 10764–10799. PMLR, 2023.\n[4] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models.\nAdvances in neural\ninformation processing systems, 36:11809–11822, 2023.\n[5] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. In International Conference on Learning\nRepresentations (ICLR), 2023.\n[6] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke\nZettlemoyer, Nicola Cancedda, and Thomas Scialom.\nToolformer: Language models can teach\nthemselves to use tools. Advances in Neural Information Processing Systems, 36:68539–68551, 2023.\n[7] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing\nSystems, 36:38154–38180, 2023.\n[8] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\nXu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of\nComputer Science, 18(6):186345, 2024.\n[9] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. Agentic retrieval-augmented\ngeneration: A survey on agentic rag. arXiv preprint arXiv:2501.09136, 2025.\n[10] Yizheng Huang and Jimmy Huang. A survey on retrieval-augmented text generation for large language\nmodels. arXiv preprint arXiv:2404.10981, 2024.\n[11] Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi\nSong, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as\ngeneralist agents. arXiv preprint arXiv:2407.16741, 2024.\n74\n"}, {"page": 75, "text": "Agentic Reasoning for Large Language Models\n[12] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building\nproduction-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025.\n[13] Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai,\nJihao Zhao, Yezhaohui Wang, et al. Memos: An operating system for memory-augmented generation\n(mag) in large language models. arXiv preprint arXiv:2505.22101, 2025.\n[14] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning. Advances in Neural Information Processing\nSystems, 36:8634–8652, 2023.\n[15] Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Kristian\nKersting, Jeff Z. Pan, Hinrich Schütze, Volker Tresp, and Yunpu Ma. Memory-r1: Enhancing large\nlanguage model agents to manage and utilize memories via reinforcement learning. arXiv preprint\narXiv:2508.19828, 2025.\n[16] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu, and Yemin\nShi. Autoagents: A framework for automatic agent generation. arXiv preprint arXiv:2309.17288,\n2023.\n[17] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. In The Twelfth International Conference on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=VtmBAGCN7o.\n[18] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing cogni-\ntive synergy in large language models: A task-solving agent through multi-persona self-collaboration.\nIn Proc. 2024 Annual Conference of the North American Chapter of the Association for Computational\nLinguistics (NAACL2024), 2024.\n[19] Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, and Jie Tang. Battleagentbench: A benchmark for\nevaluating cooperation and competition capabilities of language models in multi-agent systems. arXiv\npreprint arXiv:2408.15971, 2024.\n[20] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen\nMen, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng\nShen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench:\nEvaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. URL https://www.arxiv.org/\nabs/2308.03688.\n[21] Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong\nWang, Cheng Qian, Xiangru Tang, Heng Ji, et al. Multiagentbench: Evaluating the collaboration and\ncompetition of llm agents. arXiv preprint arXiv:2503.01935, 2025.\n[22] Ziyi Ni, Yifan Li, Ning Yang, Dou Shen, Pin Lyu, and Daxiang Dong. Tree-of-code: A self-growing\ntree framework for end-to-end code generation and execution in complex tasks. In Findings of the\nAssociation for Computational Linguistics: ACL 2025, pages 9804–9819, 2025.\n[23] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng\nDou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366,\n2025.\n75\n"}, {"page": 76, "text": "Agentic Reasoning for Large Language Models\n[24] Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic\nmemory for llm agents. arXiv preprint arXiv:2502.12110, 2025.\n[25] Tianxin Wei, Noveen Sachdeva, Benjamin Coleman, Zhankui He, Yuanchen Bei, Xuying Ning, Mengting\nAi, Yunzhe Li, Jingrui He, Ed H Chi, et al. Evo-memory: Benchmarking llm agent test-time learning\nwith self-evolving memory. arXiv preprint arXiv:2511.20857, 2025.\n[26] Hao Ma, Tianyi Hu, Zhiqiang Pu, Liu Boyin, Xiaolin Ai, Yanyan Liang, and Min Chen. Coevolving\nwith the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning.\nAdvances in Neural Information Processing Systems, 37:15497–15525, 2024.\n[27] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and\nJiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement\nlearning. arXiv preprint arXiv:2503.09516, 2025.\n[28] Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao\nZhang, Bing Yin, et al. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement\nlearning. arXiv preprint arXiv:2505.16421, 2025.\n[29] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without\nhuman demonstrations. Nature, 625(7995):476–482, 2024.\n[30] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan\nKumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al.\nMathematical discoveries from program search with large language models. Nature, 625(7995):\n468–475, 2024.\n[31] Ranjan Sapkota, Konstantinos I Roumeliotis, and Manoj Karkee. Vibe coding vs. agentic coding:\nFundamentals and practical implications of agentic AI, 2025.\n[32] Andrej Karpathy. Vibe coding — wikipedia. https://en.wikipedia.org/wiki/Vibe_coding,\n2025.\n[33] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller.\nChemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376,\n2023.\n[34] Fouad Bousetouane. Physical ai agents: Integrating cognitive intelligence with real-world action.\narXiv preprint arXiv:2501.08944, 2025.\n[35] Qianggang Ding, Santiago Miret, and Bang Liu. Matexpert: Decomposing materials discovery by\nmimicking human experts. arXiv preprint arXiv:2410.21317, 2024.\n[36] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv\npreprint arXiv:2305.16291, 2023.\n[37] Booker Meghan, Byrd Grayson, Kemp Bethany, Schmidt Aurora, and Rivera Corban. Embodiedrag:\nDynamic 3d scene graph retrieval for efficient and scalable robot task planning. arXiv preprint\narXiv:2410.23968, 2024. URL https://www.arxiv.org/abs/2410.23968.\n76\n"}, {"page": 77, "text": "Agentic Reasoning for Large Language Models\n[38] Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei\nChen, Yong Li, and Wenwu Zhu. Embodied-r: Collaborative framework for activating embodied\nspatial reasoning in foundation models via reinforcement learning. arXiv preprint arXiv:2504.12680,\n2025.\n[39] Binxu Li, Tiankai Yan, Yuanting Pan, Jie Luo, Ruiyang Ji, Jiayuan Ding, Zhe Xu, Shilong Liu, Haoyu\nDong, Zihao Lin, et al. Mmedagent: Learning to use medical tools with multi-modal agent. arXiv\npreprint arXiv:2407.02483, 2024.\n[40] Kexin Huang, Serena Zhang, Hanchen Wang, Yuanhao Qu, Yingzhou Lu, Yusuf Roohani, Ryan Li, Lin\nQiu, Gavin Li, Junze Zhang, et al. Biomni: A general-purpose biomedical ai agent. biorxiv, 2025.\n[41] Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan\nLi, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent.\narXiv preprint arXiv:2507.02592, 2025.\n[42] Boyuan Zheng, Michael Y Fatemi, Xiaolong Jin, Zora Zhiruo Wang, Apurva Gandhi, Yueqi Song, Yu Gu,\nJayanth Srinivasa, Gaowen Liu, Graham Neubig, et al. Skillweaver: Web agents can self-improve by\ndiscovering and honing skills. arXiv preprint arXiv:2504.07079, 2025.\n[43] Ranjan Sapkota, Konstantinos I Roumeliotis, and Manoj Karkee. Ai agents vs. agentic ai: A conceptual\ntaxonomy, applications and challenges. arXiv preprint arXiv:2505.10468, 2025.\n[44] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. A dynamic llm-powered agent network\nfor task-oriented agent collaboration. In First Conference on Language Modeling, 2024.\n[45] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue\nOu, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment\nfor building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. URL https://www.arxiv.\norg/abs/2307.13854.\n[46] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Gra-\nham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluat-\ning multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. URL\nhttps://www.arxiv.org/abs/2401.13649.\n[47] Lawrence Jang, Yinheng Li, Dan Zhao, Charles Ding, Justin Lin, Paul Pu Liang, Rogerio Bonatti,\nand Kazuhito Koishida. Videowebarena: Evaluating long context multimodal agents with video\nunderstanding web tasks. arXiv preprint arXiv:2410.19100, 2024.\n[48] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv\npreprint arXiv:2010.03768, 2020.\n[49] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su.\nMind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems,\n36:28091–28114, 2023.\n[50] Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao\nYu, Bernal Jiménez Gutiérrez, Yiheng Shu, et al. Mind2web 2: Evaluating agentic search with\nagent-as-a-judge. arXiv preprint arXiv:2506.21506, 2025.\n77\n"}, {"page": 78, "text": "Agentic Reasoning for Large Language Models\n[51] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey.\narXiv preprint arXiv:2212.10403, 2022.\n[52] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu,\nYuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: A survey of long chain-of-thought\nfor reasoning large language models. arXiv preprint arXiv:2503.09567, 2025.\n[53] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan,\nJiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: A survey of\nreinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025.\n[54] Zixuan Ke, Fangkai Jiao, Yifei Ming, Xuan-Phi Nguyen, Austin Xu, Do Xuan Long, Minzhi Li, Chengwei\nQin, Peifeng Wang, Silvio Savarese, et al. A survey of frontiers in llm reasoning: Inference scaling,\nlearning to reason, and agentic systems. arXiv preprint arXiv:2504.09037, 2025.\n[55] Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian,\nGuoli Jia, Pengfei Li, et al. A survey of reinforcement learning for large reasoning models. arXiv\npreprint arXiv:2509.08827, 2025.\n[56] Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi\nLi, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning for llms: A survey.\narXiv preprint arXiv:2509.02547, 2025.\n[57] Minhua Lin, Zongyu Wu, Zhichao Xu, Hui Liu, Xianfeng Tang, Qi He, Charu Aggarwal, Xiang\nZhang, and Suhang Wang. A comprehensive survey on reinforcement learning-based agentic search:\nFoundations, roles, optimizations, evaluations, and applications. arXiv preprint arXiv:2510.16724,\n2025.\n[58] Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei\nLiu, Zihao Li, et al. A comprehensive survey of self-evolving ai agents: A new paradigm bridging\nfoundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407, 2025.\n[59] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu,\nJiahao Qiu, Xuan Qi, Yiran Wu, et al. A survey of self-evolving agents: On path to artificial super\nintelligence. arXiv preprint arXiv:2507.21046, 2025.\n[60] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\n[61] Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun,\nand Jiawei Han. Deepretrieval: Hacking real search engines and retrievers with large language models\nvia reinforcement learning. arXiv preprint arXiv:2503.00223, 2025.\n[62] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[63] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300, 2024.\n78\n"}, {"page": 79, "text": "Agentic Reasoning for Large Language Models\n[64] Fanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Arpo: End-to-end policy optimization\nfor gui agents with experience replay. arXiv preprint arXiv:2505.16282, 2025.\n[65] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan,\nGaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale.\narXiv preprint arXiv:2503.14476, 2025.\n[66] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,\nShaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang.\nAutogen: Enabling next-gen LLM applications via multi-agent conversations. In First Conference on\nLanguage Modeling, 2024. URL https://openreview.net/forum?id=BAakY1hNKS.\n[67] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Commu-\nnicative agents for\" mind\" exploration of large language model society. Advances in Neural Information\nProcessing Systems, 36:51991–52008, 2023.\n[68] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmid-\nhuber. Gptswarm: Language agents as optimizable graphs. In Forty-first International Conference on\nMachine Learning, 2024.\n[69] Haoyang Hong, Jiajun Yin, Yuan Wang, Jingnan Liu, Zhe Chen, Ailing Yu, Ji Li, Zhiling Ye, Hansong\nXiao, Yefei Chen, et al. Multi-agent deep research: Training multi-agent systems with m-grpo. arXiv\npreprint arXiv:2511.13288, 2025.\n[70] Alexander Novikov, Ngân V˜u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner,\nSergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: A\ncoding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025.\n[71] Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. REWOO:\nDecoupling reasoning from observations for efficient augmented language models. arXiv preprint\narXiv:2305.18323, 2023.\n[72] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone.\nLLM+P: Empowering large language models with optimal planning proficiency.\narXiv preprint\narXiv:2304.11477, 2023.\n[73] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the\nplanning abilities of large language models: A critical investigation. Advances in Neural Information\nProcessing Systems, 36:75993–76005, 2023.\n[74] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi,\nJoanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts:\nSolving elaborate problems with large language models. In Proceedings of the AAAI conference on\nartificial intelligence, volume 38, pages 17682–17690, 2024.\n[75] Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin. Algorithm of thoughts:\nEnhancing exploration of ideas in large language models. arXiv preprint arXiv:2308.10379, 2023.\n[76] Runquan Gui, Zhihai Wang, Jie Wang, Chi Ma, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Defu Lian,\nEnhong Chen, and Feng Wu. Hypertree planning: Enhancing llm reasoning via hierarchical thinking.\narXiv preprint arXiv:2505.02322, 2025.\n79\n"}, {"page": 80, "text": "Agentic Reasoning for Large Language Models\n[77] Jihwan Jeong, Xiaoyu Wang, Jingmin Wang, Scott Sanner, and Pascal Poupart. Reflect-then-plan:\nOffline model-based planning through a doubly bayesian lens. arXiv preprint arXiv:2506.06261, 2025.\n[78] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model\nconnected with massive apis. Advances in Neural Information Processing Systems, 37:126544–126565,\n2024.\n[79] Tanmay Gupta, Luca Weihs, and Aniruddha Kembhavi. Codenav: Beyond tool-use to using real-world\ncodebases with llm agents. arXiv preprint arXiv:2406.12276, 2024.\n[80] Liyi Chen, Panrong Tong, Zhongming Jin, Ying Sun, Jieping Ye, and Hui Xiong. Plan-on-graph:\nSelf-correcting adaptive planning of large language model on knowledge graphs. Advances in Neural\nInformation Processing Systems, 37:37665–37691, 2024.\n[81] Yanming Liu, Xinyue Peng, Jiannan Cao, Yuwei Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang,\nJianwei Yin, and Tianyu Du. Tool-planner: Task planning with clusters across multiple tools. In The\nThirteenth International Conference on Learning Representations, 2025. URL https://openreview.\nnet/forum?id=dRz3cizftU.\n[82] Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B Tenenbaum, Tom Silver, João F\nHenriques, and Kevin Ellis. Visualpredicator: Learning abstract world models with neuro-symbolic\npredicates for robot planning. arXiv preprint arXiv:2410.23156, 2024.\n[83] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. Llm-\nplanner: Few-shot grounded planning for embodied agents with large language models. In Proceedings\nof the IEEE/CVF international conference on computer vision, pages 2998–3009, 2023.\n[84] Tamer Abuelsaad, Deepak Akkil, Prasenjit Dey, Ashish Jagmohan, Aditya Vempaty, and Ravi Kokku.\nAgent-e: From autonomous web navigation to foundational design principles in agentic systems. arXiv\npreprint arXiv:2407.13032, 2024.\n[85] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open\nagentic framework that uses computers like a human. arXiv preprint arXiv:2410.08164, 2024.\n[86] Minjong Yoo, Jinwoo Jang, Wei-Jin Park, and Honguk Woo. Exploratory retrieval-augmented planning\nfor continual embodied instruction following. Advances in Neural Information Processing Systems, 37:\n67034–67060, 2024.\n[87] Rohan Sinha, Amine Elhafsi, Christopher Agia, Matthew Foutter, Edward Schmerling, and Marco\nPavone. Real-time anomaly detection and reactive planning with large language models. arXiv preprint\narXiv:2407.08735, 2024.\n[88] Cristina Cornelio, Flavio Petruzzellis, and Pietro Lio. Hierarchical planning for complex tasks with\nknowledge graph-rag and symbolic verification. arXiv preprint arXiv:2504.04578, 2025.\n[89] Zikang Zhou, HU Haibo, Xinhong Chen, Jianping Wang, Nan Guan, Kui Wu, Yung-Hui Li, Yu-Kai\nHuang, and Chun Jason Xue. Behaviorgpt: Smart agent simulation for autonomous driving with\nnext-patch prediction. Advances in Neural Information Processing Systems, 37:79597–79617, 2024.\n[90] Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained\nvisual features enable zero-shot planning. arXiv preprint arXiv:2411.04983, 2024.\n80\n"}, {"page": 81, "text": "Agentic Reasoning for Large Language Models\n[91] Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, and Lin Shao. Flip: Flow-centric generative\nplanning as general-purpose manipulation world model. arXiv preprint arXiv:2412.08261, 2024.\n[92] Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma,\nAdithya Samavedhi, Qiyue Gao, et al. LLM reasoners: New evaluation, library, and analysis of\nstep-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221, 2024.\n[93] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\nPlan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 2609–2634, 2023.\n[94] Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Plan,\nverify and switch: Integrated reasoning with diverse x-of-thoughts. arXiv preprint arXiv:2310.14628,\n2023.\n[95] Fei Ni, Jianye Hao, Shiguang Wu, Longxin Kou, Yifu Yuan, Zibin Dong, Jinyi Liu, MingZhi Li, Yuzheng\nZhuang, and Yan Zheng. Peria: Perceive, reason, imagine, act via holistic language and vision planning\nfor manipulation. Advances in Neural Information Processing Systems, 37:17541–17571, 2024.\n[96] Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anumanchipalli,\nKurt Keutzer, and Amir Gholami. Plan-and-act: Improving planning of agents for long-horizon tasks.\narXiv preprint arXiv:2503.09572, 2025.\n[97] Jiaxin Wen, Jian Guan, Hongning Wang, Wei Wu, and Minlie Huang. Codeplan: Unlocking reasoning\npotential in large language models by scaling code-form planning. In The Thirteenth International\nConference on Learning Representations, 2024.\n[98] Michael Lutz, Arth Bohra, Manvel Saroyan, Artem Harutyunyan, and Giovanni Campagna. Wilbur:\nAdaptive in-context learning for robust and accurate web agents. arXiv preprint arXiv:2404.05902,\n2024.\n[99] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable\ncode actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024.\n[100] Asif Rahman, Veljko Cvetkovic, Kathleen Reece, Aidan Walters, Yasir Hassan, Aneesh Tummeti, Bryan\nTorres, Denise Cooney, Margaret Ellis, and Dimitrios S Nikolopoulos. Marco: Multi-agent code\noptimization with real-time knowledge integration for high-performance computing. arXiv preprint\narXiv:2505.03906, 2025.\n[101] Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen, Junliang Xing, and Huimin Ma. Enhancing llm\nreasoning with multi-path collaborative reactive and reflection agents. arXiv preprint arXiv:2501.00430,\n2024.\n[102] Mrinal Rawat, Ambuje Gupta, Rushil Goomer, Alessandro Di Bari, Neha Gupta, and Roberto Pier-\naccini. Pre-act: Multi-step planning and reasoning improves acting in llm agents. arXiv preprint\narXiv:2505.09970, 2025.\n[103] Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary\nFisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, et al. Rest meets react: Self-improvement for\nmulti-step reasoning llm agent. arXiv preprint arXiv:2312.10003, 2023.\n81\n"}, {"page": 82, "text": "Agentic Reasoning for Large Language Models\n[104] Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, and Wenpin Jiao.\nSelf-planning code generation with large language models. ACM Transactions on Software Engineering\nand Methodology, 33(7):1–30, 2024.\n[105] Dhruv Shah, Błażej Osiński, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-trained\nmodels of language, vision, and action. In Conference on robot learning, pages 492–504. PMLR, 2023.\n[106] Elan Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-\nWei Chang, and Aram Galstyan. Tree-of-traversals: A zero-shot reasoning algorithm for augmenting\nblack-box language models with knowledge graphs. arXiv preprint arXiv:2407.21358, 2024.\n[107] Jieyi Long. Large language model guided tree-of-thought. arXiv preprint arXiv:2305.08291, 2023.\n[108] Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language\nmodel agents. arXiv preprint arXiv:2407.01476, 2024.\n[109] Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. Q*:\nImproving multi-step reasoning for llms with deliberative planning. arXiv preprint arXiv:2406.14283,\n2024.\n[110] Silin Meng, Yiwei Wang, Cheng-Fu Yang, Nanyun Peng, and Kai-Wei Chang. Llm-a*: Large language\nmodel enhanced incremental heuristic search on path planning. arXiv preprint arXiv:2407.02511,\n2024.\n[111] Gang Liu, Michael Sun, Wojciech Matusik, Meng Jiang, and Jie Chen. Multimodal large language\nmodels for inverse molecular design with retrosynthetic planning. arXiv preprint arXiv:2410.04223,\n2024.\n[112] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.\nReasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992,\n2023.\n[113] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and\nRafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint\narXiv:2408.07199, 2024.\n[114] Henry W Sprueill, Carl Edwards, Mariefel V Olarte, Udishnu Sanyal, Heng Ji, and Sutanay Choudhury.\nMonte carlo thought search: Large language model querying for complex scientific reasoning in\ncatalyst design. arXiv preprint arXiv:2310.14420, 2023.\n[115] Xiao Yu, Maximillian Chen, and Zhou Yu. Prompt-based monte-carlo tree search for goal-oriented\ndialogue policy planning. arXiv preprint arXiv:2305.13660, 2023.\n[116] Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for\nlarge-scale task planning. Advances in neural information processing systems, 36:31967–31987, 2023.\n[117] Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan\nRajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose\ntriangle for thought generation. arXiv preprint arXiv:2311.04254, 2023.\n[118] Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search\nuseful for llm planning? it depends on the discriminator. arXiv preprint arXiv:2402.10890, 2024.\n82\n"}, {"page": 83, "text": "Agentic Reasoning for Large Language Models\n[119] Deqian Kong, Dehong Xu, Minglu Zhao, Bo Pang, Jianwen Xie, Andrew Lizarraga, Yuhao Huang, Sirui\nXie, and Ying Nian Wu. Latent plan transformer for trajectory abstraction: Planning as latent space\ninference. Advances in Neural Information Processing Systems, 37:123379–123401, 2024.\n[120] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun\nWang. Alphazero-like tree-search can guide large language model decoding and training. arXiv\npreprint arXiv:2309.17179, 2023.\n[121] Jaesik Yoon, Hyeonseo Cho, Doojin Baek, Yoshua Bengio, and Sungjin Ahn. Monte carlo tree diffusion\nfor system 2 planning. arXiv preprint arXiv:2502.07202, 2025.\n[122] John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel\nHennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, et al. Mastering board games by external and\ninternal planning with language models. arXiv preprint arXiv:2412.12119, 2024.\n[123] Zhiliang Chen, Xinyuan Niu, Chuan-Sheng Foo, and Bryan Kian Hsiang Low. Broaden your scope! effi-\ncient multi-turn conversation planning for llms with semantic space. arXiv preprint arXiv:2503.11586,\n2025.\n[124] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie.\nSelf-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems,\n36:41618–41650, 2023.\n[125] Olga Golovneva, Sean O’Brien, Ramakanth Pasunuru, Tianlu Wang, Luke Zettlemoyer, Maryam Fazel-\nZarandi, and Asli Celikyilmaz. Pathfinder: Guided search over multi-step reasoning paths. arXiv\npreprint arXiv:2312.05180, 2023.\n[126] Haofu Qian, Chenjia Bai, Jiatao Zhang, Fei Wu, Wei Song, and Xuelong Li. Discriminator-guided em-\nbodied planning for llm agent. In The Thirteenth International Conference on Learning Representations,\n2025.\n[127] Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D\nGoodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683,\n2024.\n[128] Swarnadeep Saha, Archiki Prasad, Justin Chih-Yao Chen, Peter Hase, Elias Stengel-Eskin, and Mohit\nBansal. System-1. x: Learning to balance fast and slow planning with language models. arXiv preprint\narXiv:2407.14414, 2024.\n[129] Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu Wang, Feiyue Ni, Ruihua Song, Longfei Li, Jinjie Gu,\nand Chenyi Zhuang. Intelligent virtual assistants with llm-based process automation. arXiv preprint\narXiv:2312.06677, 2023.\n[130] Junjie Chen, Haitao Li, Jingli Yang, Yiqun Liu, and Qingyao Ai. Enhancing llm-based agents via global\nplanning and hierarchical execution. arXiv preprint arXiv:2504.16563, 2025.\n[131] Zican Hu, Wei Liu, Xiaoye Qu, Xiangyu Yue, Chunlin Chen, Zhi Wang, and Yu Cheng. Divide and\nconquer: Grounding llms as efficient decision-making agents via offline hierarchical reinforcement\nlearning. arXiv preprint arXiv:2505.19761, 2025.\n83\n"}, {"page": 84, "text": "Agentic Reasoning for Large Language Models\n[132] Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, and William Wang. Swe-\nsearch: Enhancing software agents with monte carlo tree search and iterative refinement. arXiv\npreprint arXiv:2410.20285, 2024.\n[133] Artem Lykov and Dzmitry Tsetserukou. Llm-brain: Ai-driven fast generation of robot behaviour\ntree based on large language model. In 2024 2nd International Conference on Foundation and Large\nLanguage Models (FLLM), pages 392–397. IEEE, 2024.\n[134] Yue Cao and CS Lee. Robot behavior-tree-based task generation with large language models. arXiv\npreprint arXiv:2302.12927, 2023.\n[135] Riccardo Andrea Izzo, Gianluca Bardaro, and Matteo Matteucci. Btgenbot: Behavior tree generation\nfor robotic tasks with lightweight llms. In 2024 IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pages 9684–9690. IEEE, 2024.\n[136] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n[137] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022.\n[138] Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pre-\ntrained large language models to construct and utilize world models for model-based task planning.\nAdvances in Neural Information Processing Systems, 36:79081–79094, 2023.\n[139] Sadegh Mahdavi, Raquel Aoki, Keyi Tang, and Yanshuai Cao. Leveraging environment interaction for\nautomated pddl translation and planning with large language models. Advances in Neural Information\nProcessing Systems, 37:38960–39008, 2024.\n[140] Michael Katz, Harsha Kokel, Kavitha Srinivas, and Shirin Sohrabi Araghi. Thought of search: Planning\nwith language models through the lens of efficiency. Advances in Neural Information Processing Systems,\n37:138491–138568, 2024.\n[141] Yilun Hao, Yang Zhang, and Chuchu Fan. Planning anything with rigor: General-purpose zero-shot\nplanning with llm-based formalized programming. arXiv preprint arXiv:2410.12112, 2024.\n[142] Kaustubh Vyas, Damien Graux, Yijun Yang, Sébastien Montella, Chenxin Diao, Wendi Zhou, Pavlos\nVougiouklis, Ruofei Lai, Yang Ren, Keshuang Li, et al. From an llm swarm to a pddl-empowered hive:\nPlanning self-executed instructions in a multi-modal jungle. arXiv preprint arXiv:2412.12839, 2024.\n[143] Yuji Zhang, Qingyun Wang, Cheng Qian, Jiateng Liu, Chenkai Sun, Denghui Zhang, Tarek Abdelzaher,\nChengxiang Zhai, Preslav Nakov, and Heng Ji. Atomic reasoning for scientific table claim verification.\narXiv preprint arXiv:2506.06972, 2025.\n[144] Zibin Dong, Jianye Hao, Yifu Yuan, Fei Ni, Yitian Wang, Pengyi Li, and Yan Zheng. Diffuserlite:\nTowards real-time diffusion planning. Advances in Neural Information Processing Systems, 37:122556–\n122583, 2024.\n84\n"}, {"page": 85, "text": "Agentic Reasoning for Large Language Models\n[145] Chunlok Lo, Kevin Roice, Parham Mohammad Panahi, Scott M Jordan, Adam White, Gabor Mihucz,\nFarzane Aminmansour, and Martha White. Goal-space planning with subgoal models. Journal of\nMachine Learning Research, 25(330):1–57, 2024.\n[146] Ao Li, Yuexiang Xie, Songze Li, Fugee Tsung, Bolin Ding, and Yaliang Li. Agent-oriented planning in\nmulti-agent systems. arXiv preprint arXiv:2410.02189, 2024.\n[147] Mianchu Wang, Rui Yang, Xi Chen, Hao Sun, Meng Fang, and Giovanni Montana.\nGoplan:\nGoal-conditioned offline reinforcement learning by planning with learned models. arXiv preprint\narXiv:2310.20025, 2023.\n[148] Chenglong Kang, Xiaoyi Liu, and Fei Guo. Retrointext: A multimodal large language model enhanced\nframework for retrosynthetic planning via in-context representation learning. In The Thirteenth\nInternational Conference on Learning Representations, 2025.\n[149] Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong.\nBeyond autoregression: Discrete diffusion for complex reasoning and planning.\narXiv preprint\narXiv:2410.14157, 2024.\n[150] Yupeng Zheng, Zebin Xing, Qichao Zhang, Bu Jin, Pengfei Li, Yuhang Zheng, Zhongpu Xia, Kun Zhan,\nXianpeng Lang, Yaran Chen, et al. Planagent: A multi-modal large language agent for closed-loop\nvehicle motion planning. arXiv preprint arXiv:2406.01587, 2024.\n[151] Sid Nayak, Adelmo Morrison Orozco, Marina Have, Jackson Zhang, Vittal Thirumalai, Darren Chen,\nAditya Kapoor, Eric Robinson, Karthik Gopalakrishnan, James Harrison, et al. Long-horizon planning\nfor multi-agent robots in partially observable environments. Advances in Neural Information Processing\nSystems, 37:67929–67967, 2024.\n[152] Tianxin Wei, Ruizhong Qiu, Yifan Chen, Yunzhe Qi, Jiacheng Lin, Wenju Xu, Sreyashi Nag, Ruirui Li,\nHanqing Lu, Zhengyang Wang, Chen Luo, Hui Liu, Suhang Wang, Jingrui He, Qi He, and Xianfeng\nTang. Robust watermarking for diffusion models: A unified multi-dimensional recipe, 2024.\n[153] Wenxuan Bao, Ruxi Deng, Ruizhong Qiu, Tianxin Wei, Hanghang Tong, and Jingrui He. Latte:\nCollaborative test-time adaptation of vision-language models in federated learning. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, 2025.\n[154] Lingjie Chen, Ruizhong Qiu, Siyu Yuan, Zhining Liu, Tianxin Wei, Hyunsik Yoo, Zhichen Zeng, Deqing\nYang, and Hanghang Tong. WAPITI: A watermark for finetuned open-source LLMs, 2024.\n[155] Zhining Liu, Ze Yang, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Yada Zhu, Hendrik Hamann, Jingrui He,\nand Hanghang Tong. Breaking silos: Adaptive model fusion unlocks better time series forecasting. In\nProceedings of the 42nd International Conference on Machine Learning, 2025.\n[156] Lihui Liu, Zihao Wang, Ruizhong Qiu, Yikun Ban, Eunice Chan, Yangqiu Song, Jingrui He, and\nHanghang Tong. Logic query of thoughts: Guiding large language models to answer complex logic\nqueries with knowledge graphs, 2024.\n[157] Zhining Liu, Ruizhong Qiu, Zhichen Zeng, Hyunsik Yoo, David Zhou, Zhe Xu, Yada Zhu, Kommy\nWeldemariam, Jingrui He, and Hanghang Tong. Class-imbalanced graph learning without class\nrebalancing. In Proceedings of the 41st International Conference on Machine Learning, 2024.\n85\n"}, {"page": 86, "text": "Agentic Reasoning for Large Language Models\n[158] Zhining Liu, Ruizhong Qiu, Zhichen Zeng, Yada Zhu, Hendrik Hamann, and Hanghang Tong. AIM:\nAttributing, interpreting, mitigating data unfairness. In Proceedings of the 30th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, pages 2014–2025, 2024.\n[159] Zhining Liu, Zhichen Zeng, Ruizhong Qiu, Hyunsik Yoo, David Zhou, Zhe Xu, Yada Zhu, Kommy\nWeldemariam, Jingrui He, and Hanghang Tong. Topological augmentation for class-imbalanced node\nclassification, 2023.\n[160] Zhichen Zeng, Ruizhong Qiu, Wenxuan Bao, Tianxin Wei, Xiao Lin, Yuchen Yan, Tarek F. Abdelzaher,\nJiawei Han, and Hanghang Tong. Pave your own path: Graph gradual domain adaptation on fused\nGromov–Wasserstein geodesics, 2025.\n[161] Zhichen Zeng, Ruizhong Qiu, Zhe Xu, Zhining Liu, Yuchen Yan, Tianxin Wei, Lei Ying, Jingrui He,\nand Hanghang Tong. Graph mixup on approximate Gromov–Wasserstein geodesics. In Proceedings of\nthe 41st International Conference on Machine Learning, 2024.\n[162] Xiao Lin, Zhining Liu, Ze Yang, Gaotang Li, Ruizhong Qiu, Shuke Wang, Hui Liu, Haotian Li, Sumit\nKeswani, Vishwa Pardeshi, et al. Moralise: A structured benchmark for moral alignment in visual\nlanguage models, 2025.\n[163] Xiao Lin, Zhining Liu, Dongqi Fu, Ruizhong Qiu, and Hanghang Tong. BackTime: Backdoor attacks on\nmultivariate time series forecasting. In Advances in Neural Information Processing Systems, volume 37,\n2024.\n[164] Ruizhong Qiu, Gaotang Li, Tianxin Wei, Jingrui He, and Hanghang Tong. Saffron-1: Safety inference\nscaling, 2025.\n[165] Ruizhong Qiu, Zhe Xu, Wenxuan Bao, and Hanghang Tong. Ask, and it shall be given: On the Turing\ncompleteness of prompting. In 13th International Conference on Learning Representations, 2025.\n[166] Ruizhong Qiu, Weiliang Will Zeng, Hanghang Tong, James Ezick, and Christopher Lott. How efficient\nis LLM-generated code? A rigorous & high-standard benchmark. In 13th International Conference on\nLearning Representations, 2025.\n[167] Ruizhong Qiu, Jun-Gi Jang, Xiao Lin, Lihui Liu, and Hanghang Tong. TUCKET: A tensor time series\ndata structure for efficient and accurate factor analysis over time ranges. Proceedings of the VLDB\nEndowment, 17(13), 2024.\n[168] Ruizhong Qiu, Dingsu Wang, Lei Ying, H Vincent Poor, Yifang Zhang, and Hanghang Tong. Recon-\nstructing graph diffusion history from a single snapshot. In Proceedings of the 29th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining, pages 1978–1988, 2023.\n[169] Ruizhong Qiu, Zhiqing Sun, and Yiming Yang. DIMES: A differentiable meta solver for combinatorial\noptimization problems. In Advances in Neural Information Processing Systems, volume 35, pages\n25531–25546, 2022.\n[170] Zhe Xu, Ruizhong Qiu, Yuzhong Chen, Huiyuan Chen, Xiran Fan, Menghai Pan, Zhichen Zeng,\nMahashweta Das, and Hanghang Tong. Discrete-state continuous-time diffusion for graph generation.\nIn Advances in Neural Information Processing Systems, volume 37, 2024.\n[171] Ting-Wei Li, Ruizhong Qiu, and Hanghang Tong. Model-free graph data selection under distribution\nshift, 2025.\n86\n"}, {"page": 87, "text": "Agentic Reasoning for Large Language Models\n[172] Jiaru Zou, Yikun Ban, Zihao Li, Yunzhe Qi, Ruizhong Qiu, Ling Yang, and Jingrui He. Transformer\ncopilot: Learning from the mistake log in llm fine-tuning, 2025. URL https://arxiv.org/abs/\n2505.16270.\n[173] Ruizhong Qiu and Hanghang Tong. Gradient compressed sensing: A query-efficient gradient estimator\nfor high-dimensional zeroth-order optimization. In Proceedings of the 41st International Conference on\nMachine Learning, 2024.\n[174] Hyunsik Yoo, SeongKu Kang, Ruizhong Qiu, Charlie Xu, Fei Wang, and Hanghang Tong. Embracing\nplasticity: Balancing stability and plasticity in continual recommender systems. In Proceedings of the\n48th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2025.\n[175] Hyunsik Yoo, Ruizhong Qiu, Charlie Xu, Fei Wang, and Hanghang Tong. Generalizable recommender\nsystem during temporal popularity distribution shifts. In Proceedings of the 31st ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, 2025.\n[176] Hyunsik Yoo, Zhichen Zeng, Jian Kang, Ruizhong Qiu, David Zhou, Zhining Liu, Fei Wang, Charlie\nXu, Eunice Chan, and Hanghang Tong. Ensuring user-side fairness in dynamic recommender systems.\nIn Proceedings of the ACM on Web Conference 2024, pages 3667–3678, 2024.\n[177] Eunice Chan, Zhining Liu, Ruizhong Qiu, Yuheng Zhang, Ross Maciejewski, and Hanghang Tong.\nGroup fairness via group consensus. In The 2024 ACM Conference on Fairness, Accountability, and\nTransparency, pages 1788–1808, 2024.\n[178] Ziwei Wu, Lecheng Zheng, Yuancheng Yu, Ruizhong Qiu, John Birge, and Jingrui He. Fair anomaly\ndetection for imbalanced groups, 2024.\n[179] Xinyu He, Jian Kang, Ruizhong Qiu, Fei Wang, Jose Sepulveda, and Hanghang Tong. On the sensitivity\nof individual fairness: Measures and robust algorithms. In Proceedings of the 33rd ACM International\nConference on Information and Knowledge Management, pages 829–838, 2024.\n[180] Dingsu Wang, Yuchen Yan, Ruizhong Qiu, Yada Zhu, Kaiyu Guan, Andrew Margenot, and Hanghang\nTong. Networked time series imputation via position-aware graph enhanced variational autoencoders.\nIn Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages\n2256–2268, 2023.\n[181] Yue Meng and Chuchu Fan. Telograf: Temporal logic planning via graph-encoded flow matching.\narXiv preprint arXiv:2505.00562, 2025.\n[182] Ruizhe Zhong, Xingbo Du, Shixiong Kai, Zhentao Tang, Siyuan Xu, Jianye Hao, Mingxuan Yuan, and\nJunchi Yan. Flexplanner: Flexible 3d floorplanning via deep reinforcement learning in hybrid action\nspace with multi-modality representation. Advances in Neural Information Processing Systems, 37:\n49252–49278, 2024.\n[183] Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao\nZheng, Philip S Yu, Fei Huang, et al. Benchmarking multimodal retrieval augmented generation with\ndynamic vqa dataset and self-adaptive planning agent. arXiv preprint arXiv:2411.02937, 2024.\n[184] Jiaru Zou, Dongqi Fu, Sirui Chen, Xinrui He, Zihao Li, Yada Zhu, Jiawei Han, and Jingrui He.\nRag over tables: Hierarchical memory index, multi-stage retrieval, and benchmarking, 2025. URL\nhttps://arxiv.org/abs/2504.01346.\n87\n"}, {"page": 88, "text": "Agentic Reasoning for Large Language Models\n[185] Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun\nXie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural\nInformation Processing Systems, 37:114843–114871, 2024.\n[186] Zichen Liu, Guoji Fu, Chao Du, Wee Sun Lee, and Min Lin. Continual reinforcement learning by\nplanning with online world models. arXiv preprint arXiv:2507.09177, 2025.\n[187] Hang Wang, Xin Ye, Feng Tao, Chenbin Pan, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, and\nJunshan Zhang. Adawm: Adaptive world model based planning for autonomous driving. arXiv preprint\narXiv:2501.13072, 2025.\n[188] Yining Ye, Xin Cong, Shizuo Tian, Yujia Qin, Chong Liu, Yankai Lin, Zhiyuan Liu, and Maosong Sun.\nRational decision-making agent with internalized utility judgment. arXiv preprint arXiv:2308.12519,\n2023.\n[189] Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, and Chuang Gan. Scaling autonomous agents via\nautomatic reward modeling and planning. arXiv preprint arXiv:2502.12130, 2025.\n[190] Max Ruiz Luyten, Antonin Berthon, and Mihaela van der Schaar. Strategic planning: A top-down\napproach to option generation. In Forty-second International Conference on Machine Learning, 2025.\n[191] Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. Non-myopic generation of\nlanguage models for reasoning and planning. arXiv preprint arXiv:2410.17195, 2024.\n[192] Ruiqi Ni, Zherong Pan, and Ahmed H Qureshi. Physics-informed temporal difference metric learning\nfor robot motion planning. arXiv preprint arXiv:2505.05691, 2025.\n[193] Sharath Matada, Luke Bhan, Yuanyuan Shi, and Nikolay Atanasov. Generalizable motion planning\nvia operator learning. arXiv preprint arXiv:2410.17547, 2024.\n[194] Hongjin Su, Shizhe Diao, Ximing Lu, Mingjie Liu, Jiacheng Xu, Xin Dong, Yonggan Fu, Peter Belcak,\nHanrong Ye, Hongxu Yin, Yi Dong, Evelina Bakhturina, Tao Yu, Yejin Choi, Jan Kautz, and Pavlo\nMolchanov. Toolorchestra: Elevating intelligence via efficient model and tool orchestration, 2025.\nURL https://arxiv.org/abs/2511.21689.\n[195] Amber Xie, Oleh Rybkin, Dorsa Sadigh, and Chelsea Finn. Latent diffusion planning for imitation\nlearning. arXiv preprint arXiv:2504.16925, 2025.\n[196] Wei Xiao, Tsun-Hsuan Wang, Chuang Gan, Ramin Hasani, Mathias Lechner, and Daniela Rus. Safedif-\nfuser: Safe planning with diffusion probabilistic models. In The Thirteenth International Conference on\nLearning Representations, 2023.\n[197] Yixiang Shan, Zhengbang Zhu, Ting Long, Liang Qifan, Yi Chang, Weinan Zhang, and Liang Yin.\nContradiff: Planning towards high return states via contrastive learning. In The Thirteenth International\nConference on Learning Representations, 2025.\n[198] Anian Ruoss, Grégoire Delétang, Sourabh Medapati, Jordi Grau-Moya, Li K Wenliang, Elliot Catt,\nJohn Reid, Cannada A Lewis, Joel Veness, and Tim Genewein. Amortized planning with large-\nscale transformers: A case study on chess. Advances in Neural Information Processing Systems, 37:\n65765–65790, 2024.\n88\n"}, {"page": 89, "text": "Agentic Reasoning for Large Language Models\n[199] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and\nMarco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models,\n2023. URL https://arxiv.org/abs/2303.09014.\n[200] Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Xin Zhao, and Ji-Rong Wen. ChatCoT:\nTool-augmented chain-of-thought reasoning on chat-based large language models. In Houda Bouamor,\nJuan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP\n2023, pages 14777–14790, Singapore, December 2023. Association for Computational Linguis-\ntics.\ndoi: 10.18653/v1/2023.findings-emnlp.985.\nURL https://aclanthology.org/2023.\nfindings-emnlp.985/.\n[201] Yining Lu, Haoping Yu, and Daniel Khashabi. GEAR: Augmenting language models with generalizable\nand efficient tool resolution. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th\nConference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 112–138, St. Julian’s, Malta, March 2024. Association for Computational Linguistics.\ndoi: 10.18653/v1/2024.eacl-long.7. URL https://aclanthology.org/2024.eacl-long.7/.\n[202] Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N.\nIoannidis, Karthik Subbian, Jure Leskovec, and James Zou. Avatar: Optimizing llm agents for tool\nusage via contrastive reasoning. In Advances in Neural Information Processing Systems, volume 37,\npages 25981–26010. Curran Associates, Inc., 2024.\n[203] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein,\nDahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master\n16000+ real-world apis. In The Twelfth International Conference on Learning Representations, ICLR\n2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/\nforum?id=dHng2O0Jjr.\n[204] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized\ntool learning for language models with 3000 simulated cases. CoRR, abs/2306.05301, 2023. doi:\n10.48550/ARXIV.2306.05301. URL https://doi.org/10.48550/arXiv.2306.05301.\n[205] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen\nZhang, Huajun Chen, Fan Yang, et al. Learning to reason with search for llms via reinforcement\nlearning. arXiv preprint arXiv:2503.19470, 2025.\n[206] Qingxiu Dong, Li Dong, Yao Tang, Tianzhu Ye, Yutao Sun, Zhifang Sui, and Furu Wei. Reinforcement\npre-training. arXiv preprint arXiv:2506.08007, 2025.\n[207] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur,\nand Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025.\n[208] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing\ntasks by connecting foundation models with millions of apis. CoRR, abs/2303.16434, 2023. doi:\n10.48550/ARXIV.2303.16434. URL https://doi.org/10.48550/arXiv.2303.16434.\n[209] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octotools: An agentic\nframework with extensible tools for complex reasoning, 2025. URL https://arxiv.org/abs/\n2502.11271.\n89\n"}, {"page": 90, "text": "Agentic Reasoning for Large Language Models\n[210] Zijing Zhang, Zhanpeng Chen, He Zhu, Ziyang Chen, Nan Du, and Xiaolong Li. Toolexpnet: Optimizing\nmulti-tool selection in llms with similarity and dependency-aware experience networks. In Wanxiang\nChe, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the\nAssociation for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages\n15706–15722. Association for Computational Linguistics, 2025. URL https://aclanthology.\norg/2025.findings-acl.811/.\n[211] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor S. Bursztyn, Ryan A. Rossi, Somdeb\nSarkhel, and Chao Zhang. Toolchain*: Efficient action space navigation in large language models with\na* search. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna,\nAustria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=\nB6pQxqUcT8.\n[212] Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao Kurohashi. MultiTool-CoT: GPT-3 can use\nmultiple external tools with chain of thought prompting. In Anna Rogers, Jordan Boyd-Graber, and\nNaoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 1522–1532, Toronto, Canada, July 2023. Association for\nComputational Linguistics. doi: 10.18653/v1/2023.acl-short.130. URL https://aclanthology.\norg/2023.acl-short.130/.\n[213] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving re-\ntrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint\narXiv:2212.10509, 2022.\n[214] Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay\nKrishna, and Tomas Pfister. Tool documentation enables zero-shot tool-usage with large language\nmodels, 2023. URL https://arxiv.org/abs/2308.00675.\n[215] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and\nDeqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv preprint\narXiv:2401.06201, 2024.\n[216] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong\nWen. Tool learning with large language models: A survey. Frontiers of Computer Science, 19(8):\n198343, 2025.\n[217] Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng, Xiuyi Chen, Zhumin Chen, Dawei Yin, Suzan\nVerberne, and Zhaochun Ren. Tool learning in the wild: Empowering language models as automatic\ntool agents. In Proceedings of the ACM on Web Conference 2025, pages 2222–2237, 2025.\n[218] Hongru Wang, Yujia Qin, Yankai Lin, Jeff Z Pan, and Kam-Fai Wong. Empowering large language\nmodels: Tool learning for real-world interaction. In Proceedings of the 47th International ACM SIGIR\nConference on Research and Development in Information Retrieval, pages 2983–2986, 2024.\n[219] Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez,\nand Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. Advances\nin Neural Information Processing Systems, 37:113519–113544, 2024.\n[220] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu.\nSeeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935,\n2024.\n90\n"}, {"page": 91, "text": "Agentic Reasoning for Large Language Models\n[221] Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, and Mengdi Wang. Reasonflux-\nprm: Trajectory-aware prms for long chain-of-thought reasoning in llms, 2025. URL https://arxiv.\norg/abs/2506.18896.\n[222] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. Using an llm\nto help with code understanding. In Proceedings of the IEEE/ACM 46th International Conference on\nSoftware Engineering, pages 1–13, 2024.\n[223] Junde Wu, Jiayuan Zhu, Yuyuan Liu, Min Xu, and Yueming Jin. Agentic reasoning: A streamlined\nframework for enhancing llm reasoning with agentic tools. 2025. URL https://arxiv.org/abs/\n2502.04644.\n[224] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu,\nand Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models.\nAdvances in Neural Information Processing Systems, 36:43447–43478, 2023.\n[225] Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng\nLi, Ke Wang, Rong Yao, et al. Restgpt: Connecting large language models with real-world restful apis.\narXiv preprint arXiv:2306.06624, 2023.\n[226] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal,\nand Tushar Khot. Adapt: As-needed decomposition and planning with language models. arXiv preprint\narXiv:2311.05772, 2023.\n[227] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and\nBill Yuchen Lin. Agent lumos: Unified and modular training for open-source language agents. arXiv\npreprint arXiv:2311.05657, 2023.\n[228] Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Pengjie Ren,\nSuzan Verberne, and Zhaochun Ren. Learning to use tools via cooperative and interactive agents.\narXiv preprint arXiv:2403.03031, 2024.\n[229] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward\nGrefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and\ndiversity. arXiv preprint arXiv:2310.06452, 2023.\n[230] Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Zhi-Quan Luo, and Ruoyu Sun. Preserving\ndiversity in supervised fine-tuning of large language models. arXiv preprint arXiv:2408.16673, 2024.\n[231] Laura O’Mahony, Leo Grinsztajn, Hailey Schoelkopf, and Stella Biderman. Attributing mode collapse\nin the fine-tuning of large language models. In ICLR 2024 Workshop on Mathematical and Empirical\nUnderstanding of Foundation Models, volume 2, 2024.\n[232] Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Yutai Hou, Wu Ning, Xu Huang, Duyu Tang,\nDandan Tu, Bing Qin, et al. itool: Reinforced fine-tuning with dynamic deficiency calibration for\nadvanced tool use. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language\nProcessing, pages 13901–13916, 2025.\n[233] Zhaochen Yu, Ling Yang, Jiaru Zou, Shuicheng Yan, and Mengdi Wang. Demystifying reinforcement\nlearning in agentic reasoning. arXiv preprint arXiv:2510.11701, 2025.\n91\n"}, {"page": 92, "text": "Agentic Reasoning for Large Language Models\n[234] Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and\nXian Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint\narXiv:2503.15478, 2025.\n[235] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried,\nGabriel Synnaeve, Rishabh Singh, and Sida I Wang. Swe-rl: Advancing llm reasoning via reinforcement\nlearning on open software evolution. arXiv preprint arXiv:2502.18449, 2025.\n[236] Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li. Rlvmr: Reinforcement\nlearning with verifiable meta-reasoning rewards for robust long-horizon agents.\narXiv preprint\narXiv:2507.22844, 2025.\n[237] Jiaru Zou, Ling Yang, Yunzhe Qi, Sirui Chen, Mengting Ai, Ke Shen, Jingrui He, and Mengdi Wang.\nAutotool: Dynamic tool selection and integration for agentic reasoning, 2025. URL https://arxiv.\norg/abs/2512.13278.\n[238] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang,\nJinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv\npreprint arXiv:2504.11536, 2025.\n[239] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei\nHuang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching.\narXiv preprint arXiv:2505.04588, 2025.\n[240] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,\nChenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv\npreprint arXiv:2501.12599, 2025.\n[241] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon,\nMarcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with\nadvanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv\npreprint arXiv:2507.06261, 2025.\n[242] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru\nChen, Yuankun Chen, Yutian Chen, et al.\nKimi k2: Open agentic intelligence.\narXiv preprint\narXiv:2507.20534, 2025.\n[243] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin,\nHao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv\npreprint arXiv:2508.06471, 2025.\n[244] Jiaru Zou, Soumya Roy, Vinay Kumar Verma, Ziyi Wang, David Wipf, Pan Lu, Sumit Negi, James Zou,\nand Jingrui He. Tattoo: Tool-grounded thinking prm for test-time scaling in tabular reasoning. arXiv\npreprint arXiv:2510.06217, 2025.\n[245] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language\nmodels with massive tools via tool embeddings. In Alice Oh, Tristan Naumann, Amir Globerson, Kate\nSaenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems\n36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,\nLA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/\n2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html.\n92\n"}, {"page": 93, "text": "Agentic Reasoning for Large Language Models\n[246] Zhiyuan Ma, Jiayu Liu, Xianzhen Luo, Zhenya Huang, Qingfu Zhu, and Wanxiang Che. Advanc-\ning tool-augmented large language models via meta-verification and reflection learning. CoRR,\nabs/2506.04625, 2025. doi: 10.48550/ARXIV.2506.04625. URL https://doi.org/10.48550/\narXiv.2506.04625.\n[247] Mengsong Wu, Tong Zhu, Han Han, Xiang Zhang, Wenbiao Shao, and Wenliang Chen. Chain-\nof-tools: Utilizing massive unseen tools in the cot reasoning of frozen language models. CoRR,\nabs/2503.16779, 2025. doi: 10.48550/ARXIV.2503.16779. URL https://doi.org/10.48550/\narXiv.2503.16779.\n[248] Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, and Chen Wei.\nPyvision: Agentic vision with dynamic tooling. CoRR, abs/2507.07998, 2025. doi: 10.48550/ARXIV.\n2507.07998. URL https://doi.org/10.48550/arXiv.2507.07998.\n[249] Yunheng Zou, Austin H. Cheng, Abdulrahman Aldossary, Jiaru Bai, Shi Xuan Leong, Jorge A.\nCampos Gonzalez Angulo, Changhyeok Choi, Cher Tian Ser, Gary Tom, Andrew Wang, Zijian\nZhang, Ilya Yakavets, Han Hao, Chris Crebolder, Varinia Bernales, and Alán Aspuru-Guzik.\nEl\nagente: An autonomous agent for quantum chemistry.\nCoRR, abs/2505.02484, 2025.\ndoi:\n10.48550/ARXIV.2505.02484. URL https://doi.org/10.48550/arXiv.2505.02484.\n[250] Xing Cui, Yueying Zou, Zekun Li, Pei-Pei Li, Xinyuan Xu, Xuannan Liu, Huaibo Huang, and Ran\nHe. Tˆ2agent A tool-augmented multimodal misinformation detection agent with monte carlo tree\nsearch. CoRR, abs/2505.19768, 2025. doi: 10.48550/ARXIV.2505.19768. URL https://doi.org/\n10.48550/arXiv.2505.19768.\n[251] Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, and Bin Wang. Toolrerank: Adaptive and\nhierarchy-aware reranking for tool retrieval. In Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste,\nAlessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint Interna-\ntional Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING\n2024, 20-25 May, 2024, Torino, Italy, pages 16263–16273. ELRA and ICCL, 2024. URL https:\n//aclanthology.org/2024.lrec-main.1413.\n[252] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459–9474,\n2020.\n[253] Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Gui,\nZiran Jiang, Ziyu Jiang, et al. Crag-comprehensive rag benchmark. Advances in Neural Information\nProcessing Systems, 37:10470–10490, 2024.\n[254] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.\n[255] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Self-reflective\nretrieval augmented generation. In NeurIPS 2023 workshop on instruction tuning and instruction\nfollowing, 2023.\n[256] Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun,\nand Jie Zhou. Deeprag: Thinking to retrieve step by step for large language models. arXiv preprint\narXiv:2502.01142, 2025.\n93\n"}, {"page": 94, "text": "Agentic Reasoning for Large Language Models\n[257] Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen, Binyu Xie, Zheng Liu, Ji-Rong Wen, and\nZhicheng Dou. Inters: Unlocking the power of large language models in search with instruction\ntuning. arXiv preprint arXiv:2401.06532, 2024.\n[258] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering\nwith human feedback. arXiv preprint arXiv:2112.09332, 2021.\n[259] Jerry Huang, Siddarth Madala, Risham Sidhu, Cheng Niu, Hao Peng, Julia Hockenmaier, and Tong\nZhang. Rag-rl: Advancing retrieval-augmented generation via rl and curriculum learning. arXiv\npreprint arXiv:2503.12759, 2025.\n[260] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.\nDeepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv\npreprint arXiv:2504.03160, 2025.\n[261] Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Yang\nSong, and Han Li. Rearter: Retrieval-augmented reasoning with trustworthy process rewarding. In\nProceedings of the 48th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, pages 1251–1261, 2025.\n[262] Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N Ioannidis, Huzefa\nRangwala, and Christos Faloutsos. Agent-g: An agentic framework for graph retrieval augmented\ngeneration.\n[263] Xuying Ning, Dongqi Fu, Tianxin Wei, Mengting Ai, Jiaru Zou, Ting-Wei Li, and Jingrui He. Mc-search:\nBenchmarking multimodal agentic rag with structured reasoning chains. In NeurIPS 2025 Workshop\non Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling, 2025.\n[264] Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam, Enting Chen,\nDamien Graux, Andre Melo, Ruofei Lai, Zeren Jiang, et al. Gear: Graph-enhanced agent for retrieval-\naugmented generation. In Findings of the Association for Computational Linguistics: ACL 2025, pages\n12049–12072, 2025.\n[265] Han Zhang, Langshi Zhou, and Hanfang Yang. Learning to retrieve and reason on knowledge graph\nthrough active self-reflection. arXiv preprint arXiv:2502.14932, 2025.\n[266] Kelong Mao, Zheng Liu, Hongjin Qian, Fengran Mo, Chenlong Deng, and Zhicheng Dou. Rag-studio:\nTowards in-domain adaptation of retrieval augmented generation through self-alignment. In Findings\nof the Association for Computational Linguistics: EMNLP 2024, pages 725–735, 2024.\n[267] Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E\nGonzalez. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131,\n2024.\n[268] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez,\nJacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual instruction tuning.\nIn The Twelfth International Conference on Learning Representations, 2023.\n[269] Xuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan\nKe, Silvio Savarese, Caiming Xong, and Shafiq Joty. Sfr-rag: Towards contextually faithful llms. arXiv\npreprint arXiv:2409.09916, 2024.\n94\n"}, {"page": 95, "text": "Agentic Reasoning for Large Language Models\n[270] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with\nself-feedback. Advances in Neural Information Processing Systems, 36:46534–46594, 2023.\n[271] Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, and Heng Ji. Enable language\nmodels to implicitly learn self-improvement from data. In Proc. The Twelfth International Conference\non Learning Representations (ICLR2024), 2024.\n[272] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=1PL1NIMMrw.\n[273] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine\nLearning Research, 2023. URL https://openreview.net/forum?id=YfZ4ZPt8zd.\n[274] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning:\nEnabling generalized agent abilities for llms. In Findings of the Association for Computational Linguistics:\nACL 2024, pages 3053–3077, 2024.\n[275] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay\nKrishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language\nmodels with less training data and smaller model sizes. In Findings of the Association for Computational\nLinguistics: ACL 2023, pages 8003–8017, 2023.\n[276] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing systems,\n30, 2017.\n[277] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. In Advances in\nNeural Information Processing Systems (NeurIPS), 2023.\n[278] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\nfrom ai feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[279] Jiaqi Li, Xinyi Dong, Yang Liu, Zhizhuo Yang, Quansen Wang, Xiaobo Wang, Song-Chun Zhu, Zixia\nJia, and Zilong Zheng. Reflectevo: Improving meta introspection of small llms by learning self-\nreflection. In Findings of the Association for Computational Linguistics (ACL), 2025. URL https:\n//aclanthology.org/2025.findings-acl.871/.\n[280] Zhi Zheng and Wee Sun Lee. Reasoning-cv: Fine-tuning powerful reasoning llms for knowledge-\nassisted claim verification. arXiv preprint arXiv:2505.12348, 2025.\n[281] Alan Dao and Thinh Le. Rezero: Enhancing llm search ability by trying one-more-time. arXiv preprint\narXiv:2504.11001, 2025.\n[282] Nearchos Potamitis and Akhil Arora. Are retrials all you need? enhancing large language model\nreasoning without verbalized feedback. arXiv preprint arXiv:2504.12951, 2025.\n95\n"}, {"page": 96, "text": "Agentic Reasoning for Large Language Models\n[283] Hung Le, Yue Wang, Akhilesh Deepak Yu, Thanh-Tung Nguyen, Zhiwei Sun, Nan Jiang, Quoc Viet\nLe, and Steven C. H. Hoi. Coderl: Mastering code generation through pretrained models and deep\nreinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[284] Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin.\nLever: Learning to verify language-to-code generation with execution. In International Conference on\nMachine Learning, pages 26106–26128. PMLR, 2023.\n[285] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R.\nNarasimhan. Swe-bench: Can language models resolve real-world github issues? In International\nConference on Learning Representations (ICLR), 2024.\n[286] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language\nmodel. In International Conference on Machine Learning, pages 8469–8488. PMLR, 2023.\n[287] Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi,\nMuayad Ali, and Waseem AlShikh. Reflect, retry, reward: Self-improving llms via reinforcement\nlearning. arXiv preprint arXiv:2505.24726, 2025.\n[288] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton\nBishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif vs. rlhf: Scaling\nreinforcement learning from human feedback with ai feedback. 2024.\n[289] Potsawee Manakul, Adian Liusie, and Mark Gales. Selfcheckgpt: Zero-resource black-box hallucination\ndetection for generative large language models. In Proceedings of the 2023 conference on empirical\nmethods in natural language processing, pages 9004–9017, 2023.\n[290] Jishnu Ray Chowdhury and Cornelia Caragea. Zero-shot verification-guided chain of thoughts. arXiv\npreprint arXiv:2501.13122, 2025.\n[291] Dongxu Zhang, Ning Yang, Jihua Zhu, Jinnan Yang, Miao Xin, and Baoliang Tian. Ascot: An adaptive\nself-correction chain-of-thought method for late-stage fragility in llms. arXiv preprint arXiv:2508.05282,\n2025.\n[292] Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Tianpeng Li, Fan Yang, Zenan Zhou, and Wentao\nZhang. Mm-verify: Enhancing multimodal reasoning with chain-of-thought verification. In ACL, 2025.\nURL https://aclanthology.org/2025.acl-long.689/.\n[293] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph Gonza-\nlez. Memgpt: Towards llms as operating systems. ArXiv, abs/2310.08560, 2023. URL https:\n//api.semanticscholar.org/CorpusID:263909014.\n[294] Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, and Nanyun Peng. Re-rest: Reflection-\nreinforced self-training for language agents. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, pages 15394–15411, 2024.\n[295] LangChain AI. Langchain library. 2023. URL https://www.langchain.com/.\n[296] Jerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index.\n96\n"}, {"page": 97, "text": "Agentic Reasoning for Large Language Models\n[297] Wanjun Zhong, Lianghong Guo, Qi-Fei Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing\nlarge language models with long-term memory. ArXiv, abs/2305.10250, 2023. URL https://api.\nsemanticscholar.org/CorpusID:258741194.\n[298] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. ArXiv,\nabs/2409.07429, 2024. URL https://api.semanticscholar.org/CorpusID:272592995.\n[299] Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi\nYao, Mengru Wang, Shuofei Qiao, et al. Lightmem: Lightweight and efficient memory-augmented\ngeneration. arXiv preprint arXiv:2510.18866, 2025.\n[300] Jiayan Nan, Wenquan Ma, Wenlong Wu, and Yize Chen. Nemori: Self-organizing agent memory\ninspired by cognitive science. arXiv preprint arXiv:2508.03341, 2025.\n[301] Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru,\nJay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, et al. Agentic context engineering: Evolving\ncontexts for self-improving language models. arXiv preprint arXiv:2510.04618, 2025.\n[302] Siru Ouyang, Jun Yan, I Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T Le, Samira\nDaruki, Xiangru Tang, et al. Reasoningbank: Scaling agent self-evolving with reasoning memory.\narXiv preprint arXiv:2509.25140, 2025.\n[303] Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. Dynamic cheatsheet:\nTest-time learning with adaptive memory. arXiv preprint arXiv:2504.07952, 2025.\n[304] Kevin Lin, Charlie Snell, Yu Wang, Charles Packer, Sarah Wooders, Ion Stoica, and Joseph Gonzalez.\nSleep-time compute: Beyond inference scaling at test-time. ArXiv, abs/2504.13171, 2025. URL\nhttps://api.semanticscholar.org/CorpusID:277857467.\n[305] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and\nJonathan Larson. From local to global: A graph rag approach to query-focused summarization. ArXiv,\nabs/2404.16130, 2024. URL https://api.semanticscholar.org/CorpusID:269363075.\n[306] Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. Zep: a temporal\nknowledge graph architecture for agent memory. arXiv preprint arXiv:2501.13956, 2025.\n[307] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-1: Hybrid\nmultimodal memory empowered agents excel in long-horizon tasks. Advances in neural information\nprocessing systems, 37:49881–49913, 2024.\n[308] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose,\nKoki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning with contextual memory\nfor multimodal llm agents. arXiv preprint arXiv:2402.03610, 2024.\n[309] Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, and Wei Li. Seeing,\nlistening, remembering, and reasoning: A multimodal agent with long-term memory. arXiv preprint\narXiv:2508.09736, 2025.\n[310] Yuanchen Bei, Tianxin Wei, Xuying Ning, Yanjun Zhao, Zhining Liu, Xiao Lin, Yada Zhu, Hendrik\nHamann, Jingrui He, and Hanghang Tong. Mem-gallery: Benchmarking multimodal long-term\nconversational memory for mllm agents. arXiv preprint arXiv:2601.03515, 2026.\n97\n"}, {"page": 98, "text": "Agentic Reasoning for Large Language Models\n[311] Pengzhou Cheng, Lingzhong Dong, Zeng Wu, Zongru Wu, Xiangru Tang, Chengwei Qin, Zhuosheng\nZhang, and Gongshen Liu. Agent-scankit: Unraveling memory and reasoning of multimodal agents\nvia sensitivity perturbations. arXiv preprint arXiv:2510.00496, 2025.\n[312] Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan\nKian Hsiang Low, and Paul Pu Liang. MEM1: Learning to synergize memory and reasoning for efficient\nlong-horizon agents. arXiv preprint arXiv:2506.15841, 2025.\n[313] Yuqiang Zhang, Jiangming Shu, Ye Ma, Xueyuan Lin, Shangxi Wu, and Jitao Sang. Memory as action:\nAutonomous context curation for long-horizon agentic tasks. arXiv preprint arXiv:2510.12635, 2025.\nURL https://arxiv.org/abs/2510.12635.\n[314] Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang,\nWei-Ying Ma, Jingjing Liu, Mingxuan Wang, et al. Memagent: Reshaping long-context llm with\nmulti-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025.\n[315] Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian McAuley, and Xiaojian\nWu. Mem-{\\alpha}: Learning memory construction via reinforcement learning. arXiv preprint\narXiv:2509.25911, 2025.\n[316] Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning,\nZhaorun Chen, Xiaohan Fu, et al. Agent learning via early experience. arXiv preprint arXiv:2510.08558,\n2025.\n[317] Yi Yu, Liuyi Yao, Yuexiang Xie, Qingquan Tan, Jiaqi Feng, Yaliang Li, and Libing Wu. Agentic memory:\nLearning unified long-term and short-term memory management for large language model agents.\narXiv preprint arXiv:2601.01885, 2026.\n[318] Shengtao Zhang, Jiaqian Wang, Ruiwen Zhou, Junwei Liao, Yuchen Feng, Weinan Zhang, Ying Wen,\nZhiyu Li, Feiyu Xiong, Yutao Qi, et al. Memrl: Self-evolving agents via runtime reinforcement learning\non episodic memory. arXiv preprint arXiv:2601.03192, 2026.\n[319] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection. In The Twelfth International Conference on\nLearning Representations, 2024. URL https://openreview.net/forum?id=hSyW5go0v8.\n[320] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards a general\nread-write memory for large language models. ArXiv, abs/2305.14322, 2023. URL https://api.\nsemanticscholar.org/CorpusID:258841042.\n[321] Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Zhenhe Wu, ShuangZhi Wu, Zejun Ma, and Zhoujun\nLi. Scm: Enhancing large language model with self-controlled memory framework. In International\nConference on Database Systems for Advanced Applications, pages 188–203. Springer, 2025.\n[322] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang.\nEvaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753,\n2024.\n[323] Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval:\nBenchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813,\n2024.\n98\n"}, {"page": 99, "text": "Agentic Reasoning for Large Language Models\n[324] Ruihan Yang, Jiangjie Chen, Yikai Zhang, Siyu Yuan, Aili Chen, Kyle Richardson, Yanghua Xiao, and\nDeqing Yang. SELFGOAL: Your language agents already know how to achieve high-level goals. In\nLuis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of\nthe Americas Chapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 799–819, Albuquerque, New Mexico, April 2025. Association for\nComputational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.36. URL\nhttps://aclanthology.org/2025.naacl-long.36/.\n[325] Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-\nBurch, and Sepp Hochreiter. Large language models can self-improve at web agent tasks. ArXiv,\nabs/2405.20309, 2024. URL https://api.semanticscholar.org/CorpusID:270122967.\n[326] Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, and Ji-Rong Wen.\nReflective multi-agent collaboration based on large language models. Advances in Neural Information\nProcessing Systems, 37:138595–138631, 2024.\n[327] Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W.\nSuchow, and Khaldoun Khashanah. Finmem: A performance-enhanced llm trading agent with layered\nmemory and character design. arXiv preprint arXiv:2311.13743, 2023. URL https://www.arxiv.\norg/abs/2311.13743.\n[328] Yu Wang and Xi Chen. Mirix: Multi-agent memory system for llm-based agents. arXiv preprint\narXiv:2507.07957, 2025.\n[329] Siru Ouyang, Wenhao Yu, Kaixin Ma, Zi-Qiang Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han,\nHongming Zhang, and Dong Yu. Repograph: Enhancing ai software engineering with repository-\nlevel code graph. ArXiv, abs/2410.14684, 2024. URL https://api.semanticscholar.org/\nCorpusID:273502041.\n[330] Alireza Rezazadeh, Zichao Li, Wei Wei, and Yujia Bao. From isolated conversations to hierarchical\nschemas: Dynamic tree memory representation for llms. arXiv preprint arXiv:2410.14052, 2024.\n[331] Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and\nYongfeng Zhang. Autoflow: Automated workflow generation for large language model agents. ArXiv,\nabs/2407.12821, 2024. URL https://api.semanticscholar.org/CorpusID:271270428.\n[332] Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge,\nXin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv\npreprint arXiv:2410.10762, 2024.\n[333] Zhen Zeng, William Watson, Nicole Cho, Saba Rahimi, Shayleen Reynolds, Tucker Hybinette Balch,\nand Manuela Veloso. Flowmind: Automatic workflow generation with llms. Proceedings of the Fourth\nACM International Conference on AI in Finance, 2023. URL https://api.semanticscholar.org/\nCorpusID:265452485.\n[334] Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging language\nmodel agents. arXiv preprint arXiv:2506.01716, 2025.\n[335] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and\nJason E Weston. Self-rewarding language models. In Forty-first International Conference on Machine\nLearning, 2024.\n99\n"}, {"page": 100, "text": "Agentic Reasoning for Large Language Models\n[336] Toby Simonds, Kevin Lopez, Akira Yoshiyama, and Dominique Garmier. Self rewarding self improving.\narXiv preprint arXiv:2505.08827, 2025.\n[337] Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Qi Zhu, Fei Mi, Baojun Wang, Weichao\nWang, Xingshan Zeng, Lifeng Shang, et al. Self: Self-evolution with language feedback. arXiv preprint\narXiv:2310.00533, 2023.\n[338] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,\nShariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via\nreinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\n[339] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James\nZou. Textgrad: Automatic\" differentiation\" via text. arXiv preprint arXiv:2406.07496, 2024.\n[340] Tevin Wang and Chenyan Xiong. Autorule: Reasoning chain-of-thought extracted rule-based rewards\nimprove preference learning. arXiv preprint arXiv:2506.15651, 2025.\n[341] Mengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, Jian-Guang Lou, Qingwei Lin, Ping Luo, and\nSaravan Rajmohan. Agentgen: Enhancing planning abilities for large language model based agent via\nenvironment and task generation. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining V. 1, pages 496–507, 2025.\n[342] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang.\nAdaplanner: Adaptive\nplanning from feedback with language models. Advances in neural information processing systems, 36:\n58202–58245, 2023.\n[343] Maxime Robeyns, Martin Szummer, and Laurence Aitchison. A self-improving coding agent. arXiv\npreprint arXiv:2504.15228, 2025.\n[344] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan\nYu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via\nmulti-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025.\n[345] Borui Wang, Kathleen McKeown, and Rex Ying. Dystil: Dynamic strategy induction with large language\nmodels for reinforcement learning. arXiv preprint arXiv:2505.03209, 2025.\n[346] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models\nas tool makers. In The Twelfth International Conference on Learning Representations, 2024. URL\nhttps://openreview.net/forum?id=qV83K9d5WB.\n[347] Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Fung, Hao Peng, and Heng Ji. CRAFT: Customizing\nLLMs by creating and retrieving from specialized toolsets. In The Twelfth International Conference on\nLearning Representations, 2024. URL https://openreview.net/forum?id=G0vdDSt9XM.\n[348] Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. CREATOR: Tool creation for\ndisentangling abstract and concrete reasoning of large language models. In Houda Bouamor, Juan Pino,\nand Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages\n6922–6939, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/\n2023.findings-emnlp.462. URL https://aclanthology.org/2023.findings-emnlp.462/.\n[349] Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, and Jakob Nikolas Kather. Llm\nagents making agent tools, 2025. URL https://arxiv.org/abs/2502.11705.\n100\n"}, {"page": 101, "text": "Agentic Reasoning for Large Language Models\n[350] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen,\nYusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative\nagents for software development. In ACL 2024, pages 15174–15186. Association for Computational\nLinguistics, 2024.\n[351] Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui Liu, Zijie Yu, Yuchen Hou, Shuo Tang, and Siheng\nChen. Self-evolving multi-agent collaboration networks for software development. arXiv preprint\narXiv:2410.16946, 2024.\n[352] J Gregory Pauloski, Yadu Babuji, Ryan Chard, Mansi Sakarvadia, Kyle Chard, and Ian Foster. Empow-\nering scientific workflows with federated agents. arXiv preprint arXiv:2505.05428, 2025.\n[353] Shu-Heng Chen. Agent-based computational finance. In Leigh Tesfatsion and Kenneth L. Judd, editors,\nHandbook of Computational Economics, volume 3, pages 1245–1293. Elsevier, 2012.\n[354] John C. Hull. Risk Management and Financial Institutions. Wiley, 5th edition, 2018.\n[355] Yuante Li, Xu Yang, Xiao Yang, Minrui Xu, Xisen Wang, Weiqing Liu, and Jiang Bian. R&d-agent-\nquant: A multi-agent framework for data-centric factors and model joint optimization.\nCoRR,\nabs/2505.15155, 2025. doi: 10.48550/ARXIV.2505.15155. URL https://doi.org/10.48550/\narXiv.2505.15155.\n[356] Hongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun Lin, Junlin Wang, Tianyu\nZhou, Mao Guan, Runjia Zhang, et al. Finrobot: An open-source ai agent platform for financial\napplications using large language models. arXiv preprint arXiv:2405.14767, 2024.\n[357] Yiying Wang, Xiaojing Li, Binzhu Wang, Yueyang Zhou, Yingru Lin, Han Ji, Hong Chen, Jinshi Zhang,\nFei Yu, Zewei Zhao, et al. Peer: Expertizing domain-specific tasks with a multi-agent framework and\ntuning methods. arXiv preprint arXiv:2407.06985, 2024.\n[358] Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yuechen Jiang, Yupeng Cao, Zhi Chen,\nJordan W. Suchow, Zhenyu Cui, Rong Liu, Zhaozhuo Xu, Denghui Zhang, Koduvayur Subbalakshmi,\nGuojun Xiong, Yueru He, Jimin Huang, Dong Li, and Qianqian Xie. Fincon: A synthesized LLM\nmulti-agent system with conceptual verbal reinforcement for enhanced financial decision making.\nIn Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tom-\nczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual\nConference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, De-\ncember 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\nf7ae4fe91d96f50abc2211f09b6a7e49-Abstract-Conference.html.\n[359] Jingyun Sun, Chengxiao Dai, Zhongze Luo, Yangbo Chang, and Yang Li. Lawluo: A multi-agent\ncollaborative framework for multi-round chinese legal consultation. arXiv preprint arXiv:2407.16252,\n2024.\n[360] Albert Sadowski, JarosĹ Chudziak, et al. On verifiable legal reasoning: A multi-agent framework with\nformalized knowledge representations. arXiv preprint arXiv:2509.00710, 2025.\n[361] Guhong Chen, Liyang Fan, Zihan Gong, Nan Xie, Zixuan Li, Ziqiang Liu, Chengming Li, Qiang Qu,\nHamid Alinejad-Rokny, Shiwen Ni, et al. Agentcourt: Simulating court with adversarial evolvable\nlawyer agents. arXiv preprint arXiv:2408.08089, 2024.\n101\n"}, {"page": 102, "text": "Agentic Reasoning for Large Language Models\n[362] Jarosław A Chudziak and Adam Kostka. Ai-powered math tutoring: Platform for personalized and\nadaptive education. In International Conference on Artificial Intelligence in Education, pages 462–469.\nSpringer, 2025.\n[363] Xueqiao Zhang, Chao Zhang, Jianwen Sun, Jun Xiao, Yi Yang, and Yawei Luo. Eduplanner: Llm-based\nmulti-agent systems for customized and intelligent instructional design. IEEE Transactions on Learning\nTechnologies, 2025.\n[364] Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik S Chan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee,\nMarzyeh Ghassemi, Cynthia Breazeal, and Hae W Park. Mdagents: An adaptive collaboration of llms\nfor medical decision-making. Advances in Neural Information Processing Systems, 37:79410–79452,\n2024.\n[365] Fatemeh Ghezloo, Mehmet Saygin Seyfioglu, Rustin Soraki, Wisdom O Ikezogwo, Beibin Li, Tejoram\nVivekanandan, Joann G Elmore, Ranjay Krishna, and Linda Shapiro. Pathfinder: A multi-modal\nmulti-agent system for medical diagnostic decision-making applied to histopathology. arXiv preprint\narXiv:2502.08916, 2025.\n[366] Yinghao Zhu, Yifan Qi, Zixiang Wang, Lei Gu, Dehao Sui, Haoran Hu, Xichen Zhang, Ziyi He, Liantao\nMa, and Lequan Yu. Healthflow: A self-evolving ai agent with meta planning for autonomous\nhealthcare research. arXiv preprint arXiv:2508.02621, 2025.\n[367] Mingxuan Cui, Yilan Jiang, Duo Zhou, Cheng Qian, Yuji Zhang, and Qiong Wang. Shortagesim:\nSimulating drug shortages under information asymmetry. arXiv preprint arXiv:2509.01813, 2025.\n[368] Ziyue Wang, Junde Wu, Linghan Cai, Chang Han Low, Xihong Yang, Qiaxuan Li, and Yueming Jin.\nMedagent-pro: Towards evidence-based multi-modal medical diagnosis via reasoning agentic workflow.\narXiv preprint arXiv:2503.18968, 2025.\n[369] Reza Averly, Frazier N Baker, Ian A Watson, and Xia Ning. Liddia: Language-based intelligent drug\ndiscovery agent. arXiv preprint arXiv:2502.13959, 2025.\n[370] Zhaolin Hu, Yixiao Zhou, Zhongan Wang, Xin Li, Weimin Yang, Hehe Fan, and Yi Yang. OSDA agent:\nLeveraging large language models for de novo design of organic structure directing agents. In The\nThirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28,\n2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=9YNyiCJE3k.\n[371] Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Yingzhou Lu, and Yue Zhao. Drugagent:\nAutomating ai-aided drug discovery programming through llm multi-agent collaboration. arXiv\npreprint arXiv:2411.15692, 2024.\n[372] Haoyang Liu, Yijiang Li, and Haohan Wang. Genomas: A multi-agent framework for scientific discovery\nvia code-driven gene expression analysis. arXiv preprint arXiv:2507.21035, 2025.\n[373] Qixin Deng, Qikai Yang, Ruibin Yuan, Yipeng Huang, Yi Wang, Xubo Liu, Zeyue Tian, Jiahao Pan,\nGe Zhang, Hanfeng Lin, et al. Composerx: Multi-agent symbolic music composition with llms. In The\n25th International Society for Music Information Retrieval Conference, 2024.\n[374] Wentao Zhang, Liang Zeng, Yuzhen Xiao, Yongcong Li, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui\nZhou, and Bo An. Agentorchestra: Orchestrating multi-agent intelligence with the tool-environment-\nagent(tea) protocol, 2026. URL https://arxiv.org/abs/2506.12508.\n102\n"}, {"page": 103, "text": "Agentic Reasoning for Large Language Models\n[375] Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo, Evangelos B Mazomenos, and\nYueming Jin. Surgraw: Multi-agent workflow with chain-of-thought reasoning for surgical intelligence.\narXiv preprint arXiv:2503.10265, 2025.\n[376] Ran Xu, Wenqi Shi, Yuchen Zhuang, Yue Yu, Joyce C Ho, Haoyu Wang, and Carl Yang. Collab-rag:\nBoosting retrieval-augmented generation for complex question answering via white-box and black-box\nllm collaboration. arXiv preprint arXiv:2504.04915, 2025.\n[377] Thang Nguyen, Peter Chin, and Yu-Wing Tai. Ma-rag: Multi-agent retrieval-augmented generation via\ncollaborative chain-of-thought reasoning, 2025. URL https://arxiv.org/abs/2505.20096.\n[378] Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Arik. Chain of agents:\nLarge language models collaborating on long-context tasks. Advances in Neural Information Processing\nSystems, 37:132208–132237, 2024.\n[379] Hong Qing Yu and Frank McQuade. Rag-kg-il: A multi-agent hybrid framework for reducing hal-\nlucinations and enhancing llm reasoning through rag and incremental knowledge graph learning\nintegration, 2025. URL https://arxiv.org/abs/2503.13514.\n[380] Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, and Jiayi Shen. Smoa:\nImproving multi-agent large language models with sparse mixture-of-agents, 2024. URL https:\n//arxiv.org/abs/2411.03284.\n[381] Siwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li, Hongtu Zhu, and Huaxiu Yao. Mdocagent:\nA multi-modal multi-agent framework for document understanding, 2025. URL https://arxiv.\norg/abs/2503.13964.\n[382] Patara Trirat, Wonyong Jeong, and Sung Ju Hwang. Automl-agent: A multi-agent llm framework for\nfull-pipeline automl. arXiv preprint arXiv:2410.02958, 2024.\n[383] Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Friederike Niedtner,\nGrace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, et al. Magentic-one: A generalist\nmulti-agent system for solving complex tasks. arXiv preprint arXiv:2411.04468, 2024.\n[384] Rui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, and Jing Shao. Mas-gpt: Training\nllms to build llm-based multi-agent systems. arXiv preprint arXiv:2503.03686, 2025.\n[385] Yaolun Zhang, Xiaogeng Liu, and Chaowei Xiao. Metaagent: Automatically constructing multi-agent\nsystems based on finite state machines. arXiv preprint arXiv:2507.22606, 2025.\n[386] Zheyuan Zhang, Kaiwen Shi, Zhengqing Yuan, Zehong Wang, Tianyi Ma, Keerthiram Murugesan,\nVincent Galassi, Chuxu Zhang, and Yanfang Ye. Agentrouter: A knowledge-graph-guided llm router\nfor collaborative multi-agent question answering. arXiv preprint arXiv:2510.05445, 2025.\n[387] Feijie Wu, Zitao Li, Fei Wei, Yaliang Li, Bolin Ding, and Jing Gao. Talk to right specialists: Routing\nand planning in multi-agent system for question answering. arXiv preprint arXiv:2501.07813, 2025.\n[388] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, and\nKatia Sycara. Theory of mind for multi-agent collaboration via large language models. arXiv preprint\narXiv:2310.10701, 2023.\n103\n"}, {"page": 104, "text": "Agentic Reasoning for Large Language Models\n[389] Logan Cross, Violet Xiang, Agam Bhatia, Daniel LK Yamins, and Nick Haber. Hypothetical minds:\nScaffolding theory of mind for multi-agent tasks with large language models.\narXiv preprint\narXiv:2407.07086, 2024.\n[390] Mircea Lică, Ojas Shirekar, Baptiste Colle, and Chirag Raman. Mindforge: Empowering embodied\nagents with theory of mind for lifelong cultural learning. arXiv preprint arXiv:2411.12977, 2024.\n[391] Yuheng Wu, Wentao Guo, Zirui Liu, Heng Ji, Zhaozhuo Xu, and Denghui Zhang. How large language\nmodels encode theory-of-mind: a study on sparse parameter patterns. npj Artificial Intelligence, 1(1):\n20, 2025.\n[392] Bo Yang, Jiaxian Guo, Yusuke Iwasawa, and Yutaka Matsuo. Large language models as theory of\nmind aware generative agents with counterfactual reflection. arXiv preprint arXiv:2501.15355, 2025.\n[393] Rikunari Sagara, Koichiro Terao, and Naoto Iwahashi. Beliefnest: A joint action simulator for embodied\nagents with theory of mind. arXiv preprint arXiv:2505.12321, 2025.\n[394] Arnav Singhvi, Manish Shetty, Shangyin Tan, Christopher Potts, Koushik Sen, Matei Zaharia, and\nOmar Khattab. Dspy assertions: Computational constraints for self-refining language model pipelines.\narXiv preprint arXiv:2312.13382, 2023.\n[395] Han Zhou, Xingchen Wan, Ruoxi Sun, Hamid Palangi, Shariq Iqbal, Ivan Vulić, Anna Korhonen, and\nSercan Ö Arık. Multi-agent design: Optimizing agents with better prompts and topologies. arXiv\npreprint arXiv:2502.02533, 2025.\n[396] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\noptimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023.\n[397] Shengchao Hu, Li Shen, Ya Zhang, and Dacheng Tao. Learning multi-agent communication from\ngraph modeling perspective. arXiv preprint arXiv:2405.08550, 2024.\n[398] Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang,\nTianlong Chen, and Dawei Cheng. G-designer: Architecting multi-agent communication topologies\nvia graph neural networks. arXiv preprint arXiv:2410.11782, 2024.\n[399] Xianghua Zeng, Hang Su, Zhengyi Wang, and Zhiyuan Lin. Graph diffusion for robust multi-agent\ncoordination. In Forty-second International Conference on Machine Learning.\n[400] Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng,\nJeffrey Xu Yu, and Tianlong Chen. Cut the crap: An economical communication pipeline for llm-based\nmulti-agent systems. arXiv preprint arXiv:2410.02506, 2024.\n[401] Boyi Li, Zhonghan Zhao, Der-Horng Lee, and Gaoang Wang. Adaptive graph pruning for multi-agent\ncommunication. arXiv preprint arXiv:2506.02951, 2025.\n[402] Shilong Wang, Guibin Zhang, Miao Yu, Guancheng Wan, Fanci Meng, Chongye Guo, Kun Wang, and\nYang Wang. G-safeguard: A topology-guided security lens and treatment on llm-based multi-agent\nsystems. arXiv preprint arXiv:2502.11127, 2025.\n[403] Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, and Xiang Wang. Multi-agent architec-\nture search via agentic supernet. arXiv preprint arXiv:2502.04180, 2025.\n104\n"}, {"page": 105, "text": "Agentic Reasoning for Large Language Models\n[404] Hui Yi Leong and Yuqing Wu. Dynaswarm: Dynamically graph structure selection for llm-based\nmulti-agent system. arXiv preprint arXiv:2507.23261, 2025.\n[405] Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng Wan, Kun Wang, Dawei Cheng, and Yiyan Qi.\nMasrouter: Learning to route llms for multi-agent systems. arXiv preprint arXiv:2502.11133, 2025.\n[406] Jun Liu, Zhenglun Kong, Changdi Yang, Fan Yang, Tianqi Li, Peiyan Dong, Joannah Nanjekye, Hao\nTang, Geng Yuan, Wei Niu, et al. Rcr-router: Efficient role-aware context routing for multi-agent llm\nsystems with structured memory. arXiv preprint arXiv:2508.04903, 2025.\n[407] Cheng Qian, Zuxin Liu, Shirley Kokane, Akshara Prabhakar, Jielin Qiu, Haolin Chen, Zhiwei Liu, Heng\nJi, Weiran Yao, Shelby Heinecke, et al. xrouter: Training cost-aware llms orchestration system via\nreinforcement learning. arXiv preprint arXiv:2510.08439, 2025.\n[408] Jingbo Wang, Sendong Zhao, Haochun Wang, Yuzheng Fan, Lizhe Zhang, Yan Liu, and Ting Liu.\nOptimal-agent-selection: State-aware routing framework for efficient multi-agent collaboration. arXiv\npreprint arXiv:2511.02200, 2025.\n[409] Shuo Liu, Zeyu Liang, Xueguang Lyu, and Christopher Amato. Llm collaboration with multi-agent\nreinforcement learning. arXiv preprint arXiv:2508.04652, 2025.\n[410] Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, and Zenglin Xu. Heterogeneous group-\nbased reinforcement learning for llm-based multi-agent systems. arXiv preprint arXiv:2506.02718,\n2025.\n[411] Ziqi Jia, Junjie Li, Xiaoyang Qu, and Jianzong Wang. Enhancing multi-agent systems via reinforcement\nlearning with llm-based planner and graph-based policy. arXiv preprint arXiv:2503.10049, 2025.\n[412] Guobin Zhu, Rui Zhou, Wenkang Ji, and Shiyu Zhao. Lamarl: Llm-aided multi-agent reinforcement\nlearning for cooperative policy generation. IEEE Robotics and Automation Letters, 2025.\n[413] Chanwoo Park, Seungju Han, Xingzhi Guo, Asuman Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim.\nMaporl: Multi-agent post-co-training for collaborative large language models with reinforcement\nlearning. arXiv preprint arXiv:2502.18439, 2025.\n[414] Wanjia Zhao, Mert Yuksekgonul, Shirley Wu, and James Zou. Sirius: Self-improving multi-agent\nsystems via bootstrapped reasoning. arXiv preprint arXiv:2502.04780, 2025.\n[415] Vighnesh Subramaniam, Yilun Du, Joshua B Tenenbaum, Antonio Torralba, Shuang Li, and Igor\nMordatch. Multiagent finetuning: Self improvement with diverse reasoning chains. arXiv preprint\narXiv:2501.05707, 2025.\n[416] Ziyan Wang, Zhicheng Zhang, Fei Fang, and Yali Du. M3hf: Multi-agent reinforcement learning from\nmulti-phase human feedback of mixed quality. arXiv preprint arXiv:2503.02077, 2025.\n[417] The Viet Bui, Tien Mai, and Hong Thanh Nguyen. O-mapl: Offline multi-agent preference learning.\narXiv preprint arXiv:2501.18944, 2025.\n[418] Raja Ben Abdessalem, Shiva Nejati, Lionel C. Briand, and Thomas Stifter. Testing advanced driver\nassistance systems using multi-objective search and neural networks. In Proceedings of the 31st\nIEEE/ACM International Conference on Automated Software Engineering, ASE ’16, page 63–74, New\nYork, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450338455. doi: 10.1145/\n2970276.2970311. URL https://doi.org/10.1145/2970276.2970311.\n105\n"}, {"page": 106, "text": "Agentic Reasoning for Large Language Models\n[419] Ziyu Wan, Yunxiang Li, Xiaoyu Wen, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun\nWang, Weinan Zhang, Shuyue Hu, et al. Rema: Learning to meta-think for llms with multi-agent\nreinforcement learning. arXiv preprint arXiv:2503.09501, 2025.\n[420] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent\ntraining. arXiv preprint arXiv:2505.10978, 2025.\n[421] Zixuan Ke, Austin Xu, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, and Shafiq Joty. Mas-zero:\nDesigning multi-agent systems with zero supervision. arXiv preprint arXiv:2505.14996, 2025.\n[422] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel:\nLlm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 38, pages 19632–19642, 2024.\n[423] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Saiful Haq,\nAshutosh Sharma, Thomas T Joshi, Hanna Moazam, Heather Miller, et al. Dspy: Compiling declarative\nlanguage model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning\nRepresentations, 2024.\n[424] Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao,\nQihan Ren, Xun Jiang, et al. Alita: Generalist agent enabling scalable agentic reasoning with minimal\npredefinition and maximal self-evolution. arXiv preprint arXiv:2505.20286, 2025.\n[425] Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. G-memory:\nTracing hierarchical memory for multi-agent systems. arXiv preprint arXiv:2506.07398, 2025.\n[426] Haoran Xu, Jiacong Hu, Ke Zhang, Lei Yu, Yuxin Tang, Xinyuan Song, Yiqun Duan, Lynn Ai, and Bill\nShi. Sedm: Scalable self-evolving distributed memory for agents. arXiv preprint arXiv:2509.09498,\n2025.\n[427] Alireza Rezazadeh, Zichao Li, Ange Lou, Yuying Zhao, Wei Wei, and Yujia Bao. Collaborative memory:\nMulti-user memory sharing in llm agents with dynamic access control. arXiv preprint arXiv:2505.18279,\n2025.\n[428] Dongge Han, Camille Couturier, Daniel Madrigal Diaz, Xuchao Zhang, Victor Rühle, and Saravan Raj-\nmohan. Legomem: Modular procedural memory for multi-agent llm systems for workflow automation.\narXiv preprint arXiv:2510.04851, 2025.\n[429] Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo,\nGuangyu Robert Yang, and Andrew Ahn. Lyfe agents: Generative agents for low-cost real-time\nsocial interactions. arXiv preprint arXiv:2310.02172, 2023.\n[430] Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei,\nPeng Xia, Fang Wu, He Zhu, et al. Agent kb: Leveraging cross-domain experience for agentic problem\nsolving. arXiv preprint arXiv:2507.06229, 2025.\n[431] Wenyue Hua, Xianjun Yang, Mingyu Jin, Zelong Li, Wei Cheng, Ruixiang Tang, and Yongfeng Zhang.\nTrustagent: Towards safe and trustworthy llm-based agents. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2024, pages 10000–10016, 2024.\n[432] Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Self-adapting\nlanguage models. arXiv preprint arXiv:2506.10943, 2025.\n106\n"}, {"page": 107, "text": "Agentic Reasoning for Large Language Models\n[433] Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang,\nXinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084,\n2025.\n[434] Toby Simonds and Akira Yoshiyama. Ladder: Self-improving llms through recursive problem decom-\nposition. arXiv preprint arXiv:2503.00735, 2025.\n[435] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with\nreasoning. Advances in Neural Information Processing Systems, 35:15476–15488, 2022.\n[436] Mohamed Amine Ferrag, Norbert Tihanyi, and Merouane Debbah. Reasoning beyond limits: Advances\nand open problems for llms. arXiv preprint arXiv:2503.22732, 2025.\n[437] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang,\nJiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum\nreinforcement learning. arXiv preprint arXiv:2411.02337, 2024.\n[438] Jin Hwa Lee, Stefano Sarao Mannelli, and Andrew Saxe. Why do animals need shaping? a theory of\ntask composition and curriculum learning. arXiv preprint arXiv:2402.18361, 2024.\n[439] Xuechen Liang, Meiling Tao, Yinghui Xia, Jianhui Wang, Kun Li, Yijin Wang, Yangfan He, Jingsong\nYang, Tianyu Shi, Yuantao Wang, et al. Sage: Self-evolving agents with reflective and memory-\naugmented abilities. Neurocomputing, page 130470, 2025.\n[440] Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, and Yassine Benajiba.\nMeminsight: Autonomous memory augmentation for llm agents. arXiv preprint arXiv:2503.21760,\n2025.\n[441] Jiaru Zou, Xiyuan Yang, Ruizhong Qiu, Gaotang Li, Katherine Tieu, Pan Lu, Ke Shen, Hanghang Tong,\nYejin Choi, Jingrui He, James Zou, Mengdi Wang, and Ling Yang. Latent collaboration in multi-agent\nsystems, 2025. URL https://arxiv.org/abs/2511.20639.\n[442] Sizhe Yuen, Francisco Gomez Medina, Ting Su, Yali Du, and Adam J. Sobey. Intrinsic memory\nagents: Heterogeneous multi-agent llm systems through structured contextual memory. arXiv preprint\narXiv:2508.08997, 2025.\n[443] Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, and Carlee Joe-Wong. Llm-powered\ndecentralized generative agents with adaptive hierarchical knowledge graph for cooperative planning.\narXiv preprint arXiv:2502.05453, 2025.\n[444] Hang Gao and Yongfeng Zhang. Memory sharing for large language model based agents. arXiv\npreprint arXiv:2404.09982, 2024.\n[445] Ye Bai, Minghan Wang, and Thuy-Trang Vu. Maple: Multi-agent adaptive planning with long-term\nmemory for table reasoning. arXiv preprint arXiv:2506.05813, 2025.\n[446] Yixing Chen, Yiding Wang, Siqi Zhu, Haofei Yu, Tao Feng, Muhan Zhang, Mostofa Patwary, and\nJiaxuan You. Multi-agent evolve: Llm self-improve through co-evolution, 2025. URL https://\narxiv.org/abs/2510.23595.\n[447] Junwei Liao, Muning Wen, Jun Wang, and Weinan Zhang. Marft: Multi-agent reinforcement fine-\ntuning, 2025. URL https://arxiv.org/abs/2504.16129.\n107\n"}, {"page": 108, "text": "Agentic Reasoning for Large Language Models\n[448] Yujie Zhao, Lanxiang Hu, Yang Wang, Minmin Hou, Hao Zhang, Ke Ding, and Jishen Zhao. Stronger-\nmas: Multi-agent reinforcement learning for collaborative llms, 2025. URL https://arxiv.org/\nabs/2510.11062.\n[449] Natalia Zhang, Xinqi Wang, Qiwen Cui, Runlong Zhou, Sham M Kakade, and Simon S Du. Preference-\nbased multi-agent reinforcement learning: Data coverage and algorithmic techniques. arXiv preprint\narXiv:2409.00717, 2024.\n[450] Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, and Stefan Wermter. Chat with\nthe environment: Interactive multimodal perception using large language models. In 2023 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pages 3590–3596. IEEE, 2023.\n[451] Xiangyuan Xue, Yifan Zhou, Guibin Zhang, Zaibin Zhang, Yijiang Li, Chen Zhang, Zhenfei Yin, Philip\nTorr, Wanli Ouyang, and Lei Bai. Comas: Co-evolving multi-agent systems via interaction rewards,\n2025. URL https://arxiv.org/abs/2510.08529.\n[452] Sumeet Ramesh Motwani, Chandler Smith, Rocktim Jyoti Das, Rafael Rafailov, Ivan Laptev, Philip\nH. S. Torr, Fabio Pizzati, Ronald Clark, and Christian Schroeder de Witt. Malt: Improving reasoning\nwith multi-agent llm training, 2025. URL https://arxiv.org/abs/2412.01928.\n[453] Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai\nFan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, and Fei Huang. Mars: Optimizing\ndual-system deep research via multi-agent reinforcement learning, 2025. URL https://arxiv.\norg/abs/2510.04935.\n[454] Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Ben-\njamin Van Durme, Daniel Khashabi, Jason Weston, and Hongyuan Zhan. The alignment waltz: Jointly\ntraining agents to collaborate for safety, 2025. URL https://arxiv.org/abs/2510.08240.\n[455] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168, 2021.\n[456] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint\narXiv:2103.03874, 2021.\n[457] Mathematical Association of America. American invitational mathematics examination. https:\n//www.maa.org/math-competitions/aime.\n[458] Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman\nOlsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: A benchmark\nfor evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024.\n[459] Minh-Thang Luong, Dawsen Hwang, Hoang H Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo,\nJunsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, et al. Towards robust mathematical\nreasoning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing,\npages 35406–35430, 2025.\n[460] Grzegorz Swirszcz, Adam Zsolt Wagner, Geordie Williamson, Sam Blackwell, Bogdan Georgiev, Alex\nDavies, Ali Eslami, Sebastien Racaniere, Theophane Weber, and Pushmeet Kohli. Advancing geometry\nwith ai: Multi-agent generation of polytopes. arXiv preprint arXiv:2502.05199, 2025.\n108\n"}, {"page": 109, "text": "Agentic Reasoning for Large Language Models\n[461] Bogdan Georgiev, Javier Gómez-Serrano, Terence Tao, and Adam Zsolt Wagner.\nMathematical\nexploration and discovery at scale. arXiv preprint arXiv:2511.02864, 2025.\n[462] Simon Willison. Not all ai-assisted programming is vibe coding (but vibe coding rocks). https:\n//simonwillison.net/2025/Mar/19/vibe-coding/, 2025. Blog post.\n[463] Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev, Richard\nTanburn, Peter Battaglia, Charles Blundell, András Juhász, Marc Lackenby, Geordie Williamson, Demis\nHassabis, and Pushmeet Kohli. Advancing mathematics by guiding human intuition with AI. Nature,\n(7887):70–74, 2021. doi: 10.1038/s41586-021-04086-x.\n[464] Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, and Shafiq Joty. CodeChain:\nTowards modular code generation through chain of self-revisions with representative sub-modules. In\nInternational Conference on Learning Representations (ICLR), 2023.\n[465] Tao Huang, Zhihong Sun, Zhi Jin, Ge Li, and Chen Lyu. Knowledge-aware code generation with large\nlanguage models. In IEEE/ACM International Conference on Program Comprehension (ICPC), pages\n52–63, 2024.\n[466] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy,\nSriram Rajamani, Balasubramanyan Ashok, and Shashank Shet. Codeplan: Repository-level coding\nusing llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675–698, 2024.\n[467] Yewei Han and Chen Lyu. Multi-stage guided code generation for large language models. Engineering\nApplications of Artificial Intelligence, 139(PA):109491, 2025.\n[468] Jierui Li, Hung Le, Yingbo Zhou, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. Codetree: Agent-\nguided tree search for code generation with large language models. arXiv preprint arXiv:2411.04329,\n2024.\n[469] Vaibhav Aggarwal, Ojasv Kamal, Abhinav Japesh, Zhijing Jin, and Bernhard Schölkopf. DARS: Dynamic\naction re-sampling to enhance coding agent performance by adaptive tree traversal, 2025.\n[470] Nicola Dainese, Matteo Merler, Minttu Alakuijala, and Pekka Marttinen. Generating code world\nmodels with large language models guided by monte carlo tree search. In Conference on Neural\nInformation Processing Systems (NeurIPS), pages 60429–60474, 2024.\n[471] Chia-Tung Ho, Haoxing Ren, and Brucek Khailany. Verilogcoder: Autonomous Verilog coding agents\nwith graph-based planning and abstract syntax tree (ast)-based waveform tracing tool. In AAAI\nConference on Artificial Intelligence (AAAI), volume 39, pages 300–307, 2025.\n[472] Karina Zainullina, Alexander Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Daria\nLitvintseva, Simon Karasik, Filipp Fisin, Sergei Skvortsov, Maksim Nekrashevich, Anton Shevtsov, and\nBoris Yangel. Guided search strategies in non-serializable environments with applications to software\nengineering agents. In International Conference on Machine Learning (ICML), 2025.\n[473] Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri. An in-context\nlearning agent for formal theorem-proving. In Conference on Language Models, 2024.\n[474] Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, and Dawn\nSong. Formal mathematical reasoning: A new frontier in AI. arXiv preprint arXiv:2412.16075, 2024.\n109\n"}, {"page": 110, "text": "Agentic Reasoning for Large Language Models\n[475] Jordan S Ellenberg, Cristofero S Fraser-Taliente, Thomas R Harvey, Karan Srivastava, and Andrew V\nSutherland. Generative modeling for mathematical discovery. arXiv preprint arXiv:2503.11061, 2025.\n[476] AlphaProof and AlphaGeometry teams. AI achieves silver-medal standard solving International\nMathematical Olympiad problems, 2024. URL https://deepmind.google/discover/blog/\nai-solves-imo-problems-at-silver-medal-level.\n[477] Kechi Zhang, Huangzhao Zhang, Ge Li, Jia Li, Zhuo Li, and Zhi Jin. ToolCoder: Teach code generation\nmodels to use API search tools, 2023.\n[478] Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, and Haonan Li. Toolgen: Unified tool\nretrieval and calling via generation. In International Conference on Learning Representations (ICLR),\n2025.\n[479] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with tool-\nintegrated agent systems for real-world repo-level coding challenges. In Proceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13643–13658,\n2024.\n[480] Xue Jiang, Yihong Dong, Yongding Tao, Huanyu Liu, Zhi Jin, Wenpin Jiao, and Ge Li. ROCODE: Inte-\ngrating backtracking mechanism and program analysis in large language models for code generation.\nIn IEEE/ACM International Conference on Software Engineering (ICSE), pages 670–670, 2025.\n[481] Yifei Lu, Fanghua Ye, Jian Li, Qiang Gao, Cheng Liu, Haibo Luo, Nan Du, Xiaolong Li, and Feiliang\nRen. CodeTool: Enhancing programmatic tool invocation of LLMs via process supervision, 2025.\n[482] Huy Nhat Phan, Hoang Nhat Phan, Tien N Nguyen, and Nghi DQ Bui. Repohyper: Search-expand-refine\non semantic graphs for repository-level code completion, 2024.\n[483] Manish Acharya, Yifan Zhang, Kevin Leach, and Yu Huang. Optimizing code runtime performance\nthrough context-aware retrieval-augmented generation. In 2025 IEEE/ACM 33rd International Confer-\nence on Program Comprehension (ICPC), pages 1–5. IEEE Computer Society, 2025.\n[484] Mihir Athale and Vishal Vaddina. Knowledge graph based repository-level code generation. In\nIEEE/ACM International Workshop on Large Language Models for Code (LLM4Code), pages 169–176,\n2025.\n[485] Yilin Zhang, Xinran Zhao, Zora Zhiruo Wang, Chenyang Yang, Jiayi Wei, and Tongshuang Wu. cAST:\nEnhancing code retrieval-augmented generation with structural chunking via abstract syntax tree,\n2025.\n[486] Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas\nLukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, et al. Evaluating language models\nfor mathematics through interactions. Proceedings of the National Academy of Sciences, 121(24):\ne2318124121, 2024.\n[487] Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. Self-edit: Fault-aware code editor for code generation.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 769–787, 2023.\n[488] Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama.\nIs self-repair a silver bullet for code generation?, 2024.\n110\n"}, {"page": 111, "text": "Agentic Reasoning for Large Language Models\n[489] Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya B Hossain, Baishakhi Ray, Varun Kumar,\nXiaofei Ma, and Anoop Deoras. Ledex: Training LLMs to better self-debug and explain code. In Neural\nInformation Processing Systems (NeurIPS), pages 35517–35543, 2024.\n[490] Tianyou Chang, Shizhan Chen, Guodong Fan, and Zhiyong Feng. A self-iteration code generation\nmethod based on large language models. In International Conference on Parallel and Distributed\nSystems (ICPADS), pages 275–281, 2023.\n[491] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to\nself-debug. arXiv preprint arXiv:2304.05128, 2023.\n[492] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. ACM\nTransactions on Software Engineering and Methodology, 33(7):1–38, 2024.\n[493] Samuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar. L2MAC: Large language model automatic\ncomputer for extensive code generation, 2023.\n[494] Yanlong Li, Jindong Li, Qi Wang, Menglin Yang, He Kong, and Shengsheng Wang. Cogito, ergo sum:\nA neurobiologically-inspired cognition-memory-growth system for code generation, 2025.\n[495] Dong Huang, Jie M Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, and Heming Cui. AgentCoder:\nMulti-agent-based code generation with iterative testing and optimisation, 2023.\n[496] Huan Zhang, Wei Cheng, Yuhan Wu, and Wei Hu. A pair programming framework for code generation\nvia multi-plan exploration and feedback-driven refinement. In Proceedings of the 39th IEEE/ACM\nInternational Conference on Automated Software Engineering, pages 1319–1331, 2024.\n[497] Feng Lin, Dong Jae Kim, et al. Soen-101: Code generation by emulating software process models\nusing large language model agents. In International Conference on Software Engineering (ICSE), pages\n1527–1539, 2025.\n[498] Yoichi Ishibashi and Yoshimasa Nishimura. Self-organized agents: A LLM multi-agent framework\ntoward ultra large-scale code generation and optimization, 2024.\n[499] Md Ashraful Islam, Mohammed Eunus Ali, and Md Rizwan Parvez. Mapcoder: Multi-agent code\ngeneration for competitive problem solving. arXiv preprint arXiv:2405.11403, 2024.\n[500] Ana Nunez, Nafis Tanveer Islam, Sumit Kumar Jha, and Peyman Najafirad. Autosafecoder: A multi-\nagent framework for securing llm code generation through static analysis and fuzz testing, 2024.\n[501] Yaojie Hu, Qiang Zhou, Qihong Chen, Xiaopeng Li, Linbo Liu, Dejiao Zhang, Amit Kachroo, Talha Oz,\nand Omer Tripp. Qualityflow: An agentic workflow for program synthesis controlled by llm quality\nchecks, 2025.\n[502] Siwei Liu, Jinyuan Fang, Han Zhou, Yingxu Wang, and Zaiqiao Meng. SEW: Self-evolving agentic\nworkflows for automated code generation, 2025.\n[503] Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei\nHuang, and Yongbin Li. Lingma SWE-GPT: An open development-process-centric language model for\nautomated software improvement, 2024.\n[504] Ruwei Pan, Hongyu Zhang, and Chao Liu. CodeCoR: An llm-based self-reflective multi-agent frame-\nwork for code generation, 2025.\n111\n"}, {"page": 112, "text": "Agentic Reasoning for Large Language Models\n[505] Xuehang Guo, Xingyao Wang, Yangyi Chen, Sha Li, Chi Han, Manling Li, and Heng Ji. Syncmind:\nMeasuring agent out-of-sync recovery in collaborative software engineering. In International Conference\non Machine Learning (ICML), 2025.\n[506] Qinghua Xu, Guancheng Wang, Lionel Briand, and Kui Liu. Hallucination to consensus: Multi-agent\nllms for end-to-end test generation, 2025.\n[507] Alireza Ghafarollahi and Markus J Buehler. Protagents: protein discovery via large language model\nmulti-agent collaborations combining physics and machine learning. Digital Discovery, 3(7):1389–\n1409, 2024.\n[508] Mehrad Ansari and Seyed Mohamad Moosavi. Agent-based learning of materials datasets from the\nscientific literature. Digital Discovery, 3(12):2607–2617, 2024.\n[509] Patrick Tser Jern Kon, Jiachen Liu, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth\nSrinivasa, Myungjin Lee, Mosharaf Chowdhury, and Ang Chen. Curie: Toward rigorous and automated\nscientific experimentation with ai agents. arXiv preprint arXiv:2502.16069, 2025.\n[510] Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang,\nYixin Cao, Aixin Sun, Hany Awadalla, et al. Sciagent: Tool-augmented language models for scientific\nreasoning. arXiv preprint arXiv:2402.11451, 2024.\n[511] Andrew D McNaughton, Gautham Krishna Sankar Ramalaxmi, Agustin Kruel, Carter R Knutson,\nRohith A Varikoti, and Neeraj Kumar. Cactus: Chemistry agent connecting tool usage to science. ACS\nomega, 9(46):46563–46573, 2024.\n[512] Botao Yu, Frazier N Baker, Ziru Chen, Garrett Herb, Boyu Gou, Daniel Adu-Ampratwum, Xia Ning,\nand Huan Sun. Chemtoolagent: The impact of tools on language agents for chemistry problem solving.\narXiv preprint arXiv:2411.07228, 2024.\n[513] Mengsong Wu, YaFei Wang, Yidong Ming, Yuqi An, Yuwei Wan, Wenliang Chen, Binbin Lin, Yuqiang\nLi, Tong Xie, and Dongzhan Zhou. Chemagent: Enhancing llms for chemistry and materials science\nthrough tree-search based tool learning. arXiv preprint arXiv:2506.07551, 2025.\n[514] Shanghua Gao, Richard Zhu, Zhenglun Kong, Ayush Noori, Xiaorui Su, Curtis Ginder, Theodoros\nTsiligkaridis, and Marinka Zitnik. Txagent: An ai agent for therapeutic reasoning across a universe of\ntools. arXiv preprint arXiv:2503.10970, 2025.\n[515] Qiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu, Donald Wright, Thomas Huang, Nikhil Khandekar,\nNicholas Wan, Xuguang Ai, W John Wilbur, et al. Agentmd: Empowering language agents for risk\nprediction with large-scale clinical tool learning. Nature Communications, 16(1):9377, 2025.\n[516] Jakub Lála, Odhran O’Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel G Rodriques, and An-\ndrew D White. Paperqa: Retrieval-augmented generative agent for scientific research. arXiv preprint\narXiv:2312.07559, 2023.\n[517] Michael D Skarlinski, Sam Cox, Jon M Laurent, James D Braza, Michaela Hinks, Michael J Hammerling,\nManvitha Ponnapati, Samuel G Rodriques, and Andrew D White. Language agents achieve superhuman\nsynthesis of scientific knowledge. arXiv preprint arXiv:2409.13740, 2024.\n112\n"}, {"page": 113, "text": "Agentic Reasoning for Large Language Models\n[518] Yuan Chiang, Elvis Hsieh, Chia-Hong Chou, and Janosh Riebesell. Llamp: Large language model\nmade powerful for high-fidelity materials knowledge retrieval and distillation.\narXiv preprint\narXiv:2401.17244, 2024.\n[519] Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, and Bang Liu. Honeycomb: A flexible llm-based\nagent system for materials science. arXiv preprint arXiv:2409.00135, 2024.\n[520] Yuanhao Qu, Kaixuan Huang, Ming Yin, Kanghong Zhan, Dyllan Liu, Di Yin, Henry C. Cousins,\nWilliam A. Johnson, Xiaotong Wang, Mihir Shah, Russ B. Altman, Denny Zhou, Mengdi Wang,\nand Le Cong. Crispr-gpt for agentic automation of gene-editing experiments, 2025. URL https:\n//arxiv.org/abs/2404.18021.\n[521] Bowen Gao, Yanwen Huang, Yiqiao Liu, Wenxuan Xie, Wei-Ying Ma, Ya-Qin Zhang, and Yanyan\nLan. Pharmagents: Building a virtual pharma with large language model agents. arXiv preprint\narXiv:2503.22164, 2025.\n[522] Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik Som, Miroslav Bogdanovic,\nYang Cao, Han Hao, Haoping Xu, Alán Aspuru-Guzik, et al. Organa: A robotic assistant for automated\nchemistry experimentation and characterization. Matter, 8(2), 2025.\n[523] Alireza Ghafarollahi and Markus J Buehler. Atomagents: Alloy design and discovery through physics-\naware multi-modal multi-agent artificial intelligence. arXiv preprint arXiv:2407.10022, 2024.\n[524] Kexin Chen, Junyou Li, Kunyi Wang, Yuyang Du, Jiahui Yu, Jiamin Lu, Lanqing Li, Jiezhong Qiu,\nJianzhang Pan, Yi Huang, et al. Chemist-x: Large language model-empowered agent for reaction\ncondition recommendation in chemical synthesis. arXiv preprint arXiv:2311.10776, 2023.\n[525] Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B Tenenbaum, Daniela Rus,\nChuang Gan, and Wojciech Matusik. Llm and simulation as bilevel optimizers: A new paradigm to\nadvance physical scientific discovery. arXiv preprint arXiv:2405.09783, 2024.\n[526] Yihang Xiao, Jinyi Liu, Yan Zheng, Xiaohan Xie, Jianye Hao, Mingzhi Li, Ruitao Wang, Fei Ni, Yuxiao\nLi, Jintian Luo, et al. Cellagent: An llm-driven multi-agent framework for automated single-cell data\nanalysis. arXiv preprint arXiv:2407.09811, 2024.\n[527] Yusuf Roohani, Andrew Lee, Qian Huang, Jian Vora, Zachary Steinhart, Kexin Huang, Alexander\nMarson, Percy Liang, and Jure Leskovec. Biodiscoveryagent: An ai agent for designing genetic\nperturbation experiments. arXiv preprint arXiv:2405.17631, 2024.\n[528] Yoshitaka Inoue, Tianci Song, Xinling Wang, Augustin Luna, and Tianfan Fu. Drugagent: Multi-agent\nlarge language model-based reasoning for drug-target interaction prediction. ArXiv, pages arXiv–2408,\n2025.\n[529] Joaquin Ramirez-Medina, Mohammadmehdi Ataei, and Alidad Amirfazli. Accelerating scientific\nresearch through a multi-llm framework. arXiv preprint arXiv:2502.07960, 2025.\n[530] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune,\nand David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree\nsearch. arXiv preprint arXiv:2504.08066, 2025.\n[531] Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou.\nLarge language models are zero shot hypothesis proposers. arXiv preprint arXiv:2311.05965, 2023.\n113\n"}, {"page": 114, "text": "Agentic Reasoning for Large Language Models\n[532] Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou,\nPan Lu, Zhuosheng Zhang, Yilun Zhao, et al. Chemagent: Self-updating library in large language\nmodels improves chemical reasoning. arXiv preprint arXiv:2501.06590, 2025.\n[533] Izumi Takahara, Teruyasu Mizoguchi, and Bang Liu. Accelerated inorganic materials design with\ngenerative ai agents. arXiv preprint arXiv:2504.00741, 2025.\n[534] Henry W Sprueill, Carl Edwards, Khushbu Agarwal, Mariefel V Olarte, Udishnu Sanyal, Conrad John-\nston, Hongbin Liu, Heng Ji, and Sutanay Choudhury. Chemreasoner: Heuristic search over a large lan-\nguage model’s knowledge space using quantum-chemical feedback. arXiv preprint arXiv:2402.10980,\n2024.\n[535] Shuyi Jia, Chao Zhang, and Victor Fung. Llmatdesign: Autonomous materials discovery with large\nlanguage models. arXiv preprint arXiv:2406.13163, 2024.\n[536] NovelSeek Team, Bo Zhang, Shiyang Feng, Xiangchao Yan, Jiakang Yuan, Zhiyin Yu, Xiaohan He,\nSongtao Huang, Shaowei Hou, Zheng Nie, et al. Novelseek: When agent becomes the scientist–building\nclosed-loop system from hypothesis to verification. arXiv preprint arXiv:2505.16938, 2025.\n[537] Shrinidhi Kumbhar, Venkatesh Mishra, Kevin Coutinho, Divij Handa, Ashif Iquebal, and Chitta Baral.\nHypothesis generation for materials discovery and design using goal-driven and constraint-guided llm\nagents. arXiv preprint arXiv:2501.13299, 2025.\n[538] Yingming Pu, Tao Lin, and Hongyu Chen. Piflow: Principle-aware scientific discovery with multi-agent\ncollaboration. arXiv preprint arXiv:2505.15047, 2025.\n[539] Haoyang Liu, Yijiang Li, Jinglin Jian, Yuxuan Cheng, Jianrong Lu, Shuyi Guo, Jinglei Zhu, Mianchen\nZhang, Miantong Zhang, and Haohan Wang. Toward a team of ai-made scientists for scientific\ndiscovery from gene expression data. arXiv preprint arXiv:2402.12391, 2024.\n[540] Kyle Swanson, Wesley Wu, Nash L Bulaong, John E Pak, and James Zou. The virtual lab: Ai agents\ndesign new sars-cov-2 nanobodies with experimental validation. bioRxiv, pages 2024–11, 2024.\n[541] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian D. Reid, and Niko Sünderhauf.\nSayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. In\nJie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 6-9\nNovember 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages\n23–72. PMLR, 2023. URL https://proceedings.mlr.press/v229/rana23a.html.\n[542] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai,\nYu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought.\nAdvances in Neural Information Processing Systems, 36:25081–25094, 2023.\n[543] Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, and Jonghyun Choi. Context-aware\nplanning and environment-aware memory for instruction following embodied agents. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 10936–10946, 2023.\n[544] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents. CoRR,\nabs/2302.01560, 2023. doi: 10.48550/ARXIV.2302.01560. URL https://doi.org/10.48550/\narXiv.2302.01560.\n114\n"}, {"page": 115, "text": "Agentic Reasoning for Large Language Models\n[545] Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic\ncontrol via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024.\n[546] Zhekai Duan, Yuan Zhang, Shikai Geng, Gaowen Liu, Joschka Boedecker, and Chris Xiaoxuan Lu. Fast\necot: Efficient embodied chain-of-thought via thoughts reuse. arXiv preprint arXiv:2506.07639, 2025.\n[547] Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen,\nJinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, et al. Cosmos-reason1: From physical common sense\nto embodied reasoning. arXiv preprint arXiv:2503.15558, 2025.\n[548] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li,\nQianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-\nlanguage-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference,\npages 1702–1713, 2025.\n[549] Qi Sun, Pengfei Hong, Tej Deep Pala, Vernon Toh, U Tan, Deepanway Ghosal, Soujanya Poria, et al.\nEmma-x: An embodied multimodal action model with grounded chain of thought and look-ahead\nspatial reasoning. arXiv preprint arXiv:2412.11974, 2024.\n[550] Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, and Younggyo Seo. Robot-r1:\nReinforcement learning for enhanced embodied reasoning in robotics. arXiv preprint arXiv:2506.00070,\n2025.\n[551] Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang,\nXiaoqing Zhang, Qian Jiang, Zhenhao Chen, et al. Maniplvm-r1: Reinforcement learning for reasoning\nin embodied manipulation with large vision-language models. arXiv preprint arXiv:2505.16517, 2025.\n[552] Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, and Zhenfei\nYin. Viki-r: Coordinating embodied multi-agent cooperation via reinforcement learning. arXiv preprint\narXiv:2506.09049, 2025.\n[553] Wenhao Wang, Yanyan Li, Long Jiao, and Jiawei Yuan. Gsce: A prompt framework with enhanced\nreasoning for reliable llm-driven drone control. In 2025 International Conference on Unmanned Aircraft\nSystems (ICUAS), pages 441–448. IEEE, 2025.\n[554] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents\nwith internet-scale knowledge. Advances in Neural Information Processing Systems, 35:18343–18362,\n2022.\n[555] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-\nChun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint\narXiv:2311.12871, 2023.\n[556] Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner,\nAnna Walling, Haohuan Wang, Niccolo Fusai, et al. Hi robot: Open-ended instruction following with\nhierarchical vision-language-action models. arXiv preprint arXiv:2502.19417, 2025.\n[557] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gon-\nzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl,\net al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025.\n115\n"}, {"page": 116, "text": "Agentic Reasoning for Large Language Models\n[558] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Haoran Tan, Chencheng Jiang, Jiamu\nKang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from\nenvironmental feedback. In European conference on computer vision, pages 20–38. Springer, 2024.\n[559] Jie Liu, Pan Zhou, Yingjun Du, Ah-Hwee Tan, Cees GM Snoek, Jan-Jakob Sonke, and Efstratios Gavves.\nCapo: Cooperative plan optimization for efficient embodied multi-agent cooperation. arXiv preprint\narXiv:2411.04679, 2024.\n[560] Kehui Liu, Zixin Tang, Dong Wang, Zhigang Wang, Xuelong Li, and Bin Zhao. Coherent: Collaboration\nof heterogeneous multi-robot system with large language models. arXiv preprint arXiv:2409.15146,\n2024.\n[561] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing\nShao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception. In 2024\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16307–16316. IEEE,\n2024.\n[562] Bangguo Yu, Hamidreza Kasaei, and Ming Cao. L3mvn: Leveraging large language models for visual\ntarget navigation. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),\npages 3554–3560. IEEE, 2023.\n[563] Abhinav Rajvanshi, Karan Sikka, Xiao Lin, Bhoram Lee, Han-Pang Chiu, and Alvaro Velasquez.\nSaynav: Grounding large language models for dynamic planning to navigation in new environments.\nIn Proceedings of the International Conference on Automated Planning and Scheduling, volume 34, pages\n464–474, 2024.\n[564] Abrar Anwar, John Welsh, Joydeep Biswas, Soha Pouya, and Yan Chang. Remembr: Building and\nreasoning over long-horizon spatio-temporal memory for robot navigation. In 2025 IEEE International\nConference on Robotics and Automation (ICRA), pages 2838–2845. IEEE, 2025.\n[565] Quanting Xie, So Yeon Min, Pengliang Ji, Yue Yang, Tianyi Zhang, Kedi Xu, Aarav Bajaj, Ruslan\nSalakhutdinov, Matthew Johnson-Roberson, and Yonatan Bisk. Embodied-rag: General non-parametric\nembodied memory for retrieval and generation. arXiv preprint arXiv:2409.18313, 2024. URL https:\n//www.arxiv.org/abs/2409.18313.\n[566] Yichen Zhu, Zhicai Ou, Xiaofeng Mou, and Jian Tang. Retrieval-augmented embodied agents. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17985–\n17995, 2024.\n[567] Junpeng Yue, Xinrun Xu, Börje F. Karlsson, and Zongqing Lu. Mllm as retriever: Interactively\nlearning multimodal retrieval for embodied agents. arXiv preprint arXiv:2410.03450, 2024. URL\nhttps://www.arxiv.org/abs/2410.03450.\n[568] Marc Glocker, Peter Hönig, Matthias Hirschmanner, and Markus Vincze. Llm-empowered embodied\nagent for memory-augmented task planning in household robotics. arXiv preprint arXiv:2504.21716,\n2025.\n[569] Gabriel Sarch, Yue Wu, Michael J Tarr, and Katerina Fragkiadaki. Open-ended instructable embodied\nagents with memory-augmented large language models. arXiv preprint arXiv:2310.15127, 2023.\n[570] Luo Ling and Bai Qianqian. Endowing embodied agents with spatial reasoning capabilities for\nvision-and-language navigation. arXiv preprint arXiv:2504.08806, 2025.\n116\n"}, {"page": 117, "text": "Agentic Reasoning for Large Language Models\n[571] Hongxin Zhang, Zheyuan Zhang, Zeyuan Wang, Zunzhe Zhang, Lixing Fang, Qinhong Zhou, and\nChuang Gan. Ella: Embodied social agents with lifelong memory. arXiv preprint arXiv:2506.24019,\n2025.\n[572] Yuanfei Wang, Xinju Huang, Fangwei Zhong, Yaodong Yang, Yizhou Wang, Yuanpei Chen, and Hao\nDong. Communication-efficient desire alignment for embodied agent-human adaptation, 2025. URL\nhttps://arxiv.org/abs/2505.22503.\n[573] Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu,\nLeila Takayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large\nlanguage model planners. arXiv preprint arXiv:2307.01928, 2023.\n[574] Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Zhen Wang, and Xuelong Li. Towards efficient\nllm grounding for embodied multi-agent collaboration. arXiv preprint arXiv:2405.14314, 2024.\n[575] Jesus Moncada-Ramirez, Jose-Luis Matez-Bandera, Javier Gonzalez-Jimenez, and Jose-Raul Ruiz-\nSarmiento. Agentic workflows for improving large language model reasoning in robotic object-centered\nplanning. Robotics, 14(3):24, 2025.\n[576] Shuang Ao, Flora D Salim, and Simon Khan. Emac+: Embodied multimodal agent for collaborative\nplanning with vlm+ llm. arXiv preprint arXiv:2505.19905, 2025.\n[577] Shyam Sundar Kannan, Vishnunandan LN Venkatesh, and Byung-Cheol Min. Smart-llm: Smart multi-\nagent robot task planning using large language models. In 2024 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 12140–12147. IEEE, 2024.\n[578] Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin Shu, Behzad Dariush,\nKwonjoon Lee, Yilun Du, and Chuang Gan. Combo: compositional world models for embodied multi-\nagent cooperation. arXiv preprint arXiv:2404.10775, 2024.\n[579] Mandi Zhao, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large\nlanguage models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages\n286–299. IEEE, 2024.\n[580] Dyke Ferber, Omar SM El Nahhas, Georg Wölflein, Isabella C Wiest, Jan Clusmann, Marie-Elisabeth Leß-\nman, Sebastian Foersch, Jacqueline Lammert, Maximilian Tschochohei, Dirk Jäger, et al. Autonomous\nartificial intelligence agents for clinical decision making in oncology. arXiv preprint arXiv:2404.04667,\n2024.\n[581] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl\nYang, and May D. Wang. Ehragent: Code empowers large language models for few-shot complex\ntabular reasoning on electronic health records. arXiv preprint arXiv:2401.07128, 2024. URL https:\n//www.arxiv.org/abs/2401.07128.\n[582] Yexiao He, Ang Li, Boyi Liu, Zhewei Yao, and Yuxiong He. Medorch: Medical diagnosis with tool-\naugmented reasoning agents for flexible extensibility. arXiv preprint arXiv:2506.00235, 2025.\n[583] Ling Yue, Sixue Xing, Jintai Chen, and Tianfan Fu. Clinicalagent: Clinical trial multi-agent system\nwith large language model-based reasoning. In Proceedings of the 15th ACM International Conference\non Bioinformatics, Computational Biology and Health Informatics, pages 1–10, 2024.\n117\n"}, {"page": 118, "text": "Agentic Reasoning for Large Language Models\n[584] Yichun Feng, Jiawei Wang, Lu Zhou, Zhen Lei, and Yixue Li. Doctoragent-rl: A multi-agent collaborative\nreinforcement learning system for multi-turn clinical dialogue. arXiv preprint arXiv:2505.19630, 2025.\n[585] Tianqi Shang, Weiqing He, Charles Zheng, Lingyao Li, Li Shen, and Bingxin Zhao. Dynamicare:\nA dynamic multi-agent framework for interactive and open-ended medical decision-making. arXiv\npreprint arXiv:2507.02616, 2025.\n[586] Alex J Goodell, Simon N Chu, Dara Rouholiman, and Larry F Chu. Large language model agents can\nuse tools to perform clinical calculations. npj Digital Medicine, 8(1):163, 2025.\n[587] Yakun Zhu, Shaohang Wei, Xu Wang, Kui Xue, Xiaofan Zhang, and Shaoting Zhang. Menti: Bridging\nmedical calculator and llm agent with nested tool calling. arXiv preprint arXiv:2410.13610, 2024.\n[588] Andrew Hoopes. Voxelprompt: A vision-language agent for grounded medical image analysis. PhD\nthesis, Massachusetts Institute of Technology, 2025.\n[589] Huan Xu, Jinlin Wu, Guanglin Cao, Zhen Lei, Zhen Chen, and Hongbin Liu. Enhancing surgical robots\nwith embodied intelligence for autonomous ultrasound scanning. arXiv preprint arXiv:2405.00461,\n2024.\n[590] Abhishek Dutta and Yen-Che Hsiao. Adaptive reasoning and acting in medical language agents. arXiv\npreprint arXiv:2410.10020, 2024.\n[591] Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, and Bo Wang. Medrax: Medical reasoning\nagent for chest x-ray. arXiv preprint arXiv:2502.02673, 2025.\n[592] Mahyar Abbasian, Iman Azimi, Amir M Rahmani, and Ramesh Jain. Conversational health agents: A\npersonalized llm-powered agent framework. arXiv preprint arXiv:2310.02374, 2023.\n[593] Ran Xu, Yuchen Zhuang, Yishan Zhong, Yue Yu, Zifeng Wang, Xiangru Tang, Hang Wu, May D. Wang,\nPeifeng Ruan, Donghan Yang, Tao Wang, Guanghua Xiao, Xin Liu, Carl Yang, Yang Xie, and Wenqi\nShi. Medagentgym: A scalable agentic training environment for code-centric reasoning in biomedical\ndata science, 2025. URL https://arxiv.org/abs/2506.04405.\n[594] Huizi Yu, Jiayan Zhou, Lingyao Li, Shan Chen, Jack Gallifant, Anye Shi, Jie Sun, Xiang Li, Jingxian\nHe, Wenyue Hua, et al. Simulated patient systems powered by large language model-based ai agents\noffer potential for transforming medical education. Communications Medicine, 2025.\n[595] Mohammad Almansoori, Komal Kumar, and Hisham Cholakkal. Self-evolving multi-agent simulations\nfor realistic clinical interactions. arXiv preprint arXiv:2503.22678, 2025.\n[596] Namkyeong Lee, Edward De Brouwer, Ehsan Hajiramezanali, Tommaso Biancalani, Chanyoung\nPark, and Gabriele Scalia. Rag-enhanced collaborative llm agents for drug discovery. arXiv preprint\narXiv:2502.17506, 2025.\n[597] Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu,\nHyunjin Cho, Chang-In Choi, et al. Medreason: Eliciting factual medical reasoning steps in llms via\nknowledge graphs. arXiv preprint arXiv:2504.00993, 2025.\n[598] Ross Williams, Niyousha Hosseinichimeh, Aritra Majumdar, and Navid Ghaffarzadegan. Epidemic\nmodeling with generative agents. arXiv preprint arXiv:2307.04986, 2023.\n118\n"}, {"page": 119, "text": "Agentic Reasoning for Large Language Models\n[599] Zhuoyun Du, Lujie Zheng, Renjun Hu, Yuyang Xu, Xiawei Li, Ying Sun, Wei Chen, Jian Wu, Haolei\nCai, and Haohao Ying. Llms can simulate standardized patients via agent coevolution. arXiv preprint\narXiv:2412.11716, 2024.\n[600] Haochun Wang, Sendong Zhao, Zewen Qiang, Nuwa Xi, Bing Qin, and Ting Liu. Beyond direct\ndiagnosis: Llm-based multi-specialist agent consultation for automatic diagnosis. arXiv preprint\narXiv:2401.16107, 2024.\n[601] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and\nMark Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning.\narXiv preprint arXiv:2311.10537, 2023. URL https://www.arxiv.org/abs/2311.10537.\n[602] Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng\nChen, Ming Hu, et al.\nGmai-vl-r1: Harnessing reinforcement learning for multimodal medical\nreasoning. arXiv preprint arXiv:2504.01886, 2025.\n[603] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot\nlearners. Advances in neural information processing systems, 33:1877–1901, 2020.\n[604] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614, 2024.\n[605] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang,\nXiaohan Zhang, Yuxiao Dong, et al. Autowebglm: A large language model-based web navigating\nagent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,\npages 5295–5306, 2024.\n[606] Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du,\nQidi Xu, et al. Webexplorer: Explore and evolve for training long-horizon web agents. arXiv preprint\narXiv:2509.06501, 2025.\n[607] Lucas-Andrei Thil, Mirela Popa, and Gerasimos Spanakis. Navigating webai: Training agents to\ncomplete web tasks with large language models and reinforcement learning. In Proceedings of the\n39th ACM/SIGAPP Symposium on Applied Computing, pages 866–874, 2024.\n[608] Wenxuan Shi, Haochen Tan, Chuqiao Kuang, Xiaoguang Li, Xiaozhe Ren, Chen Zhang, Hanting Chen,\nYasheng Wang, Lifeng Shang, Fisher Yu, et al. Pangu deepdiver: Adaptive search intensity scaling via\nopen-web reinforcement learning. arXiv preprint arXiv:2505.24332, 2025.\n[609] Ding-Chu Zhang, Yida Zhao, Jialong Wu, Liwen Zhang, Baixuan Li, Wenbiao Yin, Yong Jiang, Yu-Feng\nLi, Kewei Tu, Pengjun Xie, et al. Evolvesearch: An iterative self-evolving search agent. In Proceedings\nof the 2025 Conference on Empirical Methods in Natural Language Processing, pages 13134–13147,\n2025.\n[610] Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, and Dong Yu.\nWebevolver: Enhancing web agent self-improvement with coevolving world model. arXiv preprint\narXiv:2504.21024, 2025.\n[611] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language\nmodel agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.\n119\n"}, {"page": 120, "text": "Agentic Reasoning for Large Language Models\n[612] Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, and\nLi Erran Li. Proposer-agent-evaluator (pae): Autonomous skill discovery for foundation model internet\nagents. In Forty-second International Conference on Machine Learning, 2025.\n[613] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu,\nand Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv\npreprint arXiv:2402.07456, 2024.\n[614] Yuhang Liu, Pengxiang Li, Zishu Wei, Congkai Xie, Xueyu Hu, Xinchen Xu, Shengyu Zhang, Xiaotian\nHan, Hongxia Yang, and Fei Wu. Infiguiagent: A multimodal generalist gui agent with native reasoning\nand reflection. arXiv preprint arXiv:2501.04575, 2025.\n[615] Zichen Zhu, Hao Tang, Yansi Li, Dingye Liu, Hongshen Xu, Kunyao Lan, Danyang Zhang, Yixuan Jiang,\nHao Zhou, Chenrun Wang, et al. Moba: multifaceted memory-enhanced adaptive planning for efficient\nmobile task automation. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (System Demonstrations),\npages 535–549, 2025.\n[616] Haowei Liu, Xi Zhang, Haiyang Xu, Yuyang Wanyan, Junyang Wang, Ming Yan, Ji Zhang, Chun-\nfeng Yuan, Changsheng Xu, Weiming Hu, et al. Pc-agent: A hierarchical multi-agent collaboration\nframework for complex task automation on pc. arXiv preprint arXiv:2502.14282, 2025.\n[617] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen\nDing, Liheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents.\narXiv preprint arXiv:2410.23218, 2024.\n[618] Xiaoqiang Wang and Bang Liu. Oscar: Operating system control via state-aware reasoning and\nre-planning. arXiv preprint arXiv:2410.18963, 2024.\n[619] Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong, Lei Chen, Longrong Yang,\nYingjie Chu, Yuzhi He, and Lin Ma. Uitron: Foundational gui agent with advanced perception and\nplanning. arXiv preprint arXiv:2508.21767, 2025.\n[620] Fanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Arpo:end-to-end policy optimization\nfor gui agents with experience replay. arXiv preprint arXiv:2505.16282, 2025. URL https://www.\narxiv.org/abs/2505.16282.\n[621] Hanyu Lai, Xiao Liu, Yanxiao Zhao, Han Xu, Hanchen Zhang, Bohao Jing, Yanyu Ren, Shuntian\nYao, Yuxiao Dong, and Jie Tang. Computerrl: Scaling end-to-end online reinforcement learning for\ncomputer use agents. arXiv preprint arXiv:2508.14040, 2025. URL https://www.arxiv.org/abs/\n2508.14040.\n[622] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing\nXiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning.\narXiv preprint arXiv:2503.21620, 2025. URL https://www.arxiv.org/abs/2503.21620.\n[623] Run Luo, Lu Wang, Wanwei He, Longze Chen, Jiaming Li, and Xiaobo Xia. Gui-r1: A generalist\nr1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025.\n[624] Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and\nFei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners.\narXiv preprint arXiv:2504.14239, 2025. URL https://www.arxiv.org/abs/2504.14239.\n120\n"}, {"page": 121, "text": "Agentic Reasoning for Large Language Models\n[625] Zhengxi Lu, Jiabo Ye, Fei Tang, Yongliang Shen, Haiyang Xu, Ziwei Zheng, Weiming Lu, Ming\nYan, Fei Huang, Jun Xiao, and Yueting Zhuang. Ui-s1: Advancing gui automation via semi-online\nreinforcement learning. arXiv preprint arXiv:2509.11543, 2025. URL https://www.arxiv.org/\nabs/2509.11543.\n[626] Yue Fan, Handong Zhao, Ruiyi Zhang, Yu Shen, Xin Eric Wang, and Gang Wu. Gui-bee: Align gui\naction grounding to novel environments via autonomous exploration. arXiv preprint arXiv:2501.13896,\n2025. URL https://www.arxiv.org/abs/2501.13896.\n[627] Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin\nHou, Jinwei Chen, Peng-Tao Jiang, and Bo Li.\nEnhancing visual grounding for gui agents via\nself-evolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025. URL https:\n//www.arxiv.org/abs/2505.12370.\n[628] Longxi Gao, Li Zhang, Pengzhi Gao, Wei Liu, Jian Luan, and Mengwei Xu. Gui-shift: Enhancing\nvlm-based gui agents through self-supervised reinforcement learning, 2025. URL https://arxiv.\norg/abs/2505.12493.\n[629] Shuquan Lian, Yuhang Wu, Jia Ma, Yifan Ding, Zihan Song, Bingqi Chen, Xiawu Zheng, and Hui\nLi. Ui-agile: Advancing gui agents with effective reinforcement learning and precise inference-time\ngrounding. arXiv preprint arXiv:2507.22025, 2025. URL https://www.arxiv.org/abs/2507.\n22025.\n[630] Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu,\nJinguo Zhu, Hao Li, Wenhai Wang, Yu Qiao, Xizhou Zhu, and Jifeng Dai. Zerogui: Automating\nonline gui learning at zero human cost. arXiv preprint arXiv:2505.23762v1, 2025. URL https:\n//www.arxiv.org/abs/2505.23762v1.\n[631] Zhang Zhong, Lu Yaxi, Fu Yikun, Huo Yupeng, Yang Shenzhi, Wu Yesai, Si Han, Cong Xin, Chen\nHaotian, Lin Yankai, Xie Jie, Zhou Wei, Xu Wang, Zhang Yuanheng, Su Zhou, Zhai Zhongwu, Liu\nXiaoming, Mei Yudong, Xu Jianming, Tian Hongyan, Wang Chongyi, Chen Chi, Yao Yuan, Liu Zhiyuan,\nand Sun Maosong. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning. arXiv\npreprint arXiv:2506.01391v2, 2025. URL https://www.arxiv.org/abs/2506.01391v2.\n[632] Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long\nIong, Jiadai Sun, Jiaqi Wang, Junjie Gao, Junjun Shan, Kangning Liu, Shudan Zhang, Shuntian Yao,\nSiyi Cheng, Wentao Yao, Wenyi Zhao, Xinghan Liu, Xinyi Liu, Xinying Chen, Xinyue Yang, Yang\nYang, Yifan Xu, Yu Yang, Yujia Wang, Yulin Xu, Zehan Qi, Yuxiao Dong, and Jie Tang. Autoglm:\nAutonomous foundation agents for guis.\narXiv preprint arXiv:2411.00820, 2024.\nURL https:\n//www.arxiv.org/abs/2411.00820.\n[633] Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao,\nJunjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, and Ming Yan. Mobile-\nagent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. URL\nhttps://www.arxiv.org/abs/2508.15144.\n[634] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Michael\nMoor, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants.\narXiv preprint arXiv:2501.04227, 2025.\n121\n"}, {"page": 122, "text": "Agentic Reasoning for Large Language Models\n[635] Assaf Elovic.\ngpt-researcher,\nJuly 2023.\nURL https://github.com/assafelovic/\ngpt-researcher.\n[636] Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming\nJiang, Yifei Xin, Ronghao Dang, et al. Chain of ideas: Revolutionizing research via novel idea\ndevelopment with llm agents. arXiv preprint arXiv:2410.13185, 2024.\n[637] Aniketh Garikaparthi, Manasi Patwardhan, Lovekesh Vig, and Arman Cohan. Iris: Interactive research\nideation system for accelerating scientific discovery. arXiv preprint arXiv:2504.16728, 2025.\n[638] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan,\nand Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv\npreprint arXiv:2401.13919, 2024.\n[639] Zhengbo Zhang, Zhiheng Lyu, Junhao Gong, Hongzhu Yi, Xinming Wang, Yuxuan Zhou, Jiabing Yang,\nPing Nie, Yan Huang, and Wenhu Chen. Browseragent: Building web agents with human-inspired\nweb browsing actions. arXiv preprint arXiv:2510.10666, 2025.\n[640] Viraj Prabhu, Yutong Dai, Matthew Fernandez, Jing Gu, Krithika Ramakrishnan, Yanqi Luo, Silvio\nSavarese, Caiming Xiong, Junnan Li, Zeyuan Chen, et al. Walt: Web agents that learn tools. arXiv\npreprint arXiv:2510.01524, 2025.\n[641] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang,\nZekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking agency.\narXiv preprint arXiv:2505.22648, 2025.\n[642] Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen\nZhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-\nseeking formalization. arXiv preprint arXiv:2507.15061, 2025.\n[643] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu,\nYaqin Zhang, and Yunxin Liu. Autodroid: Llm-powered task automation in android. In Proceedings of\nthe 30th Annual International Conference on Mobile Computing and Networking, pages 543–557, 2024.\n[644] Hao Wen, Shizuo Tian, Borislav Pavlov, Wenjie Du, Yixuan Li, Ge Chang, Shanhui Zhao, Jiacheng Liu,\nYunxin Liu, Ya-Qin Zhang, et al. Autodroid-v2: Boosting slm-based gui agents via code generation. In\nProceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services,\npages 223–235, 2025.\n[645] Jiayi Zhang, Chuang Zhao, Yihan Zhao, Zhaoyang Yu, Ming He, and Jianping Fan. Mobileexperts: A\ndynamic tool-enabled agent team in mobile devices. arXiv preprint arXiv:2407.03913, 2024.\n[646] Chengyou Jia, Minnan Luo, Zhuohang Dang, Qiushi Sun, Fangzhi Xu, Junlin Hu, Tianbao Xie, and\nZhiyong Wu. Agentstore: Scalable integration of heterogeneous agents as specialized generalist\ncomputer assistant. In Findings of the Association for Computational Linguistics: ACL 2025, pages\n8908–8934, 2025.\n[647] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and\nZhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv\npreprint arXiv:2504.21776, 2025.\n122\n"}, {"page": 123, "text": "Agentic Reasoning for Large Language Models\n[648] Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, and Daniel S Weld.\nScideator: Human-llm scientific idea generation grounded in research-paper facet recombination.\narXiv preprint arXiv:2409.14634, 2024.\n[649] Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. Mlr-copilot: Autonomous machine learning\nresearch based on large language models agents. arXiv preprint arXiv:2408.14033, 2024.\n[650] Jiakang Yuan, Xiangchao Yan, Shiyang Feng, Bo Zhang, Tao Chen, Botian Shi, Wanli Ouyang, Yu Qiao,\nLei Bai, and Bowen Zhou. Dolphin: Moving towards closed-loop auto-research through thinking,\npractice, and feedback. arXiv preprint arXiv:2501.03916, 2025.\n[651] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist:\nTowards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292v3, 2024.\nURL https://www.arxiv.org/abs/2408.06292v3.\n[652] Revanth Gangi Reddy, Sagnik Mukherjee, Jeonghwan Kim, Zhenhailong Wang, Dilek Hakkani-Tur,\nand Heng Ji. Infogent: An agent-based framework for web information aggregation. In Findings of the\nAssociation for Computational Linguistics: NAACL 2025, pages 5745–5758, 2025.\n[653] Minsoo Kim, Victor Bursztyn, Eunyee Koh, Shunan Guo, and Seung-won Hwang. Rada: Retrieval-\naugmented web agent planning with llms. In Findings of the Association for Computational Linguistics\nACL 2024, pages 13511–13525, 2024.\n[654] Anonymous. WebRAGent: Retrieval-augmented generation for multimodal web agent planning.\nIn Submitted to The Fourteenth International Conference on Learning Representations, 2025. URL\nhttps://openreview.net/forum?id=L1VPZFbAcu. under review.\n[655] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting\nwith memory for computer control. arXiv preprint arXiv:2306.07863, 2023.\n[656] Guangyi Liu, Pengxiang Zhao, Liang Liu, Zhiming Chen, Yuxiang Chai, Shuai Ren, Hao Wang, Shibo\nHe, and Wenchao Meng. Learnact: Few-shot mobile gui agent with a unified demonstration benchmark.\narXiv preprint arXiv:2504.13805, 2025.\n[657] Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steven Y Ko, Sangeun\nOh, and Insik Shin. Explore, select, derive, and recall: Augmenting llm with human-like memory for\nmobile task automation. arXiv preprint arXiv:2312.03003, 2023.\n[658] Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu,\nSong-Chun Zhu, and Qing Li. Tongui: Internet-scale trajectories from multimodal web tutorials for\ngeneralized gui agents, 2025. URL https://arxiv.org/abs/2504.12679.\n[659] Ran Xu, Kaixin Ma, Wenhao Yu, Hongming Zhang, Joyce C Ho, Carl Yang, and Dong Yu. Retrieval-\naugmented gui agents with generative guidelines. In Proceedings of the 2025 Conference on Empirical\nMethods in Natural Language Processing, pages 17877–17886, 2025.\n[660] Gabriel Sarch, Lawrence Jang, Michael Tarr, William W Cohen, Kenneth Marino, and Katerina\nFragkiadaki. Vlm agents generate their own memories: Distilling experience into embodied programs\nof thought. Advances in Neural Information Processing Systems, 37:75942–75985, 2024.\n123\n"}, {"page": 124, "text": "Agentic Reasoning for Large Language Models\n[661] Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa\nRangwala. Agentoccam: A simple yet strong baseline for llm-based web agents. arXiv preprint\narXiv:2410.13825, 2024.\n[662] Danqing Zhang, Balaji Rama, Jingyi Ni, Shiying He, Fu Zhao, Kunyu Chen, Arnold Chen, and Junyu\nCao. Litewebagent: The open-source suite for vlm-based web-agent applications. arXiv preprint\narXiv:2503.02950, 2025.\n[663] Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steve Ko, Sangeun Oh,\nand Insik Shin. Mobilegpt: Augmenting llm with human-like app memory for mobile task automation.\nIn Proceedings of the 30th Annual International Conference on Mobile Computing and Networking, pages\n1119–1133, 2024.\n[664] Xinzge Gao, Chuanrui Hu, Bin Chen, and Teng Li. Chain-of-memory: Enhancing gui agents for\ncross-application navigation. arXiv preprint arXiv:2506.18158, 2025.\n[665] Weihua Cheng, Ersheng Ni, Wenlong Wang, Yifei Sun, Junming Liu, Wangyu Shen, Yirong Chen,\nBotian Shi, and Ding Wang. Mga: Memory-driven gui agent for observation-centric interaction. arXiv\npreprint arXiv:2510.24168, 2025.\n[666] Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng\nJi. Mobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733,\n2025.\n[667] Yuquan Xie, Zaijing Li, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Dongmei Jiang, and\nLiqiang Nie. Mirage-1: Augmenting and updating gui agent with hierarchical multimodal skills. arXiv\npreprint arXiv:2506.10387, 2025.\n[668] Ruhana Azam, Aditya Vempaty, and Ashish Jagmohan. Reflection-based memory for web navigation\nagents. arXiv preprint arXiv:2506.02158, 2025.\n[669] Ruhana Azam, Tamer Abuelsaad, Aditya Vempaty, and Ashish Jagmohan. Multimodal auto validation\nfor self-refinement in web agents. arXiv preprint arXiv:2410.00689, 2024.\n[670] Kaiwen He, Zhiwei Wang, Chenyi Zhuang, and Jinjie Gu. Recon-act: A self-evolving multi-agent\nbrowser-use system via web reconnaissance, tool generation, and task execution. arXiv preprint\narXiv:2509.21072, 2025.\n[671] Revanth Gangi Reddy, Tanay Dixit, Jiaxin Qin, Cheng Qian, Daniel Lee, Jiawei Han, Kevin Small, Xing\nFan, Ruhi Sarikaya, and Heng Ji. Winell: wikipedia never-ending updating with llm agents. arXiv\npreprint arXiv:2508.03728, 2025.\n[672] Guanzhong He, Zhen Yang, Jinxin Liu, Bin Xu, Lei Hou, and Juanzi Li. Webseer: Training deeper\nsearch agents through reinforcement learning with self-reflection. arXiv preprint arXiv:2510.18798,\n2025.\n[673] Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, and Yang Li. A zero-shot language agent for computer\ncontrol with structured reflection. arXiv preprint arXiv:2310.08740, 2023.\n[674] Penghao Wu, Shengnan Ma, Bo Wang, Jiaheng Yu, Lewei Lu, and Ziwei Liu. Gui-reflection: Em-\npowering multimodal gui models with self-reflection behavior. arXiv preprint arXiv:2506.08012,\n2025.\n124\n"}, {"page": 125, "text": "Agentic Reasoning for Large Language Models\n[675] Ziwei Wang, Leyang Yang, Xiaoxuan Tang, Sheng Zhou, Dajun Chen, Wei Jiang, and Yong Li. History-\naware reasoning for gui agents. arXiv preprint arXiv:2511.09127, 2025.\n[676] Ning Li, Xiangmou Qu, Jiamu Zhou, Jun Wang, Muning Wen, Kounianhua Du, Xingyu Lou, Qiuying\nPeng, and Weinan Zhang. Mobileuse: A gui agent with hierarchical reflection for autonomous mobile\noperation. arXiv preprint arXiv:2507.16853, 2025.\n[677] Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and\nLinyi Yang. Cycleresearcher: Improving automated research via automated review. arXiv preprint\narXiv:2411.00816, 2024.\n[678] Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan\nXie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai\nChen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang,\nYangqiu Song, Irwin King, and Philip S. Yu.\nFrom web search towards agentic deep research:\nIncentivizing search with reasoning agents. arXiv preprint arXiv:2506.18959v3, 2025. URL https:\n//www.arxiv.org/abs/2506.18959v3.\n[679] Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: A versatile and\nautonomous multi-agent system for web task execution with strategic exploration. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 39, pages 23378–23386, 2025.\n[680] Yingxuan Yang, Mulei Ma, Yuxuan Huang, Huacan Chai, Chenyu Gong, Haoran Geng, Yuanjian Zhou,\nYing Wen, Meng Fang, Muhao Chen, et al. Agentic web: Weaving the next web with ai agents. arXiv\npreprint arXiv:2507.21206, 2025.\n[681] Di Zhao, Longhui Ma, Siwei Wang, Miao Wang, and Zhao Lv. Cola: A scalable multi-agent framework\nfor windows ui task automation. arXiv preprint arXiv:2503.09263, 2025.\n[682] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang,\nand Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via\nmulti-agent collaboration. Advances in Neural Information Processing Systems, 37:2686–2710, 2024.\n[683] Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-\nv: A video-guided approach for effortless and efficient operational knowledge injection in mobile\nautomation, 2025. URL https://arxiv.org/abs/2502.17110.\n[684] Quanfeng Lu, Zhantao Ma, Shuai Zhong, Jin Wang, Dahai Yu, Michael K Ng, and Ping Luo. Swirl:\nA staged workflow for interleaved reinforcement learning in mobile gui control. arXiv preprint\narXiv:2508.20018, 2025.\n[685] Samuel Schmidgall and Michael Moor. Agentrxiv: Towards collaborative autonomous research. arXiv\npreprint arXiv:2503.18102, 2025.\n[686] Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with\nlarge language models. Nature, 624(7992):570–578, 2023.\n[687] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou,\nYufei Huang, Chaojun Xiao, Chi Han, Yi R. Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu\nTian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei\nTang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang,\n125\n"}, {"page": 126, "text": "Agentic Reasoning for Large Language Models\nJunxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Guoliang\nLi, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. ACM Comput. Surv., 57(4):\n101:1–101:40, 2025. doi: 10.1145/3704435. URL https://doi.org/10.1145/3704435.\n[688] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset for LLM\nquestion answering with external tools. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,\nMoritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual\nConference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, De-\ncember 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/\n9cb2a7495900f8b602cb10159246a016-Abstract-Datasets_and_Benchmarks.html.\n[689] Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou,\nYao Wan, Neil Zhenqiang Gong, and Lichao Sun. Metatool benchmark for large language models:\nDeciding whether to use tools and which to use. In The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https:\n//openreview.net/forum?id=R0c2qtalgG.\n[690] Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo,\nSongyang Zhang, Dahua Lin, Kai Chen, and Feng Zhao. T-eval: Evaluating the tool utilization\ncapability of large language models step by step. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar,\neditors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 9510–9529.\nAssociation for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.515. URL\nhttps://doi.org/10.18653/v1/2024.acl-long.515.\n[691] Jize Wang, Zerun Ma, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. Gta: A\nbenchmark for general tool agents. arXiv preprint arXiv:2407.08713, 2024. URL https://www.\narxiv.org/abs/2407.08713.\n[692] Zhengliang Shi, Yuhan Wang, Lingyong Yan, Pengjie Ren, Shuaiqiang Wang, Dawei Yin, and Zhaochun\nRen. Retrieval models aren’t tool-savvy: Benchmarking tool retrieval for large language models. CoRR,\nabs/2503.01763, 2025. doi: 10.48550/ARXIV.2503.01763. URL https://doi.org/10.48550/\narXiv.2503.01763.\n[693] Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool\nmanipulation capability of open-source large language models. CoRR, abs/2305.16504, 2023. doi:\n10.48550/ARXIV.2305.16504. URL https://doi.org/10.48550/arXiv.2305.16504.\n[694] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. In Houda Bouamor,\nJuan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 3102–3116.\nAssociation for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.187. URL\nhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n[695] Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan\nZeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, and Qun Liu. Planning, creation, usage:\nBenchmarking llms for comprehensive tool utilization in real-world complex scenarios. In Lun-\nWei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational\nLinguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 4363–4400.\n126\n"}, {"page": 127, "text": "Agentic Reasoning for Large Language Models\nAssociation for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.259. URL\nhttps://doi.org/10.18653/v1/2024.findings-acl.259.\n[696] Pei Wang, Yanan Wu, Noah Wang, Jiaheng Liu, Xiaoshuai Song, Z. Y. Peng, Ken Deng, Chenchen\nZhang, Jiakai Wang, Junran Peng, Ge Zhang, Hangyu Guo, Zhaoxiang Zhang, Wenbo Su, and\nBo Zheng. Mtu-bench: A multi-granularity tool-use benchmark for large language models. In The\nThirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28,\n2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=6guG2OlXsr.\n[697] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan\nHe, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint\narXiv:2501.07572, 2025.\n[698] Yunjia Xi, Jianghao Lin, Menghui Zhu, Yongzhao Xiao, Zhuoying Ou, Jiaqi Liu, Tong Wan, Bo Chen,\nWeiwen Liu, Yasheng Wang, et al. Infodeepseek: Benchmarking agentic information seeking for\nretrieval-augmented generation. arXiv preprint arXiv:2505.15872, 2025.\n[699] Yilong Xu, Xiang Long, Zhi Zheng, and Jinhua Gao. Ravine: Reality-aligned evaluation for agentic\nsearch. arXiv preprint arXiv:2507.16725, 2025.\n[700] Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang,\nKai Xiang, Ge Zhang, et al. Widesearch: Benchmarking agentic broad info-seeking. arXiv preprint\narXiv:2508.07999, 2025.\n[701] Tian Lan, Bin Zhu, Qianghuai Jia, Junyang Ren, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo,\nand Kaifu Zhang. Deepwidesearch: Benchmarking depth and width in agentic information seeking.\narXiv preprint arXiv:2510.20168, 2025.\n[702] Shan Chen, Pedro Moreira, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas\nHartvigsen, Jack Gallifant, and Danielle S Bitterman. Medbrowsecomp: Benchmarking medical deep\nresearch and computer use. arXiv preprint arXiv:2505.14963, 2025.\n[703] Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang,\nJaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, et al. Finagentbench: A benchmark dataset for\nagentic retrieval in financial question answering. In Proceedings of the 6th ACM International Conference\non AI in Finance, pages 632–637, 2025.\n[704] Hang He, Chuhuai Yue, Chengqi Dong, Mingxue Tian, Zhenfeng Liu, Jiajun Chai, Xiaohan Wang, Yufei\nZhang, Qun Liao, Guojun Yin, et al. Localsearchbench: Benchmarking agentic search in real-world\nlocal life services. arXiv preprint arXiv:2512.07436, 2025.\n[705] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen,\nChaoyou Fu, Guanglu Song, et al.\nMmsearch: Benchmarking the potential of large models as\nmulti-modal search engines. arXiv preprint arXiv:2409.12959, 2024.\n[706] Xijia Tao, Yihua Teng, Xinxing Su, Xinyu Fu, Jihao Wu, Chaofan Tao, Ziru Liu, Haoli Bai, Rui Liu, and\nLingpeng Kong. Mmsearch-plus: Benchmarking provenance-aware search for multimodal browsing\nagents. arXiv preprint arXiv:2508.21475, 2025.\n[707] Shilong Li, Xingyuan Bu, Wenjie Wang, Jiaheng Liu, Jun Dong, Haoyang He, Hao Lu, Haozhe Zhang,\nChenchen Jing, Zhen Li, et al. Mm-browsecomp: A comprehensive benchmark for multimodal\nbrowsing agents. arXiv preprint arXiv:2508.13186, 2025.\n127\n"}, {"page": 128, "text": "Agentic Reasoning for Large Language Models\n[708] Yixiao Song, Katherine Thai, Chau Minh Pham, Yapei Chang, Mazin Nadaf, and Mohit Iyyer. Bearcubs:\nA benchmark for computer-using web agents. arXiv preprint arXiv:2503.07919, 2025.\n[709] Daoyu Wang, Mingyue Cheng, Shuo Yu, Zirui Liu, Ze Guo, and Qi Liu. Paperarena: An evaluation bench-\nmark for tool-augmented agentic reasoning on scientific literature. arXiv preprint arXiv:2510.10909,\n2025.\n[710] Zhengyang Liang, Yan Shu, Xiangrui Liu, Minghao Qin, Kaixin Liang, Paolo Rota, Nicu Sebe, Zheng\nLiu, and Lizi Liao. Video-browsecomp: Benchmarking agentic video research on open web. arXiv\npreprint arXiv:2512.23044, 2025.\n[711] Chengwen Liu, Xiaomin Yu, Zhuoyue Chang, Zhe Huang, Shuo Zhang, Heng Lian, Kunyi Wang,\nRui Xu, Sen Hu, Jianheng Hou, et al. Watching, reasoning, and searching: A video deep research\nbenchmark on open web for agentic video reasoning. arXiv preprint arXiv:2601.06943, 2026.\n[712] Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang,\nand Kam-Fai Wong. Perltqa: A personal long-term memory dataset for memory classification, retrieval,\nand fusion in question answering. In Proceedings of the 10th SIGHAN Workshop on Chinese Language\nProcessing (SIGHAN-10), pages 152–164, 2024.\n[713] Thibaut Thonet, Jos Rozen, and Laurent Besacier. Elitr-bench: A meeting assistant benchmark for\nlong-context language models. arXiv preprint arXiv:2403.20262, 2024.\n[714] Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu\nXu, Hongjiang Lv, et al. Multi-if: Benchmarking llms on multi-turn and multilingual instructions\nfollowing. arXiv preprint arXiv:2410.15553, 2024.\n[715] Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee,\nJeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: A realistic multi-turn\nconversation evaluation benchmark challenging to frontier llms. arXiv preprint arXiv:2501.17399,\n2025.\n[716] Yiran Zhang, Mo Wang, Xiaoyang Li, Kaixuan Ren, Chencheng Zhu, and Usman Naseem. Turnbench-\nms: A benchmark for evaluating multi-turn, multi-step reasoning in large language models. arXiv\npreprint arXiv:2506.01341, 2025.\n[717] Luanbo Wan and Weizhi Ma. Storybench: A dynamic benchmark for evaluating long-term memory\nwith multi turns. arXiv preprint arXiv:2506.13356, 2025.\n[718] Haoran Tan, Zeyu Zhang, Chen Ma, Xu Chen, Quanyu Dai, and Zhenhua Dong. Membench: Towards\nmore comprehensive evaluation on the memory of llm-based agents. arXiv preprint arXiv:2506.21605,\n2025.\n[719] Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing\nXu, Chong Zhang, Chun-Mei Feng, et al. Mmrc: A large-scale benchmark for understanding multimodal\nlarge language model in real-world conversation. arXiv preprint arXiv:2502.11903, 2025.\n[720] Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua\nDong, and Ji-Rong Wen. Memsim: A bayesian simulator for evaluating memory of llm-based personal\nassistants. arXiv preprint arXiv:2409.20163, 2024.\n128\n"}, {"page": 129, "text": "Agentic Reasoning for Large Language Models\n[721] Dong-Ho Lee, Adyasha Maharana, Jay Pujara, Xiang Ren, and Francesco Barbieri. Realtalk: A 21-day\nreal-world dataset for long-term conversation. arXiv preprint arXiv:2502.13270, 2025.\n[722] Yuanzhe Hu, Yu Wang, and Julian McAuley. Evaluating memory in llm agents via incremental\nmulti-turn interactions. arXiv preprint arXiv:2507.05257, 2025.\n[723] Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambham-\npati. Planbench: An extensible benchmark for evaluating large language models on planning and\nreasoning about change. Advances in Neural Information Processing Systems, 36:38975–38987, 2023.\n[724] Harsha Kokel, Michael Katz, Kavitha Srinivas, and Shirin Sohrabi. Acpbench: Reasoning about action,\nchange, and planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages\n26559–26568, 2025.\n[725] Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Yao Mu, Hongyuan\nZhang, Wenqi Shao, and Ping Luo. Text2world: Benchmarking large language models for symbolic\nworld model generation. arXiv preprint arXiv:2502.13092, 2025.\n[726] Longling Geng and Edward Y Chang. Realm-bench: A benchmark for evaluating multi-agent systems\non real-world, dynamic planning and scheduling tasks. arXiv preprint arXiv:2502.18836, 2025.\n[727] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and\nYu Su. Travelplanner: A benchmark for real-world planning with language agents. arXiv preprint\narXiv:2402.01622, 2024.\n[728] Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang, Fei Huang, and Yongbin\nLi. Flowbench: Revisiting and benchmarking workflow-guided planning for llm-based agents. arXiv\npreprint arXiv:2406.14884, 2024.\n[729] Yu Zheng, Longyi Liu, Yuming Lin, Jie Feng, Guozhen Zhang, Depeng Jin, and Yong Li. Urbanplan-\nbench: A comprehensive urban planning benchmark for evaluating large language models. arXiv\npreprint arXiv:2504.21027, 2025.\n[730] Lianmin Zheng, Jiacheng Yang, Han Cai, Ming Zhou, Weinan Zhang, Jun Wang, and Yong Yu. Magent:\nA many-agent reinforcement learning platform for artificial collective intelligence. In Proceedings of\nthe AAAI conference on artificial intelligence, volume 32, 2018.\n[731] Cinjon Resnick, Wes Eldridge, David Ha, Denny Britz, Jakob Foerster, Julian Togelius, Kyunghyun\nCho, and Joan Bruna. Pommerman: A multi-agent playground. arXiv preprint arXiv:1809.07124,\n2018.\n[732] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli,\nTim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft\nmulti-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\n[733] Xianhao Yu, Jiaqi Fu, Renjia Deng, and Wenjuan Han. Mineland: Simulating large-scale multi-agent\ninteractions with limited multimodal senses and physical needs. arXiv preprint arXiv:2403.19267,\n2024.\n[734] Qian Long, Zhi Li, Ran Gong, Ying Nian Wu, Demetri Terzopoulos, and Xiaofeng Gao. Teamcraft: A\nbenchmark for multi-modal multi-agent systems in minecraft. arXiv preprint arXiv:2412.05255, 2024.\n129\n"}, {"page": 130, "text": "Agentic Reasoning for Large Language Models\n[735] Joel Z Leibo, Edgar A Dueñez-Guzman, Alexander Vezhnevets, John P Agapiou, Peter Sunehag,\nRaphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch, and Thore Graepel. Scalable evaluation\nof multi-agent reinforcement learning with melting pot. In International conference on machine learning,\npages 6187–6199. PMLR, 2021.\n[736] Matteo Bettini, Amanda Prorok, and Vincent Moens. Benchmarl: Benchmarking multi-agent rein-\nforcement learning. Journal of Machine Learning Research, 25(217):1–10, 2024.\n[737] Yuhang Song, Andrzej Wojcicki, Thomas Lukasiewicz, Jianyi Wang, Abi Aryan, Zhenghua Xu, Mai\nXu, Zihan Ding, and Lianlong Wu. Arena: A general evaluation platform and building toolkit for\nmulti-agent intelligence. In Proceedings of the AAAI conference on artificial intelligence, volume 34,\npages 7253–7260, 2020.\n[738] Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang, Mont-\ngomery Alban, Iman Fadakar, Zheng Chen, et al. Smarts: Scalable multi-agent reinforcement learning\ntraining school for autonomous driving. arXiv preprint arXiv:2010.09776, 2020.\n[739] Eugene Vinitsky, Nathan Lichtlé, Xiaomeng Yang, Brandon Amos, and Jakob Foerster. Nocturne:\na scalable driving benchmark for bringing multi-agent learning one step closer to the real world.\nAdvances in Neural Information Processing Systems, 35:3962–3974, 2022.\n[740] Xianliang Yang, Zhihao Liu, Wei Jiang, Chuheng Zhang, Li Zhao, Lei Song, and Jiang Bian. A\nversatile multi-agent reinforcement learning benchmark for inventory management. arXiv preprint\narXiv:2306.07542, 2023.\n[741] Pascal Leroy, Pablo G Morato, Jonathan Pisane, Athanasios Kolios, and Damien Ernst. Imp-marl: a\nsuite of environments for large-scale infrastructure management planning via marl. Advances in neural\ninformation processing systems, 36:53522–53551, 2023.\n[742] Alexey Skrynnik, Anton Andreychuk, Anatolii Borzilov, Alexander Chernyavskiy, Konstantin Yakovlev,\nand Aleksandr Panov. Pogema: A benchmark platform for cooperative multi-agent pathfinding. arXiv\npreprint arXiv:2407.14931, 2024.\n[743] Vindula Jayawardana, Baptiste Freydt, Ao Qu, Cameron Hickert, Zhongxia Yan, and Cathy Wu.\nIntersectionzoo: Eco-driving for benchmarking multi-agent contextual reinforcement learning. arXiv\npreprint arXiv:2410.15221, 2024.\n[744] Saaket Agashe, Yue Fan, Anthony Reyna, and Xin Eric Wang. Llm-coordination: evaluating and\nanalyzing multi-agent coordination abilities in large language models. arXiv preprint arXiv:2310.03903,\n2023.\n[745] Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. Avalonbench: Evaluating llms playing the game of\navalon. arXiv preprint arXiv:2310.05036, 2023. URL https://www.arxiv.org/abs/2310.05036.\n[746] Gabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, and Jesse Clifton.\nWelfare diplomacy: Benchmarking language model cooperation. arXiv preprint arXiv:2310.08901,\n2023.\n[747] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, and Jiashi\nFeng. Magic: Investigation of large language model powered multi-agent in cognition, adaptability,\nrationality and collaboration. arXiv preprint arXiv:2311.08562, 2023.\n130\n"}, {"page": 131, "text": "Agentic Reasoning for Large Language Models\n[748] Timothy Ossowski, Jixuan Chen, Danyal Maqbool, Zefan Cai, Tyler Bradshaw, and Junjie Hu. Comma:\nA communicative multimodal multi-agent benchmark. arXiv preprint arXiv:2410.07553, 2024.\n[749] Elad Levi and Ilan Kadar. Intellagent: A multi-agent framework for evaluating conversational ai\nsystems. arXiv preprint arXiv:2501.11067, 2025.\n[750] Tajamul Ashraf, Amal Saqib, Hanan Ghani, Muhra AlMahri, Yuhao Li, Noor Ahsan, Umair Nawaz, Jean\nLahoud, Hisham Cholakkal, Mubarak Shah, et al. Agent-x: Evaluating deep multimodal reasoning in\nvision-centric agentic tasks. arXiv preprint arXiv:2505.24876, 2025.\n[751] Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan,\nEduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack Parker-\nHolder, and Tim Rocktäschel. Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv\npreprint arXiv:2411.13543, 2024. URL https://www.arxiv.org/abs/2411.13543.\n[752] Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, and Zhen Xiao. Understanding the\nweakness of large language model agents within a complex android environment. arXiv preprint\narXiv:2402.06596v1, 2024. URL https://www.arxiv.org/abs/2402.06596v1.\n[753] Weihao Tan, Changjiu Jiang, Yu Duan, Mingcong Lei, Jiageng Li, Yitian Hong, Xinrun Wang, and\nBo An. Stardojo: Benchmarking open-ended behaviors of agentic multimodal llms in production-\nliving simulations with stardew valley. arXiv preprint arXiv:2507.07445v2, 2025. URL https:\n//www.arxiv.org/abs/2507.07445v2.\n[754] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-\nChun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. Mindagent: Emergent gaming interac-\ntion. arXiv preprint arXiv:2309.09971, 2023. URL https://www.arxiv.org/abs/2309.09971.\n[755] Dominik Jeurissen, Diego Perez-Liebana, Jeremy Gow, Duygu Cakmak, and James Kwan. Playing\nnethack with llms: Potential & limitations as zero-shot agents. arXiv preprint arXiv:2403.00690, 2024.\nURL https://www.arxiv.org/abs/2403.00690.\n[756] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua,\nZhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese,\nCaiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-\nended tasks in real computer environments. arXiv preprint arXiv:2404.07972v2, 2024. URL https:\n//www.arxiv.org/abs/2404.07972v2.\n[757] Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bod-\nhisattwa Prasad Majumder, Oyvind Tafjord, and Peter Clark. Discoveryworld: A virtual environment\nfor developing and evaluating automated scientific discovery agents. Advances in Neural Information\nProcessing Systems, 37:10088–10116, 2024.\n[758] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld:\nIs your agent smarter than a 5th grader?\narXiv preprint arXiv:2203.07540, 2022. URL https:\n//www.arxiv.org/abs/2203.07540.\n[759] Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen\nWei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum,\nXuhui Huang, Xia Ning, Song Gao, Yu Su, and Huan Sun. Scienceagentbench: Toward rigorous\nassessment of language agents for data-driven scientific discovery. arXiv preprint arXiv:2410.05080,\n2024. URL https://www.arxiv.org/abs/2410.05080.\n131\n"}, {"page": 132, "text": "Agentic Reasoning for Large Language Models\n[760] Jon M. Laurent, Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling,\nSiddharth Narayanan, Manvitha Ponnapati, Andrew D. White, and Samuel G. Rodriques. Lab-bench:\nMeasuring capabilities of language models for biology research. arXiv preprint arXiv:2407.10362,\n2024. URL https://www.arxiv.org/abs/2407.10362.\n[761] Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents\non machine learning experimentation. arXiv preprint arXiv:2310.03302, 2023. URL https://www.\narxiv.org/abs/2310.03302.\n[762] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H Laradji, Manuel Del Verme, Tom Marty,\nLéo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, et al. Workarena: How capable are\nweb agents at solving common knowledge work tasks? arXiv preprint arXiv:2403.07718, 2024.\n[763] Léo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault L De Chezelles, Quentin\nCappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. Workarena++: Towards\ncompositional planning and reasoning-based common knowledge work tasks. Advances in Neural\nInformation Processing Systems, 37:5996–6051, 2024.\n[764] Zilong Wang, Yuedong Cui, Li Zhong, Zimin Zhang, Da Yin, Bill Yuchen Lin, and Jingbo Shang.\nOfficebench: Benchmarking language agents across multiple applications for office automation. arXiv\npreprint arXiv:2407.19056, 2024. URL https://www.arxiv.org/abs/2407.19056.\n[765] Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, and Rebecca\nQian. Trail: Trace reasoning and agentic issue localization. arXiv preprint arXiv:2505.08638, 2025.\nURL https://www.arxiv.org/abs/2505.08638.\n[766] Yuji Zhang, Sha Li, Jiateng Liu, Pengfei Yu, Yi R Fung, Jing Li, Manling Li, and Heng Ji. Knowl-\nedge overshadowing causes amalgamated hallucination in large language models. arXiv preprint\narXiv:2407.08039, 2024.\n[767] Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon,\nLi Zhang, Chris Callison-Burch, and Peter Clark. Clin: A continually learning language agent for\nrapid task adaptation and generalization. arXiv preprint arXiv:2310.10134, 2023. URL https:\n//www.arxiv.org/abs/2310.10134.\n[768] Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong,\nZechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chan-\ndra, and Jürgen Schmidhuber.\nAgent-as-a-judge: Evaluate agents with agents.\narXiv preprint\narXiv:2410.10934, 2024. URL https://www.arxiv.org/abs/2410.10934.\n[769] Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor.\nAgentclinic: a multimodal agent benchmark to evaluate ai in simulated clinical environments. arXiv\npreprint arXiv:2405.07960, 2024. URL https://www.arxiv.org/abs/2405.07960.\n[770] Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, James Zou, Andrew Y. Ng, and Jonathan H.\nChen. Medagentbench: A realistic virtual ehr environment to benchmark medical llm agents. arXiv\npreprint arXiv:2501.14654, 2025. URL https://www.arxiv.org/abs/2501.14654.\n[771] Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu,\nYilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, and Mark Gerstein. Medagentsbench: Bench-\nmarking thinking models and agent frameworks for complex medical reasoning. arXiv preprint\narXiv:2503.07459, 2025. URL https://www.arxiv.org/abs/2503.07459.\n132\n"}, {"page": 133, "text": "Agentic Reasoning for Large Language Models\n[772] Karishma Thakrar, Shreyas Basavatia, and Akshay Daftardar. Architecting clinical collaboration:\nMulti-agent reasoning systems for multimodal medical vqa, 2025. URL https://arxiv.org/abs/\n2507.05520.\n[773] Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong,\nChulin Xie, Carl Yang, Dawn Song, and Bo Li.\nGuardagent: Safeguard llm agents by a guard\nagent via knowledge-enabled reasoning.\narXiv preprint arXiv:2406.09187, 2024.\nURL https:\n//www.arxiv.org/abs/2406.09187.\n[774] Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang,\nShuyan Zhou, Tongshuang Wu, and Zhengyang Wu. Webcanvas: Benchmarking web agents in online\nenvironments. arXiv preprint arXiv:2406.12373v3, 2024. URL https://www.arxiv.org/abs/\n2406.12373v3.\n[775] Xing Han Lù, Zdeněk Kasner, and Siva Reddy. Weblinx: Real-world website navigation with multi-\nturn dialogue. arXiv preprint arXiv:2402.05930, 2024. URL https://www.arxiv.org/abs/2402.\n05930.\n[776] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin,\nChenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, and Yining Hua.\nBrowsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv\npreprint arXiv:2504.19314, 2025. URL https://www.arxiv.org/abs/2504.19314.\n[777] Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, Wenhao Yu, and Dong Yu. Laser: Llm\nagent with state-space exploration for web navigation. arXiv preprint arXiv:2309.08172, 2023. URL\nhttps://www.arxiv.org/abs/2309.08172.\n[778] Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey\nBradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, et al. Nestful: A benchmark for\nevaluating llms on nested sequences of api calls. In Proceedings of the 2025 Conference on Empirical\nMethods in Natural Language Processing, pages 33526–33535, 2025.\n[779] Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool\nintegration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441v1, 2025. URL\nhttps://www.arxiv.org/abs/2505.01441v1.\n[780] Divij Handa, Pavel Dolin, Shrinidhi Kumbhar, Tran Cao Son, and Chitta Baral. Actionreasoningbench:\nReasoning about actions with and without ramification constraints. arXiv preprint arXiv:2406.04046,\n2024. URL https://www.arxiv.org/abs/2406.04046.\n[781] Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin\nZhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, and Gongshen Liu. R-judge: Benchmarking safety risk\nawareness for llm agents. arXiv preprint arXiv:2401.10019, 2024. URL https://www.arxiv.org/\nabs/2401.10019.\n[782] Cheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin Qiu, Zhiwei Liu, Haolin Chen, Shirley Kokane, Heng\nJi, Weiran Yao, Shelby Heinecke, et al. Userrl: Training interactive user-centric agent via reinforcement\nlearning. arXiv preprint arXiv:2509.19736, 2025.\n[783] Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again!\nllm-powered personalized agent for long-term dialogue. In Proceedings of the 2025 Conference of the\n133\n"}, {"page": 134, "text": "Agentic Reasoning for Large Language Models\nNations of the Americas Chapter of the Association for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), pages 5259–5276, 2025.\n[784] Yufei Xiang, Yiqun Shen, Yeqin Zhang, and Nguyen Cam-Tu. Retrospex: Language agent meets offline\nreinforcement learning critic. In Proceedings of the 2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4650–4666, 2024.\n[785] Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, and Liaoni Wu. Tree search for\nllm agent reinforcement learning. arXiv preprint arXiv:2509.21240, 2025.\n[786] Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily\nMcMilin, Michel Meyer, Yuxiang Wei, David Zhang, et al. Cwm: An open-weights llm for research on\ncode generation with world models. arXiv preprint arXiv:2510.02387, 2025.\n[787] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through\nworld models. arXiv preprint arXiv:2301.04104, 2023.\n[788] Hao Tang, Darren Key, and Kevin Ellis. Worldcoder, a model-based llm agent: Building world models\nby writing code and interacting with the environment. In Conference on Neural Information Processing\nSystems (NeurIPS), pages 70148–70212, 2024.\n[789] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim,\nSunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and\nleveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232, 2024.\n[790] Dezhao Luo, Bohan Tang, Kang Li, Georgios Papoudakis, Jifei Song, Shaogang Gong, Jianye Hao,\nJun Wang, and Kun Shao. Vimo: A generative visual gui world model for app agents. arXiv preprint\narXiv:2504.13936, 2025.\n[791] Mingkai Deng, Jinyu Hou, Zhiting Hu, and Eric Xing. Simura: A world-model-driven simulative\nreasoning architecture for general goal-oriented agents. arXiv preprint arXiv:2507.23773, 2025.\n[792] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\nLu, Yi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and exploring\nemergent behaviors. In The Twelfth International Conference on Learning Representations, 2023.\n[793] Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du,\nWeize Chen, Cheng Yang, et al. Scaling large language model-based multi-agent collaboration. arXiv\npreprint arXiv:2406.07155, 2024.\n[794] Florian Grötschla, Luis Müller, Jan Tönshoff, Mikhail Galkin, and Bryan Perozzi. Agentsnet: Coordina-\ntion and collaborative reasoning in multi-agent llms. arXiv preprint arXiv:2507.08616v1, 2025. URL\nhttps://www.arxiv.org/abs/2507.08616v1.\n[795] Zhuoyun Du, Runze Wang, Huiyu Bai, Zouying Cao, Xiaoyong Zhu, Bo Zheng, Wei Chen, and Haochao\nYing. Enabling agents to communicate entirely in latent space. arXiv preprint arXiv:2511.09149,\n2025.\n[796] Tianyu Fu, Zihan Min, Hanling Zhang, Jichao Yan, Guohao Dai, Wanli Ouyang, and Yu Wang.\nCache-to-cache: Direct semantic communication between large language models. arXiv preprint\narXiv:2510.03215, 2025.\n134\n"}, {"page": 135, "text": "Agentic Reasoning for Large Language Models\n[797] Kun Wang, Guibin Zhang, Zhenhong Zhou, Jiahao Wu, Miao Yu, Shiqian Zhao, Chenlong Yin, Jinhu\nFu, Yibo Yan, Hanjun Luo, et al. A comprehensive survey in llm (-agent) full stack safety: Data,\ntraining and deployment. arXiv preprint arXiv:2504.15585, 2025.\n135\n"}]}