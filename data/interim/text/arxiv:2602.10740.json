{"doc_id": "arxiv:2602.10740", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.10740.pdf", "meta": {"doc_id": "arxiv:2602.10740", "source": "arxiv", "arxiv_id": "2602.10740", "title": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs", "authors": ["Yuming Yan", "Shuo Yang", "Kai Tang", "Sihong Chen", "Yang Zhang", "Ke Xu", "Dan Hu", "Qun Yu", "Pengfei Hu", "Edith C. H. Ngai"], "published": "2026-02-11T11:04:37Z", "updated": "2026-02-11T11:04:37Z", "summary": "Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forgetting, limiting its generalization. The central challenge, therefore, is to adapt VLMs to new domains while preserving their general-purpose capabilities. Continual pretraining is effective for expanding knowledge in Large Language Models (LLMs), but it is less feasible for VLMs due to prohibitive computational costs and the unavailability of pretraining data for most open-source models. This necessitates efficient post-training adaptation methods. Reinforcement learning (RL)-based approaches such as Group Relative Policy Optimization (GRPO) have shown promise in preserving general abilities, yet they often fail in domain adaptation scenarios where the model initially lacks sufficient domain knowledge, leading to optimization collapse. To bridge this gap, we propose Reinforced Curriculum Pre-Alignment (RCPA), a novel post-training paradigm that introduces a curriculum-aware progressive modulation mechanism. In the early phase, RCPA applies partial output constraints to safely expose the model to new domain concepts. As the model's domain familiarity increases, training gradually transitions to full generation optimization, refining responses and aligning them with domain-specific preferences. This staged adaptation balances domain knowledge acquisition with the preservation of general multimodal capabilities. Extensive experiments across specialized domains and general benchmarks validate the effectiveness of RCPA, establishing a practical pathway toward building high-performing and domain-adaptive VLMs.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.10740v1", "url_pdf": "https://arxiv.org/pdf/2602.10740.pdf", "meta_path": "data/raw/arxiv/meta/2602.10740.json", "sha256": "b02c68a099b75565abe729af7586dc4afb9bf617f836e9632ae58e1da8c9c09a", "status": "ok", "fetched_at": "2026-02-18T02:19:25.078425+00:00"}, "pages": [{"page": 1, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\nYuming Yan * 1 Shuo Yang * 2 Kai Tang 1 Sihong Chen 1 Yang Zhang 1 Ke Xu 1 Dan Hu 1\nQun Yu 1 Pengfei Hu 1 Edith C.H. Ngai 2\nAbstract\nVision-Language Models (VLMs) demonstrate\nremarkable general-purpose capabilities but often\nfall short in specialized domains such as medi-\ncal imaging or geometric problem-solving. Su-\npervised Fine-Tuning (SFT) can enhance perfor-\nmance within a target domain, but it typically\ncauses catastrophic forgetting, limiting its utility\nas a general AI agent. The central challenge, there-\nfore, is to adapt VLMs to new domains while pre-\nserving their general-purpose capabilities. Contin-\nual pretraining is effective for expanding knowl-\nedge in Large Language Models (LLMs), but it\nis less feasible for VLMs due to prohibitive com-\nputational costs and the unavailability of pretrain-\ning data for most open-source models. This ne-\ncessitates efficient post-training adaptation meth-\nods.\nReinforcement learning (RL)‚Äìbased ap-\nproaches such as Group Relative Policy Optimiza-\ntion (GRPO) have shown promise in preserving\ngeneral abilities, yet they often fail in domain\nadaptation scenarios where the model initially\nlacks sufficient domain knowledge, leading to\noptimization collapse. To bridge this gap, we\npropose Reinforced Curriculum Pre-Alignment\n(RCPA), a novel post-training paradigm that intro-\nduces a curriculum-aware progressive modulation\nmechanism. In the early phase, RCPA applies par-\ntial output constraints to safely expose the model\nto new domain concepts. As the model‚Äôs domain\nfamiliarity increases, training gradually transi-\ntions to full generation optimization, refining re-\nsponses and aligning them with domain-specific\npreferences. This staged adaptation balances do-\nmain knowledge acquisition with the preserva-\ntion of general multimodal capabilities. Exten-\nsive experiments across specialized domains (e.g.,\nOpenI for medical imaging and Geo170K for ge-\nometry) and general benchmarks (e.g., COCO\nCaptions) validate the effectiveness of RCPA. Re-\n*Equal contribution 1Tencent 2The Univerisity of Hong Kong.\nCorrespondence to: Shuo Yang <shuoyang.ee@gmail.com>.\nPreprint. February 12, 2026.\nsults show that RCPA achieves domain-specific\nperformance competitive with SFT while signif-\nicantly outperforming SFT in retaining general\nmultimodal understanding, establishing a practi-\ncal pathway toward building high-performing and\ndomain-adaptive VLMs.\n1. Introduction\nVision-Language Models (VLMs) (Bai et al., 2025; Wang\net al., 2024a; Bai et al., 2023; Wang et al., 2025b; Zhu\net al., 2025; Chen et al., 2024b; Wang et al., 2024b; Gao\net al., 2024; Liu et al., 2024; 2023a) have achieved remark-\nable progress in unifying visual perception with natural lan-\nguage understanding, enabling powerful capabilities across\ntasks such as image captioning, visual question answering\n(VQA), and multimodal dialogue (Bai et al., 2025; Wang\net al., 2024a; Bai et al., 2023). Large-scale pre-training on\nweb-scale multimodal corpora equips these models with\nbroad general-purpose competencies, making them versatile\nfoundation models. However, real-world deployment often\nrequires domain adaptation, where VLMs must acquire spe-\ncialized knowledge (e.g., medical, scientific, or industrial\ncontexts) while retaining their general reasoning abilities.\nThis dual requirement‚Äîlearning new domain knowledge\nwithout forgetting existing skills‚Äîposes a central challenge\nfor post-training adaptation.\nCurrent post-training adaptation strategies (Gekhman et al.,\n2024; Guo et al., 2023; Hu et al., 2021; Lambert, 2025;\nRafailov et al., 2023) are generally categorized into two pri-\nmary paradigms: Supervised Fine-Tuning (SFT) and Rein-\nforcement Learning (RL). SFT facilitates direct knowledge\ninjection by performing imitation learning on target-domain\ndata. While effective for domain-specific expertise transfer,\nSFT frequently induces catastrophic forgetting (Shenfeld\net al., 2025), where specialized signals overwrite established\ngeneral-purpose capabilities. To mitigate this degradation,\nvarious approaches (Li & Hoiem, 2016; Kirkpatrick et al.,\n2016; Rebuffi et al., 2016) have been proposed, ranging\nfrom incremental learning (Li & Hoiem, 2016; Kirkpatrick\net al., 2016; Rebuffi et al., 2016) to Test-Time Adaptation\n(TTA) (MA et al., 2023; Niu et al., 2023; Yang et al., 2024).\nThese methodologies typically employ one of three mecha-\n1\narXiv:2602.10740v1  [cs.CL]  11 Feb 2026\n"}, {"page": 2, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\nnisms: parameter-efficient fine-tuning, regularization-based\nknowledge preservation, or exemplar replay from source do-\nmains. However, these designs are often ill-suited for large-\nscale Vision-Language Models (VLMs). Specifically, data\nreplay is frequently rendered infeasible by stringent privacy,\nstorage, or licensing constraints, while regularization-based\nmethods often settle for a suboptimal trade-off, failing to\nachieve peak performance in both domain specialization\nand general capability retention simultaneously. Conversely,\npreference-based RL frameworks, such as Group Relative\nPolicy Optimization (GRPO) (Shao et al., 2024), utilize\nKL-regularized optimization to safeguard model generality.\nNevertheless, the efficacy of these methods hinges on the\nimplicit assumption that the pre-trained model already pos-\nsesses non-trivial domain knowledge (Wang et al., 2025a).\nWhen this prerequisite is unmet‚Äîa common scenario in\nlow-resource or highly specialized domains‚Äîthe model\nfails to generate sufficiently high-quality trajectories, in-\nevitably leading to optimization collapse.\nThe distinct limitations of SFT and RL-based adaptation\nunderscore a fundamental challenge: neither paradigm, in\nisolation, can concurrently ensure optimization stability, ef-\nfective knowledge transfer, and the robust preservation of\ngeneral-purpose capabilities. To address this bottleneck,\nwe propose that domain adaptation should be conceptu-\nalized as a progressive rather than a monolithic process.\nUnder this framework, the model must first undergo pre-\nalignment to establish a stable, foundational grounding in\nthe target domain‚Äôs concepts. This is subsequently followed\nby reinforcement-alignment, which leverages richer pref-\nerence signals to refine model behavior and ensure high-\nfidelity task performance.\nIn this paper, we propose Reinforced Curriculum Pre-\nAlignment (RCPA), a novel post-training framework that\nunifies domain knowledge acquisition with preference align-\nment under a curriculum-driven design.\nRCPA intro-\nduces a progressive modulation mechanism that transitions\nsmoothly from constrained imitation to full generation opti-\nmization, avoiding optimization collapse while mitigating\nforgetting. This framework is built upon GRPON (GRPO\nfor Non-Deep-Thinking Models), which adapts the RL\nbackbone specifically for non-reasoning VQA-style tasks.\nTo further enhance adaptability, RCPA incorporates two\ncurriculum-inspired modules: Curriculum Progress Percep-\ntion (CPP) and Curriculum Difficulty Perception (CDP).\nCPP regulates answer-prefix injection and reward threshold\nscheduling to bootstrap stable signals in early training, while\nCDP dynamically prioritizes difficult samples to maximize\nlearning benefits and prevent overfitting. Extensive exper-\niments on domain-specific VQA benchmarks demonstrate\nthat RCPA not only achieves superior domain alignment but\nalso preserves broad general-purpose multimodal capabili-\nties, outperforming existing SFT- and RL-based approaches.\n2. Related Works\n2.1. Vision Language Models (VLMs)\nVision Language Models (VLMs) (Bai et al., 2025; Wang\net al., 2024a; Bai et al., 2023; Wang et al., 2025b; Zhu\net al., 2025; Chen et al., 2024b; Wang et al., 2024b; Gao\net al., 2024; Liu et al., 2024; 2023a) have significantly ad-\nvanced cross-modal intelligence by integrating text and im-\nage modalities, progressing through three key phases. In\nthe foundational phase, early models like CLIP (?) and ViT-\nBERT (Li et al., 2021b) bridged the modal gap between text\nand images, enabling tasks like zero-shot transfer and visual\ngrounding. Subsequent models such as ALBEF (Li et al.,\n2021a) and FLAVA (Singh et al., 2022) further refined align-\nment techniques for better semantic consistency. The second\nphase focused on enhancing general capabilities through in-\nstruction tuning, with models like LLaVA (Liu et al., 2023b)\nand Flan-V5 (Chung et al., 2022) improving cross-modal\nreasoning and task handling. Recent developments include\nInternVL (Wang et al., 2025b; Zhu et al., 2025; Wang et al.,\n2024b), an open-source Vision Language Models (VLMs)\nseries, and the Qwen series (Bai et al., 2025; Wang et al.,\n2024a; Bai et al., 2023), both pushing the boundaries of\nmultimodal understanding with advanced visual encoders\nand innovative techniques for handling high-resolution im-\nages, multimodal rotation, and tool usage. Despite these\nadvances, VLMs continue to struggle with domain-specific\nadaptation. In specialized settings such as medical imaging\nor scientific problem-solving, they often fail to recognize\ndomain-specific concepts or adapt to nuanced visual fea-\ntures. To this end, post-training methods have emerged as a\npromising direction, offering lightweight yet effective mech-\nanisms for adapting VLMs to specialized domains without\nretraining from scratch.\n2.2. Post-training for VLMs\nPost-training techniques, primarily SFT and RL, have been\ncentral to adapting VLMs to domain-specific tasks (Kumar\net al., 2025; Chu et al., 2025; Lai et al., 2025; Li et al.,\n2025). SFT enables task-specific learning but often leads to\ncatastrophic forgetting of the general knowledge learned dur-\ning pre-training, especially when fine-tuning is performed\non domain-specific data (Duan et al., 2024; Dong et al.,\n2025; Chen et al., 2024a). Parameter-efficient approaches\nsuch as QLoRA (Dettmers et al., 2023), LoRA (Hu et al.,\n2021), Adapters (Hu et al., 2023), and Prompt/Prefix Tun-\ning (Lester et al., 2021; Li & Liang, 2021) alleviate com-\nputational burdens by updating only a subset of parameters,\nyet they remain prone to overfitting and limited transferabil-\nity. In contrast, RL-based methods such as Group Relative\nPolicy Optimization (GRPO) (Shao et al., 2024), Domain\nAdaptive Policy Optimization (DAPO) (Yu et al., 2025),\nand Group Sequence Policy Optimization (GSPO) (Zheng\n2\n"}, {"page": 3, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\net al., 2025a) enhance adaptability by leveraging dynamic\nfeedback and optimizing sequential decision-making. These\nmethods are effective in preserving general capabilities by\nregularizing the model‚Äôs output with reward signals, but they\nassume that the pre-trained model already possesses non-\ntrivial domain knowledge. Traditional RL approaches such\nas Proximal Policy Optimization (PPO) (Schulman et al.,\n2017), which require training both the policy and critic\nmodels (Lambert, 2025), impose high computational costs.\nRecent alternatives like GRPO and Direct Preference Opti-\nmization (DPO) (Rafailov et al., 2023) reduce this burden by\nremoving the need for a separate critic, thereby simplifying\ntraining. Nevertheless, existing RL paradigms still strug-\ngle to adapt efficiently when the model begins with limited\ndomain expertise. The commonly adopted ‚ÄúSFT-then-RL‚Äù\npipeline (Shao et al., 2024) partially alleviates this issue\nby stabilizing the reward signal in early training. However,\nrecent findings from CHORD (Zhang et al., 2025) reveal a\nfundamental flaw: SFT disrupts the pretrained model‚Äôs in-\nternal structures, causing temporary degradation of general\ncapabilities, while subsequent RL fails to recover domain\nadaptation‚Äîoften performing worse than direct RL. These\nlimitations underscore the pressing need for more efficient\nand scalable post-training strategies that can jointly achieve\ndomain specialization and general capability preservation.\n3. Preliminary\n3.1. Problem Formulation\nConsider a pre-trained VLM denoted as œÄpre, which exhibits\nstrong general-purpose multimodal capabilities. We are\ngiven a target domain-specific dataset Dtarget = {(xi, yi)}N\ni ,\nwhere xi = (imagei, prompti) represents a multimodal in-\nput (image paired with a task prompt) and yi is the ground-\ntruth response containing domain-specific knowledge. The\nobjective is to adapt œÄpre into a domain-adapted model œÄŒ∏\nthrough a post-training procedure, such that œÄŒ∏ achieves\nhigh performance on Dtarget while maximally preserving the\ngeneral capabilities of œÄpre. This defines the fundamental\nchallenge in domain-adaptive VLM alignment: how to inte-\ngrate novel domain knowledge without forgetting previously\nacquired knowledge.\n3.2. Limitations of Existing Methods\nSupervised Fine-Tuning (SFT). SFT adapts model param-\neters Œ∏ by maximizing the likelihood of expert demonstra-\ntions in Dtarget = {(xi, yi)}N\ni=1. Its objective is:\nJSFT(Œ∏) = E(x, y) ‚àºDtarget\nÔ£Æ\nÔ£∞\n|y|\nX\nt=1\nlog œÄŒ∏(yt | x, y<t)\nÔ£π\nÔ£ª,\n(1)\nwhere y<t denotes the prefix tokens of y. While SFT ef-\nfectively injects domain-specific knowledge via imitation\nlearning, it relies exclusively on supervised labels‚Äîthough\nthis reliance does not directly cause catastrophic forget-\nting. Instead, the core driver is the distributional gap be-\ntween SFT and pretraining data. When target domain data\n(for SFT) differs substantially from pretraining data, SFT-\ninduced retraining aligns the model with the target domain\ndistribution, leading to misalignment with pretraining data\nand subsequent catastrophic forgetting. Previously acquired\ncapabilities are overwritten by domain-specific information,\nundermining VLMs‚Äô generalization.\nGroup Relative Policy Optimization (GRPO). GRPO and\nother Preference-based RL methods attempt to align models\nwith human preferences while mitigating forgetting through\nregularization. The GRPO incorporates a KL-divergence\npenalty to constrain the updated policy œÄŒ∏ from deviating\nexcessively from a reference policy œÄŒ∏ref (typically the initial\npre-trained model œÄŒ∏pre). Formally, given a input x and a\ngroup of G responses O = {o1, o2, . . . , oG} sampled from\nthe old policy œÄŒ∏old. The GRPO objective maximizes the\nexpected clipped advantage for each token oi,t in response\noi:\nJGRPO(Œ∏) = Ex‚àºP (X),{oi}G\ni=1‚àºœÄŒ∏old\n\"\n1\nG\nG\nX\ni=1\n1\n|oi|\n|oi|\nX\nt=1\nmin\n \nœÅi,t(Œ∏) ÀÜAi,t, clip (œÅi,t(Œ∏), 1 ‚àíŒµ, 1 + Œµ) ÀÜAi,t\n!\n‚àíŒ≥DKL [œÄŒ∏‚à•œÄref]\n#\n,\n(2)\nwhere the œÅi,t(Œ∏) =\nœÄŒ∏(oi,t|x,oi,<t)\nœÄŒ∏old(oi,t|x,oi,<t) is the importance sam-\npling ratio, œµ is the clip factor, Œ≥ controls the strength of KL\nregularization, and ÀÜAi,t = ri‚àímean(r)\nstd(r)\nis the standardized\nadvantage for token oi,t, computed from the group rewards\nr = {r1, r2, . . . , rG}.\nGRPO alleviates forgetting by combining preference-based\noptimization with KL regularization. However, it presup-\nposes that the pretrained model already holds non-trivial\nknowledge of the target domain. If the initial model œÄpre\npossesses limited knowledge of the target domain, it cannot\ngenerate responses of sufficient quality to yield informa-\ntive reward signals, which is also known as optimization\ncollapse.\n3.3. Motivation for RCPA\nDomain-adaptive alignment of VLMs entails a dual objec-\ntive: injecting novel domain knowledge while preserving\ngeneral-purpose capabilities. This poses a substantial chal-\nlenge for existing post-training paradigms. On one hand,\n3\n"}, {"page": 4, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\n<image>\n<prompt>\n<answer>\nDescribe this image in \none sentence\nPolicy\nModel\nPre-Alignment Phase\nüî•\nReference \nModel\nReward Module\nSemantic Similarity\nFactual Consistency\nEntity Alignment\nGroup\nComputation\nAdvantages\n-1\n‚Ä¶\n1\n-1\n1\n‚Ä¶\n1\n-1\nRewards\nùê¥!\"\n‚Ä¶\nùê¥!#\nùê¥!$\nùê¥\"\"\n‚Ä¶\nùê¥\"#\nùê¥\"$\nùúè!\"\n‚Ä¶\nùúè!\"\nùúè!$\nùúè\"\"\n‚Ä¶\nùúè\"#\nùúè\"$\n1\nCurriculum Difficulty Perception\n2\n1\n1\n-1\n1\n0.5\nmean\n-1\n1\n-1\n-1\n-0.5\nmean\nGRPO for Non-Deep-Thinking Model\nVQA Example\nCurriculum Progress Perception\nStep\nThreshold\nùêæùêø\n‚ùÑ\nA dog is sitting on \nthe grass.\nStep 0\nStep 1\nStep S-1\nStep S\nReinforcement Alignment Phase\nGRPON\nGRPON\nGRPON\nGRPON\n‚Ä¶\n‚Ä¶\nimage\nprompt\nimage\nprompt\nimage\nprompt\nimage\nprompt\nThreshold Scheduling\nùõø!\"#\nùõø!$%\nToken Sliding \n0\n1\n2\n-1\n-0.5\n0\n0.5\n1\nùë§(0.5) = 0.5\nùë§(‚àí0.5) = 1.0\nRewards\n1\noffset + ùëü\nùë§=\nx&\nx'\n‚Ä¶\nx&\nx'\n‚Ä¶\nx&\nx'\n‚Ä¶\nFigure 1. The Overview of RCPA. RCPA is a post-training framework that integrates domain knowledge acquisition with preference\nalignment in a curriculum-driven manner, built upon the GRPON (GRPO for Non-Deep-Thinking Models) framework for VQA-style tasks.\nIt consists of two phases: Pre-Alignment, which introduces domain concepts with controlled constraints to bootstrap initial competence,\nand Reinforcement Alignment, which refines the model‚Äôs responses using full reward-driven optimization. Key components include\nCurriculum Progress Perception (CPP), which adjusts reward thresholds to match the model‚Äôs evolving competence, and Curriculum\nDifficulty Perception (CDP), which prioritizes difficult samples to enhance training efficiency and prevent overfitting.\nSFT is effective at incorporating domain knowledge but in-\nevitably induces catastrophic forgetting of general skills. On\nthe other hand, RL-based methods such as GRPO empha-\nsize preserving broad competencies through regularization,\nyet often suffer from optimization collapse when the model\nlacks sufficient prior knowledge of the target domain. Conse-\nquently, neither purely supervised nor purely reinforcement-\nbased adaptation can simultaneously ensure stability, effec-\ntive domain transfer, and robust generalization. This limi-\ntation motivates a progressive adaptation strategy, wherein\nthe model is first pre-aligned to safely acquire foundational\ndomain concepts and subsequently reinforcement-aligned\nto refine its behavior with richer preference signals. Build-\ning on this perspective, we propose Reinforced Curriculum\nPre-Alignment (RCPA)‚Äîa paradigm that introduces a pro-\ngressive modulation mechanism to dynamically coordinate\nthe training objective, enabling a smooth transition from\nconstrained imitation learning to full reward-driven opti-\nmization as the model‚Äôs domain familiarity evolves.\n4. Method\n4.1. Overview of RCPA\nRCPA is a post-training paradigm that unifies domain\nknowledge acquisition with preference alignment under a\ncurriculum-driven framework. As illustrated in Figure 1,\nRCPA builds upon GRPON (GRPO for Non-Deep-Thinking\nModels) as the RL backbone for VQA-style tasks. A key\ndesign is progressive modulation mechanism, which decom-\nposes adaptation into two coordinated phases:\nPre-Alignment Phase: At early stages, the model lacks\nsufficient domain knowledge, RCPA employs partial output\nconstraints to mitigate the challenges of sparse rewards\nand unavailable preference signals. By introducing domain\nconcepts in a controlled manner (e.g., through partial answer\nexposure), this phase ensures valid response generation and\nbootstraps the model‚Äôs initial domain competence.\nReinforcement Alignment Phase: Once the model has\nattained a foundational understanding of the target domain,\nthe training process gradually shifts toward full reward-\ndriven optimization. Here, constraints are relaxed, and the\nmodel refines its responses based on reinforcement signals,\nachieving stronger preference alignment and higher task\nperformance.\nTo enable smooth transitions, RCPA incorporates two\ncurriculum-inspired modules: Curriculum Progress Percep-\ntion (CPP) and Curriculum Difficulty Perception (CDP).\nWe clarify the fundamental limitation of existing GRPO-\nbased methods: their failure to effectively sample correct\nresponses, leading to inadequate positive reward signals\nfor meaningful learning. To address this, CPP is designed\nto gradually reduce the difficulty of sampling correct re-\nsponses while refining target-domain knowledge acquisition.\nCPP dynamically schedules reward thresholds to match\nthe model‚Äôs evolving competence‚Äîapplying relatively low\nthresholds in the early stage to capture weak signals, and pro-\ngressively increasing them to enforce stricter performance\ncriteria as the model matures. CDP further enhances train-\ning efficiency by prioritizing difficult samples (with greater\nlearning value) and down-weighting simpler ones, thereby\npreventing overfitting and encouraging high-value learning.\nThrough this progressive and curriculum-driven approach,\nRCPA effectively coordinates deep knowledge integration\nwith the preservation of existing capabilities, yielding a\n4\n"}, {"page": 5, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\nrobust and adaptive alignment strategy.\n4.2. RL Backbone: GRPO for Non-Deep-Thinking\nModels\nWe adapt the GRPO framework for VLMs that perform\nshort-answer generation tasks (e.g., VQA), and denote this\nvariant as GRPO for Non-Deep-Thinking (GRPON). A cen-\ntral component of GRPON is a rule-based reward function\nR(o, y), which evaluates a generated output o against a\nground-truth answer y. The reward integrates three comple-\nmentary criteria:\nSemantic Similarity (Ss(o, y)): Measured as the cosine\nsimilarity between SentenceBERT (Reimers & Gurevych,\n2019) embeddings of o and y.\nFactual Consistency (Sf(o, y)): Assessed using a Natural\nLanguage Inference (NLI) model (Conneau et al., 2019),\nwhich checks bidirectional entailment and contradiction\nbetween o and y.\nEntity Alignment (Se(o, y)): Computed as the F1-score\nover entities extracted by SpaCy (Honnibal et al., 2020)\nfrom o and y.\nThe overall similarity score is defined as a weighted combi-\nnation of these components:\nS(o, y) = Œ±¬∑Ss(o, y)+(1‚àíŒ±)¬∑[Œ≤ ¬∑ Sf(o, y) + (1 ‚àíŒ≤) ¬∑ Se(o, y)]\n(3)\nwhere Œ±, Œ≤ ‚àà[0, 1] are tunable hyperparameters that bal-\nance the contribution of each factor. This similarity score is\nthen converted into a binary reward using a threshold Œ¥:\nR(o, y; Œ¥) =\n(\n+1\nif S(o, y) > Œ¥,\n‚àí1\notherwise.\n(4)\nWithin our curriculum-driven framework, the threshold Œ¥ is\nscheduled dynamically across training steps: it begins at a\nrelatively low value to enforce knowledge acquisition during\nthe early stage, and is gradually increased to encourage\nprecision and correctness as training progresses.\n4.3. Curriculum Progress Perception\nThe Curriculum Progress Perception (CPP) module is de-\nsigned to bootstrap learnable signals during the early stages\nof adaptation and to gradually transition toward free-form\ngeneration. It operates through two key mechanisms: (1)\nanswer-prefix injection and (2) step-level threshold schedul-\ning, with the overall goal of reducing the difficulty of sam-\npling the correct answer and shaping an optimizable output\ndistribution when the model initially lacks domain-specific\nknowledge.\nAnswer-Prefix Injection.\nFor a training sample\nxi = {imagei, prompti} with ground-truth answer yi =\n(yi,1, ..., yi,|yi|), let s denote the current training step. CPP\ninjects a prefix of length k(s) = max(0, (1 ‚àís\nS √ó œÉ)) ¬∑ |yi|\ninto the input context, where œÉ is the sliding token ra-\ntio.\nEach sample is treated as one training step, so œÉ\ncontrols the overall number of steps in the Pre-Alignment\nPhase. The resulting context at step s becomes C(i, s) =\n(imagei, prompti, yi,‚â§k(s)), and the model is required to\ngenerate only the suffix yi,>k(s). At s = 0, the model pre-\ndicts only the EOS token, minimizing risk. At s ‚â•S/œÉ,\nno prefix is provided, reducing the task to standard full-\nresponse generation. This injected prefix acts as an explicit\nsupervision anchor that strengthens cross-modal attention\nbetween the image and the partial textual answer, while\nalso increasing the density of rewardable samples by reduc-\ning invalid completions, particularly crucial during early\nadaptation.\nStep-Level Threshold Scheduling. To enhance progressive\ngeneration, CPP regulates the reward threshold Œ¥ (Equa-\ntion 4). In the early stages of training, when the model is ex-\nposed to simpler tasks, a lower threshold Œ¥min is used, which\nencourages exploration and allows the model to generate\nmore diverse outputs so as to better acquire domain knowl-\nedge. As training progresses and the model gains more\ndomain knowledge, the threshold is gradually increased to\nŒ¥max, enforcing stricter alignment with the injected prefix to\nensure accurate generation. This progression ensures that\nthe model moves from broad exploration to focused, precise\noutput generation. The threshold schedule follows a linear\nprogression, with Œ¥(s) formalized as:\nŒ¥(s) = Œ¥min + (Œ¥max ‚àíŒ¥min) √ó min(1, s\nS √ó œÉ),\n(5)\nwhere Œ¥max, Œ¥min are predefined clipping parameters. The\nGRPON objective is applied only to suffix tokens (t >\nk(s)), conditioned on the context C(s) and step-dependent\nthreshold Œ¥(s).\n4.4. Curriculum Difficulty Perception\nThe Curriculum Difficulty Perception (CDP) module dynam-\nically reweights training samples based on their difficulty, as\nreflected in the model‚Äôs real-time learning state. By prioritiz-\ning challenging samples and down-weighting simpler ones,\nCDP ensures that training resources are allocated to exam-\nples with the greatest learning value, thereby improving\nefficiency and mitigating overfitting.\nFor a given query, let r = (r1, . . . , rG) denote the rewards\nobtained from G generated responses. The mean reward\nr = mean(r) serves as a proxy for sample difficulty: higher\nvalues indicate that the sample is easy, while lower values\nindicate greater difficulty. The sample weight w is defined\nas w =\n1\noffset+¬Ør.\n5\n"}, {"page": 6, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\nAlgorithm 1 Reinforced Curriculum Pre-Alignment\n(RCPA)\nInput: Pre-trained\nVLM\nœÄŒ∏pre;\nTarget\ndataset\nDtarget\n=\n{(xi, yi)}N\ni=1;\nCurriculum\nparame-\nters\n(S, Œ¥max, Œ¥min, œÉ);\nGRPO\nhyperparameters\n(Œ±, Œ≤, œµ, G).\nOutput: Domain-adapted model œÄŒ∏.\n1: Initialize œÄŒ∏ ‚ÜêœÄŒ∏pre, reference model œÄŒ∏ref ‚ÜêœÄŒ∏pre,\nstep s ‚Üê0, S ‚ÜêN\n2: Sample minibatch B ‚äÇDtarget\n3: for sample (x, y) ‚ààB do\n4:\nCompute prefix length k(s) = max(0, (1‚àís\nS √óœÉ))¬∑\n|y|\n5:\nConstruct context C(s) = (x, y‚â§k(s))\n6:\nGenerate G candidate outputs{oi}G\ni=1 ‚àºœÄŒ∏(¬∑|C(s))\n7:\nfor all oi ‚àà{oi}G\ni=1 do\n8:\nCompute reward R(oi, y; Œ¥(s)) using Eq. 4\n9:\nCompute\nmean\nreward\n¬Ør\n=\nmean({R(oi, y; Œ¥(s))}G\ni=1)\n10:\nCompute difficulty weight w =\n1\noffset+¬Ør\n11:\nCompute curriculum threshold Œ¥(s) = Œ¥min +\n(Œ¥max ‚àíŒ¥min) √ó min(1, s\nS √ó œÉ)\n12:\nCompute RCPA objective JRCPA(Œ∏; s) using Eq. 6\n13:\nUpdate œÄŒ∏ by gradient ascent on JRCPA(Œ∏; s)\n14:\nend for\n15:\ns = s+1\n16: end for\n17: return œÄŒ∏\nThis weight is incorporated into the GRPON objective by\nscaling the advantage term ÀÜAi,t for each sample, ensuring\nthat the model allocates greater capacity to high-value learn-\ning opportunities. We set the offset to 1.5 to scale w into a\nreasonable range, thereby stabilizing the training process.\nAt training step s, the complete RCPA objective is expressed\nas:\nJRCPA(Œ∏; s) = Ex,{oi}‚àºœÄŒ∏old\n\u0014 1\nG\nX\ni,t\n1\n‚àÜi\nmin\n\u0010\nœÅi,t(Œ∏) ÀÜAi,t,\nclip(œÅi,t, 1‚àíœµ, 1+œµ) ÀÜAi,t\n\u0011\nw ‚àíŒ≥DKL\n\u0002\nœÄŒ∏‚à•œÄref\n\u0003\u0015\n,\nwhere ‚àÜi = |oi| ‚àík(s),\nX\ni,t\n=\nG\nX\ni=1\n|oi|\nX\nt=k(s)+1\n.\n(6)\nwhere the advantage ÀÜAi,t is computed using the reward\nR(oi, y; Œ¥(s)). The overall training procedure is summa-\nrized in Algorithm 1.\n5. Experiment\nWe begin this section by outlining our experimental setup\n(Section 5.1). We then present a comprehensive compari-\nson of RCPA against current state-of-the-art methods (Sec-\ntion 5.2). This is followed by ablation studies that assess\nthe contribution of each core component (Section 5.3) and\nparameter sensitivity analyses that evaluate the robustness\nof key hyperparameters (Section 5.4). Additional results on\ncomputational efficiency, the effectiveness of cold start in\nRL, optimization stability, and generalization are provided\nin Section 5.5, Section 5.6, and Section 5.7.\n5.1. Experimental Setup\nBenchmark Datasets. To evaluate RCPA‚Äôs adaptability\nand performance, we conduct experiments on three bench-\nmark datasets spanning diverse domains: image captioning,\ngeometric problem-solving, and medical X-ray diagnostics.\n‚Ä¢ COCO Caption (Chen et al., 2015): A widely used dataset\nfor image captioning, containing 123,287 images. We use\nthe original training and test splits of the dataset.\n‚Ä¢ Geo170K (Gao et al., 2025): A dataset for geometric\nproblem-solving. It is divided into Phase 1 (non-deep\nthinking, direct-answer tasks) and Phase 2 (deep thinking,\nmulti-step tasks). We use Phase 1, which contains 60,252\nsamples, with 57,252 for training and 3,000 for testing.\n‚Ä¢ OpenI (Demner-Fushman et al., 2012): A chest X-ray\ndiagnostic dataset with 6,423 images and corresponding\nradiological reports. The task is to generate concise and\nclinically accurate diagnostic descriptions directly from\nX-ray images. The training set consists of 5,423 images,\nwhile the test set includes 1,000 images.\nThese datasets enable us to evaluate RCPA‚Äôs performance\nacross both general domain (e.g., COCO Caption) and spe-\ncific domain (e.g., Geo170K, OpenI) tasks, providing a\ncomprehensive assessment of domain-adaptive capabilities.\nBaselines. We employ Qwen2.5-VL-7B (Bai et al., 2025)\nas the base model and compare the following adaptation\nstrategies. (1) BASE: Direct inference using the pre-trained\nQwen2.5-VL-7B model. (2) SFT-based Methods: Includ-\ning Parameter-Efficient Fine-tuning (PEFT) via LoRA (Hu\net al., 2021) and Full Fine-tuning (FFT). Furthermore, draw-\ning on relevant methods in incremental learning (Kirk-\npatrick et al., 2016), we incorporate the Kullback-Leibler\n(KL) divergence loss into Full Fine-Tuning (FFT), and de-\nfine this improved approach as Continual Full Fine-Tuning\n(CFFT). Specifically, this KL divergence loss imposes a\nconstraint that aligns the output distribution of the fine-\ntuned model with that of the pre-trained model, effectively\n6\n"}, {"page": 7, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\nDatasets\nMethods\nDomain-Specific Ability\nGeneral-Purpose Ability\nBLEU-1\nROUGE-L\nCIDEr\nSPICE\nMMMU\nMME\nIFEval-P\nIFEval-I\nCOCO\nCaption\nBASE\n0.4457\n0.3672\n0.2259\n0.1783\n0.5122\n2333.36\n0.6211\n0.7038\nPEFT\n0.3722\n0.3081\n0.1231\n0.0862\n0.6067\n2448.67\n0.5416\n0.6535\nFFT\n0.7581\n0.5474\n1.0172\n0.2385\n0.4244\n735.10\n0.2070\n0.3405\nCFFT\n0.6518\n0.4824\n0.7654\n0.2034\n0.4967\n1934.32\n0.5324\n0.6419\nGRPO\n0.2245\n0.2767\n0.2699\n0.1297\n0.5222\n2301.53\n0.6506\n0.7410\nDAPO\n0.2431\n0.2798\n0.2687\n0.1301\n0.5111\n2330.27\n0.6577\n0.7423\nGRPON\n0.4437\n0.3656\n0.3189\n0.1790\n0.5100\n2315.97\n0.6299\n0.7238\nRCPA\n0.7478\n0.5432\n0.9814\n0.2383\n0.5278\n2289.18\n0.6470\n0.7326\nGeo170K\nBASE\n0.3859\n0.3014\n0.2740\n0.2901\n0.5122\n2333.36\n0.6211\n0.7038\nPEFT\n0.0901\n0.1192\n0.0009\n-\n0.6067\n2449.67\n0.5360\n0.6535\nFFT\n0.6098\n0.5526\n2.3109\n0.5627\n0.4667\n2172.37\n0.5693\n0.6451\nCFFT\n0.5719\n0.5091\n1.8827\n0.5079\n0.5078\n2199.59\n0.5888\n0.6676\nGRPO\n0.3799\n0.3113\n0.2661\n0.2878\n0.5022\n2346.08\n0.6373\n0.7131\nDAPO\n0.3835\n0.3189\n0.2776\n0.2989\n0.5111\n2319.25\n0.6285\n0.7110\nGRPON\n0.4086\n0.3431\n0.3543\n0.3350\n0.5122\n2320.54\n0.6414\n0.7062\nRCPA\n0.5998\n0.5501\n2.2821\n0.5623\n0.5122\n2315.37\n0.6414\n0.7278\nOpenI\nBASE\n0.1155\n0.1299\n0.0002\n0.0988\n0.5122\n2333.36\n0.6211\n0.7038\nPEFT\n0.0786\n0.0977\n-\n0.0871\n0.6067\n2449.67\n0.5508\n0.6631\nFFT\n0.3396\n0.2399\n0.0903\n0.1900\n0.4111\n1623.35\n0.5323\n0.6367\nCFFT\n0.2698\n0.1889\n0.0813\n0.1698\n0.4711\n2100.32\n0.5578\n0.6719\nGRPO\n0.1179\n0.1309\n0.0003\n0.0994\n0.5122\n2356.23\n0.6248\n0.7062\nDAPO\n0.1165\n0.1356\n0.0003\n0.0998\n0.5111\n2334.65\n0.6267\n0.7098\nGRPON\n0.1182\n0.1311\n0.0003\n0.0999\n0.5044\n2315.37\n0.6192\n0.7110\nRCPA\n0.3342\n0.2325\n0.0886\n0.1814\n0.5011\n2321.40\n0.6285\n0.7062\nTable 1. Performance comparison of different recommendation methods in terms of Domain-Specific Ability and General Ability. COCO\nCaption, Geo170k, and OpenI are used as benchmark datasets. The results across these datasets demonstrate that RCPA achieves\ndomain-specific performance comparable to that of FFT, while also preserving general capabilities. ‚Äò-‚Äô indicates that the metric is either\ntoo small or the generated output is too long, causing it to be truncated and thus unable to be calculated. The best results are shown in\nbold. The second-best results are marked with an underline.\nmitigating the overwriting of general capabilities by domain-\nspecific signals. (3) RL-based Methods: Group Relative\nPolicy Optimization (GRPO) (Shao et al., 2024), Decoupled\nClip and Dynamic sAmpling Policy Optimization (DAPO)\n(Yu et al., 2025), GRPO for Non-Deep-Thinking Models\n(GRPON). Cold start is added to GRPON to evaluate our\nmethod comprehensively.\nEvaluation Metrics. We evaluate RCPA using a combina-\ntion of task-specific metrics for domain knowledge injection\nand generalization metrics for preserving general-purpose\nperformance.\nDomain-specific ability is assessed with\nBLEU-1 (Papineni et al., 2002), CIDEr (Vedantam et al.,\n2014), ROUGE-L (Lin, 2004), and SPICE (Anderson et al.,\n2016) to measure n-gram overlap, semantic consistency,\nlong-sequence similarity, and structural alignment. General-\npurpose capabilities are evaluated using MMMU (Fu et al.,\n2023) for cross-domain reasoning, MME (Fu et al., 2023)\nfor multimodal understanding, and IFEval (Zhou et al.,\n2023)‚Äîincluding IFEval-Prompt (IFEval-P) and IFEval-\nInstruct (IFEval-I)‚Äîto measure instruction-following fi-\ndelity and consistency with human intent.\nImplementation Details. For model training and infer-\nence, we set the hyperparameters Œ± and Œ≤ in Equation 3\nto 0.6 and 0.7, respectively. In Equation 5, the threshold\nparameters are configured with Œ¥max = 0.7, Œ¥min = 0.8, and\nœÉ = 16. For the RCPA objective (Equation 6), the regu-\nlarization coefficient Œ≥ is set to 0.01. All experiments are\nimplemented using the EasyR1 RL framework (Zheng et al.,\n2025b) and LlamaFactory SFT framework (Zheng et al.,\n2024) for VLMs and conducted on a Linux-based server\nequipped with NVIDIA GPUs.\n7\n"}, {"page": 8, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\nFigure 2. Results of Ablation and Parameter studies. (a) Ablation on COCO Captions demonstrates the contribution of CPP and CDP, with\nCPP leading to larger gains in domain-specific learning, while CDP enhances specialization by reweighting samples based on difficulty.\n(b) The impact of the sliding token ratio (œÉ) on domain knowledge learning and training efficiency reveals the optimal value of 16. (c)\nand (d) Parameter study results show that optimal performance is achieved with Œ± = 0.6, Œ≤ = 0.7, Œ¥min = 0.7, and Œ¥max = 0.8, based on\naggregated evaluation metrics.\n5.2. Performance Comparison\nMain experimental results are summarized in Table 1.\nAcross all benchmarks, RCPA delivers competitive domain-\nspecific performance while preserving general-purpose ca-\npabilities. Analysis of the COCO Caption dataset reveals\nseveral key insights:\n‚Ä¢ FFT vs. Generality: While FFT achieves peak domain\nscores (e.g., 1.0172 CIDEr), it induces severe degrada-\ntion in general benchmarks, with declines of 8.78% on\nMMMU, 1598.26 on MME, and over 36% on IFEval-I.\n‚Ä¢ PEFT Limitations: LoRA mitigates generality loss but\ncompromises instruction-following‚Äîdropping to 0.5416\non IFEval-P‚Äîand remains less effective at domain knowl-\nedge acquisition, scoring only 0.0862 on CIDEr.\n‚Ä¢ GRPON Efficacy: GRPON surpasses standard GRPO\nin domain-specific captioning, yielding significant gains\nin BLEU-1 (+21.92%), ROUGE-L (+8.89%), CIDEr\n(+4.9%), and SPICE (+4.93%). This stems from bypass-\ning unnecessary deep-thinking for non-reasoning tasks.\n‚Ä¢ RCPA Superiority: RCPA matches FFT‚Äôs domain per-\nformance (e.g., 0.7478 BLEU-1 vs.\nFFT‚Äôs 0.7581)\nwhile maintaining high general capability scores, such as\n0.7326 on IFEval-I, striking a superior balance between\nspecialization and generalization.\nConsistent trends are observed on Geo170K and OpenI:\nGeo170K: RCPA achieves a 2.2821 CIDEr, comparable\nto FFT‚Äôs 2.3109, while significantly outperforming FFT\nin general understanding (MMMU: 0.5122 vs. 0.4667).\nOpenI: In medical diagnostics, RCPA maintains a 0.3342\nBLEU-1 and 0.2325 ROUGE-L, while preserving a 0.7062\nIFEval-I score, far exceeding FFT‚Äôs 0.6367.\n5.3. Ablation Study\nAblation experiments on COCO Captions (Figure 2 (a))\nconfirm that both CPP and CDP enhance domain-specific\nlearning, with CPP providing more substantial gains. By\ninjecting and progressively shortening answer prefixes, CPP\ndecomposes response generation into staged subtasks. This\napproach lowers early optimization barriers, increases re-\nward signal density, and strengthens cross-modal grounding.\nConversely, CDP improves specialization by dynamically\nprioritizing difficult, underlearned samples over easy, over-\nlearned cases. This mechanism prevents overfitting while\nencouraging broader coverage of the target domain. To-\ngether, they form a complementary curriculum: CPP guides\nthe learning path through reward densification, while CDP\nsharpens the focus via difficulty-aware prioritization.\n5.4. Parameter Study\nImpact of sliding token ratio. Figure 2 (b) shows that a\nlarger œÉ accelerates answer token restoration. While this\nreduces training iterations, it undermines domain-specific\nlearning by providing insufficient time for the model to\nthoroughly absorb and align with specialized knowledge,\nleading to superficial mastery. Conversely, a smaller ratio\nfacilitates deeper knowledge acquisition but reduces train-\ning efficiency. To balance integration depth with practical\nefficiency, we empirically select œÉ = 16. This setting mod-\nerates token recovery to support incremental learning while\nremaining computationally feasible.\nInfluence of weight and threshold parameters: We as-\nsess the impact of Œ±, Œ≤, Œ¥min, and Œ¥max using an aggregate\nscore of BLEU-1, CIDEr, ROUGE-L, and SPICE to capture\ndiverse aspects of generation quality. For each parameter,\nwe fix one value and vary others from 0 to 1 in steps of\n0.1. Results indicate that Œ± = 0.6 and Œ≤ = 0.7 achieve\nthe optimal balance between semantic similarity, factual\nconsistency, and entity alignment. Similarly, the most effec-\ntive thresholds for progressive generation are Œ¥min = 0.7\nand Œ¥max = 0.8. These configurations are adopted for all\nsubsequent experiments.\n5.5. Computation Efficiency\nAs shown in Figure 3 (a) and (b), the first pre-alignment\nstage accounts for 28% of the total training time. Compared\n8\n"}, {"page": 9, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\nFigure 3. (a) The pre-alignment stage accounts for 28% of the total training time. (b) Under the same batch size, RCPA increases the\ncomputation time per step by 56% compared with GRPO. (c) Compared with GRPO, our RCPA reduces policy update variance by about\n41% (measured via KL divergence between consecutive policies). (d) Compared with GRPO, our proposed RCPA enables a continuous\nand stable increment in reward values throughout the entire training process, demonstrating more favorable reward growth characteristics\nin terms of both sustainability and stability.\nDatasets\nMethods\nDomain-Specific Ability\nGeneral-Purpose Ability\nBLEU-1\nROUGE-L\nCIDEr\nSPICE\nMMMU\nMME\nIFEval-P\nIFEval-I\nCOCO\nCaption\nBASE\n0.4457\n0.3672\n0.2259\n0.1783\n0.5122\n2333.36\n0.6211\n0.7038\nGRPON\n0.4437\n0.3656\n0.3189\n0.1790\n0.5100\n2315.97\n0.6299\n0.7238\nGRPON+CS(0.1)\n0.6234\n0.4326\n0.6776\n0.1997\n0.4811\n1976.37\n0.5267\n0.5324\nGRPON+CS(1.0)\n0.7501\n0.5444\n0.9921\n0.2396\n0.4522\n901.43\n0.3012\n0.4123\nRCPA\n0.7478\n0.5432\n0.9814\n0.2383\n0.5278\n2289.18\n0.6470\n0.7326\nGeo170K\nBASE\n0.3859\n0.3014\n0.2740\n0.2901\n0.5122\n2333.36\n0.6211\n0.7038\nGRPON\n0.4086\n0.3431\n0.3543\n0.3350\n0.5122\n2320.54\n0.6414\n0.7062\nGRPON+CS(0.1)\n0.5545\n0.4943\n1.6893\n0.4923\n0.4922\n2219.54\n0.6026\n0.7062\nGRPON+CS(1.0)\n0.6012\n0.5504\n2.2896\n0.5629\n0.4767\n2199.34\n0.5801\n0.6594\nRCPA\n0.5998\n0.5501\n2.2821\n0.5623\n0.5122\n2315.37\n0.6414\n0.7278\nOpenI\nBASE\n0.1155\n0.1299\n0.0002\n0.0988\n0.5122\n2333.36\n0.6211\n0.7038\nGRPON\n0.1182\n0.1311\n0.0003\n0.0999\n0.5044\n2315.37\n0.6192\n0.7110\nGRPON+CS(0.1)\n0.2723\n0.1820\n0.0823\n0.1379\n0.4743\n1989.34\n0.5792\n0.6723\nGRPON+CS(1.0)\n0.3378\n0.2311\n0.0896\n0.1839\n0.4311\n1710.69\n0.5511\n0.6501\nRCPA\n0.3342\n0.2325\n0.0886\n0.1814\n0.5011\n2321.40\n0.6285\n0.7062\nTable 2. Performance comparison of different cold start data volumes in terms of Domain-Specific Ability and General Ability. COCO\nCaption, Geo170k, and OpenI are used as benchmark datasets. CS(*) means that data of * epoch is used in the cold start. The best results\nare shown in bold. The second-best results are marked with an underline.\nwith GRPO, RCPA is designed with a more sophisticated\nvalue function, which consequently increases the compu-\ntation time per step by 56% under the same batch size.\nNevertheless, considering the significant performance gains\nbrought by our method, such computational overhead is\nwell-justified.\n5.6. The Effectiveness of Cold Start in RL\nIn order to validate the efficacy of cold-start strategies, we\nconduct controlled experiments by performing cold-start\nwith partial target-domain training data, specifically adopt-\ning 0.1 epoch and 1.0 epoch of data for the cold-start phase,\nrespectively. As shown in Table 2, the experimental results\nreveal two key findings: (1) Even a modest cold-start (0.1\nepoch) leads to improved target-domain performance, yet\nit irreversibly degrades the model‚Äôs general capabilities‚Äîa\ndegradation that cannot be recuperated via subsequent re-\ninforcement learning (RL) fine-tuning. (2) Increasing the\ncold-start data volume to 1.0 epoch yields marginally su-\nperior target-domain performance compared to our RCPA\nmethod, but this gain comes at the expense of substantial\ngeneralization degradation (e.g., the MMMU score drops\nfrom 0.5122 to 0.4522 on the COCO Caption dataset).\n5.7. Optimization Stability and Generalization\nWe visualized the Kullback-Leibler (KL) divergence and av-\nerage reward throughout the training process. As illustrated\nin Figure 3 (c) and (d), our RCPA method enables stable\ntraining: its KL divergence changes in a relatively smooth\npattern, and the overall KL divergence is reduced by 41%\n9\n"}, {"page": 10, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\ncompared to that of GRPO. Meanwhile, the visualization of\nthe average reward reveals that, benefiting from its targeted\ndesign, the average reward of our method steadily rises to\na relatively high level‚Äîthis indicates that our method can\neffectively learn the knowledge of the target domain. In\ncontrast, GRPO exhibits severe fluctuations in the average\nreward, and its overall average reward does not increase sig-\nnificantly. All these observations collectively demonstrate\nthe superiority of our proposed method.\n6. Conclusion\nWe propose Reinforced Curriculum Pre-Alignment (RCPA),\na novel framework that enables VLMs to acquire special-\nized domain knowledge without compromising general-\npurpose capabilities. By integrating a two-phase process:\npre-alignment for stable knowledge grounding and reinforce-\nment alignment for behavioral refinement, RCPA effectively\nbridges the gap between SFT and RL-based methods. The\ninclusion of CPP and CDP modules further ensures efficient\nlearning while preventing optimization collapse and over-\nfitting. Extensive experiments across medical and geomet-\nric benchmarks demonstrate that RCPA achieves domain-\nspecific performance competitive with FFT while signifi-\ncantly outperforming existing baselines in retaining general\nmultimodal understanding and instruction-following.\nReferences\nAnderson, P., Fernando, B., Johnson, M., and Gould, S.\nSpice: Semantic propositional image caption evaluation.\nIn European conference on computer vision, pp. 382‚Äì398.\nSpringer, 2016.\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin,\nJ., Zhou, C., and Zhou, J. Qwen-vl: A versatile vision-\nlanguage model for understanding, localization, text read-\ning, and beyond. arXiv preprint arXiv:2308.12966, 2023.\nBai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang,\nK., Wang, P., Wang, S., Tang, J., Zhong, H., Zhu, Y.,\nYang, M., Li, Z., Wan, J., Wang, P., Ding, W., Fu, Z., Xu,\nY., Ye, J., Zhang, X., Xie, T., Cheng, Z., Zhang, H., Yang,\nZ., Xu, H., and Lin, J. Qwen2.5-vl technical report. arXiv\npreprint arXiv:2502.13923, 2025.\nChen, J., Yang, D., Jiang, Y., Li, M., Wei, J., Hou, X., and\nZhang, L. Efficiency in focus: Layernorm as a catalyst\nfor fine-tuning medical visual language models. In Pro-\nceedings of the 32nd ACM International Conference on\nMultimedia, pp. 3122‚Äì3130, 2024a.\nChen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S.,\nDoll¬¥ar, P., and Zitnick, C. L. Microsoft coco captions:\nData collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015.\nChen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E.,\nZhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding per-\nformance boundaries of open-source multimodal models\nwith model, data, and test-time scaling. arXiv preprint\narXiv:2412.05271, 2024b.\nChu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans,\nD., Le, Q. V., Levine, S., and Ma, Y. Sft memorizes, rl\ngeneralizes: A comparative study of foundation model\npost-training. arXiv preprint arXiv:2501.17161, 2025.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen,\nX., Chowdhery, A., Valter, D., Narang, S., Mishra, G.,\nYu, A. W., Zhao, V., Huang, Y., Dai, A. M., Yu, H.,\nPetrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A.,\nZhou, D., Le, Q. V., and Wei, J. Scaling instruction-\nfinetuned language models.\nArXiv, abs/2210.11416,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:253018554.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V.,\nWenzek, G., Guzm¬¥an, F., Grave, E., Ott, M., Zettlemoyer,\nL., and Stoyanov, V. Unsupervised cross-lingual repre-\nsentation learning at scale. CoRR, abs/1911.02116, 2019.\nURL http://arxiv.org/abs/1911.02116.\nDemner-Fushman, D., Antani, S., Simpson, M., and Thoma,\nG. R. Design and development of a multimodal biomedi-\ncal information retrieval system. Journal of Computing\nScience and Engineering, 6(2):168‚Äì177, 2012.\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettle-\nmoyer, L.\nQlora: Efficient finetuning of quantized\nllms.\nArXiv, abs/2305.14314, 2023.\nURL https:\n//api.semanticscholar.org/CorpusID:\n258841328.\nDong, H., Kang, Z., Yin, W., Liang, X., Feng, C., and Ran, J.\nScalable vision language model training via high quality\ndata curation. arXiv preprint arXiv:2501.05952, 2025.\nDuan, Z., Cheng, H., Xu, D., Wu, X., Zhang, X., Ye, X., and\nXie, Z. Cityllava: Efficient fine-tuning for vlms in city\nscenario. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 7180‚Äì\n7189, 2024.\nFu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu,\nZ., Lin, W., Yang, J., Zheng, X., Li, K., Sun, X., and Ji, R.\nMme: A comprehensive evaluation benchmark for mul-\ntimodal large language models. ArXiv, abs/2306.13394,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:259243928.\n10\n"}, {"page": 11, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\nGao, J., Pi, R., Zhang, J., Ye, J., Zhong, W., Wang,\nY., HONG, L., Han, J., Xu, H., Li, Z., and Kong,\nL. G-LLaVA: Solving geometric problem with multi-\nmodal large language model.\nIn The Thirteenth In-\nternational Conference on Learning Representations,\n2025. URL https://openreview.net/forum?\nid=px1674Wp3C.\nGao, Z., Chen, Z., Cui, E., Ren, Y., Wang, W., Zhu, J., Tian,\nH., Ye, S., He, J., Zhu, X., et al. Mini-internvl: a flexible-\ntransfer pocket multi-modal model with 5% parameters\nand 90% performance. Visual Intelligence, 2(1):1‚Äì17,\n2024.\nGekhman, Z., Yona, G., Aharoni, R., Eyal, M., Feder,\nA., Reichart, R., and Herzig, J.\nDoes fine-tuning\nllms on new knowledge encourage hallucinations?\nArXiv,\nabs/2405.05904,\n2024.\nURL\nhttps:\n//api.semanticscholar.org/CorpusID:\n269635770.\nGuo, K., Diefenbach, D., Gourru, A., and Gravier,\nC.\nFine-tuning strategies for domain specific ques-\ntion answering under low annotation budget con-\nstraints. 2023 IEEE 35th International Conference on\nTools with Artificial Intelligence (ICTAI), pp. 166‚Äì171,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:252620041.\nHonnibal, M., Montani, I., Van Landeghem, S., and Boyd, A.\nspaCy: Industrial-strength Natural Language Processing\nin Python. 2020. doi: 10.5281/zenodo.1212303.\nHu, J. E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y.,\nWang, S., and Chen, W.\nLora:\nLow-rank adapta-\ntion of large language models. ArXiv, abs/2106.09685,\n2021.\nURL https://api.semanticscholar.\norg/CorpusID:235458009.\nHu, Z., Lan, Y., Wang, L., Xu, W., Lim, E.-P., Lee,\nR. K.-W., Bing, L., and Poria, S.\nLlm-adapters:\nAn adapter family for parameter-efficient fine-tuning\nof large language models.\nArXiv, abs/2304.01933,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:257921386.\nKirkpatrick, J., Pascanu, R., Rabinowitz, N. C., Veness, J.,\nDesjardins, G., Rusu, A. A., Milan, K., Quan, J., Ra-\nmalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath,\nC., Kumaran, D., and Hadsell, R. Overcoming catas-\ntrophic forgetting in neural networks. Proceedings of\nthe National Academy of Sciences, 114:3521 ‚Äì 3526,\n2016.\nURL https://api.semanticscholar.\norg/CorpusID:4704285.\nKumar, K., Ashraf, T., Thawakar, O., Anwer, R. M.,\nCholakkal, H., Shah, M., Yang, M.-H., Torr, P. H. S.,\nKhan, F. S., and Khan, S. Llm post-training: A deep\ndive into reasoning large language models, 2025. URL\nhttps://arxiv.org/abs/2502.21321.\nLai, Y., Zhong, J., Li, M., Zhao, S., and Yang, X. Med-\nr1: Reinforcement learning for generalizable medical\nreasoning in vision-language models.\narXiv preprint\narXiv:2503.13939, 2025.\nLambert, N. Reinforcement learning from human feed-\nback.\nArXiv, abs/2504.12501, 2025.\nURL https:\n//api.semanticscholar.org/CorpusID:\n277857379.\nLester, B., Al-Rfou, R., and Constant, N. The power of scale\nfor parameter-efficient prompt tuning. In Conference\non Empirical Methods in Natural Language Processing,\n2021.\nURL https://api.semanticscholar.\norg/CorpusID:233296808.\nLi, J., Selvaraju, R. R., Gotmare, A. D., Joty, S. R., Xiong,\nC., and Hoi, S. C. H. Align before fuse: Vision and\nlanguage representation learning with momentum dis-\ntillation.\nIn Neural Information Processing Systems,\n2021a.\nURL https://api.semanticscholar.\norg/CorpusID:236034189.\nLi, Q., Gong, B., Cui, Y., Kondratyuk, D., Du, X.,\nYang, M.-H., and Brown, M.\nTowards a unified\nfoundation model:\nJointly pre-training transformers\non unpaired images and text. ArXiv, abs/2112.07074,\n2021b.\nURL https://api.semanticscholar.\norg/CorpusID:245131381.\nLi, X. L. and Liang, P.\nPrefix-tuning:\nOptimiz-\ning continuous prompts for generation.\nProceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1:\nLong Papers), pp. 4582‚Äì4597,\n2021.\nURL https://api.semanticscholar.\norg/CorpusID:230433941.\nLi, Y., Tian, M., Zhu, D., Zhu, J., Lin, Z., Xiong, Z., and\nZhao, X. Drive-r1: Bridging reasoning and planning in\nvlms for autonomous driving with reinforcement learning.\narXiv preprint arXiv:2506.18234, 2025.\nLi, Z. and Hoiem, D. Learning without forgetting. IEEE\nTransactions on Pattern Analysis and Machine Intel-\nligence, 40:2935‚Äì2947, 2016. URL https://api.\nsemanticscholar.org/CorpusID:4853851.\nLin, C.-Y. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pp.\n74‚Äì81, 2004.\n11\n"}, {"page": 12, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\nLiu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines\nwith visual instruction tuning, 2023a.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. In NeurIPS, 2023b.\nLiu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S.,\nand Lee, Y. J.\nLlava-next:\nImproved reasoning,\nocr, and world knowledge, January 2024.\nURL\nhttps://llava-vl.github.io/blog/\n2024-01-30-llava-next/.\nMA, X., ZHANG, J., Guo, S., and Xu, W. Swapprompt:\nTest-time\nprompt\nadaptation\nfor\nvision-language\nmodels.\nIn Oh, A., Naumann, T., Globerson, A.,\nSaenko, K., Hardt, M., and Levine, S. (eds.), Ad-\nvances in Neural Information Processing Systems,\nvolume 36, pp. 65252‚Äì65264. Curran Associates, Inc.,\n2023.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2023/file/\ncdd0640218a27e9e2c0e52e324e25db0-Paper-Conference.\npdf.\nNiu, S., Wu, J., Zhang, Y., Wen, Z., Chen, Y., Zhao,\nP., and Tan, M.\nTowards stable test-time adapta-\ntion in dynamic wild world.\nArXiv, abs/2302.12400,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:257206115.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a\nmethod for automatic evaluation of machine translation.\nIn Isabelle, P., Charniak, E., and Lin, D. (eds.), Proceed-\nings of the 40th Annual Meeting of the Association for\nComputational Linguistics, pp. 311‚Äì318, Philadelphia,\nPennsylvania, USA, July 2002. Association for Computa-\ntional Linguistics. doi: 10.3115/1073083.1073135. URL\nhttps://aclanthology.org/P02-1040/.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S.,\nManning,\nC. D.,\nand Finn,\nC.\nDirect prefer-\nence optimization:\nYour language model is se-\ncretly a reward model.\nArXiv,\nabs/2305.18290,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:258959321.\nRebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert,\nC. H.\nicarl:\nIncremental classifier and representa-\ntion learning. 2017 IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pp. 5533‚Äì5542,\n2016.\nURL https://api.semanticscholar.\norg/CorpusID:206596260.\nReimers, N. and Gurevych, I. Sentence-bert: Sentence\nembeddings using siamese bert-networks. In Proceed-\nings of the 2019 Conference on Empirical Methods in\nNatural Language Processing. Association for Compu-\ntational Linguistics, 11 2019. URL http://arxiv.\norg/abs/1908.10084.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\nArXiv, abs/1707.06347, 2017.\nURL https://api.\nsemanticscholar.org/CorpusID:28695052.\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J.-M., Zhang,\nM., Li, Y. K., Wu, Y., and Guo, D.\nDeepseek-\nmath: Pushing the limits of mathematical reasoning\nin open language models.\nArXiv, abs/2402.03300,\n2024.\nURL https://api.semanticscholar.\norg/CorpusID:267412607.\nShenfeld,\nI.,\nPari,\nJ.,\nand Agrawal,\nP.\nRl‚Äôs ra-\nzor: Why online reinforcement learning forgets less.\n2025.\nURL https://api.semanticscholar.\norg/CorpusID:281103647.\nSingh, R., Singh, A., Nair, V., Wang, Y.-X., van der Maaten,\nL., Joulin, A., and Misra, I. Flava: A foundational lan-\nguage and vision alignment model. In ICML, 2022.\nVedantam, R., Zitnick, C. L., and Parikh, D.\nCider:\nConsensus-based image description evaluation. 2015\nIEEE\nConference\non\nComputer\nVision\nand\nPat-\ntern\nRecognition\n(CVPR),\npp.\n4566‚Äì4575,\n2014.\nURL\nhttps://api.semanticscholar.org/\nCorpusID:9026666.\nWang, H., Wu, Z., Kolar, G. J., Korsapati, H. R., Bartlett,\nB., Hull, B., and Sun, J. Reinforcement learning for out-\nof-distribution reasoning in llms: An empirical study on\ndiagnosis-related group coding. ArXiv, abs/2505.21908,\n2025a.\nURL https://api.semanticscholar.\norg/CorpusID:278960217.\nWang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen,\nK., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du,\nM., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and\nLin, J. Qwen2-vl: Enhancing vision-language model‚Äôs\nperception of the world at any resolution. arXiv preprint\narXiv:2409.12191, 2024a.\nWang, W., Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z.,\nZhu, J., Zhu, X., Lu, L., Qiao, Y., and Dai, J. Enhancing\nthe reasoning ability of multimodal large language mod-\nels via mixed preference optimization. arXiv preprint\narXiv:2411.10442, 2024b.\nWang, W., Gao, Z., Gu, L., Pu, H., Cui, L., Wei, X., Liu, Z.,\nJing, L., Ye, S., Shao, J., et al. Internvl3.5: Advancing\nopen-source multimodal models in versatility, reasoning,\nand efficiency. arXiv preprint arXiv:2508.18265, 2025b.\n12\n"}, {"page": 13, "text": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs\nYang, X., Chen, X., Li, M., Wei, K.-J., and Deng, C.\nA versatile framework for continual test-time domain\nadaptation: Balancing discriminability and generaliz-\nability. 2024 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pp. 23731‚Äì23740,\n2024.\nURL https://api.semanticscholar.\norg/CorpusID:271691678.\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y.,\nFan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma,\nB., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang,\nW., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C.,\nYu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\nMa, W., Zhang, Y.-Q., Yan, L., Qiao, M., Wu, Y.-X.,\nand Wang, M. Dapo: An open-source llm reinforce-\nment learning system at scale. ArXiv, abs/2503.14476,\n2025.\nURL https://api.semanticscholar.\norg/CorpusID:277104124.\nZhang, W., Xie, Y., Sun, Y., Chen, Y., Wang, G., Li,\nY., Ding, B., and Zhou, J.\nOn-policy rl meets off-\npolicy experts:\nHarmonizing supervised fine-tuning\nand reinforcement learning via dynamic weighting.\n2025.\nURL https://api.semanticscholar.\norg/CorpusID:280671636.\nZheng, C., Liu, S., Li, M., Chen, X., Yu, B., Gao, C., Dang,\nK., Liu, Y., Men, R., Yang, A., Zhou, J., and Lin, J. Group\nsequence policy optimization. ArXiv, abs/2507.18071,\n2025a.\nURL https://api.semanticscholar.\norg/CorpusID:280017753.\nZheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng,\nZ., and Ma, Y.\nLlamafactory: Unified efficient fine-\ntuning of 100+ language models. In Proceedings of the\n62nd Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 3: System Demonstrations),\nBangkok, Thailand, 2024. Association for Computational\nLinguistics. URL http://arxiv.org/abs/2403.\n13372.\nZheng, Y., Lu, J., Wang, S., Feng, Z., Kuang, D., and Xiong,\nY. Easyr1: An efficient, scalable, multi-modality rl train-\ning framework. https://github.com/hiyouga/\nEasyR1, 2025b.\nZhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan,\nY., Zhou, D., and Hou, L. Instruction-following evalua-\ntion for large language models. ArXiv, abs/2311.07911,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:265157752.\nZhu, J., Wang, W., Chen, Z., Liu, Z., Ye, S., Gu, L., Tian,\nH., Duan, Y., Su, W., Shao, J., et al. Internvl3: Exploring\nadvanced training and test-time recipes for open-source\nmultimodal models. arXiv preprint arXiv:2504.10479,\n2025.\n13\n"}]}