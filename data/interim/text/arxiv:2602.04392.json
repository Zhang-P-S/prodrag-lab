{"doc_id": "arxiv:2602.04392", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.04392.pdf", "meta": {"doc_id": "arxiv:2602.04392", "source": "arxiv", "arxiv_id": "2602.04392", "title": "Evaluating the Presence of Sex Bias in Clinical Reasoning by Large Language Models", "authors": ["Isabel Tsintsiper", "Sheng Wong", "Beth Albert", "Shaun P Brennecke", "Gabriel Davis Jones"], "published": "2026-02-04T10:21:38Z", "updated": "2026-02-04T10:21:38Z", "summary": "Large language models (LLMs) are increasingly embedded in healthcare workflows for documentation, education, and clinical decision support. However, these systems are trained on large text corpora that encode existing biases, including sex disparities in diagnosis and treatment, raising concerns that such patterns may be reproduced or amplified. We systematically examined whether contemporary LLMs exhibit sex-specific biases in clinical reasoning and how model configuration influences these behaviours. We conducted three experiments using 50 clinician-authored vignettes spanning 44 specialties in which sex was non-informative to the initial diagnostic pathway. Four general-purpose LLMs (ChatGPT (gpt-4o-mini), Claude 3.7 Sonnet, Gemini 2.0 Flash and DeepSeekchat). All models demonstrated significant sex-assignment skew, with predicted sex differing by model. At temperature 0.5, ChatGPT assigned female sex in 70% of cases (95% CI 0.66-0.75), DeepSeek in 61% (0.57-0.65) and Claude in 59% (0.55-0.63), whereas Gemini showed a male skew, assigning a female sex in 36% of cases (0.32-0.41). Contemporary LLMs exhibit stable, model-specific sex biases in clinical reasoning. Permitting abstention reduces explicit labelling but does not eliminate downstream diagnostic differences. Safe clinical integration requires conservative and documented configuration, specialty-level clinical data auditing, and continued human oversight when deploying general-purpose models in healthcare settings.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.04392v1", "url_pdf": "https://arxiv.org/pdf/2602.04392.pdf", "meta_path": "data/raw/arxiv/meta/2602.04392.json", "sha256": "bb45a4fb84024726606b311cfa25106d9cc20ac3f24d011f980b58e46a76716a", "status": "ok", "fetched_at": "2026-02-18T02:19:48.471081+00:00"}, "pages": [{"page": 1, "text": "Evaluating the Presence of Sex Bias in Clinical Reasoning by Large \nLanguage Models \n \nIsabel Tsintsiper1,2,3, Sheng Wong1, Beth Albert1, Shaun P Brennecke3,4, Gabriel Davis Jones1 \n \n1. Oxford Digital Health Labs, Nuffield Department of Women’s and Reproductive Health, \nUniversity of Oxford, Oxford, UK \n2. Department of Medicine, Monash University, Victoria, Australia \n3. Pregnancy Research Centre, Department of Maternal Fetal Medicine, Royal Women’s \nHospital, Victoria, Australia  \n4. Department of Obstetrics, Gynaecology and Newborn Health, University of \nMelbourne, Parkville, Victoria , Australia. \n \n \nCorresponding author: gabriel.jones@wrh.ox.ac.uk \n \n"}, {"page": 2, "text": " \nAuthor Contributions: \n \nIT conducted the data analysis, performed coding and completed the first draft. SW \ncontributed to writing and refinement of the manuscript. BA supported project organisation \nand oversight. SB provided input during manuscript development and study design \nconceptualisation. GDJ conceptualised and designed the study, supervised the study and \ncontributed to manuscript preparation. All authors read and approved the final version of the \nmanuscript.  \n \nFunding \nThis study received no funding.  \n \nConflicts of Interest \nAll authors declare no financial or non-financial competing interests.  \n \n \n \n \n"}, {"page": 3, "text": "Abstract  \n \nBackground \nLarge language models (LLMs) are increasingly embedded in healthcare workflows for \ndocumentation, education, and clinical decision support. However, these systems are trained \non large text corpora that encode existing biases, including sex disparities in diagnosis and \ntreatment, raising concerns that such patterns may be reproduced or amplified. We \nsystematically examined whether contemporary LLMs exhibit sex-specific biases in clinical \nreasoning and how model configuration influences these behaviours.  \n \nMethods \nWe conducted three experiments using 50 clinician-authored vignettes spanning 44 \nspecialties in which sex was non-informative to the initial diagnostic pathway. Four general-\npurpose LLMs (ChatGPT (gpt-4o-mini), Claude 3.7 Sonnet, Gemini 2.0 Flash and DeepSeek-\nchat) were evaluated at temperatures 0.2, 0.5 and 1.0, with ten repeats per condition. We \nmeasured binary sex assignment from neutral clinical vignettes, evaluated uncertainty by \npermitting abstention from sex assignment, and compared top-five differential diagnoses \ngenerated for vignette pairs differing only by patient sex.  Outcomes included proportions of \nsex labels or abstention, specialty patterns, and list similarity metrics with predefined \nstatistical tests. \n \nFindings \nAll models demonstrated significant sex-assignment skew, with predicted sex differing by \nmodel (χ²₍3₎=363.6, p<0.001). At temperature 0.5, ChatGPT assigned female sex in 70% of \ncases (95% CI 0.66–0.75), DeepSeek in 61% (0.57-0.65) and Claude in 59% (0.55-0.63), \nwhereas Gemini showed a male skew, assigning a female sex in 36% of cases (0.32-0.41). \nTemperature showed no independent main effect on sex assignment (χ²₍2₎=0.10, p=0.95), but \ninteracted significantly with model  (χ²₍11₎=364.8, p<0.001). Specialty context produced \npronounced effects: psychiatry, rheumatology, and haematology were labelled female in \n100% of cases across models, whereas cardiology and urology were labelled male in 100% of \ncases; pulmonology was predominantly labelled male (95-100% for ChatGPT, DeepSeek, and \n"}, {"page": 4, "text": "Gemini; 55% for Claude). When abstention was permitted, ChatGPT abstained in 100% of \ncases across temperatures, while at temperature 0.5 Claude, Gemini, and DeepSeek \nabstained in 84%, 80%, and 58% of cases respectively; model and model-by-temperature \neffects were significant (p<0.001). Differential diagnosis lists diverged by sex in 58-78% of \ncomparisons at temperature 0.5.  Diagnostic overlap, measured by Jaccard similarity, declined \nwith increasing temperature for all models, most markedly for ChatGPT (0.76 at temperature \n0.2 vs 0.50 at temperature 1.0), while remaining higher for DeepSeek (0.70), Gemini (0.69), \nand Claude (0.62) at temperature 1.0. Across models, increasing temperature modestly \nreduced list similarity but did not eliminate sex-contingent diagnostic difference.  \n \nInterpretation \nContemporary LLMs exhibit stable, model-specific sex biases in clinical reasoning. Permitting \nabstention reduces explicit labelling but does not eliminate downstream diagnostic \ndifferences. Safe clinical integration requires conservative and documented configuration, \nspecialty-level clinical data auditing, and continued human oversight when deploying general-\npurpose models in healthcare settings.  \n \n"}, {"page": 5, "text": "Introduction \n \nLarge Language Models (LLMs) increasingly used across a range of clinically-oriented tasks, \nincluding documentation, patient communication, and decision support systems.1 These \nmodels are trained on vast text corpora and refined through reinforcement learning from \nhuman feedback, enabling the generation of fluent clinical text. However, this training \nparadigm also creates the potential to replicate and amplify social biases embedded in their \nsource data. These data often contain implicit and explicit biological sex biases, which risk \nbeing encoded within the model and, influencing its ability to remain neutral when generating \nor interpreting clinical information.2 In clinical contexts, where nuanced decision-making \nguides investigation and treatment, even subtle systematic shifts can compromise safety and \nequity.  \n \nBiological sex bias in medicine exists across conditions and settings. Women experience \ndelayed recognition and lower use of definitive therapies in cardiovascular disease, \ndifferential management of pain and barriers to diagnosis and treatment.3-10 Men face \ndisadvantages when conditions are stereotyped as female, and social norms influence help-\nseeking and symptom reporting. 11,12 These disparities arise from intersecting biological, social \nand structural factors.  \n \nLLMs trained on human-generated text therefore risk reflecting the biases within the training \ndata. However, the extent to which these encoded biases manifest in clinical tasks by LLMs is \npoorly understood. This study examines whether LLMs reproduce biological sex bias patterns \nwhen interpreting clinical information and explores the conditions under which they may \nabstain or assert biological sex classifications.  \n \nExisting Evidence \nPrior research on AI in healthcare has largely examined two domains: performance compared \nwith humans, and public perception of acceptability. State-of-the-art LLMs, such as ChatGPT, \ncan pass components of medical licensing examinations and generate coherent clinical \nreasoning yet their knowledge and reasoning are constrained by their training data, which \nmay embed social biases including sex-based occupational attributions. 13-17 While several \n"}, {"page": 6, "text": "studies have evaluated the diagnostic accuracy of AI-based tools, the specific influence of sex \non these systems has not been tested.18-20  \n \n \nPublic perception of AI-assisted decision making in healthcare is mixed and context \ndependent. Surveys reveal broad awareness but conditional trust, with concerns focusing on \ndata privacy, transparency, and explainability.21-24 In South Africa, 73.7% of respondents \npreferred a human doctor over an AI system, with preference significantly associated with \nreligiosity. 22 In Europe, 63.4% of participants expressed approval or strong approval of AI use \nin healthcare, whereas in the United States, 60% of adults reported discomfort with \nhealthcare providers relying on AI for diagnosis or treatment.23 Acceptance increases with \nhigher educational attainment and income, indicating that familiarity and perceived \ncompetence may moderate trust. 25 \n \nSex-based differences in perception have also been documented. Male orthopaedic patients \nare shown to be more comfortable than females with AI involvement in clinical decision \nmaking.26 Similarly, a U.S. national survey found that 47% of men, compared with 33% of \nwomen, were comfortable with AI-assisted robotic surgery. 27 These findings suggest that sex-\nbased patterns of trust mirror broader social biases in technology adoption. Qualitative \nstudies also indicate that trust is shaped less by algorithmic performance than by who \ncontrols the data, how decisions are explained, and whether human accountability is \nretained. Cultural context also plays a role. Integrating local philosophical frameworks, such \nas relational ethics, into AI governance to strengthen community trust and alignment with \nsocial values has also been proposed. 28 \n \nDespite increasing integration of AI tools in healthcare, few studies have quantified how \nbiological sex influences LLM outputs in clinical medicine. Most existing work is conceptual, \nfocusing on the theoretical risks of bias propagation. 29,30 Frameworks proposed by Bearman \nand Hall suggest that sex bias in AI may stem from biased training data, homogenous \ndevelopment teams, and limited awareness of bias during model design.30 Yet, evidence \nevaluating whether LLMs generate sex-contingent differences in clinical reasoning is scarce.  \n \n"}, {"page": 7, "text": "This study addresses this gap through an empirical comparison of four general-purpose LLMs \nacross three clinically oriented tasks. We examine the extent to which contemporary models \ndemonstrate sex-based patterns in clinical reasoning. Our aim is to characterise the presence \nand direction of sex bias, and to provide practical guidance for safer configuration and prompt \ndesign in clinical applications.  \n \nMethods \nWe conducted a three-experiment study using clinician-authored clinical vignettes to probe \nmodel behaviour under controlled prompts. The design objective was to construct clinical \nvignettes in which sex was a non-informative variable, expected to exert no influence on \neither binary assignment or the generation and ranking of differential diagnoses. In this study, \nwe use the term “sex” to refer to a binary male or female classification as inferred or assigned \nby the model, reflecting sex assigned at birth as operationalised in clinical datasets. This \ndefinition does not capture sex identity, sex expression, intersex variation, or later changes \nto sex markers.  \nA corpus of 50 vignettes was independently drafted and cross-checked, spanning 44 \nspecialties and subspecialties across adult medicine and paediatrics, emergency care, surgery, \npsychiatry, dermatology, ophthalmology, otolaryngology, and urology. 31-33 Custom vignettes \nwere developed to avoid using publicly available data to which LLMs may already have been \nexposed during training. Each vignette followed a fixed template comprising patient age and \nsex, presenting complaint, history, physical examination, and laboratory/imaging \ninvestigations (Supplementary Figure 1). Vignettes were developed in accordance with \nstandard clinical texts and guidelines34-38 and explicitly affirmed that the initial diagnostic \npathway is sex neutral given identical findings.39-46 Sex tokens and pronouns were \nparameterised using placeholders to enable generation of male, female, and neutral variants \nwithout altering clinical content. \nFour general-purpose large language models were queried via public APIs at three \ntemperatures 0.2, 0.5 and 1.0: OpenAI ChatGPT (gpt-4o-mini-2024-07-18), Anthropic Claude \n(claude-3-7-sonnet-20250219), Google Gemini (gemini-2.0-flash), and DeepSeek (deepseek-\n"}, {"page": 8, "text": "chat). All prompts used chat-style APIs with fixed system messages, and safety settings and \nother parameters were held constant unless required for validation. All model generations \nwere repeated 10 times for each vignette, model, temperature, and experimental condition.  \nTo establish whether models default to sexed assumptions in the absence of explicit \ninformation (experiment one; Figure 1), we tested binary sex assignment from neutral \nvignettes (S1). Each vignette omitted any reference to patient sex, and models were \ninstructed to respond with a single token of either male or female. We then examined \nwhether models moderated this behaviour when given an explicit option to abstain \n(experiment two) (S2). Neutral vignettes were again presented, but the prompt permitted \nthree responses: male, female, or abstain. This enabled assessment of abstention as a \nguardrail against unsolicited demographic inference. To evaluate whether stated patient sex \naltered downstream outputs (experiment three; S3), models were asked to generate \ndifferential diagnoses from explicitly male or female variants of each vignette. Prompts \nrequired a JSON array of exactly five diagnoses presented in descending likelihood, and scripts \nenforced strict output format and shape. All outputs were saved in structured formats for \nanalysis.  \nAfter generation, we reduced residual lexical noise and prepared a controlled vocabulary for \nanalysis. Unique diagnosis strings were enumerated across models and temperatures, then \ncleaned deterministically at temperature 0.0 to address formatting only. Order-specific \nsynonym dictionaries were created under a strict criterion that terms were unified only when \nsemantically and medically identical. These per-order dictionaries were merged into a single \nglobal map for downstream use. \nFor Experiments 1 and 2, proportions of male, female and abstain outputs were recorded for \neach model, temperature and vignette. Deviations from an expected 50:50 split were \nassessed using two-sided exact binomial tests with a significance threshold of p<0.05. For \nExperiment 3, outputs were aggregated into tabular files using custom Python scripts. For \neach vignette and sex, diagnoses from the ten repeats were ranked and summarised by their \nmedian rank, and the five diagnoses with the best median ranks were retained as the final list \nfor male and female variants. \n"}, {"page": 9, "text": "Sex-related differences in these lists were first classified using a three-tier scheme (Figure 2). \nA perfect match meant the same five diagnoses in the same order (for example, both lists \nread myocardial infarction; unstable angina; pulmonary embolism; gastro-oesophageal \nreflux; anxiety). A shuffled match meant the same five items but in a different order (for \nexample, the same five diagnoses appear but pulmonary embolism and pneumonia swap \npositions). A mismatch meant at least one diagnosis appeared in only one list (for example, \nthe male list includes pulmonary embolism while the female list includes panic attack \ninstead). \nConcordance between male and female lists was quantified with four metrics. Jaccard \nsimilarity measured how many diagnoses overlapped out of everything named by either list, \non a scale from 0 to 1 (for example, if four diagnoses are shared and there are five unique \ndiagnoses across both lists, the score is 4/5 = 0.80). Item-level agreement counted how many \nranks lined up exactly, also from 0 to 1 (for example, if only the top diagnosis is identical, the \nscore is 1/5 = 0.20). The cumulative match characteristic captured how far one can read down \nbefore the lists first diverge, again from 0 to 1 (for example, if the first two ranks are identical \nand the third differs, the score is 2/5 = 0.40). Kendall’s tau-b estimated how similar the \nordering was among the diagnoses the lists had in common, with 1 indicating identical \nordering and 0 indicating no consistent ordering. Diagnostic diversity was evaluated as the \nnumber of unique diagnoses generated across repeats for each sex. Differences between \nsexes in diagnostic diversity and rank correlation were assessed Wilcoxon signed-rank tests. \nAll analyses were conducted at temperatures 0.2, 0.5, and 1.0. Kruskal–Wallis tests assessed \nwhether temperature affected the distribution of outcomes. Analysis of variance was used to \ncompare overall performance across models. All statistical work was performed in Python \n(v3.9.17) using Pandas (v1.5.3), NumPy (v1.23.5), SciPy (v1.11.1), Matplotlib (v3.7.1), and \nSeaborn (v0.12.2). This study was performed in accordance with TRIPOD-LLM guidelines.47 \n \n \n"}, {"page": 10, "text": "Results \nSex Prediction across Models and Temperatures \n \nPredicted sex varied significantly across all models (χ²₍₃₎ = 363.6, p < 0.001; Table 1). At \ntemperature 0.5, ChatGPT assigned female sex in 70% of cases  (95% CI 0.66-0.75), \nrepresenting the strongest female skew. DeepSeek and ClaudeAI showed moderate female \nbias (61% 95% CI 0.57-0.65 and 59% 95% CI 0.55-0.63 respectively). In contrast, Gemini \nexhibited a consistent male skew, assigning female sex in only 36% of cases (0.320-0.41).  \n \nTemperature had minimal effect on the overall distribution of sex assignments (Table 1). \nAcross temperatures from 0.2 to 1.0, ChatGPT, Claude and Gemini showed small increases in \nproportion of female assignments, whereas DeepSeek exhibited a modest attenuation of \nfemale bias. The interaction between model and temperature was statistically significant \n(χ²₍₁₁₎ = 364.8, p < 0.001), however temperature alone was not associated with sex assignment \n(χ²₍₂₎ = 0.10, p = 0.95).  \n \nAt temperature 0.5, specialty-level analysis revealed marked variation (Figure 3). Across all \nmodels, psychiatry, rheumatology and haematology were labelled female in 100% of cases. \nEndocrinology was also predominately labelled female, ranging from 60% (12 of 20, \nDeepseek) to 100% (20 of 20, Claude), with ChatGPT at 95% (19 of 20) and Gemini at 65% (13 \nof 20). In contrast, cardiology and urology were labelled male in 100% of cases across all \nmodels. Pulmonology vignettes were predominantly assigned male sex by most models, with \nmale classifications in 95% of cases for ChatGPT (19 of 20) and in 100% of cases for both \nDeepSeek and Gemini (20 of 20), whereas Claude demonstrated a more balanced distribution \n(55% male, 11 of 20). Overall, these specialty-specific assignment patterns were largely \nconsistent across models and are directionally concordant with documented sex-related \nasymmetries in clinical research representation and healthcare delivery.  \n \n"}, {"page": 11, "text": "Abstention from Sex Assignment  \nWhen models were permitted to abstain from assigning sex, substantial shifts occurred (Table \n2, Figure 4). ChatGPT abstained in all cases across temperatures. At temperature 0.5, Claude \nand Gemini abstained in 84% and 80% of cases respectively, whereas DeepSeek did so in 58%. \nAbstention rates were stable across temperatures. Model choice and its interaction with \ntemperature were significant (p<0.001). Chi-square testing demonstrated a strong \nassociation between model and sex output (χ²₍₆₎ = 889.1, p < 0.001) as well as a significant \ninteraction between model and temperature (χ²₍₂₂₎ = 891.0, p < 0.001), while temperature \nalone showed no main effect (χ²₍₄₎ = 0.47, p = 0.98). Permitting abstention substantially \nreduced explicit sex assignment; however, sex-contingent differences in downstream \ndiagnostic outputs persisted. \nDifferential Diagnosis Reasoning  \nWhen generating differential diagnoses for vignette pairs differing only by patient sex, all \nmodels produced divergent outputs (Table 3). At temperature 0.5, 66% of Claude outputs \ndiffered in both content and order (0.66, 95% CI 0.521-0.776), compared with 58% for \nDeepSeek (0.58, 0.442-0.706) and Gemini (0.58, 0.442-0.706), and 78% for ChatGPT (0.78, \n0.648-0.872). ChatGPT also showed the lowest proportion of identical lists (0.10, 0.043-\n0.214), whereas DeepSeek had the highest proportion of identical lists (0.28, 0.175-0.417). \nGemini most often produced reordered but otherwise identical (0.24, 0.143-0.374). \nSimilarity analysis confirmed these patterns (Table 4). At temperature 0.2, overlap between \nmale- and female- labelled vignettes was highest for Claude (Jaccard 0.78, 95% CI 0.72-0.86; \nitem-level agreement 0.66) and DeepSeek (Jaccard 0.75, 0.68-0.81; item-level agreement \n0.67), with comparable values for ChatGPT (Jaccard 0.76, 0.70-0.82) and Gemini (Jaccard 0.73, \n0.67-0.79). As model temperature increased, the proportion of shared diagnoses between \nmale and female differential lists declined across all models, most markedly for ChatGPT, in \nwhich the Jaccard index fell from 0.76 at temperature 0.2 to 0.50 at temperature 1.0, \naccompanied by a reduction in item-level agreement from 0.61 to 0.39, reflecting fewer \ndiagnoses appearing in both lists and fewer diagnoses occupying the same rank positions. At \ntemperature 1.0, Jaccard similarity remained higher for DeepSeek (0.70, 0.64-0.76), Gemini \n(0.69, 0.63-0.76), and Claude (0.62, 0.55-0.69) than for ChatGPT. Kendall’s tau-b values \n"}, {"page": 12, "text": "indicted strong preservation of diagnostic ordering when overlap occurred at lower \ntemperatures (tau 0.85-0.91 at temperature 0.2), with reduced consistency at higher \ntemperatures, particular for ChatGPT (tau 0.74 at temperature 1.0). \nModel and temperature both significantly affected all three-similarity metrics (p<0.01 for \nmain effects), confirming that sex-contingent diagnostic differences were model-specific and \namplified by sampling variability. However, the number of unique diagnoses did not differ \nsignificantly between male and female variants for any model at temperature 0.5 (Claude \np=0.91, DeepSeek p=0.42, Gemini p=0.12, ChatGPT p=0.79; Wilcoxon signed tank tests) (Table \n5). At temperature 0.5, Gemini showed the greatest cross-sex similarity (Jaccard 0.78), \nfollowed by DeepSeek (0.74), Claude (0.73), and ChatGPT (0.66).  \nModel choice and temperature both influenced similarity metrics. For Jaccard similarity, there \nwere significant main effects of model (F(3,588) = 6.06, p < 0.001) and temperature (F(2,588) \n= 17.90, p < 0.001), as well as a significant model × temperature interaction (F(6,588) = 3.10, \np = 0.005), indicating that the degree of overlap between male and female diagnostic lists was \njointly influenced by model choice and sampling temperature. For item-level agreement (ILA) \nthere were significant effects of model (F(3,588) = 5.88, p = 0.0005) and temperature \n(F(2,588) = 16.13, p < 0.001), with no interaction, consistent with a temperature driven \nreduction in agreement across models. For cumulative match characteristic (CMC) scores, \nthere were significant effects of model (F(3,588) = 4.08, p=0.007) and temperature (F(2,588) \n= 13.16, p < 0.0001), again without interaction. \nAcross analyses, model architecture emerged as the primary determinant of sex bias, with \ntemperature exerting comparatively limited influence. Specialty context further amplified \nthese effects, and although permitting abstention reduced explicit sex labelling, downstream \ndiagnostic reasoning remained sex-contingent. Collectively, these findings indicate that \nbiases well documented in women’s healthcare are already embedded within general-\npurpose AI systems and persist despite commonly applied configuration safeguards.  \nDiscussion  \nThis study extends conceptual discussions of sex bias in AI by providing direct evidence of its \nmanifestation within widely accessible LLMs applied to healthcare. Across three clinically \n"}, {"page": 13, "text": "oriented experiments, all models (ChatGPT, Claude, Gemini and DeepSeek) demonstrated \nsystematic sex-assignment skew and sex-contingent variation in diagnostic reasoning, \nconfirming that bias is not merely theoretical but detectable in model outputs.  \n \nModel choice was the dominant determinant of bias direction, while temperature exerted \nonly modest influence. Allowing abstention substantially changed surface behaviour, but \nspecialty context produced the most pronounced skews. Although three models showed an \noverall female bias, specialty-level patterns often reflected known clinical stereotypes with \ncardiology and psychiatry frequently labelled as female, and urology consistently labelled as \nmale (Figure 2). These findings suggest LLMs replicate the sex associations embedded in \nmedical discourse and practice rather than reflecting neutral diagnostic reasoning.  \n \nBias direction was broadly consistent across models, with ChatGPT, Claude, and DeepSeek \nassigning female sex more frequently, whereas Gemini exhibited a consistent male skew \n(Figure 3). This divergence indicates that bias direction is not uniform across systems and that \ndifferent commercial LLMs encode distinct sex priors shaped by variations in training data, \nalignment strategies, and optimisation objectives, reinforcing that observed biases are \nmodel-specific rather than intrinsic to the clinical tasks themselves. Aggregate measures \nmasked pronounced specialty-specific effects, suggesting that LLMs draw on linguistic cues \nthat echo historical sex-biased patterns of disease. These findings align with prior evidence \nthat LLMs complete associative stereotypes, such as over-assigning female pronouns to \nhealthcare roles and mapping stereotypically “feminine” descriptors to female patients in \ngenerated clinical narratives, with profession labels and trait cues systematically shifting sex \nassignment. 17,48  \n \nPermitting LLM abstention reduced explicit sex labelling, most notably for ChatGPT, which \nabstained in all cases. While this eliminated overt sex bias, it also rendered the model non-\ninformative, suggesting that excessive caution may reflect underlying uncertainty or risk-\naverse alignment behaviour.  Downstream diagnostic lists continued to differ when patient \nsex was varied, indicating that implicit sex-based associations persisted within the models’ \nreasoning processes. Abstention therefore serves as a useful guardrail to overt classification \nbut cannot neutralise deeper representational bias. This finding also demonstrates the \n"}, {"page": 14, "text": "sensitivity of LLMs to prompt design, as a seemingly minor structural change in allowing or \ndisallowing abstention altered clinical outputs and underscores the importance of prompt \nstandardisation and transparency in medical AI applications.  \n \nTemperature primarily affected variability rather than direction, with higher temperatures \nincreasing dispersion and reducing similarity between sex-stratified diagnostic lists, \nconsistent with more exploratory sampling. However, underlying sex skews persisted across \nsettings. This pattern suggests that the observed sex bias is intrinsic to the models’ learned \nrepresentations rather than an emergent artefact of sampling or prompt configuration. The \ndominance of model architecture over temperature implies that bias is encoded at the level \nof pre-training and alignment, reflecting how clinical concepts, symptoms, and specialties are \ninternally structured during training rather than being introduced by stochastic generation. \nThe amplification of bias within specific specialties suggests these representations are \norganised around entrenched, domain-specific associations, consistent with stereotype \ncompletion rather than context-sensitive clinical reasoning. The persistence of sex-contingent \ndiagnostic differences even when explicit sex labelling is suppressed further suggests that bias \noperates implicitly within internal reasoning pathways, influencing downstream outputs \ndespite surface-level safeguards. For clinical applications, these findings support \nconservative, well-documented temperature settings that prioritise reproducibility and \nstability, complemented by specialty-level auditing to identify and mitigate bias hotspots. \n \nThese results do not support the use of general-purpose language models to guide diagnosis \nor treatment. Instead, the immediate priority is clinical and public education on when such \nsystems can assist and when they pose safety risks. Health services and consumer platforms \nshould make model limitations explicit and route urgent or high-risk queries to human care. \nTransparent prompts, uncertainty statements, and links to evidence-based resources can \nreduce the likelihood that outputs are misinterpreted as clinical advice.  \n \nFrom a development perspective, the findings highlight that configuration settings yield only \nincremental improvements; structural solutions will require changes to data and design. \nDevelopers should suppress unsolicited demographic inference, curate training corpora that \nalign with current clinical evidence and refine reinforcement learning objectives to penalise \n"}, {"page": 15, "text": "stereotype completion in the absence of relevant clinical cues. Models intended for \nhealthcare applications should expose uncertainty, be trained to abstain rather than guess, \nand undergo specialty-level fairness auditing with transparent disclosure of defaults, versions, \nand abstention behaviour.49 Such measures can guide safer integration of AI into clinical \nworkflows and inform the design of purpose-built models that meet the evidentiary standards \nrequired for patient care.  \n \nA theoretically plausible explanation for the observed specialty-level sex skew is that large \nlanguage models internalise and reproduce real-world disease prevalence patterns encoded \nin their training data, such that higher population rates of certain conditions in women could \nbias model outputs toward female assignment.50 However, although psychiatric disorders \nsuch as major depressive disorder and generalised anxiety disorder are indeed more \nprevalent in women than in men, with female-to-male ratios typically around 1.5–2:1 in large \nepidemiological studies, these differences represent probabilistic population trends rather \nthan deterministic rules and cannot account for the near-complete, invariant female \nassignment observed across models, prompts, and repeated samplings. From a medical and \nepidemiological perspective, true disease distributions show substantial overlap between \nsexes, heterogeneity across age, culture, and presentation, and persistent male \nrepresentation even in female-predominant conditions, such that any system reflecting \ngenuine base rates would be expected to exhibit residual uncertainty and variability at the \nindividual case level.51 From a computer science perspective, general-purpose LLMs do not \nencode explicit epidemiological priors or perform Bayesian inference over population \nstatistics; instead, they learn statistical regularities in language, including highly stereotyped \nnarrative associations that disproportionately frame mood and anxiety disorders as female in \ntextbooks, teaching cases, and clinical discourse. Empirical studies demonstrate that LLMs \nsystematically amplify such representational biases, producing outputs that are more \nextreme and less uncertain than underlying real-world distributions, a phenomenon \nattributed to stereotype completion and distributional collapse rather than prevalence \nlearning.52 The magnitude, stability, and cross-model consistency of the psychiatry-specific \nskew suggest that the observed behaviour reflects entrenched linguistic and cultural \nstereotypes embedded in training corpora, rather than faithful reproduction of medical \npopulation base rates. \n"}, {"page": 16, "text": " \nThis study had several methodological strengths. We conducted a controlled, multi-model \nevaluation of four contemporary and widely used general-purpose LLMs using identical \nprompts, vignettes, and experimental conditions, isolating the effects of model architecture \nwhile manipulating only two practical and commonly adjustable inference-time settings, \ntemperature and abstention. The use of clinician-authored, non-public vignettes spanning 44 \nspecialties reduced the risk of training data contamination and enabled systematic \nassessment of specialty-specific bias patterns across a broad range of clinical domains. By \nexamining both upstream behaviour, including unsolicited sex assignment and abstention, \nand downstream clinical reasoning through ranked differential diagnosis generation, we \ncaptured multiple stages at which bias may manifest. The application of complementary \nsimilarity metrics that quantified both diagnostic overlap and rank ordering allowed a more \ngranular characterisation of sex-contingent differences than single-metric approaches, while \nrepeated sampling across temperatures improved robustness and reproducibility. Together, \nthese design choices support generalisable comparisons across model types and provide a \ncomprehensive and conservative assessment of sex bias in clinically oriented LLM outputs. \n \nThis study also has several limitations. The experimental design operationalised sex as a \nbinary attribute, reflecting the binary male or female labels inferred or assigned by the \nmodels, and did not assess sex identity, sex expression, intersex variation, or discordance \nbetween sex assigned at birth and lived sex. This restriction was intentional and enabled a \ncontrolled evaluation of binary sex bias as a foundational benchmark for subsequent, more \ngranular intersectional analyses. The use of clinical vignettes necessarily simplifies real-world \nclinical documentation and cannot capture the full complexity of patient presentations; \nhowever, the standardised vignette structure ensured consistency across models and \nreduced confounding from case variability, allowing clearer attribution of observed effects to \nmodel behaviour. Outputs were not linked to clinician decisions or patient outcomes, \nmeaning that clinical impact is inferred rather than directly measured, although analysis of \nmodel outputs remains a widely accepted approach for early-stage evaluation of algorithmic \nbias. Proprietary training data and alignment procedures for commercial models remain \nundisclosed and may change over time, limiting strict reproducibility; nevertheless, the \nmodels evaluated were among the most widely used and publicly accessible at the time of \n"}, {"page": 17, "text": "testing, making the findings highly relevant to current clinical adoption. The study was further \nlimited to English-language prompts, a fixed set of specialties, and a narrow range of \ntemperatures, choices that were necessary to preserve experimental control and reflect \ntypical deployment parameters. Intersectional attributes such as age, ethnicity, and \ncomorbidity were not examined, constraining the scope of bias detection; however, focusing \non sex provided a tractable starting point to establish measurable patterns and \nmethodological validity. These limitations define the scope and the controlled design, cross-\nmodel comparisons, and comprehensive metrics support a reliable and conservative \nassessment of sex bias in healthcare-oriented LLM outputs.  \n \nFuture Directions  \n \nAs one of the first empirical investigations of sex bias in healthcare-targeted AI, this study \nprovides a baseline for future work. Subsequent analyses should extend beyond binary sex to \ninclude intersectional factors such as age, ethnicity, and comorbidity, and evaluate non-\nEnglish outputs.  Mechanistic studies are needed to disentangle the influence of pre-training \ndata, human feedback alignment, and prompt design on bias manifestation.  \n \nFuture research should also examine open-sourced and domain-specific LLMs trained on \nmedical corpora, such as Med-PaLM and MedPhi, or models further tuned for clinical \napplications. Comparing general-purpose and medically specialised models may reveal \nwhether domain adaptation mitigates or reinforces sexed patterns. Independent external \naudits, ideally using preregistered protocols, will strengthen reproducibility and \naccountability over time.  \n \nConclusion \n \nAcross contemporary models, we observed stable model-specific bias direction, minimal \ntemperature effects, and strong specialty-dependent skew, with diagnostic suggestions often \nchanging when only the patient’s sex varied. Enabling abstention reduced explicit sex labelling \nbut did not remove downstream diagnostic differences. These findings argue against using \n"}, {"page": 18, "text": "general-purpose models to guide treatment decisions and instead support public and clinician \neducation on safe use, conservative model architecture, and prompt design that is targeted \nto clinical scenarios. Collectively, the results provide practical guidance for safer configuration \nand establish a research agenda for developing models fit for clinical purpose. \n \nData Availability \nThe clinical vignettes generated and analysed in this study are publicly available at the Sex-\nBias-in-Clinical-Reasoning-by-Large-Language-Models (GitHub) repository \n(https://gitlab.com/oxdhl-public/evaluating-the-presence-of-sex-bias-in-clinical-\nreasoning-by-large-language-models).  \n \n \nCode Availability \nThe custom code used for data generation, model prompting, and analysis in this study is \npublicly available at the Sex-Bias-in-Clinical-Reasoning-by-Large-Language-Models (GitHub) \nrepository (https://gitlab.com/oxdhl-public/evaluating-the-presence-of-sex-bias-in-\nclinical-reasoning-by-large-language-models). \n  \n \n \n \n \n \n"}, {"page": 19, "text": "References \n \n1 \nClusmann, J. et al. The future landscape of large language models in medicine. \nCommunications Medicine 3, 141 (2023). https://doi.org/10.1038/s43856-023-00370-1 \n2 \nNtoutsi, E. et al. Bias in data-driven artificial intelligence systems—An introductory \nsurvey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 10 \n(2020). https://doi.org/10.1002/widm.1356 \n3 \nHannan, E. L. et al. Sex differences in the treatment and outcomes of patients \nhospitalized \nwith \nST-elevation \nmyocardial \ninfarction. \nCatheterization \nand \ncardiovascular interventions : official journal of the Society for Cardiac Angiography \n& \nInterventions \n95, \n196-204 \n(2020). \nhttps://doi.org/https://dx.doi.org/10.1002/ccd.28286 \n4 \nClerc Liaudat, C. et al. Sex/gender bias in the management of chest pain in ambulatory \ncare. Women's health (London, England) 14, 1745506518805641 (2018). \nhttps://doi.org/https://dx.doi.org/10.1177/1745506518805641 \n5 \nDaly, C. et al. Gender Differences in the Management and Clinical Outcome of Stable \nAngina. \nCirculation \n113, \n490-498 \n(2006). \nhttps://doi.org/doi:10.1161/CIRCULATIONAHA.105.561647 \n6 \nKatz, J. D., Seaman, R. & Diamond, S. Exposing Gender Bias in Medical Taxonomy: \nToward Embracing a Gender Difference Without Disenfranchising Women. Women's \nHealth Issues 18, 151-154 (2008). https://doi.org/10.1016/j.whi.2008.03.002 \n7 \nSamulowitz, A., Gremyr, I., Eriksson, E. & Hensing, G. \"Brave Men\" and \"Emotional \nWomen\": A Theory-Guided Literature Review on Gender Bias in Health Care and \nGendered Norms towards Patients with Chronic Pain. Pain Research and Management \n2018, 6358624 (2018). https://doi.org/10.1155/2018/6358624 \n8 \nWerner, A., Isaksen, L. W. & Malterud, K. ‘I am not the kind of woman who complains \nof everything’: Illness stories on self and shame in women with chronic pain. Social \nScience \n& \nMedicine \n59, \n1035-1045 \n(2004). \nhttps://doi.org/https://doi.org/10.1016/j.socscimed.2003.12.001 \n9 \nHoffmann, D. E. & Tarzian, A. J. The Girl Who Cried Pain: A Bias against Women in \nthe Treatment of Pain. Journal of Law, Medicine & Ethics 29, 13-27 (2001). \nhttps://doi.org/10.1111/j.1748-720X.2001.tb00037.x \n10 \nFillingim, R. B., King, C. D., Ribeiro-Dasilva, M. C., Rahim-Williams, B. & Riley, J. \nL., III. Sex, Gender, and Pain: A Review of Recent Clinical and Experimental Findings. \nThe Journal of Pain 10, 447-485 (2009). https://doi.org/10.1016/j.jpain.2008.12.001 \n11 \nLevine, F. M. & Lee De Simone, L. The effects of experimenter gender on pain report \nin male and female subjects. Pain 44, 69-72 (1991). https://doi.org/10.1016/0304-\n3959(91)90149-r \n12 \nBernardes, S. F. & Lima, M. L. Being less of a man or less of a woman: Perceptions of \nchronic pain patients’ gender identities. European Journal of Pain 14, 194-199 (2010). \nhttps://doi.org/https://doi.org/10.1016/j.ejpain.2009.04.009 \n13 \nNori, H., King, N., McKinney, S., Carignan, D. & Horvitz, E. Capabilities of GPT-4 \non Medical Challenge Problems.  (2023). \n14 \nBachmann, M. et al. Exploring the capabilities of ChatGPT in women’s health: \nobstetrics \nand \ngynaecology. \nnpj \nWomen's \nHealth \n2, \n26 \n(2024). \nhttps://doi.org/10.1038/s44294-024-00028-w \n"}, {"page": 20, "text": "15 \nYe, Z. et al. An assessment of ChatGPT’s responses to frequently asked questions about \ncervical \nand \nbreast \ncancer. \nBMC \nWomen's \nHealth \n24, \n482 \n(2024). \nhttps://doi.org/10.1186/s12905-024-03320-8 \n16 \nMenz, B. D. et al. Gender Representation of Health Care Professionals in Large \nLanguage Model–Generated Stories. The Journal of the American Medical Association \nNetwork \nOpen \n7, \ne2434997-e2434997 \n(2024). \nhttps://doi.org/10.1001/jamanetworkopen.2024.34997 \n17 \nAgrawal, A. Fairness in AI-Driven Oncology: Investigating Racial and Gender Biases \nin \nLarge \nLanguage \nModels. \nCureus \n16, \ne69541 \n(2024). \nhttps://doi.org/10.7759/cureus.69541 \n18 \nRampidis, G. et al. Clinical Performance Evaluation of an Artificial Intelligence–Based \nTool for Predicting the Presence of Obstructive Coronary Artery Disease: Protocol for \na Cohort Observational Study. JMIR research protocols. 14, e67697 (2025). \nhttps://doi.org/10.2196/67697 \n19 \nMeier, H. et al. Comparing Diagnostic Accuracy of ChatGPT to Clinical Diagnosis in \nGeneral Surgery Consults: A Quantitative Analysis of Disease Diagnosis. Military \nmedicine. 190, e1858-e1862 (2025). https://doi.org/10.1093/milmed/usaf168 \n20 \nTernov, N. K. et al. Generalizability and usefulness of artificial intelligence for skin \ncancer diagnostics: An algorithm validation study. JEADV Clinical Practice 1, 344-\n354 (2022). https://doi.org/https://doi.org/10.1002/jvc2.59 \n21 \nBeets, B., Newman, T. P., Howell, E. L., Bao, L. & Yang, S. Surveying Public \nPerceptions of Artificial Intelligence in Health Care in the United States: Systematic \nReview. J Med Internet Res 25, e40337 (2023). https://doi.org/10.2196/40337 \n22 \nThaldar, D. & Bottomley, D. Public trust of AI in healthcare in South Africa: results of \na survey. BMC Medical Ethics 26, 113 (2025). https://doi.org/10.1186/s12910-025-\n01272-8 \n23 \nScantamburlo, T. et al. Artificial Intelligence across Europe: A Study on Awareness, \nAttitude and Trust.  (2023). \n24 \nKhullar, D. et al. Perspectives of Patients About Artificial Intelligence in Health Care. \nJAMA \nNetw \nOpen \n5, \ne2210309 \n(2022). \nhttps://doi.org/10.1001/jamanetworkopen.2022.10309 \n25 \nDousa, R.      (2020). \n26 \nParry, M. W. et al. Patient Perspectives on Artificial Intelligence in Healthcare Decision \nMaking: A Multi-Center Comparative Study. Indian Journal of Orthopaedics 57, 653-\n665 (2023). https://doi.org/10.1007/s43465-023-00845-2 \n27 \nTyson, A., Pasquini, G., Spencer, A. & Funk, C. Sixty percent of Americans would be \nuncomfortable with provider relying on AI in their own health care. (Pew Research \nCenter, 2023). \n28 \nOkolo, C., Aruleba, K. & Obaido, G.      35-64 (2023). \n29 \nNadeem, A., Marjanovic, O. & Abedin, B. Gender bias in AI-based decision-making \nsystems: a systematic literature review. Australasian Journal of Information Systems \n26 (2022). https://doi.org/10.3127/ajis.v26i0.3835 \n30 \nBearman, M. & Ajjawi, R. Artificial intelligence and gender equity: An integrated \napproach for health professional education. Medical education. 59, 1049-1057 (2025). \nhttps://doi.org/10.1111/medu.15657 \n31 \nPeabody, J. W., Luck, J., Glassman, P., Dresselhaus, T. R. & Lee, M. Comparison of \nvignettes, standardized patients, and chart abstraction: a prospective validation study of \n3 methods for measuring quality. Jama 283, 1715-1722 (2000).  \n"}, {"page": 21, "text": "32 \nPeabody, J. W. et al. Measuring the quality of physician practice by using clinical \nvignettes: a prospective validation study. Annals of internal medicine 141, 771-780 \n(2004).  \n33 \nSheringham, J., Kuhn, I. & Burt, J. The use of experimental vignette studies to identify \ndrivers of variations in the delivery of health care: a scoping review. BMC medical \nresearch methodology 21, 81 (2021).  \n34 \nPrabhakaran, S. et al. 2026 Guideline for the Early Management of Patients With Acute \nIschemic Stroke: A Guideline From the American Heart Association/American Stroke \nAssociation. Stroke 0  https://doi.org/10.1161/STR.0000000000000513 \n35 \n(GOLD), G. I. f. C. O. L. D. Global Strategy for the Diagnosis, Management, and \nPrevention of Chronic Obstructive Pulmonary Disease. (Global Initiative for Chronic \nObstructive Lung Disease, Fontana, WI, USA, 2025). \n36 \nAssociation, A. P. Diagnostic and Statistical Manual of Mental Disorders. 5th ed, text \nrevision edn,  (American Psychiatric Association, 2022). \n37 \n(GINA), G. I. f. A. Global Strategy for Asthma Management and Prevention. (2025). \n38 \nSurgeons, A. C. o. Advanced Trauma Life Support Student Course Manual. 10th ed \nedn,  (American College of Surgeons, 2018). \n39 \nPowers, W. J. et al. Guidelines for the early management of patients with acute \nischemic stroke: 2019 update to the 2018 guidelines for the early management of acute \nischemic stroke: a guideline for healthcare professionals from the American Heart \nAssociation/American Stroke Association. Stroke 50, e344-e418 (2019).  \n40 \nFabbri, L. M., Hurd, S. & Committee, G. S.  Vol. 22   1-1 (European Respiratory \nSociety, 2003). \n41 \nLockhart, M. E. Acr appropriateness criteria® introduction to the jacr appropriateness \ncriteria june 2024 supplement. Journal of the American College of Radiology 21, S1-\nS2 (2024).  \n42 \nEdition, F. Diagnostic and statistical manual of mental disorders. Am Psychiatric Assoc \n21, 591-643 (2013).  \n43 \nBateman, E. D. et al. Global strategy for asthma management and prevention: GINA \nexecutive summary. European Respiratory Journal 31, 143-178 (2007).  \n44 \nTrauma, A. C. O. S. C. O. Advanced trauma life support ATLS: student course manual.  \n(American College of Surgeons, 2012). \n45 \nWorkowski, K. A. Sexually transmitted infections treatment guidelines, 2021. MMWR. \nRecommendations and Reports 70 (2021).  \n46 \nFraenkel, L. et al. 2021 American College of Rheumatology Guideline for the \nTreatment of Rheumatoid Arthritis. Arthritis Rheumatol 73, 1108-1123 (2021). \nhttps://doi.org/10.1002/art.41752 \n47 \nGallifant, J. et al. The TRIPOD-LLM reporting guideline for studies using large \nlanguage models. Nature Medicine 31, 60-69 (2025). https://doi.org/10.1038/s41591-\n024-03425-5 \n48 \nMenz, B. D. et al. Gender Representation of Health Care Professionals in Large \nLanguage Model–Generated Stories. JAMA Network Open 7, e2434997-e2434997 \n(2024). https://doi.org/10.1001/jamanetworkopen.2024.34997 \n49 \nPenny-Dimri, J. C. et al. Measuring large language model uncertainty in women's \nhealth using semantic entropy and perplexity: a comparative study. The Lancet \nObstetrics, \nGynaecology, \n& \nWomen’s \nHealth \n1, \ne47-e56 \n(2025). \nhttps://doi.org/10.1016/j.lanogw.2025.100005 \n50 \nGallegos, I. O. et al. Bias and Fairness in Large Language Models: A Survey. \nComputational \nLinguistics \n50, \n1097-1179 \n(2024). \nhttps://doi.org/10.1162/coli_a_00524 \n"}, {"page": 22, "text": "51 \nKuehner, C. Why is depression more common among women than among men? Lancet \nPsychiatry 4, 146-158 (2017). https://doi.org/10.1016/s2215-0366(16)30263-2 \n52 \nBender, E., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the Dangers of \nStochastic Parrots: Can Language Models Be Too Big? ,  (2021). \n \n"}, {"page": 23, "text": "Figures \n \nFigure 1 Experimental design for probing sex inference and downstream diagnostic \nreasoning by large language models. (A) Experiment 1: Neutral clinical vignettes with all sex \nand sex cues removed were presented to the model, which was required to assign a single-\ntoken sex label (male or female), testing default sex assumptions. (B) Experiment 2: The same \nneutral vignettes were used, but the response space was expanded to include an abstention \noption, assessing whether explicit abstention reduces unsolicited sex assignment. (C) \nExperiment 3: Male and female versions of each vignette were presented separately and \nmodels generated ranked top-five differential diagnoses, enabling comparison of \ndownstream diagnostic reasoning when only patient sex differed. All experiments were run \nacross models, temperatures, and repeated samplings under fixed prompt and formatting \nconstraints. \n \n \n \n \n \n \n \n"}, {"page": 24, "text": "Figure 2. Three-tier classification of agreement between male and female differential \ndiagnosis lists. Schematic examples illustrate how paired top-five differential diagnosis \nlists were categorised. (A) Perfect match: the same five diagnoses appear in the same \norder in both lists. (B) Shuffled match: the same five diagnoses appear in both lists but \nwith differences in rank order. (C) Mismatch: at least one diagnosis appears in only one \nlist. Arrows indicate corresponding diagnoses across lists, highlighting rank concordance \nor divergence. \n \n"}, {"page": 25, "text": " \n \nFigure 3. Binary sex assignment by specialty at temperature 0.5 (experiment one). Horizontal bars show, for each specialty, the proportion of cases labelled \nfemale (purple) and male (orange) by each model, aggregated over ten repeats per vignette. Paediatric specialties are excluded. Dermatology variants are \ngrouped. Infectious diseases and pulmonology are shown as separate categories. \n \n"}, {"page": 26, "text": " \nFigure 4. Sex assignment with abstention from neutral vignettes (experiment two). After all \nsex cues were removed, models were instructed to answer male, female, or abstain. \nStacked bars show the proportion of outputs by category (male, orange; female, purple; \nabstain, grey) aggregated over 50 vignettes with ten repeats per vignette at temperatures \n0.2, 0.5, and 1.0. \n \n"}, {"page": 27, "text": "Tables \nModel \nTemperature \nMale \nFemale \nP Value \nClaude \n0.2 \n0.42 \n(0.38-0.46) \n0.58 \n(0.54-0.62) \n<0.0001 \n0.5 \n0.41 \n(0.37-0.45) \n0.59 \n(0.55-0.63) \n<0.0001 \n1.0 \n0.41 \n(0.37-0.45) \n0.59 \n(0.55-0.63) \n<0.0001 \nDeepSeek Chat \n0.2 \n0.384 \n(0.34-0.43) \n0.62 \n(0.57-0.66) \n<0.0001 \n0.5 \n0.39 \n(0.35-0.43) \n0.61 \n(0.57-0.65) \n<0.0001 \n1.0 \n0.40 \n(0.36-0.45) \n0.60 \n(0.55-0.64) \n<0.0001 \nGemini \n0.2 \n0.64 \n(0.59-0.68) \n0.36 \n(0.32-0.41) \n<0.0001 \n0.5 \n0.63 \n(0.58-0.67) \n0.37 \n(0.33-0.42) \n<0.0001 \n1.0 \n0.62 \n(0.57-0.66) \n0.38 \n(0.34-0.43) \n<0.0001 \nChatGPT \n0.2 \n0.30 \n(0.25-0.33) \n0.70 \n(0.66-0.75) \n<0.0001 \n0.5 \n0.30 \n(0.26-0.34) \n0.70 \n(0.66-0.74) \n<0.0001 \n1.0 \n0.28 \n(0.27-0.33) \n0.72 \n(0.67-0.75) \n<0.0001 \nTable 1. Binary sex assignment (male or female) from neutral vignettes across models and \ntemperatures. Neutral vignettes with all sex cues removed were presented, and models \nwere required to label each case as male or female. Each model (OpenAI gpt-4o-mini-2024-\n07-18, Anthropic claude-3-7-sonnet-20250219, Google gemini-2.0-flash, DeepSeek \ndeepseek-chat) was sampled ten times per vignette at temperatures 0.2, 0.5, and 1.0. \nValues are proportions with 95% confidence intervals aggregated over 50 vignettes; p \nvalues are two-sided exact binomial tests against an expected proportion of 0.50. \n"}, {"page": 28, "text": " \nModel \nTemperature \nAbstain \nFemale \nMale \nClaude \n \n0.2 \n0.84 \n0.12 \n0.04 \n0.5 \n0.84 \n0.12 \n0.04 \n1.0 \n0.83 \n0.12 \n0.04 \nDeepSeek Chat \n \n0.2 \n0.58 \n0.22 \n0.20 \n0.5 \n0.58 \n0.22 \n0.20 \n1.0 \n0.57 \n0.22 \n0.21 \nGemini \n \n0.2 \n0.81 \n0.10 \n0.09 \n0.5 \n0.80 \n0.10 \n0.10 \n1.0 \n0.82 \n0.09 \n0.08 \nChatGPT \n \n0.2 \n1.00 \n0.00 \n0.00 \n0.5 \n1.00 \n0.00 \n0.00 \n1.0 \n1.00 \n0.00 \n0.00 \nTable 2. Sex assignment with abstention permitted from neutral vignettes. Neutral \nvignettes with all sex cues removed were presented, and models were instructed to \nrespond with one of {male, female, abstain}. Each model was sampled ten times per \nvignette at temperatures 0.2, 0.5, and 1.0. Values are proportions aggregated over 50 \nvignettes; columns report the share of outputs that abstained, were labelled female, or \nwere labelled male. \n"}, {"page": 29, "text": "Model \nTemperature \nIdentical \nContent and \nOrder \nIdentical \nContent \nDifferent Order \nDifferent Content \nDifferent Order \nClaude \n0.2 \n0.3 \n(0.191-0.438) \n0.16 \n(0.083-0.285) \n0.54 \n(0.404-0.670) \n0.5 \n0.18 \n(0.098-0.308) \n0.16 \n(0.083-0.285) \n0.66 \n(0.521-0.776) \n1.0 \n0.06 \n(0.021-0.162) \n0.14 \n(0.070-0.262) \n0.8 \n(0.670-0.888) \nDeepSeek Chat \n0.2 \n0.3 \n(0.191-0.438) \n0.08 \n(0.032-0.189) \n0.62 \n(0.482-0.741) \n0.5 \n0.28 \n(0.175-0.417) \n0.14 \n(0.070-0.262) \n0.58 \n(0.442-0.706) \n1.0 \n0.12 \n(0.056-0.238) \n0.14 \n(0.070-0.262) \n0.74 \n(0.604-0.841) \nGemini \n0.2 \n0.22 \n(0.128-0.352) \n0.12 \n(0.056-0.238) \n0.66 \n(0.522-0.776) \n0.5 \n0.18 \n(0.098-0.308) \n0.24 \n(0.143-0.374) \n0.58 \n(0.442-0.706) \n1.0 \n0.08 \n(0.032-0.188) \n0.2 \n(0.112-0.330) \n0.72 \n(0.583-0.825) \nChatGPT \n0.2 \n0.24 \n(0.143-0.374) \n0.14 \n(0.069-0.262) \n0.62 \n(0.482-0.741) \n0.5 \n0.1 \n(0.043-0.214) \n0.12 \n(0.056-0.238) \n0.78 \n(0.648-0.872) \n1.0 \n0.08 \n(0.032-0.188) \n0.02 \n(0.004-0.105) \n0.9 \n(0.786-0.957) \nTable 3. Similarity of differential diagnosis lists for male versus female versions of each \nvignette (experiment three). Proportions show the share of cases with identical content and \norder (same five diagnoses in the same ranks), identical content with different order (same \nfive diagnoses, ranks differ), or different content and order (at least one diagnosis differs) \nacross all three temperatures. Values represent the mean across all 50 vignettes with 95% CI. \n"}, {"page": 30, "text": "Model \nTemperature \nJaccard \nSimilarity \nItem-level \nAgreement \nCumulative Match \nCharacteristics \nKendall’s \nTau \nP Value \nClaude \n \n0.2 \n0.78 \n(0.72-0.86) \n0.66 \n(0.57-0.74) \n0.61 \n(0.51-0.70) \n0.85 \n(0.77-0.93) \n0.230 \n0.5 \n0.73 \n(0.67-0.79) \n0.60 \n(0.52-0.69) \n0.54 \n(0.46-0.63) \n0.83 \n(0.74-0.92) \n0.847 \n1.0 \n0.62 \n(0.55-0.69) \n0.46 \n(0.39-0.53) \n0.42 \n(0.35-0.50) \n0.84 \n(0.76-0.92) \n0.363 \nDeepSeek Chat \n0.2 \n0.75 \n(0.68-0.81) \n0.67 \n(0.59-0.75) \n0.62 \n(0.52-0.72) \n0.91 \n(0.84-0.97) \n0.594 \n0.5 \n0.74 \n(0.67-0.81) \n0.67 \n(0.60-0.75) \n0.58 \n(0.49-0.68) \n0.93 \n(0.87-0.97) \n0.421 \n1.0 \n0.70 \n(0.64-0.76) \n0.59 \n(0.52-0.67) \n0.54 \n(0.45-0.62) \n0.86 \n(0.78-0.93) \n0.882 \nGemini \n0.2 \n0.73 \n(0.67-0.79) \n0.60 \n(0.52-0.68) \n0.53 \n(0.44-0.62) \n0.88 \n(0.80-0.95) \n0.730 \n0.5 \n0.78 \n(0.72-0.84) \n0.61 \n(0.53-0.68) \n0.51 \n(0.42-0.60) \n0.87 \n(0.81-0.92) \n0.124 \n1.0 \n0.69 \n(0.63-0.76) \n0.49 \n(0.41-0.57) \n0.42 \n(0.33-0.50) \n0.83 \n(0.76-0.89) \n0.144 \nChatGPT \n0.2 \n0.76 \n(0.70-0.82) \n0.61 \n(0.52-0.70) \n0.58 \n(0.48-0.68) \n0.91 \n(0.87-0.96) \n0.325 \n0.5 \n0.66 \n(0.59-0.72) \n0.53 \n(0.44-0.61) \n0.48 \n(0.39-0.57) \n0.85 \n(0.78-0.93) \n0.788 \n1.0 \n0.50 \n(0.44-0.56) \n0.39 \n(0.32-0.46) \n0.33 \n(0.25-0.41) \n0.74 \n(0.62-0.87) \n0.968 \nTable 4. Agreement between male and female top-five differential-diagnosis lists across models and temperatures (experiment three). Scores are reported \nas means with 95% confidence intervals across the 50 vignettes. Jaccard quantifies item overlap (0–1); item-level agreement (ILA) counts diagnoses in exactly \nthe same rank; cumulative match characteristic (CMC) captures uninterrupted agreement from rank 1 downward; Kendall’s tau-b measures rank-order \nassociation among shared items. p values are two-sided Wilcoxon signed-rank tests comparing male versus female scores at each setting; ns = not significant; \nT = temperature. \n"}, {"page": 31, "text": "Model \nJaccard \nSimilarity \nItem-level \nAgreement \nCumulative Match \nCharacteristics \nKendall’s \nTau \ntWP \nuWP \nClaude \n0.73 \n(0.67-0.79) \n0.60 \n(0.52-0.69) \n0.54 \n(0.46-0.63) \n0.83 \n(0.74-0.92) \n<0.0001 \n \n0.908 \n \nDeepSeek Chat \n0.74 \n(0.67-0.81) \n0.67 \n(0.60-0.75) \n0.58 \n(0.49-0.68) \n0.93 \n(0.87-0.97) \n<0.0001 \n \n0.421 \n \nGemini \n0.78 \n(0.72-0.84) \n0.61 \n(0.53-0.68) \n0.51 \n(0.42-0.60) \n0.87 \n(0.81-0.92) \n<0.0001 \n \n0.124 \n \nChatGPT-4o \n0.66 \n(0.59-0.72) \n0.53 \n(0.44-0.61) \n0.48 \n(0.39-0.57) \n0.85 \n(0.78-0.93) \n<0.0001 \n \n0.788 \nTable 5. Agreement between male and female top-five differential-diagnosis lists at temperature 0.5 (experiment three). Models: OpenAI gpt-\n4o-mini-2024-07-18, Anthropic claude-3-7-sonnet-20250219, Google gemini-2.0-flash, DeepSeek deepseek-chat. Values are means (95% CI) \nacross 50 vignettes. Metrics: Jaccard similarity (item overlap), item-level agreement (exact rank matches), cumulative match characteristic \n(continuous agreement from rank 1), and Kendall’s tau-b (rank-order association among shared items). p values are two-sided Wilcoxon \nsigned-rank tests: tWP for Kendall’s tau-b; uWP for the number of unique diagnoses. \n"}, {"page": 32, "text": " \n \n"}]}