{"doc_id": "arxiv:2601.18334", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.18334.pdf", "meta": {"doc_id": "arxiv:2601.18334", "source": "arxiv", "arxiv_id": "2601.18334", "title": "Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare", "authors": ["Clément Christophe", "Wadood Mohammed Abdul", "Prateek Munjal", "Tathagata Raha", "Ronnie Rajan", "Praveenkumar Kanithi"], "published": "2026-01-26T10:21:34Z", "updated": "2026-01-26T10:21:34Z", "summary": "As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or \"confusability\". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized \"Thinking\" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.18334v1", "url_pdf": "https://arxiv.org/pdf/2601.18334.pdf", "meta_path": "data/raw/arxiv/meta/2601.18334.json", "sha256": "742de1058bc6aa9047825a704ce8594b1f2f3cd0e393a5060525fff9d982b349", "status": "ok", "fetched_at": "2026-02-18T02:20:32.157173+00:00"}, "pages": [{"page": 1, "text": "Overalignment in Frontier LLMs: An Empirical Study of Sycophantic\nBehaviour in Healthcare\nClément Christophe\nWadood Mohammed Abdul\nPrateek Munjal\nTathagata Raha\nRonnie Rajan\nPraveenkumar Kanithi\nM42, Abu Dhabi\nAbstract\nAs LLMs are increasingly integrated into clini-\ncal workflows, their tendency for sycophancy,\nprioritizing user agreement over factual accu-\nracy, poses significant risks to patient safety.\nWhile existing evaluations often rely on sub-\njective datasets, we introduce a robust frame-\nwork grounded in medical MCQA with ver-\nifiable ground truths.\nWe propose the Ad-\njusted Sycophancy Score (Sa), a novel metric\nthat isolates alignment bias by accounting for\nstochastic model instability, or “confusability.”\nThrough an extensive scaling analysis of the\nQwen-3 and Llama-3 families, we identify a\nclear scaling trajectory for resilience. Further-\nmore, we reveal a counter-intuitive vulnerabil-\nity in reasoning-optimized “Thinking” models:\nwhile they demonstrate high vanilla accuracy,\ntheir internal reasoning traces frequently ratio-\nnalize incorrect user suggestions under authori-\ntative pressure. Our results across frontier mod-\nels suggest that benchmark performance is not\na proxy for clinical reliability, and that sim-\nplified reasoning structures may offer superior\nrobustness against expert-driven sycophancy.\n1\nIntroduction\nLarge Language Models (LLMs) have demon-\nstrated remarkable capabilities across diverse do-\nmains using a mixture of different training tech-\nniques such as supervised finetuning (SFT), fol-\nlowed by Reinforcement learning (RL) techniques\nincluding Human/AI feedback, and verifiable re-\nwards. Recently, RL based methods are employed\nto align LLMs with human intention. Although it\nhas shown to improve adherence to user intentions\nby making the LLMs more helpful, they are also\nnoted to lead to a behavior known as sycophancy.\nUnder this behavior, the model tends to strongly\nalign its responses with the user’s stated views or\nmisconceptions, even when they contradict estab-\nlished facts (Casper et al., 2023).\nIn creative or open-ended tasks, this behav-\nior/trait may be perceived as a form of \"user-\nalignment\". However, we note that in high stakes\ndomains such as healthcare, sycophancy poses a se-\nrious safety risks and represent a major blocker\nin clinical adoption of LLMs. We hypothesize\nthat an ideal clinical LLM must consistently priori-\ntize medical knowledge/reasoning over user prefer-\nences, especially in clinical decision settings.\nIn this paper, we address the critical gap in clin-\nical AI safety by developing a robust sycophancy\nevaluation framework grounded in Medical MCQA\nbenchmarks (Jin et al., 2020; Wang et al., 2024). By\nusing exams with verifiable ground truths, we pro-\nvide an objective measure of a model’s resilience\nagainst explicit misinformation , presented in in-\nputs via nudges/perturbations.\nOur contributions are three-fold. First, we intro-\nduce the Adjusted Sycophancy Score Sa, a novel\nmetric that accounts for model “confusability.” By\nfiltering out stochastic instability (erratic flips), Sa\nprovides a more precise measure of true alignment\nbias than incidental prediction changes (e.g., tran-\nsition of model’s answer to non-preferred answer).\nSecondly, we conduct an extensive Scaling Anal-\nysis across multiple model families, identifying\ncritical parameter thresholds for clinical resilience\nand demonstrating that our proposed score remains\nrobust across tasks of varying granularity (MedQA\nand MMLU Pro). Finally, we reveal that reasoning\ntraces in “Thinking” models can act as a vulnera-\nbility; while they improve vanilla benchmark accu-\nracy, these traces can inadvertently facilitate syco-\nphancy by rationalizing incorrect user suggestions,\nthereby compromising integrity under pressure.\n2\nRelated Works\nPrior research establishes that preference optimiza-\ntion, while improving model helpfulness, rein-\nforces sycophantic tendencies by rewarding user\n1\narXiv:2601.18334v1  [cs.CL]  26 Jan 2026\n"}, {"page": 2, "text": "agreement over factual accuracy. Foundational\nstudies by (Sharma et al., 2023) and (Casper et al.,\n2023) demonstrate that LLMs often sacrifice truth-\nfulness to match perceived user preferences, a\nbyproduct of reward models struggling to distin-\nguish between actual correctness and the appear-\nance of it. Recent diagnostic frameworks have\nexpanded this to high-stakes domains: (Fanous\net al., 2025) introduced the dichotomy of progres-\nsive & regressive sycophancy to evaluate clinical\nadvice under varying rhetorical pressures, finding\nthat citation-based rebuttals most effectively trig-\nger harmful flips. Similarly, (Laban et al., 2023)\nand (Hong et al., 2025) moved toward multi-turn\nstability metrics, such as “Turn of Flip”, to cap-\nture the dynamics of conversational conformity\nunder sustained pressure. Furthermore, (Çelebi\net al., 2025) introduced the PARROT framework\nto evaluate how sycophancy impacts clinical rea-\nsoning across various medical subtasks, revealing\nthat models often abandon correct diagnostic paths\nwhen prompted with incorrect user beliefs. While\ndecoding-time interventions like (Khandelwal et al.,\n2025) attempt to resolve these knowledge conflicts\nalgorithmically, our work distinguishes by provid-\ning a noise-adjusted metric (Sa) that isolates true\nalignment bias from stochastic instability by math-\nematically accounting for model confusability.\n3\nMethodology\n3.1\nExperimental Design\nIn this section, we assess the robustness of LLMs\nagainst sycophantic perturbations in a clinical set-\nting. For this we utilize two widely cited bench-\nmarks: MedQA (Jin et al., 2020) and MMLU-Pro\n(Wang et al., 2024). We deliberately choose MCQ\ndatatsets because they offer verifiable ground truths\nwithout any ambiguity, allowing us to clearly dif-\nferentiate between factual correctness and apparent\ncorrectness arising due to sycophantic behavior.\nWith this each datapoint is associated with a single\nground truth y∗∈Cq, where Cq is the set of possi-\nble choices; where Cq is 4 and 10 for MedQA and\nMMLU-pro respectively.\nFor each question q, we evaluate the LLMs in\ntwo distinct modes to observe response stability\nand alignment: (i)Vanilla Mode: LLM is presented\nwith the original question to obtain vanilla response\nRv, and (ii)Perturbed Mode: The question is ap-\npended with a misleading nudge and a specific user\npersona to test for alignment bias.\nIn the perturbed mode, we further apply two\ntypes of perturbations/nudges to study LLMs re-\nsponse with different sources of influence: (i)Basic\nNudge: To each question, we append the mislead-\ning nudge: “I think the correct answer is x,” where\nx is incorrect option. (ii)Expert Nudge: To ana-\nlyze authority driven bias, we explicitly append a\nrole in user prompt i.e., “I am a Medical Expert, I\nthink....” before misleading nudge. Finally by sys-\ntematically varying over all (incorrect) misleading\nnudges, we construct a comprehensive perturbation\ndataset (Rp(x)) which we hypothesize is sufficient\nto expose sycophantic behavior in LLMs using our\nproposed sycophancy score.\n3.2\nSycophancy Score\nWe define the Sycophancy score(Sr) as the proba-\nbility of a model abandoning its internal parametric\nknowledge in favor of a misleading nudge:\nSr =\n1\n|Qc|\nX\nq∈Qc\n \n1\nnq −1\nX\nx∈Cq\\{y∗}\nI(Rp(x) = x)\n!\n(1)\nwhere Qc is the subset of questions where the\nmodel was initially correct (Rv = y∗), nq is the\ntotal number of choices. By restricting evaluation\nto Qc, we isolate alignment bias from a lack of\nparametric expertise.\nExisting literature typically relies on raw flip\ncounts, which can overestimate sycophancy scores\nby failing to account for model “confusability”,\ndefined as tendency to switch its answer under\nany prompt perturbations. To address this, we pro-\npose the Adjusted Sycophancy Score (Sa), which\naccounts for erratic flips by estimating True confus-\nability (Ctrue). We define an “erratic flip” as a case\nwhere q ∈Qc and the model, under a misleading\nnudge x, switches to an incorrect option other than\nx. Assuming random instability is equally likely to\nland on any incorrect choice, we define Ctrue as:\nCtrue = nq −1\nnq −2 ×\nCount(erratic_flips)\nCount(relevant_cases)\n(2)\nwhere relevant_cases are all instances where the\nmodel moved away from its correct vanilla re-\nsponse (Rp(x) ̸= y∗). Our final metric, Sa, ac-\ncounts for this randomness to provide robust mea-\nsure of alignment bias:\nSa = max\n\u0012\n0, Sr −Ctrue\nnq −1\n\u0013\n(3)\n2\n"}, {"page": 3, "text": "1.7B\n4B\n8B\n14B\n30B-A3B\n32B\n235B-A22B\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nSycophancy Score\nMedQA (4 choices)\nMMLU Pro (10 choices)\nRaw Score (Sr)\nAdjusted Score (Sa)\nFigure 1: Raw (Sr) and Adjusted (Sa) Sycophancy\nScores across the Qwen-3 model family.\n1B\n3B\n8B\n70B\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nSycophancy Score\nMedQA (4 choices)\nMMLU Pro (10 choices)\nRaw Score (Sr)\nAdjusted Score (Sa)\nFigure 2: Raw (Sr) and Adjusted (Sa) Sycophancy\nScores across the Llama-3 model family.\n4\nResults\nExperimental Setup and Model Selection.\nWe\nevaluate a diverse set of frontier LLMs to bench-\nmark clinical sycophancy. This includes closed-\nsource models (GPT-5.2 (OpenAI, 2025) and\nGPT4o (Hurst et al., 2024)) and open-weights mod-\nels (DeepSeek (DS) v3.1 (DeepSeek-AI, 2024),\nKimi K2 Think (Team et al., 2025), Mistral Large 3\n(Mistral, 2025), and GPT-OSS 120B (Agarwal et al.,\n2025)). To understand the sycophancy behavior\nacross parameter scales, we utilize two prominent\nmodel families: Qwen 3 (1.7B to 235B) (Yang et al.,\n2025) and Llama 3 (1B to 70B) (Dubey et al., 2024).\nEvaluations are conducted across the MedQA (4\nchoices) and the health-specific subsets of MMLU\nPro (10 choices), ensuring our sycophancy metric\nis robust across varying task granularities.\nFinding 1: Scaling Laws.\nFor both Qwen and\nLlama families, we observe non zero sychophantic\nscore, though consistently higher for MedQA (Fig-\nure 1) compared to MMLU-Pro (Figure 2). We also\nshow that how our proposed metric Sa consistently\nstays lower than raw sycophancy scores, which do\nnot account for erratic flips, especially noted for\nmodels under 8B parameters across both families.\nInterestingly, within the Qwen 3 family reveals\na clear inverse correlation between model scale\nand sycophancy score(Figure 1). While smaller\nlanguage models exhibit high sycophancy, we ob-\nserve a significant jump in resilience as parameter\nscale increases. Beyond this 14B threshold, the\nSa scores stabilize close to zero, suggesting that\na minimum threshold of parameters is required to\nmaintain internal belief against external pressure.\nThis highlights the need for greater caution when\ndeploying models in clinical settings and shows\nthe utility of our proposed metric in identifying\nmodels that needs additional alignment or safety\nguardrails to avoid harmful responses. In contrast,\nthe scaling trend is less pronounced for the Llama 3\nfamily (Figure 2). While the 1B variant exhibits ex-\ntreme sycophantic behavior, the 8B and 70B mod-\nels maintain elevated Sa scores compared to Qwen\n3 models of equivalent scale.\nWe also note that our proposed Sa score demon-\nstrates high robustness across benchmarks, yielding\nconsistent intra-family trends on both MedQA and\nMMLU Pro. This stability across datasets with\nvarying choice counts (4 vs. 10) confirms that the\nmetric successfully isolates intrinsic alignment bias\nfrom task-specific noise.\nFinding 2:\nThe Vulnerability of Reasoning\nTraces.\nWe analyze the effect of explicit Think-\ning traces on sycophancy behavior through compar-\nisons between thinking and non-thinking/instruct\nLLMs. As such variants are not available for all\nLLMs, hence, we restrict our analysis to family\nof Qwen 3. We analyze the sycophancy behav-\nior with both basic and expert nudge. Our evalu-\nation reveals a counter-intuitive vulnerability i.e.,\nwhile these models achieve superior performance\non unperturbed benchmarks, they show a fragile\nresilience to perceived authority (expert nudge).\nIn Figure 3, we observe that although Thinking\nmodels maintain a relatively constant sensitivity to\nbasic nudges compared to their Instruct counter-\nparts, the introduction of an Expert persona (i.e.,\nexpert nudge) triggers a significant performance\n3\n"}, {"page": 4, "text": "4B-Instruct\n4B-Thinking\n30B-A3B-Instruct\n30B-A3B-Thinking\n235B-A22B-Instruct\n235B-A22B-Thinking\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nInstruct (Vanilla)\nInstruct (Basic Nudge)\nInstruct (Expert Nudge)\nThinking (Vanilla)\nThinking (Basic Nudge)\nThinking (Expert Nudge)\n0.0\n0.2\n0.4\n0.6\n0.8\nAdjusted Sycophancy (Sa)\nSa (Basic)\nSa (Expert)\nFigure 3: Sa score and accuracy for both Instruct and Thinking Qwen-3 models on MedQA. Thinking models show\nsuperior accuracy but a fragile resilience to perceived authority.\ncollapse. This decline, reflected in Sa scores, sug-\ngests that the reasoning process in these models\nmight be prioritizing alignment with the user’s per-\nceived knowledge over its own internal parametric\nknowledge. We note that unlike self reflection hy-\npothesis observed by (DeepSeek-AI, 2024), the\nreasoning trace appears to facilitate sycophancy by\nlogically rationalizing the user’s incorrect sugges-\ntion to bridge the gap between internal facts and\nthe “expert’s” claim, making them more volatile\nfor clinical deployment.\nFinding 3: Benchmark Maturity and Author-\nity Resilience.\nEvaluation of frontier models re-\nveals a wide variance in sycophancy resilience, par-\nticularly when transitioning from neutral sugges-\ntions (basic nudge) to authoritative pressure (ex-\npert nudge). As shown in Table 1, most models\nshow robustness under the Basic Nudge, maintain-\ning low Sa scores. However, a significant vulner-\nability emerges under the Expert Nudge, where\nmodels like DS-V3.1 and Kimi K2 see their Sa\nscores jump to 0.27 (6.75x higher) and 0.15 (5x\nhigher), respectively, indicating a high suscepti-\nbility to authority bias/expert nudge. In contrast,\nSa scores for OpenAI’s GPT-5.2 and GPT-OSS,\nscores remains at or below 0.05 even under expert\nnudge. Empirically, GPT-OSS is known for having\na significantly simpler and more concise reasoning\nthought compared to the elaborate traces generated\nby DS-V3.1 and Kimi K2. We defer to future in-\nvestigation whether structurally simpler reasoning\nmechanisms inherently provide greater resistance\nBasic Nudge\nExpert Nudge\nModel\n∆Acc. ↓\nSa\n∆Acc. ↓\nSa\nGPT-4o\n−2.36\n0.03\n−4.36\n0.06\nGPT-5.2\n−1.94\n0.03\n−11.17\n0.17\nDeepSeek V3.1\n−4.22\n0.04\n−19.79\n0.27\nKimi K2\n−2.41\n0.03\n−10.86\n0.15\nMistral Large 3\n−7.48\n0.10\n−19.27\n0.31\nGPT-OSS-120b\n−0.33\n0.00\n−1.62\n0.05\nTable 1: Clinical model resilience measured by Accu-\nracy drop (∆Acc.) relative to vanilla performance and\nthe noise-adjusted sycophancy score (Sa).\nto authoritative (expert) nudges.\n5\nConclusion\nOur work highlights the critical tension between\nuser alignment and clinical safety. We introduce the\nAdjusted Sycophancy Score (Sa), a noise-aware\nmetric that isolates alignment bias from stochastic\ninstability by accounting for model confusability.\nOur results establish clear scaling laws for clini-\ncal resilience, showing that sycophancy stabilizes\nonly once models reach sufficient parameter scale.\nFurthermore, we reveal a paradoxical vulnerability\nin reasoning-optimized models: while \"Thinking\"\nvariants improve raw accuracy, their internal traces\ncan facilitate sycophancy by rationalizing incorrect\nuser suggestions under authoritative pressure. Fi-\nnally, high benchmark accuracy is an insufficient\nproxy for clinical readiness. We emphasize the\nneed for alignment strategies that reward epistemic\nintegrity over user deference to ensure that clinical\nLLMs serve as a robust check on, rather than a\nsophisticated echo of, human error.\n4\n"}, {"page": 5, "text": "6\nLimitations\nBenchmark and Linguistic Scope.\nOur evalu-\nation is primarily restricted to English-language\nMultiple-Choice\nQuestion\n(MCQA)\nformats.\nWhile MedQA and MMLU Pro serve as high-\nfidelity proxies for medical knowledge, they do\nnot capture the complexities of real-world clinical\ninteractions. In a real setting, sycophancy typi-\ncally unfolds across multi-turn conversations and\nthrough the subtle omission of contradictory evi-\ndence that are not fully captured by the binary “flip”\nof a single multiple-choice selection. Consequently,\nwhile Sa provides a robust measure of integrity, it\nmay under-represent the cumulative pressure of\nconversations.\nSimplification of User Authority.\nWhile we in-\ntroduced the Expert Nudge as a critical variable,\nour study probes a narrow subset of authority-\nbased pressure. In clinical practice, authority is\nmulti-faceted, involving specific medical special-\nties, varying degrees of assertiveness, and institu-\ntional hierarchies. Our model of authority may not\nfully represent the sophisticated strategies that can\ndegrade model integrity, such as the use of tech-\nnical jargon or the citation of fabricated clinical\nstudies to justify an incorrect diagnosis.\nAssumptions in Noise Adjustment.\nThe calcula-\ntion of our Adjusted Sycophancy Score (Sa) relies\non the assumption that stochastic erratic flips” are\nuniformly distributed across all incorrect options.\nWhile this provides a robust approximation for con-\nfusability, it may overlook instances where certain\n“distractor” choices in medical exams are more at-\ntractive due to common clinical misconceptions. A\nmore granular noise model that accounts for the\nvarying weights of specific distractors could further\nrefine the precision of Sa in future evaluations.\nReferences\nSandhini Agarwal, Lama Ahmad, Jason Ai, Sam Alt-\nman, Andy Applebaum, Edwin Arbus, Rahul K\nArora, Yu Bai, Bowen Baker, Haiming Bao, and 1\nothers. 2025. gpt-oss-120b & gpt-oss-20b model\ncard. arXiv preprint arXiv:2508.10925.\nStephen\nCasper,\nXander\nDavies,\nClaudia\nShi,\nThomas Krendl Gilbert, Jérémy Scheurer, Javier\nRando, Rachel Freedman, Tomasz Korbak, David\nLindner, Pedro Freire, and 1 others. 2023. Open\nproblems and fundamental limitations of reinforce-\nment learning from human feedback. arXiv preprint\narXiv:2307.15217.\nYusuf Çelebi, Mahmoud El Hussieni, and Özay Ezerceli.\n2025. Parrot: Persuasion and agreement robustness\nrating of output truth–a sycophancy robustness bench-\nmark for llms. arXiv preprint arXiv:2511.17220.\nDeepSeek-AI. 2024.\nDeepseek-v3 technical report.\nPreprint, arXiv:2412.19437.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and 1 others. 2024. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783.\nAaron Fanous, Jacob Goldberg, Ank Agarwal, Joanna\nLin, Anson Zhou, Sonnet Xu, Vasiliki Bikia, Rox-\nana Daneshjou, and Sanmi Koyejo. 2025. Syceval:\nEvaluating llm sycophancy. In Proceedings of the\nAAAI/ACM Conference on AI, Ethics, and Society,\nvolume 8, pages 893–900.\nJiseung Hong, Grace Byun, Seungone Kim, and Kai\nShu. 2025.\nMeasuring sycophancy of language\nmodels in multi-turn dialogues.\narXiv preprint\narXiv:2505.23840.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, and 1\nothers. 2024. Gpt-4o system card. arXiv preprint\narXiv:2410.21276.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2020. What dis-\nease does this patient have? a large-scale open do-\nmain question answering dataset from medical exams.\narXiv preprint arXiv:2009.13081.\nAnant Khandelwal, Manish Gupta, and Puneet Agrawal.\n2025. Cocoa: Confidence-and context-aware adap-\ntive decoding for resolving knowledge conflicts in\nlarge language models. In Proceedings of the 2025\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 6846–6866.\nPhilippe Laban, Lidiya Murakhovs’ ka, Caiming Xiong,\nand Chien-Sheng Wu. 2023. Are you sure? challeng-\ning llms leads to performance drops in the flipflop\nexperiment. arXiv preprint arXiv:2311.08596.\nMistral. 2025.\nMistral large 3 675b instruct\n2512.\nhttps://huggingface.co/mistralai/\nMistral-Large-3-675B-Instruct-2512.\nOpenAI. 2025.\nUpdate to gpt-5 system card:\nGpt-5.2.\nhttps://cdn.openai.com/pdf/\n3a4153c8-c748-4b71-8e31-aecbde944f8d/\noai_5_2_system-card.pdf.\nMrinank Sharma, Meg Tong, Tomasz Korbak, David Du-\nvenaud, Amanda Askell, Samuel R Bowman, Newton\nCheng, Esin Durmus, Zac Hatfield-Dodds, Scott R\nJohnston, and 1 others. 2023. Towards understand-\ning sycophancy in language models. arXiv preprint\narXiv:2310.13548.\n5\n"}, {"page": 6, "text": "Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen,\nJiahao Chen, Ningxin Chen, Ruijue Chen, Yanru\nChen, Yuankun Chen, Yutian Chen, and 1 others.\n2025. Kimi k2: Open agentic intelligence. arXiv\npreprint arXiv:2507.20534.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nAaran Arulraj, Xuan He, Ziyan Jiang, and 1 others.\n2024. Mmlu-pro: A more robust and challenging\nmulti-task language understanding benchmark. Ad-\nvances in Neural Information Processing Systems,\n37:95266–95290.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui,\nBo Zheng,\nBowen Yu,\nChang\nGao, Chengen Huang, Chenxu Lv, and 1 others.\n2025.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388.\n6\n"}, {"page": 7, "text": "A\nDetailed Experimental Results\nTable 2 provides the complete performance and\nsycophancy metrics for all evaluated models across\nthe MedQA benchmark. For all experiments, we\ndeployed the models locally using the vLLM frame-\nwork on a single compute node equipped with 8x\nNVIDIA H200 GPUs.\nB\nSensitivity to Role Placement\nOur experiments show that a model’s sycophancy is\nhighly sensitive to where the authoritative persona\nis placed. In our main paper, we used a “User-\nIntegrated Nudge”, appending the role and the in-\ncorrect suggestion together in the user prompt: \"I\nam a medical expert and I think the answer is x.\"\nThis led to major performance collapses in both\nfrontier (Table 1) and \"Thinking\" models (Fig-\nure 3).\nHowever, when we moved the role to the Sys-\ntem Prompt (\"You are an assistant to a medical\nexpert\") and kept only the basic suggestion in the\nuser prompt (\"I think the answer is x\"), the results\nchanged drastically. Under this setup, Thinking\nmodels showed almost no degradation (Figure 4),\nand frontier models were much more resilient (Ta-\nble 3).\nThis inconsistency proves that these models lack\na robust internal belief system. The fact that mov-\ning a single sentence can completely change a\nmodel’s diagnostic accuracy shows a dangerous\n\"contextual fragility.\" For clinical deployment, this\nvariability is a significant risk: a model’s medical\nreliability should not depend on whether a doctor\nintroduces themselves in the system instructions or\nthe active chat window.\n7\n"}, {"page": 8, "text": "Vanilla\nBasic Nudge\nExpert Nudge\nModel\nAcc (%)\nAcc (%)\nSr\nCtrue\nSa\nAcc (%)\nSr\nCtrue\nSa\nQwen3-4B-Instruct\n74.00\n66.95\n0.21\n0.00\n0.19\n66.95\n0.21\n0.00\n0.19\nQwen3-4B-Thinking\n77.06\n65.91\n0.27\n0.00\n0.26\n65.91\n0.27\n0.00\n0.26\nQwen3-30B-A3B-Instruct\n85.55\n79.67\n0.13\n0.00\n0.12\n79.67\n0.13\n0.00\n0.12\nQwen3-30B-A3B-Thinking\n89.55\n85.05\n0.09\n0.00\n0.08\n85.05\n0.09\n0.00\n0.08\nQwen3-235B-A22B-Instruct\n91.36\n84.60\n0.12\n0.00\n0.11\n84.60\n0.12\n0.00\n0.11\nQwen3-235B-A22B-Thinking\n92.93\n86.92\n0.10\n0.00\n0.09\n86.92\n0.10\n0.00\n0.09\nLlama-1B-Instruct\n37.94\n30.52\n0.61\n0.11\n0.57\n27.26\n0.17\n0.49\n0.00\nLlama-3B-Instruct\n56.01\n51.57\n0.19\n0.13\n0.14\n52.08\n0.19\n0.14\n0.14\nLlama-8B-Instruct\n63.47\n55.89\n0.23\n0.07\n0.21\n57.15\n0.24\n0.05\n0.22\nLlama-70B-Instruct\n84.13\n72.90\n0.23\n0.02\n0.22\n69.13\n0.30\n0.02\n0.29\nQwen3-1.7B\n52.79\n47.56\n0.28\n0.00\n0.25\n46.92\n0.30\n0.09\n0.27\nQwen3-4B\n71.88\n59.37\n0.31\n0.00\n0.30\n48.94\n0.53\n0.02\n0.52\nQwen3-8B\n77.53\n67.09\n0.25\n0.00\n0.24\n53.63\n0.50\n0.02\n0.49\nQwen3-14B\n82.64\n78.87\n0.11\n0.00\n0.10\n60.15\n0.42\n0.02\n0.42\nQwen3-32B\n84.84\n78.95\n0.11\n0.00\n0.08\n55.22\n0.39\n0.16\n0.34\nQwen3-30B-A3B\n86.10\n79.87\n0.13\n0.00\n0.11\n60.11\n0.45\n0.01\n0.44\nQwen3-235B-A22B\n91.59\n86.37\n0.09\n0.00\n0.09\n66.81\n0.38\n0.01\n0.38\nTable 2: Detailed performance and sycophancy metrics for the MedQA benchmark.\nVanilla\nBasic Nudge\nExpert Nudge\nModel\nAcc. (%)\nAcc. (%)\nSa\nAcc. (%)\nSa\nGPT4o\n88.53\n86.17\n0.03\n86.74\n0.02\nGPT-5.2\n94.34\n92.40\n0.03\n92.26\n0.03\nDeepSeek V3.1\n92.69\n88.47\n0.04\n83.48\n0.10\nMistral Large 3\n88.37\n80.89\n0.10\n80.03\n0.13\nGPT-OSS-120b\n90.02\n89.69\n0.00\n89.59\n0.02\nTable 3: MedQA evaluation. Role for the expert nudge is in the System prompt\n4B-Instruct\n4B-Thinking\n30B-A3B-Instruct\n30B-A3B-Thinking\n235B-A22B-Instruct\n235B-A22B-Thinking\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nInstruct (Vanilla)\nInstruct (Basic Nudge)\nInstruct (Expert Nudge)\nThinking (Vanilla)\nThinking (Basic Nudge)\nThinking (Expert Nudge)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nAdjusted Sycophancy (Sa)\nSa (Basic)\nSa (Expert)\nFigure 4: Sa score and accuracy for both Instruct and Thinking Qwen-3 models on MedQA when the role is in the\nSystem Prompt. Thinking models show no particular behavior change compared to the basic nudge.\n8\n"}]}