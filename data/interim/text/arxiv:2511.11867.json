{"doc_id": "arxiv:2511.11867", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.11867.pdf", "meta": {"doc_id": "arxiv:2511.11867", "source": "arxiv", "arxiv_id": "2511.11867", "title": "Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches", "authors": ["Namu Park", "Giridhar Kaushik Ramachandran", "Kevin Lybarger", "Fei Xia", "Ozlem Uzuner", "Meliha Yetisgen", "Martin Gunn"], "published": "2025-11-14T20:55:44Z", "updated": "2025-11-14T20:55:44Z", "summary": "Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.11867v1", "url_pdf": "https://arxiv.org/pdf/2511.11867.pdf", "meta_path": "data/raw/arxiv/meta/2511.11867.json", "sha256": "96c26ff3be44cffb1cfa3b5c8e448070c2f8915a3824acd44a8164afd43ed178", "status": "ok", "fetched_at": "2026-02-18T02:27:05.884800+00:00"}, "pages": [{"page": 1, "text": "Identifying Imaging Follow-Up in Radiology Reports:\nA Comparative Analysis of Traditional ML and LLM Approaches\nNamu Park1*, Giridhar Kaushik Ramachandran2*, Kevin Lybarger2\nFei Xia3, Özlem Uzuner2, Meliha Yetisgen1, Martin Gunn4\n1Department of Biomedical Informatics and Medical Education, School of Medicine,\nUniversity of Washington, Seattle, WA, USA\n2Department of Information Sciences and Technology, George Mason University, Fairfax, VA, USA\n3Department of Linguistics, University of Washington, Seattle, WA, USA\n4Department of Radiology, School of Medicine, University of Washington, Seattle, WA, USA\n*The first two authors contributed equally to this work\nAbstract\nLarge language models (LLMs) have shown considerable promise in clinical natural language processing, yet\nfew domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we\nintroduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging\nstatus, to support the development and benchmarking of follow-up adherence detection systems.\nUsing this\ncorpus, we systematically compared traditional machine-learning classifiers—logistic regression (LR), support vector\nmachines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct—with recent generative LLMs. To evaluate\ngenerative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline\n(Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and\ntheir surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance\nwas assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric\nbootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance\n(F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 =\n0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization,\ninterpretable and resource-efficient models remain valuable baselines.\nKeywords: Radiology NLP, Large Language Models, Comparative Analysis, Clinical Decision Support\n1.\nIntroduction\nFollow-up recommendations are frequently in-\ncluded in radiology reports, varying depending on\nthe modality and body region, with an average of\n19.9% of reports containing such recommenda-\ntions (Lau et al., 2020). These often, but not always,\nspecify an imaging modality, reason, and timeframe.\nThe referrer is typically responsible for ensuring pa-\ntient communication and follow-up (Larson et al.,\n2014), but radiology departments usually do not\ntrack recommendations and may be unaware if\nfollow-ups are ordered or completed. Accordingly,\nfollow-up adherence is suboptimal, with over a third\nof recommendations not being followed up as re-\nported in a recent publication (Mabotuwana et al.,\n2019). Although undoubtedly a sizeable propor-\ntion of follow-up imaging is not ordered because\nit is felt unnecessary by the patient or clinician,\none study found that more than 35% of recommen-\ndations were not followed up simply due to the\nreferring clinician not acknowledging them (Callen\net al., 2012). Reasons related to failure to follow-up,\nespecially on incidental findings, include referring\nclinician missing the recommendations or losing\ntrack of them while addressing a more acute ill-\nness, loss of information during handover between\ncare teams, the recommendation not being com-\nmunicated to the patient, and the patient failing\nto schedule or show up for the follow-up appoint-\nment(Kulon, 2016).\nMissing recommended follow-ups can have se-\nrious clinical consequences, particularly when in-\ncidental findings represent early manifestations of\nmalignancy or other progressive disease. Beyond\nindividual patient harm, inadequate follow-up adher-\nence also contributes to inefficiencies in healthcare\ndelivery and increased system costs (Kadom et al.,\n2022). Accordingly, improving the identification and\ntracking of follow-up recommendations is essential\nfor ensuring timely care and reducing preventable\nmorbidity and mortality. However, progress in this\narea has been limited in part by the lack of publicly\navailable, well-annotated corpora tailored to this\nclinical problem.\nTo address this gap, we developed a corpus of\n6,393 radiology reports from 586 patients, each\nmanually annotated for the follow-up imaging sta-\ntus. Using this resource, we systematically evalu-\nated multiple machine learning and large language\nmodel (LLM) approaches for the task of follow-up\nidentification in free-text radiology reports—a do-\nmain characterized by high linguistic variability, im-\nplicit clinical reasoning, and long-context dependen-\narXiv:2511.11867v1  [cs.CL]  14 Nov 2025\n"}, {"page": 2, "text": "cies. The evaluated models included logistic regres-\nsion (LR), support vector machines (SVM)(Hearst\net al., 1998), Longformer (Beltagy et al., 2020), and\na fully fine-tuned Llama3-8B-Instruct (lla, 2024),\nalongside generative models GPT-4o(Hurst et al.,\n2024) and GPT-OSS-20B. By introducing this an-\nnotated corpus and benchmarking across both tra-\nditional and generative paradigms, this study pro-\nvides a valuable resource and empirical foundation\nfor advancing real-world clinical NLP applications\nin radiology.\n2.\nRelated Work\nGiven the critical importance of identifying follow-\nup recommendations to prevent potentially fatal\noutcomes, numerous NLP-based methods have\nbeen developed for radiology reports. For instance,\nYetisgen et al. introduced a supervised text classi-\nfication approach to detect recommendation sen-\ntences, leveraging features beyond simple unigram\ntokens (Yetisgen-Yildiz et al., 2011). Conditional\nRandom Fields (CRF) (Lafferty et al., 2001) have\nalso been utilized to capture temporal information\nwithin follow-up recommendations (Xu et al., 2012).\nOther studies have compared traditional ma-\nchine learning models, such as Support Vector Ma-\nchines (SVMs) (Hearst et al., 1998) and Random\nForests (RFs) (Breiman, 2001), with deep learn-\ning approaches like Recurrent Neural Networks\n(RNNs) (Schuster and Paliwal, 1997) for identifying\nfollow-up recommendations (Carrodeguas et al.,\n2019). Lau et al. created a corpus of radiology\nreports annotated with entities related to follow-up\nrecommendations, trained a neural model on this\ndataset, and applied it at scale to assess adherence\nrates (Lau et al., 2020).\nIn addition, other research has focused on build-\ning tools that evaluate follow-up compliance by\nextracting essential elements such as the recom-\nmended time frame and imaging modality from ra-\ndiology reports (Mabotuwana et al., 2018). Dalal\net al. tackled the challenge of tracking follow-up\nadherence with an Extremely Randomized Trees\nmodel (Geurts et al., 2006) that integrates vari-\nous clinical features, including recommendation\nattributes, metadata, and text similarity (Dalal et al.,\n2020). This model achieved an F1 score of 0.807,\nclosely approaching the inter-annotator agreement\nof 0.853 F1, demonstrating its effectiveness in track-\ning follow-up adherence and its potential utility in\nreal-world clinical settings. However, the applica-\ntion of generative LLMs to this specific task is still\nunderstudied. Therefore, using our annotated cor-\npus as a benchmark resource, we aim to evalu-\nate both traditional machine learning models and\ngenerative LLMs for the task of follow-up identifi-\ncation, providing insights into model performance,\ngeneralizability, and practical suitability for clinical\ndeployment.\n3.\nMethods\nThis retrospective study was approved by the In-\nstitutional Review Board (IRB) at the authors’ insti-\ntution and satisfies the waiver of patient informed\nconsent. All radiology reports were de-identified\nprior to annotation. Python code used for the ex-\nperiments is available on our GitHub repository:\nhttps://github.com/XXX.\n3.1.\nTask Description\nTo track completion of follow-up imaging tests, we\ndefine two types of radiology reports. Reports that\ncontained a finding for which an imaging follow-\nup recommendation was present were termed “in-\ndex reports.” Follow-up recommendations are of-\nten associated with potential malignancy and typ-\nically (though not invariably) specify the recom-\nmended follow-up timeframe, imaging modality,\nand anatomy or pathology to be re-imaged (e.g.,\n“recommend follow-up with chest CT in 6 months\nto assess stability of the lung nodule”). For a given\nindex report, all subsequent imaging reports for\nthe same patient in the radiology information sys-\ntem (RIS) were considered—referred to hereafter\nas “candidate reports.” Thus, each patient is rep-\nresented by a single index report and its associ-\nated candidate reports. Figure 1 provides a visual\noverview of the follow-up identification task.\nFigure 1: Follow-up identification task\n3.2.\nDataset\n3.2.1.\nSampling Process\nWe used a clinical database containing 7 million\nradiology reports from 959,382 patients, covering\n"}, {"page": 3, "text": "the years 2007–2020. All reports were de-identified\nusing a neural de-identifier (Lee et al., 2021). Dur-\ning this period, each patient underwent an average\nof 7.45 radiology examinations, with 29.9% having\nonly a single report.\nWe focused on selecting the most relevant index\nreports for our task using the approaches described\nin Figure 2. We limited index report modality to\nCT and MRI, as these frequently include follow-up\nrecommendations.\nTo ensure rapid and accurate sampling, we com-\nbined automated and manual methods. First, to\nincrease the prevalence of recommendation sen-\ntences among candidate index reports, we devel-\noped a lightweight SVM-based recommendation\nsentence classifier using an existing annotated cor-\npus (Lau et al., 2022). Trained on binary sentence\nlabels (has recommendation vs. no recommenda-\ntion), the classifier evaluated each sentence and\nmarked it as a recommendation when appropri-\nate. The sentence-level performance was 0.87\nF1, and it identified 181,020 CT and 52,809 MRI\nreports with at least one recommendation sen-\ntence—12.8% and 9.8% of notes per modality, re-\nspectively.\nFrom reports with at least one follow-up recom-\nmendation, we next identified those including a\nmass lesion. After randomly selecting 25,000 re-\nports per modality, we applied a BERT-based en-\ntity and relation extractor from prior work (Park\net al., 2024) to extract detailed clinical findings\n(mass lesions, problems, attributes). We targeted\ncases with newly discovered lesion findings in six\nanatomies—lung, kidney, liver, adrenal gland, pan-\ncreas, and thyroid—yielding 2,095 CT reports and\n464 MRI reports, each with at least one new lesion\nin a target anatomy and at least one recommenda-\ntion sentence.\nFrom this refined pool, 720 index reports were\nrandomly selected. Two medical student annota-\ntors then excluded potentially incorrect samples\nfrom automated filtering, removing reports with ex-\ncessively vague recommendations (e.g., “attention\non follow-up is recommended,” “clinical correlation\nis recommended”) and those recommending or\nleading to same-day characterization of acute find-\nings. The combined filtering resulted in 586 index\nreports, with 134 inappropriate samples removed.\nFor each of the 586 index reports, corresponding\ncandidate reports were sampled without restriction\non imaging modality, resulting in 5,807 candidate\nreports (mean 9.9 per index; range [1, 168]).\n3.2.2.\nData Annotation\nAll reports were annotated by two medical students,\nwho identified the earliest qualifying follow-up re-\nport among the candidate reports, if it existed. A\ncandidate report was considered a follow-up if it\naddressed the same anatomical region as the le-\nsion in the index report and explicitly referenced or\nnegated the prior finding (e.g., “multiple pulmonary\nnodules unchanged from the previous examination,”\n“redemonstration of hypodense mass measuring\n2.6 × 2.4 cm, previously 3.2 × 3.0 cm”). In multi-\nlesion cases, identifying the most suitable follow-up\nexam was challenging; such complex cases were\nflagged and reviewed with a board-certified radiolo-\ngist. The annotation process comprised 16 rounds.\nIn the first 11 rounds, samples were doubly an-\nnotated, disagreements were resolved in weekly\nmeetings, and any controversial cases were adju-\ndicated with a board-certified radiologist. After 11\nrounds, inter-annotator agreement was 0.846 F1.\nThe subsequent 5 rounds were single annotated to\nincrease volume, resulting in 347 single-annotated\nreport pairs and 239 double-annotated report pairs.\nAmong the 586 index–candidate pairs, a qual-\nifying follow-up report was identified in 417 pairs\n(71.3%), while 169 index reports (28.7%) had\nno follow-up identified.\nFor 54% of index re-\nports, the first or second candidate chronolog-\nically was labeled as the follow-up.\nWhile in-\ndex reports included only CT and MRI, candi-\ndate reports spanned modalities: computed/dig-\nital radiography (n=2,139, 36.8%), CT (n=2,003,\n34.5%), MRI (n=520, 8.9%), ultrasound (n=516,\n8.8%), nuclear medicine (n=171, 2.9%), angiogra-\nphy (n=127, 2.2%), PET-CT (n=99, 1.7%), mam-\nmography (n=65, 1.1%), etc. Index reports aver-\naged 437.5 tokens (30.4 sentences) versus 252\ntokens (17.3 sentences) for candidate reports. We\nused the BERT tokenizer (Devlin et al., 2019),\nwhich converts text into subword tokens.\n3.3.\nModels\nPrior work (Mabotuwana et al., 2018) in follow-\nup imaging identification applied discrete machine\nlearning models combining text and engineered lin-\nguistic features to predict the likelihood of a correct\ncandidate report. We framed follow-up identifica-\ntion as a binary classification task by constructing\nindex–candidate pairs and labeling each pair posi-\ntive if the candidate report was the correct follow-up\nfor the index report.\n3.3.1.\nFeature-based Traditional Models\nWe evaluated Logistic Regression (LR) and Sup-\nport Vector Machines (SVM) in a supervised learn-\ning setting, with inputs consisting of index and can-\ndidate report text plus metadata (imaging modality\nand report time gap in days). Separate vector rep-\nresentations for index and candidate reports were\ncreated using TF-IDF (Sparck Jones, 1972). Meta-\ndata were encoded using binary indicator vectors.\nA binary vector representing words shared by index\n"}, {"page": 4, "text": "Figure 2: Sampling process for index reports\nand candidate reports was concatenated to form\nthe final representation vector. The SVM used a sig-\nmoid kernel and L2 regularization; both SVM and\nLR used class-balanced loss functions. Figure 3\nillustrates the SVM and LR pipelines. Individual\nvectors with separate encodings for metadata and\ntext represented each of the index (Vindex) and can-\ndidate (Vcandidate) reports; the final pair vector (Vpair)\nconcatenated index, candidate, and shared-word\nvectors.\n3.3.2.\nSupervised Learning using\nTransformer-based Models\nWe also investigated supervised learning with\nTransformer-based models capable of extended\ninputs, including Longformer (Beltagy et al., 2020),\nBioClinicalModernBERT (Sounack et al., 2025),\nLlama3-8B-Instruct (lla, 2024), and Llama3.1-8B-\nInstruct (Grattafiori et al., 2024), with model selec-\ntion guided by compute constraints. We report re-\nsults for Longformer and Llama3-8B-Instruct, which\ndemonstrated the highest performance. Each input\nconcatenated index and candidate report text sep-\narated by special tokens. Longformer used a linear\nclassification layer. Both Longformer and Llama3-\n8B-Instruct were trained on the complete index\nand candidate text plus metadata. For Llama3-8B-\nInstruct, we tested several prompt designs; the best\nincluded a brief instructional prefix preceding the re-\nport pair (Figure 4). The model was then instruction-\ntuned via full supervised fine-tuning (SFT) using\nthis engineered prompt. Additional implementation\ndetails are available in our GitHub repository.\nFigure 3: Follow-up identification using feature-\nbased traditional models – SVM and LR\n3.4.\nGenerative Large Language Model\nTo evaluate generative LLMs for identifying follow-\nup imaging studies, we assessed GPT-4o (Hurst\net al., 2024) (version 2024-05-13) and GPT-OSS-\n20B (Agarwal et al., 2025) in our HIPAA-compliant\nenvironment using two strategies:\na baseline\nsetting and an advanced, task-optimized setting\n"}, {"page": 5, "text": "Figure\n4:\nFollow-up\nidentification\nusing\nTransformer-based models – Longformer and\nLlama3-8B-Instruct\n(Figure 5).\nGPT-4o provides strong instruction-\nfollowing and clinical reasoning, and GPT-OSS-\n20B is a recent open-source model suitable for\nsecure institutional deployment. Larger variants\nsuch as GPT-OSS-120B were not used, as they of-\nfered only marginal gains in our task at substantially\nhigher computational cost.\nIn the baseline setting, the model received the\nfull index and candidate reports including metadata,\naccompanied by minimal task-specific instruction,\nand was asked to determine whether the candidate\nreport represented an appropriate follow-up. In con-\ntrast, the advanced setting restricted input to meta-\ndata and the recommendation sentence—identified\nusing the SVM-based sentence classifier in Sec-\ntion 3.2.1—along with its immediate preceding sen-\ntence (green box in Figure 5), emphasizing clinically\nrelevant content for follow-up determination.\nTo optimize prompt engineering, we randomly\nselected 60 index reports and their corresponding\ncandidate reports for iterative refinement during de-\nvelopment. All prompt optimization was conducted\nusing GPT-4o, and the final prompt that achieved\nthe best development performance was adopted as\nthe standard prompt for both the baseline and ad-\nvanced settings. This optimized prompt was then\ndirectly applied to GPT-OSS-20B to assess cross-\nmodel transferability. The 60 development samples\nused for prompt optimization were excluded from\nfinal evaluation, which followed the performance\ncriteria described in the next section.\nFollowing initial error inspection on the develop-\nment set, we observed that GPT-OSS-20B tended\nto judge candidate reports as incorrect when not all\nfindings in the recommendation were re-addressed.\nTo account for this behavior, we introduced an ad-\nditional instruction on top of the GPT-4o prompt\nclarifying that a follow-up remains valid even if only\na subset of the recommended findings is addressed\n(Figure 5).\n3.5.\nEvaluation\nWe evaluated performance at the index report-level\nchronologically (Figure 6) using the following cate-\ngories: 1) True Positive (TP) - correctly predicted\nthe follow-up candidate report, if it existed. If multi-\nple positive predictions existed, only the first one\nwas considered for comparison because this is the\nclinically most important examination to ensure that\nfollow-up did occur. 2) False Positive (FP) - incor-\nrectly identified a follow-up when it was not a follow-\nup; 3) False Negative (FN) - did not predict the\ncorrect follow-up which existed; and 4) True Neg-\native (TN) - correctly predicted the absence of a\nfollow-up when there was no matching follow-up\nexamination.\nThe evaluation set consisted of 526 index reports\nand their corresponding candidate reports, exclud-\ning 60 index reports and their associated candidate\nreports that were used for prompt tuning of GPT-4o.\nWe evaluated our models in two settings: (1) five-\nfold cross-validation (CV) for supervised learning\nmodels and (2) in-context learning for generative\nLLMs. For CV, we set train-validation-test splits\nand tune the hyperparameters at every fold. We\nreported precision, sensitivity, F1, and specificity\nfor all our approaches, along with their 95% Con-\nfidence Intervals (CI). We utilized non-parametric\nbootstrap test to compare the models’ F1 scores\nwith a significance threshold of 0.05.\n4.\nResults\nTable 1 summarizes the aggregated performance\nof all evaluated models.\nAmong them, GPT-\n4o (Advanced) achieved the highest overall per-\nformance, with an F1 score of 0.832 (95% CI:\n0.802–0.860)—closely approximating the inter-\nannotator agreement score of 0.846. The next best\nperformer was GPT-OSS-20B (Advanced), which\nachieved an F1 of 0.828 (0.799–0.856), demon-\nstrating nearly equivalent performance to GPT-4o\ndespite being a smaller, fully open-source model.\nBoth advanced configurations markedly outper-\nformed their respective base counterparts, under-\nscoring the benefit of the optimized input design\nthat emphasized follow-up recommendation cues\nwithin the radiology reports. Specifically, GPT-4o\n(Advanced) achieved a precision gain of ∆+0.101,\na recall increase of ∆+0.018, and a correspond-\ning F1 improvement of ∆+0.060 relative to its base\nsetting. Similarly, GPT-OSS-20B (Advanced) im-\nproved precision by ∆+0.090, recall by ∆+0.058,\n"}, {"page": 6, "text": "Figure 5: Follow-up identification using generative LLMs. GPT-OSS-20B prompt is created by adding\n2-sentence instructions on top of GPT-4o prompt. Outputs for base and advanced settings are actual\npredictions from GPT-4o.\nTable 1: Model performance metrics with 95% confidence intervals\nModel\nPrecision\nRecall\nF1\nSVM\n0.769 (0.727, 0.812)\n0.785 (0.743, 0.827)\n0.777 (0.743, 0.809)\nLR\n0.792 (0.750, 0.833)\n0.759 (0.715, 0.802)\n0.775 (0.740, 0.808)\nLongformer\n0.741 (0.694, 0.788)\n0.639 (0.590, 0.687)\n0.686 (0.646, 0.724)\nLlama3-8B-Instruct\n0.907 (0.872, 0.943)\n0.623 (0.573, 0.670)\n0.738 (0.698, 0.775)\nGPT-4o (Base)\n0.740 (0.698, 0.783)\n0.807 (0.767, 0.846)\n0.772 (0.738, 0.803)\nGPT-4o (Advanced)\n0.841 (0.803, 0.878)\n0.825 (0.786, 0.863)\n0.832 (0.802, 0.860)\nGPT-OSS-20B (Base)\n0.734 (0.690,0.777)\n0.775 (0.733,0.818)\n0.753 (0.720,0.787)\nGPT-OSS-20B (Advanced)\n0.824 (0.786,0.862)\n0.833 (0.795,0.870)\n0.828 (0.799,0.856)\nand F1 by ∆+0.075 compared to its base configu-\nration. Statistical significance testing (Table 2) con-\nfirmed that these gains were significant (p < 0.05)\nfor each model relative to their baselines. However,\nno statistically significant difference was observed\nbetween GPT-4o (Advanced) and GPT-OSS-20B\n(Advanced), suggesting that GPT-OSS-20B can\nserve as a viable and complementary alternative\nto GPT-4o—particularly in resource-constrained\nor privacy-sensitive environments where closed-\nsource APIs are less feasible.\nThe fully fine-tuned Llama3-8B-Instruct model\n"}, {"page": 7, "text": "Figure 6: Evaluation method for follow-up identification task. The check mark indicates that the target\ncandidate report has been identified as a follow-up report either by the model (Prediction) or by the\nannotators (Truth). Boxes in purple show the prediction by the model and boxes in green are the true\nlabels\nachieved the highest precision among all systems\n(0.907, 95% CI: 0.872–0.943), yet its recall re-\nmained substantially lower (0.623, 0.573–0.670),\nresulting in a reduced F1 score compared to most\nother approaches. This imbalance may reflect over-\nfitting during fine-tuning, where the model captured\ntraining-specific patterns at the expense of gener-\nalization to unseen cases. Alternatively, the limited\nrecall could stem from the pronounced class imbal-\nance in the dataset—only 417 candidate reports\n(7.2% of total pairs) represented true follow-ups.\nSuch skewed distributions can bias models toward\nnegative predictions, diminishing sensitivity to the\nminority class. Future work could explore class-\nbalanced sampling or focal loss optimization to mit-\nigate this effect and improve model robustness on\nrare follow-up cases.\n5.\nDiscussion\nDuring prompt engineering using GPT-4o, we ob-\nserved cases where the model exhibited a tendency\nto classify all candidate reports as non–follow-ups.\nPrompts that included overly detailed instructions\noften led to incorrect predictions, highlighting LLMs’\nsensitivity to prompt complexity. Even with opti-\nmized prompting, GPT-4o (Base) achieved an F1\nscore of 0.772 (95% CI: 0.738–0.803), which was\nnot statistically different from SVM (F1 = 0.777,\n95% CI: 0.743–0.809) or LR (F1 = 0.775, 95% CI:\n0.740–0.808). This finding suggests that effective\nprompt engineering is essential for GPT-4o, while\nmore sophisticated, task-tailored approaches may\nstill be required for it to consistently outperform\nsimpler, more efficient traditional ML models.\nThe examples shown in Figure 5 demonstrate the\nbenefit of using task-specific, condensed input for\nGPT-4o (Advanced). The index report documents\nmultiple findings, including “left upper lobe con-\nsolidation” and “hypodensities within the spleen,”\nbut the follow-up recommendation specifically con-\ncerns the splenic hypodensities and calls for a CT\nabdomen and pelvis. The candidate report, how-\never, is not a CT of the recommended anatomy\nand does not address the splenic findings, making\nit unqualified as an appropriate follow-up. In the\n"}, {"page": 8, "text": "Table 2: Pairwise statistical significance comparisons for our models using the non-parametric bootstrap\ntest (n=253 patients drawn with replacement, 10,000 repetitions). P-values are indicated in parentheses.\nGPT-4o\n(Advanced)\nGPT-OSS-20B\n(Advanced)\nGPT-4o\n(Base)\nGPT-OSS-20B\n(Base)\nSVM\nLR\nLlama3-8B\nLongformer\nF1\n(95% CI)\n0.832\n(0.802, 0.860)\n0.828\n(0.799, 0.856)\n0.772\n(0.738, 0.803)\n0.753\n(0.720, 0.787)\n0.777\n(0.743, 0.809)\n0.775\n(0.740, 0.808)\n0.738\n(0.698, 0.775)\n0.686\n(0.646, 0.724)\nGPT-4o\n(Advanced)\nN/A\nNo\n(0.3141)\nYes\n(0.0001)\nYes\n(0.0001)\nYes\n(0.0001)\nYes\n(0.0003)\nYes\n(0.0001)\nYes\n(0.0001)\nGPT-OSS-20B\n(Advanced)\nN/A\nYes\n(0.0002)\nYes\n(0.0001)\nYes\n(0.0005)\nYes\n(0.0001)\nYes\n(0.0001)\nYes\n(0.0001)\nGPT-4o\n(Base)\nN/A\nYes\n(0.0454)\nNo\n(0.3839)\nNo\n(0.4432)\nYes\n(0.0462)\nYes\n(0.0001)\nGPT-OSS-20B\n(Base)\nN/A\nNo\n(0.0833)\nNo\n(0.1024)\nNo\n(0.2433)\nYes\n(0.0002)\nSVM\nN/A\nNo\n(0.4567)\nYes\n(0.0376)\nYes\n(0.0007)\nLR\nN/A\nYes\n(0.0371)\nYes\n(0.0005)\nLlama3-8B\nN/A\nYes\n(0.0107)\nLongformer\nN/A\nBase setting, which provided the full index report as\ninput, the model incorrectly predicted this case as\na valid follow-up by focusing only on lexical overlap.\nIn contrast, the Advanced setting—by restricting\ninput to the recommendation sentence and meta-\ndata—correctly identified the mismatch in anatomy\nand modality. We anticipate that this targeted input\ndesign reduced false positives, leading to improved\nprecision (0.740 vs. 0.841).\nExtending this analysis, the open-source GPT-\nOSS-20B model provided additional insight into\nmodel-specific prompt sensitivity.\nWhen using\nthe same prompt as GPT-4o, GPT-OSS-20B ex-\nhibited lower performance (F1 = 0.799, 95% CI:\n0.767–0.831), primarily due to over-strict reason-\ning that rejected valid follow-ups when not all find-\nings from the recommendation were addressed.\nAfter refining the prompt to clarify that \"a follow-up\nremains valid even if only a subset of the recom-\nmended findings is discussed\", GPT-OSS-20B (Ad-\nvanced) improved substantially to 0.828 F1 (95%\nCI: 0.799–0.856), achieving performance statisti-\ncally indistinguishable from GPT-4o (Advanced) (p\n= 0.3141). This demonstrates that small, concep-\ntually meaningful adjustments to task framing can\nharmonize LLM reasoning with clinical logic, par-\nticularly for open-source models whose instruction-\nfollowing behavior may differ from proprietary coun-\nterparts.\nIt is noteworthy that feature-based models such\nas LR and SVM performed comparable or better\nthan GPT-4o (Base) and GPT-OSS-20B (Base),\nunderscoring their value as interpretable and\nresource-efficient alternatives for deployment in\nclinical applications. These models are especially\nadvantageous in settings where computational re-\nsources for model development, inference, and\nvalidation are limited. In our study, analysis of the\nLR feature weights revealed that the most influ-\nential predictors combined metadata (e.g., time\ngap, anatomy) with terms describing findings and\ntheir characteristics. Words such as “nodule,” “le-\nsion,” “unremarkable,” and “benign” were among\nthe strongest positive contributors—closely aligning\nwith the reasoning processes employed by radiolo-\ngists.\n6.\nConclusion\nIn this study, we present a newly annotated corpus\nof 6,393 radiology reports from 586 patients, de-\nsigned to support the development and benchmark-\ning of models for identifying imaging follow-ups.\nUsing this resource, we comprehensively evalu-\nated a spectrum of methods ranging from traditional\nfeature-based classifiers and transformer-based en-\ncoders to recent generative LLMs. Among all eval-\nuated systems, GPT-4o (Advanced) achieved the\nhighest performance (F1 = 0.832), closely matching\ninter-annotator agreement (F1 = 0.846). The open-\nsource GPT-OSS-20B (Advanced), when guided\nby a refined prompt incorporating task-specific clar-\nification, achieved comparable results (F1 = 0.828)\nwithout a statistically significant difference.\nThese results underscore the importance of task-\naware prompt design and input curation in optimiz-\ning LLM reasoning for clinical applications. While\nclosed-source models such as GPT-4o demon-\nstrate strong out-of-the-box performance, open-\nsource systems like GPT-OSS-20B achieve compa-\nrable accuracy with greater flexibility for fine-tuning\nand integration within secure institutional environ-\nments. Despite the advances of LLMs, traditional\nmodels such as logistic regression (LR) and sup-\nport vector machines (SVM) remain valuable, offer-\ning interpretable and computationally efficient base-\nlines. Collectively, this corpus and evaluation pro-\nvide a reproducible foundation for future research,\nhighlighting a complementary landscape where\nopen and closed LLMs, alongside interpretable\nclassical models, together advance the robustness\nand transparency of clinical NLP systems.\n"}, {"page": 9, "text": "7.\nLimitations\nAccurate identification of follow-up imaging has im-\nportant implications for clinical practice, including\nreducing unnecessary scans and improving the\nmanagement of incidental findings. To enhance\nmodel generalizability, future work should lever-\nage multi-institutional datasets encompassing a\nbroader range of imaging modalities and clinical\nscenarios. Multi-modal approaches that integrate\nimaging data with radiology text, as well as the use\nof domain-adapted vision-language models (e.g.,\nMedGemma (Sellergren et al., 2025), MedVLM-R1\n(Pan et al., 2025)), represent promising avenues\nfor advancing performance and clinical relevance.\nFinally, evaluating LLM outputs should extend\nbeyond standard performance metrics such as pre-\ncision, recall, and F1. Our current study was limited\nto these measures, but future evaluations should\nincorporate radiologist review, clinical reasoning as-\nsessments, and systematic evaluations of reason-\ning quality, factual consistency, and clinical validity.\nEstablishing such robust evaluation frameworks will\nbe essential to ensure the safe and effective de-\nployment of LLMs in real-world healthcare settings.\n8.\nEthics Statement\nWe obtained approval from our institution’s Institu-\ntional Review Board (IRB) with a waiver of informed\nconsent for the use of clinical text data. Radiology\nreports may contain patient Protected Health Infor-\nmation (PHI), including names, contact information,\nand other identifiers. To ensure patient privacy,\nall reports were automatically de-identified using a\nneural de-identification model, followed by manual\nreview and secondary de-identification by trained\nmedical student annotators to verify that no residual\nPHI remained. Both the original and de-identified\nreports were securely stored on a Health Insur-\nance Portability and Accountability Act (HIPAA)-\ncompliant server. All researchers and annotators\ncompleted human subjects training and were au-\nthorized to handle data containing PHI.\nThe annotated corpus used in this study was\nrandomly sampled from the general population of\npatients undergoing imaging examinations at a sin-\ngle academic medical center. The dataset includes\na diverse range of imaging modalities and clinical\ncontexts, and was developed to support the task\nof identifying imaging follow-ups. Demographic\nvariables were not used during sampling, and the\npatient population may not be representative of\nother institutions or the broader population. Differ-\nences in report style, structure, and documentation\npractices across sites may therefore affect the gen-\neralizability of the developed models.\n9.\nAcknowledgments\nThis work was supported in part by the National\nInstitutes of Health and the National Cancer Insti-\ntute (NCI) (GrantNr. XXX). The content is solely\nthe responsibility of the authors and does not nec-\nessarily represent the official views of the National\nInstitutes of Health.\n10.\nBibliographical References\n2024. Introducing meta llama 3: The most capable\nopenly available llm to date.\nSandhini Agarwal, Lama Ahmad, Jason Ai, Sam\nAltman, Andy Applebaum, Edwin Arbus, Rahul K\nArora, Yu Bai, Bowen Baker, Haiming Bao, et al.\n2025. gpt-oss-120b & gpt-oss-20b model card.\narXiv preprint arXiv:2508.10925.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document trans-\nformer.\nLeo Breiman. 2001. Random forests. Machine\nlearning, 45:5–32.\nJoanne L. Callen et al. 2012. Failure to follow-up\ntest results for ambulatory patients: a systematic\nreview. J Gen Intern Med, 27(10):1334–1348.\nEmmanuel Carrodeguas, Ronilda Lacson, Whit-\nney Swanson, and Ramin Khorasani. 2019. Use\nof machine learning to identify follow-up recom-\nmendations in radiology reports. Journal of the\nAmerican College of Radiology, 16(3):336–343.\nSandeep\nDalal,\nVadiraj\nHombal,\nWei-Hung\nWeng, Gabe Mankovich, Thusitha Mabotuwana,\nChristopher S. Hall, Joseph Fuller, Bruce E. Lehn-\nert, and Martin L. Gunn. 2020.\nDetermining\nFollow-Up Imaging Study Using Radiology Re-\nports. J Digit Imaging, 33(1):121–130.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 4171–4186.\nPierre Geurts, Damien Ernst, and Louis Wehenkel.\n2006. Extremely randomized trees. Machine\nlearning, 63:3–42.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav\nJauhri, Abhinav Pandey, Abhishek Kadian, Ah-\nmad Al-Dahle, Aiesha Letman, Akhil Mathur,\n"}, {"page": 10, "text": "Alan Schelten, Alex Vaughan, et al. 2024.\nThe llama 3 herd of models.\narXiv preprint\narXiv:2407.21783.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna,\nJohn Platt, and Bernhard Scholkopf. 1998. Sup-\nport vector machines. IEEE Intelligent Systems\nand their applications, 13(4):18–28.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford,\net al. 2024. Gpt-4o system card. arXiv preprint\narXiv:2410.21276.\nNadja Kadom, Arjun K Venkatesh, Samantha A\nShugarman, Judy H Burleson, Christopher L\nMoore, and David Seidenwurm. 2022. Novel\nquality measure set: closing the completion loop\non radiology follow-up recommendations for non-\ncritical actionable incidental findings. Journal of\nthe American College of Radiology, 19(7):881–\n890.\nMichal E. Kulon. 2016. Lost to Follow-Up : Auto-\nmated Detection of Patients Who Missed Follow-\nUps Which Were Recommended on Radiology\nReports.\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001. Conditional random fields: Prob-\nabilistic models for segmenting and labeling se-\nquence data. 1(2):3.\nPaul A. Larson et al. 2014. Actionable findings\nand the role of IT support: report of the ACR\nActionable Reporting Work Group. J Am Coll\nRadiol, 11(6):552–558.\nWilson Lau, Kevin Lybarger, Martin L Gunn, and\nMeliha Yetisgen. 2022. Event-based clinical find-\ning extraction from radiology reports with pre-\ntrained language model. Journal of Digital Imag-\ning, pages 1–14.\nWilson Lau, Thomas H Payne, Ozlem Uzuner, and\nMeliha Yetisgen. 2020. Extraction and analysis of\nclinically important follow-up recommendations\nin a large radiology dataset. AMIA Summits on\nTranslational Science Proceedings, 2020:335.\nKahyun Lee, Nicholas J Dobbins, Bridget McInnes,\nMeliha Yetisgen, and Özlem Uzuner. 2021. Trans-\nferability of neural network clinical deidentifica-\ntion systems. Journal of the American Medical\nInformatics Association, 28(12):2661–2669.\nThusitha Mabotuwana, Christopher S Hall, Vadiraj\nHombal, et al. 2019. Automated tracking of follow-\nup imaging recommendations. American Journal\nof Roentgenology, 212(6):1287–1294.\nThusitha Mabotuwana, Christopher S Hall, Joel\nTieder, and Martin L. Gunn. 2018.\nImprov-\ning Quality of Follow-Up Imaging Recommen-\ndations in Radiology. AMIA Annu Symp Proc,\n2017:1196–1204.\nJiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Ji-\nayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng\nOuyang, and Daniel Rueckert. 2025. Medvlm-\nr1: Incentivizing medical reasoning capability of\nvision-language models (vlms) via reinforcement\nlearning. arXiv preprint arXiv:2502.19634.\nNamu Park, Kevin Lybarger, Giridhar Kaushik Ra-\nmachandran, Spencer Lewis, Aashka Damani,\nOzlem Uzuner, Martin Gunn, and Meliha Yetis-\ngen. 2024. A novel corpus of annotated medical\nimaging reports and information extraction re-\nsults using bert-based language models. arXiv\npreprint arXiv:2403.18975.\nMike Schuster and Kuldip K Paliwal. 1997. Bidirec-\ntional recurrent neural networks. IEEE transac-\ntions on Signal Processing, 45(11):2673–2681.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam\nJaroensri, Atilla Kiraly, Madeleine Traverse,\nTimo Kohlberger, Shawn Xu, Fayaz Jamil,\nCían\nHughes,\nCharles\nLau,\net\nal.\n2025.\nMedgemma technical report.\narXiv preprint\narXiv:2507.05201.\nThomas Sounack, Joshua Davis, Brigitte Durieux,\nAntoine Chaffin, Tom J Pollard, Eric Lehman, Al-\nistair EW Johnson, Matthew McDermott, Tristan\nNaumann, and Charlotta Lindvall. 2025. Bioclini-\ncal modernbert: A state-of-the-art long-context\nencoder for biomedical and clinical nlp. arXiv\npreprint arXiv:2506.10896.\nKaren Sparck Jones. 1972. A statistical interpre-\ntation of term specificity and its application in\nretrieval. Journal of documentation, 28(1):11–\n21.\nYan Xu, Junichi Tsujii, and Eric I-Chao Chang. 2012.\nNamed entity recognition of follow-up and time\ninformation in 20 000 radiology reports. Journal\nof the American Medical Informatics Association,\n19(5):792–799.\nMeliha Yetisgen-Yildiz, Martin L Gunn, Fei Xia, and\nThomas H Payne. 2011. Automatic identification\nof critical follow-up recommendation sentences\nin radiology reports. In AMIA Annual Symposium\nProceedings, volume 2011, page 1593. Ameri-\ncan Medical Informatics Association.\n"}]}