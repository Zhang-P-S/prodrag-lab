{"doc_id": "arxiv:2511.02531", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.02531.pdf", "meta": {"doc_id": "arxiv:2511.02531", "source": "arxiv", "arxiv_id": "2511.02531", "title": "Causal Graph Neural Networks for Healthcare", "authors": ["Munib Mesinovic", "Max Buhlan", "Tingting Zhu"], "published": "2025-11-04T12:34:46Z", "updated": "2026-01-26T20:20:40Z", "summary": "Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.02531v4", "url_pdf": "https://arxiv.org/pdf/2511.02531.pdf", "meta_path": "data/raw/arxiv/meta/2511.02531.json", "sha256": "e4142eae10107b6fd9071fafdb4f3bb80cac0ed8f83bde5c9ad6bea8611b6200", "status": "ok", "fetched_at": "2026-02-18T02:28:28.573969+00:00"}, "pages": [{"page": 1, "text": "Causal Graph Neural Networks for Healthcare\nMunib Mesinovic1*, Max Buhlan2 and Tingting Zhu1\n1*Department of Engineering Science, University of Oxford, Oxford, UK.\n2Faculty of Medicine, Leipzig University, Leipzig, Germany.\n*Corresponding author(s). E-mail(s): munib.mesinovic@eng.ox.ac.uk;\nContributing authors: max.buhlan@medizin.uni-leipzig.de; tingting.zhu@eng.ox.ac.uk;\nAbstract\nHealthcare artificial intelligence systems routinely fail when deployed across institutions, with docu-\nmented performance drops and perpetuation of discriminatory patterns embedded in historical data. This\nbrittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal\ngraph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability\nby combining graph-based representations of biomedical data with causal inference principles to learn\ninvariant mechanisms rather than spurious correlations. This Perspective examines methodological foun-\ndations spanning structural causal models, disentangled causal representation learning, and techniques\nfor interventional prediction and counterfactual reasoning on graphs. We analyse applications demon-\nstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping\nvia multi-omics causal integration, continuous physiological monitoring with mechanistic interpreta-\ntion, and drug recommendation correcting prescription bias. These advances establish foundations for\npatient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large\nlanguage models for hypothesis generation and causal graph neural networks for mechanistic validation.\nSubstantial barriers remain, including computational requirements precluding real-time deployment, val-\nidation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of\ncausal-washing where methods employ causal terminology without rigorous evidentiary support. We\npropose tiered frameworks distinguishing causally-inspired architectures from causally-validated discov-\neries and identify critical research priorities, including hierarchical causal discovery exploiting biological\nmodularity, integration of diverse data modalities, and regulatory pathways for systems making causal\nrather than purely associational claims. Success requires balancing theoretical ambition with empiri-\ncal humility, computational sophistication with clinical interpretability, and transformative vision with\nuncompromising validation standards.\nKeywords: causality, graphs, healthcare, interpretability, machine learning, time-series\n1 Introduction\nIn 2019, a widely deployed risk prediction algorithm for the allocation of healthcare resources systematically\nunderestimated the severity of the disease for Black patients by relying on healthcare costs as a proxy\nfor health needs, resulting in discrimination affecting millions of patients annually [1]. That same year, a\ndiabetic retinopathy screening system with specialist-level laboratory performance (>90% sensitivity) faced\ncritical failure when deployed in community clinics. The model struggled with environmental distribution\nshift, rejecting 21% of patient images as ungradable due to lighting and protocol differences rarely seen in\ntraining data, thereby creating workflow bottlenecks rather than improving care. These failures exemplify\nthe triple crisis of AI in healthcare: models collapse under distribution shift, perpetuate discriminatory\npatterns embedded in historical data, and struggle to provide reliable mechanistic insight [2, 3]. The paradox\nof pneumonia risk prediction starkly illustrates this brittleness: models learnt to classify asthma patients as\nlower risk because they received aggressive early treatment in training hospitals, a spurious correlation that\ncould lead to dangerous under-treatment when deployed elsewhere [4]. The question facing healthcare AI is\nno longer whether systems can achieve impressive accuracy in retrospective test sets, but whether they can\nsurvive clinical deployment across diverse institutions and maintain both effectiveness and equity.\n1\narXiv:2511.02531v4  [cs.LG]  26 Jan 2026\n"}, {"page": 2, "text": "This brittleness stems from a fundamental mismatch between what machine learning optimises and what\nhealthcare requires. Clinical decisions inherently require understanding causal mechanisms, will this drug\nhelp this patient? Would this patient have developed complications under alternative treatment?, yet cur-\nrent models learn statistical associations that may reflect institutional practices, equipment characteristics,\npatient selection, or confounding variables rather than biological reality [5, 6]. The COVID-19 pandemic\nexposed these limitations with devastating clarity, where predictive models trained on historical data failed\ncatastrophically when confronted with a novel pathogen and rapidly evolving clinical practices, highlighting\nthe urgent need for methods that generalise based on causal invariance rather than statistical regularities\n[7, 8]. The consequences extend beyond accuracy metrics to patient harm, as models that encode spuri-\nous correlations can recommend interventions effective in training environments but ineffective or harmful\nin deployment settings. Moreover, mounting evidence demonstrates that AI systems trained on historical\ndata perpetuate or amplify existing healthcare disparities, with purely statistical fairness metrics proving\ninsufficient to address discrimination transmitted along causal pathways [9–12].\nRandomised controlled trials remain the gold standard for estimating causal effects, yet they address\npopulation-average treatment effects under specific protocols rather than patient-specific responses, cannot\nexplore the combinatorial space of multi-drug interactions, and are infeasible for many clinically relevant\nquestions due to ethical, temporal, or cost constraints. Causal graph methods extend this inferential capacity\nby enabling counterfactual reasoning for individual patients, leveraging observational data where randomi-\nsation is impossible, and scaling causal analysis to high-dimensional molecular and physiological networks\nbeyond experimental reach.\nThe distinction between correlation and causation maps directly to Pearl’s Causal Hierarchy, which\norganises reasoning into three levels of increasing inferential power [13, 14]. Level 1 (Association) addresses\nobservational queries through conditional probabilities P(Y |X), answering “what is?” questions via pattern\nrecognition, the domain where standard machine learning excels. Level 2 (Intervention) concerns the effects\nof actions, formalised using the do-operator P(Y |do(X)), addressing “what if we do?” questions essential for\ntreatment planning. Level 3 (Counterfactual) queries alternative outcomes under hypothetical conditions,\nanswering “what would have been?” questions critical for personalised medicine and retrospective analysis.\nHealthcare demands information at Levels 2 and 3, predicting treatment responses, comparing therapeutic\nstrategies, and identifying causal disease mechanisms, yet standard models operate solely at Level 1, iden-\ntifying statistical patterns without understanding the causal structure [15]. The Causal Hierarchy Theorem\ndemonstrates that these levels form a strict hierarchy where information at higher levels cannot be derived\nfrom lower levels without additional causal assumptions [13]. This fundamental limitation explains why\npurely associational machine learning models, regardless of architectural sophistication or scale of training\ndata, fail to answer interventional and counterfactual questions essential for robust clinical decision-making.\nBiomedical systems naturally form networks, molecular interactions, brain connectivity, metabolic path-\nways, protein complexes, and disease comorbidity patterns, making graph representations the natural\nframework for encoding biological relationships [16, 17]. Graph neural networks extend traditional graph\nanalysis beyond fixed topological measures by learning representations directly from graph-structured data\nthrough iterative message passing, where nodes aggregate information from neighbours via learnable neural\ntransformations that capture both local motifs and global network properties [18–20]. This learning paradigm\nhas demonstrated substantial advantages over handcrafted graph features in biomedical applications, from\nmolecular property prediction on chemical graphs to psychiatric diagnosis from brain connectivity networks\n[21–23]. Yet standard GNNs inherit supervised learning’s fundamental limitation: they optimise predictive\nperformance by exploiting any statistical pattern in training data, whether reflecting genuine biological\nmechanisms or spurious correlations from confounding, selection bias, or batch effects, treating all network\nedges equally regardless of underlying causal structure [24, 25].\nThe convergence of causal inference with graph neural networks addresses this challenge. Recent the-\noretical advances in causal representation learning provide a path forward.By explicitly modelling causal\nstructures within graph architectures, we can develop models that identify genuine therapeutic targets,\npredict treatment responses under interventions never observed in training data, and discover disease mech-\nanisms rather than spurious correlations [26, 27]. This convergence is particularly timely given the four\nconcurrent developments. First, the accumulation of multi-modal biomedical data, from single-cell sequenc-\ning that captures 20,000+ gene expression profiles to continuous physiological monitoring that generates\nterabytes of temporal dynamics, provides unprecedented resolution for causal discovery. Datasets now cap-\nture interventional outcomes and temporal dependencies that were previously unavailable [28–30]. Second,\nregulatory frameworks increasingly demand explainable AI in healthcare, which requires not merely predic-\ntions, but a mechanistic understanding that clinicians can verify against domain expertise and biological\nknowledge [31, 32]. Third, documented failures of correlation-based models under distribution shift have cre-\nated urgency for methods achieving robust generalisation through causal invariance. Fourth, the integration\nof causal inference with fairness constraints enables rigorous identification and mitigation of discrimination\ntransmitted along specific causal pathways, providing tools for distinguishing legitimate predictive factors\n2\n"}, {"page": 3, "text": "from discriminatory proxies, a distinction central to anti-discrimination law yet absent from traditional\nmachine learning approaches [11, 33].\nThis Perspective examines causal graph neural networks, methods that integrate causal inference prin-\nciples with graph-based deep learning architectures, as a methodological framework for transforming AI\nin healthcare from reactive pattern recognition to proactive causal medicine. Unlike previous papers that\naddressed GNNs in healthcare [34] or causal inference more broadly [14, 35] separately, we focus specifi-\ncally on their integration, methods in which both graph structure and causal reasoning prove essential for\nclinical impact. We analyse how causal principles enhance GNN architectures for healthcare tasks, mov-\ning beyond correlation to mechanism, and examine current applications that demonstrate clinical utility\nacross psychiatric diagnosis, cancer treatment, physiological monitoring, and drug discovery. We also iden-\ntify fundamental challenges in scaling these methods to real-world deployment, including computational\ncomplexity, validation without ground truth, fairness under distribution shift, and regulatory considera-\ntions. Our analysis reveals both transformative promise and substantial barriers: while CIGNNs achieve\nsuperior generalisation across healthcare institutions, identify biologically plausible mechanisms, and main-\ntain fairness properties under distribution shift, they face computational requirements precluding real-time\napplication, difficulty validating causal claims from observational data, and gaps between methodological\nsophistication and clinical interpretability.\nThe result of this research is the development of patient-specific Causal Digital Twins, dynamic computa-\ntional models built on causal GNN frameworks that enable clinicians to perform in silico experiments before\nclinical intervention. The digital twin paradigm has emerged as a transformative framework for personalised\nmedicine, offering dynamic computational representations of individual patients that evolve through contin-\nuous data integration [36, 37]. Recent conceptualisation identifies four essential capabilities that distinguish\nmedical digital twins from conventional simulation models: explainability through mechanistic rather than\nblack-box architectures, intervenability enabling virtual testing of therapeutic scenarios, learnability allowing\npatient-specific model refinement over time, and diversability supporting uncertainty quantification through\nensemble approaches [36]. Early clinical successes, notably the artificial pancreas for type 1 diabetes man-\nagement, demonstrate feasibility within circumscribed physiological systems [38, 39]. However, extending\ndigital twins to complex multisystem diseases requires methodological foundations that current data-driven\napproaches cannot provide. Causal graph neural networks could supply the missing theoretical infrastruc-\nture. Structural causal models enable the interventional reasoning essential for virtual treatment testing,\ncounterfactual computation supports patient-specific outcome prediction, and invariant causal learning can\npotentially ensure robust generalisation across healthcare contexts. The integration of causal inference with\ngraph-based patient modelling thus can allow us to transform digital twins from conceptual aspiration\nto methodologically grounded clinical tools. Imagine a clinician treating advanced cancer who could load\nthe patient’s multi-omics profile, brain imaging, and clinical history into such a system, then simulate\nfive drug combinations to predict effects on specific tumour pathways, toxicity risks, and progression-free\nsurvival, identifying optimal personalised therapy before administering a single dose. This vision, while\nambitious, rests on methodological foundations now being established through causality-inspired architec-\ntures for interventional prediction, counterfactual generation, and robust multi-modal integration. Realising\nthis potential requires not only technical innovation but fundamental rethinking of how we evaluate and\ndeploy AI in medicine. We need to shift from predictive accuracy on retrospective test sets to causal valid-\nity under prospective deployment, from statistical fairness metrics to interventional equity guarantees, and\nfrom black-box pattern recognition to mechanistic interpretability verified against biological knowledge.\nSuccess requires coordinated advances across scalable causal discovery algorithms, validation frameworks\nsuited to healthcare’s observational constraints, integration strategies for multi-modal multi-scale biological\ndata, and regulatory pathways for systems making causal rather than purely associational claims. The path\nfrom proof-of-concept demonstrations to clinical implementation is neither straight nor simple, demanding\ncollaboration between computational scientists, clinicians, biomedical researchers, and policymakers united\nby the recognition that healthcare AI’s next decade will be defined by which approaches achieve not just\nimpressive metrics but genuine clinical trust through mechanistic understanding.\nCausal graph neural networks address these fundamental limitations through principled causal inference\nframeworks (Fig. 1). By explicitly modelling causal structures within graph architectures, these meth-\nods identify invariant biological mechanisms rather than spurious correlations, enabling robust deployment\nacross heterogeneous clinical settings and supporting mechanistic treatment optimisation through counter-\nfactual reasoning. This Perspective proceeds in three stages. Section 2 establishes the theoretical foundations\nconnecting structural causal models to graph neural network architectures. Section 3 presents method-\nological advances organised by clinical capability, disentanglement, intervention prediction, counterfactual\ngeneration, robustness, and fairness, providing the technical building blocks. Section 4 demonstrates clinical\nimpact, showing how these methods translate to diagnosis, prognosis, treatment selection, and monitoring\nacross healthcare domains. Section 5 synthesises these advances toward Causal Digital Twins, articulating\nhow the methodological components combine to enable patient-specific in silico experimentation. Section\n3\n"}, {"page": 4, "text": "Fig. 1: Clinical failure modes of traditional machine learning and solutions through causal graph neu-\nral networks. A, Clinical failure modes demonstrate fundamental limitations of correlation-based models\nin healthcare deployment. A.1, Vulnerability to distribution shift: a diabetic retinopathy screening sys-\ntem achieves 94% accuracy at the training institution (Hospital A) but performance collapses to 73% at\ndeployment sites (Hospital B) due to spurious correlations with site-specific imaging protocols and patient\ndemographics rather than causal disease mechanisms. Feature distributions show that spurious features (red,\nsite-specific) undergo a distributional shift, while causal features (blue, disease markers) remain invariant\nacross environments. A.2, Lack of mechanistic insight: traditional models operate as black boxes, learning\nassociations between age, treatment, and comorbidity without identifying the underlying causal pathways,\nprecluding clinical interpretability and mechanistic validation. Question marks indicate uncertain causal\nrelationships that cannot be disentangled from observational data alone. A.3, Counterfactual blindness:\nAssociational models trained on observational data (Treatment A, 87% five-year survival) cannot simu-\nlate alternative treatment scenarios (Treatments B and C) marked with prohibition symbols, as they lack\nthe structural causal knowledge required for interventional reasoning, which is essential to personalised\ntreatment selection. B, Causal GNN solutions address these limitations through principled causal infer-\nence. B.1, Causal invariance learning: multi-environment optimisation identifies stable causal relationships\n(blue nodes with shield protection) whilst suppressing environment-specific spurious correlations (red nodes,\nfaded), ensuring robust generalisation under distribution shift through invariant risk minimisation. B.2,\nCausal digital twin for counterfactual generation: integration of multi-modal patient data (shown as con-\ncentric rings) constructs patient-specific structural causal models encoding disease mechanisms. Application\nof the do-operator enables simulation of unobserved treatment scenarios, generating counterfactual predic-\ntions (Treatment B: 73%, Treatment C: 95% five-year survival) through interventional inference rather than\nrequiring empirical observations, facilitating treatment optimisation for individual patients.\n6 addresses the substantial barriers, computational, validation, and translational, that must be overcome\nfor clinical deployment. We note that Sections 2–4 primarily synthesise existing methods and empirical evi-\ndence, while Section 5 articulates an aspirational framework, Causal Digital Twins, whose full realisation\nremains prospective. Section 6 addresses the substantial methodological, computational, and translational\nbarriers separating current capabilities from this vision.\n2 The causal graph framework\n2.1 Why biological systems demand graph-structured causal models\nBiological systems exhibit causal relationships organised as networks across multiple scales of organisation.\nAt the molecular level, gene regulatory networks encode transcriptional control in which regulatory proteins\ncausally influence the expression of target genes, with modern single-cell RNA sequencing that captures\nthe expression profiles of 20,000+ genes simultaneously [28]. Protein-protein interaction networks repre-\nsent physical binding relationships that mediate cellular signalling, with causal perturbations propagating\nthrough these networks to generate phenotypic responses [16]. At the tissue level, brain networks comprise\n4\n"}, {"page": 5, "text": "hundreds of regions with functional and structural connectivity encoding causal influences between neu-\nral populations, where activity in one region can drive or modulate activity in connected regions [40]. At\nthe population level, disease comorbidity networks represent relationships where large-scale data mining of\nelectronic health records and genetic association data are integrated into knowledge graphs to reveal how\nconditions are linked through shared molecular features [17]. This inherent graph structure makes neural\nnetwork architectures that explicitly represent and reason about network topology the natural framework\nfor biomedical machine learning.\nThe mathematical foundation connecting causal inference to graph representations is based on Struc-\ntural Causal Models (SCMs), which support a rigorous formalisation of cause-effect relationships [13]. An\nSCM M := ⟨U, V, F, P(U)⟩specifies exogenous variables U determined outside the model, endogenous vari-\nables V defined within the model, structural equations F = {f1, . . . , fm} where each Vi = fi(pa(Vi), Ui)\nwith pa(Vi) denoting the parent set of Vi, comprising all variables that directly cause Vi in the causal\ngraph, and probability distribution P(U) over exogenous variables [41]. This formulation naturally induces\na directed acyclic graph G(M) where nodes represent variables and directed edges encode functional depen-\ndencies through the parent sets pa(Vi). The joint distribution factorises according to the Markov property:\nP(V1, . . . , Vm) = Qm\ni=1 P(Vi|pa(Vi)) [13]. Interventions in this framework correspond to “graph surgery,”\nmodifying the causal graph by removing incoming edges to intervened variables and setting their values, fun-\ndamentally altering the generative process [13, 42]. This graph-theoretic representation of causation aligns\nprecisely with the message-passing architecture of graph neural networks, enabling implementation of causal\nreasoning through differentiable neural operations.\nA critical modelling choice underlying any causal graph concerns the semantic definition and resolution of\nnodes [43, 44]. In biomedical applications, nodes may represent entities at vastly different levels of abstraction\nsuch as individual genes versus pathways, single neurons versus brain regions, specific metabolites versus\nmetabolic modules. This granularity is not merely a technical convenience but fundamentally determines\nwhich causal relationships can be identified and what interventions the graph implies [45]. Causal structure\ndiscovered at one resolution may not transfer to finer or coarser scales, as relationships that hold between\nbrain regions might decompose into opposing sub-regional effects, while molecular interactions may aggregate\ninto emergent pathway-level causation invisible at the gene level. Throughout this Perspective, we explicitly\nnote the resolution at which different methods operate, as this choice carries direct consequences for both the\nbiological interpretation of discovered causal structures and the experimental designs capable of validating\nthem.\nA related question concerns the epistemic status of existing biological network representations. Gene reg-\nulatory networks encode regulatory relationships that carry causal interpretation, perturbing transcription\nfactor X should alter expression of target gene Y. However, GRNs inferred from observational expression\ndata may conflate correlation with causation, with edges reflecting shared upstream regulators or cellular\nstate confounding rather than direct regulation. Recent large-scale benchmarking on single-cell perturba-\ntion data reveals that even state-of-the-art inference methods struggle to recover true causal relationships\nwhen validated against interventional ground truth [46]. Systems biology models, comprising ordinary dif-\nferential equations encoding metabolic and signalling dynamics, occupy different epistemic ground. These\nmodels encode mechanistic hypotheses derived from biochemical knowledge, enzyme kinetics, binding affini-\nties, reaction stoichiometry, and thus represent causal mechanisms more explicitly. Yet parameters fitted\nto observational data may encode environment-specific rather than invariant relationships. Protein-protein\ninteraction networks used by methods such as PDGrapher as ”causal graph approximations” [47] contain\nboth direct causal interactions and indirect associations through protein complexes. We advocate treat-\ning biological networks as candidate causal structures requiring validation through interventional evidence,\nnot established ground truth. The CiGNN framework exemplifies this by using causal discovery algorithms\nto identify genuinely causal physiological relationships rather than assuming database networks encode\ncausation [48].\nStandard GNNs, although naturally suited to graph-structured data, learn to exploit whatever statisti-\ncal patterns maximise predictive performance regardless of causal validity or biological plausibility. In brain\nimaging, global arousal states create spurious correlations between all regions that GNNs readily exploit; in\nmolecular networks, batch effects create artificial associations between unrelated proteins; in clinical data,\ntreatment selection bias generates correlations between drug prescriptions and outcomes that reflect physi-\ncian decision-making rather than therapeutic efficacy [24, 25]. These spurious patterns often provide a strong\npredictive signal in training environments, but fail catastrophically under distribution shift, precisely the\nbrittleness observed in clinical deployment. Causality-inspired GNN architectures address this through archi-\ntectural constraints, training objectives, and inductive biases that direct learning towards invariant causal\nrelationships rather than environment-specific correlations. The DisC framework, for example, employs dual\nencoders to explicitly disentangle causal features that consistently predict outcomes across environments\nfrom spurious features that correlate with outcomes only in specific settings [49]. The CI-GNN architecture\n5\n"}, {"page": 6, "text": "for brain networks uses conditional mutual information from a Granger causality perspective to identify con-\nnections representing genuine neural communication rather than shared global fluctuations [49, 50]. These\napproaches transform GNNs from being powerful function approximators that learn arbitrary predictive\npatterns to becoming mechanistic models that capture biologically grounded causal relationships.\n2.2 From association to intervention to counterfactuals\nStandard supervised learning excels at associational queries, given covariates X, predict the outcome Y\nby learning P(Y |X) from observational data. This suffices for many pattern recognition tasks: identifying\nmalignant tumours from images, predicting patient deterioration from vital signs, or classifying disease\nsubtypes from gene expression. However, clinical decision-making fundamentally requires interventional\nreasoning: what happens if we administer treatment T? This requires estimating P(Y |do(T)), which differs\nfrom P(Y |T) whenever confounding exists, and confounding pervades observational healthcare data through\nindication bias, selection effects, and unmeasured factors [13, 14].\nGraph-structured causal models enable interventional prediction through Pearl’s do-calculus, which pro-\nvides three rules for transforming interventional distributions into estimable observational quantities under\nspecified graph conditions (Box 1) [13]. Rule 1 allows ignoring observations on variables that don’t affect the\noutcome through paths independent of the intervention. Rule 2 permits converting interventions to obser-\nvations when appropriate conditional independence holds. Rule 3 enables removing interventions when they\ndo not affect the outcome through certain graph paths. The interventional Variational Graph Auto-Encoder\n(iVGAE) implements these operations through differentiable neural layers, incorporating interventional\ntransformations in both encoder and decoder networks to ensure that the learnt representations encode\ncausal rather than associational relationships [42]. When the model identifies a connection between two pro-\nteins or two brain regions, it learns whether this connection would persist under interventions, whether it\nrepresents a causal pathway or a spurious correlation.\nCounterfactual reasoning operates at the highest level of Pearl’s hierarchy, addressing patient-specific\nquestions: would this patient have developed complications if we had chosen alternative treatment? Rea-\nsoning of this kind is essential for precision medicine, treatment optimisation, and retrospective analysis of\nclinical decisions [13]. The calculation of counterfactuals requires a three-step process, including abduction\n(inferring unobserved variables, including patient-specific factors, from observed data), action (modifying\nthe causal model to reflect alternative intervention), and prediction (computing outcomes under the modified\nmodel) [41]. The CLEAR framework implements counterfactual generation for graphs through variational\nautoencoders that learn distributions over causally plausible graph modifications. This could allow, for\nexample, the exploration of alternative biological configurations with some mechanistic validity [51]. In can-\ncer treatment planning, this makes it possible to simulate how different therapeutic targets would affect\ntumour evolution, identifying vulnerabilities that might not be apparent from observational data alone.\nThroughout this Perspective, we distinguish methods by the strength of their causal claims: causally-\ninspired architectures that employ causal concepts without explicit causal claims, causally-structured\nmethods that estimate causal quantities under stated assumptions, and causally-validated approaches whose\nclaims are corroborated through multi-modal evidence (see Section 6.3 for formal definitions).\nPatient-specific Causal Digital Twins, dynamic computational models where clinicians perform in silico\nexperiments to test interventions before clinical administration could integrate the multi-omics profile of\na patient (genomics, transcriptomics, proteomics), longitudinal imaging that captures organ structure and\nfunction, clinical history including treatments and responses, and knowledge graphs encoding established\nbiological mechanisms [28, 29]. The underlying causal GNN architecture would learn patient-specific param-\neterisations of general biological mechanisms, enabling simulation of therapeutic interventions through the\ndo-operator framework by setting treatment variables, propagating effects through learnt causal pathways,\nand predicting outcomes at molecular, cellular, and phenotypic levels. For an oncology patient, the digital\ntwin might simulate five drug combinations, predicting effects on specific oncogenic pathways, normal tissue\ntoxicity, and progression-free survival, thereby identifying optimal personalised therapy before administer-\ning a single dose. For a neuropsychiatric patient, it could simulate neuromodulation protocols, predicting\nwhich brain circuits would be affected and how symptoms would respond. This vision, while ambitious,\nrests on methodological foundations now being established of GNN architectures for interventional predic-\ntion, counterfactual generation frameworks for exploring alternative scenarios, invariant learning methods\nensuring robust generalisation, and multi-modal integration strategies combining diverse data modalities\nthrough shared causal structure [26, 52]. Figure 2A illustrates these theoretical distinctions, depicting the\nconceptual framework through which traditional graph neural networks may achieve comparable associa-\ntional performance but are expected to exhibit substantial degradation for interventional and counterfactual\nqueries essential for clinical decision-making.\n6\n"}, {"page": 7, "text": "Box 1: Formal foundations of causal graph neural networks\nStructural Causal Models\nAn SCM M := ⟨U, V, F, P(U)⟩defines the data-generating process, where exogenous variables U =\n{U1, . . . , Un} represent unmodelled background factors, endogenous variables V = {V1, . . . , Vm} are\ndefined by structural equations Vi = fi(pa(Vi), Ui), and P(U) governs exogenous randomness [13].\nThe induced causal graph G(M) contains directed edges Vj →Vi for each Vj ∈pa(Vi).\nInterventions and the do-operator\nIntervention do(X = x) modifies the SCM by replacing the structural equation for X with X := x,\nremoving incoming edges in the graph. The post-intervention distribution P(Y |do(X = x)) differs\nfrom observational P(Y |X = x) whenever confounding exists [13].\nGraph neural network implementation\nGNNs implement message-passing: h(l+1)\ni\n= σ\n\u0010\nW(l)h(l)\ni\n+ P\nj∈N (i) M(l)h(l)\nj\n\u0011\n, where h(l)\ni\nis the rep-\nresentation of node i at layer l, N(i) are neighbours, and W, M are learnable parameters. This\narchitecture naturally aligns with causal graphs when edges represent causal relationships and\nmessage-passing implements causal influence propagation [42].\nCausal identifiability\nP(Y |do(X)) is identifiable if it can be expressed in terms of the observational distribution P(V )\ngiven the causal graph G. The do-calculus provides three rules for such expressions [13]:\nP(Y |do(X), Z, W) = P(Y |do(X), W) if Y ⊥GX Z|X, W\n(1)\nP(Y |do(X), do(Z), W) = P(Y |do(X), Z, W) if Y ⊥GXZ Z|X, W\n(2)\nP(Y |do(X), do(Z), W) = P(Y |do(X), W) if Y ⊥GXZ(W ) Z|X, W\n(3)\nwhere GX denotes the graph with edges into X removed, GZ has edges out of Z removed, and\n⊥indicates d-separation. Rule 1 permits ignoring observations on variables that do not affect the\noutcome through paths independent of the intervention; Rule 2 allows converting interventions to\nobservations when appropriate conditional independence holds; Rule 3 enables removing interventions\nwhen they do not influence the outcome through certain graph paths.\nCounterfactual computation\nFor unit u and intervention X = x, the counterfactual outcome Yx(u) is computed via: (1) Abduction:\ninfer U = u from observed data; (2) Action: modify SCM to Mx with X := x; (3) Prediction:\nevaluate Y under Mx with U = u [41]. This enables patient-specific predictions unavailable from\npopulation-level interventional distributions.\n3 Methodological advances\nThe translation of causal theory into practical healthcare applications requires architectures that address\nspecific clinical capabilities. We organise methodological advances by the clinical capabilities they enable,\ndisentangling genuine causal mechanisms from confounding, predicting interventional effects without\nexperimental data, generating patient-specific counterfactuals, achieving robust generalisation across het-\nerogeneous settings, and ensuring fairness through causal rather than purely statistical criteria, discussing\nwithin each capability the technical approaches that address it. From a complementary technical perspective,\nmethods reviewed here rest on distinct theoretical foundations that determine their causal assumptions and\nevidentiary requirements. Intervention-invariance methods (Sections 3.1, 3.4) assume that causal features\nproduce stable predictions across environments while spurious correlations shift with context; these require\nmulti-environment data but not necessarily explicit causal graphs. Structural causal methods (Sections 3.2,\n3.3) implement do-calculus through neural architectures, requiring explicit graph specification, causal suf-\nficiency, and faithfulness assumptions; these provide formal identification guarantees but demand stronger\nprior knowledge. Knowledge-guided methods incorporate biomedical priors, protein-protein interactions, gene\nregulatory networks, anatomical atlases, as structural constraints without formal causal identification; these\nleverage domain expertise but inherit the epistemic status of their knowledge sources, which may conflate\ncorrelation with causation (Section 2.1). Table 2 maps individual methods to their specific assumptions\nwithin this taxonomy.\n3.1 Disentangling causal signal from confounding\nBiomedical datasets are rife with spurious correlations that mislead standard machine learning models.\nStandard GNNs, although naturally suited to graph-structured data, learn to exploit whatever statistical\n7\n"}, {"page": 8, "text": "Fig. 2: Conceptual framework illustrating theoretical advantages of causal over associational inference\nacross Pearl’s causal hierarchy. A, Theoretical dissociation between associational and interventional perfor-\nmance predicted by causal hierarchy theory. The “causal gap” quantifies expected performance loss when\nmodels trained on observational data are applied to interventional queries, reflecting the fundamental limi-\ntation that associational patterns may not persist under intervention. B, Idealised robustness to simulated\nunmeasured confounding. Conceptual curves illustrate the theoretical expectation that traditional methods\ndegrade more steeply as unmeasured confounder strength increases, while causal architectures that explic-\nitly model confounding structure are expected to maintain robustness. Confounding strength parameterised\nas maximum risk ratio for unmeasured confounder associations with treatment and outcome following sen-\nsitivity analysis frameworks [53]. Shaded regions represent idealised uncertainty bands illustrating expected\nvariance patterns. Causally-Inspired GNN (Tier 1) denotes architectures employing causal concepts such\nas invariance regularisation to improve robustness without explicit causal claims. Causally-Validated GNN\n(Tier 3) denotes methods whose causal claims are corroborated through multi-modal evidence triangula-\ntion. See Section 6.3 for formal tier definitions.\npatterns maximise predictive performance regardless of causal validity or biological plausibility. In brain\nimaging, global arousal states create spurious correlations between all regions that GNNs readily exploit\n[54]; in molecular networks, batch effects create artificial associations between unrelated proteins [55]; in\nclinical data, treatment selection bias generates correlations between drug prescriptions and outcomes that\nreflect physician decision-making rather than therapeutic efficacy [24, 25]. The challenge is to decompose\nobserved graphs into causal and spurious components, a problem addressed through information-theoretic\ndisentanglement that maximises mutual information between causal features and outcomes and minimises\nspurious correlations [56].\nThe DisC (Debiasing via Disentangled Causal Substructure) framework operationalises this separation\nthrough a dual-encoder architecture [49]. Rather than treating graphs monolithically, DisC maintains parallel\nGNN encoders: one trained to extract causal features that consistently predict outcomes across different\nenvironments, another capturing environment-specific spurious patterns. The contrastive learning objective\nenforces the independence between these representations, ensuring that causal and spurious features remain\ndisentangled. For psychiatric diagnosis, this architectural separation was proven to be immensely effective.\nTraditional GNNs identified widespread hyperconnectivity in the cerebellar regions of depressed patients,\na finding inconsistent with the clinical understanding that depression primarily influences affective rather\nthan motor circuits.\nThe CI-GNN (Causality-Inspired Graph Neural Network) advances this concept through Granger\ncausality-inspired conditional mutual information, providing principled measures of causal strength for\ngraph edges [50]. The model identifies influential subgraphs that demonstrate genuine causal connec-\ntions through disentangled subgraph-level representation learning. CI-GNN’s causal decomposition correctly\nidentified sparse cerebellar involvement while highlighting strong connections around the left rectus, a\nregion previously identified as critical for antidepressant treatment response prediction [50]. Evaluated on\nthe REST-meta-MDD dataset (n=1,604), this alignment with established neurobiology demonstrates that\ncausal disentanglement provides mechanistically meaningful insights, not merely improved prediction. In\nautism spectrum disorder evaluations on the ABIDE dataset, CI-GNN revealed tight connections within the\nsomatomotor network consistent with motor impairments characteristic of ASD, findings replicated across\nindependent cohorts and suggesting genuine causal relationships rather than dataset-specific artefacts [50].\n8\n"}, {"page": 9, "text": "Fig. 3: Canonical causal graph neural network architectures illustrated for conceptual brain connectiv-\nity analysis. While the underlying methods were developed on synthetic benchmark datasets, we illustrate\ntheir application to functional neuroimaging to demonstrate translation potential for clinical contexts. A,\nThe DisC (Debiasing via Disentangled Causal Substructure) framework [49] achieves causal disentangle-\nment through a dual-encoder architecture. An edge mask generator partitions input graphs into causal and\nspurious subgraphs, with separate GNN modules encoding each into disentangled representations. The loss\ncomprises two components: LCE, a weighted cross-entropy loss that up-weights samples where the bias\nGNN struggles, directing the causal encoder toward genuinely predictive features; and LGCE, a generalised\ncross-entropy loss that amplifies gradients on easily-classified samples, directing the bias encoder toward\nenvironment-specific shortcuts. Causal features (ZC.1–ZC.4) produce stable predictions across environments\nwhile spurious features (ZS.1–ZS.3) are explicitly identified and discarded. B, The iVGAE (interventional\nVariational Graph Auto-Encoder) framework [42] implements Pearl’s do-operator through differentiable\nneural layers. The interventional encoder maps observed graphs to a causal latent space Z, while the inter-\nventional decoder incorporates do-layers that simulate graph surgery. The output P(D|do(X)) represents\nthe interventional distribution of downstream variables D given intervention do(X = x), enabling prediction\nof effects that persist under manipulation rather than spurious correlations reflecting confounding. Applied\nto brain networks, such architectures could distinguish genuine neural communication pathways from global\narousal-induced correlations, identifying intervention targets for neuromodulation or pharmacotherapy.\nThe clinical imperative was that clinicians needed to know which connections to target through neurostimu-\nlation, pharmacotherapy, or behavioural interventions, not merely that connectivity is abnormal. Although\ndeveloped for other applications, IGCL-GNN extends these principles through multi-objective optimisa-\ntion that simultaneously enhances causal features and suppresses environment-specific spurious patterns,\npotentially being transferrable to help with robust clinical deployment which requires balancing predictive\nperformance on current datasets with generalisation to unseen environments [56].\nThese findings depend on the chosen parcellation resolution. CI-GNN operates on 116-node brain\natlases where each node aggregates thousands of neurons; causal relationships at this mesoscale resolution\nreflect functional circuit organisation rather than individual neuronal connectivity, with direct implications\nfor which neurostimulation or pharmacological interventions could target the identified causal pathways.\nFigure 3 illustrates these architectural principles applied to functional neuroimaging data. While the DisC\nand iVGAE frameworks were originally developed on synthetic benchmark datasets, their conceptual trans-\nlation here to brain connectivity analysis could demonstrate how causal disentanglement and interventional\nprediction could address the specific confounding challenges in clinical neuroimaging, separating genuine\nneural circuit dysfunction from global arousal states, scanner artefacts, or site-specific acquisition protocols.\n9\n"}, {"page": 10, "text": "3.2 Predicting interventional effects without experimental data\nPrecision medicine requires not just prediction, but mechanistic explanation, identifying the specific bio-\nlogical features driving a diagnosis. The RC-Explainer (Reinforced Causal Explainer) uses reinforcement\nlearning to discover which edges of a graph to retain to maximise the causal information flow [57]. Rather\nthan computing interventions symbolically, it learns a policy for selecting explanatory subgraphs, with\neach candidate evaluated not by correlation but by causal impact. In molecular property prediction, this\napproach identified causal functional groups (e.g., toxicophores) determining mutagenicity, distinguishing\nactive substructures from inert scaffolds, mechanistic insights missed by correlation-based methods that\ncannot distinguish causal features from spurious environmental associations [57].\nFor blood pressure monitoring, the CiGNN framework demonstrates the clinical utility of interventional\nprediction by first using the neural-accelerated causal discovery (FCI algorithm) to identify physiological\nsignals causally related to blood pressure variation, then constructing spatiotemporal GNN architectures\nguided by the discovered causal structure [48, 58]. This causal feature selection reduces dimensionality\nfrom 222 initial wearable features to a sparse set of direct and indirect causal indicators while improving\nboth accuracy and interpretability, identifying, for example, that specific PPG morphological features (e.g.,\npulse arrival time, waveform characteristics) causally influence blood pressure through arterial stiffness\nmechanisms.\nThe iVGAE (interventional Variational Graph Auto-Encoder) provides theoretical grounding for inter-\nventional GNNs by incorporating interventional layers in both encoder and decoder networks, ensuring that\nlearnt representations explicitly encode causal rather than merely associational relationships [42]. When the\nmodel identifies a connection between proteins or brain regions, it learns whether this connection repre-\nsents a causal pathway that would persist under intervention rather than a spurious correlation arising from\nconfounding. For drug discovery, PDGrapher leverages protein-protein interaction networks as approxima-\ntions of causal graphs, operating under causal sufficiency assumptions to predict combinatorial therapeutic\ntargets [47]. Evaluated on LINCS L1000 chemical perturbation data across nine cell lines (A549, MCF7,\nPC3, HT29, A375, and others), PDGrapher outperformed competing methods by 4.5–13.4% in detecting\nground-truth targets, demonstrating that even imperfect causal structures, PPI networks contain both direct\ncausal effects and indirect associations, provide superior guidance compared to purely correlational feature\nselection.\n3.3 Generating counterfactuals for personalised medicine\nCounterfactual reasoning addresses the highest level of Pearl’s causal hierarchy: what would have hap-\npened to this patient under alternative treatment? Such patient-specific predictions are essential for\nprecision medicine, treatment optimisation, and retrospective analysis of clinical decisions [13]. The CLEAR\n(Counterfactual Learning on Graphs) framework approaches counterfactual generation through variational\nautoencoders that learn distributions over possible graph modifications [51]. Rather than generating arbi-\ntrary alternative graphs, CLEAR learns which modifications are causally plausible, which edges could\nchange, maintaining validity relative to the data distribution. In molecular discovery, this leads to explo-\nration of how structural modifications would affect molecular properties, identifying functional groups that\ndrive toxicity or efficacy, vulnerabilities invisible in observational data alone [51].\nA challenge in counterfactual reasoning is that interventions can create graph configurations far outside\nthe training distribution. When simulating disruption of a critical hub protein, we can query a configuration\nlikely never appearing naturally, and thus unrepresented in training data. Traditional neural networks can\nfail potentially under such a distribution shift. For epileptic seizure prediction, dynamic graph models like\nDIGLN capture the temporal evolution of brain networks [59], future extensions could enable in silico testing\nof interventions, simulating how disrupting specific neural circuits would affect seizure propagation patterns,\nguiding neurosurgical planning or neurostimulation protocols.\nThe CXGNN (Causal Explainer based on GNN-Neural Causal Models) framework provides a rigorous\nquantification of counterfactual effects through neural implementations of structural causal models [60]. By\nparameterising each structural equation as a neural network, CXGNN learns arbitrary non-linear causal\nrelationships and stays true to the formal guarantees of SCM-based counterfactual computation. For mild\ncognitive impairment progression, CXGNN identified that hypertension, arrhythmia, and sex emerged as\ncritical causal factors, with notable sex-specific differences: female patients showed stronger causal links\nbetween vascular factors and cognitive decline, while male patients exhibited more pronounced metabolic\neffects [60, 61]. These mechanistic insights enable personalised risk assessment and targeted early interven-\ntion strategies. These counterfactual generation methods form the computational engine of Causal Digital\nTwins, the ability to simulate patient-specific responses to interventions never administered in reality\nconstitutes the core functionality enabling in silico clinical experimentation.\n10\n"}, {"page": 11, "text": "3.4 Achieving robustness across heterogeneous clinical settings\nWhile disentanglement methods (Section 3.1) achieve robustness by explicitly separating causal from spu-\nrious representations, invariant learning approaches pursue the same objective through a complementary\nmechanism: optimisation constraints that enforce feature stability across environments without requiring\nexplicit architectural separation. Disentanglement provides interpretable causal-spurious decomposition and\ninvariant learning provides deployment guarantees with potentially simpler architectures. Some methods,\nsuch as IGCL-GNN, integrate both principles.\nHealthcare AI deployment requires models that generalise across institutions with different patient demo-\ngraphics, clinical protocols, and measurement technologies, precisely the distribution shift conditions where\ncorrelation-based models fail catastrophically. The fundamental principle underlying invariant learning is\nthat causal relationships remain stable across contexts while spurious correlations change (Figure 2B) [62].\nIn medical terms, the biological mechanisms by which drugs treat diseases should be consistent across pop-\nulations, even though confounding factors such as demographics, comorbidities, and healthcare practices\nvary widely. This principle provides a powerful criterion for distinguishing causal from spurious patterns\nthat hold across diverse environments and are likely causal.\nThe IGCL-GNN framework relies on invariant causal learning through multi-objective optimisation\nthat enhances causal features and suppresses environment-specific spurious patterns [56]. Though devel-\noped for industrial sensor networks, it provides a template for robust clinical deployment requires balancing\ncompeting objectives such as maximising predictive performance on current datasets and also ensuring gen-\neralisation to unseen environments. In multi-site brain imaging studies, CI-GNN achieved mean accuracy of\n0.71–0.73 across 17 acquisition sites compared to 0.64–0.69 for baseline methods, with consistent improve-\nments in 13 of 17 sites despite individual site accuracy ranging from 0.60 to 0.85, demonstrating the value\nof learning causal invariances rather than site-specific correlations [49, 50]. This stability under distribution\nshift is not just a technical achievement, but a prerequisite for clinical trust, models that collapse when\nmoved between hospitals cannot be deployed safely.\nThe CRec (Conditional Causal Representation Learner) framework addresses the robustness of drug rec-\nommendation through disentanglement of therapeutic effects from prescription patterns [63]. For example,\nproton pump inhibitors might appear beneficial for cardiovascular outcomes in observational data, not due to\na therapeutic effect but due to a confounding pattern that varies across institutions, namely co-prescription\nwith aspirin for cardioprotection. CRec separates drug molecular representations into causal therapeutic\ncomponents and spurious prescription patterns through interventional disentanglement. In an analysis of\n47,326 patients with multiple chronic conditions, CRec identified mechanistically synergistic drug combina-\ntions that traditional methods missed because they were rarely co-prescribed, revealing genuine therapeutic\nopportunities obscured by prescribing traditions [63]. The DIR-GNN framework extends invariant learn-\ning through environment-based partitioning, explicitly learning which features remain predictive across all\nenvironments (causal invariants) versus environment-specific features (spurious correlations), then building\npredictions based solely on invariant features [64].\n3.5 Ensuring causal fairness in clinical algorithms\nTraditional fairness metrics based on statistical parity, equalising predictions across demographic groups,\noperate at Pearl’s Level 1 (association), yet clinical deployment decisions inherently require Level 2 (inter-\nvention) reasoning [11, 13]. The critical question for healthcare systems is not whether predictions correlate\nequally across demographics but whether predictions remain stable under hypothetical interventions on\nsensitive attributes, a distinction with profound implications for equity. Statistical fairness may equalise pre-\ndictions across groups while preserving discriminatory causal mechanisms embedded in the data-generating\nprocess.\nInterventional fairness requires that the predicted outcome distributions remain invariant under inter-\nventions on sensitive attributes: P( ˆY = y|do(A = a)) = P( ˆY = y|do(A = a′)) for all outcomes y and\nsensitive attribute values a, a′ [9, 10]. This criterion extends naturally to path-specific fairness, where effects\nalong specific causal pathways are constrained. For the Standard Fairness Model with sensitive attribute\nA, confounders Z, mediators W, and outcome Y , we decompose the total effects into Natural Direct Effect\n(NDE), Natural Indirect Effect (NIE), and Natural Spurious Effect (NSE), representing effects transmit-\nted through different causal pathways [11]. A causally S-fair predictor for pathway set S ensures that the\npathways in S transmit no discriminatory signal and preserves the causal structure for pathways not in S,\npreventing undesirable bias amplification [65].\nThe RaVSNet framework demonstrates the clinical necessity of causal bias correction by explicitly\nmodelling the discrepancy between historical prescription habits and therapeutic efficacy [66]. Using\nBayesian network causal discovery (GIES algorithm), RaVSNet quantifies how institutional protocols and\nco-occurrence patterns influence medication choices independently of patient-specific benefit. The system\ngenerates causal effect matrices to adjust recommendation probabilities, filtering out spurious associations\n11\n"}, {"page": 12, "text": "driven by data availability rather than medical necessity. On MIMIC-III and MIMIC-IV, RaVSNet achieved\nJaccard scores of 0.573 and 0.497 respectively (3–7% improvement), while its causal inference module specif-\nically mitigates the structural confounding in electronic health records, an important step for preventing the\npropagation of historical inequities that often manifest as data sparsity or noise in underserved populations\n[66].\nPath-specific decomposition enables explicit deliberation about equity costs and benefits [33]. Anal-\nysis reveals that removing direct effects of sensitive attributes often incurs minimal accuracy loss while\nsubstantially reducing disparity, suggesting that certain forms of discrimination can be eliminated at low\ncost. Conversely, constraining indirect effects through legitimate mediators (education, occupation) imposes\nhigher accuracy costs, reflecting genuine predictive value. This transparency enables stakeholders such as\nregulatory bodies, healthcare institutions, and patient advocacy groups, to make informed decisions about\nacceptable fairness-utility trade-offs based on quantitative causal evidence rather than speculation. For clin-\nical deployment, interventional fairness addresses a critical limitation: models fair by statistical criteria\nmay exhibit discrimination when deployed across different healthcare contexts experiencing a distribution\nshift. Causally-constrained predictors maintain fairness properties by learning invariant causal relationships\nrather than site-specific correlations [49, 50].\n4 Clinical impact across domains\nThe translation of causal graph methods into clinical practice represents a convergence of methodological\ninnovation with pressing healthcare needs. These applications span from individual patient diagnosis to\npopulation-level disease surveillance, demonstrating how causal inference transforms raw biomedical data\ninto actionable clinical insights. Success depends not only on technical sophistication, but also on the address-\ning of fundamental clinical questions about disease mechanisms, treatment responses, and intervention\nstrategies.\n4.1 Diagnosis: from correlation to mechanism\n4.1.1 Mechanistic brain network analysis\nBrain network analysis exemplifies the critical need for causal over correlational approaches. The human\nbrain comprises approximately 86 billion neurons that form trillions of connections, creating one of the most\ncomplex networks in nature. Traditional graph-theoretical methods, while foundational, often fall short in\ncapturing the high-dimensional and dynamic nature of brain connectivity [67]. The brain naturally forms\na graph where regions of interest serve as nodes and functional connectivity defines edges, yet correlation-\nbased analysis frequently identifies spurious connections driven by global brain states, arousal levels, or\nmeasurement artefacts rather than genuine neural communication pathways [50].\nCausality-Inspired Graph Neural Networks tackle this through an interpretable architecture that funda-\nmentally reimagines brain network analysis [49, 50]. Rather than treating all connectivity equally, CI-GNN\nlearns disentangled latent representations where only the causal factor induces the explanatory subgraph\nrelated to diagnosis. This methodology leverages conditional mutual information from a Granger causality\nperspective to measure causal influence, providing a principled distinction between genuine neural dys-\nfunction and epiphenomenal changes. For major depressive disorder, CI-GNN identified strong connections\naround the left rectus, a region critical for the response to antidepressant treatment, while showing sparser\nconnections in the cerebellum [50]. This finding contradicts spurious cerebellar hyperconnectivity found by\nassociation-based methods, aligning instead with clinical understanding that depression primarily involves\naffective and cognitive functions rather than motor function. Mechanistic insights such as these prove crucial\nfor developing targeted interventions through neurostimulation, pharmacotherapy, or behavioural therapy.\nThe BPI-GNN framework advances psychiatric diagnosis through prototype learning that identifies bio-\nlogically meaningful subtypes of psychiatric disorders [68]. Beyond discriminating patients from healthy\ncontrols, the model revealed subtypes with distinct clinical profiles. In major depressive disorder (REST-\nmeta-MDD, n=1,604), identified subtypes exhibited distinct patterns on HAMD-17 clinical subscales\nincluding suicide risk, psychomotor retardation, and somatic symptoms, suggesting that network-based\nbiomarkers could inform clinical characterisation. For schizophrenia (SRPBS, n=184), subtypes corre-\nsponded to differences in PANSS symptom profiles across positive, negative, and general psychopathology\ndomains [68]. The model achieved superior performance against 11 popular brain network classification meth-\nods across three psychiatric datasets (REST-meta-MDD, ABIDE with n=1,064, and SRPBS), demonstrating\nboth technical excellence and clinical relevance.\nFor epileptic seizure prediction, the Dynamic Instance-level Graph Learning Network (DIGLN) captures\npatient-specific causal relationships between intracranial electroencephalogram channels that evolve over\ntime [59]. By learning both the organisation of epileptic networks and their temporal evolution, DIGLN\ndemonstrated the ability to evaluate multiple overlapping patterns of brain functional change, particularly\n12\n"}, {"page": 13, "text": "in connectivity between the visual and sensorimotor domains [69]. In 5-fold cross-validation on the Freiburg\niEEG dataset (9 patients, 45 seizures), this patient-specific approach achieved 85.7% recall and 91.7%\nprecision (F1: 87.0%), demonstrating the potential of dynamic graph learning for seizure prediction, though\nexternal validation on larger cohorts remains necessary.\n4.1.2 Multi-omics cancer subtyping\nCancer represents a medical challenge where the integration of multiple data modalities is not only beneficial,\nbut essential to understand the mechanisms of the disease. Single-omic approaches may lack the preci-\nsion required to establish robust associations between molecular-level changes and phenotypic traits [70].\nTumours are characterised by genomic instability, epigenetic dysregulation, transcriptomic reprogramming,\nand proteomic alterations, with each layer providing complementary information about cancer biology.\nCausal methodology transforms cancer classification by focusing on causally related genes rather than on\nall differentially expressed features. The CautionGCN framework employs a causal multi-head autoencoder\nfor causal feature selection, isolating characteristic genes with genuine causal relationships to cancer subtypes\n[71]. This approach addresses a fundamental problem in cancer genomics, where thousands of genes show\ndifferential expression, but only a subset causally drives tumorigenesis. By constructing causal gene-gene\ngraphs that capture driver relationships under assumptions of causal sufficiency and faithfulness (Table 2).\nThe gene-level resolution of MoCaGCN’s causal graph carries important validation implications where\neach identified causal driver can, in principle, be experimentally tested through CRISPR knockout or phar-\nmacological inhibition of the specific gene product. This contrasts with pathway-level causal analyses where\nnodes aggregate multiple genes, creating ambiguity about which molecular targets should be perturbed to\nvalidate the discovered causal relationship.\nThe identified causal genes provide more than classification accuracy. The MOGONET framework takes\nadvantage of both omics features and correlations among samples described by similarity networks, using\nthe Correlation Discovery Network to explore cross-omics correlations in the label space [72]. This approach\nrecognises that different omics layers may capture distinct aspects of cancer biology, with genomics revealing\nmutational drivers, transcriptomics showing pathway activation, and proteomics indicating functional con-\nsequences. By learning these multi-level representations jointly, MOGONET achieved superior performance\nin breast invasive carcinoma subtype classification while identifying layer-specific biomarkers.\nThe clinical value extends beyond improved diagnostic accuracy to mechanistically informed treatment\nplanning. Understanding which molecular alterations causally drive disease progression, rather than merely\ncorrelating with it, enables the rational design of targeted therapies and the identification of vulnerabilities\nthat are amenable to pharmacological intervention. The shift from correlation-based feature selection to\ncausal driver identification represents a fundamental advance in precision oncology.\n4.2 Prognosis: predicting what interventions prevent\nThe Knowledge-aware Multi-label Network (KAMLN) addresses a significant limitation in postoperative\ncomplication prediction by explicitly modelling dependencies between complications rather than treating\nthem as independent events [73]. The framework utilizes a heterogeneous knowledge graph depicting clinical\nrelationships such as ”lung infection causes pleural effusion,” which in turn is linked to atelectasis [73]. In\na cohort of 593 lung cancer surgery patients, KAMLN achieved a micro-AUC of 0.664, demonstrating that\nintegrating prior medical knowledge can stabilize predictions for rare complications—such as pulmonary\nembolism and deep vein thrombosis—where traditional single-label models often fail [73]. The model’s use\nof SHAP analysis and a five-layer graph neural network architecture provides a transparent approach to\nunderstanding the clinical features that drive multi-label risk, identifying factors like operation duration\nand the number of dissected lymph nodes as critical predictors across multiple complication groups [73].\nWhile further external validation is required before direct clinical application, the explicit modelling of these\ncausal chains facilitates more accurate and fine-grained risk assessment for surgical patients [73].\nFor the progression of mild cognitive impairment to dementia, the CXGNN framework, which employs\ngenerative causal explainers based on the GNN-Neural Causal Model methodology, moves beyond static\nsnapshots to identify the primary causal factors driving the transition [60]. Analysis revealed that hyperten-\nsion, arrhythmia, and sex emerged as critical variables, with notable sex-specific differences in the pathways\nof cognitive decline. Female patients showed stronger causal links between vascular factors and cognitive\ndecline, while male patients exhibited more pronounced effects of metabolic factors. This mechanistic under-\nstanding enables a personalised risk assessment that goes beyond simple risk scores, providing actionable\ninsights for an early intervention tailored to individual risk profiles. Clinicians can target specific causal path-\nways rather than applying uniform interventions, potentially improving efficacy and reducing unnecessary\ntreatment burden.\n13\n"}, {"page": 14, "text": "4.3 Treatment selection: mechanism-based therapy\nDrug recommendation in the era of polypharmacy requires understanding not just individual drug effects\nbut also complex interactions and patient-specific factors. The Conditional Causal Representation Learner\ntries to tackle spurious correlations that plague traditional recommendation systems [63]. For example, pro-\nton pump inhibitors might appear beneficial for cardiovascular outcomes not due to their therapeutic effect\nbut because they are prescribed to patients who receive aspirin for cardioprotection. CRec disentangles these\nconfounded relationships by separating drug molecular representations into causal therapeutic components\nand spurious prescription patterns. Through causal intervention to eliminate spurious effects, recommenda-\ntions become genuinely mechanism-focused. In a study of 47,326 patients with multiple chronic conditions,\nCRec identified drug combinations that traditional methods failed to identify because they were rarely co-\nprescribed despite mechanistic synergy [63]. One notable discovery involved the combination of a diabetes\nmedication with an Alzheimer’s drug based on shared metabolic pathways, a combination now in clinical\ntrials showing promising results for both conditions.\nRaVSNet improves the precision of the recommendation by explicitly modelling and correcting data\ndistribution bias through Causal Effect Inference tasks [66]. Using Bayesian network causal discovery (GIES\nalgorithm), RaVSNet quantifies how clinical factors (diagnoses, symptoms) influence medication choices\nindependent of simple co-occurrence patterns. The system generates causal effect matrices that adjust\nrecommendation probabilities to reflect genuine therapeutic associations rather than spurious statistical\ncorrelations. On MIMIC-III and MIMIC-IV datasets, RaVSNet achieved Jaccard scores of 0.573 and 0.497\nrespectively, improvements over state-of-the-art baselines, while the causal framework provides a principled\nmechanism for identifying and correcting confounding biases in retrospective electronic health records [66].\nFor drug discovery, PDGrapher uses protein-protein interaction networks as approximations of causal\ngraphs, operating under the assumption that there are no unobserved confounders to predict combinato-\nrial therapeutic targets [47]. This causally-inspired approach outperforms traditional methods in identifying\neffective perturbagens across nine cell lines with chemical perturbations. The identified causal targets pro-\nvide more than simple efficacy predictions; by solving the inverse problem of perturbagen discovery, the\nmodel reveals combinatorial dependencies where targeting multiple specific proteins is required to shift\na unique disease state to a healthy profile. These mechanistic insights could allow the rational design\nof combination therapies that exploit biological structure rather than trial-and-error. Consequently, the\nclinical workflow transforms from ”drug prescribed for patients like you” to ”drug targeting your causal\nmechanisms,” representing a fundamental shift toward mechanism-based precision medicine.\n4.4 Continuous monitoring: real-time causal inference\nBiomedical monitoring generates vast streams of temporal data where the distinction of causal signals from\nnoise is essential for clinical decision-making. The challenge becomes particularly acute in intensive care\nsettings where hundreds of variables are monitored continuously, creating high-dimensional time series with\ncomplex interdependencies. Traditional monitoring systems generate frequent false alarms, with studies\nshowing that up to 92% of ICU alarms are clinically irrelevant, leading to alarm fatigue and potentially\nmissed critical events [74].\nThe CiGNN framework for cuffless blood pressure estimation exemplifies how causal methods transform\nphysiological monitoring [48, 58]. Rather than using all available features from electrocardiogram and pho-\ntoplethysmogram signals, CiGNN first applies causal discovery using FCI and CGNN algorithms to identify\nfeatures causally related to variation in blood pressure. This causal feature selection reduces dimensionality\nfrom more than 200 potential features to 23 causal indicators, dramatically improving both computa-\ntional efficiency and physiological interpretability. The causal relationships discovered provide physiological\ninsights beyond prediction. CiGNN identified that specific PPG morphological features causally influence\nblood pressure through arterial stiffness mechanisms, while certain heart rate variability patterns affect\nblood pressure through autonomic regulation. These mechanistic insights enable personalised monitoring\nstrategies in which patients with autonomic dysfunction require different monitoring parameters than those\nwith primarily vascular pathology.\nIn high-stakes clinical monitoring, missing data is often the reality which spatio-temporal modelling has\nto overcome. To this end, the Casper framework can provide a proof-of-concept to addresses the challenge\nof unknown confounders that create non-causal shortcut backdoor paths [75]. While evaluated on sensor\nnetworks, the framework’s use of frontdoor adjustment offers a methodology to eliminate spurious correla-\ntions, such as those created by hospital environmental factors influencing both monitoring frequency and\npatient physiological parameters, while preserving genuine causal relationships [75]. The Spatiotemporal\nCausal Attention module of the framework enforces sparse, interpretable causality among embeddings, with\ntheoretical guarantees of convergence to gradient-based explanations [75]. By demonstrating substantial\nimprovements in imputation quality and reduced variance in complex multi-site datasets, Casper provides\na robust foundation for more accurate clinical data reconstruction [75]. This enhanced accuracy potentially\n14\n"}, {"page": 15, "text": "improves the prediction of patient deterioration and reduces the likelihood of unnecessary interventions\ntriggered by imputation errors, effectively translating causal discovery into improved clinical outcomes.\nThe REASON framework extends causal analysis to system-level monitoring, crucial to understanding\ncascading failures in complex medical systems [76]. By modelling both intra-level and inter-level non-linear\ncausal relationships, REASON can trace how local perturbations propagate through interconnected phys-\niological systems. In sepsis monitoring, REASON identified how initial inflammatory responses cascade\nthrough the cardiovascular, respiratory, and renal systems, enabling early intervention before multi-organ\nfailure develops. This system-level causal analysis represents a paradigm shift from monitoring individual\nparameters to understanding how perturbations propagate through coupled biological networks, enabling\nproactive rather than reactive clinical management.\nTable 1: Comparative analysis of representative causal graph neural network methods by clinical capa-\nbility\nTask\nMethods\nKey Innovation\nClinical Impact\nDiagnostic sub-\ntyping\nCI-GNN [50], BPI-\nGNN [68]\nDisentangled\ncausal\nsubgraph\nidentification\nvia\nconditional\nmutual information\nPsychiatric subtypes with distinct\ntreatment responses; accuracy 0.71–\n0.73 with cross-site stability across 17\ninstitutions\nCancer\nclassifi-\ncation\nMoCaGCN[77],\nCautionGCN[71]\nCausal\ngene\ngraph\nconstruc-\ntion integrating prior biological\nknowledge\n89 causal genes vs. 1,000+ corre-\nlational features; identified KRAS-\nmetabolic axis for combination ther-\napy\nSeizure\npredic-\ntion\nDIGLN [59]\nPatient-specific dynamic causal\nconnectivity learning\n85.7%\nrecall,\n91.7%\nprecision\non\nFreiburg iEEG (n=9); mechanistic\nunderstanding of propagation pat-\nterns\nComplication\nprediction\nKAMLN [73]\nHeterogeneous knowledge graph\nencoding clinical dependencies\nbetween complications\nMicro-AUC 0.664 for multi-label pre-\ndiction;\nSHAP\nanalysis\nidentifies\nlymph node dissection as a significant\nrisk factor\nDrug\nrecom-\nmendation\nCRec [63], RaVS-\nNet [66]\nDisentanglement of therapeutic\neffects from prescription bias\nIdentified mechanistically synergis-\ntic combinations (CRec); 4–7% accu-\nracy improvement over baselines with\nreduced DDI rates (RaVSNet)\nCombination\ntherapy\nPDGrapher [47]\nInverse\ncausal\ninference\non\nprotein-protein interaction net-\nworks\nDirect prediction of combinatorial\nperturbagens for phenotype rever-\nsal; 13.3% improvement in identify-\ning therapeutic targets from interven-\ntional data\nBlood\npressure\nmonitoring\nCiGNN [48]\nFCI-based causal feature selec-\ntion for wearable sensors\n200+ features to 23 causal indicators;\nmechanistic interpretation of cardio-\nvascular regulation\nICU data impu-\ntation\nCasper [75]\nFrontdoor adjustment to block\nunknown\nspatiotemporal\ncon-\nfounders\nSignificantly\nreduced\nMAE/MSE;\nmethodological template for handling\nnon-causal shortcuts in continuous\nmonitoring\nCognitive\ndecline\nCXGNN [60]\nNeural causal models for coun-\nterfactual reasoning\nSex-specific causal pathways; person-\nalized risk assessment for MCI-to-\ndementia progression\nAbbreviations: CI-GNN, Causality-Inspired Graph Neural Network; BPI-GNN, Brain Prototype-Inspired\nGNN; MoCaGCN, Multi-omics Causal Graph Convolutional Network; DIGLN, Dynamic Instance-level Graph\nLearning Network; KAMLN, Knowledge-Aware Multi-label Network; CRec, Conditional Causal Representa-\ntion Learner; RaVSNet, Relevance-Aware Visit Similarity Network; PDGrapher, Perturbation-Driven Grapher;\nCiGNN, Causality-informed Graph Neural Network; Casper, Causality-Aware Spatiotemporal Imputation;\nCXGNN, Causal Explainer GNN; FCI, Fast Causal Inference; MCI, Mild Cognitive Impairment; AUC, Area\nUnder Curve.\n15\n"}, {"page": 16, "text": "Table 2: Causal assumptions and evidentiary status of methodological classes\nMethod Class\nRepresentative\nMethods\nRequired Assumptions\nCausal\nClaim\nTier\nDisentanglement\nDisC,\nCI-GNN,\nIGCL-GNN\nEnvironment diversity sufficient for\nseparation; causal features invariant\nacross environments\nSeparation\nof stable vs.\nunstable\nfea-\ntures\nTier 1\nInterventional\nprediction\niVGAE,\nRC-\nExplainer,\nPDGrapher\nCausal\nsufficiency\n(no\nunobserved\nconfounders);\nfaithfulness;\ncorrect\ngraph specification\nP(Y |do(X))\nestimation\nTier 2\nCounterfactual\ngeneration\nCLEAR,\nCXGNN,\nCSA\nCausal sufficiency; faithfulness; cor-\nrect functional form of structural\nequations\nPatient-\nspecific Yx(u)\ncomputation\nTier 2–3\nCausal\ndiscov-\nery\nCiGNN\n(FCI),\nMoCaGCN\nFaithfulness; causal Markov condi-\ntion;\nacyclicity\n(or\nknown\ncyclic\nstructure)\nGraph struc-\nture learning\nTier 2–3\nFairness\ncon-\nstraints\nRaVSNet,\npath-\nspecific methods\nCorrect causal graph; no unmeasured\nconfounding\nof\nsensitive\nattribute\npathways\nPath-specific\neffect removal\nTier 2\nTier 1: causally-inspired (architectural use of causal concepts without explicit causal claims); Tier 2: causally-\nstructured (causal estimates validated against external knowledge); Tier 3: causally-validated (novel causal\ndiscoveries with multi-modal triangulation). See Section 6.3 for tier definitions. Causal sufficiency assumes all\ncommon causes of measured variables are observed. Faithfulness assumes statistical independencies reflect only\ngraphical d-separation.\n5 Towards causal digital twins\nTable 3: Mapping methodological capabilities to clinical applications and digital twin components\nCapability (§3)\nClinical Application (§4)\nDigital Twin Role (§5)\nKey Methods\nCausal\ndisentangle-\nment\nPsychiatric subtyping; cancer\ndriver identification\nIsolating\npatient-specific\nmechanisms\nDisC, CI-GNN, IGCL-GNN\nIntervention predic-\ntion\nDrug combination discovery;\ntreatment selection\nVirtual therapeutic test-\ning\niVGAE,\nRC-Explainer,\nPDGrapher\nCounterfactual gen-\neration\nPersonalised prognosis; retro-\nspective analysis\nPatient-specific outcome\nsimulation\nCLEAR, CXGNN, CSA\nInvariant learning\nCross-institutional\ndeploy-\nment; monitoring robustness\nGeneralisation\nacross\nclinical contexts\nCaT-GNN, DIR-GNN\nCausal fairness\nBias-corrected\nrecommenda-\ntions; equitable risk prediction\nEquity-preserving\nsimu-\nlation\nRaVSNet,\npath-specific\nconstraints\nTable 3 summarises how the methodological capabilities developed in Section 3 and validated across\nclinical domains in Section 4 converge toward the integrative vision of patient-specific causal simulation. Dig-\nital twins have transitioned from manufacturing origins to healthcare applications, representing a paradigm\nshift toward dynamic, patient-specific computational models [36, 78]. Unlike static predictive models, digital\ntwins maintain bidirectional connections with their physical counterparts, continuously updating represen-\ntations as new patient data become available. The conceptual framework articulated by Emmert-Streib and\ncolleagues identifies four capabilities including explainability through mechanistic model structures inter-\npretable in biological terms, intervenability enabling simulation of therapeutic interventions before clinical\nadministration, learnability through iterative refinement as patient trajectories unfold, and diversability\nsupporting probabilistic exploration of alternative health states [36]. Applications spanning cardiovascu-\nlar medicine [79], immunology [37], and oncology [80] have demonstrated domain-specific utility, while\n16\n"}, {"page": 17, "text": "closed-loop systems such as the artificial pancreas illustrate mature clinical deployment for circumscribed\nphysiological control [38].\nYet a critical methodological gap constrains broader translation. Digital twins are not inherently causal\nmodels, current implementations predominantly rely on either physics-based simulations of specific organs\nor data-driven models that learn patient-specific parameters without formal causal grounding. Physics-\nbased approaches achieve mechanistic interpretability but scale poorly to multisystem diseases involving\nthousands of interacting molecular and physiological variables. Data-driven approaches scale efficiently but\ninherit the fundamental limitation of associational machine learning in which they cannot distinguish causal\nmechanisms from spurious correlations, precluding reliable intervention simulation when deployed beyond\ntraining distributions. Causal graph neural networks resolve this tension by combining the scalability of\nneural architectures with the theoretical guarantees of structural causal models. The do-calculus provides\nformal machinery for interventional prediction, counterfactual computation enables patient-specific outcome\nsimulation, and invariant causal learning ensures robustness across heterogeneous clinical contexts. This\nmethodological synthesis establishes the foundation for Causal Digital Twins, patient-specific computational\nmodels in which clinicians perform rigorous in silico experiments grounded in causal rather than merely\nassociational inference.\n5.1 Patient-specific mechanistic simulation\nCausal inference and graph learning can together enable a transformative vision for precision medicine.\nPatient-specific Causal Digital Twins represent dynamic computational models in which clinicians perform\nin silico experiments to test interventions before clinical administration. Such systems would integrate a\npatient’s multi-omics profile encompassing genomics, transcriptomics, proteomics, and metabolomics; lon-\ngitudinal imaging capturing organ structure and function over time; clinical history including treatments,\nresponses, and adverse events; and knowledge graphs encoding established biological mechanisms from\ndecades of biomedical research [17, 28, 29]. The underlying causal GNN architecture would learn patient-\nspecific parameterisations of general biological mechanisms, enabling simulation of therapeutic interventions\nthrough the do-operator framework. By setting treatment variables, propagating effects through learnt\ncausal pathways, and predicting outcomes at molecular, cellular, and phenotypic levels, the system generates\nmechanistically grounded predictions unavailable from purely associational models.\nConsider a clinician treating advanced non-small cell lung cancer with EGFR mutations. The patient’s\nCausal Digital Twin would encode the specific mutational landscape from whole-genome sequencing, tran-\nscriptomic profiles that reveal pathway dysregulation, proteomic data showing aberrant signalling cascades,\nand imaging demonstrating tumour burden and metastatic sites. The clinician would, under this framework,\nbe able to simulate five therapeutic strategies through the digital twin. First-generation EGFR inhibitors\ntargeting the primary mutation, combination therapy pairing EGFR inhibition with immune checkpoint\nblockade, metabolic targeting based on observed Warburg effect signatures, anti-angiogenic therapy address-\ning vascular supply, or sequential therapy alternating between mechanisms to prevent resistance. For each\nsimulated strategy, the digital twin generates predictions conditional on the assumed causal structure,\nestimating effects on specific oncogenic pathways using learnt causal relationships from the patient’s molec-\nular data, estimates toxicity risks by propagating drug effects through normal tissue networks, projects\nprogression-free survival by modelling tumour evolution under selection pressure, and identifies optimal per-\nsonalised therapy before administering a single dose. This capability transforms treatment planning from\nempirical trial-and-error to mechanistically informed optimisation.\nCurrent methodological advances establish foundations for realising this vision. Interventional GNNs\nsuch as iVGAE enable simulation of treatments never observed in training data by implementing do-calculus\nthrough differentiable neural operations [42]. Counterfactual generators, including CLEAR and CXGNN,\nexplore alternative scenarios while maintaining biological plausibility, computing patient-specific predic-\ntions through the abduction-action-prediction framework [51, 60]. Multi-modal integration frameworks such\nas MOGONET and MoCaGCN combine diverse data modalities through shared causal structure, learning\nrepresentations that capture biological mechanisms spanning molecular to phenotypic scales [72, 77]. Fair-\nness constraints ensure equitable simulation by identifying and mitigating discriminatory causal pathways,\npreventing digital twins from perpetuating biases embedded in training data [11].\n5.2 The LLM-Causal GNN synergy\nThe integration of large language models with causal GNNs creates a powerful hybrid architecture address-\ning complementary aspects of clinical reasoning. Large language models excel at processing unstructured\nclinical data, including free-text notes, radiology reports, pathology descriptions, and biomedical litera-\nture, extracting entities, relationships, and temporal patterns from narrative text [29, 81]. However, LLMs\nlack rigorous causal inference capabilities and cannot distinguish genuine causal relationships from spurious\ncorrelations in their training data. Causal GNNs provide the complementary capability of validating and\n17\n"}, {"page": 18, "text": "quantifying causal hypotheses using structured multi-omics data, medical imaging, and biological knowledge\ngraphs, but require well-defined graph structures and struggle with unstructured information.\nThe synergistic workflow proceeds through hypothesis generation followed by mechanistic validation. An\nLLM continuously monitors a patient’s electronic health record, parsing clinical notes, laboratory results,\nmedication orders, and vital signs. When the system detects a prescribed drug that coincides temporally\nwith emerging adverse symptoms, the LLM proposes a causal hypothesis linking drug exposure to observed\neffects. This hypothesis, expressed as a potential causal pathway, is then submitted to a Causal GNN\nthat accesses protein-protein interaction networks capturing drug-target relationships and off-target effects,\npharmacovigilance databases containing population-scale adverse event reports, and the patient’s genomic\ndata revealing drug metabolism variants. The Causal GNN assesses whether the proposed relationship\nsatisfies causal identifiability criteria, computes interventional distributions predicting effects under different\ndosing strategies, performs sensitivity analysis quantifying robustness to unmeasured confounding, and\nreturns a mechanistically validated assessment in seconds [13, 14]. If the relationship proves causal, the\nsystem alerts the clinician with a mechanistic explanation of the pathway linking the drug to adverse\neffect, a quantitative estimate of effect magnitude based on the patient’s molecular profile, and alternative\nmedication recommendations avoiding the problematic mechanism. This hybrid approach combines the\nclinical reasoning capabilities of LLMs, including natural language understanding and medical knowledge\nretrieval, with the mechanistic validation of Causal GNNs, providing rigorous quantification of causal effects\nand biological plausibility checking. The synergy proves particularly powerful for pharmacovigilance, where\nnovel adverse drug reactions often first appear in clinical narratives before accumulating sufficient structured\nreports for traditional signal detection. By enabling real-time causal evaluation of hypotheses extracted from\nunstructured text, the LLM-Causal GNN architecture could potentially identify safety signals years earlier\nthan current passive surveillance systems [82].\nSuch hybrid architectures must contend with well-documented LLM limitations in biomedical contexts.\nSystematic evaluations reveal substantial hallucination rates when LLMs generate scientific references, rang-\ning from 29% for GPT-4 to over 90% for some models, with poor adherence to specified inclusion criteria\neven for non-hallucinated outputs [83]. Comprehensive benchmarking across biomedical natural language\nprocessing tasks demonstrates that while LLMs excel at reasoning-intensive question answering, they under-\nperform fine-tuned domain-specific models in information extraction, with qualitative analyses revealing\nprevalent missing information and inconsistent outputs [84]. These findings underscore that causal GNNs\ncontribute not merely complementary functionality but essential grounding which by encoding structured\nbiomedical knowledge and enforcing mechanistic constraints, can anchor LLM-generated hypotheses to val-\nidated biological relationships, potentially mitigating the hallucination risks that currently limit clinical\ndeployment of pure LLM approaches [85].\n5.3 Implementation requirements\nTranslating the Causal Digital Twin vision to clinical reality requires coordinated advances across compu-\ntational infrastructure, data integration, validation frameworks, and regulatory pathways. Computational\nrequirements pose immediate challenges. Current causal GNN methods incur substantial computational\noverhead from causal discovery and multi-environment optimisation. While CI-GNN achieves efficient infer-\nence (approximately 33ms per instance on 116-node brain networks using the AAL atlas), training requires\ngrid search over hyperparameters across multiple independent runs [50], and realistic molecular networks\ncontain 20,000+ genes and protein-protein interaction networks span hundreds of thousands of edges.\nReal-time clinical decision support demands inference times measured in seconds, not hours. Algorithmic\ninnovations, including hierarchical causal discovery that exploits biological modularity, amortised inference\napproaches that train once but infer rapidly, and optimised neuromorphic hardware for graph opera-\ntions, represent promising directions [76]. Cloud-based computational infrastructure with distributed graph\nprocessing capabilities could enable the necessary scale while maintaining clinical response times.\nMulti-modal data integration pipelines must harmonise heterogeneous data sources spanning genomic\nsequences, gene expression matrices, proteomic mass spectrometry, metabolomic profiles, medical images\nin DICOM format, clinical notes in natural language, structured electronic health records, and contin-\nuous physiological monitoring streams. Each modality requires specialised preprocessing, quality control,\nand normalisation prior to integration through a shared causal structure [28, 29]. Temporal alignment\nproves particularly challenging when different modalities are collected at varying frequencies, from genome\nsequences measured once to physiological signals sampled thousands of times per second. Federated learn-\ning approaches that preserve privacy enable model training across institutions without centralising sensitive\npatient data, addressing both technical and regulatory constraints [52].\nClinical validation protocols must establish evidentiary standards for digital twin predictions. The multi-\nmodal triangulation framework described earlier provides a starting point, combining biological plausibility\n18\n"}, {"page": 19, "text": "assessment, replication across patient cohorts, natural experiments where available, and prospective vali-\ndation studies comparing digital twin predictions to observed outcomes [13]. Regulatory frameworks for in\nsilico experimentation remain underdeveloped. The FDA and EMA have established pathways for in silico\nclinical trials in the development of medical devices, but extending these to patient-specific treatment opti-\nmisation requires new guidance that addresses unique challenges of personalised mechanistic modelling [86].\nQuestions of liability when digital twin predictions prove inaccurate, intellectual property for patient-specific\nmodels, and equitable access to computationally intensive precision medicine tools demand policy atten-\ntion. Success requires sustained collaboration between computational scientists developing causal inference\nmethods, clinicians defining clinical requirements and validation criteria, biomedical researchers providing\nmechanistic knowledge, and regulatory bodies establishing appropriate oversight frameworks. The technical\nfoundations are established; organisational and policy innovations will determine whether Causal Digital\nTwins transform from an aspirational vision to a clinical reality.\n6 Challenges and path forward\n6.1 Computational reality\nAlgorithmic innovations targeting biological structure offer promising solutions. Hierarchical causal discovery\nmethods exploit the modular organisation of biological systems, first identifying causal relationships between\nsubsystems such as organ networks or cellular pathways, then refining the structure within the module\n[76]. This approach dramatically reduces computational complexity by leveraging natural sparsity patterns\nwhere most biological components interact locally within functional modules rather than exhibiting all-\nto-all connectivity. The REASON framework demonstrates feasibility by modelling both intra-level and\ninter-level causal relationships, enabling system-level analysis of cascading failures in complex physiological\nnetworks [76]. Amortised inference represents another transformative direction, where models are trained\nonce on diverse datasets and then perform rapid inference on new patients. Rather than discovering the\nde novo causal structure for each patient, the system learns to predict the causal relationships from the\nobserved data through end-to-end differentiable architectures [60]. This enables real-time causal inference,\nwhich is feasible for clinical decision support. For applications where distribution shift is minimal and\nintervention prediction unnecessary, correlation-based approaches remain appropriate and more practical;\ncausal methods are warranted specifically when the failure modes they address, brittleness under deployment\nshift, confounded treatment effects, counterfactual personalisation, are clinically relevant.\nHardware acceleration through neuromorphic computing and graph-specific processors offers comple-\nmentary advances. Graph neural network operations exhibit inherent parallelism amenable to specialised\nhardware, with emerging architectures achieving orders-of-magnitude speedups for message-passing com-\nputations [42]. Mixing algorithmic innovation, and exploiting biological structure and amortised learning,\nwith hardware acceleration through specialised computing substrates, establishes a viable path toward clin-\nically deployable causal GNN systems operating at the scale and speed demanded by precision medicine\napplications.\n19\n"}, {"page": 20, "text": "Box 2: Computational complexity and scaling strategies\nThe scaling challenge\nCausal discovery complexity grows super-exponentially as O(n! · 2n(n−1)/2) for n nodes, while\nconstraint-based algorithms scale as O(nk) with conditioning set size k [13].\nEmpirical costs:\n1. CI-GNN: 33ms inference per instance on 116-node networks; training cost not reported [50]\n2. Molecular networks: 20,000+ genes from single-cell sequencing [28]\n3. Clinical deployment requires sub-second inference times\nStrategy\nComplexity Reduction\nApplication\nHierarchical\nO(nk) →O(m · (n/m)k)\nREASON framework [76]\nAmortised\nPer-instance →Meta-learning\nNeural causal models [60]\nHardware\n10–1000× speedup\nNeuromorphic/GPU [42]\nStructural Priors\nAvoids O(n!) discovery\nPDGrapher [47]\nApproximation\nExact →Variational\nCLEAR (VAE-based) [51]\nWhat this means:\n1. Biological modularity: Gene networks decompose into pathways; exploiting this structure yields\norder-of-magnitude efficiency gains.\n2. Structural priors: Using established protein-protein interaction networks as causal proxies bypasses\nexpensive structure learning, enabling analysis of genome-scale perturbations.\n3. Amortised inference: Train once on diverse datasets (O(N · nk)), then perform constant-time\ninference (O(n2)) for clinical deployment.\n4. Hardware acceleration: Graph-specific processors and neuromorphic architectures achieve 10–\n1000× speedups for message-passing operations.\n5. Approximation trade-offs: Variational methods sacrifice theoretical optimality for computational\nfeasibility whilst providing uncertainty quantification.\nPath forward\nReal-time causal inference at the biomedical scale demands synergistic advances. Bringing together\nhierarchical methods exploiting biological structure, structural priors that enable genome-scale\napproximation, and specialised architectures establishes viable pathways toward clinically deployable\nCausal Digital Twins.\n6.2 Circumstances of failure\nRecent large-scale empirical work directly challenges optimistic assumptions about causal generalisation.\nNastl and Hardt evaluated 16 prediction tasks across healthcare, employment, education, and social pol-\nicy domains, each with multiple deployment environments permitting rigorous out-of-distribution testing\n[87]. Without exception, predictors using all available features, regardless of causality, achieved superior\nin-domain and out-of-domain accuracy compared with predictors restricted to causal features. Even the abso-\nlute accuracy drop from training to deployment domains was no better for causal predictors than for models\nexploiting all correlational patterns. Causal machine learning methods specifically designed for domain gen-\neralisation did not perform better than standard predictors trained on causal features alone, while causal\ndiscovery algorithms either failed to execute on real-world datasets or selected variables offering no advan-\ntage over expert-curated selections. These findings suggest that causal approaches provide benefit primarily\nwhen non-causal features experience substantial concept shift between domains; when distributional changes\nare modest, the information loss from discarding potentially predictive features outweighs robustness gains.\nSample size constraints pose particular challenges for causal GNNs in neuroimaging applications. Brain\nconnectivity studies typically involve high-dimensional graphs comprising hundreds to thousands of nodes\nwith relatively small patient cohorts, leading to a higher risk of overfitting [67]. Neural network models\nexhibit larger variance than conventional machine learning methods on small datasets, and regularisation\nstrategies effective for standard GNNs may not transfer to causal architectures with additional structural\nconstraints. The BrainGB benchmark revealed that sophisticated GNN architectures can exhaust memory\non large-scale datasets while providing minimal advantages over shallow baseline models on small datasets\nwhere overfitting dominates [88]. The fundamental tension between model complexity required for causal\nreasoning and data availability in clinical neuroimaging remains unresolved.\n20\n"}, {"page": 21, "text": "Unobserved confounding constitutes a limitation that architectural innovation cannot overcome\n(Figure 2B). Causal conclusions from observational data rest on the assumption of no unmeasured con-\nfounding, yet this assumption is empirically untestable and rarely satisfied in healthcare settings [15]. When\ntreatment decisions depend on unmeasured clinical judgement, patient preferences, or institutional prac-\ntices, even sophisticated causal GNN architectures may learn spurious relationships that appear causally\nvalid within their structural assumptions but reflect residual confounding. The CausalBench benchmark,\nevaluating causal discovery methods against interventional single-cell perturbation data with known ground\ntruth, demonstrated that state-of-the-art methods struggle to recover true regulatory relationships [46].\nSubstantial performance gaps persist between low-intervention and high-intervention experimental settings,\nsuggesting that methods validated on observational benchmarks may not transfer to genuine interventional\nprediction.\nThese empirical limitations underscore that causal GNNs should be deployed with appropriate epis-\ntemic humility. The methods excel when specific failure modes, distribution shift between training and\ndeployment, confounded treatment effects requiring debiasing, or counterfactual personalisation demanding\npatient-specific simulation, are clinically relevant and sufficiently severe to justify computational overhead.\nWhen these conditions are absent, correlation-based approaches remain appropriate and more practical.\n6.3 The validation crisis\nEstablishing trustworthy causal conclusions from observational healthcare data constitutes perhaps the most\nfundamental challenge facing the field. Cross-validation and held-out test sets effectively evaluate predictive\nperformance, but provide insufficient evidence to validate causal claims [13, 89]. The core problem is that\nstandard machine learning evaluation metrics, including accuracy, area under the curve, and calibration\nmeasure association rather than causation. A model can achieve excellent predictive performance while\nencoding spurious correlations that would fail under intervention [62, 90]. Ground truth causal structures\nremain unknown for most biomedical systems, precluding direct validation of learnt causal graphs against\nknown mechanisms.\nTable 2 lays out the assumptions underlying each methodological class reviewed in this Perspective.\nCritically, violation of these assumptions, particularly causal sufficiency in the presence of unmeasured\nconfounding, invalidates the causal interpretation of model outputs regardless of predictive performance.\nThe tiered evidentiary framework provides a mechanism for calibrating confidence: Tier 1 methods make\nno causal claims requiring these assumptions; Tier 2 methods require assumption plausibility supported\nby domain knowledge; Tier 3 claims demand empirical validation that assumptions hold in the specific\napplication context.\nThe validation crisis (Figure 2A) illustrates that methods clustered in the high-correlation, low-\nintervention quadrant achieve impressive retrospective metrics while failing the prospective deployment.\nThis ”causal gap” quantifies the expected performance loss when correlation-based models are applied to\ninterventional queries, validating the need for validation frameworks extending beyond cross-validation to\nnatural experiments, propensity-matched analyses, and prospective intervention studies [89, 91].\nMulti-modal evidence triangulation provides a rigorous framework for establishing causal validity through\nconvergent evidence across complementary modalities [89, 92]. Biological plausibility assessment evaluates\nwhether identified causal relationships align with established pathophysiological mechanisms and biochemi-\ncal pathways, providing mechanistic coherence testing against decades of accumulated biomedical knowledge\n[93]. The MoCaGCN framework demonstrates this approach by integrating prior knowledge from the Gen-\neMANIA database to constrain learnt causal gene graphs, ensuring that identified relationships respect\nestablished gene-gene interactions while remaining flexible to discover novel causal mechanisms [77]. Replica-\ntion among independent cohorts tests whether causal relationships remain stable across diverse populations,\nhealthcare systems, and temporal periods. The CiGNN blood pressure monitoring framework was evalu-\nated across three datasets, an internal cohort (n=62), VitalDB surgical patients (n=205), and a maneuver\ndataset (n=38), achieving MAE of 3.2–4.2 mmHg for systolic and 1.9–2.8 mmHg for diastolic blood pressure,\ndemonstrating consistent performance across healthy and clinical populations [48].\nValidation design must also address the resolution at which causal claims are made. Under the inter-\nventionist framework, causal relationships are defined relative to interventions that could, in principle, be\nperformed [45]. When computational nodes aggregate multiple biological entities like ”prefrontal cortex”\ncomprising dozens of sub-regions, ”inflammation” encompassing multiple cytokine cascades, or ”metabolic\nreprogramming” spanning hundreds of metabolites, experimental validation becomes ambiguous. Interven-\ntions targeting specific sub-components may not test the aggregate causal claim, while interventions at the\naggregate level may be infeasible or imprecise. This resolution mismatch between computational causal dis-\ncovery and experimental validation represents a systematic challenge: causal graphs operating at pathway\nresolution require experimental designs that modulate entire pathways coherently, whereas gene-level graphs\n21\n"}, {"page": 22, "text": "permit targeted perturbation experiments. Explicit specification of node semantics in causal graph report-\ning, analogous to intervention definitions in clinical trial protocols, would clarify what experiments could\nvalidate the claims and at what biological scale the causal relationships are asserted to hold.\nNatural experiments and quasi-experimental designs provide powerful causal validation by exploiting\nsituations where treatment assignment approximates randomisation through exogenous factors [91, 94].\nPolicy discontinuities, instrumental variables, and regression discontinuity designs enable the estimation of\ncausal effects with minimal confounding, serving as benchmarks against which observational causal models\ncan be validated. Prospective intervention studies, when feasible, constitute the gold standard by testing\nwhether the predicted intervention outcomes materialise under actual treatment [89]. Sensitivity analyses\nquantify robustness to violations of causal assumptions, particularly unmeasured confounding, through\nE-value calculations and model specification uncertainty assessment [53, 95].\nWe propose tiered evidentiary standards that distinguish the strength of causal evidence. Tier 1 encom-\npasses causally-inspired methods that employ causal concepts in architectural design but make no explicit\ncausal claims about learnt relationships, such as invariance-based regularisation promoting features sta-\nble across environments [96]. Tier 2 includes causally-structured methods that learn causal structures or\nestimate causal effects validated through consistency with external knowledge rather than novel discov-\nery, exemplified by cancer subtyping models recovering known oncogenic pathways [77]. Tier 3 represents\ncausally-validated methods that claim discovery of previously unknown causal relationships, requiring com-\nprehensive triangulation across biological plausibility with the proposed mechanism, replication across two\nor more independent cohorts, validation of natural experiments or prospective intervention studies, sensitiv-\nity analysis demonstrating robustness with E-values exceeding 2.0 for strong claims, and aggregate evidence\nscores exceeding 0.6 using quantitative integration frameworks [92].\n6.4 From methods to clinical practice\nThe gap between methodological sophistication and clinical interpretability poses critical barriers to deploy-\nment. Clinicians require not only predictions but also mechanistic explanations that they can understand,\nverify against domain expertise, and integrate with clinical reasoning [32]. A significant challenge in the\nimplementation of GNNs for biomarker discovery involves ensuring the interpretability and clinical rele-\nvance of identified patterns [67]. Computational representations learnt by neural networks often lack direct\ncorrespondence to clinical concepts, necessitating translation frameworks that bridge this semantic gap.\nIntegration of domain knowledge into model design represents a promising direction. Rather than\nlearning causal structures purely from data, hybrid approaches incorporate established biological mecha-\nnisms as structural priors, constraining model architectures according to known pathway relationships [77].\nThe MoCaGCN framework demonstrates this by integrating gene-gene interaction networks from curated\ndatabases as graph topology priors, ensuring that learnt causal relationships respect established biological\nconstraints while remaining flexible enough to discover novel mechanisms. Explainability frameworks must\ntranslate learnt causal representations into clinically meaningful concepts. The CI-GNN architecture achieves\nthis by identifying sparse subgraphs corresponding to specific disease mechanisms, where highlighted regions\nand connections of the brain are directly linked to established neural circuits known to clinicians [49, 50].\nClinician-in-the-loop validation provides essential reality checking where domain experts evaluate\nwhether computationally identified causal mechanisms align with clinical understanding and biological\nplausibility. This iterative refinement process, combining data-driven discovery with expert knowledge,\nproves critical for building clinical trust. Regulatory pathways for causal claims from observational data\nremain underdeveloped. The FDA has established frameworks for predictive algorithms but lacks specific\nguidance for systems making interventional or counterfactual claims based on causal inference from non-\nrandomised data [86]. Developing appropriate oversight requires balancing innovation incentives with patient\nsafety, establishing evidentiary standards sufficient to support causal claims without demanding infeasible\nvalidation burdens.\nThe risk of ”causal-washing”, where methods employ causal terminology without meeting rigorous\nevidentiary standards for causal claims, threatens field credibility. The appeal of causal framing is under-\nstandable given superior theoretical properties and alignment with clinical reasoning, yet the gap between\ncausal aspiration and causal validation remains substantial across much of the literature. Several pat-\nterns characterise causal-washing in current practice. Terminological overreach occurs when methods use\ncausal vocabulary, including ”causal representation,” ”causal discovery,” or ”causal reasoning,” while rely-\ning fundamentally on associational quantification without estimating interventional distributions or testing\nidentifiability conditions [13, 62]. Insufficient validation arises when causal claims rest solely on predictive\nperformance metrics without triangulating evidence from biological plausibility, replication, natural exper-\niments, or sensitivity analyses. Assumption elision involves implementing causal methods while omitting\nexplicit statements of the requisite assumptions, including faithfulness, causal sufficiency, and positivity,\npreventing the assessment of validity [26, 62].\n22\n"}, {"page": 23, "text": "Community-level practices can mitigate causal-washing through standardised reporting guidelines anal-\nogous to CONSORT for randomised trials or STROBE for observational studies [97]. Such guidelines should\nrequire an explicit statement of all causal assumptions with directed acyclic graphs when applicable, a\nspecification of which causal quantities are being estimated, including the average treatment effect or con-\nditional average treatment effect, reporting of sensitivity analyses quantifying robustness to assumption\nviolations, and a clear demarcation between causal claims versus associational findings [14]. The transi-\ntion from causal-washing to rigorous causal inference requires cultural change alongside methodological\nstandards, where funding agencies prioritise validation studies over novel method proliferation, peer review\ndemands explicit justification for causal terminology and rejects unsupported claims, and graduate training\nemphasises distinctions between causal inference, prediction, and description.\nBox 3: Example of multi-modal validation framework\nThe amyloid-β hypothesis in Alzheimer’s disease\nConsider a causal GNN identifying a directed edge from amyloid-β deposition to cognitive decline\nin Alzheimer’s disease, a relationship with validation complexities.\nEvidence triangulation:\n1. Biological plausibility (Sbio = +1): Strong mechanistic support through the amyloid cascade\nhypothesis and decades of foundational neuroscience research.\n2. Replication (Srep = +1): Association replicates consistently across major cohorts, including\nADNI, AIBL, and European EADC datasets with stable effect sizes.\n3. Natural experiments (Squasi = −1): Mendelian randomization studies using genetic variants\naffecting amyloid production demonstrate no causal effect on cognitive decline.\n4. Prospective validation (Sprosp = −1): Multiple Phase III clinical trials of amyloid-reducing\ntherapies show minimal cognitive benefit despite substantial plaque reduction.\n5. Sensitivity analysis (Ssens = 0): Moderate robustness to unmeasured confounding (E-value =\n1.8) but substantial uncertainty regarding alternative causal structures.\nAggregate evidence score:\nAES = 0.20(+1) + 0.25(+1) + 0.30(−1) + 0.15(−1) + 0.10(0) = 0.00\n(4)\nInterpretation: Despite strong observational associations and biological plausibility, the null\naggregate score could reflect contradictory evidence from high-quality natural experiments and\nprospective trials [92, 98]. This suggests the observational causal model may be confounded, with\namyloid deposition potentially representing a biomarker of disease progression rather than a causal\ndriver, or causality involving more complex dynamics, including threshold effects or critical devel-\nopmental periods that the learned graph structure fails to capture.\nClinical implication: Predictions based on this causal edge should be treated with appropriate\ncaution, and amyloid reduction alone may not constitute an effective therapeutic strategy. This exem-\nplifies how systematic evidence triangulation prevents overconfident causal claims despite compelling\nobservational evidence, ultimately protecting patients from ineffective interventions whilst guiding\nresearch toward more productive therapeutic targets.\n23\n"}, {"page": 24, "text": "Box 4: Clinical workflow for causal drug recommendation\nClinical scenario\nA 68-year-old patient presents with type 2 diabetes, hypertension, and early-stage chronic kidney\ndisease. The electronic health record shows current medications including metformin and lisinopril.\nThe clinical question: which additional antihypertensive agent would optimise cardiovascular out-\ncomes while protecting renal function?\nStep 1: Graph construction\n1. Patient node features: demographics, comorbidities, laboratory values (HbA1c, eGFR, potassium)\n2. Drug nodes: candidate medications with molecular fingerprints encoding mechanism of action\n3. Edges: drug-drug interactions, drug-disease relationships, patient-drug history\nStep 2: Causal disentanglement (CRec framework)\nThe model separates drug representations into:\n1. Causal therapeutic component: molecular mechanisms affecting blood pressure and renal protec-\ntion (ACE inhibition, calcium channel blockade, mineralocorticoid antagonism)\n2. Spurious prescription component: historical prescribing patterns reflecting physician habits,\nformulary constraints, marketing effects\nStep 3: Interventional prediction\nFor each candidate drug d, the model computes P(outcome|do(d), patient features) rather than\nP(outcome|d, patient features), removing confounding from indication bias (sicker patients receiving\nmore aggressive treatment).\nStep 4: Counterfactual comparison\nCandidate\nCV Risk Reduction\neGFR Preservation\nAmlodipine\n12%\nNeutral\nChlorthalidone\n18%\n−8% annual decline\nSpironolactone\n22%\n+4% preservation\nStep 5: Recommendation with explanation\nThe system recommends spironolactone with mechanistic rationale. Mineralocorticoid receptor\nantagonism reduces cardiac fibrosis (cardiovascular benefit) while counteracting aldosterone-\nmediated nephron loss (renal protection). The recommendation differs from historical prescribing\npatterns, where chlorthalidone was favoured due to guideline inertia despite inferior renal outcomes\nin diabetic nephropathy.\nClinical validation pathway\nThis recommendation aligns with FIDELIO-DKD trial evidence demonstrating cardiorenal benefits\nof mineralocorticoid antagonism in diabetic kidney disease [99], providing external validation for\nthe causal mechanism identified by the model. The workflow demonstrates how causal GNNs move\nbeyond “patients like you received this drug” to “this drug targets your causal disease mechanisms.”\nWe note that healthcare presents additional methodological challenges where causal reasoning proves\nessential but which exceed the scope of this Perspective. Small-sample learning remains problematic for\ncausal GNNs, as deep learning models require large datasets to avoid overfitting while neuroimaging stud-\nies typically involve high-dimensional graphs with limited patient cohorts [100]. Multi-centre generalisation,\nthough addressed by invariant learning (Section 3.4), involves domain adaptation complexities requiring\nspecialised harmonisation techniques beyond causal invariance alone [101]. Time-varying confounding in lon-\ngitudinal treatment regimes demands methods such as marginal structural models and g-computation that\nextend beyond the cross-sectional causal graphs emphasised here [102]. Missing data mechanisms, ubiqui-\ntous in electronic health records, require causal models for principled imputation yet remain inadequately\naddressed in current causal GNN frameworks [103]. These directions warrant dedicated methodological\nattention as the field matures.\n24\n"}, {"page": 25, "text": "7 Conclusion\nThe integration of causal inference with graph neural networks addresses a fundamental limitation in health-\ncare artificial intelligence. The documented triple crisis of distribution shift that causes performance collapse\nacross institutions, perpetuation of discriminatory patterns embedded in historical data, and lack of mech-\nanistic interpretability that prevents clinical verification demand a paradigm change from correlation-based\nprediction to mechanism-based understanding [1, 2, 104]. This review has demonstrated that causality-\ninspired architectures, when rigorously implemented and validated, achieve superior generalisation across\nhealthcare settings, identify biologically plausible mechanisms aligned with established pathophysiology,\nand maintain fairness properties under distribution shift through learning of invariant causal relationships\nrather than site-specific correlations [11, 49, 50].\nApplications spanning psychiatric subtyping, cancer molecular characterisation, physiological monitor-\ning, and drug discovery validate this approach across diverse clinical domains. The CI-GNN framework\nmaintained stable diagnostic performance across institutions where traditional methods varied dramati-\ncally [50]. Multi-omics causal integration identified 89 driver genes from thousands of correlational features,\naccelerating precision oncology [77]. Causal feature selection transformed continuous monitoring from\nhigh-dimensional noise to mechanistically interpretable signals [48]. The methodological foundations for\npatient-specific Causal Digital Twins now exist, enabling in silico clinical experimentation before thera-\npeutic intervention, with synergistic integration of large language models for hypothesis generation and\ncausal graph neural networks for mechanistic validation, positioning the field at the forefront of multimodal\nartificial intelligence [29, 82].\nHowever, substantial barriers constrain translation to clinical practice. Computational complexity pre-\ncludes real-time deployment, training that requires hours on specialised hardware for moderately sized\nnetworks, while realistic molecular graphs span tens of thousands of nodes [28, 49]. Validation must evolve\nbeyond predictive accuracy to establish causal claims through multi-modal evidence triangulation combin-\ning biological plausibility, cross-cohort replication, natural experiments, and sensitivity analyses [89, 92].\nThe field faces a growing risk of causal-washing, where methods employ causal terminology without meeting\nrigorous evidentiary standards, threatening scientific credibility and clinical trust. Tiered frameworks distin-\nguishing causally-inspired architectures from causally-structured models and causally-validated discoveries\nprove essential for maintaining appropriate epistemic humility while enabling continued innovation.\nHealthcare artificial intelligence is at an inflexion point. The question is no longer whether machine learn-\ning can achieve impressive metrics in retrospective datasets but whether systems can survive deployment\nacross diverse clinical contexts while maintaining effectiveness, equity, and interpretability. Causal graph\nneural networks represent the only architectural paradigm that simultaneously addresses robustness through\ninvariant learning, fairness through path-specific interventional constraints, and mechanistic understand-\ning through explicit causal structure [10, 26, 62]. Success requires coordinated action on multiple fronts.\nComputational infrastructure investments must enable hierarchical causal discovery exploiting biological\nmodularity, amortised inference achieving clinical response times, and neuromorphic hardware acceleration\nfor graph operations at scale [76]. Validation standards require community consensus on evidentiary require-\nments for causal claims from observational data, analogous to established frameworks for randomised trials\nbut adapted to healthcare’s unique constraints. Regulatory pathways must evolve to evaluate systems that\nmake interventional predictions and counterfactual reasoning, extending beyond traditional accuracy-based\nassessment to mechanistic validity. Clinical trials demonstrating impact on patient outcomes rather than\nsurrogate metrics prove essential to justify substantial implementation costs. Training the next generation\nof clinician-scientists fluent in both causal inference theory and clinical medicine will determine whether\nthese methods reach their potential.\nThe transition from correlation-based prediction to mechanism-based medicine will define which artifi-\ncial intelligence systems earn clinical trust and achieve sustainable deployment. This transformation requires\nbalancing theoretical ambition with empirical humility, acknowledging that we construct pragmatic causal\napproximations that support clinical inferences at chosen levels of abstraction rather than discovering abso-\nlute causal truth in biological systems of extraordinary complexity. It demands computational sophistication\nmatched with clinical interpretability, ensuring that mechanistic insights align with established biomed-\nical knowledge and can be verified by domain experts. Most critically, it necessitates a transformative\nvision tempered by uncompromising validation standards, where novel causal discoveries undergo rigorous\nmulti-modal triangulation before informing high-stakes clinical decisions. The methodological foundations\nare established through theoretical advances connecting structural causal models to differentiable graph\narchitectures, empirical demonstrations across diverse healthcare applications, and emerging frameworks\nfor validation under observational constraints [13, 14, 42]. The path forward demands not only technical\ninnovation, but sustained collaboration between computational scientists developing causal inference meth-\nods, clinicians defining requirements and validation criteria, biomedical researchers providing mechanistic\n25\n"}, {"page": 26, "text": "knowledge, and policymakers establishing appropriate oversight. Multi-modal biomedical data accumula-\ntion, regulatory demands for explainable artificial intelligence, and documented failures of correlation-based\nmodels under distribution shift together create both opportunity and urgency. Translation to clinical prac-\ntice would enable healthcare systems to identify causal mechanisms rather than mere associations, predict\ninterventional outcomes rather than observational patterns, and generate patient-specific counterfactuals\nthat enable truly personalised medicine. This vision warrants continued investment provided that we main-\ntain rigorous standards for genuine causal inference in complex biological systems, distinguish mechanistic\nunderstanding from pattern recognition, and prioritise patient outcomes over algorithmic sophistication.\nReferences\n[1] Obermeyer, Z., Powers, B., Vogeli, C. & Mullainathan, S. Dissecting racial bias in an algorithm used\nto manage the health of populations. Science 366, 447–453 (2019).\n[2] Futoma, J., Simons, M., Panch, T., Doshi-Velez, F. & Celi, L. A. The myth of generalisability in\nclinical research and machine learning in health care. The Lancet Digital Health 2, e489–e492 (2020).\n[3] Zech, J. R. et al. Variable generalization performance of a deep learning model to detect pneumonia\nin chest radiographs: a cross-sectional study. PLoS Medicine 15, e1002683 (2018).\n[4] Caruana, R. et al. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day\nreadmission. Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery\nand data mining 1721–1730 (2015).\n[5] Castro, D. C., Walker, I. & Glocker, B. Causality matters in medical imaging. Nature Communications\n11, 3673 (2020).\n[6] Richens, J. G., Lee, C. M. & Johri, S. Improving the accuracy of medical diagnosis with causal machine\nlearning. Nature Communications 11, 3923 (2020).\n[7] Wynants, L., Van Calster, B., Collins, G. S. et al. Prediction models for diagnosis and prognosis of\ncovid-19: systematic review and critical appraisal. BMJ 369, m1328 (2020).\n[8] Roberts, M., Driggs, D., Thorpe, M. et al. Common pitfalls and recommendations for using machine\nlearning to detect and prognosticate for covid-19 using chest radiographs and ct scans. Nature Machine\nIntelligence 3, 199–217 (2021).\n[9] Kilbertus, N. et al. Avoiding discrimination through causal reasoning. Advances in Neural Information\nProcessing Systems 30 (2017).\n[10] Kusner, M. J., Loftus, J., Russell, C. & Silva, R.\nCounterfactual fairness.\nAdvances in Neural\nInformation Processing Systems 30, 4066–4076 (2017).\n[11] Pleˇcko, D. & Bareinboim, E. Causal fairness analysis: A causal toolkit for fair machine learning.\nFoundations and Trends in Machine Learning 17, 304–589 (2024).\n[12] Adams-Prassl, J., Binns, R. & Kelly-Lyth, A. Directly discriminatory algorithms. The Modern Law\nReview 86, 144–175 (2023).\n[13] Pearl, J. Causality (Cambridge university press, 2009).\n[14] Hern´an, M. A. & Robins, J. M. Causal inference: what if (2020).\n[15] Prosperi, M., Guo, Y., Sperrin, M. et al. Causal inference and counterfactual prediction in machine\nlearning for actionable healthcare. Nature Machine Intelligence 2, 369–375 (2020).\n[16] Barab´asi, A.-L., Gulbahce, N. & Loscalzo, J. Network medicine: a network-based approach to human\ndisease. Nature Reviews Genetics 12, 56–68 (2011).\n[17] Nicholson, D. N. & Greene, C. S. Constructing knowledge graphs and their biomedical applications.\nComputational and Structural Biotechnology Journal 18, 1414–1428 (2020).\n[18] Hamilton, W. L. Graph representation learning Vol. 14 of Synthesis Lectures on Artifical Intelligence\nand Machine Learning (2020).\n26\n"}, {"page": 27, "text": "[19] Battaglia, P. W. et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint\narXiv:1806.01261 (2018). URL https://arxiv.org/abs/1806.01261.\n[20] Kipf, T. N. & Welling, M. Semi-supervised classification with graph convolutional networks (2017).\nURL https://openreview.net/forum?id=SJU4ayYgl.\n[21] Duvenaud, D. K. et al. Convolutional networks on graphs for learning molecular fingerprints. Advances\nin neural information processing systems 28 (2015).\n[22] Ktena, S. I. et al. Metric learning with spectral graph convolutions on brain connectivity networks.\nNeuroImage 169, 431–442 (2018).\n[23] Stokes, J. M. et al. A deep learning approach to antibiotic discovery. Cell (2020).\n[24] Bevilacqua, B., Zhou, Y. & Ribeiro, B. Size-invariant graph representations for graph classification\nextrapolations. International Conference on Machine Learning 837–851 (2021).\n[25] Knyazev, B., Taylor, G. W. & Amer, M. Understanding attention and generalization in graph neural\nnetworks. Advances in Neural Information Processing Systems 32 (2019).\n[26] Sch¨olkopf, B. et al. Toward causal representation learning. Proceedings of the IEEE 109, 612–634\n(2021).\n[27] Locatello, F. et al. A commentary on the unsupervised learning of disentangled representations. AAAI\nConference on Artificial Intelligence 34, 13681–13684 (2020).\n[28] Regev, A., Teichmann, S. A., Lander, E. S. et al. The human cell atlas. eLife 6, e27041 (2017).\n[29] Topol, E. J. High-performance medicine: the convergence of human and artificial intelligence. Nature\nMedicine 25, 44–56 (2019).\n[30] Conroy, M. C. et al. Uk biobank: a globally important resource for cancer research. British Journal\nof Cancer 128, 519–527 (2023).\n[31] FDA et al. Transparency for machine learning-enabled medical devices: Guiding principles. US Food\nAnd Drug Administration. Retrieved June 30, 2024 (2024).\n[32] Murdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asl, R. & Yu, B. Definitions, methods, and appli-\ncations in interpretable machine learning.\nProceedings of the National Academy of Sciences 116,\n22071–22080 (2019).\n[33] Pleˇcko, D. & Bareinboim, E. Fairness-accuracy trade-offs: A causal perspective, 26344–26353 (2025).\n[34] Li, Y., Zhang, G., Wang, P., Yu, Z.-G. & Huang, G. Graph neural networks in biomedical data: a\nreview. Current Bioinformatics 17, 483–492 (2022).\n[35] Petersen, M. L. & van der Laan, M. J. Causal models and learning from data: integrating causal\nmodeling and statistical estimation. Epidemiology 25, 418–426 (2014).\n[36] Emmert-Streib, F. et al. The role of digital twins in p4 medicine: A paradigm for modern healthcare.\nnpj Digital Medicine 8, 735 (2025).\n[37] Laubenbacher, R. et al. Building digital twins of the human immune system: toward a roadmap. NPJ\ndigital medicine 5, 64 (2022).\n[38] Kovatchev, B. The year of transition from research to clinical practice. Nature Reviews Endocrinology\n14, 74–76 (2018).\n[39] Brown, S. A., Kovatchev, B. P., Raghinaru, D. et al. Six-month randomized, multicenter trial of\nclosed-loop control in type 1 diabetes. New England Journal of Medicine 381, 1707–1717 (2019).\n[40] Wein, S. et al. A graph neural network framework for causal inference in brain networks. Scientific\nreports 11, 8061 (2021).\n27\n"}, {"page": 28, "text": "[41] Pearl, J. An introduction to causal inference. The International Journal of Biostatistics 6, Article 7\n(2010).\n[42] Zeˇcevi´c, M., Dhami, D. S., Veliˇckovi´c, P. & Kersting, K. Relating graph neural networks to structural\ncausal models. arXiv (2021).\n[43] Chalupka, K., Eberhardt, F. & Perona, P. Causal feature learning: an overview. Behaviormetrika 44,\n137–164 (2017).\n[44] Rubenstein, P. K. et al.\nCausal consistency of structural equation models.\narXiv preprint\narXiv:1707.00819 (2017).\n[45] Woodward, J. Making Things Happen: A Theory of Causal Explanation (Oxford University Press,\n2005).\n[46] Chevalley, M., Roohani, Y. H., Mehrjou, A., Leskovec, J. & Schwab, P. A large-scale benchmark for\nnetwork inference from single-cell perturbation data. Communications Biology 8, 412 (2025).\n[47] Gonzalez, G. et al.\nCombinatorial prediction of therapeutic perturbations using causally-inspired\nneural networks. bioRxiv 2024–01 (2025).\n[48] Liu, L., Lu, H., Whelan, M., Chen, Y. & Ding, X. CiGNN: A causality-informed and graph neural net-\nwork based framework for cuffless continuous blood pressure estimation. IEEE Journal of Biomedical\nand Health Informatics 28, 2674–2686 (2024).\n[49] Fan, S., Wang, X., Mo, Y., Shi, C. & Tang, J. Debiasing graph neural networks via learning disen-\ntangled causal substructure. Advances in Neural Information Processing Systems 35, 24934–24946\n(2022).\n[50] Zheng, K., Yu, S. & Chen, B.\nCI-GNN: A Granger causality-inspired graph neural network for\ninterpretable brain network-based psychiatric diagnosis. Neural Networks 172, 106147 (2024).\n[51] Ma, J., Guo, R., Mishra, S., Zhang, A. & Li, J. Clear: Generative counterfactual explanations on\ngraphs. Advances in neural information processing systems 35, 25895–25907 (2022).\n[52] Job, S. et al. Exploring causal learning through graph neural networks: An in-depth review. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge Discovery 15, e70024 (2025).\n[53] VanderWeele, T. J. & Ding, P. Sensitivity analysis in observational research: Introducing the E-value.\nAnnals of Internal Medicine 167, 268–274 (2017).\n[54] Gu, Y., Han, F. & Liu, X. Arousal contributions to resting-state fMRI connectivity and dynamics.\nFrontiers in Neuroscience 13, 1190 (2019).\n[55] ˇCuklina, J. et al. Diagnostics and correction of batch effects in large-scale proteomic studies: a tutorial.\nMolecular Systems Biology 17, e10240 (2021).\n[56] Liu, R., Xie, Y., Lin, D., Zhang, W. & Ding, S. X. Information-based gradient enhanced causal learning\ngraph neural network for fault diagnosis of complex industrial processes. Reliability Engineering &\nSystem Safety 252, 110540 (2024).\n[57] Wang, X. et al. Reinforced causal explainer for graph neural networks. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence 45, 2297–2309 (2022).\n[58] Liu, L., Zhang, Y.-T., Wang, W., Chen, Y. & Ding, X. Causal inference based cuffless blood pressure\nestimation: A pilot study. Comput. Biol. Med. 159 (2023).\n[59] Lian, Q., Wang, Y. & Qi, Y. Dynamic instance-level graph learning network of intracranial elec-\ntroencephalography signals for epileptic seizure prediction. IEEE Journal of Biomedical and Health\nInformatics (2025).\n[60] Behnam, A. & Wang, B. Graph neural network causal explanation via neural causal models. European\nConference on Computer Vision 410–428 (2024).\n28\n"}, {"page": 29, "text": "[61] Behnam, A. et al. Causal explanation from mild cognitive impairment progression using graph neural\nnetworks, 6349–6355 (IEEE, 2024).\n[62] Peters, J., Janzing, D. & Sch¨olkopf, B.\nElements of causal inference: Foundations and learning\nalgorithms. MIT Press (2017).\n[63] Zhang, J., Zang, X., Chen, H., Yan, X. & Tang, B. Revisiting drug recommendation from a causal\nperspective. IEEE Journal of Biomedical and Health Informatics (2024).\n[64] Wu, Y.-X., Wang, X., Zhang, A., He, X. & Chua, T.-S. Discovering invariant rationales for graph\nneural networks. arXiv preprint arXiv:2201.12872 (2022).\n[65] Pleˇcko, D. & Bareinboim, E. Reconciling predictive and statistical parity: A causal approach 38,\n14625–14632 (2024).\n[66] He, Y., Dong, S., Lin, Y., Zheng, X. & Hu, J.\nPretraining-based relevance-aware visit similarity\nnetwork for drug recommendation. IEEE Journal of Biomedical and Health Informatics (2025).\n[67] Mohammadi, H. & Karwowski, W. Graph neural networks in brain connectivity studies: Methods,\nchallenges, and future directions. Brain Sciences 15, 17 (2024).\n[68] Zheng, K., Yu, S., Chen, L., Dang, L. & Chen, B.\nBpi-gnn: Interpretable brain network-based\npsychiatric diagnosis and subtyping. NeuroImage 292, 120594 (2024).\n[69] Saha, R. et al. A method to estimate longitudinal change patterns in functional network connectivity\nof the developing brain relevant to psychiatric problems, cognition, and age. Brain Connectivity 14,\n130–140 (2024).\n[70] Valous, N. A., Popp, F., Z¨ornig, I., J¨ager, D. & Charoentong, P. Graph machine learning for integrated\nmulti-omics analysis. British journal of cancer 131, 205–211 (2024).\n[71] Li, N. et al. Cautiongcn: Cancer subtype classification by developing causal multi-head autoencoder\nand graph convolutional network, 626–634 (IEEE, 2024).\n[72] Wang, T. et al. Mogonet integrates multi-omics data using graph convolutional networks allowing\npatient classification and biomarker identification. Nature Communications 12, 3445 (2021).\n[73] Hu, D., Liu, B., Zhu, X., Lu, X. & Wu, N. Kamln: A knowledge-aware multi-label network for lung\ncancer complication prediction, 1–5 (IEEE, 2024).\n[74] Schmid, F., Goepfert, M. S. & Reuter, D. A. Patient monitoring alarms in the icu and in the operating\nroom. Critical care 17, 216 (2013).\n[75] Jing, B., Zhou, D., Ren, K. & Yang, C. Casper: Causality-aware spatiotemporal graph neural networks\nfor spatiotemporal time series imputation.\narXiv preprint arXiv:2403.11960 (2024).\nURL https:\n//arxiv.org/abs/2403.11960.\n[76] Wang, D. et al. Hierarchical graph neural networks for causal discovery and root cause localization.\narXiv preprint arXiv:2302.01987 (2023).\n[77] Zhang, X. et al. Mocagcn: Cancer subtype classification by developing causal graph structure learning.\n2024 IEEE International Conference on Medical Artificial Intelligence (MedAI) 617–625 (2024).\n[78] Grieves, M. W.\nProduct lifecycle management: the new paradigm for enterprises.\nInternational\nJournal of Product Development 2, 71–84 (2005).\n[79] Coorey, G., Figtree, G. A., Fletcher, D. F. & Redfern, J. The health digital twin: advancing precision\ncardiovascular medicine. Nature Reviews Cardiology 18, 803–804 (2021).\n[80] Wu, C., Lorenzo, G., Hormuth, D. A. et al. Integrating mechanism-based modeling with biomedical\nimaging to build practical digital twins for clinical oncology. Biophysics Reviews 3, 021304 (2022).\n[81] Mesinovic, M., Watkinson, P. & Zhu, T.\nExplainability in the age of large language models for\nhealthcare. Communications Engineering 4, 128 (2025).\n29\n"}, {"page": 30, "text": "[82] Toonsi, S., Gauran, I. I., Ombao, H., Schofield, P. N. & Hoehndorf, R. Causal relationships between\ndiseases mined from the literature improve the use of polygenic risk scores. Bioinformatics 40, btae639\n(2024).\n[83] Chelli, M. et al. Hallucination rates and reference accuracy of ChatGPT and Bard for systematic\nreviews: Comparative analysis. Journal of Medical Internet Research 26, e53164 (2024).\n[84] Chen, Q. et al.\nBenchmarking large language models for biomedical natural language processing\napplications and recommendations. Nature Communications 16, 3280 (2025).\n[85] Nazi, Z. A. & Peng, W.\nLarge language models in healthcare and medical domain: A review.\nInformatics 11, 57 (2024).\n[86] FDA et al. Use of real-world evidence to support regulatory decision-making for medical devices.\nGuidance for industry and Food and Drug Administration staff. USFDA, Silver Spring, Maryland,\nUSA (2017).\n[87] Nastl, V. Y. & Hardt, M. Do causal predictors generalize better to new domains? 37, 31202–31315\n(2024). NeurIPS 2024 Spotlight.\n[88] Cui, H. et al. BrainGB: A benchmark for brain network analysis with graph neural networks. IEEE\nTransactions on Medical Imaging 42, 493–506 (2023).\n[89] Feuerriegel, S. et al. Causal machine learning for predicting treatment outcomes. Nature Medicine\n30, 958–968 (2024).\n[90] Subbaswamy, A. & Saria, S. From development to deployment: Dataset shift, causality, and shift-stable\nmodels in health AI. Biostatistics 21, 345–352 (2020).\n[91] Hern´an, M. A. & Robins, J. M. Using big data to emulate a target trial when a randomized trial is\nnot available. American Journal of Epidemiology 183, 758–764 (2016).\n[92] Lawlor, D. A., Tilling, K. & Davey Smith, G. Triangulation in aetiological epidemiology. International\nJournal of Epidemiology 45, 1866–1886 (2016).\n[93] Hill, A. B. The environment and disease: Association or causation? Proceedings of the Royal Society\nof Medicine 58, 295–300 (1965).\n[94] Dunning, T.\nNatural Experiments in the Social Sciences: A Design-Based Approach (Cambridge\nUniversity Press, 2012).\n[95] Cinelli, C. & Hazlett, C. Making sense of sensitivity: Extending omitted variable bias. Journal of the\nRoyal Statistical Society Series B: Statistical Methodology 82, 39–67 (2020).\n[96] Arjovsky, M., Bottou, L., Gulrajani, I. & Lopez-Paz, D. Invariant risk minimization. arXiv preprint\narXiv:1907.02893 (2019).\n[97] Vandenbroucke, J. P. et al.\nStrengthening the reporting of observational studies in epidemiology\n(STROBE): explanation and elaboration. International Journal of Surgery 12, 1500–1524 (2014).\n[98] Hemkens, L. G. et al. Interpretation of epidemiologic studies very often lacked adequate consideration\nof confounding. Journal of clinical epidemiology 93, 94–102 (2018).\n[99] Bakris, G. L. et al. Effect of finerenone on chronic kidney disease outcomes in type 2 diabetes. New\nEngland Journal of Medicine 383, 2219–2229 (2020).\n[100] Smucny, J., Shi, G. & Davidson, I.\nDeep learning in neuroimaging: overcoming challenges with\nemerging approaches. Frontiers in Psychiatry 13, 912600 (2022).\n[101] Guan, H. et al. Multi-site MRI harmonization via attention-guided deep domain adaptation for brain\ndisorder identification. Medical Image Analysis 71, 102076 (2021).\n[102] Keogh, R. H., Gran, J. M., Seaman, S. R., Davies, G. & Vansteelandt, S. Causal inference in sur-\nvival analysis using longitudinal observational data: Sequential trials and marginal structural models.\n30\n"}, {"page": 31, "text": "Statistics in Medicine 42, 2191–2225 (2023).\n[103] Ren, W., Liu, H. et al. Moving beyond medical statistics: A systematic review on missing data handling\nin electronic health records. Health Data Science 4, 0176 (2024).\n[104] Beede, E. et al. A human-centered evaluation of a deep learning system deployed in clinics for the\ndetection of diabetic retinopathy, 1–12 (2020).\nAcknowledgements.\nMM is supported by the Rhodes Trust and the EPSRC CDT Health Data Science.\nTZ is supported by the Royal Academy of Engineering.\nAuthor contributions.\nCompeting interests.\nWe declare no competing interests.\nSupplementary information.\nSupplementary material is contained in a separate file.\n31\n"}]}