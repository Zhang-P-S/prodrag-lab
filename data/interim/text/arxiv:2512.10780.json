{"doc_id": "arxiv:2512.10780", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.10780.pdf", "meta": {"doc_id": "arxiv:2512.10780", "source": "arxiv", "arxiv_id": "2512.10780", "title": "Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting", "authors": ["Manurag Khullar", "Utkarsh Desai", "Poorva Malviya", "Aman Dalmia", "Zheyuan Ryan Shi"], "published": "2025-12-11T16:15:42Z", "updated": "2025-12-11T16:15:42Z", "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.10780v1", "url_pdf": "https://arxiv.org/pdf/2512.10780.pdf", "meta_path": "data/raw/arxiv/meta/2512.10780.json", "sha256": "01553b3f2c53c93b588f1c631322e002421ee686905630b7f9ce0eb6d7be9405", "status": "ok", "fetched_at": "2026-02-18T02:24:25.160540+00:00"}, "pages": [{"page": 1, "text": "Script Gap: Evaluating LLM Triage on Indian Languages in Native\nvs Roman Scripts in a Real World Setting\nManurag Khullar*, Utkarsh Desai, Poorva Malviya, Aman Dalmia, Zheyuan Ryan Shi*\n*School of Computing and Information, University of Pittsburgh\nAbstract\nLarge Language Models (LLMs) are increasingly deployed in high-\nstakes clinical applications in India. In many such settings, speakers\nof Indian languages frequently communicate using romanized text\nrather than native scripts, yet existing research rarely evaluates\nthis orthographic variation using real-world data. We investigate\nhow romanization impacts the reliability of LLMs in a critical do-\nmain: maternal and newborn healthcare triage. We benchmark\nleading LLMs on a real-world dataset of user-generated queries\nspanning five Indian languages and Nepali. Our results reveal con-\nsistent degradation in performance for romanized messages, with F1\nscores trailing those of native scripts by 5‚Äì12 points. At our partner\nmaternal health organization in India, this gap could cause nearly\n2 million excess errors in triage. Crucially, this performance gap by\nscripts is not due to a failure in clinical reasoning. We demonstrate\nthat LLMs often correctly infer the semantic intent of romanized\nqueries. Nevertheless, their final classification outputs remain brit-\ntle in the presence of orthographic noise in romanized inputs. Our\nfindings highlight a critical safety blind spot in LLM-based health\nsystems: models that appear to understand romanized input may\nstill fail to act on it reliably.\nKeywords\nromanized text, native text, multilingual Natural Language Process-\ning, medical triage, benchmarking, evaluation\n1\nIntroduction\nAccess to timely and reliable healthcare in many communities in\nthe global south is severely constrained [25]. Shortages of medical\nprofessionals, overburdened primary-care systems, and linguistic di-\nversity limit patients‚Äô ability to obtain high-quality guidance. Large\nlanguage models (LLMs) offer a promising path to expand scalable,\nmultilingual health communication where traditional services are\nstretched thin. LLMs are already being explored for patient-facing\napplications, such as message triage, portal-message prioritization,\nand safety-oriented medical question answering [8, 16, 20, 24, 36].\nThese systems aim to flag emergencies in patient portal messages,\nroute complex queries to clinicians, and generate draft replies that\nreduce message burden for care teams [9, 30].\nHowever, existing LLMs still exhibit several limitations that di-\nrectly affect their suitability for clinical use. They can perform\ninconsistently across languages, hallucinate medical facts, and ex-\nhibit unstable reasoning under minor input perturbations. These\nissues are exacerbated for low-resource languages and informal,\npatient-generated text, where variation in spelling, phrasing, and\ncode-mixing can substantially degrade model reliability.\nA growing body of healthcare-focused LLM benchmarks aims\nto assess these risks, but they fall short in several ways. Many are\nsourced from clinical literature, or expert-curated exams, and fea-\nture synthetic question answering, rather than real patient-provider\nconversations [4, 14, 15, 27]. Only a small number of datasets feature\nreal conversations but are limited to English and Chinese [12, 23],\nand virtually none assess triage, arguably the task most relevant\nfor early patient engagement. As a result, current benchmarks fail\nto capture the linguistic and situational diversity encountered in\nfrontline care. No wonder a growing body of work shows LLMs can\nproduce clinically unsafe or inconsistent advice, even when overall\naccuracy on QA benchmarks is high [6, 10, 11].\nFurthermore, beyond the healthcare setting, handling informal,\nmixed-script communication at scale remains a challenge to multi-\nlingual LLMs. For Indian languages in particular, romanized input\nis pervasive in online communications, yet existing multilingual\nbenchmarks minimally cover them [17, 34, 35, 39]. Furthermore,\nscript representation is often studied in highly curated and parallel\ntransliterated corpora, but not on noisy patient messages [13, 40].\nAs a result, we lack an understanding of how script choice affects\nLLM performance in high-stakes domains such as medical triage.\nIn this paper, we provide the first benchmarking evaluation of\nreal-world LLM triage performance on Indian languages across\nboth native and Roman scripts. Our contributions are threefold: (1)\nWe construct a multilingual, multi-script healthcare triage dataset\nreflecting based on authentic patient-provider conversations. (2) We\nbenchmark several leading LLMs on this dataset, revealing for the\nfirst time substantial script-dependent disparities. (3) We conduct\nexhaustive error analyses to uncover where and why models dispro-\nportionately fail on romanized inputs. We point out that romanized\ninputs lead to brittle decision boundaries, even when models seem\nto generate rationales that capture the user‚Äôs intent.\nWe partner with Maternal Health Organization A1, a nonprofit\nin India whose programs have reached over 41 million caregivers\nand patients across nine Indian states since 2014. This study builds\non real-world patient-provider message data collected as part of\nMaternal Health Organization A‚Äôs pilot studies for evaluating LLM\nsafety, where triage remains a critical step in early engagement.\nWith Maternal Health Organization A alone, the script gap revealed\nin this work could cause nearly 2 million excess errors in LLM-based\ntriage. More broadly, our work is also applicable to the many other\nhealthcare platforms currently exploring LLM-powered solutions\nto enhance care delivery. We provide a concrete framework for how\nto evaluate triage performance and insight into the script gap in\nfrontier models.\n2\nRelated Work\nLLM triage and medical QA benchmarks. Most medical LLM\nbenchmarks are built on structured QA rather than free-form patient\n1Anonymized organization name in this version for peer review.\narXiv:2512.10780v1  [cs.CL]  11 Dec 2025\n"}, {"page": 2, "text": "chats. Multiple-choice datasets such as PubMedQA and exam-style\nbenchmarks like MedQA and MedMCQA evaluate factual recall and\nreasoning on well-formed questions, not conversational triage with\nfragmented, colloquial symptom descriptions [14, 15, 27]. Broader\nclinical evaluation suites such as HealthBench, aggregate diverse\nNLP tasks and rubric-based judgments for LLMs but rely largely\non synthetic prompts, exam questions, or curated case descrip-\ntions instead of patient-authored dialogues [4]. Other resources\nmine user queries from consumer health websites but automatically\ngenerate answers with retrieval systems or LLMs, decoupling real\ninformation needs from human-written responses [1, 32]. Only a\nfew corpora contain real patient‚Äìprovider conversations. MedDi-\nalog focuses on Chinese online consultations [12], while newer\ndatasets such as AfriMed-QA include patient-style health question\ntarget African settings but remain mainly English, small-scale, and\nnon-interactive [23, 26]. Even in these cases, conversations are\ncomparatively well-typed and largely monolingual. In contrast, we\nstudy multilingual triage on genuine chat-app dialogues with typos,\ncode-mixing, and mixed scripts, and evaluate safety-critical triage\nbehavior rather than generic medical QA.\nBenchmarks for Romanized text. Beyond healthcare, there is rich\nwork on code-mixed and romanized text. LinCE provides a multi-\ntask benchmark over language identification, sentiment analysis,\netc, for Hindi-English romanized social media code-switching, with\nstandardized splits and metrics [2]. COMI-LINGUA adds a large\nexpert-annotated Hindi-English suite across Native and Roman\nscripts. Its dual-script LLM first generates Hindi references and\nthen post-edited, which risks biasing romanization toward stan-\ndardized spellings rather than organically typed variants [33]. For\nIndic transliteration, Dakshina provides word and sentence level\npairs for Indian languages, with native forms drawn from Wikipedia\nand romanizations attested by annotators [31]. Another dataset,\nAksharantar, scales by mining parallel corpora, large monolingual\ncorpora (IndicCorp), Wikidata, and manually annotated translit-\nerations [22]. Overall, these datasets are central to transliteration\nbut rely on standardized inputs, lacking the messy, conversational\ncomplexity of actual patient interactions\nImpact of script variation on model performance in Indian lan-\nguages. Most clinical LLM evaluations still assume well-formed\nEnglish input and ignore script variation. Within Indian natural lan-\nguage processing (NLP), however, several studies show that script\nchoice materially affects model behavior: Bhasha-Abhijnaanam ex-\nplicitly compare native script and Roman script inputs, finding large\ngaps in language identification and character level modeling for\nIndic languages, especially under user-generated romanization [21].\nIn recent work, RomanLID treats romanised text as noisy and shows\nimprovements on language identification, and recent work uses\nLLMs directly as normalizers to map transliterated and dialectal\ntext into standardized forms for downstream machine translation\ntask [3, 7]. At the task level, RomanSetu and DualScript-style models\nshow that carefully designed romanization and joint native‚ÄìRoman\ntraining can improve downstream tasks [13, 40], but these results\nare reported on curated or machine-transliterated corpora rather\nthan colloquial user chats.\n3\nDataset and Task\n3.1\nClinical Setting and Corpus\nWe study a de-identified corpus of short WhatsApp messages re-\nlated to maternal and newborn care provided by Maternal Health\nOrganization A. Users, typically mothers, pregnant women, or care-\ngivers, use a WhatsApp chat interface to submit free-text queries\nabout pregnancy and newborn issues; these messages are handled\nby multilingual Medical Support Executives (MSEs), referred to as\nmedical experts in this study, who provide counseling, triage guid-\nance, and escalation advice via chat. For this work, we restrict atten-\ntion to a fixed observation window and use only de-identified data,\nyielding a corpus of approximately 133k conversations spanning\nEnglish, five Indian languages: Hindi, Telugu, Kannada, Marathi,\nand Punjabi, and Nepali. From this pool, we draw a stratified ran-\ndom sample of 3,156 single-turn user messages for experiments,\nensuring coverage across languages and message lengths. Only\nuser-authored messages are provided as inputs to the models; the\ncorresponding medical expert replies are used solely as downstream\nclinical context for constructing pseudo‚Äìground truth labels. Fig-\nure 1 illustrates typical exchanges between users and medical ex-\nperts.\nFigure 1: Illustration of user and medical expert exchanges\nin different scripts. Top: An Emergency user message in noisy\nRoman Hindi with a medical expert response in native Hindi\nscript. Bottom: A Non-emergency user message in native\nscript Punjabi with a response in the same language. Each\npanel also includes English translations to support broader\naccessibility and interpretation.\n3.2\nScript and Language Annotation\nEvery user message is tagged as English if its dominant language\nis English and it is written in the Roman script; as Native script\nif it contains any characters from Indian Unicode blocks and its\ndominant language is one of our target Indian languages; and as\n2\n"}, {"page": 3, "text": "Roman script if its dominant language is an Indian language but\nthe message contains Roman script, possibly with non-standard\nspelling and code-mixing. Thus, Roman messages are not English,\nbut Indian-language content rendered in Roman script. We use\nGPT-4o to language-annotate and script-annotate data. To assess\nannotation reliability, we manually reviewed a random sample of\n200 messages and compared the inferred language and script_type\ntags to human judgements from annotators fluent in English and\nthe relevant Indian languages. In addition, we used medical expert\nresponses as an auxiliary signal, since they are multilingual and\ntypically respond in the user‚Äôs language. In 97.0% of sampled cases,\nthe automatic labels matched the human assessment.\n3.3\nTriage Labels and Definition\nThe triage task uses three mutually exclusive labels: Emergency,\nNon-emergency, and Insufficient Information. Briefly, Emergency\ncovers messages that indicate, or could plausibly indicate, symp-\ntoms requiring urgent medical attention; Non-emergency covers\nconcerns that clearly do not require urgent care such as routine\ndiet, breastfeeding, sleep, or administrative questions and Insuffi-\ncient Information is reserved for vague or underspecified messages\nwhere it is not possible to determine emergency status. In designing\nthis label, we follow prior work that explicitly models uncertainty\nvia dedicated categories such as ‚Äúnot enough information‚Äù in fact-\nchecking, unanswerable questions in Question Answer, uncertain\nassertions in clinical NLP, ambiguity-sensitive bias benchmarks,\nand abstention in selective prediction [5, 28, 29, 37, 38, 41].\n3.4\nDatasets\nTable 1: Language distribution in P set, by script types. Eng-\nlish messages serve only as a baseline.\nLanguage\nNative\nRoman\nTotal\nShare (%)\nEnglish\n-\n-\n323\n10.2\nHindi\n145\n717\n862\n27.3\nTelugu\n140\n383\n523\n16.6\nKannada\n252\n211\n463\n14.7\nMarathi\n252\n166\n418\n13.2\nPunjabi\n128\n227\n355\n11.2\nNepali\n125\n87\n212\n6.7\nTotal\n1,042\n1,791\n3,156\n100.0\nAll headline analyses in this paper are based on a pseudo-labeled\nprimary set P (Section 3.4) and a smaller human-annotated set H\nis used only for validation (Section 3.4) of primary set P.\nPrimary set P. For each message in the primary dataset P (3,156\nuser messages obtained via the stratified sampling procedure de-\nscribed in Section 3.1), we derive a pseudo-triage label using an LLM\nensemble. Descriptive statistics for P are reported in Table 1, while\nthe ensemble-based labeling procedure is detailed in Section 3.5.\nGold set H (validation only). A human-annotated subset H of\n300 messages is drawn from the same corpus. For H, trained an-\nnotators with clinical backgrounds assign triage labels directly to\nuser messages according to a standardized guideline. We use H\nonly to evaluate the quality of the ensemble pseudo-labels on P, by\ncomputing accuracy and F1 relative to the human labels. This sub-\nset serves exclusively as an external validation set for the medical\nexpert-response-based pseudo-labeling strategy.\n3.5\nEnsemble-based Pseudo-labeling\nTo construct the ensemble, we begin with a selected pool of frontier\nLLMs from multiple providers, including GPT-4o, Claude 4.5 Son-\nnet, LLaMA 4 Maverick, DeepSeek-V3, Qwen 3-80B, and Sarvam.\nTo prevent over-dependence on any individual model family, we\nsystematically evaluate all three-model majority-vote ensembles\nassembled from this pool on the gold standard dataset H, utiliz-\ning metrics such as weighted F1 score and per-label recall. Among\nthese, the ensemble comprising GPT-4o, Claude 4.5, and Qwen 3-\n80B demonstrates the optimal balance between overall performance\nand per label recall, thereby serving as the foundational backbone\nfor pseudo-labeling.\nFor each message in P, we construct a prompt that contains\n(i) the user message and (ii) its corresponding response from the\nmedical expert (Figure 1). Each ensemble member returns a deter-\nministic JSON object with a triage label. This procedure yields a\nsingle script- and language-aware pseudo-label per message.\nWe then evaluate this ensemble on the gold set H to assess how\nwell its pseudo-labels approximate clinician judgements. Table 2\nreports weighted F1 and per-label recall: the ensemble attains 89.8%\nweighted F1, with strong recall on Emergency (86.8%) and Non-\nemergency (96.8%). Recall is lower for the Insufficient Information\nclass (46.7%), reflecting its role as a residual boundary label for\ngenuinely ambiguous or underspecified messages. As a result, the\npseudo-labels in P act as a reasonably faithful proxy for medical\nexpert-level triage.\nTable 2: Performance of the ensemble (GPT-4o + Claude 4.5 +\nQwen3-80B) on the gold data H.\nMetric\nScore (%)\nF1\n89.8\nEmergency recall\n86.8\nNon-emergency recall\n96.8\nInsufficient Information recall\n46.7\n4\nExperimental Setup\n4.1\nTask\nThe primary objective is triage classification: given a user message,\nthe system must assign one of three triage labels. All models oper-\nate on the raw user messages, without normalization of spelling,\npunctuation, or code-mixing, and are evaluated using a single fixed\nprompt template, temperature = 0 and full context to ensure parity\nacross models. In addition to the discrete label, models are instructed\nto produce a brief, one- to two-sentence natural language rationale\nsummarizing the reasoning behind the predicted label.\n3\n"}, {"page": 4, "text": "4.2\nModels\nWe evaluate a variety of proprietary and open-weight large lan-\nguage models, organizing them into three buckets: frontier propri-\netary models, large open-weight models, and a compact-plus-Indic-\nspecialized bucket. Our primary focus is on relatively high-capacity\nsystems, complemented by a smaller pack of competitive mid-sized\nand Indic-focused models.\nThe frontier proprietary bucket includes GPT-4o, a recent flagship\nmodel achieving state-of-the-art results on many multilingual and\nmultimodal benchmarks, and Claude Sonnet 4.5 from Anthropic,\ndesigned for advanced reasoning and multimodal processing. The\nlarge open-weight bucket comprises DeepSeek V3, LLaMA 4 Mav-\nerick, and Qwen3-80B, all of which are reported to have strong\nmultilingual capabilities. These two large model baselines aim to\napproximate the best performance that current LLMs can reason-\nably achieve on our triage task.\nThe compact-plus-Indic bucket consists of GPT-OSS-20B , Mixtral-\n7B and Qwen2.5-7B, three mid-sized models that offer strong multi-\nlingual capabilities and are attractive from an efficiency perspective,\ntogether with Sarvam, an Indian language-specialized model. This\ndesign allows us to test whether the performance gap between\nnative script and romanized messages is consistent across model\nfamilies and capacities, rather than being an artifact of any single\narchitecture or training pipeline.\n4.3\nPrompt Template\nTo select the final fixed prompt, we conducted a small prompt-\ndesign sweep on the human-labelled subset H, comparing four\nstrategies: (i) a minimal zero-shot instruction that asks the model\nto assign one of three labels with no additional guidance; (ii) a few-\nshot prompt with labelled examples; (iii) a chain-of-thought style\nprompt augmented with a compact triage knowledge base; and (iv)\na structured standard operating procedure and knowledge base\nprompt (SOP+KB) that first injects a concise excerpt of triage guide-\nlines and standard operating procedures, and then instructs the\nmodel to match reported symptoms against an explicit emergency\nsymptom knowledge base. The KB+SOP prompt can be viewed\nas a lightweight, prompt-only analogue of retrieval-augmented\ngeneration [19] and chain-of-thought style prompting: rather than\nquerying an external index at runtime, the relevant triage knowl-\nedge and procedural steps are embedded directly in the prompt,\nand the model is instructed to reason stepwise using this context.\nIn our pilot comparison on H, the SOP+KB prompt consistently\nachieved higher F1 than baselines. We therefore fix this SOP + KB\ntemplate, summarised in Figure 2, and the template in Appendix B,\nand compute all reported metrics and subgroup analyses under this\nshared evaluation protocol.\n5\nResults\n5.1\nOverall Benchmarking\nFigure 3 summarizes overall F1 scores by model and script type on\nP. On the full set, the strongest systems are the large frontier and\nopen-weight models: Claude 4.5 Sonnet, GPT-4o, Llama 4 Maverick,\nQwen3-80B, DeepSeek-V3 and all reach around 80% -73% F1 accu-\nracy. GPT4 OSS-20B, Sarvam and Qwen2.5 7B forming a mid-tier\nPrompt design for triage classification\nStep 1: Role and input.\nYou are a medical triage assistant for maternal and newborn care.\nYou will be given: - the user‚Äôs message (mother/caregiver/pregnant\nwoman).\nStep 2: Label space.\nYour task is to classify the message into one of three Labels::\n‚Ä¢ Emergency: Definition\n‚Ä¢ Non-Emergency: Definition\n‚Ä¢ Insufficient Information: Definition\nStep 3: Triage rules.\nStep 4: Emergency-symptom knowledge base (sketch).\n‚Ä¢ antenatal care: [example red-flag symptoms ...]\n‚Ä¢ postnatal care: [example red-flag symptoms ...]\nStep 5: Output format (JSON only, no extra text).\nReturn a single JSON object:\n{\n\"label\": \"Emergency\"\n| \"Non-Emergency\"\n| \"Insufficient Information\",\n\"reasoning_summary\":\n\"<1--2 sentence justification of the label>\"\n}\nFigure 2: SOP + KB Prompt Design and output schema used\nfor all models. Full prompt text, label definitions, triage rules,\nand symptom lists are provided in Appendix B).\nand Mixtral 8x7B lagging behind substantially. However, once we\nstratify by script, a consistent pattern emerges: for every model,\nperformance on romanized messages is strictly worse than on both\nEnglish and native scripts. For high-capacity models like Claude\n4.5, GPT-4o, Qwen3-80B, and Llama 4, F1 scores on romanized text\nare usually 5 to 13 points lower than on the best English or native\nscripts for the same model. On average, messages in roman scripts\nlag behind native scripts by roughly 5-12 points in F1. Even Sarvam,\nwhich is specifically optimized for Indian languages, performs the\nweakest on Indian language texts written in Roman scripts.\n5.2\nBenchmark by Language\nTable 3 reports F1 by language, script, and model on the primary\nset P. Across all Indian languages, inputs written in native scripts\nsystematically outperform their romanized counterparts across all\nmodels. The gap is modest for Hindi: for example, Claude 4.5 attains\n84.8% in native script and 81.6% in Roman. It becomes substantial\nfor Kannada, Telugu, Marathi, and Nepali, where performance on\nromanized messages often drops by 10‚Äì20 points relative to native\nscript; Qwen3 on Kannada reaches 83.7% in native script and 57.3%\nin Roman, and Claude 4.5 on Marathi reaches 78.6% in native script\nand 61.4% in Roman script. Punjabi shows a more moderate but still\nconsistent deficit on romanized messages. English, which serves as\na baseline, achieves strong scores that are comparable to the best\nnative script results, suggesting that the degradation is specific to\n4\n"}, {"page": 5, "text": "All\nEnglish\nNative\nRoman\n30%\n40%\n50%\n60%\n70%\n80%\n90%\nClaude 4.5\nGPT-4o\nLlama 4-17B\nQwen3-80B\nDeepSeek-V3\nGPT OSS-20B\nSarvam\nQwen2.5-7B\nMixtral-8x7B\nF1 (%)\nFigure 3: F1 performance comparison of models on the P set.\nIndian languages written in Roman script rather than to the Roman\nscript itself.\nTable 3: F1 Performance by language, script, and top 5 models\non P (%).\nLanguage Script\nClaude GPT-4o LLaMA-4 Qwen3 DeepSeek\nEnglish\nEnglish\n81.7\n78.9\n78.6\n82.4\n76.5\nHindi\nNative\n84.8\n80.0\n80.0\n82.1\n75.2\nRoman\n81.6\n77.5\n74.9\n77.1\n71.8\nKannada\nNative\n83.7\n75.4\n78.6\n83.7\n77.8\nRoman\n65.4\n68.7\n69.7\n57.3\n54.0\nTelugu\nNative\n82.9\n81.4\n73.6\n82.1\n77.9\nRoman\n71.5\n67.6\n65.3\n58.2\n65.0\nPunjabi\nNative\n81.2\n85.2\n80.5\n85.2\n80.5\nRoman\n77.1\n71.8\n70.0\n69.2\n66.5\nMarathi\nNative\n78.6\n79.4\n73.0\n79.0\n74.2\nRoman\n61.4\n63.3\n63.3\n53.0\n57.8\nNepali\nNative\n83.1\n73.6\n79.2\n75.2\n67.2\nRoman\n73.6\n62.1\n59.8\n63.2\n48.3\n5.3\nCross Model Agreement\nFollowing prior work on deep ensembles, we interpret lower cross-\nmodel consensus as higher epistemic uncertainty [18]. Figure 4a\ncharacterizes, for each script type, the distribution of queries by\ntheir maximum model-consensus level. Only 52.2% of romanized\nqueries reach full (100%) agreement, compared to 63.9% for English\nand 60.4% for native script messages. Moreover, 27.3% of roman-\nized messages fall into the intermediate 40‚Äì60% consensus bin (i.e.,\n2‚Äì3 of 5 models agreeing), versus 18.9% for English and 19.3% for\nnative script messages. Taken together, these trends indicate that\nromanized messages systematically shift toward lower cross-model\nconsensus, suggesting that script type modulates perceived uncer-\ntainty in triage decisions.\n5.4\nError Analysis\n5.4.1\nConfusion Matrix Analysis. Averaging over the top five per-\nforming models in Section 5.1, the rate of missed Emergency cases\nrises from 5.3% on English and 6.5% on native script messages to\n7.7% on romanized messages. The most pronounced gap appears for\nthe Insufficient Information label: when the true label is not Insuf-\nficient Information, the models predict it for 11.0% of native script\nmessages, 13.9% of English messages, and 20.15% of romanized mes-\nsages. Similarly, even on relatively straightforward Non-Emergency\ncases, the models incorrectly predict Insufficient Information for\n13.5% of romanized messages, 9.6% of English messages, and 7.0%\nof native script messages. We illustrate these patterns with Claude\n4.5 in Figure 4b, the top-performing model in Section 5.1. We illus-\ntrate this with an example of Claude 4.5 in Figure 4b, which is the\ntop-performing model in Section 5.1.\n5.4.2\nImpact of Code Switching on Performance. In Table 4, we\ndo a fine grained analysis of messages in roman script, separating\ncode-mixed from non-code-mixed romanized messages. Averaged\n5\n"}, {"page": 6, "text": "English\nNative\nRoman\nConsensus level (‚â• % of models in agreement)\nProportion of messages (%)\n40\n60\n80\n100\n40\n50\n60\n70\n80\n90\n100\n(a) Cumulative share of messages reach-\ning each cross-model consensus among\ntop-five performing models by script\ntype. Roman-script messages systemati-\ncally shift toward lower consensus, indi-\ncating higher epistemic uncertainty.\n(b) Confusion matrices for Claude 4.5 Sonnet, stratified by script type (English, native script,\nRoman). Each cell shows the percentage of instances with a given true label (rows) that are\nassigned to each predicted label (columns). Roman-script messages are associated with both a\nhigher rate of missed Emergencies and a markedly higher tendency to be assigned Insufficient\nInformation.\nFigure 4: a:Cross-model consensus averaged over the top 5 models (left); b: Confusion Matrix on Claude 4.5 (right) by script\ntype.\nover the top-performing models in the benchmark, code-mixed\nromanized queries achieve substantially higher performance (F1\n80.3%) than non‚Äìcode-mixed romanized queries (F1 70.4%). Within\nthe romanized subset, the most challenging condition is not the\nuse of Roman script per se, but messages written exclusively in\nromanized form without additional lexical signals. In our corpus,\ncode-mixing is predominantly with English rather than with other\nIndian languages. Fully romanized messages require the model to\nparse user-generated transliterations, which are noisier and more\nheterogeneous in the absence of English lexical anchors.\nTable 4: Average F1 over the top-five performing models in\nthe overall benchmark, stratified by code-mixing status for\nmessages in roman script.\nScript\nCode-mixing status\nF1 (%)\nRoman\nCode-mixed\n80.3\nRoman\nNon‚Äìcode-mixed\n70.4\n5.4.3\nScript-sensitive misclassification. Beyond aggregate confu-\nsion matrices, we observe systematic script-dependent discrepan-\ncies in how models label otherwise comparable queries. Figure 4b\nshows representative examples for two frequent query types: gen-\neral guidance in pregnancy and routine vaccination schedules. For\neach scenario, we include English, native script messages, and their\nroman script equivalent variants that express the same underly-\ning information need. In both scenarios, the English and native\nscript variants are reliably classified as Non-emergency, reflecting\ntheir status as routine informational queries without acute symp-\ntoms. In contrast, romanized variants tend to fall into Insufficient\nInformation, even though the underlying information need is simi-\nlar. This qualitative pattern mirrors the aggregate error rates from\nSection 5.4.1, where romanized inputs exhibit higher rates of over-\nassigning Insufficient Information relative to both English and native\nscripts.\n5.5\nModel-generated Reasoning\nTo analyse model-generated reasoning, we treat the collection of\nreasoning summary generated by models as a dataset and design\nthree lightweight tests. We first tokenise each reasoning, remove\nstop words, apply part-of-speech tagging, and retain only nouns\nand verbs; on this reduced vocabulary we compute the most fre-\nquent content lemmas per model and script type to quantify (1)\nshared content vocabulary across models. Second, we measure\nthe frequency of explicit language and script aware cues such as\n‚Äúlanguage‚Äù, ‚Äútranslate‚Äù, ‚Äúromanized‚Äù in the summaries to probe (2)\nexplicit language awareness. Third, we compute lexical overlap be-\ntween each user message and its corresponding reasoning summary\nto estimate (3) lexical copying from user texts into explanations,\nincorrect vs correct queries. All analyses are by script type.\n5.5.1\nShared content vocabulary across models. Across models, the\nlexical space is highly overlapping: for each system, 15‚Äì21 of the\n30 most frequent content lemmas are shared with at least one other\nmodel, suggesting that differences in performance are not driven\nby completely disjoint vocabularies in the rationales.\n5.5.2\nLanguage and script-aware cues in rationales. Language cues\nsuch as ‚Äúlanguage‚Äù, and‚Äútranslate‚Äù appear disproportionately often\namong the top-30 most frequent content words in the rationales of\nClaude 4.5, LLaMA 4 and Qwen3 when the predicted label is incor-\nrect, with phrases like ‚Äúthe language appears to be ...‚Äù occurring\nmore frequently for roman messages. This pattern indicates that\nthe models are explicitly aware of language and script as potential\nsources of difficulty, even when this awareness does not translate\ninto correct triage labels.\n5.5.3\nLexical copying from user queries into rationales. The copy-\ning rates in Table 5 reveal two broad trends. First, there is a clear\nscript-familiarity gradient: across all models, copying from the user\nmessage into the rationale is most frequent for English inputs, in-\ntermediate for Roman script inputs, and least frequent for native\n6\n"}, {"page": 7, "text": "Figure 5: Examples of semantically similar user messages across English, native script, and roman script. In all cases, the true\nlabel is Non-emergency, yet romanized variants are more likely to be predicted as Insufficient Information (see Section 5.4.3).\nAnalysis of the model-generated summaries indicates that the models generally paraphrase the queries correctly as non-urgent\ninformation requests and can decode noisy tokens, but nonetheless misclassify the final triage label (see Section 6.3).\nscript inputs. Second, for native script inputs, copying is system-\natically higher on incorrect than on correct predictions across all\nmodels, suggesting that direct lexical reuse often reflects an echoing\nbehaviour rather than successful comprehension. In contrast, for\nEnglish and roman inputs, the relationship between copying and\ncorrectness is positive correlated, albeit model-dependent where\nseveral models, Claude, GPT-4o, DeepSeek, exhibit slightly higher\ncopying on correct cases. Overall, for English and Roman script\ninputs, copying could indicate an analytical anchor, whereas for\nnative script input, it could be an echoing behavior.\nTable 5: Share of cases in which at least three tokens are\ncopied after (removing stop words) from the user message\ninto the model-generated reasoning (%), by script type, pre-\ndiction correctness, and model. P = Prediction Correctness\ncolumn indicates whether the model‚Äôs triage label matches\nground truth (T = True, F = False)\nScript\nP Claude GPT-4o LLaMA 4 Qwen3 DeepSeek\nEnglish F\n41.4\n35.8\n47.1\n45.6\n29.3\nT\n45.5\n37.3\n42.9\n45.3\n32.8\nNative\nF\n28.3\n10.8\n8.8\n24.6\n9.0\nT\n21.3\n9.7\n5.7\n12.3\n3.3\nRoman\nF\n36.9\n22.4\n21.4\n58.2\n17.1\nT\n44.2\n26.4\n26.6\n44.4\n21.4\n6\nDiagnosing the Script Gap\n6.1\nEffect of Script-normalizing Translations\nTo test whether the Roman gap is primarily driven by script and or-\nthography rather than by differences in underlying clinical content,\nwe conduct two script-normalization experiments: (1) translate both\nnative script and romanized messages into a common pivot language\n(English), and (2) translate romanized messages back into their cor-\nresponding native scripts using GPT-4o. We then re-evaluate triage\nclassification on these normalized subsets with LLaMA 4 and GPT-\n4o.\n6.1.1\nNative, Roman ‚ÜíEnglish. Table 6 shows that for GPT-4o,\nnative script F1 is essentially unchanged (81.3% ‚Üí81.4%), and\nLLaMA 4 shows a similarly small shift (77.6% ‚Üí78.1%), indicating\nthat translating native messages into English has a negligible effect.\nIn contrast, Roman script inputs benefit more from English normal-\nization: GPT-4o improves from 75.3% to 77.5%, and LLaMA 4 from\n73.0% to 76.0%.\n6.1.2\nRoman ‚ÜíNative. Table 6 shows that for GPT-4o, F1 on\nRoman inputs rises from 75.3% to 80.1%, bringing performance\nwithin 1.2 points of the native baseline (81.3%). LLaMA 4 shows a\nsimilar pattern, improving from 73.1% to 76.4% compared to 77.7%\non original native script messages.\nOverall, script normalization recovers most of the Roman gap, re-\ninforcing the view that the deficit stems largely from orthographic\nand tokenization effects rather than from clinically different cases\nbeing written in Roman script. Moreover, normalization into na-\ntive scripts yields the largest gains, suggesting that native script\n7\n"}, {"page": 8, "text": "mapping preserves clinical nuance more faithfully than translation\ninto English, which can introduce additional semantic changes.\nTable 6: Effect of script normalization by GPT-4o on F1 (%)\nfor GPT-4o and LLaMA 4 on Native and Roman script.\nInput\nSetting\nGPT-4o\nLLaMA 4\nNative\nbaseline (native)\n81.3\n77.6\nRoman\nbaseline (roman)\n75.3\n73.0\nNative\n‚ÜíEnglish\n81.4\n78.1\nRoman\n‚ÜíEnglish\n77.5\n76.0\nRoman\n‚Üínative script\n80.1\n76.4\n6.2\nEffect of Script on Token-level Uncertainty\nWe quantify how script choice affects model uncertainty by ana-\nlyzing token-level entropy for messages written in native versus\nRoman script. For each message, we tokenize the text with the\nmodel‚Äôs subword tokenizer and, at every position, condition the\nmodel on the observed prefix while forcing the next token to be the\ngold token from the message. We then record the full next-token\nprobability distribution, compute its Shannon entropy in bits, and\naverage these entropies over all positions to obtain a per-message\ntoken-level entropy score, which we subsequently aggregate by\nscript type.\nTable 7 reports the alternate-token mean entropy ùêªalt for GPT-\nOSS-20B and Qwen2.5-14B. In both models, romanized messages\nexhibit substantially higher entropy than native script counterparts,\nthat is, 3.6 vs. 5.6 bits for GPT-OSS-20B, 1.7 vs. 4.6 bits for Qwen2.5-\n14B. This elevated entropy indicates that, at each position, the model\nspreads probability mass over a wider set of plausible continuations\nfor Roman text, introducing additional representational uncertainty\nbefore triage-specific reasoning even begins.\nTable 7: Alternate-token mean entropy ùêªalt (in bits) for GPT-\nOSS-20B and Qwen2.5-14B, by script type.\nScript\nGPT-OSS-20B\nQwen2.5-14B\nEnglish\n4.9\n3.7\nNative\n3.6\n1.7\nRoman\n5.6\n4.6\n6.3\nRoman Script Noise and the Stability of\nClassification Boundaries\nIn error analysis (Section 5.4.3), we observed that romanized inputs\nare disproportionately assigned the label Insufficient Information.\nThe confusion-matrix analysis in Section 5.4.1 further showed that\nthis overuse persists even when the underlying message is semanti-\ncally similar to its English or native script variants. We now extend\nthis analysis by examining the model-generated natural language\nrationales produced alongside each prediction, focusing specifically\non romanized messages. As illustrated in Figure 5, these messages\noften express the same underlying information needs as their Eng-\nlish or native script counterparts. For general pregnancy queries\nsuch as ‚Äú4 manth me kya kya karna chaiye‚Äù (What all should be\ndone in the 4th month) and ‚Äú8 month kya kuch krna chaiye ky ni\nkrna chaiye‚Äù (What should or shouldn‚Äôt be done in the 8th month),\nthe GPT-4o reasoning summaries typically characterise the mes-\nsage as a general request for guidance in pregnancy and explicitly\nnote that no acute symptoms are mentioned‚Äîconsistent with a\nNon-emergency interpretation. Nevertheless, the model frequently\nassigns Insufficient Information. A similar pattern holds for vaccina-\ntion queries: romanized questions such as ‚Äú2 teka kab lagega baby\nko‚Äù (When will baby get second vaccine shot) in GPT-4o and closely\nrelated variants in Claude 4.5 such as ‚Äútika kb lgega isko‚Äù(When will\nthis one get the vaccine) are paraphrased as routine immunisation\nschedule questions or general informational inquiries, yet they are\nstill more often labelled Insufficient Information than analogous\nEnglish or native script variants. These cases suggest that the LLMs\noften recover a reasonable semantic interpretation of romanized\nmessages, but the mapping from meaning to label is brittle: Ro-\nman inputs introduce orthographic noise that shrinks the margin\nbetween Non-emergency and Insufficient Information.\n7\nDiscussion\nIncreasingly, organizations in low-resource settings are integrating\nLLM-based solutions into critical workflows, often driven by anec-\ndotal evidence of general capability rather than systematic safety\ntesting for specific downstream tasks. Our study addresses this gap\nby introducing targeted metrics that can also be used for future\nevaluation of LLMs‚Äô classification performance in high-stakes, and\nnoisy environments. Our most critical finding is that the failure\nmode is not semantic (model understanding what the message en-\ntailed), but decisional (triaging based on that understanding). The\nmodels focus on the message form rather than the underlying mean-\ning. These findings also challenge current research trends; while\nmost studies track performance gains on synthetically generated ro-\nmanized datasets and structured languages, few address the chaotic,\nevolving nature of romanized code-mixing, which lacks the formal\nstructure of learned English or native scripts.\nAcross models, we observe a performance penalty of 5‚Äì12% (aver-\nage 8.5%) on romanized queries. Projecting this to a full deployment\nscale of 41 million patients reach of Maternal Health Organization A,\nwhere our sampling indicates a 56% prevalence of Roman script, im-\nplies that 23 million users would be exposed to significantly higher\ntriage risks than their native script counterparts. This differential\ncould cause nearly 2 million excess misclassifications. Script Gap,\nthen, is not a marginal performance variance, but a critical safety\nliability. A system that grants safety only to those who type in stan-\ndard scripts inadvertently establishes a hidden digital hierarchy of\ncare, where the most vulnerable are left voiceless by the very tools\ndesigned to protect them.\nReferences\n[1] Asma Ben Abacha, Yassine Mrabet, Mark Sharp, Travis R Goodwin, Sonya E\nShooshan, and Dina Demner-Fushman. 2019. Bridging the gap between con-\nsumers‚Äô medication questions and trusted answers. In MEDINFO 2019: Health\nand Wellbeing e-Networks for All. IOS Press, 25‚Äì29.\n[2] Gustavo Aguilar, Victor Soto, Fahad AlGhamdi, and Thamar Solorio. 2020. LinCE:\nA Benchmark for Linguistic Code-switching Evaluation. In Proceedings of the 12th\n8\n"}, {"page": 9, "text": "Language Resources and Evaluation Conference. European Language Resources\nAssociation, Marseille, France.\n[3] Md Mahfuz Ibn Alam and Antonios Anastasopoulos. 2025. Large Language\nModels as a Normalizer for Transliteration and Dialectal Translation. In Pro-\nceedings of the 12th Workshop on NLP for Similar Languages, Varieties and\nDialects (VarDial). Association for Computational Linguistics, Torino, Italy.\nhttps://aclanthology.org/ COLING 2025 workshop.\n[4] Rahul K Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin\nQui√±onero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, An-\ndrea Vallone, Alex Beutel, et al. 2025. Healthbench: Evaluating large language\nmodels towards improved human health. arXiv preprint (2025).\n[5] Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein.\n2022. Fact Checking with Insufficient Evidence. Transactions of the Association\nfor Computational Linguistics 10 (2022), 746‚Äì763.\n[6] Suhana Bedi, Yutong Liu, Lucy Orr-Ewing, Dev Dash, Sanmi Koyejo, Alison\nCallahan, Jason A. Fries, Michael Wornow, Akshay Swaminathan, Lisa Soley-\nmani Lehmann, Hyo Jung Hong, Mehr Kashyap, Akash R. Chaurasia, Nirav R.\nShah, Karandeep Singh, Troy Tazbaz, Arnold Milstein, Michael A. Pfeffer, and\nNigam H. Shah. 2025. Testing and Evaluation of Health Care Applications of\nLarge Language Models: A Systematic Review. JAMA 333, 4 (2025), 319‚Äì328.\ndoi:10.1001/jama.2024.21700\n[7] Adrian Benton, Abhijeet Awasthi, Ramakanth Pasunuru, Nithum Thain, Myle\nOtt, and Mona Diab. 2025. Improving Informally Romanized Language Identifi-\ncation through Transliteration. In Proceedings of the 2025 Conference on Empirical\nMethods in Natural Language Processing. To appear..\n[8] Andrew A. Borkowski, Colleen E. Jakey, Stephen M. Mastorides, Ana L. Kraus, Gi-\ntanjali Vidyarthi, Narayan Viswanadhan, and Jose L. Lezama. 2023. Applications\nof ChatGPT and Large Language Models in Medicine and Health Care: Benefits\nand Pitfalls. Federal Practitioner 40, 6 (2023), 170‚Äì173. doi:10.12788/fp.0386\n[9] Wenyuan Chen, Fateme Nateghi Haredasht, Kameron C. Black, Francois Grol-\nleau, Emily Alsentzer, Jonathan H. Chen, and Stephen P. Ma. 2025. Retrieval-\nAugmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy\nConstruction and Large-Scale Evaluation. arXiv preprint arXiv:2509.22565 (2025).\narXiv:2509.22565 [cs.CL] doi:10.48550/arXiv.2509.22565\n[10] Yella Diekmann, Chase M. Fensore, Rodrigo M. Carrillo-Larco, Nishant Pradhan,\nBhavya Appana, and Joyce C. Ho. 2025. Evaluating Safety of Large Language Mod-\nels for Patient-facing Medical Question Answering. In Proceedings of the 4th Ma-\nchine Learning for Health Symposium (Proceedings of Machine Learning Research,\nVol. 259). PMLR, 267‚Äì290. https://proceedings.mlr.press/v259/diekmann25a.html\n[11] Rachel L. Draelos, Samina Afreen, Barbara Blasko, Tiffany L. Brazile, Natasha\nChase, Dimple Patel Desai, Jessica Evert, Heather L. Gardner, Lauren Herrmann,\nAswathy Vaikom House, Stephanie Kass, Marianne Kavan, Kirshma Khemani,\nAmanda Koire, Lauren M. McDonald, Zahraa Rabeeah, and Amy Shah. 2025.\nLarge language models provide unsafe answers to patient-posed medical ques-\ntions. arXiv preprint arXiv:2507.18905 (2025). doi:10.48550/arXiv.2507.18905\n[12] Xuehai He, Shu Chen, Zeqian Ju, Xiangyu Dong, Hongchao Fang, Sicheng Wang,\nYue Yang, Jiaqi Zeng, Ruisi Zhang, Ruoyu Zhang, et al. 2020. Meddialog: Two\nlarge-scale medical dialogue datasets. arXiv preprint arXiv:2004.03329 (2020).\n[13] Jaavid Aktar Husain, Raj Dabre, Aswanth Kumar, Jay Gala, Thanmay Jayakumar,\nRatish Puduppully, and Anoop Kunchukuttan. 2024. RomanSetu: Efficiently\nunlocking multilingual capabilities of Large Language Models via Romanization.\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). Association for Computational Linguistics,\nBangkok, Thailand, 15593‚Äì15615. doi:10.18653/v1/2024.acl-long.833\n[14] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter\nSzolovits. 2021. What disease does this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Applied Sciences 11, 14 (2021),\n6421.\n[15] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.\n2019. PubMedQA: A Dataset for Biomedical Research Question Answering. In\nEMNLP-IJCNLP 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan\n(Eds.). Hong Kong, China, 2567‚Äì2577.\n[16] Amarpreet Kaur, Alexander Budko, Katrina Liu, Eric Eaton, Bryan D. Steitz, and\nKevin B. Johnson. 2025. Automating Responses to Patient Portal Messages Using\nGenerative AI. Applied Clinical Informatics 16, 3 (2025), 718‚Äì731. doi:10.1055/a-\n2565-9155\n[17] Aman Kumar, Himani Shrotriya, Prachi Sahu, Amogh Mishra, Raj Dabre, Ratish\nPuduppully, Anoop Kunchukuttan, Mitesh M Khapra, and Pratyush Kumar.\n2022. IndicNLG benchmark: Multilingual datasets for diverse NLG tasks in Indic\nlanguages. In EMNLP 2022. 5363‚Äì5394.\n[18] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Sim-\nple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. In\nAdvances in Neural Information Processing Systems, Vol. 30.\n[19] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim\nRockt√§schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS 2020), Vol. 33. 9459‚Äì9474.\n[20] Shuhan Liu et al. 2025. Detecting emergencies in patient portal messages using\nlarge language models and a knowledge graph. Journal of the American Medical\nInformatics Association 32, 6 (2025), 1032‚Äì1043. https://academic.oup.com/jamia/\narticle-pdf/32/6/1032/62922452/ocaf059.pdf\n[21] Yash Madhani, Mitesh M. Khapra, and Anoop Kunchukuttan. 2023. Bhasha-\nAbhijnaanam: Native-script and Romanized Language Identification for 22 Indic\nLanguages. In Proceedings of the 61st Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers). Association for Computational\nLinguistics, Toronto, Canada.\n[22] Yash Madhani, Sushane Parthan, Priyanka Bedekar, N. C. Gokul, Ruchi Jain\nKhapra, Anoop Kunchukuttan, Pratyush Kumar, and Mitesh M. Khapra. 2024.\nAksharantar: Open Indic-language Transliteration Datasets and Models for the\nNext Billion Users. Transactions of the Association for Computational Linguistics\n(2024). Also available as an open dataset via AI4Bharat / IndicXlit..\n[23] Itay Manes, Naama Ronn, David Cohen, Ran Ilan Ber, Zehavi Horowitz-Kugler,\nand Gabriel Stanovsky. 2024. K-QA: A Real-World Medical Q&A Benchmark.\nIn Proceedings of the 23rd Workshop on Biomedical Natural Language Processing.\nBangkok, Thailand, 277‚Äì294.\n[24] Lucas Masanneck et al. 2024. Triage Performance Across Large Language Models\nand ChatGPT. Journal of Medical Internet Research (2024). https://www.jmir.org/\n2024/1/e53297/\n[25] Vini Mehta, Puneeta Ajmera, Sheetal Kalra, Mohammad Miraj, Ruchika Gallani,\nRiyaz Ahamed Shaik, Hashem Abu Serhan, and Ranjit Sah. 2024. Human resource\nshortage in India‚Äôs health sector: a scoping review of the current landscape. BMC\nPublic Health 24, 1 (2024), 1368.\n[26] Charles Nimo, Tobi Olatunji, Abraham Toluwase Owodunni, Tassallah Abdul-\nlahi, Emmanuel Ayodele, Mardhiyah Sanni, Ezinwanne C. Aka, Folafunmi Omo-\nfoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood O.\nYekini, Jonas Kemp, Katherine A Heller, Jude Chidubem Omeke, Chidi Asuzu\nMd, Naome A Etori, A√Øm√©rou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy\nKinara, Michael L. Best, Irfan Essa, Stephen Edward Moore, Chris Fourie, and\nMercy Nyamewaa Asiedu. 2025. AfriMed-QA: A Pan-African, Multi-Specialty,\nMedical Question-Answering Benchmark Dataset. In ACL 2025. Vienna, Austria.\n[27] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.\nMedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical do-\nmain Question Answering. In Proceedings of the Conference on Health, Inference,\nand Learning (Proceedings of Machine Learning Research, Vol. 174), Gerardo Flores,\nGeorge H Chen, Tom Pollard, Joyce C Ho, and Tristan Naumann (Eds.). PMLR,\n248‚Äì260.\n[28] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang,\nJana Thompson, Phu Mon Htut, and Samuel Bowman. 2022. BBQ: A Hand-built\nBias Benchmark for Question Answering. In Findings of the Association for\nComputational Linguistics: ACL 2022. Association for Computational Linguistics,\nDublin, Ireland, 2086‚Äì2105.\n[29] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don‚Äôt\nKnow: Unanswerable Questions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 2: Short Papers).\nAssociation for Computational Linguistics, Melbourne, Australia, 784‚Äì789.\n[30] Yang Ren, Yuqi Wu, Jungwei W. Fan, Aditya Khurana, Sunyang Fu, Dezhi Wu,\nHongfang Liu, and Ming Huang. 2024. Automatic Uncovering of Patient Primary\nConcerns in Portal Messages Using a Fusion Framework of Pretrained Language\nModels. Journal of the American Medical Informatics Association 31, 8 (2024),\n1714‚Äì1724. doi:10.1093/jamia/ocae144\n[31] Brian Roark, Lawrence Wolf-Sonkin, Christo Kirov, Sabrina J. Mielke, Cibu Johny,\nIsin Demirsahin, and Keith B. Hall. 2020. Processing South Asian Languages\nWritten in the Latin Script: The Dakshina Dataset. In Proceedings of the 12th\nLanguage Resources and Evaluation Conference. European Language Resources\nAssociation, Marseille, France.\n[32] Max Savery, Asma Ben Abacha, Soumya Gayen, and Dina Demner-Fushman.\n2020. Question-driven summarization of answers to consumer health questions.\nScientific Data 7, 1 (2020), 322.\n[33] Rajvee Sheth, Himanshu Beniwal, and Mayank Singh. 2025. COMI-LINGUA:\nExpert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-\nMixing. In Findings of the Association for Computational Linguistics: EMNLP\n2025, Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and\nViolet Peng (Eds.). Association for Computational Linguistics, Suzhou, China,\n7973‚Äì7992. https://aclanthology.org/2025.findings-emnlp.422/\n[34] Abhishek Kumar Singh, Vishwajeet Kumar, Rudra Murthy, Jaydeep Sen, Ashish\nMittal, and Ganesh Ramakrishnan. 2025. Indic qa benchmark: A multilingual\nbenchmark to evaluate question answering capability of llms for indic languages.\nIn Findings of NAACL 2025.\n[35] Harman Singh, Nitish Gupta, Shikhar Bharadwaj, Dinesh Tewari, and Partha\nTalukdar. 2024. Indicgenbench: A multilingual benchmark to evaluate generation\ncapabilities of llms on indic languages. arXiv preprint arXiv:2404.16816 (2024).\n[36] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed\nAmin, Le Hou, Kevin Clark, Stephen R. Pfohl, Heather Cole-Lewis, Darlene Neal,\nQazi Mamunur Rashid, Mike Schaekermann, Amy Wang, Dev Dash, Jonathan H.\nChen, Nigam H. Shah, Sami Lachgar, Philip Andrew Mansfield, Sushant Prakash,\n9\n"}, {"page": 10, "text": "Bradley Green, Ewa Dominowska, Blaise Ag√ºera y Arcas, Nenad Toma≈°ev, Yun\nLiu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle K. Barral, Dale R.\nWebster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam,\nand Vivek Natarajan. 2025. Toward Expert-Level Medical Question Answering\nwith Large Language Models. Nature Medicine 31, 4 (2025), 943‚Äì950. doi:10.1038/\ns41591-024-03423-7\n[37] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.\n2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification. In\nProceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers). Association for Computational Linguistics, New Orleans, Louisiana,\n809‚Äì819.\n[38] √ñzlem Uzuner, Brett R. South, Shuying Shen, and Scott L. DuVall. 2011. 2010\ni2b2/VA Challenge on Concepts, Assertions, and Relations in Clinical Text. Jour-\nnal of the American Medical Informatics Association 18, 5 (2011), 552‚Äì556.\n[39] Sshubam Verma, Mohammed Safi Ur Rahman Khan, Vishwajeet Kumar, Rudra\nMurthy, and Jaydeep Sen. 2025. Milu: A multi-task indic language understanding\nbenchmark. In NAACL 2025.\n[40] Lianxi Wang, Yujia Tian, and Zhuowei Chen. 2024. Enhancing Hindi Feature\nRepresentation through Fusion of Dual-Script Word Embeddings. In Proceedings\nof the 2024 Joint International Conference on Computational Linguistics, Language\nResources and Evaluation (LREC-COLING 2024). ELRA and ICCL, Torino, Italia,\n5966‚Äì5976. https://aclanthology.org/2024.lrec-main.528/\n[41] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021. The Art of Abstention:\nSelective Prediction and Error Regularization for Natural Language Processing.\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers). Association for Computational Linguistics, Online, 1040‚Äì\n1051.\n10\n"}, {"page": 11, "text": "System prompt for triage classification\nRole and input.\nYou are a medical triage assistant for maternal and newborn care. You\nwill be given:\n‚Ä¢ the user‚Äôs message (mother/caregiver/pregnant woman).\nMessages may be written in Indian languages or English, may include\ncode-mixed content, and can be short, informal, romanized, and chatty\nin tone.\nLabel space.\nYour task is to classify the message into one of three labels:\n‚Ä¢ Emergency: The message indicates, or could plausibly indicate, a\nsymptom that may require immediate medical attention or urgent\ncare. If the message overlaps with any emergency symptom in the\nknowledge base, or describes something that could reasonably map\nto it (even if severity details are missing), label it Emergency. When\nin doubt between Emergency and Insufficient Information, prefer\nEmergency.\n‚Ä¢ Non-Emergency: The message describes a symptom or concern that\nclearly does not require urgent attention. These include general\ndiet questions, breastfeeding without emergency symptoms, in-\ntroducing food, minor constipation/gas, administrative questions,\nsleep/teething/routine issues with no possible overlap with emer-\ngency symptoms.\n‚Ä¢ Insufficient Information: Use when the message lacks enough detail\nto decide if it is an emergency. This includes vague, incomplete, or\npoorly written messages that might hint at symptoms but do not\nconfirm them. If a symptom clearly matches the emergency list,\nclassify as Emergency. If it could be an emergency but more details\nare needed to know for sure, classify as Insufficient Information.\nAnalysis rules.\n‚Ä¢ Treat questions as reports of situations that occurred.\n‚Ä¢ If multiple overlaps and at least one could be an emergency, label\nEmergency immediately.\n‚Ä¢ Severity cues (e.g., ‚Äúvery‚Äù, ‚Äúa lot‚Äù, ‚Äúconstant‚Äù, ‚Äúunbearable‚Äù, ‚Äúmultiple\ntimes‚Äù) strengthen the Emergency signal, but absence of severity\nshould not downgrade to Insufficient Information if the symptom\nitself is inherently urgent (e.g., bleeding, localised pain, difficulty\nbreathing, high fever, jaundice).\n‚Ä¢ For ambiguous mentions (e.g., ‚Äúbleeding‚Äù without source), assume\nworst-case relevant to the knowledge base (vaginal bleeding).\n‚Ä¢ For cases where localised pain (such as headache, back pain, leg\npain), bleeding, or dizziness is mentioned, label as Emergency.\n‚Ä¢ For infant feeding issues, if feeding has stopped or the baby is ‚Äúnot\nfeeding‚Äù, treat as Emergency unless clearly stated otherwise.\n‚Ä¢ If information is so vague that no symptoms can be matched at all,\nlabel Insufficient Information.\n‚Ä¢ Do not penalize informal, chatty, or ungrammatical messages if a\nsymptom match is still possible.\n‚Ä¢ Use the knowledge base as the primary guide for emergency symp-\ntoms, but allow for reasonable clinical interpretation when appro-\npriate.\n‚Ä¢ Output is routing support, not medical advice; do not give care\ninstructions.\nFigure 6: Full SOP prompt used for triage classification.\nA\nTriage Annotation Definition\nEmergency. The message indicates, or could plausibly indicate, a\nsymptom that may require immediate medical attention or urgent\ncare. If the message overlaps with any symptom in the emergency\nknowledge base, or can reasonably be mapped to one (even if sever-\nity details are missing), it is labeled Emergency. When in doubt\nbetween Emergency and Insufficient Information, annotators are\ninstructed to prefer Emergency.\nNon-Emergency. The message describes a symptom or concern\nthat clearly does not require urgent attention. Typical examples\ninclude general diet questions, breastfeeding without red-flag symp-\ntoms, introducing foods, mild constipation or gas, administrative\nqueries, and sleep/teething/routine issues with no overlap with\nemergency symptoms. These are labeled Non-Emergency.\nInsufficient Information. The message lacks enough detail to de-\ntermine whether the situation is an emergency. This includes vague,\nincomplete, or poorly written messages that may hint at concerning\nsymptoms but do not clearly confirm them. If a symptom clearly\nmatches the emergency list, the label should be Emergency; if it\ncould be an emergency but key details are missing to know for sure,\nthe label is Insufficient Information.\nB\nKB+SOP Triage Prompt Template\nThe full prompt can be found in Figure 6.\nC\nModel Generated Reasoning Analysis\nSupplemental tables for Section 5.5\nTable 8: Overlap in top-30 lemmas used in rationales when\npredictions are correct (number of shared lemmas).\nGPT-4o\nClaude 4.5\nLLaMA 4\nDeepSeek\nQwen3-80B\nGPT-4o\n30\n18\n18\n21\n18\nClaude 4.5\n18\n30\n19\n19\n19\nLLaMA 4\n18\n19\n30\n20\n18\nDeepSeek\n21\n19\n20\n30\n15\nQwen3-80B\n18\n19\n18\n15\n30\nTable 9: Top lemmas in model rationales when predictions\nare correct vs. incorrect (ordered by frequency, aggregated\nover all labels and scripts) for Claude 4.5 and Qwen3-80B.\nModel\nCorrect predictions (top lemmas)\nIncorrect predictions (top lemmas)\nClaude 4.5\nsymptom,\nemergency,\npregnancy,\nmonth, indicate, pain, require, baby,\nconcern, care, evaluation, base, knowl-\nedge, delivery, condition, match, feed,\nfever, eat, feeding, issue, nutrition,\nday, food, leg, complication, milk,\nattention, infant, newborn\nsymptom, emergency, concern, ap-\npear, indicate, determine, baby, re-\nquire, month, pregnancy, pain, con-\ntext, lack, evaluation, detail, match,\nbase, knowledge, care, fever, lan-\nguage, condition, difficulty, feed, re-\nlate, identify, movement, issue, breath-\ning, severity\nQwen3-80B\nsymptom, emergency, pregnancy, con-\ncern, pain, indicate, month, care, baby,\nfever, bleeding, base, knowledge, feed-\ning, relate, feed, health, inquiry, over-\nlap, delivery, issue, infant, sign, com-\nplication, preeclampsia, labor, infec-\ntion, condition, context, preterm\nsymptom, emergency, concern, indi-\ncate, pregnancy, pain, baby, context,\nmonth, fever, knowledge, base, lack,\ncare, relate, bleeding, feed, sign, feed-\ning, condition, newborn, issue, trans-\nlate, detail, infection, match, stop,\nmovement, refer, phrase\n11\n"}]}