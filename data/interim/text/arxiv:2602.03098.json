{"doc_id": "arxiv:2602.03098", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.03098.pdf", "meta": {"doc_id": "arxiv:2602.03098", "source": "arxiv", "arxiv_id": "2602.03098", "title": "TextME: Bridging Unseen Modalities Through Text Descriptions", "authors": ["Soyeon Hong", "Jinchan Kim", "Jaegook You", "Seungtaek Choi", "Suha Kwak", "Hyunsouk Cho"], "published": "2026-02-03T04:43:13Z", "updated": "2026-02-03T04:43:13Z", "summary": "Expanding multimodal representations to novel modalities is constrained by reliance on large-scale paired datasets (e.g., text-image, text-audio, text-3D, text-molecule), which are costly and often infeasible in domains requiring expert annotation such as medical imaging and molecular analysis. We introduce TextME, the first text-only modality expansion framework, to the best of our knowledge, projecting diverse modalities into LLM embedding space as a unified anchor. Our approach exploits the geometric structure of pretrained contrastive encoders to enable zero-shot cross-modal transfer using only text descriptions, without paired supervision. We empirically validate that such consistent modality gaps exist across image, video, audio, 3D, X-ray, and molecular domains, demonstrating that text-only training can preserve substantial performance of pretrained encoders. We further show that our framework enables emergent cross-modal retrieval between modality pairs not explicitly aligned during training (e.g., audio-to-image, 3D-to-image). These results establish text-only training as a practical alternative to paired supervision for modality expansion.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.03098v1", "url_pdf": "https://arxiv.org/pdf/2602.03098.pdf", "meta_path": "data/raw/arxiv/meta/2602.03098.json", "sha256": "f687f8776afd9b86bf03bbcbc79626af9725e235aaae213ca57aaea4e6e1f210", "status": "ok", "fetched_at": "2026-02-18T02:19:57.897459+00:00"}, "pages": [{"page": 1, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nSoyeon Hong 1 Jinchan Kim 1 Jaegook You 1 Seungtaek Choi 2 Suha Kwak 3 Hyunsouk Cho 4\nAbstract\nExpanding multimodal representations to novel\nmodalities is constrained by reliance on large-\nscale paired datasets (e.g., text–image, text–audio,\ntext–3D, text–molecule), which are costly and\noften infeasible in domains requiring expert an-\nnotation such as medical imaging and molecular\nanalysis. We introduce TextME, the first text-\nonly modality expansion framework, to the best\nof our knowledge, projecting diverse modalities\ninto LLM embedding space as a unified anchor.\nOur approach exploits the geometric structure of\npretrained contrastive encoders to enable zero-\nshot cross-modal transfer using only text descrip-\ntions, without paired supervision. We empirically\nvalidate that such consistent modality gaps exist\nacross image, video, audio, 3D, X-ray, and molec-\nular domains, demonstrating that text-only train-\ning can preserve substantial performance of pre-\ntrained encoders. We further show that our frame-\nwork enables emergent cross-modal retrieval be-\ntween modality pairs not explicitly aligned dur-\ning training (e.g., audio-to-image, 3D-to-image).\nThese results establish text-only training as a prac-\ntical alternative to paired supervision for modality\nexpansion.\n1. Introduction\nModality expansion, which aligns heterogeneous data\nmodalities into a unified embedding space, has emerged\nas a core challenge in multimodal representation learning\n(Baltruˇsaitis et al., 2018; Manzoor et al., 2023; Liang et al.,\n2024; Yuan et al., 2025). Recent approaches leverage large-\nscale paired datasets to project diverse modalities—such as\nimages, audio, and 3D point clouds—into shared seman-\ntic spaces where equivalent content maintains proximity\n1Department of Artificial Intelligence, Ajou University, Su-\nwon, South Korea 2Division of Language & AI, Hankuk Univer-\nsity of Foreign Studies, Seoul, Korea 3Graduate School of AI,\nPOSTECH, Pohang, Korea 4Department of Software, Ajou Uni-\nversity, Suwon, South Korea. Correspondence to: Hyunsouk Cho\n<hyunsouk@ajou.ac.kr>.\nPreprint. February 4, 2026.\nFigure 1. Comparison of modality expansion approaches. Un-\nlike prior methods that require large-scale paired data or pseudo-\npair construction through overlapping encoders, TextME achieves\nmodality expansion using only unpaired text descriptions while\nreusing pretrained encoders.\n(Zhang et al., 2023a; Han et al., 2023; Zhu et al., 2023;\nLyu et al., 2024; Guo et al., 2023). While text–image and\ntext–audio corpora have enabled remarkable progress in\nvision–language (Radford et al., 2021; Jia et al., 2021)\nand audio–language modeling (Wu et al., 2023; Manco\net al., 2022), extending this paradigm to specialized do-\nmains proves prohibitively expensive or infeasible. Medical\nimaging requires costly expert annotations while navigating\nprivacy constraints (Wang et al., 2025; Ziller et al., 2021),\nmolecular analysis demands complex domain-specific repre-\nsentations (Xiao et al., 2024), and 3D modeling necessitates\nlabor-intensive curation (Deitke et al., 2023). Consequently,\nthe scalability of modality expansion remains fundamentally\nlimited by the availability of paired supervision.\nRecent methods reduce computational costs by reusing pre-\ntrained encoders through lightweight projection networks\n(Wang et al., 2023b; Zhang et al., 2024b; Wang et al.,\n2024a;b), yet they still require constructing semantically\naligned pseudo pairs across all target modalities through\noverlapping encoders. Meanwhile, prior work has revealed\nthat contrastive encoders exhibit a consistent modality\ngap—a systematic offset between text and modality em-\nbeddings—that can enable cross-modal transfer via sim-\nple geometric operations (Liang et al., 2022; Zhang et al.,\n2023b; 2024a). However, these studies have primarily fo-\ncused on analyzing the gap in vision-language models or\n1\narXiv:2602.03098v1  [cs.LG]  3 Feb 2026\n"}, {"page": 2, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nmitigating the gap within paired-data settings; whether this\ngeometric property can be exploited to eliminate the need for\npaired supervision altogether remains unexplored. In addi-\ntion, there is no guidance on which modalities are amenable\nto effective alignment and which are not.\nIn this work, we demonstrate that the modality gap can en-\nable modality expansion without paired supervision. We pro-\npose TextME, a framework that projects modality-specific\nembeddings into LLM embedding space as a unified se-\nmantic anchor by applying precomputed offset corrections\nderived from the gap structure. As illustrated in Figure 1,\nunlike prior methods that require large-scale bi-modal pairs\nor pseudo-pair construction through overlapping encoders,\nour proposed framework achieves modality expansion using\nonly unpaired text descriptions—with substantially reduced\ndata requirements while fully leveraging pretrained encoders\nthrough lightweight projectors.\nWe evaluate the framework across six diverse modali-\nties—image, video, audio, 3D point clouds, X-ray, and\nmolecules—on both cross-modal retrieval and zero-shot\nclassification tasks.\nOur experiments demonstrate that\nTextME achieves competitive performance relative to\npaired-data methods and, notably, enables emergent cross-\nmodal capabilities between modality pairs never observed\nduring training, such as audio-to-3D and molecule-to-image\nretrieval. These results suggest that text modality can cre-\nate meaningful semantic bridges across arbitrary modalities\nwithout explicit cross-modal supervision. To better under-\nstand the variation in performance across modalities, we\nfurther analyze the geometric properties of each encoder\nand find that the consistency of gap-content orthogonality\ncorrelates with downstream performance, providing insight\ninto when text-only expansion is most effective.\nOur contribution is three-fold:\n• We propose TextME, a text-only modality expansion\nframework that exploits modality gap geometry to learn\ncross-modal projections using only text descriptions,\neliminating the need for paired multimodal supervision\nduring training.\n• We investigate LLM embedding space as a unified\nanchor for modality expansion and compare it against\nmultimodal encoder representations, analyzing their\nvarying effectiveness across tasks and modalities.\n• We empirically validate the framework across six di-\nverse modalities, demonstrating competitive perfor-\nmance on retrieval and classification tasks and identify-\ning encoder characteristics that predict when text-only\nexpansion is most effective.\n2. Preliminaries\n2.1. Problem Formulation\nModality expansion aims to integrate pretrained modality-\nspecific encoders into a unified semantic space where simi-\nlar concepts maintain proximity regardless of their source\nmodality. Let M = {m1, . . . , mk} denote a set of tar-\nget modalities to be aligned. For each modality m ∈M,\na pretrained contrastive encoder consists of a text branch\nEtext\nm : T →Rdm and a modal branch Emodal\nm\n: Xm →Rdm,\nwhere T is the space of text descriptions and Xm is the input\nspace for modality m. Our objective is to learn projection\nnetworks Pm : Rdm →Rdh that map modal embeddings\ninto a shared dh-dimensional anchor space.\nExisting methods require instance-level paired data\n{(xi, ti)}N\ni=1 of modal inputs xi ∈Xm and text descrip-\ntions ti ∈T to train cross-modal projections (Han et al.,\n2023; Zhu et al., 2023; Lyu et al., 2024). Recent approaches\nconnect multiple pretrained encoders via overlapping modal-\nities: given encoders pretrained on modality pairs (A, B)\nand (B, C), they leverage data from the shared modality B to\nalign encoder spaces, enabling transfer to non-overlapping\npairs (A, C) (Wang et al., 2023b; Zhang et al., 2024b; Wang\net al., 2024a;b). This requires modality overlap across all\ntarget encoders. In this work, we consider a more practical\nscenario: learning projection networks independently for\neach modality using only unpaired text descriptions {ti}N\ni=1,\nwithout requiring cross-encoder alignment or access to tar-\nget modality samples.\n2.2. Modality Gap and Interchangeable Space\nPrior work has shown that contrastive encoders trained with\nobjectives such as InfoNCE exhibit a systematic offset be-\ntween text and modal embedding spaces (Liang et al., 2022;\nZhang et al., 2023b; 2024a). For each encoder Em, the\nmodality gap is characterized by the difference between the\ncentroids of modal and text embeddings:\n∆m = µmodal\nm\n−µtext\nm ,\n(1)\nwhere µmodal\nm\n= E[Emodal\nm\n(x)] and µtext\nm = E[Etext\nm (t)] denote\nthe expected embeddings over their respective distributions.\nThis gap presents a fundamental challenge for text-only\ntraining, as projection networks learned from text embed-\ndings cannot directly transfer to modal embeddings that\noccupy a different region of the space.\nInterchangeable Space via Centering.\nA key observa-\ntion from Zhang et al. (2024a) is that this challenge can be\naddressed through independent centering operations. Con-\nsider a semantically matched pair (t, x) with embeddings\net = Etext\nm (t) and ex = Emodal\nm\n(x). Although these em-\nbeddings differ due to the modality gap, subtracting their\n2\n"}, {"page": 3, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nFigure 2. Overview of the TextME pipeline. (a) Offset computation estimates modality-specific centroids from unpaired samples, creating\nan interchangeable space where centered text and modal embeddings become functionally equivalent. (b) During training, projection\nnetworks are learned by aligning centered text embeddings with a unified LLM anchor space, requiring only text descriptions. (c) At\ninference, centering modal embeddings with the precomputed offset enables zero-shot cross-modal transfer without paired supervision.\nrespective centroids yields centered embeddings\nˆet = et −µtext\nm ,\nˆex = ex −µmodal\nm\n,\n(2)\nthat satisfy ˆet ≈ˆex for semantically corresponding pairs.\nThat is, centering removes the modality-specific bias while\npreserving the shared semantic content, creating an inter-\nchangeable space where text and modal embeddings be-\ncome functionally equivalent (An et al., 2025). This prop-\nerty enables projection networks trained on centered text\nembeddings to generalize to centered modal embeddings at\ninference time, forming the basis of our text-only training\napproach.\n3. TextME: Text-only Modality Expansion\nWe present TextME, a framework that enables modality\nexpansion using only text descriptions by exploiting the\ngeometric properties of pretrained contrastive encoders. Fig-\nure 2 illustrates the overall pipeline.\n3.1. Overview\nThe key insight of TextME is that the interchangeable space\ndescribed in Section 2 allows projection networks trained on\ncentered text embeddings to generalize to centered modal\nembeddings at inference time. Our framework operates\nin two phases. During training, we precompute modality-\nspecific centroids and train lightweight projection networks\nto map centered text embeddings into a shared anchor space.\nAt inference, we apply the same centering operation to\nmodal embeddings before projection, enabling zero-shot\ncross-modal transfer without having observed any modal\nsamples during training.\n3.2. Offset Computation\nAs established in Section 2, creating an interchangeable\nspace requires estimating the centroids µtext\nm and µmodal\nm\nfor\neach modality. We compute these centroids from represen-\ntative samples:\nµtext\nm = 1\nN\nN\nX\ni=1\nEtext\nm (ti),\nµmodal\nm\n= 1\nM\nM\nX\nj=1\nEmodal\nm\n(xj),\n(3)\nwhere {ti}N\ni=1 ⊂T and {xj}M\nj=1 ⊂Xm are sampled inde-\npendently from text and modal distributions. Unlike pro-\njection training, these samples need not be instance-level\npaired—only representative coverage of each distribution is\nrequired for accurate centroid estimation.\nImportantly, accurate centroid estimation requires only a\nsmall number of samples. In our experiments, we find that\n5K samples suffice for stable estimation across all evaluated\nmodalities, representing less than 5% of typical paired train-\ning requirements (Zhu et al., 2023; Zhang et al., 2024b). The\ncentroids are precomputed once and remain fixed through-\nout training.\n3.3. Text-to-Anchor Alignment\nGiven the precomputed offsets, we train projection networks\nusing only text descriptions from the target domain. For\neach modality m, a projection network Pm : Rdm →Rdh\nmaps centered text embeddings into a shared anchor space.\nAnchor Space Selection.\nWe adopt LLM embedding\nspace as our unified anchor rather than multimodal text\nencoders. While multimodal encoders such as CLIP are\noptimized for cross-modal matching, LLMs trained on large-\nscale text corpora capture richer semantic relationships that\ngeneralize across diverse domains. To assess cross-domain\n3\n"}, {"page": 4, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nFigure 3. Semantic anchoring comparison. LLM embeddings\nand multimodal encoders are compared on 3K semantically equiv-\nalent cross-modal description pairs. LLM embeddings exhibit\nclearer separation between matched and unmatched pairs, demon-\nstrating superior cross-domain alignment capability.\nalignment capabilities, we analyze 3K audio-image cap-\ntion pairs from FlickrNet (Senocak et al., 2018), where we\ngenerated linguistically distinct but semantically equivalent\ndescriptions using the Gemini API (Google, 2024)—for\ninstance, an image caption “a red sports car speeding on\nhighway” is paired with its audio equivalent “loud engine\nroar with wind rushing past.” As shown in Figure 3, LLM\nembeddings (i.e., Qwen) exhibit clearer separation between\nsemantically equivalent and unrelated pairs (0.56 vs. 0.23–\n0.26 mean cosine similarity) compared to multimodal en-\ncoders, suggesting better suitability for bridging heteroge-\nneous descriptions. This advantage is further corroborated\nby semantic textual similarity benchmarks, where LLM\nembeddings achieve Spearman correlations of 85–90 com-\npared to 67–68 for multimodal encoders (see Appendix A\nfor details). Based on these findings, we adopt Qwen3-\nEmbedding (Zhang et al., 2025) as our default anchor space.\nTraining Objective.\nGiven text descriptions Dtext =\n{ti}N\ni=1 from the target modality domain, we train the pro-\njection network by aligning centered text embeddings with\ntheir corresponding LLM embeddings:\nLalign = −1\nB\nB\nX\ni=1\nlog\nexp(sim(zi, z′\ni)/τ)\nP\nj∈Ni∪{i} exp(sim(zi, z′\nj)/τ) (4)\nwhere zi = Pm(ˆeti) is the projected centered text embed-\nding with ˆeti = Etext\nm (ti) −µtext\nm , z′\ni = ELLM(ti) is the\ncorresponding LLM embedding, and Ni contains hard neg-\natives. Following recent language embedding models (Lee\net al., 2024; Moreira et al., 2024; R¨osch et al., 2024), we\nemploy hard negative mining to focus training on challeng-\ning examples near the decision boundary, improving the\ndiscriminative quality of learned projections.\n3.4. Inference\nAt inference time, TextME enables zero-shot cross-modal\ntransfer by mapping modal embeddings into the interchange-\nable space. For a non-text input x from modality m, the\nfinal embedding is computed as:\nefinal = Pm(ˆex) = Pm(Emodal\nm\n(x) −µmodal\nm\n).\n(5)\nThe centering operation transforms the modal embedding\ninto the interchangeable space where text embeddings re-\nside during training. As established in Section 2, centering\npreserves semantic relationships while removing modality-\nspecific bias, allowing the text-trained projection network\nPm to process modal inputs without modification. The fi-\nnal embeddings are thus aligned within the unified LLM\nanchor space alongside text representations, enabling direct\ncross-modal retrieval and zero-shot classification.\n4. Experiments\nWe evaluate TextME on cross-modal retrieval and zero-shot\nclassification across six modalities: image, video, audio, 3D,\nX-ray, and molecules. Our experiments address three key\nquestions: (1) whether text-only training can achieve com-\npetitive performance relative to paired-data methods and\npretrained encoders (Section 4.2); (2) what geometric prop-\nerties predict success or failure across different modalities\n(Section 4.3); and (3) how different design choices includ-\ning anchor space selection and offset correction influence\noverall performance (Section 4.4).\n4.1. Experimental Setup\nModalities and Encoders.\nWe adopt LanguageBind (Zhu\net al., 2023) as the text encoder for all text-to-modal retrieval\nand zero-shot classification tasks. For modal encoders, we\nadopt CLIP (Radford et al., 2021) for image, ViCLIP (Wang\net al., 2023a) for video, CLAP (Elizalde et al., 2023) for\naudio, Uni3D (Zhou et al., 2023) for 3D, CXR-CLIP (You\net al., 2023) for X-ray, and MoleculeSTM (Liu et al., 2023)\nfor molecule. Each encoder pair is independently projected\ninto the shared LLM anchor space for cross-modal match-\ning. We sample 100K text descriptions per modality for\nprojection training, with offset computation on 5K samples.\nBaselines.\nWe compare against three categories of meth-\nods. First, we report the performance of the original pre-\ntrained encoders as reference points. Second, for paired-\ndata approaches, we include LanguageBind (Zhu et al.,\n2023) and Ex-MCR (Zhang et al., 2024b), both of which\nperform modality expansion using fully-paired multimodal\ndata. Third, for unpaired-data methods, we compare with\nCOX† (Huang et al., 2025), which learns target modality rep-\nresentations from scratch without instance-level pairing but\nrequires substantial target modality data and classification\n4\n"}, {"page": 5, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nTable 1. Zero-shot performance across all evaluation benchmarks. PPR: Performance Preservation Ratio (%) relative to pretrained\nencoders. × indicates unavailable results due to missing official implementations or incompatible evaluation protocols. Bold indicates\nbest among unpaired methods. †Our reproduction.\nText→X Retrieval\nClassification\nEmergent X→X\nImage\nVideo\nAudio\nMol.\nAudio\n3D\nX-ray\nA→I\n3D→I\nData\nCOCO\nFlkr.\nMSR.\nMSVD\nDiDe.\nACaps.\nClo.\nDrug.\nASet.\nESC\nMN40.\nScan.\nRSNA\nFlkr.\nObja.\nRequirements\nPretrained\n48.29\n77.70\n37.00\n51.06\n31.27\n22.47\n16.90\n79.19\n9.32\n85.20\n67.75\n42.21\n52.64\n×\n×\nPaired-data methods\nLanguageBind\n44.53\n73.42\n45.30\n65.22\n36.85\n12.42\n11.32\n×\n18.33\n94.00\n×\n×\n×\n1.52\n×\n10M pairs\nEx-MCR\n40.24\n71.89\n×\n×\n×\n19.07\n7.01\n×\n6.67\n71.20\n66.53\n40.31\n×\n1.57\n5.67\n1M pairs∗\nUnpaired-data methods\nNa¨ıve\n0.01\n0.04\n0.00\n0.00\n0.00\n0.02\n0.04\n10.17\n1.14\n2.90\n0.81\n3.32\n26.36\n0.02\n0.00\n0\nCOX†\n0.02\n0.20\n5.10\n0.00\n0.10\n0.08\n0.11\n7.63\n1.26\n2.00\n4.05\n2.84\n22.53\n0.02\n0.00\n10K labels\nTextME\n28.63\n51.66\n26.40\n45.82\n24.10\n15.35\n7.81\n34.75\n5.80\n77.25\n70.86\n42.15\n46.59\n1.06\n10.27\n100K text\nPPR(%)\n59.3\n66.5\n71.4\n89.7\n77.1\n68.3\n46.2\n43.9\n62.2\n90.7\n104.6\n99.9\n88.5\n×\n×\n∗Indirect: uses overlapping modality from existing MCR spaces. TextME requires zero paired data and zero labeled target data.\nFigure 4. Emergent cross-modal retrieval without paired supervision. Audio queries retrieve semantically related 3D objects (top),\nand molecular structures retrieve contextually appropriate images (bottom). These modality pairs were never seen during training,\ndemonstrating that text-anchored alignment creates semantic bridges across arbitrary modalities.\nlabels. We also include a Na¨ıve baseline that simply aligns\nembedding dimensions via PCA without any learned pro-\njection. Unlike COX, TextME requires no target modality\ndata during training.\nEvaluation.\nWe evaluate on three task categories:\nText→X retrieval, emergent cross-modal retrieval between\nunseen modality pairs, and zero-shot classification. Table 1\nreports results on representative benchmarks per modality,\nselected based on prevalence in prior work (Zhu et al., 2023;\nZhang et al., 2024b). We report Recall@k (R@k) for re-\ntrieval, MRR@k for molecule retrieval following Liu et al.\n(2023), and Top-k accuracy for classification. We define\nPerformance Preservation Ratio (PPR) as the percentage\nof pretrained encoder performance retained by our method:\nPPR = (TextME score/Pretrained score) × 100%. Com-\nplete results across all benchmarks appear in Appendix B.\n4.2. Does Text-Only Training Preserve Pretrained\nPerformance?\nTable 1 reports performance across all evaluation tasks.\nTextME achieves an average of 74.5% PPR across all tasks,\nwith classification at 89.2% consistently outperforming re-\ntrieval at 65.3%, suggesting that offset-based alignment\npreserves categorical boundaries more effectively than fine-\ngrained similarity structure. Among unpaired baselines,\nCOX (Huang et al., 2025) yields substantially lower perfor-\nmance, as it requires a pretrained classifier on labeled target\ndata. Since official implementations are not publicly avail-\nable, we trained this classifier from scratch on evaluation\n5\n"}, {"page": 6, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\ndata. In contrast, our framework eliminates the need for\nlabeled target data and paired supervision, thereby enabling\ndirect generalization to novel modalities.\nZero-Shot Retrieval and Classification.\nAcross both\ntask categories, TextME substantially outperforms unpaired\nbaselines and achieves comparable results to paired-data\nmethods. As reported in the Data Requirements column of\nTable 1, this performance is achieved using only 100K text\ndescriptions, compared to 1–10M paired samples required\nby methods such as LanguageBind and Ex-MCR. This re-\nduction of over two orders of magnitude in supervision\nrequirements makes modality expansion practical for spe-\ncialized domains where paired annotation is prohibitively\nexpensive. In terms of task-specific patterns, classification\nconsistently demonstrates higher preservation than retrieval,\nas categorical discrimination requires only well-separated\ndecision boundaries whereas retrieval demands fine-grained\ninstance-level similarity that is more sensitive to distortions\nintroduced by offset correction. Notably, 3D zero-shot clas-\nsification surpasses pretrained Uni3D with 104.6% PPR on\nModelNet40, while retrieval preservation varies substan-\ntially across modalities. We examine the factors underlying\nthese variations in Section 4.3.\nEmergent Cross-Modal Capabilities.\nThe unified an-\nchor space additionally enables retrieval between modality\npairs not explicitly aligned during training. As reported in\nthe Emergent X→X columns of Table 1, TextME outper-\nforms Ex-MCR on 3D→Image despite the latter requiring\npaired supervision, and achieves comparable performance\nto paired-data methods on Audio→Image. To qualitatively\nexamine whether the learned representations enable retrieval\nbetween modality pairs without any paired annotations, we\nconduct cross-modal retrieval experiments using indepen-\ndently collected datasets. Specifically, we sample instances\nfrom each modality and perform retrieval across disjoint\nmodality pairs such as Audio→3D and Molecule→Image.\nFigure 4 presents representative results, demonstrating that\naudio queries retrieve semantically coherent 3D models\nand molecular queries retrieve contextually relevant images.\nThese findings suggest that text-anchored alignment estab-\nlishes implicit semantic correspondences without explicit\ncross-modal supervision.\n4.3. When Does Text-Only Expansion Succeed?\nWe hypothesize that the effectiveness of text-only expansion\ndepends on how well pretrained encoders satisfy the geomet-\nric properties underlying modality gap alignment. The re-\nsults above support this view, revealing substantial variation\nin performance preservation across modalities—ranging\nfrom over 100% for 3D classification to approximately 42%\nfor Molecule retrieval. To investigate this relationship, we\nTable 2. Geometric properties of contrastive encoders. (i) intra-\nmodal independence, (ii) gap consistency, (iii) bounded deviation,\n(iv) gap-content orthogonality. †: weaker satisfaction (>0.1 for (i),\n<0.96 for (ii)).\nEncoder\nMod.\n(i)↓\n(ii)↑\n(iii)↓\n(iv)↓\nCLIP\nImage\n.28†±.11 .97±.00 .00±.00 .00±.11\nViCLIP\nVideo\n.18†±.11 .98±.00 .00±.00 .00±.06\nCLAP\nAudio\n.12†±.18 .97±.01 .00±.00 .00±.15\nUni3D\n3D\n.07±.06\n.96±.00 .00±.00 .00±.04\nCXR-CLIP\nX-ray\n.37†±.13 .99±.00 .00±.00 .00±.06\nMoleculeSTM Molecule .01±.19 .78†±.05 .00±.00 .00±.18\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nOrthogonality Variance ( )\n30\n40\n50\n60\n70\n80\n90\n100\n110\nPerformance Preservation (%)\nr = -0.67\np < 0.001\n3D\nVideo\nX-ray\nImage\nAudio\nMolecule\nFigure 5. Orthogonality variance vs. performance preserva-\ntion. Each point represents a single evaluation metric from six\nmodalities, with tasks sharing the same encoder aligned vertically\nat identical variance values. Lower variance in gap-content orthog-\nonality corresponds to higher downstream performance.\nmeasure the geometric characteristics of each encoder and\nexamine their correlation with downstream performance.\nGeometric Properties.\nFollowing prior work (Zhang\net al., 2024a), we measure four geometric properties for\neach encoder using 5K paired samples: (i) Intra-modal\nindependence: E[cos(ˆe, ˆµm)], measuring whether embed-\ndings are statistically independent from the modality cen-\ntroid; (ii) Gap consistency: cos(∆(k)\nm , ∆m), measuring\nwhether instance-level offsets ∆(k)\nm = e(k)\nmodal −e(k)\ntext align\ndirectionally with the group-level offset ∆m; (iii) Bounded\ndeviation: std(ϵk) where ϵk = ∆(k)\nm −∆m, measuring\nthe variance of instance-level offsets around the mean;\n(iv) Gap-content orthogonality: | cos(∆m, r(p,q))| where\nr(p,q) = ep −eq, measuring whether the modality gap is\nindependent of intra-modal semantic variations. Properties\n(i) and (ii) ensure that a single offset vector can character-\nize the modality gap, while (iii) and (iv) ensure that offset\ncorrection preserves semantic relationships.\n6\n"}, {"page": 7, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nTable 3. Effect of offset correction. Modalities with strong\ngap consistency benefit substantially, while Molecule with weak\nconsistency shows degradation.\nModality\nBenchmark\nw/o offset\nw/ offset\n∆\n3D\nModelNet40\n4.05\n70.86\n+94.30%\n3D\nScanObjectNN\n5.40\n42.15\n+87.20%\nAudio\nAudioCaps\n8.68\n15.35\n+43.50%\nAudio\nClotho\n4.77\n7.81\n+38.90%\nX-ray\nRSNA\n31.35\n46.59\n+32.70%\nMolecule\nDrugBank\n36.44\n34.75\n−4.60%\nObservations.\nTable 2 demonstrates that all encoders sat-\nisfy the requirements for offset-based alignment in expec-\ntation. Gap consistency exceeds 0.96 for five of six modal-\nities, and mean orthogonality remains near zero across all\nencoders, indicating that properties (ii) and (iv) hold on av-\nerage. However, the degree to which these properties hold\nat the instance level varies substantially. We find that the\nvariance of property (iv), gap-content orthogonality, serves\nas a particularly informative predictor of downstream per-\nformance. To quantify this relationship, we analyze perfor-\nmance preservation at the individual task level, treating each\nevaluation metric as a separate observation. This yields 33\ndata points spanning retrieval benchmarks such as Audio-\nCaps R@1, COCO R@5, and DrugBank MRR, as well as\nclassification benchmarks such as ModelNet40 Top-1 and\nESC-50 accuracy. Since tasks evaluated on the same en-\ncoder share identical orthogonality variance, they appear\nvertically aligned in Figure 5. The analysis reveals a mod-\nerate negative correlation between orthogonality variance\nand performance preservation, with Pearson r = −0.67 and\np < 0.001. Encoders exhibiting lower variance in prop-\nerty (iv), notably Uni3D at ±0.04 and ViCLIP at ±0.06,\nachieve preservation rates consistently above 80%, whereas\nthose with higher variance such as CLAP at ±0.15 and\nMoleculeSTM at ±0.18 show greater performance degra-\ndation. This pattern suggests that inconsistent orthogo-\nnality introduces variable distortions during offset correc-\ntion, with fine-grained retrieval tasks affected more severely\nthan categorical classification. We additionally note that\nMoleculeSTM exhibits a distinct failure mode in property\n(ii), as its gap consistency of only 0.78 indicates that a single\noffset vector inadequately characterizes the modality gap\nfor molecular embeddings.\n4.4. How Do Design Choices Affect Performance?\nEffect of Offset Correction.\nTable 3 examines the contri-\nbution of offset correction across modalities with varying\ngap consistency. For modalities satisfying strong gap con-\nsistency above 0.95, offset correction yields substantial im-\nprovements, with 3D classification increasing from 4.05%\nto 70.86% on ModelNet40 and Audio retrieval improving\nby 43.5% on AudioCaps. In contrast, Molecule with gap\nTable 4. Anchor space comparison. LLM-based anchors yield\nstronger Text→X retrieval performance, while multimodal anchors\nachieve comparable results on classification.\nRetrieval\nClassification\nAnchor\nAudioCaps\nR@1\nClotho\nR@1\nDrugBank\nMRR\n3D\nTop-1\nAudio\nTop-1\nX-ray\nTop-1\nMultimodal encoders\nCLIP\n15.91\n6.60\n36.44\n78.04\n86.70\n48.31\nLanguageBind\n14.54\n6.93\n29.66\n81.12\n74.65\n44.99\nLLM embedding models\nNV-Embed-v2\n16.20\n7.75\n26.27\n76.30\n79.40\n48.59\nQwen3-Embed\n15.35\n7.81\n34.75\n70.86\n77.25\n46.59\nTable 5. Effect of training data source. Domain-specific captions\nsubstantially outperform general-purpose text across all modalities.\nTraining Data\nAudio\nR@1\n3D\nTop-1\nX-ray\nTop-1\nMol.\nMRR\nall-NLI\n6.36\n12.10\n22.48\n16.10\nDomain captions\n15.35\n70.86\n46.59\n34.75\nImprovement\n+141%\n+485%\n+107%\n+116%\nconsistency of only 0.78 exhibits a slight performance degra-\ndation of 4.6%, indicating that unreliable offset estimation\ncan introduce harmful distortions. These results suggest that\npractitioners should verify gap consistency before applying\ngeometric alignment.\nAnchor Space Selection.\nTable 4 compares two cate-\ngories of anchor spaces, which are LLM embeddings and\nmultimodal encoders. The results reveal a task-dependent\npattern in anchor space effectiveness. For retrieval tasks,\nLLM-based anchors such as NV-Embed-v2 and Qwen3-\nEmbedding consistently outperform multimodal encoders\non audio benchmarks, achieving 16.20 and 15.35 R@1 on\nAudioCaps compared to 14.54–15.91 for multimodal an-\nchors. We attribute this advantage to the semantic represen-\ntations acquired through large-scale language pretraining,\nwhich capture fine-grained similarity relationships required\nfor retrieval. For classification tasks, multimodal anchors\nsuch as CLIP and LanguageBind demonstrate superior per-\nformance, with CLIP achieving 86.70 on Audio and Lan-\nguageBind achieving 81.12 on 3D modality. We attribute\nthis advantage to the discriminative decision boundaries ac-\nquired through vision-language contrastive training. Based\non these findings, we adopt Qwen3-Embedding as the de-\nfault anchor space, as it provides balanced performance\nacross retrieval and classification.\nTraining Data Source.\nTo validate whether general-\npurpose text corpora that have never been associated with\nany target modality can enable cross-modal transfer, we\ntrain projection networks using all-NLI, a corpus combin-\ning MNLI (Williams et al., 2018) and SNLI (Bowman\n7\n"}, {"page": 8, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\net al., 2015) with 100K sentence pairs, following the pre-\nvious work of text-only training (Xiao et al., 2025). Ta-\nble 5 presents the results. Training on all-NLI yields sub-\nstantially lower performance compared to domain-specific\ncaptions across all modalities, with the degradation being\nmost pronounced for 3D, which drops from 70.86% to\n12.10%. This performance gap reflects the distributional\nmismatch between general linguistic expressions and the\nspecialized vocabularies characteristic of each modality do-\nmain. Nevertheless, the non-trivial cross-modal transfer\nachieved with general-purpose text validates our text-only\ntraining paradigm. A systematic analysis of how distri-\nbutional characteristics such as domain coverage and vo-\ncabulary specificity influence alignment quality presents a\npromising avenue for developing more refined data selection\nstrategies.\n5. Related Work\nModality Expansion.\nContrastive learning has enabled ef-\nfective multimodal alignment by projecting different modal-\nities into shared semantic spaces (Radford et al., 2021;\nJia et al., 2021). Subsequent work extends this paradigm\nto multiple modalities through central hubs: ImageBind\n(Girdhar et al., 2023) uses images as the anchor modality,\nwhile LanguageBind (Zhu et al., 2023) leverages text for\nbroader semantic coverage. To reduce computational costs,\nrecent methods connect frozen pretrained encoders through\nlightweight projectors. C-MCR (Wang et al., 2023b) and\nEx-MCR (Zhang et al., 2024b) learn adapters between en-\ncoder pairs, while FreeBind (Wang et al., 2024a) and Om-\nniBind (Wang et al., 2024b) ensemble multiple encoders per\nmodality. However, all these approaches require instance-\nlevel paired supervision during training, which becomes pro-\nhibitive in specialized domains where paired data is scarce.\nTextME eliminates this requirement through text-only train-\ning of projection networks.\nModality Gap Analysis.\nThe modality gap—a system-\natic offset between text and non-text embeddings in con-\ntrastive models—was first identified by Liang et al. (2022),\nwho characterized its geometric structure in CLIP. Subse-\nquent work has sought to understand this phenomenon from\nmultiple perspectives, including linear separability anal-\nysis (Shi et al., 2023), double-ellipsoid geometry (Levi\n& Gilboa, 2024), and the distinction between modality-\nspecific and contrastive components (Fahim et al., 2024).\nBuilding on these insights, several methods exploit the gap\nfor downstream applications such as vision model diagno-\nsis (Zhang et al., 2023b) and cross-modal transfer via zero-\ncentering (Zhang et al., 2024a). More recent efforts focus\non mitigating the gap through learnable correction mod-\nels (Park et al., 2024; Eslami & de Melo, 2024), embedding\nstandardization (An et al., 2025), or centroid alignment for\nmixed-modality retrieval (Li et al., 2025). However, these\nmethods operate in paired-data settings and focus on improv-\ning alignment within existing modality pairs. While prior\nwork has primarily analyzed the gap in vision-language mod-\nels, we demonstrate that this geometric property generalizes\nacross six diverse modalities—including 3D, X-ray, and\nmolecules—and can be exploited for modality expansion\nwithout paired supervision.\nLLM-Anchored Multimodal Learning.\nRecent work\nleverages LLMs as semantic anchors for multimodal align-\nment, exploiting their broad semantic coverage and contex-\ntual understanding acquired through large-scale language\npretraining. Generative approaches integrate LLMs with\nmultimodal encoders for instruction tuning (Han et al.,\n2023) and unified cross-modal generation (Han et al., 2024).\nRepresentation-focused methods include UniBind (Lyu\net al., 2024), which creates LLM-augmented unified spaces,\nand LLM2CLIP (Huang et al., 2024), which enhances dense\ncaption understanding through large-scale paired training on\ntens of millions of image-caption pairs. More recently, E5-\nV (Jiang et al., 2024) and LCO-Emb (Xiao et al., 2025) show\nthat text-only contrastive learning can enhance MLLM em-\nbedding quality without multimodal training data. However,\nthese methods operate within unified MLLM architectures\nwhere cross-modal alignment is implicitly established dur-\ning generative pretraining. In contrast, TextME addresses\nthe alignment of independently trained contrastive encoders\nwith architecturally heterogeneous embedding spaces, en-\nabling expansion to specialized domains such as 3D, X-ray,\nand molecules without requiring a shared backbone.\n6. Conclusion\nWe introduced TextME, a framework that leverages the\nconsistent modality gap in pretrained contrastive encoders\nto enable text-only modality expansion. By projecting di-\nverse modalities into LLM embedding space as a unified\nanchor, our approach preserves substantial performance of\npretrained encoders across six modalities using only text\ndescriptions for projection learning. Compared to existing\npaired-data methods, TextME reduces data requirements by\nover 95% while eliminating the need for paired multimodal\nsupervision. Furthermore, the framework enables emergent\ncross-modal retrieval between modality pairs never seen\nduring training (e.g., audio-to-image, 3D-to-image), demon-\nstrating that text-anchored alignment can establish implicit\ncorrespondences across arbitrary modalities without direct\ncross-modal pairing. These results suggest that text-only\ntraining offers a practical pathway for integrating special-\nized modalities—such as medical imaging and molecular\nstructures—into unified multimodal systems without the\nprohibitive cost of expert annotation.\n8\n"}, {"page": 9, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nImpact Statement\nThis work aims to reduce the data annotation burden in mul-\ntimodal learning, potentially democratizing access to multi-\nmodal AI systems in specialized domains such as medical\nimaging and molecular analysis where paired supervision is\nprohibitively expensive. While this could accelerate bene-\nficial applications in healthcare and drug discovery, practi-\ntioners should exercise appropriate caution when deploying\nsuch models in safety-critical settings, ensuring thorough\nvalidation before clinical or real-world use. We do not fore-\nsee immediate negative societal consequences beyond those\ncommon to advances in representation learning.\nReferences\nAn, N. M., Kim, E., Thorne, J., and Shim, H. I0t: Embed-\nding standardization method towards zero modality gap.\nIn Proceedings of the 63rd Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers), pp. 27182–27199, 2025.\nAnne Hendricks, L., Wang, O., Shechtman, E., Sivic, J.,\nDarrell, T., and Russell, B. Localizing moments in video\nwith natural language. In Proceedings of the IEEE inter-\nnational conference on computer vision, pp. 5803–5812,\n2017.\nBaltruˇsaitis, T., Ahuja, C., and Morency, L.-P. Multimodal\nmachine learning: A survey and taxonomy. IEEE trans-\nactions on pattern analysis and machine intelligence, 41\n(2):423–443, 2018.\nBowman, S., Angeli, G., Potts, C., and Manning, C. D.\nA large annotated corpus for learning natural language\ninference. In Proceedings of the 2015 conference on\nempirical methods in natural language processing, pp.\n632–642, 2015.\nChen, D. and Dolan, W. B. Collecting highly parallel data\nfor paraphrase evaluation. In Proceedings of the 49th\nannual meeting of the association for computational lin-\nguistics: human language technologies, pp. 190–200,\n2011.\nDeitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel,\nO., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi,\nA., and Farhadi, A. Objaverse: A universe of annotated\n3d objects. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 13142–\n13153, 2023.\nDrossos, K., Lipping, S., and Virtanen, T. Clotho: An audio\ncaptioning dataset. In ICASSP 2020-2020 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 736–740. IEEE, 2020.\nElizalde, B., Deshmukh, S., Al Ismail, M., and Wang, H.\nClap learning audio concepts from natural language su-\npervision. In ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 1–5. IEEE, 2023.\nEslami, S. and de Melo, G. Mitigate the gap: Investigating\napproaches for improving cross-modal alignment in clip.\narXiv preprint arXiv:2406.17639, 2024.\nFahim, A., Murphy, A., and Fyshe, A. It’s not a modality\ngap: Characterizing and addressing the contrastive gap.\narXiv preprint arXiv:2405.18570, 2024.\nGemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A.,\nLawrence, W., Moore, R. C., Plakal, M., and Ritter, M.\nAudio set: An ontology and human-labeled dataset for\naudio events. In 2017 IEEE international conference on\nacoustics, speech and signal processing (ICASSP), pp.\n776–780. IEEE, 2017.\nGirdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K. V.,\nJoulin, A., and Misra, I. Imagebind: One embedding\nspace to bind them all. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npp. 15180–15190, 2023.\nGoogle. Gemini api, 2024. URL https://ai.google.\ndev/gemini-api. Accessed: January 2025.\nGuo, Z., Zhang, R., Zhu, X., Tang, Y., Ma, X., Han, J.,\nChen, K., Gao, P., Li, X., Li, H., et al. Point-bind &\npoint-llm: Aligning point cloud with multi-modality for\n3d understanding, generation, and instruction following.\narXiv preprint arXiv:2309.00615, 2023.\nHan, J., Zhang, R., Shao, W., Gao, P., Xu, P., Xiao, H.,\nZhang, K., Liu, C., Wen, S., Guo, Z., et al. Imagebind-\nllm: Multi-modality instruction tuning. arXiv preprint\narXiv:2309.03905, 2023.\nHan, J., Gong, K., Zhang, Y., Wang, J., Zhang, K., Lin, D.,\nQiao, Y., Gao, P., and Yue, X. Onellm: One framework to\nalign all modalities with language. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 26584–26595, 2024.\nHuang, W., Wu, A., Yang, Y., Luo, X., Yang, Y., Hu, L., Dai,\nQ., Wang, C., Dai, X., Chen, D., et al. Llm2clip: Power-\nful language model unlocks richer visual representation.\narXiv preprint arXiv:2411.04997, 2024.\nHuang, Z., Niu, G., Han, B., Sugiyama, M., and Liu, T. To-\nwards out-of-modal generalization without instance-level\nmodal correspondence. In The Thirteenth International\nConference on Learning Representations, 2025.\n9\n"}, {"page": 10, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nIrvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S.,\nChute, C., Marklund, H., Haghgoo, B., Ball, R., Shpan-\nskaya, K., et al. Chexpert: A large chest radiograph\ndataset with uncertainty labels and expert comparison. In\nProceedings of the AAAI conference on artificial intelli-\ngence, volume 33, pp. 590–597, 2019.\nJia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H.,\nLe, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up\nvisual and vision-language representation learning with\nnoisy text supervision. In International conference on\nmachine learning, pp. 4904–4916. PMLR, 2021.\nJiang, T., Song, M., Zhang, Z., Huang, H., Deng, W., Sun,\nF., Zhang, Q., Wang, D., and Zhuang, F. E5-v: Univer-\nsal embeddings with multimodal large language models.\narXiv preprint arXiv:2407.12580, 2024.\nKim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps: Gen-\nerating captions for audios in the wild. In Proceedings\nof the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and Short\nPapers), pp. 119–132, 2019.\nKim, S., Chen, J., Cheng, T., Gindulyte, A., He, J., He, S.,\nLi, Q., Shoemaker, B. A., Thiessen, P. A., Yu, B., et al.\nPubchem 2025 update. Nucleic acids research, 53(D1):\nD1516–D1525, 2025.\nKnox, C., Wilson, M., Klinger, C. M., Franklin, M., Oler,\nE., Wilson, A., Pon, A., Cox, J., Chin, N. E., Strawbridge,\nS. A., et al. Drugbank 6.0: the drugbank knowledgebase\nfor 2024. Nucleic acids research, 52(D1):D1265–D1275,\n2024.\nLee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catan-\nzaro, B., and Ping, W. Nv-embed: Improved techniques\nfor training llms as generalist embedding models. arXiv\npreprint arXiv:2405.17428, 2024.\nLevi, M. Y. and Gilboa, G. The double-ellipsoid geometry\nof clip. arXiv preprint arXiv:2411.14517, 2024.\nLi, B., Zhang, Y., Wang, X., Liang, W., Schmidt, L., and\nYeung-Levy, S.\nClosing the modality gap for mixed\nmodality search. arXiv preprint arXiv:2507.19054, 2025.\nLiang, P. P., Zadeh, A., and Morency, L.-P. Foundations\n& trends in multimodal machine learning: Principles,\nchallenges, and open questions. ACM Computing Surveys,\n56(10):1–42, 2024.\nLiang, V. W., Zhang, Y., Kwon, Y., Yeung, S., and Zou,\nJ. Y. Mind the gap: Understanding the modality gap\nin multi-modal contrastive representation learning. Ad-\nvances in Neural Information Processing Systems, 35:\n17612–17625, 2022.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In European conference on\ncomputer vision, pp. 740–755. Springer, 2014.\nLiu, S., Nie, W., Wang, C., Lu, J., Qiao, Z., Liu, L., Tang,\nJ., Xiao, C., and Anandkumar, A. Multi-modal molecule\nstructure–text model for text-based retrieval and editing.\nNature Machine Intelligence, 5(12):1447–1457, 2023.\nLyu, Y., Zheng, X., Zhou, J., and Wang, L. Unibind: Llm-\naugmented unified and balanced representation space to\nbind them all. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp.\n26752–26762, 2024.\nManco, I., Benetos, E., Quinton, E., and Fazekas, G. Con-\ntrastive audio-language learning for music. arXiv preprint\narXiv:2208.12208, 2022.\nManzoor, M. A., Albarri, S., Xian, Z., Meng, Z., Nakov,\nP., and Liang, S. Multimodality representation learning:\nA survey on evolution, pretraining and its applications.\nACM Transactions on Multimedia Computing, Communi-\ncations and Applications, 20(3):1–34, 2023.\nMoreira, G. d. S. P., Osmulski, R., Xu, M., Ak, R., Schif-\nferer, B., and Oldridge, E. Nv-retriever: Improving text\nembedding models with effective hard-negative mining.\narXiv preprint arXiv:2407.15831, 2024.\nPark, J., Lee, J., and Sohn, K. Bridging vision and lan-\nguage spaces with assignment prediction. arXiv preprint\narXiv:2404.09632, 2024.\nPiczak, K. J. Esc: Dataset for environmental sound classi-\nfication. In Proceedings of the 23rd ACM international\nconference on Multimedia, pp. 1015–1018, 2015.\nPlummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C.,\nHockenmaier, J., and Lazebnik, S. Flickr30k entities:\nCollecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE\ninternational conference on computer vision, pp. 2641–\n2649, 2015.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pp. 8748–8763. PmLR, 2021.\nR¨osch, P. J., Oswald, N., Geierhos, M., and Libovick`y,\nJ. Enhancing conceptual understanding in multimodal\ncontrastive learning through hard negative samples. arXiv\npreprint arXiv:2403.02875, 2024.\n10\n"}, {"page": 11, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nRSNA.\nRSNA\nPneumonia\nDetection\nChallenge.\nhttps://www.kaggle.com/competitions/\nrsna-pneumonia-detection-challenge,\n2018. [Online; accessed 28-Aug-2018].\nSenocak, A., Oh, T.-H., Kim, J., Yang, M.-H., and Kweon,\nI. S. Learning to localize sound source in visual scenes. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 4358–4366, 2018.\nShi, P., Welle, M. C., Bj¨orkman, M., and Kragic, D. Towards\nunderstanding the modality gap in clip. In ICLR 2023\nworkshop on multimodal representation learning: perks\nand pitfalls, 2023.\nSun, J., Zhang, Q., Kailkhura, B., Yu, Z., Xiao, C., and\nMao, Z. M. Benchmarking robustness of 3d point cloud\nrecognition against common corruptions. arXiv preprint\narXiv:2201.12296, 2022.\nUy, M. A., Pham, Q.-H., Hua, B.-S., Nguyen, T., and Yeung,\nS.-K. Revisiting point cloud classification: A new bench-\nmark dataset and classification model on real-world data.\nIn Proceedings of the IEEE/CVF international conference\non computer vision, pp. 1588–1597, 2019.\nWang, X., Wang, F., Li, Y., Ma, Q., Wang, S., Jiang, B.,\nand Tang, J. Cxpmrg-bench: Pre-training and bench-\nmarking for x-ray medical report generation on chexpert\nplus dataset. In Proceedings of the Computer Vision and\nPattern Recognition Conference, pp. 5123–5133, 2025.\nWang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Li, X.,\nChen, G., Chen, X., Wang, Y., et al. Internvid: A large-\nscale video-text dataset for multimodal understanding\nand generation. arXiv preprint arXiv:2307.06942, 2023a.\nWang, Z., Zhao, Y., Huang, H., Liu, J., Yin, A., Tang, L., Li,\nL., Wang, Y., Zhang, Z., and Zhao, Z. Connecting multi-\nmodal contrastive representations. Advances in Neural In-\nformation Processing Systems, 36:22099–22114, 2023b.\nWang, Z., Zhang, Z., Cheng, X., Huang, R., Liu, L., Ye, Z.,\nHuang, H., Zhao, Y., Jin, T., Gao, P., et al. Freebind: Free\nlunch in unified multimodal space via knowledge fusion.\narXiv preprint arXiv:2405.04883, 2024a.\nWang, Z., Zhang, Z., Zhang, H., Liu, L., Huang, R., Cheng,\nX., Zhao, H., and Zhao, Z. Omnibind: Large-scale omni\nmultimodal representation via binding spaces.\narXiv\npreprint arXiv:2407.11895, 2024b.\nWilliams, A., Nangia, N., and Bowman, S.\nA broad-\ncoverage challenge corpus for sentence understanding\nthrough inference. In Proceedings of the 2018 confer-\nence of the North American chapter of the association for\ncomputational linguistics: human language technologies,\nvolume 1 (long papers), pp. 1112–1122, 2018.\nWu, Y., Chen, K., Zhang, T., Hui, Y., Berg-Kirkpatrick, T.,\nand Dubnov, S. Large-scale contrastive language-audio\npretraining with feature fusion and keyword-to-caption\naugmentation. In ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 1–5. IEEE, 2023.\nXiao, C., Chan, H. P., Zhang, H., Xu, W., Aljunied, M.,\nand Rong, Y. Scaling language-centric omnimodal rep-\nresentation learning. arXiv preprint arXiv:2510.11693,\n2025.\nXiao, T., Cui, C., Zhu, H., and Honavar, V. G. Molbind:\nMultimodal alignment of language, molecules, and pro-\nteins. arXiv preprint arXiv:2403.08167, 2024.\nXu, J., Mei, T., Yao, T., and Rui, Y. Msr-vtt: A large video\ndescription dataset for bridging video and language. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 5288–5296, 2016.\nYou, K., Gu, J., Ham, J., Park, B., Kim, J., Hong, E. K.,\nBaek, W., and Roh, B. Cxr-clip: Toward large scale\nchest x-ray language-image pre-training. In International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, pp. 101–111. Springer, 2023.\nYuan, Y., Li, Z., and Zhao, B. A survey of multimodal learn-\ning: Methods, applications, and future. ACM Computing\nSurveys, 57(7):1–34, 2025.\nZhang, Y., Gong, K., Zhang, K., Li, H., Qiao, Y.,\nOuyang, W., and Yue, X.\nMeta-transformer: A uni-\nfied framework for multimodal learning. arXiv preprint\narXiv:2307.10802, 2023a.\nZhang, Y., HaoChen, J. Z., Huang, S.-C., Wang, K.-C., Zou,\nJ., and Yeung, S. Diagnosing and rectifying vision models\nusing language. arXiv preprint arXiv:2302.04269, 2023b.\nZhang, Y., Sui, E., and Yeung-Levy, S. Connect, collapse,\ncorrupt: Learning cross-modal tasks with uni-modal data.\narXiv preprint arXiv:2401.08567, 2024a.\nZhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B.,\nXie, P., Yang, A., Liu, D., Lin, J., et al. Qwen3 embed-\nding: Advancing text embedding and reranking through\nfoundation models. arXiv preprint arXiv:2506.05176,\n2025.\nZhang, Z., Wang, Z., Liu, L., Huang, R., Cheng, X., Ye,\nZ., Liu, H., Huang, H., Zhao, Y., Jin, T., et al. Extend-\ning multi-modal contrastive representations. Advances\nin Neural Information Processing Systems, 37:91880–\n91903, 2024b.\nZhou, J., Wang, J., Ma, B., Liu, Y.-S., Huang, T., and Wang,\nX. Uni3d: Exploring unified 3d representation at scale.\narXiv preprint arXiv:2310.06773, 2023.\n11\n"}, {"page": 12, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nZhu, B., Lin, B., Ning, M., Yan, Y., Cui, J., Wang, H.,\nPang, Y., Jiang, W., Zhang, J., Li, Z., et al. Language-\nbind: Extending video-language pretraining to n-modality\nby language-based semantic alignment. arXiv preprint\narXiv:2310.01852, 2023.\nZiller, A., Usynin, D., Braren, R., Makowski, M., Rueck-\nert, D., and Kaissis, G. Medical imaging deep learning\nwith differential privacy. Scientific Reports, 11(1):13524,\n2021.\n12\n"}, {"page": 13, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nA. Semantic Textual Similarity Benchmark Analysis\nTo validate our choice of LLM embeddings as the semantic anchor space, we conduct comprehensive evaluation on the\nSemantic Textual Similarity (STS) benchmark suite. Table 6 presents Spearman correlation scores across six STS tasks\n(STS12–16 and STSBenchmark) comparing multimodal encoders (CLIP (Radford et al., 2021), LanguageBind (Zhu et al.,\n2023)) against LLM embedding models (NV-Embed-v2 (Lee et al., 2024), Qwen3-Embedding variants (Zhang et al., 2025)).\nTable 6. Semantic Textual Similarity (STS) benchmark performance. Spearman correlation (ρ) scores across six STS tasks are\nreported, comparing multimodal encoders and LLM embedding models.\nModel\nSTS Tasks (Spearman ρ)\nAvg.\nSTS12\nSTS13\nSTS14\nSTS15\nSTS16\nSTSBench\nMultimodal Encoders\nCLIP\n61.87\n63.83\n62.09\n76.82\n72.89\n72.26\n68.29\nLanguageBind\n63.12\n67.46\n63.27\n73.82\n73.73\n71.60\n68.83\nLLM Embedding Models\nNV-Embed-v2\n77.89\n88.30\n84.30\n89.04\n86.77\n88.41\n85.79\nQwen3-Embed-0.6B\n79.35\n87.31\n79.81\n87.28\n87.07\n86.51\n84.56\nQwen3-Embed-4B\n84.31\n93.20\n88.61\n92.31\n92.07\n91.92\n90.40\nB. Extended Experimental Results\nThis appendix provides comprehensive experimental results that supplement the main findings in Section 4. We report\ndetailed metrics across all evaluation benchmarks to enable thorough comparison and reproducibility.\nB.1. Evaluation Benchmarks\nTable 7 provides a complete overview of all evaluation benchmarks. In the main text (Table 1), we report representative\nbenchmarks per modality for clarity: Flickr30k for image, MSVD for video, AudioCaps for audio, and DrugBank for\nmolecule retrieval; ESC-50, ModelNet40, ScanObjectNN, and RSNA for classification.\nTable 7. Complete evaluation benchmarks. All datasets used for retrieval and classification tasks are organized by task type and target\nmodality.\nTask\nModality\nDatasets\nText→X Retrieval\nImage\nCOCO (Lin et al., 2014), Flickr30k (Plummer et al., 2015)\nVideo\nMSRVTT (Xu et al., 2016), MSVD (Chen & Dolan, 2011), DiDeMo (Anne Hendricks et al., 2017)\nAudio\nAudioCaps (Kim et al., 2019), Clotho (Drossos et al., 2020)\nMolecule\nDrugBank (Knox et al., 2024)\nEmergent X→X\nAudio→Image\nFlickrNet (Senocak et al., 2018)\n3D→Image\nObjaverse (Deitke et al., 2023)\nZero-shot Cls.\n3D\nModelNet40 (Sun et al., 2022), ScanObjectNN (Uy et al., 2019)\nAudio\nAudioSet (Gemmeke et al., 2017), ESC-50 (Piczak, 2015)\nX-ray\nRSNA (RSNA, 2018)\nB.2. Detailed Cross-Modal Retrieval Performance\nTable 1 in the main text reports representative R@1 metrics for brevity. Here, we provide complete retrieval results including\nR@5 metrics, which capture whether relevant items appear within the top-5 retrieved candidates. This relaxed criterion is\nparticularly informative for assessing approximate semantic alignment quality.\nAs shown in Table 8, TextME consistently achieves higher PPR on R@5 compared to R@1 across all benchmarks (e.g.,\n75.6% vs. 59.3% on COCO, 98.4% vs. 89.7% on MSVD). This pattern indicates that text-only training effectively preserves\ncoarse-grained semantic structure, with the performance gap primarily arising from fine-grained ranking precision. Notably,\non MSVD, TextME achieves near-perfect R@5 preservation (98.4%), suggesting that the learned projections successfully\ncapture the underlying semantic relationships for video-text alignment.\n13\n"}, {"page": 14, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nTable 8. Detailed Text→Image and Text→Video retrieval performance. R@1 and R@5 metrics are reported. PPR denotes Performance\nPreservation Ratio (%) relative to pretrained encoders (CLIP for Image, ViCLIP for Video).\nImage Retrieval\nVideo Retrieval\nCOCO\nFlickr30k\nMSRVTT\nMSVD\nR@1\nR@5\nR@1\nR@5\nR@1\nR@5\nR@1\nR@5\nPretrained 48.29 72.51 77.70 94.16 37.00 63.70 51.06 78.29\nTextME\n28.63 54.81 51.66 77.90 26.40 50.50 45.82 77.01\nPPR (%)\n59.3\n75.6\n66.5\n82.7\n71.4\n79.3\n89.7\n98.4\nTable 9. Detailed Text→Audio and Text→Molecule retrieval performance. R@1 and R@5 metrics are reported for audio, and\nMRR@10 and MRR@20 for molecule retrieval. PPR denotes Performance Preservation Ratio (%) relative to pretrained encoders (CLAP\nfor Audio, MoleculeSTM for Molecule).\nAudio Retrieval\nMolecule Retrieval\nAudioCaps\nClotho\nDrugBank\nR@1\nR@5\nR@1\nR@5 MRR@10 MRR@20\nPretrained 22.47 54.43 16.90 39.75\n79.19\n69.17\nTextME\n15.35 43.88\n7.81\n23.81\n34.75\n27.97\nPPR (%)\n68.3\n80.6\n46.2\n59.9\n43.9\n40.4\nThe audio retrieval results exhibit a similar trend, with R@5 preservation consistently exceeding R@1. However, the\nmolecule retrieval task shows lower overall preservation rates, which we attribute to the highly specialized vocabulary in\nchemical descriptions that differs substantially from the general text distributions used in LLM pretraining.\nB.3. Detailed Zero-shot Classification Results\nTable 10 provides complete zero-shot classification results including Top-5 accuracy.\nTable 10. Detailed zero-shot classification performance. Top-1 and Top-5 accuracy are reported across audio (ESC-50) and 3D\n(ModelNet40, ScanObjectNN) modalities, along with mAP for AudioSet. PPR denotes Performance Preservation Ratio (%) relative to\npretrained encoders.\nAudioSet\nESC-50\nModelNet40\nScanObjectNN\nMethod\nmAP\nTop-1\nTop-5\nTop-1\nTop-5\nTop-1\nTop-5\nPretrained\n9.32\n85.20\n97.70\n67.75\n90.07\n42.21\n77.23\nLanguageBind\n18.33\n94.00\n99.70\n–\n–\n–\n–\nEx-MCR\n6.67\n71.20\n96.80\n66.53\n93.60\n40.31\n77.20\nNa¨ıve\n1.14\n2.90\n8.45\n0.81\n8.95\n3.32\n30.52\nCOX†\n1.26\n2.00\n10.00\n4.05\n13.70\n2.84\n26.68\nTextME\n5.80\n77.25\n96.85\n70.86\n92.14\n42.15\n77.89\nPPR (%)\n62.2\n90.7\n99.1\n104.6\n102.3\n99.9\n100.9\nNotably, TextME achieves PPR exceeding 100% on 3D classification benchmarks (ModelNet40: 104.6%, ScanObjectNN:\n99.9%), demonstrating that text-only training can sometimes improve upon pretrained encoder performance. Top-5\npreservation consistently exceeds Top-1, indicating that approximate categorical boundaries are well-preserved even when\nprecise rankings differ.\nC. Implementation Details\nC.1. Model Architecture\nEach projection network Pm is implemented as a 2-layer MLP with GeLU activation:\nPm(x) = W2 · GeLU(W1 · x + b1) + b2\n(6)\n14\n"}, {"page": 15, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nwhere the hidden dimension matches the source encoder’s embedding dimension dm, and the output dimension is fixed at\ndh = 2560 to match Qwen3-Embedding. Total trainable parameters per modality: ∼10M.\nC.2. Training Configuration\nTable 11. Training hyperparameters. All projection networks are trained with the same configuration across modalities.\nHyperparameter\nValue\nBatch size\n512\nOptimizer\nAdamW (β1 = 0.9, β2 = 0.999)\nWeight decay\n0.01\nLearning rate\n5 × 10−4\nLR schedule\nCosine annealing\nTraining epochs\n50\nTemperature τ\n0.07\nHard negative range\n[0.1 · si, 0.9 · si]\nPrecision\nfp16\nC.3. Offset Computation\nFor each modality, we compute centroids using 5,000 randomly sampled text-modal pairs. Table 12 summarizes the datasets\nused for offset estimation across all evaluated encoders.\nTable 12. Datasets used for offset computation. For each encoder, centroids are estimated using 5,000 randomly sampled text-modal\npairs from the listed datasets.\nEncoder\nModality\nOffset Dataset\nDomain\nCLIP\nImage\nCOCO (Lin et al., 2014)\nNatural images\nViCLIP\nVideo\nMSRVTT (Xu et al., 2016)\nWeb videos\nCLAP\nAudio\nAudioCaps (Kim et al., 2019)\nAudio events\nUni3D\n3D\nObjaverse (Deitke et al., 2023)\nSynthetic objects\nCXR-CLIP\nX-ray\nCheXpert (Irvin et al., 2019)\nMedical imaging\nMoleculeSTM\nMolecule\nPubChem (Kim et al., 2025)\nChemical compounds\nLanguageBind\nText\nCOCO (Lin et al., 2014)\nNatural images\nText inputs are tokenized with a maximum sequence length of 77 tokens. Offsets are pre-computed once and remain fixed\nthroughout training.\nWe note that the choice of dataset for offset computation may influence both the estimated gap properties and downstream\nperformance. Since the modality gap is computed as the difference between text and modal centroids, the semantic\ndistribution of the offset dataset could affect the resulting offset vector. For instance, encoders whose offset datasets\nclosely match the evaluation domain may exhibit more favorable gap properties, while domain mismatch between offset\ncomputation and downstream evaluation could introduce additional variability. Although our experiments demonstrate\nrobust performance across diverse benchmarks, a systematic investigation of how offset dataset selection affects alignment\nquality remains an important direction for future work.\nC.4. Computational Resources\nAll experiments are conducted on a single NVIDIA A6000 GPU (48GB). Training time averages 2 hours per modality with\npeak memory usage of approximately 8GB.\nD. COX Baseline Implementation\nSince the original COX (Huang et al., 2025) codebase is not publicly available, we re-implemented the method following\nthe paper specifications with adaptations for our zero-shot evaluation setting.\nArchitecture. We employ Vision Transformer Tiny (ViT-T/16) as the encoder backbone with 12 layers, 3 attention heads,\nand embedding dimension 192. Following the original design, we incorporate a Variational Information Bottleneck (VIB)\n15\n"}, {"page": 16, "text": "TextME: Bridging Unseen Modalities Through Text Descriptions\nlayer with stochastic dimensionality reduction to 256 dimensions.\nTraining Protocol. We follow the two-stage methodology: (1) supervised pre-training on labeled target data for 10 epochs,\nand (2) information bottleneck fine-tuning for 50 epochs. We use batch size 256, Adam optimizer with learning rate 1×10−3\nand weight decay 1 × 10−5.\nKey Difference from TextME. COX requires labeled target modality data (∼10K samples), trains encoders from scratch\n(>300M parameters), and demands architectural alignment between source and target encoders. In contrast, TextME\nleverages pre-trained encoders with only text descriptions, requires merely ∼10M trainable parameters, and imposes no\narchitectural constraints.\nE. Additional Ablation Studies\nE.1. Sample Size for Offset Estimation\nWe investigate sensitivity to the number of samples used for computing centering offsets.\nTable 13. Impact of sample size for offset estimation. Performance remains stable for N ≥1,000.\n# Samples\nAudio R@1\n3D Top-1\nMol. MRR\nRel. Perf.\n100\n14.91\n70.66\n34.75\n90%\n500\n14.77\n70.58\n33.05\n95%\n1,000\n14.89\n70.62\n36.44\n97%\n5,000 (default)\n15.35\n70.86\n34.75\n100%\n10,000\n14.95\n70.58\n32.20\n100%\nResults demonstrate that offset estimation is robust to sample size, with performance plateauing between 1,000–10,000\nsamples. Even with only 100 samples, the method achieves 90% of default performance, validating the efficiency of our\napproach.\nE.2. Offset Noise Sensitivity\nTo assess robustness to offset estimation errors, we perturb the pre-computed offset ∆with additive Gaussian noise:\n∆′ = ∆+ N(0, σ2I).\nTable 14. Offset noise sensitivity. Text→Audio retrieval (R@1), 3D classification (Top-1), and Text→Molecule retrieval (MRR) are\nreported. Performance degrades gracefully for σ < 0.05.\nNoise σ\nAudio R@1\n3D Top-1\nMol. MRR\n0.000\n14.95\n70.46\n27.97\n0.001\n14.95\n70.30\n24.58\n0.01\n15.04\n67.50\n22.88\n0.05\n14.93\n34.32\n17.80\n0.10\n14.25\n14.91\n11.02\nAudio demonstrates remarkable stability, maintaining near-baseline performance even at σ = 0.10. In contrast, 3D and\nMolecule show sharper degradation at σ ≥0.05, indicating these modalities require more precise offset estimation. For\npractical deployment, 5,000 samples provide sufficient precision with empirical standard error well below σ = 0.01.\n16\n"}]}