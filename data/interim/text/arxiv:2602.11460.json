{"doc_id": "arxiv:2602.11460", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.11460.pdf", "meta": {"doc_id": "arxiv:2602.11460", "source": "arxiv", "arxiv_id": "2602.11460", "title": "ADRD-Bench: A Preliminary LLM Benchmark for Alzheimer's Disease and Related Dementias", "authors": ["Guangxin Zhao", "Jiahao Zheng", "Malaz Boustani", "Jarek Nabrzyski", "Meng Jiang", "Yiyu Shi", "Zhi Zheng"], "published": "2026-02-12T00:38:21Z", "updated": "2026-02-12T00:38:21Z", "summary": "Large language models (LLMs) have shown great potential for healthcare applications. However, existing evaluation benchmarks provide minimal coverage of Alzheimer's Disease and Related Dementias (ADRD). To address this gap, we introduce ADRD-Bench, the first ADRD-specific benchmark dataset designed for rigorous evaluation of LLMs. ADRD-Bench has two components: 1) ADRD Unified QA, a synthesis of 1,352 questions consolidated from seven established medical benchmarks, providing a unified assessment of clinical knowledge; and 2) ADRD Caregiving QA, a novel set of 149 questions derived from the Aging Brain Care (ABC) program, a widely used, evidence-based brain health management program. Guided by a program with national expertise in comprehensive ADRD care, this new set was designed to mitigate the lack of practical caregiving context in existing benchmarks. We evaluated 33 state-of-the-art LLMs on the proposed ADRD-Bench. Results showed that the accuracy of open-weight general models ranged from 0.63 to 0.93 (mean: 0.78; std: 0.09). The accuracy of open-weight medical models ranged from 0.48 to 0.93 (mean: 0.82; std: 0.13). The accuracy of closed-source general models ranged from 0.83 to 0.91 (mean: 0.89; std: 0.03). While top-tier models achieved high accuracies (>0.9), case studies revealed that inconsistent reasoning quality and stability limit their reliability, highlighting a critical need for domain-specific improvement to enhance LLMs' knowledge and reasoning grounded in daily caregiving data. The entire dataset is available at https://github.com/IIRL-ND/ADRD-Bench.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.11460v1", "url_pdf": "https://arxiv.org/pdf/2602.11460.pdf", "meta_path": "data/raw/arxiv/meta/2602.11460.json", "sha256": "cf43761f2fe063c13d6dd9240cc3180df7c08d083fe6eba803af290294e49d80", "status": "ok", "fetched_at": "2026-02-18T02:19:22.108224+00:00"}, "pages": [{"page": 1, "text": "ADRD-Bench: A Preliminary LLM Benchmark for Alzheimerâ€™s\nDisease and Related Dementias\nGuangxin Zhao\nElectrical Engineering\nUniversity of Notre Dame\ngzhao23@nd.edu\nJiahao Zheng\nComputer Science and Engineering\nUniversity of Notre Dame\njzheng7@nd.edu\nMalaz Boustani\nSchool of Medicine\nIndiana University\nmboustan@iu.edu\nJarek Nabrzyski\nComputer Science and Engineering\nUniversity of Notre Dame\nnaber@nd.edu\nMeng Jiang\nComputer Science and Engineering\nUniversity of Notre Dame\nmjiang2@nd.edu\nYiyu Shi\nComputer Science and Engineering\nUniversity of Notre Dame\nyshi4@nd.edu\nZhi Zheng\nElectrical Engineering\nUniversity of Notre Dame\nzzheng3@nd.edu\nAbstract\nLarge language models (LLMs) have shown great potential for\nhealthcare applications. However, existing evaluation benchmarks\nprovide minimal coverage of Alzheimerâ€™s Disease and Related De-\nmentias (ADRD). To address this gap, we introduce ADRD-Bench,\nthe first ADRD-specific benchmark dataset designed for rigorous\nevaluation of LLMs. ADRD-Bench has two components: 1) ADRD\nUnified QA, a synthesis of 1,352 questions consolidated from seven\nestablished medical benchmarks, providing a unified assessment\nof clinical knowledge; and 2) ADRD Caregiving QA, a novel set\nof 149 questions derived from the Aging Brain Care (ABC) pro-\ngram, a widely used, evidence-based brain health management\nprogram. Guided by a program with national expertise in compre-\nhensive ADRD care, this new set was designed to mitigate the lack of\npractical caregiving context in existing benchmarks. We evaluated\n33 state-of-the-art LLMs on the proposed ADRD-Bench. Results\nshowed that the accuracy of open-weight general models ranged\nfrom 0.63 to 0.93 (mean: 0.78; std: 0.09). The accuracy of open-weight\nmedical models ranged from 0.48 to 0.93 (mean: 0.82; std: 0.13). The\naccuracy of closed-source general models ranged from 0.83 to 0.91\n(mean: 0.89; std: 0.03). While top-tier models achieved high accura-\ncies (>0.9), case studies revealed that inconsistent reasoning quality\nand stability limit their reliability, highlighting a critical need for\ndomain-specific improvement to enhance LLMsâ€™ knowledge and\nreasoning grounded in daily caregiving data. The entire dataset is\navailable at https://github.com/IIRL-ND/ADRD-Bench.\nKeywords\nLarge Language Models, Alzheimerâ€™s Disease and Related Demen-\ntias (ADRD), Medical Benchmark\n1\nIntroduction\nLarge language models (LLMs) have achieved remarkable progress\nin healthcare applications. They have been used for assisting clini-\ncal decision support, providing medical education [51], improving\ndiagnostic reasoning, and enhancing patient care [29]. For example,\nthe GPT series has demonstrated the ability to answer medical\nlicensure examinations at or near the passing threshold [26, 31];\nPaLM [13] and its variants achieved encouraging performance on\nspecialty-specific tasks [40]. However, using LLMs in healthcare\ncarries substantial risks due to potential LLM hallucinations [42],\nwhich might be harmless in the general domain but can be danger-\nous in healthcare that requires high precision and rigor [34]. Re-\nsearch has pointed out that LLMs providing incorrect drug dosages\nor misdiagnosing conditions may jeopardize patient safety [25].\nTherefore, rigorous evaluations are necessary before LLM deploy-\nment. Recent surveys of medical LLMs also highlighted mitigating\nhallucinations and establishing domain-specific benchmarks as crit-\nical steps for safety in clinical practice [21, 40, 51].\nAlzheimerâ€™s disease and Related Dementias (ADRD) represent a\ncritical medical domain, with over 57 million people worldwide liv-\ning with ADRD as of 2021 and nearly 10 million new cases diagnosed\neach year [47]. Managing ADRD is not only a medical but also a soci-\netal challenge, as it heavily involves caregiving, long-term support,\nand complex decision-making by families [7]. LLMs are promising\nin serving ADRD care needs due to their capability of communica-\ntion in natural language and cognitive support [43]. For example,\nwith proper design, LLM-driven content may boost patientsâ€™ cogni-\ntive abilities and enrich interactions, while offering opportunities\nto improve diagnostic strategies and patient-caregiver communi-\ncation [43]. Like any other medical device, LLMs for ADRD care\nshould be evaluated for technical performance before planning for\nreal-world deployment [46]. However, to the best of our knowledge,\nthere has not been an existing ADRD-specific LLM benchmark for\nthis purpose. Some existing medical benchmarks include ADRD-\nrelated items; however, our investigation showed that these items\nconstitute only a tiny proportion of the total content, mostly under\n1%, indicating that the ADRD domain is largely absent from current\nLLM performance evaluations. In addition, we found that there are\nalmost no caregiving types of testing in existing datasets. While\nLLMs are often targeted to provide support for caregivers, missing\nthe caregiving type leaves a huge gap for real-world applications.\narXiv:2602.11460v1  [cs.CL]  12 Feb 2026\n"}, {"page": 2, "text": "To mitigate this gap, we introduce an ADRD-specific bench-\nmark for evaluating LLMs. This benchmark has two parts (ADRD\nUnified QA set and ADRD Caregiving QA set) constructed from\ntwo complementary sources. The ADRD Unified QA set includes\n1,352 ADRD-related items extracted from seven existing bench-\nmark datasets. By aggregating these scattered items into a single\nset, we create a concentrated evaluation of how well current models\nhandle ADRD knowledge that was previously only sparsely repre-\nsented. The ADRD Caregiving QA set includes 149 new questions\nderived from the well-validated Aging Brain Care (ABC) program\nwhich is one of the USâ€™s leading evidence-based dementia care mod-\nels [4, 5, 27]. From de-identified ABC caregiver education materials,\nwe crafted question-answer pairs that reflect realistic caregiving\ncontent, including daily caring situations (e.g., patient aggressive\nbehavior management at home). The design of the questions was\nguided by a senior clinician with national expertise in compre-\nhensive ADRD care to ensure that the questions are grounded in\ndomain expert-verified knowledge and caregiving strategies.\nIn summary, our work presents the first benchmark tailored\nto ADRD tasks. It brings together a wide range of question types\n(e.g., factual Q&A, clinical reasoning, caregiver guidance) under\nthe ADRD umbrella. This resource enables a focused technical as-\nsessment of LLM capabilities in a domain where factual accuracy\nand empathetic reasoning are paramount. We hope that evaluating\nLLMs on ADRD-specific tasks will help identify their current capa-\nbilities and limitations in this domain and drive the development\nof safer, more reliable medical AI for the science of ADRD care.\n2\nRelated Work\n2.1\nCurrent Medical Benchmarks\nThe advent of GPT-3 and successor models spurred extensive re-\nsearch into medical applications of LLMs [51], leading to the de-\nvelopment of many benchmarks to evaluate LLMsâ€™ performance.\nGeneral multitask evaluations such as MMLU assess knowledge\nacross 57 subjects (from math and history to law and medicine),\nproviding a broad measure of academic and professional under-\nstanding [18]. Medical-specific benchmarks have also emerged.\nResearchers used United States Medical Licensing Exam (USMLE)\nquestion banks to assess models, where GPT-3.5/ChatGPT achieved\n~60% accuracy (the passing threshold) without specialized train-\ning [26]. Medical Q&A datasets such as PubMedQA [20], HEAD-\nQA [45], MedQA [19], MedMCQA [35], Medbullets [12] have been\nused to quantify clinical knowledge recall. More recently, Googleâ€™s\nMultiMedQA benchmark combined several of these resources, span-\nning professional medical exams, research questions, and consumer\nhealth queries, to holistically evaluate medical LLM knowledge [40].\nOpenAIâ€™s HealthBench [8] focuses on open-ended generation tasks,\nutilizing 48,562 unique rubric criteria to enable more realistic and\ngranular clinical evaluations.\nThese benchmarks have driven rapid progresses; for instance,\nan instruction-tuned PaLM model (Med-PaLM) achieved state-of-\nthe-art (SOTA) performance on multiple-choice questions, reaching\n67.6% accuracy on MedQA [40]. Despite these gains, high scores\non structured, exam-style questions do not necessarily translate\nto real-world clinical competence [23]. Human evaluations reveal\nTable 1: Number of total and ADRD-related questions in\ncommonly used public healthcare benchmarks.\nBenchmark Name\nNumber of\nQuestions\nNumber of ADRD-\nRelated Questions\nNon-Generation Tasks\nMEDCALC-BENCH\n11,000\n112 (1.02%)\nMEDEC\n3,360\n29 (0.86%)\nHEAD-QA\n6,765\n41 (0.61%)\nMedBullets\n616\n12 (1.95%)\nMedQA\n12,723\n98 (0.77%)\nMedMCQA\n193,155\n1,148 (0.59%)\nPubMedQA\n1,000\n12 (1.20%)\nMedHallu\n1,000\n12 (1.20%)\nTotal\n229,619\n1,464 (0.64%)\nGeneration Tasks\nHealthBench\n5,000\n46 (0.92%)\nMedicationQA\n690\n3 (0.43%)\nMedDialog\n181,122\n909 (0.50%)\nMEDIQA\n383\n50 (13.05%)\nTotal\n187,195\n1,008 (0.54%)\npersistent reasoning errors, limited robustness, and a lack of trust-\nworthiness outside narrow formats [40]. This has reinforced the\nconsensus that new evaluation frameworks are needed, ones that\nextend beyond exam accuracy to assess real-world clinical compe-\ntencies [51].\nIn response, researchers have begun creating specialized eval-\nuation sets targeting particular failure modes. MEDEC [2] is the\nfirst publicly available benchmark for medical error detection and\ncorrection in clinical notes. MedHallu [36] focuses on medical hal-\nlucination detection, comprising question-answer pairs from Pub-\nMedQA with deliberately fabricated answers, challenging a model\nto judge correctness, and results show that even top models strug-\ngle to reliably identify incorrect medical statements. In addition, as\nshown in Table 1, except for MEDIQA [1], a small dataset, the pro-\nportion of ADRD-related questions in each benchmark is generally\nbelow 3%, mostly under 1%. Collectively, these findings indicate\nthat ADRD-specific evaluation remains underdeveloped. We aim to\naddress this gap by introducing a benchmark focused on the ADRD\ndomain, which has been largely overlooked in prior works.\n2.2\nLLMs for ADRD Applications\nNatural language processing research in the ADRD field has tradi-\ntionally been dominated by ADRD detection tasks (e.g., automated\nscreening and diagnosis). For example, many works focus on screen-\ning for cognitive impairment [38], and numerous studies tackle the\ndiagnosis of dementia from patient speech or text [3]. A recent sys-\ntematic review found that about 56% of ADRD-related NLP studies\nfocus on detecting cognitive impairment, such as through classi-\nfying transcripts [38]. In contrast, far fewer studies have explored\nNLP for ADRD care and support. The same review identifies two\nemerging task categories beyond detection: caregiver support and\npatient assistance. However, only âˆ¼6% of existing works fell under\ncaregiver support, highlighting the nascent state of this area [38].\nNevertheless, there is growing interest in applying NLP and\nLLMs to ADRD. Recent work explored caregiver-oriented dialogue\n2\n"}, {"page": 3, "text": "systems, such as ADQueryAid [17], which leverages ADRD-specific\nknowledge and retrieval augmentation to provide more relevant\nand supportive responses than general-purpose models, as well as\ndomain-adapted language models such as the Caregiving Language\nModel (CaLM) [37], fine-tuned on dementia care data. Beyond direct\nQ&A, LLMs have also been applied to analyze caregiver narratives\nand clinical texts, for example, to identify overburdened informal\ncaregivers from long-term care notes. Collectively, these studies\nillustrate the emerging interests in language technologies beyond\ndetection tasks, while highlighting the need for rigorous, ADRD-\nspecific evaluation.\nExisting medical benchmarks provide very limited coverage of\nADRD-related tasks, and the questions for evaluating models in\ndaily caregiving scenarios are even more scarce. As a result, highly\nrelevant ADRD caregiving questions, such as how to respond when\na patient becomes agitated at night, are absent from current evalua-\ntions, leaving little systematic, quantitative evidence of how LLMs\nperform in the realities of daily caring. To address this gap, we con-\nstruct an evaluation set that integrates the fragmented, knowledge-\noriented questions from seven existing healthcare benchmarks with\na complementary set of daily caregiving questions grounded on\nthe ABC model. Overall, this new benchmark dataset provides an\ninitial testbed for assessing LLM performance across both ADRD\nclinical knowledge and the practical, human-centered aspects of\ndaily care, enabling more realistic, rigorous, and targeted research\non ADRD-related LLM applications.\n3\nADRD Benchmark Construction\n3.1\nADRD Unified QA\nTo identify ADRD-related questions from existing benchmarks, we\nsearched 12 public benchmark datasets using ADRD domain key-\nwords containing â€œAlzheimerâ€™sâ€, â€œDementiaâ€, â€œADRDâ€, and â€œADâ€.\nExcept for MEDIQA, the proportion of ADRD-related questions in\neach benchmark is below 3%, and most are under 1%. The propor-\ntion of MEDIQA is 13.05%; however, since MEDIQA contains 383\nquestions, 13.05% leads to 50 questions only. Across all benchmarks,\nthe total number of ADRD-related questions accounts for 0.64% of\ntotal non-generation task questions and 0.54% of generation task\nquestions, respectively. In addition, manual checking showed that\nthere were only seven questions in total about caregiversâ€™ tasks.\nHere, a caregiver can be either formal or informal, defined as a\nperson who provides paid or unpaid assistance with daily living\nand cognitive support as a family member or close friend, or de-\nlivers such care as a non-nurse, non-medical licensed personnel\n(e.g., community health workers, certified nurse aides). These low\npercentages show that existing medical benchmarks contain only a\ntiny proportion of ADRD-relevant content, especially on caregiver\ntasks, indicating a substantial gap and leaving significant room for\nfuture exploration. The results are shown in Table 1.\nWe consolidated ADRD questions from seven existing non-\ngeneration task benchmarks in Table 1, including PubMedQA [20],\nHEAD-QA [45], MedBullets [12], MedMCQA [35], MedQA [19],\nMEDEC [2], and MedHallu [36]. These datasets have been the lead-\ning healthcare benchmarks to test SOTA LLMs [24, 40]. This led\nto a unified question and answer set, named ADRD Unified QA.\nWe retain the fully original benchmark content in its entirety with-\nout modification. MEDCALC-BENCH [22] is excluded from our\nbenchmark because we focus on evaluating modelsâ€™ ADRD-related\nknowledge and reasoning capabilities, whereas this dataset empha-\nsizes numerical calculation and arithmetic reasoning. In addition,\nADRD Unified QA does not include generation tasks, as existing\nevaluation metrics are insufficient to reliably and consistently assess\nthe quality of LLM responses [40].\nTherefore, ADRD Unified QA consists of two primary question\ntypes. The first comprises multiple-choice questions designed to\nevaluate modelsâ€™ knowledge of ADRD-related concepts, which were\nfrom PubMedQA, HEAD-QA, MedBullets, MedMCQA, and MedQA.\nThe second focuses on error detection, assessing modelsâ€™ ability to\nidentify hallucinations and factual errors present in textual inputs,\nwhich were from MEDEC and MedHallu.\n3.2\nADRD Caregiving QA\nOne notable limitation of existing benchmarks is that ADRD ques-\ntions are almost exclusively framed as clinical knowledge or exam-\nstyle reasoning tasks, with almost no coverage of daily caregiving.\nFor instance, a manual check of the seven datasets used in section\n3.1 yielded only seven caregiving items. However, managing ADRD\nin real-world settings extends far beyond diagnosis and pharma-\ncological treatment, and largely revolves around day-to-day care-\ngiving challenges such as behavioral management, communication\nwith patients, safety supervision, and caregiver decision-making in\nhome and community settings.\nTo address this gap, we constructed an additional ADRD Caregiv-\ning QA set based on materials from the Aging Brain Care (ABC) pro-\ngram. The ABC program is a comprehensive, evidence-based model\nof care for Alzheimerâ€™s disease and related dementias (ADRD) that\nhas been continuously developed, evaluated, and implemented for\nnearly two decades [4, 5, 27]. Supported initially by the U.S. Agency\nfor Healthcare Research and Quality, the ABC model was designed\nas a longitudinal, biopsychosocial intervention to address the com-\nplex clinical, behavioral, and caregiver challenges associated with\nADRD. In 2006, the program demonstrated efficacy in a random-\nized controlled trial, showing significant reductions in behavioral\nand psychological symptoms of dementia, caregiver burden, and\ninappropriate medication use among underserved populations with\nhigh social frailty [11]. Since then, the ABC program has been con-\ntinuously operated, refined, and scaled, serving more than 5,000\npatientâ€“caregiver dyads [10, 15, 27]. Recognized by the National\nAcademies of Medicine, ABC integrates interdisciplinary team-\nbased care, non-pharmacologic interventions, caregiver support,\nmedication optimization, care coordination, and community part-\nnerships across the full ADRD trajectory. ABC directly informed the\ndevelopment of an alternative payment approach for comprehen-\nsive dementia care, later adopted by the U.S. Centers for Medicare\n& Medicaid Services through the GUIDE Model [9, 16]. As such,\nthe ABC program represents a mature, rigorously evaluated, and\npolicy-relevant standard for high-quality ADRD care.\nTherefore, the ABC program naturally serves as a solid founda-\ntion to extract benchmark questions. The ADRD Caregiving QA set\ncovers essential dimensions of ADRD-related caregiving, such as un-\nderstanding the condition, effective communication with patients,\nand behavioral management strategies. We follow a structured\n3\n"}, {"page": 4, "text": "abstraction process in which recurring caregiving situations and\nrecommended care strategies were identified from the education\ncontent and caregiver handout, and transformed into evaluable\nquestion-answer pairs. The resulting QA sets include 120 polar\n(True/False) QAs and 29 multiple-choice QAs. To ensure data qual-\nity, all constructed questions were reviewed and refined for accu-\nracy, clarity, and appropriateness by a senior clinician who served\nas the lead designer of the ABC program and has been involved in\nits development and refinement for two decades.\nAlthough modest in size, this question set provides targeted\ncoverage of ADRD daily caring scenarios that are largely absent\nfrom existing benchmarks. By integrating these questions with\nthe extracted ADRD Unified QA set, the ADRD-Bench enables a\nmore comprehensive evaluation of LLMs that spans both clinical\nknowledge and practical caregiving support for ADRD.\n3.3\nQuantitative and Qualitative Evaluations\nTo validate the performance of LLMs on the proposed ADRD-Bench,\nwe tested 33 LLMs as listed in Table 2, including both open-weight\nLLMs and closed-source LLMs. The open-weight LLMs included\ngeneral and medical LLMs, varying in size from 3.8B to 235B pa-\nrameters. This evaluation on a diverse set of models with different\nparameter scales and deployment settings not only helped establish\na comprehensive understanding of the modelsâ€™ performance but\nalso the overall difficulty level of the benchmark across various\nmodel settings.\nFor open-weight models with 14.8B parameters or fewer, all\nexperiments were conducted on a local workstation with an NVIDIA\nRTX 6000 Ada Generation GPU (48 GB VRAM). For larger open-\nweight models with 32B parameters and above, experiments are\nconducted on an Oracle Cloud Infrastructure server with eight\nNVIDIA H100 GPUs (80 GB HBM3 memory each). All closed-source\ngeneral models were evaluated via their official APIs/terminals.\nFor quantitative evaluation, given that all tasks in our benchmark\nare formulated as discrete-choice questions, including multiple-\nchoice, True/False, and error sentence detection formats, we adopt\nexact match accuracy as the primary metric. A modelâ€™s prediction is\nconsidered correct if and only if the selected option exactly matches\nthe ground-truth label. For each model, we reported accuracies\nseparately for each set of the benchmark and the overall accuracy\naggregated across all questions.\nFor qualitative analysis, we focused on the newly proposed\nADRD Caregiving QA set to better understand model failures in\ndaily caregiving scenarios, which have not been covered in the liter-\nature. Specifically, we selected questions on which LLMs frequently\nproduced incorrect answers, as indicated by the quantitative results.\nThe selected cases were reviewed by our clinical expert to analyze\ncommon error patterns. The qualitative analyses were intended\nto contextualize the findings and highlight practical limitations of\ncurrent LLMs in daily caregiving scenarios.\n4\nResults and Analyses\nTable 2 shows the accuracy of 33 LLMs on each subset of ADRD\nUnified QA and ADRD Caregiving QA. LLMs are grouped into open-\nweight general, open-weight medical, and closed-source general\ncategories, with model size indicated where applicable. Boldface\nindicates the best performance for each task, while underlining\ndenotes the second-best result.\n4.1\nLLM Performance on ADRD Unified QA\nFigure 1 shows a significant, moderate-to-strong positive correla-\ntion (Pearsonâ€™s ð‘Ÿ= 0.5486, ð‘= 0.0025) between model size and\naccuracy on the ADRD Unified QA set. Most smaller models in\nthe 4â€“7B parameter range achieved relatively low performance\n(approximately 0.45â€“0.75), whereas large models (70B+ parameters)\ndemonstrated substantially higher accuracy, mostly â‰¥0.85. More-\nover, most models with more than 30B parameters performed above\nthe overall mean accuracy (0.7972). The best-performing model was\nLlama-3.1-70B-Instruct [30], which achieved an accuracy of 0.9334.\nWhen comparing different model categories, open-weight medi-\ncal models (red points) generally tended to outperform open-weight\ngeneral models (blue points) of comparable size. Across the param-\neter scale, open-weight medical models were mostly positioned\nabove open-weight general models, highlighting the benefits of\ndomain-specific fine-tuning for clinical question answering and\nmedical error detection. However, an exception was observed for\nthe top-performing model, Llama-3.1-70B-Instruct [30], which is\na general model and slightly outperformed the strongest medical\nmodel, Meditron3-70B [33] (0.9312). All closed-source general mod-\nels achieved performance above the overall mean accuracy, with\nGemini 2.5 Pro [14] performing the best among them, reaching an\naccuracy of 0.9142. Nevertheless, it was surpassed by several open-\nweight models (e.g., Llama-3.1-70B-Instruct and Meditron3-70B)\nwith significantly fewer parameters. In addition, some open-weight\nmedical LLMs exhibited substantially lower accuracy than open-\nweight general models with similar parameter counts, indicating\nthat domain-specific fine-tuning does not universally guarantee\nsuperior performance.\nOverall, these results suggest that model size is an important\ncontributor to performance on the ADRD Unified QA benchmark;\nhowever, it does not fully determine model effectiveness. Domain-\nspecific fine-tuning may provide performance advantages in certain\ncases, but general models can still outperform specialized models,\npossibly due to stronger general reasoning capabilities and more ro-\nbust language representations learned from large-scale and diverse\npretraining corpora.\n4.2\nLLM Performance on ADRD Caregiving QA\nOn the ADRD Caregiving QA set (Figure 2), the performance gap be-\ntween smaller and larger models was narrower than that observed\non the ADRD Unified QA set. This may be attributed to the fact\nthat the questions in this benchmark focus more on daily living and\nare closer to â€œcommon-senseâ€ reasoning, which is generally well\ncovered by large language models. Nevertheless, an upward trend\nin accuracy with increasing model size remained evident. Pearsonâ€™s\ncorrelation analysis revealed a significant, moderate-to-strong pos-\nitive correlation between model size and accuracy (ð‘Ÿ= 0.5277,\nð‘= 0.0039). Most smaller models in the 4Bâ€“7B parameter range\nachieved accuracies between 0.75 and 0.85, all of which (except\nfor the smallest model, phi3:3.8b-mini-128k-instruct-fp16 [39]) fell\nbelow the overall mean accuracy of 0.8629. In contrast, the majority\n4\n"}, {"page": 5, "text": "Table 2: Accuracy of LLMs on ADRD-Bench.\nADRD Unified QA\nADRD Caregiving QA\nModel Name\nModel\nSize\nMed\nMCQA\nMed\nQA\nHEAD-\nQA\nMED\nEC\nPubMed\nQA\nMed\nBullets\nMed\nHallu\nScore\nTrue/\nFalse\nMultiple\nChoice\nScore\n1148 (size)\n98\n41\n29\n12\n12\n12\n1352\n120\n29\n149\nOpen-Weight General Model\nphi3:3.8b-mini-128k-instruct-fp16\n3.8B\n0.6455\n0.6020\n0.8049\n0.1034\n0.8333\n0.6667\n0.8333\n0.6391\n0.8833\n0.8276\n0.8725\nqwen3-4b-instruct-2507-fp16\n4B\n0.7108\n0.6429\n0.8293\n0.1379\n0.8333\n0.7500\n0.9167\n0.7004\n0.8833\n0.7586\n0.8591\nfalcon3.7b-instruct-fp16\n7.5B\n0.6838\n0.5714\n0.7317\n0.2069\n0.8333\n0.6667\n0.5833\n0.6672\n0.8583\n0.7241\n0.8322\nqwen2.5-7b-instruct-fp16\n7.6B\n0.7308\n0.6224\n0.7317\n0.3793\n0.7500\n0.6667\n0.8333\n0.7160\n0.8583\n0.7241\n0.8322\nllama3.3-8b-instruct-fp16\n8B\n0.9251\n0.6837\n0.7317\n0.1034\n0.7500\n0.9167\n0.6667\n0.8802\n0.8167\n0.6897\n0.7919\nllama3.1-8b-instruct-fp16\n8B\n0.8780\n0.6939\n0.7805\n0.0000\n0.8333\n0.7500\n0.7500\n0.8402\n0.8417\n0.7586\n0.8255\nyi-9b-chat-v1.5-fp16\n9B\n0.6185\n0.5408\n0.7561\n0.1034\n0.8333\n0.5000\n0.9167\n0.6095\n0.8583\n0.7241\n0.8322\ngemma2:9b-instruct-fp16\n9.2B\n0.7387\n0.6020\n0.7561\n0.2069\n0.7500\n0.6667\n0.9167\n0.7189\n0.8750\n0.7931\n0.8591\ngemma3:12b-it-fp16\n12.2B\n0.7439\n0.6327\n0.8049\n0.1379\n0.7500\n0.5833\n0.8333\n0.7241\n0.9083\n0.7931\n0.8859\nphi3:14b-medium-4k-instruct-fp16\n14B\n0.7622\n0.7449\n0.8293\n0.2069\n0.8333\n0.8333\n0.8333\n0.7530\n0.8917\n0.7586\n0.8658\nqwen2.5:14b-instruct-fp16\n14.8B\n0.7787\n0.6531\n0.8049\n0.2069\n0.7500\n0.6667\n0.8333\n0.7574\n0.8750\n0.7241\n0.8456\nQwen3-30B-A3B-Instruct-2507\n30B\n0.8467\n0.7551\n0.8537\n0.4483\n0.7500\n0.8333\n0.8333\n0.8306\n0.9083\n0.7586\n0.8792\nLlama-3.1-70B-Instruct\n70B\n0.9495\n0.9286\n0.8537\n0.4483\n0.8333\n0.9167\n1.0000\n0.9334\n0.9333\n0.7931\n0.9060\nQwen2.5-72B-Instruct\n70B\n0.8571\n0.7347\n0.8780\n0.2414\n0.7500\n0.9167\n0.8333\n0.8351\n0.8917\n0.8276\n0.8792\nQwen3-235B-A22B-Instruct-2507\n235B\n0.8902\n0.8980\n0.9024\n0.4828\n0.7500\n0.9167\n0.9167\n0.8817\n0.9167\n0.8966\n0.9128\nOpen-Weight Medical Model\nMedGemma-4b-it\n4B\n0.7343\n0.6020\n0.8049\n0.2069\n0.6667\n0.7500\n0.5833\n0.7138\n0.8250\n0.5862\n0.7785\nHulu-Med-4B\n4B\n0.8667\n0.8163\n0.8293\n0.3793\n0.8333\n0.6667\n0.7500\n0.8484\n0.8417\n0.8276\n0.8389\nLingshu-7B\n7B\n0.6977\n0.7143\n0.8293\n0.1724\n0.7500\n0.8333\n0.7500\n0.6938\n0.8333\n0.6897\n0.8054\nHulu-Med-7B\n7B\n0.9077\n0.8469\n0.8049\n0.2414\n0.8333\n0.9167\n0.8333\n0.8846\n0.8667\n0.7931\n0.8523\nBioMistral-7B\n7B\n0.4634\n0.4388\n0.6585\n0.3103\n0.5000\n0.5833\n0.6667\n0.4675\n0.6750\n0.4483\n0.6309\nHulu-Med-14B\n14B\n0.9268\n0.9286\n0.8537\n0.3103\n0.8333\n1.0000\n0.8333\n0.9105\n0.9000\n0.7586\n0.8725\nHulu-Med-32B\n32B\n0.9373\n0.9592\n0.9024\n0.2759\n0.8333\n0.9167\n0.9167\n0.9223\n0.9333\n0.7931\n0.9060\nLingshu-32B\n32B\n0.8249\n0.9082\n0.8293\n0.5172\n0.8333\n0.9167\n0.8333\n0.8254\n0.9667\n0.8621\n0.9463\nLlama3-OpenBioLLM-70B\n70B\n0.9460\n0.9082\n0.8293\n0.4483\n0.7500\n1.0000\n0.7500\n0.9260\n0.9167\n0.7931\n0.8926\nClinicalCamel-70B\n70B\n0.6568\n0.5816\n0.7561\n0.3793\n0.5833\n0.5833\n1.0000\n0.6501\n0.8667\n0.7586\n0.8456\nMed42-70B\n70B\n0.9077\n0.8980\n0.8537\n0.3448\n0.8333\n0.6667\n1.0000\n0.8913\n0.8833\n0.7586\n0.8591\nMeditron3-70B\n70B\n0.9460\n0.9082\n0.9268\n0.4483\n0.8333\n1.0000\n0.9167\n0.9312\n0.9250\n0.7931\n0.8993\nMMed-Llama3.1-70B\n70B\n0.9564\n0.9184\n0.9268\n0.0345\n0.7500\n0.9167\n0.5000\n0.9268\n0.9000\n0.7586\n0.8725\nClosed-Source General Model\nGemini 2.5 Pro\nâ€“\n0.9207\n0.9694\n0.9268\n0.4828\n0.8333\n1.0000\n0.8333\n0.9142\n0.9250\n0.7931\n0.8993\nqwen3-max-2025-09-23\nâ€“\n0.9024\n0.9388\n0.9268\n0.4138\n0.8333\n0.9167\n0.9167\n0.8950\n0.9417\n0.8276\n0.9195\nGPT-5.2\nâ€“\n0.8981\n0.8878\n0.9024\n0.9310\n0.8333\n0.8333\n0.7500\n0.8957\n0.9667\n0.9655\n0.9664\nclaude-opus-4.5-20251101\nâ€“\n0.9181\n0.9490\n0.9268\n0.6207\n0.8333\n1.0000\n0.8333\n0.9135\n0.9333\n0.8966\n0.9262\ngrok-4.1-fast-non-reasoning\nâ€“\n0.8328\n0.8367\n0.8780\n0.3793\n0.8333\n0.8333\n0.9167\n0.8254\n0.9000\n0.8276\n0.8859\nBoldface: the best performance; Underlining: the second-best result. Within each QA set, all questions across subcategories are equally weighted; scores are computed over all\nquestions.\nof models with more than 14B parameters performed above the\naverage accuracy.\nExcept for grok-4-1-fast-non-reasoning [48] and Gemini 2.5\nPro [14], all other closed-source general models achieved accuracies\nabove 0.9, outperforming the majority of open-weight models. This\nobservation suggests that closed-source general models currently\nexhibit stronger overall capabilities in ADRD daily caregiving tasks.\nGPT-5.2 [32] achieved the best performance among all evaluated\nmodels, with an accuracy of 0.9664, slightly surpassing the best-\nperforming open-weight medical model, Lingshu-32B [49] (0.9463),\nas well as the best-performing open-weight general model, Qwen3-\n235B-A22B-Instruct-2507 [50] (0.9128).\nMedical models did not demonstrate a clear performance ad-\nvantage over general models. In a few cases, they underperformed\ngeneral models of comparable size. This pattern suggested that\nanswering practical daily caregiving related questions relied more\non broad commonsense reasoning capabilities typically acquired\nby large general models, rather than on specialized medical fine-\ntuning. At the same time, this observation might reflect the fact\nthat current domain-specific fine-tuning efforts primarily focus on\n5\n"}, {"page": 6, "text": "Figure 1: Model accuracy vs. model size on the ADRD Unified\nQA. Blue points: open-weight general models; Red points:\nopen-weight medical models; Green points: closed-source\ngeneral models. Dashed horizontal line: mean accuracy\nacross all models.\nFigure 2: Model accuracy vs. model size on the ADRD Care-\ngiving QA. Blue points: open-weight general models; Red\npoints: open-weight medical models; Green points: closed-\nsource general models. Dashed horizontal line: mean accu-\nracy across all models.\nmedical knowledge and incorporate relatively limited information\nabout daily caregiving.\n4.3\nCorrelation Analysis\nTo understand how modelsâ€™ performance on clinical knowledge is\nrelated to that on caregiving, we analyzed the correlation between\naccuracies on ADRD Caregiving QA and accuracies on the ADRD\nUnified QA, as shown in Figure 3 and 4. Across all models, the\nPearsonâ€™s correlation analysis revealed a significant and strong\npositive correlation between the two QA sets (ð‘Ÿ= 0.6801, ð‘=\n0.00001). This indicated that, at an aggregated level, models that\nperformed well on clinical questions also tended to achieve a higher\naccuracy on the caregiving questions.\nHowever, when examining different model categories separately,\nwe observed that only open-weight medical models exhibited this\nsignificant and strong correlation (ð‘Ÿ= 0.8416, ð‘= 0.0003). In\ncontrast, open-weight general models (ð‘Ÿ= 0.2730, ð‘= 0.3248)\nFigure 3: Correlation between accuracies on ADRD Caregiv-\ning QA and accuracies on ADRD Unified QA on all models,\nwhere larger points mean larger model parameters.\nFigure 4: Correlation between accuracies on ADRD Caregiv-\ning QA and accuracies on ADRD Unified QA on (a) general\nmodels; (b) medical models; (c) closed-source models.\nand closed-source models (ð‘Ÿ= 0.4678, ð‘= 0.4269) showed only\nweak-moderate, non-significant positive correlations. These results\ndemonstrated that open-weight medical models, which incorpo-\nrated specific medical fine-tuning, had more consistent performance\nacross both clinical and caregiving questions. A possible cause of\nthe non-significant correlations of the general models could be the\nrelatively small portion of medical data in model training, and thus\nthe modelsâ€™ performance depends more on general common sense\nand social reasoning. This contributes to more socially appropriate\nresponses, a key point in daily caregiving. This analysis also aligned\nwith our earlier observation that, on ADRD Caregiving QA, open-\nweight general models achieved higher accuracy than open-weight\nmedical models at comparable parameter scales. On the other hand,\n6\n"}, {"page": 7, "text": "these findings also implied that, to develop healthcare LLMs that fit\nADRD care needs, more training data on caregiving tasks should\nbe included to balance the modelsâ€™ knowledge and reasoning on\nthis aspect.\n4.4\nCase Analysis\nTo better understand large language model (LLM) performance\nbeyond accuracy metrics, we examined the modelsâ€™ reasoning pro-\ncesses on ADRD Caregiving QA questions for which over 75% of the\nevaluated models generated incorrect answers. In reviewing these\nchallenging cases, feedback from a clinical expert was incorpo-\nrated to ensure that the analysis was grounded in real-world ADRD\ncaregiving principles. Two representative cases are presented to\nillustrate common failure patterns and model behaviors.\n4.4.1\nCase 1: True/False question. As shown in Figure 5, the correct\nanswer is â€œFalse,â€ as the statement overgeneralizes across all stages\nof Alzheimerâ€™s disease. In the early stages, individuals are often still\nable to perform most basic activities independently, and the loss of\nself-care ability typically occurs in moderate to severe ADRD stages.\nHowever, 85% LLMs answered â€œTrue,â€ implying that all individuals\nwith Alzheimerâ€™s disease are universally unable to manage daily\nliving activities. This result reflected a common failure pattern of\novergeneralization.\nFigure 6 shows LLMsâ€™ reasoning steps obtained via Zero-Shot\nChain-of-Thought prompting. This zero-shot setting was selected to\nenable models to explicitly show their reasoning processes, while\nmitigating the influence of example selection biases. Complete\nprompts and outputs are in the Appendix.\nThe output of claude-opus-4-5-20251101 showed another typical\ntype of error. This model achieved an overall accuracy exceeding\n90% on the ADRD Caregiving QA and was able to distinguish be-\ntween different stages of Alzheimerâ€™s disease and recognize that\npatients in the early stage generally retain the ability to perform\nactivities of daily living. However, this LLM based its final judgment\non a hallmark of the advanced stage and used it to generalize a\nconclusion across the entire disease course, producing an incor-\nrect answer. This suggested a tendency to rigidly focus on salient\nfeatures, while overlooking conditional factors like disease progres-\nsion. This could also be understood as a bias that when a fact was\ncommonly associated with a condition, some LLMs tended to treat\nit as a universal truth, without adequately reasoning about scope\nor exceptions.\n4.4.2\nCase 2: Multiple-Choice question. Figure 7 shows a multiple-\nchoice question example. In patients living with ADRD, losing eye\ncontact during a conversation often signals cognitive overload, fa-\ntigue, or a need to disengage, rather than a visual tracking problem.\nThus, the recommended approach is to respect the individualâ€™s need\nfor space and reduce stimulation, rather than attempting to imme-\ndiately re-engage them. However, 79% LLMs chose â€œBâ€. It reflects\nan LLM tendency to treat behavioral cues as attention problems\nto be corrected, prioritizing task or conversation continuity over\nrespecting disengagement and emotion regulation.\nFigure 8 presents examples of LLMsâ€™ key reasoning steps. The\nprompting and the entire outputs are in the Appendix. Both qwen3-\nmax-2025-09-23 and claude-opus-4-5-20251101 achieved an overall\nFigure 5: True/False question example of ADRD Caregiving\nQA that most LLMs answered incorrectly.\nFigure 6: Reasoning by llama3:8b-instruct-fp16 and claude-\nopus-4-5-20251101 of Case 1.\naccuracy of > 90% on the ADRD Caregiving QA. However, both\nfailed to answer this question. The models could identify that a\npatient living with dementia might experience attentional, percep-\ntual, or visual field challenges. Then, both subsequently treated\nthis partial clinical insight as sufficient justification for active re-\nengagement, recommending the caregiver to reposition and thus\nre-enter the individualâ€™s visual field. This reasoning reflects a form\nof goal misalignment. Rather than evaluating caregiving interaction\nthrough the loss of eye contact, the models focused on preserving\nconversational continuity. In doing so, they implicitly assumed that\nmaintaining engagement is always desirable, failing to recognize\nthat, in ADRD care, averted gaze is often a low-intensity disengage-\nment or overload signal rather than a problem to be corrected. Un-\nlike errors stemming from a lack of medical knowledge, this failure\nis the inability to correctly interpret contextual qualifiers embed-\nded in dementia care, particularly those related to disengagement\nand emotion regulation. When demonstrating the understanding of\ndementia-related perceptual limitations, many LLMs emphasized\ncognitive characteristics more than care principles. As a result,\ndisengagement cues were framed as attention deficits, leading to\nrecommending actions that may increase pressure or distress.\n5\nLimitations and Ethical Considerations\nADRD-Bench does not involve confidential data. All content was\nderived from publicly available, de-identified resources. The ADRD\nCaregiving QA items were developed from widely used, clinically\nvalidated educational resources and reviewed by a clinician with\nnational expertise in comprehensive ADRD care to ensure quality\nand relevance. This benchmark is intended for research and evalua-\ntion purposes. We caution against the direct use of LLMsâ€™ outputs\nin real-world clinical or caregiving settings without appropriate\n7\n"}, {"page": 8, "text": "Figure 7: Multiple-choice example of ADRD Caregiving QA\nthat most models answered incorrectly.\nFigure 8: Reasoning by qwen3-max-2025-09-23 and claude-\nopus-4-5-20251101 of Case 2.\nhuman oversight. Responsible use and safety should remain central\nconsiderations in downstream applications.\nAs the first and a preliminary benchmark, this work has sev-\neral limitations. Since the ADRD Unified QA portion was drawn\nfrom existing public benchmarks, there was a possibility that some\ncontext had been used in training certain models. Such exposure\ncould artificially boost performance scores. Current questions of\nADRD Caregiving QA are in multiple-choice or true/false format,\nwhich forces models to choose an answer even when they might be\nuncertain. This may alter performance scores by educated guesses\nor the exploitation of answer cues [28]. We did not include open-\nended generative tasks, since automatic evaluation of long-form\nresponses remains challenging [40]. As a result, this benchmark\nprimarily emphasizes factual correctness and does not measure a\nmodelâ€™s ability in conversational interactions, such as proactively\ndemonstrating empathy.\nThe current questions are single-turn Q&A pairs. Multi-turn\ndialogues and interactive problem-solving are not included yet\nto test a modelâ€™s ability in carrying out extended conversations,\nsuch as counseling a caregiver through a stressful situation over\nseveral exchanges, an important aspect of real-world AI assistance.\nIn addition, caregiving decisions are often context-dependent and\nmay vary across patient needs and professional perspectives. At this\nstage, ADRD-Bench only covers a small scope of ADRD knowledge,\nand it should be expanded by the broader community in future\niterations for better coverage and precision.\nFinally, the benchmark is developed in English and rooted in the\nABC model, primarily focusing on ADRD care in the U.S. Therefore,\nADRD-Benchâ€™s generalizability to other languages and healthcare\nsystems needs further investigation. Note that achieving high accu-\nracy on this benchmark does not guarantee an LLM will be reliable\nor effective in real clinical or home-care deployment. The bench-\nmark is a step toward identifying strengths and weaknesses. Users\nshould be cautious not to over-interpret the results, but use them\nin accordance with other necessary evaluations (e.g., user studies).\n6\nConclusion, Discussion, and Future Work\nWe presented ADRD-Bench, the first benchmark dedicated to ADRD.\nBy curating 1,352 ADRD-focused questions from seven existing\nmedical QA datasets and introducing 149 daily caregiving ques-\ntions from national, widely adopted caregiver education materials,\nthis work provided an evaluation platform for both factual clinical\nknowledge and practical caregiving reasoning. The evaluation of\nSOTA LLMs on ADRD-Bench yielded several key findings. On one\nhand, top models achieved high accuracies on questions (best per-\nformance was 0.9334 on ADRD Unified QA and 0.9664 on ADRD\nCaregiving QA, respectively). On the other hand, failure modes\nbeyond numerical accuracy were discovered. For example, LLMs\noften overgeneralize clinical facts and sometimes fail to identify the\nmajor goal in a question [23]. Therefore, high overall accuracy does\nnot guarantee true competency in ADRD care [23]. Even advanced\nLLMs may produce misguided responses due to shallow reasoning\nor misalignment with care goals [44]. Our benchmark and evalua-\ntion together lay a groundwork for rigorously assessing LLMs in a\nhigh-impact, yet underdeveloped area of medical AI [41].\nTo ensure safer and more effective AI for ADRD care, domain-\nspecific alignment techniques that bridge the gap between clinical\nknowledge and caregiving practices should be a focus in future\nresearch [44]. This benchmark and our findings reinforce the im-\nportance of rigorous, targeted evaluation in high-stakes domains.\nAs with any other medical specialty, ADRD care is a complex and\nevolving field [6]. As a result, benchmark questions and answers\nneed to be updated in a timely manner according to the state-of-the-\nart practice and research. We encourage the broader community to\nbuild on this work to advance safe, reliable, and compassionate AI\nin the ADRD domain.\nAcknowledgments\nThis research was partially supported by the National Science Foun-\ndation under Grant No. 2426639. The authors acknowledge Oracle\nCloud Infrastructure for providing computing resources that con-\ntributed to parts of the research results reported.\n8\n"}, {"page": 9, "text": "References\n[1] Asma Ben Abacha, Chaitanya Shivade, and Dina Demner-Fushman. 2019.\nOverview of the MEDIQA 2019 shared task on textual inference, question entail-\nment and question answering. In Proceedings of the 18th BioNLP workshop and\nshared task. 370â€“379.\n[2] Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen-Yildiz,\nFei Xia, and Thomas Lin. 2025. MedEC: A benchmark for medical error detection\nand correction in clinical notes. In Findings of the Association for Computational\nLinguistics: ACL 2025. 22539â€“22550.\n[3] Felix Agbavor and Hualou Liang. 2022. Predicting dementia from spontaneous\nspeech using large language models. PLOS Digital Health 1, 12 (2022), e0000168.\n[4] Catherine A Alder, Michael A LaMantia, Mary Guerriero Austrom, and Malaz A\nBoustani. 2024. The Indiana Aging Brain Care Project. In Geriatrics Models of\nCare: Bringing Best Practice to an Aging America. Springer, 317â€“324.\n[5] Alzheimerâ€™s Association. 2024. Dementia Care Delivery and Accreditation. Re-\ntrieved January 22, 2026 from https://www.alz.org/professionals/health-systems-\nmedical-professionals/health-systems/dementia-care-delivery-accreditation\n[6] Alzheimerâ€™s Association. 2024. Mapping a Better Future for Dementia Care Navi-\ngation. Retrieved January 22, 2026 from https://www.alz.org/getmedia/a7390c29-\ne4c0-4d3c-b000-377b6c112eea/alzheimers-facts-and-figures-special-\nreport.pdf\n[7] Alzheimerâ€™s Association. 2024.\n2024 Alzheimerâ€™s disease facts and figures.\nAlzheimerâ€™s & Dementia 20, 5 (2024), 3708â€“3821. doi:10.1002/alz.13809\n[8] Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin\nQuiÃ±onero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, An-\ndrea Vallone, Alex Beutel, et al. 2025. HealthBench: Evaluating large language\nmodels towards improved human health. arXiv preprint arXiv:2505.08775 (2025).\n[9] Malaz Boustani, Catherine A. Alder, Craig A. Solid, and David Reuben. 2019. An\nalternative payment model to support widespread use of collaborative dementia\ncare models. Health Affairs 38, 1 (2019), 54â€“59.\n[10] Malaz A Boustani, Steven R Counsell, Anthony Perkins, Abdelfattah Alhader,\nKathryn I Frank, Diana P Summanwar, and Karen L Fortuna. 2025. Effectiveness\nof a Telephonic Aging Brain Care Model for Medicaid Home and Community\nServices for Dementia Patients and Their Caregivers. Journal of the American\nGeriatrics Society (2025).\n[11] Christopher M. Callahan, Malaz A. Boustani, Frederick W. Unverzagt, Mary G.\nAustrom, Teresa M. Damush, Anthony J. Perkins, Bridget A. Fultz, Siu L. Hui,\nSteven R. Counsell, and Hugh C. Hendrie. 2006. Effectiveness of collaborative\ncare for older adults with Alzheimer disease in primary care: A randomized\ncontrolled trial. JAMA 295, 18 (2006), 2148â€“2157.\n[12] Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. 2025. Benchmark-\ning large language models on answering and explaining challenging medical\nquestions. In Proceedings of the 2025 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers). 3563â€“3599.\n[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2023. PaLM: Scaling language modeling with pathways.\nJournal of Machine Learning Research 24, 240 (2023), 1â€“113.\n[14] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen\nSachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen,\net al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multi-\nmodality, long context, and next generation agentic capabilities. arXiv preprint\narXiv:2507.06261 (2025).\n[15] Dustin D. French, Michael A. LaMantia, Lee R. Livin, Dorian Herceg, Catherine A.\nAlder, and Malaz A. Boustani. 2014. Healthy Aging Brain Center improved care\ncoordination and produced net savings. Health Affairs 33, 4 (2014), 613â€“618.\n[16] Kristin Lees Haggerty, David B. Reuben, Rebecca Stoeckle, David Bass, Malaz\nBoustani, Carolyn Clevenger, Ian Kremer, David R. Lee, Madelyn Johnson, Mor-\ngan J. Minyo, et al. 2025. GUIDE and Beyond: Strategies for Comprehensive\nDementia Care Integration. Journal of the American Geriatrics Society 73, 11\n(2025), 3548â€“3554.\n[17] Wordh Ul Hasan, Kimia Tuz Zaman, Xin Wang, Juan Li, Bo Xie, and Cui Tao. 2024.\nEmpowering alzheimerâ€™s caregivers with conversational AI: A novel approach for\nenhanced communication and personalized support. npj Biomedical Innovations\n1, 1 (2024), 3.\n[18] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. 2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300 (2020).\n[19] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter\nSzolovits. 2021. What disease does this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Applied Sciences 11, 14 (2021),\n6421.\n[20] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019.\nPubMedQA: A dataset for biomedical research question answering. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP). 2567â€“2577.\n[21] Mert Karabacak and Konstantinos Margetis. 2023. Embracing large language\nmodels for medical applications: Opportunities and challenges. Cureus 15, 5\n(2023).\n[22] Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina Applebaum,\nZain Anwar, Maame Sarfo-Gyamfi, Conrad Safranek, Abid Anwar, Andrew Zhang,\net al. 2024. Medcalc-bench: Evaluating large language models for medical calcula-\ntions. Advances in Neural Information Processing Systems 37 (2024), 84730â€“84745.\n[23] Jonathan Kim, Anna Podlasek, Kie Shidara, Feng Liu, Ahmed Alaa, and Danilo\nBernardo. 2025. Limitations of large language models in clinical problem-solving\narising from inflexible reasoning. Scientific Reports 15, 1 (2025), 39426.\n[24] Siun Kim and Hyung-Jin Yoon. 2025. Questioning our questions: How well do\nmedical QA benchmarks evaluate clinical capabilities of language models?. In\nProceedings of the 24th Workshop on Biomedical Language Processing. 274â€“296.\n[25] Yubin Kim, Hyewon Jeong, Shan Chen, Shuyue Stella Li, Chanwoo Park, Mingyu\nLu, Kumail Alhamoud, Jimin Mun, Cristina Grau, Minseok Jung, et al. 2025.\nMedical hallucinations in foundation models and their impact on healthcare.\narXiv preprint arXiv:2503.05777 (2025).\n[26] Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De\nLeon, Camille ElepaÃ±o, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido,\nJames Maningo, et al. 2023. Performance of ChatGPT on USMLE: Potential for\nAI-assisted medical education using large language models. PLOS Digital Health\n2, 2 (2023), e0000198.\n[27] Michael A. LaMantia, Catherine A. Alder, Christopher M. Callahan, Sujuan Gao,\nDustin D. French, Mary G. Austrom, Karim Boustany, Lee Livin, Bharath Bynagari,\nand Malaz A. Boustani. 2015. The aging brain care medical home: Preliminary\ndata. Journal of the American Geriatrics Society 63, 6 (2015), 1209â€“1213.\n[28] Yesheng Liu, Hao Li, Haiyu Xu, Baoqi Pei, Jiahao Wang, Mingxuan Zhao, Jing-\nShu Zheng, Zheqi He, J. G. Yao, Bowen Qin, Xi Yang, and Jiajun Zhang. 2025.\nBeyond multiple choice: A hybrid framework for unifying robust evaluation and\nverifiable reasoning training. arXiv preprint arXiv:2511.17405 (2025).\n[29] Subhankar Maity and Manob Jyoti Saikia. 2025. Large language models in\nhealthcare and medical applications: A review. Bioengineering 12, 6 (2025), 631.\n[30] Meta AI. 2024. Llama-3.1-70B-Instruct. Retrieved January 10, 2026 from https:\n//huggingface.co/meta-llama/Llama-3.1-70B-Instruct\n[31] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric\nHorvitz. 2023. Capabilities of GPT-4 on medical challenge problems. arXiv\npreprint arXiv:2303.13375 (2023).\n[32] OpenAI. 2025. Introducing GPT-5.2.\nRetrieved January 10, 2026 from https:\n//openai.com/index/introducing-gpt-5-2/\n[33] OpenMeditron. 2025. Meditron3-70B. Retrieved January 10, 2026 from https:\n//huggingface.co/OpenMeditron/Meditron3-70B\n[34] Ankit Pal, Pasquale Minervini, Andreas Geert Motzfeldt, Aryo Pradipta Gema,\nand Beatrice Alex. 2024. Open Medical LLM Leaderboard. Retrieved January 22,\n2026 from https://huggingface.co/spaces/openlifescienceai/open_medical_llm_\nleaderboard\n[35] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.\nMedMCQA: A large-scale multi-subject multi-choice dataset for medical domain\nquestion answering. In Conference on Health, Inference, and Learning. 248â€“260.\n[36] Shrey Pandit, Jiawei Xu, Junyuan Hong, Zhangyang Wang, Tianlong Chen, Kaidi\nXu, and Ying Ding. 2025. MedHallu: A comprehensive benchmark for detecting\nmedical hallucinations in large language models. arXiv preprint arXiv:2502.14302\n(2025).\n[37] Bambang\nParmanto,\nBayu\nAryoyudanta,\nTimothius\nWilbert\nSoekinto,\nI. Made Agus Setiawan, Yuhan Wang, Haomin Hu, Andi Saptono, and\nYong Kyung Choi. 2024. A reliable and accessible caregiving language model\n(CaLM) to support tools for caregivers: Development and evaluation study. JMIR\nFormative Research 8 (2024), e54633.\n[38] Lotem Peled-Cohen and Roi Reichart. 2025. A systematic review of NLP for\ndementia: Tasks, datasets, and opportunities. Transactions of the Association for\nComputational Linguistics 13 (2025), 1204â€“1244.\n[39] Phi Team. 2024. Phi-3 technical report: A highly capable language model locally\non your phone. arXiv preprint arXiv:2404.14219 (2024).\n[40] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won\nChung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.\n2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023),\n172â€“180.\n[41] Dirk Steijger, Hannah Christie, Sil Aarts, Wijnand IJselsteijn, Hilde Verbeek, and\nMarjolein de Vugt. 2025. Use of artificial intelligence to support quality of life of\npeople with dementia: A scoping review. Ageing Research Reviews (2025), 102741.\n[42] Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G. Nestor, Ali Soroush, Pierre A.\nElias, Ziyang Xu, Ying Ding, Greg Durrett, and Justin F. Rousseau. 2023. Evalu-\nating large language models on medical evidence summarization. NPJ Digital\nMedicine 6, 1 (2023), 158.\n[43] Matthias S. Treder, Sojin Lee, and Kamen A. Tsvetanov. 2024. Introduction to large\nlanguage models (LLMs) for dementia care and research. Frontiers in Dementia 3\n(2024), 1385303.\n9\n"}, {"page": 10, "text": "[44] Emre Umucu, Guillermina Solis, Leon Garza, Emilia Rivas, Beatrice Lee, Anantaa\nKotal, and Aritran Piplai. 2025. Empathy by design: Aligning large language\nmodels for healthcare dialogue. arXiv preprint arXiv:2512.06097 (2025).\n[45] David Vilares and Carlos GÃ³mez-RodrÃ­guez. 2019. HEAD-QA: A healthcare\ndataset for complex reasoning. arXiv preprint arXiv:1906.04701 (2019).\n[46] World Health Organization. 2024. Ethics and Governance of Artificial Intelligence\nfor Health: Large Multi-Modal Models. WHO Guidance. World Health Organization,\nGeneva, Switzerland.\n[47] World Health Organization. 2025. Dementia. Retrieved January 22, 2026 from\nhttps://www.who.int/news-room/fact-sheets/detail/dementia\n[48] xAI. 2024. Grok 4.1 Fast (Non-Reasoning).\nRetrieved January 10, 2026 from\nhttps://docs.x.ai/docs/models/grok-4-1-fast-non-reasoning\n[49] Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu\nWang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, et al.\n2025. Lingshu: A generalist foundation model for unified multimodal medical\nunderstanding and reasoning. arXiv preprint arXiv:2506.07044 (2025).\n[50] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical\nreport. arXiv preprint arXiv:2505.09388 (2025).\n[51] Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu,\nYiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, et al. 2023. A survey of large\nlanguage models in medicine: Progress, application, and challenge. arXiv preprint\narXiv:2311.05112 (2023).\n10\n"}, {"page": 11, "text": "A\nAppendix\nA.1\nADRD-Bench Prompts\nPrompt for PubMedQA\nYou are a highly knowledgeable AI assistant specializing in biomedical sciences. Answer A for yes, B for no, or C for maybe. Do\nnot include any explanation or additional text. Output only the letter on a single line.\nContext: {context}\nQuestion: {question}\nA. yes\nB. no\nC. maybe\nONLY OUTPUT ONE LETTER.\nYour answer MUST be exactly one of: A, B, or C.\nDo NOT include explanation or text after the letter.\nAnswer:\nPrompt for HEAD-QA (Four Options)\nYou are a highly knowledgeable AI assistant specializing in biomedical sciences. Your task is to answer multiple-choice questions\naccurately based on the options provided. Each question will relate to biomedical concepts, and you will be asked to choose the\nmost appropriate answer.\nSelect the correct answer by outputting only the letter corresponding to your choice (A, B, C, D). Please only use a capital letter,\nwithout including any content or explanation.\nQuestion: {question}\nA. options[0]\nB. options[1]\nC. options[2]\nD. options[3]\nONLY OUTPUT ONE LETTER.\nYour answer MUST be exactly one of: A, B, C, or D.\nDo NOT include explanation or text after the letter.\nAnswer:\nPrompt for MedBullets (Four Options)\nYou are a highly knowledgeable AI assistant specializing in medicine. Your task is to answer medical questions similar to those\nfound on the USMLE Step 2/3 exams. You will be provided with a clinical scenario followed by several multiple-choice options.\nSelect the correct answer by outputting only the letter corresponding to your choice (A, B, C, D). Please only use a capital letter,\nwithout including any content.\nClinical Scenario: {question}\nA. options[0]\nB. options[1]\nC. options[2]\nD. options[3]\nONLY OUTPUT ONE LETTER.\nYour answer MUST be exactly one of: A, B, C or D.\nDo NOT include explanation or text after the letter.\nAnswer:\n11\n"}, {"page": 12, "text": "Prompt for MedMCQA and MedQA\nYou are a highly knowledgeable AI assistant specializing in medicine. Give a letter answer among A, B, C, or D. Do not include\nany explanation or additional text.\nQuestion: {question}\nA. options[0]\nB. options[1]\nC. options[2]\nD. options[3]\nONLY OUTPUT ONE LETTER.\nYour answer MUST be exactly one of: A, B, C, or D.\nDo NOT include explanation or text after the letter.\nAnswer:\nPrompt for MedHallu\nYou are an AI assistant with extensive knowledge in the field of medicine. I want you act as an answer judge. Given a question\nand an answer, your objective is to determine if the provided answer contains non-factual or hallucinated information. You\nSHOULD give your judgment based on the following hallucination types and the world knowledge.\nYou are trying to determine if there is a factual contradiction between the answer and the world knowledge. Some information in\nthe answer might be fabricated. Answer â€™0â€™ if the answer is factual and â€™1â€™ if the answer is hallucinated. You can categorize the\nanswer as hallucinated if it falls in either of the categories:\nType: Misinterpretation of Question - These are hallucinated answers that misunderstand the question, leading to an off-topic or\nirrelevant response.\nType: Incomplete Information - These are hallucinated answers that point out what is not true without providing correct\ninformation.\nType: Mechanism and Pathway Misattribution - These are hallucinated answers that falsely attribute biological mechanisms,\nmolecular pathways, or disease processes that contradict established medical knowledge.\nType: Methodological and Evidence Fabrication - Inventing false research methods, statistical data, or specific clinical outcomes.\nDo not return anything else, just the answer.\nReturn just an integer value, â€™0â€™ if the answer is factual and â€™1â€™ if the answer is hallucinated. No letter or word, just the integer\nvalue.\nWorld Knowledge: {knowledge}\nQuestion: {question}\nAnswer: {hallucinated answer}\nONLY OUTPUT ONE NUMBER.\nYour answer MUST be exactly one of: 0 or 1.\nDo NOT include explanation or text after the letter.\nYour Judgment:\n12\n"}, {"page": 13, "text": "Prompt for MEDEC\nThe following is a medical narrative about a patient. You are a skilled medical doctor reviewing the clinical text.\nThe text is either correct or contains only one error. The text has a sentence per line. Each line starts with the sentence ID,\nfollowed by a space character then the sentence to check. You should carefully check every sentence of the text.\nIf every sentence in the text is correct then only return the following output: -1.\nIf one sentence in the text has a medical error, only return the sentence ID of the sentence containing the error.\nMake sure not to add any other words or sentences.\nClinical Note: {sentences}\nONLY OUTPUT ONE NUMBER.\nYour answer MUST be exactly one number.\nDo NOT include explanation or text after the number.\nAnswer:\nPrompt for ADRD Caregiving Multiple-Choice QA\nYou are a highly knowledgeable AI assistant specializing in medicine. Give a letter answer among A, B, C, D, or E. Do not include\nany explanation or additional text.\nQuestion: {question}\nA. options[0]\nB. options[1]\nC. options[2]\nD. options[3]\nE. options[4]\nONLY OUTPUT ONE LETTER.\nYour answer MUST be exactly one of: A, B, C, D, or E.\nDo NOT include explanation or text after the letter.\nAnswer:\nPrompt for ADRD Caregiving True/False QA\nYou are a highly knowledgeable AI assistant specializing in medicine. Answer A for yes, B for no. Do not include any explanation\nor additional text. Output only the letter on a single line.\nQuestion: {question}\nA. yes\nB. no\nONLY OUTPUT ONE LETTER.\nYour answer MUST be exactly one of: A or B.\nDo NOT include explanation or text after the letter.\nAnswer:\n13\n"}, {"page": 14, "text": "A.2\nZero-Shot Chain-of-Thought prompts for Case Analysis\nPrompt for ADRD Caregiving Multiple-Choice QA\nYou are a highly knowledgeable AI assistant specializing in medicine. Give a letter answer among A, B, C, D, or E.\nReason through the question step by step and explicitly write out your reasoning before giving the final answer.\nQuestion: {question}\nA. options[0]\nB. options[1]\nC. options[2]\nD. options[3]\nE. options[4]\nFirst, output the reasoning process.\nThen, on a new line, output the final answer as a single letter (A, B, C, D, or E).\nFormat:\nReasoning:\n<your reasoning>\nAnswer:\n<one letter>\nPrompt for ADRD Caregiving True/False QA\nYou are a highly knowledgeable AI assistant specializing in medicine. Answer A for yes, B for no.\nReason through the question step by step and explicitly write out your reasoning before giving the final answer.\nQuestion: {question}\nA. yes\nB. no\nFirst, output the reasoning process.\nThen, on a new line, output the final answer as a single letter (A or B).\nFormat:\nReasoning:\n<your reasoning>\nAnswer:\n<one letter>\n14\n"}, {"page": 15, "text": "A.3\nCompleted Reasoning Output of Case Analysis Examples\nReasoning Output for True/False Case\nReasoning by llama3:8b-instruct-fp16:\n{\nHere is the reasoning process:\nReasoning: The question states that the Alzheimerâ€™s patient is unable to perform basic activities of daily living. This implies that\nthe patient has difficulty with tasks such as bathing, dressing, eating, and using the bathroom, among others. These are essential\nskills for independent living, and if someone is unable to perform them, it suggests a significant impairment in their cognitive or\nphysical abilities.\nAnswer: Yes\n}\nReasoning by claude-opus-4-5-20251101:\n{\nReasoning:\nThe question asks whether Alzheimerâ€™s patients are unable to perform basic activities of daily living (ADLs).\nAlzheimerâ€™s disease is a progressive neurodegenerative disorder that affects memory, thinking, and behavior. The impact on daily\nliving activities depends on the stage of the disease:\n1. Early stage: Patients may have mild memory problems but can generally still perform most ADLs independently (bathing,\ndressing, eating, toileting, etc.)\n2. Middle stage: Patients begin to have more difficulty with ADLs and may need assistance with some tasks.\n3. Late/Advanced stage: Patients typically lose the ability to perform basic ADLs and require full-time care and assistance.\nThe question states \"The Alzheimerâ€™s patient\" in a general sense, implying the condition overall. In advanced stages of Alzheimerâ€™s\ndisease, patients do become unable to perform basic activities of daily living such as bathing, dressing, eating, and using the toilet\nindependently.\nSince the question appears to be asking about a characteristic feature of Alzheimerâ€™s disease (particularly in its progression), and\ninability to perform ADLs is indeed a hallmark of advanced Alzheimerâ€™s disease, the answer would be yes - Alzheimerâ€™s patients\n(especially in later stages) are unable to perform basic activities of daily living.\nAnswer: Yes\n}\n15\n"}, {"page": 16, "text": "Reasoning Output for Multiple-Choice Case\nReasoning by qwen3-max-2025-09-23:\n{\nReasoning:\nWhen caring for someone with dementia, maintaining effective communication is crucial. Individuals with dementia often\nexperience difficulties with attention, perception, and processing sensory information. Losing eye contact may indicate that they\nare distracted, overwhelmed, or simply have shifted their visual focus due to cognitive changesâ€”not necessarily disinterest or\ndefiance.\nOption A suggests moving closer and increasing body language, which could be perceived as intrusive or overwhelming, especially\nif the person is already feeling overstimulated.\nOption B involves following the patientâ€™s gaze and repositioning to stay in their visual field. This is a respectful, non-confrontational\nway to re-establish connection without demanding attention. It aligns with best practices in dementia care, which emphasize\nadapting to the personâ€™s current state rather than forcing engagement.\nOption C assumes the person needs space, but losing eye contact doesnâ€™t always mean they need a breakâ€”it might just reflect\ndifficulty maintaining focus. Pausing unnecessarily could disrupt meaningful interaction.\nOption D calling their name firmly and reminding them of the topicâ€”can come across as scolding or pressuring, which may\nincrease anxiety or confusion.\nOption E assumes the issue is auditory distraction, but the problem described is loss of eye contact (a visual cue), not lack of\nhearing or response to speech.\nTherefore, the most appropriate action is to gently reposition yourself within their visual field to support their ability to re-engage\nnaturally.\nAnswer: B\n}\nReasoning by claude-opus-4-5-20251101:\n{\nReasoning:\nWhen caring for someone with dementia, maintaining effective communication requires understanding their cognitive and\nperceptual challenges. Let me analyze each option:\nA. Moving closer and using increased body language - While body language is helpful, moving closer without repositioning to\ntheir visual field may not address the core issue of lost eye contact. This could also feel intrusive.\nB. Following the patientâ€™s gaze and repositioning yourself to remain in their visual field - This approach acknowledges that\npeople with dementia may have difficulty tracking movement or maintaining focus. By repositioning yourself to where theyâ€™re\nlooking, you work with their current attention rather than against it. This is a person-centered, non-confrontational approach.\nC. Recognizing they need space and pausing - While sometimes appropriate, automatically assuming they need space when eye\ncontact is lost may not be the best first response. The loss of eye contact could simply be due to visual field issues or distraction,\nnot necessarily a need for space.\nD. Calling their name firmly and reminding them of the topic - This approach can be perceived as confrontational or demanding,\nwhich can increase anxiety and agitation in people with dementia. Itâ€™s generally not recommended to use firm tones or put\npressure on them.\nE. Checking for environmental noise and increasing volume - While environmental factors matter, simply increasing volume\ndoesnâ€™t address the visual attention issue and could be startling or perceived as aggressive.\nThe best practice in dementia care is to adapt to the personâ€™s current state and work within their capabilities. Option B represents\na gentle, person-centered approach that maintains connection without being demanding or intrusive.\nAnswer: B\n}\n16\n"}]}