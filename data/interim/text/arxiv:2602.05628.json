{"doc_id": "arxiv:2602.05628", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.05628.pdf", "meta": {"doc_id": "arxiv:2602.05628", "source": "arxiv", "arxiv_id": "2602.05628", "title": "AI chatbots versus human healthcare professionals: a systematic review and meta-analysis of empathy in patient care", "authors": ["Alastair Howcroft", "Amber Bennett-Weston", "Ahmad Khan", "Joseff Griffiths", "Simon Gay", "Jeremy Howick"], "published": "2026-02-05T13:09:19Z", "updated": "2026-02-05T13:09:19Z", "summary": "Background: Empathy is widely recognized for improving patient outcomes, including reduced pain and anxiety and improved satisfaction, and its absence can cause harm. Meanwhile, use of artificial intelligence (AI)-based chatbots in healthcare is rapidly expanding, with one in five general practitioners using generative AI to assist with tasks such as writing letters. Some studies suggest AI chatbots can outperform human healthcare professionals (HCPs) in empathy, though findings are mixed and lack synthesis.   Sources of data: We searched multiple databases for studies comparing AI chatbots using large language models with human HCPs on empathy measures. We assessed risk of bias with ROBINS-I and synthesized findings using random-effects meta-analysis where feasible, whilst avoiding double counting.   Areas of agreement: We identified 15 studies (2023-2024). Thirteen studies reported statistically significantly higher empathy ratings for AI, with only two studies situated in dermatology favouring human responses. Of the 15 studies, 13 provided extractable data and were suitable for pooling. Meta-analysis of those 13 studies, all utilising ChatGPT-3.5/4, showed a standardized mean difference of 0.87 (95% CI, 0.54-1.20) favouring AI (P < .00001), roughly equivalent to a two-point increase on a 10-point scale.   Areas of controversy: Studies relied on text-based assessments that overlook non-verbal cues and evaluated empathy through proxy raters.   Growing points: Our findings indicate that, in text-only scenarios, AI chatbots are frequently perceived as more empathic than human HCPs.   Areas timely for developing research: Future research should validate these findings with direct patient evaluations and assess whether emerging voice-enabled AI systems can deliver similar empathic advantages.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.05628v1", "url_pdf": "https://arxiv.org/pdf/2602.05628.pdf", "meta_path": "data/raw/arxiv/meta/2602.05628.json", "sha256": "c50b0da1c965cdf192945c797cb7d38968a9bfc5b232d7b85a5d8d7faf0e1f9b", "status": "ok", "fetched_at": "2026-02-18T02:19:43.064549+00:00"}, "pages": [{"page": 1, "text": "Received 28 June 2025; Revised 11 August 2025; Accepted 20 August 2025\n© The Author(s) 2025. Published by Oxford University Press.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/\nlicenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly\ncited.\nBritish Medical Bulletin, 2025, 156, 1–13\nhttps://doi.org/10.1093/bmb/ldaf017\nInvited Review\nAI chatbots versus human healthcare professionals:\na systematic review and meta-analysis of empathy\nin patient care\nAlastair Howcroft1,2,*\n, Amber Bennett-Weston2\n, Ahmad Khan3\n, Joseff Grifﬁths4,\nSimon Gay3\n, Jeremy Howick2\n1School of Computer Science, University of Nottingham, Jubilee Campus, Wollaton Road, Nottingham, Nottinghamshire NG8\n1BB, United Kingdom\n2Stoneygate Centre for Empathic Healthcare, Leicester Medical School, University of Leicester, George Davies Centre,\nLancaster Road, Leicester, Leicestershire LE1 7HA, United Kingdom\n3Leicester Medical School, University of Leicester, George Davies Centre, Lancaster Road, Leicester, Leicestershire LE1 7HA,\nUnited Kingdom\n4School of Medicine, University of Nottingham, Queen’s Medical Centre, Derby Road, Nottingham, Nottinghamshire NG7 2UH,\nUnited Kingdom\n*Corresponding author. School of Computer Science, University of Nottingham, Jubilee Campus, Wollaton Road, Nottingham NG8 1BB, UK.\nE-mail: psyah9@nottingham.ac.uk\nAbstract\nBackground: Empathy is widely recognized for improving patient outcomes, including reduced pain and anxiety and improved\nsatisfaction, and its absence can cause harm. Meanwhile, use of artiﬁcial intelligence (AI)–based chatbots in healthcare is rapidly\nexpanding, with one in ﬁve general practitioners using generative AI to assist with tasks such as writing letters. Some studies\nsuggest AI chatbots can outperform human healthcare professionals (HCPs) in empathy, though ﬁndings are mixed and lack\nsynthesis.\nSources of data: We searched multiple databases for studies comparing AI chatbots using large language models with human\nHCPs on empathy measures. We assessed risk of bias with ROBINS-I and synthesized ﬁndings using random-effects meta-\nanalysis where feasible, whilst avoiding double counting.\nAreas of agreement: We identiﬁed 15 studies (2023–2024). Thirteen studies reported statistically signiﬁcantly higher empathy\nratings for AI, with only two studies situated in dermatology favouring human responses. Of the 15 studies, 13 provided extractable\ndata and were suitable for pooling. Meta-analysis of those 13 studies, all utilising ChatGPT-3.5/4, showed a standardized mean\ndifference of 0.87 (95% CI, 0.54–1.20) favouring AI (P < .00001), roughly equivalent to a two-point increase on a 10-point scale.\nAreas of controversy: Studies relied on text-based assessments that overlook non-verbal cues and evaluated empathy through\nproxy raters.\nGrowing points: Our ﬁndings indicate that, in text-only scenarios, AI chatbots are frequently perceived as more empathic than\nhuman HCPs.\nAreas timely for developing research: Future research should validate these ﬁndings with direct patient evaluations and assess\nwhether emerging voice-enabled AI systems can deliver similar empathic advantages.\nKeywords: empathy; artiﬁcial intelligence; digital health; patient-centred care; physician-patient relations; systematic review; meta-analysis\nIntroduction\nEmpathic healthcare is well recognized for its positive\nimpact on patient quality of life, satisfaction with\ncare, and reduction of pain and psychological distress\n[1]. With recent advances in artificial intelligence (AI)\ntechnologies, chatbots are increasingly being integrated\ninto patient care, even replacing human practitioner\nroles at times. For example, Wysa—a digital therapist—\nhas been used by over 117 000 patients across 31 NHS\nTalking Therapy services, according to Wysa’s official\nwebsite [2]. These AI systems can interact with patients\nthrough text or speech, providing information, moni-\ntoring symptoms, offering support, and fulfilling other\nroles\nhistorically\nprovided\nby\nhuman\nhealthcare\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 2, "text": "2\nHowcroft et al.\nprofessionals (HCPs) [3]. It has also been reported\nthat 20% of UK general practitioners (GPs) now use\ngenerative AI like ChatGPT for tasks such as assistance\nwith writing patient correspondence [4]. However,\ndespite the growing use of these technologies [5], there\nare concerns about whether AI chatbots can be as\nempathic as human HCPs [6], and thus leverage the\nbenefits of empathic care for patients [7]. Such doubts\nalign with the 2019 Topol Review, a UK government-\ncommissioned roadmap for NHS technology, which\nconcluded that ‘empathy and compassion’ remain\n‘essential human skill [s] that AI cannot replicate’ [8].\nWhilst individual studies have explored the potential\nof AI technologies to display empathic behaviours\n[9], findings are heterogeneous. Some report that\nAI bots can produce responses with higher levels\nof perceived empathy [10], whilst others cite issues\nsuggesting the contrary, maintaining that they lack the\nwarmth, nuanced understanding, and ability to form\ndeep emotional bonds inherent in human interactions\n[11]. Existing literature reviews on the use of AI bots\nhave primarily focused on disease diagnosis, clinical\nefficiency, ethical considerations, and their integration\ninto healthcare systems [5, 12–14] and factual accuracy\nof medical advice compared to humans [15].\nThe lack of a synthesis of studies comparing AI tech-\nnology with human empathy represents a significant\ngap in the literature. Such a synthesis can pave the way\nfor more informed decisions about the integration of\nAI technologies into patient care, ensuring that such\ntechnological advancements contribute to (and don’t\ndetract from) the positive patient outcomes histori-\ncally associated with empathic healthcare delivered by\nhumans.\nMethods\nThe review is reported according to the PRISMA 2020\nChecklist.\nEligibility criteria\nStudies were eligible if they empirically compared\nempathy between AI chatbots and human HCPs.\nEligible study designs included quantitative or quali-\ntative studies involving real patients, healthcare users,\nor authentic patient-generated data, such as emails,\nportal messages, or public forum posts. We included\nparticipants of any age, gender, or demographic engag-\ning in formal or informal healthcare interactions. AI\ninterventions were restricted to conversational agents\nusing Large Language Models (LLMs), such as GPT-\n3/4, Claude, or Gemini, capable of unscripted dialogue.\nExclusions\nincluded\nhealthy\nvolunteers\nwithout\nhealthcare needs and interactions outside healthcare\ncontexts (e.g. education, customer service). Hypo-\nthetical patient scenarios (e.g. researcher-generated\nquestions without a real patient source) [16] were\nexcluded because they are further removed from real-\nworld practice and may not capture the authentic\nphrasing or context of genuine healthcare interactions.\nOur aim was to include only interactions that, while\nvaried in setting, still originated from actual patient-\ngenerated content. We also excluded studies utilising\nrule-based or scripted AI systems, opinion pieces,\ntheoretical discussions, editorials, commentaries, and\nreviews lacking original empirical data.\nInformation sources\nWe searched PubMed, Cochrane Library, Embase,\nPsycINFO, CINAHL, Scopus, and IEEE Xplore from\ninception to 11 November 2024. ClinicalTrials.gov,\nICTRP, and ISRCTN were searched for completed\nstudies with published results. Reference lists of\nincluded studies and relevant reviews were screened,\nand grey literature was searched via Google Scholar on\n12 November 2024.\nSearch strategy\nThe search strategy focused on three key groups\nof\nterms:\nempathy,\nAI\ntechnologies,\nand\nHCPs.\nRelevant keywords and index terms were identified\nthrough preliminary searches in IEEE Xplore and\nPubMed, supported by consultation with an academic\nlibrarian. Broad terms for AI and HCPs captured\ndiverse technologies and roles, whereas empathy terms\n(‘empathy’, ‘empathic’, ‘empathetic’, ‘compassion’, and\nderivatives) were narrower to maintain specificity.\nGroups were combined with ‘OR’ within categories\nand ‘AND’ across categories. The full strategy is\ndetailed in Appendix A.\nSelection process\nThe primary reviewer independently screened all titles\nand abstracts to ascertain which studies to include\nor exclude based on the predefined eligibility criteria.\nThe abstracts were divided between two secondary\nreviewers, who independently screened their assigned\nportions. Discrepancies between the primary reviewer\nand the specific secondary reviewer who assessed the\nabstract were resolved through discussion. Similarly,\nthis process was repeated for the full-text screening\nstage.\nData collection process\nAll identified records were imported into EndNote\n[17] for organisation and removal of duplicates. Study\nselection was managed using Rayyan [18]. The primary\nreviewer developed and independently completed the\ndata extraction form, with two secondary reviewers\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 3, "text": "Empathy in AI chatbots vs human clinicians\n3\nindependently extracting data on assigned portions,\nguided initially by an example form to ensure consis-\ntency.\nData items\nWe collected data on study design, participants, set-\ntings, AI interventions, human comparators, empathy\nmeasures, and key findings related to empathy.\nSynthesis methods\nDespite heterogeneity, particularly in terms of empa-\nthy measurement instruments (ranging from custom\nLikert-type scales to qualitative coding) and empa-\nthy evaluators (patient proxies, medical students, etc.),\nstudies were sufficiently similar for meta-analysis due\nto shared structural elements. All studies directly com-\npared AI-generated responses to human HCPs using\nblinded evaluations (except one not stating blinding),\nand all but that same study quantitatively provided\nempathy results (mostly via single-item Likert scales),\nenabling relative effect size calculations. All compar-\nisons were in text form. In one case, the text was\nconverted to audio for patients, but empathy raters\nassessed only the written transcript. We also narratively\nsynthesized the results (Appendix B). We organized\nstudies by the large language model (GPT-3.5, GPT-\n4, other models) and then compared outcomes against\nits HCP comparators (e.g. physicians, nurses). Where\navailable, we reported empathy scores, mean differ-\nences, p-values (for statistical significance), and other\nrelevant summary statistics.\nReporting bias assessment\nTwo reviewers independently assessed the risk of bias\n(ROB). The ROBINS-I tool [19] was used to assess the\nRoB across the 15 studies, given that 14 were non-\nrandomized cross-sectional comparative analyses and\none was a randomized controlled trial (RCT) [20].\nDiscrepancies were resolved through discussion, and\nwith a third reviewer where necessary.\nMeta-analysis\nWe conducted a single meta-analysis with two sub-\ngroups (GPT-3.5 and GPT-4) due to their prevalence\namongst the included studies (five and 10 appearances,\nrespectively; other models appeared once). To prevent\ndouble-counting, we excluded additional AI arms from\nmulti-model studies and entirely omitted studies eval-\nuating both GPT versions on the same dataset. We\nextracted SMDs with 95% confidence intervals (CIs)\nand constructed random-effects models to pool effect\nsizes. Subgroup differences were examined by compar-\ning pooled effect estimates, with P < .05 considered\nstatistically significant. Analyses and plots used Review\nManager (RevMan) 5.4 [21].\nResults\nStudy selection\nThe search identified 987 unique results that were\nscreened by title/abstract and by full text. Following\ntitle and abstract screening, 34 articles were included.\nDuring full-text screening, an additional 19 studies\nwere excluded (Appendix C). Ultimately, 15 studies met\nthe inclusion criteria for this systematic review (see\nFig. 1).\nRisk of bias\nOverall, nine studies had a moderate ROB, and six\nhad a serious ROB (Appendix D). Seven used curated\npatient queries, potentially introducing selection bias.\nFour relied on Reddit communities [10, 22–24]—\nonline forums where people publicly post health\nquestions and receive free answers from strangers.\nThese often attract users in ‘desperate’ circumstances\n[25], facing barriers to timely formal care, raising\nconcerns about representativeness, thoroughness, and\nconfounding. Other serious cases employed supervised\ndesigns (i.e. where a human expert reviewed AI outputs\nand blocked unsafe replies) [20, 26], complicating\nisolation of chatbot performance. Heterogeneity arose\nfrom diverse empathy raters (e.g. patient proxies, clini-\ncians, psychology trainees), complicating comparisons\nbecause perceptions of empathy may vary by evaluator\nbackground. Finally, 14 of 15 studies assessed empathy\nusing non-validated methods (e.g. single-item custom\nLikert scales), rather than a validated instrument (e.g.\nConsultation and Relational Empathy [CARE]) [27],\nlimiting standardisation and heightening subjectivity.\nCharacteristics of included studies\nFourteen of the 15 studies included in the review were\npublished in 2024; the remaining study was published\nin 2023 [10].\nTypes of health concerns and specialty\nThe studies covered diverse health conditions and\nspecialties (see Table 1). Four involved non-specific\nmedical concerns, reflecting routine and general patient\ninquiries. These included outpatient queries across\nmultiple departments [20], general health questions\nfrom social media [10, 28], and internal medicine\npatient portal interactions, including lab results and\nadministrative requests [29]. Other studies addressed\nspecialized clinical conditions, such as dermatology\n[26, 30], oncology [22], and thyroid conditions [31].\nChronic diseases were also covered, including systemic\nlupus erythematosus [32] and multiple sclerosis [27],\nalongside reconstructive surgery [33] and complete\nblood\ncount\nlab\nresult\ninterpretation\n[23]. Two\naddressed mental and neurodevelopmental health,\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 4, "text": "4\nHowcroft et al.\nFigure 1. PRISMA ﬂow diagram.\none examining mental health support inquiries [24]\nand the other focusing on autism-related queries [34].\nFinally, one service-oriented study examined patient\ncomplaints from various departments [35].\nHuman HCP comparators\nThe included studies compared AI-generated responses\nto a variety of human comparators. Two studies com-\npared AI responses to those from surgeons [31, 33],\nwhile six studies involved comparisons with physicians\nspecialising in specific fields [10, 22, 26, 27, 30, 34].\nThree studies compared AI responses to physicians\nwith no specified specialties [10, 23, 28]. Other com-\nparisons included advanced practice providers in one\nstudy [33], nurses and frontline staff in one study\n[29], and reception nurses in another [20]. Mental\nhealth licensed professionals were the comparators in\none study [24], whilst patient relations officers were\nincluded in another [35].\nInteraction modalities and AI models\nAll but one of the included studies relied exclusively\non text-based interaction with the AI system. In one\nstudy, patient speech was transcribed (through a real-\ntime voice transcribing software) into text for the\nLLM (GPT-3.5) and then the AI’s text response was\nconverted back into audio [20], but the written\ntranscript\nwas\nultimately\nused\nfor\nthe\nempathy\nevaluation. One study allowed patients to upload\nimages, however they were described textually by a\nhuman intermediary before input [26]. All but one\nstudy [26] used versions of GPT (general-purpose\nlanguage models). Three studies also evaluated GPT\nalongside other LLMs: one with ERNIE Bot [34], one\nwith Claude [22], and one with Gemini Pro and Le\nChat [23]. The exception was a study exclusively using\nMed-PaLM2, a model specifically designed for medical\nquestion-answering tasks [26].\nEmpathy measurement tools\nAll, bar one study, relied on unvalidated or custom tools\nfor measuring empathy; the exception used the CARE\nscale [27], which is a validated instrument. Eight studies\nrelied on single-item 1–5 Likert scales, where responses\nwere rated from ‘not empathetic’ to ‘very empathetic’\nor similar descriptors [10, 20, 24, 28, 30, 31, 33, 34].\nOne additional study used a 1–5 Likert scale, however,\nassessed separate scores for cognitive and emotional\nempathy [22]. Two studies used single-item 0/1–10\nscales [32, 35] and another employed a single-item 1–\n6 scale [23]. Another study used a thematic coding\nframework, identifying and counting empathy-related\nstatements—appreciation, acknowledgment, and com-\npassion—as part of content analysis [26]. Additionally,\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 5, "text": "Empathy in AI chatbots vs human clinicians\n5\none study measured empathy conditionally: reviewers\nfirst determined if responses (AI or human generated)\nwere usable, then selected empathy as a reason for\nusefulness if applicable, rather than using any scale\n[29].\nQuery sources\nSeven studies utilized historical emails or message logs\nfrom private medical records [13, 27, 29–33, 35],\nincluding the smallest sample, four commonly asked\npatient emails [27]; six drew on publicly available\nquestions from Reddit and other online forums [10,\n22–24, 28, 34]; one collected real-time chat transcripts\n[26]; and one analysed in-person reception interactions,\nwhich yielded the largest sample, 2164 live outpatient\nqueries [20]. Among the 11 studies that reported a\nspecific location, most patient queries originated from\nWestern countries. Four were from the USA [26, 29,\n30, 33], one from Germany [28], one from Italy, one\nfrom Singapore [35], four from China [20, 31, 32, 34],\nand four from the global platform Reddit [10, 22–24],\nwhose user base is predominantly from the USA and\nUK [36].\nHow empathy was assessed\nAcross these studies, all evaluators functioned as\n‘observers’ (not those asking/answering the inquiries)\nbut varied in their backgrounds. One study used\npatient-proxies [27]; five employed HCPs [10, 20, 22,\n29, 31]; three combined patient-proxies and HCPs\n[28, 31, 32]; three involved laypeople alongside HCPs\n[20, 30, 35]; one used medical students [33]; and one\nfeatured a psychology undergraduate and psychologist\nintern [24]. All reviewers in these studies were blinded\nto whether the responses were generated by AI or\nhumans. The additional study relied on researchers\nor coders with mixed expertise in healthcare, con-\nversational AI, and human-computer interaction, and\nblinding was not mentioned [26].\nResults of included studies\nTable 1 summarizes the characteristics of the 15\nincluded studies, with statistical results reported as\npresented in the studies (sub-scores, overall means,\netc.). Detailed study-specific results, including granular\nmetrics and model comparisons, are provided in\nAppendix E.\nResults of syntheses\nOverall summary of included comparisons\nIn 13 out of the 15 comparisons, one or more\nAI chatbots demonstrated a statistically significant\nadvantage in projection of empathy over human\nhealthcare practitioners [10, 20, 22–24, 27–29, 31–\n35] (see Fig. 2). In one of these 13 comparisons, one AI\nmodel (ERNIE Bot) of the two assessed did not show\na statistically significant difference from humans, but\nthe other model (GPT-4) within the same study did\noutperform ERNIE Bot, demonstrating a statistically\nsignificant result [34]. In the remaining two studies,\nboth involving dermatology, human dermatologists\noutperformed AI (Med-PaLM 2 and ChatGPT-3.5) in\nperceived empathy [26, 30].\nWe first present the results of our meta-analyses\nfor GPT-3.5 and GPT-4. Afterwards, we briefly dis-\ncuss the two studies that could not be included in\nthe meta-analysis (n = 1 to prevent double-counting\n[30], n = 1 due to missing outcome data [26]) and\nother LLM arms (GPT-3, Gemini, Le Chat, and ERNIE\nBot) beyond GPT-3.5/4. These additional models were\nexcluded from the meta-analysis to avoid overlapping\ndata. A full narrative synthesis of all models and studies\nare included in Appendix B.\nMeta-analyses of GPT-3.5 and GPT-4 chatbots\nThirteen studies provided data suitable for meta-\nanalysis. Overall, ChatGPT demonstrated significantly\nhigher empathy than human practitioners (standard-\nized mean difference [SMD] 0.87, 95% CI 0.54–1.20;\nP < .00001). Heterogeneity was moderate between\nGPT-3.5 and GPT-4 subgroups (I2 = 49.4%) but high\nacross all studies (I2 = 97%). In Meyer et al. [23],\nthe same 100 questions were tested with ChatGPT,\nGemini, and Le Chat, risking double counting if\npooled. Thus, it could not be pooled. As highlighted\nby Hussein, et al. [37], analyses of health record\ndata are particularly prone to such double-counting\nerrors. Similarly, Soroudi, et al. [33] compared GPT-\n3, GPT-4, and human HCPs; we retained only GPT-4\ndata to prevent double-counting. He, et al. [34] also\ntested ERNIE Bot and GPT-4 on the same dataset,\nso we included only GPT-4. Finally, Chen, et al. [22]\nsimultaneously assessed GPT-3.5 and GPT-4 (and\nClaude) on the same dataset, creating an irresolvable\nconflict between our GPT-3.5 and GPT-4 subgroups;\ntherefore, that study was omitted entirely from the\npooled analysis. Li, et al. [26] evaluated Med-PaLM\n2, but didn’t provide specific metrics, so was similarly\nexcluded from the following meta-analysis.\nSubgroup: GPT-3.5\nFour studies provided data compatible with a GPT-\n3.5 subgroup meta-analysis (excluding Chen et al.).\nThey reported empathy outcomes on validated (CARE)\nor custom Likert scales allowing calculation of stan-\ndardized mean differences (SMDs). Pooled analysis\n(random-effects model) yielded an SMD of 0.51 (95%\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 6, "text": "6\nHowcroft et al.\nTable 1. Empathy comparisons between AI chatbots and human healthcare practitioners—summary of ﬁndings across studies.\nStudy\nAI Model(s) HCP Com-\nparator(s)\nInstrument/Scale or\nApproach (range of\npossible scores)\nEvaluator\nType\nReported Overall\nDifference (95%\nCI), p-value\nInterpretation\nArmbruster\n(2024)\nChatGPT-4\nPhysicians\n(mixed\nspecialties)\nCustom Likert (1–5;\n1 = very poor, 5 = very\ngood).\nItem: ‘How empathetic\nor friendly is the\nresponse?’\nPatient-\nproxies and\nHCP\nobservers\nP < .001\nChatGPT-4 statistically\nsignificantly outperformed\nthe Expert Panel in empathy,\nas rated by both patients\nand specialists.\nAyers\n(2023)\nChatGPT-\n3.5\nPhysicians\n(mixed\nspecialties)\nCustom Likert (1–5;\n1 = not empathetic,\n5 = very empathetic).\nItem: ‘Evaluate the\nempathy or bedside\nmanner provided.’\nHCP\nobservers\nP < .001\nChatGPT-3.5 responses\nwere statistically\nsignificantly more empathic\nthan physicians’, with\n45.1% of chatbot responses\nrated as empathic compared\nto only 4.6% for physicians.\nChen\n(2024)\nGPT-3.5,\nGPT-4,\nClaude\nOncologists\nCustom Likert (1–5;\n1 = very poor, 5 = very\ngood) assessing two\ndimensions—Cognitive\nand Emotional Empathy.\nItem: ‘Evaluate\nemotional and cognitive\nempathy.’\nHCP\nobservers\nP < .001 (any\nchatbot vs.\nphysicians)\nAll three chatbots\noutperformed physicians\n(statistically significant).\nClaude had the highest\nempathy rating, followed by\nGPT-4, GPT-3.5, and then\nphysicians.\nGuo (2024)\nChatGPT-4\nJunior &\nSenior\nSurgeons\nCustom Likert (1–5;\n1 = very unsympathetic,\n5 = very sympathetic).\nItem: ‘Evaluate the\ncompassion of the\nresponse.’\nPatient-\nproxies and\nHCP\nobservers\nPatient-reviewed\ncomparison:\nP < .001;\nSurgeon-reviewed\ncomparison:\nP = .007\nChatGPT-4’s responses were\nrated statistically\nsignificantly more empathic\nthan responses from both\njunior and senior specialists.\nThe difference was larger (in\nstatistical terms) for patient\nreviewers than for surgeon\nreviewers.\nHe (2024)\nChatGPT-4,\nERNIE Bot\nPhysicians\n(mixed\nspecialties)\nCustom Likert (1–5;\n1 = lacking, 5 = very\nhumane).\nItem: ‘Evaluate empathy\nin terms of respect,\ncommunication,\ncompassion, and\nemotional connection.’\nHCP\nobservers\nPhysicians vs.\nChatGPT:\nP < .001;\nPhysicians vs.\nERNIE Bot:\nP = .14;\nChatGPT vs.\nERNIE Bot:\nP < .001\nChatGPT-4 scored\nsignificantly better than\nphysicians. ERNIE Bot\nscored slightly lower than\nphysicians, but not\nsignificantly so.\nLi (2024)\nMed-PaLM\n2\nDermatolo-\ngists\nFrequency count of\nempathy markers\n(‘appreciation’,\n‘acknowledgment’,\n‘compassion’) via\nqualitative coding of\ntranscripts.\nCoders\n(observers)\nwith mixed\nexpertise in\nartificial\nintelligence/\nhealthcare/\nhuman-\ncomputer\ninteraction\nP < .05 (between\ngroups across\nappreciation,\nacknowledgment,\ncompassion\nmarkers)\nHuman clinicians used more\ntotal empathy-related\nlanguage (across\n‘appreciation’,\n‘acknowledgment’,\n‘compassion’ markers)—this\ndifference was statistically\nsignificant—but AI used\nmore ‘compassion’ markers.\nMaida\n(2024)\nChatGPT-\n3.5\nNeurolo-\ngists\nCARE scale (overall\n10–50; 10 items rated on\na 5-point Likert\n[1 = poor, 5 = excellent]).\nMeasures therapeutic\nempathy during\none-on-one\nconsultations.\nPatient\nproxies\n1.38 (0.65–2.11);\nP < .01\nChatGPT-3.5 was rated\nhigher in empathy than\nneurologists, with a\nstatistically significant\noverall difference.\n(continued)\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 7, "text": "Empathy in AI chatbots vs human clinicians\n7\nTable 1. Continued.\nStudy\nAI Model(s) HCP Com-\nparator(s)\nInstrument/Scale or\nApproach (range of\npossible scores)\nEvaluator\nType\nReported Overall\nDifference (95% CI),\np-value\nInterpretation\nMeyer\n(2024)\nChatGPT-4,\nGemini Pro,\nLe Chat\nPhysicians\n(mixed\nspecialties)\nCustom Likert (1–6;\n1 = excellent,\n6 = inadequate).\n‘Level of empathy’ with\nresponse distributions\nprovided as percentages\nacross categories.\nHCP\nobservers\nChatGPT vs. Physicians:\nP < .001;\nGemini vs. Physicians:\nP < .001;\nLe Chat vs. Physicians:\nP < .001;\nChatGPT vs. Gemini:\nP = .50;\nChatGPT vs. Le Chat:\nP < .001;\nGemini vs. Le Chat:\nP < .001\nAll three chatbots were\nrated as more empathic than\nphysicians. Among chatbots,\nChatGPT and Gemini did\nnot differ significantly; both\nwere rated statistically\nsignificantly higher than Le\nChat.\nReynolds\n(2024)\nChatGPT-\n3.5\nDermatolo-\ngists\nCustom Likert (1–5;\n1 = very poor, 5 = very\ngood).\nItem: ‘Evaluate the level\nof empathy\ndemonstrated.’\nLaypeople\nobservers\nand HCP\nobservers\nPhysician raters:\nP = .001;\nNon-Physician raters:\nP = .09\nPhysicians were rated as\nmore empathic by physician\nreviewers (significant).\nNon-physicians also tended\nto rate physicians higher, but\nthat was not statistically\nsignificant.\nSmall\n(2024)\nChatGPT-4\nVariety of\nHCPs\n(physicians,\nnurses, and\nfrontline\nstaff)\nCount of empathic\nresponses (i.e. responses\nclassified as ‘empathic’)\nplus linguistic analysis\n(measures of subjectivity\nand positive polarity).\nItem: ‘Would you find\nthis draft useful?\n(Empathy options)’\nHCP\nobservers\n(No single ‘overall\ndifference’ beyond the\nabove proportions.)\nChatGPT-4 generated a\nhigher proportion of\nempathic responses than\nhealthcare professionals; its\nresponses featured\nstatistically significantly\ngreater subjectivity and\npositive language.\nSoroudi\n(2024)\nChatGPT-\n3/4 (Full vs.\nBrief)\nPlastic\nSurgeons &\nAdvanced-\nPractice\nProviders\nCustom Likert (1–5;\n1 = not empathetic,\n5 = very empathetic).\nItem: Rate empathy for\nboth full and brief\nresponse formats.\nMedical\nStudents\n(observers)\nCombined Chatbot vs.\nProviders: P < .001;\nBrief Chatbot vs.\nProviders: P = .125\nChatGPT-generated\nresponses (any version) were\nperceived as statistically\nsignificantly more empathic\nthan plastic surgeon/APP\nresponses overall. The brief\nChatGPT responses alone\ndid not differ significantly\nfrom providers’ scores, but\nfull ChatGPT responses did.\nWan\n(2024)\n‘SSPEC’\n(GPT-3.5\nwith\nsupervision)\nReception\nnurse-only\nCustom Likert (1–5;\n1 = detached tone,\n5 = strongly connects).\nItem: ‘Empathy—\nconsideration for the\npatient’s perspective.’\nLaypeople\nobservers\nand HCP\nobservers\nP < .001 (Internal\nValidation)\nP < .001 (RCT)\nThe SPPEC scored\nstatistically significantly\nhigher in empathy than\nnurses in both internal\nvalidation (retrospective)\nand RCT (prospective).\nXu\n(2024)\nChatGPT-4\nRheumatol-\nogists\nCustom Likert (0–10);\n0 = no empathy and\n10 = highly empathetic.\nItem: ‘Evaluate how\nwell the answer conveys\nunderstanding and care\nfor patient concerns.’\nPatient-\nproxies and\nHCP\nobservers\nOverall\n(Chinese + English):\nRheumatologists’\ndifference: 0.46\n(0.23–0.69); P < .01\nChinese only:\nRheumatol. Eval:\n0.48; P < .001\nSLE Patients Eval:\n0.18; P = .249\nEnglish only:\nRheumatol. Eval:\n0.13; P = .80\nSLE Patients Eval:\n1.32; P < .001\nChatGPT-4 outperformed\nrheumatologists overall with\na statistically significant\ndifference. For Chinese\nevaluations, both\nrheumatologists and\npatients rated ChatGPT-4\nhigher (with statistical\nsignificance for\nrheumatologists only), while\nin English, only patient\nratings were statistically\nsignificantly higher.\n(continued)\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 8, "text": "8\nHowcroft et al.\nTable 1. Continued.\nStudy\nAI Model(s) HCP Com-\nparator(s)\nInstrument/Scale or\nApproach (range of\npossible scores)\nEvaluator\nType\nReported Overall\nDifference (95% CI),\np-value\nInterpretation\nYonatan-\nLeus\n(2024)\nChatGPT-4 Mental\nHealth\nLicensed\nProfession-\nals\nCustom Likert (1–5;\n1 = no empathic\nconcern, 5 = strong\nempathic concern).\nItem: ‘Evaluate\nempathic concern.’\nPsychology\nundergraduate\nand\npsychologist\nintern (both\nobservers)\n0.60 (large effect);\nP < .001\nChatGPT-4 was perceived as\nstatistically significantly\nmore empathic than human\nmental health professionals,\nconsistently showing greater\nwarmth—a statistically\nsignificant difference with a\nlarge effect size.\nYong\n(2024)\nChatGPT-4 Patient\nRelations\nOfficers\nCustom Likert (1–10;\n1 = not empathetic,\n10 = very empathetic;\nno midpoint provided).\nItem: ‘Empathy.’\nLaypeople\nobservers and\nHCP\nobservers\nP < .001\nChatGPT-4 was rated higher\nthan humans in empathy.\nThis difference was\nstatistically significant.\nCI -0.14–1.16) favouring GPT-3.5 over human health-\ncare practitioners (I2 = 99%), indicating high hetero-\ngeneity. This result was not statistically significant,\nhowever (P = .12), due to the one instance where HCPs\nscored higher in Reynolds, et al. [30].\nAcross four studies, GPT-3.5 outperformed human\nclinicians in three settings. Maida, et al. [27] reported\nan SMD of 0.15 (95% CI: 0.07–0.23) for neurologic\ninquiries, Wan, et al. [20] found an SMD of 0.79 (95%\nCI: 0.70–0.87) in an outpatient reception setting, and\nAyers, et al. [10] observed an SMD of 1.91 (95% CI:\n1.67–2.15) based on social media queries. In contrast,\nReynolds, et al. [30] showed that for dermatology\nqueries, human responses were rated higher (SMD\n−0.99, 95% CI: −1.52 to −0.46). We narratively syn-\nthesize these results in Appendix B.\nSubgroup: GPT-4\nNine studies contributed data for the GPT-4 subgroup,\nyielding a pooled SMD of 1.03 (95% CI 0.71–1.35)\nin favour of GPT-4 (I2 = 87%, Tau [2] = 0.19), again\nreflecting high heterogeneity. This result was statisti-\ncally significant (P < .00001).\nAcross nine studies, GPT-4 consistently outper-\nformed human clinicians. Guo, et al. [31] reported\nan SMD of 1.42 (95% CI: 0.85–1.99) for thyroid-\nrelated inquiries; Yonatan-Leus and Brukner [24]\nfound an SMD of 0.97 (95% CI: 0.73–1.21) for mental\nhealth queries on social media; and Soroudi, et al. [33]\nobserved an SMD of 0.83 (95% CI: −0.09–1.75) for\nbreast reconstruction questions. Armbruster, et al. [28]\nshowed an SMD of 1.44 (95% CI: 1.13–1.76) in a web-\nbased setting, while He, et al. [34] reported an SMD of\n0.80 (95% CI: 0.62–0.99) for autism-related inquiries.\nXu, et al. [32] found an SMD of 0.23 (95% CI: −0.06–\n0.51) for systemic lupus erythematosus questions, and\nYong, et al. [35] reported an SMD of 2.08 (95% CI:\n1.27–2.88) for patient complaints. Additionally, Small,\net al. [29] observed an SMD of 0.48 (95% CI: 0.16–\n0.80) for outpatient internal medicine queries, and\nMeyer, et al. [23] reported an SMD of 1.44 (95% CI:\n1.12–1.75) for laboratory-interpretation queries. We\nnarratively synthesize these results in Appendix B.\nSubgroup comparison: GPT-3.5 vs. GPT-4\nWhen examining GPT-3.5 and GPT-4 side by side, GPT-\n4 consistently outperformed human clinicians in empa-\nthy, whilst GPT-3.5 showed mixed results and failed\nto demonstrate a statistically significant advantage.\nA subgroup analysis testing for differences between\nGPT-3.5 and GPT-4 revealed no statistically significant\ndifference (P = .16). Thus, while GPT-4 achieved more\nconsistent results, the current data do not conclusively\nindicate that it is definitively more empathic than GPT-\n3.5 (Fig. 2). The pooled SMD of both models (n = 13\npooled studies) was 0.87 (95% CI: 0.54–1.20), with\nboth GPT-3.5 and GPT-4 collectively demonstrating\nstatistically significantly (P < .00001) higher empathy\nratings than HCPs.\nNon-meta-analysed results\nAcross the non-meta-analysed studies, Chen, et al.\n[22] found that GPT-3.5, GPT-4, and Claude all\noutperformed oncologists on oncology queries—with\nGPT-4 scoring highest. Li, et al. [26] showed that,\nwhile human clinicians used more empathy-related\nlanguage overall, Med-PaLM 2 demonstrated a slight\nadvantage in expressing compassion. Meyer, et al.\n[23] reported that both Gemini Pro and Le Chat\n(Mistral Large) were rated significantly higher than\nphysicians, whereas He, et al. [34] found that ERNIE\nBot slightly underperformed compared to clinicians.\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 9, "text": "Empathy in AI chatbots vs human clinicians\n9\nFigure 2. Forest plot comparing empathy ratings: GPT-3.5 vs. human practitioners and GPT-4 vs. human practitioners (excluding overlap).\nFinally, Soroudi, et al. [33] observed that GPT-3’s\n‘full’ format outperformed its ‘brief’ format, with both\nformats exceeding providers’ ratings. For full details,\nplease refer to Appendix B.\nDiscussion\nInterpretation of results\nThis is the first systematic review, we are aware of,\nthat compares the empathy of AI chatbots with human\npractitioners. Our meta-analysis of 13 studies shows\nthat ChatGPT has a 73% likelihood of being perceived\nas more empathic than a human practitioner in a\nhead-to-head matchup, using text-based interactions\n(representing the probability of superiority). This\nfinding was across multiple clinical specialties and\nvarious evaluative methods. This would be roughly\nequivalent to a difference of 2 points on a 10-point\nscale. An important implication is that text-based AI-\ndriven interactions are unlikely to cause harm through\ndeficits in empathy, aligning with broader evidence\nsupporting\nAI’s\npotential\nto\nenhance\nhealthcare\nengagement, quality, and efficiency [3].\nLimitations\nThis study has several limitations. Firstly, all studies\nanalysed text-based interactions. This is a problem\nbecause empathy in healthcare consultations often\nrelies on both verbal and non-verbal cues (e.g. nodding\nand leaning forward) [38], and because text-based\ncommunication represents a relatively small portion of\nhealthcare interactions [39]. That being said, healthcare\npractitioners are increasingly using purely text-based\ncommunication, suggesting that, at least in these cases,\nour results are relevant to actual practice [40]. Another\nlimitation is that the included studies evaluated empa-\nthy from proxy measures rather than the perspectives\nof the patients directly receiving care. Given that HCP\nand direct care recipients’ empathy ratings have been\nshown to differ [41, 42], it is possible that patient\nratings would have been different. Also, only two of\nthe 15 [29, 30] involved HCPs replying to their own\npatients with access to records and prior care context;\nthe remaining studies assessed one-off interactions,\noften from public forums such as Reddit, where tone\nand disclosure may differ from private clinical settings.\nAdditionally, the potential gains in empathic com-\nmunication must be weighed against ongoing concerns\nregarding the reliability of AI-driven clinical content\n[43]; any benefits in empathic delivery risk being over-\nshadowed if the medical advice offered is inaccurate.\nAlthough empathy in healthcare (also called therapeu-\ntic or clinical empathy) is widely recognized as the abil-\nity to understand a patient’s emotions [44], the included\nstudies defined and operationalized it in varied ways,\nwith some using wording that overlapped with related\nconstructs such as compassion, complicating compara-\nbility and interpretability.\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 10, "text": "10\nHowcroft et al.\nMoreover, six of the included studies sourced patient\nquestions\nand/or\nclinician\nresponses\nfrom\npublic\nforums. It could be that the AI chatbots (trained on\nvast internet datasets) [45] encountered the material\nduring training, giving them an unfair advantage\nthrough test-set contamination. However, given the\nenormous volume of training data, any influence from\nspecific queries is likely minimal. Also, models typically\ngenerate original, context-specific responses rather\nthan reproducing exact copies [45]. However, this\nlimitation could also work against the success of the\nAI chatbots: since human responses were generally\nless empathic, mimicking them might reduce the\nchatbots’ relative empathy scores. Most (14/15) of\nthe included studies focused on GPT-3/3.5 or GPT-\n4, which may limit generalisability to clinical practice.\nWidely deployed clinical chatbots (e.g. Wysa) often\nrely on proprietary models with distinct architectures\nand training data that could influence empathic\ncommunication. This variability raises questions about\nhow well the findings translate to tools used in real-\nworld care. Finally, all included studies were from\n2024/2023; newer models such as GPT-4.5, released in\nFebruary 2025 and claimed by OpenAI to demonstrate\ngreater emotional intelligence [46], and GPT-5 have not\nyet been evaluated in the literature. Nine of 15 studies\nlacked a priori power calculations and used conve-\nnience samples, limiting precision and generalisability.\nMost outcome measures were unvalidated (e.g. single-\nitem Likert scales, proportion-based assessments), with\nonly one study using the validated CARE scale [27] and\nthree reporting reliability testing [24, 32, 34], reducing\ncomparability. Comparator groups varied widely in\nroles and specialties, so the pooled average offers a\nbroad estimate but should not be over-interpreted for\nspecific roles, and no study compared empathy across\nthem (e.g. surgeons vs. mental health professionals).\nIn addition, the evaluators of empathy varied (includ-\ning patient proxies, lay people, students, and HCPs,\noften in mixed combinations), further complicating\npooled inferences. Relatedly, many studies lacked the\ndetailed statistics (e.g. means, standard deviations, or\nstandard errors) typically required for SMD calcula-\ntions, necessitating approximations or the combina-\ntion of categories following guidance in the Cochrane\nHandbook [47].\nFinally,\nwhile\nstatistically\nsignificant\nempathy\ndifferences were identified, their clinical relevance—\nsuch as direct impacts on patient outcomes—remains\nuncertain. However, the magnitude of these differences\n(∼20% absolute difference) suggests potential clinical\nrelevance, warranting further investigation into how\nenhanced AI empathy might influence healthcare\noutcomes.\nRecommendations for future research and\npractice\nFuture research should explore leveraging AI’s empathic\nadvantages\nin\ntext-based\ncommunication\nwithout\ncompromising accuracy or safety. One promising\napproach is using AI to draft patient-facing messages,\nsuch as responses to medical or administrative queries,\nfreeing clinician time and enhancing care quality [10].\nBuilding on this, we propose a collaborative human–\nAI interaction model: clinicians produce an initial\nresponse, while AI augments these drafts by refining\ntone and integrating empathy, functioning as an\n‘empathic enhancer’. This approach, ensuring accuracy\nthrough clinician oversight (as advocated by Reynolds,\net al. [30] and Chen, et al. [22]), mitigates risks of AI-\ngenerated inaccuracies by having clinicians generate\nthe core content, with AI solely providing refinement.\nRigorous randomized trials are recommended\nto\nevaluate impacts on patient satisfaction and clinician\nworkload.\nEmerging voice-enabled chatbots (e.g. ChatGPT’s\nAdvanced Voice Mode) claim capabilities to ‘respond\nwith emotion’ and ‘pick up on non-verbal cues’ [48],\nyet no studies in our review compared these systems\nto HCPs. Given the observed empathic advantage\nof AI in text-only interactions, trials in telephone\nconsultations (which account for 26% of all GP\nappointments [49]) could test whether this persists in\nvoice communications.\nNearly all studies blinded raters to the source of\neach response (AI vs. human) to mitigate bias. How-\never, real-world standards require disclosing AI involve-\nment. This disclosure might have diminished perceived\nempathy once evaluators know the response is AI-\ngenerated. Future studies should investigate empathy\nratings in scenarios where participants/reviewers are\nexplicitly informed if they are communicating with\nAI, to ascertain whether the favourable impressions\nseen under blinded conditions persist. Supporting this\nconcern, Perry et al. [50] found that AI-assisted replies\nwere initially rated more empathic than human ones\nin online emotional-support chats, but this advantage\ndisappeared once users learned the responses came\nfrom AI.\nFurther research is necessary on how prompt design\ninfluences empathic outcomes. For instance, two stud-\nies highlighted that restricting AI’s response length—\nmatching typical clinician brevity—reduces perceived\nempathy, while allowing longer responses correlates\nwith higher ratings [10, 33]. Additionally, studies\ndid not instruct the chatbot to emphasize empathy,\npotentially influencing the outcomes, with the study\ninvestigating Med-PaLM 2 suggesting it fell short in\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 11, "text": "Empathy in AI chatbots vs human clinicians\n11\ncomparison to humans, as the ‘agent was not explic-\nitly prompted to produce empathic language’ [26].\nOptimising prompts to balance accuracy, brevity, and\nempathy is essential for real-world implementation.\nBeyond prompt engineering, empathy benchmarking\nshould expand to diverse models beyond the GPT\nfamily (e.g. Claude, Llama) to identify model-specific\nstrengths across clinical contexts.\nFinally, further research should assess patients’ per-\nceptions of empathy based on their own experiences of\nhealthcare consultations.\nConclusion\nOur review indicates that generative AI chatbots—\nparticularly GPT-4—are often perceived as more\nempathic than human practitioners in text-based\ninteractions, a finding consistent across various clinical\ncontexts though with notable exceptions in dermatol-\nogy. While methodological limitations, such as reliance\non unvalidated scales and text-only evaluations, temper\nthese\nresults,\nthe\ncompelling\nevidence\nchallenges\nlongstanding assumptions about human clinicians’\nexclusive capacity for empathic communication—\nincluding assertions from the 2019 Topol Review (a\nUK government-commissioned roadmap for healthcare\ntechnology), which deemed ‘empathy and compassion’\nan ‘essential human skill[s] that AI cannot replicate’\n[8]. Future research should extend to evaluations\nusing\nvoice-based\ninteractions\nand\ndirect\npatient\nfeedback, and ensure rigorous validation and trans-\nparency through randomized trials to uphold clinical\nreliability.\nAcknowledgements\nWe thank Selina Lock, Research Librarian, for verifying the\nsearch strategy.\nAuthor contributions\nAlastair Howcroft (Conceptualization, Data curation, Formal\nanalysis, Investigation, Methodology, Project administration,\nSoftware, Validation, Visualization, Writing—original draft,\nWriting—review & editing), Amber Bennett Weston (Supervi-\nsion, Writing—review & editing), Ahmad Khan (Data curation),\nJoseff Griffiths (Data curation), Simon Gay (Writing—review\n& editing), and Jeremy Howick (Supervision, Writing—review\n& editing)\nSupplementary data\nSupplementary data are available at British Medical Bulletin\nJournal online.\nConﬂict of interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared\nto influence the work reported in this paper.\nFunding\nThis research did not receive any specific grant from funding\nagencies in the public, commercial, or not-for-profit sectors.\nData availability\nAll data underlying this article are available within the main\ntext and its supplementary materials (Appendices A–E), which\ninclude the full search strategy, list of excluded studies, narra-\ntive syntheses, ROB assessments, and study-specific results. No\nadditional datasets were generated or analysed.\nProtocol and registration\nThe protocol was developed following the Preferred Report-\ning Items for Systematic Reviews and Meta-Analysis Protocols\n(PRISMA-P) guidelines and was registered prospectively with\nthe Open Science Framework on 28 October 2024 (https://o\nsf.io/2rt3f). Contrary to the original protocol’s description of a\nscoping review, this study was conducted as a systematic review\nwith meta-analysis. Following protocol registration, clarifica-\ntions were incorporated to refine the eligibility criteria. These\nincluded specifying the inclusion of studies utilising patient-\ngenerated data (e.g. emails), defining ‘AI’ as LLMs to improve\nscope precision, and broadening ‘healthcare settings’ to encom-\npass informal health-related online contexts. In response to the\nobserved quantitative outcome measures across included stud-\nies, the analysis plan was adapted from the originally proposed\nnarrative synthesis to a meta-analysis, enabling the estimation\nof an overall effect size.\nReferences\n1. Howick J, Moscrop A, Mebius A, et al. Effects of\nempathic\nand\npositive\ncommunication\nin\nhealthcare\nconsultations,\na\nsystematic\nreview\nand\nmeta-\nanalysis. J R Soc Med 2018;111:240–52. https://doi.o\nrg/10.1177/0141076818769477.\n2. Wysa. NHS Tackles Mental Health Crisis with Wysa’s\nAI [Internet]. Reading, UK: Wysa Ltd; 2023 [cited 2025\nJan 29]. https://blogs.wysa.io/blog/company-news/nhs-ta\nckles-mental-health-crisis-with-ai.\n3. Alowais SA, Alghamdi SS, Alsuhebany N, et al. Revolu-\ntionizing healthcare, the role of artificial intelligence in\nclinical practice. BMC Med Educ 2023;23:689. https://\ndoi.org/10.1186/s12909-023-04698-z.\n4. Charlotte RB, Cosima L, Jens G, et al. Generative artificial\nintelligence in primary care, an online survey of UK general\npractitioners. BMJ Health Care Inform 2024;31:e101102.\n5. Yu K-H, Beam AL, Kohane IS. Artificial intelligence in\nhealthcare. Nat Biomed Eng 2018;2:719–31. https://doi.o\nrg/10.1038/s41551-018-0305-z.\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 12, "text": "12\nHowcroft et al.\n6. Kurian N. ‘No, Alexa, no!’, Designing child-safe AI and\nprotecting children from the risks of the ‘empathy gap’\nin large language models. Learn Media Technol 49:1–14.\nhttps://doi.org/10.1080/17439884.2024.2367052.\n7. Brown JEH, Halpern J. AI chatbots cannot replace human\ninteractions in the pursuit of more inclusive mental\nhealthcare. SSM Ment Health 2021;1:100017. https://doi.o\nrg/10.1016/j.ssmmh.2021.100017.\n8. Topol E. The Topol Review, Preparing the Health-\ncare Workforce to Deliver the Digital Future [Internet].\nLondon, Health Education England; 2019 [cited 2025\nFeb 7]. https://topol.hee.nhs.uk/wp-content/uploads/HEE-\nTopol-Review-2019.pdf\n9. Morrow E, Zidaru T, Ross F, et al. Artificial intelligence\ntechnologies and compassion in healthcare, a systematic\nscoping review. Front Psychol 2023;13:971044. https://\ndoi.org/10.3389/fpsyg.2022.971044.\n10. Ayers JW, Poliak A, Dredze M, et al. Comparing physi-\ncian and artificial intelligence chatbot responses to patient\nquestions posted to a public social media forum. JAMA\nIntern Med 2023;183:589–96. https://doi.org/10.1001/ja\nmainternmed.2023.1838.\n11. Montemayor C, Halpern J, Fairweather A. In principle\nobstacles for empathic AI, why we can’t replace human\nempathy in healthcare. AI Soc 2022;37:1353–9. https://\ndoi.org/10.1007/s00146-021-01230-z.\n12. Shen J, Zhang CJP, Jiang B, et al. Artificial intelli-\ngence versus clinicians in disease diagnosis, systematic\nreview. JMIR Med Inform 2019;7:e10010. https://doi.o\nrg/10.2196/10010.\n13. Jiang F, Jiang Y, Zhi H, et al. Artificial intelligence\nin healthcare, past, present and future. Stroke Vasc\nNeurol 2017;2:230–43. https://doi.org/10.1136/svn-2017-\n000101.\n14. Topol EJ. High-performance medicine, the convergence of\nhuman and artificial intelligence. Nat Med 2019;25:44–56.\nhttps://doi.org/10.1038/s41591-018-0300-7.\n15. Giuffrè M, Kresevic S, You K, et al. Systematic review, the\nuse of large language models as medical chatbots in diges-\ntive diseases. Aliment Pharmacol Ther 2024;60:144–66.\nhttps://doi.org/10.1111/apt.18058.\n16. Durairaj KK, Baker O, Bertossi D, et al. Artificial intel-\nligence versus expert plastic surgeon, comparative study\nshows ChatGPT “wins” rhinoplasty consultations, should\nwe be worried? Facial Plast Surg Aesthet Med 2024;26:\n270–5. https://doi.org/10.1089/fpsam.2023.0224.\n17. Clarivate Analytics. EndNote [Internet]. Philadelphia, PA,\nUSA: Clarivate; 2025 [cited 2025 Jan 29]. https://endnote.\ncom/?language=en.\n18. Rayyan. Rayyan [Internet]. Cambridge, MA, USA: Rayyan\nSystems, Inc.; 2025 [cited 2025 Jan 30]. https://www.rayya\nn.ai/, 2025, https://doi.org/10.5339/jist.2025.15\n19. The Cochrane Collaboration. ROBINS-I Tool [Internet].\nLondon, UK: Cochrane; 2025 [cited 2025 Feb 28]. https://\nmethods.cochrane.org/robins-i.\n20. Wan P, Huang Z, Tang W, et al. Outpatient reception via\ncollaboration between nurses and a large language model,\na randomized controlled trial. Nat Med 2024;30:2878–85.\nhttps://doi.org/10.1038/s41591-024-03148-7.\n21. The\nCochrane\nCollaboration. Download\nRevMan\n5\n[Internet]. London, UK: Cochrane; 2020 [cited 2025 Feb\n3]. https://test-training.cochrane.org/online-learning/core-\nsoftware-cochrane-reviews/review-manager-revman/do\nwnload-revman-5.\n22. Chen D, Parsa R, Hope A, et al. Physician and artificial\nintelligence chatbot responses to cancer questions from\nsocial media. JAMA Oncol 2024;10:956–60. https://doi.o\nrg/10.1001/jamaoncol.2024.0836.\n23. Meyer A, Soleman A, Riese J, et al. Comparison of Chat-\nGPT, Gemini, and le chat with physician interpretations\nof medical laboratory questions from an online health\nforum. Clin Chem Lab Med 2024;62:2425–34. https://\ndoi.org/10.1515/cclm-2024-0246.\n24. Yonatan R, Brukner H. Comparing perceived empathy\nand intervention strategies of an AI chatbot and human\npsychotherapists in online mental health support. Couns.\nPsychother\nRes\n2024;25.\nhttps://doi.org/10.1002/capr.\n12832.\n25. External Medicine Podcast. John Ayers, PhD, ChatGPT\nand the Future of Medicine [Internet]. Mountain View, CA,\nUSA: Google LLC; 2023 [cited 2025 Feb 17]. https://www.\nyoutube.com/watch?v=ECzx49KmPtA.\n26. Li B, Wang A, Strachan P. et al. Conversational AI in\nhealth, design considerations from a wizard-of-oz derma-\ntology case study with users, clinicians and a medical LLM.\nIn: Mueller FF, Kyburz P, Williamson JR, Sas C, Wilson\nML, Toups Dugas P, Shklovski I (eds.), CHI ’24, Proceed-\nings of the 2024 CHI Conference on Human Factors in\nComputing Systems. New York, NY, USA: Association for\nComputing Machinery; 2024.\n27. Maida E, Moccia M, Palladino R, et al. ChatGPT vs.\nneurologists, a cross-sectional study investigating pref-\nerence, satisfaction ratings and perceived empathy in\nresponses among people living with multiple sclero-\nsis. J Neurol 2024;271:4057–66. https://doi.org/10.1007/\ns00415-024-12328-x.\n28. Armbruster J, Bussmann F, Rothhaas C, et al. “Doctor\nChatGPT, can you help me?”, the patient’s perspective,\ncross-sectional study. J Med Internet Res 2024;26:e58831.\nhttps://doi.org/10.2196/58831.\n29. Small WR, Wiesenfeld B, Brandfield-Harvey B, et al. Large\nlanguage model–based responses to patients’ in-basket\nmessages. JAMA Netw Open 2024;7:e2422399. https://\ndoi.org/10.1001/jamanetworkopen.2024.22399.\n30. Reynolds K, Nadelman D, Durgin J, et al. Comparing the\nquality of ChatGPT- and physician-generated responses to\npatients’ dermatology questions in the electronic medical\nrecord. Clin Exp Dermatol 2024;49:715–8. https://doi.o\nrg/10.1093/ced/llad456.\n31. Guo S, Li R, Li G, et al. Comparing ChatGPT’s and\nsurgeon’s responses to thyroid-related questions from\npatients. J Clin Endocrinol Metab 2025;110:e841–50.\nhttps://doi.org/10.1210/clinem/dgae235.\n32. Xu D, Zhao J, Liu R, et al. ChatGPT4’s proficiency\nin addressing patients’ questions on systemic lupus ery-\nthematosus, a blinded comparative study with special-\nists. Rheumatology (Oxford) 2024;63:2450–6. https://\ndoi.org/10.1093/rheumatology/keae238.\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}, {"page": 13, "text": "Empathy in AI chatbots vs human clinicians\n13\n33. Soroudi\nD, Gozali\nA, Knox\nJA, et\nal. Comparing\nprovider and ChatGPT responses to breast reconstruc-\ntion patient questions in the electronic health record.\nAnn Plast Surg 2024;93:541–5. https://doi.org/10.1097/SA\nP.0000000000004090.\n34. He W, Zhang W, Jin Y, et al. Physician versus large\nlanguage model chatbot responses to web-based questions\nfrom autistic patients in Chinese, cross-sectional compara-\ntive analysis. J Med Internet Res 2024;26:e54706. https://\ndoi.org/10.2196/54706.\n35. Yong LPX, Tung JYM, Lee ZY, et al. Performance of\nlarge language models in patient complaint resolution,\nweb-based cross-sectional survey. J Med Internet Res\n2024;26:e56413. https://doi.org/10.2196/56413.\n36. World Population Review. Reddit Users by Country 2025\n[Internet]. Lancaster, PA, USA: World Population Review;\n2025 [cited 2025 Aug 11]. https://worldpopulationreview.\ncom/country-rankings/reddit-users-by-country.\n37. Hussein H, Nevill CR, Meffen A, et al. Double-counting\nof populations in evidence synthesis in public health, a\ncall for awareness and future methodological develop-\nment. BMC Public Health 2022;22:1827. https://doi.o\nrg/10.1186/s12889-022-14213-6.\n38. Egede J, Trigo MJG, Hazzard A, et al. Designing an adap-\ntive embodied conversational agent for health literacy, a\nuser study. In: Broekens J, Jokinen K, Ogawa K (eds.),\nProceedings of the 21st ACM International Conference on\nIntelligent Virtual Agents (IVA ’21). New York, NY, USA:\nAssociation for Computing Machinery; 2021. p. 112–19.\n39. Leahy D, Lyons A, Dahm M, et al. Use of text messaging\nin general practice, a mixed methods investigation on GPs’\nand patients’ views. Br J Gen Pract 2017;67:e744–50.\nhttps://doi.org/10.3399/bjgp17X693065.\n40. Ward A, Morrison C. A collaboration on teaching commu-\nnication by text. Clin Teach 2022;19:294–8. https://doi.o\nrg/10.1111/tct.13498.\n41. Bernardo MO, Cecílio-Fernandes D, Costa P, et al.\nPhysicians’ self-assessed empathy levels do not correlate\nwith patients’ assessments. PloS One 2018;13:e0198488.\nhttps://doi.org/10.1371/journal.pone.0198488.\n42. Hermans L, Olde Hartman T, Dielissen PW. Differences\nbetween GP perception of delivered empathy and patient-\nperceived empathy, a cross-sectional study in primary care.\nBr J Gen Pract 2018;68:e621–6. https://doi.org/10.3399/\nbjgp18X698381.\n43. Barile J, Margolis A, Cason G, et al. Diagnostic accu-\nracy of a large language model in pediatric case studies.\nJAMA Pediatr 2024;178:313–5. https://doi.org/10.1001/ja\nmapediatrics.2023.5750.\n44. Howick J, Bennett-Weston A, Dudko M, et al. Uncover-\ning the components of therapeutic empathy through the-\nmatic analysis of existing definitions. Patient Educ Couns\n2024;131:108596.\n45. Deng C, Zhao Y, Heng Y, et al. Unveiling the spectrum\nof data contamination in language model, a survey from\ndetection to remediation. In: Findings of the Association\nfor Computational Linguistics: ACL 2024. Bangkok, Thai-\nland: Association for Computational Linguistics; 2024. p.\n16078–92.\n46. OpenAI. Introducing GPT-4.5 [Internet]. San Francisco,\nCA, USA: OpenAI; 2025 [cited 2025 Mar 14]. https://ope\nnai.com/index/introducing-gpt-4-5/.\n47. Higgins JPT, Thomas J, Chandler J et al. Cochrane Hand-\nbook for Systematic Reviews of Interventions [Internet].\nLondon, UK: Cochrane; 2024 [cited 2025 Feb 1]. https://\nwww.training.cochrane.org/handbook.\n48. OpenAI. Voice Mode FAQ [Internet]. San Francisco, CA,\nUSA: OpenAI; 2024 [cited 2024 Dec 28]. https://help.ope\nnai.com/en/articles/8400625-voice-mode-faq.\n49. The King’s Fund. Activity in the NHS [Internet]. Lon-\ndon, UK: The King’s Fund; 2024 [cited 2024 Jan\n5]. https://www.kingsfund.org.uk/insight-and-analysis/da\nta-and-charts/NHS-activity-nutshell.\n50. Perry A. AI will never convey the essence of human\nempathy. Nat Hum Behav 2023;7:1808–9. https://doi.o\nrg/10.1038/s41562-023-01675-w.\n© The Author(s) 2025. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution,\nand reproduction in any medium, provided the original work is properly cited.\nBritish Medical Bulletin, 2025, 156, 1–13\nhttps://doi.org/10.1093/bmb/ldaf017\nInvited Review\nDownloaded from https://academic.oup.com/bmb/article/156/1/ldaf017/8293249 by guest on 31 December 2025\n"}]}