{"doc_id": "arxiv:2512.20586", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.20586.pdf", "meta": {"doc_id": "arxiv:2512.20586", "source": "arxiv", "arxiv_id": "2512.20586", "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent", "authors": ["Humza Nusrat", "Luke Francisco", "Bing Luo", "Hassan Bagher-Ebadian", "Joshua Kim", "Karen Chin-Snyder", "Salim Siddiqui", "Mira Shah", "Eric Mellon", "Mohammad Ghassemi", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"], "published": "2025-12-23T18:32:17Z", "updated": "2025-12-23T18:32:17Z", "summary": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.20586v1", "url_pdf": "https://arxiv.org/pdf/2512.20586.pdf", "meta_path": "data/raw/arxiv/meta/2512.20586.json", "sha256": "f2052cf42b63b785a750fb0f403a3a2df36596cc177dae473b43807dcb9895de", "status": "ok", "fetched_at": "2026-02-18T02:23:53.065530+00:00"}, "pages": [{"page": 1, "text": "Automated stereotactic radiosurgery planning using\na human-in-the-loop reasoning large language\nmodel agent\nHumza Nusrat1,2*, Luke Francisco1,3, Bing Luo1,\nHassan Bagher-Ebadian1,2, Joshua Kim1, Karen Chin-Snyder1,\nSalim Siddiqui1,2, Mira Shah1,2, Eric Mellon1,2,\nMohammad Ghassemi4, Anthony Doemer1, Benjamin Movsas1,2,\nKundan Thind1,2\n1*Department of Radiation Oncology, Henry Ford Health, Detroit, MI.\n2Department of Radiology, Michigan State University, East Lansing, MI.\n3Department of Statistics, University of Michigan, Ann Arbor, MI.\n4College of Engineering, Michigan State University, East Lansing, MI.\n*Corresponding author(s). E-mail(s): hnusrat1@hfhs.org;\nAbstract\nStereotactic radiosurgery (SRS) demands precise dose shaping around critical\nstructures, yet black-box AI systems have limited clinical adoption due to opacity\nconcerns. We tested whether chain-of-thought reasoning improves agentic plan-\nning in a retrospective cohort of 41 patients with brain metastases treated with 18\nGy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose\nExpertise), an LLM-based planning agent for automated SRS treatment planning.\nTwo variants generated plans for each case: one using a non-reasoning model, one\nusing a reasoning model. The reasoning variant showed comparable plan dosimetry\nrelative to human planners on primary endpoints (PTV coverage, maximum dose,\nconformity index, gradient index; all p > 0.21) while reducing cochlear dose below\nhuman baselines (p = 0.022). When prompted to improve conformity, the rea-\nsoning model demonstrated systematic planning behaviors including prospective\nconstraint verification (457 instances) and trade-off deliberation (609 instances),\nwhile the standard model exhibited none of these deliberative processes (0 and 7\ninstances, respectively). Content analysis revealed that constraint verification and\ncausal explanation concentrated in the reasoning agent. The optimization traces\nserve as auditable logs, offering a path toward transparent automated planning.\nKeywords: Reasoning model, Radiotherapy planning, Agentic AI\n1\narXiv:2512.20586v1  [cs.AI]  23 Dec 2025\n"}, {"page": 2, "text": "1 Introduction\nTreatment planning in radiation oncology has grown increasingly complex. A treatment\nplan consists of a set of instructions generated within a treatment planning system\n(TPS) that directs the linear accelerator during dose delivery. This task, performed\nby dosimetrists and medical physicists, requires specialized expertise, substantial time\ninvestment, and is subject to variability based on individual planner skill. The degree of\ncomplexity varies considerably by tumor site and treatment technique. For convention-\nally fractionated treatments of anatomically stable sites such as prostate cancer, target\nvolumes are relatively homogeneous and organs at risk occupy predictable positions,\nfacilitating more standardized planning approaches [1, 2].\nIn contrast to this, stereotactic radiosurgery (SRS) for brain metastases represents\nthe opposite end of this spectrum. SRS delivers a large radiation dose to intracranial\ntumors in a single treatment fraction [3]. The targets are typically brain metastases,\nwhich often present with clinical urgency and compressed treatment timelines. Several\nfactors contribute to the technical difficulty of SRS planning: critical OARs in close\nproximity to targets, the need for steep dose gradients to minimize normal brain expo-\nsure, and the extra precision required when the entire prescribed dose is delivered in\none session. Given the current shortage of qualified treatment planners [4, 5] and the\nspecialized nature of SRS, these treatments are largely confined to large academic med-\nical centers [6, 7]. Automated treatment planning using artificial intelligence (AI) offers\na potential solution to improve access and reduce workforce burden. Prior work in AI-\ndriven treatment planning has largely relied on neural networks trained on institutional\nretrospective data for specific tumor sites. Several groups have reported successful imple-\nmentations of this approach [8–10]. However, these methods have notable limitations.\nThey are constrained to the anatomical site and treatment technique represented in the\ntraining data. Such systems function as black boxes that offer limited transparency or\nexplainability regarding their optimization decisions [11–23]. This approach does not\nscale well across institutions because each implementation remains siloed to the center\nthat performed the development and training.\nRegulatory frameworks for AI-based medical devices increasingly emphasize inter-\npretability and transparency [24–28], while surveys of radiation oncology professionals\nidentify explainability as a key determinant of clinical acceptance [29–31]. These\nconcerns around model opacity represent substantial barriers to widespread adoption.\nThe application of large language models (LLMs) to radiation oncology is an emerg-\ning area of investigation. Published work has primarily focused on retrieval-augmented\ngeneration (RAG) for clinical question answering, protocol compliance verification, and\nknowledge-grounded decision support [32–34]. These applications leverage LLMs’ ability\nto retrieve and synthesize information from clinical guidelines.\nBy contrast, the present work employs LLMs for iterative, reasoning-driven treat-\nment plan optimization, a fundamentally different task requiring spatial reasoning,\nconstraint satisfaction, and forward simulation of dosimetric consequences. This dis-\ntinction is critical: retrieval-based systems improve reliability by grounding responses\nin validated knowledge sources, whereas reasoning-based planning requires the model\nto perform multi-step logical inference over complex geometric and dosimetric trade-\noffs. To date, all reported LLM-based planning studies have employed non-reasoning\n2\n"}, {"page": 3, "text": "models without explicit reasoning capabilities, and none have addressed the geometric\ncomplexity of SRS planning where transparent, stepwise reasoning is essential.\nThe present work addresses these gaps. We employ SAGE, a general agentic frame-\nwork previously validated for prostate cancer planning [35], and apply it to SRS. We\ndirectly compare a reasoning LLM against a non-reasoning LLM within the same plan-\nning framework and tasks. We include a mechanistic dialogue analysis that connects\nmodel behavior to planning outcomes, providing insight into how reasoning architecture\ninfluences optimization strategy.\nRecent LLM development has produced models optimized for different computational\nbehaviors. While all LLMs fundamentally operate through next-token prediction, some\nmodels are specifically trained to generate extended intermediate reasoning steps during\ninference before producing final outputs. For practical purposes, we refer to these as\n”reasoning models” versus ”non-reasoning models,” recognizing this as a behavioral\ndistinction rather than a fundamental architectural dichotomy [36–38].\nThis behavioral difference, at a functional level, resembles Kahneman’s distinction\nbetween System 1 (fast, automatic) and System 2 (slow, deliberative) thinking [39]. We\nstress that this is an analogy drawn from observable behavior; it is not a claim of cog-\nnitive equivalence between artificial and human intelligence. Dual-process theory has\nshaped how clinical medicine understands complex decision-making. System 2 think-\ning, with its explicit hypothesis testing, constraint checking, and iterative refinement,\nappears essential for diagnostic and therapeutic tasks that resist pattern-based shortcuts\n[40–45]. SRS planning shares several characteristics with tasks that empirically benefit\nfrom deliberative processing: tightly coupled geometric constraints, competing objec-\ntives, three-dimensional spatial reasoning. These properties set SRS apart from more\nstereotyped planning workflows.\nWe hypothesized, given these task characteristics, that SRS planning would par-\nticularly benefit from LLM architectures exhibiting deliberative, multi-step reasoning\nbehavior. A second hypothesis followed from the first. Intermediate reasoning traces\nwould serve a dual purpose: improving the model’s spatial reasoning through self-\nprompting mechanisms documented in 3D reasoning tasks and constituting an auditable\ndecision log. This log, a structured record of constraint verification, trade-off evaluation,\nand iterative refinement, can be reviewed by human planners, incorporated into quality\nassurance documentation, and examined in the event of adverse outcomes.\n2 Methods\n2.1 SAGE architecture\nThe software architecture of SAGE is shown in Figure 1. Upon initialization, the agent\nreceives the clinical scenario (patient anatomy, target volume location and size, spatial\nrelationship between PTV and organs at risk), prescription dose (18 Gy to the PTV in\na single fraction), and current state of the optimizer including all dosimetric parameters\n(DVH metrics for PTV and all OARs). SAGE is then prompted to achieve target cov-\nerage while respecting OAR constraints. We tested two variants: a non-reasoning model\nand a reasoning model. Throughout this manuscript, we use ”non-reasoning model” to\n3\n"}, {"page": 4, "text": "refer to general-purpose LLMs that generate responses through direct next-token pre-\ndiction, in contrast to reasoning models that produce intermediate chain-of-thought\nsteps.\nBoth variants performed up to ten (SAGE stops once clinical goals are met) iterations\nof optimization, dose calculation, and plan evaluation. Both variants were subjected\nto identical, deterministic stopping logic. SAGE terminated optimization when all of\nthe clinical goals were simultaneously satisfied. If these criteria were not met after ten\niterations, optimization terminated, and the best-performing plan was selected.\nThe human-in-the-loop stage served as the decision point for plan disposition. At\nthis stage, a human reviewer either accepted the plan or redirected it to SAGE for a\nsecondary refinement step focused on improving dose conformity. This two-stage archi-\ntecture allowed us to evaluate both autonomous planning capability and responsiveness\nto human feedback.\n2.2 Human-in-the-loop refinement\nA single board-certified medical physicist evaluated all plans. Plans were accepted if\nthey met all quantitative clinical criteria; those failing to meet conformity benchmarks\nwere directed to the refinement stage.\nRefinement followed a standardized protocol. All plans requiring refinement received\nan identical natural language prompt requesting conformity improvement while main-\ntaining target coverage and OAR constraints. This prompt was applied uniformly across\ncases regardless of model variant. No case-specific modifications were permitted.\nFig. 1 The agent receives clinical inputs (patient anatomy, prescription, physician constraints) and\ncurrent optimizer state. Two model variants are shown: non-reasoning (top) and reasoning (bottom).\nEach executes iterative optimization cycles comprising LLM-based parameter adjustment, dose calcu-\nlation, plan evaluation, and objective updates. The optimal plan proceeds to human review, where it\nis either accepted or returned with refinement feedback.\n4\n"}, {"page": 5, "text": "2.3 Model Details\nThe non-reasoning model was Llama 3.1-70B [46]. The reasoning model was Qwen QwQ-\n32B-Reasoning [47]. LLM hyperparameters were held constant across all experiments\n(top k = 2; T = 0.4); these values were optimized in prior work by our group [35]. RAG\nwas enabled, allowing SAGE to access its previous priority number selections and their\nresulting dose distributions. Both models were hosted locally on eight NVIDIA A100\nGPUs within an institutional high-performance computing cluster.\n2.4 Patient Cohort\nWe retrospectively obtained treatment data for patients with brain metastases who\nreceived single target SRS at our institution between 2022 and 2024. All treatments\nfollowed institutional clinical practice guidelines with a prescription of 18 Gy in a single\nfraction [48, 49]. The cohort comprised 41 patients for whom CT images, segmented\nstructures, clinical treatment plans, and dosimetric data were available. This study was\nconducted with institutional review board approval.\nAll retrospective clinical plans and SAGE-generated plans were created and housed\nwithin the Varian Eclipse Treatment Planning System (version 16.1). Dose calculations\nwere performed using the AAA algorithm (version 15.6.06) with a dose grid resolution\nof 1.25 mm. For each patient, beam geometry was fixed to match the clinical plan\nconfiguration. Dose volume histogram (DVH) estimation (version 15.6.05) and photon\noptimization (version 15.6.05) algorithms were used for all cases.\n2.5 Mechanistic Content Analysis\nTo characterize behavioral differences between the reasoning and non-reasoning mod-\nels beyond dosimetric endpoints, we performed a systematic content analysis of the\noptimization dialogues generated during planning. Within radiation oncology, we\nhypothesize treatment planning to be a clinical task that necessitates System 2 reasoning\nbecause of the complex nature of optimization tradeoffs.\nWe operationalized six categories of System 2 cognitive processing derived from\nthe dual-process literature: problem decomposition (breaking complex objectives into\nsub-goals), prospective verification (checking constraints before action), self-correction\n(revising approach after recognizing errors), mathematical reasoning (explicit numerical\ncomputation), trade-off deliberation (weighing competing objectives), and forward sim-\nulation (predicting dosimetric consequences of proposed actions). This categorization\nwas adapted from previous work in System 2 LLM classification [50]. We additionally\nquantified format errors, defined as malformed structured output that failed to parse.\nWe employed a hybrid automated-manual approach for content analysis. A custom\nscript performed initial detection of System 2 cognitive processes using keyword and\nphrase pattern matching. The script searched for linguistic markers associated with\neach cognitive category (Table 1). A random sample of 10% of automated classifications\nwas manually verified assess concordance. Format errors were defined as malformed\nstructured output that failed JSON parsing.\n5\n"}, {"page": 6, "text": "Table 1 Cognitive categories identified in the reasoning analysis, including linguistic markers and example\nterminology.\nCognitive category\nLinguistic marker\nExample words\nProblem decomposition\nPlanning language\n‘First’, ‘then’, ‘next’, ‘I will start by’\nProspective verification\nConditional statements with con-\nstraint references\n‘If... then’, ‘would exceed’, ‘checking\nwhether’, ‘to make sure V12Gy stays\nunder’\nSelf-correction\nRevision language\n‘reverting’,\n‘instead’,\n‘previous\nattempt’, ‘I will revise’, ‘This assump-\ntion was incorrect’\nMathematical reasoning\nNumerical expressions and calcula-\ntions\n‘delta’,\n‘fraction’,\n‘greater\nthan’,\n‘increase from X to Y’\nTrade-off deliberation\nComparative\nlanguage\ninvolving\ncompeting objectives\n‘balance’, ‘prioritize’, ‘versus’, ‘at the\ncost of’\nForward simulation\nPredictive language\n‘will cause’, ‘expected to’, ‘will result\nin’\n2.6 Statistical Analysis and Experimental Design\nWe used paired, non-parametric Wilcoxon signed-rank tests for all plan-to-plan compar-\nisons, with statistical significance defined as p < 0.05. The choice of a non-parametric\napproach was supported by Shapiro-Wilk testing of paired differences and by visual\ninspection of Q-Q plots. Multiplicity correction was performed using the Benjamini-\nHochberg (BH) procedure to control the false discovery rate (FDR) at q < 0.05 within\ntwo pre-specified hypothesis families: primary endpoints (target coverage, maximum\ndose, conformity index, gradient index) and secondary endpoints (all seven OARs).\nData were visualized using violin plots for each dosimetric endpoint. Individual\npatient values were displayed as jittered points, with interquartile range (IQR) boxes\nand median values overlaid. Significance brackets were applied only to comparisons that\nremained significant after BH correction. All statistical analyses and visualizations were\nperformed in R using the tidyverse, ggsignif, and ggbeeswarm packages.\n3 Results\n3.1 Target coverage and dose homogeneity\nBoth model variants met clinical acceptance criteria for target dosimetry (Figure 2). The\nreasoning model achieved median PTV coverage of 96.8% (IQR: 95.9-97.4%), compared\nto 96.5% (IQR: 95.6-97.2%) for clinical plans. The non-reasoning model achieved median\ncoverage of 96.2% (IQR: 95.1-97.0%). All three groups maintained median maximum\ndoses below the 21.6 Gy threshold, with the reasoning model producing a distribution\nmost closely aligned with clinical plans. No patient in either AI cohort fell below the\n95% coverage threshold.\n6\n"}, {"page": 7, "text": "Fig. 2 PTV coverage (left) and maximum dose (right) for clinical plans (grey), non-reasoning model\n(red), and reasoning model (blue). Violin contours represent kernel density estimates. White boxes\nindicate IQR; red points indicate median values; black points represent individual patients (n = 41).\nDashed lines denote clinical acceptance thresholds (coverage ¿ 95%; maximum dose < 21.6 Gy). Brackets\nindicate comparisons between reasoning and clinical groups that remained significant after BH correction\n(q < 0.05).\n3.2 OAR Sparing\nFor the brainstem and optic chiasm, the reasoning model produced dose distributions\ncomparable to clinical plans, with all cases remaining below institutional tolerance\nthresholds. Normal brain exposure, quantified as V12Gy (volume receiving at least 12\nGy), remained within the clinical limit of 10 cc for most cases across all three cohorts.\nNo significant differences were observed between the reasoning model and clinical plans\nfor any central OAR endpoint after Benjamini-Hochberg correction.\nLateral OARs, including the bilateral optic nerves and cochleae, were assessed against\na maximum dose threshold of 9 Gy (Figure 4). Both AI variants maintained doses below\nthis limit across all structures. The reasoning model achieved significantly lower doses\nto the right cochlea compared to clinical plans (p = 0.022 after BH correction). Doses\nto the left cochlea, right optic nerve, and left optic nerve did not differ significantly\nbetween the reasoning model and clinical plans.\n7\n"}, {"page": 8, "text": "Fig. 3 Maximum doses to brainstem and optic chiasm, and V12Gy for normal brain (defined as\nbrain minus gross tumor volume (GTV)) across clinical plans (grey), non-reasoning model (red), and\nreasoning model (blue). Violin contours represent kernel density estimates. White boxes indicate IQR;\nred points indicate median values; black points represent individual patients (n = 41). Brackets indicate\ncomparisons between reasoning and clinical groups that remained significant after BH correction (q <\n0.05).\n8\n"}, {"page": 9, "text": "Fig. 4 Maximum doses to bilateral optic nerves and cochleae across clinical plans (grey), non-reasoning\nmodel (red), and reasoning model (blue). Violin contours represent kernel density estimates. White\nboxes indicate IQR; red points indicate median values; black points represent individual patients (n =\n41). The reasoning model achieved significantly lower right cochlear doses compared to clinical plans.\nAll other comparisons were not significant.\n3.3 Response to human-in-the-loop refinement\nWe first assessed whether the AI agent could respond appropriately to human feedback\nregarding plan conformity. Both model variants demonstrated statistically significant\nimprovement in conformity index (CI) following a natural language refinement prompt\nby human (Figure 5). Both models achieved statistically significant improvements in CI\n(reasoning: p < 0.001, non-reasoning: p = 0.007). The smaller p-value for the reasoning\nmodel reflects greater consistency of improvement across patients (lower variance), as\nevidenced by narrower confidence intervals, rather than necessarily greater magnitude\nof improvement.\nAfter refinement, the reasoning model achieved a median CI that closely approxi-\nmated the clinical benchmark, whereas the non-reasoning model, although significantly\n9\n"}, {"page": 10, "text": "improved, remained further from clinical values. Both models interpreted and acted on\nnatural language feedback, but the reasoning model achieved the larger conformity gain.\nFig. 5 Conformity index values before (Round 1) and after (Round 2) the refinement prompt for rea-\nsoning (blue) and non-reasoning (yellow) model variants. Both models showed significant improvement\nfollowing refinement (non-reasoning: p = 0.007; reasoning: p < 0.001, paired Wilcoxon signed-rank\ntest). The reasoning model achieved median conformity approaching clinical benchmark values. Boxes\nindicate IQR; horizontal lines indicate median; whiskers extend to a 1.5 multiple of IQR.\n3.4 Paired comparison of reasoning agent versus clinical plans\nWe performed paired difference analysis comparing the reasoning agent to clinical plans\nacross all dosimetric endpoints (Figure 6). The reasoning agent demonstrated dosimetric\noutcomes comparable to clinical plans across primary target coverage metrics, with no\nstatistically significant differences detected. PTV coverage did not differ significantly (p\n= 0.21), nor did maximum dose (p = 0.98). Plan quality metrics, including conformity\nindex (p = 0.23) and gradient index (p = 0.71), did not differ significantly between the\nreasoning agent and human planners. For OAR endpoints, the reasoning agent achieved\ncomparable or superior performance across all structures. The right cochlea was the only\nstructure demonstrating a statistically significant difference, favoring SAGE (p = 0.022\nafter BH correction).\n10\n"}, {"page": 11, "text": "Fig. 6 Median paired differences (reasoning minus clinical) across all dosimetric endpoints. For OAR\ndoses and plan quality metrics (CI, GI), values left of zero indicate AI superiority (lower doses, better\nconformity/gradient); for PTV coverage, values right of zero indicate AI superiority (higher coverage).\nn = 41 patients; error bars represent 95% confidence intervals; paired Wilcoxon signed-rank tests with\nBH correction for secondary endpoints. The reasoning agent demonstrated significantly improved right\ncochlear sparing (q < 0.05). No significant differences were observed for target coverage, maximum dose,\nconformity index, gradient index, or other OAR endpoints.\n3.5 Mechanistic differences between reasoning and\nnon-reasoning agents\nProblem decomposition, prospective verification, and self-correction were detected exclu-\nsively in the reasoning model (n = 537, 457, and 735 instances, respectively). The\nnon-reasoning model produced zero instances of these behaviors across all 41 patients\nand all optimization iterations. The remaining cognitive processes, while present in\nboth models, showed similar asymmetric distributions. Mathematical reasoning occurred\nprimarily in the reasoning model (1,162 vs. 49 instances; 96% share). Trade-off deliber-\nation (609 vs. 7 instances; 99% share) and forward simulation (1,888 vs. 58 instances;\n97% share) followed the same pattern. The reasoning model dominated across all six\ncognitive categories. These behavioral differences translated to measurable reliability\nimprovements. The reasoning model produced five-fold fewer format errors than the\nnon-reasoning model (25 vs. 122 total; median 0 vs. 3 per patient).\n11\n"}, {"page": 12, "text": "Fig. 7 System 2 cognitive processes (problem decomposition, prospective verification, self-correction,\nmathematical reasoning, trade-off deliberation, forward simulation) were quantified across all patients.\nThree processes were detected exclusively in the reasoning model; the remaining three occurred in both\nmodels but were concentrated in the reasoning agent. Output reliability was assessed by format errors\nper patient; the reasoning model achieved median 0 errors versus median 3 for non-reasoning.\n4 Discussion\nThis study demonstrates that an AI planning agent equipped with System 2 reasoning\ncapabilities can produce SRS plans that are equivalent to those generated by experi-\nenced human dosimetrists. In the case of cochlear sparing, the reasoning agent achieved\nstatistically superior performance. The cognitive architecture of an AI system may\nbe as consequential for clinical performance as the quality of its training data or the\nsophistication of its optimization algorithms.\nOur comparison of System 1 and System 2 thinking within the same agent frame-\nwork distinguishes this work from prior autoplanning studies. Both SAGE variants met\nclinical constraints for target coverage and OAR dose limits. However, the reasoning\nmodel achieved significantly lower doses to the right cochlea (p = 0.022 after BH FDR\ncorrection), despite both AI and human plans remaining well below the 9 Gy tolerance\nthreshold.\n12\n"}, {"page": 13, "text": "This difference may potentially reflect distinct optimization strategies. Human plan-\nners operate under pragmatic constraints; once an OAR meets its dose threshold,\nclinical workflow pressures discourage further optimization [5]. Shift schedules, compet-\ning responsibilities, and cognitive fatigue all favor satisficing behavior. The reasoning\nagent, free from these pressures, appears to have implemented the ALARA (as low as\nreasonably achievable) principle more completely. The cochlear result does not indicate\na failure of human planning. Rather, it may reflect an inherent limitation of the satisfic-\ning heuristic in dose optimization. Whether this additional OAR sparing translates to\nreduced late toxicity remains to be determined in prospective studies.\nThe CI results merit particular consideration [51]. Given that SRS delivers high\ndoses in a single fraction to targets surrounded by sensitive structures, dose conformity\nis of primary clinical importance. Our human-in-the-loop experiment demonstrated that\nboth AI variants improved their conformity indices following a natural language refine-\nment prompt. The reasoning model exhibited more pronounced improvement compared\nto the non-reasoning model, with median values approaching clinical benchmarks. Spa-\ntial reasoning in three dimensions, specifically the manipulation of dose distributions\naround geometrically complex targets, appears to benefit from deliberative processing.\nThe reasoning agent’s use of spherical rind structures to improve conformity exempli-\nfies the strategic planning approaches employed by experienced dosimetrists, approaches\nthat simpler models appear unable to generalize. This agrees with several previous works\nin other domains that have demonstrated the improved performance of reasoning or\nSystem 2 models at three dimensional tasks [52–54].\nThe deployment of autonomous reasoning tools such as SAGE suggest a potential\nreorganization of roles in future radiation oncology treatment planning. If optimization\ncan be reliably delegated to a System 2 agent, dosimetrists and physicists may focus\non clinical judgment, quality assurance, and strategic decisions that remain beyond\ncurrent AI capabilities. The human-in-the-loop architecture represents a model for\nhuman-AI collaboration in which each contributes according to its strengths. Humans\nprovide clinical context, identifies when conformity requires refinement, and exercises\nfinal approval.\nSeveral limitations should be acknowledged. First, our comparisons were based\non non-inferiority rather than formal equivalence testing with pre-specified margins.\nThe observed associations between reasoning behaviors and dosimetric outcomes do\nnot establish causation; differences could reflect model architecture, training data, or\nprompting rather than intrinsic reasoning capabilities. Our 41-patient cohort derives\nfrom a single institution. While this sample provides adequate statistical power for\nthe comparisons we undertook, external validation across centers with different plan-\nning conventions and beam configurations will be necessary before broader claims of\ngeneralizability can be supported. The computational demands of System 2 reasoning,\nparticularly the inference-time costs of models such as QwQ-32B, remain substantial and\nmay limit deployment in resource-constrained settings. The reasoning model required\napproximately threefold longer inference time per plan compared to the non-reasoning\nvariant. Although we demonstrated non-inferiority across a comprehensive set of dosi-\nmetric endpoints, the ultimate outcome of interest, patient survival and toxicity, lies\nbeyond the scope of this retrospective analysis. Finally, the asymmetric cochlear finding\nwarrants comment. We cannot definitively explain this laterality. Possible contributors\n13\n"}, {"page": 14, "text": "include the spatial distribution of tumor locations in our cohort, which may have placed\nmore lesions in proximity to the right cochlea; systematic asymmetries in beam geometry\ninherited from the clinical plans; or statistical variation inherent to multiple comparisons\nacross a 41-patient sample.\nThis study is best understood as a behavioral analysis of AI architecture rather than\na clinical efficacy trial. The effect sizes of behavioral differences suggest that the distinc-\ntion between System 1 and System 2 architectures is robust regardless of cohort scale.\nWe cannot fully disentangle the contribution of explicit reasoning from other differences\nbetween the two LLMs, including training data composition and model architecture. An\nimportant practical consideration is that the reasoning model is not universally supe-\nrior for all treatment planning scenarios. Our findings suggest the reasoning model’s\nadvantages emerge primarily in cases involving subtle dosimetric trade-offs or when\ntransparency in the optimization process is clinically valuable, such as understanding\nwhy certain OAR doses could not be further reduced. For routine cases with well-defined\nconstraints and limited degrees of freedom, the non-reasoning model often achieves clin-\nically acceptable plans more efficiently. Inference times were approximately threefold\nfaster with reduced computational cost. The choice between models should therefore be\nguided by case complexity, the need for optimization transparency, and available com-\nputational resources. Future work will include prospective, multi-institutional validation\nwith outcome assessment; reader studies comparing oncologist and physicist ratings of\nAI-generated versus human plans; and extension of the reasoning framework to other\ncomplex indications including spine SRS, multiple brain metastases, and extracranial\nstereotactic body radiotherapy (SBRT).\n5 Conclusion\nWe have demonstrated that a System 2 reasoning agent can generate stereotactic radio-\nsurgery plans that meet or exceed the quality of those produced by experienced human\nplanners. Across 41 patients, the reasoning variant of SAGE achieved equivalent perfor-\nmance on all primary dosimetric endpoints, including target coverage, maximum dose,\nconformity index, and gradient index. On one secondary endpoint, right cochlear dose,\nthe reasoning agent achieved statistically superior sparing compared to clinical plans.\nThese results were accompanied by qualitatively distinct planning behavior: the reason-\ning model exhibited constraint verification, causal explanation, and iterative memory\nreference patterns consistent with deliberative cognition, whereas the non-reasoning\nmodel exhibited reactive parameter adjustment without explicit justification. The dis-\ntinction between reasoning and non-reasoning architectures has practical consequences\nfor both plan quality and interpretability. As reasoning models improve in capability\nand efficiency, their integration into radiation oncology workflows should be actively\nconsidered.\nReferences\n[1] Bresolin, A., Garibaldi, E., Faiella, A. et al. Predictors of 2-year incidence of patient-\nreported urinary incontinence after post-prostatectomy radiotherapy: Evidence of\ndose and fractionation effects. Front Oncol 10 (2020).\n14\n"}, {"page": 15, "text": "[2] Zaorsky, N. G., Harrison, A. S., Trabulsi, E. J. et al.\nEvolution of advanced\ntechnologies in prostate cancer radiotherapy. Nat Rev Urol 10, 565–579 (2013).\n[3] Meier, R. Stereotactic radiosurgery for brain metastases. Transl Cancer Res 3,\n358–366 (2014).\n[4] Das, I. J., Weisman, M., Bates, J. E. et al. The medical physicist workforce shortage:\nPutting the cart in front of the horse. Int J Radiat Oncol Biol Phys 0 (2025).\n[5] Baumgartner, T. J. et al. Perceptions of burnout in medical dosimetry within a\npostpandemic work environment. Medical Dosimetry 48, 77–81 (2023).\n[6] Hodgson, D. C., Charpentier, A. M., Cigsar, C. et al. A multi-institutional study\nof factors influencing the use of stereotactic radiosurgery for brain metastases. Int\nJ Radiat Oncol Biol Phys 85, 335–340 (2013).\n[7] Kann, B. H., Park, H. S., Johnson, S. B., Chiang, V. L. & Yu, J. B. Radiosurgery for\nbrain metastases: Changing practice patterns and disparities in the united states.\nJ Natl Compr Canc Netw 15, 1494–1502 (2017).\n[8] Court, L. E., Kisling, K., McCarroll, R. et al. Radiation planning assistant - a\nstreamlined, fully automated radiotherapy treatment planning system. J Vis Exp\n57411 (2018).\n[9] Cagni, E., Botti, A., Micera, R. et al. Knowledge-based treatment planning: An\ninter-technique and inter-system feasibility study for prostate cancer. Phys Med\n36, 38–45 (2017).\n[10] Mahmood, R., Babier, A., Mcniven, A. & Chan, T. C. Y. Automated treatment\nplanning in radiation therapy using generative adversarial networks. Proc Mach\nLearn Res 85, 1–15 (2018).\n[11] Rudin, C.\nStop explaining black box machine learning models for high stakes\ndecisions and use interpretable models instead. Nat Mach Intell 1, 206–215 (2019).\n[12] Court, L. E., Kisling, K., McCarroll, R. et al. Radiation planning assistant - a\nstreamlined, fully automated radiotherapy treatment planning system. J Vis Exp\n(2018).\n[13] Tol, J. P., Delaney, A. R., Dahele, M., Slotman, B. J. & Verbakel, W. F. A. R.\nEvaluation of a knowledge-based planning solution for head and neck cancer. Int\nJ Radiat Oncol Biol Phys 91, 612–620 (2015).\n[14] Tambe, N. S., Pires, I. M., Moore, C., Cawthorne, C. & Beavis, A. W. Validation\nof in-house knowledge-based planning model for advance-stage lung cancer patients\ntreated using VMAT radiotherapy. Br J Radiol 93, 20190535 (2020).\n[15] Chanyavanich, V., Das, S. K., Lee, W. R. & Lo, J. Y. Knowledge-based IMRT\ntreatment planning for prostate cancer. Med Phys 38, 2515–2522 (2011).\n15\n"}, {"page": 16, "text": "[16] Mahmood, R., Babier, A., Mcniven, A. & Chan, T. C. Y. Automated treatment\nplanning in radiation therapy using generative adversarial networks. Proc Mach\nLearn Res 85, 1–15 (2018).\n[17] Zarepisheh, M., Long, T., Li, N. et al. A DVH-guided IMRT optimization algorithm\nfor automatic treatment planning and adaptive radiotherapy replanning. Med Phys\n41 (2014).\n[18] Esposito, P. G., Castriconi, R., Mangili, P. et al. Knowledge-based automatic plan\noptimization for left-sided whole breast tomotherapy. Phys Imaging Radiat Oncol\n23, 54–59 (2022).\n[19] Shiraishi, S. & Moore, K. L. Knowledge-based prediction of three-dimensional dose\ndistributions for external beam radiotherapy. Med Phys 43, 378–387 (2016).\n[20] Ge, Y. & Wu, Q. J. Knowledge-based planning for intensity-modulated radiation\ntherapy: A review of data-driven approaches. Med Phys 46, 2760 (2019).\n[21] Li, C., Guo, Y., Feng, X., Xu, D. & Yang, R.\nDeep reinforcement learning in\nradiation therapy planning optimization: A comprehensive review. Physica Medica:\nEuropean Journal of Medical Physics (2025).\n[22] Shen, C., Nguyen, D., Chen, L. et al.\nOperating a treatment planning system\nusing a deep-reinforcement learning-based virtual treatment planner for prostate\ncancer intensity-modulated radiation therapy treatment planning. Med Phys 47,\n2329–2336 (2020).\n[23] Mahmood, R., Babier, A., Mcniven, A. & Chan, T. C. Y. Automated treatment\nplanning in radiation therapy using generative adversarial networks. Proc Mach\nLearn Res 85, 1–15 (2018).\n[24] United States of America, Food and Drug Administration (FDA). Software as a\nmedical device (SaMD) action plan (2021). URL www.fda.gov.\n[25] IMDRF AIMD Working Group. Machine learning-enabled medical devices-a subset\nof artificial intelligence-enabled medical devices: Key terms and definitions. Tech.\nRep., International Medical Device Regulators Forum (2021).\n[26] Mayo, C. S., Moran, J. M., Bosch, W. et al. American association of physicists in\nmedicine task group 263: Standardizing nomenclatures in radiation oncology. Int J\nRadiat Oncol Biol Phys 100, 1057–1066 (2018).\n[27] Smith, K., Blasi, O., Casey, D. et al. AAPM task group report 332: Verification\nof vendor-provided data, tools, and test procedures in radiotherapy. Med Phys 52,\n3509–3527 (2025).\n[28] Hurkmans, C., Bibault, J. E., Brock, K. K. et al. A joint ESTRO and AAPM\nguideline for development, clinical validation and reporting of artificial intelligence\nmodels in radiation therapy. Radiother Oncol 197 (2024).\n16\n"}, {"page": 17, "text": "[29] Zhai, H., Yang, X., Xue, J. et al. Radiation oncologists’ perceptions of adopting\nan artificial intelligence–assisted contouring technology: Model development and\nquestionnaire study. J Med Internet Res 23, e27122 (2021).\n[30] Wong, K., Gallant, F. & Szumacher, E. Perceptions of canadian radiation oncol-\nogists, radiation physicists, radiation therapists and radiation trainees about the\nimpact of artificial intelligence in radiation oncology – national survey.\nJ Med\nImaging Radiat Sci 52, 44–48 (2021).\n[31] Batumalai, V., Jameson, M. G., King, O. et al.\nCautiously optimistic: A sur-\nvey of radiation oncology professionals’ perceptions of automation in radiotherapy\nplanning. Tech Innov Patient Support Radiat Oncol 16, 58 (2020).\n[32] Liu, T. et al. Development of a RAG-based expert LLM for clinical support in\nradiation oncology. medRxiv (2025).\n[33] Oh, Y., Park, S., Byun, H. K. et al.\nLLM-driven multimodal target volume\ncontouring in radiation oncology. Nat Commun 15, 9186 (2024).\n[34] Yalamanchili, A., Sengupta, B., Song, J. et al. Quality of large language model\nresponses to radiation oncology patient care questions.\nJAMA Netw Open 7,\ne244630 (2024).\n[35] Nusrat, H., Luo, B., Hall, R. et al. Autonomous radiotherapy treatment planning\nusing DOLA: A privacy-preserving, LLM-based optimization agent (2025). URL\nhttps://arxiv.org/abs/2503.17553v1.\n[36] Li, Z. Z., Zhang, D., Zhang, M. L. et al. From system 1 to system 2: A survey of\nreasoning large language models (2025). URL https://arxiv.org/abs/2502.17419v6.\n[37] Sun, R. Can a cognitive architecture fundamentally enhance LLMs? or vice versa?\n(2024). URL https://arxiv.org/abs/2401.10444v1.\n[38] Xu, F. et al.\nAre large language models really good logical reasoners? a com-\nprehensive evaluation and beyond. IEEE Trans Knowl Data Eng 37, 1620–1634\n(2025).\n[39] Kahneman, D. Thinking, Fast and Slow (Farrar, Straus and Giroux, New York,\n2011).\n[40] Croskerry, P. Clinical cognition and diagnostic error: applications of a dual process\nmodel of reasoning. Adv Health Sci Educ Theory Pract 14 Suppl 1, 27–35 (2009).\n[41] Norman, G. R. et al. The causes of errors in clinical reasoning: Cognitive biases,\nknowledge deficits, and dual process thinking. Acad Med 92, 23–30 (2017).\n[42] Graber, M. L., Franklin, N. & Gordon, R. Diagnostic error in internal medicine.\nArch Intern Med 165, 1493–1499 (2005).\n17\n"}, {"page": 18, "text": "[43] Flin, R. et al.\nTeaching surgeons about non-technical skills.\nSurgeon 5, 86–89\n(2007).\n[44] Pelaccia, T., Tardif, J., Triby, E. & Charlin, B. An analysis of clinical reasoning\nthrough a recent and comprehensive approach: the dual-process theory. Med Educ\nOnline 16 (2011).\n[45] Charlin, B., Tardif, J. & Boshuizen, H. P. A. Scripts and medical diagnostic knowl-\nedge: theory and applications for clinical reasoning instruction and research. Acad\nMed 75, 182–190 (2000).\n[46] Grattafiori, A., Dubey, A., Jauhri, A. et al. The llama 3 herd of models (2024).\nURL https://arxiv.org/abs/2407.21783v3.\n[47] Qwen Team. Qwen3 technical report (2025). URL https://github.com/QwenLM/\nQwen3.\n[48] Chao, S. T., De Salles, A., Hayashi, M. et al.\nStereotactic radiosurgery in the\nmanagement of limited (1-4) brain metasteses: Systematic review and international\nstereotactic radiosurgery society practice guideline.\nNeurosurgery 83, 345–353\n(2018).\n[49] Cirino, E., Benedict, S. H., Dupre, P. J. et al. AAPM-RSS medical physics practice\nguideline 9.b: SRS-SBRT. J Appl Clin Med Phys 26, e14624 (2025).\n[50] Xiang, V., Snell, C., Gandhi, K. et al.\nTowards system 2 reasoning in LLMs:\nLearning how to think with meta chain-of-thought (2025). URL https://arxiv.org/\nabs/2501.04682v1.\n[51] Dimitriadis, A. & Paddick, I. A novel index for assessing treatment plan quality in\nstereotactic radiosurgery. J Neurosurg 129, 118–124 (2018).\n[52] Li, C., Wu, W., Zhang, H. et al. Imagine while reasoning in space: Multimodal\nvisualization-of-thought (2025). URL https://arxiv.org/abs/2501.07542v1.\n[53] Ji, B., Agrawal, S., Tang, Q. & Wu, Y. Enhancing spatial reasoning in vision-\nlanguage models via chain-of-thought prompting and reinforcement learning (2025).\nURL https://arxiv.org/abs/2507.13362v1.\n[54] Daxberger, E., Wenzel, N., Griffiths, D. et al.\nMM-Spatial: Exploring 3D spa-\ntial understanding in multimodal LLMs (2025). URL https://arxiv.org/abs/2503.\n13111v2.\n18\n"}]}