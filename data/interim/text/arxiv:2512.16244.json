{"doc_id": "arxiv:2512.16244", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.16244.pdf", "meta": {"doc_id": "arxiv:2512.16244", "source": "arxiv", "arxiv_id": "2512.16244", "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models", "authors": ["Xueqi Ma", "Xingjun Ma", "Sarah Monazam Erfani", "Danilo Mandic", "James Bailey"], "published": "2025-12-18T06:50:13Z", "updated": "2025-12-21T10:28:24Z", "summary": "Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.16244v2", "url_pdf": "https://arxiv.org/pdf/2512.16244.pdf", "meta_path": "data/raw/arxiv/meta/2512.16244.json", "sha256": "f2d906ba70e37fc5613b4a02815a299d58bdecc2ee3555feae68062a084eb3a8", "status": "ok", "fetched_at": "2026-02-18T02:24:08.271176+00:00"}, "pages": [{"page": 1, "text": "Coarse-to-Fine Open-Set Graph Node Classification\nwith Large Language Models\nXueqi Ma1, Xingjun Ma2, Sarah Monazam Erfani1, Danilo Mandic3, James Bailey1\n1The University of Melbourne, Australia\n2 Fudan University, China\n3 Imperial College London, UK\nAbstract\nDeveloping open-set classification methods capable of clas-\nsifying in-distribution (ID) data while detecting out-of-\ndistribution (OOD) samples is essential for deploying graph\nneural networks (GNNs) in open-world scenarios. Existing\nmethods typically treat all OOD samples as a single class,\ndespite real-world applications—especially high-stake set-\ntings like fraud detection and medical diagnosis—demanding\ndeeper insights into OOD samples, including their probable\nlabels. This raises a critical question: Can OOD detection be\nextended to OOD classification without true label informa-\ntion? To answer this question, we introduce a Coarse-to-Fine\nopen-set Classification (CFC) method that leverages large lan-\nguage models (LLMs) for graph datasets. CFC consists of\nthree key components: 1) A coarse classifier that utilizes LLM\nprompts for OOD detection and outlier label generation; 2)\nA GNN-based fine classifier trained with OOD samples from\ncoarse classifier for enhanced OOD detection and ID classifi-\ncation; and 3) Refined OOD classification achieved through\nLLM prompts and post-processed OOD labels. Unlike meth-\nods relying on synthetic or auxiliary OOD samples, CFC em-\nploys semantic OOD data-instances that are genuinely out-of-\ndistribution based on their inherent meaning, thus improving\ninterpretability and practical utility. CFC enhances OOD de-\ntection by 10% compared to state-of-the-art approaches on\ngraph and text domain, while achieving up to 70% accuracy\nin OOD classification on graph datasets. The code is available\nat https://github.com/sihuo-design/CFC.\nIntroduction\nGraph neural networks (GNNs) have demonstrated excel-\nlent performance in closed-set scenarios, where the train\nand test datasets share the same distributions. However, in\nmany real-world applications, models are deployed on data\ncontaining previously unseen classes. Traditional GNN meth-\nods (Kipf and Welling 2017; Hamilton, Ying, and Leskovec\n2017; Veliˇckovi´c et al. 2018) typically classify all unlabeled\nnodes into known classes, failing to identify nodes that belong\nto unknown classes, which degrades overall model perfor-\nmance. Addressing this limitation requires the development\nof models that can accurately classify in-distribution (ID)\nsamples from known classes while effectively rejecting out-\nof-distribution (OOD) samples from unknown classes. This\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nClass 1\nClass 2\nClass 3\nClass 4\nClass n\nClass 1\nClass 2\nClass 3\nClass 4\nClass n\n(a) Subspace without\nSemantic OOD Information\n(b) Subspace with\nSemantic OOD Information\nFigure 1: Comparison of subspaces of methods without se-\nmantic OOD information and our proposed CFC, which incor-\nporates such information. Blue regions denote ID subspaces;\nother regions show OOD subspaces. CFC provides larger\nembedding space (pink), enabling direct OOD identification.\ncritical challenge is commonly referred to as the open-set\nclassification problem.\nRecent approaches (Hendrycks and Gimpel 2016; Song\nand Wang 2022) to open-set node classification problem (Wu,\nPan, and Zhu 2021) have employed thresholding methods,\nusing the maximum output probability as a confidence score\nto distinguish OOD samples from ID ones. While intuitive,\ndetermining an optimal threshold (Yang, Lu, and Gan 2023)\nto separate unknown from known classes is both challeng-\ning and time-consuming (Perera et al. 2020). Another line\nof research redefines open-set classification as a closed-set\nproblem by estimating the distribution of unknown classes\nand adjusting the network’s confidence. For example, meth-\nods such as (Ge et al. 2017; Neal et al. 2018; Perera et al.\n2020; Zhou, Ye, and Zhan 2021) generate synthetic samples\nas OOD, while others, like (Hendrycks, Mazeika, and Diet-\nterich 2018; Wang et al. 2023), incorporate auxiliary training\ndata—referred to as outlier exposure—to train image clas-\nsifiers for OOD detection. Building on these works, Zhang\net al. (2023) proposed generating proxy unknown nodes to\nsimulate open-set data for graph node OOD detection.\nWhile these approaches have mitigated the problem of\nOOD detection to some extent, they encounter several chal-\nlenges, as follow as: i) To enable OOD detection, they require\nthe use of a large number of synthetic/auxiliary OOD sam-\nples during training, imposing significant computation cost.\nii) Using generated samples or auxiliary training data may\nnot accurately reflect real-world OOD variations. As such,\narXiv:2512.16244v2  [cs.LG]  21 Dec 2025\n"}, {"page": 2, "text": "these approaches may lack true semantic understanding and\nrisk overfitting to specific datasets pairs. iii) Without seman-\ntic OOD samples that are realistic and meaningful, they fail\nto accurately represent the true OOD space, resulting in a\nsmall subspace and sharp boundaries for OOD detection, as\nillustrated in Figure 1 (a). iv) More critically, these methods\noften group multiple unknown classes into a single OOD cat-\negory, which significantly reduces their utility. In real-world\nscenarios, distinguishing between various unknown classes\nis crucial for informed decision-making, efficient data uti-\nlization, and performance in high-stakes applications such as\nmedical diagnosis, autonomous driving, and fraud detection.\nFor instance, in financial networks, grouping diverse fraudu-\nlent behaviors—such as phishing attacks, insider threats, and\nmoney-laundering schemes—into a single unknown category,\noversimplifies their distinctions, hindering the nuanced un-\nderstanding required for effective mitigation. This challenge\nraises a fundamental question: Can we develop a comprehen-\nsive classification approach that seamlessly classifies both\nknown and unknown classes, without requiring labeled sam-\nples for OODs?\nAccurate OOD classification poses significant challenges\ndue to the uncertainty surrounding potential OOD labels.\nSpecifically, the OOD domains—whether proximal or distant\nfrom the ID domains—are unknown, and even more so the\nnumber of unknown classes. To overcome this, we map the\ngraph into text space and propose a Coarse-to-Fine Classifi-\ncation (CFC) approach to explore the OOD label space. In\nthe first step, leveraging the expert knowledge and reason-\ning capabilities of LLMs, we design a coarse classifier by\ncreating LLM prompts to detect OOD samples on the test\nset without prior OOD information and generate potential\noutlier class labels. The identified OOD data from the coarse\nclassifier provides a potential OOD space with semantic in-\nformation (i.e., possible meaningful and real-world outlier\nclasses, such as topics of papers, news domains, etc.). Using\nthese noisy coarse OOD data, we proceed to the second step:\nconstructing a GNN-based fine classifier to further detect\nOOD samples and perform ID classification. More specifi-\ncally, we use a label propagation method to remove falsely\nidentified OOD samples, and an improved manifold mixup\nmethod (Verma et al. 2019) for OOD data augmentation,\nenabling us to effectively predict the distribution of novel\nclasses. Unlike approaches that rely on additional synthetic\nor auxiliary data for training, CFC captures semantic OOD\nsamples, reducing the distribution discrepancy (Wang et al.\n2023) between the training data and real OOD data. As a re-\nsult, CFC constructs a larger OOD subspace with a smoother\nboundary for OOD detection, as shown in Figure 1 (b). An\nempirical example demonstrating clearer boundaries with\nmore semantic OOD samples is provided in Appendix. The\nadvantages of semantic OOD are further demonstrated by\nthe improved OOD detection performance achieved with a\nsmall number of semantic OOD samples. Finally, we achieve\nOOD classification using LLM prompts designed with a post-\nprocessed OOD label space. It is important to note that we do\nnot have any true OOD label information. Our contributions\ncan be summarized as follows:\n• Recognizing the critical need for accurately distinguish-\ning between various unknown classes to ensure safety and\nenable effective decision-making in unpredictable environ-\nments, we introduce a novel challenge in the open-world\nsetting: OOD classification on graphs. This task involves\nnot only detecting OOD samples but also classifying them\ninto their respective unknown classes, thereby extending\nthe scope of traditional OOD detection.\n• We propose a general coarse-to-fine classification method\nthat integrates semantic OOD samples and a potential\nOOD label space, enabling the model to effectively per-\nform both ID and OOD classification for the open-set\ngraph node classification problem.\n• Our CFC method demonstrates strong performance,\nachieving up to a 70% improvement in graph OOD classi-\nfication and a 10% improvement in OOD detection com-\npared to baseline methods. Furthermore, the proposed\nCFC framework offers a flexible and effective open-set\nclassification solution that can be easily applied to text\ndata.\nRelated Works\nOpen-set classification, which identifies unknown classes\nwhile classifying known classes, has been widely studied in\nimages and text. Representative OOD detection methods can\nbe broadly categorized into post-hoc detection (Hendrycks\nand Gimpel 2016; Liu et al. 2020; Park, Jung, and Teoh\n2023), generative model-based approaches (Cai and Li 2023;\nKirichenko, Izmailov, and Wilson 2020; Nalisnick et al.\n2018; Neal et al. 2018), and outlier exposure techniques\n(Hendrycks, Mazeika, and Dietterich 2018; Wang et al. 2023;\nHu and Khan 2021). Post-hoc models employ various scoring\nfunctions (Liang, Li, and Srikant 2017; Sun and Li 2022; Zhu\net al. 2023; Liu et al. 2020; Huang, Geng, and Li 2021) to\nidentify OOD samples, but struggle when test label spaces\ndiffer from training, often requiring costly retraining. Gen-\nerative methods and outlier exposure approaches attempt to\nincorporate synthetic OOD samples or auxiliary data to train\nmodels for OOD detection. However, these generated OOD\nsamples or auxiliary data often fail to accurately represent\ntrue OOD samples, limiting their effectiveness.\nRecently, there has been growing interest in graph open-\nset classification (Wu, Pan, and Zhu 2020; Um et al. 2025;\nChen et al. 2025; Ma et al. 2024). Many classification-based\nmethods (Song and Wang 2022; Yang, Lu, and Gan 2023;\nWu et al. 2023; Ma et al. 2024) have been proposed to utilize\nstructural information for ID classification and OOD detec-\ntion. However, these methods are often time-consuming and\nlack flexibility. To improve efficiency, several works (Gong\nand Sun 2024; Yang et al. 2025; Wu et al. 2023; Wang et al.\n2025) employ energy-based propagation schemes to detect\nOOD samples. Additionally, Zhang et al. (2023) attempted\nto generate synthetic samples to approximate the OOD space.\nHowever, the generated sample distribution often fails to\naccurately reflect the true OOD distribution, causing these\nmethods to struggle with identifying challenging OOD sam-\nples. Importantly, existing open-set classification methods,\nincluding node-level (Bao et al. 2024; Gong and Sun 2024;\n"}, {"page": 3, "text": "Zhang et al. 2024b,a) and graph-level (Yin et al. 2024; Shen\net al. 2024; Guo et al. 2023b; Liu et al. 2023b), typically\nidentify multiple unknown classes as one OOD label. In this\npaper, we propose a more challenging problem: OOD classi-\nfication, which involves identifying multiple classes. More\nrelated works on LLMs for text-attributed graphs can be\nfound in Appendix.\nProblem Definition and Preliminaries\nWe study open-set node classification in graphs. Given a\ngraph G = (V, E) with node set V and edge set E, let\n|V| = N. Each node vi ∈V has a feature vector xi ∈Rd,\nforming a feature matrix X ∈RN×d, and a label vector\nyi ∈{0, 1}C, where C is the number of ID classes. Node con-\nnections are represented by the adjacency matrix A, where\nAij = 1 if (vi, vj) ∈E, otherwise Aij = 0. We consider\nthat the full node set V is partitioned into training set Vtrain,\nvalidation set Vval, and test set Vtest. In a typical closed-set\nnode classification task on graph G, with an ID label space\nY = {1, . . . , C}, GNN models predict each node in the test\nset with a certain ID class in Y.\nOOD detection in open-set node classification problem.\nIn open-world scenarios, the test set may contain unknown\nclass nodes whose labels fall outside the ID label space. Given\na set of ID training samples T = {(x1, y1) , . . . , (xn, yn)},\nthe goal of OOD detection is to learn a (C + 1)-class classi-\nfier fC+1 using T. This classifier should be capable of: (1)\nclassifying ID samples into their respective C ID classes, and\n(2) identifying OOD samples as belonging to a single OOD\nclass.\nOOD classification in open-set node classification prob-\nlem. In this paper, we extend the challenge from OOD detec-\ntion to OOD classification, formulating it as a comprehensive\nopen-set classification problem. Specifically, we aim to learn\na (C + u)-class classifier fC+u using T to classify (1) ID\nsamples into the corresponding C ID classes, and (2) OOD\nsamples into u distinct OOD classes. Notably, u is not prede-\nfined in the open-set scenario.\nMethods\nTo address the challenges of OOD classification, we must\ntackle two critical questions: (1) How can we approximate\nthe OOD space without labeled information? (2) How can\nwe derive meaningful outlier class labels?\nIn this paper, we propose a Coarse-to-Fine open-set Classi-\nfication (CFC) framework to progressively achieve advanced\nOOD classification. First, we design LLM-based prompts\nspecifically tailored for coarse-grained graph node OOD iden-\ntification. In this step, we leverage the expert knowledge and\nreasoning capabilities of LLMs to detect OOD samples rele-\nvant to the test domain and generate a candidate OOD label\nspace. Next, based on the semantic OOD samples identified\nby the LLM, we introduce a GNN-based fine-grained clas-\nsifier for ID classification and precise OOD detection. This\nstep enhances granularity by denoising and OOD data aug-\nmentation. Furthermore, we provide a theoretical analysis\ndemonstrating the benefits of integrating semantic OOD sam-\nples and the refined augmentation method, which expand\nthe OOD subspace and smooth decision boundaries. Finally,\nwe conduct OOD classification by employing LLM prompts\nwith the refined OOD label space.\nA Coarse-Classifier with LLMs\nLarge Language Models, with their extensive knowledge,\nhave demonstrated impressive zero-shot and few-shot ca-\npabilities, particularly for node classification tasks on text-\nattributed graphs (TAGs) (Chen et al. 2023; Guo et al. 2023a;\nChen et al. 2024a), where each node and edge in the graph\nis associated with a text sentence. In an open-set setting,\nwe explore the OOD space by leveraging the capabilities of\nLLMs. Using ID labels from the training set, the LLM is\nemployed to predict whether the label of a test node belongs\nto the provided ID label space, acting as a binary classifier to\ndifferentiate between ID and OOD samples.\nAccording to ID label space, we categorize identification\ntasks into two types: Easy-Reject and Hard-Reject, as de-\nscribed below. We then elaborate on the corresponding LLM\nprompts designed to facilitate confidence-aware OOD identi-\nfication and to generate the potential OOD label space.\nEasy-Reject. This refers to scenarios where the ID classes\nin the label space contain a small proportion of their respec-\ntive major categories, making it easier to reject ID samples\nas OOD class. Building on the existing ID label space, we\nprompt the LLM to determine whether the label of the input\ntest node belongs to the provided ID classes using confidence-\naware prompts (Chen et al. 2023). The confidence score\nassociated with this identification is essential, as LLM anno-\ntations, similar to human annotations, can exhibit a degree\nof label noise. This confidence score helps assess the quality\nof detection and filter out noisy labels. Since these samples\nare likely to be rejected as OOD, we design the LLM prompt\nwith a restriction: annotate samples as OOD only when the\nLLM is highly confident. If the test node is identified as an ID\nsample, we prompt the LLM to provide its category within\nthe specified ID label space. Otherwise, the LLM will offer\nan outlier class label beyond the ID label space. Finally, we\nobtain the label (ID or OOD), the LLM’s confidence score,\nand the category for each test sample, as illustrated in Figure\n2.\nHard-Reject.\nThis refers to cases where the ID classes in\nthe label space encompass a large proportion of their respec-\ntive main categories, making it easier to accept OOD samples\nas ID ones. Building upon the existing ID label space, we first\nguide the LLM to summarize these classes and identify their\nrespective major categories. Next, we prompt the LLM to\nprovide possible outlier class labels that fall within the major\ncategories but are not included in the ID class labels, creating\na candidate OOD label space. Subsequently, input with the\ncandidate OOD classes, we use the LLM to determine the\nlabel of the input test node, generate a confidence score, and\nprovide a predicted category (as illustrated in Figure 2).\nIn general, Easy-Reject is used for small coverage and\nfar-OOD cases, while Hard-Reject is used for large coverage\nand near-OOD cases.\n"}, {"page": 4, "text": "Q:\nPaper: [text]\nTask: There are following categories: [ID label space].\nIs the topic of this paper in the category list? Provide\nyour answer and a confidence number between [0-1].\nChoose False only if you are very certain that the\npaper does not belong to any of the listed categories.\nIf True, specify which category in [ID label space]\nthe paper belongs to. If False, provide a suggested\ncategory that is not in the category list.\n[{\"answer:\":<True\nor\nFalse>,\n\"confidence\":\n<confidence_here>, \"category\": <category_here>}]’\nA: [{\"answer\": , \"confidence\":  , \"category\": }]\nEasy-Reject Prompt        \nHard-Reject  Prompt\nQ:\nTask: There are following listed Paper topics:\n[ID label space]. Which major category do\nthese themes belong to?\nOutput : [{\"answer\": <your_answer>}]\nA: [{\"answer\": [Major Category]}]\nA: [{\"answer:\": [candidate OOD label space]}]\nQ:\nGenerate 10 possible paper topics that belong\nto [Major Category] but distinct from the\nprovided topics [ID label space].\nOutput\n:\n[{\"answer\":\n<your_answer>},\n{\"answer\": <your_answer>}, ...]\nA: [{\"answer:\": , \"confidence\":  , \"category\": }]\nQ:\nPaper: [text]\nTask: There are following categories: [ID label space].\nIs the topic of this paper in the category list?\nIf True, specify which category in [ID label space]\nthe paper belongs to.\nIf False, provide a suggested category that is not in\nthe category list. The suggested category includes but\nnot limited to the following: [candidate OOD label\nspace, ...].\nProvide your answer, a confidence number between [0-\n1],\nand\nsuggested\ncategory.\n[{\"answer:\":<True\nor\nFalse>, \"confidence\": <confidence_here>, \"category\":\n<category_here>}\nFigure 2: LLM prompts for Easy-Reject and Hard-Reject OOD detection include both Q(uestion) and A(nswer) contents. The\ninputs are [text] (describing the graph node) and [ID label space] (a list of ID categories, e.g., [machine learning, neural networks,\n...]). For Hard-Reject OOD detection, we first determine the [Major Category] of ID classes and the [candidate OOD label space],\nthen use [text], [ID label space], and [candidate OOD label space] for OOD detection and category generation.\nGNN-based Fine-Classification\nAssume we have obtained a coarse-grained OOD set Vood\nthrough LLM-based OOD detection. The samples in Vood\nare semantic OODs with potential categories, and are closely\naligned with the true test OOD space, providing a more rep-\nresentative and structured foundation for OOD detection.\nWe further construct a GNN-based classifier with (C +\n1) labels to perform ID classification and OOD detection.\nGiven that the OOD samples identified by the LLM may\ncontain some noise (i.e., misclassified ID samples) or be\ninsufficient in number, we employ a label propagation method\nfor denoising and utilize an improved mixup method (Verma\net al. 2019; Han et al. 2022) for OOD data augmentation.\nDenoising.\nWe correct falsely identified OOD samples\nfrom LLM-based OOD detection using a label propaga-\ntion method. We assume the initial label matrix Yl(0) =\n[yl(0)\n1\n, yl(0)\n2\n, · · · , yl(0)\nN ] consists of one-hot label indicator vec-\ntors for ID training nodes in Vtrain and OOD nodes in Vood,\nwhile having zero vectors for unlabeled nodes. By propagat-\ning the labels with the normalized adjacency D−1A, the kth\niteration of label propagation (Zhu 2005; Wang and Leskovec\n2020) is formulated as Yl(k) = D−1AYl(k−1). At each iter-\nation, the ID training samples are reset to their initial labels:\nyl(k)\ni\n= yl(0)\ni\n, ∀i ∈Vtrain. This is to maintain the label infor-\nmation of the ID training nodes so that the other nodes do\nnot overpower the original labeled ones, as the initial labels\nwould otherwise fade away.\nAfter K-order label propagation, we can obtain the label\nmatrix Y. For candidate OOD samples, their labels are up-\ndated using the maximum probability in Y K. We discard\nOOD samples that are predicted as ID in Vood, and achieve\nnew OOD set V\n′\nood.\nOOD Data Augmentation.\nWe consider the practical case\nin which LLM just identifies a small number of samples as\nOOD. Having a sufficient number of semantic OOD samples\nis crucial for representing the OOD space and improving\nopen-set classification. How to obtain more stable OOD sam-\nples? Manifold mixup (Verma et al. 2019), as a data augmen-\ntation method, has been theoretically and empirically shown\nto improve the generalization and robustness of deep neural\nnetworks for images, by training neural networks on linear\ncombinations of hidden representations of training examples.\nIn this work, we extend manifold mixup to augment OOD\ndata for improved performance.\nFor a well-trained classifier, features of nodes that belong\nto the same classes are close to each other, while those from\ndifferent classes are distant. Ideally, clear boundaries separate\nthe different classes. Since nodes whose features are close\nto the boundaries are more likely to be less representative\nto their own classes, we generate OOD samples using nodes\nnear the boundary regions.\nWe collect K nodes with low classification confidence in\nthe training set. Then, the manifold mixup is applied on these\nnear boundary nodes and the center of the OOD samples in\nV\n′\nood as\n\u001a\n˜xi = αhk\ni + (1 −α)hk\nc, i ≤K\n˜yi = C + 1\n(1)\nwhere hk = GNN\n\u0010\nA, hk−1\u0011\nis the hidden embedding with\na GNN encoder, hk\nc is the center embedding of OOD samples,\nα > 0 is a hyperparameter to control the distance between\nthe generated samples and the existing OOD samples. Finally,\nwe obtain an augmented OOD set Va\nood = {V\n′\nood, Va} where\nVa is the generated OOD set.\nID Classification and OOD Detection.\nWe train a GNN-\nbased classifier fC+1 on training set Vtrain and the augmented\nOOD set Va\nood for ID classification and OOD detection. Tak-\ning GCN as an example, a two-layer GCN model can be\nformulated as\nZ = softmax\n\u0010\nˆA ReLU( ˆAXW(0))W(1)\u0011\n,\n(2)\nwhere, Z\nis the GCN’s output predictions,\nˆA\n=\nˆD−1\n2 (A + In) ˆD−1\n2 is the normalized A+In matrix by the\ndegree matrix ˆD, and W = (W(0), W(1)) are the weights\nof the two-layer GCN model. For graph node classification,\nthe objective function L is\nL = −\n1\n|Vtrain ∪Va\nood|\nX\nvi∈Vtrain∪Va\nood\ny⊤\ni log(zi),\n(3)\n"}, {"page": 5, "text": "Figure 3: LLM prompts with [text] and [post-OOD label\nspace] for OOD classification.\nwhere yi and zi are the label and prediction of node vi. We\nuse the trained GNN-based open-set classifier to predict test\nlabels and obtain the final predicted OOD set, denoted as\nVf\nood.\nOOD Classification\nAfter detecting the OOD samples, we proceed with OOD\nclassification for the nodes in Vf\nood, by leveraging the potential\nOOD label space (consisting of the possible OOD categories)\ndiscussed in subsection Coarse-Classifier with LLMs.\nWe employ similarity measures (such as word-level or\nsemantic-level comparisons, e.g., TF-IDF (Ramos et al.\n2003)) to merge similar categories and filter out categories\nwith too few samples. After post-processing, we obtain a set\nof OOD categories, {l1, l2, · · · , lu}, forming the post-OOD\nlabel space. We then use LLMs to generate annotations for\nthe OOD samples in Vf\nood based on this post-OOD label space,\nas illustrated in Figure 3.\nTheoretical Analysis\nIn CFC, we integrate augmented semantic OOD samples by\nproposing a method to mix the hidden embeddings of samples\nfrom ID classes and coarse OOD samples obtained through\ncoarse-grained OOD detection. We theoretically demonstrate\nthe advantages of this approach in extending and flattening\nthe OOD subspace (as shown in Figure 1 (b)), which leads to\nimproved OOD detection.\nDefinition 3.1 (Space Dimension). Given a hypothesis\nspace H, the space dimension is the rank of the matrix formed\nby the set of vectors that span the subspace.\nAssumption 3.1 (Feature Space Dimension). Given a fea-\nture space X ⊂Rd, assume that the features of X belong to\nC distinct classes. Then, the dimension of the feature space\nis d −C.\nTheorem 3.1 Given an open-set classification task with\nan ID label space Yid = {y1, y2, . . . , yC} and an OOD label\nyood, assume the existence of the ID feature space H and the\nOOD space H\n′ related to practical test domain. The proposed\nCFC, trained on both ID samples and semantic OOD samples\nrelated to the OOD space H\n′, forms a larger subspace with\ndimension dim(H + H\n′) −(C + 1), compared to general\nmethods that lie within dim(H) −(C + 1). Consequently,\nCFC results in a smoother and flatter decision boundary for\nOOD detection.\nA theoretical analysis of how embedding mixup further\nexpands the representation subspace and influences the clas-\nsifier’s decision boundary, and the proof of Theorem 3.1, is\nprovided in Appendix.\nExperiments\nIn this section, we evaluate the performance of our proposed\nCFC method for graph node classification in an open-set set-\nting by investigating the following questions: Q1. How does\nthe performance of CFC compare to other classification meth-\nods? Q2. What is the effect of different prompts and LLMs\non coarse-grained OOD detection? Q3. How do denoising\nand OOD data augmentation in fine-grained OOD detection\naffect the performance of CFC? Q4. What is the impact of\nthe potential OOD label space on OOD classification? Addi-\ntionally, we demonstrate that the proposed CFC framework is\nversatile and can be extended to the text domain in Appendix.\nExperimental Settings.\nIn this paper, we utilize widely\nused graph datasets from different domains—textual graphs\n(Cora (McCallum et al. 2000), Citeseer (Giles, Bollacker, and\nLawrence 1998), WikiCS (Mernyei and Cangea 2020), and\nDBLP (Ji et al. 2010)) and non-textual graphs (Amazon-\nComputer and Amazon-Photo (Ni, Li, and McAuley\n2019))—for open-set node classification. The statistics for\nthese datasets are presented in Appendix. For each dataset,\nmultiple classes are designated as out-of-distribution (OOD)\nclasses (i.e., u >= 2), while the remaining classes are con-\nsidered in-distribution (ID) classes. Further details for the\ncase with u = 2 and u = 3 can be found in the Appendix.\nFor the ID classes, 50% of the nodes are sampled for training.\nThe remaining ID samples and all OOD samples are split as\n40% for validation and 60% for testing.\nFor all datasets, we adopt the text-attributed graph ver-\nsions from (Chen et al. 2023; Liu et al. 2023a; Chen et al.\n2024b). We use four popular large language models (LLMs),\nincluding e5-large-v2 (e5, (Wang et al. 2022b)), Sentence\nTransformer (ST, (Reimers 2019)), Llama2-7b and Llama2-\n13b (Touvron et al. 2023), to generate embeddings as original\ninput features. We utilize GPT-4o (Achiam et al. 2023) for\ndetecting OOD samples and generating potential outlier class\nlabels for coarse-grained OOD identification, as well as for\nthe final OOD classification. In the coarse classifier with\nLLM, we apply a confidence threshold of 0.7 to identify\nOOD samples, using Easy-Reject for Cora, DBLP, and Wi-\nkiCS, and Hard-Reject for Citeseer, Computer, and Photo. In\nfine-grained classification, we generate 100 OOD samples\nfor text datasets, and over 2000 for Computers and Photo\ndatasets using improved manifold mixup. The hyperparam-\neter analysis of α used to balance ID features and OOD\nfeatures in Eq.(1) can be found in Appendix. We compare\nour method with popular closed-set classification methods\nand state-of-the-art open-set classification methods, includ-\ning GCN_Poser (Zhou, Ye, and Zhan 2021), G2Pxy (Zhang\net al. 2023), GNNSafe (Wu et al. 2023), NodeSafe (Yang\net al. 2025), and GOLD (Wang et al. 2025).\n"}, {"page": 6, "text": "Table 1: Comparison of different methods for ID classification and OOD detection across four datasets with two OOD classes.\nNote that CFC (w/o D/M) refers to the CFC framework without the Denoising and Manifold Mixup data augmentation techniques.\nCFC uses GCN as the backbone for fine classification. (mean accuracy (%) and standard deviation over 5 different runs).\nMethods\nCora\nCiteseer\nWikiCS\nDBLP\nID\nOOD\noverall\nID\nOOD\noverall\nID\nOOD\noverall\nID\nOOD\noverall\nGCN_softmax\n90.25±0.85\n0.0\n62.76±0.72\n76.15±2.25\n0.00\n38.60±1.18\n73.67±0.56\n0.0\n53.01±0.32\n91.11±1.34\n0.0\n56.62±0.82\nGCN_sigmoid\n90.64±0.54\n0.0\n63.03±0.53\n76.07±1.17\n0.00\n38.56±0.58\n60.18±1.17\n0.00\n43.30±0.68\n91.54±0.58\n0.00\n56.89±0.50\nGCN_softmax_τ\n81.13±2.81\n66.98±9.87\n76.84±1.36\n57.64±4.27\n81.97\n69.60±1.21\n41.91±7.75\n58.84±12.31\n46.68±2.75\n79.15±4.52\n45.14±6.59\n66.28±0.73\nGCN_sigmoid_τ\n85.17±2.18\n62.18±5.30\n78.16±1.02\n67.52±1.46\n75.41±2.31\n71.41±1.01\n54.13±0.64\n67.03±1.66\n57.74±0.67\n71.11±7.32\n61.54±7.92\n66.31±1.25\nGCN_PROSER\n84.21±0.73\n71.18±1.75\n80.65±0.73\n71.25±5.15\n76.01±3.65\n72.74±1.57\n48.13±0.99\n44.19±1.63\n46.74±0.64\n63.01±10.88\n68.75±14.98\n65.18±1.27\nG2Pxy\n85.65±1.06\n72.46±1.58\n81.63±0.77\n71.52±1.43\n77.30±2.63\n74.36±1.30\n58.40±0.84\n57.09±1.50\n58.03±0.45\n65.88±1.03\n62.63±0.98\n64.65±1.23\nGNNSafe\n79.06±1.53\n62.92±9.28\n74.14±3.11\n71.43±1.37\n36.18±5.08\n53.64±2.76\n83.56±2.85\n73.29±13.91\n79.59±4.21\n93.85±0.47\n47.25±2.34\n76.21±0.55\nNodeSafe\n87.93±1.13\n80.63±4.76\n85.71±0.82\n73.97±1.88\n53.81±4.43\n64.03±1.99\n86.66±0.42\n42.99±3.31\n70.03±1.51\n94.52±0.28\n42.50±3.33\n74.83±1.10\nGOLD\n87.48±1.07\n66.54±2.84\n81.11±1.35\n70.33±1.44\n37.75±6.39\n53.26±3.02\n73.60±1.93\n32.83±14.98\n58.12±6.68\n94.05±0.41\n43.68±1.44\n74.98±0.61\nGTP-4o\n72.23\n58.08\n68.62\n46.82\n48.18\n47.50\n67.33\n67.65\n67.43\n84.26\n56.78\n66.11\nCFC (wo / D/M)\n85.44±0.20\n94.50±0.31\n88.20±0.20\n76.59±2.12\n72.92±2.63\n74.77±0.53\n79.18±0.13\n81.76±0.16\n79.91±0.10\n75.18±2.84\n87.62±1.11\n83.40±0.27\nCFC (wo / M)\n88.63±0.53\n91.75±1.57\n89.58±0.35\n79.29±2.10\n67.49±1.50\n73.44±0.77\n80.19±1.04\n77.13±0.68\n79.32±0.65\n76.25±3.76\n85.00±1.61\n82.03±0.38\nCFC (wo / D)\n86.23±0.64\n94.91±0.85\n88.87±2.15\n72.40±1.52\n83.00±1.66\n77.65±0.86\n75.15±0.24\n89.95±0.21\n79.34±0.19\n75.37±1.79\n88.37±0.29\n83.96±0.47\nCFC\n87.49±0.80\n95.74±0.67\n90.00±0.37\n73.92±0.97\n80.57±1.69\n77.21±0.68\n80.19±0.73\n81.89±0.53\n80.44±0.10\n78.47±1.01\n86.89±0.71\n84.03±0.15\nComparison With Other Node Classification\nMethods\nWe conducted two tasks: the traditional open-set graph node\nclassification (ID classification and OOD detection) and the\nnewly proposed OOD classification (mean accuracy (%) and\nstandard deviation over 5 different runs).\nOOD Detection\nTable 1 presents a comparison of closed-\nset and recent state-of-the-art open-set graph node classi-\nfication methods for ID classification and OOD detection\nacross four text-attributed graph datasets. Here, CFC adopts\ne5-large-v2 as the feature encoder. We observe that the pro-\nposed CFC method outperforms all other baselines across\nall datasets by significantly large margins. Specifically, CFC\nachieves over or around a 10% improvement over the second-\nbest in terms of overall accuracy on Cora, WikiCS, and DBLP.\nFurthermore, even without denoising and OOD data augmen-\ntation, CFC (wo / D/M) delivers comparable or better results\non all datasets compared to other baselines. This suggests\nthat incorporating semantic OOD information related to true\nOOD domain during training benefits the model, which is a\nreasonable outcome. Additionally, we observe that the LLM\n(GPT-4o) can recognize only about half of the OOD sam-\nples when provided with the ID label space in most cases,\nmaking it unsuitable for high-stakes applications. We report\nthe experimental results of various graph node classification\nmethods under the scenario of three OOD classes in each\ndataset in Appendix. Since DBLP has only four classes, we\nonly report results on the other three datasets. Our proposed\nCFC outperforms the other baselines by a significant margin\n(over 10% overall accuracy) across all datasets. The effective-\nness of CFC on non-textual graph datasets is demonstrated\nin Table 2.\nTable 11 in Appendix demonstrates the robustness of CFC\nunder limited ID data. Table 12 in Appendix compares dif-\nferent LLM feature encoders. Overall, e5-large-v2 performs\nbetter and more stably in joint training. Additionally, LLMs\nspecialize in different topics—for instance, Llama2 excels\nin ID topics for Citeseer, while e5 and ST perform well in\nOOD.\nTable 1 provides detailed classification accuracy for both\nknown (ID) and unknown (OOD) classes. While ID classi-\nTable 2: Comparison of different methods for ID classifica-\ntion and OOD detection across two nontextual datasets with\ntwo OOD classes.\nMethods\nAmazon-Computer\nAmazon-Photo\nID\nOOD\noverall\nID\nOOD\noverall\nGCN_softmax\n81.87±1.77\n0.0\n41.98±0.90\n82.46±0.45\n0.0\n70.03±0.38\nGCN_sigmoid\n73.85±0.55\n0.0\n37.51±0.28\n67.55±0.64\n0.0\n57.37±0.54\nGCN_softmax_τ\n81.84±1.78\n0.08±0.08\n41.60±0.90\n82.43±0.44\n0.60±0.57\n70.10±0.36\nGCN_sigmoid_τ\n14.15±12.77\n93.11±6.60\n52.33±3.39\n41.20±1.39\n86.66±3.22\n47.92±1.60\nGNNSafe\n70.62±1.21\n42.66±4.21\n56.86±1.01\n69.10±0.98\n12.85±5.68\n60.62±2.54\nNodeSafe\n86.73±0.52\n66.64±2.47\n76.90±1.29\n84.00±0.58\n48.09±3.39\n78.58±0.29\nGOLD\n73.18±3.25\n32.74±10.86\n52.87±6.15\n69.58±2.64\n3.50±1.32\n59.61±2.32\nCFC\n78.15±0.96\n86.54±1.03\n82.28±0.41\n82.81±0.77\n76.14±4.76\n81.81±0.07\nfication performance slightly decreases compared to closed-\nset methods (e.g., from 90.64% to 87.49% on Cora when\ncomparing CFC to GCN_sigmoid), OOD detection accu-\nracy significantly improves from 0% to 95.74%, which is\nremarkable. Compared to open-set classification methods\nsuch as G2Pxy, CFC improves OOD detection accuracy\nfrom 72.46% to 95.74%, while also enhancing ID classifi-\ncation from 82.65% to 87.49% on Cora. The same trend is\nobserved in other datasets. This shows that CFC better sepa-\nrates known and unknown classes by using semantic OOD\nsamples.\nOOD Classification\nWithout extra label information, it is\nevident that existing closed-set and open-set classification\nmethods are not equipped to handle OOD classification when\nmultiple OOD classes are present. Although some methods,\nlike GCN_Poser and G2Pxy, generate OOD samples during\ntraining, they struggle to differentiate between various OOD\nclasses without access to OOD label information. Leveraging\nthe annotation capabilities of LLMs and special designed\nprompt, our proposed CFC method successfully classifies\nunknown samples into different OOD labels. Notably, using\nthe post-OOD label space and GPT-4o, we achieve accuracies\nof 69.76%, 70.30%, 57.96%, and 48.45% on Cora, Citeseer,\nWikiCS, and DBLP, respectively, in the case of two OOD\nclasses. More OOD classification results with other encoders\ncan be found in Appendix.\n"}, {"page": 7, "text": "Figure 4: Ablation study on (a) LLM prompts for OOD identification, (b) Various LLM for OOD identification, and (c) LLM\nprompts for OOD classification on Cora and Citeseer.\nFigure 5: Study on the effect of (a) the number of identified\nOOD samples from coarse-grained classification, and (b)\nthe number of generated OOD samples by manifold mixup\nmethod for the CFC performance on Cora and Citeseer.\nImpact of Different Prompts and LLMs in\nCoarse-Classifier\nLLM Prompts.\nWe investigated the effectiveness of using\na constraint (rejecting with high confidence for Easy-Reject\ndetection and integrating a candidate OOD label space for\nHard-Reject detection) when designing LLM prompts. The\nrest of the LLM prompt content remains unchanged. Specifi-\ncally, the reject with high confidence LLM prompt instructs\nthe model to classify an input sample as OOD only when it is\nhighly confident. The LLM prompt with the candidate OOD\nlabel space provides the model with additional label choices\nand encourages consideration of extra labels when making\ndecisions. As shown in Figure 4 (a), without the constraint,\nOOD performance degrades on AUROC metrics, underscor-\ning the importance of the proposed constraint (Cora utilizes\nthe reject with high confidence LLM prompt, and Citeseer\nuses the candidate OOD label space LLM prompt).\nVarious LLMs.\nWe conducted experiments with various\nLLMs to gain a more comprehensive understanding of their\nability to detect OOD samples. Specifically, we use Llama\n(Llama2-7b and Llama3-8b) (Touvron et al. 2023) and GPT-\n4o for OOD detection. Llama models use few-shot learning\nwith one ID and one OOD example; GPT-4o uses zero-shot\nlearning. All prompts follow the constriction strategy. Results\non Cora and Citeseer (Figure 4 (b)) show GPT-4o outper-\nforms Llama, with Llama3-8b slightly better than Llama2-7b.\nImpact of Different Strategies in Fine-Grained\nDetection\nWe conducted ablation studies to evaluate different strate-\ngies, including denoising and data augmentation methods for\nGNN-based fine-grained classification under the case u = 2.\nAs shown in Tables 1, both CFC (wo / D/M) and CFC (wo / D)\nmodels, which exclude the denoising, consistently achieve\nlower ID accuracies across all datasets compared to their\ncounterparts, CFC (wo / M) and CFC with denoising. The\nCFC (wo / D) model, which employs the improved manifold\nmixup method to generate more OOD data, demonstrates\nsignificant improvement over CFC (wo / D/M) across all\ndatasets. Manifold mixup serves as a regularizer, encourag-\ning the neural network to make less confident predictions on\ninterpolated hidden representations. By integrating ID classes\nwith coarse OOD classes, CFC produces a GNN-based clas-\nsifier with smoother decision boundaries across multiple rep-\nresentation levels. We also found that data augmentation has\na greater impact than denoising, since the LLM-designed\nprompt already helps reduce noise.\nIn fine-grained detection, the OOD samples identified from\ncoarse-grained detection play a crucial role. Given LLMs’\nhigh cost on large test sets, we evaluate how the number\nof identified OOD samples affects CFC’s performance. As\nshown in Figure 5 (a), CFC performs well even with few\nidentified OOD samples, highlighting the advantages of in-\ncorporating semantic OOD samples. This demonstrates the\nscalability of CFC, making it applicable to large-scale test\ndatasets, as evidenced by WikiCS. Additionally, increasing\nthe number of identified OOD samples consistently boosts\nCFC accuracy on both Cora and Citeseer.\nThe manifold mixup-based data augmentation method is a\nkey component of CFC. To assess its impact, we examine how\nthe generated OOD samples influence CFC’s performance.\nIn Figure 5 (b), we observe the performance varies across\ndifferent datasets, specifically, an improved performance on\nCiteseer and stable performance on Cora with an increasing\nnumber of generated OOD samples.\n"}, {"page": 8, "text": "Impact of OOD Label Space for OOD Classification\nWe examine the impact of the post-OOD label space in LLM\nprompts for OOD classification. Specifically, we evaluate the\nperformance of LLM annotations with and without the po-\ntential OOD label space on the predicted OOD samples from\nfine-grained OOD detection. In coarse-grained detection, the\nLLM generates potential OOD labels using a general prompt\nwithout the post-OOD label space as a baseline. For datasets\nlike Citeseer, even when candidate OOD labels are provided,\nthe label space remains broad and comparable to the general\nprompt. As shown in Figure 4 (c), using a conditional prompt\nwith a post-processed OOD label space greatly improves\nprediction accuracy.\nConclusion\nWe tackle the open-world challenge of OOD classification\nwith CFC, a coarse-to-fine framework leveraging LLMs.\nLLM prompts guide coarse OOD detection and build a can-\ndidate OOD label space, from which semantic OOD samples\nare generated. These samples are used to train a GNN-based\nfine-grained classifier with denoising and data augmentation.\nA refined LLM prompt then performs OOD classification.\nExperiments show CFC achieves state-of-the-art OOD de-\ntection and strong OOD classification across graph and text\ndomains. We leave graph-level OOD classification for future\nwork and aim to inspire further research in open-set learning.\nCFC assumes graph nodes can be described by text; if not,\nLLMs may struggle with OOD detection and classification.\nIt also depends on the LLM’s knowledge of ID categories. To\nreduce reliance on large LLMs, fine-tuning smaller models\non domain-specific data and integrating retrieval-augmented\ngeneration (RAG) into CFC is a promising direction.\nAcknowledgments\nThis work is in part supported by the National Natural Sci-\nence Foundation of China (Grant No. 62276067) and Aus-\ntralian Research Council Discovery project DP230101534.\nReferences\nAchiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya,\nI.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman,\nS.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774.\nAntypas, D.; Ushio, A.; Camacho-Collados, J.; Neves, L.;\nSilva, V.; and Barbieri, F. 2022. Twitter topic classification.\narXiv preprint arXiv:2209.09824.\nBao, T.; Wu, Q.; Jiang, Z.; Chen, Y.; Sun, J.; and Yan, J.\n2024. Graph out-of-distribution detection goes neighborhood\nshaping. In ICML.\nBaran, M.; Baran, J.; Wójcik, M.; Zi˛eba, M.; and Gonczarek,\nA. 2023. Classical out-of-distribution detection methods\nbenchmark in text classification tasks.\narXiv preprint\narXiv:2307.07002.\nCai, M.; and Li, Y. 2023. Out-of-distribution detection via\nfrequency-regularized generative models. In WACV, 5521–\n5530.\nChen, Y.; Luo, Y.; Song, Y.; Dai, P.; Tang, J.; and Cao, X.\n2025. Decoupled graph energy-based model for node out-of-\ndistribution detection on heterophilic graphs. arXiv preprint\narXiv:2502.17912.\nChen, Z.; Mao, H.; Li, H.; Jin, W.; Wen, H.; Wei, X.; Wang,\nS.; Yin, D.; Fan, W.; Liu, H.; et al. 2024a. Exploring the po-\ntential of large language models (llms) in learning on graphs.\nACM SIGKDD Explorations Newsletter, 25(2): 42–61.\nChen, Z.; Mao, H.; Liu, J.; Song, Y.; Li, B.; Jin, W.; Fatemi,\nB.; Tsitsulin, A.; Perozzi, B.; Liu, H.; et al. 2024b. Text-space\ngraph foundation models: Comprehensive benchmarks and\nnew insights. NeurIPS, 37: 7464–7492.\nChen, Z.; Mao, H.; Wen, H.; Han, H.; Jin, W.; Zhang, H.;\nLiu, H.; and Tang, J. 2023. Label-free node classification\non graphs with large language models (llms). arXiv preprint\narXiv:2310.04668.\nGe, Z.; Demyanov, S.; Chen, Z.; and Garnavi, R. 2017. Gen-\nerative openmax for multi-class open set classification. arXiv\npreprint arXiv:1707.07418.\nGiles, C. L.; Bollacker, K. D.; and Lawrence, S. 1998. Cite-\nSeer: An automatic citation indexing system. In Proceedings\nof the third ACM conference on Digital libraries, 89–98.\nGong, Z.; and Sun, Y. 2024.\nAn Energy-centric Frame-\nwork for Category-free Out-of-distribution Node Detection\nin Graphs. In KDD, 908–919.\nGuo, J.; Du, L.; Liu, H.; Zhou, M.; He, X.; and Han, S. 2023a.\nGpt4graph: Can large language models understand graph\nstructured data? an empirical evaluation and benchmarking.\narXiv preprint arXiv:2305.15066.\nGuo, Y.; Yang, C.; Chen, Y.; Liu, J.; Shi, C.; and Du, J. 2023b.\nA Data-centric Framework to Endow Graph Neural Networks\nwith Out-Of-Distribution Detection Ability. In KDD, 638–\n648.\nHamilton, W.; Ying, Z.; and Leskovec, J. 2017. Inductive\nRepresentation Learning on Large Graphs. In Guyon, I.;\nLuxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vish-\nwanathan, S.; and Garnett, R., eds., NeurIPS, volume 30.\nHan, X.; Jiang, Z.; Liu, N.; and Hu, X. 2022. G-mixup:\nGraph data augmentation for graph classification. In ICML,\n8230–8248. PMLR.\nHe, X.; Bresson, X.; Laurent, T.; Hooi, B.; et al. 2023. Ex-\nplanations as features: Llm-based features for text-attributed\ngraphs. arXiv preprint arXiv:2305.19523, 2(4): 8.\nHendrycks, D.; Basart, S.; Mazeika, M.; Zou, A.; Kwon, J.;\nMostajabi, M.; Steinhardt, J.; and Song, D. 2019. Scaling\nout-of-distribution detection for real-world settings. arXiv\npreprint arXiv:1911.11132.\nHendrycks, D.; and Gimpel, K. 2016. A baseline for detect-\ning misclassified and out-of-distribution examples in neural\nnetworks. arXiv preprint arXiv:1610.02136.\nHendrycks, D.; Mazeika, M.; and Dietterich, T. 2018. Deep\nanomaly detection with outlier exposure. arXiv preprint\narXiv:1812.04606.\nHu, Y.; and Khan, L. 2021. Uncertainty-aware reliable text\nclassification. In KDD, 628–636.\n"}, {"page": 9, "text": "Huang, R.; Geng, A.; and Li, Y. 2021. On the importance\nof gradients for detecting distributional shifts in the wild.\nNeurIPS, 34: 677–689.\nJi, M.; Sun, Y.; Danilevsky, M.; Han, J.; and Gao, J. 2010.\nGraph regularized transductive classification on heteroge-\nneous information networks.\nIn Joint European Confer-\nence on Machine Learning and Knowledge Discovery in\nDatabases, 570–586. Springer.\nKipf, T. N.; and Welling, M. 2017. Semi-Supervised Classifi-\ncation with Graph Convolutional Networks. In Proc. 5th Int.\nConf. Learn. Representations.\nKirichenko, P.; Izmailov, P.; and Wilson, A. G. 2020. Why\nnormalizing flows fail to detect out-of-distribution data.\nNeurIPS, 33: 20578–20589.\nLiang, S.; Li, Y.; and Srikant, R. 2017. Enhancing the reliabil-\nity of out-of-distribution image detection in neural networks.\narXiv preprint arXiv:1706.02690.\nLiu, H.; Feng, J.; Kong, L.; Liang, N.; Tao, D.; Chen, Y.;\nand Zhang, M. 2023a. One for all: Towards training one\ngraph model for all classification tasks.\narXiv preprint\narXiv:2310.00149.\nLiu, W.; Wang, X.; Owens, J.; and Li, Y. 2020. Energy-based\nout-of-distribution detection. NeurIPS, 33: 21464–21475.\nLiu, Y.; Ding, K.; Liu, H.; and Pan, S. 2023b. Good-d: On\nunsupervised graph out-of-distribution detection. In Proceed-\nings of the Sixteenth ACM International Conference on Web\nSearch and Data Mining, 339–347.\nMa, L.; Sun, Y.; Ding, K.; Liu, Z.; and Wu, F. 2024. Revisit-\ning Score Propagation in Graph Out-of-Distribution Detec-\ntion. In NeurIPS.\nMcCallum, A. K.; Nigam, K.; Rennie, J.; and Seymore, K.\n2000. Automating the construction of internet portals with\nmachine learning. Information Retrieval, 3: 127–163.\nMernyei, P.; and Cangea, C. 2020. Wiki-cs: A wikipedia-\nbased benchmark for graph neural networks. arXiv preprint\narXiv:2007.02901.\nMisra, R. 2022.\nNews category dataset.\narXiv preprint\narXiv:2209.11429.\nNalisnick, E.; Matsukawa, A.; Teh, Y. W.; Gorur, D.; and Lak-\nshminarayanan, B. 2018. Do deep generative models know\nwhat they don’t know? arXiv preprint arXiv:1810.09136.\nNeal, L.; Olson, M.; Fern, X.; Wong, W.-K.; and Li, F. 2018.\nOpen set learning with counterfactual images. In ECCV,\n613–628.\nNi, J.; Li, J.; and McAuley, J. 2019. Justifying recommenda-\ntions using distantly-labeled reviews and fine-grained aspects.\nIn EMNLP-IJCNLP, 188–197.\nPark, J.; Jung, Y. G.; and Teoh, A. B. J. 2023.\nNearest\nneighbor guidance for out-of-distribution detection. In ICCV,\n1686–1695.\nPerera, P.; Morariu, V. I.; Jain, R.; Manjunatha, V.; Wiging-\nton, C.; Ordonez, V.; and Patel, V. M. 2020. Generative-\ndiscriminative feature representations for open-set recogni-\ntion. In CVPR, 11814–11823.\nRamos, J.; et al. 2003. Using tf-idf to determine word rel-\nevance in document queries.\nIn Proceedings of the first\ninstructional conference on machine learning, volume 242,\n29–48. Citeseer.\nReimers, N. 2019.\nSentence-BERT: Sentence Embed-\ndings using Siamese BERT-Networks.\narXiv preprint\narXiv:1908.10084.\nShen, X.; Wang, Y.; Zhou, K.; Pan, S.; and Wang, X. 2024.\nOptimizing ood detection in molecular graphs: A novel ap-\nproach with diffusion models. In KDD, 2640–2650.\nSong, Y.; and Wang, D. 2022. Learning on graphs with\nout-of-distribution nodes. In KDD, 1635–1645.\nSun, Y.; Guo, C.; and Li, Y. 2021. React: Out-of-distribution\ndetection with rectified activations. NeurIPS, 34: 144–157.\nSun, Y.; and Li, Y. 2022. Dice: Leveraging sparsification for\nout-of-distribution detection. In ECCV, 691–708. Springer.\nSun, Y.; Ming, Y.; Zhu, X.; and Li, Y. 2022.\nOut-of-\ndistribution detection with deep nearest neighbors. In ICML,\n20827–20840. PMLR.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nUm, D.; Lim, J.; Kim, S.; Yeo, Y.; and Jung, Y. 2025. Spread-\ning Out-of-Distribution Detection on Graphs. In The Thir-\nteenth International Conference on Learning Representa-\ntions.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Liò,\nP.; and Bengio, Y. 2018. Graph Attention Networks. In Proc.\n6th Int. Conf. Learn. Representations.\nVerma, V.; Lamb, A.; Beckham, C.; Najafi, A.; Mitliagkas, I.;\nLopez-Paz, D.; and Bengio, Y. 2019. Manifold mixup: Better\nrepresentations by interpolating hidden states.\nIn ICML,\n6438–6447. PMLR.\nWang, D.; Qiu, R.; Bai, G.; and Huang, Z. 2025. GOLD:\nGraph Out-of-Distribution Detection via Implicit Adversarial\nLatent Generation. arXiv preprint arXiv:2502.05780.\nWang, H.; Feng, S.; He, T.; Tan, Z.; Han, X.; and Tsvetkov, Y.\n2024. Can language models solve graph problems in natural\nlanguage? NeurIPS, 36.\nWang, H.; and Leskovec, J. 2020. Unifying graph convolu-\ntional neural networks and label propagation. arXiv preprint\narXiv:2002.06755.\nWang, H.; Li, Z.; Feng, L.; and Zhang, W. 2022a. Vim:\nOut-of-distribution with virtual-logit matching. In CVPR,\n4921–4930.\nWang, L.; Yang, N.; Huang, X.; Jiao, B.; Yang, L.; Jiang,\nD.; Majumder, R.; and Wei, F. 2022b. Text embeddings by\nweakly-supervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\nWang, Q.; Fang, Z.; Zhang, Y.; Liu, F.; Li, Y.; and Han,\nB. 2023.\nLearning to augment distributions for out-of-\ndistribution detection. NeurIPS, 36: 73274–73286.\nWu, M.; Pan, S.; and Zhu, X. 2020. Openwgl: Open-world\ngraph learning. In ICDM, 681–690. IEEE.\n"}, {"page": 10, "text": "Wu, M.; Pan, S.; and Zhu, X. 2021. Openwgl: open-world\ngraph learning for unseen class node classification. Knowl-\nedge and Information Systems, 63(9): 2405–2430.\nWu, Q.; Chen, Y.; Yang, C.; and Yan, J. 2023. Energy-based\nout-of-distribution detection for graph neural networks. arXiv\npreprint arXiv:2302.02914.\nYang, L.; Lu, B.; and Gan, X. 2023. Graph Open-Set Recog-\nnition via Entropy Message Passing. In ICDM, 1469–1474.\nIEEE.\nYang, S.; Liang, B.; Liu, A.; Gui, L.; Yao, X.; and Zhang, X.\n2025. Bounded and uniform energy-based out-of-distribution\ndetection for graphs. arXiv preprint arXiv:2504.13429.\nYe, R.; Zhang, C.; Wang, R.; Xu, S.; Zhang, Y.; et al.\n2023. Natural language is all a graph needs. arXiv preprint\narXiv:2308.07134, 4(5): 7.\nYin, N.; Wang, M.; Chen, Z.; Shen, L.; Xiong, H.; Gu, B.;\nand Luo, X. 2024. DREAM: Dual structured exploration\nwith mixup for open-set graph domain adaption. In ICLR.\nZhang, Q.; Li, X.; Lu, J.; Qiu, L.; Pan, S.; Chen, X.; and\nChen, J. 2024a. ROG_PL: Robust Open-Set Graph Learning\nvia Region-Based Prototype Learning. In AAAI, volume 38,\n9350–9358.\nZhang, Q.; Lu, J.; Li, X.; Wu, H.; Pan, S.; and Chen, J. 2024b.\nCONC: complex-noise-resistant open-set node classification\nwith adaptive noise detection. In IJCAI. ICJAI.\nZhang, Q.; Shi, Z.; Zhang, X.; Chen, X.; Fournier-Viger, P.;\nand Pan, S. 2023. G2Pxy: generative open-set node classifica-\ntion on graphs with proxy unknowns. In IJCAI, 4576–4583.\nZhou, D.-W.; Ye, H.-J.; and Zhan, D.-C. 2021. Learning\nplaceholders for open-set recognition. In CVPR, 4401–4410.\nZhu, J.; Li, H.; Yao, J.; Liu, T.; Xu, J.; and Han, B. 2023.\nUnleashing mask: Explore the intrinsic out-of-distribution\ndetection capability. arXiv preprint arXiv:2306.03715.\nZhu, X. 2005.\nSemi-supervised learning with graphs.\nCarnegie Mellon University.\n"}, {"page": 11, "text": "Empirical case study\nIn Figure 1, we observe that GNNs can form clear bound-\naries between ID and OOD samples when more actual OOD\ndata is incorporated, leading to improved OOD detection\nperformance.\nFigure 6: Comparison of OOD detection on Cora with differ-\nent number of true OOD samples. Adding actual OOD labels\ncan largely increase the OOD detection performance.\nMore Related Works\nLLM for Text-Attributed Graphs.\nRecent progress in ap-\nplying LLMs to graphs (He et al. 2023; Guo et al. 2023a; Liu\net al. 2023a; Chen et al. 2024b) aims to harness the power\nof LLMs to enhance the performance of graph-related tasks.\nLLMs can either be used as predictors (Chen et al. 2024a;\nWang et al. 2024; Ye et al. 2023), which directly generate\nsolutions, or as enhancers (He et al. 2023), which leverage the\ncapabilities of LLMs to improve the performance of smaller\nmodels with greater efficiency. Additionally, LLMs are ex-\nplored for graph active learning, serving as annotators (Chen\net al. 2023) to train efficient models that achieve promising\nperformance without the need for ground truth labels. In this\npaper, we utilize LLMs for graph node OOD detection and\nhelp to address the problem of graph node OOD classifica-\ntion.\nPrompts\nIn this section, we present the prompts designed for coarse-\ngrained OOD identification and OOD classification. Specif-\nically, Table 3 provides an example using the Easy-Reject\nprompt on Cora, while Table 6 illustrates the Hard-Reject\nprompt on Citeseer. Tables 4 and 5 offer examples for gener-\nating the major category and a candidate OOD label space.\nFinally, Table 7 presents the prompt incorporating the post-\nOOD label space for OOD classification.\nTable 3: Full prompt example with Easy-Reject scenarios for\nzero-shot coarse classifier on the Cora dataset.\nInput:\nPaper:\nA self-adjusting dynamic logic module: This paper\npresents an ASOCS (Adaptive Self-Organizing Con-\ncurrent System) model for massively parallel process-\ning of incrementally defined rule systems in such areas\nas adaptive logic, robotics, logical inference, and dy-\nnamic control. An ASOCS is an adaptive network com-\nposed of many simple computing elements operating\nasynchronously and in parallel. This paper focuses on\nAdaptive Algorithm 2 (AA2) and details its architecture\nand learning algorithm. AA2 has significant memory\nand knowledge maintenance advantages over previous\nASOCS models. An ASOCS can operate in either a\ndata processing mode or a learning mode. During learn-\ning mode, the ASOCS is given a new rule expressed\nas a boolean conjunction. The AA2 learning algorithm\nincorporates the new rule in a distributed fashion in a\nshort, bounded time. During data processing mode, the\nASOCS acts as a parallel hardware circuit.\nTask:\nThere are following categories:\nneural_networks, genetic_algorithms, theory, reinforce-\nment_learning, probabilistic_methods.\nIs the topic of this paper in the category list? Provide\nyour answer and a confidence number between [0-1].\nChoose False only if you are very certain that the\npaper does not belong to any of the listed categories.\nIf True, specify which category in\nneural_networks, genetic_algorithms, theory, reinforce-\nment_learning, probabilistic_methods the paper be-\nlongs to. If False, provide a suggested category that\nis not in the category list.\n[{\"answer\"\n:\n⟨True or False⟩, \"confidence\"\n:\n⟨confidence_here⟩, \"category\" : ⟨category_here⟩}]\nOutput:\n"}, {"page": 12, "text": "Table 5: Prompt example for generating possible outlier labels\nbased on the provided ID label space and the major category.\nInput:\nTask:\nGenerate 10 possible paper topics that belong to Com-\nputer Science but are distinct from the provided topics\n{agents, information retrieval, database,\nartificial intelligence}.\nOutput : [{\"answer\" : ⟨your_answer⟩}, {\"answer\" :\n⟨your_answer⟩}, . . . ].\nOutput:\nTable 4: Prompt example for generating major category based\non the provided ID label space.\nInput:\nTask:\nThere are following listed Paper topics:\n{agents, information retrieval, database,\nartificial intelligence}.\nWhich major category do these themes belong to?\nOutput : [{\"answer\" : ⟨your_answer⟩}].\nOutput:\nProof\nProof of Theorem 3.1\nProof. Let the input ID space be denoted by X, where each\ninput x ∈X belongs to an ID class yi ∈Yid. Let f(x) denote\nthe model’s output probability distribution over the classes\nYid ∪{yood}, where yood is the OOD class under a open-set\nsetting.\nTo produce generated OOD samples, in the traditional\nmanifold mixup within ID classes, the representation of the\nnew sample hmix is created by linearly interpolating between\nthe hidden embeddings of two different ID samples hi and\nhj as:\n(hmix, ymix) := (λhi + (1 −λ)hj, yood),\nλ ∈[0, 1].\nDenote the generated OOD samples, which lie in the con-\nvex hull of the ID classes, as Xmix. These samples encourage\nthe model to generalize by smoothing the decision bound-\nary between ID regions in the feature space. However, such\ngenerated samples do not explicitly extend the feature space\ntowards new region.\nAssume that LLMs possess expert knowledge to under-\nstand the categories in Yid. In the first step of CFC, the coarse-\ngrained classifier is able to detect certain outlier data. These\noutlier samples related to practical test space are realistic\nand carry semantic meaning. Thus, CFC explore a new space\ndifferent from ID space. For these semantic OOD samples,\nlet their mean hidden representation be denoted as hood. By\napplying manifold mixup between the hidden embedding of\nan ID sample xi and OOD center xood, the mixing would\noccur as:\n(h\n′\nmix, y\n′\nmix) := (λhi + (1 −λ)hood, yood),\nλ ∈[0, 1].\nTable 6: Full prompt example with Hard-Reject scenarios for\nzero-shot coarse classifier on the Citeseer dataset.\nInput:\nPaper:\nLogical Case Memory Systems: Foundations And Learning\nIssues The focus of this paper is on the introduction of a\nquite general type of case-based reasoning systems called\nlogical case memory systems. The development of the under-\nlying concepts has been driven by investigations in certain\nproblems of case-based learning. Therefore, the present devel-\nopment of the target concepts is accompanied by an in-depth\ndiscussion of related learning problems. Logical case memory\nsystems provide some formal framework for the investiga-\ntion and for the application of structural similarity concepts.\nThose concepts have some crucial advantage over traditional\nnumerical similarity concepts: The result of determining a\nnew case´s similarity to some formerly experienced case can\nbe directly taken as a basis for performing case adaptation.\nEssentially, every logical case memory system consists of\ntwo constituents, some partially ordered case base and some\npartially ordered set of predicates. Cases are terms, in a logi-\ncal sense. Given some problem case, every predicat...\nTask:\nThere are the following categories:\n[agents, information retrieval, database, artificial intelligence].\nIf True, specify which category in\n[agents, information retrieval, database, artificial intelligence]\nthe paper belongs to.\nIf False, provide a suggested category that is not in the cat-\negory list. The suggested category includes but not limited\nto the following: [Quantum Computing and Its Applications\nin Cryptography, Blockchain Technology and Decentralized\nApplications, Edge Computing, Natural Language Process-\ning for Sentiment Analysis, Cybersecurity, Human-Computer\nInteraction, Machine Learning for Predictive Maintenance\nin Industrial Systems, Computer Vision, Augmented Reality,\nBig Data Analytics, ...]\nProvide your answer, a confidence number between [0-1],\nand suggested category.\n[{\"answer\"\n:\n⟨True or False⟩, \"confidence\"\n:\n⟨confidence_here⟩, \"category\" : ⟨category_here⟩}]\nOutput:\n"}, {"page": 13, "text": "Table 7: Full prompt example with post-OOD label space for\nzero-shot OOD classifier on the Citeseer dataset.\nInput:\nPaper:\nLogical Case Memory Systems: Foundations And\nLearning Issues The focus of this paper is on the intro-\nduction of a quite general type of case-based reasoning\nsystems called logical case memory systems. The de-\nvelopment of the underlying concepts has been driven\nby investigations in certain problems of case-based\nlearning. Therefore, the present development of the\ntarget concepts is accompanied by an in-depth discus-\nsion of related learning problems. Logical case memory\nsystems provide some formal framework for the inves-\ntigation and for the application of structural similarity\nconcepts. Those concepts have some crucial advantage\nover traditional numerical similarity concepts: The re-\nsult of determining a new case´s similarity to some for-\nmerly experienced case can be directly taken as a basis\nfor performing case adaptation. Essentially, every logi-\ncal case memory system consists of two constituents,\nsome partially ordered case base and some partially\nordered set of predicates. Cases are terms, in a logical\nsense. Given some problem case, every predicat...\nTask:\nThere are the following categories:\n{machine learning, human computer interaction,\naugmented, edge, cybersecurity, big}. Which category\ndoes this paper belong to? Provide your best guess with\na confidence ranging from 0 to 1. For example:\n[{\"answer\"\n:\n⟨category_here⟩, \"confidence\"\n:\n⟨confidence_here⟩}]\nOutput:\nDenote the generated OOD samples using the improved man-\nifold mixup method as X\n′\nmix.\nLet H be a vector space of dimension dim(H). Assume the\ndimension of the ID input space is dim(H). The dimension\nof Xmix generated within the ID space is also dim(H). Let\nthe dimension of the new space of semantic OOD samples\nbe dim(H\n′). Then, the dimension of X\n′\nmix is dim(H + H\n′),\nwhere dim(H + H\n′) > dim(H).\nAs the number of ID and OOD classes is C + 1, the repre-\nsentations of the training points lie fully on a dim(H) −\n(C + 1) subspace for pure ID class mixup, and on a\ndim(H + H\n′) −(C + 1) subspace for our ID and OOD\nmixed augmentation.\nWe have dim(H+H\n′)−(C+1) > dim(H)−(C+1). Thus,\ncompared to manifold mixup applied only to ID classes, the\nmixup between ID and OOD classes extends the OOD feature\nspace beyond the convex hull of ID samples. Consequently,\nCFC results in a smoother and flatter decision boundary for\nOOD detection.\nManifold Mixup-based Subspace Extension on\nClassifier Decision Boundary\nWe provide a theoretical analysis of how embedding mixup\nexpands the representation subspace and influences the clas-\nsifier’s decision boundary.\nLet X = xii = 1n ⊂Rd denote the graph node fea-\ntures, and Y = yin\ni=1 their corresponding labels, where\nyi ∈1, . . . , C. We consider a simple linear classifier f(x) =\nsoftmax(W ⊤x) trained on labeled data.\nTo regularize the classifier and expand the representation\nspace, we apply manifold mixup in the latent space. Given\ntwo embeddings xi and xj, the mixed feature and its corre-\nsponding soft label are given by:\n˜x = λxi + (1 −λ)xj,\nλ ∼Beta(α, α),\n˜y = λyi + (1 −λ)yj.\n(4)\nIn CFC, we perform OOD data augmentation using mani-\nfold mixup on the latent representations of identified OOD\nsamples. Let H′ denote the subspace spanned by the OOD\nembeddings. The mixup operation constructs an extended\nsubspace f\nH′ ⊃H′.\nAs shown in (Verma et al. 2019), the mixup-based training\nobjective:\nLmix = Eλ, i, j [L(f(˜x), ˜y)]\n(5)\ncan be interpreted as introducing an implicit regularization\neffect:\nLmix ≈Lemp + Ω(W),\n(6)\nwhere Lemp is the empirical risk, and Ω(W) penalizes\nabrupt changes in the decision function. This regularization\nencourages smoother transitions between classes, effectively\nenlarging inter-class margins and promoting a more robust,\nwell-structured decision boundary for both ID and OOD\ndetection.\n"}, {"page": 14, "text": "Table 8: The details of graph datasets.\nDataset\nNodes\nEdges\nFeatures\nLabels\nCategories\nCora\n2708\n5429\n1433\n7\nRule Learning, Neural Networks, Case Based, Genetic\nAlgorithms, Theory, Reinforcement Learning, Proba-\nbilistic Methods\nCiteseer\n3312\n4732\n3703\n6\nAgents, Machine Learning, Information Retrieval,\nDatabase, Human Computer Interaction, Artificial In-\ntelligence\nWikiCS\n11,701\n216,123\n300\n10\nComputational Linguistics, Databases, Operating Sys-\ntems, Computer Architecture, Computer Security, In-\nternet Protocols, Computer File Systems, Distributed\nComputing Architecture, Web Technology, Program-\nming Language Topics\nDBLP\n17,716\n105,734\n1639\n4\nData Mining, Database, Artificial Intelligence, Infor-\nmation Retrieval\nAmazon-\nComputer\n87,229\n1,256,548\n–\n10\nLaptop Accessories, Computer Accessories and Pe-\nripherals, Computer Components, Storage, Network-\ning Products, Monitors, Computers and Tablets, Tablet\nAccessories, Servers, Tablet Replacement Parts\nAmazon-Photo\n48,362\n873,782\n–\n12\nFilm Photography, Video, Digital Cameras, Acces-\nsories, Binoculars and Lenses, Bags and Cases, Light-\ning and Studio, Flashes, Tripods and Monopods, Un-\nderwater Photography, Video Surveillance\nTable 9: The details of text datasets.\nDataset\nNumber Labels\nCategories\nNews Category\n210, 000\n17\narts, business, crime, education, entertainment, environment,\nfood & drink, healthy living, home & living, news, parents,\npolitics, queer voices, religion, science, style, travel\nTwitter Topic\n3184\n6\narts & culture, business & entrepreneurs, daily life\npop culture, science & technology, sports & gaming\n20 Newsgroups\n18, 000\n20\nalt.atheism, comp.graphics, comp.os.ms-windows.misc, comp.sys.ibm.pc.hardware,\ncomp.sys.mac.hardware, comp.windows.x, misc.forsale, rec.autos, rec.motorcycles,\nrec.sport.baseball, rec.sport.hockey, sci.crypt, sci.electronics, sci.med, sci.space,\nsoc.religion.christian, talk.politics.guns, talk.politics.mideast, talk.politics.misc, talk.religion.misc\nDatasets\nOther Experimental Settings and Results for\nGraph Node Classification\nDetails of Baselines\nCCN_softmax: GCN ((Kipf and Welling 2017)) with a soft-\nmax layer is used as the final output layer.\nCCN_sigmoid: The softmax layer of GCN is replaced\nwith multiple 1-vs-rest of sigmoids.\nCCN_softmax_τ: Based on GCN_softmax, a probability\nthreshold chosen in {0.1, 0.2, ..., 0.9} is used for the classifi-\ncation of each class.\nCCN_sigmoid_τ: Based on GCN_sigmoid, a probability\nthreshold chosen in {0.1, 0.2, ..., 0.9} is used for the classifi-\ncation of each class.\nGCN_Poser: Employing the method in Poser ((Zhou, Ye,\nand Zhan 2021)) with GCN.\nG2Pxy: Using two kinds of proxy unknown nodes for\nunknown node detection.\nGNNSafe: proposing an effective OOD discriminator\nbased on an energy function extracted from graph neural\nnetworks.\nFor fair comparisons, CFC employs GCN as the back-\nbone for GNN-based fine classification, consistent with the\nother methods. All experiments are repeated five times using\ndifferent random seeds.\nSettings for OOD classes\nIn the case of two OOD classes (u=2), the classes Rule\nLearning and Case Based are selected as OOD for Cora,\nMachine Learning and Human-Computer Interaction for Cite-\nseer, Web Technology and Programming Language Topics for\nWikiCS, and Artificial Intelligence and Information Retrieval\nfor DBLP.\nIn the case of three OOD classes (u = 3), the classes Rule\n"}, {"page": 15, "text": "Learning, Case Based, and Probabilistic Methods are selected\nas OOD for Cora, Machine Learning, Human-Computer In-\nteraction, and DataBase for Citeseer, Distributed computing\narchitecture, Web Technology and Programming Language\nTopics for WikiCS.\nHyperparameter analysis\nWe analyze the impact of the parameter α, which balances ID\nand OOD features in Equation ˜xi = αhk\ni +(1−α)hk\nc, i ≤K,\nwhere hk\ni is the hidden embedding of ID nodes and hk\nc de-\nnotes the center embedding of identified OOD samples from\nthe coarse classifier with LLMs. The results in the following\ntable show that although the performance of CFC varies with\nα, it consistently outperforms other OOD detection methods.\nRobustness of CFC under limited ID data\nWe evaluated the robustness of CFC under limited ID data.\nAs shown in Table 11, reducing ID training data may slightly\ndecrease ID classification accuracy, but can in some cases\nimprove OOD detection.\nTable 11: The results on Cora and Citeseer. Here 0.2 means\nwe use 20% training data compared to the ID training data in\nour paper.\nRatio\nCora\nCiteseer\nID\nOOD\noverall\nID\nOOD\noverall\n0.2\n81.28±2.12\n93.73±1.47\n85.07±1.20\n69.98±1.84\n86.07±1.49\n77.87±0.37\n0.5\n84.09±1.67\n95.12±0.91\n87.45±0.10\n70.82±0.96\n84.96±0.90\n77.82±0.49\n1\n87.49±0.96\n95.74±0.61\n90.00±0.54\n73.92±1.02\n80.57±0.78\n77.21±0.82\nThe results of various LLMs as feature encoder\nTable 12 compares different LLM feature encoders. Overall,\ne5-large-v2 performs better and more stably in joint train-\ning. Additionally, LLMs specialize in different topics—for\ninstance, Llama2 excels in ID topics for Citeseer, while e5\nand ST perform well in OOD.\nOther Results of graph tasks\nWith an increasing number of OOD samples in the test set,\nwe observe higher OOD accuracies for the case of three OOD\nclasses in Table 13 in compared to two OOD classes in Table\n1 for the proposed CFC on Citeseer and WikiCS.\nEvaluation on Other Task\nWe show that the proposed CFC is a general framework for\ncomprehensively solving open-set classification problem in-\ncluding ID classification and OOD classification by applying\nCFC on text domain.\nIntroduction of Datasets\nWe use two text datasets including News Category dataset\n(Misra 2022), and Twitter Topic dataset (Antypas et al. 2022).\nNews Category Dataset\nis one of the largest news datasets,\ncontaining approximately 210k news headlines from Huff-\nPost, published between 2012 and 2022. The dataset includes\n42 classes, which are significantly imbalanced. To mitigate\nconfusion among similar classes, closely related categories\nwere merged. As a result, the dataset was reduced to 17 dis-\ntinct representative classes as in (Baran et al. 2023). We use\n7 classes—entertainment, healthy living, news, parents, poli-\ntics, style, and travel—as the ID space, with the remaining\n10 classes designated as OOD. The ID data is split for train-\ning, validation, and testing following the same procedure as\n(Baran et al. 2023). Additionally, 30% of the OOD data is\nselected for validation, with the remaining portion reserved\nfor testing.\nTwitter Topic Classification\nis a topic classification\ndataset derived from Twitter posts. It contains 3,184 high-\nquality tweets, each categorized into one of six distinct\nclasses.\nBaselines\nMaximum Softmax Probability(MSP)\nemploys the soft-\nmax score to check the certainty of whether a sample belongs\nto a domain.\nEnergy-based\n(Liu et al. 2020) leverages an energy score\nfunction to quantify the model’s confidence.\nRectified Activations (ReAct)\n(Sun, Guo, and Li 2021) is a\nstraightforward method for mitigating model overconfidence\non OOD examples by truncating excessively high activations\nduring evaluation.\nKL-Matching (KLM)\n(Hendrycks et al. 2019) computes\nthe minimum KL-divergence between the softmax probabili-\nties and the mean class-conditional distributions.\nGradNorm\n(Huang, Geng, and Li 2021) uses the vector\nnorm of the gradients to differentiate between in-distribution\n(ID) and out-of-distribution (OOD) samples, operating under\nthe assumption that larger norm values are indicative of in-\ndistribution data.\nDirected Sparsification (DICE)\n(Sun and Li 2022) em-\nploys selective sparsification to retain a subset of weights,\neffectively removing irrelevant information from the model’s\noutput.\nVirtual-logit Matching (ViM)\n(Wang et al. 2022a) inte-\ngrates information from both the feature space and the out-\nput logits. This approach provides a combination of class-\nagnostic and class-dependent knowledge, enabling improved\nseparation of OOD data.\nK-nearest Neighbors (KNN)\n(Sun et al. 2022) calculates\nthe distance between the embedding of a given input and the\nembeddings from the training set to determine whether the\ninput belongs to the in-distribution (ID) or not.\nExperiment Setting\nFor all methods, we use the RoBERTabase model (Liu et al.,\n2019) as the backbone, paired with a fully connected layer\n"}, {"page": 16, "text": "Table 10: The results of CFC with different values of parameter α.\nDataset\n0.1\n0.3\n0.5\n0.7\n0.9\nID\nOOD\nID\nOOD\nID\nOOD\nID\nOOD\nID\nOOD\nCora\n90.04±0.43\n82.13±0.81\n89.92±0.34\n83.09±0.93\n86.01 ±0.96\n96.21±0.61\n86.02±0.96\n96.22±0.61\n86.02±0.96\n96.22±0.61\nTable 12: Comparison of various LLMs as feature encoder on ID classification and OOD detection across four datasets.\nLLM Encoder\nCora\nCiteseer\nWikiCS\nDBLP\nID\nOOD\noverall\nID\nOOD\noverall\nID\nOOD\noverall\nID\nOOD\noverall\nCFC-e5\n87.49\n95.74\n90.00\n73.92\n80.57\n77.21\n81.22\n75.93\n79.73\n76.06\n73.61\n74.44\nCFC-ST\n84.24\n90.03\n86.00\n72.78\n81.71\n77.20\n78.84\n65.35\n75.02\n83.02\n70.61\n74.82\nCFC-Llama2-7b\n84.72\n92.99\n87.24\n76.06\n73.14\n74.62\n79.05\n73.10\n77.37\n83.04\n50.30\n61.42\nCFC-Llama2-13b\n82.14\n94.98\n86.05\n76.53\n72.29\n74.43\n75.55\n68.06\n73.43\n87.14\n45.42\n59.59\nTable 13: Comparison of various methods on ID classification\nand OOD detection across three datasets: Cora, Citeseer,\nWikiCS (3 OOD classes).\nMethods\nCora\nCiteseer\nWikiCS\nID\nOOD\noverall\nID\nOOD\noverall\nID\nOOD\noverall\nGCN_softmax\n91.53\n0.00\n45.61\n74.56\n0.00\n22.55\n61.42\n0.00\n38.03\nGCN_sigmoid\n92.42\n0.00\n46.05\n79.21\n0.00\n23.96\n70.62\n0.00\n43.73\nGCN_softmax_τ\n61.37\n74.13\n67.69\n35.62\n91.50\n74.61\n35.23\n70.82\n48.79\nGCN_sigmoid_τ\n77.10\n57.16\n66.85\n45.90\n88.22\n75.38\n45.15\n86.90\n61.04\nGCN_PROSER\n71.01\n80.31\n75.61\n53.96\n77.09\n70.10\n50.94\n90.20\n65.49\nG2Pxy\n69.62\n76.62\n73.21\n61.43\n76.92\n72.23\n49.57\n92.07\n64.88\nCFC (wo / D/M)\n85.08\n86.67\n85.89\n63.55\n91.75\n83.64\n76.96\n88.84\n81.53\nCFC (wo / M)\n89.98\n81.20\n85.52\n67.57\n90.11\n83.63\n81.75\n75.39\n79.30\nCFC (wo / D)\n82.48\n89.04\n85.81\n63.97\n92.17\n84.06\n76.63\n90.13\n81.82\nCFC\n83.23\n91.87\n87.62\n66.73\n91.40\n84.30\n80.66\n81.64\n81.04\nTable 14: The results of OOD classification with different\nLLM encoder.\nEncoder\nCora\nCiteseer\nWikiCS\nDBLP\nGPT-4o\n67.76\n70.30\n57.96\n48.45\nLlama2\n59.45\n65.88\n15.40\n59.97\nLlama3\n62.20\n69.83\n15.00\n50.73\nTable 15: Comparison of various methods on OOD detection\nacross two datasets: News Category and Twitter.\nMethods\nNews Category\nTwitter\nmsp\n71.07\n61.83\nEnergy-based\n71.89\n53.52\nReAct\n72.16\n56.15\nKLM\n60.12\n50.03\nGradNorm\n71.33\n51.54\nDICE\n64.32\n44.68\nVim\n75.33\n60.89\nKNN\n76.20\n58.66\nCFC\n82.04\n71.68\nTable 16: The accuracy of single OOD category across two\ndatasets: News Category and Twitter.\nNews Category\nTwitter\nID\nOOD\nAccuracy\nID\nOOD\nAccuracy\nentertainment\narts\n50.48\nhealthy_living\ncrime\n66.32\nnews\neducation\n46.43\narts_&_culture\npop_culture\n37.45\nparents\nhome_&_living\n16.67\nbusiness_&_entrepreneurs\nscience_&_technology\n32.08\npolitics\nqueer_voices\n27.90\ndaily_life\nsports_&_gaming\n39.70\nstyle\nscience\n30.00\ntravel\nfor classification. All baseline methods are fine-tuned on the\ntraining data with a learning rate of 0.0001, using the Adam\noptimizer with a weight decay of 5 × 10−4.\nIn the proposed CFC approach, OOD samples with low\nconfidence scores from the LLM (below 0.8) are removed\nduring the denoising procedure. For data augmentation, the\nidentified OOD samples from the LLM are mixed with other\nsamples in a batch to generate additional synthetic OOD\nexamples. To ensure the robustness of the results, all experi-\nments are repeated five times with different initial seeds to\nminimize the impact of randomness.\n"}, {"page": 17, "text": "Results\nTable 15 presents the results of CFC compared to other OOD\ndetection methods on text datasets. CFC consistently out-\nperforms all baseline methods, mirroring the improvements\nobserved in graph datasets.\nTo further validate CFC’s effectiveness in OOD classifi-\ncation for text data, we report the results in Table 16. In the\nNews Category dataset, 7 classes are used as ID labels, while\nthe Twitter Topic dataset has an ID label space of 3 classes.\nOur findings demonstrate that CFC successfully identifies\nvarious OOD classes, highlighting its capability in addressing\nthe comprehensive open-world classification problem.\n"}]}