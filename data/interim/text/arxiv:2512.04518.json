{"doc_id": "arxiv:2512.04518", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.04518.pdf", "meta": {"doc_id": "arxiv:2512.04518", "source": "arxiv", "arxiv_id": "2512.04518", "title": "UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction", "authors": ["Tianmai M. Zhang", "Zhaoyi Sun", "Sihang Zeng", "Chenxi Li", "Neil F. Abernethy", "Barbara D. Lam", "Fei Xia", "Meliha Yetisgen"], "published": "2025-12-04T06:59:59Z", "updated": "2025-12-04T06:59:59Z", "summary": "The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.04518v1", "url_pdf": "https://arxiv.org/pdf/2512.04518.pdf", "meta_path": "data/raw/arxiv/meta/2512.04518.json", "sha256": "acbb5a43c40565e781b3236e4db4ac9b120995e9948d1bba2bc352bcfacff812", "status": "ok", "fetched_at": "2026-02-18T02:25:26.739233+00:00"}, "pages": [{"page": 1, "text": "UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and\nDictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction\nTianmai M. Zhang*, Zhaoyi Sun*, Sihang Zeng*, Chenxi Li*,\nNeil F. Abernethy, Barbara D. Lam, Fei Xia, Meliha Yetisgen\nUniversity of Washington\nCorrespondence: melihay@uw.edu\nAbstract\nThe ChemoTimelines shared task benchmarks\nmethods for constructing timelines of systemic\nanticancer treatment from electronic health\nrecords of cancer patients. This paper describes\nour methods, results, and findings for subtask\n2—generating patient chemotherapy timelines\nfrom raw clinical notes. We evaluated strate-\ngies involving chain-of-thought thinking, super-\nvised fine-tuning, direct preference optimiza-\ntion, and dictionary-based lookup to improve\ntimeline extraction. All of our approaches fol-\nlowed a two-step workflow, wherein an LLM\nfirst extracted chemotherapy events from in-\ndividual clinical notes, and then an algorithm\nnormalized and aggregated events into patient-\nlevel timelines. Each specific method differed\nin how the associated LLM was utilized and\ntrained. Multiple approaches yielded compet-\nitive performances on the test set leaderboard,\nwith fine-tuned Qwen3-14B achieving the best\nofficial score of 0.678. Our results and anal-\nyses could provide useful insights for future\nattempts on this task as well as the design of\nsimilar tasks.\n1\nIntroduction\nElectronic health records (EHRs) contain rich infor-\nmation on treatment courses, but extracting tempo-\nral relationships is challenging due to variability in\ncare and linguistic complexity (Olex and McInnes,\n2021; Gholipour et al., 2023). Oncology regimens\noften deviate from planned schedules through dose\nchanges or delays, with such modifications usually\nrecorded only in unstructured notes that require\nchronological alignment (Wang et al., 2020). Clin-\nical narratives add further difficulty with relative\nor vague time expressions and inconsistent date\nformats (Sun et al., 2013, 2015). Even experts\nmay diverge in interpreting underspecified terms,\nmaking accurate normalization and sequencing a\npersistent challenge for clinical NLP systems.\n*These authors contributed equally.\nThe ChemoTimelines shared task1 (Yao et al.,\n2024, 2025) was created to benchmark systems for\nconstructing systemic anticancer treatment (SACT)\ntimelines directly from EHR notes. It consists\nof two subtasks. In subtask 1, besides the raw\nEHRs, gold standard annotations of treatment\nevents (EVENTs) and time expressions (TIMEX3s)\nfor each patient EHR note are provided, and the\ntask is to determine temporal relations between\nthem on the patient level. In subtask 2, the task is\nto extract the patient-level treatment timeline with\nonly the raw EHR notes available. We focus on\nsubtask 2 to provide insights into an end-to-end\ntreatment timeline extraction system.\nLarge language models (LLMs) demonstrate su-\nperior comprehension and information extraction\nability, and were widely used in the previous year\nof the challenge (Haddadan et al., 2024; Zhang\net al., 2024). Without dedicated prompt engineer-\ning and chain-of-thought reasoning (Wei et al.,\n2023), zero-shot prompting on LLMs has shown\npoor performance (Zhang et al., 2024) in the time-\nline extraction task. Domain-adapted fine-tuning\nhas proven effective for SACT timeline extraction,\nwith models like Flan-T5-XXL (Chung et al., 2022)\nand PubMedBERT (Gu et al., 2021) achieving\nstrong results (Haddadan et al., 2024; Tan et al.,\n2024). However, these approaches have predomi-\nnantly utilized older or smaller-scale architectures,\nsuch as BART (Lewis et al., 2019) and Flan-T5-\nXXL (Chung et al., 2022), and predicted timelines\nbased on sentence-level contexts. Recent studies on\nscaling laws suggest that leveraging larger powerful\nmodels with rich context presents a clear opportu-\nnity for further improvement (Kaplan et al., 2020).\nIn parallel, pipeline systems—which first extract\nevents with a curated dictionary and then iden-\ntify relations (Haddadan et al., 2024; Wang et al.,\n1https://sites.google.com/view/\nchemotimelines2025\narXiv:2512.04518v1  [cs.CL]  4 Dec 2025\n"}, {"page": 2, "text": "2024)—have been developed but typically show\ninferior performance to end-to-end systems. De-\nspite integrating external knowledge, the pipeline\napproach may still be suboptimal.\nBuilding on previous efforts, we explore a vari-\nety of strategies to fill the gaps. First, to analyze\nthe impact of LLM-based reasoning, we compare\na baseline prompting system with a reasoning sys-\ntem. Second, to rethink the impact of external\nknowledge, we design a dictionary-enhanced ex-\ntraction approach. Finally, to explore multiple train-\ning strategies, we conduct supervised fine-tuning\n(SFT) and direct preference optimization (DPO) on\nthe latest LLMs. Our fine-tuned Qwen3-14B sys-\ntem wins first place in the challenge leaderboard.\nWe provide several novel insights into the task that\nmay inform future attempts on this task, as well as\nthe design of similar tasks.\n2\nProblem Formulation\nThe SACT timeline extraction task for each pa-\ntient is formulated as extracting m triplets T =\n{<sj, rj, tj>}m\nj=1 from a series of n clinical notes\nX = {x1, ..., xn} of the patient, where s indicates\na SACT entity, t is a TIMEX, and r indicates the re-\nlation between s and t selected from BEGINS-ON,\nENDS-ON, and CONTAINS-1. Following prac-\ntices of last year’s teams (Haddadan et al., 2024),\nwe used note-level gold-standard relation annota-\ntions on the training set as the training data for our\nsystems.\nDetailed descriptions of the task framework and\nthe dataset can be found on the shared task’s web-\nsite1 or in the overview paper (Yao et al., 2025). In\nshort, the dataset covers three cancer types (breast\ncancer, melanoma, and ovarian cancer) and was\nsplit by the task organizer into a training set (69\npatients, 2,910 note files), a development set (27\npatients, 1,272 note files), and a test set (53 patients,\n2,121 note files). Teams participating in the shared\ntask received the annotated training and develop-\nment sets for the development of their systems. The\nunannotated test set was released a few days be-\nfore the submission deadline for teams to run their\nsystems and submit predictions.\nTo extract triplets T from clinical notes X,\nprevious patient-level approaches (Zhang et al.,\n2024) directly processed the entire X, which may\noverwhelm the LLM, while the sentence-level ap-\nproach (Haddadan et al., 2024) separately pro-\ncessed sentences in each xi, which may lack global\ncontext. In contrast to these approaches, we lever-\nage a note-level approach that splits the entire task\ninto two steps: (1) note-level extraction: extract-\ning triplets Ti from individual notes xi, with or\nwithout format postprocessing, and (2) timeline\naggregation: normalizing the TIMEXs and aggre-\ngating {Ti}n\ni=1 into a patient-level timeline T . This\nsetting allows decoupling of LLM extraction per-\nformance from the final timeline-level performance,\nenabling us to evaluate and optimize the methods\nfor each step. LLM-based methods for step 1 are\ndescribed in Section 3, and the aggregation method\nfor step 2 is explained in Section 4.\n3\nNote-Level Extraction\nWe compared 5 different strategies for the note-\nlevel extraction task, providing insights from var-\nious aspects. We further included an ensemble\nmethod in our challenge submission to probe the\nrelationship between note-level extraction and time-\nline aggregation.\n3.1\nPrompting Baseline\nThe baseline approach uses prompt-based, one-\npass LLM inference.\nA prompt template (Ap-\npendix A.1) was carefully curated based on the task\ndefinition and provided note-level gold annotations,\nencompassing detailed task instructions, in-context\nexamples, and formatting requirements for a struc-\ntured output. Each clinical note was appended to\nthe prompt without preprocessing. LLMs generate\nextracted chemotherapy events Ti from each note\nxi as a JSON array.\n3.2\nThinking\nRecent advances have shown improved reasoning\nand end-task performance when enabling a chain-\nof-thought (CoT) before generating answers (Wei\net al., 2023). In light of this, we enabled the think-\ning mode of the models in the prompting baseline\nusing the same prompt to explore whether CoT\ncould improve the timeline extraction.\nDuring error analysis, we observed text span\ndiscrepancies between LLM extractions and note-\nlevel gold-standard annotations, which sometimes\nresulted in false negatives in exact match evalua-\ntion. Therefore, we further designed the follow-\ning postprocessing rules for our prediction submis-\nsion based on the thinking method: (1) for SACT\nnames containing \"chemo\", remove all descriptors\nbefore them, such as \"adjuvant\" and \"neoadjuvant\";\n"}, {"page": 3, "text": "(2) for SACT names combined with a slash (e.g.,\n\"Doxorubicin/Cyclophosphamide\"), split them into\nseparate events; (3) remove unnecessary words in\ntime expressions, such as \"approximately\", \"about\",\n\"around\", and \"in\". We do not include the postpro-\ncessing step in development set evaluation results\n(Table 1) for a fairer initial evaluation.\n3.3\nDictionary-Enhanced Extraction\nWe rethought and adapted the approach used by the\nLAILab team in last year’s Task 2 (Haddadan et al.,\n2024), structuring it into a three-step pipeline.\nStep 1: Dictionary-based chemotherapy event\nextraction.\nGiven a clinical note, we first ap-\nplied a self-constructed chemotherapy dictionary\nfor keyword matching. All matches were tagged\nwith <e> and </e>. The dictionary was built from\nthree sources: (1) HemOnc.org2 , where we created\nseparate dictionaries for breast cancer, melanoma,\nand ovarian cancer including regimen names, drug\nnames, and abbreviations; abbreviations with only\ntwo letters were removed to reduce false posi-\ntives (e.g., \"AT\"); (2) generic mentions such as\n\"chemotherapy\" and \"chemo\" from the baseline\nsystem3; and (3) annotated chemotherapy mentions\nfrom the training and development sets of Subtask\n1. No test set annotations were used. Only the\ndrug names were incorporated into the dictionary;\nno labeled spans or relations were carried over to\nSubtask 2. The complete dictionary is provided in\nAppendix B.\nStep 2: LLM-based double checking and aug-\nmentation. Sentences containing dictionary tags\nwere passed to the Qwen-3 Thinking model for\nverification, which reduced false positives and re-\ncovered false negatives. The prompt template is in\nAppendix A.2.\nStep 3: Context-enhanced relation extraction.\nFor each verified sentence, we constructed a win-\ndow of the anchor sentence plus its preceding and\nfollowing sentences. This context was fed into\nQwen-3 for generating chemotherapy–time rela-\ntion triplets.\nThe motivation for using a local\nwindow was efficiency: fewer than 6% of sen-\ntences in the dataset contain SACT annotations\n(Table S1), and chemotherapy events and time ex-\npressions generally appear within two consecutive\nsentences. Based on these observations, we modi-\nfied the baseline system’s prompt for local sentence-\n2https://hemonc.org/wiki/Main_Page\n3https://github.com/HealthNLPorg/\nchemoTimelinesEval\nlevel relation extraction to improve efficiency (Ap-\npendix A.3).\n3.4\nSupervised Fine-Tuning (SFT)\nMotivated by LAILab’s success in using SFT in\nthe previous challenge (Haddadan et al., 2024), we\nperformed SFT to adapt LLMs for note-level ex-\ntraction using the provided gold annotations. Input\nprompts were structured using the same template\nas our prompting baseline. For the training targets,\nthe output for each note was serialized into a JSON\nobject containing a list of dictionaries. Each dictio-\nnary represented a single extracted event with three\nmandatory keys: \"SACT\", \"relation\", and \"time\".\nOur SFT approach differs from the method pro-\nposed by LAILab (Haddadan et al., 2024) in three\nkey aspects. First, their method operates at the\nsentence level, providing the model with only a\ntarget sentence and its immediate neighbors as con-\ntext. In contrast, our note-level approach allows\nthe model to leverage the richer contextual infor-\nmation present in the entire clinical note. Second,\nfor output generation, they employed a specialized\ntriplet linearization algorithm (Huguet Cabot and\nNavigli, 2021). We adopt a potentially more flex-\nible strategy by serializing the extracted relations\ninto a structured JSON object. Finally, while their\nbest performance was achieved by finetuning Flan-\nT5-XXL (Chung et al., 2022), we scale up to a\n14B-parameter model from Qwen3 (Yang et al.,\n2025), a more recent and advanced model family.\n3.5\nDirect Preference Optimization (DPO)\nRecent work suggests that models trained via SFT\ntend to memorize the training data, while subse-\nquent training with reinforcement learning can en-\nhance generalization and alignment with human\npreference (Chu et al., 2025). Motivated by this,\nwe framed the note-level extraction task as a prefer-\nence alignment problem. Specifically, we defined\nthe preference as: (1) the extraction is expected\nto align with the style in gold annotations and (2)\nthe note-level extraction may favor outputs with\nhigher recall over precision, operating on the as-\nsumption that the downstream timeline aggregation\nprocess would manage deduplication and resolve\nconflicting extractions.\nTo implement this, we employed an iterative\nDPO approach to construct a preference dataset and\nrefine the policy model (Zhang et al., 2025; Tu et al.,\n2025; Rafailov et al., 2024). First, we warmed\nup a policy model by training it for 5 epochs via\n"}, {"page": 4, "text": "SFT. Next, to generate preference pairs, we used\nthis initial model to produce 8 candidate outputs\nfor each instance in the training set. For each set\nof candidates, we identified the output with the\nhighest recall as the chosen response (yw) and the\none with the lowest recall as the rejected response\n(yl). This process yielded a preference dataset of\npairs where the chosen and rejected responses were\ndistinct. We then further trained the SFT warmup\nmodel using DPO (Rafailov et al., 2024) on this\ndataset.\n3.6\nEnsemble Method\nFor the ensemble method, we concatenated note-\nlevel predictions generated by three models: SFT,\nSFT + DPO, and Thinking + Postprocessing. These\ncombined predictions were then passed through\nthe normalization and aggregation pipeline to pro-\nduce final patient-level timelines. We excluded the\ndictionary-enhanced extraction approach from the\nensemble because its pipeline differs substantially\nfrom the other methods and introduces systematic\nfalse positives due to keyword matching.\n4\nTimeline Aggregation\nSimilar to how the task organizer constructed gold\ntimelines automatically (Yao et al., 2024), all note-\nlevel system outputs underwent two subsequent\nsteps, normalization and aggregation, to obtain\npatient-level timelines as final outputs.\nIn the normalization step, time expressions in\nnote-level outputs were converted into standard-\nized ISO time using the CLUlab’s Timenorm syn-\nchronous context-free grammar module (Bethard,\n2013). The original Timenorm was written in Scala,\nwe reproduced its core functions in Java.\nThe\n\"DOCTIME\" of each note was used as a temporal\nanchor for relative time expressions extracted from\nthat note. Such \"DOCTIME\" was identified via a\nregular expression that detects eight consecutive\ndigits in the note text. Relative time expressions\nthat could not be normalized by Timenorm were\ndiscarded.\nNormalized events were then de-duplicated and\naggregated using the official aggregation script\n(docker_output_to_timeline.py) provided by\nthe task organizer4.\n4https://github.com/HealthNLPorg/\nchemoTimelinesEval\n5\nExperimental Setup\n5.1\nModels\nOur experiments for note-level event extraction\nutilized the following open-source LLMs: Qwen3\nseries of general-purpose dense models (Yang et al.,\n2025) from 4B to 32B, plus a mixture-of-experts\nmodel Qwen3-30B-A3B (2507), and Google’s spe-\ncialized model for medicine, MedGemma-27B\n(Sellergren et al., 2025). All models were obtained\nfrom Hugging Face.\nWe used the vllm package under Python 3.10 for\nLLM inference, and LlamaFactory for LLM fine-\ntuning. Sampling parameters for LLM inference\nfollowed the setting recommended by the Qwen3\nteam: temperature=0.6, top_p=0.95, top_k=20,\nand min_p=0. The default maximum output length\n(max_tokens) was set to 4,096. Up to 4 NVIDIA\nA100 GPUs were utilized for either model infer-\nence or model training.\nFor dense Qwen3 models, the thinking mode\nwas enabled by setting the enable_thinking pa-\nrameter to True, and max_tokens was changed to\n20,480 to allow complete outputs; for Qwen3-30B-\nA3B (2507), the non-thinking model and the think-\ning model are two separate models.\nFor SFT, we turned off the thinking mode of the\nQwen3 model and employed LoRA for parameter-\nefficient finetuning of the model over 10 epochs.\nFor DPO, we obtained preference datasets of 9\npairs for Qwen3-14B, 27 pairs for Qwen3-8B, and\n30 pairs for Qwen3-4B. We trained with DPO for\n10 epochs.\nDespite the small sample size, we\nobserved a consistent improvement in reward ac-\ncuracy during training (Figure 1), which aligns\nwith recent studies demonstrating effective rein-\nforcement learning from a limited number of sam-\nples (Wang et al., 2025).\n5.2\nSystem Evaluation\nAs instructed, system performance was assessed us-\ning the strict matching criterion, where all compo-\nnents of a predicted triplet must exactly match the\ncorresponding gold standard triplet to be counted\nas correct. For the development set, evaluation\nwas performed locally using the official evaluation\nscript (eval_timeline.py) provided by the shared\ntask organizers4. Since all pipelines followed 2 ma-\njor steps as described in Section 2, we additionally\ncalculated note-level micro precision, recall, and F1\nas intermediate metrics for LLM extraction perfor-\nmance. Evaluation on the test set was performed by\n"}, {"page": 5, "text": "the task organizer and announced using the leader-\nboard on the shared task website1.\nTwo types of patient-level F1 scores were cal-\nculated by the official evaluation script: Type A,\nwhich includes all patients regardless of whether\nthey have gold-standard timelines, and Type\nB, which includes only patients with confirmed\nchemotherapy timelines (Yao et al., 2025). The of-\nficial score is the average of the Type A and Type B\nF1 scores, where each patient’s score is computed\nindividually and then averaged across patients.\n6\nResults\n6.1\nDevelopment Performance\nTable 1 shows both note-level and timeline-level\nevaluation results of each method-model combina-\ntion. Major findings are as follows.\nFirst, better note-level performance is gener-\nally associated with, but does not necessarily indi-\ncate, better final performance on the timeline level,\nwhich highlights the crucial role of normalization\nand aggregation. For example, under the thinking\napproach, Qwen3-30B-A3B achieved much better\nmicro precision, recall, and F1 than Qwen3-14B\nand Qwen3-32B, but their official scores were al-\nmost the same. This could be attributed to the dedu-\nplication of repeated events from different notes\nduring event aggregation, and time expressions that\nTimenorm was unable to handle. Another note-\nworthy case is Qwen3-14B’s exceptionally high\nofficial score under the baseline approach. Similar\nto other models, its note-level extractions contain a\nsubstantial proportion of irregular time expressions\n(see Section 7 for examples), but those events were\nluckily discarded by Timenorm, leading to a high\nF1 score.\nSecond, 14B might be the best dense model\nsize for this task. Under both the baseline and\nthinking approaches, the 14B model outperformed\nother dense models on both the note level and the\npatient level. In light of this, we applied dictionary-\nbased and fine-tuning-based methods to models\nup to 14B. As expected, Qwen3-14B consistently\noutperformed its 8B and 4B siblings.\nThird, fine-tuning a dense model reliably\nyielded the largest performance gain, while a\nthinking mixture-of-experts model performed\ncomparably. After introducing rule-based post-\nprocessing (described in Section 3), Qwen3-30B-\nA3B’s official F1 score on the development set\nimproved from 0.596 to 0.625 (Table 2). We at-\ntribute most of the performance gain of thinking to\nthe self-checking behavior exhibited by the CoT,\nwhich improved both the precision and recall of\nnote-level event extraction. For details and exam-\nples, please see Section 7.\nFourth,\nthe dictionary-enhanced method\nachieved the highest recall among all systems,\nand incorporating LLM verification further\nimproved precision by filtering out false posi-\ntives. We examined the intermediate results of\nLLM verification on top of dictionary-based sen-\ntence tagging.\nAs shown in Table S2 in Ap-\npendix C, dictionary tagging alone achieved nearly\nperfect recall across cancer types, but precision\nwas lower. Adding LLM verification consistently\nincreased precision for these sentences (e.g., breast:\n0.732→0.824; melanoma: 0.802→0.830) while\nkeeping recall near 1.0 (a small trade-off appears\nfor ovarian, 1.000→0.994, with F1 unchanged).\nOverall, the dictionary-based pipeline attains the\nbest recall among all systems, and the second-best\ndevelopment performance, trailing only SFT.\n6.2\nTest Performance\nOn the test set, our SFT approach (submission 1)\nattained the highest overall average score of 0.678.\nThe SFT + DPO model (submission 2) closely\nfollowed with an average of 0.666. The think-\ning approach with postprocessing (submission 4)\nalso performed competitively, reaching an average\nscore of 0.644. The ensemble method (submission\n5), which combined SFT, SFT + DPO, and think-\ning achieved an overall score of 0.603, which was\nlower than any of the individual model. This sug-\ngests that differences in error patterns limited the\nbenefit of ensembling. The dictionary-enhanced\nsentence-level extraction (submission 3) produced\nweaker results, with an overall score of only 0.545,\nsuggesting potential limitations in the term cover-\nage of our SACT dictionary with respect to what ap-\npears in the test set. Together, test results again indi-\ncate that fine-tuning-based methods were the most\neffective in our experiments, while LLM thinking\nwas also a competitive approach.\n7\nError Analysis\nWe empirically investigated noteworthy errors\nmade by our systems on the development set, aim-\ning to inform both future system development and\npotential refinements to the challenge design in\nsubsequent rounds. Gold-standard timelines of the\n"}, {"page": 6, "text": "Method\nModel\nNote-Level Micro\nTimeline-Level Macro F1\nPrecis.\nRecall\nF1\nType A\nType B\nOfficial\nPrompting\nQwen3-4B\n.039\n.278\n.069\n.173\n.082\n.127\nBaseline\nQwen3-8B\n.040\n.283\n.070\n.060\n.103\n.082\nQwen3-14B\n.139\n.276\n.185\n.466\n.370\n.418\nQwen3-32B\n.103\n.209\n.138\n.253\n.220\n.236\nQwen3-30B-A3B\n.068\n.243\n.106\n.104\n.178\n.141\nMedGemma-27B\n.085\n.439\n.142\n.158\n.199\n.178\nThinking\nQwen3-4B\n.338\n.382\n.358\n.471\n.378\n.424\nQwen3-8B\n.355\n.335\n.345\n.614\n.410\n.512\nQwen3-14B\n.517\n.346\n.415\n.676\n.515\n.595\nQwen3-32B\n.355\n.325\n.339\n.623\n.568\n.596\nQwen3-30B-A3B\n.600\n.468\n.526\n.676\n.516\n.596\nDictionary +\nQwen3-8B\n.294\n.509\n.372\n.689\n.468\n.578\nSentence-level\nQwen3-14B\n.434\n.657\n.522\n.729\n.536\n.632\nSFT\nQwen3-4B\n.379\n.507\n.434\n.651\n.473\n.562\nQwen3-8B\n.419\n.569\n.483\n.650\n.542\n.596\nQwen3-14B\n.397\n.615\n.483\n.711\n.577\n.644\nDPO + SFT\nQwen3-4B\n.390\n.483\n.431\n.670\n.435\n.553\nQwen3-8B\n.409\n.574\n.478\n.651\n.545\n.598\nQwen3-14B\n.401\n.620\n.487\n.695\n.549\n.622\nTable 1: Development set performance. The best official score under each method is highlighted.\nSubmission #\nMethod\nDev Official\nTest Official\nSubmission 1\nSFT\n.644\n.678\nSubmission 2\nSFT + DPO\n.622\n.666\nSubmission 3\nDictionary + Sentence-level\n.632\n.545\nSubmission 4\nThinking + Postprocessing\n.625\n.644\nSubmission 5\nEnsemble of 1, 2, and 4\n.562\n.603\nTable 2: Development and test set performance of final submissions. The best scores are highlighted.\ntest set are held private by the organizer to enable\nfuture versions of the shared task, hence we are\nunable to perform error analysis on the test set.\n7.1\nErrors in Prompt-Based Extraction\nUnder the prompting baseline, LLMs often extract\nmedications and procedures that are not part of\nSACT (e.g., \"Neupogen\", \"ProHance\", \"MRI\") and\nirregular time expressions (e.g., \"04/26/2012 at\n12:13 PM\", \"4 cycles\", \"midway through chemo\"),\neven when explicitly instructed not to. They also\nproduce errors related to text span boundaries and\nformatting. This explains the overly low precision\nand recall values on the note level.\nWhen thinking mode is enabled, we observed\nthat Qwen3 models would spontaneously check\nwhether each candidate event belonged to SACT\n(e.g., \"Neulasta is a G-CSF, not an SACT, so it’s\nexcluded\") and whether its associated time ex-\npressions satisfied the extraction instructions (e.g.,\n\"’status post’ refers to something that happened\nin the past but doesn’t give an exact time\"), which\nsignificantly reduced false positives. This double-\nchecking behavior also helped decrease false nega-\ntives (e.g., \"Double-checking to make sure I didn’t\nmiss any hidden events. Maybe ...\").\nMost remaining errors produced by the thinking\nmodels were commonly encountered by other meth-\nods, and are discussed in the following sections. A\nnoteworthy category is incorrect inference caused\nby ambiguous language or formatting in clinical\nnotes. For example, consider a note containing a\ntable of medications administered on a given date\nwith the SACT note \"TRASTUZUMAB\" followed\nby \"None Entered\"; based on the column names in\nthe context, this means that the start or end date of\nthe therapy is not entered. However, Qwen3-30B-\nA3B interpreted this to mean that even though the\n"}, {"page": 7, "text": "date was present, it should be excluded because\nthe time was missing. We also observed confusion\nabout whether to include scheduled events. Our\nmanual inspection of the gold standard timelines\nrevealed that scheduled events were inconsistently\nannotated. In addition, a large proportion of clinical\nnotes in the dataset contained no gold annotation,\nand any LLM-extracted events from these notes\nwould become false positives.\n7.2\nErrors in Dictionary-Based Extraction\nThe dictionary-enhanced extraction approach pro-\nvided strong coverage of chemotherapy mentions\nbut also revealed several important limitations.\nFirst, the main limitation came from false posi-\ntives, which lowered overall precision. Because\nthe method matched any token found in the dictio-\nnary, it might incorrectly identify unrelated terms\nas chemotherapy mentions. For example, the sys-\ntem recognized the word \"FEC\" in \"Normal FEV1\nand FEV1/FEC ratio\" as the regimen consisting of\nFluorouracil, Epirubicin, and Cyclophosphamide,\nalthough it was in fact a typographical error for\nForced Vital Capacity (FVC).\nSecond, the system also suffered from false neg-\natives when encountering typographical errors or\nabbreviations not present in the dictionary. For in-\nstance, the test set included terms such as \"bev\"\nfor Bevacizumab and \"interfuron\" for interferon.\nThese variants were not captured, leading to missed\nextractions. This limitation helps explain the per-\nformance gap between the development set, where\ndictionary coverage was stronger, and the test set,\nwhere more novel variants appeared.\nFinally, we observed interesting cases of internal\ninconsistency between the model’s intermediate\nreasoning and its final output. For instance, in the\nthinking process, the model may explicitly state\nthat a tag such as <e>tc</e> should be removed,\nbut in the final output the tag still appears (see\nAppendix D for an example). This mismatch sug-\ngests that controlling the alignment between reason-\ning and output remains a challenge for dictionary-\nenhanced extraction with LLMs. It also points the\nway to potentially useful future work in explainable\nAI to use reasoning traces to better understand how\nLLMs understand complex clinical notes.\n7.3\nErrors in Training-Based Methods\nAs we pooled all available annotations into the\nsame training set, the performance of our training-\nbased model is sensitive to imbalances in the train-\ning data. For instance, the model often defaults\nto the generic CONTAINS-1 relation, misclassi-\nfying more specific BEGINS-ON and ENDS-ON\nrelations. This tendency reflects a class imbalance\nwhere CONTAINS-1 instances are overrepresented\nin the training set. Furthermore, we observed a no-\ntable performance degradation on melanoma notes\ncompared to breast and ovarian cancer, which man-\nifests as low precision. This may be a consequence\nof data skew, as our training set contains signifi-\ncantly fewer melanoma notes, potentially leading\nthe model to overfit to the majority of cancer types.\n7.4\nErrors in Normalization and Aggregation\nThe normalization process relies heavily on heuris-\ntic rules in the Timenorm pipeline, which can both\nimprove alignment with the gold standard and intro-\nduce systematic errors. In general, using Timenorm\nfacilitates consistent normalization of relative time\nexpressions, but we found instances where the out-\nput diverged from expected interpretations. For\nexample, the expression \"last week\" relative to\na document time of 2013-01-15 is normalized to\n\"2013-01-08\" in full date format (YYYY-MM-DD),\nwhereas \"next week\" relative to 2013-07-23 is nor-\nmalized to \"2013-w31\" in week format (YYYY-\nw##).\nLikewise, expressions that specify only\nmonth and day can be incorrectly anchored to the\nprevious year. If the document time is 2013-02-10,\nthe expression \"January 9\" is normalized as \"2012-\n01-09\" rather than the correct \"2013-01-09\". Such\ninconsistencies suggest that while Timenorm is\npowerful, it may require task-specific adjustments\nto handle edge cases in clinical timelines.\nIn aggregation, the lack of entity consolidation\nintroduces redundancy and inconsistency across pa-\ntient timelines. For example, the same chemother-\napy drug can appear under slightly different surface\nforms, such as \"il2\", \"il-2\", and \"interleukin-2\", all\nlinked to the same date and relation. Similarly,\nregimen-level mentions can coexist with individual\ndrug mentions. The gold standard may annotate\nboth \"AC-T\" as a regimen and its components Adri-\namycin (A), Cyclophosphamide (C), and Taxol (T),\nleading to multiple overlapping entries.\nA further source of discrepancy arises from how\nstart and end events are aligned within the same\ntimeline. When both BEGINS-ON and ENDS-\nON relations are identified for the same drug on\nthe same date, our system retains both events for\ncompleteness, whereas the gold timelines may\narbitrarily keep only one. For example, in the\n"}, {"page": 8, "text": "gold timeline, Cabotaxol and Taxol are annotated\nas [cabotaxol, BEGINS-ON, 2012-01-12] and\n[taxol, ENDS-ON, 2011-12-15]. The complete\nrepresentation, however, should include both start\nand end events for each drug, i.e., 4 events in total.\n8\nConclusions and Discussion\nExtracting clinical events from unstructured notes\nhas always been a challenging task (Olex and\nMcInnes, 2021). Under the ChemoTimelines 2025\nshared task framework, our work explores several\napproaches based on modern and emerging model\ntraining and inference techniques. Major findings\nare as follows:\n1. The aggregation of note-level events into patient-\nlevel timelines is crucial for the final perfor-\nmance of a system.\n2. Fine-tuning a dense model, especially of size\n14B, reliably yielded the largest performance\ngain, while a thinking mixture-of-experts model\nperformed comparably.\n3. The dictionary-enhanced method achieved the\nbest recall, while LLM verification improved\nprecision by reducing false positives.\nWe found that the dictionary-based approach of-\nfered both efficiency and interpretability, while still\nmaintaining acceptable performance despite some\ninformation loss at the sentence level. Instead of\nreasoning over all notes in the development set, the\nmethod reduced the burden by restricting LLM ver-\nification to a much smaller number of candidate\nsentences flagged by the dictionary, plus context-\nenhanced sentences for relation extraction. This\nsubstantially lowered input token volume, reason-\ning time, and computational cost. Although focus-\ning on sentences inevitably sacrifices some contex-\ntual information compared with note-level extrac-\ntion, the resulting performance remained strong,\nsupported by very high recall from dictionary tag-\nging and improved precision from LLM verifica-\ntion. Moreover, the transparent matching rules en-\nhance interpretability and facilitate systematic re-\nfinements, such as synonym expansion or ontology-\nbased extensions. Together, these features make the\ndictionary-based pipeline a lightweight, resource-\nefficient, and interpretable complement to learning-\nbased systems.\nThe comparable performance of the training-\nfree LLM thinking approach and fine-tuning-based\nmethods suggests a potential cost-effectiveness\ntrade-off for this specific task. Once fine-tuning is\nsupported by a sufficient amount of high-quality\ndata, it is capable of yielding a trustworthy per-\nformance gain while maintaining the speed of di-\nrect output generation. In contrast, CoT think-\ning, as a core component of recent inference-time\nscaling techniques for LLMs, is characterized by\nits higher latency at test time, despite that it re-\nquires much less data annotation in the develop-\nment phase. Given the substantial performance\ngain from LLM thinking in our experiments (and\npotential benefits to explainability), we recommend\nthat future attempts on similar tasks consider in-\ncluding it as a baseline method, especially in con-\nsideration of the high cost of EHR annotation by\nhuman experts.\nThe ensemble method did not lead to perfor-\nmance gains. Instead, the overall score was lower\nthan any of the individual models. This suggests\nthat errors from different systems tend to accumu-\nlate when combined, and these mistakes cannot be\neffectively corrected through the normalization and\naggregation pipelines. As a result, simple ensem-\nbling is not a viable strategy for this task.\nWe conceived several other methods that were\nnot implemented due to time and resource con-\nstraints, and we hope providing them here may\nbenefit clinical timeline extraction. First, our cur-\nrent prompt-based approaches (both non-thinking\nand thinking) utilized static ad hoc in-context exam-\nples. Including dynamically-retrieved training ex-\namples related to test time queries has the potential\nto further improve performance. Second, current\ntraining methods do not have an explicit reasoning\nprocess before generating the extractions. Future\nmethods may synthesize reasoning data through\nrejection sampling (Yuan et al., 2023) or apply re-\ninforcement learning (DeepSeek-AI et al., 2025)\nfor better performance.\nThe chemotherapy events in the training data\nrepresented the specific EHR documentation style\nof the source facilities and systems. The more\ngeneral task of extracting clinical event timelines\nmay involve a diversity of local documentation\nstyles, event sources (e.g., treatment, laboratory,\nbilling, etc.), and levels of standardization.\nAdditional insights into the capabilities and\nweaknesses of various LLM-based strategies might\nbe obtained with introspection into performance\nagainst specific evaluation data examples, addi-\ntional layers of case review with expert oncologists,\nand testing with the newest generation of emerging\nLLMs.\n"}, {"page": 9, "text": "Limitations\nOur methods were highly customized to the Chemo-\nTimelines challenge, hence our findings may not\ngeneralize well to other clinical extraction tasks.\nDue to time, resource, and privacy constraints,\nwe did not assess a full range of contemporary\nopen- and closed-source LLMs (e.g., larger Qwen3\nmodels, the Llama series, GPT series, etc.), there-\nfore our findings may not generalize. MedGemma\nwas also the only medicine-specialized LLM in-\ncluded in our experiments. Although a general-\npurpose LLM combined with a tailored aggrega-\ntion pipeline was sufficient for this task, future\nwork may benefit from models more familiar with\nclinical notes. For technical methods that we con-\nceptualized but did not have a chance to implement\nand test, please refer to Section 8.\nReferences\nSteven Bethard. 2013.\nA synchronous context free\ngrammar for time normalization. In Proceedings\nof the 2013 Conference on Empirical Methods in Nat-\nural Language Processing, pages 821–826, Seattle,\nWashington, USA. Association for Computational\nLinguistics.\nTianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang\nTong, Saining Xie, Dale Schuurmans, Quoc V. Le,\nSergey Levine, and Yi Ma. 2025. Sft memorizes,\nrl generalizes: A comparative study of foundation\nmodel post-training. Preprint, arXiv:2501.17161.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert\nWebson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex\nCastro-Ros, Marie Pellat, Kevin Robinson, and 16\nothers. 2022. Scaling instruction-finetuned language\nmodels. Preprint, arXiv:2210.11416.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,\nXingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-\nhong Shao, Zhuoshu Li, Ziyi Gao, and 181 others.\n2025. Deepseek-r1: Incentivizing reasoning capa-\nbility in llms via reinforcement learning. Preprint,\narXiv:2501.12948.\nMaryam Gholipour, Reza Khajouei, Parastoo Amiri,\nSadrieh Hajesmaeel Gohari, and Leila Ahmadian.\n2023. Extracting cancer concepts from clinical notes\nusing natural language processing: a systematic re-\nview. BMC bioinformatics, 24(1):405.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare, 3(1):1–23.\nShohreh Haddadan, Tuan-Dung Le, Thanh Duong, and\nThanh Thieu. 2024.\nLAILab at chemotimelines\n2024: Finetuning sequence-to-sequence language\nmodels for temporal relation extraction towards can-\ncer patient undergoing chemotherapy treatment. In\nProceedings of the 6th Clinical Natural Language\nProcessing Workshop, pages 382–393, Mexico City,\nMexico. Association for Computational Linguistics.\nPere-Lluís Huguet Cabot and Roberto Navigli. 2021.\nREBEL: Relation extraction by end-to-end language\ngeneration. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, pages 2370–\n2381, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. Preprint,\narXiv:2001.08361.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\nPreprint, arXiv:1910.13461.\nAmy L. Olex and Bridget T. McInnes. 2021. Review of\ntemporal reasoning in the clinical domain for timeline\nextraction: Where we are and where we need to be.\nJournal of Biomedical Informatics, 118(103784).\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D. Manning, and Chelsea Finn.\n2024.\nDirect preference optimization: Your lan-\nguage model is secretly a reward model. Preprint,\narXiv:2305.18290.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri,\nAtilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau,\nand 1 others. 2025. Medgemma technical report.\narXiv preprint arXiv:2507.05201.\nWeiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.\nTemporal reasoning over clinical text: the state of\nthe art. Journal of the American Medical Informatics\nAssociation, 20(5):814–819.\nWeiyi Sun, Anna Rumshisky, and Ozlem Uzuner.\n2015.\nNormalization of relative and incomplete\ntemporal expressions in clinical narratives.\nJour-\nnal of the American Medical Informatics Association,\n22(5):1001–1008.\nYukun Tan, Merve Dede, and Ken Chen. 2024. KCLab\nat chemotimelines 2024: End-to-end system for\nchemotherapy timeline extraction – subtask2.\nIn\nProceedings of the 6th Clinical Natural Language\n"}, {"page": 10, "text": "Processing Workshop, pages 417–421, Mexico City,\nMexico. Association for Computational Linguistics.\nSongjun Tu, Jiahao Lin, Xiangyu Tian, Qichao Zhang,\nLinjing Li, Yuqian Fu, Nan Xu, Wei He, Xiangyuan\nLan, Dongmei Jiang, and Dongbin Zhao. 2025.\nEnhancing llm reasoning with iterative dpo:\nA\ncomprehensive empirical investigation.\nPreprint,\narXiv:2503.12854.\nLiwei Wang, Qiuhao Lu, Rui Li, Sunyang Fu, and Hong-\nfang Liu. 2024. Wonder at chemotimelines 2024:\nMedTimeline: An end-to-end NLP system for time-\nline extraction from clinical narratives. In Proceed-\nings of the 6th Clinical Natural Language Processing\nWorkshop, pages 483–487, Mexico City, Mexico. As-\nsociation for Computational Linguistics.\nLiwei Wang, Jason Wampfler, Angela Dispenzieri, Hua\nXu, Ping Yang, and Hongfang Liu. 2020. Achiev-\nability to extract specific date information for cancer\nresearch. In AMIA Annual Symposium Proceedings,\nvolume 2019, page 893.\nYiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren,\nLiyuan Liu, Baolin Peng, Hao Cheng, Xuehai He,\nKuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang\nWang, Simon Shaolei Du, and Yelong Shen. 2025.\nReinforcement learning for reasoning in large lan-\nguage models with one training example. Preprint,\narXiv:2504.20571.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models. Preprint,\narXiv:2201.11903.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Day-\niheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao\nGe, Haoran Wei, Huan Lin, Jialong Tang, and 41\nothers. 2025.\nQwen3 technical report.\nPreprint,\narXiv:2505.09388.\nJiarui Yao, Harry Hochheiser, WonJin Yoon, Eli Gold-\nner, and Guergana Savova. 2024. Overview of the\n2024 shared task on chemotherapy treatment timeline\nextraction. In Proceedings of the 6th Clinical Natu-\nral Language Processing Workshop, pages 557–569,\nMexico City, Mexico. Association for Computational\nLinguistics.\nJiarui Yao, Harry Hochheiser, WonJin Yoon, Eli Gold-\nner, and Guergana Savova. 2025. Overview of the\n2025 shared task on chemotherapy treatment time-\nline extraction. In Proceedings of the 7th Clinical\nNatural Language Processing Workshop.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting\nDong, Keming Lu, Chuanqi Tan, Chang Zhou, and\nJingren Zhou. 2023. Scaling relationship on learning\nmathematical reasoning with large language models.\nPreprint, arXiv:2308.01825.\nHanning Zhang, Jiarui Yao, Chenlu Ye, Wei Xiong,\nand Tong Zhang. 2025. Online-dpo-r1: Unlocking\neffective reasoning without the ppo overhead. Notion\nBlog.\nJeff Zhang, Yin Aphinyanaphongs, and Anthony\nCardillo. 2024. NYULangone at chemotimelines\n2024: Utilizing open-weights large language models\nfor chemotherapy event extraction. In Proceedings of\nthe 6th Clinical Natural Language Processing Work-\nshop, pages 428–430, Mexico City, Mexico. Associa-\ntion for Computational Linguistics.\nA\nPrompt Templates\nA.1\nPrompt for Baseline and Thinking\nYou are an experienced medical annotator tasked\nwith\nextracting\nsystemic\nanticancer\ntherapy\n(SACT) events from a given clinical note.\nWHAT A SACT IS:\nSACT encompasses medications used in traditional\ncytotoxic\nchemotherapy,\nendocrine\ntherapy,\ntargeted therapy, and immunotherapy.\nSACT\nmay appear in generic names (e.g., Anastrozole),\nbrand names (e.g., Arimidex), or combined names\n(e.g., TCH). Non-specific SACT mentions such\nas \"chemotherapy\" or \"chemo\" should also be\nincluded.\nExclude therapies, medications, and\ndiagnostic procedures used not for anticancer\npurposes, such as dietary supplements and biopsies.\nExclude therapy candidates that you don’t know\nwhat they are or aren’t sure if they are SACT.\nWHAT YOU SHOULD EXTRACT:\nYou should only extract SACT events that are\nexplicitly associated with specific time expressions.\nHere are some format examples of time expres-\nsions you are expected to extract: \"December 27,\n2011\", \"May 21st, 2013\", \"7/20/2012\", \"today\",\n\"3 weeks ago\", \"1 year\". Ignore nonspecific time\nmentions such as cycle or dose numbers (e.g.,\n\"cycle 1 of 6\" and \"1/6 dose\") and ambiguous\nrelative time (e.g., \"midway through\" or \"at the\nsame time as\" another event).\nExclude SACT\nmentions without an associated time.\nAdditionally,\nfor each event,\nselect a rela-\ntion label from BEGINS-ON, ENDS-ON, and\nCONTAINS-1 to indicate the relation between the\nSACT and its time based on the note’s language.\nCONTAINS-1 means the SACT happened at a\nspecific time; if the note explicitly mentions the\nstart or end of an event, use BEGINS-ON or\nENDS-ON. For example, if the note says \"She\n"}, {"page": 11, "text": "received Herceptin on May 21st, 2013\", your\nextracted event will be \"Herceptin\", \"CONTAINS-\n1\", \"May 21st, 2013\"; if the note says \"Start\nipilimumab on today’s date\", your extracted event\nwill be \"ipilimumab\", \"BEGINS-ON\", \"today\".\nIf a SACT is associated with multiple time\npoints, extract them as separate events.\nFor\nexample, if the note says \"Herceptin was initiated\non 12/27/2011 and completed on April 10, 2012\",\nyou should extract two events: one is \"Herceptin\",\n\"BEGINS-ON\", \"12/27/2011\", and the other\nis \"Herceptin\", \"ENDS-ON\", \"April 10, 2012\".\nSimilarly, if multiple SACTs are associated with\nthe same date, you should also extract them as\nseparate events.\nHOW\nYOU\nSHOULD\nFORMAT\nYOUR\nRESPONSE:\nSACT names and their associated time expressions\nshould be kept exactly as they appear, even if there\nis a typo. Do not alter them, normalize the time\nexpression, or infer the exact date. For example, if\na SACT event appears in the note as \"Alibercept\nreceived yesterdat\", your extracted event will be\n\"Alibercept\", \"CONTAINS-1\", \"yesterdat\".\nDo not combine SACT mentions that refer\nto the same therapy but appear in different\nnames, even if one appears in parentheses as\nthe alternative name for another; extract them\nseparately. For example, if there are three SACT\nevents, \"il-2\", \"il2\", and \"interleukin-2\", and all\nhave corresponding time expressions, treat them\nas three separate SACT events; if the note says\n\"TRASTUZUMAB\n(HERCEPTIN)\nreceived\ntoday\", you should extract two events: one is\n\"TRASTUZUMAB\", \"CONTAINS-1\", \"today\",\nand the other is \"HERCEPTIN\", \"CONTAINS-1\",\n\"today\".\nIf multiple SACTs are administered together,\ntreat them as separate events. For example, for\n\"Doxorubicin/Cyclophosphamide\" you should\nextract 2 events, one for Doxorubicin and the other\nfor Cyclophosphamide. However, if a SACT name\nis already a combined treatment name (e.g., TCH),\ntreat it as a single event.\nIgnore\nsupplementary\ndescriptors\nof\nSACT\nnames, such as dose (e.g., \"high dose\") and\nadministration method (e.g., \"IV\").\nFor non-\nspecific SACT like \"chemotherapy\" or \"chemo\",\nignore their descriptors, such as \"adjuvant\" and\n\"neoadjuvant\".\nYour response must be a JSON array under\nthe following schema:\n{\n\"type\": \"array\",\n\"description\": \"An array of SCAT events\nextracted from the clinical note.\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"SACT\": {\n\"description\": \"A SACT name\nextracted as it is.\",\n\"type\": \"string\"\n},\n\"relation\": {\n\"description\": \"The relation\nbetween the SACT and its associated time\nexpression.\nMust\nbe\none\nof\nBEGINS-ON,\nENDS-ON, and CONTAINS-1.\",\n\"type\": \"string\"\n}\n\"time\": {\n\"description\": \"The time expression\nassociated with the SACT, extracted as it\nis.\",\n\"type\": \"string\"\n}\n},\n\"required\":\n[\"SACT\",\n\"relation\",\n\"time\"],\n}\n}\nIf there is no SACT event in the clinical note,\nreturn an empty array.\nNow, extract SACT events from the follow-\ning clinical note:\n{note}\nA.2\nPrompt for LLM-based Chemotherapy\nTag Verification\nYou are an experienced medical annotator tasked\nwith verifying and extracting systemic anticancer\ntherapy (SACT) mentions from a given clinical\nnote. Some SACT candidates have already been\ntagged using a dictionary-based method.\nSACT encompasses medications used in tra-\n"}, {"page": 12, "text": "ditional cytotoxic chemotherapy, endocrine ther-\napy, targeted therapy, and immunotherapy. An\nSACT mention may appear as a generic name\n(e.g., Anastrozole), a brand name (e.g., Arim-\nidex), or a combined name (e.g., TCH). Non-\nspecific chemotherapy-related SACT mentions like\n\"chemotherapy\", \"chemo\", \"chemotherapy’s\", etc.\nAnd even mentions with typos like \"chemotheray\"\nshould also be retained.\nYour task is twofold:\n1. Review the pre-tagged mentions ONE BY\nONE and remove any incorrect tags caused by dic-\ntionary false positives.\n2. Identify and tag any additional SACT men-\ntions that are missing due to typos or uncommon\nabbreviations not found in the dictionary.\nExtract each SACT mention exactly as it appears\nin the note, even if there is a typo; do not alter or\nnormalize it. For example, if an SACT Aflibercept\nappears in the note as \"Alibercept\", your extracted\nSACT should be \"Alibercept\". Do not combine\nSACT mentions that refer to the same therapy but\nappear in different forms; extract them as separate\nmentions. For example, if there are three mentions:\n\"il-2\", \"il2\", and \"interleukin-2\", extract them all\nseparately.\nIgnore supplementary information such as dose,\nadministration method, or diagnostic/therapeutic\ncontext not related to anticancer treatment. Exclude\ntherapies, medications, or procedures used for non-\ncancer purposes, such as dietary supplements or\nbiopsies.\nYou should remove or add tags in the raw text.\nDo not output any other text.\nBoth the input and the output should be put in\n\" \". Please strictly follow the format of the output.\nYou MUST only wrap the correct SACT mentions\nwith <e> and </e> tags in your outputs. Do not add\nany other tags or quote marks.\nFor example, given the input text:\n\"This is a sentence with both <e>correct\nSACT</e> and <e>wrong SACT</e> mentioned.\"\nThe expected output is:\n\"This is a sentence with both <e>correct\nSACT</e> and wrong SACT mentioned.\"\nSometimes the input can be extremely long, like:\n\"======Here are some background details\nabout the patient========\nThis is a sentence with both <e>correct\nSACT1</e> and <e>wrong SACT</e> mentioned,\nand another <e>correct SACT2</e> mentioned.\"\nThe expected output is:\n\"======Here are some background details\nabout the patient========\nThis is a sentence with both <e>correct\nSACT1</e> and wrong SACT mentioned, and an-\nother <e>correct SACT2</e> mentioned.\"\nNow, extract SACT events from the following\nsentences in a clinical note:\nA.3\nPrompt for Context-Enhanced\nSentence-Level Relation Extraction\nYou are an experienced medical annotator tasked\nwith extracting systemic anticancer therapy (SACT)\nevents from a given clinical note.\nWHAT A SACT IS: SACT encompasses med-\nications used in traditional cytotoxic chemother-\napy, endocrine therapy, targeted therapy, and\nimmunotherapy.\nSACT may appear in generic\nnames (e.g., Anastrozole), brand names (e.g.,\nArimidex), or combined names (e.g., TCH). Non-\nspecific SACT mentions such as \"chemotherapy\" or\n\"chemo\" should also be included. I have extracted\nall the SACT events for you between the tags <e>\nand </e> in my input, so you don’t need to extract\nSACT yourself.\nWHAT YOU SHOULD EXTRACT: You should\ndo this step by step. First, identify all the SACT\nevents between <e> and </e> in my input and\nONLY focus on these SACT events. Then, exclude\nthe SACT events if they are macro information\nrather than patient-specific information, or if they\nare negations of SACTs. Next, for each valid SACT\nevent, extract specific time expressions that are ex-\nplicitly associated with that SACT event. Here are\nsome format examples of time expressions you are\nexpected to extract: \"December 27, 2011\", \"May\n21st, 2013\", \"7/20/2012\", \"today\", \"3 weeks ago\",\n\"1 year\". Ignore nonspecific time mentions such as\ncycle or dose numbers (e.g., \"cycle 1 of 6\" and \"1/6\ndose\") and ambiguous relative time (e.g., \"midway\nthrough\" or \"at the same time as\" another event).\nExclude time expressions that are not associated\nwith the current SACT event. If there is not a time\nexpression related to the current SACT event, then\nskip it and check the next SACT event.\nAdditionally, for each event, select a rela-\ntion label from BEGINS-ON, ENDS-ON, and\nCONTAINS-1 to indicate the relation between the\nSACT and its time based on the note’s language.\nCONTAINS-1 means the SACT happened at a spe-\ncific time; if the note explicitly mentions the start\nor end of an event, use BEGINS-ON or ENDS-ON.\nFor example, if the note says \"She received Her-\n"}, {"page": 13, "text": "ceptin on May 21st, 2013\", your extracted event\nwill be \"Herceptin\", \"CONTAINS-1\", \"May 21st,\n2013\"; if the note says \"Start ipilimumab on today’s\ndate\", your extracted event will be \"ipilimumab\",\n\"BEGINS-ON\", \"today\".\nIf a SACT is associated with multiple time\npoints, extract them as separate events. For ex-\nample, if the note says \"Herceptin was initiated\non 12/27/2011 and completed on April 10, 2012\",\nyou should extract two events: one is \"Herceptin\",\n\"BEGINS-ON\", \"12/27/2011\", and the other is\n\"Herceptin\", \"ENDS-ON\", \"April 10, 2012\". Sim-\nilarly, if multiple SACTs are associated with the\nsame date, you should also extract them as separate\nevents.\nHOW YOU SHOULD FORMAT YOUR RE-\nSPONSE: SACT names and their associated time\nexpressions should be kept exactly as they appear,\neven if there is a typo. Do not alter them, nor-\nmalize the time expression, or infer the exact date.\nFor example, if a SACT event appears in the note\nas \"Alibercept received yesterdat\", your extracted\nevent will be \"Alibercept\", \"CONTAINS-1\", \"yes-\nterdat\".\nYour response MUST be in a json format under\nthe following schema:\nhn [\"SACT event1\", \"relation1\", \"time expres-\nsion1\"], [\"SACT event2\", \"relation2\", \"time expres-\nsion2\"], . . .\nB\nChemotherapy Events Dictionary\nB.1\nBreast Cancer\n5-fu\na-cmf\na.c\na/c\nabemaciclib\nabraxane\nac\nac-cmf\nac-d\nac-h\nac-t\nac-th\nac-thl\nac-thp\nach\nact\nadriamycin\nafc\nafinitor\nairuika\nalimta\nalpelisib\nanastrozole\nanthracycline\narimedex\narimidex\naromasin\naromatase inhibitor\nat-cmf\natc\natezolizumab\navastin\nbev\nbevacizumab\nbilateral oophorectomy\ncaf\ncamrelizumab\ncapecitabine\ncapivasertib\ncarbo\ncarboplatin\ncbd\ncef\ncef-t\nchemo\nchemo therapy\nchemo-rt\nchemoembolization\nchemorad\nchemort\nchemotherap\nchemotherapeutic\nchemotherapeutic\nchemotherapies\nchemotherapy\nchemotherapy’s\nchemotheray\nchidamide\ncisplatin\ncmf\ncmf-e\ncmf-h\ncmft\ncnp\ncp-ac\ncp-ddac\ncp-ec\ncvb\ncyclophosphamide\ncytoxan\nd-ac\nd-ac+bev\nd-ec\nd-fec\nd-fec+bev\ndatopotamab deruxtecan\ndatroway\ndcb\ndda-ddt-ddc\nddac\nddac-ddt\nddac-ddth\nddac-pacph\nddac-t\nddac-th\nddac-thp\nddat\ndde\ndde-iddcmf\nddec-ddcmf\nddec-ddd\nddec-ddt\nddec-t\nddec-th\nddec-thp\nddfec-d\n"}, {"page": 14, "text": "ddp\nddt\nddt-ddec\nddt-ec\nddth\ndocetaxel\ndocetaxol\ndoxil\ndoxorubicin\ne-cmf\ne-d\ne-x\nec-cmf\nec-d\nec-ddt\nec-dt\nec-h\nec-p\nec-t\nec-th\nec-thp\necd-gc\nech\nech-th\nedc\nehp\nelacestrant\nellence\nendocrine therapy\nenhertu\nenzalutamide\nep-ddcmf\nepidaza\nepirubicin\neribulin\neverolimus\nexemestane\nfac\nfac-t\nfac-th\nfac-thp\nfareston\nfaslodex\nfec\nfec-d\nfec-h\nfec-p\nfec-t\nfec-th\nfec-thp\nfemara\nfluorouracil\nfulvestrant\ngcb\ngdoc\ngemcitabine\ngemzar\nghp\ngnrh analogs\ngoserelin\nh+d\nhalaven\nherceptin\nherceptin hylecta\nibrance\nidd-etc\niddenpc\niddepc\ninavolisib\nirene\nitovebi\nixabepilone\nixempra\njavlor\nkadcyla\nkeytruda\nkisqali\nl+t\nlapatinib\nletrozole\nleuprolide\nloqtorzi\nlupron\nlynparza\nmargenza\nmargetuximab\nmethotrexate\nmillipred\nmitomycin\nmitoxantrone\nmmm\nmtx\nmutamycin\nmyocet\nnavelbine\nneratinib\nnerlynx\nnolvadex\nnovantrone\nnp-ddac\nnp-ddec\nnp-ec\nnpc-ddec\nnpld\nofs\nolaparib\norserdu\novarian irradiation\npaclitaxel\npalbociclib\nparaplatin\npcb\npembrolizumab\npemetrexed\nperjeta\npertuzumab\nphesgo\npiqray\nplatinol\nplatinum\npld\nprednisolone\npyrotinib\nq2wk\nribociclib\ns-1\nsacituzumab govitecan\nt-ac\nt-cef\nt-ddac\nt-ddec\nt-dm1\nt-ec\nt-fac\nt-fec\nt-h\nt-t\ntac\ntalazoparib\ntalzenna\n"}, {"page": 15, "text": "tamoxifen\ntaxane\ntaxol\ntaxotere\ntaxtotere\ntc\ntc-h\ntcbh\ntch\ntchp\ntcyh\ntecentriq\nth-ac\nth-ddac\nth-ech\nth-fec\nthl\nthp\ntoremifene\ntoripalimab\ntpc\ntrastuzumab\ntrastuzumab deruxtecan\ntrastuzumab emtansine\ntrelstar la\ntriptorelin\ntrodelvy\ntruqap\ntucatinib\ntukysa\ntx-cex\ntykerb\nv-fec\nverzenio\nvh-fec\nvhp\nvinflunine\nvinorelbine\nxeloda\nxhp\nxtandi\nzoladex\nB.2\nMelanoma\nabc\nabraxane\nafiblercept\naflibercept\nalfa-2b interferon\nalflibercept\nalibercept\nalpha 2b interferon\nalpha interferon\nalpha-2b interferon\nalpha-2b interferon\nalpha-2binterferon\natezolizumab\navastin\nbevacizumab\nbinimetinib\nbraftovi\ncarboplatin\nchemo\nchemo therapy\nchemo-rt\nchemorad\nchemoradiation\nchemort\nchemotherap\nchemotherapeutic\nchemotherapeutic\nchemotherapies\nchemotherapy\nchemotherapy’s\nchemotheray\ncisplatin\ncnp\ncobimetinib\ncomplete resection\ncontego\ncotellic\ncpb\ncvd\ndabrafenib\ndacarbazine\ndocetaxel\ndtic\neldisine\nencorafenib\nfotemustine\ngleevec\nhepzato kit\nil 2\nil-2\nil2\nimatinib\nimlygic\ninteferon\ninterferon\ninterferon\ninterleukin\ninterleukin 2\ninterleukin-2\nipilimumab\nkeytruda\nkimmtrak\nkolupin\nkoselugo\nleukine\nlifileucel\nloqtorzi\nmekinist\nmektovi\nmelphalan\nmethotrexate\nmuphoran\nnivolumab\nopdivo\nopdualag\npaclitaxel\nparaplatin\npembrolizumab\nplatinol\nproleukin\nsargramostim\nselumetinib\ntace\ntafinlar\ntalimogene laherparepvec\ntasisulam\ntaxol\ntaxotere\ntebentafusp\ntecentriq\ntemodar\ntemozolomide\ntils\ntoripalimab\ntrametinib\ntunlametinib\n"}, {"page": 16, "text": "vaccinia\nvaccinia virus\nvaccinia virus\nvemurafenib\nvindesine\nyervoy\nzelboraf\nB.3\nOvarian Cancer\nabraxane\nalimta\nataxol\navastin\navstin\nbevacizumab\ncaboplatin\ncabotaxol\ncarbo\ncarboplat\ncarboplatin\ncarbotaxol\nchemo\nchemo therapy\nchemo-rt\nchemoembolization\nchemorad\nchemort\nchemotherap\nchemotherapeutic\nchemotherapeutic\nchemotherapies\nchemotherapy\nchemotherapy’s\nchemotheray\nchmeo\ncisplatin\ncistoplatin\ncytoreductive surgery\ndcb\ndocetaxel\ndocetaxil\ndoxil\ndoxorubicin\nelahere\netoposide\nfemara\ngcb\ngemcitabine\ngemzar\nhycamtin\nihcp\nintraperitoneal hyperthemicchemoperfusion\nintraperitonealhyperthemic chemoperfusion\nkoselugo\nletrozole\nliposomal doxorubicin\nlynparza\nmekinist\nmirvetuximab soravtansine\nnavelbine\nnintedanib\nniraparib\nnolvadex\nolaparib\novastat\npaciltaxel\npaclitaxel\npaclitaxela\nparaplatin\npaxil\npazopanib\npemetrexed\nplatinol\nplatinum\npldc\nrubraca\nrucaparib\nselumetinib\nt/c\ntamoxifen\ntax\ntaxo\ntaxol\ntaxotere\ntc\ntc-bev\ntopotecan\ntrabectedin\ntrametinib\ntreosulfan\nvargatef\nvepesid\nvinorelbine\nvotrient\nyondelis\nzejula\nC\nSupplementary Figures and Tables\nFigure 1: DPO reward accuracy curve of Qwen3-14B.\nCancer Type # Total # Annotated (%)\nBreast\n14,234\n216 (1.52%)\nMelanoma\n9,279\n523 (5.64%)\nOvarian\n16,782\n523 (3.12%)\nTable S1: Proportion of sentences with annotations in\nthe development set\nD\nExample of Inconsistency Between\nReasoning and Output\nModel thinking trace:\n<think> First, I need to check if \"Tc\" is a correct\nSACT. Wait, Tc-99m MDP is Technetium-99m\n"}, {"page": 17, "text": "Cancer type\nanno\nDictionary only\nDictionary + LLM verification\nPrecis.\nRecall\nF1\nPrecis.\nRecall\nF1\nbreast\n0\n0.9944\n1.0000\n0.9972\n0.9967\n1.0000\n0.9984\nbreast\n1\n0.7322\n1.0000\n0.8454\n0.8244\n1.0000\n0.9038\nmelanoma\n0\n0.9853\n1.0000\n0.9926\n0.9878\n0.9998\n0.9937\nmelanoma\n1\n0.8021\n1.0000\n0.8902\n0.8296\n0.9962\n0.9053\novarian\n0\n0.9926\n0.9998\n0.9962\n0.9927\n0.9998\n0.9963\novarian\n1\n0.8112\n1.0000\n0.8958\n0.8150\n0.9943\n0.8958\nTable S2: Sentence-level tagging on the dev set: dictionary tagging vs. dictionary tagging with LLM verification.\nanno_indicator=1 means the sentence has gold chemotherapy annotations.\nMethane Diphosphate, which is a radiopharma-\nceutical used in bone scans. But SACT refers to\nsystemic anticancer therapies like chemotherapy,\nendocrine therapy, etc. Radiopharmaceuticals are\nused for diagnostic purposes, not as part of cancer\ntreatment. So \"Tc\" here is a false positive from the\ndictionary method. The user’s instruction says to\nremove incorrect tags caused by false positives.\nTherefore, I should remove the <e>Tc</e> tag.\n</think>\nModel output:\nTECHNIQUE: Whole body bone scan was\nobtained following intravenous administration of\n26.5mCi of <e>Tc</e>-99m MDP.\nExpected corrected output:\nTECHNIQUE: Whole body bone scan was\nobtained following intravenous administration of\n26.5mCi of Tc-99m MDP.\nThis example highlights a misalignment between\nthe model’s reasoning and its generated output: al-\nthough the reasoning correctly identifies “Tc” as\na false positive (radiopharmaceutical rather than\nchemotherapy), the structured output still preserves\nthe erroneous tag.\n"}]}