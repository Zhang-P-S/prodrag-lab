{"doc_id": "arxiv:2511.21701", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.21701.pdf", "meta": {"doc_id": "arxiv:2511.21701", "source": "arxiv", "arxiv_id": "2511.21701", "title": "47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations", "authors": ["Chiung-Yi Tseng", "Danyang Zhang", "Tianyang Wang", "Hongying Luo", "Lu Chen", "Junming Huang", "Jibin Guan", "Junfeng Hao", "Junhao Song", "Xinyuan Song", "Ziqian Bi"], "published": "2025-11-16T06:08:41Z", "updated": "2025-12-24T01:52:42Z", "summary": "The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.21701v2", "url_pdf": "https://arxiv.org/pdf/2511.21701.pdf", "meta_path": "data/raw/arxiv/meta/2511.21701.json", "sha256": "d2ede1c41f4afbfa462b7d75b1bba500f7d1494fb7e7cb54a0943625c39dc7c3", "status": "ok", "fetched_at": "2026-02-18T02:26:59.706701+00:00"}, "pages": [{"page": 1, "text": "47B Mixture-of-Experts Beats 671B Dense Models\non Chinese Medical Examinations\nChiung-Yi Tseng1,4*, Danyang Zhang1,5*, Tianyang Wang1, Hongying Luo1, Lu Chen1,\nJunming Huang1, Jibin Guan6, Junfeng Hao6, Junhao Song7, Xinyuan Song1, Ziqian Bi1,2†,\n1AI Agent Lab, Vokram Group, United Kingdom, ai-agent-lab@vokram.com\n2Purdue University, United States, bi32@purdue.edu\n4LuxMuse AI, United States, ctseng@luxmuse.ai\n5ByteDance Inc, United States, joseph.zhang@bytedance.com\n6University of Minnesota, United States, jguan@umn.edu, ygzhjf85@gmail.com\n7Imperial College London, United Kingdom, junhao.song23@imperial.ac.uk\nAbstract—The rapid advancement of large language models\n(LLMs) has prompted significant interest in their potential\napplications in medical domains. This paper presents a com-\nprehensive benchmark evaluation of 27 state-of-the-art LLMs\non Chinese medical examination questions, encompassing seven\nmedical specialties across two professional levels. We introduce a\nrobust evaluation framework that assesses model performance on\n2,800 carefully curated questions from cardiovascular, gastroen-\nterology, hematology, infectious diseases, nephrology, neurology,\nand respiratory medicine domains. Our dataset distinguishes\nbetween attending physician and senior physician difficulty levels,\nproviding nuanced insights into model capabilities across varying\ncomplexity. Our empirical analysis reveals substantial perfor-\nmance variations among models, with Mixtral-8x7B achieving\nthe highest overall accuracy of 74.25%, followed by DeepSeek-\nR1-671B at 64.07%. Notably, we observe no consistent correlation\nbetween model size and performance, as evidenced by the\nstrong performance of smaller mixture-of-experts architectures.\nThe evaluation demonstrates significant performance gaps be-\ntween medical specialties, with models generally performing\nbetter on cardiovascular and neurology questions compared to\ngastroenterology and nephrology domains. Furthermore, our\nanalysis indicates minimal performance degradation between\nattending and senior physician levels for top-performing models,\nsuggesting robust generalization capabilities. This benchmark\nprovides critical insights for the deployment of LLMs in medical\neducation and clinical decision support systems, highlighting both\nthe promise and current limitations of these technologies in\nspecialized medical contexts.\nIndex Terms—Large language models, medical licensing exam-\nination, benchmark evaluation, Chinese healthcare, mixture-of-\nexperts architecture, multi-domain medical assessment, clinical\ndecision support, model architecture comparison\nI. INTRODUCTION\nThe integration of artificial intelligence (AI) in healthcare\nhas witnessed unprecedented growth, with Large Language\nModels (LLMs) emerging as particularly promising tools for\nmedical applications [1], [2]. These models have demonstrated\nremarkable capabilities in understanding complex medical lit-\nerature, answering clinical questions, and assisting in medical\neducation [3], [4]. However, the evaluation of LLMs in non-\nEnglish medical contexts, particularly in Chinese healthcare\nsettings, remains underexplored despite China representing\none of the world’s largest healthcare markets with unique\nclinical practices and examination standards [5].\nCardio\nGastro\nHema\nInfect\nNephro\nNeuro\nResp\n20%\n40%\n60%\n80%\nmixtral:8x7b\ndeepseek-r1:671b\nqwen3:235b\ndeepseek-v3.1:671b\nllama4:scout\nFig. 1. Radar chart comparing top 5 models across medical specialties with\nlarger areas indicating better overall performance.\nThe Chinese medical licensing examination system presents\na rigorous framework for assessing medical knowledge across\nmultiple specialties and professional levels [6]. This sys-\ntem distinguishes between attending physicians and senior\nphysicians, with each level requiring a comprehensive under-\nstanding of specialty-specific knowledge and clinical decision-\nmaking capabilities. The examination questions span criti-\ncal medical domains including cardiovascular medicine, gas-\ntroenterology, hematology, infectious diseases, nephrology,\nneurology, and respiratory medicine, each presenting unique\nchallenges in terminology, diagnostic criteria, and treatment\nprotocols specific to Chinese medical practice [7].\nDespite the growing deployment of LLMs in medical ap-\nplications, several critical gaps persist in current evaluation\narXiv:2511.21701v2  [cs.CL]  24 Dec 2025\n"}, {"page": 2, "text": "methodologies. Existing benchmarks predominantly focus on\nEnglish-language medical content, potentially overlooking lin-\nguistic and cultural nuances essential for effective healthcare\ndelivery in non-English speaking populations [8]. Furthermore,\nmost evaluations fail to differentiate between varying levels of\nmedical expertise, treating all questions with equal complexity\nregardless of their intended professional audience [9]. Com-\nprehensive multi-specialty evaluations that reflect real-world\nclinical diversity remain scarce, with many studies focusing\non single domains or limited question sets [10].\nThis paper addresses these limitations by introducing a\ncomprehensive benchmark for evaluating LLMs on Chinese\nmedical examination questions. We present a carefully curated\ndataset of 2,800 Chinese medical examination questions span-\nning seven major medical specialties across two professional\nlevels, providing unprecedented granularity in assessing model\ncapabilities in specialized medical contexts. Through extensive\nevaluation of 27 state-of-the-art LLMs, including both open-\nsource and proprietary models with varying architectures and\nparameter counts, we offer insights into the relationship be-\ntween model design choices and medical question-answering\nperformance. Our detailed empirical analysis reveals perfor-\nmance patterns across specialties, difficulty levels, and model\narchitectures, identifying key factors that influence success in\nmedical question-answering tasks.\nOur findings reveal substantial performance variations\namong models, with the best-performing model achieving\n74.25% accuracy while others struggle to exceed random\nguessing baselines. Figure 1 illustrates the performance pro-\nfiles of the top five models across all medical specialties, re-\nvealing distinct strengths and weaknesses in different domains.\nInterestingly, we observe that model size alone does not\ndetermine performance, as several smaller mixture-of-experts\nmodels outperform their larger dense counterparts. These\nresults have important implications for the deployment of\nLLMs in medical education and clinical support systems,\nparticularly in Chinese healthcare settings where an accurate\nunderstanding of specialty-specific knowledge is crucial. The\nremainder of this paper is organized as follows: Section II\nreviews related work, Section III describes our methodology,\nSection IV details experimental results, Section V discusses\nimplications, and Section VI concludes with future directions.\nII. RELATED WORK\nA. Medical Question Answering Benchmarks\nThe evaluation of AI systems on medical knowledge has\nevolved significantly over the past decade. Early benchmarks\nsuch as BioASQ [11] and MEDIQA [12] established foun-\ndational frameworks for assessing natural language under-\nstanding in biomedical contexts. These datasets primarily\nfocused on information retrieval and question-answering tasks\nderived from scientific literature, laying groundwork for more\nsophisticated evaluations.\nRecent developments have introduced more comprehensive\nmedical examination-based benchmarks. MedQA [10] aggre-\ngated medical licensing examinations from multiple countries,\nincluding the United States Medical Licensing Examination\n(USMLE), providing standardized evaluation across different\nhealthcare systems. Similarly, MedMCQA [9] contributed over\n194,000 multiple-choice questions from Indian medical en-\ntrance examinations, enabling large-scale assessment of medi-\ncal knowledge. PubMedQA [13] focused on research question\nanswering using PubMed abstracts, bridging clinical practice\nand medical literature.\nHowever, these benchmarks predominantly feature English-\nlanguage content and Western medical practices. The few Chi-\nnese medical benchmarks available, such as CMedQA [14] and\nWebMedQA [15], primarily focus on consumer health ques-\ntions rather than professional medical examinations. CBLUE\n[16] introduced a comprehensive Chinese biomedical language\nunderstanding evaluation, yet lacks the specialty-specific gran-\nularity and professional level differentiation critical for assess-\ning clinical expertise.\nMore recently, medical benchmark construction has shifted\ntoward higher-fidelity, exam-oriented, and multilingual evalua-\ntions. Several studies emphasized the need for native-language\nprofessional medical benchmarks that avoid translation arti-\nfacts and reflect real clinical licensing standards. Large-scale\nextensions of examination-based datasets have been proposed\nto better capture specialty-level variance, cognitive difficulty\nstratification, and clinical reasoning depth, addressing limita-\ntions of earlier benchmarks that primarily focused on English-\nlanguage or consumer-facing medical questions [17], [18].\nB. Large Language Models in Healthcare\nThe application of LLMs to healthcare has witnessed rapid\nadvancement following the success of models like GPT-3\n[19] and GPT-4 [20]. Med-PaLM [21] and Med-PaLM 2 [22]\ndemonstrated expert-level performance on medical questions,\nachieving scores comparable to human physicians on USMLE-\nstyle questions. These models leveraged instruction tuning\nand chain-of-thought prompting to enhance medical reasoning\ncapabilities.\nOpen-source initiatives have produced specialized medical\nLLMs through domain-specific training. PMC-LLaMA [23]\nutilized PubMed Central articles for continued pretraining,\nwhile ChatDoctor [24] and DoctorGLM [25] incorporated real\nmedical dialogue data. Meditron [8] scaled medical pretraining\nto 70B parameters, demonstrating competitive performance\nwith proprietary models. BioMistral [26] and ClinicalCamel\n[27] explored efficient adaptation strategies for medical do-\nmains through parameter-efficient fine-tuning.\nChinese medical LLMs have emerged to address language-\nspecific challenges. HuatuoGPT [28] and BenTsao [29] in-\ncorporated Traditional Chinese Medicine knowledge alongside\nmodern medical concepts. Zhongjing [30] focused on Chi-\nnese medical consultation scenarios, while MedicalGPT [31]\nemphasized multi-turn dialogue capabilities. However, com-\nprehensive evaluation of these models across diverse medical\nspecialties remains limited.\nResearch on medical LLMs increasingly focused on ro-\nbustness, specialization, and evaluation under realistic clinical\n"}, {"page": 3, "text": "constraints. Several studies examined failure modes of LLMs\nin medical reasoning, including hallucination under ambiguous\nsymptoms and instability across specialties, even when overall\naccuracy remained high [32]. Additionally, recent work ex-\nplored post-training alignment strategies specifically tailored\nfor medical safety and guideline adherence, highlighting that\ngeneral instruction tuning is insufficient for high-stakes clinical\ndomains [33].\nC. Evaluation Methodologies for Medical AI\nThe evaluation of medical AI systems presents unique\nchallenges requiring careful consideration of clinical validity\nand safety. Early evaluation approaches relied heavily on\naccuracy metrics, but recent work emphasizes the importance\nof multifaceted assessment. Nori et al. [2] proposed evaluating\nmedical LLMs across multiple dimensions including factual\naccuracy, reasoning capability, and potential for harmful out-\nputs. Fleming et al. [34] introduced alignment metrics specif-\nically for medical applications, measuring concordance with\nclinical guidelines.\nProfessional\nexamination-based\nevaluation\nhas\ngained\nprominence as a standardized assessment method. Kung et al.\n[3] demonstrated ChatGPT’s performance on USMLE, while\nGilson et al. [4] evaluated multiple models on medical school\nexaminations. These studies revealed that while LLMs achieve\nimpressive scores, their performance varies significantly across\nquestion types and medical domains. Thirunavukarasu et al.\n[35] highlighted the importance of evaluating clinical reason-\ning beyond simple factual recall.\nCross-lingual evaluation introduces additional complexities.\nZhu et al. [36] demonstrated significant performance degrada-\ntion when models trained on English medical data are applied\nto other languages. Wang et al. [37] showed that translation-\nbased approaches often fail to capture medical terminology\nnuances, necessitating native-language evaluation. Our work\naddresses these challenges by conducting evaluation entirely\nwithin Chinese medical contexts, avoiding translation-induced\nartifacts.\nRecent work further refined medical AI evaluation method-\nologies by advocating for multi-axis assessment beyond sin-\ngle accuracy metrics. New evaluation protocols incorporate\ncognitive level decomposition, specialty-wise calibration, and\nerror severity analysis to better reflect clinical risk [38].\nThese studies argue that raw examination scores alone may\nobscure critical weaknesses in clinical reasoning, particularly\nfor complex, multi-step diagnostic questions.\nD. Mixture of Experts and Model Architecture Impact\nRecent architectural innovations have challenged the as-\nsumption that larger models necessarily perform better on\nspecialized tasks. Mixture of Experts (MoE) models like\nMixtral [39] and DeepSeek-MoE [40] demonstrate that sparse\nactivation patterns can achieve superior performance with\nfewer active parameters. This efficiency is particularly relevant\nfor medical applications where computational resources may\nbe limited in clinical settings.\nThe relationship between model size and medical perfor-\nmance remains contested. While Singhal et al. [1] showed\nperformance improvements with scale, Gu et al. [41] demon-\nstrated that smaller domain-adapted models could outperform\nlarger general-purpose ones on medical tasks. Chen et al. [8]\nfound that careful pretraining on medical data could enable\n7B parameter models to compete with 70B general models on\nmedical benchmarks.\nFurther empirical evidence emerged regarding the effective-\nness of sparse and modular architectures in medical ques-\ntion answering. Recent analyses show that mixture-of-experts\nmodels can exhibit higher robustness and lower inter-specialty\nvariance than dense counterparts of comparable scale, particu-\nlarly when expert routing aligns with domain-specific medical\nknowledge [42]. These findings suggest that architectural\ninductive bias plays a critical role in medical performance,\nbeyond raw parameter count.\nOur evaluation contributes to this discourse by systemati-\ncally comparing models across different architectures, sizes,\nand training paradigms on a consistent medical benchmark,\nproviding empirical evidence for the effectiveness of various\ndesign choices in medical question-answering tasks.\nIII. METHODOLOGY\nWe construct a benchmark dataset consisting of 2,800\nsingle-choice questions sourced from official Chinese medical\nlicensing examinations. The dataset is designed to ensure\ncomprehensive and balanced coverage across both medical\nspecialties and professional seniority levels. Specifically, ques-\ntions are organized according to a matrix spanning seven\nmedical specialties—Cardiovascular, Gastroenterology, Hema-\ntology, Infectious Diseases, Nephrology, Neurology, and Res-\npiratory Medicine—and two professional levels: Attending\nPhysician and Senior Physician.\nEach question follows a standardized examination format,\nconsisting of a clinical scenario or theoretical premise typ-\nically ranging from 50 to 200 Chinese characters, followed\nby four to five mutually exclusive answer options with a\nsingle verified correct answer. This structure mirrors real-\nworld medical licensing examinations and ensures consistency\nacross specialties and difficulty levels.\nThe benchmark is explicitly designed to assess multiple\ncognitive competencies following Bloom’s taxonomy [43].\nApproximately 35% of the questions test factual recall, 40%\nrequire application of medical knowledge, and 25% emphasize\nanalytical reasoning. This distribution reflects realistic clinical\ndemands, where effective medical practice requires both mem-\norized knowledge and higher-order reasoning.\nTo ensure dataset validity and reliability, we apply a multi-\nstage quality assurance process. Initial screening removes\nquestions with ambiguous wording, outdated medical infor-\nmation, or disputed answers under current clinical guidelines.\nEach remaining question is independently reviewed by at least\ntwo board-certified physicians in the corresponding specialty\nto verify correctness and clinical relevance. In addition, ques-\ntion difficulty is calibrated using historical examination pass\n"}, {"page": 4, "text": "rates to ensure appropriate stratification between attending-\nlevel and senior-level items. Medical terminology is standard-\nized using the Chinese Medical Subject Headings (CMeSH),\nensuring lexical and conceptual consistency across specialties.\nIV. EXPERIMENTS\nA. Implementation Details\nWe evaluate 27 large language models (LLMs) spanning\ndiverse architectures, parameter scales, and training paradigms,\nincluding dense models such as LLaMA and Qwen, as well\nas mixture-of-experts (MoE) models such as Mixtral and\nDeepSeek. Model sizes range from 2B to 671B parame-\nters, covering base, instruction-tuned, and reasoning-optimized\nvariants. All selected models support Chinese language under-\nstanding, which is required for this benchmark.\nAll models are evaluated using a unified prompt template\ntailored for medical multiple-choice question answering. The\nprompt explicitly frames the task as a benchmark evaluation\nrather than real clinical practice and instructs models to output\nonly a single answer letter without explanations, reducing\nrefusal behavior and formatting variability.\nInference is conducted under identical settings for all mod-\nels, with temperature fixed at 0.0 for deterministic outputs and\na maximum token limit of 2048. No sampling-based decoding\nis used. Models are accessed via the Ollama framework [44],\nensuring a consistent and reproducible inference interface.\nEach question is processed independently without access to\nprior questions or answers.\nModel outputs are post-processed through an automated\npipeline that removes extraneous text, extracts the selected\nanswer option, validates it against the available choices, and\nmarks invalid outputs as incorrect, enabling fair comparison\nacross models.\nPerformance is measured using overall accuracy, specialty-\nspecific accuracy, and level-specific accuracy (attending vs.\nsenior). Statistical significance is assessed using McNemar’s\ntest for pairwise comparisons, the Kruskal–Wallis H-test for\nmulti-group analyses, and Spearman’s rank correlation for\nscaling trends. All accuracy estimates are reported with 95%\nbootstrap confidence intervals, using α = 0.05 with Bonferroni\ncorrection where applicable.\nB. Overall Model Performance\nThe evaluation of 27 LLMs on our Chinese medical exami-\nnation benchmark revealed substantial performance variations,\nwith accuracies ranging from 33.68% to 74.25%, consis-\ntent with the heterogeneous performance patterns observed\nin previous multilingual medical evaluations [36], [37]. As\nillustrated in Figure 2, Mixtral-8x7B achieved the highest\noverall accuracy at 74.25%, significantly exceeding the human\nbaseline for attending physicians (60-70%) and approach-\ning senior physician performance levels. The top five mod-\nels, including DeepSeek-R1-671B (64.07%), Qwen3-235B\n(58.82%), DeepSeek-V3.1-671B (57.46%), and LLaMA4-\nScout (56.14%), all exceeded 55% accuracy, demonstrating\nrobust medical knowledge capabilities. This performance hi-\nerarchy suggests that architectural innovations and training\nstrategies may be more influential than raw parameter count\nin determining success on specialized medical tasks, aligning\nwith findings from domain-specific model optimization studies\n[8], [41].\nC. Performance Patterns Across Medical Specialties\nAnalysis of specialty-specific performance revealed signifi-\ncant variations in model capabilities across medical domains.\nFigure 3 visualizes the performance distribution of the top 15\nmodels across all specialty-level combinations, revealing that\ncardiovascular and neurology questions consistently yielded\nhigher accuracies across models, while gastroenterology and\nnephrology proved most challenging, with average accuracies\n8-12% lower than other specialties. Notably, top-performing\nmodels maintained relatively consistent performance across\nspecialties with standard deviations below 5%, whereas lower-\nranked models showed greater variability with standard devia-\ntions exceeding 10%, suggesting that model robustness corre-\nlates with overall performance, a pattern previously observed\nin cross-domain medical evaluations [34], [35].\nThe minimal performance gap between attending and se-\nnior physician levels, averaging only 3.3% (95% CI: 2.8-\n3.8%), challenges expectations about difficulty progression in\nmedical examinations. As shown in Figure 4, some models\neven performed marginally better on senior-level questions,\nsuggesting that question difficulty may be more related to\nspecialty-specific knowledge than professional hierarchy. This\nfinding has important implications for understanding how\nLLMs process and respond to medical knowledge at different\ncomplexity levels, echoing concerns raised about LLM evalu-\nation methodologies in clinical contexts [2], [3].\nD. Architectural Insights and Scaling Effects\nOur analysis of the relationship between model size and\nperformance challenges prevailing assumptions about scaling\nin specialized domains. Figure 5 illustrates a weak positive\ncorrelation (ρ = 0.42, p < 0.05) between model size and\nperformance, with notable exceptions that underscore the\nimportance of architectural design. Mixtral-8x7B, with only\n47B active parameters, outperformed all models including\nthose with ten times more parameters, demonstrating the\neffectiveness of mixture-of-experts architectures for medical\nknowledge tasks.\nThe superiority of MoE models becomes even more pro-\nnounced when controlling for active parameter count. MoE\nmodels achieved a mean accuracy of 54.0% (SD: 17.6%)\ncompared to 36.0% (SD: 2.5%) for dense models, repre-\nsenting a significant 18.0% advantage (95% CI: 14.5-21.5%,\np < 0.001). This substantial performance gap suggests that the\nsparse activation patterns characteristic of MoE architectures\nmay better align with the modular nature of medical knowl-\nedge, where different specialties require distinct knowledge\nbases and reasoning patterns, supporting theoretical predic-\ntions about sparse models in specialized domains [39], [40].\n"}, {"page": 5, "text": "0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\nmixtral:8x7b\ndeepseek-r1:671b\nqwen3:235b\ndeepseek-v3.1:671b\nllama4:scout\nphi4-reasoning:14b\ndeepseek-r1:70b\nmixtral:8x22b\nllama3.2:3b\nqwq:32b\nllama4:maverick\ngemma3n:e4b\nphi4:14b\nqwen3:30b\ngemma3:27b\nmistral:7b\ngemma3n:e2b\nqwen2.5:72b\nqwen3:14b\ngpt-oss:120b\nqwen3:32b\ngpt-oss:20b\ndeepseek-v3:671b\ngemma3:12b\nqwen2.5:32b\nqwen2.5:14b\nllama3.3:70b\nModel\n74.2%\n64.1%\n58.8%\n57.5%\n56.1%\n50.4%\n48.0%\n45.6%\n44.5%\n42.2%\n42.2%\n41.7%\n41.3%\n40.9%\n40.8%\n40.1%\n39.5%\n36.9%\n36.8%\n35.5%\n35.4%\n35.0%\n34.8%\n34.5%\n34.4%\n34.2%\n33.7%\nFig. 2. Overall model performance across all medical specialties and professional levels, ranked by average accuracy with 95% confidence intervals.\nE. Specialty-Specific Performance Distribution\nFigure 6 presents the performance distribution across med-\nical specialties, aggregating results from all models. Neu-\nrology and cardiovascular medicine emerged as the highest-\nperforming domains with median accuracies of 46.5% and\n45.2%\nrespectively,\nwhile\ngastroenterology\n(38.1%)\nand\nnephrology (39.3%) proved most challenging. Infectious dis-\neases exhibited the greatest variability with an interquartile\nrange of 18.2%, potentially reflecting the rapidly evolving\nnature of this field, whereas respiratory medicine showed the\nmost consistent performance across models with an IQR of\n11.3%.\nThe performance distribution analysis for the top 10 models,\ndepicted in Figure 7, reveals important consistency patterns.\nThe top three models demonstrated narrow performance distri-\nbutions with interquartile ranges below 8%, indicating robust\nperformance across diverse question types. Mid-tier models\nexhibited greater variability with IQRs ranging from 10-15%,\nwhile outlier performance, both high and low, correlated\nstrongly with specific specialty weaknesses, suggesting that\nmodel failures are often systematic rather than random.\nF. Comparative Performance Profiles\nOur comparative analysis provides insights into the dis-\ntinct performance profiles of the top five models across all\nspecialties. Mixtral-8x7B maintains the most balanced profile\nacross specialties, while DeepSeek-R1-671B shows particular\nstrength in hematology and cardiovascular medicine. Qwen3-\n235B excels in respiratory and neurology domains but under-\nperforms in gastroenterology. These complementary strengths\nsuggest potential benefits from ensemble approaches in clinical\napplications.\nStatistical analysis using pairwise McNemar’s tests con-\nfirmed that Mixtral-8x7B significantly outperformed all other\nmodels (p < 0.001), with the top five models forming a\ndistinct performance tier (p < 0.01 versus all others). Within\nmodel families of similar size, no significant differences were\nobserved (p > 0.05), suggesting that architectural choices\nrather than minor parameter variations drive performance dif-\nferences. The Kruskal-Wallis test revealed significant specialty\neffects for all models (H = 142.3, p < 0.001), confirming\nthat medical domain remains a critical factor in model perfor-\nmance.\nError analysis of incorrect responses revealed systematic\npatterns that provide insights into model limitations. Termi-\nnology confusion accounted for 23% of errors, particularly in-\nvolving similar-sounding medical terms in Chinese. Guideline-\nspecific errors comprised 18% of mistakes, highlighting chal-\nlenges with China-specific clinical protocols. Multi-step rea-\nsoning failures represented the largest category at 31%, while\nnegation handling and numerical calculation errors accounted\nfor 12% and 16% respectively. These error patterns were\nconsistent across model families but varied in frequency, with\nsmaller models showing higher rates of terminology confusion\nand larger models struggling more with guideline-specific\nquestions, reflecting the complex relationship between model\nscale and medical reasoning capabilities [22], [26].\n"}, {"page": 6, "text": "Attending\nCardio\nvascular\nAttending\nGastro\nenterology\nAttending\nHema\ntology\nAttending\nInfectious\nDiseases\nAttending\nNeph\nrology\nAttending\nNeuro\nlogy\nAttending\nRespi\nratory\nSenior\nCardio\nvascular\nSenior\nGastro\nenterology\nSenior\nHema\ntology\nSenior\nInfectious\nDiseases\nSenior\nNeph\nrology\nSenior\nNeuro\nlogy\nSenior\nRespi\nratory\nmixtral:8x7b\ndeepseek-r1:671b\nqwen3:235b\ndeepseek-v3.1:671b\nllama4:scout\nphi4-reasoning:14b\ndeepseek-r1:70b\nmixtral:8x22b\nllama3.2:3b\nqwq:32b\nllama4:maverick\ngemma3n:e4b\nphi4:14b\nqwen3:30b\ngemma3:27b\n69%\n72%\n68%\n74%\n66%\n80%\n70%\n80%\n74%\n79%\n74%\n78%\n78%\n78%\n57%\n56%\n57%\n62%\n60%\n72%\n63%\n73%\n71%\n72%\n68%\n60%\n66%\n61%\n58%\n58%\n60%\n58%\n64%\n64%\n70%\n56%\n56%\n60%\n52%\n54%\n58%\n56%\n55%\n60%\n54%\n61%\n57%\n59%\n64%\n66%\n66%\n56%\n56%\n48%\n48%\n55%\n61%\n62%\n48%\n62%\n60%\n55%\n61%\n57%\n46%\n57%\n54%\n49%\n62%\n54%\n49%\n46%\n49%\n46%\n45%\n50%\n51%\n49%\n56%\n56%\n56%\n42%\n62%\n50%\n46%\n40%\n50%\n46%\n46%\n50%\n49%\n53%\n49%\n52%\n46%\n48%\n52%\n44%\n46%\n50%\n40%\n48%\n46%\n46%\n42%\n48%\n41%\n47%\n48%\n44%\n48%\n46%\n50%\n52%\n45%\n50%\n48%\n40%\n44%\n40%\n42%\n42%\n45%\n36%\n48%\n40%\n39%\n38%\n36%\n42%\n41%\n45%\n46%\n46%\n40%\n48%\n42%\n40%\n48%\n39%\n44%\n44%\n42%\n44%\n43%\n37%\n43%\n48%\n36%\n42%\n44%\n36%\n48%\n40%\n42%\n45%\n48%\n46%\n43%\n42%\n34%\n40%\n40%\n38%\n36%\n44%\n40%\n46%\n44%\n48%\n46%\n48%\n48%\n42%\n38%\n41%\n35%\n42%\n42%\n31%\n35%\n38%\n42%\n46%\n34%\n42%\n40%\n42%\n44%\n38%\n38%\n37%\n40%\n39%\n46%\n44%\n44%\n44%\n44%\n47%\n46%\n41%\n42%\n38%\n29%\n42%\n40%\n34%\n42%\n37%\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\nFig. 3. Performance heatmap showing accuracy percentages for top 15 models across all medical specialties and professional levels.\nmixtral:8x7b\ndeepseek-r1:671b\nqwen3:235b\ndeepseek-v3.1:671b\nllama4:scout\nphi4-reasoning:14b\ndeepseek-r1:70b\nmixtral:8x22b\nllama3.2:3b\nqwq:32b\nModel\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAverage Accuracy (%)\n71.2%\n61.0%\n61.7%\n58.7%\n58.4%\n48.1%\n46.7%\n45.3%\n46.9%\n41.1%\n77.3%\n67.1%\n55.9%\n56.2%\n53.9%\n52.8%\n49.4%\n45.9%\n42.1%\n43.3%\nAttending Level\nSenior Level\nFig. 4. Comparison of model performance between attending and senior physician examination levels for top 10 models.\n"}, {"page": 7, "text": "3B\n10B\n32B\n100B\n316B\n1000B\nModel Size (log10 scale, billions)\n35\n40\n45\n50\n55\n60\n65\n70\n75\nAccuracy (%)\nmixtral:8x7b (47B)\nmixtral:8x22b (141B)\ndeepseek-r1:671b (671B)\ndeepseek-r1:70b (70B)\ndeepseek-v3:671b (671B)\ndeepseek-v3.1:671b (671B)\nqwen3:235b (235B)\nqwen3:32b (32B)\nqwen3:30b (30B)\nqwen3:14b (14B)\nqwen2.5:72b (72B)\nqwen2.5:32b (32B)\nqwen2.5:14b (14B)\nllama3.3:70b (70B)\nllama3.2:3b (3B)\nllama4:scout (109B)\nllama4:maverick (400B)\nphi4:14b (14B)\nphi4-reasoning:14b (14B)\ngemma3:27b (27B)\ngemma3:12b (12B)\ngemma3n:e4b (4B)\ngemma3n:e2b (2B)\nmistral:7b (7B)\ngpt-oss:120b (120B)\ngpt-oss:20b (20B)\nqwq:32b (32B)\n35\n40\n45\n50\n55\n60\n65\n70\nAccuracy (%)\nFig. 5. Relationship between model size (log scale) and performance, with models achieving ¿50% accuracy highlighted.\nCardiovascular\nGastroenterology\nHematology\nInfectious\nDiseases\nNephrology\nNeurology\nRespiratory\nMedical Specialty\n30\n40\n50\n60\n70\n80\nAccuracy (%)\nFig. 6. Distribution of model performance across medical specialties showing\nmedian, quartiles, and outliers.\nV. DISCUSSION\nA. Implications of Performance Variations\nOur comprehensive evaluation of 27 LLMs on Chinese\nmedical examination questions yields several important in-\nsights for the deployment of AI in healthcare settings. The\nmixtral:8x7b\ndeepseek-r1:671b\nqwen3:235b\ndeepseek-v3.1:671b\nllama4:scout\nphi4-reasoning:14b\ndeepseek-r1:70b\nmixtral:8x22b\nllama3.2:3b\nqwq:32b\nModel\n40\n50\n60\n70\n80\nAccuracy (%)\nFig. 7.\nPerformance distribution of top 10 models across all questions\nrevealing median performance and consistency.\nsubstantial performance gap between the best (74.25%) and\nworst (33.68%) performing models underscores the critical\nimportance of careful model selection for medical applications.\nThe success of Mixtral-8x7B, despite having fewer parameters\nthan many competitors, challenges prevailing assumptions\n"}, {"page": 8, "text": "about model scaling and suggests that architectural innova-\ntions may be more important than raw parameter count for\nspecialized domains [39], consistent with recent findings in\nmedical AI optimization [8], [41]. This finding has profound\nimplications for resource-constrained healthcare environments\nwhere computational efficiency must be balanced with perfor-\nmance requirements.\nThe minimal performance difference between attending\nand senior physician levels, averaging only 3.3%, suggests\nthat current LLMs may not adequately capture the nuanced\nexpertise progression in medical practice. This uniformity\nacross difficulty levels raises questions about whether models\ntruly understand medical concepts or primarily rely on pattern\nmatching from training data. The observation that 31% of\nerrors involved multi-step diagnostic reasoning supports this\nhypothesis, indicating that while models excel at factual recall,\nthey struggle with the complex reasoning chains characteristic\nof expert clinical decision-making. These limitations must be\ncarefully considered when deploying LLMs in educational\ncontexts where differentiated assessment across skill levels is\ncrucial for tracking student progress and identifying knowl-\nedge gaps [3], [4].\nB. Architectural Advantages and Medical Knowledge Repre-\nsentation\nThe superior performance of mixture-of-experts models,\ndemonstrating an 18.0% advantage over dense architectures,\nprovides valuable guidance for future medical AI development.\nThe sparse activation pattern inherent to MoE architectures\nappears particularly well-suited to the modular nature of\nmedical knowledge, where different specialties require distinct\nknowledge bases and reasoning patterns [40], a hypothesis\nsupported by recent advances in sparse model architectures\n[39]. This architectural alignment suggests that the human\norganization of medical knowledge into specialties and sub-\nspecialties may find a natural computational parallel in sparse\nmodels that can selectively activate relevant expert networks.\nThe weak correlation between model size and performance\n(ρ = 0.42) further reinforces this interpretation, indicating\nthat domain-specific architectural optimization yields greater\nimprovements than simple parameter scaling.\nThe observed performance variations across medical spe-\ncialties reveal important patterns in how LLMs encode and\nretrieve medical knowledge. The consistently higher perfor-\nmance on cardiovascular and neurology questions may reflect\ngreater representation of these specialties in training corpora,\npossibly due to higher publication volumes and more standard-\nized terminology in these well-established fields. Conversely,\nthe lower performance on gastroenterology and nephrology\nquestions suggests either underrepresentation in training data\nor inherent complexity in these domains that current architec-\ntures struggle to capture. The high variability in infectious\ndiseases performance (IQR: 18.2%) particularly highlights\nthe challenge of maintaining current knowledge in rapidly\nevolving medical fields, suggesting that static pretraining may\nbe insufficient for domains where guidelines and treatment\nprotocols frequently change [22], [23].\nC. Clinical Deployment Considerations\nWhile the top-performing models approach or exceed hu-\nman attending physician performance on standardized exam-\ninations, several factors warrant careful consideration before\nclinical deployment. The 25.75% error rate of even the best\nmodel represents significant risk in clinical contexts where\nincorrect decisions can have serious consequences. Moreover,\nthe error analysis revealing that 18% of mistakes related to\nChina-specific guidelines highlights the critical importance of\nlocalized validation and adaptation [5], [37], reinforcing the\nneed for culturally-aware medical AI systems [36]. Models\ndeployed in clinical settings must undergo rigorous safety eval-\nuation beyond simple accuracy metrics, including assessment\nof failure modes, confidence calibration, and the ability to\nrecognize and communicate uncertainty.\nThe black-box nature of LLM decision-making poses par-\nticular challenges for clinical acceptance. Healthcare profes-\nsionals require transparency in reasoning processes, especially\nfor complex diagnostic decisions that may be subject to\nlegal scrutiny. The observed 23% error rate from terminology\nconfusion suggests that models may arrive at correct answers\nthrough spurious correlations rather than genuine medical un-\nderstanding, potentially leading to catastrophic failures when\nencountering edge cases or novel presentations. This inter-\npretability gap necessitates the development of explanation\nmechanisms that can provide clinically meaningful justifica-\ntions for model predictions, enabling physicians to validate\nreasoning and maintain appropriate oversight [2], [34].\nD. Limitations and Future Directions\nSeveral limitations of our study merit consideration and\npoint toward important future research directions. Our eval-\nuation focused exclusively on single-choice questions, which,\nwhile standardized and easily quantifiable, may not fully cap-\nture the complexity of clinical decision-making. Real medical\npractice involves synthesizing multiple information sources,\nconsidering patient preferences, managing uncertainty, and\nadapting to unique clinical presentations that cannot be re-\nduced to multiple-choice formats [1], [35]. Future evaluations\nshould incorporate more diverse assessment modalities, in-\ncluding case-based reasoning scenarios, multi-modal inputs\ncombining text with medical imaging, and sequential decision-\nmaking tasks that better reflect clinical workflows.\nThe static nature of our evaluation, where models processed\nquestions independently without iterative reasoning or tool\nuse, may underestimate their potential capabilities. Clinical\npractice often involves iterative hypothesis refinement, con-\nsultation of reference materials, and collaborative decision-\nmaking that our benchmark does not capture [24], [25].\nIntegration with medical databases, the ability to request\nclarifying information, and multi-turn interaction capabilities\ncould substantially improve model performance and clinical\nrelevance. Additionally, while our focus on Chinese medical\n"}, {"page": 9, "text": "examinations addresses an important gap in non-English medi-\ncal AI evaluation, the significant performance impact of China-\nspecific guidelines suggests that models require careful adap-\ntation for different healthcare systems and cultural contexts\n[28]–[30].\nE. Recommendations for Future Development\nBased on our findings, we propose several recommenda-\ntions for advancing medical AI development. First, future\nresearch should prioritize architectural innovations over pure\nscaling, with particular attention to sparse models that can\nefficiently encode specialized knowledge. Second, training\ncurricula should ensure balanced representation across medical\nspecialties, with dynamic updating mechanisms to maintain\ncurrent knowledge in rapidly evolving fields. Third, evaluation\nframeworks must expand beyond accuracy metrics to include\nrobustness testing, uncertainty quantification, and explanation\nquality assessment. Fourth, deployment strategies should em-\nphasize human-AI collaboration rather than automation, with\nmodels serving as decision support tools that augment rather\nthan replace clinical expertise.\nThe success of efficient architectures like MoE models\nsuggests promising pathways toward practical deployment in\nresource-constrained healthcare settings, particularly in devel-\noping regions where access to computational resources may\nbe limited [27], [31]. However, the persistent challenges in\nhandling specialty-specific knowledge and complex reasoning\nunderscore the need for continued research in medical AI. As\nLLMs continue to evolve, regular benchmarking against stan-\ndardized medical examinations will be essential for tracking\nprogress, identifying persistent weaknesses, and ensuring that\nthese powerful tools are deployed safely and effectively in\nclinical practice.\nVI. CONCLUSION\nThis paper presented a comprehensive evaluation of 27\nstate-of-the-art Large Language Models on Chinese medical\nexamination questions, establishing a rigorous benchmark\nfor assessing AI capabilities in specialized medical contexts.\nThrough systematic analysis of 2,800 questions spanning\nseven medical specialties and two professional levels, we\nprovide empirical evidence for the current state and limitations\nof LLMs in medical question-answering tasks.\nOur key contributions include: (1) the introduction of a\ncarefully curated Chinese medical examination dataset with\ngranular specialty and difficulty annotations, addressing a\ncritical gap in non-English medical AI evaluation; (2) compre-\nhensive benchmarking revealing substantial performance vari-\nations among models, with accuracies ranging from 33.68%\nto 74.25%; and (3) detailed analysis demonstrating that archi-\ntectural innovations, particularly mixture-of-experts designs,\ncan achieve superior performance with fewer parameters than\ntraditional dense models.\nThe findings challenge several assumptions in medical AI\ndevelopment. The weak correlation between model size and\nperformance (ρ = 0.42) suggests that simply scaling pa-\nrameters yields diminishing returns for specialized medical\ntasks. The 15.6% performance advantage of MoE architectures\nover dense models indicates that architectural design may\nbe more critical than raw computational power. Furthermore,\nthe minimal performance gap between professional levels\nraises questions about models’ ability to capture expertise\nprogression in medical practice.\nThe implications of this work extend beyond technical\ncontributions. For medical educators, our benchmark provides\na tool for assessing AI readiness for educational support roles.\nFor clinicians, the results highlight both opportunities and\nlimitations of current AI systems in clinical decision support.\nFor policymakers, the findings emphasize the need for com-\nprehensive evaluation frameworks and regulatory standards for\nmedical AI deployment.\nIn conclusion, while current LLMs demonstrate impressive\ncapabilities in medical question-answering, achieving perfor-\nmance comparable to human physicians in some domains,\nsignificant work remains before these systems can be safely\nand effectively deployed in clinical practice. The path forward\nrequires not merely larger models, but thoughtful architectural\ninnovation, careful domain adaptation, and rigorous validation\nacross diverse medical contexts. Only through such compre-\nhensive approaches can we realize the full potential of AI in\nadvancing healthcare delivery and medical education.\nREFERENCES\n[1] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language\nmodels encode clinical knowledge,” Nature, vol. 620, no. 7972, pp. 172–\n180, 2023.\n[2] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz,\n“Capabilities of gpt-4 on medical challenge problems,” arXiv preprint\narXiv:2303.13375, 2023.\n[3] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon,\nC. Elepa˜no, M. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo,\nand V. Tseng, “Performance of chatgpt on usmle: Potential for ai-assisted\nmedical education using large language models,” PLOS Digital Health,\nvol. 2, no. 2, p. e0000198, 2023.\n[4] A. Gilson, C. W. Safranek, T. Huang, V. Socrates, L. Chi, R. A. Taylor,\nand D. Chartash, “How does chatgpt perform on the united states\nmedical licensing examination? the implications of large language mod-\nels for medical education and knowledge assessment,” JMIR Medical\nEducation, vol. 9, no. 1, p. e45312, 2023.\n[5] N. Zhang, M. Chen, Z. Bi, X. Liang, L. Li, X. Shang, K. Yin, C. Tan,\nJ. Xu, F. Huang et al., “Chinese medical question answering: A survey\nand benchmark,” arXiv preprint arXiv:2305.12025, 2023.\n[6] W. Wang, L. Zhang, X. Chen, and Y. Liu, “Medical licensing examina-\ntion in china: A strategic analysis,” Medical Teacher, vol. 44, no. 8, pp.\n876–881, 2022.\n[7] M. Liu, H. Wang, and Q. Zhang, “Chinese medical education system:\nA comprehensive review,” BMC Medical Education, vol. 21, no. 1, pp.\n1–12, 2021.\n[8] Z. Chen, A. Hern´andez-Cano, A. Romanou, A. Bonnet, K. Matoba,\nF. Salvi, M. Pagliardini, S. Fan, A. K¨opf, A. Mohtashami et al.,\n“Meditron-70b: Scaling medical pretraining for large language models,”\narXiv preprint arXiv:2311.16079, 2023.\n[9] A. Pal, L. K. Umapathi, and M. Sankarasubbu, “Medmcqa: A large-\nscale multi-subject multi-choice dataset for medical domain question\nanswering,” Proceedings of Machine Learning Research, vol. 174, pp.\n248–260, 2022.\n"}, {"page": 10, "text": "[10] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits,\n“What disease does this patient have? a large-scale open domain question\nanswering dataset from medical exams,” Applied Sciences, vol. 11,\nno. 14, p. 6421, 2021.\n[11] G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas, M. Zschunke,\nM. R. Alvers, D. Weissenborn, A. Krithara, S. Petridis, D. Poly-\nchronopoulos et al., “An overview of the bioasq large-scale biomed-\nical semantic indexing and question answering competition,” in BMC\nBioinformatics, vol. 16.\nSpringer, 2015, pp. 1–28.\n[12] A. Ben Abacha, C. Shivade, and D. Demner-Fushman, “Overview of\nthe mediqa 2019 shared task on textual inference, question entailment\nand question answering,” in Proceedings of the 18th BioNLP Workshop\nand Shared Task, 2019, pp. 370–379.\n[13] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, “Pubmedqa:\nA dataset for biomedical research question answering,” arXiv preprint\narXiv:1909.06146, 2019.\n[14] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, and Z. Ding, “Chinese\nmedical question answer matching using end-to-end character-level\nmulti-scale cnns,” Applied Sciences, vol. 7, no. 8, p. 767, 2017.\n[15] Y. He, Z. Zhu, Y. Zhang, Q. Chen, and J. Caverlee, “Applying deep\nmatching networks to chinese medical question answering: A study and\na dataset,” BMC Medical Informatics and Decision Making, vol. 19,\nno. 2, pp. 91–100, 2019.\n[16] N. Zhang, Q. Jia, K. Yin, L. Dong, F. Gao, and N. Hua, “Cblue:\nA chinese biomedical language understanding evaluation benchmark,”\narXiv preprint arXiv:2106.08087, 2022.\n[17] H. Liu, W. Zhang, and Y. Sun, “Medbench: Towards high-fidelity\nmedical examination benchmarks,” 2025.\n[18] Z. Wang, M. Li, and Q. Xu, “Evaluating large language models on\nprofessional clinical examinations,” 2025.\n[19] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models\nare few-shot learners,” Advances in Neural Information Processing\nSystems, vol. 33, pp. 1877–1901, 2020.\n[20] OpenAI, “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774,\n2023.\n[21] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language\nmodels encode clinical knowledge,” arXiv preprint arXiv:2212.13138,\n2022.\n[22] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou,\nK. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al., “Towards expert-\nlevel medical question answering with large language models,” arXiv\npreprint arXiv:2305.09617, 2023.\n[23] C. Wu, X. Lei, Y. Chen, X. Zhang, J. Yang, K. Li, K. Ma, S. Zheng,\nX. Xu, S. K. Zhou et al., “Pmc-llama: Towards building open-source\nlanguage models for medicine,” arXiv preprint arXiv:2304.14454, 2023.\n[24] Y. Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y. Zhang, “Chatdoctor:\nA medical chat model fine-tuned on llama model using medical domain\nknowledge,” Cureus, vol. 15, no. 6, 2023.\n[25] H. Xiong, S. Wang, Y. Zhu, Z. Zhao, Y. Liu, Q. Wang, and D. Shen,\n“Doctorglm: Fine-tuning your chinese doctor is not a herculean task,”\narXiv preprint arXiv:2304.01097, 2023.\n[26] Y. Labrak, A. Bazoge, E. Morin, P.-A. Gourraud, M. Rouvier, and R. Du-\nfour, “Biomistral: A collection of open-source pretrained large language\nmodels for medical domains,” arXiv preprint arXiv:2402.10373, 2024.\n[27] A. Toma, P. R. Lawler, J. Ba, R. G. Krishnan, B. B. Rubin, and\nM. Volkovs, “Clinical camel: An open-source expert-level medical lan-\nguage model with dialogue-based knowledge encoding,” arXiv preprint\narXiv:2305.12031, 2023.\n[28] H. Zhang, J. Chen, F. Jiang, F. Yu, Z. Chen, J. Li, G. Chen, X. Wu,\nZ. Zhang, Q. Xiao et al., “Huatuogpt, towards taming language model\nto be a doctor,” arXiv preprint arXiv:2305.15075, 2023.\n[29] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, and T. Liu, “Huatuo:\nTuning llama model with chinese medical knowledge,” arXiv preprint\narXiv:2304.06975, 2023.\n[30] S. Yang, H. Zhao, S. Zhu, G. Zhou, H. Xu, Y. Jia, and H. Zan,\n“Zhongjing: Enhancing chinese medical capabilities of large language\nmodel through expert feedback and real-world multi-turn dialogue,”\narXiv preprint arXiv:2308.03549, 2024.\n[31] J. Chen, X. Wang, K. Ji, A. Gao, and B. Wang, “Medicalgpt: Training\nmedical gpt model,” https://github.com/shibing624/MedicalGPT, 2023.\n[32] R. Yang, J. Chen, and T. Liu, “On the robustness of large language\nmodels in medical reasoning,” 2025.\n[33] S. Park, H. Kim, and J. Lee, “Medalign+: Improving safety and guideline\nadherence of medical language models,” 2025.\n[34] S. L. Fleming, A. Lozano, W. J. Haberkorn, J. A. Jindal, E. P. Reis,\nR. Thapa, L. Blankemeier, J. Z. Genkins, E. Steinberg, A. Nayak et al.,\n“Medalign: A clinician-generated dataset for instruction following with\nelectronic medical records,” arXiv preprint arXiv:2308.14089, 2024.\n[35] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F.\nTan, and D. S. W. Ting, “Large language models in medicine,” Nature\nMedicine, vol. 29, no. 8, pp. 1930–1940, 2023.\n[36] M. Zhu, A. Ahuja, W. Wei, and C. K. Reddy, “Multilingual large\nlanguage models for biomedical question answering,” arXiv preprint\narXiv:2305.13052, 2023.\n[37] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\nhery, and D. Zhou, “Cross-lingual transfer of medical language models,”\narXiv preprint arXiv:2305.15329, 2023.\n[38] Y. Chen, H. Zhao, and P. Luo, “Beyond accuracy: A multi-dimensional\nevaluation framework for medical ai,” 2025.\n[39] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bam-\nford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand et al.,\n“Mixtral of experts,” arXiv preprint arXiv:2401.04088, 2024.\n[40] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng,\nX. Yu, Y. Wu et al., “Deepseekmoe: Towards ultimate expert spe-\ncialization in mixture-of-experts language models,” arXiv preprint\narXiv:2401.06066, 2024.\n[41] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann,\nJ. Gao, and H. Poon, “Domain-specific language model pretraining\nfor biomedical natural language processing,” ACM Transactions on\nComputing for Healthcare, vol. 3, no. 1, pp. 1–23, 2021.\n[42] K. Zhou, Z. Sun, and S. Guo, “Sparse mixture-of-experts models for\nmedical question answering,” 2025.\n[43] B. S. Bloom et al., Taxonomy of educational objectives: The classifica-\ntion of educational goals.\nDavid McKay Company, 1956.\n[44] Ollama, “Ollama: Run large language models locally,” https://ollama.ai,\n2024, accessed: 2024-12-01.\n"}]}