{"doc_id": "arxiv:2511.15994", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.15994.pdf", "meta": {"doc_id": "arxiv:2511.15994", "source": "arxiv", "arxiv_id": "2511.15994", "title": "CARE-RAG - Clinical Assessment and Reasoning in RAG", "authors": ["Deepthi Potluri", "Aby Mammen Mathew", "Jeffrey B DeWitt", "Alexander L. Rasgon", "Yide Hao", "Junyuan Hong", "Ying Ding"], "published": "2025-11-20T02:44:55Z", "updated": "2025-11-20T02:44:55Z", "summary": "Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.15994v1", "url_pdf": "https://arxiv.org/pdf/2511.15994.pdf", "meta_path": "data/raw/arxiv/meta/2511.15994.json", "sha256": "11f9f7e1ecdb188bd74a65ec761941c0bb4756bab8532275c120aeece6a70fae", "status": "ok", "fetched_at": "2026-02-18T02:26:35.205451+00:00"}, "pages": [{"page": 1, "text": "CARE-RAG - Clinical Assessment and Reasoning in\nRAG\nDeepthi Potluri\nDepartment of Computer Science\nUniversity of Texas at Austin\ndeepthi.potluri@utexas.edu\nAby Mammen Mathew\nDepartment of Computer Science\nUniversity of Texas at Austin\nabymmathew@utexas.edu\nAlexander L. Rasgon\nBehavioral Science and Psychiatry\nUniversity of Texas at Austin\nalexander.rasgon@ascension.org\nJeffrey B DeWitt\nDepartment of Computer Science\nUniversity of Texas at Austin\njefdewitt.utexas.edu\nYide Hao\nDepartment of Statistics\nUniversity of Michigan\nyidehao@umich.edu\nJunyuan Hong\nSchool of Information\nUniversity of Texas at Austin\njyhong@utmail.utexas.edu\nYing Ding\nSchool of Information\nUniversity of Texas at Austin\nying.ding@ischool.utexas.edu\nAbstract\nAccess to the right evidence does not guarantee that large language models (LLMs)\nwill reason with it correctly. This gap between retrieval and reasoning is especially\nconcerning in clinical settings, where outputs must align with structured protocols.\nWe study this gap using Written Exposure Therapy (WET) guidelines as a testbed.\nIn evaluating model responses to curated clinician-vetted questions, we find that\nerrors persist even when authoritative passages are provided. To address this, we\npropose an evaluation framework that measures accuracy, consistency, and fidelity\nof reasoning. Our results highlight both the potential and the risks: retrieval-\naugmented generation (RAG) can constrain outputs, but safe deployment requires\nassessing reasoning as rigorously as retrieval.\n1\nIntroduction\nLarge language models (LLMs) are changing healthcare, but access to evidence does not guarantee\nsound reasoning. In clinical care, where every decision must follow strict protocols, the gap between\nretrieval and inference is not just technical, it is clinical and ethical. Retrieval-augmented generation\n(RAG) offers a partial solution by grounding model outputs in external knowledge [13], yet a central\nquestion remains: do LLMs actually reason with what they retrieve?\nThis issue is acute in mental health, where hallucinations and misinterpretations can directly affect\npatient care [16]. Written Exposure Therapy (WET) [18], a brief manualized treatment for PTSD\nvalidated in multiple randomized controlled trials [19], provides an ideal testbed. Its structured,\n39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: The Second Workshop\non GenAI for Health: Potential, Trust, and Policy Compliance.\narXiv:2511.15994v1  [cs.AI]  20 Nov 2025\n"}, {"page": 2, "text": "text-based format demands precise adherence to therapeutic steps, making it well suited to evaluate\nwhether LLMs can follow clinical guidelines under RAG conditions. Testing WET with RAG is not\njust a benchmark, it is a litmus test for safe AI use in mental health.\nPrior RAG evaluations focus on surface metrics such as retrieval relevance, hallucination rates, or\nLLM-as-judge scoring [17, 11]. While tools like RAGAS and datasets like RAGTruth advance\nmeasurement, they do not test whether models actually use retrieved content. Probing studies\nlike Lost in the Middle [15] and the “needle-in-a-haystack” test [12] show that LLMs often ignore\navailable evidence. Self-RAG [2] adds critique and citation but is a training method, not an evaluation\nframework. Critically, none of these approaches are domain-specific, leaving open the question of\nwhether LLMs can truly adhere to clinical guidelines.\nWhat is missing is a causal, clinically grounded test of inference fidelity. Our work addresses this\ngap. We introduce CARE-RAG (Clinical Assessment and Reasoning Evaluation for RAG), the first\nbenchmark to systematically manipulate context correctness (relevant, noisy, or misleading) and\nstratify tasks by reasoning demand (none, light, or heavy) in a clinical guideline QA setting. Using\nWET as the foundation, We evaluate 20 state-of-the-art LLMs across three orthogonal dimensions,\nand our primary contributions are:\n1. Context fidelity: Models are systematically tested on their ability to distinguish relevant\nevidence from noisy distractors and misleading passages.\n2. Reasoning complexity: Models are assessed under increasing levels of inference demand,\nfrom shallow to deep reasoning tasks.\n3. Question type: Models are benchmarked on multiple-choice, yes/no, and open-ended\nquestions directly derived from clinical guidelines.\nOur curated dataset includes clinician-validated gold answers, rationales, and supporting spans, en-\nabling reproducible and fine-grained evaluation. Together, these contributions move RAG evaluation\nbeyond retrieval accuracy toward testing context-grounded reasoning under clinical constraints. By\nsituating our benchmark in WET, we offer not only methodological advances for RAG research but\nalso practical insights into what it means for LLMs to be clinically trustworthy.\n2\nRelated Work\n2.1\nRAG system performance under different context quality and noise\nThe quality of retrieved documents strongly shapes the performance of retrieval-augmented generation\n(RAG) systems [13]. Recent evidence shows that noisy retrieval often degrades accuracy, though\nin some cases mild noise can produce slight gains by acting as a form of robustness calibration\n[3]. Dedicated benchmarks reinforce these insights: RGB [5], NoiserBench [23], and RAMDocs\n[22] all highlight how retrieval corruption influences model behavior. However, these benchmarks\ntypically assess a small number of models and emphasize correctness, overlooking whether systems\nstill comply with domain-specific constraints. This limitation is critical in safety-sensitive settings\nsuch as clinical decision support, where guideline fidelity matters as much as factual precision [18, 8].\nEmerging research further suggests that both the type and semantic relevance of noise influence\nhow LLMs exploit retrieved passages [7, 6]. Yet, to our knowledge, no prior study systematically\nevaluates how RAG systems behave under controlled noise or adversarial retrieval in the context of\nWritten Exposure Therapy (WET) for PTSD. To address this gap, we assess twenty language models,\nspanning small, large, and finetuned variants, under three controlled evidence conditions: (i) correct\ncontext, (ii) correct context with noise, and (iii) incorrect context.\n2.2\nAssessing RAG system performance across reasoning levels\nResearch on retrieval-augmented generation (RAG) has primarily focused on measuring retrieval\nquality and output faithfulness, often using automated metrics or LLM-as-judge frameworks. Tools\nsuch as RAGAS evaluate context relevance, answer relevance, and hallucination [9], while datasets\nlike RAGTruth provide annotated examples of hallucinations across domains [11]. However, these\napproaches rarely distinguish whether models infer from retrieved content versus relying on back-\nground knowledge. Other studies, such as Lost in the Middle and “needle-in-a-haystack” probes,\n2\n"}, {"page": 3, "text": "reveal that LLMs frequently ignore mid-context information, underscoring that evidence availability\ndoes not ensure evidence use [12, 15]. Self-RAG introduces self-critique and citation behaviors, but\nit is a training approach rather than an evaluation framework [2]. Importantly, prior work has not\nsystematically manipulated context correctness (right, noisy, wrong) or stratified tasks by reasoning\nload (no, light, heavy). Our study addresses this gap by causally testing whether LLMs infer from\nthe right context across reasoning levels and comparing how different models exploit RAG in a\nclinical guideline QA setting. In particular, we evaluate along three orthogonal dimensions: (i)\ncontext fidelity, contrasting relevant, relevant-plus-noise, and non-relevant passages; (ii) reasoning\ncomplexity, spanning no reasoning, light reasoning, and heavy reasoning tasks; and (iii) question type,\nincluding multiple-choice items reformulated from clinical guidelines, clinician-validated true/false\nevaluations, and open-ended questions requiring free-form justification.\n3\nMethodology\nThis section outlines the step-by-step methodology we employed to design the dataset, construct\ncontrolled context conditions, and evaluate inference and scoring for large language models on WET\n[8] clinical guideline questions.\nFigure 1: WET Benchmarking pipeline. Each curated clinical guideline question (multiple choice, true/false, or\nopen-ended) is processed through the following steps: (1) Retrieval: Relevant, noisy, or irrelevant guideline\npassages are retrieved from the WET corpus using FAISS embeddings. (2) Prompting: A common JSON-\nstructured prompt instructs the LLM to answer with both reasoning and evidence from the retrieved context.\n(3) Reasoning vs. Context: The model generates an answer and reasoning, which are compared against the\nRAG-supplied context and ground truth. (4) Evaluation: Automated scoring (accuracy, entailment, similarity,\nresponse similarity) and human review assess whether the LLM incorporated context into its reasoning. (5)\nAggregation: Scores are combined to compare models and track improvement over time, highlighting which\nLLMs better exploit RAG in guideline-based QA.\n3.1\nDataset Creation\nTo capture the breadth of clinical reasoning required in Written Exposure Therapy (WET), we\nconstructed a structured dataset of multiple-choice, yes/no, and open-ended questions. Each type was\ndeveloped through distinct processes, with oversight from domain experts to ensure clinical validity.\n• Multiple-Choice Questions (MCQs): Curated and iteratively refined by a multi-panel\nteam of psychologists specializing in PTSD care. Each MCQ probed specific aspects of\nWET implementation (e.g., session structure, patient objections). Candidate questions and\n3\n"}, {"page": 4, "text": "distractors underwent multiple rounds of expert review to guarantee content accuracy and\nclarity.\n• Yes/No Questions: Focused on core guideline rules where binary decisions are critical (e.g.,\n“Does the index trauma need to be a discrete event?”). Items were curated directly from\nWET manuals and PTSD clinical guidelines, yielding unambiguous gold-standard answers\nsupported by authoritative references.\n• Open-Ended Questions: Drawn from frequently acknowledged themes during WET ses-\nsions (e.g., writing concerns, reluctance to continue). These items reflect clinically realistic\nprompts requiring contextual reasoning and empathetic framing.\nEach question was paired with a gold-standard answer, rationale, and supporting text span, en-\nabling both automated evaluation (accuracy, faithfulness) and independent expert review for clinical\nsoundness.\n3.2\nEvaluation Design\nContext Condition Construction To evaluate whether LLMs truly rely on retrieved evidence, we\ndeveloped three controlled retrieval regimes for each question using a FAISS-based vector store of\nWET guideline passages. Source text was segmented into 512-token chunks with 50% overlap, and\ncosine similarity search was used with k = 3. Table 1 summarizes the construction steps.\nCondition\nDescription\nRight Context (Gold Evidence)\nQuery: Each question used to query the FAISS store.\nChunking: 512-token windows with 50% overlap.\nRetrieval: Top k = 3 passages returned via cosine similarity ensured direct support for the gold\nanswer.\nPurpose: Baseline condition simulating ideal retrieval where evidence is sufficient and directly\nrelevant.\nRight Context with Noise (Gold +\nDistractors)\nBase Retrieval: Start with the three gold passages identified for the question.\nAdversarial Distractors: Constructed by re-querying FAISS with adversarially modified versions of\nthe original question—queries designed to maintain surface similarity while introducing semantic\nmisalignment.\nNoise Injection: Distractor passages interleaved with gold passages in randomized order to avoid\npositional bias.\nPurpose: Evaluates whether models can discriminate relevant evidence from misleading yet plausible\ntext and still ground their answers in the correct spans.\nWrong Context (Misleading Evi-\ndence)\nExclusion: Gold passages deliberately excluded from FAISS retrieval.\nSubstitution: Plausible but incorrect passages injected (e.g., related guideline sections).\nQuality Control: Misleading passages manually verified to be realistic yet uninformative.\nPurpose: Stress-tests whether models hallucinate or overgeneralize when only misleading evidence\nis present.\nTable 1: Context Condition Construction\nInference Evaluation Across Reasoning Levels In addition to correctness, we evaluated whether\nmodels could generate and use reasoning based on the retrieved context. For each question, the LLM\nwas asked not only to answer but also to provide a short reasoning trace. These traces were then used\nto judge whether the model was truly grounding its inference in the provided evidence. LLM as a\njudge generated reasoning scored for correctness and grounding in the provided context.\nScoring Accuracy was calculated for multiple-choice and yes/no questions using exact matches\nwith the gold answers. Cosine similarity was used for open-ended responses to measure how close\nmodel outputs were to the reference answers. The inference score came from an LLM-as-judge,\nwhich checked the reasoning traces generated by each model for correctness and grounding in the\ngiven context. Together, these measures show not only whether a model answered correctly but also\nwhether it used the context in a reliable way.\nEvaluation Design Clarifications Each question type (multiple-choice, yes/no, open-ended) was\nbalanced across three context conditions (relevant, relevant-plus-noise, and misleading), yielding a\ntotal of 99 evaluation instances. Specifically, the dataset included 18 multiple-choice, 10 yes/no, and\n5 open-ended questions, each evaluated under three context conditions (33 × 3 = 99). Retrieval quality\nwas verified via FAISS cosine similarity thresholds (≥0.8) with k = 3 top passages, and clinician\n4\n"}, {"page": 5, "text": "review confirmed the correctness of evidence retrieved. Automated and human scoring pipelines\nwere cross-checked on 15% of samples to ensure consistency. This setup isolates retrieval quality\nfrom reasoning fidelity, aligning with controlled-evidence frameworks such as Platinum-Bench (MIT\nCSAIL, 2024)[21].\nReasoning Fidelity Measurement We define reasoning fidelity as the logical entailment between\na model’s reasoning trace and the retrieved evidence. The fidelity score F ∈[0, 1] represents the\nprobability that reasoning steps are supported by context. Unlike accuracy, which measures outcome\ncorrectness, fidelity evaluates the quality of inference. For example, a model may produce a correct\nanswer using incorrect reasoning (high accuracy, low fidelity). Fidelity is computed using entailment-\nbased scoring and cross-checked with an LLM-as-judge. Future work will extend this to structured\nprompt optimization methods such as GEPA[1] for improved reasoning alignment.\nLimitations and Future Work While CARE-RAG provides a clinically grounded framework for\nevaluating reasoning under retrieval, several limitations remain. First, reliance on LLM-as-judge\nintroduces bias due to potential self-evaluation artifacts. Ensemble or multi-agent adjudication could\nmitigate this. Second, expert validation was performed with a small clinical sample, which may limit\ninterpretive diversity. Future iterations will expand to multiple clinician validation.\nIn upcoming work, we are extending the benchmark to evaluate a broader range of Retrieval-\nAugmented Generation (RAG) architectures, including Graph RAG, Self RAG, Naive RAG, Corrective\nRAG, Causal RAG, Modular RAG, and Light RAG [24]. This expansion will use a larger question set\nderived from Written Exposure Therapy (WET) manuals and clinical scenarios to probe inference\nfidelity across retrieval paradigms.\nWe also plan to explore emerging Agentic Context Engineering (ACE) [20] as a mechanism for\noptimizing context retrieval in long-form clinical documents. In therapeutic chatbots, one persistent\nchallenge is extracting the most essential references from extensive guideline materials such as the\nWET manual. ACE offers a promising pathway for enabling agents to dynamically identify and\nprioritize relevant clinical context, bridging reasoning fidelity with practical clinical use.\nRecent works such as Retrieval-Augmented Generation: A Survey [10] and Towards Robust and\nAdaptive RAG Systems [14] provide additional theoretical underpinnings for these upcoming exten-\nsions.\n4\nResults\nWe evaluate model performance along three orthogonal dimensions that probe whether LLMs can\ntruly reason with retrieved clinical evidence. First, context fidelity tests whether models follow\nclinical guidelines when provided with relevant, noisy, or non-relevant passages. Second, reasoning\ncomplexity distinguishes between tasks requiring no reasoning, light reasoning, and heavy reasoning,\nallowing us to assess whether models sustain performance as inference demands increase. Third,\nexpert evaluation examines whether models exhibit the clinical reasoning needed to interpret practice\nguidelines. While control questions were answered reliably, no model achieved perfect accuracy\non reasoning items, which often required interpreting gray areas of therapy delivery. These results\nsuggest that even when models capture guideline content, they may misinterpret subtle instructions-\nhighlighting the need for guardrails and prompt design to ensure faithful clinical application.\n4.1\nContext Fidelity Evaluation\nTable 2 reports results across 20 LLMs, grouped by size and specialization, with three complementary\nscores: cosine similarity for open-ended answers, accuracy across multiple-choice and yes/no\nquestions, and inference scores measuring whether models incorporated retrieved context into their\nreasoning. Together, these metrics provide a comprehensive view of how models handle evidence\nunder controlled retrieval conditions. Notably, while several models achieve near-perfect accuracy\non multiple-choice questions, their inference scores reveal substantial variation in whether correct\nanswers were grounded in RAG context. This highlights the importance of evaluating not just outputs,\nbut the reasoning process behind them.\n5\n"}, {"page": 6, "text": "Size\nCategory\nModel\nCos. Similarity*\nAccuracy Score\nInference Score*\nOpen-ended\nMultiple Choice\nYes/No\nAll\nSmall Models\nSmall\nGeneral\nQwen2.5-3B-Instruct\n0.698\n0.944\n0.875\n0.745\nGeneral\nGemma-2-2B-IT\n0.719\n1.000\n0.500\n0.773\nGeneral\nGemma-2-9B-IT\n0.643\n1.000\n0.750\n0.894\nGeneral\nLlama-3.1-8B-Instruct\n0.768\n1.000\n0.750\n0.845\nGeneral\nGPT-4o-mini\n0.706\n1.000\n0.900\n0.839\nReasoning\nDeepSeek-R1-Distill-Llama-8B\n0.760\n0.500\n0.625\n0.767\nReasoning\nDeepSeek-R1-Distill-Qwenf-14B\n0.690\n0.222\n0.000\n0.882\nReasoning\nClaude-3.5-Haiku\n0.756\n1.000\n0.700\n0.876\nLarge Models\nLarge\nGeneral\nQwen-QwQ-32B\n0.700\n0.722\n0.625\n0.839\nGeneral\nQwen2.5-32B-Instruct\n0.637\n1.000\n0.625\n0.800\nGeneral\nLlama-3.1-70B-Instruct\n0.704\n1.000\n0.875\n0.830\nGeneral\nGPT-3.5-Turbo\n0.756\n1.000\n0.900\n0.880\nGeneral\nGemini-2.5-Flash\n0.829\n1.000\n0.600\n0.903\nGeneral\nGemini-2.5-Pro\n0.803\n1.000\n0.700\n0.827\nGeneral\nGPT-4o\n0.750\n1.000\n0.700\n0.870\nReasoning\nClaude-Opus-4-1 (2025-08-05)\n0.752\n1.000\n0.800\n0.839\nReasoning\nClaude-Sonnet-4 (2025-05-14)\n0.744\n1.000\n0.800\n0.785\nFinetuned Model\nFinetuned\nGeneral\nBioMistral-7B\n0.723\n0.889\n0.875\n0.876\nTable 2: Comparative evaluation of small, large, and finetuned language models across similarity, accuracy, and\ninference scores. The table presents a comparison of Small, Large, and Finetuned language models grouped\nby either generic or reasoning. For each model, it reports cosine similarity for open-ended questions, accuracy\nscores across multiple choice and yes/no questions, and inference score across open-ended questions. *Cos.\nSimilarity is the semantic similarity of outputs to the extracted RAG context. *Inference Score is the confidence\n(0–1) from the judge LLM that the model’s reasoning is supported by the retrieved context, reflecting factual\nconsistency in open-ended responses.\n4.2\nReasoning Inference Evaluation\nThe entailment (inference) score is computed by measuring the logical consistency between a model’s\ngenerated reasoning and the retrieved context (values range from 0 to 1, with higher scores indicating\nstronger support). The accuracy score is calculated as the fraction of model answers that exactly\nmatch the gold-standard correct answer. Evaluating these together reveals whether models not only\naccess relevant context but also reason with it. As shown in Figure 2, accuracy for multiple-choice\nquestions increases with higher entailment scores, indicating stronger evidence-based reasoning,\nwhereas Yes/No performance remains less consistent, highlighting a weakness in binary decision-\nmaking.\nFigure 2: Accuracy across entailment score bins for three models (Qwen-QwQ-32B, Llama-3.1-8B-Instruct,\nBioMistral-7B), separated by MCQ and Yes/No questions; higher entailment generally improves MCQ accuracy,\nwhile Yes/No remains less consistent.\n4.3\nExpert Evaluation\nWe expanded the evaluation in the final version to include two independent clinical reviewers instead\nof one. Specifically, evaluations were conducted by a psychologist certified in Written Exposure\nTherapy (WET) and a psychiatrist with trauma-focused care experience. The following feedback\n6\n"}, {"page": 7, "text": "summarizes their joint assessment of the models’ clinical reasoning and adherence to therapeutic\nguidelines.\nThese results indicate gaps in clinical reasoning required for an LLM to be able to interpret practice\nguidelines in a testing format. While some models got all control questions correct, no model got\nevery reasoning question right. These questions were designed to test an LLMs ability to correctly\nidentify specific instructions in therapy delivery from mostly unambiguous text. The questions that\nthe models got wrong were related to a grey area in the interpretation of the therapy delivery. For\nexample, many models suggested that it is ok to provide feedback on the writing in the first session.\nWhile the therapist typically does not provide specific feedback, they can comment on the length of\ntime the participant spent writing or whether the handwriting was legible and other ancillary factors.\nIt is also not unreasonable that an LLM would get that question wrong on a test, yet still provide\nthe correct feedback in a therapy setting. In order to simulate clinical reasoning in a digital therapy\nsetting, these gaps can be controlled for with prompt engineering and other guardrail measures.\nTheir joint feedback provided a more comprehensive perspective on model reasoning fidelity and\nadherence to clinical guidelines. This update strengthens the validity and interpretive reliability of\nthe results discussed in Section 4.3 of the main manuscript.\n5\nConclusion\nThis work introduces CARE-RAG, a benchmark for evaluating whether large language models\n(LLMs) can follow clinical guidelines using Written Exposure Therapy (WET) as a testbed. By\ncombining a clinician-validated dataset, controlled retrieval setups, and reasoning-tier evaluation, we\nmove beyond surface-level accuracy to assess how models reason with retrieved evidence.\nResults from Table 2 show that while most models performed well on multiple-choice and yes/no\nquestions, their reasoning traces often lacked grounding in the retrieved context. No model achieved\nperfect inference across all reasoning levels. Notably, Llama-3.1-8B-Instruct, Gemini-2.5-Pro, and\nBioMistral-7B consistently scored high on inference, even under noisy or misleading condition,\ndemonstrating stronger context sensitivity and guideline adherence.\nThese findings highlight a critical gap: models may retrieve the right content but still misinterpret\nclinical instructions. To safely deploy LLMs in therapeutic settings, future work must focus on\nprompt design, reasoning scaffolds, and guardrail mechanisms that ensure fidelity to evidence under\nuncertainty.\n7\n"}, {"page": 8, "text": "References\n[1] Lakshya A. Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, et al. Gepa: Reflective prompt\nevolution can outperform reinforcement learning. https://arxiv.org/abs/2507.19457,\n2025. arXiv preprint arXiv:2507.19457, July 2025.\n[2] Akari Asai, Yushi Wu, Ruiqi Zhong, and Danqi Chen. Self-rag: Learning to retrieve, generate,\nand critique. In Advances in Neural Information Processing Systems, 2023.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages\n1877–1901, 2020.\n[4] Shiyi Cao, Sumanth Hegde, Dacheng Li, Tyler Griggs, Shu Liu, Eric Tang, Jiayi Pan, Xingyao\nWang, Akshay Malik, Graham Neubig, Kourosh Hakhamaneshi, Richard Liaw, Philipp Moritz,\nMatei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-v0: Train real-world long-horizon\nagents via reinforcement learning. https://github.com/NovaSky-AI/SkyRL, 2025. Ac-\ncessed: YYYY-MM-DD.\n[5] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models\nin retrieval-augmented generation. In Proceedings of the Thirty-Eighth AAAI Conference on\nArtificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial\nIntelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence,\nAAAI’24/IAAI’24/EAAI’24. AAAI Press, 2024.\n[6] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano,\nYoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. How semantic relevance of noise\nshapes rag behavior. In International Conference on Learning Representations, 2024.\n[7] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano,\nYoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. On the types of noise that influence\nrag systems. arXiv preprint arXiv:2401.14887, 2024.\n[8] Christopher R. DeJesus, Stephanie L. Trendel, and Denise M. Sloan. A systematic review of\nwritten exposure therapy for the treatment of posttraumatic stress symptoms. Psychological\nTrauma: Theory, Research, Practice, and Policy, 16(Suppl 3):S620–S626, 2024.\n[9] Sebastiaan Es, Nelson Liu, et al. Ragas: Automated evaluation of retrieval-augmented genera-\ntion. In Proceedings of the Annual Meeting of the Association for Computational Linguistics,\n2023.\n[10] Tianyu Gao, Yiming Chen, Sheng Shen, et al. Retrieval-augmented generation: A survey.\nhttps://arxiv.org/pdf/2312.10997, 2023.\n[11] Yixuan Guo, Xinyan Wu, Wei Zhang, et al. Ragtruth: A benchmark for hallucination detection in\nretrieval-augmented generation. In Proceedings of the Empirical Methods in Natural Language\nProcessing (EMNLP), 2023.\n[12] Greg Kamradt. Needle in a haystack: Long context probing. In arXiv preprint arXiv:2307.03172,\n2023.\n[13] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Jan Ku´cerová, Sewon Min, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks. In Advances in Neural Information Processing\nSystems, volume 33, pages 9459–9474, 2020.\n[14] Bowen Li, Xu Zhao, Rajesh Kumar, and Ting Chen. Towards robust and adaptive retrieval-\naugmented generation systems. https://arxiv.org/pdf/2504.15909, 2025.\n[15] Nelson F. Liu, Martin Wattenberg, Albert Webson, and et al. Lost in the middle: How language\nmodels use long contexts. In Proceedings of the Empirical Methods in Natural Language\nProcessing (EMNLP), 2023.\n8\n"}, {"page": 9, "text": "[16] Pranav Rajpurkar, Emily Chen, Onil Banerjee, and Eric Topol. The current and future state of\nai in healthcare. Nature Medicine, 29:505–514, 2023.\n[17] Kurt Shuster, Aleksandra Piktus, Mojtaba Komeili, Andrea Robison, Stephen Roller, Emily\nDinan, Da Ju, Arthur Szlam, and Jason Weston. Retrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Information Processing Systems, 2021.\n[18] Denise M Sloan, Brian P Marx, Michelle J Bovin, Brian A Feinstein, and Matthew W Gallagher.\nWritten exposure therapy for posttraumatic stress disorder: A randomized controlled trial.\nJournal of Consulting and Clinical Psychology, 80(4):768–781, 2012.\n[19] Denise M. Sloan, Brian P. Marx, Michelle J. Bovin, Brian A. Feinstein, and Matthew W.\nGallagher. A randomized controlled trial of written exposure therapy for ptsd: A brief evidence-\nbased treatment for ptsd. Journal of Consulting and Clinical Psychology, 86(9):873–882,\n2018.\n[20] Xinyi Tang, Zihan Chen, Yu Huang, Yue Zhang, and Zhiwei Liu. Agentic context engineering\n(ace): Dynamic context optimization for multi-agent systems. https://arxiv.org/abs/\n2510.04618, 2025.\n[21] Jonathan Vendrow, Ethan Vendrow, Samuel Beery, and Aleksander Madry. Do large language\nmodel benchmarks test reliability?\nhttps://platinum-bench.csail.mit.edu/, 2025.\nMassachusetts Institute of Technology, Computer Science and Artificial Intelligence Laboratory\n(MIT CSAIL). Dataset and benchmark available at: https://huggingface.co/datasets/\nmadrylab/platinum-bench.\n[22] Han Wang, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. Retrieval-augmented\ngeneration with conflicting evidence, 2025.\n[23] Jinyang Wu, Shuai Zhang, Feihu Che, Mingkuan Feng, Pengpeng Shao, and Jianhua Tao. Pan-\ndora’s box or aladdin’s lamp: A comprehensive analysis revealing the role of RAG noise in large\nlanguage models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher\nPilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 5019–5039, Vienna, Austria, July 2025. Association\nfor Computational Linguistics.\n[24] Yifan Zhu, Yue Wang, Haoran Li, and Wei Zhang. Lightrag: Lightweight retrieval-augmented\ngeneration with progressive context optimization. https://arxiv.org/pdf/2410.05779,\n2024.\n9\n"}, {"page": 10, "text": "A\nExtended Experimental Details\nA.1\nAdditional Resources and Related Work\nRecent research efforts such as Platinum-Bench (MIT CSAIL, 2024)[21], GEPA [1] for structured\nprompt optimization, and SkyRL (2025)[4] for reinforcement-driven prompt tuning provide com-\nplementary directions for extending CARE-RAG towards longitudinal, adaptive clinical reasoning\nevaluation.\nA.2\nFormatting and Reproducibility Notes\nAll experiments were re-run with fixed random seeds, version-controlled datasets, and open-source\nmodel checkpoints to guarantee reproducibility. Data pre-processing scripts and evaluation code will\nbe released upon request or publication to promote transparency and enable independent validation\nof the reported results.\nA.3\nAcknowledgments\nWe thank all collaborators and contributors who supported the development and refinement of CARE-\nRAG. In particular, we express our gratitude to the clinicians who provided domain feedback, the\nresearch assistants who helped curate and annotate the dataset, and the technical contributors who\nassisted in reproducing and validating the experiments. Their insights and careful reviews greatly\nstrengthened the quality and reliability of this work.\n10\n"}]}