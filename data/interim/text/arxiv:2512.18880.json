{"doc_id": "arxiv:2512.18880", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.18880.pdf", "meta": {"doc_id": "arxiv:2512.18880", "source": "arxiv", "arxiv_id": "2512.18880", "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction", "authors": ["Ming Li", "Han Chen", "Yunze Xiao", "Jian Chen", "Hong Jiao", "Tianyi Zhou"], "published": "2025-12-21T20:41:36Z", "updated": "2025-12-21T20:41:36Z", "summary": "Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.18880v1", "url_pdf": "https://arxiv.org/pdf/2512.18880.pdf", "meta_path": "data/raw/arxiv/meta/2512.18880.json", "sha256": "a140383697a1e5be45bedc4f7e4edb115eb1173900d55168f543f29b8f1d03ff", "status": "ok", "fetched_at": "2026-02-18T02:23:59.980584+00:00"}, "pages": [{"page": 1, "text": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment\nwith Proficiency Simulation for Item Difficulty Prediction\nMing Li*1, Han Chen*, Yunze Xiao2, Jian Chen3, Hong Jiao1, Tianyi Zhou\n1University of Maryland\n2Carnegie Mellon University\n3University at Buffalo\nminglii@umd.edu, tianyi.david.zhou@gmail.com\n Project: https://github.com/MingLiiii/Difficulty_Alignment\nAbstract\nAccurate estimation of item (question or task)\ndifficulty is critical for educational assessment\nbut suffers from the cold start problem. While\nLarge Language Models demonstrate superhu-\nman problem-solving capabilities, it remains\nan open question whether they can perceive the\ncognitive struggles of human learners. In this\nwork, we present a large-scale empirical anal-\nysis of Human-AI Difficulty Alignment for\nover 20 models across diverse domains such as\nmedical knowledge and mathematical reason-\ning. Our findings reveal a systematic misalign-\nment where scaling up model size is not reliably\nhelpful; instead of aligning with humans,\nmodels converge toward a shared machine\nconsensus. We observe that high performance\noften impedes accurate difficulty estimation, as\nmodels struggle to simulate the capability lim-\nitations of students even when being explicitly\nprompted to adopt specific proficiency levels.\nFurthermore, we identify a critical lack of in-\ntrospection, as models fail to predict their own\nlimitations. These results suggest that general\nproblem-solving capability does not imply an\nunderstanding of human cognitive struggles,\nhighlighting the challenge of using current\nmodels for automated difficulty prediction.\n1\nIntroduction\nAccurate estimation of item difficulty is the\ncornerstone of educational assessment (Hambleton\net al., 1991; Hsu et al., 2018; AlKhuzaey et al.,\n2021; Peters et al., 2025). It underpins critical\napplications such as curriculum design, automated\ntest generation, and automated item generation\nwith controlled difficulty levels (DeMars, 2010;\nLord, 2012).\nTraditionally, obtaining accurate\ndifficulty parameters (e.g., within Item Response\nTheory (IRT) models (Baker, 2001; Lalor et al.,\n2024)) relies on extensive field testing, a process\nthat requires administering questions to large\n*Equal Contribution.\ncohorts of real test-takers to observe response\npatterns. This reliance creates a significant cold\nstart problem: newly generated questions lack the\nhistorical response data necessary to statistically es-\ntimate their parameters, effectively rendering them\nunusable in adaptive systems until they undergo\nexpensive and time-consuming pre-testing cycles.\nPrior approaches to Item Difficulty Prediction\n(IDP) generally treated the task as a supervised\nlearning problem, relying on linguistic features or\ndeep learning models trained on known item param-\neters estimated based on item response data (Hsu\net al., 2018; Benedetto, 2023; Li et al., 2025b).\nWhile effective within specific domains, these\nmethods depend heavily on the availability of his-\ntorical performance data for training, limiting their\nutility in cold-start scenarios (i.e., no historical\ntested data is available for training). The emer-\ngence of LLMs (OpenAI, 2024b; Hurst et al., 2024;\nTouvron et al., 2023; Qwen-Team, 2024, 2025a)\noffers a potential paradigm shift. With their vast\npre-training and exceptional problem-solving capa-\nbilities, LLMs seemingly possess the knowledge\nrequired to analyze complex content. However, it\nremains an open question whether these general-\npurpose models can align with human percep-\ntion of difficulty without task-specific fine-tuning.\nThere is a fundamental distinction between solving\na problem and evaluating its difficulty: a model\nthat effortlessly surpasses human baselines in per-\nformance may fail to recognize the cognitive hur-\ndles faced by an average learner (Sweller, 1988,\n2011; Noroozi and Karami, 2022; Li et al., 2025c).\nThis study investigates this Human-AI Difficulty\nAlignment, exploring whether off-the-shelf LLMs\ncan bridge the gap between their own capabilities\nand the student struggles, whose difficulty values\nare obtained from real student field testing.\nTo investigate this, we propose a comprehensive\nempirical study that evaluates this Difficulty Align-\nment through two distinct lenses: the model as an\n1\narXiv:2512.18880v1  [cs.CL]  21 Dec 2025\n"}, {"page": 2, "text": "external observer (predicting others’ difficulty)\nand an internal actor (experiencing difficulty it-\nself).\nOur study operates at scale, benchmark-\ning over 20 LLMs, spanning both open-weights\nand closed-source families, including reasoning-\nspecialized models, across four diverse educational\ndomains: language proficiency (Cambridge) (Mul-\nlooly et al., 2023), reasoning and logic (SAT Read-\ning/Writing, SAT Math), and professional medical\nknowledge (USMLE) (Yaneva et al., 2024).\nWe structure our investigation around three pri-\nmary dimensions to disentangle the relationship\nbetween intrinsic capability and extrinsic percep-\ntion. First, we go beyond simple ground-truth cor-\nrelation to analyze inter-model consensus, examin-\ning whether models form a cohesive machine per-\nception that systematically diverges from students.\nSecond, we quantify the capability-perception gap\nusing Item Response Theory (IRT). By treating\nthe model pool as a cohort of synthetic students,\nwe derive empirical machine difficulty based on\nthe actual correctness of LLMs, allowing us to test\nfor the Curse of Knowledge, where items challeng-\ning for humans are trivial for machines. Finally,\nwe evaluate Metacognitive Alignment and Profi-\nciency Simulation (Tseng et al., 2024; Zhang et al.,\n2024b), rigorously testing whether models possess\nthe introspection to predict their own limitations or\nthe flexibility to authentically simulate the cogni-\ntive struggles of lower-proficiency students.\nKey Findings.\n1. Systematic Misalignment: Contrary to stan-\ndard capability metrics, scaling does not reliably\ntranslate into alignment. Increasing model scale\ndoes not improve difficulty predictions; instead,\nmodels form a cohesive Machine Consensus,\naligning significantly stronger with each other\nthan with human reality.\n2. Limits of Simulation: Neither extrinsic ensem-\nbling nor proficiency simulation serves as a re-\nliable fix for the misalignment. Ensemble per-\nformance is strictly bounded by weaker models,\nwhile proficiency simulation proves highly in-\nconsistent as models struggle to authentically\nmimic different proficiency levels.\n3. The Curse of Knowledge: Our IRT-based anal-\nysis reveals a fundamental mechanistic diver-\ngence: the difficulty derived from models’ actual\ncorrectness correlates even worse with humans\nthan their explicit perceptions. Items that are\ndifficult for humans are frequently trivial for\nmodels, and this capability exhibits significant\ninertia even under weak student prompts.\n4. Metacognitive Blindness: We identify a criti-\ncal lack of introspection. With AUROC scores\nhovering near random guessing, models fail to\npredict their own limitations, indicating that ex-\nplicit difficulty estimates are effectively decou-\npled from the model’s actual correctness, lacking\nthe internal signal to ground their predictions.\n2\nFormulation and Evaluation Metrics\n2.1\nTask Formulation\nWe formalize the IDP task as a function approxima-\ntion problem over a dataset D = {(xi, a∗\ni , yi)}N\ni=1.\nEach entry comprises the item context xi (includ-\ning the question stem, optional passages, and can-\ndidate options), the ground truth answer a∗\ni , and\nthe difficulty label yi ∈Y obtained from real-\nworld student field testing.\nThe label space Y\nadapts to the domain, ranging from continuous val-\nues (e.g., yi ∈[0, 1]) to discrete categories (e.g.,\nY = {Easy, Medium, Hard}).\nLet M denote the set of LLMs under evalua-\ntion. We investigate the alignment between human\ndifficulty yi and AI cognition through two distinct\nmodalities: Difficulty Perception (the model’s esti-\nmation as an observer) and Problem-Solving Capa-\nbility (the model’s performance as an actor).\nThe Observer View: Difficulty Perception.\nIn\nthis mode, the model simulates an educator or test\ndeveloper tasked with estimating item difficulty.\nTo isolate perception from solving capability, the\nmodel is provided with the full item context xi,\nthe correct solution a∗\ni , and an optional proficiency\nprompt p. The predicted difficulty ˆyi,m for model\nm ∈M is formulated as:\nˆyi,m = ϕ (Genm(xi, a∗\ni , p))\n(1)\nwhere Genm(·) denotes the natural language gen-\neration process, and ϕ(·) is a parsing function that\nmaps the generated response to a normalized nu-\nmerical difficulty score.\nThe Actor View: Intrinsic Capability.\nTo as-\nsess the model’s actual performance, we evaluate\nit in a standard zero-shot test-taking setting where\nthe correct answer is hidden. The model generates\na solution:\nˆai,m = ψ(Genm(xi, p))\n(2)\n2\n"}, {"page": 3, "text": "where ψ(·) extracts the final answer. The binary\ncorrectness vi,m ∈{0, 1} is subsequently deter-\nmined by vi,m = I(ˆai,m = a∗\ni ).\n2.2\nEvaluation Framework\nTo analyze the divergence between human and\nAI cognition, we evaluate alignment across two\ndistinct dimensions: Perceived Difficulty (what\nmodels predict) and Empirical Difficulty (what\nmodels experience). We adopt Spearman’s Rank\nCorrelation (ρ) as the unified metric for both di-\nmensions, as it robustly measures monotonicity, the\nability to correctly distinguish that item A is harder\nthan item B, while remaining invariant to system-\natic scaling shifts. Moreover, Spearman correlation\nallows unified comparison across heterogeneous\nlabel granularities, discrete or continuous.\nPerception Alignment (ρpred).\nThis metric eval-\nuates the model’s accuracy as an observer. We\ncalculate the Spearman correlation between the\nmodel’s predicted difficulty scores ˆyi,m and the\nhuman ground truth yi.\nρpred,m = Spearman({ˆyi,m}N\ni=1, {yi}N\ni=1)\n(3)\nA higher ρpred,m indicates that the model’s explicit\nperception of difficulty aligns with the human hi-\nerarchy. For simplicity, we omit the subscript m\nwhen not referring to a specific model.\nCapability Alignment (ρirt).\nThis metric evalu-\nates the model’s alignment as an actor. To quantify\nthis, we first derive the Empirical Machine Diffi-\nculty βi using Item Response Theory (IRT). We\ntreat the set of models M as a population of syn-\nthetic examinees and construct a binary correctness\nmatrix. We fit a Rasch Model (1-Parameter Lo-\ngistic Model) where the probability of model m\nanswering item i correctly is:\nP(vi,m = 1 | θm, βi) =\n1\n1 + exp(−(θm −βi))\n(4)\nHere, βi represents the intrinsic machine difficulty\nof item i, estimated via Marginal Maximum Likeli-\nhood Estimation. Crucially, we then calculate the\ncorrelation between this empirical difficulty and\nhuman difficulty:\nρirt = Spearman({βi}N\ni=1, {yi}N\ni=1)\n(5)\nWith ρpred and ρirt, we can obtain a systemic view\nof the Human-AI Difficulty Alignment.\n2.3\nProficiency Simulation\nTo systematically investigate the model’s abil-\nity to simulate different cognitive states and\nalign with student populations, we define a set\nof four distinct proficiency configurations P =\n{p0, plow, pmid, phigh}. These configurations serve\nas system-level instructions that condition the\nmodel’s generation process.\n• Baseline: No Proficiency (p0). This represents\nthe control setting (or vanilla mode). We provide\nthe model with standard instructions to predict\ndifficulty or solve the problem without adopt-\ning a specific student proficiency. This setting\nevaluates the model’s intrinsic alignment, i.e.,\nits default perception of difficulty.\n• Low-Proficiency Student (plow). We prompt\nthe model to simulate a student with limited\nsubject mastery. This proficiency aims to test\nwhether the model can suppress its own knowl-\nedge to accurately estimate high-difficulty items\nfor struggling learners.\n• Average-Proficiency Student (pmid). This pro-\nficiency represents the median student. This\nserves as a proxy for the general population av-\nerage.\n• High-Proficiency Student (phigh). We simulate\na top-tier student who has high proficiency in\nthe subject. This setting investigates whether\nthe model aligns closely with the most capable\nsubset of the human population.\nFor different domains, the proficiency prompts\nare slightly different to align with the domain-\nspecific scenarios, as shown in the Appendix D.\nMoreover, we do not provide much description on\nwhat a low/average/high-proficiency student would\nbe like, since this is also a part of the investigation\non how the model understands them.\n2.4\nExperimental Setup\nDatasets.\nThe key challenge for Human-AI Dif-\nficulty Alignment is to find the dataset that has\nthe ground truth difficulty values that are obtained\nfrom real student field testing, since most of the\nexisting research on IDP is conducted on private\ndatasets or datasets from multiple resources, result-\ning really large discrepancy between the model’s\ncapability and the real students’ performance. To\nensure the robustness of our findings across differ-\nent educational domains, we select four datasets\n3\n"}, {"page": 4, "text": "Model\nUSMLE Cambridge SAT-R SAT-M Average\nGPT-3.5-Turbo\n0.09\n0.20\n0.26\n0.40\n0.24\nGPT-4o\n0.19\n0.48\n0.38\n0.55\n0.40\nGPT-4o-mini\n0.05\n0.42\n0.26\n0.49\n0.31\nGPT-4.1\n0.30\n0.49\n0.49\n0.48\n0.44\nGPT-4.1-mini\n0.20\n0.46\n0.48\n0.45\n0.40\nGPT-o4-mini\n0.28\n0.36\n0.46\n0.29\n0.35\nGPT-5\n0.36\n0.36\n0.38\n0.25\n0.34\nLlama2-7B\n0.05\n0.00\n0.08\n0.03\n0.04\nLlama2-13B\n0.00\n0.10\n0.14\n0.16\n0.10\nPhi3\n0.01\n-0.01\n0.09\n0.30\n0.10\nPhi3.5\n0.06\n0.06\n0.12\n0.32\n0.14\nLlama3.1-8B\n0.04\n0.04\n0.23\n0.43\n0.19\nQwen2.5-7B\n0.10\n0.16\n0.22\n0.49\n0.24\nQwen2.5-32B\n0.09\n0.44\n0.30\n0.56\n0.35\nPhi4\n0.11\n0.40\n0.38\n0.50\n0.35\nQwen3-8B\n0.03\n0.35\n0.20\n0.46\n0.26\nQwen3-32B\n0.11\n0.38\n0.23\n0.52\n0.31\nDeepSeek-R1\n0.28\n0.40\n0.50\n0.44\n0.40\nQWQ-32B\n0.20\n0.41\n0.43\n0.52\n0.39\nR1-Qwen32B\n-0.01\n0.40\n0.25\n0.42\n0.27\nQwen3-32B (R)\n0.21\n0.42\n0.33\n0.48\n0.36\nAverage\n0.13\n0.30\n0.29\n0.41\n0.28\nTable 1: The Spearman’s rank correlation results. Over-\nall alignment remains weak, indicating systematic\nmisalignment.\ncovering reading comprehension, verbal reason-\ning, math reasoning, and specialized professional\nknowledge.\nUSMLE (Medical Knowledge):\nSourced from\nthe United States Medical Licensing Examination\n(Yaneva et al., 2024), this dataset represents a high-\nstakes, knowledge-intensive domain. It contains\n667 items developed by the NBME and FSMB,\nensuring high reliability with field-test data from\nover 300 medical students per item. We utilize the\nprovided continuous difficulty values (transformed\np-values), which range from [0, 1.3].\nCambridge (Linguistic Proficiency):\nSourced\nfrom the Cambridge Multiple-Choice Questions\nReading Dataset (Mullooly et al., 2023), this\ndataset evaluates English reading comprehension\nthrough 120 text passages and 793 distinct ques-\ntions. A key characteristic is the long context win-\ndow required for inference. We utilize the rescaled\nIRT b-parameters provided in the dataset as ground\ntruth, which represent continuous difficulty values\nin the range of [0, 100].\nSAT Reading & Writing (Verbal Reasoning):\nComprises reading comprehension and writing me-\nchanics questions from standardized US college ad-\nmission tests. This dataset challenges the model’s\nability to process rhetorical structure and stan-\ndard English conventions. The dataset contains\nFigure 1: The violin plot of the difficulty prediction\ndistributions of several representative models. Current\nadvanced models exhibit severe distribution shift.\n1338 questions after removing the figure-dependent\nquestions. Unlike the datasets above, the ground\ntruth difficulty is provided as discrete categories:\n{Easy, Medium, Hard}.\nSAT Math (Mathematical Logic):\nIncludes al-\ngebra, geometry, and data analysis problems from\nthe SAT. This dataset tests the model’s ability to\ngauge difficulty in multi-step logical reasoning and\nmathematical computation tasks. Similar to the ver-\nbal component, it contains 1385 questions, and the\ndifficulty values are provided as discrete categories:\n{Easy, Medium, Hard}.\nModels.\nWe evaluate a comprehensive suite of\nover 20 LLMs to disentangle the effects of mod-\nels. We establish a high-performance baseline us-\ning proprietary models, including the GPT series\n(GPT-3.5-Turbo (OpenAI, 2024b), GPT-4o (Hurst\net al., 2024), GPT-4o-mini (OpenAI, 2024a),\nGPT-4.1 (OpenAI, 2025b), GPT-4.1-mini (Ope-\nnAI, 2025b), GPT-o4-mini (OpenAI, 2024c), GPT-\n5 (OpenAI, 2025a)), which represent the current\nstate-of-the-art in general-purpose tasks.\nThe\ngeneral instruction-following open-weights mod-\nels including Llama2-7B (Touvron et al., 2023),\nLlama2-13B (Touvron et al., 2023), Llama3.1-\n8B (Dubey et al., 2024), Qwen2.5-7B (Qwen-\n4\n"}, {"page": 5, "text": "Figure 2: The Spearman correlation trends when greed-\nily ensembling the predictions of the top-K models.\nThe curve indicates the upper bound of the ensemble\nperformance, which is still weak.\nTeam, 2024), Qwen2.5-32B (Qwen-Team, 2024),\nPhi3 (Abdin et al., 2024a), Phi3.5 (Abdin et al.,\n2024a), Phi4 (Abdin et al., 2024b), Qwen3-8B, and\nQwen3-32B (Qwen-Team, 2025a). To specifically\ninvestigate the impact of reasoning capabilities,\nwe incorporate reasoning-focused open-weights\nmodels, including DeepSeek-R1(0528) (DeepSeek-\nAI, 2025), QWQ-32B (Qwen-Team, 2025b), R1-\nDistill-Qwen2.5-32B (DeepSeek-AI, 2025), and\nQwen3-32B (reasoning mode).\n3\nThe Landscape of Explicit Perception\nWe first establish a baseline for zero-shot IDP, ex-\namining whether current state-of-the-art LLMs can\nintrinsically estimate item difficulty without access\nto student response data. The results, summarized\nin Table 1, reveal three primary findings regard-\ning the current limitations of Human-AI difficulty\nalignment.\n3.1\nSystematic Misalignment\nKey Finding 1: Systematic Misalignment.\nWe observe a systematic misalignment be-\ntween human perception and LLM estimation\nof difficulty across all domains. Contrary to\nstandard capability metrics, scaling does not\nreliably translate into alignment: increased\nmodel scale and reasoning power do not linearly\ntranslate to better predictions, as even frontier\nmodels consistently fail to capture human diffi-\nculty rankings. Crucially, this misalignment is\nnot driven by random noise but by a cohesive\nMachine Consensus, where models exhibit rel-\natively stronger alignment with each other\nthan with human reality.\nFigure 3: Heatmap showing the correlation change\nwhen applying specific personas compared to the base-\nline. The impact of individual personas is highly\ninconsistent and noisy.\nDespite their profound problem-solving capa-\nbilities, LLMs struggle with cold-started IDP.\nAs shown in Table 1, Spearman’s ρ averages\nbelow 0.50, with significant domain sensitiv-\nity: alignment is stronger in logic-driven tasks\nlike SAT Math (ρ ≈0.41) but collapses in\nknowledge-intensive domains like USMLE (ρ ≈\n0.13). Counter-intuitively, scaling laws do not\nlinearly translate to difficulty estimation. New-\ngeneration models (e.g., GPT-5, ρ = 0.34) and\nreasoning specialists fail to outperform generalist\nbaselines like GPT-4.1 (ρ = 0.44), indicating that\nreasoning power does not linearly translate to better\ndifficulty alignment with humans.\nFigure 1 identifies the root cause: severe distri-\nbution shift and variance collapse. While ground\ntruth difficulty spans a broad spectrum, model pre-\ndictions are narrowly clustered and systematically\nskewed towards lower values. Advanced models\neffectively overestimate student capability, lack-\ning the granularity to distinguish specific degrees\nof human struggle.\nFinally, we observe a Machine Consensus that\ndiverges from human reality. While alignment with\nground truth is low, inter-model correlations are rel-\natively higher (indicated by the detailed heatmaps\nin Appendix B Figure 4, 5, 6, 7). This consensus is\ncapability-dependent: while weaker models exhibit\n5\n"}, {"page": 6, "text": "Model\nUSMLE Cambridge\nSAT-R\nSAT-M\nAverage\nGPT-3.5-Turbo\n0.14 (0.05)\n0.32 (0.12)\n0.31 (0.06) 0.53 (0.13) 0.33 (0.09)\nGPT-4o\n0.22 (0.04)\n0.50 (0.02)\n0.49 (0.11) 0.65 (0.09) 0.47 (0.07)\nGPT-4o-mini\n0.12 (0.07)\n0.46 (0.04)\n0.35 (0.09) 0.57 (0.08) 0.38 (0.07)\nGPT-4.1\n0.31 (0.01)\n0.51 (0.02)\n0.60 (0.11) 0.63 (0.15) 0.51 (0.07)\nGPT-4.1-mini\n0.23 (0.03)\n0.49 (0.03)\n0.56 (0.08) 0.56 (0.11) 0.46 (0.06)\nGPT-o4-mini\n0.26 (-0.02)\n0.46 (0.10)\n0.56 (0.10) 0.49 (0.20) 0.44 (0.10)\nGPT-5\n0.39 (0.03)\n0.52 (0.16)\n0.57 (0.19) 0.40 (0.15) 0.47 (0.13)\nLlama2-7B\n0.09 (0.02)\n-0.03 (-0.03)\n0.10 (0.03) 0.14 (0.11) 0.08 (0.03)\nLlama2-13B\n0.03 (0.03)\n0.02 (-0.06)\n0.12 (-0.02) 0.19 (0.03) 0.09 (-0.01)\nPhi3\n0.04 (0.04)\n0.08 (0.09)\n0.07 (0.01) 0.01 (-0.04) 0.05 (0.03)\nPhi3.5\n0.02 (-0.03)\n0.17 (0.12)\n0.18 (0.03) 0.35 (0.03) 0.18 (0.04)\nLlama3.1-8B\n0.01 (-0.03)\n0.20 (0.16)\n0.33 (0.10) 0.55 (0.11) 0.27 (0.09)\nQwen2.5-7B\n0.13 (0.03)\n0.34 (0.18)\n0.36 (0.14) 0.63 (0.13) 0.37 (0.12)\nQwen2.5-32B\n0.16 (0.07)\n0.47 (0.04)\n0.41 (0.12) 0.64 (0.08) 0.42 (0.08)\nPhi4\n0.14 (0.03)\n0.45 (0.05)\n0.47 (0.09) 0.56 (0.06) 0.41 (0.06)\nQwen3-8B\n0.06 (0.04)\n0.41 (0.07)\n0.31 (0.11) 0.56 (0.10) 0.34 (0.08)\nQwen3-32B\n0.19 (0.08)\n0.43 (0.05)\n0.39 (0.16) 0.61 (0.09) 0.41 (0.10)\nDeepSeek-R1\n0.26 (-0.01)\n0.42 (0.02)\n0.59 (0.09) 0.62 (0.18) 0.47 (0.07)\nQWQ-32B\n0.23 (0.04)\n0.46 (0.11)\n0.50 (0.08) 0.62 (0.08) 0.45 (0.08)\nR1-Qwen32B\n0.07 (0.08)\n0.46 (0.06)\n0.17 (0.02) 0.35 (0.00) 0.26 (0.04)\nQwen3-32B (R) 0.23 (0.04)\n0.45 (0.06)\n0.37 (0.10) 0.59 (0.08) 0.41 (0.07)\nAverage\n0.16 (0.03)\n0.36 (0.07)\n0.37 (0.09) 0.49 (0.10) 0.35 (0.07)\nTable 2: Spearman correlation results using the ensem-\nble average of all persona configurations. The values in\nred parentheses indicate the absolute improvement over\nthe baseline. While smaller models show negligible\ngains, stronger models like GPT-5 exhibit a signifi-\ncant sensitivity to personas.\nstochastic behavior, advanced models converge\non a shared, non-human machine perception\nof difficulty. This confirms that the Human-AI\nDifficulty Alignment is a stable, systematic mis-\nalignment rather than random error.\n3.2\nEnsemble and Proficiency Simulation\nKey Finding 2: Limits of Simulation.\nNeither extrinsic ensembling nor intrinsic\nrole-playing serves as a reliable solution for\nalignment. We find that ensemble performance\nis strictly bounded by individual model capa-\nbilities: rather than contributing diverse insights,\nweaker models introduce noise that degrades\nthe performance of high-capability baselines.\nSimilarly, proficiency simulation shows highly\ninconsistent results, as models struggle to au-\nthentically mimic different proficiency levels or\nsuppress their intrinsic knowledge: it fails as a\nfaithful cognitive model, but proficiency ensem-\nbling can act as a variance-reduction heuristic.\nWe first perform a greedy ensemble analysis, it-\neratively aggregating the predicted difficulty scores\nof the top-K performing models. As shown in\nFigure 2, the performance trend implies an upper\nbound for alignment capability that is strictly gov-\nerned by the density of high-quality models within\nDomain\nIRT\nCapacity\nCognitive Divergence\nGlobal\nSaturated\nSavant\nBrittle\n(ρ)\n(90% M ✓)\n(H⇑M⇓)\n(H⇓M⇑)\nUSMLE\n0.134\n75.6%\n70.4%\n0.0%\nCambridge\n0.309\n35.6%\n22.1%\n0.4%\nSAT-R\n0.304\n45.5%\n25.5%\n0.0%\nSAT-M\n0.386\n54.6%\n32.2%\n1.3%\nTable 3: Analysis of implicit difficulty alignment based\non Machine IRT. H: Human difficulty; M: Model diffi-\nculty. Saturated represents the total ratio of items that\n90% of the model can solve correctly. Savant represents\nthe ratio of items that are hard for humans (difficulty top\n33%) but trivial for most models (correct for 90% of the\nmodels). Brittleness represents the ratio of items that\nare easy for humans, where most of the models fail. The\nsaturated rate and savant rate present the diagnosis\nof Human-LLM Difficulty Misalignment.\nthe pool. In domains with a dense cluster of ca-\npable models (SAT Math), the ensemble acts as a\ndenoising mechanism, steadily improving correla-\ntion from 0.56 to a peak of 0.66. However, this gain\nis fragile: once the threshold (K = 14) is crossed,\nadding weaker models causes immediate signal\ndilution. In sparse domains like USMLE, this di-\nlution happens immediately, confirming that the\nlong tail of weaker models introduces destructive\nnoise rather than helpful diversity. Thus, extrinsic\naggregation is not a scalable solution but a bounded\nanalysis.\nWe further explore explicit cognitive simulation\nby prompting models to adopt specific student pro-\nficiencies. A granular analysis in Figure 3, the\nchanging ratio of utilizing different proficiencies,\nreveals that single-proficiency simulation is highly\nunstable: adopting a specific role often either im-\nproves or degrades alignment, suggesting models\nstruggle to faithfully simulate a specific cognitive\nstate in isolation.\nHowever, Table 2 reveals another crucial nuance:\nwhile individual simulations are stochastic, the en-\nsemble average of proficiencies consistently im-\nproves alignment. This benefit is particularly ev-\nident in frontier models; for instance, GPT-5 im-\nproves its average correlation to 0.47 compared to\nits baseline of 0.34. This suggests that while mod-\nels cannot reliably act as a specific student, aggre-\ngating their intrinsic variations provides a more ro-\nbust estimate than a single pass, effectively smooth-\ning out the randomness of individual proficiency\nprompts. This improvement does not indicate suc-\ncessful student simulation, but rather reflects noise\naveraging over inconsistent internal states.\n6\n"}, {"page": 7, "text": "USMLE\nCambridge\nSAT Reading&Writing\nSAT Math\nModel\nBaseline\nWeak\nMedium\nStrong\nBaseline\nWeak\nMedium\nStrong\nBaseline\nWeak\nMedium\nStrong\nBaseline\nWeak\nMedium\nStrong\nAverage\nGPT-3.5-Turbo\n0.868\n0.862↓\n0.882\n0.877↑\n0.632\n0.628↓\n0.612\n0.613\n0.660\n0.667\n0.685\n0.687↑\n0.727\n0.750\n0.749\n0.762↑\n0.729\nGPT-4o\n0.952\n0.960\n0.957\n0.955↑\n0.900\n0.893↓\n0.898\n0.897\n0.871\n0.870↓\n0.874\n0.874↑\n0.908\n0.907↓\n0.915\n0.913↑\n0.909\nGPT-4o-mini\n0.925\n0.928\n0.933\n0.934↑\n0.786\n0.765↓\n0.783\n0.793↑\n0.780\n0.786\n0.787\n0.783↑\n0.899\n0.898↓\n0.898\n0.901↑\n0.849\nGPT-4.1\n0.895\n0.906\n0.906\n0.898↑\n0.919\n0.922\n0.922\n0.918\n0.888\n0.915\n0.913\n0.905↑\n0.904\n0.905\n0.896\n0.910↑\n0.908\nGPT-4.1-mini\n0.966\n0.939↓\n0.966\n0.957\n0.897\n0.868↓\n0.895\n0.880\n0.895\n0.892↓\n0.907\n0.904↑\n0.922\n0.920↓\n0.927\n0.925↑\n0.916\nGPT-o4-mini\n0.780\n0.799\n0.810\n0.772\n0.909\n0.895↓\n0.898\n0.904\n0.908\n0.898↓\n0.909\n0.904\n0.906\n0.909\n0.909\n0.902\n0.876\nGPT-5\n0.946\n0.939↓\n0.942\n0.946\n0.958\n0.957↓\n0.956\n0.961↑\n0.976\n0.978\n0.976\n0.977↑\n0.924\n0.914↓\n0.926\n0.923\n0.950\nLlama2-7B\n0.712\n0.718\n0.687\n0.700\n0.397\n0.397\n0.405\n0.409↑\n0.396\n0.380↓\n0.401\n0.392\n0.230\n0.224↓\n0.221\n0.222\n0.431\nLlama2-13B\n0.775\n0.720↓\n0.763\n0.768\n0.459\n0.428↓\n0.455\n0.436\n0.469\n0.477\n0.466\n0.461\n0.267\n0.264↓\n0.262\n0.268↑\n0.484\nPhi3\n0.844\n0.843↓\n0.843\n0.844\n0.493\n0.489↓\n0.489\n0.504↑\n0.632\n0.662\n0.638\n0.649↑\n0.774\n0.777\n0.788\n0.770\n0.690\nPhi3.5\n0.867\n0.853↓\n0.870\n0.865\n0.559\n0.550↓\n0.540\n0.551\n0.647\n0.646↓\n0.645\n0.642\n0.775\n0.787\n0.784\n0.782↑\n0.710\nLlama3.1-8B\n0.891\n0.897\n0.892\n0.880\n0.646\n0.633↓\n0.649\n0.639\n0.685\n0.683↓\n0.674\n0.674\n0.766\n0.751↓\n0.778\n0.771↑\n0.744\nQwen2.5-7B\n0.850\n0.856\n0.847\n0.852↑\n0.675\n0.686\n0.663\n0.690↑\n0.730\n0.719↓\n0.727\n0.736↑\n0.895\n0.900\n0.897\n0.894\n0.789\nQwen2.5-32B\n0.922\n0.931\n0.921\n0.933↑\n0.783\n0.812\n0.812\n0.803↑\n0.812\n0.813\n0.819\n0.816↑\n0.919\n0.917↓\n0.920\n0.921↑\n0.866\nPhi4\n0.921\n0.918↓\n0.928\n0.924↑\n0.798\n0.791↓\n0.778\n0.784\n0.802\n0.802\n0.801\n0.798\n0.919\n0.913↓\n0.908\n0.913\n0.856\nQwen3-8B\n0.903\n0.907\n0.909\n0.901\n0.767\n0.776\n0.765\n0.768↑\n0.820\n0.816↓\n0.811\n0.802\n0.919\n0.926\n0.916\n0.925↑\n0.852\nQwen3-32B\n0.946\n0.957\n0.948\n0.948↑\n0.845\n0.863\n0.849\n0.856↑\n0.881\n0.879↓\n0.868\n0.868\n0.927\n0.932\n0.923\n0.926\n0.901\nDeepSeek-R1\n0.961\n0.966\n0.955\n0.967↑\n0.933\n0.919↓\n0.931\n0.922\n0.964\n0.959↓\n0.968\n0.954\n0.914\n0.919\n0.915\n0.913\n0.941\nQWQ-32B\n0.954\n0.948↓\n0.960\n0.954\n0.898\n0.890↓\n0.899\n0.899↑\n0.921\n0.922\n0.915\n0.920\n0.953\n0.943↓\n0.943\n0.941\n0.929\nR1-Qwen32B\n0.939\n0.936↓\n0.940\n0.940↑\n0.856\n0.827↓\n0.868\n0.849\n0.886\n0.877↓\n0.881\n0.884\n0.944\n0.947\n0.942\n0.941\n0.904\nQwen3-32B (R)\n0.954\n0.963\n0.969\n0.966↑\n0.907\n0.904↓\n0.898\n0.897\n0.932\n0.917↓\n0.926\n0.925\n0.952\n0.943↓\n0.938\n0.938\n0.933\nTable 4: Problem-solving accuracy across different personas. An arrow ↑or ↓is added if the accuracy is improved\nor degraded compared to the baseline with the corresponding persona. The magnitude of change is marginal,\nindicating that models struggle to significantly suppress or enhance their intrinsic capabilities on command.\n4\nCapability vs. Perception\nWhile the previous section examined the model as\nan external observer attempting to predict student\nstruggles, we now turn our focus to the model as\nan internal actor. To understand the root causes\nof the observed misalignment, in this section, we\ndisentangle the relationship between what models\npredict and what they experience.\n4.1\nThe Curse of Knowledge\nKey Finding 3: The Curse of Knowledge\nOur IRT-based analysis reveals that the difficulty\nderived from models’ actual correctness corre-\nlates even worse with human than what they\nexplicitly perceive. This stems from a distinct\nmechanistic divergence: items that are con-\nceptually difficult for humans are frequently\ntrivial for models, resulting in high saturation\nrates on hard items. Furthermore, this capability\nexhibits significant inertia: even when explic-\nitly prompted to simulate a lower-proficiency\nstudent, models fail to meaningfully suppress\ntheir problem-solving capability.\nWe shift our perspective from prediction to ex-\nperience by treating our suite of 21 models as a\ncohort of students to estimate intrinsic machine\ndifficulty (b parameter) via Item Response Theory\n(IRT). In addition, to further quantify and analyze\nthe divergence, we define additional metrics to be\nconditional on human difficulty tiers: the Savant\nRate measures the percentage of items within the\ntop 33% of human difficulty that are solved by over\n90% of models, while the Brittleness Rate mea-\nsures the percentage of items within the bottom\n33% of human difficulty where the model pass rate\nis below 50%.\nTable 3 exposes a profound Cognitive Diver-\ngence, where the correlation of model IRT dif-\nficulty is even lower than the model perception.\nIn domains like USMLE, the misalignment is ex-\ntremely discouraging. The Savant Rate of 70.4%\nimplies that for over two-thirds of the questions,\nhumans find most difficult, can be solved by over\n90% of models easily. Coupled with a massive\ndataset-wide saturation of 75.6%, this creates a flat\ndifficulty that explains the poor alignment.\nIn contrast, reasoning domains like SAT Math\nshow partial alignment but remain limited by hyper-\ncompetence. While the correlation (ρ = 0.386) is\nhigher than in knowledge domains, the Savant Rate\nis still significant at 32.2%. It indicates that even\nin logical reasoning, one-third of the problems that\nchallenge humans are trivial for machines. Fur-\nthermore, with 54.6% of all items being saturated,\nmodels are getting too powerful capabilities to sim-\nulate student struggles.\nFinally, we examine whether simulating profi-\nciencies shifts intrinsic capability. As shown in Ta-\nble 4, while applying a proficiency generally shifts\naccuracy in the expected direction, the magnitude\nis negligible (typically < 1% change). This reveals\nthe Curse of Knowledge. Highly capable models\nare too strong to fail, and their intrinsic objective\nto maximize correctness overrides proficiency in-\nstructions. This prevents them from authentically\nsimulating the specific misconceptions of strug-\ngling students, as they cannot unsee the correct\nanswers they already possess.\n7\n"}, {"page": 8, "text": "Model\nUSMLE Cambridge SAT-R SAT-M Average\nGPT-3.5-Turbo\n0.54\n0.51\n0.49\n0.65\n0.55\nGPT-4o\n0.62\n0.59\n0.54\n0.63\n0.60\nGPT-4o-mini\n0.55\n0.57\n0.51\n0.63\n0.56\nGPT-4.1\n0.55\n0.61\n0.58\n0.57\n0.57\nGPT-4.1-mini\n0.55\n0.56\n0.59\n0.62\n0.58\nGPT-o4-mini\n0.52\n0.59\n0.55\n0.53\n0.55\nGPT-5\n0.60\n0.73\n0.72\n0.60\n0.67\nLlama2-7B\n0.49\n0.50\n0.46\n0.54\n0.50\nLlama2-13B\n0.48\n0.49\n0.50\n0.54\n0.50\nPhi3\n0.47\n0.48\n0.49\n0.62\n0.51\nPhi3.5\n0.50\n0.51\n0.48\n0.58\n0.52\nLlama3.1-8B\n0.53\n0.51\n0.49\n0.63\n0.54\nQwen2.5-7B\n0.53\n0.52\n0.47\n0.57\n0.52\nQwen2.5-32B\n0.55\n0.54\n0.49\n0.62\n0.55\nPhi4\n0.53\n0.54\n0.57\n0.65\n0.57\nQwen3-8B\n0.55\n0.52\n0.49\n0.64\n0.55\nQwen3-32B\n0.62\n0.52\n0.52\n0.61\n0.57\nDeepSeek-R1\n0.64\n0.53\n0.64\n0.62\n0.61\nQWQ-32B\n0.65\n0.58\n0.59\n0.65\n0.62\nR1-Qwen32B\n0.58\n0.54\n0.55\n0.65\n0.58\nQwen3-32B (R)\n0.69\n0.56\n0.54\n0.58\n0.59\nAverage\n0.55\n0.55\n0.54\n0.60\n0.56\nTable 5: AUROC (Area Under the ROC Curve) that\nmeasures the alignment between predicted difficulty\nand the model’s own correctness. A value of 0.5 indi-\ncates random alignment. Most models hover near 0.55,\nrevealing a critical lack of self-awareness: they fail\nto predict their own potential errors.\n4.2\nMetacognitive Blindness.\nKey Finding 4: Metacognitive Blindness.\nOur AUROC analysis reveals a critical lack of\nintrospection: models are unable to accurately\npredict their own potential limitations. With\nAUROC scores hovering near random guessing\n(approximately 0.55) across most models, we\nfind that explicit difficulty estimates are ef-\nfectively decoupled from the model’s actual\ncorrectness. This suggests a fundamental blind\nspot: because models cannot reliably identify\nwhich tasks exceed their own capabilities, they\nlack the necessary internal signal to ground their\nestimates of human difficulty.\nA fundamental question remains regarding\nwhether the model’s perception of difficulty is\naligned with its own problem-solving capability.\nTo quantify this Metacognition, we formulate dif-\nficulty prediction as a binary classification task re-\ngarding the model’s own correctness.\nFor a given item xi, we assign a label li = 1 if\nthe model answers incorrectly (vi = 0) and li = 0\nif it answers correctly (vi = 1), using the predicted\ndifficulty score ˆyi as the prediction probability. We\ncalculate the Area Under the Receiver Operating\nCharacteristic Curve (AUROC) to measure sepa-\nrability, where the value represents the probability\nthat the model assigns a higher difficulty score to\nan item it answers incorrectly compared to one it\nanswers correctly. An AUROC of approximately\n0.5 indicates random alignment where the model\nlacks self-awareness, whereas values significantly\ngreater than 0.5 demonstrate positive metacogni-\ntion where the model correctly identifies its failure.\nTable 5 presents the results of this analysis and\nexposes a critical Metacognitive Blind Spot across\nthe majority of evaluated LLMs. Despite achiev-\ning high problem-solving accuracy, most models\nexhibit AUROC scores ranging between 0.50 and\n0.60. This weak internal alignment implies that\nthey lack the introspection to identify when a task\nexceeds their capabilities. Furthermore, even fron-\ntier models fail to demonstrate robust metacogni-\ntion. While GPT-5 and DeepSeek-R1 show slight\ndeviations from random guessing by achieving lo-\ncalized highs of 0.73 on Cambridge and 0.64 on\nUSMLE, respectively, their overall discrimination\nremains poor, with averages hovering near or be-\nlow 0.67. These values indicate that even the most\nadvanced models still struggle to reliably distin-\nguish between questions they can answer and\nthose they cannot, highlighting that accurate\nself-awareness remains a persistent deficiency\nin current LLMs.\n5\nConclusion\nThis study demonstrates that Large Language Mod-\nels currently struggle to align with human percep-\ntion of difficulty despite their advanced problem-\nsolving capabilities. We find that increasing model\nscale does not guarantee better alignment but rather\nfosters a machine consensus that systematically\ndiverges from student reality. Our investigation\nattributes this failure to a fundamental capability\ngap where models cannot effectively suppress their\nknowledge to simulate struggling students, cou-\npled with a critical lack of metacognitive introspec-\ntion regarding their own limitations. Ultimately,\nthese results highlight that bridging the gap be-\ntween solving a problem and estimating its diffi-\nculty requires more than just stronger models or\nproficiency prompting, calling for new approaches\nto ground machine cognition in human educational\nneeds.\n8\n"}, {"page": 9, "text": "Limitations\nOur investigation into proficiency simulation re-\nlied on zero-shot prompting strategies. While this\nreflects the most common and accessible usage\nof LLMs, it assumes that models can internally\ncalibrate to a proficiency level without examples.\nWe did not explore few-shot prompting with real\nstudent error patterns or fine-tuning on student re-\nsponse logs (Student Trace Modeling). It is possi-\nble that few-shot scenarios can improve the corre-\nlation between the Human-AI alignment. However,\nin this paper, we focus on the intrinsic capability\nof LLMs that are not affected by further learning\nor training processes.\nReferences\nMarah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed\nAwadallah, Ammar Ahmad Awan, Nguyen Bach,\nAmit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat\nBehl, Alon Benhaim, Misha Bilenko, Johan Bjorck,\nSébastien Bubeck, Martin Cai, Qin Cai, Vishrav\nChaudhary, Dong Chen, Dongdong Chen, and 110\nothers. 2024a. Phi-3 technical report: A highly capa-\nble language model locally on your phone. Preprint,\narXiv:2404.14219.\nMarah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien\nBubeck, Ronen Eldan, Suriya Gunasekar, Michael\nHarrison, Russell J Hewett, Mojan Javaheripi, Piero\nKauffmann, and 1 others. 2024b. Phi-4 technical\nreport. arXiv preprint arXiv:2412.08905.\nSamah AlKhuzaey, Floriana Grasso, Terry R Payne, and\nValentina Tamma. 2021. A systematic review of data-\ndriven approaches to item difficulty prediction. In\nInternational conference on artificial intelligence in\neducation, pages 29–41. Springer.\nSamah AlKhuzaey, Floriana Grasso, Terry R Payne,\nand Valentina Tamma. 2024. Text-based question\ndifficulty prediction: A systematic review of auto-\nmatic approaches. International Journal of Artificial\nIntelligence in Education, 34(3):862–914.\nAlfonso Amayuelas, Kyle Wong, Liangming Pan,\nWenhu Chen, and William Wang. 2024. Knowledge\nof\nKnowledge:\nExploring\nKnown-Unknowns\nUncertainty\nwith\nLarge\nLanguage\nModels.\n(arXiv:2305.13712).\nFrank B Baker. 2001. The basics of item response the-\nory. ERIC.\nLuca Benedetto. 2023. A quantitative study of nlp ap-\nproaches to question difficulty estimation. In Inter-\nnational conference on artificial intelligence in edu-\ncation, pages 428–434. Springer.\nFaeze Brahman, Sachin Kumar, Vidhisha Balachan-\ndran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha\nRavichander, Sarah Wiegreffe, Nouha Dziri, Khyathi\nChandu, Jack Hessel, and 1 others. 2024. The art of\nsaying no: Contextual noncompliance in language\nmodels. Advances in Neural Information Processing\nSystems, 37:49706–49748.\nLida Chen, Zujie Liang, Xintao Wang, Jiaqing\nLiang, Yanghua Xiao, Feng Wei, Jinglei Chen,\nZhenghong Hao, Bing Han, and Wei Wang. 2024.\nTeaching Large Language Models to Express\nKnowledge Boundary from Their Own Signals.\n(arXiv:2406.10881).\nKonstantina Chrysafiadi and Maria Virvou. 2013. Stu-\ndent modeling approaches: A literature review for\nthe last decade. Expert Systems with Applications,\n40(11):4715–4729.\nRicardo Conejo, Eduardo Guzmán, Jose-Luis Perez-De-\nLa-Cruz, and Beatriz Barros. 2014. An empirical\nstudy on the quantitative notion of task difficulty.\nExpert Systems with Applications, 41(2):594–606.\nAlbert T Corbett and John R Anderson. 1994. Knowl-\nedge tracing: Modeling the acquisition of procedural\nknowledge. User modeling and user-adapted inter-\naction, 4(4):253–278.\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nsoning capability in llms via reinforcement learning.\nPreprint, arXiv:2501.12948.\nChristine DeMars. 2010. Item response theory. Oxford\nUniversity Press.\nYang Deng, Yong Zhao, Moxin Li, See-Kiong Ng, and\nTat-Seng Chua. 2024. Don‘t Just Say “I don‘t know”!\nSelf-aligning Large Language Models for Respond-\ning to Unknown Questions with Explanations. In Pro-\nceedings of the 2024 Conference on Empirical Meth-\nods in Natural Language Processing, pages 13652–\n13673. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 conference of the\nNorth American chapter of the association for com-\nputational linguistics: human language technologies,\nvolume 1 (long and short papers), pages 4171–4186.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and 1 others. 2024. The llama 3 herd of models.\narXiv e-prints, pages arXiv–2407.\nGeorge Dueñas, Sergio Jimenez, and Geral Mateus\nFerro. 2024. Upn-icc at bea 2024 shared task: Lever-\naging llms for multiple-choice questions difficulty\nprediction. In Proceedings of the 19th Workshop\non Innovative Use of NLP for Building Educational\nApplications (BEA 2024), pages 542–550.\nSusan E Embretson and Steven P Reise. 2013. Item\nresponse theory for psychologists. Psychology Press.\n9\n"}, {"page": 10, "text": "Chenrui Fan, Ming Li, Lichao Sun, and Tianyi Zhou.\n2025. Missing premise exacerbates overthinking:\nAre reasoning models losing critical thinking skill?\nIn Second Conference on Language Modeling.\nWanyong Feng, Peter Tran, Stephen Sireci, and An-\ndrew S Lan. 2025.\nReasoning and sampling-\naugmented mcq difficulty prediction via llms. In\nInternational Conference on Artificial Intelligence in\nEducation, pages 31–45. Springer.\nArthur C Graesser, Patrick Chipman, Brian C Haynes,\nand Andrew Olney. 2005. Autotutor: An intelligent\ntutoring system with mixed-initiative dialogue. IEEE\nTransactions on Education, 48(4):612–618.\nRonald K Hambleton, Hariharan Swaminathan, and\nH Jane Rogers. 1991. Fundamentals of item response\ntheory, volume 2. Sage.\nJun He, Li Peng, Bo Sun, Lejun Yu, and Yinghui Zhang.\n2021. Automatically predict question difficulty for\nreading comprehension exercises. In 2021 ieee 33rd\ninternational conference on tools with artificial intel-\nligence (ictai), pages 1398–1402. IEEE.\nFu-Yuan Hsu, Hahn-Ming Lee, Tao-Hsing Chang, and\nYao-Ting Sung. 2018. Automated estimation of item\ndifficulty for multiple-choice tests: An application of\nword embedding techniques. Information Processing\n& Management, 54(6):969–984.\nZhenya Huang, Qi Liu, Enhong Chen, Hongke Zhao,\nMingyong Gao, Si Wei, Yu Su, and Guoping Hu.\n2017. Question difficulty prediction for reading prob-\nlems in standard tests. In Proceedings of the AAAI\nconference on artificial intelligence, volume 31.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, and 1\nothers. 2024. Gpt-4o system card. arXiv preprint\narXiv:2410.21276.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El-Showk,\nAndy Jones, Nelson Elhage, Tristan Hume, Anna\nChen, Yuntao Bai, Sam Bowman, Stanislav Fort, and\n17 others. 2022. Language Models (Mostly) Know\nWhat They Know. (arXiv:2207.05221).\nSanyam Kapoor, Nate Gruver, Manley Roberts, Kather-\nine Collins, Arka Pal, Umang Bhatt, Adrian Weller,\nSamuel Dooley, Micah Goldblum, and Andrew Gor-\ndon Wilson. 2024. Large Language Models Must\nBe Taught to Know What They Don’t Know.\n(arXiv:2406.08391).\nTanja Käser and Giora Alexandron. 2024. Simulated\nlearners in educational technology: A systematic liter-\nature review and a turing-like test. International Jour-\nnal of Artificial Intelligence in Education, 34(2):545–\n585.\nJohn P Lalor, Pedro Rodriguez, João Sedoc, and Jose\nHernandez-Orallo. 2024. Item response theory for\nnatural language processing. In Proceedings of the\n18th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Tutorial\nAbstracts, pages 9–13.\nUnggi Lee, Sanghyeok Lee, Junbo Koh, Yeil Jeong,\nHaewon Jung, Gyuri Byun, Yunseo Lee, Jewoong\nMoon, Jieun Lim, and Hyeoncheol Kim. 2023. Gen-\nerative agent for teacher training: Designing educa-\ntional problem-solving simulations with large lan-\nguage model-based agents for pre-service teachers.\nIn NeurIPS’23 Workshop on Generative AI for Edu-\ncation (GAIED).\nBelinda Z. Li, Been Kim, and Zi Wang. 2025a. Quest-\nBench: Can LLMs ask the right question to acquire\ninformation in reasoning tasks? (arXiv:2503.22674).\nMing Li, Hong Jiao, Tianyi Zhou, Nan Zhang, Sydney\nPeters, and Robert W Lissitz. 2025b. Item difficulty\nmodeling using fine-tuned small and large language\nmodels. Educational and Psychological Measure-\nment, page 00131644251344973.\nMing Li, Nan Zhang, Chenrui Fan, Hong Jiao, Yanbin\nFu, Sydney Peters, Qingshu Xu, Robert Lissitz, and\nTianyi Zhou. 2025c.\nUnderstanding the thinking\nprocess of reasoning models: A perspective from\nschoenfeld’s episode theory. In Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing, pages 18278–18299, Suzhou,\nChina. Association for Computational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTeaching Models to Express Their Uncertainty in\nWords. (arXiv:2205.14334).\nYunting Liu, Shreya Bhandari, and Zachary A Pardos.\n2025. Leveraging llm respondents for item evalua-\ntion: A psychometric analysis. British Journal of\nEducational Technology, 56(3):1028–1052.\nFrederic M Lord. 2012. Applications of item response\ntheory to practical testing problems. Routledge.\nAnastassia Loukina, Su-Youn Yoon, Jennifer Sakano,\nYouhua Wei, and Kathy Sheehan. 2016.\nTextual\ncomplexity as a predictor of difficulty of listening\nitems in language proficiency tests. In Proceedings of\nCOLING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, pages\n3245–3253.\nNishanth Madhusudhan, Sathwik Tejaswi Madhusud-\nhan, Vikas Yadav, and Masoud Hashemi. 2025. Do\nLLMs Know When to NOT Answer? Investigating\nAbstention Abilities of Large Language Models. In\nProceedings of the 31st International Conference on\nComputational Linguistics, pages 9329–9345. Asso-\nciation for Computational Linguistics.\nJulia M Markel, Steven G Opferman, James A Lan-\nday, and Chris Piech. 2023. Gpteach: Interactive ta\ntraining with gpt-based students. In Proceedings of\n10\n"}, {"page": 11, "text": "the tenth acm conference on learning@ scale, pages\n226–236.\nMantas Mazeika, Long Phan, Xuwang Yin, Andy Zou,\nZifan Wang, Norman Mu, Elham Sakhaee, Nathaniel\nLi, Steven Basart, Bo Li, David Forsyth, and Dan\nHendrycks. 2024. HarmBench: A Standardized Eval-\nuation Framework for Automated Red Teaming and\nRobust Refusal. (arXiv:2402.04249).\nArya D McCarthy, Kevin P Yancey, Geoffrey T LaFlair,\nJesse Egbert, Manqian Liao, and Burr Settles. 2021.\nJump-starting item parameters for adaptive language\ntests.\nIn Proceedings of the 2021 conference on\nempirical methods in natural language processing,\npages 883–899.\nMihran Miroyan, Rose Niousha, Joseph E Gonzalez,\nGireeja Ranade, and Narges Norouzi. 2025. Paras-\ntudent: Generating and evaluating realistic student\ncode by teaching llms to struggle. arXiv preprint\narXiv:2507.12674.\nFelix B. Mueller, Rebekka Görge, Anna K. Bernzen,\nJanna C. Pirk, and Maximilian Poretschkin. 2024.\nLlms and memorization: On quality and specificity of\ncopyright compliance. In Proceedings of the Seventh\nAAAI/ACM Conference on AI, Ethics, and Society\n(AIES-24) - Full Archival Papers, October 21-23,\n2024, San Jose, California, USA - Volume 1, pages\n984–996. AAAI Press.\nAndrew Mullooly, Øistein Andersen, Luca Benedetto,\nPaula Buttery, Andrew Caines, Mark J. F. Gales,\nYasin Karatay, Kate Knill, Adian Liusie, Vatsal\nRaina, and Shiva Taslimipoor. 2023. The Cambridge\nMultiple-Choice Questions Reading Dataset.\nShadi Noroozi and Hossein Karami. 2022. A scrutiny of\nthe relationship between cognitive load and difficulty\nestimates of language test items. Language Testing\nin Asia, 12(1):13.\nOpenAI. 2024a. Gpt-4o mini: advancing cost-efficient\nintelligence.\nhttps://openai.com/index/\ngpt-4o-mini-advancing-cost-efficient-intelligence/\n?utm_source=chatgpt.com.\nOpenAI. 2024b.\nIntroducing apis for gpt-3.5\nturbo and whisper. https://openai.com/index/\nintroducing-chatgpt-and-whisper-apis/.\nOpenAI.\n2024c.\nIntroducing\nopenai\no3\nand\no4-mini.\nhttps://openai.com/index/\nintroducing-o3-and-o4-mini/.\nOpenAI. 2025a. Gpt-5 system card. https://openai.\ncom/index/gpt-5-system-card/.\nOpenAI. 2025b. Introducing gpt-4.1 in the api. https:\n//openai.com/index/gpt-4-1/.\nJoon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bern-\nstein. 2023. Generative agents: Interactive simulacra\nof human behavior. In Proceedings of the 36th an-\nnual acm symposium on user interface software and\ntechnology, pages 1–22.\nKyle Perkins, Lalit Gupta, and Ravi Tammana. 1995.\nPredicting item difficulty in a reading comprehen-\nsion test with an artificial neural network. Language\ntesting, 12(1):34–53.\nSydney Peters, Nan Zhang, Hong Jiao, Ming Li, Tianyi\nZhou, and Robert Lissitz. 2025.\nText-based ap-\nproaches to item difficulty modeling in large-scale\nassessments: A systematic review. arXiv preprint\narXiv:2509.23486.\nQwen-Team. 2024. Qwen2.5: A party of foundation\nmodels.\nQwen-Team. 2025a. Qwen3 technical report. Preprint,\narXiv:2505.09388.\nQwen-Team. 2025b. Qwq-32b: Embracing the power\nof reinforcement learning.\nGeorg Rasch. 1993. Probabilistic models for some in-\ntelligence and attainment tests. ERIC.\nAna-Cristina Rogoz and Radu Tudor Ionescu. 2024.\nUnibucllm: Harnessing llms for automated predic-\ntion of item difficulty and response time for multiple-\nchoice questions. arXiv preprint arXiv:2404.13343.\nAlexis Ross, Megha Srivastava, Jeremiah Blanchard,\nand Jacob Andreas. 2025. Modeling student learn-\ning with 3.8 million program traces. arXiv preprint\narXiv:2510.05056.\nMakoto Sano. 2015. Automated capturing of psycho-\nlinguistic features in reading assessment text.\nIn\nannual meeting of the National Council on Measure-\nment in Education, Chicago, IL.\nAviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Da-\ngan, and Shauli Ravfogel. 2023. The Curious Case of\nHallucinatory (Un)answerability: Finding Truths in\nthe Hidden States of Over-Confident Large Language\nModels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3607–3625. Association for Computational\nLinguistics.\nKatherine Stasaski, Grace Hui Yang, and Marti A Hearst.\n2020. More diverse dialogue datasets via diversity-\ninformed data collection. In Proceedings of the 58th\nannual meeting of the association for computational\nlinguistics, pages 4958–4968.\nJohn Sweller. 1988. Cognitive load during problem\nsolving: Effects on learning.\nCognitive science,\n12(2):257–285.\nJohn Sweller. 2011. Cognitive load theory. In Psychol-\nogy of learning and motivation, volume 55, pages\n37–76. Elsevier.\nAnaïs Tack, Siem Buseyne, Changsheng Chen, Robbe\nD’hondt, Michiel De Vrindt, Alireza Gharahighehi,\nSameh Metwaly, Felipe Kenji Nakano, and Ann-\nSophie Noreillie. 2024. Itec at bea 2024 shared task:\nPredicting difficulty and response time of medical\n11\n"}, {"page": 12, "text": "exam questions with statistical, machine learning,\nand language models. In Proceedings of the 19th\nWorkshop on Innovative Use of NLP for Building Ed-\nucational Applications (BEA 2024), pages 512–521.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit\nSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,\nand Christopher D. Manning. 2023. Just Ask for\nCalibration: Strategies for Eliciting Calibrated Con-\nfidence Scores from Language Models Fine-Tuned\nwith Human Feedback. (arXiv:2305.14975).\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, and 1 others. 2023. Llama 2: Open foun-\ndation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nYu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Wei-\nLin Chen, Chao-Wei Huang, Yu Meng, and Yun-\nNung Chen. 2024. Two tales of persona in llms: A\nsurvey of role-playing and personalization. arXiv\npreprint arXiv:2406.01171.\nKurt VanLehn, Stellan Ohlsson, and Rod Nason. 1994.\nApplications of simulated students: An exploration.\nJournal of artificial intelligence in education, 5:135–\n135.\nRoman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev,\nLyudmila Rvanova, Daniil Vasilev, Akim Tsvigun,\nSergey Petrakov, Rui Xing, Abdelrahman Sadallah,\nKirill Grishchenkov, Alexander Panchenko, Timothy\nBaldwin, Preslav Nakov, Maxim Panov, and Artem\nShelmanov. 2024. Benchmarking Uncertainty Quan-\ntification Methods for Large Language Models with\nLM-Polygraph. Transactions of the Association for\nComputational Linguistics, 13:220–248.\nTobias Weber, Michael Ingrisch, Bernd Bischl, and\nDavid Rügamer. 2024.\nConstrained probabilistic\nmask learning for task-specific undersampled mri\nreconstruction. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision,\npages 7665–7674.\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie\nFu, Junxian He, and Bryan Hooi. 2024.\nCan\nLLMs Express Their Uncertainty?\nAn Empiri-\ncal Evaluation of Confidence Elicitation in LLMs.\n(arXiv:2306.13063).\nVictoria Yaneva, Kai North, Peter Baldwin, Le An Ha,\nSaed Rezayi, Yiyun Zhou, Sagnik Ray Choudhury,\nPolina Harik, and Brian Clauser. 2024. Findings\nfrom the first shared task on automated prediction\nof difficulty and response time for multiple-choice\nquestions. In Proceedings of the 19th Workshop on\nInnovative Use of NLP for Building Educational Ap-\nplications (BEA 2024), pages 470–482.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,\nXipeng Qiu, and Xuanjing Huang. 2023. Do Large\nLanguage Models Know What They Don‘t Know?\nIn Findings of the Association for Computational Lin-\nguistics: ACL 2023, pages 8653–8665. Association\nfor Computational Linguistics.\nKe Zhang and Ayse Begum Aslan. 2021. Ai technolo-\ngies for education: Recent research & future direc-\ntions. Computers and education: Artificial intelli-\ngence, 2:100025.\nTong Zhang, Peixin Qin, Yang Deng, Chen Huang,\nWenqiang Lei, Junhong Liu, Dingnan Jin, Hongru\nLiang, and Tat-Seng Chua. 2024a. CLAMBER: A\nBenchmark of Identifying and Clarifying Ambigu-\nous Information Needs in Large Language Models.\nIn Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 10746–10766. Association\nfor Computational Linguistics.\nZhehao Zhang, Ryan A Rossi, Branislav Kveton, Yijia\nShao, Diyi Yang, Hamed Zamani, Franck Dernon-\ncourt, Joe Barrow, Tong Yu, Sungchul Kim, and 1\nothers. 2024b.\nPersonalization of large language\nmodels: A survey. arXiv preprint arXiv:2411.00027.\nLeonidas Zotos, Hedderik van Rijn, and Malvina Nis-\nsim. 2024. Are you doubtful? oh, it might be dif-\nficult then! exploring the use of model uncertainty\nfor question difficulty estimation.\narXiv preprint\narXiv:2412.11831.\n12\n"}, {"page": 13, "text": "Table of Contents for Appendix\nA Related Work\n14\nA.1\nItem Difficulty Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nA.2\nStudent Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nA.3\nLLM Self-Awareness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nB\nConsensus of Machines\n16\nC Case Study\n20\nD Experimental Prompts\n21\n13\n"}, {"page": 14, "text": "A\nRelated Work\nA.1\nItem Difficulty Prediction\nConventional item difficulty prediction in large-scale assessments typically depends on item response\ndata collected through field testing, where newly created items are embedded in operational forms\n(but not scored) and then analyzed within classical test theory (CTT) or item response theory (IRT)\nframeworks (Hsu et al., 2018; Benedetto, 2023). Within these frameworks, CTT operationalizes difficulty\nas the proportion correct (p-value), whereas IRT models the probability of a correct response as a function\nof latent ability and item parameters via statistical models such as logit/probit links (DeMars, 2010;\nHsu et al., 2018). Despite their accuracy, field-testing and calibration are frequently criticized as time-\nconsuming and costly, because data collection can take several months and IRT calibration may require\nadministering items to several thousand examinees (Hambleton et al., 1991; Hsu et al., 2018; AlKhuzaey\net al., 2024). Embedding non-scored pretest items into operational tests also lengthens administrations\nand raises concerns about test-taker engagement and item exposure, which can compromise test security\nin high-stakes contexts (Loukina et al., 2016; Hsu et al., 2018; Benedetto, 2023). Expert-based difficulty\nratings have been proposed as an alternative, but they are seldom used at scale due to subjectivity and\nweak alignment with psychometric difficulty, with reported evidence suggesting weak alignment between\nexpert judgments and IRT-based difficulty estimates (Conejo et al., 2014). In response, text-based item\ndifficulty modeling predicts difficulty directly from item text and related metadata using machine learning,\nthereby avoiding response-data collection and potentially reducing time, cost, and reliance on subjective\nratings (Sano, 2015; Loukina et al., 2016; Huang et al., 2017; Hsu et al., 2018). Early text-based work was\ndominated by feature engineering grounded in linguistic, cognitive, or psychometric theory, exemplified\nby combining linguistic indicators and item-level metadata, but such approaches can require substantial\nmanual extraction and may generalize poorly across domains (Perkins et al., 1995; Loukina et al., 2016).\nThe learned representations (Devlin et al., 2019) are typically combined with regressors/classifiers such\nas CNNs or LSTMs for prediction, with empirical results showing that a CNN yielded superior perfor-\nmance to both a regular CNN variant and a TF-IDF+SVM baseline on TOEFL reading comprehension\nitems (He et al., 2021). More recently, transformer-based small language models have been fine-tuned\nend-to-end for item difficulty prediction, with BERT first explored in large-scale assessment settings in\n2021 and subsequent work showing that fine-tuned BERT and DistilBERT can outperform traditional\nlinguistic/readability features and TF-IDF or Word2Vec-based approaches, while DistilBERT can match\nBERT performance at lower cost (McCarthy et al., 2021; Benedetto, 2023). In parallel, large language\nmodels have been used to directly predict difficulty or to generate auxiliary inputs and features, such as\nrationales, predicted answers, reasoning steps, uncertainty proxies, or simulated test-taker behaviors, that\nare then fed into downstream models for difficulty estimation (Rogoz and Ionescu, 2024; Li et al., 2025b;\nFeng et al., 2025; Zotos et al., 2024; Dueñas et al., 2024). Across this section, the choice of modeling\nparadigm reflects a recurring trade-off between predictive power and interpretability, and some studies\nreport that hand-crafted linguistic and metadata features can contribute more than BERT embeddings in\nspecific settings (Tack et al., 2024).\nA.2\nStudent Simulation\nStudent simulation refers to the use of artificial agents to generate learner-like behavior or data (e.g.,\nbehavioral traits, performance patterns, and learning progression), and is conceptually distinguished\nfrom student modeling systems that primarily infer latent states for adaptation (Chrysafiadi and Virvou,\n2013). Early work emphasized three enduring motivations for student simulation: enabling teachers\nto “practice the art of tutoring,” supporting learning-by-teaching with a simulated peer, and allowing\nformative evaluation of instructional materials without relying entirely on human learner data (VanLehn\net al., 1994). Before the LLM era, building such simulations often required extensive hand-crafting\nof dialogue moves and misconception models in rule-based tutoring systems (Graesser et al., 2005),\nor large-scale human role-play data collection for tutoring dialogues (Stasaski et al., 2020). With the\nadvent of large language models, student simulation has been reframed as more feasible and scalable,\ndriven by capabilities such as reproducing population-level behavioral distributions (Weber et al., 2024),\n14\n"}, {"page": 15, "text": "expressing reasoning/misunderstanding in natural language (Zhang and Aslan, 2021), and supporting\nmore agent-like behavior via memory and planning (Park et al., 2023). Simulated students have been used\nacross major contexts including data generation, teacher training, learning by teaching/collaboration, and\ncontent evaluation, illustrating their broad educational utility (Käser and Alexandron, 2024).\nLLM-based student simulation methods vary with the simulation goal, including modeling student traits,\nperformance patterns, or learning progression, and are commonly instantiated via prompting, fine-tuning\non learner traces, and agentic designs that support memory and planning (Corbett and Anderson, 1994;\nRasch, 1993; Lord, 2012; Embretson and Reise, 2013). Prompt-based role conditioning remains a common\nbaseline, where the simulator is instructed to “act as” a student with specified demographic/background/af-\nfective attributes (Markel et al., 2023; Lee et al., 2023), though direct prompting can struggle to reliably\ninstantiate the intended persona characteristics. To better capture authentic learning dynamics (e.g.,\niterative struggle and revision), some approaches fine-tune LLMs on real student submission trajectories\nand temporally ordered traces (Miroyan et al., 2025; Ross et al., 2025). In parallel, agentic designs\nextend simulators beyond static role-play by endowing them with memory/reflection/decision-making\ncomponents, building on the broader notion of LLM-based generative agents (Park et al., 2023). Cognitive\ngrounding can be strengthened by integrating knowledge tracing or IRT as internal state mechanisms to\ncontrol and drive simulated behavior over time (e.g., evolving mastery/ability and its interaction with\ntask difficulty), rather than only relying on surface-level role-play (Corbett and Anderson, 1994; Rasch,\n1993; Embretson and Reise, 2013). Despite these advances, the prior studies explicitly caution that fluent\nlanguage output does not guarantee behavioral fidelity and note that many simulated-learner studies lack\nempirical realism validation, motivating more standardized and context-aware evaluation practices (Käser\nand Alexandron, 2024). Liu et al. (2025) propose leveraging LLMs as synthetic examinees to estimate\nitem difficulty through IRT calibration, demonstrating that model-generated responses can approximate\nhuman-derived item parameters under certain conditions. In contrast, our work does not treat alignment as\na purely psychometric estimation problem, but instead interrogates the cognitive validity of such alignment\nby disentangling difficulty perception, intrinsic capability, and metacognitive awareness.\nA.3\nLLM Self-Awareness\nNumerous benchmarks study model self-knowledge through abstention, but they typically instantiate\nabstention under a single failure mode, such as unanswerable questions (Yin et al., 2023; Amayuelas et al.,\n2024), multiple-choice questions with no correct option (Madhusudhan et al., 2025), or underspecified\ninputs (Slobodkin et al., 2023; Zhang et al., 2024a; Li et al., 2025a). Closely related, verbalized uncertainty\nelicits a model’s explicit expression of doubt and uses it as a downstream signal of whether the model can\nanswer appropriately (Lin et al., 2022; Tian et al., 2023). However, several studies report that verbalized\nuncertainty can be brittle and may generalize poorly as a practical uncertainty-quantification mechanism\n(Vashurin et al., 2024; Lin et al., 2022; Xiong et al., 2024). At the same time, other work shows that these\nsignals can be improved: Kapoor et al. (2024) demonstrates that fine-tuning can strengthen verbalized\nuncertainty, while Kadavath et al. (2022) shows that suitable prompting can elicit explicit correctness\nprobabilities that become increasingly calibrated as model scale increases. Beyond uncertainty elicitation,\nabstention behavior itself has been improved via fine-tuning (Chen et al., 2024; Brahman et al., 2024) and\nvia explanation generation to justify refusals (Deng et al., 2024), and related benchmarks also evaluate\npolicy compliance and safety-related refusal behavior (Brahman et al., 2024; Mueller et al., 2024; Mazeika\net al., 2024). With the recent surge of interest in large reasoning models, MiP-Overthinking further reports\nthat longer “thinking” traces may not improve abstention on unsolvable questions and can even exacerbate\noverconfident answering (Fan et al., 2025). Complementing these lines of work, we study self-knowledge\nin the context of educational difficulty estimation: rather than only asking whether a model can abstain,\nwe test whether its explicit difficulty judgments provide an introspective signal about its own likelihood of\nerror on the same items. Our results indicate a pronounced metacognitive gap, difficulty estimates are\nonly weakly predictive of failure cases, highlighting that off-the-shelf LLMs may lack the self-awareness\nneeded to ground difficulty prediction in genuine student struggles.\n15\n"}, {"page": 16, "text": "B\nConsensus of Machines\nFigure 4, 5, 6, 7 show the consensus heatmaps of the spearman correlation between the models on the\nUSMLE, CMCQRD, SAT Math, and SAT Reading datasets. The heatmaps show that the correlation\nbetween the models is relatively higher than the correlation between the models and the human.\nFigure 4: The consensus heatmap of the spearman correlation between the models on the USMLE dataset.\n16\n"}, {"page": 17, "text": "Figure 5: The consensus heatmap of the spearman correlation between the models on the CMCQRD dataset.\n17\n"}, {"page": 18, "text": "Figure 6: The consensus heatmap of the spearman correlation between the models on the SAT Math dataset.\n18\n"}, {"page": 19, "text": "Figure 7: The consensus heatmap of the spearman correlation between the models on the SAT Reading dataset.\n19\n"}, {"page": 20, "text": "C\nCase Study\nFigure 8 shows the violin plot of the difficulty prediction results of GPT-5 with different personas. The\nviolin plot shows that the Low-Proficiency persona effectively expands the distribution, more closely\nresembling the Ground Truth. However, a systematic underestimation persists, indicating that the model\nremains optimistic about student performance compared to reality.\nFigure 8: The violin plot of the difficulty prediction results of GPT-5 with different personas. The Low-Proficiency\nsimulation effectively expands the distribution, more closely resembling the Ground Truth. However, a\nsystematic underestimation persists, indicating that the model remains optimistic about student performance\ncompared to reality.\n20\n"}, {"page": 21, "text": "D\nExperimental Prompts\nFigure 9, 10, 11 show the prompt templates for difficulty prediction, including Low-Proficiency Student,\nMedium-Proficiency Student, and High-Proficiency Student across four tasks. Figure 12, 13, 14 show\nthe prompt templates for question answering, including Low-Proficiency Student, Medium-Proficiency\nStudent, and High-Proficiency Student across four tasks.\n21\n"}, {"page": 22, "text": "Prompt for Difficulty Prediction: Low-Proficiency Student\n---------------------------------- For USMLE ---------------------------------\n(System Prompt)\nSuppose you are a student taking the USMLE exam. You are a weak student with low-level\nmedical proficiency.\n(User Prompt)\nAnalyze the difficulty values of the question. The difficulty values range from 0 to\n1.0, where 0 is the easiest and 1.0 is the hardest. Analyze the difficulty step by step,\nand provide the final value in \\boxed{...}:\n[Item Context]\n-------------------------------- For Cambridge -------------------------------\n(System Prompt)\nSuppose you are a student taking the Cambridge English Test. You are a weak student\nwith low-level English proficiency.\n(User Prompt)\nAnalyze the difficulty values of the question. The difficulty values range from 1 to\n100, where 1 is the easiest and 100 is the hardest. Analyze the difficulty step by step,\nand provide the final value in \\boxed{...}:\n[Item Context]\n------------------------------ For SAT Reading -------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT Reading exam. You are a weak student with low\n-level English proficiency.\n(User Prompt)\nAnalyze the difficulty levels of the question. The difficulty levels contain 3\ncategories: easy, medium, and hard. Analyze the difficulty step by step, and provide the\nfinal category in \\boxed{...}:\n[Item Context]\n-------------------------------- For SAT Math --------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT math exam. You are a weak student with low-\nlevel math proficiency.\n(User Prompt)\nAnalyze the difficulty levels of the question. The difficulty levels contain 3\ncategories: easy, medium, and hard. Analyze the difficulty step by step, and provide the\nfinal category in \\boxed{...}:\n[Item Context]\nFigure 9: Prompt templates for difficulty prediction (Low-Proficiency Student) across four tasks.\n22\n"}, {"page": 23, "text": "Prompt for Difficulty Prediction: Medium-Proficiency Student\n---------------------------------- For USMLE ---------------------------------\n(System Prompt)\nSuppose you are a student taking the USMLE exam. You are an average student with\nmedium-level medical proficiency.\n(User Prompt)\nAnalyze the difficulty values of the question. The difficulty values range from 0 to\n1.0, where 0 is the easiest and 1.0 is the hardest. Analyze the difficulty step by step,\nand provide the final value in \\boxed{...}:\n[Item Context]\n-------------------------------- For Cambridge -------------------------------\n(System Prompt)\nSuppose you are a student taking the Cambridge English Test. You are an average\nstudent with medium-level English proficiency.\n(User Prompt)\nAnalyze the difficulty values of the question. The difficulty values range from 1 to\n100, where 1 is the easiest and 100 is the hardest. Analyze the difficulty step by step,\nand provide the final value in \\boxed{...}:\n[Item Context]\n------------------------------ For SAT Reading -------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT Reading exam. You are an average student with\nmedium-level English proficiency.\n(User Prompt)\nAnalyze the difficulty levels of the question. The difficulty levels contain 3\ncategories: easy, medium, and hard. Analyze the difficulty step by step, and provide the\nfinal category in \\boxed{...}:\n[Item Context]\n-------------------------------- For SAT Math --------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT math exam. You are an average student with\nmedium-level math proficiency.\n(User Prompt)\nAnalyze the difficulty levels of the question. The difficulty levels contain 3\ncategories: easy, medium, and hard. Analyze the difficulty step by step, and provide the\nfinal category in \\boxed{...}:\n[Item Context]\nFigure 10: Prompt templates for difficulty prediction (Medium-Proficiency Student) across four tasks.\n23\n"}, {"page": 24, "text": "Prompt for Difficulty Prediction: High-Proficiency Student\n---------------------------------- For USMLE ---------------------------------\n(System Prompt)\nSuppose you are a student taking the USMLE exam. You are a good student with high-\nlevel medical proficiency.\n(User Prompt)\nAnalyze the difficulty values of the question. The difficulty values range from 0\nto 1.0, where 0 is the easiest and 1.0 is the hardest. Analyze the difficulty step by\nstep, and provide the final value in \\boxed{...}:\n[Item Context]\n-------------------------------- For Cambridge -------------------------------\n(System Prompt)\nSuppose you are a student taking the Cambridge English Test. You are a good\nstudent with high-level English proficiency.\n(User Prompt)\nAnalyze the difficulty values of the question. The difficulty values range from 1\nto 100, where 1 is the easiest and 100 is the hardest. Analyze the difficulty step by\nstep, and provide the final value in \\boxed{...}:\n[Item Context]\n------------------------------ For SAT Reading -------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT Reading exam. You are a good student with\nhigh-level English proficiency.\n(User Prompt)\nAnalyze the difficulty levels of the question. The difficulty levels contain 3\ncategories: easy, medium, and hard. Analyze the difficulty step by step, and provide the\nfinal category in \\boxed{...}:\n[Item Context]\n-------------------------------- For SAT Math --------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT math exam. You are a good student with\nhigh-level math proficiency.\n(User Prompt)\nAnalyze the difficulty levels of the question. The difficulty levels contain 3\ncategories: easy, medium, and hard. Analyze the difficulty step by step, and provide the\nfinal category in \\boxed{...}:\n[Item Context]\nFigure 11: Prompt templates for difficulty prediction (High-Proficiency Student) across four tasks.\n24\n"}, {"page": 25, "text": "Prompt for Question Answering: Low-Proficiency Student\n---------------------------------- For USMLE ---------------------------------\n(System Prompt)\nSuppose you are a student taking the USMLE exam. You are a weak student with low-level\nmedical proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\n-------------------------------- For Cambridge -------------------------------\n(System Prompt)\nSuppose you are a student taking the Cambridge English Test. You are a weak student\nwith low-level English proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\n------------------------------ For SAT Reading -------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT Reading exam. You are a weak student with low\n-level English proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\n-------------------------------- For SAT Math --------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT math exam. You are a weak student with low-\nlevel math proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\nFigure 12: Prompt templates for question answering (Low-Proficiency Student) across four tasks.\n25\n"}, {"page": 26, "text": "Prompt for Question Answering: Medium-Proficiency Student\n---------------------------------- For USMLE ---------------------------------\n(System Prompt)\nSuppose you are a student taking the USMLE exam. You are an average student with medium-\nlevel medical proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\n-------------------------------- For Cambridge -------------------------------\n(System Prompt)\nSuppose you are a student taking the Cambridge English Test. You are an average student\nwith medium-level English proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\n-------------------------------- For SAT Reading --------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT Reading exam. You are an average student with\nmedium-level English proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\n-------------------------------- For SAT Math --------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT math exam. You are an average student with medium\n-level math proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\nFigure 13: Prompt templates for question answering (Medium-Proficiency Student) across four tasks.\n26\n"}, {"page": 27, "text": "Prompt for Question Answering: High-Proficiency Student\n---------------------------------- For USMLE ---------------------------------\n(System Prompt)\nSuppose you are a student taking the USMLE exam. You are a good student with high-\nlevel medical proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\n-------------------------------- For Cambridge -------------------------------\n(System Prompt)\nSuppose you are a student taking the Cambridge English Test. You are a good student\nwith high-level English proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\n------------------------------ For SAT Reading -------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT Reading exam. You are a good student with\nhigh-level English proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\n-------------------------------- For SAT Math --------------------------------\n(System Prompt)\nSuppose you are a student taking the SAT math exam. You are a good student with high-\nlevel math proficiency.\n(User Prompt)\nAnswer the question below step by step, and provide the final answer in \\boxed{...}:\n[Item Context]\nFigure 14: Prompt templates for question answering (High-Proficiency Student) across four tasks.\n27\n"}]}