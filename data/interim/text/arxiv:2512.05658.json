{"doc_id": "arxiv:2512.05658", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.05658.pdf", "meta": {"doc_id": "arxiv:2512.05658", "source": "arxiv", "arxiv_id": "2512.05658", "title": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models", "authors": ["Pietro Ferrazzi", "Aitor Soroa", "Rodrigo Agerri"], "published": "2025-12-05T12:05:46Z", "updated": "2025-12-05T12:05:46Z", "summary": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.05658v1", "url_pdf": "https://arxiv.org/pdf/2512.05658.pdf", "meta_path": "data/raw/arxiv/meta/2512.05658.json", "sha256": "ca8f6bba5c5d7955ea50a5788ee9821c005b88599ce528f4a6fab8fca2958ee9", "status": "ok", "fetched_at": "2026-02-18T02:25:14.344835+00:00"}, "pages": [{"page": 1, "text": "Grounded Multilingual Medical Reasoning for Question Answering\nwith Large Language Models\nPietro Ferrazzia,∗, Aitor Soroab, Rodrigo Agerrib\naFondazione Bruno Kessler, Trento, Italy\nbHiTZ Center - Ixa, University of the Basque Country EHU, Spain\nAbstract\nLarge Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question\nAnswering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs,\nraising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual\nreasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrieval-\naugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions\ndrawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and out-\nof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when\nutilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs.\nWe believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual\nsettings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.\nKeywords: Medical Question Answering, Multilingualism, Reasoning, Wikipedia, Large Language Models, Artificial Intelligence\n1. Introduction\nMedical reasoning with large language models (LLMs) is\na rapidly evolving area aimed at improving Natural Language\nProcessing systems in clinical settings while addressing the black-\nbox limitations of standard models. Generating explicit reason-\ning traces, or chains of thought, has emerged as a promising ap-\nproach to guide models toward more accurate and interpretable\nanswers (Wei et al., 2022). The development of models explic-\nitly trained to generate answers via complex, multi-step reason-\ning traces has marked a notable achievement for many domains\n(Plaat et al., 2025). While reasoning about logic, mathemat-\nics, and science is naturally embedded in the advancement of\ncommon reasoning models (Zhang et al., 2024; OpenAI et al.,\n2024b; DeepSeek-AI et al., 2025), the medical domain presents\nunique challenges: medical problems are often not determinis-\ntic, the context is usually incomplete, concepts can be ambigu-\nous, and there is a great safety concern. Such a scenario has\nfostered increased interest in the field, although a few important\naspects remain underexplored. Recent approaches (Chen et al.,\n2025; Huang et al., 2025; Yu et al., 2025) have primarily re-\nlied on distilling reasoning traces from powerful models to train\nsmaller systems, often suffering from limited human validation\nand heavy dependence on synthetic data. Furthermore, multi-\nlingual reasoning has been largely neglected. Only Chen et al.\n(2025) proposes a bilingual approach, including English and\nChinese, that relies solely on distillation from OpenAI’s models\n∗Corresponding author.\nEmail address: pferrazzi@fbk.eu (Pietro Ferrazzi)\nas a source of medical knowledge. While achieving good per-\nformance, this approach falls short in grounding its reasoning in\nfactual multilingual medical knowledge. In this paper, we pro-\npose a methodology to generate multilingual reasoning traces\nfor multiple-choice medical question answering, grounded in\nfactual medical knowledge. We evaluate such traces in various\nsettings and demonstrate that training on them systematically\nyields state-of-the-art performance. Our contributions can be\nsummarised as follows:\n• We present the first dataset of medical reasoning traces\nfor Italian, Spanish and English, grounded on manually\nrevised factual knowledge1 from medical text, with the\npotential to be extended to any language in Wikipedia.\n• Comprehensive experiments demonstrate the effective-\nness of applying our reasoning traces to multilingual, multiple-\nchoice medical QA via in-context learning and super-\nvised fine-tuning.\n• We release a multilingual reasoning model that achieves\nstate-of-the-art results in Italian, English, and Spanish2.\nIn addition, we release two other datasets. First, a collection\nof all Wikipedia pages related to medicine in English, Italian\nand Spanish (Medical-Wikipedia3). Second, a translated ver-\n1https://huggingface.co/datasets/NLP-FBK/\nmultilingual-medical-reasoning-traces\n2https://huggingface.co/NLP-FBK/Qwen3-8B-medical-reasoning\n3https://huggingface.co/collections/NLP-FBK/\nmedical-wikipedia\narXiv:2512.05658v1  [cs.CL]  5 Dec 2025\n"}, {"page": 2, "text": "Figure 1: Schema of our proposed pipeline to generate reasoning traces for multilingual multiple-choice medical question answering (QA). First, we create a\nKnowledge Base (KB) of medical information for each language. We extract relevant chunks from the KB for each pair of Question-Options (QO pair) in the\nsource QA datasets, which we automatically port from English into Italian and Spanish. We prompt an LLM with the retrieved chunks and the QO pair for context\nrearrangement. Finally, we utilize the rearranged context, the question, the options and the correct answer to generate a reasoning trace that answers the question\nitself. Answers that lead to the wrong conclusion are dropped, while the remaining form our reasoning traces dataset.\nsion of two common, English-based medical question answer-\ning datasets (MedMCQA and MedQA) in Italian and Spanish4\nthat we use to guide and test the reasoning traces generation\npipeline.\n2. Related work\nIn this section, we review recent advances in reasoning ca-\npabilities for LLMs, starting with the development of foun-\ndational reasoning models like GPT-o1 and their open-source\ncounterparts, and then examining how these capabilities have\nbeen adapted for the medical domain, where reasoning-enhanced\nmodels combine domain-specific knowledge with multi-step in-\nference processes. We survey approaches ranging from distillation-\nbased methods to knowledge-grounded training, highlighting\nthe main trends in medical reasoning research.\nReasoning with LLMs. In the context of Large Language Mod-\nels, reasoning refers to the ability to answer questions by com-\nplex, multi-step processes with intermediate steps (Zhang et al.,\n2025a), producing a long chain of thought before providing\nthe actual answer to the user. The first model explicitly de-\nsigned for this purpose was OpenAI’s GPT-o1 (OpenAI et al.,\n2024c), which was trained with reinforcement learning to re-\nfine thinking process capabilities, and whose development pro-\ncess remains mostly undisclosed.\nExtensive work has been\ndone on understanding and replicating GPT-o1 training phases\n4https://huggingface.co/collections/NLP-FBK/\nmedical-qa-translated-en-es-it\nand performances (Qin et al., 2024; Huang et al., 2024; Zeng\net al., 2024). As a result of such efforts, DeepSeek’s R1 (Guo\net al., 2025) was among the first open source models to address\nthe same objective, similarly to QwQ (Team, 2025), marco-\no1 (Zhao et al., 2024), and skywork-o1 (He et al., 2024). By\nachieving high performance in several benchmarks, these mod-\nels have opened a line of research focused on investigating the\nimpact of reasoning capabilities in several domains, including\nmath (Ahn et al., 2024), physics (Xu et al., 2025), and biology\n(Liu et al., 2025b).\nReasoning for the medical domain. Previous work showed how\nmedical-oriented tuning of foundational LLMs can lead to ma-\njor improvements (Luo et al., 2022; Wu et al., 2024), even sur-\npassing human experts on some benchmarks (Singhal et al.,\n2025). These results highlighted that domain-specific align-\nment can be beneficial for medical tasks. More recently, these\nfindings have led to the integration of domain-shift and reason-\ning capabilities.\nChen et al. (2025) proposed Huatuo, among the first works\nto investigate the generation of reasoning traces for medical\nQA. Huatuo relies on automatic generation of verifiable prob-\nlems from multiple-choice medical QA datasets. The gener-\nated traces, distilled from closed-source LLMs, are used to train\nmodels via supervised fine-tuning (SFT) and reinforcement learn-\ning (RL) in both Chinese and English. The authors found that\nmodels trained on those traces outperform their base counter-\nparts on medical QA, even if the sole source of medical knowl-\nedge is a closed-source LLM.\nWu et al. (2025) advance beyond pure distillation by linking\neach question with factual medical knowledge. Such knowl-\n2\n"}, {"page": 3, "text": "Lang\nBERT\nCOMET\nchrF\nchrF++\nItalian\n0.965\n0.887\n0.780\n0.763\nSpanish\n0.967\n0.891\n0.792\n0.775\nTable 1: Evaluation metrics for back-translation in Italian and Spanish.\nedge is derived from PrimeKG (Chandak et al., 2023), a knowl-\nedge graph of 17k diseases designed for medicine analysis in\nEnglish. The model trained on those traces (MedReason) achieves\nslightly better performance than Huatuo, highlighting that med-\nically grounded data can enhance reasoning capabilities for the\nEnglish language.\nHuang et al. (2025) further advanced the field focusing on\nthe effect of test-time-scaling (Muennighoff et al., 2025). The\nauthors train a model on a few thousand examples generated\nvia distillation, highlighting that the quality of the data is what\nmatters.\nOther recent works follow the approach of generating traces\nand training on them via SFT or RL. Sun et al. (2025) build on\ndistillation approaches with architectural adjustments for trace\ngeneration, while Yu et al. (2025) construct hundreds of thou-\nsands of synthetic medical instructions derived from Common\nCrawl. While innovative, this method relies heavily on LLM-\ngenerated content with limited human validation and uncertain\nunderlying medical quality. Liu et al. (2025a) explore elicit-\ning reasoning capabilities from models without distillation and\navoiding intensive use of resources, similarly to Thapa et al.\n(2025), who present a method to optimize the use of existing\ntraces. Finally, Wang et al. (2025) provide a comprehensive\nreview of recent advances.\n3. Methodology\nWe introduce a novel methodology for distilling reasoning\ntraces that are explicitly grounded in reliable medical knowl-\nedge. The process involves four main stages: i) selecting the\nmedical questions to handle, ii) constructing a curated Knowl-\nedge Base (KB) of reliable medical texts, iii) enriching each\nmedical question with evidence retrieved from the KB using\nretrieval-augmented generation (Lewis et al., 2020), and iv) lever-\naging a large, high-capacity language model to generate a step-\nby-step reasoning trace that leads to the correct answer among\nthe given options.\n3.1. Selection of Medical Questions\nThe first step of the pipeline involves identifying the medi-\ncal questions on which to base the answer reasoning traces. We\nselect those questions from existing datasets in the field.\nDatasets. The MultiMedQA benchmark introduced in the Med-\nPaLM paper (Singhal et al., 2023), including MedMCQA (Pal\net al., 2022), MedQA (Jin et al., 2021), PubMedQA (Jin et al.,\n2019), and MMLU clinical (Hendrycks et al., 2021) is the main\nattempt to standardize multiple-choice medical QA datasets.\nFollowing prior work (Wu et al., 2025; Chen et al., 2025), we\nfocus our approach on MedMCQA and MedQA, which are con-\nstructed using medical exams from India and the USA, respec-\ntively.\nAdditionally, we include MedExpQA (Alonso et al.,\n2024), as it is currently the only manually validated multiple-\nchoice medical QA dataset available for Italian, Spanish, and\nEnglish. A summary of the selected datasets is provided in Ta-\nble 2.\nOriginal Split\nOur Split\ntrain\nval\ntest\ntrain\ntest\nMedExpQA\n434\n63\n125\n-\n622\nMedMCQA\n182.822\n4.183\n6.150\n182.822\n4.183\nMedQA\n10.178\n1.272\n1.273\n10.178\n1.273\nTable 2: Medical QA datasets distribution. For each of the selected datasets,\nwe report the original split and the split we utilized. We employed MedExpQA\nonly for testing purposes, which is why we joined all three splits into one.\nFollowing prior work, we used the original training split of both MedQA and\nMedMCQA, and tested on the validation for the latter.\nDataset translation. MedMCQA and MedQA are originally\nEnglish datasets. Given the multilingual objective of our work,\nwe translated each Question-Options pair into Italian and Span-\nish using Qwen-2.5-72B (Qwen et al., 2025), prompted with a\n5-shot example setup. To assess the quality of these translations\nand ensure that evaluation on the translated datasets is mean-\ningful, we employ back-translation: the non-English items are\ntranslated back into English and compared against the origi-\nnal. Prior work has demonstrated that back-translation scores\ncan serve as a useful proxy for translation quality and corre-\nlate with human judgments (Rapp, 2009; Zhuo et al., 2023).\nFurthermore, the widespread use of back-translation as a data\naugmentation and validation technique (Sennrich et al., 2016;\nBourgeade et al., 2024; Sugiyama and Yoshinaga, 2019) sup-\nports our use of it for assessing translation quality in Italian\nand Spanish. We compared the original and back-translated\nquestion-answer pairs by means of COMET, CHRF (Popovi´c,\n2015), CHRF++ (Popovi´c, 2017), and BERTScore (Zhang et al.,\n2020) metrics.\nAs shown in Table 1, the back-translation quality is con-\nsistently high across both Italian and Spanish, with strong se-\nmantic similarity indicated by BERTscore and COMET scores\n(0.89/0.97). The chrF and chrF++ values likewise show ro-\nbust similarity, confirming that the translated datasets remain\nfaithful to the original English content. Fine-grained results\nare reported in Appendix A.2. Nevertheless, since automatic\nmetrics may not fully reflect translation inaccuracies, we use\nthe native multilingual MedExpQA dataset solely for testing,\nensuring that our out-of-domain evaluation is performed with\nhuman-validated data.\n3.2. Knowledge Base Generation\nOur goal is to generate reasoning traces grounded in fac-\ntual medical knowledge across three languages: English, Ital-\nian, and Spanish. To achieve this, we require reliable source\n3\n"}, {"page": 4, "text": "material that comprehensively covers the range of medical spe-\ncialties. Moreover, to ensure a fair comparison of results across\nlanguages, the underlying knowledge must be aligned across\nall three of them. Without such alignment, it would be hard to\ndisentangle whether downstream performance differences arise\nfrom linguistic characteristics or from discrepancies in the knowl-\nedge sources. Accordingly, the requirements for the KB are: i)\nreliability and diversity of the medical texts Smith (2020), and\nii) parallel information availability across the three languages.\nTo satisfy these conditions, and taking advantage of its open\ncollaborative editing model and of WikiProject Medicine, we\nconsidered Wikipedia as our primary source of knowledge.\nMedical-Wikipedia creation. The construction of our medical\nknowledge base builds upon WikiProject Medicine, a Wikipedia\nproject that aims to collect all pages related to the medical do-\nmain. First, we collected links to all the English relevant pages5.\nFor each page, we extracted the main text, infobox content, and\ninterlanguage links. Using the links, we retrieved the corre-\nsponding pages in Italian and Spanish. This procedure resulted\nin a multilingual Medical-Wikipedia dataset in English, Italian,\nand Spanish, suitable for a variety of use cases beyond medical\nquestion answering, which we release publicly for the research\ncommunity.\nFrom Medical-Wikipedia to Knowledge Base. We employed\nthe constructed dataset to build language-specific knowledge\nbases that could be queried to retrieve context to support an-\nswering medical questions. To ensure a meaningful cross-lingual\ncomparison of medical reasoning, we enforced consistency across\nthe three knowledge bases by retaining only pages available in\nall three languages. While this filtering step significantly re-\nduced the dataset size, particularly for English, it resulted in a\nnearly parallel multilingual corpus. Since we rely on a project\noriginating from the English Wikipedia, pages without an En-\nglish counterpart are not included; however, given the extensive\ncoverage in English, this omission is unlikely to be significant.\nAlthough we cannot guarantee perfect alignment in page\ncontent and information density across languages, the resulting\nknowledge bases remain sufficiently comparable, ensuring that\nthe same medical topics are present in all three languages.\nWe segmented the collected pages into chunks to construct\nthe target knowledge bases. Each section of a page is treated\nas a chunk. Sections exceeding 5,000 words were discarded,\nwhile sections containing fewer than 250 words were merged\nwith the preceding ones. We filtered out irrelevant sections,\nsuch as “Bibliography” and “External links” (a comprehensive\nlist is provided in the Appendix). An overview of the outcome\nof each step is presented in Table 3.\n3.3. Retrieval Augmentation\nOnce the knowledge base is constructed, we enriched each\nmedical question from MedQA and MedMCQA with the most\n5Pages are listed at https://en.wikipedia.org/wiki/Wikipedia:\nWikiProject_Medicine/Lists_of_pages/Articles\nNumber of pages (million words)\nSource\nDe-duplicated\nAll 3 lang\nen\n33.859 (81)\n29.306 (72)\n8.577 (20)\nit\n11.045 (15)\n8.492 (8)\n8.492 (8)\nes\n12.886 (20)\n10.124 (15)\n8.513 (11)\nTable 3: Number of pages extracted from the Wikipedia Project for the three\nlanguages (Source). De-duplication combines medical topics repeated in mul-\ntiple links. Finally, the KB is restricted to pages available in all three languages\n(All 3 lang), the number of which can slightly vary as certain concepts are split\nacross multiple pages in one language but merged into a single page in another.\nrelevant contextual information.\nFollowing the paradigm of\nRetrieval-Augmented Generation, we pre-pended the retrieved\npassages to the model input prior to prompting. More specif-\nically, we pre-compute the embedding of each chunk in the KB,\ncalculating its cosine similarity with each Question-Options pair.\nWe select the top five most similar chunks as context. Em-\nbeddings are computed using the top-performing model from\nthe MTEB leaderboard (Enevoldsen et al., 2025) retrieval task\nat the time of writing (Qwen3-Embedding-8B by Zhang et al.\n(2025b)). To address the issue of potential redundancy and in-\nclusion of irrelevant information in the context highlighted by\n(Wang et al., 2024), we prompt an LLM (Qwen3-32B, Yang\net al. (2025)) to rewrite the retrieved chunks. This step ensures\nconsistency within the retrieved chunks, enhancing conciseness\nand avoiding duplication. The prompt template and more de-\ntails on the results of this step are presented in Appendix A.1.\n3.4. Generation of Reasoning Traces\nWe prompt an open-source reasoning model, Qwen3-32B,\nto generate reasoning traces, providing an input composed by\nfour parts, each placed to enhance the quality of the generated\ntraces: i) the formatted contextual information retrieved from\nthe KB, to provide factual knowledge on the medical topic; ii)\nthe medical question that defines the problem iii) the answer op-\ntions that constrain the space of reasoning paths; iv) the correct\nanswer to guide the model toward selecting the most appropri-\nate path. We enforce the use of the correct answer, motivated\nby our goal of generating the most accurate and informative\ntraces possible. To prevent the model from collapsing into sim-\nply stating the correct answer, we design the prompt to encour-\nage exploration of a space of potential answers before reaching\nthe conclusion. The system prompt, with a structure based on\nthe findings of Wu et al. (2025), is described in Appendix A.1.\nWe verify the traces’ conclusions by extracting the answer\nidentifier via regular expression matching and discarding the\nincorrect ones, as suggested by the “philosophy of verifiabil-\nity” in Chen et al. (2025). Notably, this resulted in discarding\nless than 5% of the traces, meaning that our pipeline produces\nanswers that usually lead to correct answers.\n3.5. Multilingual Reasoning Traces Dataset\nWe obtain a dataset of more than 500k reasoning traces,\neach answering a different medical question given the options in\none of the three languages. The Italian split consists of 166.257\n4\n"}, {"page": 5, "text": "Figure 2: Example of a reasoning trace given a question and the options. The generated trace is composed of 3 parts. First (violet), there is a search for potential\npaths and useful knowledge required to answer the question, which are then utilised to elaborate on the provided options (green part). Finally (red), the final answer\nis reported as conclusion.\ntraces from MedMCQA, 9.468 from MedQA; the Spanish split\nof 168.771 from MedMCQA, 9.584 from MedQA; the English\nsplit of 169.098 from MedMCQA, 9.520 from MedQA. An ex-\nample of a generated trace can be seen in Figure 2.\n4. Traces Evaluation\nWe evaluate the usefulness of the reasoning traces we gen-\nerate. Specifically, we aim to determine how well reasoning\ntraces assist in performing the Medical QA task when applied in\nin-context learning (ICL) or supervised fine-tuning (SFT) set-\ntings. Our primary measure of interest is downstream accuracy,\nwhich directly reflects whether the traces fulfill their intended\npurpose.\nFor both ICL and SFT experiments, examples are drawn\nfrom the training splits of MedQA and MedMCQA. Evaluation\nis conducted on the combined train, validation, and test splits of\nMedExpQA, guaranteeing out-of-distribution testing on origi-\nnal multilingual data, as well as on the test set of MedQA and\nthe validation set of MedMCQA6 (previously described in Ta-\nble 2).\nIn addition to the main evaluation dimension, we also ex-\namine two further aspects: (i) the comparison of our approach\nagainst prior methods, and (ii) the impact of multilingualism.\n6MedMCQA test labels are not released. We follow prior works and keep\nthe validation set for testing purposes only.\nBaselines. To determine if our traces are helpful, we need to\ndefine a baseline to compare against. We adopt a few-shot eval-\nuation setting, where each prompt includes two example ques-\ntions with their options and correct answers, followed by the\ntest question. To ensure a fair and competitive baseline, the\nexamples are selected through similarity search in an embed-\nding space: for each test question, we retrieve the most similar\ntraining questions (along with their answers) to use as few-shot\nexamples.\n4.1. In-Context Learning\nWe aim to determine the impact of utilizing our traces via\nin-context learning. To do so, we test a variety of models by\nproviding medical questions enriched with the 2 most similar\nexamples of questions, options, reasoning trace and correct an-\nswer. The only difference with the baselines lies in the use of\nreasoning traces: baseline prompts include only questions, op-\ntions, and answers, while our systems also include the corre-\nsponding reasoning traces. The retrieval strategy remains iden-\ntical across both settings, ensuring that any observed improve-\nments can be attributed to the inclusion of our traces.\nTo ensure our evaluation represents a diverse set of mod-\nels, we tested several families using vLLM7: Qwen 3 (Yang\net al., 2025) (1.7B, 8B, 32B), Llama 3 (Grattafiori et al., 2024)\n(1B, 8B, 70B), Gemma 3 (Team et al., 2025) (1B, 4B, 27B),\nMedGemma 3 (Sellergren et al., 2025) (4B, 27B) in their in-\nstructed versions, and GPT-4o (OpenAI et al., 2024a).\n7https://github.com/vllm-project/vllm\n5\n"}, {"page": 6, "text": "MedExpQA\nMedMCQA\nMedQA\nAVG\nfamily\nsize\nIT\nEN\nES\nIT\nEN\nES\nIT\nEN\nES\nLlama3\n1B\n26.3 +2.6\n33.0 -1.2\n27.3 +0.0\n31.4 +0.6\n40.2 +3.8\n33.6 +2.6\n27.4 +0.1\n35.9 +2.5\n32.0 +3.5\n31.9 +1.6\nQwen3\n1.7B\n45.1 +8.1\n51.9 +3.6\n41.6 +4.7\n51.5 +16.9\n57.2 +19.8\n53.4 +18.0\n46.8 +11.3\n57.2 +16.7\n48.3 +13.4\n50.3 +12.5\nGemma3\n4B\n47.8 -1.9\n52.5 +11.8\n52.5 -3.1\n54.1 +11.4\n58.7 +11.9\n55.1 +12.3\n50.7 +7.2\n62.9 +13.4\n53.5 +9.1\n54.2 +8.0\nMedGemma\n4B\n53.8 -2.7\n61.8 +3.1\n59.2 +5.6\n56.5 +9.9\n62.5 +10.7\n57.9 +11.1\n55.1 +4.6\n67.2 +11.8\n57.0 +7.1\n59.0 +6.8\nLlama3\n8B\n52.4 +3.2\n69.4 +6.4\n50.7 +2.8\n55.7 +9.4\n63.4 +6.0\n58.0 +10.1\n53.4 +3.0\n70.9 +7.8\n57.4 +7.6\n59.0 +6.2\nQwen3\n8B\n64.7 +4.6\n74.6 +8.6\n64.8 +0.0\n62.4 +9.3\n67.4 +8.8\n64.3 +10.4\n67.5 +10.5\n75.6 +8.5\n66.5 +6.4\n67.5 +7.5\nGemma3\n27B\n74.9 -7.0\n79.4 -0.3\n77.7 -4.5\n67.3 +9.4\n71.0 +8.2\n68.4 +8.9\n75.2 +6.0\n79.8 +6.9\n74.4 +5.5\n74.2 +3.7\nMedGemma\n27B\n76.8 -3.4\n84.6 +4.5\n81.9 -2.7\n70.4 +8.7\n76.0 +7.7\n72.5 +9.8\n82.4 +9.6\n87.1 +9.5\n81.5 +8.8\n79.2 +5.8\nQwen3\n32B\n77.6 +0.2\n80.6 -2.1\n81.9 +0.8\n69.6 +5.7\n74.7 +5.8\n70.9 +5.7\n76.4 +4.1\n84.3 +7.8\n78.4 +4.5\n77.1 +3.6\nLlama3\n70B\n78.9 +4.9\n83.8 +1.8\n82.2 +5.2\n71.1 +5.7\n76.0 +4.2\n73.3 +6.9\n80.0 +14.7\n85.5 +13.7\n80.0 +13.7\n79.0 +7.9\nAVG δ\n+0.9\n+3.6\n+0.9\n+8.7\n+8.7\n+9.6\n+7.1\n+9.9\n+8.0\n+5.8\nGPT-4o\n-\n87.0 -2.6\n88.1 +2.9\n89.9 +2.4\n87.2 +1.1\n91.2 +1.8\n88.5 +0.8\n75.0 -0.5\n79.1 -1.4\n76.0 -0.4\n84.7 +0.5\nTable 4: Performances of models exposed to our traces via in-context learning (2-shot examples, each question example is provided with our trace as the answer).\nAccuracy is reported with deltas (δ) from baseline (2-shot setting without traces); in bold: best overall results per language for open-weights LLMs. All models\nare used in their instruct version.\nMedExpQA\nMedMCQA\nMedQA\nAVG δ\nfamily\nsize\nIT\nEN\nES\nIT\nEN\nES\nIT\nEN\nES\nLlama3\n8B\n58.0 +9.0\n65.4 +2.7\n54.3 +6.4\n46.8 +0.4\n59.0 +1.5\n50.6 +2.7\n54.0 +2.3\n71.6 +4.4\n59.1 +6.2\n+4.0\nQwen3\n8B\n62.2 +2.1\n69.7 +3.7\n69.7 +4.8\n52.3 -0.7\n60.9 +2.3\n53.2 -0.6\n61.7 +3.1\n71.7 +5.4\n65.0 +4.0\n+2.7\nAVG δ\n+5.6\n+3.2\n+5.6\n-0.1\n+1.9\n+1.0\n+2.7\n+4.9\n+5.1\n+3.9\nTable 5: Performances of models fine-tuned on our reasoning traces. Accuracy is reported with deltas from baseline in parentheses.\nResults. We observed that our traces enable all open-source\nmodels to generate more accurate answers compared to the base-\nline, as reported in Table 4. The average effect of our traces\namong datasets and languages is shown in Figure 3.\nFor medical questions in MedMCQA and MedQA, we ob-\nserve an average increase in accuracy of +7 to +10 points across\nall languages. The out-of-distribution dataset (MedExpQA) is\nthe one that benefits the least, with the case of Gemma-3-27B\nand Qwen-3-32B not getting any benefit at all. Nevertheless,\nthe overall impact among models on this dataset is positive,\nwith an average increase in accuracy of +1.8 points. To ver-\nify the hypothesis of such a positive impact being significant,\nwe employ the t-test on the deltas between model performances\nwith and without our traces, resulting in a p-value of 0.02.\nIn the case of GPT-4o, the overall performance increase due\nto exposing the model to our transcripts via in-context learning\nis +0.5 across datasets and languages. We test the significance\nof this impact by means of the t-test, resulting in a p-value of\n0.2. This lack of significance is mainly due to the negative im-\npact on MedMCQA and the null impact on Italian.\n4.2. Supervised Fine-Tuning\nTo measure how our reasoning traces influence model learn-\ning, we fine-tuned models in a supervised setting. Due to com-\nputational constraints, we restricted our experiments to Llama-3-8B\nand Qwen3-8B. We trained each model on examples formatted\nas <q>{question}</q><o>{options}</o>{trace}.\nWe refined the training data by distinguishing between “reasoning-\nintensive” and “knowledge-intensive” questions, following the\nclassification method proposed by Thapa et al. (2025). Their\nwork demonstrated that prioritizing reasoning-intensive exam-\nples during training yields better downstream QA performance\nthan using a random sample. We also experimented with us-\ning the full dataset, but observed worse results.\nConsistent\nwith the findings of Liu et al. (2025a), which suggest priori-\ntizing MedQA sampling ratio as training source over MedM-\nCQA, our final fine-tuning dataset comprises 5,837 traces from\nMedMCQA and 5,594 from MedQA per language, for a total\nof 34,293 examples across English, Italian, and Spanish.\nWe fine-tuned the models by tuning all parameters for 3\nepochs on two H200 GPUs, with a per-device batch size of 32,\nusing AdamW optimiser with a learning rate of 5e −6, a co-\nsine scheduler, and a warm-up ratio of 0.1. Training on these\nsettings took 1.5 hours per model.\nResults. We observed that training on our traces benefits both\nLlama-3.1-8B-Instruct and Qwen3-8B, yielding an aver-\nage accuracy improvement of 3.9 points (Table 5). Interest-\n6\n"}, {"page": 7, "text": "Figure 3: Impact of our traces via in-context learning per dataset and language.\nEach boxplot represents the distribution of the improvement due to exposing\nmodels to our traces as two-shot examples for the eleven tested open models.\nThe improvement is measured in terms of an increase in accuracy on the testing\nsplits of the datasets. The baseline is represented by the same model prompted\nvia two-shot, where each example contains the correct answer without our gen-\nerated reasoning.\ningly, the gains are smaller than those seen in the few-shot\nsetting. We attribute this to the nature of the traces: they are\nhighly informative, and when provided directly as few-shot ex-\namples, they act as strong, targeted guidance for the test ques-\ntion to instructed models that are highly capable of handling\nlong prompts. In contrast, fine-tuning aims to generalize such\nknowledge, which turns out to be less effective than deliver-\ning the exact relevant information at inference time. We further\nanalyse this behaviour in the next section, showing that com-\nbining fine-tuned models with our traces in few-shot prompts\nleads to the best overall performance.\n4.3. Comparison with previous work\nAs described in Section 2, the main previous efforts on gen-\nerating reasoning traces for medical question answering are Hu-\natuo (Chen et al., 2025), MedReason (Wu et al., 2025), and\nm1-m23k (Huang et al., 2025). They generated and released\ntraces of 19.704, 32.682, and 23.493 question-trace pairs, re-\nspectively, on which they trained models using supervised fine-\ntuning and reinforcement learning techniques. We aim to com-\npare our traces against those from previous work. We do so\nin three steps. First, we evaluate their effect in (i) few-shot\nprompting and (ii) supervised fine-tuning. Finally, (iii) we eval-\nuate each method under its strongest reported configuration.\nThis final step goes beyond a trace-level comparison, highlight-\ning overall effectiveness and demonstrating that our best model\nconsistently outperforms any other alternatives.\nFew-Shot. We test the effect of MedReason and m1 traces at\ninference time on the same eleven models described in Sec-\ntion 4.1 and measuring accuracy improvements, directly com-\nparing against our traces. Huatuo traces could not be evaluated\nin this setting because they are framed as open-ended questions\nwithout multiple-choice options or unique correct answers. We\nobserved that our traces provide the greatest average improve-\nment among datasets and languages. We report the overall im-\npact in Figure 4 (top left); detailed results are presented in Ap-\npendix C.\nFine-Tuning. We fine-tuned Llama-3.1-8B-Instruct and Qwen3-8B\nusing the reasoning traces and official codebases provided by\nMedReason, m1, and Huatuo. We retained the hyperparameters\nspecified in each work, training on two H200 GPUs for a total\nof approximately 18 hours. The resulting models are evaluated\nin our multilingual setting. Since the evaluated trace types are\nEnglish-only, models trained on them tend to produce reason-\ning in English at inference time. To mitigate this, we explicitly\nprompted them to generate responses in the target language of\neach question, as shown in Appendix A.1.\nDownstream accuracy demonstrates that our traces achieve\nsuperior performance across all datasets and languages com-\npared to other models (Figure 4, top right; detailed results in\nAppendix C), with the sole exception of Huatuo traces applied\nto Qwen on MedMCQA and MedQA.\nState-of-the-art performance. Finally, we compare the overall\nperformance of the strongest models from prior work to our\nbest fine-tuned model based on Qwen3-8B. The results aggre-\ngated by language and dataset are presented in Figure 4 (bottom\nleft, “state-of-the-art performances”), while detailed results can\nbe found in the Appendix, Table C.10. We find that our model\nin its basic configuration outperforms the best prior model, Hu-\natuo, by an average of 3 points. In addition, improving our\nfine-tuned model with two of our traces as examples at infer-\nence time obtains the highest overall accuracy, surpassing Hu-\natuo by 7 points. When considering only the English portion of\nthe datasets, our enhanced model still outperforms Huatuo by a\nmargin of +1.7 points.\n4.4. Language-specific SFT\nPrevious sections demonstrated that integrating multilingual\ndata during fine-tuning leads to improved accuracy on down-\nstream tasks. To further examine this phenomenon, this study\nanalyzes cross-lingual transfer effects throughout model train-\ning.\nMore specifically, we trained Qwen3-8B using reasoning\ntraces from individual languages as well as from all three lan-\nguages put together, following the training protocol outlined\nin Section 4.2. The results indicate that multilingual training\nconsistently matches or surpasses the performance of single-\nlanguage training. In contrast, models trained exclusively on\none language exhibit worse accuracy when evaluated on other\nlanguages (see Figure 4, bottom-right). These findings under-\nscore the positive impact of cross-lingual learning and empha-\nsize the value of incorporating diverse languages into the train-\ning data.\n5. Error analysis\nTo gain deeper insight into model errors, we conducted an\nerror analysis on the highest-performing model in our study,\nQwen3-8B.\n7\n"}, {"page": 8, "text": "Figure 4: Comparison of our traces with previous work and impact of multilingual fine-tuning. The top-left box-plots represent the distribution of the average\nimprovement in accuracy among 11 open-source models by trace type when provided at inference time (two-shot). The top-right histogram shows the average\nimpact of different types of traces when provided at fine-tuning time. The bottom-left histogram shows the performances of the best models proposed in the\nliterature against our fine-tuned version of Qwen3-8B (\"OUR FT\") and the same model further enhanced with 2 of our traces. The bottom-right histogram reports\non the impact of fine-tuning on single-language (‘Only ES/EN/IT”) versus training on all the languages (“ES+EN+IT”).\nQuantitative analysis. We first analyzed questions that the model\nanswered incorrectly in the 2-shot baseline but correctly after\nfine-tuning with our traces. On average, 45% of baseline er-\nrors were corrected. Correction rates showed minimal cross-\nlinguistic variation (English: 46%, Italian: 44%, Spanish: 46%),\nindicating no language-specific advantage.\nComparing in-domain (46% correction rate) and out-of-domain\n(38%) datasets revealed a gap. This gap, yet modest, resulted\nin a statistically significant difference when tested via a two-\nproportion z-test (p-val of 0.05). While this difference reflects\nthe expected benefit of domain-aligned training data, the rel-\natively small magnitude suggests reasonable generalization of\nour traces across domains.\nThis quantitative analysis revealed minimal variation across\nlanguages and modest differences across datasets, providing lim-\nited insight into model behavior. To gain a deeper understand-\ning, we conducted manual error analysis focusing on cases where\nthe fine-tuned model continued to fail.\nQualitative analysis. Although aggregate accuracy metrics sum-\nmarize model performance, they do not explain the underlying\nsources of error. To address this limitation, manual error anal-\nysis is used to identify systematic failure modes, including fac-\ntual inaccuracies, deficits in medical knowledge, and inconsis-\ntencies in reasoning.\nWe performed a manual analysis of a subset of 20 ques-\ntions that our best-performing model (Qwen3-8B) trained on\nour traces answered incorrectly. We asked two medical doc-\ntors to determine which dimensions would be useful to anal-\nyse to help us understand the primary error sources. Based on\nthis sample, they identified that issues were raised because of:\n(i) incomplete or incorrect use of information provided in the\nquestion, (ii) insufficient factual medical knowledge, and (iii)\nlimitations in applying logical reasoning based on that knowl-\nedge. Based on these findings, we constructed three questions\nto guide further analysis:\n• Does the answer take into account the useful elements\npresent in the question?\n• Does the answer report medical knowledge mistakes?\n• Does the answer contain logical mistakes?\nTo quantify the contribution of each error source, we ran-\ndomly sampled an additional 75 question-option-reasoning triplets\n8\n"}, {"page": 9, "text": "Figure 5: Example of the expert analysis on a model answer which presents a critical clinical knowledge mistake.\nand reviewed them with the support of eleven medical doc-\ntors selected to cover different linguistic areas8. The physicians\nwere asked to analyze triplets in their native language (English,\nItalian, or Spanish). For each triplet, we ask one doctor to anal-\nyse it in the light of the three defined questions. We carefully\nread the answers and found that the main performance bottle-\nneck is the limited medical knowledge of the model, with the\nmost apparent inconsistencies of reasoning arising from diffi-\nculties in integrating relevant clinical information rather than\nfrom reasoning alone.\nFurthermore, overlooked information\nfrom the question frequently related to the interactions between\npatient characteristics and underlying medical conditions.\nOverall, the model’s errors can be primarily attributed to the\nlack of medical knowledge. This includes failure to integrate\nthe full patient history, instead overemphasizing isolated details\n(Appendix D, Figure D.8); difficulty in correctly applying stan-\n8Six from Italy for Italian; two from Peru and one from Spain for Spanish;\none from Canada and one from Vietnam for English.\ndard diagnostic protocols (Figure 5); and restricted capacity to\nexercise nuanced clinical judgment (Appendix D Figure D.7).\nAdditional illustrative cases from the manual error analysis for\neach language are provided in Appendix D.\n6. Conclusion\nThis paper introduces a new methodology for generating\nmultilingual medical reasoning traces grounded in manually re-\nvised factual medical knowledge extracted from the WikiProject\nMedicine, addressing critical gaps in current LLM approaches\nfor medical question answering. Our main contributions are\nthreefold: First, we present the first dataset of medical rea-\nsoning traces for Italian, Spanish, and English, generated from\nanswering medical questions from MedQA and MedMCQA.\nSecond, we conduct comprehensive experiments demonstrating\nthat training on these traces improves both in-context learning\nand supervised fine-tuning, consistently achieving state-of-the-\nart performance in multilingual medical QA. Third, we release\n9\n"}, {"page": 10, "text": "a multilingual reasoning model alongside two new resources: a\nmultilingual collection of reasoning traces and translated ver-\nsions of established medical QA benchmarks.\nOur evaluation demonstrates that the traces we generate are\nconsistently useful across model families applied via in-context\nlearning as well as supervised fine-tuning, helping models to\nlearn to answer medical questions more accurately.\nComparative evaluation against prior work confirms that our\napproach obtains better accuracy in multilingual, multiple-choice\nmedical question answering, with marginal improvements ob-\nserved even on English-specific benchmarks. Crucially, we es-\ntablish that multilingual training confers significant performance\nadvantages: models trained on reasoning traces from a sin-\ngle language consistently underperform relative to those trained\njointly on all three languages.\nLimitations\nOur work has some limitations. First, we focus on only\nthree languages, namely, English, Italian, and Spanish, whereas\nmultilingual medical reasoning could be extended to a much\nbroader set of languages. Second, we rely on Wikipedia as our\nprimary source of medical knowledge due to its cross-lingual\nparallelism, though more authoritative medical sources could\nfurther strengthen grounding. Furthermore, the English knowl-\nedge base is significantly larger than the others due to the na-\nture of Wikipedia pages. Third, our fine-tuning experiments\nare restricted to 8B-parameter models because of computational\nconstraints; scaling to larger models remains unexplored. Fi-\nnally, we limit our study to multiple-choice medical question\nanswering, leaving open-ended clinical reasoning tasks for fu-\nture work.\nAcknowledgments\nWe acknowledge the contributions of the following medical\ndoctors who contributed to performing the manual error analy-\nsis described in Section 5: Marcello Albini, Bianca Bernante,\nHong Anh Chau, Davide Dotto, Marta Dziech, Daniel García\nGuadix, Sara Malerba, Elena Maurin, Matteo Savio, José Luis\nParedes Sosa, Luca Vavolo, Juan Luis Cuentas Zavala.\nThis work has been supported by the HiTZ center and the\nBasque Government (Research group funding IT-1805-22). We\nalso thank the following MCIN/AEI/10.13039/501100011033\nprojects: (i) DeepKnowledge (PID2021-127777OB-C21) and\nERDF A way of making Europe; (ii) DeepMinor (CNS2023-\n144375) and European Union NextGenerationEU/PRTR.\nReferences\nAhn, J., Verma, R., Lou, R., Liu, D., Zhang, R., Yin, W.,\n2024. Large language models for mathematical reasoning:\nProgresses and challenges, in: Falk, N., Papi, S., Zhang, M.\n(Eds.), Proceedings of the 18th Conference of the European\nChapter of the Association for Computational Linguistics:\nStudent Research Workshop, pp. 225–237.\nAlonso, I., Oronoz, M., Agerri, R., 2024. Medexpqa: Multi-\nlingual benchmarking of large language models for medical\nquestion answering. Artificial Intelligence in Medicine 155,\n102938.\nBourgeade, T., Casola, S., Mahmoud Wizan, A., Bosco,\nC., 2024. Data augmentation through back-translation for\nstereotypes and irony detection, in: Dell’Orletta, F., Lenci,\nA., Montemagni, S., Sprugnoli, R. (Eds.), Proceedings of\nthe Tenth Italian Conference on Computational Linguistics\n(CLiC-it 2024), CEUR Workshop Proceedings, Pisa, Italy.\npp. 90–97.\nChandak, P., Huang, K., Zitnik, M., 2023. Building a knowl-\nedge graph to enable precision medicine. Scientific Data 10,\n67.\nChen, J., Cai, Z., Ji, K., Wang, X., Liu, W., Wang, R., Wang,\nB., 2025. Towards medical complex reasoning with LLMs\nthrough medical verifiable problems, in: Che, W., Nabende,\nJ., Shutova, E., Pilehvar, M.T. (Eds.), Findings of the Asso-\nciation for Computational Linguistics: ACL 2025, Vienna,\nAustria. pp. 14552–14573.\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang,\nR., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu,\nX., Wu, Y., Wu, Z.F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu,\nA., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C.,\nDeng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li,\nE., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang,\nH., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H.,\nQu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J.,\nQiu, J., Li, J., Cai, J.L., Ni, J., Liang, J., Chen, J., Dong, K.,\nHu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L.,\nZhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M.,\nTian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q.,\nGe, R., Zhang, R., Pan, R., Wang, R., Chen, R.J., Jin, R.L.,\nChen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu,\nS., Zhou, S., Pan, S., Li, S.S., Zhou, S., Wu, S., Ye, S., Yun,\nT., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W.,\nLiang, W., Gao, W., Yu, W., Zhang, W., Xiao, W.L., An, W.,\nLiu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X.,\nXie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X.Q.,\nJin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X.,\nZhou, X., Wang, X., Shan, X., Li, Y.K., Wang, Y.Q., Wei,\nY.X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y.,\nYu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang,\nY., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong,\nY., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou,\nY., Zhu, Y.X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y.,\nMa, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z.Z., Ren, Z., Sha,\nZ., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan,\nZ., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song,\nZ., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., Zhang, Z., 2025.\nDeepseek-r1: Incentivizing reasoning capability in llms via\nreinforcement learning. arXiv:2501.12948.\n10\n"}, {"page": 11, "text": "Enevoldsen, K., Chung, I., Kerboua, I., Kardos, M., Mathur, A.,\nStap, D., Gala, J., Siblini, W., Krzemi´nski, D., Winata, G.I.,\nSturua, S., Utpala, S., Ciancone, M., Schaeffer, M., Misra,\nD., Dhakal, S., Rystrøm, J., Solomatin, R., Ça˘gatan, Ö.V.,\nKundu, A., Bernstorff, M., Xiao, S., Sukhlecha, A., Pahwa,\nB., Po´swiata, R., GV, K.K., Ashraf, S., Auras, D., Plüster,\nB., Harries, J.P., Magne, L., Mohr, I., Zhu, D., Gisserot-\nBoukhlef, H., Aarsen, T., Kostkan, J., Wojtasik, K., Lee,\nT., Suppa, M., Zhang, C., Rocca, R., Hamdy, M., Michail,\nA., Yang, J., Faysse, M., Vatolin, A., Thakur, N., Dey, M.,\nVasani, D., Chitale, P.A., Tedeschi, S., Tai, N., Snegirev, A.,\nHendriksen, M., Günther, M., Xia, M., Shi, W., Lù, X.H.,\nClive, J., K, G., Anna, M., Wehrli, S., Tikhonova, M., Pan-\nchal, H.S., Abramov, A., Ostendorff, M., Liu, Z., Clematide,\nS., Miranda, L.J.V., Fenogenova, A., Song, G., Safi, R.B.,\nLi, W.D., Borghini, A., Cassano, F., Hansen, L., Hooker, S.,\nXiao, C., Adlakha, V., Weller, O., Reddy, S., Muennighoff,\nN., 2025. MMTEB: Massive multilingual text embedding\nbenchmark, in: The Thirteenth International Conference on\nLearning Representations.\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian,\nA., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A.,\nVaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn,\nA., Yang, A., Mitra, A., Sravankumar, A., Korenev, A.,\nHinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson,\nA., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B.,\nCaucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C.,\nKeller, C., Touret, C., Wu, C., Wong, C., Ferrer, C.C., Niko-\nlaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D.,\nWyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-\nOlano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy,\nE., Lobanova, E., Dinan, E., Smith, E.M., Radenovic, F.,\nGuzman, F., Zhang, F., Synnaeve, G., Lee, G., Anderson,\nG.L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell,\nG., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov,\nI., Ibarra, I.A., Kloumann, I., Misra, I., Evtimov, I., Zhang,\nJ., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Ma-\nhadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong,\nJ., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J.,\nBitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe,\nJ., Jia, J., Alwala, K.V., Prasad, K., Upasani, K., Plawiak, K.,\nLi, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Ma-\nlik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L.,\nvan der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L.,\nMadaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira,\nL., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kar-\ndas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova,\nM., Kambadur, M., Lewis, M., Si, M., Singh, M.K., Has-\nsan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev,\nN., Chatterji, N., Zhang, N., Duchenne, O., Celebi, O., Al-\nrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P.,\nDubal, P., Krishnan, P., Koura, P.S., Xu, P., He, Q., Dong,\nQ., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R.S.,\nStojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Pa-\ntel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R.,\nSilva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa,\nS., Singh, S., Bell, S., Kim, S.S., Edunov, S., Nie, S., Narang,\nS., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S.,\nVandenhende, S., Batra, S., Whitman, S., Sootla, S., Col-\nlot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler,\nT., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T.,\nMihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V.,\nRamanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V.,\nAlbiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers,\nW., Martinet, X., Wang, X., Wang, X., Tan, X.E., Xia, X.,\nXie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei,\nY., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert,\nZ.D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava,\nA., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victo-\nria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg,\nA., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo,\nA., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A.,\nHo, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A.,\nFranco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A.,\nBharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer,\nB., Leonhardi, B., Huang, B., Loyd, B., Paola, B.D., Paran-\njape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B.,\nSpence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker,\nC., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou,\nC., Hu, C., Chu, C.H., Cai, C., Tindal, C., Feichtenhofer, C.,\nGao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins,\nD., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich,\nD., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E.,\nJamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E.,\nLe, E.T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers,\nE., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F.,\nCaggioni, F., Kanayet, F., Seide, F., Florez, G.M., Schwarz,\nG., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G.,\nGuangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanaz-\neri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph,\nH., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj,\nI., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.E., Gat,\nI., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J.,\nGaya, J.B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizen-\nstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings,\nJ., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg,\nJ., Wang, J., Wu, K., U, K.H., Saxena, K., Khandelwal, K.,\nZand, K., Matosich, K., Veeraraghavan, K., Michelena, K.,\nLi, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K.,\nChen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L.,\nGuo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M.,\nAvalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie,\nM., Reso, M., Groshev, M., Naumov, M., Lathi, M., Ke-\nneally, M., Liu, M., Seltzer, M.L., Valko, M., Restrepo, M.,\nPatel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey,\nM., Wang, M., Hermoso, M.J., Metanat, M., Rastegari, M.,\nBansal, M., Santhanam, N., Parks, N., White, N., Bawa, N.,\nSinghal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N.P.,\nDong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O.,\nKalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P.,\nBontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchan-\ndani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub,\nR., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li,\n11\n"}, {"page": 12, "text": "R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R.,\nMehta, S., Siby, S., Bondu, S.J., Datta, S., Chugh, S., Hunt,\nS., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma,\nS., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S.,\nFeng, S., Lin, S., Zha, S.C., Patil, S., Shankar, S., Zhang, S.,\nZhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S.,\nMax, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad,\nS., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S.,\nChoudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T.,\nKoehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T.,\nChou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V.,\nMohan, V., Kumar, V.S., Mangla, V., Ionescu, V., Poenaru,\nV., Mihailescu, V.T., Ivanov, V., Li, W., Wang, W., Jiang, W.,\nBouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X.,\nWu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi,\nY., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang,\nZhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito,\nZ., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., Ma, Z., 2024.\nThe llama 3 herd of models. arXiv:2407.21783.\nGuo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu,\nR., Zhang, R., Ma, S., Bi, X., Zhang, X., Yu, X., Wu, Y.,\nWu, Z.F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue,\nB., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C.,\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F.,\nLuo, F., Hao, G., Chen, G., Li, G., Zhang, H., Xu, H., Ding,\nH., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Chen, J., Yuan,\nJ., Tu, J., Qiu, J., Li, J., Cai, J.L., Ni, J., Liang, J., Chen,\nJ., Dong, K., Hu, K., You, K., Gao, K., Guan, K., Huang,\nK., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang,\nL., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Zhou,\nM., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang,\nP., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R.,\nWang, R., Chen, R.J., Jin, R.L., Chen, R., Lu, S., Zhou, S.,\nChen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S.S.,\nZhou, S., Wu, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng,\nW., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao,\nW.L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng,\nX., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin,\nX., Li, X.Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang,\nX., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y.K., Wang,\nY.Q., Wei, Y.X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y.,\nWang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao,\nY., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang,\nY., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y.,\nLiu, Y., Zhou, Y., Zhu, Y.X., Huang, Y., Li, Y., Zheng, Y.,\nZhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z.Z., Ren,\nZ., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma,\nZ., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z.,\nSong, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., Zhang, Z.,\n2025. Deepseek-r1 incentivizes reasoning in llms through\nreinforcement learning. Nature 645, 633–638.\nHe, J., Wei, T., Yan, R., Liu, J., Wang, C., Gan, Y., Tu, S., Liu,\nC.Y., Zeng, L., Wang, X., Wang, B., Li, Y., Zhang, F., Xu, J.,\nAn, B., Liu, Y., Zhou, Y., 2024. Skywork-o1 open series.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D., Steinhardt, J., 2021.\nMeasuring massive multi-\ntask language understanding, in: International Conference on\nLearning Representations.\nHuang, X., Wu, J., Liu, H., Tang, X., Zhou, Y., 2025. m1:\nUnleash the potential of test-time scaling for medical reason-\ning in large language models, in: The Second Workshop on\nGenAI for Health: Potential, Trust, and Policy Compliance.\nHuang, Z., Zou, H., Li, X., Liu, Y., Zheng, Y., Chern, E., Xia,\nS., Qin, Y., Yuan, W., Liu, P., 2024. O1 replication journey\n– part 2: Surpassing o1-preview through simple distillation,\nbig progress or bitter lesson? arXiv:2411.16489.\nJin, D., Pan, E., Oufattole, N., Weng, W.H., Fang, H., Szolovits,\nP., 2021. What disease does this patient have? a large-scale\nopen domain question answering dataset from medical ex-\nams. Applied Sciences 11, 6421.\nJin, Q., Dhingra, B., Liu, Z., Cohen, W., Lu, X., 2019. Pub-\nMedQA: A dataset for biomedical research question answer-\ning, in: Inui, K., Jiang, J., Ng, V., Wan, X. (Eds.), Proceed-\nings of the 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International Joint Con-\nference on Natural Language Processing (EMNLP-IJCNLP),\nHong Kong, China. pp. 2567–2577.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,\nGoyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T.,\nRiedel, S., Kiela, D., 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks, in: Larochelle, H., Ran-\nzato, M., Hadsell, R., Balcan, M., Lin, H. (Eds.), Advances in\nNeural Information Processing Systems, Curran Associates,\nInc.. pp. 9459–9474.\nLiu, C., Wang, H., Pan, J., Wan, Z., Dai, Y., Lin, F., Bai, W.,\nRueckert, D., Arcucci, R., 2025a. Beyond distillation: Push-\ning the limits of medical llm reasoning with minimalist rule-\nbased rl. arXiv:2505.17952.\nLiu, Y., Lv, L., Zhang, X., Yuan, L., Tian, Y., 2025b. Bio-\nprobench: Comprehensive dataset and benchmark in biolog-\nical protocol understanding and reasoning.\nLuo, R., Sun, L., Xia, Y., Qin, T., Zhang, S., Poon, H., Liu,\nT.Y., 2022. Biogpt: Generative pre-trained transformer for\nbiomedical text generation and mining. Briefings in Bioin-\nformatics 23.\nMuennighoff, N., Yang, Z., Shi, W., Li, X.L., Fei-Fei, L.,\nHajishirzi, H., Zettlemoyer, L., Liang, P., Candes, E.,\nHashimoto, T., 2025.\ns1:\nSimple test-time scaling, in:\nChristodoulopoulos, C., Chakraborty, T., Rose, C., Peng,\nV. (Eds.), Proceedings of the 2025 Conference on Empir-\nical Methods in Natural Language Processing, Association\nfor Computational Linguistics, Suzhou, China. pp. 20286–\n20332.\nOpenAI, :, Hurst, A., Lerer, A., Goucher, A.P., Perelman, A.,\nRamesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes,\n12\n"}, {"page": 13, "text": "A., Radford, A., Madry, A., Baker-Whitcomb, A., Beutel, A.,\nBorzunov, A., Carney, A., Chow, A., Kirillov, A., Nichol, A.,\nPaino, A., Renzin, A., Passos, A.T., Kirillov, A., Christakis,\nA., Conneau, A., Kamali, A., Jabri, A., Moyer, A., Tam, A.,\nCrookes, A., Tootoochian, A., Tootoonchian, A., Kumar, A.,\nVallone, A., Karpathy, A., Braunstein, A., Cann, A., Codis-\npoti, A., Galu, A., Kondrich, A., Tulloch, A., Mishchenko,\nA., Baek, A., Jiang, A., Pelisse, A., Woodford, A., Gosalia,\nA., Dhar, A., Pantuliano, A., Nayak, A., Oliver, A., Zoph,\nB., Ghorbani, B., Leimberger, B., Rossen, B., Sokolowsky,\nB., Wang, B., Zweig, B., Hoover, B., Samic, B., McGrew,\nB., Spero, B., Giertler, B., Cheng, B., Lightcap, B., Walkin,\nB., Quinn, B., Guarraci, B., Hsu, B., Kellogg, B., Eastman,\nB., Lugaresi, C., Wainwright, C., Bassin, C., Hudson, C.,\nChu, C., Nelson, C., Li, C., Shern, C.J., Conger, C., Barette,\nC., Voss, C., Ding, C., Lu, C., Zhang, C., Beaumont, C., Hal-\nlacy, C., Koch, C., Gibson, C., Kim, C., Choi, C., McLeavey,\nC., Hesse, C., Fischer, C., Winter, C., Czarnecki, C., Jarvis,\nC., Wei, C., Koumouzelis, C., Sherburn, D., Kappler, D.,\nLevin, D., Levy, D., Carr, D., Farhi, D., Mely, D., Robin-\nson, D., Sasaki, D., Jin, D., Valladares, D., Tsipras, D., Li,\nD., Nguyen, D.P., Findlay, D., Oiwoh, E., Wong, E., As-\ndar, E., Proehl, E., Yang, E., Antonow, E., Kramer, E., Pe-\nterson, E., Sigler, E., Wallace, E., Brevdo, E., Mays, E.,\nKhorasani, F., Such, F.P., Raso, F., Zhang, F., von Lohmann,\nF., Sulit, F., Goh, G., Oden, G., Salmon, G., Starace, G.,\nBrockman, G., Salman, H., Bao, H., Hu, H., Wong, H.,\nWang, H., Schmidt, H., Whitney, H., Jun, H., Kirchner, H.,\nde Oliveira Pinto, H.P., Ren, H., Chang, H., Chung, H.W.,\nKivlichan, I., O’Connell, I., O’Connell, I., Osband, I., Sil-\nber, I., Sohl, I., Okuyucu, I., Lan, I., Kostrikov, I., Sutskever,\nI., Kanitscheider, I., Gulrajani, I., Coxon, J., Menick, J., Pa-\nchocki, J., Aung, J., Betker, J., Crooks, J., Lennon, J., Kiros,\nJ., Leike, J., Park, J., Kwon, J., Phang, J., Teplitz, J., Wei, J.,\nWolfe, J., Chen, J., Harris, J., Varavva, J., Lee, J.G., Shieh,\nJ., Lin, J., Yu, J., Weng, J., Tang, J., Yu, J., Jang, J., Candela,\nJ.Q., Beutler, J., Landers, J., Parish, J., Heidecke, J., Schul-\nman, J., Lachman, J., McKay, J., Uesato, J., Ward, J., Kim,\nJ.W., Huizinga, J., Sitkin, J., Kraaijeveld, J., Gross, J., Ka-\nplan, J., Snyder, J., Achiam, J., Jiao, J., Lee, J., Zhuang, J.,\nHarriman, J., Fricke, K., Hayashi, K., Singhal, K., Shi, K.,\nKarthik, K., Wood, K., Rimbach, K., Hsu, K., Nguyen, K.,\nGu-Lemberg, K., Button, K., Liu, K., Howe, K., Muthuku-\nmar, K., Luther, K., Ahmad, L., Kai, L., Itow, L., Work-\nman, L., Pathak, L., Chen, L., Jing, L., Guy, L., Fedus, L.,\nZhou, L., Mamitsuka, L., Weng, L., McCallum, L., Held, L.,\nOuyang, L., Feuvrier, L., Zhang, L., Kondraciuk, L., Kaiser,\nL., Hewitt, L., Metz, L., Doshi, L., Aflak, M., Simens, M.,\nBoyd, M., Thompson, M., Dukhan, M., Chen, M., Gray, M.,\nHudnall, M., Zhang, M., Aljubeh, M., Litwin, M., Zeng,\nM., Johnson, M., Shetty, M., Gupta, M., Shah, M., Yatbaz,\nM., Yang, M.J., Zhong, M., Glaese, M., Chen, M., Janner,\nM., Lampe, M., Petrov, M., Wu, M., Wang, M., Fradin,\nM., Pokrass, M., Castro, M., de Castro, M.O.T., Pavlov, M.,\nBrundage, M., Wang, M., Khan, M., Murati, M., Bavarian,\nM., Lin, M., Yesildal, M., Soto, N., Gimelshein, N., Cone,\nN., Staudacher, N., Summers, N., LaFontaine, N., Chowd-\nhury, N., Ryder, N., Stathas, N., Turley, N., Tezak, N., Felix,\nN., Kudige, N., Keskar, N., Deutsch, N., Bundick, N., Puck-\nett, N., Nachum, O., Okelola, O., Boiko, O., Murk, O., Jaffe,\nO., Watkins, O., Godement, O., Campbell-Moore, O., Chao,\nP., McMillan, P., Belov, P., Su, P., Bak, P., Bakkum, P., Deng,\nP., Dolan, P., Hoeschele, P., Welinder, P., Tillet, P., Pronin, P.,\nTillet, P., Dhariwal, P., Yuan, Q., Dias, R., Lim, R., Arora,\nR., Troll, R., Lin, R., Lopes, R.G., Puri, R., Miyara, R.,\nLeike, R., Gaubert, R., Zamani, R., Wang, R., Donnelly, R.,\nHonsby, R., Smith, R., Sahai, R., Ramchandani, R., Huet, R.,\nCarmichael, R., Zellers, R., Chen, R., Chen, R., Nigmatullin,\nR., Cheu, R., Jain, S., Altman, S., Schoenholz, S., Toizer,\nS., Miserendino, S., Agarwal, S., Culver, S., Ethersmith, S.,\nGray, S., Grove, S., Metzger, S., Hermani, S., Jain, S., Zhao,\nS., Wu, S., Jomoto, S., Wu, S., Shuaiqi, Xia, Phene, S., Pa-\npay, S., Narayanan, S., Coffey, S., Lee, S., Hall, S., Balaji,\nS., Broda, T., Stramer, T., Xu, T., Gogineni, T., Christianson,\nT., Sanders, T., Patwardhan, T., Cunninghman, T., Degry, T.,\nDimson, T., Raoux, T., Shadwell, T., Zheng, T., Underwood,\nT., Markov, T., Sherbakov, T., Rubin, T., Stasi, T., Kaftan,\nT., Heywood, T., Peterson, T., Walters, T., Eloundou, T., Qi,\nV., Moeller, V., Monaco, V., Kuo, V., Fomenko, V., Chang,\nW., Zheng, W., Zhou, W., Manassra, W., Sheu, W., Zaremba,\nW., Patil, Y., Qian, Y., Kim, Y., Cheng, Y., Zhang, Y., He, Y.,\nZhang, Y., Jin, Y., Dai, Y., Malkov, Y., 2024a. Gpt-4o system\ncard. arXiv:2410.21276.\nOpenAI, :, Jaech, A., Kalai, A., Lerer, A., Richardson, A.,\nEl-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A.,\nCarney, A., Iftimie, A., Karpenko, A., Passos, A.T., Neitz,\nA., Prokofiev, A., Wei, A., Tam, A., Bennett, A., Kumar,\nA., Saraiva, A., Vallone, A., Duberstein, A., Kondrich, A.,\nMishchenko, A., Applebaum, A., Jiang, A., Nair, A., Zoph,\nB., Ghorbani, B., Rossen, B., Sokolowsky, B., Barak, B.,\nMcGrew, B., Minaiev, B., Hao, B., Baker, B., Houghton, B.,\nMcKinzie, B., Eastman, B., Lugaresi, C., Bassin, C., Hud-\nson, C., Li, C.M., de Bourcy, C., Voss, C., Shen, C., Zhang,\nC., Koch, C., Orsinger, C., Hesse, C., Fischer, C., Chan, C.,\nRoberts, D., Kappler, D., Levy, D., Selsam, D., Dohan, D.,\nFarhi, D., Mely, D., Robinson, D., Tsipras, D., Li, D., Oprica,\nD., Freeman, E., Zhang, E., Wong, E., Proehl, E., Cheung,\nE., Mitchell, E., Wallace, E., Ritter, E., Mays, E., Wang, F.,\nSuch, F.P., Raso, F., Leoni, F., Tsimpourlas, F., Song, F.,\nvon Lohmann, F., Sulit, F., Salmon, G., Parascandolo, G.,\nChabot, G., Zhao, G., Brockman, G., Leclerc, G., Salman,\nH., Bao, H., Sheng, H., Andrin, H., Bagherinezhad, H., Ren,\nH., Lightman, H., Chung, H.W., Kivlichan, I., O’Connell,\nI., Osband, I., Gilaberte, I.C., Akkaya, I., Kostrikov, I.,\nSutskever, I., Kofman, I., Pachocki, J., Lennon, J., Wei, J.,\nHarb, J., Twore, J., Feng, J., Yu, J., Weng, J., Tang, J., Yu,\nJ., Candela, J.Q., Palermo, J., Parish, J., Heidecke, J., Hall-\nman, J., Rizzo, J., Gordon, J., Uesato, J., Ward, J., Huizinga,\nJ., Wang, J., Chen, K., Xiao, K., Singhal, K., Nguyen, K.,\nCobbe, K., Shi, K., Wood, K., Rimbach, K., Gu-Lemberg,\nK., Liu, K., Lu, K., Stone, K., Yu, K., Ahmad, L., Yang,\nL., Liu, L., Maksin, L., Ho, L., Fedus, L., Weng, L., Li, L.,\nMcCallum, L., Held, L., Kuhn, L., Kondraciuk, L., Kaiser,\n13\n"}, {"page": 14, "text": "L., Metz, L., Boyd, M., Trebacz, M., Joglekar, M., Chen, M.,\nTintor, M., Meyer, M., Jones, M., Kaufer, M., Schwarzer, M.,\nShah, M., Yatbaz, M., Guan, M.Y., Xu, M., Yan, M., Glaese,\nM., Chen, M., Lampe, M., Malek, M., Wang, M., Fradin,\nM., McClay, M., Pavlov, M., Wang, M., Wang, M., Murati,\nM., Bavarian, M., Rohaninejad, M., McAleese, N., Chowd-\nhury, N., Chowdhury, N., Ryder, N., Tezak, N., Brown, N.,\nNachum, O., Boiko, O., Murk, O., Watkins, O., Chao, P.,\nAshbourne, P., Izmailov, P., Zhokhov, P., Dias, R., Arora,\nR., Lin, R., Lopes, R.G., Gaon, R., Miyara, R., Leike, R.,\nHwang, R., Garg, R., Brown, R., James, R., Shu, R., Cheu,\nR., Greene, R., Jain, S., Altman, S., Toizer, S., Toyer, S., Mis-\nerendino, S., Agarwal, S., Hernandez, S., Baker, S., McKin-\nney, S., Yan, S., Zhao, S., Hu, S., Santurkar, S., Chaudhuri,\nS.R., Zhang, S., Fu, S., Papay, S., Lin, S., Balaji, S., San-\njeev, S., Sidor, S., Broda, T., Clark, A., Wang, T., Gordon, T.,\nSanders, T., Patwardhan, T., Sottiaux, T., Degry, T., Dimson,\nT., Zheng, T., Garipov, T., Stasi, T., Bansal, T., Creech, T.,\nPeterson, T., Eloundou, T., Qi, V., Kosaraju, V., Monaco, V.,\nPong, V., Fomenko, V., Zheng, W., Zhou, W., McCabe, W.,\nZaremba, W., Dubois, Y., Lu, Y., Chen, Y., Cha, Y., Bai, Y.,\nHe, Y., Zhang, Y., Wang, Y., Shao, Z., Li, Z., 2024b. Openai\no1 system card. arXiv:2412.16720.\nOpenAI, :, Jaech, A., Kalai, A., Lerer, A., Richardson, A.,\nEl-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A.,\nCarney, A., Iftimie, A., Karpenko, A., Passos, A.T., Neitz,\nA., Prokofiev, A., Wei, A., Tam, A., Bennett, A., Kumar,\nA., Saraiva, A., Vallone, A., Duberstein, A., Kondrich, A.,\nMishchenko, A., Applebaum, A., Jiang, A., Nair, A., Zoph,\nB., Ghorbani, B., Rossen, B., Sokolowsky, B., Barak, B.,\nMcGrew, B., Minaiev, B., Hao, B., Baker, B., Houghton, B.,\nMcKinzie, B., Eastman, B., Lugaresi, C., Bassin, C., Hud-\nson, C., Li, C.M., de Bourcy, C., Voss, C., Shen, C., Zhang,\nC., Koch, C., Orsinger, C., Hesse, C., Fischer, C., Chan, C.,\nRoberts, D., Kappler, D., Levy, D., Selsam, D., Dohan, D.,\nFarhi, D., Mely, D., Robinson, D., Tsipras, D., Li, D., Oprica,\nD., Freeman, E., Zhang, E., Wong, E., Proehl, E., Cheung,\nE., Mitchell, E., Wallace, E., Ritter, E., Mays, E., Wang, F.,\nSuch, F.P., Raso, F., Leoni, F., Tsimpourlas, F., Song, F.,\nvon Lohmann, F., Sulit, F., Salmon, G., Parascandolo, G.,\nChabot, G., Zhao, G., Brockman, G., Leclerc, G., Salman,\nH., Bao, H., Sheng, H., Andrin, H., Bagherinezhad, H., Ren,\nH., Lightman, H., Chung, H.W., Kivlichan, I., O’Connell,\nI., Osband, I., Gilaberte, I.C., Akkaya, I., Kostrikov, I.,\nSutskever, I., Kofman, I., Pachocki, J., Lennon, J., Wei, J.,\nHarb, J., Twore, J., Feng, J., Yu, J., Weng, J., Tang, J., Yu,\nJ., Candela, J.Q., Palermo, J., Parish, J., Heidecke, J., Hall-\nman, J., Rizzo, J., Gordon, J., Uesato, J., Ward, J., Huizinga,\nJ., Wang, J., Chen, K., Xiao, K., Singhal, K., Nguyen, K.,\nCobbe, K., Shi, K., Wood, K., Rimbach, K., Gu-Lemberg,\nK., Liu, K., Lu, K., Stone, K., Yu, K., Ahmad, L., Yang,\nL., Liu, L., Maksin, L., Ho, L., Fedus, L., Weng, L., Li, L.,\nMcCallum, L., Held, L., Kuhn, L., Kondraciuk, L., Kaiser,\nL., Metz, L., Boyd, M., Trebacz, M., Joglekar, M., Chen, M.,\nTintor, M., Meyer, M., Jones, M., Kaufer, M., Schwarzer, M.,\nShah, M., Yatbaz, M., Guan, M.Y., Xu, M., Yan, M., Glaese,\nM., Chen, M., Lampe, M., Malek, M., Wang, M., Fradin,\nM., McClay, M., Pavlov, M., Wang, M., Wang, M., Murati,\nM., Bavarian, M., Rohaninejad, M., McAleese, N., Chowd-\nhury, N., Chowdhury, N., Ryder, N., Tezak, N., Brown, N.,\nNachum, O., Boiko, O., Murk, O., Watkins, O., Chao, P.,\nAshbourne, P., Izmailov, P., Zhokhov, P., Dias, R., Arora,\nR., Lin, R., Lopes, R.G., Gaon, R., Miyara, R., Leike, R.,\nHwang, R., Garg, R., Brown, R., James, R., Shu, R., Cheu,\nR., Greene, R., Jain, S., Altman, S., Toizer, S., Toyer, S., Mis-\nerendino, S., Agarwal, S., Hernandez, S., Baker, S., McKin-\nney, S., Yan, S., Zhao, S., Hu, S., Santurkar, S., Chaudhuri,\nS.R., Zhang, S., Fu, S., Papay, S., Lin, S., Balaji, S., San-\njeev, S., Sidor, S., Broda, T., Clark, A., Wang, T., Gordon, T.,\nSanders, T., Patwardhan, T., Sottiaux, T., Degry, T., Dimson,\nT., Zheng, T., Garipov, T., Stasi, T., Bansal, T., Creech, T.,\nPeterson, T., Eloundou, T., Qi, V., Kosaraju, V., Monaco, V.,\nPong, V., Fomenko, V., Zheng, W., Zhou, W., McCabe, W.,\nZaremba, W., Dubois, Y., Lu, Y., Chen, Y., Cha, Y., Bai, Y.,\nHe, Y., Zhang, Y., Wang, Y., Shao, Z., Li, Z., 2024c. Openai\no1 system card. arXiv:2412.16720 .\nPal, A., Umapathi, L.K., Sankarasubbu, M., 2022. Medmcqa:\nA large-scale multi-subject multi-choice dataset for medical\ndomain question answering, in: Flores, G., Chen, G.H., Pol-\nlard, T., Ho, J.C., Naumann, T. (Eds.), Proceedings of Ma-\nchine Learning Research, PMLR. pp. 248–260.\nPlaat, A., Wong, A., Verberne, S., Broekens, J., van Stein, N.,\nBack, T., 2025. Multi-step reasoning with large language\nmodels, a survey. arXiv:2407.11511.\nPopovi´c, M., 2015. chrF: character n-gram F-score for auto-\nmatic MT evaluation, in: Bojar, O., Chatterjee, R., Feder-\nmann, C., Haddow, B., Hokamp, C., Huck, M., Logacheva,\nV., Pecina, P. (Eds.), Proceedings of the Tenth Workshop on\nStatistical Machine Translation, pp. 392–395.\nPopovi´c, M., 2017. chrF++: words helping character n-grams,\nin: Bojar, O., Buck, C., Chatterjee, R., Federmann, C., Gra-\nham, Y., Haddow, B., Huck, M., Yepes, A.J., Koehn, P.,\nKreutzer, J. (Eds.), Proceedings of the Second Conference\non Machine Translation, Association for Computational Lin-\nguistics, Copenhagen, Denmark. pp. 612–618.\nQin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye,\nY., Yuan, W., Liu, H., Li, Y., Liu, P., 2024.\nO1 replica-\ntion journey: A strategic progress report - part 1.\nCoRR\nabs/2410.18982.\nQwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu,\nB., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu,\nJ., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K.,\nLu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang,\nP., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren,\nX., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y.,\nCui, Z., Zhang, Z., Qiu, Z., 2025. Qwen2.5 technical report.\narXiv:2412.15115.\nRapp, R., 2009. The backtranslation score: Automatic MT eva-\nlution at the sentence level without reference translations, in:\n14\n"}, {"page": 15, "text": "Su, K.Y., Su, J., Wiebe, J., Li, H. (Eds.), Proceedings of the\nACL-IJCNLP 2009 Conference Short Papers, Suntec, Singa-\npore. pp. 133–136.\nSellergren, A., Kazemzadeh, S., Jaroensri, T., Kiraly, A., Tra-\nverse, M., Kohlberger, T., Xu, S., Jamil, F., Hughes, C., Lau,\nC., Chen, J., Mahvar, F., Yatziv, L., Chen, T., Sterling, B.,\nBaby, S.A., Baby, S.M., Lai, J., Schmidgall, S., Yang, L.,\nChen, K., Bjornsson, P., Reddy, S., Brush, R., Philbrick, K.,\nHu, H., Yang, H., Tiwari, R., Jansen, S., Singh, P., Liu, Y.,\nAzizi, S., Kamath, A., Ferret, J., Pathak, S., Vieillard, N.,\nMerhej, R., Perrin, S., Matejovicova, T., Rame, A., Riv-\niere, M., Rouillard, L., Mesnard, T., Cideron, G., bastien\nGrill, J., Ramos, S., Yvinec, E., Casbon, M., Buchatskaya,\nE., Alayrac, J.B., Lepikhin, D., Feinberg, V., Borgeaud, S.,\nAndreev, A., Hardin, C., Dadashi, R., Hussenot, L., Joulin,\nA., Bachem, O., Matias, Y., Chou, K., Hassidim, A., Goel,\nK., Farabet, C., Barral, J., Warkentin, T., Shlens, J., Fleet,\nD., Cotruta, V., Sanseviero, O., Martins, G., Kirk, P., Rao,\nA., Shetty, S., Steiner, D.F., Kirmizibayrak, C., Pilgrim, R.,\nGolden, D., Yang, L., 2025. Medgemma technical report.\narXiv:2507.05201.\nSennrich, R., Haddow, B., Birch, A., 2016. Improving neu-\nral machine translation models with monolingual data, in:\nErk, K., Smith, N.A. (Eds.), Proceedings of the 54th An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), Berlin, Germany. pp. 86–96.\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S.S., Wei, J., Chung,\nH.W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S.,\nPayne, P., Seneviratne, M., Gamble, P., Kelly, C., Babiker,\nA., Schärli, N., Chowdhery, A., Mansfield, P., Demner-\nFushman, D., y Arcas, B.A., Webster, D., Corrado, G.S.,\nMatias, Y., Chou, K., Gottweis, J., Tomasev, N., Liu, Y., Ra-\njkomar, A., Barral, J., Semturs, C., Karthikesalingam, A.,\nNatarajan, V., 2023. Large language models encode clinical\nknowledge. Nature 620, 172–180.\nSinghal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Amin,\nM., Hou, L., Clark, K., Pfohl, S.R., Cole-Lewis, H., Neal, D.,\nRashid, Q.M., Schaekermann, M., Wang, A., Dash, D., Chen,\nJ.H., Shah, N.H., Lachgar, S., Mansfield, P.A., Prakash, S.,\nGreen, B., Dominowska, E., Agüera y Arcas, B., Tomašev,\nN., Liu, Y., Wong, R., Semturs, C., Mahdavi, S.S., Barral,\nJ.K., Webster, D.R., Corrado, G.S., Matias, Y., Azizi, S.,\nKarthikesalingam, A., Natarajan, V., 2025. Toward expert-\nlevel medical question answering with large language mod-\nels.\nSmith, D.A., 2020. Situating wikipedia as a health information\nresource in various contexts: A scoping review. PloS one 15,\ne0228786.\nSugiyama, A., Yoshinaga, N., 2019.\nData augmentation us-\ning back-translation for context-aware neural machine trans-\nlation, in: Popescu-Belis, A., Loáiciga, S., Hardmeier, C.,\nXiong, D. (Eds.), Proceedings of the Fourth Workshop on\nDiscourse in Machine Translation (DiscoMT 2019), Hong\nKong, China. pp. 35–44.\nSun, Y., Qian, X., Xu, W., Zhang, H., Xiao, C., Li, L., Zhao,\nD., Huang, W., Xu, T., Bai, Q., Rong, Y., 2025. ReasonMed:\nA 370K multi-agent generated dataset for advancing medi-\ncal reasoning, in: Christodoulopoulos, C., Chakraborty, T.,\nRose, C., Peng, V. (Eds.), Proceedings of the 2025 Confer-\nence on Empirical Methods in Natural Language Processing,\nSuzhou, China. pp. 26457–26478.\nTeam, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Mer-\nhej, R., Perrin, S., Matejovicova, T., Rame, A., Riviere, M.,\nRouillard, L., Mesnard, T., Cideron, G., bastien Grill, J.,\nRamos, S., Yvinec, E., Casbon, M., Pot, E., Penchev, I.,\nLiu, G., Visin, F., Kenealy, K., Beyer, L., Zhai, X., Tsit-\nsulin, A., Busa-Fekete, R., Feng, A., Sachdeva, N., Cole-\nman, B., Gao, Y., Mustafa, B., Barr, I., Parisotto, E., Tian,\nD., Eyal, M., Cherry, C., Peter, J.T., Sinopalnikov, D., Bhu-\npatiraju, S., Agarwal, R., Kazemi, M., Malkin, D., Kumar,\nR., Vilar, D., Brusilovsky, I., Luo, J., Steiner, A., Friesen,\nA., Sharma, A., Sharma, A., Gilady, A.M., Goedeckemeyer,\nA., Saade, A., Feng, A., Kolesnikov, A., Bendebury, A.,\nAbdagic, A., Vadi, A., Gyorgy, A., Pinto, A.S., Das, A.,\nBapna, A., Miech, A., Yang, A., Paterson, A., Shenoy, A.,\nChakrabarti, A., Piot, B., Wu, B., Shahriari, B., Petrini,\nB., Chen, C., Lan, C.L., Choquette-Choo, C.A., Carey, C.,\nBrick, C., Deutsch, D., Eisenbud, D., Cattle, D., Cheng,\nD., Paparas, D., Sreepathihalli, D.S., Reid, D., Tran, D.,\nZelle, D., Noland, E., Huizenga, E., Kharitonov, E., Liu,\nF., Amirkhanyan, G., Cameron, G., Hashemi, H., Klimczak-\nPlucinska, H., Singh, H., Mehta, H., Lehri, H.T., Hazimeh,\nH., Ballantyne, I., Szpektor, I., Nardini, I., Pouget-Abadie,\nJ., Chan, J., Stanton, J., Wieting, J., Lai, J., Orbay, J., Fer-\nnandez, J., Newlan, J., yeong Ji, J., Singh, J., Black, K.,\nYu, K., Hui, K., Vodrahalli, K., Greff, K., Qiu, L., Valen-\ntine, M., Coelho, M., Ritter, M., Hoffman, M., Watson, M.,\nChaturvedi, M., Moynihan, M., Ma, M., Babar, N., Noy, N.,\nByrd, N., Roy, N., Momchev, N., Chauhan, N., Sachdeva, N.,\nBunyan, O., Botarda, P., Caron, P., Rubenstein, P.K., Culli-\nton, P., Schmid, P., Sessa, P.G., Xu, P., Stanczyk, P., Tafti,\nP., Shivanna, R., Wu, R., Pan, R., Rokni, R., Willoughby,\nR., Vallu, R., Mullins, R., Jerome, S., Smoot, S., Girgin,\nS., Iqbal, S., Reddy, S., Sheth, S., Poder, S., Bhatnagar, S.,\nPanyam, S.R., Eiger, S., Zhang, S., Liu, T., Yacovone, T.,\nLiechty, T., Kalra, U., Evci, U., Misra, V., Roseberry, V.,\nFeinberg, V., Kolesnikov, V., Han, W., Kwon, W., Chen, X.,\nChow, Y., Zhu, Y., Wei, Z., Egyed, Z., Cotruta, V., Giang,\nM., Kirk, P., Rao, A., Black, K., Babar, N., Lo, J., Moreira,\nE., Martins, L.G., Sanseviero, O., Gonzalez, L., Gleicher, Z.,\nWarkentin, T., Mirrokni, V., Senter, E., Collins, E., Barral, J.,\nGhahramani, Z., Hadsell, R., Matias, Y., Sculley, D., Petrov,\nS., Fiedel, N., Shazeer, N., Vinyals, O., Dean, J., Hassabis,\nD., Kavukcuoglu, K., Farabet, C., Buchatskaya, E., Alayrac,\nJ.B., Anil, R., Dmitry, Lepikhin, Borgeaud, S., Bachem, O.,\nJoulin, A., Andreev, A., Hardin, C., Dadashi, R., Hussenot,\nL., 2025. Gemma 3 technical report. arXiv:2503.19786.\n15\n"}, {"page": 16, "text": "Team, Q., 2025. Qwq-32b: Embracing the power of reinforce-\nment learning.\nThapa, R., Wu, Q., Wu, K., Zhang, H., Zhang, A., Wu, E.,\nYe, H., Bedi, S., Aresh, N., Boen, J., Reddy, S., Athi-\nwaratkun, B., Song, S.L., Zou, J., 2025. Disentangling rea-\nsoning and knowledge in medical large language models.\narXiv:2505.11462.\nWang, W., Ma, Z., Ding, M., Zheng, S., Liu, S., Liu, J., Ji, J.,\nChen, W., Li, X., Shen, L., Yuan, Y., 2025. Medical reason-\ning in the era of llms: A systematic review of enhancement\ntechniques and applications. arXiv:2508.00669.\nWang, X., Wang, Z., Gao, X., Zhang, F., Wu, Y., Xu, Z., Shi,\nT., Wang, Z., Li, S., Qian, Q., Yin, R., Lv, C., Zheng, X.,\nHuang, X., 2024. Searching for best practices in retrieval-\naugmented generation, in: Al-Onaizan, Y., Bansal, M., Chen,\nY.N. (Eds.), Proceedings of the 2024 Conference on Empir-\nical Methods in Natural Language Processing, pp. 17716–\n17736.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia,\nF., Chi, E., Le, Q.V., Zhou, D., 2022.\nChain-of-thought\nprompting elicits reasoning in large language models, in:\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho,\nK., Oh, A. (Eds.), Advances in Neural Information Process-\ning Systems, Curran Associates, Inc.. pp. 24824–24837.\nWu, C., Lin, W., Zhang, X., Zhang, Y., Xie, W., Wang, Y., 2024.\nPmc-llama: toward building open-source language models\nfor medicine. Journal of the American Medical Informatics\nAssociation 31, 1833–1843.\nWu, J., Deng, W., Li, X., Liu, S., Mi, T., Peng, Y., Xu, Z., Liu,\nY., Cho, H., Choi, C.I., Cao, Y., Ren, H., Li, X., Li, X., Zhou,\nY., 2025. Medreason: Eliciting factual medical reasoning\nsteps in llms via knowledge graphs. arXiv:2504.00993.\nXu, X., Xu, Q., Xiao, T., Chen, T., Yan, Y., ZHANG, J., Diao,\nS., Yang, C., Wang, Y., 2025. UGPhysics: A comprehensive\nbenchmark for undergraduate physics reasoning with large\nlanguage models, in: Forty-second International Conference\non Machine Learning.\nYang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu,\nB., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F.,\nHuang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J.,\nTu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin,\nJ., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M.,\nXue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao,\nR., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang,\nX., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y.,\nWan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., Qiu,\nZ., 2025. Qwen3 technical report. arXiv:2505.09388.\nYu, H., Cheng, T., Wang, Y., He, W., Wang, Q., Cheng, Y.,\nZhang, Y., Feng, R., Zhang, X., 2025. FinemedLM-o1: En-\nhancing medical knowledge reasoning ability of LLM from\nsupervised fine-tuning to test-time training, in: Second Con-\nference on Language Modeling.\nZeng, Z., Cheng, Q., Yin, Z., Wang, B., Li, S., Zhou, Y., Guo,\nQ., Huang, X., Qiu, X., 2024. Scaling of search and learn-\ning: A roadmap to reproduce o1 from reinforcement learning\nperspective. arXiv:2412.14135.\nZhang, D., Hu, Z., Zhoubian, S., Du, Z., Yang, K., Wang,\nZ., Yue, Y., Dong, Y., Tang, J., 2024. Sciinstruct: a self-\nreflective instruction annotated dataset for training scientific\nlanguage models, in: Proceedings of the 38th International\nConference on Neural Information Processing Systems, Cur-\nran Associates Inc.\nZhang, D., Li, Z.Z., Zhang, M.L., Zhang, J., Liu, Z., Yao, Y.,\nXu, H., Zheng, J., Chen, X., Zhang, Y., Yin, F., Dong, J.,\nGuo, Z., Song, L., Liu, C.L., 2025a. From system 1 to system\n2: A survey of reasoning large language models. IEEE Trans.\nPattern Anal. Mach. Intell. PP, 1–20.\nZhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.,\n2020. Bertscore: Evaluating text generation with bert, in:\nInternational Conference on Learning Representations.\nZhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B., Xie,\nP., Yang, A., Liu, D., Lin, J., Huang, F., Zhou, J., 2025b.\nQwen3 embedding: Advancing text embedding and rerank-\ning through foundation models. arXiv:2506.05176.\nZhao, Y., Yin, H., Zeng, B., Wang, H., Shi, T., Lyu, C.,\nWang, L., Luo, W., Zhang, K., 2024.\nMarco-o1:\nTo-\nwards open reasoning models for open-ended solutions.\narXiv:2411.14405.\nZhuo, T.Y., Xu, Q., He, X., Cohn, T., 2023.\nRethinking\nround-trip translation for machine translation evaluation, in:\nRogers, A., Boyd-Graber, J., Okazaki, N. (Eds.), Findings of\nthe Association for Computational Linguistics: ACL 2023,\nToronto, Canada. pp. 319–337.\nAppendix A. Appendix\nAppendix A.1. Prompts\nPrompt template for automatic translation. The automatic trans-\nlation is performed in 5-shot. Here we report the prompt and the\nexamples provided for each Language.\nSystem prompt:\nYou are a helpful assistant that translates text\nfrom {source_language} to {target_language}.\nYou must translate the text without adding\nany additional information or explanations.\nNever add any additional information or\nexplanations.\nUser prompt:\nTranslate the following text to {target_language}.\nReturn only the translated text.\\n{text}\\n\nExamples:\n16\n"}, {"page": 17, "text": "Figure A.6: Length distribution of the retrieved context per question versus\nthe LLM-formatted context. The formatted context is notably shorter than the\noriginal one.\ninput: \"Growth hormone has its effect on growth\nthrough?\"\noutput it: \"L’ormone della crescita esercita il\nsuo effetto sulla crescita attraverso?\"\noutput es: \"La hormona del crecimiento tiene su\nefecto sobre el crecimiento a través de?\"\ninput: \"What is the main function of the liver?\"\noutput it:\n\"Qual è la funzione principale del\nfegato?\"\noutput es: \"Cuál es la función principal del\nhígado?\"\ninput: \"Thyroxine\"\noutput it: \"Tiroxina\"\noutput es: \"Tiroxina\"\ninput: \"IG1-1\"\noutput it: \"IG1-1\"\noutput es: \"IG1-1\"\ninput: \"Duodenal Switch\"\noutput it: \"Duodenal Switch\"\noutput es: \"Switch Duodenal\"\nPrompt template for context rewriting.\nYou are a helpful assistant that rewrites\nmedical text to be more clear and concise.\nYour task is to rearrange the text to improve\nits clarity and coherence.\nReturn the rewritten text without saying\nanything else.\"\nThe formatting of the context produced the results reported in Fig-\nure A.6.\nPrompt template for reasoning trace generation. System prompt:\nYou are a medical expert that reason about\nmedical questions given some provided\ncontext.\nUser prompt:\n<question>{question}</question>\n<context>{context}</context>\nPlease respond to the above question\n<question> using the Chain of Thought (CoT)\nreasoning method considering the information\nin the <context>.\nYou evaluate and reason on this options and\ndetermine a path to the correct answer.\n<possible answers>\n{options}\n</possible answers>\nThe correct answer is <answer>{answer}\n</answer>.\n1. Explore the question. If any of the\npossible answers paths seem useful,\nincorporate them naturally as if you\ndiscovered them yourself.\n2. Analyze and evaluate the <possible\nanswers> carefully, expanding on the most\nrelevant ones to construct a logical,\nwell-supported explanation.\n3. Do not mention the existence of\n<context>\nnor provided <answer> in your response.\n4. Do not assume the given <answer> is\ncorrect.\nInstead, determine the answer solely\nbased on your reasoning.\n5. If your final conclusion contradicts\nthe given <answer>, acknowledge potential\ndiscrepancies (e.g., \"Wait, there might\nbe something wrong) and refine your\nresponse accordingly.\n6. The answer should just report the\nanswer’s number/identifier\n### Output:\nSearch for potential reasoning traces:\n(Find potentially useful reasoning traces.\nIt must be a concise list of traces)\nReasoning process:\n(Step-by-step reasoning process)\nConclusion:\n(The final answer resulting from your\nreasoning.\nMust be the identifier of the correct option,\ne.g. \"1\", \"2\", \"3\", \"4\" o \"5\".)\nPrompt for 2-shot inference (Baseline). System prompt:\nYou are an {language} Al assistant that helps people\nfind information.\nUser prompt:\nYou are an expert in the medical domain.\nGiven a medical <Question> and <Possible answers>,\nyour task is to select the correct answer.\nYou must return the answer identifier. Do not provide\nany reasoning or explanation.\nQuestion: {question}\nPossible Answers: {options}\n17\n"}, {"page": 18, "text": "Prompt for 2-shot inference with reasoning traces. System prompt:\nYou are an {language} Al assistant that helps people\nfind information.\nFor the user prompt, here we report the Italian version. We found\nthat providing the output formatting instructions in the target language\nenforces the generation in such a language. The Spanish and English\nprompts are the minimal adaptation of the following:\nYou are an expert in the medical domain.\nGiven a medical <Question> and <Possible answers>,\nyour task is to reason step by step as if you are\nindependently determining the possible reasoning\npaths and deriving the correct answer.\n1. Explore the question and generate a reasoning\npath to answer it.\n2. You consider and reason on the <Possible answers>\nto determine a path to the correct answer.\n3. Analyze and evaluate the <Possible answers>\ncarefully, expanding on the most relevant ones to\nconstruct a logical, well-supported explanation.\n4. Generate your own reasoning approach based on\nyour expertise.\n### Input:\nQuestion: {question}\nPossible Answers: {options}\n### Output:\nRicerca di potenziali tracce di ragionamento:\n(trovi tracce di ragionamento potenzialmente utili.\nDeve essere una lista concisa di tracce)\nProcesso di ragionamento:\n(processo di ragionamento fatto step-by-step.)\nConclusione:\n(La risposta finale che deriva dal tuo reasoning.\nDeve essere il numero dell’opzione corretta, ad\nesempio \"1\", \"2\", \"3\", \"4\" o \"5\".)\nYour answer must be in Italian.\nPrompt for inference with SOTA models. Huatuo, MedReason, and\nm1 need to be prompted to generate the reasoning in the target lan-\nguage. Otherwise, they generate in English no matter what language\nthe question is. Therefore, we designed these minimalist prompts.\nItalian\nRispondi alla seguente domanda a scelta\nmultipla in italiano:\\n{question}\\n\n{option_str}. \\nLa risposta deve essere\nin italiano!\nSpanish\nPor favor, responda a la siguiente\npregunta de opción múltiple en\nespañol:\\n{question}\\n\n{option_str}. \\n¡La respuesta debe\nestar en español!\nAppendix A.2. Translation quality\nIn Tables A.6 and A.7 we report the scores for back-translation\ncalculated for each dataset split. They are divided into \"options\" and\n\"questions\", which are the two parts of the datasets we had to translate.\nAppendix B. Wikipedia Sections excluded by the KB\nHere is the list of Wikipedia sections that have been removed from\neach page before creating the KB. The scope is to drop unrelevant and\nnoisy sections.\n’it’ : [’Note’, ’Altri progetti’, ’Voci correlate’, ’Collegamenti es-\nterni’, ’Bibliografia’, ’Nella cultura di massa’, ’Storia’], ’es’: [’Ref-\nerencias’, ’Referencia’, ’Véase también’, ’Bibliografía complemen-\ntaria’, ’Lectura adicional’, ’Referencias y notas de pie’, ’Enlaces ex-\nternos’, ’Historia’, ’Bibliografía’, ’Notas’, ’En la cultura popular’,\n’Otras lecturas’, ’Ver también’, ’Bibliografía adicional’], ’en’: [’Ref-\nerences’, ’Additional bibliography’, ’References and footnotes’, ’Ex-\nternal links’, ’See also’, ’History’, ’Notes’, ’Bibliography’, ’Cultural\nreferences’, ’Related articles’, ’Related topics’, ’Literature’, ’Society\nand culture’, ’Further reading’]\nAppendix C. Comparison between different traces types\nTable C.8 reports results on the impact of different types of traces\non eleven open source models. Models are exposed to the traces via\ntwo-shot examples, where each example is composed of a question,\nthe options, and the trace. The two examples are selected via similarity\nsearch in an embedding space, comparing the testing question-options\npair against all the training question-options pairs.\nTable C.9 reports results on the impact of different types of traces on\nQwen3-8B and Llama-3.1-8B-Instruct at fine-tuning-time.\nAppendix D. Error analysis\nHere we report three examples of the qualitative analysis we per-\nformed, one for each language. Each example is provided together\nwith the expert comment on it.\nAppendix D.1. English\nQUESTION:\nGustavo comes to the emergency room with skin lesions and gen-\neral malaise of several days of evolution. He has psoriasiform lesions\non the trunk with involvement of palms and soles. He also presents\nasymmetric non-suppurative joint inflammation and bilateral ocular\nredness as well as erosions on the glans penis. In the subsequent anam-\nnesis Gustavo recognizes a risky sexual contact 20 days before. What\nis his diagnosis?\nOPTIONS:\n’1’: ’HIV infection.’, ’2’: ’Secondary syphilis.’, ’3’: \"Reiter’s syn-\ndrome.\", ’4’: ’Erythema multiforme.’, ’5’: None\nCORRECT ANSWER:\n3\nMODEL ANSWER:\nSearch for potential reasoning traces: - Psoriasiform lesions on\nthe trunk with involvement of palms and soles. - Asymmetric non-\nsuppurative joint inflammation. - Bilateral ocular redness. - Erosions\non the glans penis. - History of risky sexual contact 20 days before.\n- These features suggest a sexually transmitted infection with a reac-\ntive or disseminated phase. - Secondary syphilis is known to present\nwith a rash, mucocutaneous lesions, and systemic symptoms. - Reiter’s\nsyndrome is a reactive arthritis that follows a genitourinary infection\nbut does not typically present with psoriasiform lesions or mucocuta-\nneous erosions. - Erythema multiforme is a hypersensitivity reaction\nand does not typically involve the glans penis or joints. - HIV infection\nmay present with systemic symptoms but does not typically include the\ndescribed mucocutaneous and ocular features.\n18\n"}, {"page": 19, "text": "Options\nQuestions\nSplit\nBERT COMET chrF chrF++ BERT COMET chrF chrF++ AVG\nMedMCQA Test\n96.9\n92.6\n80.3\n78.5\n95.5\n88.3\n76.9\n74.5\n85.4\nMedMCQA Train\n96.9\n92.8\n80.5\n78.5\n95.9\n88.6\n76.3\n73.9\n85.4\nMedMCQA Val\n96.8\n92.0\n78.6\n76.8\n96.0\n88.0\n75.7\n73.4\n84.7\nMedQA Test\n97.7\n94.1\n84.9\n83.5\n95.7\n80.4\n71.1\n70.0\n84.7\nMedQA Train\n97.7\n94.1\n84.7\n83.3\n95.7\n79.9\n71.0\n69.8\n84.5\nMedQA Val\n97.7\n94.1\n84.6\n83.1\n96.0\n80.9\n72.7\n71.5\n85.1\nMicro AVG\n97.0\n93.0\n82.0\n81.0\n96.0\n84.0\n74.0\n72.0\nTable A.6: Back-translation scores on Italian MedMCQA and MedQA (options and questions). Reported scores are BERTScore, COMET, chrF, and chrF++. The\nper-column average is weighted by the number of examples per dataset split.\nOptions\nQuestions\nSplit\nBERT COMET chrF chrF++ BERT COMET chrF chrF++ AVG\nMedMCQA Test\n97.3\n93.7\n82.3\n80.5\n95.6\n88.9\n79.0\n76.5\n86.7\nMedMCQA Train\n97.2\n93.7\n82.6\n80.7\n96.2\n89.3\n80.0\n77.6\n87.2\nMedMCQA Val\n97.2\n93.1\n80.8\n79.1\n96.2\n88.6\n78.1\n75.8\n86.1\nMedQA Test\n98.0\n94.9\n87.0\n85.7\n97.4\n83.7\n81.2\n80.3\n88.5\nMedQA Train\n98.0\n94.7\n86.6\n85.3\n97.5\n83.9\n82.1\n81.1\n88.7\nMedQA Val\n98.1\n94.8\n86.7\n85.3\n97.6\n84.4\n82.8\n81.9\n89.0\nMicro AVG\n98.0\n94.0\n84.0\n83.0\n97.0\n86.0\n80.0\n79.0\nTable A.7: Back-translation scores on Spanish MedMCQA and MedQA (options and questions). Reported scores are BERTScore, COMET, chrF, and chrF++. The\nper-column average is weighted by the number of examples per dataset split.\n19\n"}, {"page": 20, "text": "model\nMedExpQA\nMedMCQA\nMedQA\nIT\nEN\nES\nIT\nEN\nES\nIT\nEN\nES\nLlama-3.2-1B-Instruct\n23.7\n34.2\n27.3\n30.8\n36.5\n31.1\n27.3\n33.3\n28.5\n+ m23k\n-2.9\n-1.4\n-8.1\n-1.3\n+0.4\n-0.5\n-0.5\n+5.0\n+0.2\n+ MedReason\n-12.5\n-11.0\n-8.1\n-12.1\n-7.1\n-8.7\n-10.4\n-1.0\n-8.8\n+ OUR\n+2.6\n-1.2\n+0.0\n+0.6\n+3.8\n+2.6\n+0.1\n+2.5\n+3.5\nQwen3-1.7B\n37.0\n48.3\n37.0\n34.5\n37.4\n35.4\n35.5\n40.5\n34.9\n+ m23k\n+15.0\n+9.3\n+15.8\n+9.1\n+9.9\n+9.2\n+16.3\n+16.7\n+18.0\n+ MedReason\n-0.2\n+4.5\n+11.0\n+7.5\n+11.7\n+8.0\n+14.9\n+19.3\n+18.1\n+ OUR\n+8.1\n+3.6\n+4.7\n+16.9\n+19.8\n+18.0\n+11.3\n+16.7\n+13.4\ngemma-3-4b-it\n49.7\n50.7\n55.6\n42.7\n46.8\n42.8\n43.5\n49.5\n44.5\n+ m23k\n-4.1\n-0.3\n-7.6\n+6.6\n+5.1\n+6.5\n+6.5\n+9.1\n+6.9\n+ MedReason\n+5.5\n+6.9\n-5.2\n+4.1\n+5.5\n+6.0\n+10.8\n+12.3\n+11.8\n+ OUR\n-1.9\n+11.8\n-3.1\n+11.4\n+11.9\n+12.3\n+7.2\n+13.4\n+9.1\nmedgemma-4b-it\n56.5\n58.8\n53.6\n46.6\n51.8\n46.7\n50.5\n55.5\n49.9\n+ m23k\n-6.1\n+4.4\n+5.6\n+8.6\n+7.9\n+8.0\n+8.8\n+11.2\n+10.2\n+ MedReason\n-3.7\n-17.2\n-8.8\n-0.9\n-8.1\n-2.9\n+4.8\n-3.8\n+4.7\n+ OUR\n-2.7\n+3.1\n+5.6\n+9.9\n+10.7\n+11.1\n+4.6\n+11.8\n+7.1\nLlama-3.1-8B-Instruct\n49.2\n63.0\n47.9\n46.4\n57.5\n47.9\n50.4\n63.1\n49.7\n+ m23k\n-2.8\n+0.2\n+4.9\n+2.7\n+2.1\n+4.8\n+2.4\n+5.7\n+5.7\n+ MedReason\n+4.4\n-1.4\n+12.9\n+1.4\n+1.1\n+2.3\n+7.3\n+5.6\n+8.6\n+ OUR\n+3.2\n+6.4\n+2.8\n+9.4\n+6.0\n+10.1\n+3.0\n+7.8\n+7.6\nQwen3-8B\n60.1\n66.0\n64.8\n53.0\n58.6\n53.8\n57.0\n67.1\n60.1\n+ m23k\n-0.1\n+2.0\n-3.2\n+5.5\n+4.4\n+4.5\n+5.4\n+3.8\n+4.9\n+ MedReason\n+3.9\n+5.2\n+0.8\n+3.7\n+3.9\n+4.6\n+11.8\n+10.3\n+9.7\n+ OUR\n+4.6\n+8.6\n+0.0\n+9.3\n+8.8\n+10.4\n+10.5\n+8.5\n+6.4\ngemma-3-27b-it\n81.9\n79.7\n82.2\n57.9\n62.8\n59.4\n69.2\n72.9\n68.9\n+ m23k\n-5.9\n-3.7\n-7.8\n+2.7\n+5.0\n+5.3\n+7.1\n+5.4\n+6.2\n+ MedReason\n-7.5\n-5.3\n-6.2\n+6.4\n+7.5\n+6.8\n+4.2\n+4.5\n+3.7\n+ OUR\n-7.0\n-0.3\n-4.5\n+6.0\n+6.9\n+5.5\n+9.4\n+8.2\n+8.9\nmedgemma-27b-text-it\n80.2\n80.1\n84.6\n61.7\n68.3\n62.7\n72.8\n77.7\n72.7\n+ m23k\n-1.8\n-7.3\n-6.2\n+5.9\n+2.2\n+6.3\n+5.7\n+0.3\n+4.9\n+ MedReason\n-5.8\n-0.9\n-8.6\n+6.7\n+9.5\n+2.7\n+1.1\n+4.2\n+1.2\n+ OUR\n-3.4\n+4.5\n-2.7\n+9.6\n+9.5\n+8.8\n+8.7\n+7.7\n+9.8\nQwen3-32B\n77.4\n82.7\n81.1\n63.9\n69.0\n65.1\n72.3\n76.5\n73.9\n+ m23k\n-4.6\n+2.1\n-1.9\n+3.3\n+3.9\n+2.8\n+3.6\n+2.5\n+2.9\n+ MedReason\n+1.0\n-4.3\n+2.1\n+4.9\n+7.0\n+6.2\n+1.8\n+1.9\n+0.6\n+ OUR\n+0.2\n-2.1\n+0.8\n+4.1\n+7.8\n+4.5\n+5.7\n+5.8\n+5.7\nLlama-3.3-70B-Instruct\n74.0\n82.0\n77.0\n65.3\n71.8\n66.4\n65.3\n71.8\n66.4\n+ m23k\n+3.6\n+0.4\n-4.2\n+11.8\n+10.6\n+10.4\n+2.4\n+1.1\n+2.5\n+ MedReason\n+5.2\n+0.4\n+7.0\n+14.1\n+14.8\n+14.8\n+2.6\n+2.3\n+3.1\n+ OUR\n+4.9\n+1.8\n+5.2\n+14.7\n+13.7\n+13.7\n+5.7\n+4.2\n+6.9\nTable C.8: Performances of baseline models with deltas produced by exposure to to different types of reasoning traces at inference time. Baselines are models\nprompted via 2-shot examples, each example being a question-options-answer triplet, while the deltas are obtained by prompting the same models via 2-shot\nexamples, each example being a triplet question-options-trace triplet.\n20\n"}, {"page": 21, "text": "Model\nMedExpQA\nMedMCQA\nMedQA\nIT\nEN\nES\nIT\nEN\nES\nIT\nEN\nES\nLlama-3.1-8B-Instruct 48.9 62.8\n47.9\n46.4 57.5 47.9 50.4\n63.1\n49.7\n+ m23k FT\n+4.0 +4.3 +10.3 +6.8 +3.7 +3.1 +5.9 +10.0 +12.0\n+ MedReason FT\n+3.6 -0.7\n+5.5\n-1.1 -3.9 -0.6 +0.2 +0.5\n+5.2\n+ Huatuo FT\n-5.2 +0.1 +0.0\n-2.3 -0.3 -0.8 -2.0\n+3.8\n+1.3\n+ OUR FT\n+9.1 +2.6 +6.4 +0.4 +1.5 +2.7 +3.6 +8.5\n+9.4\nQwen3-8B\n60.1 66.0\n64.9\n53.0 58.6 53.8 57.0\n67.1\n60.1\n+ m23k FT\n-1.7 -2.1\n-5.7\n-3.0 -2.0 -2.7 -3.4\n-1.9\n-3.9\n+ MedReason FT\n+0.8 -1.2\n-1.9\n+0.1 +0.0 -0.7 +1.6 +0.5\n-0.2\n+ Huatuo FT\n+4.2 +3.2 +2.1 +3.4 +0.3 +2.2 +8.2 +6.0\n+4.2\n+ OUR FT\n+2.1 +3.7 +4.8\n-0.7 +2.3 -0.6 +4.7 +4.6\n+4.9\nTable C.9: Performances of baseline models with deltas produced by exposure to to different types of reasoning traces at fine-tuning time. Baselines are models\nprompted via 2-shot examples, each example being a question-options-answer triplet, while the deltas are obtained by fine-tuning the same models on the dataset of\ntraces.\nMedExpQA\nMedMCQA\nMedQA\nAVG\nfamily\nsize IT\nEN\nES\nIT\nEN\nES\nIT\nEN\nES\nHuatuoGPT\n8B\n56.4 67.0 56.3 51.1 64.1 54.4 57.4 75.6 58.3 60.1\nMedReason\n8B\n55.1 64.3 58.4 49.5 61.1 50.3 54.8 70.9 62.0 58.5\nm1\n7B\n48.6 62.2 48.1 47.7 59.3 48.6 50.8 66.9 51.5 53.7\nOur\n8B\n62.2 69.7 69.7 52.3 60.9 53.2 61.7 71.7 65.0 63.0\nOur + 2-shot 8B\n64.0 68.1 65.0 61.9 67.6 63.8 68.1 76.7 69.1 67.1\nTable C.10: Performance (accuracy) of best performing models on MedExpQA, MedMCQA, and MedQA in Italian, English and Spanish. Our proposed model\nachieves the best performances.\nReasoning process: 1. Gustavo presents with a combination of\nskin lesions, joint inflammation, ocular redness, and mucocutaneous\nerosions. 2. The psoriasiform rash on the trunk and palms/soles is a\nkey feature of secondary syphilis. 3. The mucocutaneous erosion on\nthe glans penis is also consistent with secondary syphilis. 4. The bilat-\neral ocular redness and asymmetric joint inflammation are additional\nfeatures of secondary syphilis. 5. The history of a risky sexual con-\ntact 20 days before is consistent with the incubation period of syphilis.\n6. Reiter’s syndrome is a reactive arthritis that follows a genitourinary\ninfection but does not typically present with psoriasiform lesions or\nmucocutaneous erosions. 7. Erythema multiforme is a hypersensitiv-\nity reaction and does not typically involve the glans penis or joints.\n8. HIV infection may present with systemic symptoms but does not\ntypically include the described mucocutaneous and ocular features.\nConclusion: 2\nExpert Comment. The model makes a critical medical knowl-\nedge error regarding Reiter’s syndrome. It incorrectly states\nthat Reiter’s does not typically present with psoriasiform le-\nsions or mucocutaneous erosions. In fact, these features are\nclassic characteristics of the syndrome (known as keratoderma\nblennorrhagicum and circinate balanitis). This fundamental mis-\ntake led the model to incorrectly rule out the correct diagnosis.\nAppendix D.2. Italian\nQUESTION: Un uomo di 30 anni è caduto in piedi da un’altezza di\n2 metri. All’arrivo al Pronto Soccorso era supino e aveva cateterizzato\nla vescica con urine chiare. Presenta un gonfiore al tallone sinistro, un\nleggero formicolio alla parte anteriore della coscia destra e allo scroto\ne disestesia a entrambi i piedi. Il paziente saluta il barelliere con una\nstretta di mano e una contemporanea pacca sulla spalla. La rotazione\ndel MMII sulla barella afferrata dalle caviglie è indolore e la compres-\nsione del bacino è asintomatica. Il prossimo:\nOPZIONI:\n’1’: ’Il paziente viene fatto sedere sulla barella in modo da poter\nesplorare il rachide cervicale.’, ’2’: ’Fare riferimento alla Rx per lo stu-\ndio degli arti inferiori.’, ’3’: \"Palpare l’addome e ruotarlo in blocco per\npalpare la colonna vertebrale toracolombare.\", ’4’: ’Il paziente verrà\nsottoposto a un collare cervicale e inviato in radiologia per la diag-\nnostica per immagini del rachide cervicale e degli arti inferiori.’, ’5’:\n\"Verrà richiesta un’ecografia addominale.\"\nRISPOSTA CORRETTA:\n3\nRISPOSTA DEL MODELLO:\n21\n"}, {"page": 22, "text": "**Ricerca di potenziali tracce di ragionamento:** - Il paziente ha\nsubito una caduta da 2 metri in piedi, con impatto potenzialmente trau-\nmatico. - All’arrivo al Pronto Soccorso presenta sintomi neurologici:\nformicolio alla coscia destra, scroto e disestesia ai piedi. - La stretta\ndi mano e la pacca sulla spalla suggeriscono una funzionalità motoria\nconservata, ma non esclude una lesione spinale. - Il gonfiore al tal-\nlone sinistro potrebbe indicare una frattura o una contusione, ma non\nspiega i sintomi neurologici. - La manovra di rotazione del MMII e\nla compressione del bacino sono asintomatiche, suggerendo una pos-\nsibile assenza di lesione spinale a livello lombare o sacrale. - I sintomi\nneurologici (formicolio, disestesia) suggeriscono una possibile lesione\nspinale a livello toracico o cervicale. - La manovra di rotazione del\nMMII e la compressione del bacino non sono sufficienti per escludere\nuna lesione spinale. - È necessario un esame neurologico più appro-\nfondito, iniziando con la valutazione del rachide cervicale.\n**Processo di ragionamento:** 1. Il paziente ha subito una caduta\nda 2 metri in piedi, un evento che può causare lesioni spinali, special-\nmente se l’atterraggio non è stato attutito. 2. I sintomi neurologici\n(formicolio, disestesia) indicano una possibile lesione spinale, in par-\nticolare a livello toracico o cervicale. 3. La stretta di mano e la pacca\nsulla spalla suggeriscono una funzionalità motoria conservata, ma non\nescludono una lesione spinale. 4. La manovra di rotazione del MMII e\nla compressione del bacino sono asintomatiche, suggerendo una pos-\nsibile assenza di lesione spinale a livello lombare o sacrale. 5. Per\nescludere una lesione spinale a livello cervicale, è necessario effet-\ntuare una valutazione neurologica iniziale, iniziando con la valutazione\ndel rachide cervicale. 6. Il paziente deve essere posizionato in modo\nda permettere una valutazione neurologica completa, iniziando con il\nrachide cervicale. 7. L’opzione 1 propone di far sedere il paziente\nper esplorare il rachide cervicale, il che è coerente con la valutazione\niniziale richiesta. 8. Le altre opzioni non sono appropriate per la valu-\ntazione iniziale di un paziente con sospetta lesione spinale.\n**Conclusione:** 1\nExpert Comment. Presenta alcuni errori di conoscenza medica:\nConcentra la valutazione sul rachiede cervicale senza senza va-\nlutare l’esame neurologico complessivo\nContiene alcuni errori logici: Non considera la possibilità\nche la lesione spianale si aggravi facendo mettere seduto io\npaziente prima di aver controllato tutti i distretti\nAppendix D.3. Spanish\nPREGUNTA:\nEn un paciente con enfermedad pulmonar obstructiva crónica (EPOC)\nleve en fase estable se evidencia poliglobulia e insuficiencia respirato-\nria. En la radiografía de tórax no se aprecian alteraciones reseñables.\n¿Cuál de los siguientes procedimientos consideraría realizar en primer\nlugar para descartar la coexistencia de otras enfermedades que puedan\njustificar los hallazgos descritos?:\nOPCIONES:\n’1’: ’Poligrafía respiratoria.’, ’2’: ’Ecocardiograma.’, ’3’: ’Tomo-\ngrafía computarizada torácica.’, ’4’: ’Gammagrafía pulmonar.’, ’5’:\n’Punción de médula ósea.’\nRESPUESTA CORRECTA:\n1\nRESPUESTA DEL MODELO:\nBúsqueda de posibles muestras de razonamiento: - La poliglob-\nulia en un paciente con EPOC puede deberse a hipoxia crónica, pero\ntambién puede ser secundaria a otras condiciones como mieloprolif-\nerativas o tumores. - La insuficiencia respiratoria en un paciente con\nEPOC leve sugiere que podría haber una causa subyacente no eval-\nuada. - La radiografía de tórax no muestra alteraciones, lo cual no\nexcluye patología pulmonar o sistémica. - La gammagrafía pulmonar\npuede ayudar a evaluar la perfusión y la ventilación pulmonar, lo cual\nes útil para descartar neumonía, embolia pulmonar o neoplasias. - La\ntomografía computarizada torácica es más sensible que la radiografía,\npero no es el primer paso en un paciente con EPOC estable. - La eco-\ncardiografía puede ser útil para evaluar hipertensión pulmonar, pero no\nes el primer procedimiento para descartar otras causas de poliglobulia.\n- La poligrafía respiratoria es útil para evaluar la función pulmonar,\npero no aporta información sobre causas sistémicas de poliglobulia. -\nLa punción de médula ósea es invasiva y no es el primer paso para\ndescartar causas no hematológicas de poliglobulia.\nProceso de razonamiento: 1. El paciente tiene EPOC leve en fase\nestable, pero presenta poliglobulia e insuficiencia respiratoria, lo cual\nsugiere una causa subyacente no evaluada. 2. La poliglobulia puede\ndeberse a hipoxia crónica, pero también puede ser secundaria a otras\ncondiciones como mieloproliferativas, tumores o infecciones. 3. La\ngammagrafía pulmonar permite evaluar la perfusión y la ventilación\npulmonar, lo cual es útil para descartar neoplasias, embolia pulmonar\no neumonía. 4. La tomografía computarizada torácica es más sen-\nsible, pero no es el primer procedimiento a realizar en un paciente\ncon EPOC estable. 5. La ecocardiografía puede ser útil para evaluar\nhipertensión pulmonar, pero no es el primer paso para descartar otras\ncausas de poliglobulia. 6. La poligrafía respiratoria es útil para evaluar\nla función pulmonar, pero no aporta información sobre causas sistémi-\ncas de poliglobulia. 7. La punción de médula ósea es invasiva y no es\nel primer paso para descartar causas no hematológicas de poliglobu-\nlia. 8. Por lo tanto, la gammagrafía pulmonar es el procedimiento más\nadecuado para descartar otras enfermedades que puedan justificar los\nhallazgos descritos.\nConclusión: 4\nExpert Comment. La respuesta no tiene en cuenta todos los el-\nementos útiles: Información clínica del paciente, usualmente se\nrepetiría el hemograma para ver si sigue teniendo policitemia.\n22\n"}, {"page": 23, "text": "Figure D.7: Example of the expert analysis on a model answer which presents a mistake in linking the provided medical information with the most appropriate\nanswer.\n23\n"}, {"page": 24, "text": "Figure D.8: Example of the expert analysis on a model answer which presents misuse of the patient information.\n24\n"}]}