{"doc_id": "arxiv:2601.16419", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.16419.pdf", "meta": {"doc_id": "arxiv:2601.16419", "source": "arxiv", "arxiv_id": "2601.16419", "title": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning", "authors": ["Qinglong Cao", "Yuntian Chen", "Chao Ma", "Xiaokang Yang"], "published": "2026-01-23T03:10:08Z", "updated": "2026-02-04T01:33:52Z", "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities in multimodal perception and understanding tasks. However, their effectiveness in specialized domains, such as remote sensing and medical imaging, remains limited. A natural approach to domain adaptation is to inject domain knowledge through textual instructions, prompts, or auxiliary captions. Surprisingly, we find that such input-level domain knowledge injection yields little to no improvement on scientific multimodal tasks, even when the domain knowledge is explicitly provided. This observation suggests that current MLLMs fail to internalize domain-specific priors through language alone, and that domain knowledge must be integrated at the optimization level. Motivated by this insight, we propose a reinforcement fine-tuning framework that incorporates domain knowledge directly into the learning objective. Instead of treating domain knowledge as descriptive information, we encode it as domain-informed constraints and reward signals, shaping the model's behavior in the output space. Extensive experiments across multiple datasets in remote sensing and medical domains consistently demonstrate good performance gains, achieving state-of-the-art results on multimodal domain tasks. Our results highlight the necessity of optimization-level domain knowledge integration and reveal a fundamental limitation of textual domain conditioning in current MLLMs.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.16419v2", "url_pdf": "https://arxiv.org/pdf/2601.16419.pdf", "meta_path": "data/raw/arxiv/meta/2601.16419.json", "sha256": "05aacbc1bd211364277bdee8eae2d770d1a35da0549a9d367e0083bd7539983f", "status": "ok", "fetched_at": "2026-02-18T02:20:43.833619+00:00"}, "pages": [{"page": 1, "text": "Learning Domain Knowledge in Multimodal Large Language Models through\nReinforcement Fine-Tuning\nQinglong Cao 1 2 Yuntian Chen 2 Chao Ma 1 Xiaokang Yang 1\nAbstract\nMultimodal large language models (MLLMs)\nhave shown remarkable capabilities in multimodal\nperception and understanding tasks. However,\ntheir effectiveness in specialized domains, such\nas remote sensing and medical imaging, remains\nlimited. A natural approach to domain adaptation\nis to inject domain knowledge through textual in-\nstructions, prompts, or auxiliary captions. Surpris-\ningly, we find that such input-level domain knowl-\nedge injection yields little to no improvement on\nscientific multimodal tasks, even when the domain\nknowledge is explicitly provided. This observa-\ntion suggests that current MLLMs fail to inter-\nnalize domain-specific priors through language\nalone, and that domain knowledge must be inte-\ngrated at the optimization level. Motivated by this\ninsight, we propose a reinforcement fine-tuning\nframework that incorporates domain knowledge\ndirectly into the learning objective. Instead of\ntreating domain knowledge as descriptive infor-\nmation, we encode it as domain-informed con-\nstraints and reward signals, shaping the model’s\nbehavior in the output space. Extensive experi-\nments across multiple datasets in remote sensing\nand medical domains consistently demonstrate\ngood performance gains, achieving state-of-the-\nart results on multimodal domain tasks. Our re-\nsults highlight the necessity of optimization-level\ndomain knowledge integration and reveal a funda-\nmental limitation of textual domain conditioning\nin current MLLMs.\n1. Introduction\nMultimodal large language models (MLLMs) have demon-\nstrated strong generalization abilities across a wide range of\n1MoE Key Lab of Artificial Intelligence, AI Institute, Shang-\nhai Jiao Tong University, Shanghai, China 2Eastern Institute of\nTechnology, Ningbo, China. Correspondence to: Yuntian Chen\n<ychen@eitech.edu.cn>.\nPreprint. February 5, 2026.\nFigure 1. Performance comparison in specialized domains with\nmultimodal large language models.\nUCM AID RSICD WHURS19 PatternNet NWPU\nQwenVL\n8.91\n18.6\n15.5\n29.3\n8.62\n14.9\n+ Domain Prompt\n8.89\n18.5\n15.4\n29.1\n8.71\n14.9\n+ Caption(MLLM)\n6.42\n15.8\n15.3\n26.1\n5.23\n10.5\n+ Caption(BLIP)\n8.62\n18.1\n15.2\n28.8\n8.52\n14.7\nOurs\n28.8\n38.9\n26.4\n44.4\n19.1\n36.4\nTable 1. Performance comparison by injecting domain knowledge\nwith different fine-tuning methods.\nvision–language tasks (Achiam et al., 2023; Bai et al., 2025;\nLu et al., 2025; Xu et al., 2025; Cao et al., 2025), leading to\nremarkable progress in diverse application domains (Jiang\net al., 2025a; Wang et al., 2025b; Sun et al., 2025; Jiang\net al., 2025b). Despite these successes, their effectiveness\nin specialized scientific domains remains limited. As illus-\ntrated in Figure 1, advanced MLLMs typically achieve only\n20%–30% accuracy on benchmarks from remote sensing\nand medical imaging, indicating substantial challenges in\ncross-domain generalization.\nA natural strategy to address this limitation is to explicitly\ninject domain knowledge into MLLMs through prompts\nor auxiliary textual descriptions. Thus, we systematically\ninvestigate such approaches in the remote sensing domain.\nSpecifically, we explore (i) domain-aware prompting that\n1\narXiv:2601.16419v2  [cs.CL]  4 Feb 2026\n"}, {"page": 2, "text": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning\nexplicitly encodes domain properties (e.g., rotation invari-\nance), and (ii) caption-based augmentation, where auxiliary\ncaptions are either generated by MLLMs themselves or\nprovided by an external captioning model (BLIP (Li et al.,\n2023)). However, as shown in Table 1, these naive forms\nof domain knowledge injection do not consistently improve\nperformance. In several cases, caption-based augmentation\neven degrades accuracy, suggesting that simply providing\nadditional domain-related text is insufficient and may intro-\nduce misleading or noisy signals.\nThese observations suggest that current MLLMs are un-\nable to directly internalize high-level domain knowledge\nthrough naive textual conditioning. A key challenge is that\ndomain knowledge is often abstract and conceptual rather\nthan instance-level, and therefore cannot be easily translated\ninto large-scale supervised annotations. Moreover, for spe-\ncialized domains, such annotations are typically expensive\nand difficult to acquire. As a result, conventional super-\nvised fine-tuning (SFT) (Dong et al., 2024; Zhang et al.,\n2025b; Yuan et al., 2024), which relies heavily on explicit\ninput–output pairs, is insufficient for effectively injecting\ndomain knowledge into the model.\nIn contrast, reinforcement learning (Guo et al., 2025; Zhang\net al., 2024; Wang et al., 2024b; Yue et al., 2025), which\ndoes not require dense supervision, provides a natural mech-\nanism for incorporating abstract domain principles at the\noptimization level. By defining reward signals that reflect\ndomain-specific constraints or desiderata, reinforcement\nlearning enables models to gradually align their reasoning\nbehaviors with domain knowledge, even in the absence of\nlarge-scale annotations.\nMotivated by this insight, we propose a reinforcement fine-\ntuning framework that integrates domain knowledge directly\ninto the learning objective, as show in Table 1, enabling\nMLLMs to acquire domain-aware reasoning capabilities be-\nyond what is achievable with prompt-based or supervised\napproaches. Specifically, we introduce a domain-aware\nconstraint loss that explicitly encodes domain knowledge\nas optimization constraints. This constraint loss is incor-\nporated into the reinforcement learning process, shaping\nthe model’s policy toward domain-consistent reasoning be-\nhaviors. Rather than relying on explicit supervision, the\nproposed constraint guides the model to satisfy abstract\ndomain principles during optimization.\nIn addition, we quantify the degree of domain relevance\nfor each training sample and leverage this information to\nreweight the reinforcement learning advantages. By as-\nsigning higher advantages to samples that exhibit stronger\ndomain-specific characteristics, the learning process empha-\nsizes domain-aware trajectories and suppresses spurious or\ndomain-agnostic behaviors. Together, the constraint loss\nand the advantage reweighting mechanism jointly constitute\na domain knowledge–aware reinforcement signal, enabling\neffective domain knowledge injection even under limited\nannotations.\nThrough this unified framework, domain knowledge is no\nlonger treated as external conditioning, but is internalized\nas part of the model’s optimization objective, resulting in\nrobust and generalizable domain-aware reasoning. We con-\nduct extensive evaluations across diverse datasets in both\nremote sensing and medical imaging domains. The results\nconsistently validate the effectiveness of the proposed frame-\nwork, demonstrating its ability to induce domain-aware rea-\nsoning behaviors across heterogeneous multimodal tasks. In\nsummary, we make the following contributions:\n• To the best of our knowledge, we first proposed a re-\ninforcement fine-tuning paradigm that explicitly in-\njects domain knowledge into multimodal large lan-\nguage models at the optimization level, moving beyond\nprompt-based or supervised domain adaptation.\n• We develop a domain-aware reinforcement learning\nframework that integrates domain knowledge directly\ninto the learning objective through domain-specific\nconstraints and reward shaping, together with a domain\nknowledge–aware advantage reweighting strategy that\nemphasizes domain-relevant samples during training.\n• We demonstrate the effectiveness of the proposed\nframework through extensive experiments on remote\nsensing and medical imaging benchmarks, achieving\nstate-of-the-art performance across a wide range of\nmultimodal domain-specific tasks.\n2. Related Work\nMultimodal Large Language Models. Multimodal large\nlanguage models (MLLMs), such as GPT-4V (Achiam et al.,\n2023) and Qwen-VL (Bai et al., 2025), have demonstrated\nremarkable capabilities in jointly understanding and rea-\nsoning over visual and textual inputs. Recent advances\nshow that MLLMs can effectively serve as general-purpose\nmultimodal agents, enabling a wide range of downstream\ntasks that require vision–language perception, interaction,\nand decision making (Li et al., 2024; Huang et al., 2024a;\nHan et al., 2024). These models have been successfully\napplied to diverse multimodal applications, including visual\nediting, multimodal reasoning, and interactive response gen-\neration (Huang et al., 2024b; Zang et al., 2025; Luo et al.,\n2025b; Wang et al., 2025a). From a training perspective,\nthe development of MLLMs typically consists of two stages.\nThe first stage is large-scale multimodal pre-training (Luo\net al., 2025a; Lin et al., 2024; Fan et al., 2024), which lever-\nages massive image–text data to endow the model with gen-\neral multimodal representations and broad world knowledge.\nThe second stage is post-training (Cheng et al., 2024; Wang\n2\n"}, {"page": 3, "text": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning\net al., 2024a; Cheng et al., 2025), which includes instruction\ntuning, reinforcement learning, and alignment techniques,\naiming to improve instruction following, response quality,\nand reasoning behavior. While these post-training strategies\nhave proven effective for general-purpose tasks, their ability\nto inject domain knowledge into MLLMs remains limited,\nespecially in specialized scientific domains.\nReinforcement Learning for MLLMs. With the emer-\ngence of reasoning-oriented models such as OpenAI-\no1 (Jaech et al., 2024) and DeepSeek-R1 (Liu et al., 2024;\nGuo et al., 2025), increasing attention has been devoted to\nleveraging reinforcement learning (RL) to enhance the rea-\nsoning capabilities of MLLMs. Reinforcement learning pro-\nvides a flexible optimization framework that allows models\nto be trained with preference signals or task-level objectives,\nwithout relying on dense supervised annotations. Conven-\ntional RL-based alignment methods include Proximal Policy\nOptimization (PPO) (Schulman et al., 2017), which opti-\nmizes policies under a KL-regularized constraint, and Di-\nrect Preference Optimization (DPO) (Rafailov et al., 2023)\nwhich eliminates the need for an explicit reward model by\ndirectly learning from pairwise preferences. More recently,\nGroup Relative Policy Optimization (GRPO) (Shao et al.,\n2024) has been proposed as an efficient alternative that\nestimates advantages using group-wise comparisons, signif-\nicantly reducing computational overhead while maintaining\nstable training dynamics. Due to its favorable efficiency\nand scalability, GRPO has become a widely adopted rein-\nforcement learning paradigm for training large language\nmodels (Zhang et al., 2025a;c; Ramesh et al., 2024; Gao\net al., 2024). Particularly, Visual-RFT (Liu et al., 2025)\nadopts it for visual perception tasks. In this work, we adopt\nGRPO as the underlying reinforcement learning framework.\nBy integrating domain-aware constraints and domain knowl-\nedge–guided reward shaping into the GRPO objective, our\napproach enables reinforcement learning to explicitly guide\nMLLMs toward domain-consistent reasoning behaviors.\n3. Method\nAs illustrated in Figure 2, our goal is to adapt multimodal\nlarge language models (MLLMs) to specialized domains\nsuch as remote sensing and medical imaging. Unlike con-\nventional supervised fine-tuning (SFT) or prompt-based do-\nmain injection, our approach internalizes domain knowledge\nthrough reinforcement fine-tuning. We focus on abstract and\nstructural domain knowledge that cannot be easily expressed\nvia textual supervision, such as rotation invariance in remote\nsensing and symmetry consistency in medical imaging. To\nincorporate such knowledge, we adopt GRPO as the base\nreinforcement learning framework, enabling domain knowl-\nedge to be integrated directly at the policy optimization level.\nOur method consists of two key components. First, we con-\nstruct a domain-support sampling distribution that explicitly\nencodes domain knowledge, and introduce a domain-aware\nconstraint to align the policy sampling distribution with\nthis domain support during reinforcement learning. This\nconstraint encourages domain-consistent behaviors such as\ninvariance and symmetry. Second, we quantify the degree\nof domain knowledge exhibited by each sampled output by\nmeasuring the divergence between the policy distribution\nand the domain-support distribution, and use this signal\nto reweight reinforcement learning advantages. Together,\nthese two mechanisms enable effective and stable injection\nof abstract domain knowledge into MLLMs.\n3.1. Preliminary\nGRPO (Shao et al., 2024) is a value-free policy optimization\nmethod designed to improve training stability and sample\nefficiency in reinforcement learning. Derived from Prox-\nimal Policy Optimization (PPO) (Schulman et al., 2017),\nGRPO avoids explicit value function estimation by adopting\na group-based relative advantage formulation, which effec-\ntively regularizes policy updates while preserving sufficient\noptimization flexibility. Let πθ denote a policy parameter-\nized by θ. Given an input context c, GRPO first draws a\ngroup of G candidate outputs {o1, o2, . . . , oG} from the pre-\nvious policy πθold. Each sampled output is then evaluated\nby predefined, verifiable reward functions, producing a set\nof scalar rewards {r1, r2, . . . , rG}. Instead of relying on a\nlearned value baseline, GRPO computes relative advantages\nwithin the sampled group by normalizing rewards as\nAi = ri −mean({r1, r2, . . . , rG})\nstd({r1, r2, . . . , rG})\n.\n(1)\nUsing\nthe\nresulting\ngroup-relative\nadvantages\n{A1, A2, . . . , AG},\nthe\npolicy\nπθ\nis\noptimized\nby\nmaximizing the following objective:\nL(θ) = E{oi}G\ni=1∼πθold\n\"\n1\nG\nG\nX\ni=1\nπθ(oi)\nπθold(oi)Ai −β DKL\n\u0000πθ ∥πref\n\u0001\n#\n,\n(2)\nwhere DKL(·∥·) denotes the Kullback–Leibler divergence\nthat constrains the updated policy πθ to remain close to a\nreference policy πref, and β controls the strength of this\nregularization.\n3.2. Domain-aware Constraints\nWe adopt Group Relative Policy Optimization (GRPO) as\nthe base reinforcement learning framework and employ the\nreward functions proposed in Visual-RFT (Liu et al., 2025)\nto supervise policy optimization. Given multimodal inputs\nconsisting of prompt questions and domain-specific images,\nthe model produces a policy sampling distribution πθ, while\n3\n"}, {"page": 4, "text": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning\nFigure 2. Overview of the proposed domain-aware reinforcement fine-tuning framework. Unlike prompt-based or caption-based domain\nadaptation, domain knowledge is incorporated at the optimization level through domain-aware constraints and reward shaping. By\nmodifying the policy sampling distributions under reinforcement learning, the model is guided toward domain-consistent reasoning\nbehaviors without relying on dense annotations.\na reference model provides a reference distribution πref.\nIn GRPO, the Kullback–Leibler (KL) divergence between\nπθ and πref is used to regularize policy updates, ensuring\nthat the optimized policy remains close to the reference\nmodel and thus preserves general prior knowledge. Inspired\nby this mechanism, we further introduce a domain-aware\nconstraint to explicitly inject domain knowledge into the\npolicy optimization process.\nSpecifically, we consider rotation invariance in remote sens-\ning and symmetry consistency in medical imaging as rep-\nresentative forms of domain knowledge. A straightforward\napproach would be to directly enforce invariance or consis-\ntency at the output or reward level. However, such explicit\nconstraints are often brittle and task-dependent, and may fail\nto generalize across different prompts or output structures.\nInstead, we incorporate domain knowledge at the distribu-\ntion level by regulating the policy sampling behavior under\ndomain-specific transformations. To construct a domain-\nsupport sampling distribution, we apply domain-specific\ntransformations, random rotations for remote sensing im-\nages and symmetric transformations for medical images,\nand feed the transformed inputs, together with the same\nprompt questions, into the policy model. This process yields\na domain-support sampling distribution denoted as πD\nθ . To\nenforce invariance and consistency, we minimize the KL di-\nvergence between the original policy distribution πθ and the\ndomain-support distribution πD\nθ , which defines the domain\nloss:\nLdom = DKL\n\u0000πD\nθ ∥πθ\n\u0001\n.\n(3)\nBy constraining the policy sampling distribution to re-\nmain consistent under domain-specific transformations, this\nloss encourages the model to exhibit domain-invariant and\ndomain-consistent behaviors.\n3.3. Domain-aware Reward Shaping\nBeyond domain-aware constraints, we further introduce\ndomain-aware reward shaping to provide fine-grained\noptimization signals that explicitly emphasize domain-\nconsistent behaviors. While the constraint term enforces\ndistribution-level invariance globally, it treats all samples\nequally. In contrast, domain-aware reward shaping allows\nthe optimization process to adaptively focus on samples that\nbetter satisfy domain principles.\nInspired by group-based relative evaluation in GRPO, we\nquantify the degree of domain knowledge exhibited by each\nsampled output by measuring the divergence between the\npolicy sampling distribution πθ and the domain-support\ndistribution πD\nθ . We use KL divergence for constraints due\nto its asymmetry and stronger penalization of distributional\nmismatch, while JS divergence is adopted for advantage\nreweighting due to its boundedness and stability Specifically,\n4\n"}, {"page": 5, "text": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning\nwe compute the Jensen–Shannon (JS) divergence as\nDi = DJS\n\u0000πD\nθ ∥πθ\n\u0001\n,\n(4)\nwhich yields a bounded and symmetric measure in [0, 1] that\nreflects how well a sample aligns with the domain-support\ndistribution. Using this divergence, we reweight the origi-\nnal GRPO advantages to obtain domain-aware advantages.\nSamples that are more consistent with domain knowledge\n(i.e., smaller divergence) are assigned larger advantages:\nAd\ni = (1 −Di) · Ai.\n(5)\nBy incorporating the domain-aware advantages, the GRPO\nobjective is modified as\nL(θ) = E{oi}G\ni=1∼πθold\n\"\n1\nG\nG\nX\ni=1\nπθ(oi)\nπθold(oi)Ad\ni\n−β DKL\n\u0000πθ ∥πref\n\u0001\n−DKL\n\u0000πD\nθ ∥πθ\n\u0001\n#\n.\n(6)\nMore generally, the resulting domain-aware reinforcement\nlearning objective can be written as\nLDA(θ) = E{oi}G\ni=1∼πθold\n\"\n1\nG\nG\nX\ni=1\nπθ(oi)\nπθold(oi)Ad\ni\n−β DKL\n\u0000πθ ∥πref\n\u0001\n−Ldom\n#\n.\n(7)\nAlthough we instantiate framework with rotation invariance\nand symmetry consistency, the proposed formulation is gen-\neral and can incorporate arbitrary domain priors that can be\nexpressed via transformations or distributional constraints.\n4. Experiments\n4.1. Experimental Setups\nTo evaluate our method under realistic specialized-domain\nscenarios, we conduct few-shot learning experiments in\nboth remote sensing and medical imaging domains. For the\nremote sensing domain, we consider six widely used bench-\nmark datasets: UCM (Yang & Newsam, 2010), AID (Xia\net al., 2017), RSICD (Lu et al., 2017), WHURS19 (Dai\n& Yang, 2011), PatternNet (Zhou et al., 2018), and\nNWPU (Cheng et al., 2017). For the medical imaging do-\nmain, we adopt datasets from MedMNIST v2 (Yang et al.,\n2023), including OrganAMNIST, BloodMNIST, DermaM-\nNIST, PathMNIST, and TissueMNIST.\nWe conducted experiments under 1-shot, 2-shot, 4-shot,\nand 8-shot settings, where the number of shots denotes\nthe number of annotated samples per category. We adopt\nTable 2. Few-shot results on the remote sensing domain. We\nconducted experiments on six remote sensing datasets. ∆denotes\nthe performance benefits compared with SFT.\nModels\nAverage\nUCM\nAID\nRSICD\nWHURS19\nPatternNet\nNWPU\nQwen2.5-VL\n16.0\n8.91\n18.6\n15.5\n29.3\n8.61\n14.9\n1-shot\nSFT\n16.8\n10.1\n19.8\n15.7\n30.3\n9.15\n15.6\nVisual-RFT\n17.4\n10.3\n20.9\n16.1\n30.7\n9.61\n16.5\nOurs\n19.7\n16.1\n22.0\n17.1\n33.7\n10.8\n18.4\n∆\n+2.9\n+6.0\n+2.2\n+1.4\n+3.4\n+1.7\n+2.8\n2-shot\nSFT\n18.3\n14.9\n20.6\n16.0\n31.2\n10.4\n16.6\nVisual-RFT\n19.1\n15.7\n21.8\n16.2\n31.7\n11.9\n17.2\nOurs\n20.9\n17.1\n23.8\n19.6\n33.6\n12.6\n18.7\n∆\n+2.6\n+2.2\n+3.2\n+3.6\n+2.4\n+1.2\n+2.1\n4-shot\nSFT\n20.3\n17.0\n21.8\n19.8\n32.3\n12.0\n18.9\nVisual-RFT\n21.9\n18.4\n22.4\n23.5\n32.6\n12.5\n21.8\nOurs\n25.0\n20.1\n24.4\n25.8\n38.8\n17.5\n23.5\n∆\n+4.7\n+3.1\n+2.6\n+6.0\n+6.5\n+5.5\n+4.6\n8-shot\nSFT\n25.5\n21.8\n29.9\n22.6\n39.5\n15.4\n23.6\nVisual-RFT\n28.5\n23.6\n32.9\n25.6\n40.2\n18.2\n30.5\nOurs\n32.3\n28.8\n38.9\n26.4\n44.4\n19.1\n36.4\n∆\n+7.8\n+7.0\n+9.0\n+3.8\n+4.9\n+3.7\n+12.8\nthe reward functions in Visual-RFT (Liu et al., 2025) to\nreward the policy model. To further assess the effectiveness\nof our approach in multimodal reasoning, we additionally\nconduct few-shot grounding experiments on the DIOR (Li\net al., 2020) dataset. All experiments are conducted using\nQwen2.5-VL as the base MLLM. Training is performed with\na batch size of 4 on 2 or 4 NVIDIA A100 GPUs (80GB).\nThe model is trained for 2 epochs in the 1-shot and 2-shot\nsettings, and for 4 epochs in the 4-shot and 8-shot settings.\nWe use the Adam optimizer with a learning rate of 5×10−5.\nUnless otherwise specified, all experiments use the same\nprompt templates and evaluation protocols.\n4.2. Quantitative Results\nRemote sensing domain.\nTable 2 reports the few-shot\nrecognition results on six remote sensing datasets using\nQwen2.5-VL-3B.Our method consistently outperforms both\nSFT and Visual-RFT across all shot settings and all datasets.\nNotably, the performance gains become more pronounced as\nthe number of shots increases, indicating that domain-aware\nreinforcement fine-tuning can more effectively leverage lim-\nited supervision. On average, our approach improves over\nSFT by +2.9, +2.6, +4.7, and +7.8 points under the 1-shot,\n2-shot, 4-shot, and 8-shot settings, respectively. Substantial\n5\n"}, {"page": 6, "text": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning\nTable 3. Few-shot results on the medical imaging domain. We\nconducted experiments on five medical imaging datasets. ∆de-\nnotes the performance benefits compared with SFT.\nModels\nAverage\nOrganAMNIST\nBloodMNIST\nDermaMNIST\nPathMNIST\nTissueMNIST\nQwen2.5-VL 20.7\n8.70\n14.8\n21.7\n29.0\n29.1\n1-shot\nSFT\n20.9\n9.10\n15.2\n21.3\n29.8\n29.5\nVisual-RFT\n21.0\n9.39\n15.2\n21.1\n29.9\n29.5\nOurs\n21.5\n9.85\n15.6\n21.8\n30.6\n29.7\n∆\n+0.6\n+0.75 +0.4 +0.5 +0.8\n+0.2\n2-shot\nSFT\n21.4\n9.76\n15.5\n21.8\n30.1\n29.8\nVisual-RFT\n21.6\n9.81\n15.6\n22.3\n30.3\n29.8\nOurs\n23.2\n12.4\n16.1\n23.5\n33.2\n30.9\n∆\n+1.8\n+2.6\n+0.6 +1.7 +3.1\n+1.1\n4-shot\nSFT\n22.9\n10.0\n16.1\n27.3\n30.6\n30.3\nVisual-RFT\n23.0\n10.1\n16.0\n27.6\n30.9\n30.6\nOurs\n25.3\n13.2\n17.2\n32.8\n31.4\n31.7\n∆\n+2.4\n+3.2\n+1.1 +5.5 +0.8\n+1.4\n8-shot\nSFT\n24.1\n10.7\n17.1\n28.9\n32.3\n31.4\nVisual-RFT\n24.4\n10.8\n17.4\n29.9\n32.6\n31.5\nOurs\n26.8\n13.3\n18.9\n33.1\n35.4\n33.1\n∆\n+2.7\n+2.5\n+1.8 +4.2 +3.1\n+1.7\nimprovements are observed on datasets that exhibit strong\nrotational characteristics, such as WHURS19, PatternNet,\nand NWPU, where our method achieves gains of up to\n+6.5, +5.5, and +12.8 points. These results validate the\neffectiveness of injecting domain knowledge at the policy\noptimization level.\nMedical imaging domain.\nTable 3 summarizes the few-\nshot recognition results on five medical imaging benchmarks\nusing Qwen2.5-VL-3B. Compared with remote sensing, the\nimprovements in the 1-shot setting are relatively modest,\nwhich is expected due to the higher visual homogeneity\nand label ambiguity in medical images. Nevertheless, our\nmethod consistently outperforms both SFT and Visual-RFT\nacross all shot settings, achieving average gains over SFT of\n+0.6, +1.8, +2.4, and +2.7 points from 1-shot to 8-shot. The\nadvantages become more evident as the number of shots\nincreases, particularly on datasets with strong anatomical\nsymmetry such as OrganAMNIST and PathMNIST, where\nTable 4. Few-shot grounding results on DIOR dataset of 6\nrepresentative categories. We conducted 4-shot and 8-shot exper-\niments. ∆denotes the performance benefits compared with SFT.\nThe first column denotes the average mAP.\nModels\nmAP\nAirplane\nBridge\nHarbor\nStadium\nStorage Tank\nShip\nQwen2.5-VL 11.3\n15.6 5.71 13.2 19.8 8.66 4.60\n4-shot\nSFT\n17.8\n22.2 15.4 15.8 23.8 13.6 15.9\nVisual-RFT\n19.6\n25.3 17.7 16.8 24.2 15.8 17.9\nOurs\n21.0\n27.6 18.6 17.9 25.8 16.7 19.5\n∆\n+3.2 +5.4 +3.2 +2.1 +2.0 +3.1 +3.6\n8-shot\nSFT\n22.1\n27.1 17.9 26.7 25.1 16.9 18.7\nVisual-RFT\n22.7\n27.3 18.8 27.9 25.9 17.1 19.2\nOurs\n24.6\n29.9 20.6 29.6 27.8 18.4 21.3\n∆\n+2.5 +2.8 +2.7 +2.9 +2.7 +2.5 +2.6\nour method yields gains of up to +3.2 and +5.5 points. These\nresults demonstrate that enforcing symmetry consistency at\nthe distribution level effectively injects structural medical\npriors and improves few-shot generalization.\nFew-shot grounding.\nFollowing the experimental proto-\ncol of Visual-RFT (Liu et al., 2025), we randomly sample\nsix representative categories from the DIOR dataset and\nevaluate few-shot grounding performance using the larger\nQwen2.5-VL-7B model, as reported in Table 4. Our method\nconsistently outperforms both SFT and Visual-RFT under\nthe 4-shot and 8-shot settings. In particular, it achieves av-\nerage mAP improvements of +3.2 and +2.5 over SFT in\nthe 4-shot and 8-shot scenarios, respectively. Performance\ngains are observed across all six object categories, including\nAirplane, Bridge, and Ship, demonstrating more accurate\nspatial localization and region-level reasoning. These re-\nsults indicate that domain-aware reinforcement fine-tuning\nnot only improves classification performance but also ef-\nfectively enhances multimodal grounding capabilities when\napplied to larger-capacity models.\n4.3. Qualitative Results\nFigure 3 presents the qualitative comparisons between SFT\nand our method across both remote sensing and medical\nimaging domains. Particularly, the first row denotes sam-\nples in the remote sensing domain, while the second row\ndenotes results in medical imaging domain. For remote\nsensing scenes, SFT often produces coarse or semantically\nincorrect predictions, such as confusing bridges with forests,\n6\n"}, {"page": 7, "text": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning\nFigure 3. Qualitative results on remote sensing and medical imaging tasks. Compared with directly supervised fine-tuning (SFT), our\nmethod yields accurate and fine-grained categories by grounding predictions in discriminative visual patterns and domain-specific cues.\nFirst Row: Remote Sensing. Second Row: Medical Imaging.\nparking lots with generic vehicle-related categories, or rail-\nway stations with urban areas. In contrast, our method cor-\nrectly identifies fine-grained scene categories by grounding\npredictions in discriminative spatial structures and object\narrangements, e.g., linear bridge spans over water, dense\nvehicle layouts in parking lots, and track-centered config-\nurations in railway stations. Similar improvements are ob-\nserved in the medical imaging domain. While SFT tends\nto generate superficial or noisy predictions (e.g., confusing\ndebris with lymphocytes or misclassifying tissue types), our\napproach produces clinically meaningful categories such\nas colorectal adenocarcinoma, lymphocytes, and immature\ngranulocytes. These predictions are supported by coher-\nent visual reasoning that aligns cellular morphology with\ndomain-specific semantics. Overall, the results demonstrate\nthat domain-aware reinforcement fine-tuning effectively en-\nhances cross-domain visual reasoning and semantic ground-\ning. Our method not only improves answer correctness but\nalso yields more interpretable and visually consistent pre-\ndictions, highlighting its robustness in both complex remote\nsensing scenes and fine-grained medical imagery.\n4.4. Ablation Study\nEffect of domain-aware reinforcement learning compo-\nnents.\nTable 5 reports the ablation results of the proposed\ndomain-aware reinforcement learning components on the\nUCM dataset under different few-shot settings. Starting\nfrom the baseline without any domain-aware design, in-\nTable 5. Ablation study of domain-aware reinforcement learning\ncomponents on the UCM dataset. DC denotes domain-aware\nconstraint and DR denotes domain-aware reward shaping.\nDC\nDR\n1-shot\n2-shot\n4-shot\n8-shot\n×\n×\n10.3\n15.7\n18.4\n23.6\n✓\n×\n14.5\n16.8\n19.1\n26.5\n×\n✓\n15.4\n16.6\n19.8\n27.7\n✓\n✓\n16.1\n17.1\n20.1\n28.8\ntroducing either the domain-aware constraint (DC) or the\ndomain-aware reward shaping (DR) consistently improves\nperformance across all shot numbers, demonstrating that\nboth components are individually effective. In particular,\nDC yields more pronounced gains in extremely low-shot\nsettings, indicating its effectiveness in constraining the pol-\nicy toward domain-consistent behaviors, while DR provides\nlarger improvements in higher-shot regimes by delivering\nmore informative and stable reward signals. When both\nDC and DR are jointly applied, the model achieves the\nbest performance in all settings, with clear and consistent\nmargins over using either component alone. These results\nvalidate the complementary nature of DC and DR, and con-\nfirm that their combination is crucial for fully exploiting\ndomain knowledge in reinforcement fine-tuning.\nComparison with Direct Data Augmentation.\nTable 6\ncompares our method with standard data augmentation (DA)\nstrategies applied during training on the UCM dataset. As\n7\n"}, {"page": 8, "text": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning\nTable 6. Comparison with direct data augmentation on the UCM\ndataset. DA denotes standard data augmentation applied during\ntraining.VRFT denotes Visual-RFT (Liu et al., 2025).\nMethod\n1-shot\n2-shot\n4-shot\n8-shot\nSFT\n10.1\n14.9\n17.0\n21.8\nSFT + DA\n10.2\n14.8\n17.1\n21.6\nVRFT\n10.3\n15.7\n18.4\n23.6\nVRFT + DA\n10.4\n15.8\n18.3\n23.7\nOurs\n16.1\n17.1\n20.1\n28.8\nTable 7. Ablation study comparing output-level constraints and\ndistribution-level constraints on the UCM dataset.OC denotes\noutput-level constraint, DC denotes domain-aware constraint and\nDR denotes domain-aware reward shaping.\nMethod\n1-shot\n2-shot\n4-shot\n8-shot\nBaseline\n10.3\n15.7\n18.4\n23.6\nBaseline + OC\n11.5\n15.9\n18.6\n23.5\nBaseline + DC\n14.5\n16.8\n19.1\n26.5\nDC + DR (Ours)\n16.1\n17.1\n20.1\n28.8\nshown, incorporating conventional data augmentation into\neither SFT or Visual-RFT leads to only marginal perfor-\nmance changes, and in some cases even results in slight\ndegradation, particularly under low-shot settings. This sug-\ngests that naive data-level transformations are insufficient\nto effectively capture domain-specific invariances and may\nintroduce additional noise when supervision is extremely\nlimited. In contrast, our domain-aware reinforcement fine-\ntuning significantly outperforms all baselines by a large mar-\ngin across all shot numbers. Notably, the performance gap\nbecomes increasingly pronounced as the number of shots in-\ncreases, highlighting that explicitly injecting domain knowl-\nedge at the policy optimization level is substantially more\neffective than relying on direct data augmentation. These re-\nsults further confirm that domain-aware RL provides a more\nprincipled and robust mechanism for exploiting structural\npriors than conventional augmentation-based approaches.\nOutput-level vs. Distribution-level Constraints.\nTable 7\ninvestigates the effectiveness of enforcing domain knowl-\nedge at different levels, comparing output-level constraints\n(OC) with distribution-level constraints incorporated into\nreinforcement learning. Applying output-level constraints\non top of the baseline yields only marginal improvements,\nand the gains are inconsistent across shot settings, indicating\nlimited capability in capturing domain-specific invariances.\nIn contrast, introducing domain-aware constraints at the dis-\ntribution level (DC) leads to substantially larger and more\nstable performance gains across all shot numbers, demon-\nstrating that enforcing consistency over model behaviors is\nmore effective than directly regularizing final predictions.\nFurthermore, combining distribution-level constraints with\ndomain-aware reward shaping (DC + DR) achieves the best\nperformance by a clear margin, highlighting the comple-\nmentary roles of constraint-based regularization and reward-\nTable 8. Effect of divergence choices for domain-aware constraint\n(DC) and domain-aware reward shaping (DR) on the UCM dataset.\nDiv. denotes Divergence.\nDC Div.\nDR Div.\n1-shot\n2-shot\n4-shot\n8-shot\nKL\nKL\n14.8\n16.5\n18.7\n26.5\nJS\nJS\n15.7\n16.8\n19.5\n27.6\nJS\nKL\n13.9\n16.1\n17.6\n25.3\nKL(Ours)\nJS (Ours)\n16.1\n17.1\n20.1\n28.8\ndriven optimization. These results confirm that injecting\ndomain knowledge at the policy and distribution level is cru-\ncial for robust few-shot generalization, while output-level\nheuristics alone are insufficient.\nDivergence Choices for Varying Components.\nTable 8\nanalyzes the impact of different divergence functions used\nin domain-aware constraint (DC) and domain-aware reward\nshaping (DR). When the same divergence is applied to both\ncomponents (KL–KL or JS–JS), performance improves over\nthe baseline but remains suboptimal, suggesting that DC\nand DR play distinct roles and benefit from different diver-\ngence characteristics. Using JS divergence for DC provides\nmore stable improvements than KL, as JS is symmetric and\nbounded, making it better suited for enforcing distribution-\nlevel consistency. Conversely, employing KL divergence\nfor DR leads to inferior performance, indicating that asym-\nmetric penalties in reward shaping may destabilize policy\noptimization. Our final design, which applies KL diver-\ngence for DC and JS divergence for DR, achieves the best\nresults across all shot settings. This complementary combi-\nnation effectively balances strict distribution alignment and\nstable reward optimization, highlighting the importance of\ncarefully selecting divergence functions for different com-\nponents in domain-aware reinforcement learning.\n5. Conclusion\nWe propose a domain-aware reinforcement fine-tuning\nframework that injects abstract domain knowledge into\nmultimodal large language models at the policy distribu-\ntion level. By combining domain-aware constraints with\ndomain-aware reward shaping, our method enforces domain-\nknowledge consistent behaviors directly during reinforce-\nment learning. Extensive experiments across remote sensing\nand medical imaging benchmarks demonstrate consistent\nimprovements under few-shot settings, and ablation studies\nconfirm the complementary roles of the proposed compo-\nnents. These results suggest that domain-aware reinforce-\nment learning is an effective and general approach for adapt-\ning multimodal foundation models to specialized domains\nwith limited supervision. We believe this framework pro-\nvides a promising direction for systematically integrating\ndomain priors into multimodal models beyond the specific\ntasks studied in this work.\n8\n"}, {"page": 9, "text": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof machine learning. There are many potential societal\nconsequences of our work, none of which we feel must be\nspecifically highlighted here.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,\nI., Aleman, F. L., Almeida, D., Altenschmidt, J., Alt-\nman, S., Anadkat, S., et al.\nGpt-4 technical report.\narXiv:2303.08774, 2023.\nBai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang,\nK., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl\ntechnical report. arXiv:2502.13923, 2025.\nCao, Q., Chen, Y., Lu, L., Sun, H., Zeng, Z., Yang, X.,\nand Zhang, D. Generalized domain prompt learning for\naccessible scientific vision-language models. Nexus, 2(2),\n2025.\nCheng, D., Huang, S., Zhu, Z., Zhang, X., Zhao, W. X.,\nLuan, Z., Dai, B., and Zhang, Z. On domain-adaptive\npost-training for multimodal large language models.\narXiv:2411.19930, 2024.\nCheng, D., Huang, S., Zhu, Z., Zhang, X., Zhao, W. X.,\nLuan, Z., Dai, B., and Zhang, Z. On domain-adaptive\npost-training for multimodal large language models. In\nFindings of the Association for Computational Linguis-\ntics: EMNLP 2025, pp. 274–296, 2025.\nCheng, G., Han, J., and Lu, X. Remote sensing image scene\nclassification: Benchmark and state of the art. Proceed-\nings of the IEEE, 105(10):1865–1883, 2017.\nDai, D. and Yang, W. Satellite image classification via\ntwo-layer sparse coding with biased image representation.\nIEEE Transactions on Geoscience and Remote Sensing,\n8(1):173–176, 2011.\nDong, G., Yuan, H., Lu, K., Li, C., Xue, M., Liu, D., Wang,\nW., Yuan, Z., Zhou, C., and Zhou, J. How abilities in large\nlanguage models are affected by supervised fine-tuning\ndata composition. In Proceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 177–198, 2024.\nFan, W.-C., Chen, Y.-C., Liu, M., Yuan, L., and Sigal, L. On\npre-training of multimodal language models customized\nfor chart understanding. arXiv:2407.14506, 2024.\nGao, Z., Chang, J., Zhan, W., Oertell, O., Swamy, G., Brant-\nley, K., Joachims, T., Bagnell, D., Lee, J. D., and Sun,\nW. Rebel: Reinforcement learning via regressing relative\nrewards. Advances in Neural Information Processing\nSystems, 37:52354–52400, 2024.\nGuo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu,\nQ., Xu, R., Zhang, R., Ma, S., Bi, X., et al. Deepseek-\nr1 incentivizes reasoning in llms through reinforcement\nlearning. Nature, 645(8081):633–638, 2025.\nHan, S. C., Cao, F., Poon, J., and Navigli, R. Multimodal\nlarge language models and tunings: Vision, language,\nsensors, audio, and beyond. In Proceedings of the 32nd\nACM International Conference on Multimedia, pp. 11294–\n11295, 2024.\nHuang, D., Yan, C., Li, Q., and Peng, X. From large lan-\nguage models to large multimodal models: A literature\nreview. Applied Sciences, 14(12):5068, 2024a.\nHuang, Y., Xie, L., Wang, X., Yuan, Z., Cun, X., Ge, Y.,\nZhou, J., Dong, C., Huang, R., Zhang, R., et al. Smartedit:\nExploring complex instruction-based image editing with\nmultimodal large language models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 8362–8371, 2024b.\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky,\nA., Low, A., Helyar, A., Madry, A., Beutel, A., Carney,\nA., et al. Openai o1 system card. arXiv:2412.16720,\n2024.\nJiang, J., Ma, C., Song, X., Zhang, H., and Luo, J.\nCorvid: Improving multimodal large language models\ntowards chain-of-thought reasoning. In Proceedings of\nthe IEEE/CVF International Conference on Computer\nVision, pp. 3034–3046, 2025a.\nJiang, J.-P., Xia, Y., Sun, H.-L., Lu, S., Chen, Q.-G., Luo,\nW., Zhang, K., Zhan, D.-C., and Ye, H.-J. Multimodal\ntabular reasoning with privileged structured information.\narXiv:2506.04088, 2025b.\nLi, B., Ge, Y., Ge, Y., Wang, G., Wang, R., Zhang, R.,\nand Shan, Y. Seed-bench: Benchmarking multimodal\nlarge language models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npp. 13299–13308, 2024.\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders\nand large language models. In International conference\non machine learning, pp. 19730–19742. PMLR, 2023.\nLi, K., Wan, G., Cheng, G., Meng, L., and Han, J. Object\ndetection in optical remote sensing images: A survey and\na new benchmark. ISPRS journal of photogrammetry and\nremote sensing, 159:296–307, 2020.\nLin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M., and\nHan, S. Vila: On pre-training for visual language models.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 26689–26699, 2024.\n9\n"}, {"page": 10, "text": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao,\nC., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3\ntechnical report. arXiv:2412.19437, 2024.\nLiu, Z., Sun, Z., Zang, Y., Dong, X., Cao, Y., Duan, H., Lin,\nD., and Wang, J. Visual-rft: Visual reinforcement fine-\ntuning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pp. 2034–2044,\nOctober 2025.\nLu, S., Li, Y., Xia, Y., Hu, Y., Zhao, S., Ma, Y., Wei, Z., Li,\nY., Duan, L., Zhao, J., et al. Ovis2. 5 technical report.\narXiv:2508.11737, 2025.\nLu, X., Wang, B., Zheng, X., and Li, X. Exploring models\nand data for remote sensing image caption generation.\nIEEE Transactions on Geoscience and Remote Sensing,\n56(4):2183–2195, 2017.\nLuo, G., Yang, X., Dou, W., Wang, Z., Liu, J., Dai, J., Qiao,\nY., and Zhu, X. Mono-internvl: Pushing the boundaries\nof monolithic multimodal large language models with\nendogenous visual pre-training. In Proceedings of the\nComputer Vision and Pattern Recognition Conference, pp.\n24960–24971, 2025a.\nLuo, R., Zhang, H., Chen, L., Lin, T.-E., Liu, X., Wu, Y.,\nYang, M., Li, Y., Wang, M., Zeng, P., et al. Mmevol: Em-\npowering multimodal large language models with evol-\ninstruct. In Findings of the Association for Computational\nLinguistics: ACL 2025, pp. 19655–19682, 2025b.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,\nErmon, S., and Finn, C. Direct preference optimiza-\ntion: Your language model is secretly a reward model.\nAdvances in neural information processing systems, 36:\n53728–53741, 2023.\nRamesh, S. S., Hu, Y., Chaimalas, I., Mehta, V., Sessa,\nP. G., Bou Ammar, H., and Bogunovic, I. Group robust\npreference optimization in reward-free rlhf. Advances\nin Neural Information Processing Systems, 37:37100–\n37137, 2024.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv:1707.06347, 2017.\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang,\nH., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath:\nPushing the limits of mathematical reasoning in open\nlanguage models. arXiv:2402.03300, 2024.\nSun, H.-L., Zhou, D.-W., Li, Y., Lu, S., Yi, C., Chen, Q.-G.,\nXu, Z., Luo, W., Zhang, K., Zhan, D.-C., et al. Parrot:\nMultilingual visual instruction tuning. In Forty-second\nInternational Conference on Machine Learning, 2025.\nWang, C., Wang, Z., Xu, X., Tang, Y., Zhou, J., and Lu, J. Q-\nvlm: Post-training quantization for large vision-language\nmodels.\nAdvances in Neural Information Processing\nSystems, 37:114553–114573, 2024a.\nWang, W., Ding, L., Zeng, M., Zhou, X., Shen, L., Luo, Y.,\nYu, W., and Tao, D. Divide, conquer and combine: A\ntraining-free framework for high-resolution image percep-\ntion in multimodal large language models. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 39, pp. 7907–7915, 2025a.\nWang, Y., Sun, Z., Zhang, J., Xian, Z., Biyik, E., Held,\nD., and Erickson, Z. Rl-vlm-f: Reinforcement learn-\ning from vision language foundation model feedback.\narXiv:2402.03681, 2024b.\nWang, Y., Sun, H.-L., Huzhang, G., Chen, Q.-G., Xu, Z.,\nLuo, W., Zhang, K., and Zhang, L. Triplets better than\npairs: Towards stable and effective self-play fine-tuning\nfor llms. In The Thirty-ninth Annual Conference on Neu-\nral Information Processing Systems, 2025b.\nXia, G.-S., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang,\nL., and Lu, X. Aid: A benchmark data set for performance\nevaluation of aerial scene classification. IEEE Transac-\ntions on Geoscience and Remote Sensing, 55:3965–3981,\n2017.\nXu, W., Zhou, Y., Zhou, Y., Cao, Q., Li, S., Bu, J., Liu, B.,\nChen, Y., He, X., Zhao, X., et al. Probing scientific gen-\neral intelligence of llms with scientist-aligned workflows.\narXiv:2512.16969, 2025.\nYang, J., Shi, R., Wei, D., Liu, Z., Zhao, L., Ke, B., Pfister,\nH., and Ni, B. Medmnist v2-a large-scale lightweight\nbenchmark for 2d and 3d biomedical image classification.\nScientific Data, 10(1):41, 2023.\nYang, Y. and Newsam, S. Bag-of-visual-words and spatial\nextensions for land-use classification. In Proceedings\nof the 18th SIGSPATIAL international conference on ad-\nvances in geographic information systems, 2010.\nYuan, H., Chen, Z., Ji, K., and Gu, Q. Self-play fine-tuning\nof diffusion models for text-to-image generation. Ad-\nvances in Neural Information Processing Systems, 37:\n73366–73398, 2024.\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S.,\nand Huang, G. Does reinforcement learning really incen-\ntivize reasoning capacity in llms beyond the base model?\narXiv:2504.13837, 2025.\nZang, Y., Li, W., Han, J., Zhou, K., and Loy, C. C. Con-\ntextual object detection with multimodal large language\nmodels. International Journal of Computer Vision, 133\n(2):825–843, 2025.\n10\n"}, {"page": 11, "text": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning\nZhang, J., Huang, J., Yao, H., Liu, S., Zhang, X., Lu, S.,\nand Tao, D. R1-vl: Learning to reason with multimodal\nlarge language models via step-wise group relative policy\noptimization. arXiv:2503.12937, 2025a.\nZhang, J., Zhang, C., Lu, J., and Zhao, Y. Domain-specific\nlarge language models for fault diagnosis of heating, ven-\ntilation, and air conditioning systems by labeled-data-\nsupervised fine-tuning.\nApplied Energy, 377:124378,\n2025b.\nZhang, X., Sun, H., Zhang, Y., Feng, K., Lu, C., Yang,\nC., and Meng, H. Critique-grpo: Advancing llm rea-\nsoning with natural language and numerical feedback.\narXiv:2506.03106, 2025c.\nZhang, Y., Tzeng, E., Du, Y., and Kislyuk, D. Large-scale\nreinforcement learning for diffusion models. In Euro-\npean Conference on Computer Vision, pp. 1–17. Springer,\n2024.\nZhou, W., Newsam, S., Li, C., and Shao, Z. Patternnet: A\nbenchmark dataset for performance evaluation of remote\nsensing image retrieval. ISPRS journal of photogramme-\ntry and remote sensing, 145:197–209, 2018.\n11\n"}]}