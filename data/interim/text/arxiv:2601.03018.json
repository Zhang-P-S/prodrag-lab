{"doc_id": "arxiv:2601.03018", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.03018.pdf", "meta": {"doc_id": "arxiv:2601.03018", "source": "arxiv", "arxiv_id": "2601.03018", "title": "Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured Clinical Notes for Real-World Dementia Prognosis", "authors": ["Choonghan Kim", "Hyunmin Hwang", "Hangeol Chang", "Jaemin Kim", "Jinse Park", "Jae-Sung Lim", "Jong Chul Ye"], "published": "2026-01-06T13:44:04Z", "updated": "2026-01-06T13:44:04Z", "summary": "While Large Language Models (LLMs) have shown strong performance on clinical text understanding, they struggle with longitudinal prediction tasks such as dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple visits. Standard supervised training lacks explicit annotations for symptom evolution, while direct Reinforcement Learning (RL) is hindered by sparse binary rewards. To address this challenge, we introduce Dementia-R1, an RL-based framework for longitudinal dementia prognosis from unstructured clinical notes. Our approach adopts a Cold-Start RL strategy that pre-trains the model to predict verifiable clinical indices extracted from patient histories, enhancing the capability to reason about disease progression before determining the final clinical status. Extensive experiments demonstrate that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets. Notably, on the ADNI benchmark, our 7B model rivals GPT-4o, effectively capturing fluctuating cognitive trajectories. Code is available at https://anonymous.4open.science/r/dementiar1-CDB5", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.03018v1", "url_pdf": "https://arxiv.org/pdf/2601.03018.pdf", "meta_path": "data/raw/arxiv/meta/2601.03018.json", "sha256": "a085426a6340da86b7cff850f0ee860bda1e0da92d00614da91586c55817dd03", "status": "ok", "fetched_at": "2026-02-18T02:23:09.904048+00:00"}, "pages": [{"page": 1, "text": "Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured\nClinical Notes for Real-World Dementia Prognosis\nChoonghan Kim1,‚àó\nHyunmin Hwang1,‚àó\nHangeol Chang1,‚àó\nJaemin Kim1,‚àó\nJinse Park2\nJae-Sung Lim3,‚Ä†\nJong Chul Ye1,‚Ä†\n1Graduate School of AI, KAIST, Republic of Korea\n2Haeundae Paik Hospital, Inje University, Republic of Korea\n3Asan Medical Center, University of Ulsan College of Medicine, Republic of Korea\n{choonghankim, hyunmin_hwang, hangeol, kjm981995}@kaist.ac.kr\njinsepark@gmail.com\njaesunglim@amc.seoul.kr\njong.ye@kaist.ac.kr\n‚àóEqual contribution\n‚Ä†Corresponding author\nAbstract\nWhile Large Language Models (LLMs) have\nshown strong performance on clinical text un-\nderstanding, they struggle with longitudinal\nprediction tasks such as dementia prognosis,\nwhich require reasoning over complex, non-\nmonotonic symptom trajectories across mul-\ntiple visits. Standard supervised training lacks\nexplicit annotations for symptom evolution,\nwhile direct Reinforcement Learning (RL) is\nhindered by sparse binary rewards.\nTo ad-\ndress this challenge, we introduce Dementia-\nR1, an RL-based framework for longitudinal\ndementia prognosis from unstructured clinical\nnotes. Our approach adopts a Cold-Start RL\nstrategy that pre-trains the model to predict\nverifiable clinical indices extracted from pa-\ntient histories, enhancing the capability to rea-\nson about disease progression before determin-\ning the final clinical status. Extensive experi-\nments demonstrate that Dementia-R1 achieves\nan F1 score of 77.03% on real-world unstruc-\ntured clinical datasets. Notably, on the ADNI\nbenchmark, our 7B model rivals GPT-4o, effec-\ntively capturing fluctuating cognitive trajecto-\nries. Code is available at https://anonymous.\n4open.science/r/dementiar1-CDB5.\n1\nIntroduction\nThe digitalization of healthcare and the widespread\nadoption of Electronic Health Records (EHRs)\nhave resulted in massive amounts of longitudinal\npatient data that capture individuals‚Äô clinical his-\ntories across months or years. However, approx-\nimately 80% of EHR data is recorded as unstruc-\ntured text, including physician notes and imaging\nreports (Kong, 2019; Jensen et al., 2012). These\nnarratives contain rich descriptions of symptom\nevolution and clinical assessments, yet temporal\nchanges are often documented implicitly rather\nthan in structured form. Since many clinical out-\ncomes are defined retrospectively based on how\na patient‚Äôs condition evolves over time, effective\nFigure 1: Multi-dimensional Performance Profile.\nDementia-R1 demonstrates a consistent and balanced\nperformance gain across all dimensions, including inter-\nmediate clinical reasoning tasks (e.g., MMSE, CDR-SB,\nADAS-Cog) and the final dementia prognosis (F1-score)\nmodeling requires longitudinal analysis rather than\nreliance on information from a single visit. De-\nspite this need, most existing longitudinal disease\nmodeling frameworks are designed for structured\ndata representations and therefore struggle to sys-\ntematically incorporate unstructured clinical narra-\ntives (Waxler et al., 2025; Steinberg et al., 2024;\nShmatko et al., 2025).\nRecent advances in Large Language Models\n(LLMs) have demonstrated strong capabilities in\nunderstanding unstructured medical text for clinical\ndecision support (Wachter and Brynjolfsson, 2024;\nSilcox et al., 2024).\nIn particular, LLM-based\nmethods achieve impressive performance on static,\nsnapshot-style benchmarks such as MedQA (Jin\net al., 2021), where inputs represent isolated clini-\ncal scenarios (Singhal et al., 2025). However, such\nbenchmarks largely ignore longitudinal disease pro-\ngression. This limitation is critical for diseases\n1\narXiv:2601.03018v1  [cs.CL]  6 Jan 2026\n"}, {"page": 2, "text": "characterized by slow and cumulative progression,\nsuch as dementia, where diagnosis requires inte-\ngrating evidence of cognitive and functional de-\ncline across multiple clinical encounters (Grand\net al., 2011; Borson et al., 2013; Knopman and\nPetersen, 2014). Crucially, these trajectories are of-\nten non-monotonic; clinical status may fluctuate or\ntemporarily improve, necessitating a holistic evalu-\nation of the patient‚Äôs condition rather than simple\nonset detection. In real-world practice, these longi-\ntudinal signals are predominantly documented in\nunstructured clinical notes rather than standardized\nfields, making dementia a particularly challeng-\ning testbed for longitudinal reasoning over clinical\ntext (Kruse et al., 2025a).\nTo\naddress\nthis\nchallenge,\nwe\nintroduce\nDementia-R1, a framework designed for longitudi-\nnal reasoning using LLMs through Reinforcement\nLearning (RL). We focus on dementia prognosis\nas a representative task of complex longitudinal\ndisease progression. Unlike acute diseases, demen-\ntia diagnosis requires tracking longitudinal cogni-\ntive and functional changes over months or years.\nThese signals are described in clinical narratives,\nyet they are difficult to quantify explicitly (Borson\net al., 2013; Knopman and Petersen, 2014). While\nstandard Supervised Fine-Tuning (SFT) optimizes\nmodels to directly predict final labels, RL-based\nfine-tuning enables the model to learn reasoning\nprocesses before making a prediction (DeepSeek-\nAI, 2025; Shao et al., 2024), making it a natural\nfit for longitudinal clinical inference. However, di-\nrectly applying RL to a high-level binary prognosis\ntask (e.g., Dementia vs. Non-Dementia) is chal-\nlenging due to the sparsity of the reward signal and\nthe implicit nature of the underlying reasoning.\nWe address this issue through a Cold-Start RL\nstrategy with verifiable clinical rewards. Prior work\ntypically relies on SFT to introduce step-wise ratio-\nnales explicitly (Chen and et al., 2024; DeepSeek-\nAI, 2025). However, in the context of dementia\nprognosis, constructing rational trajectories is par-\nticularly challenging. Longitudinal reasoning re-\nquires temporally consistent analysis across multi-\nple visits and substantial effort from clinical experts\nto validate them (Kruse et al., 2025a). To miti-\ngate these challenges, we adopt an RL-based pre-\ntraining stage using clinically established indices as\nreward signals rather than explicit reasoning anno-\ntations. Specifically, we train the model to predict\nscores measured at each visit, such as the Mini-\nMental State Examination (MMSE) (Folstein et al.,\n1975), Global Deterioration Scale (GDS) (Reis-\nberg et al., 2022), and Clinical Dementia Rating\n(CDR) (Morris, 1993). By inferring these indices\nfrom longitudinal unstructured notes, the model au-\ntonomously acquires essential reasoning primitives,\nwhich are subsequently refined in a second stage\nfor the final dementia prediction task.\nWe validate our approach on both real-world un-\nstructured clinical notes from the Asan Medical\nCenter (AMC) real-world cohort and the structured\nbenchmark (ADNI) (Jack Jr et al., 2008). As illus-\ntrated in Figure 1, our model demonstrates compre-\nhensive multi-dimensional reasoning capabilities\ncompared to baselines. Our contributions are as\nfollows:\n‚Ä¢ We propose Dementia-R1, an RL-based frame-\nwork that enables explicit temporal reasoning\non unstructured clinical notes to predict de-\nmentia prognosis.\n‚Ä¢ We introduce a Cold-Start RL method using\nverifiable rewards, demonstrating that learn-\ning to estimate intermediate clinical scores is\ncrucial for an accurate dementia prognosis.\n‚Ä¢ We validate our approach on both private real-\nworld unstructured datasets and a public struc-\ntured benchmark, demonstrating consistent\nimprovements over the strong baselines, in-\ncluding general-purpose LLMs and medical-\nspecialized reasoning models.\n2\nRelated Work\nLongitudinal Clinical Modeling.\nTraditional ap-\nproaches for longitudinal disease modeling have\nprimarily focused on structured electronic health\nrecords (EHRs), utilizing Recurrent Neural Net-\nworks (RNNs) to process temporal sequences\nof medical codes (Choi et al., 2016).\nRecent\nTransformer-based models have advanced longi-\ntudinal forecasting by leveraging large-scale struc-\ntured records for tasks such as time-to-events pre-\ndiction (Steinberg et al., 2024), disease trajectory\nmodeling (Shmatko et al., 2025), and medical\nevents modeling (Waxler et al., 2025). While these\nmodels show effectiveness for structured data, they\nfail to capture the nuanced behavioral and symp-\ntomatic descriptions found in unstructured clinical\nnotes, which constitute the majority of EHR data.\nRecent works such as NYUTron (Jiang et al., 2023)\nand CARE-AD (Li et al., 2025) have demonstrated\n2\n"}, {"page": 3, "text": "Think\n1. Trajectory: 24 ‚Üí 21 \n(Decline). \n2. Interval: 6 month passed.\n3. Prediction: Decline continues.\n    Estimated ~ 19.\nAnswer\n\\boxed{19}\nStage 1: Cold-Start Pre-training\n‚Ä¶\n<Objective>\n[2022-02] MMSE 24.\n[2022-04] GDS 3.\n[2022-05] MMSE 21, ApoE4(+)\n<Assessment>\n[2022-05] Prodromal AD likely.\n‚Ä¶\nLLM\nClinical Notes History ùìó\n‚Ä¶\n<Objective>\n[2022-02] MMSE 24.\n[2022-04] GDS 3.\n[2022-05] MMSE 21, ApoE4(+)\n[2022-11] MMSE 19.\n<Assessment>\n[2022-05] Prodromal AD likely.\n[2022-11] Dementia\n‚Ä¶\nClinical Notes History ùìó<ùíï\nAnchor: [2022-11]\nTarget MMSE: 19\nExtracted Clinical Scores ùíîùíï\nGRPO Update\nStage 2: Task Fine-tuning\nDementia-R1\n‚Ä¶\n<Objective>\n[2022-02] MMSE 24.\n[2022-04] GDS 3.\n[2022-05] MMSE 21, ApoE4(+)\n<Assessment>\n[2022-05] Prodromal AD likely.\n‚Ä¶\nClinical Notes History ùìó<ùíï\nGRPO Update\nDementia Label ùíö\nAnchor: [2022-11]\nLabel: Dementia\nThink\n1. Trajectory: 24 ‚Üí 21 \n(Decline). \n2. Interval: 6 month passed.\n3. Prediction: Decline continues.\n    Estimated ~ 19.\nAnswer\n\\boxed{19}\nThink\n1. Trajectory: 24 ‚Üí 21 \n(Decline). \n2. Interval: 6 month passed.\n3. Prediction: Decline continues.\n    Estimated ~ 19.\nAnswer\n\\boxed{19}\nThink\n1. Trajectory: 24 ‚Üí 21 \n(Decline). \n2. Interval: 6 month passed.\n3. Prediction: Decline continues.\n    Estimated ~ 19.\nAnswer\n\\boxed{19}\nThink\n1. Trajectory: 24 ‚Üí 21 \n(Decline). \n2. Interval: 6 month passed.\n3. Prediction: Decline continues.\n    Estimated ~ 19.\nAnswer\n\\boxed{19}\nThink\n1. MMSE: 24 ‚Üí 21 (Dropped).\n2. Risk: ApoE4+, GDS 4\n3. Age: 65\n4. Prognosis: Likely (1) \n‚Ä¶\nAnswer\nDementia\nFigure 2: Overview of the Dementia-R1 Framework. The pipeline consists of two phases: Stage 1: Cold-Start\nPre-training, where the base model learns longitudinal reasoning via GRPO on forecasting tasks; and Stage 2:\nTask Fine-tuning, where the reasoning-aligned model is adapted for the final dementia prediction task.\nthe potential of LLMs for longitudinal prediction\nusing unstructured clinical text. However, these\napproaches primarily optimize for final clinical out-\ncomes and do not explicitly train models to reason\nover intermediate disease trajectories or temporal\nprogression patterns. As a result, current frame-\nworks for unstructured clinical text still lack mech-\nanisms for explicit longitudinal reasoning (Kruse\net al., 2025b), motivating our approach.\nReasoning Capabilities of Medical LLMs.\nThe\nreasoning capabilities of LLMs in the medical do-\nmain have been largely enhanced through Chain-of-\nThought (CoT) prompting, which encourages mod-\nels to generate intermediate rationales (Wei et al.,\n2022). HuatuoGPT-o1 (Chen and et al., 2024) fur-\nther improves medical reasoning by combining Su-\npervised Fine-Tuning (SFT) on reasoning trajecto-\nries with Reinforcement Learning (RL). In the gen-\neral domain, recent advances have shifted from SFT\nto RL with Verifiable Rewards (RLVR), demon-\nstrating that models can learn reasoning when the\nreward is easily verifiable (DeepSeek-AI, 2025).\nHowever, applying this paradigm to clinical tasks\nremains challenging due to the sparsity of the re-\nward signal and the implicit nature of the required\nreasoning steps. C-Reason (Kim et al., 2025) par-\ntially addresses this challenge using Group Relative\nPolicy Optimization (GRPO) (Shao et al., 2024) for\nsepsis management via masked value prediction;\nhowever, it does not address long-term disease pro-\ngression. We extend this line of work to longitu-\ndinal dementia prediction by training the model\nto track disease progression by estimating clinical\nscores before determining the final prognosis.\n3\nMethodology: Dementia-R1\nGiven a sequence of unstructured clinical notes\nH = {x1, x2, . . . , xt}, we formulate the task as\ndetermining the final clinical status y ‚àà{0, 1} at\na target anchor Tanchor, conditioned on the pa-\ntient‚Äôs history H<T = {xi|i < T, xi ‚ààH}. This\napproach requires distinguishing temporary fluc-\ntuations from persistent decline across the trajec-\ntory, rather than assuming simple linear progres-\nsion. To enable explicit reasoning over disease\nprogression, we employ a two-stage reinforcement\nlearning framework utilizing Group Relative Pol-\nicy Optimization (GRPO) (Shao et al., 2024) with\nverifiable clinical rewards (see Figure 2).\n3.1\nConstructing Verifiable Pretraining Data\nSince raw unstructured text lacks explicit ground\ntruth for longitudinal reasoning, we construct a\npre-training dataset paired with verifiable clinical\nindices. We employ a strong auxiliary LLM as an\nextractor E to parse unstructured notes into struc-\ntured clinical scores:\nst = E(xt),\nst ‚ààS\n(1)\nwhere S represents the set of target indices:\nMMSE (0‚Äì30), GDS (1‚Äì7), and CDR (0‚Äì3). Using\nthese extracted values as ground truth, we generate\na pre-training dataset Dpre = {(H<t, st)} where\nthe model is trained to forecast the score st at the\ntarget visit based on the preceding history H<t. To\nprevent data leakage, patients reserved for the final\ndementia prognosis test set are strictly excluded\nfrom this phase.\n3.2\nStage 1: Cold-Start Pre-training\nIn this stage, we align the model to reason about\nclinical trajectories by optimizing it to predict the\nextracted scores st from Dpre. We utilize GRPO,\nwhich eliminates the need for a value function by\nestimating the baseline from a group of outputs.\nVerifiable Reward Function (Rcold)\nTo accom-\nmodate the varying granularity of clinical scales,\nwe define a tolerance-aware reward function. Let\n3\n"}, {"page": 4, "text": "ÀÜst be the predicted score from the output ot of the\nLLM, st be the ground truth, and Œ¥ be the allow-\nable error margin. Considering the range of the\nMMSE score (0‚Äì30), we set a tolerance of Œ¥ = 2,\ntreating predictions within this range as correct.\nFor coarser scales like GDS and CDR, we enforce\nexact matching by setting Œ¥ = 0. The reward is\ndefined as:\nRcold = I (|ÀÜst ‚àíst| ‚â§Œ¥) ,\n(2)\nwhere I(¬∑) is the indicator function that returns 1 if\nthe score is met and 0 otherwise.\nOptimization Objective\nFor each input query qt\nwith clinical history H<t, we sample a group of G\noutputs {o1\nt , o2\nt , . . . , oG\nt } from the old policy œÄŒ∏old.\nThe policy is optimized to maximize the following:\nL(Œ∏) = Eqt,{oi\nt}\n\"\n1\nG\nG\nX\ni=1\nmin\n \nœÄŒ∏(oi\nt|qt)\nœÄŒ∏old(oi\nt|qt)Ai,\nclip\n \nœÄŒ∏(oi\nt|qt)\nœÄŒ∏old(oi\nt|qt), 1 ‚àíœµ, 1 + œµ\n!\nAi\n!\n‚àíŒ≤DKL\n#\n.\n(3)\nHere, Œ≤DKL controls the KL-regularization term,\nœµ is the clipping hyperparameter and Ai is the ad-\nvantage computed by group-based normalization:\nAi =\nRcold(oi\nt) ‚àímean({Rcold(oj\nt)}G\nj=1)\nstd({Rcold(oj\nt)}G\nj=1)\n.\n(4)\nThis stabilizes training and encourages the model\nto generate reasoning paths that outperform the\naverage of its own samples.\n3.3\nStage 2: Task Fine-tuning\nAfter Cold-Start pre-training (Stage 1), we then\nfine-tune the model on the downstream prognostic\nclassification task (Dementia vs. Non-Dementia)\nusing the same GRPO framework in Eq. (3).\nSparse Reward Function (Rtask)\nUnlike the\ngranular scores in Stage 1, the final diagnosis is\nbinary. Therefore, the reward is defined as:\nRtask =\n(\n1,\nif prediction is correct\n0,\nif prediction is incorrect\n(5)\nAlthough this reward signal is sparse, the reasoning\ncapabilities acquired in Stage 1 allow the model to\nenhance the capability to reason about longitudi-\nnal disease progression. In this training stage, the\nmodel is optimized for the final prognostic accu-\nracy by generating reasoning traces.\n4\nExperimental Setup\n4.1\nDatasets\nWe validate the efficacy of Dementia-R1 on two\ndistinct cohorts: the real-world unstructured clin-\nical notes from Asan Medical Center (AMC) and\nthe structured Alzheimer‚Äôs Disease Neuroimaging\nInitiative (ADNI) benchmark.\n4.1.1\nData Sources and Processing\nReal-World Unstructured Cohort (AMC).\nWe\nconstructed a large-scale longitudinal dataset us-\ning raw clinical notes from Asan Medical Cen-\nter (AMC). Clinical data were retrospectively col-\nlected from approximately 3,000 patients diag-\nnosed with neurocognitive disorders between Jan-\nuary 1, 2021, and September 30, 2023.\nInclu-\nsion criteria were based on ICD-10 codes covering\nAlzheimer‚Äôs disease, vascular dementia, and mild\ncognitive impairment. Electronic Medical Records\n(EMRs) covering initial and follow-up visits were\nreviewed to extract SOAP-formatted notes. To en-\nsure privacy, all personally identifiable information\nwas anonymized. Since the target clinical indices\n(MMSE, CDR, GDS) are predominantly embedded\nwithin the free-text ‚ÄúObjective‚Äù section, we utilized\nthe LLM-based extraction pipeline (described in\nSec 3.1) to isolate these values as verifiable re-\nwards.\nStructured Benchmark Cohort (ADNI).\nTo\ndemonstrate generalizability, we employed the\nADNI dataset (Jack Jr et al., 2008), a widely recog-\nnized benchmark for Alzheimer‚Äôs research. Unlike\nAMC, ADNI consists of structured tabular records.\nTo adapt this for our LLM-based framework, we ap-\nplied linearization, transforming tabular rows into\nchronological textual logs. For verifiable rewards,\nwe selected seven clinically significant indices (e.g.,\nMMSE, CDR-SB) via neurological consultation\nand feature analysis (Gelir et al., 2025), applying\nstandardized proportional tolerance thresholds (de-\ntails in Appendix A.3.2).\n4.1.2\nLongitudinal Sample Construction\nTo handle the fluctuating nature of cognitive de-\ncline across both modalities, we applied a unified\nconstruction protocol defined by three key compo-\nnents (illustrated in Figure 3):\n‚Ä¢ Target Anchor: The patient‚Äôs last clinical\nvisit with a confirmed assessment. The model\nutilizes the full aggregated history prior to\n4\n"}, {"page": 5, "text": "Figure 3: Examples of Longitudinal Sample Con-\nstruction. Patient history is retrospectively sliced rel-\native to a Target Anchor, applying the unified protocol\nacross both unstructured (AMC) and structured (ADNI)\ndata.\nthis anchor to distinguish between persistent\ndeterioration and temporary fluctuations.\n‚Ä¢ Prediction Target: The ground-truth out-\ncome varies by training stage:\n‚Äì Stage 1 (Pre-training): Verifiable clini-\ncal indices (extracted scores for AMC;\nstandardized metrics for ADNI).\n‚Äì Stage 2 (Fine-tuning):\nThe final bi-\nnary diagnosis, defined as neurologist-\nadjudicated labels for AMC and stan-\ndardized DX outcomes (Dementia vs.\nNon-Dementia) for ADNI.\n‚Ä¢ Gap Bucket: To model temporal sensitivity,\nwe discretized the interval between the last\ninput note and the target anchor:\n‚Äì Stage 1: Fine-grained 1-month incre-\nments (e.g., 0‚Äì1m, . . . , 23‚Äì24m).\n‚Äì Stage 2: Coarser intervals (e.g., 6‚Äì12m)\nto ensure clinical utility, excluding short-\nterm gaps (<6m).\n‚Äì ADNI Adaptation:\nAdopting AMC‚Äôs\nstrategy, intervals beyond 24 months\nwere consolidated into a single bucket\n(>24m) to accommodate longer observa-\ntion periods.\n4.1.3\nData Splitting and Leakage Prevention\nTo prevent data leakage, we implemented a strict\nPatient-Level splitting protocol governed by three\nprinciples:\n1. Patient-Level Isolation: Data is split by Pa-\ntient ID to strictly prevent overlap between\ntraining and test sets.\n2. Holistic Test Set Exclusion: Patients re-\nserved for the Stage 2 test set are excluded\nFigure 4: Dataset Overview. Visualization of sample\nand patient counts. Training sets are balanced to prevent\nbias, while test sets retain natural patient prevalence.\nfrom Stage 1 pre-training to ensure full blind-\nness.\n3. Future Information Exclusion: We aggre-\ngate all notes recorded prior to the target an-\nchor, ensuring predictions rely solely on his-\ntorical symptom trajectories.\nUnder this protocol, we use a balanced training set\n(1:1) while retaining natural prevalence in the test\nset (see Figure 4).\n4.2\nBaselines\nWe evaluate six configurations based on Qwen2.5-\n7B-Instruct (Team, 2024) to validate the efficacy of\nour pure RL pipeline:\n‚Ä¢ Zero-shot CoT: Base model prompted with\nChain-of-Thought to elicit reasoning without\ntraining.\n‚Ä¢ SFT on single stage: Standard Supervised\nfine-tuning directly at each stage. Training\nutilizes Chain-of-Thought rationales distilled\nfrom a teacher model.\n‚Ä¢ GRPO on single stage: GRPO applied di-\nrectly to the prediction task at each stage.\n‚Ä¢ SFT ‚ÜíSFT: A multi-stage SFT pipeline con-\nsisting of pre-training on clinical indices fol-\nlowed by fine-tuning on diagnosis, serving as\na supervised counterpart to our method.\n‚Ä¢ SFT ‚ÜíGRPO: The conventional RLHF\npipeline consisting of SFT warm-up on clini-\ncal indices followed by GRPO fine-tuning.\n5\n"}, {"page": 6, "text": "Table 1: Experimental Results on Asan Medical Center Dataset. We compare Dementia-R1 against general-\npurpose LLMs and medical-specific models. Bold and underline indicate the best and second-best performance. All\nresults represent mean ¬± standard deviation across five random seeds.\nMethod\nSize\nAccuracy (‚Üë)\nPrecision (‚Üë)\nRecall (‚Üë)\nF1 score (‚Üë)\nExternal LLMs\nHuatuoGPT-o1\n8B\n67.19 ¬± 1.3\n71.55 ¬± 1.5\n58.99 ¬± 1.6\n64.67 ¬± 1.5\nQwen2.5-7B-Inst\n7B\n71.94 ¬± 0.8\n72.82 ¬± 0.7\n71.60 ¬± 1.1\n72.20 ¬± 0.8\nQwen2.5-32B-Inst\n32B\n61.99 ¬± 0.7\n57.65 ¬± 0.4\n95.46 ¬± 0.7\n71.89 ¬± 0.4\nSpecialized Models\nSFT\nw/o Stage 1\n7B\n74.01 ¬± 1.0\n72.21 ¬± 1.0\n79.58 ¬± 1.0\n75.72 ¬± 0.9\nGRPO w/o Stage 1\n7B\n74.10 ¬± 0.9\n70.96 ¬± 0.9\n83.15 ¬± 1.0\n76.57 ¬± 0.8\nSFT\nw/o Stage 2\n7B\n65.60 ¬± 0.8\n61.55 ¬± 0.6\n86.43 ¬± 1.1\n71.90 ¬± 0.6\nGRPO w/o Stage 2\n7B\n72.47 ¬± 0.9\n70.28 ¬± 0.8\n79.58 ¬± 0.9\n74.64 ¬± 0.8\nSFT ‚ÜíSFT\n7B\n75.14 ¬± 0.6\n73.43 ¬± 0.6\n80.21 ¬± 0.6\n76.67 ¬± 0.6\nSFT ‚ÜíGRPO\n7B\n73.26 ¬± 0.6\n70.39 ¬± 0.6\n82.24 ¬± 0.8\n75.85 ¬± 0.6\nDementia-R1\n7B\n74.93 ¬± 0.7\n72.19 ¬± 0.6\n82.56 ¬± 1.1\n77.03 ¬± 0.7\nTable 2: Generalization Results on ADNI Benchmark. Comparison extended to include strong ML baselines\n(Random Forest) and state-of-the-art proprietary models (GPT-4o). Notation and experimental settings follow\nTable 1 (highlighting performance within the LLM category).\nModel Method\nSize\nAccuracy (‚Üë)\nPrecision (‚Üë)\nRecall (‚Üë)\nF1 score (‚Üë)\nML Baseline\nRandom Forest\n‚Äî\n83.46 ¬± 0.6\n83.57 ¬± 0.7\n77.13 ¬± 1.1\n80.22 ¬± 0.7\nExternal LLMs\nGPT-4o\n‚Äî\n81.39 ¬± 0.7\n86.94 ¬± 1.8\n67.64 ¬± 1.8\n76.05 ¬± 1.1\nGPT-4o-mini\n‚Äî\n75.76 ¬± 0.8\n73.04 ¬± 1.3\n70.64 ¬± 0.0\n71.82 ¬± 0.0\nHuatuoGPT-o1\n8B\n63.11 ¬± 1.3\n56.15 ¬± 1.3\n71.59 ¬± 1.2\n62.93 ¬± 1.0\nQwen2.5-7B-Inst\n7B\n61.54 ¬± 0.8\n54.53 ¬± 0.7\n72.36 ¬± 1.6\n62.19 ¬± 0.9\nQwen2.5-32B-Inst\n32B\n76.47 ¬± 0.6\n71.82 ¬± 1.1\n76.05 ¬± 1.1\n73.86 ¬± 0.6\nSpecialized Models\nSFT\nw/o Stage 1\n7B\n75.65 ¬± 1.5\n70.90 ¬± 1.8\n75.19 ¬± 2.3\n72.97 ¬± 1.6\nGRPO w/o Stage 1\n7B\n76.32 ¬± 0.5\n74.08 ¬± 1.0\n70.56 ¬± 1.1\n72.26 ¬± 0.5\nSFT\nw/o Stage 2\n7B\n69.68 ¬± 2.2\n62.49 ¬± 2.0\n76.65 ¬± 2.7\n68.85 ¬± 2.2\nGRPO w/o Stage 2\n7B\n67.39 ¬± 1.1\n60.19 ¬± 1.0\n75.02 ¬± 1.3\n66.79 ¬± 1.1\nSFT ‚ÜíSFT\n7B\n76.32 ¬± 0.9\n74.64 ¬± 1.3\n69.44 ¬± 0.9\n71.95 ¬± 1.1\nSFT ‚ÜíGRPO\n7B\n76.25 ¬± 0.9\n71.10 ¬± 1.4\n77.00 ¬± 0.6\n73.92 ¬± 0.8\nDementia-R1\n7B\n76.77 ¬± 1.4\n70.99 ¬± 1.7\n79.31 ¬± 1.8\n74.91 ¬± 1.5\n‚Ä¢ ML-based Baseline: Random Forest, se-\nlected as the top-performing traditional al-\ngorithm on ADNI. Unlike LLMs, it is re-\nstricted to the most recent visit due to its in-\nability to handle variable-length longitudinal\nsequences.\nOur proposed method, Dementia-R1 (GRPO ‚Üí\nGRPO), represents a pure reinforcement learning\napproach and is compared against these baselines.\n4.3\nImplementation Details\nSFT.\nWe conducted Supervised Fine-Tuning\n(SFT) via knowledge distillation using Qwen2.5-\n32B-Instruct-AWQ. To construct the training\ndataset, we prompted the teacher model to gener-\nate Chain-of-Thought (CoT) rationales by reverse-\nengineering the ground-truth labels from the clini-\ncal notes. The student model was then fine-tuned\non these concatenated (Question, Patient Note,\nCoT, and Answer) sequences for three epochs with\na per-device batch size of 2.\nDementia-R1.\nWe train Dementia-R1 using\nGroup Relative Policy Optimization (GRPO) with\na group size of G = 8 and an effective batch size\nof 8. Detailed training configurations and hardware\nspecifications are provided in Appendix A.8.\nEvaluation protocol.\nTo ensure statistical reli-\nability, we conducted all experiments across five\ndistinct random seeds. Consequently, all reported\nresults represent the mean performance ¬± standard\ndeviation.\n5\nResults\n5.1\nReal-World Unstructured Data Results\nDementia prognosis.\nTable 1 presents the com-\nparative performance on the Asan Medical Cen-\n6\n"}, {"page": 7, "text": "Table 3: Performance on Clinical Index Prediction\nfor the AMC cohort. We evaluate the accuracy of\npredicting MMSE, GDS, and CDR scores.\nModel Method\nMMSE\nGDS\nCDR\nAverage\nQwen2.5-32B-Inst\n57.9 ¬± 0.3\n46.1 ¬± 0.3\n69.8 ¬± 0.3\n57.9 ¬± 0.1\nQwen2.5-7B-Inst\n56.1 ¬± 0.7\n45.1 ¬± 0.1\n62.8 ¬± 0.7\n54.7 ¬± 0.3\nSFT ‚ÜíSFT\n52.2 ¬± 0.2\n38.9 ¬± 0.5\n64.1 ¬± 1.6\n51.7 ¬± 0.5\nSFT ‚ÜíGRPO\n54.3 ¬± 0.5\n43.5 ¬± 0.6\n69.7 ¬± 0.8\n55.8 ¬± 0.5\nDementia-R1\n57.3 ¬± 0.3\n47.7 ¬± 0.4\n73.9 ¬± 1.1\n59.6 ¬± 0.5\nter (AMC) dataset, which consists of real-world,\nunstructured clinical narratives.\nDementia-R1\nachieves the highest F1 score of 77.03%, highlight-\ning the effectiveness of our framework. Specifi-\ncally, Dementia-R1 outperforms the GRPO base-\nline (GRPO w/o Stage 1: 76.57%), indicating that\navoiding the sparse reward problem with verifiable\nclinical indices effectively contributes to perfor-\nmance improvement. Furthermore, our pipeline ex-\nceeds the standard hybrid approach (SFT ‚ÜíGRPO,\n75.85%), indicating that active exploration in RL-\nbased pre-training (Stage 1) facilitates more effec-\ntive modeling of symptom trajectories than super-\nvised fine-tuning.\nClinical index prediction.\nBeyond categorical\ndementia classification, we further evaluate the\nmodel‚Äôs reasoning capability through quantitative\nclinical index prediction on the AMC cohort. As\nshown in Table 3, Dementia-R1 achieves the high-\nest average accuracy (59.61%), surpassing the 7B\nbaselines. Notably, it outperforms the 32B model\non GDS and CDR‚Äìrigorous metrics used by neurol-\nogists for precise disease staging‚Äìwhile maintain-\ning competitive performance on the simpler MMSE\nscreening tool. This capability to infer fine-grained\nseverity demonstrates the model‚Äôs alignment with\nexpert clinical judgment.\n5.2\nGeneralization to Structured Benchmarks\nTo demonstrate the generalizability of our frame-\nwork across different data modalities, we applied\nthe Dementia-R1 methodology to the structured\nADNI benchmark. By training on linearized tab-\nular records as described in Sec 4.1.1, we verify\nwhether our reinforcement learning approach re-\nmains effective on structured data. Table 2 summa-\nrizes the performance. Dementia-R1 achieves an F1\nscore of 74.91%, demonstrating that our framework\nsuccessfully adapts to structured clinical logs. This\nperformance is comparable to substantially larger\nmodels such as GPT-4o (76.05%) and Qwen2.5-\n32B (73.86%).\nTo further probe fine-grained reasoning be-\nyond dementia-level classification, we visualize\nthe multi-dimensional performance in Figure 1.\nDespite having only 7B parameters, Dementia-R1\nmatches or closely approaches the best-performing\nmodels on CDRSB and ADAS scores (see Ap-\npendix Table 11). This confirms that our methodol-\nogy ‚Äì reinforcement learning with verifiable clini-\ncal rewards ‚Äì is not limited to unstructured text but\ngeneralizes effectively to structured data represen-\ntations.\n5.3\nNeurologist Evaluation\nTo validate the clinical utility and reasoning quality\nof Dementia-R1, we conducted a blinded human\nevaluation involving two board-certified neurolo-\ngists. We adopted a pairwise comparison protocol\non a subset of test cases to analyze the alignment of\nthe models‚Äô internal logic with clinical standards.\nThe experts assessed responses across six dimen-\nsions: (1) Temporal Reasoning Accuracy, (2) Evi-\ndence Grounding, (3) Clinically Relevant Evidence\nSelection, (4) Medical Soundness, (5) Complete-\nness of Key Findings, and (6) Overall Clinical Util-\nity. For each comparison, evaluators selected the\nsuperior response (Win) or marked them as equal\n(Tie), restricted to cases where both models pro-\nvided the correct final prognosis. To assess the\nreliability of the human evaluation, we measured\ninter-rater agreement, resulting in a Cohen‚Äôs Kappa\nscore of 0.56, indicating moderate agreement.\nFor this comparative assessment, we selected\nQwen2.5-32B-Instruct as the baseline. Although\nsignificantly larger than our 7B backbone, this\nmodel demonstrated the second-best performance\nin the quantitative clinical index prediction task\n(Section 5.1), surpassing other 7B baselines. This\nselection enables a rigorous investigation into\nwhether our reasoning-aligned framework gener-\nates more clinically valid trajectories than simply\nscaling model parameters.\nAs shown in Figure 5, Dementia-R1 recorded a\n55% win rate in Overall Clinical Utility. Regard-\ning evidence usage, the model obtained 60% win\nrates in both Evidence Grounding and Clinically\nRelevant Evidence Selection. These results sug-\ngest that the proposed two-stage RL framework,\nwhich incorporates Cold-Start pre-training on clini-\ncal indices, enables more clinically grounded lon-\ngitudinal reasoning compared to parameter scaling\nalone. In terms of Temporal Reasoning Accuracy,\nthe model achieved a combined Win/Tie rate of\n95% (40% Win, 55% Tie) against the 32B base-\n7\n"}, {"page": 8, "text": "Figure 5: Neurologist Blind Pairwise Evaluation.\nComparison between Dementia-R1 and the baseline\nmodel across six clinical dimensions.\n0\n5\n10\n15\n20\nEpoch\n0.60\n0.65\n0.70\n0.75\n0.80\nF1 Score\n(a) AMC Cohort\nDementia-R1 (Ours)\nGRPO w/o Stage 1\n0\n5\n10\n15\n20\n25\n30\nEpoch\n0.60\n0.65\n0.70\n0.75\n0.80\n(b) ADNI Cohort\nDementia-R1 (Ours)\nGRPO w/o Stage 1\nFigure 6: Training Dynamics. F1 score trajectories on\n(a) AMC cohort and (b) ADNI cohort. The inclusion\nof Stage 1 leads to significantly faster convergence and\nhigher stability across both unstructured and structured\ndomains.\nline. These findings suggest that the Cold-Start\npre-training can yield clinical reasoning capabili-\nties comparable to those of larger models.\n5.4\nAblation Study\nWe investigate the impact of Stage 1 pre-training on\nlearning dynamics. Figure 6 displays the F1 score\ntrajectories evaluated on the test set for Dementia-\nR1 and a baseline model trained without Stage 1.\nAs observed, the model utilizing Stage 1 shows\nearlier convergence and higher final F1 scores com-\npared to the baseline across both datasets. These\nresults suggest that alignment with verifiable clin-\nical rewards aids in stabilizing the reinforcement\nlearning process in sparse-reward environments.\n5.5\nTemporal Robustness Analysis\nWe evaluate the model‚Äôs robustness across varying\ntemporal intervals between the last clinical note\nand diagnosis, as visualized in Figure 7. Detailed\nnumerical results are provided in Tables 12 and 13.\nIn the AMC cohort, Dementia-R1 shows consistent\nperformance, peaking at the 12‚Äì18 month interval\nwith an F1 score of 79.28%, exceeding the hybrid\nbaseline (SFT ‚ÜíGRPO: 78.00%) and the 32B\nmodel (74.38%). A similar trend is observed on\nFigure 7: Performance across time gaps. Dementia-\nR1 demonstrates consistent stability, especially in long-\nterm predictions, compared to baselines.\nthe ADNI cohort. Specifically, based on F1 scores,\nDementia-R1 outperforms GPT-4o in the 18‚Äì24\nmonth interval (80.30% vs. 78.78%) and main-\ntains higher performance in the >24 month horizon\n(73.11% vs. 71.18%). These findings suggest that\naligning with longitudinal trajectories through ver-\nifiable rewards contributes to sustained reasoning\ncapabilities in long-term forecasting scenarios.\n6\nConclusion\nIn this work, we presented Dementia-R1, a Rein-\nforcement Learning framework designed to infer\nlongitudinal disease progression from unstructured\nclinical narratives. Addressing the limitations of\nsparse rewards in prognostic tasks, we introduced\na Cold-Start RL strategy that aligns the model with\nverifiable clinical indices before fine-tuning for the\nfinal diagnosis. Empirical results on both the real-\nworld AMC cohort and the structured ADNI bench-\nmark demonstrate that our approach enables a 7B\nparameter model to achieve performance compa-\nrable to, or exceeding, that of significantly larger\nbaselines. Furthermore, qualitative evaluations by\nneurologists indicate that explicit training on in-\ntermediate clinical scores fosters more grounded\nand transparent reasoning trajectories. We hope\nthis work inspires further research into reinforce-\nment learning with verifiable rewards for complex,\nlong-horizon clinical decision-making.\n8\n"}, {"page": 9, "text": "Limitations\nWe acknowledge several limitations in our study.\n‚Ä¢ First, regarding data generalization, our un-\nstructured dataset comes from a single institu-\ntion (Asan Medical Center). This may limit\nthe model‚Äôs ability to generalize to other de-\nmographics or documentation styles. Future\nvalidation on diverse, multi-center datasets is\nnecessary.\n‚Ä¢ Second, linguistic limitations may arise from\nthe translation process. Converting Korean\nnotes into English might result in the loss of\nsubtle nuances, such as syntax errors, which\nare important for assessing cognitive decline.\nFuture work should apply our method directly\nto native-language texts.\n‚Ä¢ Third, our framework relies on the perfor-\nmance of the auxiliary Large Language Mod-\nels (LLMs). We utilized the Qwen2.5 series\nfor data preprocessing, including the transla-\ntion of clinical notes and the extraction of clin-\nical scores. Consequently, our reward mecha-\nnism depends on the accuracy of these models;\nsince we use the extracted clinical scores as re-\nwards, any extraction errors or hallucinations\ncould introduce noise into the reinforcement\nlearning process.\n‚Ä¢ Finally, our approach relies on quantifiable\nclinical indices (e.g., MMSE) for rewards.\nThis limits immediate application to diseases\nthat lack standardized numerical records. Ex-\ntending this framework to conditions with sub-\njective or qualitative markers remains a chal-\nlenge for future work.\nEthics Statement\nThis retrospective study was approved by the In-\nstitutional Review Board (IRB No. 2023-1628),\nwhich waived the requirement for informed con-\nsent due to the use of de-identified medical records.\nAll methods were performed in accordance with\nthe relevant guidelines and regulations of the Asan\nMedical Center Ethics Committee and the Decla-\nration of Helsinki. Data used in the preparation\nof this article were obtained from the Alzheimer‚Äôs\nDisease Neuroimaging Initiative (ADNI) database\n(adni.loni.usc.edu). The investigators within ADNI\ncontributed to the design and implementation of\nADNI and/or provided data but did not participate\nin analysis or writing of this report.\nReferences\nSoo Borson, Lori Frank, Peter J Bayley, Malaz Boustani,\nMarge Dean, Pei-Jung Lin, J Riley McCarten, John C\nMorris, David P Salmon, Frederick A Schmitt, and\n1 others. 2013. Improving dementia care: the role\nof screening and detection of cognitive impairment.\nAlzheimer‚Äôs & Dementia, 9(2):151‚Äì159.\nJunying Chen and et al. 2024. Huatuogpt-o1, towards\nmedical complex reasoning with llms. arXiv preprint\narXiv:2412.18925.\nEdward Choi, Mohammad Taha Bahadori, Andy\nSchuetz, Walter F Stewart, and Jimeng Sun. 2016.\nDoctor ai: Predicting clinical events via recurrent\nneural networks. In Machine learning for healthcare\nconference, pages 301‚Äì318. PMLR.\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nsoning capability in llms via reinforcement learning.\nPreprint, arXiv:2501.12948.\nMarshal F Folstein, Susan E Folstein, and Paul R\nMcHugh. 1975.\n‚Äúmini-mental state‚Äù: a practical\nmethod for grading the cognitive state of patients\nfor the clinician. Journal of psychiatric research,\n12(3):189‚Äì198.\nFatih Gelir, Taymaz Akan, Sait Alp, Emrah Ge-\ncili, Md Shenuarin Bhuiyan, Elizabeth A Disbrow,\nSteven A Conrad, John A Vanchiere, Christopher G\nKevil, and Mohammad Alfrad Nobel Bhuiyan. 2025.\nMachine learning approaches for predicting progres-\nsion to alzheimer‚Äôs disease in patients with mild cog-\nnitive impairment. Journal of Medical and Biologi-\ncal Engineering, 45:63‚Äì83.\nJacob HG Grand, Sienna Caspar, and Stuart WS Mac-\nDonald. 2011. Clinical features and multidisciplinary\napproaches to dementia care. Journal of multidisci-\nplinary healthcare, pages 125‚Äì147.\nClifford R Jack Jr, Matt A Bernstein, Nick C Fox, Paul\nThompson, Gene Alexander, Danielle Harvey, Bret\nBorowski, Paula J Britson, Jennifer L. Whitwell,\nChadwick Ward, and 1 others. 2008. The alzheimer‚Äôs\ndisease neuroimaging initiative (adni): Mri methods.\nJournal of Magnetic Resonance Imaging: An Official\nJournal of the International Society for Magnetic\nResonance in Medicine, 27(4):685‚Äì691.\nPeter B Jensen, Lars J Jensen, and S√∏ren Brunak. 2012.\nMining electronic health records: towards better re-\nsearch applications and clinical care. Nature Reviews\nGenetics, 13(6):395‚Äì405.\nLavender Yao Jiang, Xujin Chris Liu, Nima Pour Neja-\ntian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin,\nKevin Eaton, Howard Antony Riina, Ilya Laufer,\nPaawan Punjabi, and 1 others. 2023. Health system-\nscale language models are all-purpose prediction en-\ngines. Nature, 619(7969):357‚Äì362.\n9\n"}, {"page": 10, "text": "Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nJunu Kim, Chaeeun Shim, Sungjin Park, Su Yeon Lee,\nGee Young Suh, Chae-Man Lim, Seong Jin Choi,\nSong Mi Moon, Kyoung-Ho Song, Sejoong Kim, and\n1 others. 2025. Enhancing llms‚Äô clinical reasoning\nwith real-world data from a nationwide sepsis registry.\narXiv preprint arXiv:2505.02722.\nDavid S Knopman and Ronald C Petersen. 2014. Mild\ncognitive impairment and mild dementia: a clinical\nperspective. In Mayo clinic proceedings, volume 89,\npages 1452‚Äì1459. Elsevier.\nHyoun-Joong Kong. 2019. Managing unstructured big\ndata in healthcare system. Healthcare Informatics\nResearch, 25(1):1‚Äì2.\nMaya Kruse, Shiyue Hu, Nicholas Derby, Yifu Wu,\nSamantha Stonbraker, Bingsheng Yao, Dakuo Wang,\nElizabeth Goldberg, and Yanjun Gao. 2025a. Large\nlanguage models with temporal reasoning for lon-\ngitudinal clinical summarization and prediction. In\nFindings of ACL. EMNLP. Conference on Empirical\nMethods in Natural Language Processing, volume\n2025, pages 20715‚Äì20735.\nMaya Kruse, Shiyue Hu, Nicholas Derby, Yifu Wu,\nSamantha Stonbraker, Bingsheng Yao, Dakuo Wang,\nElizabeth Goldberg, and Yanjun Gao. 2025b. Zero-\nshot large language models for long clinical text sum-\nmarization with temporal reasoning. medRxiv, pages\n2025‚Äì07.\nRumeng Li, Xun Wang, Dan Berlowitz, Jesse Mez,\nHonghuang Lin, and Hong Yu. 2025. Care-ad: a\nmulti-agent large language model framework for\nalzheimer‚Äôs disease prediction using longitudinal clin-\nical notes. npj Digital Medicine, 8(1):541.\nJohn C Morris. 1993.\nThe clinical dementia rating\n(cdr) current version and scoring rules. Neurology,\n43(11):2412‚Äì2412.\nBarry Reisberg, Ramu Vadukapuram, and Sunnie\nKenowsky. 2022.\nThe global deterioration scale\n(gds). World Alzheimer Report 2022, page 44.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, and 1 others. 2024. Deepseek-\nmath: Pushing the limits of mathematical reason-\ning in open language models.\narXiv preprint\narXiv:2402.03300.\nArtem Shmatko, Alexander Wolfgang Jung, Kumar Gau-\nrav, and 1 others. 2025. Learning the natural history\nof human disease with generative transformers. Na-\nture.\nChristina Silcox, Eyal Zimlichmann, Katie Huber,\nNeil Rowen, Robert Saunders, Mark McClellan,\nCharles N Kahn III, Claudia A Salzberg, and\nDavid W Bates. 2024. The potential for artificial in-\ntelligence to transform healthcare: perspectives from\ninternational health leaders. NPJ Digital Medicine,\n7(1):88.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\nClark, Stephen R Pfohl, Heather Cole-Lewis, and\n1 others. 2025. Toward expert-level medical ques-\ntion answering with large language models. Nature\nMedicine, 31(3):943‚Äì950.\nEthan Steinberg, Jason Alan Fries, Yizhe Xu, and\nNigam H Shah. 2024. Motor: A time-to-event foun-\ndation model for structured medical records.\nIn\nICLR.\nQwen Team. 2024. Qwen2.5 technical report. arXiv\npreprint arXiv:2412.15115.\nRobert M Wachter and Erik Brynjolfsson. 2024. Will\ngenerative artificial intelligence deliver on its promise\nin health care? Jama, 331(1):65‚Äì69.\nShane Waxler, Paul Blazek, Davis White, and 1 others.\n2025. Generative medical event models improve with\nscale. arXiv preprint arXiv:2508.12104.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\nand 1 others. 2022. Chain-of-thought prompting elic-\nits reasoning in large language models. Advances\nin neural information processing systems, 35:24824‚Äì\n24837.\n10\n"}, {"page": 11, "text": "Table 4: Table A1: Stage 1 (Pretraining) dataset\nstatistics (AMC). Fine-tuning test patients are fully\nexcluded to prevent leakage. A patient-level split is\napplied for Stage 1.\nItem\n#Patients\n#Samples\nOriginal cohort (raw)\n11,163\n‚Äì\nExcluded: FT test patients\n577\n‚Äì\nPretraining (after exclusion, before split)\n‚Äì\n46,746\nTrain split (patient-level)\n3,568\n37,112\nTest split (patient-level)\n892\n9,634\nAfter token filter (‚â§8,000 tokens)\n‚Äì\n‚Äì\nTrain kept / removed\n‚Äì\n32,681 / 4,431\nTest kept / removed\n‚Äì\n722 / 78\nTable 5: Table A2: Task distribution for Stage 1 pre-\ntraining (AMC).\nTask\nTrain\nTest\nMMSE\n17,131\n4,593\nGDS\n15,787\n3,972\nCDR\n4,194\n1,069\nTotal\n37,112\n9,634\nA\nAppendix\nA.1\nData Preprocessing Details\nA.2\nPretraining Data Statistics (AMC)\nTo prevent patient-level leakage, we exclude all\npatients reserved for downstream fine-tuning from\nthe Stage 1 pretraining corpus. After removing 577\nfine-tuning test patients from the original cohort\nof 11,163 patients, we obtain 46,746 longitudinal\nsamples for intermediate clinical score forecasting\n(MMSE/GDS/CDR). We perform a patient-level\nsplit with a test ratio of 0.20, resulting in 3,568\ntraining patients (37,112 samples) and 892 test pa-\ntients (9,634 samples). For evaluation efficiency,\nthe test split is task-stratified and reduced to 800\nsamples (401 patients). Finally, samples exceed-\ning 8,000 tokens under the Qwen2.5-7B-Instruct\ntokenizer are removed, yielding 32,681 training\nsamples and 722 test samples. Table 4 summarizes\nthe overall dataset composition, and Table 5 reports\nthe task-wise distribution.\nA.3\nPretraining Data Statistics (ADNI)\nFor Stage 1 pretraining on the ADNI benchmark,\nwe construct longitudinal next-visit prediction sam-\nples across six cognitive targets (MMSE, CDRSB,\nADAS11, ADAS13, ADASQ4, and RAVLT_learning)\nfrom linearized structured records (Sec. A.3.2).\nTo prevent leakage, we exclude all participants\nreserved for the downstream fine-tuning test set.\nAfter removing DX targets (not used in our pretrain-\ning), we obtain 11,319 candidate samples before\nTable 6: Table A3: Stage 1 (Pretraining) dataset\nstatistics (ADNI). Fine-tuning test participants are fully\nexcluded to prevent leakage.\nItem\nValue\nCandidate samples (before filtering; 6 tasks)\n11,319\nKept samples (‚â§8,000 tokens & excl. FT-test)\n9,953\nExcluded: fine-tuning test participants\n1,366\nExcluded: token length / parsing / ID issues\n0 / 0 / 0\nStage 1 split (samples)\ntrain 7,958; test 1,995\nTable 7: Table A4: Task-wise distribution for Stage 1\npretraining (ADNI). ‚ÄúInput‚Äù counts are computed\nbefore excluding fine-tuning test participants; ‚ÄúKept‚Äù\ncounts are used for Stage 1 training/evaluation.\nTask\nInput\nKept\nTrain\nTest\nMMSE\n1,899\n1,671\n1,331\n340\nCDRSB\n1,882\n1,656\n1,322\n334\nADAS11\n1,891\n1,663\n1,311\n352\nADAS13\n1,865\n1,637\n1,307\n330\nADASQ4\n1,897\n1,669\n1,340\n329\nRAVLT_learning\n1,885\n1,657\n1,347\n310\nTotal\n11,319\n9,953\n7,958\n1,995\nfiltering and keep 9,953 samples after excluding\nfine-tuning test participants. No samples are re-\nmoved by the token-length constraint (‚â§8,000 to-\nkens) or parsing/ID issues in our pipeline. We\nbucket the time gap to the prediction target into\n1-month bins up to 6 months and an additional >6m\nbin (see Sec. A.4 for the bucket definition). Finally,\nwe perform a patient-level split to create Stage 1\ntrain/test sets, resulting in 7,958 training samples\nand 1,995 test samples. Table 6 summarizes the\noverall dataset composition, and Table 7 reports\ntask-wise statistics.\nTo adapt distinct data modalities for our unified\nreasoning framework, we developed specialized\npreprocessing pipelines for both unstructured clin-\nical notes (Asan) and structured tabular records\n(ADNI). We applied a consistent protocol consist-\ning of Data Transformation followed by Dataset\nConstruction.\nA.3.1\nAsan Medical Center (Unstructured\nClinical Notes)\nData Transformation (Translation & Extrac-\ntion).\nWe transformed raw Korean clinical notes\ninto English reasoning contexts using a secure\npipeline. We utilized Qwen2.5-14B-Instruct as an\nauxiliary LLM to translate notes and extract clin-\nical indices (MMSE, GDS, and CDR) to serve as\nverifiable ground truth targets. Crucially, all infer-\nence processes were conducted in a strictly isolated\non-premise environment to prevent any external\n11\n"}, {"page": 12, "text": "data transmission.\nStage 1 Construction Pipeline.\nWe constructed\nthe pre-training dataset with the following criteria:\n1. Tolerance-Aware Labeling: We defined the\nprediction targets as extracted clinical indices.\nRecognizing extraction variability, we applied\na tolerance of ¬±2 for MMSE, treating predic-\ntions within this range as correct. Exact match-\ning was enforced for coarser scales (GDS,\nCDR).\n2. Token Filtering: Using the Qwen2.5 tok-\nenizer, we filtered out samples exceeding\n8,000 tokens to fit context constraints.\n3. Evaluation Set: The dataset was split into\ntraining and test sets at a patient level (80:20)\nto evaluate Stage 1 performance.\nA.3.2\nADNI Benchmark (Structured Tabular\nData)\nData\nTransformation\n(Linearization).\nWe\ntransformed\nstructured\ntabular\nrecords\ninto\nlongitudinal textual logs suitable for LLM input.\nFor each visit, we aggregated key biomark-\ners‚Äîincluding cognitive scores (MMSE, CDR-SB,\nADAS-Cog), CSF biomarkers (AŒ≤, Tau), and\nMRI measures‚Äîinto a structured text block\n(e.g., ‚Äú2011-05-12: ¬´<VISIT 1¬ª> CDRSB: 0.5,\nMMSE: 28...‚Äù). These blocks were concatenated\nchronologically to form the patient history.\nStage 1 Construction Pipeline.\nWe applied a\nconstruction protocol parallel to the Asan dataset\nbut adapted for the continuous nature of ADNI\nbiomarkers:\n1. Target Indices: We selected seven key indi-\ncators: MMSE, CDR-SB, ADAS-Cog (11, 13,\nQ4), RAVLT (Learning), and LDELTOTAL.\n2. Proportional Tolerance-Aware Labeling:\nUnlike categorical labels, these indices vary\nwidely in range. To standardize difficulty, we\ndefined a relative tolerance ratio œÅ ‚âà6.7%\n(derived from the standard allowance of ¬±2\npoints on the 30-point MMSE scale). For each\nindex, the allowable error margin Œ¥ was calcu-\nlated as ‚åàRange √ó œÅ‚åâ. The specific thresholds\nare detailed in Table 8.\n3. Token Filtering: Samples exceeding 8,000\ntokens were filtered out using the tokenizer\nconstraints.\n4. Evaluation Set: Consistent with the Asan\nprotocol, we applied a stratified patient-level\nsplit (80:20). Due to the high computational\ncost of longitudinal reasoning, the final evalu-\nation was conducted on a stratified 50% sub-\nsample of the test set.\nTable 8: Tolerance Thresholds for ADNI Indices. Er-\nror margins (Œ¥) were scaled proportionally to the range\nof each metric.\nClinical Index\nRange\nTolerance (Œ¥)\nMMSE\n0‚Äì30\n¬±2\nCDRSB\n0‚Äì18\n¬±1.0\nADAS-Cog 11\n0‚Äì70\n¬±5\nADAS-Cog 13\n0‚Äì85\n¬±6\nADAS-Cog Q4\n0‚Äì10\n¬±1\nRAVLT (Learning)\n-20‚Äì20\n¬±3\nLDELTOTAL\n0‚Äì25\n¬±2\nA.4\nDetailed Temporal Distributions\nTo validate the model‚Äôs capability in long-term pre-\ndiction, we analyze the time intervals between the\ninput data and the prediction target. Figure 8 de-\ntails the distribution of these time gaps for the test\nsets. Notably, the ADNI cohort (right) presents a\nsignificantly more challenging scenario, with ap-\nproximately half (‚àº50%) of the samples having a\ngap exceeding 24 months, including a long tail ex-\ntending beyond 36 months. This contrasts sharply\nwith the Asan cohort, which is predominantly con-\ncentrated in the short-term range (6‚Äì12 months).\nThis diversity ensures that our evaluation covers\nboth immediate screening and long-term prognos-\ntic scenarios.\nFigure 8: Time Gap Distribution by Cohort (Test\nSets). The histograms show the interval between the\nlast available clinical note and the diagnosis date. The\nAsan cohort is concentrated in shorter intervals (6‚Äì18m),\nreflecting relatively dense clinical follow-up prior to\ndiagnosis. In contrast, the ADNI cohort displays a sub-\nstantially wider temporal range, extending beyond 36\nmonths, which reflects the longitudinal nature of MCI\nprogression monitoring.\n12\n"}, {"page": 13, "text": "A.5\nStage 1: Performance of Intermediate\nClinical Indices\nWe analyze the model‚Äôs capability to predict inter-\nmediate clinical indices. On the structured ADNI\nbenchmark (Tables 10, 11), Dementia-R1 demon-\nstrates superior performance in long-term forecast-\ning (>18 months), particularly on the critical CDR-\nSB metric. Similarly, on the unstructured Asan co-\nhort (Tables 9), the model consistently outperforms\nbaselines in short-to-mid term intervals, achieving\nnotable gains in GDS and CDR prediction.\nA.6\nStage 2: Performance of Final Binary\nPrognosis\nBuilding on the clinical indicators established in\nStage 1, Stage 2 assesses the model‚Äôs ultimate ca-\npacity to determine the final binary prognosis. As\nsummarized in Table 12, Dementia-R1 achieves\nthe highest overall F1-score on the Asan dataset,\ndemonstrating notable consistency across the term\nintervals. Notably, it maintains competitive perfor-\nmance in both short-term (6‚Äì12m) and long-term\n(18‚Äì24m) intervals, proving its stability across dif-\nferent forecasting ranges. Similarly, the results on\nthe ADNI benchmark (Table 13) further highlight\nthe model‚Äôs enhanced robustness in long-term fore-\ncasting (> 18 months). In these extended horizons,\nDementia-R1 not only surpasses proprietary fron-\ntier models such as GPT-4o but also outperforms\nlarger specialized baselines, effectively validating\nconfirming its effectiveness in modeling longitudi-\nnal disease trajectories.\nA.7\nQualitative Analysis: Comparative\nReasoning\nTo demonstrate the impact of our proposed method\non reasoning quality, we compare the outputs of\nDementia-R1 against the Qwen2.5-32B model us-\ning a representative longitudinal case from the\nAMC cohort. Figure 9 illustrates the input clin-\nical note, which follows a semi-structured SOAP\nformat. In this record, critical signals such as cogni-\ntive scores (MMSE, GDS) and medication changes\nare embedded within the free-text Objective and\nPlan sections across multiple visits spanning from\n2020 to 2023. This presents a complex reasoning\nchallenge, requiring the model to aggregate scat-\ntered clinical indicators and correctly reconstruct\nthe patient‚Äôs disease trajectory from the unstruc-\ntured narrative.\nWhile both models correctly predict the final\ndiagnosis, their reasoning processes diverge signif-\nicantly. Figure 10 presents the reasoning outputs\ngenerated by both models. Dementia-R1 effec-\ntively structures the longitudinal information by\norganizing the output into distinct sections for cog-\nnitive assessment history, diagnosis, and current\nstatus. This structural clarity allows clinicians to\nrapidly verify the evidence. Notably, the model\naccurately reconstructs the temporal trajectory of\ncognitive decline and correctly identifies the drop\nin MMSE scores from 23 down to 17 alongside the\nplateau in the most recent visits. Furthermore, it\ncorrectly identifies the medication switch involving\nthe discontinuation of Bearcept and the addition\nof Ebixa, demonstrating precise grounding in the\nclinical text.\nIn contrast, despite arriving at the correct label,\nQwen2.5-32B produces a dense, unstructured block\nof text that is difficult to audit for clinical decision-\nmaking. More critically, it exhibits significant fail-\nures in medical soundness and domain knowledge.\nFirst, the baseline hallucinates a pharmacologi-\ncal equivalence by describing \"Ebixa (donepezil)\"\neven though Ebixa is memantine, an NMDA re-\nceptor antagonist distinct from the cholinesterase\ninhibitor donepezil. Such hallucinations pose po-\ntential safety risks in clinical settings. Second,\nthe baseline misinterprets the Global Deterioration\nScale (GDS) scores of 3-4 as indicators of \"mild de-\npression\" and confuses the dementia staging scale\nwith a depression inventory. These findings un-\nderscore that general-purpose reasoning capabili-\nties, even in larger models, do not automatically\ntranslate to clinical accuracy. Our results demon-\nstrate that the domain-specific alignment integrated\ninto Dementia-R1 is essential for correcting such\nmisconceptions and ensuring the high reliability\nrequired for longitudinal dementia prognosis.\nA.8\nTraining Implementation Details\nWe implemented our framework using PyTorch.\nAll experiments were conducted on four NVIDIA\nH100 (80GB) GPUs.\nReinforcement Learning (Dementia-R1)\nFor\nthe RL stage, we utilized the Open-R1 frame-\nwork.\nWe employed DeepSpeed ZeRO-3 and\nvLLM (colocate mode) to optimize memory usage\nfor processing long clinical narratives. We set the\nper-device batch size to 1 with a gradient accumula-\ntion of 2 and a group size of G = 8 (effective batch\nsize of 8). The model was trained for 5,000 steps in\n13\n"}, {"page": 14, "text": "Table 9: Stage 1 Performance by Time Gap (Asan). Dementia-R1 achieves superior accuracy in short-to-mid\nterm intervals (0‚Äì18 months), validating the effectiveness of the Cold-Start strategy.\nOverall\nAccuracy by Time Gap to Prediction Target\nMethod\nAcc (‚Üë)\n0‚Äì6m\n6‚Äì12m\n12‚Äì18m\n18‚Äì24m\nExternal LLMs\nQwen2.5-32B-Inst\n57.90 ¬± 0.05\n60.66 ¬± 0.43\n54.91 ¬± 0.53\n55.15 ¬± 1.21\n50.43 ¬± 5.08\nQwen2.5-7B-Inst\n54.68 ¬± 0.32\n57.56 ¬± 0.69\n51.19 ¬± 1.16\n52.76 ¬± 0.99\n46.32 ¬± 2.28\nSpecialized Models\nSFT ‚ÜíSFT\n51.73 ¬± 0.53\n55.07 ¬± 0.87\n48.37 ¬± 0.75\n46.95 ¬± 0.59\n46.90 ¬± 5.02\nSFT ‚ÜíGRPO\n55.81 ¬± 0.46\n59.60 ¬± 0.69\n51.21 ¬± 1.15\n52.64 ¬± 0.41\n49.62 ¬± 5.43\nDementia-R1\n59.61 ¬± 0.46\n63.38 ¬± 0.45\n55.25 ¬± 0.69\n56.41 ¬± 0.58\n49.49 ¬± 2.84\nTable 10: Stage 1 Performance by Time Gap (ADNI). Dementia-R1 demonstrates superior long-term reasoning\ncapability (>18 months) compared to larger baselines (GPT-4o, 32B), validating the efficacy of the proposed RL\nframework.\nOverall\nAccuracy by Time Gap to Prediction Target\nMethod\nAcc (‚Üë)\n0‚Äì6m\n6‚Äì12m\n12‚Äì18m\n18‚Äì24m\n>24m\nExternal LLMs\nGPT-4o\n77.04 ¬± 0.19\n79.52 ¬± 0.67\n77.06 ¬± 0.31\n76.22 ¬± 0.49\n79.33 ¬± 0.99\n77.55 ¬± 0.59\nQwen2.5-32B-Inst\n77.91 ¬± 0.18\n81.26 ¬± 0.27\n77.38 ¬± 0.25\n76.77 ¬± 0.52\n80.87 ¬± 0.62\n79.72 ¬± 0.72\nQwen2.5-7B-Inst\n75.67 ¬± 0.31\n77.87 ¬± 0.46\n75.09 ¬± 0.43\n75.46 ¬± 0.58\n78.89 ¬± 0.91\n77.48 ¬± 1.17\nSpecialized Models\nSFT ‚ÜíSFT\n64.42 ¬± 0.23\n66.60 ¬± 0.28\n63.58 ¬± 0.69\n63.26 ¬± 0.68\n67.65 ¬± 1.40\n68.55 ¬± 2.02\nSFT ‚ÜíGRPO\n67.61 ¬± 0.33\n71.26 ¬± 1.16\n67.47 ¬± 0.62\n65.05 ¬± 0.51\n69.16 ¬± 1.07\n71.50 ¬± 1.56\nDementia-R1\n77.04 ¬± 0.28\n79.44 ¬± 0.92\n76.30 ¬± 0.51\n76.23 ¬± 0.52\n80.97 ¬± 1.09\n80.27 ¬± 0.89\nTable 11: Stage 1 Accuracy by Clinical Index (ADNI). Dementia-R1 outperforms larger 32B models on CDR-SB\nwhile maintaining competitive performance against GPT-4o on other key metrics (e.g., ADAS-Cog, RAVLT).\nOverall\nCDRSB\nADAS11\nADAS13\nRAVLT\nMMSE\nADASQ4\nLDEL\nMethod\nAcc (‚Üë)\nAcc (‚Üë)\nAcc (‚Üë)\nAcc (‚Üë)\nAcc (‚Üë)\nAcc (‚Üë)\nAcc (‚Üë)\nAcc (‚Üë)\nExternal LLMs\nGPT-4o\n77.04 ¬± 0.19\n84.85 ¬± 0.51\n85.23 ¬± 0.32\n84.04 ¬± 0.24\n81.83 ¬± 0.39\n77.01 ¬± 0.47\n65.56 ¬± 0.29\n60.76 ¬± 0.95\nQwen2.5-32B-Inst\n77.91 ¬± 0.18\n87.52 ¬± 0.51\n86.74 ¬± 0.47\n84.09 ¬± 0.54\n82.52 ¬± 0.30\n77.86 ¬± 0.24\n65.43 ¬± 0.61\n61.21 ¬± 0.61\nQwen2.5-7B-Inst\n75.67 ¬± 0.31\n87.55 ¬± 0.47\n84.06 ¬± 0.26\n82.32 ¬± 0.65\n79.48 ¬± 0.90\n75.65 ¬± 1.26\n60.65 ¬± 1.49\n60.00 ¬± 0.75\nSpecialized Models\nSFT ‚ÜíSFT\n64.42 ¬± 0.23\n75.66 ¬± 0.88\n71.17 ¬± 0.84\n68.18 ¬± 0.99\n74.35 ¬± 0.97\n64.35 ¬± 1.81\n42.92 ¬± 1.86\n54.34 ¬± 1.72\nSFT ‚ÜíGRPO\n67.61 ¬± 0.33\n77.91 ¬± 0.85\n79.65 ¬± 1.16\n73.76 ¬± 1.42\n79.50 ¬± 1.08\n61.36 ¬± 1.03\n45.58 ¬± 0.98\n55.48 ¬± 1.21\nDementia-R1\n77.04 ¬± 0.28\n87.89 ¬± 0.42\n86.02 ¬± 0.54\n83.58 ¬± 0.94\n82.28 ¬± 1.13\n76.00 ¬± 0.93\n63.17 ¬± 1.33\n60.35 ¬± 1.06\nTable 12: Stage 2 Fine-tuning Performance by Time Gap (Asan). Dementia-R1 achieves the highest overall F1\nscore, demonstrating its robust reasoning capabilities across the entire temporal intervals.\nOverall\nF1 score by Time Gap to Prediction Target\nMethod\nF1 (‚Üë)\n6‚Äì12m\n12‚Äì18m\n18‚Äì24m\nExternal LLMs\nQwen2.5-32B-Inst\n71.89 ¬± 0.80\n69.29 ¬± 0.70\n74.38 ¬± 0.70\n77.12 ¬± 1.60\nQwen2.5-7B-Inst\n72.20 ¬± 0.80\n71.80 ¬± 0.60\n74.00 ¬± 1.60\n64.94 ¬± 3.20\nSpecialized Models\nSFT ‚ÜíSFT\n76.67 ¬± 0.60\n76.27 ¬± 0.90\n77.55 ¬± 1.00\n74.82 ¬± 3.50\nSFT ‚ÜíGRPO\n75.85 ¬± 0.60\n74.66 ¬± 0.90\n78.00 ¬± 1.10\n72.21 ¬± 0.90\nDementia-R1\n77.03 ¬± 0.72\n75.43 ¬± 0.69\n79.28 ¬± 0.80\n76.02 ¬± 2.73\nBfloat16 precision with a 2,000-token completion\nlimit.\nA.9\nHuman evaluation protocol\nTo assess the clinical quality of reasoning, we\nconducted a blinded human evaluation with med-\n14\n"}, {"page": 15, "text": "Table 13: Stage 2 Fine-tuning Performance by Time Gap (ADNI). While large-scale general models (e.g.,\nGPT-4o) excel in short-term forecasting, Dementia-R1 demonstrates superior robustness in long-term reasoning\n(> 18 months).\nOverall\nF1 score by Time Gap to Prediction Target\nMethod\nF1 (‚Üë)\n6‚Äì12m\n12‚Äì18m\n18‚Äì24m\n>24m\nExternal LLMs\nGPT-4o\n76.05 ¬± 1.05\n79.66 ¬± 2.51\n79.26 ¬± 0.78\n78.78 ¬± 1.68\n71.18 ¬± 6.98\nQwen2.5-32B-Inst\n73.86 ¬± 0.57\n74.21 ¬± 2.15\n74.76 ¬± 2.00\n74.04 ¬± 1.63\n72.10 ¬± 4.64\nQwen2.5-7B-Inst\n62.19 ¬± 0.94\n68.37 ¬± 4.76\n65.39 ¬± 2.99\n62.76 ¬± 2.27\n58.18 ¬± 5.95\nSpecialized Models\nSFT ‚ÜíSFT\n71.95 ¬± 1.07\n70.58 ¬± 4.87\n74.21 ¬± 2.78\n73.91 ¬± 1.96\n68.65 ¬± 6.43\nSFT ‚ÜíGRPO\n73.92 ¬± 0.76\n78.46 ¬± 4.64\n70.35 ¬± 1.47\n76.21 ¬± 3.01\n73.15 ¬± 5.48\nDementia-R1\n74.91 ¬± 1.49\n74.54 ¬± 4.43\n74.95 ¬± 1.73\n80.30 ¬± 3.81\n73.11 ¬± 3.03\nical experts using a pairwise comparison proto-\ncol. For each case, experts were presented with\ntwo anonymized model responses (Model A and\nModel B) generated for the same patient record\nand prediction task. For each evaluation criterion,\nexperts were asked to select one of three options:\nModel A, Model B, or Tie.\nEach model pair was evaluated using 10\nquestion-answer cases per comparison, and judg-\nments were collected independently for the follow-\ning six clinically motivated dimensions:\n1. Temporal Reasoning Accuracy: Which re-\nsponse appropriately interprets changes in\nsymptoms and the rate of progression by com-\nparing earlier records with the most recent\nrecords?\n2. Evidence Grounding: Which response cites\nevidence that is explicitly present in the orig-\ninal clinical notes and does not introduce in-\nformation that is absent from the records?\n3. Clinically Relevant Evidence Selection:\nWhich response avoids being influenced by\nclinically irrelevant details or overlooking key\nevidence, and instead bases its reasoning on\ndiagnostically important evidence from the\nclinical notes?\n4. Medical Soundness: Which response is more\nmedically sound with respect to dementia di-\nagnostic criteria and clinical judgment, in\nterms of both the reasoning process and the\nfinal conclusion?\n5. Completeness of Key Findings: Which re-\nsponse reflects all important symptoms docu-\nmented in the clinical notes without omitting\nkey findings?\n6. Overall Clinical Utility: When used as ref-\nerence material in real-world clinical prac-\ntice, which response is more reliable and more\nhelpful for reducing clinical decision-making\ntime?\nA.10\nPrompt Templates\nStage 1 (Cold-Start Pre-training).\nFigures 11\nand 12 present the prompt templates for the Asan\nMedical Center and ADNI pre-training tasks, re-\nspectively. In this stage, the model is trained to\npredict verifiable intermediate clinical indices (e.g.,\nMMSE/GDS/CDR) extracted from unstructured\nnotes or structured records. All templates enforce\na unified <think> / <answer> format, enabling re-\nliable parsing of the predicted value for training.\nStage 2 (Task Fine-tuning).\nFigures 13 and 14\npresent the prompt templates for the cohort-specific\ndownstream tasks of dementia detection on Asan\nand MCI-to-dementia conversion prediction on\nADNI. These templates retain the same constrained\noutput format as Stage 1 and guide the model to\nbase its prediction on longitudinal evidence across\nthe provided history.\nOther prompts and reuse across settings.\nFig-\nure 15 shows a separate prompt used to generate\nteacher rationales for constructing CoT-supervised\ndata for SFT baselines. We reuse the same task\nprompts across all experimental pipelines, includ-\ning SFT‚ÜíSFT, SFT‚ÜíGRPO, GRPO‚ÜíGRPO, and\nsingle-stage baselines. The pipelines differ only in\nthe optimization procedure and in whether teacher-\ngenerated rationales are included.\n15\n"}, {"page": 16, "text": "Longitudinal Clinical Note Input\n2023-08-30:\nSubjective\nFollow-up with Professor **, Constipation, pletaal dosage reduced Sometimes forgetful, but there are times when it‚Äôs okay Caregiver observes that there is\na slight decline Handles all household chores personally Forgets what they went for when crossing the room\nObjective\nF/79y; Date of Birth (anonymized): ****/**/**\n2020/08/27 MMSE 23 GDS 4\n2022/02/03 MMSE 21 GDS 3\n2022/09/01 MMSE 17 GDS 4\n2023-08-30 MMSE; 17 (recall 2) GDS; 4\nRight-handed\nHighest Education Level; Illiterate\nAssessment\n# major neurocognitive disorder * VaD * MTA2/3 D3P3 (2022/09)\nPlan\nAdd ebixa and discontinue bearcept LICA\nPletaal tab [50mg] 1 TAB DP 1 time 35 days PO\nLexapro tab [10mg] 1 TAB N 1 time 35 days PO\nEbixa tab [10mg] 0.5 TAB BNP 2 times 35 days PO\n2023-10-04:\nSubjective\nFollow-up patient of Professor\nScheduled for LICA after dementia team consultation on 2023/08/30\nDiscontinued bearcept and added ebixa\nBowel movements improved after changing the medication\nObjective\nF/79y; Date of Birth (anonymized): ****/**/**\n2020/08/27 MMSE 23 GDS 4\n2022/02/03 MMSE 21 GDS 3\n2022/09/01 MMSE 17 GDS 4\n2023/08/30 MMSE 17 (recall 2) GDS 4 (illiterate)\n2022-08-21 eGFR(CKD-EPI) (Qn), Blood 69 ml/min/1.73m2\nAssessment\n# major neurocognitive disorder\n* VaD\n* MTA2/3 D3P3 (2022/08)\nPlan\n* Reduced pletaal due to incontinence (outpatient of Professor **)\nLICA on 07/07\nIncrease ebixa dosage, reduce back to half tablet if side effects occur\nPletaal tab [50mg] 1 TAB DP 1 time 56 days PO\nLexapro tab [10mg] 1 TAB N 1 time 56 days PO\nEbixa tab [10mg] 1 TAB BNP 2 times 56 days PO\n2023-12-02:\nSubjective\nPost LICA visit\nNo gastrointestinal side effects with current medication\nmood: so so\nObjective\nF/79y; Date of Birth (anonymized): ****/**/**\nUnlearned\n2020/08/27 MMSE 23 GDS 4\n2022/02/03 MMSE 21 GDS 3\n2022/09/01 MMSE 17 GDS 4\n2023/08/30 MMSE 17 GDS 4 (recall 2)\n2023/11/24 GDS 3 CDR 0.5 SB 1.0 BI 20 SIADL 5 NPI 2\nNote> Z score -1.5 or lower in some cognitive domains. The test results suggest a retrieval deficit in verbal memory and a deficit in visual memory.\nOther functions such as frontal/executive functions, attention, language and related functions, and visuoconstruction ability are all within normal levels.\nTherefore, bilateral frontal lobe dysfunction is suggested. The patient does not report any decline in ADL and is currently in an amnestic mild cognitive\nimpairment state. The patient has shown cognitive decline since around ‚Äô18, which has gradually worsened, and with multiple vascular risk factors and\ntest results indicating a retrieval deficit, subcortical vascular cognitive impairment needs to be considered first. Given the patient‚Äôs advanced age and the\nmore pronounced memory impairment compared to other cognitive domains, Alzheimer‚Äôs disease underlying pathology or a combined condition should\nalso be considered, and the influence of hypothyroidism cannot be completely ruled out. A correlation with neuroimaging studies is recommended for an\naccurate diagnosis, and follow-up is recommended if cognitive function decline worsens. Treatment for depression is also recommended.\n2022-08-21 eGFR(CKD-EPI) (Qn), Blood 69 ml/min/1.73m2\nAssessment\n# impending dementia\n* VaD * MTA2/3 D3P3 (2022/08) Plan\n* Reduce pletaal due to fecal incontinence (outpatient visit with Professor **)\nditto\nContinued lexapro after consultation\nConsultation to change medication if fecal incontinence persists\nPletaal tab [50mg] 1 TAB DP 1 time 91 days PO\nLexapro tab [10mg] 1 TAB N 1 time 91 days PO\nEbixa tab [10mg] 1 TAB BNP 2 times 91 days PO\nFigure 9: Example of longitudinal clinical notes. To protect patient privacy, all personal identifiers (e.g., names)\nwere removed, and dates were randomly shifted.\n16\n"}, {"page": 17, "text": "Qualitative Comparison of Longitudinal Reasoning for 6-12 Month Dementia Prognosis\n[Dementia-R1 output]\nThe patient‚Äôs clinical notes and cognitive assessments provide a clear picture of their cognitive decline over time. Here‚Äôs\nthe step-by-step reasoning:\n1. **Cognitive Assessments Over Time:**\n- 2020/08/27: MMSE 23, GDS 4 (Severe cognitive decline)\n- 2022/02/03: MMSE 21, GDS 3 (Moderate cognitive decline)\n- 2022/09/01: MMSE 17, GDS 4 (Severe cognitive decline)\n- 2023/08/30: MMSE 17 (recall 2), GDS 4 (Severe cognitive decline)\n2. **Clinical Dementia Rating (CDR):**\n- The CDR scores are consistently 0.5 or 1, indicating very mild to mild dementia.\n3. **Symptoms and Functional Decline:**\n- The patient reports forgetfulness, sometimes forgetting what they went for when crossing the room.\n- Caregiver observes a slight decline.\n- The patient handles all household chores personally but shows signs of memory impairment.\n4. **Diagnosis and Plan:**\n- The patient is diagnosed with VaD (Vascular Dementia) and MTA2/3 D3P3.\n- The patient is currently in an amnestic mild cognitive impairment state, with a retrieval deficit in verbal and visual\nmemory.\n- The patient has shown cognitive decline since around 2019, which has gradually worsened.\n5. **Current Status:**\n- The most recent MMSE score (17) and GDS score (4) indicate severe cognitive decline.\n- The patient‚Äôs CDR score is 0.5 or 1, indicating very mild to mild dementia.\n- The patient‚Äôs condition has not improved over the past year, and the decline is consistent with a diagnosis of dementia.\nGiven the consistent cognitive decline, severe MMSE and GDS scores, and the diagnosis of VaD, it is clear\nthat the patient is likely to have dementia.\n[QWEN 2.5 32B output]\n1. The patient‚Äôs MMSE scores have been declining over time: 23 in 2020, 21 in 2022, and 17 in 2022 and 2023. A score\nof 17 is indicative of mild to moderate cognitive impairment.\n2. The patient has been diagnosed with major neurocognitive disorder, specifically vascular dementia (VaD), and is\nclassified as MTA2/3 D3P3.\n3. The caregiver observes a slight decline in the patient‚Äôs cognitive function, and the patient sometimes forgets what\nthey went for when crossing the room, which is a sign of cognitive impairment.\n4. The patient‚Äôs GDS score has been consistently around 3-4, indicating mild depression, which can be associated with\ncognitive decline.\n5. The patient‚Äôs cognitive test results suggest a retrieval deficit in verbal memory and a deficit in visual memory, with\nother cognitive functions within normal levels.\n6. The patient is in an amnestic mild cognitive impairment state, with cognitive decline since around 2019, which has\ngradually worsened.\n7. The patient has multiple vascular risk factors, and the test results indicate a retrieval deficit, suggesting subcortical\nvascular cognitive impairment.\n8. The patient‚Äôs advanced age and more pronounced memory impairment compared to other cognitive domains suggest\nAlzheimer‚Äôs disease underlying pathology or a combined condition.\n9. The patient is being treated with Ebixa (donepezil), which is commonly used in the treatment of Alzheimer‚Äôs disease\nand other forms of dementia\nFigure 10: A comparison of the reasoning processes generated by Dementia-R1 and the Qwen2.5-32B for the\nclinical case shown in Figure 9. While Dementia-R1 produces a structured and grounded rationale, the baseline\ngenerates a dense narrative containing medical hallucinations.\n17\n"}, {"page": 18, "text": "Pre-training Task: Asan Medical Center (Future Score Prediction)\n[System Prompt]\nA conversation between a User and an Assistant. The User provides clinical notes and metadata describing a patient‚Äôs\ncondition before a cognitive or functional assessment. Each input explicitly specifies the target scale (GDS, MMSE, or\nCDR), the time interval, and the required output format. The Assistant must carefully read the provided instructions,\nunderstand which scale is being predicted, and output the correct numerical value according to the described scoring\nrule.\n[Input Data]\nInstruction: You are given longitudinal clinical notes recorded BEFORE a cutoff relative to a cognitive assessment.\nThe most recent included note lies <TIME_INTERVAL> prior to the anchor assessment date.\nTask: Predict the target score (Example: MMSE) for the anchor assessment.\nFormat: Output step-by-step reasoning in <think> tags and the final value in \\boxed{} within <answer> tags.\nScoring Indicators Glossary:\n- MMSE: Integer score ranging from 0 to 30 (Higher = better global cognition).\n- GDS: Global Deterioration Scale from 1 to 7 (Higher = more severe impairment).\n- CDR: Clinical Dementia Rating global score chosen from {0, 0.5, 1, 2, 3}.\n=== Clinical note ===\n<CLINICAL_NOTE>\nFigure 11: Pre-training prompt template for the Asan Medical Center dataset. While MMSE is shown as an example,\nthe model is pre-trained to predict various global cognitive scores, including GDS and CDR, based on unstructured\nclinical notes.\nPre-training Task: ADNI (Future Score Prediction)\n[System Prompt]\nA conversation between a User and an Assistant. The User provides longitudinal structured ADNI clinical, cognitive,\nimaging, and biomarker data across multiple visits. The Assistant must predict the future score or diagnosis at the NEXT\nvisit within a specified time window. Target tasks include MMSE, CDRSB, ADAS11, ADAS13, ADASQ4, RAVLT_learning,\nand LDELTOTAL. Respond only in the specified <think> and <answer> format.\n[Input Data]\nInstruction: You are given longitudinal records for a single participant. All visits occur before the target visit.\nTask: Predict the target score (Example: MMSE) at the NEXT visit.\nConstraint: Time gap bucket = 2‚Äì3 months.\nFormat: Output step-by-step reasoning in <think> tags and the final predicted value in \\boxed{} within <answer>\ntags.\nVariable Glossary:\n- PTEDUCAT/APOE4: Education years / Number of APOE œµ4 alleles.\n- CDRSB/ADAS13/MMSE/MOCA: Clinical severity and cognitive scores (Higher CDRSB/ADAS = worse; Higher\nMMSE/MOCA = better).\n- RAVLT/LDELTOTAL: Memory scores (Lower = poorer memory).\n- FAQ: Functional Activities Questionnaire (Higher = worse daily function).\n- ABETA/TAU/PTAU: CSF biomarkers for amyloid and tau pathology.\n- Ventricles/Hippocampus/WholeBrain: MRI volumetric measures (Structural atrophy).\n=== Clinical Assessment Data ===\n2006-12-11: ¬´<VISIT 1/2¬ª>\nABETA: 446.8, ADAS13: 25.0, MMSE: 27, CDRSB: 0.5, LDELTOTAL: 12, ...\n‚Äî‚Äì(Longitudinal history continues)‚Äî‚Äì\n(Prediction target: MMSE score at the next visit)\nFigure 12: Pre-training prompt template for the ADNI dataset. The model predicts future indicators (e.g., MMSE,\nCDRSB) by analyzing longitudinal structured assessment data. The input includes a variable glossary to assist in\ninterpreting clinical indicators.\n18\n"}, {"page": 19, "text": "Fine-tuning Task: Asan Medical Center (Dementia Detection)\n[System Prompt]\nA conversation between a User and an Assistant. The User provides longitudinal clinical notes and metadata describing\na patient‚Äôs condition. The Assistant must determine whether the patient is likely to be diagnosed with dementia. Output\n0 if the patient is unlikely to have dementia, and 1 if the patient is likely to have dementia. Respond only in the specified\n<think> and <answer> format.\n[Input Data]\nInstruction: You are given longitudinal clinical notes collected BEFORE a cutoff relative to a dementia diagnosis date.\nThe interval to the diagnosis date is: <TIME_INTERVAL> (e.g., 12‚Äì18m).\nTask: Predict whether the patient is likely to have dementia (0: unlikely, 1: likely).\nFormat: Output step-by-step reasoning in <think> tags and the final answer in \\boxed{0 or 1} within <answer>\ntags.\nScoring Indicators Glossary:\n- CDR (Global Score): 0 (No dementia), 0.5 (Very mild), 1 (Mild), 2 (Moderate), 3 (Severe). Higher = worse.\n- MMSE (Total Score): Integer from 0 to 30. Higher = better cognitive function; Lower = more impairment.\n- GDS (Global Deterioration Scale): 1 (No decline) to 7 (Very severe cognitive decline). Higher = worse.\n=== Clinical note ===\n<CLINICAL_NOTE>\nFigure 13: Fine-tuning prompt template for the Asan Medical Center dataset. The task requires detecting dementia\npresence based on unstructured clinical notes and integrated scoring indicators.\nFine-tuning Task: ADNI Cohort (MCI Conversion Prediction)\n[System Prompt]\nA conversation between a User and an Assistant. The User provides longitudinal clinical assessment data and metadata\ndescribing a patient‚Äôs cognitive and functional trajectory. The Assistant must determine whether the patient has\nprogressed from a baseline status of Mild Cognitive Impairment (MCI) to dementia by the time of the final diagnosis.\nOutput 0 if the final diagnosis is non-dementia (MCI or CN), and 1 if the patient has converted to dementia. Use trends\nacross longitudinal data (cognition, function, severity scores) for reasoning. Respond only in the specified <think> and\n<answer> format.\n[Input Data]\nInstruction: You are given longitudinal clinical assessment data for a patient with baseline MCI. Records are collected\nbefore a cutoff set prior to the patient‚Äôs last diagnostic assessment. The interval between the last diagnosis and the most\nrecent visit is: <TIME_INTERVAL> (e.g., 6‚Äì12m).\nTask: Predict whether the patient has progressed to dementia (0: non-dementia, 1: converted).\nFormat: Output step-by-step reasoning in <think> tags and the final answer in \\boxed{0 or 1} within <answer>\ntags.\nVariable Glossary:\n- PTEDUCAT/APOE4: Education years / Number of APOE œµ4 alleles.\n- CDRSB/ADAS13/MMSE/MOCA: Clinical severity and cognitive scores (Higher CDRSB/ADAS = worse; Higher\nMMSE/MOCA = better).\n- RAVLT/LDELTOTAL: Memory scores (Lower = poorer memory).\n- FAQ: Functional Activities Questionnaire (Higher = worse daily function).\n- ABETA/TAU/PTAU: CSF biomarkers for amyloid and tau pathology.\n- Ventricles/Hippocampus/WholeBrain: MRI volumetric measures (Structural atrophy).\n=== Clinical Assessment Data ===\n2011-11-28: ¬´<VISIT 1/7¬ª>\nCDRSB: 0.5, ADAS13: 14.0, MMSE: 28, FAQ: 0, Hippocampus: 6521, ...\n‚Äî‚Äì(Longitudinal history continues)‚Äî‚Äì\nFigure 14: Fine-tuning prompt template for the ADNI dataset. The model predicts MCI-to-dementia conversion\nusing longitudinal trends of clinical scores and biomarkers.\n19\n"}, {"page": 20, "text": "Diagnostic Rationale Generation\n[System Prompt]\nYou are an AI assistant that generates step-by-step reasoning paths.\n[User Prompt]\nProblem: {problem}\nAnswer: {answer}\nTask: Generate a clear step-by-step reasoning path that explains how to solve the problem and arrive at the answer.\nReasoning:\nFigure 15: Prompt template for generating diagnostic rationales for Supervised Fine-Tuning.\n20\n"}]}