{"doc_id": "arxiv:2512.07132", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.07132.pdf", "meta": {"doc_id": "arxiv:2512.07132", "source": "arxiv", "arxiv_id": "2512.07132", "title": "DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning", "authors": ["Nithin Sivakumaran", "Justin Chih-Yao Chen", "David Wan", "Yue Zhang", "Jaehong Yoon", "Elias Stengel-Eskin", "Mohit Bansal"], "published": "2025-12-08T03:33:38Z", "updated": "2025-12-08T03:33:38Z", "summary": "Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.07132v1", "url_pdf": "https://arxiv.org/pdf/2512.07132.pdf", "meta_path": "data/raw/arxiv/meta/2512.07132.json", "sha256": "5e253f3392e9698e81741793a191b015fcaa65d7a84fca40e877ca0522b1ca9a", "status": "ok", "fetched_at": "2026-02-18T02:24:49.862281+00:00"}, "pages": [{"page": 1, "text": "DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in\nMultimodal Reasoning\nNithin Sivakumaran1\nJustin Chih-Yao Chen1\nDavid Wan1\nYue Zhang1\nJaehong Yoon2\nElias Stengel-Eskin3\nMohit Bansal1\n1UNC Chapel Hill\n2Nanyang Technological University\n3The University of Texas at Austin\nAbstract\nSpecialized visual tools can augment large lan-\nguage models or vision language models with\nexpert knowledge (e.g., grounding, spatial rea-\nsoning, medical knowledge, etc.), but knowing\nwhich tools to call (and when to call them) can\nbe challenging. We introduce DART, a multi-\nagent framework that uses disagreements be-\ntween multiple debating visual agents to iden-\ntify useful visual tools (e.g., object detection,\nOCR, spatial reasoning, etc.) that can resolve\ninter-agent disagreement. These tools allow\nfor fruitful multi-agent discussion by introduc-\ning new information, and by providing tool-\naligned agreement scores that highlight agents\nin agreement with expert tools, thereby facil-\nitating discussion. We utilize an aggregator\nagent to select the best answer by providing\nthe agent outputs and tool information. We test\nDART on four diverse benchmarks and show\nthat our approach improves over multi-agent\ndebate as well as over single agent tool-calling\nframeworks, beating the next-strongest base-\nline (multi-agent debate with a judge model)\nby 3.4% and 2.4% on A-OKVQA and MMMU\nrespectively. We also find that DART adapts\nwell to new tools in applied domains, with a\n1.3% improvement on the M3D medical dataset\nover other strong tool-calling, single agent, and\nmulti-agent baselines. Additionally, we mea-\nsure text overlap across rounds to highlight the\nrich discussion in DART compared to existing\nmulti-agent methods. Finally, we study the tool\ncall distribution, finding that diverse tools are\nreliably used to help resolve disagreement.1\n1\nIntroduction\nA key strength of human intelligence is the ability\nto debate and discuss our reasoning with others.\nHowever, everyone has had the experience of en-\ngaging in a debate that goes nowhere because of a\nlack of new information: without the incorporation\n1Code: https://github.com/nsivaku/dart\nof new knowledge, each party becomes entrenched\nin their point of view. While past work (Wang\net al., 2024a; Chen et al., 2024b; Estornell et al.,\n2025a; Estornell and Liu, 2024) has found that\nlarge-language model agents can improve their per-\nformance by engaging in multi-agent debate, this\nhas largely been explored in unimodal, text-only\nsettings. We find that when extending debate to\nmultimodal settings with vision-language models\n(VLMs), the discussion between agents frequently\nbecomes unproductive. The lack of progress in\nconversation is in part due to a lack of new infor-\nmation, with the VLM-based agents participating\nin the discussion often being unable to resolve per-\nceptual differences (see Fig. 1.A). Moreover, each\nVLM suffers from similar perceptual limitations:\npast work has found that VLMs often lag behind\nspecially-trained expert models in domains like\ngrounding, OCR, and spatial reasoning (Chiu et al.,\n2025; Chen et al., 2024a). Indeed, because of these\nlimitations, a separate line of work has explored\nusing LLMs to call expert models as tools rather\nthan relying on VLMs (see Fig. 1.B) (Yang et al.,\n2023; Lu et al., 2023; Wu et al., 2023; Surís et al.,\n2023; Mialon et al., 2023; Yu et al., 2025b), bene-\nfiting from the reasoning abilities of LLMs and the\nvisual perceptual abilities of expert models.\nWe unify these three approaches (VLMs, multi-\nagent debate, and tool calling) with Disagreement\namong Agents for Recruitment of Tools (DART),\na multi-agent approach which approaches visual\nquestion-answering (VQA) tasks by engaging\nVLMs in a multi-agent debate, resolving the dis-\nagreement between agents with information from\nexpert tools (see Fig. 1.C). This has several key ad-\nvantages: first, the novel information from expert\ntools helps move the discussion between agents for-\nward productively and augments their perceptual\nabilities. Second, it simplifies the tool-calling pro-\ncess; unlike top-down LLM-based solutions which\nplan what tools to execute in a single step based\narXiv:2512.07132v1  [cs.CL]  8 Dec 2025\n"}, {"page": 2, "text": "(B) Single LLM Agent with Tools\n(A) Multiple VLM Agents\n(C) DART: Disagreement among Agents for Recruitment of Tools (Ours)\nNo Vision Conditioned\nTool Recruitment\nSpecialized Vision Tools\nDiverse Reasoning\nNo Specialized \nVision Tools \nRecruiter\nAgreement\nScorer\nReasoning\nConflicts\nAgreement\nScores\nTool\nOutputs\nPool of Tools\nPool of Tools\nVLMs in\nDebate\nAgreement Scorer\nRecruiter\nLLM Answerer\nTool Planner\nVision Conditioned \nTool Recruitment\nSpecialized Vision Tools\nDiverse Reasoning\nNo Diverse Reasoning\nFigure 1: Previous work has explored using (A) multiple agents in debate to refine their reasoning, but this approach\nis limited to the abilities of the agents. Alternatively, some methods employ a (B) top-down LLM agent that invokes\nvision tools, yet they plan tool usage based solely on the question and overlook the visual information itself. In our\nmethod (C), we facilitate a discussion among multiple agents with targeted intervention from a pool of vision tools.\nThese tools address disagreements detected in a debate of VLM agents, with their specialized vision outputs and\nagreement scores being used for future discussion.\nonly on the question – e.g., ViperGPT (Surís et al.,\n2023), Chameleon (Lu et al., 2023) – DART adap-\ntively calls tools based only on the points which\nthe VLMs disagree on. This has the added bene-\nfit of combining the strong reasoning abilities of\nVLMs (backed by LLMs) with the specialized per-\nception of vision tools. Finally, the added informa-\ntion given by tools serves as an intuitive measure\nof confidence in each agent: the more a given agent\nagrees with the expert tools (which have stronger\nperceptual abilities), the more it can be relied on.\nDART consists of five steps, outlined in Fig. 2.\nDART starts with (1) Initial Answer Generation,\nwhere each agent creates an answer and reason-\ning for an image-question pair. Next, our method\nuses the answers and reasoning chains from the\nanswering agents in (2) Tool-Based Disagreement\nResolution, which aims to resolve disagreements\nthat occurred during the initial answering step by in-\nvoking tools based on the identified conflicts. Then,\nin (3) Tool-Based Agreement Scoring, DART com-\npares the expert tool outputs and agent answers to\ncalculate agreement scores. These new tool outputs\nand per agent agreement scores are used during (4)\nDiscussion where agents refine their own answer\nand reasoning. Finally, DART composes the tool\noutputs, recalculated agreement scores, and post-\ndiscussion answer outputs into a final answer in the\n(5) Aggregation step.\nWe test DART on four different benchmarks that\nrequire strong and diverse perception capabilities\nand reasoning. We find that DART improves over\nthe next best baseline (multi-agent debate with a\njudge model) by 3.4% on A-OKVQA (Schwenk\net al., 2022), 2.4% on MMMU (Yue et al., 2024),\nand 1.2% on NaturalBench (Li et al., 2024). Sim-\nilarly, DART outperforms tool-calling baselines\nlike ViperGPT (Surís et al., 2023) and Chameleon\n(Lu et al., 2023) on A-OKVQA, MMMU, and\nNaturalBench by 19.0%, 11.5%, and 5.3% respec-\ntively. We also test how DART adapts to a subject-\nspecific domain, adding in a relevant domain ex-\npert tool. Specifically, we test DART’s adaptabil-\nity to medical data by testing on the M3D dataset\n(Bai et al., 2024) while adding in MedGemma 4B\n(Google, 2025) into the expert tool set. We find\nthat our system improves performance on M3D\nby 1.3% over Qwen2.5-VL (Bai et al., 2025), the\nstrongest single agent baseline, and achieve pos-\nitive gains in a multi-agent setting compared to\nnaive multi-agent debate (which decreases by 2.7%\ncompared to Qwen2.5-VL). Our results indicate\nthat DART leads to stronger discussion, introduc-\ning more novel ideas into the discussion round com-\npared to multi-agent debate. Moreover, DART ben-\nefits from calling a variety of tools based on reason-\ning disagreements from multiple models, without\noverly relying on any singular tool.\n"}, {"page": 3, "text": "2\nMethod\nThe problem setup is a Visual Question Answering\n(VQA) task where we use a set of VLM agents\nand tools in DART to arrive at a final answer. The\nframework takes in an image and question as input\nand generates a single natural language answer.\nFormally, given a question Q and image I, we aim\nto generate a single answer a based on a discussion\namong generalist answering agents A = {Ai}n\ni=1\nand vision expert tools V = {Vi}m\ni=1.\n2.1\nInitial Answer Generation\nIn initial answer generation (see Fig. 2 (Step 1),\nwe prompt each answering agent Ai to use CoT to\ngenerate a candidate answer ai, a corresponding\nreasoning chain ri, and self-reported confidence\nscore ci. Before being passed to future stages of\nthe pipeline, we group answers {ai}n\ni=1, reasoning\n{ri}n\ni=1, and confidence {ci}n\ni=1 following Chen\net al. (2024b) to form GI. The grouping process\nlists each unique agent answer with the number of\nagents supporting it and their reasoning.\n2.2\nTool-Based Disagreement Resolution\nWe expect answering agents with distinct outputs\nto have reasoning chains that disagree at specific\npoints. These disagreements mean that agents di-\nverge in their perception or analysis of the scene\nand question. As such, we aim to address these di-\nvergences with tool-based disagreement resolution\nto directly provide visual information from relevant\nvision expert tools that will ground future conversa-\ntions (see Fig. 2 (Step 2). We detect disagreements\namong the agents’ answers {ai}n\ni=1 and answer\nreasoning chains {ri}n\ni=1 using a recruitment agent.\nFor each of our vision tools, the recruitment agent\nis parameterized by an LLM provided with the ac-\nceptable input parameters (either none, text query,\nlist of text queries) as well as potential use cases for\neach tool in the prompt (e.g., the grounder can be\nused if an agent mentioned an object that another\nagent missed). Then, given a grouped answer GI,\nthe recruitment agent detects disagreements and\nproceeds to construct a vision tool set and corre-\nsponding input parameters. The recruitment agent\nis instructed to state the disagreement and justify\nhow a given tool would address that disagreement\n(see Appendix E for the prompt). For example, if\nthe recruitment agent finds that “Agents disagree on\nthe color and specific markings of the planes, with\none agent mentioning a green color while others\ndescribe the distinctive blue,” the recruitment agent\nwould call on the attribute detection tool to “clarify\nthe colors and markings of the planes.” DART uses\ngrounding, object detection, OCR, spatial reason-\ning, captioning, and attribute detection tools.\nThe grounding, object detection, OCR, and spa-\ntial reasoning tools are implemented as expert, fine-\ntuned models. In contrast, reasoning, captioning,\nand attribute detection are handled by a general-\npurpose VLM instructed to focus on those specific\ntasks without leveraging task-specific models. We\nuse a separate VLM from the debate agents to avoid\nbias, as using the same VLM might bias the output\ntowards that particular VLM’s idiosyncrasies.\nThe vision tools process image I with their re-\nspective generated input parameters to get expert\ntool outputs E = {Ei}l\ni=1. Note that certain vi-\nsion tools, such as grounding, object detection, and\nOCR, are not naturally suited for natural language\noutputs. For tools that do not natively output natu-\nral language (Grounder, Object Detector) we per-\nform minimal post-processing to convert outputs\ninto statements suitable for language-model con-\nsumption (see Appendix E for the prompt).\n2.3\nTool-Based Agreement Scoring\nDomain-specific models have been shown to be\nstronger than VLMs within their specific skill set\n(Chiu et al., 2025; Chen et al., 2024a). As such, we\nhypothesize that VLMs whose outputs agree more\nclosely with those of specialized tool models are\nmore likely to be accurate. The expert tools have\nbetter perception, meaning VLMs that match the\nexperts are likely to have correctly perceived the\nimage. Thus, these agreement scores serve as an\nindicator for whether or not to trust a VLM output.\nFor every pair ((ai, ri), Ej) of output from an-\nswering agent i and output from tool j, an agree-\nment scorer LLM produces a binary agreement\nscore between answering agent i and output from\ninvoked tool j as sij ∈{0, 1} after comparing\n(ai, ri) with the Ej ∈E (see Fig. 2 (Step 3)). The\nagreement scorer agent produces scores that rep-\nresent the degree to which an agent agrees with\none specific expert tool output Ej. Agreement of\nanswering agent i with the full set of tools is then\nrepresented by the mean:\nSi =\n1\n|E|\nX\nEj∈E\nsij,\nsi ∈[0, 1].\nEach (ai, ri) is now annotated with its agreement\nscore Si and carried into the subsequent discussion\n"}, {"page": 4, "text": "a2: …The traits are \ntypical of a cat…\nQuestion: What genus of animal is visible here?\nPool of Tools\ngrounder(\nimg, \n“cat”,\n“leopard”, \n“panthera”\n)\nattribute(\nimg, \nquestion,\n“animal”\n)\nRecruiter\nV1: Grounder \nThe grounding expert \nfound a leopard with score \n0.53 and a panthera with \nscore 0.32.\nV2: Attribute\nColor: white\nState: static image on \nmonitor\nCategory: animal\n(1) Initial Answer Generation\n(2) Tool-Based Disagreement Resolution\n(3) Tool-Based Agreement Scoring\nAgreement\nScorer\nAggregator\nTool-Agreement \nScores\nS1 =2/2\nS2 =1/2\nS3=2/2\nAnswer\nPanthera\nGrounder\nAttribute\na3: …The leopard is \nidentifiable…\na1: The image… \ngenus panthera\n(4) Discussion\na2: …features typical \nof a snow leopard…\na3: …align with the \ntraits of the panthera\ngenus.\na1: …animal is in the \ngenus panthera\n(5) Aggregation\na2: …The traits are \ntypical of a cat…\na3: …The leopard is \nidentifiable…\na1: The image… genus \npanthera\nPrompt:\nGF: panthera (2 agents), leopard (1 agent)…\nS: 2/2, 1/2, 2/2\nV: The grounding tool… The attribute tool…\nPlease select the best answer for the question.\nFigure 2: Overview of DART. We start with (1) Initial Answer Generation from a set of answering/reasoning agents.\nThis is followed by (2) Tool-Based Disagreement Resolution and (3) Agreement Scoring. The newly generated tool\noutputs and agreement scores are incorporated into the (4) Discussion and (5) Aggregation phases.\nand aggregation steps. These steps provide agree-\nment scores such that more trust is placed on agents\nthat align with expert information. The agents that\ndo have high scores are more likely to have accu-\nrate visual understanding of the scene. We provide\nthe agreement scorer prompt in Appendix E.\n2.4\nDiscussion\nIn the discussion, each agent takes in the grouped\ninput GI, tool outputs E, and the expert-based\nagreement scores SI (see Fig. 2 (Step 4)). Recall\nthat the grouped agent solution form includes self-\nreported confidence. This means that agents have\nboth self-reported confidence and agreement scores\nduring the discussion step. Agents are instructed\nto place high priority on the expert outputs, even\nover the agent solutions. Answering agents are also\ngiven a 3-shot prompt with simple synthetic exam-\nples to guide them to follow the provided output\nformat (see Appendix E for the full prompt). If the\nagent solutions contain information that contradicts\nthe tool outputs, models are told to assume that the\nvision tool is correct. Finally, agents are provided\nwith the tool-based agreement scores as well as the\nself-reported confidence score. This outputs the\nfinal group of agent answers defined as GF (see\nFig. 2 (Step 3)), and we recalculate final confidence\nscores following Section 2.3 to obtain SF .\n2.5\nAggregation\nSimilar to the discussion step, we provide an aggre-\ngator agent with grouped solutions GF , tool out-\nputs E, and tool-based agreement scores SF (see\nFig. 2 (Step 5)). The aggregator VLM is instructed\nto choose, with CoT, the best answer out of all the\nprovided solutions (see Appendix E for prompt).\n3\nResults and Analysis\n3.1\nExperimental Setup\nDatasets and Metrics.\nWe evaluate our frame-\nwork on four Visual Question Answering (VQA)\ndatasets:\nA-OKVQA (Schwenk et al., 2022),\nMMMU (Yue et al., 2024), NaturalBench (Li\net al., 2024), and M3D-VQA (Bai et al., 2024).\nWe aim to test whether DART performs well on a\ndataset that focuses on reasoning and world knowl-\nedge, such as A-OKVQA. We evaluate on the di-\nrect answer subset using the metric from Antol\net al. (2015) following the A-OKVQA direct an-\nswer evaluation scheme. We also test on MMMU\nto assess DART’s effect on a large, diverse set of\nVQA data, and on NaturalBench, which is designed\nto rigorously test VLMs on their visual comprehen-\nsion using adversarial VQA pairs that are easy for\nhumans but challenging for VLMs. Additionally,\nwe evaluate how well DART adapts to a new tool\nby testing it on M3D-VQA, which is a medical\nVQA dataset on 3D medical scans of real patients.\nMMMU, NaturalBench, and M3D are all multiple\nchoice datasets, so we measure accuracy.\nImplementation Details.\nWe use a diverse set of\nanswering agents and visual tools. For answering\n"}, {"page": 5, "text": "Method\nAgent\nA-OKVQA\nMMMU\nNaturalBench\nTool Systems\nViperGPT (Surís et al., 2023)\nQwen\n49.9\n54.0\n75.3\nChameleon (Lu et al., 2023)\nQwen\n47.5\n51.6\n77.2\nSingle Agent\nCoT (Wei et al., 2022)\nQwenVL\n61.3\n58.8\n79.2\nMiniCPM-o\n55.7\n50.4\n77.9\nOvis2\n61.0\n50.7\n78.7\nLLaVA 1.6\n62.8\n39.4\n70.9\nSelf-Refinement\n(Madaan et al., 2023)\nQwenVL\n61.8\n56.5\n79.0\nMiniCPM-o\n56.7\n46.7\n78.2\nOvis2\n61.7\n47.2\n78.4\nLLaVA 1.6\n62.9\n39.7\n70.6\nSelf-Consistency (5-way)\n(Wang et al., 2023)\nQwenVL\n63.7\n60.8\n80.1\nMiniCPM-o\n58.2\n52.5\n78.1\nOvis2\n64.6\n52.8\n79.3\nLLaVA 1.6\n64.2\n41.3\n72.4\nMultiple Agents\nDebate with Consensus\n3×QwenVL\n61.9\n60.5\n80.0\n3×MiniCPM-o\n57.4\n53.2\n78.3\n3×Ovis2\n62.8\n57.0\n80.1\nQ, M, O\n64.2\n63.1\n80.6\nDebate with Judge\n3×QwenVL\n62.1\n60.3\n79.9\n3×MiniCPM-o\n59.7\n54.8\n78.3\n3×Ovis2\n63.0\n58.3\n80.4\nQ, M, O\n65.5\n63.1\n81.3\nDART (Ours)\nBest Model\n65.3\n61.6\n80.8\nQ, M, O\n68.9\n65.5\n82.5\nTable 1: Comparison of DART and different baselines on A-OKVQA, MMMU, and NaturalBench. Q, M, and O\nstand for QwenVL, MiniCPM-o, and Ovis2 respectively. For the DART (Best Model) setting, we take the single\nbest performing agent from multi-agent debate with judge and run DART with three instances of that model. The\nbest models are Ovis2, QwenVL, and Ovis2 for A-OKVQA, MMMU, and NaturalBench respectively.\nagents, we test with Qwen2.5-VL (Bai et al., 2025),\nMiniCPM-o 2.6 (Yao et al., 2024), and Ovis2 (Lu\net al., 2024) as our VLMs. As an additional inde-\npendent single agent baseline, we include LLaVA-\n1.6 (Mistral) (Liu et al., 2024). Then, we con-\nstruct a pool of visual expert tools with Ground-\ningDINO (Liu et al., 2023) (grounding), YOLOv11\n(Khanam and Hussain, 2024) (object detection),\nSpaceLLaVA (Chen et al., 2024a) (spatial), OCR-\nQwen (OCR), and InternVL-2.5 MPO (Wang et al.,\n2024b) (captioning, attribute, and reasoning) as\nvision tools. We use InternVL-2.5 to serve as an\nexpert tool in multiple domains, as prior work has\nshown that models can be designed to be more ef-\nfective in specific domains when prompted to do\nso (Xu et al., 2025; Wang et al., 2024c). We intro-\nduce a separate VLM for this task to reduce over-\nlap between answering agent information and tool\ncalling information. We also test the effectiveness\nof DART by ablating the VLM-based tools (see\nSection 3.3). We use Qwen2.5 (7B) as the recruit-\nment agent and agreement scorer. We also find that\nOvis2 serves as the best aggregator among a pool\nof similar VLMs. When testing on M3D-VQA, we\nadditionally insert MedGemma 4B (Google, 2025)\ninto the pool of tools to serve as a medical expert.\nBaselines.\nWe compare DART to multiple single-\nagent, multi-agent, and tool-calling baselines. We\ninclude comparisons to training-free single-agent\ntool-calling baselines: ViperGPT (Surís et al.,\n2023) and Chameleon (Lu et al., 2023). ViperGPT\nutilizes a code generation model to predict exe-\ncutable code that leverages required tools for a\ngiven question. In contrast to the flexibility of-\nfered by free-form code generation in ViperGPT,\nChameleon adopts a more robust compositional ap-\nproach, utilizing an LLM-based planner to directly\ninvoke various tools in sequence via word match-\ning. We update these methods to use our pool of\nexperts and also incorporate more recent models as\nplanners to ensure a fair comparison.\nOur single agent baselines are zero-shot CoT\n(Wei et al., 2022), self-refinement (Madaan et al.,\n"}, {"page": 6, "text": "2023), and self-consistency (Wang et al., 2023).\nFor multi-agent baselines, we use Debate with\nConsensus and Debate with Judge. Specifically,\nwe follow the ReConcile LLM debate framework\noutlined by Chen et al. (2024b) for both consen-\nsus and InternVL2.5 as a judge. Debate with con-\nsensus counts the most frequent answer from the\nagents while debate with judge uses a separate\nVLM model to choose an answer. See Appendix B\nfor more details about the baselines.\n3.2\nResults\nDART is stronger than all tool-calling, single-\nagent, and multi-agent baselines.\nDART outper-\nforms training-free tool-calling systems by a wide\nmargin as seen in Table 1. On A-OKVQA, we see\n19.0% and 21.4% improvements over ViperGPT\nand Chameleon respectively (see Table 1 for full re-\nsults). The gap on both MMMU and NaturalBench\ncompared to gap between DART and single/multi-\nagent baselines. These tool-calling systems plan\ntheir tool execution in a single step based solely\non the question, and they lack the ability to rea-\nson directly on the image as an input. This is in\ncontrast to our own system that effectively incor-\nporates tools on top of existing VLM systems, al-\nlowing for better vision understanding with tools\nwhile building on the existing capabilities of VLMs.\nThe performance gain of DART over LLM-based\ntool calling methods illustrates the strengths of our\ntargeted tool-calling approach for a debate among\nmultiple VLM agents.\nWe\nalso\nsee\nconsistent\ngains\nacross\nall\nthree benchmarks over all baselines.\nOn A-\nOKVQA, DART reaches 68.9%, exceeding the\nbest single-agent baseline (Self-Consistency with\nOvis2) by 4.3% and the best multi-agent baseline\n(multi-model debate with judge) by 3.4%. This\ntrend continues across both MMMU and Natural-\nBench, where DART consistently outperforms both\nthe strongest single and multi-agent baselines. The\ngains are highest on A-OKVQA, which requires\nadvanced reasoning and world knowledge, indicat-\ning that our system improves the overall reason-\ning process of the answering agents. These im-\nprovements indicate that resolving disagreements\nwith expert tools and using tool-aligned agreement\nscores yields better reasoning and perception than\nprior single- or multi-agent approaches.\nDART can adapt to diverse domains by adding\nappropriate expert tools.\nTo show how DART\ncan adapt to domains where its current pool of ex-\nperts is unsuited for a task, we test on M3D with\nan additional medical expert. If DART is able to\nadapt to new experts, we expect the accuracy to\nincrease, as the medical tool provides valuable info\nto guide a better discussion. In Table 2, our method\noutperforms the baselines by 1.54% even given the\ndisparities in individual model strengths on this\ntask (i.e., Ovis2 is 10.0% weaker than QwenVL\non this dataset). While multi-agent debate under-\nperforms compared to the strongest single-agent\nbaseline by 2.72%, DART can effectively use the\nmedical expert and multiple agent reasoning paths\nto bridge this gap and improve even further over\nthe next best method (Qwen2.5-VL) by 1.5%. This\nindicates that DART can effectively incorporate\nnew experts, allowing it to adapt to new tasks with\nminimal implementation effort.\nDART leads to more diverse and fruitful discus-\nsion compared to existing multi-agent methods.\nTo evaluate the diversity and novelty of ideas dur-\ning discussion, we measure the textual overlap be-\ntween agents’ initial responses and their responses\nafter one round of discussion, comparing DART\nwith the standard multi-agent debate. For DART,\ntool outputs are appended to the initial responses,\nenabling the metrics to account for all available in-\nformation during the discussion round. As shown\nin Table 3, DART exhibits substantially lower over-\nlap across all metrics (ROUGE-1/2/L (Lin, 2004)\nand Jaccard Index), with ROUGE-L and Jaccard\nIndex decreasing by 0.173 and 0.289 respectively\ncompared to multi-agent debate (MAD) (Du et al.,\n2024). This indicates that discussions in DART\nare more diverse, with agents introducing novel\nreasoning paths rather than repeating prior content.\nThe integration of tool-based resolution enables\neach agent to refine or challenge other perspectives\nwith visual or textual evidence, resulting in richer\ndiscussion. In contrast, multi-agent debate seems\nto converge in terms of word usage, with agents re-\npeating similar or nearly identical reasoning. These\nfindings show that DART fosters more fruitful dis-\ncussion among agents.\nAdditional Results.\nWe conduct additional ex-\nperiments to study calibration, multi-round perfor-\nmance, efficiency, tool benefits, and prompt sensi-\ntivity in DART. This additional analysis is included\nin Appendix D.\n"}, {"page": 7, "text": "Method\nPlane Phase Organ Abnormality Location\nAvg.\nCREMA (Yu et al., 2025a)\n14.9\n26.7\n15.9\n17.3\n13.0\n17.2\nGPT-4o (OpenAI et al., 2024)\n83.3\n42.7\n50.0\n41.3\n41.3\n51.7\nMEXA (Yu et al., 2025b)\n65.0\n48.1\n60.9\n44.8\n48.0\n53.3\nOvis2\n59.6\n34.0\n39.8\n55.4\n43.0\n46.4\nMiniCPM-o\n79.8\n28.4\n53.9\n49.2\n37.8\n49.8\nQwenVL\n91.2\n33.5\n49.7\n60.0\n47.7\n56.4\nDebate with Consensus\n81.2\n32.0\n43.1\n60.9\n51.9\n53.7\nDebate with Judge\n79.5\n32.0\n45.6\n61.2\n51.7\n53.4\nDART (Ours)\n92.8\n40.7\n47.6\n55.9\n51.8\n57.8\nTable 2: Accuracy on the M3D medical dataset by question type and overall. DART performs the best out of the\ntested methods.\nMethod\nR-1\nR-2\nR-L\nJac. Index\nDebate with Consensus 0.679 0.529 0.550\n0.559\nDART\n0.452 0.310 0.377\n0.270\nTable 3: Comparison of DART and multi-agent debate\nacross text similarity metrics. R and Jac. Index stand\nfor ROUGE and Jaccard Index respectively.\nAttribute Detection: 21.23%\nCaptioning: 22.82%\nObject Detection: 5.20%\nReasoning: 7.22%\nGrounder: 27.14%\nOCR: 10.04%\nSpatial Reasoning: 6.34%\nFigure 3: Breakdown of total tool calling of DART on\nA-OKVQA.\n3.3\nAnalysis and Ablation Study\nDART calls on a variety of tools.\nIn Figure 3,\nwe present the distribution of tool calls made by\nDART when evaluated on A-OKVQA. The most\ncommon tools called were Grounder, Captioning,\nand Attribute Detection, with each called for over\n20% of questions with tool calls. These tools are\nmostly focused on physical scene understanding\nand composition, indicating that the VLM agents\noften differ in their vision understanding capabili-\nties. The less called tools were OCR, Reasoning,\nSpatial Reasoning, and Object Detection respec-\ntively, with each called for ≤10% of questions.\nThese are less holistic, more fine-grained vision\nskills that may not be needed for most questions.\nOverall, every tool is being invoked and contribut-\ning towards more detailed vision understanding.\nMethod\nAccuracy\nDART\n68.9\nw/o Diverse Answerers (only Ovis)\n65.3\nw/o Tool Agreement Scores\n67.8\nw/o Expert Tool Outputs†\n65.7\nTable\n4:\nAblations\nof\nDART\non\nA-OKVQA.\n†Agreement scores are impossible to calculate without\ntool outputs, so they are withheld.\nMethod\nAccuracy\nDART\n68.9\nDART (VLM as all tools)\n66.0\nDART (no VLM-based tools)\n66.7\nDART (no specialized model tools)\n65.6\nTable 5: Comparison between the full DART system\nand its variant using VLMs as tools. VLM-based tools\nare the captioning, attribute detection, and reasoning\ntools. Specialized model tools are the grounding, object\ndetection, spatial reasoning, and OCR tools.\nMultiple different answerers lead to more dis-\nagreements.\nWe compare using multiple models\n(i.e., QwenVL, MiniCPM-o, and Ovis2) as answers\nto multiple instances of the same model (i.e., Ovis2\nwith higher temperature). From Table 1, we see\nthat using a single model in DART results in a de-\ncrease of 4.6% accuracy on A-OKVQA. Then, we\nfind that using a single model results in less than\nhalf the disagreements than DART with multiple\nmodels, where we count 0.49 and 1.32 disagree-\nments on average respectively. This indicates that\nthe diverse perspectives inherent in a multi-model\nsystem yields a richer disagreement signal that al-\nlows for better expert tool calling.\nEffect of using a VLM as an expert tool.\nWhile\nVLMs exhibit strong general capabilities across\n"}, {"page": 8, "text": "perception and reasoning tasks, they often un-\nderperform compared to specialized models on\ndomain-specific tasks (i.e., grounding, spatial rea-\nsoning, etc.). In our original setting, we use a\ngeneral VLM for three tasks: captioning, attribute\ndetection, and reasoning. We extend this to check\nwhether a single VLM could indeed serve as a\nsubstitute for all tools. To quantify the trade-off\nbetween specialized models and general VLMs, we\nreplace all specialized tools in DART with a sin-\ngle general-purpose VLM (InternVL2.5) prompted\nto handle OCR, spatial reasoning, grounding, and\nobject detection on top of its original tasks. As\nshown in Table 5, this setting results in a 2.9%\ndecrease in overall accuracy on A-OKVQA. Al-\nthough VLMs are a convenient solution, their gen-\nerality may come at the cost of reliability particu-\nlarly for fine-grained visual understanding tasks.\nWe also measure the effect of ablating each\ntool type in this heterogeneous tool set: VLM-\nbased tools and specialized model tools. Removing\nthe VLM-based tools (captioning, attribute detec-\ntion, reasoning) lowers accuracy from 68.9% to\n66.7%, suggesting that the VLM tools are still\nable to provide a complementary signal for a\ndomain-specific task even without designated ex-\npert capabilities. Removing only specialized model\ntools (grounder, object detection, spatial reasoning,\nOCR) while keeping VLM-based tools lowers ac-\ncuracy to 65.3%, showing the importance of us-\ning specialized models in the tool pool. Overall,\nthese ablations indicate that the strongest perfor-\nmance occurs when combining specialized model\ntools with VLM-based tools, with specialized ex-\npert tools contributing the larger share of the per-\nformance gains.\nDART component ablations.\nWe assess the im-\npact of key components in our system, including\ndiverse answering agents, expert tools, and tool-\naligned agreement scores (results in Table 4). We\nconduct the ablation studies on A-OKVQA as it\nis specifically designed to test commonsense rea-\nsoning and needs precise visual understanding for\nquestions reliant on world knowledge. Removing\nthe multiple agent answerers or vision-expert com-\nponents degrades performance compared to the full\nDART system. We evaluate DART without diverse\nanswerers and with only the strongest performing\nmodel (i.e., Ovis2) on A-OKVQA. Using only the\nstrongest single model (Ovis2) on A-OKVQA (i.e.,\nwithout diverse answerers) leads to a decline in\naccuracy from 68.9% to 65.3%, highlighting the\nvalue of model diversity. This indicates that DART\nbenefits from having the diverse reasoning and ex-\npanded knowledge base of multiple VLMs. Fur-\nthermore, retaining a diverse set of VLM agents\nwithout expert confidence yields a 1.1% perfor-\nmance drop, showing that the calibrated confidence\nfrom the experts improves the discussion beyond\nregular self-reported confidence. Additionally, we\ntest the discussion framework without vision expert\noutputs. This setting is almost equivalent to Debate\nwith Judge (from Table 1) but with only one debate\nround. Accuracy drops by 3.2% in this experiment.\nThis result suggests that visual information from ex-\nperts leads to a higher-quality discussion compared\nto that of only answerers. In fact, the performance\ndip is comparable to that of having no diverse an-\nswerers, indicating that they are relatively equally\nimportant to the overall system. The ablations high-\nlight the importance of having multiple VLMs and\nvision experts working together in discussion.\n4\nRelated Work\nTool-Calling Systems.\nPast efforts have aug-\nmented large language models with external tools\nto overcome knowledge and reasoning limitations,\nespecially in vision-language tasks (Surís et al.,\n2023; Lu et al., 2023; Qin et al., 2024; Gupta and\nKembhavi, 2022; Chung-En et al., 2025; Yang et al.,\n2023; Shen et al., 2023; Schick et al., 2023; Parisi\net al., 2022). This tool augmentation has enabled\nLLMs to tackle tasks such as finding real-time in-\nformation through web search (Lu et al., 2023;\nNakano et al., 2022), accessing vision information\n(Lu et al., 2023; Surís et al., 2023; Wu et al., 2023;\nYang et al., 2023; Gupta and Kembhavi, 2022),\nand acquiring expert domain-specific knowledge\n(Das et al., 2024; Yu et al., 2025c). One direction\nhas used a programmatic approach by leveraging\nLLMs inherent code generation capabilities to han-\ndle tool calls (Shen et al., 2024; Surís et al., 2023)\nwhile other work has experimented with directly\nprompting an LLM to construct a series of tool\ncalls (Lu et al., 2023; Chung-En et al., 2025; Yang\net al., 2023). While past work has mostly focused\non using a single LLM agent for tool calling, we\ninvestigate how using disagreements in multimodal\nmulti-agent debate can lead to more effective tool\ncalling and discussion.\nMulti-Agent Systems.\nAnother line of research\nimproves reasoning by enlisting multiple LLM\n"}, {"page": 9, "text": "agents that collaborate or compete among one an-\nother to arrive at a stronger conclusion (Chen et al.,\n2024b; Estornell et al., 2025b; Qian et al., 2024;\nZhao et al., 2024; Tang et al., 2024; Yin et al.,\n2024; Zhang et al., 2024; Du et al., 2023).\nA\nlarge body of work has explored designing a de-\nbate framework for agents to interact and refine\ntheir answers through multiple rounds of discus-\nsion (Chen et al., 2024b; Estornell and Liu, 2024;\nDu et al., 2023). Others have tried assigning agents\nspecific functions or roles to accomplish different\ngoals or subtasks for a given query (Estornell et al.,\n2025b; Hong et al., 2024; Wang et al., 2024c). Past\nwork has also tested how to control the level of\ndisagreement in a debate (Khan et al., 2024; Liang\net al., 2024; Chang, 2025). Our method contributes\nseveral novelties along a number of axes. First,\nprior multi-agent debate methods have largely omit-\nted tool use while we inject novel information via\ndisagreement-triggered tool use. Moreover, we ex-\npand this into a less explored frontier by examining\nmultimodal agents instead of text.\n5\nConclusion\nIn this work, we introduce DART, a multi-agent\nframework that recruits domain-specific vision\ntools based on disagreements among VLM agents.\nBy identifying points of disagreement in agent dis-\ncussions and selectively invoking expert tools to\nresolve these disagreements, DART combines the\nreasoning flexibility of VLMs with the specialized\nperception capabilities of expert tools. Through\nthis, DART sees consistent improvements across\nfour diverse benchmarks over strong baselines on\nA-OKVQA, MMMU, NaturalBench, and M3D.\nDART represents a promising direction for mul-\ntimodal reasoning that harnesses the strengths of\ngeneralist models and specialized tools through\ndisagreement resolution.\nAcknowledgments\nThis work was supported by NSF-CAREER\nAward 1846185, DARPA ECOLE Program No.\nHR00112390060, ONR Grant N00014-23-1-2356,\nARO Award W911NF2110220, Microsoft Accel-\nerate Foundation Models Research (AFMR) grant\nprogram, NSF AI Engage Institute DRL-2112635,\nNational Institutes of Health (NIH) under other\ntransactions 1OT2OD038045-01, and Cisco and\nCapital One Faculty Awards. The views and con-\nclusions contained in this document are those of the\nauthors and should not be interpreted as represent-\ning official policies, either expressed or implied, of\nthe NIH or other sponsors.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: Visual Question An-\nswering. In International Conference on Computer\nVision (ICCV).\nFan Bai, Yuxin Du, Tiejun Huang, Max Q. H. Meng, and\nBo Zhao. 2024. M3d: Advancing 3d medical image\nanalysis with multi-modal large language models.\nPreprint, arXiv:2404.00578.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wen-\nbin Ge, Sibo Song, Kai Dang, Peng Wang, Shi-\njie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu,\nMingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei\nWang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others.\n2025. Qwen2.5-vl technical report. arXiv preprint\narXiv:2502.13923.\nEdward Y. Chang. 2025. Evince: Optimizing multi-llm\ndialogues using conditional statistics and information\ntheory. Preprint, arXiv:2408.14575.\nBoyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter,\nDorsa Sadigh, Leonidas Guibas, and Fei Xia. 2024a.\nSpatialvlm: Endowing vision-language models with\nspatial reasoning capabilities. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 14455–14465.\nJustin Chih-Yao Chen, Swarnadeep Saha, and Mohit\nBansal. 2024b. Reconcile: Round-table conference\nimproves reasoning via consensus among diverse\nllms. Preprint, arXiv:2309.13007.\nMing-Chang Chiu, Fuxiao Liu, Karan Sapra, Andrew\nTao, Yaser Jacoob, Xuezhe Ma, Zhiding Yu, and\nGuilin Liu. 2025. Aide: Agentically improve vi-\nsual language model with domain experts. Preprint,\narXiv:2502.09051.\nChung-En, Yu, Hsuan-Chih, Chen, Brian Jalaian, and\nNathaniel D. Bastian. 2025. Hydra: An agentic rea-\nsoning approach for enhancing adversarial robust-\nness and mitigating hallucinations in vision-language\nmodels. Preprint, arXiv:2504.14395.\nDebrup Das, Debopriyo Banerjee, Somak Aditya,\nand Ashish Kulkarni. 2024. Mathsensei: A tool-\naugmented large language model for mathematical\nreasoning. Preprint, arXiv:2402.17231.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B.\nTenenbaum, and Igor Mordatch. 2023. Improving\nfactuality and reasoning in language models through\nmultiagent debate. Preprint, arXiv:2305.14325.\n"}, {"page": 10, "text": "Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenen-\nbaum, and Igor Mordatch. 2024. Improving factual-\nity and reasoning in language models through multia-\ngent debate. In International Conference on Machine\nLearning, pages 11733–11763. PMLR.\nAndrew Estornell and Yang Liu. 2024. Multi-LLM\ndebate: Framework, principals, and interventions.\nIn The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems.\nAndrew Estornell, Jean-Francois Ton, Yuanshun Yao,\nand Yang Liu. 2025a. ACC-collab: An actor-critic\napproach to multi-agent LLM collaboration. In The\nThirteenth International Conference on Learning\nRepresentations.\nAndrew Estornell, Jean-Francois Ton, Yuanshun Yao,\nand Yang Liu. 2025b. Acc-collab: An actor-critic\napproach to multi-agent llm collaboration. Preprint,\narXiv:2411.00053.\nGoogle. 2025.\nMedgemma hugging face.\nhttps:\n//huggingface.co/collections/google/\nmedgemma-release-680aade845f90bec6a3f60c4.\nTanmay Gupta and Aniruddha Kembhavi. 2022. Vi-\nsual programming: Compositional visual reasoning\nwithout training. Preprint, arXiv:2211.11559.\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu\nZheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang,\nZili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang\nZhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu,\nand Jürgen Schmidhuber. 2024. MetaGPT: Meta pro-\ngramming for a multi-agent collaborative framework.\nIn The Twelfth International Conference on Learning\nRepresentations.\nAkbir Khan, John Hughes, Dan Valentine, Laura\nRuis, Kshitij Sachan, Ansh Radhakrishnan, Edward\nGrefenstette, Samuel R. Bowman, Tim Rocktäschel,\nand Ethan Perez. 2024. Debating with more persua-\nsive llms leads to more truthful answers. Preprint,\narXiv:2402.06782.\nRahima Khanam and Muhammad Hussain. 2024.\nYolov11: An overview of the key architectural en-\nhancements. Preprint, arXiv:2410.17725.\nBaiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu\nNyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja,\nRanjay Krishna, Graham Neubig, and Deva Ramanan.\n2024.\nNaturalbench: Evaluating vision-language\nmodels on natural adversarial samples. In The Thirty-\neight Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023. BLIP-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. In Proceedings of the 40th Interna-\ntional Conference on Machine Learning, volume 202\nof Proceedings of Machine Learning Research, pages\n19730–19742. PMLR.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang,\nYan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and\nZhaopeng Tu. 2024. Encouraging divergent thinking\nin large language models through multi-agent debate.\nPreprint, arXiv:2305.19118.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. 2024. Lla-\nvanext: Improved reasoning, ocr, and world knowl-\nedge.\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li,\nHao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, and 1 others. 2023.\nGround-\ning dino: Marrying dino with grounded pre-training\nfor open-set object detection.\narXiv preprint\narXiv:2303.05499.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and\nJianfeng Gao. 2023. Chameleon: Plug-and-play com-\npositional reasoning with large language models. In\nThe 37th Conference on Neural Information Process-\ning Systems (NeurIPS).\nShiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Wei-\nhua Luo, Kaifu Zhang, and Han-Jia Ye. 2024. Ovis:\nStructural embedding alignment for multimodal large\nlanguage model. arXiv:2405.20797.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck,\nBodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. In 37th Conference on Neural Infor-\nmation Processing Systems.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, Edouard Grave, Yann LeCun, and\nThomas Scialom. 2023. Augmented language mod-\nels: a survey. Preprint, arXiv:2302.07842.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2022. Webgpt: Browser-\nassisted question-answering with human feedback.\nPreprint, arXiv:2112.09332.\nOpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher,\nAdam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec\nRadford, Aleksander M ˛adry, Alex Baker-Whitcomb,\nAlex Beutel, Alex Borzunov, Alex Carney, Alex\nChow, Alex Kirillov, and 401 others. 2024. Gpt-4o\nsystem card. Preprint, arXiv:2410.21276.\n"}, {"page": 11, "text": "OpenAI. 2025. GPT-5 system card. Technical report,\nOpenAI. OpenAI system card, August 13, 2025.\nMahdi Pakdaman Naeini, Gregory Cooper, and Milos\nHauskrecht. 2015. Obtaining well calibrated proba-\nbilities using bayesian binning. Proceedings of the\nAAAI Conference on Artificial Intelligence, 29(1).\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022.\nTalm: Tool augmented language models. Preprint,\narXiv:2205.12255.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024.\nChatdev: Communica-\ntive agents for software development.\nPreprint,\narXiv:2307.07924.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,\nRuobing Xie, Jie Zhou, Mark Gerstein, dahai li,\nZhiyuan Liu, and Maosong Sun. 2024. ToolLLM:\nFacilitating large language models to master 16000+\nreal-world APIs. In The Twelfth International Con-\nference on Learning Representations.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nPreprint, arXiv:2302.04761.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\nA-okvqa: A benchmark for visual question answering\nusing world knowledge. In Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23–27, 2022, Proceedings, Part VIII, pages\n146–162. Springer.\nRuoyue Shen, Nakamasa Inoue, and Koichi Shinoda.\n2024.\nPyramid coder: Hierarchical code gener-\nator for compositional visual question answering.\nPreprint, arXiv:2407.20563.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends\nin huggingface. In Advances in Neural Information\nProcessing Systems.\nDídac Surís, Sachit Menon, and Carl Vondrick. 2023.\nVipergpt: Visual inference via python execution for\nreasoning. Proceedings of IEEE International Con-\nference on Computer Vision (ICCV).\nXunzhu Tang, Kisub Kim, Yewei Song, Cedric Lothritz,\nBei Li, Saad Ezzini, Haoye Tian, Jacques Klein, and\nTegawende F. Bissyande. 2024.\nCodeagent: Au-\ntonomous communicative agents for code review.\nPreprint, arXiv:2402.02172.\nQineng Wang, Zihao Wang, Ying Su, Hanghang Tong,\nand Yangqiu Song. 2024a. Rethinking the bounds of\nllm reasoning: Are multi-agent discussions the key?\nIn Long Papers, Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics,\npages 6106–6131. Association for Computational\nLinguistics (ACL). The authors of this paper were\nsupported by the NSFC Fund (U20B2053) from the\nNSFC of China, the RIF (R6020-19 and R6021-20),\nand the GRF (16211520 and 16205322) from RGC\nof Hong Kong. We also thank the support from the\nUGC Research Matching Grants (RMGS20EG01-\nD, RMGS20CR11, RMGS20CR12, RMGS20EG19,\nRMGS20EG21, RMGS23CR05, RMGS23EG08).;\n62nd Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2024 ; Conference date:\n11-08-2024 Through 16-08-2024.\nWeiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao,\nYangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou\nZhu, Lewei Lu, Yu Qiao, and Jifeng Dai. 2024b. En-\nhancing the reasoning ability of multimodal large\nlanguage models via mixed preference optimization.\narXiv preprint arXiv:2411.10442.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nZhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao\nGe, Furu Wei, and Heng Ji. 2024c. Unleashing the\nemergent cognitive synergy in large language mod-\nels: A task-solving agent through multi-persona self-\ncollaboration. Preprint, arXiv:2307.05300.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. 2023. Visual\nchatgpt: Talking, drawing and editing with visual\nfoundation models. Preprint, arXiv:2303.04671.\nBenfeng Xu, An Yang, Junyang Lin, Quan Wang,\nChang Zhou, Yongdong Zhang, and Zhendong Mao.\n2025.\nExpertprompting:\nInstructing large lan-\nguage models to be distinguished experts. Preprint,\narXiv:2305.14688.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin\nLin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-\nreact: Prompting chatgpt for multimodal reasoning\nand action. arXiv preprint.\nYuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo\nCui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin\nZhao, Zhihui He, and 1 others. 2024. Minicpm-v:\nA gpt-4v level mllm on your phone. arXiv preprint\narXiv:2408.01800.\n"}, {"page": 12, "text": "Xiangyu Yin, Chuqiao Shi, Yimo Han, and Yi Jiang.\n2024. Pear: A robust and flexible automation frame-\nwork for ptychography enabled by multiple large\nlanguage model agents. Preprint, arXiv:2410.09034.\nShoubin Yu, Jaehong Yoon, and Mohit Bansal. 2025a.\nCREMA: Generalizable and efficient video-language\nreasoning via multimodal modular fusion. In The\nThirteenth International Conference on Learning\nRepresentations.\nShoubin Yu, Yue Zhang, Ziyang Wang, Jaehong Yoon,\nand Mohit Bansal. 2025b. Mexa: Towards general\nmultimodal reasoning with dynamic multi-expert ag-\ngregation. arXiv preprint arXiv:2506.17113.\nShoubin Yu, Yue Zhang, Ziyang Wang, Jaehong Yoon,\nand Mohit Bansal. 2025c. Mexa: Towards general\nmultimodal reasoning with dynamic multi-expert ag-\ngregation. Preprint, arXiv:2506.17113.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao\nYu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan\nZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, and\n3 others. 2024. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark\nfor expert agi. In Proceedings of CVPR.\nCeyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang,\nGuanghe Li, Yihang Sun, Cheng Zhang, Zhaowei\nZhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang,\nJunge Zhang, Feng Yin, Yitao Liang, and Yaodong\nYang. 2024. Proagent: Building proactive cooper-\native agents with large language models. Preprint,\narXiv:2308.11339.\nXiutian Zhao,\nKe Wang,\nand Wei Peng. 2024.\nAn electoral approach to diversify llm-based\nmulti-agent collective decision-making. Preprint,\narXiv:2410.15168.\nA\nModel Configurations\nWe use the default generation configurations from\nHuggingFace for Qwen2.5-VL 7B (Apache-2.0 li-\ncense), MiniCPM-o 2.6 8B (Apache-2.0 license),\nand Ovis2 8B (Apache-2.0 license). Qwen2.5-VL\nuses a temperature of 10−6 and repetition penalty\nof 1.05. MiniCPM-o and Ovis2 both use a tem-\nperature of 0.7 and repetition penalty of 1.05. For\ninstances of multi-agent debate and DART where\nwe would like more answer diversity with a sin-\ngle model, we increase temperature to 0.9 for all\nagents.\nWe use the grounding-dino-base implementa-\ntion in HuggingFace of GroundingDINO (Apache-\n2.0 license) with box and text thresholds of 0.3. We\nuse OCR-Qwen (Apache 2.0 License), a Qwen2.5\n7B model finetuned for OCR, with default genera-\ntion configuration of temperature 0.1 and repetition\npenalty 1.05. We use SpaceLLaVA (Apache-2.0\nLicense) for spatial reasoning with a temperature\nof 0.2 and repetition penalty of 1.1. InternVL2.5-\nMPO 8B (MIT License) uses temperature 0.8 for\nall three expert tool functionalities.\nB\nBaseline Details\nWe compare DART to multiple single-agent and\nmulti-agent baselines.\nThe single agent base-\nlines include zero-shot CoT (Wei et al., 2022),\nself-refinement (Madaan et al., 2023), and self-\nconsistency (Wang et al., 2023). In zero-shot COT,\nwe prompt models to first output their reasoning\nbefore providing their final answer.\nWith self-\nrefinement, models are asked to provide feedback\non their own output and update their answer accord-\ningly. This assesses whether models can improve\nthemselves based on their own feedback. With self-\nconsistency, we sample multiple reasoning paths\nfrom a model and take the most consistent answer.\nThis takes advantage of the existence of multiple\nreasoning paths to arrive at an answer by increasing\nthe number of generated samples per question.\nWe include comparisons to training-free single-\nagent tool-calling baselines: ViperGPT (Surís et al.,\n2023) and Chameleon (Lu et al., 2023). ViperGPT\nutilizes a code generation model to predict exe-\ncutable code that leverages required tools for a\ngiven question. In contrast to the flexibility of-\nfered by free-form code generation in ViperGPT,\nChameleon adopts a more robust compositional ap-\nproach, utilizing an LLM-based planner to directly\ninvoke various tools in sequence via word matching.\nWe update both implementations, using Qwen2.5\n7B Instruct as the code generation model and in-\ncorporating our expert tools into both systems. For\nViperGPT specifically, we also update its VLM\nmodule (called on by the code generation model)\nfrom BLIP2 (Li et al., 2023) to InternVL-2.5 MPO\nfor a fairer comparison with our system.\nFinally, we also evaluate against strong multi-\nagent baselines designed for text-based tasks, in-\ncluding both consensus-based debate and debate\nwith a judge. We follow the ReConcile LLM de-\nbate framework outlined by Chen et al. (2024b)\nfor both consensus and InternVL2.5 as a judge. In\nthe consensus setting, the majority – or consensus\n– answer is automatically taken (a random answer\nis taken if all agents disagree). In the judge setting,\na judge model is given the image along with each\nanswer, justification, and confidence before picking\n"}, {"page": 13, "text": "a single final answer.\nTool Conf.? Metric\nECE↓\n✗\nSelf-Reported\n0.4581\n✓\nSelf-Reported\n0.3859\n✓\nTool-Confidence 0.4901\nTable 6: Expected calibration error (ECE) of DART with\nand without tool confidence on self-reported confidence\nmetrics and our tool-based agreement score metric.\nC\nDataset Details\n• A-OKVQA (Apache-2.0 License): We evalu-\nate on the validation split of 1135 samples.\n• MMMU (Apache-2.0 License): We evaluate\non the validation split of 900 samples.\n• NaturalBench (Apache-2.0 License): We\nevaluate on the full dataset of 7600 question-\nimage pairs.\nD\nAdditional Results\nDART provides a stronger confidence metric\nbased on vision tools.\nWe test the effectiveness\nof calculating confidence based on agreement to\nvision tools. For this, we run DART under two set-\ntings: 1) using only self-reported confidence, and\n2) combining self-reported confidence with tool-\nbased agreement scores. Both settings still involve\ndisagreement resolution with tools. We then mea-\nsure the effectiveness using expected calibration\nerror (ECE) (Pakdaman Naeini et al., 2015) of self-\nreported confidence only, tool-based confidence,\nand self-reported confidence given tool-based con-\nfidence during discussion. Recall that both self-\nreported confidence and tool-based confidence are\nused in DART during discussion.\nWe observe higher ECE when relying solely on\nself-reported confidence (i.e., without tool-based\nagreement scores) than when including those\nscores: 0.4581 vs. 0.3859 respectively (see Ta-\nble 6).\nNote however that the raw tool-based\nconfidence calculated in DART has ECE of\n0.4901, which indicates weaker calibration than\nself-reported confidence on its own.\nMultiple rounds of debate in multimodal multi-\nagent systems is ineffective.\nEncouraged by the\nsuccess of multiple debate rounds in text-based\nmulti-agent environments, we apply our framework\nacross successive debate rounds to see if the same\nRound QwenVL MiniCPM Ovis w/ Aggregation\nInitial\n61.3\n55.7\n61.0\n59.4\n1\n68.6\n66.8\n69.2\n68.9\n2\n68.7\n67.2\n68.7\n68.4\n3\n68.4\n67.4\n68.7\n68.5\nTable 7: DART performance over three rounds of\ndisagreement resolution and expert confidence on A-\nOKVQA.\n0\n1\n2\n3\nRounds\n56\n58\n60\n62\n64\n66\n68\n70\nAccuracy (%)\nAccuracy on A-OKVQA over Multiple Rounds\nMAD with Judge\nDART\nFigure 4: Performance of DART and multi-agent debate\nover three rounds. The error bars indicate the standard\ndeviation among the individual answering agents.\npattern emerges. We compare the improvement of\nDART to multi-agent debate with judge over three\nrounds in Fig. 4. DART after one round of expert\ntool intervention outperforms the round three per-\nformance of multi-agent debate with judge. Both\nmethods have the largest improvement in round\nafter the initial answer generation. The improve-\nment is likely due to initial exposure to other agent\noutputs and, in the case of DART, tool outputs.\nHowever, after the first round of discussion both\nstagnate in subsequent rounds (with <2% accuracy\ngains between rounds 1 and 3). For multi-agent\ndebate, the agents are not able to effectively refine\ntheir reasoning and answer after round one as there\nis no new information being introduced. For DART,\nwe observe that the most major disagreements are\nresolved in round 1. Future rounds are prone to re-\nsolving more minor disagreements that can distract\nfrom the question. We find that it is insufficient to\ngive any piece of new information in a multimodal\nmulti-agent system. In fact, the new expert tool\ninformation must be sufficiently good and relevant\nto advance discussion.\nRobustness of DART aggregator to model choice.\nWe test four different VLMs as aggregator models:\nQwen2.5-VL, MiniCPM-o, Ovis2, and InternVL-\n"}, {"page": 14, "text": "Model\nAll Choices Accuracy\nInternVL2.5\n✗\n68.3\nQwen2.5-VL\n✗\n68.7\nMiniCPM-o 2.6\n✗\n67.8\nOvis2\n✓\n68.5\nOvis2\n✗\n68.9\nTable 8:\nAggregator Ablation on A-OKVQA. All\nchoices indicates whether answers from initial genera-\ntion are used.\n2.5 MPO. The results are shown in Table 8. Be-\ntween the strongest and weakest aggregator (Ovis2\nand MiniCPM-o, respectively), there is a differ-\nence in accuracy of 1.1%. In fact, among the top 3\nmodels, accuracies are within 0.6% of each other.\nThis indicates DART has relatively low sensitivity\nto aggregator choice and will generalize well to\nnew models as aggregators. Additionally, we test\nhow the aggregator model responds when answer\nchoices from the initial generation stage are pro-\nvided in addition to the discussion stage answers.\nWe find that including these additional answers\nslightly lowers performance by 0.4%. This sug-\ngests that having a more concise answer set after\nusing tools (i.e., post discussion answers) makes ag-\ngregation easier compared to lower quality answers\nfrom initial generation.\nWe also find that the tool-based agreement scores\nare beneficial in improving calibration during ag-\ngregation (see Table 6 in the Appendix.). When\naggregating with agreement scores, the final self-\nreported confidence values in DART are better cal-\nibrated and obtain a relative decrease of 15.8%\nin expected calibration error (ECE) compared to\nself-reported confidence values without tool-based\nagreement scores. More discussion on the calibra-\ntion calculations is in Appendix D.\nMethod\nAccuracy\nDART\n68.9\nw/o Aggregation (i.e., Majority Vote)\n67.9\nw/o Discussion\n62.5\nTable 9: Ablations of discussion and aggregation steps\nof DART on A-OKVQA.\nAdditional Method Ablations.\nWe observe that\nremoving aggregation and instead using a simple\nmajority vote leads to reduced accuracy by 1.0%\nin Table 9, confirming that the aggregation step is\nmore effective than naive majority vote. In fact,\nmajority vote underperforms aggregation in all but\none setting tested in Table 8, demonstrating that\nproper processing of agent reasoning and tool out-\nputs creates better final answers. Finally, removing\nthe discussion phase causes the largest performance\ndrop by 6.4%, emphasizing its role in refining the\npreliminary answers. In fact, the quality of an-\nswers after the initial reasoning phase is substan-\ntially higher (up to 11.1% higher for the weakest\nagent as reported in Table 7). This indicates the\ndiscussion step allows for stronger reasoning and\nresponses, yielding to a more accurate downstream\nsystem output. The ablations emphasize the impor-\ntance of incorporating multiple VLMs, expert tool\ninformation, and discussion steps to the system.\nModel\nAvg. Tokens Generated\nOvis2\n74.3\nMiniCPM-o\n36.7\nQwenVL\n80.5\nMAD with Consensus\n1031.8\nMAD with Judge\n1196.8\nDART\n928.5\nTable 10: We calculate the average tokens generated of\neach method on A-OKVQA. This represents the relative\ncost of each method.\nMethod\nTotal Runtime (hrs) TFLOPs Latency (s)\nOvis2\n0.13\n21.17\n0.38\nMiniCPM-o\n0.09\n18.92\n0.26\nQwenVL (7B)\n0.05\n19.04\n0.16\nQwenVL (32B)\n0.20\n88.01\n0.64\nMAD w/ Consensus\n7.97\n215.66\n21.84\nMAD w/ Judge\n8.88\n236.83\n27.92\nDART\n10.52\n158.15\n32.75\nTable 11: Comparison of system efficiency across dif-\nferent methods on A-OKVQA.\nDART provides a balance between performance\nand computation compared to both single and\nmulti-agent baselines.\nWe investigate the cost\nof using DART compared to single and multi-agent\nbaselines. We first count the average number of\ntokens generated per method on A-OKVQA and\npresent the results in Table 10. As expected, sin-\ngle agent methods with CoT are the cheapest, with\nQwenVL (the most token-heavy single agent base-\nline) generating 80.5 tokens compared to DART’s\n928.5. However, compared to multi-agent systems,\nDART needs 103.3 fewer tokens on average than a\nmulti-agent debate system.\nWe further assess computational efficiency in\nTable 11, reporting total runtime, floating-point op-\n"}, {"page": 15, "text": "erations (TFLOPs), and latency per query. While\nDART incurs higher runtime and latency than\nsingle-model systems such as QwenVL, its cost\nremains competitive with other multi-agent meth-\nods. In fact, in terms of FLOPs, DART requires\nfewer FLOPs than the multi-agent baselines (al-\nthough still behind in total runtime and latency).\nThe observed difference in runtime and latency be-\ntween multi-agent debate and DART is likely due\nto inefficient tool-calling implementation relative\nto discussion/answering implementations.\nAdditionally, we test a larger VLM (Qwen2.5-\nVL 32B) to compare efficiency and performance\nwhen scaling versus using multiple agents. While\nQwen2.5-VL 32B is more cost-effective, its per-\nformance gains are modest (scoring 62.7% on\nA-OKVQA). Although pricier, multi-agent ap-\nproaches like DART may offer a viable path to\nimproved results when scaling yields diminishing\nreturns. Overall, DART achieves stronger perfor-\nmance from the incorporation of tools at the cost of\nreduced efficiency compared to multi-agent meth-\nods and single-agent methods.\nTool Witheld\nAccuracy\nDART\n68.9\nw/o Grounder\n66.5\nw/o Captioning\n67.5\nw/o Attribute Detection\n68.6\nw/o OCR\n68.1\nw/o Reasoning\n68.4\nw/o Spatial Reasoning\n68.0\nw/o Object Detection\n68.9\nTable 12: Ablation for tools of DART on A-OKVQA.\nWe measure performance impact on the entire system\nwhen we remove a single tool from the tool set. Tools\nare ordered in terms of most to least frequent as reported\nin Fig. 3.\nEach tool complements DART’s performance.\nWe seek to test whether DART is able to effectively\nuse the tools that it has been equipped with. From\nTable 12 we can infer the relative importance of\neach tool to DART’s overall performance on A-\nOKVQA. In fact, we observe that the performance\ndrops roughly correlate with the tool usage fre-\nquency recorded in Fig. 3. More specifically, the\ngrounder and captioning tools – each contributing\nto over 20% of tool invocations on A-OKVQA –\nare the most critical, as their removal leads to a\ndegradation of more than 2% in accuracy. The re-\nmaining tools (OCR, spatial reasoning, attribute\ndetection, and reasoning) yield smaller decreases,\ntypically within 1%, suggesting that DART remains\nrelatively robust to their absence. Interestingly, the\nomission of object detection does not noticeably\naffect accuracy, implying that visual grounding and\ncaption-based context already provide sufficient\nperceptual cues for most questions. Overall, these\nresults highlight that while certain tools (particu-\nlarly grounding and captioning) play a dominant\nrole, the complementary interaction among all tools\ncontributes to the system’s strong performance,\ndemonstrating that DART effectively leverages its\ntool reasoning capabilities.\nDART is robust to prompt choice.\nWe test\nDART’s performance under an altered set of\nprompts to test the system’s sensitivity to prompt\nchoice. Each stage’s prompt is passed into GPT-5\n(OpenAI, 2025) with the prompt “I am creating\na prompt for a system that uses tools and agents.\nRephrase the following prompt:” while we manu-\nally adjust the newly generated prompt to ensure\nthe output format and handling of string variables\nis consistent.\nTesting this new set of prompts\non A-OKVQA, we achieve accuracy of 68.7%, a\n0.2% decrease from our previous baseline of 68.9%.\nWhile there is a dip in performance, the change is\nrelatively small, and our method still outperforms\nthe other baselines.\nE\nPrompts\nWe provide the prompts used in our methods for\nreproducibility. The details on how these prompts\nare used are provided in Section 2.\nF\nQualitative Examples\nWe provide two qualitative examples showcasing\nthe full DART pipeline in Fig. 6 and Fig. 5.\n"}, {"page": 16, "text": "Initial Answer Generation Prompt\n<question>\nAnswer the question with only one word or phrase, then provide step-by-step reasoning for the answer. Finally, provide a confidence score from 0-1 for\nyour answer (0 meaning not confident at all, 1 meaning complete confidence)\nYou must output in the following format:\nAnswer: [answer]\nReasoning: [reasoning]\nConfidence: [confidence]\nDisagreement Resolution Prompt\nHere was the initial prompt: <full initial answer generation prompt>\nCarefully review the following solutions from other agents for the provided question. Now, analyze what disagreements are occurring between the\ndifferent agents.\n<grouped solution>\nYou have the ability to call on different experts, each with their own specialized capabilities. Based on the disagreements you observed, pick out the set of\nexperts (could be just one) that would be best equipped to solve all the disagreements.\nHere are all the experts, their inputs, and their capabilities/usage:\n\"spatial\" (input: list. objects that have confused spatial relations) - Has perfect understanding of spatial relations between objects. Use this when agents\nare unsure about the placement of items in a scene. \"ocr\" (input: none)- Can correctly read all text in an image. Use this when agents have differing\nviews on what the text is in an image. \"grounder\" (input: list. objects you are trying to find) - Will find any object if it is an image, otherwise it will\nreturn nothing. Use this when agents are not agreeing on what’s present in an image. \"detector\" (input: none) - Will provide a list of objects in the\nimage, their counts, and their bounding boxes. Only use this when agents are differing in their counts of objects in an image. \"captioning\" (input:\nlist. objects you want captions for) - Can give a detailed description of what’s going in the image relevant to the question. Use this when agents\nmight need a better idea of the general scene or descriptions of specific objects. \"attribute\" (input: list. objects you want attributes for) - Will give\ninformation on different features of objects in the image, including color, properties, catgories, and more. Use this when agents are confused about\nthe features of relevant objects and need many surface level features. \"reasoning\" (input: list. objects you want reasoning for) - Has better world\nknowledge and advanced reasoning capabilities about what might be going on in an image. Use this when agents are confused or conflicting in their in-\nferences about the scene. This is essentially a meta-reasoning agent that intervenes when models have different conclusions based on the same assumptions.\nOutput the expert(s) you need to resolve the disagreements to answer the original question: . This should be a JSON with this format like this. You can\ncall more or less experts than this as needed:\n{\n\"experts\": [\"grounder\", \"attribute\", \"ocr\"],\n\"inputs\": {\n\"grounder\": {\n\"disagreement\": \"Agent 1 mentioned that there is a cat, but Agent 2 said there is no cat and instead said it is a dog.\",\n\"justification\": \"The grounder will help resolve the disagreement about the presence of a cat or dog in the image.\",\n\"arguments\": [\"cat\", \"dog\"]\n},\n\"attribute\": {\n\"disagreement\": \"Agent 1 said the flower is red, Agent 2 said it is orange, and Agent 3 did not specifically mention anything about the flower.\nThere also was confusion about the details of the car.\",\n\"justification\": \"The attribute expert will help resolve the disagreement about the color of the flower and provide details about the car.\",\n\"arguments\": [\"flower\", \"car\"]\n},\n\"ocr\": {\n\"disagreement\": \"Agents conflict on the text they see in the image.\",\n\"justification\": \"The OCR expert will see the text in the image and resolve the disagreement.\",\n\"arguments\": []\n}\n}\n}\nNow give the expert output in the given format for the previous agent solutions and the question provided above. Do not be redudant on disagreements\nunless the expert is adding new information that better resolves the disagreement. Do not add an expert unless absolutely necessary.\nReminder, the question is: <question>\nTool Agreement Prompt\nYou are an aligning agent. That means that you determine whether or not two different outputs are misaligned (0) or aligned (1). An expert output has\nprovided a response to a disagreement between multiple different agents. Your task is to determine whether or not the expert output is aligned with a\nsingle agent’s output. Reason about the alignment of the agent output with the expert output. Then, output a 0 if the agent output is not aligned with the\nexpert’s output, and a 1 if it is aligned.\nOutput Format:\n{\n\"reasoning\": \"Your reasoning for the alignment of the agent output with the expert output.\",\n\"alignment\": \"0 if the agent output is not aligned with the expert’s output, and 1 if it is aligned.\"\n}\nDisagreement: <disagreement>\nExpert Output: <expert_output>\nAgent Output: <agent_output>\n"}, {"page": 17, "text": "Discussion Prompt\nCarefully review the following solutions from other agents as additional information, and provide your own answer and step-by-step reasoning to the\nquestion. Then, give your confidence in your new answer.\nClearly state which point of view you agree or disagree with and why.\n<grouped_solutions>\nHere is information about the image from expert(s). You should take any information presented in this as factual unless you truly notice something wrong\nwith the expert info (and make sure to state this).\n<tool_outputs>\nOutput your response in the following format:\nReasoning: [reasoning for your answer to the original question based on the other agents responses]\nAnswer: [new answer]\nConfidence: [confidence]\nSome example outputs:\nReasoning: I agree with Agent 1’s reasoning that the answer is A because it aligns with the medical expert’s analysis of the CT scan. The medical expert\nconfirmed that the condition described in the question is consistent with the findings in the image.\nAnswer: Option A\nConfidence: 0.65\nReasoning: I disagree with Agent 2’s reasoning that the answer is B. The medical expert’s analysis suggests that the condition is more likely to be A, as it\nmatches the symptoms described in the question and the findings in the image.\nAnswer: Option A\nConfidence: 0.95\nReasoning: I agree with Agent 1 and 3’s reasoning that the answer is C. The medical expert’s analysis supports this conclusion, as it aligns with the\nfindings in the image and the symptoms described in the question.\nAnswer: Option C\nConfidence: 0.30\nYou must follow the provided format no matter what. This rule is unbreakable.\nAggregator Prompt\nYou are an aggregator model that will be given an image, question, and set of answer choices. Your tasks is to select the best final answer for the question.\nYou will also be given various sources of information to help inform your decision. Each answer was generated by a different agent, and these agent will\nprovide their reasoning for why they gave their answer. You will also be given tool outputs from expert models that directly relate to the question and were\nused to resolve disagreement among the answering agents. Feel free to defer to these expert tool outputs if the answers contradict the info from the tools.\nThe question is: <question>\nHere are the different agent answers:\n<grouped_output>\nHere are the tool outputs:\n<tool_outputs>\nNow based on the provided information, provide your step-by-step reasoning for selecting the best, most correct answer to the question provided. Then,\ngive your confidence in your selected answer.\nOutput in the following format:\nReasoning: [reasoning]\nAnswer: [answer]\nConfidence: [confidence]\n"}, {"page": 18, "text": "SUNDAY, the parking meter indicates that meter enforcement is active from \n9am to 5pm Monday through Friday. Therefore, the days when meter \nenforcement has their days off would be Saturday and Sunday. (CL: 70%).\nWeekends and holidays, the meter enforcement hours are stated as \"9am-\n6pm M-F\", which means Monday to Friday. Therefore, the days off would be \nSaturday, Sunday, and all public holidays (which typically fall on weekends). \n(CL: 85%)\nSaturday and Sunday, the parking meter in the image indicates that meter \nenforcement occurs from 8am to 6pm, Monday through Friday. This implies \nthat meter enforcement does not occur on weekends, specifically Saturday \nand Sunday. (CL: 90%)\n1. Initial Round\nQwenVL\nMiniCPM-o\nOvis\nSaturday and Sunday, the parking meter in the image clearly \nindicates that meter enforcement is active from 9am to 6pm, Monday \nthrough Friday. This means that meter enforcement does not occur \non weekends, specifically Saturday and Sunday. The OCR tool \nconfirms this information, as it reads \"METER ENFORCEMENT 9am–\n6pm M–F\" on the parking meter. Therefore, the days when meter \nenforcement has their days off are Saturday and Sunday.\n(CL: 90%, TA: 1/1) \nSaturday and Sunday, he parking meter indicates that enforcement is \nactive from 9am to 6pm, Monday through Friday. This means the days \nwhen there is no meter enforcement would be Saturday and Sunday.\n(CL: 95%, TA: 1/1)\nSaturday and Sunday, the OCR tool provides clear information about the \nparking meter\\'s operation hours, which are \"9am–6pm M–F\". This \ndirectly contradicts the agent solutions that suggest meter enforcement \noperates on weekends and holidays. The OCR output is definitive and \nshould be considered authoritative in this context. (CL: 100%, TA: 1/1)\n3. Final Round\nQwenVL\nMiniCPM-o\nOvis\nQuestion: When does meter enforcement have \ntheir days off?\nWeekends\nGold Answer\n{\n\"experts\": [\"ocr\"],\n\"inputs\": {\n\"ocr\": {\n\"disagreement\": \"Agents conflict on the text they see in the image \nregarding the meter enforcement hours.\",\n\"justification\": \"The OCR expert will read the text in the image and \nresolve the disagreement about the meter enforcement hours.\",\n\"arguments\": []\n}\n}\n}\n2. Disagreement Resolution\nQwen (Recruiter)\nOn the parking meter:\nTop section:\n- ”2”, \"pay meter\"\nBottom section:\n- \"METER ENFORCEMENT”, \"9am–6pm M–F\"\nOn the sign in the background:\nThe text is partially visible and appears to include: \"NO”, \"2 HOURS”, \"PARKING”, \n\"EXCEPT”, \"DISABLED”, \"PERMITS\"\nTool Outputs\nThe OCR tool output clearly shows that the parking meter operates from 9am \nto 6pm, Monday through Friday. This information is definitive and directly \naddresses the question about the days when meter enforcement is not active. \nThe OCR tool's output is authoritative and aligns with the agent solutions that \ncorrectly identify Saturday and Sunday as the days when meter enforcement \nis off. The other agent solutions also correctly interpret the meter's \ninformation, reinforcing the conclusion. The tool output confirms that meter \nenforcement does not occur on weekends, specifically Saturday and Sunday.\nAnswer: Saturday and Sunday\nConfidence: 100%\n4. Aggregation\nOvis 2 (Aggregator)\nFigure 5: Qualitative Example for DART. We have input question “When does meter enforcement have their days\noff?” with gold answer Weekends. Ovis is the only model to get it correct, with MiniCPM-o jumping to an improper\nconclusion and QwenVL being objectively incorrect. The recruiter identifies this disagreement on what text is said\non the meter and calls on the OCR tool to resolve the disagreement. The OCR tools correctly identifies the text as\nM-F 9am-6pm. As a result, the models and aggregators are able to get to the correct answer in subsequent steps.\n"}, {"page": 19, "text": "Left, the flag is positioned on top of the tower, with its narrow end pointing \ntowards the left side of the image. This indicates that the flag is blowing in \nthe wind from right to left, which is consistent with the direction of the flag's \nnarrow end. (CL: 90%).\nRight, the flag is flying on the left side of the building, which would be to the \nwest if we assume this image was taken in London. Given that it's a \nstandard way for flags to fly (with the blue part at the top), and assuming no \nwind direction changes during the day when photos are often taken, the \nnarrow end pointing right makes sense as it means the wind is blowing from \neast to west or north-south depending on what time of year the photo was \ntaken. (CL: 95%)\nDown, the flag appears to be at half-mast, which means it is positioned with \nits narrow end pointing downward. This is a common practice in many \ncountries to show respect or mourning. (CL: 80%)\n1. Initial Round\nQwenVL\nMiniCPM-o\nOvis\nLeft, the Spatial Tool indicates that the flag is waving in the wind \nfrom the right side of the image towards the left side. The Captioning \nTool confirms that the flag is flying at the top left of the building, \nwhich is consistent with the direction of the wind. The agent \nsolutions suggest that the narrow end of the flag is pointing left, \nright, or down. However, the Spatial Tool's analysis is more reliable in \nthis context, as it directly addresses the direction of the flag's \nmovement. Therefore, the narrow end of the flag is pointing to the \nleft. (CL: 90%, TA: 2/2)\nRight, to determine the direction of the narrow end of the flag, we need \nto analyze the image provided. The experts have given us valuable \ninformation about the location and orientation of the flag.\n1. Spatial Tool Analysis: According to the spatial tool analysis, the flag is \nwaving in the wind from right to left.\n2. Captioning Tool Analysis: The caption indicates that the flag is flying \nat the top left of a building (Westminster Abbey), which suggests it's \nblowing towards the bottom right if we consider the perspective shown in \nthe image.\nGiven these two pieces of evidence:\n- The flag appears to be moving from right to left based on its position \nrelative to the tower.\n- The flag's movement seems consistent with being blown by wind \ncoming from the west or north depending on the time of year when the \nphoto was taken.\nConsidering both analyses together, the most logical conclusion is that \nthe narrow end of the flag is pointing to the right side of the image \nbecause this aligns well with the observed wind direction indicated by the \nflag's motion (CL: 80%, TA: 1/2)\nLeft, I agree with Agent 1 who states that the flag is positioned on top of \nthe tower, with its narrow end pointing towards the left side of the image. \nThis interpretation matches the wind direction suggested by the flag's \npositioning as seen in the expert description. (CL: 95%, TA: 2/2)\n3. Final Round\nQwenVL\nMiniCPM-o\nOvis\nQuestion: What direction is the narrow end of\nthe flag pointing?\nLeft\nGold Answer\n{\n\"experts\": [\"spatial\", \"captioning\"],\n\"inputs\": {\n\"spatial\": {\n\"disagreement\": \"One agent says the flag points left, another says it \npoints right, and another says it points down.\",\n\"justification\": \"The spatial expert can determine the actual direction of \nthe flag.\",\n\"arguments\": [\"direction of flag\"]\n},\n\"captioning\": {\n\"disagreement\": \"Agents also disagree about whether the flag is at half-\nmast or flying normally.\",\n\"justification\": \"The captioning expert can give a broader scene \ndescription, clarifying whether the flag is raised fully or at half-mast.\",\n\"arguments\": [\"half-mast\", \"flag\"]\n}\n}\n}\n2. Disagreement Resolution\nQwen (Recruiter)\nSpatial Tool Output: The flag is waving in the wind, which is blowing from \nthe right side of the image towards the left side.\nCaptioning Tool Output: This image shows Westminster Abbey, the Houses \nof Parliament, and the clock tower known as the Elizabeth Tower in London. \nThe flag is flying at the top left of the building, indicating it is not a half-\nmast or a sign of mourning. The Westminster Bridge is visible in front of the \nbuilding, spanning across the River Thames. The sky is clear and blue.\nTool Outputs\nThe Spatial Tool Output confirms that the flag is waving in the wind from the \nright side of the image towards the left side. The Captioning Tool Output \nsupports this by indicating that the flag is flying at the top left of the building, \nwhich aligns with the direction of the wind. The Spatial Tool and Captioning \nTool both agree that the flag's narrow end is pointing to the left. The agent \nsolutions that agree with this analysis provide strong evidence that the narrow \nend of the flag is pointing to the left. The Spatial Tool's analysis is particularly \nreliable in this context, as it directly addresses the direction of the flag's \nmovement. Thus, the narrow end of the flag is pointing to the left.\nAnswer: Left\nConfidence: 95%\n4. Aggregation\nOvis 2 (Aggregator)\nFigure 6: Qualitative Example for DART. We have input question “What direction is the narrow end of the flag\npointing?” with gold answer Left. The models all initially disagree, each pointing out different directions (down,\nleft, right). The recruiter identifies this disagreement and calls on the spatial and captioning tools to resolve the\ndisagreement. With the intervention of these tools specifying the direction of the flag, the models refine their\nanswers to get closer to the gold answer, and the aggregator successfully picks the correct answer.\n"}]}