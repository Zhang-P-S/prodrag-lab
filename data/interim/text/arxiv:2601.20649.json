{"doc_id": "arxiv:2601.20649", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.20649.pdf", "meta": {"doc_id": "arxiv:2601.20649", "source": "arxiv", "arxiv_id": "2601.20649", "title": "P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering", "authors": ["Wenlin Zhong", "Chengyuan Liu", "Yiquan Wu", "Bovin Tan", "Changlong Sun", "Yi Wang", "Xiaozhong Liu", "Kun Kuang"], "published": "2026-01-28T14:35:20Z", "updated": "2026-01-28T14:35:20Z", "summary": "While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.20649v1", "url_pdf": "https://arxiv.org/pdf/2601.20649.pdf", "meta_path": "data/raw/arxiv/meta/2601.20649.json", "sha256": "8469c173bd495ee8ce631033d335e085cb71aeab66d5c435294d028a2206e698", "status": "ok", "fetched_at": "2026-02-18T02:20:13.686645+00:00"}, "pages": [{"page": 1, "text": "P2S: Probabilistic Process Supervision for General-Domain Reasoning Question\nAnswering\nWenlin Zhong1, Chengyuan Liu2, Yiquan Wu3*, Bovin Tan3, Changlong Sun3, Yi Wang4,\nXiaozhong Liu5, Kun Kuang2\n1School of Software Technology, Zhejiang Unirersity\n2College of Computer Science and Technology, Zhejiang Unirersity\n3Guanghua Law School, Zhejiang University\n4Chongqing Ant Consumer Finance Co,. Ltd , Ant Group\n5Worcester Polytechnic Institute, Worcester, USA\n{22451152, liucy1, wuyiquan, bovintan, 11921173, kunkuang}@zju.edu.cn, haonan.wy@myxiaojin.cn, xliu14@wpi.edu\nAbstract\nWhile reinforcement learning with verifiable rewards (RLVR)\nhas advanced LLM reasoning in structured domains like\nmathematics and programming, its application to general-\ndomain reasoning tasks remains challenging due to the ab-\nsence of verifiable reward signals. To this end, methods like\nReinforcement Learning with Reference Probability Reward\n(RLPR) have emerged, leveraging the probability of gener-\nating the final answer as a reward signal. However, these\noutcome-focused approaches neglect crucial step-by-step su-\npervision of the reasoning process itself. To address this\ngap, we introduce Probabilistic Process Supervision (P2S), a\nnovel self-supervision framework that provides fine-grained\nprocess rewards without requiring a separate reward model\nor human-annotated reasoning steps. During reinforcement\nlearning, P2S synthesizes and filters a high-quality reference\nreasoning chain (gold-CoT). The core of our method is to cal-\nculate a Path Faithfulness Reward (PFR) for each reasoning\nstep, which is derived from the conditional probability of gen-\nerating the gold-CoT’s suffix, given the model’s current rea-\nsoning prefix. Crucially, this PFR can be flexibly integrated\nwith any outcome-based reward, directly tackling the reward\nsparsity problem by providing dense guidance. Extensive ex-\nperiments on reading comprehension and medical Question\nAnswering benchmarks show that P2S significantly outper-\nforms strong baselines.\nIntroduction\nLarge-scale Reinforcement Learning with Verifiable Re-\nwards (RLVR) has emerged as a promising paradigm to ad-\nvance the reasoning capabilities of Large Language Models\n(LLMs) (Guo et al. 2025a; Wen et al. 2025; Xu et al. 2025b).\nThis approach has fueled a major leap forward, particularly\nin structured, verifiable domains such as mathematics and\nprogramming (Shao et al. 2024; Havrilla et al. 2024; Kumar\net al. 2024; Cao et al. 2024). Within this paradigm, LLMs\nare trained using verifiable rewards computed directly from\nthe model’s own final outcomes, such as matching ground\n*Corresponding author.\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Comparing reward mechanisms: P2S rewards the\nentire reasoning process.\ntruth answers, passing unit tests, or selecting the correct op-\ntion in multiple-choice questions (MCQ) (Schulman et al.\n2017; Setlur et al. 2024; Xie et al. 2025).\nWhile RLVR has excelled in specific domains, its suc-\ncess does not readily transfer to general-domain reasoning.\nThe free-form and stylistically diverse nature of answers\nin these tasks makes designing a direct, verifiable reward\nsignal a challenge. Conventional solutions are inadequate:\nmanually engineering reward functions is unscalable (Zeng\net al. 2025; Hu et al. 2025), and training a specialized LLM\nas a verifier (Ma et al. 2025) demands extensive data an-\nnotation, yields unsatisfactory reward quality, and compli-\ncates the training pipeline. A more promising direction, Re-\ninforcement Learning with Reference Probability Reward\n(RLPR) (Xu et al. 2025a; Yu et al. 2025b; Zhou et al. 2025),\nleverages the generation probability of the final answer as a\nreward. However, all these outcome-focused methods share\ncritical flaws: they neglect step-by-step process supervision,\nwhich can lead models to discover “shortcut” solutions via\nflawed logic and exacerbates reward sparsity in complex\nproblems.\nAs shown in Figure 1, we compare P2S with RLVR\narXiv:2601.20649v1  [cs.CL]  28 Jan 2026\n"}, {"page": 2, "text": "and RLPR in general-domain QA. In contrast to domain-\nspecific, sparse-reward verifiers (Figure 1, left) and purely\noutcome-focused RLPR (Figure 1, center), we argue that\nthe supervisory signal within the reasoning chain itself re-\nmains a valuable, untapped resource. Therefore, we aim to\ndesign a new reward mechanism that moves beyond sparse\noutcomes and learns directly from the step-by-step reason-\ning process, providing more effective and fine-grained su-\npervision for general-domain tasks.\nTo remedy this oversight, directly supervising the rea-\nsoning process is a natural next step. However, prevailing\napproaches introduce significant burdens. Training a sep-\narate reward model necessitates a large corpus of human-\nannotated or LLM-generated preference data\n(Lightman\net al. 2023), incurring substantial annotation and computa-\ntional costs. Alternatively, Monte Carlo search-based (Wang\net al. 2023) methods, which score each step via multiple roll-\nouts to a terminal state, face severe scalability challenges.\nThe required sample count grows prohibitively with the rea-\nsoning chain’s length, leading to immense computational\noverhead. This highlights a crucial need for a process su-\npervision method that is both low-cost and computation-\nally tractable.\nOur work addresses this challenge by introducing\nProbabilistic\nProcess\nSupervision\n(P2S),\na\nlow-cost,\nself-bootstrapping mechanism that provides fine-grained,\nprocess-level supervision by scoring and learning from the\nmodel’s own reasoning paths, eliminating the need for ex-\nternal reward models or human annotations. To achieve this,\nwe introduce two core techniques.\nFirst, we introduce a dynamic gold-CoT synthesis mecha-\nnism. For each problem, we prompt the model with the ques-\ntion and its ground truth answer to generate multiple candi-\ndate reasoning paths. These paths are then filtered based on\nboth their final answer’s correctness and their internal rea-\nsoning quality, creating a high-quality, dynamically updated\nset of reference chains that adapts to the model’s evolving\ncapabilities. Second, we introduce the Path Faithfulness Re-\nward (PFR), our core innovation for dense, step-level super-\nvision. PFR measures how “faithful” a generated reasoning\npath is to a reference gold-CoT. At each step of the gen-\nerated path, PFR calculates the conditional probability of\ncompleting the rest of the gold-CoT from that point. This\nstep-wise score quantifies whether the model is on a logi-\ncally sound trajectory. These scores are then aggregated into\na sample-level reward that penalizes early deviations and re-\nwards consistent logical progression, thereby directly pro-\nviding the dense, process-level signal needed to overcome\nreward sparsity. Finally, P2S operates within a flexible rein-\nforcement learning paradigm. Our process-based PFR can\nbe seamlessly combined with any outcome-based reward,\ncreating a hybrid signal. This joint optimization ensures the\nmodel learns not only from successful outcomes but also\nfrom the quality of its reasoning process, providing a dense\nand robust reward signal even when all samples in a batch\nare incorrect.\nExtensive experiments on diverse benchmarks, including\ngeneral-domain reading comprehension and medical QA,\ndemonstrate that P2S significantly outperforms strong base-\nlines. Our main contributions are summarized as follows:\n• We explore the challenging task of reinforcement learn-\ning for reasoning in general-domain QA, where tra-\nditional verifiable rewards are often unavailable. we\nidentify the limitations of current outcome-focused ap-\nproaches and propose a new direction centered on\nprocess-level supervision derived from the model’s own\ngeneration probabilities.\n• We introduce Probabilistic Process Supervision (P2S),\na novel self-supervision framework that generates fine-\ngrained, process-level rewards without costly external\nreward models or human annotations. At its core, P2S\nleverages two innovations: a dynamic Gold-CoT synthe-\nsis mechanism and our Path Faithfulness Reward (PFR).\n• We demonstrate through extensive experiments on di-\nverse benchmarks, including general-domain reading\ncomprehension and medical QA, that P2S consis-\ntently and significantly outperforms strong state-of-the-\nart baselines.\nRelated Work\nReinforcement Learning for Reasoning\nTo advance beyond simple prompting for Chain-of-Thought\n(CoT) reasoning (Kojima et al. 2022; Wei et al. 2022), re-\ncent paradigms directly train LLMs, notably via reinforce-\nment learning (RL) on reasoning traces (Shao et al. 2024;\nHe et al. 2025). A successful branch, RLVR, excels in struc-\ntured domains like math and code by using deterministic, bi-\nnary outcome rewards from verifiers (Guo et al. 2025a; Yu\net al. 2025a; Ye et al. 2025). However, this reliance on veri-\nfiers makes RLVR unsuitable for general-domain reasoning,\nwhere such clear verification is often impossible.\nReasoning in General Domains\nTo enable reinforcement learning in general reasoning do-\nmains without clear verifiers, research has focused on de-\nsigning reliable reward signals. One major direction is to\ntrain an external generative reward model to act as a judge\n(Mahan et al. 2024; Ma et al. 2025), which introduces the\noverhead of developing and maintaining an additional re-\nward model during RL training. A competing approach\navoids this by using the policy model’s internal feedback as a\nreward, leveraging signals such as self-certainty or the prob-\nability of the ground truth answer as a reward signal. (Xu\net al. 2025a; Yu et al. 2025b; Zhou et al. 2025).\nProcess Reward Supervision\nProcess supervision improves LLM reasoning consistency\nby rewarding intermediate steps. While training reward\nmodels on human-annotated steps (Li et al. 2024; Lightman\net al. 2023) is costly and unscalable, search-based alterna-\ntives like Monte Carlo search estimate step values via roll-\nouts (Wang et al. 2023; Guo et al. 2025b). However, these\nmethods incur prohibitive computational costs that scale\npoorly with reasoning length.\n"}, {"page": 3, "text": "Preliminaries\nWe first introduce the reasoning optimization with RL, upon\nwhich many works build to perform RLVR. Then, we intro-\nduce the emerging approach of RLPR (Xu et al. 2025a; Yu\net al. 2025b; Zhou et al. 2025).\nReasoning Optimization With RL\nIn order to enhance the reasoning ability of large models, we\nadopt Group Relative Policy Optimization (GRPO) (Shao\net al. 2024) following the recent advancements such as\nDeepSeek-R1 (Guo et al. 2025a). Given a question-answer\npair (q, a), a behavior policy πθold samples a group of G in-\ndividual responses {oi}G\ni=1. The GRPO objective updates\nmodel parameters θ as follows:\nJGRPO(θ) =E(q,a)∼D,{oi}G\ni=1∼πθold(·|q)\n\"\n1\nG\nG\nX\ni=1\n1\n|oi|\n|oi|\nX\nt=1\n\u001a\nmin\n\u0014 πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t)\nˆAi,t,\nclip\n\u0012 πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t), 1 −ϵ, 1 + ϵ\n\u0013\nˆAi,t\n\u0015\n−βDKL(πθ∥πref)\n\u001b#\n(1)\nThe key distinction of GRPO is its advantage estimation\nfor the t-th token in the i-th output, ˆAi,t. This involves a\nstructured comparison across a group of G outputs {oi}G\ni=1\nsampled for the same prompt. Given corresponding rewards\n{Ri}G\ni=1, the advantage is estimated as:\nˆAi,t = ri −mean({Ri}G\ni=1)\nstd({Ri}G\ni=1)\n(2)\nIn the context of RLVR, the reward ri is typically a verifi-\nable signal, such as 1 if the final answer is correct and 0 oth-\nerwise. This group-normalized formulation steers the policy\nto assign higher probabilities to trajectories that outperform\ntheir peers within the same generation batch.\nReinforcement Learning with Reference\nProbability Reward (RLPR)\nTo address the scalability limitations of RLVR, a recent\ntrend in general-domain reasoning is to adopt reinforcement\nlearning paradigms that use probability-based reward sig-\nnals. It leverages the LLM’s own knowledge.\nIn a typical RLPR setup, for a given input query q, the\npolicy model πθ first generates a full response o, which in-\ncludes both a reasoning path z and a final answer y. The\nreward is not based on the correctness of the generated an-\nswer y. Instead, it is computed from the model’s conditional\nprobability of generating the tokens of the ground truth an-\nswer y∗, given the generated reasoning path z. This can be\nformally expressed as the aggregated log-probability:\nrRLPR =\n|y∗|\nX\nt=1\nlog πθ(y∗\nt |q, z, y∗\n<t)\n(3)\nwhere y∗\nt is the t-th token of the ground truth answer.\nMethodology\nIn this section, we begin by formally defining the problem,\nthen outline the overall architecture of Probabilistic Pro-\ncess Supervision (P2S) framework, and finally, detail its core\ncomponents.\nProblem Definition\nWe consider the task of learning a reasoning policy for\ngeneral-domain question answering. Formally, we are given\na dataset D = {(qi, y∗\ni )}N\ni=1, where qi is a question or\nprompt, and y∗\ni is its corresponding ground-truth final an-\nswer. A key characteristic of these tasks is their diversity,\nspanning multiple domains and featuring answers that are\nfree-form text of varying lengths and styles.\nOur goal is to learn a policy πθ that, given a prompt\nq, generates a logically sound reasoning path z\n=\n(z1, z2, . . . , zT, ) which culminates in a final answer y. This\ndiversity in the target answers y∗makes exact string match-\ning an unsuitable objective. Therefore, our ultimate goal is\nto maximize the semantic similarity between the generated\nanswer y and the ground-truth y∗.\nOverall Architecture\nAs illustrated in Figure 2, our Probabilistic Process Supervi-\nsion (P2S) framework operates as a self-improving loop that\nprovides dense, process-level rewards for policy optimiza-\ntion. Firstly, within each iteration of the GRPO, a dynamic\nGold-CoT synthesis mechanism leverages the current pol-\nicy πθ, guided by a ground truth answer, to generate and\nfilter multiple candidate reasoning paths. This yields a high-\nquality set of Gold-CoTs specifically tailored for the current\nlearning state. Concurrently, for each generated reasoning\ntrace in the batch, our Path Faithfulness Reward (PFR) is\ncomputed by aligning it against a reference Gold-CoT and\ncalculating step-wise conditional probabilities. These step-\nwise rewards are then weighted and aggregated into a single,\nsample-level process reward and used to update the policy\nπθ, which provides a nuanced score for the entire reasoning\npath.\nDynamic Gold-CoT Synthesis and Filtering\nTo ensure a high-fidelity and adaptive supervision signal,\nP2S dynamically synthesizes and filters reference reasoning\npaths (Gold-CoTs) in each training iteration. This process\ninvolves two main steps: Candidate Synthesis and Quality-\nBased Filtering.\nCandidate Synthesis.\nTo encourage the model to explore\npaths that lead to the correct answer, we prompt the policy\nmodel πθ with both the query q and the ground truth answer\ny∗to generate a diverse set of K candidate reasoning paths,\n{ok}K\nk=1 during this synthesis stage. This guided generation\nhelps to efficiently sample trajectories within the vicinity of\nthe correct solution space.\n{ok}K\nk=1 ∼πθ(·|q, y∗)\n(4)\n"}, {"page": 4, "text": "Figure 2: An overview of our Probabilistic Process Supervision (P2S) framework. (1) Gold-CoT Synthesis (Top): A dynamic\nreference path (Gold-CoT) is created by generating and filtering the policy model’s own reasoning outputs. (2) PFR Calculation\n(Bottom): For each new trace, a step-wise Path Faithfulness Reward (PFR) is computed by aligning it against the Gold-CoT.\n(3) Reward Shaping & Aggregation: The step-wise rewards are shaped using a sigmoid function to assign progressively higher\nweights to later reasoning steps. These weighted scores are then summed to produce the final, sample-level Path Faithfulness\nReward (PFR) used for policy optimization.\nQuality-Based Filtering.\nSimply generating paths guided\nby the ground truth answer y∗is insufficient, as they may\nstill be logically flawed, trivial, or fail to reach the correct\nfinal answer. Therefore, a filtering stage is crucial to isolate\nonly the highest-quality candidates.\nFirst, we discard any candidate ok that does not\nadhere\nto\na\nrequired\nstructural\nformat.\nFollowing\nthe standard of (Guo et al. 2025a), this format is\n<think>Reasoning</think><answer>Answer</answer>.\nThis preliminary step ensures that the reasoning path zk\nand the final answer yk can be reliably parsed. Let the set of\nformat-correct candidates be Cformatted.\nThen, for each candidate in Cformatted, we compute a qual-\nity score Sk as the conditional log-probability of generating\nthe ground truth answer y∗given the candidate’s reasoning\nzk:\nSk =\n|y∗|\nX\nt=1\nlog πθ(y∗\nt |q, zk, y∗\n<t)\n(5)\nFor each problem q, the definitive gold-CoT o∗is then se-\nlected by finding the candidate that maximizes this score:\no∗= arg max\nok∈Cformatted\nSk\nThe resulting set of candidates Cgold forms a dynamic and\nhigh-quality benchmark for the current training step. This\nself-improving mechanism creates a virtuous cycle: as the\npolicy model πθ improves, so does the quality of its self-\ngenerated supervision.\nPath Faithfulness Reward (PFR)\nThe core of our P2S framework is the Path Faithfulness Re-\nward (PFR), which provides a dense, step-level reward to\nguide the model’s reasoning process. The central intuition is\nthat a high-quality reasoning prefix should significantly in-\ncrease the likelihood of generating a subsequent, logically\nsound reasoning segment from a verified gold-CoT.\nWe first segment the generated chain z into a sequence\nof up to MAX STEP NUM equally-sized steps, denoted\nas (z1, z2, . . . , zm). This yields a sequence of prefixes\np1, p2, . . . , pm, where pi = z[: i] is the concatenation of the\nfirst i steps. Similarly, we define a suffix of the gold-CoT o∗\nstarting at step t as st = o∗[t :].\nFor each intermediate step zi (where i < m), we com-\npute its reward by evaluating the quality of the full prefix\npi = (z1, . . . , zi) that it concludes. This prefix-based evalu-\nation not only assesses zi within its full contextual history to\nensure logical coherence, but also allows the prefix’s score\nto be directly attributed to zi as the final, decisive step guid-\ning the path forward.\nA naive approach would be to measure the conditional\nprobability of generating a gold-CoT suffix given the prefix\npi. However, a high probability might arise simply because\nthe suffix itself is a common or high-probability sequence,\nregardless of the prefix’s quality. Following the work of (Xu\net al. 2025a), to isolate the actual contribution of the prefix,\nwe normalize the raw conditional probability by subtract-\ning a baseline. This baseline is defined as the probability of\ngenerating the same suffix given the initial question q and\na masked version of the prefix pi, denoted pmasked. The re-\n"}, {"page": 5, "text": "sulting score can thus be interpreted as the information gain\nprovided by the final step zi within the context of its preced-\ning steps.\nThe reward for step zi, denoted rstep(zi), is therefore de-\nfined by evaluating its corresponding prefix pi and finding\nthe maximum log-probability gain over all valid suffixes\nwithin the definitive gold-CoT o∗:\nrstep(zi) := max\nt\n(log πθ(st|q, pi) −log πθ(st|q, pmasked))\n(6)\nFor the final step zm, however, the reward is treated dif-\nferently. This step completes the entire reasoning path z, and\nits quality is best assessed by its ability to produce the cor-\nrect final answer. For this terminal step, the objective shifts\nfrom measuring information gain to ensuring absolute cor-\nrectness. Therefore, its reward is defined directly by the con-\nditional log-probability of generating the ground-truth an-\nswer y∗, given the full reasoning path z:\nrstep(zm) := log πθ(y∗|q, z)\n(7)\nTime Complexity Analysis.\nThe computational overhead\nof P2S for a single problem instance is dominated by the\nnumber of forward passes (Cfwd) through the policy model\nπθ. The process involves two main cost components per it-\neration. First, the Gold-CoT synthesis requires sampling and\nfiltering K candidate paths, incurring a cost proportional to\nK · Cfwd. Second, the PFR calculation for a reasoning path\nwith m steps involves a search over suffixes, resulting in\na complexity of approximately O(m2 · Cfwd). Since m is\ncapped by a constant MAX STEP NUM, this complexity\nis well-controlled. Therefore, the total time complexity is\nO((K +m2)·Cfwd). This is a manageable trade-off, and the\ncomputation is highly parallelizable.\nReward Shaping with Step-wise Weighting\nA simple averaging of step-wise rewards is suboptimal be-\ncause it treats all steps equally. Instead, we adopt a strategy\nthat allows the model a “grace period” for initial exploration,\nsuch as analyzing the problem or self-correcting from early\nmissteps. To implement this, we introduce a weight shaping\nmechanism that assigns progressively higher importance to\nlater reasoning steps, thereby focusing supervision on the\nmore converged and critical stages of the reasoning process.\nTo assign greater importance to later reasoning steps,\nwe compute the final sample-level reward, RPFR-w, as a\nweighted average of the step-wise rewards rstep(zi). The\nweight for each step, wi, is generated using a monotonically\nincreasing standard sigmoid σ(i), ensuring that later steps\ncontribute more significantly to the final reward. The formu-\nlation is as follows:\nRPFR-w =\nPm\ni=1 wi · rstep(zi)\nPm\ni=1 wi\n(8)\nHierarchical Reward Integration\nA key advantage of our P2S framework is its flexibility,\nas the Path Faithfulness Reward (RPFR-w) can function ei-\nther as a standalone process signal or be integrated with\nother rewards. We present a powerful hierarchical paradigm\nthat combines P2S with an outcome-based reward, assign-\ning scores with a clear priority. First, malformed trajectories\nare heavily penalized. If any trajectory yields a correct an-\nswer, we exclusively use this outcome signal to rapidly am-\nplify the advantage of successful paths. Only when all valid\npaths fail does our dense PFR serve as a fallback, ensuring\na fine-grained learning signal is always available to mitigate\nreward sparsity.\nThis hierarchical logic can be formalized concisely. Let\nF(i) ∈{0, 1} be an indicator function where F(i) = 1 if the\nformat of trajectory i is correct. Let SG = maxj∈G Routcome,j\nbe a binary variable indicating whether any trajectory in the\ngroup G was successful. The final reward Ri for trajectory i\nis then:\nRi =\n\n\n\n−1\nif F(i) = 0\nRoutcome,i\nif F(i) = 1 and SG = 1\nRPFR-w,i\nif F(i) = 1 and SG = 0\n(9)\nCold-Start.\nTo ensure training stability, we adopt a cur-\nriculum warm-up strategy (Liu et al. 2025). For the initial\nSwarmup training steps, the model learns the basic task struc-\nture using only format-based rewards, with our PFR compo-\nnent deactivated. Subsequently, the full P2S reward mecha-\nnism is enabled to refine the logical quality of the reasoning\nprocess.\nExperiments\nExperimental Setup\nDatasets\nWe focus on reasoning tasks that lack strict struc-\ntural verifiers due to their open-ended and stylistically di-\nverse answers, but still possess objectively correct outcomes.\nAccordingly, we train and evaluate our method on two\ndatasets selected to reflect this challenge. (1) DROP (Dua\net al. 2019): A challenging reading comprehension bench-\nmark that requires discrete reasoning over open-domain\nWikipedia text, such as arithmetic and sorting. (2) Medical\nQA (Chen et al. 2024): An open-ended medical question-\nanswering dataset derived from challenging medical exams.\nFor both datasets, we process into a question-answering for-\nmat and filter to include questions under 2000 and answers\nbetween 1-50 characters, creating a 10k/2k random train/test\nsplit for each.\nEvaluation Metrics\nOur evaluation employs two com-\nplementary metrics for final answers. For lexical similar-\nity, we use ROUGE-1 F1 to measure overlap with the\nground truth answers. To assess semantic correctness, we\nuse LLM-as-a-Judge (Gu et al. 2024) to judge semantic\nequivalence, including: Claude 4 Sonnet (ACCClaude), GPT-\n4o (ACCGPT), and a trained 1.5B general-domain Verifier\n(ACCVerifier) (Ma et al. 2025). Finally, we report the mean\nof these three accuracy scores, ACCAvg, as a single, robust\nmeasure of correctness.\nBaselines\nWe compare our method against several base-\nlines, all built upon the Qwen2.5-1.5B-Instruct model. Full\nimplementation details for all experiments are provided in\n"}, {"page": 6, "text": "Model\nDrop\nMedicalQA\nROUGE\nACCClaude\nACCGPT\nACCVerifier\nACCAvg\nROUGE\nACCClaude\nACCGPT\nACCVerifier\nACCAvg\nQwen2.5-1.5B-Instruct\n42.23\n51.67\n50.75\n49.15\n50.52\n40.30\n19.20\n19.20\n27.00\n21.80\nPrompt-Based\nCOT\n41.97\n45.33\n49.00\n48.85\n47.73\n40.09\n20.40\n21.60\n26.60\n22.87\nSelf-Consistency\n45.51\n51.17\n52.67\n52.35\n52.06\n38.76\n14.10\n17.13\n23.75\n18.33\nFine-tuning and RL methods\nFull-Sft\n71.44\n66.00\n64.50\n63.42\n64.64\n50.92\n20.80\n20.04\n22.65\n21.28\nGRPO\n70.89\n60.00\n62.25\n62.12\n61.46\n46.21\n17.67\n21.00\n25.50\n21.39\nGRPO+SFT-loss\n66.18\n59.50\n63.00\n58.90\n60.47\n45.79\n21.00\n20.40\n24.15\n21.85\nSFT+GRPO\n75.28\n66.50\n70.14\n68.55\n68.40\n50.57\n23.33\n20.00\n23.80\n22.38\nGeneral Reasoner\n73.03\n67.89\n65.32\n66.30\n66.50\n51.57\n19.18\n17.20\n27.45\n21.28\nRLPR methods\nDRO\n74.85\n66.28\n67.17\n66.65\n66.70\n50.52\n20.11\n19.20\n23.50\n20.94\nRLPR\n75.48\n67.18\n68.04\n67.57\n67.60\n51.14\n21.16\n20.75\n26.92\n22.94\nVeriFree\n71.98\n64.42\n62.17\n63.40\n63.33\n51.46\n21.98\n21.68\n22.85\n22.17\nP2S\n76.78\n69.11\n72.14\n70.85\n70.70\n52.90\n24.33\n22.67\n25.85\n24.28\nTable 1: Performance comparison of various Reasoning methods on general-domian QA task. Bold and underline indicate the\nbest and second-best results, respectively.\nAppendix A. And our baselines are grouped into three cat-\negories. (1) Prompt-based methods that require no fine-\ntuning: Chain-of-Thought (CoT) (Wei et al. 2022) and Self-\nConsistency (Wang et al. 2022). (2) Fine-tuning and RL\nmethods, including full supervised fine-tuning (Full-SFT)\nand several GRPO (Shao et al. 2024) variants. Standalone\nGRPO, the two-stage SFT+GRPO, and GRPO+SFT-loss\n(which integrates off-policy knowledge via an auxiliary\nSFT loss) all use ROUGE-1 F1 as their outcome-based re-\nward. In contrast, General Reasoner (Ma et al. 2025) also\nemploys GRPO but replaces this reward with judgments\nfrom a trained 1.5B LLM verifier that assesses semantic\nequivalence. (3) RLPR-based methods, which leverage the\nmodel’s own probabilities for reward, including DRO (Xu\net al. 2025a), the original RLPR (Yu et al. 2025b), and Ver-\niFree (Zhou et al. 2025). To ensure a fair comparison and\nmitigate reward collapse during RL phases, P2S along with\nthe General Reasoner and RLPR-based baselines, adheres to\na same cold-start Supervised Fine-Tuning paradigm before\nRL training (Guo et al. 2025a).\nMain Results\nMain Results in Table 1 show our method, P2S, outperforms\nall baselines on both the DROP and MedicalQA datasets.\nWe can draw several key conclusions from the results:\n1) P2S significantly improves general-domain reasoning\nperformance. On DROP, it reaches an ACCAvg of 70.70,\nexceeding the strongest fine-tuned baseline (SFT+GRPO at\n68.40) by 2.3 points. This leadership extends to MedicalQA,\nwhere P2S achieves an ACCAvg of 24.28, outperforming the\nnext best method (RLPR at 22.94) by over 1.3 points.\n2) Our core hypothesis—that dense process supervision\nis critical—is validated by these results. P2S’s superior-\nity is particularly clear against RLPR-based methods (e.g.,\nRLPR, VeriFree). On DROP, for instance, P2S surpasses\nthe strongest RLPR-based method (RLPR) by 1.3 points in\nROUGE (76.78 vs. 75.48) and by over 3 points in ACCAvg\n(70.70 vs. 67.60). This dual improvement proves that our\nprocess-focused supervision not only mitigates the reward\nsparsity of outcome-only approaches but also guides the\nmodel to produce answers superior in both form and sub-\nstance.\n3) P2S outperforms representative fine-tuning and RL\nparadigms, highlighting the efficacy of verifier-free rewards.\nOn DROP, P2S surpasses all GRPO and RLPR variants.\nMore notably, it outperforms General Reasoner by a signif-\nicant margin of over 4 points in ACCAvg (70.70 vs. 66.50),\nwhich uses a 1.5B LLM verifier for its reward signal. This\nis a crucial finding: our internal, process-based rewards are\nmore effective than guidance from a costly external veri-\nfier. Furthermore, the reliability of such verifiers is ques-\ntionable, as evidenced on MedicalQA. General Reasoner’s\nACCVerifier score (27.45) is substantially inflated compared\nto judgments from large-scale models like Claude (19.18)\nand GPT (17.20). This discrepancy underscores the robust-\nness and efficiency of our verifier-free P2S framework, es-\npecially in new domains.\nAblation Study\nOur ablation study on DROP (Table 2) validates the con-\ntribution of each key component in the P2S framework by\nsystematically removing them from the full model.\nGold-CoT Filtering (GCF) is Crucial. Replacing our\nGold-CoT filtering with random path selection (w/o GCF)\ncauses the most substantial performance drop, reducing\nACCAvg by 4.5 points. This confirms that high-quality, faith-\nful reasoning paths are a critical foundation for effective\nprocess supervision. Path Faithfulness Reward (PFR) is the\n"}, {"page": 7, "text": "core contribution. Removing our core PFR component (w/o\nPFR) results in a 2.3-point decrease in ACCAvg. This directly\nvalidates the effectiveness of our proposed PFR as a criti-\ncal component for process supervision. Advanced Reward\nMechanisms are Effective. We also validated our reward de-\nsign choices. Replacing sigmoid-based weight shaping with\nsimple averaging (w/o RS) drops ACCAvg by 2.7 points, con-\nfirming the benefit of prioritizing later reasoning steps. Sim-\nilarly, a naive reward summation (w/o HRI) is less effective\nthan our hierarchical integration, proving the advantage of\nour dynamic fusion strategy.\nModel\nROUGE ACCClaude ACCGPT-4o ACCVerifier ACCAvg\nP2S (Full)\n76.78\n69.11\n72.14\n70.85\n70.70\nw/o GCF\n71.46\n64.21\n67.22\n67.21\n66.21\nw/o PFR\n75.28\n66.50\n70.14\n68.55\n68.40\nw/o RS\n74.91\n64.10\n69.88\n69.94\n67.97\nw/o HRI\n76.70\n68.20\n71.27\n70.20\n69.89\nTable 2: Ablation study of P2S components on DROP. P2S\n(Full) is our complete model; w/o GCF removes Gold-CoT\nfiltering; w/o PFR removes our core Path Faithfulness Re-\nward; w/o RS removes sigmoid-based reward shaping; and\nw/o HRI removes hierarchical reward integration.\nEffect of Model Scale\nTo investigate its scalability, we evaluate P2S against the\nuntuned base model and the strong SFT+GRPO baseline\nat 1.5B and 3B scales on DROP (Table 3). Results high-\nlight two key findings. First, P2S shows remarkable effi-\nciency: our 1.5B model (70.70 ACCAvg) significantly out-\nperforms the much larger 3B base model (62.77), suggesting\nour process supervision unlocks capabilities beyond simply\nscaling parameters. Second, P2S’s consistent superiority at\nboth scales confirms it is a robust and effective enhancement\nacross different model sizes.\nModel Scale\nBase\nSFT+GRPO\nP2S (Ours)\nROUGE ACCAvg ROUGE ACCAvg ROUGE ACCAvg\n1.5B\n42.23\n50.52\n75.28\n68.40\n76.78\n70.70\n3B\n47.96\n62.77\n81.16\n74.41\n82.08\n77.30\nTable 3: Performance on DROP across model scales.\nAnalysis on Verifiable Subsets\nWe study the effectiveness of P2S on domains with read-\nily available verifiers. To this end, we created two verifiable\nsubsets—DROP-verifiable (5k) and MedicalQA-verifiable\n(2.35k)—by filtering for instances with single-word an-\nswers. On these, we compare P2S against two outcome-only\nbaselines: the probabilistic RLPR and the rule-based RLVR.\nAs\nshown\nin\nFigure\n3,\nP2S\nconsistently\noutper-\nforms both baselines on both subsets, across both ex-\nact match (ACCexact) and verifier-based average accuracy\n(ACCAvg).This crucial finding proves that our P2S provides\nFigure 3: P2S outperforms in verifiable tasks\na fundamentally superior learning signal, extending its ben-\nefits far beyond merely overcoming reward sparsity, even in\nideal settings for outcome-only methods.\nCase Study\nFigure 4 provides a case study to illustrate how our Path\nFaithfulness Reward (PFR) works. Given a Gold-CoT, we\nanalyze two incorrect reasoning paths, z1 and z2.\nIn path z1, the model makes an early error by analyzing\nthe wrong dates (highlighted in light blue), leading to a low\nreward score for that step (e.g., 0.12). The error propagates,\nresulting in even lower scores for subsequent steps (0.09). In\ncontrast, path z2 correctly identifies the initial entities (high-\nlighted in orange), and our PFR mechanism appropriately\nassigns a high reward to this correct step (0.87).\nAlthough both paths ultimately fail to produce the correct\nfinal answer, our PFR is capable of discerning valuable, cor-\nrect sub-steps within an overall incorrect reasoning process.\nThis fine-grained reward allows our framework to reinforce\npartially correct reasoning even within failed attempts.\nFigure 4: Case Study\nConclusion\nIn this paper, we introduced Probabilistic Process Supervi-\nsion (P2S), a novel, low-cost self-supervision framework.\nAt its core, P2S leverages two key innovations: a dynamic\nmechanism for synthesizing high-quality Gold-CoTs and the\nPath Faithfulness Reward (PFR), which provides a dense,\nstep-by-step signal by measuring the faithfulness of a gen-\nerated reasoning path to a reference. Our extensive ex-\nperiments demonstrated that P2S significantly outperforms\nstrong baselines on challenging reasoning benchmarks. This\nwork proves that it is both feasible and effective to learn\ndirectly from the reasoning process itself without external\nreward models or human annotation.\n"}, {"page": 8, "text": "Acknowledgments\nThis work was supported in part by the “Pioneer” and “Lead-\ning Goose” R&D Program of Zhejiang (2025C02037),\nthe\nNational\nNatural\nScience\nFoundation\nof\nChina\n(62376243, 62406287), Key R&D Program of Hangzhou\n(2025SZDA0254), and Ant Group, Chongqing Ant Con-\nsumer Finance Co. All opinions in this paper are those of\nthe authors and do not necessarily reflect the views of the\nfunding agencies.\nReferences\nCao, Y.; Zhao, H.; Cheng, Y.; Shu, T.; Chen, Y.; Liu, G.;\nLiang, G.; Zhao, J.; Yan, J.; and Li, Y. 2024. Survey on large\nlanguage model-enhanced reinforcement learning: Concept,\ntaxonomy, and methods. IEEE Transactions on Neural Net-\nworks and Learning Systems.\nChen, J.; Cai, Z.; Ji, K.; Wang, X.; Liu, W.; Wang, R.; Hou,\nJ.; and Wang, B. 2024. HuatuoGPT-o1, Towards Medical\nComplex Reasoning with LLMs. arXiv:2412.18925.\nDua, D.; Wang, Y.; Dasigi, P.; Stanovsky, G.; Singh, S.;\nand Gardner, M. 2019. DROP: A Reading Comprehension\nBenchmark Requiring Discrete Reasoning Over Paragraphs.\nIn Proc. of NAACL.\nGu, J.; Jiang, X.; Shi, Z.; Tan, H.; Zhai, X.; Xu, C.; Li, W.;\nShen, Y.; Ma, S.; Liu, H.; et al. 2024. A survey on llm-as-a-\njudge. arXiv preprint arXiv:2411.15594.\nGuo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.;\nZhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025a. Deepseek-r1:\nIncentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948.\nGuo, Y.; Xu, L.; Liu, J.; Ye, D.; and Qiu, S. 2025b. Seg-\nment policy optimization: Effective segment-level credit as-\nsignment in rl for large language models.\narXiv preprint\narXiv:2505.23564.\nHavrilla, A.; Du, Y.; Raparthy, S. C.; Nalmpantis, C.;\nDwivedi-Yu, J.; Zhuravinskyi, M.; Hambro, E.; Sukhbaatar,\nS.; and Raileanu, R. 2024. Teaching large language mod-\nels to reason with reinforcement learning. arXiv preprint\narXiv:2403.04642.\nHe, J.; Liu, J.; Liu, C. Y.; Yan, R.; Wang, C.; Cheng, P.;\nZhang, X.; Zhang, F.; Xu, J.; Shen, W.; et al. 2025. Sky-\nwork open reasoner 1 technical report.\narXiv preprint\narXiv:2505.22312.\nHu, J.; Zhang, Y.; Han, Q.; Jiang, D.; Zhang, X.; and Shum,\nH.-Y. 2025. Open-reasoner-zero: An open source approach\nto scaling up reinforcement learning on the base model.\narXiv preprint arXiv:2503.24290.\nHugging Face. 2025. Open R1: A fully open reproduction\nof DeepSeek-R1.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa,\nY. 2022.\nLarge language models are zero-shot reason-\ners. Advances in neural information processing systems, 35:\n22199–22213.\nKumar, A.; Zhuang, V.; Agarwal, R.; Su, Y.; Co-Reyes, J. D.;\nSingh, A.; Baumli, K.; Iqbal, S.; Bishop, C.; Roelofs, R.;\net al. 2024.\nTraining language models to self-correct via\nreinforcement learning. arXiv preprint arXiv:2409.12917.\nLi, J.; Liang, X.; Zhang, J.; Yang, Y.; Feng, C.; and Gao,\nY. 2024.\nPSPO*: An Effective Process-supervised Pol-\nicy Optimization for Reasoning Alignment. arXiv preprint\narXiv:2411.11681.\nLightman, H.; Kosaraju, V.; Burda, Y.; Edwards, H.; Baker,\nB.; Lee, T.; Leike, J.; Schulman, J.; Sutskever, I.; and Cobbe,\nK. 2023. Let’s verify step by step. In The Twelfth Interna-\ntional Conference on Learning Representations.\nLiu, Z.; Gong, C.; Fu, X.; Liu, Y.; Chen, R.; Hu, S.; Zhang,\nS.; Liu, R.; Zhang, Q.; and Tu, D. 2025.\nGHPO: Adap-\ntive Guidance for Stable and Efficient LLM Reinforcement\nLearning. arXiv preprint arXiv:2507.10628.\nMa, X.; Liu, Q.; Jiang, D.; Zhang, G.; Ma, Z.; and Chen, W.\n2025. General-reasoner: Advancing llm reasoning across all\ndomains. arXiv preprint arXiv:2505.14652.\nMahan, D.; Van Phung, D.; Rafailov, R.; Blagden, C.;\nLile, N.; Castricato, L.; Fr¨anken, J.-P.; Finn, C.; and Al-\nbalak, A. 2024. Generative reward models. arXiv preprint\narXiv:2410.12832.\nQwen; :; Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.;\nYu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; Lin, H.; Yang,\nJ.; Tu, J.; Zhang, J.; Yang, J.; Yang, J.; Zhou, J.; Lin, J.;\nDang, K.; Lu, K.; Bao, K.; Yang, K.; Yu, L.; Li, M.; Xue,\nM.; Zhang, P.; Zhu, Q.; Men, R.; Lin, R.; Li, T.; Tang, T.;\nXia, T.; Ren, X.; Ren, X.; Fan, Y.; Su, Y.; Zhang, Y.; Wan,\nY.; Liu, Y.; Cui, Z.; Zhang, Z.; and Qiu, Z. 2025. Qwen2.5\nTechnical Report. arXiv:2412.15115.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347.\nSetlur, A.; Nagpal, C.; Fisch, A.; Geng, X.; Eisenstein, J.;\nAgarwal, R.; Agarwal, A.; Berant, J.; and Kumar, A. 2024.\nRewarding progress: Scaling automated process verifiers for\nllm reasoning. arXiv preprint arXiv:2410.08146.\nShao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang,\nH.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024. Deepseekmath:\nPushing the limits of mathematical reasoning in open lan-\nguage models. arXiv preprint arXiv:2402.03300.\nvon Werra, L.; Belkada, Y.; Tunstall, L.; Beeching, E.;\nThrush, T.; Lambert, N.; Huang, S.; Rasul, K.; and Gal-\nlou´edec, Q. 2020. TRL: Transformer Reinforcement Learn-\ning. https://github.com/huggingface/trl.\nWang, P.; Li, L.; Shao, Z.; Xu, R.; Dai, D.; Li, Y.; Chen, D.;\nWu, Y.; and Sui, Z. 2023. Math-shepherd: Verify and rein-\nforce llms step-by-step without human annotations. arXiv\npreprint arXiv:2312.08935.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,\nS.; Chowdhery, A.; and Zhou, D. 2022.\nSelf-consistency\nimproves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V.; Zhou, D.; et al. 2022.\nChain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in neural information processing systems, 35:\n24824–24837.\n"}, {"page": 9, "text": "Wen, L.; Cai, Y.; Xiao, F.; He, X.; An, Q.; Duan, Z.; Du, Y.;\nLiu, J.; Tang, L.; Lv, X.; et al. 2025. Light-r1: Curriculum\nsft, dpo and rl for long cot from scratch and beyond. arXiv\npreprint arXiv:2503.10460.\nXie, T.; Gao, Z.; Ren, Q.; Luo, H.; Hong, Y.; Dai, B.; Zhou,\nJ.; Qiu, K.; Wu, Z.; and Luo, C. 2025. Logic-rl: Unleashing\nllm reasoning with rule-based reinforcement learning. arXiv\npreprint arXiv:2502.14768.\nXu, Y.; Chakraborty, T.; Sharma, S.; Nunes, L.; Kıcıman,\nE.; Lu, S.; and Chandra, R. 2025a. Direct reasoning opti-\nmization: Llms can reward and refine their own reasoning\nfor open-ended tasks. arXiv preprint arXiv:2506.13351.\nXu, Z.; Yue, X.; Wang, Z.; Liu, Q.; Zhao, X.; Zhang,\nJ.; Zeng, W.; Xing, W.; Kong, D.; Lin, C.; et al. 2025b.\nCopyright Protection for Large Language Models: A Sur-\nvey of Methods, Challenges, and Trends.\narXiv preprint\narXiv:2508.11548.\nYe, Y.; Huang, Z.; Xiao, Y.; Chern, E.; Xia, S.; and Liu, P.\n2025. Limo: Less is more for reasoning. arXiv preprint\narXiv:2502.03387.\nYu, Q.; Zhang, Z.; Zhu, R.; Yuan, Y.; Zuo, X.; Yue, Y.; Dai,\nW.; Fan, T.; Liu, G.; Liu, L.; et al. 2025a. Dapo: An open-\nsource llm reinforcement learning system at scale.\narXiv\npreprint arXiv:2503.14476.\nYu, T.; Ji, B.; Wang, S.; Yao, S.; Wang, Z.; Cui, G.; Yuan, L.;\nDing, N.; Yao, Y.; Liu, Z.; et al. 2025b. RLPR: Extrapolating\nRLVR to General Domains without Verifiers. arXiv preprint\narXiv:2506.18254.\nZeng, W.; Huang, Y.; Liu, Q.; Liu, W.; He, K.; Ma, Z.; and\nHe, J. 2025. Simplerl-zoo: Investigating and taming zero re-\ninforcement learning for open base models in the wild. arXiv\npreprint arXiv:2503.18892.\nZhou, X.; Liu, Z.; Sims, A.; Wang, H.; Pang, T.; Li,\nC.; Wang, L.; Lin, M.; and Du, C. 2025.\nReinforc-\ning General Reasoning without Verifiers.\narXiv preprint\narXiv:2505.21493.\n"}, {"page": 10, "text": "A\nExperimental Details\nOur experiments, including P2S and all baselines, are con-\nducted on Qwen2.5-1.5B-Instruct(Qwen et al. 2025) if not\nadditionally specified. We conducted our experiments us-\ning the openr1 (Hugging Face 2025) codebase and the TRL\n(von Werra et al. 2020) framework. We are thankful for these\nopen-source repositories. For training efficiency, we utilize\nbfloat16 precision and enable FlashAttention-2. P2S and\nall baselines training was performed on 2 powerful H800\nGPUs, each equipped with 80GB of memory and high mem-\nory bandwidth. The prompt template is shown in figure 5.\nFigure 5: We adopt the training and inference prompt of R1\n(Guo et al. 2025a)\nA.1\nImplementation Details and\nHyperparameters\nKey hyperparameters for our main experiment are as\nfollows. We set the learning rate to 3.0e-6 with a cosine\nlearning rate scheduler and a warmup ratio of 0.1. We use\na per-device training batch size of 16 with 8 gradient accu-\nmulation steps, resulting in an effective batch size of 256\nusing a temperature of 1. To ensure fairness, we maintain\n4 samples per prompt for all RL-trained models. Unless\notherwise specified, the implementations of all baseline\nmethods follow their original papers and official codebases.\nFor the RL rollout phase, inference is accelerated by vLLM,\nwhich is configured to use 80% of the GPU memory.\nWith minimal truncation observed, the maximum prompt\nand completion lengths are set to 1024 and 2048 tokens,\nrespectively. We train 500 steps for all RL models and three\nepochs for SFT models. For reproducibility, all runs use a\nfixed random seed of 42. In our method, the Swarmup is con-\nfigured to 20. For reliable answer extraction, we adopt the\n“<think>Reasoning</think><answer>Answer</answer>”\ntemplate of R1 (Guo et al. 2025a) during training and use\nthe striped content inside answer tags as the generated\nanswer.\nThe prompt used for all our large model-based verifiers is\ndetailed in Figure 6.\nA.2\nTraining Dynamics Analysis\nWe analyze the training dynamics in Figure\n7 and\n8.\nFigure 7 illustrates the internal characteristics of our P2S\nmethod compared to the SFT+GRPO baseline. The comple-\ntion length (left) of P2S stabilizes at a significantly higher\nFigure 6: LLM-Based Verifier Prompt\nFigure 7: completion step reward comparison\nFigure 8: rewards comparison\nlevel (approx. 150 vs. 60 tokens), encouraging more de-\ntailed reasoning. Concurrently, its unique Path Faithfulness\nReward (PFR, right) remains consistently positive, indicat-\ning that the model is effectively learning to generate faithful\nreasoning steps.\nThese strong internal signals translate to superior exter-\nnal performance, as shown in Figure 8. While both meth-\nods quickly master the required response format (right), P2S\nconsistently achieves a higher Rouge-1 F1 reward (left).\nThis demonstrates that our framework’s ability to foster\nlonger and more faithful reasoning directly results in higher-\nquality final outputs\nB\nAlgorithm Workflow\nThe complete workflow is outlined in Algorithm 1.\n"}, {"page": 11, "text": "Algorithm 1: The P2S Algorithm Flow\nInput: Query q, reference answer y∗, policy model πθ, number of candidates K, current training step Scurrent, the set of rollouts\nRz generated for the query q by the policy model πθ at the step Scurrent.\nOutput: The set of rewards {Rz}z∈G for each path in the group.\n1: // ——————– Phase 1: Dynamic Gold-CoT Synthesis and Filtering ——————–\n2: Generate a set of K candidate paths: C ←{ok ∼πθ(·|q, y∗)}K\nk=1.\n3: Filter for format-correct paths: Cformatted ←{ok ∈C | Rformat(ok) == 1}.\n4: if Cformatted ̸= ∅then\n5:\nFor each candidate in Cformatted, we compute a quality score Sk = P|y∗\nk|\nt=1 log πθ(y∗\nk,t|q, zk, y∗\nk,<t)\n6:\nSelect\no∗←arg max\nok∈Cformatted\nSk\n7: else\n8:\no∗←null. {No valid candidates were found.}\n9: end if\n10: // ——————– Phase 2: Reward Calculation for a Generated Path ——————–\n11: for each path z in the group G do\n12:\n// ——————– Hierarchical Reward Logic and Cold Start for current path z ——————–\n13:\nif Format of z is invalid then\n14:\nRz ←−Cpenalty; continue\n15:\nend if\n16:\nLet SG = 1 if any path in the batch G produced the correct answer, else SG = 0.\n17:\nif SG == 1 then\n18:\nRz ←Routcome(z); continue\n19:\nend if\n20:\nif Scurrent < Swarmup then\n21:\nRz ←0; continue\n22:\nend if\n23:\nif o∗== null then\n24:\nRz ←0; continue\n25:\nend if\n26:\n// ——————– PFR Calculation for current path z ——————–\n27:\nSegment z into m steps (z1, . . . , zm).\n28:\nLet pi = (p1, . . . , pm) be the prefix of the path.\n29:\nLet st = (s1, . . . , s|o∗|) be the suffix of the gold-CoT path o∗.\n30:\nfor i = 1 to m do\n31:\nif i < m then\n32:\nCompute rstep(zi) := maxt (log πθ(st|q, pi) −log πθ(st|q, pmasked))\n33:\nelse if i == m then\n34:\nCompute rstep(zm) := log πθ(y∗|q, z)\n35:\nend if\n36:\nend for\n37:\nCompute step weights using the sigmoid function: wi = σ(i) =\n1\n1+e−i (for i = 1, . . . , m).\n38:\nCompute RPFR-w =\nPm\ni=1 wi·rstep(zi)\nPm\ni=1 wi\n.\n39:\nRz ←RPFR-w.\n40: end for\n41: return {Rz}z∈G\n"}]}