{"doc_id": "arxiv:2601.19503", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.19503.pdf", "meta": {"doc_id": "arxiv:2601.19503", "source": "arxiv", "arxiv_id": "2601.19503", "title": "GradPruner: Gradient-Guided Layer Pruning Enabling Efficient Fine-Tuning and Inference for LLMs", "authors": ["Wei Huang", "Anda Cheng", "Yinggui Wang"], "published": "2026-01-27T11:41:26Z", "updated": "2026-01-27T11:41:26Z", "summary": "Fine-tuning Large Language Models (LLMs) with downstream data is often considered time-consuming and expensive. Structured pruning methods are primarily employed to improve the inference efficiency of pre-trained models. Meanwhile, they often require additional time and memory for training, knowledge distillation, structure search, and other strategies, making efficient model fine-tuning challenging to achieve. To simultaneously enhance the training and inference efficiency of downstream task fine-tuning, we introduce GradPruner, which can prune layers of LLMs guided by gradients in the early stages of fine-tuning. GradPruner uses the cumulative gradients of each parameter during the initial phase of fine-tuning to compute the Initial Gradient Information Accumulation Matrix (IGIA-Matrix) to assess the importance of layers and perform pruning. We sparsify the pruned layers based on the IGIA-Matrix and merge them with the remaining layers. Only elements with the same sign are merged to reduce interference from sign variations. We conducted extensive experiments on two LLMs across eight downstream datasets. Including medical, financial, and general benchmark tasks. The results demonstrate that GradPruner has achieved a parameter reduction of 40% with only a 0.99% decrease in accuracy. Our code is publicly available.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.19503v1", "url_pdf": "https://arxiv.org/pdf/2601.19503.pdf", "meta_path": "data/raw/arxiv/meta/2601.19503.json", "sha256": "8d887d6375511db3f4ab339ca34f4f52b380f82e4d59f8b9ae15a77455e7862b", "status": "ok", "fetched_at": "2026-02-18T02:20:25.143583+00:00"}, "pages": [{"page": 1, "text": "Published as a conference paper at ICLR 2026\nGRADPRUNER: GRADIENT-GUIDED LAYER PRUNING\nENABLING EFFICIENT FINE-TUNING AND INFERENCE\nFOR LLMS\nWei Huang∗Anda Cheng∗\nYinggui Wang†\nAnt Group\nBeijing, China\n{hw19970202, andacheng.cad, wyinggui}@gmail.com\nABSTRACT\nFine-tuning Large Language Models (LLMs) with downstream data is often con-\nsidered time-consuming and expensive. Structured pruning methods are primarily\nemployed to improve the inference efficiency of pre-trained models. Meanwhile,\nthey often require additional time and memory for training, knowledge distillation,\nstructure search, and other strategies, making efficient model fine-tuning challeng-\ning to achieve. To simultaneously enhance the training and inference efficiency of\ndownstream task fine-tuning, we introduce GradPruner, which can prune layers\nof LLMs guided by gradients in the early stages of fine-tuning. GradPruner uses\nthe cumulative gradients of each parameter during the initial phase of fine-tuning\nto compute the Initial Gradient Information Accumulation Matrix (IGIA-Matrix)\nto assess the importance of layers and perform pruning. We sparsify the pruned\nlayers based on the IGIA-Matrix and merge them with the remaining layers. Only\nelements with the same sign are merged to reduce interference from sign vari-\nations. We conducted extensive experiments on two LLMs across eight down-\nstream datasets. Including medical, financial, and general benchmark tasks. The\nresults demonstrate that GradPruner has achieved a parameter reduction of 40%\nwith only a 0.99% decrease in accuracy. Our code is publicly available 1.\n1\nINTRODUCTION\nLLMs have currently gained remarkable performance across various tasks Grattafiori et al. (2024).\nHowever, when handling more specialized tasks, such as medical or financial domains, LLMs often\nexhibit a decline in performance (Zhang et al., 2023). We can fine-tune them on downstream data to\nenhance their capabilities. Fine-tuning LLMs on domain-specific data typically requires substantial\ntime and is expensive. For instance, Yang et al (Yang et al., 2024a). trained a medical LLM that\ntook approximately 221 hours. Furthermore, LoRA fine-tuning over pretrained LMs reduces training\nmemory but does not improve inference efficiency (Han et al., 2024).\nStructured pruning improves inference efficiency by removing parameter blocks. These structured\npruning approaches typically involve two steps Muralidharan et al. (2024); Ma et al. (2023). The\nfirst step uses calibration data to identify important parameters within the model. Since this process\ngenerally does not introduce additional training, it relies more on the LLM’s inherent ability to\nprocess the calibration data. However, due to the LLM’s suboptimal performance on domain-specific\ndata, this may lead to significant biases. The second step involves training or distilling the pruned\nmodel, which requires more time and memory. Some works have also explored structured pruning\nfor efficient LLM training and inference, such as APT Zhao et al. (2024) and SAT Ma et al. (2024).\nAPT can only be applied to LoRA fine-tuning. In SAT, the model structure varies across different\ntraining steps, and the final training step restores the model to its dense form, meaning it cannot\naccelerate inference.\n∗Equal contribution. † Corresponding author.\n1https://github.com/secretflow/ACoLab/tree/main/PaperCode/GradPrune\n1\narXiv:2601.19503v1  [cs.CL]  27 Jan 2026\n"}, {"page": 2, "text": "Published as a conference paper at ICLR 2026\nTo ensure the accuracy of downstream tasks while efficiently training and inference with LLMs,\nwe address three key challenges: 1) Developing a method to measure the importance of model\nparameters tailored to specific downstream data and models without increasing memory or training\ntime, 2) Preserving the original model structure as much as possible while maximizing parameter\npruning, and 3) Supporting both full fine-tuning and LoRA fine-tuning. In this paper, we introduce\nGradPruner, an efficient fine-tuning approach inspired by the observation that loss decreases sharply\nin the initial fine-tuning steps, indicating rapid learning of downstream tasks. Because different\nparameters hold varying levels of importance for downstream tasks, this leads to differences in\nlearning capabilities Zhao et al. (2024). Leveraging this insight, to simultaneously save time and\nmemory, we employ LoRA fine-tuning Hu et al. (2022) to compute the accumulation of gradients\nduring the initial phase of training (which is significantly fewer than the total number of training\nsteps) to obtain the Initial Gradient Information Accumulation Matrix (IGIA-Matrix), which is used\nto assess the importance of each parameter.\nWe adopted layer-level pruning of the model to address the second and third issues. Through ab-\nlation studies, we found that pruning 30% of the layers has almost no impact on the accuracy of\ndownstream tasks, but pruning an additional layer causes a sharp decline in accuracy. To further\nincrease the pruning ratios, we introduced a layer merging method. This method involves sparsify-\ning the pruned layers using the IGIA-Matrix and then merging them with the remaining layers. To\nminimize interference from sign conflicts, we only merge elements with the same sign. By employ-\ning layer merging, we are able to prune three additional layers while maintaining the accuracy of\ndownstream tasks.\nWe conducted extensive experiments on two LLMs and eight downstream datasets. The results\ndemonstrate that GradPruner can prune 40% of the parameters while ensuring that the accuracy on\ndownstream tasks decreases by only 0.99%. Compared to structured pruning methods, our approach\nclearly outperforms them. Additionally, the pruned Llama3.1-8B model achieves better accuracy on\ndownstream tasks than the Llama3.2-3B.\nThis paper makes the following key contributions: 1) We propose GradPruner, which calculates the\nIGIA-Matrix using the initial gradients from LoRA fine-tuning to evaluate the importance of each\nmodel parameter for downstream tasks. 2) To prune as many layers as possible, we sparsify the\npruned layers based on the IGIA-Matrix and then merge them with the remaining layers, only com-\nbining elements with the same sign. 3) Extensive experiments show that GradPruner has achieved a\nparameter reduction of 40% with only a 0.99% decrease in downstream tasks accuracy.\n2\nBACKGROUND AND MOTIVATION\n2.1\nPROBLEM FORMULATION\nOur goal is to enhance the efficiency of fine-tuning and inference for LLMs while ensuring the ac-\ncuracy of downstream tasks. Intuitively, the more parameters that are pruned, the faster the training\nand inference will be, but the accuracy on downstream tasks will decrease. Conversely, the fewer\nparameters that are pruned, the better the accuracy on downstream tasks, but the training and infer-\nence speed will slow down. However, numerous researchers have pointed out that LLMs contain a\nsignificant number of redundant parameters Yadav et al. (2023); Muralidharan et al. (2024). There-\nfore, we aim to identify the minimal set of model parameters that can be retained to preserve the\naccuracy of downstream tasks.\nWe define the downstream task dataset as D, consisting of N samples (x1, y1), ..., (xN, yN). The\nobjective of our problem is to achieve the maximum pruning ratio ψ while ensuring the minimization\nof the task loss L. We describe the optimization process as:\narg min\nθ∗ψ\n1\n|D|\nX\nx,y∈D\nL(x, y|θ, ψ)\n(1)\nwhere θ represents all the model parameters.\n2\n"}, {"page": 3, "text": "Published as a conference paper at ICLR 2026\nTraining steps\nLoss value\n(a)\n(b)\n0   100  200 300 400 500 600 700\n0        200     400     600     800\n0   2   4   6   8  10  12\n0  1  2  3   4  5  6   7  8\nTraining steps\nLoss value\nFigure 1: The loss of training while LoRA fine-tuning PubMedQA (a) and PIQA (b) on Llama3.1-\n8B. The loss value showed a rapid decrease during the initial training steps.\n2.2\nMOTIVATION\nMany existing pruning methods rely on calibration data, using forward propagation (such as interme-\ndiate layer outputs) to assess the importance of various parameters. Such judgments depend heavily\non the LLM’s ability to process the calibration data. Nevertheless, when dealing with domain-\nspecific tasks, the LLM’s limited processing capabilities may introduce biases.\n1%\nFigure 2: Gradient Sensitivity Analysis of\nthe IGIA-Matrix on Llama3.1-8B. We can\nsee that the layer importance measured at\nsuch an early stage can accurately reflect the\nresults after the entire training process.\nWe found that during experiments, when an\nLLM is fine-tuned on downstream tasks, the\nloss sharply drops within the initial 1% of the\ntraining steps (as shown in Figure 1). This\nindicates that the model quickly grasps the\nknowledge required for the downstream task,\nand different parameters contribute variably\nto the learning process. This phenomenon\nprovides us with insights.\nBased on the above observations and reason-\ning, it is natural to consider using the ini-\ntial gradients from LoRA fine-tuning to mea-\nsure the importance of different parameters.\nThis approach can not only accurately iden-\ntify parameters that are crucial for down-\nstream tasks but also save time and reduce\nmemory consumption.\n2.3\nGRADIENT SENSITIVITY ANALYSIS OF THE IGIA-MATRIX\nExisting research has demonstrated that performing full training and subsequently using gradient\ninformation to assess the importance of different parameters is a reasonable approach Matena &\nRaffel (2022); Daheim et al. (2023). However, our proposed IGIA-Matrix method requires only the\ninitial 1% of the training steps to measure the importance of each layer. To analyze the relationship\nbetween the layer importance derived from our method and that obtained after complete training,\nwe conducted a Gradient Sensitivity Analysis of the IGIA-Matrix.\nIn our study, we recorded the layer importance at various stages of the training process and used the\nlayer importance obtained after full training as the reference label list. Since we need to prune layers,\nonly the indices of the top 20 most important layers were retained in the label list. We then compared\nthe top 20 layers identified at each stage with this label list. If any layer among the top 20 from a\ngiven stage was not present in the label list, we considered that layer’s result as mismatched with\nthe labels, resulting in a drop in accuracy. The experimental results of this analysis are presented\n3\n"}, {"page": 4, "text": "Published as a conference paper at ICLR 2026\nLayer i\nMulti Head \nAttention\nAdd & Norm\nFeed \nForward\nAdd & Norm\nLayer i+1\nDataset\nLayer n\n……\nOutput/Loss\nGradient Calculation and Backward\nLora_A\nLora_B\nLora_A\nLora_B\nMulti Head \nAttention\nAdd & Norm\nFeed \nForward\nAdd & Norm\nLayer i+2\nLora_A\nLora_B\nLora_A\nLora_B\nInitial LoRA gradient acquisition (model training)\n1\n IGIA-Matrix computation\n2\nLora_A_grad\nLora_B_grad\nLora_B_grad ! Lora_A_grad\n  Merged \nLora Grad\ncompute the IGIA-Matrix of Linear \nLayer pruning\n3\nLayer i\nLayer i+1\nLayer i+2\nLayer i+3\n……\nLayer n\nLayer i+4\n✂\nLayer 0\nIGIA-Matrix\nForward computation\nFigure 3: The framework diagram of Parameter Importance Evaluation and Layer Pruning in Grad-\nPruner. The first step involves obtaining gradients through a small amount of LoRA fine-tuning.\nThe second step calculates the IGIA-Matrix based on gradients. In the third step, we assess the\nimportance of each parameter and each layer based on the IGIA-Matrix and subsequently prune the\nlayers accordingly.\nin Figure 2. From the figure, we can conclude that the layer importance measured at such an early\nstage can accurately reflect the results after the entire training process.\n3\nMETHODOLOGY\nIn this section, we provide a detailed explanation of GradPruner. GradPruner consists of three\nsteps: (1) Parameter Importance Measurement Phase. This step focuses on identifying the model\nparameters that are important for downstream tasks. (2) Layer Pruning Phase. Once the importance\nof different parameters is determined, the second step assesses the importance of various layers and\nprunes them accordingly. (3) Layer Merging Phase. This step involves merging the pruned layers\nwith the remaining layers.\n3.1\nPARAMETER IMPORTANCE EVALUATION AND LAYER PRUNING\nMotivated by the observations from Figure 1, we observe that the loss decreases rapidly during the\ninitial training phase as the model quickly adapts to downstream tasks. Recognizing that different\nparameters hold varying levels of importance, reflected in their gradient update magnitudes. Based\non this insight, we measure parameter significance by capturing gradient values in each step of the\nearly LoRA training phase. The overall process of the algorithm is illustrated in Figure 3.\nParameter Importance Evaluation: Formally, we consider a linear layer with weight W.The cor-\nresponding LoRA weights are WA and WB. We freeze the parameter W and train the model using\nthe downstream dataset D. We define the model to undergo a total of T training steps; however,\nwe only need to obtain the gradients for the first t steps (t << T),meaning that the model training\nterminates after t steps. After t rounds of training, we obtain the per-step gradient values for WA\nand WB, denoted as ∇WAL(x, y) and ∇WBL(x, y),respectively. Specifically, ∇WAL(x, y) consists\nof t gradients: {∇WAL(x, y)1, ∇WAL(x, y)2, ..., ∇WAL(x, y)t}. After obtaining ∇WAL(x, y) and\n∇WBL(x, y), we need to evaluate the importance of each parameter in W for the downstream task.\nFirst, we align the matrix dimensions of ∇WAL(x, y) and ∇WBL(x, y) with those of W. We Inspire\nfor LoRA to be mergeable with the original parameters after fine-tuning, so we align them with W\nby performing matrix multiplication on ∇WAL(x, y) and ∇WBL(x, y) to simulate the gradient of\nW. The formula is as follows:\n∇W L(x, y)i\nsim\n= ∇WBL(x, y)i · ∇WBL(x, y)i,\ni ∈{1, ..., t}\n(2)\n4\n"}, {"page": 5, "text": "Published as a conference paper at ICLR 2026\nFigure 4: The framework diagram of Layer Merging in GradPruner. The first step is to sparsify the\npruned modules using the IGIA-Matrix as the criterion. The second step is to merge the pruned layer\nwith the preceding retained layer based on their signs.\nWhere, sim represents the simulated gradient.\nUsing Equation (2), we have obtained the gradients of the W at each step during the first t training\nsteps. To comprehensively evaluate the relationship between the gradients from the initial t training\nsteps and parameter importance, we calculate the IGIA-Matrix using the following formula.\nFW = 1\nt\nt\nX\ni=1\n(∇W L(x, y)i)2\n(3)\nWe obtain the IGIA-Matrix FW for W through the steps above. Similarly, all linear layers in LLM\ncan acquire their corresponding IGIA-Matrix using this approach.\nLayer Pruning: We choose to prune the model’s layers based on the following two considera-\ntions: 1) Preservation of the Model’s Overall Structure: To ensure that the overall architecture of\nthe model remains as unchanged as possible. 2) Impact on Downstream Task Accuracy: Our exper-\niments demonstrate that pruning the parameters of important layers adversely affects the accuracy\nof downstream tasks (see ablation study). We sum the IGIA-Matrix of all linear layers within each\nlayer of the LLM to obtain the importance score of the current layer, as shown in the following:\nLayerj =\nM\nX\nk=1\nH\nX\nl=1\nFWkl\n(4)\nWhere Layerj represents the importance score of the j-th layer, M denotes the number of linear\nlayers within the j-th layer, and H signifies the number of parameters in each linear layer. Based on\nthe above formula, we can obtain the importance score for each layer in the model. The pseudocode\nfor layer pruning can be found in Appendix A.\n3.2\nLAYER MERGING\nWhen only performing layer pruning, our ablation study indicates that pruning more than 30% of\nthe layers results in a significant decrease in accuracy. To prune as many layers of the model as\npossible, we do not directly discard the pruned layers but instead merge them with the remaining\nlayers. The overall process of the algorithm is illustrated in Figure 4.\nTo merge multiple linear layers {Wj}n\nj=1 from different layers. We designate W1 as the linear layer\nto be retained and {W2, ..., Wn} as the linear layers to be pruned. Our method follows two steps to\nperform the merging.\n1) Sparsification: Sparsification aims to reduce interference between pruned and retained layers\nwhile preserving downstream task accuracy. Research shows that keeping only a subset of model\nparameters can maintain this accuracy. The key challenge is determining which parameters are\nmost important. To tackle this, we use the IGIA-Matri as a metric to evaluate the importance\nof each parameter for downstream tasks. The IGIA-Matrix corresponding to {W2, ..., Wn} are\n{FW2, ..., FWn}. We retain the top-p% of parameters based on the magnitude of the IGIA-Matrix\n5\n"}, {"page": 6, "text": "Published as a conference paper at ICLR 2026\nMethod\nPubMedQA\nMedMCQA\nBillSum\nFinGPT\nHellaSwag\nWinoGrande\nARC\nPIQA\nAvg.\nLlama3.1-8B\nFull Fine-Tuning (FFT)\nDense Model\n0.593\n0.572\n0.696\n0.869\n0.943\n0.868\n0.865\n0.867\n0.784\nLLMPruner\n0.560\n0.521\n0.641\n0.818\n0.898\n0.810\n0.817\n0.805\n0.734\nLaco\n0.556\n0.514\n0.649\n0.814\n0.901\n0.818\n0.824\n0.809\n0.736\nMINITRON\n0.555\n0.527\n0.640\n0.808\n0.898\n0.814\n0.826\n0.803\n0.734\nSAT\n0.567\n0.552\n0.656\n0.832\n0.908\n0.835\n0.822\n0.833\n0.750\nFT(Llama3.2)\n0.591\n0.568\n0.688\n0.867\n0.932\n0.836\n0.874\n0.861\n0.777\nGradPruner(ours)\n0.591\n0.586\n0.687\n0.867\n0.939\n0.861\n0.849\n0.876\n0.782\nParameter-Efficient Fine Tuning (LoRA)\nDense Model\n0.607\n0.633\n0.677\n0.831\n0.959\n0.821\n0.931\n0.893\n0.794\nLLMPruner\n0.538\n0.555\n0.636\n0.766\n0.883\n0.790\n0.870\n0.826\n0.733\nLaco\n0.553\n0.556\n0.631\n0.775\n0.907\n0.792\n0.875\n0.837\n0.740\nMINITRON\n0.546\n0.557\n0.631\n0.765\n0.899\n0.781\n0.873\n0.822\n0.734\nSAT\n0.550\n0.596\n0.621\n0.779\n0.909\n0.780\n0.882\n0.844\n0.745\nAPT\n0.561\n0.607\n0.645\n0.802\n0.922\n0.790\n0.891\n0.859\n0.759\nFT(Llama3.2)\n0.592\n0.619\n0.662\n0.825\n0.926\n0.808\n0.902\n0.859\n0.774\nGradPruner(ours)\n0.594\n0.637\n0.659\n0.817\n0.954\n0.812\n0.923\n0.891\n0.786\nMistral-7B\nFull Fine-Tuning (FFT)\nDense Model\n0.591\n0.583\n0.684\n0.862\n0.841\n0.878\n0.905\n0.903\n0.781\nLLMPruner\n0.547\n0.512\n0.639\n0.817\n0.810\n0.815\n0.867\n0.839\n0.730\nLaco\n0.552\n0.525\n0.641\n0.819\n0.825\n0.828\n0.864\n0.857\n0.738\nMINITRON\n0.544\n0.511\n0.650\n0.823\n0.822\n0.830\n0.851\n0.846\n0.734\nSAT\n0.561\n0.543\n0.657\n0.823\n0.834\n0.836\n0.867\n0.870\n0.748\nGradPruner(ours)\n0.586\n0.568\n0.670\n0.846\n0.840\n0.860\n0.895\n0.897\n0.770\nParameter-Efficient Fine Tuning (LoRA)\nDense Model\n0.607\n0.565\n0.681\n0.853\n0.963\n0.846\n0.909\n0.896\n0.790\nLLMPruner\n0.527\n0.508\n0.625\n0.793\n0.904\n0.801\n0.840\n0.827\n0.728\nLaco\n0.541\n0.499\n0.643\n0.804\n0.904\n0.812\n0.851\n0.846\n0.737\nMINITRON\n0.533\n0.504\n0.633\n0.802\n0.900\n0.805\n0.840\n0.835\n0.731\nSAT\n0.545\n0.524\n0.629\n0.807\n0.931\n0.815\n0.857\n0.843\n0.743\nAPT\n0.555\n0.533\n0.631\n0.813\n0.926\n0.815\n0.866\n0.854\n0.750\nGradPruner(ours)\n0.588\n0.565\n0.659\n0.840\n0.963\n0.832\n0.893\n0.896\n0.780\nTable 1:\nThe main results of our experiments under 40% sparsity pruning. “Avg.” refers to the\naverage score between eight datasets. “Dense Model” represents the results of the unpruned LLMs\nafter fine-tuning. “FT” represents fine-tuning Llama3.2-3B. Since APT can only be applied to LoRA\nfine-tuning, we only report the results of APT in the context of LoRA fine-tuning.\nand set the remaining parameters to zero, thereby creating { ˆ\nW2, ..., ˆ\nWn}. Notably, we do not need\nto sparsify W1, as it is a more critical linear layer, and sparsifying it could adversely affect the\naccuracy of downstream tasks.\n2) Symbol-based merging. A given parameter may have positive values for some layers and nega-\ntive values for others. In both cases, simply merging these values can lead to interference, thereby\nshrinking the value of that parameter in the merged layer. Therefore, we using the parameter signs\nof W1 as the total sign. Only the parameters in { ˆ\nW2, ..., ˆ\nWn} with signs matching the total sign\nare merged with W1. For more details, please refer to Equation 5.\nM (Wj)kl =\n\n\n\n\n\n(Wj)kl\nif (γj)kl! = (γj+1)kl! = (γj+n)kl\n(Wj)kl +\nˆ\n(Wj+1)kl\nif (γj)kl == (γj+1)kl! = (γj+n)kl\n(Wj)kl +\nˆ\n(Wj+n)kl\nif (γj)kl! = (γj+1)kl == (γj+n)kl\n(5)\nWhere (Wj)kl denotes the l-th parameter of the k-th linear layer in the j-th layer, and γ represents\nthe sign matrix corresponding to the parameter matrix of the linear layer. Similarly, each linear layer\nin the pruned layer can be merged with the corresponding linear layer of the retained layer following\nthe above process. The pruned layer is only merged with the preceding retained layer.\n4\nEXPERIMENTAL SETTINGS\n4.1\nDATASETS AND SETTING\nTo comprehensively evaluate the effectiveness of GradPruner in the vertical domain, we conducted\nextensive experiments using two widely adopted LLMs and eight downstream datasets. For the\n6\n"}, {"page": 7, "text": "Published as a conference paper at ICLR 2026\nMethod\nTrain Time(⇓)\nTrain Mem(⇓)\nInf Time(⇓)\nInf Mem(⇓)\nFull Fine-Tuning (FFT)\nDense Model\n100.0%\n100.0%\n100.0%\n100.0%\nSAT\n75.5%\n79.4%\n98.9%\n103.6%\nLaco\n73.8%\n64.4%\n59.7%\n61.3%\nLLMPruner\n78.3%\n284.4%\n67.4%\n65.3%\nGradPruner\n62.4%\n65.8%\n61.5%\n60.9%\nParameter-Efficient Fine Tuning (LoRA)\nDense Model\n100.0%\n100.0%\n100.0%\n100.0%\nAPT\n158.0%\n65.9%\n87.5%\n62.4%\nSAT\n74.8%\n82.9%\n102.3%\n99.4%\nLaco\n71.5%\n65.7%\n61.1%\n61.4%\nLLMPruner\n81.8%\n266.4%\n69.7%\n64.6%\nGradPruner\n66.9%\n64.4%\n61.8%\n60.7%\nTable 2: Comparison of GradPruner with other methods in terms of training and inference time, as\nwell as GPU memory usage. Our measurement method is based on APT’s paper. All efficiency met-\nrics are normalized to Dense Model, that is, the relative time or memory overhead compared to the\nDense Model. ⇓denotes smaller is better. We do not compare to distillation method (MINITRON)\nbecause the training cost of distillation is too large.\nLLMs, we selected Llama3.1-8B Dubey et al. (2024) and Mistral-7B-v0.3 Jiang et al. (2023). Re-\ngarding the downstream datasets, we included four specialized domain datasets: PubMedQA Jin\net al. (2019) and MedMCQA Pal et al. (2022), which focus on medical tasks, and BillSum Ko-\nrnilova & Eidelman (2019) and fingpt-sentiment-train (FinGPT) Yang et al. (2023), which pertain to\nfinancial tasks. Additionally, we incorporated four general-domain reasoning benchmark datasets:\nHellaSwag, WinoGrande, ARC, and PIQA Yao et al. (2024).\nDue to space constraints in the main paper, a detailed description of the datasets and the setup of our\nexperiment is presented in the appendix C.\nOur evaluation metrics are formulated based on the characteristics of the dataset. The QA data in\nthe medical and financial fields, we adopt the method of evaluating the similarity between the output\nfrom LLMs and the standard label. We evaluate using 1/2 * ( BertScore (Zhang* et al., 2020) +\nROUGE-L (Lin, 2004)). As for other reasoning benchmarks, we directly calculate the Accuracy\nscore.\n4.2\nBASELINES\nWe conducted extensive comparisons between GradPruner and six baselines. Categorizing the base-\nlines into three groups. The first group consists of structured pruning methods focused on addressing\nhow to efficiently train and infer when LLMs are adapted to downstream tasks. 1) APT Zhao et al.\n(2024), which dynamically adds salient tuning parameters for fast and accurate convergence while\ndiscarding unimportant parameters to improve efficiency; 2) SAT Ma et al. (2024), which extends\nexisting neuron importance evaluation metrics and introduces a ladder omission rate scheduler.\nThe second group of structured pruning methods focuses on accelerating inference.\n1) LLM-\nPruner Ma et al. (2023) adopts structural pruning that selectively removes non-critical coupled struc-\ntures based on gradient information. 2) LaCo Yang et al. (2024b), a concise layer-wise structured\npruner called Layer Collapse, in which rear model layers collapse into a prior layer; 3) MINI-\nTRON Muralidharan et al. (2024), which combines depth, width, attention, and MLP pruning with\nknowledge distillation-based retraining. We fine-tuned the models pruned by LLMPruner, LaCo,\nand MINITRON on downstream data for comparison. Since MINITRON requires pre-training data\nfor knowledge distillation, which we could not access, we used the Alpaca dataset as a substitute.\nThe third group of baselines consists of smaller parameter models. We used the fine-tuning of\nLlama3.2-3B Dubey et al. (2024) on downstream tasks as a comparison method.\nIt is noted that, to ensure a fair comparison, the pruning baselines also utilize the downstream\ndatasets as calibration data.\n7\n"}, {"page": 8, "text": "Published as a conference paper at ICLR 2026\nNumber\nMethod\nPubMedQA\nMedMCQA\nBillSum\nFinGPT\nHellaSwag\nWinoGrande\nARC\nPIQA\nAvg.\nLlama3.1-8B\nFull Fine-Tuning (FFT)\nDense Model\n0.593\n0.572\n0.696\n0.869\n0.943\n0.868\n0.865\n0.867\n0.784\n3\nGradPruner\n0.591\n0.586\n0.687\n0.867\n0.939\n0.861\n0.849\n0.876\n0.782\nw/o Merging\n0.560\n0.535\n0.663\n0.816\n0.893\n0.803\n0.830\n0.826\n0.741\n2\nGradPruner\n0.593\n0.590\n0.695\n0.867\n0.942\n0.868\n0.871\n0.861\n0.786\nw/o Merging\n0.581\n0.566\n0.677\n0.841\n0.930\n0.840\n0.846\n0.843\n0.767\n1\nGradPruner\n0.590\n0.588\n0.695\n0.866\n0.945\n0.863\n0.866\n0.868\n0.785\nw/o Merging\n0.585\n0.580\n0.688\n0.855\n0.936\n0.852\n0.853\n0.851\n0.775\nTable 3:\nExperimental results on the impact of the number of merged layers on downstream task\naccuracy. w/o Merging indicates that no layer merging. The experimental results of LoRA fine-\ntuning and Mistral-7B are presented in the appendix D.\n5\nMAIN RESULTS\nTable 1 evaluates different pruning methods and fine-tuning approaches across multiple datasets.\nThe results show that GradPruner consistently outperforms other methods, achieving the best per-\nformance on all datasets. To summarize the findings, we compute the average scores of each pruner,\nrevealing that GradPruner incurs only a 0.99% average performance drop comparing with the dense\nmodel. Notably, our pruned models outperform directly fine-tuned versions of Llama3.2-3B, demon-\nstrating competitiveness with advanced pre-trained LLMs. GradPruner exhibits particularly strong\ngains, improving accuracy by approximately 5 percentage points on average for LLMPruner, Laco,\nand MINITRON, while also surpassing SAT and APT in accuracy.\nIn addition to accuracy, Table 2 compares GradPruner with other methods in terms of time and GPU\nmemory usage during training and inference. From Table 2, GradPruner achieves approximately\n36% reductions in both training time and memory usage, along with 39% savings in inference time\nand memory, performing comparably to Laco. In contrast, the APT method shows significantly\nhigher training time due to its reliance on knowledge distillation, while the SAT method exhibits\nsimilar inference time and memory usage to the dense model, as it gradually restores pruned param-\neters during training.\n6\nABLATION STUDY AND ANALYSIS\nWe conducted five ablation experiments to validate the design decisions and effectiveness of the\nproposed method.\nPerform Only the Layer Pruning in GradPruner: Figure 5 presents the results of using only the\nlayer pruning strategy. For Llama3.1-8B, pruning up to 10 layers has minimal impact on downstream\ntask accuracy. However, beyond 10 layers, accuracy begins to decline significantly, as increasingly\ncritical layers are removed. These findings suggest that while layer pruning alone is effective to a\ncertain extent, it becomes insufficient for aggressive pruning. The experimental results of Mistral-7B\ncan be found in the appendix D.\nThe Impact of the Number of Merged Layers on Accuracy: Table 3 demonstrates the impact on\ndownstream task accuracy after incorporating the layer merging algorithm on top of layer pruning.\nFrom the table, we can observe that for both models, whether fine-tuned using FFT or LoRA, layer\nmerging significantly improves accuracy. Specifically, for Llama3.1-8B, merging 1 to 3 layers on\ntop of pruning 10 layers results in a substantial accuracy improvement, nearly matching the dense\nmodel accuracy. Overall, layer merging ensures the accuracy of downstream tasks while further\nincreasing the pruning rate.\nThe Impact of Different Pruning Ratios: To investigate the impact of different sparsity rates on\ndownstream task accuracy, we conducted relevant experiments. Specifically, we set the sparsity rates\nto 50%, 60%, 70%, 80%, and 90% to observe their effects on task performance. The experimental\nresults are shown in Figure 6. By analyzing the data in the figure, we found that both excessively\nhigh and low sparsity rates have varying degrees of impact on task accuracy. When the sparsity rate\n8\n"}, {"page": 9, "text": "Published as a conference paper at ICLR 2026\nFigure 5: Experimental results of perform-\ning Only the layer pruning in GradPruner.\nWe report the average accuracy across eight\ndatasets.\n50\n55\n60\n65\n70\n75\n80\n85\n90\nThe parsity ratio (%)\n0.74\n0.75\n0.76\n0.77\n0.78\n0.79\nAccuracy(Avg.)\nThe Impact of Different Pruning Ratios\nLlama3.1-8B(FFT)\nLlama3.1-8B(LoRA)\nMistral-7B(FFT)\nMistral-7B(LoRA)\nFigure 6: Experimental results of different\npruning ratios in GradPruner. We report the\naverage accuracy across eight datasets.\nFigure 7: Experimental results of replacing\nsymbol-based merging with weighted aver-\naging in GradPruner. We report the average\naccuracy across eight datasets.\nis too high, too many important parame-\nters are pruned; conversely, when the spar-\nsity rate is too low, too many redundant pa-\nrameters are retained, leading to increased\ninterference among parameters during layer\nmerging. Based on the ablation study results,\nwe selected an 80% sparsity rate as the opti-\nmal setting.\nReplacing Symbol-Based Merging with\nWeighted Averaging: GradPruner, during\nlayer merging, employs a sign-based merg-\ning strategy on top of sparsification.\nTo\ndemonstrate its superiority, we replaced the\nsign-based merging with a weight-averaging\nmethod for comparison.\nThe results are\nshown in Figure 7.\nIt is evident that sign-based merging significantly outperforms weight averaging. This indicates that\nmerging only parameters with matching signs can effectively reduce interference among different\nparameters, thereby enhancing the accuracy of downstream tasks.\nReplacing Layer Pruning with Kernel Pruning: Readers may wonder why GradPruner focuses\non pruning layers rather than adopting a finer-grained approach, such as pruning kernels (rows or\ncolumns). Due to space constraints in the main paper, we have included this portion of the experi-\nmental results in the appendix D.\n7\nRELATED WORK\nStructured Model Pruning refers to techniques for improving model efficiency by sparsification\nor parameter removal. While some considerations for pruning are from the perspective of hard-\nware (Xia et al., 2023).\nGenerally, considerations are made from the depth or width of the\nLLMs (Ling et al., 2024). During the pruning process, the parameters of the target part are typi-\ncally directly deleted without considering the reliability of the parameter-judgment process (Men\net al., 2025), and the performance of the model often suffers significant losses. As a result, some\nworks consider merging parameters into more important layers (Liu et al., 2024; Yang et al., 2024b).\nHowever, these works always consider pruning in depth and width respectively. NASH constructs a\nnarrow encoder and a shallow decoder for T5 models (Ko et al., 2023). He et al. (2025) investigate\n9\n"}, {"page": 10, "text": "Published as a conference paper at ICLR 2026\nredundancy across different modules within Transformers, including Layers, MLP, and Attention\nlayers, none of them consider jointly pruning by a combination of multiple importance scores.\nSparse Training is a training approach that makes model parameters sparsely distributed during\ntraining (Graesser et al., 2022). Frankle & Carbin (2019); Evci et al. (2019) discovered that training\nsparse networks from a random initialization is difficult compared to dense neural networks. Despite\nthis, dynamic sparse training (Liu et al., 2021), and one-shot pruning (Tanaka et al., 2020) were\nproposed to improve sparse training. Sparse Training needs to change the structure of the model\nduring initialization, so it is hard to adapt to downstream tasks for well-trained LLMs.\n8\nCONCLUSION\nIn this study, we propose GradPruner to address the challenges of time-consuming fine-tuning and\nhigh GPU memory usage in LLMs. We compute the initial training gradients to obtain the IGIA-\nMatrix, which is used to evaluate the importance of different layers. To further increase the number\nof pruned layers, we introduce a layer merging technique, which includes sparsification using the\nIGIA-Matrix and resolving sign interference issues. We conducted experiments on various down-\nstream datasets and models. The results demonstrate that GradPruner can maintain nearly the same\naccuracy while reducing the number of parameters by 40%.\nREFERENCES\nYonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language. Proceedings of the AAAI Conference on Artificial\nIntelligence, 34(05):7432–7439, Apr. 2020. doi: 10.1609/aaai.v34i05.6239. URL https://\nojs.aaai.org/index.php/AAAI/article/view/6239.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nNico Daheim, Thomas M¨ollenhoff, Edoardo Maria Ponti, Iryna Gurevych, and Mohammad Emtiyaz\nKhan. Model merging by uncertainty-based gradient matching. arXiv preprint arXiv:2310.12808,\n2023.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nUtku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen. The difficulty of training sparse\nneural networks. In ICML 2019 Workshop on Identifying and Understanding Deep Learning\nPhenomena, 2019. URL https://openreview.net/forum?id=SyeyPEH23N.\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\nnetworks. In International Conference on Learning Representations, 2019. URL https://\nopenreview.net/forum?id=rJl-b3RcF7.\nLaura Graesser, Utku Evci, Erich Elsen, and Pablo Samuel Castro. The state of sparse training\nin deep reinforcement learning.\nIn Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba\nSzepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Confer-\nence on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp.\n7766–7792. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/\ngraesser22a.html.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783, 2024.\nZeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning\nfor large models: A comprehensive survey. arXiv preprint arXiv:2403.14608, 2024.\n10\n"}, {"page": 11, "text": "Published as a conference paper at ICLR 2026\nShwai He, Guoheng Sun, Zheyu Shen, and Ang Li. What matters in transformers? not all attention\nis needed, 2025. URL https://openreview.net/forum?id=YLTWwEjkdx.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-\nlot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\nL´elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timoth´ee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https:\n//arxiv.org/abs/2310.06825.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A dataset\nfor biomedical research question answering. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pp. 2567–2577, 2019.\nHyo Seo Kim, Dongyoon Han, and Junsuk Choe. Negmerge: Sign-consensual weight merging for\nmachine unlearning. In International Conference on Machine Learning, 2025.\nJongwoo Ko, Seungjoon Park, Yujin Kim, Sumyeong Ahn, Du-Seong Chang, Euijai Ahn, and Se-\nYoung Yun. NASH: A simple unified framework of structured pruning for accelerating encoder-\ndecoder language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pp. 6076–6093, Singapore, December\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.404.\nURL https://aclanthology.org/2023.findings-emnlp.404/.\nAnastassia Kornilova and Vladimir Eidelman. BillSum: A corpus for automatic summarization\nof US legislation. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, and Fei Liu (eds.),\nProceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 48–56, Hong Kong,\nChina, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5406.\nURL https://aclanthology.org/D19-5406/.\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguis-\ntics. URL https://aclanthology.org/W04-1013/.\nGui Ling, Ziyang Wang, YuliangYan, and Qingwen Liu. SlimGPT: Layer-wise structured prun-\ning for large language models. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. URL https://openreview.net/forum?id=MxF0IKJtKW.\nDeyuan Liu, Zhanyue Qin, Hairu Wang, Zhao Yang, Zecheng Wang, Fangying Rong, Qingbin Liu,\nYanchao Hao, Bo Li, Xi Chen, Cunhang Fan, Zhao Lv, Dianhui Chu, Zhiying Tu, and Dianbo\nSui. Pruning via merging: Compressing LLMs via manifold alignment based layer merging. In\nYaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing, pp. 17817–17829, Miami, Florida, USA,\nNovember 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.\n987. URL https://aclanthology.org/2024.emnlp-main.987/.\nShiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually need\ndense over-parameterization? in-time over-parameterization in sparse training. In Marina Meila\nand Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,\nvolume 139 of Proceedings of Machine Learning Research, pp. 6989–7000. PMLR, 18–24 Jul\n2021. URL https://proceedings.mlr.press/v139/liu21y.html.\nDa Ma, Lu Chen, Pengyu Wang, Hongshen Xu, Hanqi Li, Liangtai Sun, Su Zhu, Shuai Fan, and Kai\nYu. Sparsity-accelerated training for large language models. In Findings of the Association for\nComputational Linguistics ACL 2024, pp. 14696–14707, 2024.\nXinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large\nlanguage models. Advances in neural information processing systems, 36:21702–21720, 2023.\n11\n"}, {"page": 12, "text": "Published as a conference paper at ICLR 2026\nMichael S Matena and Colin A Raffel. Merging models with fisher-weighted averaging. Advances\nin Neural Information Processing Systems, 35:17703–17716, 2022.\nXin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and\nweipeng chen. ShortGPT: Layers in large language models are more redundant than you expect,\n2025. URL https://openreview.net/forum?id=JMNht3SmcG.\nSaurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa\nPatwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov.\nCom-\npact language models via pruning and knowledge distillation.\nIn A. Globerson, L. Mackey,\nD. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neu-\nral Information Processing Systems, volume 37, pp. 41076–41102. Curran Associates, Inc.,\n2024.\nURL https://proceedings.neurips.cc/paper_files/paper/2024/\nfile/4822991365c962105b1b95b1107d30e5-Paper-Conference.pdf.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical domain question answering. In Gerardo Flores,\nGeorge H Chen, Tom Pollard, Joyce C Ho, and Tristan Naumann (eds.), Proceedings of the Con-\nference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning Re-\nsearch, pp. 248–260. PMLR, 07–08 Apr 2022. URL https://proceedings.mlr.press/\nv174/pal22a.html.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\nsarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.\nHidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, and Surya Ganguli. Pruning neural networks\nwithout any data by iteratively conserving synaptic flow. In Proceedings of the 34th International\nConference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020.\nCurran Associates Inc. ISBN 9781713829546.\nHaojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei\nLin, and Shuaiwen Leon Song. Flash-llm: Enabling cost-effective and highly-efficient large gen-\nerative model inference with unstructured sparsity. Proc. VLDB Endow., 17(2):211–224, October\n2023. ISSN 2150-8097. doi: 10.14778/3626292.3626303. URL https://doi.org/10.\n14778/3626292.3626303.\nPrateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. Ties-merging: Re-\nsolving interference when merging models. Advances in Neural Information Processing Systems,\n36:7093–7115, 2023.\nHongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large\nlanguage models. FinLLM Symposium at IJCAI 2023, 2023.\nSonghua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, and Hongying\nZan. Zhongjing: Enhancing the chinese medical capabilities of large language model through\nexpert feedback and real-world multi-turn dialogue. In Proceedings of the AAAI conference on\nartificial intelligence, volume 38, pp. 19368–19376, 2024a.\nYifei Yang, Zouying Cao, and Hai Zhao.\nLaCo: Large language model pruning via layer col-\nlapse. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association\nfor Computational Linguistics: EMNLP 2024, pp. 6401–6417, Miami, Florida, USA, November\n2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.372.\nURL https://aclanthology.org/2024.findings-emnlp.372/.\nKai Yao, Zhaorui Tan, Tiandi Ye, Lichun Li, Yuan Zhao, Wenyan Liu, Wei Wang, and Jianke Zhu.\nScaleot: Privacy-utility-scalable offsite-tuning with dynamic layerreplace and selective rank com-\npression, 2024. URL https://arxiv.org/abs/2412.09812.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\nHellaSwag: Can a\nmachine really finish your sentence?\nIn Anna Korhonen, David Traum, and Llu´ıs M`arquez\n(eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\npp. 4791–4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.\n18653/v1/P19-1472. URL https://aclanthology.org/P19-1472/.\n12\n"}, {"page": 13, "text": "Published as a conference paper at ICLR 2026\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi\nHu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv\npreprint arXiv:2308.10792, 2023.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore:\nEvaluating text generation with bert. In International Conference on Learning Representations,\n2020. URL https://openreview.net/forum?id=SkeHuCVFDr.\nBowen Zhao, Hannaneh Hajishirzi, and Qingqing Cao. Apt: adaptive pruning and tuning pretrained\nlanguage models for efficient training and inference. In Proceedings of the 41st International\nConference on Machine Learning, pp. 60812–60831, 2024.\n13\n"}, {"page": 14, "text": "Published as a conference paper at ICLR 2026\nA\nPSEUDOCODE FOR LAYER PRUNING\nAlgorithm 1 Layer Pruning\nInput: Dataset D, total training steps T, initial training steps t (with t << T), model M, a linear\nlayer W within M, the corresponding LoRA weights WA and WB for W, the number of pruned\nlayers N, and the total number of layers K\nOutput: Pruned Model Mprune\n1: Initialize the weights of WA and WB.\n2: ⊵Step 1: Obtain the gradient corresponding to W at every training step during training.\n3: for i in {1,...,t} do\n4:\n∇WAL(x, y)i, ∇WBL(x, y)i\n←Training-and-obtaining-gradients(L(x, y)i) # Obtaining\nGradients of WA and WB\n5:\n∇W L(x, y)i = ∇WBL(x, y)i · ∇WBL(x, y)i\n6: end for\n7: ⊵Step 2: Calculate the IGIA-Matrix corresponding to W.\n8: FW = 1\nt\nPt\ni=1 (∇W L(x, y)i)2\n9: ⊵Step 3: Obtain each layer’s importance score based on the IGIA-Matrix.\n10: for j in 1,...,K do\n11:\nLayerj = PM\nk=1\nPH\nl=1 FWkl\n12: end for\n13: ⊵Step 4: Prune model M based on the layer importance scores.\n14: Mprune ←Prune(M, Layerj, N) # Pruned layer\n15: return Mprune\nB\nTHEORETICAL ANALYSIS\nB.1\nTHEORY OF EARLY GRADIENT ACCUMULATION\nDuring the early stages of fine-tuning, the model rapidly shift from its pre-trained state toward a\nconfiguration better suited for the downstream task. The accumulated gradient information during\nthis phase capture the most informative weight updates, reflecting the critical sub-networks that con-\ntribute significantly to task-specific learning. This phenomenon resonates with the foundation of the\nLottery Ticket Hypothesis (LTH; Frankle & Carbin, 2019), which posits that a randomly initialized,\ndense neural network contains a subnetwork—referred to as a ”winning ticket”—that, when trained\nin isolation, can achieve performance comparable to the original network. LTH-style pruning typi-\ncally involves training the network for a few steps and then identifying important weights based on\ntheir magnitudes. Notably, our proposed method can be interpreted as a gradient-based generaliza-\ntion of this idea. The magnitude of accumulated gradient information over initial fine-tuning steps\nserves as a proxy for parameter sensitivity to task-specific signals. This information aligns closely\nwith the most salient features in the loss landscape, and pruning along the less sensitive directions\npreserves the core learning capacity of the model. Therefore, our gradient-based early pruning\nmethod is consistent with LTH’s principle of identifying winning subnetworks through short peri-\nods of training. This connection supports the effectiveness of our approach in capturing long-term\nparameter importance through early gradient accumulation.\nB.2\nTHEORY OF MERGING IN GRADPRUNER\nIn principle, GradPruner is theoretically equal to the isotropic merging method with adaptive im-\nportance weights. Furthermore, this adaptive importance weighting scheme can also be similarly\napplied to Fisher merging. We will elaborate on these two parts in detail below.\nIsotropic merging with adaptive importance weights.\nTo merge M layers, isotropic merging\nwith per-layer weights (Matena & Raffel, 2022) approximates the posterior distribution of each layer\nusing an isotropic Gaussian whose mean is set to the layer’s parameters. This approach introduces\n14\n"}, {"page": 15, "text": "Published as a conference paper at ICLR 2026\nlayer-specific scalar hyperparameters λi, for i ∈{1, . . . , M}, which can be formally expressed as\nθ∗= arg max\nθ\nM\nX\ni=1\nλi log p(θ | θi, I),\nwhere p(θ | θi, I)denotes the probability density function of the isotropic Gaussian posterior, and\nthe hyperparameters satisfy λi ≥0and PM\ni=1 λi = 1. These hyperparameters govern the relative\nimportance assigned to each layer during the merging process. For instance, if all layers are assumed\nto contribute equally, one may set λi =\n1\nM for all i. However, this approach suffers from two\nprimary limitations: (1) How should the importance weights be adaptively determined when the\nlayers exhibit differing levels of importance? (2) When individual weights within the same layer\npossess varying degrees of importance, how can adaptive weight-specific coefficients be assigned?\nGradPruner introduces an adaptive weighting scheme to solve the above challenges. Specifically,\nconsider two layers to be merged: let Wrdenote the base layer to be retained, and Wmthe layer\nto be merged. To assign appropriate weights λj\nrand λj\nmto the j-th parameter entries W j\nr and W j\nm,\nGradPruner can be formulated as the following adaptive weight assignment function:\n(λj\nr, λj\nm) =\n(\n(0.5, 0.5),\nif\n(FWm)2\nj ≥T\nand\nSign(W j\nm) = Sign(W j\nr ),\n(1.0, 0.0),\notherwise.\n(6)\nwhere Tdenotes a pruning threshold. This formulation implies that if the squared gradient infor-\nmation of a weight in the merged layer exceeds the threshold T and its sign aligns with that of the\ncorresponding weight in the retained layer, both weights are deemed equally important and are as-\nsigned equal coefficients. Otherwise, the weight from the merged layer is considered negligible, and\nonly the retained weight is preserved. GradPruner’s importance criterion simultaneously leverages\n(1) magnitude — quantified by the squared gradient information — and (2) directional consistency\n— captured by the agreement in sign between the two weights — as complementary indicators of\nparameter significance. This design integrates insights from prior work: methods such as Matena\n& Raffel (2022); Daheim et al. (2023) emphasize the utility of gradient magnitude in assessing pa-\nrameter importance, while approaches like Kim et al. (2025); Yadav et al. (2023) demonstrate the\nefficacy of sign consistency as a relevance signal. GradPruner thus unifies both perspectives into a\nsingle adaptive framework. Consequently, the merged weight under isotropic merging with adaptive\nimportance weights is given by θj = λj\nrθj\nr + λj\nmθj\nm.\nIntegrating GradPruner’s Adaptive Weights with Fisher Merging.\nAs outlined above, Grad-\nPruner fundamentally constitutes an adaptive strategy to determine importance weights. In this\nwork, we apply this strategy to isotropic merging. Moreover, we demonstrate that GradPruner’s\nadaptive weighting mechanism can be seamlessly extended to Fisher Merging (FM; Matena &\nRaffel, 2022). Similar to isotropic merging, FM also formulates the merging objective as θ∗=\narg maxθ\nPM\ni=1 λi log p(θ | θi, I),but differs in that it employs the Laplace approximation to the\nposterior p, yielding a Gaussian approximation N(θ, H−1), where H is the Hessian matrix. To ren-\nder computation tractable, FM approximates the Hessian using the diagonal of the Fisher informa-\ntion matrix F. The resulting closed-form expression for the merged parameter is θj =\nPM\ni=1 λiF j\ni θj\ni\nPM\ni=1 λiF j\ni .\nWhen integrating GradPruner with FM, the adaptive weighting scheme modifies only the assignment\nof λi, leaving the computation of the Fisher matrix unchanged. Therefore, in the case of merging\ntwo layers, the final merged weight becomes θj = λj\nrF j\nr θj\nr+λj\nmF j\nmθj\nm\nλj\nrF j\nr +λj\nmF j\nm\n, where (λj\nr, λj\nm)are adaptively\ndetermined via GradPruner as described above.\nC\nSUPPLEMENTARY EXPERIMENTAL SETUP\nC.1\nDATASET DESCRIPTION\n1) PubMedQA (Jin et al., 2019) consists of 19717 scientific publications from the PubMed database\nof diabetes classified into one of three classes, which is in the medical domain.\n2) MedMCQA Pal et al. (2022) is a large-scale, Question Answering (QA) dataset designed to\naddress real-world medical questions. We randomly selected 40,000 samples as the training set.\n15\n"}, {"page": 16, "text": "Published as a conference paper at ICLR 2026\nNumber\nMethod\nPubMedQA\nMedMCQA\nBillSum\nFinGPT\nHellaSwag\nWinoGrande\nARC\nPIQA\nAvg.\nLlama3.1-8B\nParameter-Efficient Fine Tuning (LoRA)\nDense Model\n0.607\n0.633\n0.677\n0.831\n0.959\n0.821\n0.931\n0.893\n0.794\n3\nGradPruner\n0.594\n0.637\n0.659\n0.817\n0.954\n0.812\n0.923\n0.891\n0.786\nw/o Merging\n0.576\n0.599\n0.646\n0.787\n0.902\n0.776\n0.885\n0.857\n0.755\n2\nGradPruner\n0.597\n0.637\n0.673\n0.818\n0.958\n0.818\n0.928\n0.886\n0.789\nw/o Merging\n0.587\n0.601\n0.656\n0.808\n0.931\n0.802\n0.899\n0.858\n0.768\n1\nGradPruner\n0.605\n0.632\n0.675\n0.824\n0.958\n0.825\n0.933\n0.885\n0.792\nw/o Merging\n0.593\n0.616\n0.665\n0.816\n0.944\n0.811\n0.919\n0.877\n0.779\nMistral-7B\nFull Fine-Tuning (FFT)\nDense Model\n0.591\n0.583\n0.684\n0.862\n0.841\n0.878\n0.905\n0.903\n0.781\n2\nGradPruner\n0.586\n0.568\n0.670\n0.846\n0.840\n0.860\n0.895\n0.897\n0.770\nw/o Merging\n0.551\n0.568\n0.646\n0.822\n0.811\n0.805\n0.856\n0.863\n0.740\n1\nGradPruner\n0.585\n0.578\n0.685\n0.860\n0.840\n0.872\n0.902\n0.897\n0.777\nw/o Merging\n0.579\n0.567\n0.653\n0.847\n0.826\n0.853\n0.877\n0.878\n0.760\nParameter-Efficient Fine Tuning (LoRA)\nDense Model\n0.607\n0.565\n0.681\n0.853\n0.963\n0.846\n0.909\n0.896\n0.790\n2\nGradPruner\n0.588\n0.565\n0.659\n0.840\n0.963\n0.832\n0.893\n0.896\n0.780\nw/o Merging\n0.561\n0.540\n0.632\n0.827\n0.911\n0.799\n0.858\n0.860\n0.749\n1\nGradPruner\n0.606\n0.566\n0.677\n0.849\n0.960\n0.846\n0.905\n0.902\n0.789\nw/o Merging\n0.590\n0.544\n0.657\n0.826\n0.933\n0.819\n0.875\n0.877\n0.765\nTable 4:\nExperimental results on the impact of the number of merged layers on downstream task\naccuracy (LoRA fine-tuning).\n3) BillSum (Kornilova & Eidelman, 2019) is the first dataset for summarization of US Congressional\nand California state bills, which is in the financial domain.\n4) fingpt-sentiment-train Yang et al., 2023 is a financial sentiment analysis question-answering\ndataset containing 76,000 samples. We randomly selected 40,000 samples as the training set.\n5) HellaSwag (Zellers et al., 2019) is a challenging dataset for evaluating commonsense NLI that\nis especially hard for state-of-the-art models, though its questions are trivial for humans (¿95%\naccuracy).\n6) WinoGrande (Sakaguchi et al., 2021)consists of 10.2k training samples for cloze tests, measuring\nperformance with accuracy.\n7) ARC (ARC-Easy) (Clark et al., 2018) is a multiple-choice question-answering benchmark de-\nsigned to test the model’s ability to reason about scientific knowledge.\n8) PIQA (Bisk et al., 2020) is a physical commonsense reasoning dataset that is designed to test the\nmodel’s ability to build, craft, or manipulate objects using everyday physical knowledge.\nC.2\nTRAINING DETAILS\nThere are two ways of fine-tuning. One is Full Fine-Tuning (FFT). The other is Parameter-Efficient\nFine-Tuning, and we adopt the LoRA method. During training, we set the learning rate to 1e-5 and\nthe batch size to 64. Each dataset was trained for 3 epochs. The AdamW optimizer was used for fine-\ntuning. We employed SWIFT as the training platform and vLLM for inference. We set the sparsity\nratio during merging to 80% and pruned 13 layers (approximately 40% of the total parameters),\nthree of which applied the layer merging technique.\nD\nSUPPLEMENTARY ABLATION STUDY\nPerform Only the Layer Pruning in GradPruner: Figure 8 presents the results of using only the\nlayer pruning strategy. For Mistral-7B, pruning up to 11 layers has minimal impact on downstream\ntask accuracy. However, beyond 11 layers, accuracy begins to decline significantly, as increasingly\ncritical layers are removed.\nThe Impact of the Number of Merged Layers on Accuracy: Table 4 demonstrates the impact on\ndownstream task accuracy after incorporating the layer merging algorithm on top of layer pruning.\n16\n"}, {"page": 17, "text": "Published as a conference paper at ICLR 2026\nFrom the table, we can observe that for both models, whether fine-tuned using FFT or LoRA, layer\nmerging significantly improves accuracy. For Mistral-7B, merging 1 layer after pruning 11 layers\naligns its accuracy with the dense model. Although merging 2 layers introduces some accuracy\ndifferences, the model still maintains relatively high performance.\nReplacing Layer Pruning with Kernel Pruning: Readers may wonder why GradPruner focuses\non pruning layers rather than adopting a finer-grained approach, such as pruning kernels (rows or\ncolumns) within parameter matrices. To address this question, we applied pruning to the Hidden\nSize (HZ) within the GradPruner framework. Specifically, we used IGIA-Matrix to compute the\nimportance of each hidden size, pruned the less important parts, and merged the pruned hidden sizes\nwith the unpruned ones. The pruning rate was set to 40%. The experimental results are shown\nin Figure 9 below. Compared to layer pruning, using the finer-grained kernel pruning leads to a\nnoticeable decline in accuracy. We believe this phenomenon is mainly caused by two reasons: 1)\nThe importance of layers may take precedence over that of kernels, and pruning kernels in critical\nlayers could lead to a drop in accuracy. 2) Pruning hidden sizes requires merging a larger number of\nhidden sizes, which is less conducive to maintaining accuracy.\nFigure 8: Experimental results of performing\nOnly the layer pruning in GradPruner.\nFigure 9: Experimental results of replacing\nlayer pruning with kernel pruning in Grad-\nPruner.\nWe report the average accuracy\nacross eight datasets.\nLayer-importance estimation for tasks with varying gradient noise: To further explore the dif-\nferent behaviors of layer importance estimation for downstream tasks under various noise levels, we\nsupplemented the following experiments: Based on the WinoGrande dataset (original size 10.2k),\nwe randomly sampled 1%, 10%, 30%, and 100% of its data as training sets, systematically evaluating\nthe different behaviors of layer importance estimation under different training data sizes (different\ngradient variances), paying particular attention to whether early gradient signals remain reliable with\nvery small amounts of data. The relevant results are shown in Tab. 5.\nTable 5: Ablation experiments with different training data sizes were conducted on the WinoGrande\ndataset, based on Llama 3.1-8B (Full Fine-tuning). The Dense Model represents the accuracy of the\nunpruned model after fine-tuning on downstream tasks.\nTraining data ratio and size\n1% (102)\n10% (1.02k)\n30% (3.06k)\n100% (10.2k)\nDense Model\n0.853\n0.863\n0.866\n0.868\nGradPruner (ours)\n0.842\n0.857\n0.862\n0.861\nExperiments show that even with a significant reduction in training data, our layer importance esti-\nmation maintains good stability. Only when the data volume is extremely low (e.g., only about 100\nsamples) does the model performance fluctuate slightly, but the overall trend remains robust. This\nindicates that our method’s reliance on early-step gradients is reasonable under typical data scales.\nPerformance across Sparsity Ratio: To comprehensively evaluate the relative advantages of Grad-\nPruner across different sparsity levels, Tab. 6 extends the main experiment at 40% sparsity by addi-\ntional comparisons against baseline methods at 30% and 20% sparsity\n17\n"}, {"page": 18, "text": "Published as a conference paper at ICLR 2026\nWe did not consider sparsity levels higher than 40% because, under which settings most meth-\nods—including GradPruner—exhibit significant performance degradation. Existing baseline meth-\nods already struggle to maintain reasonable accuracy at 40% sparsity, and increasing sparsity further\nwould exacerbate this performance drop, rendering comparisons at higher sparsity levels uninfor-\nmative.\nTable 6: Comparative experiments between our method and other approaches under different spar-\nsity levels, using the Llama3.1-8B model with full fine-tuning on the HellaSwag dataset. ”Dense\nModel” denotes the accuracy of the unpruned model after fine-tuning on the downstream task.\nMethods\nDense Model\nLLMPruner\nLaco\nMINITRON\nSAT\nGradPruner (ours)\nSparsity Ratio (30%)\n0.943\n0.916\n0.923\n0.923\n0.934\n0.940\nSparsity ratio(20%)\n0.943\n0.927\n0.936\n0.929\n0.940\n0.942\nThe experimental results in Tab. 6 show the following:\nAt 30% sparsity, GradPruner continues to maintain a clear advantage over all baseline methods. At\n20% sparsity (i.e., under milder pruning), some methods—such as Laco and SAT—also demonstrate\nstrong performance; however, our method remains competitive and exhibits consistently robust over-\nall performance. These findings indicate that the advantages of GradPruner are not limited to mod-\nerate sparsity levels (e.g., 40%) and can be generalized to lighter pruning settings, demonstrating its\neffectiveness and robustness across a range of sparsity regimes.\nPerformance across Model Scales:To verify the applicability of GradPruner across different model\nscales, Tab. 7 shows experiments on both smaller (approximately 1B) and larger (approximately\n13B) models, using Llama-3.2-1B and Llama-2-13B as testbeds, respectively. The experiments\ndemonstrate that our method performs effectively across both model scales: it not only maintains\nstrong performance on the larger Llama-2-13B but also exhibits even more pronounced advantages\non the smaller Llama-3.2-1B. This indicates that GradPruner’s pruning mechanism is highly scal-\nable, with its effectiveness consistently extending from 1B-scale models to those exceeding 10B\nparameters, showcasing strong generality.\nTable 7: We evaluate GradPruner on models of different sizes: for Llama-3.2-1B (originally 16\nlayers), we prune 6 layers; for Llama-2-13B (originally 40 layers), we prune 16 layers. ”Dense\nModel” denotes the accuracy of the unpruned model after fine-tuning on the downstream task.\nDatasets\nMedMCQA\nFinGPT\nHellaSwag\nLlama-3.2-1B(Dense Model)\n0.434\n0.543\n0.584\nLlama-3.2-1B(GradPruner)\n0.434\n0.538\n0.585\nLlama-2-13B(Dense Model)\n0.542\n0.806\n0.907\nLlama-2-13B(GradPruner)\n0.532\n0.798\n0.894\nThe Impact of the Number of Initial Fine-tuning Steps: We conducted ablation experiments\non the HellaSwag dataset with different gradient accumulation steps. The steps we selected were\n0.02%, 0.05%, 0.08%, and 1%. The experimental results are shown in Tab. 8. Experimental re-\nsults show that our method has certain requirements for the number of gradient accumulation steps;\nperformance will decrease when the number is less than 0.05%, but this requirement is not strin-\ngent—experiments show that only about 1% of the training steps are needed to achieve good align-\nment results and stable performance.\nTable 8: Ablation experiments with different gradient accumulation steps were conducted on the\nHellaSwag dataset, based on Llama 3.1-8B (Full Fine-tuning). The Dense Model represents the\naccuracy of the unpruned model after fine-tuning on downstream tasks.\n0.02%\n0.05%\n0.08%\n1%\nDense Model\n0.943\n0.943\n0.943\n0.943\nGradPruner (ours)\n0.911\n0.929\n0.939\n0.939\n18\n"}, {"page": 19, "text": "Published as a conference paper at ICLR 2026\nTo check the effectiveness with even fewer gradient accumulation steps, we conducted ablation\nexperiments on the HellaSwag dataset with different gradient accumulation steps. The steps we\nselected were 0.02%, 0.05%, 0.08%, and 1%. The experimental results are shown in Tab. 9.\nExperimental results show that our method has certain requirements for the number of gradient accu-\nmulation steps; performance will decrease when the number is less than 0.05%, but this requirement\nis not stringent—experiments show that only about 1% of the training steps are needed to achieve\ngood alignment results and stable performance.\nTable 9: Ablation experiments with different gradient accumulation steps were conducted on the\nHellaSwag dataset using the Llama 3.1-8B model. The Dense Model represents the accuracy of the\nunpruned model after fine-tuning on downstream tasks.\n0.02%\n0.05%\n0.08%\n1%\nDense Model\n0.943\n0.943\n0.943\n0.943\nGradPruner (ours)\n0.911\n0.929\n0.939\n0.939\nD.1\nSTABILITY OF EARLY-STEP GRADIENTS\nTo further explore the stability of early-step gradients, we added the following experiments: Based\non the WinoGrande dataset (original size 10.2k), we randomly sampled 1%, 10%, 30%, and 100% of\nits data as training sets, systematically evaluating the stability of early-step gradients under different\ntraining data sizes, paying particular attention to whether the early gradient signals remain reliable\nwith extremely small datasets. The relevant results are shown in the Tab. 10.\nExperiments show that even with a significant reduction in training data, layer importance estimation\nbased on early-step gradients maintains good stability. Only when the amount of data is extremely\nlow (e.g., only about 100 samples) does the model performance fluctuate slightly, but the overall\ntrend remains robust. This indicates that our method’s reliance on early-step gradients is reasonable\nunder typical data scales.\nTable 10: Ablation experiments with different training data sizes were conducted on the WinoGrande\ndataset, based on Llama 3.1-8B (Full Fine-tuning). The Dense Model represents the accuracy of the\nunpruned model after fine-tuning on downstream tasks.\ntraining data ration and size\n1% (102)\n10% (1.02k)\n30% (3.06k)\n100% (10.2k)\nDense Model\n0.853\n0.863\n0.866\n0.868\nGradPruner (ours)\n0.842\n0.857\n0.862\n0.861\nD.2\nEFFICIENCY FROM A FLOPS PERSPECTIVE\nTab. 11 above shows a comparison of the efficiency of the original llama3.1-8B model and the\nmodel pruned using GradPruner. During testing, we used an input length of 128 tokens. The results\nshow that the pruned model reduced the number of parameters by approximately 38%, and saved\napproximately 40% in FLOPs and MACs.\nD.3\nGENERALIZATION IN CROSS-DATASET SCENARIOS\nTo verify the generalization ability of the method in cross-dataset scenarios, we supplemented it\nwith cross-dataset evaluation experiments within the domain: pruning and fine-tuning were per-\nTable 11: Two metrics, FLOPs and MACs, represent the efficiency of GradPruner\nllama3.1-8B\nFLOPs (TFLOPS)\nMACs (GMACs)\nParams (B)\noriginal\n1.79\n893.35\n7.5\npruned\n1.06\n530.43\n4.67\n19\n"}, {"page": 20, "text": "Published as a conference paper at ICLR 2026\nformed using the original medical domain training set MedMCQA from the paper, and testing was\nconducted on the medical subset clinical knowledge in MMLU. The results are shown in Tab. 12.\nDespite a slight performance drop compared to Dense models in cross-dataset settings (i.e., train-\ning and testing distributions are not perfectly consistent), GradPruner still maintains a significant\naccuracy advantage over other existing pruning methods.\nTable 12: Cross-dataset evaluations in MedMCQA, based on Llama 3.1-8B (Full Fine-tuning).\nTraining data is MedMCQA, and test data is clinical knowledge. The Dense Model represents\nthe accuracy of the unpruned model after fine-tuning on downstream tasks.\nMethods\nDense Model\nLLMPruner\nLaco\nMINITRON\nSAT\nGradPruner (ours)\n0.735\n0.688\n0.711\n0.703\n0.709\n0.717\nD.4\nGENERALIZATION WITH OTHER ADAPTER METHODS\nTo verify the stability of the IGIA matrix under different adapter methods, we conducted additional\nexperiments: we replaced the strategy in GradPruner that originally used LoRA fine-tuning to eval-\nuate the importance of pre-trained parameters with QLoRA and DoRA, respectively. The relevant\nresults are summarized in Tab. 13. As the experimental results show, the performance of DoRA is\ncomparable to LoRA, with almost no significant difference; however, a slight decrease in accuracy\noccurs when using QLoRA. This phenomenon can be attributed to the technical characteristics of\nthe two methods: DoRA decomposes the pre-trained weights, achieving an optimization behavior\ncloser to full parameter fine-tuning than LoRA, thus better evaluating the importance of pre-trained\nparameters; in contrast, QLoRA introduces quantization operations on the pre-trained weights on\ntop of LoRA, which introduces quantization noise, leading to some bias in the estimated parameter\nimportance.\nTable 13: Stability tests were conducted on the HellaSwag dataset using different adapters based on\nLlama 3.1-8B (Full Fine-tuning). The Dense Model represents the accuracy of the unpruned model\nafter fine-tuning on downstream tasks.\nLoRA\nDoRA\nQLoRA\nDense Model\n0.943\n0.943\n0.943\nGradPruner (ours)\n0.939\n0.941\n0.925\n20\n"}]}