{"doc_id": "arxiv:2511.12008", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.12008.pdf", "meta": {"doc_id": "arxiv:2511.12008", "source": "arxiv", "arxiv_id": "2511.12008", "title": "Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models", "authors": ["Yunqi Hong", "Johnson Kao", "Liam Edwards", "Nein-Tzu Liu", "Chung-Yen Huang", "Alex Oliveira-Kowaleski", "Cho-Jui Hsieh", "Neil Y. C. Lin"], "published": "2025-11-15T03:06:59Z", "updated": "2025-11-15T03:06:59Z", "summary": "AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.12008v1", "url_pdf": "https://arxiv.org/pdf/2511.12008.pdf", "meta_path": "data/raw/arxiv/meta/2511.12008.json", "sha256": "f5dd8e23694b7cb8ca6254a89580ea9bfc841cb03b3e98f9de81ead3489b4bc6", "status": "ok", "fetched_at": "2026-02-18T02:27:00.605436+00:00"}, "pages": [{"page": 1, "text": "Adaptive Diagnostic Reasoning Framework for\nPathology with Multimodal Large Language Models\nYunqi Hong1, Johnson Kao1, Liam Edwards2, Nein-Tzu Liu3, Chung-Yen Huang4, Alex\nOliveira-Kowaleski5, Cho-Jui Hsieh1*, and Neil Y.C. Lin2, 6, 7, 8*\n1Computer Science Department, University of California, Los Angeles, CA, USA\n2Mechanical and Aerospace Engineering Department, University of California, Los Angeles, CA, USA\n3Department of Pathology, Tri-Service General Hospital, National Defense Medical Center, Taipei, Taiwan\n4Department of Pathology, National Taiwan University Hospital, Taipei, Taiwan\n5Department of Pathology, David Geffen School of Medicine, University of California, Los Angeles, CA, USA\n6Bioengineering Department, University of California, Los Angeles, CA, USA\n7Institute for Quantitative and Computational Biosciences, University of California, CA, USA\n8Jonsson Comprehensive Cancer Center, University of California, Los Angeles, CA, USA\n*chohsieh@cs.ucla.edu, neillin@g.ucla.edu\nABSTRACT\nAI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that\ninform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed\nto audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning\nparadigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic\nreasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands\npathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small\nlabeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate\ndatasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic\naccuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and\ndemonstrates a generalizable path toward evidence-linked interpretation.\nIntroduction\nOver the past two decades, diagnostic pathology has been transformed by increasingly sophisticated artificial intelligence (AI)\nand computer vision methods1–3. Early efforts focused on extracting handcrafted features such as texture, color, and shape,\nbut today deep neural networks dominate the field, powering a wide range of clinical and research applications4–6. These\nadvances are reshaping pathology laboratories on multiple fronts. Automated systems can now scan digitized histopathological\nslides at scale, rapidly identifying tumors and enabling pathologists to focus on urgent cases7,8. Algorithms that quantify\nimmunohistochemical (IHC) markers further improve prognostic accuracy and reproducibility9,10. At the same time, AI-\npowered quality control dashboards monitor staining variability, providing near real-time feedback to maintain laboratory\nstandards11–13. Together, these innovations are moving pathology from subjective interpretation to algorithmic precision,\nredefining the pathologist’s role in the future of medicine.\nDespite substantial progress in classification accuracy and diagnostic efficiency, most current models remain black boxes\nthat cannot explain predictions in a way that reflects the reasoning of human experts14,15. In clinical practice, trust depends\non explanations that align with evidence-based reasoning, both for pathologists and for regulatory oversight16,17. Traditional\ninterpretability methods such as attention maps18,19, concept-level explanations20,21, and data attributions22 provide only partial\npost-hoc insights. More recent approaches in biomedical imaging23,24 often demand densely annotated datasets or generate\ncoarse saliency maps that lack the granularity needed for auditability and clinical trust25. Without transparent and audit-ready\njustifications, even highly accurate AI models cannot be reliably integrated into pathology, where every diagnostic call must be\ndefensible to tumor boards and regulatory bodies.\nIn parallel, recent breakthroughs in Multimodal Large Language Models (MLLMs)26–32 demonstrate strong image un-\nderstanding and emerging reasoning skills. LLM prompting and in-context learning23,33–39 can enable medical image\ndiagnosis40–42. Bu many current systems often provide explanations that are unstructured, weakly grounded in morphological\nfeatures, or inconsistent across runs43. Such limitations reduce their suitability for clinical audit and trust. Even so, MLLMs\narXiv:2511.12008v1  [cs.AI]  15 Nov 2025\n"}, {"page": 2, "text": "Prompt scoring\n(diversity or accuracy)\nHypothesizing fail reasons\n…\nFeedbacks\n…\nNewly hypothesized prompts (criteria)\nHypothesizing new prompts (criteria)\nDescription\ngeneration\nImage\ndescription\nDiagnosis:\nBenign / cancer\nLLM\nUnseen\nimage\nModel\ndeployment\nPhase 1 learning:\nAnalysis diversity\nPhase 2 learning:\nDiagnosis accuracy\nOptimized prompt\n(diagnosis criteria)\nLabeled\nimages\nDiagnosis\nlearning\n(A)\n(B)\nPathology images\n…\nDescription generation\n…\nDescriptions\nDiagnosis\n…\n…\nTop k prompts with highest scores \n…\nErroneous examples\nFigure 1. Framework of RECAP-PATH. (A) Overview of the RECAP-PATH learning framework and deployment pipeline.\nUsing a small set of labeled pathology images (left), RECAP-PATH conducts a two-phase diagnostic learning process that\nyields an optimized prompt encapsulating the diagnosis criteria. During inference on unseen pathology images (right), the\nmodel generates detailed image descriptions guided by the optimized criteria and then produces classification predictions\ninformed by both visual features and textual descriptions. (B) Schematic of the automatic prompt refinement workflow. In each\niteration, RECAP-PATH identifies error cases, prompts the model to reflect on failure modes, and generates revised prompts\naimed at enhancing diagnostic accuracy and improving human-readable diagnostic rationales. Through this iterative,\nerror-driven refinement, the framework produces prompts that are both clinically meaningful and performance-optimized.\nRepresentative examples of such prompts are shown in Fig. S1.\n2/15\n"}, {"page": 3, "text": "remain a compelling foundation for addressing the central challenge of trustworthy AI in pathology, as recent models support\nhigher-resolution visual inputs and longer multimodal context that can be harnessed to structure stepwise, feature-grounded\nreasoning26,28.\nBuilding on these recent advances, we propose RECAP-PATH (REasoning and Classification via Automated Prompting in\nPATHology images), an interpretable framework that leverages MLLMs to deliver accurate diagnoses with pathologist-aligned\njustifications. As illustrated in Fig. 1A, the system articulates the morphologic evidence supporting each diagnostic decision\nand jointly evaluates this evidence with the image to produce the final prediction. At its core is a “think-and-speak” stage,\nwhere an MLLM, guided by a description-generation prompt enriched with histopathological features, generates a detailed\naccount of the image and the diagnostic significance of each feature. To identify diagnosis guidelines that yield accurate\nand task-specific rationales, we iteratively refine the description-generation prompt using diagnostic feedback derived from\ndiscrepancies between predictions and ground truth labels, leveraging the error reflection ability discovered in recent prompt\noptimization work44–47. This refinement progressively enhances clarity and diagnostic relevance without finetuning model\nweights. The framework is highly efficient, requiring as few as 100 labeled examples, no white-box access, and no additional\nMLLM retraining, while operating entirely through standard API calls. To the best of our knowledge, this is the first approach\nto jointly optimize visual description generation with prompt-based optimization to deliver audit-ready, human-interpretable\ndiagnostic outputs.\nResults\nTwo-Phase Prompt Optimization Enables Accurate and Interpretable Triage of Invasive Carcinoma\nTo implement RECAP-PATH’s adaptive reasoning capabilities, we introduce an iterative two-phase description-based prompt\noptimization algorithm that enables an MLLM to adapt and iteratively refine diagnostic rationales through feedback derived\nfrom discrepancies between predictions and ground-truth labels expressed in natural language. As illustrated in Fig. 1B, this\nprompt optimization workflow is designed to enhance both diagnostic accuracy and interpretability by guiding the MLLM\nto generate structured, morphologically grounded, pathologist-style descriptions. The process begins with baseline prompts\nthat provide minimal diagnostic guidance, instructing the MLLM to describe visual observations of tissue images and produce\ncorresponding diagnostic predictions. In each iteration, the model reflects on misclassifications, identifies error patterns and\nlikely sources of diagnostic confusion, and proposes strategies for improvement. It then generates a diverse set of candidate\nprompts embedding domain-specific morphological criteria and explicit reasoning steps aligned with expert pathology practice.\nCandidate prompts are evaluated against labeled data based on diagnostic accuracy, with the top three retained for further\nrefinement. Focusing on the optimization phase for clarity, we provide a representative example of the prompts used to guide\neach step of the refinement process, spanning description generation, classification, error collection, feedback synthesis, and\nguideline formulation, in Fig. S1.\nOur two-phase description-optimized prompting strategy is designed to improve diagnostic accuracy while fostering inter-\npretable, expert-aligned reasoning. Unlike standard automatic prompt optimization methods that focus solely on performance\nmetrics44,45, our approach leverages the model’s own diagnostic errors to guide prompt evolution and encourage diverse\nexplanatory strategies. To achieve this, the process is divided into two complementary phases. Phase 1 (Diversification)\nprompts the MLLM to expand the semantic and conceptual scope of its diagnostic explanations. The model is encouraged to\nintroduce novel terminology, explore alternative morphologic perspectives, and apply varied interpretive lenses. This phase\naims to maximize interpretive diversity, enabling the exploration of multiple diagnostic reasoning pathways without being\nconstrained by immediate accuracy. Phase 2 (Optimization) then refines this pool of prompts by selecting those that enhance\ndiagnostic performance while maintaining clinical coherence. Prompts that do not contribute meaningfully to accuracy are\ndiscarded, while those yielding the most effective and interpretable outputs are iteratively improved. Together, these phases\ncreate a structured pipeline that first explores diverse reasoning strategies and then converges on clinically high-performing\nexplanations.\nTo demonstrate the capabilities of our framework, we first evaluate it on a clinically critical task: binary classification of\nnormal versus invasive carcinoma in breast pathology images, a foundational distinction for diagnosis and prognosis. Consistent\nwith our design principles, the two-phase optimization process exhibits distinct learning dynamics in each phase (Fig. 2A). In\nPhase 1, classification accuracy declines slightly as the model explores a wider range of diagnostic reasoning strategies that\nincrease diversity but are not yet optimized for performance. Once Phase 2 begins, accuracy rises rapidly, with substantial gains\nappearing within the first few iterations. Convergence is typically reached in about six rounds, and the final prediction accuracy\nexceeds 0.9. This nonlinear trajectory indicates the effectiveness of decoupling interpretive exploration from performance\noptimization, resulting in a set of prompts that are both diagnostically accurate and clinically audit-ready.\nTo evaluate the effectiveness of prompt diversification, we quantified terminology diversity across successive rounds of\noptimization. Diversity was measured using two criteria: (i) the number of biologically relevant terms and (ii) the number\nof words unique to the current prompt relative to all previously generated prompts. As shown in Fig.2 B, diversity increased\n3/15\n"}, {"page": 4, "text": "Optimized prompt (diagnosis criteria):\n… as an expert pathologist, tasked with differentiating breast tissue samples as either \nNormal or Invasive Carcinoma ... The definitive diagnostic criterion for Invasive \nCarcinoma is ... tumor cell invasion past the basement membrane and into ... Stroma...\nAssess the following features, weighted by importance:\n1. Invasion: Is there ... tumor cells infiltrating the established stroma, extending \nbeyond the normal boundaries of ducts or lobules? ... clear and consistent infiltration \nof the stroma by tumor cells ... Cohesive groups or larger nests of cells infiltrating the \nstroma...\n2. Architecture: ... Is the typical lobular arrangement maintained? Are ducts and acini \nregularly sized and spaced? Be impartial - minor irregularities can be normal...\n- Architectural Disruption: Loss of lobular structure, irregular duct/acinar spacing, \ndisorganized cell arrangements.\n3. Nuclear Morphology: ... Are they uniform in size and shape, or are there significant \nvariations (pleomorphism)? … Normal Nuclei: Uniform, fine chromatin, inconspicuous \nnucleoli. Atypical Nuclei: Pleomorphism, coarse chromatin, prominent nucleoli.\n4. Stroma: ... Is it loose and cellular, or dense and collagenous (desmoplastic)? Keep \nin mind that desmoplasia can be seen in both benign and malignant conditions. Normal \nStroma: Loose, cellular connective tissue. Desmoplastic Stroma: Dense, collagenous \nstroma.\n5. Mitotic Count: Are there many cells undergoing division (mitosis)? ...\nFeatures Suggestive of Benign Conditions:\n•\nWell-defined lobular architecture\n•\nUniform cell populations with minimal nuclear atypia\n•\nIntact basement membranes\n•\nAbsence of single-cell invasion ...\nInitial prompt:\nWhat attributes does this pathology image show? Try to generate highly \ndiscriminative attributes that can help identify the type of the tumor.\nDiversified & Optimized\n(A)\n(B)\n(D)\n(E)\n(F)\nDiversity score\nPrediction accuracy\n0.0\n0.4\n0.8\n1.2\nIntermediate\nNormal\nInvasive carcinoma\nUMAP 1\nUMAP 2\nOptimized\nNormal\nInvasive carcinoma\nUMAP 1\nUMAP 2\nInitial\nNormal\nInvasive carcinoma\nUMAP 1\nUMAP 2\n0.45\n(n=17)\n0.55\n(n=21)\n0.0\n(n=0)\n1.0\n(n=42)\nInitial prompt\n0.11\n(n=4)\n0.89\n(n=34)\n0.0\n(n=0)\n1.0\n(n=42)\nDiversified\n0.95\n(n=36)\n0.05\n(n=2)\n0.10\n(n=4)\n0.90\n(n=38)\nOptimized\n0.34\n(n=13)\n0.66\n(n=25)\n0.0\n(n=0)\n1.0\n(n=42)\nZero-shot\nNo Phase 1\nPhase 2\n(C)\nDiversity score\nNormalized diversity\n0\n1\n2\n3\n4\n5\n6\n0.0\n0.4\n0.8\n1.2\nRound number\nNormalized diversity\nRound number\nPhase 1:\nDiversity\nPhase 2:\nAccuracy\n12\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11\n1.0\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nRound number\nPhase 1:\nDiversity\nPhase 2:\nAccuracy\n12\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11\nFigure 2. Learning dynamics and prompt evolution in RECAP-PATH. (A) Prediction accuracy over learning iterations.\nAccuracy decreases slightly during Phase 1 (diversification) as the model explores a broader range of diagnostic reasoning\nstrategies. In Phase 2 (accuracy), accuracy increases rapidly, with substantial improvements achieved within a few iterations\nand convergence around six rounds. (B) Prompt diversity over learning iterations. Diversity steadily increases during Phase 1\nas the goal is more diverse reasoning strategies. During Phase 2, diversity decreases slightly as prompts are refined for accuracy\nbut remains substantially higher than the starting point. (C) Impact of the diversification phase. Incorporating Phase 1 leads to\nsignificantly greater lexical and conceptual diversity in the final prompt compared to training without diversification. (D)\nEvolution of the test confusion matrix. The model progresses from an initial zero-shot bias toward one category to a\nwell-balanced, optimized diagnostic performance. (E) Example of prompt evolution. The initial seed prompt is generic and\nsimple, while the final optimized prompt reflects a structured, clinically meaningful diagnostic framework. (F) UMAP\nvisualization of the description embeddings. Initially, descriptions for the two classes overlap with poor separability. After\noptimization, descriptions form two well-separated clusters, demonstrating semantic disentanglement aligned with diagnostic\ncategories.\n4/15\n"}, {"page": 5, "text": "steadily during Phase 1, reflecting a successful expansion of the diagnostic reasoning space. In Phase 2, a modest reduction in\ndiversity was observed as the optimization process shifted focus toward maximizing diagnostic accuracy. However, the final\nset of prompts maintained a substantially higher level of lexical and conceptual diversity, more than twice that of prompts\ngenerated without the diversification phase (Fig.2 C). While single-phase optimization (focused solely on accuracy) achieved\ncomparable classification performance, the resulting diagnostic guidelines and descriptions were noticeably less structured and\nlacked the depth provided by diversified prompts (Fig. S2). These results show that our framework preserves diverse diagnostic\nlanguage while integrating it into high-performing prompts, supporting robustness against overfitting or narrow reasoning.\nPrompt evolution drives bias rebalancing and emergent diagnostic structure\nTo assess the impact of prompt refinement on diagnostic behavior, we tracked changes in the model’s confusion matrix across\nboth optimization phases (Fig. 2D). In the initial zero-shot setting, the MLLM exhibited a pronounced bias toward invasive\ncarcinoma classifications, likely reflecting distributional priors from its pretraining data. Introducing a generic seed prompt\nslightly mitigated this bias, but it was only after completing the full two-phase prompt optimization that the confusion matrix\nreflected well-calibrated and accurate predictions. This progression shows that our framework mitigates model bias by guiding\ndiagnostics toward evidence-based decisions. Moreover, the two-phase optimized diagnostic guidelines enhanced consistency,\nreducing classification variance by nearly half compared to the initial seed prompt (Fig. S3). This improved consistency was\nalso evident in the corresponding image descriptions, which demonstrated substantially greater coherence and depth (Fig. S3).\nWe next evaluated whether our framework enhances diagnostic reasoning by analyzing the content and structure of the\noptimized prompt guiding the MLLM’s predictions (Fig. 2E). The initial baseline prompt, which only requested discriminative\ncharacteristics, gradually evolved into well-structured and pathologically coherent guidelines through iterative optimization. It\nconsistently invoked five hallmark dimensions of histopathologic assessment: invasion phenotype, tissue architecture, nuclear\nmorphology, stromal context, and mitotic activity. Notably, these clinically salient criteria emerged without explicit supervision\nor handcrafted rule encoding. Together, these findings demonstrate that our two-phase description-optimized prompting\nframework induces reasoning patterns that mirror established pathology workflows, reinforcing its potential as an interpretable\ndiagnostic tool.\nTo investigate how our prompt refinement method shapes the MLLM’s alignment with diagnostic tasks, we analyzed the text\nembeddings of MLLM-generated image descriptions and visualized their distribution using Uniform Manifold Approximation\nand Projection (UMAP) (Fig. 2F). Descriptions generated from the initial baseline prompts produced highly overlapping\nclusters for normal tissue and invasive carcinoma, indicating limited discriminative capacity. As the prompts were iteratively\nrefined, the embedding space progressively disentangled, ultimately forming two well-separated clusters corresponding to\nbenign and malignant phenotypes. Interestingly, early descriptions of invasive carcinoma briefly diverged into two distinct\nsemantic trajectories (intermediate panel in Fig. 2F) before merging into a single coherent group. This transient bifurcation\nsuggests that the optimization process not only enhances semantic clarity but also captures intermediate variability on the path\ntoward a unified diagnostic narrative. Taken together, these findings indicate that interpretability in our framework emerges\nnaturally from the prompt optimization process rather than being imposed as a post hoc addition.\nMLLM descriptions enable precise histological interpretation\nAlthough the two-phase description-optimized prompting process enhances interpretability and diagnostic performance,\nachieving clinically meaningful reasoning requires alignment with expert judgment. To address this, we integrated human-AI\ninteraction into the optimization loop by soliciting feedback from three board-certified pathologists (Fig. 3A). These experts\nperformed blinded evaluations of LLM-generated image descriptions, covering both normal and invasive carcinoma cases at\ndifferent stages of the optimization process (example questionnaire shown in Fig. S4). Each description was rated for precision\nand histopathological accuracy, and these assessments directly informed subsequent prompt revisions (example feedback shown\nin Fig. S5). The MLLM was then instructed to incorporate expert-derived principles to improve diagnostic reliability. This\nprocess ensured that the model’s reasoning was not only internally consistent but also aligned with established clinical standards.\nAs shown in Fig. 3B, expert-guided optimization increased clinical coherence ratings of image descriptions by nearly 20%,\ndemonstrating the importance of domain expertise in bridging the gap between algorithmic inference and medical practice.\nA central advancement of our framework is its ability to provide case-specific transparency through the MLLM’s image-level\ndescriptions, which explicitly articulate the visual features underlying each diagnostic decision. As shown in Fig. 3C, for a\nrepresentative normal tissue sample the model identifies hallmark features of benign histology, including well-organized tubular\narchitecture, uniform glandular cell size, absence of prominent nucleoli, and abundant cytoplasm. These findings are fully\nconsistent with expert pathological criteria for this image. For invasive carcinoma, the descriptions highlight malignant features\nsuch as infiltrative growth patterns, nuclear pleomorphism, prominent nucleoli, and stromal invasion. These outputs parallel\nexpert annotations, showing that the model justifies its predictions in a clinically intelligible way. We reviewed misclassified\ncases and found that most errors arose from limited field of view or imaging artifacts that obscured key morphologic features\n(Fig. S6). These failures indicate the framework’s sensitivity to input image quality and suggest that artifact detection or\n5/15\n"}, {"page": 6, "text": "RECAP-PATH description: normal\nNormal\no\nTissue\nOrganization:\nThe\ntissue\narchitecture\nis\nwell-organized,\ndisplaying regular and distinct tubular structures...\no\nCellular Features: The cells lining the tubules are uniform in size and\nshape ...\no\nNuclear Morphology: The nuclei are round, with finely dispersed\nchromatin\nand\nno\nprominent\nnucleoli.\nThere\nis\nno\nevidence\nof\nhyperchromasia, pleomorphism, or increased nuclear-to-cytoplasmic ratio.\no\nCytoplasmic\nCharacteristics:\nThe\ncytoplasm\nis\nabundant\nand\neosinophilic. There are no atypical features in the cytoplasm.\no\nDiagnostic Markers: There are no features suggestive of atypical ductal\nhyperplasia … The overall appearance of the tissue is consistent with\nnormal breast ductal epithelium.\nu\nv\nw\nx\no\nTissue Organization: The tissue architecture is significantly disrupted,\nwith a loss of normal breast tissue organization. Abnormal, irregular,\nand infiltrative growth patterns ...\no\nCellular Features: Cellular pleomorphism is prominent, with marked\nvariation in size and shape of the tumor cells. The cells exhibit a high\nnuclear-to-cytoplasmic ratio ...\no\nNuclear Morphology: Nuclei are hyperchromatic, ... Prominent nucleoli\n...\no\nCytoplasmic Characteristics: The cytoplasm is … scant and eosinophilic .\no\nDiagnostic Markers: The combination of stromal invasion, cellular\npleomorphism, hyperchromatic nuclei, prominent nucleoli, and irregular\ngrowth patterns ...\nu\nv\nw\nx\nRECAP-PATH description: invasive carcinoma\nInvasive carcinoma\nu\nx\nv\nw\nu\nv\nw\nx\nPathology \ndescription\nPathologists \nfeedback\nPathologists \nfeedback\nPathologists \nfeedback\nKnowledge \nsummary\nRECAP-\nPATH\nExpert knowledge augmentation\n•\nEmphasize Precision in Histopathological Terminology\n•\nRequire Verification Before Stating Key Features\n•\nStructure Descriptions in a Logical Diagnostic Flow\n•\nClarify Expectations for Assessing Cellular and Structural Organization\n•\nPromote Objectivity and Avoid Overinterpretation\nBefore\naugmentation\nAfter\naugmentation\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nDescription accuracy\n*\n(A)\n(B)\n(C)\nFigure 3. Pathologist knowledge augmentation for RECAP-PATH optimization. (A) Integration of expert feedback into\nthe RECAP-PATH framework. Three board-certified pathologists provided blinded evaluations of LLM-generated image\ndescriptions across normal and invasive carcinoma cases, rating their precision and histopathological accuracy. These\nassessments were incorporated into the refinement process. The assessment summary is shown. (B) Pathologist ratings\ndemonstrated that incorporating expert feedback improved the clinical coherence and histopathological correctness of\ngenerated descriptions by nearly 20%. (C) Case-level analysis of optimized outputs. Representative examples of image-specific\ndiagnostic narratives illustrate how the optimized diagnosis criteria guide the MLLM to identify key histopathological features.\nFor benign samples, the model describes hallmark features such as organized tubular architecture, uniform gland size, absence\nof nucleoli, and abundant cytoplasm. For invasive carcinoma, it highlights malignant patterns including infiltrative growth,\nnuclear pleomorphism, prominent nucleoli, and stromal invasion.\n6/15\n"}, {"page": 7, "text": "correction could further improve robustness. Overall, our method delivers pathology-aligned explanations that strengthen the\ndiagnostic trustworthiness.\nRECAP-PATH enables interpretable subtype classification in breast cancer pathology\nBeyond distinguishing normal from malignant cases, precise differentiation between breast cancer subtypes, particularly ductal\ncarcinoma in situ (DCIS) and invasive carcinoma (IC), is essential for accurate prognosis and treatment planning. DCIS\nremains confined to the ductal system, is typically associated with an excellent prognosis, and is often managed conservatively.\nIn contrast, IC is defined by stromal invasion, carries a higher risk of metastasis, and generally requires more aggressive\nintervention. As shown in Fig. 4A, RECAP-PATH achieved strong classification performance, with true positive rates of 0.85\nfor DCIS and 0.90 for IC. Through prompt refinement, the MLLM autonomously generated subtype-specific diagnostic criteria\nthat emphasized key distinguishing features (Fig. 4B), including confinement of malignant cells within ducts, the presence or\nabsence of stromal invasion, and contrasts between intraductal and infiltrative growth patterns. These features are consistent\nwith canonical DCIS presentations, as illustrated in Fig. 4C. Notably, these diagnostic guidelines emerged spontaneously\nwithout manual annotation of subtype-specific cues, indicating the model’s capacity to internalize and articulate clinically\nrelevant histopathologic distinctions.\nFurthermore, the UMAP visualization of MLLM-generated descriptions in Fig. 4D revealed two separated clusters\ncorresponding to DCIS and IC, indicating that the model’s descriptions were both subtype-specific and semantically disentangled.\nCritically, the MLLM-generated descriptions for DCIS consistently highlighted hallmark histopathologic features, such as\nmarked nuclear pleomorphism and confinement of atypical cells within intact ducts, closely mirroring established diagnostic\ncriteria. This semantic separation confirms the model’s ability to generate phenotype-specific explanations and articulate true\nmorphologic differences between subtypes.\nWe further extended the classification task to a more complex multiclass setting involving normal tissue, DCIS, and IC.\nMulticlass pathology triage is inherently challenging due to diagnostic ambiguity and overlapping morphological features.\nDespite these complexities, the model maintained strong performance, as demonstrated by the confusion matrices in Fig. 4E\nand UMAP visualizations in Fig. 4F. The semantic embeddings of descriptions for DCIS and IC formed well-separated\nclusters, underscoring the model’s capacity to distinguish invasive from non-invasive disease. A subset of normal cases was\nmisclassified as DCIS, likely reflecting shared architectural features such as ductal confinement. These errors suggest that while\nthe framework effectively captures global tissue architecture, further refinement to emphasize subtler cytological cues, such as\nnuclear pleomorphism, could reduce residual ambiguity. Overall, these results support RECAP-PATH’s ability to scale from\nbinary to multiclass diagnostic reasoning while also defining opportunities for future enhancement.\nLastly, we assessed robustness and model-agnostic generalization. We ran experiments with two independent state-of-the-art\ngeneral-purpose MLLMs, Google Gemini 2.0 Flash48 and OpenAI gpt-4o-2024-05-1349, using black-box API access and no\nfinetuning. As summarized in Table 1, the optimized prompt consistently outperformed the initial prompt across all experimental\nsettings. Because these models are general-purpose rather than pathology-specific, the cross-model gains indicate that our\nprompt optimization strategy transfers across foundation models without white-box access, supporting robustness to model\nchoice and practical deployment in API-constrained clinical environments.\nModel\nMetric\nN/IC\nDCIS/IC\nN/DCIS/IC\nInit.\nOpti.\nInit.\nOpti.\nInit.\nOpti.\nGemini 2.0 Flash\nAccuracy\n71.87\n91.25\n82.50\n87.81\n60.28\n77.78\nLower CI\n69.26\n90.29\n79.69\n84.83\n55.63\n76.40\nUpper CI\n74.48\n92.21\n85.31\n90.80\n64.93\n79.16\nGPT-4o\nAccuracy\n52.25\n80.25\n59.50\n89.50\n52.00\n63.78\nLower CI\n46.20\n77.95\n57.73\n86.68\n47.70\n60.18\nUpper CI\n58.30\n82.55\n61.27\n92.32\n56.30\n67.38\nTable 1. Cross-model evaluation of RECAP-PATH optimization on the BRACS dataset. Classification accuracy (%, with\n95% confidence intervals) for normal vs. invasive carcinoma (N/IC), ductal carcinoma in situ vs. invasive carcinoma\n(DCIS/IC), and multiclass classification (N/DCIS/IC) using two general-purpose MLLMs (Gemini 2.0 Flash and GPT-4o).\nAcross all tasks, optimized diagnosis criteria consistently outperformed initial prompts, demonstrating the model-agnostic\ngeneralizability of RECAP-PATH.\nRECAP-PATH Generalizes Across Datasets: Prostate Gleason Grading and Breast Histology\nComplementing our cross-model analysis, we next evaluate cross-dataset generalization by applying RECAP-PATH to\nadditional histopathology datasets. We first applied the framework to the Breast Cancer Histology (BACH) dataset50, focusing\n7/15\n"}, {"page": 8, "text": "Initial prompt\nDCIS/IC\nOptimized prompt\nDCIS/IC\n0.64\n(n=25)\n0.36\n(n=14)\n0.0\n(n=0)\n1.0\n(n=41)\n0.85\n(n=33)\n0.15\n(n=6)\n0.10\n(n=4)\n0.90\n(n=37)\nInitial prompt\nN/DCIS/IC\nOptimized prompt\nN/DCIS/IC\nInvasive carcinoma\nDuctal carcinoma in situ\nUMAP 1\nUMAP 2\nOptimized prompt (diagnosis criteria)\n… analyze the breast tumor pathology slide, differentiating between … DCIS\nand Invasive Carcinoma based exclusively on its morphology.\n1. Distinguishing Feature: The critical difference hinges on whether the\nneoplastic cells are restricted to the confines of the mammary ducts (DCIS)\nor have breached the ductal basement membrane and infiltrated the\nsurrounding stroma (Invasive Carcinoma).\n2. Invasion Indicators: Stromal Extension: … irregular clusters or individual\ncells extending into the stroma beyond the ductal architecture...Reactive\nStroma: … desmoplastic response. Myoepithelial Cell Absence: … assess\nthe presence or absence of the myoepithelial layer around the duct ...\n3.\nDCIS\nIndicators:\nIntraductal\nLocation:\nThe\nneoplastic\ncells\nare\ncontained within the ducts …the overall confinement is paramount.\nCircumscription: The tumor mass often exhibits relatively well-defined\nborders...\nMicroinvasion Awareness: Remain cognizant that DCIS may\nexhibit minimal microinvasion…\nRECAP-PATH description: DCIS\nOverall Architecture: … appear to be mostly contained within a\nduct-like structure... The cells exhibit significant pleomorphism,\nwith varying sizes and shapes...\nReactive Stroma: Absent. The stroma around the tumor does\nnot exhibit a clear desmoplastic reaction.\nIntraductal Location: Present. A large portion of the cells\nappears to remain within the ductal structure.\n(A)\n(B)\n(C)\n(D)\n(F)\n(E)\nNormal\nInvasive carcinoma\nDuctal carcinoma in situ\nUMAP 1\nUMAP 2\n0.33\n(n=10)\n0.30\n(n=9)\n0.10\n(n=3)\n0.47\n(n=14)\n0.43\n(n=13)\n0.97\n(n=29)\n0.37\n(n=11)\n0.0\n(n=0)\n0.03\n(n=1)\n0.60\n(n=18)\n0.33\n(n=10)\n0.13\n(n=4)\n0.83\n(n=25)\n0.03\n(n=1)\n0.93\n(n=28)\n0.07\n(n=2)\n0.03\n(n=1)\n0.03\n(n=1)\nFigure 4. Subtype-specific classification performance and semantic interpretability of RECAP-PATH. (A) Confusion\nmatrices for binary classification of ductal carcinoma in situ (DCIS) versus invasive carcinoma (IC), showing improved\nperformance after prompt optimization (true positive rates: 0.85 for DCIS, 0.90 for IC). (B) Example of an optimized prompt\nillustrating how the model autonomously generated subtype-specific diagnostic criteria, emphasizing features such as stromal\ninvasion, ductal confinement, and differences between intraductal and infiltrative growth. (C) Representative image and\ncorresponding generated description for DCIS, highlighting hallmark features aligned with established pathological criteria.\n(D) UMAP visualization of description embeddings, demonstrating clear semantic separation between DCIS and IC, consistent\nwith phenotype-specific explanations. (E) Confusion matrices for multiclass classification (Normal, DCIS, IC), showing\nprogressive performance gains after optimization. (F) UMAP visualization of description embeddings in the multiclass setting,\ndemonstrating improved clustering and subtype differentiation, with most errors involving normal cases misclassified as DCIS\ndue to overlapping ductal features.\n8/15\n"}, {"page": 9, "text": "Prostate cancer: SICAP_v2\nBreast cancer: BACH\n0.33\n(n=13)\n0.67\n(n=27)\n0.03\n(n=1)\n0.97\n(n=39)\nZero-shot\n0.28\n(n=11)\n0.72\n(n=29)\n0.03\n(n=1)\n0.97\n(n=39)\nInitial prompt\n0.87\n(n=35)\n0.13\n(n=5)\n0.15\n(n=6)\n0.85\n(n=34)\nOptimized\n0.94\n(n=601)\n0.06\n(n=41)\n0.36\n(n=228)\n0.64\n(n=414)\nZero-shot\n0.61\n(n=394)\n0.39\n(n=248)\n0.18\n(n=115)\n0.82\n(n=527)\nInitial prompt\n0.77\n(n=492)\n0.23\n(n=150)\n0.25\n(n=158)\n0.75\n(n=484)\nOptimized\nOverall Impression: Malignant. Key Features:\nArchitecture:\no\nCribriform: Present; poorly-formed, \nwidespread\no\nGlandular Crowding: Severe\no\nInfiltration: Present; irregular margins\no\nGlandular Shape: Irregular\nCytology:\no\nNuclear Size: Moderately Enlarged\no\nNuclear Pleomorphism: Moderate\no\nNucleoli: Prominent\nIII. Gleason Score (if applicable): 4+4. The dominant \narchitectural pattern is poorly-formed cribriform \nglands with significant glandular crowding, \nThere is also presence of fused glands.\nu\nv\nw\nx\nu\nv\nx\nw\nAnalyze the prostate tissue image to classify it as either: A. Non-cancerous (NC) or B. Prostate cancer (Gleason score 3-5)...\nI. Overall Impression: Briefly state your initial impression: Benign or Malignant? Indicate confidence (High, Moderate, Low).\nII. Key Features:\nArchitecture: Describe the most significant architectural features:\n•\nCribriform (Present / Absent; if present, describe: well-formed / poorly-formed, extent); Glandular Crowdin (Minimal / Moderate / \nSevere); Infiltration (Present / Absent; if present, describe: well-defined margins/irregular margins); Glandular Shape (Regular / \nIrregular)\nCytology: Describe the most significant cytological features:\n•\nNuclear Size (Normal / Slightly Enlarged / Moderately Enlarged / Markedly Enlarged); Nuclear Pleomorphism (Minimal / Mild / \nModerate / Marked); Nucleoli (Absent / Inconspicuous / Prominent)\nIII. Gleason Score (if applicable): If classifying as Prostate cancer (G3-G5), provide a likely Gleason pattern (e.g., 3+3, 3+4, 4+3, \netc.) and briefly explain why. Focus on the dominant architectural pattern driving the score.\nIV. Justification: In one sentence, summarize the key features that support your classification.\n(A)\n(C)\n(E)\n(F)\n(B)\n(D)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12\nRound number\n0.5\n0.6\n0.7\n0.8\nAccuracy\nPhase 1:\nDiversity\nPhase 2:\nAccuracy\n1.0\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12\nRound number\nPhase 1:\nDiversity\nPhase 2:\nAccuracy\nFigure 5. Generalization of RECAP-PATH across pathology datasets. (A) Confusion matrices for normal versus invasive\ncarcinoma in the BACH dataset, showing improved performance after prompt optimization despite lower resolution and smaller\ndataset size compared to BRACS. (B) Accuracy trajectory in BACH, demonstrating a similar non-monotonic two-phase\nlearning dynamic as observed in BRACS. (C) Confusion matrices for benign versus malignant classification in the prostate\ncancer SICAPv2 dataset, showing balanced performance gains after optimization. (D) Reproduction of the two-phase learning\ndynamics in SICAPv2. (E) Example optimized diagnostic criteria in prostate histology, illustrating key features such as\nglandular architecture, arrangement, and cytological atypia. (F) Case-level description of a malignant prostate sample (Gleason\n4+4), where the optimized prompt guided the model to identify hallmark features including cribriform architecture, infiltrative\ngrowth, and nuclear pleomorphism, consistent with expert diagnostic criteria.\n9/15\n"}, {"page": 10, "text": "on distinguishing normal breast tissue from invasive carcinoma for direct comparison with our previous BRACS-based results.\nUnlike BRACS, the BACH dataset is smaller in size and has lower spatial resolution (∼0.42 µm/pixel, approximately half that\nof BRACS). Despite these differences, our prompt optimization pipeline achieved a classification accuracy comparable to that\nobserved with BRACS (Fig. 5A), following a similar diversification and optimization trajectory (Fig. 5B). This consistency\nsuggests that our prompt-learning mechanism generalizes effectively across datasets. As in our prior analysis, the optimized\nprompts highlighted comprehensive morphological descriptors, explicitly incorporating architectural organization, the presence\nand morphology of myoepithelial cells, cellular density, and stromal composition. These refinements facilitated more precise\ndiscrimination between normal breast tissue and invasive carcinoma.\nWe also examined a substantially different dataset consisting of prostate cancer images annotated with patch-level Gleason\ngrades (SICAPv2)51. Despite differences in tissue type and diagnostic standards, RECAP-PATH achieved high classification\naccuracy after prompt learning, generating balanced predictions (Fig. 5C) and clinically reasonable analyses. The two-phase\nlearning dynamics was also reproduced in this dataset (Fig. 5D). The optimized prompts were adapted to prostate histology,\nguiding the model toward critical diagnostic hallmarks, including overall glandular architecture, glandular arrangement, and\ncytological atypia (Fig. 5E). The prompts further directed the model to provide Gleason grading and justification based on these\ncriteria (Fig. 5F). This methodological approach enabled the MLLM to produce structured, clinically interpretable outputs\naligned with routine diagnostic workflows. For instance, the model correctly identified hallmark features such as cribriform\narchitecture, infiltrative glandular growth, and nuclear pleomorphism in a case diagnosed as malignant with a Gleason score of\n4+4, consistent with the ground-truth annotation. Collectively, these experiments demonstrate the dataset-specific adaptability\nof RECAP-PATH. By iteratively refining prompts to reflect domain-specific histological features, our framework not only\nenhances classification accuracy but also produces pathologically coherent reasoning across diverse diagnostic contexts.\nDiscussion\nWe introduce RECAP-PATH, a two-phase learning framework for trustworthy AI pathology. It achieves > 0.9 accuracy in\ndistinguishing normal from invasive carcinoma and delivers up to ∼20% gains across tasks and models, while providing\npathologist-aligned rationales. By generating image-grounded explanations34, RECAP-PATH highlights the promise of MLLMs\nand directly addresses the opacity of post-hoc methods in conventional AI pathology. Although general-purpose MLLMs excel\nat image description, their pathology performance often lags without adaptation. Prior efforts, such as in-context examples33\nand domain-specific fine-tuning30, enhance alignment but require extensive high-quality data and often fall short of delivering\ntransparent reasoning. RECAP-PATH complements these approaches with a self-learning, model-agnostic prompting framework\nthat elicits pathology-style reasoning before prediction via a diversify-to-optimize axis. Using small labeled sets and standard\nAPI access (rather than weight updates), it refines diagnosis criteria to yield evidence-linked image descriptions aligned with\ndiagnostic reasoning.\nFrom a biomedical perspective, RECAP-PATH’s principal advance is practical auditability: by producing explicit,\nmorphology-level narratives that document the evidence behind each call, it generates the traceable outputs that clinicians and\ntumor boards increasingly expect for AI in health. Its description-first pipeline elicits pathology-relevant criteria (invasion,\narchitecture, nuclear features, stroma, mitoses) and integrates them with the image for final decisions, aligning the model’s\nreasoning with established diagnostic practice and avoiding the unstable cues of post-hoc explainer52. Importantly, blinded\nexpert-in-the-loop evaluation shows that pathologist feedback measurably improves the histopathological correctness of histol-\nogy image descriptions53. These elements position RECAP-PATH as a pragmatic, evidence-focused solution that supports\nclinical trust and responsible deployment in pathology.\nOne limitation is that the optimized prompt remains dataset-specific: each new dataset requires a separate optimization\nprocess, as prompts tuned on one cohort (e.g., a particular breast cancer dataset) may not generalize effectively to another,\neven within the same disease type. Future studies should investigate slot-based calibration with a small number of labeled\nexamples as a lightweight alternative to re-running prompt optimization from scratch, enabling more efficient cross-dataset\ngeneralization. A further limitation is that our experiments relied on ROI/patch crops rather than whole-slide images (WSIs),\nsidestepping tissue selection and aggregation steps that are critical for clinical realism. An immediate next step is to embed\nRECAP-PATH in a full WSI pipeline and benchmark against external datasets and state-of-the-art slide-level baselines54–56.\nFinally, the framework depends on vendor-hosted MLLMs, raising concerns about undocumented updates and version drift that\nmay undermine reproducibility57. To address this, future work should stress-test open-source local models for comparability\nand adopt standardized, multi-metric evaluation protocols and clinical reporting guidelines to ensure robustness, calibration,\nand transparent documentation58–60.\nIn sum, RECAP-PATH shows that reasoning itself can be optimized into a reliable, auditable signal—transforming MLLMs\nfrom opaque classifiers into systems capable of delivering clinically aligned decisions. By embedding domain-specific\ndiagnostic criteria directly into the decision process, it bridges methodological innovation in machine learning with the practical\ndemands of pathology. More broadly, it exemplifies a paradigm in which interpretability and performance are not opposing\n10/15\n"}, {"page": 11, "text": "goals but mutually reinforcing, charting a path toward AI systems that are both scientifically rigorous and ready for real-world\nclinical integration.\nMethods\nDatasets\nOur experiments are conducted on the following, histopathology image datasets:\n• BRACS61 (BReAst Carcinoma Subtyping) is a dataset of hematoxylin and eosin (H&E) stained histopathological images\ndesigned for automated detection and classification of breast tumors. It consists of 4539 labeled Regions of Interest\n(RoIs) extracted from 387 whole-slide images (WSIs) collected from 151 patients. The RoIs vary in size and can exceed\n4,000 by 4,000 pixels. In our experiments, we consider two classification tasks: (1) distinguishing cancerous tissue\nfrom non-cancerous tissue (i.e., Normal (N) vs. Invasive Carcinoma (IC)), and (2) classifying two cancer subtypes:\nDuctal Carcinoma In Situ (DCIS) and Invasive Carcinoma (IC). Each group includes hundreds of training samples and\napproximately 80 test samples. For optimization, we use only 30 training samples per category, which is sufficient to\nachieve strong performance with reduced computational cost.\n• BACH50 (Breast Cancer Histology) dataset is a public collection of high-resolution microscopy images designed for\nthe classification of breast cancer subtypes. The complete dataset contains 400 RGB images in TIFF format, equally\ndistributed across four clinically relevant classes: normal, benign, in situ carcinoma, and invasive carcinoma, with 100\nimages per class. Each image has a resolution of 2048x1536 pixels with a pixel scale of 0.42 µm, and was annotated by\nexpert pathologists. For our experiment, we focused on a binary classification task to distinguish between the Normal and\nInvasive carcinoma these two categories. We combined the 100 images from each of these two classes and performed\na random 60/40 split while maintaining label balance. This resulted in a training set of 120 images (60 Normal, 60\nInvasive) and a test set of 80 images (40 Normal, 40 Invasive) for the experiment.\n• SICAPv251 dataset is a comprehensive collection of 18,783 10x magnified histological prostate images from 155 patients,\neach with a resolution of 512x512 pixels and detailed per-pixel annotations under Gleason grading (GG), which is the\nmain diagnostic and evaluative tool for prostate cancer in clinical practice. The dataset is originally graded into four\nclasses: Non-cancer (NC), G3, G4, and G5. For our experiment, we focused on a binary classification task to distinguish\nbetween Non-cancer and Cancer. To accomplish this, we first consolidated the G3 (characterized by atrophic, highly\ndifferentiated, dense gland areas), G4 (characterized by cribriform, pathological, large confluent, papillary glands), and\nG5 (characterized by individual cells, no light cavity forming, pseudorosette cell nests) grades into a single \"Cancer\"\ncategory. From this structure, we created a balanced training set by randomly sampling 100 images from the Non-cancer\nclass and 100 images from the combined Cancer class. For robust evaluation, we then constructed a test set by randomly\nsampling 644 images from the remaining Non-cancer pool and 642 images from the remaining Cancer pool, resulting in\na nearly balanced test set of 1,286 images.\nAutomatic prompt optimization with description generation\nImage description generation and diganosis\nLet (x,y) denote an image-label pair from the dataset D = {(xi,yi)N\ni=1}, and let p be a classification prompt associated with the\ndataset. In a standard zero-shot setting, a general-purpose MLLM predicts the label directly from the image using the prompt,\nformulated as ˆy = MLLM(x, p).\nOur method introduces a two-step prediction pipeline to enhance interpretability and diagnostic accuracy, as illustrated in\nFig. 1(A). Specifically, we incorporate a description generation prompt q to first guide the model to generate a structured textual\ndescription of the input image. This process is denoted as s = MLLM(x,q). The generated description s can provide additional\ncontextual cues to the model. In the second step, we concatenate the classification prompt p with the generated description s\nand pass them, along with the image x, to the MLLM for final prediction. This step is represented as ˆy = MLLM(x,[p,s]).\nAutomated Prompt Refinement for Image Descriptions\nShown in Fig. 1(B), we utilize a small number of labeled training data to optimize the generation prompt q, so that more precise\nand informative descriptions could be generated to make the diagnosis more reliable. We formalize an optimization objective\nbased on the prompt diversity or training set prediction accuracy to evaluate the effectiveness of each candidate generation\nprompt. The algorithm is illustrated in Algorithm 1.\n11/15\n"}, {"page": 12, "text": "Algorithm 1 Automatic Prompt Optimization of Image Descriptions\nRequire: q0: initial generation prompt, p: prediction prompt, N: iterations, D: training dataset, b: top prompts retained per iteration, l:\nnumber of error examples for reflections, S(·): scoring function\n1: Q0 ←{q0}\n▷Initialize candidate prompt set\n2: Γ ←{γ0}, where γ0 = {(xi,si) | si = MLLM(xi,q0),(xi,yi) ∈D}\n▷Generate descriptions\n3: for t = 1 to N do\n4:\nQc ←Qt−1\n5:\nfor q ∈Qt−1 do\n6:\nJerror = {(xi,yi) | ˆyi ̸= yi, ˆyi = MLLM(xi,[p,si]),(xi,si) ∈γ,γ ∈Γ,(xi,yi) ∈D}\n▷Collect errors\n7:\nJierror ⊂Jerror for i = 1,...,l\n▷Sample a subset for each i = 1,...,l\n8:\nG = {g1,g2,...gl} = S\ni=1,...l Reflect(p,Jierror)\n▷Reflect on the errors\n9:\nH = {h1,h2,...hl} = S\ni=1,...l Modify(p,gi,Jierror)\n▷Modify prompts\n10:\nΓ ←Γ∪{{(xi,si) | si = MLLM(xi,h),(xi,yi) ∈D} | h ∈H}\n▷Generate descriptions for new prompts\n11:\nend for\n12:\nSc = {S(q) | q ∈Qc}\n▷Evaluate prompts\n13:\nQt ←{q ∈Qc | S(q) ≥τ}, where τ is the bth highest score in Sc\n14: end for\n15: Return q∗←argmaxq∈QN S(q)\n2-phase optimization\nOur method consists of two optimization phases with different scoring functions S(·). In the first phase, we optimize for prompt\ndiversity. We quantify this diversity using two components: the terminology count (T(qi)), which is the number of biomedical\nterms in a prompt qi, and term uniqueness (U(qi)), the number of biomedical terms in qi that do not appear in any other\nprompt qj (where j ̸= i). To ensure a balanced contribution, we normalize these counts by the maximum value observed across\nall prompts:\n˜T(qi) =\nT(qi)\nmaxj T(qj),\n˜U(qi) =\nU(qi)\nmaxjU(qj).\nThe final diversity score D(qi) is defined as the sum of these two normalized components:\nD(qi) = ˜T(qi)+ ˜U(qi).\nWe set the diversity scoring function S(q) = D(q) for this phase. This formulation ensures that both the richness of terminology\nand the uniqueness of each prompt contribute equally to the overall diversity measure.\nIn the second phase, we define the scoring function as the diagnostic accuracy on a training dataset D = {(xi,yi)}N\ni=1, which\nis essentially the proportion of correctly classified samples:\nS(q) = Accuracy = 1\nN\nN\n∑\ni=1\n1\n\u0002\nˆyi = yi\n\u0003\n= 1−|Jerror|\n|D| ,\nwhere ˆyi = MLLM(xi,[p,si]) is the model’s prediction given image xi and description si generated with prompt q, and 1[·]\ndenotes the indicator function.\nModel and hyperparameter setup\nAll the experiments in our study were conducted using Gemini 2.0 Flash48, a state-of-the-art general-purpose MLLM released\non February 5, 2025 (model string: \"gemini-2.0-flash-001\"), and OpenAI GPT-4o (model string: \"gpt-4o-2024-05-13\"). We set\nthe temperature to 0.0 for deterministic classification predictions and 0.7 for more creative and informative image description\ngeneration. We set the number of top prompts retained per iteration b to be 4, the number of error examples for reflections l to\nbe 4.\nData analysis and visualization\nTo generate the UMAP visualizations, we first embedded the generated image descriptions using Gemini text-embedding-004\n(released on May 14, 2024), which projects each description into a 768-dimensional semantic space. We then applied UMAP62\nwith Euclidean distance as the similarity metric to reduce the high-dimensional embeddings to a two-dimensional space for\nvisualization.\n12/15\n"}, {"page": 13, "text": "Expert blind evaluation survey\nTo assess the clinical relevance and interpretability of RECAP-PATH, we conducted a blind evaluation survey involving three\nboard-certified pathologists. with images and their corresponding descriptions generated by MLLM. Each pathologist was\npresented with histopathology images alongside corresponding descriptions generated by the MLLM. They evaluated the\naccuracy, completeness, and clarity of the descriptions, and provided structured feedback on diagnostic relevance. This expert\nfeedback was incorporated into the optimization process, guiding refinements in prompt design and improving the quality of\ngenerated outputs. In addition, we performed a blinded comparative evaluation in which pathologists were asked to review\ndescriptions generated with and without pathology-specific knowledge augmentation. Without being informed of the source,\nthey rated each description on criteria such as diagnostic accuracy, interpretability, and usefulness for clinical decision-making.\nData Availability\nThe datasets used in this study are publicly available and can be downloaded from https://www.bracs.icar.cnr.it/ (BRACS),\nhttps://iciar2018-challenge.grand-challenge.org/Dataset/ (BACH), and https://data.mendeley.com/datasets/9xxm58dvs3/1 (SICAPv2).\nCode Availability\nThe code for this work is publicly available at https://github.com/yq-hong/RECAP-PATH.\nReferences\n1. Coudray, N. et al. Classification and mutation prediction from non–small cell lung cancer histopathology images using\ndeep learning. Nat. medicine 24, 1559–1567 (2018).\n2. Campanella, G. et al. Clinical-grade computational pathology using weakly supervised deep learning on whole slide\nimages. Nat. medicine 25, 1301–1309 (2019).\n3. Chang, H. Y. et al. Artificial intelligence in pathology. J. pathology translational medicine 53, 1–12 (2019).\n4. Meroueh, C. & Chen, Z. E. Artificial intelligence in anatomical pathology: building a strong foundation for precision\nmedicine. Hum. Pathol. 132, 31–38 (2023).\n5. Xu, Z., Lin, A. & Han, X. Current ai applications and challenges in oral pathology. Oral 5, 2 (2025).\n6. Poalelungi, D. G. et al. Revolutionizing pathology with artificial intelligence: Innovations in immunohistochemistry. J.\nPers. Medicine 14, 693 (2024).\n7. Mayall, F. G. et al. Artificial intelligence-based triage of large bowel biopsies can improve workflow. J. Pathol. Informatics\n14, 100181 (2023).\n8. Sankarapandian, S. et al. A pathology deep learning system capable of triage of melanoma specimens utilizing der-\nmatopathologist consensus as ground truth. In Proceedings of the IEEE/CVF international conference on computer vision,\n629–638 (2021).\n9. Wen, Z. et al. Deep learning–based h-score quantification of immunohistochemistry-stained images. Mod. Pathol. 37,\n100398 (2024).\n10. Qaiser, T. et al. Her 2 challenge contest: a detailed assessment of automated her 2 scoring algorithms in whole slide images\nof breast cancer tissues. Histopathology 72, 227–238 (2018).\n11. Grifoll, Correas. Aiosyn expands its ai-powered quality control solution for digital pathology slides to support immunohis-\ntochemistry (ihc) staining (2023).\n12. Huo, X. et al. A comprehensive ai model development framework for consistent gleason grading. Commun. Medicine 4,\n84 (2024).\n13. Weng, Z. et al. Grandqc: A comprehensive solution to quality control problem in digital pathology. Nat. Commun. 15,\n10685 (2024).\n14. Stiglic, G. et al. Interpretability of machine learning-based prediction models in healthcare. Wiley Interdiscip. Rev. Data\nMin. Knowl. Discov. 10, e1379 (2020).\n15. Zhang, Z. et al. Pathologist-level interpretable whole-slide cancer diagnosis with deep learning. Nat. Mach. Intell. 1,\n236–245 (2019).\n16. Acs, B., Rantalainen, M. & Hartman, J. Artificial intelligence as the next step towards precision pathology. J. internal\nmedicine 288, 62–81 (2020).\n13/15\n"}, {"page": 14, "text": "17. Salahuddin, Z., Woodruff, H. C., Chatterjee, A. & Lambin, P. Transparency of deep neural networks for medical image\nanalysis: A review of interpretability methods. Comput. biology medicine 140, 105111 (2022).\n18. Albuquerque, T., Yüce, A., Herrmann, M. D. & Gomariz, A. Characterizing the interpretability of attention maps in digital\npathology. arXiv preprint arXiv:2407.02484 (2024).\n19. Chung, M., Won, J. B., Kim, G., Kim, Y. & Ozbulak, U. Evaluating visual explanations of attention maps for transformer-\nbased medical imaging. In International Conference on Medical Image Computing and Computer-Assisted Intervention,\n110–120 (Springer, 2024).\n20. Kim, B. et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In\nInternational conference on machine learning, 2668–2677 (PMLR, 2018).\n21. Bai, A., Yeh, C.-K., Ravikumar, P., Lin, N. Y. & Hsieh, C.-J. Concept gradient: Concept-based interpretation without linear\nassumption. In International Conference on Learning Representation (2022).\n22. Koh, P. W. & Liang, P. Understanding black-box predictions via influence functions. In International conference on\nmachine learning, 1885–1894 (PMLR, 2017).\n23. Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T. J. & Zou, J. A visual–language foundation model for pathology\nimage analysis using medical twitter. Nat. medicine 29, 2307–2316 (2023).\n24. Ikezogwo, W. O. et al. Quilt-1m: One million image-text pairs for histopathology. arXiv preprint arXiv:2306.11207\n(2023).\n25. Arun, N. et al. Assessing the trustworthiness of saliency maps for localizing abnormalities in medical imaging. Radiol.\nArtif. Intell. 3, e200267 (2021).\n26. Wang, P. et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution. arXiv preprint\narXiv:2409.12191 (2024).\n27. Chen, Z. et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time\nscaling. arXiv preprint arXiv:2412.05271 (2024).\n28. Team, G. et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786 (2025).\n29. Lu, H. et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525 (2024).\n30. Li, C. et al. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. arXiv preprint\narXiv:2306.00890 (2023).\n31. Zhang, K. et al. A generalist vision–language foundation model for diverse biomedical tasks. Nat. Medicine 1–13 (2024).\n32. Lu, M. Y. et al. A multimodal generative ai copilot for human pathology. Nature 634, 466–473 (2024).\n33. Ferber, D. et al. In-context learning enables multimodal large language models to classify cancer pathology images. Nat.\nCommun. 15, 10104 (2024).\n34. Ding, L. et al. Evaluating chatgpt’s diagnostic potential for pathology images. Front. Medicine 11, 1507203 (2025).\n35. Guo, D. & Terzopoulos, D. Prompting medical large vision-language models to diagnose pathologies by visual question\nanswering. arXiv preprint arXiv:2407.21368 (2024).\n36. Zhou, S. et al. Large language models for disease diagnosis: A scoping review. npj Artif. Intell. 1, 1–17 (2025).\n37. Xiang, J. et al. A vision–language foundation model for precision oncology. Nature 638, 769–778 (2025).\n38. Lu, M. Y. et al. A visual-language foundation model for computational pathology. Nat. Medicine 30, 863–874 (2024).\n39. Clusmann, J. et al. Incidental prompt injections on vision–language models in real-life histopathology. NEJM AI 2,\nAIcs2500078 (2025).\n40. Cecchini, M. J. et al. Harnessing the power of generative artificial intelligence in pathology education: Opportunities,\nchallenges, and future directions. Arch. Pathol. & Lab. Medicine 149, 142–151 (2025).\n41. Brodsky, V. et al. Generative artificial intelligence in anatomic pathology. Arch. Pathol. & Lab. Medicine (2025).\n42. Perez-Garcia, F. et al. Exploring scalable medical image encoders beyond text supervision. Nat. Mach. Intell. 7, 119–130\n(2025).\n43. Ullah, E., Parwani, A., Baig, M. M. & Singh, R. Challenges and barriers of using large language models (llm) such as\nchatgpt for diagnostic medicine with a focus on digital pathology–a recent scoping review. Diagn. pathology 19, 43 (2024).\n14/15\n"}, {"page": 15, "text": "44. Pryzant, R. et al. Automatic prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495\n(2023).\n45. Yang, C. et al. Large language models as optimizers. arXiv preprint arXiv:2309.03409 (2023).\n46. Guo, Q. et al. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers (2024).\n2309.08532.\n47. Hong, Y., An, S., Bai, A., Lin, N. Y. & Hsieh, C.-J. Unlabeled data improves fine-grained image zero-shot classification\nwith multimodal llms. arXiv preprint arXiv:2506.03195 (2025).\n48. Pichai, Sundar and Hassabis, Demis and Kavukcuoglu, Koray. Introducing gemini 2.0: our new ai model for the agentic\nera (2024). Accessed: 2025-04-21.\n49. Hurst, A. et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024).\n50. Aresta, G. et al. Bach: Grand challenge on breast cancer histology images. Med. image analysis 56, 122–139 (2019).\n51. Silva-Rodríguez, J., Colomer, A., Sales, M. A., Molina, R. & Naranjo, V. Going deeper through the gleason scoring\nscale: An automatic end-to-end system for histology prostate grading and cribriform pattern detection. Comput. methods\nprograms biomedicine 195, 105637 (2020).\n52. Adebayo, J. et al. Sanity checks for saliency maps. Adv. neural information processing systems 31 (2018).\n53. Vasey, B. et al. Reporting guideline for the early stage clinical evaluation of decision support systems driven by artificial\nintelligence: Decide-ai. bmj 377 (2022).\n54. Ilse, M., Tomczak, J. & Welling, M. Attention-based deep multiple instance learning. In International conference on\nmachine learning, 2127–2136 (PMLR, 2018).\n55. Lu, M. Y. et al. Data-efficient and weakly supervised computational pathology on whole-slide images. Nat. biomedical\nengineering 5, 555–570 (2021).\n56. Li, H. et al. Rethinking transformer for long contextual histopathology whole slide image analysis. Adv. Neural Inf.\nProcess. Syst. 37, 101498–101528 (2024).\n57. Chen, L., Zaharia, M. & Zou, J. How is chatgpt’s behavior changing over time? Harv. Data Sci. Rev. 6 (2024).\n58. Mitchell, M. et al. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and\ntransparency, 220–229 (2019).\n59. Liang, P. et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).\n60. Hernandez-Boussard, T., Bozkurt, S., Ioannidis, J. P. & Shah, N. H. Minimar (minimum information for medical ai\nreporting): Developing reporting standards for artificial intelligence in health care. J. Am. Med. Informatics Assoc. 27,\n2011–2015 (2020).\n61. Brancati, N. et al. Bracs: A dataset for breast carcinoma subtyping in h&e histology images. Database 2022, baac093\n(2022).\n62. McInnes, L., Healy, J. & Melville, J. Umap: Uniform manifold approximation and projection for dimension reduction.\narXiv preprint arXiv:1802.03426 (2018).\nAcknowledgements\nThis work was partially supported by NSF (CBET-2244760, DBI-2325121, IIS-2048280) and NIH NIGMS (R35GM146735).\nAuthor contributions statement\nY.H., C.J.H., and N.Y.C.L. conceived the project. Y.H. and L.E. developed the framework. Y.H., J.K., and L.E. performed the\ntests. All authors contributed to data analysis, manuscript writing, and review.\nCompeting Interests\nThe authors declare no competing interests.\n15/15\n"}]}