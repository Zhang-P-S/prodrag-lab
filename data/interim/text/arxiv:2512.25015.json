{"doc_id": "arxiv:2512.25015", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.25015.pdf", "meta": {"doc_id": "arxiv:2512.25015", "source": "arxiv", "arxiv_id": "2512.25015", "title": "MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes", "authors": ["Siddhant Agarwal", "Adya Dhuler", "Polly Ruhnke", "Melvin Speisman", "Md Shad Akhtar", "Shweta Yadav"], "published": "2025-12-31T18:06:21Z", "updated": "2025-12-31T18:06:21Z", "summary": "Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a vital resource for detecting depressive symptoms in memes on social media through the Large Language Model (LLM) generated and human-annotated explanations. We introduce MAMAMemeia, a collaborative multi-agent multi-aspect discussion framework grounded in the clinical psychology method of Cognitive Analytic Therapy (CAT) Competencies. MAMAMemeia improves upon the current state-of-the-art by 7.55% in macro-F1 and is established as the new benchmark compared to over 30 methods.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.25015v1", "url_pdf": "https://arxiv.org/pdf/2512.25015.pdf", "meta_path": "data/raw/arxiv/meta/2512.25015.json", "sha256": "594d8fcfceb1a34f9191d008211486a60880afb11293f04ffc79dd19cd340f8c", "status": "ok", "fetched_at": "2026-02-18T02:23:33.187151+00:00"}, "pages": [{"page": 1, "text": "MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive\nSymptoms Identification in Memes\nSiddhant Agarwal1*, Adya Dhuler2, Polly Ruhnke1,\nMelvin Speisman1, Md Shad Akhtar3*, Shweta Yadav1*\n1 University of Illinois at Chicago, USA\n2 Creighton University, USA\n3 Indraprastha Institute of Information Technology Delhi, India\n*Corresponding authors: {sagarw38, shwetay}@uic.edu, shad.akhtar@iiitd.ac.in\nAbstract\nOver the past years, memes have evolved from being exclu-\nsively a medium of humorous exchanges to one that allows\nusers to express a range of emotions freely and easily. With\nthe ever-growing utilization of memes in expressing depres-\nsive sentiments, we conduct a study on identifying depressive\nsymptoms exhibited by memes shared by users of online so-\ncial media platforms. We introduce RESTOREx as a vital re-\nsource for detecting depressive symptoms in memes on social\nmedia through the Large Language Model (LLM) generated\nand human-annotated explanations. We introduce MAMA-\nMemeia, a collaborative multi-agent multi-aspect discussion\nframework grounded in the clinical psychology method of\nCognitive Analytic Therapy (CAT) Competencies. MAMA-\nMemeia improves upon the current state-of-the-art by 7.55%\nin macro-F1 and is established as the new benchmark com-\npared to over 30 methods.\n1\nIntroduction\nIn the digital age, memes have emerged as a pervasive\nform of content on online social media platforms, under-\nscoring their escalating significance. In 2020 alone, Insta-\ngram, a popular social media app, reported at least one mil-\nlion posts shared every day mentioning the word “meme”\n(Brown 2022). Beyond entertainment, memes have grown\nto become an important outlet for expressive posting as well\n(Wang et al. 2019), as people have increasingly begun to\nuse memes to communicate their emotional struggles, most\nparticularly depression, where a simple Google search for\n“depression memes” shows more than 84, 700, 000 results.\nAkram et al. (2020) observe that sharing humor-intended de-\npressive memes can be beneficial for some individuals, as\npresenting a negative experience humorously may create a\nsense of peer support among viewers who have gone through\nsimilar situations. The importance of the visual modality is\nfurther cemented considering that the majority of social me-\ndia posts today contain image or video content which in-\ncreases engagement by around 100% (London 2024).\nRecognizing the importance of memes in online social\nmedia today, meme analysis research has gained traction,\nstarting with the Facebook Hateful Memes Challenge (Kiela\net al. 2020). However, research in the area has been mostly\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nConcentration Problem\nSleeping Disorder\nFeeling Down\nLow Self-Esteem\n...The image of the puppy, which is often associated with innocence and\nhappiness, juxtaposed with the DVD of mistakes, emphasizes the irony\nand the emotional burden that the person is carrying.\n...brain is unable to concentrate on sleeping and instead the memories of\nthe person's part mistakes are played like a movie from a DVD in their\nbrain. This indicates that the person is fixated on their past mistakes...\nFigure 1: An example of a depressive meme from\nRESTOREx with labeled fine-grained depressive symptoms\nand explanations from the LLM agent and Human\nfocused on aspects such as harmfulness (Pramanick et al.\n2021) and cyber-bullying (Jha et al. 2024b). In this work,\nwe work on a relatively understudied area in meme analysis\nand apply it to the mental health domain. We work to iden-\ntify seven fine-grained depressive symptoms based on the\nclinically established 9-scale Patient Health Questionnaire\n(PHQ-9) (Kroenke, Spitzer, and Williams 2001).\nThe task involves multi-label classification for the seven\nidentified symptoms in the multimodal memes as pro-\nposed by Yadav et al. (2023). We derive a new dataset,\nRESTOREx, consisting of explanations generated by state-\nof-the-art Multimodal Large Language Models and supple-\nmented with human-annotated explanations for the memes.\nFor this task, we examine the application of Multimodal\nLarge Language Models in understanding complex multi-\nmodal data of memes in the context of mental health. This\narXiv:2512.25015v1  [cs.CL]  31 Dec 2025\n"}, {"page": 2, "text": "is a complex task for language models given the mix of sar-\ncasm, irony and other types of figurative speech involved\nin memes. For example, in Figure 1, the meme has the\ncaption “Me: Alright time to sleep.”, “My brain:”, with\na picture of a puppy holding a DVD that says, “Top 50\nmistakes you’ve made”. The human annotator emphasizes\non the meme author’s fixation on their past mistakes, cor-\nrectly identifying the ‘Low Self-Esteem’ symptom. Further,\nthe LLM-generated explanation, recognizing the image of\nthe sad expression of the puppy (often used in a positive\ncontext) juxtaposed with negative emotions relating to the\nDVD, understands the irony of the situation to correctly as-\nsign the ‘Feeling Down’ label to this meme. Overall, through\na multi-dimensional understanding of the various figura-\ntive speeches in the meme, the method should capture all\nfour depressive symptoms expressed by the meme author –\nLow Self-Esteem, Concentration Problem, Sleeping Disor-\nder, and Feeling Down.\nIn this work, we introduce MAMA-Memeia, a Multi-\nAspect Multi-Agent collaborative framework for identifying\ndepressive symptoms from memes. We ground this proposed\nmethodology in clinical psychology by adapting the Cog-\nnitive Analytic Therapy (CAT) Competencies (Parry et al.\n2021) into our Multi-Aspect prompting setup which is used\nby medical professionals for analyzing thinking patterns. We\nsupplement our analysis with extensive comparative results\nestablishing the efficacy of MAMA-Memeia as the preemi-\nnent method for the task of depression symptom analysis.\nOur contributions are summarized as follows-\n• Novel Dataset: Introducing the RESTOREx dataset, with\nLLM-generated explanations and human annotated gold\nlabel explanations.\n• Novel Methodology: Introducing the MAMA-Memeia\nframework, a novel state-of-the-art multi-agent discus-\nsion framework that builds upon the foundations of clin-\nical psychology for multi-aspect prompting.\n• In-depth Human Evaluation: Conducted a human eval-\nuation with domain experts to address key research ques-\ntions regarding LLM-generated content across five as-\npects.\n2\nRelated Work\nIn recent years, efforts towards the analysis of memes and\nsubsequent datasets in the area have been focussed mainly\non aspects such as hatefulness (Kiela et al. 2020), harm-\nfulness (Sharma et al. 2023a), and emotions (Sharma et al.\n2020; Mishra et al. 2023). Recently, Joshi, Ilievski, and\nLuceri (2024) attempted to understand the propagation of\nmeme content across social media platforms.\nWhile most work on modeling for meme analysis has fo-\ncused on contrastive learning setups (Mei et al. 2024) and\nknowledge fusion techniques (Sharma et al. 2023b), the re-\ncent direction of the area has been towards the use of LLMs\n(Jha et al. 2024a). Works such as those by Agarwal et al.\n(2024) and Zhong and Baghel (2024) have explored the ap-\nplication of explanations generated by LLMs for explaining\nmemes in various contexts. However, rigorous analysis of\nTable 1: Class-wise distribution of fine-grained depression\nsymptom labels in RESTOREx.\nLOI\nFD\nED\nSD\nLSE\nCP\nSH\nTotal\nTrain\n441\n1781\n1714\n997\n703\n348\n1357\n7096\nTest\n66\n219\n85\n78\n114\n73\n82\n520\nValidation\n37\n130\n54\n36\n79\n28\n54\n310\nthe generated content remains a challenge that we attempt to\nresolve with this work.\nWhile research on memes expanded, their application in\nthe mental health domain remained limited. Yadav et al.\n(2023) extended meme analysis research towards the mental\nhealth domain with the release of the RESTORE dataset for\ndetecting fine-grain depressive symptoms in memes. Mazhar\net al. (2025) expand this research by utilizing a knowledge-\nfusion and retrieval framework for depression symptom\nidentification as well as introducing a dataset for detecting\nanxiety symptoms. We aim to expand on these works with a\nmore elaborate method by utilizing LLM agents.\nPrevious work on depressive symptom analysis has fo-\ncused mostly on textual data such as Tweets and Reddit\nposts, with limited work on multimodal content such as\nmemes (Yadav et al. 2023). Works such as Yadav et al.\n(2021, 2018); Benton, Mitchell, and Hovy (2017) have\nfocused on analyzing depressive symptoms by collecting\ndata from online social media platforms, mostly on tweets\nof users for detection or tracking of depression symp-\ntoms in users. Works such as Yates, Cohan, and Gohar-\nian (2017); Yadav et al. (2020) have served as important\nbenchmarks for analyzing textual depressive data. Typi-\ncally text-based depression symptom analysis works uti-\nlize simpler BERT-based methods with specialized mod-\nules (Zhang et al. 2023). Recently, Wang, Inkpen, and\nKirinde Gamaarachchige (2024) analyzed the utility of\nLLMs for estimation of the levels of depression by prompt-\ning LLMs with a questionnaire. Chen et al. (2024) have uti-\nlized LLMs for synthetic data creation in the form of in-\nterviews of depressed users highlighting the capabilities of\nLLMs to work in the mental health context. This work aims\nto extend depression analysis to the multimodal setting and\napplying advanced LLM-based methods, including multi-\nagent methods which have not yet been effectively utilized\nin depression related tasks.\n3\nThe RESTOREx Dataset\nWe introduce RESTOREx, a dataset for the multi-label clas-\nsification of seven fine-grained PHQ-9 depressive symptom\ncategories – Feeling Down (FD), Lack of Interest (LOI), Eat-\ning Disorder (ED), Sleeping Disorder (SD), Concentration\nProblem (CP), Low Self Esteem (LSE), and Self Harm (SH).\nWe provide expert annotated human explanations and con-\nduct extensive expert analysis of all generated content. Table\n1 provides label-wise distribution of RESTOREx.\n3.1\nDataset Curation and Filtering\nThe RESTOREx dataset is derived from the RESTORE\ndataset, which collects meme images from the social media\n"}, {"page": 3, "text": "(a) Misclassifications in Test Samples:\nOriginal- [FD, LOI], Updated- [FD, LSE]\n(b) Non-meme Images: image that corre-\nsponds to a quote\n(c) Inaccurate ‘Lack of Energy’ sam-\nples\nFigure 2: Examples of meme samples from RESTORE that were re-annotated for the curation of RESTOREx\nplatforms, Twitter and Reddit. The dataset consists of hu-\nman annotations for eight fine-grained depression symptom\ncategories in the test and validation subsets of the dataset.\nThe training subset is supplemented with automatically cu-\nrated samples using keyword-based search (such as ‘eating\ndisorder memes’, ‘feeling down memes’, etc.).\nIn the curation of the RESTOREx dataset, we identify\nand correct the following inconsistencies in the original\nRESTORE dataset: (i) Re-annotated the test and validation\nsets of the dataset with additional annotators to mitigate is-\nsues with misclassified labels as shown in Figure 2a. (ii) Re-\nmoved non-meme images such as quotes from the dataset\nas shown in Figure 2b. leading to a reduction of about 20%\nin the dataset size. (iii) Removed the ‘Lack of Energy’ label\ndue to the lack of accurate training labels as shown in Figure\n2c. This was done as all of the samples for this class in the\ntraining set (471 samples) were automatically curated with\nsignificant issues.\n3.2\nExplanations in RESTOREx\nTo enhance the utility of RESTOREx as a resource, we\nintroduce ground-truth human-written explanations for the\ntest subset (520 samples). This ground-truth explanation\nserves as a resource for future research to compare model-\ngenerated explanations with a human-annotated dataset. An\nexplanation by the human-annotators, through their nuanced\nthought process, encapsulates important aspects such as fig-\nurative language (for example, sarcasm and metaphors),\ncommonsense reasoning, and is grounded in their cultural\nawareness which allows explanations to be aware of the cul-\ntural phenomena of the time. These detail-rich explanations\nserve as a textual description of the cross-modal information\nin a meme image.\n3.3\nAnnotation Guidelines\nThe curation of the RESTOREx dataset consists of two an-\nnotation tasks for dataset re-annotation and filtering, and for\ncuration of human-written gold label explanations.\nTask 1: For a given meme, the annotators are required\nto determine one or more of the seven depressive symp-\ntoms conveyed in the meme according to PHQ-9 definitions\n(Kroenke, Spitzer, and Williams 2001). Any non-meme im-\nage is filtered in this process. This annotation is performed\nby two annotators trained on the task of fine-grained depres-\nsion symptom identification as per the guidelines of the orig-\ninal dataset curation (Yadav et al. 2023). The inter-annotator\nagreement was captured as the Krippendorff’s Alpha coef-\nficient (krippendorff 2004), a metric widely used for mea-\nsuring reliability in annotations for multi-label tasks. The\ncoefficient is obtained as 0.833 using MASI distance (Pas-\nsonneau 2006) as the distance function, representing strong-\nagreement between the annotators.\nTask 2: For each meme, the annotators are required to pro-\nvide the human-annotated ground-truth explanations in the\nprovided text field. The annotator is instructed to capture\ntheir thought process and incorporate details such as the use\nof figurative language, commonsense knowledge, cultural\nreferences, and visual cues in their provided explanations.\nThis annotation process is performed by two domain-expert\nannotators.\n3.4\nLLM Generated Explanations\nWe propose to extend the dataset further with Multimodal\nLLM generated explanations as a proxy for human-written\nannotations for large datasets where human annotation is\nnot feasible such as for the over 7000 samples in the train\ndataset. We generate explanations using three open-source\nand three closed-source LLMs and conduct a human analy-\nsis on these explanations to evaluate their utility as a proxy\nfor human-written explanations.\nAnalysis on LLM Generated Explanations\nWe focus on\nfive research questions in order to comprehensively judge\nthe quality of LLM generated explanations: RQ1: Are LLMs\nfluent?, RQ2: Are generated explanations relevant?, RQ3:\nDo LLMs capture figurative meaning?, RQ4: Are LLMs per-\nsuasive? and RQ5: Understanding the appeal of some ex-\nplanations. We conduct this analysis on six selected model\nexplanations and human-annotated explanations based on\na detailed questionnaire on a subset of test samples. We\nchoose three open source models – LLaVA 1.5, LLaVA-\nNeXT, MiniCPM-Vand three closed source models – GPT-\n4o, Claude 3.5 Sonnet and Gemini-2.0-flash. The results\nof this human analysis clearly establish the superiority of\nclosed-source LLMs such as Gemini (Team 2024) over\nopen-source models for the task of RESTOREx motivating\nour decision of utilizing such closed-source models as the\nbackbone of our methodology.\n4\nMethodology\nIn this section we propose the MAMA-Memeia framework, a\ncollaborative multi-agent discussion framework that utilizes\nclinical psychology principles to detect depressive symp-\ntoms. We first describe the domain expert’s perspective\non the task of depressive symptom classification and then\n"}, {"page": 4, "text": "Cultural Competence\nKnowledge of Depression\nAbility to Work with\nEmotional Content\nKnowledge and\nUnderstanding of Mental\nHealth Problems\nAbility to Foster Therapeutic\nAlliance\nAbility to Engage the\nSubject/Client (Meme\nCreator)\nidentify and describe the\nreasoning behind the following\ndepressive symptoms: 1. Loss of\nInterest: A decline... 7. Self-\nHarm: Frequent or recurrent...\nYou are an expert in understanding\ncultural and metaphorical\nreferences in memes. You can\nidentify and explain all cultural\nand metaphorical references.\nYou are an expert in understanding\nthe emotional context of a\ndepressive meme. You can identify\nand explain all emotions shown in\nthe meme.\nCultural Knowledge (Pc)\nEmotional Knowledge (Pe)\nDepression Knowledge (Pd)\nFigure 3: Cognitive Analytic Therapy (CAT) Competencies (Parry et al. 2021) adapted as guidelines form the basis for Multi-\nAspect Prompting. Multiple criteria combine to form each Knowledge Aspect. We design three Aspect-specific prompts – Pd\n(Depression Knowledge), Pe (Emotional Knowledge), and Pc (Cultural Knowledge) – to generate the corresponding explana-\ntions – Ed, Ee, and Ec. We use these explanations individually and concatenated as < Ed, Ee, Ec > for our experiments.\npresent our approach to integrating this perspective in a\nmulti-agent setup through multi-aspect prompting.\n4.1\nDomain Expert Perspective\nThe psychology underlying meme with mental health is cen-\ntered around emotional expressions and coping mechanisms\n(particularly, in the form of humor). Specifically, meme al-\nlows individuals to express complex emotions surrounding\ntheir mental health in a relatable manner through their hy-\npernarrativity (Wagener 2021). They provide a platform for\ncommunal discourse and validation for individuals in sim-\nilar situations by supporting a larger cultural framework to\nhelp process individual experiences (Wagener 2021).\nHumor can be used to create emotional distance and to\nhelp people process feelings. This enables them to cope with\nstress, depression, and anxiety, especially during important\ndevelopmental transitions (Sarink and Garc´ıa-Montes 2023;\nErickson and Feldstein 2006). While positive humor (affil-\niative, self-enhancing) is associated with stable psycholog-\nical adjustment, negative humor (especially self-defeating\nhumor seen in memes) is associated with higher depres-\nsive symptoms (Erickson and Feldstein 2006). Therefore,\nalthough it helps tops to build social relationships and nav-\nigate depressive symptoms, it harms self-esteem in the pro-\ncess (Erickson and Feldstein 2006). Specific to memes, hu-\nmor provides an avenue for emotional catharsis by channel-\ning dark or uncomfortable topics toward humor (Sarink and\nGarc´ıa-Montes 2023).\nCognitive Analytic Therapy for Understanding Depres-\nsive Memes\nCognitive Analytic Therapy (CAT) is a form\nof talking therapy that focuses primarily on identifying pat-\nterns of thinking, feeling, and behavior (NHS 2025). It is\nwidely used in the clinical therapy setting for people living\nwith depression, anxiety, or eating problems, who self-harm\nor have personal or relationship problems. In this study, we\nadapted CAT - General Therapeutic Competencies (Parry\net al. 2021) to formulate evaluative guidelines for interpret-\ning memes on mental health themes. It is highly applicable\nto the task due to its structured approach to categorizing and\nassessing both larger psychological patterns and more subtle\nemotional nuances and risk assessments. We leverage eight\nCAT criteria focusing on meme understanding – (i) Knowl-\nedge and Understanding of Mental Health Problems, (ii)\nKnowledge of Depression, (iii) Cultural Competence, (iv)\nAbility to Engage the Subject/Client (Meme Creator), (v)\nAbility to Foster Therapeutic Alliance, (vi) Ability to Work\nwith Emotional Content, (vii) Ability to Assess and Manage\nRisk of Self-Harm, (viii) Knowledge of Ethical Guidelines.\n4.2\nMulti-Aspect Prompting\nWe ground our approach in the CAT criteria described in\nsection 4.1. These guidelines highlight three major aspects\nof knowledge required for understanding depressive memes:\n(i) Depression Knowledge - This aspect deals with provid-\ning knowledge about the specific depressive symptoms to\nthe model. We provide this knowledge with the use of defi-\nnitions for the seven symptoms as laid out by (Yadav et al.\n2023). These definitions ensure that the model avoids misin-\nterpretation of the depressive symptoms.\n(ii) Emotional Knowledge - A key consideration while de-\ntermining depressive symptoms according to the CAT guide-\nlines is the consideration of the emotional state of the user.\nTo this effect, we include this knowledge aspect to ensure\nthat the model considers the emotional states that may be\nassociated with depressive themes in the meme.\n(iii) Cultural Knowledge - This knowledge aspect aims\nto provide information about important cultural phenomena\nsuch as pop culture references or metaphorical interpreta-\ntions that form a key part of meme understanding. With the\ninclusion of such knowledge it is expected that the model\nwould be able to recognize the figurative language of the\nmeme across the textual and visual modalities.\nBased on these, we design three Aspect-specific prompts\nPd (Depression Knowledge), Pe (Emotional Knowledge),\nand Pc (Cultural Knowledge) as described in Figure 3.\n"}, {"page": 5, "text": "FD : 0.8, CP : 0.6\nDepression Knowledge (Pd)\nCultural Knowledge (Pc)\nEmotional Knowledge (Pe)\nThe meme depicts a common\nanxiety cycle where an initial\nthreat (Coronavirus can be deadly)\ncauses panic...\n FD : 0.9, LSE : 0.8, SH : 0.85\nThis meme uses the popular 'Panik\nKalm Panik' format to illustrate a\ncomplex relationship with\nmortality and self-worth...\nFD : 0.7, LSE : 0.6, CP : 0.5\nThe meme uses a sequence of\npanels to depict a cycle of panic and\ncalmness triggered by thoughts\nabout a potentially deadly...\nFD : 0.8, LSE : 0.85, SH : 0.7, CP: 0.6\nConsidering the other agents'\nanalysis, particularly the\ninterpretation of 'I won't die' ...\nFD : 0.9, LSE : 0.8, SH : 0.85, CP: 0.7\nAfter reviewing other\ninterpretations, I'll refine my\nanalysis....\nFD : 0.8, LSE : 0.6, CP : 0.5\nUpon reviewing the previous\nanalyses, the image prominently\nfeatures...\nR\nCollaborative\nDiscussion\nA1\nA2\nA3\nIndependent Ideation\nRound 0\nRound 1\nRound N\np1\np2\np3\np1\np2\np3\ne1\ne2\ne3\ne1\ne2\ne3\nPdsc\nPini\n(pj, cj, ej)j!=i\nHistory, hi\npi,j : ci,j\nFeeling Down : 0.9\nLow Self-Esteem  : 0.85 \nSelf-Harm : 0.9 \nConcentration Problem : 0.7\npi,j : ci,j\nFeeling Down : 0.85\nLow Self-Esteem  : 0.8 \nSelf-Harm : 075 \nConcentration Problem : 0.6\npi,j : ci,j\nFeeling Down : 0.9\nLow Self-Esteem  : 0.85 \nSelf-Harm : 0.9 \nConcentration Problem : 0.7\nConsensus Resolution\nFeeling Down\nLow Self-Esteem\nSelf-Harm\nConcentration\nProblem\nFeeling Down\nLow Self-\nEsteem\nSelf-Harm\nWeighted Vote\n0.88\n0.82\n0.83\n0.67\np1\np2\np3\nFigure 4: Overview of the MAMA-Memeia framework with Gemini-2.0-flash\n, Claude 3.5 Sonnet\n, GPT-4o\n, consisting\nof three phases- (1) Independent Ideation: Agents generate their initial predictions and reasoning, (2) Collaborative Discussion:\nAgents collaborate to discuss and reconsider their predictions, (3) Consensus Resolution: Weighted vote on final confidence\nestimates to determine predicted labels.\n4.3\nMAMA-Memeia: A Multi-Agent Multi-Aspect\nInference Framework\nWe introduce the MAMA-Memeia framework, which builds\nupon the Multi-Aspect Prompting setup by utilizing mul-\ntiple agents that capture specific knowledge of depressive\nmemes in the prediction of the final labels. Our approach is\ninspired from multi-agent works (Wu et al. 2024; Du et al.\n2024; Chen, Saha, and Bansal 2024) which highlights the\nefficacy of multi-agent setups.\nWe utilize this knowledge in combination with our study\nof Multi-Aspect Prompting by deploying three recent state-\nof-the-art LLMs - GPT-4o (OpenAI 2024), Claude-3.5 Son-\nnet (Anthropic 2024) and Gemini-2.0-flash (Team 2024) - in\na multi-agent setup. Our choice of these models is based on\nthe human analysis provided in subsection 3.4. The MAMA-\nMemeia framework consists of three phases: Independent\nIdeation, Collaborative Discussion and Consensus Resolu-\ntion. It is demonstrated in Figure 4 and described in detail\nbelow for n agents.\nIndependent Ideation\nThe first step in the MAMA-\nMemeia framework is the generation of initial symptom pre-\ndictions and reasoning for the given meme from each agent.\nFormally, given a list of agents A = {A1, A2, ..., An} of\nsize n, each agent Ai ∈A generates an initial prediction,\np(0)\ni , which is a list of si symptoms predicted by it. The\nstudy (Xiong et al. 2024) has shown the effectiveness of\nLLM-derived confidence scores, therefore, we also gener-\nate confidence estimates, c(0)\ni,j ∈[0, 1] ∀j ∈si. Each agent,\nAi, also generates an initial explanation, e(0)\ni\nthat explains\nthe thought process behind it’s predictions. To achieve this,\nAi is designated a Aspect-Specific System Prompt, Pa,i ∈\n{Pd, Pe, Pc}, for each agent to focus on one knowledge\naspect. Each agent Ai is then prompted with the Aspect-\nspecific Prompt, Pa,i, the meme image, Mimg, and the user\nprompt, Pusr (detailed prompts in suppl.). For each agent,\nAi, this can be formulated as:\np(0)\ni , c(0)\ni , e(0)\ni\n= Ai(Pa,i, Mimg, Pusr)\n(1)\nCollaborative Discussion\nIn the next phase of our frame-\nwork, all agents undergo R rounds of collaborative discus-\nsion. This is done to ensure each agent is fairly exposed to\nthe thought process of other agents before coming up with\ntheir final predictions. For each discussion round, r, every\nagent is provided a Discussion prompt, Pdsc, which has mul-\ntiple characteristics. The discussion prompt first introduces\nthe agents to the idea that they are in a collaborative envi-\nronment by asking them to review the responses of other\nagents and then reconsider their response. We call this the\ninitiator prompt, Pini. Secondly, the discussion prompt for\ndiscussion round, r, consists of the responses of the other\nagents for round, r −1 (for r = 1 this refers to the Indepen-\ndent Ideation phase). Each agent is provided with both ex-\nplanation, e(r−1)\nj\n, as well as predictions, p(r−1)\nj\n, generated\nby the other agents, A \\ Ai. This is supplemented with the\nconfidence estimates, c(r−1)\nj\n, for each prediction in p(r−1)\nj\n.\nThe availability of all three components, explanation, pre-\ndictions, and confidence estimates, allows the agent to con-\ndition on the reasoning and confidence associated with the\npredictions before it reconsiders its initial judgement. The\nround r discussion prompt, for agent Ai, is:\nP (r)\ndsc,i = Pini +\n\r\rn\nj=1,j̸=i{e(r−1)\nj\n, p(r−1)\nj\n, c(r−1)\nj\n}\nThis discussion prompt is then passed along with the entire\nconversation history of the agent, h(r)\ni , to obtain the predic-\ntions, confidence estimates and explanation of the agent for\nround r as:\np(r)\ni , c(r)\ni , e(r)\ni\n= Ai(P (r)\ndsc,i, h(r)\ni )\n(2)\n"}, {"page": 6, "text": "Table 2: Comparison of MAMA-Memeia with unimodal and\nmultimodal baselines along with previous state-of-the-art.\nModel\nMacro-F1\nWeighted-F1\nUnimodal Text\nOCR:\nBERT (Devlin et al. 2019)\n62.02\n62.63\nMentalBERT (Ji et al. 2022)\n63.77\n64.47\nBART (Lewis et al. 2020)\n44.71\n49.46\nMentalBART (Yang et al. 2023)\n61.76\n62.43\nExplanation:\nBERT\n63.39\n63.96\nMentalBERT\n64.62\n65.27\nBART\n55.81\n57.11\nMentalBART\n64.96\n64.30\nUnimodal Image\nViT (Dosovitskiy et al. 2021)\n34.96\n39.04\nResNet (He et al. 2015)\n27.14\n33.46\nEfficientNet (Tan and Le 2019)\n25.18\n31.61\nMultimodal\nImage + OCR\nCLIP (Radford et al. 2021)\n45.83\n48.09\nVisualBERT (Li et al. 2019)\n62.70\n63.67\nViT + BERT\n38.57\n42.24\nImage + Explanation\nCLIP\n39.23\n42.56\nVisualBERT\n62.37\n62.25\nViT + BERT\n37.78\n40.58\nPrevious SOTA\nYadav et al. (2023) †\n65.18\n64.67\nMAMA-Memeia\n72.73\n72.45\n∆MAMA-Memeia - †\n↑7.55%\n↑7.78%\nConsensus Resolution\nAt the end of N = R + 1 rounds,\nwe determine the final predictions through our consensus\nresolution algorithm which performs a weighted vote be-\ntween the agents based on the confidence estimates. For a\ngiven threshold, t, the final labels, L, are determined as fol-\nlows:\nL =\n\"\nj\n\f\f\f\f\f ( 1\nn\nn\nX\ni=1\nc(N)\ni,j I{j ∈p(N)\ni\n}) > t ]\n#\n(3)\nThe consensus resolution algorithm is used in order to\neliminate labels for which the agents have low confidence.\nIt also ensures that there is a balance between the agents\nand extremely high confidence of one agent does not bias\nMAMA-Memeia.\n5\nExperiments: Benchmarking RESTOREx\nBaselines\nTo establish the effectiveness of the MAMA-\nMemeia framework, we first compare its performance to\nseveral relevant unimodal and multimodal baseline architec-\ntures as described in Table 2. These baselines use the fol-\nlowing models: (i) Unimodal Text: BERT, MentalBERT,\nBART, MentalBART (ii) Unimodal Image: ViT, ResNet,\nEfficientNet (iii) Multimodal: CLIP, VisualBERT, BERT +\nViT, SOTA (Yadav et al. 2023)\nBaseline Results\nAs shown in Table 2, MAMA-Memeia\noutperforms the previous State-of-the-Art results of (Ya-\ndav et al. 2023) by 7.55% in macro-F1 score and 7.78%\n   MAMA-Memeia : FD, SH,  LSE\n   Gemini : FD, SH,  LSE, LOI\n   Claude : FD, SH,  LSE\n \n  GPT-4o : FD, SH\nFigure 5: Meme example with predicted labels from GPT-\n4o, Claude 3.5 Sonnet, Gemini-2.0-flash and MAMA-\nMemeia.\nin weighted-F1 score. The setup by Yadav et al. (2023) uti-\nlizes multimodal fusion based on conditional adaptive gat-\ning with pre-trained ResNet and BERT models which are\nfine-tuned for the task of depressive symptom classification.\nThe superior performance of MAMA-Memeia highlights the\neffectiveness of Large Language Model based approaches\nover these traditional pre-trained models. This is further con-\nfirmed compared to the fine-tuned unimodal and multimodal\nbaselines which sit well below the benchmark set by MAMA-\nMemeia. Further, Table 2 highlights the importance of the\ntextual modality for the fine-grained depressive symptom\nanalysis task as shown by the sharp improvement in per-\nformance from the unimodal image setup to the unimodal\ntext setup. The utility of the LLM generated explanations is\nalso established as a consistent increase in performance can\nbe seen by substituting OCR with LLM generated explana-\ntions as the textual modality as seen with the BART model\nimproving in macro-F1 from 44.71% with OCR to 55.81%\nwith explanations.\nExperimentation with Multimodal LLMs\nWe experi-\nment with a variety of different open-source and closed-\nsource models to provide explanations for memes as shown\nin Table 3. These setups are compared in both Single-Agent\nand Multi-Agent setups, as well as with a variety of prompt-\ning setups including the Multi-Aspect prompting (cf. Sec-\ntion 4.2). Finally, we compare MAMA-Memeia and other\nLLM based approaches with results derived using gold-label\nhuman-written explanations. For a representative variety in\nthe generated content, we put careful consideration into the\nchoice of our models. We take particular effort to utilise\na mix of open-source and closed-source models. We also\nensure a variety in the base foundational LLMs that these\nMultimodal LLMs are built upon. Based on these crite-\nrias, we generate explanations from six popular Multimodal\nLLMs - LLaVA 1.5 (Liu et al. 2023), LLaVA-NeXT (Liu\net al. 2024), MiniCPM-V (Hu et al. 2024), GPT-4o (Ope-\nnAI 2024), Claude 3.5 Sonnet (Anthropic 2024), Gemini-\n2.0-flash (Team 2024).\nResults and Ablation\nAs seen in Table 3, the performance\nof closed-source models such as Claude 3.5 Sonnet is sig-\nnificantly better compared to the open-source models such\nas LLaVA 1.5. The significant performance boost for the\nGPT-4o and Gemini-2.0-flash models with the Multi-Aspect\nPrompting setup indicates towards the effectiveness of these\nknowledge-based prompting setups. This is also noted with\n"}, {"page": 7, "text": "Table 3: Comparison of MAMA-Memeia with Mulitmodal LLM based single-agent setups and Human-annotated explanations.\nMAMA-Memeia is the best performing method in macro, micro and weighted F1 scores. Vanilla LLM Explanations refer to\nthe prompting setup without the inclusion of CAT knowledge based prompts. FD: Feeling Down, LOI: Lack of Interest, SH:\nSelf-Harm, ED: Eating Disorder, LSE: Low Self-Esteem, CP: Concentration Problem, SD: Sleeping Disorder.\nMethod\nFD\nLOI\nSH\nED\nLSE\nCP\nSD\nMacro-F1\nMicro-F1\nWeighted-F1\nHuman Explanations\n63.00\n47.00\n77.00\n72.00\n54.00\n57.00\n81.00\n65.00\n62.00\n64.00\nVanila LLM Explanations\nOpen-Source LLMs\nLLaVA 1.5\n55.00\n37.00\n34.00\n48.00\n38.00\n47.00\n67.00\n47.00\n48.00\n48.00\nLLaVA-NeXT\n62.00\n43.00\n60.00\n71.00\n46.00\n53.00\n79.00\n59.00\n57.00\n59.00\nMiniCPM-V\n62.00\n40.00\n53.00\n73.00\n51.00\n53.00\n78.00\n59.00\n58.00\n59.00\nClosed-Source LLMs\nGPT-4o\n63.00\n39.00\n58.00\n73.00\n46.00\n54.00\n86.00\n60.00\n57.00\n60.00\nCluade 3.5 Sonnet\n70.78\n48.25\n77.71\n81.01\n54.35\n57.92\n88.76\n68.39\n66.19\n68.74\nGemini-2.0-flash\n63.09\n32.02\n75.86\n75.47\n49.22\n60.47\n80.22\n62.34\n58.19\n62.55\nMulti-Aspect Prompting\nDepression Knowledge (Pd)\nGPT-4o\n65.13\n53.33\n79.52\n78.21\n45.02\n58.46\n86.22\n66.56\n62.67\n65.66\nCluade 3.5 Sonnet\n68.27\n45.45\n77.30\n82.93\n48.46\n60.09\n84.71\n66.74\n63.60\n66.75\nGemini-2.0-flash\n66.40\n46.60\n73.85\n74.21\n55.86\n65.92\n78.95\n65.97\n64.93\n65.99\nCultural Knowledge (Pc)\nGPT-4o\n69.55\n49.43\n69.50\n79.22\n49.87\n60.60\n86.06\n66.31\n64.67\n66.60\nCluade 3.5 Sonnet\n71.09\n45.57\n72.99\n83.23\n44.95\n56.67\n85.21\n65.67\n62.61\n66.31\nGemini-2.0-flash\n66.77\n38.96\n61.88\n75.74\n45.55\n53.46\n76.50\n59.84\n57.99\n61.04\nEmotional Knowledge (Pe)\nGPT-4o\n65.52\n48.61\n76.39\n76.39\n49.87\n68.57\n83.87\n67.03\n64.36\n66.31\nCluade 3.5 Sonnet\n62.27\n40.14\n70.73\n80.77\n42.04\n47.39\n86.75\n61.44\n56.62\n61.33\nGemini-2.0-flash\n66.54\n48.43\n73.47\n77.78\n53.98\n65.93\n79.37\n66.50\n64.92\n66.33\nCombined (Pd + Pc + Pe)\nGPT-4o\n64.94\n51.37\n79.52\n79.77\n44.73\n59.69\n86.75\n66.68\n62.62\n65.74\nCluade 3.5 Sonnet\n67.65\n45.45\n76.83\n83.64\n49.53\n58.72\n84.71\n66.65\n63.51\n66.62\nGemini-2.0-flash\n65.28\n48.18\n72.73\n76.83\n53.71\n66.67\n78.95\n66.05\n64.41\n65.71\nMAMA-Memeia\n72.46\n52.27\n76.06\n80.50\n59.87\n77.37\n90.57\n72.73\n71.15\n72.45\n−Aspect-specific Prompting\n70.95\n49.70\n80.28\n82.12\n58.27\n73.91\n87.90\n71.88\n70.31\n71.51\nthe ablation with removing aspect-specific prompting (refer-\nring to the Multi-Aspect setup). Compared to the Human Ex-\nplanations, the MAMA-Memeia framework achieves an im-\nprovement of more than 8% and Claude 3.5 Sonnet achieves\nan improvement of more than 4% in weighted-F1 score sup-\nporting our proposal for utilizing LLM generated explana-\ntions as an automated and low-resource alternative to human\nannotations.\nQualitative Analysis\nWe perform a qualitative analysis of\nthe outputs of the GPT-4o, Claude 3.5 Sonnet and Gemini-\n2.0-flash models along with the MAMA-Memeia framework\nto infer trends in predictions made by these models. An ex-\nample of this analysis is provided in Figure 5. As seen in\nFigure 5, we observe the that the Gemini-2.0-flash model is\nprone to over-prediction while the GPT-4o model is prone\nto under-prediction. This behavior is partly explained when\nobserving the length of the explanations generated by these\nmodels where the Gemini-2.0-flash model consistently out-\nputs the lengthiest explanations among the set. However,\ngiven the black-box nature of these models, further analysis\nis required for detailed understanding of this phenomenon.\nThis offers an explanation for the improved performance of\nthe MAMA-Memeia framework where the averaging of the\nconfidence of each of these models yields better results.\nAnalyzing the patterns in the multiple rounds of debate\nbetween the models, we observe that the models frequently\ncorrect and influence each other through their detailed expla-\nnations. For instance, as in Figure 4, only one of the mod-\nels (Claude 3.5 Sonnet) correctly predicts the symptom of\nSelf-Harm and corrects the other two models in their pre-\ndictions over the rounds of debate. This self-correction ten-\ndency stands out as a major strength of MAMA-Memeia.\n6\nConclusion\nWe introduced RESTOREx a dataset for fine-grained classi-\nfication of depressive symptoms in memes augmented with\nground-truth human explanations and LLM generated expla-\nnations for memes. We presented MAMA-Memeia, a Multi-\nAgent Multi-Aspect inferencing framework, grounded in lit-\nerature in Psychology, for classifying depressive symptoms\nin multimodal meme data. We demonstrated that MAMA-\nMemeia consistently outperformed traditional methods and\nimproved upon single-agent LLM approaches for the task.\nWhile we utilize closed-source models for our MAMA-\nMemeia framework, we look forward to developing methods\nfor effective utilization of open-source LLMs in future work\ngiven the pressing need for open and transparent research on\nLLMs in sensitive domains such as mental health.\n"}, {"page": 8, "text": "Ethics Statement\nThis work builds upon the original RESTORE dataset (Yadav\net al. 2023), which contains publicly available meme images\ncollected from social media; the original authors ensured\nthat all data were anonymized. Because our study concerns\nthe sensitive domain of mental health content, we emphasize\nthat the proposed system identifies depressive symptoms ex-\npressed within meme content only and is not intended for di-\nagnosing or profiling any individual. Our multi-agent frame-\nwork leverages large language models that may introduce\nadditional biases, and we release all methods and data re-\nsources solely for research purposes. We caution against\ndeploying such LLM-based systems in high-stakes men-\ntal health or content-moderation settings without substan-\ntial expert human oversight, particularly for critical symp-\ntoms such as self-harm. We further acknowledge that the\nclosed-source models used in our framework provide lim-\nited transparency regarding their architectures and training\ndata, which constrains interpretability. Finally, because our\napproach performs inference using existing pretrained mod-\nels rather than training large models from scratch, its envi-\nronmental impact is comparatively limited.\nReferences\nAgarwal, S.; Sharma, S.; Nakov, P.; and Chakraborty, T. 2024.\nMemeMQA: Multimodal Question Answering for Memes via\nRationale-Based Inferencing.\nIn Findings of the Association\nfor Computational Linguistics: ACL 2024, 5042–5078. Bangkok,\nThailand.\nAkram, U.; Drabble, J.; Cau, G.; Hershaw, F.; Rajenthran, A.;\nLowe, M.; Trommelen, C.; and Ellis, J. G. 2020. Exploratory study\non the role of emotion regulation in perceived valence, humour, and\nbeneficial use of depressive internet memes in depression. Scien-\ntific Reports, 10(1): 899.\nAnthropic. 2024. Introducing Claude 3.5 Sonnet.\nBenton, A.; Mitchell, M.; and Hovy, D. 2017. Multitask Learn-\ning for Mental Health Conditions with Limited Social Media Data.\nIn Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics: Volume 1, Long\nPapers, 152–162. Valencia, Spain.\nBrown, H. 2022.\nThe surprising power of internet memes —\nbbc.com. [Accessed 15-10-2024].\nChen, J.; Saha, S.; and Bansal, M. 2024. ReConcile: Round-Table\nConference Improves Reasoning via Consensus among Diverse\nLLMs. In Proceedings of the 62nd Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers),\n7066–7085. Bangkok, Thailand.\nChen, Z.; Deng, J.; Zhou, J.; Wu, J.; Qian, T.; and Huang, M.\n2024.\nDepression Detection in Clinical Interviews with LLM-\nEmpowered Structural Element Graph. In Proceedings of the 2024\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (Vol-\nume 1: Long Papers), 8181–8194. Mexico City, Mexico.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long and Short\nPapers), 4171–4186. Minneapolis, Minnesota.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai,\nX.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.;\nGelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth\n16x16 Words: Transformers for Image Recognition at Scale. In\nInternational Conference on Learning Representations.\nDu, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; and Mordatch, I.\n2024.\nImproving factuality and reasoning in language models\nthrough multiagent debate. In Proceedings of the 41st International\nConference on Machine Learning, ICML’24. JMLR.org.\nErickson, S. J.; and Feldstein, S. W. 2006. Adolescent Humor and\nits Relationship to Coping, Defense Strategies, Psychological Dis-\ntress, and Well-Being. Child Psychiatry and Human Development,\n37(3): 255–271.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Deep Residual Learn-\ning for Image Recognition. 2016 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 770–778.\nHu, S.; Tu, Y.; Han, X.; Cui, G.; He, C.; Zhao, W.; Long, X.; Zheng,\nZ.; Fang, Y.; Huang, Y.; Zhang, X.; Thai, Z. L.; Wang, C.; Yao, Y.;\nZhao, C.; Zhou, J.; Cai, J.; Zhai, Z.; Ding, N.; Jia, C.; Zeng, G.; da-\nhai li; Liu, Z.; and Sun, M. 2024. MiniCPM: Unveiling the Poten-\ntial of Small Language Models with Scalable Training Strategies.\nIn First Conference on Language Modeling.\nJha, P.; Jain, R.; Mandal, K.; Chadha, A.; Saha, S.; and Bhat-\ntacharyya, P. 2024a.\nMemeGuard: An LLM and VLM-based\nFramework for Advancing Content Moderation via Meme Inter-\nvention. In Proceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: Long Papers),\n8084–8104. Bangkok, Thailand.\nJha, P.; Maity, K.; Jain, R.; Verma, A.; Saha, S.; and Bhattacharyya,\nP. 2024b. Meme-ingful Analysis: Enhanced Understanding of Cy-\nberbullying in Memes Through Multimodal Explanations. In Pro-\nceedings of the 18th Conference of the European Chapter of the As-\nsociation for Computational Linguistics (Volume 1: Long Papers),\n930–943. St. Julian’s, Malta.\nJi, S.; Zhang, T.; Ansari, L.; Fu, J.; Tiwari, P.; and Cambria, E.\n2022. MentalBERT: Publicly Available Pretrained Language Mod-\nels for Mental Healthcare. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, 7184–7190. Mar-\nseille, France.\nJoshi, S.; Ilievski, F.; and Luceri, L. 2024. Contextualizing Internet\nMemes Across Social Media Platforms. In Companion Proceed-\nings of the ACM Web Conference 2024, 1831–1840. New York,\nNY, USA: ACM. ISBN 9798400701726.\nKiela, D.; Firooz, H.; Mohan, A.; Goswami, V.; Singh, A.; Ring-\nshia, P.; and Testuggine, D. 2020. The Hateful Memes Challenge:\nDetecting Hate Speech in Multimodal Memes. In Advances in Neu-\nral Information Processing Systems, volume 33, 2611–2624. Cur-\nran Associates, Inc.\nkrippendorff, K. 2004.\nMeasuring the Reliability of Qualitative\nText Analysis Data. Quality and Quantity, 38(6): 787–800.\nKroenke, K.; Spitzer, R. L.; and Williams, J. B. 2001. The PHQ-9:\nvalidity of a brief depression severity measure. Journal of General\nInternal Medicine, 16(9): 606–613.\nLewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.;\nLevy, O.; Stoyanov, V.; and Zettlemoyer, L. 2020. BART: Denois-\ning Sequence-to-Sequence Pre-training for Natural Language Gen-\neration, Translation, and Comprehension. In Proceedings of the\n58th Annual Meeting of the Association for Computational Lin-\nguistics, 7871–7880. Online.\nLi, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.-J.; and Chang, K.-W.\n2019. VisualBERT: A Simple and Performant Baseline for Vision\nand Language. arXiv:1908.03557.\n"}, {"page": 9, "text": "Liu, H.; Li, C.; Li, Y.; Li, B.; Zhang, Y.; Shen, S.; and Lee, Y. J.\n2024. LLaVA-NeXT: Improved reasoning, OCR, and world knowl-\nedge.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual Instruction\nTuning.\nLondon, S. C. i. v. p. o. p. a. E. i. 2024. Research: Photo posts pro-\nduce significantly more engagement than link posts on Facebook.\nMazhar, A.; hasan shaik, Z.; Srivastava, A.; Ruhnke, P.; Vad-\ndavalli, L.; Katragadda, S. K.; Yadav, S.; and Akhtar, M. S. 2025.\nFigurative-cum-Commonsense Knowledge Infusion for Multi-\nmodal Mental Health Meme Classification. arXiv:2501.15321.\nMei, J.; Chen, J.; Lin, W.; Byrne, B.; and Tomalin, M. 2024. Im-\nproving Hateful Meme Detection through Retrieval-Guided Con-\ntrastive Learning. In Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Pa-\npers), 5333–5347. Bangkok, Thailand.\nMishra, S.; Suryavardan, S.; Patwa, P.; Chakraborty, M.; Rani, A.;\nReganti, A. N.; Chadha, A.; Das, A.; Sheth, A. P.; Chinnakotla, M.;\nEkbal, A.; and Kumar, S. 2023. Memotion 3: Dataset on sentiment\nand emotion analysis of codemixed Hindi-English Memes. In DE-\nFACTIFY@AAAI.\nNHS. 2025.\nCognitive analytic therapy (CAT).\nAccessed on\n15.02.2025.\nOpenAI. 2024. GPT-4o System Card. arXiv:2410.21276.\nParry, G.; Bennett, D.; Roth, A. D.; and Kellett, S. 2021.\nDe-\nveloping a competence framework for cognitive analytic therapy.\nPsychology and Psychotherapy: Theory, Research and Practice,\n94(S1): 151–170.\nPassonneau, R. 2006. Measuring Agreement on Set-valued Items\n(MASI) for Semantic and Pragmatic Annotation. In Proceedings\nof the Fifth International Conference on Language Resources and\nEvaluation (LREC’06). Genoa, Italy.\nPramanick, S.; Dimitrov, D.; Mukherjee, R.; Sharma, S.; Akhtar,\nM. S.; Nakov, P.; and Chakraborty, T. 2021. Detecting Harmful\nMemes and Their Targets. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021, 2783–2796. Online.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agar-\nwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.;\nand Sutskever, I. 2021. Learning Transferable Visual Models From\nNatural Language Supervision. arXiv:2103.00020.\nSarink, F.; and Garc´ıa-Montes, J. 2023. Humor interventions in\npsychotherapy and their effect on levels of depression and anxiety\nin adult clients, a systematic review. Frontiers in Psychiatry, 13:\n1049476.\nSharma, C.; Bhageria, D.; Scott, W.; PYKL, S.; Das, A.;\nChakraborty, T.; Pulabaigari, V.; and Gamb¨ack, B. 2020. SemEval-\n2020 Task 8: Memotion Analysis- the Visuo-Lingual Metaphor! In\nProceedings of the Fourteenth Workshop on Semantic Evaluation,\n759–773. Barcelona (online).\nSharma, S.; Agarwal, S.; Suresh, T.; Nakov, P.; Akhtar, M. S.; and\nChakraborty, T. 2023a.\nWhat Do You MEME? Generating Ex-\nplanations for Visual Semantic Role Labelling in Memes.\nPro-\nceedings of the AAAI Conference on Artificial Intelligence, 37(8):\n9763–9771.\nSharma, S.; S, R.; Arora, U.; Akhtar, M. S.; and Chakraborty, T.\n2023b. MEMEX: Detecting Explanatory Evidence for Memes via\nKnowledge-Enriched Contextualization. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 5272–5290. Toronto, Canada.\nTan, M.; and Le, Q. 2019. EfficientNet: Rethinking Model Scaling\nfor Convolutional Neural Networks. In Proceedings of the 36th In-\nternational Conference on Machine Learning, 6105–6114. PMLR.\nISSN: 2640-3498.\nTeam, G. 2024. Gemini: A Family of Highly Capable Multimodal\nModels. arXiv:2312.11805.\nWagener, A. 2021. The Postdigital Emergence of Memes and GIFs:\nMeaning, Discourse, and Hypernarrative Creativity.\nPostdigital\nScience and Education, 3: 831–850.\nWang, Y.; Inkpen, D.; and Kirinde Gamaarachchige, P. 2024. Ex-\nplainable Depression Detection Using Large Language Models on\nSocial Media Data. In Proceedings of the 9th Workshop on Com-\nputational Linguistics and Clinical Psychology (CLPsych 2024),\n108–126. St. Julians, Malta.\nWang, Y.; Li, Y.; Gui, X.; Kou, Y.; and Liu, F. 2019. Culturally-\nEmbedded Visual Literacy: A Study of Impression Management\nvia Emoticon, Emoji, Sticker, and Meme on Social Media in China.\nProc. ACM Hum.-Comput. Interact., 3(CSCW).\nWu, Q.; Bansal, G.; Zhang, J.; Wu, Y.; Li, B.; Zhu, E. E.; Jiang, L.;\nZhang, X.; Zhang, S.; Awadallah, A.; White, R. W.; Burger, D.; and\nWang, C. 2024. AutoGen: Enabling Next-Gen LLM Applications\nvia Multi-Agent Conversation. In COLM 2024.\nXiong, M.; Hu, Z.; Lu, X.; LI, Y.; Fu, J.; He, J.; and Hooi, B. 2024.\nCan LLMs Express Their Uncertainty? An Empirical Evaluation\nof Confidence Elicitation in LLMs. In The Twelfth International\nConference on Learning Representations.\nYadav, S.; Caragea, C.; Zhao, C.; Kumari, N.; Solberg, M.; and\nSharma, T. 2023. Towards Identifying Fine-Grained Depression\nSymptoms from Memes. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics (Volume 1:\nLong Papers), 8890–8905. Toronto, Canada.\nYadav, S.; Chauhan, J.; Sain, J. P.; Thirunarayan, K.; Sheth, A.; and\nSchumm, J. 2020. Identifying depressive symptoms from tweets:\nFigurative language enabled multitask learning framework. arXiv\npreprint arXiv:2011.06149.\nYadav, S.; Ekbal, A.; Saha, S.; Bhattacharyya, P.; and Sheth, A.\n2018. Multi-Task Learning Framework for Mining Crowd Intel-\nligence towards Clinical Treatment. In Proceedings of the 2018\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Vol-\nume 2 (Short Papers), 271–277. New Orleans, Louisiana.\nYadav, S.; Lokala, U.; Daniulaityte, R.; Thirunarayan, K.; Lamy, F.;\nand Sheth, A. 2021. “When they say weed causes depression, but\nit’s your fav antidepressant”: Knowledge-aware attention frame-\nwork for relationship extraction. PLOS ONE, 16(3): 1–18.\nYang, K.; Zhang, T.; Kuang, Z.; Xie, Q.; and Ananiadou, S. 2023.\nMentalLLaMA: Interpretable Mental Health Analysis on Social\nMedia with Large Language Models. arXiv:2309.13567.\nYates, A.; Cohan, A.; and Goharian, N. 2017. Depression and Self-\nHarm Risk Assessment in Online Forums. In Proceedings of the\n2017 Conference on Empirical Methods in Natural Language Pro-\ncessing, 2968–2978. Copenhagen, Denmark.\nZhang, T.; Yang, K.; Alhuzali, H.; Liu, B.; and Ananiadou, S.\n2023. PHQ-aware depressive symptoms identification with simi-\nlarity contrastive learning on social media. Information Processing\n& Management, 60(5): 103417.\nZhong, Y.; and Baghel, B. K. 2024. Multimodal Understanding of\nMemes with Fair Explanations. In 2024 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops (CVPRW),\n2007–2017. Seattle, WA, USA: IEEE. ISBN 9798350365474.\n"}]}