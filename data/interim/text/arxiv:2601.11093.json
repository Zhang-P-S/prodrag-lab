{"doc_id": "arxiv:2601.11093", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.11093.pdf", "meta": {"doc_id": "arxiv:2601.11093", "source": "arxiv", "arxiv_id": "2601.11093", "title": "Integrity Shield A System for Ethical AI Use & Authorship Transparency in Assessments", "authors": ["Ashish Raj Shekhar", "Shiven Agarwal", "Priyanuj Bordoloi", "Yash Shah", "Tejas Anvekar", "Vivek Gupta"], "published": "2026-01-16T08:44:58Z", "updated": "2026-01-16T08:44:58Z", "summary": "Large Language Models (LLMs) can now solve entire exams directly from uploaded PDF assessments, raising urgent concerns about academic integrity and the reliability of grades and credentials. Existing watermarking techniques either operate at the token level or assume control over the model's decoding process, making them ineffective when students query proprietary black-box systems with instructor-provided documents. We present Integrity Shield, a document-layer watermarking system that embeds schema-aware, item-level watermarks into assessment PDFs while keeping their human-visible appearance unchanged. These watermarks consistently prevent MLLMs from answering shielded exam PDFs and encode stable, item-level signatures that can be reliably recovered from model or student responses. Across 30 exams spanning STEM, humanities, and medical reasoning, Integrity Shield achieves exceptionally high prevention (91-94% exam-level blocking) and strong detection reliability (89-93% signature retrieval) across four commercial MLLMs. Our demo showcases an interactive interface where instructors upload an exam, preview watermark behavior, and inspect pre/post AI performance & authorship evidence.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.11093v1", "url_pdf": "https://arxiv.org/pdf/2601.11093.pdf", "meta_path": "data/raw/arxiv/meta/2601.11093.json", "sha256": "41e860f84911ac2c8977c6cd53d96dc89e05b6779b2fb48dd8275749b65c548a", "status": "ok", "fetched_at": "2026-02-18T02:21:25.981522+00:00"}, "pages": [{"page": 1, "text": "INTEGRITY è SHIELD\nA System for Ethical AI Use & Authorship Transparency in Assessments\nAshish Raj Shekhar*\nShiven Agarwal*\nPriyanuj Bordoloi\nYash Shah\nTejas Anvekar\nVivek Gupta\nArizona State University\n Project Page\n Demo\nÅ Video\n§ Code\n{ashekha9, sagar147, pbordolo, yshah124, tanvekar, vgupt140}@asu.edu\nAbstract\nLarge Language Models (LLMs) can now solve\nentire exams directly from uploaded PDF as-\nsessments, raising urgent concerns about aca-\ndemic integrity and the reliability of grades\nand credentials. Existing watermarking tech-\nniques either operate at the token level or\nassume control over the model’s decoding\nprocess, making them ineffective when stu-\ndents query proprietary black-box systems with\ninstructor-provided documents. We present IN-\nTEGRITY è SHIELD, a document-layer wa-\ntermarking system that embeds schema-aware,\nitem-level watermarks into assessment PDFs\nwhile keeping their human-visible appearance\nunchanged. These watermarks consistently pre-\nvent MLLMs from answering shielded exam\nPDFs and encode stable, item-level signatures\nthat can be reliably recovered from model or\nstudent responses. Across 30 exams spanning\nSTEM, humanities, and medical reasoning, IN-\nTEGRITY è SHIELD achieves exceptionally\nhigh prevention (91-94% exam-level blocking)\nand strong detection reliability (89-93% signa-\nture retrieval) across four commercial MLLMs.\nOur demo showcases an interactive interface\nwhere instructors upload an exam, preview wa-\ntermark behavior, and inspect pre/post AI per-\nformance & authorship evidence.\n1\nIntroduction\nLLMs & MLLMs can now interpret full PDF as-\nsessments, reason over diagrams and tables, and\nproduce fluent step-by-step solutions within sec-\nonds. While these capabilities expand access to\nhigh-quality assistance, they simultaneously un-\ndermine the credibility of homework and online\nexams by enabling students to outsource entire as-\nsessments to AI tools (OpenAI, 2023; Team, 2024;\nSusnjak, 2022).\nInstitutions have responded with post-hoc de-\ntection (e.g., authorship classifiers (Emi and Spero,\n*contributed equally\nShielded PDFs & Attribution Report\nVulnerability \nReport\nExtraction\nPymuPDF + MLLM\nPlanner\nQuestion Type\nDomain\nAnswer \nType Logic\nLayout \nPositioning\nWatermark Engine\nICW\ncode-glyph\nTrapDoc\nI   S-v1\nI   S-v2\nFigure 1: Overview of INTEGRITY è SHIELD. The sys-\ntem extracts question structure from an assessment PDF,\nuses LLM-based planner to select schema-aware water-\nmarking tactics, & applies document-layer perturbations\nthrough the watermark engine. It outputs shielded PDF\nvariants (IèS-v1, IèS-v2) & an attribution report sum-\nmarizing AI vulnerability along with authorship signals.\n2024; Thai et al., 2025)) & surveillance-heavy proc-\ntoring (e.g., keystroke, browser, or gaze monitoring\n(Atoum et al., 2017; Kundu et al., 2024)). How-\never, detectors struggle with short answers, code,\nparaphrasing, & mixed authorship (Mitchell et al.,\n2023; Niu et al., 2024), while invasive monitoring\nraises significant privacy, accessibility, & equity\nconcerns.\nThese approaches share a fundamental limita-\ntion: they analyze the student’s output. In practice,\nthe dominant workflow is the opposite-students\nupload instructor-provided PDFs to black-box AI\nsystems. Existing watermarking methods, which\nmodify generation at the model’s decoder (Kirchen-\nbauer et al., 2023; Liu et al., 2025), cannot operate\nin this setting.\n1\narXiv:2601.11093v1  [cs.CL]  16 Jan 2026\n"}, {"page": 2, "text": "This motivates a new question: Can assessments\nthemselves be instrumented so that AI reliance be-\ncomes observable, without altering visible exam\ncontent or student workflows?\nFrom detection to document-level watermark-\ning.\nWe exploit the render-parse gap in PDFs:\nwhat humans see often differs from what AI parsers\ningest. By injecting invisible text, glyph remap-\npings, and lightweight overlays, we influence\nmodel interpretation while leaving the exam vi-\nsually unchanged. INTEGRITY è SHIELD oper-\nationalizes this idea as an authorship-aware wa-\ntermarking system. Rather than asking whether a\nstudent cheated, we ask: to what extent do model-\ngenerated responses follow a consistent watermark\nsignature embedded in the exam? This reframing\nprovides instructors with an interpretable notion of\nauthorship degree while maintaining fairness for\nhonest students.\nFinally, we summarize our contributions as:\n• We introduce INTEGRITY è SHIELD, a\ndocument-layer watermarking system that em-\nbeds schema-aware watermarks into assess-\nment PDFs while keeping their human-visible\nappearance unchanged.\n• We develop an LLM-driven planner and PDF\nwatermark engine that adapt tactics to ques-\ntion type,achieving consistently high preven-\ntion (91-94% exam-level blocking) and strong\ndetection reliability (89-93% retrieval) across\nfour commercial MLLMs on a ten-exam\nbenchmark.\n• We release an interactive demo that allows\ninstructors to upload exams, preview water-\nmarks, and inspect pre/post AI performance\nand authorship evidence, enabling ethical and\ntransparent AI use in education.\n2\nBackground and Related Work\n2.1\nAI assistance and mixed authorship\nLLMs increasingly participate in writing and\nproblem-solving tasks, often producing blended\nhuman-AI content. Recent work formalizes this as\nhomogeneous vs. heterogeneous mixed authorship\n(Thai et al., 2025). Existing detectors-including\nperplexity-based methods (Mitchell et al., 2023),\nstyle-based classifiers (Emi and Spero, 2024),\nand multilingual cheating detectors (Niu et al.,\n2024)-struggle with short answers, paraphrasing,\nand multi-author mixtures, and they analyze only\nthe output, leaving the assessment itself uninstru-\nmented.\n2.2\nAI watermarking\nWatermarking embeds provenance signals into gen-\nerated content, typically by modifying decoding\ndistributions (Kirchenbauer et al., 2023) or via\nprompt-based in-context cues (Liu et al., 2025).\nParallel work explores invisible watermarks for AI-\ngenerated writing that survive paraphrasing and\nediting (Liu and Bu, 2024). These methods assume\ncontrol over generation, which is infeasible when\nstudents query proprietary black-box systems using\ninstructor-provided PDFs.\n2.3\nDocument-layer perturbations\nRecent work shows that perturbing the PDF\nsubstrate-via phantom tokens, font CMaps, or off-\npage text-can induce systematic model errors with-\nout affecting human readability (Jin et al., 2025;\nXiong et al., 2025). Our work builds on these in-\nsights but shifts the objective: rather than deceiving\nmodels or detecting cheating, we embed recover-\nable watermark signatures that quantify the extent\nof AI involvement in solving an exam.\n3\nINTEGRITY è SHIELD\nSystem Architecture & Workflow\n3.1\nDesign Principles\nINTEGRITY è SHIELD is designed as a practi-\ncal tool for instructors who want to harden PDF-\nbased assessments against AI assistants without\nredesigning their exams or changing grading work-\nflows. The system keeps all human-facing content\n(layout, typography, pagination, item numbering)\nunchanged while embedding signals that reliably\ninfluence model-side parsing. It adapts watermark\ntactics to the item schema, treating MCQ, true/-\nfalse, and Long-Form questions differently; re-\nmains robust across black box MLLMs; and ex-\nposes a lightweight web interface where instructors\ncan upload assessments, preview watermark behav-\nior, and inspect calibration and authorship signals\nwith minimal configuration.\n3.2\nEnd-to-End Architecture and Workflow\nFigure 1 summarizes the end-to-end workflow of\nINTEGRITY è SHIELD . A single-page web front\nend communicates with a stateless backend that op-\nerates directly on the PDF substrate. The backend\n2\n"}, {"page": 3, "text": "is organized around four logical services: docu-\nment ingestion, which parses the uploaded PDF\ninto a structured item schema with stems, options,\ndiagrams, and answer keys; strategy planning, via\na lightweight instruction-tuned LLM that assigns\na watermark plan to each item based on its type,\ngold answer, and local layout metadata; PDF rewrit-\ning, which applies the plan while enforcing that the\nrendered appearance of the document remains un-\nchanged; and an authorship and calibration service\nthat runs reference models on original and water-\nmarked PDFs and later scores submitted answers\nagainst stored watermark signatures.\nFrom an instructor’s perspective, this architec-\nture is depicted in three-stage interaction flow.\nFigure 2: Stage 1: Upload & Watermark Planning. In-\nstructors upload an assessment PDF & answer key, after\nwhich the system extracts question structure & previews\nthe planned schema-aware watermarking strategies.\nStage 1: Upload & Watermark Planning.\nThe\ninstructor uploads an exam PDF through the\nbrowser. The ingestion service segments pages into\nquestions, detects answer options & numbering, &\nassociates inline figures or tables with the relevant\nitems, producing a compact item schema with con-\ntent spans, page coordinates, and answer keys when\navailable, best depicted in Figure 2. The strategy\nplanner then assigns, for each item, either a target\ndistractor (for multiple-choice and true/false ques-\ntions) or a small set of signature keyphrases (for\nlong-form questions), and decides which document-\nlayer mechanisms to apply. The interface presents\na split-screen preview of original and watermarked\npages with per-question summaries of the chosen\nstrategy, allowing instructors to inspect and option-\nally disable aggressive tactics (such as strong glyph\nremapping) before proceeding.\nStage 2: Watermark Embedding & AI Calibra-\ntion.\nOnce the plan is confirmed, the PDF rewrit-\ning service applies it directly to the assessment file.\nIt injects invisible text spans anchored near stems\nFigure 3: Stage 2: Watermark Embedding & AI Calibra-\ntion. After planning, the system applies document-layer\nwatermarks to the assessment PDF and evaluates origi-\nnal vs. watermarked versions against multiple MLLMs\nto generate prevention and detection reports.\nand options, applies CMap-based glyph remap-\nping so that visually identical tokens are parsed\ndifferently by models, and, when appropriate, adds\nclipped or off-page overlays that insert distractor-\noriented cues into the parseable stream while keep-\ning them outside the visible canvas, best depicted\nin Figure 3. We instantiate two watermark con-\nfigurations, IèS-v1 and IèS-v2, which differ in\nthe density and combination of these mechanisms:\nIèS-v1 uses a lighter mix of hidden-text and min-\nimal glyph remapping, whereas IèS-v2 employs\nstronger multi-layer perturbations for maximal ro-\nbustness across parsing pipelines. After rewriting,\nthe system verifies that the rendered appearance of\nthe PDF matches the original across common view-\ners (Adobe Reader, Chrome, macOS Preview). In\nthe same stage, the authorship and calibration ser-\nvice evaluates both the original and watermarked\nversions with a panel of reference models in a sim-\nulated “student uploads the exam” setting, com-\nputing pre- versus post-watermark accuracy, the\nfraction of incorrect answers that land on intended\ndistractors, and per-item watermark retrieval rates.\nAn interactive report summarizes these statistics\nand assists instructors in selecting an appropriate\nwatermark “strength” preset.\nStage 3: Authorship Analysis.\nAfter an assess-\nment has been protected with our IèS water-\nmarked PDFs, instructors can use it to analyze re-\nsponses. The interface accepts either raw model\noutputs (for research) or, can also anonymized stu-\ndent responses exported from a learning manage-\nment system, best depicted in Figure 4. For each\nquestion, the authorship engine checks whether the\nresponse follows the stored watermark signature:\nfor objective questions, this reduces to matching\nthe target distractor (or a small tied set); for long-\n3\n"}, {"page": 4, "text": "Figure 4: Stage 3: Authorship Analysis. The dashboard\ndisplays per-question watermark retrieval, exam-level\nauthorship scores, and previewable shielded PDFs, en-\nabling instructors to inspect AI-reliance signals.\nform questions, a judge LLM scores how closely\nthe response tracks the watermark’s keyphrases\nor erroneous reasoning patterns. These per-item\nscores are aggregated into an exam-level authorship\ndegree and displayed on a dashboard with cohort-\nlevel distributions and drill-down views for indi-\nvidual questions. The tool is explicitly positioned\nas an aid for triage rather than an automatic deci-\nsion system: high authorship scores are intended to\ntrigger follow-up actions such as brief oral checks\nor additional written assessments, keeping human\njudgment in the loop.\n4\nExperiments\n4.1\nExperimental Setup\nModels and prompts.\nWe evaluate INTEGRITY\nè SHIELD against a panel of four proprietary fron-\ntier MLLMs that support direct PDF ingestion:\nGPT-5, Claude Sonnet-4.5, Grok-4.1, and Gemini-\n2.5 Flash. All models are treated as black boxes and\naccessed via their official APIs with temperature set\nto 0 and maximum output length sufficient to cover\nall questions in an exam. For each exam, we use a\nminimal, instruction-style prompt that (i) asks the\nmodel to answer all questions in order, (ii) returns a\nstructured list of answers (e.g., “Q1: A, Q2: C, . . . ”\nfor MCQ and T/F; numbered paragraphs for Long-\nForm (LF)), and (iii) forbids external tools or web\nbrowsing. We use the same prompting templates\nfor original and watermarked PDFs; full prompt\ntext for MCQ, T/F, and LF questions appears in Ap-\npendix A as Prompt A, Prompt B, and Prompt C.\nBaselines.\nWe compare our two watermark con-\nfigurations, IèS-v1 and IèS-v2, against three\ndocument- or prompt-level baselines. ICW is an\nin-context watermarking method that attempts to\nsteer model outputs using prompt-side patterns\nwithout modifying the PDF content using invisible\nwhite color small sized font size (0.1-0.5) . (Liu\net al., 2025). code-glyph is a document-layer base-\nline that manipulates bitcode-to-glpyh mapping on\nquestion text to perturb parsing while keeping hu-\nman readability intact(Xiong et al., 2025). TRAP-\nDOC adapts document-layer perturbations that in-\ntroduce phantom tokens and layout tricks to cause\nmodels to produce plausible but incorrect answers\nwithout visible changes to the PDF (Jin et al., 2025).\nIn contrast, IèS-v1 and IèS-v2 operate directly at\nthe PDF substrate with schema-aware hidden text,\nglyph remapping, and overlays; IèS-v1 applies a\nlighter combination aimed at minimal perturbation,\nwhile IèS-v2 uses denser, multi-layer perturba-\ntions for maximal robustness.\nBenchmark dataset.\nTo approximate real assess-\nment settings, we compile a diverse benchmark of\nexam-style PDFs by web-scraping publicly avail-\nable quizzes, homework sets, and midterm assess-\nments from university course websites (e.g., Stan-\nford and other institutions). The collected material\nspans mathematics, science, programming, human-\nities, and medical reasoning, and includes a mix\nof MCQ, T/F, and long-form questions. From this\npool, we sample ≈10% of items to construct our\nbenchmark, selecting documents that (i) contain at\nleast ten questions, (ii) include at least three ques-\ntion formats, and (iii) render cleanly as PDFs. All\nitems and answer keys are qualitatively reviewed\nby two authors and spot-validated quantitatively\n(e.g., via official solutions when available) to filter\nout ambiguous or mislabeled questions.\nEvaluation metrics.\nWe evaluate INTEGRITY è\nSHIELD along two complementary axes: preven-\ntion, which measures how strongly watermarking\ndisrupts a model’s ability to answer correctly, &\ndetection, which captures how reliably watermark\nsignatures can be recovered from model or student\nresponses.\nFor prevention, we simply check whether wa-\ntermarking causes the model to fail or refuse to\nanswer the exam PDF. For exam d, Prev(f, d) =\n1[ywm contains no usable answers], and we report\nthe percentage of PDFs where this occurs.\nFor detection, we measure the degree to which\nmodel outputs follow the embedded watermark sig-\nnature. For each item, the authorship engine as-\nsigns a retrieval score ci ∈[0, 1]: for MCQ; T/F,\nci = 1 iff the model selects the target distractor;\nfor long-form, ci is produced by a judge LLM eval-\n4\n"}, {"page": 5, "text": "uating alignment with watermark keyphrases. The\nexam-level detection score is\nDet(d′, y) = 1\nnd′\nnd′\nX\ni=1\nci,\nrepresenting the proportion of responses that ex-\nhibit watermark-consistent behavior. We report\ndetection scores per model and method, with break-\ndowns by question type.\n4.2\nQuantitative Analysis on Prevention and\nDetection\nMethod\nGPT\nClaude\nGrok\nGemini\nPrevention-ASR\nICW\n07.20\n05.80\n04.10\n03.50\ncode-glyph\n86.30\n84.7\n83.20\n81.90\nTRAPDOC\n88.70\n86.40\n85.10\n40.50\nIèS-v1\n91.20\n90.80\n90.50\n90.10\nIèS-v2\n93.60\n92.90\n92.30\n91.70\nDetection\nICW\n06.80\n05.30\n04.60\n03.20\ncode-glyph\n85.90\n84.20\n82.70\n81.40\nTRAPDOC\n87.90\n85.80\n84.60\n43.40\nIèS-v1\n90.70\n90.30\n89.90\n89.50\nIèS-v2\n92.80\n92.10\n91.60\n91.00\nTable 1: Prevention and detection performance across\nmodels. Prevention-ASR is the percentage of exam\nPDFs on which watermarking causes the model to refuse\nor fail to produce usable answers. Detection is the per-\ncentage of questions whose responses follow the em-\nbedded watermark signature. For both metrics, higher\nis better. ICW: in-context watermarking; code-glyph:\nglyph perturbation; TRAPDOC: phantom-token PDF\nattack; IS: INTEGRITY è SHIELD variants.\nTable 1 summarizes prevention and detection\nperformance for all baselines and our IèS variants\nacross GPT, Claude, Grok, and Gemini.\nIn the Prevention-ASR block, ICW almost never\nprevents models from answering full exams, with\nsingle-digit prevention rates across all models.\nThis confirms that prompt-only steering is inef-\nfective when students upload raw PDFs to black-\nbox MLLMs.\nDocument-layer baselines such\nas code-glyph and TRAPDOC are substantially\nstronger on GPT, Claude, and Grok (around 83–\n89% prevention), but TRAPDOC degrades sharply\non Gemini (40.5%), suggesting that its perturba-\ntions do not transfer reliably across parsing and\nmodel stacks. By contrast, IèS-v1 and IèS-v2\nachieve consistently high prevention on all mod-\nels (90–94%), indicating that schema-aware, multi-\nlayer PDF watermarking can robustly block end-to-\nend exam solving for contemporary MLLMs.\nThe Detection block shows a similar pattern.\nICW again yields negligible detection rates, while\ncode-glyph and TRAPDOC achieve strong detec-\ntion on GPT, Claude, and Grok (mid–80s), but\nTRAPDOC drops to 43.4% on Gemini. In contrast,\nIèS-v1 and especially IèS-v2 maintain high de-\ntection performance across all four models (around\n89–93%), meaning that whenever models do at-\ntempt to answer on watermarked exams, their out-\nputs follow the embedded watermark signatures in\na highly consistent way, enabling reliable author-\nship attribution.\n4.3\nIèS Performance for Question-Category\nType\nGPT\nClaude\nGrok\nGemini\nw/o\nw/\nw/o\nw/\nw/o\nw/\nw/o\nw/\nMCQ\n96.2\n7.8\n95.8\n6.9\n94.9\n5.7\n94.1\n4.3\nT/F\n95.7\n6.5\n95.3\n5.8\n94.6\n4.9\n93.8\n3.6\nLF\n96.8\n5.2\n96.4\n4.6\n95.3\n3.8\n94.7\n3.1\nTable 2: Per-question-type answer accuracy without\n(w/o) and with (w/) our INTEGRITY SHIELD è wa-\ntermarks. Values show the residual accuracy of each\nmodel on shielded exams; lower w/ accuracy indicates\nstronger prevention for that question type.\nTable 2 breaks down the impact of IS on an-\nswer accuracy by question type (MCQ, T/F, LF)\nand model, comparing performance on original\n(w/o) and watermarked (w/) exams. Without wa-\ntermarking, all four MLLMs attain very high ac-\ncuracy across categories (typically 94-97%), re-\nflecting their strong baseline performance on our\nexam-style benchmark.\nWith INTEGRITY è SHIELD enabled, residual\naccuracy collapses into the low single digits for\nevery model and question type (3-8%), correspond-\ning to an 85-90 point drop. Long-form questions\nshow the largest reductions for GPT and Claude,\nwhile MCQ and T/F items are also heavily dis-\nrupted across all models. These results indicate that\nour document-layer watermarks are effective not\nonly at the exam level, but also uniformly across\ndifferent assessment formats.\n4.4\nUtility of INTEGRITY è SHIELD\nBeyond aggregate metrics, INTEGRITY è SHIELD\nprovides instructors with actionable, item-level evi-\ndence of AI reliance. Table 3 illustrates this with\na qualitative example: on an OSI-model question,\nall baseline attacks (ICW, code-glyph, TRAPDOC)\ncollapse to the same incorrect prediction across\nmodels, offering no consistent signal for attribution.\n5\n"}, {"page": 6, "text": "Attack Method\nGPT\nClaude\nGrok\nGemini\nICW\nA\nA\nA\nA\ncode-glyph\nA\nA\nA\nA\nTRAPDOC\nA\nA\nA\nA\nIèS-v1\nB\nB\nB\nB\nIèS-v2\nC\nC\nC\nC\nTable 3: Model predictions across attack methods for the\nOSI model question. Q: Which layer of the OSI model\nis responsible for routing packets between networks?\nGold Answer: B\nIn contrast, our schema-aware variants (IèS-v1,\nIèS-v2) drive models toward distinct, watermark-\naligned distractors (B and C, respectively), en-\nabling clear and separable authorship signatures.\nIn a prevention-focused deployment, the sys-\ntem summarizes where watermarking fully blocks\na model from answering an exam, providing a\ndocument-level view of which assessments are re-\nsilient to AI-based shortcuts. In a detection-focused\ndeployment, the system aggregates authorship evi-\ndence across questions, showing, for example, that\n“Q3 follows the IèS-v2 signature across multiple\nmodels”.\nThese reports are intended as triage tools: in-\nstructors can identify items likely influenced by\nAI, perform brief oral checks or follow-up tasks,\nand intervene proportionally.\nBy surfacing in-\nterpretable authorship signals rather than relying\non opaque classifiers or intrusive proctoring IN-\nTEGRITY è SHIELD enables ethical, transparent,\nand governance-aligned AI use in educational as-\nsessments.\n5\nConclusion\nINTEGRITY è SHIELD demonstrates that assess-\nment integrity can be strengthened without invasive\nmonitoring by instrumenting the exam document\nitself. By operating directly at the PDF substrate,\nour system embeds schema-aware watermarks that\nboth (i) prevent modern MLLMs from answering\nshielded exams (91-94% exam-level blocking) &\n(ii) yield stable, recoverable authorship signatures\n(89-93% retrieval) when AI is used. These effects\nhold consistently across question types & four com-\nmercial MLLMs, highlighting the robustness of\ndocument-layer watermarking as a practical de-\nfense.\nThe demo showcases a complete workflow for\nreal instructional use: uploading an exam, preview-\ning watermark strategies, generating shielded vari-\nants, running automated AI calibration, and inspect-\ning item-level authorship evidence. This combina-\ntion of prevention and attribution provides instruc-\ntors and institutions with actionable, interpretable\nsignals? supporting fair assessment practices, tar-\ngeted follow-up, and transparent communication\nwith students.\nWe hope INTEGRITY è SHIELD serves as a step\ntoward ethically grounded AI governance in edu-\ncation, enabling institutions to observe AI reliance\nwithout resorting to surveillance or restricting ac-\ncess to assistive technologies.\n6\nLimitations\nOur evaluation is limited to a ten-exam benchmark,\na fixed set of frontier MLLMs, and simulated usage\nin which models directly consume instructor PDFs.\nReal-world deployments may involve broader varia-\ntion in domains, languages, accessibility workflows\n(e.g., screen readers), and institution-specific for-\nmats. As MLLMs and their PDF-parsing pipelines\nevolve, watermark robustness may drift, necessitat-\ning periodic re-calibration.\nINTEGRITY è SHIELD is not a definitive de-\ntector of misconduct. Authorship scores indicate\nalignment with watermark signatures-not whether\na student violated policy-and should be used as a\ntriage signal for human follow-up (e.g., brief oral\nchecks), not as automatic evidence for sanctions.\nFinally, our approach assumes institutional con-\ntrol over assessment PDFs. Similar watermarking\ntechniques could be misapplied to non-assessment\ndocuments, so we explicitly restrict the intended\nuse to formal educational settings with clear gover-\nnance, transparency, & AI-use policies.\n7\nEthics Statement\nThis work aims to support ethical and transpar-\nent AI use in educational assessment settings.\nINTEGRITY è SHIELD operates exclusively on\ninstructor-provided documents and does not moni-\ntor students, avoiding surveillance-heavy practices\nsuch as keystroke logging, webcam tracking, or\ndevice control. The system is designed to keep\nall responses and analyses within institutional in-\nfrastructure, respecting student privacy and data-\ngovernance requirements.\nAuthorship scores produced by our watermark-\ning framework indicate alignment with embedded\nwatermark signatures; they do not constitute evi-\ndence of misconduct. We recommend that institu-\ntions (i) clearly communicate AI-use policies and\n6\n"}, {"page": 7, "text": "the presence of watermarking to students, (ii) treat\nhigh authorship scores only as signals for human\nreview (e.g., follow-up questions or oral checks),\nand (iii) ensure that any use of these signals aligns\nwith local policies, academic integrity guidelines,\nand privacy regulations.\nAll experiments were conducted with fixed\nmodel parameters (e.g., temperature, topp, topk) to\nmitigate stochastic variability in black-box LLMs.\nModels used in this work (e.g., GPT-5, Gemni-2.5\nFlash, Grok-4.1, Claude Sonnet-4.5) were accessed\nin accordance with their respective usage policies.\nData labeling and verification were performed by\nauthor-annotators, and AI-based tools (e.g., Gram-\nmarly, ChatGPT) were used strictly for language\nrefinement. To the best of our knowledge, this\nstudy introduces no additional ethical risks beyond\nthose common to LLM evaluation in controlled\neducational settings.\nReferences\nYousef Atoum, Liping Chen, Alex X. Liu, Stephen D. H.\nHsu, and Xiaoming Liu. 2017. Automated Online\nExam Proctoring. IEEE Transactions on Multimedia,\n19(7):1609–1624.\nBradley Emi and Max Spero. 2024. Technical Report on\nthe Pangram AI-Generated Text Classifier. Technical\nreport, Pangram Labs.\nHyundong Jin, Sicheol Sung, Shinwoo Park, SeungYeop\nBaik, and Yo-Sub Han. 2025. TRAPDOC: Deceiv-\ning LLM Users by Injecting Imperceptible Phan-\ntom Tokens into Documents.\nEMNLP Findings.\nArXiv:2506.00089.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, and\n1 others. 2023. A Watermark for Large Language\nModels. arXiv preprint arXiv:2301.10226.\nDebnath Kundu, Atharva Mehta, Rajesh Kumar, Naman\nLal, Avinash Anand, Apoorv Singh, and Rajiv Ratn\nShah. 2024. Keystroke Dynamics Against Academic\nDishonesty in the Age of LLMs. In Proceedings of\nthe IEEE International Joint Conference on Biomet-\nrics (IJCB).\nYepeng Liu and Yuheng Bu. 2024. Adaptive Text Water-\nmark for Large Language Models. In Proceedings of\nthe 41st International Conference on Machine Learn-\ning, volume 235 of Proceedings of Machine Learning\nResearch, pages 30718–30737. PMLR.\nYepeng Liu, Xuandong Zhao, Christopher Kruegel,\nDawn Song, and Yuheng Bu. 2025. In-Context Wa-\ntermarks for Large Language Models. arXiv preprint\narXiv:2505.16934.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky,\nChristopher D. Manning, and Chelsea Finn. 2023.\nDetectGPT: Zero-shot Machine-Generated Text De-\ntection Using Probability Curvature. Proceedings of\nthe 40th International Conference on Machine Learn-\ning.\nChenhao Niu,\nKevin P. Yancey,\nRuidong Liu,\nMirza Basim Baig, André Kenji Horie, and James\nSharpnack. 2024. Detecting LLM-Assisted Cheating\non Open-Ended Writing Tasks on Language Profi-\nciency Tests. EMNLP Industry Track.\nOpenAI. 2023. ChatGPT. OpenAI blog.\nTeo Susnjak. 2022. ChatGPT: The End of Online Exam\nIntegrity? Preprint, arXiv:2212.09292.\nGemini Team. 2024. Gemini 1.5: Unlocking Multi-\nmodal Understanding Across Millions of Tokens of\nContext. arXiv preprint arXiv:2403.05530.\nKatherine Thai, Bradley Emi, Elyas Masrour, and Mohit\nIyyer. 2025. EditLens: Quantifying the Extent of AI\nEditing in Text. Preprint, arXiv:2510.03154.\nJunjie Xiong, Changjia Zhu, Shuhang Lin, Chong\nZhang, Yongfeng Zhang, Yao Liu, and Lingyao Li.\n2025. Invisible Prompts, Visible Threats: Malicious\nFont Injection in External Resources for Large Lan-\nguage Models. Preprint, arXiv:2505.16957.\nA\nPrompts Details\n7\n"}, {"page": 8, "text": "Prompt A: MCQ Perturbation\nYou are an expert at generating text substitutions for academic multiple-choice questions.\nGiven:\n- LaTeX code for the question stem: {latex_stem_text}\n- Gold answer: {gold_answer}\n- Question type: {question_type}\n- Options: {options}\n- Strategy: replacement\n- Reasoning steps:\n{reasoning_steps}\n- Copyable text (use this exact text when selecting substrings):\n<<<COPY\n{copyable_text}\n>>>\n{prefix_note}{answer_guidance}{retry_instructions}\nYour task:\nGenerate {k} valid mappings that satisfy the replacement strategy. Each mapping should:\n1. Replace one contiguous substring of the question stem\n2. Change the answer from the gold option ({gold_answer}) to a different incorrect option\n3. Ensure the replacement is semantically meaningful and natural\n4. Cause a verifiable deviation in the answer\nFor each mapping, provide:\n1. question_index: The question number ({question_index})\n2. latex_stem_text: Exact LaTeX text of the question stem (must match the input exactly)\n3. original_substring: The substring to replace (must be a contiguous substring of latex_stem_text)\n4. replacement_substring: The replacement text\n5. start_pos: Start position of original_substring relative to latex_stem_text (0-based index)\n6. end_pos: End position of original_substring relative to latex_stem_text (exclusive, 0-based index)\n7. target_wrong_answer: The target incorrect option label (e.g., \"B\", \"C\", \"D\")\n8. reasoning: Brief explanation of why this mapping satisfies the strategy\nIMPORTANT:\n- The original_substring MUST be an exact substring of latex_stem_text\n- The start_pos and end_pos MUST be accurate (start_pos + len(original_substring) = end_pos)\n- The target_wrong_answer MUST be different from the gold answer\n- CRITICAL: The replacement_substring MUST be DIFFERENT from the original_substring. Do NOT generate mappings where\n,→original_substring == replacement_substring (e.g., \"power\" --> \"power\" is INVALID). The replacement MUST change the\n,→text to create actual manipulation.\n- CRITICAL: Neither original_substring nor replacement_substring can be empty strings. Both must contain actual text.\n- LENGTH CONSTRAINT: The replacement_substring MUST be smaller or equal in length to the original_substring (len(\n,→replacement_substring) <= len(original_substring)). This is critical for maintaining document layout and preventing\n,→text overflow.\n- latex_stem_text is provided exactly as it appears in the LaTeX source. Do NOT trim, normalise, or reformat it when\n,→determining positions.\n- The latex_stem_text may include \\item tokens from enumerate environments. Keep the \\item token intact and operate on the\n,→descriptive text that follows it whenever possible.\n- The replacement should be natural and semantically meaningful\nReturn as JSON array:\n[\n{{\n\"question_index\": {question_index},\n\"latex_stem_text\": \"...\",\n\"original_substring\": \"...\",\n\"replacement_substring\": \"...\",\n\"start_pos\": 0,\n\"end_pos\": 5,\n\"target_wrong_answer\": \"B\",\n\"reasoning\": \"...\"\n}},\n...\n]\nReturn ONLY valid JSON, no markdown or additional text.\n8\n"}, {"page": 9, "text": "Prompt B: True False Perturbation\nYou are an expert at generating text substitutions for True/False questions.\nGiven:\n- LaTeX code for the question stem: {latex_stem_text}\n- Gold answer: {gold_answer}\n- Question type: {question_type}\n- Strategy: replacement\n- Reasoning steps:\n{reasoning_steps}\n- Copyable text (use this exact text when selecting substrings):\n<<<COPY\n{copyable_text}\n>>>\n{prefix_note}{answer_guidance}{retry_instructions}\nYour task:\nGenerate {k} valid mappings that satisfy the replacement strategy. Each mapping should:\n1. Replace one contiguous substring of the question stem\n2. Flip the answer from {gold_answer} to the opposite answer\n3. Ensure the replacement is semantically meaningful and natural\n4. Cause a verifiable deviation in the answer\nFor each mapping, provide:\n1. question_index: The question number ({question_index})\n2. latex_stem_text: Exact LaTeX text of the question stem (must match the input exactly)\n3. original_substring: The substring to replace (must be a contiguous substring of latex_stem_text)\n4. replacement_substring: The replacement text\n5. start_pos: Start position of original_substring relative to latex_stem_text (0-based index)\n6. end_pos: End position of original_substring relative to latex_stem_text (exclusive, 0-based index)\n7. target_wrong_answer: The opposite answer (e.g., \"False\" if gold is \"True\", or \"True\" if gold is \"False\")\n8. reasoning: Brief explanation of why this mapping satisfies the strategy\nIMPORTANT:\n- The original_substring MUST be an exact substring of latex_stem_text\n- The start_pos and end_pos MUST be accurate (start_pos + len(original_substring) = end_pos)\n- The target_wrong_answer MUST be the opposite of the gold answer\n- CRITICAL: The replacement_substring MUST be DIFFERENT from the original_substring. Do NOT generate mappings where\n,→original_substring == replacement_substring (e.g., \"force\" --> \"force\" is INVALID). The replacement MUST change the\n,→text to create actual manipulation.\n- CRITICAL: Neither original_substring nor replacement_substring can be empty strings. Both must contain actual text.\n- LENGTH CONSTRAINT: The replacement_substring MUST be smaller or equal in length to the original_substring (len(\n,→replacement_substring) <= len(original_substring)). This is critical for maintaining document layout and preventing\n,→text overflow.\n- latex_stem_text is provided exactly as it appears in the LaTeX source. Do NOT trim, normalise, or reformat it when\n,→determining positions.\n- The latex_stem_text may include \\item tokens from enumerate environments. Keep the \\item token intact and operate on the\n,→descriptive text that follows it whenever possible.\n- The replacement should be natural and semantically meaningful\nReturn as JSON array:\n[\n{{\n\"question_index\": {question_index},\n\"latex_stem_text\": \"...\",\n\"original_substring\": \"...\",\n\"replacement_substring\": \"...\",\n\"start_pos\": 0,\n\"end_pos\": 5,\n\"target_wrong_answer\": \"False\",\n\"reasoning\": \"...\"\n}},\n...\n]\nReturn ONLY valid JSON, no markdown or additional text.\n9\n"}, {"page": 10, "text": "Prompt C: LongForm Perturbation\nYou are an expert at generating text substitutions for long-form questions (essay, short answer, etc.).\nGiven:\n- LaTeX code for the question stem: {latex_stem_text}\n- Gold answer: {gold_answer}\n- Question type: {question_type}\n- Strategy: replacement\n- Reasoning steps:\n{reasoning_steps}\n- Copyable text (use this exact text when selecting substrings):\n<<<COPY\n{copyable_text}\n>>>\n{prefix_note}{answer_guidance}{retry_instructions}\nYour task:\nGenerate {k} valid mappings that satisfy the replacement strategy. Each mapping should:\n1. Replace one contiguous substring of the question stem\n2. Cause a verifiable and detectable deviation from the gold answer\n3. Ensure the replacement is semantically meaningful and natural\n4. Change the question focus in a way that affects the expected answer\nFor each mapping, provide:\n1. question_index: The question number ({question_index})\n2. latex_stem_text: Exact LaTeX text of the question stem (must match the input exactly)\n3. original_substring: The substring to replace (must be a contiguous substring of latex_stem_text)\n4. replacement_substring: The replacement text\n5. start_pos: Start position of original_substring relative to latex_stem_text (0-based index)\n6. end_pos: End position of original_substring relative to latex_stem_text (exclusive, 0-based index)\n7. target_wrong_answer: Description of how the answer should deviate (e.g., \"focuses on different aspect\", \"changes key\n,→concept\")\n8. reasoning: Brief explanation of why this mapping satisfies the strategy and how it causes deviation\nIMPORTANT:\n- The original_substring MUST be an exact substring of latex_stem_text\n- The start_pos and end_pos MUST be accurate (start_pos + len(original_substring) = end_pos)\n- The replacement should cause a verifiable deviation in the answer\n- CRITICAL: The replacement_substring MUST be DIFFERENT from the original_substring. Do NOT generate mappings where\n,→original_substring == replacement_substring. The replacement MUST change the text to create actual manipulation.\n- CRITICAL: Neither original_substring nor replacement_substring can be empty strings. Both must contain actual text.\n- LENGTH CONSTRAINT: The replacement_substring MUST be smaller or equal in length to the original_substring (len(\n,→replacement_substring) <= len(original_substring)). This is critical for maintaining document layout and preventing\n,→text overflow.\n- latex_stem_text is provided exactly as it appears in the LaTeX source. Do NOT trim, normalise, or reformat it when\n,→determining positions.\n- The latex_stem_text may include \\item tokens from enumerate environments. Keep the \\item token intact and operate on the\n,→descriptive text that follows it whenever possible.\n- The replacement should be natural and semantically meaningful\nReturn as JSON array:\n[\n{{\n\"question_index\": {question_index},\n\"latex_stem_text\": \"...\",\n\"original_substring\": \"...\",\n\"replacement_substring\": \"...\",\n\"start_pos\": 0,\n\"end_pos\": 5,\n\"target_wrong_answer\": \"focuses on different aspect\",\n\"reasoning\": \"...\"\n}},\n...\n]\nReturn ONLY valid JSON, no markdown or additional text.\n10\n"}]}