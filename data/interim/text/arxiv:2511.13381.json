{"doc_id": "arxiv:2511.13381", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.13381.pdf", "meta": {"doc_id": "arxiv:2511.13381", "source": "arxiv", "arxiv_id": "2511.13381", "title": "Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts", "authors": ["Siyu Zhu", "Mouxiao Bian", "Yue Xie", "Yongyu Tang", "Zhikang Yu", "Tianbin Li", "Pengcheng Chen", "Bing Han", "Jie Xu", "Xiaoyan Dong"], "published": "2025-11-17T13:54:00Z", "updated": "2025-11-17T13:54:00Z", "summary": "With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.13381v1", "url_pdf": "https://arxiv.org/pdf/2511.13381.pdf", "meta_path": "data/raw/arxiv/meta/2511.13381.json", "sha256": "9ee24c90fb24719df853d54beb639a71585924a81d2d546a01aa3bc3cb03b86c", "status": "ok", "fetched_at": "2026-02-18T02:26:48.619356+00:00"}, "pages": [{"page": 1, "text": "CAN LARGE LANGUAGE MODELS FUNCTION AS QUALIFIED\nPEDIATRICIANS? A SYSTEMATIC EVALUATION IN REAL-WORLD\nCLINICAL CONTEXTS\nSiyu Zhu1,†, Mouxiao Bian2,†, Yue Xie1,†, Yongyu Tang3, Zhikang Yu2, Tianbin Li2, Pengcheng Chen2,4, Bing Han2,\nJie Xu2,*, and Xiaoyan Dong1,*\n1 Shanghai Children’s Hospital, School of Medicine,Shanghai Jiao Tong University , Shanghai, China\n2 Shanghai Artificial Intelligence Laboratory, Shanghai, China\n3 Longhua Hospital Shanghai University of Traditional Chinese Medicine, Shanghai, China\n4 University of Washington, Washington, USA\nABSTRACT\nWith the rapid rise of large language models (LLMs) in medicine, a key question is whether they can\nfunction as competent pediatricians in real-world clinical settings. We developed PEDIASBench\n(Pediatric Evaluation of Dynamic Intelligence, Adaptability, and Safety Benchmark), a systematic\nevaluation framework centered on a knowledge-system framework and tailored to realistic clinical\nenvironments. PEDIASBench assesses LLMs across three dimensions: application of basic knowl-\nedge foundational knowledge, dynamic diagnosis and treatment capability, and pediatric medical\nsafety and medical ethics. We evaluated 12 representative models released over the past two years,\nincluding GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and\n211 prototypical diseases.State-of-the-art models performed well on foundational knowledge, with\nQwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance de-\nclined ˜15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice\nassessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diag-\nnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most\nmodels struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks,\nQwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited.\nThese findings indicate that pediatric LLMs are constrained by limited dynamic decision-making\nand underdeveloped humanistic care. Future development should focus on multimodal integration\n(text, imaging, physiological signals) and a clinical feedback–model iteration loop to enhance safety,\ninterpretability, and human–AI collaboration. While current LLMs cannot independently perform\npediatric care, they hold promise for decision support, medical education, and patient communication,\nlaying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare\nsystem.\nKeywords Benchmark · Pediatric · Evaluation · Large language Model\n1\nIntroduction\nWith the advent of the Transformer architecture and the release of the GPT series, LLMs have demonstrated human-like\nreasoning abilities and achieved impressive performance in medical examinations. For example, GPT-4 has achieved\npassing scores on simulated United States Medical Licensing Examination (USMLE) tasks and can provide detailed\nexplanations for its answers[1]. These cross-domain capabilities have drawn significant attention in the medical\n1†These authors contributed equally.\n2*Correspondence: Xiaoyan Dong(dongxy@shchildren.com.cn), Jie Xu (xujie@pjlab.org.cn)\narXiv:2511.13381v1  [cs.CL]  17 Nov 2025\n"}, {"page": 2, "text": "community, with applications spanning documentation summarization, information retrieval, and clinical decision\nsupport[2][3].\nAs LLMs become increasingly integrated into healthcare, their potential to enhance pediatric diagnostics and decision-\nmaking is attracting attention. Pediatric medicine differs from adult medicine in scope and complexity, encompassing\nrapidly changing developmental stages, disease heterogeneity, and communication challenges unique to children and\nfamilies. Recent studies show that LLMs can help reduce pediatric medication dosing errors[4], support individualized\ntreatment planning by integrating updated clinical guidelines and case data[5][6], and improve efficiency in clinical\ndocumentation through natural language processing. Conversational AI tools can facilitate parent–child–clinician\ncommunication by translating complex medical concepts into accessible language, and may even supplement pediatric\nmental health education through bias-free dialogue environments[7][8][9]. In medical education, LLMs are emerging\nas valuable learning tools. GPT-4, for instance, can already achieve or surpass average human performance in several\nphysician licensing examinations[10][11][12]. Nevertheless, their reliability in specialized pediatric assessments\nremains inconsistent. Beam et al. reported that ChatGPT-3.5 correctly answered only 46% of 936 neonatal medicine\nquestions[13], indicating substantial limitations. Furthermore, clinical trials reveal that LLMs, though individually\naccurate, do not yet translate into measurable improvements when used as diagnostic assistants by physicians[14].\nPediatrics, however, presents unique challenges.\nIt encompasses an exceptionally broad disease spectrum[15],\ninvolves patients in continuous developmental stages[16],\nand requires family-centered communication\napproaches[17][18][19]. Pediatric practice is characterized by precise weight-based dosing[20][21] and age-adapted\ndiagnostic communication[22]Pediatric medicine differs from adult medicine in scope and complexity, encompassing\nrapidly changing developmental stages, disease heterogeneity, and communication challenges unique to children and\nfamilies. Recent studies show that LLMs can help reduce pediatric medication dosing errors[23][24].\nExisting medical benchmarks, such as Pediabench[25] and general-purpose datasets including MedQA[26], and\nPubMedQA[27], provide partial insights but lack systematic evaluation of pediatric-specific clinical reasoning. Pediatric\nmedicine differs from adult medicine in scope and complexity, encompassing rapidly changing developmental stages,\ndisease heterogeneity, and communication challenges unique to children and families. Recent studies show that LLMs\ncan help reduce pediatric medication dosing errors[28].\nGiven these challenges, we proposed PEDIASBench, a systematic, clinically authentic, and dynamically adaptive\nevaluation framework that integrates a three-dimensional pediatric competency system(Figure 1). This system mirrors\nthe real-world capabilities required of pediatricians, encompassing application of basic medical knowledge, dynamic\ndiagnostic and treatment capacity, and pediatric medical safety and ethics.The first two dimensions reflect professional\nskills—rooted in clinical reasoning and procedural competence—while the latter forms the ethical foundation of safe\nand humanistic pediatric care.\nTo ensure comprehensive coverage across pediatric subspecialties, PEDIASBench incorporates 19 departments, in-\ncluding pediatric internal medicine and pediatric surgery, encompassing 211 representative diseases. Each disease\nmodule integrates authentic clinical cases and corresponding question types drawn from four standardized examination,\nthus bridging standardized medical education assessments with real clinical complexity[29][30]. This structure allows\nthe benchmark to evaluate not only factual recall but also adaptive reasoning in uncertain, context-rich scenarios—an\nessential characteristic of clinical expertise[31].\nBeyond professional competence, PEDIASBench embeds ethical and communicative dimensions, addressing clinical\nethics, patient safety, and doctor–patient communication. These components align with contemporary pediatric ethics\nframeworks, emphasizing beneficence, nonmaleficence, autonomy, and justice in child healthcare. By integrating\nthese dimensions, PEDIASBench extends beyond knowledge-based testing to offer a holistic assessment of a model’s\nreadiness to function as a trustworthy, safe, and empathetic collaborator in pediatric practice[32].\nIn summary, PEDIASBench establishes a comprehensive, multidimensional, and ethically grounded paradigm for\nevaluating large language models within the pediatric clinical context. By integrating foundational medical knowledge,\ndynamic diagnostic reasoning, and ethical–safety dimensions into a unified assessment framework, it reflects the\nauthentic cognitive and moral demands placed upon pediatricians in real-world practice[29]. Unlike conventional\nbenchmarks that isolate factual recall or task-specific accuracy, PEDIASBench advances toward a competency-based,\ncontext-aware evaluation model aligned with modern medical education principles. This integration not only facilitates\ncross-specialty performance analysis across 19 pediatric disciplines but also underscores the indispensable role of\nempathy, communication, and safety in the responsible deployment of medical AI systems. As such, PEDIASBench\nprovides a foundational pathway toward verifying whether large language models can truly approach the standards of a\nqualified pediatric practitioner—one who combines precision with compassion, and intelligence with integrity.\n2\n"}, {"page": 3, "text": "Figure 1: Pediatric Evaluation of Dynamic Intelligence, Adaptability, and Safety Benchmark\n2\nEvaluation Models and Framework\n2.1\nEvaluation Models\nA total of 12 representative LLMs released within the past two years were selected, encompassing both commercial and\nopen-source models, as well as small-scale (<10B parameters) and large-scale (>200B parameters) architectures. The\ndetailed information is presented in Table 1.\nTable 1: Overview of Evaluated Models\nName\nModel\nSize\nRelease Date\nModel Type\nOpen Source\nDeepSeek\nDeepSeek-R1\n671B\n2025-05\nText Only\nDeepSeek\nDeepSeek-V3\n671B\n2024-12\nText Only\nGLM\nGLM-4-9B-Chat\n9B\n2024-11\nText Only\nInternLM\nIntern2.5-7B-Chat\n7B\n2024-06\nText Only\nLlama\nLlama-4-Maverick\n˜400B\n2025-04\nMultimodal\nMistral AI\nMistral-Small-3.1-24B-Instruct\n24B\n2025-03\nText Only\nQianwen\nQwen2.5-72B\n72B\n2024-09\nText Only\nQianwen\nQwen3-32B\n32B\n2025-04\nText Only\nQianwen\nQwen3-235B-A22B\n235B\n2025-04\nText Only\nClosed Source\nGemini\nGemini-2.5-Flash\n—\n2025-04\nMultimodal\nGPT\nGPT-4o-2024-11-20\n˜200B\n2024-11\nMultimodal\nGPT\nGPT-5-Mini-2025-08-07\n—\n2025-08\nMultimodal\n2.2\nEvaluation Framework\nThe framework was constructed to reflect the competency architecture of a qualified pediatrician within real-world\nclinical contexts. It comprises three principal domains: Application of Basic Knowledge, Dynamic Diagnostic\n3\n"}, {"page": 4, "text": "and Therapeutic Capacity, and Medical Ethics and Patient Safety.The Medical Ethics and Patient Safety domain\nextends beyond ethical conduct to incorporate medical communication, empathy, professionalism, and patient-centered\nsafety awareness, thereby integrating both humanistic and ethical dimensions of pediatric care.While the first two\ndomains primarily evaluate professional and cognitive competencies, the third domain serves as the moral and\nbehavioral foundation that underpins the integrity, empathy, and safety of pediatric practice. To ensure comprehensive\nassessment, the first two domains were further subdivided into 19 pediatric subspecialties (covering both pediatric\ninternal medicine and pediatric surgery). A total of 211 representative diseases were selected across these subspecialties,\nwith corresponding clinical cases and four levels of standardized clinical examinations curated for each. The medical\nsafety and ethics dimension included 10 subcategories, such as clinical ethics, patient communication, informed consent,\nmedical quality control, and pediatric safety management.\n2.3\nEvaluation Metrics\n2.3.1\nSingle-choice Questions\nPerformance on single-choice items was quantified by accuracy, defined as(Equation 1):\nAccuracy =\nTP + TN\nTP + TN + FP + FN\n(1)\nwhere TP (True Positives) and TN (True Negatives) represent correctly predicted cases, while FP (False Positives)\nand FN (False Negatives) denote incorrect classifications. Accuracy thus measures overall correctness of discrete\npredictions.\n2.3.2\nMultiple-choice Questions\nPerformance was measured by the F1 score (Equation 2), combining precision and recall and accuracy (Equation 1 ).\nF1 = 2 × Precision × Recall\nPrecision + Recall\n(2)\nWhere:\n- Precision represents the proportion of samples predicted as positive that are actually positive(Equation 3):\nPrecision =\nTP\nTP + FP\n(3)\n- Recall represents the proportion of actual positive samples that are correctly predicted as positive(Equation 4):\nRecall =\nTP\nTP + FN\n(4)\nIn the above formulas, TP stands for True Positives, FP stands for False Positives, and FN stands for False Negatives.\n2.3.3\nShort-answer Questions\nShort-answer (open-ended) items were evaluated on two dimensions (Equation 5):\nTotal = 0.7 × TotalMacro Recall + 0.3 × TotalBERTScore\n(5)\nwhere: - Answer points refer to key information points (e.g., critical details related to diagnosis, treatment, and care)\npre-labeled by clinical experts based on clinical guidelines and practical experience. These points are defined as\nessential elements that a complete and accurate answer should cover.\n- Macro Recall is calculated as the average recall rate of all answer points within each question. Specifically, for each\npre-labeled answer point i in a question, its recall rate is computed by:\nRecalli = Number of times answer point i is covered in the model’s output\nTotal number of answer point i in the reference answer\nTotalMacro Recall is then the arithmetic mean of Recalli across all answer points within the question, implemented via\nPython (using custom scripts to match and count key points).\n- BERTScore serves as an auxiliary tool: when there are expression differences between the model-generated answer\nand the reference answer (e.g., paraphrasing of answer points), BERTScore helps assess whether the semantic essence\nof the answer point is covered, thereby improving the accuracy of Recalli calculation. It does not directly measure the\ncorrectness of expressions but assists in key point matching for Macro Recall.\nA weighted composite score combining these indicators was computed to derive the final evaluation metric for generative\nperformance.\n4\n"}, {"page": 5, "text": "3\nDataset and Experimental Design\n3.1\nData Sources and Preprocessing\n3.1.1\nApplication of Basic Knowledge\nThis dataset was constructed according to the pediatric subspecialty and disease taxonomy established in the framework.\nQuestion banks were collected from four standardized examination levels—Resident, Junior, Intermediate, and Senior\nPediatrician exams—and categorized into single-choice and multiple-choice items. Items with missing or fewer than\nfive options were excluded.\n3.1.2\nDynamic Diagnostic Capacity\nOver 200 anonymized real-world pediatric cases were curated and structured according to the disease taxonomy. Each\ncase was divided into two diagnostic phases—initial consultation and post-investigation management—to emulate the\ntemporal dynamics of real clinical reasoning. Case segmentation and narrative standardization were performed using\nGPT-4o, ensuring logical progression and contextual consistency, prompt can be found in table 2.\nTable 2: Prompt Design for Dataset Construction\nTask\nPrompt\nDynamic Diagnostic Capacity\nRequirements: As a clinical education expert, split a complete case\ninto 2 key nodes: T1 (Initial Consultation): Use only initial info\n(chief complaint, history, physical exam). Generate 1 clinical question\n(e.g., preliminary diagnosis/further tests). Provide reference answer with\ndiagnostic reasoning and needed supplements. \"key_points\" includes\nkeywords strictly from the answer (no additions). T2 (Progress/Follow-\nup): Add new info (test results, progression) to T1. Generate 1 question\n(e.g., final diagnosis/treatment). Provide answer with final diagnosis\nand management key points. \"key_points\" includes keywords from the\nanswer (separated by \",\"; no additions). Format: JSON Lines (1 line\nfor T1, 1 for T2). T2 \"patient_info\" must include all T1 + new info. No\nfictional data.\nMedical Ethics and Safety\nYou are a pediatrician with extensive clinical knowledge. Your task\nis to answer the following multiple-choice question based on clinical\nknowledge. Output all correct options without any additional content.\nExample output: ABC\n3.1.3\nMedical Ethics and Safety\nThe ethics and safety dataset was built upon 10 thematic dimensions, drawing from authoritative sources such as\npediatric ethics textbooks, medical regulations, clinical safety guidelines, and health policy documents. Corresponding\nsingle-choice questions were generated using GPT-4o to ensure domain coverage and terminological accuracy, prompt\ncan be found in table2.\n3.2\nData Review and Quality Assurance\nAll items underwent multi-expert validation, with at least two pediatricians (≥5 years experience) independently\nreviewing each item. In case of disagreement, a senior reviewer (≥10 years experience) performed adjudication. The\nreview criteria included: a. Accuracy: Content must be grounded in authoritative references (guidelines, textbooks,\nor policies) and free of factual or outdated errors. b. Completeness: All items must have full, non-missing options;\nmultiple-choice questions must include ≥5 options. c. Clinical Relevance: Dynamic diagnostic items must reflect\nauthentic clinical logic, with clear patient evolution across diagnostic phases. d. Ethical and Safety Compliance: Items\nmust conform to pediatric ethical and safety standards, covering topics such as privacy, informed consent, and error\nprevention. e. Communication and Empathy: Items on doctor–patient interaction must encompass common barriers\nand empathy strategies. f. Consistency and Reliability: Independent double review; discrepancies resolved by senior\nadjudicator to ensure dataset consistency and integrity.\n5\n"}, {"page": 6, "text": "Table 3: Prompt Design for Different Task Types\nCategory\nPrompt\nSingle-choice question\nYou are a pediatrician with extensive clinical knowledge. Your task is to\nanswer the following single-choice question based on clinical knowledge.\nOnly output the most appropriate option, without any additional content.\nExample output: A\nMultiple-choice question\nYou are a pediatrician with extensive clinical knowledge. Your task\nis to answer the following multiple-choice question based on clinical\nknowledge. Output all correct options without any additional content.\nExample output: ABC\nShort-answer question\nYou are a pediatrician with extensive clinical knowledge. Your task is\nto answer the following case analysis question briefly based on clinical\nknowledge.\n3.3\nConstruction of Evaluation Tasks\nA zero-shot evaluation paradigm was employed to objectively assess intrinsic model capability. To ensure consistency,\nprompts were standardized across all tasks(table 3). For multiple-choice and single-choice questions, models were\ninstructed to output only the answer option(s); for short-answer items, concise reasoning responses were required.\n3.4\nModel Output Acquisition\nAll models were accessed via official API interfaces to ensure fairness and reproducibility.Each evaluation used a\nsingle non-streaming call per prompt.Uniform API parameters were applied to eliminate systemic differences caused\nby temperature, top-k sampling, or decoding variations.For models with multi-step reasoning modes, this feature was\ndisabled to ensure comparable evaluation conditions.\n4\nResults\n4.1\nApplication of Foundational Pediatric Knowledge\nThis section evaluates the performance of 12 LLMs in applying foundational pediatric knowledge across varying levels\nof task difficulty, question formats, and pediatric subspecialties. The models assessed include Qwen3-235B-A22B,\nQwen2.5-72B, and others, with test questions spanning subspecialties such as pediatric neurosurgery and cardiovascular\ndisorders. The tasks were categorized into four difficulty levels aligned with clinical experience—resident, junior,\nintermediate, and senior physicians—and comprised both single-choice and multiple-choice questions.\n4.1.1\nPerformance in Single-Choice Tasks\nSingle-choice questions primarily evaluated factual recall and basic knowledge application. As shown in Figure 2, model\nperformance exhibited a pronounced sensitivity to difficulty level.The leading models, particularly Qwen3-235B-A22B,\nconsistently achieved the highest accuracies across all physician levels, exceeding 90% for resident-level tasks and\nmaintaining 88.75% accuracy even at the senior level. Qwen2.5-72B and Qwen3-32B followed closely, each surpassing\n90% accuracy in resident-level tasks and demonstrating only modest declines with increasing task difficulty. In contrast,\ntail models such as GLM-4-9B-chat and Mistral-Small-3.1-24B-Instruct showed clear deficiencies, with accuracies\nbelow 80% even at the simplest (resident) level. Notably, Mistral-Small-3.1-24B-Instruct achieved only 65.99% at the\nresident level and performed particularly poorly in pediatric surgery, with accuracies of 66.40% and 64.30% at the\nintermediate and senior levels, respectively—substantially lower than in internal medicine–related tasks.All 12 models\nexhibited a downward trend in accuracy with increasing task complexity, suggesting insufficient mastery of rare or\nintricate knowledge points and limited ability to reason through complex clinical problems.\n4.1.2\nPerformance in Multiple-Choice Tasks\nIn contrast to single-choice questions, multiple-choice tasks required the integrated application of knowledge, demanding\nboth precision and recall. As illustrated in Figure 3, models displayed strong performance at lower difficulty levels\n6\n"}, {"page": 7, "text": "Figure 2: Accuracy comparison of large language models across four physician levels in single-choice tasks.\nbut experienced sharp declines as complexity increased. At the resident level, Llama-4-Maverick and Gemini-2.5-\nFlash achieved accuracies of 80.00%, with average F1 scores of 97.56% and 96.80%, respectively. Models such as\nDeepSeek-V3 and GPT-4o-2024-11-20 also performed well, achieving accuracies above 76% and F1 scores exceeding\n0.96. However, performance deteriorated steeply at higher difficulty levels: at the intermediate level, GLM-4-9B-\nchat achieved only 9.89% accuracy, and DeepSeek-R1’s F1 score dropped from 0.96 to 0.68. At the senior level,\nMistral-Small-3.1-24B-Instruct reached only 20.50% accuracy with F1 scores below 0.80.\nOverall, the leading models in multiple-choice tasks were Gemini-2.5-Flash (overall accuracy 44%, F1 0.87), GPT-5-\nMini-2025-08-07 (accuracy 43%, F1 0.86), and DeepSeek-V3 (accuracy 41%, F1 0.85), whereas the weakest performers\nwere Mistral-Small-3.1-24B-Instruct (accuracy 20%, F1 0.78) and GLM-4-9B-chat (accuracy 17%, F1 0.77).\n4.1.3\nSubspecialty-Level Insights\nAt the subspecialty level, knowledge complexity directly determined model performance. Models achieved relatively\nhigh accuracies in domains with stable, foundational knowledge structures—such as child health and developmental-\nbehavioral pediatrics (resident-level mean accuracy 88.33%, F1 0.92) and respiratory medicine (mean accuracy 73.08%,\nF1 0.89).\nIn contrast, performance was poor in subspecialties characterized by dynamic, individualized, or treatment-specific\nreasoning, such as pediatric oncology surgery (intermediate-level mean accuracy 0.00%) and cardiovascular disorders\n(intermediate-level mean accuracy 4.17%), reflecting the models’ limited capacity for deep clinical reasoning.\nCollectively, the 12 LLMs demonstrated marked stratification in pediatric knowledge performance. Top-performing\nmodels (e.g., Qwen3-235B-A22B) exhibited strong adaptability to foundational clinical contexts, while lower-tier mod-\nels revealed both knowledge and algorithmic limitations. Model performance was strongly influenced by task difficulty\nand subspecialty complexity—high in low-difficulty or foundational tasks, but substantially lower in high-difficulty\nor complex domains. These findings indicate that current LLMs have not yet achieved clinical-level competency in\nhandling complex pediatric problems. The primary bottlenecks remain dynamic knowledge updating and simulation of\nclinical reasoning.\nFuture optimization should focus on enriching training data for complex subspecialties such as pediatric oncology and\ncardiovascular medicine, and incorporating modules for clinical case reasoning. Additionally, integrating multimodal\n7\n"}, {"page": 8, "text": "Figure 3: Performance of large language models across four physician levels in multiple-choice tasks.\ndata (e.g., clinical imaging) and extending training cycles could enhance robustness and contextual adaptability, thereby\nadvancing LLM applications in medical education, clinical decision support, and diagnostic assistance.\n4.2\nDynamic Diagnostic and Therapeutic Capability\nAcross the overall evaluation dimension, model performance exhibited substantial variability. The mean overall score\nacross all LLMs was approximately 0.54, indicating a moderate level of diagnostic and reasoning proficiency(Figure 4).\nAmong the tested models, DeepSeek-R1 ranked highest with an average score of 0.58, demonstrating relatively strong\ncomprehensive answering ability, suggesting potential advantages in both knowledge completeness and diagnostic\naccuracy. In contrast, GPT-5-Mini-2025-08-07 achieved a lower average score of 0.48, reflecting weaker overall\nmedical problem-solving ability and limited performance in certain question categories. This dimension provides an\ninitial assessment of each model’s capacity to handle a broad range of medical diagnostic and therapeutic tasks.At the\ndisciplinary level, model performance varied considerably between pediatric internal medicine and pediatric surgery.\nIn pediatric internal medicine (198 questions), DeepSeek-R1 achieved an average score of 0.62, higher than the mean\nscore across models (≈0.59), indicating stronger mastery and application of internal medicine knowledge. By contrast,\nIntern2.5-7B-chat achieved a relatively low score of 0.55, suggesting weaker understanding in this domain.In pediatric\nsurgery (226 questions), GPT-4o-2024-11-20 performed best with an average score of 0.54. Such inter-model variation\nhighlights differing proficiencies across medical subdomains, reflecting the models’ specialized learning tendencies. At\nthe subspecialty level, performance disparities became more pronounced. DeepSeek-V3 achieved an average score of\n0.75 in respiratory diseases, far exceeding the subspecialty mean (≈0.65), indicating strong learning and reasoning\nability within this domain. Conversely, in rheumatology and immunology, GPT-4o-2024-11-20 demonstrated competent\nperformance, whereas Intern2.5-7B-chat showed inconsistent results. These variations reveal model-specific strengths\nand weaknesses in subspecialty-level knowledge representation and reasoning.\nAt the disease-specific level, model performance varied widely across conditions. DeepSeek-R1 achieved an average\nscore of 0.92 for gastroesophageal reflux disease, demonstrating precise clinical knowledge recall. Gemini-2.5-Flash\nshowed strong performance in pectus excavatum (chest wall deformity) with an average score of 0.90, indicating\nsuperior competence in surgical disease reasoning. Similarly, Qwen2.5-72B achieved an identical score (0.90) in the\nsame condition, while GPT-4o-2024-11-20 displayed consistent competence across multiple disease categories. These\n8\n"}, {"page": 9, "text": "Figure 4: Performance of large language models in dynamic diagnosis and treatment capability.\nFigure 5: Comparative performance of large language models in pediatric medical safety and medical ethics.\nfindings suggest that each model possesses domain-specific advantages and may be optimally applied to particular\ndiseases or clinical tasks depending on the diagnostic context.\n4.3\nPediatric Medical Safety and Medical Ethics\nModel performance in the domain of medical safety and ethics revealed notable heterogeneity (Figure 5). Although\nno single model emerged as a “universal champion,” each demonstrated distinct strengths across specific ethical and\nsafety dimensions, reflecting differentiated task adaptability. Overall, Qwen2.5-72B achieved the highest accuracy\n(92.05%), followed by DeepSeek-V3, while Intern2.5-7B-chat recorded the lowest score, indicating that Qwen2.5-72B\nholds a clear overall advantage and could serve as a reliable reference model for ethics-related medical applications.\nWithin specific subdomains, LLMs exhibited varying specializations. In clinical practice ethics, DeepSeek-V3 and\nQwen2.5-72B achieved the top scores, while GLM-4-9B-chat and Intern2.5-7B-chat performed poorly. In doctor–patient\ncommunication and dispute management, Qwen3-32B achieved an accuracy exceeding 90%, compared with 85.24%\nfor DeepSeek-V3.\nNotably, no single model consistently achieved either the highest or lowest score across all subdomains, indicating\nthat different models excel in distinct ethical or communicative aspects of medical reasoning. Collectively, these\nresults underscore the task-dependent adaptability of current LLMs: performance varies substantially according to\nthe specific ethical or safety context. For real-world deployment, model selection should therefore be guided by\ntask-specific requirements—for example, employing Qwen2.5-72B for clinical ethics reasoning or Qwen3-32B for\npatient communication support—to ensure safe, context-appropriate, and ethically aligned application in pediatric care.\n9\n"}, {"page": 10, "text": "5\nDiscussion\nThe study provides a systematic evaluation of large language models (LLMs) in pediatric clinical reasoning, diagnosis,\nand medical ethics, using a structured competency framework aligned with real-world pediatric practice. The findings\nreveal substantial variability across models, with top performers such as Qwen3-235B-A22B, DeepSeek-V3, and\nQwen2.5-72B demonstrating strong performance in foundational knowledge and ethical reasoning, yet showing\nlimitations in dynamic diagnostic and subspecialty-specific reasoning. These results underscore the heterogeneous\nmaturity of current LLM architectures in handling the multidimensional complexity of pediatric medicine. A central\nobservation of this evaluation is that LLMs exhibit excellent recall and synthesis of structured knowledge but remain\nfragile when required to integrate context-dependent information or reason through ambiguous clinical scenarios. This\naligns with prior evidence that LLMs perform best on factual and pattern-based tasks but degrade significantly in\nreasoning tasks requiring causal inference or temporal context[33][34]. In particular, pediatric subspecialties such as\ncardiology and oncology—which require dynamic interpretation of evolving clinical parameters—revealed performance\ndeficits, suggesting that current models struggle to simulate longitudinal reasoning and the iterative decision-making\nintrinsic to clinical care.\nThe downward trend in accuracy with increasing difficulty mirrors prior findings in general medicine. Studies of\nMed-PaLM 2 and GPT-4 across clinical benchmarks have shown that while models approach or exceed human-level\naccuracy on simple medical board questions, they lag markedly in tasks involving uncertainty, multimorbidity, or ethical\nnuance[35]. Our results extend this observation into pediatrics, a domain characterized by age-specific physiology,\ndevelopmental variability, and ethical sensitivity—factors that exacerbate model fragility.\nAn important aspect of pediatric AI evaluation is safety. As highlighted by Amann et al. [36], even high-performing AI\nsystems can introduce harm if deployed without rigorous domain adaptation and continuous monitoring. In our study,\nalthough some LLMs achieved >90% accuracy in medical ethics and safety domains, performance heterogeneity across\nsubcategories indicates inconsistent value alignment. This echoes the broader challenge of ensuring that generative\nmodels uphold the principles of beneficence, non-maleficence, and respect for autonomy when generating patient-facing\nor clinical content[37].\nFrom an educational standpoint, our findings support the notion that LLMs can serve as valuable augmentative tools in\npediatric medical training. Models such as Qwen3-235B-A22B and DeepSeek-V3 demonstrated robust knowledge\nacross foundational topics and performed comparably to junior physicians in structured assessments, paralleling results\nfrom studies showing that GPT-4 can achieve or exceed average human medical licensing scores[38]. Nevertheless,\nconsistent underperformance in complex or rare-disease reasoning suggests that LLMs should complement—not\nreplace—structured medical education, functioning as adaptive tutoring systems or clinical reasoning simulators under\nexpert supervision.\nAnother key implication concerns data curation and domain adaptation. Pediatric datasets are inherently underrepre-\nsented in large-scale medical corpora, which are typically dominated by adult cases and English-language clinical notes.\nModel performance improves dramatically when training data include age-specific physiological and developmental\ncontexts. Therefore, targeted pretraining or fine-tuning on pediatric case repositories and guideline-based datasets may\nbridge the current performance gap.\nFuture optimization should also address reasoning architecture. Studies introducing retrieval-augmented generation\n(RAG) and tool-use integration[39] show that LLMs equipped with external medical knowledge bases or dynamic\nquerying mechanisms outperform static models in diagnosis and treatment planning. Integrating these architectures into\npediatric AI frameworks could enhance transparency and reduce hallucination risks in rare or ambiguous cases.\nBeyond model design, evaluation methodology requires advancement. Existing benchmarks—such as MedQA and\nPubMedQA—are insufficient for pediatrics due to limited representation of developmental disorders, growth parameters,\nand pediatric pharmacology. Our proposed competency-based evaluation, which integrates both knowledge recall and\nethical safety assessment, may serve as a foundational template for future pediatric AI audits. This approach aligns with\nrecent calls for “clinically grounded” AI evaluation that prioritizes real-world safety and equity over abstract accuracy\nmetrics[40].\nDespite its contributions, this study has several limitations. First, the evaluation was limited to text-based tasks;\nmultimodal reasoning involving images, physiological signals, or laboratory data was not assessed. Prior research has\ndemonstrated that combining visual and textual data—such as in vision-language models (VLMs)—can significantly\nenhance diagnostic reasoning[41]. Second, although the framework covered 19 pediatric subspecialties and 211\ndiseases, it may not fully capture the breadth of pediatric heterogeneity, particularly in emergent or rare disorders. Third,\nmodel training data, internal architecture, and proprietary fine-tuning methods were not fully transparent, restricting\ninterpretability and reproducibility of performance differences. Finally, while our ethics and safety assessments\n10\n"}, {"page": 11, "text": "addressed communication, consent, and risk management, they did not encompass legal liability or institutional\ncompliance—critical factors for real-world clinical deployment.\n6\nFuture Perspectives\n6.1\nSafety,interpretability, and humanism\nSafety is paramount in pediatric AI. Pediatric dosing, for instance, relies on precise weight-based calculations, and even\nminor errors may have serious consequences. Implementing explicit clinical governance, mandatory human oversight,\nand automated high-risk alerts is essential[42]. Model explainability can enhance clinician trust: retrieval-augmented\nmodels that cite authoritative sources such as the American Academy of Pediatrics (AAP) or NICE guidelines allow\nverification and auditing. Furthermore, training models on child-centered communication data and including empathic\nresponse mechanisms may improve interactions with children and caregivers[43].\n6.2\nReal-world validation and multicenter research\nLaboratory results must be corroborated with multicenter clinical evaluations, measuring diagnostic accuracy, treatment\nappropriateness, workflow efficiency, and caregiver satisfaction[44]. Pilot projects integrating LLM support into triage,\ndischarge documentation, or chronic disease follow-up can identify context-specific risks and guide iterative model\nrefinement[45]. Prospective, feedback-driven adaptation mirrors the continuous learning processes inherent in clinical\ntraining[46].\n6.3\nHuman–AI collaboration paradigm\nFuture pediatric practice is likely to adopt a human–AI collaborative model, where clinicians maintain interpretive\nauthority and empathy while LLMs provide evidence synthesis and structured recommendations[47]. Integrating\nmodel-augmented simulations into residency curricula can help trainees critically assess AI outputs while reinforcing\nclinical judgment. This dual learning loop—AI refinement via clinician oversight and clinician learning through AI\nfeedback—may represent the most sustainable approach for safe, effective pediatric AI integration[48].\n6.4\nKey shortcomings and next steps\nPediatric data scarcity limits model generalizability and equity. Federated learning and synthetic data generation with\nprivacy safeguards can mitigate this challenge[49]. Existing benchmarks often emphasize accuracy while overlooking\ninterpretability, safety, and empathy—dimensions central to pediatrics. Establishing standardized reporting frameworks\nthat include ethical compliance, explainability, and empathy metrics is essential for responsible AI development in child\nhealthcare[50].\n7\nLimitations\nThis study has several important limitations. First, the evaluation relied solely on text-based large language models,\nwithout integrating multimodal data such as medical imaging, laboratory results, or real-time physiological signals,\nwhich are critical in pediatric diagnosis and management[51]. Second, the dataset did not cover the full spectrum\nof pediatric diseases; subspecialties like neonatal intensive care, pediatric oncology, and rare genetic disorders were\nunderrepresented, limiting the generalizability of results to uncommon or high-risk conditions. Third, the assessment\nfocused on zero-shot model performance without domain-specific fine-tuning or clinician-in-the-loop validation, which\nmay underestimate potential performance improvements achievable with targeted training and real-world feedback.\nAddressing these limitations will require multimodal integration, broader disease representation, and iterative refinement\nwith clinical supervision to ensure safe, reliable, and clinically meaningful pediatric AI applications.\n8\nConclusion\nPEDIASBench provides a clinically grounded framework for assessing LLM readiness in pediatric practice across\nfoundational knowledge, dynamic diagnostic/therapeutic reasoning, and medical ethics and safety. Our findings indicate\nthat contemporary LLMs possess meaningful competence in baseline knowledge tasks and certain communication\nscenarios, but they remain limited in dynamic decision-making and in delivering consistent, developmentally attuned\nhumanistic care. Consequently, LLMs should presently be deployed as assistive tools under clinician supervision\n11\n"}, {"page": 12, "text": "rather than as autonomous decision-makers. Progress will depend on multimodal data integration, rigorous real-world\nvalidation, improved explainability, and robust safety governance to ensure that pediatric applications of LLMs are both\neffective and ethically defensible.\n9\nData Availability Statement\nThe dataset supporting the findings of this study has been made publicly available through the MedBench repository. It\ncan be accessed at https://medbench.opencompass.org.cn/home.\nReferences\n[1] D. Ferber, O. S. El Nahhas, G. Wölflein, I. C. Wiest, J. Clusmann, M.-E. Leßmann, S. Foersch, J. Lammert,\nM. Tschochohei, D. Jäger et al., “Development and validation of an autonomous artificial intelligence agent for\nclinical decision-making in oncology,” Nature cancer, pp. 1–13, 2025.\n[2] L. Riedemann, M. Labonne, and S. Gilbert, “The path forward for large language models in medicine is open,”\nnpj Digital Medicine, vol. 7, no. 1, p. 339, 2024.\n[3] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, M. Amin, L. Hou, K. Clark, S. R. Pfohl, H. Cole-Lewis\net al., “Toward expert-level medical question answering with large language models,” Nature Medicine, vol. 31,\nno. 3, pp. 943–950, 2025.\n[4] C. Levin, B. Orkaby, E. Kerner, and M. Saban, “Can large language models assist with pediatric dosing accuracy?”\nPediatric Research, pp. 1–6, 2025.\n[5] J. Li, Z. Guan, J. Wang, C. Y. Cheung, Y. Zheng, L.-L. Lim, C. C. Lim, P. Ruamviboonsuk, R. Raman, L. Corsino\net al., “Integrated image-based deep learning and language models for primary diabetes care,” Nature medicine,\nvol. 30, no. 10, pp. 2886–2896, 2024.\n[6] A. S. Huang, K. Hirabayashi, L. Barna, D. Parikh, and L. R. Pasquale, “Assessment of a large language model’s\nresponses to questions and cases about glaucoma and retina management,” JAMA ophthalmology, vol. 142, no. 4,\npp. 371–375, 2024.\n[7] L. J. Fahrner, E. Chen, E. Topol, and P. Rajpurkar, “The generative era of medical ai,” Cell, vol. 188, no. 14, pp.\n3648–3660, 2025.\n[8] J. Barile, A. Margolis, G. Cason, R. Kim, S. Kalash, A. Tchaconas, and R. Milanaik, “Diagnostic accuracy of a\nlarge language model in pediatric case studies,” JAMA pediatrics, vol. 178, no. 3, pp. 313–315, 2024.\n[9] M. Mansoor, A. Hamide, and T. Tran, “Conversational ai in pediatric mental health: A narrative review,” Children,\nvol. 12, no. 3, p. 359, 2025.\n[10] U. Katz, E. Cohen, E. Shachar, J. Somer, A. Fink, E. Morse, B. Shreiber, and I. Wolf, “Gpt versus resident\nphysicians—a benchmark based on official board scores,” Nejm Ai, vol. 1, no. 5, p. AIdbp2300192, 2024.\n[11] M. Kipp, “From gpt-3.5 to gpt-4. o: a leap in ai’s medical exam performance,” Information, vol. 15, no. 9, p. 543,\n2024.\n[12] M. Liu, T. Okuhara, X. Chang, R. Shirabe, Y. Nishiie, H. Okada, and T. Kiuchi, “Performance of chatgpt across\ndifferent versions in medical licensing examinations worldwide: systematic review and meta-analysis,” Journal of\nmedical Internet research, vol. 26, p. e60807, 2024.\n[13] K. Beam, P. Sharma, B. Kumar, C. Wang, D. Brodsky, C. R. Martin, and A. Beam, “Performance of a large\nlanguage model on practice questions for the neonatal board examination,” JAMA pediatrics, vol. 177, no. 9, pp.\n977–979, 2023.\n[14] E. Goh, R. Gallo, J. Hom, E. Strong, Y. Weng, H. Kerman, J. A. Cool, Z. Kanjee, A. S. Parsons, N. Ahuja et al.,\n“Large language model influence on diagnostic reasoning: a randomized clinical trial,” JAMA network open, vol. 7,\nno. 10, pp. e2 440 969–e2 440 969, 2024.\n[15] T.-J. Chen, B. Dong, Y. Dong, J. Li, Y. Ma, D. Liu, Y. Zhang, Y. Xing, Y. Zheng, X. Luo et al., “Matching actions\nto needs: shifting policy responses to the changing health needs of chinese children and adolescents,” The Lancet,\nvol. 403, no. 10438, pp. 1808–1820, 2024.\n[16] M. M. Black, S. P. Walker, L. C. Fernald, C. T. Andersen, A. M. DiGirolamo, C. Lu, D. C. McCoy, G. Fink, Y. R.\nShawar, J. Shiffman et al., “Early childhood development coming of age: science through the life course,” The\nlancet, vol. 389, no. 10064, pp. 77–90, 2017.\n12\n"}, {"page": 13, "text": "[17] T. Seniwati, D. Wanda, and N. Nurhaeni, “Effects of patient and family-centered care on quality of care in pediatric\npatients: A systematic review,” Nurse Media Journal of Nursing, vol. 13, pp. 68–84, 04 2023.\n[18] C. R. Hodgson, R. Mehra, and L. S. Franck, “Child and family outcomes and experiences related to family-centered\ncare interventions for hospitalized pediatric patients: A systematic review,” Children, vol. 11, no. 8, p. 949, 2024.\n[19] E. McCarthy and S. Guerin, “Family-centred care in early intervention: A systematic review of the processes and\noutcomes of family-centred care and impacting factors,” Child: Care, Health and Development, vol. 48, no. 1, pp.\n1–32, 2022.\n[20] B. L. Arnold, 2009, “Personalized medicine: a pediatric perspective,” Current allergy and asthma reports, vol. 9,\nno. 6, pp. 426–432, 2009.\n[21] G. L. Kearns, S. M. Abdel-Rahman, S. W. Alander, D. L. Blowey, J. S. Leeder, and R. E. Kauffman, “Develop-\nmental pharmacology—drug disposition, action, and therapy in infants and children,” New England Journal of\nMedicine, vol. 349, no. 12, pp. 1157–1167, 2003.\n[22] A. Stein, L. Dalton, E. Rapa, M. Bluebond-Langner, L. Hanington, K. F. Stein, S. Ziebland, T. Rochat, E. Harrop,\nB. Kelly et al., “Communication with children and adolescents about the diagnosis of their own life-threatening\ncondition,” The Lancet, vol. 393, no. 10176, pp. 1150–1163, 2019.\n[23] T. P. Klassen, L. Hartling, J. C. Craig, and M. Offringa, “Children are not just small adults: the urgent need for\nhigh-quality trial evidence in children,” PLoS medicine, vol. 5, no. 8, p. e172, 2008.\n[24] S. Y. Chng, M. J. W. Tern, Y. S. Lee, L. T.-E. Cheng, J. Kapur, J. G. Eriksson, Y. S. Chong, and J. Savulescu,\n“Ethical considerations in ai for child health and recommendations for child-centered medical ai,” npj Digital\nMedicine, vol. 8, no. 1, p. 152, 2025.\n[25] Q. Zhang, P. Chen, J. Li, L. Feng, S. Liu, H. Zhao, M. Chen, H. Li, and Y. Wang, “Pediabench: A comprehensive\nchinese pediatric dataset for benchmarking large language models,” arXiv preprint arXiv:2412.06287, 2024.\n[26] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits, “What disease does this patient have? a\nlarge-scale open domain question answering dataset from medical exams,” Applied Sciences, vol. 11, no. 14, p.\n6421, 2021.\n[27] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, “Pubmedqa: A dataset for biomedical research question\nanswering,” arXiv preprint arXiv:1909.06146, 2019.\n[28] D. Wang and S. Zhang, “Large language models in medical and healthcare fields: applications, advances, and\nchallenges,” Artificial intelligence review, vol. 57, no. 11, p. 299, 2024.\n[29] J. R. Frank, L. S. Snell, O. T. Cate, E. S. Holmboe, C. Carraccio, S. R. Swing, P. Harris, N. J. Glasgow, C. Campbell,\nD. Dath et al., “Competency-based medical education: theory to practice,” Medical teacher, vol. 32, no. 8, pp.\n638–645, 2010.\n[30] T. J. O. ten Cate, L. Snell, and C. Carraccio, “Medical competence: the interplay between individual ability and\nthe health care environment,” Medical teacher, vol. 32, no. 8, pp. 669–675, 2010.\n[31] G. Norman, “Research in clinical reasoning: past history and current trends,” Medical education, vol. 39, no. 4,\npp. 418–427, 2005.\n[32] B. Li, P. Qi, B. Liu, S. Di, J. Liu, J. Pei, J. Yi, and B. Zhou, “Trustworthy ai: From principles to practices,” ACM\nComputing Surveys, vol. 55, no. 9, pp. 1–46, 2023.\n[33] R. Bommasani, “On the opportunities and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021.\n[34] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl\net al., “Large language models encode clinical knowledge,” Nature, vol. 620, no. 7972, pp. 172–180, 2023.\n[35] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting, “Large language\nmodels in medicine,” Nature medicine, vol. 29, no. 8, pp. 1930–1940, 2023.\n[36] J. Amann, A. Blasimme, E. Vayena, D. Frey, V. I. Madai, and P. Consortium, “Explainability for artificial\nintelligence in healthcare: a multidisciplinary perspective,” BMC medical informatics and decision making,\nvol. 20, no. 1, p. 310, 2020.\n[37] A. Jobin, M. Ienca, and E. Vayena, “The global landscape of ai ethics guidelines,” Nature machine intelligence,\nvol. 1, no. 9, pp. 389–399, 2019.\n[38] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon, C. Elepaño, M. Madriaga, R. Aggabao, G. Diaz-\nCandido, J. Maningo et al., “Performance of chatgpt on usmle: potential for ai-assisted medical education using\nlarge language models,” PLoS digital health, vol. 2, no. 2, p. e0000198, 2023.\n13\n"}, {"page": 14, "text": "[39] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao, “React: Synergizing reasoning and\nacting in language models,” in The eleventh international conference on learning representations, 2022.\n[40] T. Panch, J. Pearson-Stuttard, F. Greaves, and R. Atun, “Artificial intelligence: opportunities and risks for public\nhealth,” The Lancet Digital Health, vol. 1, no. 1, pp. e13–e14, 2019.\n[41] K. Sun, S. Xue, F. Sun, H. Sun, Y. Luo, L. Wang, S. Wang, N. Guo, L. Liu, T. Zhao et al., “Medical multimodal\nfoundation models in clinical diagnosis and treatment: Applications, challenges, and future directions,” arXiv\npreprint arXiv:2412.02621, 2024.\n[42] J. He, S. L. Baxter, J. Xu, J. Xu, X. Zhou, and K. Zhang, “The practical implementation of artificial intelligence\ntechnologies in medicine,” Nature medicine, vol. 25, no. 1, pp. 30–36, 2019.\n[43] S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam, and I. S. Kohane, “Adversarial attacks on medical\nmachine learning,” Science, vol. 363, no. 6433, pp. 1287–1289, 2019.\n[44] M. P. Sendak, J. D’Arcy, S. Kashyap, M. Gao, M. Nichols, K. Corey, W. Ratliff, and S. Balu, “A path for translation\nof machine learning products into healthcare delivery,” EMJ Innov, vol. 10, pp. 19–00 172, 2020.\n[45] A. Wong, E. Otles, J. P. Donnelly, A. Krumm, J. McCullough, O. DeTroyer-Cooley, J. Pestrue, M. Phillips,\nJ. Konye, C. Penoza et al., “External validation of a widely implemented proprietary sepsis prediction model in\nhospitalized patients,” JAMA internal medicine, vol. 181, no. 8, pp. 1065–1070, 2021.\n[46] X. Liu, L. Faes, A. U. Kale, S. K. Wagner, D. J. Fu, A. Bruynseels, T. Mahendiran, G. Moraes, M. Shamdas,\nC. Kern et al., “A comparison of deep learning performance against health-care professionals in detecting diseases\nfrom medical imaging: a systematic review and meta-analysis,” The lancet digital health, vol. 1, no. 6, pp.\ne271–e297, 2019.\n[47] F. Jiang, Y. Jiang, H. Zhi, Y. Dong, H. Li, S. Ma, Y. Wang, Q. Dong, H. Shen, and Y. Wang, “Artificial intelligence\nin healthcare: past, present and future,” Stroke and vascular neurology, vol. 2, no. 4, 2017.\n[48] K.-H. Yu and I. S. Kohane, “Framing the challenges of artificial intelligence in medicine,” BMJ quality & safety,\nvol. 28, no. 3, pp. 238–241, 2019.\n[49] G. A. Kaissis, M. R. Makowski, D. Rückert, and R. F. Braren, “Secure, privacy-preserving and federated machine\nlearning in medical imaging,” Nature Machine Intelligence, vol. 2, no. 6, pp. 305–311, 2020.\n[50] K. Bærøe, A. Miyata-Sturm, and E. Henden, “How to achieve trustworthy artificial intelligence for health,” Bulletin\nof the World Health Organization, vol. 98, no. 4, p. 257, 2020.\n[51] A. Patil, V. Patil, S. Sankpal, T. S. Patankar, and H. Bhute, “Multimodal decision support system for improved\ndiagnosis and healthcare decision making,” Journal of Biology and Health Science, 2025.\n14\n"}]}