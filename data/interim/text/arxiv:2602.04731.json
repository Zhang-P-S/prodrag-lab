{"doc_id": "arxiv:2602.04731", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.04731.pdf", "meta": {"doc_id": "arxiv:2602.04731", "source": "arxiv", "arxiv_id": "2602.04731", "title": "Less Finetuning, Better Retrieval: Rethinking LLM Adaptation for Biomedical Retrievers via Synthetic Data and Model Merging", "authors": ["Sameh Khattab", "Jean-Philippe Corbeil", "Osman Alperen Koraş", "Amin Dada", "Julian Friedrich", "François Beaulieu", "Paul Vozila", "Jens Kleesiek"], "published": "2026-02-04T16:36:00Z", "updated": "2026-02-04T16:36:00Z", "summary": "Retrieval-augmented generation (RAG) has become the backbone of grounding Large Language Models (LLMs), improving knowledge updates and reducing hallucinations. Recently, LLM-based retriever models have shown state-of-the-art performance for RAG applications. However, several technical aspects remain underexplored on how to adapt general-purpose LLMs into effective domain-specific retrievers, especially in specialized domains such as biomedicine. We present Synthesize-Train-Merge (STM), a modular framework that enhances decoder-only LLMs with synthetic hard negatives, retrieval prompt optimization, and model merging. Experiments on a subset of 12 medical and general tasks from the MTEB benchmark show STM boosts task-specific experts by up to 23.5\\% (average 7.5\\%) and produces merged models that outperform both single experts and strong baselines without extensive pretraining. Our results demonstrate a scalable, efficient path for turning general LLMs into high-performing, domain-specialized retrievers, preserving general-domain capabilities while excelling on specialized tasks.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.04731v1", "url_pdf": "https://arxiv.org/pdf/2602.04731.pdf", "meta_path": "data/raw/arxiv/meta/2602.04731.json", "sha256": "8a37bc6ce69f9c4ab5fa93a3844bb44572c968b878a07f1b67492a862d09efae", "status": "ok", "fetched_at": "2026-02-18T02:19:47.986246+00:00"}, "pages": [{"page": 1, "text": "Less Finetuning, Better Retrieval: Rethinking LLM Adaptation\nfor Biomedical Retrievers via Synthetic Data and Model Merging\nSameh Khattab1*, Jean-Philippe Corbeil2*, Osman Alperen Kora¸s1, Amin Dada1\nJulian Friedrich1, François Beaulieu2, Paul Vozila2, Jens Kleesiek1†\n1IKIM, University Hospital Essen, Germany\n2Microsoft Healthcare & Life Sciences\nAbstract\nRetrieval-augmented generation (RAG) has be-\ncome the backbone of grounding Large Lan-\nguage Models (LLMs), improving knowledge\nupdates and reducing hallucinations. Recently,\nLLM-based retriever models have shown state-\nof-the-art performance for RAG applications.\nHowever, several technical aspects remain un-\nderexplored on how to adapt general-purpose\nLLMs into effective domain-specific retriev-\ners, especially in specialized domains such\nas biomedicine. We present Synthesize-Train-\nMerge (STM), a modular framework that en-\nhances decoder-only LLMs with synthetic hard\nnegatives, retrieval prompt optimization, and\nmodel merging. Experiments on a subset of\n12 medical and general tasks from the MTEB\nbenchmark show STM boosts task-specific ex-\nperts by up to 23.5% (average 7.5%) and pro-\nduces merged models that outperform both sin-\ngle experts and strong baselines without exten-\nsive pretraining. Our results demonstrate a scal-\nable, efficient path for turning general LLMs\ninto high-performing, domain-specialized re-\ntrievers, preserving general-domain capabilities\nwhile excelling on specialized tasks.\n1\nIntroduction\nRetrieval-augmented generation (RAG) (Lewis\net al., 2020) has become a standard approach for\ngrounding Large Language Models (LLMs), lead-\ning to improved knowledge updating and reduced\nhallucinations (Xiong et al., 2024; Fan et al., 2024;\nAbo El-Enen et al., 2025; Ayala and Bechard, 2024;\nBarry et al., 2025). RAG systems typically rely\non lexical and semantic search methods (Sawarkar\net al., 2024; Wang et al., 2024c), implemented via\nsparse or dense retrievers. Dense retrievers based\n*Corresponding authors:\nsameh.khattab@uk-essen.de,\njcorbeil@microsoft.com\n†Other affiliations: Cancer Research Center Cologne Essen\n(CCCE), German Cancer Consortium (DKTK, Partner site Es-\nsen) and Department of Physics of TU Dortmund (Dortmund,\nGermany).\nReal\nMedical\nSynthetic\nMedical\nNLU\nSearch\nModels\n- Qwen3 0.6B\n- Gemma 2B\n- Phi4 3.8B\n2. LoRA\nFine-tuning\nLLM\nPositive\nNegative\nQuery\nHarder\nNegative\n3. Merge\n1.1 Hard Negative\nGeneration\nPrompt\n1.2 Prompt\nOptimization\nBetter\nPrompt\n1. Synthetic Data\nSTM\nFigure 1: Diagram of our recipe to obtain the STM\nretrievers: 1) synthetic data — including 1.1) hard nega-\ntive generation and 1.2) retrieval prompt optimization\n—, 2) LoRA fine-tuning, and 3) model merging. We\nsegment the BMRetriever dataset into four splits: Real\nMedical, Synthetic Medical, NLU, and Search.\non decoder-only LLMs achieve state-of-the-art per-\nformances on embedding-related tasks (Wang et al.,\n2024a; BehnamGhader et al., 2024). These results\nsuggest that general-purpose LLMs already provide\na strong foundation for retrieval.\nHowever, important questions remain on how\nsuch models should be adapted, especially for\ndomain-specific retrieval such as in the biomedical\nfield. While zero-shot approaches (Li and Zhou,\n2025; Springer et al., 2025) reported some suc-\ncesses, fine-tuned methods (BehnamGhader et al.,\n2024; Ni et al., 2022; Wang et al., 2022) are at the\ntop of the MTEB leaderboard (Muennighoff et al.,\n2022). However, certain technical aspects remain\nunderexplored about how to best convert general-\npurpose LLMs into domain-specific retrievers.\n1\narXiv:2602.04731v1  [cs.CL]  4 Feb 2026\n"}, {"page": 2, "text": "Previous work (Xu et al., 2024) has shown that\ncontrastive pre-training of LLMs on a large corpus,\nfollowed by instruction tuning, yields strong dense\nretrieval models for the medical domain. Hard\nnegative mining has also been shown to substan-\ntially improve retriever performance (Moreira et al.,\n2024; Lee et al., 2025; Shao et al., 2025), and\nWeller et al. (2025) highlight the importance of\nprompts for retriever models. Despite this progress,\nseveral questions remain open: is continual pre-\ntraining or all the finetuning data necessary to ob-\ntain strong retrievers? Which subsets of the data\nmix are the most effective for fine-tuning? Can\nprompt optimization lead to further gains? Can\ntop-tier LLMs be used to synthesize effective hard\nnegatives?\nIn parallel, model merging (Goddard et al., 2024)\nhas emerged as techniques to compose robust mod-\nels (Wortsman et al., 2022; Ahmadian et al., 2024)\nfrom expert models, enabling modular develop-\nment (Corbeil et al., 2025) and efficient data abla-\ntion (Na et al., 2024). Basic model-merging tech-\nniques such as ModelSoup (Wortsman et al., 2022)\nhave been incorporated into the training recipes\nof two recent models: EmbeddingGemma (Vera\net al., 2025), and Qwen3 Embedding (Zhang et al.,\n2025). Nonetheless, questions remain about using\nmodel merging to build retriever models: are there\nclear gains compared to full fine-tuning? Which\ndata subsets are most effective? Which merging\ntechnique offers the best performance?\nIn this work, we present Synthesize-Train-Merge\n(STM), a modular framework, for enhancing LLM-\nbased dense retrievers along three axes: synthetic\nhard negatives, retriever prompt optimization, and\nmodel merging. We focus on biomedical retrieval\nwhile maintaining performance on general do-\nmains.\nOur contributions are as follows:\n• We present the first systematic evaluation\nof two model-merging techniques for LM-\nbased retrievers, demonstrating significant\ngains over fine-tuning, see Figure 2.\n• We conduct a systematic study on two under-\nexplored axes for retriever models: synthetic\nhard negatives, and prompt optimization.\n• We achieve better results with less data: no\npre-training (i.e.. 11.4M to 1.4M pairs), merg-\ning 3 experts out of 4 (i.e., -29% of pairs), and\nfine-tuning on less than 10% of the pairs.\nQwen 0.6B\nGemma 2B\nPhi4 3.8B\n0.4\n0.5\n0.6\n0.7\nAvg NDCG@10\nSTM\nFT\n0.616\n0.599\n0.622\n0.600\n0.646\n0.621\nFigure 2: Performance comparison of STM Merged\nModels versus models fine-tuned on the combined\ndatasets of all merged experts, across three base models,\nusing the average NDCG@10 metric across all datasets.\nLLM2Vec BMRetriever\nSTM (Ours)\nAttention Mask\nBidir.\nCausal\nBidir.\nPooling\nAverage\nEOS\nEOS\nTraining Setup\nLoRA\nLoRA\nLoRA\nTraining Recipe\nMNTP +\nSimSCE\nPT + FT\nFT +\nMerging\nNegatives\nSimSCE\nSampled\nTop-k\nSynthetic\nDataset size\n1.5M\n11.4M\n1.4M\nDomain\nGeneral\nBiomedical\nGeneral &\nBiomedical\nTable 1: Comparison of attributes between previous\nmethods (LLM2Vec, BMRetriever) and ours. LLM2Vec\nis a multi-task contrastive embedding model, and BM-\nRetriever a biomedical dense retriever pretrained with\npseudo-queries (PT) and finetuned on a mix of real and\nsynthetic labeled data with mined hard negatives (FT).\n• We release three types of artifacts: source\ncode, improved retriever fine-tuning dataset,\nand STM model(s).1\n2\nRelated Works\n2.1\nRetrievers from Decoder-Only Models\nE5 (Wang et al., 2024b) first demonstrated state-\nof-the-art performances on the MTEB benchmark\n(Muennighoff et al., 2022) from fine-tuning Mistral\n7B (Jiang et al., 2023), a decoder-only model, on\nreal and synthetic paired retrieval data. LLM2Vec\n(BehnamGhader et al., 2024) showed that using\nbidirectional attention masking with average pool-\ning on LLMs, and training them with masked next\ntoken prediction and SimCSE (Gao et al., 2021)\nled to improvements on retrieval tasks. NV-Embed\n(Lee et al., 2025) introduced the latent attention\n1Code, data and model(s) will be released upon acceptance.\n2\n"}, {"page": 3, "text": "pooling layer during training with positive-aware\nhard negatives and non-retrieval task data to reach\nstronger performances. BMRetriever (Xu et al.,\n2024) leveraged a vast contrastive pre-training, fol-\nlowed by instruction tuning on a mix of real and\nsynthetic datasets, yielding strong dense retrieval\nmodels for the biomedical domain.\n2.2\nHard Negative Mining\nHard negative mining has become a crucial de-\nsign choice for training modern dense retrievers.\nClassical negative mining schemes (Xiong et al.,\n2021; Zhan et al., 2021) showed that retrieving\ntop-ranked passages during training can accelerate\ncontrastive learning. Despite its limitation to ex-\nact word overlap, BM25 (Robertson and Zaragoza,\n2009) is still widely used to surface hard negatives\n(Karpukhin et al., 2020; Zhan et al., 2021). NV-\nRetriever (Moreira et al., 2024) revisits this space\nwith positive-aware hard negative mining, which\nboosts the performance on the MTEB benchmark.\nBeyond mined hard negatives, a small but grow-\ning line of work starts to explore generated hard\nnegatives using LLMs. SyNeg (Li et al., 2024)\nuses an LLM with a multi-attribute self-reflection\nprompting strategy to synthesize hard negatives and\ncombines them with retrieved negatives in a hybrid\nsampling scheme. Their ablation study shows that\ngains only arise from the hybrid method. In con-\ntrast, we show that only prompting a top-tier LLM\nto generate hard negatives yields substantial gains.\n2.3\nModel Merging\nLinear-mode connectivity (Frankle et al., 2020;\nMirzadeh et al.) established that independently\ntrained solutions can be connected by low-loss\npaths in parameter space, motivating model combi-\nnation methods based on interpolation. Building on\nthis insight, Model Soup (Wortsman et al., 2022)\nshowed that averaging multiple training check-\npoints can outperform selecting a single one. These\nideas naturally extend from combining checkpoints\nof a single model to merging distinct expert mod-\nels: task arithmetic (Ilharco et al., 2023) formulates\nmerging as adding and subtracting task-specific\ndeltas from a base model, while methods such as\nTies-merging (Yadav et al., 2023) and DARE (Yu\net al., 2024) explicitly tackle parameter interfer-\nence when merging multiple experts. Recent work\nfurther shows that such parameter-level merging\ncan be competitive with data-mixing strategies (Ah-\nmadian et al., 2024; Na et al., 2024).\nAuthors (Labrak et al., 2024; Corbeil et al., 2025)\nemployed merging to build robust medical LLMs.\nEmbeddingGemma (Vera et al., 2025) and Qwen3\nEmbedding (Zhang et al., 2025) exploit merging in\ntheir recipe without studying its impact.\n2.4\nPrompt Optimization\nPrior work has extensively studied automatic\nprompt optimization for LLMs (Ramnath et al.,\n2025). Automatic Prompt Engineer (APE) success-\nfully performs black-box optimization over gener-\nated candidate prompts from a handwritten seed\nprompt (Zhou et al., 2022). PromptWizard (Agar-\nwal et al., 2025) and GEPA (Agrawal et al., 2025)\nextend this line of work by coordinating multiple\nagents or applying reflective feedback, respectively.\nAlthough Promptriever (Weller et al., 2025)\nshows that prompting can substantially affect em-\nbedding quality, systematic studies of prompt opti-\nmization specifically for retrievers remain limited.\n3\nMethods\n3.1\nSynthetic Data Utilization\nWe leverage LLMs to synthetically augment exist-\ning datasets along two complementary dimensions:\n(1) generating synthetic hard negatives, and (2) op-\ntimizing retrieval prompts.\n3.1.1\nHard Negative Generation\nTraining dense retrievers with contrastive objec-\ntives requires negatives that are both topically re-\nlated and semantically distinct from the positives.\nExisting hard negative mining strategies frequently\nstruggle to balance informativeness and correct-\nness, often introducing either trivial negatives or\nfalse negatives that are actually relevant (Moreira\net al., 2024).\nTo alleviate this issue, we employ GPT-4.1 to\ngenerate synthetic hard negatives. Given a query\nq, a positive passage p+, and an existing mined\nnegative p−, we prompt the LLM to generate a\nnew negative passage ˜p−that remains lexically and\ntopically aligned with q while being semantically\nirrelevant or contradictory. The prompt provides\nthe full context (q, p+, p−) and explicitly instructs\nthe model to preserve surface-level similarity while\naltering semantic intent. The exact prompt template\nis provided in Appendix B.1.\n3.1.2\nPrompt Optimization\nPrompting plays a critical role in decoder-only\nembedding models (Moreira et al., 2024), as the\n3\n"}, {"page": 4, "text": "prompt directly conditions the resulting representa-\ntion space. To investigate this effect systematically,\nwe first use the DSpy framework (Khattab et al.,\n2023) to apply GEPA for automatically optimizing\nretrieval prompts. Starting from an initial prompt\nπ, GEPA iteratively proposes refined prompts π′\naimed at improving downstream retrieval perfor-\nmance on a held-out validation set. We employ\ntwo instances of LLaMA-70B for this: an fp8-\nquantized model for prompt generation and an fp16\nmodel for reflective evaluation. The full GEPA\nconfiguration details are reported in Table 8 of Ap-\npendix A.\nSecond, we examine the impact of randomly\nsampled retrieval prompts. Using an fp8-quantized\nLLaMA-70B, we generate sets of 10, 20, 50, and\n100 generic retrieval prompts, which are randomly\nassigned to queries during fine-tuning. Represen-\ntative examples of both optimized and randomly\ngenerated prompts are provided in Appendix B.2.\nIn all prompt-based settings, prompts are\nprepended to queries during fine-tuning and ap-\nplied consistently at inference time. Full templates\nand example LLM-generated prompts are provided\nin Appendix B.2.1.\n3.2\nInstruction Fine-Tuning\nWe fine-tune decoder-only backbone models using\na contrastive learning objective to obtain dense re-\ntrievers. We adopt the InfoNCE loss (Henderson\net al., 2017), which encourages each query em-\nbedding to be closer to its corresponding positive\npassage than to all other passages in the batch and\nto any provided hard negative passage.\nFormally,\ngiven\na\nbatch\nof\nN\ntriplets\n{(qi, p+\ni , p−\ni )}N\ni=1, the loss term for a given\nquery qi is defined as:\nLi = −log\nesim(qi,p+\ni )\nesim(qi,p−\ni ) + PN\nj=1 esim(qi,p+\nj ) ,\nwhere sim(·, ·) denotes cosine similarity be-\ntween embeddings, and the denominator includes\ntwo terms: the first one uses a hard negative p−\ni ,\nand the second one employs other passages p+\nj for\nj ̸= i as in-batch negatives. We average over the\nloss terms in the batch to obtain the total loss.\n3.2.1\nExpert Model Fine-Tuning\nTo enable modular specialization, we fine-tune mul-\ntiple expert retrievers on different, coherent subsets\nof the BMRetriever fine-tuning dataset (Xu et al.,\n2024): medical synthetic, medical real, natural\nlanguage understanding (NLU), and search.\nThe medical real subset includes sentence-level\nbiomedical inference and similarity datasets such\nas MedNLI (Shivade, 2017) and Medical Ques-\ntion Pairs (McCreery et al., 2020), as well as\npassage-level biomedical QA benchmarks includ-\ning MEDIQA (Ben Abacha et al., 2019), medi-\ncal StackExchange QA (Team, 2021), and med-\nical dialogue data (Li et al., 2023). The medi-\ncal synthetic subset consists of LLM-generated\nbiomedical retrieval pairs. To improve general-\ndomain relevance modeling, the dataset further in-\ncorporates NLU benchmarks such as Natural Ques-\ntions (Kwiatkowski et al., 2019), FEVER (Thorne\net al., 2018a), ELI5 (Fan et al., 2019), SNLI (Bow-\nman et al., 2015), and the MS MARCO passage\nranking dataset (Bajaj et al., 2016).\nWe train four experts per backbone model, each\nemphasizing a distinct data composition or train-\ning configuration (e.g., synthetic hard negatives,\nprompt optimization).\n3.3\nModel Merging\nModel merging aims to combine multiple expert\nretrievers into a single model that inherits com-\nplementary strengths without additional training.\nGiven a set of expert models {Mk}K\nk=1 with pa-\nrameters {θk}, merging methods compute a unified\nmodel ˆ\nM by operating directly in parameter space.\nIn the simplest case, linear merging (Wortsman\net al., 2022) is defined as:\nθ ˆ\nM =\nK\nX\nk=1\nαkθk,\n(1)\nwhere αk are the weight coefficients with 0 ≤\nαk ≤1.\nThe task arithmetic approach (Ilharco et al.,\n2023) relies instead on a linear combination of task\nvectors τk = θk −θB, which is the delta of param-\neters between the kth expert and the parameters of\nthe base model θB. The merged model becomes\nθ ˆ\nM = θB +\nK\nX\nk=1\nαkτk,\n(2)\nTies merging (Yadav et al., 2023) also leverages\ntask vectors τk with two strategies to mitigate the\ntask-interference phenomenon: keeping only high-\nmagnitude parameter changes by introducing a sec-\nond parameter named density δk ∈[0, 1], and the\n4\n"}, {"page": 5, "text": "sign agreement algorithm which is a majority vote\non the signs across all τk.\n4\nExperiments\n4.1\nDatasets\nFor continual pre-training experiments, we follow\nthe BMRetriever setup (Xu et al., 2024) in which\nthey employ large-scale unlabeled biomedical and\nscientific corpora. For fine-tuning, we also use their\nfine-tuning data mixture. We separate it into four\ncoherent subsets as shown in Table 2.\nMedical\nGeneral\nSplit\nMed-Synth\nMed-Real\nSearch\nNLU\n#Pairs\n431,000\n306,000\n438,000\n251,000\nTable 2: Pair counts for our custom splits of the BMRe-\ntriever fine-tuning dataset, comprising four splits: two\nin the medical domain and two in the general domain.\n4.2\nTraining Setup\nWe experiment with three decoder-only backbone\narchitectures of varying sizes; Qwen3 (Yang et al.,\n2025), Gemma (Team et al., 2024), and Phi-4 (Ab-\ndin et al., 2024); the configurations are summarized\nin Table 3.\nQwen3\nGemma\nPhi4 mini instruct\nParameters\n0.6B\n2B\n3.8B\nDimensions\n1,024\n2,048\n3,072\nTable 3: Backbone model used for expert training. Pa-\nrameters indicate the total model size, and Dimensions\nrefers to the hidden size.\nFollowing prior work (BehnamGhader et al.,\n2024; Lee et al., 2025), we disable causal attention\nmasking during fine-tuning to enable bidirectional\nattention. In preliminary experiments, we consider\nboth EOS-token pooling and mean pooling strate-\ngies. However, we observe that the former yields\nslightly stronger performance. Therefore, we adopt\nEOS pooling for all reported results.\nDuring\nfine-tuning,\nretrieval\nprompts\nare\nprepended to each query, and models are trained\nusing the InfoNCE loss. We fine-tune all mod-\nels using LoRA adapters (Hu et al., 2022) ap-\nplied to all linear layers following prior works\n(BehnamGhader et al., 2024; Lee et al., 2025; Xu\net al., 2024). Hyperparameters, including learning\nrate, batch size, number of steps, and LoRA con-\nfiguration, are summarized in Table 9 of Appendix\nA.\nWe conduct fine-tuning under several configura-\ntions. We begin with standard fine-tuning on the\nbase dataset. We then fine-tune models on datasets\naugmented with synthetic hard negatives, followed\nby datasets generated using optimized prompts. Fi-\nnally, we evaluate a configuration that combines\nsynthetic hard negatives with the best optimized\nprompts.\n4.3\nModel Merging\nWe perform model merging using MergeKit (God-\ndard et al., 2024), which supports parameter-space\nmerging for HuggingFace-compatible models. We\nevaluate two merging strategies: linear interpola-\ntion (Wortsman et al., 2022) and Ties merging (Ya-\ndav et al., 2023).\nWe select the best merged models following\na grid search approach. For linear merging, we\nsweep weight coefficients α ∈{0, 0.1, . . . , 0.9}.\nFor Ties merging, we sweep the weight coefficients\nover the same range, and vary the density parameter\nover ρ ∈{0.1, 0.2, . . . , 0.9}.\nAll merged models are evaluated without fur-\nther training on development sets from four BEIR\nbenchmark datasets (Thakur et al., 2021): NFCor-\npus (Boteva et al., 2016), FiQA-2018 (Thakur et al.,\n2021), Quora (DataCanary et al., 2017), and DB-\nPedia (Hasibi et al., 2017). We use the official dev\nsplits and evaluate performance using NDCG@10\nand Recall@10 metrics. To enable scalable evalua-\ntion across numerous merged models, we evaluate\non a sampled subset of queries and documents for\neach dataset.\n4.4\nBaselines\nWe compare our models against a diverse set of\nretrieval baselines, including BM25 (Lù, 2024),\nContriever(Izacard et al., 2021), E5-v2 (Wang\net al., 2022), GTR (Ni et al., 2021), LLM2Vec\n3B (BehnamGhader et al., 2024), and BMRetriever\n2B (Xu et al., 2024). Baselines are selected to\nmatch our models in parameter scale or architec-\ntural family.\n4.5\nEvaluation\nWe evaluate retrieval performance on the En-\nglish medical subset of the Medical MTEB bench-\nmark (Muennighoff et al., 2022; Enevoldsen et al.,\n2025), which includes TREC-COVID (Roberts\n5\n"}, {"page": 6, "text": "Qwen3 0.6B\nGemma 2B\nPhi4 3.8B\nVariant\nMed-\nSynth\nMed-\nReal Search NLU\nAvg\nMed-\nSynth\nMed-\nReal Search NLU\nAvg\nMed-\nSynth\nMed-\nReal Search NLU\nAvg\nFT\n0.583 0.568 0.506 0.546 0.551 0.581 0.591 0.503 0.520 0.549 0.611 0.611 0.497 0.585 0.576\nFTSHN\n0.592 0.583 0.557 0.563 0.574 0.598 0.575 0.508 0.559 0.560 0.605 0.600 0.591 0.566 0.590\nFTP O\n0.594 0.568 0.566 0.583 0.578 0.599 0.569 0.567 0.613 0.587 0.622 0.574 0.614 0.619 0.607\nFTSHN+P O 0.586 0.587 0.571 0.555 0.575 0.588 0.549 0.511 0.569 0.554 0.607 0.608 0.599 0.575 0.597\nTable 4: NDCG@10 scores of experts averaged over 12 MTEB tasks. Each expert is trained on one of the four\nsubsets of the BMRetriever fine-tuning dataset. FT refers to fine-tuning on the corresponding data subset. SHN\ndenotes fine-tuning with synthetic hard negatives, PO applies the best-performing prompt optimization per model\nfamily, and SHN+PO combines both. Bold and underlined entries indicate the best and second-best performance\nwithin each expert column.\net al., 2021), SciFact (Cohan et al., 2020a), NF-\nCorpus (Boteva et al., 2016), Cure (Athar Sheikh\net al., 2025), PublicHealthQA (Xing Han Lu, 2024),\nand MedicalQA (Asma and Dina, 2019).\nTo\nassess general-domain generalization, we addi-\ntionally evaluate on five general-domain MTEB\ndatasets, including FiQA (Thakur et al., 2021), Ar-\nguAna (Wachsmuth et al., 2018), SciDocs (Co-\nhan et al., 2020b), and two NanoMTEB subsets\n(FEVER (Thorne et al., 2018b) and Quora (Data-\nCanary et al., 2017)).\nWe use the official MTEB evaluation pipeline\n(Muennighoff et al., 2022), and report nDCG@10\nas the evaluation metric. We average the scores\nacross three possible splits: medical subset, general\nsubset, and all datasets. At evaluation time, we\nuse the same retrieval prompt for all the models\nfinetuned with instructions.\n5\nResults and Analysis\n5.1\nPre-training\nQwen3 0.6B\nGemma 2B\nPhi4 3.8B\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAvg NDCG@10\n0.495\n0.498\n0.513\n0.595\n0.588\n0.597\n0.599\n0.600\n0.621\nPT (10M)\nPT+FT (11.4M)\nFT (1.4M)\nFigure 3: Performance averages of three base models\npre-trained (PT) and/or fine-tuned (FT) on the BMRe-\ntriever datasets with 10M and 1.4M samples, respec-\ntively.\nWe displayed in Figure 3 results comparing pre-\ntraining (PT) and fine-tuning (FT) setups. We ob-\nserve that pretraining on 10M unlabeled pairs un-\nderperforms models fine-tuned only on 1.4M pairs,\ndespite BMRetriever (Xu et al., 2024) previously\nbenefitting from a pretraining phase. We therefore\ndrop the pretraining step and use fine-tuning only\nin the following experiments.\n5.2\nFine-Tuning Experts with Synthetic Data\n5.2.1\nPrompt Optimization\nMethod\nQwen3 0.6B\nGemma 2B\nPhi4 3.8B\nFT-all\n0.599\n0.600\n0.621\nFT-allGP 10\n0.581†\n0.557\n0.604\nFT-allGP 20\n0.571\n0.556\n0.603\nFT-allGP 50\n0.575\n0.560\n0.604†\nFT-allGP 100\n0.580\n0.555\n0.604\nFT-allGEP A−l\n0.559\n0.571\n0.592\nFT-allGEP A−m\n0.563\n0.577†\n0.595\nFT-allGEP A−h\n0.560\n0.568\n0.597\nMed-Real\n0.568\n0.591\n0.611\nMed-RealP O\n0.568\n0.569\n0.574\nMed-Synth\n0.583\n0.581\n0.611\nMed-SynthP O\n0.594\n0.599\n0.622\nSearch\n0.506\n0.503\n0.497\nSearchP O\n0.566\n0.567\n0.614\nNLU\n0.546\n0.520\n0.585\nNLUP O\n0.583\n0.613\n0.619\nSTMTies\n0.615\n0.619\n0.643\nSTMLinear\n0.616\n0.622\n0.646\n† Highest-performing prompt optimization for this base model.\nTable 5: Prompt Optimization results across models\n(average NDCG@10 over 12 MTEB tasks). Bold and\nunderlined entries indicate the best and second-best per-\nformance within each backbone group. See full Table\n10 in Appendix C.\nWe displayed results for two prompt optimiza-\ntion techniques (generic prompts 10/20/50/100 and\nGEPA light/medium/heavy) in Table 5. While fine-\ntuning on all the dataset is surpassing optimized\nFT-all variants, we observe substantial improve-\nments when applying the top-performing technique\nper model architecture (noted by † in the first sec-\ntion of the table) at the expert level (noted by the\n6\n"}, {"page": 7, "text": "PO in the second section) of the table. The gains\nare more considerable for the non-medical experts,\nwhile the Med-Real expert without prompt opti-\nmization remains superior. We carry over the best\nprompt optimization technique per model in the\nnext section.\n5.2.2\nExpert Optimization\nIn Table 4, both SHN and PO consistently improve\nretrieval performance compared to standard fine-\ntuning when averaged over all experts. PO yields\nthe strongest overall gains — average improvement\nof 5-7% over FT. This suggests that prompt-level\nadaptations are generally effective strategy across\nheterogeneous retrieval settings.\nHowever, we observe that combining SHN and\nPO does not reliably outperform SHN alone. In\nfact, except for the Med-Real and Search experts\nwithin the Qwen family, the SHN+PO variant con-\nsistently ranks below the PO-only approach, indi-\ncating that the benefits of SHN and PO are not\nadditive and may partially interfere.\nLooking at individual experts, the largest rel-\native gains are observed for the Search expert,\nparticularly for Phi-4 Mini, where PO achieves\na +23.5% relative improvement over standard fine-\ntuning. The second-largest improvement is seen\nfor the NLU expert of the Gemma family, with\na +17.9% relative gain. More generally, PO con-\nsistently delivers larger relative improvements for\ngeneral domain experts (Search & NLU) than for\nmedical domain experts.\nIn contrast, SHN tends to degrade performance\nfor medical experts as model size increases, most\nnotably for Med-Real retrieval. This trend suggests\nthat high-quality medical training data may already\ncontain sufficiently challenging negatives, which\nlarger models are better able to exploit.\nOverall, while the results are not consistent,\nprompt optimization emerges as the more robust\nand effective method compared to SHN. SHN can\nprovide gains in selected settings, but its impact\nis model- and task-dependent, and its combination\nwith PO rarely yields additional benefits.\n5.3\nModel Merging\nAs summarized in Table 6, linear interpolation con-\nsistently yields the strongest performance when\ncombining the four individual experts across all\nmodel families. Linear merging slightly but con-\nsistently outperforms Ties merging, indicating that\nsimple weighted interpolation is sufficient to effec-\nModel\nAvg\nMedical\nAvg\nGeneral\nAvg\nAll\nQwen3 0.6B\nMed-Real SHN+P O\n0.613\n0.551\n0.587\nMed-Synth P O\n0.623\n0.553\n0.594\nSearch SHN+P O\n0.597\n0.535\n0.571\nNLU P O\n0.606\n0.550\n0.583\nFT-all\n0.633\n0.551\n0.599\nFT-all SHN\n0.632\n0.555\n0.600\nSTMQwen3−T ies\n0.637\n0.585\n0.615\nSTMQwen3−Linear\n0.638\n0.585\n0.616\nGemma 2B\nMed-Real\n0.625\n0.542\n0.591\nMed-Synth P O\n0.625\n0.564\n0.599\nSearch P O\n0.577\n0.554\n0.567\nNLU P O\n0.647\n0.566\n0.613\nFT-all\n0.638\n0.548\n0.600\nFT-all SHN\n0.637\n0.561\n0.605\nSTMGemma−T ies\n0.651\n0.576\n0.619\nSTMGemma−Linear\n0.654\n0.577\n0.622\nPhi4 3.8B\nMed-Real†\n0.643\n0.567\n0.611\nMed-Synth P O\n0.654\n0.577\n0.622\nSearch P O\n0.636\n0.583\n0.614\nNLU P O\n0.647\n0.580\n0.619\nFT-all\n0.655\n0.573\n0.621\nFT-all SHN\n0.661\n0.585\n0.629\nSTMP hi4−T ies\n0.669\n0.606\n0.643\nSTMP hi4−Linear\n0.677\n0.603\n0.646\n† For the STM Linear Merge, the PO variation was used.\nTable 6: Best-performing experts and their STM-merged\nresults (average NDCG@10 across 12 MTEB tasks).\nFT-allSHN indicates the model finetuned on the full\ndataset along with synthetic hard negatives. Bold indi-\ncates the best average, underline the second best within\neach backbone group.\ntively integrate complementary expert representa-\ntions. Overall, merged models uniformly outper-\nform fully fine-tuned counterparts across all back-\nbones as shown in Figure 2 as well. We note that\nthese gains are consistent across model sizes.\nWhen compared against the strongest individ-\nual expert, merged models also achieve superior\nperformance. In all three model families, the linear-\nmerged STM surpasses the best-performing single\nexpert, confirming that merging captures comple-\nmentary strengths across domain-specialized ex-\nperts rather than amplifying a single dominant ex-\npert.\n5.4\nComparison with Prior Retrievers\nWe further evaluate the best merged STMs along\nbaselines from the literature on our targeted 12\n7\n"}, {"page": 8, "text": "Model\nSize\nAvg\nMed\nAvg\nGeneral\nAvg\nAll\nBM25\n-\n0.532\n0.515\n0.525\nContriever\n150M\n0.508\n0.533\n0.519\nE5 Large V2\n335M\n0.654\n0.576\n0.622\nGTR T5 XL\n1.2B\n0.581\n0.586\n0.583\nBMRetriever\n2B\n0.645\n0.560\n0.609\nLLM2Vec\n3B\n0.635\n0.597\n0.619\nSTMQwen3\n0.6B\n0.638\n0.585\n0.616\nSTMGemma\n2B\n0.654\n0.577\n0.622\nSTMP hi4\n3.8B\n0.677\n0.603\n0.646\nTable 7: Summary of retrieval performance (averages)\nagainst base lines across medical and general domains.\nBold indicates the best average, underline the second\nbest. Full results are reported in Appendix C.\ndatasets from MTEB. As shown in Table 7,\nSTMPhi4−Linear achieves the strongest perfor-\nmance across both medical and general tasks, out-\nperforming all baselines. In particular, it surpasses\nBMRetriever2B and LLM2V ec, demonstrating\nthat expert merging scales effectively to multi-\ndomain retrieval and remains competitive with\nstate-of-the-art retrievers trained on large and di-\nverse corpora.\nNotably,\nSTMGemma−Linear\nalso delivers\nstrong performance despite its smaller model size.\nIt consistently outperforms BMRetriever2B,\nwhich shares the same base model. These results\nhighlight the efficiency of the proposed approach\nwithout relying on larger backbones or additional\npre-training.\n5.5\nMerging Coefficients\nWe provide the merging weight coefficients2 in\nFigure 5 of Appendix C for each model. We notice\nsimilar coefficients for Qwen3 and Phi4 in contrast\nto the ones used for Gemma. Qwen3 and Phi4\ndid not use the Search expert at all to build their\nrespective STM final models, and both utilize with\nhigher amplitudes the medical experts along with\nthe NLU expert at a weight of 0.5. For Gemma, the\nweight coefficients tend to be lower than 0.5 with\nno use of Med-Real or Med-Synth experts for linear\nmerging or Ties, respectively. Overall, Ties-merged\noptimal models have lower coefficients compared\nto the linear merging ones, but coefficients of both\nmethods are correlated.\n2We ignore the density coefficients for the Ties method in\nthis analysis since the weight coefficients modulate directly\nthe final amplitude of that expert in the merged models.\nFrom analyzing the weight coefficients in terms\nof data ablation, we note that generally all optimal\nconfigurations for each model remove one expert.\nThus, we infer that removing one of the expert\ncould reduce the overall data budget from 18% for\nthe NLU subset up to 29% for the Search subset\nout of the 1.4M available pairs.\n5.6\nTraining Data Size Considerations\n10K\n100K\n1.4M\nDataset Size\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAvg NDCG@10\n0.308\n0.603\n0.599\n0.391\n0.608\n0.600\n0.020\n0.633\n0.621\nQwen3 0.6B\nGemma 2B\nPhi4 3.8B\nFigure 4: Performance averages across 3 runs of three\nbase models fine-tuned on three different sample sizes\nof the BMRetriever dataset. Standard deviations are not\ndisplayed since they are below 0.01.\nAblations on dataset sizes visualized in Figure 4\nreveal clear patterns. Models’ performances aver-\naged across 3 runs saturate at around 100K samples\noutperforming those trained on the full 1.4M sam-\nples across all three base models. Therefore, cu-\nrated high-quality data can be more effective than\nlarge-scale datasets; in line with the trends of the\npretraining and merging coefficient results in sec-\ntions 5.1 and 5.5, respectively. While experiments\nof previous sections did not leverage this finding,\nfuture works could further explore this direction.\n6\nConclusion\nWe presented Synthesize-Train-Merge (STM), a\nmodular framework for adapting decoder-only\nLLMs into effective dense retrievers for domain-\nspecific tasks. By combining synthetic hard neg-\natives, retrieval prompt optimization, and model\nmerging, STM improves task-specific experts by\nup to 23.5% and produce unified models that out-\nperform both individual experts and baselines fine-\ntuned on the experts datasets combined. Our re-\nsults show that careful dataset selection and modu-\nlar merging can yield strong retrieval performance\nwithout extensive pre-training or larger backbones.\nThese findings suggest a scalable, efficient path for\n8\n"}, {"page": 9, "text": "adapting LLMs to specialized retrieval tasks while\nmaintaining general-domain generalization.\nLimitations\nDespite strong empirical results, our study has sev-\neral limitations. First, we only explore two merg-\ning strategies (linear interpolation and Ties); more\nadaptive or task-aware merging approaches could\nprovide further gains but are beyond the scope of\nthis work. Second, our synthetic hard negative\ngeneration and prompt optimization rely on large\nLLMs, adding computational cost and potential\nsensitivity to the choice of generator model. We do\nnot evaluate robustness across different LLMs or\nprompt variants.\nAcknowledgments\nThree AI assistants were utilized to accomplish\nparts of this work for writing and coding purposes.\nChatGPT 5 was used for proofreading. Cursor was\nleveraged while coding the source code, specifi-\ncally to draft routine functions and code blocks.\nGitHub Copilot was employed while coding fig-\nures. All outputs were thoroughly edited, revised,\nfact checked, and/or debugged.\nReferences\nMarah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien\nBubeck, Ronen Eldan, Suriya Gunasekar, Michael\nHarrison, Russell J Hewett, Mojan Javaheripi, Piero\nKauffmann, and 1 others. 2024. Phi-4 technical re-\nport. arXiv preprint arXiv:2412.08905.\nMohamed Abo El-Enen, Sally Saad, and Taymoor\nNazmy. 2025. A survey on retrieval-augmentation\ngeneration (rag) models for healthcare applications.\nNeural Computing and Applications, 37(33):28191–\n28267.\nEshaan Agarwal, Raghav Magazine, Joykirat Singh,\nVivek Dani, Tanuja Ganu, and Akshay Nambi. 2025.\nPromptwizard: Optimizing prompts via task-aware,\nfeedback-driven self-evolution. In Findings of the As-\nsociation for Computational Linguistics: ACL 2025,\npages 19974–20003.\nLakshya A Agrawal, Shangyin Tan, Dilara Soylu,\nNoah Ziems, Rishi Khare, Krista Opsahl-Ong, Ar-\nnav Singhvi, Herumb Shandilya, Michael J Ryan,\nMeng Jiang, and 1 others. 2025.\nGepa: Reflec-\ntive prompt evolution can outperform reinforcement\nlearning. arXiv preprint arXiv:2507.19457.\nArash Ahmadian, Seraphina Goldfarb-Tarrant, Beyza\nErmis, Marzieh Fadaee, Sara Hooker, and 1 others.\n2024. Mix data or merge models? optimizing for\ndiverse multi-task learning. In Safe Generative AI\nWorkshop.\nBen Abacha Asma and Demner-Fushman Dina. 2019. A\nquestion-entailment approach to question answering.\nBMC Bioinform., 20(1):511:1–511:23.\nNadia Athar Sheikh, Daniel Buades Marcos, Anne-\nLaure Jousse, Akintunde Oladipo, Olivier Rousseau,\nand Jimmy Lin. 2025. Cure: A dataset for clinical\nunderstanding &amp; retrieval evaluation. In Pro-\nceedings of the 31st ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining V.2, KDD\n’25, page 5270–5277. ACM.\nOrlando Ayala and Patrice Bechard. 2024.\nReduc-\ning hallucination in structured outputs via retrieval-\naugmented generation. In Proceedings of the 2024\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (Volume 6: Industry Track),\npages 228–238, Mexico City, Mexico. Association\nfor Computational Linguistics.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, and 1\nothers. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nMariam Barry, Gaetan Caillaut, Pierre Halftermeyer,\nRaheel Qader, Mehdi Mouayad, Fabrice Le Deit,\nDimitri Cariolaro, and Joseph Gesnouin. 2025.\nGraphRAG: Leveraging graph-based efficiency to\nminimize hallucinations in LLM-driven RAG for fi-\nnance data. In Proceedings of the Workshop on Gen-\nerative AI and Knowledge Graphs (GenAIK), pages\n54–65, Abu Dhabi, UAE. International Committee\non Computational Linguistics.\nParishad BehnamGhader, Vaibhav Adlakha, Marius\nMosbach, Dzmitry Bahdanau, Nicolas Chapados, and\nSiva Reddy. 2024. Llm2vec: Large language models\nare secretly powerful text encoders. In First Confer-\nence on Language Modeling.\nAsma Ben Abacha, Chaitanya Shivade, and Dina\nDemner-Fushman. 2019. Overview of the MEDIQA\n2019 shared task on textual inference, question entail-\nment and question answering. In Proceedings of the\n18th BioNLP Workshop and Shared Task, pages 370–\n379, Florence, Italy. Association for Computational\nLinguistics.\nVera Boteva, Demian Gholipour, Artem Sokolov, and\nStefan Riezler. 2016. A full-text learning to rank\ndataset for medical information retrieval.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\n9\n"}, {"page": 10, "text": "Arman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel S. Weld. 2020a.\nSpecter:\nDocument-level\nrepresentation\nlearning\nusing\ncitation-informed transformers. In ACL.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel S. Weld. 2020b.\nSpecter:\nDocument-level\nrepresentation\nlearning\nusing\ncitation-informed transformers. In ACL.\nJean-Philippe Corbeil, Amin Dada, Jean-Michel At-\ntendu, Asma Ben Abacha, Alessandro Sordoni, Lu-\ncas Caccia, Francois Beaulieu, Thomas Lin, Jens\nKleesiek, and Paul Vozila. 2025.\nA modular ap-\nproach for clinical SLMs driven by synthetic data\nwith pre-instruction tuning, model merging, and\nclinical-tasks alignment. In Proceedings of the 63rd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 19352–\n19374, Vienna, Austria. Association for Computa-\ntional Linguistics.\nDataCanary, Lili Jiang hilfialkaff, Meg Risdal, Nikhil\nDandekar, and tomtung. 2017. Quora question pairs.\nKenneth Enevoldsen, Isaac Chung, Imene Kerboua,\nMárton Kardos,\nAshwin Mathur,\nDavid Stap,\nJay Gala, Wissam Siblini, Dominik Krzemi´nski,\nGenta Indra Winata, Saba Sturua, Saiteja Utpala,\nMathieu Ciancone, Marion Schaeffer, Gabriel Se-\nqueira, Diganta Misra, Shreeya Dhakal, Jonathan\nRystrøm, Roman Solomatin, and 67 others. 2025.\nMmteb: Massive multilingual text embedding bench-\nmark. arXiv preprint arXiv:2502.13595.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3558–3567, Florence,\nItaly. Association for Computational Linguistics.\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang,\nHengyun Li, Dawei Yin, Tat-Seng Chua, and Qing\nLi. 2024. A survey on rag meeting llms: Towards\nretrieval-augmented large language models. In Pro-\nceedings of the 30th ACM SIGKDD conference on\nknowledge discovery and data mining, pages 6491–\n6501.\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel\nRoy, and Michael Carbin. 2020. Linear mode con-\nnectivity and the lottery ticket hypothesis. In Inter-\nnational Conference on Machine Learning, pages\n3259–3269. PMLR.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6894–6910.\nCharles Goddard, Shamane Siriwardhana, Malikeh\nEhghaghi, Luke Meyers, Vladimir Karpukhin, Brian\nBenedict, Mark McQuade, and Jacob Solawetz. 2024.\nArcee’s mergekit: A toolkit for merging large lan-\nguage models. In Proceedings of the 2024 Confer-\nence on Empirical Methods in Natural Language\nProcessing: Industry Track, pages 477–485.\nFaegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisz-\ntian Balog, Svein Erik Bratsberg, Alexander Kotov,\nand Jamie Callan. 2017. Dbpedia-entity v2: A test\ncollection for entity search. In Proceedings of the\n40th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval,\nSIGIR ’17, pages 1265–1268. ACM.\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yun-\nHsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Ku-\nmar, Balint Miklos, and Ray Kurzweil. 2017. Effi-\ncient natural language response suggestion for smart\nreply. arXiv preprint arXiv:1705.00652.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\nand 1 others. 2022. Lora: Low-rank adaptation of\nlarge language models. In International Conference\non Learning Representations.\nGabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-\nman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali\nFarhadi. 2023. Editing models with task arithmetic.\nIn The Eleventh International Conference on Learn-\ning Representations.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Unsupervised dense infor-\nmation retrieval with contrastive learning.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timo-\nthée Lacroix, and William El Sayed. 2023. Mistral\n7b. ArXiv:2310.06825 [cs].\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781.\nAssociation for Computational Linguistics.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari,\nZhiyuan Zhang, Keshav Santhanam, Sri Vard-\nhamanan, Saiful Haq, Ashutosh Sharma, Thomas T.\nJoshi, Hanna Moazam, Heather Miller, Matei Za-\nharia, and Christopher Potts. 2023. Dspy: Compiling\ndeclarative language model calls into self-improving\npipelines. Preprint, arXiv:2310.03714.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\n10\n"}, {"page": 11, "text": "Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nYanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-\nAntoine Gourraud, Mickaël Rouvier, and Richard\nDufour. 2024.\nBiomistral: A collection of open-\nsource pretrained large language models for medical\ndomains. In Findings of the Association for Compu-\ntational Linguistics: ACL 2024, pages 5848–5864.\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan\nRaiman, Mohammad Shoeybi, Bryan Catanzaro, and\nWei Ping. 2025. Nv-embed: Improved techniques\nfor training llms as generalist embedding models. In\nThe Thirteenth International Conference on Learning\nRepresentations.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, and 1 others. 2020. Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks. Advances\nin neural information processing systems, 33:9459–\n9474.\nXiaopeng Li, Xiangyang Li, Hao Zhang, Zhaocheng Du,\nPengyue Jia, Yichao Wang, Xiangyu Zhao, Huifeng\nGuo, and Ruiming Tang. 2024. Syneg: Llm-driven\nsynthetic hard-negatives for dense retrieval. arXiv\npreprint arXiv:2412.17250.\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve\nJiang, and You Zhang. 2023. Chatdoctor: A medical\nchat model fine-tuned on a large language model\nmeta-ai (llama) using medical domain knowledge.\nCureus, 15(6).\nZiyue Li and Tianyi Zhou. 2025.\nYour mixture-of-\nexperts llm is secretly an embedding model for free.\nIn The Thirteenth International Conference on Learn-\ning Representations.\nXing Han Lù. 2024. Bm25s: Orders of magnitude faster\nlexical search via eager sparse scoring. Preprint,\narXiv:2407.03618.\nClara H McCreery, Namit Katariya, Anitha Kannan,\nManish Chablani, and Xavier Amatriain. 2020. Ef-\nfective transfer learning for identifying similar ques-\ntions: matching user questions to covid-19 faqs. In\nProceedings of the 26th ACM SIGKDD international\nconference on knowledge discovery & data mining,\npages 3458–3465.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur,\nRazvan Pascanu, and Hassan Ghasemzadeh. Linear\nmode connectivity in multitask and continual learn-\ning. In International Conference on Learning Repre-\nsentations.\nGabriel de Souza P Moreira, Radek Osmulski, Mengyao\nXu, Ronay Ak, Benedikt Schifferer, and Even\nOldridge. 2024. Nv-retriever: Improving text em-\nbedding models with effective hard-negative mining.\narXiv preprint arXiv:2407.15831.\nNiklas Muennighoff, Nouamane Tazi, Loïc Magne, and\nNils Reimers. 2022. Mteb: Massive text embedding\nbenchmark. arXiv preprint arXiv:2210.07316.\nClara Na, Ian Magnusson, Ananya Harsh Jha, Tom Sher-\nborne, Emma Strubell, Jesse Dodge, and Pradeep\nDasigi. 2024. Scalable data ablation approximations\nfor language models through modular training and\nmerging. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\npages 21125–21141.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Her-\nnandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith\nHall, Ming-Wei Chang, and 1 others. 2022. Large\ndual encoders are generalizable retrievers. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 9844–\n9855.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021. Large dual encoders are generalizable\nretrievers. Preprint, arXiv:2112.07899.\nKiran Ramnath, Kang Zhou, Sheng Guan, Soumya Sm-\nruti Mishra, Xuan Qi, Zhengyuan Shen, Shuai Wang,\nSangmin Woo, Sullam Jeoung, Yawei Wang, Haozhu\nWang, Han Ding, Yuzhe Lu, Zhichao Xu, Yun Zhou,\nBalasubramaniam Srinivasan, Qiaojing Yan, Yueyan\nChen, Haibo Ding, and 2 others. 2025. A systematic\nsurvey of automatic prompt optimization techniques.\nIn Proceedings of the 2025 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n33066–33098, Suzhou, China. Association for Com-\nputational Linguistics.\nKirk Roberts, Tasmeer Alam, Steven Bedrick, Dina\nDemner-Fushman, Kyle Lo, Ian Soboroff, Ellen\nVoorhees, Lucy Lu Wang, and William R Hersh.\n2021. Searching for scientific evidence in a pan-\ndemic:\nAn overview of trec-covid.\nPreprint,\narXiv:2104.09632.\nStephen Robertson and Hugo Zaragoza. 2009.\nThe\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval, 3(4):333–389.\nKunal Sawarkar, Abhilasha Mangal, and Shivam Raj\nSolanki. 2024.\nBlended rag:\nImproving rag\n(retriever-augmented generation) accuracy with se-\nmantic search and hybrid query-based retrievers. In\n2024 IEEE 7th international conference on multi-\nmedia information processing and retrieval (MIPR),\npages 155–161. IEEE.\nRulin Shao, Rui Qiao, Varsha Kishore, Niklas Muen-\nnighoff, Xi Victoria Lin, Daniela Rus, Bryan\nKian Hsiang Low, Sewon Min, Wen-tau Yih,\nPang Wei Koh, and 1 others. 2025. Reasonir: Train-\ning retrievers for reasoning tasks. arXiv preprint\narXiv:2504.20595.\n11\n"}, {"page": 12, "text": "Chaitanya Shivade. 2017. Mednli — a natural language\ninference dataset for the clinical domain.\nJacob Mitchell Springer, Suhas Kotha, Daniel Fried,\nGraham Neubig, and Aditi Raghunathan. 2025. Rep-\netition improves language model embeddings. In\nThe Thirteenth International Conference on Learn-\ning Representations.\nFlax Sentence Embeddings Team. 2021.\nStack ex-\nchange question pairs.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivière, Mihir Sanjay Kale,\nJuliette Love, and 1 others. 2024. Gemma: Open\nmodels based on gemini research and technology.\narXiv preprint arXiv:2403.08295.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2).\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018a.\nFEVER: a large-scale dataset for fact extraction\nand VERification.\nIn Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018b.\nFEVER: a large-scale dataset for fact extraction\nand VERification.\nIn Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nHenrique Schechter Vera, Sahil Dua, Biao Zhang,\nDaniel Salz, Ryan Mullins, Sindhu Raghuram Pa-\nnyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang\nChen, and 1 others. 2025. Embeddinggemma: Pow-\nerful and lightweight text representations.\narXiv\npreprint arXiv:2509.20354.\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein.\n2018. Retrieval of the best counterargument without\nprior topic knowledge. In ACL.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2024a. Improv-\ning text embeddings with large language models. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 11897–11916, Bangkok, Thai-\nland. Association for Computational Linguistics.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2024b. Improv-\ning text embeddings with large language models. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 11897–11916.\nXiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran\nZhang,\nYixin Wu,\nZhibo Xu,\nTianyuan Shi,\nZhengyuan Wang, Shizheng Li, Qi Qian, and 1 oth-\ners. 2024c. Searching for best practices in retrieval-\naugmented generation. In Proceedings of the 2024\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 17716–17736.\nOrion Weller, Benjamin Van Durme, Dawn Lawrie, Ash-\nwin Paranjape, Yuhao Zhang, and Jack Hessel. 2025.\nPromptriever: Instruction-trained retrievers can be\nprompted like language models. In The Thirteenth\nInternational Conference on Learning Representa-\ntions.\nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre,\nRebecca Roelofs, Raphael Gontijo-Lopes, Ari S Mor-\ncos, Hongseok Namkoong, Ali Farhadi, Yair Car-\nmon, Simon Kornblith, and 1 others. 2022. Model\nsoups: averaging weights of multiple fine-tuned mod-\nels improves accuracy without increasing inference\ntime. In International conference on machine learn-\ning, pages 23965–23998. PMLR.\nXing Han Lu. 2024.\npublichealth-qa (revision\n3b67b6b).\nGuangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong\nZhang. 2024. Benchmarking retrieval-augmented\ngeneration for medicine. In Findings of the Associa-\ntion for Computational Linguistics ACL 2024, pages\n6233–6251, Bangkok, Thailand and virtual meeting.\nAssociation for Computational Linguistics.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In Proceedings of the 9th International Con-\nference on Learning Representations.\nRan Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao\nZhu, May Dongmei Wang, Joyce C Ho, Chao Zhang,\nand Carl Yang. 2024. Bmretriever: Tuning large\nlanguage models as better biomedical text retrievers.\nIn Proceedings of the 2024 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n22234–22254.\nPrateek Yadav, Derek Tam, Leshem Choshen, Colin A\nRaffel, and Mohit Bansal. 2023. Ties-merging: Re-\nsolving interference when merging models.\nAd-\nvances in Neural Information Processing Systems,\n36:7093–7115.\n12\n"}, {"page": 13, "text": "An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui,\nBo Zheng,\nBowen Yu,\nChang\nGao, Chengen Huang, Chenxu Lv, and 1 others.\n2025.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388.\nLe Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin\nLi. 2024. Language models are super mario: Absorb-\ning abilities from homologous models as a free lunch.\nIn Forty-first International Conference on Machine\nLearning.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\nZhang, and Shaoping Ma. 2021. Optimizing dense\nretrieval model training with hard negatives. In Pro-\nceedings of the 44th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval, pages 1503–1512. ACM.\nYanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang,\nHuan Lin, Baosong Yang, Pengjun Xie, An Yang,\nDayiheng Liu, Junyang Lin, and 1 others. 2025.\nQwen3 embedding: Advancing text embedding and\nreranking through foundation models. arXiv preprint\narXiv:2506.05176.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2022. Large language models are human-level\nprompt engineers. In The eleventh international con-\nference on learning representations.\nA\nImplementation Details\nParameter\nValue\nLLM Configuration\nPrompt Generation Model\nLLaMA 70B (FP8)\nReflection Model\nLLaMA 70B (FP16)\nTemperature\n0.7\nMax Tokens\n1000\nGEPA Hyperparameters\nBudget Level\nAuto\n(Light / Medium / Heavy)\nTraining Examples\n100 / 200 / 300\nEvaluation Metric\nNDCG@10\nValidation Examples\n100 / 200 / 300\nReflection Minibatch Size\n3 / 5 / 8\nNumber of Threads\n1\nTable 8: GEPA configuration details.\nHyperparameter\nMaximum Tokens\n512\nOptimizer\nAdamW\nLR Scheduler\nLinear Warmup\n# Warmup Steps\n100\nbf16\nTrue\nLearning Rate\nPhi4 mini instruct (3.8B)\n2 × 10−5\nGemma (2B)\n1 × 10−5\nQwen3 (0.6B)\n5 × 10−5\n# Epochs\n1\nLoRA Rank\n16\nLoRA α\n32\nTotal Batch Size\nPhi4 mini instruct (3.8B)\n64\nGemma (2B)\n128\nQwen3 (0.6B)\n256\nTable 9: Fine-tuning Hyperparameters.\nB\nPrompt Examples\nB.1\nHard Negative Generation Prompts\nThe following prompt template was used for hard\nnegative mining:\nHard Negative Generation Prompt\nGiven this query: \"{query}\"\nOriginal prompt: \"{original prompt}\"\nOriginal positive document: \"{positive doc}\"\nOriginal\nnegative\ndocument:\n\"{original\nnegative}\"\nGenerate a HARD negative document that is:\n1. Related to the same domain (extract domain from\nthe query)\n2. Contains similar terminology and concepts as the\npositive document\n3. But is NOT relevant to answering the specific\nquery\n4. Should be moderately challenging to distinguish\nfrom the positive document\n5. Should be a realistic document in the domain\n6. Should be harder than the original negative docu-\nment but not as hard as a super hard negative\nIMPORTANT: Return ONLY the document text. Do\nnot include any introductory text, explanations, sum-\nmaries, or meta-commentary. Just return the raw\ndocument content.\nB.2\nOptimized Prompts\nB.2.1\nGeneric Prompts\nWe generated random generic prompts using the\nfollowing template.\n13\n"}, {"page": 14, "text": "Generic Prompts Generation Prompt\nYou are an expert in information retrieval prompt\nengineering.\nGenerate a creative and diverse prompt specifically\nfor DOCUMENT RETRIEVAL tasks.\nThe prompt should help a retrieval model find rele-\nvant documents for any query.\nUse a different style/approach from these examples:\n- Direct retrieval instruction format\n- Document ranking format\n- Relevance scoring format\n- Query-document matching format\n- Information seeking format\n- Context-aware retrieval format\n- Domain-specific retrieval format\n- Simple document finding format\nMake it unique and varied. The prompt should be\neffective for DOCUMENT RETRIEVAL.\nGenerate only the prompt text, no explanations, do\nnot specify any domain or document type to avoid\nconfusing the model:\nGeneric Retrieval Prompts for General and\nMedical Domains\nBaseline (all variations)\nGeneral Domain (MS MARCO): “Given a web search\nquery, retrieve relevant passages that answer the query.”\nMedical Domain (Synthetic): “Given a query, find\narticles that discuss the correlation between a specific\nlifestyle factor and a disease.”\n10 Generic prompts (all domains)\n“Imagine you’re a curator of a vast library, tasked with\nuncovering hidden gems that shed new light on a specific\ntopic of interest. Given the query, navigate through the\nshelves of knowledge to gather a collection of documents\nthat not only resonate with the inquiry but also offer\ndiverse perspectives, insightful analysis, and thought-\nprovoking discussions. The goal is to assemble a compre-\nhensive anthology that enriches understanding, sparks\ncuriosity, and fuels further exploration, ensuring that\nevery included document contributes a unique voice to\nthe chorus of knowledge on the subject at hand.”\n20 Generic prompts (all domains)\n“Imagine you’re a librarian tasked with curating a person-\nalized bookshelf for a curious reader. Given the topic of\ninterest, navigate through a vast library and handpick\na selection of texts that would spark fascinating discus-\nsions, provide insightful knowledge, and resonate deeply\nwith the reader’s query, as if you were recommending\nbooks to a close friend.”\n50 Generic prompts (all domains)\n“Imagine you’re a librarian tasked with curating a person-\nalized anthology for a curious reader; given the threads\nof inquiry woven into the phrase, navigate the vast ex-\npanse of written works to unearth the most enlightening\nand informative texts that intricately weave together con-\ncepts, ideas, and narratives, and present a collection\nthat not only resonates with the essence of the inquiry\nbut also expands its boundaries, fostering a deeper un-\nderstanding and sparking further exploration.”\n100 Generic prompts (all domains)\n“Imagine you’re a librarian tasked with uncovering hid-\nden gems in a vast archive, and someone has whispered\na cryptic clue in your ear; given this whispered clue,\nwhat documents would you pull from the shelves to un-\nravel the mystery, and what threads of connection would\nyou follow to weave together a tapestry of insight and\nunderstanding?”\nB.2.2\nGEPA-Optimized Retrieval Prompts\nGEPA-Optimized Retrieval Prompts for\nGeneral and Medical Domains\nBaseline (all budgets)\nGeneral Domain (MS MARCO): “Given a web\nsearch query, retrieve relevant passages that answer\nthe query.”\nMedical Domain (Synthetic): “Given a query, find\narticles that discuss the correlation between a specific\nlifestyle factor and a disease.”\nGEPA-Light Budget\nGeneral Domain (MS MARCO): “Design an effi-\ncient algorithm to retrieve relevant textual passages\nfrom the web that directly answer a given search\nquery, considering factors such as query clarity, con-\ntextual relevance, and the specificity of the information\nsought, and optimize the search results based on the\ntype of information required, such as factual, explana-\ntory, or comparative analysis, to achieve high scores\nin evaluation metrics like NDCG@10, MAP@10, and\nRecall@10.”\nMedical Domain (Synthetic): “Retrieve recent, high-\nquality Pubmed passages that directly answer the\ngiven biomedical question, prioritizing peer-reviewed\narticles from reputable journals published within the\nlast five years, and considering specific contexts such\nas demographics or health conditions if applicable.”\nGEPA-Medium Budget\nGeneral Domain (MS MARCO): “Retrieve fac-\ntual and explanatory passages from reputable online\nsources that directly answer the given web search\nquery, ensuring the passages include specific keywords\nrelated to the query topic, are relevant to the context\nor domain of interest, and provide a clear, concise,\nand relevant response that matches the desired type of\nanswer.”\nMedical Domain (Synthetic): “Retrieve recent, peer-\nreviewed Pubmed articles or passages that directly an-\nswer the given biomedical question, focusing on high-\nquality studies published within the last five years, and\nprovide abstracts or summaries of these articles in the\nsearch results.”\nGEPA-Heavy Budget\nGeneral Domain (MS MARCO): “Design an effi-\ncient algorithm to retrieve relevant textual passages\n14\n"}, {"page": 15, "text": "from the web that directly answer a given search query,\nconsidering factors such as query clarity, contextual\nrelevance, and the specificity of the information sought.”\nMedical Domain (Synthetic): “Retrieve recent, rele-\nvant Pubmed passages that directly answer the given\nbiomedical question, prioritizing articles from rep-\nutable journals published within the last five years,\nand providing accurate and up-to-date information on\nthe topic.”\nC\nDetailed Results\n15\n"}, {"page": 16, "text": "Method\nQwen3 0.6B\nGemma 2B\nPhi4 3.8B\nAvg\nMed\nAvg\nGen\nAvg\nAll\nAvg\nMed\nAvg\nGen\nAvg\nAll\nAvg\nMed\nAvg\nGen\nAvg\nAll\nMed-Real\n0.609\n0.510\n0.568\n0.625\n0.542\n0.591\n0.643\n0.567\n0.611\nMed-RealP O\n0.581\n0.550\n0.568\n0.586\n0.545\n0.569\n0.566\n0.585\n0.574\nMed-Synth\n0.614\n0.540\n0.583\n0.606\n0.546\n0.581\n0.642\n0.567\n0.611\nMed-SynthP O\n0.623\n0.553\n0.594\n0.625\n0.564\n0.599\n0.654\n0.577\n0.622\nSearch\n0.527\n0.477\n0.506\n0.523\n0.475\n0.503\n0.515\n0.472\n0.497\nSearchP O\n0.583\n0.542\n0.566\n0.577\n0.554\n0.567\n0.636\n0.583\n0.614\nNLU\n0.573\n0.509\n0.546\n0.546\n0.484\n0.520\n0.628\n0.525\n0.585\nNLUP O\n0.606\n0.550\n0.583\n0.647\n0.566\n0.613\n0.647\n0.580\n0.619\nFT-all\n0.633\n0.551\n0.599\n0.638\n0.548\n0.600\n0.655\n0.573\n0.621\nFT-allGP 10\n0.609\n0.543\n0.581†\n0.579\n0.526\n0.557\n0.641\n0.552\n0.604\nFT-allGP 20\n0.595\n0.537\n0.571\n0.576\n0.530\n0.556\n0.643\n0.547\n0.603\nFT-allGP 50\n0.603\n0.535\n0.575\n0.586\n0.524\n0.560\n0.642\n0.552\n0.604†\nFT-allGP 100\n0.608\n0.541\n0.580\n0.575\n0.528\n0.555\n0.641\n0.551\n0.604\nFT-allGEP A−l\n0.592\n0.513\n0.559\n0.607\n0.521\n0.571\n0.632\n0.537\n0.592\nFT-allGEP A−m\n0.600\n0.510\n0.563\n0.608\n0.533\n0.577†\n0.631\n0.545\n0.595\nFT-allGEP A−h\n0.594\n0.513\n0.560\n0.600\n0.523\n0.568\n0.636\n0.544\n0.597\nSTMTies\n0.637\n0.585\n0.615\n0.651\n0.576\n0.619\n0.669\n0.606\n0.643\nSTMLinear\n0.638\n0.585\n0.616\n0.654\n0.577\n0.622\n0.677\n0.603\n0.646\n† Highest-performing prompt optimization for this base model; used as the main prompt for experts.\nTable 10: Prompt Optimization results across the base models (average NDCG@10 over 12 MTEB tasks).\nMedical\nGeneral\nModel\nSize\nFeedback\nQA\nMedical\nQA\nCUREv1\nPublic\nHealthQA\nNF\nCorpus\nTREC\nCOVID\nSci\nFact\nSCI\nDOCS\nNano\nFEVER\nArgu\nAna\nNano\nQuora\nFiQA\n2018\nAvg\nMed.\nAvg\nGen.\nAvg\nAll\nBM25\n-\n0.563\n0.458\n0.355\n0.718\n0.321 0.623 0.686 0.158 0.809 0.492 0.863 0.251 0.532 0.515 0.525\nContriever\n150M 0.505\n0.592\n0.351\n0.694\n0.313 0.448 0.655 0.171 0.794 0.484 0.944 0.274 0.508 0.533 0.519\nE5 Large V2\n335M 0.704\n0.699\n0.562\n0.856\n0.372 0.666 0.722 0.205 0.889 0.464 0.912 0.411 0.654 0.576 0.622\nGTR T5 XL\n1.2B\n0.577\n0.692\n0.507\n0.713\n0.333 0.601 0.642 0.157 0.846 0.528 0.957 0.442 0.581 0.586 0.583\nBMRetriever\n2B\n0.587\n0.727\n0.471\n0.812\n0.347 0.839 0.729 0.186 0.940 0.356 0.960 0.357 0.645 0.560 0.609\nLLM2Vec\n3B\n0.708\n0.731\n0.490\n0.814\n0.385 0.572 0.746 0.190 0.864 0.553 0.953 0.423 0.635 0.597 0.619\nSTMQwen3\n0.6B\n0.681\n0.697\n0.487\n0.819\n0.340 0.761 0.681 0.190 0.865 0.548 0.967 0.354 0.638 0.585 0.616\nSTMGemma\n2B\n0.621\n0.717\n0.503\n0.864\n0.368 0.793 0.715 0.201 0.898 0.479 0.969 0.338 0.654 0.577 0.622\nSTMP hi4\n3.8B\n0.697\n0.732\n0.531\n0.860\n0.382 0.791 0.744 0.214 0.853 0.562 0.969 0.414 0.677 0.603 0.646\nTable 11: Comparison of retrieval performance on English tasks from MTEB. Results are reported primarily on\nthe medical subset, with additional evaluation on five general-domain subsets. Performance is measured using\nNDCG@10.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWeight\n0.4\n1.0\n0.2\n0.7\n0.5\n1.0\n0.5\n0.1\n0.3 0.3 0.3\n0.5\n0.4\n0.8\n0.2\n0.9\n0.1\n0.5 0.5\nMed Real\nMed Synth\nSearch\nNLU\nMed Real\nMed Synth\nSearch\nNLU\nMed Real\nMed Synth\nSearch\nNLU\nQwen 0.6B\nGemma 2B\nPhi4 3.8B\nLinear\nTIES\nFigure 5: Merging weight coefficients for each expert for Linear and TIES techniques for each model.\n16\n"}]}