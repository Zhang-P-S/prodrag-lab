{"doc_id": "arxiv:2512.22258", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.22258.pdf", "meta": {"doc_id": "arxiv:2512.22258", "source": "arxiv", "arxiv_id": "2512.22258", "title": "Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method", "authors": ["Satvik Tripathi"], "published": "2025-12-24T09:20:35Z", "updated": "2025-12-24T09:20:35Z", "summary": "Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.22258v1", "url_pdf": "https://arxiv.org/pdf/2512.22258.pdf", "meta_path": "data/raw/arxiv/meta/2512.22258.json", "sha256": "c7d974d686938a80b48615fb344018bc450d8cd956526b38e755fc1dcdabc3e2", "status": "ok", "fetched_at": "2026-02-18T02:23:51.098618+00:00"}, "pages": [{"page": 1, "text": "1 \n \nLogic Sketch Prompting (LSP): A Deterministic \nand Interpretable Prompting Method \nSatvik Tripathi1 \n1Department of Radiology, University of Pennsylvania, Philadelphia, PA, USA \nDecember 24, 2025 \nAbstract \n \nLarge language models (LLMs) excel at natural-language reasoning but remain unreliable \non tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompt- \ning (LSP) is a lightweight prompting framework that introduces typed variables, deterministic \ncondition evaluators, and a rule-based validator that produces traceable and repeatable out- \nputs. Using two pharmacologic logic-compliance tasks, we benchmark LSP against zero-shot \nprompting, chain-of-thought (CoT), and concise (brief) prompts across three open-weight mod- \nels: Gemma 2, Mistral, and Llama 3. \nAcross both tasks and all models, LSP consistently achieves the highest accuracy (0.83–0.89) \nand F1 (0.83–0.89), drastically outperforming zero-shot (0.24–0.60), brief prompts (0.16–0.30), \nand CoT (0.56–0.75). McNemar’s tests show statistically significant gains for LSP across \nnearly all comparisons (p < 0.01). \nThese results demonstrate that LSP enhances determinism, interpretability, and consistency \nwithout sacrificing performance, making it suitable for clinical, regulated, and safety-critical \ndecision-support systems. \nGithub: https://github.com/satviktri/LSP  \n \n \n1 Introduction \n \nLarge language models have become widely used tools for classification, information extraction, and \ngeneral-purpose reasoning across both clinical and non-clinical domains [1, 2]. Their ability to \nproduce coherent natural language output has made them attractive for decision-support tasks \n"}, {"page": 2, "text": "Logic Sketch Prompting \nTripathi \n2 \n \n \nthat historically required structured rule-based systems. Despite these advantages, current prompt- \ning strategies continue to exhibit substantial limitations when tasks require strict rule adherence, \ntransparent decision pathways, and reliable behavior across repeated evaluations [3, 4]. These re- \nquirements are especially important in healthcare, where inconsistent model outputs may directly \ninfluence clinical actions and patient outcomes [5]. \nZero-shot prompting provides minimal structure and depends entirely on implicit model reasoning. \nThis approach often produces variable results that are sensitive to small changes in phrasing, context \norder, or decoding parameters. Chain-of-thought prompting attempts to introduce inter- mediate \nreasoning steps, but the resulting explanations are not verifiable and frequently contain internal \ninconsistencies [6, 4]. These responses also depend on stochastic sampling, which limits their \nreproducibility in regulated environments [3]. Code-style prompting introduces more explicit rule \nstructure, yet it requires the model to interpret pseudo-code expressed in natural language or \nprogramming-like syntax [7]. This strategy can be brittle because small variations in formatting \nor phrasing often cause significant changes in output. Furthermore, the combination of detection \nrules and decision logic within the same prompt makes systematic debugging difficult. \nThese limitations have motivated the development of prompting frameworks that provide greater \nstructure and stability. Recent regulatory guidance, including the NIST AI Risk Management \nFramework, the European Union AI Act, and the joint FDA, Health Canada, and MHRA Good \nMachine Learning Practice principles, consistently emphasize the need for transparency, traceabil- \nity, and predictable system behavior [8, 9, 10]. Current prompting techniques do not fully satisfy \nthese expectations, particularly when models must apply deterministic logic or adhere to domain- \nspecific safety requirements. \nLogic Sketch Prompting (LSP) is designed to address these gaps by introducing a \nlightweight, interpretable structure on top of natural language input [11]. LSP separates the \ndetection of relevant information from the final decision through the use of explicit condition functions, \ntyped variables, and a deterministic validator that maps variable states to outcomes (shown in \nFigure 1). This design provides several advantages, including clear visibility into which rules were \ntriggered, consistent outputs across repeated runs, and a clean separation between pattern \ndetection and logical decision-making. LSP remains compatible with existing models and does \nnot require any changes to underlying model parameters or architectures. \n"}, {"page": 3, "text": "Logic Sketch Prompting \nTripathi \n3 \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 1: Comparison of prompting strategies. Zero-shot prompting relies entirely on implicit model \ninference, leading to non-deterministic and unverifiable outputs. Chain-of-thought prompting \nintroduces intermediate reasoning steps, but these remain stochastic, inconsistent, and difficult to \naudit. Code-style prompting attempts to impose structure through pseudo-code, yet often conflates \nextraction with decision logic and is prone to misinterpretation and execution errors. In contrast, \nLogic Sketch Prompting (LSP) separates learned pattern detection from symbolic decision making \nby populating a typed variable store through deterministic condition evaluators and applying a rule-\nbased validator to produce stable, auditable, and traceable outputs. \n"}, {"page": 4, "text": "Logic Sketch Prompting \nTripathi \n4 \n \n \ni \nIn this study, we evaluate LSP on two pharmacologic logic-compliance tasks using three open- \nweight large language models: Gemma, Mistral, and Llama. We compare LSP with zero-shot \nprompting, brief natural-language prompting, and chain-of-thought prompting. Our evaluation \nincludes accuracy, precision, recall, F1 score, and significance testing using McNemar’s test. Across \nall models and tasks, LSP demonstrates consistently superior performance and improved stability. \nThese findings support the use of minimal structured prompting layers to enhance reliability and \ninterpretability in applications that require deterministic rule evaluation [12]. \n \n2 \nMethods \n \n2.1 Logic Sketch Prompting Framework \n \nLogic Sketch Prompting (LSP) formalizes prompt-based reasoning as a sequence of deterministic \nlogical transformations applied to natural language text. Let the input text be represented as a \ntoken sequence \nT = (t1, t2, . . . , tn). \n \nLSP evaluates a set of condition functions over T , updates a typed variable store, and applies \na deterministic validator that maps the variable state to a final decision. This process yields a \ntransparent and reproducible reasoning trace. Core components of LSP are summarized in table 1. \n \n2.1.1 Typed Variable Store \n \nThe variable store is defined as a mapping \n \nV : {v1, v2, . . . , vk} → D, \n \nwhere D denotes the set of supported datatypes, including Boolean, integer, float, and enumerated \ncategorical values. Each variable vi is initialized to a default value v(0). Throughout evaluation, \nvariable updates follow the rule \nv(t+1) = fi(v(t), ci(T )), \ni \ni \n \nwhere ci(T ) is the output of the condition function associated with vi. \n"}, {"page": 5, "text": "Logic Sketch Prompting \nTripathi \n5 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nComponent \nDescription \nRole in LSP \nInput Text \nUnstructured  natural-language  input \nsuch as clinical notes, reports, or \nsentences \n \nProvides the raw linguistic \nsignal for downstream \nevaluation \nCondition \nEvalua- tors \nDeterministic checks applied to the in- \nput, including regex matching, key- \nword detection, semantic similarity, \nand numeric comparisons \n \nDetects whether predefined \nlogical conditions are \nsatisfied \nTyped Variables \nExplicit variables with defined data \ntypes (Boolean, Integer, Float, Enum) \ninitialized to default values \n \nStores condition outcomes \nin a structured and \ninterpretable form \nVariable \nUpdate \nRules \nDeterministic update functions map- \nping condition triggers to variable \nstates \n \nEnsures consistent and  \nrepeatable state transitions \n \nExecution Trace \nOrdered log of triggered conditions and \nvariable updates \nEnables transparency, \nauditability, and error \nanalysis \n \nDeterministic \nValidator \nRule-based decision function (e.g., \nIF– THEN logic, CNF/DNF \nexpressions) operating on final \nvariable states \nProduces a stable and re- \nproducible decision \nboundary \n \n \nOutput \nStructured model output, such as a la- \nbel with supporting evidence \nDelivers deterministic pre- \ndictions with an \ninterpretable reasoning \ntrace \n \n \nTable 1: Core components of Logic Sketch Prompting (LSP). LSP separates learned pattern \ndetection from deterministic decision logic, enabling interpretable, auditable, and reproducible language \nmodel behavior. \n"}, {"page": 6, "text": "Logic Sketch Prompting \nTripathi \n6 \n \n \n2.1.2 Condition Functions \n \nA condition function is expressed as \n \ncj : T → {0, 1} × Cj, \n \nwhere the first component indicates whether the condition was triggered and Cj contains optional \ncaptured values such as matched spans or extracted numerics. \nFour families of conditions were implemented: \n \nRegex(p), Keywords(A), \nNumericCompare(x, θ), \nSemanticSim(q, τ ), \n \nwhere p is a compiled pattern, A is a keyword set, and θ and τ denote numeric thresholds. A \ntriggered condition results in an update \nvj ← 1, \n \nand the update is appended to the execution trace E. \n \n2.1.3 Deterministic Validator \n \nAfter all conditions have been evaluated, the final state of the variable store \n \nV ∗ = (v∗, v∗, . . . , v∗) \n1 \n2 \nk \n \nis passed to a deterministic validator \ng : V → Y, \n \nwhere Y is the set of outcome labels. Two validator formulations were used: \n \nIF–THEN  chains. \nIf ϕ1(V ∗) then y1, \nelse if ϕ2(V ∗) then y2, . . . \n \nLogical normal forms. \ng(V ∗) = yc if ψc(V ∗) = 1, \n"}, {"page": 7, "text": "Logic Sketch Prompting \nTripathi \n7 \n \n \n\nwhere each ψc is written in conjunctive or disjunctive normal form. The validator has no side effects \nand contains no stochastic components. Figure 2 provides a schematic overview of the LSP pipeline, \nhighlighting the separation between condition evaluation, typed state tracking, and deterministic \nvalidation. \n \n2.2 Baseline Prompting Strategies \n \nThree comparison conditions were used. Zero-shot prompting involved a direct classification in- \nstruction without intermediate structure. Brief prompting provided a concise natural-language \ndescription of the task and the label space. Chain-of-thought prompting required the model to \nproduce intermediate reasoning steps before the final decision. All baselines relied entirely on \nnatural-language inference without external logic or variable storage. \n \n2.3 Models \n \nThree open-weight large language models were evaluated: Gemma, Mistral, and Llama. Each model \nprocessed identical prompts for all methods. Inference used deterministic decoding with fixed \ntemperature, top-p, and top-k parameters to ensure repeatability. \n \n2.4 Tasks and Data \n \nExperiments were conducted on the ADE-Corpus-V2 benchmark, which contains annotated sen- \ntences describing adverse drug events (ADEs) [12]. Two supervised tasks were evaluated. \n \nTask 1: Sentence-level ADE detection.  Given an input sentence s, the objective is to estimate \nthe binary variable \n \nY  =   \n1, if the sentence reports a patient-level ADE, \n      0, otherwise. \n \nThe “classification” split was used, containing 17,637 training instances and 5,879 test instances. \nSubsamples of size 5,000 were used where noted. This task requires identifying the presence of a \ndrug mention, a clinical effect, and an implied causal or temporal linkage. \n"}, {"page": 8, "text": "Logic Sketch Prompting \nTripathi \n8 \n \n \n \n \nFigure 2: Logic Sketch Prompting (LSP) framework. Natural-language input is processed \nthrough deterministic condition evaluators (e.g., pattern matching, semantic checks, and \nnumeric constraints) that populate a typed variable store. Variable states and execution traces are \nthen passed to a rule-based deterministic validator, which maps the final variable configuration to a stable \ndecision with an interpretable justification. By separating learned pattern detection from symbolic \ndecision logic, LSP enables reproducible, auditable, and model-agnostic reasoning for safety-critical \nclassification tasks. \n"}, {"page": 9, "text": "Logic Sketch Prompting \nTripathi \n9 \n \n \nTask 2:  Drug–effect relation classification. Each input consists of a triple (s, d, e) where d \nis a drug span and e is an effect span. The goal is to predict \n \ny = 1 iff the sentence asserts that drug d caused effect e. \n \nPositive instances originate from the drug ade relation subset. Natural negatives (i.e., sentences \ncontaining both d and e but without a causal relation) are included from the corpus to preserve \ndistributional realism. \n \n2.5 Models \n \nThree instruction-tuned, open-weight large language models were evaluated: \n \nGemma, \nMistral-Instruct, \nLlama 3.2. \n \nAll models were queried with identical decoding parameters. No fine-tuning, retrieval, or external \ncontext injection was used. \n \n2.6 Prompting Conditions \n \nLet p denote a prompting method, and let yˆ(m,p) denote the predicted label under model m and \nprompt p. Three prompting strategies were evaluated. \n \nLSP-prompt (ours). The model receives a compact, rule-structured checklist consisting of five \nbinary questions. The final decision is defined by the symbolic rule: \n \nyˆ = 1 ⇐⇒\n \nd ∈ s\n \n∧\n \ne ∈ s\n \n∧patient scope(s)∧\n \nexplicit link(s)∨temporal link(s)\n \n∧¬negated(s). Prompts \nrequire strict JSON output of the form \n{\"label\" : yˆ, \"evidence\" : . . . }. \n \nExactly one positive and one negative example are included. \n"}, {"page": 10, "text": "Logic Sketch Prompting \nTripathi \n10 \n \n \nZero-shot control.  This baseline is intentionally conservative. A prediction of 1 is permitted \nonly if an explicit causal trigger phrase is present (for example, ”caused by”, ”due to”, ”secondary \nto”, ”induced by”). Temporal relations without explicit causality (”after starting”, ”while taking”) \ndo not count as causal evidence. If uncertainty is present, the instruction defaults to yˆ = 0. This \nfavors precision at the cost of recall. \n \nBrief baseline. A minimal instruction directing the model to check for drug mention, effect \nmention, linkage, and absence of negation. As with LSP-prompt, strict JSON output is required. \nNo examples are provided. \n \n2.7 Evaluation Metrics \n \nFor a test set of size N , let yˆi be predictions and let yi be the corresponding ground truth labels. \nAccuracy, precision, recall, and F1 were computed to evaluate the performance. Uncertainty was \nsummarized using 95% bootstrap confidence intervals with 1,000 resamples. Paired system \ncomparisons were analyzed with McNemar’s test. Let n01 be the number of instances where method \nA is incorrect and method B is correct, and let n10 be the reverse. The statistic is given by \nχ2 = (|n01 − n10| − 1)2 . \nn01 + n10 \n \nThis test evaluates whether the disagreement pattern is symmetric, thereby supporting or rejecting the \nhypothesis of equal error rates. \n"}, {"page": 11, "text": "Logic Sketch Prompting \nTripathi \n11 \n \n \n2.8 Reproducibility Details \n \nAll experiments were conducted using deterministic decoding configurations. The temperature was \nset to zero, and top-p and top-k sampling were disabled to eliminate stochastic variation in token \ngeneration. Each model was queried using identical prompts and fixed decoding parameters across \nall prompting methods. No fine-tuning, retrieval augmentation, external tools, or additional context \ninjection was used. Condition evaluators based on pattern matching, keyword detection, and numeric \ncomparison were implemented deterministically. Semantic similarity checks were performed using \nthe same model instance used for classification, ensuring consistency within each experimental \ncondition. All reported metrics were computed on fixed test splits, and paired statistical comparisons \nwere performed on identical prediction sets. \n \n3 Results \n \nThe evaluation across the two ADE-Corpus-V2 tasks demonstrates that the LSP-prompt produces \nconsistently superior performance compared to the zero-shot and brief prompting baselines for all \nthree models. The results follow a stable pattern across Tasks 1 and 2, indicating that the observed \nimprovements derive from the structure of the prompt rather than model-specific behavior. In what \nfollows, we describe the empirical findings for each task in detail, emphasizing accuracy, F1 \nperformance, and statistical significance through paired McNemar analyses. \n \n3.1 Task 1: Sentence-level ADE Detection \n \nTask 1 involves determining whether a sentence contains a patient-level adverse drug event. Across \nGemma, Mistral, and Llama, the LSP-prompt achieved the strongest performance. For Gemma, \nthe LSP-prompt reached an accuracy of 0.85 with a corresponding F1 score of 0.85, whereas the \nzero-shot control achieved 0.58 accuracy and 0.45 F1. The brief baseline performed similarly in \naccuracy to zero-shot but suffered from extremely low recall, yielding an F1 score of only 0.17. \nThese results show that although the brief prompt contains the essential semantic criteria, it fails to \nconsistently detect drug–effect links, particularly when phrased indirectly or described in the context \nof patient history. \nFor Mistral, the superiority of LSP-prompt remained evident. The LSP-prompt attained 0.83 \n"}, {"page": 12, "text": "Logic Sketch Prompting \nTripathi \n12 \n \n \naccuracy and an F1 of 0.83. The zero-shot control achieved 0.54 accuracy and 0.48 F1, and the brief \nbaseline reached 0.65 accuracy but only 0.15 F1. Mistral exhibited the same sensitivity to phrasing \nobserved in Gemma: when ADEs were expressed through temporal cues such as “after starting the \nmedication” or “while taking,” the zero-shot control systematically defaulted to negation unless an \nexplicit causal trigger was present. The LSP-prompt, by contrast, captured temporal relationships \nreliably due to the explicit condition in its decision rule. \nFor Llama, the same pattern held. The LSP-prompt achieved 0.84 accuracy and 0.84 F1, \noutperforming the zero-shot configuration, which achieved 0.60 accuracy and 0.49 F1. The brief \nbaseline again demonstrated reasonable accuracy (0.62) but very weak F1 (0.16), driven largely by \nmissed causal statements. The consistency of these results across all three model families suggests \nthat the LSP-prompt’s performance gains reflect the benefits of rule-structured prompting rather than \nmodel-specific advantages. \nStatistical analyses further confirm these differences. For all three models, the paired McNe- mar \ntests indicate that the distributions of disagreements between LSP-prompt and each baseline are \nsignificantly asymmetric. For Gemma, the LSP-prompt vs. zero-shot and LSP-prompt vs. brief \ncomparisons yield approximate p-values of 0.006 and 0.003, respectively. For Mistral, the \ncorresponding values are 0.0047 and 0.0021, while for Llama the differences produce p ≈ 0.0080 \nand p ≈ 0.0032. These results provide strong statistical support for the claim that LSP-prompt \nconsistently reduces classification errors compared to natural-language prompting strategies. \n \n3.2 Task 2: Drug–Effect Relation Classification \n \nTask 2 evaluates whether the sentence explicitly asserts that a specific drug causes a specific effect. \nThis task requires careful handling of lexical variation, temporal expressions, mention matching, \nnegation, and alternative explanations. The LSP-style relation prompt introduces separate checks for \nexplicit causal phrases and temporal causal phrasing, constrains inference to patient-level scope, and \nenforces that both the drug and effect mentions correspond exactly to the spans under evaluation. \nThese constraints substantially influence performance. \nAcross all three models, the LSP-prompt again produced the best performance. Zero-shot \nprompting behaved consistently with its definition, which requires explicit causal markers such \nas “caused by”, “induced”, or “secondary to.” As a consequence, zero-shot prompting achieved \n"}, {"page": 13, "text": "Logic Sketch Prompting \nTripathi \n13 \n \n \nrelatively high precision but substantially lower recall, and it missed many valid ADE relations that \nwere expressed through temporal framing. The brief prompt lacked these structural safeguards and \nfrequently produced false positives by misinterpreting indication statements or medication benefit \ndescriptions as adverse outcomes. \nThe empirical pattern observed in Task 1 therefore persists in Task 2. The LSP-prompt achieves \nthe highest raw accuracy across Gemma, Mistral, and Llama, followed by zero-shot as a distant \nsecond, and brief as the weakest baseline. McNemar comparisons confirm that the LSP-prompt \nsignificantly outperforms both baselines in a majority of model-wise pairings, reinforcing that the \nadvantage arises from the rule-shaped architecture of the prompt rather than idiosyncratic model \nbehavior. The LSP-prompt’s explicit handling of temporal links is a major source of improvement, \nespecially for ADE-Corpus-V2, where many causal statements are written temporally rather than \nthrough explicit causal connectors. \n \n3.3 Combined Results Across Tasks \n \nAveraging across the two tasks and all three models produces a consistent ranking that holds \nthroughout all experimental conditions: \n \nLSP-prompt > zero-shot > brief(+CoT) \n \nThis ordering is stable whether the model is evaluated on sentence-level ADE identification or on \nstructured drug–effect relational reasoning. The combined performance gains of the LSP-prompt are \nparticularly pronounced in F1 score, reflecting a more balanced tradeoff between precision and recall \nrelative to the baselines. The brief baseline fails consistently on recall in both tasks, and the zero-\nshot baseline fails primarily on recall in Task 2 and partially on Task 1. \nLSP-prompt performance also exhibits lower variance across models compared to the baselines. Zero-\nshot and brief prompting exhibit high sensitivity to phrasing, contextual ambiguity, and ex- pression of \ncausal relations. In contrast, LSP-prompt constrains the inference pathway to a fixed sequence of checks, \nresulting in more uniform behavior across model families. \n"}, {"page": 14, "text": "Logic Sketch Prompting \nTripathi \n14 \n \n \n \nModel \nPrompting Method Mean Accuracy Mean F1 \nGemma \nZero-shot \n0.58 \n0.47 \nBrief \n0.52 \n0.22 \nChain-of-Thought \n0.68 \n0.64 \nLSP (ours) \n0.86 \n0.86 \nMistral \nZero-shot \n0.55 \n0.50 \nBrief \n0.61 \n0.21 \nChain-of-Thought \n0.70 \n0.65 \nLSP (ours) \n0.83 \n0.83 \nLlama \nZero-shot \n0.60 \n0.51 \nBrief \n0.63 \n0.23 \nChain-of-Thought \n0.74 \n0.73 \nLSP (ours) \n0.84 \n0.84 \nTable 2: Combined performance across both tasks from ADE-Corpus-V2 (Task 1: sentence-level \nADE detection; Task 2: drug–effect relation classification). Reported values represent mean accu- \nracy and mean F1 score across the two tasks. Logic Sketch Prompting (LSP) consistently achieves \nthe strongest overall performance across all evaluated models. \n \n3.4 Qualitative Error Patterns \n \nThe nature of the remaining errors is aligned with the structural design of each prompt. For \nLSP-prompt, the most common false positives occur in sentences that mention both the drug \nand the effect but describe intended therapeutic outcomes rather than adverse reactions. These false \npositives arise because the lexical patterns sometimes resemble ADE statements despite the presence \nof contextual cues indicating therapeutic intent. \nFor zero-shot prompting, false negatives are concentrated heavily in sentences where causality is \nexpressed temporally, such as “after beginning the medication the patient developed rash” or \n“symptoms appeared shortly after initiation.” Since the zero-shot prompt disallows temporal linkage \nas evidence for causality, the model systematically predicts the negative class in such cases. The \nbrief prompt suffers from both false positives and false negatives due to its lack of structural \nconstraints; without explicit rules for scope, negation, or evidence type, the model frequently \nmisinterprets mention co-occurrence as causal relation. \n"}, {"page": 15, "text": "Logic Sketch Prompting \nTripathi \n15 \n \n \n3.5 Summary of Findings \n \nThe collective results demonstrate that a compact, rule-governed prompting structure yields \nsubstantial gains in accuracy, F1, stability, and interpretability across both ADE-Corpus-V2 tasks \nand all tested model families. The improvements are statistically significant and consistent, \nindicating that the LSP-prompt methodology provides a robust and generalizable framework for \nreasoning over medication safety statements in natural language clinical text. \n \n4 Discussion \n \nAcross both ADE-Corpus-V2 tasks and all three model families, the LSP-prompt consistently \noutperformed natural-language prompting methods. The gains were observed not only in accuracy \nand F1 but also in the stability of predictions across Gemma, Mistral, and Llama [13, 14]. This \nuniformity indicates that the improvement arises from the prompting structure itself rather than \nmodel-specific training or architecture. \nA key finding is that the LSP-prompt reduces the performance gap between smaller and larger \nmodels. Under zero-shot and brief prompting, smaller models struggle to infer implicit causal rules, \nresolve temporal associations, or correctly apply negation, which results in high variability and \nfrequent classification errors [15, 13]. The LSP-prompt mitigates these limitations by externalizing \nthe decision process. Instead of requiring the model to reconstruct a multi-step reasoning chain \ninternally, the prompt directs it through a fixed set of binary checks governing drug detection, effect \ndetection, patient scope, linkage type, and negation. This structure reduces the burden on latent \nreasoning and allows smaller models to approximate the performance of larger instruction-tuned \nmodels more closely. \nLarger models also benefit from this structure. Even when instruction-tuning improves zero- shot \nreasoning, uncontrolled natural-language prompts still introduce ambiguity, particularly when \nsentences permit several plausible interpretations [14]. The LSP-prompt narrows this ambiguity \nby defining a deterministic decision boundary that prioritizes clinically relevant cues over stylistic \nphrasing. As a result, recall improves substantially without sacrificing precision. \nThe statistical analyses reinforce this interpretation. McNemar tests showed strongly asymmetric \ndisagreement patterns in favor of LSP-prompt across all model pairs [16]. The magnitude of \n"}, {"page": 16, "text": "Logic Sketch Prompting \nTripathi \n16 \n \n \nthese differences was similar for smaller and larger models, suggesting that LSP-prompt provides \na scale-independent improvement in the reliability of ADE reasoning. \nThe method also aligns well with the linguistic properties of clinical text. Many true ADE \nstatements rely on temporal rather than explicit causal phrasing [15]. Natural-language prompting, \nespecially zero-shot, frequently misses these patterns. The LSP-prompt encodes temporal relations \nas valid evidence, enabling models of all sizes to capture an important information channel. \nRemaining errors under LSP often arise from therapeutic indication statements, which can resemble \nADE descriptions in surface form and may require additional contextual features to disambiguate \n[17, 14]. \nLSP does enforce determinism at the level of variable updates and final decision validation, but it \ndoes not render the entire inference pipeline independent of the underlying language model. In \nparticular, some condition evaluators, such as semantic similarity checks, rely on model-internal \nrepresentations that may vary across architectures or implementations. However, this model \ndependence is localized to the detection stage and does not affect the deterministic execution of the \nrule-based validator once variable states are set. As a result, while complete end-to-end determinism \ncannot be guaranteed, LSP ensures that decision logic, rule application, and outcome mapping \nremain stable, traceable, and reproducible for a fixed set of condition outputs. \nOverall, these findings show that structuring the inference pathway is an effective and lightweight \nmeans of stabilizing language model behavior in biomedical extraction tasks [13]. By constraining \ndecision-making through explicit rules and leaving pattern recognition to the model, the LSP- \nprompt creates a hybrid reasoning environment that is both interpretable and robust across model \nscales. \n \n5 Limitations & Scope \n \nThis work has a few limitations that define the scope of applicability of LSP. First, LSP depends on \nthe coverage and quality of predefined condition evaluators, including pattern matching, keyword \ndetection, and semantic checks. Errors or omissions at this stage can lead to incorrect variable states \nand propagate deterministically to the final decision. As a result, LSP does not eliminate detection \nerrors but rather constrains how such errors influence downstream reasoning. \n"}, {"page": 17, "text": "Logic Sketch Prompting \nTripathi \n17 \n \n \nSecond, constructing LSP rules requires domain knowledge to specify relevant variables, \nconditions, and decision boundaries. While this explicit structure improves transparency and \nauditability, it introduces manual design effort and may limit the ability to transfer to new tasks \nwithout additional rule engineering. The method therefore favors settings where task requirements \nand logical criteria can be clearly articulated. \nThird, although LSP enforces determinism in variable updates and final validation, some \ncondition evaluators may still rely on model-dependent components, such as semantic similarity \njudgments. In these cases, stochasticity is confined to localized detection steps and does not affect \nthe deterministic execution of the decision logic, but complete end-to-end determinism is not guaran- \nteed. \nFinally, the experiments in this study focus on rule-constrained biomedical classification tasks \ninvolving adverse drug event reasoning. The effectiveness of LSP for open-ended generation, creative \nreasoning, or functions without well-defined logical structure was not evaluated. LSP is therefore \nbest suited for decision-support settings that prioritize reproducibility, interpretability, and stable rule \nadherence over unconstrained language generation. \n \n6 Conclusion \n \nThis study demonstrates that a compact rule-structured prompting strategy substantially improves \nadverse drug event extraction for open-weight language models. The LSP-prompt consistently out- \nperformed zero-shot and brief prompting across two tasks and three model families, achieving higher \naccuracy, stronger F1 scores, and more stable decision boundaries. These gains are independent of \nmodel size. Smaller models benefit from reduced reliance on implicit multi-step reasoning, while \nlarger models benefit from clearer and less ambiguous decision criteria. The method requires no \nfine-tuning, external tools, or additional training data, yet meaningfully improves error profiles and \nthe capture of both explicit and temporal causal cues. Overall, these findings show that lightweight \nsymbolic scaffolding can guide language models toward more reliable and interpretable clinical \nreasoning, offering a practical and scalable approach for biomedical text analysis in settings where \ndeterminism, transparency, and cross-model stability are essential. \n"}, {"page": 18, "text": "Logic Sketch Prompting \nTripathi \n18 \n \n \nReferences \n \n[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla \nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini \nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya \nRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, \nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, \nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-\nshot learners. In Advances in Neural Information Processing Systems 33: NeurIPS 2020, \nAnnual Conference on Neural Information Processing Systems 2020, pages 1877–1901. \nCurran Associates, Inc., 2020. \nAlso available as arXiv:2005.14165 with DOI \n10.48550/arXiv.2005.14165. \n[2] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. \nChi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large \nlanguage models. In Advances in Neural Information Processing Systems 35: NeurIPS \n2022, Annual Conference on Neural Information Processing Systems 2022, pages 5482–5496. \nCurran \nAssociates, \nInc., \n2022. \nSee \nalso \narXiv:2201.11903, \nDOI \n10.48550/arXiv.2201.11903. \n[3] Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. arXiv \npreprint, 2022. Preprint; DOI 10.48550/arXiv.2208.14271. \n[4] Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models d o n ’t  \nalways say what they think: Unfaithful explanations in chain-of-thought prompting. In Ad- \nvances in Neural Information Processing Systems 36: NeurIPS 2023, Annual Conference on \nNeural Information Processing Systems 2023. Curran Associates, Inc., 2023. Also available as \narXiv:2305.04388 with DOI 10.48550/arXiv.2305.04388. \n[5] Andrew L. Beam and Isaac S. Kohane. Big data and machine learning in health care. \nJAMA, 319(13):1317–1318, 2018. \n[6] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we \ndefine and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association \nfor Computational Linguistics, pages 4198–4205, 2020. \n"}, {"page": 19, "text": "Logic Sketch Prompting \nTripathi \n19 \n \n \n[7] Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Text \nmodular networks: Learning to decompose tasks in the language of existing models. In Pro- \nceedings of the 2021 Conference of the North American Chapter of the Association for Com- \nputational Linguistics: Human Language Technologies, pages 1264–1279, 2021. \n[8] National Institute of Standards and Technology. Artificial intelligence risk management frame- \nwork (ai rmf 1.0). NIST publication, January 2023. \n[9] U.S. Food and Drug Administration, Health Canada, and Medicines and Healthcare products \nRegulatory Agency. Good machine learning practice for medical device development: Guiding \nprinciples. Guidance document released by regulatory agencies, October 2021. Joint guidance \noutlining principles for good machine learning practice in medical device development. \n[10] European Union. Regulation (eu) 2024/1689 of the european parliament and of the council \nlaying down harmonised rules on artificial intelligence (artificial intelligence act). Official \nJournal of the European Union, June 2024. Adopted 13 June 2024. \n[11] Tarek R. Besold, Artur d’Avila Garcez, Sebastian Bader, Howard Bowman, Pedro \nDomingos, Pascal Hitzler, Kai-Uwe Kºhnberger, Luis C. Lamb, Daniel Lowd, Priscila \nMachado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung Poon, and Gerson \nZaverucha. Neural-symbolic learning and reasoning: A survey and interpretation. arXiv \npreprint, 2017. Preprint; DOI 10.48550/arXiv.1711.03902. \n[12] Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann- \nApitius, and Luca Toldo. Development of a benchmark corpus to support the automatic \nextraction of drug-related adverse effects from medical case reports. Journal of Biomedical \nInformatics, 45(5):885–892, 2012. \n[13] Sina Shool, Sara Adimi, Reza Saboori Amleshi, Ehsan Bitaraf, Reza Golpira, and \nMahmood Tara. A systematic review of large language model (llm) evaluations in clinical \nmedicine. BMC Medical Informatics and Decision Making, 25:117, March 2025. \n[14] Sonish Sivarajkumar, Mark Kelley, Alyssa SamolykMazzanti, Shyam Visweswaran, and \nYan- shan Wang. An empirical evaluation of prompting strategies for large language \nmodels in \n"}, {"page": 20, "text": "Logic Sketch Prompting \nTripathi \n20 \n \n \nzeroshot clinical natural language processing: Algorithm development and validation study. \nJMIR Medical Informatics, 12:e55318, April 2024. \n \n[15] Yohan Bonescki Gumiel, Lucas Emanuel Silva e Oliveira, Vincent Claveau, Natalia Grabar, \nEmerson Cabrera Paraiso, Claudia Moro, and Deborah Ribeiro Carvalho. Temporal relation \nextraction in clinical texts: A systematic review. ACM Computing Surveys, 54(7):1–36, 2022. \nPublished online September 2021; final version 2022. \n[16] Thomas G. Dietterich. Approximate statistical tests for comparing supervised classification \nlearning algorithms. Neural Computation, 10(7):1895–1923, September 1998. \n[17] Salisu Modi, Khairul Azhar Kasmiran, Nurfadhlina Mohd Sharef, and Mohd Yunus Sharum. \nExtracting adverse drug events from clinical notes: A systematic review of approaches used. \nJournal of Biomedical Informatics, 151:104603, March 2024. \n"}, {"page": 21, "text": "Logic Sketch Prompting \nTripathi \n21 \n \n \nA Appendix \n \nA.1 Terminology and Abbreviations \n \nTable 3 summarizes the primary abbreviations and technical terms used throughout this work. \nDefinitions are written to reflect practical usage in ADE extraction and LLM-based clinical \nreasoning. \n \nTerm \nDefinition \nADE \nAdverse Drug Event; an undesirable clinical effect caused by \na medication. \nDrug–Effect Rela- A structured assertion that a drug d caused an effect e for \ntion \na patient. \nLSP \nLogic Sketch Prompting; a rule-structured prompting frame- \nwork with typed variables and deterministic validation. \nExplicit Link \nA causal phrase such as “caused by”, “induced by”, or “sec- \nondary to”. \nTemporal Link \nA temporal description implying causality, for example “af- \nter starting” or “shortly after initiation”. \nNegation \nAny linguistic cue that reverses or cancels a causal relation \n(e.g., “did not cause”, “no evidence of”). \nPatient Scope \nRestriction that the ADE pertains to the patient described in \nthe sentence rather than a drug class or general knowl- edge. \nZero-shot \nPrompting \nClassification using minimal instructions and no specific ex- \nemplars. \nCoT \nChain-of-Thought prompting; a prompting style that re- \nquires the model to produce explicit intermediate reasoning \nsteps. \nBrief Prompting \nMinimal structured instruction without explicit logical scaf- \nfolding. \nValidator \nDeterministic rule mapping from the variable store V to a \nfinal label. \nTable 3: Glossary of abbreviations and task-specific terminology. \n \nA.2 Examples of Prompting Techniques \n \nTo illustrate the differences among prompt families, the following examples demonstrate how the \nsame ADE classification query may be instructed using zero-shot, brief, CoT, and LSP-prompt \nstyles. \n \nZero-shot. \n \nClassify whether the sentence reports that the drug caused the effect. \nOutput \n"}, {"page": 22, "text": "Logic Sketch Prompting \nTripathi \n22 \n \n \n1 or 0. \nSentence: \"The patient developed rash after starting amoxicillin.\" \n \nBrief. \n \nLabel 1 if the drug is present, the effect is present, the effect is due to \nthe drug, and the statement is not negated. \nOutput JSON: {\"label\": ...}. \nSentence: \"...\" \n \nChain-of-Thought. \n \nThink step by step. \nDoes the sentence mention the drug? Does it describe \nthe effect? \nDoes it link them? Finally output JSON with a label based \nonly on the final reasoning. \nSentence: \n\"...\" \n \nLSP-prompt (ours). \n \nAnswer the following binary questions silently. \n1. \nIs the DRUG mentioned? \n2. \nIs the EFFECT mentioned? 3. \nIs the statement about the patient? \n4. \nIs there an explicit causal link?  5. If not, is there a temporal link? \nSet \nlabel 1 if and only if: (1)& (2)& (3)& ((4) OR (5))& no negation.\n \nReturn JSON: {\"label\": ..., \"evidence\": \n...}.\n \nSentence: \n\"...\" \nA.3 C. How to Write an LSP \n \nLSP can be viewed as a structured program defined by a sequence of typed variables, condition \nfunctions, and a deterministic validator. A generic LSP specification can be written in the following \nform. \n \nStep 1: Define variables. Let the variable store be \n \nV = { v1, v2, . . . , vk }. \n \nEach variable has a type, default state, and allowable updates. \n"}, {"page": 23, "text": "Logic Sketch Prompting \nTripathi \n23 \n \n \nStep 2:  Define condition evaluators. For each variable vi, specify a condition function \n \nci(T ) → {0, 1} × Ci, \n \nwhere the first output denotes trigger status and the second optionally returns captured spans. \n \nStep 3:  Specify rule updates. Rules apply the update \n \nvi ← 1 if ci(T ) =1, \nand each update is recorded in the execution trace. \nStep 4: Define the validator. \nA final decision rule is defined as         \n                                                            g(V ∗)    =       1,                if ϕ(V ∗) is true, \n                                                                                                       0,                otherwise, \n \nwhere ϕ may be written either as an ordered set of IF–THEN clauses or in CNF/DNF form. \n \nWorked Example. An LSP for drug–effect relation classification can be written as: \n \n \n \nvdrug = 1 \niff a drug name is detected, \nveffect = 1 \niff an effect keyword is detected, \nvscope = 1 \niff patient-level reference is present, \nvexplicit = 1 iff an explicit causal phrase is matched, \nvtemporal = 1 \niff a temporal causal construction is matched, \nvnegated = 1 \niff negation cues are detected. \n \nThe validator applies: \n \n \nyˆ = 1 iff vdrug ∧ veffect ∧ vscope ∧ (vexplicit ∨ vtemporal) ∧ ¬vnegated. \n"}, {"page": 24, "text": "Logic Sketch Prompting \nTripathi \n24 \n \n \nA.4 D. A Prompt That Converts Any Prompt Into LSP Format \n \nThe following meta-prompt can be used to automatically transform any natural-language prompt into \nan LSP-style rule-structured prompt. The meta-prompt instructs the model to identify the \nclassification criteria, extract the relevant variables, formalize the conditions, and construct a de- \nterministic validator. \n \n \n1. Extract all binary conditions required for the decision. 2. Rewrite each condition \nas a silent yes/no question. 3. Assign each condition to a variable name. 4. Construct \na single decision rule that uses AND, OR, and NOT to combine these variables. 5. \nRequire strict JSON output: {”label”: ..., ”evidence”: ...}. 6. Include exactly one \npositive and one negative example if possible. \nReturn the final LSP-form prompt. Input prompt: ¡¡¡USERP ROMPT >>> Y ouareanLSP − \ncompiler.Givenanypromptthatrequiresclassificationordecision−making, convertitintoanLSP − \nstylepromptasfollows : \n1. Extract all binary conditions required for the decision. 2. Rewrite each condition \nas a silent yes/no question. 3. Assign each condition to a variable name. 4. Construct \na single decision rule that uses AND, OR, and NOT to combine these variables. 5. \nRequire strict JSON output: {”label”: ..., ”evidence”: ...}. 6. Include exactly one \npositive and one negative example if possible. \nReturn the final LSP-form prompt. Input prompt: ¡¡¡USERP ROMPT >>> \n \nThis meta-prompt provides a template that can convert high-level instructions into determin- istic \nlogic programs suitable for biomedical information extraction and other structured reasoning \nsettings. \n"}]}