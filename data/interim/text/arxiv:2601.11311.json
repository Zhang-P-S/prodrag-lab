{"doc_id": "arxiv:2601.11311", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.11311.pdf", "meta": {"doc_id": "arxiv:2601.11311", "source": "arxiv", "arxiv_id": "2601.11311", "title": "FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning", "authors": ["Zhihan Yang", "Jiaqi Wei", "Xiang Zhang", "Haoyu Dong", "Yiwen Wang", "Xiaoke Guo", "Pengkun Zhang", "Yiwei Xu", "Chenyu You"], "published": "2026-01-16T14:08:51Z", "updated": "2026-01-16T14:08:51Z", "summary": "Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.11311v1", "url_pdf": "https://arxiv.org/pdf/2601.11311.pdf", "meta_path": "data/raw/arxiv/meta/2601.11311.json", "sha256": "5ee689cfe5ab3eb7d3ddc5bd3863f2bdafea0143053a4a2b47ce7fa356bd9a5a", "status": "ok", "fetched_at": "2026-02-18T02:21:24.992218+00:00"}, "pages": [{"page": 1, "text": "FORESTLLM: Large Language Models Make Random Forest Great\non Few-shot Tabular Learning\nZhihan Yang1*†, Jiaqi Wei2*, Xiang Zhang3, Haoyu Dong4, Yiwen Wang5, Xiaoke Guo2,\nPengkun Zhang6, Yiwei Xu7, Chenyu You8\n1 National University of Singapore 2 Zhejiang University 3 University of British Columbia 4 Microsoft\n5 University of Science and Technology of China 6 South China University of Technology\n7 Cisco 8 Stony Brook University\nAbstract\nTabular data high-stakes critical decision-making in domains\nsuch as finance, healthcare, and scientific discovery. Yet,\nlearning effectively from tabular data in few-shot settings,\nwhere labeled examples are scarce, remains a fundamen-\ntal challenge. Traditional tree-based methods often falter in\nthese regimes due to their reliance on statistical purity met-\nrics, which become unstable and prone to overfitting with\nlimited supervision. At the same time, direct applications\nof large language models (LLMs) often overlook its inher-\nent structure, leading to suboptimal performance. To over-\ncome these limitations, we propose FORESTLLM, a novel\nframework that unifies the structural inductive biases of de-\ncision forests with the semantic reasoning capabilities of\nLLMs. Crucially, FORESTLLM leverages the LLM only dur-\ning training, treating it as an offline model designer that en-\ncodes rich, contextual knowledge into a lightweight, inter-\npretable forest model, eliminating the need for LLM infer-\nence at test time. Our method is two-fold. First, we intro-\nduce a semantic splitting criterion in which the LLM eval-\nuates candidate partitions based on their coherence over both\nlabeled and unlabeled data, enabling the induction of more\nrobust and generalizable tree structures under few-shot su-\npervision. Second, we propose a one-time in-context infer-\nence mechanism for leaf node stabilization, where the LLM\ndistills the decision path and its supporting examples into a\nconcise, deterministic prediction, replacing noisy empirical\nestimates with semantically informed outputs. Across a di-\nverse suite of few-shot classification and regression bench-\nmarks, FORESTLLM achieves state-of-the-art performance.\nThese results highlight the promise of using LLMs as offline\nmodel designers, rather than online predictors, for scalable\nand efficient tabular learning under data scarcity. Our code is\navailable at https://github.com/kelvin715/ForestLLM.\nIntroduction\nTabular data, structured as collections of heterogeneous,\nfeature-label pairs, is central to decision-making in critical\ndomains such as healthcare, finance, and public policy (Her-\nnandez et al. 2022; Frosch et al. 2010; Assefa et al. 2020;\nJohnson et al. 2016; Ulmer, Meijerink, and Cin`a 2020; Arun,\nIshan, and Sanmeet 2016; Chen et al. 2023; Zhang et al.\n2025a). However, building reliable models from such data in\n*Equal contribution.\n†Work done during an internship at Microsoft Research Asia.\nTable 1: Comparison of LLM-based Tabular Methods.\nMethod\nNo LLM\nFine-tuning Needed\nNo LLM\nNeeded at Inference\nTask Support\nClassification\nRegression\nLIFT (NeurIPS'22)\n✗\n✗\n✓\n✓\nTabLLM (AISTATS'23)\n✗\n✗\n✓\n✗\nFeatLLM (ICML'24)\n✓\n✓\n✓\n✗\nTP-BERTa (ICLR'24)\n✓\n✗\n✓\n✓\nP2T (COLM'24)\n✓\n✗\n✓\n✓\nFORESTLLM (Ours)\n✓\n✓\n✓\n✓\nthe few-shot regime remains a longstanding challenge. Un-\nlike images or text, tabular data lacks spatial or sequential\nstructure, and its highly combinatorial, non-Euclidean na-\nture undermines the geometric assumptions underlying most\nmodern few-shot learning techniques (Chen et al. 2018; Ma-\njumder et al. 2022; Nam et al. 2023b). As a result, these\nmethods often perform poorly on tabular tasks with limited\nsupervision.\nTree-based models such as Random Forests and Gra-\ndient Boosted Trees (Chen and Guestrin 2016; Ke et al.\n2017; Prokhorenkova et al. 2018) remain strong perform-\ners on tabular benchmarks, thanks to their inductive bias to-\nward hierarchical data partitioning. Yet under low-data con-\nditions, their reliability degrades. Conventional split selec-\ntion heuristics, like information gain or Gini impurity, rely\nentirely on labeled data and become unstable when estimates\nare noisy. Moreover, predictions at the leaves are often based\non only a handful of examples, resulting in high variance and\npoor calibration. These issues are further exacerbated by the\nfact that unlabeled data, though often available, is ignored\nentirely during training.\nIn parallel, the generalization and reasoning capabilities\nof Large Language Models (LLMs) have spurred interest in\napplying them to tabular learning (Dinh et al. 2022; Hegsel-\nmann et al. 2023; Han et al. 2024; Yan et al. 2024; Nam\net al. 2024; Wen et al. 2024; Zhang et al. 2025b; Yan et al.\n2025). But most existing approaches serialize rows into text\nand rely on prompting or fine-tuning, discarding the struc-\ntural regularities that classical models are designed to ex-\nploit. Despite their flexibility, such methods frequently un-\nderperform tree ensembles on core supervised tasks, partic-\nularly in few-shot settings (Grinsztajn, Oyallon, and Varo-\nquaux 2022; Borisov et al. 2022; McElfresh et al. 2023;\nZab¨ergja, Kadra, and Grabocka 2024).\narXiv:2601.11311v1  [cs.LG]  16 Jan 2026\n"}, {"page": 2, "text": "In this paper, we explore a new direction: rather than re-\nplacing classical tabular models with foundation models,\ncan we instead enlist LLMs as offline architects, guiding\nmodel design during training but removed entirely at test\ntime? Can we use LLMs to imbue decision trees with seman-\ntic inductive priors, while preserving their efficiency, robust-\nness, and interpretability?\nTo this end, we propose FORESTLLM, a novel deci-\nsion forest framework that leverages a LLM as an offline\nmodel designer, guiding model construction without partic-\nipating in inference. Rather than using the LLM for infer-\nence, FORESTLLM engages it once during training to guide\nboth split selection and leaf prediction. Our FORESTLLM is\nfast, interpretable, and entirely LLM-free at test time. At its\ncore, FORESTLLM rethinks decision tree construction un-\nder few-shot supervision through a new framework for split-\nting and prediction. Our approach is two-fold.\nFirst, we introduce semi-supervised semantic tree induc-\ntion, a split selection strategy that replaces conventional\nlabel-driven heuristics with LLM-guided semantic evalua-\ntion. Each candidate split is summarized into a prompt that\ncaptures labeled and unlabeled feature distributions along\nwith local tree context. The LLM then ranks these candi-\ndates based on their semantic plausibility, effectively lever-\naging unlabeled data to guide generalization under limited\nsupervision. Second, we propose in-context leaf label in-\nference, a one-time procedure for producing stable, static\npredictions at leaf nodes. Each decision path is translated\ninto a natural language rule, which, together with its sup-\nporting examples, is used to prompt the LLM for a final\nprediction. This decouples inference from sparse empiri-\ncal estimates, reducing variance while preserving efficiency.\nFORESTLLM bakes in the structural bias of decision forests\nwith the semantic abstraction capabilities of LLMs, without\nrequiring fine-tuning or inference time access (Table 1). It\noperates entirely in a few-shot, in-context regime and sup-\nports both classification and regression tasks. By reposition-\ning the LLM as an offline architect rather than an online or-\nacle, FORESTLLM enables scalable, low-resource learning\nfrom tabular data.\nWe evaluate FORESTLLM across a broad set of few-shot\ntabular benchmarks and find that it consistently matches or\noutperforms strong baselines, often achieving state-of-the-\nart or second-best performance with as few as 4 to 48 labeled\nexamples. Notably, it retains strong performance on datasets\nthat were unseen during LLM pretraining, underscoring its\ngeneralization ability beyond memorized priors.\nOur key contributions are as follows:\n• We introduce the concept of using a LLM as an offline\nmodel designer, guiding training-time structure induc-\ntion without relying on the LLM at test time.\n• We propose a novel semantic splitting strategy that re-\nplaces statistical heuristics with LLM-evaluated prompts\nover labeled and unlabeled data.\n• We develop a single-pass, in-context leaf prediction\nmechanism that produces deterministic, low-variance\noutputs without empirical averaging or LLM inference.\n• We demonstrate that FORESTLLMachieves state-of-the-\nart few-shot performance on a wide range of tabular\nclassification and regression benchmarks, without fine-\ntuning or inference-time prompting.\nRelated Work\nFew-Shot\nand\nSemi-Supervised\nTabular\nLearning.\nFew-shot tabular learning poses a core challenge: enabling\ngeneralization from limited labeled data (Chen et al. 2018;\nMajumder et al. 2022). A common line of work ad-\ndresses this by leveraging unlabeled data to learn expres-\nsive representations via semi- or self-supervised pretrain-\ning (Somepalli et al. 2021; Yoon et al. 2020; Ucar, Haji-\nramezanali, and Edwards 2021; Bahri et al. 2022; Wang and\nSun 2022). In contrast, FORESTLLM takes a fundamentally\ndifferent approach: rather than using unlabeled data to pre-\ntrain encoders, it incorporates feature-level statistics from\nunlabeled examples directly into tree construction. Through\nsemi-supervised semantic tree induction, these statistics are\nsummarized and passed to an LLM that guides split deci-\nsions based on semantic coherence, effectively treating un-\nlabeled data as a prior over model structure, rather than input\nembeddings.\nDecision Trees, Deep Models, and LLMs for Tabular\nData.\nModeling tabular data has traditionally relied on\ntwo dominant paradigms: tree-based ensembles such as GB-\nDTs (Chen and Guestrin 2016; Ke et al. 2017), and deep\nneural networks (Kadra et al. 2021; Gorishniy et al. 2021;\nBorisov et al. 2022). While tree models encode strong in-\nductive biases for hierarchical partitioning, their perfor-\nmance degrades in low-data regimes due to brittle splitting\nheuristics and high-variance leaf estimates. Deep models,\nin contrast, often fail to align with the structural proper-\nties of tabular data, resulting in poor generalization (McEl-\nfresh et al. 2023; Zab¨ergja, Kadra, and Grabocka 2024).\nFORESTLLM addresses both limitations by replacing un-\nstable statistical splits with LLM-guided semantic partition-\ning, and by stabilizing predictions through in-context leaf la-\nbel inference, a one-time procedure that bypasses noisy ag-\ngregation. In parallel, recent work has explored LLMs for\ntabular tasks (Dinh et al. 2022; Hegselmann et al. 2023),\nleveraging fine-tuning (Hu et al. 2021), in-context prompt-\ning (Nam et al. 2023a), serialization-based inference (Nam\net al. 2024), or synthetic data generation (Han et al. 2024).\nHowever, these methods often incur high inference costs,\nlose structural information through serialization, or fail to\nproduce standalone models (Manikandan, Jiang, and Kolter\n2023). In contrast, FORESTLLM casts the LLM as an of-\nfline model designer, used solely during training to synthe-\nsize a semantically grounded decision forest. The resulting\nmodel preserves the computational efficiency of classical\ntrees while benefiting from the semantic priors encoded in\nLLMs, without introducing inference-time latency or depen-\ndence.\n"}, {"page": 3, "text": "Unlabeled\nLabeled\nStep A: Tabular Decision Trees Construction\nStep B: Leaf Label Inference\nTabular\nSamples\nTabular \nDecision Tree\nFeature \nSummary \nSemantic Data \nDistillation\nRow-wise \nSerialization\nNode Characteristics\nLocal location awareness\nContextual history awareness\nSemantic-Prioritized Splitting\nClassification\nVoting\nIn-Context Leaf Label Inference\nLabel Inference\nGiven the rule parsed from root to leaf \nand matched few-shot examples, predict the \nmost appropriate label for the leaf.\nMatch Few-shot \nSamples\nRegression\nAveraging\nFor Numerical Feature: If Age > 42.5, go left; else, go right.\nFor Categorical Feature: If Job∈{“A”, “B”}, go left; otherwise, go right.\nSemantic\nSplit Criteria\nRule Induction\nIF\nExtract conditions \nfrom root-to-leaf path as rules\nRandom Forest\nStep C: Prediction\nStep 1: Causal Feature Probing\nAnalyze the causal tendencies \nbetween features and the task, \nbased on general knowledge.\nStep 2: Evidential Split Synthesis\nBased on the prior reasoning and \nlocal characteristics, infer one \nsplit condition to divide the data.\nTree!\nTree\"\nTree#\nTree$\n…\nFigure 1: An overview of our proposed FORESTLLM framework for few-shot tabular learning.\nMethod\nProblem Formulation and Our Approach\nWe consider the problem of few-shot tabular learning, where\nthe goal is to learn a function h : Rd →Y from a small\nlabeled dataset DL = {(xi, yi)}NL\ni=1 and an optional unla-\nbeled dataset DU = {xj}NU\nj=1, with NL ≪NU. Here, the\nlabeled sample size NL corresponds to the shot count in\nour experiments. Classical decision tree methods optimize\neach node split s∗by optimizing statistical objectives such\nas information gain or Gini impurity. These heuristics, how-\never, rely entirely on DL and often fail in low-data regimes\ndue to high variance and weak statistical signals. In contrast,\nFORESTLLM constructs a forest F = {Tk}K\nk=1 of seman-\ntically grounded decision trees, where each split is selected\nvia LLM-guided reasoning that integrates both labeled and\nunlabeled data.\nAt each internal node, we generate a semantically en-\nriched prompt using natural language representations of lo-\ncal samples (via semantic data distillation) and structural\npriors (node position and contextual history). The LLM eval-\nuates optimal candidate splits: s∗= arg maxs∈S ΦL(s |\nDL, DU, c), where ΦL denotes a plausibility score based\non causal reasoning and c encodes node-local features.\nAt the leaf level, label prediction is performed via in-\ncontext prompting: each decision path rℓis translated\ninto a rule ΠNL(rℓ) and a set of few-shot exemplars\nDfew\nL\nis retrieved. The LLM then produces a static la-\nbel: ˆyℓ= arg maxy∈Y PL(y | ΠNL(rℓ), Dfew\nL ). At infer-\nence time, predictions from each tree are aggregated via\nmajority voting (classification) or averaging (regression):\nˆy(x′) = AGG({Tk(x′)}K\nk=1). By incorporating unlabeled\ndata and structural context during tree construction, and\noffloading label inference to a rule-driven reasoning step,\nFORESTLLM frames the LLM as a few-shot structure opti-\nmizer and label reasoner, used only during training.\nSemi-Supervised Semantic Tree Induction\nWe propose semantic-prioritized splitting, a novel decision\ntree induction framework that leverages LLMs as a symbolic\nreasoner to guide recursive partitioning. Unlike classical ap-\nproaches that rely solely on label-based heuristics (e.g., in-\nformation gain), our method incorporates both labeled and\nunlabeled data, making it particularly suited for few-shot\nregimes. The LLM uses sparse labeled examples for super-\nvision and dense unlabeled instances to infer semantically\ncoherent splits aligned with the underlying data manifold.\nAt each node n, given data Dn = DL,n ∪DU,n, we per-\nform semantic data distillation to construct a prompt for the\nLLM. Labeled portion DL,n is converted into textual ex-\nemplars via row-wise serialization, while unlabeled portion\nDU,n are summarized into a compact representation of the\nmarginal feature distribution P(X) (the Feature Summary).\nThese components are merged with structural metadata, in-\ncluding the node’s depth and its contextual path from the\nroot, to form the final prompt Πprompt = Ψ(DL,n, DU,n) ⊕\nΓ(pn). Here, Ψ is the distillation function, and Γ(pn) en-\ncodes the node characteristics that provide (1) local depth\nawareness, which calibrates split complexity, and (2) path\nhistory awareness, which delineates the submanifold being\npartitioned.\nThe LLM processes this prompt in two steps: (1) Causal\nfeature probing, which hypothesizes relationships of the\nform Xi →Y , and (2) Evidential split synthesis which gen-\nerates candidate split rules s ∈Sn. The final split is selected\nby maximizing a composite score:\ns∗\nn = arg max\ns∈Sn\n\nϕL\nCP (s | Πprompt)\n|\n{z\n}\nCausal Plausibility\n· ϕL\nES(s | Πprompt)\n|\n{z\n}\nEvidential Support\n\n(1)\nHere, ϕL\nCP measures semantic coherence with respect to a\nhypothesized causal relation, and ϕL\nES reflects compatibility\n"}, {"page": 4, "text": "with evidence in the prompt.\nTheoretical Analysis.\nClassical splitting criteria, such as\nimpurity reduction ∆I(s), rely exclusively on labeled data\nDL,n:\n∆I(s) = I(DL,n) −\nX\nk∈{L,R}\n|D(k)\nL,n|\n|DL,n|I(D(k)\nL,n)\n(2)\nBecause ∆I(s) is computed solely from the empirical dis-\ntribution ˆP(Y\n| X) over DL,n, unlabeled data DU,n is\ncompletely ignored: ∆I(s; DL,n ∪DU,n) = ∆I(s; DL,n).\nThis fundamental limitation prevents classical trees from ex-\nploiting structural information in DL,n. FORESTLLM ad-\ndresses this limitation by introducing a feature summary\nas a proxy for the marginal distribution P(X), estimated\nfrom ˆp(x | Dn). This enables the LLM to reason over the\ndata geometry and apply the cluster assumption, favoring\nsplits Bs that occur in low-density regions, i.e., minimiz-\ning\nR\nx∈Bs ˆp(x | Dn) dx. By aligning splits with the intrin-\nsic structure of the data, FORESTLLM regularizes tree con-\nstruction, mitigates overfitting to sparse labels, and improves\ngeneralization in few-shot regimes.\nIn-Context Leaf Label Inference\nOnce a decision tree is constructed, assigning a label to a test\ninstance reaching a leaf node ℓrequires more than conven-\ntional majority voting, which is often unreliable in few-shot\nor low-resource regimes. We propose in-context leaf label\ninference, a framework that reuses a pretrained LLM as a\nstructured reasoning engine. Instead of relying on empirical\naggregation over sparse leaf data, we prompt the LLM with\na symbolic description of the decision path and a small set\nof semantically relevant exemplars, enabling context-aware\nprediction beyond the training distribution.\nFor each leaf ℓ, we extract its decision path rℓfrom the\nroot and translate it into a natural language rule Rℓ=\nΠNL(rℓ) via Rule Induction function. This rule captures\nthe logic of the path as a structured, interpretable condi-\ntion. To ground the inference, we retrieve a support set\nDLfew ⊂DL using a semantic retrieval function σ, which\nselects labeled examples most aligned with the semantics of\nrℓ: DLfew = σ(DL, rℓ). We then compose a prompt using\na template ΠICL(Rℓ, Dfew\nL ), which is passed to the LLM to\ngenerate a label via analogical reasoning and rule-based ab-\nstraction. We formalize this as probabilistic inference over a\nlatent space of reasoning traces Z, where each z ∈Z repre-\nsents a candidate thought process induced by the LLM:\nˆyℓ= arg max\ny∈Y\nZ\nZ\nPL(y | z, Rℓ) PL(z | promptℓ) dz\n(3)\nHere, PL(z | promptℓ) captures the distribution over latent\nreasoning chains, and PL(y | z, Rℓ) scores the plausibility\nof a predicted label given the rule and reasoning trace.\nThis formulation casts the LLM as a soft neuro-symbolic\nreasoner, integrating logical structure with contextual evi-\ndence to yield data-efficient predictions. By abstracting the\ndecision path and grounding it in retrieved examples, our ap-\nproach addresses key limitations of prior tree-based methods\nand opens new directions for LLM-driven inference in struc-\ntured prediction.\nOverall Objective and Forest-Based Inference\nFORESTLLM defines a semantically grounded random for-\nest F = {Tk}K\nk=1, where each tree Tk is constructed via\nrecursive application of the language-informed splitting ob-\njective (Eq. 1). To promote diversity, each tree is trained on\na bootstrap sample D(k) ⊂DL ∪DU, and splits are selected\nfrom a random subset of features. This design preserves the\nstatistical benefits of bagging while aligning tree structure\nwith semantic priors induced by the LLM.\nInference on a test input x′ proceeds in two stages. First,\neach tree Tk routes x′ to a leaf node ℓk, where a prediction\nˆyk is produced via in-context leaf label inference, incorpo-\nrating both the decision rule and retrieved exemplars for con-\ntextual reasoning. Second, the predictions {ˆy1, . . . , ˆyK} are\naggregated into the final output ˆy(x′) using a task-specific\nrule: majority vote for classification or arithmetic mean for\nregression. By integrating semantic tree construction with\noffline LLM-guided label reasoning, FORESTLLM extends\nclassical forests to the few-shot regime, achieving robust,\ndata-efficient prediction without requiring inference-time\naccess to the LLM.\nExperiments\nDatasets. We evaluate FORESTLLM on a diverse suite of\ntabular datasets spanning both classification and regression\ntasks. For binary classification, we include Adult (Asuncion,\nNewman et al. 2007), Bank (Yeh, Yang, and Ting 2009),\nBlood (Moro, Cortez, and Rita 2014), Credit-g (Kadra et al.\n2021), Cultivars (de Oliveira et al. 2023), and NHANES1.\nFor multi-class classification, we use Car (Kadra et al.\n2021), CDC Diabetes2, Heart3, Communities (Redmond\n2009), and Myocardial (Golovenkin et al. 2020). For\nregression, we evaluate on: Cultivars, Cpu small4, Di-\namonds5, Plasma retinol6, Forest-fires7, Housing8, Insur-\nance9, Bike (Fanaee-T 2013), and Wine (Cortez et al. 2009).\nTo reduce the risk of data contamination from the\nLLM’s pretraining corpus (Bordt et al. 2024), we addi-\ntionally include two datasets that were publicly released\nafter the GPT-4o model’s training cutoff (October 2023):\nGallstone (Esen et al. 2024) for classification and In-\nfrared Thermography Temperature (Wang et al. 2023) for\nregression. Both datasets are sourced from the UCI Machine\nLearning Repository and serve to evaluate post-training gen-\neralization on data that was inaccessible during LLM pre-\ntraining.\n1archive.ics.uci.edu/dataset/887\n2kaggle.com/datasets/alexteboul/diabetes-health-indicators-\ndataset\n3kaggle.com/datasets/fedesoriano/heart-failure-prediction\n4openml.org/search?type=data&id=562\n5openml.org/search?type=data&id=42225\n6openml.org/search?type=data&id=511\n7openml.org/search?type=data&id=42363\n8openml.org/search?type=data&id=43996\n9kaggle.com/datasets/teertha/ushealthinsurancedataset\n"}, {"page": 5, "text": "Table 2: Classification performance (AUC) comparison of various models on 13 datasets under different few-shot settings.\nData\nShot\nCART\nRF\nXGBoost\nLogReg\nSCARF\nSTUNT\nTabPFN\nTP-BERTa\nTabLLM\nTABLET\nLIFT\nFeatLLM\nP2T\nOurs\nadult\n4\n0.600\n0.629\n0.500\n0.539\n0.595\n0.503\n0.691\n0.582\n0.800\n0.807\n0.710\n0.870\n0.765\n0.833\nadult\n8\n0.618\n0.666\n0.596\n0.603\n0.658\n0.532\n0.733\n0.610\n0.795\n0.826\n0.694\n0.880\n0.779\n0.827\n16\n0.639\n0.725\n0.685\n0.667\n0.658\n0.541\n0.746\n0.658\n0.821\n0.819\n0.667\n0.877\n0.777\n0.823\nblood\n4\n0.493\n0.500\n0.500\n0.523\n0.502\n0.562\n0.563\n0.446\n0.561\n0.626\n0.511\n0.530\n0.568\n0.691\n8\n0.542\n0.575\n0.567\n0.564\n0.617\n0.553\n0.604\n0.473\n0.578\n0.648\n0.551\n0.598\n0.618\n0.698\n16\n0.602\n0.635\n0.601\n0.673\n0.593\n0.551\n0.636\n0.497\n0.605\n0.655\n0.523\n0.621\n0.664\n0.709\nbank\n4\n0.535\n0.613\n0.500\n0.585\n0.556\n0.531\n0.602\n0.401\n0.609\n0.829\n0.616\n0.723\n0.683\n0.837\nbank\n8\n0.608\n0.699\n0.587\n0.676\n0.572\n0.544\n0.706\n0.408\n0.638\n0.830\n0.628\n0.742\n0.723\n0.835\n16\n0.642\n0.730\n0.675\n0.738\n0.584\n0.559\n0.773\n0.447\n0.661\n0.836\n0.616\n0.752\n0.740\n0.843\ncar\n4\n0.562\n0.554\n0.500\n0.580\n0.571\n0.564\n0.612\n0.585\n0.558\n0.840\n0.799\n0.703\n0.656\n0.864\n8\n0.581\n0.643\n0.620\n0.622\n0.622\n0.609\n0.673\n0.660\n0.617\n0.918\n0.779\n0.702\n0.704\n0.879\n16\n0.669\n0.736\n0.679\n0.642\n0.673\n0.652\n0.785\n0.724\n0.628\n0.845\n0.804\n0.744\n0.760\n0.881\ncommunities\n4\n0.568\n0.606\n0.500\n0.559\n0.654\n0.560\n0.687\n0.469\nN/A\n0.799\n0.686\n0.633\n0.586\n0.784\ncommunities\n8\n0.603\n0.718\n0.664\n0.586\n0.726\n0.612\n0.743\n0.519\nN/A\n0.798\n0.725\n0.691\n0.594\n0.782\n16\n0.623\n0.738\n0.689\n0.639\n0.745\n0.646\n0.769\n0.556\nN/A\n0.794\n0.725\n0.699\n0.675\n0.789\ncredit-g\n4\n0.515\n0.526\n0.500\n0.521\n0.539\n0.508\n0.532\n0.467\n0.580\n0.623\n0.506\n0.498\n0.538\n0.669\n8\n0.543\n0.571\n0.546\n0.600\n0.548\n0.506\n0.584\n0.469\n0.624\n0.627\n0.499\n0.531\n0.588\n0.687\n16\n0.572\n0.599\n0.561\n0.582\n0.558\n0.527\n0.645\n0.477\n0.658\n0.632\n0.513\n0.548\n0.661\n0.693\ncdc diabetes\n4\n0.539\n0.526\n0.500\n0.565\n0.604\n0.557\n0.605\n0.541\n0.624\n0.702\n0.655\n0.604\n0.570\n0.700\ncdc diabetes\n8\n0.551\n0.605\n0.578\n0.559\n0.622\n0.581\n0.613\n0.578\n0.638\n0.703\n0.656\n0.590\n0.609\n0.699\n16\n0.562\n0.611\n0.600\n0.590\n0.600\n0.572\n0.630\n0.602\n0.620\n0.701\n0.649\n0.633\n0.599\n0.705\nheart\n4\n0.595\n0.711\n0.500\n0.510\n0.816\n0.611\n0.767\n0.497\n0.734\n0.858\n0.652\n0.870\n0.602\n0.887\n8\n0.639\n0.721\n0.594\n0.659\n0.867\n0.594\n0.828\n0.521\n0.809\n0.858\n0.563\n0.880\n0.763\n0.896\n16\n0.695\n0.840\n0.804\n0.742\n0.891\n0.632\n0.880\n0.630\n0.848\n0.857\n0.570\n0.877\n0.763\n0.898\nmyocardial\n4\n0.513\n0.519\n0.500\n0.536\n0.551\n0.527\n0.522\n0.516\nN/A\n0.609\n0.503\n0.568\n0.580\n0.653\nmyocardial\n8\n0.512\n0.537\n0.543\n0.521\n0.513\n0.528\n0.573\n0.538\nN/A\n0.616\n0.519\n0.549\n0.583\n0.660\n16\n0.547\n0.592\n0.569\n0.537\n0.519\n0.538\n0.608\n0.587\nN/A\n0.632\n0.570\n0.577\n0.576\n0.687\nbreast-w\n4\n0.836\n0.960\n0.500\n0.783\n0.985\n0.883\n0.986\n0.923\n0.985\n0.974\n0.759\n0.986\n0.979\n0.992\n8\n0.856\n0.974\n0.832\n0.887\n0.983\n0.927\n0.985\n0.926\n0.983\n0.978\n0.768\n0.987\n0.981\n0.993\n16\n0.851\n0.970\n0.876\n0.962\n0.985\n0.921\n0.984\n0.934\n0.979\n0.978\n0.912\n0.988\n0.985\n0.993\ncultivars\n4\n0.514\n0.493\n0.500\n0.504\n0.494\n0.514\n0.499\n0.445\n0.524\n0.525\n0.531\n0.528\n0.536\n0.661\ncultivars\n8\n0.486\n0.497\n0.503\n0.528\n0.519\n0.498\n0.541\n0.495\n0.554\n0.546\n0.522\n0.561\n0.543\n0.653\n16\n0.495\n0.523\n0.510\n0.530\n0.527\n0.519\n0.511\n0.491\n0.584\n0.531\n0.563\n0.582\n0.576\n0.679\nNHANES\n4\n0.662\n0.805\n0.500\n0.870\n0.773\n0.512\n0.898\n0.457\n0.975\n0.525\n0.531\n0.527\n0.673\n0.987\n8\n0.962\n0.900\n0.880\n0.968\n0.812\n0.518\n0.976\n0.519\n0.999\n0.546\n0.522\n0.750\n0.925\n0.998\n16\n0.967\n0.974\n0.956\n0.988\n0.844\n0.532\n0.999\n0.521\n0.999\n0.531\n0.563\n0.899\n0.969\n0.999\ngallstone\n4\n0.559\n0.564\n0.500\n0.493\n0.505\n0.509\n0.570\n0.440\n0.465\n0.565\n0.533\n0.540\n0.471\n0.658\ngallstone\n8\n0.555\n0.571\n0.508\n0.532\n0.522\n0.505\n0.597\n0.432\n0.473\n0.558\n0.543\n0.532\n0.436\n0.653\n16\n0.592\n0.591\n0.589\n0.596\n0.537\n0.528\n0.612\n0.426\n0.461\n0.563\n0.529\n0.557\n0.481\n0.662\nAverage\n4\n0.576\n0.616\n0.500\n0.582\n0.627\n0.565\n0.656\n0.521\n0.674\n0.714\n0.615\n0.660\n0.631\n0.786\n8\n0.620\n0.667\n0.617\n0.639\n0.660\n0.577\n0.704\n0.550\n0.701\n0.727\n0.613\n0.692\n0.680\n0.789\n16\n0.650\n0.713\n0.676\n0.684\n0.670\n0.594\n0.737\n0.581\n0.715\n0.721\n0.631\n0.720\n0.710\n0.797\nOur benchmark suite covers a broad range of applica-\ntion domains, including healthcare, finance, and food, and\nexhibits significant diversity in both feature dimensionality\nand dataset scale. For example, Myocardial includes up to\n111 features, while the largest dataset contains over 253,680\nsamples. Full dataset statistics are provided in Appendix A.\nBaselines and Implementation Details. To evaluate the ef-\nfectiveness of our FORESTLLM, we compare against a com-\nprehensive set of baselines spanning both LLM-based and\nconventional non-LLM methods.\nAmong\nLLM-based\napproaches,\nwe\nevaludate\nFeatLLM (Han et al. 2024), LIFT (Dinh et al. 2022),\nTABLET (Slack and Singh 2023), TabLLM (Hegselmann\net al. 2023), TP-BERTa (Yan et al. 2024), and P2T (Nam\net al. 2024). FORESTLLM, FeatLLM, LIFT, and TABLET\nuse GPT-4o (gpt-4o-2024-11-20) as the base LLM, accessed\nvia Azure. To eliminate output stochasticity, all LLM-based\nmodels are evaluated with temperature fixed to 0. Due to\nthe high cost of fine-tuning and the small number of labeled\nexamples, we use the in-context variant of LIFT only. For\nTabLLM and TP-BERTa, we adopt the model configurations\nand training protocols reported in their original papers,\nusing T0 and RoBERTa as respective backbones.\nFor traditional baselines, we include CART (Loh 2011),\nRandom Forest (RF)(Breiman 2001), XGBoost(Chen and\nGuestrin 2016), MLP (LeCun, Bengio, and Hinton 2015),\nElasticNet (Zou and Hastie 2005), and Logistic Regression\n(LogReg), along with specialized tabular learners such as\nTabPFN (Hollmann et al. 2025), STUNT (Nam et al. 2023b),\nand SCARF (Bahri et al. 2021). Notably, FeatLLM, STUNT,\nSCARF, and FORESTLLM leverage unlabeled data. For all\nsuch methods, unlabeled samples are consistently defined as\nthe full dataset excluding the few-shot labeled examples and\ntest set. All non-LLM baselines are tuned via grid search\nwith k-fold cross-validation. Additional implementation de-\ntails are provided in Appendix B.\nMain Results\nTables 2 and 3 present comprehensive performance compar-\nisons across a wide range of classification and regression\ntasks. For each dataset, the best-performing method is shown\nin bold, and the second-best is underlined. The bottom row\nsummarizes the average performance across all datasets, vi-\nsualized using a red–yellow–green gradient to indicate low,\nmoderate, and high performance, respectively.\nTo account for variance introduced by train/test split ran-\ndomness in few-shot scenarios, we repeat each experiment\n10 times using different random seeds (0–9) for data par-\ntitioning. We report the mean AUC (for classification) and\nNRMSE (for regression), with standard deviations provided\n"}, {"page": 6, "text": "2\n3\n4\n5\n0.73\n0.74\n0.75\n0.76\nAUC\nn_estimators=5\n2\n3\n4\n5\n0.73\n0.74\n0.75\n0.76\n0.77\nn_estimators=7\n2\n3\n4\n5\n0.73\n0.74\n0.75\n0.76\n0.77\nn_estimators=9\n2\n3\n4\n5\n0.72\n0.73\n0.74\n0.75\n0.76\n0.77\nn_estimators=11\n2\n3\n4\n5\n0.140\n0.145\n0.150\n0.155\n0.160\nNRMSE\n2\n3\n4\n5\n0.140\n0.145\n0.150\n0.155\n0.160\n2\n3\n4\n5\n0.145\n0.150\n0.155\n0.160\n2\n3\n4\n5\n0.145\n0.150\n0.155\n0.160\n0.165\n4-shot\n8-shot\n16-shot\n32-shot\n48-shot\nFigure 2: Hyperparameter sensitivity analysis of FORESTLLM with respect to tree depth and number of estimators under\nvarying few-shot settings. The top row shows classification results, and the bottom row shows regression results.\n4\n8\n16\n32\n48\n0.60\n0.65\n0.70\n0.75\n0.80\nAUC\n(a)\n4\n8\n16\n32\n48\n0.14\n0.15\n0.16\n0.17\n0.18\n0.19\nNRMSE\n(b)\nRandom Forest\nForestLLM w/o unlabeled data\nForestLLM\nFigure 3: Ablation study of the impact of unlabeled data. (a)\nClassification performance measured by AUC. (b) Regres-\nsion performance measured by NRMSE.\nin Appendix E.\nThe results show that FORESTLLM consistently achieves\neither the best or second-best performance across both clas-\nsification and regression tasks. This superiority persists\neven in higher-shot settings (32- and 48-shot). Notably,\nFORESTLLM also performs strongly on the Gallstone and\nInfrared Thermography Temperature datasets—both pub-\nlished after the LLM’s pretraining cutoff—demonstrating\nrobust generalization to out-of-distribution and recently in-\ntroduced data.\nWe further observe that LLM-based methods tend to out-\nperform traditional machine learning approaches on clas-\nsification tasks, while traditional methods often perform\nbetter on regression. This pattern is consistent with prior\nwork highlighting the strength of LLMs in classification\ntasks (Kostina et al. 2025), and their limitations in numerical\nreasoning and precise estimation (Drinkall, Pierrehumbert,\nand Zohren 2024; Siddiqui et al. 2025). FORESTLLM mit-\nigates this limitation through a structured prediction mech-\nanism based on random forest-style bagging and ensemble\naggregation, rather than relying solely on raw LLM outputs.\nThis design reduces uncertainty and mitigates errors intro-\nduced by the probabilistic nature of LLMs in numerical pre-\ndiction tasks.\nAblations\nUnlabeled Data Helps. We perform an ablation study to as-\nsess the contribution of unlabeled data to FORESTLLM’s\nperformance.\nAs\nshown\nin\nFigure\n3,\nwe\ncompare\nFORESTLLM against (i) a variant trained using labeled\ndata only, and (ii) a traditional Random Forest, under both\nclassification and regression tasks. To ensure fair compar-\nison, we adapt tree depth to the number of labeled ex-\namples. Specifically, for 4-, 8-, and 16-shot settings, we\nuse max depth=3 and n estimators=9; for 32- and\n48-shot settings, we increase the depth to max depth=5,\nkeeping n estimators=9. This adjustment accounts for\nthe fact that, under extreme data scarcity, the limited number\nof labeled examples constrains tree growth depth, and shal-\nlow structures also help mitigate potential overfitting risks.\nIn contrast, FORESTLLM is capable of leveraging unlabeled\ndata to construct deeper, semantically guided trees even in\nlow-shot regimes, rendering it less sensitive to the number\nof labeled instances. The results show that FORESTLLM\nconsistently outperforms both baselines across all settings,\nconfirming the effectiveness of utilizing unlabeled data for\nimproved generalization.\nHyperparameters. We also evaluate the robustness of\nFORESTLLM with respect to its key hyperparameters.\nAs shown in Figure 2, we conduct a grid search over\nmax depth and n estimators, reporting both AUC and\nNRMSE across 4-, 8-, 16-, 32-, and 48-shot settings. We\nvary max depth from 2 to 5 and sweep n estimators\nover 5, 7, 9, 11. Results indicate that FORESTLLM main-\ntains strong performance across a wide range of configu-\nrations. While deeper trees and larger ensembles can offer\nmarginal gains, the model is notably stable, especially in\n"}, {"page": 7, "text": "Table 3: Regression performance (NRMSE) comparison of various models on 10 datasets under different few-shot settings. *\nrefers to infrared thermography temperature.\nData\nShot\nCART\nRF\nXGBoost\nElasticNet\nMLP\nTabPFN\nTP-BERTa\nLIFT\nP2T\nOurs\nbike\n4\n0.220\n0.194\n0.224\n0.266\n0.225\n0.192\n0.264\n0.161\n0.190\n0.156\nbike\n8\n0.300\n0.197\n0.243\n0.211\n0.247\n0.191\n0.262\n0.259\n0.185\n0.149\n16\n0.304\n0.200\n0.193\n0.202\n0.256\n0.165\n0.261\n0.156\n0.194\n0.144\ncpu small\n4\n0.168\n0.180\n0.198\n0.322\n49.042\n0.167\n0.789\n0.173\n0.234\n0.161\n8\n0.172\n0.167\n0.187\n0.389\n9.865\n0.164\n0.770\n0.172\n0.167\n0.154\n16\n0.156\n0.122\n0.180\n0.335\n0.987\n0.118\n0.745\n0.101\n0.147\n0.114\ndiamonds\n4\n0.152\n0.152\n0.156\n0.142\n0.153\n0.157\n0.302\n0.080\n0.116\n0.090\ndiamonds\n8\n0.150\n0.118\n0.129\n0.169\n0.116\n0.153\n0.302\n0.080\n0.109\n0.095\n16\n0.126\n0.120\n0.134\n0.123\n0.115\n0.119\n0.302\n0.082\n0.109\n0.095\nforest-fires\n4\n0.127\n0.127\n0.127\n0.128\n0.130\n0.127\n0.128\n0.127\n0.127\n0.126\n8\n0.204\n0.143\n0.140\n0.170\n0.128\n0.170\n0.127\n0.153\n0.136\n0.126\n16\n0.214\n0.146\n0.144\n0.147\n0.133\n0.136\n0.126\n0.158\n0.143\n0.123\nhouses\n4\n0.252\n0.180\n0.156\n0.393\n6.056\n0.176\n1.270\n0.214\n0.358\n0.150\nhouses\n8\n0.181\n0.170\n0.161\n0.286\n6.218\n0.142\n0.801\n0.169\n0.192\n0.131\n16\n0.175\n0.150\n0.156\n0.183\n2.840\n0.131\n0.285\n0.148\n0.163\n0.128\ninsurance\n4\n0.282\n0.236\n0.236\n0.240\n0.235\n0.194\n0.325\n0.125\n0.186\n0.115\n8\n0.295\n0.255\n0.245\n0.224\n0.207\n0.163\n0.325\n0.116\n0.145\n0.107\n16\n0.230\n0.232\n0.234\n0.185\n0.193\n0.158\n0.325\n0.116\n0.119\n0.105\nplasma retinol\n4\n0.292\n0.239\n0.215\n0.490\n0.396\n0.227\n0.633\n0.331\n0.412\n0.216\nretinol\n8\n0.304\n0.240\n0.234\n0.388\n0.472\n0.242\n0.631\n0.340\n0.316\n0.203\n16\n0.296\n0.217\n0.220\n0.301\n0.445\n0.203\n0.628\n0.340\n0.268\n0.192\nwine\n4\n0.209\n0.170\n0.169\n0.206\n0.361\n0.166\n0.150\n0.164\n0.252\n0.141\n8\n0.188\n0.167\n0.161\n0.193\n0.228\n0.153\n0.146\n0.158\n0.183\n0.137\n16\n0.187\n0.156\n0.158\n0.178\n0.207\n0.155\n0.148\n0.158\n0.161\n0.132\ncultivars\n4\n0.268\n0.234\n0.218\n0.325\n0.314\n0.300\n1.403\n0.370\n0.328\n0.206\ncultivars\n8\n0.263\n0.235\n0.224\n0.369\n0.330\n0.221\n1.402\n0.321\n0.288\n0.196\n16\n0.277\n0.234\n0.231\n0.224\n0.369\n0.217\n1.400\n0.270\n0.266\n0.198\ninfrared t t*\n4\n0.130\n0.116\n0.151\n0.152\n0.288\n0.138\n6.628\n0.129\n1.637\n0.108\n8\n0.129\n0.109\n0.160\n0.132\n0.138\n0.110\n5.898\n0.105\n0.722\n0.102\n16\n0.117\n0.099\n0.109\n0.120\n0.140\n0.112\n5.036\n0.098\n0.106\n0.103\nAverage\n4\n0.210\n0.183\n0.185\n0.266\n5.720\n0.184\n1.189\n0.187\n0.384\n0.147\n8\n0.219\n0.180\n0.188\n0.253\n1.795\n0.171\n1.066\n0.187\n0.244\n0.140\n16\n0.208\n0.168\n0.176\n0.200\n0.569\n0.151\n0.926\n0.163\n0.168\n0.133\nhigher-shot settings (e.g., 32- and 48-shot). This robustness\nsuggests that FORESTLLM performs reliably without exten-\nsive hyperparameter tuning, making it a practical choice for\nreal-world few-shot tabular learning.\n0\n5\n10\n15\n20\n25\n30\nFrequency (%)\nRank 1\nRank 2\nRank 3\nRank 4\nRank 5\nHepatic fat \naccumulation\nBody mass index\nTriglycerides\nTotal cholesterol\nObesity rate\nC-reactive protein\nVitamin D\nAlkaline phosphatase\nAspartate aminotransferase\nExtracellular fluid / total body fluid ratio\nForestLLM\nRandom Forest\nFigure 4: Comparison of the top 5 most frequently split fea-\ntures on the Gallstone dataset.\nSemantic Reasoning vs. Statistical Splitting. To as-\nsess interpretability in feature selection, we compared\nFORESTLLM (under a 4-shot setup) with a fully supervised\nRandom Forest on the Gallstone dataset. Feature importance\nwas quantified via split frequency, revealing distinct selec-\ntion patterns between the two models. FORESTLLM pri-\noritized upstream, causally relevant factors, such as hepatic\nfat, BMI, and cholesterol, consistent with known metabolic\npathways implicated in gallstone formation (Portincasa et al.\n2023; Wu et al. 2024; Tang et al. 2025; Deng et al. 2025). In\ncontrast, Random Forest emphasized downstream biomark-\ners including CRP, ALP, and AST, which are more reflective\nof inflammation and hepatic injury (Zhou et al. 2021; Esen\net al. 2024; Van Nynatten et al. 2025).\nThis divergence illustrates two fundamentally different\nparadigms: FORESTLLM leverages semantic priors and\ncausal abstractions derived from the LLM to surface clini-\ncally meaningful features, even in sparse data regimes. Ran-\ndom Forest, by contrast, relies on purely statistical associa-\ntions, which often highlight features correlated with disease\nseverity rather than etiology (Esen et al. 2024; Van Nynatten\net al. 2025). These results suggest that FORESTLLM pro-\nvides a causally grounded and interpretable alternative to\nconventional correlation-driven models, particularly in few-\nshot settings. Prior studies have highlighted the value of\ncausal feature selection for improving interpretability and\nclinical insight (Chen, Zhang, and Qin 2022; Malec et al.\n2023).\nConclusion\nIn this paper, we introduced FORESTLLM, a novel frame-\nwork that repositions LLMs as offline model designers,\nbridging symbolic reasoning with structured inductive bias\nfor few-shot tabular learning. By harnessing unlabeled data\nand LLM-driven semantic evaluations for both split selec-\ntion and leaf-level label synthesis, FORESTLLM constructs\ndecision trees that retain the efficiency, interpretability, and\ninference-time frugality of classical forests, while achiev-\ning strong generalization in low-data regimes. Extensive ex-\nperiments demonstrate that LLMs, when deployed as one-\ntime architectural guides rather than inference-time oracles,\ncan distill transferable knowledge into lightweight, symbolic\n"}, {"page": 8, "text": "models. Our work opens a new direction for leveraging foun-\ndation models as design-time optimizers for neuro-symbolic\nlearners.\n"}, {"page": 9, "text": "References\nArun, K.; Ishan, G.; and Sanmeet, K. 2016. Loan approval\nprediction based on machine learning approach.\nIOSR J.\nComput. Eng, 18(3): 18–21.\nAssefa, S. A.; Dervovic, D.; Mahfouz, M.; Tillman, R. E.;\nReddy, P.; and Veloso, M. 2020. Generating synthetic data in\nfinance: opportunities, challenges and pitfalls. Proceedings\nof the First ACM International Conference on AI in Finance,\n1–8.\nAsuncion, A.; Newman, D.; et al. 2007. UCI machine learn-\ning repository.\nBahri, D.; Jiang, H.; Tay, Y.; and Metzler, D. 2021. Scarf:\nSelf-supervised contrastive learning using random feature\ncorruption. arXiv preprint arXiv:2106.15147.\nBahri, D.; Jiang, H.; Tay, Y.; and Metzler, D. 2022. Scarf:\nSelf-Supervised Contrastive Learning using Random Fea-\nture Corruption. In International Conference on Learning\nRepresentations.\nBischl, B.; Casalicchio, G.; Feurer, M.; Hutter, F.; Lang,\nM.; Mantovani, R. G.; van Rijn, J. N.; and Vanschoren, J.\n2019. OpenML Benchmarking Suites. arXiv:1708.03731v2\n[stat.ML].\nBordt, S.; Nori, H.; Rodrigues, V.; Nushi, B.; and Caruana,\nR. 2024. Elephants never forget: Memorization and learn-\ning of tabular data in large language models. arXiv preprint\narXiv:2404.06209.\nBorisov, V.; Leemann, T.; Seßler, K.; Haug, J.; Pawelczyk,\nM.; and Kasneci, G. 2022. Deep neural networks and tabular\ndata: A survey. IEEE transactions on neural networks and\nlearning systems.\nBreiman, L. 2001. Random forests. Machine learning, 45:\n5–32.\nChen, T.; and Guestrin, C. 2016. Xgboost: A scalable tree\nboosting system. Proceedings of the 22nd acm sigkdd inter-\nnational conference on knowledge discovery and data min-\ning, 785–794.\nChen, T.; Sampath, V.; May, M. C.; Shan, S.; Jorg, O. J.;\nAguilar Mart´ın, J. J.; Stamer, F.; Fantoni, G.; Tosello, G.;\nand Calaon, M. 2023. Machine learning in manufacturing\ntowards industry 4.0: From ‘for now’to ‘four-know’. Ap-\nplied Sciences, 13(3): 1903.\nChen, W.-Y.; Liu, Y.-C.; Kira, Z.; Wang, Y.-C. F.; and\nHuang, J.-B. 2018. A Closer Look at Few-shot Classifica-\ntion. In International Conference on Learning Representa-\ntions.\nCortez, P.; Cerdeira, A.; Almeida, F.; Matos, T.; and Reis,\nJ. 2009. Modeling wine preferences by data mining from\nphysicochemical properties.\nDecision support systems,\n47(4): 547–553.\nde Oliveira, B. R.; Zuffo, A. M.; dos Santos Silva, F. C.;\nMezzomo, R.; Barrozo, L. M.; da Costa Zanatta, T. S.;\ndos Santos, J. C.; Sousa, C. H. C.; and Coelho, Y. P.\n2023. Dataset: Forty soybean cultivars from subsequent har-\nvests. Trends in Agricultural and Environmental Sciences,\ne230005–e230005.\nDeng, L.; Wang, S.; Wan, D.; Zhang, Q.; Shen, W.; Liu, X.;\nand Zhang, Y. 2025. Relative fat mass and physical indices\nas predictors of gallstone formation: insights from machine\nlearning and logistic regression.\nInternational Journal of\nGeneral Medicine, 509–527.\nDinh, T.; Zeng, Y.; Zhang, R.; Lin, Z.; Gira, M.; Rajput,\nS.; Sohn, J.-y.; Papailiopoulos, D.; and Lee, K. 2022. Lift:\nLanguage-interfaced fine-tuning for non-language machine\nlearning tasks. Advances in Neural Information Processing\nSystems, 35: 11763–11784.\nDrinkall, F.; Pierrehumbert, J. B.; and Zohren, S. 2024. Fore-\ncasting Credit Ratings: A Case Study where Traditional\nMethods Outperform Generative LLMs.\narXiv preprint\narXiv:2407.17624.\nEsen, ˙I.; Arslan, H.; Esen, S. A.; G¨uls¸en, M.; K¨ultekin, N.;\nand ¨Ozdemir, O. 2024. Early prediction of gallstone disease\nwith a machine learning-based method from bioimpedance\nand laboratory data. Medicine, 103(8): e37258.\nFanaee-T, H. 2013. Bike Sharing. UCI Machine Learning\nRepository. DOI: https://doi.org/10.24432/C5W894.\nFischer, S. F.; Feurer, M.; and Bischl, B. 2023. OpenML-\nCTR23–a curated tabular regression benchmarking suite. In\nAutoML Conference 2023 (Workshop).\nFrosch, D. L.; Grande, D.; Tarn, D. M.; and Kravitz, R. L.\n2010. A decade of controversy: balancing policy with ev-\nidence in the regulation of prescription drug advertising.\nAmerican Journal of Public Health, 100(1): 24–32.\nGolovenkin, S.; Shulman, V.; Rossiev, D.; Shesternya, P.;\nNikulina, S. Y.; Orlova, Y. V.; and Voino-Yasenetsky, V.\n2020. Myocardial infarction complications. UCI Machine\nLearning Repository. UCI Machine Learning Repository.\nGorishniy, Y.; Rubachev, I.; Khrulkov, V.; and Babenko, A.\n2021. Revisiting Deep Learning Models for Tabular Data.\nIn NeurIPS.\nGrinsztajn, L.; Oyallon, E.; and Varoquaux, G. 2022. Why\ndo tree-based models still outperform deep learning on typi-\ncal tabular data? In Thirty-sixth Conference on Neural Infor-\nmation Processing Systems Datasets and Benchmarks Track.\nHan, S.; Yoon, J.; Arik, S. O.; and Pfister, T. 2024. Large\nLanguage Models Can Automatically Engineer Features for\nFew-Shot Tabular Learning. In International Conference on\nMachine Learning, 17454–17479. PMLR.\nHegselmann, S.; Buendia, A.; Lang, H.; Agrawal, M.; Jiang,\nX.; and Sontag, D. 2023. Tabllm: Few-shot classification of\ntabular data with large language models. In International\nConference on Artificial Intelligence and Statistics, 5549–\n5581. PMLR.\nHernandez, M.; Epelde, G.; Alberdi, A.; Cilla, R.; and\nRankin, D. 2022.\nSynthetic data generation for tabular\nhealth records: A systematic review. Neurocomputing, 493:\n28–45.\nHollmann, N.; M¨uller, S.; Purucker, L.; Krishnakumar, A.;\nK¨orfer, M.; Hoo, S. B.; Schirrmeister, R. T.; and Hutter, F.\n2025.\nAccurate predictions on small data with a tabular\nfoundation model. Nature.\n"}, {"page": 10, "text": "Hu, E. J.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang,\nL.; Chen, W.; et al. 2021. LoRA: Low-Rank Adaptation of\nLarge Language Models.\nIn International Conference on\nLearning Representations.\nJohnson, A. E. W.; Pollard, T. J.; Shen, L.; wei H. Lehman,\nL.; Feng, M.; Ghassemi, M. M.; Moody, B.; Szolovits, P.;\nCeli, L. A.; and Mark, R. G. 2016.\nMIMIC-III, a freely\naccessible critical care database. Scientific Data, 3.\nKadra, A.; Lindauer, M.; Hutter, F.; and Grabocka, J. 2021.\nWell-tuned Simple Nets Excel on Tabular Datasets.\nIn\nThirty-Fifth Conference on Neural Information Processing\nSystems.\nKe, G.; Meng, Q.; Finley, T.; Wang, T.; Chen, W.; Ma, W.;\nYe, Q.; and Liu, T.-Y. 2017. Lightgbm: A highly efficient\ngradient boosting decision tree. Advances in neural infor-\nmation processing systems, 30.\nKostina, A.; Dikaiakos, M. D.; Stefanidis, D.; and Pal-\nlis, G. 2025.\nLarge language models for text classifica-\ntion: Case study and comprehensive review. arXiv preprint\narXiv:2501.08457.\nLeCun, Y.; Bengio, Y.; and Hinton, G. 2015. Deep learning.\nnature, 521(7553): 436–444.\nLoh, W.-Y. 2011. Classification and regression trees. Wiley\ninterdisciplinary reviews: data mining and knowledge dis-\ncovery, 1(1): 14–23.\nMajumder, S.; Chen, C.; Al-Halah, Z.; and Grauman, K.\n2022. Few-shot audio-visual learning of environment acous-\ntics. Advances in Neural Information Processing Systems,\n35: 2522–2536.\nManikandan, H.; Jiang, Y.; and Kolter, J. Z. 2023. Language\nModels are Weak Learners. In Thirty-seventh Conference on\nNeural Information Processing Systems.\nMcElfresh, D.; Khandagale, S.; Valverde, J.; Ramakrishnan,\nG.; Prasad, V.; Goldblum, M.; and White, C. 2023. When Do\nNeural Nets Outperform Boosted Trees on Tabular Data? In\nAdvances in Neural Information Processing Systems.\nMoro, S.; Cortez, P.; and Rita, P. 2014. A data-driven ap-\nproach to predict the success of bank telemarketing. Deci-\nsion Support Systems, 62: 22–31.\nNam, J.; Song, W.; Park, S. H.; Tack, J.; Yun, S.; Kim, J.;\nOh, K. H.; and Shin, J. 2024. Tabular Transfer Learning via\nPrompting LLMs. In First Conference on Language Model-\ning.\nNam, J.; Song, W.; Park, S. H.; Tack, J.; Yun, S.; Kim,\nJ.; and Shin, J. 2023a.\nSemi-supervised Tabular Classifi-\ncation via In-context Learning of Large Language Models.\nIn Workshop on Efficient Systems for Foundation Models@\nICML2023.\nNam, J.; Tack, J.; Lee, K.; Lee, H.; and Shin, J. 2023b.\nSTUNT: Few-shot Tabular Learning with Self-generated\nTasks from Unlabeled Tables. In The Eleventh International\nConference on Learning Representations.\nPortincasa, P.; Di Ciaula, A.; Bonfrate, L.; Stella, A.; Gar-\nruti, G.; and Lamont, J. T. 2023. Metabolic dysfunction-\nassociated gallstone disease: expecting more from critical\ncare manifestations.\nInternal and emergency medicine,\n18(7): 1897–1918.\nProkhorenkova, L.; Gusev, G.; Vorobev, A.; Dorogush,\nA. V.; and Gulin, A. 2018.\nCatBoost: unbiased boosting\nwith categorical features. Advances in neural information\nprocessing systems, 31.\nRedmond, M. 2009. Communities and crime. UCI machine\nlearning repository.\nSiddiqui, S. A.; Chen, Y.; Heo, J.; Xia, M.; and Weller, A.\n2025. On Evaluating LLMs’ Capabilities as Functional Ap-\nproximators: A Bayesian Evaluation Framework. In Ram-\nbow, O.; Wanner, L.; Apidianaki, M.; Al-Khalifa, H.; Eu-\ngenio, B. D.; and Schockaert, S., eds., Proceedings of the\n31st International Conference on Computational Linguis-\ntics, 5826–5835. Abu Dhabi, UAE: Association for Com-\nputational Linguistics.\nSlack, D.; and Singh, S. 2023. Tablet: Learning from instruc-\ntions for tabular data. arXiv preprint arXiv:2304.13188.\nSomepalli, G.; Goldblum, M.; Schwarzschild, A.; Bruss,\nC. B.; and Goldstein, T. 2021.\nSAINT: Improved Neu-\nral Networks for Tabular Data via Row Attention and Con-\ntrastive Pre-Training. arXiv preprint arXiv:2106.01342.\nTang, Y. F.; Su, Y. T.; Liang, L. J.; Feng, Y.; Huang, X. J.;\nXiang, X. L.; and Liang, Z. H. 2025. Association Between\nMetabolic Dysfunction and Gallstone Disease in US Adults:\nAn Analysis of the National Health and Nutrition Examina-\ntion Survey. Journal of Digestive Diseases.\nUcar, T.; Hajiramezanali, E.; and Edwards, L. 2021. Subtab:\nSubsetting features of tabular data for self-supervised rep-\nresentation learning. Advances in Neural Information Pro-\ncessing Systems, 34: 18853–18865.\nUlmer, D.; Meijerink, L.; and Cin`a, G. 2020.\nTrust is-\nsues: Uncertainty estimation does not enable reliable ood\ndetection on medical tabular data. In Machine Learning for\nHealth, 341–354. PMLR.\nVan Nynatten, L. R.; Patel, M. A.; Daley, M.; Miller, M. R.;\nCepinskas, G.; Slessarev, M.; Russell, J. A.; and Fraser,\nD. D. 2025.\nPutative biomarkers of hepatic dysfunction\nin critically ill sepsis patients. Clinical and Experimental\nMedicine, 25(1): 28.\nWang, Q.; Zhou, Y.; Ghassemi, P.; Chenna, D.; Chen, M.;\nCasamento, J.; Pfefer, J.; and Mcbride, D. 2023. Facial and\noral temperature data from a large set of human subject vol-\nunteers. PhysioNet, May.\nWang, Z.; and Sun, J. 2022. Transtab: Learning transfer-\nable tabular transformers across tables. Advances in Neural\nInformation Processing Systems, 35: 2902–2915.\nWen, X.; Zhang, H.; Zheng, S.; Xu, W.; and Bian, J. 2024.\nFrom supervised to generative: A novel paradigm for tabular\ndeep learning with large language models. In Proceedings\nof the 30th ACM SIGKDD Conference on Knowledge Dis-\ncovery and Data Mining, 3323–3333.\nWu, W.; Pei, Y.; Wang, J.; Liang, Q.; and Chen, W. 2024.\nAssociation between visceral lipid accumulation indicators\nand gallstones: a cross-sectional study based on NHANES\n2017–2020. Lipids in Health and Disease, 23(1): 345.\nYan, J.; Chen, J.; Hu, C.; Zheng, B.; Hu, Y.; Sun, J.; and Wu,\nJ. 2025. Small Models are LLM Knowledge Triggers for\n"}, {"page": 11, "text": "Medical Tabular Prediction. In The Thirteenth International\nConference on Learning Representations.\nYan, J.; Zheng, B.; Xu, H.; Zhu, Y.; Chen, D.; Sun, J.; Wu,\nJ.; and Chen, J. 2024. Making Pre-trained Language Models\nGreat on Tabular Prediction. In The Twelfth International\nConference on Learning Representations.\nYeh, I.-C.; Yang, K.-J.; and Ting, T.-M. 2009. Knowledge\ndiscovery on RFM model using Bernoulli sequence. Expert\nSystems with applications, 36(3): 5866–5871.\nYoon, J.; Zhang, Y.; Jordon, J.; and van der Schaar, M. 2020.\nVime: Extending the success of self-and semi-supervised\nlearning to tabular domain. Advances in Neural Information\nProcessing Systems, 33: 11033–11043.\nZab¨ergja, G.; Kadra, A.; and Grabocka, J. 2024. Is Deep\nLearning finally better than Decision Trees on Tabular Data.\narXiv preprint arXiv:2402.03970.\nZhang, X.; Cao, J.; Wei, J.; Xu, Y.; and You, C. 2025a.\nTokenization Constraints in LLMs: A Study of Sym-\nbolic and Arithmetic Reasoning Limits.\narXiv preprint\narXiv:2505.14178.\nZhang, X.; Cao, J.; Wei, J.; You, C.; and Ding, D. 2025b.\nWhy Prompt Design Matters and Works: A Complexity\nAnalysis of Prompt Search Space in LLMs. arXiv preprint\narXiv:2503.10084.\nZhou, J.-Y.; Song, L.-W.; Yuan, R.; Lu, X.-P.; and Wang,\nG.-Q. 2021. Prediction of hepatic inflammation in chronic\nhepatitis B patients with a random forest-backward feature\nelimination algorithm. World journal of gastroenterology,\n27(21): 2910.\nZou, H.; and Hastie, T. 2005. Regularization and variable\nselection via the elastic net. Journal of the Royal Statistical\nSociety Series B: Statistical Methodology, 67(2): 301–320.\n"}, {"page": 12, "text": "Appendix\nA. Dataset Details\nWe evaluate our method on a diverse suite of tabular datasets drawn from popular benchmarks: OpenML-CC18 (Bischl et al.\n2019) for classification, OpenML-CTR23 (Fischer, Feurer, and Bischl 2023) for regression, and additional datasets from the\nUCI Machine Learning Repository and Kaggle. Table 4 provides a summary organized by task type: binary classification,\nmulti-class classification, and regression. For each dataset, we report key statistics, including the number of samples, number of\nfeatures, label distribution, and dataset identifier. These datasets span a wide range of domains, scales, and structural properties,\nproviding a rigorous testbed for assessing generalization and robustness in few-shot tabular learning.\nTable 4: Dataset Characteristics\nDataset\n#Samples\n#Features\n#Label ratio(%)\nSource\nID/Name\nBinary Classification\nadult\n48842\n14\n76:24\nOpenML\n1590\nbank\n45211\n16\n88:12\nOpenML\n1461\nblood\n748\n4\n76:24\nOpenML\n1464\nbreast-w\n699\n9\n66:34\nOpenML\n15\ncredit-g\n1000\n20\n70:30\nOpenML\n31\ncultivars\n320\n10\n50:50\nUCI\nForty Soybean Cultivars\nfrom Subsequent Harvests\ngallstone\n320\n38\n50:50\nUCI\nGallstone\nNHANES\n6287\n8\n84:16\nUCI\nNational Health and Nutrition Health\nSurvey 2013-2014 Age Prediction Subset\nMulti-class Classification\ncar\n1728\n6\n70:22:4:4\nOpenML\n40975\ncommunities\n1994\n103\n34:33:33\nUCI\nCommunities and Crime\nmyocardial\n1700\n111\n22:78\nUCI\nMyocardial infarction complications\nheart\n918\n11\n45:55\nKaggle\nHeart Failure Prediction Dataset\ncdc diabetes\n253680\n21\n84:14:2\nKaggle\nDiabetes Health Indicators Dataset\nRegression\nCpu small\n8192\n12\nN/A\nOpenML\n562\ndiamonds\n53940\n19\nN/A\nOpenML\n42225\nforest-fires\n517\n13\nN/A\nOpenML\n42363\nhousing\n20640\n9\nN/A\nOpenML\n43996\nplasma retinol\n315\n13\nN/A\nOpenML\n511\nbike\n17389\n11\nN/A\nUCI\nBike Sharing\ncultivars\n320\n10\nN/A\nUCI\nForty Soybean Cultivars\nfrom Subsequent Harvests\ninfrared thermography\ntemperature\n1020\n33\nN/A\nUCI\nInfrared Thermography Temperature\nwine\n4898\n10\nN/A\nUCI\nWine Quality\ninsurance\n1338\n7\nN/A\nKaggle\nUS Health Insurance Dataset\nB. Implementation Details\nData Preparation. Following the popular setting (Han et al. 2024), we partition each dataset into training and test sets, reserv-\ning 20% of the data for testing. For classification tasks, we apply stratified sampling to preserve the label distribution across\nclasses. From the training set, we construct a k-shot subset for supervision, selecting examples in a class-balanced manner\nwhenever possible. In scenarios with limited data or many classes, we permit approximate balancing to maintain representa-\ntiveness. For regression tasks, we discretize the continuous response variable into quantile-based bins and sample within each\nbin to maintain the marginal target distribution. To account for variability due to stochastic sampling, especially prominent\nin few-shot regimes, we repeat each experiment using 10 different random seeds (0 through 9) and report mean performance\nmetrics. All methods receive the same train/test partitions under each seed to ensure consistency and fairness. To minimize\nconfounding effects introduced by preprocessing, we adopt a lightweight and uniform imputation strategy. For LLM-based\n"}, {"page": 13, "text": "methods, all missing values—whether numerical or categorical—are replaced with the string “Unknown”, which serves as a\nsymbolic placeholder compatible with text prompts. For classical models, we impute categorical variables with “Unknown”\nand continuous variables with the column-wise mean. This design ensures that preprocessing does not bias the evaluation of\nsemantic modeling capabilities or statistical performance across methods.\nHyperparameter Tuning. Hyper-parameter selection is essential for fair comparison, particularly for classical machine learn-\ning baselines such as Logistic Regression (LogReg), Random Forest (RF), XGBoost, ElasticNet, Multilayer Perceptron (MLP),\nand TabPFN. We perform hyperparameter tuning for these models using grid search combined with k-shot cross-validation on\nthe training data. Specific hyperparameter grids are detailed in Tables 5–11. For FORESTLLM, we perform hyperparameter\ntuning via grid search without cross-validation to minimize API costs. In the few-shot regime, conventional train/validation\nsplits (e.g., 70%/30%) yield validation sets too small for stable evaluation. Instead, we evaluate each configuration directly on\nthe training set, which we find to be a practical compromise under severe data constraints. The hyperparameter search space is\nsummarized in Table 12.\nEvaluation. Classification performance is reported using the macro-averaged Area Under the ROC Curve (AUC), computed\nvia the roc auc score function from sklearn.metrics. For LLM-based baselines including LIFT, TABLET, and P2T,\nprediction confidences are derived from token-level log-probabilities returned by the OpenAI GPT-4o API, following best\npractices for prompt-based classification. For regression, we report normalized root mean squared error (NRMSE), with nor-\nmalization based on the standard deviation of the target variable in the test set. All reported metrics are averaged over 10\nindependent trials with different train/test seeds. Experiments are executed on a dedicated Ubuntu 22.04.4 LTS server equipped\nwith an AMD EPYC 7V13 64-core CPU, 432 GB of RAM, and two NVIDIA A100 80GB GPUs. OpenAI GPT-4o (engine:\ngpt-4o-2024-11-20) is accessed via the Azure OpenAI API, with temperature set to 0 for deterministic outputs across all\nLLM-based baselines. For ForestLLM, we follow this setting during leaf label assignment to ensure output consistency, while\nsetting a higher temperature of 0.5 during tree construction to encourage diversity across trees.\nTable 5: Hyperparameter search space for CART\nParameter\nSearch space\ncriterion\ngini, entropy\nsquared error, friedman mse, absolute error\nmax depth\n3, 5, 7, 9, 11\nTable 6: Hyperparameter search space for Random Forest\nParameter\nSearch space\nn estimators\n2, 4, 8, 16, 32, 64, 128, 256\nbootstrap\nTrue, False\nmax depth\n3, 5, 7, 9, 11\nmax features\n0.5, 0.7, 1.0\nTable 7: Hyperparameter search space for XGBoost\nParameter\nSearch space\nmax depth\n3, 5, 7, 9, 11\nalpha\n1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100\nlambda\n1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1\neta\n0.01, 0.03, 0.1, 0.3\nC. Case Study of Reasoning During Tree Splits\nTo examine how FORESTLLM performs semantic reasoning during decision tree construction, we visualize its generated\nreasoning traces, split criteria, and leaf label predictions for both classification and regression tasks (Figures 7 and 8). To\nensure consistency and parseability in split decisions, we adopt structured prompting via function calling. The corresponding\nprompting template and function schema are shown in Figure 5, while the leaf inference prompt is provided in Figure 6.\n"}, {"page": 14, "text": "Table 8: Hyperparameter search space for LogReg\nParameter\nSearch space\nC\n1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100\npenalty\nl1, l2\nTable 9: Hyperparameter search space for ElasticNet\nParameter\nSearch space\nalpha\n1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100\nl1 ratio\n0.0, 1.0\nTable 10: Hyperparameter search space for MLP\nParameter\nSearch space\nhidden layer sizes\n(64, ), (128,), (256,), (512,), (1024,)\nalpha\n0.1, 0.01, 0.001\nlearning rate init\n0.1, 0.01, 0.001\nTable 11: Hyperparameter search space for TabPFN\nParameter\nSearch space\nn estimators\n2, 4, 6, 8, 10, 12\nTable 12: Hyperparameter search space for FORESTLLM\nParameter\nSearch space\nn estimators\n3, 4, 5, 6, 7, 8, 9, 10\nmax depth\n2, 3, 4, 5, 6, 7\nbootstrap\nTrue\nmax features\n0.9\n"}, {"page": 15, "text": "We select two representative datasets under a 4-shot setting, one for classification and one for regression, to illustrate\nFORESTLLM’s reasoning process:\n• Credit-g is a binary classification dataset containing 1,000 loan applicants, with approximately 700 labeled as “good\ncredit” and 300 as “bad credit.” It consists of 20 features, including employment status, savings level, and credit history, and\nis widely used in credit risk prediction and interpretability research.\n• Cpu small is a system performance regression dataset collected from a multi-user Sun Sparcstation system. Under real-\nworld workloads, the system records performance metrics every 5 seconds, such as exec (number of system executions per\nsecond), rchar (characters read per second), runqsz (run queue size), and freemem (available memory pages). The task is to\npredict the CPU’s user-mode utilization (usr) based on 12 continuous system-level features.\nBy examining the reasoning traces generated during node splitting, we find that FORESTLLM not only identifies predictive\nfeatures but also provides semantically meaningful justifications grounded in commonsense and domain-specific knowledge.\nFor example, in the Credit-g dataset, the model selects attributes such as employment status, savings level, and credit history,\naccompanied by explanations like “stable employment is a key factor in creditworthiness” and “higher savings indicate stronger\nrepayment ability.” In the Cpu small dataset, it highlights features such as rchar, runqsz, and exec, correctly associating them\nwith system throughput and scheduling behavior, indicating an understanding of low-level performance dynamics.\nMoreover, FORESTLLM demonstrates the ability to infer semantically meaningful split thresholds by jointly leveraging the\ndistributions of both labeled and unlabeled data. In Cpu small, for instance, the model selects a freeswap threshold of\n500,000 to distinguish between resource-constrained and resource-rich systems; it splits on exec at 3.0 or 4.0 to identify\ndifferences in execution efficiency; and sets runqsz at 500.0 or 3.0 to reflect varying levels of system load. These thresholds are\nnot only statistically valid but also align with intuitive interpretations of performance bottlenecks, highlighting FORESTLLM’s\ncapacity for human-aligned, data-driven reasoning under limited supervision.\nWhen assigning labels at leaf nodes, FORESTLLM eschews conventional strategies such as majority voting or numerical av-\neraging. Instead, it performs semantic generalization over the decision path by integrating few-shot labeled examples, domain\nknowledge, and logical reasoning. In the Credit-g dataset, for example, a path characterized by“stable employment + suf-\nficient savings + good credit history” is semantically interpreted as a low-risk credit profile, leading to the label “YES.” In\nCpu small, a path reflecting “ample system resources and efficient execution” yields a high predicted utilization score. This\npath-aware reasoning framework allows FORESTLLM to transcend low-level statistical heuristics, producing label assignments\nthat are both interpretable and contextually aligned with the underlying semantics of the task.\nOverall, FORESTLLM exhibits strong semantic reasoning and interpretability throughout the decision tree construction process.\nOperating under few-shot conditions with limited supervision, the model effectively fuses pretrained language knowledge with\ndistributional cues from both labeled and unlabeled data to derive meaningful split criteria and label assignments. This synergy\nenables FORESTLLM to overcome the inherent limitations of data scarcity, enhancing both its adaptability and the transparency\nof its predictions in low-resource tabular learning scenarios.\nD. Results with Different LLM Backbones\nTo evaluate the impact of different LLM backbones on the performance of FORESTLLM, we conduct additional experiments\nusing gpt-4-32k-0613 and gpt-3.5-turbo-0125, both deployed via Azure, as shown in Tables 13 and 14. Although\nGPT-4-32k is based on an older architecture and lacks the enhanced capabilities of GPT-4o, it delivers comparable results\nacross a wide range of datasets and even surpasses GPT-4o in certain cases. Interestingly, despite sharing the same knowledge\ncutoff date (September 2021), GPT-4-32k consistently outperforms GPT-3.5-turbo across tasks. This performance gap\nsuggests that the underlying reasoning capabilities of the model, not simply knowledge recency, are critical for success in\nstructured, few-shot tabular settings.\nE. Full Results\nThis section presents the complete experimental results across all datasets, baselines, and shot configurations (4, 8, 16, 32, and\n48). While the main paper reports a representative subset due to space constraints, we include the full performance tables here\nfor both classification and regression tasks, as shown in Tables 15 and 16. These tables provide a comprehensive view of model\nbehavior across varying supervision levels and further validate the robustness of our findings.\n"}, {"page": 16, "text": "Prompt for Node Split Condition Inference\n## Objective\nYou are an expert. Given the task description, few-shot examples, and feature summary below, you are extracting\nconditions as rules to solve the task. \n## Task Description\n{task_description}\n## Few-shot Examples (Optional — include if few-shot examples exist for the current node)\n{row_wise_serialized_few_shot_examples}\n## Feature Summary\n{feature_summary}\n## Instructions\nLet's first understand the problem and solve the problem step by step.\nStep 1. Analyze the causal relationship or tendency between each feature and task description based on general\nknowledge and common sense.\nStep 2. Based on the above data distribution patterns and Step 1's results, infer one condition to split the\ndata into two parts, following the format below. The condition should make sense, well match features.\n## Response Format\nFor your response,\n- If splitting on a numerical feature, specify a threshold value based on the data distribution.\n- If splitting on a categorical feature, specify a list of values.\nFunction to Call\n{\n    \"name\": \"Infer_split_condition\",\n    \"description\": f\"Infer a condition to split the data into two parts.\",\n    \"parameters\": {\n        \"properties\": {\n            \"Reasoning\": {\n                \"type\": \"string\",\n                \"title\": \"reasoning\",\n                \"description\": \"Detailed reasoning about the split decision based on data patterns.\"\n            },\n            \"Feature\": {\n                \"type\": \"string\",\n                \"title\": \"Feature\",\n                \"description\": \"The feature selected for the split.\"\n            },\n            \"Threshold\": {\n                \"type\": [\"number\", \"null\"],\n                \"title\": \"Threshold\",\n                \"description\": \"If the selected feature to split on is numerical, specify the threshold value.\nSet null if the selected feature is not numerical.\"\n            },\n            \"Categorical_values\": {\n                \"type\": [\"array\", \"null\"],\n                \"items\": {\"type\": \"string\"},\n                \"title\": \"Categorical Values\",\n                \"description\": \"If the selected feature to split on is categorical, specify the values that\nshould go to the left child. Set null if the selected feature is not categorical.\"\n            }\n        }\n    }\n}\nFigure 5: Prompt template and function calling schema used by FORESTLLM to generate structured and interpretable split\nconditions. The task description provides a concise natural language description of the prediction task, drawn from\ndomain knowledge or dataset metadata (e.g., “Given this person’s characteristics, does this person have diabetes?”). The\nrow wise serialized few shot examples lists labeled samples under the current node in a row-wise serialized for-\nmat, serving as few-shot examples to guide the model’s reasoning. The feature summary summarizes the features of all\nsamples within the current node — including both labeled and unlabeled data — describing feature types, value distributions\nfor categorical features, and statistics for numerical features.\n"}, {"page": 17, "text": "Prompt for Leaf Label Inference\n## Objective\nYou are an expert in reasoning and classification tasks. Given the task description, decision path rule, and\nfew-shot examples, predict the most appropriate class label for this leaf node using general knowledge and\ncommon sense.\n \n## Task Description\n{task_description}\n \n## Decision Path Rule\n{decision_path_rule}\n## Few-shot Examples (Optional — include if few-shot examples exist for the current leaf node)\n{row_wise_serialized_few_shot_examples}\n## Classes (Optional — include only for classification tasks)\n{classes}\n \n## Question:\nBased on the decision path rule above, which class/what value is most likely for samples that reach this leaf\nnode?\n \n## Response Format\nPlease respond strictly in the following JSON format:\n```json\n{\n  \"reasoning\": \"[your brief explanation]\",\n  \"label\": \"[your predicted label]\"\n}\n```\nFigure 6: Prompt template used by FORESTLLM for leaf node label inference.The task description provides a con-\ncise natural language description of the prediction task. The decision path rule summarizes the conditions along the\npath from the root to the current leaf node, representing the accumulated split decisions applied to reach this node. The\nrow wise serialized few shot examples lists representative labeled samples that have arrived at the current leaf\nnode, formatted in a row-wise serialized manner to support in-context few-shot reasoning. The classes enumerates all pos-\nsible class labels for the classification task, including those that may not appear in the few-shot examples.\n"}, {"page": 18, "text": "According to general knowledge, the financial\nstability of an individual highly depends on their\nemployment status. People with stable and long-\nterm employment have a higher likelihood of\nreceiving credit as they demonstrate the capacity\nto meet payment obligations. The categories\nwithin the employment feature indicate different\ntime periods which can affect the financial\nstability assessment.\nSavings status is one of the major indicators\nof financial health. Individuals with higher\namounts of savings or significant financial\nreserves are more likely to be considered\ncreditworthy. This is evident because higher\nsavings demonstrate better financial\nstability and lower credit risk, which makes\nthose individuals more likely to receive\ncredit.\nCredit history holds strong predictive power\nin financial creditworthiness assessment. A\nhistory of 'existing paid' or 'all paid'\nsuggests timely repayments and financial\nreliability, increasing the likelihood of\nreceiving credit, while 'delayed previously'\nor 'critical/other existing credit' indicates\npast financial issues, posing a higher credit\nrisk.\nThe checking_status feature is a strong indicator of\nimmediate liquidity and financial health. Categories such\nas '>=200' or '0<=X<200' suggest that the person has a\nbetter or stable financial situation and a checking\naccount balance, which makes them more likely to\nreceive credit. Conversely, categories like '<0' or 'no\nchecking' indicate a lack of funds or past issues related\nto financial management, which increases\ncreditworthiness risk.\nEmployment stability is a critical factor when assessing\nan individual's creditworthiness. People with stable jobs\nor employment status are generally prioritized for credit\napprovals due to a steady income stream, while\nunemployed individuals have a higher risk of defaulting\non repayments.\nSavings status often correlates with creditworthiness\nbecause sufficient savings indicate financial stability and\nthe capacity to repay loans. People with higher savings\nare usually considered less risky.\nSavings status often reflects an individual's financial\ndiscipline and stability. People with higher savings are\ngenerally deemed more creditworthy as they\ndemonstrate good financial practices and a reduced risk\nof default. In this case, savings amounts can be used as a\nstrong indicator for a yes or no credit decision.\nemployment is in [4<=X<7, 1<=X<4, >=7]\nemployment is not in [4<=X<7, 1<=X<4, >=7]\nsavings_status is in [500<=X<1000, >=1000]\nsavings_status is not in [500<=X<1000, >=1000]\nThe rule ['employment is in [4<=X<7, 1<=X<4,\n>=7]', 'savings_status is in [500<=X<1000,\n>=1000]', 'credit_history is in [existing paid, all\npaid]'] is labeled as YES\nThe rule ['employment is in [4<=X<7, 1<=X<4,\n>=7]', 'savings_status is in [500<=X<1000,\n>=1000]', 'credit_history is in [existing paid, all\npaid]'] is labeled as YES\nThe rule ['employment is in [4<=X<7, 1<=X<4,\n>=7]', 'savings_status is not in [500<=X<1000,\n>=1000]', 'checking_status is in [0<=X<200,\n>=200]'] is labeled as NO\nThe rule ['employment is in [4<=X<7, 1<=X<4,\n>=7]', 'savings_status is not in [500<=X<1000,\n>=1000]', 'checking_status is not in [0<=X<200,\n>=200]'] is labeled as NO\nThe rule ['employment is not in [4<=X<7, 1<=X<4,\n>=7]', 'employment is in [<1]', 'savings_status is in\n[>=1000, 500<=X<1000]'] is labeled as YES\nThe rule ['employment is not in [4<=X<7, 1<=X<4,\n>=7]', 'employment is in [<1]', 'savings_status is not\nin [>=1000, 500<=X<1000]'] is labeled as NO\nThe rule ['employment is not in [4<=X<7, 1<=X<4,\n>=7]', 'employment is not in [<1]', 'savings_status is\nin [100<=X<500, 500<=X<1000]'] is labeled as\nYES\nThe rule ['employment is not in [4<=X<7, 1<=X<4,\n>=7]', 'employment is not in [<1]', 'savings_status is\nnot in [100<=X<500, 500<=X<1000]'] is labeled as\nNO\ncredit_history is in [existing paid, all paid]\ncredit_history is in not in [existing paid, all paid]\nchecking_status is in [0<=X<200, >=200]\nchecking_status is not in [0<=X<200, >=200]\nsavings_status is in [>=1000, 500<=X<1000]\nsavings_status is not in [>=1000, 500<=X<1000]\nsavings_status is in [100<=X<500, 500<=X<1000]\nsavings_status is not in [100<=X<500, 500<=X<1000]\nemployment is in [<1]\nemployment is not in [<1]\nFigure 7: A decision tree generated by FORESTLLM on the Credit-g dataset.\n"}, {"page": 19, "text": "Among the features, 'freeswap' represents free swap\nmemory and has a wide range, indicating potential\nvariability in system performance. Systems with\nextremely low free swap memory might behave\ndifferently compared to those with higher values.\nGiven the mean and range, a threshold around the 25th\npercentile (closer to the mean) can help distinguish\nbetween potentially constrained and unconstrained\nsystems.\nThe feature 'rchar', which measures the number of\ncharacters read, shows a very wide range and a high\nmean value, indicating significant variability in\nsystem performance. CPUs with higher 'rchar' values\nlikely handle more intensive workloads, which could\ncorrelate with higher performance. Splitting on\n'rchar' allows us to separate systems handling light\nworkloads from those managing heavier workloads.\nThe feature 'runqsz' measures the size of the run\nqueue. A smaller run queue size generally indicates\nbetter CPU performance since fewer processes are\nwaiting for CPU time. Given its narrow range and\nlow mean, splitting on 'runqsz' with a threshold near\nits mean can effectively separate systems with\npotentially higher performance from those with\nlower performance.\nThe feature 'runqsz', which represents the size of the run queue,\nhas a wide range and a high mean value, indicating variability\nin CPU workload. A larger run queue size may suggest a higher\nworkload or contention, which could correlate with differences\nin CPU performance. Splitting on this feature allows us to\ndistinguish between systems with potentially higher workloads\nand those with lighter workloads.\nThe feature 'wchar' (number of characters written) has a wide\nrange and a high mean, indicating significant variability in CPU\nworkload. CPUs with higher 'wchar' values are likely handling\nmore intensive tasks, which could correlate with higher\nperformance. Splitting on 'wchar' allows us to distinguish\nbetween systems managing lighter versus heavier workloads.\nThe feature 'exec' measures the number of executions\nperformed by the system. Higher execution rates often correlate\nwith better CPU performance, as it indicates the CPU's ability\nto handle tasks efficiently. Given the range and mean, splitting\non 'exec' can help distinguish CPUs with higher performance\nlevels from those with lower levels.\nThe feature 'exec' measures the execution time, which is a\ncritical determinant of CPU performance. A higher 'exec' value\nindicates longer execution times, potentially pointing to lower\nCPU efficiency. Given the range and mean, splitting on 'exec' at\na threshold slightly above the mean (e.g., 4.0) can effectively\nseparate CPUs with faster execution times from those with\nslower execution times.\nrchar <= 500000.0\nrchar > 500000.0\nThe rule is ['freeswap <= 500000.0', 'rchar <=\n500000.0', 'runqsz <= 500.0'] is labeled as 67.0\nThe rule is ['freeswap <= 500000.0', 'rchar <=\n500000.0', 'runqsz > 500.0'] is labeled as 45.0\nThe rule is ['freeswap <= 500000.0', 'rchar >\n500000.0', 'wchar <= 800000.0'] is labeled as 62.0\nThe rule is ['freeswap > 500000.0', 'runqsz <= 3.0',\n'exec <= 3.0'] is labeled as 91.0\nThe rule is ['freeswap > 500000.0', 'runqsz <= 3.0',\n'exec > 3.0'] is labeled as 72.0\nThe rule is ['freeswap > 500000.0', 'runqsz > 3.0',\n'exec <= 4.0'] is labeled as 87.0\nThe rule is ['freeswap > 500000.0', 'runqsz > 3.0',\n'exec > 4.0'] is labeled as 89.0\nrunqsz <= 500.0\nwchar <= 800000.0\nwchar > 800000.0\nexec <= 3.0\nexec > 3.0\nexec <= 4.0\nexec > 4.0\nrunqsz <= 3.0\nrunqsz > 3.0\nThe rule is ['freeswap <= 500000.0', 'rchar >\n500000.0', 'wchar > 800000.0'] is labeled as 62.0\nfreeswap <= 500000.0\nfreeswap > 500000.0\nrunqsz > 500.0\nFigure 8: A decision tree generated by FORESTLLM on the Cpu small dataset.\n"}, {"page": 20, "text": "Table 13: Effect of LLM backbone choice on classification performance\nData\nShot\nGPT-4o\nGPT-4-32k\nGPT-3.5-turbo\nadult\n4\n0.833 ± 0.041\n0.838±0.030\n0.816±0.034\n8\n0.827 ± 0.020\n0.839±0.021\n0.824±0.022\n16\n0.823 ± 0.028\n0.835±0.027\n0.823±0.014\n32\n0.846 ± 0.020\n0.847±0.020\n0.814±0.026\n48\n0.834 ± 0.032\n0.852±0.018\n0.833±0.011\nblood\n4\n0.691 ± 0.066\n0.719±0.060\n0.699±0.051\n8\n0.698 ± 0.050\n0.720±0.035\n0.727±0.030\n16\n0.709 ± 0.042\n0.734±0.038\n0.729±0.038\n32\n0.723 ± 0.043\n0.743±0.037\n0.739±0.035\n48\n0.724 ± 0.049\n0.744±0.030\n0.732±0.035\nbank\n4\n0.837 ± 0.014\n0.843±0.009\n0.755±0.036\n8\n0.835 ± 0.013\n0.842±0.012\n0.747±0.042\n16\n0.843 ± 0.008\n0.843±0.012\n0.758±0.024\n32\n0.841 ± 0.010\n0.847±0.009\n0.740±0.031\n48\n0.843 ± 0.011\n0.841±0.011\n0.749±0.036\ncar\n4\n0.864 ± 0.034\n0.878±0.022\n0.771±0.030\n8\n0.879 ± 0.028\n0.903±0.018\n0.728±0.022\n16\n0.881 ± 0.021\n0.893±0.023\n0.755±0.026\n32\n0.881 ± 0.029\n0.909±0.021\n0.769±0.030\n48\n0.893 ± 0.020\n0.910±0.022\n0.758±0.030\ncommunities\n4\n0.784 ± 0.012\n0.774±0.006\n0.643±0.043\n8\n0.782 ± 0.008\n0.771±0.008\n0.627±0.037\n16\n0.789 ± 0.010\n0.772±0.010\n0.642±0.042\n32\n0.781 ± 0.013\n0.780±0.010\n0.660±0.026\n48\n0.779 ± 0.011\n0.774±0.011\n0.661±0.046\ncredit-g\n4\n0.669 ± 0.042\n0.670±0.034\n0.625±0.019\n8\n0.687 ± 0.053\n0.686±0.050\n0.628±0.030\n16\n0.693 ± 0.044\n0.679±0.039\n0.650±0.034\n32\n0.709 ± 0.031\n0.679±0.034\n0.630±0.034\n48\n0.713 ± 0.045\n0.704±0.035\n0.627±0.030\ncdc diabetes\n4\n0.700 ± 0.012\n0.702±0.010\n0.612±0.030\n8\n0.699 ± 0.012\n0.703±0.012\n0.594±0.034\n16\n0.705 ± 0.011\n0.704±0.011\n0.593±0.035\n32\n0.709 ± 0.005\n0.710±0.011\n0.594±0.026\n48\n0.706 ± 0.009\n0.706±0.007\n0.598±0.019\nheart\n4\n0.887 ± 0.022\n0.894±0.015\n0.834±0.038\n8\n0.896 ± 0.015\n0.899±0.012\n0.785±0.022\n16\n0.898 ± 0.011\n0.893±0.017\n0.773±0.024\n32\n0.896 ± 0.013\n0.895±0.012\n0.790±0.032\n48\n0.898 ± 0.018\n0.891±0.014\n0.794±0.022\nmyocardial\n4\n0.653 ± 0.048\n0.619±0.069\n0.667±0.037\n8\n0.660 ± 0.046\n0.623±0.053\n0.643±0.038\n16\n0.687 ± 0.048\n0.645±0.025\n0.647±0.041\n32\n0.681 ± 0.055\n0.639±0.022\n0.644±0.028\n48\n0.658 ± 0.034\n0.656±0.023\n0.636±0.027\nbreast-w\n4\n0.992 ± 0.006\n0.993±0.005\n0.989±0.008\n8\n0.993 ± 0.006\n0.992±0.006\n0.991±0.007\n16\n0.993 ± 0.006\n0.994±0.005\n0.992±0.005\n32\n0.994 ± 0.004\n0.993±0.006\n0.990±0.006\n48\n0.994 ± 0.006\n0.992±0.006\n0.992±0.006\ncultivars\n4\n0.661 ± 0.031\n0.653±0.044\n0.630±0.040\n8\n0.653 ± 0.025\n0.657±0.032\n0.595±0.034\n16\n0.679 ± 0.027\n0.677±0.044\n0.628±0.030\n32\n0.676 ± 0.041\n0.670±0.055\n0.631±0.039\n48\n0.670 ± 0.049\n0.671±0.045\n0.639±0.034\nNHANES\n4\n0.987 ± 0.010\n0.974±0.055\n0.892±0.128\n8\n0.998 ± 0.002\n0.996±0.003\n0.887±0.080\n16\n0.999 ± 0.001\n0.996±0.006\n0.947±0.049\n32\n0.999 ± 0.001\n0.996±0.008\n0.953±0.037\n48\n0.999 ± 0.001\n0.995±0.007\n0.964±0.054\ngallstone\n4\n0.658 ± 0.042\n0.674±0.058\n0.698±0.037\n8\n0.653 ± 0.054\n0.666±0.066\n0.679±0.045\n16\n0.662 ± 0.045\n0.674±0.042\n0.677±0.058\n32\n0.659 ± 0.045\n0.637±0.049\n0.667±0.033\n48\n0.663 ± 0.054\n0.656±0.046\n0.689±0.038\n"}, {"page": 21, "text": "Table 14: Effect of LLM backbone choice on regression performance\nData\nShot\nGPT-4o\nGPT-4-32k\nGPT-3.5-turbo\nbike\n4\n0.156 ± 0.007\n0.155±0.009\n0.175±0.014\n8\n0.149 ± 0.007\n0.145±0.006\n0.177±0.012\n16\n0.144 ± 0.011\n0.146±0.008\n0.170±0.006\n32\n0.142 ± 0.006\n0.142±0.004\n0.168±0.007\n48\n0.141 ± 0.007\n0.139±0.004\n0.170±0.006\ncpu small\n4\n0.161 ± 0.038\n0.153±0.008\n0.188±0.016\n8\n0.154 ± 0.024\n0.151±0.026\n0.195±0.010\n16\n0.114 ± 0.027\n0.112±0.027\n0.172±0.011\n32\n0.116 ± 0.022\n0.115±0.021\n0.176±0.009\n48\n0.122 ± 0.021\n0.128±0.015\n0.177±0.009\ndiamonds\n4\n0.090 ± 0.010\n0.089±0.008\n0.141±0.055\n8\n0.095 ± 0.013\n0.092±0.012\n0.158±0.033\n16\n0.095 ± 0.007\n0.090±0.008\n0.114±0.012\n32\n0.090 ± 0.005\n0.092±0.004\n0.117±0.014\n48\n0.090 ± 0.005\n0.090±0.004\n0.117±0.013\nforest-fires\n4\n0.126 ± 0.029\n0.125±0.029\n0.126±0.029\n8\n0.126 ± 0.029\n0.125±0.029\n0.124±0.030\n16\n0.123 ± 0.029\n0.122±0.029\n0.123±0.027\n32\n0.122 ± 0.029\n0.122±0.030\n0.122±0.030\n48\n0.118 ± 0.031\n0.118±0.031\n0.121±0.029\nhouses\n4\n0.150 ± 0.030\n0.151±0.028\n0.180±0.056\n8\n0.131 ± 0.011\n0.133±0.014\n0.150±0.016\n16\n0.128 ± 0.011\n0.128±0.011\n0.143±0.013\n32\n0.127 ± 0.011\n0.128±0.010\n0.146±0.012\n48\n0.126 ± 0.011\n0.127±0.009\n0.148±0.011\ninsurance\n4\n0.115 ± 0.021\n0.112±0.017\n0.141±0.029\n8\n0.107 ± 0.014\n0.107±0.015\n0.133±0.023\n16\n0.105 ± 0.012\n0.105±0.014\n0.150±0.018\n32\n0.100 ± 0.010\n0.100±0.012\n0.153±0.022\n48\n0.102 ± 0.014\n0.099±0.011\n0.157±0.025\nplasma retinol\n4\n0.216 ± 0.038\n0.209±0.031\n0.213±0.041\n8\n0.203 ± 0.028\n0.203±0.034\n0.201±0.027\n16\n0.192 ± 0.018\n0.191±0.017\n0.195±0.022\n32\n0.193 ± 0.017\n0.193±0.021\n0.198±0.026\n48\n0.193 ± 0.019\n0.194±0.023\n0.198±0.024\nwine\n4\n0.141 ± 0.010\n0.141±0.010\n0.134±0.004\n8\n0.137 ± 0.006\n0.135±0.006\n0.136±0.006\n16\n0.132 ± 0.002\n0.132±0.005\n0.136±0.005\n32\n0.131 ± 0.002\n0.130±0.003\n0.138±0.005\n48\n0.131 ± 0.003\n0.130±0.003\n0.141±0.006\ncultivars\n4\n0.206 ± 0.021\n0.213±0.027\n0.216±0.033\n8\n0.196 ± 0.017\n0.199±0.017\n0.204±0.017\n16\n0.198 ± 0.023\n0.198±0.024\n0.198±0.019\n32\n0.193 ± 0.019\n0.194±0.019\n0.196±0.017\n48\n0.191 ± 0.022\n0.192±0.017\n0.198±0.016\ninfrared thermography\ntemperature\n4\n0.108 ± 0.012\n0.105±0.012\n0.126±0.009\n8\n0.102 ± 0.010\n0.100±0.010\n0.135±0.026\n16\n0.103 ± 0.008\n0.107±0.010\n0.129±0.011\n32\n0.097 ± 0.007\n0.100±0.010\n0.125±0.010\n48\n0.096 ± 0.009\n0.101±0.007\n0.127±0.010\n"}, {"page": 22, "text": "Table 15: Performance comparison of traditional methods and LLM-based methods across various datasets and 4-, 8-, 16-, 32-,\nand 48-shot settings for classification tasks. The best-performing method is bolded, and the second-best is underlined. Each\nvalue represents the AUC score, reported as the mean ± standard deviation across 10 random seeds.\nDataset\nShot\nTraditional Method\nLLM-based Method\nTraining Required\nFine-tuned\nRequired\nNo Fine-tuning Required\nCART\nRandom\nForest\nXGBoost\nLogReg\nSCARF\nSTUNT\nTabPFN\nTP-BERTa\nTabLLM\nTABLET\nLIFT\nFeatLLM\nP2T\nOurs\nadult\n4\n0.600 ± 0.075\n0.629 ± 0.131\n0.500 ± 0.000\n0.539 ± 0.057\n0.595 ± 0.131\n0.503 ± 0.030\n0.691 ± 0.159\n0.582 ± 0.096\n0.800 ± 0.019\n0.807 ± 0.039\n0.710 ± 0.052\n0.870 ± 0.025\n0.765 ± 0.035\n0.833 ± 0.041\n8\n0.618 ± 0.091\n0.666 ± 0.121\n0.596 ± 0.081\n0.603 ± 0.070\n0.658 ± 0.124\n0.532 ± 0.033\n0.733 ± 0.132\n0.610 ± 0.095\n0.795 ± 0.050\n0.826 ± 0.024\n0.694 ± 0.039\n0.880 ± 0.009\n0.779 ± 0.036\n0.827 ± 0.020\n16\n0.639 ± 0.098\n0.725 ± 0.092\n0.685 ± 0.120\n0.667 ± 0.066\n0.658 ± 0.122\n0.541 ± 0.025\n0.746 ± 0.089\n0.658 ± 0.056\n0.821 ± 0.025\n0.819 ± 0.020\n0.667 ± 0.038\n0.877 ± 0.013\n0.777 ± 0.030\n0.823 ± 0.028\n32\n0.672 ± 0.060\n0.759 ± 0.074\n0.743 ± 0.089\n0.687 ± 0.057\n0.717 ± 0.097\n0.532 ± 0.021\n0.791 ± 0.089\n0.659 ± 0.067\n0.819 ± 0.029\n0.820 ± 0.029\n0.763 ± 0.057\n0.874 ± 0.013\n0.787 ± 0.041\n0.846 ± 0.020\n48\n0.672 ± 0.029\n0.795 ± 0.051\n0.757 ± 0.093\n0.726 ± 0.076\n0.763 ± 0.072\n0.537 ± 0.021\n0.832 ± 0.035\n0.690 ± 0.024\n0.832 ± 0.021\n0.828 ± 0.026\n0.796 ± 0.025\n0.876 ± 0.007\n0.797 ± 0.031\n0.834 ± 0.032\nblood\n4\n0.493 ± 0.098\n0.500 ± 0.116\n0.500 ± 0.000\n0.523 ± 0.128\n0.502 ± 0.138\n0.562 ± 0.077\n0.563 ± 0.112\n0.446 ± 0.069\n0.561 ± 0.130\n0.626 ± 0.072\n0.511 ± 0.011\n0.530 ± 0.134\n0.568 ± 0.113\n0.691 ± 0.066\n8\n0.542 ± 0.099\n0.575 ± 0.095\n0.567 ± 0.060\n0.564 ± 0.127\n0.617 ± 0.098\n0.553 ± 0.084\n0.604 ± 0.118\n0.473 ± 0.055\n0.578 ± 0.109\n0.648 ± 0.071\n0.551 ± 0.055\n0.598 ± 0.121\n0.618 ± 0.084\n0.698 ± 0.050\n16\n0.602 ± 0.069\n0.635 ± 0.073\n0.601 ± 0.087\n0.673 ± 0.089\n0.593 ± 0.108\n0.551 ± 0.085\n0.636 ± 0.080\n0.497 ± 0.089\n0.605 ± 0.097\n0.655 ± 0.063\n0.523 ± 0.052\n0.621 ± 0.090\n0.664 ± 0.071\n0.709 ± 0.042\n32\n0.605 ± 0.056\n0.658 ± 0.065\n0.682 ± 0.041\n0.705 ± 0.047\n0.666 ± 0.047\n0.571 ± 0.067\n0.692 ± 0.035\n0.484 ± 0.078\n0.644 ± 0.037\n0.680 ± 0.032\n0.618 ± 0.063\n0.695 ± 0.079\n0.701 ± 0.040\n0.723 ± 0.043\n48\n0.626 ± 0.064\n0.683 ± 0.046\n0.679 ± 0.034\n0.686 ± 0.082\n0.693 ± 0.052\n0.571 ± 0.086\n0.713 ± 0.043\n0.524 ± 0.057\n0.663 ± 0.058\n0.699 ± 0.032\n0.601 ± 0.064\n0.692 ± 0.056\n0.702 ± 0.033\n0.724 ± 0.049\nbank\n4\n0.535 ± 0.029\n0.613 ± 0.050\n0.500 ± 0.000\n0.585 ± 0.137\n0.556 ± 0.070\n0.531 ± 0.103\n0.602 ± 0.088\n0.401 ± 0.156\n0.609 ± 0.079\n0.829 ± 0.016\n0.616 ± 0.053\n0.723 ± 0.022\n0.683 ± 0.041\n0.837 ± 0.014\n8\n0.608 ± 0.089\n0.699 ± 0.085\n0.587 ± 0.095\n0.676 ± 0.098\n0.572 ± 0.075\n0.544 ± 0.075\n0.706 ± 0.074\n0.408 ± 0.139\n0.638 ± 0.058\n0.830 ± 0.027\n0.628 ± 0.047\n0.742 ± 0.021\n0.723 ± 0.047\n0.835 ± 0.013\n16\n0.642 ± 0.071\n0.730 ± 0.062\n0.675 ± 0.088\n0.738 ± 0.060\n0.584 ± 0.065\n0.559 ± 0.053\n0.773 ± 0.084\n0.447 ± 0.123\n0.661 ± 0.041\n0.836 ± 0.020\n0.616 ± 0.036\n0.752 ± 0.025\n0.740 ± 0.040\n0.843 ± 0.008\n32\n0.691 ± 0.035\n0.774 ± 0.033\n0.752 ± 0.034\n0.717 ± 0.083\n0.614 ± 0.048\n0.608 ± 0.068\n0.815 ± 0.040\n0.530 ± 0.080\n0.674 ± 0.039\n0.838 ± 0.013\n0.713 ± 0.029\n0.768 ± 0.036\n0.731 ± 0.043\n0.841 ± 0.010\n48\n0.709 ± 0.044\n0.773 ± 0.027\n0.779 ± 0.030\n0.771 ± 0.039\n0.616 ± 0.048\n0.665 ± 0.037\n0.829 ± 0.022\n0.506 ± 0.145\n0.721 ± 0.074\n0.838 ± 0.008\n0.727 ± 0.023\n0.791 ± 0.031\n0.732 ± 0.030\n0.843 ± 0.011\ncar\n4\n0.562 ± 0.051\n0.554 ± 0.051\n0.500 ± 0.000\n0.580 ± 0.057\n0.571 ± 0.074\n0.564 ± 0.026\n0.612 ± 0.059\n0.585 ± 0.082\n0.558 ± 0.043\n0.840 ± 0.057\n0.799 ± 0.070\n0.703 ± 0.060\n0.656 ± 0.045\n0.864 ± 0.034\n8\n0.581 ± 0.061\n0.643 ± 0.048\n0.620 ± 0.036\n0.622 ± 0.033\n0.622 ± 0.059\n0.609 ± 0.027\n0.673 ± 0.060\n0.660 ± 0.051\n0.617 ± 0.041\n0.918 ± 0.017\n0.779 ± 0.054\n0.702 ± 0.072\n0.704 ± 0.022\n0.879 ± 0.028\n16\n0.669 ± 0.064\n0.736 ± 0.036\n0.679 ± 0.041\n0.642 ± 0.082\n0.673 ± 0.061\n0.652 ± 0.038\n0.785 ± 0.060\n0.724 ± 0.025\n0.628 ± 0.116\n0.845 ± 0.047\n0.804 ± 0.023\n0.744 ± 0.086\n0.760 ± 0.023\n0.881 ± 0.021\n32\n0.745 ± 0.075\n0.838 ± 0.038\n0.809 ± 0.038\n0.707 ± 0.038\n0.739 ± 0.025\n0.654 ± 0.036\n0.892 ± 0.028\n0.812 ± 0.040\n0.782 ± 0.050\n0.858 ± 0.042\n0.836 ± 0.022\n0.837 ± 0.027\n0.784 ± 0.022\n0.881 ± 0.029\n48\n0.776 ± 0.051\n0.853 ± 0.035\n0.861 ± 0.029\n0.713 ± 0.032\n0.778 ± 0.040\n0.679 ± 0.054\n0.935 ± 0.021\n0.889 ± 0.037\n0.854 ± 0.038\n0.834 ± 0.068\n0.856 ± 0.024\n0.709 ± 0.131\n0.805 ± 0.024\n0.893 ± 0.020\ncommunities\n4\n0.568 ± 0.069\n0.606 ± 0.037\n0.500 ± 0.000\n0.559 ± 0.043\n0.654 ± 0.082\n0.560 ± 0.030\n0.687 ± 0.052\n0.469 ± 0.061\nN/A\n0.799 ± 0.013\n0.686 ± 0.053\n0.633 ± 0.110\n0.586 ± 0.071\n0.784 ± 0.012\n8\n0.603 ± 0.056\n0.718 ± 0.039\n0.664 ± 0.060\n0.586 ± 0.073\n0.726 ± 0.044\n0.612 ± 0.053\n0.743 ± 0.053\n0.519 ± 0.070\nN/A\n0.798 ± 0.025\n0.725 ± 0.024\n0.691 ± 0.057\n0.594 ± 0.095\n0.782 ± 0.008\n16\n0.623 ± 0.044\n0.738 ± 0.021\n0.689 ± 0.072\n0.639 ± 0.082\n0.745 ± 0.045\n0.646 ± 0.033\n0.769 ± 0.040\n0.556 ± 0.032\nN/A\n0.794 ± 0.028\n0.725 ± 0.028\n0.699 ± 0.051\n0.675 ± 0.071\n0.789 ± 0.010\n32\n0.655 ± 0.064\n0.782 ± 0.038\n0.752 ± 0.038\n0.727 ± 0.037\n0.765 ± 0.039\n0.650 ± 0.030\n0.806 ± 0.029\n0.534 ± 0.049\nN/A\n0.799 ± 0.023\n0.668 ± 0.019\n0.730 ± 0.039\n0.693 ± 0.105\n0.781 ± 0.013\n48\n0.689 ± 0.032\n0.800 ± 0.024\n0.783 ± 0.027\n0.742 ± 0.066\n0.783 ± 0.029\n0.661 ± 0.028\n0.824 ± 0.024\n0.624 ± 0.103\nN/A\n0.802 ± 0.016\n0.637 ± 0.025\n0.767 ± 0.021\n0.622 ± 0.094\n0.779 ± 0.011\ncredit-g\n4\n0.515 ± 0.063\n0.526 ± 0.056\n0.500 ± 0.000\n0.521 ± 0.049\n0.539 ± 0.072\n0.508 ± 0.051\n0.532 ± 0.059\n0.467 ± 0.077\n0.580 ± 0.092\n0.623 ± 0.065\n0.506 ± 0.018\n0.498 ± 0.042\n0.538 ± 0.114\n0.669 ± 0.042\n8\n0.543 ± 0.074\n0.571 ± 0.094\n0.546 ± 0.064\n0.600 ± 0.062\n0.548 ± 0.053\n0.506 ± 0.059\n0.584 ± 0.070\n0.469 ± 0.082\n0.624 ± 0.110\n0.627 ± 0.063\n0.499 ± 0.005\n0.531 ± 0.053\n0.588 ± 0.097\n0.687 ± 0.053\n16\n0.572 ± 0.068\n0.599 ± 0.083\n0.561 ± 0.059\n0.582 ± 0.075\n0.558 ± 0.058\n0.527 ± 0.055\n0.645 ± 0.085\n0.477 ± 0.105\n0.658 ± 0.098\n0.632 ± 0.072\n0.513 ± 0.018\n0.548 ± 0.071\n0.661 ± 0.059\n0.693 ± 0.044\n32\n0.595 ± 0.054\n0.628 ± 0.062\n0.635 ± 0.065\n0.626 ± 0.053\n0.642 ± 0.058\n0.515 ± 0.060\n0.640 ± 0.060\n0.425 ± 0.142\n0.685 ± 0.066\n0.647 ± 0.045\n0.546 ± 0.038\n0.569 ± 0.082\n0.682 ± 0.051\n0.709 ± 0.031\n48\n0.620 ± 0.030\n0.679 ± 0.059\n0.677 ± 0.046\n0.669 ± 0.077\n0.683 ± 0.058\n0.561 ± 0.028\n0.693 ± 0.043\n0.491 ± 0.073\n0.702 ± 0.059\n0.646 ± 0.032\n0.597 ± 0.048\n0.616 ± 0.090\n0.670 ± 0.052\n0.713 ± 0.045\ncdc diabetes\n4\n0.539 ± 0.041\n0.526 ± 0.039\n0.500 ± 0.000\n0.565 ± 0.050\n0.604 ± 0.061\n0.557 ± 0.035\n0.605 ± 0.040\n0.541 ± 0.080\n0.624 ± 0.013\n0.702 ± 0.015\n0.655 ± 0.008\n0.604 ± 0.059\n0.570 ± 0.080\n0.700 ± 0.012\n8\n0.551 ± 0.041\n0.605 ± 0.045\n0.578 ± 0.055\n0.559 ± 0.051\n0.622 ± 0.066\n0.581 ± 0.035\n0.613 ± 0.051\n0.578 ± 0.069\n0.638 ± 0.019\n0.703 ± 0.012\n0.656 ± 0.007\n0.590 ± 0.073\n0.609 ± 0.084\n0.699 ± 0.012\n16\n0.562 ± 0.050\n0.611 ± 0.056\n0.600 ± 0.069\n0.590 ± 0.050\n0.600 ± 0.069\n0.572 ± 0.045\n0.630 ± 0.070\n0.602 ± 0.048\n0.620 ± 0.041\n0.701 ± 0.025\n0.649 ± 0.016\n0.633 ± 0.072\n0.599 ± 0.085\n0.705 ± 0.011\n32\n0.596 ± 0.047\n0.666 ± 0.038\n0.657 ± 0.034\n0.624 ± 0.033\n0.642 ± 0.074\n0.605 ± 0.025\n0.691 ± 0.044\n0.638 ± 0.043\n0.660 ± 0.027\n0.710 ± 0.013\n0.649 ± 0.011\n0.673 ± 0.045\n0.685 ± 0.024\n0.709 ± 0.005\n48\n0.595 ± 0.031\n0.672 ± 0.033\n0.655 ± 0.022\n0.653 ± 0.026\n0.667 ± 0.018\n0.610 ± 0.029\n0.698 ± 0.030\n0.630 ± 0.024\n0.668 ± 0.021\n0.709 ± 0.008\n0.636 ± 0.013\n0.684 ± 0.029\n0.675 ± 0.019\n0.706 ± 0.009\nheart\n4\n0.595 ± 0.092\n0.711 ± 0.123\n0.500 ± 0.000\n0.510 ± 0.119\n0.816 ± 0.123\n0.611 ± 0.062\n0.767 ± 0.123\n0.497 ± 0.089\n0.734 ± 0.109\n0.858 ± 0.017\n0.652 ± 0.084\n0.870 ± 0.024\n0.602 ± 0.125\n0.887 ± 0.022\n8\n0.639 ± 0.104\n0.721 ± 0.092\n0.594 ± 0.112\n0.659 ± 0.081\n0.867 ± 0.045\n0.594 ± 0.066\n0.828 ± 0.051\n0.521 ± 0.100\n0.809 ± 0.066\n0.858 ± 0.025\n0.563 ± 0.054\n0.880 ± 0.025\n0.763 ± 0.073\n0.896 ± 0.015\n16\n0.695 ± 0.091\n0.840 ± 0.040\n0.804 ± 0.077\n0.742 ± 0.108\n0.891 ± 0.034\n0.632 ± 0.078\n0.880 ± 0.026\n0.630 ± 0.090\n0.848 ± 0.034\n0.857 ± 0.014\n0.570 ± 0.056\n0.877 ± 0.037\n0.763 ± 0.073\n0.898 ± 0.011\n32\n0.748 ± 0.067\n0.879 ± 0.020\n0.858 ± 0.017\n0.794 ± 0.061\n0.900 ± 0.028\n0.643 ± 0.040\n0.891 ± 0.019\n0.810 ± 0.029\n0.862 ± 0.047\n0.854 ± 0.019\n0.665 ± 0.048\n0.892 ± 0.027\n0.853 ± 0.043\n0.896 ± 0.013\n48\n0.786 ± 0.056\n0.884 ± 0.020\n0.877 ± 0.018\n0.864 ± 0.032\n0.903 ± 0.021\n0.655 ± 0.046\n0.898 ± 0.016\n0.825 ± 0.026\n0.871 ± 0.036\n0.853 ± 0.015\n0.727 ± 0.034\n0.900 ± 0.017\n0.854 ± 0.032\n0.898 ± 0.018\nmyocardial\n4\n0.513 ± 0.038\n0.519 ± 0.071\n0.500 ± 0.000\n0.536 ± 0.061\n0.551 ± 0.079\n0.527 ± 0.063\n0.522 ± 0.074\n0.516 ± 0.082\nN/A\n0.609 ± 0.073\n0.503 ± 0.004\n0.568 ± 0.070\n0.580 ± 0.031\n0.653 ± 0.048\n8\n0.512 ± 0.047\n0.537 ± 0.062\n0.543 ± 0.085\n0.521 ± 0.078\n0.513 ± 0.078\n0.528 ± 0.045\n0.573 ± 0.102\n0.538 ± 0.058\nN/A\n0.616 ± 0.047\n0.519 ± 0.033\n0.549 ± 0.067\n0.583 ± 0.037\n0.660 ± 0.046\n16\n0.547 ± 0.060\n0.592 ± 0.084\n0.569 ± 0.067\n0.537 ± 0.098\n0.519 ± 0.048\n0.538 ± 0.052\n0.608 ± 0.059\n0.587 ± 0.053\nN/A\n0.632 ± 0.033\n0.570 ± 0.062\n0.577 ± 0.062\n0.576 ± 0.062\n0.687 ± 0.048\n32\n0.532 ± 0.072\n0.559 ± 0.066\n0.540 ± 0.055\n0.553 ± 0.057\n0.545 ± 0.091\n0.531 ± 0.067\n0.601 ± 0.078\n0.591 ± 0.058\nN/A\n0.625 ± 0.040\n0.572 ± 0.044\n0.589 ± 0.074\n0.587 ± 0.055\n0.681 ± 0.055\n48\n0.536 ± 0.061\n0.561 ± 0.063\n0.557 ± 0.065\n0.538 ± 0.077\n0.559 ± 0.100\n0.515 ± 0.056\n0.596 ± 0.086\n0.583 ± 0.072\nN/A\n0.625 ± 0.033\n0.580 ± 0.038\n0.547 ± 0.066\n0.539 ± 0.050\n0.658 ± 0.034\nbreast-w\n4\n0.836 ± 0.066\n0.960 ± 0.021\n0.500 ± 0.000\n0.783 ± 0.220\n0.985 ± 0.009\n0.883 ± 0.137\n0.986 ± 0.010\n0.923 ± 0.040\n0.985 ± 0.010\n0.974 ± 0.012\n0.759 ± 0.041\n0.986 ± 0.008\n0.979 ± 0.013\n0.992 ± 0.006\n8\n0.856 ± 0.080\n0.974 ± 0.016\n0.832 ± 0.087\n0.887 ± 0.114\n0.983 ± 0.011\n0.927 ± 0.039\n0.985 ± 0.009\n0.926 ± 0.036\n0.983 ± 0.009\n0.978 ± 0.012\n0.768 ± 0.065\n0.987 ± 0.008\n0.981 ± 0.015\n0.993 ± 0.006\n16\n0.851 ± 0.084\n0.970 ± 0.020\n0.876 ± 0.078\n0.962 ± 0.045\n0.985 ± 0.010\n0.921 ± 0.052\n0.984 ± 0.009\n0.934 ± 0.029\n0.979 ± 0.012\n0.978 ± 0.013\n0.912 ± 0.051\n0.988 ± 0.008\n0.985 ± 0.010\n0.993 ± 0.006\n32\n0.910 ± 0.024\n0.977 ± 0.014\n0.949 ± 0.035\n0.971 ± 0.035\n0.983 ± 0.017\n0.920 ± 0.043\n0.988 ± 0.007\n0.950 ± 0.018\n0.982 ± 0.013\n0.977 ± 0.012\n0.960 ± 0.009\n0.987 ± 0.007\n0.988 ± 0.008\n0.994 ± 0.004\n48\n0.909 ± 0.048\n0.966 ± 0.032\n0.964 ± 0.030\n0.981 ± 0.014\n0.987 ± 0.010\n0.929 ± 0.040\n0.988 ± 0.006\n0.949 ± 0.018\n0.982 ± 0.009\n0.978 ± 0.012\n0.976 ± 0.013\n0.988 ± 0.006\n0.989 ± 0.006\n0.994 ± 0.006\ncultivars\n4\n0.514 ± 0.073\n0.493 ± 0.070\n0.500 ± 0.000\n0.504 ± 0.064\n0.494 ± 0.087\n0.514 ± 0.071\n0.499 ± 0.091\n0.445 ± 0.085\n0.524 ± 0.095\n0.525 ± 0.047\n0.531 ± 0.032\n0.528 ± 0.050\n0.536 ± 0.076\n0.661 ± 0.031\n8\n0.486 ± 0.063\n0.497 ± 0.062\n0.503 ± 0.062\n0.528 ± 0.075\n0.519 ± 0.095\n0.498 ± 0.090\n0.541 ± 0.069\n0.495 ± 0.056\n0.554 ± 0.075\n0.546 ± 0.061\n0.522 ± 0.066\n0.561 ± 0.058\n0.543 ± 0.073\n0.653 ± 0.025\n16\n0.495 ± 0.066\n0.523 ± 0.076\n0.510 ± 0.042\n0.530 ± 0.055\n0.527 ± 0.082\n0.519 ± 0.093\n0.511 ± 0.071\n0.491 ± 0.110\n0.584 ± 0.084\n0.531 ± 0.092\n0.563 ± 0.070\n0.582 ± 0.063\n0.576 ± 0.043\n0.679 ± 0.027\n32\n0.536 ± 0.083\n0.525 ± 0.055\n0.531 ± 0.049\n0.527 ± 0.050\n0.525 ± 0.071\n0.540 ± 0.063\n0.530 ± 0.083\n0.517 ± 0.099\n0.591 ± 0.062\n0.527 ± 0.088\n0.564 ± 0.068\n0.568 ± 0.075\n0.622 ± 0.057\n0.676 ± 0.041\n48\n0.547 ± 0.094\n0.570 ± 0.067\n0.546 ± 0.065\n0.518 ± 0.052\n0.519 ± 0.076\n0.555 ± 0.070\n0.555 ± 0.094\n0.558 ± 0.095\n0.608 ± 0.039\n0.557 ± 0.082\n0.568 ± 0.038\n0.579 ± 0.094\n0.615 ± 0.049\n0.670 ± 0.049\nNHANES\n4\n0.662 ± 0.183\n0.805 ± 0.152\n0.500 ± 0.000\n0.870 ± 0.200\n0.773 ± 0.162\n0.512 ± 0.018\n0.898 ± 0.098\n0.457 ± 0.124\n0.975 ± 0.062\n0.525 ± 0.047\n0.531 ± 0.032\n0.527 ± 0.189\n0.673 ± 0.212\n0.987 ± 0.010\n8\n0.962 ± 0.031\n0.900 ± 0.137\n0.880 ± 0.135\n0.968 ± 0.034\n0.812 ± 0.119\n0.518 ± 0.016\n0.976 ± 0.043\n0.519 ± 0.114\n0.999 ± 0.001\n0.546 ± 0.061\n0.522 ± 0.066\n0.750 ± 0.116\n0.925 ± 0.037\n0.998 ± 0.002\n16\n0.967 ± 0.023\n0.974 ± 0.020\n0.956 ± 0.039\n0.988 ± 0.011\n0.844 ± 0.116\n0.532 ± 0.061\n0.999 ± 0.001\n0.521 ± 0.058\n0.999 ± 0.002\n0.531 ± 0.092\n0.563 ± 0.070\n0.899 ± 0.082\n0.969 ± 0.018\n0.999 ± 0.001\n32\n0.988 ± 0.016\n0.990 ± 0.012\n0.989 ± 0.028\n0.987 ± 0.011\n0.835 ± 0.142\n0.535 ± 0.069\n1.000 ± 0.000\n0.792 ± 0.073\n1.000 ± 0.000\n0.527 ± 0.088\n0.564 ± 0.068\n0.960 ± 0.023\n0.959 ± 0.012\n0.999 ± 0.001\n48\n0.989 ± 0.015\n0.992 ± 0.013\n0.998 ± 0.007\n0.990 ± 0.009\n0.810 ± 0.084\n0.579 ± 0.103\n1.000 ± 0.000\n0.840 ± 0.092\n0.999 ± 0.002\n0.557 ± 0.082\n0.568 ± 0.038\n0.977 ± 0.026\n0.967 ± 0.008\n0.999 ± 0.001\ngallstone\n4\n0.559 ± 0.090\n0.564 ± 0.085\n0.500 ± 0.000\n0.493 ± 0.064\n0.505 ± 0.103\n0.509 ± 0.053\n0.570 ± 0.095\n0.440 ± 0.081\n0.465 ± 0.071\n0.565 ± 0.071\n0.533 ± 0.046\n0.540 ± 0.078\n0.471 ± 0.076\n0.658 ± 0.042\n8\n0.555 ± 0.094\n0.571 ± 0.115\n0.508 ± 0.089\n0.532 ± 0.088\n0.522 ± 0.097\n0.505 ± 0.058\n0.597 ± 0.100\n0.432 ± 0.070\n0.473 ± 0.102\n0.558 ± 0.074\n0.543 ± 0.043\n0.532 ± 0.122\n0.436 ± 0.067\n0.653 ± 0.054\n16\n0.592 ± 0.099\n0.591 ± 0.101\n0.589 ± 0.133\n0.596 ± 0.102\n0.537 ± 0.128\n0.528 ± 0.091\n0.612 ± 0.122\n0.426 ± 0.071\n0.461 ± 0.077\n0.563 ± 0.112\n0.529 ± 0.062\n0.557 ± 0.083\n0.481 ± 0.040\n0.662 ± 0.045\n32\n0.598 ± 0.071\n0.668 ± 0.084\n0.665 ± 0.086\n0.667 ± 0.093\n0.559 ± 0.074\n0.558 ± 0.076\n0.696 ± 0.103\n0.454 ± 0.077\n0.559 ± 0.053\n0.631 ± 0.079\n0.551 ± 0.040\n0.613 ± 0.077\n0.485 ± 0.074\n0.659 ± 0.045\n48\n0.663 ± 0.054\n0.725 ± 0.076\n0.740 ± 0.050\n0.711 ± 0.087\n0.598 ± 0.080\n0.556 ± 0.092\n0.793 ± 0.052\n0.436 ± 0.098\n0.548 ± 0.077\n0.628 ± 0.087\n0.546 ± 0.061\n0.661 ± 0.053\n0.499 ± 0.067\n0.663 ± 0.054\n"}, {"page": 23, "text": "Table 16: Performance comparison of traditional methods and LLM-based methods across various datasets and 4-, 8-, 16-, 32-,\nand 48-shot settings for regression tasks. The best-performing method is bolded, and the second-best is underlined. Each value\nrepresents the NRMSE score, reported as the mean ± standard deviation across 10 random seeds.\nDataset\nShot\nTraditional Method\nLLM-based Method\nTraining Required\nFine-tuned\nRequired\nNo Fine-tuning Required\nElasticNet\nMLP\nCART\nRandom\nForest\nXGBoost\nTabPFN\nTP-BERTa\nLIFT\nP2T\nOurs\nbike\n4\n0.266 ± 0.080\n0.225 ± 0.038\n0.220 ± 0.026\n0.194 ± 0.025\n0.224 ± 0.055\n0.192 ± 0.013\n0.264 ± 0.004\n0.161 ± 0.011\n0.190 ± 0.020\n0.156 ± 0.007\n8\n0.211 ± 0.065\n0.247 ± 0.082\n0.300 ± 0.086\n0.197 ± 0.024\n0.243 ± 0.121\n0.191 ± 0.028\n0.262 ± 0.004\n0.259 ± 0.058\n0.185 ± 0.013\n0.149 ± 0.007\n16\n0.202 ± 0.053\n0.256 ± 0.066\n0.304 ± 0.132\n0.200 ± 0.068\n0.193 ± 0.024\n0.165 ± 0.012\n0.261 ± 0.004\n0.156 ± 0.015\n0.194 ± 0.014\n0.144 ± 0.011\n32\n0.205 ± 0.033\n0.264 ± 0.106\n0.200 ± 0.085\n0.170 ± 0.077\n0.184 ± 0.084\n0.140 ± 0.053\n0.259 ± 0.004\n0.152 ± 0.015\n0.165 ± 0.060\n0.142 ± 0.006\n48\n0.184 ± 0.016\n0.208 ± 0.076\n0.215 ± 0.101\n0.153 ± 0.059\n0.153 ± 0.055\n0.126 ± 0.047\n0.258 ± 0.004\n0.155 ± 0.020\n0.176 ± 0.009\n0.141 ± 0.007\ncpu small\n4\n0.322 ± 0.157\n49.042 ± 82.905\n0.168 ± 0.050\n0.180 ± 0.017\n0.198 ± 0.019\n0.167 ± 0.024\n0.789 ± 0.003\n0.173 ± 0.082\n0.234 ± 0.071\n0.161 ± 0.038\n8\n0.389 ± 0.314\n9.865 ± 27.918\n0.172 ± 0.019\n0.167 ± 0.022\n0.187 ± 0.009\n0.164 ± 0.029\n0.770 ± 0.004\n0.172 ± 0.036\n0.167 ± 0.022\n0.154 ± 0.024\n16\n0.335 ± 0.437\n0.987 ± 0.375\n0.156 ± 0.067\n0.122 ± 0.040\n0.180 ± 0.037\n0.118 ± 0.039\n0.745 ± 0.004\n0.101 ± 0.045\n0.147 ± 0.036\n0.114 ± 0.027\n32\n0.234 ± 0.146\n1.674 ± 2.491\n0.101 ± 0.047\n0.098 ± 0.048\n0.151 ± 0.057\n0.114 ± 0.060\n0.720 ± 0.004\n0.098 ± 0.048\n0.101 ± 0.043\n0.116 ± 0.022\n48\n0.230 ± 0.142\n0.867 ± 0.003\n0.084 ± 0.051\n0.094 ± 0.047\n0.129 ± 0.053\n0.129 ± 0.056\n0.704 ± 0.004\n0.090 ± 0.052\n0.090 ± 0.040\n0.122 ± 0.021\ndiamonds\n4\n0.142 ± 0.063\n0.153 ± 0.038\n0.152 ± 0.020\n0.152 ± 0.021\n0.156 ± 0.019\n0.157 ± 0.078\n0.302 ± 0.002\n0.080 ± 0.009\n0.116 ± 0.021\n0.090 ± 0.010\n8\n0.169 ± 0.053\n0.116 ± 0.026\n0.150 ± 0.057\n0.118 ± 0.019\n0.129 ± 0.014\n0.153 ± 0.103\n0.302 ± 0.002\n0.080 ± 0.007\n0.109 ± 0.014\n0.095 ± 0.013\n16\n0.123 ± 0.031\n0.115 ± 0.028\n0.126 ± 0.019\n0.120 ± 0.015\n0.134 ± 0.021\n0.119 ± 0.059\n0.302 ± 0.002\n0.082 ± 0.011\n0.109 ± 0.008\n0.095 ± 0.007\n32\n0.092 ± 0.013\n0.101 ± 0.029\n0.116 ± 0.017\n0.096 ± 0.009\n0.107 ± 0.009\n0.089 ± 0.020\n0.302 ± 0.002\n0.084 ± 0.013\n0.095 ± 0.006\n0.090 ± 0.005\n48\n0.087 ± 0.009\n0.094 ± 0.037\n0.112 ± 0.017\n0.095 ± 0.010\n0.097 ± 0.010\n0.089 ± 0.035\n0.302 ± 0.002\n0.078 ± 0.011\n0.092 ± 0.006\n0.090 ± 0.005\nforest-fires\n4\n0.128 ± 0.031\n0.130 ± 0.031\n0.127 ± 0.029\n0.127 ± 0.030\n0.127 ± 0.030\n0.127 ± 0.029\n0.128 ± 0.030\n0.127 ± 0.030\n0.127 ± 0.030\n0.126 ± 0.029\n8\n0.170 ± 0.109\n0.128 ± 0.030\n0.204 ± 0.221\n0.143 ± 0.042\n0.140 ± 0.036\n0.170 ± 0.131\n0.127 ± 0.029\n0.153 ± 0.071\n0.136 ± 0.033\n0.126 ± 0.029\n16\n0.147 ± 0.051\n0.133 ± 0.034\n0.214 ± 0.192\n0.146 ± 0.046\n0.144 ± 0.044\n0.136 ± 0.037\n0.126 ± 0.029\n0.158 ± 0.070\n0.143 ± 0.045\n0.123 ± 0.029\n32\n0.127 ± 0.027\n0.132 ± 0.029\n0.162 ± 0.079\n0.139 ± 0.037\n0.128 ± 0.030\n0.125 ± 0.029\n0.126 ± 0.030\n0.149 ± 0.053\n0.121 ± 0.054\n0.122 ± 0.029\n48\n0.126 ± 0.027\n0.152 ± 0.072\n0.197 ± 0.112\n0.136 ± 0.037\n0.136 ± 0.031\n0.125 ± 0.028\n0.126 ± 0.029\n0.160 ± 0.052\n0.119 ± 0.052\n0.118 ± 0.031\nhouses\n4\n0.393 ± 0.232\n6.056 ± 2.924\n0.252 ± 0.044\n0.180 ± 0.034\n0.156 ± 0.020\n0.176 ± 0.023\n1.270 ± 0.143\n0.214 ± 0.085\n0.358 ± 0.209\n0.150 ± 0.030\n8\n0.286 ± 0.211\n6.218 ± 6.041\n0.181 ± 0.028\n0.170 ± 0.015\n0.161 ± 0.023\n0.142 ± 0.023\n0.801 ± 0.108\n0.169 ± 0.032\n0.192 ± 0.052\n0.131 ± 0.011\n16\n0.183 ± 0.037\n2.840 ± 1.398\n0.175 ± 0.025\n0.150 ± 0.012\n0.156 ± 0.015\n0.131 ± 0.012\n0.285 ± 0.059\n0.148 ± 0.013\n0.163 ± 0.018\n0.128 ± 0.011\n32\n0.134 ± 0.024\n3.275 ± 4.016\n0.174 ± 0.025\n0.135 ± 0.017\n0.142 ± 0.020\n0.112 ± 0.013\n0.172 ± 0.014\n0.153 ± 0.021\n0.149 ± 0.014\n0.127 ± 0.011\n48\n0.121 ± 0.018\n2.081 ± 2.099\n0.162 ± 0.022\n0.131 ± 0.012\n0.131 ± 0.016\n0.104 ± 0.012\n0.172 ± 0.014\n0.156 ± 0.025\n0.141 ± 0.011\n0.126 ± 0.011\ninsurance\n4\n0.240 ± 0.044\n0.235 ± 0.056\n0.282 ± 0.070\n0.236 ± 0.035\n0.236 ± 0.027\n0.194 ± 0.041\n0.325 ± 0.039\n0.125 ± 0.018\n0.186 ± 0.055\n0.115 ± 0.021\n8\n0.224 ± 0.045\n0.207 ± 0.023\n0.295 ± 0.126\n0.255 ± 0.065\n0.245 ± 0.042\n0.163 ± 0.038\n0.325 ± 0.039\n0.116 ± 0.020\n0.145 ± 0.026\n0.107 ± 0.014\n16\n0.185 ± 0.017\n0.193 ± 0.012\n0.230 ± 0.049\n0.232 ± 0.028\n0.234 ± 0.031\n0.158 ± 0.048\n0.325 ± 0.039\n0.116 ± 0.017\n0.119 ± 0.044\n0.105 ± 0.012\n32\n0.156 ± 0.023\n0.158 ± 0.021\n0.181 ± 0.086\n0.212 ± 0.079\n0.207 ± 0.077\n0.117 ± 0.045\n0.325 ± 0.039\n0.112 ± 0.015\n0.119 ± 0.013\n0.100 ± 0.010\n48\n0.153 ± 0.020\n0.162 ± 0.030\n0.222 ± 0.056\n0.236 ± 0.030\n0.233 ± 0.037\n0.122 ± 0.017\n0.325 ± 0.039\n0.112 ± 0.016\n0.114 ± 0.042\n0.102 ± 0.014\nplasma retinol\n4\n0.490 ± 0.329\n0.396 ± 0.112\n0.292 ± 0.100\n0.239 ± 0.049\n0.215 ± 0.028\n0.227 ± 0.046\n0.633 ± 0.140\n0.331 ± 0.112\n0.412 ± 0.163\n0.216 ± 0.038\n8\n0.388 ± 0.101\n0.472 ± 0.248\n0.304 ± 0.097\n0.240 ± 0.063\n0.234 ± 0.048\n0.242 ± 0.060\n0.631 ± 0.140\n0.340 ± 0.108\n0.316 ± 0.076\n0.203 ± 0.028\n16\n0.301 ± 0.171\n0.445 ± 0.371\n0.296 ± 0.080\n0.217 ± 0.038\n0.220 ± 0.025\n0.203 ± 0.023\n0.628 ± 0.139\n0.340 ± 0.129\n0.268 ± 0.064\n0.192 ± 0.018\n32\n0.315 ± 0.281\n0.439 ± 0.410\n0.307 ± 0.084\n0.231 ± 0.040\n0.215 ± 0.032\n0.203 ± 0.024\n0.626 ± 0.138\n0.327 ± 0.079\n0.270 ± 0.053\n0.193 ± 0.017\n48\n0.216 ± 0.031\n0.389 ± 0.268\n0.287 ± 0.083\n0.214 ± 0.025\n0.210 ± 0.029\n0.202 ± 0.025\n0.624 ± 0.138\n0.309 ± 0.051\n0.268 ± 0.053\n0.193 ± 0.019\nwine\n4\n0.206 ± 0.065\n0.361 ± 0.185\n0.209 ± 0.030\n0.170 ± 0.027\n0.169 ± 0.030\n0.166 ± 0.029\n0.150 ± 0.005\n0.164 ± 0.023\n0.252 ± 0.068\n0.141 ± 0.010\n8\n0.193 ± 0.042\n0.228 ± 0.077\n0.188 ± 0.033\n0.167 ± 0.032\n0.161 ± 0.024\n0.153 ± 0.023\n0.146 ± 0.003\n0.158 ± 0.010\n0.183 ± 0.026\n0.137 ± 0.006\n16\n0.178 ± 0.032\n0.207 ± 0.060\n0.187 ± 0.021\n0.156 ± 0.018\n0.158 ± 0.016\n0.155 ± 0.023\n0.148 ± 0.004\n0.158 ± 0.011\n0.161 ± 0.018\n0.132 ± 0.002\n32\n0.157 ± 0.022\n0.170 ± 0.024\n0.186 ± 0.009\n0.146 ± 0.007\n0.149 ± 0.009\n0.137 ± 0.006\n0.154 ± 0.012\n0.158 ± 0.011\n0.135 ± 0.047\n0.131 ± 0.002\n48\n0.143 ± 0.010\n0.161 ± 0.016\n0.178 ± 0.017\n0.141 ± 0.006\n0.141 ± 0.006\n0.135 ± 0.006\n0.151 ± 0.009\n0.159 ± 0.007\n0.133 ± 0.047\n0.131 ± 0.003\ncultivars\n4\n0.325 ± 0.143\n0.314 ± 0.056\n0.268 ± 0.056\n0.234 ± 0.024\n0.218 ± 0.023\n0.300 ± 0.240\n1.403 ± 0.151\n0.370 ± 0.131\n0.328 ± 0.103\n0.206 ± 0.021\n8\n0.369 ± 0.221\n0.330 ± 0.097\n0.263 ± 0.039\n0.235 ± 0.049\n0.224 ± 0.039\n0.221 ± 0.040\n1.402 ± 0.151\n0.321 ± 0.121\n0.288 ± 0.085\n0.196 ± 0.017\n16\n0.224 ± 0.034\n0.369 ± 0.284\n0.277 ± 0.042\n0.234 ± 0.035\n0.231 ± 0.041\n0.217 ± 0.032\n1.400 ± 0.151\n0.270 ± 0.074\n0.266 ± 0.077\n0.198 ± 0.023\n32\n0.214 ± 0.025\n0.248 ± 0.043\n0.273 ± 0.041\n0.216 ± 0.021\n0.223 ± 0.025\n0.211 ± 0.024\n1.398 ± 0.151\n0.266 ± 0.065\n0.259 ± 0.086\n0.193 ± 0.019\n48\n0.209 ± 0.023\n0.227 ± 0.029\n0.260 ± 0.039\n0.212 ± 0.025\n0.212 ± 0.021\n0.206 ± 0.023\n1.396 ± 0.151\n0.267 ± 0.062\n0.272 ± 0.099\n0.191 ± 0.022\ninfrared thermography\ntemperature\n4\n0.152 ± 0.025\n0.288 ± 0.148\n0.130 ± 0.025\n0.116 ± 0.018\n0.151 ± 0.021\n0.138 ± 0.057\n6.628 ± 0.707\n0.129 ± 0.017\n1.637 ± 1.137\n0.108 ± 0.012\n8\n0.132 ± 0.042\n0.138 ± 0.049\n0.129 ± 0.016\n0.109 ± 0.022\n0.160 ± 0.049\n0.110 ± 0.024\n5.898 ± 0.628\n0.105 ± 0.013\n0.722 ± 1.062\n0.102 ± 0.010\n16\n0.120 ± 0.048\n0.140 ± 0.049\n0.117 ± 0.017\n0.099 ± 0.012\n0.109 ± 0.016\n0.112 ± 0.058\n5.036 ± 0.536\n0.098 ± 0.011\n0.106 ± 0.014\n0.103 ± 0.008\n32\n0.109 ± 0.085\n0.121 ± 0.037\n0.103 ± 0.012\n0.085 ± 0.012\n0.093 ± 0.009\n0.093 ± 0.046\n3.516 ± 0.375\n0.095 ± 0.014\n0.096 ± 0.011\n0.097 ± 0.007\n48\n0.079 ± 0.006\n0.437 ± 0.619\n0.103 ± 0.015\n0.084 ± 0.010\n0.088 ± 0.009\n0.078 ± 0.015\n1.842 ± 0.678\n0.096 ± 0.012\n0.140 ± 0.152\n0.096 ± 0.009\n"}]}