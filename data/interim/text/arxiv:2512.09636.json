{"doc_id": "arxiv:2512.09636", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.09636.pdf", "meta": {"doc_id": "arxiv:2512.09636", "source": "arxiv", "arxiv_id": "2512.09636", "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment", "authors": ["Mengxi Xiao", "Kailai Yang", "Pengde Zhao", "Enze Zhang", "Ziyan Kuang", "Zhiwei Liu", "Weiguang Han", "Shu Liao", "Lianting Huang", "Jinpeng Hu", "Min Peng", "Qianqian Xie", "Sophia Ananiadou"], "published": "2025-12-10T13:26:22Z", "updated": "2025-12-16T10:08:59Z", "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.09636v2", "url_pdf": "https://arxiv.org/pdf/2512.09636.pdf", "meta_path": "data/raw/arxiv/meta/2512.09636.json", "sha256": "bbb33432eecb7714e4d5b6a6779f32a883d633f082457b8f2a81a42d3c4f60f6", "status": "ok", "fetched_at": "2026-02-18T02:24:31.673469+00:00"}, "pages": [{"page": 1, "text": "MentraSuite: Post-Training Large Language Models for Mental\nHealth Reasoning and Assessment\nMengxi Xiao\nSchool of Artificial Intelligence,\nWuhan University;\nCenter for Language and Information\nResearch, Wuhan University\nChina\nKailai Yang\nThe University of Manchester\nUnited Kingdom\nPengde Zhao\nSchool of Computer Science, Wuhan\nUniversity\nChina\nEnze Zhang\nSchool of Artificial Intelligence,\nWuhan University;\nCenter for Language and Information\nResearch, Wuhan University\nChina\nZiyan Kuang\nCenter for Language and Information\nResearch, Wuhan University\nChina\nZhiwei Liu\nThe University of Manchester\nUnited Kingdom\nWeiguang Han\nSchool of Computer Science, Wuhan\nUniversity\nChina\nShu Liao\nCenter for Language and Information\nResearch, Wuhan University\nChina\nLianting Huang\nMount Holyoke College\nUnited States\nJinpeng Hu\nHefei University of Technology\nChina\nMin Peng\nSchool of Artificial Intelligence,\nWuhan University;\nCenter for Language and Information\nResearch, Wuhan University\nChina\npengm@whu.edu.cn\nQianqian Xieâˆ—\nSchool of Artificial Intelligence,\nWuhan University;\nCenter for Language and Information\nResearch, Wuhan University\nChina\nxieq@whu.edu.cn\nSophia Ananiadou\nThe University of Manchester\nUnited Kingdom\nsophia.ananiadou@manchester.ac.uk\nAbstract\nMental health disorders affect hundreds of millions globally, and\nthe Web now serves as a primary medium for accessing support,\ninformation, and assessment. Large language models (LLMs) offer\nscalable and accessible assistance, yet their deployment in mental-\nhealth settings remains risky when their reasoning is incomplete,\nâˆ—Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full cita-\ntion on the first page. Copyrights for components of this work owned by others than\nthe author(s) must be honored. Abstracting with credit is permitted. To copy other-\nwise, or republish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nConference acronym â€™XX, Woodstock, NY\nÂ© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/2018/06\nhttps://doi.org/XXXXXXX.XXXXXXX\ninconsistent, or ungrounded. Existing psychological LLMs empha-\nsize emotional understanding or knowledge recall but overlook the\nstep-wise, clinically aligned reasoning required for appraisal, diag-\nnosis, intervention planning, abstraction, and verification. To ad-\ndress these issues, we introduce MentraSuite, a unified framework\nfor advancing reliable mental-health reasoning. We propose Men-\ntraBench, a comprehensive benchmark spanning five core reason-\ning aspects, six tasks, and 13 datasets, evaluating both task per-\nformance and reasoning quality across five dimensions: concise-\nness, coherence, hallucination avoidance, task understanding, and\ninternal consistency. We further present Mindora, a post-trained\nmodel optimized through a hybrid SFTâ€“RL framework with an\ninconsistency-detection reward to enforce faithful and coherent\nreasoning. To support training, we construct high-quality trajec-\ntories using a novel reasoning trajectory generation strategy, that\narXiv:2512.09636v2  [cs.CL]  16 Dec 2025\n"}, {"page": 2, "text": "Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nMengxi Xiao et al.\nstrategically filters difficult samples and applies a structured, consistency-\noriented rewriting process to produce concise, readable, and well-\nbalanced trajectories. Across 20 evaluated LLMs, Mindora achieves\nthe highest average performance on MentraBench and shows re-\nmarkable performances in reasoning reliability, demonstrating its\neffectiveness for complex mental-health scenarios.\nCCS Concepts\nâ€¢ Computing methodologies â†’Causal reasoning and diag-\nnostics; Natural language generation.\nKeywords\nMental Health Reasoning, Large Language Models, Post-training\nACM Reference Format:\nMengxi Xiao, Kailai Yang, Pengde Zhao, Enze Zhang, Ziyan Kuang, Zhi-\nwei Liu, Weiguang Han, Shu Liao, Lianting Huang, Jinpeng Hu, Min Peng,\nQianqian Xie, and Sophia Ananiadou. 2018. MentraSuite: Post-Training\nLarge Language Models for Mental Health Reasoning and Assessment. In\nProceedings of Make sure to enter the correct conference title from your rights\nconfirmation email (Conference acronym â€™XX). ACM, New York, NY, USA,\n12 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nMental health disorders affect hundreds of millions of people world-\nwide and remain a leading contributor to disability, social inequal-\nity, and unmet clinical needs [9, 21, 33]. As individuals increas-\ningly turn to online platforms for information, support, and self-\nassessment, the Web has become a critical medium for expanding\naccess to mental health care and advancing social good. Artificial\nintelligence (AI) plays a growing role in this shift: assisting indi-\nviduals understand their conditions [30, 36, 39], supporting coun-\nselors in developing treatment strategies [23, 34], and helping clin-\nicians in decision-making [35]. Recently, large language models\n(LLMs) [11, 18] have been rapidly adopted across web-based men-\ntal health applications, due to their strong linguistic capabilities,\nbroad world knowledge, and ability to engage in natural, empa-\nthetic dialogue. Their potential impact is considerable: LLMs can\nprovide scalable, always-available guidance and reach populations\nthat traditional services often fail to serve.\nHowever, deploying LLMs in such sensitive, high-stakes settings\nrequires more than producing fluent responses or superficially ac-\ncurate predictions. Effective mental health support depends on trans-\nparent, coherent, and context-grounded reasoning that reflects how\nhuman clinicians interpret complex, subjective narratives. When\nLLMs misread user self-reports, rely on incomplete reasoning, or\naccept subjective statements as factual, they may exaggerate symp-\ntoms, provide misleading feedback, or inadvertently amplify usersâ€™\nanxiety [27]. At scale, these risks threaten public trust and can un-\ndermine the social-good promise of AI-enabled mental health sup-\nport systems. These risks underscore the urgent need for LLMs ca-\npable of responsible, clinically aligned reasoning rather than merely\ngenerating plausible answers [10, 17].\nRecent efforts [4, 8, 40], have begun exploring how LLMs can\nbetter support mental-health tasks. Such as, Psyche-R1 [4] jointly\nintegrates empathy, psychological knowledge, and chain-of-thought\nreasoning through a large-scale synthesis pipeline and hybrid GRPO-\nSFT training. Psy-Interpreter [8] enhances implicit mental-state in-\nference using expert-annotated scenarios and a trajectory-aware\nreinforcement learning framework that imitates clinician-like rea-\nsoning. PsychCounsel-Bench [40] assesses whether LLMs meet coun-\nseling knowledge standards, finding that only frontier models sur-\npass certification-level performance [31].\nDespite these advances, current methods still have important\nlimitations in both method design and evaluation as showin in Ta-\nble 1. Most existing approaches focus on emotional understanding,\nknowledge tests, or supervised reasoning tailored to a narrow set\nof tasks, without systematically modeling core stages of clinical\nreasoning. Yet effective mental-health support requires reasoning\nacross several interconnected processes: appraisal (recognizing\nmaladaptive thought patterns), diagnosis (identifying likely con-\nditions), intervention (selecting appropriate therapeutic strate-\ngies), abstraction (synthesizing structured evidence), and verifi-\ncation (detecting inaccurate or misleading mental-health informa-\ntion). Furthermore, existing methods focus primarily on task accu-\nracy while overlooking the quality and reliability of the reasoning\nprocess. Reliable mental-health support requires LLMs to produce\ntransparent, coherent, and context-grounded reasoning across sev-\neral key dimensions: reasoning conciseness (avoiding unneces-\nsary complexity or repetition), logical coherence (providing step-\nwise, case-specific justification), hallucination avoidance (not\nintroducing unsupported facts), task understanding (following\nthe intended objective without drift), and internal consistency\n(maintaining non-contradictory reasoning throughout). Address-\ning these aspects is essential for assessing and developing LLMs\nthat can perform the step-wise, integrative reasoning that under-\nlies appraisal, diagnosis, intervention, abstraction, and verification\nin real mental-health practice.\nTo fill this gap, we present MentraSuite1, a unified suite of bench-\nmarks, datasets, and models for advancing reliable mental-health\nreasoning. We introduce MentraBench, a comprehensive bench-\nmark that evaluates five essential aspects of clinical and counsel-\ning cognition: appraisal, diagnosis, intervention, abstraction, and\nverification. MentraBench spans six tasks and 13 datasets, built\nby refining existing resources and constructing new ones. Unlike\nprior benchmarks focused primarily on accuracy, MentraBench\nemphasizes the quality of reasoning trajectories, assessing five key\ndimensions: reasoning conciseness, logical coherence, hallucina-\ntion avoidance, task understanding, and internal consistency. We\nthen introduce Mindora, a post-trained model optimized for di-\nverse mental-health reasoning tasks and more reliable reasoning\nprocesses. Mindora adopts a novel hybrid supervised fine tuning-\nreinforcement learning (SFTâ€“RL) training strategy with an LLM-\nbased inconsistency-detection reward that dynamically enforces\ninternal consistency while enhancing reasoning depth and gener-\nalization to unseen cases. To support Mindoraâ€™s training, we con-\nstruct high-quality reasoning data through a Reasoning Trajectory\nGeneration (RTG) strategy. RTG filters samples by difficulty and\napplies a structured rewriting procedure that produces concise, read-\nable, and well-balanced reasoning trajectories, directly mitigating\nissues such as over-elaboration and improving reasoning clarity.\n1The code and data are available in MentraSuite.\n"}, {"page": 3, "text": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nDiagnosis\nDetermine what mental condition is likely.\nIt's kinda always been an issue. I wouldn't say it's as \nbad as my peers but it's definitely bad. There's a lot \nthat I obviously can't write cause it would take a while \nbut I'll just say the grand scheme of things. So basically \nI'm not really happy. I haven't been happy...\nQ: What is the depression severity (0/1/2)?\nAppraisal\nIdentify what cognitive pattern is present.\nThis year, one of my best friends has gone through a lot \nof issue with his other friends. He really feels like he has \nno one left besides me. All he ever does is work or stay \nat home. He rarely comes to church anymore...\nQ: The most likely cognitive error is? <choices>\nCognitive Error \nIdentification\nCognitiveReframing\nPatternReframe\nTherapist Q&A\nMental Health \nCondition Detection\nDepSign\nSWMH\nT-SID\nIntervention\nDecide what to do therapeutically.\nMrs. Smith has recently returned home after a period \naway, likely due to health challenges, and is confronted \nwith mixed emotions about her current situation. She \nexpresses happiness about being home...\nQ: select appropriate counseling strategies. <choices>\nCounseling Strategy \nFormulation\nPsyDTCorpusM\nAnnoMIM\nAbstraction\nDetermine what the evidence shows.\nTitle: Family-based interventions versus standard care \nfor people with schizophrenia\nAbstract: ... shifting from high to low expressed emotion \n(RR 3.90, 95% CI 1.11 to 13.71; 2 RCTs, 72 participants... \nQ: The main findings of the systematic review is?\nPsychiatry \nSystematic Review \nSummarization\nPSRS*\nVerification\nDetermine whether the mental-health information is accurate.\nVideo title: waves of depression #depression \n#mentalhealth  #mentalillness  #shorts  #youtube \nAudio transcript:  wanted to be famous, just being real...\nQ: whether the video contains mental health \nmisinformation? \nMental Health \nMisinfomation \nIdentification\nMentalMisinfo\nPsychiatry QA\n... Her vital signs are within normal limits. On mental \nstatus examination, she is oriented to person, place, and \ntime. She reports feeling anxious. Physical examination \nshows no abnormalities. \nQ: The most appropriate next step is? <choices>   \nFigure 1: Tasks and datasets included in MentraBench.\nTable 1: Comparison of MentraBench with existing mental-health reasoning works.\nStatistics\nTask Choice\nEvaluation Aspects\nTraining Strategy\nTask\nDataset\nAppraisal\nDiagnosis\nIntervention\nMulti-step\nAbstraction\nVerification\nCorrectness\nReasoning Chain\nAbility\nReward\nCoT Construction\nPsyche-R1 [4]\n3\n4\n\u0000Ã—\nÃ—\n\u0000Ã—\nÃ—\n\u0000Ã—\nempathy, logic\nformat, correctness\nprompt-rationale optimization\nPsy-Interpreter [8]\n3\n6\nÃ—\nÃ—\nÃ—\nÃ—\nÃ—\nÃ—\n\u0000Ã—\nknowledge\nbilateral reward\nknowledge injection\nPsychCounsel-Bench [40]\n1\n1\nÃ—\nÃ—\nÃ—\nÃ—\nÃ—\nÃ—\n\u0000Ã—\n\\\n\\\n\\\nMentraBench (ours)\n6\n13\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000logic, interpretability, consistency\nformat, length, correctness, consistency\nstructured, generation-verifier refinement\nExtensive experiments on MentraBench covering 20 evaluated\nLLMs, show that Mindora achieves the highest average performance\nacross all 13 datasets, outperforming strong baselines such as GPT-\n4o-mini and DeepSeek-R1. Trajectory-level analysis further con-\nfirms remarkable performance across all five reasoning dimensions,\ndemonstrating Mindoraâ€™s superior ability to reason concisely, ac-\ncurately, and coherently in complex mental-health scenarios.\nIn summary, our contribution can be summarized as follows:\n(1) We present MentraBench, the first comprehensive bench-\nmark designed to evaluate LLMsâ€™ reasoning abilities across five\nkey aspects of mental-health practice: appraisal, diagnosis, inter-\nvention, abstraction, and verification. It includes 6 tasks, and 13\ndatasets, emphasizing both task accuracy and reasoning quality.\n(2) We develop Mindora, a post-trained model that combines su-\npervised fine-tuning and reinforcement learning with a novel llm\nbased consistency detection rewards to enhance reasoning concise-\nness, consistency, and factual grounding.\n(3) We propose the Reasoning Trajectory Generation strategy,\nwhich produces structured and concise reasoning data through\ndifficulty filtering and structured transformation, improving inter-\npretability and mitigating reasoning redundancy.\n(4) Extensive experiments demonstrate Mindoraâ€™s superior per-\nformance across task performance and all reasoning dimensions,\nsurpassing state-of-the-art models such as GPT-4o-mini and DeepSeek-\nR1.\n2\nMethods\n2.1\nMentraBench\nMental health reasoning requires a range of cognitive and decision-\nmaking skills, from recognizing distorted thought patterns to inter-\npreting clinical evidence and selecting interventions. To systemat-\nically evaluate these capabilities in LLMs, we design MentraBench\naround six complementary dimensions. Appraisal tests cognitive-\npattern reasoning, identifying subtle maladaptive thought processes.\nDiagnosis assesses condition classification, determining likely men-\ntal health problems from client data. Intervention evaluates therapeutic-\nplanning skills, generating appropriate counseling strategies. Multi-\nstep probes multi-step reasoning, integrating knowledge, diagno-\nsis, and intervention in complex scenarios. Abstraction examines\nevidence synthesis, summarizing findings from structured research\nor experimental data. Verification challenges models to discern\naccurate from misleading mental health information. Each dimen-\nsion is instantiated through a representative task paired with cor-\nresponding datasets. Figure 1 provides an overview of tasks and\ndatasets included in MentraBench, and Table 2 summarizes key\ndataset statistics. MentraBench comprises six tasks and 13 datasets,\nconstructed through refinements of existing resources and the cre-\nation of new ones. In Table 2, datasets marked with ğ‘€indicate\nthose processed in this work, while those marked with âˆ—denote\nnewly curated and annotated datasets.\n2.1.1\nAppraisal: Cognitive-Pattern Reasoning. The Appraisal dimen-\nsion evaluates an LLMâ€™s ability to identify what cognitive error is\n"}, {"page": 4, "text": "Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nMengxi Xiao et al.\nTable 2: Dataset statistics.\nDataset\nTrain/Valid/Test\nSource\nAnnotation\nMetric\nCognitive Error Identification\nCognitiveReframing [28]\n751/173/284\nsynthetic, MHA\nHuman\nMicroF1\nPatternReframe [15]\n1302/661/300\nsynthetic\nHuman\nMicroF1\nTherapist Q&A [29]\n1662/409/507\ncounseling dialogues (real)\nHuman\nMicroF1\nMental Health Condition Detection\nDepSign [22]\n1198/300/600\nReddit\nweak supervision\nMicroF1\nSWMH [12]\n1200/300/500\nReddit\nweak supervision\nMicroF1\nT-SID [12]\n1200/300/942\nTwitter\nweak supervision\nMicroF1\nCounseling Strategy Formulation\nPsyDTCorpusğ‘€\n1200/300/-\ncounseling dialogues (syn)\nHuman\nJaccard\nAnnoMIğ‘€\n-/-/133\ncounseling dialogues (real)\nHuman\nJaccard\nPsychiatry QA\nMHQA [25]\n1200/300/717\nPubMed\nHuman/LLM\nMicroF1\nMedQAğ‘€\n464/58/121\nMedical exams\nHuman\nMicroF1\nMedMCQAğ‘€\n318/120/446\nMedical exams\nHuman\nMicroF1\nPubMedQAğ‘€\n-/-/89\nPubMed\nHuman\nMicroF1\nPsychiatry Systematic Review Summarization\nPSRS*\n-/-/108\nCochrane Library\nHuman\nRecall\nMental Health Misinformation Identification\nMentalMisinfo [16]\n123/31/130\nYoutube, Bitchute\nHuman\nMacroF1\nNote: Datasets with ğ‘€are processed in this work. Dataset with * is newly curated and anno-\ntated in this work, where the â€recallâ€ metric means the coverage of annotated scoring points.\npresent in a clientâ€™s self-reported statement. This dimension tar-\ngets fine-grained cognitive-pattern reasoning, requiring the model\nto recognize subtle distorted appraisal cues and distinguish among\nsimilar forms of maladaptive thinking. We instantiate this dimen-\nsion through the cognitive error identification task, evaluated us-\ning three high-quality datasets that span more than a dozen cognitive-\ndistortion categories. CognitiveReframing [28] combines simulated\nnegative thoughts from the Thought Records Dataset [2] and self-\nreports from the Mental Health America website2, with distortions\nannotated by 15 trained mental-health professionals. PatternRe-\nframe [15], constructed from PERSONA-CHAT personas [41], con-\ntains statements crafted to manifest specific distortions and labeled\nby five independent raters. Therapist Q&A [29], derived from real\ntherapistâ€“client interactions in the Therapist Q&A corpus3, pro-\nvides naturally occurring distorted statements annotated by two\nclinical raters. These three datasets adopt slightly different tax-\nonomies of cognitive errors. In our prompt design, we follow each\ndatasetâ€™s original definitions and examples. The cognitive-error cat-\negories shared across all three datasets include: All-or-Nothing Think-\ning, Overgeneralization, Labeling, Fortune Telling, Mind Reading,\nShould Statements, and Personalization. Unique categories in Cog-\nnitiveReframing include: Emotional Reasoning, Comparing and De-\nspairing, Blaming, Negative Feeling or Emotion, Catastrophizing,\nand Discounting the Positive. Unique categories in PatternReframe\ninclude: Mental Filtering, Catastrophizing, and Discounting the Pos-\nitive. Unique categories in Therapist Q&A include: Emotional Rea-\nsoning, Mental Filtering, Magnification, and No Distortion.\n2.1.2\nDiagnosis: Mental-Condition Reasoning. The Diagnosis dimen-\nsion assesses an LLMâ€™s ability to determine what mental condition\nis likely given a clientâ€™s textual expression. This type of reason-\ning requires the model to make calibrated, clinically aligned judg-\nments about potential psychological problems, distinguishing be-\ntween similar symptom presentations, avoiding overpathologizing,\nand assessing severity with nuance. We instantiate this dimension\n2https://screening.mhanational.org/\n3https://www.kaggle.com/arnmaud/therapist-qa\nthrough the mental health condition detection task, which eval-\nuates whether a model can identify and classify possible mental\nhealth issues from real-world social-media posts. To benchmark di-\nagnostic performance, we employ one depression-specific dataset,\nDepSign [22], and two multi-condition mental-health classification\ndatasets collected from different platforms: SWMH [12] from Red-\ndit and T-SID [12] from Twitter.\n2.1.3\nIntervention: Therapeutic-Action Reasoning. The Intervention\ndimension evaluates an LLMâ€™s ability to determine what counsel-\ning action should be taken in response to a clientâ€™s situation. This\ndimension targets therapeutic-action reasoning, requiring the model\nto analyze the clientâ€™s presentation and select the intervention strat-\negy that is most contextually appropriate, rather than offering generic,\nmisplaced, or logically inconsistent responses. We instantiate this\ndimension through the counseling-strategy formulation task, cov-\nering thirteen commonly taught intervention types.4\nTo construct evaluation data, we use two high-quality counsel-\ning dialogue corpora. Client utterances are compressed into con-\ncise case summaries using GPT-4o prompts, and counselor utter-\nances are annotated to extract strategy labels as reference answers.\nAll constructed items are manually reviewed and verified by an\nexpert counselor with over ten years of clinical experience, result-\ning in the final [dataset]ğ‘€versions. PsyDTCorpusğ‘€: Based on\nPsyDTCorpus [37], which contains 5,000 high-quality single-turn\ndialogues from SoulChatCorpus and 12 anonymized real counsel-\ning cases synthesized into multi-turn interactions. Due to its par-\ntially synthetic nature, this dataset is used to train Mindora but\nexcluded from benchmark evaluation. AnnoMIğ‘€: Based on An-\nnoMI [34], constructed from authorized motivational interviewing\n(MI) demonstration videos sourced from YouTube and Vimeo, tran-\nscribed and curated into high-quality multi-turn counseling inter-\nactions.\n2.1.4\nMulti-step: Multi-Step Clinical Reasoning. The Multi-step di-\nmension assesses whether an LLM can perform multi-step clini-\ncal reasoning across appraisal, diagnosis, and intervention. Mod-\nels must integrate symptom interpretation, condition identifica-\ntion, treatment selection, and research-level analysis within a sin-\ngle pipeline, mirroring the sequential decision-making process of\nmental-health professionals. We instantiate this dimension through\na psychiatry QA task and evaluate it using four psychiatry-focused\ndatasets. MHQA [25] provides knowledge-intensive question-answer\npairs from 471k PubMed abstracts covering major mental-health\nconditions. From MedQAğ‘’ğ‘›[13], MedMCQA [20], and PubMedQA [14],\nwe manually extracted psychiatric-related questions to form the\n[ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡]ğ‘€version, ensuring that they test their diagnostic, inter-\nvention, and evidence-based reasoning abilities. Together, these\ndatasets comprehensively benchmark an LLMâ€™s capacity for end-\nto-end, multi-step clinical reasoning grounded in both practice and\nresearch evidence.\n4The strategies include Clarification, Paraphrasing, Reflection of Feeling, Summariz-\ning, Questioning Skills, Immediacy, Use of Silence, Self-Disclosure, Confrontation, En-\ncouragement, Repetition, Interpretation, and Guidance.\n"}, {"page": 5, "text": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\n2.1.5\nAbstraction: Evidence-Based Reasoning. The Abstraction di-\nmension evaluates whether an LLM can determine what the ev-\nidence shows by interpreting and summarizing complex psychi-\natric research reports. This task targets free-text evidence reason-\ning, requiring models to process long, highly structured system-\natic review abstracts, extract key numerical and methodological\ninformation, and convert it into clinically meaningful conclusions,\nincluding effect direction and certainty levels. We instantiate this\ndimension through the Psychiatry Systematic Review Summariza-\ntion (PSRS) dataset. Each case in PSRS was manually curated from\nthe Cochrane Library [26] to cover a broad spectrum of psychiatric\nconditions, populations, and intervention types, and expert anno-\ntators generated scoring points for every instance to capture the\nmain findings.\nIn this task, LLMs are required to summarize the main results\nreported in the studiesâ€”turning quantitative outcomes, confidence\nintervals, and methodological notes into concise, interpretable con-\nclusions. This involves synthesizing statistical evidence into clini-\ncal interpretation, inferring effect direction, and assessing the cer-\ntainty of evidence, rather than merely reproducing numerical re-\nsults. All abstracts are publicly available and retain study identi-\nfiers to ensure traceability; no patient-level or individual trial data\nare included.\n2.1.6\nVerification: Misinformation Detection. The Verification di-\nmension evaluates whether an LLM can determine whether mental-\nhealth information is accurate. This task targets misinformation de-\ntection reasoning, requiring models to identify misleading, anecdo-\ntal, or non-evidence-based claims in user-generated content while\nrelying on authoritative clinical knowledge. We instantiate this di-\nmension through the Mental Health Misinformation Identification\ntask, where models must distinguish reliable mental-health infor-\nmation from potentially harmful narratives in textual scripts de-\nrived from social-media videos.\nWe evaluate this task using the MentalMisinfo dataset [16], which\ncontains video content from platforms such as YouTube Shorts and\nBitChute. The materials were systematically filtered, transcribed,\nand manually annotated to indicate whether each statement is ac-\ncurate or misleading. By including this task, our benchmark as-\nsesses an LLMâ€™s ability to perform evidence-based verification un-\nder real-world, noisy, and informal language conditions commonly\nencountered in online mental-health discourse.\n2.2\nMindora\nTo enable more reliable and clinically aligned mental-health rea-\nsoning, we propose Mindora, a post-trained large language model\ndesigned to improve both reasoning depth and reasoning fidelity.\nMindora builds on a hybrid SFTâ€“RL training framework that inte-\ngrates high-quality reasoning trajectories with reinforcement sig-\nnals targeting consistency.\n2.2.1\nReasoning Trajectory Generation. To support Mindoraâ€™s train-\ning with high-quality supervision signals, we propose a reason-\ning trajectory generation strategy that focuses on genuinely chal-\nlenging reasoning steps rather than trivial pattern completion. Al-\nthough the tasks and datasets described above are demonstrably\nreasoning-intensive, directly using all training samples would di-\nlute the supervision with many instances that models can already\nsolve through surface-level cues. To obtain training data with suf-\nficient difficulty, we perform zero-shot question answering on the\ntraining split using Llama-3-8B-Instruct and retain only the cases\nwhere the model produces incorrect answers. This filtering proce-\ndure ensures that the collected trajectories focus on problems that\nrequire deeper reasoning and are more informative for model train-\ning. The number of retained training cases is shown in Table 2.\nTo address the readability and structural incoherence of reason-\ning trajectories caused by backtracking in iterative search, we pro-\npose a structured reasoning trajectory generation method inspired\nby the search-based complex reasoning framework [3]. The core\nworkflow involves two key stages: iterative optimal path search\nguided by a verifier and structured formatting of the optimal tra-\njectory, ensuring both reasoning depth and interpretability.\n2.2.2\nIterative Optimal Reasoning Path Search. We first leverage\nGPT-4o to explore and refine reasoning trajectories for verifiable\nmental health problems, following a feedback-driven iterative search\nparadigm:\nReasoning Generation. For a given verifiable problem ğ‘¥with\nground-truth answer ğ‘¦âˆ—, GPT-4o generates an initial Chain-of-Thought\n(CoT), denoted as ğ‘’0, and preliminary answer ğ‘¦0 by analyzing the\ncase context and applying domain knowledge.\nVerifier-Guided Refinement. A medical verifier implemented us-\ning GPT-4o checks if ğ‘¦0 aligns with ğ‘¦âˆ—. If the verification returns\nFalse, GPT-4o samples a search strategy to refine the trajectory:\n(1) Backtracking: Revisits earlier reasoning steps ğ‘’ğ‘—(ğ‘—< ğ‘–âˆ’1) to\nidentify and resolve logical flaws. (2) Exploring New Paths: Devel-\nops an alternative reasoning approach distinct from prior attempts\nğ‘’0, ..., ğ‘’ğ‘–âˆ’1. (3) Verification: Validates the logical consistency and fac-\ntual accuracy of the current trajectory ğ‘’ğ‘–âˆ’1. (4) Correction: Directly\namends errors in the latest reasoning step ğ‘’ğ‘–âˆ’1 to align with domain\nprinciples.\nTermination Conditions. The iteration continues until the ver-\nifier confirms the answer is correct. If the maximum number of\niterations ğ‘= 3 is reached without a correct answer, the search\nrestarts for up to ğ‘‡= 3 attempts and all failed trajectories are\ndiscarded.\nThis process ensures that the final trajectory [ğ‘’0, ğ‘¦0, ..., ğ‘’ğ‘–, ğ‘¦ğ‘–] em-\nbodies iterative reflection and optimal reasoning, while avoiding\nstagnation in suboptimal paths.\n2.2.3\nStructured Reasoning Formats. To mitigate readability degra-\ndation caused by backtracking and unstructured reflection, the op-\ntimal reasoning trajectory is formatted into a standardized struc-\nture with two mandatory phases. This formatting enforces clarity,\nconsistency, and alignment between reasoning and answers, pro-\nhibiting deviations from the predefined schema.\nIn the reasoning phase: (1) All analytical content is enclosed\nwithin <think> tags. (2) Structured subtitles (e.g., ###Symptom Anal-\nysis, ###Differential Diagnosis) are used to segment reasoning steps,\neach on a separate line. (3) The phase concludes with a mandatory\n###Final Conclusion section that summarizes the core logical chain\nand justifies the subsequent answer.\nIn the answer phase: (1) The final judgment is enclosed within\n<answer> tags. (2) The phase strictly ends with the format Answer:\n"}, {"page": 6, "text": "Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nMengxi Xiao et al.\nMindora\nCase: A 60-year-old man is found roaming aimlessly by a \nshopkeeper and brought him to the hospital. He was unable \nto tell his address. The blood investigation assay revealed: \nNa+115 mEq/L, K+ 3.2 mEq/L. \nQ: Which of the following is the most likely diagnosis? \n<choices>  a. Multi-infarct dementia b. Delirium \nc. Alzheimer's disease d. Dissociative fugue\n?\nLLaMA3.1-8B\nfilter and retain only \nthe hard cases where \nLLaMA fails\ntraining data\n<think>\n###Analyzing Symptoms and Investigations: ...\n###Differential Diagnosis of Confusion:...\n###Assessing Impact of Electrolyte Imbalance: ...\n###Final Conclusion: The most likely diagnosis for \nthe patient's acute confusion due to severe \nhyponatremia is delirium. This conclusion aligns with \nthe symptom profile and the effects of the \nelectrolyte imbalance.\n</think>\n<answer>Answer: b</answer>\nGPT-4o\nAgent-based \nreasoning trajectory \ngeneration\nInner \nThinking\nInner \nThinking\nFinal \nConclusion\nInner \nThinking\nFinal \nConclusion\nretained cases (       )\nrollout BRL from current policy (K=8 candidates)\nQwen3-32B\nGRPO\nsampling a batch           from \ncompute\nSFT\n* SFT and RL are executed jointly in each training step, enabling dynamic loss fusion \ninstead of a two-stage SFTâ†’RL procedure.\njoint training\nsampling a batch        from \nafter 463 iterations ...\nÌŒ\niteration\ntrajectory \nstructuring\nFigure 2: The framework of Mindora.\n[option/result] to ensure unambiguous output. (3) The answer must\nbe logically consistent with the conclusion derived in the Reason-\ning Phase.\nAn example of structured trajectory is shown in Figure 2.\n2.2.4\nRationale for Structural Constraints. The mandatory format-\nting rules address several critical limitations of unstructured tra-\njectories, specifically enhancing logical coherence, ensuring con-\nsistency and improving readability.\nEnhancing Logical Coherence. Unstructured trajectories often\ncontain fragmented backtracking, such as Wait, earlier I forgot to\ncheck symptom duration, Let me revisit that which disrupts logical\nflow. By segmenting reasoning into titled modules and isolating\nbacktracking within iterative search rather than the final output,\nthe structured trajectory maintains a linear and coherent chain of\nlogic.\nEnsuring Consistency. The mandatory ###Final Conclusion and\nAnswer: [result] elements ensure that the answer directly reflects\nthe reasoning process. This aims to avoiding inconsistencies, such\nas a conclusion favoring Major Depressive Disorder while the an-\nswer lists Generalized Anxiety Disorder. This alignment is crucial\nfor training models to produce logically grounded outputs in mental-\nhealth tasks, where misalignment could lead to clinical misjudg-\nments.\nImproving Interpretability. Structured subtitles (e.g., ###Differ-\nential Diagnosis Exclusion) teach the model to decompose complex\nmental health judgments into domain-specific sub-tasksâ€”mirroring\nhow clinicians systematically analyze cases. This modular learning\nimproves the modelâ€™s ability to replicate interpretable, professional\nreasoning patterns.\n2.2.5\nTraining Procedure. The training of the mental health rea-\nsoning model adheres to the CHORD [42] algorithmâ€™s core para-\ndigm of dynamic balancing between sft and rl exploration, with\na formalized reward mechanism to ensure the quality and valid-\nity of model outputs. The training framework integrates dual data\nstreams, adaptive weight scheduling, and a multi-criteria reward\nfunction, as formally detailed below.\nTraining Framework and Notations. Let â„³denote the base\nmodel (Qwen3-8B), â„³aux represent the auxiliary model (Qwen3-\n32B) for internal consistency detection, and ğ’ŸSFT âŠ‚ğ’³Ã— ğ’´denote\nthe expert SFT dataset where ğ’³is the set of mental health reason-\ning prompts and ğ’´is the set of expert solutions. Let ğ’ŸRL âŠ‚ğ’³Ã— ğ’´\nbe the RL exploration dataset generated by â„³during rollout. The\ntraining objective is to optimize the policy ğœ‹ğœƒ(parameterized by ğœƒ)\nvia the CHORD algorithm, which dynamically fuses SFT loss â„’SFT\nand RL loss â„’RL using two-level weights: global weight ğœ‡(ğ‘¡) and\ntoken-level weight ğœ™(â‹…).\nThe reward ğ‘Ÿ(ğ‘ , ğ‘) for an action ğ‘(model solution) given a state\nğ‘ (input prompt) is a composite function consisting of four sequen-\ntial validity and quality checks, formally defined as:\nğ‘Ÿ(ğ‘ , ğ‘) = ğ•€(FormatValid(ğ‘)) â‹…ğ•€(LengthValid(ğ‘)) â‹…ğ•€(Consistency(ğ‘)) â‹…ğ‘„Quality(ğ‘)\n(1)\nwhere: ğ•€(â‹…) is the indicator function (1 if the condition holds, 0\notherwise). In detail, FormatValid(ğ‘) verifies if ğ‘adheres to the\nmandatory format <think>...</think><answer>...</answer>.\nLengthValid(ğ‘) ensures the token length of the inner thinking tra-\njectory ğ’¯in ğ‘falls within a valid range [ğ¿min, ğ¿max], where ğ¿min =\n10 tokens and ğ¿max = 2048 tokens. Consistency(ğ‘) detects factual\ninconsistencies or errors in ğ’¯using the auxiliary model â„³aux.\nğ‘„Quality(ğ‘) quantifies the correctness of ğ‘based on task-specific\nbenchmark criteria, with values in [0, 1]. It is defined separately\nfor three task types:\n(1) Single-choice questions: Let ğ‘¦âˆ—be the ground-truth answer.\nThen:\nğ‘„Quality(ğ‘) = {1\nif the final conclusion in ğ’¯= ğ‘¦âˆ—,\n0\notherwise.\n(2)\n(2) Multiple-choice questions: Let ğ‘Œâˆ—= {ğ‘¦âˆ—1 , ğ‘¦âˆ—2 , ..., ğ‘¦âˆ—\nğ‘˜} be the\nset of ground-truth answers, and ğ‘Œ= {ğ‘¦1, ğ‘¦2, ..., ğ‘¦ğ‘š} be the set of\n"}, {"page": 7, "text": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nanswers in ğ’¯. The quality score is the Jaccard similarity between\nğ‘Œand ğ‘Œâˆ—:\nğ‘„Quality(ğ‘) = |ğ‘Œâˆ©ğ‘Œâˆ—|\n|ğ‘Œâˆªğ‘Œâˆ—| .\n(3)\n(3) Short-answer questions: Let ğ’¦= {ğ‘˜1, ğ‘˜2, ..., ğ‘˜ğ‘›} be the set of\nkey scoring points for the task, and ğ’¦hit âŠ†ğ’¦be the subset of\npoints covered in ğ’¯. Then:\nğ‘„Quality(ğ‘) = |ğ’¦hit|\n|ğ’¦| .\n(4)\nTraining Pipeline. The training process proceeds in iterations ğ‘¡=\n1, 2, ..., ğ‘‡max (where ğ‘‡max is the total number of training steps), with\neach iteration consisting of data sampling, weight scheduling, loss\ncomputation, and parameter update stages.\nStep 1: Data Sampling. At each step ğ‘¡, we sample two mini-batches:\nAn SFT mini-batch â„¬SFT âˆ¼ğ’ŸSFT with batch size ğµSFT = 64 (con-\nsistent with the experimental setup), where each sample is (ğ‘¥ğ‘–, ğ‘¦âˆ—\nğ‘–) âˆˆ\nğ’³Ã—ğ’´( ğ‘¥ğ‘–is the prompt, ğ‘¦âˆ—\nğ‘–is the expert solution). An RL mini-batch\nâ„¬RL âˆ¼ğ’ŸRL with dynamic batch size ğµRL, where each sample is\n(ğ‘¥ğ‘—, ğ‘ğ‘—) âˆˆğ’³Ã— ğ’´( ğ‘ğ‘—is the model-generated solution via rollout).\nThe rollout for ğ’ŸRL generation uses a temperature ğœ= 1.0, with\nğ¾= 8 candidate solutions sampled per prompt.\nStep 2: Adaptive Weight Scheduling. The global weight ğœ‡(ğ‘¡)\n(balancing â„’SFT and â„’RL) follows a warmup-decay schedule to\ntransition from expert imitation to RL exploration:\n(1) Warmup phase (1 â‰¤ğ‘¡â‰¤ğ‘¡warmup, ğ‘¡warmup = 200 steps):\nğœ‡(ğ‘¡) = ğœ‡valley + (ğœ‡peak âˆ’ğœ‡valley) â‹…\nğ‘¡\nğ‘¡warmup\n,\n(5)\nwhere ğœ‡peak = 0.5 (maximum SFT influence) and ğœ‡valley = 0.02\n(minimum SFT influence).\n(2) Decay phase (ğ‘¡warmup < ğ‘¡â‰¤ğ‘¡warmup + ğ‘¡decay, ğ‘¡decay = 400\nsteps):\nğœ‡(ğ‘¡) = ğœ‡peak âˆ’(ğœ‡peak âˆ’ğœ‡valley) â‹…\nğ‘¡âˆ’ğ‘¡warmup\nğ‘¡decay\n.\n(6)\nThe token-wise weight ğœ™(ğ‘¦âˆ—ğ‘¡; ğœ‹ğœƒ) (for SFT loss modulation) is\ndefined based on the policyâ€™s probability of generating the expert\ntoken ğ‘¦âˆ—ğ‘¡(given prompt ğ‘¥and prefix ğ‘¦âˆ—<ğ‘¡):\nğœ™(ğ‘¦âˆ—ğ‘¡; ğœ‹ğœƒ) = ğ‘ğ‘¡(1 âˆ’ğ‘ğ‘¡),\n(7)\nwhere ğ‘ğ‘¡= ğœ‹ğœƒ(ğ‘¦âˆ—ğ‘¡|ğ‘¥, ğ‘¦âˆ—<ğ‘¡). This parabolic function prioritizes learn-\ning for tokens where the policy is uncertain ( ğ‘ğ‘¡â‰ˆ0.5 ) while\ndownweighting certain ( ğ‘ğ‘¡â‰ˆ1 ) or irrelevant ( ğ‘ğ‘¡â‰ˆ0 ) tokens.\nStep 3: Loss Computation and Parameter Update. The total\nloss â„’total(ğœƒ) is a weighted combination of the SFT loss (with token-\nwise weighting) and the RL loss:\nâ„’total(ğœƒ) = (1 âˆ’ğœ‡(ğ‘¡)) â‹…â„’GRPO(ğœƒ) + ğœ‡(ğ‘¡) â‹…â„’SFTâˆ’ğœ™(ğœƒ),\n(8)\nSFT loss with token-wise weighting â„’SFTâˆ’ğœ™(ğœƒ) is computed over\nâ„¬SFT, minimizing the weighted negative log-likelihood of expert\nsolutions:\nâ„’SFTâˆ’ğœ™(ğœƒ)\n= âˆ’\n1\nâˆ‘(ğ‘¥ğ‘–,ğ‘¦âˆ—ğ‘–)âˆˆâ„¬SFT |ğ‘¦âˆ—ğ‘–|\nâˆ‘\n(ğ‘¥ğ‘–,ğ‘¦âˆ—ğ‘–)âˆˆâ„¬SFT\n|ğ‘¦âˆ—ğ‘–|\nâˆ‘\nğ‘¡=1\nğœ™(ğ‘¦âˆ—ğ‘–,ğ‘¡; ğœ‹ğœƒ) â‹…log ğœ‹ğœƒ(ğ‘¦âˆ—ğ‘–,ğ‘¡|ğ‘¥ğ‘–, ğ‘¦âˆ—ğ‘–,<ğ‘¡).\n(9)\nGRPO loss â„’GRPO(ğœƒ) is optimized over â„¬RL to maximize the\nexpected reward, using a clipped surrogate objective (consistent\nwith PPO-style updates):\nâ„’GRPO(ğœƒ)\n= âˆ’\n1\nâˆ‘(ğ‘¥ğ‘—,ğ‘ğ‘—)âˆˆâ„¬RL |ğ‘ğ‘—|\nâˆ‘\n(ğ‘¥ğ‘—,ğ‘ğ‘—)âˆˆâ„¬RL\n|ğ‘ğ‘—|\nâˆ‘\nğ‘¡=1\nmin (ğ‘Ÿğ‘—,ğ‘¡(ğœƒ)ğ´ğ‘—, clip(ğ‘Ÿğ‘—,ğ‘¡(ğœƒ), 1 âˆ’ğœ–, 1 + ğœ–)ğ´ğ‘—) ,\n(10)\nwhere: ğ‘Ÿğ‘—,ğ‘¡(ğœƒ) =\nğœ‹ğœƒ(ğ‘ğ‘—,ğ‘¡|ğ‘¥ğ‘—,ğ‘ğ‘—,<ğ‘¡)\nğœ‹sample(ğ‘ğ‘—,ğ‘¡|ğ‘¥ğ‘—,ğ‘ğ‘—,<ğ‘¡) (importance sampling ratio, ğœ‹sample\nis the reference policy), ğ´ğ‘—= ğ‘Ÿ(ğ‘¥ğ‘—,ğ‘ğ‘—)âˆ’ğœ‡ğ‘…\nğœğ‘…+ğœ–ğ‘§\n(normalized advantage,\nğœ‡ğ‘…/ğœğ‘…are the mean/std of rewards in the rollout group, ğœ–ğ‘§= 10âˆ’8\nfor stability), ğœ–= 0.2. The policy parameters ğœƒare updated by\nminimizing â„’total(ğœƒ) using the Adam optimizer with ğ›½1 = 0.9,\nğ›½2 = 0.999, and learning rate ğœ‚= 2Ã—10âˆ’6. Training checkpointing\nis performed every 10 steps to preserve intermediate model states.\n3\nExperiments\n3.1\nExperimental Settings\nEvaluated LLMs. To assess whether model scale, distilled rea-\nsoning variants, and the gap between reasoning-oriented and chat-\noriented LLMs affect performance on mental health reasoning tasks,\nwe evaluated a broad range of GPT, DeepSeek, Qwen, and LLaMA\nmodels. For closed-source systems, we included the reasoning mod-\nels GPT-o (o4-mini), DeepSeek-R (DeepSeek-R1), and Qwenâ€™s QwQ\nseries (QwQ-plus), as well as the leading chat models GPT-4o, DeepSeek-\nV3, and Qwen-plus5. For open-source models, we tested Qwen,\nLLaMA, and DeepSeek-distilled variants across multiple scales, with\nthe full list provided in Table 3. For psychology-focused LLMs, we\nevaluated EmoLLM6 , finetuned from Qwen2-7B-Instruct as a well-\nknown mental-health chat model, and Psyche-R17 , an SFT+GRPO-\ntrained Qwen2.5-7B-Instruct model representing the latest mental-\nhealth-reasoning LLMs.\nEvaluation Settings. All closed-source LLMs are accessed through\ntheir official APIs, while open-source models are deployed on a sin-\ngle NVIDIA A800-SXM4-80GB GPU. Parameters such as tempera-\nture are kept at their default values. Prompts are aligned with the\nMindora format to generate structured reasoning chains for con-\nsistent comparison.\n3.2\nMain Results\nIn this benchmark, we evaluate the reasoning performance of vari-\nous models across 13 datasets covering five core aspects of mental-\nhealth practice. The results, presented in Table 3, highlight several\nkey findings.\nIn general, Mindorağ¶ğ»ğ‘‚ğ‘…ğ·achieves the highest average score\nacross all datasets, followed by Mindorağ‘†ğ¹ğ‘‡+ ğ‘…ğ¿, both surpass-\ning leading proprietary reasoning models such as GPT-o4-mini and\nDeepSeek-R1. This demonstrates the effectiveness of our post-training\nstrategy in enhancing reasoning performance within complex, context-\nsensitive mental-health tasks. Across every dataset, Mindorağ¶ğ»ğ‘‚ğ‘…ğ·\noutperform the backbone model Qwen3-8B, showing consistent\ngains in both accuracy and reasoning quality. Furthermore, Mindorağ¶ğ»ğ‘‚ğ‘…ğ·\nalso exceeds the separately trained Mindorağ‘†ğ¹ğ‘‡and Mindorağ‘†ğ¹ğ‘‡+ğ‘…ğ¿\nvariants, confirming that the joint SFTâ€“RL training paradigm more\neffectively balances imitation and exploration, avoiding overfitting\nwhile improving generalization to unseen cases.\nPerformance analysis across the five task categories reveals that\nMindoraCHORD demonstrates strong appraisal and diagnostic rea-\nsoning, showing improved recognition of subtle cognitive patterns\nand accurate differentiation of overlapping symptom presentations.\n5Qwen-plus is Qwenâ€™s versatile chat model, also trained with reasoning capabilities.\n6https://www.modelscope.cn/models/aJupyter/EmoLLM_Qwen2-7B-Instruct_lora/.\n7https://huggingface.co/MindIntLab/Psyche-R1\n"}, {"page": 8, "text": "Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nMengxi Xiao et al.\nIt also achieves notable gains in intervention and abstraction tasks,\nreflecting enhanced multi-stage reasoning and evidence synthesis.\nThe modelâ€™s consistent results in verification tasks further suggest\nreliable factual grounding and resistance to misinformation.\nWe also observe several interesting phenomena. Among open-\nsource LLMs, different versions of the same model (e.g., DSDistill-\nQwen3-32B, Qwen3-32B, and QwQ-32B) show only minor differ-\nences in overall reasoning performance, regardless of whether they\nare distilled, chat, or reasoning variants. This suggests that mental-\nhealth reasoning tasks demand specialized reasoning abilities that\ngeneral-purpose post-training cannot fully capture, underscoring\nthe need for targeted reasoning optimization for mental health sce-\nnarios. Moreover, we find that open-source models ranging from\n14B to 70B parameters achieve similar average scores around 0.6,\nwhile 8B-scale models remain near 0.55. In contrast, our Mindora\nseries and the baseline Psyche-R1, both specifically optimized for\nmental-health reasoning, exceed the average performance of 8B\nmodels, demonstrating the strong potential of targeted post-training\nin this domain.\n3.3\nReasoning Trajectory Evaluation\nIn this section, we evaluate the reasoning trajectory quality of best\nperforming models in MentraBench from each major family (GPT,\nDeepSeek, Qwen, and LLaMA), the mental-health-oriented Psyche-\nR1, our backbone Qwen3-8B, and our proposed Mindorağ¶ğ»ğ‘‚ğ‘…ğ·.\nTo assess reasoning quality beyond task accuracy, we conduct a\ndetailed reasoning trajectory evaluation based on five criteria: rea-\nsoning conciseness, logical coherence, hallucination, task under-\nstanding, and internal consistency. Each reasoning chain is manu-\nally evaluated following a binary guideline, where a score of 1 is\nassigned if no errors are observed and 0 otherwise, with detailed\nscoring criteria shown in Appendix A. The final reasoning trajec-\ntory score is computed as the average across all five dimensions,\nproviding a comprehensive measure of reasoning reliability and\ntransparency in mental-health tasks.\nFor each model and each dataset, we sample four representa-\ntive cases, two where all models produce correct answers and two\nwhere all models fail, to ensure fairness in comparison.\nTable 4 shows that Mindorağ¶ğ»ğ‘‚ğ‘…ğ·achieves remarkable aver-\nage trajectory score, demonstrating balanced performance across\nmultiple evaluation dimensions. Through refined training and bal-\nanced optimization, our model attains the best overall correctness\nand interpretability, highlighting its strength in both reasoning ac-\ncuracy and clarity.\nFor redundancy and backtracking related dimensions, Mindorağ¶ğ»ğ‘‚ğ‘…ğ·\nshows substantial improvement over the backbone model Qwen3-\n8B. This confirms the effectiveness of the structured trajectory gen-\neration step in our training data, which enables the model to pro-\nduce reasoning chains that are more organized, concise, and logi-\ncally coherent.\n3.4\nCase Study\nIn this section, we analyze a challenging case of cognitive error\nidentification shown in Appendix Figure 3, where all compared\nmodels produced incorrect answers. In this case, the client expresses\nthe thought Am I insane? after experiencing the situation of feel-\ning watched by others. The correct reasoning requires recognizing\nthat the cognitive error lies in the thought itself, specifically, la-\nbeling, as the client directly labels themselves as insane. However,\nmost models mistakenly focused on the situation rather than the\nthought, interpreting the error as treating feelings as facts due to\nthe clientâ€™s perception of being watched. Only our model correctly\nconcentrated on the thought itself, avoiding confusion between the\nexternal situation and the internal self-labeling process.\n4\nRelated Work\nRecent efforts have begun to explore psychological reasoning and\ncounseling intelligence in LLMs. Psyche-R1 [4] represents a ma-\njor step toward domain-specific psychological LLMs that combine\nempathy, expertise, and reasoning. It introduces a synthetic bench-\nmark and a hybrid training pipeline that mixes supervised fine-\ntuning on easier samples with reinforcement learning on filtered\nhard cases, improving interpretive reasoning and emotional under-\nstanding. Psy-Interpreter [8] further advances psychological and\nsocial-cognitive reasoning through the StimuliQA dataset, composed\nof expert-annotated narrative stimuli capturing emotions and col-\nlective cognition. Its bilateral reinforcement-learning design aligns\nmodel trajectories with expert reasoning patterns, enhancing inter-\npretive and social-cognitive generalization. PsychCounsel-Bench\n[40] complements these modeling efforts by constructing a 2,200-\nquestion benchmark derived from counselor-certification exams\ncovering counseling methods, abnormal and developmental psy-\nchology, and ethics. Evaluations on leading models (e.g., GPT-4o,\nLlama 3.3-70B, Gemma 3-27B) suggest that current LLMs can al-\nready master exam-level psychological knowledge. Despite their\ncontributions, existing works largely emphasize emotional under-\nstanding, social inference, or theoretical knowledge, leaving deeper\nreasoning processes underexplored. Psyche-R1 and Psy-Interpreter\nenhance empathy and emotion-related reasoning but do not assess\nmulti-stage chains that integrate appraisal, diagnosis, and interven-\ntion. PsychCounsel-Bench evaluates professional knowledge but\nis limited to exam-style multiple-choice questions, testing recall\nrather than open-ended, context-dependent reasoning. As a result,\nkey abilities:such as synthesizing conflicting evidence, distinguish-\ning overlapping symptoms, generating context-appropriate inter-\nventions, and verifying factual accuracy, remain unmeasured.\n5\nConclusion\nIn this work, we introduced MentraSuite, comprising the Mentra-\nBench benchmark and the Mindora model, to systematically ad-\nvance and evaluate mental-health reasoning. Unlike prior studies\nthat focus primarily on emotional understanding or knowledge-\nbased assessment, our benchmark targets five clinically grounded\nreasoning aspects to capture the multi-stage and context-sensitive\nnature of real mental-health practice: appraisal, diagnosis, inter-\nvention, abstraction, and verification. We further developed a struc-\ntured Reasoning Trajectory Generation method and the post-trained\nmodel Mindora, which integrates supervised fine-tuning and re-\ninforcement learning with consistency-aware reward design. Ex-\nperimental results demonstrate that Mindoraâ€™s overall reasoning\nperformance surpasses strong baselines such as GPT-4o-mini and\n"}, {"page": 9, "text": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nTable 3: Experimental results on MentraBench.\nModel\nCognitiveReframing\nPatternReframe\nTherapist Q&A\nğ´ğ‘£ğ‘”1\nDepSign\nSWMH\nT-SID\nğ´ğ‘£ğ‘”2\nAnnoMIğ‘€\nMHQA\nMedQAğ‘€\nMedMCQAğ‘€\nPubMedQAğ‘€\nğ´ğ‘£ğ‘”4\nPSRS\nMentalMisinfo\nğ´ğ‘£ğ‘”ğ‘ğ‘™ğ‘™\nclose-source models\nGPT-o4-mini[19]\n0.7002\n0.6607\n0.4468\n0.6026\n0.3607\n0.7374\n0.7515\n0.6165\n0.2086\n0.5462\n0.8925\n0.8102\n0.7118\n0.7402\n0.8650\n0.7781\n0.6515\nGPT-4o[18]\n0.6791\n0.6667\n0.4543\n0.6000\n0.4064\n0.7500\n0.7114\n0.6226\n0.3358\n0.4531\n0.6138\n0.7936\n0.6949\n0.6389\n0.9065\n0.6894\n0.6273\nDeepseek-R1[6]\n0.7516\n0.7069\n0.4472\n0.6352\n0.4085\n0.7624\n0.7218\n0.6309\n0.2020\n0.4984\n0.8608\n0.8988\n0.7256\n0.7459\n0.9037\n0.5689\n0.6505\nDeepseek-V3[7]\n0.6540\n0.6755\n0.4131\n0.5809\n0.4211\n0.7715\n0.7581\n0.6502\n0.2140\n0.4768\n0.8184\n0.8696\n0.6848\n0.7124\n0.9296\n0.6156\n0.6386\nQwen-plus[38]\n0.6821\n0.6395\n0.4057\n0.5758\n0.4043\n0.7624\n0.7662\n0.6443\n0.1866\n0.4795\n0.8242\n0.8895\n0.7199\n0.7283\n0.9048\n0.6018\n0.6387\nQwQ-plus[24]\n0.6698\n0.6871\n0.3698\n0.5756\n0.4085\n0.7547\n0.7330\n0.6321\n0.1848\n0.4201\n0.6405\n0.8354\n0.7226\n0.6546\n0.9335\n0.4838\n0.6034\n70B+ open-source models\nLLaMA-4[1]\n0.6588\n0.6486\n0.4031\n0.5702\n0.3914\n0.7310\n0.7884\n0.6369\n0.2066\n0.4684\n0.7414\n0.8900\n0.7239\n0.7059\n0.7634\n0.6158\n0.6178\nLLaMA-3.3-70B[11]\n0.6444\n0.6301\n0.3566\n0.5437\n0.4148\n0.7163\n0.7157\n0.6156\n0.2023\n0.4148\n0.7712\n0.8904\n0.7299\n0.7016\n0.6318\n0.6894\n0.6006\ndsdistill-LLaMA-70B[6]\n0.6667\n0.6667\n0.3905\n0.5746\n0.4000\n0.7516\n0.6953\n0.6156\n0.2367\n0.4404\n0.7097\n0.7957\n0.7452\n0.6727\n0.8914\n0.6814\n0.6209\nQwen2.5-72B[24]\n0.6852\n0.6207\n0.3930\n0.5663\n0.3806\n0.7639\n0.8136\n0.6527\n0.2796\n0.4022\n0.7449\n0.8402\n0.7229\n0.6775\n0.9555\n0.6091\n0.6316\n32B open-source models\ndsdistill-Qwen32B[6]\n0.6540\n0.6696\n0.3539\n0.5592\n0.3979\n0.7358\n0.7373\n0.6237\n0.2178\n0.4010\n0.6697\n0.7795\n0.7229\n0.6433\n0.9225\n0.6177\n0.6061\nQwen3-32B[38]\n0.6247\n0.6395\n0.4255\n0.5632\n0.3806\n0.7685\n0.8038\n0.6510\n0.2016\n0.4355\n0.7415\n0.8206\n0.7083\n0.6765\n0.9333\n0.5025\n0.6143\nQwQ-32B[24]\n0.6791\n0.6842\n0.3905\n0.5846\n0.4085\n0.7500\n0.7296\n0.6294\n0.1749\n0.4305\n0.6831\n0.8470\n0.7375\n0.6745\n0.9296\n0.4642\n0.6084\n14B open-source models\ndsdistill-Qwen14B[6]\n0.6635\n0.5981\n0.3404\n0.5340\n0.4169\n0.7421\n0.7750\n0.6447\n0.2470\n0.3823\n0.6198\n0.6660\n0.7497\n0.6045\n0.8641\n0.5795\n0.5880\nQwen3-14B[38]\n0.6540\n0.6486\n0.4820\n0.5949\n0.3871\n0.7484\n0.7540\n0.6298\n0.1680\n0.4215\n0.7047\n0.8102\n0.6882\n0.6562\n0.8155\n0.6600\n0.6109\n7âˆ¼8B open-source models\nLLaMA3.1-8B[11]\n0.5871\n0.5714\n0.1957\n0.4514\n0.4476\n0.7358\n0.7581\n0.6472\n0.2147\n0.2771\n0.5791\n0.7925\n0.7096\n0.5896\n0.6805\n0.6703\n0.5553\ndsdistill-LLaMA-8B[6]\n0.5906\n0.5507\n0.2396\n0.4603\n0.4190\n0.7294\n0.7490\n0.6325\n0.2001\n0.3321\n0.4430\n0.6070\n0.7587\n0.5352\n0.8157\n0.7617\n0.5536\nEmoLLM[32]\n0.6180\n0.5472\n0.3322\n0.4991\n0.4334\n0.7229\n0.7931\n0.6498\n0.1408\n0.3373\n0.4311\n0.5749\n0.6735\n0.5042\n0.7906\n0.6346\n0.5407\nPsyche-R1[5]\n0.5013\n0.5222\n0.2021\n0.4085\n0.4169\n0.6877\n0.7962\n0.6336\n0.2424\n0.5872\n0.6989\n0.8401\n0.7194\n0.7114\n0.8164\n0.6954\n0.5943\nQwen3-8B[38]\n0.5941\n0.6395\n0.4057\n0.5464\n0.4169\n0.7358\n0.7515\n0.6347\n0.1489\n0.4009\n0.5824\n0.7927\n0.6899\n0.6165\n0.8115\n0.4773\n0.5729\nMindorağ‘†ğ¹ğ‘‡\n0.5693\n0.6207\n0.4472\n0.5457\n0.4887\n0.7437\n0.7597\n0.6640\n0.4193\n0.5830\n0.7263\n0.8535\n0.7376\n0.7251\n0.7764\n0.5512\n0.6367\nMindorağ‘†ğ¹ğ‘‡+ğ‘…ğ¿\n0.5975\n0.6207\n0.4566\n0.5583\n0.5240\n0.7715\n0.8140\n0.7032\n0.3803\n0.5996\n0.7839\n0.8593\n0.8212\n0.7660\n0.7159\n0.5681\n0.6548\nMindorağ¶ğ»ğ‘‚ğ‘…ğ·\n0.7293\n0.6842\n0.5088\n0.6408\n0.4655\n0.7760\n0.8030\n0.6815\n0.4016\n0.6317\n0.7590\n0.8535\n0.8442\n0.7721\n0.8379\n0.7178\n0.6933\nTable 4: Reasoning trajectory evaluation.\nModel\nR1\nR2\nR3\nR4\nR5\nRğ‘ğ‘£ğ‘”\nAnnotation Scores\nGPT-o4-mini\n1.0000\n0.4135\n1.0000\n0.9519\n1.0000\n0.8731\nDeepseek-R1\n0.9519\n0.9808\n1.0000\n0.9808\n1.0000\n0.9827\nQwen-plus\n0.6538\n1.0000\n1.0000\n0.9808\n1.0000\n0.9270\nLLaMA-4\n1.0000\n0.2596\n1.0000\n0.8942\n0.9423\n0.8192\nPsyche-R1\n0.9423\n0.9038\n1.0000\n0.9712\n0.9038\n0.9442\nQwen3-8B\n0.5769\n0.9712\n1.0000\n0.9808\n0.9904\n0.9039\nMindorağ¶ğ»ğ‘‚ğ‘…ğ·\n0.9423\n0.9519\n1.0000\n0.9904\n0.9808\n0.9731\nInner-Annotator Agreement\nGwet AC1\n0.9607\n0.8918\n1.0000\n0.9705\n0.9855\n0.9617\nCohenâ€™s Kappa\n0.8692\n0.7906\n1.0000\n0.6033\n0.7298\n0.7986\nConsistency\n0.9698\n0.9286\n1.0000\n0.9725\n0.9863\n0.9714\nNote: R1: Reasoning Conciseness; R2: Logical Coherence; R3: No Hallucination; R4:\nTask Understanding; R5: Internal Consistency.\nDeepSeek-R1, achieving balanced performance across datasets and\nsuperior reasoning-chain quality.\nBeyond performance improvements, our findings highlight the\nimportance of transparent, coherent, and context-grounded rea-\nsoning in clinical applications. Structured trajectory data and tar-\ngeted post-training effectively reduce reasoning redundancy, en-\nhance internal consistency, and improve interpretability, which\nare key steps toward reliable AI-assisted mental-health assessment.\nWe believe this work provides a solid foundation for studying how\nreasoning-oriented alignment can enable LLMs to assist in clinical\ndecision-making responsibly and ethically, aligning with the Web\nfor Good vision of developing AI systems that serve human well-\nbeing with trustworthiness and social value.\nAcknowledgments\nThis work is partially supported by Key Project of the National Nat-\nural Science Foundation of China (U23A20316), and CCF-Tencent\nRhino-Bird Open Research Fund (CCF-Tencent RAGR20250115).\nReferences\n[1] Meta AI. 2025. Introducing LLaMA 4: Advancing Multimodal Intelligence. https:\n//ai.meta.com/blog/llama-4-multimodal-intelligence/\n[2] Franziska Burger, Mark A Neerincx, and Willem-Paul Brinkman. 2021. Natu-\nral language processing for cognitive therapy: extracting schemas from thought\nrecords. PloS one 16, 10 (2021), e0257832.\n[3] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng\nWang, Jianye Hou, and Benyou Wang. 2024. HuatuoGPT-o1, Towards Medical\nComplex Reasoning with LLMs. arXiv:2412.18925 [cs.CL] https://arxiv.org/abs/\n2412.18925\n[4] Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, and Meng\nWang. 2025. Psyche-R1: Towards Reliable Psychological LLMs through Unified\nEmpathy, Expertise, and Reasoning. arXiv preprint arXiv:2508.10848 (2025).\n[5] Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, and Meng\nWang. 2025. Psyche-R1: Towards Reliable Psychological LLMs through Unified\nEmpathy, Expertise, and Reasoning. arXiv:2508.10848 [cs.CL] https://arxiv.org/\nabs/2508.10848\n[6] DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs\nvia Reinforcement Learning.\narXiv:2501.12948 [cs.CL] https://arxiv.org/abs/\n2501.12948\n[7] DeepSeek-AI. 2025. DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs.CL]\nhttps://arxiv.org/abs/2412.19437\n[8] Yichao Feng. 2025. From Stimuli to Minds: Enhancing Psychological Reasoning\nin LLMs via Bilateral Reinforcement Learning. arXiv preprint arXiv:2508.02458\n(2025).\n[9] GBD 2019 Mental Disorders Collaborators. 2022. Global, regional, and national\nburden of 12 mental disorders in 204 countries and territories, 1990â€“2019: a sys-\ntematic analysis for the Global Burden of Disease Study 2019. The Lancet Psy-\nchiatry 9, 2 (Feb. 2022), 137â€“150. doi:10.1016/S2215-0366(21)00395-3 Epub 2022\nJan 10.\n[10] Declan Grabb, Max Lamparth, and Nina Vasan. 2024. Risks from Language Mod-\nels for Automated Mental Healthcare: Ethics and Structure for Implementation.\narXiv:2406.11852 [cs.CY] https://arxiv.org/abs/2406.11852\n[11] Aaron Grattafiori, Abhimanyu Dubey, and et al Abhinav Jauhri. 2024. The Llama\n3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783\n[12] Shaoxiong Ji, Xue Li, Zi Huang, and Erik Cambria. 2022. Suicidal ideation and\nmental disorder detection with attentive relation networks. Neural Computing\nand Applications 34, 13 (2022), 10309â€“10319.\n[13] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter\nSzolovits. 2020.\nWhat Disease does this Patient Have? A Large-scale Open\n"}, {"page": 10, "text": "Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nMengxi Xiao et al.\nDomain Question Answering Dataset from Medical Exams.\narXiv preprint\narXiv:2009.13081 (2020).\n[14] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.\n2019. PubMedQA: A Dataset for Biomedical Research Question Answering. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP). 2567â€“2577.\n[15] Mounica Maddela, Megan Ung, Jing Xu, Andrea Madotto, Heather Foran, and\nY-Lan Boureau. 2023. Training Models to Generate, Recognize, and Reframe\nUnhelpful Thoughts. In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.).\nAssociation for Computational Linguistics, 13641â€“13660. doi:10.18653/V1/2023.\nACL-LONG.763\n[16] Viet Cuong Nguyen, Mini Jain, Abhijat Chauhan, Heather Jamie Soled, San-\ntiago Alvarez Lesmes, Zihang Li, Michael L Birnbaum, Sunny X Tang, Srijan\nKumar, and Munmun De Choudhury. 2025.\nSupporters and Skeptics: LLM-\nbased Analysis of Engagement with Mental Health (Mis) Information Content\non Video-sharing Platforms. In Proceedings of the International AAAI Conference\non Web and Social Media, Vol. 19. 1329â€“1345.\n[17] Francis C. Ohu, Darrell Norman Burrell, and Laura A. Jones. 2025. Public Health\nRisk Management, Policy, and Ethical Imperatives in the Use of AI Tools for Men-\ntal Health Therapy. Healthcare 13, 21 (2025). doi:10.3390/healthcare13212721\n[18] OpenAI. 2024. GPT-4o System Card.\nhttps://cdn.openai.com/gpt-4o-system-\ncard.pdf\n[19] OpenAI. 2025. OpenAI o3 and o4-mini System Card.\nhttps://cdn.openai.com/\npdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf\n[20] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.\nMedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical do-\nmain Question Answering. arXiv:2203.14371 [cs.CL] https://arxiv.org/abs/2203.\n14371\n[21] Vikram Patel, Shekhar Saxena, Crick Lund, Graham Thornicroft, Florence Bain-\ngana, Paul Bolton, Dan Chisholm, Pamela Y. Collins, Janice L. Cooper, John\nEaton, Helen Herrman, Mohammad M. Herzallah, Y. Huang, Mark J. D. Jordans,\nArthur Kleinman, Maria E. Medina-Mora, Emily Morgan, Uzma Niaz, Olayinka\nOmigbodun, Martin Prince, Atif Rahman, Benedetto Saraceno, Bidyut K. Sarkar,\nMary De Silva, Indira Singh, Dan J. Stein, Charlene Sunkel, and JÃ¼rgen UnÃ¼tzer.\n2018. The Lancet Commission on global mental health and sustainable develop-\nment. The Lancet 392, 10157 (Oct. 2018), 1553â€“1598. doi:10.1016/S0140-6736(18)\n31612-X Epub 2018 Oct 9; Erratum in: Lancet. 2018 Oct 27;392(10157):1518. doi:\n10.1016/S0140-6736(18)32624-2.\n[22] RafaÅ‚ PoÅ›wiata and MichaÅ‚ PereÅ‚kiewicz. 2022. OPI@LT-EDI-ACL2022: Detect-\ning Signs of Depression from Social Media Text using RoBERTa Pre-trained Lan-\nguage Models. In Proceedings of the Second Workshop on Language Technology for\nEquality, Diversity and Inclusion, Bharathi Raja Chakravarthi, B Bharathi, John P\nMcCrae, Manel Zarrouk, Kalika Bali, and Paul Buitelaar (Eds.). Association for\nComputational Linguistics, Dublin, Ireland, 276â€“282. doi:10.18653/v1/2022.ltedi-\n1.40\n[23] Huachuan Qiu and Zhenzhong Lan. 2024.\nInteractive agents: Simulating\ncounselor-client psychological counseling via role-playing llm-to-llm interac-\ntions. arXiv preprint arXiv:2408.15787 (2024).\n[24] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian\nYang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Jun-\nyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng\nXue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu\nXia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu\nWan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5\nTechnical Report. arXiv:2412.15115 [cs.CL] https://arxiv.org/abs/2412.15115\n[25] Suraj Racha, Prashant Joshi, Anshika Raman, Nikita Jangid, Mridul Sharma,\nGanesh Ramakrishnan, and Nirmal Punjabi. 2025. MHQA: A Diverse, Knowl-\nedge Intensive Mental Health Question Answering Challenge for Language\nModels. arXiv preprint arXiv:2502.15418 (2025).\n[26] E Ri, E Je, D Arikpo, M Mm, and C Ja. 2015. Cochrane Library Trusted evidence.\nInformed decisions. Better health. Cochrane Database of Systematic Reviews [In-\ntervention Review] Hand washing promotion for preventing diarrhoea. DOI 10,\n14651858 (2015), 1â€“94.\n[27] K. L. Rosen, M. Sui, K. Heydari, E. J. Enichen, and J. C. Kvedar. 2025. The perils\nof politeness: how large language models may amplify medical misinformation.\nNPJ Digital Medicine 8, 1 (Nov. 2025), 644. doi:10.1038/s41746-025-02135-7\n[28] Ashish Sharma, Kevin Rushton, Inna Lin, David Wadden, Khendra Lucas, Adam\nMiner, Theresa Nguyen, and Tim Althoff. 2023. Cognitive Reframing of Neg-\native Thoughts through Human-Language Model Interaction. In Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers). 9977â€“10000.\n[29] Sagarika Shreevastava and Peter Foltz. 2021. Detecting Cognitive Distortions\nfrom Patient-Therapist Interactions. In Proceedings of the Seventh Workshop on\nComputational Linguistics and Clinical Psychology: Improving Access, Nazli Go-\nharian, Philip Resnik, Andrew Yates, Molly Ireland, Kate Niederhoffer, and Re-\nbecca Resnik (Eds.). Association for Computational Linguistics, Online, 151â€“158.\ndoi:10.18653/v1/2021.clpsych-1.17\n[30] Gopendra Vikram Singh, Sai Vardhan Vemulapalli, Mauajama Firdaus, and Asif\nEkbal. 2024. Deciphering cognitive distortions in patient-doctor mental health\nconversations: A multimodal llm-based detection and reasoning framework. In\nProceedings of the 2024 conference on empirical methods in natural language pro-\ncessing. 22546â€“22570.\n[31] Hoyun Song, Huije Lee, Jisu Shin, Sukmin Cho, Changgeon Ko, and Jong C. Park.\n2025. Does Rationale Quality Matter? Enhancing Mental Disorder Detection via\nSelective Reasoning Distillation. In Findings of the Association for Computational\nLinguistics: ACL 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and\nMohammad Taher Pilehvar (Eds.). Association for Computational Linguistics,\nVienna, Austria, 21738â€“21756. doi:10.18653/v1/2025.findings-acl.1119\n[32] EmoLLM Team. 2024. EmoLLM: Reinventing Mental Health Support with Large\nLanguage Models. https://github.com/SmartFlowAI/EmoLLM.\n[33] World\nHealth\nOrganization.\n2025.\nOver\na\nbillion\npeople\nliving\nwith\nmental\nhealth\nconditions\nâ€“\nservices\nrequire\nurgent\nscale-up.\nhttps://www.who.int/news/item/02-09-2025-over-a-billion-people-living-\nwith-mental-health-conditions-services-require-urgent-scale-up.\nNews\nrelease; accessed 2025-12-01.\n[34] Zixiu Wu, Simone Balloccu, Vivek Kumar, Rim Helaoui, Ehud Reiter, Diego Re-\nforgiato Recupero, and Daniele Riboni. 2022. Anno-MI: A Dataset of Expert-\nAnnotated Counselling Dialogues. In ICASSP 2022 - 2022 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP). 6177â€“6181. doi:10.\n1109/ICASSP43922.2022.9746035\n[35] Mengxi Xiao, Ben Liu, He Li, Jimin Huang, Qianqian Xie, Xiaofen Zong, Mang Ye,\nand Min Peng. [n. d.]. MoodAngels: A Retrieval-augmented Multi-agent Frame-\nwork for Psychiatry Diagnosis. In The Thirty-ninth Annual Conference on Neural\nInformation Processing Systems.\n[36] Mengxi Xiao, Qianqian Xie, Ziyan Kuang, Zhicheng Liu, Kailai Yang, Min Peng,\nWeiguang Han, and Jimin Huang. 2024. HealMe: Harnessing Cognitive Refram-\ning in Large Language Models for Psychotherapy. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association\nfor Computational Linguistics, Bangkok, Thailand, 1707â€“1725. doi:10.18653/v1/\n2024.acl-long.93\n[37] Haojie Xie, Yirong Chen, Xiaofen Xing, Jingkai Lin, and Xiangmin Xu. 2025.\nPsyDT: Using LLMs to Construct the Digital Twin of Psychological Counselor\nwith Personalized Counseling Style for Psychological Counseling. In Proceed-\nings of the 63rd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and\nMohammad Taher Pilehvar (Eds.). Association for Computational Linguistics,\nVienna, Austria, 1081â€“1115. doi:10.18653/v1/2025.acl-long.55\n[38] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng\nLiu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong\nTang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou,\nJingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao\nDeng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui\nMen, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao\nYin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan,\nYang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang,\nZeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 Techni-\ncal Report. arXiv:2505.09388 [cs.CL] https://arxiv.org/abs/2505.09388\n[39] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and\nSophia Ananiadou. 2024. MentaLLaMA: interpretable mental health analysis\non social media with large language models. In Proceedings of the ACM Web Con-\nference 2024. 4489â€“4500.\n[40] Min Zeng. 2025. PsychCounsel-Bench: Evaluating the Psychology Intelligence\nof Large Language Models. arXiv preprint arXiv:2510.01611 (2025).\n[41] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and\nJason Weston. 2018. Personalizing Dialogue Agents: I have a dog, do you have\npets too?. In Proceedings of the 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers). 2204â€“2213.\n[42] Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang\nLi, Bolin Ding, and Jingren Zhou. 2025. On-Policy RL Meets Off-Policy Experts:\nHarmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic\nWeighting. arXiv:2508.11408 [cs.LG] https://arxiv.org/abs/2508.11408\nA\nReasoning Chain Evaluation Guideline\nAssign 1 if no instances of the issue are present in the reasoning\nchain; assign 0 if any instance is observed.\n"}, {"page": 11, "text": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nReasoning Conciseness. The reasoning chain should contain no\nunnecessary complexity, repetition, or backtracking. Error indica-\ntors include:\nâ€¢ Over-elaborating a straightforward case (e.g., exhaustively eval-\nuating all options when the answer is obvious).\nâ€¢ Repeating the same evidence or argument across multiple steps.\nâ€¢ Reversing earlier conclusions without justification.\nLogical Coherence. Each step should provide clear and case-specific\nreasoning, not merely labels or unsupported claims. Error indica-\ntors include:\nâ€¢ Steps that function only as headings without substantive elabo-\nration.\nâ€¢ Claims presented without corresponding explanations or evi-\ndence.\nHallucination Avoidance. The reasoning chain should accurately\nreflect the case information and avoid hallucinations. Error indica-\ntors include Introducing facts not mentioned in the case.\nTask Understanding. The reasoning chain should correctly fol-\nlow the task objective and not drift to a different task. The modelâ€™s\nreply shouldnâ€™t address a different task than instructed. For exam-\nple, if a model misunderstands the counseling strategy formulation\ntask, it may generate counselor utterances instead of selecting an\nappropriate counseling strategy.\nInternal Consistency. The reasoning chain should exhibit no con-\ntradictions across steps. Error indicators include:\nâ€¢ Later steps contradict earlier interpretations of symptoms, diag-\nnoses, or risk levels.\nâ€¢ Changing conclusions mid-chain without reconciling prior evi-\ndence.\nB\nCase Study\nThe detail of case study is illustrated in Figure 3.\n"}, {"page": 12, "text": "Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nMengxi Xiao et al.\nSituation: In class I feel like others hear my thoughts and judge me. When some things really don't work(for example I have some things for school for tommorow and I can't find how to do that thing after searching hours on \ninternet I get angry and sometimes results to self harm(punching). I sometimes feel like everybody is here just to watch me and see how I react to things(sounds insane).\nThought: Am I insane?\nQ: The most likely cognitive error is?\nCognitiveReframing\nKey Analysis: The client believes others are judging them and feels watched, indicating a distorted belief about others' \nperceptions. \nStep Reasoning: \nThe client assumes others know their thoughts (mind reading). \nFeeling judged without evidence aligns with mind reading. \nSelf-harm stems from distress, not the primary cognitive error. \nFinal Answer: Answer: [Mind reading]\nQwen3-8B\n1. Key Analysis: The client believes others can hear/assess their thoughts and judge them, and feels observed, indicating an \nassumption about others' perceptions without evidence. \n2. Step Reasoning: \nPrimary distortion involves assuming othersâ€™ internal states (thoughts/judgments). \nDirectly aligns with \"Mind reading\": attributing unverified mental states to others. \nOther errors (e.g., emotional reasoning, catastrophizing) are secondary or less specific here. \n3. Final Answer: Answer: [Mind reading]\nQwen-plus\n1.\"In class I feel like others hear my thoughts and judge me.\": This is a classic example of Mind Reading. The client is \nassuming they know what others are thinking (\"judge me\") and even attributing impossible knowledge to them (\"hear my \nthoughts\") without evidence.\n2.\"I sometimes feel like everybody is here just to watch me and see how I react to things\": This reinforces the Mind \nReading error. The client assumes they know the intentions and focus of others (\"everybody is here just to watch me\"), \nbelieving everyone is paying specific, judgmental attention to them without proof.\n3.\"(sounds insane)\" / \"Am I insane?\": The client's awareness that this belief sounds irrational highlights the distorted nature \nof this thinking pattern, which is characteristic of cognitive errors like Mind Reading.\nWhile other errors might be present (like Emotional Reasoning - feeling judged must mean it's true, or Personalization - \ntaking others' presence/actions personally as being \"about\" them), the core distortion driving the distress described in the \nquery is the persistent assumption of knowing others' thoughts and intentions (Mind Reading).\nAnswer: Mind reading\nDeepseek-R1\n###Understanding the Thought Process: \nThe client is questioning their own sanity, indicating a focus on their internal feelings and perceptions.\n###Evaluating Emotional Influence: \nThe client's feelings of being judged and overwhelmed by thoughts suggest they might be treating these emotions as factual \nevidence of insanity.\n###Assessing Emotional Reasoning: \nThe client's thought process reflects a tendency to accept their emotions as truth, indicating emotional reasoning.\n###Final Conclusion: \nThe cognitive error most applicable to the client's thought process is 'Emotional Reasoning', where they treat their feelings \nas facts about their sanity.\ngenerated_answer:\n Answer: emotional reasoning\nMindora\n1. Key Analysis:  \nThe client assumes others know and judge their thoughts, feeling constantly observed and evaluated.\n2. Step Reasoning:  \nâ€¢ Client believes they can read othersâ€™ minds (â€œothers hear my thoughts and judge meâ€)  \nâ€¢ They infer judgment without evidence, fitting the definition of mind reading  \n3. Final Answer:  \nAnswer: Mind reading\nGPT-o4-mini\nreasoning: The clientâ€™s perception of others hearing their thoughts and judging them aligns with the [Mind Reading] cognitive \nerror, where they assume others are aware of their inner thoughts and judgments. Additionally, the clientâ€™s tendency to \nengage in self-harm when facing difficulties is indicative of the [Emotional Reasoning] cognitive error, as they treat their \nemotional reactions as facts. The clientâ€™s extreme reaction to not being able to find an answer online and resorting to self-\nharm suggests [Catastrophizing], focusing on the worst-case scenario. The overall sense of being judged and the extreme \nreactions indicate a pattern of [All-or-Nothing Thinking] and [Comparing and Despairing].\ngenerated_answer:\n Answer: [Mind reading]\nPsyche-R1\n1. Key Analysis: The client feels like others can hear their thoughts and judge them, experiences intense anger and self-harm \nwhen faced with problems, and believes everyone is watching them. These feelings and thoughts indicate a distorted \nperception of reality and self.\n2. Step Reasoning:\n- The client assumes others can hear their thoughts and judge them, indicating an assumption about others' thoughts.\n- They experience intense emotions leading to self-harm when faced with problems, suggesting an overwhelming sense of \nhopelessness or despair.\n- The client wonders if they are \"insane,\" indicating a possible distortion in thinking.\n3. Final Answer: Answer: Mind reading\nLLaMA-4\nFigure 3: A challenging case of cognitive error identification.\n"}]}