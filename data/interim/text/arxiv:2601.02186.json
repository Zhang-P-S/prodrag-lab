{"doc_id": "arxiv:2601.02186", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.02186.pdf", "meta": {"doc_id": "arxiv:2601.02186", "source": "arxiv", "arxiv_id": "2601.02186", "title": "Toward Global Large Language Models in Medicine", "authors": ["Rui Yang", "Huitao Li", "Weihao Xuan", "Heli Qi", "Xin Li", "Kunyu Yu", "Yingjian Chen", "Rongrong Wang", "Jacques Behmoaras", "Tianxi Cai", "Bibhas Chakraborty", "Qingyu Chen", "Lionel Tim-Ee Cheng", "Marie-Louise Damwanza", "Chido Dzinotyiwei", "Aosong Feng", "Chuan Hong", "Yusuke Iwasawa", "Yuhe Ke", "Linah Kitala", "Taehoon Ko", "Jisan Lee", "Irene Li", "Jonathan Chong Kai Liew", "Hongfang Liu", "Lian Leng Low", "Edison Marrese-Taylor", "Yutaka Matsuo", "Isheanesu Misi", "Yilin Ning", "Jasmine Chiat Ling Ong", "Marcus Eng Hock Ong", "Enrico Petretto", "Hossein Rouhizadeh", "Abiram Sandralegar", "Oren Schreier", "Iain Bee Huat Tan", "Patrick Tan", "Daniel Shu Wei Ting", "Junjue Wang", "Chunhua Weng", "Matthew Yu Heng Wong", "Fang Wu", "Yunze Xiao", "Xuhai Xu", "Qingcheng Zeng", "Zhuo Zheng", "Yifan Peng", "Douglas Teodoro", "Nan Liu"], "published": "2026-01-05T15:05:49Z", "updated": "2026-01-05T15:05:49Z", "summary": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.02186v1", "url_pdf": "https://arxiv.org/pdf/2601.02186.pdf", "meta_path": "data/raw/arxiv/meta/2601.02186.json", "sha256": "d6ba750412193344eeb4847a0f3befb54cf9437d0aea5d93338e0b8bf66dab91", "status": "ok", "fetched_at": "2026-02-18T02:23:11.626709+00:00"}, "pages": [{"page": 1, "text": " \n1 \nToward Global Large Language Models in Medicine \n \nRui Yang1,2, Huitao Li1,2, Weihao Xuan3†, Heli Qi4, Xin Li5, Kunyu Yu1,2, Yingjian Chen6, \nRongrong Wang7, Jacques Behmoaras1,2, Tianxi Cai8,9, Bibhas Chakraborty1,2,10,11,12, \nQingyu Chen13, Lionel Tim-Ee Cheng14,15, Marie-Louise Damwanza16, Chido \nDzinotyiwei16, Aosong Feng17, Chuan Hong12,18, Yusuke Iwasawa6, Yuhe Ke1,19,20, Linah \nKitala16, Taehoon Ko21,22,23, Jisan Lee24, Irene Li6, Jonathan Chong Kai Liew1,25, Hongfang \nLiu5, Lian Leng Low26,27,28, Edison Marrese-Taylor6,29, Yutaka Matsuo6, Isheanesu Misi16, \nYilin Ning1,2, Jasmine Chiat Ling Ong1,30, Marcus Eng Hock Ong31,32, Enrico Petretto1,2, \nHossein Rouhizadeh33, Abiram Sandralegar33, Oren Schreier33, Iain Bee Huat \nTan34,35,36,37, Patrick Tan34,37,38,39, Daniel Shu Wei Ting40,41,42,43, Junjue Wang3, Chunhua \nWeng44, Matthew Yu Heng Wong45, Fang Wu46, Yunze Xiao47, Xuhai Xu44, Qingcheng \nZeng48, Zhuo Zheng46, Yifan Peng7,49†, Douglas Teodoro33†, Nan Liu1,2,12,31,50† \n \n1 Duke-NUS AI + Medical Sciences Initiative, Duke-NUS Medical School, Singapore, Singapore \n2 Centre for Biomedical Data Science, Duke-NUS Medical School, Singapore, Singapore \n3 Graduate School of Frontier Sciences, The University of Tokyo, Tokyo, Japan \n4 Faculty of Science and Engineering, Waseda University, Tokyo, Japan \n5 McWilliams School of Biomedical Informatics, University of Texas Health Science Center at \nHouston, Houston, TX, USA \n6 Graduate School of Engineering, The University of Tokyo, Tokyo, Japan \n7 Department of Population Health Sciences, Weill Cornell Medicine, New York, NY, USA \n8 Department of Biostatistics, Harvard University, Boston, MA, USA \n9 Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA \n10 Health Services Research & Population Health, Duke-NUS Medical School, Singapore, Singapore \n11 Department of Statistics and Data Science, National University of Singapore, Singapore, \nSingapore \n12 Department of Biostatistics and Bioinformatics, Duke University, Durham, NC, USA \n13 Department of Biomedical Informatics and Data Science, Yale School of Medicine, Yale \nUniversity, New Haven, CT, USA \n14 Radiological Sciences Academic Clinical Programme, SingHealth Duke-NUS, Singapore, \nSingapore \n15 Department of Cardiothoracic and Abdominal Radiology, Singapore General Hospital, Singapore, \nSingapore \n"}, {"page": 2, "text": " \n2 \n16 Vambo AI, Johannesburg, South Africa \n17 Department of Computer Science, Yale University, New Haven, CT, USA \n18 Duke Clinical Research Institute, Durham, NC, USA \n19 Department of Anesthesiology, Singapore General Hospital, Singapore, Singapore \n20 Data Science and Artificial Intelligence Lab, Singapore General Hospital, Singapore, Singapore \n21 Department of Medical Informatics, College of Medicine, The Catholic University of Korea, Seoul, \nRepublic of Korea \n22 Department of Medical Sciences, College of Medicine, The Catholic University of Korea, Seoul, \nRepublic of Korea \n23 CMC Institute for Basic Medical Science, The Catholic Medical Center of the Catholic University of \nKorea, Seoul, Republic of Korea \n24 Department of Nursing, Gangneung–Wonju National University, Wonju, Republic of Korea \n25 Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, USA \n26 Family Medicine Academic Clinical Programme, Duke-NUS Medical School, Singapore, Singapore \n27 Division of Population Health and Integrated Care, Singapore General Hospital, Singapore, \nSingapore \n28 Centre for Population Health Research and Implementation, Singapore Health Services, \nSingapore, Singapore \n29 National Institute of Advanced Industrial Science and Technology, Tokyo, Japan \n30 Division of Pharmacy, Singapore General Hospital, Singapore, Singapore \n31 Pre-hospital & Emergency Research Centre, Health Services Research & Population Health, Duke-\nNUS Medical School, Singapore, Singapore \n32 Department of Emergency Medicine, Singapore General Hospital, Singapore, Singapore \n33 Department of Radiology and Medical Informatics, University of Geneva, Geneva, Switzerland \n34 Cancer and Stem Cell Biology Programme, Duke-NUS Medical School, Singapore, Singapore \n35 Division of Medical Oncology, National Cancer Centre Singapore, Singapore, Singapore \n36 Office of Deputy Group Chief Medical Informatics Officer (Research), Singapore Health Services, \nSingapore, Singapore \n37 Genome Institute of Singapore, Agency for Science, Technology and Research, Singapore, \nSingapore \n38 SingHealth Duke-NUS Institute of Precision Medicine, Singapore, Singapore \n39 Precision Health Research, Singapore, Singapore \n40 Ophthalmology and Visual Sciences Academic Clinical Programme, Duke-NUS Medical School, \nSingapore, Singapore \n41 Singapore National Eye Centre, Singapore Eye Research Institute, Singapore, Singapore \n42 Artificial Intelligence Office, Singapore Health Services, Singapore, Singapore \n"}, {"page": 3, "text": " \n3 \n43 Byers Eye Institute, Stanford University, Stanford, CA, USA \n44 Department of Biomedical Informatics, Columbia University, New York, NY, USA \n45 School of Clinical Medicine, University of Cambridge, Cambridge, UK \n46 Department of Computer Science, Stanford University, Stanford, CA, USA \n47 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA \n48 Department of Linguistics, Northwestern University, Evanston, IL, USA \n49 Institute of Artificial Intelligence for Digital Health, Weill Cornell Medicine, New York, NY, USA \n50 NUS Artificial Intelligence Institute, National University of Singapore, Singapore, Singapore \n \nThis work was jointly supervised by Yifan Peng, Douglas Teodoro, and Nan Liu. \n \nCorresponding Authors:  \nWeihao Xuan, Graduate School of Frontier Sciences, The University of Tokyo, Kiban-to 406, \n5-1-5 Kashiwanoha, Kashiwa, Chiba 277-8561, Japan \nEmail: xuan@ms.k.u-tokyo.ac.jp \n \nYifan Peng, Department of Population Health Sciences, Weill Cornell Medicine, 575 Lex \nave, New York, 10022, USA \nEmail: yip4002@med.cornell.edu  \n \nDouglas Teodoro, Department of Radiology and Medical Informatics, University of Geneva, \nCampus Biotech, G6-N3, Chemin des Mines 9, CH-1202 Geneva, Switzerland \nEmail: douglas.teodoro@unige.ch \n \nNan Liu, Centre for Biomedical Data Science, Duke-NUS Medical School, 8 College Road, \nSingapore 169857, Singapore \nEmail: liu.nan@duke-nus.edu.sg \n \n \n \n \n \n \n \n"}, {"page": 4, "text": " \n4 \nAbstract \nDespite continuous advances in medical technology, the global distribution of health \ncare resources remains uneven. The development of large language models (LLMs) has \ntransformed the landscape of medicine and holds promise for improving health care \nquality and expanding access to medical information globally. However, existing LLMs \nare primarily trained on high-resource languages, limiting their applicability in global \nmedical scenarios. To address this gap, we constructed GlobMed, a large multilingual \nmedical dataset, containing over 500,000 entries spanning 12 languages, including four \nlow-resource languages. Building on this, we established GlobMed-Bench, which \nsystematically assesses 56 state-of-the-art proprietary and open-weight LLMs across \nmultiple multilingual medical tasks, revealing significant performance disparities across \nlanguages, particularly for low-resource languages. Additionally, we introduced \nGlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with \nparameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance \nimprovement of over 40% relative to baseline models, with a more than threefold \nincrease in performance on low-resource languages. Together, these resources provide \nan important foundation for advancing the equitable development and application of \nLLMs globally, enabling broader language communities to benefit from technological \nadvances. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 5, "text": " \n5 \nIntroduction \nDespite continuous advances in medical technology, important innovations have not \ntranslated into a more equitable global distribution of health care resources, \nparticularly in low-resource regions1,2. Currently, nearly half of the global population \nstill lacks access to essential health services3. This disparity is largely driven by fragile \nhealth systems, shortages of trained health care providers, and inadequate \ninfrastructure, all of which continue to constrain progress in population health3. In \naddition, insufficient access to reliable medical information further limits public health \nliteracy and self-care capability, contributing to delayed diagnoses and preventable \nhealth burdens4. \n \nThe rapid development of large language models (LLMs) has begun to reshape the \nlandscape of medicine, with promising applications in clinical consultation, disease \ndiagnosis, health management, and medical education5–7. Early evidence suggests that \nLLMs have the potential to alleviate the workload of clinicians while simultaneously \nimproving the quality and consistency of patient care8–10. Beyond frontline clinical \nsettings, LLMs are increasingly integrated in clinical and translational research11,12 to \nsupport tasks such as literature screening13, quality appraisal14, and knowledge \nsynthesis15. As these models become more capable and widely accessible, they offer a \ncompelling opportunity to strengthen health care delivery and expand access to medical \ninformation globally16,17. \n \nHowever, the global applicability of LLMs in medicine is limited by several key \nchallenges18,19. Most existing LLMs are trained predominantly on data from high-\nresource languages (i.e., languages with abundant linguistic resources and technical \nsupport20). For instance, over 92% of GPT-3’s pretraining corpus is derived from \nEnglish sources, while low-resource languages (i.e., languages with scarce linguistic \nresources and limited technical support20) remain severely underrepresented21. This \nimbalance has led to substantial performance disparities across languages, undermining \nthe reliability and generalizability of LLMs in global medical contexts22,23. Moreover, \ncurrent medical benchmarks are limited in both scale and linguistic diversity, making it \ndifficult to systematically evaluate LLM performance across a wide range of real-world \nuse cases24. These limitations risk exacerbating global health disparities, particularly \n"}, {"page": 6, "text": " \n6 \naffecting communities that rely on low-resource languages, precisely those who stand to \nbenefit most from the equitable development of LLM applications18,19. Addressing these \nchallenges is essential to ensure that advances in LLM technology can effectively \nsupport health care delivery and access to medical information in diverse global \nsettings. \n \nTherefore, developing high-quality multilingual medical datasets and comprehensive \nevaluation benchmarks is crucial24. Such resources would facilitate the systematic \nevaluation of LLM performance across languages and uncover gaps in generalizability. \nEqually important is the inclusion of languages that are currently underrepresented, \nensuring that medical LLM innovations benefit a broader range of language \ncommunities. Building on this foundation, specialized medical LLMs optimized for \nmultilingual contexts, particularly for lower-resource languages, are needed to extend \nthe reach of technological advances to health communities that have historically been \nexcluded. \n \nTo advance the global development of medical LLMs, our study makes three core \ncontributions (Fig. 1): (1) GlobMed. We constructed GlobMed, a large multilingual \nmedical dataset, spanning 12 languages (including four low-resource languages) that \ncollectively represent nearly six billion people (~75% of the global population)25. \nGlobMed contains more than 500,000 entries across three core tasks: natural language \ninference (NLI), long-form question answering (QA), and multiple-choice question \nanswering (MCQA). GlobMed was evaluated by bilingual medical experts to ensure \nlinguistic accuracy and clinical validity. (2) GlobMed-Bench. We established GlobMed-\nBench, a comprehensive evaluation benchmark that assesses 56 state-of-the-art \nproprietary and open-weight LLMs using over 40,000 independent experiments and \ngenerating over 125 million responses. This benchmark provides the most extensive \nand systematic multilingual medical evaluation of LLMs to date, revealing significant \nperformance disparities across languages, particularly for low-resource languages. (3) \nGlobMed-LLMs. We introduced GlobMed-LLMs, a suite of multilingual medical LLMs \nranging from 1.7B to 8B parameters, trained on GlobMed and optimized for improved \nperformance in low-resource languages. Across six multilingual medical benchmarks \nand all 12 languages, GlobMed-LLMs achieved an average performance improvement of \n"}, {"page": 7, "text": " \n7 \nover 40% relative to baseline models and demonstrated more than a threefold \nimprovement in performance in low-resource languages. \n \n \n1.7B\n4B\n8B\n4B\n"}, {"page": 8, "text": " \n8 \nFig. 1: Overview of the three main contributions of the study. a, GlobMed: A large \nmultilingual medical dataset, which covers 12 languages across three core tasks: natural \nlanguage inference, long-form question answering, and multiple-choice question answering. \nGlobMed includes eight high-resource languages (Chinese, English, French, German, Japanese, \nKorean, Portuguese, Spanish) and four low-resource languages (Swahili, Wolof, Yoruba, Zulu). b, \nGlobMed-Bench: A comprehensive multilingual medical benchmark assessing 56 state-of-the-\nart proprietary and open-weight LLMs. The benchmark contains more than 40,000 independent \nexperiments and generated over 125 million responses, enabling systematic evaluation of \nperformance disparities across languages. c, GlobMed-LLMs: A suite of multilingual medical \nLLMs ranging from 1.7B to 8B parameters, trained on GlobMed and optimized to improve \nperformance in low-resource languages. \n \nResults \nGlobMed \nWe constructed GlobMed through three steps: data collection and screening, agentic \nmachine translation, and expert evaluation (Fig. 2). GlobMed comprises over 500,000 \nhigh-quality entries across 12 languages, including eight high-resource languages \n(Chinese, English, Spanish, French, German, Portuguese, Korean, and Japanese) and four \nlow-resource languages (Swahili, Wolof, Yoruba, and Zulu). These entries cover three \ncore tasks: NLI, Long-Form QA, and MCQA. Additionally, GlobMed was independently \nevaluated by bilingual medical experts around the world to ensure both linguistic \naccuracy and clinical validity. For more information about GlobMed, please refer to \nSupplementary Information S1. \n"}, {"page": 9, "text": " \n9 \n \nFig. 2: Overview of the GlobMed curation workflow. a, Data Collection and Screening: Data \nwere sourced from nine medical datasets, including BioNLI and MedNLI for the NLI task, \nExpertQA-Bio, ExpertQA-Med, and LiveQA for the long-form QA task, and HeadQA, MedExpQA, \nMedQA, and MMLU-Pro for the MCQA task. All original data were manually reviewed, during \nwhich 3,114 entries were removed for incompleteness, irrelevance, and disorganization. The \nremaining high-quality screened data served as the foundation for subsequent multilingual \nexpansion. b, Agentic Machine Translation: This process involved three steps: (1) named \nentity recognition and medical entities retrieval from a custom-built multilingual medical \ndictionary, followed by initial translation generation by an LLM; (2) reflection by Expert Agent I \nto identify semantic or structural issues and provide refinement suggestions; (3) optimized \ntranslation generated by Expert Agent II, incorporating the suggested improvements. c, Expert \nEvaluation: To ensure diverse topical coverage, topic modeling was performed on each dataset \nto select representative samples across multiple thematic clusters. Each entry was then \nindependently evaluated by at least two bilingual medical experts on three dimensions, \nCompleteness\nFluency\nAccuracy\nLLM\nDiverse\nSelection\n"}, {"page": 10, "text": " \n10 \naccuracy, fluency, and completeness, to ensure linguistic accuracy and clinical validity. d, Global \nCountry Distribution: A world map illustrates the geographic distribution of the 12 languages \nrepresented in GlobMed. Countries are shaded to indicate inclusion, with color intensity \nrepresenting the number of GlobMed-supported languages spoken as official or national \nlanguages. GlobMed spans countries across Asia, Europe, Africa, and the Americas. \n \nGlobMed-Bench \nOverall Performance of Proprietary LLMs and Open-Weight LLMs \nWe systematically evaluated 12 proprietary LLMs and 44 open-weight LLMs on \nGlobMed-Bench (Fig. 3). For more information about GlobMed-Bench, please refer to \nSupplementary Information S2. \n \nProprietary LLMs generally achieved stronger performance, with accuracies ranging \nfrom 54.60% to 77.25%. Meanwhile, multiple proprietary LLMs surpassed the 75% \naccuracy threshold, underscoring their current advantage in multilingual medical tasks. \nThe top-performing LLMs were Gemini-2.5-Flash26 (77.25%), o4-mini27 (77.22%), and \nGPT-528 (75.98%), with Claude-4.0-Sonnet29 (75.19%) also demonstrating strong \nmultilingual medical capability. \n \nOpen-weight LLMs ranged from 1.7B to 671B parameters. Overall, performance showed \na clear positive correlation with parameter scale. The top-performing LLMs achieved \naccuracies of approximately 75%, while the lowest-performing LLMs scored around \n20%. Among all evaluated open-weight LLMs, gpt-oss-120B30 (74.74%), DeepSeek-R131 \n(74.56%), and LLaMA-4-Maverick32 (71.94%) demonstrated the most outstanding \nperformance, as the only ones exceeding the 70% accuracy threshold. Notably, several \nmedium-sized LLMs, such as gpt-oss-20B30 (67.37%), delivered unexpectedly strong \nperformance despite their relatively modest parameter counts, highlighting promising \nscaling efficiency.\n"}, {"page": 11, "text": " \n11 \nFig. 3: Overview of the GlobMed-Bench. a, Overall performance of proprietary LLMs. The \nbar chart displays the overall performance (y-axis, measured by accuracy) of 12 state-of-the-art \nproprietary LLMs on GlobMed-Bench. The evaluated LLMs include the Anthropic series (Claude-\n"}, {"page": 12, "text": " \n12 \n3.5-Haiku and Claude-4.0-Sonnet), Google's Gemini-2.5-Flash, and the OpenAI series (GPT-4o-\nmini, GPT-4o, GPT-4.1-nano, GPT-4.1-mini, GPT-4.1, GPT-5-nano, GPT-5-mini, GPT-5, o4-mini). \nFor the OpenAI GPT-5 series, we set the “reasoning effort” to “minimal”. b, Overall \nperformance of open-weight LLMs: The scatter plot displays the relationship between model \nsize (x-axis, measured in billions of parameters) and overall performance (y-axis, measured by \naccuracy) for 44 open-weight LLMs evaluated on GlobMed-Bench. The evaluated LLMs include \nthe DeepSeek, Meta LLaMA, Microsoft Phi, Qwen, Google Gemma, Mistral AI, OpenAI gpt-oss \nseries, and specialized medical LLMs. For Qwen3-1.7B, we set it to “non-thinking” mode. \nAdditionally, we marked the performance of GlobMed-LLMs for comparison. \n \nCross-Lingual Performance Disparities across 56 LLMs \nTo systematically evaluate the cross-lingual performance consistency, we used the \noriginal language of each dataset as the reference and compared performance across 56 \nLLMs for all other languages (Fig. 4). Specifically, for datasets originally in English, \nEnglish served as the reference; for those in Spanish, Spanish served as the reference. \nWhen English was the reference, most languages, including high-resource languages, \nexhibited significant performance gaps relative to English. In contrast, when Spanish \nserved as the reference, disparities among high-resource languages were notably \nsmaller. Under both reference conditions, however, low-resource languages (e.g., \nSwahili, Wolof, Yoruba, Zulu) consistently showed pronounced performance gaps \nrelative to the reference language across nearly all LLMs, indicating that current LLMs \nstill have markedly insufficient medical knowledge comprehension capability in low-\nresource languages. \n"}, {"page": 13, "text": " \n13 \n \nFig. 4: Cross-Lingual Performance Disparities across 56 LLMs. a, English as the reference \nlanguage: For datasets originally in English, each cell represents the statistical significance of \nthe performance disparity between English and the target language (y-axis) for a given LLM (x-\naxis). b, Spanish as the reference language: For datasets originally in Spanish, each cell \nrepresents the statistical significance of the performance disparity between Spanish and the \ntarget language (y-axis) for a given LLM (x-axis). Statistical significance is indicated by asterisks \n(*p<0.05, **p<0.01, ***p<0.001, ****p<0.0001). \n \nPerformance Comparison Between General LLMs and Medical LLMs \nTo evaluate the effect of medical-specific training on multilingual performance, we \ncompared general LLMs with their corresponding medical variants on GlobMed-Bench \nZulu\nYoruba\nWolof\nSwahili\nSpanish\nPortuguese\nKorean\nJapanese\nGerman\nFrench\nChinese\nSignificance\nSignificance\nZulu\nYoruba\nWolof\nSwahili\nPortuguese\nKorean\nJapanese\nGerman\nFrench\nEnglish\nChinese\na. Performance Disparity using English as the Reference\nb. Performance Disparity using Spanish as the Reference\n"}, {"page": 14, "text": " \n14 \n(Fig. 5). Overall, medical LLMs did not consistently outperform their general \ncounterparts. Meanwhile, performance varied substantially across LLMs, indicating that \nthe benefits of medical-specific training depend heavily on training strategy. \n \nAmong the medical LLMs, the MedGemma series33 showed the most consistent \nperformance gains, with MedGemma-4B33 improving accuracy from 37.74% to 42.00% \nrelative to Gemma3-4B34, and MedGemma-27B33 improving accuracy from 58.84% to \n64.79% relative to Gemma3-27B34. Radar chart analysis revealed that MedGemma33 \nconsistently improved performance across all six medical benchmarks and 12 \nlanguages. The HuatuoGPT-o1 series35 achieved modest improvements over their \ngeneral LLM counterparts at multiple scales (8B, 70B, 72B), but were generally below \n4%. In contrast, several medical LLMs performed substantially worse than their general \nLLM counterparts, such as HuatuoGPT-o1-7B35, Bio-Medical-LLaMA-3-8B36, \nMedReason-8B37, and OpenBioLLM-8B38/70B39, with some models exhibiting accuracy \ndeclines exceeding 20%. \n"}, {"page": 15, "text": " \n15 \n \nFig. 5: Performance comparison between general LLMs and medical LLMs. The figure \ncontains six panels (a-f), each comparing general LLMs with their medical variants across six \nmedical benchmarks (left radar chart), 12 languages (right radar chart), and overall accuracy \n(middle bar chart). a, Gemma-3-4B: Comparison between Gemma3-4B and MedGemma-4B. b, \nGemma-3-27B: Comparison between Gemma3-27B and MedGemma-27B. c, Qwen2.5-7B: \nComparison between Qwen2.5-7B and HuatuoGPT-o1-7B. d, Qwen2.5-72B: Comparison \nbetween Qwen2.5-72B and HuatuoGPT-o1-72B. e, LLaMA-3.1-8B: Comparison between \nLLaMA-3.1-8B and four medical LLMs (Bio-Medical-LLaMA-3-8B, HuatuoGPT-o1-8B, \nMedReason-8B, OpenBioLLM-8B). f, LLaMA-3.1-70B: Comparison between LLaMA-3.1-70B and \ntwo medical LLMs (HuatuoGPT-o1-70B, OpenBioLLM-70B). Statistical significance is indicated \nby asterisks (*p<0.05, **p<0.01, ***p<0.001). \n"}, {"page": 16, "text": " \n16 \n \nPerformance comparison between non-reasoning LLMs and reasoning LLMs \nWe compared non-reasoning LLMs with their reasoning-enhanced counterparts on \nGlobMed-Bench (Fig. 6). Overall, reasoning LLMs demonstrated clear advantages in \nmultilingual medical tasks. For instance, DeepSeek-R131 improved accuracy from \n69.15% to 74.56%, compared to DeepSeek-V340, and reasoning variants of the Qwen3 \nseries41 outperformed their non-reasoning counterparts across all scales (4B, 8B, 14B), \nwith improvements ranging from 1% to 6%. Similarly, Phi-4-reasoning42 improved \naccuracy from 55.93% to 63.79%. Radar chart analysis confirmed consistent \nimprovements across all six medical benchmarks and 12 languages. However, the \nbenefits were not universal; some reasoning variants, such as Phi-4-mini-reasoning42, \nunderperformed relative to their non-reasoning versions, with a 2.58% decrease in \naccuracy. \n \n \nFig. 6: Performance comparison between non-reasoning LLMs and reasoning LLMs. The \nfigure contains six panels (a-f), each comparing non-reasoning LLMs and their reasoning-\nenhanced counterparts across six medical benchmarks (left radar chart), 12 languages (right \n"}, {"page": 17, "text": " \n17 \nradar chart), and overall accuracy (middle bar chart). a, DeepSeek: Comparison between \nDeepSeek-V3 and DeepSeek-R1. b, Qwen3-4B: Comparison between Qwen3-4B and Qwen3-4B-\nthinking. c, Qwen3-8B: Comparison between Qwen3-8B and Qwen3-8B-thinking. d, Qwen3-\n14B: Comparison between Qwen3-14B and Qwen3-14B-thinking. e, Phi-4-mini: Comparison \nbetween Phi-4-mini and Phi-4-mini-reasoning. f, Phi-4: Comparison between Phi-4 and Phi-4-\nreasoning. Statistical significance is indicated by asterisks (*p<0.05, **p<0.01, ***p<0.001). \n \nGlobMed-LLMs \nTo enhance multilingual medical capability and mitigate performance disparities in \nunderrepresented languages, we fine-tuned MedGemma-4B33 and the Qwen3 series41 \n(1.7B, 4B, 8B) using GlobMed. The resulting fine-tuned LLMs, collectively referred to as \nGlobMed-LLMs, demonstrated substantial improvements over their baseline \ncounterparts across multiple benchmarks and languages (Fig. 7). For more information \nabout GlobMed-LLMs, please refer to Supplementary Information S3. \n \nSpecifically, GlobMed-MedGemma-4B increased overall accuracy from 42.00% to \n57.30%, outperforming the original MedGemma-4B33 on nearly all medical benchmarks, \nwith only a slight decrease on HeadQA. Across 12 languages, it maintained consistent \nimprovements in all high-resource languages and showed notable gains in low-resource \nlanguages, including Swahili, Wolof, Yoruba, and Zulu. \n \nSimilarly, GlobMed-Qwen3-4B improved overall accuracy from 43.80% to 62.17%, \noutperforming Qwen3-4B41 across all evaluated benchmarks and demonstrating \nsubstantial improvements in every language. Collectively, these results indicate that \nfine-tuning with GlobMed effectively improved the multilingual medical capabilities of \nLLMs while narrowing performance gaps across languages. \n"}, {"page": 18, "text": " \n18 \nFig. 7: Performance comparison of GlobMed-LLMs versus baseline LLMs. a, GlobMed-\nMedGemma-4B overall performance: Average accuracy across all benchmarks and languages \nimproved from 42.00% to 57.30% compared with MedGemma-4B. b, Task-wise performance: \nGlobMed-MedGemma-4B outperformed MedGemma-4B on nearly all medical benchmarks, with \nGlobMed-Qwen-4B\n***\n45\n50\n55\n60\n65\nChinese\nAccuracy\n***\n55\n60\n65\n70\nAccuracy\n***\n50\n55\n60\n65\n70\nFrench\nAccuracy\n***\n50\n55\n60\n65\nGerman\nAccuracy\n***\n40\n45\n50\n55\n60\n65\nAccuracy\n***\n40\n45\n50\n55\n60\n65\nKorean\nAccuracy\n***\n45\n50\n55\n60\n65\nAccuracy\n***\n45\n50\n55\n60\n65\nAccuracy\n***\n35\n40\n45\n50\n55\n60\nSwahili\nAccuracy\n***\n20\n30\n40\n50\nWolof\nAccuracy\n***\n20\n30\n40\n50\nYoruba\nAccuracy\n***\n30\n40\n50\nZulu\nAccuracy\n***\n60\n65\n70\n75\nAccuracy\n***\n65\n70\n75\n80\nAccuracy\n***\n55\n60\n65\n70\n75\nAccuracy\n***\n50\n55\n60\n65\n70\n75\nAccuracy\n***\n55\n60\n65\n70\nAccuracy\n***\n50\n55\n60\n65\n70\nAccuracy\n***\n55\n60\n65\n70\n75\nAccuracy\n***\n55\n60\n65\n70\n75\nAccuracy\n***\n20\n30\n40\n50\nSwahili\nAccuracy\n***\n10\n20\n30\n40\n50\nWolof\nAccuracy\n***\n20\n30\n40\n50\nYoruba\nAccuracy\n***\n10\n20\n30\n40\n50\nZulu\nAccuracy\nMedGemma\nGlobMed-MedGemma\n40\n45\n50\n55\n60\n42.00\n57.30\n15.30\n18.37\nQwen3\nGlobMed-Qwen3\n40\n45\n50\n55\n60\n65\n62.17\n43.80\nPortuguese\nEnglish\nSpanish\nJapanese\nJapanese\nKorean\nPortuguese\nSpanish\nChinese\nEnglish\nFrench\nGerman\na.\nc.\nd.\nf.\nGlobMed-MedGemma-4B\nMedQA\nMMLU−Pro\nBioNLI\nMedExpQA\nMedNLI\nHeadQA\n0\n50\n100\nMedGemma\nGlobMed−MedGemma\nMedQA\nMMLU−Pro\nBioNLI\nMedExpQA\nMedNLI\nHeadQA\n0\n50\n100\nQwen\nGlobMed−Qwen\nb.\ne.\n"}, {"page": 19, "text": " \n19 \na slight decrease on HeadQA. c, Language-wise performance: GlobMed-MedGemma-4B \nachieved higher average accuracy across all 12 languages compared with MedGemma-4B, with \nparticularly notable improvements in low-resource languages. d, GlobMed-Qwen3-4B overall \nperformance: Average accuracy across all benchmarks and languages improved from 43.80% \nto 62.17% compared with Qwen3-4B. e, Task-wise performance: GlobMed-Qwen3-4B \nconsistently outperformed Qwen3-4B across all medical benchmarks. f, Language-wise \nperformance: GlobMed-Qwen3-4B achieved higher average accuracy across all 12 languages \ncompared with Qwen3-4B, with particularly notable improvements in low-resource languages. \nStatistical significance is indicated by asterisks (*p<0.05, **p<0.01, ***p<0.001). \n \nDiscussion \nLLMs are rapidly transforming medical AI, yet their development has unintentionally \nwidened global disparities in access to medical information, particularly in regions \nwhere underrepresented languages are spoken. Building medical AI systems that can \neffectively serve linguistically diverse populations is, therefore, not only a technical \nchallenge but also a critical step toward improving global health equity. \n \nIn this study, we address this gap through three key contributions: (1) GlobMed, the \nlargest multilingual medical dataset to date spanning 12 languages (including four low-\nresource languages) and continuously expanding to 20 languages; (2) GlobMed-Bench, a \nlarge-scale evaluation benchmark assessing multilingual medical capabilities across 56 \nproprietary and open-weight LLMs; and (3) GlobMed-LLMs, a suite of fine-tuned LLMs, \nscaling from 1.7B to 8B parameters, which substantially enhance performance and \nreduce cross-lingual disparities. \n \nSystematic evaluation on GlobMed-Bench revealed distinct performance trends \nbetween proprietary and open-weight LLMs. Proprietary LLMs consistently achieved \nhigh accuracy within a narrow range, whereas open-weight LLMs exhibited broader \nvariability. These differences likely reflect disparities in training data, computing \nresources, and optimization strategies. Proprietary LLMs are typically developed by \nwell-resourced technology companies, whereas many open-weight LLMs originate from \nacademic teams with more limited resources. \n \n"}, {"page": 20, "text": " \n20 \nCross-lingual analysis highlighted persistent performance gaps across languages. \nModels generally achieved better performance in high-resource languages (e.g., English, \nFrench, German, Portuguese, Spanish), reflecting their greater representation in \npretraining data. In contrast, underrepresented languages, particularly Wolof, Yoruba, \nand Zulu, remained challenging for all evaluated models. Notably, model performance \nshowed certain associations with their development context. For instance, LLMs \ndeveloped in China performed particularly well in Chinese, suggesting strong language-\nspecific adaptation. \n \nMedical LLMs exhibited substantial variability in performance, with only a small subset \ndemonstrating clear advantages. This suggests that fine-tuning on medical data alone is \ninsufficient; effective domain adaptation likely requires high-quality pretraining, \noptimized training strategies, and careful incorporation of domain knowledge. \nMeanwhile, LLMs equipped with explicit reasoning mechanisms showed significant \ngains across multiple multilingual medical benchmarks, though their higher \ncomputational demands and longer inference times may limit practical deployment in \nresource-constrained settings. \n \nGlobMed-LLMs achieved significant improvements over baseline models, offering a \npossible path toward globally deployable medical AI. Despite their relatively modest \nsizes, GlobMed-LLMs outperformed much larger LLMs. This efficiency substantially \nreduces deployment barriers and operational costs, a critical consideration for under-\nresourced regions.  \n \nTo summarize, this work lays the groundwork for globally deployable medical AI \nsystems and marks a step toward broader access to AI-driven health care. Achieving \ntruly global deployment, however, will require sustained collaboration among \nacademia, industry, health care institutions, and governments to expand language \ncoverage, enhance cultural and contextual adaptation, optimize computational \nefficiency, rigorously validate safety in real-world clinical settings, and establish robust \nethical and regulatory frameworks. Through such coordinated, long-term efforts, the \nvision of medical AI that serves and benefits every language community globally can be \nrealized. \n"}, {"page": 21, "text": " \n21 \n \nLimitations \nFirst, although GlobMed currently covers languages representing over 75% of the global \npopulation, its overall scale and linguistic coverage remain limited relative to the global \nlandscape. Many low-resource languages still lack sufficient medical data, limiting the \napplicability of the proposed models across diverse medical scenarios. Second, GlobMed \nis primarily focused on QA tasks, including NLI, MCQA, and long-form QA. While these \ntasks effectively assess medical knowledge understanding and reasoning, they do not \nfully reflect real-world clinical applications, such as medical report generation or multi-\nturn physician-patient interactions. Meanwhile, dedicated safety evaluation tasks under \nmultilingual settings are still lacking. Finally, the current model training framework \nrelies solely on post-training and lacks a multilingual pre-training stage specifically \ntailored to the medical domain. This limitation hinders the establishment of a fully \nbalanced understanding of multilingual medical knowledge. Addressing these \nchallenges will be crucial for developing more comprehensive, equitable, and clinically \nrelevant multilingual medical AI systems in the future. \n \nMethods \nGlobMed: Constructing the Multilingual Medical Dataset \nData Collection and Screening \nWe collected data encompassing three core tasks: NLI, long-form QA, and MCQA. \nSpecifically, the NLI task includes BioNLI43 and MedNLI44; the long-form QA task \nincludes ExpertQA-Bio45, ExpertQA-Med45, and LiveQA46; and the MCQA task includes \nHeadQA47, MedExpQA48, MedQA49, and MMLU-Pro50. All datasets are publicly accessible, \nexcept MedNLI, which can be accessed through the PhysioNet platform51. \n \nDespite the value of these resources for medical research, our manual review identified \nthree main quality issues within the original data: incompleteness (missing critical \ninformation), irrelevance (weak relevance to medicine), and disorganization \n(inconsistent formatting or structural irregularities). To ensure data reliability, we \nconducted a multi-stage quality control process on the original datasets, which \ncontained over 40,000 entries, resulting in the removal of 3,114 entries that did not \nmeet quality standards. Ambiguous cases were verified by medical experts. This \n"}, {"page": 22, "text": " \n22 \nrigorous screening established a high-quality foundation for subsequent multilingual \nmachine translation, LLM fine-tuning, and benchmark evaluation within the GlobMed \nframework. \n \nAgentic Machine Translation \nFollowing data screening, we developed a flexible agentic machine translation \nframework to expand GlobMed into multiple languages. The framework comprises \nthree stages. In the first stage, we utilized the “Medical NER Model”52 to extract medical \nentities from the original data, which were matched to translations from our custom-\nbuilt multilingual medical dictionary comprising approximately 350,000 translation \npairs for resource-available languages (Chinese, English, French, German, Japanese, \nKorean, Portuguese, Spanish). The extracted entities and dictionary translations, along \nwith the original text, were then provided to an LLM to generate an initial translation. In \nthe second stage, Expert Agent I reviewed the initial translation, identified semantic or \nstructural issues, and provided suggestions for refinement. In the third stage, Expert \nAgent II incorporated these suggestions to generate the final optimized translation. \nLeveraging this framework, GlobMed was expanded to 12 languages, including eight \nhigh-resource languages (Chinese, English, French, German, Japanese, Korean, \nPortuguese, and Spanish) and four low-resource languages (Swahili, Wolof, Yoruba, and \nZulu). \n \nFor the translator selection, we evaluated several LLMs and commercial products, \nincluding Claude-3.5-Sonnet53, GPT-4o-mini54, GPT-4o54, Google Translate55, and DeepL \nTranslate56. Multiple medical experts independently evaluated translation quality, \nincluding terminology precision, semantic consistency, and content fluency. The \nevaluation results demonstrated that Claude-3.5-Sonnet53 achieved the best overall \nperformance and was therefore adopted as the primary translator. As more advanced \nLLMs became available, we subsequently upgraded to Claude-4.0-Sonnet29, further \nimproving the quality and stability of multilingual translation. \n \nExpert Evaluation \nTo further mitigate potential biases introduced by machine translation and enhance \ndata reliability, we implemented an expert evaluation process. Topic modeling57 was \n"}, {"page": 23, "text": " \n23 \napplied to select representative samples from multiple thematic clusters for each task. \nFor the multilingual data, each entry was independently evaluated by at least two \nbilingual medical experts proficient in the corresponding language. During evaluation, \nexperts scored entries on accuracy, fluency, and completeness using a five-point (1-5) \nscale to ensure linguistic accuracy and clinical validity. \n \nGlobMed-Bench: Evaluating 56 LLMs Across 12 Languages \nLLM Evaluation \nTo systematically evaluate current LLMs on multilingual medical benchmarks, we \nconstructed GlobMed-Bench, incorporating proprietary LLMs, open-weight general \nLLMs, and open-weight medical-specific LLMs, covering both reasoning and non-\nreasoning variants. \n \nProprietary LLMs included the Anthropic series29,58 (Claude-3.5-Haiku, Claude-4.0-\nSonnet), Google's Gemini-2.5-Flash26, and the OpenAI series27,28,54,59 (GPT-4o-mini, GPT-\n4o, GPT-4.1-nano, GPT-4.1-mini, GPT-4.1, GPT-5-nano, GPT-5-mini, GPT-5, o4-mini). For \nthe OpenAI GPT-5 series, we set the “reasoning effort” to “minimal”. Open-weight \ngeneral LLMs comprised the DeepSeek series31,40 (DeepSeek-V3, DeepSeek-R1, \nDeepSeek-R1-Qwen3-8B), the Gemma series34 (Gemma-3-4B/12B/27B), gpt-oss \nseries30 (gpt-oss-20B/120B), the LLaMA series32,60–62 (LLaMA-3.1-8B/70B, LLaMA-3.2-\n3B, LLaMA-3.3-70B, LLaMA-4-Scout, LLaMA-4-Maverick), the Mistral series63,64 \n(Mistral-7B-v0.3, Mistral-Small-3.1-24B), the Phi series42 (Phi-4-mini, Phi-4 and their \ncorresponding reasoning variants), and the Qwen series41,65 (Qwen2.5-\n3B/7B/14B/72B, QwQ-32B, Qwen3-1.7B/4B/8B/14B and their corresponding thinking \nvariants). For Qwen3-1.7B, we set it to “non-thinking” mode. Open-weight medical-\nspecific LLMs included the MedGemma series33 (MedGemma-4B/27B), the HuatuoGPT \nseries35 (HuatuoGPT-o1-7B/8B/70B/72B), OpenBioLLM-8B38/70B39, Baichuan-M2-\n32B66, Bio-Medical-LLaMA3-8B36, MediPhi67, and MedReason-8B37. \n \nAll LLMs were evaluated on NLI and MCQA tasks in 12 languages, with five independent \nruns per evaluation to ensure statistical reliability. \n \nPrompt Design \n"}, {"page": 24, "text": " \n24 \nTo authentically capture LLM capabilities in multilingual medical scenarios, prompts \nwere delivered in the target language rather than mixed-language or English prompts, \nadhering to a strict language-consistency principle. This design is critical, as prompts in \nnon-target languages can introduce comprehension bias and fail to reliably measure \ntrue multilingual performance68. All prompt templates were carefully designed by \nbilingual medical experts for each of the 12 targeted languages to guarantee both \nlinguistic naturalness and cultural appropriateness. \n \nGlobMed-LLMs: Developing Multilingual Medical LLMs \nFine-Tuning Multilingual Medical LLMs \nIn the fine-tuning stage, we selected MedGemma-4B33 and Qwen3 series (1.7B, 4B, 8B)41 \nas baseline LLMs. These models represent the top-performing non-reasoning LLMs at \ncomparable parameter scales. Meanwhile, we focused on relatively small-parameter \nLLMs to improve accessibility in regions with limited AI infrastructure and low-\nresource medical communities. \n \nFull-parameter fine-tuning was applied with two complementary approaches: (1) Direct \nSupervised Fine-Tuning, which trained the LLMs directly on GlobMed. This approach \nsignificantly enhances the LLMs' instruction-following capability, demonstrating greater \nadaptability, particularly in low-resource language adaptability; (2) Distillation-\nEnhanced Supervised Fine-Tuning, which first leveraged gpt-oss-120B30 to distill high-\nquality reasoning processes and answers. Subsequently, GPT-528 was used to translate \nthe distilled data into 12 target languages, thereby creating a multilingual, reasoning-\nenhanced training set. Additionally, we implemented language randomization \n(assigning each training instance to a language at random) during training. This strategy \nhelps prevent overfitting to any single language and enhances the generalization. \n \nTraining Setup \nAll fine-tuning experiments were conducted on a server equipped with 16 NVIDIA H100 \nGPUs (96GB memory each). We employed full-parameter fine-tuning with an initial \nlearning rate of 1.0e-5, a global batch size of 256, and a single training epoch. Mixed \nprecision training (bfloat16) was employed to improve computational efficiency. The \nAdamW optimizer69 was selected with a weight decay coefficient of 0.0. For learning \n"}, {"page": 25, "text": " \n25 \nrate scheduling, we applied a cosine annealing strategy70 with a warmup71 period \nduring the first 10% of training steps. All experiments were implemented using the \nHugging Face Transformers framework72. \n \nData Availability \nThe GlobMed dataset constructed in this study is publicly available through the Hugging \nFace platform at https://huggingface.co/collections/ruiyang-medinfo/globmed. For \nMedNLI-related data, due to privacy protection requirements and institutional policies \ngoverning the use and distribution of MedNLI, please request access through the \nPhysioNet platform (https://physionet.org/content/mednli/1.0.0/). \n \nCode Availability \nAll code used for evaluation and training in this study will be made publicly available on \nGitHub at https://github.com/ruiyang-medinfo/GlobMed. However, the weights of \nGlobMed-LLMs cannot be released, as the training process incorporated MedNLI-related \ndata, which is subject to usage and distribution restrictions. \n \nAcknowledgements \nThis work was supported by the Duke-NUS Signature Research Programme funded by \nthe Ministry of Health, Singapore. Any opinions, findings and conclusions or \nrecommendations expressed in this material are those of the author(s) and do not \nreflect the views of the Ministry of Health. This work was supported by Innosuisse - \nSwiss Innovation Agency: Innovation project 55441.1 IP-ICT. This work was partially \nsupported by the NIH R01LM014344, R01LM014573 and R01LM014604. The findings \nand conclusions presented in this paper are those of the author(s) and do not \nnecessarily reflect the views of the NIH or the U.S. Department of Health and Human \nServices. Additionally, we thank Leticia Johnson from the World Health Organization for \nsupporting parts of the data evaluation, and Irene Li for providing partial translation \nAPIs and early-stage HPC resources, which were supported by JST ACT-X \n(JPMJAX24CU), JSPS KAKENHI (24K20832), Kyushu University Research Institute for \nInformation Technology through the HPCI System Research Project (hp250092), \n"}, {"page": 26, "text": " \n26 \nNVIDIA Academic Grant Programme, Google Cloud (Gemma 3 Academic Programme), \nand Google Research Scholar Award 2025.  \n \nCompeting Interests \nThe authors declare no competing interests. \n \nReference \n1. \nJamison, D. T. et al. Global health 2035: a world converging within a generation. \nLancet 382, 1898–1955 (2013). \n2. \nYang, R. et al. Disparities in clinical studies of AI enabled applications from a global \nperspective. NPJ Digit Med 7, 209 (2024). \n3. \nKruk, M. E. et al. High-quality health systems in the Sustainable Development Goals \nera: time for a revolution. Lancet Glob Health 6, e1196–e1252 (2018). \n4. \nYao, R. et al. Inequities in Health Care Services Caused by the Adoption of Digital \nHealth Technologies: Scoping Review. J Med Internet Res 24, e34144 (2022). \n5. \nThirunavukarasu, A. J. et al. Large language models in medicine. Nat Med 29, 1930–\n1940 (2023). \n6. \nYang, R. et al. Large language models in health care: Development, applications, and \nchallenges. Health Care Sci 2, 255–263 (2023). \n7. \nYang, R. et al. Ascle-A Python Natural Language Processing Toolkit for Medical Text \nGeneration: Development and Evaluation Study. J Med Internet Res 26, e60601 \n(2024). \n8. \nWan, P. et al. Outpatient reception via collaboration between nurses and a large \nlanguage model: a randomized controlled trial. Nat Med 30, 2878–2885 (2024). \n9. \nGoh, E. et al. GPT-4 assistance for improvement of physician performance on \npatient care tasks: a randomized controlled trial. Nat Med 31, 1233–1238 (2025). \n10. Khasentino, J. et al. A personal health large language model for sleep and fitness \ncoaching. Nat Med (2025) doi:10.1038/s41591-025-03888-0. \n11. Liang, W. et al. Can large language models provide useful feedback on research \npapers? A large-scale empirical analysis. NEJM AI 1, (2024). \n12. Yang, R. et al. Enabling inclusive systematic reviews: incorporating preprint articles \nwith large language model-driven evaluations. J Am Med Inform Assoc (2025).  \n13. Scherbakov, D., Hubig, N., Jansari, V., Bakumenko, A. & Lenert, L. A. The emergence \n"}, {"page": 27, "text": " \n27 \nof large language models as tools in literature reviews: a large language model-\nassisted systematic review. J. Am. Med. Inform. Assoc. 32, 1071–1086 (2025). \n14. Wang, H. et al. An evaluation framework for ambient digital scribing tools in clinical \napplications. NPJ Digit. Med. 8, 358 (2025). \n15. Yang, R. et al. Graphusion: A RAG framework for scientific knowledge graph \nconstruction with a global perspective. in Companion Proceedings of the ACM on \nWeb Conference 2025 2579–2588 (ACM, New York, NY, USA, 2025). \n16. Yang, R. et al. Retrieval-augmented generation for generative artificial intelligence \nin health care. Npj Health Syst. 2, (2025). \n17. Akbarialiabad, H. et al. The utility of generative AI in advancing global health. NEJM \nAI 2, (2025). \n18. Localizing AI in the global south. Nat. Mach. Intell. (2025) doi:10.1038/s42256-\n025-01057-z. \n19. Wild, S. AI models are neglecting African languages - scientists want to change that. \nNature (2025) doi:10.1038/d41586-025-02292-5. \n20. NLLB Team. Scaling neural machine translation to 200 languages. Nature 630, 841–\n846 (2024). \n21. Brown, T. et al. Language Models are Few-Shot Learners. Advances in Neural \nInformation Processing Systems 33, 1877–1901 (2020). \n22. Qiu, P. et al. Towards building multilingual language model for medicine. Nat \nCommun 15, 8384 (2024). \n23. Xuan, W. et al. MMLU-ProX: A multilingual benchmark for advanced large language \nmodel evaluation. arXiv [cs.CL] (2025) doi:10.48550/ARXIV.2503.10497. \n24. Wu, J. et al. Clinical text datasets for medical artificial intelligence and large \nlanguage models — A systematic review. NEJM AI 1, (2024). \n25. World Population Review. Total Population by Country 2025. World Population \nReview https://worldpopulationreview.com/countries (2025). \n26. Google DeepMind. Gemini 2.5 Flash. Google DeepMind \nhttps://deepmind.google/models/gemini/flash/ (2025). \n27. OpenAI. Introducing o3 and o4 mini. OpenAI \nhttps://openai.com/index/introducing-o3-and-o4-mini/ (2025). \n28. OpenAI. GPT-5. OpenAI https://openai.com/gpt-5/ (2025). \n29. Anthropic. Introducing Claude 4. Anthropic \n"}, {"page": 28, "text": " \n28 \nhttps://www.anthropic.com/news/claude-4 (2025).  \n30. OpenAI et al. gpt-oss-120b & gpt-oss-20b Model Card. arXiv [cs.CL] (2025) \ndoi:10.48550/ARXIV.2508.10925. \n31. DeepSeek-AI et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via \nReinforcement Learning. arXiv [cs.CL] (2025) doi:10.48550/ARXIV.2501.12948. \n32. Meta AI. The Llama 4 herd: The beginning of a new era of natively multimodal AI \ninnovation. Meta AI https://ai.meta.com/blog/llama-4-multimodal-intelligence/ \n(2025). \n33. Sellergren, A. et al. MedGemma Technical Report. arXiv [cs.AI] (2025) \ndoi:10.48550/ARXIV.2507.05201. \n34. Gemma Team et al. Gemma 3 Technical Report. arXiv [cs.CL] (2025) \ndoi:10.48550/ARXIV.2503.19786. \n35. Chen, J. et al. HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs. arXiv \n[cs.CL] (2024) doi:10.48550/ARXIV.2412.18925. \n36. Bio-Medical-Llama-3-8B. Hugging Face https://huggingface.co/ContactDoctor/Bio-\nMedical-Llama-3-8B (2025). \n37. Wu, J. et al. MedReason: Eliciting factual medical reasoning steps in LLMs via \nknowledge graphs. arXiv [cs.CL] (2025) doi:10.48550/ARXIV.2504.00993. \n38. Llama3-OpenBioLLM-8B. Hugging Face https://huggingface.co/aaditya/Llama3-\nOpenBioLLM-8B (2025). \n39. Llama3-OpenBioLLM-70B. Hugging Face https://huggingface.co/aaditya/Llama3-\nOpenBioLLM-70B (2025). \n40. DeepSeek-AI et al. DeepSeek-V3 Technical Report. arXiv [cs.CL] (2024) \ndoi:10.48550/ARXIV.2412.19437. \n41. Yang, A. et al. Qwen3 Technical Report. arXiv [cs.CL] (2025) \ndoi:10.48550/ARXIV.2505.09388. \n42. Abdin, M. et al. Phi-4 Technical Report. arXiv [cs.CL] (2024) \ndoi:10.48550/ARXIV.2412.08905. \n43. Bastan, M., Surdeanu, M. & Balasubramanian, N. BioNLI: Generating a Biomedical \nNLI Dataset Using Lexico-semantic Constraints for Adversarial Examples. in \nFindings of the Association for Computational Linguistics: EMNLP 2022 5093–5104 \n(Association for Computational Linguistics, Stroudsburg, PA, USA, 2022). \n44. Romanov, A. & Shivade, C. Lessons from natural language inference in the clinical \n"}, {"page": 29, "text": " \n29 \ndomain. in Proceedings of the 2018 Conference on Empirical Methods in Natural \nLanguage Processing (Association for Computational Linguistics, Stroudsburg, PA, \nUSA, 2018). doi:10.18653/v1/d18-1187. \n45. Malaviya, C. et al. ExpertQA: Expert-curated questions and attributed answers. in \nProceedings of the 2024 Conference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies (Volume 1: Long \nPapers) (Association for Computational Linguistics, Stroudsburg, PA, USA, 2024). \ndoi:10.18653/v1/2024.naacl-long.167. \n46. Ben Abacha, A., Agichtein, E., Pinter, Y. & Demner-Fushman, D. Overview of the \nmedical question answering task at TREC 2017 LiveQA. in Proceedings of the \nTwenty-Sixth Text REtrieval Conference (TREC 2017) (2017). \n47. Vilares, D. & Gómez-Rodríguez, C. HEAD-QA: A Healthcare Dataset for Complex \nReasoning. in Proceedings of the 57th Annual Meeting of the Association for \nComputational Linguistics 960–966 (Association for Computational Linguistics, \nStroudsburg, PA, USA, 2019). \n48. Alonso, I., Oronoz, M. & Agerri, R. MedExpQA: Multilingual benchmarking of Large \nLanguage Models for Medical Question Answering. Artif Intell Med 155, 102938 \n(2024). \n49. Jin, D. et al. What disease does this patient have? A large-scale open domain \nquestion answering dataset from medical exams. Appl. Sci. (Basel) 11, 6421 (2021). \n50. Wang, Y. et al. MMLU-Pro: A More Robust and Challenging Multi-Task Language \nUnderstanding Benchmark. Advances in Neural Information Processing Systems 37, \n95266–95290 (2024). \n51. PhysioNet. PhysioNet https://physionet.org/ (2025). \n52. Medical-NER. Hugging Face https://huggingface.co/blaze999/Medical-NER (2025). \n53. Anthropic. Claude 3.5 Sonnet. Anthropic \nhttps://www.anthropic.com/news/claude-3-5-sonnet (2024). \n54. OpenAI et al. GPT-4o System Card. arXiv [cs.CL] (2024) \ndoi:10.48550/ARXIV.2410.21276. \n55. Google. Google Translate. Google https://translate.google.com/ (2025). \n56. DeepL. DeepL Translate: The world's most accurate translator. DeepL \nhttps://www.deepl.com/translator (2025). \n57. Grootendorst, M. BERTopic: Neural topic modeling with a class-based TF-IDF \n"}, {"page": 30, "text": " \n30 \nprocedure. arXiv [cs.CL] (2022) doi:10.48550/ARXIV.2203.05794. \n58. Anthropic. Claude Haiku 3.5. Anthropic https://www.anthropic.com/claude/haiku \n(2025). \n59. OpenAI. GPT-4. OpenAI https://openai.com/index/gpt-4-1/ (2025). \n60. Grattafiori, A. et al. The Llama 3 herd of models. arXiv [cs.AI] (2024) \ndoi:10.48550/ARXIV.2407.21783. \n61. Meta AI. Llama 3.2: Revolutionizing edge AI and vision with open, customizable \nmodels. Meta AI https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-\nmobile-devices/ (2024). \n62. Meta. Llama 3.3. Llama Documentation https://www.llama.com/docs/model-cards-\nand-prompt-formats/llama3_3/ (2024). \n63. Jiang, A. Q. et al. Mistral 7B. arXiv [cs.CL] (2023) doi:10.48550/ARXIV.2310.06825. \n64. Mistral AI. Mistral Small 3.1. Mistral AI https://mistral.ai/news/mistral-small-3-1 \n(2025). \n65. Qwen et al. Qwen2.5 Technical Report. arXiv [cs.CL] (2024) \ndoi:10.48550/ARXIV.2412.15115. \n66. M2 Team et al. Baichuan-M2: Scaling medical capability with large verifier system. \narXiv [cs.LG] (2025) doi:10.48550/ARXIV.2509.02208. \n67. Corbeil, J.-P. et al. A modular approach for clinical SLMs driven by synthetic data \nwith pre-instruction tuning, model merging, and clinical-tasks alignment. in \nProceedings of the 63rd Annual Meeting of the Association for Computational \nLinguistics (Volume 1: Long Papers) 19352–19374 (Association for Computational \nLinguistics, Stroudsburg, PA, USA, 2025). \n68. Kmainasi, M. B. et al. Native vs non-native language prompting: A comparative \nanalysis. arXiv [cs.CL] (2024) doi:10.48550/ARXIV.2409.07054. \n69. Loshchilov, I. & Hutter, F. Decoupled Weight Decay Regularization. in International \nConference on Learning Representations (2018). \n70. Loshchilov, I. & Hutter, F. SGDR: Stochastic Gradient Descent with Warm Restarts. \nin International Conference on Learning Representations (2017). \n71. Vaswani, A. et al. Attention is All you Need. Advances in Neural Information \nProcessing Systems 30, (2017). \n72. Hugging Face. Transformers. Hugging Face Documentation \nhttps://huggingface.co/docs/transformers/index (2025). \n"}, {"page": 31, "text": " \n1 \nToward Global Large Language Models in Medicine \nSupplementary Information \nContents \nS1. GlobMed .............................................................................................................................. 2 \nS1.1. Data Curation ..................................................................................................................... 2 \nS1.2. Data Statistics .................................................................................................................... 2 \nS1.3. Data Example ..................................................................................................................... 4 \nS1.4. Agentic Machine Translation Prompt .................................................................... 22 \nS1.5. Expert Evaluation Criteria .......................................................................................... 28 \nS2. GlobMed-Bench ............................................................................................................. 32 \nS2.1. LLM Information ............................................................................................................ 32 \nS2.2. Evaluation Prompt ........................................................................................................ 33 \nS2.3. Detailed Results of 56 Proprietary and Open-Weight LLMs ........................... 49 \nS2.4. Language Disparity Analysis ..................................................................................... 85 \nS3. GlobMed-LLMs ............................................................................................................. 141 \nS3.1. GlobMed-Qwen3-1.7B/8B through Direct Supervised Fine-Tuning .......... 141 \nS3.2. Performance Comparison between GlobMed-LLMs and Baseline LLMs .. 144 \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 32, "text": " \n2 \nS1. GlobMed \nS1.1. Data Curation \nOur manual review identified three major quality issues within the original data: \nincompleteness (missing critical information), irrelevance (weak relevance to \nmedicine), and disorganization (inconsistent formatting or structural \nirregularities), as shown in SFig. 1. To ensure data reliability, we conducted a \nmulti-stage quality control process to the original datasets, which contained over \n40,000 entries, resulting in the removal of 3,114 entries that did not meet quality \nstandards. Ambiguous cases were verified by medical experts. \n \n \nSFig. 1: Three major quality issues identified during manual review. \n \nS1.2. Data Statistics \nDataset \nTrain \nTest \nLicense \nOriginal Screened Original Screened \nBioNLI \n5,544 \n5,540 \n6,308 \n4,450 \nCC BY 4.0 \nMedNLI \n11,232 \n11,219 \n1,419 \n1,417 \nPhysioNet Credentialed Health Data License 1.5.0 \nExpertQA-Bio \n96 \n96 \n/ \n/ \nMIT License \nExpertQA-Med \n504 \n500 \n/ \n/ \nMIT License \nLiveQA \n634 \n627 \n/ \n/ \nPublic release for research in medical QA \nHeadQA \n2,657 \n2,164 \n2,742 \n2,218 \nMIT License \nMedExpQA \n434 \n434 \n125 \n125 \nCC BY 4.0 \nMedQA \n10,178 \n10,166 \n1,273 \n1,273 \nMIT License \nMMLU-Pro \n573 \n434 \n245 \n187 \nMIT License \nSTab. 1: Statistics on the number of screened datasets. \nQuestion:\nA study is performed to assess the intelligence quotient and the crime rate in a neighborhood. \nStudents at a local high school are given an assessment and their criminal and disciplinary \nrecords are reviewed. One of the subjects scores 2 standard deviations over the mean. What \npercent of students did he score higher than? \nQuestion:\nA study is performed to assess the intelligence quotient and the crime rate in a neighborhood. \nStudents at a local high school are given an assessment and their criminal and disciplinary \nrecords are reviewed. One of the subjects scores 2 standard deviations over the mean. What \npercent of students did he score higher than? \nPremise:\nNow returns to the emergency department after increased lethargy and abdominal pain over \nthe last two weeks.\nHypothesis:\nThe patient has\nAnswer\nLifestyle changes often can help relieve acute (short-term) insomnia. These changes might \nmake it easier to fall asleep and stay asleep.\n              \nA type of counseling called cognitive-\nbehavioral therapy (CBT) can help relieve the anxiety linked to chronic (ongoing) insomnia. \nAnxiety tends to prolong insomnia. \nIncompleteness\n Disorganization\nIrrelevance\n"}, {"page": 33, "text": " \n3 \nSTab. 2–4 report the average number of tokens across 12 languages in the \ndatasets for the three tasks. All token counts were calculated using the NLLB-\n200. \nAverage Tokens \nBioNLI \nMedNLI \ntrain \ntest \ntrain \ntest \nChinese \n417 \n443 \n49 \n47 \nEnglish \n406 \n435 \n40 \n39 \nFrench \n528 \n563 \n56 \n55 \nGerman \n509 \n545 \n55 \n53 \nJapanese \n434 \n467 \n49 \n47 \nKorean \n434 \n466 \n49 \n47 \nPortuguese \n471 \n499 \n51 \n49 \nSpanish \n474 \n504 \n51 \n49 \nSwahili \n470 \n495 \n50 \n49 \nWolof \n477 \n496 \n55 \n53 \nYoruba \n564 \n584 \n66 \n65 \nZulu \n510 \n539 \n54 \n53 \nSTab. 2: Average number of tokens in NLI datasets across 12 languages. \n \nAverage Tokens \nExpertQA-Bio \nExpertQA-Med \nLiveQA \nChinese \n252 \n241 \n309 \nEnglish \n248 \n240 \n309 \nFrench \n339 \n324 \n421 \nGerman \n324 \n317 \n400 \nJapanese \n247 \n232 \n298 \nKorean \n249 \n238 \n303 \nPortuguese \n294 \n281 \n360 \nSpanish \n293 \n281 \n360 \nSwahili \n302 \n292 \n368 \nWolof \n341 \n334 \n421 \nYoruba \n384 \n379 \n471 \nZulu \n320 \n312 \n392 \nSTab. 3: Average number of tokens in long-form QA datasets across 12 languages. \n \nAverage Tokens \nHeadQA \nMedQA \nMedExpQA \nMMLU-Pro \ntrain \ntest \ntrain \ntest \ntrain \ntest \ntrain \ntest \nChinese \n96 \n91 \n223 \n228 \n174 \n176 \n155 \n153 \nEnglish \n98 \n92 \n224 \n230 \n178 \n181 \n158 \n151 \nFrench \n118 \n112 \n282 \n289 \n215 \n221 \n194 \n188 \nGerman \n116 \n110 \n271 \n276 \n212 \n217 \n188 \n183 \nJapanese \n97 \n91 \n223 \n228 \n173 \n176 \n155 \n152 \nKorean \n95 \n90 \n222 \n227 \n171 \n174 \n153 \n151 \nPortuguese \n106 \n100 \n247 \n252 \n190 \n194 \n171 \n169 \nSpanish \n123 \n117 \n249 \n255 \n219 \n224 \n174 \n171 \nSwahili \n109 \n103 \n259 \n266 \n201 \n207 \n184 \n175 \nWolof \n115 \n109 \n271 \n279 \n205 \n211 \n191 \n181 \nYoruba \n143 \n135 \n346 \n354 \n265 \n272 \n234 \n219 \nZulu \n118 \n112 \n280 \n287 \n220 \n226 \n201 \n190 \nSTab. 4: Average number of tokens in MCQA datasets across 12 languages. \n"}, {"page": 34, "text": " \n4 \nS1.3. Data Example \nNLI \n \n前提: \n七个实验室合作研究两种中等纯度纤溶酶原制剂（64/23，63/6），观察\n到激活剂（尿激酶或链激酶）的量和纤溶酶原的激活时间影响生成的纤溶\n酶量。使用酪蛋白和合成多肽（S-2251）作为底物，作者随后证明，在不\n因纤溶酶自身消化作用而导致活性损失的情况下，很难实现纤溶酶原的完\n全激活。 \n \n假设: \n通过比较各种AKT 诱导激活混合物的多肽亚基（在SDS 电泳）与其纤溶\n酶活性，可以得出结论：在AKT 诱导[物]产生最大量纤溶酶时，一些AKT\n诱导物仍以非活性AKT 诱导中间体（PLG-i）的形式存在 \n \n答案: 矛盾 \nSTab. 5: NLI data example in Chinese. \n \n \n \n \nPremise: \nSeven laboratories collaborating in a study of two intermediate purity \nplasminogen preparations (64/23, 63/6) observed that the amount of \nactivator (urokinase or streptokinase) and the time of activation of \nplasminogen influenced the amount of plasmin generated. Using casein and \na synthetic polypeptide (S-2251) as substrates, the authors subsequently \nshowed that complete activation of plasminogen was difficult to achieve \nwithout activity losses due to plasmin autodigestion. \n \nHypothesis: \nComparison of the polypeptide subunits (on SDS electrophoresis) of the \nvarious AKT-induced activation mixtures with their plasmin activity \nallowed the conclusion that at maximum generation of plasmin from AKT-\ninduced, some AKT-induced remains in the form of an inactive AKT-\ninduced intermediate (PLG-i). \n \nAnswer: contradiction \nSTab. 6: NLI data example in English. \n \n \n \n"}, {"page": 35, "text": " \n5 \n \nPrémisse: \nSept laboratoires collaborant à une étude sur deux préparations de \nplasminogène de pureté intermédiaire (64/23, 63/6) ont observé que la \nquantité d'activateur (urokinase ou streptokinase) et le temps d'activation \ndu plasminogène influençaient la quantité de plasmine générée. En utilisant \nla caséine et un polypeptide synthétique (S-2251) comme substrats, les \nauteurs ont par la suite montré qu'une activation complète du \nplasminogène était difficile à réaliser sans pertes d'activité dues à \nl'autodigestion de la plasmine. \n \nHypothèse: \nLa comparaison des sous-unités polypeptidiques (par électrophorèse SDS) \ndes différents mélanges d'activation induits par l'AKT avec leur activité \nplasmine a permis de conclure que lors de la génération maximale de \nplasmine à partir du plasminogène induit par l'AKT, une partie du \nplasminogène induit par l'AKT reste sous forme d'un intermédiaire inactif \n(PLG-i, plasminogène intermédiaire). \n \nRéponse: Contradiction \nSTab. 7: NLI data example in French. \n \n \nPrämisse: \nSieben Labore, die an einer Studie zu zwei Plasminogenpräparaten \nmittleren Reinheitsgrades (64/23, 63/6) zusammenarbeiteten, \nbeobachteten, dass die Menge der Aktivatorsubstanz (Urokinase oder \nStreptokinase) und die Aktivierungszeit des Plasminogens die Menge des \nerzeugten Plasmins beeinflussten. Unter Verwendung von jeweils Casein \nund einem synthetischen Polypeptid (S-2251) als Substrate zeigten die \nAutoren anschließend, dass eine vollständige Aktivierung des \nPlasminogens schwierig zu erzielen war, ohne dass es zu \nAktivitätseinbußen durch Plasmin-Auto-Digestion kam. \n \nHypothese: \nDer Vergleich der Polypeptid-Untereinheiten (mittels SDS-Elektrophorese) \nder verschiedenen AKT-induzierten Aktivierungsgemische mit ihrer \nPlasminaktivität ermöglichte folgende Schlussfolgerung: Bei maximaler \nPlasminbildung aus AKT-induziertem Plasminogen verbleibt ein Teil des \nAKT-induzierten Plasminogens in Form eines inaktiven AKT-induzierten \nZwischenprodukts (PLG-i, Plasminogen-Intermediat). \n \nAntwort: Widerspruch \nSTab. 8: NLI data example in German. \n \n"}, {"page": 36, "text": " \n6 \n \n前提: \n7 つの研究室が協力して、2 種類の中間純度プラスミノーゲン製剤\n（64/23、63/6）の研究を行った結果、活性化剤（ウロキナーゼまたは\nストレプトキナーゼ）の量とプラスミノーゲンの活性化時間が生成され\nるプラスミンの量に影響を与えることを観察した。カゼインと合成ポリ\nペプチド（S-2251）を基質として用いた後続の研究で、著者らはプラス\nミンの自己消化による活性の損失なしにプラスミノーゲンの完全な活性\n化を達成することが困難であることを示した。 \n \n仮説: \n様々なAKT 誘導型活性化混合物のポリペプチドサブユニット（SDS 電気\n泳動法による）をそのプラスミン活性と比較した。この比較により、以\n下の結論が導き出された。AKT 誘導型からのプラスミン生成が最大に達\nした時点でも、一部のAKT 誘導型が不活性のAKT 誘導型中間体（PLG-\ni、プラスミノゲン中間体）の形で残っている。 \n \n仮説: 矛盾 \nSTab. 9: NLI data example in Japanese. \n \n \n \n \n전제: \n두 가지 중간 순도의 플라스미노겐 제제(64/23, 63/6)에 대한 연구에 \n협력한 7 개 실험실에서 활성제(우로키나제 또는 스트렙토키나제)의 \n양과 플라스미노겐의 활성화 시간이 생성된 플라스민의 양에 영향을 \n미친다는 것을 관찰하였다. 이후 저자들은 카세인과 합성 \n폴리펩타이드(S-2251)를 기질로 사용하여 연구를 진행하였다. 그 결과, \n플라스민의 자가소화(자가분해)로 인한 활성도 손실 없이 \n플라스미노겐의 완전한 활성화를 달성하기 어렵다는 것을 보여주었다. \n \n가설: \n다양한 AKT-유도 활성화 혼합물의 폴리펩티드 소단위체(SDS \n전기영동법 상에서)를 플라스민 활성과 비교한 결과, AKT-유도로부터 \n플라스민이 최대로 생성되는 시점에서도 일부 AKT-유도는 비활성 \nAKT-유도 중간체(PLG-i, 플라스미노겐 중간체) 형태로 남아있다는 \n결론을 내릴 수 있었다. \n \n답변: 모순 \nSTab. 10: NLI data example in Korean. \n \n \n \n"}, {"page": 37, "text": " \n7 \n \nPremissa: \nSete laboratórios que colaboraram em um estudo de duas preparações de \nplasminogênio de pureza intermediária (64/23, 63/6) observaram que a \nquantidade de ativador (uroquinase ou estreptoquinase) e o tempo de \nativação do plasminogênio influenciaram a quantidade de plasmina gerada. \nUsando caseína e um polipeptídeo sintético (S-2251) como substratos, os \nautores subsequentemente demonstraram que a ativação completa do \nplasminogênio era difícil de ser alcançada sem perdas de atividade devido à \nauto-digestão da plasmina. \n \nHipótese: \nA comparação das subunidades polipeptídicas (por eletroforese SDS) das \nvárias misturas de ativação AKT-induzidas com sua atividade de plasmina \npermitiu concluir que, na geração máxima de plasmina a partir de AKT-\ninduzida, alguma indução por AKT permanece na forma de um \nintermediário AKT-induzido inativo (PLG-i). \n \nResposta: Contradição \nSTab. 11: NLI data example in Portuguese. \n \n \n \nPremisa: \nSiete laboratorios que colaboraron en un estudio de dos preparaciones de \nplasminógeno de pureza intermedia (64/23, 63/6) observaron que la \ncantidad de activador (urocinasa o estreptocinasa) y el tiempo de \nactivación del plasminógeno influían en la cantidad de plasmina generada. \nUtilizando caseína y un polipéptido sintético (S-2251) como sustratos, los \nautores posteriormente demostraron que era difícil lograr la activación \ncompleta del plasminógeno sin pérdidas de actividad debido a la \nautodigestión de la plasmina. \n \nHipótesis: \nLa comparación de las subunidades polipeptídicas (en electroforesis en \nSDS) de las diversas mezclas de activación AKT-inducida de plasminógeno \ncon su actividad de plasmina permitió concluir que, en el punto de máxima \ngeneración de plasmina a partir de AKT-inducida, una parte del AKT-\ninducido permanece en forma de un intermediario AKT-inducido inactivo \n(PLG-i). \n \nRespuesta: Contradicción \nSTab. 12: NLI data example in Spanish. \n \n \n"}, {"page": 38, "text": " \n8 \n \nHoja: \nMaabara saba zilizoshirikiana katika utafiti wa maandalizi mawili ya upeo \nwa kati wa plasminojeni (64/23, 63/6) zilibaini kwamba kiwango cha \nkiamsho (urokinasi au streptokinasi) na muda wa kuamsha plasminojeni \nviliathiri kiwango cha plasmini kilichozalishwa. Kwa kutumia kaseini na \npolipeptidi ya sintetiki (S-2251) kama substrati, waandishi baadaye \nwalionyesha kwamba uamsho kamili wa plasminojeni ulikuwa mgumu \nkupatikana bila upotezaji wa utendaji kutokana na kujimeng'enya kwa \nplasmini. \n \nWazo: \nUlinganisho wa vitengo vidogo vya polipeptidi (kwenye electrophoresis ya \nSDS) vya mchanganyiko mbalimbali wa AKT-iliyochochewa pamoja na \nshughuli zao za plasmin uliruhusu hitimisho kwamba wakati wa uzalishaji \nwa juu wa plasmin kutoka AKT-iliyochochewa, baadhi ya AKT-\niliyochochewa inabaki katika muundo wa kati wa AKT-iliyochochewa \nisiyofanya kazi (PLG-i). \n \nJibu: Kupingana \nSTab. 13: NLI data example in Swahili. \n \n \n \nDigle: \nJuróom ñaari làboratwaar yu bokk ci natt gu ñuy def ci ñaari jumtukaay yu \nsét yu plasminogen (64/23, 63/6) gis nañu ne limu activateur bi (urokinase \nwalla streptokinase) ak jamono ji ñuy yokk doxiinu plasminogen bi am na \nsolo ci limu plasmin bi ñu meññal. Ñu jëfandikoo casein ak peptide (S-\n2251) ni substrat yi, bindkat yi mën nañu wone ne yokk doxiinu \nplasminogen ba mu mat dafa jafe ndax ñàkk doxiin ndax plasmin bi di lekk \nboppam. \n \nNjortu: \nMoñ yu polypeptide subunit yi (ci SDS electrophoresis) ci xeeti jaxasé AKT-\ninduced activation ak seen activité plasmin dafa may xalaat ne bu plasmin \ngénnee ba mu eppeeku ci AKT-induced, am na AKT-induced bu des ci xeetu \nAKT-induced bu taxul dara (PLG-i). \n \nTontu: Dëddu \nSTab. 14: NLI data example in Wolof. \n \n \n \n"}, {"page": 39, "text": " \n9 \n \nÌsẹ̀lẹ̀: \nÀwọn ilé-iṣẹ́ tẹ̀síìgì méje tí wọ́n ṣe ìforúkọsílẹ̀ papọ̀ ní ìwádìí lórí àwọn \nìgbékalẹ̀ méjì tí a mọ̀ gẹ́gẹ́ bí intermediate purity plasminogen (64/23, \n63/6) ṣe àkíyèsí pé iye activator (urokinase tàbí streptokinase) àti àkókò \nactivation tí plasminogen ni ìpaṣẹ lórí iye plasmin tí a fi lésí. Nípa lílo casein \nàti synthetic polypeptide (S-2251) gẹ́gẹ́ bí substrates, àwọn onímọ̀ yìí fi hàn \nlẹ́hìnná pé activation tó yẹ kí ó pé fún plasminogen ṣòro láti ṣe láìsí \npípadànù iṣẹ́ nítorí plasmin autodigestion. \n \nÌbéèrè: \nÌfiwéra àwọn abẹṣẹ́ polypeptide (lórí SDS electrophoresis) ti àwọn ìwọ̀n \nìdàpọ̀ ìmúnára AKT pẹ̀lú ìṣe plasmin wọn fàyè gba ìparí pé ní ìpele \nàgbàyégbà ìpilẹ̀ṣẹ̀ plasmin láti AKT-induced, díẹ̀ nínú AKT-induced ṣì wà ní \nìrísí àárín-gbùngbùn AKT-induced tí kò ṣiṣẹ́ (PLG-i). \n \nÌdáhùn: Ìlòdì \nSTab. 15: NLI data example in Yoruba. \n \n \n \nIsitatimende: \nAmalabhorethri ayisikhombisa abebambisene ocwaningweni lwezinhlobo \nezimbili ezingangokuphelele zokuhlelwa kweplasminogen (64/23, 63/6) \nbabona ukuthi inani lama-activator (i-urokinase noma i-streptokinase) \nkanye nesikhathi sokusebenza kweplasminogen kuye kube nomthelela \nkwinani leplasmin elakhiwayo. Ngokusebenzisa i-casein kanye ne-synthetic \npolypeptide (S-2251) njengezinto eziyisisekelo, ababhali kamuva babonisa \nukuthi ukusebenza okuphelele kweplasminogen bekunzima ukufinyeleleka \nngaphandle kokulahlekelwa wumsebenzi ngenxa ye-autodigestion \nyeplasmin. \n \nUmqondo: \nUkuqhathaniswa kwamayunithi amancane epholipeptidi (ku-SDS \nelectrophoresis) inhlanganisela ezahlukene eziqaliswe yi-AKT nezenzo \nzazo ze-plasmin kuvumele isiphetho sokuthi ekukhiqizweni okukhulu kwe-\nplasmin esuka kwi-AKT-induced, i-AKT-induced ethile isasele esimweni se-\nAKT-induced intermediate engasebenzi (PLG-i). \n \nImpendulo: Ukuphikisana \nSTab. 16: NLI data example in Zulu. \n \n \n \n"}, {"page": 40, "text": " \n10 \nLong-form QA \n \n问题: \n一位6 个月大的患儿由母亲带来就诊，母亲报告孩子生长发育迟缓，以及\n牙列异常、反复呕吐、动作不协调和难以控制的癫痫发作。体格检查显示\n患儿有脂溢性皮炎、小头畸形和下颌异常。根据患者表现的临床数据，患\n者呈现的是什么综合征？ \n \n答案: \n根据所呈现的临床数据，患者似乎患有西症候群 (West Syndrome)，也称\n为婴儿痉挛症 (Infantile Spasms)。西症候群是一种罕见的癫痫性障碍，\n特征包括生长迟缓、动作不协调、难以控制的发作（婴儿痉挛）、智力障\n碍和其他症状。脂溢性皮炎、小头症和突出的下巴也可能是该综合征的指\n示性特征。 \nSTab. 17: Long-Form QA data example in Chinese. \n \n \n \n \n \nQuestion: \nA 6-month-old patient is taken to consult by his mother, who reports \ndelayed growth in her son, as well as changes in the dentition, recurrent \nvomiting, uncoordinated movements, and difficult-to-control seizures. On \nphysical examination, she is noted to have seborrheic skin, microcephaly, \nand a promiscuous jaw. Based on the clinical data presented by the patient, \nwhat syndrome does the patient present? \n \nAnswer: \nBased on the clinical data presented, the patient appears to have West \nSyndrome, also known as Infantile Spasms. West Syndrome is a rare \nepileptic disorder characterized by delayed growth, uncoordinated \nmovements, difficult-to-control seizures (infantile spasms), intellectual \ndisability, and other symptoms. Seborrheic skin, microcephaly, and a \nprominent jaw may also be indicative of this syndrome. \nSTab. 18: Long-Form QA data example in English. \n \n \n \n \n \n"}, {"page": 41, "text": " \n11 \n \nQuestion: \nUn patient de 6 mois est amené en consultation par sa mère, qui signale un \nretard de croissance, ainsi que des changements dans la dentition, des \nvomissements récurrents, des mouvements non coordonnés et des \nconvulsions difficiles à contrôler. À l'examen physique, on note une peau \nséborrhéique, une microcéphalie et une mâchoire lâche. Sur la base des \ndonnées cliniques présentées par le patient, quel syndrome présente-t-il ? \n \nRéponse: \nD'après les données cliniques présentées, le patient semble atteint du \nsyndrome de West, également connu sous le nom de spasmes infantiles \n(Infantile Spasms). Le syndrome de West est un trouble épileptique rare \ncaractérisé par un retard de croissance, des mouvements non coordonnés, \ndes crises épileptiques difficiles à maîtriser (spasmes infantiles), une \ndéficience intellectuelle et d'autres symptômes. Une peau séborrhéique, \nune microcéphalie et une mâchoire proéminente peuvent également être \nindicateurs de ce syndrome. \nSTab. 19: Long-Form QA data example in French. \n \n \n \nFrage: \nEin 6 Monate alter Patient wird von seiner Mutter zur Untersuchung \ngebracht, die von einer Wachstumsverzögerung ihres Sohnes berichtet, \nsowie Veränderungen des Zahnstatus, rezidivierendem Erbrechen, \nunkoordinierten Bewegungen und schwer kontrollierbaren Anfällen. Bei \nder körperlichen Untersuchung fallen eine seborrhoische Haut, \nMikrozephalie und ein vorspringender Kiefer auf. Basierend auf den \nklinischen Daten des Patienten, welches Syndrom weist der Patient auf? \n \nAntwort: \nBasierend auf den vorliegenden klinischen Daten scheint der Patient an \nWest-Syndrom zu leiden, auch bekannt als Infantile Spasmen. Das West-\nSyndrom ist eine seltene epileptische Erkrankung, die durch verzögertes \nWachstum, unkoordinierte Bewegungen, schwer zu kontrollierende Anfälle \n(Infantile Spasmen), geistige Behinderung und andere Symptome \ngekennzeichnet und charakterisiert ist. Seborrhoische Dermatitis, \nMikrozephalie und ein prominenter Kiefer können ebenfalls auf dieses \nSyndrom hinweisen. \nSTab. 20: Long-Form QA data example in German. \n \n \n \n"}, {"page": 42, "text": " \n12 \n \n質問: \n6 ヶ月の患者が母親に連れられて受診しました。母親は息子の成長遅\n延、歯列の変化、再発性の嘔吐、協調運動障害、制御困難な発作につい\nて報告しています。身体診察では、脂漏性の皮膚、小頭症、そして下顎\n前突が認められました。これらの臨床所見に基づいて、患者はどの症候\n群を呈していますか？ \n \n答え: \n提示された臨床データに基づくと、患者はウエスト症候群（別名：点頭\nてんかん（Infantile Spasms））を有しているように見受けられます。ウ\nエスト症候群は、発育遅延、協調運動障害、制御が困難な発作（点頭発\n作）、知的障害、およびその他の症状を特徴とする稀なてんかん性障害\nです。脂漏性皮膚、小頭症、および突出した顎も、この症候群を示唆す\nる可能性があります。 \nSTab. 21: Long-Form QA data example in Japanese. \n \n \n \n \n \n질문: \n6 개월 된 환자가 어머니와 함께 진료를 받으러 왔습니다. 어머니는 \n아들의 발육 지연과 함께 치열의 변화, 재발성 구토, 협응되지 않은 \n움직임, 그리고 조절이 어려운 발작 증상을 보고합니다. 진찰 시 \n지루성 피부염, 소두증, 그리고 비정상적으로 돌출된 턱이 관찰됩니다. \n환자가 나타내는 임상 데이터를 바탕으로, 환자가 나타내는 증후군은 \n무엇입니까? \n \n답변: \n제시된 임상 데이터를 바탕으로, 환자는 웨스트 증후군(West \nSyndrome), 또는 영아연축(Infantile Spasms)으로도 알려진 질환을 \n가지고 있는 것으로 보입니다. 웨스트 증후군은 성장 지연, 협응 운동 \n장애, 조절하기 어려운 발작(영아연축), 지적 발달 장애 및 기타 \n증상을 특징으로 하는 희귀 뇌전증성 장애입니다. 지루성 피부염, \n소두증, 그리고 돌출된 턱 또한 이 증후군의 징후일 수 있습니다. \nSTab. 22: Long-Form QA data example in Korean. \n \n \n \n \n \n"}, {"page": 43, "text": " \n13 \n \nPergunta: \nUm paciente de 6 meses é levado à consulta pela mãe, que relata atraso no \ncrescimento do filho, bem como alterações na dentição, vômitos \nrecorrentes, movimentos descoordenados e convulsões de difícil controle. \nNo exame físico, observa-se dermatite seborreica, microcefalia e mandíbula \nprotuberante. Com base nos dados clínicos apresentados pelo paciente, de \nqual síndrome suspeita-se que o paciente apresenta? \n \nResposta: \nCom base nos dados clínicos apresentados, o paciente parece ter Síndrome \nde West, também conhecida como Espasmos Infantis. A Síndrome de West é \num distúrbio epiléptico raro caracterizado por atraso no crescimento, \nmovimentos descoordenados, crises convulsivas de difícil controle \n(espasmos infantis), deficiência intelectual e outros sintomas. Pele \nseborreica, microcefalia e mandíbula proeminente podem também ser \nindicativos desta síndrome. \nSTab. 23: Long-Form QA data example in Portuguese. \n \n \n \n \nPregunta: \nUn paciente de 6 meses es llevado a consulta médica por su madre, quien \ninforma retraso en el crecimiento del paciente, así como cambios en la \ndentición, vómito recurrente, movimientos descoordinados y convulsiones \nde difícil control. En la exploración física, se observa piel seborreica, \nmicrocefalia y mandíbula prognática. Basándose en los datos clínicos \npresentados por el paciente, ¿qué síndrome presenta el paciente? \n \nRespuesta: \nBasándose en los datos clínicos presentados, el paciente parece tener \nSíndrome de West, también conocido como Espasmos Infantiles. El \nSíndrome de West es un trastorno epiléptico raro caracterizado por retraso \nen el crecimiento, movimientos descoordinados, convulsiones difíciles de \ncontrolar (conocidas como espasmos infantiles), discapacidad intelectual y \notros síntomas, entre otros. La piel seborreica, la microcefalia y una \nmandíbula prominente también pueden ser indicadores de este síndrome. \nSTab. 24: Long-Form QA data example in Spanish. \n \n \n \n \n"}, {"page": 44, "text": " \n14 \n \nSwali: \nMgonjwa wa miezi 6 anapelekwa kuonana na daktari na mama yake, \nambaye anaripoti kuchelewa kukua kwa mwanawe, pamoja na mabadiliko \nkatika meno, kutapika mara kwa mara, mwendo usiokuwa na uratibu, na \nkifafa kisichoweza kudhibitiwa kwa urahisi. Katika uchunguzi wa kimwili, \nanaonekana kuwa na ngozi yenye magamba, kichwa kidogo, na taya \niliyovimba. Kulingana na data za kitabibu zinazoonyeshwa na mgonjwa, ni \nmfumo wa dalili gani mgonjwa anaonyesha? \n \nJibu: \nKulingana na data za kitabibu zilizowasilishwa, mgonjwa anaonekana kuwa \nna Sindrome ya West, pia inajulikana kama Misukosuko ya Watoto \nWachanga. Sindrome ya West ni hali adimu ya kifafa inayojulikana kwa \nukuaji wa kuchelewa, mienendo isiyoratibiwa, misukosuko isiyodhibitiwa \nkwa urahisi (misukosuko ya watoto wachanga), ulemavu wa akili na \nkuzuiwa kwa ukuaji wa akili, na dalili zingine. Ngozi yenye mafuta mengi \n(seborrhea), utasa wa kichwa (kichwa kidogo kuliko kawaida), na taya \ninayojitokeza pia zinaweza kuashiria dalili za sindrome hii. \nSTab. 25: Long-Form QA data example in Swahili. \n \n \n \n \nLaaj: \nNdey liir bu am juróom benn weer indi na doom am bu góor ci doktoor bi, \nndax mu wax ne doom ji dafa yéex ci màgg, ak yëngu-yëngu yu baaxul ci \nbëñ yi, xëb xëb bu bari, yëngu-yëngu yu amul takku, ak jaxase yu jafe-jafe yu \nkenn mënul dajale. Bi ñu ko seetee, gis nañu deram bi dafa mel ni lu am diw \nbu bari, bopp bi dafa tuuti (mikrosefali), ak yaxu gémmiñ gu gëna yaatu. Bu \nfekkee ni nga seet xaaraay yi liir bi am, ban xeeti feebar la liir bi am? \n \nTontu: \nCi li ñu gis ci baatukaay bi, feebar bi mëna nekk Syndrome West, walla \nPerlu yu Ndaw. Syndrome West dafa nekk feebar bu ñaaw bu xeeti ci \nmàggate bu yées, yëngatu yu amul topplante, perlu yu metti ñu faj (perlu yu \nndaw), ñàkk xam-xam, ak yeneen balaawaan. Dërëm bu teel sew, bopp bu \nndaw (microcéphalie), ak sàggay bu génn it mën na nekk ci seeni màndarga. \nSTab. 26: Long-Form QA data example in Wolof. \n \n \n \n \n"}, {"page": 45, "text": " \n15 \n \nÌbéèrè: \nỌmọ oṣu mẹfa ni a mu lọ si ayẹwo lati ọwọ iya rẹ, ti o sọ pe idagbasoke ọmọ \nrẹ ọkunrin ti lọra, pẹlu awọn iyipada ni eto eyín, igbogbo-igbogbo èébì, \nawọn ìṣísẹ́ ara ti ko ni ibamu, ati awọn igbẹwọ ti o ṣoro lati ṣakoso. Ni \nayẹwo ara, wọn ṣe akiyesi pe o ni awọ ara seborrheic ti ko dara, ori kekere \n(microcephaly), ati ẹyin-agbọn ti ko ni ipo to yẹ. Ni ipilẹ awọn alaye aisan ti \naisan naa fihan, irú àìsàn pataki wo ni o dabi pe alaisan yii ni? \n \nÌdáhùn: \nNípasẹ̀ àwọn ìwádìí ìṣègùn tí a ṣe, ó dàbí pé aláìsàn náà ní West Syndrome \n(Àìsàn West), tí a tún mọ̀ sí Infantile Spasms (Ìgbàgbẹ́ Ọmọdé). West \nSyndrome jẹ́ àìsàn ìgbàgbẹ́ tó jẹ́ ọ̀kan lára àwọn àìsàn tó ṣẹlẹ̀ lọ́ọ̀kọ̀ọ̀kan, tí ó \nní àwọn àmì bíi ìdàgbàsókè tó lọra, àìní ìdarí ara, ìgbàgbẹ́ tó ṣòro láti dá \ndúró (infantile spasms), àìlè ronú dáadáa, àti àwọn àmì mìíràn. Awọ̀ ara tó \nní ọ̀rá púpọ̀ (seborrheic skin), orí kékeré (microcephaly), àti erẹ̀kẹ́ tó hàn \ngbangba lè jẹ́ àmì fún àìsàn yìí. \nSTab. 27: Long-Form QA data example in Yoruba. \n \n \n \nUmbuzo: \nIsiguli sezinyanga eziyisithupha sithathwa ukuyobonana nodokotela unina, \nobika ukukhula kancane kwengane yakhe yomfana, kanye noguquko \nemazinyweni, ukuhlanza okuphindaphindayo, ukungahambelani \nkweminyakazo yomzimba, kanye nokuphathwa izifo zokuwa okunzima \nukuzilawula. Ekuhlolweni komzimba kwengane, kuyabonakala ukuthi \nunesifo sesikhumba esibizwa nge-seborrheic, ikhanda elincane, kanye \nnomhlathi ongajwayelekile. Ngokususela kumininingwane yezempilo \nevezwe yisiguli, yisiphi isimo esihlangene esivezwa yisiguli? \n \nIzinketho: \nNgokususela kwi-data yezokunakekelwa kweziguli ephakanyisiwe, \nkubonakala ukuthi isiguli sinesifo esibizwa ngokuthi yi-West Syndrome, \nesaziwa futhi ngokuthi yi-Infantile Spasms (ukudlidliza okuthile \nkwezingane ezincane). I-West Syndrome iyisifo esiyingcosana sokuwa \nesichazwa ngokukhula okuhlehlayo, ukunyakaza okungahlelekile, \nukudlidliza okungalawuleki (ukudlidliza okukhethekile kwezingane \nezincane), ukungakwazi ukucabanga kahle, kanye nezinye izimpawu. \nIsikhumba esineseborrhoea (esinamafutha amaningi), ikhanda elincane (i-\nmicrocephaly), kanye nomhlathi oqavile kungase kube izimpawu zalesi sifo \nesiyingqayizivele. \nSTab. 28: Long-Form QA data example in Zulu. \n \n \n"}, {"page": 46, "text": " \n16 \nMCQA \n \n问题: \n一位23 岁的孕妇，妊娠22 周，出现排尿时灼痛。她表示这种症状始于1\n天前，并且尽管增加饮水量和服用蔓越莓提取物，症状仍在恶化。除此之\n外，她感觉良好，并由医生定期进行产前检查。她的体温为97.7¡ãF\n（36.5¡ãC），血压为122/77 毫米汞柱，脉搏为80 次/分钟，呼吸频率为\n19 次/分钟，室内空气下氧饱和度为98%。体格检查显示无肋脊角压痛，\n子宫增大。以下哪项是该患者的最佳治疗方案？ \n \n选项: \nA: 氨苄青霉素 \nB: 头孢曲松 \nC: 多西环素 \nD: 呋喃妥因 \n \n答案: 呋喃妥因 \nSTab. 29: MCQA data example in Chinese. \n \n \n \nQuestion: \nA 23-year-old pregnant woman at 22 weeks gestation presents with \nburning upon urination. She states it started 1 day ago and has been \nworsening despite drinking more water and taking cranberry extract. She \notherwise feels well and is followed by a doctor for her pregnancy. Her \ntemperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is \n80/min, respirations are 19/min, and oxygen saturation is 98% on room \nair. Physical exam is notable for an absence of costovertebral angle \ntenderness and a gravid uterus. Which of the following is the best \ntreatment for this patient? \n \nOptions: \nA: Ampicillin \nB: Ceftriaxone \nC: Doxycycline \nD: Nitrofurantoin \n \nAnswer: Nitrofurantoin \nSTab. 30: MCQA data example in English. \n \n \n \n"}, {"page": 47, "text": " \n17 \n \nQuestion: \nUne femme enceinte de 23 ans à 22 semaines de grossesse se présente avec \ndes brûlures urinaires. Elle déclare que cela a commencé il y a 1 jour et s'est \naggravé malgré une augmentation de sa consommation d'eau et la prise \nd'extrait de canneberge. Elle se sent par ailleurs bien et est suivie par un \nmédecin pour sa grossesse. Sa température est de 36,5°C, sa tension \nartérielle est de 122/77 mmHg, son pouls est de 80/min, sa fréquence \nrespiratoire est de 19/min et sa saturation en oxygène est de 98% à l'air \nambiant. L'examen physique est remarquable par l'absence de sensibilité \nde l'angle costo-vertébral et un utérus gravide palpable. Quelle est le \nmeilleur traitement pour cette patiente? \n \nOptions: \nA: Ampicilline \nB: Ceftriaxone \nC: Doxycycline \nD: Nitrofurantoïne \n \nRéponse: Nitrofurantoïne \nSTab. 31: MCQA data example in French. \n \n \nFrage: \nEine 23-jährige schwangere Frau in der 22. Schwangerschaftswoche stellt \nsich mit Brennen beim Urinieren vor. Sie gibt an, dass es vor 1 Tag begann \nund sich trotz erhöhter Wasseraufnahme und Einnahme von Cranberry-\nExtrakt verschlimmert hat. Ansonsten fühlt sie sich gut und wird von einem \nArzt bezüglich ihrer Schwangerschaft betreut. Ihre Temperatur beträgt \n97,7°F (36,5°C), der Blutdruck liegt bei 122/77 mmHg, der Puls bei 80/min, \ndie Atmung bei 19/min und die Sauerstoffsättigung bei 98% unter \nRaumluft. Die körperliche Untersuchung zeigt keine \nDruckschmerzhaftigkeit im Costovertebralwinkel und einen graviden \nUterus. Welche der folgenden Behandlungen ist für diese Patientin am \nbesten geeignet? \n \nOptionen: \nA: Ampicillin \nB: Ceftriaxon \nC: Doxycyclin \nD: Nitrofurantoin \n \nAntwort: Nitrofurantoin \nSTab. 32: MCQA data example in German. \n \n \n"}, {"page": 48, "text": " \n18 \n \n質問: \n妊娠22 週の23 歳の妊婦が排尿時痛を訴えて来院しました。彼女は1 日\n前から症状が始まり、水分摂取量を増やしクランベリーエキスを摂取し\nているにもかかわらず悪化していると述べています。それ以外は体調は\n良好で、妊娠については医師によるフォローアップを受けています。彼\n女の体温は36.5¡ãC、血圧は122/77 mmHg、脈拍は80/分、呼吸数は\n19/分、室内気での酸素飽和度は98％です。身体診察では、肋骨脊椎角\nの圧痛がないことと、妊娠子宮が認められます。この患者に対する最適\nな治療は以下のうちどれですか？ \n \n選択肢: \nA: アンピシリン \nB: セフトリアキソン \nC: ドキシサイクリン \nD: ニトロフラントイン \n \n答え: ニトロフラントイン \nSTab. 33: MCQA data example in Japanese. \n \n \n \n질문: \n임신 22 주차인 23 세 임산부가 배뇨통을 호소하며 내원했습니다. 이 \n증상이 1 일 전에 시작되었고 물을 더 많이 마시고 크랜베리 \n추출물을 복용했음에도 악화되고 있다고 합니다. 그 외에는 \n전반적으로 건강한 상태이며 임신 관련 의사의 관리를 받고 있습니다. \n체온은 97.7°F (36.5°C, 섭씨), 혈압은 122/77 mmHg (수은주 밀리미터), \n맥박은 80 회/분, 호흡수는 19 회/분, 그리고 실내 공기에서의 \n산소포화도는 98%입니다. 신체 검사상 늑골척추각 압통은 없으며 \n임신자궁이 관찰됩니다. 이 환자에게 가장 적절한 치료는 다음 중 \n무엇입니까? \n \n옵션: \nA: 앰피실린 \nB: 세프트리악손 \nC: 독시사이클린 \nD: 니트로푸란토인 \n \n답변: 니트로푸란토인 \nSTab. 34: MCQA data example in Korean. \n \n \n \n"}, {"page": 49, "text": " \n19 \n \nPergunta: \nUma mulher grávida de 23 anos com 22 semanas de gestação apresenta \nardência ao urinar. Ela afirma que começou há 1 dia e tem piorado apesar \nde beber mais água e tomar extrato de cranberry. De resto, sente-se bem e é \nacompanhada por um médico durante a gravidez. Sua temperatura é de \n36,5°C, a pressão arterial é 122/77 mmHg, o pulso é 80/min, as respirações \nsão 19/min e a saturação de oxigênio é 98% em ar ambiente. O exame físico \né notável pela ausência de dor à percussão do ângulo costovertebral e um \nútero gravídico. Qual das seguintes opções é o melhor tratamento para esta \npaciente? \n \nOpções: \nA: Ampicilina \nB: Ceftriaxona \nC: Doxiciclina \nD: Nitrofurantoína \n \nResposta: Nitrofurantoína \nSTab. 35: MCQA data example in Portuguese. \n \n \n \nPregunta: \nUna mujer embarazada de 23 años con 22 semanas de gestación se \npresenta con ardor al orinar. Ella afirma que comenzó hace 1 día y ha \nestado empeorando a pesar de beber más agua y tomar extracto de \narándano. Por lo demás, se siente bien y está siendo controlada por un \nmédico durante su embarazo. Su temperatura es de 97.7°F (36.5°C), la \npresión arterial es de 122/77 mmHg, el pulso es de 80/min, las \nrespiraciones son de 19/min y la saturación de oxígeno es del 98% en aire \nambiente. El examen físico es notable por la ausencia de dolor a la \npalpación en el ángulo costovertebral y un útero gestante. ¿Cuál de los \nsiguientes es el mejor tratamiento para esta paciente? \n \nOpciones: \nA: Ampicilina \nB: Ceftriaxona \nC: Doxiciclina \nD: Nitrofurantoína \n \nRespuesta: Nitrofurantoína \nSTab. 36: MCQA data example in Spanish. \n \n \n"}, {"page": 50, "text": " \n20 \n \nSwali: \nMwanamke mjamzito mwenye umri wa miaka 23 akiwa na ujauzito wa wiki \n22 anaripoti kuhisi maumivu wakati wa kukojoa. Anasema ilianza siku 1 \niliyopita na imeendelea kuwa mbaya licha ya kunywa maji zaidi na kutumia \ndawa ya cranberry. Kwa vyovyote anaendelea kujisikia vizuri na \nanafuatiliwa na daktari kwa ujauzito wake. Joto lake la mwili ni 97.7°F \n(36.5°C), shinikizo la damu ni 122/77 mmHg, mapigo ya moyo ni \n80/dakika, upumuaji ni 19/dakika, na ushibaji wa oksijeni ni 98% katika \nhewa ya kawaida. Uchunguzi wa mwili unaonyesha kutokuwepo kwa \nmaumivu kwenye pembe ya uti wa mgongo na figo (costovertebral angle) \nna tumbo la ujauzito. Ni matibabu gani kati ya yafuatayo yanayofaa zaidi \nkwa mgonjwa huyu? \n \nChaguzi: \nA: Ampicillin \nB: Ceftriaxone \nC: Doxycycline \nD: Nitrofurantoin \n \nJibu: Nitrofurantoin \nSTab. 37: MCQA data example in Swahili. \n \n \n \nLaaj: \nJigéen bu am 23 at bu ëmb bu nekk ci 22 ayubés gis na ñu ko ndax lakk bi \nmuy yég saa su santaan. Wax na ni lakk bi tàmbali na bëy 1 bes te di gëna \nmetti doonte mu nàn na ndox bu bari ak di jëfandikoo tànku cranberry. \nMoo ti dara jakkaaraluko ko te ab doktoor di ko ubbi. Tamperatuur bi \n97.7°F (36.5°C) la, tànsiyo bi 122/77 mmHg, puls bi 80/min, noyyi gi \n19/min, te oksijeen bi 98% la ci ngelaw mi. Saytu jëmm ji dafa wone ni \namul metit ci costovertebral angle bi te biir bi dafa diis. Ban ci fàppkay yi ñu \ntëral moo gën ci faj gi jaambur bi? \n \nTànneefi: \nA: Ampicillin \nB: Ceftriaxone \nC: Doxycycline \nD: Nitrofurantoin \n \nTontu: Nitrofurantoin \nSTab. 38: MCQA data example in Wolof. \n \n \n"}, {"page": 51, "text": " \n21 \n \nÌbéèrè: \nObìnrin tó lóyún tó jẹ́ ọmọ ọdún mẹ́tàlélógún (23) tí oyún rẹ̀ ti tó ọ̀sẹ̀ \nméjìlélógún (22) wá pẹ̀lú ìrora tí ó ń gbóná nígbà tí ó bá ń tọ̀. Ó sọ pé ó bẹ̀rẹ̀ \nní ọjọ́ kan sẹ́yìn tí ó sì ti ń burú sí i láìsí bí òun ti ń mu omi pọ̀ tí òun sì ti ń lo \nòògùn cranberry extract. Yàtọ̀ sí èyí, ara rẹ̀ dá òun tí dókítà sì ń mojútó \noyún rẹ̀. Ìgbóná ara rẹ̀ jẹ́ 97.7°F (36.5°C), ìfunpa rẹ̀ jẹ́ 122/77 mmHg, ìlù \nọkàn rẹ̀ jẹ́ 80/dákíkà, ìmísì rẹ̀ jẹ́ 19/dákíkà, àti ìwọ̀n oxygen saturation rẹ̀ jẹ́ \n98% ní gbangba. Àyẹ̀wò ara rẹ̀ fi hàn pé kò sí costovertebral angle \ntenderness àti pé ilé ọmú rẹ̀ ti tòbi nítorí oyún. Nínú àwọn wọ̀nyí, èwo ni ó \njẹ́ ìtọ́jú tó dára jùlọ fún aláìsàn yìí? \n \nÀwọn àṣàyàn: \nA: Ampicillin \nB: Ceftriaxone \nC: Doxycycline \nD: Nitrofurantoin \n \nÌdáhùn: Nitrofurantoin \nSTab. 39: MCQA data example in Yoruba. \n \n \nUmbuzo: \nOwesifazane okhulelwe oneminyaka engu-23 ubudala osemasonteni angu-\n22 ekhulelwe ufika enokusha uma enza umshobingo. Uthi kuqale izolo futhi \nkuya ngokuba kubi nakuba ephuza amanzi amaningi futhi ethatha okuthi \ncranberry extract. Ngaphandle kwalokho uzizwa ephilile futhi ulandelwa \nudokotela ngokukhulelwa kwakhe. Izinga lokushisa komzimba wakhe \nkungu-97.7°F (36.5°C), umfutho wegazi ungu-122/77 mmHg, inhliziyo \nishaya ku-80/min, ukuphefumula kungu-19/min, futhi ukugcwala kwe-\noxygen kungamaphesenti angu-98% emoyeni wasekamelweni. Ukuhlolwa \nkomzimba kukhombisa ukungabikho kwezinhlungu zesiphambano \nsemisipha nomgogodla kanye nesisu esikhulelwe. Yikuphi kulokhu \nokulandelayo okungukwelashwa okungcono kwalesi siguli? \n \nIzinketho: \nA: I-Ampicillin \nB: I-Ceftriaxone \nC: I-Doxycycline \nD: I-Nitrofurantoin \n \nImpendulo: I-Nitrofurantoin \nSTab. 40: MCQA data example in Zulu. \n \n \n"}, {"page": 52, "text": " \n22 \nS1.4. Agentic Machine Translation Prompt \nNLI \n \n \nSystem Message: \nYou are a professional translator specializing in accurate translation of medical content from \n{source_lang} to {target_lang}. \n \nYour task is to translate the medical questions while: \n \n1. Preserve all numerals, decimal points, scientific notation, comparison signs, percentages, and \nbrackets exactly as written (e.g., 2.5, μg/ml, 100%, [3H] leucine). Do NOT localize decimal \nseparators or convert units/currencies. \n \n2. Keep biochemical/clinical entities and abbreviations (e.g., NGF, tyrosine hydroxylase, ELISA, \nmRNA) exactly as in the source unless the target language has a well-established localized \ncommon name (rare). If localized, do NOT add the original in parentheses. \n \n3. Preserve LaTeX/math/code verbatim; do not translate or alter content inside LaTeX/code \ndelimiters. \n \n4. Maintain tone and register appropriate for professional medical assessments. Avoid explanatory \nadditions. \n \nTask: \nPlease translate the following NLI item: \n \n<SOURCE_TEXT> \n{source_text} \n</SOURCE_TEXT> \n \nOutput: \nOnly provide the {target_lang} translation for the above text. Do not include any explanations or text \napart from the translation. \n \nReturn ONLY a single JSON object with the exact keys: \"premise\" and \"hypothesis\". Do not include \nany explanations or text outside the JSON. Do NOT add extra keys or metadata. All JSON keys must \nremain in English exactly as shown and only translate the content inside square brackets. \n \n<TRANSLATION> \n{ \n\"premise\": \"[translation of premise]\", \n\"hypothesis\": \"[translation of hypothesis]\" \n} \n</TRANSLATION> \nSTab. 41: Agentic machine translation prompt for the NLI task (Initial Translation). \n \n \n \n \n"}, {"page": 53, "text": " \n23 \n \n \nSystem Message:  \nYou are a medical translation expert, specializing in translation from {source_lang} to {target_lang}. \n \nTask Description:  \nCarefully review the source text and its translation from {source_lang} to {target_lang}, and then \nprovide constructive suggestions in English. \n \nRequirements: \n1. No additions, removals, or explanations of domain content. \n \n2. Preserve anonymization and any specialized placeholders exactly (e.g., masked IDs, [***], \n{VAR}). \n \n3. Numerals & notation: verify all numbers, decimal points, scientific notation, comparison signs, \npercentages, brackets, and Greek letters are unchanged; DO NOT localize decimal separators or \nconvert currencies/units. \n \n4. Units & symbols: confirm μg/ml, %, ×, ±, →, ≥/≤, and SI prefixes/symbols are preserved exactly \n(no written-out forms). \n \n5. Biomedical entities/abbreviations (e.g., NGF, tyrosine hydroxylase, ELISA, mRNA): ensure they \nremain as in the source unless a well-established {target_lang} equivalent exists; if localized, the \noriginal term MUST NOT be added in parentheses; check consistent capitalization. \n \n6. Field integrity: ensure exactly the two fields are present and correctly mapped –\"premise\": \ntranslated premise text –\"hypothesis\": translated hypothesis text. \n \n7. Tone/register: maintain professional assessment style; remove any explanatory additions or \ncommentary. \n \nInput: \n \n<SOURCE_TEXT> \n{source_text} \n</SOURCE_TEXT> \n \n<INITIAL_TRANSLATION> \n{initial_trans} \n</INITIAL_TRANSLATION> \n \nOutput: \n \n<SUGGESTIONS> \n[Your suggestions here] \n</SUGGESTIONS> \nSTab. 42: Agentic machine translation prompt for the NLI task (Reflection). \n \n \n \n"}, {"page": 54, "text": " \n24 \n \n \nSystem Message:  \nYou are a senior medical NLI translation reviser, specializing in translation from {source_lang} to \n{target_lang}. \n \nTask Description:  \nCarefully review and edit the NLI translation from {source_lang} to {target_lang}, incorporating the \nexpert feedback below. \n \nRequirements: \n1. DO NOT explain anything; produce the improved translation JSON only. \n \n2. Preserve every single quote from the source; do not add new single or double quotes. \n \n3. Remove unnecessary explanations or original terms from {source_lang} if present in the \ntranslation. \n \nInput: \n \n<SOURCE_TEXT> \n{source_text} \n</SOURCE_TEXT> \n \n<INITIAL_TRANSLATION> \n{initial_trans} \n</INITIAL_TRANSLATION> \n \n<EXPERT_SUGGESTIONS> \n{reflection} \n</EXPERT_SUGGESTIONS> \n \nOutput:  \nOnly provide the {target_lang} translation for the above text. Do not include any explanations or text \napart from the translation.  \n \nReturn ONLY a single JSON object with the exact keys: \"premise\" and \"hypothesis\". Do not include \nany explanations or text outside the JSON. Do NOT add extra keys or metadata. All JSON keys must \nremain in English exactly as shown and only translate the content inside square brackets. \n \n<IMPROVED_TRANSLATION> \n{ \n\"premise\": \"[improved translation of premise]\", \n\"hypothesis\": \"[improved translation of hypothesis]\" \n} \n</IMPROVED_TRANSLATION> \nSTab. 43: Agentic machine translation prompt for the NLI task (Improved Translation). \n \n \n \n \n"}, {"page": 55, "text": " \n25 \nMCQA \n \n \nSystem Message: \nYou are a professional translator specializing in accurate translation of medical content from \n{source_lang} to {target_lang}. \n \nYour task is to translate the medical questions while: \n \n1. Preserve all numerals, decimal points, scientific notation, comparison signs, percentages, and \nbrackets exactly as written (e.g., 2.5, μg/ml, 100%, [3H] leucine). Do NOT localize decimal \nseparators or convert units/currencies. \n \n2. Keep biochemical/clinical entities and abbreviations (e.g., NGF, tyrosine hydroxylase, ELISA, \nmRNA) exactly as in the source unless the target language has a well-established localized \ncommon name (rare). If localized, do NOT add the original in parentheses. \n \n3. Preserve LaTeX/math/code verbatim; do not translate or alter content inside LaTeX/code \ndelimiters. \n \n4. Keep option labels and the number of options exactly as in the source (e.g., A/B/C/D). Do not \nmerge or split options. \n \n5. Maintain tone and register appropriate for professional medical assessments. Avoid explanatory \nadditions. \n \nTask: \nPlease translate the following MCQA item: \n \n<SOURCE_TEXT> \n{source_text} \n</SOURCE_TEXT> \n \nOutput: \nOnly provide the {target_lang} translation for the above text. Do not include any explanations or text \napart from the translation. Different options are separated by newline characters (\\n). The number \nof options in the output must match the input exactly. Do not skip or combine any options.  \n \nReturn the translation in the following JSON format, with keys: \"question\" and \"options\", where the \nvalue of \"options\" is a dictionary with keys option1, option2, option3, etc. All JSON keys must remain \nin English exactly as shown and only translate the content inside square brackets: \n \n<TRANSLATION> \n{ \n\"question\": \"[translation of question]\", \n\"options\": { \n\"option1\": \"[translation of option1]\", \n\"option2\": \"[translation of option2]\", \n\"option3\": \"[translation of option3]\", \n... \n} \n} \n</TRANSLATION> \nSTab. 44: Agentic machine translation prompt for the MCQA task (Initial Translation). \n"}, {"page": 56, "text": " \n26 \n \n \nSystem Message:  \nYou are a medical translation expert, specializing in translation from {source_lang} to \n{target_lang}. \n \nTask Description:  \nCarefully review the source text and its translation from {source_lang} to {target_lang}, and then \nprovide constructive suggestions in English. \n \nRequirements: \n1. No additions, removals, or explanations of domain content. \n \n2. Preserve anonymization and any specialized placeholders exactly (e.g., masked IDs, [***], \n{VAR}). \n \n3. Numerals & notation: verify all numbers, decimal points, scientific notation, comparison signs, \npercentages, brackets, and Greek letters are unchanged; DO NOT localize decimal separators or \nconvert currencies/units. \n \n4. Units & symbols: confirm μg/ml, %, ×, ±, →, ≥/≤, and SI prefixes/symbols are preserved exactly \n(no written-out forms). \n \n5. Biomedical entities/abbreviations (e.g., NGF, tyrosine hydroxylase, ELISA, mRNA): ensure they \nremain as in the source unless a well-established {target_lang} equivalent exists; if localized, the \noriginal term MUST NOT be added in parentheses; check consistent capitalization. \n \n6. Options integrity: the number of options and their order must match the source exactly; no \nmerge/split; no added symbols or written-out labels; each option remains a single line separated \nby \\n. \n \n7. Tone/register: maintain professional assessment style; remove any explanatory additions or \ncommentary. \n \nInput: \n \n<SOURCE_TEXT> \n{source_text} \n</SOURCE_TEXT> \n \n<INITIAL_TRANSLATION> \n{initial_trans} \n</INITIAL_TRANSLATION> \n \nOutput: \n \n<SUGGESTIONS> \n[Your suggestions here] \n</SUGGESTIONS> \nSTab. 45: Agentic machine translation prompt for the MCQA task (Reflection). \n \n \n"}, {"page": 57, "text": " \n27 \n \n \nSystem Message:  \nYou are a senior medical MCQA translation reviser, specializing in translation from \n{source_lang} to {target_lang}. \n \nTask Description:  \nCarefully review and edit the MCQA translation from {source_lang} to {target_lang}, incorporating \nthe expert feedback below. \n \nRequirements: \n1. DO NOT explain anything; produce the improved translation JSON only. \n \n2. Preserve every single quote from the source; do not add new single or double quotes. \n \n3. Remove unnecessary explanations or original terms from {source_lang} if present in the \ntranslation. \n \nInput: \n \n<SOURCE_TEXT> \n{source_text} \n</SOURCE_TEXT> \n \n<INITIAL_TRANSLATION> \n{initial_trans} \n</INITIAL_TRANSLATION> \n \n<EXPERT_SUGGESTIONS> \n{reflection} \n</EXPERT_SUGGESTIONS> \n \nOutput: \nOnly provide the improved translation. Do not include any explanations or text apart from the \ntranslation. Different options are separated by newline characters (\\n). The number of options in \nthe output must match the input exactly. Do not skip or combine any options.  \n \nReturn the translation in the following JSON format, with keys: \"question\" and \"options\", where the \nvalue of \"options\" is a dictionary with keys option1, option2, option3, etc. All JSON keys must remain \nin English exactly as shown and only translate the content inside square brackets. \n \n<IMPROVED_TRANSLATION> \n{ \n\"question\": \"[improved translation of question]\", \n\"options\": { \n\"option1\": \"[improved translation of option1]\", \n\"option2\": \"[improved translation of option2]\", \n\"option3\": \"[improved translation of option3]\", \n... \n} \n} \n</IMPROVED_TRANSLATION> \nSTab. 46: Agentic machine translation prompt for the MCQA task (Improved Translation). \n \n"}, {"page": 58, "text": " \n28 \nS1.5. Expert Evaluation Criteria \nBoth experts provide three scores (each on a scale of 1–5) based on the original \ntranslation for accuracy, fluency, and completeness. STab. 47-49 provide \ndetailed evaluation criteria for these three metrics. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 59, "text": " \n29 \nAccuracy Evaluation Criteria \n \nAccuracy (1-5) \n \nEvaluation Criteria: \n \n5 points (Very Accurate) \nMedical terms and concepts are completely and correctly translated, \nwith no errors. \n- All professional terms correspond to the original text, with no \nmistranslations or incorrect translations. \nThe most appropriate and professional medical terms are used. \n- Expressions conform to common medical conventions, with standardized \nterminology use. \n \n4 points (Accurate) \nMost medical terms and concepts are correctly translated, with only \nvery few minor errors that do not affect overall understanding. \n- Some terms may not be precise, but overall accuracy is maintained. \nGenerally appropriate medical terms are used. \n- In a few places, more colloquial words may be used, but they are still \nunderstandable to professionals. \n \n3 points (Moderately Accurate) \nMajor medical terms and concepts are generally correct, but there are \nsome errors that may cause partial misunderstanding. \n- Some key terms are inaccurately translated, requiring the reader to infer. \nDeviation in the use of medical terms. \n- Occasionally uses less common or outdated terms. \n \n2 points (Not Very Accurate) \nMost medical terms and concepts are mistranslated, severely affecting \nunderstanding. \n- Key concepts are mistranslated, possibly leading to misunderstanding of \nthe original meaning. \nIncorrect or inappropriate medical terms are used. \n- Terminology is confused, lacking professionalism. \n \n1 point (Inaccurate) \nMedical terms and concepts are riddled with errors, failing to \ncorrectly convey the original information. \n- Most of the content does not match the original text. \nLacks correct use of medical terms. \n- Terminology is chaotic, possibly using non-medical vocabulary entirely. \nSTab. 47: Evaluation criteria for accuracy. \n"}, {"page": 60, "text": " \n30 \nFluency Evaluation Criteria \n \nFluency (1-5) \n \nEvaluation Criteria: \n \n5 points (Very Fluent) \nNatural and smooth expression, with no reading obstacles. \n- Elegant language, conforming to professional literature style. \nSentence structure fully adheres to linguistic habits, with no \ngrammatical or lexical errors. \n \n4 points (Fluent) \nExpression is mostly natural, with occasional minor language flaws \nthat do not affect understanding. \n- Some sentences may seem slightly awkward. \nSentence structure mostly conforms to habits, with very few \ngrammatical errors. \n \n3 points (Moderately Fluent) \nExpression is somewhat unnatural, requiring readers to slightly \nadjust for understanding. \n- Some improper word use or awkward sentence structure. \nSentence structure is generally correct, but there are some \ngrammatical errors. \n \n2 points (Not Very Fluent) \nExpression is not fluent, with obvious reading obstacles. \n- Sentences are not smoothly connected, and logic is unclear. \nSentence structure has many problems, with frequent grammatical \nerrors. \n \n1 point (Not Fluent) \nExpression is very awkward or disjointed, making understanding \ndifficult. \n- There may be a literal translation with a lack of proper expression. \nSentence structure is chaotic, with severe grammatical errors, making \nthe text unreadable. \nSTab. 48: Evaluation criteria for fluency. \n \n \n \n \n"}, {"page": 61, "text": " \n31 \nCompleteness Evaluation Criteria \n \nCompleteness (1-5) \n \nEvaluation Criteria: \n \n5 points (Very Complete) \nFully retains the original meaning, with no omissions or additions of \ninformation. \n- Details, data, and annotations are accurately conveyed. \nThe translated content completely matches the original in length and \ndepth. \n \n4 points (Complete) \nThe main meaning of the original is preserved, with only very few \nminor omissions or unclear details. \n- Some minor information may be omitted. \nThe translated content mostly corresponds to the original. \n \n3 points (Moderately Complete) \nMost of the original meaning is conveyed, but some information is \nomitted or added. \n- Important details may be overlooked. \nThe translated content differs from the original, requiring the reader \nto infer part of the content. \n \n2 points (Not Very Complete) \nThe main information of the original is not fully conveyed, with \nsignificant omissions or unnecessary additions. \n- Information unrelated to the original may be introduced. \nThe translated content does not fully correspond to the original, \naffecting understanding. \n \n1 point (Incomplete) \nLarge amounts of information are missing, or incorrect information is \nadded, failing to reflect the main content of the original. \n- Important paragraphs or sentences are missing. \nThe translated content significantly deviates from the original, \nmaking it impossible to understand the original meaning. \nSTab. 49: Evaluation criteria for completeness. \n \n \n"}, {"page": 62, "text": " \n32 \nS2. GlobMed-Bench \nS2.1. LLM Information \nLLMs \nDeveloper \nParameters (B) \nDomain \nReasoning \nLicense \nClaude-3.5-Haiku \nAnthropic \nUndisclosed \nGeneral \nNo \nProprietary \nClaude-4.0-Sonnet \nAnthropic \nUndisclosed \nGeneral \nYes \nProprietary \nGemini-2.5-Flash \nGoogle \nUndisclosed \nGeneral \nYes \nProprietary \nGPT-4o-mini \nOpenAI \nUndisclosed \nGeneral \nNo \nProprietary \nGPT-4o \nOpenAI \nUndisclosed \nGeneral \nNo \nProprietary \nGPT-4.1-nano \nOpenAI \nUndisclosed \nGeneral \nNo \nProprietary \nGPT-4.1-mini \nOpenAI \nUndisclosed \nGeneral \nNo \nProprietary \nGPT-4.1 \nOpenAI \nUndisclosed \nGeneral \nNo \nProprietary \nGPT-5-nano \nOpenAI \nUndisclosed \nGeneral \nYes \nProprietary \nGPT-5-mini \nOpenAI \nUndisclosed \nGeneral \nYes \nProprietary \nGPT-5 \nOpenAI \nUndisclosed \nGeneral \nYes \nProprietary \no4-mini \nOpenAI \nUndisclosed \nGeneral \nYes \nProprietary \nDeepSeek-V3 \nDeepSeek \n671B \nGeneral \nNo \nMIT \nDeepSeek-R1 \nDeepSeek \n671B \nGeneral \nYes \nMIT \nDeepSeek-R1-Qwen3-8B \nDeepSeek \n8B \nGeneral \nYes \nMIT \nGemma-3-4B \nGoogle \n4B \nGeneral \nNo \nGemma \nGemma-3-12B \nGoogle \n12B \nGeneral \nNo \nGemma \nGemma-3-27B \nGoogle \n27B \nGeneral \nNo \nGemma \ngpt-oss-20B \nOpenAI \n20B \nGeneral \nYes \nApache 2.0 \ngpt-oss-120B \nOpenAI \n120B \nGeneral \nYes \nApache 2.0 \nLLaMA-3.1-8B \nMeta-llama \n8B \nGeneral \nNo \nLlama-3.1 \nLLaMA-3.1-70B \nMeta-llama \n70B \nGeneral \nNo \nLlama-3.1 \nLLaMA-3.2-3B \nMeta-llama \n3B \nGeneral \nNo \nLlama-3.2 \nLLaMA-3.3-70B \nMeta-llama \n70B \nGeneral \nNo \nLlama-3.3 \nLLaMA-4-Scout \nMeta-llama \n109B \nGeneral \nNo \nLlama-4 \nLLaMA-4-Maverick \nMeta-llama \n400B \nGeneral \nNo \nLlama-4 \nMistral-7B-v0.3 \nMistral AI \n7B \nGeneral \nNo \nApache 2.0 \nMistral-Small-3.1-24B \nMistral AI \n24B \nGeneral \nNo \nApache 2.0 \nPhi-4-mini \nMicrosoft \n3.8B \nGeneral \nNo \nMIT \nPhi-4-mini-Reasoning \nMicrosoft \n3.8B \nGeneral \nYes \nMIT \nPhi-4 \nMicrosoft \n14B \nGeneral \nNo \nMIT \nPhi-4-Reasoning \nMicrosoft \n14B \nGeneral \nYes \nMIT \nQwen2.5-3B \nQwen \n3B \nGeneral \nNo \nQwen-Research \nQwen2.5-7B \nQwen \n7B \nGeneral \nNo \nQwen \nQwen2.5-14B \nQwen \n14B \nGeneral \nNo \nApache 2.0 \nQwen2.5-72B \nQwen \n72B \nGeneral \nNo \nQwen \nQwQ-32B \nQwen \n32B \nGeneral \nYes \nApache 2.0 \nQwen3-1.7B \nQwen \n1.7B \nGeneral \nNo \nApache 2.0 \nQwen3-4B \nQwen \n4B \nGeneral \nNo \nApache 2.0 \nQwen3-4B-thinking \nQwen \n4B \nGeneral \nYes \nApache 2.0 \nQwen3-8B \nQwen \n8B \nGeneral \nNo \nApache 2.0 \nQwen3-8B-thinking \nQwen \n8B \nGeneral \nYes \nApache 2.0 \nQwen3-14B \nQwen \n14B \nGeneral \nNo \nApache 2.0 \nQwen3-14B-thinking \nQwen \n14B \nGeneral \nYes \nApache 2.0 \nBaichuan-M2-32B \nBaichuan Inc. \n32B \nMedical \nYes \nApache 2.0 \nBio-Medical-LLaMA-3-8B \nContactDoctor \n8B \nMedical \nNo \nOther \nMediPhi \nMicrosoft \n3.8B \nMedical \nNo \nMIT \nMedGemma-4B \nGoogle \n4B \nMedical \nNo \nHealth-AI-Developer-Foundations \nMedGemma-27B \nGoogle \n27B \nMedical \nNo \nHealth-AI-Developer-Foundations \nMedReason-8B \nUCSC-VLAA \n8B \nMedical \nYes \nApache 2.0 \nHuatuoGPT-o1-7B \nFreedomAI \n7B \nMedical \nYes \nApache 2.0 \nHuatuoGPT-o1-8B \nFreedomAI \n8B \nMedical \nYes \nApache 2.0 \nHuatuoGPT-o1-70B \nFreedomAI \n70B \nMedical \nYes \nApache 2.0 \nHuatuoGPT-o1-72B \nFreedomAI \n72B \nMedical \nYes \nApache 2.0 \nOpenBioLLM-8B \nSaama \n8B \nMedical \nNo \nLlama-3 \nOpenBioLLM-70B \nSaama \n70B \nMedical \nNo \nLlama-3 \nSTab. 50: LLM information. \n"}, {"page": 63, "text": " \n33 \nS2.2. Evaluation Prompt \nMedNLI \n \n任务: \n \n请通过选择正确的选项来确定给定前提和假设之间的关系。只提供直接答\n案 \"X\" ，其中 X 是正确的字母选择。 \n \n前提: \n{前提} \n \n假设: \n{假设} \n \n选项: \nA. 蕴含 \nB. 中立 \nC. 矛盾 \n \n答案: \nSTab. 51: MedNLI evaluation prompt for Chinese. \n \n \n \nTASK: \n \nPlease determine the relationship between the given premise and \nhypothesis by selecting the correct option. Provide direct answers only \nwith \"X\" where X is the correct letter choice. \n \nPremise: \n{premise} \n \nHypothesis: \n{hypothesis} \n \nOptions: \nA. Entailment \nB. Neutral \nC. Contradiction \n \nAnswer: \nSTab. 52: MedNLI evaluation prompt for English. \n \n"}, {"page": 64, "text": " \n34 \n \nTÂCHE: \n \nTÂCHE: Veuillez déterminer la relation entre la prémisse et l'hypothèse \ndonnées en sélectionnant l'option correcte. Fournissez uniquement des \nréponses directes avec \"X\" où X est la lettre correcte. \n \nPrémisse: \n{Prémisse} \n \nHypothèse: \n{Hypothèse} \n \nOptions: \nA. Implication \nB. Neutre \nC. Contradiction \n \nRéponse: \nSTab. 53: MedNLI evaluation prompt for French. \n \n \n \nAUFGABE: \n \nBitte bestimmen Sie die Beziehung zwischen der gegebenen Prämisse und \nHypothese, indem Sie die richtige Option auswählen. Geben Sie nur direkte \nAntworten mit \"X\", wobei X die richtige Buchstabenoption ist. \n \nPrämisse: \n{Prämisse} \n \nHypothese: \n{Hypothese} \n \nOptionen: \nA. Implikation \nB. Neutral \nC. Widerspruch \n \nAntwort: \nSTab. 54: MedNLI evaluation prompt for German. \n \n \n"}, {"page": 65, "text": " \n35 \n \nタスク: \n \n与えられた前提と仮説の関係を、正しい選択肢を選んで判断してくださ\nい。正しい選択肢は「X」のみ、直接答えてください。 \n  \n前提: \n{前提} \n \n仮説: \n{仮説} \n \n選択肢: \nA. 含意 \nB. 中立 \nC. 矛盾 \n \n答え: \nSTab. 55: MedNLI evaluation prompt for Japanese. \n \n \n \n작업: \n \n주어진 \n전제와 \n가설 \n사이의 \n관계를 \n판단하여 \n올바른 \n선택지를 \n고르십시오. 정답이 \"X\" 일 경우 \"X\" 로만 직접적으로 답하십시오. \n \n전제: \n{전제} \n \n가설: \n{가설} \n \n선택지: \nA. 함의 \nB. 중립 \nC. 모순 \n \n답변: \nSTab. 56: MedNLI evaluation prompt for Korean. \n \n \n \n"}, {"page": 66, "text": " \n36 \n \nTAREFA: \n \nPor favor, determine a relação entre a premissa e a hipótese dadas \nselecionando a opção correta. Forneça apenas respostas diretas com \"X\" \nonde X é a letra correta. \n \nPremissa: \n{Premissa} \n \nHipótese: \n{Hipótese} \n \nOpções: \nA. Implicação \nB. Neutro \nC. Contradição \n \nResposta: \nSTab. 57: MedNLI evaluation prompt for Portuguese. \n \n \n \nTAREA: \n \nPor favor, determine la relación entre la premisa y la hipótesis dadas \nseleccionando la opción correcta. Proporcione solo respuestas directas con \n\"X\" donde X es la letra correcta. \n \nPremisa: \n{Premisa} \n \nHipótesis: \n{Hipótesis} \n \nOpciones: \nA. Implicación \nB. Neutral \nC. Contradicción \n \nRespuesta: \nSTab. 58: MedNLI evaluation prompt for Spanish. \n \n \n"}, {"page": 67, "text": " \n37 \n \nKAZI: \n \nTafadhali amua uhusiano kati ya hoja iliyotolewa na wazo kwa kuchagua \nchaguo sahihi. Toa majibu ya moja kwa moja tu na \"X\" ambapo X ni chaguo \nsahihi cha herufi. \n \nHoja: \n{Hoja} \n \nWazo: \n{Wazo} \n \nChaguzi: \nA. Uthibitisho \nB. Katikati \nC. Kupingana \n \nJibu: \nSTab. 59: MedNLI evaluation prompt for Swahili. \n \n \n \nLIGGÉEY: \n \nJërëjëfël na nga xelal diggante bi nekk ci digle bi ak njortu bi ci tann ci option \nyi dëgg. Joxeel tontu yu jub rekk ak \"X\" fu X di araf bi dëgg. \n \nDigle: \n{Digle} \n \nNjortu: \n{Njortu} \n \nOptions: \nA. Jagleel \nB. Moytu \nC. Dëddu \n \nTontu: \nSTab. 60: MedNLI evaluation prompt for Wolof. \n \n \n \n"}, {"page": 68, "text": " \n38 \n \nIṢẸv: \n \nJọwọ pinnu ìbátan láàrin ìsẹ̀lẹ̀ tí a fún ọ àti ìbéèrè nípa yíyan àṣàyàn tó tọ́. Ṣe \nìdáhùn tààrà nìkan pẹ̀lú \"X\" níbi tí X jẹ́ àṣàyàn lẹ́tà tó tọ́. \n \nÌsẹ̀lẹ̀: \n{Ìsẹ̀lẹ̀} \n \nÌbéèrè: \n{Ìbéèrè} \n \nÀwọn àṣàyàn: \nA. Ìbámu \nB. Àárín \nC. Ìlòdì \n \nÌdáhùn: \nSTab. 61: MedNLI evaluation prompt for Yoruba. \n \n \n \nUMSEBENZI: \n \nSicela unqume ubudlelwano phakathi kwesitatimende esinikeziwe kanye \nnomqondo ngokukhetha inketho efanele. Nikeza izimpendulo eziqondile \nkuphela ngo-\"X\" lapho u-X eyinketho encwadi efanele. \n \nIsitatimende: \n{Isitatimende} \n \nUmqondo: \n{Umqondo} \n \nIzinketho: \nA. Ukuvumelana \nB. Okungathathi hlangothi \nC. Ukuphikisana \n \nImpendulo: \nSTab. 62: MedNLI evaluation prompt for Zulu. \n \n \n \n"}, {"page": 69, "text": " \n39 \nBioNLI \n \n任务: \n \n请通过选择正确的选项来确定给定前提和假设之间的关系。只提供直接答\n案 \"X\" ，其中 X 是正确的字母选择。 \n  \n前提: \n{前提} \n  \n假设: \n{假设} \n  \n选项: \nA. 蕴含 \nB. 矛盾 \n  \n答案: \nSTab. 63: BioNLI evaluation prompt for Chinese. \n \n \n \n \nTASK: \n \nPlease determine the relationship between the given premise and \nhypothesis by selecting the correct option. Provide direct answers only with \n\"X\" where X is the correct letter choice. \n  \nPremise: \n{premise} \n  \nHypothesis: \n{hypothesis} \n  \nOptions: \nA. Entailment \nB. Contradiction \n  \nAnswer: \nSTab. 64: BioNLI evaluation prompt for English. \n \n \n"}, {"page": 70, "text": " \n40 \n \nTÂCHE: \n \nTÂCHE: Veuillez déterminer la relation entre la prémisse et l'hypothèse \ndonnées en sélectionnant l'option correcte. Fournissez uniquement des \nréponses directes avec \"X\" où X est la lettre correcte. \n \nPrémisse: \n{Prémisse} \n \nHypothèse: \n{Hypothèse} \n \nOptions: \nA. Implication \nB. Contradiction \n \nRéponse: \nSTab. 65: BioNLI evaluation prompt for French. \n \n \n \n \nAUFGABE: \n \nBitte bestimmen Sie die Beziehung zwischen der gegebenen Prämisse und \nHypothese, indem Sie die richtige Option auswählen. Geben Sie nur direkte \nAntworten mit \"X\", wobei X die richtige Buchstabenoption ist. \n \nPrämisse: \n{Prämisse} \n \nHypothese: \n{Hypothese} \n \nOptionen: \nA. Implikation \nB. Widerspruch \n \nAntwort: \nSTab. 66: BioNLI evaluation prompt for German. \n \n \n"}, {"page": 71, "text": " \n41 \n \nタスク: \n \n与えられた前提と仮説の関係を、正しい選択肢を選んで判断してくださ\nい。正しい選択肢は「X」のみ、直接答えてください。 \n  \n前提: \n{前提} \n \n仮説: \n{仮説} \n \n選択肢: \nA. 含意 \nB. 矛盾 \n \n答え: \nSTab. 67: BioNLI evaluation prompt for Japanese. \n \n \n \n \n작업: \n \n주어진 \n전제와 \n가설 \n사이의 \n관계를 \n판단하여 \n올바른 \n선택지를 \n고르십시오. 정답이 \"X\" 일 경우 \"X\" 로만 직접적으로 답하십시오. \n \n전제: \n{전제} \n \n가설: \n가설} \n \n선택지: \nA. 함의 \nB. 모순 \n \n답변: \nSTab. 68: BioNLI evaluation prompt for Korean. \n \n \n \n \n"}, {"page": 72, "text": " \n42 \n \nTAREFA: \n \nPor favor, determine a relação entre a premissa e a hipótese dadas \nselecionando a opção correta. Forneça apenas respostas diretas com \"X\" \nonde X é a letra correta. \n \nPremissa: \n{Premissa} \n \nHipótese: \n{Hipótese} \n \nOpções: \nA. Implicação \nB. Contradição \n \nResposta: \nSTab. 69: BioNLI evaluation prompt for Portuguese. \n \n \n \n \nTAREA: \n \nPor favor, determine la relación entre la premisa y la hipótesis dadas \nseleccionando la opción correcta. Proporcione solo respuestas directas con \n\"X\" donde X es la letra correcta. \n \nPremisa: \n{Premisa} \n \nHipótesis: \n{Hipótesis} \n \nOpciones: \nA. Implicación \nB. Contradicción \n \nRespuesta: \nSTab. 70: BioNLI evaluation prompt for Spanish. \n \n \n"}, {"page": 73, "text": " \n43 \n \nKAZI: \n \nTafadhali amua uhusiano kati ya hoja iliyotolewa na wazo kwa kuchagua \nchaguo sahihi. Toa majibu ya moja kwa moja tu na \"X\" ambapo X ni chaguo \nsahihi cha herufi. \n \nHoja: \n{Hoja} \n \nWazo: \n{Wazo} \n \nChaguzi: \nA. Uthibitisho \nB. Kupingana \n \nJibu: \nSTab. 71: BioNLI evaluation prompt for Swahili. \n \n \n \n \nLIGGÉEY: \n \nJërëjëfël na nga xelal diggante bi nekk ci digle bi ak njortu bi ci tann ci option \nyi dëgg. Joxeel tontu yu jub rekk ak \"X\" fu X di araf bi dëgg. \n \nDigle: \n{Digle} \n \nNjortu: \n{Njortu} \n \nOptions: \nA. Jagleel \nB. Dëddu \n \nTontu: \nSTab. 72: BioNLI evaluation prompt for Wolof. \n \n \n \n"}, {"page": 74, "text": " \n44 \n \nIṢẸv: \n \nJọwọ pinnu ìbátan láàrin ìsẹ̀lẹ̀ tí a fún ọ àti ìbéèrè nípa yíyan àṣàyàn tó tọ́. Ṣe \nìdáhùn tààrà nìkan pẹ̀lú \"X\" níbi tí X jẹ́ àṣàyàn lẹ́tà tó tọ́. \n \nÌsẹ̀lẹ̀: \n{Ìsẹ̀lẹ̀} \n \nÌbéèrè: \n{Ìbéèrè} \n \nÀwọn àṣàyàn: \nA. Ìbámu \nB. Ìlòdì \n \nÌdáhùn: \nSTab. 73: BioNLI evaluation prompt for Yoruba. \n \n \n \n \nUMSEBENZI: \n \nSicela unqume ubudlelwano phakathi kwesitatimende esinikeziwe kanye \nnomqondo ngokukhetha inketho efanele. Nikeza izimpendulo eziqondile \nkuphela ngo-\"X\" lapho u-X eyinketho encwadi efanele. \n \nIsitatimende: \n{Isitatimende} \n \nUmqondo: \n{Umqondo} \n \nIzinketho: \nA. Ukuvumelana \nB. Ukuphikisana \n \nImpendulo: \nSTab. 74: BioNLI evaluation prompt for Zulu. \n \n \n \n"}, {"page": 75, "text": " \n45 \nMCQA \n \n任务: \n \n根据所给的问题和选项选择正确答案。仅提供直接答案，用 \"X\" 表示正确\n的字母选项。 \n \n问题: \n{问题} \n \n选项: \n{选项} \n  \n答案: \nSTab. 75: MCQA evaluation prompt for Chinese. \n \n \nTASK: \n \nChoose the correct answer based on the given question and options. Provide \ndirect answers only with \"X\" where X is the correct letter choice. \n \nQuestion: \n{question} \n \nOptions: \n{options} \n \nAnswer: \nSTab. 76: MCQA evaluation prompt for English. \n \n \nTÂCHE: \n \nChoisissez la bonne réponse en vous basant sur la question et les options \ndonnées. Fournissez uniquement la réponse directe avec \"X\", où X est la \nbonne lettre. \n \nQuestion: \n{Question} \n \nOptions: \n{Options} \n \nRéponse: \nSTab. 77: MCQA evaluation prompt for French. \n"}, {"page": 76, "text": " \n46 \n \nAUFGABE: \n \nBitte wählen Sie die richtige Antwort basierend auf der gestellten Frage und \nden Optionen. Geben Sie nur die direkte Antwort mit \"X\", wobei X der \nrichtige Buchstabe ist. \n \nFrage: \n{Frage} \n \nOptionen: \n{Optionen} \n \nAntwort: \nSTab. 78: MCQA evaluation prompt for German. \n \n \nタスク: \n \n与えられた質問題と選択肢に基づいて正しい答えを選んでください。正\n解の「X」のみで答えてください。 \n \n質問: \n{質問} \n \n選択肢: \n{選択肢} \n \n答え: \nSTab. 79: MCQA evaluation prompt for Japanese. \n \n \n작업: \n \n주어진 질문과 선택지를 바탕으로 정답을 고르십시오. 정답이 \"X\" 일 \n경우 \"X\" 로만 직접적으로 답하십시오. \n \n질문: \n{질문} \n \n옵션: \n{옵션} \n \n답변: \nSTab. 80: MCQA evaluation prompt for Korean. \n \n"}, {"page": 77, "text": " \n47 \n \nTAREFA: \n \nEscolha a resposta correta com base na pergunta e nas opções fornecidas. \nForneça apenas a resposta direta com \"X\", onde X é a letra correta. \n \nPergunta: \n{Pergunta} \n \nOpções: \n{Opções} \n \nResposta: \nSTab. 81: MCQA evaluation prompt for Portuguese. \n \n \nTAREA: \n \nElige la respuesta correcta según la pregunta y las opciones dadas. \nProporciona solo la respuesta directa con \"X\", donde X es la letra correcta. \n \nPregunta: \n{Pregunta} \n \nOpciones: \n{Opciones} \n \nRespuesta: \nSTab. 82: MCQA evaluation prompt for Spanish. \n \n \nKAZI: \n \nChagua jibu sahihi kulingana na swali na chaguzi zilizotolewa. Toa majibu ya \nmoja kwa moja tu na \"X\" ambapo X ni chaguo sahihi cha herufi. \n \nSwali: \n{Swali} \n \nChaguzi: \n{Chaguzi} \n \nJibu: \nSTab. 83: MCQA evaluation prompt for Swahili. \n \n"}, {"page": 78, "text": " \n48 \n \nLIGGÉEY: \n \nTànnal tontu bi dëgg ci li ñu laaj ak tànneefi yi. Joxeel tontu yu jub rekk ak \n\"X\" fu X di araf bi dëgg. \n \nLaaj: \n{Laaj} \n \nTànneefi: \n{Tànneefi} \n \nTontu: \nSTab. 84: MCQA evaluation prompt for Wolof. \n \n \nIṢẸv: \n \nYan ìdáhùn tó tọ́ lórí ìbéèrè àti àwọn àṣàyàn tí a fún ọ. Ṣe ìdáhùn tààrà nìkan \npẹ̀lú \"X\" níbi tí X jẹ́ àṣàyàn lẹ́tà tó tọ́. \n \nÌbéèrè: \n{Ìbéèrè} \n \nÀwọn àṣàyàn: \n{Àwọn àṣàyàn} \n \nÌdáhùn: \nSTab. 85: MCQA evaluation prompt for Yoruba. \n \n \nUMSEBENZI: \n \nKhetha impendulo efanele ngokusekelwe kumbuzo kanye nezinketho \nezinikeziwe. Nikeza izimpendulo eziqondile kuphela ngo-\"X\" lapho u-X \neyinketho encwadi efanele. \n \nUmbuzo: \n{Umbuzo} \n \nIzinketho: \n{Izinketho} \n \nImpendulo: \nSTab. 86: MCQA evaluation prompt for Zulu.\n \n"}, {"page": 79, "text": " \n \n49 \nS2.3. Detailed Results of 56 Proprietary and Open-Weight LLMs \nLLMs \nChinese \nEnglish \nFrench \nGerman Japanese \nKorean Portuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nProprietary LLMs \nClaude-3.5-Haiku \n37.75±0.02 50.18±0.01 39.77±0.01 40.47±0.01 40.11±0.02 44.79±0.01 \n40.49±0.03 \n40.68±0.01 37.09±0.02 47.12±0.03 70.89±0.03 25.76±0.03 42.93±10.28 \nClaude-4.0-Sonnet \n68.31±0.30 75.46±0.30 63.88±0.12 59.72±0.41 62.98±0.18 58.72±0.35 \n68.48±0.15 \n67.95±0.24 54.74±0.10 45.47±0.53 47.38±0.20 53.82±0.25 \n60.58±8.74 \nGemini-2.5-Flash \n62.44±0.42 69.06±0.56 58.13±0.24 59.40±0.14 58.74±0.24 61.82±0.19 \n61.29±0.41 \n60.66±0.18 53.76±0.36 55.54±0.46 63.93±0.25 54.59±0.16 \n59.95±4.15 \nGPT-4o-mini \n63.52±0.36 63.71±0.22 49.23±0.25 46.17±0.29 53.56±0.25 57.05±0.69 \n49.98±0.35 \n50.12±0.28 42.23±0.23 27.65±0.44 40.99±0.30 31.44±0.29 47.97±10.84 \nGPT-4o \n38.77±0.30 45.35±0.14 42.09±0.04 40.10±0.37 41.20±0.28 47.08±0.30 \n42.96±0.22 \n42.76±0.37 34.31±0.28 15.88±0.38 33.78±0.25 34.17±0.22 \n38.20±7.98 \nGPT-4.1-nano \n46.35±0.22 55.66±0.25 48.35±0.53 45.15±0.40 53.86±0.87 52.72±0.87 \n50.24±0.38 \n49.21±0.30 48.22±0.39 46.26±1.10 43.89±0.48 39.74±0.72 \n48.31±4.35 \nGPT-4.1-mini \n59.10±0.22 64.68±0.36 55.96±0.33 52.49±0.17 55.87±0.45 54.89±0.20 \n58.95±0.22 \n59.87±0.23 46.81±0.34 21.39±0.31 36.37±0.50 37.96±0.40 50.36±12.13 \nGPT-4.1 \n49.67±0.18 64.53±0.21 51.20±0.14 49.07±0.26 49.68±0.33 52.27±0.33 \n51.45±0.09 \n51.39±0.18 40.29±0.32 45.09±0.29 41.09±0.38 34.23±0.15 \n48.33±7.34 \nGPT-5-nano \n45.90±0.46 41.14±0.96 34.17±0.56 45.48±0.46 51.23±0.60 42.11±0.65 \n49.31±0.47 \n51.51±0.54 58.19±0.48 31.55±0.88 58.94±0.73 55.01±0.87 \n47.04±8.46 \nGPT-5-mini \n60.99±0.37 68.93±0.37 60.20±0.22 58.04±0.28 59.24±0.44 59.09±0.27 \n63.31±0.21 \n63.86±0.17 53.63±0.18 26.32±0.34 33.52±0.17 52.49±0.13 54.97±12.15 \nGPT-5 \n65.70±0.34 73.41±0.21 57.73±0.48 56.70±0.18 63.00±0.25 67.47±0.26 \n60.32±0.20 \n62.98±0.19 51.64±0.43 48.43±0.57 45.86±0.41 45.43±0.32 \n58.22±8.64 \no4-mini \n68.97±0.31 73.34±0.20 67.13±0.19 66.90±0.25 68.42±0.22 67.08±0.27 \n70.24±0.41 \n71.06±0.23 61.42±0.22 32.30±0.45 49.20±0.31 55.24±0.62 62.61±11.40 \nOpen-Weight LLMs \nDeepSeek-V3 \n51.37±0.29 63.91±0.26 49.80±0.17 51.90±0.48 49.28±0.37 48.66±0.36 \n52.21±0.15 \n50.81±0.18 43.59±0.18 42.33±0.90 37.78±0.52 32.85±0.22 \n47.87±7.69 \nDeepSeek-R1 \n62.59±0.17 74.17±0.12 60.19±0.08 60.48±0.18 56.87±0.29 55.42±0.30 \n64.32±0.16 \n62.49±0.36 51.09±0.35 50.52±0.49 54.59±0.22 49.14±0.40 \n58.49±6.85 \nDeepSeek-R1-Qwen3-8B \n59.35±0.50 64.43±0.22 51.34±0.39 48.34±0.27 52.42±0.21 48.93±0.55 \n52.63±0.61 \n54.97±0.58 44.67±0.48 11.54±0.31 18.20±0.31 13.92±0.53 43.39±17.56 \nGemma-3-4B \n35.05±0.29 39.87±0.35 35.29±0.30 34.47±0.11 36.01±0.10 33.38±0.44 \n34.40±0.29 \n33.80±0.28 34.21±0.35 \n2.32±0.26 \n26.58±0.46 32.98±0.50 \n31.53±9.34 \nGemma-3-12B \n32.39±0.23 38.32±0.21 34.52±0.08 32.29±0.08 35.82±0.39 37.37±0.30 \n36.00±0.27 \n35.28±0.19 39.35±0.38 \n4.83±0.40 \n55.15±0.59 35.69±0.33 34.75±10.77 \nGemma-3-27B \n36.93±0.25 49.26±0.29 36.72±0.11 39.24±0.25 39.79±0.29 44.00±0.22 \n39.22±0.26 \n39.61±0.17 35.47±0.28 12.51±0.31 31.70±0.46 37.02±0.21 \n36.79±8.51 \ngpt-oss-20B \n68.88±0.18 62.77±0.71 61.55±0.20 60.24±0.40 64.22±0.45 64.60±0.48 \n62.81±0.09 \n63.91±0.25 55.16±0.52 24.47±0.49 46.64±0.76 54.23±0.52 57.46±11.55 \ngpt-oss-120B \n64.07±0.32 70.93±0.44 59.62±0.24 60.13±0.28 63.76±0.30 64.62±0.37 \n63.10±0.19 \n63.29±0.30 67.20±0.37 41.49±1.26 49.74±0.21 58.14±0.21 \n60.51±7.67 \nLLaMA-3.1-8B \n52.10±0.49 38.37±0.29 36.39±0.52 35.42±0.17 68.51±0.55 45.40±0.84 \n43.84±0.38 \n40.94±0.41 39.99±0.36 49.94±1.04 38.47±0.29 12.67±0.44 41.84±12.53 \nLLaMA-3.1-70B \n35.17±0.23 58.56±0.28 35.90±0.20 35.95±0.49 36.19±0.38 40.67±0.45 \n37.70±0.12 \n35.69±0.25 34.54±0.41 58.88±0.39 32.91±0.29 26.15±0.13 \n39.03±9.47 \nLLaMA-3.2-3B \n72.46±0.26 45.05±0.53 36.17±0.18 43.47±0.59 60.40±0.72 53.49±0.39 \n54.48±0.80 \n55.69±0.44 31.20±0.55 20.09±0.52 27.16±0.57 20.14±0.90 43.32±16.12 \nLLaMA-3.3-70B \n46.80±0.08 58.40±0.32 45.35±0.24 41.29±0.09 42.40±0.57 44.20±0.44 \n46.68±0.36 \n46.01±0.23 39.00±0.28 69.33±0.44 48.76±0.27 29.38±0.11 \n46.47±9.52 \nLLaMA-4-Scout \n48.14±0.42 56.46±0.41 42.52±0.31 42.47±0.26 43.55±0.28 45.14±0.21 \n47.65±0.21 \n44.04±0.29 40.84±0.30 54.19±0.32 62.82±0.15 44.04±0.31 \n47.65±6.51 \nLLaMA-4-Maverick \n50.31±0.28 61.59±0.16 46.13±0.12 45.74±0.11 50.17±0.29 53.14±0.47 \n48.98±0.32 \n49.04±0.12 39.67±0.24 63.19±0.36 42.39±0.51 37.39±0.32 \n48.98±7.51 \nMistral-7B-v0.3 \n33.77±0.47 \n2.96±0.24 \n15.37±0.18 32.37±0.81 26.52±0.49 27.15±0.67 \n22.79±0.58 \n19.82±0.64 14.17±0.47 \n9.84±0.39 \n10.87±0.66 10.78±0.43 \n18.87±9.43 \nMistral-Small-3.1-24B \n53.63±0.37 65.01±0.36 48.41±0.29 43.97±0.30 47.28±0.15 48.82±0.87 \n43.90±0.25 \n48.11±0.55 42.66±0.63 24.00±0.65 21.30±0.38 32.49±0.24 43.30±11.83 \nPhi-4-mini \n53.49±0.44 54.39±0.27 51.75±0.31 48.41±0.27 54.65±0.33 43.90±0.83 \n43.60±0.40 \n49.32±0.32 38.82±0.44 31.50±1.18 31.71±0.74 41.10±0.39 \n45.22±7.96 \nPhi-4-mini-Reasoning \n43.18±0.49 59.28±0.50 48.11±0.76 44.69±0.35 31.63±0.47 20.37±0.65 \n37.43±0.68 \n37.06±0.47 35.98±0.60 45.14±0.60 35.04±0.37 23.85±0.65 38.48±10.27 \nPhi-4 \n52.26±0.74 59.40±0.28 52.55±0.26 51.69±0.37 53.78±0.25 52.24±0.64 \n54.52±0.41 \n54.96±0.41 44.45±0.22 34.99±0.54 44.64±0.57 35.97±0.60 \n49.29±7.39 \nPhi-4-Reasoning \n63.14±0.30 69.16±0.23 59.47±0.48 55.84±0.25 61.87±0.44 59.33±0.21 \n57.68±0.36 \n61.65±0.47 57.74±0.62 15.25±0.35 48.78±0.74 37.89±0.68 53.98±14.00 \nQwen2.5-3B \n58.53±0.31 63.52±0.61 49.35±0.47 46.79±0.73 58.77±0.60 58.81±0.86 \n55.05±0.29 \n54.30±0.24 20.77±0.31 \n1.72±0.14 \n12.86±0.76 10.49±0.40 40.91±21.79 \nQwen2.5-7B \n50.22±0.47 62.04±0.48 53.87±0.67 47.88±0.58 49.61±0.54 44.70±0.75 \n54.62±0.19 \n50.11±0.37 45.43±0.97 28.77±0.29 38.67±0.50 29.16±0.82 \n46.26±9.58 \nQwen2.5-14B \n52.19±0.35 67.40±0.56 48.87±0.19 45.29±0.34 45.68±0.25 48.63±0.20 \n51.31±0.36 \n53.24±0.19 45.47±0.52 59.04±0.33 70.53±0.32 38.01±0.65 \n52.14±9.10 \nQwen2.5-72B \n47.66±0.56 65.00±0.41 55.39±0.15 52.62±0.28 50.07±0.51 53.59±0.22 \n58.92±0.30 \n60.07±0.21 38.96±0.34 71.40±0.14 55.46±0.83 29.40±0.30 53.21±10.78 \nQwQ-32B \n51.84±0.34 59.03±0.45 49.35±0.10 47.15±0.26 48.46±0.19 47.31±0.40 \n50.77±0.25 \n50.86±0.21 47.19±0.69 39.39±0.65 43.35±0.82 45.30±0.89 \n48.33±4.70 \nQwen3-1.7B \n51.29±0.36 54.77±0.46 45.41±0.53 55.56±0.44 49.77±0.53 41.56±0.44 \n45.10±0.52 \n47.07±0.40 49.64±0.65 37.22±0.44 47.39±0.45 42.44±1.41 \n47.27±5.22 \nQwen3-4B \n52.17±0.26 59.04±0.38 40.76±0.50 38.76±0.44 49.69±0.43 43.75±0.51 \n50.20±0.32 \n49.51±0.30 11.72±0.31 \n3.21±0.19 \n24.30±0.88 14.56±0.41 36.47±17.74 \nQwen3-4B-thinking \n50.26±0.48 61.98±0.24 50.45±0.28 47.11±0.16 51.75±0.33 49.65±0.31 \n54.19±0.30 \n54.57±0.37 26.59±0.43 \n3.43±0.37 \n27.40±0.74 11.66±0.25 40.75±18.10 \nQwen3-8B \n47.02±0.20 61.20±0.37 41.10±0.30 38.88±0.12 51.16±0.53 45.59±0.50 \n45.29±0.50 \n48.19±0.39 35.02±0.53 \n3.79±0.29 \n52.74±0.54 17.08±0.11 40.59±15.33 \nQwen3-8B-thinking \n48.72±0.59 62.34±0.23 52.52±0.36 48.36±0.18 56.03±0.40 50.50±0.57 \n56.11±0.37 \n56.56±0.35 29.98±0.41 \n8.27±0.17 \n29.72±0.87 23.39±0.73 43.54±16.06 \nQwen3-14B \n58.21±0.38 68.72±0.32 57.62±0.38 52.86±0.56 58.35±0.41 57.16±0.34 \n63.26±0.22 \n61.61±0.28 49.15±0.70 \n2.41±0.27 \n32.63±0.70 29.04±0.38 49.25±18.22 \nQwen3-14B-thinking \n57.79±0.32 69.96±0.21 59.83±0.44 56.71±0.15 61.92±0.36 60.43±0.53 \n65.14±0.56 \n62.68±0.41 55.64±0.47 \n2.60±0.19 \n27.19±0.84 33.88±0.91 51.15±19.06 \nBaichuan-M2-32B \n62.71±0.33 57.50±0.43 50.36±0.45 59.86±0.37 58.83±0.41 59.71±0.47 \n54.70±0.51 \n55.67±0.44 36.48±0.69 30.79±0.20 41.07±0.74 43.31±1.02 50.92±10.13 \nBio-Medical-LLaMA-3-8B \n35.03±0.44 40.84±0.43 25.97±0.09 31.76±0.26 34.83±0.49 25.73±0.15 \n32.19±0.30 \n29.83±0.31 26.07±0.06 36.28±0.79 30.89±0.20 35.01±0.32 \n32.04±4.52 \nMediPhi \n64.56±0.81 72.46±0.37 68.53±0.16 73.89±0.22 60.16±0.42 41.85±0.53 \n72.57±0.27 \n74.08±0.45 25.73±0.87 44.63±0.91 21.35±0.59 42.70±0.58 55.21±18.55 \nMedGemma-4B \n30.72±0.30 36.53±0.19 39.85±0.30 35.28±0.54 32.59±0.22 36.40±0.33 \n35.67±0.20 \n36.33±0.25 32.33±0.23 16.32±0.80 28.52±0.55 28.07±0.19 \n32.38±5.96 \nMedGemma-27B \n46.39±0.52 54.81±0.56 45.30±0.17 51.22±0.42 48.42±0.37 52.05±0.39 \n51.33±0.39 \n50.76±0.26 48.75±0.29 24.67±0.69 41.94±0.69 46.10±0.75 \n46.81±7.55 \nMedReason-8B \n27.60±0.20 \n6.41±0.22 \n4.32±0.21 \n4.15±0.21 \n25.53±0.50 30.91±0.22 \n24.10±0.12 \n1.69±0.18 \n8.43±0.31 \n0.49±0.10 \n37.09±0.43 23.86±0.11 16.22±12.66 \nHuatuoGPT-o1-7B \n49.02±0.18 57.66±0.80 52.03±0.39 51.43±0.40 54.46±0.56 49.95±0.20 \n52.49±0.28 \n55.11±0.35 16.97±0.56 \n2.74±0.16 \n6.52±0.21 \n4.01±0.32 \n37.70±21.85 \nHuatuoGPT-o1-8B \n33.68±0.31 34.41±0.13 36.26±0.37 31.37±0.48 44.02±0.39 35.35±0.45 \n37.86±0.53 \n37.12±0.31 27.49±0.39 \n1.78±0.18 \n21.64±0.68 \n7.56±0.37 \n29.04±12.28 \nHuatuoGPT-o1-70B \n44.62±0.29 58.75±0.13 47.65±0.22 46.30±0.27 43.58±0.48 46.88±0.20 \n48.04±0.27 \n49.35±0.08 37.81±0.20 55.35±0.63 39.05±0.21 33.87±0.33 \n45.94±6.77 \nHuatuoGPT-o1-72B \n52.81±0.40 64.92±0.17 59.08±0.35 57.54±0.24 53.93±0.26 55.59±0.50 \n61.61±0.33 \n63.33±0.30 44.26±0.51 14.88±0.52 40.32±0.60 25.41±0.13 49.47±15.10 \nOpenBioLLM-8B \n9.19±0.27 \n11.68±0.57 13.83±0.56 17.50±0.29 \n8.92±0.52 \n8.73±0.55 \n17.15±0.39 \n19.13±0.64 10.21±0.41 14.51±0.40 11.25±0.72 12.28±0.55 \n12.86±3.48 \nOpenBioLLM-70B \n12.77±0.41 32.56±0.13 26.86±0.21 15.55±0.22 10.26±0.47 10.92±0.56 \n22.36±0.43 \n27.53±0.20 11.47±0.24 11.47±0.55 16.16±0.40 15.93±0.50 \n17.82±7.35 \nSTab. 87: Performance evaluation of 56 LLMs on BioNLI. \n"}, {"page": 80, "text": " \n \n50 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n37.75 \n50.18 \n39.78 \n40.47 \n40.09 \n44.79 \n40.49 \n40.67 \n37.10 \n47.15 \n70.90 \n25.78 \nClaude-4.0-Sonnet \n68.45 \n75.17 \n63.91 \n59.96 \n63.12 \n58.54 \n68.29 \n67.62 \n54.74 \n45.03 \n47.24 \n53.42 \nGemini-2.5-Flash \n61.98 \n69.71 \n57.87 \n59.57 \n58.58 \n61.64 \n61.46 \n60.67 \n53.60 \n55.39 \n63.53 \n54.58 \nGPT-4o-mini \n63.98 \n63.93 \n49.53 \n46.13 \n53.21 \n57.87 \n50.47 \n50.45 \n42.25 \n27.84 \n41.17 \n31.24 \nGPT-4o \n38.58 \n45.39 \n42.04 \n40.31 \n41.21 \n46.81 \n42.85 \n42.56 \n34.29 \n15.62 \n34.00 \n34.13 \nGPT-4.1-nano \n46.04 \n55.39 \n47.98 \n45.46 \n53.06 \n51.24 \n49.89 \n49.51 \n48.49 \n46.09 \n43.33 \n39.87 \nGPT-4.1-mini \n59.01 \n64.38 \n56.09 \n52.22 \n55.69 \n54.94 \n59.03 \n59.78 \n46.92 \n21.71 \n36.74 \n38.38 \nGPT-4.1 \n49.89 \n64.67 \n51.24 \n49.35 \n49.73 \n52.36 \n51.55 \n51.66 \n39.93 \n44.99 \n40.85 \n34.36 \nGPT-5-nano \n45.73 \n42.18 \n34.54 \n45.73 \n50.58 \n41.03 \n48.85 \n52.16 \n57.71 \n30.58 \n58.29 \n55.66 \nGPT-5-mini \n60.56 \n68.97 \n60.11 \n57.80 \n58.79 \n59.33 \n63.39 \n63.89 \n53.84 \n25.84 \n33.37 \n52.34 \nGPT-5 \n66.27 \n73.37 \n57.64 \n56.65 \n62.70 \n67.82 \n60.29 \n62.76 \n51.69 \n47.84 \n46.00 \n45.30 \no4-mini \n69.30 \n73.35 \n66.94 \n67.15 \n68.52 \n66.67 \n70.09 \n71.17 \n61.46 \n32.04 \n48.72 \n54.45 \nOpen-Weight LLMs \nDeepSeek-V3 \n51.30 \n64.00 \n49.98 \n51.96 \n48.92 \n49.10 \n52.25 \n50.74 \n43.48 \n41.91 \n37.15 \n32.47 \nDeepSeek-R1 \n62.58 \n74.34 \n60.27 \n60.67 \n56.94 \n55.37 \n64.36 \n62.11 \n50.92 \n50.18 \n54.52 \n49.51 \nDeepSeek-R1-Qwen3-8B \n59.66 \n64.38 \n51.37 \n48.36 \n52.63 \n49.26 \n51.62 \n55.51 \n44.47 \n12.00 \n18.56 \n14.07 \nGemma-3-4B \n34.72 \n40.22 \n35.15 \n34.36 \n35.84 \n32.72 \n34.45 \n33.78 \n33.89 \n2.22 \n25.87 \n32.22 \nGemma-3-12B \n32.74 \n38.02 \n34.45 \n32.20 \n35.48 \n37.66 \n36.27 \n35.21 \n38.83 \n4.99 \n55.15 \n36.04 \nGemma-3-27B \n37.19 \n49.75 \n36.61 \n39.28 \n39.89 \n44.27 \n39.62 \n39.73 \n35.51 \n12.11 \n32.34 \n37.35 \ngpt-oss-20B \n68.99 \n63.69 \n61.28 \n60.16 \n64.22 \n63.91 \n62.70 \n64.34 \n54.90 \n24.11 \n45.96 \n54.49 \ngpt-oss-120B \n64.25 \n70.61 \n59.91 \n59.89 \n63.69 \n64.40 \n62.94 \n63.03 \n67.73 \n42.36 \n49.75 \n57.78 \nLLaMA-3.1-8B \n52.61 \n38.02 \n36.27 \n35.15 \n68.07 \n46.00 \n43.39 \n40.70 \n40.27 \n50.29 \n38.67 \n12.45 \nLLaMA-3.1-70B \n34.90 \n58.36 \n35.80 \n35.53 \n36.11 \n40.92 \n37.62 \n35.91 \n34.83 \n59.15 \n32.99 \n26.36 \nLLaMA-3.2-3B \n72.29 \n45.78 \n36.43 \n43.87 \n59.73 \n53.37 \n55.57 \n56.16 \n30.61 \n20.65 \n26.83 \n19.82 \nLLaMA-3.3-70B \n46.85 \n58.47 \n45.55 \n41.37 \n42.65 \n44.47 \n46.45 \n45.87 \n38.90 \n69.12 \n48.90 \n29.39 \nLLaMA-4-Scout \n48.07 \n56.90 \n42.83 \n42.13 \n44.02 \n45.26 \n47.62 \n43.98 \n40.90 \n54.31 \n63.01 \n44.45 \nLLaMA-4-Maverick \n50.49 \n61.71 \n46.22 \n45.80 \n50.25 \n53.03 \n49.15 \n49.03 \n39.87 \n63.39 \n43.12 \n37.87 \nMistral-7B-v0.3 \n34.11 \n2.81 \n15.60 \n31.98 \n26.22 \n27.75 \n22.94 \n19.21 \n13.89 \n10.11 \n11.19 \n10.25 \nMistral-Small-3.1-24B \n53.35 \n64.94 \n48.00 \n44.02 \n47.21 \n48.40 \n44.04 \n48.16 \n42.20 \n23.71 \n21.84 \n32.47 \nPhi-4-mini \n53.08 \n54.81 \n52.07 \n48.27 \n54.79 \n43.37 \n43.80 \n49.21 \n39.21 \n31.26 \n32.61 \n40.65 \nPhi-4-mini-Reasoning \n43.19 \n58.99 \n48.97 \n44.49 \n31.28 \n20.31 \n36.67 \n37.75 \n36.38 \n44.27 \n34.99 \n24.43 \nPhi-4 \n51.21 \n59.48 \n52.43 \n51.26 \n53.57 \n52.38 \n55.03 \n54.40 \n44.34 \n34.09 \n44.13 \n35.93 \nPhi-4-Reasoning \n63.46 \n69.51 \n58.81 \n56.18 \n61.17 \n59.01 \n57.87 \n61.42 \n58.02 \n15.75 \n49.21 \n36.90 \nQwen2.5-3B \n58.88 \n63.44 \n48.70 \n47.35 \n59.26 \n59.55 \n55.12 \n54.40 \n20.79 \n1.82 \n13.80 \n10.34 \nQwen2.5-7B \n50.29 \n62.20 \n53.96 \n47.01 \n50.43 \n44.72 \n54.61 \n50.20 \n45.80 \n28.63 \n38.09 \n27.89 \nQwen2.5-14B \n52.45 \n68.11 \n48.79 \n45.48 \n45.37 \n48.45 \n51.19 \n53.39 \n45.64 \n59.08 \n70.72 \n38.16 \nQwen2.5-72B \n47.17 \n65.46 \n55.60 \n52.20 \n49.51 \n53.66 \n59.10 \n60.29 \n39.33 \n71.35 \n56.16 \n29.26 \nQwQ-32B \n51.82 \n59.17 \n49.39 \n46.88 \n48.52 \n46.61 \n50.40 \n51.12 \n46.61 \n39.75 \n44.70 \n45.21 \nQwen3-1.7B \n51.24 \n54.67 \n46.07 \n55.30 \n50.02 \n41.46 \n45.24 \n46.47 \n50.40 \n37.51 \n47.10 \n42.74 \nQwen3-4B \n51.91 \n58.92 \n40.47 \n38.20 \n49.55 \n43.06 \n50.29 \n49.24 \n11.35 \n3.17 \n23.15 \n15.19 \nQwen3-4B-thinking \n50.67 \n61.87 \n50.07 \n47.35 \n52.31 \n50.09 \n53.80 \n55.19 \n26.54 \n3.66 \n27.30 \n11.48 \nQwen3-8B \n47.17 \n61.26 \n41.21 \n39.01 \n51.12 \n45.78 \n45.73 \n48.74 \n34.83 \n3.55 \n52.52 \n17.12 \nQwen3-8B-thinking \n48.90 \n62.58 \n52.34 \n48.45 \n56.18 \n50.34 \n55.98 \n57.03 \n29.44 \n8.07 \n28.92 \n23.53 \nQwen3-14B \n58.38 \n69.06 \n57.39 \n53.44 \n58.13 \n57.46 \n63.44 \n61.30 \n50.20 \n2.07 \n33.24 \n29.01 \nQwen3-14B-thinking \n57.73 \n70.18 \n59.82 \n56.47 \n62.13 \n60.76 \n64.49 \n63.06 \n55.60 \n2.63 \n26.52 \n32.65 \nBaichuan-M2-32B \n62.29 \n57.10 \n49.82 \n60.36 \n58.45 \n59.19 \n54.56 \n55.39 \n35.69 \n30.85 \n42.34 \n41.73 \nBio-Medical-LLaMA-3-8B \n35.53 \n40.70 \n26.07 \n31.66 \n34.40 \n25.48 \n32.00 \n29.89 \n26.11 \n36.02 \n31.17 \n34.83 \nMediPhi \n65.08 \n71.82 \n68.79 \n73.91 \n60.43 \n41.24 \n72.18 \n74.20 \n26.11 \n43.66 \n21.62 \n42.58 \nMedGemma-4B \n30.79 \n36.36 \n39.55 \n34.38 \n32.88 \n36.31 \n35.89 \n36.63 \n32.36 \n14.92 \n28.11 \n28.20 \nMedGemma-27B \n45.64 \n53.93 \n45.35 \n51.53 \n47.98 \n51.91 \n51.30 \n50.45 \n48.94 \n24.94 \n42.63 \n45.75 \nMedReason-8B \n27.82 \n6.72 \n4.22 \n4.02 \n25.08 \n31.08 \n24.22 \n1.87 \n8.38 \n0.34 \n37.75 \n23.84 \nHuatuoGPT-o1-7B \n48.79 \n57.33 \n52.18 \n51.42 \n54.43 \n49.84 \n52.94 \n54.74 \n17.46 \n2.72 \n6.52 \n4.52 \nHuatuoGPT-o1-8B \n33.24 \n34.52 \n36.72 \n31.82 \n44.13 \n35.12 \n37.24 \n37.64 \n28.04 \n1.91 \n22.29 \n7.24 \nHuatuoGPT-o1-70B \n44.90 \n58.88 \n47.53 \n46.20 \n43.42 \n46.94 \n48.02 \n49.39 \n37.55 \n54.99 \n38.94 \n33.75 \nHuatuoGPT-o1-72B \n52.67 \n65.15 \n59.46 \n57.33 \n53.96 \n55.12 \n61.24 \n63.46 \n43.82 \n14.67 \n41.06 \n25.28 \nOpenBioLLM-8B \n9.28 \n11.19 \n13.10 \n17.35 \n9.10 \n9.39 \n17.42 \n19.66 \n9.71 \n14.70 \n11.10 \n12.29 \nOpenBioLLM-70B \n12.81 \n32.47 \n26.74 \n15.57 \n9.98 \n11.30 \n22.65 \n27.71 \n11.75 \n11.48 \n15.69 \n16.04 \nSTab. 88: Zero-Shot performance evaluation of 56 LLMs on BioNLI (Run 1). \n"}, {"page": 81, "text": " \n \n51 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n37.75 \n50.18 \n39.78 \n40.45 \n40.09 \n44.79 \n40.54 \n40.70 \n37.08 \n47.15 \n70.88 \n25.73 \nClaude-4.0-Sonnet \n68.61 \n75.73 \n63.98 \n59.44 \n62.97 \n58.25 \n68.65 \n68.04 \n54.63 \n45.35 \n47.42 \n53.80 \nGemini-2.5-Flash \n63.08 \n69.28 \n58.27 \n59.26 \n58.99 \n61.82 \n61.93 \n60.85 \n53.64 \n55.98 \n64.13 \n54.40 \nGPT-4o-mini \n63.19 \n63.37 \n48.90 \n46.38 \n53.62 \n57.60 \n49.51 \n49.71 \n42.20 \n27.82 \n41.15 \n31.10 \nGPT-4o \n38.58 \n45.37 \n42.09 \n40.22 \n41.28 \n47.42 \n42.85 \n42.29 \n34.04 \n15.82 \n33.64 \n34.27 \nGPT-4.1-nano \n46.63 \n55.46 \n47.60 \n44.45 \n55.17 \n52.92 \n50.07 \n49.53 \n47.89 \n47.91 \n43.82 \n39.93 \nGPT-4.1-mini \n59.19 \n64.67 \n55.42 \n52.52 \n56.09 \n54.85 \n58.79 \n59.75 \n47.06 \n21.44 \n36.94 \n38.18 \nGPT-4.1 \n49.60 \n64.20 \n51.37 \n48.90 \n49.89 \n52.13 \n51.44 \n51.21 \n40.09 \n45.06 \n41.28 \n33.98 \nGPT-5-nano \n46.27 \n40.25 \n34.72 \n45.91 \n50.88 \n42.79 \n48.97 \n50.90 \n58.88 \n31.35 \n59.78 \n54.31 \nGPT-5-mini \n61.51 \n69.21 \n59.93 \n57.73 \n59.46 \n59.19 \n63.21 \n63.73 \n53.66 \n26.65 \n33.69 \n52.40 \nGPT-5 \n65.51 \n73.48 \n58.09 \n56.67 \n62.83 \n67.37 \n60.54 \n62.85 \n52.29 \n49.12 \n46.27 \n44.97 \no4-mini \n69.10 \n73.10 \n67.19 \n66.67 \n68.31 \n66.99 \n69.98 \n71.30 \n61.60 \n32.40 \n49.46 \n56.16 \nOpen-Weight LLMs \nDeepSeek-V3 \n51.71 \n63.82 \n49.71 \n52.65 \n49.89 \n48.45 \n52.25 \n50.72 \n43.60 \n43.75 \n38.18 \n32.97 \nDeepSeek-R1 \n62.36 \n74.04 \n60.11 \n60.22 \n56.72 \n55.57 \n64.56 \n62.11 \n50.72 \n51.24 \n54.27 \n49.33 \nDeepSeek-R1-Qwen3-8B \n59.06 \n64.65 \n50.67 \n48.11 \n52.38 \n49.37 \n52.54 \n54.02 \n44.16 \n11.28 \n18.45 \n14.25 \nGemma-3-4B \n34.97 \n39.96 \n35.33 \n34.38 \n36.09 \n33.42 \n34.04 \n33.39 \n34.02 \n1.98 \n26.63 \n33.30 \nGemma-3-12B \n32.18 \n38.25 \n34.49 \n32.22 \n35.64 \n37.64 \n35.73 \n35.57 \n39.82 \n4.36 \n56.04 \n35.60 \nGemma-3-27B \n37.03 \n49.03 \n36.81 \n39.66 \n39.91 \n44.16 \n39.01 \n39.69 \n35.33 \n12.54 \n31.24 \n36.79 \ngpt-oss-20B \n68.65 \n61.98 \n61.84 \n60.67 \n64.22 \n65.17 \n62.94 \n63.84 \n54.81 \n24.52 \n47.51 \n54.25 \ngpt-oss-120B \n64.09 \n70.34 \n59.64 \n60.11 \n64.04 \n64.65 \n63.15 \n63.55 \n67.24 \n41.84 \n49.51 \n58.27 \nLLaMA-3.1-8B \n52.36 \n38.74 \n36.83 \n35.55 \n68.43 \n46.40 \n44.13 \n41.48 \n40.31 \n49.08 \n38.13 \n12.34 \nLLaMA-3.1-70B \n35.48 \n58.90 \n35.96 \n35.69 \n36.49 \n40.90 \n37.75 \n36.00 \n34.97 \n59.19 \n33.06 \n26.16 \nLLaMA-3.2-3B \n72.47 \n44.49 \n36.09 \n42.85 \n60.45 \n53.78 \n53.62 \n55.78 \n31.55 \n20.11 \n26.97 \n19.06 \nLLaMA-3.3-70B \n46.85 \n58.02 \n45.35 \n41.35 \n42.00 \n43.46 \n47.26 \n45.98 \n38.74 \n69.69 \n48.85 \n29.33 \nLLaMA-4-Scout \n48.43 \n56.72 \n42.36 \n42.72 \n43.35 \n45.08 \n47.55 \n43.87 \n40.92 \n54.09 \n62.79 \n44.11 \nLLaMA-4-Maverick \n50.11 \n61.57 \n45.96 \n45.84 \n50.47 \n52.40 \n48.72 \n48.94 \n39.26 \n63.57 \n42.02 \n37.24 \nMistral-7B-v0.3 \n34.20 \n3.24 \n15.15 \n33.26 \n25.80 \n26.74 \n22.94 \n19.28 \n14.76 \n9.37 \n11.39 \n11.26 \nMistral-Small-3.1-24B \n54.07 \n64.58 \n48.58 \n43.46 \n47.08 \n48.79 \n44.18 \n48.04 \n41.80 \n23.91 \n21.37 \n32.76 \nPhi-4-mini \n53.55 \n54.13 \n51.80 \n48.58 \n54.70 \n43.93 \n43.33 \n49.84 \n38.92 \n31.01 \n31.26 \n41.10 \nPhi-4-mini-Reasoning \n42.54 \n60.11 \n47.78 \n44.47 \n31.33 \n19.89 \n36.79 \n36.99 \n35.26 \n44.83 \n35.10 \n23.26 \nPhi-4 \n53.24 \n58.92 \n52.81 \n51.80 \n53.48 \n52.72 \n54.65 \n55.17 \n44.79 \n34.90 \n45.42 \n36.79 \nPhi-4-Reasoning \n62.67 \n69.01 \n59.51 \n55.51 \n62.20 \n59.39 \n57.37 \n61.06 \n56.85 \n15.35 \n49.30 \n38.79 \nQwen2.5-3B \n58.61 \n63.62 \n49.26 \n45.93 \n58.79 \n58.99 \n54.65 \n54.36 \n21.24 \n1.55 \n13.46 \n11.15 \nQwen2.5-7B \n50.92 \n62.40 \n53.15 \n47.69 \n49.42 \n43.93 \n54.58 \n49.98 \n45.69 \n28.97 \n38.22 \n29.37 \nQwen2.5-14B \n52.22 \n67.48 \n49.15 \n45.62 \n45.64 \n48.47 \n50.83 \n53.17 \n45.06 \n58.97 \n70.65 \n37.57 \nQwen2.5-72B \n48.29 \n64.45 \n55.30 \n52.61 \n50.70 \n53.75 \n58.92 \n60.18 \n39.21 \n71.48 \n54.85 \n29.53 \nQwQ-32B \n51.60 \n58.92 \n49.44 \n47.19 \n48.43 \n47.46 \n51.03 \n50.99 \n47.51 \n39.12 \n43.37 \n45.44 \nQwen3-1.7B \n51.51 \n54.52 \n44.76 \n55.01 \n50.22 \n42.00 \n44.25 \n47.30 \n49.84 \n36.81 \n48.13 \n40.25 \nQwen3-4B \n52.13 \n59.17 \n40.18 \n39.08 \n49.44 \n44.09 \n50.20 \n49.15 \n11.60 \n3.35 \n23.64 \n14.72 \nQwen3-4B-thinking \n50.65 \n61.66 \n50.79 \n46.99 \n51.48 \n49.51 \n54.45 \n54.54 \n27.26 \n3.96 \n26.27 \n12.09 \nQwen3-8B \n46.74 \n60.65 \n41.55 \n38.99 \n51.66 \n44.97 \n44.94 \n48.22 \n35.21 \n3.91 \n53.21 \n17.03 \nQwen3-8B-thinking \n48.40 \n62.31 \n53.10 \n48.61 \n56.31 \n51.10 \n56.70 \n56.63 \n30.29 \n8.25 \n29.73 \n24.56 \nQwen3-14B \n58.40 \n68.34 \n57.73 \n52.76 \n59.06 \n57.10 \n63.12 \n62.02 \n49.46 \n2.79 \n31.80 \n29.01 \nQwen3-14B-thinking \n57.48 \n69.82 \n60.11 \n56.88 \n62.18 \n60.67 \n65.24 \n62.25 \n56.22 \n2.40 \n27.17 \n33.44 \nBaichuan-M2-32B \n62.63 \n57.48 \n50.13 \n59.60 \n58.97 \n60.40 \n54.67 \n56.00 \n36.90 \n30.72 \n41.10 \n44.27 \nBio-Medical-LLaMA-3-8B \n35.28 \n40.20 \n26.02 \n31.37 \n34.67 \n25.75 \n32.54 \n29.48 \n25.96 \n35.55 \n30.67 \n34.56 \nMediPhi \n64.99 \n72.52 \n68.40 \n73.51 \n60.27 \n41.37 \n72.81 \n74.72 \n24.58 \n43.98 \n21.06 \n43.28 \nMedGemma-4B \n30.65 \n36.31 \n39.84 \n35.78 \n32.40 \n36.90 \n35.75 \n36.18 \n32.20 \n16.70 \n28.11 \n27.75 \nMedGemma-27B \n47.10 \n55.28 \n45.10 \n51.55 \n48.81 \n52.45 \n51.82 \n50.72 \n48.45 \n23.89 \n41.75 \n46.72 \nMedReason-8B \n27.33 \n6.20 \n4.40 \n4.29 \n25.75 \n31.06 \n23.98 \n1.62 \n8.70 \n0.61 \n36.61 \n23.87 \nHuatuoGPT-o1-7B \n49.10 \n58.16 \n51.75 \n51.91 \n53.51 \n49.80 \n52.40 \n54.92 \n17.30 \n2.56 \n6.52 \n3.80 \nHuatuoGPT-o1-8B \n33.55 \n34.54 \n36.13 \n31.10 \n44.31 \n34.90 \n38.52 \n37.10 \n27.17 \n1.55 \n20.72 \n8.09 \nHuatuoGPT-o1-70B \n44.18 \n58.76 \n47.39 \n46.27 \n43.87 \n46.85 \n48.34 \n49.26 \n38.02 \n55.33 \n38.90 \n33.78 \nHuatuoGPT-o1-72B \n52.56 \n64.88 \n59.08 \n57.84 \n54.29 \n55.42 \n61.78 \n63.28 \n44.27 \n14.11 \n39.73 \n25.55 \nOpenBioLLM-8B \n8.76 \n12.02 \n14.04 \n17.35 \n9.69 \n7.91 \n17.48 \n19.60 \n10.58 \n14.99 \n10.72 \n11.69 \nOpenBioLLM-70B \n12.52 \n32.61 \n26.92 \n15.91 \n10.25 \n11.44 \n21.89 \n27.71 \n11.42 \n10.63 \n16.00 \n16.52 \nSTab. 89: Zero-Shot performance evaluation of 56 LLMs on BioNLI (Run 2). \n"}, {"page": 82, "text": " \n \n52 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n37.73 \n50.20 \n39.78 \n40.47 \n40.11 \n44.81 \n40.49 \n40.67 \n37.06 \n47.12 \n70.85 \n25.75 \nClaude-4.0-Sonnet \n67.82 \n75.82 \n63.82 \n59.33 \n62.67 \n58.76 \n68.61 \n68.25 \n54.85 \n46.09 \n47.39 \n53.80 \nGemini-2.5-Flash \n62.40 \n68.20 \n58.40 \n59.26 \n59.03 \n61.62 \n61.03 \n60.43 \n54.20 \n54.88 \n63.98 \n54.47 \nGPT-4o-mini \n63.33 \n63.60 \n49.42 \n45.69 \n53.46 \n56.34 \n50.04 \n50.04 \n42.07 \n26.88 \n40.72 \n31.64 \nGPT-4o \n38.72 \n45.39 \n42.09 \n40.49 \n41.21 \n46.97 \n43.33 \n43.01 \n34.52 \n16.25 \n33.84 \n34.16 \nGPT-4.1-nano \n46.31 \n55.60 \n48.63 \n45.35 \n54.29 \n53.26 \n50.22 \n49.17 \n48.47 \n46.27 \n43.60 \n39.37 \nGPT-4.1-mini \n58.83 \n64.52 \n56.16 \n52.70 \n55.21 \n55.17 \n58.92 \n59.84 \n46.22 \n21.33 \n36.13 \n38.16 \nGPT-4.1 \n49.42 \n64.63 \n51.12 \n49.35 \n49.10 \n52.70 \n51.53 \n51.37 \n40.74 \n44.88 \n41.42 \n34.31 \nGPT-5-nano \n45.44 \n40.34 \n34.45 \n44.74 \n50.97 \n42.13 \n49.10 \n51.91 \n58.49 \n32.54 \n58.58 \n56.20 \nGPT-5-mini \n60.88 \n69.33 \n60.45 \n58.40 \n59.08 \n59.15 \n63.08 \n63.87 \n53.73 \n26.20 \n33.66 \n52.58 \nGPT-5 \n65.39 \n73.08 \n57.03 \n57.01 \n63.03 \n67.55 \n60.00 \n62.97 \n51.26 \n48.49 \n45.98 \n45.46 \no4-mini \n69.12 \n73.44 \n67.28 \n66.94 \n68.52 \n67.39 \n70.76 \n70.74 \n61.51 \n32.11 \n49.08 \n55.33 \nOpen-Weight LLMs \nDeepSeek-V3 \n51.53 \n64.29 \n49.93 \n51.71 \n49.12 \n48.18 \n52.16 \n50.63 \n43.69 \n42.02 \n38.25 \n32.99 \nDeepSeek-R1 \n62.58 \n74.18 \n60.13 \n60.56 \n57.12 \n55.62 \n64.13 \n62.88 \n51.01 \n50.81 \n54.85 \n48.70 \nDeepSeek-R1-Qwen3-8B \n59.51 \n64.09 \n51.62 \n48.61 \n52.09 \n49.33 \n53.21 \n55.37 \n45.46 \n11.26 \n17.78 \n13.17 \nGemma-3-4B \n35.51 \n40.11 \n35.37 \n34.63 \n36.04 \n33.39 \n34.76 \n33.89 \n34.65 \n2.54 \n26.47 \n33.48 \nGemma-3-12B \n32.47 \n38.31 \n34.45 \n32.34 \n35.84 \n36.99 \n35.78 \n35.15 \n39.51 \n4.52 \n55.08 \n35.17 \nGemma-3-27B \n36.83 \n49.12 \n36.72 \n39.15 \n40.04 \n43.84 \n39.28 \n39.39 \n35.82 \n12.83 \n31.73 \n37.06 \ngpt-oss-20B \n68.99 \n62.58 \n61.55 \n59.62 \n63.64 \n64.52 \n62.74 \n63.78 \n55.53 \n24.67 \n47.42 \n54.76 \ngpt-oss-120B \n63.53 \n71.17 \n59.37 \n59.89 \n64.04 \n65.15 \n63.24 \n62.99 \n67.03 \n42.40 \n49.91 \n58.16 \nLLaMA-3.1-8B \n51.75 \n38.16 \n37.01 \n35.39 \n67.91 \n45.17 \n43.48 \n41.26 \n40.02 \n48.61 \n38.18 \n12.61 \nLLaMA-3.1-70B \n35.15 \n58.72 \n35.96 \n35.71 \n35.55 \n40.38 \n37.55 \n35.55 \n34.70 \n58.49 \n33.28 \n26.11 \nLLaMA-3.2-3B \n72.81 \n45.39 \n36.29 \n43.87 \n60.63 \n53.06 \n55.06 \n54.97 \n31.78 \n20.49 \n26.49 \n19.75 \nLLaMA-3.3-70B \n46.74 \n58.38 \n45.10 \n41.30 \n42.72 \n44.61 \n46.40 \n46.11 \n38.76 \n69.26 \n48.27 \n29.57 \nLLaMA-4-Scout \n47.44 \n56.54 \n42.07 \n42.25 \n43.53 \n45.33 \n48.00 \n43.82 \n41.28 \n53.69 \n62.67 \n44.00 \nLLaMA-4-Maverick \n50.70 \n61.39 \n46.09 \n45.55 \n49.84 \n53.66 \n48.76 \n49.10 \n39.78 \n62.63 \n42.63 \n37.53 \nMistral-7B-v0.3 \n33.39 \n2.85 \n15.44 \n31.48 \n26.90 \n27.55 \n22.79 \n20.54 \n13.93 \n9.80 \n11.39 \n10.99 \nMistral-Small-3.1-24B \n53.96 \n65.24 \n48.49 \n44.09 \n47.46 \n47.89 \n43.51 \n48.76 \n42.97 \n24.43 \n20.79 \n32.43 \nPhi-4-mini \n53.98 \n54.25 \n51.46 \n48.00 \n54.07 \n43.66 \n44.13 \n49.24 \n39.19 \n31.48 \n31.53 \n41.55 \nPhi-4-mini-Reasoning \n42.97 \n58.81 \n47.08 \n45.30 \n31.64 \n21.48 \n38.27 \n37.19 \n36.61 \n45.84 \n35.62 \n23.28 \nPhi-4 \n52.54 \n59.53 \n52.18 \n52.02 \n54.09 \n52.94 \n53.98 \n55.48 \n44.22 \n35.24 \n44.83 \n35.57 \nPhi-4-Reasoning \n63.15 \n68.97 \n59.91 \n55.93 \n62.22 \n59.53 \n57.57 \n61.80 \n58.18 \n14.94 \n49.42 \n38.02 \nQwen2.5-3B \n58.04 \n64.31 \n49.57 \n47.64 \n58.29 \n59.35 \n54.88 \n54.27 \n20.40 \n1.84 \n12.02 \n10.11 \nQwen2.5-7B \n50.09 \n61.53 \n54.20 \n48.16 \n49.62 \n44.74 \n54.94 \n50.70 \n45.51 \n28.67 \n39.06 \n29.84 \nQwen2.5-14B \n52.11 \n67.48 \n48.65 \n44.74 \n45.71 \n48.56 \n51.19 \n52.94 \n44.90 \n58.56 \n70.45 \n38.88 \nQwen2.5-72B \n47.48 \n65.15 \n55.44 \n52.83 \n50.43 \n53.44 \n58.85 \n59.75 \n39.01 \n71.53 \n55.89 \n29.15 \nQwQ-32B \n51.66 \n58.31 \n49.24 \n47.42 \n48.20 \n47.44 \n50.90 \n50.88 \n48.07 \n38.43 \n42.90 \n46.72 \nQwen3-1.7B \n51.78 \n54.18 \n44.99 \n56.02 \n49.71 \n40.94 \n45.55 \n47.42 \n49.44 \n37.35 \n47.03 \n42.45 \nQwen3-4B \n52.45 \n59.42 \n41.51 \n39.28 \n49.19 \n44.36 \n50.67 \n49.60 \n12.16 \n3.21 \n24.90 \n14.49 \nQwen3-4B-thinking \n50.43 \n62.09 \n50.49 \n46.97 \n51.71 \n49.26 \n54.52 \n54.22 \n26.22 \n3.12 \n27.93 \n11.48 \nQwen3-8B \n47.01 \n61.21 \n41.03 \n38.83 \n50.31 \n46.25 \n45.82 \n47.69 \n35.71 \n4.13 \n52.38 \n17.08 \nQwen3-8B-thinking \n49.39 \n61.98 \n52.63 \n48.27 \n55.71 \n50.38 \n55.69 \n56.11 \n30.36 \n8.54 \n31.06 \n23.33 \nQwen3-14B \n57.69 \n69.03 \n57.19 \n53.39 \n58.11 \n57.53 \n63.17 \n61.44 \n48.45 \n2.27 \n32.11 \n29.39 \nQwen3-14B-thinking \n58.27 \n70.13 \n60.02 \n56.76 \n61.55 \n60.36 \n64.94 \n62.29 \n55.51 \n2.40 \n27.39 \n35.08 \nBaichuan-M2-32B \n63.06 \n57.51 \n50.76 \n60.11 \n58.97 \n59.82 \n55.08 \n55.35 \n37.44 \n30.79 \n40.79 \n42.94 \nBio-Medical-LLaMA-3-8B \n35.15 \n41.21 \n25.87 \n31.82 \n35.37 \n25.75 \n32.09 \n30.31 \n26.09 \n37.62 \n30.72 \n35.12 \nMediPhi \n65.10 \n72.56 \n68.40 \n74.07 \n60.18 \n42.43 \n72.72 \n74.13 \n25.69 \n44.47 \n20.94 \n42.74 \nMedGemma-4B \n31.03 \n36.76 \n39.62 \n35.39 \n32.56 \n36.36 \n35.78 \n36.45 \n32.20 \n16.81 \n28.52 \n28.22 \nMedGemma-27B \n46.54 \n55.33 \n45.26 \n50.58 \n48.49 \n51.62 \n50.85 \n50.61 \n48.43 \n25.71 \n40.88 \n46.76 \nMedReason-8B \n27.48 \n6.52 \n4.47 \n4.09 \n25.10 \n30.54 \n24.07 \n1.62 \n8.16 \n0.47 \n36.94 \n23.69 \nHuatuoGPT-o1-7B \n48.90 \n56.94 \n52.56 \n51.44 \n54.70 \n49.82 \n52.18 \n55.33 \n16.07 \n2.99 \n6.29 \n3.93 \nHuatuoGPT-o1-8B \n33.66 \n34.36 \n36.00 \n31.53 \n43.84 \n35.19 \n38.13 \n36.94 \n27.60 \n2.00 \n21.15 \n7.26 \nHuatuoGPT-o1-70B \n44.81 \n58.54 \n47.87 \n46.07 \n43.89 \n46.63 \n48.22 \n49.26 \n38.00 \n55.60 \n39.42 \n34.13 \nHuatuoGPT-o1-72B \n53.17 \n65.01 \n59.35 \n57.73 \n53.91 \n55.39 \n61.35 \n63.48 \n44.70 \n15.08 \n39.98 \n25.51 \nOpenBioLLM-8B \n9.10 \n11.64 \n13.60 \n17.24 \n8.49 \n8.81 \n17.33 \n18.47 \n10.63 \n14.00 \n12.27 \n11.78 \nOpenBioLLM-70B \n13.46 \n32.67 \n26.56 \n15.39 \n10.94 \n11.17 \n22.36 \n27.28 \n11.15 \n11.46 \n16.72 \n15.75 \nSTab. 90: Zero-Shot performance evaluation of 56 LLMs on BioNLI (Run 3). \n"}, {"page": 83, "text": " \n \n53 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n37.78 \n50.18 \n39.75 \n40.47 \n40.11 \n44.79 \n40.47 \n40.67 \n37.10 \n47.08 \n70.88 \n25.80 \nClaude-4.0-Sonnet \n68.27 \n75.39 \n63.98 \n60.31 \n63.03 \n58.90 \n68.40 \n68.02 \n54.65 \n45.96 \n47.69 \n54.04 \nGemini-2.5-Flash \n62.54 \n68.90 \n58.22 \n59.46 \n58.58 \n62.00 \n61.08 \n60.52 \n54.04 \n55.48 \n63.87 \n54.74 \nGPT-4o-mini \n63.26 \n63.82 \n49.17 \n46.29 \n53.89 \n56.40 \n49.82 \n50.29 \n42.61 \n27.80 \n41.30 \n31.80 \nGPT-4o \n39.30 \n45.10 \n42.16 \n39.55 \n41.53 \n47.39 \n42.97 \n42.72 \n34.65 \n15.42 \n34.02 \n34.45 \nGPT-4.1-nano \n46.31 \n55.89 \n48.74 \n45.24 \n53.28 \n53.44 \n50.13 \n48.90 \n48.54 \n46.22 \n44.13 \n38.81 \nGPT-4.1-mini \n59.42 \n64.54 \n55.89 \n52.54 \n56.40 \n54.61 \n59.28 \n59.71 \n46.99 \n20.90 \n35.69 \n37.60 \nGPT-4.1 \n49.80 \n64.47 \n51.01 \n48.81 \n49.91 \n52.34 \n51.37 \n51.26 \n40.22 \n45.60 \n40.54 \n34.22 \nGPT-5-nano \n46.49 \n40.79 \n33.55 \n45.33 \n51.66 \n42.27 \n49.69 \n51.53 \n58.00 \n32.38 \n59.69 \n54.27 \nGPT-5-mini \n60.83 \n68.40 \n60.40 \n58.04 \n59.89 \n59.15 \n63.24 \n64.13 \n53.37 \n26.29 \n33.30 \n52.67 \nGPT-5 \n65.60 \n73.48 \n57.64 \n56.56 \n63.08 \n67.10 \n60.38 \n63.15 \n51.24 \n48.83 \n45.89 \n45.82 \no4-mini \n68.85 \n73.60 \n67.33 \n67.12 \n68.09 \n67.15 \n69.80 \n70.92 \n61.03 \n31.91 \n49.46 \n55.01 \nOpen-Weight LLMs \nDeepSeek-V3 \n50.94 \n63.82 \n49.82 \n51.82 \n49.19 \n48.90 \n52.40 \n51.10 \n43.82 \n42.58 \n38.02 \n32.94 \nDeepSeek-R1 \n62.85 \n74.09 \n60.29 \n60.38 \n56.45 \n55.64 \n64.22 \n62.63 \n51.66 \n50.07 \n54.70 \n49.44 \nDeepSeek-R1-Qwen3-8B \n59.89 \n64.61 \n51.51 \n48.02 \n52.45 \n48.47 \n52.83 \n54.99 \n44.58 \n11.69 \n18.09 \n13.60 \nGemma-3-4B \n35.03 \n39.35 \n35.71 \n34.49 \n36.07 \n33.96 \n34.58 \n33.75 \n33.98 \n2.27 \n27.10 \n32.76 \nGemma-3-12B \n32.38 \n38.58 \n34.63 \n32.38 \n35.66 \n37.39 \n36.29 \n35.10 \n39.15 \n5.37 \n54.38 \n35.82 \nGemma-3-27B \n36.56 \n49.28 \n36.83 \n39.01 \n39.80 \n43.96 \n38.99 \n39.78 \n35.60 \n12.79 \n31.89 \n37.01 \ngpt-oss-20B \n69.06 \n62.29 \n61.48 \n60.27 \n64.11 \n64.90 \n62.85 \n63.75 \n55.89 \n23.91 \n46.07 \n54.27 \ngpt-oss-120B \n64.13 \n71.37 \n59.39 \n60.16 \n63.71 \n64.72 \n62.85 \n63.66 \n67.30 \n41.51 \n49.55 \n58.29 \nLLaMA-3.1-8B \n51.44 \n38.56 \n35.78 \n35.57 \n69.15 \n45.21 \n44.18 \n40.61 \n39.42 \n50.99 \n38.67 \n12.49 \nLLaMA-3.1-70B \n35.33 \n58.20 \n35.62 \n36.76 \n36.36 \n40.02 \n37.73 \n35.46 \n34.04 \n58.43 \n32.61 \n26.09 \nLLaMA-3.2-3B \n72.13 \n44.70 \n36.00 \n43.96 \n59.75 \n53.26 \n54.04 \n55.84 \n31.44 \n19.82 \n27.71 \n21.39 \nLLaMA-3.3-70B \n46.70 \n58.90 \n45.62 \n41.15 \n42.99 \n44.22 \n46.79 \n45.75 \n39.33 \n69.84 \n48.92 \n29.35 \nLLaMA-4-Scout \n48.25 \n55.87 \n42.72 \n42.61 \n43.30 \n44.81 \n47.44 \n44.00 \n40.54 \n54.38 \n62.70 \n43.57 \nLLaMA-4-Maverick \n50.00 \n61.78 \n46.11 \n45.73 \n50.38 \n53.35 \n48.81 \n49.21 \n39.71 \n63.08 \n41.84 \n37.03 \nMistral-7B-v0.3 \n33.98 \n2.70 \n15.44 \n31.91 \n26.83 \n26.16 \n23.42 \n20.45 \n14.58 \n10.34 \n10.45 \n10.40 \nMistral-Small-3.1-24B \n53.24 \n65.48 \n48.72 \n44.22 \n47.39 \n50.22 \n43.93 \n48.34 \n43.17 \n24.81 \n21.24 \n32.13 \nPhi-4-mini \n52.99 \n54.27 \n51.39 \n48.52 \n54.83 \n45.30 \n43.10 \n49.33 \n38.63 \n30.29 \n30.83 \n40.79 \nPhi-4-mini-Reasoning \n43.87 \n59.33 \n48.00 \n44.52 \n31.48 \n20.25 \n37.80 \n36.88 \n35.42 \n45.44 \n34.83 \n23.64 \nPhi-4 \n52.04 \n59.44 \n52.58 \n51.33 \n53.87 \n51.42 \n54.67 \n55.01 \n44.54 \n35.28 \n44.79 \n35.26 \nPhi-4-Reasoning \n63.33 \n69.01 \n59.93 \n55.82 \n61.71 \n59.48 \n57.37 \n61.64 \n57.35 \n15.30 \n47.82 \n37.71 \nQwen2.5-3B \n58.47 \n62.61 \n49.98 \n46.81 \n59.46 \n58.81 \n55.19 \n53.91 \n20.81 \n1.60 \n12.27 \n10.54 \nQwen2.5-7B \n49.62 \n62.54 \n54.76 \n48.56 \n48.94 \n45.89 \n54.56 \n50.00 \n43.80 \n28.43 \n39.21 \n29.84 \nQwen2.5-14B \n51.64 \n66.54 \n48.97 \n45.37 \n45.62 \n48.92 \n51.64 \n53.39 \n45.57 \n59.12 \n70.81 \n37.21 \nQwen2.5-72B \n47.15 \n64.72 \n55.21 \n52.90 \n49.62 \n53.28 \n59.28 \n59.98 \n38.49 \n71.46 \n54.31 \n29.87 \nQwQ-32B \n52.43 \n59.39 \n49.26 \n46.88 \n48.72 \n47.55 \n50.63 \n50.56 \n46.38 \n39.53 \n43.26 \n44.49 \nQwen3-1.7B \n50.90 \n55.19 \n45.69 \n55.51 \n48.88 \n41.46 \n45.44 \n46.88 \n48.65 \n37.73 \n47.17 \n42.58 \nQwen3-4B \n52.43 \n58.43 \n40.94 \n38.79 \n50.00 \n43.51 \n49.89 \n49.75 \n11.91 \n2.92 \n24.58 \n14.13 \nQwen3-4B-thinking \n49.96 \n62.29 \n50.29 \n47.19 \n51.60 \n49.75 \n54.18 \n54.52 \n26.70 \n3.10 \n28.18 \n11.57 \nQwen3-8B \n46.94 \n61.21 \n40.74 \n38.83 \n51.53 \n45.73 \n44.65 \n48.34 \n34.27 \n3.42 \n53.42 \n16.94 \nQwen3-8B-thinking \n47.87 \n62.38 \n52.20 \n48.16 \n56.43 \n51.01 \n56.09 \n56.36 \n29.64 \n8.22 \n29.91 \n22.70 \nQwen3-14B \n58.63 \n68.72 \n58.20 \n52.11 \n58.38 \n56.72 \n63.03 \n61.57 \n48.65 \n2.43 \n32.56 \n29.35 \nQwen3-14B-thinking \n57.93 \n69.69 \n59.08 \n56.67 \n61.51 \n59.53 \n66.02 \n62.67 \n54.97 \n2.81 \n26.40 \n34.25 \nBaichuan-M2-32B \n62.56 \n57.19 \n50.20 \n59.73 \n58.38 \n59.80 \n53.93 \n55.33 \n36.18 \n31.06 \n40.67 \n43.60 \nBio-Medical-LLaMA-3-8B \n34.76 \n40.83 \n26.02 \n32.07 \n35.33 \n25.91 \n31.87 \n29.69 \n26.09 \n35.93 \n30.94 \n35.19 \nMediPhi \n63.19 \n72.76 \n68.52 \n73.98 \n59.44 \n41.93 \n72.40 \n73.80 \n26.92 \n45.96 \n20.88 \n41.78 \nMedGemma-4B \n30.25 \n36.56 \n39.96 \n35.26 \n32.38 \n36.43 \n35.42 \n36.40 \n32.72 \n16.38 \n29.44 \n28.16 \nMedGemma-27B \n46.31 \n54.74 \n45.57 \n51.42 \n48.72 \n51.78 \n51.60 \n51.10 \n49.03 \n24.43 \n41.98 \n44.97 \nMedReason-8B \n27.78 \n6.20 \n4.00 \n4.43 \n25.46 \n30.99 \n24.00 \n1.89 \n8.81 \n0.49 \n37.21 \n24.00 \nHuatuoGPT-o1-7B \n49.06 \n58.81 \n52.13 \n50.81 \n54.83 \n50.02 \n52.49 \n54.94 \n16.79 \n2.67 \n6.85 \n4.07 \nHuatuoGPT-o1-8B \n34.00 \n34.22 \n35.89 \n31.73 \n44.40 \n36.07 \n38.00 \n36.85 \n27.06 \n1.75 \n21.87 \n7.80 \nHuatuoGPT-o1-70B \n44.72 \n58.85 \n47.89 \n46.76 \n43.91 \n46.83 \n47.64 \n49.39 \n37.73 \n56.25 \n38.97 \n34.27 \nHuatuoGPT-o1-72B \n52.36 \n64.72 \n58.58 \n57.30 \n53.57 \n55.57 \n61.60 \n63.60 \n43.69 \n15.01 \n40.88 \n25.46 \nOpenBioLLM-8B \n9.33 \n11.08 \n13.80 \n17.62 \n8.90 \n9.01 \n16.97 \n18.38 \n10.22 \n14.20 \n11.66 \n12.76 \nOpenBioLLM-70B \n12.67 \n32.38 \n27.01 \n15.37 \n10.43 \n10.54 \n22.00 \n27.35 \n11.66 \n11.60 \n16.40 \n15.19 \nSTab. 91: Zero-Shot performance evaluation of 56 LLMs on BioNLI (Run 4). \n"}, {"page": 84, "text": " \n \n54 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n37.75 \n50.18 \n39.78 \n40.47 \n40.13 \n44.79 \n40.47 \n40.67 \n37.10 \n47.12 \n70.92 \n25.73 \nClaude-4.0-Sonnet \n68.38 \n75.21 \n63.71 \n59.57 \n63.10 \n59.17 \n68.47 \n67.80 \n54.81 \n44.94 \n47.17 \n54.02 \nGemini-2.5-Flash \n62.20 \n69.19 \n57.89 \n59.44 \n58.54 \n62.00 \n60.94 \n60.81 \n53.30 \n55.98 \n64.16 \n54.76 \nGPT-4o-mini \n63.84 \n63.82 \n49.15 \n46.38 \n53.60 \n57.03 \n50.07 \n50.09 \n42.04 \n27.93 \n40.63 \n31.42 \nGPT-4o \n38.65 \n45.48 \n42.07 \n39.91 \n40.76 \n46.81 \n42.79 \n43.24 \n34.04 \n16.27 \n33.42 \n33.84 \nGPT-4.1-nano \n46.47 \n55.96 \n48.81 \n45.24 \n53.51 \n52.76 \n50.88 \n48.94 \n47.71 \n44.81 \n44.58 \n40.74 \nGPT-4.1-mini \n59.03 \n65.30 \n56.25 \n52.47 \n55.98 \n54.88 \n58.72 \n60.27 \n46.85 \n21.55 \n36.36 \n37.48 \nGPT-4.1 \n49.62 \n64.70 \n51.24 \n48.94 \n49.78 \n51.80 \n51.35 \n51.44 \n40.49 \n44.94 \n41.35 \n34.29 \nGPT-5-nano \n45.55 \n42.16 \n33.57 \n45.69 \n52.04 \n42.31 \n49.93 \n51.06 \n57.89 \n30.88 \n58.36 \n54.63 \nGPT-5-mini \n61.19 \n68.76 \n60.11 \n58.22 \n58.97 \n58.63 \n63.62 \n63.69 \n53.57 \n26.63 \n33.57 \n52.47 \nGPT-5 \n65.71 \n73.64 \n58.25 \n56.61 \n63.35 \n67.51 \n60.40 \n63.19 \n51.71 \n47.87 \n45.17 \n45.62 \no4-mini \n68.49 \n73.19 \n66.92 \n66.61 \n68.65 \n67.21 \n70.56 \n71.17 \n61.48 \n33.03 \n49.30 \n55.26 \nOpen-Weight LLMs \nDeepSeek-V3 \n51.37 \n63.60 \n49.55 \n51.35 \n49.30 \n48.65 \n51.98 \n50.88 \n43.37 \n41.37 \n37.30 \n32.90 \nDeepSeek-R1 \n62.58 \n74.22 \n60.16 \n60.58 \n57.12 \n54.92 \n64.31 \n62.74 \n51.15 \n50.31 \n54.61 \n48.72 \nDeepSeek-R1-Qwen3-8B \n58.63 \n64.40 \n51.55 \n48.58 \n52.54 \n48.20 \n52.94 \n54.94 \n44.70 \n11.46 \n18.11 \n14.49 \nGemma-3-4B \n35.03 \n39.69 \n34.88 \n34.47 \n36.02 \n33.42 \n34.18 \n34.18 \n34.52 \n2.61 \n26.83 \n33.15 \nGemma-3-12B \n32.20 \n38.43 \n34.56 \n32.31 \n36.47 \n37.15 \n35.93 \n35.39 \n39.46 \n4.92 \n55.10 \n35.82 \nGemma-3-27B \n37.06 \n49.12 \n36.61 \n39.10 \n39.30 \n43.75 \n39.21 \n39.46 \n35.08 \n12.29 \n31.28 \n36.90 \ngpt-oss-20B \n68.72 \n63.30 \n61.60 \n60.47 \n64.90 \n64.49 \n62.83 \n63.82 \n54.67 \n25.15 \n46.25 \n53.37 \ngpt-oss-120B \n64.34 \n71.17 \n59.78 \n60.58 \n63.33 \n64.18 \n63.30 \n63.24 \n66.72 \n39.33 \n49.98 \n58.20 \nLLaMA-3.1-8B \n52.36 \n38.38 \n36.07 \n35.46 \n68.99 \n44.22 \n44.02 \n40.63 \n39.93 \n50.72 \n38.70 \n13.44 \nLLaMA-3.1-70B \n35.01 \n58.61 \n36.16 \n36.04 \n36.43 \n41.12 \n37.87 \n35.53 \n34.18 \n59.15 \n32.63 \n26.02 \nLLaMA-3.2-3B \n72.58 \n44.88 \n36.04 \n42.79 \n61.46 \n54.00 \n54.13 \n55.69 \n30.61 \n19.37 \n27.80 \n20.67 \nLLaMA-3.3-70B \n46.88 \n58.25 \n45.12 \n41.26 \n41.62 \n44.25 \n46.49 \n46.34 \n39.28 \n68.74 \n48.85 \n29.28 \nLLaMA-4-Scout \n48.49 \n56.25 \n42.61 \n42.63 \n43.55 \n45.24 \n47.62 \n44.54 \n40.56 \n54.49 \n62.94 \n44.07 \nLLaMA-4-Maverick \n50.25 \n61.48 \n46.25 \n45.78 \n49.89 \n53.24 \n49.46 \n48.92 \n39.71 \n63.26 \n42.36 \n37.28 \nMistral-7B-v0.3 \n33.15 \n3.19 \n15.24 \n33.21 \n26.83 \n27.53 \n21.84 \n19.62 \n13.69 \n9.60 \n9.91 \n11.01 \nMistral-Small-3.1-24B \n53.51 \n64.79 \n48.25 \n44.07 \n47.26 \n48.81 \n43.84 \n47.26 \n43.17 \n23.12 \n21.24 \n32.65 \nPhi-4-mini \n53.84 \n54.47 \n52.02 \n48.67 \n54.85 \n43.24 \n43.64 \n48.97 \n38.16 \n33.44 \n32.34 \n41.42 \nPhi-4-mini-Reasoning \n43.33 \n59.17 \n48.74 \n44.67 \n32.43 \n19.93 \n37.60 \n36.47 \n36.25 \n45.30 \n34.65 \n24.65 \nPhi-4 \n52.29 \n59.62 \n52.76 \n52.02 \n53.87 \n51.75 \n54.25 \n54.72 \n44.38 \n35.42 \n44.04 \n36.31 \nPhi-4-Reasoning \n63.10 \n69.28 \n59.17 \n55.75 \n62.07 \n59.26 \n58.22 \n62.34 \n58.29 \n14.90 \n48.13 \n38.02 \nQwen2.5-3B \n58.65 \n63.60 \n49.26 \n46.20 \n58.07 \n57.37 \n55.39 \n54.56 \n20.63 \n1.80 \n12.74 \n10.29 \nQwen2.5-7B \n50.16 \n61.55 \n53.28 \n47.96 \n49.66 \n44.22 \n54.43 \n49.69 \n46.36 \n29.15 \n38.79 \n28.85 \nQwen2.5-14B \n52.54 \n67.39 \n48.81 \n45.26 \n46.07 \n48.74 \n51.69 \n53.33 \n46.20 \n59.48 \n70.00 \n38.25 \nQwen2.5-72B \n48.22 \n65.21 \n55.39 \n52.54 \n50.09 \n53.82 \n58.47 \n60.13 \n38.76 \n71.17 \n56.07 \n29.21 \nQwQ-32B \n51.71 \n59.37 \n49.44 \n47.37 \n48.45 \n47.51 \n50.88 \n50.76 \n47.37 \n40.13 \n42.54 \n44.63 \nQwen3-1.7B \n51.03 \n55.28 \n45.55 \n55.98 \n50.02 \n41.96 \n45.03 \n47.30 \n49.87 \n36.72 \n47.51 \n44.18 \nQwen3-4B \n51.93 \n59.24 \n40.70 \n38.47 \n50.25 \n43.71 \n49.93 \n49.80 \n11.60 \n3.42 \n25.24 \n14.29 \nQwen3-4B-thinking \n49.57 \n62.00 \n50.61 \n47.03 \n51.64 \n49.64 \n54.02 \n54.38 \n26.22 \n3.30 \n27.33 \n11.66 \nQwen3-8B \n47.24 \n61.69 \n40.97 \n38.72 \n51.17 \n45.21 \n45.33 \n47.98 \n35.10 \n3.93 \n52.18 \n17.24 \nQwen3-8B-thinking \n49.03 \n62.47 \n52.34 \n48.29 \n55.51 \n49.69 \n56.07 \n56.65 \n30.16 \n8.25 \n28.99 \n22.85 \nQwen3-14B \n57.96 \n68.47 \n57.60 \n52.58 \n58.09 \n56.97 \n63.53 \n61.73 \n49.01 \n2.47 \n33.42 \n28.43 \nQwen3-14B-thinking \n57.55 \n69.98 \n60.13 \n56.76 \n62.22 \n60.83 \n64.99 \n63.12 \n55.91 \n2.76 \n28.49 \n33.98 \nBaichuan-M2-32B \n63.03 \n58.20 \n50.90 \n59.48 \n59.37 \n59.35 \n55.24 \n56.27 \n36.18 \n30.52 \n40.47 \n44.02 \nBio-Medical-LLaMA-3-8B \n34.43 \n41.26 \n25.87 \n31.87 \n34.38 \n25.75 \n32.47 \n29.80 \n26.09 \n36.29 \n30.97 \n35.37 \nMediPhi \n64.45 \n72.63 \n68.52 \n73.96 \n60.47 \n42.27 \n72.74 \n73.53 \n25.33 \n45.06 \n22.27 \n43.10 \nMedGemma-4B \n30.88 \n36.65 \n40.29 \n35.60 \n32.74 \n35.98 \n35.51 \n35.98 \n32.18 \n16.81 \n28.40 \n28.04 \nMedGemma-27B \n46.36 \n54.79 \n45.24 \n51.03 \n48.11 \n52.47 \n51.10 \n50.94 \n48.92 \n24.36 \n42.47 \n46.29 \nMedReason-8B \n27.60 \n6.43 \n4.49 \n3.91 \n26.27 \n30.90 \n24.22 \n1.46 \n8.11 \n0.56 \n36.92 \n23.91 \nHuatuoGPT-o1-7B \n49.24 \n57.08 \n51.55 \n51.55 \n54.83 \n50.27 \n52.43 \n55.60 \n17.21 \n2.74 \n6.43 \n3.71 \nHuatuoGPT-o1-8B \n33.93 \n34.43 \n36.58 \n30.67 \n43.44 \n35.46 \n37.39 \n37.06 \n27.57 \n1.71 \n22.18 \n7.39 \nHuatuoGPT-o1-70B \n44.49 \n58.74 \n47.55 \n46.22 \n42.81 \n47.17 \n47.96 \n49.44 \n37.75 \n54.58 \n39.03 \n33.44 \nHuatuoGPT-o1-72B \n53.30 \n64.83 \n58.94 \n57.48 \n53.91 \n56.43 \n62.07 \n62.83 \n44.83 \n15.51 \n39.96 \n25.26 \nOpenBioLLM-8B \n9.46 \n12.45 \n14.61 \n17.96 \n8.40 \n8.54 \n16.54 \n19.53 \n9.89 \n14.67 \n10.49 \n12.90 \nOpenBioLLM-70B \n12.40 \n32.65 \n27.06 \n15.53 \n9.69 \n10.13 \n22.92 \n27.60 \n11.39 \n12.16 \n16.00 \n16.13 \nSTab. 92: Zero-Shot performance evaluation of 56 LLMs on BioNLI (Run 5). \n"}, {"page": 85, "text": " \n \n55 \n \nLLMs \nChinese \nEnglish \nFrench \nGerman Japanese \nKorean Portuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nProprietary LLMs \nClaude-3.5-Haiku \n69.28±0.48 78.14±0.74 67.47±0.25 66.41±0.48 67.89±0.48 69.54±0.50 \n66.73±0.41 \n68.78±0.24 51.38±0.69 \n9.98±0.60 \n34.05±0.99 32.65±1.10 56.86±19.90 \nClaude-4.0-Sonnet \n81.40±0.57 85.66±0.43 79.32±0.42 75.75±0.34 78.53±0.42 80.10±0.36 \n76.13±0.46 \n78.30±0.26 68.35±0.18 34.52±0.57 57.84±0.45 56.88±0.34 71.07±14.10 \nGemini-2.5-Flash \n71.83±0.84 83.48±0.91 70.71±0.45 69.60±0.52 67.78±0.26 69.80±0.58 \n69.26±0.41 \n70.12±0.36 67.14±0.44 46.91±0.76 59.20±0.51 62.22±0.63 \n67.34±8.38 \nGPT-4o-mini \n79.77±1.03 84.84±0.54 81.31±0.94 79.19±0.80 78.11±0.74 77.05±0.87 \n80.30±1.23 \n79.65±0.42 62.03±0.61 29.12±1.81 34.66±0.50 50.77±0.81 68.07±18.78 \nGPT-4o \n78.18±0.63 81.75±1.12 75.44±0.96 75.47±0.26 76.12±1.13 76.87±0.38 \n78.62±0.74 \n75.15±0.24 71.29±0.34 27.27±1.42 46.88±0.84 61.18±0.78 68.68±15.61 \nGPT-4.1-nano \n71.52±0.92 83.50±0.42 75.13±0.96 75.09±0.46 70.97±1.15 69.63±0.44 \n72.42±0.80 \n75.81±0.88 56.56±1.18 24.53±1.16 33.28±0.44 47.57±1.04 63.00±17.94 \nGPT-4.1-mini \n82.74±0.46 87.72±0.43 85.39±0.42 82.86±0.42 79.27±0.54 80.93±0.39 \n82.65±0.26 \n82.75±0.29 63.47±0.61 19.90±1.20 47.30±0.80 59.69±0.53 71.22±19.67 \nGPT-4.1 \n83.12±0.39 87.75±0.23 85.33±0.33 84.88±0.96 82.24±0.35 82.41±0.23 \n85.12±0.26 \n86.43±0.44 71.60±0.37 38.81±0.74 50.02±0.98 63.42±0.39 75.10±15.56 \nGPT-5-nano \n73.25±0.75 77.88±1.02 70.02±0.54 78.97±0.89 72.28±1.02 69.65±1.18 \n78.62±0.24 \n80.93±0.99 54.82±1.30 16.75±0.54 32.38±0.80 41.40±0.65 62.25±20.46 \nGPT-5-mini \n83.77±0.42 88.56±0.17 87.71±0.35 86.48±0.57 83.36±0.34 82.86±0.70 \n86.79±0.63 \n87.23±0.36 72.55±0.65 32.19±0.86 46.96±0.45 63.09±0.65 75.13±17.81 \nGPT-5 \n81.00±0.54 86.35±0.11 85.89±0.18 85.76±0.35 82.12±0.62 83.36±0.75 \n84.74±0.31 \n85.63±0.24 80.39±0.50 43.22±0.81 54.05±0.70 65.65±0.41 76.51±13.87 \no4-mini \n84.92±0.54 88.76±0.45 87.69±0.39 87.45±0.35 85.05±0.23 84.40±0.57 \n87.26±0.25 \n87.79±0.42 79.10±0.68 21.67±1.34 51.86±1.05 67.75±0.28 76.14±19.63 \nOpen-Weight LLMs \nDeepSeek-V3 \n78.42±0.52 84.35±0.39 77.99±0.47 79.15±0.72 75.03±0.32 75.05±0.17 \n76.50±0.56 \n78.53±0.35 68.10±0.89 34.21±0.98 45.91±0.38 51.76±1.25 68.75±15.33 \nDeepSeek-R1 \n77.93±0.34 83.37±0.34 74.91±0.56 76.75±0.44 76.10±0.45 73.03±0.76 \n74.65±0.29 \n77.52±0.50 59.46±0.43 34.96±0.36 45.97±0.46 54.67±0.76 67.44±14.59 \nDeepSeek-R1-Qwen3-8B \n77.73±0.77 80.58±0.48 75.88±1.38 76.00±0.74 75.51±0.49 71.28±0.94 \n76.36±0.78 \n76.50±0.73 31.53±1.16 12.21±0.17 12.41±0.71 15.53±1.54 56.79±28.19 \nGemma-3-4B \n64.84±0.99 73.72±0.70 61.31±0.59 58.89±0.79 58.08±0.70 63.25±0.95 \n62.67±0.64 \n62.81±0.85 45.93±1.03 \n7.56±0.67 \n29.63±0.34 21.35±0.80 50.84±19.76 \nGemma-3-12B \n63.54±0.80 82.12±0.63 66.92±0.78 67.02±0.72 67.97±0.59 69.49±0.52 \n64.67±0.31 \n66.41±0.44 65.55±0.84 \n7.31±0.80 \n34.95±1.60 49.30±0.97 58.77±19.21 \nGemma-3-27B \n77.50±0.91 82.75±0.61 70.16±0.57 73.69±0.23 73.55±0.77 78.00±0.50 \n72.50±0.46 \n73.32±0.28 71.93±1.09 24.54±1.28 38.73±1.27 55.62±0.39 66.03±16.97 \ngpt-oss-20B \n81.82±0.52 76.82±1.09 83.56±0.61 84.19±0.89 80.89±0.88 79.97±0.39 \n83.44±0.68 \n82.98±0.61 53.11±0.73 15.86±1.26 34.36±0.87 57.05±1.63 67.84±22.06 \ngpt-oss-120B \n82.23±0.68 87.21±0.26 84.71±0.60 86.07±0.32 82.29±0.18 82.71±0.18 \n85.29±0.29 \n85.70±0.40 72.53±1.18 25.42±0.76 48.85±0.95 65.45±0.39 74.04±18.37 \nLLaMA-3.1-8B \n50.57±0.54 66.56±0.67 61.71±0.66 60.61±0.74 52.70±1.37 57.93±1.55 \n62.37±0.43 \n63.23±0.81 38.53±1.15 23.84±0.43 22.57±1.18 18.14±0.78 48.23±17.16 \nLLaMA-3.1-70B \n66.27±0.62 77.45±0.74 67.85±0.43 67.61±0.50 65.86±0.60 67.23±0.77 \n68.10±0.52 \n67.88±0.38 61.96±0.72 32.90±1.11 34.73±1.39 36.19±0.95 59.50±14.91 \nLLaMA-3.2-3B \n30.59±0.98 63.46±1.11 49.88±1.15 40.78±0.30 36.44±0.74 31.01±0.40 \n50.42±1.49 \n53.90±0.69 33.29±1.26 20.80±0.76 22.85±0.93 24.62±0.51 38.17±13.16 \nLLaMA-3.3-70B \n61.49±0.61 82.16±0.35 67.65±0.44 67.31±0.45 66.58±0.37 69.79±0.52 \n67.23±0.27 \n67.78±0.54 64.08±0.56 33.47±0.76 37.08±0.86 39.49±0.64 60.34±14.63 \nLLaMA-4-Scout \n76.71±0.36 81.45±0.79 71.25±0.28 72.28±0.24 68.40±0.40 70.58±0.69 \n72.71±0.64 \n73.14±0.59 68.75±0.43 34.59±0.46 42.49±0.57 56.80±0.50 65.76±13.58 \nLLaMA-4-Maverick \n74.03±0.31 85.01±0.50 73.75±0.65 70.56±0.48 71.07±0.37 78.16±0.58 \n72.79±0.56 \n75.44±0.18 65.80±0.34 36.39±0.58 48.48±0.42 62.78±0.54 67.85±12.92 \nMistral-7B-v0.3 \n37.22±0.50 13.69±0.71 19.82±0.79 41.54±1.38 28.31±0.91 24.83±0.35 \n34.68±1.22 \n41.27±0.74 16.09±0.52 12.76±0.56 11.32±0.18 11.76±0.71 24.44±11.43 \nMistral-Small-3.1-24B \n75.69±0.84 83.56±0.75 73.73±0.90 68.58±0.76 63.95±0.79 64.12±0.71 \n73.38±0.66 \n74.17±0.86 30.15±0.87 12.15±1.33 11.46±0.86 19.92±0.51 54.24±26.41 \nPhi-4-mini \n64.33±0.67 77.51±0.47 68.52±0.78 66.94±0.78 61.85±0.69 45.12±1.09 \n65.94±0.86 \n68.17±0.94 32.56±1.40 18.47±0.57 21.54±0.61 23.67±1.08 51.22±20.85 \nPhi-4-mini-Reasoning \n49.33±0.43 75.65±0.74 58.34±0.79 60.86±1.35 21.36±0.92 17.60±1.33 \n40.69±1.13 \n39.79±1.06 30.43±1.14 24.38±1.09 25.00±0.95 26.59±0.62 39.17±17.76 \nPhi-4 \n75.92±0.62 83.88±0.24 77.91±0.93 79.39±0.72 77.80±0.78 74.16±0.67 \n76.64±0.71 \n78.47±0.67 47.84±0.92 23.90±1.01 31.68±1.18 30.09±0.53 63.14±21.96 \nPhi-4-Reasoning \n79.82±0.87 85.53±0.33 81.30±0.38 82.06±0.70 79.59±0.74 78.39±0.40 \n76.99±0.70 \n82.38±0.55 57.47±0.70 11.31±0.58 32.99±1.26 23.34±0.97 64.26±25.60 \nQwen2.5-3B \n75.10±0.44 79.17±0.43 69.74±0.48 65.89±0.71 59.68±0.43 61.72±0.95 \n69.79±0.69 \n72.15±1.17 12.83±0.66 14.55±0.56 12.21±0.88 13.85±0.75 50.56±27.01 \nQwen2.5-7B \n79.41±0.47 82.99±0.44 79.06±0.74 77.25±0.45 67.76±0.56 71.57±0.83 \n78.36±0.41 \n77.23±0.65 27.11±1.23 \n9.27±0.56 \n13.83±0.80 10.55±0.95 56.20±29.77 \nQwen2.5-14B \n80.06±0.55 82.03±0.42 80.88±0.84 79.15±0.87 75.21±0.62 76.81±0.46 \n81.14±0.30 \n81.00±0.82 33.38±1.60 13.06±0.40 27.34±0.92 14.27±0.73 60.36±27.87 \nQwen2.5-72B \n80.45±0.48 85.31±0.55 83.80±0.41 83.60±0.29 77.57±0.42 78.61±0.70 \n82.78±0.69 \n82.73±0.44 49.33±0.60 34.06±0.27 33.65±0.44 32.43±0.78 67.03±21.64 \nQwQ-32B \n77.71±0.31 83.81±0.27 76.68±0.58 77.61±0.61 75.47±0.37 78.12±0.63 \n77.08±0.52 \n77.22±0.46 34.14±1.00 28.13±1.93 31.02±0.94 31.81±0.64 62.40±22.32 \nQwen3-1.7B \n68.44±0.96 75.29±0.66 66.58±0.43 62.15±0.82 54.20±0.75 49.33±0.60 \n65.14±0.63 \n63.79±0.91 27.38±1.61 23.59±0.96 30.61±1.47 32.43±1.20 51.58±17.74 \nQwen3-4B \n76.78±0.51 82.33±0.69 68.57±0.78 57.73±1.32 72.90±1.18 71.87±0.50 \n75.40±0.68 \n73.37±0.71 17.78±1.22 18.53±1.25 11.01±0.71 14.99±0.92 53.44±27.62 \nQwen3-4B-thinking \n78.77±0.65 84.53±0.29 78.05±0.77 76.06±0.42 78.52±0.62 76.79±0.63 \n76.67±0.57 \n76.84±0.59 23.82±0.43 11.90±0.58 11.97±0.78 11.49±0.82 57.12±30.41 \nQwen3-8B \n75.98±1.16 77.49±0.50 72.01±0.49 69.49±0.81 77.15±0.63 72.55±0.31 \n70.97±0.36 \n70.90±1.01 27.30±0.78 13.06±1.01 24.80±0.88 20.24±0.48 55.99±25.03 \nQwen3-8B-thinking \n77.85±0.41 82.29±0.48 80.20±0.62 79.73±0.51 78.29±0.40 75.65±0.76 \n80.20±0.86 \n80.42±0.71 31.52±1.52 \n8.33±0.59 \n19.17±1.67 15.24±0.98 59.07±29.35 \nQwen3-14B \n80.72±0.42 82.72±0.56 81.33±0.50 77.88±0.78 79.06±0.51 78.19±0.49 \n81.57±0.74 \n82.53±0.30 44.42±0.88 21.27±0.96 22.81±0.97 32.14±1.52 63.72±24.58 \nQwen3-14B-thinking \n80.49±0.71 84.69±0.41 80.59±0.53 79.25±0.61 79.52±0.95 78.62±0.58 \n81.86±0.75 \n80.99±0.75 53.27±1.04 \n8.55±0.42 \n17.37±0.58 28.89±0.63 62.84±27.39 \nBaichuan-M2-32B \n76.61±0.24 80.88±0.29 78.55±0.38 77.59±0.80 74.61±0.69 73.32±0.44 \n78.19±0.36 \n78.05±0.32 30.67±1.50 19.55±0.31 30.53±1.52 28.69±2.05 60.60±23.94 \nBio-Medical-LLaMA-3-8B \n40.38±0.55 57.53±0.97 44.53±0.99 39.49±0.45 38.57±0.31 33.69±0.77 \n38.51±0.64 \n40.30±0.71 32.67±0.69 33.51±0.83 33.07±1.11 33.17±0.93 \n38.78±6.83 \nMediPhi \n29.33±1.28 74.04±0.94 61.41±0.89 56.37±0.62 36.73±0.24 35.47±0.63 \n56.75±0.60 \n58.73±0.30 18.19±0.61 27.75±0.55 11.98±0.87 21.20±1.93 40.66±19.37 \nMedGemma-4B \n58.70±0.53 72.20±0.68 60.00±0.84 61.92±0.79 59.16±0.46 58.16±0.72 \n59.66±0.63 \n63.66±0.78 44.26±1.44 15.23±0.58 31.80±0.95 31.39±1.30 51.35±16.33 \nMedGemma-27B \n77.19±0.34 81.58±0.74 74.30±0.51 74.86±0.36 75.03±0.53 77.84±0.71 \n76.17±0.54 \n77.26±0.51 67.96±0.44 26.27±0.53 36.51±1.20 54.88±0.87 66.66±17.33 \nMedReason-8B \n41.61±0.92 11.25±0.48 \n8.75±0.32 \n9.56±0.50 \n40.85±1.40 45.33±0.76 \n23.77±0.29 \n3.66±0.29 \n8.44±0.99 \n11.32±0.51 24.25±0.71 24.53±0.64 21.11±14.19 \nHuatuoGPT-o1-7B \n73.71±1.13 79.32±1.05 71.21±0.33 72.52±0.53 57.18±0.56 65.87±0.85 \n72.67±0.39 \n73.97±0.56 \n5.66±0.61 \n5.58±0.50 \n7.96±0.88 \n4.98±0.37 \n49.22±31.22 \nHuatuoGPT-o1-8B \n53.14±1.03 60.78±0.82 58.17±0.48 59.45±0.66 45.80±1.43 58.17±0.88 \n59.05±1.09 \n59.03±0.67 32.63±1.56 \n5.91±0.53 \n11.04±1.24 \n9.95±0.85 \n42.76±21.15 \nHuatuoGPT-o1-70B \n67.28±0.34 79.24±0.75 66.79±0.36 66.27±0.16 67.58±0.65 66.96±1.00 \n66.76±0.40 \n67.44±0.80 62.03±0.86 31.38±0.88 34.00±1.21 39.58±0.83 59.61±14.94 \nHuatuoGPT-o1-72B \n77.98±0.63 84.57±0.34 80.87±0.41 78.70±0.60 74.19±0.46 77.09±0.55 \n79.44±0.53 \n80.14±0.31 47.41±0.31 19.27±0.93 28.76±1.04 19.68±0.65 62.34±24.95 \nOpenBioLLM-8B \n5.59±0.93 \n20.93±1.12 11.95±0.76 17.06±0.74 \n5.12±0.56 \n6.31±1.13 \n20.14±1.27 \n21.20±0.63 12.29±0.64 \n9.67±0.79 \n10.22±0.52 15.62±0.76 \n13.01±5.78 \nOpenBioLLM-70B \n28.23±0.80 72.63±0.61 58.03±0.82 30.44±0.83 29.19±1.36 31.16±1.31 \n52.11±1.59 \n62.29±0.57 23.02±1.23 12.01±1.10 14.55±0.52 19.19±0.73 36.07±19.39 \nSTab. 93: Performance evaluation of 56 LLMs on MedNLI. \n"}, {"page": 86, "text": " \n \n56 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n69.58 \n78.33 \n67.68 \n66.69 \n68.24 \n68.67 \n67.11 \n68.88 \n52.01 \n9.74 \n35.22 \n33.31 \nClaude-4.0-Sonnet \n80.45 \n86.24 \n79.39 \n75.51 \n78.69 \n80.03 \n75.44 \n78.76 \n68.45 \n35.07 \n58.01 \n56.81 \nGemini-2.5-Flash \n71.77 \n83.27 \n70.15 \n69.37 \n68.17 \n69.44 \n69.72 \n70.15 \n67.82 \n47.21 \n59.28 \n62.24 \nGPT-4o-mini \n80.31 \n85.11 \n82.07 \n78.76 \n77.42 \n77.56 \n81.79 \n79.96 \n62.81 \n31.05 \n35.14 \n51.31 \nGPT-4o \n79.04 \n80.80 \n76.36 \n75.72 \n74.81 \n77.06 \n78.12 \n75.30 \n70.92 \n26.53 \n46.37 \n60.76 \nGPT-4.1-nano \n70.85 \n82.99 \n75.94 \n74.66 \n71.28 \n69.02 \n72.12 \n74.31 \n56.60 \n24.84 \n33.73 \n46.65 \nGPT-4.1-mini \n82.43 \n87.44 \n85.11 \n83.27 \n78.83 \n80.66 \n82.43 \n83.06 \n63.09 \n19.76 \n46.44 \n60.27 \nGPT-4.1 \n82.71 \n87.58 \n85.11 \n83.84 \n81.93 \n82.07 \n85.18 \n85.67 \n71.28 \n37.69 \n50.53 \n62.81 \nGPT-5-nano \n73.25 \n79.18 \n70.29 \n78.19 \n72.12 \n69.51 \n78.97 \n81.51 \n52.51 \n16.94 \n31.55 \n41.07 \nGPT-5-mini \n84.26 \n88.64 \n88.07 \n87.01 \n83.42 \n82.71 \n86.66 \n87.30 \n71.63 \n31.40 \n46.65 \n63.16 \nGPT-5 \n80.24 \n86.31 \n85.74 \n85.39 \n81.37 \n84.47 \n84.90 \n85.74 \n79.60 \n44.32 \n54.27 \n65.28 \no4-mini \n84.40 \n89.20 \n87.58 \n88.07 \n84.90 \n83.84 \n87.09 \n87.86 \n78.55 \n23.08 \n53.00 \n67.89 \nOpen-Weight LLMs \nDeepSeek-V3 \n78.69 \n83.77 \n78.33 \n78.76 \n75.09 \n74.81 \n77.35 \n78.55 \n68.53 \n33.38 \n46.08 \n51.02 \nDeepSeek-R1 \n78.41 \n82.92 \n75.23 \n77.28 \n76.29 \n72.76 \n74.38 \n77.49 \n59.56 \n35.29 \n45.66 \n55.26 \nDeepSeek-R1-Qwen3-8B \n76.50 \n80.31 \n75.16 \n75.58 \n75.37 \n71.91 \n76.22 \n76.50 \n30.63 \n12.00 \n13.20 \n16.65 \nGemma-3-4B \n63.87 \n74.38 \n61.12 \n59.84 \n58.50 \n62.60 \n63.30 \n63.37 \n44.46 \n7.69 \n29.22 \n21.03 \nGemma-3-12B \n63.02 \n81.23 \n67.40 \n66.27 \n68.38 \n69.80 \n64.57 \n67.04 \n64.71 \n8.19 \n35.57 \n49.68 \nGemma-3-27B \n76.50 \n82.64 \n70.43 \n73.47 \n74.24 \n77.28 \n72.69 \n73.18 \n71.77 \n24.28 \n37.40 \n55.96 \ngpt-oss-20B \n81.58 \n77.77 \n82.64 \n83.27 \n81.02 \n79.53 \n83.35 \n83.63 \n53.28 \n13.97 \n33.17 \n57.80 \ngpt-oss-120B \n81.72 \n86.87 \n85.60 \n85.96 \n82.29 \n82.64 \n85.04 \n85.67 \n72.62 \n24.49 \n47.71 \n66.06 \nLLaMA-3.1-8B \n50.81 \n65.63 \n61.47 \n60.62 \n53.49 \n55.82 \n61.75 \n62.67 \n39.03 \n23.71 \n21.81 \n18.98 \nLLaMA-3.1-70B \n65.91 \n77.35 \n67.75 \n67.04 \n66.41 \n66.41 \n67.75 \n67.47 \n61.68 \n33.45 \n34.51 \n35.85 \nLLaMA-3.2-3B \n31.05 \n62.17 \n49.40 \n41.14 \n35.57 \n31.40 \n51.66 \n54.55 \n35.00 \n20.89 \n21.95 \n24.14 \nLLaMA-3.3-70B \n62.24 \n82.64 \n68.31 \n67.04 \n65.98 \n70.57 \n66.90 \n68.24 \n64.50 \n34.16 \n36.56 \n39.66 \nLLaMA-4-Scout \n76.71 \n80.80 \n71.28 \n72.19 \n68.24 \n70.78 \n71.98 \n73.75 \n68.31 \n33.94 \n42.70 \n56.18 \nLLaMA-4-Maverick \n74.31 \n84.76 \n74.45 \n70.15 \n71.00 \n77.91 \n72.76 \n75.44 \n65.63 \n36.63 \n48.84 \n62.67 \nMistral-7B-v0.3 \n37.12 \n14.61 \n19.12 \n43.19 \n27.95 \n24.56 \n36.63 \n41.92 \n16.09 \n12.42 \n11.08 \n12.49 \nMistral-Small-3.1-24B \n76.22 \n84.40 \n73.82 \n69.72 \n63.80 \n65.21 \n72.41 \n74.17 \n30.63 \n12.14 \n12.63 \n19.48 \nPhi-4-mini \n64.57 \n77.42 \n67.96 \n67.04 \n61.68 \n46.65 \n65.28 \n69.02 \n34.09 \n18.91 \n21.17 \n22.37 \nPhi-4-mini-Reasoning \n48.84 \n75.51 \n58.72 \n61.12 \n22.72 \n18.98 \n41.99 \n40.86 \n32.39 \n24.56 \n26.39 \n27.17 \nPhi-4 \n75.86 \n83.98 \n78.33 \n79.75 \n78.55 \n74.66 \n77.13 \n78.26 \n46.51 \n23.01 \n31.26 \n29.43 \nPhi-4-Reasoning \n79.32 \n85.32 \n81.65 \n82.50 \n80.66 \n78.33 \n77.13 \n82.99 \n56.81 \n10.87 \n34.93 \n24.06 \nQwen2.5-3B \n75.79 \n79.46 \n69.80 \n66.27 \n59.21 \n60.41 \n69.58 \n73.18 \n13.34 \n13.97 \n13.76 \n14.04 \nQwen2.5-7B \n79.60 \n83.13 \n79.68 \n77.77 \n68.03 \n71.14 \n77.70 \n77.06 \n28.86 \n9.32 \n14.54 \n10.94 \nQwen2.5-14B \n80.59 \n82.36 \n81.58 \n79.82 \n75.79 \n77.49 \n80.88 \n80.38 \n32.96 \n13.48 \n26.89 \n13.62 \nQwen2.5-72B \n80.95 \n84.47 \n83.70 \n83.84 \n77.70 \n79.11 \n82.78 \n82.43 \n49.61 \n33.73 \n34.30 \n33.45 \nQwQ-32B \n77.35 \n84.05 \n76.15 \n76.85 \n75.65 \n77.91 \n77.13 \n76.71 \n34.02 \n25.41 \n30.28 \n30.91 \nQwen3-1.7B \n68.03 \n75.37 \n66.69 \n61.54 \n54.20 \n49.26 \n64.22 \n63.30 \n27.45 \n22.23 \n31.90 \n33.17 \nQwen3-4B \n76.71 \n82.99 \n67.68 \n58.65 \n72.34 \n71.63 \n75.44 \n72.34 \n18.35 \n19.97 \n10.80 \n14.68 \nQwen3-4B-thinking \n78.48 \n84.90 \n77.70 \n75.51 \n77.49 \n76.64 \n75.65 \n75.79 \n23.57 \n12.14 \n12.91 \n10.94 \nQwen3-8B \n76.64 \n77.42 \n72.12 \n68.45 \n77.63 \n72.90 \n71.21 \n70.71 \n27.52 \n14.11 \n24.35 \n20.89 \nQwen3-8B-thinking \n77.98 \n82.15 \n80.31 \n80.24 \n78.33 \n74.74 \n81.23 \n81.44 \n31.90 \n8.82 \n17.64 \n15.31 \nQwen3-14B \n81.16 \n82.92 \n81.30 \n78.76 \n79.46 \n78.05 \n80.80 \n82.43 \n45.10 \n20.68 \n22.65 \n31.26 \nQwen3-14B-thinking \n81.37 \n84.33 \n80.31 \n79.04 \n79.46 \n79.39 \n81.72 \n80.52 \n52.51 \n8.19 \n16.44 \n28.93 \nBaichuan-M2-32B \n76.57 \n80.88 \n78.41 \n76.57 \n73.82 \n73.39 \n77.70 \n77.70 \n30.20 \n19.34 \n31.26 \n28.86 \nBio-Medical-LLaMA-3-8B \n40.79 \n57.94 \n45.80 \n39.31 \n39.03 \n34.79 \n38.18 \n39.94 \n32.32 \n32.39 \n33.52 \n34.23 \nMediPhi \n30.77 \n73.25 \n62.39 \n56.88 \n36.34 \n35.64 \n55.96 \n58.65 \n17.71 \n28.23 \n11.64 \n22.02 \nMedGemma-4B \n59.35 \n72.05 \n59.00 \n62.31 \n59.07 \n57.73 \n59.77 \n64.29 \n45.73 \n15.60 \n30.28 \n30.77 \nMedGemma-27B \n77.21 \n81.51 \n74.74 \n74.66 \n74.88 \n78.76 \n75.51 \n76.85 \n67.54 \n26.46 \n37.40 \n55.33 \nMedReason-8B \n41.92 \n11.93 \n9.10 \n9.46 \n41.99 \n45.02 \n23.64 \n3.53 \n8.33 \n11.15 \n24.91 \n25.19 \nHuatuoGPT-o1-7B \n74.03 \n78.05 \n71.63 \n72.69 \n57.23 \n65.35 \n72.05 \n74.59 \n5.72 \n4.80 \n8.68 \n5.58 \nHuatuoGPT-o1-8B \n54.06 \n61.40 \n57.73 \n59.14 \n43.75 \n57.02 \n59.14 \n60.06 \n31.62 \n5.50 \n9.10 \n10.80 \nHuatuoGPT-o1-70B \n67.33 \n80.24 \n66.62 \n66.13 \n66.69 \n67.75 \n66.27 \n67.96 \n62.31 \n31.40 \n34.44 \n39.31 \nHuatuoGPT-o1-72B \n77.84 \n84.69 \n81.44 \n79.32 \n74.45 \n76.78 \n79.89 \n79.89 \n47.85 \n19.76 \n27.73 \n19.97 \nOpenBioLLM-8B \n4.30 \n20.18 \n11.43 \n17.50 \n5.36 \n7.69 \n21.38 \n20.89 \n12.00 \n9.88 \n9.74 \n15.10 \nOpenBioLLM-70B \n27.52 \n73.32 \n57.52 \n31.33 \n29.36 \n32.67 \n54.41 \n63.02 \n25.19 \n13.34 \n15.03 \n20.47 \nSTab. 94: Zero-Shot performance evaluation of 56 LLMs on MedNLI (Run 1). \n"}, {"page": 87, "text": " \n \n57 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n68.45 \n77.84 \n67.68 \n65.63 \n68.03 \n69.87 \n66.62 \n69.02 \n50.32 \n10.30 \n33.24 \n33.66 \nClaude-4.0-Sonnet \n81.37 \n85.67 \n79.82 \n76.01 \n77.91 \n79.96 \n76.01 \n78.19 \n68.24 \n34.23 \n57.23 \n57.23 \nGemini-2.5-Flash \n70.57 \n84.54 \n70.36 \n70.08 \n67.82 \n69.02 \n68.67 \n70.01 \n67.11 \n46.22 \n59.99 \n62.17 \nGPT-4o-mini \n79.39 \n85.25 \n80.17 \n79.11 \n77.21 \n76.78 \n81.16 \n79.89 \n62.24 \n27.52 \n35.07 \n51.24 \nGPT-4o \n77.91 \n83.27 \n74.45 \n75.58 \n77.91 \n76.92 \n78.69 \n74.81 \n71.28 \n27.03 \n45.73 \n61.54 \nGPT-4.1-nano \n72.48 \n83.13 \n74.45 \n74.95 \n70.64 \n69.80 \n72.62 \n75.79 \n58.15 \n23.99 \n33.52 \n49.19 \nGPT-4.1-mini \n83.27 \n87.37 \n85.89 \n83.13 \n79.32 \n81.16 \n83.06 \n82.99 \n63.66 \n18.49 \n47.28 \n59.63 \nGPT-4.1 \n83.63 \n87.44 \n84.97 \n85.60 \n82.36 \n82.71 \n85.11 \n86.52 \n71.84 \n39.66 \n48.41 \n63.66 \nGPT-5-nano \n72.69 \n78.62 \n70.01 \n78.26 \n72.05 \n68.10 \n78.48 \n81.86 \n55.33 \n17.57 \n32.18 \n41.07 \nGPT-5-mini \n83.77 \n88.43 \n87.16 \n86.73 \n83.63 \n83.91 \n87.01 \n87.37 \n72.97 \n31.97 \n46.37 \n62.60 \nGPT-5 \n81.72 \n86.38 \n85.89 \n86.24 \n82.15 \n83.27 \n84.40 \n85.82 \n80.31 \n43.68 \n53.14 \n66.27 \no4-mini \n85.67 \n88.14 \n87.30 \n87.30 \n85.04 \n84.47 \n87.51 \n88.50 \n79.82 \n20.18 \n51.02 \n67.47 \nOpen-Weight LLMs \nDeepSeek-V3 \n78.97 \n84.40 \n77.21 \n79.89 \n74.52 \n74.95 \n76.50 \n78.97 \n68.38 \n34.51 \n45.59 \n53.35 \nDeepSeek-R1 \n78.12 \n83.20 \n74.17 \n76.64 \n76.50 \n74.17 \n74.31 \n77.28 \n58.79 \n35.07 \n45.80 \n54.34 \nDeepSeek-R1-Qwen3-8B \n77.91 \n81.23 \n75.51 \n75.37 \n75.09 \n71.84 \n77.42 \n75.79 \n32.11 \n12.42 \n13.06 \n15.46 \nGemma-3-4B \n66.48 \n73.25 \n61.33 \n58.86 \n56.88 \n62.46 \n62.81 \n62.39 \n45.45 \n7.69 \n29.85 \n20.18 \nGemma-3-12B \n63.80 \n82.00 \n67.82 \n66.27 \n68.17 \n69.94 \n64.71 \n66.69 \n65.28 \n6.70 \n36.13 \n48.27 \nGemma-3-27B \n76.57 \n82.78 \n70.29 \n73.47 \n73.96 \n77.98 \n72.83 \n73.25 \n71.49 \n22.94 \n38.39 \n55.47 \ngpt-oss-20B \n81.44 \n75.16 \n83.56 \n84.47 \n80.38 \n80.45 \n82.85 \n82.00 \n53.56 \n15.81 \n34.23 \n59.14 \ngpt-oss-120B \n81.72 \n87.09 \n84.83 \n85.82 \n82.50 \n82.71 \n85.04 \n86.03 \n72.12 \n26.53 \n49.75 \n65.00 \nLLaMA-3.1-8B \n49.89 \n67.18 \n62.17 \n59.70 \n54.69 \n57.45 \n62.60 \n64.64 \n38.39 \n23.64 \n21.88 \n18.42 \nLLaMA-3.1-70B \n65.35 \n77.28 \n67.75 \n68.10 \n66.13 \n66.69 \n68.88 \n68.10 \n62.60 \n32.18 \n34.72 \n37.47 \nLLaMA-3.2-3B \n29.57 \n64.57 \n51.09 \n40.93 \n37.33 \n30.56 \n50.67 \n54.69 \n31.69 \n20.04 \n23.57 \n24.84 \nLLaMA-3.3-70B \n60.90 \n81.79 \n67.75 \n68.10 \n66.83 \n69.65 \n66.97 \n67.68 \n64.57 \n33.73 \n37.90 \n39.66 \nLLaMA-4-Scout \n77.28 \n80.95 \n70.85 \n72.19 \n68.60 \n70.57 \n73.32 \n72.34 \n68.74 \n35.22 \n41.85 \n57.16 \nLLaMA-4-Maverick \n73.54 \n85.53 \n72.97 \n70.15 \n70.78 \n78.33 \n73.61 \n75.65 \n65.63 \n36.27 \n48.13 \n62.53 \nMistral-7B-v0.3 \n37.97 \n13.13 \n20.32 \n39.59 \n28.65 \n24.35 \n34.79 \n41.14 \n16.65 \n12.07 \n11.22 \n12.42 \nMistral-Small-3.1-24B \n74.59 \n83.13 \n72.97 \n68.38 \n62.88 \n63.94 \n73.25 \n74.45 \n30.42 \n13.97 \n11.36 \n20.68 \nPhi-4-mini \n65.35 \n77.13 \n69.37 \n67.96 \n61.12 \n44.88 \n65.77 \n67.89 \n31.47 \n18.84 \n22.23 \n22.79 \nPhi-4-mini-Reasoning \n49.12 \n74.59 \n58.15 \n58.65 \n21.10 \n16.51 \n39.73 \n40.72 \n29.57 \n23.50 \n24.98 \n27.17 \nPhi-4 \n75.94 \n83.70 \n77.98 \n80.03 \n78.62 \n73.68 \n76.22 \n78.69 \n49.05 \n24.49 \n30.20 \n30.42 \nPhi-4-Reasoning \n78.97 \n85.32 \n80.73 \n81.44 \n79.46 \n78.69 \n76.57 \n82.85 \n57.16 \n10.52 \n32.18 \n22.09 \nQwen2.5-3B \n75.16 \n78.48 \n68.95 \n66.27 \n60.06 \n62.31 \n69.58 \n72.55 \n11.86 \n14.40 \n11.86 \n12.63 \nQwen2.5-7B \n80.10 \n83.49 \n78.48 \n77.63 \n67.82 \n72.12 \n78.69 \n77.98 \n27.73 \n9.46 \n14.26 \n11.86 \nQwen2.5-14B \n79.75 \n81.79 \n81.16 \n80.31 \n75.02 \n76.78 \n81.44 \n79.89 \n31.62 \n12.91 \n26.39 \n14.33 \nQwen2.5-72B \n80.88 \n85.96 \n83.35 \n83.63 \n77.06 \n78.05 \n83.49 \n82.78 \n48.84 \n33.87 \n33.45 \n31.47 \nQwQ-32B \n77.77 \n83.49 \n76.78 \n77.13 \n74.95 \n77.13 \n76.36 \n76.92 \n35.85 \n28.58 \n31.26 \n31.47 \nQwen3-1.7B \n67.47 \n76.29 \n66.06 \n63.37 \n53.00 \n49.82 \n65.21 \n63.44 \n27.38 \n22.94 \n30.70 \n32.53 \nQwen3-4B \n77.21 \n82.07 \n69.58 \n55.96 \n73.39 \n71.35 \n74.95 \n73.82 \n17.64 \n16.73 \n10.94 \n16.30 \nQwen3-4B-thinking \n79.04 \n84.40 \n76.99 \n76.08 \n78.90 \n77.13 \n76.78 \n77.13 \n23.92 \n11.64 \n11.29 \n11.93 \nQwen3-8B \n76.57 \n76.85 \n72.27 \n70.29 \n77.56 \n72.69 \n71.00 \n71.56 \n28.51 \n13.41 \n23.57 \n19.83 \nQwen3-8B-thinking \n77.13 \n82.99 \n79.89 \n80.03 \n78.05 \n75.51 \n79.32 \n80.52 \n32.67 \n8.05 \n19.62 \n15.53 \nQwen3-14B \n80.52 \n82.36 \n82.07 \n76.92 \n78.41 \n78.55 \n82.78 \n82.99 \n44.04 \n21.52 \n21.24 \n31.83 \nQwen3-14B-thinking \n80.59 \n84.83 \n80.03 \n78.90 \n78.33 \n78.12 \n82.64 \n81.79 \n54.90 \n8.82 \n17.64 \n28.23 \nBaichuan-M2-32B \n76.78 \n81.30 \n78.33 \n77.56 \n74.24 \n74.03 \n78.33 \n78.19 \n29.08 \n19.48 \n30.84 \n25.76 \nBio-Medical-LLaMA-3-8B \n41.07 \n58.15 \n44.32 \n39.45 \n38.25 \n34.16 \n37.76 \n39.31 \n32.60 \n33.52 \n31.83 \n31.83 \nMediPhi \n28.44 \n75.65 \n61.33 \n57.09 \n36.98 \n34.79 \n56.46 \n58.72 \n17.71 \n28.44 \n10.94 \n19.90 \nMedGemma-4B \n59.00 \n72.27 \n59.56 \n62.88 \n59.14 \n58.50 \n58.86 \n62.67 \n42.70 \n15.74 \n31.90 \n31.55 \nMedGemma-27B \n76.64 \n81.37 \n73.96 \n75.30 \n75.58 \n78.12 \n76.85 \n77.98 \n68.53 \n26.25 \n35.22 \n55.96 \nMedReason-8B \n42.98 \n10.87 \n8.47 \n9.53 \n39.73 \n44.39 \n24.14 \n3.32 \n8.54 \n11.79 \n24.06 \n23.85 \nHuatuoGPT-o1-7B \n72.76 \n79.75 \n71.14 \n73.32 \n58.08 \n66.48 \n72.97 \n73.39 \n5.15 \n5.43 \n8.82 \n4.66 \nHuatuoGPT-o1-8B \n51.80 \n60.90 \n58.22 \n58.72 \n46.15 \n58.86 \n59.77 \n58.43 \n34.58 \n6.28 \n10.87 \n8.75 \nHuatuoGPT-o1-70B \n67.11 \n79.75 \n67.04 \n66.41 \n68.53 \n68.10 \n67.18 \n66.97 \n62.24 \n30.77 \n33.66 \n39.10 \nHuatuoGPT-o1-72B \n78.55 \n84.33 \n80.45 \n78.62 \n73.75 \n78.05 \n79.18 \n80.59 \n47.21 \n19.97 \n30.20 \n19.27 \nOpenBioLLM-8B \n6.07 \n20.75 \n13.27 \n16.44 \n5.29 \n6.56 \n19.90 \n22.30 \n12.07 \n8.33 \n9.74 \n16.51 \nOpenBioLLM-70B \n28.16 \n73.18 \n58.79 \n31.19 \n28.23 \n30.28 \n50.53 \n62.10 \n22.23 \n12.28 \n15.17 \n18.70 \nSTab. 95: Zero-Shot performance evaluation of 56 LLMs on MedNLI (Run 2). \n"}, {"page": 88, "text": " \n \n58 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n69.44 \n78.76 \n67.33 \n66.27 \n67.25 \n69.72 \n67.11 \n68.38 \n51.45 \n9.39 \n34.58 \n31.19 \nClaude-4.0-Sonnet \n81.51 \n85.18 \n79.18 \n75.44 \n78.76 \n79.89 \n76.64 \n78.12 \n68.53 \n34.86 \n58.43 \n57.23 \nGemini-2.5-Flash \n72.48 \n82.71 \n70.78 \n70.01 \n67.47 \n70.08 \n69.16 \n69.58 \n66.97 \n47.14 \n58.65 \n63.02 \nGPT-4o-mini \n78.12 \n83.91 \n82.29 \n80.38 \n78.83 \n76.01 \n80.10 \n79.04 \n61.26 \n28.86 \n34.16 \n51.24 \nGPT-4o \n77.98 \n82.22 \n74.74 \n75.09 \n75.79 \n76.64 \n79.82 \n75.16 \n71.21 \n25.41 \n47.64 \n60.27 \nGPT-4.1-nano \n72.55 \n83.70 \n73.89 \n74.88 \n72.34 \n69.44 \n72.34 \n76.15 \n55.05 \n25.41 \n33.10 \n46.93 \nGPT-4.1-mini \n82.50 \n88.07 \n85.32 \n83.06 \n78.62 \n80.73 \n82.64 \n82.57 \n63.51 \n19.12 \n48.41 \n58.86 \nGPT-4.1 \n82.99 \n88.00 \n85.25 \n85.18 \n81.86 \n82.43 \n84.90 \n86.52 \n71.28 \n38.60 \n49.82 \n63.66 \nGPT-5-nano \n74.03 \n76.78 \n69.09 \n80.24 \n70.92 \n70.43 \n78.62 \n79.32 \n55.40 \n16.65 \n31.76 \n42.48 \nGPT-5-mini \n84.12 \n88.36 \n87.72 \n86.87 \n83.56 \n83.13 \n86.52 \n86.66 \n73.32 \n33.03 \n47.49 \n63.94 \nGPT-5 \n81.09 \n86.31 \n85.74 \n85.74 \n82.99 \n83.06 \n85.18 \n85.82 \n80.52 \n42.55 \n53.85 \n65.28 \no4-mini \n84.97 \n89.20 \n88.14 \n87.37 \n85.25 \n84.54 \n87.51 \n87.51 \n78.26 \n23.08 \n52.58 \n67.47 \nOpen-Weight LLMs \nDeepSeek-V3 \n77.98 \n84.69 \n77.91 \n78.19 \n75.30 \n75.09 \n76.36 \n78.19 \n68.03 \n34.79 \n45.94 \n50.25 \nDeepSeek-R1 \n77.70 \n83.70 \n74.74 \n76.85 \n75.86 \n73.11 \n74.88 \n78.19 \n59.99 \n35.29 \n46.65 \n54.06 \nDeepSeek-R1-Qwen3-8B \n77.56 \n79.96 \n77.91 \n75.44 \n76.36 \n71.07 \n75.79 \n76.08 \n30.20 \n12.35 \n11.57 \n16.87 \nGemma-3-4B \n64.93 \n73.89 \n60.90 \n57.66 \n58.65 \n63.51 \n62.67 \n61.54 \n46.51 \n7.13 \n29.85 \n22.30 \nGemma-3-12B \n64.78 \n82.99 \n65.77 \n67.25 \n68.38 \n69.16 \n64.78 \n66.20 \n66.13 \n7.83 \n35.85 \n50.39 \nGemma-3-27B \n77.84 \n82.22 \n69.16 \n74.03 \n73.47 \n77.91 \n72.97 \n73.11 \n71.98 \n23.99 \n40.30 \n55.19 \ngpt-oss-20B \n82.57 \n77.84 \n83.77 \n83.35 \n80.52 \n80.17 \n82.92 \n82.92 \n52.36 \n16.02 \n35.57 \n54.69 \ngpt-oss-120B \n81.86 \n87.51 \n84.83 \n85.82 \n82.36 \n82.50 \n85.53 \n85.25 \n74.38 \n25.05 \n49.12 \n65.35 \nLLaMA-3.1-8B \n51.31 \n66.41 \n61.33 \n60.55 \n51.31 \n60.06 \n62.60 \n62.95 \n40.23 \n23.29 \n21.45 \n18.28 \nLLaMA-3.1-70B \n66.69 \n77.42 \n67.47 \n67.18 \n65.00 \n68.38 \n67.89 \n67.89 \n62.31 \n33.24 \n32.60 \n35.85 \nLLaMA-3.2-3B \n30.56 \n63.80 \n48.13 \n40.86 \n35.78 \n31.05 \n51.45 \n53.21 \n33.45 \n22.02 \n23.29 \n25.41 \nLLaMA-3.3-70B \n60.83 \n82.36 \n67.47 \n67.04 \n66.69 \n69.16 \n67.47 \n68.17 \n63.37 \n32.46 \n36.56 \n38.39 \nLLaMA-4-Scout \n76.64 \n82.00 \n71.28 \n72.69 \n67.82 \n71.42 \n73.39 \n72.83 \n69.37 \n34.44 \n43.26 \n57.23 \nLLaMA-4-Maverick \n74.10 \n84.83 \n73.89 \n70.43 \n71.63 \n78.90 \n72.12 \n75.30 \n66.20 \n35.85 \n48.76 \n63.09 \nMistral-7B-v0.3 \n37.26 \n14.18 \n19.34 \n41.21 \n27.03 \n24.98 \n33.31 \n40.72 \n16.51 \n13.48 \n11.50 \n11.79 \nMistral-Small-3.1-24B \n76.78 \n84.05 \n73.75 \n68.88 \n64.36 \n64.15 \n73.75 \n72.76 \n29.43 \n12.00 \n11.36 \n19.48 \nPhi-4-mini \n64.22 \n77.06 \n67.82 \n66.62 \n62.95 \n44.11 \n65.35 \n67.40 \n34.09 \n17.78 \n21.81 \n23.85 \nPhi-4-mini-Reasoning \n49.75 \n75.94 \n59.49 \n60.76 \n20.68 \n15.88 \n41.14 \n38.39 \n29.78 \n26.04 \n25.34 \n26.18 \nPhi-4 \n76.01 \n84.05 \n77.28 \n79.32 \n76.78 \n74.95 \n76.29 \n77.91 \n47.64 \n24.35 \n31.19 \n30.77 \nPhi-4-Reasoning \n79.39 \n85.25 \n81.16 \n82.85 \n79.96 \n78.90 \n77.91 \n82.36 \n57.73 \n11.86 \n32.75 \n22.51 \nQwen2.5-3B \n75.09 \n79.04 \n70.15 \n64.93 \n59.70 \n62.88 \n70.29 \n71.21 \n12.70 \n14.82 \n11.71 \n14.33 \nQwen2.5-7B \n79.39 \n83.27 \n78.62 \n77.21 \n68.53 \n72.55 \n78.48 \n76.99 \n25.62 \n9.10 \n12.49 \n10.66 \nQwen2.5-14B \n79.25 \n82.57 \n79.75 \n78.76 \n74.31 \n76.43 \n81.44 \n81.79 \n33.10 \n12.99 \n27.59 \n13.76 \nQwen2.5-72B \n79.82 \n85.39 \n83.77 \n83.13 \n77.21 \n78.19 \n82.50 \n82.71 \n48.62 \n34.09 \n33.59 \n32.96 \nQwQ-32B \n77.70 \n84.12 \n76.08 \n78.33 \n75.94 \n78.48 \n76.78 \n77.77 \n34.02 \n29.64 \n29.99 \n32.25 \nQwen3-1.7B \n68.17 \n75.09 \n66.97 \n61.47 \n54.41 \n48.34 \n65.28 \n65.42 \n26.61 \n24.28 \n28.86 \n30.42 \nQwen3-4B \n75.94 \n81.79 \n68.88 \n59.21 \n72.12 \n71.56 \n76.15 \n73.89 \n17.71 \n19.27 \n12.07 \n14.61 \nQwen3-4B-thinking \n77.77 \n84.69 \n77.98 \n76.22 \n78.41 \n75.79 \n76.92 \n77.06 \n23.92 \n11.15 \n11.29 \n12.42 \nQwen3-8B \n77.13 \n78.19 \n71.42 \n68.81 \n77.56 \n72.12 \n70.64 \n69.23 \n26.46 \n12.21 \n25.34 \n20.61 \nQwen3-8B-thinking \n78.12 \n82.36 \n80.52 \n79.68 \n78.97 \n75.58 \n80.03 \n79.46 \n33.10 \n7.83 \n21.03 \n15.88 \nQwen3-14B \n80.24 \n83.27 \n81.51 \n77.28 \n78.90 \n78.12 \n81.44 \n82.64 \n43.61 \n20.04 \n23.36 \n30.35 \nQwen3-14B-thinking \n80.59 \n84.62 \n80.52 \n79.39 \n78.97 \n78.69 \n82.00 \n80.10 \n53.35 \n8.68 \n18.00 \n29.92 \nBaichuan-M2-32B \n76.22 \n80.73 \n78.12 \n78.48 \n75.30 \n73.18 \n78.69 \n77.77 \n32.89 \n19.20 \n30.98 \n28.86 \nBio-Medical-LLaMA-3-8B \n39.73 \n55.96 \n44.81 \n38.88 \n38.32 \n33.24 \n39.10 \n40.58 \n33.80 \n34.72 \n32.25 \n33.52 \nMediPhi \n30.70 \n73.68 \n62.17 \n55.68 \n36.84 \n34.93 \n57.52 \n58.29 \n19.20 \n27.24 \n12.70 \n19.12 \nMedGemma-4B \n58.01 \n71.14 \n60.97 \n62.10 \n58.93 \n59.21 \n59.21 \n64.29 \n45.73 \n14.26 \n32.89 \n30.35 \nMedGemma-27B \n77.35 \n82.00 \n74.88 \n74.45 \n75.44 \n77.56 \n75.86 \n76.71 \n67.61 \n26.18 \n35.78 \n55.05 \nMedReason-8B \n41.35 \n11.57 \n8.75 \n10.30 \n42.70 \n46.01 \n23.85 \n3.88 \n7.20 \n11.50 \n24.98 \n25.19 \nHuatuoGPT-o1-7B \n72.34 \n80.52 \n71.28 \n72.48 \n56.74 \n66.97 \n72.69 \n74.10 \n6.35 \n5.79 \n7.13 \n4.73 \nHuatuoGPT-o1-8B \n53.92 \n61.04 \n58.79 \n59.14 \n47.49 \n57.80 \n59.49 \n58.65 \n33.17 \n5.22 \n11.86 \n9.39 \nHuatuoGPT-o1-70B \n67.40 \n78.48 \n66.27 \n66.41 \n67.61 \n65.70 \n66.41 \n68.17 \n61.12 \n31.33 \n35.57 \n38.60 \nHuatuoGPT-o1-72B \n78.69 \n84.83 \n81.16 \n78.05 \n74.81 \n76.85 \n79.11 \n80.17 \n47.07 \n18.00 \n29.29 \n19.97 \nOpenBioLLM-8B \n6.56 \n19.69 \n11.50 \n17.57 \n5.01 \n6.99 \n18.77 \n21.10 \n11.50 \n10.16 \n10.66 \n15.81 \nOpenBioLLM-70B \n29.15 \n71.91 \n58.01 \n29.99 \n28.65 \n32.53 \n51.24 \n62.74 \n22.72 \n12.28 \n14.04 \n18.77 \nSTab. 96: Zero-Shot performance evaluation of 56 LLMs on MedNLI (Run 3). \n"}, {"page": 89, "text": " \n \n59 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n69.58 \n76.99 \n67.11 \n66.69 \n67.54 \n69.58 \n66.69 \n68.81 \n51.16 \n10.87 \n32.82 \n31.76 \nClaude-4.0-Sonnet \n81.72 \n85.89 \n78.69 \n75.58 \n78.33 \n80.73 \n76.15 \n78.19 \n68.45 \n34.79 \n57.59 \n56.67 \nGemini-2.5-Flash \n72.69 \n82.57 \n71.14 \n69.72 \n67.68 \n69.94 \n69.16 \n70.36 \n66.62 \n46.08 \n59.21 \n62.39 \nGPT-4o-mini \n80.52 \n85.04 \n80.52 \n78.26 \n78.55 \n76.64 \n78.62 \n79.96 \n62.24 \n27.24 \n34.09 \n49.40 \nGPT-4o \n77.42 \n80.52 \n76.57 \n75.30 \n76.22 \n77.35 \n77.91 \n75.02 \n71.84 \n28.72 \n47.71 \n62.31 \nGPT-4.1-nano \n71.07 \n83.98 \n75.23 \n75.09 \n71.35 \n70.22 \n73.61 \n76.29 \n57.09 \n22.79 \n32.60 \n47.07 \nGPT-4.1-mini \n83.20 \n88.29 \n84.90 \n82.57 \n79.89 \n80.59 \n82.43 \n82.36 \n64.36 \n21.52 \n47.71 \n59.99 \nGPT-4.1 \n82.85 \n87.86 \n85.60 \n83.91 \n82.71 \n82.36 \n85.53 \n86.73 \n72.12 \n39.24 \n50.46 \n63.23 \nGPT-5-nano \n72.34 \n77.06 \n70.36 \n78.62 \n73.75 \n69.09 \n78.69 \n80.73 \n55.68 \n16.16 \n33.38 \n41.50 \nGPT-5-mini \n83.35 \n88.78 \n87.93 \n86.10 \n83.42 \n82.07 \n86.03 \n87.16 \n72.48 \n31.40 \n47.21 \n62.31 \nGPT-5 \n80.80 \n86.52 \n85.89 \n85.46 \n81.72 \n83.56 \n84.69 \n85.25 \n80.95 \n43.19 \n53.92 \n65.63 \no4-mini \n85.18 \n88.71 \n88.07 \n87.23 \n85.32 \n85.25 \n86.94 \n87.51 \n79.18 \n21.24 \n50.53 \n67.82 \nOpen-Weight LLMs \nDeepSeek-V3 \n78.69 \n84.19 \n78.19 \n79.82 \n75.30 \n75.23 \n75.79 \n78.76 \n68.95 \n33.03 \n46.44 \n52.65 \nDeepSeek-R1 \n77.56 \n83.70 \n75.65 \n76.08 \n76.43 \n72.05 \n74.95 \n76.85 \n59.56 \n34.65 \n46.22 \n53.99 \nDeepSeek-R1-Qwen3-8B \n78.48 \n80.73 \n76.50 \n76.78 \n75.30 \n69.72 \n75.51 \n77.70 \n33.10 \n12.14 \n12.28 \n12.99 \nGemma-3-4B \n64.50 \n72.76 \n60.90 \n59.14 \n58.08 \n62.88 \n62.95 \n63.09 \n47.14 \n8.54 \n29.92 \n21.67 \nGemma-3-12B \n62.74 \n82.07 \n66.69 \n67.89 \n66.97 \n69.80 \n65.07 \n66.13 \n66.69 \n7.55 \n35.00 \n48.27 \nGemma-3-27B \n78.48 \n83.77 \n70.36 \n73.75 \n73.82 \n78.12 \n71.91 \n73.25 \n70.71 \n25.19 \n37.76 \n55.40 \ngpt-oss-20B \n82.15 \n76.78 \n83.49 \n85.39 \n80.17 \n79.60 \n84.54 \n83.27 \n52.36 \n16.02 \n34.65 \n56.67 \ngpt-oss-120B \n82.57 \n87.44 \n84.12 \n86.59 \n82.00 \n82.71 \n85.18 \n86.17 \n72.41 \n25.34 \n47.99 \n65.35 \nLLaMA-3.1-8B \n50.25 \n67.25 \n62.60 \n61.75 \n52.01 \n57.80 \n62.10 \n62.74 \n37.61 \n24.28 \n23.85 \n16.87 \nLLaMA-3.1-70B \n66.55 \n78.62 \n67.68 \n67.61 \n65.49 \n67.11 \n68.38 \n67.54 \n62.39 \n31.40 \n36.34 \n35.00 \nLLaMA-3.2-3B \n29.78 \n64.36 \n50.53 \n40.58 \n36.77 \n30.63 \n50.39 \n53.28 \n32.53 \n20.75 \n23.71 \n24.35 \nLLaMA-3.3-70B \n61.82 \n81.86 \n67.61 \n67.18 \n66.90 \n69.94 \n67.47 \n67.89 \n64.36 \n32.89 \n38.11 \n40.08 \nLLaMA-4-Scout \n76.64 \n82.57 \n71.21 \n72.27 \n68.88 \n70.64 \n72.19 \n73.68 \n68.95 \n34.72 \n42.63 \n56.32 \nLLaMA-4-Maverick \n74.24 \n85.53 \n73.18 \n70.78 \n71.21 \n77.35 \n72.97 \n75.58 \n65.42 \n35.92 \n48.76 \n63.51 \nMistral-7B-v0.3 \n36.56 \n13.62 \n20.96 \n42.48 \n28.44 \n25.12 \n34.30 \n40.44 \n15.38 \n13.13 \n11.50 \n11.01 \nMistral-Small-3.1-24B \n75.30 \n82.50 \n72.97 \n67.75 \n65.00 \n64.08 \n74.17 \n75.09 \n29.08 \n12.42 \n10.23 \n19.76 \nPhi-4-mini \n63.59 \n78.19 \n68.10 \n67.25 \n61.54 \n45.80 \n67.40 \n67.25 \n31.47 \n17.93 \n20.68 \n24.35 \nPhi-4-mini-Reasoning \n49.12 \n76.64 \n57.80 \n61.54 \n20.47 \n18.21 \n41.28 \n39.10 \n30.42 \n23.29 \n23.92 \n25.76 \nPhi-4 \n75.02 \n83.56 \n79.18 \n78.19 \n77.42 \n73.32 \n77.63 \n77.98 \n47.85 \n24.98 \n32.67 \n30.06 \nPhi-4-Reasoning \n80.31 \n85.89 \n81.65 \n82.29 \n78.83 \n78.05 \n77.28 \n81.65 \n58.57 \n11.57 \n31.69 \n23.85 \nQwen2.5-3B \n74.81 \n79.32 \n70.08 \n66.62 \n60.13 \n61.26 \n70.64 \n73.18 \n12.70 \n14.18 \n11.71 \n14.54 \nQwen2.5-7B \n78.97 \n82.57 \n78.48 \n76.78 \n67.11 \n70.43 \n78.26 \n76.36 \n26.75 \n8.47 \n13.83 \n9.81 \nQwen2.5-14B \n80.31 \n81.86 \n81.65 \n78.62 \n75.16 \n76.36 \n80.80 \n81.58 \n33.24 \n12.49 \n27.03 \n14.18 \nQwen2.5-72B \n80.17 \n85.53 \n84.47 \n83.56 \n77.98 \n79.60 \n83.35 \n82.29 \n49.47 \n34.16 \n33.10 \n32.11 \nQwQ-32B \n78.19 \n83.63 \n77.49 \n77.77 \n75.51 \n78.33 \n77.49 \n77.06 \n33.45 \n30.06 \n32.39 \n31.90 \nQwen3-1.7B \n70.01 \n74.45 \n66.20 \n61.75 \n55.05 \n49.47 \n65.00 \n63.44 \n29.92 \n24.42 \n32.18 \n33.52 \nQwen3-4B \n77.13 \n83.13 \n67.89 \n57.94 \n74.74 \n72.41 \n75.94 \n72.90 \n15.95 \n17.93 \n10.09 \n15.46 \nQwen3-4B-thinking \n79.18 \n84.12 \n78.83 \n75.86 \n78.76 \n77.42 \n76.99 \n77.21 \n23.29 \n11.86 \n11.64 \n11.79 \nQwen3-8B \n74.38 \n77.28 \n72.62 \n69.87 \n76.22 \n72.69 \n70.57 \n71.77 \n27.03 \n11.79 \n25.83 \n19.83 \nQwen3-8B-thinking \n78.05 \n82.29 \n80.95 \n79.82 \n78.12 \n75.58 \n79.46 \n80.17 \n29.50 \n7.83 \n17.22 \n13.55 \nQwen3-14B \n81.16 \n83.13 \n80.80 \n77.98 \n78.83 \n78.76 \n81.58 \n82.22 \n43.75 \n21.52 \n23.01 \n34.23 \nQwen3-14B-thinking \n80.52 \n85.32 \n80.66 \n78.69 \n80.03 \n77.98 \n82.29 \n80.80 \n52.22 \n8.05 \n17.50 \n28.72 \nBaichuan-M2-32B \n76.71 \n80.95 \n78.83 \n78.26 \n74.31 \n73.18 \n78.05 \n78.12 \n31.40 \n19.97 \n27.88 \n28.44 \nBio-Medical-LLaMA-3-8B \n40.30 \n57.23 \n43.05 \n40.08 \n38.67 \n33.38 \n39.24 \n40.44 \n31.97 \n33.59 \n33.10 \n32.67 \nMediPhi \n28.44 \n73.61 \n60.20 \n56.39 \n36.77 \n36.34 \n56.74 \n59.07 \n18.14 \n27.52 \n11.57 \n20.89 \nMedGemma-4B \n58.79 \n72.55 \n60.76 \n61.47 \n59.92 \n57.37 \n60.06 \n64.08 \n44.11 \n15.31 \n31.83 \n33.59 \nMedGemma-27B \n77.56 \n82.50 \n74.24 \n74.74 \n75.02 \n77.91 \n76.57 \n77.42 \n68.31 \n25.48 \n38.11 \n53.71 \nMedReason-8B \n40.51 \n11.01 \n8.40 \n8.89 \n40.16 \n45.02 \n23.36 \n4.02 \n9.95 \n10.52 \n23.29 \n24.42 \nHuatuoGPT-o1-7B \n75.09 \n79.89 \n71.28 \n71.98 \n57.16 \n65.70 \n72.62 \n73.39 \n6.14 \n6.14 \n6.92 \n5.08 \nHuatuoGPT-o1-8B \n52.29 \n61.19 \n58.43 \n60.34 \n46.51 \n57.94 \n57.16 \n59.35 \n30.56 \n6.49 \n12.35 \n10.37 \nHuatuoGPT-o1-70B \n67.75 \n79.11 \n67.18 \n66.34 \n67.47 \n66.27 \n67.04 \n66.27 \n63.23 \n30.56 \n32.25 \n40.44 \nHuatuoGPT-o1-72B \n77.21 \n84.12 \n80.59 \n79.32 \n73.75 \n76.71 \n78.90 \n79.82 \n47.35 \n20.04 \n27.81 \n20.40 \nOpenBioLLM-8B \n6.07 \n22.51 \n11.93 \n17.71 \n4.23 \n5.22 \n21.52 \n20.75 \n12.91 \n9.67 \n10.09 \n16.09 \nOpenBioLLM-70B \n27.38 \n72.19 \n58.86 \n29.36 \n31.47 \n30.13 \n51.31 \n61.75 \n22.51 \n10.30 \n14.40 \n18.91 \nSTab. 97: Zero-Shot performance evaluation of 56 LLMs on MedNLI (Run 4). \n"}, {"page": 90, "text": " \n \n60 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n69.37 \n78.76 \n67.54 \n66.76 \n68.38 \n69.87 \n66.13 \n68.81 \n51.94 \n9.60 \n34.37 \n33.31 \nClaude-4.0-Sonnet \n81.93 \n85.32 \n79.53 \n76.22 \n78.97 \n79.89 \n76.43 \n78.26 \n68.10 \n33.66 \n57.94 \n56.46 \nGemini-2.5-Flash \n71.63 \n84.33 \n71.14 \n68.81 \n67.75 \n70.50 \n69.58 \n70.50 \n67.18 \n47.92 \n58.86 \n61.26 \nGPT-4o-mini \n80.52 \n84.90 \n81.51 \n79.46 \n78.55 \n78.26 \n79.82 \n79.39 \n61.61 \n30.91 \n34.86 \n50.67 \nGPT-4o \n78.55 \n81.93 \n75.09 \n75.65 \n75.86 \n76.36 \n78.55 \n75.44 \n71.21 \n28.65 \n46.93 \n61.04 \nGPT-4.1-nano \n70.64 \n83.70 \n76.15 \n75.86 \n69.23 \n69.65 \n71.42 \n76.50 \n55.89 \n25.62 \n33.45 \n47.99 \nGPT-4.1-mini \n82.29 \n87.44 \n85.74 \n82.29 \n79.68 \n81.51 \n82.71 \n82.78 \n62.74 \n20.61 \n46.65 \n59.70 \nGPT-4.1 \n83.42 \n87.86 \n85.74 \n85.89 \n82.36 \n82.50 \n84.90 \n86.73 \n71.49 \n38.88 \n50.88 \n63.73 \nGPT-5-nano \n73.96 \n77.77 \n70.36 \n79.53 \n72.55 \n71.14 \n78.33 \n81.23 \n55.19 \n16.44 \n33.03 \n40.86 \nGPT-5-mini \n83.35 \n88.57 \n87.65 \n85.67 \n82.78 \n82.50 \n87.72 \n87.65 \n72.34 \n33.17 \n47.07 \n63.44 \nGPT-5 \n81.16 \n86.24 \n86.17 \n85.96 \n82.36 \n82.43 \n84.54 \n85.53 \n80.59 \n42.34 \n55.05 \n65.77 \no4-mini \n84.40 \n88.57 \n87.37 \n87.30 \n84.76 \n83.91 \n87.23 \n87.58 \n79.68 \n20.75 \n52.15 \n68.10 \nOpen-Weight LLMs \nDeepSeek-V3 \n77.77 \n84.69 \n78.33 \n79.11 \n74.95 \n75.16 \n76.50 \n78.19 \n66.62 \n35.36 \n45.52 \n51.52 \nDeepSeek-R1 \n77.84 \n83.35 \n74.74 \n76.92 \n75.44 \n73.04 \n74.74 \n77.77 \n59.42 \n34.51 \n45.52 \n55.68 \nDeepSeek-R1-Qwen3-8B \n78.19 \n80.66 \n74.31 \n76.85 \n75.44 \n71.84 \n76.85 \n76.43 \n31.62 \n12.14 \n11.93 \n15.67 \nGemma-3-4B \n64.43 \n74.31 \n62.31 \n58.93 \n58.29 \n64.78 \n61.61 \n63.66 \n46.08 \n6.77 \n29.29 \n21.59 \nGemma-3-12B \n63.37 \n82.29 \n66.90 \n67.40 \n67.96 \n68.74 \n64.22 \n65.98 \n64.93 \n6.28 \n32.18 \n49.89 \nGemma-3-27B \n78.12 \n82.36 \n70.57 \n73.75 \n72.27 \n78.69 \n72.12 \n73.82 \n73.68 \n26.32 \n39.80 \n56.10 \ngpt-oss-20B \n81.37 \n76.57 \n84.33 \n84.47 \n82.36 \n80.10 \n83.56 \n83.06 \n53.99 \n17.50 \n34.16 \n56.95 \ngpt-oss-120B \n83.27 \n87.16 \n84.19 \n86.17 \n82.29 \n82.99 \n85.67 \n85.39 \n71.14 \n25.69 \n49.68 \n65.49 \nLLaMA-3.1-8B \n50.60 \n66.34 \n60.97 \n60.41 \n52.01 \n58.50 \n62.81 \n63.16 \n37.40 \n24.28 \n23.85 \n18.14 \nLLaMA-3.1-70B \n66.83 \n76.57 \n68.60 \n68.10 \n66.27 \n67.54 \n67.61 \n68.38 \n60.83 \n34.23 \n35.50 \n36.77 \nLLaMA-3.2-3B \n31.97 \n62.39 \n50.25 \n40.37 \n36.77 \n31.40 \n47.92 \n53.78 \n33.80 \n20.32 \n21.74 \n24.35 \nLLaMA-3.3-70B \n61.68 \n82.15 \n67.11 \n67.18 \n66.48 \n69.65 \n67.33 \n66.90 \n63.59 \n34.09 \n36.27 \n39.66 \nLLaMA-4-Scout \n76.29 \n80.95 \n71.63 \n72.05 \n68.45 \n69.51 \n72.69 \n73.11 \n68.38 \n34.65 \n41.99 \n57.09 \nLLaMA-4-Maverick \n73.96 \n84.40 \n74.24 \n71.28 \n70.71 \n78.33 \n72.48 \n75.23 \n66.13 \n37.26 \n47.92 \n62.10 \nMistral-7B-v0.3 \n37.19 \n12.91 \n19.34 \n41.21 \n29.50 \n25.12 \n34.37 \n42.13 \n15.81 \n12.70 \n11.29 \n11.08 \nMistral-Small-3.1-24B \n75.58 \n83.70 \n75.16 \n68.17 \n63.73 \n63.23 \n73.32 \n74.38 \n31.19 \n10.23 \n11.71 \n20.18 \nPhi-4-mini \n63.94 \n77.77 \n69.37 \n65.84 \n61.96 \n44.18 \n65.91 \n69.30 \n31.69 \n18.91 \n21.81 \n24.98 \nPhi-4-mini-Reasoning \n49.82 \n75.58 \n57.52 \n62.24 \n21.81 \n18.42 \n39.31 \n39.87 \n29.99 \n24.49 \n24.35 \n26.68 \nPhi-4 \n76.78 \n84.12 \n76.78 \n79.68 \n77.63 \n74.17 \n75.94 \n79.53 \n48.13 \n22.65 \n33.10 \n29.78 \nPhi-4-Reasoning \n81.09 \n85.89 \n81.30 \n81.23 \n79.04 \n77.98 \n76.08 \n82.07 \n57.09 \n11.71 \n33.38 \n24.21 \nQwen2.5-3B \n74.66 \n79.53 \n69.72 \n65.35 \n59.28 \n61.75 \n68.88 \n70.64 \n13.55 \n15.38 \n12.00 \n13.69 \nQwen2.5-7B \n78.97 \n82.50 \n80.03 \n76.85 \n67.33 \n71.63 \n78.69 \n77.77 \n26.61 \n10.02 \n14.04 \n9.46 \nQwen2.5-14B \n80.38 \n81.58 \n80.24 \n78.26 \n75.79 \n76.99 \n81.16 \n81.37 \n35.99 \n13.41 \n28.79 \n15.46 \nQwen2.5-72B \n80.45 \n85.18 \n83.70 \n83.84 \n77.91 \n78.12 \n81.79 \n83.42 \n50.11 \n34.44 \n33.80 \n32.18 \nQwQ-32B \n77.56 \n83.77 \n76.92 \n77.98 \n75.30 \n78.76 \n77.63 \n77.63 \n33.38 \n26.96 \n31.19 \n32.53 \nQwen3-1.7B \n68.53 \n75.23 \n66.97 \n62.60 \n54.34 \n49.75 \n65.98 \n63.37 \n25.55 \n24.06 \n29.43 \n32.53 \nQwen3-4B \n76.92 \n81.65 \n68.81 \n56.88 \n71.91 \n72.41 \n74.52 \n73.89 \n19.27 \n18.77 \n11.15 \n13.90 \nQwen3-4B-thinking \n79.39 \n84.54 \n78.76 \n76.64 \n79.04 \n76.99 \n76.99 \n76.99 \n24.42 \n12.70 \n12.70 \n10.37 \nQwen3-8B \n75.16 \n77.70 \n71.63 \n70.01 \n76.78 \n72.34 \n71.42 \n71.21 \n26.96 \n13.76 \n24.91 \n20.04 \nQwen3-8B-thinking \n77.98 \n81.65 \n79.32 \n78.90 \n77.98 \n76.85 \n80.95 \n80.52 \n30.42 \n9.10 \n20.32 \n15.95 \nQwen3-14B \n80.52 \n81.93 \n80.95 \n78.48 \n79.68 \n77.49 \n81.23 \n82.36 \n45.59 \n22.58 \n23.78 \n33.03 \nQwen3-14B-thinking \n79.39 \n84.33 \n81.44 \n80.24 \n80.80 \n78.90 \n80.66 \n81.72 \n53.35 \n9.03 \n17.29 \n28.65 \nBaichuan-M2-32B \n76.78 \n80.52 \n79.04 \n77.06 \n75.37 \n72.83 \n78.19 \n78.48 \n29.78 \n19.76 \n31.69 \n31.55 \nBio-Medical-LLaMA-3-8B \n40.01 \n58.36 \n44.67 \n39.73 \n38.60 \n32.89 \n38.25 \n41.21 \n32.67 \n33.31 \n34.65 \n33.59 \nMediPhi \n28.30 \n74.03 \n60.97 \n55.82 \n36.70 \n35.64 \n57.09 \n58.93 \n18.21 \n27.31 \n13.06 \n24.06 \nMedGemma-4B \n58.36 \n72.97 \n59.70 \n60.83 \n58.72 \n58.01 \n60.41 \n62.95 \n43.05 \n15.24 \n32.11 \n30.70 \nMedGemma-27B \n77.21 \n80.52 \n73.68 \n75.16 \n74.24 \n76.85 \n76.08 \n77.35 \n67.82 \n26.96 \n36.06 \n54.34 \nMedReason-8B \n41.28 \n10.87 \n9.03 \n9.60 \n39.66 \n46.22 \n23.85 \n3.53 \n8.19 \n11.64 \n23.99 \n23.99 \nHuatuoGPT-o1-7B \n74.31 \n78.41 \n70.71 \n72.12 \n56.67 \n64.86 \n73.04 \n74.38 \n4.94 \n5.72 \n8.26 \n4.87 \nHuatuoGPT-o1-8B \n53.63 \n59.35 \n57.66 \n59.92 \n45.10 \n59.21 \n59.70 \n58.65 \n33.24 \n6.07 \n11.01 \n10.44 \nHuatuoGPT-o1-70B \n66.83 \n78.62 \n66.83 \n66.06 \n67.61 \n66.97 \n66.90 \n67.82 \n61.26 \n32.82 \n34.09 \n40.44 \nHuatuoGPT-o1-72B \n77.63 \n84.90 \n80.73 \n78.19 \n74.17 \n77.06 \n80.10 \n80.24 \n47.57 \n18.56 \n28.79 \n18.77 \nOpenBioLLM-8B \n4.94 \n21.52 \n11.64 \n16.09 \n5.72 \n5.08 \n19.12 \n20.96 \n12.99 \n10.30 \n10.87 \n14.61 \nOpenBioLLM-70B \n28.93 \n72.55 \n56.95 \n30.35 \n28.23 \n30.20 \n53.07 \n61.82 \n22.44 \n11.86 \n14.11 \n19.12 \nSTab. 98: Zero-Shot performance evaluation of 56 LLMs on MedNLI (Run 5). \n"}, {"page": 91, "text": " \n \n61 \n \nLLMs \nChinese \nEnglish \nFrench \nGerman Japanese Korean \nPortuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nProprietary LLMs \nClaude-3.5-Haiku \n76.96±0.05 83.14±0.00 82.79±0.04 81.82±0.02 79.36±0.04 74.99±0.06 \n83.34±0.05 \n83.04±0.04 59.26±0.04 19.45±0.46 42.53±0.13 42.95±0.08 67.47±20.72 \nClaude-4.0-Sonnet \n87.78±0.29 89.56±0.14 89.97±0.14 89.32±0.10 88.42±0.21 87.83±0.34 \n90.32±0.27 \n90.72±0.30 80.47±0.31 55.56±0.56 70.72±0.43 76.12±0.49 83.07±10.41 \nGemini-2.5-Flash \n88.83±0.16 88.11±0.48 90.35±0.28 89.60±0.21 88.84±0.16 88.96±0.23 \n90.27±0.20 \n90.61±0.33 88.23±0.34 78.26±0.54 80.43±0.55 83.53±0.32 87.17±3.99 \nGPT-4o-mini \n76.93±0.11 83.54±0.38 82.68±0.47 81.97±0.28 78.10±0.37 72.34±0.29 \n82.37±0.35 \n82.18±0.25 69.38±0.39 35.69±0.52 48.67±0.47 58.10±0.44 71.00±15.05 \nGPT-4o \n86.96±0.32 87.87±0.16 88.95±0.23 88.29±0.28 87.69±0.20 85.07±0.14 \n88.81±0.21 \n89.18±0.09 83.97±0.25 37.49±0.51 61.44±0.36 73.68±0.28 79.95±15.16 \nGPT-4.1-nano \n76.59±0.44 84.59±0.19 81.86±0.54 80.96±0.74 74.56±0.89 72.63±0.81 \n82.20±0.27 \n81.20±0.47 59.13±0.54 33.51±0.70 44.72±0.87 53.54±0.34 68.79±16.38 \nGPT-4.1-mini \n86.13±0.31 88.59±0.15 87.75±0.23 87.61±0.27 86.53±0.26 83.05±0.35 \n88.89±0.25 \n88.41±0.22 77.55±0.23 27.20±0.85 61.77±0.36 72.59±0.25 78.01±17.38 \nGPT-4.1 \n88.57±0.39 90.05±0.17 90.19±0.18 89.59±0.37 89.51±0.33 86.14±0.47 \n90.52±0.10 \n89.28±0.11 86.67±0.42 58.72±0.57 67.49±0.28 78.58±0.36 83.78±10.00 \nGPT-5-nano \n67.63±0.69 80.66±0.40 78.25±0.35 77.47±0.61 74.14±0.57 66.74±0.53 \n75.31±0.28 \n77.05±0.66 57.85±1.09 17.02±0.47 41.53±0.46 50.74±0.56 63.70±18.44 \nGPT-5-mini \n85.53±0.14 88.36±0.43 87.90±0.24 87.37±0.40 86.53±0.41 82.71±0.18 \n88.22±0.37 \n88.34±0.22 77.87±0.41 33.48±0.40 59.40±0.42 73.02±0.57 78.23±15.97 \nGPT-5 \n88.30±0.11 90.44±0.29 89.99±0.23 89.61±0.19 89.11±0.15 86.01±0.37 \n90.41±0.32 \n90.86±0.16 86.71±0.20 57.47±0.09 71.58±0.31 79.55±0.45 84.17±9.79 \no4-mini \n88.88±0.17 90.28±0.16 90.22±0.09 89.97±0.32 89.48±0.13 87.08±0.42 \n90.77±0.17 \n90.50±0.21 86.74±0.38 47.23±0.87 76.07±0.20 82.47±0.27 84.14±11.97 \nOpen-Weight LLMs \nDeepSeek-V3 \n87.64±0.13 89.67±0.13 88.59±0.32 88.74±0.21 87.44±0.44 84.10±0.47 \n88.48±0.22 \n89.02±0.09 77.07±0.61 51.05±0.44 58.45±0.88 64.69±0.36 79.58±13.25 \nDeepSeek-R1 \n88.28±0.03 89.64±0.17 89.63±0.14 89.25±0.18 87.85±0.41 86.69±0.17 \n90.10±0.21 \n89.85±0.33 84.13±0.60 65.64±0.41 69.79±0.41 77.36±0.48 84.02±8.17 \nDeepSeek-R1-Qwen3-8B \n79.34±0.58 82.46±0.35 77.59±0.47 77.49±0.62 75.66±0.44 73.91±0.41 \n78.48±0.68 \n78.63±0.15 36.65±1.00 11.08±0.49 10.83±0.63 13.17±0.67 57.94±29.29 \nGemma-3-4B \n52.83±0.43 64.58±0.47 59.58±0.36 58.23±0.55 53.69±0.35 48.51±0.43 \n59.74±0.76 \n59.42±0.50 42.89±0.78 12.90±0.53 25.55±0.69 20.01±0.45 46.49±16.87 \nGemma-3-12B \n70.80±0.43 78.31±0.29 75.85±0.52 75.65±0.44 71.93±0.68 69.44±0.49 \n75.22±0.58 \n75.47±0.28 62.87±0.57 11.15±0.46 43.19±0.87 54.35±0.76 63.69±18.86 \nGemma-3-27B \n76.73±0.57 82.14±0.38 80.95±0.52 80.88±0.43 77.86±0.51 76.83±0.65 \n80.71±0.40 \n81.34±0.42 71.33±0.63 23.93±0.40 52.41±0.61 62.27±0.19 70.62±16.66 \ngpt-oss-20B \n82.64±0.46 78.47±0.75 84.91±0.19 84.63±0.41 82.95±0.61 81.27±0.27 \n85.03±0.34 \n84.74±0.23 68.96±0.52 33.72±0.68 61.98±0.66 69.13±0.62 74.87±14.57 \ngpt-oss-120B \n86.91±0.25 88.00±0.12 88.48±0.21 88.13±0.20 86.81±0.13 85.46±0.37 \n88.13±0.28 \n88.26±0.33 77.86±0.41 52.62±0.84 68.26±0.44 76.32±0.65 81.27±10.69 \nLLaMA-3.1-8B \n56.79±0.93 74.27±0.94 63.01±0.65 62.34±0.50 50.87±0.46 42.49±0.63 \n62.70±0.85 \n62.16±1.07 37.13±0.65 23.72±0.59 19.75±0.84 14.36±0.68 47.47±19.08 \nLLaMA-3.1-70B \n78.36±0.44 85.67±0.35 84.26±0.62 83.43±0.63 77.57±1.15 67.09±0.72 \n83.76±0.36 \n84.28±0.39 70.40±0.56 41.28±0.68 45.09±0.50 49.39±0.51 70.88±16.01 \nLLaMA-3.2-3B \n49.20±0.79 64.07±0.52 53.22±1.29 49.62±0.68 41.37±0.25 31.63±0.97 \n50.68±0.68 \n43.97±1.11 33.28±0.60 14.19±0.69 18.89±0.78 17.28±0.52 38.95±15.40 \nLLaMA-3.3-70B \n61.85±1.13 86.28±0.33 85.89±0.28 84.91±0.27 65.94±0.74 76.32±0.35 \n85.67±0.42 \n85.93±0.24 72.72±0.36 41.81±0.51 47.93±0.52 51.22±0.34 70.54±15.95 \nLLaMA-4-Scout \n84.12±0.26 87.10±0.27 86.31±0.23 86.83±0.16 84.23±0.22 82.29±0.27 \n86.43±0.35 \n87.00±0.32 77.98±0.31 50.25±0.78 50.64±0.15 69.34±0.27 77.71±13.24 \nLLaMA-4-Maverick \n87.73±0.24 89.55±0.15 90.02±0.21 89.57±0.14 87.02±0.32 86.00±0.35 \n89.98±0.17 \n90.05±0.16 84.15±0.30 54.47±0.27 69.27±0.58 77.79±0.32 82.97±10.55 \nMistral-7B-v0.3 \n28.39±0.73 36.61±0.44 21.04±0.80 40.89±1.09 24.09±0.59 27.67±0.66 \n30.07±0.56 \n37.35±0.91 18.13±0.24 14.69±0.66 10.40±0.50 9.78±0.79 \n24.93±10.10 \nMistral-Small-3.1-24B \n76.22±0.81 83.46±0.24 80.23±0.47 80.87±0.55 74.67±0.75 68.63±1.04 \n81.02±0.55 \n81.13±0.68 38.85±0.36 11.82±1.19 13.73±0.47 20.57±0.44 59.27±28.07 \nPhi-4-mini \n42.01±0.90 71.17±0.59 52.71±0.88 55.27±1.11 39.40±0.81 31.97±0.78 \n50.59±1.09 \n50.39±0.79 30.29±0.29 14.39±0.76 19.51±0.48 17.02±0.91 39.56±16.87 \nPhi-4-mini-Reasoning \n36.42±0.55 74.05±0.48 52.82±1.02 60.22±0.66 26.83±1.00 14.56±0.55 \n49.83±0.52 \n44.06±0.47 28.71±0.83 20.84±0.93 18.50±0.55 19.80±0.89 37.22±18.31 \nPhi-4 \n70.17±0.66 84.60±0.55 81.31±0.15 81.12±0.29 72.99±0.38 66.18±0.82 \n80.93±0.59 \n80.43±0.24 52.61±0.45 23.43±0.53 38.22±0.80 34.67±0.70 63.89±20.59 \nPhi-4-Reasoning \n79.89±0.71 85.50±0.39 84.47±0.13 84.67±0.22 83.64±0.34 79.93±0.58 \n78.34±0.36 \n85.09±0.64 71.75±0.76 17.47±0.55 48.91±0.69 37.77±0.61 69.79±21.77 \nQwen2.5-3B \n61.05±0.53 66.95±0.35 58.48±0.58 55.81±0.68 50.25±0.47 44.89±0.48 \n56.09±0.72 \n55.37±0.27 16.09±0.67 9.17±0.24 13.37±0.55 11.75±0.81 41.61±21.36 \nQwen2.5-7B \n71.29±0.31 74.45±0.57 68.09±0.20 65.88±0.83 63.10±0.50 57.78±0.58 \n66.76±0.12 \n66.51±0.55 32.25±0.59 9.13±0.44 24.63±0.58 18.55±0.66 51.54±22.57 \nQwen2.5-14B \n77.17±0.72 80.30±0.33 76.86±0.37 75.79±0.56 72.86±0.31 68.43±0.35 \n75.99±0.49 \n75.73±0.54 37.04±0.25 26.76±0.76 35.73±0.46 35.38±0.67 61.50±20.13 \nQwen2.5-72B \n83.21±0.33 85.62±0.33 84.48±0.19 84.17±0.09 81.56±0.26 78.96±0.38 \n85.10±0.33 \n84.97±0.37 54.45±0.67 43.90±0.23 41.74±0.43 43.98±0.32 71.01±18.14 \nQwQ-32B \n85.03±0.33 86.24±0.36 86.57±0.45 85.38±0.20 82.29±0.26 79.60±0.73 \n86.68±0.30 \n86.00±0.12 62.21±0.46 23.75±0.67 29.49±1.19 29.87±0.77 68.59±24.72 \nQwen3-1.7B \n57.75±0.84 65.09±0.52 55.49±0.62 52.98±0.72 45.46±0.65 42.18±0.44 \n52.99±0.51 \n52.82±0.50 23.75±0.59 22.52±0.38 26.30±0.85 23.17±0.88 43.38±14.92 \nQwen3-4B \n72.91±0.56 76.47±0.37 70.40±0.15 70.43±0.35 66.57±0.97 61.10±1.07 \n70.00±0.54 \n69.67±0.39 19.33±0.91 14.16±0.41 16.25±0.53 9.66±0.83 \n51.41±26.39 \nQwen3-4B-thinking \n76.64±0.28 80.40±0.43 78.85±0.46 78.31±0.65 75.53±0.34 72.08±0.50 \n77.34±0.70 \n78.18±0.48 29.54±0.79 17.14±1.02 13.21±0.65 8.99±0.98 \n57.18±28.92 \nQwen3-8B \n76.95±0.56 80.75±0.42 75.54±0.11 76.37±0.49 72.29±0.25 67.65±0.70 \n75.36±0.67 \n75.22±0.37 31.28±0.81 10.53±0.50 22.73±0.61 11.42±0.82 56.34±27.26 \nQwen3-8B-thinking \n81.15±0.36 83.88±0.22 82.17±0.40 82.52±0.48 79.96±0.42 78.42±0.65 \n81.90±0.16 \n82.18±0.30 36.87±0.69 14.73±0.69 17.74±0.43 12.06±0.74 61.13±29.66 \nQwen3-14B \n80.88±0.51 83.93±0.27 80.96±0.51 80.53±0.32 76.98±0.53 73.88±0.71 \n80.59±0.43 \n80.92±0.71 46.86±0.89 15.01±0.25 24.17±1.02 27.17±0.95 62.66±25.53 \nQwen3-14B-thinking \n83.59±0.31 86.03±0.34 84.71±0.29 84.81±0.33 83.42±0.30 80.83±0.49 \n84.66±0.40 \n85.02±0.37 64.36±0.73 17.63±0.63 24.89±0.79 27.53±0.62 67.29±26.27 \nBaichuan-M2-32B \n84.76±0.28 86.10±0.28 83.98±0.30 82.91±0.52 81.52±0.44 78.42±0.66 \n83.88±0.67 \n83.72±0.21 34.33±0.87 25.98±0.47 33.35±0.87 29.88±0.61 65.74±25.00 \nBio-Medical-LLaMA-3-8B \n50.07±0.36 69.06±0.45 59.37±0.26 58.85±0.34 49.40±0.41 44.72±0.42 \n57.13±0.13 \n54.77±0.47 38.93±0.49 35.79±0.46 34.01±0.47 33.82±0.55 48.83±11.12 \nMediPhi \n38.13±0.66 68.04±0.32 62.01±0.55 61.21±0.33 31.25±1.05 32.76±0.35 \n57.91±0.32 \n53.80±0.52 16.53±0.63 14.77±0.78 15.83±0.90 17.52±0.70 39.15±19.82 \nMedGemma-4B \n55.00±0.66 69.39±0.50 63.51±0.68 61.79±0.91 55.02±0.42 50.91±1.44 \n62.94±0.47 \n62.57±0.61 45.48±0.32 20.66±0.57 21.98±0.68 30.95±0.75 50.02±16.23 \nMedGemma-27B \n82.20±0.33 86.11±0.38 84.85±0.47 84.64±0.37 81.91±0.20 80.72±0.73 \n85.48±0.28 \n85.96±0.23 77.75±0.32 18.04±0.24 50.94±0.34 65.88±0.77 73.71±19.68 \nMedReason-8B \n47.48±0.29 63.47±0.87 18.40±0.90 14.44±0.35 48.26±0.44 41.14±0.79 \n26.60±0.31 \n34.88±0.93 12.92±0.15 8.11±0.28 13.19±0.53 11.66±0.29 28.38±17.60 \nHuatuoGPT-o1-7B \n73.80±0.39 69.96±0.56 72.11±0.52 71.80±0.67 65.12±0.63 62.83±0.55 \n72.32±0.26 \n70.88±0.17 7.32±0.42 \n7.04±0.30 \n7.19±0.40 \n6.26±0.42 \n48.89±30.05 \nHuatuoGPT-o1-8B \n64.48±0.74 73.33±0.34 70.71±0.36 70.50±0.82 62.24±0.27 54.89±0.25 \n69.30±0.54 \n69.15±0.47 50.34±0.27 9.62±0.49 \n9.04±0.56 \n8.46±0.51 \n51.01±25.28 \nHuatuoGPT-o1-70B \n72.87±0.53 85.19±0.23 84.22±0.23 84.27±0.36 72.69±0.32 77.43±0.46 \n84.77±0.46 \n84.83±0.26 74.90±0.38 43.44±0.95 50.15±0.59 56.66±0.82 72.62±14.16 \nHuatuoGPT-o1-72B \n84.34±0.08 85.54±0.18 86.20±0.22 84.46±0.48 83.03±0.39 78.55±0.61 \n86.84±0.21 \n86.85±0.33 62.18±0.44 44.44±0.61 44.91±1.39 29.51±0.71 71.41±19.94 \nOpenBioLLM-8B \n22.73±0.63 35.28±0.59 24.55±0.71 32.01±0.36 8.99±0.34 12.06±0.54 \n34.79±0.97 \n35.48±1.40 11.49±0.82 6.95±0.21 10.10±0.76 11.10±0.75 20.46±11.17 \nOpenBioLLM-70B \n31.62±0.98 73.19±0.85 74.86±0.39 52.14±0.98 32.53±0.58 31.09±0.66 \n58.81±0.64 \n70.78±0.41 31.04±0.91 10.13±0.36 11.79±0.56 23.73±0.75 41.81±22.49 \nSTab. 99: Performance evaluation of 56 LLMs on HeadQA. \n"}, {"page": 92, "text": " \n \n62 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n76.92 \n83.14 \n82.78 \n81.83 \n79.40 \n75.02 \n83.32 \n83.00 \n59.29 \n19.43 \n42.56 \n42.88 \nClaude-4.0-Sonnet \n88.28 \n89.50 \n90.08 \n89.18 \n88.77 \n87.56 \n90.17 \n90.22 \n79.94 \n55.09 \n70.74 \n75.92 \nGemini-2.5-Flash \n89.04 \n88.59 \n90.22 \n89.72 \n88.73 \n89.18 \n90.49 \n90.98 \n87.96 \n78.54 \n79.71 \n83.81 \nGPT-4o-mini \n76.87 \n83.72 \n82.37 \n82.19 \n78.18 \n71.96 \n82.64 \n82.19 \n69.39 \n35.66 \n48.87 \n57.75 \nGPT-4o \n87.06 \n88.10 \n89.09 \n88.68 \n87.92 \n85.12 \n88.82 \n89.22 \n84.22 \n38.10 \n61.63 \n73.62 \nGPT-4.1-nano \n77.05 \n84.72 \n81.51 \n81.51 \n75.07 \n72.50 \n81.79 \n81.51 \n59.83 \n32.64 \n45.31 \n53.20 \nGPT-4.1-mini \n86.52 \n88.68 \n88.05 \n87.92 \n86.43 \n83.41 \n88.73 \n88.32 \n77.37 \n28.00 \n61.59 \n72.86 \nGPT-4.1 \n89.09 \n90.22 \n90.26 \n89.50 \n89.22 \n86.20 \n90.53 \n89.18 \n86.61 \n59.38 \n67.13 \n78.22 \nGPT-5-nano \n67.49 \n80.84 \n77.64 \n76.47 \n73.72 \n67.40 \n75.56 \n78.09 \n56.67 \n17.27 \n40.98 \n50.63 \nGPT-5-mini \n85.48 \n88.77 \n87.69 \n87.02 \n86.20 \n82.64 \n88.10 \n88.59 \n77.68 \n33.72 \n59.33 \n73.94 \nGPT-5 \n88.23 \n90.85 \n90.31 \n89.36 \n88.95 \n86.20 \n90.62 \n90.89 \n86.61 \n57.53 \n71.37 \n79.89 \no4-mini \n88.91 \n90.17 \n90.13 \n90.35 \n90.35 \n87.65 \n91.03 \n90.44 \n87.33 \n48.77 \n76.28 \n82.60 \nOpen-Weight LLMs \nDeepSeek-V3 \n87.60 \n89.63 \n88.37 \n88.59 \n86.97 \n83.32 \n88.32 \n89.09 \n77.10 \n51.35 \n58.57 \n64.70 \nDeepSeek-R1 \n88.28 \n89.72 \n89.59 \n88.95 \n87.51 \n86.93 \n89.77 \n89.90 \n84.17 \n65.42 \n70.51 \n77.23 \nDeepSeek-R1-Qwen3-8B \n79.94 \n82.82 \n78.36 \n76.56 \n76.19 \n74.21 \n78.13 \n78.49 \n38.10 \n11.27 \n10.69 \n12.76 \nGemma-3-4B \n52.52 \n64.20 \n59.74 \n57.75 \n54.28 \n48.11 \n60.14 \n59.51 \n41.84 \n12.98 \n25.88 \n20.20 \nGemma-3-12B \n70.78 \n78.09 \n76.24 \n75.97 \n71.24 \n69.75 \n75.07 \n75.70 \n62.08 \n11.18 \n44.14 \n55.14 \nGemma-3-27B \n77.64 \n82.28 \n80.52 \n80.30 \n78.18 \n75.70 \n80.93 \n81.24 \n72.09 \n24.44 \n52.34 \n62.13 \ngpt-oss-20B \n82.28 \n78.40 \n84.99 \n84.36 \n83.72 \n81.33 \n85.17 \n84.45 \n69.21 \n32.91 \n62.76 \n68.94 \ngpt-oss-120B \n86.52 \n88.01 \n88.41 \n88.41 \n87.02 \n85.17 \n88.19 \n88.37 \n78.00 \n53.47 \n68.35 \n75.38 \nLLaMA-3.1-8B \n57.26 \n73.22 \n61.95 \n62.13 \n50.50 \n42.88 \n62.62 \n62.71 \n37.06 \n24.17 \n18.98 \n13.80 \nLLaMA-3.1-70B \n79.08 \n86.11 \n83.32 \n83.14 \n76.51 \n67.94 \n84.27 \n83.68 \n69.66 \n40.62 \n45.45 \n49.05 \nLLaMA-3.2-3B \n47.88 \n64.88 \n51.35 \n49.95 \n41.16 \n30.88 \n51.44 \n42.70 \n32.46 \n14.56 \n19.07 \n17.49 \nLLaMA-3.3-70B \n60.64 \n86.74 \n86.16 \n84.81 \n65.78 \n76.74 \n85.66 \n85.89 \n73.08 \n41.34 \n48.60 \n51.08 \nLLaMA-4-Scout \n83.72 \n87.51 \n86.20 \n86.88 \n83.90 \n82.37 \n86.25 \n86.93 \n78.40 \n49.19 \n50.41 \n69.70 \nLLaMA-4-Maverick \n87.87 \n89.63 \n90.31 \n89.54 \n87.29 \n85.53 \n89.77 \n90.04 \n84.27 \n54.73 \n69.88 \n77.86 \nMistral-7B-v0.3 \n28.67 \n36.47 \n22.14 \n40.13 \n23.35 \n27.23 \n30.61 \n37.24 \n18.21 \n14.83 \n10.10 \n10.19 \nMistral-Small-3.1-24B \n75.02 \n83.05 \n79.44 \n81.47 \n73.72 \n68.03 \n81.24 \n81.15 \n38.77 \n10.82 \n13.84 \n20.15 \nPhi-4-mini \n42.70 \n70.20 \n52.12 \n53.38 \n39.00 \n32.69 \n51.71 \n50.05 \n30.61 \n13.80 \n20.15 \n16.32 \nPhi-4-mini-Reasoning \n35.80 \n74.44 \n54.01 \n59.24 \n25.70 \n14.20 \n49.95 \n43.78 \n29.17 \n19.75 \n18.03 \n20.87 \nPhi-4 \n70.74 \n84.22 \n81.38 \n81.02 \n72.50 \n66.23 \n81.65 \n80.57 \n52.52 \n24.21 \n38.82 \n33.86 \nPhi-4-Reasoning \n78.81 \n86.02 \n84.45 \n84.76 \n83.41 \n80.84 \n78.04 \n84.04 \n72.09 \n16.86 \n49.73 \n38.05 \nQwen2.5-3B \n61.00 \n66.95 \n57.98 \n54.87 \n50.63 \n45.18 \n55.86 \n55.28 \n17.09 \n8.75 \n13.39 \n11.68 \nQwen2.5-7B \n71.33 \n74.39 \n67.94 \n66.73 \n63.44 \n58.03 \n66.73 \n66.46 \n32.69 \n9.42 \n24.35 \n18.94 \nQwen2.5-14B \n78.36 \n79.98 \n76.60 \n75.88 \n73.08 \n67.85 \n76.65 \n75.11 \n36.79 \n26.56 \n35.17 \n34.90 \nQwen2.5-72B \n82.69 \n85.75 \n84.17 \n84.13 \n81.79 \n78.58 \n85.21 \n85.26 \n54.96 \n44.18 \n41.79 \n44.41 \nQwQ-32B \n85.44 \n86.02 \n86.70 \n85.30 \n82.24 \n80.57 \n86.88 \n86.02 \n61.86 \n24.03 \n30.66 \n30.03 \nQwen3-1.7B \n57.39 \n65.15 \n54.73 \n52.21 \n44.72 \n42.06 \n53.11 \n52.98 \n23.62 \n22.45 \n26.38 \n22.09 \nQwen3-4B \n72.72 \n76.56 \n70.24 \n70.65 \n67.45 \n62.53 \n69.48 \n69.16 \n18.94 \n14.52 \n16.14 \n10.37 \nQwen3-4B-thinking \n77.05 \n80.66 \n78.67 \n79.35 \n75.56 \n71.69 \n77.01 \n78.18 \n28.36 \n18.03 \n12.53 \n8.93 \nQwen3-8B \n76.92 \n80.39 \n75.52 \n77.19 \n72.27 \n67.40 \n75.11 \n74.80 \n32.42 \n11.00 \n22.99 \n11.18 \nQwen3-8B-thinking \n80.88 \n83.99 \n82.24 \n81.92 \n79.94 \n79.08 \n81.74 \n82.06 \n36.02 \n14.61 \n18.12 \n11.27 \nQwen3-14B \n81.47 \n83.59 \n80.61 \n80.79 \n76.47 \n73.94 \n80.25 \n79.71 \n45.31 \n14.70 \n22.99 \n28.36 \nQwen3-14B-thinking \n83.90 \n86.61 \n84.58 \n84.67 \n83.09 \n80.79 \n84.45 \n85.48 \n64.83 \n18.03 \n24.89 \n27.37 \nBaichuan-M2-32B \n84.63 \n86.20 \n83.63 \n83.36 \n81.38 \n77.73 \n84.08 \n83.95 \n34.85 \n26.65 \n32.28 \n30.88 \nBio-Medical-LLaMA-3-8B \n50.50 \n69.03 \n59.69 \n59.33 \n49.37 \n44.50 \n56.94 \n55.41 \n38.14 \n35.57 \n33.54 \n34.40 \nMediPhi \n38.28 \n68.17 \n61.14 \n61.18 \n32.78 \n32.82 \n57.75 \n54.19 \n16.23 \n14.43 \n14.65 \n17.45 \nMedGemma-4B \n55.23 \n69.57 \n63.71 \n61.00 \n54.69 \n50.36 \n63.17 \n62.67 \n45.18 \n20.20 \n21.46 \n29.94 \nMedGemma-27B \n82.64 \n85.66 \n84.85 \n84.17 \n81.79 \n80.84 \n85.08 \n86.02 \n78.00 \n18.21 \n50.50 \n66.41 \nMedReason-8B \n47.29 \n62.31 \n18.98 \n14.02 \n48.78 \n41.16 \n26.51 \n35.48 \n13.03 \n7.62 \n12.44 \n11.54 \nHuatuoGPT-o1-7B \n73.17 \n70.33 \n72.59 \n71.78 \n65.64 \n62.17 \n72.54 \n70.83 \n7.71 \n7.08 \n7.53 \n6.40 \nHuatuoGPT-o1-8B \n63.48 \n73.72 \n71.06 \n69.52 \n62.08 \n55.05 \n69.30 \n69.12 \n50.63 \n9.65 \n9.11 \n8.34 \nHuatuoGPT-o1-70B \n72.36 \n85.44 \n84.13 \n84.45 \n72.77 \n77.46 \n85.48 \n84.58 \n74.93 \n42.92 \n50.23 \n56.94 \nHuatuoGPT-o1-72B \n84.27 \n85.48 \n85.93 \n84.76 \n82.60 \n78.76 \n87.11 \n86.52 \n61.54 \n44.77 \n45.22 \n30.52 \nOpenBioLLM-8B \n22.36 \n35.84 \n25.43 \n31.88 \n8.84 \n11.36 \n34.36 \n35.12 \n12.62 \n6.58 \n8.79 \n10.78 \nOpenBioLLM-70B \n30.12 \n74.21 \n74.57 \n52.48 \n32.37 \n32.15 \n57.75 \n70.51 \n31.92 \n10.14 \n11.68 \n23.08 \nSTab. 100: Zero-Shot performance evaluation of 56 LLMs on HeadQA (Run 1). \n"}, {"page": 93, "text": " \n \n63 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n76.96 \n83.14 \n82.73 \n81.83 \n79.35 \n74.98 \n83.36 \n83.09 \n59.29 \n19.57 \n42.56 \n42.88 \nClaude-4.0-Sonnet \n87.51 \n89.63 \n89.95 \n89.40 \n88.28 \n87.96 \n90.22 \n90.89 \n80.75 \n55.86 \n70.96 \n75.43 \nGemini-2.5-Flash \n88.91 \n88.55 \n90.85 \n89.81 \n88.86 \n89.18 \n90.31 \n90.94 \n88.05 \n78.04 \n81.24 \n83.59 \nGPT-4o-mini \n76.83 \n83.36 \n83.32 \n82.06 \n77.73 \n72.59 \n82.28 \n81.97 \n69.97 \n35.84 \n48.20 \n57.57 \nGPT-4o \n86.47 \n87.65 \n89.00 \n88.01 \n87.42 \n84.90 \n88.55 \n89.04 \n83.81 \n36.79 \n61.14 \n73.94 \nGPT-4.1-nano \n75.88 \n84.58 \n82.19 \n81.51 \n75.74 \n73.76 \n82.55 \n80.39 \n59.42 \n34.58 \n45.09 \n53.56 \nGPT-4.1-mini \n86.34 \n88.46 \n87.60 \n87.60 \n86.52 \n83.05 \n88.77 \n88.19 \n77.32 \n26.96 \n61.81 \n72.86 \nGPT-4.1 \n88.86 \n90.22 \n89.95 \n89.95 \n89.31 \n85.44 \n90.53 \n89.22 \n87.33 \n58.16 \n67.67 \n79.13 \nGPT-5-nano \n68.30 \n80.16 \n78.36 \n77.64 \n74.21 \n67.13 \n75.34 \n77.23 \n57.26 \n16.77 \n41.75 \n50.95 \nGPT-5-mini \n85.30 \n88.05 \n87.74 \n87.02 \n86.29 \n82.73 \n87.87 \n88.32 \n77.46 \n33.81 \n59.38 \n72.68 \nGPT-5 \n88.19 \n90.53 \n89.86 \n89.81 \n89.22 \n85.35 \n90.71 \n90.80 \n86.56 \n57.57 \n71.78 \n78.81 \no4-mini \n89.00 \n90.13 \n90.17 \n89.59 \n89.45 \n87.02 \n90.80 \n90.49 \n86.38 \n45.76 \n76.15 \n82.73 \nOpen-Weight LLMs \nDeepSeek-V3 \n87.74 \n89.54 \n88.23 \n88.95 \n87.69 \n84.27 \n88.32 \n89.04 \n76.06 \n51.17 \n58.57 \n64.16 \nDeepSeek-R1 \n88.28 \n89.68 \n89.54 \n89.36 \n88.32 \n86.79 \n90.04 \n89.72 \n84.90 \n65.60 \n69.75 \n77.55 \nDeepSeek-R1-Qwen3-8B \n78.72 \n82.82 \n77.55 \n77.91 \n75.83 \n73.40 \n78.40 \n78.72 \n36.29 \n11.27 \n11.05 \n13.44 \nGemma-3-4B \n53.43 \n65.19 \n59.20 \n58.30 \n53.56 \n48.65 \n60.05 \n59.24 \n43.73 \n13.57 \n25.34 \n19.30 \nGemma-3-12B \n71.37 \n78.27 \n76.33 \n75.34 \n73.04 \n68.58 \n75.25 \n75.34 \n62.62 \n10.82 \n41.97 \n53.20 \nGemma-3-27B \n76.87 \n82.24 \n81.51 \n80.97 \n77.41 \n77.19 \n80.52 \n80.79 \n70.38 \n24.12 \n53.34 \n62.22 \ngpt-oss-20B \n82.64 \n77.32 \n84.67 \n85.12 \n82.87 \n80.84 \n85.53 \n84.85 \n69.66 \n34.67 \n62.26 \n68.62 \ngpt-oss-120B \n87.11 \n87.87 \n88.59 \n87.87 \n86.83 \n85.57 \n88.55 \n88.28 \n77.14 \n52.30 \n67.94 \n77.05 \nLLaMA-3.1-8B \n57.53 \n75.61 \n63.66 \n61.63 \n50.81 \n41.75 \n64.16 \n63.17 \n36.88 \n22.77 \n19.21 \n15.42 \nLLaMA-3.1-70B \n77.95 \n85.80 \n84.49 \n83.27 \n77.91 \n67.40 \n83.63 \n84.72 \n70.74 \n40.71 \n44.68 \n49.10 \nLLaMA-3.2-3B \n49.41 \n64.20 \n54.33 \n49.41 \n41.34 \n31.42 \n50.41 \n45.40 \n33.59 \n13.57 \n18.39 \n17.76 \nLLaMA-3.3-70B \n63.44 \n86.20 \n85.84 \n85.30 \n66.14 \n76.28 \n85.08 \n85.71 \n72.81 \n41.79 \n48.29 \n51.17 \nLLaMA-4-Scout \n84.31 \n86.93 \n86.43 \n86.97 \n84.36 \n82.46 \n85.98 \n86.52 \n78.04 \n50.18 \n50.63 \n69.21 \nLLaMA-4-Maverick \n87.60 \n89.31 \n89.95 \n89.77 \n86.74 \n86.29 \n90.17 \n89.81 \n84.13 \n54.01 \n69.07 \n77.37 \nMistral-7B-v0.3 \n28.36 \n37.06 \n21.01 \n39.81 \n24.84 \n28.27 \n29.53 \n37.78 \n17.94 \n14.56 \n10.78 \n10.41 \nMistral-Small-3.1-24B \n76.56 \n83.59 \n80.30 \n81.47 \n75.11 \n70.38 \n81.79 \n81.56 \n38.46 \n13.53 \n14.16 \n21.19 \nPhi-4-mini \n41.84 \n71.15 \n53.65 \n55.55 \n38.19 \n31.20 \n49.82 \n51.53 \n30.57 \n15.15 \n19.84 \n16.95 \nPhi-4-mini-Reasoning \n36.11 \n73.67 \n52.25 \n60.10 \n27.37 \n13.89 \n50.45 \n43.64 \n27.82 \n21.55 \n18.67 \n19.79 \nPhi-4 \n70.11 \n85.57 \n81.47 \n81.11 \n73.22 \n66.73 \n81.24 \n80.34 \n52.80 \n23.17 \n38.55 \n34.13 \nPhi-4-Reasoning \n79.89 \n84.94 \n84.49 \n84.31 \n83.50 \n80.03 \n78.13 \n84.99 \n72.23 \n17.18 \n49.01 \n38.64 \nQwen2.5-3B \n61.41 \n67.04 \n58.16 \n56.54 \n50.81 \n44.50 \n55.77 \n55.59 \n15.46 \n9.33 \n13.53 \n11.36 \nQwen2.5-7B \n70.92 \n73.76 \n68.35 \n65.96 \n63.12 \n57.62 \n66.77 \n66.10 \n32.46 \n9.20 \n24.84 \n19.03 \nQwen2.5-14B \n76.51 \n80.66 \n76.60 \n76.69 \n72.90 \n68.49 \n76.10 \n75.65 \n37.38 \n27.28 \n36.20 \n35.08 \nQwen2.5-72B \n83.59 \n85.57 \n84.49 \n84.22 \n81.70 \n79.31 \n84.63 \n85.44 \n55.23 \n43.78 \n41.12 \n43.91 \nQwQ-32B \n84.81 \n86.02 \n87.11 \n85.53 \n82.10 \n79.35 \n87.11 \n85.93 \n62.62 \n23.26 \n27.50 \n30.43 \nQwen3-1.7B \n57.57 \n64.74 \n55.91 \n53.74 \n46.39 \n42.92 \n53.34 \n53.61 \n22.77 \n23.13 \n26.96 \n23.99 \nQwen3-4B \n73.31 \n76.83 \n70.65 \n70.47 \n66.46 \n61.77 \n70.65 \n69.57 \n19.88 \n13.53 \n15.55 \n10.14 \nQwen3-4B-thinking \n76.51 \n79.80 \n79.35 \n77.95 \n75.02 \n72.72 \n76.69 \n78.72 \n30.03 \n16.95 \n13.84 \n7.66 \nQwen3-8B \n77.41 \n81.02 \n75.70 \n76.38 \n72.05 \n68.39 \n75.02 \n74.84 \n31.83 \n10.37 \n23.62 \n12.04 \nQwen3-8B-thinking \n81.61 \n83.68 \n81.47 \n83.23 \n80.61 \n78.09 \n82.01 \n82.42 \n36.93 \n14.29 \n17.94 \n12.62 \nQwen3-14B \n80.34 \n84.13 \n80.84 \n80.16 \n77.50 \n74.93 \n80.93 \n81.56 \n47.29 \n15.24 \n24.71 \n26.28 \nQwen3-14B-thinking \n83.54 \n86.02 \n84.85 \n84.90 \n83.72 \n81.24 \n84.81 \n84.49 \n64.16 \n17.63 \n25.25 \n27.64 \nBaichuan-M2-32B \n84.72 \n86.11 \n83.81 \n82.60 \n81.15 \n78.40 \n83.23 \n83.45 \n32.87 \n25.97 \n33.41 \n29.49 \nBio-Medical-LLaMA-3-8B \n50.32 \n69.52 \n59.02 \n58.57 \n48.87 \n44.54 \n57.12 \n54.42 \n38.91 \n35.84 \n33.59 \n33.54 \nMediPhi \n38.10 \n67.94 \n62.35 \n61.09 \n30.21 \n32.33 \n58.25 \n54.28 \n16.01 \n14.20 \n15.73 \n16.68 \nMedGemma-4B \n54.55 \n68.53 \n63.35 \n61.27 \n54.46 \n49.28 \n62.85 \n62.35 \n45.49 \n20.20 \n21.06 \n31.51 \nMedGemma-27B \n82.24 \n86.38 \n84.22 \n84.63 \n81.74 \n79.80 \n85.57 \n86.20 \n77.50 \n17.72 \n51.08 \n65.01 \nMedReason-8B \n47.20 \n62.89 \n17.49 \n14.83 \n47.66 \n39.81 \n26.47 \n35.89 \n13.03 \n8.12 \n13.66 \n12.13 \nHuatuoGPT-o1-7B \n74.17 \n69.30 \n71.24 \n71.55 \n65.33 \n62.35 \n72.36 \n70.78 \n6.67 \n6.90 \n6.76 \n5.95 \nHuatuoGPT-o1-8B \n64.29 \n73.35 \n71.10 \n70.06 \n62.13 \n54.91 \n69.39 \n69.12 \n49.91 \n9.02 \n8.48 \n8.84 \nHuatuoGPT-o1-70B \n72.86 \n85.39 \n84.36 \n84.17 \n72.95 \n77.55 \n84.40 \n84.54 \n74.75 \n43.10 \n49.50 \n56.54 \nHuatuoGPT-o1-72B \n84.31 \n85.30 \n86.07 \n83.95 \n83.09 \n78.58 \n86.65 \n86.47 \n62.31 \n43.60 \n43.19 \n29.35 \nOpenBioLLM-8B \n22.45 \n35.84 \n23.53 \n31.70 \n8.66 \n12.04 \n34.13 \n37.11 \n11.36 \n6.99 \n10.28 \n10.10 \nOpenBioLLM-70B \n31.24 \n72.23 \n75.29 \n51.04 \n32.78 \n30.52 \n59.11 \n71.10 \n30.75 \n9.60 \n11.72 \n23.62 \nSTab. 101: Zero-Shot performance evaluation of 56 LLMs on HeadQA (Run 2). \n"}, {"page": 94, "text": " \n \n64 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n77.01 \n83.14 \n82.82 \n81.79 \n79.35 \n75.02 \n83.41 \n83.05 \n59.29 \n18.67 \n42.34 \n43.01 \nClaude-4.0-Sonnet \n87.74 \n89.40 \n90.13 \n89.27 \n88.37 \n88.37 \n90.80 \n90.71 \n80.52 \n55.59 \n71.15 \n76.47 \nGemini-2.5-Flash \n88.68 \n87.92 \n90.26 \n89.36 \n89.09 \n88.91 \n90.08 \n90.22 \n87.96 \n78.63 \n80.52 \n83.23 \nGPT-4o-mini \n77.05 \n82.96 \n82.87 \n82.15 \n78.18 \n72.59 \n81.88 \n82.60 \n68.89 \n36.47 \n48.24 \n58.66 \nGPT-4o \n87.24 \n87.92 \n89.22 \n88.05 \n87.56 \n85.08 \n89.09 \n89.27 \n83.77 \n37.24 \n61.41 \n73.85 \nGPT-4.1-nano \n76.69 \n84.76 \n82.64 \n79.89 \n74.12 \n72.27 \n82.19 \n81.24 \n58.52 \n33.63 \n43.37 \n54.10 \nGPT-4.1-mini \n86.11 \n88.73 \n87.87 \n87.47 \n86.34 \n82.82 \n88.68 \n88.41 \n77.46 \n28.18 \n61.95 \n72.45 \nGPT-4.1 \n88.28 \n89.90 \n90.35 \n89.63 \n89.77 \n86.25 \n90.62 \n89.18 \n86.20 \n58.48 \n67.45 \n78.36 \nGPT-5-nano \n67.13 \n81.20 \n78.45 \n77.68 \n73.81 \n66.14 \n74.84 \n76.69 \n59.29 \n17.72 \n41.39 \n49.82 \nGPT-5-mini \n85.62 \n88.86 \n87.74 \n87.92 \n87.15 \n82.96 \n88.01 \n88.01 \n77.77 \n33.77 \n59.02 \n72.95 \nGPT-5 \n88.23 \n90.49 \n89.77 \n89.59 \n89.09 \n86.11 \n90.17 \n90.76 \n86.93 \n57.44 \n71.96 \n79.53 \no4-mini \n89.09 \n90.53 \n90.35 \n90.04 \n89.63 \n86.52 \n90.67 \n90.80 \n86.74 \n47.39 \n76.10 \n82.10 \nOpen-Weight LLMs \nDeepSeek-V3 \n87.74 \n89.59 \n88.91 \n88.95 \n87.65 \n84.22 \n88.50 \n88.91 \n77.41 \n50.41 \n57.53 \n64.92 \nDeepSeek-R1 \n88.32 \n89.50 \n89.68 \n89.40 \n87.78 \n86.56 \n90.13 \n90.40 \n83.68 \n66.23 \n69.61 \n77.95 \nDeepSeek-R1-Qwen3-8B \n79.67 \n82.42 \n77.59 \n77.91 \n75.25 \n74.35 \n78.54 \n78.81 \n37.15 \n11.18 \n10.05 \n13.93 \nGemma-3-4B \n52.84 \n64.11 \n60.10 \n57.66 \n53.65 \n48.20 \n59.42 \n60.23 \n43.55 \n13.12 \n24.62 \n20.38 \nGemma-3-12B \n71.01 \n78.54 \n75.07 \n75.79 \n72.05 \n69.66 \n74.48 \n75.43 \n63.07 \n11.90 \n42.74 \n54.46 \nGemma-3-27B \n76.51 \n82.33 \n81.51 \n80.70 \n78.09 \n76.92 \n81.29 \n81.70 \n71.46 \n23.94 \n51.62 \n62.53 \ngpt-oss-20B \n83.00 \n78.85 \n84.76 \n84.08 \n82.15 \n81.24 \n84.72 \n84.85 \n68.98 \n33.27 \n62.22 \n69.97 \ngpt-oss-120B \n87.11 \n88.19 \n88.77 \n88.05 \n86.70 \n85.03 \n88.10 \n88.19 \n77.95 \n51.40 \n67.76 \n76.01 \nLLaMA-3.1-8B \n56.00 \n74.71 \n62.89 \n62.40 \n51.40 \n43.33 \n62.49 \n60.50 \n37.11 \n24.26 \n19.25 \n13.75 \nLLaMA-3.1-70B \n78.36 \n85.75 \n84.72 \n84.40 \n78.04 \n67.31 \n83.77 \n84.31 \n70.87 \n42.29 \n45.76 \n49.10 \nLLaMA-3.2-3B \n49.14 \n63.48 \n54.55 \n49.19 \n41.79 \n30.61 \n50.59 \n44.54 \n33.45 \n13.80 \n18.94 \n17.09 \nLLaMA-3.3-70B \n60.96 \n86.47 \n86.20 \n84.85 \n66.77 \n76.33 \n85.48 \n85.71 \n72.32 \n41.93 \n47.84 \n50.90 \nLLaMA-4-Scout \n84.36 \n87.11 \n86.65 \n86.79 \n84.13 \n81.88 \n86.79 \n87.06 \n77.77 \n51.40 \n50.68 \n69.43 \nLLaMA-4-Maverick \n88.05 \n89.68 \n89.72 \n89.45 \n87.38 \n86.07 \n90.13 \n90.17 \n84.58 \n54.55 \n69.84 \n78.18 \nMistral-7B-v0.3 \n28.00 \n35.93 \n21.46 \n42.61 \n24.21 \n28.00 \n29.76 \n37.65 \n18.44 \n13.80 \n10.91 \n9.24 \nMistral-Small-3.1-24B \n76.69 \n83.63 \n80.30 \n80.48 \n74.17 \n68.71 \n81.02 \n81.79 \n38.59 \n12.40 \n13.80 \n20.78 \nPhi-4-mini \n40.98 \n71.73 \n53.38 \n56.04 \n40.13 \n32.55 \n50.23 \n50.59 \n30.16 \n15.19 \n18.94 \n16.46 \nPhi-4-mini-Reasoning \n36.74 \n74.66 \n51.44 \n60.14 \n28.09 \n14.56 \n49.01 \n44.18 \n29.80 \n21.82 \n17.94 \n20.02 \nPhi-4 \n69.75 \n84.54 \n81.15 \n81.29 \n72.86 \n66.86 \n80.61 \n80.25 \n53.29 \n23.40 \n38.64 \n35.57 \nPhi-4-Reasoning \n80.30 \n85.66 \n84.45 \n84.90 \n83.86 \n79.44 \n78.94 \n85.53 \n72.54 \n18.21 \n49.23 \n37.65 \nQwen2.5-3B \n60.69 \n66.37 \n58.61 \n55.50 \n50.23 \n44.50 \n55.91 \n55.37 \n15.55 \n9.24 \n13.66 \n12.80 \nQwen2.5-7B \n71.78 \n74.62 \n68.21 \n66.46 \n62.31 \n58.34 \n66.91 \n67.04 \n31.20 \n9.65 \n24.30 \n18.30 \nQwen2.5-14B \n76.87 \n80.07 \n77.37 \n75.43 \n72.32 \n68.76 \n75.83 \n75.56 \n36.83 \n27.64 \n36.20 \n35.98 \nQwen2.5-72B \n83.32 \n85.26 \n84.54 \n84.31 \n81.61 \n78.54 \n85.53 \n84.72 \n53.83 \n44.09 \n42.34 \n43.91 \nQwQ-32B \n84.94 \n86.11 \n86.74 \n85.66 \n82.73 \n79.67 \n86.52 \n85.84 \n62.08 \n24.80 \n29.53 \n30.57 \nQwen3-1.7B \n56.76 \n64.70 \n55.00 \n53.02 \n45.81 \n42.06 \n53.43 \n52.66 \n24.12 \n22.09 \n26.96 \n22.81 \nQwen3-4B \n72.45 \n76.78 \n70.42 \n69.84 \n66.41 \n60.78 \n69.66 \n69.88 \n20.56 \n14.47 \n16.01 \n9.24 \nQwen3-4B-thinking \n76.51 \n80.66 \n78.76 \n77.64 \n75.47 \n72.05 \n78.00 \n77.82 \n29.13 \n16.32 \n13.17 \n9.15 \nQwen3-8B \n77.50 \n80.34 \n75.38 \n75.92 \n72.05 \n68.39 \n74.57 \n75.47 \n30.88 \n10.28 \n22.72 \n12.08 \nQwen3-8B-thinking \n81.24 \n83.72 \n82.46 \n82.33 \n79.44 \n77.64 \n81.74 \n82.10 \n37.47 \n15.73 \n17.99 \n12.98 \nQwen3-14B \n80.61 \n84.13 \n81.70 \n80.25 \n76.78 \n74.08 \n80.16 \n81.15 \n47.43 \n15.28 \n24.26 \n26.42 \nQwen3-14B-thinking \n83.09 \n85.89 \n84.63 \n84.99 \n83.27 \n80.12 \n84.31 \n85.12 \n65.37 \n17.31 \n24.17 \n26.78 \nBaichuan-M2-32B \n85.26 \n85.66 \n83.86 \n83.50 \n81.38 \n79.22 \n84.94 \n83.72 \n34.94 \n25.52 \n34.36 \n30.07 \nBio-Medical-LLaMA-3-8B \n49.95 \n69.03 \n59.38 \n58.75 \n50.00 \n44.23 \n57.12 \n55.14 \n39.04 \n36.47 \n33.99 \n33.68 \nMediPhi \n37.33 \n67.54 \n62.53 \n61.23 \n31.83 \n33.09 \n58.25 \n53.47 \n16.41 \n14.02 \n15.92 \n18.62 \nMedGemma-4B \n55.68 \n69.66 \n64.07 \n63.03 \n55.32 \n51.98 \n62.22 \n63.07 \n45.94 \n21.19 \n22.32 \n31.83 \nMedGemma-27B \n82.37 \n85.75 \n85.44 \n85.21 \n81.88 \n81.02 \n85.84 \n85.98 \n77.77 \n18.17 \n50.72 \n65.15 \nMedReason-8B \n47.88 \n64.34 \n19.52 \n14.29 \n48.20 \n41.88 \n26.51 \n33.45 \n12.80 \n8.34 \n13.21 \n11.59 \nHuatuoGPT-o1-7B \n74.03 \n70.65 \n72.36 \n71.46 \n65.06 \n62.98 \n72.05 \n71.06 \n7.17 \n6.63 \n7.35 \n6.90 \nHuatuoGPT-o1-8B \n65.46 \n73.49 \n70.29 \n71.46 \n62.62 \n54.51 \n69.93 \n68.44 \n50.50 \n9.24 \n8.79 \n8.84 \nHuatuoGPT-o1-70B \n73.26 \n85.08 \n84.40 \n83.72 \n72.59 \n78.13 \n84.94 \n84.90 \n75.34 \n42.65 \n50.41 \n56.40 \nHuatuoGPT-o1-72B \n84.27 \n85.80 \n86.52 \n84.49 \n83.00 \n77.50 \n87.02 \n87.06 \n62.44 \n45.22 \n44.00 \n29.62 \nOpenBioLLM-8B \n23.13 \n34.63 \n24.89 \n31.70 \n8.75 \n12.17 \n34.76 \n36.61 \n11.95 \n7.12 \n10.32 \n12.08 \nOpenBioLLM-70B \n32.55 \n72.95 \n75.07 \n51.44 \n31.79 \n30.84 \n58.84 \n70.20 \n30.84 \n10.55 \n10.96 \n25.02 \nSTab. 102: Zero-Shot performance evaluation of 56 LLMs on HeadQA (Run 3). \n"}, {"page": 95, "text": " \n \n65 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n76.92 \n83.14 \n82.82 \n81.83 \n79.40 \n74.89 \n83.32 \n83.00 \n59.24 \n19.79 \n42.70 \n42.92 \nClaude-4.0-Sonnet \n87.65 \n89.77 \n89.77 \n89.40 \n88.46 \n87.69 \n90.22 \n90.98 \n80.52 \n54.96 \n70.74 \n76.10 \nGemini-2.5-Flash \n88.68 \n87.42 \n90.26 \n89.72 \n88.68 \n88.91 \n90.44 \n90.44 \n88.50 \n78.67 \n80.30 \n83.86 \nGPT-4o-mini \n76.87 \n83.81 \n82.73 \n81.92 \n78.63 \n72.09 \n82.28 \n82.15 \n69.25 \n35.35 \n49.32 \n58.30 \nGPT-4o \n86.83 \n87.83 \n88.64 \n88.23 \n87.83 \n84.99 \n88.68 \n89.13 \n83.77 \n37.51 \n61.09 \n73.22 \nGPT-4.1-nano \n76.78 \n84.27 \n81.38 \n80.48 \n74.48 \n73.04 \n82.19 \n81.29 \n58.66 \n33.27 \n45.49 \n53.34 \nGPT-4.1-mini \n85.75 \n88.41 \n87.47 \n87.83 \n86.97 \n83.36 \n89.00 \n88.37 \n77.82 \n26.60 \n61.27 \n72.45 \nGPT-4.1 \n88.23 \n90.04 \n90.04 \n89.86 \n89.31 \n86.74 \n90.58 \n89.40 \n86.47 \n58.30 \n67.85 \n78.72 \nGPT-5-nano \n66.86 \n80.66 \n78.45 \n78.13 \n73.85 \n66.73 \n75.34 \n76.87 \n58.70 \n16.68 \n41.34 \n51.04 \nGPT-5-mini \n85.66 \n87.92 \n88.14 \n87.24 \n86.74 \n82.78 \n88.82 \n88.28 \n78.54 \n32.96 \n60.10 \n73.08 \nGPT-5 \n88.37 \n90.26 \n89.86 \n89.50 \n89.31 \n86.25 \n90.58 \n91.12 \n86.52 \n57.35 \n71.19 \n79.58 \no4-mini \n88.73 \n90.26 \n90.17 \n89.68 \n89.54 \n86.93 \n90.58 \n90.53 \n86.83 \n47.25 \n76.10 \n82.28 \nOpen-Weight LLMs \nDeepSeek-V3 \n87.42 \n89.86 \n88.50 \n88.50 \n86.97 \n84.13 \n88.41 \n88.95 \n77.68 \n51.49 \n59.78 \n65.10 \nDeepSeek-R1 \n88.28 \n89.86 \n89.86 \n89.22 \n88.23 \n86.65 \n90.26 \n89.59 \n83.41 \n65.78 \n69.52 \n76.65 \nDeepSeek-R1-Qwen3-8B \n79.67 \n82.10 \n77.10 \n77.91 \n75.88 \n74.03 \n79.58 \n78.45 \n36.16 \n10.23 \n11.77 \n13.48 \nGemma-3-4B \n53.02 \n64.47 \n59.56 \n59.02 \n53.34 \n49.19 \n58.57 \n58.93 \n42.88 \n12.13 \n26.47 \n20.33 \nGemma-3-12B \n70.20 \n78.00 \n76.01 \n76.10 \n71.60 \n69.66 \n76.10 \n75.79 \n62.98 \n10.73 \n43.82 \n54.10 \nGemma-3-27B \n76.19 \n82.37 \n80.75 \n81.47 \n77.23 \n77.01 \n80.52 \n81.15 \n71.19 \n23.81 \n52.34 \n62.08 \ngpt-oss-20B \n83.18 \n78.45 \n85.03 \n84.76 \n82.64 \n81.38 \n85.03 \n84.54 \n68.35 \n33.77 \n61.59 \n69.57 \ngpt-oss-120B \n87.02 \n87.96 \n88.41 \n88.14 \n86.79 \n85.98 \n87.78 \n87.78 \n78.18 \n53.34 \n68.35 \n76.47 \nLLaMA-3.1-8B \n57.57 \n73.58 \n63.21 \n62.62 \n50.36 \n42.43 \n61.99 \n61.72 \n36.43 \n23.76 \n20.47 \n14.52 \nLLaMA-3.1-70B \n78.31 \n85.17 \n83.95 \n82.73 \n76.33 \n66.05 \n83.27 \n84.49 \n69.93 \n41.21 \n44.95 \n49.41 \nLLaMA-3.2-3B \n49.82 \n63.89 \n52.89 \n48.92 \n41.21 \n32.37 \n51.22 \n43.01 \n32.91 \n13.80 \n20.06 \n16.46 \nLLaMA-3.3-70B \n62.40 \n86.11 \n85.62 \n84.58 \n64.79 \n76.47 \n85.98 \n86.20 \n72.36 \n42.61 \n47.57 \n51.17 \nLLaMA-4-Scout \n84.17 \n86.79 \n86.16 \n86.93 \n84.31 \n82.55 \n86.34 \n87.42 \n77.59 \n50.27 \n50.81 \n69.39 \nLLaMA-4-Maverick \n87.42 \n89.50 \n90.08 \n89.45 \n87.02 \n86.34 \n89.95 \n90.22 \n83.95 \n54.55 \n68.53 \n77.95 \nMistral-7B-v0.3 \n29.44 \n36.70 \n20.11 \n41.03 \n24.39 \n26.74 \n29.71 \n38.23 \n17.85 \n14.61 \n9.69 \n8.66 \nMistral-Small-3.1-24B \n77.05 \n83.50 \n80.70 \n80.52 \n75.61 \n68.30 \n80.34 \n81.11 \n39.18 \n10.64 \n13.93 \n20.15 \nPhi-4-mini \n41.39 \n71.28 \n51.53 \n55.28 \n40.08 \n31.06 \n51.76 \n50.41 \n30.16 \n14.29 \n19.30 \n16.77 \nPhi-4-mini-Reasoning \n36.25 \n73.90 \n53.56 \n60.91 \n25.92 \n14.88 \n49.82 \n44.82 \n27.95 \n19.97 \n18.58 \n18.39 \nPhi-4 \n70.92 \n84.31 \n81.42 \n80.70 \n72.86 \n66.28 \n81.02 \n80.21 \n52.21 \n22.77 \n36.83 \n34.67 \nPhi-4-Reasoning \n80.70 \n85.39 \n84.31 \n84.67 \n83.32 \n79.94 \n78.40 \n85.26 \n70.78 \n17.85 \n47.88 \n37.02 \nQwen2.5-3B \n60.41 \n67.27 \n59.42 \n56.40 \n49.86 \n44.68 \n57.35 \n55.64 \n16.41 \n9.24 \n12.44 \n10.69 \nQwen2.5-7B \n71.19 \n74.17 \n68.08 \n65.64 \n63.62 \n56.85 \n66.59 \n65.87 \n32.46 \n8.61 \n25.56 \n17.49 \nQwen2.5-14B \n77.28 \n80.12 \n77.14 \n75.70 \n73.08 \n68.44 \n76.06 \n75.74 \n37.02 \n25.65 \n35.62 \n34.72 \nQwen2.5-72B \n83.14 \n85.39 \n84.67 \n84.13 \n81.61 \n79.31 \n84.99 \n84.85 \n53.70 \n43.87 \n41.70 \n43.55 \nQwQ-32B \n85.30 \n86.16 \n86.43 \n85.21 \n82.28 \n78.58 \n86.38 \n86.11 \n62.76 \n23.26 \n29.85 \n29.71 \nQwen3-1.7B \n59.02 \n65.96 \n55.59 \n53.65 \n45.13 \n41.75 \n52.93 \n52.30 \n24.17 \n22.41 \n24.89 \n24.17 \nQwen3-4B \n72.41 \n76.01 \n70.38 \n70.74 \n65.10 \n60.69 \n69.70 \n69.52 \n18.21 \n13.98 \n16.86 \n10.14 \nQwen3-4B-thinking \n76.33 \n80.79 \n79.26 \n78.22 \n75.97 \n71.51 \n78.18 \n78.58 \n30.30 \n18.35 \n13.89 \n8.79 \nQwen3-8B \n76.83 \n80.66 \n75.56 \n76.19 \n72.54 \n66.86 \n76.10 \n75.47 \n30.48 \n9.92 \n22.14 \n10.10 \nQwen3-8B-thinking \n81.33 \n83.81 \n82.42 \n82.46 \n79.98 \n79.13 \n81.92 \n82.55 \n36.34 \n15.06 \n17.04 \n11.45 \nQwen3-14B \n81.38 \n84.13 \n81.20 \n80.57 \n76.56 \n73.17 \n80.48 \n80.93 \n47.34 \n14.97 \n25.52 \n28.00 \nQwen3-14B-thinking \n83.68 \n85.89 \n85.12 \n84.31 \n83.77 \n80.66 \n84.45 \n85.17 \n63.62 \n18.39 \n24.12 \n28.49 \nBaichuan-M2-32B \n84.58 \n86.43 \n84.27 \n82.87 \n82.28 \n78.94 \n83.59 \n83.59 \n34.81 \n25.56 \n33.99 \n29.53 \nBio-Medical-LLaMA-3-8B \n50.05 \n68.35 \n59.51 \n59.06 \n49.28 \n45.13 \n57.30 \n54.37 \n39.45 \n35.84 \n34.67 \n33.14 \nMediPhi \n39.13 \n68.39 \n61.86 \n61.72 \n30.48 \n32.46 \n57.71 \n54.01 \n17.63 \n15.42 \n17.18 \n17.58 \nMedGemma-4B \n55.46 \n69.79 \n64.02 \n61.18 \n55.37 \n52.80 \n63.48 \n61.63 \n45.18 \n20.33 \n22.45 \n30.88 \nMedGemma-27B \n81.83 \n86.29 \n85.12 \n84.58 \n81.92 \n81.70 \n85.57 \n85.57 \n77.37 \n18.26 \n51.40 \n66.05 \nMedReason-8B \n47.34 \n63.57 \n18.53 \n14.29 \n48.60 \n41.52 \n27.14 \n34.85 \n12.71 \n8.21 \n13.71 \n11.68 \nHuatuoGPT-o1-7B \n73.72 \n69.52 \n72.27 \n71.28 \n65.51 \n63.44 \n72.05 \n71.06 \n7.62 \n7.44 \n6.76 \n6.22 \nHuatuoGPT-o1-8B \n64.29 \n73.26 \n70.47 \n71.24 \n62.40 \n54.82 \n69.43 \n69.39 \n50.32 \n10.05 \n9.96 \n8.66 \nHuatuoGPT-o1-70B \n72.36 \n84.90 \n84.36 \n84.67 \n72.18 \n77.01 \n84.36 \n85.17 \n74.35 \n43.51 \n49.64 \n55.59 \nHuatuoGPT-o1-72B \n84.45 \n85.57 \n86.20 \n84.04 \n83.63 \n78.85 \n86.65 \n87.20 \n62.67 \n44.27 \n45.31 \n29.53 \nOpenBioLLM-8B \n23.62 \n35.39 \n24.26 \n32.51 \n9.33 \n12.85 \n36.47 \n33.59 \n10.96 \n7.03 \n10.32 \n11.54 \nOpenBioLLM-70B \n31.83 \n72.63 \n74.35 \n52.16 \n33.36 \n30.66 \n58.88 \n70.96 \n29.76 \n10.01 \n12.44 \n23.53 \nSTab. 103: Zero-Shot performance evaluation of 56 LLMs on HeadQA (Run 4). \n"}, {"page": 96, "text": " \n \n66 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n77.01 \n83.14 \n82.82 \n81.83 \n79.31 \n75.02 \n83.27 \n83.05 \n59.20 \n19.79 \n42.47 \n43.06 \nClaude-4.0-Sonnet \n87.74 \n89.50 \n89.90 \n89.36 \n88.23 \n87.56 \n90.17 \n90.80 \n80.61 \n56.31 \n70.02 \n76.69 \nGemini-2.5-Flash \n88.86 \n88.05 \n90.17 \n89.40 \n88.86 \n88.64 \n90.04 \n90.49 \n88.68 \n77.41 \n80.39 \n83.18 \nGPT-4o-mini \n77.05 \n83.86 \n82.10 \n81.51 \n77.77 \n72.45 \n82.78 \n82.01 \n69.39 \n35.12 \n48.74 \n58.21 \nGPT-4o \n87.20 \n87.83 \n88.82 \n88.46 \n87.74 \n85.26 \n88.91 \n89.22 \n84.27 \n37.83 \n61.95 \n73.76 \nGPT-4.1-nano \n76.56 \n84.63 \n81.56 \n81.42 \n73.40 \n71.60 \n82.28 \n81.56 \n59.20 \n33.41 \n44.36 \n53.52 \nGPT-4.1-mini \n85.93 \n88.68 \n87.74 \n87.24 \n86.38 \n82.60 \n89.27 \n88.77 \n77.77 \n26.28 \n62.22 \n72.32 \nGPT-4.1 \n88.37 \n89.86 \n90.35 \n89.00 \n89.95 \n86.07 \n90.35 \n89.40 \n86.74 \n59.29 \n67.36 \n78.49 \nGPT-5-nano \n68.39 \n80.43 \n78.36 \n77.41 \n75.11 \n66.32 \n75.47 \n76.38 \n57.35 \n16.64 \n42.20 \n51.26 \nGPT-5-mini \n85.57 \n88.19 \n88.19 \n87.65 \n86.25 \n82.46 \n88.32 \n88.50 \n77.91 \n33.14 \n59.15 \n72.45 \nGPT-5 \n88.46 \n90.08 \n90.13 \n89.77 \n89.00 \n86.16 \n89.99 \n90.71 \n86.93 \n57.44 \n71.60 \n79.94 \no4-mini \n88.68 \n90.31 \n90.26 \n90.17 \n89.50 \n87.29 \n90.76 \n90.22 \n86.43 \n47.88 \n75.74 \n82.64 \nOpen-Weight LLMs \nDeepSeek-V3 \n87.69 \n89.72 \n88.95 \n88.73 \n87.92 \n84.58 \n88.86 \n89.13 \n77.10 \n50.81 \n57.80 \n64.56 \nDeepSeek-R1 \n88.23 \n89.45 \n89.50 \n89.31 \n87.42 \n86.52 \n90.31 \n89.63 \n84.49 \n65.15 \n69.57 \n77.41 \nDeepSeek-R1-Qwen3-8B \n78.72 \n82.15 \n77.37 \n77.14 \n75.16 \n73.58 \n77.77 \n78.67 \n35.53 \n11.45 \n10.60 \n12.22 \nGemma-3-4B \n52.34 \n64.92 \n59.29 \n58.43 \n53.61 \n48.42 \n60.50 \n59.20 \n42.47 \n12.71 \n25.43 \n19.84 \nGemma-3-12B \n70.65 \n78.67 \n75.61 \n75.07 \n71.73 \n69.57 \n75.20 \n75.11 \n63.62 \n11.14 \n43.28 \n54.87 \nGemma-3-27B \n76.42 \n81.47 \n80.48 \n80.97 \n78.40 \n77.32 \n80.30 \n81.83 \n71.55 \n23.35 \n52.39 \n62.40 \ngpt-oss-20B \n82.10 \n79.35 \n85.12 \n84.85 \n83.36 \n81.56 \n84.72 \n84.99 \n68.58 \n33.99 \n61.05 \n68.53 \ngpt-oss-120B \n86.79 \n87.96 \n88.23 \n88.19 \n86.70 \n85.53 \n88.01 \n88.68 \n78.04 \n52.61 \n68.89 \n76.69 \nLLaMA-3.1-8B \n55.59 \n74.21 \n63.35 \n62.94 \n51.26 \n42.06 \n62.26 \n62.71 \n38.19 \n23.62 \n20.83 \n14.29 \nLLaMA-3.1-70B \n78.09 \n85.53 \n84.81 \n83.63 \n79.08 \n66.77 \n83.86 \n84.22 \n70.78 \n41.57 \n44.59 \n50.27 \nLLaMA-3.2-3B \n49.77 \n63.89 \n52.98 \n50.63 \n41.34 \n32.87 \n49.73 \n44.18 \n33.99 \n15.24 \n17.99 \n17.58 \nLLaMA-3.3-70B \n61.81 \n85.89 \n85.62 \n85.03 \n66.23 \n75.79 \n86.16 \n86.16 \n73.04 \n41.39 \n47.34 \n51.80 \nLLaMA-4-Scout \n84.04 \n87.15 \n86.11 \n86.56 \n84.45 \n82.19 \n86.79 \n87.06 \n78.09 \n50.23 \n50.68 \n68.98 \nLLaMA-4-Maverick \n87.69 \n89.63 \n90.04 \n89.63 \n86.65 \n85.75 \n89.90 \n89.99 \n83.81 \n54.51 \n69.03 \n77.59 \nMistral-7B-v0.3 \n27.50 \n36.88 \n20.47 \n40.89 \n23.67 \n28.13 \n30.75 \n35.84 \n18.21 \n15.64 \n10.50 \n10.41 \nMistral-Small-3.1-24B \n75.79 \n83.54 \n80.43 \n80.39 \n74.75 \n67.72 \n80.70 \n80.03 \n39.27 \n11.72 \n12.94 \n20.60 \nPhi-4-mini \n43.15 \n71.51 \n52.89 \n56.09 \n39.59 \n32.33 \n49.41 \n49.37 \n29.94 \n13.53 \n19.34 \n18.58 \nPhi-4-mini-Reasoning \n37.20 \n73.58 \n52.84 \n60.73 \n27.05 \n15.28 \n49.91 \n43.87 \n28.81 \n21.10 \n19.30 \n19.93 \nPhi-4 \n69.34 \n84.36 \n81.15 \n81.47 \n73.49 \n64.79 \n80.12 \n80.79 \n52.21 \n23.58 \n38.28 \n35.12 \nPhi-4-Reasoning \n79.76 \n85.48 \n84.67 \n84.72 \n84.13 \n79.40 \n78.18 \n85.62 \n71.10 \n17.27 \n48.69 \n37.47 \nQwen2.5-3B \n61.72 \n67.13 \n58.21 \n55.73 \n49.73 \n45.58 \n55.55 \n54.96 \n15.96 \n9.29 \n13.84 \n12.22 \nQwen2.5-7B \n71.24 \n75.29 \n67.85 \n64.61 \n63.03 \n58.07 \n66.82 \n67.09 \n32.42 \n8.75 \n24.12 \n18.98 \nQwen2.5-14B \n76.83 \n80.66 \n76.60 \n75.25 \n72.90 \n68.62 \n75.29 \n76.60 \n37.20 \n26.69 \n35.48 \n36.20 \nQwen2.5-72B \n83.32 \n86.11 \n84.54 \n84.08 \n81.11 \n79.08 \n85.12 \n84.58 \n54.51 \n43.60 \n41.75 \n44.14 \nQwQ-32B \n84.67 \n86.88 \n85.89 \n85.21 \n82.10 \n79.85 \n86.52 \n86.11 \n61.72 \n23.40 \n29.89 \n28.63 \nQwen3-1.7B \n58.03 \n64.88 \n56.22 \n52.30 \n45.27 \n42.11 \n52.16 \n52.57 \n24.08 \n22.54 \n26.33 \n22.77 \nQwen3-4B \n73.67 \n76.15 \n70.33 \n70.47 \n67.45 \n59.74 \n70.51 \n70.20 \n19.07 \n14.29 \n16.68 \n8.39 \nQwen3-4B-thinking \n76.78 \n80.07 \n78.22 \n78.40 \n75.65 \n72.41 \n76.83 \n77.59 \n29.89 \n16.05 \n12.62 \n10.41 \nQwen3-8B \n76.10 \n81.33 \n75.56 \n76.15 \n72.54 \n67.22 \n76.01 \n75.52 \n30.79 \n11.09 \n22.18 \n11.68 \nQwen3-8B-thinking \n80.70 \n84.22 \n82.28 \n82.64 \n79.85 \n78.18 \n82.10 \n81.79 \n37.60 \n13.98 \n17.63 \n11.99 \nQwen3-14B \n80.61 \n83.68 \n80.43 \n80.88 \n77.59 \n73.26 \n81.15 \n81.24 \n46.93 \n14.88 \n23.35 \n26.78 \nQwen3-14B-thinking \n83.72 \n85.75 \n84.36 \n85.17 \n83.27 \n81.33 \n85.30 \n84.85 \n63.80 \n16.77 \n26.01 \n27.37 \nBaichuan-M2-32B \n84.63 \n86.11 \n84.31 \n82.24 \n81.42 \n77.82 \n83.54 \n83.90 \n34.17 \n26.19 \n32.69 \n29.44 \nBio-Medical-LLaMA-3-8B \n49.55 \n69.39 \n59.24 \n58.52 \n49.46 \n45.18 \n57.17 \n54.51 \n39.13 \n35.21 \n34.27 \n34.36 \nMediPhi \n37.83 \n68.17 \n62.17 \n60.82 \n30.97 \n33.09 \n57.57 \n53.07 \n16.37 \n15.78 \n15.69 \n17.27 \nMedGemma-4B \n54.10 \n69.39 \n62.40 \n62.49 \n55.28 \n50.14 \n62.98 \n63.12 \n45.63 \n21.37 \n22.59 \n30.57 \nMedGemma-27B \n81.92 \n86.47 \n84.63 \n84.63 \n82.24 \n80.25 \n85.35 \n86.02 \n78.13 \n17.85 \n50.99 \n66.77 \nMedReason-8B \n47.70 \n64.25 \n17.49 \n14.79 \n48.06 \n41.34 \n26.38 \n34.72 \n13.03 \n8.25 \n12.94 \n11.36 \nHuatuoGPT-o1-7B \n73.90 \n70.02 \n72.09 \n72.95 \n64.07 \n63.21 \n72.59 \n70.69 \n7.44 \n7.17 \n7.53 \n5.82 \nHuatuoGPT-o1-8B \n64.88 \n72.81 \n70.65 \n70.24 \n61.95 \n55.18 \n68.44 \n69.70 \n50.36 \n10.14 \n8.84 \n7.62 \nHuatuoGPT-o1-70B \n73.53 \n85.12 \n83.86 \n84.36 \n72.95 \n77.01 \n84.67 \n84.94 \n75.11 \n45.04 \n50.95 \n57.84 \nHuatuoGPT-o1-72B \n84.40 \n85.57 \n86.29 \n85.08 \n82.82 \n79.04 \n86.79 \n87.02 \n61.95 \n44.32 \n46.84 \n28.54 \nOpenBioLLM-8B \n22.09 \n34.72 \n24.62 \n32.24 \n9.38 \n11.86 \n34.22 \n34.99 \n10.55 \n7.03 \n10.78 \n11.00 \nOpenBioLLM-70B \n32.37 \n73.94 \n75.02 \n53.56 \n32.37 \n31.29 \n59.47 \n71.15 \n31.92 \n10.37 \n12.17 \n23.40 \nSTab. 104: Zero-Shot performance evaluation of 56 LLMs on HeadQA (Run 5). \n"}, {"page": 97, "text": " \n \n67 \n \nLLMs \nChinese \nEnglish \nFrench \nGerman Japanese Korean \nPortuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nProprietary LLMs \nClaude-3.5-Haiku \n72.64±0.36 \n78.40±0.00 80.00±0.00 75.36±0.36 74.40±0.00 72.00±0.00 \n80.80±0.00 \n80.00±0.00 49.76±0.36 33.76±0.67 43.20±0.00 35.20±0.00 64.63±17.83 \nClaude-4.0-Sonnet \n83.04±1.04 \n87.04±1.19 88.16±1.04 86.88±0.72 83.68±1.21 87.04±0.67 \n88.16±1.04 \n88.32±0.91 80.00±0.98 56.32±2.30 68.96±0.36 70.24±1.04 \n80.65±9.90 \nGemini-2.5-Flash \n85.92±0.91 \n88.00±0.80 88.96±1.04 88.64±1.04 87.68±1.21 86.08±1.34 \n88.48±0.72 \n88.48±1.66 88.96±1.31 77.76±0.67 83.04±1.91 82.24±0.67 \n86.19±3.53 \nGPT-4o-mini \n68.16±1.73 \n75.52±1.34 78.88±2.30 73.28±1.66 70.24±1.19 62.88±1.75 \n76.32±2.92 \n74.72±1.45 61.76±1.73 27.84±1.73 38.08±2.97 45.28±2.92 62.75±16.26 \nGPT-4o \n83.20±0.98 \n85.44±1.04 87.04±1.54 87.52±0.91 82.88±1.07 82.40±1.60 \n85.76±0.88 \n85.44±0.36 82.08±2.09 36.16±3.46 52.48±1.45 71.84±2.49 76.85±15.63 \nGPT-4.1-nano \n66.08±3.69 \n78.08±2.30 73.44±1.99 72.96±3.27 61.92±2.01 57.12±4.22 \n74.56±1.31 \n71.68±1.56 52.00±1.39 29.44±1.19 35.20±2.19 42.72±2.30 59.60±16.05 \nGPT-4.1-mini \n80.32±2.09 \n86.08±0.91 85.92±0.72 86.40±1.70 82.08±1.21 80.48±1.45 \n85.12±1.21 \n84.64±0.67 75.84±2.07 25.44±3.46 56.64±2.49 68.48±2.30 74.79±17.33 \nGPT-4.1 \n84.32±1.56 \n84.00±1.50 87.36±1.73 84.80±0.98 86.72±1.34 81.12±1.21 \n88.16±0.67 \n85.28±0.72 84.96±1.19 52.64±1.19 64.80±2.33 73.92±1.34 79.84±10.54 \nGPT-5-nano \n51.84±3.89 \n64.00±1.88 60.96±1.82 65.92±3.82 68.00±1.50 52.64±2.74 \n64.48±0.91 \n63.68±4.14 57.12±2.30 17.76±3.46 39.20±2.47 40.00±0.98 53.80±14.57 \nGPT-5-mini \n80.00±1.26 \n84.00±1.13 85.60±1.70 85.76±1.91 82.88±0.91 80.32±3.08 \n85.44±1.31 \n84.48±0.91 80.00±2.04 29.92±3.47 61.60±2.99 73.28±2.30 76.11±15.65 \nGPT-5 \n84.96±1.43 \n87.20±0.98 87.52±0.91 87.68±1.84 85.92±1.21 86.40±1.50 \n86.72±2.16 \n86.24±1.73 86.72±2.63 56.96±2.07 73.28±0.91 81.92±1.45 \n82.63±8.83 \no4-mini \n84.16±1.04 \n88.48±1.21 90.08±1.34 88.64±0.67 87.04±0.67 85.44±1.43 \n88.80±0.80 \n88.64±0.67 87.84±0.36 44.96±4.54 79.04±1.73 82.72±2.30 82.99±12.06 \nOpen-Weight LLMs \nDeepSeek-V3 \n82.56±0.88 \n84.48±1.07 86.08±2.23 82.40±1.13 84.96±0.67 80.96±1.19 \n84.48±2.57 \n87.36±1.82 76.96±1.82 47.36±2.49 54.56±1.82 59.36±1.43 75.96±13.50 \nDeepSeek-R1 \n84.96±1.31 \n87.68±0.91 88.32±1.66 87.84±2.29 88.00±1.60 78.72±3.38 \n87.52±0.91 \n88.64±1.82 84.16±1.54 64.80±3.35 71.52±0.44 74.88±2.09 \n82.25±7.89 \nDeepSeek-R1-Qwen3-8B \n70.24±1.73 \n73.76±3.46 66.56±3.81 64.00±2.71 63.84±1.91 63.68±4.14 \n66.56±1.04 \n66.24±3.51 28.32±2.30 11.68±1.56 12.96±4.85 13.12±4.48 50.08±24.58 \nGemma-3-4B \n42.88±1.84 \n52.96±3.37 47.68±2.01 45.76±2.85 42.08±1.84 33.60±4.83 \n44.96±2.49 \n44.96±3.68 35.52±2.23 10.72±2.57 15.36±3.85 11.84±1.54 35.69±14.55 \nGemma-3-12B \n61.92±2.75 \n68.00±2.71 64.80±2.71 65.44±1.91 55.36±1.43 54.72±4.26 \n66.24±1.73 \n65.76±3.46 53.60±1.60 10.08±3.03 27.04±2.07 41.12±2.37 52.84±17.63 \nGemma-3-27B \n67.84±1.54 \n74.72±3.33 71.36±2.85 74.24±2.43 66.88±2.81 64.16±1.54 \n72.32±1.07 \n72.96±2.15 62.40±3.49 16.16±1.54 41.76±3.81 56.64±1.54 61.79±16.69 \ngpt-oss-20B \n81.28±0.91 \n76.96±2.96 82.08±2.63 84.32±2.57 80.80±2.47 76.16±2.62 \n82.40±1.60 \n81.12±1.34 68.64±2.96 28.48±2.30 58.56±4.71 66.88±2.23 72.31±15.48 \ngpt-oss-120B \n86.40±2.88 \n88.16±1.54 88.64±1.04 90.24±0.67 87.04±0.67 84.96±1.64 \n88.00±0.98 \n87.68±0.91 78.08±1.56 55.20±2.71 74.56±0.67 73.92±2.30 \n81.91±9.88 \nLLaMA-3.1-8B \n44.96±3.73 \n68.64±3.55 52.32±3.86 50.72±1.45 41.60±4.35 30.24±5.04 \n55.04±4.09 \n47.84±2.29 30.56±3.68 21.92±3.65 13.12±2.09 10.56±2.49 38.96±17.48 \nLLaMA-3.1-70B \n73.60±1.50 \n80.64±2.62 78.88±1.45 78.08±2.75 72.48±0.72 66.56±1.91 \n76.48±1.07 \n77.28±1.66 67.84±1.99 43.68±2.01 38.56±3.17 43.68±1.93 66.48±14.99 \nLLaMA-3.2-3B \n39.84±3.12 \n58.88±1.66 45.60±3.62 39.52±4.26 37.44±0.67 22.88±2.37 \n43.84±4.43 \n27.84±4.71 25.76±3.81 12.64±2.36 16.32±4.69 12.64±3.01 31.93±14.39 \nLLaMA-3.3-70B \n66.08±2.50 \n82.40±1.96 81.44±1.04 80.48±1.75 58.72±2.16 66.88±2.16 \n78.56±0.67 \n81.44±1.64 67.52±1.21 43.04±1.54 42.72±1.66 48.80±1.88 66.51±14.73 \nLLaMA-4-Scout \n75.20±3.20 \n83.20±1.26 79.20±2.71 81.28±0.91 72.16±1.54 72.80±1.50 \n78.56±1.04 \n77.12±1.93 68.00±0.98 44.32±2.81 38.24±0.67 58.40±1.88 69.04±14.20 \nLLaMA-4-Maverick \n83.36±1.54 \n86.24±0.36 88.16±0.88 86.88±1.07 83.36±0.88 82.88±0.91 \n88.16±1.99 \n87.68±1.45 80.48±1.21 54.72±2.01 67.68±2.92 72.48±2.37 80.17±10.01 \nMistral-7B-v0.3 \n28.48±2.63 \n32.64±1.43 15.52±2.09 39.04±3.60 19.36±5.32 19.52±3.28 \n19.20±3.39 \n30.40±2.71 16.96±4.32 18.56±2.29 11.20±2.19 10.72±1.56 \n21.80±8.98 \nMistral-Small-3.1-24B \n68.96±1.82 \n78.24±1.73 71.36±1.91 76.16±2.68 66.88±3.18 54.24±3.46 \n72.96±2.96 \n72.48±2.69 30.72±4.98 10.88±3.51 12.48±1.84 12.00±2.71 52.28±26.69 \nPhi-4-mini \n34.56±2.74 \n58.08±1.21 36.80±2.33 40.80±1.13 35.84±2.62 28.00±2.47 \n35.68±3.08 \n34.08±3.08 22.88±3.03 14.56±6.02 16.64±1.04 16.64±3.41 31.21±12.25 \nPhi-4-mini-Reasoning \n28.96±3.55 \n61.12±1.66 46.24±4.13 49.60±1.26 24.80±6.25 15.68±2.09 \n35.20±1.60 \n39.04±3.32 24.00±1.88 18.40±4.90 16.32±2.16 16.48±3.51 31.32±14.80 \nPhi-4 \n59.04±3.51 \n79.20±2.65 69.60±2.99 74.56±2.07 62.40±2.71 52.00±3.88 \n71.36±3.37 \n69.44±2.29 41.44±2.22 19.04±4.13 30.56±5.53 26.24±1.73 54.57±20.04 \nPhi-4-Reasoning \n79.52±2.37 \n85.60±0.80 85.44±0.36 84.16±2.07 83.52±2.23 75.52±1.75 \n74.08±3.60 \n84.00±1.70 72.00±2.33 17.76±2.36 44.16±3.27 30.72±3.42 68.04±22.83 \nQwen2.5-3B \n50.40±2.47 \n49.28±4.02 46.88±1.21 43.68±2.75 44.00±2.83 36.16±2.62 \n43.84±3.46 \n44.80±1.60 14.88±2.69 13.12±0.91 14.88±2.69 12.48±2.09 34.53±15.32 \nQwen2.5-7B \n57.76±2.43 \n64.80±1.13 59.52±1.45 52.32±5.26 52.96±2.07 48.16±1.99 \n55.20±1.13 \n54.08±2.30 31.20±2.83 14.24±1.04 20.16±2.36 14.56±1.31 43.75±17.93 \nQwen2.5-14B \n67.52±2.69 \n73.28±1.07 60.00±1.26 69.28±1.75 62.08±3.60 55.36±3.73 \n68.00±2.53 \n67.36±1.31 31.84±1.91 20.32±1.75 28.16±0.88 31.84±3.22 52.92±18.61 \nQwen2.5-72B \n76.48±1.84 \n78.88±1.21 79.68±1.75 77.60±1.26 75.52±1.84 70.08±1.66 \n80.48±1.21 \n79.20±1.13 50.24±1.82 42.24±1.04 28.96±2.22 35.68±1.84 64.59±18.85 \nQwQ-32B \n81.92±1.07 \n83.68±1.45 84.48±1.84 82.56±0.88 79.04±1.73 68.96±1.04 \n84.16±1.31 \n83.68±2.09 64.00±3.20 17.92±4.62 15.84±3.12 19.84±2.62 63.84±27.56 \nQwen3-1.7B \n49.28±3.60 \n51.36±1.43 44.00±2.94 41.28±2.30 35.52±2.50 33.44±1.73 \n41.44±2.07 \n41.92±3.94 15.84±1.31 22.40±2.83 25.76±2.96 21.60±2.88 35.32±11.43 \nQwen3-4B \n61.92±1.93 \n66.88±1.21 56.16±4.21 61.28±2.50 53.12±1.66 47.36±2.29 \n56.00±3.44 \n56.96±2.15 17.28±2.01 15.52±2.01 16.16±3.81 10.08±1.56 43.23±21.00 \nQwen3-4B-thinking \n66.08±2.69 \n72.00±1.60 66.24±2.07 66.08±2.01 62.40±1.88 56.00±2.83 \n66.56±2.49 \n63.52±3.60 21.12±4.62 18.08±0.91 13.60±2.88 8.80±2.33 \n48.37±24.05 \nQwen3-8B \n63.68±2.37 \n70.24±2.22 58.88±4.85 63.36±3.89 58.56±1.19 53.44±3.41 \n61.44±2.74 \n60.32±3.18 16.96±3.37 11.52±2.57 22.24±1.54 9.12±1.45 \n45.81±22.67 \nQwen3-8B-thinking \n73.28±1.21 \n76.64±1.04 73.28±1.34 74.72±1.07 70.88±3.03 67.36±1.43 \n74.24±1.99 \n74.72±2.97 26.08±4.10 14.72±2.16 13.28±3.60 10.08±3.23 54.11±27.55 \nQwen3-14B \n73.12±1.34 \n74.08±1.45 73.28±2.30 72.80±1.96 67.84±2.15 58.88±2.30 \n75.52±1.21 \n73.44±3.32 38.24±3.01 13.12±2.44 13.92±2.30 15.84±3.46 54.17±25.36 \nQwen3-14B-thinking \n78.72±0.72 \n79.68±1.45 79.52±1.21 81.76±1.19 80.64±1.31 73.60±2.40 \n80.16±0.88 \n77.92±1.21 57.60±1.50 18.08±3.42 11.68±3.33 13.60±3.39 61.08±27.94 \nBaichuan-M2-32B \n79.36±1.64 \n83.04±0.88 76.80±2.40 79.52±1.66 77.12±2.01 67.52±2.44 \n80.00±1.60 \n74.56±2.29 26.56±3.94 20.64±3.12 23.04±1.91 23.84±5.35 59.33±25.93 \nBio-Medical-LLaMA-3-8B \n43.84±1.73 \n62.88±1.34 46.40±2.33 48.16±1.31 41.76±1.73 31.52±2.16 \n40.00±1.26 \n42.88±1.34 30.88±1.45 24.80±2.94 27.36±2.74 24.48±1.75 38.75±11.13 \nMediPhi \n28.96±1.91 \n53.28±3.13 44.16±2.43 50.56±2.68 24.16±1.91 29.12±3.56 \n38.72±3.18 \n39.52±2.86 13.44±2.07 17.76±1.91 17.28±4.33 20.64±3.81 31.47±13.33 \nMedGemma-4B \n48.00±1.26 \n66.56±1.82 55.68±1.66 60.64±2.22 45.12±3.08 41.44±2.07 \n54.56±1.64 \n54.24±4.06 41.60±2.88 17.12±1.21 21.76±2.49 30.56±1.82 44.77±14.87 \nMedGemma-27B \n75.04±0.67 \n82.56±0.67 80.00±1.39 80.80±2.33 74.72±0.72 74.08±3.51 \n80.16±1.91 \n80.16±2.68 76.00±3.10 17.60±4.08 45.60±1.60 61.60±1.96 69.03±18.72 \nMedReason-8B \n46.56±2.49 \n54.56±3.98 16.96±3.32 16.64±2.62 39.84±2.91 32.00±3.25 \n18.40±3.05 \n21.76±0.67 9.76±1.54 \n9.60±0.98 \n9.60±2.77 \n6.72±1.56 \n23.53±15.61 \nHuatuoGPT-o1-7B \n59.20±1.50 \n58.08±3.74 62.24±1.82 63.52±2.30 56.48±2.63 54.72±0.72 \n57.92±1.56 \n57.76±2.91 10.72±0.72 11.68±2.75 9.76±2.07 \n7.68±1.34 \n42.48±23.40 \nHuatuoGPT-o1-8B \n55.52±2.63 \n63.84±3.60 60.32±2.57 57.28±1.75 47.52±3.74 42.56±3.64 \n57.76±3.32 \n58.88±5.76 40.64±1.73 11.20±1.39 11.68±2.30 8.32±2.57 \n42.96±20.34 \nHuatuoGPT-o1-70B \n62.08±2.30 \n80.16±1.43 78.88±1.84 79.84±1.73 64.00±1.50 72.48±2.81 \n79.36±1.04 \n80.80±2.26 70.88±2.57 33.92±1.66 43.84±2.43 54.24±3.12 66.71±15.23 \nHuatuoGPT-o1-72B \n79.52±1.84 \n80.32±1.66 84.16±1.43 83.04±1.04 80.80±1.70 76.48±2.16 \n85.60±1.13 \n84.00±1.70 61.92±3.51 39.68±1.66 36.00±1.60 19.04±3.12 67.55±22.30 \nOpenBioLLM-8B \n17.44±4.32 \n30.24±2.22 14.56±2.85 24.80±2.99 \n9.28±3.23 \n10.40±3.25 \n24.80±3.75 \n21.60±2.53 12.00±2.71 6.56±2.85 \n9.12±3.33 13.76±2.43 \n16.21±7.80 \nOpenBioLLM-70B \n25.76±1.31 \n70.72±1.56 63.68±2.30 40.80±3.96 26.72±4.55 21.44±3.81 \n47.20±3.49 \n58.56±1.91 29.12±4.48 10.72±3.13 14.24±2.22 22.72±3.33 35.97±19.45 \nSTab. 105: Performance evaluation of 56 LLMs on MedExpQA. \n"}, {"page": 98, "text": " \n \n68 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n72.00 \n78.40 \n80.00 \n75.20 \n74.40 \n72.00 \n80.80 \n80.00 \n49.60 \n34.40 \n43.20 \n35.20 \nClaude-4.0-Sonnet \n84.00 \n88.80 \n87.20 \n86.40 \n84.80 \n87.20 \n86.40 \n89.60 \n80.80 \n56.00 \n68.80 \n72.00 \nGemini-2.5-Flash \n85.60 \n88.80 \n88.80 \n89.60 \n88.80 \n86.40 \n88.00 \n86.40 \n89.60 \n77.60 \n84.00 \n81.60 \nGPT-4o-mini \n68.80 \n74.40 \n82.40 \n75.20 \n69.60 \n63.20 \n76.00 \n75.20 \n63.20 \n28.80 \n37.60 \n44.00 \nGPT-4o \n83.20 \n86.40 \n86.40 \n87.20 \n82.40 \n84.00 \n84.80 \n85.60 \n83.20 \n36.80 \n52.00 \n75.20 \nGPT-4.1-nano \n63.20 \n81.60 \n73.60 \n69.60 \n61.60 \n52.80 \n74.40 \n71.20 \n51.20 \n31.20 \n32.80 \n45.60 \nGPT-4.1-mini \n78.40 \n86.40 \n86.40 \n84.00 \n81.60 \n80.80 \n85.60 \n84.00 \n76.00 \n29.60 \n56.00 \n65.60 \nGPT-4.1 \n86.40 \n84.80 \n88.00 \n84.80 \n85.60 \n82.40 \n88.80 \n84.80 \n84.80 \n51.20 \n68.00 \n73.60 \nGPT-5-nano \n54.40 \n62.40 \n60.80 \n62.40 \n68.00 \n49.60 \n65.60 \n61.60 \n59.20 \n19.20 \n39.20 \n40.00 \nGPT-5-mini \n79.20 \n84.00 \n83.20 \n83.20 \n84.00 \n82.40 \n85.60 \n83.20 \n78.40 \n28.80 \n59.20 \n75.20 \nGPT-5 \n84.80 \n88.00 \n87.20 \n89.60 \n85.60 \n86.40 \n87.20 \n84.00 \n84.00 \n58.40 \n72.80 \n81.60 \no4-mini \n85.60 \n89.60 \n90.40 \n88.00 \n87.20 \n84.80 \n88.80 \n88.80 \n87.20 \n52.80 \n79.20 \n83.20 \nOpen-Weight LLMs \nDeepSeek-V3 \n84.00 \n84.00 \n85.60 \n82.40 \n84.80 \n80.80 \n84.80 \n85.60 \n76.80 \n49.60 \n54.40 \n58.40 \nDeepSeek-R1 \n84.00 \n88.00 \n88.00 \n84.80 \n88.00 \n80.00 \n88.00 \n90.40 \n85.60 \n70.40 \n72.00 \n71.20 \nDeepSeek-R1-Qwen3-8B \n70.40 \n76.00 \n72.80 \n60.80 \n60.80 \n56.80 \n67.20 \n66.40 \n31.20 \n13.60 \n14.40 \n20.80 \nGemma-3-4B \n40.00 \n49.60 \n46.40 \n48.80 \n40.00 \n41.60 \n43.20 \n51.20 \n32.80 \n8.80 \n17.60 \n12.80 \nGemma-3-12B \n59.20 \n68.80 \n61.60 \n64.00 \n56.80 \n51.20 \n64.00 \n66.40 \n52.00 \n8.80 \n28.00 \n40.00 \nGemma-3-27B \n69.60 \n69.60 \n71.20 \n72.00 \n67.20 \n63.20 \n73.60 \n75.20 \n62.40 \n16.80 \n47.20 \n54.40 \ngpt-oss-20B \n82.40 \n76.00 \n84.00 \n85.60 \n84.80 \n72.00 \n80.80 \n83.20 \n64.80 \n26.40 \n55.20 \n63.20 \ngpt-oss-120B \n85.60 \n86.40 \n87.20 \n90.40 \n87.20 \n86.40 \n87.20 \n88.00 \n76.80 \n58.40 \n74.40 \n73.60 \nLLaMA-3.1-8B \n40.00 \n65.60 \n47.20 \n49.60 \n39.20 \n22.40 \n56.80 \n48.00 \n35.20 \n23.20 \n11.20 \n12.00 \nLLaMA-3.1-70B \n72.00 \n81.60 \n79.20 \n76.00 \n72.00 \n67.20 \n76.00 \n78.40 \n67.20 \n42.40 \n34.40 \n46.40 \nLLaMA-3.2-3B \n41.60 \n57.60 \n47.20 \n45.60 \n38.40 \n25.60 \n48.00 \n34.40 \n24.00 \n12.00 \n19.20 \n12.00 \nLLaMA-3.3-70B \n68.00 \n80.00 \n80.00 \n80.80 \n58.40 \n65.60 \n79.20 \n84.00 \n68.00 \n42.40 \n40.80 \n48.80 \nLLaMA-4-Scout \n79.20 \n84.00 \n78.40 \n80.00 \n74.40 \n73.60 \n78.40 \n79.20 \n68.00 \n44.00 \n37.60 \n58.40 \nLLaMA-4-Maverick \n81.60 \n86.40 \n88.80 \n85.60 \n84.80 \n81.60 \n89.60 \n87.20 \n81.60 \n55.20 \n68.00 \n72.80 \nMistral-7B-v0.3 \n24.80 \n34.40 \n15.20 \n36.00 \n17.60 \n20.00 \n17.60 \n32.00 \n12.80 \n21.60 \n14.40 \n11.20 \nMistral-Small-3.1-24B \n68.80 \n80.80 \n70.40 \n76.00 \n66.40 \n51.20 \n77.60 \n71.20 \n31.20 \n14.40 \n12.80 \n10.40 \nPhi-4-mini \n34.40 \n60.00 \n35.20 \n41.60 \n38.40 \n32.00 \n36.80 \n32.80 \n24.00 \n11.20 \n15.20 \n16.00 \nPhi-4-mini-Reasoning \n24.80 \n60.80 \n40.80 \n49.60 \n16.00 \n16.80 \n36.00 \n40.80 \n22.40 \n24.80 \n15.20 \n21.60 \nPhi-4 \n59.20 \n81.60 \n70.40 \n75.20 \n57.60 \n55.20 \n75.20 \n69.60 \n39.20 \n16.00 \n27.20 \n24.80 \nPhi-4-Reasoning \n83.20 \n84.80 \n84.80 \n84.80 \n85.60 \n76.00 \n76.00 \n83.20 \n74.40 \n19.20 \n49.60 \n34.40 \nQwen2.5-3B \n48.00 \n51.20 \n45.60 \n44.80 \n42.40 \n36.00 \n43.20 \n43.20 \n12.00 \n12.80 \n11.20 \n9.60 \nQwen2.5-7B \n54.40 \n64.00 \n60.00 \n47.20 \n53.60 \n48.80 \n55.20 \n53.60 \n31.20 \n15.20 \n24.00 \n15.20 \nQwen2.5-14B \n70.40 \n72.80 \n60.00 \n72.00 \n65.60 \n51.20 \n64.80 \n66.40 \n33.60 \n20.80 \n27.20 \n28.00 \nQwen2.5-72B \n78.40 \n80.00 \n80.80 \n77.60 \n77.60 \n72.80 \n79.20 \n80.00 \n51.20 \n40.80 \n31.20 \n37.60 \nQwQ-32B \n82.40 \n82.40 \n84.80 \n83.20 \n80.80 \n68.80 \n84.80 \n80.80 \n66.40 \n25.60 \n12.80 \n16.80 \nQwen3-1.7B \n52.00 \n52.80 \n41.60 \n43.20 \n36.00 \n33.60 \n43.20 \n43.20 \n13.60 \n23.20 \n28.00 \n21.60 \nQwen3-4B \n60.80 \n67.20 \n53.60 \n60.80 \n52.00 \n44.80 \n60.80 \n58.40 \n14.40 \n12.80 \n18.40 \n12.00 \nQwen3-4B-thinking \n63.20 \n72.80 \n63.20 \n69.60 \n60.80 \n56.00 \n66.40 \n66.40 \n28.80 \n18.40 \n17.60 \n12.00 \nQwen3-8B \n61.60 \n71.20 \n60.80 \n56.80 \n56.80 \n56.80 \n61.60 \n59.20 \n16.80 \n8.00 \n20.00 \n9.60 \nQwen3-8B-thinking \n73.60 \n76.00 \n73.60 \n75.20 \n74.40 \n68.80 \n76.00 \n70.40 \n32.00 \n16.00 \n12.80 \n9.60 \nQwen3-14B \n75.20 \n74.40 \n73.60 \n72.80 \n65.60 \n60.80 \n77.60 \n71.20 \n36.80 \n14.40 \n12.00 \n10.40 \nQwen3-14B-thinking \n78.40 \n81.60 \n80.00 \n80.00 \n80.80 \n72.80 \n80.00 \n76.00 \n59.20 \n17.60 \n10.40 \n9.60 \nBaichuan-M2-32B \n77.60 \n82.40 \n80.80 \n78.40 \n77.60 \n67.20 \n80.80 \n72.80 \n23.20 \n22.40 \n20.80 \n24.80 \nBio-Medical-LLaMA-3-8B \n42.40 \n63.20 \n48.80 \n49.60 \n40.80 \n30.40 \n40.80 \n43.20 \n28.80 \n21.60 \n23.20 \n21.60 \nMediPhi \n28.00 \n53.60 \n45.60 \n52.00 \n21.60 \n23.20 \n39.20 \n35.20 \n13.60 \n19.20 \n18.40 \n19.20 \nMedGemma-4B \n47.20 \n65.60 \n56.00 \n60.00 \n48.00 \n41.60 \n52.80 \n55.20 \n39.20 \n18.40 \n24.00 \n31.20 \nMedGemma-27B \n75.20 \n81.60 \n77.60 \n84.80 \n75.20 \n75.20 \n79.20 \n82.40 \n73.60 \n20.00 \n46.40 \n63.20 \nMedReason-8B \n44.80 \n54.40 \n17.60 \n12.80 \n40.80 \n33.60 \n21.60 \n21.60 \n12.00 \n9.60 \n11.20 \n5.60 \nHuatuoGPT-o1-7B \n58.40 \n57.60 \n61.60 \n61.60 \n52.80 \n55.20 \n58.40 \n56.80 \n11.20 \n10.40 \n7.20 \n8.80 \nHuatuoGPT-o1-8B \n56.00 \n64.00 \n57.60 \n57.60 \n44.80 \n46.40 \n54.40 \n61.60 \n43.20 \n12.00 \n12.80 \n8.00 \nHuatuoGPT-o1-70B \n62.40 \n79.20 \n81.60 \n80.00 \n62.40 \n69.60 \n79.20 \n81.60 \n75.20 \n35.20 \n45.60 \n51.20 \nHuatuoGPT-o1-72B \n81.60 \n78.40 \n85.60 \n84.00 \n78.40 \n78.40 \n87.20 \n84.80 \n64.80 \n38.40 \n37.60 \n13.60 \nOpenBioLLM-8B \n12.00 \n32.00 \n10.40 \n20.80 \n4.80 \n8.80 \n28.80 \n17.60 \n8.00 \n4.00 \n8.80 \n14.40 \nOpenBioLLM-70B \n26.40 \n72.00 \n67.20 \n40.00 \n32.00 \n17.60 \n43.20 \n59.20 \n28.00 \n13.60 \n17.60 \n18.40 \nSTab. 106: Zero-Shot performance evaluation of 56 LLMs on MedExpQA (Run 1). \n"}, {"page": 99, "text": " \n \n69 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n72.80 \n78.40 \n80.00 \n75.20 \n74.40 \n72.00 \n80.80 \n80.00 \n50.40 \n33.60 \n43.20 \n35.20 \nClaude-4.0-Sonnet \n84.00 \n87.20 \n88.00 \n86.40 \n84.00 \n87.20 \n88.80 \n87.20 \n80.80 \n58.40 \n69.60 \n70.40 \nGemini-2.5-Flash \n84.80 \n88.00 \n88.00 \n87.20 \n88.80 \n87.20 \n89.60 \n87.20 \n88.00 \n77.60 \n85.60 \n83.20 \nGPT-4o-mini \n65.60 \n74.40 \n77.60 \n74.40 \n70.40 \n63.20 \n80.80 \n72.80 \n62.40 \n26.40 \n40.80 \n42.40 \nGPT-4o \n83.20 \n86.40 \n88.00 \n88.80 \n82.40 \n80.80 \n85.60 \n85.60 \n80.00 \n30.40 \n52.80 \n69.60 \nGPT-4.1-nano \n64.00 \n78.40 \n73.60 \n72.80 \n59.20 \n59.20 \n76.80 \n73.60 \n51.20 \n29.60 \n36.00 \n40.80 \nGPT-4.1-mini \n80.00 \n87.20 \n86.40 \n88.80 \n80.80 \n80.00 \n85.60 \n84.80 \n76.80 \n26.40 \n60.80 \n68.00 \nGPT-4.1 \n84.80 \n84.80 \n85.60 \n84.80 \n87.20 \n82.40 \n88.80 \n84.80 \n85.60 \n54.40 \n64.00 \n75.20 \nGPT-5-nano \n54.40 \n65.60 \n62.40 \n67.20 \n67.20 \n51.20 \n63.20 \n68.00 \n59.20 \n13.60 \n40.80 \n40.00 \nGPT-5-mini \n78.40 \n82.40 \n85.60 \n85.60 \n81.60 \n80.00 \n85.60 \n85.60 \n78.40 \n26.40 \n57.60 \n74.40 \nGPT-5 \n86.40 \n88.00 \n86.40 \n84.80 \n87.20 \n85.60 \n88.80 \n88.80 \n88.80 \n55.20 \n73.60 \n82.40 \no4-mini \n84.00 \n87.20 \n89.60 \n88.80 \n88.00 \n84.80 \n88.00 \n88.80 \n88.00 \n42.40 \n78.40 \n82.40 \nOpen-Weight LLMs \nDeepSeek-V3 \n82.40 \n84.00 \n88.00 \n84.00 \n85.60 \n79.20 \n81.60 \n87.20 \n76.80 \n43.20 \n54.40 \n57.60 \nDeepSeek-R1 \n83.20 \n86.40 \n87.20 \n89.60 \n86.40 \n74.40 \n88.80 \n87.20 \n84.00 \n64.80 \n71.20 \n76.00 \nDeepSeek-R1-Qwen3-8B \n70.40 \n68.00 \n64.00 \n63.20 \n65.60 \n66.40 \n65.60 \n69.60 \n24.80 \n11.20 \n18.40 \n12.00 \nGemma-3-4B \n44.80 \n50.40 \n47.20 \n48.80 \n44.00 \n34.40 \n47.20 \n41.60 \n37.60 \n14.40 \n11.20 \n11.20 \nGemma-3-12B \n63.20 \n64.80 \n62.40 \n66.40 \n55.20 \n52.80 \n66.40 \n70.40 \n55.20 \n7.20 \n29.60 \n44.00 \nGemma-3-27B \n65.60 \n73.60 \n71.20 \n76.00 \n71.20 \n64.00 \n71.20 \n72.80 \n61.60 \n16.00 \n42.40 \n58.40 \ngpt-oss-20B \n80.80 \n72.80 \n85.60 \n84.00 \n80.80 \n76.80 \n84.00 \n80.80 \n68.80 \n25.60 \n59.20 \n68.80 \ngpt-oss-120B \n87.20 \n88.00 \n88.80 \n89.60 \n87.20 \n83.20 \n89.60 \n88.00 \n77.60 \n54.40 \n74.40 \n76.00 \nLLaMA-3.1-8B \n50.40 \n73.60 \n53.60 \n52.00 \n35.20 \n28.00 \n55.20 \n44.80 \n25.60 \n26.40 \n12.80 \n8.00 \nLLaMA-3.1-70B \n72.80 \n77.60 \n78.40 \n76.80 \n72.00 \n65.60 \n76.00 \n77.60 \n69.60 \n45.60 \n38.40 \n42.40 \nLLaMA-3.2-3B \n38.40 \n60.80 \n46.40 \n36.80 \n37.60 \n24.00 \n41.60 \n28.80 \n29.60 \n9.60 \n23.20 \n11.20 \nLLaMA-3.3-70B \n62.40 \n83.20 \n80.80 \n77.60 \n55.20 \n67.20 \n79.20 \n80.00 \n65.60 \n43.20 \n44.00 \n50.40 \nLLaMA-4-Scout \n71.20 \n82.40 \n76.00 \n81.60 \n72.80 \n74.40 \n79.20 \n79.20 \n68.80 \n44.00 \n38.40 \n57.60 \nLLaMA-4-Maverick \n85.60 \n86.40 \n88.80 \n86.40 \n83.20 \n82.40 \n89.60 \n85.60 \n79.20 \n51.20 \n71.20 \n73.60 \nMistral-7B-v0.3 \n29.60 \n32.80 \n17.60 \n34.40 \n24.00 \n16.00 \n22.40 \n32.80 \n22.40 \n16.00 \n9.60 \n9.60 \nMistral-Small-3.1-24B \n67.20 \n77.60 \n74.40 \n74.40 \n68.80 \n58.40 \n72.80 \n73.60 \n28.00 \n10.40 \n9.60 \n13.60 \nPhi-4-mini \n38.40 \n56.80 \n35.20 \n41.60 \n37.60 \n28.00 \n36.80 \n31.20 \n23.20 \n24.80 \n17.60 \n12.00 \nPhi-4-mini-Reasoning \n28.80 \n62.40 \n52.00 \n50.40 \n25.60 \n18.40 \n36.80 \n37.60 \n26.40 \n20.00 \n19.20 \n17.60 \nPhi-4 \n62.40 \n76.80 \n65.60 \n76.00 \n64.00 \n45.60 \n74.40 \n71.20 \n43.20 \n22.40 \n39.20 \n28.80 \nPhi-4-Reasoning \n78.40 \n86.40 \n85.60 \n84.00 \n84.00 \n75.20 \n76.80 \n85.60 \n72.80 \n19.20 \n41.60 \n27.20 \nQwen2.5-3B \n52.00 \n50.40 \n47.20 \n46.40 \n44.00 \n32.80 \n47.20 \n44.00 \n19.20 \n12.00 \n18.40 \n11.20 \nQwen2.5-7B \n56.80 \n65.60 \n57.60 \n50.40 \n53.60 \n48.00 \n53.60 \n54.40 \n26.40 \n14.40 \n18.40 \n13.60 \nQwen2.5-14B \n68.00 \n72.00 \n59.20 \n68.80 \n64.00 \n52.00 \n71.20 \n68.80 \n32.80 \n22.40 \n28.00 \n35.20 \nQwen2.5-72B \n76.00 \n80.00 \n80.80 \n76.80 \n72.80 \n69.60 \n80.80 \n78.40 \n48.00 \n42.40 \n27.20 \n35.20 \nQwQ-32B \n80.80 \n84.80 \n83.20 \n83.20 \n80.00 \n70.40 \n85.60 \n84.80 \n63.20 \n17.60 \n20.80 \n24.00 \nQwen3-1.7B \n51.20 \n49.60 \n42.40 \n43.20 \n39.20 \n31.20 \n44.00 \n35.20 \n16.80 \n18.40 \n26.40 \n17.60 \nQwen3-4B \n64.00 \n67.20 \n56.80 \n60.80 \n55.20 \n50.40 \n53.60 \n56.00 \n19.20 \n15.20 \n21.60 \n8.00 \nQwen3-4B-thinking \n68.00 \n71.20 \n66.40 \n65.60 \n64.80 \n53.60 \n68.00 \n67.20 \n20.80 \n17.60 \n12.80 \n10.40 \nQwen3-8B \n61.60 \n70.40 \n51.20 \n64.00 \n59.20 \n52.80 \n57.60 \n57.60 \n20.80 \n10.40 \n21.60 \n8.00 \nQwen3-8B-thinking \n72.00 \n76.00 \n73.60 \n73.60 \n72.00 \n65.60 \n74.40 \n78.40 \n21.60 \n14.40 \n10.40 \n6.40 \nQwen3-14B \n72.00 \n76.00 \n76.00 \n71.20 \n70.40 \n61.60 \n74.40 \n72.00 \n40.00 \n11.20 \n14.40 \n14.40 \nQwen3-14B-thinking \n78.40 \n80.00 \n80.00 \n82.40 \n81.60 \n71.20 \n79.20 \n78.40 \n57.60 \n21.60 \n15.20 \n18.40 \nBaichuan-M2-32B \n77.60 \n82.40 \n76.00 \n77.60 \n75.20 \n64.80 \n77.60 \n77.60 \n23.20 \n16.00 \n24.00 \n20.80 \nBio-Medical-LLaMA-3-8B \n44.80 \n60.80 \n44.80 \n47.20 \n43.20 \n32.80 \n41.60 \n44.80 \n32.80 \n24.80 \n28.80 \n24.80 \nMediPhi \n32.00 \n51.20 \n43.20 \n50.40 \n24.00 \n32.80 \n42.40 \n39.20 \n14.40 \n20.00 \n23.20 \n25.60 \nMedGemma-4B \n48.80 \n64.00 \n58.40 \n58.40 \n44.00 \n42.40 \n56.00 \n57.60 \n42.40 \n16.80 \n24.00 \n32.80 \nMedGemma-27B \n75.20 \n82.40 \n80.80 \n79.20 \n75.20 \n74.40 \n78.40 \n80.00 \n75.20 \n19.20 \n46.40 \n58.40 \nMedReason-8B \n47.20 \n55.20 \n14.40 \n19.20 \n42.40 \n33.60 \n14.40 \n22.40 \n10.40 \n10.40 \n7.20 \n7.20 \nHuatuoGPT-o1-7B \n57.60 \n57.60 \n63.20 \n65.60 \n60.00 \n54.40 \n58.40 \n60.80 \n11.20 \n11.20 \n9.60 \n7.20 \nHuatuoGPT-o1-8B \n56.00 \n60.00 \n61.60 \n54.40 \n43.20 \n41.60 \n57.60 \n67.20 \n39.20 \n12.00 \n10.40 \n4.80 \nHuatuoGPT-o1-70B \n64.00 \n79.20 \n79.20 \n80.00 \n64.00 \n73.60 \n78.40 \n81.60 \n69.60 \n34.40 \n45.60 \n51.20 \nHuatuoGPT-o1-72B \n79.20 \n79.20 \n83.20 \n84.00 \n80.80 \n72.80 \n85.60 \n86.40 \n60.80 \n40.00 \n34.40 \n21.60 \nOpenBioLLM-8B \n17.60 \n31.20 \n17.60 \n28.80 \n12.80 \n16.00 \n27.20 \n23.20 \n13.60 \n4.80 \n5.60 \n9.60 \nOpenBioLLM-70B \n26.40 \n69.60 \n61.60 \n45.60 \n28.00 \n20.00 \n50.40 \n55.20 \n28.80 \n13.60 \n12.00 \n26.40 \nSTab. 107: Zero-Shot performance evaluation of 56 LLMs on MedExpQA (Run 2). \n"}, {"page": 100, "text": " \n \n70 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n72.80 \n78.40 \n80.00 \n75.20 \n74.40 \n72.00 \n80.80 \n80.00 \n49.60 \n32.80 \n43.20 \n35.20 \nClaude-4.0-Sonnet \n81.60 \n87.20 \n87.20 \n87.20 \n84.00 \n86.40 \n88.80 \n88.00 \n80.00 \n52.80 \n68.80 \n69.60 \nGemini-2.5-Flash \n87.20 \n87.20 \n88.00 \n89.60 \n86.40 \n87.20 \n88.00 \n89.60 \n87.20 \n78.40 \n81.60 \n82.40 \nGPT-4o-mini \n68.00 \n77.60 \n77.60 \n73.60 \n68.80 \n63.20 \n75.20 \n76.00 \n60.80 \n26.40 \n37.60 \n48.00 \nGPT-4o \n81.60 \n84.00 \n84.80 \n88.00 \n84.00 \n84.00 \n87.20 \n85.60 \n82.40 \n39.20 \n52.80 \n73.60 \nGPT-4.1-nano \n71.20 \n76.00 \n73.60 \n72.00 \n61.60 \n62.40 \n74.40 \n72.80 \n54.40 \n29.60 \n38.40 \n44.00 \nGPT-4.1-mini \n83.20 \n84.80 \n85.60 \n86.40 \n84.00 \n80.80 \n86.40 \n84.80 \n75.20 \n27.20 \n56.80 \n71.20 \nGPT-4.1 \n83.20 \n82.40 \n89.60 \n86.40 \n86.40 \n80.80 \n88.00 \n84.80 \n84.80 \n52.00 \n66.40 \n73.60 \nGPT-5-nano \n48.00 \n62.40 \n59.20 \n64.00 \n70.40 \n53.60 \n64.80 \n66.40 \n56.80 \n22.40 \n35.20 \n40.80 \nGPT-5-mini \n81.60 \n85.60 \n87.20 \n84.80 \n82.40 \n84.00 \n86.40 \n84.80 \n80.80 \n33.60 \n63.20 \n70.40 \nGPT-5 \n83.20 \n87.20 \n88.00 \n88.80 \n87.20 \n88.80 \n86.40 \n85.60 \n84.00 \n59.20 \n74.40 \n80.00 \no4-mini \n84.80 \n89.60 \n88.00 \n88.00 \n86.40 \n84.80 \n89.60 \n89.60 \n88.00 \n43.20 \n79.20 \n79.20 \nOpen-Weight LLMs \nDeepSeek-V3 \n81.60 \n83.20 \n82.40 \n80.80 \n84.00 \n80.80 \n82.40 \n88.80 \n75.20 \n48.00 \n57.60 \n60.80 \nDeepSeek-R1 \n85.60 \n88.80 \n89.60 \n86.40 \n86.40 \n80.80 \n86.40 \n86.40 \n84.80 \n61.60 \n71.20 \n75.20 \nDeepSeek-R1-Qwen3-8B \n69.60 \n74.40 \n63.20 \n62.40 \n64.80 \n67.20 \n65.60 \n69.60 \n28.80 \n11.20 \n11.20 \n12.80 \nGemma-3-4B \n42.40 \n55.20 \n47.20 \n43.20 \n41.60 \n29.60 \n41.60 \n43.20 \n37.60 \n8.00 \n11.20 \n9.60 \nGemma-3-12B \n65.60 \n72.00 \n68.00 \n68.00 \n54.40 \n52.00 \n65.60 \n61.60 \n55.20 \n14.40 \n24.00 \n40.00 \nGemma-3-27B \n68.80 \n76.00 \n75.20 \n71.20 \n67.20 \n66.40 \n72.80 \n72.80 \n61.60 \n13.60 \n36.80 \n56.80 \ngpt-oss-20B \n81.60 \n76.80 \n80.80 \n85.60 \n80.80 \n76.80 \n84.00 \n81.60 \n67.20 \n30.40 \n60.80 \n68.00 \ngpt-oss-120B \n86.40 \n88.80 \n88.00 \n89.60 \n88.00 \n86.40 \n88.00 \n88.80 \n80.80 \n53.60 \n73.60 \n76.00 \nLLaMA-3.1-8B \n45.60 \n71.20 \n49.60 \n51.20 \n43.20 \n32.80 \n52.00 \n51.20 \n30.40 \n20.00 \n11.20 \n8.00 \nLLaMA-3.1-70B \n76.00 \n78.40 \n79.20 \n76.00 \n72.80 \n64.80 \n77.60 \n75.20 \n64.80 \n40.80 \n42.40 \n41.60 \nLLaMA-3.2-3B \n43.20 \n59.20 \n48.00 \n40.00 \n36.80 \n24.00 \n48.00 \n25.60 \n27.20 \n12.00 \n13.60 \n17.60 \nLLaMA-3.3-70B \n65.60 \n84.80 \n81.60 \n82.40 \n59.20 \n68.00 \n77.60 \n81.60 \n67.20 \n44.80 \n42.40 \n49.60 \nLLaMA-4-Scout \n73.60 \n84.80 \n77.60 \n81.60 \n71.20 \n73.60 \n80.00 \n75.20 \n66.40 \n40.00 \n38.40 \n61.60 \nLLaMA-4-Maverick \n82.40 \n85.60 \n87.20 \n88.00 \n83.20 \n83.20 \n85.60 \n88.00 \n79.20 \n55.20 \n68.80 \n75.20 \nMistral-7B-v0.3 \n28.00 \n32.80 \n14.40 \n42.40 \n16.80 \n24.80 \n14.40 \n28.80 \n14.40 \n18.40 \n8.80 \n8.80 \nMistral-Small-3.1-24B \n72.00 \n76.80 \n70.40 \n74.40 \n64.80 \n50.40 \n70.40 \n76.00 \n27.20 \n8.80 \n12.00 \n13.60 \nPhi-4-mini \n32.00 \n57.60 \n36.80 \n39.20 \n32.00 \n28.00 \n30.40 \n34.40 \n20.00 \n10.40 \n17.60 \n21.60 \nPhi-4-mini-Reasoning \n31.20 \n62.40 \n45.60 \n48.00 \n28.80 \n16.00 \n36.00 \n43.20 \n25.60 \n17.60 \n17.60 \n12.80 \nPhi-4 \n62.40 \n82.40 \n70.40 \n76.80 \n63.20 \n53.60 \n70.40 \n71.20 \n41.60 \n20.00 \n28.00 \n25.60 \nPhi-4-Reasoning \n76.80 \n84.80 \n85.60 \n87.20 \n81.60 \n77.60 \n76.00 \n81.60 \n68.80 \n15.20 \n44.00 \n31.20 \nQwen2.5-3B \n53.60 \n44.80 \n48.00 \n44.80 \n41.60 \n36.80 \n38.40 \n45.60 \n13.60 \n13.60 \n13.60 \n14.40 \nQwen2.5-7B \n57.60 \n66.40 \n59.20 \n60.80 \n55.20 \n44.80 \n56.80 \n51.20 \n32.00 \n13.60 \n19.20 \n15.20 \nQwen2.5-14B \n63.20 \n74.40 \n61.60 \n68.80 \n56.80 \n57.60 \n68.00 \n68.80 \n31.20 \n20.00 \n28.00 \n28.80 \nQwen2.5-72B \n74.40 \n79.20 \n76.80 \n78.40 \n75.20 \n68.80 \n81.60 \n77.60 \n49.60 \n41.60 \n31.20 \n36.80 \nQwQ-32B \n80.80 \n82.40 \n87.20 \n81.60 \n77.60 \n69.60 \n83.20 \n83.20 \n68.00 \n17.60 \n16.00 \n20.00 \nQwen3-1.7B \n43.20 \n52.80 \n48.80 \n37.60 \n32.80 \n36.00 \n40.00 \n43.20 \n16.00 \n25.60 \n28.80 \n20.80 \nQwen3-4B \n60.80 \n68.00 \n52.80 \n63.20 \n54.40 \n45.60 \n56.00 \n58.40 \n18.40 \n16.00 \n14.40 \n9.60 \nQwen3-4B-thinking \n65.60 \n71.20 \n68.80 \n65.60 \n61.60 \n55.20 \n62.40 \n61.60 \n18.40 \n18.40 \n13.60 \n7.20 \nQwen3-8B \n67.20 \n72.00 \n57.60 \n64.80 \n60.00 \n48.80 \n64.00 \n57.60 \n16.00 \n14.40 \n22.40 \n7.20 \nQwen3-8B-thinking \n72.80 \n76.00 \n72.00 \n73.60 \n69.60 \n68.80 \n76.00 \n73.60 \n23.20 \n11.20 \n15.20 \n8.80 \nQwen3-14B \n72.00 \n72.00 \n73.60 \n76.00 \n68.80 \n57.60 \n75.20 \n70.40 \n33.60 \n12.00 \n15.20 \n17.60 \nQwen3-14B-thinking \n80.00 \n79.20 \n80.80 \n83.20 \n80.80 \n72.80 \n80.00 \n78.40 \n58.40 \n15.20 \n9.60 \n11.20 \nBaichuan-M2-32B \n80.00 \n84.00 \n76.80 \n81.60 \n75.20 \n69.60 \n80.80 \n72.00 \n30.40 \n19.20 \n23.20 \n32.80 \nBio-Medical-LLaMA-3-8B \n41.60 \n62.40 \n43.20 \n48.80 \n39.20 \n28.80 \n38.40 \n41.60 \n31.20 \n22.40 \n26.40 \n24.80 \nMediPhi \n27.20 \n50.40 \n47.20 \n50.40 \n23.20 \n29.60 \n39.20 \n40.00 \n10.40 \n17.60 \n16.00 \n15.20 \nMedGemma-4B \n46.40 \n67.20 \n54.40 \n59.20 \n48.80 \n40.80 \n56.00 \n56.00 \n38.40 \n17.60 \n20.00 \n28.00 \nMedGemma-27B \n74.40 \n83.20 \n80.80 \n79.20 \n75.20 \n76.80 \n83.20 \n76.80 \n80.00 \n20.00 \n44.80 \n63.20 \nMedReason-8B \n43.20 \n60.80 \n22.40 \n15.20 \n37.60 \n32.00 \n20.00 \n20.80 \n8.00 \n8.00 \n8.80 \n4.80 \nHuatuoGPT-o1-7B \n59.20 \n53.60 \n60.00 \n62.40 \n56.00 \n53.60 \n59.20 \n54.40 \n10.40 \n8.00 \n10.40 \n8.00 \nHuatuoGPT-o1-8B \n58.40 \n68.80 \n58.40 \n59.20 \n48.80 \n44.00 \n63.20 \n53.60 \n39.20 \n8.80 \n15.20 \n7.20 \nHuatuoGPT-o1-70B \n58.40 \n82.40 \n79.20 \n82.40 \n64.00 \n76.80 \n80.00 \n81.60 \n71.20 \n35.20 \n45.60 \n54.40 \nHuatuoGPT-o1-72B \n76.80 \n82.40 \n85.60 \n82.40 \n80.00 \n77.60 \n85.60 \n82.40 \n66.40 \n40.80 \n34.40 \n20.00 \nOpenBioLLM-8B \n17.60 \n26.40 \n14.40 \n23.20 \n7.20 \n8.00 \n26.40 \n20.80 \n10.40 \n7.20 \n14.40 \n14.40 \nOpenBioLLM-70B \n27.20 \n69.60 \n64.00 \n38.40 \n24.80 \n23.20 \n51.20 \n59.20 \n26.40 \n6.40 \n12.80 \n22.40 \nSTab. 108: Zero-Shot performance evaluation of 56 LLMs on MedExpQA (Run 3). \n"}, {"page": 101, "text": " \n \n71 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n72.80 \n78.40 \n80.00 \n75.20 \n74.40 \n72.00 \n80.80 \n80.00 \n49.60 \n33.60 \n43.20 \n35.20 \nClaude-4.0-Sonnet \n82.40 \n85.60 \n89.60 \n88.00 \n84.00 \n88.00 \n88.80 \n88.00 \n80.00 \n58.40 \n68.80 \n69.60 \nGemini-2.5-Flash \n86.40 \n87.20 \n89.60 \n88.80 \n88.00 \n85.60 \n88.00 \n90.40 \n89.60 \n78.40 \n83.20 \n81.60 \nGPT-4o-mini \n68.00 \n75.20 \n76.80 \n72.00 \n72.00 \n64.80 \n76.80 \n73.60 \n63.20 \n30.40 \n33.60 \n43.20 \nGPT-4o \n84.00 \n85.60 \n87.20 \n87.20 \n81.60 \n82.40 \n85.60 \n84.80 \n80.00 \n36.00 \n50.40 \n69.60 \nGPT-4.1-nano \n68.80 \n78.40 \n76.00 \n72.00 \n62.40 \n58.40 \n73.60 \n71.20 \n52.00 \n28.00 \n35.20 \n43.20 \nGPT-4.1-mini \n81.60 \n85.60 \n84.80 \n86.40 \n81.60 \n78.40 \n83.20 \n84.00 \n72.80 \n20.80 \n55.20 \n70.40 \nGPT-4.1 \n84.80 \n85.60 \n85.60 \n84.00 \n88.80 \n80.00 \n87.20 \n86.40 \n86.40 \n52.80 \n63.20 \n75.20 \nGPT-5-nano \n47.20 \n66.40 \n59.20 \n72.00 \n66.40 \n52.00 \n64.80 \n57.60 \n53.60 \n15.20 \n39.20 \n38.40 \nGPT-5-mini \n80.00 \n84.00 \n84.80 \n88.00 \n83.20 \n79.20 \n83.20 \n84.80 \n83.20 \n33.60 \n64.00 \n71.20 \nGPT-5 \n86.40 \n85.60 \n87.20 \n88.00 \n84.80 \n86.40 \n83.20 \n86.40 \n89.60 \n57.60 \n72.00 \n84.00 \no4-mini \n83.20 \n87.20 \n91.20 \n88.80 \n86.40 \n88.00 \n89.60 \n88.00 \n88.00 \n41.60 \n76.80 \n85.60 \nOpen-Weight LLMs \nDeepSeek-V3 \n82.40 \n85.60 \n87.20 \n82.40 \n85.60 \n81.60 \n88.00 \n89.60 \n80.00 \n48.80 \n52.80 \n59.20 \nDeepSeek-R1 \n85.60 \n88.00 \n90.40 \n88.00 \n89.60 \n76.00 \n87.20 \n88.80 \n81.60 \n64.00 \n71.20 \n76.00 \nDeepSeek-R1-Qwen3-8B \n68.00 \n73.60 \n65.60 \n66.40 \n63.20 \n64.80 \n66.40 \n64.00 \n28.00 \n12.80 \n5.60 \n9.60 \nGemma-3-4B \n43.20 \n57.60 \n51.20 \n44.80 \n44.00 \n32.00 \n47.20 \n44.00 \n36.00 \n10.40 \n19.20 \n12.00 \nGemma-3-12B \n59.20 \n66.40 \n66.40 \n65.60 \n53.60 \n61.60 \n68.80 \n63.20 \n52.00 \n12.00 \n27.20 \n38.40 \nGemma-3-27B \n68.00 \n78.40 \n67.20 \n76.00 \n64.80 \n62.40 \n71.20 \n69.60 \n68.00 \n16.80 \n40.00 \n56.00 \ngpt-oss-20B \n80.00 \n78.40 \n79.20 \n86.40 \n79.20 \n79.20 \n82.40 \n80.00 \n69.60 \n30.40 \n64.80 \n66.40 \ngpt-oss-120B \n82.40 \n87.20 \n89.60 \n91.20 \n86.40 \n85.60 \n88.00 \n86.40 \n77.60 \n52.00 \n75.20 \n70.40 \nLLaMA-3.1-8B \n44.00 \n66.40 \n56.80 \n48.80 \n45.60 \n34.40 \n50.40 \n48.00 \n28.80 \n16.80 \n16.00 \n11.20 \nLLaMA-3.1-70B \n73.60 \n84.00 \n76.80 \n82.40 \n73.60 \n65.60 \n75.20 \n79.20 \n68.00 \n44.80 \n40.80 \n43.20 \nLLaMA-3.2-3B \n35.20 \n56.80 \n39.20 \n40.80 \n37.60 \n20.80 \n44.00 \n21.60 \n20.00 \n13.60 \n12.80 \n9.60 \nLLaMA-3.3-70B \n65.60 \n80.80 \n82.40 \n80.80 \n60.80 \n64.00 \n78.40 \n80.00 \n68.00 \n40.80 \n41.60 \n49.60 \nLLaMA-4-Scout \n77.60 \n81.60 \n82.40 \n80.80 \n70.40 \n71.20 \n77.60 \n76.00 \n68.00 \n46.40 \n39.20 \n56.80 \nLLaMA-4-Maverick \n84.00 \n86.40 \n87.20 \n86.40 \n82.40 \n83.20 \n86.40 \n88.00 \n80.80 \n56.00 \n67.20 \n72.00 \nMistral-7B-v0.3 \n28.00 \n32.80 \n17.60 \n41.60 \n12.80 \n18.40 \n22.40 \n32.00 \n20.80 \n16.80 \n11.20 \n12.80 \nMistral-Small-3.1-24B \n68.80 \n76.80 \n69.60 \n75.20 \n63.20 \n54.40 \n73.60 \n72.80 \n28.00 \n6.40 \n13.60 \n14.40 \nPhi-4-mini \n32.00 \n57.60 \n36.00 \n40.00 \n36.80 \n25.60 \n36.00 \n39.20 \n27.20 \n11.20 \n16.00 \n16.80 \nPhi-4-mini-Reasoning \n26.40 \n58.40 \n44.80 \n51.20 \n21.60 \n13.60 \n32.80 \n34.40 \n22.40 \n11.20 \n16.00 \n13.60 \nPhi-4 \n54.40 \n78.40 \n73.60 \n72.00 \n63.20 \n54.40 \n69.60 \n69.60 \n44.00 \n23.20 \n25.60 \n27.20 \nPhi-4-Reasoning \n80.00 \n86.40 \n85.60 \n83.20 \n85.60 \n72.80 \n68.00 \n85.60 \n73.60 \n15.20 \n41.60 \n27.20 \nQwen2.5-3B \n50.40 \n54.40 \n48.00 \n39.20 \n43.20 \n40.00 \n44.00 \n47.20 \n15.20 \n12.80 \n16.00 \n12.80 \nQwen2.5-7B \n59.20 \n64.00 \n61.60 \n53.60 \n52.80 \n49.60 \n55.20 \n53.60 \n32.80 \n15.20 \n18.40 \n16.00 \nQwen2.5-14B \n68.80 \n74.40 \n58.40 \n69.60 \n60.00 \n60.00 \n66.40 \n66.40 \n32.80 \n17.60 \n28.00 \n33.60 \nQwen2.5-72B \n75.20 \n77.60 \n79.20 \n76.00 \n76.80 \n70.40 \n81.60 \n80.00 \n49.60 \n43.20 \n26.40 \n36.00 \nQwQ-32B \n82.40 \n83.20 \n82.40 \n81.60 \n76.80 \n68.00 \n84.80 \n86.40 \n62.40 \n13.60 \n13.60 \n19.20 \nQwen3-1.7B \n51.20 \n51.20 \n42.40 \n40.80 \n36.00 \n32.80 \n39.20 \n45.60 \n16.80 \n24.00 \n21.60 \n22.40 \nQwen3-4B \n64.00 \n64.80 \n63.20 \n64.00 \n52.80 \n47.20 \n52.00 \n58.40 \n18.40 \n18.40 \n14.40 \n11.20 \nQwen3-4B-thinking \n69.60 \n74.40 \n67.20 \n64.80 \n60.80 \n54.40 \n67.20 \n64.00 \n20.80 \n16.80 \n14.40 \n6.40 \nQwen3-8B \n63.20 \n66.40 \n60.80 \n64.00 \n58.40 \n56.80 \n64.00 \n62.40 \n19.20 \n11.20 \n23.20 \n10.40 \nQwen3-8B-thinking \n72.80 \n78.40 \n75.20 \n75.20 \n66.40 \n67.20 \n71.20 \n75.20 \n25.60 \n16.80 \n18.40 \n15.20 \nQwen3-14B \n72.80 \n73.60 \n69.60 \n72.80 \n68.80 \n56.00 \n75.20 \n78.40 \n40.00 \n16.80 \n11.20 \n18.40 \nQwen3-14B-thinking \n78.40 \n80.00 \n77.60 \n81.60 \n81.60 \n73.60 \n80.00 \n79.20 \n55.20 \n14.40 \n8.00 \n14.40 \nBaichuan-M2-32B \n80.80 \n82.40 \n74.40 \n80.80 \n77.60 \n70.40 \n79.20 \n76.00 \n31.20 \n21.60 \n25.60 \n20.80 \nBio-Medical-LLaMA-3-8B \n44.80 \n64.00 \n48.00 \n48.80 \n43.20 \n31.20 \n39.20 \n43.20 \n30.40 \n26.40 \n28.00 \n24.80 \nMediPhi \n28.00 \n58.40 \n40.80 \n46.40 \n25.60 \n30.40 \n39.20 \n43.20 \n16.00 \n15.20 \n17.60 \n21.60 \nMedGemma-4B \n49.60 \n68.80 \n54.40 \n64.00 \n42.40 \n38.40 \n52.80 \n47.20 \n45.60 \n15.20 \n22.40 \n29.60 \nMedGemma-27B \n76.00 \n83.20 \n80.80 \n80.80 \n73.60 \n68.00 \n80.80 \n83.20 \n72.80 \n18.40 \n43.20 \n61.60 \nMedReason-8B \n48.80 \n50.40 \n16.00 \n17.60 \n42.40 \n26.40 \n20.00 \n22.40 \n8.80 \n9.60 \n7.20 \n7.20 \nHuatuoGPT-o1-7B \n59.20 \n64.00 \n61.60 \n61.60 \n57.60 \n55.20 \n58.40 \n56.00 \n9.60 \n14.40 \n12.80 \n5.60 \nHuatuoGPT-o1-8B \n51.20 \n60.80 \n60.00 \n57.60 \n52.80 \n36.80 \n56.00 \n53.60 \n40.00 \n11.20 \n10.40 \n10.40 \nHuatuoGPT-o1-70B \n64.00 \n79.20 \n76.80 \n77.60 \n66.40 \n71.20 \n78.40 \n82.40 \n68.80 \n31.20 \n41.60 \n56.00 \nHuatuoGPT-o1-72B \n79.20 \n80.00 \n84.00 \n83.20 \n82.40 \n76.80 \n85.60 \n82.40 \n59.20 \n37.60 \n36.00 \n20.00 \nOpenBioLLM-8B \n24.00 \n31.20 \n16.80 \n25.60 \n10.40 \n10.40 \n20.80 \n22.40 \n14.40 \n11.20 \n7.20 \n16.00 \nOpenBioLLM-70B \n24.80 \n69.60 \n61.60 \n44.00 \n20.00 \n27.20 \n46.40 \n60.00 \n25.60 \n11.20 \n13.60 \n20.80 \nSTab. 109: Zero-Shot performance evaluation of 56 LLMs on MedExpQA (Run 4). \n"}, {"page": 102, "text": " \n \n72 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n72.80 \n78.40 \n80.00 \n76.00 \n74.40 \n72.00 \n80.80 \n80.00 \n49.60 \n34.40 \n43.20 \n35.20 \nClaude-4.0-Sonnet \n83.20 \n86.40 \n88.80 \n86.40 \n81.60 \n86.40 \n88.00 \n88.80 \n78.40 \n56.00 \n68.80 \n69.60 \nGemini-2.5-Flash \n85.60 \n88.80 \n90.40 \n88.00 \n86.40 \n84.00 \n88.80 \n88.80 \n90.40 \n76.80 \n80.80 \n82.40 \nGPT-4o-mini \n70.40 \n76.00 \n80.00 \n71.20 \n70.40 \n60.00 \n72.80 \n76.00 \n59.20 \n27.20 \n40.80 \n48.80 \nGPT-4o \n84.00 \n84.80 \n88.80 \n86.40 \n84.00 \n80.80 \n85.60 \n85.60 \n84.80 \n38.40 \n54.40 \n71.20 \nGPT-4.1-nano \n63.20 \n76.00 \n70.40 \n78.40 \n64.80 \n52.80 \n73.60 \n69.60 \n51.20 \n28.80 \n33.60 \n40.00 \nGPT-4.1-mini \n78.40 \n86.40 \n86.40 \n86.40 \n82.40 \n82.40 \n84.80 \n85.60 \n78.40 \n23.20 \n54.40 \n67.20 \nGPT-4.1 \n82.40 \n82.40 \n88.00 \n84.00 \n85.60 \n80.00 \n88.00 \n85.60 \n83.20 \n52.80 \n62.40 \n72.00 \nGPT-5-nano \n55.20 \n63.20 \n63.20 \n64.00 \n68.00 \n56.80 \n64.00 \n64.80 \n56.80 \n18.40 \n41.60 \n40.80 \nGPT-5-mini \n80.80 \n84.00 \n87.20 \n87.20 \n83.20 \n76.00 \n86.40 \n84.00 \n79.20 \n27.20 \n64.00 \n75.20 \nGPT-5 \n84.00 \n87.20 \n88.80 \n87.20 \n84.80 \n84.80 \n88.00 \n86.40 \n87.20 \n54.40 \n73.60 \n81.60 \no4-mini \n83.20 \n88.80 \n91.20 \n89.60 \n87.20 \n84.80 \n88.00 \n88.00 \n88.00 \n44.80 \n81.60 \n83.20 \nOpen-Weight LLMs \nDeepSeek-V3 \n82.40 \n85.60 \n87.20 \n82.40 \n84.80 \n82.40 \n85.60 \n85.60 \n76.00 \n47.20 \n53.60 \n60.80 \nDeepSeek-R1 \n86.40 \n87.20 \n86.40 \n90.40 \n89.60 \n82.40 \n87.20 \n90.40 \n84.80 \n63.20 \n72.00 \n76.00 \nDeepSeek-R1-Qwen3-8B \n72.80 \n76.80 \n67.20 \n67.20 \n64.80 \n63.20 \n68.00 \n61.60 \n28.80 \n9.60 \n15.20 \n10.40 \nGemma-3-4B \n44.00 \n52.00 \n46.40 \n43.20 \n40.80 \n30.40 \n45.60 \n44.80 \n33.60 \n12.00 \n17.60 \n13.60 \nGemma-3-12B \n62.40 \n68.00 \n65.60 \n63.20 \n56.80 \n56.00 \n66.40 \n67.20 \n53.60 \n8.00 \n26.40 \n43.20 \nGemma-3-27B \n67.20 \n76.00 \n72.00 \n76.00 \n64.00 \n64.80 \n72.80 \n74.40 \n58.40 \n17.60 \n42.40 \n57.60 \ngpt-oss-20B \n81.60 \n80.80 \n80.80 \n80.00 \n78.40 \n76.00 \n80.80 \n80.00 \n72.80 \n29.60 \n52.80 \n68.00 \ngpt-oss-120B \n90.40 \n90.40 \n89.60 \n90.40 \n86.40 \n83.20 \n87.20 \n87.20 \n77.60 \n57.60 \n75.20 \n73.60 \nLLaMA-3.1-8B \n44.80 \n66.40 \n54.40 \n52.00 \n44.80 \n33.60 \n60.80 \n47.20 \n32.80 \n23.20 \n14.40 \n13.60 \nLLaMA-3.1-70B \n73.60 \n81.60 \n80.80 \n79.20 \n72.00 \n69.60 \n77.60 \n76.00 \n69.60 \n44.80 \n36.80 \n44.80 \nLLaMA-3.2-3B \n40.80 \n60.00 \n47.20 \n34.40 \n36.80 \n20.00 \n37.60 \n28.80 \n28.00 \n16.00 \n12.80 \n12.80 \nLLaMA-3.3-70B \n68.80 \n83.20 \n82.40 \n80.80 \n60.00 \n69.60 \n78.40 \n81.60 \n68.80 \n44.00 \n44.80 \n45.60 \nLLaMA-4-Scout \n74.40 \n83.20 \n81.60 \n82.40 \n72.00 \n71.20 \n77.60 \n76.00 \n68.80 \n47.20 \n37.60 \n57.60 \nLLaMA-4-Maverick \n83.20 \n86.40 \n88.80 \n88.00 \n83.20 \n84.00 \n89.60 \n89.60 \n81.60 \n56.00 \n63.20 \n68.80 \nMistral-7B-v0.3 \n32.00 \n30.40 \n12.80 \n40.80 \n25.60 \n18.40 \n19.20 \n26.40 \n14.40 \n20.00 \n12.00 \n11.20 \nMistral-Small-3.1-24B \n68.00 \n79.20 \n72.00 \n80.80 \n71.20 \n56.80 \n70.40 \n68.80 \n39.20 \n14.40 \n14.40 \n8.00 \nPhi-4-mini \n36.00 \n58.40 \n40.80 \n41.60 \n34.40 \n26.40 \n38.40 \n32.80 \n20.00 \n15.20 \n16.80 \n16.80 \nPhi-4-mini-Reasoning \n33.60 \n61.60 \n48.00 \n48.80 \n32.00 \n13.60 \n34.40 \n39.20 \n23.20 \n18.40 \n13.60 \n16.80 \nPhi-4 \n56.80 \n76.80 \n68.00 \n72.80 \n64.00 \n51.20 \n67.20 \n65.60 \n39.20 \n13.60 \n32.80 \n24.80 \nPhi-4-Reasoning \n79.20 \n85.60 \n85.60 \n81.60 \n80.80 \n76.00 \n73.60 \n84.00 \n70.40 \n20.00 \n44.00 \n33.60 \nQwen2.5-3B \n48.00 \n45.60 \n45.60 \n43.20 \n48.80 \n35.20 \n46.40 \n44.00 \n14.40 \n14.40 \n15.20 \n14.40 \nQwen2.5-7B \n60.80 \n64.00 \n59.20 \n49.60 \n49.60 \n49.60 \n55.20 \n57.60 \n33.60 \n12.80 \n20.80 \n12.80 \nQwen2.5-14B \n67.20 \n72.80 \n60.80 \n67.20 \n64.00 \n56.00 \n69.60 \n66.40 \n28.80 \n20.80 \n29.60 \n33.60 \nQwen2.5-72B \n78.40 \n77.60 \n80.80 \n79.20 \n75.20 \n68.80 \n79.20 \n80.00 \n52.80 \n43.20 \n28.80 \n32.80 \nQwQ-32B \n83.20 \n85.60 \n84.80 \n83.20 \n80.00 \n68.00 \n82.40 \n83.20 \n60.00 \n15.20 \n16.00 \n19.20 \nQwen3-1.7B \n48.80 \n50.40 \n44.80 \n41.60 \n33.60 \n33.60 \n40.80 \n42.40 \n16.00 \n20.80 \n24.00 \n25.60 \nQwen3-4B \n60.00 \n67.20 \n54.40 \n57.60 \n51.20 \n48.80 \n57.60 \n53.60 \n16.00 \n15.20 \n12.00 \n9.60 \nQwen3-4B-thinking \n64.00 \n70.40 \n65.60 \n64.80 \n64.00 \n60.80 \n68.80 \n58.40 \n16.80 \n19.20 \n9.60 \n8.00 \nQwen3-8B \n64.80 \n71.20 \n64.00 \n67.20 \n58.40 \n52.00 \n60.00 \n64.80 \n12.00 \n13.60 \n24.00 \n10.40 \nQwen3-8B-thinking \n75.20 \n76.80 \n72.00 \n76.00 \n72.00 \n66.40 \n73.60 \n76.00 \n28.00 \n15.20 \n9.60 \n10.40 \nQwen3-14B \n73.60 \n74.40 \n73.60 \n71.20 \n65.60 \n58.40 \n75.20 \n75.20 \n40.80 \n11.20 \n16.80 \n18.40 \nQwen3-14B-thinking \n78.40 \n77.60 \n79.20 \n81.60 \n78.40 \n77.60 \n81.60 \n77.60 \n57.60 \n21.60 \n15.20 \n14.40 \nBaichuan-M2-32B \n80.80 \n84.00 \n76.00 \n79.20 \n80.00 \n65.60 \n81.60 \n74.40 \n24.80 \n24.00 \n21.60 \n20.00 \nBio-Medical-LLaMA-3-8B \n45.60 \n64.00 \n47.20 \n46.40 \n42.40 \n34.40 \n40.00 \n41.60 \n31.20 \n28.80 \n30.40 \n26.40 \nMediPhi \n29.60 \n52.80 \n44.00 \n53.60 \n26.40 \n29.60 \n33.60 \n40.00 \n12.80 \n16.80 \n11.20 \n21.60 \nMedGemma-4B \n48.00 \n67.20 \n55.20 \n61.60 \n42.40 \n44.00 \n55.20 \n55.20 \n42.40 \n17.60 \n18.40 \n31.20 \nMedGemma-27B \n74.40 \n82.40 \n80.00 \n80.00 \n74.40 \n76.00 \n79.20 \n78.40 \n78.40 \n10.40 \n47.20 \n61.60 \nMedReason-8B \n48.80 \n52.00 \n14.40 \n18.40 \n36.00 \n34.40 \n16.00 \n21.60 \n9.60 \n10.40 \n13.60 \n8.80 \nHuatuoGPT-o1-7B \n61.60 \n57.60 \n64.80 \n66.40 \n56.00 \n55.20 \n55.20 \n60.80 \n11.20 \n14.40 \n8.80 \n8.80 \nHuatuoGPT-o1-8B \n56.00 \n65.60 \n64.00 \n57.60 \n48.00 \n44.00 \n57.60 \n58.40 \n41.60 \n12.00 \n9.60 \n11.20 \nHuatuoGPT-o1-70B \n61.60 \n80.80 \n77.60 \n79.20 \n63.20 \n71.20 \n80.80 \n76.80 \n69.60 \n33.60 \n40.80 \n58.40 \nHuatuoGPT-o1-72B \n80.80 \n81.60 \n82.40 \n81.60 \n82.40 \n76.80 \n84.00 \n84.00 \n58.40 \n41.60 \n37.60 \n20.00 \nOpenBioLLM-8B \n16.00 \n30.40 \n13.60 \n25.60 \n11.20 \n8.80 \n20.80 \n24.00 \n13.60 \n5.60 \n9.60 \n14.40 \nOpenBioLLM-70B \n24.00 \n72.80 \n64.00 \n36.00 \n28.80 \n19.20 \n44.80 \n59.20 \n36.80 \n8.80 \n15.20 \n25.60 \nSTab. 110: Zero-Shot performance evaluation of 56 LLMs on MedExpQA (Run 5). \n"}, {"page": 103, "text": " \n \n73 \n \nLLMs \nChinese \nEnglish \nFrench \nGerman Japanese \nKorean Portuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nProprietary LLMs \nClaude-3.5-Haiku \n66.87±0.04 77.08±0.04 73.04±0.04 73.29±0.00 68.03±0.06 68.74±0.09 \n72.11±0.09 \n73.47±0.04 52.54±0.08 34.33±0.08 39.83±0.00 37.05±0.04 61.36±15.35 \nClaude-4.0-Sonnet \n89.57±0.40 91.96±0.27 89.55±0.57 90.10±0.22 89.79±0.33 89.77±0.47 \n91.22±0.13 \n90.68±0.46 81.82±0.23 59.07±0.35 70.83±0.56 74.63±0.66 84.08±10.15 \nGemini-2.5-Flash \n90.16±0.42 91.26±0.35 90.89±0.26 91.39±0.12 90.04±0.30 90.06±0.22 \n90.92±0.49 \n91.14±0.38 89.68±0.36 79.31±0.84 82.81±0.66 84.77±0.40 \n88.54±3.86 \nGPT-4o-mini \n67.60±0.47 77.88±0.50 71.77±0.61 73.39±0.41 68.47±0.92 65.26±0.35 \n72.90±0.48 \n73.56±0.49 60.74±0.53 32.49±0.51 43.93±0.76 50.50±0.86 63.21±13.44 \nGPT-4o \n86.21±0.52 89.29±0.40 88.45±0.49 88.34±0.43 85.44±0.99 84.85±0.44 \n89.18±0.25 \n88.91±0.22 82.73±0.46 32.22±1.20 50.05±1.11 69.29±0.51 77.91±17.79 \nGPT-4.1-nano \n66.90±0.86 81.41±0.80 72.33±1.11 71.25±0.85 63.31±1.25 60.77±1.00 \n72.54±1.01 \n73.87±0.45 53.17±0.70 27.84±0.85 38.32±1.07 45.37±1.02 60.59±15.72 \nGPT-4.1-mini \n85.89±1.00 91.18±0.38 86.46±0.87 87.87±0.23 85.66±0.36 82.41±0.78 \n87.67±0.47 \n88.12±0.42 78.38±0.95 25.33±1.18 58.40±1.07 67.62±0.85 77.08±18.28 \nGPT-4.1 \n89.02±0.47 89.87±0.43 90.01±0.15 89.19±0.20 90.04±0.44 86.68±0.73 \n91.55±0.26 \n89.50±0.56 88.50±0.45 51.85±0.50 68.37±0.55 75.99±0.82 83.38±11.69 \nGPT-5-nano \n51.31±1.24 69.43±1.36 65.53±0.79 69.50±0.42 65.25±0.63 54.88±1.27 \n62.56±0.93 \n71.58±1.04 56.60±1.45 20.02±1.32 38.67±0.70 43.82±0.93 55.76±14.85 \nGPT-5-mini \n86.36±0.89 91.11±0.44 88.22±0.96 88.27±0.72 87.75±0.37 84.59±0.31 \n89.02±0.60 \n89.03±0.49 80.90±0.66 35.63±1.03 61.59±0.83 71.50±1.16 79.50±15.75 \nGPT-5 \n91.12±0.45 92.99±0.31 92.05±0.58 91.19±0.48 91.06±0.20 84.37±0.36 \n92.44±0.64 \n91.97±0.44 89.14±0.20 54.00±0.88 74.47±1.02 75.38±0.67 85.02±11.33 \no4-mini \n93.23±0.36 94.33±0.43 93.43±0.41 93.31±0.21 93.29±0.49 91.97±0.59 \n93.89±0.27 \n93.38±0.61 91.33±0.21 39.75±1.59 80.35±0.90 85.58±0.58 86.99±14.92 \nOpen-Weight LLMs \nDeepSeek-V3 \n86.35±0.86 89.19±0.44 86.00±0.77 86.57±0.35 85.40±0.47 82.83±0.53 \n86.54±0.38 \n87.30±0.50 75.04±0.41 44.32±0.40 56.10±1.08 62.01±1.22 77.30±14.40 \nDeepSeek-R1 \n90.82±0.47 91.39±0.26 90.82±0.44 91.01±0.50 90.37±0.44 80.90±0.55 \n91.70±0.44 \n91.47±0.23 87.59±0.40 66.14±0.68 72.33±0.68 79.07±0.81 \n85.30±8.42 \nDeepSeek-R1-Qwen3-8B \n73.07±0.57 79.76±0.39 69.02±0.95 70.54±0.30 66.66±0.67 68.42±1.22 \n71.70±0.77 \n69.43±0.56 33.56±1.47 10.73±0.66 11.36±0.69 12.17±0.89 53.04±26.52 \nGemma-3-4B \n43.80±0.54 51.89±0.62 45.53±0.74 44.93±0.91 42.06±0.62 40.06±1.25 \n46.30±0.42 \n47.07±0.57 37.63±0.60 \n9.84±0.95 \n16.67±0.76 14.66±0.33 36.70±13.90 \nGemma-3-12B \n61.48±0.90 69.79±0.65 64.93±0.70 64.91±0.75 60.91±0.60 59.85±0.94 \n64.29±0.75 \n65.89±0.83 56.21±1.02 \n9.79±0.79 \n36.28±0.62 43.65±1.44 54.83±16.60 \nGemma-3-27B \n69.35±0.83 76.06±0.72 72.17±0.64 72.66±0.52 69.74±0.75 68.50±0.75 \n72.11±0.74 \n72.37±0.32 65.31±0.80 19.12±0.90 46.17±0.85 53.04±0.61 63.05±15.82 \ngpt-oss-20B \n82.70±0.41 80.36±0.66 84.12±0.83 83.52±0.86 83.10±1.29 80.82±0.92 \n84.09±0.78 \n84.09±0.21 68.09±0.97 30.93±1.37 60.82±0.88 66.69±0.86 74.11±15.31 \ngpt-oss-120B \n89.32±0.78 91.45±0.54 90.41±0.45 90.37±0.59 89.49±0.64 87.68±0.57 \n90.78±0.91 \n90.81±0.44 81.15±0.50 54.74±0.61 71.88±0.61 77.64±0.39 83.81±10.72 \nLLaMA-3.1-8B \n45.12±0.39 67.01±0.67 49.07±0.88 48.75±1.09 38.54±0.54 32.96±1.54 \n49.30±1.04 \n49.85±0.97 36.23±0.90 25.59±0.95 17.14±0.56 12.52±0.78 39.34±14.99 \nLLaMA-3.1-70B \n71.58±0.74 83.24±0.92 76.56±0.78 76.91±0.27 69.08±0.83 62.69±0.80 \n76.54±0.92 \n78.30±0.43 61.89±0.40 40.06±1.68 43.14±0.90 47.04±0.64 65.59±14.31 \nLLaMA-3.2-3B \n39.34±0.77 56.92±0.79 40.67±0.50 35.16±0.59 33.83±0.68 27.18±0.92 \n37.94±1.08 \n36.40±1.68 30.82±1.09 16.00±0.79 18.49±1.78 15.41±0.52 32.35±11.52 \nLLaMA-3.3-70B \n61.26±0.55 84.89±0.15 80.16±0.86 80.68±0.73 58.84±0.38 70.59±0.39 \n80.69±0.53 \n81.16±0.71 67.90±0.39 41.40±0.85 42.98±0.35 47.45±0.53 66.50±15.39 \nLLaMA-4-Scout \n79.64±0.70 85.29±0.67 81.64±0.86 82.75±0.42 79.18±1.02 78.24±0.63 \n81.62±0.60 \n82.19±0.54 74.45±0.53 46.44±0.96 46.54±0.10 63.72±0.35 73.48±13.29 \nLLaMA-4-Maverick \n85.80±0.52 90.24±0.43 88.72±0.33 88.30±0.77 86.26±0.69 85.58±0.39 \n88.97±0.48 \n88.75±0.42 84.32±0.44 56.84±0.56 68.74±0.43 75.90±0.61 \n82.37±9.85 \nMistral-7B-v0.3 \n22.32±0.83 28.95±1.72 14.50±0.90 29.28±0.74 20.82±1.18 17.97±0.76 \n22.29±1.30 \n27.71±1.21 19.68±1.05 16.76±0.94 10.07±0.66 10.20±0.65 \n20.05±6.43 \nMistral-Small-3.1-24B \n63.98±0.87 75.88±1.06 65.09±1.48 67.79±1.25 60.38±1.38 54.96±0.92 \n67.92±0.72 \n68.72±0.74 32.36±1.20 11.97±0.65 15.43±1.98 19.28±0.99 50.31±22.78 \nPhi-4-mini \n32.82±1.32 53.83±0.76 36.02±1.20 38.68±1.48 31.80±0.89 24.07±0.98 \n36.01±0.99 \n37.72±0.91 27.82±1.63 17.53±0.42 19.57±1.18 17.11±0.98 31.08±10.31 \nPhi-4-mini-Reasoning \n28.72±1.79 65.17±0.59 47.42±1.31 47.94±0.34 23.53±0.73 14.04±0.56 \n40.12±1.70 \n34.99±0.56 27.64±1.24 23.66±1.15 21.56±0.82 20.80±1.40 32.97±14.25 \nPhi-4 \n58.62±1.18 79.78±0.23 70.01±0.85 71.03±0.69 60.52±1.70 54.06±0.84 \n70.16±1.13 \n70.64±1.27 46.53±1.11 25.44±0.60 36.65±0.94 31.19±1.14 56.22±17.12 \nPhi-4-Reasoning \n80.72±0.84 85.42±0.49 84.05±0.15 84.56±0.99 83.06±0.76 80.19±0.77 \n75.81±1.46 \n84.37±0.47 72.55±0.52 13.31±0.64 47.68±0.70 29.58±0.58 68.44±23.66 \nQwen2.5-3B \n46.18±1.29 49.71±1.30 40.46±0.59 40.68±0.87 39.00±0.97 33.37±0.49 \n38.92±0.63 \n39.31±0.29 14.56±0.99 \n9.76±0.53 \n11.09±0.56 \n8.96±0.41 \n31.00±14.77 \nQwen2.5-7B \n56.86±0.83 61.12±1.02 52.18±0.72 50.21±0.88 50.01±0.61 44.21±0.44 \n51.89±1.03 \n52.74±1.22 29.87±0.90 \n9.21±0.50 \n21.93±0.69 13.40±0.67 41.13±17.17 \nQwen2.5-14B \n65.39±0.63 70.30±0.80 62.11±1.11 62.36±0.87 60.21±0.80 55.08±0.72 \n61.56±0.36 \n62.42±0.64 36.86±0.26 21.16±1.27 36.07±0.24 33.20±0.48 52.23±15.38 \nQwen2.5-72B \n75.04±1.02 79.69±0.39 76.10±0.58 75.62±0.20 73.01±0.98 70.51±0.24 \n75.13±0.46 \n76.00±0.68 49.29±0.69 39.97±0.48 38.49±0.88 37.93±0.49 63.90±16.39 \nQwQ-32B \n82.33±0.43 84.98±0.55 83.71±0.49 83.55±0.39 75.60±0.48 72.91±0.85 \n83.80±0.87 \n84.81±0.86 62.36±1.09 32.08±1.21 30.49±0.88 32.43±1.34 67.42±21.77 \nQwen3-1.7B \n44.10±0.69 49.14±0.88 40.02±0.82 38.93±0.78 35.88±0.52 34.28±1.01 \n37.82±0.66 \n39.43±0.93 26.61±0.54 26.27±1.13 25.64±0.52 25.25±0.86 \n35.28±7.63 \nQwen3-4B \n59.54±1.08 64.45±0.72 54.27±1.07 53.20±0.84 53.01±0.63 45.10±0.72 \n54.52±0.48 \n55.99±1.02 22.91±0.82 \n9.96±0.69 \n16.98±1.06 \n9.69±1.06 \n41.64±19.82 \nQwen3-4B-thinking \n65.94±0.56 72.38±0.40 68.78±0.62 68.45±0.74 65.72±0.83 60.06±0.56 \n67.83±1.01 \n68.17±0.47 26.35±0.31 15.27±1.10 12.08±0.67 \n8.91±0.54 \n49.99±24.94 \nQwen3-8B \n67.41±0.62 71.72±0.41 52.30±0.57 62.66±0.67 60.60±0.72 56.42±0.63 \n61.60±1.07 \n61.48±1.03 21.02±1.23 \n8.83±0.39 \n21.98±0.84 10.43±0.84 46.37±22.72 \nQwen3-8B-thinking \n74.15±0.78 80.17±0.94 76.95±0.71 77.39±0.30 75.10±0.42 73.39±0.44 \n76.65±0.62 \n76.23±0.51 28.82±1.11 10.54±1.56 14.72±0.70 10.12±1.16 56.19±29.01 \nQwen3-14B \n73.24±0.97 76.73±0.56 68.52±0.44 70.12±0.78 66.57±0.77 61.19±0.94 \n70.15±0.81 \n69.79±0.39 41.54±0.75 11.23±0.61 16.67±0.85 14.56±0.48 53.36±24.37 \nQwen3-14B-thinking \n79.43±0.32 83.02±0.53 80.66±0.72 80.71±0.55 80.52±0.63 77.75±1.00 \n80.72±0.59 \n81.26±0.64 62.00±0.96 13.26±0.71 18.22±1.09 14.27±0.84 62.65±28.11 \nBaichuan-M2-32B \n82.89±1.02 85.47±0.29 73.39±0.83 78.98±1.28 77.17±0.96 70.17±1.06 \n77.56±0.51 \n75.60±0.81 30.93±0.72 24.26±0.77 28.92±1.81 27.24±1.72 61.05±24.04 \nBio-Medical-LLaMA-3-8B \n47.79±0.76 77.50±0.20 55.54±0.61 53.01±0.42 42.12±0.14 41.79±0.57 \n54.24±0.61 \n54.02±0.41 39.67±0.84 37.60±0.91 36.14±0.73 34.05±0.69 47.79±11.70 \nMediPhi \n28.63±0.65 52.63±1.18 43.17±1.13 42.56±0.95 24.82±1.51 27.57±0.72 \n40.06±1.10 \n40.80±0.84 19.14±1.12 22.09±0.76 18.37±1.33 23.41±0.46 31.94±10.97 \nMedGemma-4B \n47.67±0.91 62.88±0.84 52.94±0.97 54.60±0.45 47.89±1.13 43.57±0.43 \n53.01±0.64 \n53.56±1.20 42.81±0.68 16.47±0.66 19.37±1.29 28.80±1.16 43.63±14.12 \nMedGemma-27B \n82.06±0.70 87.26±0.67 83.19±0.71 83.30±0.48 80.55±1.30 79.23±0.85 \n83.52±0.59 \n83.82±0.82 76.62±1.16 18.30±1.40 46.73±1.06 58.74±0.94 71.94±19.98 \nMedReason-8B \n44.68±1.07 58.51±0.49 14.33±0.90 18.24±1.28 44.29±1.32 36.12±0.92 \n22.33±0.72 \n36.42±0.63 10.60±0.47 \n7.87±0.54 \n10.92±0.37 \n9.54±0.72 \n26.15±16.55 \nHuatuoGPT-o1-7B \n63.28±0.97 65.36±0.65 64.57±1.11 65.14±0.83 54.67±0.89 54.26±0.94 \n65.10±0.43 \n65.89±0.71 \n7.29±0.62 \n7.37±0.65 \n6.96±0.49 \n6.66±0.52 \n43.88±26.53 \nHuatuoGPT-o1-8B \n57.22±0.80 68.88±0.81 65.78±0.74 65.22±1.07 49.87±1.16 48.97±1.27 \n64.12±1.15 \n64.88±1.88 50.09±1.07 \n7.71±0.34 \n7.68±0.46 \n7.21±0.77 \n46.47±23.62 \nHuatuoGPT-o1-70B \n67.51±0.56 87.51±0.50 85.42±0.32 85.17±0.31 66.44±0.64 74.22±0.67 \n83.77±0.84 \n85.47±0.50 73.81±0.18 29.91±0.59 46.74±0.86 57.25±1.49 70.27±17.39 \nHuatuoGPT-o1-72B \n82.03±0.55 82.25±0.96 85.72±0.71 80.61±0.44 79.75±0.95 74.86±0.69 \n85.15±0.41 \n84.08±0.52 64.56±0.51 42.23±0.53 43.30±1.23 21.89±1.18 68.87±20.63 \nOpenBioLLM-8B \n13.86±0.88 25.36±0.99 16.39±1.07 25.34±0.84 10.62±0.78 \n9.77±0.82 \n29.18±0.63 \n30.68±1.22 11.25±0.70 \n7.73±0.79 \n9.36±1.12 \n11.44±0.37 \n16.75±8.20 \nOpenBioLLM-70B \n20.79±0.62 72.24±0.78 60.61±1.43 34.50±1.46 20.17±0.80 19.20±0.28 \n46.91±1.10 \n66.77±1.14 26.94±1.18 10.43±0.63 12.07±0.82 25.75±1.37 34.70±20.90 \nSTab. 111: Performance evaluation of 56 LLMs on MedQA. \n"}, {"page": 104, "text": " \n \n74 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n66.85 \n77.06 \n73.06 \n73.29 \n67.95 \n68.58 \n71.96 \n73.45 \n52.40 \n34.25 \n39.83 \n37.08 \nClaude-4.0-Sonnet \n89.71 \n92.07 \n88.69 \n89.87 \n89.95 \n90.02 \n91.12 \n91.28 \n82.01 \n59.23 \n70.07 \n75.33 \nGemini-2.5-Flash \n90.10 \n91.67 \n91.28 \n91.36 \n90.49 \n89.95 \n90.89 \n91.52 \n89.71 \n78.55 \n82.88 \n84.52 \nGPT-4o-mini \n67.71 \n78.32 \n71.56 \n73.06 \n69.21 \n65.12 \n72.58 \n74.16 \n60.80 \n33.23 \n42.89 \n51.30 \nGPT-4o \n86.49 \n89.32 \n88.53 \n88.85 \n85.07 \n85.00 \n89.24 \n89.16 \n82.56 \n34.25 \n49.80 \n69.29 \nGPT-4.1-nano \n66.61 \n82.48 \n72.98 \n70.15 \n63.94 \n61.51 \n72.35 \n74.39 \n52.71 \n29.22 \n38.88 \n46.43 \nGPT-4.1-mini \n85.31 \n91.36 \n87.04 \n88.14 \n85.39 \n83.35 \n88.06 \n88.85 \n76.98 \n25.14 \n60.02 \n68.34 \nGPT-4.1 \n88.61 \n90.26 \n90.26 \n89.08 \n90.57 \n86.10 \n91.20 \n89.00 \n87.90 \n51.30 \n68.26 \n75.18 \nGPT-5-nano \n50.27 \n69.84 \n65.59 \n70.23 \n65.83 \n55.77 \n63.39 \n72.43 \n57.42 \n21.13 \n38.10 \n43.21 \nGPT-5-mini \n85.78 \n91.67 \n87.43 \n87.51 \n87.59 \n84.13 \n88.37 \n89.63 \n80.36 \n34.80 \n61.67 \n73.21 \nGPT-5 \n90.81 \n92.69 \n92.22 \n90.81 \n91.12 \n83.82 \n92.30 \n92.46 \n88.92 \n53.89 \n73.84 \n75.73 \no4-mini \n93.09 \n94.03 \n93.01 \n93.40 \n93.24 \n91.67 \n94.03 \n92.77 \n91.36 \n37.63 \n78.87 \n85.55 \nOpen-Weight LLMs \nDeepSeek-V3 \n85.15 \n88.85 \n85.39 \n86.25 \n85.15 \n82.64 \n86.41 \n87.59 \n75.57 \n44.07 \n56.64 \n61.04 \nDeepSeek-R1 \n91.12 \n91.20 \n90.57 \n90.81 \n90.34 \n80.68 \n92.14 \n91.36 \n87.90 \n65.36 \n72.19 \n78.71 \nDeepSeek-R1-Qwen3-8B \n72.11 \n80.13 \n68.03 \n70.93 \n67.71 \n67.71 \n72.11 \n68.66 \n33.07 \n10.05 \n12.02 \n12.25 \nGemma-3-4B \n43.60 \n52.79 \n46.66 \n45.64 \n41.16 \n40.14 \n45.56 \n46.58 \n37.47 \n8.72 \n16.73 \n14.53 \nGemma-3-12B \n62.29 \n70.86 \n65.83 \n65.51 \n61.74 \n58.29 \n63.39 \n65.99 \n55.54 \n9.11 \n35.74 \n41.95 \nGemma-3-27B \n70.23 \n75.26 \n71.48 \n73.13 \n68.66 \n68.89 \n72.03 \n72.51 \n65.91 \n18.38 \n46.03 \n53.10 \ngpt-oss-20B \n83.03 \n79.42 \n85.39 \n84.60 \n83.58 \n80.28 \n84.13 \n84.29 \n68.97 \n29.54 \n60.64 \n65.67 \ngpt-oss-120B \n88.61 \n90.89 \n91.04 \n90.10 \n89.24 \n86.80 \n90.42 \n91.36 \n80.68 \n55.22 \n71.56 \n77.38 \nLLaMA-3.1-8B \n45.33 \n67.95 \n48.47 \n49.10 \n39.28 \n32.05 \n47.92 \n49.80 \n36.92 \n27.02 \n16.97 \n11.63 \nLLaMA-3.1-70B \n70.93 \n84.76 \n76.67 \n77.38 \n68.58 \n62.77 \n77.45 \n78.71 \n61.35 \n42.89 \n41.95 \n46.98 \nLLaMA-3.2-3B \n39.12 \n57.66 \n39.91 \n35.59 \n33.39 \n26.47 \n38.26 \n34.56 \n29.22 \n15.48 \n16.97 \n14.85 \nLLaMA-3.3-70B \n61.04 \n85.00 \n81.23 \n81.62 \n59.39 \n70.31 \n80.75 \n81.30 \n67.56 \n40.61 \n42.50 \n47.84 \nLLaMA-4-Scout \n79.03 \n85.62 \n81.15 \n82.64 \n79.50 \n77.61 \n82.33 \n82.33 \n73.84 \n46.82 \n46.43 \n64.02 \nLLaMA-4-Maverick \n85.55 \n89.95 \n88.14 \n87.04 \n87.20 \n85.23 \n88.85 \n88.14 \n83.97 \n56.48 \n69.21 \n75.41 \nMistral-7B-v0.3 \n22.23 \n26.79 \n13.59 \n30.16 \n21.13 \n18.15 \n22.39 \n27.97 \n20.66 \n16.42 \n9.66 \n10.21 \nMistral-Small-3.1-24B \n64.10 \n75.65 \n64.41 \n67.16 \n58.92 \n54.36 \n68.34 \n69.99 \n32.76 \n12.41 \n14.93 \n18.54 \nPhi-4-mini \n33.39 \n54.44 \n37.47 \n37.16 \n31.81 \n24.27 \n35.51 \n38.49 \n26.63 \n17.75 \n19.56 \n16.65 \nPhi-4-mini-Reasoning \n29.77 \n65.59 \n48.55 \n48.08 \n24.35 \n14.77 \n41.63 \n34.88 \n29.46 \n24.90 \n22.94 \n22.62 \nPhi-4 \n57.34 \n79.97 \n69.44 \n71.72 \n58.60 \n53.81 \n70.70 \n69.68 \n47.60 \n25.92 \n37.23 \n30.01 \nPhi-4-Reasoning \n80.99 \n85.07 \n84.05 \n85.70 \n82.48 \n80.28 \n76.28 \n83.66 \n72.03 \n12.96 \n46.98 \n29.14 \nQwen2.5-3B \n47.45 \n48.39 \n40.69 \n39.51 \n40.30 \n33.31 \n38.49 \n39.43 \n15.87 \n9.11 \n11.15 \n8.80 \nQwen2.5-7B \n57.82 \n60.80 \n53.02 \n50.90 \n50.51 \n44.93 \n52.00 \n53.89 \n30.24 \n8.80 \n22.55 \n13.75 \nQwen2.5-14B \n65.12 \n70.93 \n62.45 \n61.59 \n60.72 \n56.17 \n61.74 \n62.69 \n37.08 \n21.05 \n35.90 \n33.86 \nQwen2.5-72B \n73.84 \n79.58 \n75.96 \n75.81 \n74.00 \n70.46 \n75.88 \n76.83 \n48.39 \n39.83 \n39.75 \n37.47 \nQwQ-32B \n81.70 \n84.92 \n83.82 \n83.35 \n75.26 \n74.08 \n83.58 \n85.23 \n62.14 \n32.13 \n30.24 \n34.41 \nQwen3-1.7B \n43.28 \n49.41 \n40.77 \n38.41 \n35.59 \n35.43 \n38.49 \n38.02 \n26.32 \n26.63 \n26.08 \n25.45 \nQwen3-4B \n59.54 \n64.73 \n54.75 \n52.24 \n52.24 \n44.07 \n54.12 \n56.95 \n22.86 \n10.45 \n16.03 \n8.64 \nQwen3-4B-thinking \n66.14 \n72.19 \n69.05 \n68.26 \n65.91 \n59.78 \n67.32 \n68.03 \n26.79 \n14.61 \n12.57 \n8.25 \nQwen3-8B \n67.16 \n72.03 \n52.71 \n62.22 \n61.35 \n57.11 \n61.27 \n62.92 \n22.15 \n9.27 \n21.45 \n11.55 \nQwen3-8B-thinking \n74.47 \n79.03 \n77.38 \n77.77 \n74.94 \n74.00 \n76.98 \n75.88 \n28.52 \n9.90 \n13.90 \n11.08 \nQwen3-14B \n73.29 \n77.22 \n68.74 \n69.91 \n66.14 \n60.17 \n71.01 \n69.76 \n41.16 \n12.10 \n17.99 \n13.75 \nQwen3-14B-thinking \n79.58 \n83.03 \n80.75 \n81.30 \n79.65 \n79.03 \n80.75 \n81.70 \n61.43 \n14.06 \n17.67 \n13.67 \nBaichuan-M2-32B \n82.40 \n85.15 \n74.23 \n80.36 \n76.98 \n71.41 \n77.61 \n76.43 \n30.95 \n25.06 \n29.54 \n25.45 \nBio-Medical-LLaMA-3-8B \n48.08 \n77.53 \n54.91 \n52.95 \n42.11 \n41.87 \n53.65 \n53.89 \n40.85 \n36.92 \n37.08 \n34.25 \nMediPhi \n27.89 \n52.71 \n43.83 \n43.28 \n26.39 \n26.71 \n41.71 \n40.14 \n19.95 \n21.45 \n17.75 \n23.17 \nMedGemma-4B \n46.27 \n62.53 \n52.47 \n54.60 \n48.47 \n43.68 \n52.79 \n54.67 \n41.79 \n17.44 \n20.35 \n27.97 \nMedGemma-27B \n82.33 \n88.22 \n82.17 \n82.88 \n79.42 \n79.89 \n83.27 \n84.76 \n77.61 \n19.32 \n47.76 \n59.47 \nMedReason-8B \n44.62 \n58.37 \n15.48 \n19.80 \n43.21 \n35.98 \n22.47 \n37.31 \n10.29 \n7.78 \n11.39 \n9.82 \nHuatuoGPT-o1-7B \n64.49 \n65.12 \n66.22 \n65.75 \n54.52 \n53.02 \n64.49 \n65.99 \n6.60 \n7.93 \n6.99 \n7.46 \nHuatuoGPT-o1-8B \n56.87 \n67.95 \n65.51 \n66.22 \n49.96 \n46.90 \n63.71 \n65.59 \n50.35 \n7.31 \n7.86 \n7.86 \nHuatuoGPT-o1-70B \n67.24 \n87.75 \n85.00 \n85.31 \n66.93 \n75.18 \n83.82 \n85.23 \n73.84 \n29.69 \n47.45 \n58.76 \nHuatuoGPT-o1-72B \n82.09 \n82.80 \n86.88 \n80.75 \n80.13 \n75.41 \n85.31 \n83.66 \n64.81 \n42.81 \n42.97 \n20.42 \nOpenBioLLM-8B \n13.98 \n24.12 \n16.03 \n24.67 \n11.94 \n9.74 \n29.22 \n29.14 \n11.31 \n8.01 \n10.29 \n11.86 \nOpenBioLLM-70B \n20.27 \n72.82 \n60.57 \n36.76 \n20.82 \n19.17 \n47.68 \n66.85 \n26.55 \n11.00 \n11.08 \n24.19 \nSTab. 112: Zero-Shot performance evaluation of 56 LLMs on MedQA (Run 1). \n"}, {"page": 105, "text": " \n \n75 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n66.85 \n77.06 \n73.06 \n73.29 \n68.11 \n68.74 \n72.11 \n73.53 \n52.63 \n34.41 \n39.83 \n37.08 \nClaude-4.0-Sonnet \n89.95 \n92.38 \n89.95 \n89.87 \n89.71 \n89.32 \n91.12 \n90.10 \n81.46 \n58.92 \n70.78 \n75.33 \nGemini-2.5-Flash \n89.55 \n90.81 \n90.65 \n91.44 \n90.18 \n89.95 \n91.12 \n91.44 \n89.63 \n80.75 \n82.64 \n84.21 \nGPT-4o-mini \n68.34 \n78.08 \n72.27 \n74.00 \n69.05 \n65.83 \n73.21 \n73.37 \n60.02 \n32.76 \n43.60 \n51.53 \nGPT-4o \n86.10 \n89.16 \n88.61 \n87.82 \n84.84 \n85.39 \n88.85 \n89.08 \n82.17 \n31.66 \n49.80 \n69.84 \nGPT-4.1-nano \n66.61 \n82.01 \n73.84 \n72.51 \n63.63 \n60.09 \n72.58 \n73.45 \n52.63 \n27.73 \n37.31 \n44.85 \nGPT-4.1-mini \n86.80 \n91.20 \n87.20 \n87.59 \n85.70 \n81.23 \n87.98 \n87.82 \n77.93 \n27.10 \n58.60 \n67.32 \nGPT-4.1 \n89.24 \n90.26 \n89.87 \n89.08 \n90.42 \n86.02 \n91.52 \n90.42 \n88.85 \n52.55 \n68.26 \n75.41 \nGPT-5-nano \n52.95 \n67.64 \n64.65 \n69.44 \n64.73 \n52.95 \n61.59 \n70.93 \n57.34 \n20.35 \n37.78 \n45.40 \nGPT-5-mini \n86.33 \n91.04 \n87.82 \n88.77 \n88.14 \n84.92 \n88.69 \n88.69 \n80.91 \n37.31 \n62.53 \n71.56 \nGPT-5 \n91.67 \n92.62 \n92.93 \n91.75 \n90.73 \n84.21 \n92.69 \n91.52 \n89.08 \n53.26 \n75.18 \n75.18 \no4-mini \n93.79 \n94.58 \n93.32 \n93.09 \n93.56 \n92.38 \n93.87 \n93.95 \n91.12 \n41.48 \n80.68 \n86.49 \nOpen-Weight LLMs \nDeepSeek-V3 \n85.70 \n89.40 \n86.49 \n86.25 \n85.31 \n82.72 \n86.33 \n86.96 \n74.47 \n45.01 \n56.25 \n61.12 \nDeepSeek-R1 \n91.20 \n91.59 \n90.26 \n90.49 \n89.95 \n81.07 \n92.07 \n91.83 \n88.06 \n66.38 \n72.19 \n78.48 \nDeepSeek-R1-Qwen3-8B \n73.37 \n79.73 \n68.42 \n70.78 \n66.54 \n67.24 \n71.48 \n69.05 \n31.97 \n11.23 \n10.45 \n13.04 \nGemma-3-4B \n44.70 \n51.92 \n45.40 \n43.75 \n42.34 \n40.61 \n46.50 \n46.82 \n37.23 \n10.13 \n15.48 \n14.45 \nGemma-3-12B \n61.27 \n69.76 \n65.20 \n64.41 \n60.33 \n60.02 \n64.34 \n65.36 \n55.38 \n10.53 \n36.92 \n45.33 \nGemma-3-27B \n69.99 \n76.36 \n71.48 \n71.88 \n70.31 \n67.87 \n71.48 \n72.35 \n64.41 \n18.22 \n44.93 \n52.87 \ngpt-oss-20B \n82.72 \n80.13 \n84.45 \n82.95 \n81.23 \n80.99 \n82.88 \n84.21 \n68.58 \n33.23 \n59.78 \n66.46 \ngpt-oss-120B \n89.24 \n92.14 \n90.49 \n89.87 \n89.87 \n87.75 \n89.79 \n90.18 \n81.15 \n54.28 \n72.58 \n77.22 \nLLaMA-3.1-8B \n44.93 \n67.40 \n49.10 \n49.49 \n38.73 \n33.23 \n50.51 \n48.70 \n37.39 \n25.92 \n17.67 \n13.28 \nLLaMA-3.1-70B \n71.72 \n82.95 \n77.69 \n76.83 \n69.84 \n63.94 \n76.20 \n77.77 \n61.67 \n39.91 \n44.15 \n46.27 \nLLaMA-3.2-3B \n38.49 \n57.66 \n41.08 \n35.27 \n34.41 \n28.52 \n38.18 \n36.84 \n30.24 \n14.93 \n18.54 \n15.95 \nLLaMA-3.3-70B \n62.14 \n84.84 \n79.81 \n80.20 \n58.68 \n70.62 \n79.81 \n81.93 \n67.48 \n40.46 \n42.97 \n46.58 \nLLaMA-4-Scout \n80.60 \n86.10 \n82.64 \n82.95 \n78.95 \n77.77 \n81.46 \n81.62 \n74.78 \n46.35 \n46.58 \n63.47 \nLLaMA-4-Maverick \n85.55 \n90.57 \n88.92 \n88.22 \n85.39 \n85.23 \n89.08 \n88.53 \n84.68 \n57.74 \n69.05 \n76.12 \nMistral-7B-v0.3 \n23.72 \n31.42 \n14.85 \n28.44 \n18.93 \n18.07 \n20.82 \n29.30 \n18.85 \n15.40 \n10.68 \n10.92 \nMistral-Small-3.1-24B \n63.39 \n74.23 \n67.48 \n68.03 \n59.94 \n54.83 \n67.09 \n68.42 \n30.87 \n11.70 \n18.62 \n18.54 \nPhi-4-mini \n33.39 \n53.50 \n35.66 \n37.31 \n31.50 \n25.29 \n35.35 \n36.53 \n27.57 \n17.36 \n18.62 \n16.03 \nPhi-4-mini-Reasoning \n29.07 \n65.59 \n48.47 \n48.39 \n22.78 \n13.98 \n40.93 \n35.19 \n27.26 \n22.39 \n21.13 \n21.13 \nPhi-4 \n59.47 \n79.89 \n69.05 \n71.72 \n58.76 \n53.81 \n70.93 \n71.72 \n45.25 \n24.90 \n37.78 \n32.91 \nPhi-4-Reasoning \n81.15 \n85.39 \n83.97 \n85.55 \n84.37 \n81.07 \n74.94 \n84.13 \n72.11 \n12.96 \n48.86 \n29.77 \nQwen2.5-3B \n46.98 \n49.80 \n40.30 \n40.69 \n38.88 \n34.09 \n38.02 \n39.51 \n13.90 \n9.90 \n11.47 \n9.43 \nQwen2.5-7B \n56.79 \n60.49 \n51.45 \n50.67 \n49.73 \n43.75 \n50.90 \n53.02 \n28.44 \n8.88 \n21.60 \n13.59 \nQwen2.5-14B \n64.81 \n70.70 \n60.96 \n61.82 \n60.57 \n54.28 \n61.35 \n62.14 \n36.76 \n19.64 \n36.29 \n32.84 \nQwen2.5-72B \n76.59 \n79.97 \n75.65 \n75.41 \n73.53 \n70.23 \n74.63 \n76.28 \n49.80 \n40.14 \n38.73 \n37.63 \nQwQ-32B \n82.17 \n84.76 \n83.90 \n83.58 \n75.26 \n72.11 \n84.60 \n84.76 \n61.98 \n30.64 \n30.16 \n32.13 \nQwen3-1.7B \n43.52 \n47.68 \n40.77 \n38.49 \n36.29 \n34.88 \n38.57 \n38.96 \n26.71 \n25.61 \n25.77 \n25.45 \nQwen3-4B \n60.72 \n64.49 \n54.20 \n53.34 \n53.73 \n45.09 \n53.97 \n56.72 \n21.60 \n9.66 \n16.10 \n8.96 \nQwen3-4B-thinking \n66.69 \n72.03 \n69.36 \n68.97 \n64.26 \n60.72 \n69.60 \n68.81 \n25.92 \n15.32 \n12.88 \n9.58 \nQwen3-8B \n68.42 \n71.96 \n52.95 \n63.79 \n60.72 \n55.70 \n60.57 \n60.80 \n20.90 \n8.25 \n22.78 \n10.60 \nQwen3-8B-thinking \n75.33 \n81.15 \n76.98 \n77.53 \n74.63 \n73.21 \n77.45 \n75.88 \n29.30 \n9.82 \n14.38 \n9.35 \nQwen3-14B \n72.90 \n76.90 \n68.97 \n69.21 \n67.71 \n61.12 \n68.89 \n69.13 \n41.40 \n10.76 \n16.03 \n15.00 \nQwen3-14B-thinking \n79.10 \n82.64 \n79.97 \n80.83 \n80.13 \n77.22 \n81.54 \n80.44 \n60.72 \n13.67 \n17.05 \n14.53 \nBaichuan-M2-32B \n83.58 \n85.47 \n72.90 \n78.95 \n78.48 \n68.74 \n78.32 \n74.86 \n32.13 \n23.25 \n30.71 \n26.71 \nBio-Medical-LLaMA-3-8B \n48.63 \n77.45 \n55.38 \n53.57 \n42.34 \n41.63 \n53.81 \n53.50 \n39.59 \n38.10 \n36.37 \n33.94 \nMediPhi \n29.22 \n52.40 \n43.44 \n41.32 \n24.74 \n28.36 \n39.36 \n40.22 \n17.60 \n22.94 \n20.58 \n23.72 \nMedGemma-4B \n48.15 \n61.82 \n53.97 \n54.28 \n46.11 \n42.89 \n54.05 \n53.42 \n43.21 \n15.63 \n18.54 \n28.52 \nMedGemma-27B \n81.78 \n86.72 \n83.74 \n83.27 \n82.01 \n78.40 \n83.97 \n84.13 \n74.71 \n18.15 \n47.53 \n59.94 \nMedReason-8B \n43.28 \n58.37 \n14.14 \n18.93 \n45.80 \n37.47 \n21.84 \n35.90 \n10.37 \n7.78 \n11.23 \n10.37 \nHuatuoGPT-o1-7B \n62.22 \n66.46 \n64.18 \n65.99 \n54.75 \n54.36 \n65.44 \n66.46 \n6.83 \n8.01 \n6.44 \n6.68 \nHuatuoGPT-o1-8B \n56.48 \n69.76 \n65.36 \n66.22 \n49.96 \n49.96 \n65.59 \n64.10 \n51.30 \n7.46 \n7.07 \n7.86 \nHuatuoGPT-o1-70B \n66.93 \n88.06 \n85.31 \n84.84 \n65.75 \n74.16 \n84.13 \n84.76 \n74.00 \n29.77 \n45.88 \n57.34 \nHuatuoGPT-o1-72B \n82.01 \n81.38 \n85.15 \n80.20 \n80.52 \n75.73 \n85.78 \n84.52 \n65.20 \n42.42 \n42.50 \n21.13 \nOpenBioLLM-8B \n13.83 \n26.71 \n17.60 \n24.59 \n10.45 \n11.15 \n29.30 \n31.42 \n12.33 \n7.93 \n9.90 \n11.31 \nOpenBioLLM-70B \n20.03 \n72.03 \n60.72 \n33.23 \n20.74 \n18.77 \n48.08 \n66.22 \n28.04 \n10.45 \n11.78 \n27.57 \nSTab. 113: Zero-Shot performance evaluation of 56 LLMs on MedQA (Run 2). \n"}, {"page": 106, "text": " \n \n76 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n66.85 \n77.06 \n73.06 \n73.29 \n68.03 \n68.81 \n72.11 \n73.45 \n52.55 \n34.33 \n39.83 \n37.00 \nClaude-4.0-Sonnet \n88.92 \n91.75 \n89.40 \n90.34 \n90.26 \n90.42 \n91.12 \n90.97 \n82.01 \n59.54 \n71.41 \n74.39 \nGemini-2.5-Flash \n90.18 \n91.04 \n90.89 \n91.52 \n89.95 \n89.87 \n91.52 \n90.57 \n89.32 \n79.18 \n83.66 \n84.92 \nGPT-4o-mini \n67.56 \n77.38 \n72.11 \n73.53 \n67.24 \n65.36 \n73.45 \n72.90 \n60.49 \n32.05 \n44.30 \n49.96 \nGPT-4o \n85.47 \n89.32 \n89.16 \n88.61 \n87.20 \n85.07 \n89.00 \n88.92 \n83.03 \n32.36 \n48.70 \n69.21 \nGPT-4.1-nano \n67.87 \n80.75 \n72.27 \n71.17 \n64.41 \n61.12 \n71.25 \n74.16 \n52.63 \n27.10 \n37.23 \n46.43 \nGPT-4.1-mini \n84.52 \n91.04 \n85.55 \n88.06 \n85.39 \n82.25 \n87.20 \n87.90 \n79.18 \n25.77 \n57.89 \n68.50 \nGPT-4.1 \n89.24 \n89.71 \n89.95 \n89.55 \n89.63 \n87.43 \n91.91 \n89.55 \n88.77 \n51.85 \n67.79 \n76.67 \nGPT-5-nano \n49.88 \n70.93 \n66.61 \n69.13 \n65.67 \n55.38 \n62.77 \n70.31 \n57.42 \n17.99 \n39.36 \n43.21 \nGPT-5-mini \n85.23 \n90.73 \n88.45 \n87.75 \n87.98 \n84.68 \n89.40 \n88.45 \n82.01 \n35.27 \n61.35 \n69.99 \nGPT-5 \n91.12 \n93.24 \n91.67 \n90.81 \n91.04 \n84.45 \n92.07 \n92.07 \n89.16 \n54.05 \n75.33 \n74.39 \no4-mini \n93.32 \n94.58 \n93.32 \n93.56 \n93.87 \n92.62 \n94.27 \n93.87 \n91.28 \n41.16 \n81.30 \n85.70 \nOpen-Weight LLMs \nDeepSeek-V3 \n86.96 \n89.63 \n86.80 \n86.80 \n85.47 \n82.33 \n87.04 \n87.98 \n75.02 \n44.30 \n54.44 \n62.22 \nDeepSeek-R1 \n90.81 \n91.44 \n90.81 \n90.97 \n90.18 \n80.99 \n91.59 \n91.44 \n87.04 \n65.75 \n73.37 \n80.20 \nDeepSeek-R1-Qwen3-8B \n73.37 \n79.18 \n68.81 \n70.23 \n66.85 \n69.99 \n72.35 \n69.60 \n35.19 \n11.31 \n11.39 \n12.49 \nGemma-3-4B \n43.91 \n51.06 \n44.78 \n44.23 \n42.73 \n41.79 \n46.35 \n47.37 \n38.49 \n10.53 \n16.89 \n14.45 \nGemma-3-12B \n60.57 \n69.13 \n64.26 \n63.94 \n61.12 \n60.57 \n63.71 \n66.93 \n55.62 \n9.82 \n36.68 \n44.46 \nGemma-3-27B \n69.29 \n75.81 \n72.58 \n73.06 \n69.29 \n67.56 \n73.21 \n72.74 \n64.57 \n20.03 \n46.50 \n52.71 \ngpt-oss-20B \n82.80 \n80.28 \n83.82 \n84.13 \n82.72 \n79.97 \n83.90 \n83.82 \n66.46 \n30.71 \n61.27 \n68.03 \ngpt-oss-120B \n90.65 \n91.44 \n90.26 \n90.65 \n90.42 \n88.37 \n90.97 \n90.65 \n81.38 \n53.89 \n72.51 \n77.53 \nLLaMA-3.1-8B \n45.09 \n66.30 \n50.27 \n46.82 \n38.41 \n33.86 \n49.73 \n50.43 \n35.19 \n24.90 \n17.75 \n12.41 \nLLaMA-3.1-70B \n72.11 \n82.72 \n76.51 \n76.83 \n69.60 \n62.22 \n75.10 \n77.93 \n62.37 \n39.75 \n43.44 \n47.21 \nLLaMA-3.2-3B \n40.06 \n55.77 \n40.61 \n34.41 \n34.17 \n26.79 \n38.81 \n34.80 \n31.34 \n16.89 \n17.75 \n15.32 \nLLaMA-3.3-70B \n60.80 \n85.07 \n80.75 \n81.30 \n58.76 \n71.25 \n80.83 \n80.83 \n68.11 \n42.18 \n43.28 \n47.92 \nLLaMA-4-Scout \n78.95 \n85.55 \n82.17 \n82.25 \n77.53 \n78.79 \n82.09 \n82.95 \n74.94 \n47.84 \n46.66 \n64.18 \nLLaMA-4-Maverick \n86.72 \n90.57 \n88.77 \n89.08 \n86.02 \n86.10 \n89.63 \n88.85 \n84.52 \n56.95 \n68.74 \n76.43 \nMistral-7B-v0.3 \n22.15 \n29.14 \n15.87 \n28.67 \n20.58 \n19.09 \n21.13 \n27.26 \n20.50 \n16.73 \n10.53 \n9.90 \nMistral-Small-3.1-24B \n63.63 \n76.98 \n63.79 \n69.68 \n59.70 \n54.99 \n68.89 \n68.74 \n32.36 \n11.47 \n14.93 \n20.58 \nPhi-4-mini \n31.26 \n54.83 \n36.68 \n38.57 \n32.99 \n24.12 \n37.55 \n37.63 \n26.00 \n17.67 \n18.22 \n16.81 \nPhi-4-mini-Reasoning \n30.40 \n64.41 \n47.13 \n47.53 \n23.02 \n13.51 \n38.88 \n34.72 \n26.39 \n22.55 \n21.37 \n20.66 \nPhi-4 \n60.02 \n79.58 \n71.25 \n70.38 \n61.90 \n53.42 \n71.17 \n71.33 \n47.60 \n25.06 \n36.53 \n30.48 \nPhi-4-Reasoning \n80.83 \n86.25 \n84.29 \n83.58 \n83.03 \n80.68 \n73.92 \n84.52 \n73.06 \n13.59 \n47.60 \n30.24 \nQwen2.5-3B \n44.23 \n48.39 \n41.32 \n41.95 \n39.36 \n33.15 \n39.36 \n39.04 \n15.08 \n9.35 \n10.53 \n9.35 \nQwen2.5-7B \n56.40 \n62.06 \n52.87 \n50.59 \n50.75 \n44.07 \n50.90 \n52.55 \n29.69 \n9.19 \n21.21 \n12.25 \nQwen2.5-14B \n66.14 \n71.01 \n61.67 \n63.79 \n59.47 \n55.15 \n61.51 \n61.43 \n37.00 \n23.17 \n35.74 \n32.99 \nQwen2.5-72B \n75.26 \n79.42 \n76.67 \n75.81 \n73.06 \n70.38 \n75.02 \n75.26 \n48.94 \n39.43 \n37.31 \n38.73 \nQwQ-32B \n82.40 \n85.94 \n83.11 \n84.05 \n75.49 \n72.27 \n84.68 \n86.02 \n61.51 \n33.62 \n29.46 \n31.19 \nQwen3-1.7B \n44.54 \n50.04 \n39.28 \n38.33 \n35.74 \n34.56 \n37.16 \n39.98 \n25.84 \n25.92 \n25.14 \n24.27 \nQwen3-4B \n60.25 \n65.44 \n55.62 \n54.52 \n52.55 \n45.95 \n55.15 \n56.40 \n23.33 \n10.92 \n17.52 \n9.74 \nQwen3-4B-thinking \n65.28 \n72.98 \n68.42 \n69.05 \n65.99 \n59.39 \n67.09 \n67.71 \n26.32 \n16.89 \n11.86 \n9.11 \nQwen3-8B \n67.48 \n71.01 \n51.61 \n62.45 \n61.12 \n55.85 \n63.24 \n61.90 \n20.74 \n9.11 \n23.02 \n10.29 \nQwen3-8B-thinking \n73.92 \n81.15 \n77.69 \n76.98 \n75.73 \n73.61 \n76.20 \n75.81 \n30.40 \n11.39 \n14.69 \n9.03 \nQwen3-14B \n72.03 \n75.96 \n68.26 \n70.93 \n66.85 \n62.37 \n70.15 \n70.07 \n41.48 \n11.15 \n16.97 \n14.53 \nQwen3-14B-thinking \n79.10 \n83.27 \n80.28 \n80.91 \n81.23 \n77.06 \n80.60 \n81.30 \n61.98 \n13.28 \n18.46 \n13.83 \nBaichuan-M2-32B \n81.78 \n85.23 \n72.98 \n76.90 \n77.14 \n69.44 \n76.98 \n74.71 \n30.79 \n23.72 \n29.62 \n26.16 \nBio-Medical-LLaMA-3-8B \n48.23 \n77.53 \n56.32 \n52.95 \n41.95 \n41.24 \n54.05 \n53.81 \n38.57 \n38.96 \n35.43 \n35.04 \nMediPhi \n29.38 \n51.85 \n43.75 \n43.60 \n26.24 \n27.34 \n40.22 \n41.87 \n20.11 \n22.70 \n18.62 \n24.04 \nMedGemma-4B \n48.55 \n63.16 \n51.53 \n54.36 \n49.02 \n43.52 \n52.32 \n53.26 \n43.60 \n16.58 \n17.52 \n30.40 \nMedGemma-27B \n81.07 \n86.57 \n82.72 \n82.88 \n81.85 \n79.58 \n82.64 \n82.56 \n76.98 \n17.91 \n46.43 \n58.52 \nMedReason-8B \n44.46 \n58.68 \n13.28 \n18.38 \n43.13 \n35.11 \n23.33 \n36.53 \n10.68 \n8.33 \n10.76 \n8.48 \nHuatuoGPT-o1-7B \n64.10 \n64.81 \n63.47 \n63.86 \n54.75 \n55.46 \n64.96 \n66.69 \n8.17 \n7.07 \n7.62 \n6.28 \nHuatuoGPT-o1-8B \n57.42 \n69.68 \n66.06 \n64.26 \n48.86 \n49.80 \n62.92 \n62.92 \n50.82 \n8.09 \n8.33 \n5.97 \nHuatuoGPT-o1-70B \n68.11 \n87.59 \n85.70 \n85.62 \n65.83 \n74.16 \n84.52 \n85.86 \n73.76 \n29.46 \n46.98 \n58.44 \nHuatuoGPT-o1-72B \n82.88 \n82.33 \n85.15 \n80.52 \n79.42 \n74.71 \n84.76 \n83.66 \n64.26 \n42.42 \n41.95 \n23.49 \nOpenBioLLM-8B \n12.96 \n25.06 \n17.44 \n24.98 \n10.13 \n9.19 \n28.91 \n32.05 \n10.45 \n6.60 \n10.29 \n11.70 \nOpenBioLLM-70B \n21.45 \n71.01 \n59.39 \n33.46 \n20.58 \n19.40 \n46.19 \n68.58 \n27.57 \n10.37 \n12.10 \n26.63 \nSTab. 114: Zero-Shot performance evaluation of 56 LLMs on MedQA (Run 3). \n"}, {"page": 107, "text": " \n \n77 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n66.93 \n77.06 \n72.98 \n73.29 \n68.03 \n68.81 \n72.19 \n73.45 \n52.55 \n34.41 \n39.83 \n37.08 \nClaude-4.0-Sonnet \n89.79 \n91.75 \n90.18 \n90.18 \n89.63 \n89.79 \n91.36 \n90.57 \n81.78 \n59.07 \n70.54 \n74.16 \nGemini-2.5-Flash \n90.26 \n91.28 \n90.65 \n91.20 \n89.87 \n90.42 \n90.89 \n91.04 \n89.47 \n78.95 \n83.03 \n85.07 \nGPT-4o-mini \n67.32 \n77.30 \n70.78 \n72.98 \n69.13 \n64.96 \n72.27 \n73.92 \n61.43 \n32.05 \n43.91 \n49.65 \nGPT-4o \n86.10 \n88.77 \n87.98 \n88.45 \n85.07 \n84.52 \n89.40 \n88.69 \n83.35 \n31.26 \n50.20 \n68.50 \nGPT-4.1-nano \n65.75 \n80.68 \n71.41 \n71.41 \n61.19 \n59.39 \n74.08 \n74.00 \n53.81 \n27.97 \n39.75 \n45.01 \nGPT-4.1-mini \n85.94 \n90.65 \n85.47 \n87.82 \n85.55 \n82.72 \n87.98 \n87.90 \n78.55 \n24.59 \n57.11 \n66.38 \nGPT-4.1 \n89.55 \n89.87 \n89.95 \n89.16 \n89.63 \n87.51 \n91.44 \n89.08 \n88.85 \n51.45 \n68.26 \n75.65 \nGPT-5-nano \n51.85 \n68.42 \n65.91 \n69.36 \n64.41 \n56.01 \n63.47 \n72.82 \n56.79 \n19.48 \n38.81 \n43.91 \nGPT-5-mini \n87.12 \n91.44 \n89.79 \n88.06 \n87.20 \n84.45 \n89.87 \n89.00 \n80.52 \n35.90 \n60.33 \n71.64 \nGPT-5 \n91.44 \n93.17 \n91.44 \n90.89 \n91.12 \n84.68 \n93.40 \n91.52 \n89.08 \n55.46 \n75.02 \n75.41 \no4-mini \n92.85 \n94.74 \n94.11 \n93.40 \n93.24 \n92.07 \n93.64 \n92.69 \n91.20 \n39.51 \n80.60 \n85.15 \nOpen-Weight LLMs \nDeepSeek-V3 \n86.88 \n88.61 \n85.00 \n87.04 \n86.17 \n83.74 \n86.10 \n86.72 \n74.86 \n44.15 \n57.34 \n61.67 \nDeepSeek-R1 \n90.97 \n91.67 \n91.36 \n91.83 \n91.12 \n81.62 \n91.04 \n91.20 \n87.51 \n67.16 \n72.43 \n78.32 \nDeepSeek-R1-Qwen3-8B \n72.98 \n79.65 \n70.46 \n70.46 \n66.22 \n69.44 \n72.11 \n69.84 \n32.52 \n11.08 \n10.92 \n12.41 \nGemma-3-4B \n43.44 \n52.00 \n45.80 \n45.88 \n41.71 \n39.20 \n46.50 \n46.66 \n37.00 \n8.96 \n16.65 \n15.24 \nGemma-3-12B \n62.53 \n69.44 \n65.20 \n65.75 \n60.33 \n60.57 \n64.81 \n66.38 \n57.74 \n10.60 \n36.53 \n42.34 \nGemma-3-27B \n69.13 \n77.14 \n72.82 \n72.82 \n69.99 \n68.81 \n72.43 \n72.35 \n66.22 \n18.85 \n47.29 \n54.05 \ngpt-oss-20B \n82.95 \n81.07 \n83.27 \n83.42 \n83.19 \n80.52 \n84.60 \n83.90 \n68.42 \n30.48 \n62.06 \n66.85 \ngpt-oss-120B \n89.08 \n91.83 \n89.79 \n89.95 \n89.08 \n87.59 \n90.49 \n90.89 \n81.85 \n55.07 \n71.33 \n78.16 \nLLaMA-3.1-8B \n44.62 \n66.85 \n48.00 \n49.25 \n38.49 \n34.80 \n49.80 \n49.18 \n35.74 \n25.53 \n16.42 \n13.35 \nLLaMA-3.1-70B \n70.70 \n82.40 \n75.49 \n76.75 \n67.87 \n61.82 \n76.90 \n78.63 \n61.90 \n38.41 \n43.68 \n48.00 \nLLaMA-3.2-3B \n40.22 \n56.64 \n41.16 \n35.82 \n34.33 \n26.39 \n36.06 \n37.31 \n31.34 \n16.18 \n17.67 \n15.95 \nLLaMA-3.3-70B \n61.43 \n84.68 \n79.03 \n80.13 \n58.37 \n70.38 \n80.83 \n80.13 \n68.42 \n41.48 \n42.81 \n47.37 \nLLaMA-4-Scout \n80.05 \n84.52 \n80.44 \n82.56 \n80.13 \n79.03 \n81.38 \n81.70 \n73.92 \n45.33 \n46.58 \n63.55 \nLLaMA-4-Maverick \n85.55 \n89.63 \n88.92 \n88.61 \n86.65 \n85.47 \n89.00 \n89.16 \n84.68 \n56.32 \n68.58 \n75.10 \nMistral-7B-v0.3 \n22.00 \n28.04 \n14.30 \n29.30 \n21.37 \n17.12 \n23.49 \n26.00 \n18.30 \n17.52 \n10.37 \n10.68 \nMistral-Small-3.1-24B \n65.44 \n75.96 \n64.26 \n66.30 \n60.80 \n54.12 \n67.87 \n68.11 \n34.09 \n11.39 \n13.20 \n20.11 \nPhi-4-mini \n31.66 \n53.10 \n36.06 \n40.38 \n32.13 \n22.55 \n35.19 \n38.73 \n28.99 \n16.89 \n20.50 \n17.44 \nPhi-4-mini-Reasoning \n28.59 \n65.59 \n47.60 \n47.68 \n24.27 \n14.45 \n37.78 \n35.82 \n28.28 \n24.59 \n21.52 \n18.70 \nPhi-4 \n57.50 \n79.97 \n69.99 \n70.31 \n61.27 \n53.73 \n68.50 \n68.89 \n46.66 \n26.24 \n35.27 \n31.66 \nPhi-4-Reasoning \n79.26 \n85.31 \n83.90 \n83.90 \n82.56 \n79.81 \n77.77 \n84.68 \n73.13 \n12.73 \n47.45 \n29.93 \nQwen2.5-3B \n45.56 \n50.82 \n40.22 \n40.46 \n38.81 \n32.76 \n39.43 \n39.59 \n14.61 \n10.45 \n10.53 \n8.56 \nQwen2.5-7B \n57.50 \n62.29 \n51.69 \n50.20 \n49.25 \n44.07 \n53.34 \n53.50 \n30.87 \n10.05 \n22.78 \n13.43 \nQwen2.5-14B \n64.89 \n69.44 \n63.86 \n62.53 \n59.23 \n55.22 \n62.06 \n62.92 \n37.00 \n21.13 \n36.29 \n33.54 \nQwen2.5-72B \n74.94 \n79.26 \n76.75 \n75.41 \n71.41 \n70.86 \n75.18 \n76.28 \n50.12 \n40.69 \n38.26 \n37.78 \nQwQ-32B \n82.48 \n84.60 \n83.35 \n83.74 \n75.57 \n73.53 \n82.56 \n84.05 \n64.26 \n32.84 \n30.79 \n33.07 \nQwen3-1.7B \n44.93 \n49.18 \n40.22 \n40.14 \n36.53 \n32.91 \n37.47 \n40.22 \n27.02 \n28.04 \n25.06 \n24.59 \nQwen3-4B \n57.89 \n63.55 \n54.05 \n52.95 \n53.02 \n45.56 \n54.60 \n55.38 \n23.80 \n9.51 \n18.54 \n11.39 \nQwen3-4B-thinking \n65.51 \n72.58 \n69.21 \n67.24 \n66.22 \n60.57 \n67.56 \n67.79 \n26.39 \n13.98 \n11.15 \n8.48 \nQwen3-8B \n66.77 \n71.88 \n52.40 \n62.14 \n60.25 \n56.48 \n60.88 \n61.51 \n19.17 \n8.72 \n21.29 \n9.19 \nQwen3-8B-thinking \n73.76 \n79.89 \n75.81 \n77.45 \n74.94 \n73.29 \n76.75 \n76.75 \n28.44 \n12.80 \n14.85 \n11.63 \nQwen3-14B \n73.29 \n76.36 \n68.74 \n69.60 \n65.67 \n61.90 \n70.70 \n69.91 \n42.81 \n10.60 \n15.87 \n14.77 \nQwen3-14B-thinking \n79.81 \n82.40 \n81.85 \n79.81 \n80.68 \n78.63 \n80.83 \n82.01 \n63.08 \n12.18 \n19.95 \n13.67 \nBaichuan-M2-32B \n82.40 \n85.62 \n72.51 \n79.18 \n77.45 \n70.54 \n77.22 \n76.36 \n30.56 \n24.35 \n25.92 \n28.12 \nBio-Medical-LLaMA-3-8B \n47.29 \n77.77 \n55.07 \n53.18 \n42.11 \n41.48 \n54.52 \n54.44 \n39.36 \n37.16 \n36.45 \n33.86 \nMediPhi \n28.20 \n54.60 \n41.16 \n42.73 \n22.94 \n28.28 \n40.22 \n40.22 \n18.30 \n21.21 \n17.44 \n22.94 \nMedGemma-4B \n48.08 \n62.77 \n53.10 \n54.36 \n48.31 \n43.68 \n52.87 \n54.67 \n42.73 \n16.50 \n19.95 \n29.54 \nMedGemma-27B \n82.17 \n87.20 \n83.66 \n84.05 \n80.13 \n80.05 \n84.13 \n83.58 \n76.43 \n19.87 \n46.82 \n58.05 \nMedReason-8B \n46.27 \n57.89 \n15.00 \n17.67 \n43.68 \n35.51 \n22.55 \n35.74 \n11.39 \n7.07 \n10.68 \n9.19 \nHuatuoGPT-o1-7B \n63.00 \n65.36 \n65.12 \n65.04 \n53.42 \n53.73 \n65.59 \n65.12 \n7.31 \n7.38 \n6.52 \n6.13 \nHuatuoGPT-o1-8B \n56.79 \n68.66 \n66.93 \n63.94 \n48.86 \n48.63 \n65.04 \n67.79 \n49.10 \n7.70 \n7.54 \n7.15 \nHuatuoGPT-o1-70B \n67.16 \n86.72 \n85.31 \n84.92 \n66.54 \n74.31 \n82.33 \n86.02 \n73.53 \n30.95 \n45.80 \n55.07 \nHuatuoGPT-o1-72B \n81.78 \n81.23 \n85.55 \n80.28 \n80.44 \n74.08 \n84.84 \n84.76 \n64.65 \n41.40 \n45.01 \n22.00 \nOpenBioLLM-8B \n13.28 \n24.98 \n15.63 \n26.08 \n10.60 \n9.11 \n28.36 \n29.69 \n11.31 \n7.38 \n8.48 \n11.39 \nOpenBioLLM-70B \n21.29 \n72.98 \n62.92 \n35.11 \n18.93 \n19.48 \n47.21 \n66.69 \n25.06 \n9.43 \n13.35 \n24.74 \nSTab. 115: Zero-Shot performance evaluation of 56 LLMs on MedQA (Run 4). \n"}, {"page": 108, "text": " \n \n78 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n66.85 \n77.14 \n73.06 \n73.29 \n68.03 \n68.74 \n72.19 \n73.45 \n52.55 \n34.25 \n39.83 \n37.00 \nClaude-4.0-Sonnet \n89.47 \n91.83 \n89.55 \n90.26 \n89.40 \n89.32 \n91.36 \n90.49 \n81.85 \n58.60 \n71.33 \n73.92 \nGemini-2.5-Flash \n90.73 \n91.52 \n90.97 \n91.44 \n89.71 \n90.10 \n90.18 \n91.12 \n90.26 \n79.10 \n81.85 \n85.15 \nGPT-4o-mini \n67.09 \n78.32 \n72.11 \n73.37 \n67.71 \n65.04 \n72.98 \n73.45 \n60.96 \n32.36 \n44.93 \n50.04 \nGPT-4o \n86.88 \n89.87 \n87.98 \n87.98 \n85.00 \n84.29 \n89.40 \n88.69 \n82.56 \n31.58 \n51.77 \n69.60 \nGPT-4.1-nano \n67.64 \n81.15 \n71.17 \n71.01 \n63.39 \n61.74 \n72.43 \n73.37 \n54.05 \n27.18 \n38.41 \n44.15 \nGPT-4.1-mini \n86.88 \n91.67 \n87.04 \n87.75 \n86.25 \n82.48 \n87.12 \n88.14 \n79.26 \n24.04 \n58.37 \n67.56 \nGPT-4.1 \n88.45 \n89.24 \n90.02 \n89.08 \n89.95 \n86.33 \n91.67 \n89.47 \n88.14 \n52.08 \n69.29 \n77.06 \nGPT-5-nano \n51.61 \n70.31 \n64.89 \n69.36 \n65.59 \n54.28 \n61.59 \n71.41 \n54.05 \n21.13 \n39.28 \n43.36 \nGPT-5-mini \n87.35 \n90.65 \n87.59 \n89.24 \n87.82 \n84.76 \n88.77 \n89.40 \n80.68 \n34.88 \n62.06 \n71.09 \nGPT-5 \n90.57 \n93.24 \n91.99 \n91.67 \n91.28 \n84.68 \n91.75 \n92.30 \n89.47 \n53.34 \n72.98 \n76.20 \no4-mini \n93.09 \n93.72 \n93.40 \n93.09 \n92.54 \n91.12 \n93.64 \n93.64 \n91.67 \n38.96 \n80.28 \n85.00 \nOpen-Weight LLMs \nDeepSeek-V3 \n87.04 \n89.47 \n86.33 \n86.49 \n84.92 \n82.72 \n86.80 \n87.27 \n75.26 \n44.07 \n55.85 \n64.02 \nDeepSeek-R1 \n90.02 \n91.04 \n91.12 \n90.97 \n90.26 \n80.13 \n91.67 \n91.52 \n87.43 \n66.06 \n71.48 \n79.65 \nDeepSeek-R1-Qwen3-8B \n73.53 \n80.13 \n69.36 \n70.31 \n65.99 \n67.71 \n70.46 \n69.99 \n35.04 \n9.98 \n12.02 \n10.68 \nGemma-3-4B \n43.36 \n51.69 \n45.01 \n45.17 \n42.34 \n38.57 \n46.58 \n47.92 \n37.94 \n10.84 \n17.60 \n14.61 \nGemma-3-12B \n60.72 \n69.76 \n64.18 \n64.96 \n61.04 \n59.78 \n65.20 \n64.81 \n56.79 \n8.88 \n35.51 \n44.15 \nGemma-3-27B \n68.11 \n75.73 \n72.51 \n72.43 \n70.46 \n69.36 \n71.41 \n71.88 \n65.44 \n20.11 \n46.11 \n52.47 \ngpt-oss-20B \n82.01 \n80.91 \n83.66 \n82.48 \n84.76 \n82.33 \n84.92 \n84.21 \n68.03 \n30.71 \n60.33 \n66.46 \ngpt-oss-120B \n89.00 \n90.97 \n90.49 \n91.28 \n88.85 \n87.90 \n92.22 \n90.97 \n80.68 \n55.22 \n71.41 \n77.93 \nLLaMA-3.1-8B \n45.64 \n66.54 \n49.49 \n49.10 \n37.78 \n30.87 \n48.55 \n51.14 \n35.90 \n24.59 \n16.89 \n11.94 \nLLaMA-3.1-70B \n72.43 \n83.35 \n76.43 \n76.75 \n69.52 \n62.69 \n77.06 \n78.48 \n62.14 \n39.36 \n42.50 \n46.74 \nLLaMA-3.2-3B \n38.81 \n56.87 \n40.61 \n34.72 \n32.84 \n27.73 \n38.41 \n38.49 \n31.97 \n16.50 \n21.52 \n15.00 \nLLaMA-3.3-70B \n60.88 \n84.84 \n79.97 \n80.13 \n58.99 \n70.38 \n81.23 \n81.62 \n67.95 \n42.26 \n43.36 \n47.53 \nLLaMA-4-Scout \n79.58 \n84.68 \n81.78 \n83.35 \n79.81 \n78.00 \n80.83 \n82.33 \n74.78 \n45.88 \n46.43 \n63.39 \nLLaMA-4-Maverick \n85.62 \n90.49 \n88.85 \n88.53 \n86.02 \n85.86 \n88.30 \n89.08 \n83.74 \n56.72 \n68.11 \n76.43 \nMistral-7B-v0.3 \n21.52 \n29.38 \n13.90 \n29.85 \n22.07 \n17.44 \n23.64 \n28.04 \n20.11 \n17.75 \n9.11 \n9.27 \nMistral-Small-3.1-24B \n63.32 \n76.59 \n65.51 \n67.79 \n62.53 \n56.48 \n67.40 \n68.34 \n31.74 \n12.88 \n15.48 \n18.62 \nPhi-4-mini \n34.41 \n53.26 \n34.25 \n39.98 \n30.56 \n24.12 \n36.45 \n37.23 \n29.93 \n17.99 \n20.97 \n18.62 \nPhi-4-mini-Reasoning \n25.77 \n64.65 \n45.33 \n48.00 \n23.25 \n13.51 \n41.40 \n34.33 \n26.79 \n23.88 \n20.82 \n20.90 \nPhi-4 \n58.76 \n79.50 \n70.31 \n71.01 \n62.06 \n55.54 \n69.52 \n71.56 \n45.56 \n25.06 \n36.45 \n30.87 \nPhi-4-Reasoning \n81.38 \n85.07 \n84.05 \n84.05 \n82.88 \n79.10 \n76.12 \n84.84 \n72.43 \n14.30 \n47.53 \n28.83 \nQwen2.5-3B \n46.66 \n51.14 \n39.75 \n40.77 \n37.63 \n33.54 \n39.28 \n38.96 \n13.35 \n9.98 \n11.78 \n8.64 \nQwen2.5-7B \n55.77 \n59.94 \n51.85 \n48.70 \n49.80 \n44.23 \n52.32 \n50.75 \n30.09 \n9.11 \n21.52 \n13.98 \nQwen2.5-14B \n65.99 \n69.44 \n61.59 \n62.06 \n61.04 \n54.60 \n61.12 \n62.92 \n36.45 \n20.82 \n36.14 \n32.76 \nQwen2.5-72B \n74.55 \n80.20 \n75.49 \n75.65 \n73.06 \n70.62 \n74.94 \n75.33 \n49.18 \n39.75 \n38.41 \n38.02 \nQwQ-32B \n82.88 \n84.68 \n84.37 \n83.03 \n76.43 \n72.58 \n83.58 \n83.97 \n61.90 \n31.19 \n31.81 \n31.34 \nQwen3-1.7B \n44.23 \n49.41 \n39.04 \n39.28 \n35.27 \n33.62 \n37.39 \n39.98 \n27.18 \n25.14 \n26.16 \n26.47 \nQwen3-4B \n59.31 \n64.02 \n52.71 \n52.95 \n53.50 \n44.85 \n54.75 \n54.52 \n22.94 \n9.27 \n16.73 \n9.74 \nQwen3-4B-thinking \n66.06 \n72.11 \n67.87 \n68.74 \n66.22 \n59.86 \n67.56 \n68.50 \n26.32 \n15.55 \n11.94 \n9.11 \nQwen3-8B \n67.24 \n71.72 \n51.85 \n62.69 \n59.54 \n56.95 \n62.06 \n60.25 \n22.15 \n8.80 \n21.37 \n10.53 \nQwen3-8B-thinking \n73.29 \n79.65 \n76.90 \n77.22 \n75.26 \n72.82 \n75.88 \n76.83 \n27.42 \n8.80 \n15.79 \n9.51 \nQwen3-14B \n74.71 \n77.22 \n67.87 \n70.93 \n66.46 \n60.41 \n69.99 \n70.07 \n40.85 \n11.55 \n16.50 \n14.77 \nQwen3-14B-thinking \n79.58 \n83.74 \n80.44 \n80.68 \n80.91 \n76.83 \n79.89 \n80.83 \n62.77 \n13.12 \n17.99 \n15.63 \nBaichuan-M2-32B \n84.29 \n85.86 \n74.31 \n79.50 \n75.81 \n70.70 \n77.69 \n75.65 \n30.24 \n24.90 \n28.83 \n29.77 \nBio-Medical-LLaMA-3-8B \n46.74 \n77.22 \n56.01 \n52.40 \n42.11 \n42.73 \n55.15 \n54.44 \n39.98 \n36.84 \n35.35 \n33.15 \nMediPhi \n28.44 \n51.61 \n43.68 \n41.87 \n23.80 \n27.18 \n38.81 \n41.56 \n19.72 \n22.15 \n17.44 \n23.17 \nMedGemma-4B \n47.29 \n64.10 \n53.65 \n55.38 \n47.53 \n44.07 \n53.02 \n51.77 \n42.73 \n16.18 \n20.50 \n27.57 \nMedGemma-27B \n82.95 \n87.59 \n83.66 \n83.42 \n79.34 \n78.24 \n83.58 \n84.05 \n77.38 \n16.26 \n45.09 \n57.74 \nMedReason-8B \n44.78 \n59.23 \n13.75 \n16.42 \n45.64 \n36.53 \n21.45 \n36.61 \n10.29 \n8.41 \n10.53 \n9.82 \nHuatuoGPT-o1-7B \n62.61 \n65.04 \n63.86 \n65.04 \n55.93 \n54.75 \n65.04 \n65.20 \n7.54 \n6.44 \n7.23 \n6.76 \nHuatuoGPT-o1-8B \n58.52 \n68.34 \n65.04 \n65.44 \n51.69 \n49.57 \n63.32 \n64.02 \n48.86 \n8.01 \n7.62 \n7.23 \nHuatuoGPT-o1-70B \n68.11 \n87.43 \n85.78 \n85.15 \n67.16 \n73.29 \n84.05 \n85.47 \n73.92 \n29.69 \n47.60 \n56.64 \nHuatuoGPT-o1-72B \n81.38 \n83.50 \n85.86 \n81.30 \n78.24 \n74.39 \n85.07 \n83.82 \n63.86 \n42.11 \n44.07 \n22.39 \nOpenBioLLM-8B \n15.24 \n25.92 \n15.24 \n26.39 \n9.98 \n9.66 \n30.09 \n31.11 \n10.84 \n8.72 \n7.86 \n10.92 \nOpenBioLLM-70B \n20.90 \n72.35 \n59.47 \n33.94 \n19.80 \n19.17 \n45.40 \n65.51 \n27.49 \n10.92 \n12.02 \n25.61 \nSTab. 116: Zero-Shot performance evaluation of 56 LLMs on MedQA (Run 5). \n"}, {"page": 109, "text": " \n \n79 \n \nLLMs \nChinese \nEnglish \nFrench \nGerman Japanese \nKorean Portuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nProprietary LLMs \nClaude-3.5-Haiku \n59.36±0.00 68.45±0.00 71.66±0.00 63.10±0.00 59.89±0.00 55.61±0.00 \n68.45±0.00 \n72.62±0.24 36.36±0.00 15.51±1.46 20.96±0.45 29.84±0.24 51.82±19.82 \nClaude-4.0-Sonnet \n77.65±1.57 82.78±0.59 80.75±1.07 77.86±0.97 78.61±0.76 76.47±0.54 \n80.00±0.81 \n81.39±1.22 70.59±1.85 35.08±1.54 55.83±1.54 63.31±2.35 71.69±13.63 \nGemini-2.5-Flash \n76.26±1.11 75.73±0.90 76.90±0.88 77.33±1.63 75.72±0.48 76.79±1.63 \n78.72±1.22 \n78.29±0.89 75.72±0.61 63.42±1.54 68.66±1.17 68.12±0.90 \n74.30±4.77 \nGPT-4o-mini \n57.11±1.91 71.98±1.11 68.13±1.59 65.88±1.44 60.43±2.48 56.68±3.10 \n66.63±1.23 \n68.34±0.88 48.02±1.83 16.47±0.88 27.81±2.14 34.54±1.04 53.50±17.52 \nGPT-4o \n72.73±1.07 76.26±1.95 75.72±2.09 73.58±1.88 73.26±1.25 69.52±1.25 \n76.37±1.75 \n76.37±1.83 66.41±2.34 25.99±2.26 42.68±1.28 55.51±1.83 65.37±15.58 \nGPT-4.1-nano \n56.58±3.24 72.73±2.03 65.56±1.17 65.99±1.23 54.87±1.95 47.49±2.57 \n66.52±0.97 \n68.34±2.25 37.22±2.90 14.65±2.95 21.28±1.71 30.59±1.98 50.15±19.22 \nGPT-4.1-mini \n70.05±1.69 78.07±1.07 72.41±1.29 74.97±1.16 70.91±1.34 66.10±0.48 \n75.51±1.49 \n73.05±2.32 62.03±2.07 15.51±1.47 44.06±0.90 53.16±2.52 62.99±17.40 \nGPT-4.1 \n74.65±0.97 77.65±1.33 77.54±0.84 75.72±1.54 74.87±1.41 71.34±1.54 \n79.57±1.28 \n79.25±0.96 74.12±1.34 31.44±3.24 46.10±1.83 63.32±1.59 68.80±14.55 \nGPT-5-nano \n47.59±2.17 65.78±1.96 58.29±1.82 60.32±2.09 52.83±1.62 40.43±3.48 \n56.47±2.22 \n59.68±1.17 39.14±1.38 12.51±1.54 20.75±2.37 26.84±1.62 45.05±16.79 \nGPT-5-mini \n73.16±1.67 76.69±0.61 74.22±2.34 73.58±2.64 71.34±1.72 64.81±2.46 \n74.76±1.03 \n76.26±1.29 61.50±2.39 19.89±1.91 44.17±1.23 56.47±2.66 63.90±16.47 \nGPT-5 \n75.30±1.67 76.90±0.95 77.33±1.17 74.12±2.06 74.22±1.43 68.55±1.67 \n79.78±1.03 \n79.36±0.61 70.70±0.79 36.47±2.25 55.61±2.80 63.42±2.74 69.31±12.17 \no4-mini \n74.97±1.48 79.79±0.24 80.10±0.79 74.87±1.85 75.72±0.90 71.23±1.91 \n80.43±0.72 \n78.39±1.91 72.51±1.45 30.38±1.83 60.11±1.40 67.27±1.27 70.48±13.52 \nOpen-Weight LLMs \nDeepSeek-V3 \n74.98±1.33 79.68±0.85 75.72±0.61 75.72±1.34 75.40±1.07 69.20±1.17 \n78.82±1.49 \n75.30±1.48 66.20±1.62 30.16±1.91 36.90±1.89 46.74±2.09 65.40±16.79 \nDeepSeek-R1 \n70.05±1.00 78.39±0.81 78.18±1.22 76.79±0.72 76.36±1.22 63.85±2.09 \n78.29±0.90 \n77.33±0.97 74.44±1.58 46.10±1.79 57.01±2.64 61.18±0.97 69.83±10.26 \nDeepSeek-R1-Qwen3-8B \n61.39±1.33 68.23±0.81 58.40±3.87 61.93±2.76 54.87±2.49 56.90±1.54 \n63.74±2.46 \n59.68±1.59 19.57±2.66 10.91±2.41 10.91±2.84 10.59±2.15 44.76±23.09 \nGemma-3-4B \n31.02±1.20 40.86±2.69 31.98±1.58 35.29±2.75 27.81±1.14 22.67±2.22 \n29.95±2.04 \n27.70±2.94 22.35±1.03 11.44±2.02 11.02±1.58 10.05±3.15 \n25.18±9.83 \nGemma-3-12B \n51.12±2.53 61.50±0.66 58.71±1.44 58.39±2.25 53.05±2.63 51.87±2.67 \n59.04±2.32 \n58.61±1.63 45.99±2.90 \n9.41±2.02 \n23.63±1.28 32.94±1.80 47.02±16.04 \nGemma-3-27B \n62.46±3.52 67.38±1.65 68.98±2.39 64.49±1.17 58.50±0.72 57.97±1.72 \n70.27±1.80 \n67.70±2.94 54.54±2.00 13.15±1.34 27.59±0.81 44.28±1.09 54.78±17.38 \ngpt-oss-20B \n63.32±2.09 64.92±3.13 69.09±1.67 67.49±1.58 64.07±0.45 63.74±1.91 \n68.34±1.48 \n69.09±2.22 49.09±1.28 22.78±0.81 42.67±3.01 47.38±2.02 57.66±13.94 \ngpt-oss-120B \n70.27±0.61 77.22±1.34 75.93±2.07 74.12±1.29 72.94±1.39 69.84±2.19 \n75.83±1.03 \n75.83±2.12 62.03±1.69 35.40±3.06 54.44±2.55 58.72±2.66 66.88±12.11 \nLLaMA-3.1-8B \n34.87±1.71 54.44±1.38 38.61±3.24 38.93±1.48 24.60±1.73 20.00±3.48 \n38.29±1.44 \n40.21±2.05 22.25±0.81 15.83±2.23 10.70±3.07 10.48±3.87 29.10±13.46 \nLLaMA-3.1-70B \n59.89±2.73 72.30±1.22 66.63±1.04 67.81±2.21 59.89±2.62 52.30±1.33 \n66.63±1.63 \n68.66±2.55 50.80±2.17 20.75±1.39 22.99±2.33 28.77±3.37 53.12±18.14 \nLLaMA-3.2-3B \n29.84±1.16 40.64±2.33 27.59±1.95 30.91±1.91 19.68±2.76 12.73±1.67 \n22.99±1.20 \n25.13±3.05 16.04±1.46 11.01±2.66 \n8.13±2.96 \n10.80±1.48 \n21.29±9.80 \nLLaMA-3.3-70B \n53.15±2.02 75.94±0.65 73.58±1.54 71.98±0.97 51.87±2.30 60.32±1.16 \n71.66±0.00 \n72.83±0.70 57.65±1.48 24.06±1.77 27.17±1.58 34.23±1.56 56.20±18.14 \nLLaMA-4-Scout \n69.41±1.16 74.44±1.43 73.69±1.58 70.37±0.48 65.99±1.29 67.59±0.97 \n74.12±1.17 \n73.05±1.23 62.14±2.25 29.95±2.30 29.41±0.38 54.23±0.97 62.03±15.68 \nLLaMA-4-Maverick \n73.90±1.03 80.85±0.70 77.65±1.16 75.83±1.16 74.33±2.14 71.98±1.04 \n77.33±1.11 \n77.22±1.11 72.09±1.16 37.32±0.96 53.48±2.10 59.68±1.34 69.30±12.41 \nMistral-7B-v0.3 \n13.90±1.31 20.75±3.90 13.90±1.26 17.65±2.68 12.94±1.09 11.76±2.33 \n18.08±1.75 \n16.79±1.23 \n9.41±1.72 \n10.05±0.79 10.38±1.39 10.27±1.66 \n13.82±4.00 \nMistral-Small-3.1-24B \n56.79±2.05 70.05±2.17 65.24±3.49 64.92±2.35 58.93±1.28 48.98±3.68 \n65.13±2.43 \n65.78±1.81 23.64±3.99 11.34±2.25 10.48±1.49 11.23±2.78 46.04±23.64 \nPhi-4-mini \n25.45±2.74 48.56±3.17 31.77±3.64 32.09±2.42 20.53±4.01 16.26±2.69 \n26.52±3.52 \n28.13±2.97 16.15±0.45 10.37±1.59 11.34±1.16 10.91±1.23 23.17±11.15 \nPhi-4-mini-Reasoning \n20.86±2.10 60.00±2.31 43.32±3.36 44.17±2.35 18.39±3.31 10.91±3.15 \n36.47±5.91 \n31.87±2.35 16.69±2.99 13.91±3.47 11.12±1.79 14.01±1.03 26.81±15.74 \nPhi-4 \n51.66±2.44 72.51±0.90 65.56±2.02 68.02±1.66 56.68±1.73 48.88±3.13 \n68.02±0.70 \n66.31±0.84 33.58±1.94 13.58±1.68 18.61±1.38 18.08±2.25 48.46±21.23 \nPhi-4-Reasoning \n67.48±0.88 74.76±2.12 74.44±1.33 72.19±1.65 71.66±1.36 64.81±2.01 \n68.77±1.54 \n74.33±1.00 56.04±1.75 16.58±1.69 32.19±2.05 25.35±1.54 58.22±20.45 \nQwen2.5-3B \n37.75±1.80 39.79±1.88 32.19±3.10 28.23±2.94 24.92±1.72 20.32±3.65 \n31.77±2.12 \n32.84±1.91 11.34±0.79 10.05±1.90 11.87±2.57 \n9.63±4.25 \n24.22±11.07 \nQwen2.5-7B \n47.70±2.84 58.82±3.36 49.41±2.06 42.57±2.19 40.86±1.95 31.77±2.29 \n45.56±2.28 \n46.52±1.36 17.22±1.16 11.02±1.34 12.41±1.75 10.16±1.25 34.50±16.82 \nQwen2.5-14B \n59.57±2.26 68.13±1.54 57.97±2.63 57.75±2.24 52.51±0.45 49.20±2.83 \n60.86±2.31 \n59.89±3.36 20.43±1.53 16.90±2.79 14.12±1.23 19.47±1.50 44.73±19.89 \nQwen2.5-72B \n69.95±0.45 73.80±1.20 69.73±0.97 68.24±1.40 66.84±1.00 61.60±1.58 \n71.55±1.98 \n69.30±0.97 35.51±2.32 18.18±1.07 15.40±0.45 21.50±1.27 53.47±22.64 \nQwQ-32B \n69.41±1.83 74.76±1.58 73.90±1.03 72.94±1.29 68.45±1.00 64.38±2.22 \n76.15±1.23 \n74.97±1.67 46.63±2.12 18.61±2.38 19.14±2.22 20.86±1.65 56.68±22.98 \nQwen3-1.7B \n29.95±0.54 38.40±2.49 28.77±2.22 23.32±2.92 20.54±1.04 21.07±2.79 \n24.28±1.76 \n25.78±2.09 14.76±2.44 11.44±2.35 11.98±0.30 13.58±1.40 \n21.99±8.11 \nQwen3-4B \n53.16±1.95 59.15±1.76 50.80±2.48 50.59±1.72 43.64±2.22 37.75±2.23 \n48.34±1.45 \n50.27±3.49 12.51±1.80 11.23±0.65 11.02±2.29 10.70±1.46 36.60±18.74 \nQwen3-4B-thinking \n58.18±0.88 63.75±1.44 64.39±1.72 60.75±1.63 57.43±1.04 53.58±2.84 \n61.50±2.30 \n61.60±0.70 15.30±1.91 12.08±0.97 12.84±3.03 \n9.84±2.74 \n44.27±22.90 \nQwen3-8B \n58.50±2.16 66.95±1.16 54.65±0.79 54.76±1.71 52.30±2.12 45.67±2.52 \n56.47±1.84 \n57.86±3.01 19.68±2.86 11.66±1.38 13.05±1.88 12.09±3.28 41.97±20.58 \nQwen3-8B-thinking \n62.46±2.44 72.30±1.53 68.87±2.02 66.52±2.29 64.92±1.59 62.57±2.42 \n68.13±2.47 \n68.13±0.81 24.06±3.23 12.94±2.66 13.58±2.16 13.05±1.44 49.79±24.53 \nQwen3-14B \n65.13±1.38 71.34±2.41 65.24±1.41 65.99±1.34 58.40±2.08 57.22±2.48 \n68.77±2.03 \n65.67±1.75 24.38±0.72 11.34±2.52 12.83±1.20 16.15±3.15 48.54±23.62 \nQwen3-14B-thinking \n66.31±1.31 75.29±1.75 72.84±0.79 71.55±1.09 70.69±1.75 66.31±1.69 \n71.02±2.05 \n71.34±1.63 44.92±1.97 12.73±1.62 17.43±2.06 16.04±2.21 54.71±24.14 \nBaichuan-M2-32B \n69.52±1.93 75.40±1.81 69.41±1.38 67.91±1.56 63.85±2.63 56.04±1.53 \n67.91±1.19 \n67.59±4.29 18.61±2.46 16.36±2.97 18.29±2.96 15.83±1.87 50.56±24.22 \nBio-Medical-LLaMA-3-8B \n29.84±1.44 55.08±1.13 36.79±1.62 39.57±1.00 28.34±1.36 22.89±1.43 \n35.51±0.61 \n38.93±0.70 25.99±1.68 19.04±1.54 13.37±0.76 15.83±0.90 30.10±11.49 \nMediPhi \n19.15±2.68 42.03±2.32 32.94±3.64 28.23±2.22 12.51±1.17 10.27±2.08 \n31.45±1.48 \n30.70±0.90 \n9.41±3.35 \n10.59±1.86 11.12±1.27 10.59±1.22 20.75±11.37 \nMedGemma-4B \n33.48±1.04 46.31±1.95 42.25±3.12 37.22±1.72 30.91±1.75 29.63±2.23 \n36.04±1.49 \n36.68±1.29 23.64±2.18 13.05±2.44 12.41±2.40 16.26±2.99 29.82±11.02 \nMedGemma-27B \n68.88±2.08 75.29±1.87 73.80±1.89 74.01±1.23 69.20±1.54 67.27±1.38 \n73.15±1.71 \n73.69±2.31 61.28±2.06 14.86±2.63 29.73±0.61 46.31±3.44 60.62±19.25 \nMedReason-8B \n38.07±2.46 48.13±4.02 20.75±0.79 21.82±1.16 37.22±1.39 26.52±1.23 \n21.39±3.76 \n28.88±3.32 14.54±1.49 11.66±0.70 \n9.31±0.30 \n10.59±1.48 24.07±11.97 \nHuatuoGPT-o1-7B \n53.80±1.67 58.29±3.38 57.43±2.35 53.16±3.13 48.77±1.48 41.39±1.34 \n59.36±0.54 \n58.61±2.99 10.37±1.11 11.34±0.88 \n9.52±2.08 \n10.48±0.89 39.38±21.27 \nHuatuoGPT-o1-8B \n47.06±1.31 54.33±1.11 51.66±4.32 49.52±3.25 41.18±1.93 35.61±1.44 \n48.66±1.07 \n45.24±3.22 28.88±1.13 11.66±1.83 11.76±2.30 10.27±1.94 36.32±16.21 \nHuatuoGPT-o1-70B \n60.53±2.19 72.94±1.23 69.62±2.05 69.20±1.34 61.39±2.22 65.45±1.83 \n70.59±2.17 \n70.69±1.03 57.11±1.10 24.17±1.66 30.48±1.65 38.82±2.19 57.58±16.40 \nHuatuoGPT-o1-72B \n69.52±1.81 75.30±1.79 73.37±1.16 68.77±2.55 70.69±1.67 65.56±0.90 \n74.55±1.17 \n73.90±1.44 44.92±2.33 20.64±2.74 24.49±1.48 17.11±3.10 56.57±22.37 \nOpenBioLLM-8B \n12.41±1.27 22.25±3.64 16.15±2.37 16.79±1.68 11.44±1.63 10.59±3.30 \n17.33±1.50 \n15.08±1.49 10.59±1.48 10.37±2.02 \n9.52±2.76 \n9.84±1.39 \n13.53±4.30 \nOpenBioLLM-70B \n26.42±3.37 62.35±1.23 56.90±2.26 38.50±2.00 24.49±3.19 21.93±2.45 \n42.68±3.77 \n59.89±1.36 26.31±1.91 \n9.20±1.49 \n11.12±1.90 15.29±2.41 32.92±18.36 \nSTab. 117: Performance evaluation of 56 LLMs on MMLU-Pro. \n"}, {"page": 110, "text": " \n \n80 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n59.36 \n68.45 \n71.66 \n63.10 \n59.89 \n55.61 \n68.45 \n72.73 \n36.36 \n16.58 \n20.32 \n29.95 \nClaude-4.0-Sonnet \n75.94 \n82.35 \n80.21 \n77.54 \n78.61 \n77.01 \n81.28 \n82.89 \n73.26 \n33.69 \n54.01 \n66.84 \nGemini-2.5-Flash \n77.01 \n74.87 \n76.47 \n76.47 \n76.47 \n75.94 \n78.61 \n78.07 \n75.94 \n63.10 \n68.45 \n67.91 \nGPT-4o-mini \n54.01 \n73.26 \n66.31 \n65.24 \n58.29 \n51.87 \n67.91 \n66.84 \n49.73 \n14.97 \n26.74 \n35.29 \nGPT-4o \n71.66 \n77.54 \n75.40 \n75.94 \n74.33 \n70.59 \n74.33 \n77.01 \n62.57 \n25.67 \n40.64 \n55.08 \nGPT-4.1-nano \n54.01 \n71.66 \n65.24 \n64.71 \n55.08 \n48.13 \n67.38 \n66.31 \n35.29 \n12.30 \n23.53 \n29.41 \nGPT-4.1-mini \n72.19 \n79.14 \n70.59 \n74.33 \n72.19 \n66.31 \n76.47 \n70.05 \n62.57 \n17.65 \n43.85 \n54.01 \nGPT-4.1 \n74.87 \n78.61 \n76.47 \n76.47 \n76.47 \n71.66 \n78.07 \n80.21 \n74.87 \n31.02 \n45.99 \n65.78 \nGPT-5-nano \n48.66 \n66.31 \n59.36 \n59.36 \n51.87 \n43.85 \n55.61 \n59.89 \n40.64 \n13.90 \n19.25 \n27.27 \nGPT-5-mini \n75.94 \n77.54 \n75.40 \n75.40 \n71.12 \n67.38 \n75.94 \n77.01 \n62.03 \n17.11 \n45.45 \n60.43 \nGPT-5 \n76.47 \n77.54 \n78.61 \n76.47 \n74.33 \n68.45 \n79.68 \n79.14 \n71.12 \n36.90 \n52.41 \n65.78 \no4-mini \n74.33 \n79.68 \n81.28 \n77.54 \n76.47 \n73.26 \n79.68 \n76.47 \n71.66 \n29.41 \n59.89 \n65.78 \nOpen-Weight LLMs \nDeepSeek-V3 \n77.01 \n78.61 \n74.87 \n75.94 \n74.87 \n70.59 \n81.28 \n77.54 \n65.24 \n28.88 \n35.29 \n44.39 \nDeepSeek-R1 \n68.98 \n79.68 \n79.14 \n77.54 \n74.33 \n62.57 \n78.61 \n77.01 \n71.66 \n44.92 \n56.68 \n60.96 \nDeepSeek-R1-Qwen3-8B \n61.50 \n68.98 \n57.22 \n64.17 \n57.75 \n55.61 \n64.71 \n58.29 \n23.53 \n9.09 \n7.49 \n8.02 \nGemma-3-4B \n30.48 \n41.71 \n34.22 \n39.57 \n28.88 \n19.79 \n28.88 \n26.74 \n24.06 \n10.16 \n11.76 \n7.49 \nGemma-3-12B \n52.41 \n62.57 \n56.68 \n57.22 \n55.08 \n48.13 \n59.36 \n56.15 \n43.32 \n8.02 \n22.99 \n35.83 \nGemma-3-27B \n65.24 \n68.45 \n69.52 \n64.17 \n57.75 \n55.08 \n69.52 \n64.17 \n55.61 \n11.23 \n27.81 \n43.85 \ngpt-oss-20B \n63.10 \n59.89 \n70.59 \n65.78 \n64.71 \n64.17 \n66.31 \n66.84 \n48.66 \n22.99 \n43.85 \n49.20 \ngpt-oss-120B \n71.12 \n77.54 \n73.26 \n72.73 \n72.73 \n71.66 \n74.33 \n75.94 \n63.10 \n34.76 \n55.61 \n63.10 \nLLaMA-3.1-8B \n35.83 \n54.55 \n33.69 \n40.64 \n26.20 \n19.79 \n40.64 \n39.04 \n21.39 \n13.90 \n13.37 \n16.04 \nLLaMA-3.1-70B \n63.64 \n73.26 \n67.38 \n68.98 \n62.57 \n54.01 \n67.38 \n67.38 \n51.34 \n22.46 \n25.13 \n26.20 \nLLaMA-3.2-3B \n30.48 \n40.11 \n27.27 \n29.41 \n18.72 \n11.23 \n22.99 \n23.53 \n16.04 \n9.09 \n6.95 \n10.16 \nLLaMA-3.3-70B \n55.61 \n76.47 \n71.66 \n70.59 \n52.94 \n59.36 \n71.66 \n72.19 \n59.89 \n24.06 \n29.41 \n34.76 \nLLaMA-4-Scout \n69.52 \n73.80 \n75.94 \n70.59 \n66.31 \n68.45 \n75.40 \n74.33 \n60.96 \n33.16 \n29.41 \n55.08 \nLLaMA-4-Maverick \n72.19 \n81.28 \n78.61 \n76.47 \n74.87 \n72.73 \n77.54 \n76.47 \n71.12 \n38.50 \n50.80 \n60.96 \nMistral-7B-v0.3 \n12.30 \n18.18 \n15.51 \n18.18 \n13.90 \n14.44 \n20.86 \n16.58 \n11.23 \n10.16 \n10.70 \n10.70 \nMistral-Small-3.1-24B \n57.22 \n68.98 \n64.17 \n67.91 \n58.29 \n51.87 \n67.91 \n64.71 \n21.93 \n12.83 \n10.16 \n12.83 \nPhi-4-mini \n22.99 \n49.20 \n26.74 \n32.09 \n18.18 \n15.51 \n24.06 \n32.62 \n16.58 \n10.16 \n12.83 \n12.30 \nPhi-4-mini-Reasoning \n22.46 \n61.50 \n43.32 \n46.52 \n14.44 \n7.49 \n41.18 \n33.16 \n14.44 \n12.30 \n12.30 \n14.44 \nPhi-4 \n54.55 \n73.26 \n65.78 \n66.31 \n56.68 \n48.13 \n67.91 \n66.84 \n33.69 \n14.97 \n20.32 \n18.72 \nPhi-4-Reasoning \n67.91 \n76.47 \n74.87 \n74.33 \n72.73 \n62.57 \n70.05 \n75.40 \n57.22 \n16.04 \n29.41 \n25.67 \nQwen2.5-3B \n40.11 \n39.57 \n35.83 \n25.13 \n25.67 \n16.04 \n34.22 \n34.76 \n10.16 \n10.70 \n13.37 \n10.16 \nQwen2.5-7B \n47.59 \n52.94 \n48.66 \n40.64 \n40.64 \n32.62 \n42.25 \n46.52 \n16.58 \n11.23 \n11.23 \n9.09 \nQwen2.5-14B \n60.43 \n68.45 \n59.89 \n58.82 \n52.41 \n45.99 \n58.29 \n63.10 \n20.32 \n20.32 \n14.44 \n18.72 \nQwen2.5-72B \n70.05 \n74.33 \n70.05 \n68.45 \n65.78 \n61.50 \n74.33 \n68.45 \n34.76 \n17.11 \n15.51 \n22.99 \nQwQ-32B \n66.84 \n74.33 \n74.33 \n72.19 \n67.38 \n62.57 \n74.87 \n73.26 \n47.06 \n19.79 \n15.51 \n21.93 \nQwen3-1.7B \n29.95 \n35.83 \n30.48 \n22.46 \n20.86 \n24.60 \n24.60 \n25.67 \n18.72 \n14.97 \n11.76 \n14.44 \nQwen3-4B \n54.55 \n59.36 \n54.01 \n49.73 \n42.78 \n37.97 \n49.20 \n51.34 \n11.23 \n11.76 \n13.37 \n9.63 \nQwen3-4B-thinking \n57.22 \n64.71 \n63.64 \n60.43 \n56.15 \n49.73 \n64.17 \n62.03 \n15.51 \n11.23 \n13.37 \n10.70 \nQwen3-8B \n59.36 \n68.45 \n55.08 \n52.41 \n49.73 \n45.45 \n58.29 \n55.61 \n19.79 \n9.63 \n12.83 \n9.63 \nQwen3-8B-thinking \n59.89 \n74.33 \n67.91 \n63.64 \n66.84 \n66.31 \n72.19 \n68.98 \n25.13 \n14.44 \n11.23 \n12.83 \nQwen3-14B \n65.78 \n70.05 \n65.78 \n66.31 \n59.89 \n59.36 \n66.84 \n65.78 \n24.06 \n12.30 \n11.23 \n15.51 \nQwen3-14B-thinking \n64.71 \n74.33 \n72.73 \n71.66 \n70.05 \n65.78 \n72.73 \n72.19 \n42.78 \n12.30 \n16.04 \n19.79 \nBaichuan-M2-32B \n68.98 \n73.26 \n68.98 \n66.84 \n64.17 \n56.15 \n67.38 \n73.80 \n17.11 \n14.44 \n18.18 \n18.18 \nBio-Medical-LLaMA-3-8B \n30.48 \n54.01 \n37.43 \n39.57 \n27.81 \n21.93 \n35.29 \n37.97 \n25.13 \n18.18 \n12.30 \n17.11 \nMediPhi \n16.58 \n42.25 \n28.34 \n29.41 \n12.30 \n10.16 \n31.55 \n32.09 \n12.30 \n8.56 \n9.63 \n11.76 \nMedGemma-4B \n33.16 \n48.13 \n37.43 \n37.43 \n31.55 \n28.34 \n35.29 \n37.43 \n21.93 \n14.44 \n10.16 \n13.90 \nMedGemma-27B \n67.91 \n74.33 \n74.87 \n74.87 \n70.59 \n68.45 \n72.19 \n76.47 \n61.50 \n11.76 \n30.48 \n43.32 \nMedReason-8B \n40.64 \n50.80 \n21.93 \n22.46 \n36.90 \n27.81 \n19.25 \n26.20 \n13.90 \n10.70 \n9.09 \n8.02 \nHuatuoGPT-o1-7B \n51.34 \n54.55 \n55.08 \n49.73 \n49.20 \n42.25 \n58.82 \n54.01 \n10.16 \n12.83 \n7.49 \n9.63 \nHuatuoGPT-o1-8B \n47.59 \n55.61 \n45.45 \n48.13 \n43.85 \n36.36 \n48.66 \n45.99 \n27.81 \n11.76 \n14.97 \n11.23 \nHuatuoGPT-o1-70B \n59.36 \n72.73 \n67.91 \n69.52 \n65.24 \n64.17 \n70.05 \n71.12 \n58.29 \n25.13 \n31.55 \n36.90 \nHuatuoGPT-o1-72B \n72.19 \n72.73 \n71.66 \n65.24 \n71.12 \n65.78 \n74.33 \n75.40 \n41.18 \n22.46 \n26.20 \n14.44 \nOpenBioLLM-8B \n11.76 \n27.81 \n17.11 \n17.65 \n12.83 \n10.70 \n18.72 \n12.83 \n9.09 \n10.16 \n11.76 \n9.63 \nOpenBioLLM-70B \n28.88 \n61.50 \n58.29 \n40.11 \n27.27 \n23.53 \n47.06 \n58.29 \n24.60 \n10.70 \n11.23 \n17.65 \nSTab. 118: Zero-Shot performance evaluation of 56 LLMs on MMLU-Pro (Run 1). \n"}, {"page": 111, "text": " \n \n81 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n59.36 \n68.45 \n71.66 \n63.10 \n59.89 \n55.61 \n68.45 \n72.73 \n36.36 \n17.11 \n20.86 \n29.95 \nClaude-4.0-Sonnet \n77.01 \n83.42 \n82.35 \n78.07 \n77.54 \n75.94 \n79.68 \n80.75 \n70.05 \n33.16 \n56.68 \n60.96 \nGemini-2.5-Flash \n74.87 \n75.94 \n78.07 \n77.01 \n75.40 \n79.14 \n77.54 \n79.14 \n76.47 \n65.24 \n68.45 \n68.98 \nGPT-4o-mini \n57.75 \n72.19 \n67.38 \n67.38 \n59.36 \n58.29 \n67.38 \n68.45 \n47.06 \n16.58 \n28.88 \n34.22 \nGPT-4o \n72.19 \n75.40 \n78.61 \n73.26 \n73.80 \n68.45 \n78.61 \n77.01 \n68.98 \n23.53 \n43.85 \n55.08 \nGPT-4.1-nano \n58.29 \n74.33 \n65.78 \n67.91 \n54.01 \n50.80 \n65.78 \n68.98 \n34.76 \n11.76 \n19.25 \n32.09 \nGPT-4.1-mini \n70.05 \n78.07 \n72.73 \n77.01 \n72.19 \n65.24 \n75.94 \n75.40 \n60.43 \n15.51 \n43.85 \n57.22 \nGPT-4.1 \n74.33 \n76.47 \n77.01 \n74.33 \n75.40 \n72.19 \n78.61 \n79.14 \n73.26 \n30.48 \n45.45 \n61.50 \nGPT-5-nano \n45.99 \n65.78 \n60.43 \n59.36 \n55.08 \n35.83 \n55.61 \n57.75 \n37.43 \n11.76 \n22.46 \n26.20 \nGPT-5-mini \n72.19 \n75.94 \n73.26 \n70.59 \n72.73 \n66.31 \n74.87 \n75.40 \n60.96 \n19.25 \n44.92 \n57.22 \nGPT-5 \n75.40 \n77.01 \n77.54 \n74.33 \n76.47 \n67.38 \n79.14 \n78.61 \n71.66 \n35.29 \n57.22 \n63.64 \no4-mini \n77.54 \n79.68 \n80.21 \n73.26 \n76.47 \n71.66 \n80.75 \n77.01 \n74.33 \n28.34 \n58.29 \n66.31 \nOpen-Weight LLMs \nDeepSeek-V3 \n73.80 \n79.14 \n75.40 \n73.80 \n75.94 \n69.52 \n78.07 \n74.33 \n68.45 \n28.88 \n40.11 \n49.20 \nDeepSeek-R1 \n71.12 \n78.61 \n78.07 \n76.47 \n77.54 \n61.50 \n79.68 \n78.61 \n74.87 \n45.99 \n56.15 \n60.96 \nDeepSeek-R1-Qwen3-8B \n61.50 \n67.38 \n58.82 \n63.64 \n56.68 \n57.22 \n60.96 \n58.29 \n19.25 \n9.09 \n12.30 \n11.76 \nGemma-3-4B \n31.55 \n40.64 \n30.48 \n35.83 \n28.88 \n21.39 \n31.02 \n23.53 \n21.93 \n13.37 \n9.63 \n12.30 \nGemma-3-12B \n48.66 \n61.50 \n59.89 \n57.75 \n49.73 \n51.87 \n56.15 \n57.75 \n50.27 \n8.56 \n24.06 \n33.16 \nGemma-3-27B \n65.24 \n69.52 \n68.45 \n64.17 \n59.36 \n57.75 \n70.05 \n67.91 \n54.01 \n12.30 \n26.20 \n43.32 \ngpt-oss-20B \n66.31 \n67.91 \n69.52 \n67.38 \n64.17 \n64.17 \n67.91 \n70.59 \n49.73 \n24.06 \n46.52 \n47.06 \ngpt-oss-120B \n69.52 \n77.54 \n78.07 \n73.80 \n74.33 \n67.91 \n77.01 \n76.47 \n60.96 \n33.69 \n53.48 \n57.22 \nLLaMA-3.1-8B \n35.29 \n54.01 \n39.04 \n39.57 \n22.99 \n16.58 \n37.43 \n40.64 \n21.93 \n14.97 \n10.16 \n7.49 \nLLaMA-3.1-70B \n60.96 \n72.19 \n66.31 \n68.98 \n61.50 \n52.41 \n64.71 \n67.38 \n52.41 \n20.32 \n24.60 \n25.67 \nLLaMA-3.2-3B \n29.41 \n40.64 \n28.34 \n31.55 \n19.25 \n15.51 \n23.53 \n26.20 \n18.18 \n11.76 \n7.49 \n10.70 \nLLaMA-3.3-70B \n55.08 \n76.47 \n72.19 \n71.66 \n52.94 \n62.03 \n71.66 \n72.19 \n56.15 \n25.67 \n25.67 \n32.09 \nLLaMA-4-Scout \n71.12 \n75.40 \n73.26 \n70.05 \n64.17 \n68.45 \n73.26 \n72.73 \n64.71 \n28.34 \n29.41 \n55.08 \nLLaMA-4-Maverick \n74.33 \n79.68 \n76.47 \n74.87 \n74.87 \n73.26 \n79.14 \n75.94 \n72.73 \n37.43 \n53.48 \n58.29 \nMistral-7B-v0.3 \n13.37 \n25.67 \n12.83 \n21.39 \n11.23 \n13.90 \n17.65 \n15.51 \n6.95 \n9.09 \n10.70 \n9.09 \nMistral-Small-3.1-24B \n59.36 \n71.66 \n62.03 \n62.03 \n58.29 \n43.85 \n67.38 \n64.17 \n25.13 \n11.23 \n8.02 \n7.49 \nPhi-4-mini \n25.67 \n51.34 \n33.16 \n35.83 \n17.11 \n12.83 \n31.55 \n25.13 \n16.04 \n8.02 \n11.76 \n11.76 \nPhi-4-mini-Reasoning \n19.79 \n63.10 \n44.92 \n42.78 \n22.99 \n7.49 \n35.29 \n33.69 \n15.51 \n13.37 \n12.30 \n14.97 \nPhi-4 \n49.20 \n71.12 \n64.17 \n68.98 \n54.01 \n51.87 \n67.38 \n66.31 \n30.48 \n11.76 \n16.58 \n19.79 \nPhi-4-Reasoning \n66.84 \n72.73 \n76.47 \n70.59 \n69.52 \n63.10 \n66.31 \n74.33 \n57.75 \n17.11 \n34.76 \n24.06 \nQwen2.5-3B \n37.43 \n36.90 \n32.09 \n27.27 \n27.27 \n17.11 \n32.09 \n33.16 \n12.30 \n8.02 \n12.83 \n5.35 \nQwen2.5-7B \n44.39 \n59.89 \n51.87 \n43.32 \n40.11 \n32.09 \n45.99 \n45.45 \n16.04 \n9.09 \n13.37 \n8.56 \nQwen2.5-14B \n57.75 \n66.84 \n58.29 \n55.08 \n52.94 \n53.48 \n62.03 \n56.68 \n22.46 \n17.11 \n12.83 \n18.18 \nQwen2.5-72B \n69.52 \n75.40 \n68.98 \n69.52 \n66.31 \n61.50 \n70.05 \n70.59 \n33.16 \n18.18 \n15.51 \n22.46 \nQwQ-32B \n70.05 \n75.40 \n75.40 \n71.66 \n67.91 \n67.91 \n75.40 \n73.26 \n49.73 \n14.97 \n21.39 \n20.32 \nQwen3-1.7B \n30.48 \n36.36 \n28.34 \n21.93 \n20.86 \n18.72 \n21.39 \n25.67 \n12.83 \n9.63 \n11.76 \n14.44 \nQwen3-4B \n55.08 \n60.43 \n52.41 \n51.87 \n40.64 \n39.04 \n49.73 \n44.39 \n11.23 \n11.23 \n8.56 \n12.83 \nQwen3-4B-thinking \n58.82 \n62.57 \n64.17 \n58.82 \n57.75 \n55.61 \n63.64 \n61.50 \n15.51 \n11.76 \n11.23 \n12.30 \nQwen3-8B \n59.89 \n67.91 \n53.48 \n54.55 \n52.41 \n43.32 \n56.15 \n60.43 \n20.32 \n11.23 \n14.97 \n16.04 \nQwen3-8B-thinking \n63.10 \n71.12 \n66.84 \n66.31 \n64.17 \n63.64 \n66.84 \n67.91 \n28.88 \n13.90 \n16.58 \n10.70 \nQwen3-14B \n65.24 \n68.45 \n67.38 \n64.17 \n58.29 \n56.15 \n71.66 \n66.31 \n25.13 \n14.44 \n12.30 \n21.39 \nQwen3-14B-thinking \n67.91 \n73.80 \n73.80 \n71.66 \n68.45 \n66.84 \n68.45 \n73.26 \n44.39 \n14.44 \n14.97 \n14.44 \nBaichuan-M2-32B \n68.98 \n77.54 \n70.05 \n65.78 \n61.50 \n54.01 \n67.38 \n65.78 \n16.04 \n16.04 \n13.37 \n14.97 \nBio-Medical-LLaMA-3-8B \n30.48 \n55.61 \n36.36 \n39.57 \n29.41 \n25.13 \n35.29 \n39.57 \n26.74 \n19.79 \n13.37 \n14.97 \nMediPhi \n20.86 \n42.78 \n33.16 \n31.55 \n12.30 \n13.90 \n32.09 \n29.95 \n4.81 \n9.63 \n12.83 \n10.70 \nMedGemma-4B \n34.76 \n45.45 \n42.78 \n37.97 \n31.02 \n31.55 \n34.76 \n35.29 \n22.46 \n11.23 \n15.51 \n14.44 \nMedGemma-27B \n71.12 \n77.54 \n74.33 \n72.73 \n66.84 \n68.98 \n75.40 \n74.33 \n57.75 \n14.97 \n28.88 \n48.13 \nMedReason-8B \n38.50 \n47.06 \n20.32 \n22.99 \n36.90 \n26.20 \n20.86 \n27.27 \n16.58 \n12.30 \n9.63 \n11.76 \nHuatuoGPT-o1-7B \n54.55 \n55.61 \n60.96 \n57.22 \n47.06 \n41.71 \n59.89 \n59.89 \n11.23 \n11.23 \n9.63 \n10.70 \nHuatuoGPT-o1-8B \n45.45 \n52.94 \n49.20 \n53.48 \n41.18 \n34.76 \n47.59 \n44.39 \n27.81 \n13.37 \n11.23 \n11.76 \nHuatuoGPT-o1-70B \n63.64 \n71.66 \n67.38 \n69.52 \n60.96 \n64.71 \n70.59 \n72.19 \n56.68 \n24.60 \n29.95 \n40.64 \nHuatuoGPT-o1-72B \n68.45 \n74.87 \n73.80 \n67.38 \n70.59 \n66.31 \n75.94 \n75.40 \n46.52 \n17.65 \n22.46 \n16.58 \nOpenBioLLM-8B \n13.37 \n19.25 \n17.65 \n17.11 \n9.63 \n12.30 \n16.04 \n15.51 \n12.30 \n9.09 \n8.02 \n8.56 \nOpenBioLLM-70B \n21.39 \n63.64 \n57.22 \n40.11 \n20.32 \n24.60 \n41.18 \n59.89 \n26.74 \n6.95 \n8.56 \n13.37 \nSTab. 119: Zero-Shot performance evaluation of 56 LLMs on MMLU-Pro (Run 2). \n"}, {"page": 112, "text": " \n \n82 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n59.36 \n68.45 \n71.66 \n63.10 \n59.89 \n55.61 \n68.45 \n72.73 \n36.36 \n15.51 \n21.39 \n29.95 \nClaude-4.0-Sonnet \n77.54 \n82.35 \n81.28 \n78.07 \n79.68 \n75.94 \n79.68 \n81.82 \n68.98 \n36.36 \n56.15 \n63.10 \nGemini-2.5-Flash \n75.40 \n74.87 \n76.47 \n76.47 \n75.40 \n76.47 \n79.68 \n78.07 \n75.40 \n61.50 \n67.38 \n66.84 \nGPT-4o-mini \n56.68 \n70.59 \n67.91 \n67.38 \n63.10 \n59.89 \n66.84 \n68.98 \n46.52 \n17.11 \n25.67 \n33.16 \nGPT-4o \n72.19 \n77.01 \n77.01 \n72.73 \n72.19 \n68.45 \n77.01 \n78.61 \n66.84 \n24.60 \n42.25 \n56.68 \nGPT-4.1-nano \n60.96 \n70.59 \n67.38 \n66.31 \n57.75 \n45.45 \n66.84 \n69.52 \n35.29 \n16.58 \n20.86 \n28.34 \nGPT-4.1-mini \n67.91 \n77.01 \n73.26 \n74.33 \n70.59 \n66.31 \n74.87 \n75.40 \n63.64 \n13.90 \n42.78 \n51.34 \nGPT-4.1 \n75.94 \n75.94 \n78.61 \n74.87 \n74.33 \n70.05 \n81.28 \n78.07 \n72.19 \n34.22 \n49.20 \n63.10 \nGPT-5-nano \n50.80 \n62.57 \n55.61 \n63.64 \n54.01 \n40.11 \n57.22 \n59.89 \n39.57 \n13.37 \n20.86 \n29.41 \nGPT-5-mini \n71.66 \n76.47 \n76.47 \n71.66 \n70.59 \n65.24 \n73.26 \n78.07 \n60.43 \n21.39 \n42.25 \n56.68 \nGPT-5 \n74.87 \n78.07 \n75.40 \n73.26 \n72.73 \n71.12 \n78.61 \n80.21 \n70.59 \n34.22 \n56.68 \n66.31 \no4-mini \n73.80 \n79.68 \n80.21 \n73.26 \n75.94 \n72.73 \n79.68 \n78.07 \n71.12 \n29.95 \n61.50 \n67.91 \nOpen-Weight LLMs \nDeepSeek-V3 \n75.40 \n80.21 \n76.47 \n77.54 \n73.80 \n67.38 \n79.14 \n74.87 \n66.84 \n28.88 \n36.90 \n45.99 \nDeepSeek-R1 \n71.12 \n78.07 \n77.01 \n76.47 \n77.01 \n65.78 \n77.54 \n77.54 \n75.40 \n45.45 \n56.15 \n62.57 \nDeepSeek-R1-Qwen3-8B \n61.50 \n67.38 \n52.94 \n62.03 \n54.01 \n56.68 \n61.50 \n59.36 \n19.25 \n10.70 \n11.76 \n8.56 \nGemma-3-4B \n29.41 \n36.90 \n32.62 \n35.29 \n26.20 \n23.53 \n29.95 \n27.81 \n22.46 \n10.16 \n10.70 \n13.90 \nGemma-3-12B \n48.66 \n61.50 \n59.36 \n60.43 \n52.94 \n51.34 \n57.22 \n59.36 \n47.59 \n9.63 \n25.67 \n31.02 \nGemma-3-27B \n62.03 \n65.78 \n71.66 \n64.71 \n58.82 \n59.36 \n73.26 \n66.31 \n51.87 \n14.44 \n27.81 \n43.32 \ngpt-oss-20B \n63.64 \n65.78 \n66.31 \n66.84 \n63.64 \n60.43 \n69.52 \n67.91 \n50.27 \n21.93 \n38.50 \n45.45 \ngpt-oss-120B \n70.05 \n78.07 \n76.47 \n75.94 \n74.33 \n68.45 \n76.47 \n72.19 \n62.03 \n40.11 \n51.87 \n57.22 \nLLaMA-3.1-8B \n36.90 \n55.08 \n37.43 \n36.90 \n23.53 \n21.39 \n37.97 \n37.43 \n23.53 \n13.90 \n10.70 \n9.09 \nLLaMA-3.1-70B \n57.22 \n71.12 \n66.31 \n70.05 \n58.29 \n50.27 \n68.98 \n69.52 \n52.94 \n21.93 \n22.46 \n33.16 \nLLaMA-3.2-3B \n28.88 \n37.97 \n29.95 \n31.02 \n16.04 \n12.30 \n24.60 \n20.86 \n14.44 \n13.90 \n4.81 \n10.16 \nLLaMA-3.3-70B \n51.87 \n75.94 \n74.33 \n72.19 \n54.01 \n59.89 \n71.66 \n73.80 \n58.29 \n25.67 \n27.27 \n35.83 \nLLaMA-4-Scout \n68.98 \n75.40 \n73.26 \n70.05 \n65.24 \n66.31 \n73.26 \n73.26 \n64.17 \n31.55 \n29.41 \n52.94 \nLLaMA-4-Maverick \n74.33 \n81.28 \n79.14 \n77.01 \n73.80 \n71.66 \n76.47 \n78.07 \n71.12 \n37.97 \n52.94 \n58.29 \nMistral-7B-v0.3 \n15.51 \n16.04 \n13.37 \n17.11 \n13.90 \n9.09 \n17.65 \n18.72 \n8.56 \n9.63 \n12.30 \n10.16 \nMistral-Small-3.1-24B \n55.61 \n70.59 \n71.12 \n63.10 \n59.36 \n50.27 \n64.71 \n68.45 \n29.95 \n13.90 \n11.23 \n12.83 \nPhi-4-mini \n27.27 \n43.85 \n33.69 \n29.95 \n17.65 \n18.72 \n23.53 \n29.41 \n15.51 \n12.30 \n11.23 \n11.23 \nPhi-4-mini-Reasoning \n22.46 \n57.22 \n47.59 \n41.18 \n17.11 \n12.83 \n35.29 \n32.09 \n18.72 \n19.79 \n8.02 \n12.30 \nPhi-4 \n49.20 \n72.19 \n64.17 \n66.31 \n58.82 \n43.85 \n68.98 \n65.24 \n34.22 \n14.44 \n18.18 \n16.58 \nPhi-4-Reasoning \n68.45 \n72.19 \n74.33 \n70.59 \n71.12 \n64.71 \n70.05 \n74.33 \n55.08 \n19.25 \n32.62 \n27.81 \nQwen2.5-3B \n39.04 \n39.57 \n29.41 \n32.09 \n23.53 \n24.06 \n33.16 \n34.22 \n11.76 \n8.56 \n13.90 \n16.58 \nQwen2.5-7B \n45.99 \n60.96 \n50.80 \n45.45 \n38.50 \n32.62 \n45.45 \n48.13 \n18.18 \n10.70 \n10.70 \n11.23 \nQwen2.5-14B \n60.96 \n70.05 \n53.48 \n59.36 \n52.94 \n48.13 \n59.36 \n56.68 \n21.39 \n18.72 \n15.51 \n21.93 \nQwen2.5-72B \n69.52 \n73.26 \n68.45 \n67.38 \n67.91 \n60.96 \n71.12 \n68.45 \n37.97 \n19.25 \n14.97 \n19.79 \nQwQ-32B \n71.66 \n72.73 \n73.80 \n74.33 \n67.91 \n63.10 \n78.07 \n77.01 \n47.06 \n21.39 \n20.32 \n18.72 \nQwen3-1.7B \n29.41 \n41.71 \n31.55 \n19.79 \n21.39 \n19.79 \n24.06 \n28.88 \n13.37 \n11.23 \n11.76 \n11.23 \nQwen3-4B \n51.34 \n56.15 \n50.27 \n48.13 \n43.32 \n35.83 \n49.20 \n50.27 \n15.51 \n11.76 \n10.70 \n9.09 \nQwen3-4B-thinking \n58.82 \n65.78 \n66.31 \n59.89 \n58.82 \n51.34 \n58.82 \n60.96 \n12.30 \n12.83 \n12.30 \n7.49 \nQwen3-8B \n55.08 \n65.78 \n54.55 \n56.68 \n51.87 \n45.99 \n54.01 \n54.01 \n23.53 \n12.30 \n12.83 \n11.23 \nQwen3-8B-thinking \n59.89 \n72.19 \n72.19 \n65.24 \n63.10 \n61.50 \n67.38 \n67.38 \n21.39 \n16.04 \n14.97 \n14.44 \nQwen3-14B \n63.10 \n74.87 \n64.71 \n67.91 \n60.43 \n59.36 \n69.52 \n67.91 \n24.06 \n8.02 \n12.83 \n15.51 \nQwen3-14B-thinking \n65.78 \n75.94 \n73.26 \n70.59 \n72.73 \n68.98 \n73.26 \n68.98 \n48.13 \n10.16 \n17.11 \n14.44 \nBaichuan-M2-32B \n71.12 \n73.80 \n67.38 \n68.45 \n64.17 \n57.22 \n67.38 \n62.03 \n18.18 \n21.39 \n20.32 \n15.51 \nBio-Medical-LLaMA-3-8B \n31.55 \n56.68 \n34.22 \n39.04 \n26.20 \n21.93 \n34.76 \n38.50 \n23.53 \n17.65 \n14.44 \n16.04 \nMediPhi \n17.65 \n38.50 \n32.09 \n26.74 \n11.23 \n9.09 \n32.62 \n31.02 \n9.63 \n12.30 \n11.76 \n10.70 \nMedGemma-4B \n34.22 \n47.59 \n44.92 \n34.22 \n33.16 \n32.09 \n35.29 \n36.36 \n26.74 \n16.58 \n11.23 \n13.90 \nMedGemma-27B \n69.52 \n75.40 \n73.80 \n72.73 \n70.05 \n66.31 \n71.12 \n73.80 \n62.03 \n16.58 \n29.95 \n51.34 \nMedReason-8B \n37.43 \n41.71 \n19.79 \n20.86 \n35.83 \n25.67 \n25.67 \n29.95 \n13.90 \n11.76 \n9.09 \n10.70 \nHuatuoGPT-o1-7B \n55.61 \n59.36 \n57.22 \n54.01 \n49.20 \n41.71 \n59.36 \n60.96 \n9.63 \n11.23 \n7.49 \n10.70 \nHuatuoGPT-o1-8B \n48.66 \n55.08 \n52.94 \n45.99 \n40.64 \n33.69 \n49.73 \n48.13 \n28.88 \n13.37 \n8.56 \n10.16 \nHuatuoGPT-o1-70B \n57.75 \n72.19 \n69.52 \n67.91 \n59.89 \n66.84 \n73.26 \n70.59 \n56.15 \n26.20 \n27.81 \n37.43 \nHuatuoGPT-o1-72B \n69.52 \n77.01 \n74.87 \n70.59 \n72.19 \n65.24 \n72.73 \n73.26 \n45.45 \n18.18 \n25.67 \n13.90 \nOpenBioLLM-8B \n13.90 \n20.32 \n18.18 \n13.90 \n10.16 \n6.42 \n17.65 \n14.44 \n9.09 \n9.63 \n11.23 \n8.56 \nOpenBioLLM-70B \n27.81 \n63.64 \n57.75 \n37.97 \n21.93 \n18.18 \n37.97 \n59.89 \n29.41 \n9.63 \n10.70 \n13.90 \nSTab. 120: Zero-Shot performance evaluation of 56 LLMs on MMLU-Pro (Run 3). \n"}, {"page": 113, "text": " \n \n83 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n59.36 \n68.45 \n71.66 \n63.10 \n59.89 \n55.61 \n68.45 \n72.73 \n36.36 \n13.37 \n20.86 \n29.95 \nClaude-4.0-Sonnet \n77.54 \n83.42 \n79.68 \n76.47 \n78.61 \n76.47 \n79.14 \n79.68 \n68.98 \n36.36 \n57.75 \n61.50 \nGemini-2.5-Flash \n77.54 \n75.94 \n75.94 \n80.21 \n75.40 \n74.87 \n77.54 \n77.01 \n74.87 \n64.71 \n70.59 \n68.98 \nGPT-4o-mini \n58.29 \n71.12 \n68.45 \n65.24 \n58.29 \n55.61 \n66.31 \n68.45 \n46.52 \n17.11 \n31.02 \n34.22 \nGPT-4o \n73.26 \n73.26 \n73.80 \n74.87 \n71.66 \n68.98 \n77.01 \n75.40 \n66.84 \n26.74 \n43.32 \n52.94 \nGPT-4.1-nano \n56.68 \n71.66 \n64.17 \n65.24 \n52.41 \n44.39 \n67.38 \n65.78 \n40.11 \n13.90 \n22.46 \n29.95 \nGPT-4.1-mini \n68.98 \n77.01 \n71.66 \n74.33 \n70.59 \n66.31 \n73.26 \n72.19 \n59.36 \n14.44 \n44.92 \n51.87 \nGPT-4.1 \n74.87 \n78.61 \n77.54 \n78.07 \n72.73 \n73.26 \n80.21 \n78.61 \n74.87 \n34.76 \n44.39 \n62.57 \nGPT-5-nano \n45.45 \n67.91 \n58.29 \n58.29 \n51.34 \n38.50 \n59.89 \n60.96 \n40.11 \n13.37 \n17.65 \n26.20 \nGPT-5-mini \n72.73 \n76.47 \n75.40 \n77.01 \n68.98 \n60.96 \n75.40 \n75.94 \n58.82 \n21.93 \n44.39 \n54.01 \nGPT-5 \n72.73 \n75.94 \n77.54 \n75.40 \n74.33 \n66.84 \n80.21 \n79.68 \n70.59 \n40.11 \n52.94 \n59.89 \no4-mini \n74.33 \n79.68 \n79.14 \n74.33 \n74.33 \n69.52 \n80.75 \n79.14 \n71.66 \n31.02 \n59.36 \n68.98 \nOpen-Weight LLMs \nDeepSeek-V3 \n73.80 \n80.75 \n75.94 \n75.94 \n75.94 \n69.52 \n78.07 \n73.80 \n66.31 \n31.02 \n35.83 \n48.66 \nDeepSeek-R1 \n69.52 \n77.54 \n77.01 \n77.54 \n76.47 \n63.10 \n78.07 \n77.54 \n74.87 \n49.20 \n61.50 \n59.89 \nDeepSeek-R1-Qwen3-8B \n63.10 \n68.98 \n59.36 \n57.22 \n54.55 \n59.36 \n66.84 \n62.03 \n19.79 \n14.97 \n8.56 \n12.83 \nGemma-3-4B \n32.62 \n40.64 \n30.48 \n33.16 \n27.27 \n25.67 \n32.62 \n31.55 \n21.93 \n13.90 \n13.37 \n10.16 \nGemma-3-12B \n54.55 \n60.96 \n59.89 \n55.61 \n51.34 \n52.41 \n61.50 \n59.89 \n43.85 \n8.02 \n22.99 \n32.09 \nGemma-3-27B \n56.68 \n65.78 \n70.05 \n66.31 \n57.75 \n58.82 \n70.05 \n72.19 \n57.22 \n13.90 \n28.34 \n45.45 \ngpt-oss-20B \n60.43 \n66.84 \n68.98 \n67.38 \n63.64 \n64.71 \n70.05 \n72.19 \n49.73 \n22.46 \n43.32 \n49.73 \ngpt-oss-120B \n70.59 \n78.07 \n77.54 \n73.26 \n71.12 \n68.45 \n75.94 \n77.01 \n64.17 \n36.36 \n58.29 \n56.68 \nLLaMA-3.1-8B \n32.62 \n56.15 \n41.18 \n37.97 \n26.74 \n17.11 \n38.50 \n42.78 \n22.46 \n18.72 \n13.37 \n6.95 \nLLaMA-3.1-70B \n57.22 \n71.12 \n67.91 \n66.31 \n60.96 \n52.41 \n66.31 \n72.73 \n47.59 \n19.25 \n19.25 \n27.27 \nLLaMA-3.2-3B \n31.55 \n44.39 \n27.81 \n28.88 \n20.86 \n12.83 \n22.46 \n28.88 \n16.58 \n12.83 \n8.56 \n13.37 \nLLaMA-3.3-70B \n51.34 \n74.87 \n74.87 \n73.26 \n48.13 \n59.36 \n71.66 \n73.26 \n57.22 \n23.53 \n25.67 \n35.29 \nLLaMA-4-Scout \n67.91 \n75.40 \n71.66 \n71.12 \n66.84 \n66.84 \n73.26 \n73.80 \n59.36 \n28.88 \n28.88 \n54.55 \nLLaMA-4-Maverick \n73.80 \n81.28 \n77.01 \n76.47 \n77.01 \n71.66 \n76.47 \n78.61 \n73.80 \n36.36 \n53.48 \n60.96 \nMistral-7B-v0.3 \n13.37 \n23.53 \n12.83 \n17.65 \n12.83 \n11.23 \n18.18 \n17.11 \n9.63 \n11.23 \n8.56 \n8.56 \nMistral-Small-3.1-24B \n57.75 \n72.19 \n63.64 \n65.78 \n57.75 \n52.41 \n62.57 \n64.71 \n20.32 \n10.70 \n11.76 \n13.90 \nPhi-4-mini \n22.46 \n47.06 \n29.41 \n32.62 \n24.06 \n14.97 \n28.88 \n26.20 \n16.58 \n10.16 \n9.63 \n9.63 \nPhi-4-mini-Reasoning \n21.93 \n59.36 \n38.50 \n43.85 \n20.32 \n13.90 \n42.78 \n27.81 \n20.86 \n13.37 \n11.76 \n13.90 \nPhi-4 \n53.48 \n72.73 \n68.98 \n68.45 \n56.68 \n49.73 \n68.45 \n65.78 \n35.83 \n14.97 \n19.25 \n20.32 \nPhi-4-Reasoning \n66.31 \n76.47 \n73.26 \n73.26 \n72.19 \n66.84 \n68.45 \n74.87 \n53.48 \n15.51 \n33.16 \n25.13 \nQwen2.5-3B \n36.36 \n41.71 \n28.88 \n26.20 \n22.99 \n23.53 \n28.88 \n29.95 \n11.23 \n12.83 \n7.49 \n8.02 \nQwen2.5-7B \n48.66 \n60.96 \n49.20 \n43.32 \n41.18 \n27.81 \n48.66 \n44.92 \n18.72 \n11.23 \n11.76 \n11.23 \nQwen2.5-14B \n62.03 \n68.98 \n58.29 \n55.61 \n52.41 \n50.27 \n64.17 \n63.64 \n18.72 \n13.37 \n12.83 \n19.79 \nQwen2.5-72B \n70.05 \n73.80 \n70.59 \n66.31 \n67.91 \n64.17 \n72.73 \n68.98 \n33.69 \n19.25 \n14.97 \n20.86 \nQwQ-32B \n70.05 \n74.33 \n73.26 \n72.19 \n69.52 \n63.10 \n76.47 \n75.40 \n44.92 \n18.72 \n19.25 \n22.99 \nQwen3-1.7B \n30.48 \n37.97 \n26.20 \n27.27 \n18.72 \n23.53 \n25.67 \n22.99 \n15.51 \n9.09 \n12.30 \n13.37 \nQwen3-4B \n54.01 \n59.36 \n47.59 \n50.80 \n46.52 \n40.64 \n46.52 \n51.87 \n11.76 \n10.16 \n9.09 \n10.70 \nQwen3-4B-thinking \n57.22 \n62.57 \n65.78 \n61.50 \n56.68 \n55.61 \n60.43 \n62.57 \n15.51 \n11.23 \n17.65 \n6.42 \nQwen3-8B \n57.75 \n66.31 \n54.55 \n54.01 \n55.61 \n49.73 \n55.61 \n60.96 \n19.25 \n13.37 \n14.44 \n14.97 \nQwen3-8B-thinking \n64.71 \n70.59 \n68.98 \n69.52 \n66.31 \n60.96 \n65.78 \n68.98 \n24.06 \n10.16 \n12.30 \n13.37 \nQwen3-14B \n64.71 \n72.19 \n64.71 \n65.78 \n55.08 \n57.75 \n66.84 \n63.10 \n25.13 \n9.63 \n14.44 \n15.51 \nQwen3-14B-thinking \n67.38 \n78.07 \n72.73 \n73.26 \n72.19 \n65.24 \n69.52 \n71.66 \n44.39 \n13.37 \n19.79 \n16.04 \nBaichuan-M2-32B \n71.66 \n76.47 \n71.12 \n69.52 \n67.91 \n55.08 \n70.05 \n67.91 \n19.25 \n13.90 \n18.72 \n13.37 \nBio-Medical-LLaMA-3-8B \n28.34 \n54.01 \n38.50 \n38.50 \n28.88 \n21.93 \n35.83 \n39.57 \n27.81 \n21.39 \n13.37 \n16.04 \nMediPhi \n17.65 \n41.71 \n38.50 \n27.27 \n12.30 \n9.09 \n32.09 \n29.95 \n12.83 \n12.83 \n11.23 \n8.56 \nMedGemma-4B \n32.09 \n47.06 \n44.92 \n38.50 \n30.48 \n26.74 \n38.50 \n35.83 \n25.13 \n12.30 \n10.70 \n19.79 \nMedGemma-27B \n65.78 \n72.73 \n75.40 \n75.40 \n70.05 \n66.84 \n74.33 \n73.80 \n63.10 \n18.18 \n29.95 \n43.32 \nMedReason-8B \n34.22 \n49.20 \n20.86 \n22.46 \n36.90 \n27.81 \n24.60 \n26.74 \n12.83 \n11.23 \n9.09 \n11.23 \nHuatuoGPT-o1-7B \n54.55 \n58.82 \n55.61 \n50.27 \n50.80 \n39.04 \n59.89 \n57.22 \n9.09 \n10.70 \n10.70 \n9.63 \nHuatuoGPT-o1-8B \n47.59 \n53.48 \n56.15 \n52.41 \n41.71 \n37.43 \n49.73 \n40.11 \n30.48 \n10.70 \n11.76 \n11.23 \nHuatuoGPT-o1-70B \n60.96 \n73.26 \n71.12 \n71.12 \n59.89 \n63.64 \n67.38 \n69.52 \n56.15 \n22.46 \n31.55 \n41.71 \nHuatuoGPT-o1-72B \n70.05 \n74.87 \n73.26 \n68.98 \n71.66 \n64.17 \n74.87 \n72.19 \n44.39 \n20.86 \n24.06 \n20.32 \nOpenBioLLM-8B \n12.30 \n24.06 \n12.30 \n17.11 \n11.23 \n14.97 \n15.51 \n16.58 \n10.70 \n13.90 \n5.35 \n10.70 \nOpenBioLLM-70B \n29.41 \n60.96 \n52.94 \n39.04 \n25.67 \n21.39 \n45.99 \n62.03 \n25.13 \n8.56 \n11.23 \n18.18 \nSTab. 121: Zero-Shot performance evaluation of 56 LLMs on MMLU-Pro (Run 4). \n"}, {"page": 114, "text": " \n \n84 \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nProprietary LLMs \nClaude-3.5-Haiku \n59.36 \n68.45 \n71.66 \n63.10 \n59.89 \n55.61 \n68.45 \n72.19 \n36.36 \n14.97 \n21.39 \n29.41 \nClaude-4.0-Sonnet \n80.21 \n82.35 \n80.21 \n79.14 \n78.61 \n77.01 \n80.21 \n81.82 \n71.66 \n35.83 \n54.55 \n64.17 \nGemini-2.5-Flash \n76.47 \n77.01 \n77.54 \n76.47 \n75.94 \n77.54 \n80.21 \n79.14 \n75.94 \n62.57 \n68.45 \n67.91 \nGPT-4o-mini \n58.82 \n72.73 \n70.59 \n64.17 \n63.10 \n57.75 \n64.71 \n68.98 \n50.27 \n16.58 \n26.74 \n35.83 \nGPT-4o \n74.33 \n78.07 \n73.80 \n71.12 \n74.33 \n71.12 \n74.87 \n73.80 \n66.84 \n29.41 \n43.32 \n57.75 \nGPT-4.1-nano \n52.94 \n75.40 \n65.24 \n65.78 \n55.08 \n48.66 \n65.24 \n71.12 \n40.64 \n18.72 \n20.32 \n33.16 \nGPT-4.1-mini \n71.12 \n79.14 \n73.80 \n74.87 \n68.98 \n66.31 \n77.01 \n72.19 \n64.17 \n16.04 \n44.92 \n51.34 \nGPT-4.1 \n73.26 \n78.61 \n78.07 \n74.87 \n75.40 \n69.52 \n79.68 \n80.21 \n75.40 \n26.74 \n45.45 \n63.64 \nGPT-5-nano \n47.06 \n66.31 \n57.75 \n60.96 \n51.87 \n43.85 \n54.01 \n59.89 \n37.97 \n10.16 \n23.53 \n25.13 \nGPT-5-mini \n73.26 \n77.01 \n70.59 \n73.26 \n73.26 \n64.17 \n74.33 \n74.87 \n65.24 \n19.79 \n43.85 \n54.01 \nGPT-5 \n77.01 \n75.94 \n77.54 \n71.12 \n73.26 \n68.98 \n81.28 \n79.14 \n69.52 \n35.83 \n58.82 \n61.50 \no4-mini \n74.87 \n80.21 \n79.68 \n75.94 \n75.40 \n68.98 \n81.28 \n81.28 \n73.80 \n33.16 \n61.50 \n67.38 \nOpen-Weight LLMs \nDeepSeek-V3 \n74.87 \n79.68 \n75.94 \n75.40 \n76.47 \n68.98 \n77.54 \n75.94 \n64.17 \n33.16 \n36.36 \n45.45 \nDeepSeek-R1 \n69.52 \n78.07 \n79.68 \n75.94 \n76.47 \n66.31 \n77.54 \n75.94 \n75.40 \n44.92 \n54.55 \n61.50 \nDeepSeek-R1-Qwen3-8B \n59.36 \n68.45 \n63.64 \n62.57 \n51.34 \n55.61 \n64.71 \n60.43 \n16.04 \n10.70 \n14.44 \n11.76 \nGemma-3-4B \n31.02 \n44.39 \n32.09 \n32.62 \n27.81 \n22.99 \n27.27 \n28.88 \n21.39 \n9.63 \n9.63 \n6.42 \nGemma-3-12B \n51.34 \n60.96 \n57.75 \n60.96 \n56.15 \n55.61 \n60.96 \n59.89 \n44.92 \n12.83 \n22.46 \n32.62 \nGemma-3-27B \n63.10 \n67.38 \n65.24 \n63.10 \n58.82 \n58.82 \n68.45 \n67.91 \n54.01 \n13.90 \n27.81 \n45.45 \ngpt-oss-20B \n63.10 \n64.17 \n70.05 \n70.05 \n64.17 \n65.24 \n67.91 \n67.91 \n47.06 \n22.46 \n41.18 \n45.45 \ngpt-oss-120B \n70.05 \n74.87 \n74.33 \n74.87 \n72.19 \n72.73 \n75.40 \n77.54 \n59.89 \n32.09 \n52.94 \n59.36 \nLLaMA-3.1-8B \n33.69 \n52.41 \n41.71 \n39.57 \n23.53 \n25.13 \n36.90 \n41.18 \n21.93 \n17.65 \n5.88 \n12.83 \nLLaMA-3.1-70B \n60.43 \n73.80 \n65.24 \n64.71 \n56.15 \n52.41 \n65.78 \n66.31 \n49.73 \n19.79 \n23.53 \n31.55 \nLLaMA-3.2-3B \n28.88 \n40.11 \n24.60 \n33.69 \n23.53 \n11.76 \n21.39 \n26.20 \n14.97 \n7.49 \n12.83 \n9.63 \nLLaMA-3.3-70B \n51.87 \n75.94 \n74.87 \n72.19 \n51.34 \n60.96 \n71.66 \n72.73 \n56.68 \n21.39 \n27.81 \n33.16 \nLLaMA-4-Scout \n69.52 \n72.19 \n74.33 \n70.05 \n67.38 \n67.91 \n75.40 \n71.12 \n61.50 \n27.81 \n29.95 \n53.48 \nLLaMA-4-Maverick \n74.87 \n80.75 \n77.01 \n74.33 \n71.12 \n70.59 \n77.01 \n77.01 \n71.66 \n36.36 \n56.68 \n59.89 \nMistral-7B-v0.3 \n14.97 \n20.32 \n14.97 \n13.90 \n12.83 \n10.16 \n16.04 \n16.04 \n10.70 \n10.16 \n9.63 \n12.83 \nMistral-Small-3.1-24B \n54.01 \n66.84 \n65.24 \n65.78 \n60.96 \n46.52 \n63.10 \n66.84 \n20.86 \n8.02 \n11.23 \n9.09 \nPhi-4-mini \n28.88 \n51.34 \n35.83 \n29.95 \n25.67 \n19.25 \n24.60 \n27.27 \n16.04 \n11.23 \n11.23 \n9.63 \nPhi-4-mini-Reasoning \n17.65 \n58.82 \n42.25 \n46.52 \n17.11 \n12.83 \n27.81 \n32.62 \n13.90 \n10.70 \n11.23 \n14.44 \nPhi-4 \n51.87 \n73.26 \n64.71 \n70.05 \n57.22 \n50.80 \n67.38 \n67.38 \n33.69 \n11.76 \n18.72 \n14.97 \nPhi-4-Reasoning \n67.91 \n75.94 \n73.26 \n72.19 \n72.73 \n66.84 \n68.98 \n72.73 \n56.68 \n14.97 \n31.02 \n24.06 \nQwen2.5-3B \n35.83 \n41.18 \n34.76 \n30.48 \n25.13 \n20.86 \n30.48 \n32.09 \n11.23 \n10.16 \n11.76 \n8.02 \nQwen2.5-7B \n51.87 \n59.36 \n46.52 \n40.11 \n43.85 \n33.69 \n45.45 \n47.59 \n16.58 \n12.83 \n14.97 \n10.70 \nQwen2.5-14B \n56.68 \n66.31 \n59.89 \n59.89 \n51.87 \n48.13 \n60.43 \n59.36 \n19.25 \n14.97 \n14.97 \n18.72 \nQwen2.5-72B \n70.59 \n72.19 \n70.59 \n69.52 \n66.31 \n59.89 \n69.52 \n70.05 \n37.97 \n17.11 \n16.04 \n21.39 \nQwQ-32B \n68.45 \n77.01 \n72.73 \n74.33 \n69.52 \n65.24 \n75.94 \n75.94 \n44.39 \n18.18 \n19.25 \n20.32 \nQwen3-1.7B \n29.41 \n40.11 \n27.27 \n25.13 \n20.86 \n18.72 \n25.67 \n25.67 \n13.37 \n12.30 \n12.30 \n14.44 \nQwen3-4B \n50.80 \n60.43 \n49.73 \n52.41 \n44.92 \n35.29 \n47.06 \n53.48 \n12.83 \n11.23 \n13.37 \n11.23 \nQwen3-4B-thinking \n58.82 \n63.10 \n62.03 \n63.10 \n57.75 \n55.61 \n60.43 \n60.96 \n17.65 \n13.37 \n9.63 \n12.30 \nQwen3-8B \n60.43 \n66.31 \n55.61 \n56.15 \n51.87 \n43.85 \n58.29 \n58.29 \n15.51 \n11.76 \n10.16 \n8.56 \nQwen3-8B-thinking \n64.71 \n73.26 \n68.45 \n67.91 \n64.17 \n60.43 \n68.45 \n67.38 \n20.86 \n10.16 \n12.83 \n13.90 \nQwen3-14B \n66.84 \n71.12 \n63.64 \n65.78 \n58.29 \n53.48 \n68.98 \n65.24 \n23.53 \n12.30 \n13.37 \n12.83 \nQwen3-14B-thinking \n65.78 \n74.33 \n71.66 \n70.59 \n70.05 \n64.71 \n71.12 \n70.59 \n44.92 \n13.37 \n19.25 \n15.51 \nBaichuan-M2-32B \n66.84 \n75.94 \n69.52 \n68.98 \n61.50 \n57.75 \n67.38 \n68.45 \n22.46 \n16.04 \n20.86 \n17.11 \nBio-Medical-LLaMA-3-8B \n28.34 \n55.08 \n37.43 \n41.18 \n29.41 \n23.53 \n36.36 \n39.04 \n26.74 \n18.18 \n13.37 \n14.97 \nMediPhi \n22.99 \n44.92 \n32.62 \n26.20 \n14.44 \n9.09 \n28.88 \n30.48 \n7.49 \n9.63 \n10.16 \n11.23 \nMedGemma-4B \n33.16 \n43.32 \n41.18 \n37.97 \n28.34 \n29.41 \n36.36 \n38.50 \n21.93 \n10.70 \n14.44 \n19.25 \nMedGemma-27B \n70.05 \n76.47 \n70.59 \n74.33 \n68.45 \n65.78 \n72.73 \n70.05 \n62.03 \n12.83 \n29.41 \n45.45 \nMedReason-8B \n39.57 \n51.87 \n20.86 \n20.32 \n39.57 \n25.13 \n16.58 \n34.22 \n15.51 \n12.30 \n9.63 \n11.23 \nHuatuoGPT-o1-7B \n52.94 \n63.10 \n58.29 \n54.55 \n47.59 \n42.25 \n58.82 \n60.96 \n11.76 \n10.70 \n12.30 \n11.76 \nHuatuoGPT-o1-8B \n45.99 \n54.55 \n54.55 \n47.59 \n38.50 \n35.83 \n47.59 \n47.59 \n29.41 \n9.09 \n12.30 \n6.95 \nHuatuoGPT-o1-70B \n60.96 \n74.87 \n72.19 \n67.91 \n60.96 \n67.91 \n71.66 \n70.05 \n58.29 \n22.46 \n31.55 \n37.43 \nHuatuoGPT-o1-72B \n67.38 \n77.01 \n73.26 \n71.66 \n67.91 \n66.31 \n74.87 \n73.26 \n47.06 \n24.06 \n24.06 \n20.32 \nOpenBioLLM-8B \n10.70 \n19.79 \n15.51 \n18.18 \n13.37 \n8.56 \n18.72 \n16.04 \n11.76 \n9.09 \n11.23 \n11.76 \nOpenBioLLM-70B \n24.60 \n62.03 \n58.29 \n35.29 \n27.27 \n21.93 \n41.18 \n59.36 \n25.67 \n10.16 \n13.90 \n13.37 \nSTab. 122: Zero-Shot performance evaluation of 56 LLMs on MMLU-Pro (Run 5). \n"}, {"page": 115, "text": " \n \n85 \nS2.4. Language Disparity Analysis \n \nSFig. 2: Multilingual performance evaluation on 6 medical benchmarks with \nClaude-3.5-Haiku (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nClaude-3.5-Haiku\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n**\n"}, {"page": 116, "text": " \n \n86 \n \nSFig. 3: Multilingual performance evaluation on 6 medical benchmarks with \nClaude-4.0-Sonnet (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nClaude-4.0-Sonnet\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n*\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n**\n*\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n**\n**\n***\n**\n***\n***\n***\n***\n"}, {"page": 117, "text": " \n \n87 \n \nSFig. 4: Multilingual performance evaluation on 6 medical benchmarks with \nGemini-2.5-Flash (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nGemini-2.5-Flash\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n*\n**\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n***\n*\n*\n*\n"}, {"page": 118, "text": " \n \n88 \n \nSFig. 5: Multilingual performance evaluation on 6 medical benchmarks with GPT-\n4o-mini (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nGPT-4o-mini\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n**\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n*\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n**\n***\n***\n**\n*\n***\n***\n***\n***\n"}, {"page": 119, "text": " \n \n89 \n \nSFig. 6: Multilingual performance evaluation on 6 medical benchmarks with GPT-\n4o (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The experiment \ncompared the accuracy disparities between the original language and target languages, \nwith each condition repeated five times. Statistical significance is indicated by asterisks \n(*p<0.05, **p<0.01, ***p<0.001). \nGPT-4o\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n**\n**\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n**\n*\n*\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n*\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n***\n**\n***\n***\n***\n"}, {"page": 120, "text": " \n \n90 \n \nSFig. 7: Multilingual performance evaluation on 6 medical benchmarks with GPT-\n4.1-nano (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nGPT-4.1-nano\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n*\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n*\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n**\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n***\n**\n**\n***\n***\n***\n***\n"}, {"page": 121, "text": " \n \n91 \n \nSFig. 8: Multilingual performance evaluation on 6 medical benchmarks with GPT-\n4.1-mini (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nGPT-4.1-mini\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n*\n***\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n**\n*\n***\n***\n***\n***\n*\n*\n"}, {"page": 122, "text": " \n \n92 \n \nSFig. 9: Multilingual performance evaluation on 6 medical benchmarks with GPT-\n4.1 (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The experiment \ncompared the accuracy disparities between the original language and target languages, \nwith each condition repeated five times. Statistical significance is indicated by asterisks \n(*p<0.05, **p<0.01, ***p<0.001). \nGPT-4.1\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n*\n**\n**\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n*\n*\n**\n*\n***\n***\n***\n***\n"}, {"page": 123, "text": " \n \n93 \n \nSFig. 10: Multilingual performance evaluation on 6 medical benchmarks with GPT-\n5-nano (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The experiment \ncompared the accuracy disparities between the original language and target languages, \nwith each condition repeated five times. Statistical significance is indicated by asterisks \n(*p<0.05, **p<0.01, ***p<0.001). \nGPT-5-nano\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n**\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n*\n***\n***\n**\n**\n***\n***\n***\n***\n"}, {"page": 124, "text": " \n \n94 \n \nSFig. 11: Multilingual performance evaluation on 6 medical benchmarks with GPT-\n5-mini (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The experiment \ncompared the accuracy disparities between the original language and target languages, \nwith each condition repeated five times. Statistical significance is indicated by asterisks \n(*p<0.05, **p<0.01, ***p<0.001). \nGPT-5-mini\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n**\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n*\n**\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n**\n***\n*\n***\n***\n***\n***\n"}, {"page": 125, "text": " \n \n95 \n \nSFig. 12: Multilingual performance evaluation on 6 medical benchmarks with GPT-\n5 (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The experiment \ncompared the accuracy disparities between the original language and target languages, \nwith each condition repeated five times. Statistical significance is indicated by asterisks \n(*p<0.05, **p<0.01, ***p<0.001). \nGPT-5\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n***\n***\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n**\n***\n***\n***\n*\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n*\n***\n*\n**\n***\n***\n***\n***\n*\n"}, {"page": 126, "text": " \n \n96 \n \nSFig. 13: Multilingual performance evaluation on 6 medical benchmarks with o4-\nmini (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The experiment \ncompared the accuracy disparities between the original language and target languages, \nwith each condition repeated five times. Statistical significance is indicated by asterisks \n(*p<0.05, **p<0.01, ***p<0.001). \no4-mini\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n***\n***\n*\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n**\n**\n***\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n***\n***\n***\n***\n*\n*\n"}, {"page": 127, "text": " \n \n97 \n \nSFig. 14: Multilingual performance evaluation on 6 medical benchmarks with \nDeepSeek-V3 (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \n \nDeepSeek-V3\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n*\n**\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n*\n*\n*\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n**\n**\n***\n*\n***\n***\n***\n***\n"}, {"page": 128, "text": " \n \n98 \n \nSFig. 15: Multilingual performance evaluation on 6 medical benchmarks with \nDeepSeek-R1 (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nDeepSeek-R1\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n**\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n***\n*\n***\n***\n***\n"}, {"page": 129, "text": " \n \n99 \n \nSFig. 16: Multilingual performance evaluation on 6 medical benchmarks with \nDeepSeek-R1-Qwen3-8B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-\nPro). The experiment compared the accuracy disparities between the original language \nand target languages, with each condition repeated five times. Statistical significance is \nindicated by asterisks (*p<0.05, **p<0.01, ***p<0.001). \nDeepSeek-R1-Qwen3-8B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n*\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n*\n***\n***\n**\n***\n***\n***\n***\n***\n"}, {"page": 130, "text": " \n \n100 \n \nSFig. 17: Multilingual performance evaluation on 6 medical benchmarks with \nGemma-3-4B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nGemma-3-4B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n*\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n"}, {"page": 131, "text": " \n \n101 \n \nSFig. 18: Multilingual performance evaluation on 6 medical benchmarks with \nGemma-3-12B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nGemma-3-12B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n**\n***\n***\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n*\n**\n**\n*\n***\n***\n***\n***\n"}, {"page": 132, "text": " \n \n102 \n \nSFig. 19: Multilingual performance evaluation on 6 medical benchmarks with \nGemma-3-27B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nGemma-3-27B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n***\n*\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n**\n***\n***\n***\n***\n"}, {"page": 133, "text": " \n \n103 \n \nSFig. 20: Multilingual performance evaluation on 6 medical benchmarks with gpt-\noss-20B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \ngpt-oss-20B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n**\n**\n*\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n**\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n*\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n**\n**\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n***\n***\n***\n***\n"}, {"page": 134, "text": " \n \n104 \n \nSFig. 21: Multilingual performance evaluation on 6 medical benchmarks with gpt-\noss-120B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \ngpt-oss-120B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n**\n**\n***\n***\n***\n***\n*\n"}, {"page": 135, "text": " \n \n105 \n \nSFig. 22: Multilingual performance evaluation on 6 medical benchmarks with \nLLaMA-3.1-8B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nLLaMA-3.1-8B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n**\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n**\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n"}, {"page": 136, "text": " \n \n106 \n \nSFig. 23: Multilingual performance evaluation on 6 medical benchmarks with \nLLaMA-3.1-70B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nLLaMA-3.1-70B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n*\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n*\n***\n***\n**\n***\n***\n***\n***\n"}, {"page": 137, "text": " \n \n107 \n \nSFig. 24: Multilingual performance evaluation on 6 medical benchmarks with \nLLaMA-3.2-3B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nLLaMA-3.2-3B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n**\n**\n**\n**\n**\n**\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n***\n***\n***\n***\n***\n*\n"}, {"page": 138, "text": " \n \n108 \n \nSFig. 25: Multilingual performance evaluation on 6 medical benchmarks with \nLLaMA-3.3-70B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nLLaMA-3.3-70B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n**\n***\n***\n***\n***\n"}, {"page": 139, "text": " \n \n109 \n \nSFig. 25: Multilingual performance evaluation on 6 medical benchmarks with \nLLaMA-4-Scout (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nLLaMA-4-Scout\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n**\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n***\n***\n*\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n**\n**\n***\n***\n***\n***\n"}, {"page": 140, "text": " \n \n110 \n \nSFig. 27: Multilingual performance evaluation on 6 medical benchmarks with \nLLaMA-4-Maverick (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nLLaMA-4-Maverick\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n*\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n**\n***\n**\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n**\n***\n*\n***\n***\n***\n***\n***\n"}, {"page": 141, "text": " \n \n111 \n \nSFig. 28: Multilingual performance evaluation on 6 medical benchmarks with \nMistral-7B-v0.3 (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nMistral-7B-v0.3\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n**\n**\n**\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n*\n**\n**\n**\n**\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n*\n*\n**\n**\n**\n**\n**\n"}, {"page": 142, "text": " \n \n112 \n \nSFig. 29: Multilingual performance evaluation on 6 medical benchmarks with \nMistral-Small-3.1-24B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). \nThe experiment compared the accuracy disparities between the original language and \ntarget languages, with each condition repeated five times. Statistical significance is \nindicated by asterisks (*p<0.05, **p<0.01, ***p<0.001). \nMistral-Small-3.1-24B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n**\n***\n*\n*\n***\n***\n***\n***\n*\n"}, {"page": 143, "text": " \n \n113 \n \nSFig. 30: Multilingual performance evaluation on 6 medical benchmarks with Phi-\n4-mini (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The experiment \ncompared the accuracy disparities between the original language and target languages, \nwith each condition repeated five times. Statistical significance is indicated by asterisks \n(*p<0.05, **p<0.01, ***p<0.001). \nPhi-4-mini\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n"}, {"page": 144, "text": " \n \n114 \n \nSFig. 31: Multilingual performance evaluation on 6 medical benchmarks with Phi-\n4-mini-Reasoning (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nPhi-4-mini-Reasoning\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n*\n**\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n"}, {"page": 145, "text": " \n \n115 \n \nSFig. 32: Multilingual performance evaluation on 6 medical benchmarks with Phi-\n4 (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The experiment \ncompared the accuracy disparities between the original language and target languages, \nwith each condition repeated five times. Statistical significance is indicated by asterisks \n(*p<0.05, **p<0.01, ***p<0.001). \nPhi-4\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n*\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n***\n***\n***\n***\n***\n***\n**\n"}, {"page": 146, "text": " \n \n116 \n \nSFig. 33: Multilingual performance evaluation on 6 medical benchmarks with Phi-\n4-Reasoning (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nPhi-4-Reasoning\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n**\n***\n**\n***\n***\n***\n***\n*\n"}, {"page": 147, "text": " \n \n117 \n \nSFig. 34: Multilingual performance evaluation on 6 medical benchmarks with \nQwen2.5-3B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen2.5-3B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n*\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n***\n***\n**\n**\n***\n***\n***\n***\n**\n"}, {"page": 148, "text": " \n \n118 \n \nSFig. 35: Multilingual performance evaluation on 6 medical benchmarks with \nQwen2.5-7B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen2.5-7B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n**\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n*\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n***\n***\n**\n***\n***\n***\n***\n"}, {"page": 149, "text": " \n \n119 \n \nSFig. 36: Multilingual performance evaluation on 6 medical benchmarks with \nQwen2.5-14B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen2.5-14B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n*\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n*\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n**\n**\n***\n***\n***\n***\n"}, {"page": 150, "text": " \n \n120 \n \nSFig. 37: Multilingual performance evaluation on 6 medical benchmarks with \nQwen2.5-72B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen2.5-72B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n**\n***\n***\n**\n***\n***\n***\n***\n"}, {"page": 151, "text": " \n \n121 \n \nSFig. 38: Multilingual performance evaluation on 6 medical benchmarks with \nQwQ-32B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwQ-32B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n***\n*\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n**\n***\n***\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n***\n***\n***\n***\n***\n**\n"}, {"page": 152, "text": " \n \n122 \n \nSFig. 39: Multilingual performance evaluation on 6 medical benchmarks with \nQwen3-1.7B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen3-1.7B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n*\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n**\n***\n***\n**\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n**\n***\n**\n***\n***\n***\n***\n***\n***\n"}, {"page": 153, "text": " \n \n123 \n \nSFig. 40: Multilingual performance evaluation on 6 medical benchmarks with \nQwen3-4B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen3-4B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n*\n**\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n**\n*\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n***\n***\n**\n***\n***\n***\n***\n"}, {"page": 154, "text": " \n \n124 \n \nSFig. 41: Multilingual performance evaluation on 6 medical benchmarks with \nQwen3-4B-thinking (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). \nThe experiment compared the accuracy disparities between the original language and \ntarget languages, with each condition repeated five times. Statistical significance is \nindicated by asterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen3-4B-thinking\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n***\n**\n***\n***\n***\n***\n***\n"}, {"page": 155, "text": " \n \n125 \n \nSFig. 42: Multilingual performance evaluation on 6 medical benchmarks with \nQwen3-8B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen3-8B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n*\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n**\n***\n***\n***\n***\n"}, {"page": 156, "text": " \n \n126 \n \nSFig. 43: Multilingual performance evaluation on 6 medical benchmarks with \nQwen3-8B-thinking (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). \nThe experiment compared the accuracy disparities between the original language and \ntarget languages, with each condition repeated five times. Statistical significance is \nindicated by asterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen3-8B-thinking\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n*\n*\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n**\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n*\n**\n***\n**\n**\n***\n***\n***\n***\n"}, {"page": 157, "text": " \n \n127 \n \nSFig. 44: Multilingual performance evaluation on 6 medical benchmarks with \nQwen3-14B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen3-14B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n*\n***\n***\n***\n**\n***\n***\n***\n***\n"}, {"page": 158, "text": " \n \n128 \n \nSFig. 45: Multilingual performance evaluation on 6 medical benchmarks with \nQwen3-14B-thinking (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). \nThe experiment compared the accuracy disparities between the original language and \ntarget languages, with each condition repeated five times. Statistical significance is \nindicated by asterisks (*p<0.05, **p<0.01, ***p<0.001). \nQwen3-14B-thinking\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n**\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n*\n"}, {"page": 159, "text": " \n \n129 \n \nSFig. 46: Multilingual performance evaluation on 6 medical benchmarks with \nBaichuan-M2-32B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nBaichuan-M2-32B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n**\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n*\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n*\n*\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n**\n**\n***\n***\n*\n***\n***\n***\n***\n"}, {"page": 160, "text": " \n \n130 \n \nSFig. 47: Multilingual performance evaluation on 6 medical benchmarks with Bio-\nMedical-LLaMA-3-8B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). \nThe experiment compared the accuracy disparities between the original language and \ntarget languages, with each condition repeated five times. Statistical significance is \nindicated by asterisks (*p<0.05, **p<0.01, ***p<0.001). \nBio-Medical-LLaMA-3-8B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n**\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n"}, {"page": 161, "text": " \n \n131 \n \nSFig. 48: Multilingual performance evaluation on 6 medical benchmarks with \nMediPhi (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nMediPhi\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n**\n***\n***\n***\n***\n***\n"}, {"page": 162, "text": " \n \n132 \n \nSFig. 49: Multilingual performance evaluation on 6 medical benchmarks with \nMedGemma-4B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nMedGemma-4B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n**\n***\n*\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n**\n**\n***\n***\n***\n***\n"}, {"page": 163, "text": " \n \n133 \n \nSFig. 50: Multilingual performance evaluation on 6 medical benchmarks with \nMedGemma-27B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nMedGemma-27B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n**\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n**\n**\n***\n***\n***\n"}, {"page": 164, "text": " \n \n134 \n \nSFig. 51: Multilingual performance evaluation on 6 medical benchmarks with \nMedReason-8B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nMedReason-8B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n***\n**\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n**\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n**\n***\n**\n***\n***\n***\n***\n***\n"}, {"page": 165, "text": " \n \n135 \n \nSFig. 52: Multilingual performance evaluation on 6 medical benchmarks with \nHuatuoGPT-o1-7B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nHuatuoGPT-o1-7B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n**\n***\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n***\n***\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n**\n***\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n**\n***\n***\n***\n***\n***\n"}, {"page": 166, "text": " \n \n136 \n \nSFig. 53: Multilingual performance evaluation on 6 medical benchmarks with \nHuatuoGPT-o1-8B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nHuatuoGPT-o1-8B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n***\n***\n*\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n*\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n**\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n**\n***\n***\n***\n*\n***\n***\n***\n***\n"}, {"page": 167, "text": " \n \n137 \n \nSFig. 54: Multilingual performance evaluation on 6 medical benchmarks with \nHuatuoGPT-o1-70B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). \nThe experiment compared the accuracy disparities between the original language and \ntarget languages, with each condition repeated five times. Statistical significance is \nindicated by asterisks (*p<0.05, **p<0.01, ***p<0.001). \nHuatuoGPT-o1-70B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n*\n*\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n**\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n*\n***\n***\n***\n***\n***\n***\n"}, {"page": 168, "text": " \n \n138 \n \nSFig. 55: Multilingual performance evaluation on 6 medical benchmarks with \nHuatuoGPT-o1-72B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). \nThe experiment compared the accuracy disparities between the original language and \ntarget languages, with each condition repeated five times. Statistical significance is \nindicated by asterisks (*p<0.05, **p<0.01, ***p<0.001). \nHuatuoGPT-o1-72B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n*\n***\n**\n*\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n*\n***\n*\n***\n***\n***\n***\n***\n*\n"}, {"page": 169, "text": " \n \n139 \n \nSFig. 56: Multilingual performance evaluation on 6 medical benchmarks with \nOpenBioLLM-8B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nOpenBioLLM-8B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n***\n***\n**\n**\n***\n***\n**\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n*\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n**\n**\n***\n**\n***\n***\n**\n*\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n**\n*\n*\n**\n**\n*\n*\n**\n**\n**\n**\n"}, {"page": 170, "text": " \n \n140 \n \nSFig. 57: Multilingual performance evaluation on 6 medical benchmarks with \nOpenBioLLM-70B (BioNLI, MedNLI, HeadQA, MedExpQA, MedQA, MMLU-Pro). The \nexperiment compared the accuracy disparities between the original language and target \nlanguages, with each condition repeated five times. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001). \nOpenBioLLM-70B\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\n***\n***\n***\nChinese\nEnglish\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n**\n**\n***\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n***\n***\n***\n***\n***\n**\n***\n***\n***\n***\nChinese\nFrench\nGerman\nJapanese\nKorean\nPortuguese\nSpanish\nSwahili\nWolof\nYoruba\nZulu\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n***\n**\n***\n***\n***\n***\n***\n***\n***\n***\n"}, {"page": 171, "text": " \n \n141 \nS3. GlobMed-LLMs \nS3.1. GlobMed-Qwen3-1.7B/8B through Direct Supervised Fine-Tuning \nIn addition to GlobMed-MedGemma-4B and GlobMed-Qwen3-4B discussed in the \nmain text, both GlobMed-Qwen3-1.7B and GlobMed-Qwen3-8B demonstrated \nsignificant improvements on multilingual medical benchmarks as well (SFig. 58). \nGlobMed-Qwen3-1.7B improved overall accuracy from 39.14% to 54.44%, while \nGlobMed-Qwen3-8B increased from 47.85% to 66.15%, surpassing their baseline \nmodels. Improvements were consistent across all benchmarks and languages, \nwith particularly strong gains in low-resource languages such as Swahili, Wolof, \nYoruba, and Zulu. \n"}, {"page": 172, "text": " \n \n142 \n \nSFig. 58: Performance comparison of GlobMed-LLMs versus baseline LLMs. a, \nGlobMed-Qwen3-1.7B overall performance: Average accuracy across all benchmarks \nand languages improved from 39.14% to 54.44% compared with Qwen3-1.7B. b, Task-\nwise performance: GlobMed-Qwen3-1.7B outperformed Qwen3-1.7B across all medical \nbenchmarks. c, Language-wise performance: GlobMed- Qwen3-1.7B achieved higher \nGlobMed-Qwen-8B\na.\nc.\nd.\nf.\nGlobMed-Qwen-1.7B\nb.\ne.\n39.14\n54.44\nQwen3-1.7B\n35\n40\n45\n50\n55\n60\n15.30\nGlobMed-Qwen3-1.7B\nMedQA\nMMLU−Pro\nBioNLI\nMedExpQA\nMedNLI\nHeadQA\n0\n50\n100\nQwen3−1.7B\nGlobMed−Qwen3−1.7B\n18.30\n47.85\n66.15\nQwen3-8B\nGlobMed-Qwen3-8B\n45\n50\n55\n60\n65\n70\nMedQA\nMMLU−Pro\nBioNLI\nMedExpQA\nMedNLI\nHeadQA\n0\n50\n100\nQwen3−8B\nGlobMed−Qwen3−8B\n***\n35\n40\n45\n50\n55\n60\nKorean\n***\n40\n45\n50\n55\n60\n65\nPortuguese\nAccuracy\n***\n20\n30\n40\n50\nSwahili\nAccuracy\n***\n20\n30\n40\n50\nWolof\nAccuracy\n***\n25\n30\n35\n40\n45\n50\nYoruba\nAccuracy\n***\nChinese\n***\n25\n30\n35\n40\n45\nZulu\nAccuracy\n***\n40\n50\n60\nJapanese\nAccuracy\n***\nGerman\n***\nEnglish\n***\n40\n45\n50\n55\n60\n65\nSpanish\nAccuracy\n***\nFrench\n***\n55\n60\n65\n70\n75\n80\nGerman\n***\n65\n70\n75\n80\nEnglish\n***\n55\n60\n65\n70\n75\nSpanish\n***\n55\n60\n65\n70\n75\nFrench\nAccuracy\n***\n60\n65\n70\n75\n80\nJapanese\nAccuracy\n***\n55\n60\n65\n70\n75\nKorean\nAccuracy\n***\n55\n60\n65\n70\n75\n80\nPortuguese\nAccuracy\n***\n20\n30\n40\n50\n60\nSwahili\nAccuracy\n***\n10\n20\n30\n40\n50\nWolof\nAccuracy\n***\n20\n30\n40\n50\nYoruba\nAccuracy\n***\n60\n65\n70\n75\n80\nChinese\nAccuracy\n***\n10\n20\n30\n40\n50\nZulu\nAccuracy\n45\n50\n55\n60\n65\nAccuracy\n40\n45\n50\n55\n60\n65\nAccuracy\n55\n60\n65\n70\nAccuracy\n40\n45\n50\n55\n60\n65\nAccuracy\nAccuracy\nAccuracy\nAccuracy\nAccuracy\n"}, {"page": 173, "text": " \n \n143 \naverage accuracy across all 12 languages compared with Qwen3-1.7B, with particularly \nnotable improvements in low-resource languages. d, GlobMed-Qwen3-8B overall \nperformance: Average accuracy across all benchmarks and languages improved from \n47.85% to 66.15% compared with Qwen3-8B. e, Task-wise performance: GlobMed-\nQwen3-8B consistently outperformed Qwen3-8B across all medical benchmarks. f, \nLanguage-wise performance: GlobMed-Qwen3-8B achieved higher average accuracy \nacross all 12 languages compared with Qwen3-8B, with particularly notable \nimprovements in low-resource languages. Statistical significance is indicated by \nasterisks (*p<0.05, **p<0.01, ***p<0.001).\n"}, {"page": 174, "text": " \n \n144 \nS3.2. Performance Comparison between GlobMed-LLMs and Baseline LLMs \nLLMs \nChinese \nEnglish \nFrench \nGerman \nJapanese \nKorean Portuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nMedGemma-4B \n30.72±0.30 36.53±0.19 39.85±0.30 35.28±0.54 32.59±0.22 36.40±0.33 \n35.67±0.20 \n36.33±0.25 32.33±0.23 16.32±0.80 28.52±0.55 28.07±0.19 \n32.38±5.96 \nGlobMed-MedGemma-4B 75.22±0.47 83.99±0.27 72.43±0.28 73.10±0.51 75.43±0.19 74.61±0.30 \n77.24±0.39 \n77.39±0.53 74.45±0.44 67.19±0.56 69.34±0.32 73.48±0.42 \n74.49±4.08 \nQwen3-1.7B \n51.29±0.36 54.77±0.46 45.41±0.53 55.56±0.44 49.77±0.53 41.56±0.44 \n45.10±0.52 \n47.07±0.40 49.64±0.65 37.22±0.44 47.39±0.45 42.44±1.41 \n47.27±5.22 \nGlobMed-Qwen3-1.7B \n77.11±0.36 85.50±0.14 72.13±0.38 73.18±0.29 75.91±0.40 75.65±0.26 \n78.80±0.38 \n78.44±0.41 67.41±0.50 67.51±0.66 65.68±0.59 66.15±0.23 \n73.62±5.90 \nQwen3-4B \n52.17±0.26 59.04±0.38 40.76±0.50 38.76±0.44 49.69±0.43 43.75±0.51 \n50.20±0.32 \n49.51±0.30 11.72±0.31 \n3.21±0.19 \n24.30±0.88 14.56±0.41 \n36.47±17.74 \nGlobMed-Qwen3-4B \n80.92±0.10 88.62±0.12 74.31±0.19 77.26±0.22 80.08±0.29 79.07±0.36 \n81.76±0.37 \n81.80±0.17 74.26±0.15 69.63±0.26 70.84±0.79 70.27±0.50 \n77.40±5.54 \nQwen3-8B \n47.02±0.20 61.20±0.37 41.10±0.30 38.88±0.12 51.16±0.53 45.59±0.50 \n45.29±0.50 \n48.19±0.39 35.02±0.53 \n3.79±0.29 \n52.74±0.54 17.08±0.11 \n40.59±15.33 \nGlobMed-Qwen3-8B \n82.29±0.20 90.44±0.10 76.21±0.25 78.70±0.17 81.44±0.22 81.93±0.53 \n84.23±0.26 \n83.71±0.15 77.46±0.46 70.32±0.42 72.43±0.47 72.35±0.50 \n79.29±5.67 \nSTab. 123: Performance comparison across 12 languages on BioNLI. \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n30.79 \n36.36 \n39.55 \n34.38 \n32.88 \n36.31 \n35.89 \n36.63 \n32.36 \n14.92 \n28.11 \n28.20 \nGlobMed-MedGemma-4B \n75.91 \n83.64 \n72.40 \n73.12 \n75.69 \n75.01 \n76.56 \n76.72 \n74.65 \n67.21 \n68.94 \n73.75 \nQwen3-1.7B \n51.24 \n54.67 \n46.07 \n55.30 \n50.02 \n41.46 \n45.24 \n46.47 \n50.40 \n37.51 \n47.10 \n42.74 \nGlobMed-Qwen3-1.7B \n76.85 \n85.37 \n72.00 \n73.66 \n76.16 \n75.84 \n78.61 \n78.34 \n67.28 \n67.53 \n65.10 \n66.07 \nQwen3-4B \n51.91 \n58.92 \n40.47 \n38.20 \n49.55 \n43.06 \n50.29 \n49.24 \n11.35 \n3.17 \n23.15 \n15.19 \nGlobMed-Qwen3-4B \n80.81 \n88.83 \n74.40 \n77.24 \n79.91 \n78.74 \n82.34 \n81.51 \n74.22 \n69.51 \n71.30 \n70.04 \nQwen3-8B \n47.17 \n61.26 \n41.21 \n39.01 \n51.12 \n45.78 \n45.73 \n48.74 \n34.83 \n3.55 \n52.52 \n17.12 \nGlobMed-Qwen3-8B \n82.29 \n90.38 \n76.20 \n78.52 \n81.30 \n82.58 \n84.54 \n83.89 \n77.78 \n70.18 \n72.94 \n72.70 \nSTab. 124: Zero-Shot performance comparison across 12 languages on BioNLI (Run 1). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n30.65 \n36.31 \n39.84 \n35.78 \n32.40 \n36.90 \n35.75 \n36.18 \n32.20 \n16.70 \n28.11 \n27.75 \nGlobMed-MedGemma-4B \n75.15 \n84.09 \n72.31 \n72.85 \n75.55 \n74.49 \n77.55 \n78.11 \n74.56 \n66.76 \n69.37 \n74.02 \nQwen3-1.7B \n51.51 \n54.52 \n44.76 \n55.01 \n50.22 \n42.00 \n44.25 \n47.30 \n49.84 \n36.81 \n48.13 \n40.25 \nGlobMed-Qwen3-1.7B \n77.64 \n85.55 \n72.54 \n73.10 \n75.75 \n75.75 \n78.92 \n77.98 \n67.55 \n67.26 \n66.00 \n66.36 \nQwen3-4B \n52.13 \n59.17 \n40.18 \n39.08 \n49.44 \n44.09 \n50.20 \n49.15 \n11.60 \n3.35 \n23.64 \n14.72 \nGlobMed-Qwen3-4B \n80.90 \n88.54 \n74.58 \n77.24 \n79.98 \n78.81 \n81.89 \n81.82 \n74.52 \n69.75 \n69.80 \n70.34 \nQwen3-8B \n46.74 \n60.65 \n41.55 \n38.99 \n51.66 \n44.97 \n44.94 \n48.22 \n35.21 \n3.91 \n53.21 \n17.03 \nGlobMed-Qwen3-8B \n82.34 \n90.40 \n76.25 \n78.92 \n81.51 \n81.10 \n83.82 \n83.51 \n76.74 \n70.02 \n71.89 \n72.00 \nSTab. 125: Zero-Shot performance comparison across 12 languages on BioNLI (Run 2). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n31.03 \n36.76 \n39.62 \n35.39 \n32.56 \n36.36 \n35.78 \n36.45 \n32.20 \n16.81 \n28.52 \n28.22 \nGlobMed-MedGemma-4B \n75.44 \n84.34 \n72.36 \n73.91 \n75.39 \n74.18 \n77.33 \n77.71 \n73.69 \n66.81 \n69.10 \n73.35 \nQwen3-1.7B \n51.78 \n54.18 \n44.99 \n56.02 \n49.71 \n40.94 \n45.55 \n47.42 \n49.44 \n37.35 \n47.03 \n42.45 \nGlobMed-Qwen3-1.7B \n77.30 \n85.57 \n72.02 \n73.17 \n75.53 \n75.87 \n78.76 \n78.18 \n67.89 \n68.07 \n64.99 \n66.27 \nQwen3-4B \n52.45 \n59.42 \n41.51 \n39.28 \n49.19 \n44.36 \n50.67 \n49.60 \n12.16 \n3.21 \n24.90 \n14.49 \nGlobMed-Qwen3-4B \n80.88 \n88.56 \n74.29 \n77.06 \n80.58 \n78.94 \n81.44 \n81.80 \n74.20 \n69.62 \n71.89 \n70.76 \nQwen3-8B \n47.01 \n61.21 \n41.03 \n38.83 \n50.31 \n46.25 \n45.82 \n47.69 \n35.71 \n4.13 \n52.38 \n17.08 \nGlobMed-Qwen3-8B \n81.98 \n90.56 \n76.38 \n78.61 \n81.46 \n82.00 \n84.31 \n83.82 \n77.91 \n69.89 \n71.98 \n72.72 \nSTab. 126: Zero-Shot performance comparison across 12 languages on BioNLI (Run 3). \n \n \n"}, {"page": 175, "text": " \n \n145 \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n30.25 \n36.56 \n39.96 \n35.26 \n32.38 \n36.43 \n35.42 \n36.40 \n32.72 \n16.38 \n29.44 \n28.16 \nGlobMed-MedGemma-4B \n74.70 \n83.82 \n72.92 \n72.54 \n75.21 \n74.65 \n77.30 \n77.21 \n74.85 \n67.03 \n69.64 \n72.94 \nQwen3-1.7B \n50.90 \n55.19 \n45.69 \n55.51 \n48.88 \n41.46 \n45.44 \n46.88 \n48.65 \n37.73 \n47.17 \n42.58 \nGlobMed-Qwen3-1.7B \n77.03 \n85.33 \n71.62 \n73.08 \n75.62 \n75.53 \n79.37 \n78.67 \n67.71 \n68.16 \n66.22 \n66.25 \nQwen3-4B \n52.43 \n58.43 \n40.94 \n38.79 \n50.00 \n43.51 \n49.89 \n49.75 \n11.91 \n2.92 \n24.58 \n14.13 \nGlobMed-Qwen3-4B \n81.08 \n88.61 \n74.09 \n77.62 \n79.87 \n79.28 \n81.44 \n81.98 \n74.25 \n69.28 \n70.67 \n70.67 \nQwen3-8B \n46.94 \n61.21 \n40.74 \n38.83 \n51.53 \n45.73 \n44.65 \n48.34 \n34.27 \n3.42 \n53.42 \n16.94 \nGlobMed-Qwen3-8B \n82.29 \n90.34 \n76.43 \n78.83 \n81.75 \n81.93 \n84.18 \n83.66 \n77.35 \n70.72 \n72.76 \n72.67 \nSTab. 127: Zero-Shot performance comparison across 12 languages on BioNLI (Run 4). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n30.88 \n36.65 \n40.29 \n35.60 \n32.74 \n35.98 \n35.51 \n35.98 \n32.18 \n16.81 \n28.40 \n28.04 \nGlobMed-MedGemma-4B \n74.92 \n84.04 \n72.18 \n73.10 \n75.30 \n74.70 \n77.44 \n77.21 \n74.49 \n68.13 \n69.64 \n73.35 \nQwen3-1.7B \n51.03 \n55.28 \n45.55 \n55.98 \n50.02 \n41.96 \n45.03 \n47.30 \n49.87 \n36.72 \n47.51 \n44.18 \nGlobMed-Qwen3-1.7B \n76.74 \n85.66 \n72.49 \n72.90 \n76.49 \n75.24 \n78.36 \n79.01 \n66.61 \n66.54 \n66.11 \n65.78 \nQwen3-4B \n51.93 \n59.24 \n40.70 \n38.47 \n50.25 \n43.71 \n49.93 \n49.80 \n11.60 \n3.42 \n25.24 \n14.29 \nGlobMed-Qwen3-4B \n80.94 \n88.58 \n74.18 \n77.12 \n80.04 \n79.60 \n81.71 \n81.87 \n74.11 \n69.98 \n70.52 \n69.53 \nQwen3-8B \n47.24 \n61.69 \n40.97 \n38.72 \n51.17 \n45.21 \n45.33 \n47.98 \n35.10 \n3.93 \n52.18 \n17.24 \nGlobMed-Qwen3-8B \n82.54 \n90.54 \n75.80 \n78.61 \n81.17 \n82.02 \n84.31 \n83.69 \n77.53 \n70.81 \n72.56 \n71.64 \nSTab. 128: Zero-Shot performance comparison across 12 languages on BioNLI (Run 5). \n \n \nLLMs \nChinese \nEnglish \nFrench \nGerman \nJapanese \nKorean Portuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nMedGemma-4B \n58.70±0.53 72.20±0.68 60.00±0.84 61.92±0.79 59.16±0.46 58.16±0.72 \n59.66±0.63 \n63.66±0.78 \n44.26±1.44 15.23±0.58 31.80±0.95 31.39±1.30 \n51.35±16.33 \nGlobMed-MedGemma-4B 86.89±0.44 88.55±0.37 87.69±0.38 87.06±0.84 86.46±0.53 85.17±0.55 \n86.96±0.42 \n87.78±0.50 \n82.96±0.69 57.95±0.43 64.42±0.86 70.40±0.65 \n81.02±10.20 \nQwen3-1.7B \n68.44±0.96 75.29±0.66 66.58±0.43 62.15±0.82 54.20±0.75 49.33±0.60 \n65.14±0.63 \n63.79±0.91 \n27.38±1.61 23.59±0.96 30.61±1.47 32.43±1.20 \n51.58±17.74 \nGlobMed-Qwen3-1.7B \n87.31±0.44 88.60±0.45 86.80±0.27 86.99±0.50 86.08±0.37 84.42±0.66 \n86.38±0.40 \n86.48±0.34 \n69.98±0.98 60.58±0.48 61.67±0.62 56.95±0.56 \n78.52±11.94 \nQwen3-4B \n76.78±0.51 82.33±0.69 68.57±0.78 57.73±1.32 72.90±1.18 71.87±0.50 \n75.40±0.68 \n73.37±0.71 \n17.78±1.22 18.53±1.25 11.01±0.71 14.99±0.92 \n53.44±27.62 \nGlobMed-Qwen3-4B \n89.13±0.27 89.99±0.23 88.86±0.28 89.14±0.21 88.68±0.33 86.83±0.17 \n88.57±0.41 \n89.17±0.47 \n76.90±0.65 64.22±1.29 65.04±1.12 64.62±1.07 \n81.76±10.54 \nQwen3-8B \n75.98±1.16 77.49±0.50 72.01±0.49 69.49±0.81 77.15±0.63 72.55±0.31 \n70.97±0.36 \n70.90±1.01 \n27.30±0.78 13.06±1.01 24.80±0.88 20.24±0.48 \n55.99±25.03 \nGlobMed-Qwen3-8B \n89.22±0.37 90.32±0.17 89.68±0.17 89.64±0.33 88.70±0.19 87.88±0.37 \n88.89±0.32 \n88.74±0.42 \n78.93±0.37 63.73±0.65 69.74±0.65 68.33±0.67 \n82.81±9.59 \nSTab. 129: Performance comparison across 12 languages on MedNLI. \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n59.35 \n72.05 \n59.00 \n62.31 \n59.07 \n57.73 \n59.77 \n64.29 \n45.73 \n15.60 \n30.28 \n30.77 \nGlobMed-MedGemma-4B \n86.45 \n88.07 \n88.14 \n86.45 \n87.01 \n84.90 \n86.31 \n88.00 \n82.85 \n57.87 \n63.87 \n69.65 \nQwen3-1.7B \n68.03 \n75.37 \n66.69 \n61.54 \n54.20 \n49.26 \n64.22 \n63.30 \n27.45 \n22.23 \n31.90 \n33.17 \nGlobMed-Qwen3-1.7B \n86.66 \n89.13 \n86.59 \n86.31 \n86.03 \n85.25 \n86.17 \n86.24 \n68.88 \n60.13 \n60.90 \n56.60 \nQwen3-4B \n76.71 \n82.99 \n67.68 \n58.65 \n72.34 \n71.63 \n75.44 \n72.34 \n18.35 \n19.97 \n10.80 \n14.68 \nGlobMed-Qwen3-4B \n89.48 \n90.33 \n88.92 \n88.99 \n88.85 \n86.59 \n87.93 \n89.13 \n76.08 \n63.59 \n66.48 \n66.48 \nQwen3-8B \n76.64 \n77.42 \n72.12 \n68.45 \n77.63 \n72.90 \n71.21 \n70.71 \n27.52 \n14.11 \n24.35 \n20.89 \nGlobMed-Qwen3-8B \n89.56 \n90.19 \n89.84 \n89.34 \n88.43 \n87.65 \n88.57 \n88.57 \n78.69 \n63.59 \n70.85 \n67.61 \nSTab. 130: Zero-Shot performance comparison across 12 languages on MedNLI (Run 1). \n \n \n \n"}, {"page": 176, "text": " \n \n146 \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n59.00 \n72.27 \n59.56 \n62.88 \n59.14 \n58.50 \n58.86 \n62.67 \n42.70 \n15.74 \n31.90 \n31.55 \nGlobMed-MedGemma-4B \n87.01 \n88.85 \n87.37 \n87.23 \n87.01 \n84.47 \n86.87 \n88.36 \n82.99 \n57.23 \n64.64 \n70.85 \nQwen3-1.7B \n67.47 \n76.29 \n66.06 \n63.37 \n53.00 \n49.82 \n65.21 \n63.44 \n27.38 \n22.94 \n30.70 \n32.53 \nGlobMed-Qwen3-1.7B \n87.16 \n88.29 \n87.23 \n87.65 \n86.66 \n84.47 \n85.96 \n86.80 \n70.64 \n60.41 \n61.82 \n56.32 \nQwen3-4B \n77.21 \n82.07 \n69.58 \n55.96 \n73.39 \n71.35 \n74.95 \n73.82 \n17.64 \n16.73 \n10.94 \n16.30 \nGlobMed-Qwen3-4B \n88.99 \n89.84 \n88.92 \n88.99 \n89.13 \n86.87 \n88.57 \n88.92 \n77.21 \n64.86 \n64.36 \n64.50 \nQwen3-8B \n76.57 \n76.85 \n72.27 \n70.29 \n77.56 \n72.69 \n71.00 \n71.56 \n28.51 \n13.41 \n23.57 \n19.83 \nGlobMed-Qwen3-8B \n88.71 \n90.33 \n89.77 \n89.91 \n88.85 \n88.14 \n88.71 \n89.20 \n78.55 \n63.02 \n69.65 \n67.75 \nSTab. 131: Zero-Shot performance comparison across 12 languages on MedNLI (Run 2). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n58.01 \n71.14 \n60.97 \n62.10 \n58.93 \n59.21 \n59.21 \n64.29 \n45.73 \n14.26 \n32.89 \n30.35 \nGlobMed-MedGemma-4B \n86.80 \n88.92 \n87.37 \n88.43 \n86.24 \n85.32 \n86.94 \n87.65 \n82.50 \n58.15 \n63.37 \n71.21 \nQwen3-1.7B \n68.17 \n75.09 \n66.97 \n61.47 \n54.41 \n48.34 \n65.28 \n65.42 \n26.61 \n24.28 \n28.86 \n30.42 \nGlobMed-Qwen3-1.7B \n87.86 \n88.99 \n86.59 \n87.16 \n86.17 \n83.91 \n86.87 \n86.38 \n71.14 \n60.20 \n61.61 \n57.80 \nQwen3-4B \n75.94 \n81.79 \n68.88 \n59.21 \n72.12 \n71.56 \n76.15 \n73.89 \n17.71 \n19.27 \n12.07 \n14.61 \nGlobMed-Qwen3-4B \n88.99 \n90.12 \n89.27 \n89.06 \n88.36 \n86.73 \n88.64 \n89.48 \n77.21 \n62.39 \n64.64 \n64.15 \nQwen3-8B \n77.13 \n78.19 \n71.42 \n68.81 \n77.56 \n72.12 \n70.64 \n69.23 \n26.46 \n12.21 \n25.34 \n20.61 \nGlobMed-Qwen3-8B \n89.27 \n90.19 \n89.41 \n89.56 \n88.64 \n87.79 \n88.92 \n89.13 \n79.25 \n63.73 \n69.16 \n69.23 \nSTab. 132: Zero-Shot performance comparison across 12 languages on MedNLI (Run 3). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n58.79 \n72.55 \n60.76 \n61.47 \n59.92 \n57.37 \n60.06 \n64.08 \n44.11 \n15.31 \n31.83 \n33.59 \nGlobMed-MedGemma-4B \n87.58 \n88.29 \n87.51 \n86.80 \n86.24 \n85.96 \n87.37 \n87.01 \n84.12 \n58.29 \n64.57 \n70.43 \nQwen3-1.7B \n70.01 \n74.45 \n66.20 \n61.75 \n55.05 \n49.47 \n65.00 \n63.44 \n29.92 \n24.42 \n32.18 \n33.52 \nGlobMed-Qwen3-1.7B \n87.37 \n88.50 \n86.87 \n87.09 \n85.89 \n83.63 \n86.73 \n86.10 \n69.09 \n61.26 \n61.40 \n56.95 \nQwen3-4B \n77.13 \n83.13 \n67.89 \n57.94 \n74.74 \n72.41 \n75.94 \n72.90 \n15.95 \n17.93 \n10.09 \n15.46 \nGlobMed-Qwen3-4B \n88.85 \n89.91 \n88.57 \n89.20 \n88.36 \n87.01 \n88.64 \n89.77 \n77.63 \n65.77 \n65.91 \n64.08 \nQwen3-8B \n74.38 \n77.28 \n72.62 \n69.87 \n76.22 \n72.69 \n70.57 \n71.77 \n27.03 \n11.79 \n25.83 \n19.83 \nGlobMed-Qwen3-8B \n88.99 \n90.26 \n89.77 \n90.05 \n88.92 \n88.36 \n88.85 \n88.57 \n79.39 \n64.78 \n69.51 \n68.38 \nSTab. 133: Zero-Shot performance comparison across 12 languages on MedNLI (Run 4). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n58.36 \n72.97 \n59.70 \n60.83 \n58.72 \n58.01 \n60.41 \n62.95 \n43.05 \n15.24 \n32.11 \n30.70 \nGlobMed-MedGemma-4B \n86.59 \n88.64 \n88.07 \n86.38 \n85.82 \n85.18 \n87.30 \n87.86 \n82.36 \n58.22 \n65.63 \n69.87 \nQwen3-1.7B \n68.53 \n75.23 \n66.97 \n62.60 \n54.34 \n49.75 \n65.98 \n63.37 \n25.55 \n24.06 \n29.43 \n32.53 \nGlobMed-Qwen3-1.7B \n87.51 \n88.07 \n86.73 \n86.73 \n85.67 \n84.83 \n86.17 \n86.87 \n70.15 \n60.90 \n62.60 \n57.09 \nQwen3-4B \n76.92 \n81.65 \n68.81 \n56.88 \n71.91 \n72.41 \n74.52 \n73.89 \n19.27 \n18.77 \n11.15 \n13.90 \nGlobMed-Qwen3-4B \n89.34 \n89.77 \n88.64 \n89.48 \n88.71 \n86.94 \n89.06 \n88.57 \n76.36 \n64.50 \n63.80 \n63.87 \nQwen3-8B \n75.16 \n77.70 \n71.63 \n70.01 \n76.78 \n72.34 \n71.42 \n71.21 \n26.96 \n13.76 \n24.91 \n20.04 \nGlobMed-Qwen3-8B \n89.56 \n90.61 \n89.63 \n89.34 \n88.64 \n87.44 \n89.41 \n88.21 \n78.76 \n63.51 \n69.51 \n68.67 \nSTab. 134: Zero-Shot performance comparison across 12 languages on MedNLI (Run 5). \n \n \n \n"}, {"page": 177, "text": " \n \n147 \nLLMs \nChinese \nEnglish \nFrench \nGerman \nJapanese \nKorean Portuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nMedGemma-4B \n55.00±0.66 69.39±0.50 63.51±0.68 61.79±0.91 55.02±0.42 50.91±1.44 \n62.94±0.47 \n62.57±0.61 \n45.48±0.32 20.66±0.57 21.98±0.68 30.95±0.75 \n50.02±16.23 \nGlobMed-MedGemma-4B 49.97±0.31 61.40±0.42 56.44±0.42 55.65±0.56 50.88±0.59 49.03±0.77 \n55.73±0.49 \n43.57±0.97 \n44.36±1.04 36.63±0.60 36.69±0.65 40.82±0.45 \n48.43±7.84 \nQwen3-1.7B \n57.75±0.84 65.09±0.52 55.49±0.62 52.98±0.72 45.46±0.65 42.18±0.44 \n52.99±0.51 \n52.82±0.50 \n23.75±0.59 22.52±0.38 26.30±0.85 23.17±0.88 \n43.38±14.92 \nGlobMed-Qwen3-1.7B \n58.76±0.29 63.54±0.66 57.04±0.31 56.68±0.51 50.95±0.96 47.69±0.28 \n56.63±0.31 \n49.56±0.34 \n34.93±0.58 36.37±0.31 34.45±0.58 34.96±0.95 \n48.46±10.33 \nQwen3-4B \n72.91±0.56 76.47±0.37 70.40±0.15 70.43±0.35 66.57±0.97 61.10±1.07 \n70.00±0.54 \n69.67±0.39 \n19.33±0.91 14.16±0.41 16.25±0.53 \n9.66±0.83 \n51.41±26.39 \nGlobMed-Qwen3-4B \n68.88±0.26 71.71±0.20 67.76±0.34 68.38±0.38 65.15±0.19 60.71±0.43 \n65.88±0.32 \n56.43±0.17 \n43.52±0.74 41.70±0.83 39.76±0.17 39.38±0.54 \n57.44±12.30 \nQwen3-8B \n76.95±0.56 80.75±0.42 75.54±0.11 76.37±0.49 72.29±0.25 67.65±0.70 \n75.36±0.67 \n75.22±0.37 \n31.28±0.81 10.53±0.50 22.73±0.61 11.42±0.82 \n56.34±27.26 \nGlobMed-Qwen3-8B \n74.65±0.26 77.29±0.34 73.67±0.46 73.90±0.37 71.27±0.25 69.19±0.40 \n74.02±0.21 \n64.18±0.39 \n49.60±0.86 45.79±0.22 43.21±0.65 44.53±0.60 \n63.44±13.05 \nSTab. 135: Performance comparison across 12 languages on HeadQA. \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n55.23 \n69.57 \n63.71 \n61.00 \n54.69 \n50.36 \n63.17 \n62.67 \n45.18 \n20.20 \n21.46 \n29.94 \nGlobMed-MedGemma-4B \n50.05 \n61.41 \n56.85 \n55.95 \n50.63 \n49.86 \n55.59 \n43.42 \n43.91 \n36.52 \n36.88 \n40.67 \nQwen3-1.7B \n57.39 \n65.15 \n54.73 \n52.21 \n44.72 \n42.06 \n53.11 \n52.98 \n23.62 \n22.45 \n26.38 \n22.09 \nGlobMed-Qwen3-1.7B \n58.88 \n63.26 \n56.63 \n56.76 \n49.91 \n47.84 \n56.31 \n49.46 \n35.48 \n36.74 \n35.08 \n35.17 \nQwen3-4B \n72.72 \n76.56 \n70.24 \n70.65 \n67.45 \n62.53 \n69.48 \n69.16 \n18.94 \n14.52 \n16.14 \n10.37 \nGlobMed-Qwen3-4B \n68.49 \n71.73 \n67.45 \n68.26 \n64.92 \n61.09 \n66.19 \n56.49 \n43.42 \n42.61 \n39.86 \n39.77 \nQwen3-8B \n76.92 \n80.39 \n75.52 \n77.19 \n72.27 \n67.40 \n75.11 \n74.80 \n32.42 \n11.00 \n22.99 \n11.18 \nGlobMed-Qwen3-8B \n74.62 \n77.01 \n73.04 \n73.44 \n71.28 \n69.12 \n74.39 \n64.52 \n50.86 \n46.08 \n42.52 \n44.86 \nSTab. 136: Zero-Shot performance comparison across 12 languages on HeadQA (Run 1). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n54.55 \n68.53 \n63.35 \n61.27 \n54.46 \n49.28 \n62.85 \n62.35 \n45.49 \n20.20 \n21.06 \n31.51 \nGlobMed-MedGemma-4B \n50.14 \n61.05 \n56.13 \n55.37 \n51.40 \n49.64 \n55.59 \n44.32 \n46.17 \n36.38 \n36.79 \n41.25 \nQwen3-1.7B \n57.57 \n64.74 \n55.91 \n53.74 \n46.39 \n42.92 \n53.34 \n53.61 \n22.77 \n23.13 \n26.96 \n23.99 \nGlobMed-Qwen3-1.7B \n58.39 \n62.62 \n56.99 \n56.76 \n51.53 \n47.20 \n57.12 \n49.32 \n34.94 \n36.34 \n33.63 \n34.72 \nQwen3-4B \n73.31 \n76.83 \n70.65 \n70.47 \n66.46 \n61.77 \n70.65 \n69.57 \n19.88 \n13.53 \n15.55 \n10.14 \nGlobMed-Qwen3-4B \n68.76 \n71.60 \n67.63 \n68.76 \n65.33 \n60.60 \n65.69 \n56.54 \n44.45 \n42.56 \n39.72 \n39.40 \nQwen3-8B \n77.41 \n81.02 \n75.70 \n76.38 \n72.05 \n68.39 \n75.02 \n74.84 \n31.83 \n10.37 \n23.62 \n12.04 \nGlobMed-Qwen3-8B \n74.53 \n77.14 \n73.90 \n74.21 \n71.69 \n69.34 \n73.90 \n64.56 \n49.55 \n45.94 \n42.83 \n43.91 \nSTab. 137: Zero-Shot performance comparison across 12 languages on HeadQA (Run 2). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n55.68 \n69.66 \n64.07 \n63.03 \n55.32 \n51.98 \n62.22 \n63.07 \n45.94 \n21.19 \n22.32 \n31.83 \nGlobMed-MedGemma-4B \n50.27 \n60.96 \n56.45 \n56.45 \n50.41 \n48.11 \n55.23 \n44.72 \n44.27 \n36.25 \n37.20 \n40.31 \nQwen3-1.7B \n56.76 \n64.70 \n55.00 \n53.02 \n45.81 \n42.06 \n53.43 \n52.66 \n24.12 \n22.09 \n26.96 \n22.81 \nGlobMed-Qwen3-1.7B \n59.06 \n64.43 \n57.48 \n57.17 \n52.16 \n47.75 \n56.67 \n49.23 \n34.45 \n36.56 \n34.58 \n36.34 \nQwen3-4B \n72.45 \n76.78 \n70.42 \n69.84 \n66.41 \n60.78 \n69.66 \n69.88 \n20.56 \n14.47 \n16.01 \n9.24 \nGlobMed-Qwen3-4B \n69.03 \n71.91 \n68.03 \n67.99 \n65.33 \n60.87 \n66.05 \n56.54 \n43.01 \n41.16 \n39.99 \n38.64 \nQwen3-8B \n77.50 \n80.34 \n75.38 \n75.92 \n72.05 \n68.39 \n74.57 \n75.47 \n30.88 \n10.28 \n22.72 \n12.08 \nGlobMed-Qwen3-8B \n74.44 \n77.05 \n73.44 \n74.26 \n71.01 \n68.53 \n73.99 \n64.25 \n48.65 \n45.67 \n43.10 \n44.14 \nSTab. 138: Zero-Shot performance comparison across 12 languages on HeadQA (Run 3). \n \n \n \n"}, {"page": 178, "text": " \n \n148 \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n55.46 \n69.79 \n64.02 \n61.18 \n55.37 \n52.80 \n63.48 \n61.63 \n45.18 \n20.33 \n22.45 \n30.88 \nGlobMed-MedGemma-4B \n49.95 \n61.99 \n56.85 \n55.00 \n50.36 \n49.14 \n55.68 \n43.10 \n43.60 \n36.29 \n35.57 \n41.34 \nQwen3-1.7B \n59.02 \n65.96 \n55.59 \n53.65 \n45.13 \n41.75 \n52.93 \n52.30 \n24.17 \n22.41 \n24.89 \n24.17 \nGlobMed-Qwen3-1.7B \n58.93 \n63.62 \n56.99 \n55.82 \n51.08 \n47.84 \n56.58 \n50.09 \n35.53 \n36.29 \n34.13 \n34.90 \nQwen3-4B \n72.41 \n76.01 \n70.38 \n70.74 \n65.10 \n60.69 \n69.70 \n69.52 \n18.21 \n13.98 \n16.86 \n10.14 \nGlobMed-Qwen3-4B \n68.98 \n71.42 \n68.21 \n68.80 \n65.15 \n60.01 \n65.42 \n56.13 \n42.65 \n41.39 \n39.63 \n39.99 \nQwen3-8B \n76.83 \n80.66 \n75.56 \n76.19 \n72.54 \n66.86 \n76.10 \n75.47 \n30.48 \n9.92 \n22.14 \n10.10 \nGlobMed-Qwen3-8B \n74.57 \n77.41 \n74.26 \n73.99 \n71.15 \n69.43 \n73.85 \n63.80 \n49.91 \n45.72 \n43.37 \n44.36 \nSTab. 139: Zero-Shot performance comparison across 12 languages on HeadQA (Run 4). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n54.10 \n69.39 \n62.40 \n62.49 \n55.28 \n50.14 \n62.98 \n63.12 \n45.63 \n21.37 \n22.59 \n30.57 \nGlobMed-MedGemma-4B \n49.46 \n61.59 \n55.91 \n55.50 \n51.62 \n48.38 \n56.54 \n42.29 \n43.87 \n37.69 \n37.02 \n40.53 \nQwen3-1.7B \n58.03 \n64.88 \n56.22 \n52.30 \n45.27 \n42.11 \n52.16 \n52.57 \n24.08 \n22.54 \n26.33 \n22.77 \nGlobMed-Qwen3-1.7B \n58.52 \n63.75 \n57.12 \n56.90 \n50.05 \n47.84 \n56.45 \n49.68 \n34.27 \n35.93 \n34.85 \n33.68 \nQwen3-4B \n73.67 \n76.15 \n70.33 \n70.47 \n67.45 \n59.74 \n70.51 \n70.20 \n19.07 \n14.29 \n16.68 \n8.39 \nGlobMed-Qwen3-4B \n69.16 \n71.87 \n67.49 \n68.08 \n65.01 \n61.00 \n66.05 \n56.45 \n44.09 \n40.80 \n39.59 \n39.09 \nQwen3-8B \n76.10 \n81.33 \n75.56 \n76.15 \n72.54 \n67.22 \n76.01 \n75.52 \n30.79 \n11.09 \n22.18 \n11.68 \nGlobMed-Qwen3-8B \n75.11 \n77.82 \n73.72 \n73.58 \n71.24 \n69.52 \n73.99 \n63.75 \n49.01 \n45.54 \n44.23 \n45.40 \nSTab. 140: Zero-Shot performance comparison across 12 languages on HeadQA (Run 5). \n \n \nLLMs \nChinese \nEnglish \nFrench \nGerman \nJapanese \nKorean Portuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nMedGemma-4B \n48.00±1.26 66.56±1.82 55.68±1.66 60.64±2.22 45.12±3.08 41.44±2.07 \n54.56±1.64 \n54.24±4.06 \n41.60±2.88 17.12±1.21 21.76±2.49 30.56±1.82 \n44.77±14.87 \nGlobMed-MedGemma-4B 67.52±2.57 71.68±2.37 66.24±3.60 64.32±0.91 65.12±0.91 63.36±2.68 \n64.64±2.15 \n61.28±2.69 \n56.64±2.74 49.92±2.37 49.28±1.66 52.16±1.43 \n61.01±7.36 \nQwen3-1.7B \n49.28±3.60 51.36±1.43 44.00±2.94 41.28±2.30 35.52±2.50 33.44±1.73 \n41.44±2.07 \n41.92±3.94 \n15.84±1.31 22.40±2.83 25.76±2.96 21.60±2.88 \n35.32±11.43 \nGlobMed-Qwen3-1.7B \n64.48±1.84 65.92±0.91 64.32±1.34 61.92±1.84 61.76±2.85 51.20±2.04 \n64.16±1.31 \n65.76±0.36 \n36.00±2.88 40.64±2.49 42.40±3.84 39.84±2.07 \n54.87±11.66 \nQwen3-4B \n61.92±1.93 66.88±1.21 56.16±4.21 61.28±2.50 53.12±1.66 47.36±2.29 \n56.00±3.44 \n56.96±2.15 \n17.28±2.01 15.52±2.01 16.16±3.81 10.08±1.56 \n43.23±21.00 \nGlobMed-Qwen3-4B \n69.12±1.45 72.80±2.33 69.60±0.57 69.76±2.29 66.24±2.22 64.00±1.88 \n69.60±0.80 \n69.12±1.66 \n50.88±2.57 48.48±0.72 44.16±2.79 48.80±3.96 \n61.88±10.33 \nQwen3-8B \n63.68±2.37 70.24±2.22 58.88±4.85 63.36±3.89 58.56±1.19 53.44±3.41 \n61.44±2.74 \n60.32±3.18 \n16.96±3.37 11.52±2.57 22.24±1.54 \n9.12±1.45 \n45.81±22.67 \nGlobMed-Qwen3-8B \n79.52±1.21 77.60±0.98 73.92±0.44 76.48±0.72 76.16±1.82 73.44±1.31 \n74.40±2.33 \n75.52±1.66 \n57.44±2.96 52.96±2.22 47.04±1.73 58.24±4.02 \n68.56±11.02 \nSTab. 141: Performance comparison across 12 languages on MedExpQA. \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n47.20 \n65.60 \n56.00 \n60.00 \n48.00 \n41.60 \n52.80 \n55.20 \n39.20 \n18.40 \n24.00 \n31.20 \nGlobMed-MedGemma-4B \n67.20 \n75.20 \n63.20 \n65.60 \n64.80 \n61.60 \n63.20 \n58.40 \n54.40 \n48.80 \n47.20 \n53.60 \nQwen3-1.7B \n52.00 \n52.80 \n41.60 \n43.20 \n36.00 \n33.60 \n43.20 \n43.20 \n13.60 \n23.20 \n28.00 \n21.60 \nGlobMed-Qwen3-1.7B \n64.80 \n64.80 \n64.80 \n60.00 \n62.40 \n53.60 \n63.20 \n65.60 \n40.00 \n44.80 \n41.60 \n39.20 \nQwen3-4B \n60.80 \n67.20 \n53.60 \n60.80 \n52.00 \n44.80 \n60.80 \n58.40 \n14.40 \n12.80 \n18.40 \n12.00 \nGlobMed-Qwen3-4B \n69.60 \n74.40 \n69.60 \n69.60 \n69.60 \n62.40 \n70.40 \n68.00 \n49.60 \n48.80 \n45.60 \n55.20 \nQwen3-8B \n61.60 \n71.20 \n60.80 \n56.80 \n56.80 \n56.80 \n61.60 \n59.20 \n16.80 \n8.00 \n20.00 \n9.60 \nGlobMed-Qwen3-8B \n80.80 \n77.60 \n73.60 \n76.80 \n74.40 \n73.60 \n76.00 \n76.80 \n57.60 \n56.00 \n44.00 \n54.40 \nSTab. 142: Zero-Shot performance comparison across 12 languages on MedExpQA (Run 1). \n \n \n \n"}, {"page": 179, "text": " \n \n149 \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n48.80 \n64.00 \n58.40 \n58.40 \n44.00 \n42.40 \n56.00 \n57.60 \n42.40 \n16.80 \n24.00 \n32.80 \nGlobMed-MedGemma-4B \n71.20 \n69.60 \n68.80 \n64.80 \n66.40 \n63.20 \n66.40 \n58.40 \n60.00 \n46.40 \n48.00 \n51.20 \nQwen3-1.7B \n51.20 \n49.60 \n42.40 \n43.20 \n39.20 \n31.20 \n44.00 \n35.20 \n16.80 \n18.40 \n26.40 \n17.60 \nGlobMed-Qwen3-1.7B \n62.40 \n65.60 \n63.20 \n60.00 \n56.80 \n51.20 \n62.40 \n65.60 \n36.00 \n40.80 \n38.40 \n40.00 \nQwen3-4B \n64.00 \n67.20 \n56.80 \n60.80 \n55.20 \n50.40 \n53.60 \n56.00 \n19.20 \n15.20 \n21.60 \n8.00 \nGlobMed-Qwen3-4B \n71.20 \n73.60 \n70.40 \n72.80 \n64.80 \n63.20 \n70.40 \n70.40 \n51.20 \n48.80 \n44.00 \n46.40 \nQwen3-8B \n61.60 \n70.40 \n51.20 \n64.00 \n59.20 \n52.80 \n57.60 \n57.60 \n20.80 \n10.40 \n21.60 \n8.00 \nGlobMed-Qwen3-8B \n78.40 \n77.60 \n73.60 \n76.80 \n77.60 \n74.40 \n76.00 \n75.20 \n53.60 \n52.00 \n47.20 \n62.40 \nSTab. 143: Zero-Shot performance comparison across 12 languages on MedExpQA (Run 2). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n46.40 \n67.20 \n54.40 \n59.20 \n48.80 \n40.80 \n56.00 \n56.00 \n38.40 \n17.60 \n20.00 \n28.00 \nGlobMed-MedGemma-4B \n68.00 \n71.20 \n61.60 \n64.00 \n65.60 \n62.40 \n65.60 \n64.00 \n54.40 \n50.40 \n49.60 \n53.60 \nQwen3-1.7B \n43.20 \n52.80 \n48.80 \n37.60 \n32.80 \n36.00 \n40.00 \n43.20 \n16.00 \n25.60 \n28.80 \n20.80 \nGlobMed-Qwen3-1.7B \n67.20 \n67.20 \n64.00 \n62.40 \n64.00 \n49.60 \n64.80 \n66.40 \n35.20 \n39.20 \n46.40 \n37.60 \nQwen3-4B \n60.80 \n68.00 \n52.80 \n63.20 \n54.40 \n45.60 \n56.00 \n58.40 \n18.40 \n16.00 \n14.40 \n9.60 \nGlobMed-Qwen3-4B \n68.80 \n68.80 \n68.80 \n67.20 \n67.20 \n66.40 \n68.80 \n71.20 \n48.80 \n47.20 \n42.40 \n48.80 \nQwen3-8B \n67.20 \n72.00 \n57.60 \n64.80 \n60.00 \n48.80 \n64.00 \n57.60 \n16.00 \n14.40 \n22.40 \n7.20 \nGlobMed-Qwen3-8B \n79.20 \n76.80 \n74.40 \n75.20 \n78.40 \n73.60 \n74.40 \n76.80 \n56.00 \n54.40 \n48.00 \n57.60 \nSTab. 144: Zero-Shot performance comparison across 12 languages on MedExpQA (Run 3). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n49.60 \n68.80 \n54.40 \n64.00 \n42.40 \n38.40 \n52.80 \n47.20 \n45.60 \n15.20 \n22.40 \n29.60 \nGlobMed-MedGemma-4B \n64.00 \n69.60 \n69.60 \n63.20 \n64.00 \n68.00 \n66.40 \n63.20 \n55.20 \n52.00 \n51.20 \n50.40 \nQwen3-1.7B \n51.20 \n51.20 \n42.40 \n40.80 \n36.00 \n32.80 \n39.20 \n45.60 \n16.80 \n24.00 \n21.60 \n22.40 \nGlobMed-Qwen3-1.7B \n63.20 \n66.40 \n66.40 \n64.00 \n62.40 \n48.80 \n65.60 \n65.60 \n36.80 \n40.00 \n46.40 \n43.20 \nQwen3-4B \n64.00 \n64.80 \n63.20 \n64.00 \n52.80 \n47.20 \n52.00 \n58.40 \n18.40 \n18.40 \n14.40 \n11.20 \nGlobMed-Qwen3-4B \n68.80 \n72.80 \n69.60 \n71.20 \n65.60 \n62.40 \n69.60 \n67.20 \n49.60 \n48.80 \n48.00 \n48.80 \nQwen3-8B \n63.20 \n66.40 \n60.80 \n64.00 \n58.40 \n56.80 \n64.00 \n62.40 \n19.20 \n11.20 \n23.20 \n10.40 \nGlobMed-Qwen3-8B \n80.80 \n79.20 \n74.40 \n76.80 \n76.00 \n74.40 \n70.40 \n76.00 \n58.40 \n50.40 \n48.00 \n62.40 \nSTab. 145: Zero-Shot performance comparison across 12 languages on MedExpQA (Run 4). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n48.00 \n67.20 \n55.20 \n61.60 \n42.40 \n44.00 \n55.20 \n55.20 \n42.40 \n17.60 \n18.40 \n31.20 \nGlobMed-MedGemma-4B \n67.20 \n72.80 \n68.00 \n64.00 \n64.80 \n61.60 \n61.60 \n62.40 \n59.20 \n52.00 \n50.40 \n52.00 \nQwen3-1.7B \n48.80 \n50.40 \n44.80 \n41.60 \n33.60 \n33.60 \n40.80 \n42.40 \n16.00 \n20.80 \n24.00 \n25.60 \nGlobMed-Qwen3-1.7B \n64.80 \n65.60 \n63.20 \n63.20 \n63.20 \n52.80 \n64.80 \n65.60 \n32.00 \n38.40 \n39.20 \n39.20 \nQwen3-4B \n60.00 \n67.20 \n54.40 \n57.60 \n51.20 \n48.80 \n57.60 \n53.60 \n16.00 \n15.20 \n12.00 \n9.60 \nGlobMed-Qwen3-4B \n67.20 \n74.40 \n69.60 \n68.00 \n64.00 \n65.60 \n68.80 \n68.80 \n55.20 \n48.80 \n40.80 \n44.80 \nQwen3-8B \n64.80 \n71.20 \n64.00 \n67.20 \n58.40 \n52.00 \n60.00 \n64.80 \n12.00 \n13.60 \n24.00 \n10.40 \nGlobMed-Qwen3-8B \n78.40 \n76.80 \n73.60 \n76.80 \n74.40 \n71.20 \n75.20 \n72.80 \n61.60 \n52.00 \n48.00 \n54.40 \nSTab. 146: Zero-Shot performance comparison across 12 languages on MedExpQA (Run 5). \n \n \n \n"}, {"page": 180, "text": " \n \n150 \nLLMs \nChinese \nEnglish \nFrench \nGerman \nJapanese \nKorean Portuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nMedGemma-4B \n47.67±0.91 62.88±0.84 52.94±0.97 54.60±0.45 47.89±1.13 43.57±0.43 \n53.01±0.64 \n53.56±1.20 \n42.81±0.68 16.47±0.66 19.37±1.29 28.80±1.16 \n43.63±14.12 \nGlobMed-MedGemma-4B 46.98±0.84 54.75±0.51 50.90±0.50 50.81±0.63 47.34±1.20 47.46±0.67 \n50.95±0.85 \n50.38±0.51 \n46.20±0.63 39.50±0.88 37.92±0.68 41.51±0.98 \n47.06±4.98 \nQwen3-1.7B \n44.10±0.69 49.14±0.88 40.02±0.82 38.93±0.78 35.88±0.52 34.28±1.01 \n37.82±0.66 \n39.43±0.93 \n26.61±0.54 26.27±1.13 25.64±0.52 25.25±0.86 \n35.28±7.63 \nGlobMed-Qwen3-1.7B \n49.00±0.39 52.51±0.56 48.22±0.82 47.78±0.81 44.19±0.70 40.71±0.41 \n47.60±0.33 \n47.75±0.66 \n35.46±0.65 37.86±0.59 35.02±0.64 33.10±0.81 \n43.27±6.34 \nQwen3-4B \n59.54±1.08 64.45±0.72 54.27±1.07 53.20±0.84 53.01±0.63 45.10±0.72 \n54.52±0.48 \n55.99±1.02 \n22.91±0.82 \n9.96±0.69 \n16.98±1.06 \n9.69±1.06 \n41.64±19.82 \nGlobMed-Qwen3-4B \n58.52±0.92 62.26±0.67 58.48±0.44 57.04±0.21 54.69±0.42 50.34±1.07 \n59.56±0.26 \n59.10±0.29 \n42.22±0.32 43.76±0.54 39.00±0.79 42.23±0.59 \n52.27±8.05 \nQwen3-8B \n67.41±0.62 71.72±0.41 52.30±0.57 62.66±0.67 60.60±0.72 56.42±0.63 \n61.60±1.07 \n61.48±1.03 \n21.02±1.23 \n8.83±0.39 \n21.98±0.84 10.43±0.84 \n46.37±22.72 \nGlobMed-Qwen3-8B \n63.94±0.41 66.77±0.47 63.69±0.54 63.88±0.41 61.90±0.39 58.79±0.66 \n65.23±0.52 \n63.98±0.52 \n47.15±0.52 47.02±0.77 43.36±1.00 45.40±0.95 \n57.59±8.71 \nSTab. 147: Performance comparison across 12 languages on MedQA. \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n46.27 \n62.53 \n52.47 \n54.60 \n48.47 \n43.68 \n52.79 \n54.67 \n41.79 \n17.44 \n20.35 \n27.97 \nGlobMed-MedGemma-4B \n46.03 \n54.20 \n50.35 \n50.04 \n46.90 \n46.90 \n50.59 \n51.06 \n46.11 \n38.96 \n37.78 \n40.06 \nQwen3-1.7B \n43.28 \n49.41 \n40.77 \n38.41 \n35.59 \n35.43 \n38.49 \n38.02 \n26.32 \n26.63 \n26.08 \n25.45 \nGlobMed-Qwen3-1.7B \n48.78 \n52.71 \n48.70 \n48.63 \n45.17 \n40.46 \n47.29 \n47.84 \n35.98 \n38.18 \n34.49 \n32.52 \nQwen3-4B \n59.54 \n64.73 \n54.75 \n52.24 \n52.24 \n44.07 \n54.12 \n56.95 \n22.86 \n10.45 \n16.03 \n8.64 \nGlobMed-Qwen3-4B \n58.52 \n63.32 \n58.29 \n57.34 \n54.52 \n49.10 \n59.94 \n59.47 \n41.79 \n43.68 \n38.57 \n42.26 \nQwen3-8B \n67.16 \n72.03 \n52.71 \n62.22 \n61.35 \n57.11 \n61.27 \n62.92 \n22.15 \n9.27 \n21.45 \n11.55 \nGlobMed-Qwen3-8B \n63.32 \n66.77 \n63.32 \n64.10 \n61.51 \n58.60 \n64.96 \n64.57 \n47.29 \n47.21 \n43.13 \n45.48 \nSTab. 148: Zero-Shot performance comparison across 12 languages on MedQA (Run 1). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n48.15 \n61.82 \n53.97 \n54.28 \n46.11 \n42.89 \n54.05 \n53.42 \n43.21 \n15.63 \n18.54 \n28.52 \nGlobMed-MedGemma-4B \n46.11 \n55.54 \n50.82 \n51.45 \n45.95 \n46.58 \n49.73 \n50.75 \n45.40 \n40.77 \n38.88 \n40.93 \nQwen3-1.7B \n43.52 \n47.68 \n40.77 \n38.49 \n36.29 \n34.88 \n38.57 \n38.96 \n26.71 \n25.61 \n25.77 \n25.45 \nGlobMed-Qwen3-1.7B \n49.57 \n52.55 \n47.45 \n48.23 \n43.75 \n40.77 \n47.60 \n46.74 \n35.66 \n38.02 \n35.82 \n33.39 \nQwen3-4B \n60.72 \n64.49 \n54.20 \n53.34 \n53.73 \n45.09 \n53.97 \n56.72 \n21.60 \n9.66 \n16.10 \n8.96 \nGlobMed-Qwen3-4B \n58.37 \n62.45 \n58.60 \n56.87 \n54.05 \n50.51 \n59.23 \n59.15 \n42.11 \n43.52 \n38.18 \n41.24 \nQwen3-8B \n68.42 \n71.96 \n52.95 \n63.79 \n60.72 \n55.70 \n60.57 \n60.80 \n20.90 \n8.25 \n22.78 \n10.60 \nGlobMed-Qwen3-8B \n63.86 \n67.16 \n64.49 \n63.71 \n61.82 \n57.89 \n65.20 \n63.71 \n47.92 \n46.27 \n43.44 \n44.70 \nSTab. 149: Zero-Shot performance comparison across 12 languages on MedQA (Run 2). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n48.55 \n63.16 \n51.53 \n54.36 \n49.02 \n43.52 \n52.32 \n53.26 \n43.60 \n16.58 \n17.52 \n30.40 \nGlobMed-MedGemma-4B \n47.53 \n54.83 \n51.45 \n50.43 \n46.98 \n48.00 \n51.61 \n49.96 \n46.03 \n38.73 \n37.78 \n42.11 \nQwen3-1.7B \n44.54 \n50.04 \n39.28 \n38.33 \n35.74 \n34.56 \n37.16 \n39.98 \n25.84 \n25.92 \n25.14 \n24.27 \nGlobMed-Qwen3-1.7B \n48.78 \n52.79 \n48.78 \n47.37 \n43.36 \n40.53 \n47.45 \n48.00 \n35.74 \n38.33 \n34.25 \n34.09 \nQwen3-4B \n60.25 \n65.44 \n55.62 \n54.52 \n52.55 \n45.95 \n55.15 \n56.40 \n23.33 \n10.92 \n17.52 \n9.74 \nGlobMed-Qwen3-4B \n58.21 \n61.90 \n57.82 \n56.87 \n54.83 \n51.92 \n59.54 \n58.68 \n42.66 \n44.07 \n38.73 \n42.42 \nQwen3-8B \n67.48 \n71.01 \n51.61 \n62.45 \n61.12 \n55.85 \n63.24 \n61.90 \n20.74 \n9.11 \n23.02 \n10.29 \nGlobMed-Qwen3-8B \n64.41 \n67.09 \n63.08 \n63.32 \n62.45 \n59.07 \n65.28 \n64.10 \n47.21 \n46.50 \n45.01 \n44.46 \nSTab. 150: Zero-Shot performance comparison across 12 languages on MedQA (Run 3). \n \n \n \n"}, {"page": 181, "text": " \n \n151 \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n48.08 \n62.77 \n53.10 \n54.36 \n48.31 \n43.68 \n52.87 \n54.67 \n42.73 \n16.50 \n19.95 \n29.54 \nGlobMed-MedGemma-4B \n47.45 \n54.44 \n51.37 \n51.45 \n49.18 \n47.92 \n51.85 \n50.27 \n46.35 \n38.96 \n38.18 \n42.26 \nQwen3-1.7B \n44.93 \n49.18 \n40.22 \n40.14 \n36.53 \n32.91 \n37.47 \n40.22 \n27.02 \n28.04 \n25.06 \n24.59 \nGlobMed-Qwen3-1.7B \n48.63 \n52.95 \n47.21 \n48.08 \n44.54 \n40.38 \n48.15 \n47.60 \n35.59 \n36.84 \n35.35 \n33.46 \nQwen3-4B \n57.89 \n63.55 \n54.05 \n52.95 \n53.02 \n45.56 \n54.60 \n55.38 \n23.80 \n9.51 \n18.54 \n11.39 \nGlobMed-Qwen3-4B \n57.50 \n62.06 \n58.76 \n57.19 \n54.99 \n49.65 \n59.62 \n58.99 \n42.18 \n44.46 \n39.28 \n42.81 \nQwen3-8B \n66.77 \n71.88 \n52.40 \n62.14 \n60.25 \n56.48 \n60.88 \n61.51 \n19.17 \n8.72 \n21.29 \n9.19 \nGlobMed-Qwen3-8B \n63.94 \n65.99 \n63.86 \n63.86 \n62.14 \n59.70 \n66.06 \n63.24 \n46.66 \n46.90 \n42.42 \n45.48 \nSTab. 151: Zero-Shot performance comparison across 12 languages on MedQA (Run 4). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n47.29 \n64.10 \n53.65 \n55.38 \n47.53 \n44.07 \n53.02 \n51.77 \n42.73 \n16.18 \n20.50 \n27.57 \nGlobMed-MedGemma-4B \n47.76 \n54.75 \n50.51 \n50.67 \n47.68 \n47.92 \n50.98 \n49.88 \n47.13 \n40.06 \n37.00 \n42.18 \nQwen3-1.7B \n44.23 \n49.41 \n39.04 \n39.28 \n35.27 \n33.62 \n37.39 \n39.98 \n27.18 \n25.14 \n26.16 \n26.47 \nGlobMed-Qwen3-1.7B \n49.25 \n51.53 \n48.94 \n46.58 \n44.15 \n41.40 \n47.53 \n48.55 \n34.33 \n37.94 \n35.19 \n32.05 \nQwen3-4B \n59.31 \n64.02 \n52.71 \n52.95 \n53.50 \n44.85 \n54.75 \n54.52 \n22.94 \n9.27 \n16.73 \n9.74 \nGlobMed-Qwen3-4B \n60.02 \n61.59 \n58.92 \n56.95 \n55.07 \n50.51 \n59.47 \n59.23 \n42.34 \n43.05 \n40.22 \n42.42 \nQwen3-8B \n67.24 \n71.72 \n51.85 \n62.69 \n59.54 \n56.95 \n62.06 \n60.25 \n22.15 \n8.80 \n21.37 \n10.53 \nGlobMed-Qwen3-8B \n64.18 \n66.85 \n63.71 \n64.41 \n61.59 \n58.68 \n64.65 \n64.26 \n46.66 \n48.23 \n42.81 \n46.90 \nSTab. 152: Zero-Shot performance comparison across 12 languages on MedQA (Run 5). \n \n \nLLMs \nChinese \nEnglish \nFrench \nGerman \nJapanese \nKorean Portuguese Spanish \nSwahili \nWolof \nYoruba \nZulu \nOverall \nMedGemma-4B \n33.48±1.04 46.31±1.95 42.25±3.12 37.22±1.72 30.91±1.75 29.63±2.23 \n36.04±1.49 \n36.68±1.29 \n23.64±2.18 13.05±2.44 12.41±2.40 16.26±2.99 \n29.82±11.02 \nGlobMed-MedGemma-4B 32.94±1.80 41.07±1.91 37.97±1.25 39.89±2.97 32.73±1.48 30.16±1.68 \n36.36±0.54 \n34.22±1.25 \n30.27±1.63 20.54±3.24 20.96±3.88 24.49±1.33 \n31.80±6.92 \nQwen3-1.7B \n29.95±0.54 38.40±2.49 28.77±2.22 23.32±2.92 20.54±1.04 21.07±2.79 \n24.28±1.76 \n25.78±2.09 \n14.76±2.44 11.44±2.35 11.98±0.30 13.58±1.40 \n21.99±8.11 \nGlobMed-Qwen3-1.7B \n34.65±1.16 39.25±0.90 36.79±1.90 34.97±1.39 26.74±2.07 23.21±2.52 \n36.79±1.16 \n32.83±1.80 \n19.36±1.79 18.18±2.81 16.04±2.60 16.26±2.02 \n27.92±8.78 \nQwen3-4B \n53.16±1.95 59.15±1.76 50.80±2.48 50.59±1.72 43.64±2.22 37.75±2.23 \n48.34±1.45 \n50.27±3.49 \n12.51±1.80 11.23±0.65 11.02±2.29 10.70±1.46 \n36.60±18.74 \nGlobMed-Qwen3-4B \n50.59±2.16 56.04±0.58 53.26±1.80 54.12±1.66 42.03±2.13 40.75±0.70 \n54.01±0.85 \n55.93±0.81 \n25.99±2.29 26.63±2.02 24.71±1.44 23.42±2.43 \n42.29±13.17 \nQwen3-8B \n58.50±2.16 66.95±1.16 54.65±0.79 54.76±1.71 52.30±2.12 45.67±2.52 \n56.47±1.84 \n57.86±3.01 \n19.68±2.86 11.66±1.38 13.05±1.88 12.09±3.28 \n41.97±20.58 \nGlobMed-Qwen3-8B \n56.90±2.09 59.89±1.26 56.90±1.29 55.40±1.11 53.69±1.23 50.80±0.85 \n55.19±1.16 \n55.29±0.81 \n25.99±1.23 24.60±2.17 24.81±1.95 22.78±0.97 \n45.19±14.93 \nSTab. 153: Performance comparison across 12 languages on MMLU-Pro. \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n33.16 \n48.13 \n37.43 \n37.43 \n31.55 \n28.34 \n35.29 \n37.43 \n21.93 \n14.44 \n10.16 \n13.90 \nGlobMed-MedGemma-4B \n32.62 \n41.18 \n38.50 \n40.64 \n34.22 \n31.02 \n36.36 \n35.29 \n31.02 \n16.58 \n19.25 \n23.53 \nQwen3-1.7B \n29.95 \n35.83 \n30.48 \n22.46 \n20.86 \n24.60 \n24.60 \n25.67 \n18.72 \n14.97 \n11.76 \n14.44 \nGlobMed-Qwen3-1.7B \n34.76 \n40.11 \n38.50 \n36.36 \n24.06 \n19.79 \n37.43 \n31.55 \n19.25 \n16.04 \n12.83 \n15.51 \nQwen3-4B \n54.55 \n59.36 \n54.01 \n49.73 \n42.78 \n37.97 \n49.20 \n51.34 \n11.23 \n11.76 \n13.37 \n9.63 \nGlobMed-Qwen3-4B \n50.27 \n55.08 \n52.41 \n56.15 \n44.92 \n41.18 \n52.94 \n57.22 \n27.27 \n23.53 \n24.06 \n19.25 \nQwen3-8B \n59.36 \n68.45 \n55.08 \n52.41 \n49.73 \n45.45 \n58.29 \n55.61 \n19.79 \n9.63 \n12.83 \n9.63 \nGlobMed-Qwen3-8B \n58.29 \n61.50 \n58.29 \n54.01 \n54.55 \n50.27 \n55.61 \n55.61 \n26.20 \n21.93 \n26.20 \n24.06 \nSTab. 154: Zero-Shot performance comparison across 12 languages on MMLU-Pro (Run 1). \n \n \n \n"}, {"page": 182, "text": " \n \n152 \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n34.76 \n45.45 \n42.78 \n37.97 \n31.02 \n31.55 \n34.76 \n35.29 \n22.46 \n11.23 \n15.51 \n14.44 \nGlobMed-MedGemma-4B \n35.83 \n43.85 \n39.04 \n38.50 \n31.02 \n30.48 \n35.83 \n35.29 \n32.62 \n21.93 \n22.99 \n24.06 \nQwen3-1.7B \n30.48 \n36.36 \n28.34 \n21.93 \n20.86 \n18.72 \n21.39 \n25.67 \n12.83 \n9.63 \n11.76 \n14.44 \nGlobMed-Qwen3-1.7B \n34.22 \n39.04 \n38.50 \n33.16 \n27.27 \n26.20 \n35.83 \n34.22 \n19.25 \n21.93 \n17.11 \n15.51 \nQwen3-4B \n55.08 \n60.43 \n52.41 \n51.87 \n40.64 \n39.04 \n49.73 \n44.39 \n11.23 \n11.23 \n8.56 \n12.83 \nGlobMed-Qwen3-4B \n52.94 \n56.15 \n54.01 \n52.41 \n39.57 \n41.18 \n54.55 \n55.08 \n28.34 \n27.81 \n25.67 \n25.13 \nQwen3-8B \n59.89 \n67.91 \n53.48 \n54.55 \n52.41 \n43.32 \n56.15 \n60.43 \n20.32 \n11.23 \n14.97 \n16.04 \nGlobMed-Qwen3-8B \n58.82 \n60.96 \n55.08 \n56.15 \n54.01 \n49.73 \n55.08 \n56.15 \n24.06 \n25.13 \n22.99 \n22.99 \nSTab. 155: Zero-Shot performance comparison across 12 languages on MMLU-Pro (Run 2). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n34.22 \n47.59 \n44.92 \n34.22 \n33.16 \n32.09 \n35.29 \n36.36 \n26.74 \n16.58 \n11.23 \n13.90 \nGlobMed-MedGemma-4B \n32.09 \n40.64 \n38.50 \n36.36 \n34.22 \n31.55 \n35.83 \n32.62 \n28.34 \n24.06 \n14.97 \n26.74 \nQwen3-1.7B \n29.41 \n41.71 \n31.55 \n19.79 \n21.39 \n19.79 \n24.06 \n28.88 \n13.37 \n11.23 \n11.76 \n11.23 \nGlobMed-Qwen3-1.7B \n33.16 \n39.04 \n34.76 \n34.22 \n28.88 \n25.13 \n37.43 \n34.76 \n16.58 \n17.11 \n14.97 \n17.11 \nQwen3-4B \n51.34 \n56.15 \n50.27 \n48.13 \n43.32 \n35.83 \n49.20 \n50.27 \n15.51 \n11.76 \n10.70 \n9.09 \nGlobMed-Qwen3-4B \n47.59 \n56.15 \n55.61 \n55.61 \n43.32 \n40.64 \n54.01 \n55.61 \n27.27 \n28.88 \n22.46 \n25.13 \nQwen3-8B \n55.08 \n65.78 \n54.55 \n56.68 \n51.87 \n45.99 \n54.01 \n54.01 \n23.53 \n12.30 \n12.83 \n11.23 \nGlobMed-Qwen3-8B \n53.48 \n58.82 \n57.75 \n56.68 \n55.08 \n50.80 \n56.68 \n55.08 \n27.27 \n27.81 \n26.74 \n22.99 \nSTab. 156: Zero-Shot performance comparison across 12 languages on MMLU-Pro (Run 3). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n32.09 \n47.06 \n44.92 \n38.50 \n30.48 \n26.74 \n38.50 \n35.83 \n25.13 \n12.30 \n10.70 \n19.79 \nGlobMed-MedGemma-4B \n33.16 \n41.18 \n35.83 \n39.57 \n31.55 \n30.48 \n36.90 \n34.76 \n29.95 \n22.46 \n24.60 \n24.60 \nQwen3-1.7B \n30.48 \n37.97 \n26.20 \n27.27 \n18.72 \n23.53 \n25.67 \n22.99 \n15.51 \n9.09 \n12.30 \n13.37 \nGlobMed-Qwen3-1.7B \n34.76 \n40.11 \n34.76 \n36.36 \n25.13 \n22.46 \n35.29 \n33.16 \n20.32 \n15.51 \n19.79 \n13.90 \nQwen3-4B \n54.01 \n59.36 \n47.59 \n50.80 \n46.52 \n40.64 \n46.52 \n51.87 \n11.76 \n10.16 \n9.09 \n10.70 \nGlobMed-Qwen3-4B \n49.73 \n56.68 \n53.48 \n52.94 \n41.71 \n39.57 \n53.48 \n55.61 \n23.53 \n26.20 \n25.67 \n24.06 \nQwen3-8B \n57.75 \n66.31 \n54.55 \n54.01 \n55.61 \n49.73 \n55.61 \n60.96 \n19.25 \n13.37 \n14.44 \n14.97 \nGlobMed-Qwen3-8B \n56.68 \n58.82 \n56.15 \n55.61 \n52.41 \n51.87 \n55.08 \n55.61 \n25.67 \n23.53 \n25.67 \n21.39 \nSTab. 157: Zero-Shot performance comparison across 12 languages on MMLU-Pro (Run 4). \n \n \nLLMs \nChinese English French German Japanese Korean Portuguese Spanish Swahili Wolof Yoruba Zulu \nMedGemma-4B \n33.16 \n43.32 \n41.18 \n37.97 \n28.34 \n29.41 \n36.36 \n38.50 \n21.93 \n10.70 \n14.44 \n19.25 \nGlobMed-MedGemma-4B \n31.02 \n38.50 \n37.97 \n44.39 \n32.62 \n27.27 \n36.90 \n33.16 \n29.41 \n17.65 \n22.99 \n23.53 \nQwen3-1.7B \n29.41 \n40.11 \n27.27 \n25.13 \n20.86 \n18.72 \n25.67 \n25.67 \n13.37 \n12.30 \n12.30 \n14.44 \nGlobMed-Qwen3-1.7B \n36.36 \n37.97 \n37.43 \n34.76 \n28.34 \n22.46 \n37.97 \n30.48 \n21.39 \n20.32 \n15.51 \n19.25 \nQwen3-4B \n50.80 \n60.43 \n49.73 \n52.41 \n44.92 \n35.29 \n47.06 \n53.48 \n12.83 \n11.23 \n13.37 \n11.23 \nGlobMed-Qwen3-4B \n52.41 \n56.15 \n50.80 \n53.48 \n40.64 \n41.18 \n55.08 \n56.15 \n23.53 \n26.74 \n25.67 \n23.53 \nQwen3-8B \n60.43 \n66.31 \n55.61 \n56.15 \n51.87 \n43.85 \n58.29 \n58.29 \n15.51 \n11.76 \n10.16 \n8.56 \nGlobMed-Qwen3-8B \n57.22 \n59.36 \n57.22 \n54.55 \n52.41 \n51.34 \n53.48 \n54.01 \n26.74 \n24.60 \n22.46 \n22.46 \nSTab. 158: Zero-Shot performance comparison across 12 languages on MMLU-Pro (Run 5). \n"}]}