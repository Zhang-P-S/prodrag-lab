{"doc_id": "arxiv:2512.18999", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.18999.pdf", "meta": {"doc_id": "arxiv:2512.18999", "source": "arxiv", "arxiv_id": "2512.18999", "title": "Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework", "authors": ["Jinyan Liu", "Zikang Chen", "Qinchuan Wang", "Tan Xie", "Heming Zheng", "Xudong Lv"], "published": "2025-12-22T03:33:43Z", "updated": "2025-12-22T03:33:43Z", "summary": "When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.18999v1", "url_pdf": "https://arxiv.org/pdf/2512.18999.pdf", "meta_path": "data/raw/arxiv/meta/2512.18999.json", "sha256": "6c715c91c2f9135ab6f7875c7d0bc45658fa131856b255ed12bad5fb17ccf80a", "status": "ok", "fetched_at": "2026-02-18T02:23:58.413856+00:00"}, "pages": [{"page": 1, "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nEvaluating the Challenges of LLMs in Real-world Medical Follow-up: A \nComparative Study and An Optimized Framework  \nJINYAN LIU, College of Biomedical Engineering and Instrument Science, Zhejiang University, China \nZIKANGCHEN, College of Biomedical Engineering and Instrument Science, Zhejiang University, China \nQINCHUAN WANG, Department of Surgical oncology, Zhejiang University School of Medicine Sir Run Run Shaw \nHospital, China \nTAN XIE, Department of Surgical oncology, Zhejiang University School of Medicine Sir Run Run Shaw Hospital, China \nHEMING ZHENG, Department of Surgical oncology, Zhejiang University School of Medicine Sir Run Run Shaw \nHospital, China \nXUDONG LV*, College of Biomedical Engineering and Instrument Science, Zhejiang University, China \nWhen applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from \nuncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this \nlimitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and \na modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end \napproach frequently fails on lengthy and complex forms, our modular method—built on task decomposition, semantic \nclustering, and flow management—substantially improves dialog stability and extraction accuracy. Moreover, it reduces the \nnumber of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity \nof integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios. \nCCS CONCEPTS • Computing methodologies → Artificial; intelligence • Applied computing → Health \ninformatics; • Human-centered computing → Human-computer interaction (HCI) \nAdditional Keywords and Phrases: Large language model, Medical follow-up, Chatbot \nACM Reference Format: \n \n \n \n                                                 \n* Corresponding author.  \n"}, {"page": 2, "text": " \n1 INTRODUCTION \nWith advances in the economy and medical technology, people’s expectations for health and quality of life \nhave risen. Many patients not only seek effective treatment but also hope to maintain a high quality of life even \nunder health limitations[11]. Regular follow-up has become an important means to improve treatment outcomes \nand patient well-being. Medical follow-up refers to the process in which healthcare professionals monitor and \ntrack patients’ health status over time, serving as essential feedback for treatment. \nFollow-up is an indispensable part of modern healthcare systems, playing a vital role not only in individual \nhealth management but also in advancing medical research and clinical education. Traditional follow-up \nmethods include in-person visits, telephone follow-ups, and paper-based questionnaires. While in-person visits \nare suitable for complex cases, they are limited by high costs and patient travel burdens [7]. Telephone follow-\nups, conducted by trained hospital staff, enhance communication, improve adherence, and reduce hospital \nreadmission rates[1]. Paper forms allow for detailed reporting but face challenges in efficiency, data \nmanagement, and long-term tracking. \nWith the development of information technologies, electronic and online follow-up methods have gained \ntraction due to their improved efficiency, reduced labor costs, and better data handling[4, 17]. Recently, \nresearchers have explored using dialogue-based chatbots for intelligent follow-up data collection. Compared to \nhuman interviewers, patients are often more willing to disclose truthful information when interacting with \nconversational agents[6]. Chatbots also offer greater interactivity and engagement than form-based methods[2], \nand help alleviate the workload of healthcare professionals. However, most existing systems rely heavily on \nlarge datasets for training, making them difficult to generalize across diverse follow-up scenarios. Furthermore, \nrule-based systems often lack the flexibility for meaningful interaction with patients. \nThe emergence of large language models (LLMs) offers new possibilities for rapidly building follow-up \ndialogue systems. LLMs are self-supervised models trained on massive text (and multimodal) data, capable of \nunderstanding, generating, reasoning, and interacting through language [12]. In medical follow-up scenarios, \nLLM-based systems require neither complex rule sets nor large annotated datasets, reducing development and \nmaintenance costs. Their strong language capabilities enable more natural and humanized conversations, \nimproving patient empathy and engagement[15]. Therefore, LLM-based follow-up chatbots show both technical \nfeasibility and potential in enhancing patient adherence and care. Chen et al.[3] developed a follow-up chatbot \nfor postoperative management with report generation and flow adaptation but lacked support for complex forms. \nYang et al. [20] proposed Talk2Care for elderly users, supporting natural conversation and summarization to \nimprove home follow-up. Tu et al. [16] introduced TRUST for PTSD assessment via LLM-driven clinical interview \nsimulation, achieving expert-level diagnosis. Wei et al. [18] showed that prompt design significantly affects the \nperformance of self-report data collection. \nDirectly inputting medical follow-up forms into LLMs to automatically generate questions, extract answers, \nand store data is a straightforward approach to building follow-up chatbots. However, our investigation into \nfollow-up forms for the ten most common cancers revealed significant diversity and structural complexity—\nvarying by disease type, clinical stage, and patient profile, and often involving conditional jumps, nested logic, \nand mixed question types. In real-world long-context, multi-turn interactions, this leads to information loss or \nhallucinations [9, 10], making accurate follow-up challenging. Moreover, end-to-end generation struggles to \ngeneralize across varied forms and fails to capture hierarchical and logical dependencies, reducing \ncontrollability and reliability [8].  \n"}, {"page": 3, "text": " \nEnsuring both flexibility and structured control thus remains a key challenge in applying LLMs to medical \nfollow-up. \nTo investigate the limitations of applying large language models in an end-to-end manner for medical follow-\nup chatbots, we conducted experiments based on prior research on follow-up and patient data collection \nsystems, as well as forms obtained from clinical experts. We selected three types of follow-up forms for \ncomparison: (1) simple forms with a small number of questions (10, single choice), (2) medium-complexity forms \nwith more questions (45, single choice) but no logic jumps or complex types, and (3) high-complexity forms (53 \nquestions) that include multiple question types (single choice, multiple choice, and fill-in-the-blank) along with \nnested and conditional logic. \nBy comparing an end-to-end LLM-based approach with our proposed modular follow-up strategy, we analyze \nperformance across different levels of form complexity and systematically identify key limitations of LLMs in \nreal-world follow-up scenarios. \nThe main contributions of this work are as follows: \n⚫ \nWe conduct comparative experiments across follow-up forms with varying complexity (in terms of \nquestion volume, types, and branching logic) to analyze the performance of large language models in \nquestion generation, context retention, option recognition, and logic handling. Our results systematically \nreveal key limitations of end-to-end approaches in complex follow-up scenarios, including repeated \nquestioning, premature termination, and failure to produce structured outputs. \n⚫ \nTo address these issues, we propose a form-aware modular control framework that orchestrates \nquestion generation, answer extraction, and logic transitions. This structured mechanism ensures the \nintegrity of the dialogue flow and the accuracy of structured outputs, thereby enhancing both the \nflexibility and the practical deployability of LLM-based follow-up systems.. \nOur implementation, prompts, and demo examples are available at https://github.com/LiuJnYn/LLM_follow-\nup \n2 METHODS \nOur main research methodology is illustrated in Figure 1. First, we analyzed all the follow-up forms currently \nobtained from discussions with clinical informatics experts to understand real-world follow-up needs. Second, \nbased on Qwen-plus, we utilized prompt engineering to set up three distinct virtual patients to simulate real-\nworld conversations between patients and the follow-up dialogue chatbots. Next, we established an end-to-end \nfollow-up dialogue robot as the control group and a modularly controlled follow-up dialogue robot as the \nexperimental group. \n2.1 Follow-up Form Analysis \nBased on discussions with clinical informatics experts and the analysis of follow-up forms collected from \nreal-world medical projects, we obtained a total of 32 follow-up forms. We examined key parameters such as \nthe number of questions, question types, and the presence of logical branching. Our analysis revealed that the \nnumber of questions per form ranges from 10 to 146. Over half of the follow-up forms fall within the 10–50 \nquestion range, reflecting a concentrated distribution around this interval. Question types include single-choice, \nmultiple-choice, free-text, mixed choice-text input, checklist, and body map formats. Additionally, over 50% of \n"}, {"page": 4, "text": " \nthe forms involve logical branching between questions, indicating that handling branching logic is an essential \nand unavoidable aspect of medical follow-up forms. \n \n2.2 Follow-up Chatbot Designs \n2.2.1 Virtual Patient Design \nTo simulate real-world interactions between patients and follow-up dialogue agents, we designed structured \nbut content-varied virtual patient prompts to guide the Qwen-plus[19] language model in role-playing virtual \npatients. These virtual patients engaged separately with both the end-to-end and the flow-controlled follow-up \nchatbots. As illustrated in Figure 1(a), each prompt consisted of three components. First, we defined personal \nbackground information—such as name, age, occupation, and residence—to establish the patient's identity. \nSecond, we assigned distinct profiles for each of the three virtual patients, in order to introduce diversity in \nlanguage style and behavioral responses. Finally, we extracted and revised several turns of real follow-up \nconversations from previously collected data and used them as few-shot examples aligned with each virtual \npersona. These examples helped guide the model to generate contextually coherent and realistic responses, \nenabling a comprehensive evaluation of the chatbot’s stability and adaptability across diverse patient types.  \n2.2.2 End-to-End LLM-based Chatbot \nAs shown in Figure 1(b), we designed an end-to-end follow-up dialogue system based on a large language \nmodel (LLM) as the control group. The system is driven by a natural language prompt, which consists of three \ncomponents: 1) Form information: the three follow-up forms introduced in Section 3.1 are transformed into a \nFigure 1.Overall Architecture of the Follow-up Dialogue System and Experimental Framework. (a) Three virtual patients \nwere constructed by inputting their basic information, personality traits, and few-shot dialogue examples into the language \nmodel to simulate distinct verbal behavior. (b) In the control group, an end-to-end follow-up chatbot was configured by \nprompting the language model with dialogue history, form information, and instructions, allowing the model to \nautonomously handle question generation, intent extraction, and dialogue flow management. We test three LLM. (c) In \nthe experimental group, a programmatically controlled follow-up chatbot was implemented by decomposing key tasks \nand independently designing modules for question generation, intent extraction, and dialogue management. \n"}, {"page": 5, "text": " \nunified textual structure that includes the full questionnaire content, each question’s options, and their skip and \nnested logic, to guide question generation and dialogue progression; 2) Dialogue history: the completed \ndialogue rounds are provided to help the model track the dialogue state within context; 3) Instruction setting: a \nsystem prompt and procedural task chain instruct the model to read and understand the form and dialogue \nhistory, perform question generation, answer extraction, and logical reasoning, thus constraining the model's \nbehavior to ensure the dialogue progresses appropriately. \nBased on this prompt, the model must independently perform question generation, intent extraction, and \ndialogue flow control without any external programmatic guidance, and output a structured JSON result. We \nselected three mainstream general-purpose LLMs—Qwen-plus, DeepSeek-chat[5], and GPT-4.1[13]—as base \nmodels. Under identical input conditions, each virtual patient interacted with each model on each follow-up form \nthrough 10 rounds of simulated Q&A to evaluate performance in handling long forms, multi-turn understanding, \nand dialogue management. \n2.2.3 Modular LLM-based Chatbot \nTo enhance the reliability and controllability of the follow-up dialogue process, we designed a modular LLM-\nbased follow-up chatbot using Qwen-plus, which serves as the experimental group. This system comprises five \nmain modules: form structure construction, question preprocessing, question generation, dialogue information \nextraction, and dialogue flow control. \nIn the form structure construction phase, we converted the original follow-up forms into structured JSON \nformat. Each entry includes the question text, question type, options for single/multiple-choice questions, suffix \ntext for fill-in-the-blank items, and conditional logic for branching. \nNext, during the preprocessing phase, the form questions were categorized by type (single-choice, multiple-\nchoice, fill-in-the-blank), and questions of the same type were input into the LLM for semantic clustering. This \ndesign mirrors real-world follow-up practices observed in actual recordings, where interviewers often combine \nrelated questions into a single inquiry to improve communication efficiency and naturalness. Specifically, we \nfirst constructed an abstraction prompt to instruct the LLM to summarize and analyze all questions, producing \na content-level description. Then, this summary and the original questions were jointly input into a clustering \nprompt, guiding the model to group semantically or structurally similar questions into clusters. Each form \nunderwent multiple clustering trials, and the most frequently occurring result was selected as the final grouping. \nFor each question group, we designed distinct question generation prompts according to the question type, \nenabling the model to generate human-like, conversational queries. This approach addresses patients' potential \ndifficulty in understanding highly clinical or technical phrasing in the original forms [14]. \nAfter receiving a response from the virtual patient, we combined the form entry and current dialogue context \nand input them into the LLM along with an intent extraction prompt, tailored to each question type. We designed \na RAG-based method for intent extraction to enhance the large language model's ability to extract information \nbased on the current dialogue context. The construction process for the RAG knowledge base is as follows: \nAfter the LLM generates natural language questions from the form content, we randomly select a preset user \nintent corresponding to the dialogue and instruct the three virtual patients to provide a natural language \nresponse based on that intent. This process automatically generates a \"question - patient response - intent \nextraction result\" dataset. During the intent extraction process, we retrieve the dialogues from the knowledge \nbase that are most similar to the current conversation. These are used as few-shot examples to prompt the \n"}, {"page": 6, "text": " \nlarge language model with the extraction results from these similar dialogues. LLM was instructed to extract the \ncorrect structured information corresponding to the form item from the user's response. \nThe question selection module then determines the next group of questions based on three rules: \n(1) If any items in the current question group fail to yield extracted intent, the unanswered items are \nregrouped and asked again. \n(2) If all items have valid intent extracted, the system checks whether any responses trigger follow-up \nquestions (e.g., if the patient answers \"quit smoking\" to \"Do you smoke?\", additional questions about quitting \nare required). If so, the follow-up questions are returned to the clustering module and re-enter the full processing \nloop. \n(3) If no new questions are triggered, the system proceeds to the next predefined group of questions. \n3 RESULT  \n3.1 Form Selection Results \nBased on the statistical analysis result, we selected three representative forms to evaluate the capabilities \nof large language models. Form-1 is a simple form containing a small number of questions (10 questions, all \nsingle-choice), primarily used to assess how post-treatment pain affects patients’ daily lives. Form-2 is a \nmedium-complexity form with a larger number of questions (45 questions, also single-choice) focused on \nevaluating patients’ quality of life. Form-3 is a high-complexity form consisting of 53 questions that include \nsingle-choice, multiple-choice, and fill-in-the-blank types, along with embedded skip logic. It is designed to \ncollect comprehensive information on patients’ overall health status. \n3.2 Evaluation of Follow-up Chatbot Systems \nBy combining demographic information, personality traits, and real few-shot dialogue examples, we \nsuccessfully constructed three types of virtual patients to reflect common response styles in real-world follow-\nup scenarios: clear and concise (patient-1), clear but verbose (patient-2), and vague or off-topic (patient-3). The \nbenefit of this design is that it transcends a simple \"correct answer\" test, providing a more challenging and \nrealistic benchmark for evaluating the robustness and adaptability of dialogue systems.  \nThrough systematic analysis of the dialogue generation process in the control group, we observed that all \ntested LLMs were able to complete the follow-up tasks for Form-1 with high accuracy in flow control and intent \nextraction. However, when handling longer or more complex forms involving skip logic, various issues emerged. \nSome models failed to initiate the conversation from the first question, leading to deviations from the intended \ndialogue flow. Others prematurely terminated the interaction before completing all questions, resulting in \nincomplete information collection. Additionally, excessively long response times negatively impacted interaction \nefficiency. In content generation, certain models altered the wording or structure of the original questions—an \nissue most frequently observed with patient-3. Other common problems included repeated questions, errors in \nlogic jumps, and skipped or missed items. These findings highlight the limitations of the end-to-end approach \nin managing structured questionnaires, handling complex skip logic, and enforcing task constraints. A detailed \nclassification of error types and their frequency across different models and forms is provided in Table 1. \n \n \n"}, {"page": 7, "text": " \nTable 1: Summary of Error Types Presented in Form-2 and Form-3 \nForm Type \nError Type \nQwen-Plus \nDeepSeek-Chat \nGPT-4.1 \nForm-2 \nStarting from the middle \n0 \n0 \n1 \nEnding dialogue prematurely \n2 \n0 \n0 \nExcessive response time \n1 \n11 \n5 \nAltering questions \n3 \n8 \n8 \nRepetitive questioning \n19 \n16 \n29 \nLogical jump errors \n\\ \n\\ \n\\ \nSkipping/Missing questions \n6 \n4 \n1 \nForm-3 \nStarting from the middle \n3 \n2 \n0 \nEnding dialogue prematurely \n20 \n3 \n5 \nExcessive response time \n4 \n9 \n1 \nAltering questions \n12 \n13 \n18 \nRepetitive questioning \n11 \n23 \n15 \nLogical jump errors \n10 \n8 \n5 \nSkipping/Missing questions \n13 \n7 \n5 \nWe also manually evaluated the intent extraction accuracy of different large language models across \nvarious forms and virtual patient profiles. Results show that under the experimental setting of Form-2, the \nmodels initially maintained a structured mapping of intents with relatively high accuracy. However, as the \ndialogue progressed, the error rate increased, and issues emerged where responses were not aligned with the \npredefined options. In the more complex Form-3, extraction errors were primarily due to the models failing to \nstructure free-text responses from fill-in-the-blank questions—instead copying the virtual patient’s full sentence \nwithout parsing it. A comparison of intent extraction accuracy across models is illustrated in Figure 2, where we \nobserve a consistent decline in performance as the form length and complexity increase. To enhance the \nvisibility of performance difference, the y-axis is intentionally truncated to start at 0.5. \n \nFigure 2. Comparison of Intent Extraction Accuracy in End-to-End Follow-up Chatbot. Note that the Y-axis (Accuracy) starts \nfrom 0.5. \nIn the experimental setup, the semantic clustering module powered by a large language model automatically \ngrouped semantically similar items within the form and merged them into composite questions, enabling more \nefficient and natural follow-up interactions. Compared to the end-to-end chatbot, the average number of \ndialogue turns was reduced by 64% for Form-1, 48.1% for Form-2, and 28.1% for Form-3. It is worth noting that \nfor Form-2, the average number of turns only reflects results from Qwen-Plus and DeepSeek-Chat, as GPT-4.1 \n"}, {"page": 8, "text": " \nfailed to complete form completion within the maximum allowed 80 turns in all test cases. Overall, the average \nreduction in dialogue turns across all forms was 46.73%, significantly improving information collection efficiency. \nFigure 3 compares the average response time, token consumption, and intent extraction accuracy between \nthe experimental and control methods using the Qwen-Plus model. The slightly longer average response time \nin the control group (+0.47 seconds) is attributed to longer context lengths that the model needs to process. \nWhile the difference is not large, the control group occasionally experienced timeout failures due to excessively \nlong responses (see Table 1), which did not occur in the experimental group. \n \nIn terms of system efficiency, the token consumption per completed turn in the control group was 5–8 times \nhigher than that of the experimental group, and unexpected behavior was also observed. When dealing with \nshorter forms, both groups achieved intent extraction accuracy around 0.9, demonstrating the language model's \ncapability in understanding and extracting intent. However, as the form length and complexity increased (in \nForm-2 and Form-3), the control group’s accuracy dropped significantly—mainly due to poor handling of \nfrequency-related items, unstructured extraction from fill-in-the-blank answers, and failure to adhere to \npredefined option sets. In contrast, the experimental group consistently maintained high and stable accuracy \nacross conditions. \n4 DISCUSSION \nThis study systematically compares the performance differences between end-to-end and modular LLM-\ndriven follow-up dialogue systems under varying form complexities, revealing the core challenges and potential \noptimization directions for current large models in medical follow-up applications. \nFirst, the experimental results indicate that while the end-to-end method possesses basic usability for short \nforms, it is prone to issues such as dialogue flow interruption, repetitive questioning, and skipping or missing \nquestions when faced with a large volume of items and high logical complexity. This primarily stems from the \nLLM's deficiencies in long-context retention, logical reasoning, and task-boundary awareness. Lacking external \nstructural constraints, the model easily falls into infinite generation loops or transition failures. This is especially \ntrue in intent extraction tasks, where accuracy significantly declines with increasing context length, severely \nimpacting system stability and reliability. Furthermore, the token consumption of the control group models on \ncomplex tasks was 5–8 times that of the experimental group, with extreme resource waste observed, such as \none session reaching 3,133 turns and consuming nearly 200 million tokens. \nFigure 3. Performance Comparison Between Experimental and Control Chatbots Across Key Metrics. Note that the LLM \nResponse Time Y-axis (Accuracy) starts from 0.5. \n"}, {"page": 9, "text": " \nIn contrast, our proposed modular approach—by leveraging form structure parsing, semantic clustering, \nquestion generation, intent extraction, and a rule-driven flow scheduling mechanism—can effectively guarantee \nthe completeness of task progression and the structured nature of the output. It also retains the LLM's natural \nlanguage understanding and generation advantages in terms of interaction flexibility and patient adaptability. \nThis method not only reduces the number of interaction turns by 46.73% but also demonstrates higher accuracy \nand robustness in handling complex logic and information extraction, all while preserving the LLM's natural \nconversational ability, thus possessing stronger deployment feasibility. \nDespite this, our method still relies on rule and prompt design and lacks high-level generalizability. Although \nthe virtual patient setup enhanced the controllability of the experiment, there is still a gap compared to \ninteractions with real patients. Future research can further explore adaptive flow control mechanisms, patient-\nfeature-driven personalization strategies, and multimodal follow-up systems to promote the safe, controllable, \nand efficient application of LLMs in medical follow-up. \n5 CONCLUSION \nThis study focuses on exploring the limitations of large language models in real-world medical settings and \ncompares the performance of end-to-end and modular LLM-based follow-up dialogue systems in interactions \nwith virtual patients. Experimental results show that as form complexity increases, end-to-end methods face \nchallenges such as uncontrollable dialogue flow, inaccurate intent extraction, and excessive resource \nconsumption. To address these issues, we propose a modular approach that decomposes the follow-up task \nand incorporates semantic clustering and flow control mechanisms. This method not only effectively completes \nthe follow-up process but also significantly reduces the number of dialogue turns and token consumption. Our \nfindings highlight that LLMs alone are insufficient for handling complex tasks in highly structured and accuracy-\ncritical medical scenarios. External control mechanisms and carefully designed workflows are essential. Future \nwork may explore integrating personalized patient information to enhance user experience, as well as \ndeveloping more automated dialogue management strategies to advance intelligent follow-up systems toward \ngreater safety, personalization, controllability, and efficiency. Furthermore, the current method remains based \non paper forms and is limited to text-based interaction. Future research should explore multimodal capabilities \nto accommodate the demands of more complex medical follow-up scenarios. \nACKNOWLEDGMENTS \nThis work was supported by the Research and Development Project of “Jianbing” “Lingyan” of Zhejiang \nProvince [grant numbers 2025C01128] \nREFERENCES \n[1] \nEyal Braun, Amjad Baidusi, Gideon Alroy, and Zaher S. Azzam. 2009. Telephone follow-up improves patients satisfaction \nfollowing hospital discharge. Eur. J. Intern. Med. 20, 2 (March 2009), 221–225. https://doi.org/10.1016/j.ejim.2008.07.021 \n[2] \nBenjamin Chaix, Jean-Emmanuel Bibault, Arthur Pienkowski, Guillaume Delamon, Arthur Guillemassé, Pierre Nectoux, and \nBenoît Brouard. 2019. When chatbots meet patients: one-year prospective study of conversations between patients with \nbreast cancer and a chatbot. JMIR Cancer 5, 1 (May 2019), e12856. https://doi.org/10.2196/12856 \n[3] \nChen Chen, Jianing Yin, Jiannong Cao, Zhiyuan Wen, Mingjin Zhang, Weixun Gao, Xiang Wang, and Haihua Shu. 2025. \nFollowUpBot: An LLM-Based Conversational Robot for Automatic Postoperative Follow-up. \nhttps://doi.org/10.48550/arXiv.2507.15502 \n"}, {"page": 10, "text": " \n[4] \nStephen Joel Coons, Sonya Eremenco, J. Jason Lundy, Paul O’Donohoe, Hannah O’Gorman, and William Malizia. 2015. \nCapturing patient-reported outcome (PRO) data electronically: the past, present, and promise of ePRO measurement in \nclinical trials. Patient - Patient-centered Outcomes Res. 8, 4 (August 2015), 301–309. https://doi.org/10.1007/s40271-014-\n0090-z \n[5] \nDeepSeek-AI, Daya Guo, Dejian Yang et al. 2025. DeepSeek-R1: incentivizing reasoning capability in LLMs via \nreinforcement learning. https://doi.org/10.48550/arXiv.2501.12948 \n[6] \nKathleen Kara Fitzpatrick, Alison Darcy, and Molly Vierhile. 2017. Delivering cognitive behavior therapy to young adults with \nsymptoms of depression and anxiety using a fully automated conversational agent (woebot): a randomized controlled trial. \nJMIR Ment. Health 4, 2 (June 2017), e19. https://doi.org/10.2196/mental.7785 \n[7] \nElham Hatef, Renee F. Wilson, Allen Zhang, Susan M. Hannum, Hadi Kharrazi, Stacey A. Davis, Iman Foroughmand, \nJonathan P. Weiner, and Karen A. Robinson. 2024. Effectiveness of telehealth versus in-person care during the COVID-19 \npandemic: a systematic review. npj Digital Med. 7, 1 (June 2024), 157. https://doi.org/10.1038/s41746-024-01152-2 \n[8] \nShreya Johri, Jaehwan Jeong, Benjamin A. Tran, Daniel I. Schlessinger, Shannon Wongvibulsin, Leandra A. Barnes, Hong-\nYu Zhou, Zhuo Ran Cai, Eliezer M. Van Allen, David Kim, Roxana Daneshjou, and Pranav Rajpurkar. 2025. An evaluation \nframework for clinical use of large language models in patient interaction tasks. Nat. Med. 31, 1 (January 2025), 77–86. \nhttps://doi.org/10.1038/s41591-024-03328-5 \n[9] \nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the \nmiddle: how language models use long contexts.  \n[10] \nTaiming Lu, Muhan Gao, Kuai Yu, Adam Byerly, and Daniel Khashabi. 2024. Insights into LLM long-context failures: when \ntransformers know but don’t tell. https://doi.org/10.48550/arXiv.2406.14673 \n[11] \nKalliopi Megari. 2013. Quality of life in chronic disease patients. Health Psychol. Res. 1, 3 (September 2013), e27. \nhttps://doi.org/10.4081/hpr.2013.e27 \n[12] \nSeyed Mahmoud Sajjadi Mohammadabadi, Burak Cem Kara, Can Eyupoglu, Can Uzay, Mehmet Serkan Tosun, and Oktay \nKarakuş. A survey of large language models: evolution, architectures, adaptation, benchmarking, applications, challenges, \nand societal implications.  \n[13] \nOpenAI, Josh Achiam, Steven Adler et al.2024. GPT-4 technical report. https://doi.org/10.48550/arXiv.2303.08774 \n[14] \nBenjamin J. Powers, Jane V. Trinh, and Hayden B. Bosworth. 2010. Can this patient read and understand written health \ninformation? JAMA 304, 1 (July 2010), 76. https://doi.org/10.1001/jama.2010.896 \n[15] \nArun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu \nWei Ting. 2023. Large language models in medicine. Nat Med 29, 8 (August 2023), 1930–1940. \nhttps://doi.org/10.1038/s41591-023-02448-8 \n[16] \nSichang Tu, Abigail Powers, Stephen Doogan, and Jinho D. Choi. 2025. TRUST: An LLM-Based Dialogue System for \nTrauma Understanding and Structured Assessments. https://doi.org/10.48550/arXiv.2504.21851 \n[17] \nSabine N. van der Veer, Nicola E. Anderson, Rob Finnigan, and Derek Kyte. 2024. Electronic collection of patient-reported \noutcomes to improve kidney care: benefits, drawbacks, and next steps. Semin. Nephrol. 44, 3 (May 2024), 151552. \nhttps://doi.org/10.1016/j.semnephrol.2024.151552 \n[18] \nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim. 2024. Leveraging Large Language Models to Power \nChatbots for Collecting User Self-Reported Data. Proc. ACM Hum.-Comput. Interact. 8, CSCW1 (April 2024), 1–35. \nhttps://doi.org/10.1145/3637364 \n[19] \nAn Yang, Bowen Yu, Chengyuan Li et al. 2025. Qwen2.5-1M technical report. https://doi.org/10.48550/arXiv.2501.15383 \n[20] \nZiqi Yang, Xuhai Xu, Bingsheng Yao, Ethan Rogers, Shao Zhang, Stephen Intille, Nawar Shara, Guodong Gordon Gao, and \nDakuo Wang. 2024. Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older \nAdults. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 8, 2 (May 2024), 1–35. https://doi.org/10.1145/3659625 \n \n"}]}