{"doc_id": "arxiv:2602.13979", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.13979.pdf", "meta": {"doc_id": "arxiv:2602.13979", "source": "arxiv", "arxiv_id": "2602.13979", "title": "Chain-of-Thought Reasoning with Large Language Models for Clinical Alzheimer's Disease Assessment and Diagnosis", "authors": ["Tongze Zhang", "Jun-En Ding", "Melik Ozolcer", "Fang-Ming Hung", "Albert Chih-Chieh Yang", "Feng Liu", "Yi-Rou Ji", "Sang Won Bae"], "published": "2026-02-15T03:56:24Z", "updated": "2026-02-15T03:56:24Z", "summary": "Alzheimer's disease (AD) has become a prevalent neurodegenerative disease worldwide. Traditional diagnosis still relies heavily on medical imaging and clinical assessment by physicians, which is often time-consuming and resource-intensive in terms of both human expertise and healthcare resources. In recent years, large language models (LLMs) have been increasingly applied to the medical field using electronic health records (EHRs), yet their application in Alzheimer's disease assessment remains limited, particularly given that AD involves complex multifactorial etiologies that are difficult to observe directly through imaging modalities. In this work, we propose leveraging LLMs to perform Chain-of-Thought (CoT) reasoning on patients' clinical EHRs. Unlike direct fine-tuning of LLMs on EHR data for AD classification, our approach utilizes LLM-generated CoT reasoning paths to provide the model with explicit diagnostic rationale for AD assessment, followed by structured CoT-based predictions. This pipeline not only enhances the model's ability to diagnose intrinsically complex factors but also improves the interpretability of the prediction process across different stages of AD progression. Experimental results demonstrate that the proposed CoT-based diagnostic framework significantly enhances stability and diagnostic performance across multiple CDR grading tasks, achieving up to a 15% improvement in F1 score compared to the zero-shot baseline method.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.13979v1", "url_pdf": "https://arxiv.org/pdf/2602.13979.pdf", "meta_path": "data/raw/arxiv/meta/2602.13979.json", "sha256": "fe9e5033204735049e70cab691a885bfd567dc62d2dd6213539fc5530cdff705", "status": "ok", "fetched_at": "2026-02-18T02:19:17.202209+00:00"}, "pages": [{"page": 1, "text": "Chain-of-Thought Reasoning with Large Language\nModels for Clinical Alzheimer’s Disease\nAssessment and Diagnosis\nTongze Zhang\nStevens Institute of Technology\nHoboken, New Jersey\nJun-En Ding\nStevens Institute of Technology\nHoboken, New Jersey\nMelik Ozolcer\nStevens Institute of Technology\nHoboken, New Jersey\nFang-Ming Hung\nSurgical Trauma Intensive Care\nUnit\nFar Eastern Memorial Hospital\nAlbert Chih-Chieh Yang\nInstitute of Brain Science\nNational Yang Ming Chiao Tung\nUniversity\nFeng Liu\nStevens Institute of Technology\nHoboken, New Jersey\nYi-Rou Ji\nSurgical Trauma Intensive Care Unit\nNational Yang Ming Chiao Tung\nUniversity\nSang Won Bae*\nStevens Institute of Technology\nHoboken, New Jersey\nAbstract—Alzheimer’s disease (AD) has become a prevalent\nneurodegenerative disease worldwide. Traditional diagnosis still\nrelies heavily on medical imaging and clinical assessment by\nphysicians, which is often time-consuming and resource-intensive\nin terms of both human expertise and healthcare resources. In\nrecent years, large language models (LLMs) have been increas-\ningly applied to the medical field using electronic health records\n(EHRs), yet their application in Alzheimer’s disease assessment\nremains limited, particularly given that AD involves complex\nmultifactorial etiologies that are difficult to observe directly\nthrough imaging modalities. In this work, we propose leveraging\nLLMs to perform Chain-of-Thought (CoT) reasoning on patients’\nclinical EHRs. Unlike direct fine-tuning of LLMs on EHR data\nfor AD classification, our approach utilizes LLM-generated CoT\nreasoning paths to provide the model with explicit diagnostic\nrationale for AD assessment, followed by structured CoT-based\npredictions. This pipeline not only enhances the model’s ability to\ndiagnose intrinsically complex factors but also improves the inter-\npretability of the prediction process across different stages of AD\nprogression. Experimental results demonstrate that the proposed\nCoT-based diagnostic framework significantly enhances stability\nand diagnostic performance across multiple CDR grading tasks,\nachieving up to a 15% improvement in F1 score compared to\nthe zero-shot baseline method.\nIndex Terms—Alzheimer’s Disease, Large Language Models,\nChain-of-Thought Reasoning, Clinical Decision Support, Elec-\ntronic Health Records, Neurodegenerative Disorders\nI. INTRODUCTION\nAlzheimer’s disease (AD) is the most prevalent neurode-\ngenerative disorder worldwide, posing a growing challenge\nto healthcare systems and patients’ quality of life. The rising\nglobal prevalence of AD creates an urgent need for accurate,\nscalable, and resource-efficient diagnostic tools. Traditionally,\nstandard diagnostic protocols remain heavily reliant on a\ncombination of time-consuming and costly methods, includ-\ning advanced medical imaging modalities such as PET and\nstructural MRI, alongside clinical assessments by specialized\nphysicians [1] [2]. This high dependence on human expertise\nand expensive infrastructure limits accessibility.\nAlzheimer’s disease is characterized by progressive im-\npairment of cognitive and functional behavior, encompassing\nmemory, orientation, judgement, and the ability to perform\ndaily activities. Such behavioral changes are typically doc-\numented within the clinical narratives of electronic health\nrecords (EHRs), where clinicians describe patients’ functional\nstatus, behavioral symptoms, and activities of daily living\nusing natural language. The Clinical Dementia Rating (CDR)\nserves as a standardized clinical assessment tool [3], synthe-\nsizing multidimensional cognitive and functional behavioral\nperformance.\nIn recent years, the integration of large language models\n(LLMs) with EHR has transformed predictive analytics in\nmedicine. LLMs are increasingly applied to diverse tasks rang-\ning from clinical document summarization to patient outcome\nprediction. However, their direct application to inherently mul-\ntifactorial complex diseases like AD remains constrained. De-\nspite advances in LLMs for understanding medical texts, their\napplication in diagnostic decision-making remains somewhat\nlimited. Traditional LLM fine-tuning often produces black-\nbox classifiers with limited interpretability and traceability,\nconstraining their clinical adoption [4].\nTo bridge this critical gap in explainability and complex\nreasoning, we propose a novel diagnostic workflow leveraging\nChain-of-Thought (CoT) reasoning within LLMs for AD as-\nsessment based on comprehensive clinical EHR data [5]. Our\napproach achieves this by explicitly generating intermediate\ndiagnostic reasoning. These LLM-generated CoT paths are\narXiv:2602.13979v1  [cs.CL]  15 Feb 2026\n"}, {"page": 2, "text": "designed to mimic the explicit, stepwise logical reasoning\nprocess employed by clinical experts, transforming hetero-\ngeneous EHR features into structured, verifiable explanations\nbefore arriving at a final prediction. By integrating this explicit\nreasoning layer, the model demonstrates enhanced capability\nin handling the intrinsically complex factors defining AD\npathogenesis and progression. The results demonstrate that\nincorporating structured reasoning significantly enhances di-\nagnostic consistency and interpretability while maintaining\nperformance. This research highlights the potential of CoT-\nenhanced large language models to bridge automated predic-\ntion with interpretable clinical reasoning, laying the foundation\nfor trustworthy AI-assisted diagnostic systems in neurode-\ngenerative disease research. Existing research primarily relies\non longitudinal clinical records to predict whether or when\nAlzheimer’s disease will occur. In contrast, this study focuses\non clinical staging through the grading of CDR within current\nelectronic health record texts, emphasizing interpretable, real-\ntime assessments to support clinical decision-making. This\nwork presents a structured, multi-stage Chain-of-Thought rea-\nsoning framework for interpretable and stable behavioral as-\nsessment from clinical narratives using large language models.\nTo address the current lack of interpretability and reliability\nin language models for Alzheimer’s disease (AD) diagnosis,\nthis study primarily explores the following three key questions:\n1. Will LLM reliably identify subtle CDR grading differ-\nences from unstructured EHRs?\n2. Can incorporating CoT reasoning enhance interpretability\nand credibility?\n3. Can multi-stage reasoning structures reduce performance\nfluctuations across different CDR grading tasks, thereby en-\nhancing prediction consistency?\nII. RELATED WORK\nA. Traditional Approaches to Alzheimer’s Disease Diagnosis\nThe diagnosis of Alzheimer’s disease (AD) has traditionally\nrelied upon neuropsychological assessments, imaging exam-\ninations (such as MRI and PET), and cerebrospinal fluid\nbiomarkers [1]. Whilst these methods possess clinical value,\nthey are costly, invasive, and difficult to deploy at scale for\nscreening purposes [6]. In recent years, machine learning and\ndeep learning approaches have been applied to neuroimaging\ndata for automated AD detection and disease progression pre-\ndiction. For instance, convolutional neural networks (CNNs)\napplied to MRI or PET scans have achieved high accuracy\nin distinguishing mild cognitive impairment (MCI) from AD\n[7] [8]. However, these models often exhibit “black box”\nbehaviour, lacking transparent reasoning pathways. Comple-\nmenting imaging research, natural language processing (NLP)\nmethods have also been applied to EHR texts: analysing\nnarrative clinical notes, cognitive test summaries, and EHR\ntime-series data can detect cognitive decline and predict con-\nversion to AD [9]. However, many NLP-based systems rely\non bag-of-words models, sequence classification, or fine-tuned\nTransformer models. While these approaches capture surface-\nlevel patterns, they fail to explicitly model clinical reasoning\nprocesses or achieve interpretability. Basic clinical staging and\nbiomarker frameworks further underscore the importance of\nmild cognitive impairment as the biological precursor stage of\nAlzheimer’s disease. These findings have guided the design\nof recent machine learning models [10] [11]. Systematic re-\nviews and meta-analyses of MRI/PET-based machine learning\nsystems have reconfirmed performance improvements while\nsimultaneously demonstrating comparability and data leakage\nrisks within convolutional neural network pipelines [12] [13].\nB. Large Language Models in Medical Text Understanding\nThe advent of LLMs, such as ClinicalBERT, LLaMA, and\ndomain-adapted models, has significantly advanced medical\nnatural language processing. Recent reviews on LLMs in\nhealthcare highlight their broad applicability across tasks\nincluding entity extraction, report summarisation, and out-\ncome prediction [14]. Research indicates that when applied to\nclinical texts, LLMs surpass earlier smaller models, demon-\nstrating potential for diagnostic reasoning [15]. Despite these\nadvances, however, many LLM-based clinical systems suffer\nfrom opacity issues: they provide only classification results or\nrecommendations without displaying intermediate reasoning\nsteps, thereby limiting clinician trust and interpretability.\nRecent clinical text studies on LLMs have primarily focused\non information extraction and classification tasks, typically\nproviding only classification results or evidence snippets with-\nout systematic generative clinical reasoning chains. Simulta-\nneously, many validation studies suffer from small data scales\n(e.g., evaluating a limited number of manually reviewed cases)\nor are constrained by single-center, controlled settings, mak-\ning it difficult to cover complex real-world clinical contexts\n[16]. Beyond early domain models such as ClinicalBERT\n[17], recent studies indicate that LLMs with safety alignment\nmechanisms and instruction-based fine-tuning—such as the\nMed-PaLM series—can encode extensive clinical knowledge\nyet still underperform compared to clinicians in fine-grained\ntasks [18] [19]. Randomized trials also indicate that providing\nlarge language model assistance does not uniformly enhance\nphysicians’ diagnostic reasoning capabilities, underscoring the\nnecessity of establishing transparent, auditable workflows [20].\nOur approach addresses this by designing an auditable multi-\nstage reasoning template that generates clinically readable\nintermediate conclusions and final consolidated opinions for\neach record. This aims to enhance interpretability and clinical\nutility while maintaining performance.\nC. Chain-of-Thought (CoT) Reasoning and Explainable AI in\nMedicine\nCoT prompts represent a recently developed methodology\nwithin large language model research, requiring models to\narticulate intermediate reasoning steps rather than directly\noutputting answers [21]. This stepwise reasoning approach\nhas demonstrated improved performance on arithmetic and\ncommon-sense benchmarks. Within the medical domain, CoT\nhas been explored to enhance the interpretability, auditability,\nand coordination with clinicians of large language model\n"}, {"page": 3, "text": "decision-making [22]. For instance, a renal disease diagnosis\nstudy demonstrated that CoT prompts enable LLMs to expose\ndecision pathways and facilitate error tracing [22]. Recent\nstructured clinical reasoning prompts—incorporating differ-\nential reasoning, analytical reasoning, and Bayesian infer-\nence frameworks—further expand LLM potential in medical\ntasks [23]. Despite these advances, CoT reasoning applica-\ntions in neurodegenerative diseases like Alzheimer’s remain\nin their exploratory infancy. Recent work proposes a CoT-\nbased Alzheimer’s disease classification method, achieving ap-\nproximately 16.7% performance improvement through super-\nvised fine-tuning with reasoning cues [24]. Moreover, emerg-\ning research employing multi-agent large language model\nframeworks for early Alzheimer’s detection from longitudinal\nclinical records highlights a trend towards simulating expert\nconsultation workflows [3]. These research gaps motivated our\npresent work: we propose a multi-stage CoT diagnostic system\nspecifically designed for Alzheimer’s assessment. By integrat-\ning structured reasoning with consensus-building mechanisms,\nit significantly enhances interpretability, consistency, and clin-\nical relevance.\nStrengthening CoT by sampling diverse reasoning paths\nand aggregating consensus provides a principled approach to\nreducing variability in clinical reasoning chains [25]. CoT-\nbased diagnostic reasoning has also demonstrated improved\ninterpretability in controlled clinical benchmarks [26], while\nhealthcare multi-agent frameworks based on longitudinal med-\nical records have proven the feasibility of simulating col-\nlaborative team-based diagnosis and treatment processes [3].\nResearch on CoT and structured diagnostic prompts in medical\nsettings has been explored, but most work remains focused\non promoting accurate classification rather than generating\ncomprehensive, verifiable explanatory texts based on clinical\nsemantics [26]. Additionally, existing studies often evaluate\nperformance on limited-scale tasks or benchmarks. Our design\naddresses the challenges of small-sample and out-of-domain\ngeneralization while ensuring interpretability.\nIII. METHOD\nThis study aims to construct an interpretable automated\nCDR grading system based on a two-stage experimental\nframework. The first stage focuses on establishing multiple\nhigh-performance classification baselines to provide impartial\nperformance benchmarks. The second stage introduces the\ncore innovation that a CoT Alzheimer’s Disease Diagnostic\nsystem to address the transparency issues of traditional black-\nbox models in complex clinical reasoning.\nA. Data Preprocessing\nThe raw data used in this study originated from a five\nyear clinical dataset containing patients’ longitudinal EHR,\nprimarily comprising the patient’s Subject (S) and clinical\ndiagnostic Assessment (A) fields. To ensure data quality and\nanalytical validity, we first executed a rigorous data cleaning\nprocess. Since multiple records might exist for the same patient\nacross different time points, we utilized unique medical record\nidentifiers and performed deduplication based on the longest\ntext length principle. The initial dataset contained 745 raw\npatient records. Following preprocessing, all samples with\nempty assessment (A) fields were excluded, yielding 698\ndistinct and complete patient records suitable for subsequent\nmodeling. The CDR labels in our dataset cover four clinically\nrecognized Alzheimer’s disease severity levels: 0.5, 1.0, 2.0,\nand 3.0, corresponding to very mild, mild, moderate, and\nsevere dementia, respectively. Each stage exhibits distinct\nfunctional characteristics: CDR 0.5 patients typically show\nmild memory deficits while maintaining daily independence;\nCDR 1.0 patients exhibit more pronounced cognitive and\nfunctional impairments; those with 2.0 require assistance for\ndaily tasks; and those with 3.0 exhibit severe disorientation\nand complete dependence on caregivers.\nThis experimental design employs a systematic one-versus-\none binary classification strategy, decomposing the complex\nmulti-class CDR problem into a series of more specific and\ncontrollable diagnostic subtasks. The CDR scale is a widely\nrecognized clinical standard for quantifying the degree of cog-\nnitive decline across multifunctional domains such as memory,\norientation, judgment, and self-care.\nTo ensure consistency in assessment across different levels\nof disease severity, we constructed four binary classification\nsubsets from the final dataset: 0.5 vs 1.0, 0.5 vs 2.0, 0.5 vs 3.0,\nand 1.0 vs 3.0. The 0.5 vs 1.0 subset contained 429 patient\nrecords, the 0.5 vs 2.0 subset contained 340 patient records,\nthe 0.5 vs 3.0 subset contained 263 patient records, and the 1.0\nvs 3.0 subset contained 358 patient records. Each subset was\nindependently processed through the proposed CoT inference\nframework to evaluate diagnostic performance across varying\ndegrees of cognitive impairment. Representative samples of\nthese records are presented in Table I. Each binary experiment\nfocused on adjacent or clinically significant stage differences,\nenabling the system to precisely identify diagnostic bound-\naries. The Clinical Dementia Rating (CDR) itself reflects\nthe progressive deterioration of multidimensional functions in\nAlzheimer’s disease patients, including memory, orientation,\njudgment, and self-care abilities. Therefore, the 0.5 vs 1.0 and\n0.5 vs 2.0 comparisons assess whether the model can detect\nsubtle early-stage differences in disease progression, while the\n0.5 vs 3.0 and 1.0 vs 3.0 comparisons validate the model’s\nrobustness across scenarios with significant functional gaps.\nCompared to directly constructing a multi-class classification\nmodel, decomposing the task into binary sub-tasks corre-\nsponding to clinically meaningful decision boundaries reduces\nlabel ambiguity and enhances interpretability. This grouping\napproach thus possesses clear medical significance while pro-\nviding the model with well-defined, verifiable classification\nobjectives. This enables systematic evaluation of reasoning\nstability and discriminative capability across different levels\nof cognitive decline.\nFor each paired task, the system dynamically extracts patient\nrecords corresponding to the two target CDR grades from\nthe preprocessed dataset. These subsets are then divided into\ntraining (80%) and test (20%) sets, ensuring balanced repre-\n"}, {"page": 4, "text": "TABLE I\nREPRESENTATIVE REAL-WORLD CDR-LABELED CLINICAL RECORDS SHOWING THE COMPLEXITY OF SUBJECTIVE (S) AND ASSESSMENT (A) TEXTS.\nCDR\nSubjective Note (S)\nAssessment (A)\n0.5\n“insidious onset with progressive poor memory forgets con-\nversation details but able to manage home affairs; occasional\nconfusion reported by spouse.”\n“Autistic thinking (+) vague responses; mild impairment in\norientation and memory domains; functional independence\nmaintained.”\n1.0\n“WITH daughter deterioration multiple complaints of forget-\nfulness, misplacing items, and poor concentration; sometimes\nfails to find way home.”\n“Suspect depression (treated at Psyche) reports, mild to moder-\nate decline, impaired attention span, partial insight preserved.”\n2.0\n“progressive for years with poor memory Forgetfulness noted\nby family members and reduced self-care; occasional urinary\nincontinence reported.”\n“(favor) in progression with incontinence? cognitive decline\nwith temporal disorientation, impaired judgment, and depen-\ndency for daily activities.”\n3.0\n“request application for disability certificate due to long-term\nconfusion, unable to recognize relatives; total dependence for\nself-care.”\n“Right PCA territory infarct (onset: )(TOAST type: ) consistent\nwith severe dementia picture; bed bound, nonverbal, requires\nfull assistance.”\nsentation across severity levels. This design not only simplifies\nthe model’s decision space but also enables targeted evaluation\nof its reasoning stability and generalization capabilities under\ndiverse clinical contrast conditions. Through this approach,\nthe study systematically explores how the proposed reasoning\nframework adapts to varying degrees of cognitive impairment,\nrevealing its diagnostic interpretability and sensitivity to dis-\nease progression.\nB. CoT Diagnostic System: Independent CoT Generation and\nReasoning Validation\nListing 1. Chain-of-Thought Prompting Structure for CDR Reasoning\n1 System prompt:\n2 \"You are an experienced neurologist\nspecializing in Alzheimer’s disease.\n3 Respond professionally in English and output\nvalid JSON.\"\n4\n5 User prompt:\n6 # Task: Generate a full clinical analysis\nfrom a subjective note\n7 Input: \"Patient reports occasional\ndisorientation...\"\n8 Instructions:\n9 1. Extract key reasoning steps.\n10 2. Provide structured domain-specific\nassessment (Memory, Orientation, etc.).\n11 3. Output JSON:\n12 {\n13\n\"reasoning_steps\": [...],\n14\n\"assessment\": \"...\",\n15\n\"cdr_score\": \"[choose from 0.5, 1]\"\n16 }\nWe developed a four-stage CoT integrated diagnostic frame-\nwork designed to simulate the collective reasoning process\nof expert clinical panels. This architecture overcomes the\nlimitations of traditional single-threaded CoT prompts by\nintegrating mechanisms for reasoning diversity, information\nfusion, logical calibration, and auditability. The entire system\naims to enhance the interpretability and reliability of CDR\ngrading, with the overall workflow illustrated in Figure 1 and\nList 1.\nThe foundation of this framework is the CoT Generation\nand Diversity stage, serving as the core reasoning layer. For\neach patient record composed of subjective clinical notes (S)\nand a predefined binary CDR label pair (e.g., [0.5, 1.0]), the\nsystem initiates four independent reasoning processes. Each\nprocess is executed by a fine-tuned large language model. The\nmodel generates structured JSON-formatted reasoning out-\nputs covering six diagnostic domains—memory, orientation,\njudgment and problem-solving, community affairs, family and\nhobbies, personal care—along with independent preliminary\nCDR scores. To ensure reasoning diversity and mitigate po-\ntential biases from deterministic reasoning, each reasoning\nattempt employs an independent random seed. This deliberate\nrandomness, termed seed cracking, fosters diverse interpretive\nperspectives while maintaining consistency in the medical\nreasoning context. The system instantly rejects any JSON\noutputs with formatting errors or invalid structures, ensuring\nonly complete analyses proceed to the next stage. This phase\nyields four fully independent, semantically rich diagnostic\nreports, each representing a unique reasoning pathway.\nThese four assessment results are treated equally. All evalu-\nation texts are subsequently fed into a language model, which\nsynthesizes the texts into a single coherent diagnostic narra-\ntive from a physician’s perspective. This process integrates\nrecurring clinical clues while resolving contradictions. The\nfinal CDR grade is determined by classifying this consoli-\ndated narrative. This means the final decision originates from\nthe interpretation of the integrated text, rather than a direct\naggregation of the initial scores.\nThe Final Classification and Logical Consistency Check\nstage transforms the integrated assessment into an authoritative\ndiagnostic judgment. Here, the model assumes the role of a\nCDR scoring expert, generating final scores based on prede-\nfined label sets. To ensure robustness and data integrity, the\nsystem programmatically extracts predicted scores from model\ntext responses using strict regular expression patterns. Should\nthe model generate out-of-range values (e.g., 0.0 or 2.0), the\nsystem automatically executes a clamping operation, adjusting\nthe score to the nearest valid label (e.g., [0.5, 1.0]). This\nmechanism mitigates rare yet potentially disruptive prediction\nerrors while preserving the integrity of valid reasoning chains.\nIn the final stage, summary Generation and Auditability\nEnhances interpretability by translating complex multi-stage\n"}, {"page": 5, "text": "Fig. 1. Four-Step Pipeline for Binary CDR Classification\nreasoning into transparent, reviewable narratives. The system\nre-engages the model as a senior clinical consultant, synthesiz-\ning four initial CoT reports, evaluations, and final classification\noutcomes to summarize the entire diagnostic process. The\ngenerated textual audit trail provides clinicians with clear\nvisibility into model reasoning transparency, ensuring each\ndiagnostic conclusion is traceable to its underlying evidence,\nwith the overall output example illustrated in Figure 2.\nIn summary, this four-phase CoT integration framework\ntransforms traditional black-box LLM reasoning into a trans-\nparent, verifiable diagnostic process. By integrating reason-\ning diversity, structured aggregation, logical calibration, and\nexplicit reasoning, the system achieves high robustness and\ninterpretability, establishing itself as a reliable and explainable\nclinical AI solution for Alzheimer’s disease assessment.\nIV. RESULT\nUnder the one-versus-one binary classification strategy, we\nconducted a comprehensive evaluation of model performance\nto reveal the actual capabilities of different models in distin-\nguishing subtle CDR levels. Table II compares zero-shot and\nfine-tuned models based on large language models for CoT\ndiagnostic system.\nTo evaluate the effectiveness of the proposed CoT frame-\nwork, we conducted a comprehensive comparison across\nmultiple LLMs under two distinct settings: standard zero-\nshot prompting, involving single-shot inference without ex-\nplicit reasoning steps, and CoT-enhanced reasoning, involv-\ning stepwise intermediate reasoning. Experimental results are\npresented in Table II and F1 scores across different mod-\nels are shown in Figure 3. Findings demonstrate significant\nperformance gains through CoT integration, particularly in\ndistinguishing binary classification tasks across different CDR\nlevels.\nThe CoT-based diagnostic system outperformed traditional\nprompting methods across most evaluation metrics, achieving\nsuperior F1 scores, accuracy, and AUC values. For instance,\nin the 0.5 vs. 1.0 classification task, the Qwen2-7B (CoT)\nmodel achieved an F1 score of 0.54 and accuracy of 0.61,\noutperforming the zero-shot version (F1 = 0.39, accuracy\n= 0.58) by +0.15 F1 score, demonstrating the effectiveness\nof explicit reasoning in fine-grained clinical differentiation.\nSimilarly, the Qwen3-4B (CoT) model showed substantial\nimprovement in fine-grained discrimination tasks like 0.5 vs.\n2 (F1 = 0.45, accuracy = 0.56). These results indicate that\nstructured reasoning effectively enhances models’ ability to\ndistinguish borderline dementia stages. The Qwen2-7B (CoT)\nmodel exhibits the most stable performance characteristics,\nmaintaining balanced precision and recall across all task pairs.\nNotably, both metrics exceed 0.57 in the 1 vs 3 classification\ntask. This stability indicates the model’s ability to handle\nbroader CDR variations while preserving decision reliability.\nIn contrast, the Qwen3-4B (CoT) model performs stronger\non moderate classification tasks (e.g., 0.5 vs 2) but shows\ndeclining performance on more distant class pairs (e.g., 1 vs\n3), suggesting that its smaller parameter size may limit its\ngeneralization ability across varying degrees of severity.\nIn contrast, the Phi-3B (CoT) model exhibits significant\nperformance fluctuations, revealing its limitations in complex\ndiagnostic reasoning tasks. Although the model achieves com-\npetitive recall rates in certain subtasks, its precision and AUC\nvalues fluctuate dramatically, indicating unstable decision\nboundaries and inconsistent reasoning chains. For instance,\nthe Phi-3B model achieved an F1 score of 0.56 in the 0.5 vs\n"}, {"page": 6, "text": "Fig. 2. A CoT AI Workflow for Clinical Dementia Rating (CDR) Assessment\n"}, {"page": 7, "text": "TABLE II\nPERFORMANCE OF COT-BASED LARGE LANGUAGE MODELS IN BINARY CLASSIFICATION TASKS\nCDR Group\nModels\nPrecision\nRecall\nF1-score\nAccuracy\nAUC\n0.5 vs. 1\nQWEN2-7B zero shot prompting\n0.55\n0.49\n0.39\n0.58\n0.51\nMicrosoft Phi-3B (CoT)\n0.23\n0.50\n0.32\n0.46\n0.50\nQWEN3-4B (CoT)\n0.43\n0.45\n0.42\n0.49\n0.45\nQWEN2-7B (CoT)\n0.60\n0.56\n0.54\n0.61\n0.56\n0.5 vs. 2\nQWEN2-7B zero shot prompting\n0.64\n0.53\n0.42\n0.54\n0.53\nMicrosoft Phi-3B (CoT)\n0.59\n0.58\n0.56\n0.59\n0.58\nQWEN3-4B (CoT)\n0.58\n0.53\n0.45\n0.56\n0.53\nQWEN2-7B (CoT)\n0.29\n0.45\n0.35\n0.54\n0.45\n0.5 vs. 3\nQWEN2-7B zero shot prompting\n0.62\n0.54\n0.41\n0.47\n0.54\nMicrosoft Phi-3B (CoT)\n1.00\n0.33\n0.50\n0.33\nNaN\nQWEN3-4B (CoT)\n0.72\n0.55\n0.40\n0.47\n0.55\nQWEN2-7B (CoT)\n0.54\n0.54\n0.53\n0.54\n0.54\n1 vs. 3\nQWEN2-7B zero shot prompting\n0.50\n0.50\n0.44\n0.44\n0.50\nMicrosoft Phi-3B (CoT)\n0.56\n0.56\n0.53\n0.53\n0.56\nQWEN3-4B (CoT)\n0.46\n0.47\n0.32\n0.32\n0.47\nQWEN2-7B (CoT)\n0.58\n0.57\n0.50\n0.50\n0.57\nFig. 3.\nF1 Score Improvement of CoT-Based Models over Zero-Shot\nPrompting\n2 task, yet its performance plummeted to 0.50 in the 0.5 vs\n3 task. These inconsistencies suggest that small-scale models\nmay struggle to maintain logical coherence across multi-step\nreasoning sequences, highlighting the need for sufficient model\ncapacity to ensure the stability of CoT-based reasoning in\nmedical diagnostic scenarios.\nAnalysis of cross-model F1 score trends reveals model-scale\neffects and algorithmic influences. Qwen2-7B (CoT) consis-\ntently maintains the highest and most stable F1 score per-\nformance across all CDR tasks, indicating that greater model\ncapacity enables construction of more coherent reasoning\nchains and enhances diagnostic discrimination capabilities. In\ncontrast, Qwen3-4B (CoT) shows moderate gains at moderate\ndiscrepancy levels (0.5 vs 2) but exhibits weaker stability on\ndistant category pairs (1 vs 3), indicating limited generalization\nin scenarios with severe cognitive discrepancies. Meanwhile,\nPhi-3B (CoT) exhibits fluctuating F1 scores, reflecting the\nsensitivity of smaller architectures to reasoning depth and\ntask complexity. These trends collectively demonstrate that pa-\nrameter scale and CoT prompt design synergistically enhance\nreasoning fidelity and stability. Notably, models exceeding 7\nbillion parameters appear to strike a critical balance between\nreasoning diversity and consistency, yielding more reliable\ndiagnostic outputs.\nSimultaneously, we observed the performance of multi-stage\nCoT frameworks on heterogeneous CDR classification tasks.\nAlthough absolute F1 scores fluctuated with task difficulty,\nCoT-enhanced models (particularly Qwen2-7B) avoided fail-\nure and maintained balanced precision-recall curves across all\nfour binary settings. In contrast, the smaller Phi-3B (CoT)\nmodel exhibited significant instability, indicating poor dis-\ncrimination capabilities under the most challenging scenarios.\nThese results demonstrate that multi-stage reasoning struc-\ntures, when combined with sufficient model capacity, can miti-\ngate extreme performance fluctuations and maintain consistent\ndecision boundaries across both adjacent and distant CDR\ncomparisons.\nCoT reasoning enhances diagnostic consistency and inter-\npretability. Unlike traditional LLM prompts relying solely on\nimplicit statistical correlations, CoT-based reasoning generates\ntransparent intermediate reasoning paths that mirror clini-\ncians’ cognitive processes during dementia assessments. Fur-\nthermore, the multi-stage in CoT reasoning mitigates output\ninstability by enforcing internal logical consistency between\nreasoning steps. In contrast, zero-shot prompts lacking CoT\nguidance often yield inconsistent and less reliable predictions\ndue to the absence of structured reasoning supervision.\nV. DISCUSSION\nClinical dementia assessments based on electronic health\nrecords rely on interpreting cognitive and functional behav-\nioral descriptions recorded in routine clinical practice. These\ndescriptions are often heterogeneous, implicit, and context-\ndependent, posing challenges for automated evaluation. In\nsuch scenarios, effective assessment depends not only on\npredictive accuracy but also on the stability of reasoning\n"}, {"page": 8, "text": "processes, the transparency of intermediate judgements, and\nthe ability to trace diagnostic conclusions back to observable\nclinical evidence. To address clinical behavioral assessment,\nthis study employs a structured, multi-stage CoT reasoning\nprocess to organize intermediate inferences and support con-\nsistent interpretation of clinical narratives. This design enables\nsystematic analysis of complex behavioral information through\nexplicit, auditable reasoning steps.\nExperimental results validate the effectiveness of integrating\nCoT reasoning into large language models for Alzheimer’s\ndisease assessment. Compared to existing diagnostic frame-\nworks based on CoT or reasoning, this system focuses on\nstructured electronic health record analysis rather than speech\nor synthetic datasets [24], providing evaluations with greater\nclinical evidence value. Furthermore, unlike general medical\nreasoning studies, this model is specifically optimized for\nAlzheimer’s disease staging and validated across diverse pa-\ntient datasets, demonstrating robustness and scalability. These\nfindings highlight the practical application potential of LLM-\nbased diagnostic systems for real-world dementia assessment\nwhile maintaining transparency and clinical interpretability.\nFurthermore, unlike studies on general medical reasoning, our\nmodel is specifically optimized for Alzheimer’s disease staging\nand validated across diverse patient datasets, demonstrating\nrobustness and scalability [3]. From a clinical perspective,\nthe enhancement of reasoning stability increases clinician\nconfidence, reduces diagnostic variability among raters, and\nthereby strengthens the interpretability and reliability of AI-\nassisted assessments. These findings highlight the practical\npotential of LLM-based diagnostic systems for real-world de-\nmentia evaluation while maintaining transparency and clinical\ninterpretability.\nThis study demonstrates that integrating CoT reasoning into\nlarge language models significantly enhances their diagnostic\nstability and interpretability in CDR assessments. Beyond\nnumerical performance gains, the CoT framework transforms\nconventional large language models into structured reasoning\nsystems capable of articulating clinical evidence, weighing\ndiagnostic clues, and integrating multi-perspective judgments.\nConsistent improvements observed across both Qwen2-7B and\nQwen3-4B models indicate that CoT integration offers benefits\nindependent of specific model architectures.\nThe CoT-based diagnostic system introduces a transparent\nreasoning mechanism aligned with clinical expert decision-\nmaking processes. By explicitly generating intermediate rea-\nsoning paths and employing multi-stage evaluation, this system\ntransforms the previously opaque prediction process into a\ntraceable, verifiable chain of reasoning, significantly enhancing\nthe model’s credibility in medical applications. Simultane-\nously, the multi-agent design effectively mitigates the ran-\ndomness inherent in large language models during reasoning,\nleading to more stable performance on complex borderline\ntasks.\nIt should be noted that while CoT reasoning incurs ad-\nditional computational overhead (due to multiple inference\nsteps), this trade-off is acceptable as the resulting gains in\ndiagnostic reliability and interpretability hold significant value\nfor clinical applications. This approach bridges automated\nprediction with explainable AI, providing a scalable techni-\ncal foundation for future deployment in real-world medical\nsettings.\nAlthough these findings are encouraging, the study has\nlimitations. The dataset size is relatively small, and the analysis\nrelies solely on text-based electronic health records (EHRs),\nexcluding imaging data such as magnetic resonance imag-\ning (MRI) or positron emission tomography (PET). Further-\nmore, external validation using independent cohorts is still\nrequired to confirm its generalizability. Future research will\nexplore multimodal integration (e.g., combining EHR and\nMRI features) and human-machine collaborative validation\nframeworks to enhance interpretability and clinical reliability.\nVI. CONCLUSION\nThis study proposes a CoT reasoning framework based\non large language models for the clinical assessment and\ndiagnosis of AD. Unlike traditional black-box fine-tuning\napproaches, this system explicitly simulates the clinical physi-\ncian’s reasoning process by generating intermediate diagnostic\ninferences and multi-layer evaluations. Experimental results\ndemonstrate that the proposed CoT diagnostic system not\nonly significantly enhances model interpretability and decision\ntransparency but also exhibits higher consistency and stability\nin distinguishing between adjacent CDR grades. Enhancing\nreasoning transparency not only improves interpretability at\nthe individual diagnostic level but also establishes a scalable\nframework for trustworthy clinical artificial intelligence. This\nadvancement facilitates the integration of interpretable large\nlanguage models into healthcare systems.\nBy transforming the model’s reasoning process into a\nstructured, traceable chain of reasoning, this study establishes\na novel connection between automated prediction and ex-\nplainable artificial intelligence, providing a scalable technical\nfoundation for large language models in clinical decision\nsupport systems.\nFuture research will further expand the CoT reasoning\nframework and integrate longitudinal EHR information to\nachieve multimodal diagnostic fusion. This will enable a more\ncomprehensive characterization of disease dynamics and ad-\nvance precision medicine applications in dementia diagnosis.\nAdditionally, external validation using multicenter datasets\nwill be conducted, alongside exploring the design of human-\nmachine collaborative reasoning systems, laying the ground-\nwork for practical deployment in clinical settings.\nREFERENCES\n[1] J.-E. Ding, A. Zilverstand, S. Yang, A. C.-C. Yang, and F. Liu, “Varia-\ntional mixture of graph neural experts for alzheimer’s disease biomarker\nrecognition in eeg brain networks,” arXiv preprint arXiv:2510.11917,\n2025.\n[2] S. S. Alia and P. Lago, “Daily routine recognition from longitudinal,\nreal-life wearable sensor data for the elderly,” in 2024 International\nConference on Activity and Behavior Computing (ABC).\nIEEE, 2024,\npp. 1–9.\n"}, {"page": 9, "text": "[3] R. Li, X. Wang, D. Berlowitz, J. Mez, H. Lin, and H. Yu, “Care-ad:\na multi-agent large language model framework for alzheimer’s disease\nprediction using longitudinal clinical notes,” npj Digital Medicine, vol. 8,\nno. 1, p. 541, 2025.\n[4] X. Du, Z. Zhou, Y. Wang, Y. Chuang, Y. Li, R. Yang, W. Zhang,\nX. Wang, X. Chen, H. Guan et al., “Testing and evaluation of gen-\nerative large language models in electronic health record applications:\nA systematic review.” Medrxiv: the Preprint Server for Health Sciences,\npp. 2024–08, 2025.\n[5] M. M. Lucas, J. Yang, J. K. Pomeroy, and C. C. Yang, “Reasoning\nwith large language models for medical question answering,” Journal\nof the American Medical Informatics Association, vol. 31, no. 9, pp.\n1964–1975, 2024.\n[6] M. Fikry, N. Mairittha, and S. Inoue, “Modelling reminder system for\ndementia by reinforcement learning,” in Sensor-and Video-Based Activity\nand Behavior Computing: Proceedings of 3rd International Conference\non Activity and Behavior Computing (ABC 2021).\nSpringer, 2022, pp.\n149–166.\n[7] J. Wang, J.-E. Ding, F. Liu, E. Kallioniemi, S. Wang, W.-X. Tsai, and\nA. C. Yang, “Flexible and explainable graph analysis for eeg-based\nalzheimer’s disease classification,” arXiv preprint arXiv:2504.01329,\n2025.\n[8] C.-T. Dao, N. M. T. Phan, J.-E. Ding, C. Wu, D. Restrepo, D. Luo,\nF. Zhao, C.-C. Liao, W.-C. Peng, C.-T. Wang et al., “Curenet: combining\nunified representations for efficient chronic disease prediction,” Health\nInformation Science and Systems, vol. 14, no. 1, p. 7, 2025.\n[9] S. Amini, B. Hao, J. Yang, C. Karjadi, V. B. Kolachalama, R. Au, and\nI. C. Paschalidis, “Prediction of alzheimer’s disease progression within\n6 years using speech: A novel approach leveraging language models,”\nAlzheimer’s & Dementia, vol. 20, no. 8, pp. 5262–5270, 2024.\n[10] R. C. Petersen, “Mild cognitive impairment,” New England Journal of\nMedicine, vol. 364, no. 23, pp. 2227–2234, 2011.\n[11] C. R. Jack Jr, D. A. Bennett, K. Blennow, M. C. Carrillo, B. Dunn,\nS. B. Haeberlein, D. M. Holtzman, W. Jagust, F. Jessen, J. Karlawish\net al., “Nia-aa research framework: toward a biological definition of\nalzheimer’s disease,” Alzheimer’s & dementia, vol. 14, no. 4, pp. 535–\n562, 2018.\n[12] J.\nWen,\nE.\nThibeau-Sutre,\nM.\nDiaz-Melo,\nJ.\nSamper-Gonz´alez,\nA. Routier, S. Bottani, D. Dormont, S. Durrleman, N. Burgos, O. Colliot\net al., “Convolutional neural networks for classification of alzheimer’s\ndisease: overview and reproducible evaluation,” Medical image analysis,\nvol. 63, p. 101694, 2020.\n[13] G. Battineni, N. Chintalapudi, and F. Amenta, “Machine learning driven\nby magnetic resonance imaging for the classification of alzheimer\ndisease progression: Systematic review and meta-analysis,” JMIR aging,\nvol. 7, p. e59370, 2024.\n[14] Z. A. Nazi and W. Peng, “Large language models in healthcare and\nmedical domain: A review,” in Informatics, vol. 11, no. 3.\nMDPI,\n2024, p. 57.\n[15] V. Li´evin, C. E. Hother, A. G. Motzfeldt, and O. Winther, “Can large\nlanguage models reason about medical questions?” Patterns, vol. 5,\nno. 3, 2024.\n[16] H. Zhang, N. Jethani, S. Jones, N. Genes, V. J. Major, I. S. Jaffe, A. B.\nCardillo, N. Heilenbach, N. F. Ali, L. J. Bonanni et al., “Evaluating\nlarge language models in extracting cognitive exam dates and scores,”\nPLOS Digital Health, vol. 3, no. 12, p. e0000685, 2024.\n[17] K. Huang, J. Altosaar, and R. Ranganath, “Clinicalbert: Modeling\nclinical notes and predicting hospital readmission,” arXiv preprint\narXiv:1904.05342, 2019.\n[18] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language\nmodels encode clinical knowledge,” Nature, vol. 620, no. 7972, pp. 172–\n180, 2023.\n[19] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, M. Amin,\nL. Hou, K. Clark, S. R. Pfohl, H. Cole-Lewis et al., “Toward expert-\nlevel medical question answering with large language models,” Nature\nMedicine, vol. 31, no. 3, pp. 943–950, 2025.\n[20] E. Goh, R. Gallo, J. Hom, E. Strong, Y. Weng, H. Kerman, J. A. Cool,\nZ. Kanjee, A. S. Parsons, N. Ahuja et al., “Large language model\ninfluence on diagnostic reasoning: a randomized clinical trial,” JAMA\nnetwork open, vol. 7, no. 10, pp. e2 440 969–e2 440 969, 2024.\n[21] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” Advances in neural information processing systems,\nvol. 35, pp. 24 824–24 837, 2022.\n[22] J. Miao, C. Thongprayoon, S. Suppadungsuk, P. Krisanapan, Y. Rad-\nhakrishnan, and W. Cheungpasitporn, “Chain of thought utilization in\nlarge language models and application in nephrology,” Medicina, vol. 60,\nno. 1, p. 148, 2024.\n[23] Y. Sonoda, R. Kurokawa, A. Hagiwara, Y. Asari, T. Fukushima, J. Kan-\nzawa, W. Gonoi, and O. Abe, “Structured clinical reasoning prompt\nenhances llm’s diagnostic capabilities in diagnosis please quiz cases,”\nJapanese Journal of Radiology, vol. 43, no. 4, pp. 586–592, 2025.\n[24] C. Park, A. S. G. Choi, S. Cho, and C. Kim, “Reasoning-based approach\nwith chain-of-thought for alzheimer’s detection using speech and large\nlanguage models,” arXiv preprint arXiv:2506.01683, 2025.\n[25] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdh-\nery, and D. Zhou, “Self-consistency improves chain of thought reasoning\nin language models,” arXiv preprint arXiv:2203.11171, 2022.\n[26] T. Savage, A. Nayak, R. Gallo, E. Rangan, and J. H. Chen, “Diagnostic\nreasoning prompts reveal the potential for large language model inter-\npretability in medicine,” NPJ Digital Medicine, vol. 7, no. 1, p. 20,\n2024.\n"}]}