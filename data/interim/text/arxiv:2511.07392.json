{"doc_id": "arxiv:2511.07392", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.07392.pdf", "meta": {"doc_id": "arxiv:2511.07392", "source": "arxiv", "arxiv_id": "2511.07392", "title": "Voice-Interactive Surgical Agent for Multimodal Patient Data Control", "authors": ["Hyeryun Park", "Byung Mo Gu", "Jun Hee Lee", "Byeong Hyeon Choi", "Sekeun Kim", "Hyun Koo Kim", "Kyungsang Kim"], "published": "2025-11-10T18:47:24Z", "updated": "2025-12-17T21:32:33Z", "summary": "In robotic surgery, surgeons fully engage their hands and visual attention in procedures, making it difficult to access and manipulate multimodal patient data without interrupting the workflow. To overcome this problem, we propose a Voice-Interactive Surgical Agent (VISA) built on a hierarchical multi-agent framework consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to interpret voice commands and execute tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models within surgical video. We construct a dataset of 240 user commands organized into hierarchical categories and introduce the Multi-level Orchestration Evaluation Metric (MOEM) that evaluates the performance and robustness at both the command and category levels. Experimental results demonstrate that VISA achieves high stage-level accuracy and workflow-level success rates, while also enhancing its robustness by correcting transcription errors, resolving linguistic ambiguity, and interpreting diverse free-form expressions. These findings highlight the strong potential of VISA to support robotic surgery and its scalability for integrating new functions and agents.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.07392v3", "url_pdf": "https://arxiv.org/pdf/2511.07392.pdf", "meta_path": "data/raw/arxiv/meta/2511.07392.json", "sha256": "9742073aa922d75905a2302a3fea45a96c7988b51fd9383e801a434c61402cfe", "status": "ok", "fetched_at": "2026-02-18T02:27:24.257582+00:00"}, "pages": [{"page": 1, "text": "1\nVoice-Interactive Surgical Agent for\nMultimodal Patient Data Control\nHyeryun Park, Byung Mo Gu, Jun Hee Lee, Byeong Hyeon Choi, Sekeun Kim, Hyun Koo Kim, and\nKyungsang Kim\nAbstract— In robotic surgery, surgeons fully engage\ntheir hands and visual attention in procedures, making it\ndifficult to access and manipulate multimodal patient data\nwithout interrupting the workflow. To overcome this prob-\nlem, we propose a Voice-Interactive Surgical Agent (VISA)\nbuilt on a hierarchical multi-agent framework consisting\nof an orchestration agent and three task-specific agents\ndriven by Large Language Models (LLMs). These LLM-\nbased agents autonomously plan, refine, validate, and rea-\nson to interpret voice commands and execute tasks such\nas retrieving clinical information, manipulating CT scans,\nor navigating 3D anatomical models within surgical video.\nWe construct a dataset of 240 user commands organized\ninto hierarchical categories and introduce the Multi-level\nOrchestration Evaluation Metric (MOEM) that evaluates the\nperformance and robustness at both the command and cat-\negory levels. Experimental results demonstrate that VISA\nachieves high stage-level accuracy and workflow-level suc-\ncess rates, while also enhancing its robustness by correct-\ning transcription errors, resolving linguistic ambiguity, and\ninterpreting diverse free-form expressions. These findings\nhighlight the strong potential of VISA to support robotic\nsurgery and its scalability for integrating new functions and\nagents.\nIndex Terms— Hierarchical Multi-agent System, Large\nLanguage Models, Robotic Surgery, Voice Interaction\nThis research was supported by a grant of the Korea Health Tech-\nnology R&D Project through the Korea Health Industry Development\nInstitute (KHIDI), funded by the Ministry of Health & Welfare, Republic\nof Korea (grant number: RS-2024-00436472). Additionally, this work\nwas supported by the National Research Foundation of Korea (NRF)\ngrant, funded by the Korea government (MSIT) (grant number: RS-2025-\n00518091). (Corresponding authors: Hyun Koo Kim; Kyungsang Kim.)\nAll authors declare no conflict of interest.\nH. Park is with the Image Guided Precision Cancer Surgery Institute,\nCollege of Medicine, Korea University, Seoul, Republic of Korea, and\nalso with the Department of Radiology, Massachusetts General Hospi-\ntal, Boston, Massachusetts, USA (e-mail: hyerpark1115@gmail.com).\nB. M. Gu and J. H. Lee are with the Image Guided Precision\nCancer Surgery Institute, College of Medicine, Korea University, Seoul,\nRepublic of Korea, and the Department of Thoracic and Cardiovascular\nSurgery, Korea University Guro Hospital, College of Medicine, Korea\nUniversity, Seoul, Republic of Korea (e-mail: luvotomy7@naver.com;\nlee2632@naver.com).\nB. H. Choi and H. K. Kim are with the Image Guided Precision Cancer\nSurgery Institute, College of Medicine, Korea University, Seoul, Republic\nof Korea, and the Department of Thoracic and Cardiovascular Surgery,\nKorea University Guro Hospital, College of Medicine, Korea University,\nSeoul, Republic of Korea, and the Department of Biomedical Sciences,\nCollege of Medicine, Korea University, Seoul, Republic of Korea (e-mail:\nbaby2music@gmail.com; kimhyunkoo@korea.ac.kr).\nS.\nKim\nand\nK.\nKim\nare\nwith\nthe\nDepartment\nof\nRadiol-\nogy, Massachusetts General Hospital and Harvard Medical School,\nBoston,\nMassachusetts,\nUSA\n(e-mail:\nskim207@mgh.harvard.edu;\nkkim24@mgb.org).\nI. INTRODUCTION\nT\nHE primary surgeon must maintain concentration, with\nboth hands and eyes fully engaged throughout robotic\nsurgery. However, accessing and manipulating patient data dur-\ning the procedure requires shifting attention away from the sur-\ngeon console to another monitor or auxiliary interfaces. These\ninterruptions become more significant when surgeons need to\ncheck diverse multimodal data such as clinical information, CT\nimages, MRI images, and 3D models, underscoring the need\nfor a dynamic interaction system. Early interaction studies on\nvoice-controlled robotic surgery mainly focused on camera\nand equipment manipulation using systems such as AESOP\n[1]–[4], the HERMES operating room control center [5], and\nPARAMIS [6]. These systems reduced operation time and\nthe need for a second assistant [4], while improving surgical\nefficiency and decreasing communication burden [5]. Subse-\nquent studies broadened the scope of tasks, including image\nretrieval and 3D distance measurement [7], real-time camera\ncontrol [8], [9], and tissue manipulation with robotic assistant\narms [10]. Recent studies have progressed toward interpret-\ning natural language commands utilizing Large Language\nModels (LLMs). A ChatGPT-integrated system mapped free-\nform user speech to the most relevant predefined commands\n[11]. LLM-based mixed reality systems further enable natural\nlanguage–driven robot operation [12] and real-time surgical\nassessment and procedural guidance [13].\nLLMs exhibit advanced capabilities for understanding and\ngenerating natural language and such models include GPT-4\n[14], PaLM [15], LLaMA [16]–[19], and Gemma [20]–[22].\nThese models can interpret natural instructions and perform\nmulti-step problem solving through advanced prompting and\nreasoning techniques, including Chain-of-Thought (CoT) [23],\nSelf-Consistency CoT [24], Tree-of-Thought [25], and Graph-\nof-Thought [26]. These advancements in reasoning have trans-\nformed LLMs from language generators into autonomous\nagents that can perceive their environment, plan to achieve\nspecific goals, and act accordingly [27]–[29]. Various frame-\nworks such as ReAct [30], AutoGPT, BabyAGI, AutoGen [31],\nand CAMEL [32] have been proposed to integrate reasoning\nand action. Recent research further applies reinforcement\nlearning to help LLMs learn from past reasoning failures and\noptimize their decision-making [33]. These diverse reasoning\nand interaction methods have become the driving force of\nLLM-based autonomous agents capable of planning, tool use,\nand collaborative problem solving.\narXiv:2511.07392v3  [cs.CL]  17 Dec 2025\n"}, {"page": 2, "text": "2\nFig. 1.\nOverall framework of the proposed VISA: (a) Workflow orchestrator agent that autonomously plans the order of function execution; (b)\nWorkflow functions that capture, transcribe, correct, and route voice commands; (c) Task-specific agent functions that control patient data on the\ntarget video clip; (d) Memory state with local memory for each clip and a global memory for shared contextual understanding across clips.\nThe LLM-based agent typically consists of five key mod-\nules: a profile module that defines objectives and constraints,\na memory module that stores past states, a reasoning and\nplanning module that formulates strategies, an action module\nthat executes tasks via models or tools, and a feedback module\nthat evaluates results and refines behavior [34]. These agents\ncan operate as a single-agent system using one LLM or\nas a multi-agent system (MAS) with specialized agents for\ndistinct tasks, offering advantages in scalability, efficiency, and\ncoordinated workflows [35], [36]. The MAS coordination is\nenabled by LLM-based orchestration, where an LLM functions\nas the cognitive core of the system to manage interactions\namong agents, plan execution sequences, interpret intermediate\nresults, and control the overall workflow [36], [37], [37]–\n[41]. Such systems may adopt centralized, decentralized, or\nhierarchical structures and exhibit cooperative, competitive,\nor negotiated behaviors [36], [37]. To implement practical\norchestration, various agent frameworks have emerged such\nas LangGraph [42], AutoGen [31], LlamaIndex [43], CrewAI,\nand AgentKit.\nIn this work, we highlight the importance of effective sur-\ngical tool interaction during operation, with voice interaction\noffering a promising approach. Based on this motivation, we\nmodularize key tool functionalities into specialized agents\nand coordinate them through an orchestrator. Specifically,\nwe introduce a Voice-Interactive Surgical Agent (VISA) that\ninterprets free-form voice commands and enables context-\naware multimodal data interaction directly on surgical videos.\nTo our knowledge, this is the first hierarchical multi-agent\nframework, consisting of a workflow orchestrator agent and\nmultiple task-specific agents as demonstrated in Fig.1.\nIn contrast to prior studies that rely on predefined commands\nor a single LLM for direct voice-to-function mapping, our\nVISA autonomously plans and executes functions through in-\ntermediate steps, providing more flexible handling of complex\nvoice commands. The workflow orchestrator agent uses an\nLLM to perform probabilistic decision-making across work-\nflow functions and task-specific agent functions, predicting\nthe next function to execute. The workflow functions capture\naudio, perform Speech-to-Text (STT), correct and validate\ncommands, and select the appropriate task agent. The Infor-\nmation Retrieval (IR) agent retrieves clinical information, the\nImage Viewer (IV) agent slides through CT scans, and the\nAnatomy Rendering (AR) agent manipulates 3D anatomical\nmodels. Each task-specific agent includes an action determi-\nnation function that leverages an LLM to infer the appropriate\naction and its parameters. This hierarchical design enables\nscalable integration of new functions or agents. VISA also\nincorporates a memory state that stores previous commands,\nagents, and various parameters for continuous task execution\nand adaptive responses to ambiguous commands.\nFor evaluation, we construct a command dataset of 240 user\ncommands, each annotated with three hierarchical categories\nof command structure, type, and expression. We also introduce\na Multi-level Orchestration Evaluation Metric (MOEM) to\nassess stage-level accuracy and workflow-level success rates.\nExperimental results demonstrate that VISA achieves high\nperformance and strong robustness to linguistic variability,\nspeech recognition errors, and ambiguous commands through\nLLM-based command correction and reasoning. Further anal-\nyses considering real surgical settings and comparisons with\nsynthesized speech underscore the strong potential of VISA.\n"}, {"page": 3, "text": "3\nFig. 2. Overall workflow planned by the workflow orchestrator agent with a hybrid approach of LLM and mathematical rules.\nII. METHODS\nA. Overall Workflow\nOur VISA framework is a hierarchical MAS that automat-\nically plans and executes the overall workflow as shown in\nFig. 2. We divide the surgical video into 10-second clips,\nwith each clip designed to handle a single command. The\nworkflow orchestrator agent initializes the memory state for\nthe target video clip and uses the LLM to select the next\nfunction to execute. The LLM prompt includes instructions,\nfunction names with definitions, predifined workflow decision\nrules, the current status, and the output format.\nIn determining return points or termination, relying solely\non function names and descriptions can lead to mispredictions.\nTo mitigate this, workflow decision rules guide the overall\nprocess and define when to return and end. In addition, current\nstatus such as “No audio recorded”, “Last command invalid,\nneed new input”, and “Agent completed, workflow finished”\nprovide contextual cues that help navigate branching points.\nThe workflow decision rules and current status allow the\norchestrator to make more accurate and consistent decisions\nin various scenarios.\nThe LLM output should be in json format, containing each\nfunction name along with its associated probability for the\nnext step. Incorporating probabilities allows the system to\nprovide a quantitative measure of confidence for decision-\nmaking. Since the LLM output may omit certain functions, the\nmissing function completion rule includes all absent functions\nM with their probabilities set to zero. Subsequently, the\nfunction selection rule chooses the next function fnext with\nthe highest probability over the complete function set. This\nhybrid approach combines the capabilities of LLMs with\nthe probability-based decision rules, ensuring reliability and\nmathematical grounding.\nB. Workflow Functions\n1) Real-time Audio Function: This function handles voice\ninteraction by capturing audio on the edge device and trans-\nmitting it to a remote server. For fast real-time processing on\nCPU, it integrates Silero-VAD1 for voice activity detection\n(VAD), Whisper-small model [44] via faster-whisper2 for\nspeech recognition and wake-word detection, and Microsoft\nEdge TTS3 for text-to-speech (TTS). The process begins\nwith microphone streaming, where VAD detects speech ac-\ntivity, Whisper transcribes the audio to identify the wake-\nword ”davinci”, records the command until four seconds of\nsilence, and sends it to the remote server. Wake-word detection\nis further improved by optimizing Whisper-small decoding\nparameters, setting “davinci” as a hotword, the temperature to\n0, beam size to 5, no-speech threshold to 0.25, compression\nratio threshold to 1.2, and log probability-threshold to −1.0.\n2) Speech-to-Text Function: The STT function employs the\nWhisper-medium model [44] via faster-whisper for efficient\ninference. Among the SpeechBrain model [45], the Whis-\nper models, and Microsoft Phi-4-multimodal model [46], the\nWhisper-medium model offers a practical balance between\nspeed and accuracy. Testing on 35 randomly selected com-\nmands, larger models generally achieve higher accuracy but\nincur slower inference. The Whisper-medium model is con-\nfigured with a beam size of 8, the language set to “en”,\na temperature of 0, a speech threshold of 0.3, and a log\nprob threshold of −2.0 to enhance transcription accuracy\nfor English speech while effectively filtering out non-speech\nsegments and low-confidence outputs. Lastly, the transcribed\ncommand is updated in the memory state.\n1https://github.com/snakers4/silero-vad\n2https://github.com/SYSTRAN/faster-whisper\n3https://github.com/rany2/edge-tts\n"}, {"page": 4, "text": "4\n3) Correct and Validate Function: This function employs\nGemma3 [22] LLM to correct the transcribed command and\ndetermine its validity. The prompt4 includes the available task\nagents with their descriptions, instructions, the current tran-\nscribed command, correction rules, validation rules, the output\nformat, and the global memory state. Since the STT model is\nunfamiliar with medical terminology, we provide correction\nrules such as replacing “city” with “CT” or COVID-related\nterms with “coronal”. Ambiguity also arises with commands\nlike “zoom in/out”, which may refer either to the IV agent or\nthe AR agent. To resolve these problems, we provide the last\nthree revised commands and associated agents from the global\nmemory state. If the command is “None”, the LLM revises it\nto “Select {agent name}” by inheriting the agent from the\nprevious memory state. The LLM outputs a json containing\nthe corrected command and its validity, which is updated in\nthe memory state. Valid commands proceed to the command\nreasoning function, while invalid ones return to the real-time\naudio function for a new voice input.\n4) Command Reasoning Function: This function takes the\nrevised command as input and employs Gemma3 model to\nreason and select the most appropriate task-specific agent.\nThe prompt4 includes the available task agents with brief\ndescriptions, instructions, the current revised command, and\nglobal memory state. Providing the last three revised com-\nmands and their corresponding agents from the global memory\nstate helps the model resolve ambiguous commands. We\nalso apply Chain-of-Thought (CoT) prompting [23] to the\nreasoning process to analyze the cause of failures and use\nthese insights to refine agent descriptions. The model finally\nreturns a json specifying the name of the selected agent, which\nis also updated in the memory state.\nC. Task-specific Agent Functions\n1) IR Agent Functions: The IR agent overlays the requested\nclinical data on the surgical video and consists of three\nfunctions: an action determiner (AD) function that interprets\ncommands and decides the action and parameters, and two\nfunctions for showing or removing the overlay as shown in\nFig. 3. The AD function leverages Gemma3 to infer the action\nuniquely mapped to an agent function and state parameters4.\nWe denote the command by x ∈X, the instruction by i,\nand a set of patient data columns by C = {c1, . . . , cm}\nwith corresponding values v = {v1, . . . , vm}. The LLM\nparameterized by θ produces an action distribution πθ over\nA = {SHOW, HIDE} as in (1) and a per-field inclusion\nprobability ϕθ as in (2):\nπθ(a | x, i) = Pr(A = a | x, i),\n(1)\nϕθ(j | x, i, a) = Pr(Yj = 1 | x, i, a),\nj = 1, . . . , m.\n(2)\nThe agent selects the action a⋆with the higher probability and\npredicts the indicator vector y⋆∈{0, 1}m that specifies which\nfields to display, as in (4). To handle ambiguous expressions\nlike physical information, the prompt guides their mapping to\n4Prompts available at https://github.com/helena-lena/SAOP\nspecific fields in C, adjusting the ϕθ.\na⋆= arg\nmax\na∈{SHOW,HIDE} πθ(a | x, i),\n(3)\ny⋆\nj = 1[ϕθ(j | x, i, a⋆) ≥τ] ,\nj = 1, . . . , m.\n(4)\nThe state parameters of the IR agent are defined as:\nΨ = (y, s),\ny ∈{0, 1}m,\ns ∈final string.\n(5)\nHere, y denotes the binary indicator vector specifying which\ndata fields are active, and s is the final patient information\nstring to overlay. When a⋆= HIDE, the agent sets y⋆= 0\nand assigns an empty string s. When a⋆= SHOW, the agent\nretrieves the relevant clinical data and formats it according\nto physician preferences to construct s. Given the indicator\nvector y⋆, the subset of columns to display is:\nCy⋆= { cj ∈C | y⋆\nj = 1 }.\n(6)\nFor each selected column cj ∈Cy⋆with value vj, a field-\nspecific formatting function r(cj, vj) transforms the raw data\ninto a textual representation as in (7).\nr(cj, vj) : (cj, vj) 7→string,\ncj ∈Cy⋆.\n(7)\nThis formatting function reflects physician feedback, such\nas combining pulmonary function test values in liters and\npercentages, merging relevant fields like sex and age, and\nomitting unnecessary units. The final patient information string\nis the result of concatenating all the formatted outputs with\nnewline delimiters as in (8).\ns = concatcj∈Cy⋆r(cj, vj), delimiter = “\\n”.\n(8)\nFinally, the SHOW action links to a display function that\noverlays the generated text on the top-right corner of each\nvideo frame. In contrast, the HIDE action connects to a\nremoval function that restores the original frame without any\noverlay. Given a sequence of video frames {It}T\nt=1 and an\noverlay box position p, the overlay operator OIR places the\npatient information string s on each frame as follows:\n˜It = OIR(It; s, p),\nt = 1, . . . , T.\n(9)\nThe selection of data columns and the visual presentation style\nis refined through continuous consultation with clinicians to\nensure readability.\nFig. 3.\nThe overview of the IR agent, which executes either the show\nor remove function with inferred state parameters.\n"}, {"page": 5, "text": "5\nFig. 4. The overview of the IV agent, which executes show, zoom in, zoom out, or remove function with inferred state parameters.\n2) IV Agent Functions: The IV agent overlays CT DICOM\nimages onto the surgical video, allowing surgeons to scroll\nthrough axial, coronal, and sagittal planes as shown in Fig. 4.\nThe AD function interprets commands and selects one of four\nactions: displaying and moving multi-planar views, zooming\ninto a large single view, zooming out to the small multi-planar\nviews, and removing all CT views4. Given the command x ∈\nX and the instructions i, the Gemma3 model parameterized\nby θ infers the action a ∈A and updates the state parameters\nΨ following the joint policy:\nπθ(a, Ψ | x, i) = πθ(a | x, i) πθ(Ψ | x, i, a),\n(10)\na⋆= arg max\na∈A πθ(a | x, i),\n(11)\nΨ⋆= arg max\nΨ\nπθ(Ψ | x, i, a⋆).\n(12)\nThe policy first infers the action a⋆, and then predicts the\nupdated state parameters Ψ⋆conditioned on that action. The\nΨ denotes the current state and Ψ⋆is the updated state.\nSpecifically, the state parameters are defined as Ψ\n=\n(p, δ, v), where p = (paxi, pcor, psag) is the slice positions,\nδ ∈{none, small views, zoom view} is the display mode, and\nv ∈{axial, coronal, sagittal} is the main view plane. The slice\npositions are updated based on movement commands:\np⋆= p + ∆p(x, i, a⋆),\n(13)\nwhere ∆p encodes positional adjustments such as “move\nright” or “sagittal plus 50” (increase sagittal), “move down”\nor “axial minus 100” (decrease axial), and “move forward”\nor “coronal plus 30” (increase coronal). The display mode\nδ determines how CT slices appear on the surgical video.\nIf a⋆is SHOW MOVE, δ is small views, showing three CT\nviews Iaxi\nt\n, Icor\nt\n, Isag\nt\non the right side of the video, each with\nthe updated positions p⋆. If a⋆is ZOOM IN MOVE, δ is\nzoom view, displaying a main CT view Imain\nt\nin the center\nof the video. The main view v is either explicitly specified in\nthe command or implicitly inferred from the movement axis:\nImain\nt\n= V(p⋆, v). If a⋆= ZOOM OUT, δ is small views,\nremoving the large main CT view. If a⋆= REMOVE, δ is\nnone, which removes all CT views.\nGiven video frames {It}T\nt=1, action a⋆, and the updated\nstate Ψ⋆, the overlay operator generates\n˜It = OIV (It; p⋆, a⋆, δ, v),\nt = 1, . . . , T.\n(14)\nTo ensure smooth transitions, OIV interpolates slice positions\nfrom p to p⋆over five seconds.\n3) AR Agent Functions: The AR agent renders 3D anatomi-\ncal models onto the surgical video for interactive visualization\nas shown in Fig. 5. The 3D models are reconstructed from\nCT images using TotalSegmentator5 in the 3D Slicer6, which\ngenerates anatomical structures with corresponding labels. The\nAD function interprets commands and selects one of five\nactions: displaying static views, rotating the model, zooming in\non specific structures, zooming out, and removing all models4.\nThe Gemma3 LLM jointly infers the action a and updates the\nstate parameters Ψ following the same joint policy formulation\nas (12) in the IV agent. The difference is that a, Ψ, and i are\nall specific to the AR Agent. The state parameters Ψ include\nactive anatomical structures α, viewpoint v, rotation mode r,\ntarget structure for zoom τ, and zoom parameters z.\nThe active structures α represent the list of visible anatom-\nical labels. The physician selects the labels of interest, in-\ncluding left lower lobe, left upper lobe, right lower lobe,\nright middle lobe, right upper lobe, lung nodules, and the\ntrachea/bronchia. The default setting activates the lobe con-\ntaining the nodules together with all non-lobe structures, and\nsubsequent commands add or remove structures accordingly.\nThe viewpoint v ∈{anterior, posterior, left, right, superior,\ninferior, surgical} defines the initial view of the 3D model.\nThe default viewpoint is set to surgical view, which aligns to\nthe perspective of a patient lying on the operating table as\nshown in Figure 5. For example, commands like “Show the\nanterior view” or “Show from the front” display the anterior\nside, while rotation commands such as “Rotate the posterior\nview to the left” set the v to the posterior view.\n5https://github.com/wasserth/TotalSegmentator\n6https://www.slicer.org/\n"}, {"page": 6, "text": "6\nFig. 5.\nThe overview of the AR agent. The action determination function predicts the action and state parameters, then the AR agent executes\nstatic, zoom in, rotate, zoom out, or remove function with predicted parameters.\nThe rotation mode r\n∈{static, left, right, up, down,\nhorizontal, vertical} is the relative rotation direction from the\ninitial viewpoint v, with the static mode used by default.\nThe static mode leaves the model fixed, the left/right/up/down\nmodes rotate by 30 degrees toward the specified direction and\nreturn, and the horizontal/vertical modes perform a continuous\n360 degrees rotation.\nIf a⋆is zoom in/out, the LLM extracts the target structure\nτ from the command and updates the zoom parameters z⋆=\n(z⋆\nc, z⋆\ns, z⋆\nℓ), where z⋆\nc is the center of τ, z⋆\ns is the zoom scale,\nand z⋆\nℓis the zoom depth level. The zoom in moves toward\nz⋆\nc and stores the zoom path in a history stack, which is later\ntraversed in reverse order during the zoom out. The zoom scale\nstarts at zs = 1.0, doubles when zooming in, and halves when\nzooming out until reaching the minimum scale of 1.0, while\nthe zoom depth level increases or decreases accordingly.\nGiven video frames {It}T\nt=1, action a, and updated state Ψ⋆,\nthe overlay operator OAR generates\n˜It = OAR(It; Ψ⋆, a⋆),\nt = 1, . . . , T.\n(15)\nOAR smoothly interpolates the rotation and zoom transitions,\nwith zooms completing in three seconds, left/right/up/down\nrotations in seven seconds (three seconds movement, one\nsecond hold, three seconds return), and full horizontal/vertical\nrotations in six seconds. The resulting overlays appear in the\nupper-right corner of the surgical video, allowing the surgeon\nto identify critical anatomical structures during the procedures.\nD. Dataset\nWe construct a dataset of 240 user commands7: 44 for the\nIR agent, 81 for the IV agent, and 115 for the AR agent.\n7Data and code on https://github.com/helena-lena/SAOP\nThis distribution reflects the differing functional loads of the\nagents, with the AR agent accounting for nearly half of all\ncommands due to its broader range of actions and parameters.\nEach command also includes category annotations along three\nhierarchical dimensions: command structure, command type,\nand command expression, as summarized in Table I.\nThe command structure dimension categorizes each com-\nmand as single or composite, indicating whether it requires\ndecomposition into sequential sub-commands. The command\ntype dimension classifies commands into explicit, implicit, and\nNatural Language Question (NLQ) forms, capturing different\nlevels of linguistic clarity and ambiguity. Finally, the com-\nmand expression dimension groups commands into baseline,\nabbreviation, and paraphrase to reflect lexical and phrasing\nvariability, as clinicians express the same intent using different\nterminology or styles.\nWe additionally synthesize voice command datasets7 for\nfour different speakers using Microsoft Edge TTS that we use\nin real-time speech function. Among the available voices, we\nselect two female and two male speakers categorized under\n“News”, as they are native English speakers and provide clear\npronunciation. Specifically, en-US-AriaNeural conveys polite\nand confident tones, en-US-JennyNeural sounds friendly, con-\nsiderate, and comforting, en-US-GuyNeural expresses passion,\nand en-US-ChristopherNeural reflects reliability and authority.\nPatient data were obtained from the Department of Thoracic\nand Cardiovascular Surgery at Korea University Guro Hospital\nand include surgical videos, clinical information, CT images,\nand 3D anatomical models from a patient who underwent\nlung surgery using the da Vinci robotic system. This study\nwas ethically approved by the Institutional Review Board at\nKorea University Guro Hospital (IRB No. 2025GR0018), and\ninformed consent was waived due to its retrospective design.\n"}, {"page": 7, "text": "7\nTABLE I\nHIERARCHICAL COMMAND CATEGORY DEFINITIONS AND DISTRIBUTIONS.\nDimension\nCategory\nDescription and Examples\nCount\nCommand\nStructure\nSingle\nDefinition: Contains one clearly defined action or data request.\n225\nExample: “Show patient information.”, “Display CT images.”, “Remove models.”\nComposite\nDefinition: Combines two or more actions or data requests.\n15\nExample: “Move axial to the middle slice and zoom in.”, “Show physical and PFT info.”\nCommand\nType\nExplicit\nDefinition: States both subject (data/agent) and verb (action), and does not require contextual reasoning.\n80\nExample: “Show the anterior view.”, “Add the right upper lobe.”\nImplicit\nDefinition: Omits either the subject or the verb, making it dependent on the previous memory context.\n80\nExample: “Zoom in.”, “Initialize.”, “Surgical view.”, “Diagnosis.”, “Rotate to the right.”\nNLQ\nDefinition: Uses conversational or question-style phrasing.\n80\nExample: “How old is the patient?”, “Can you activate the right lung?”\nCommand\nExpression\nBaseline\nDefinition: Expressions without any abbreviations or paraphrasing, matching phrasing defined in the prompt.\n145\nExample: “Display the pulmonary function test.”, “Coronal plus 256.”, “Show from the anterior.”\nAbbreviation\nDefinition: Includes abbreviated forms used in both medical terminology and general expressions.\n15\nExample: “Display the PFT info.”, “Remove RUL.”, “Show patient info.”\nParaphrase\nDefinition: Expresses the same intent as baseline but uses alternative synonyms or phrasing styles.\n80\nExample: “Move front, front, front.”, “Move coronal to the middle slice.”, “Show from the front.”\nIII. EVALUATION METRICS\nFor each command, we evaluate the outcome of each\nworkflow stage using a binary scheme (1: correct, 0: incorrect),\nas shown in Table II. STT checks whether the transcribed\ncommand, command correction (CC) evaluates the revised\ncommand, and command reasoning (CR) determines whether\nthe LLM assigns the command to the appropriate agent. Agent\nfunction (AF) and agent parameters (AP) are outputs of AD\nfunction, which is correct only if both AF and AP are accu-\nrate. Orchestration flow (OF) assesses whether the workflow\nproceeds in the correct order, while invalid cycle (IC) is the\nnumber of times the workflow enters an invalid loop. Finally,\nwe introduce the Multi-level Orchestration Evaluation Metric\n(MOEM), which comprehensively evaluates VISA from two\nperspectives: command-level orchestration performance and\ncommand-category performance.\nThe command-level orchestration evaluation measures two\naspects: the stage-level accuracy that measures correctness at\neach workflow stage, and the workflow-level success rate (SR)\nthat quantifies overall success throughout the entire workflow.\nThe stage-level accuracy is the mean of binary outcomes\nacross all commands for each stage:\nAccuracystage = 1\nN\nN\nX\ni=1\noi,stage,\noi,stage ∈{0, 1}\n(16)\nwhere N denotes the total number of commands, i indicates\nthe command index, and oi,stage is the binary result of the\ni-th command at a stage. The workflow-level SR evaluates\neach command under three conditions: strict, single-pass, and\nmulti-pass as shown in Fig. 6. For each command i, a binary\nlabel oi,condition ∈{0, 1} indicates whether it satisfies the\ngiven success condition, and the SRcondition is the average of\nthese labels over all N commands:\nSRcondition = 1\nN\nN\nX\ni=1\noi,condition.\n(17)\nThe command-category evaluation examines how structural\nand linguistic variations affect orchestration performance and\nidentifies which categories are more prone to errors. For each\ncategory c, SRmulti−pass is the mean of binary outcomes:\nSRmulti−pass(c) = 1\nNc\nX\ni∈c\noi,multi-pass,\n(18)\nwhere Nc is the number of commands in category c. We also\ncalculate the cross-category SRmulti−pass for structure-type\nand type-expression pairs to explore how it jointly affects the\nperformance. For each pair of categories (c1, c2), the cross-\ncategory SRmulti−pass is:\nSRmulti-pass(c1, c2) =\n1\nNc1,c2\nX\ni∈(c1,c2)\noi,multi-pass,\n(19)\nwhere Nc1,c2 is the number of commands in the pair.\nFig. 6.\nSuccess conditions for workflow-level SR, where green boxes\nrepresent success and red boxes denote success or fail: (a) In strict\ncondition, the command is successful only if all stages succeed with\nno invalid loops; (b) In single-pass condition, it is also successful when\nintermediate failures are resolved in later stages, leading to a correct\nfinal outcome in a single attempt with no invalid loop; (c) In multi-pass\ncondition, it is also successful when the final outcome is correct after\nmultiple attempts, allowing up to three invalid loops.\n"}, {"page": 8, "text": "8\nTABLE II\nEXAMPLE OF VISA WORKFLOW STAGE OUTCOMES FOR EACH COMMAND.\nAgent\nCommand\nStructure\nType\nExpression\nSTT\nCC\nCR\nAF\nAP\nAD\nOF\nIC\nIR\nShow patient information\nSingle\nExplicit\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nIR\nDisplay the PFT info\nSingle\nExplicit\nAbbreviation\n1\n1\n1\n1\n1\n1\n1\n0\nIR\nCan you show pulmonary function test?\nSingle\nNLQ\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nIR\nPFT results\nSingle\nImplicit\nAbbreviation\n1\n1\n1\n1\n1\n1\n1\n0\nIR\nlung function results\nSingle\nImplicit\nParaphrase\n0\n1\n1\n1\n1\n1\n1\n0\nIR\nShow physical and PFT info\nComposite\nExplicit\nAbbreviation\n1\n1\n1\n1\n1\n1\n1\n0\nIR\nReset\nSingle\nImplicit\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nIV\nShow the CT views\nSingle\nExplicit\nBaseline\n0\n1\n1\n1\n1\n1\n1\n0\nIV\nCoronal plus 100\nSingle\nExplicit\nBaseline\n0\n1\n1\n1\n1\n1\n1\n0\nIV\nSagittal minus 30\nSingle\nExplicit\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nIV\nCan you move CT forward?\nSingle\nNLQ\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nIV\nMove front, front, front\nSingle\nImplicit\nParaphrase\n0\n1\n1\n1\n1\n1\n1\n0\nIV\nStep to posterior\nSingle\nImplicit\nParaphrase\n0\n1\n1\n1\n1\n1\n1\n0\nIV\nAxial zoom in\nSingle\nExplicit\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nIV\nCan you get axial closer?\nSingle\nNLQ\nParaphrase\n1\n1\n1\n1\n1\n1\n1\n0\nIV\nMinimize the axial image\nSingle\nExplicit\nParaphrase\n1\n1\n1\n1\n1\n1\n1\n0\nIV\nCould you reduce coronal image?\nSingle\nNLQ\nParaphrase\n0\n1\n1\n0\n0\n0\n1\n0\nIV\nZoom out\nSingle\nImplicit\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nIV\nMove axial to the middle slice and zoom in\nComposite\nExplicit\nParaphrase\n1\n1\n1\n1\n1\n1\n1\n0\nIV\nCan you move axial to 200 and coronal to 230?\nComposite\nNLQ\nBaseline\n0\n1\n1\n1\n1\n1\n1\n0\nAR\nShow the 3D recon image\nSingle\nExplicit\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nCan you load anatomical reconstruction?\nSingle\nNLQ\nParaphrase\n1\n1\n1\n1\n1\n1\n1\n1\nAR\nTurn on RLL\nSingle\nExplicit\nAbbreviation\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nCan you hide left lung?\nSingle\nNLQ\nParaphrase\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nShow anterior view\nSingle\nExplicit\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nCan you look from the front?\nSingle\nNLQ\nParaphrase\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nSurgical view\nSingle\nImplicit\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nSurgeon’s view\nSingle\nImplicit\nParaphrase\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nRotate to the right\nSingle\nExplicit\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nWould you rotate up?\nSingle\nNLQ\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nCan you zoom in to RLL?\nSingle\nNLQ\nAbbreviation\n1\n1\n1\n0\n1\n0\n1\n0\nAR\nZoom out\nSingle\nImplicit\nBaseline\n1\n1\n1\n1\n0\n0\n1\n0\nAR\nCan you zoom in and rotate to the left?\nComposite\nNLQ\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nCan you initialize and zoom in?\nComposite\nNLQ\nBaseline\n1\n1\n1\n1\n1\n1\n1\n0\nAR\nCan you erase all?\nSingle\nNLQ\nParaphrase\n1\n1\n1\n1\n1\n1\n1\n0\nAll results available at https://github.com/helena-lena/SAOP.\nIV. RESULTS\nA. Command-level Orchestration Evaluation\n1) Stage-level Accuracy: The VISA demonstrates strong\nerror recovery capability from STT stage failures, maintaining\nhigh accuracy across subsequent stages as shown in Fig. 7\n(a). The real-time audio stage accuracy is the performance\nof the wake-word detection model for the keyword “davinci”,\nwhich correctly recognized 266 out of 300 attempts (88.6%).\nThe remaining 34 attempts correspond to a false rejection rate\nof 11.3%, mainly due to the model misinterpreting “davinci”\nsuch as “Dabinci”, “David”, “Benchi”, or “dubbing”. For\nsuccessful detections, the latency between speech detection\nand the “davinci” recognition averaged 1.97 seconds, showing\nsmooth real-time interaction. Although STT accuracy is lower\ndue to transcription errors, most are corrected in the CC\nstage. The CR stage maintains high accuracy in selecting the\nagent, AD accuracy shows a slight decline due to occasional\nmispredictions in the agent function or parameters. Finally,\nthe OF achieves 100% accuracy, confirming that all workflow\nstages execute in the correct order.\nThe stage-level accuracies across the three task-specific\nagents exhibit similar trends (Fig. 7b). The STT stage shows\nthe largest variation, with the IV agent exhibiting the lowest\naccuracy due to frequent misrecognition of “CT” as “city”.\nNevertheless, most recognition errors are corrected during\nthe CC stage, the subsequent CR stage also maintains high\nperformance across all agents, and the AD stage shows a slight\ndecrease. Later stages generally show lower accuracy than the\nCC stage, reflecting the natural accumulation of prediction\nerrors when performing stages step-by-step with an LLM.\nInterestingly, the IR agent even compensates for imperfectly\ncorrected commands during the CR stage. For example, when\nthe STT transcribes “age info” as “Age in full”, the CC stage\nrefines it to “Age information in full”, allowing the CR stage to\ninfer the intent as a request for the age column only. Overall,\nVISA demonstrates strong robustness, effectively mitigating\nearly-stage errors and ensuring reliable agent execution.\nIn the AD stage, each task-specific agent determines the\naction uniquely mapped to an agent function, and its pa-\nrameters specifying how the action should be executed. Our\nVISA achieves consistently high accuracy for the function\nand parameters, and because the mispredictions in the two\ncomponents do not overlap, the combined AD accuracy is\nslightly lower (Fig. 7c). The accuracies across the three task\nagents exhibit similar trends, where the integrated AD accu-\nracy is comparable to or slightly lower than the accuracies of\nfunction and parameter predictions (Fig. 7d). A slight decrease\nin accuracy from the IR to IV and AR agents is likely due\nto the greater variability and complexity of the commands in\nthe latter tasks. Nevertheless, the differences remain marginal,\nindicating that the LLM effectively generalizes its reasoning\ncapabilities across heterogeneous task agents.\n"}, {"page": 9, "text": "9\nFig. 7.\nStage-level accuracy results: (a) Overall stage-level accuracy; (b) Stage-level accuracy by task agent type; (c) Overall accuracy of action\ndetermination components; (d) Action determination component accuracy by task agent type; Error bars indicate 95% confidence intervals.\nFig. 8. Workflow-level sucess rates results: (a) Overall results; (b) Results by task agent type; Error bars indicate 95% confidence intervals.\n2) Workflow-level Success Rate: The workflow-level SR\nimproves progressively from strict to single-pass to multi-\npass conditions as shown in Fig. 8 (a). The SRstrict remains\nat 65.8%, primarily limited by transcription errors in the\nSTT. In contrast, the SRsingle−pass increases to 89.2% as\nthe LLM resolves early-stage failures through correction and\nreasoning. The SRmulti−pass reaches 95.8%, showing that\nthe workflow orchestrator agent returns to the real-time audio\nstage for invalid commands, allowing the user to restate the\ncommand more clearly. A similar trend appears in task-specific\nagents as shown in Fig. 8 (b). The IV agent shows the\nlargest gap between strict and single-pass due to frequent STT\nerrors, whereas the IR shows the smallest. As task complexity\nincreases from IR to IV and AR, both SRsingle−pass and\nSRmulti−pass decrease.\n3) Failure and Recovery Paths: We also analyze command\nprocessing paths from STT to AD stages as shown in Fig. 9.\nMost STT errors arise from confusing medical terms with\nphonetically similar non-medical words, such as recognizing\n“CT” as “city”, “coronal” as “corona”, and “lung” as “long”.\nMinor lexical errors also occur like recognizing “to” as “2”,\n“right” as “write”, “zoom” as “June”, and “add” as “at”. While\nmost are resolved in the CC stage, a few remain, converting\nmedical terms into non-medical words or nouns into adjective\nforms. CR failures occur when commands remain invalid\nafter correction, whereas minor CC errors often do not affect\nagent selection, resulting in recovery. AD failures mainly arise\nfrom composite commands or mispredictions on functions or\nparameters. These findings highlight the need to support multi-\nstep actions and refine prompts for challenging cases.\n"}, {"page": 10, "text": "10\nFig. 9. A sankey diagram illustrating recovery and failure paths of the command through stages from (a) STT, (b) CC, (c) CR, and (d) AD.\nB. Command-category Evaluation\nWe evaluate SRmulti−pass across hierarchical command\ncategories and VISA robustly handles a wide range of struc-\ntural and linguistic variations in user commands, as shown\nin Fig. 10 (a). Within the command structure category, com-\nposite commands are more challenging than single commands\nbecause the platform struggles to determine the correct agent\nfunction when multiple functions must be executed sequen-\ntially. For the command type category, the platform shows\nstrong performance for explicit commands, implicit com-\nmands, and NLQ, demonstrating that the LLM can follow user\nintent with minimal performance degradation despite linguistic\nvariations. Regarding the command expression category, com-\nmands using abbreviations achieve perfect performance be-\ncause the LLM prompt explicitly includes the medical domain\nabbreviations. Commands with paraphrased expressions show\nslightly lower performance, as such variations are not included\nin the prompt and may introduce additional ambiguity.\nWe further analyze cross-category SRmulti−pass as illus-\ntrated in Fig. 10 (b) and (c). In the structure–type relation-\nship, single commands generally outperform composite ones,\nwith composite failures occurring in three cases that require\nmultiple actions, such as “Can you zoom in and rotate to the\nleft?” or “Can you initialize and zoom in?”. All command\ntypes under the single structure maintain high performance,\ndemonstrating that the VISA reliably processes straightfor-\nward, single-step commands. In the type–expression relation-\nship, explicit commands achieve strong performance across all\nexpression forms, whereas and only two failures arise from\ncomposite structure and entering invalid loops. In contrast,\nimplicit commands are less robust to unseen synonyms or\nparaphrases, yet still maintain reasonable performance. Three\nfailures are due to entering invalid loops, and one results from\nmisinterpreting “pre-existing condition” and displaying both\n“comorbidities” and “diagnosis”. NLQs perform slightly better\nwith paraphrase expressions because they usually contain\nricher contextual detail than short and implicit commands.\nTwo failures appear: one when the VISA interprets “Could\nyou reduce coronal image?” as adjusting slice position instead\nof zooming out, and another when it interprets “Can you view\nthe superior angle?” as rotating upward rather than switching\nto the superior view. These results show that VISA generalizes\nwell across different categories, with performance decreasing\nfor composite or paraphrased commands that require multi-\naction and deeper reasoning on unseen expressions.\nWe further investigate the successful cases among chal-\nlenging categories, composite commands and commands in\nparaphrase format. For composite commands, the IR agent\nretrieves all requested data in cases like “show physical and\nPFT info” and “show surgery and tumor information”. The IV\nagent accurately executes commands like “move axial to the\nmiddle slice and zoom in” or “sagittal to the center slice and\nzoom in”, computing mid-slice positions automatically with\nzoom in action, and correctly handles multi-plane commands\nsuch as “move to axial 230, coronal 220, sagittal 220” or\n“move to 10, 20, 50” even without explicit plane order. The\nAR agent correctly executes composite commands like “view\nfrom the front and rotate left” or “view from the posterior and\nrotate right”, when a single function can handle both actions\nthrough parameter adjustment. For paraphrased commands, we\nexamine cases where such expressions are not explicitly de-\nfined in the prompt but convey semantically similar meanings.\nThe IR agent can handle expressions such as retrieving the\nage for “How old is the patient?” and the sex for “What’s\nthe gender?”. The IV agent interprets various expressions,\nexecuting commands like “Move front front front”, “Move\nleft left left left”, and “Move down down” as movements to\n“Coronal plus 30”, “Sagittal minus 40”, and “Axial minus\n20”. As the prompt defines the default movement distance\nfor 10 units, the LLM infers the repetitions to multiples of\nthat distance. It also predicts positions correctly for “Coronal\nto maximum”, “minimum”, and “middle slice”, and adapts to\nunseen verbs such as “step” or “slide”. For zoom operations, it\nperforms “Zoom in” and “Zoom out” correctly for paraphrases\nlike “Coronal enlarge,” “Focus on coronal plane,” or “Return to\nsmaller view.” The AR agent recognizes “Add airway” as the\ntrachea and bronchia, sets the anterior view for “look from the\nfront,” and correctly interprets unseen synonyms for rotation\nand zoom actions. Overall, these results show that the LLM\nhandles synonymous and paraphrased expressions with strong\nlinguistic flexibility.\n"}, {"page": 11, "text": "11\nFig. 10.\nMulti-pass success rate across command categories: (a) Multi-pass success rate by command category; (b) Multi-pass success rate by\nstructure × type; (c) Multi-pass success rate by type × expression; Error bars in all plots indicate 95% confidence intervals.\nFig. 11. Comparison by voice type: (a) Stage-level accuracy by voice type; (b) Success rate by voice type; Error bars are 95% confidence intervals.\nC. Applicability for Real Surgical Settings\nIn surgical environments, voice characteristics differ across\nspeakers in tone, accent, speech rate, and pronunciation. To\nevaluate robustness to speaker variability, we compare four\nsynthesized neural voices with human speech as shown in\nFig. 11. Synthesized voices achieve higher STT accuracy\ndue to their clear and native-like pronunciation, whereas\nhuman speakers recover more effectively from invalid loops\nby restating the command more clearly and emphasizing key\ninformation. Although some reasoning errors occur during\nthe AD stage, all voice types maintain high stage-level ac-\ncuracy. The workflow-level SR under strict, single-pass, and\nmulti-pass conditions exhibit similar trends. Human speech\nexhibits lower SRstrict because of STT failures, but ultimately\nachieves the highest SRmulti−pass by recovering from invalid\ncommands. These results demonstrate that VISA maintains\nstable performance across diverse voice conditions.\nFurthermore, the operating room contains background noise\nand conversations from multiple surgeons and nurses. To\nensure that wake-word “davinci” detection does not activate\naccidentally, we evaluate 15 unsupported command examples\nas shown in Table III. The wake-word detection module\nremains active for one hour, and an unsupported command\nis spoken every four minutes. During one hour of testing,\nthe VISA records a false alarm rate of 0, indicating no\nunintended activations. We also evaluate whether these 15\nunsupported commands are correctly classified as invalid after\nthe wake word “davinci” is triggered. Three are misclassified\nas valid patient information requests, asking about vital sign,\nCO2 pressure, and frozen section. Since these fields are not\npresent in the given data columns, the VISA either overlays\nthe previous video information or hides the overlay. Overall,\nour VISA reliably distinguishes task-related commands from\nunsupported commands, though a few cases still require re-\nfinement to prevent incorrect validation.\n"}, {"page": 12, "text": "12\nTABLE III\nRESULTS ON UNSUPPORTED COMMANDS\nExample\nWake-word\nInvalid\nReplace the gauze.\nNo\nYes\nApply suction.\nNo\nYes\nClean the camera lens.\nNo\nYes\nWipe the third arm.\nNo\nYes\nSwitch between arm 1 and 2.\nNo\nYes\nPrepare the hemostatic agent.\nNo\nYes\nHow long has the surgery been going on?\nNo\nYes\nIs the patient’s vital sign stable?\nNo\nNo\nWhat is the current CO2 pressure?\nNo\nNo\nHow long since the frozen section was sent?\nNo\nNo\nThe patient is coughing.\nNo\nYes\nPrepare the stapler.\nNo\nYes\nPrepare the ICG injection.\nNo\nYes\nPrepare for the air leak test.\nNo\nYes\nInsert the retrieval bag.\nNo\nYes\nD. Analysis of Sequential Command Execution\nWe further evaluate whether VISA maintains continuity\nacross sequential video clips8 using its memory state, as shown\nin Fig. 12 and Fig. 13. The IV agent continuously updates\naxial, coronal, and sagittal slice positions within the prede-\nfined minimum and maximum constraints. For the AR agent,\nVISA preserves continuity in zoom operations by accurately\ncomputing the starting and ending scales while maintaining\nthe minimum scale constraint of 1. The rotation and view\nadjustments also function correctly within the zoomed state,\nand the zoom out action follows the exact reverse path of zoom\nin. Additionally, when no command appears in the target video\nclip, the CC stage revises the empty input to “Select agent”,\nand the AD stage restores the previous agent state. For the\n“Reset” command, each agent restores the default settings\nof its function and the parameters defined in the prompt.\nThese results demonstrate that VISA preserves temporal and\nfunctional consistency across video clips, ensuring stable and\ncontinuous state transitions.\nV. DISCUSSION\nWe demonstrate the effectiveness of the hierarchical multi-\nagent framework with LLM-based orchestration. The work-\nflow orchestrator agent reliably plans and selects appropriate\nfunctions based on the current state and predefined deci-\nsion rules. The hybrid design of LLM-driven reasoning with\nprobability-based decision rules resolves invalid loops and en-\nables workflow stages to progress smoothly. Although comput-\ning probabilities for all functions slows inference compared to\ndirect function prediction, it could enable conditional probabil-\nity–based planning to handle increasingly complex paths as the\nnumber of agents and functions grows. Separating workflow\nfunctions from task-specific agent functions further improves\nmodularity and scalability, allowing flexible integration of new\nagents or functions. Expanding clinically useful agents, such\nas detecting critical blood vessels, supporting intraoperative\nnavigation, and assisting in robotic planning, will enhance the\nclinical applicability.\n8Videos available at https://helena-lena.github.io/SAOP/\nWhile our VISA demonstrates the feasibility of LLM-based\norchestration for surgical assistance, the observed errors reveal\nopportunities for improvement. At the STT stage, most errors\noccur with medical domain-specific terminology, suggesting\nthe need to fine-tune lightweight STT models on medical terms\nto improve transcription accuracy. In the CC stage, the LLM\nrevises misrecognized words to their intended medical terms,\nyet augmenting text correction rules or fine-tuning a correction\nmodel on medical context could further enhance robustness\nin failure cases. At the AD stage, most errors arise from\ncomposite commands, indicating the need to extend the VISA\nto support multi-step sequential operations. Additionally, since\nthe current prompting strategy are closely coupled with the\nGemma3 [22] model, performance may vary across different\nLLMs. Developing a self-evolving orchestration mechanism\nthat learns from prior errors and dynamically refines its\nprompts could ensure adaptability and consistent performance\nacross models in future implementations.\nFrom a dataset perspective, the evaluation uses 240 com-\nmands, including both human and synthesized voice inputs\nfrom four speakers to simulate variability. However, the dataset\ndoes not fully represent real clinical variability in phrasing,\nterminology, or multilingual mixing observed in real clinical\nsettings. Future work may involve fine-tuning models on\nmultilingual medical data or using fast and commercially\navailable multilingual STT systems. Moreover, the clinical\ninformation displayed by the IR agent is based on a predefined\nset of columns selected by clinicians. However, the required\nfields may vary across departments and individual surgeons,\nindicating the need for customizable data configurations. For\n3D anatomical models displayed by AR agent, using com-\nmercial or clinically validated software could provide more\nprecise anatomical structures and support more sophisticated\nmanipulation for clinical use.\nRegarding computational efficiency, we employ Gemma3\n27B (gemma3:27b-it-qat) as the core LLM for all VISA\nstages, a quantization-aware trained version that preserves\nhigh performance with reduced memory usage. We set the\ntemperature to 0 to minimize randomness, use num predict\nof −1 for unrestricted generation, and fix the seed at 42\nfor reproducibility. The model runs locally via Ollama9 and\noccupies approximately 17 GB when preloaded onto a single\nNVIDIA RTX 4500 Ada GPU (24 GB), while the second\nGPU handles other APIs, models, or agents. However, cumu-\nlative latency from sequential LLM calls remains a challenge\nin time-sensitive surgical environments. This delay becomes\nmore pronounced when the LLM engages in complex reason-\ning or generates longer output. Latency may be reduced by\nlimiting LLM reasoning to essential stages, caching reasoning\npatterns, shortening outputs, and adapting to surgeon-specific\ninteraction patterns. Lastly, for 3D model rendering, increasing\nmesh decimation or frame skipping improves speed but may\nreduce smoothness during rotation and zoom.\n9https://github.com/ollama/ollama\n"}, {"page": 13, "text": "13\nFig. 12. Video examples of consecutive scenarios for the IV agent.\nFig. 13. Video examples of consecutive scenarios for the AR agent.\nREFERENCES\n[1] M. E. Allaf, S. V. Jackman, P. G. Schulam, J. A. Cadeddu, B. R. Lee,\nR. G. Moore et al., “Laparoscopic visual field: Voice vs foot pedal\ninterfaces for control of the AESOP robot,” Surg. Endosc., vol. 12, no.\n12, pp. 1415–1418, 1998.\n[2] L. Mettler, M. Ibrahim, and W. Jonat, “One year of experience working\nwith the aid of a robotic assistant (the voice-controlled optic holder\nAESOP) in gynaecological endoscopic surgery,” Hum. Reprod., vol. 13,\nno. 10, pp. 2748–2750, 1998.\n[3] H. Reichenspurner, R. J. Damiano, M. Mack, D. H. Boehm, H. Gulbins,\nC. Detter et al., “Use of the voice-controlled and computer-assisted\nsurgical system ZEUS for endoscopic coronary artery bypass grafting,”\nJ. Thorac. Cardiovasc. Surg., vol. 118, no. 1, pp. 11–16, 1999.\n[4] C. O. Nathan, V. Chakradeo, K. Malhotra, H. D’Agostino, and R.\nPatwardhan, “The voice-controlled robotic assist scope holder AESOP\nfor the endoscopic approach to the sella,” Skull Base, vol. 16, no. 3, pp.\n123–131, 2006.\n[5] I. A. Salama and S. D. Schwaitzberg, “Utility of a voice-activated system\nin minimally invasive surgery,” J. Laparoendosc. Adv. Surg. Tech., vol.\n15, no. 5, pp. 443–446, 2005.\n[6] C.\nVaida,\nD.\nPisla,\nN.\nPlitea,\nB.\nGherman,\nB.\nGyurka,\nF.\nGraur et al., “Development of a voice controlled surgical robot,”\nin\nNew\nTrends\nin\nMechanism\nScience:\nAnalysis\nand\nDesign.\nDordrecht, The Netherlands: Springer, 2010, pp. 567–574. [On-\n"}, {"page": 14, "text": "14\nline]. Available: https://link.springer.com/chapter/10.1007/978-90-481-\n9689-0 65?utm source=chatgpt.com\n[7] M. P. Forte, R. Gourishetti, B. Javot, T. Engler, E. D. Gomez, and K.\nJ. Kuchenbecker, “Design of interactive augmented reality functions for\nrobotic surgery and evaluation in dry-lab lymphadenectomy,” Int. J. Med.\nRobot., vol. 18, no. 2, p. e2351, 2022.\n[8] M. Elazzazi, L. Jawad, M. Hilfi, and A. Pandya, “A natural language\ninterface for an autonomous camera control system on the da Vinci\nsurgical robot,” Robotics, vol. 11, no. 2, p. 40, 2022.\n[9] Y. G. Kim, J. W. Shim, G. Gimm, S. Kang, W. Rhee, J. H. Lee et\nal., “Speech-mediated manipulation of da Vinci surgical system for\ncontinuous surgical flow,” Biomed. Eng. Lett., vol. 15, no. 1, pp. 117–\n129, 2025.\n[10] A. Davila, J. Colan, and Y. Hasegawa, “Voice control interface for surgi-\ncal robot assistants,” in Proc. 2024 Int. Symp. Micro-NanoMehatronics\nHum. Sci. IEEE, 2024, pp. 1–5.\n[11] A. Pandya, “ChatGPT-enabled daVinci surgical robot prototype: Ad-\nvancements and limitations,” Robotics, vol. 12, no. 4, p. 97, 2023.\n[12] Y. Zhang, B. Orthmann, M. C. Welle, J. Van Haastregt, and D.\nKragic, “LLM-driven augmented reality puppeteer: Controller-free\nvoice-commanded robot teleoperation,” in Proc. Int. Conf. Human-\nComputer Interaction. Springer, 2025, pp. 97–112.\n[13] A. Gavric, D. Bork, and H. A. Proper, “Surgery AI: Multimodal process\nmining and mixed reality for real-time surgical conformance checking\nand guidance,” ZEUS 2025, p. 48, 2025.\n[14] OpenAI, “GPT-4 technical report,” arXiv:2303.08774, 2023.\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts\net al., “PaLM: Scaling language modeling with pathways,” J. Mach.\nLearn. Res., vol. 24, no. 240, pp. 1–113, 2023.\n[16] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T.\nLacroix et al., “LLaMA: Open and efficient foundation language mod-\nels,” arXiv:2302.13971, 2023.\n[17] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei\net al., “LLaMA 2: Open foundation and fine-tuned chat models,”\narXiv:2307.09288, 2023.\n[18] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle\net al., “The LLaMA 3 herd of models,” arXiv:2407.21783, 2024.\n[19] Meta, “The LLaMA 4 herd: The beginning of a new era of\nnatively\nmultimodal\nAI\ninnovation,”\n2025.\n[Online].\nAvailable:\nhttps://ai.meta.com/blog/llama-4-multimodal-intelligence/\n[20] Gemma Team, “Gemma: Open models based on gemini research and\ntechnology,” arXiv:2403.08295, 2024.\n[21] Gemma Team, “Gemma 2: Improving open language models at a\npractical size,” arXiv:2408.00118, 2024.\n[22] Gemma Team, “Gemma 3 technical report,” arXiv:2503.19786, 2025.\n[23] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia et\nal., “Chain-of-thought prompting elicits reasoning in large language\nmodels,” in Proc. Adv. Neural Inf. Process. Syst., vol. 35. NeurIPS,\n2022, pp. 24 824–24 837.\n[24] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang et al., “Self-\nconsistency improves chain of thought reasoning in language models,”\narXiv:2203.11171, 2022.\n[25] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao et al., “Tree\nof thoughts: Deliberate problem solving with large language models,”\nin Proc. Adv. Neural Inf. Process. Syst., vol. 36. NeurIPS, 2023, pp.\n11 809–11 822.\n[26] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L.\nGianinazzi et al., “Graph of thoughts: Solving elaborate problems with\nlarge language models,” in Proc. AAAI Conf. Artif. Intell., vol. 38. AAAI,\n2024, pp. 17 682–17 690.\n[27] S. Franklin and A. Graesser, “Is it an agent, or just a program?: A\ntaxonomy for autonomous agents,” in Int. Workshop Agent Theories,\nArchitectures, Lang. Springer, 1996, pp. 21–35.\n[28] S. Russell and P. Norvig, “Artificial intelligence,” in A Modern Ap-\nproach. Englewood Cliffs, NJ, USA: Prentice Hall, 1995, pp. 31–33.\n[29] K. Huang and C. Hughes, “Introduction to agentic AI: Foundations,\ndrivers, and risks,” in Securing AI Agents: Foundations, Frameworks,\nand Real-World Deployment. Cham, Switzerland: Springer, 2025, pp.\n3–16.\n[30] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan et al.,\n“ReAct: Synergizing reasoning and acting in language models,” in Proc.\nInt. Conf. Learn. Representations. ICLR, 2022.\n[31] Q. Wu, G. Bansal, J. Zhang, Y. Wu, B. Li, E. Zhu et al., “AutoGen:\nEnabling next-gen LLM applications via multi-agent conversations,” in\nProc. First Conf. Lang. Modeling, 2024.\n[32] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “CAMEL:\nCommunicative agents for “mind” exploration of large language model\nsociety,” in Proc. Adv. Neural Inf. Process. Syst., vol. 36. NeurIPS, 2023,\npp. 51 991–52 008.\n[33] K. Zhang, Y. Zuo, B. He, Y. Sun, R. Liu, C. Jiang et al., “A survey of\nreinforcement learning for large reasoning models,” arXiv:2509.08827,\n2025.\n[34] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang et al., “A survey\non large language model based autonomous agents,” Front. Comput. Sci.,\nvol. 18, no. 6, p. 186345, 2024.\n[35] X. Li, S. Wang, S. Zeng, Y. Wu, and Y. Yang, “A survey on LLM-\nbased multi-agent systems: Workflow, infrastructure, and challenges,”\nVicinagearth, vol. 1, no. 1, p. 9, 2024.\n[36] B. Zhao, L. G. Foo, P. Hu, C. Theobalt, H. Rahmani, and J. Liu,\n“LLM-based agentic reasoning frameworks: A survey from methods to\nscenarios,” arXiv:2508.17692, 2025.\n[37] K. T. Tran, D. Dao, M.-D. Nguyen, Q.-V. Pham, B. O’Sullivan, and H. D.\nNguyen, “Multi-agent collaboration mechanisms: A survey of LLMs,”\narXiv:2501.06322, 2025.\n[38] S. Rasal and E. J. Hauer, “Navigating complexity: Orchestrated problem\nsolving with multi-agent LLMs,” arXiv:2402.16713, 2024.\n[39] Z. Du, C. Qian, W. Liu, Z. Xie, Y. Wang, R. Qiu et al., “Multi-agent\ncollaboration via cross-team orchestration,” in Proc. Findings Assoc.\nComput. Linguistics. ACL, 2025, pp. 10 386–10 406.\n[40] Y. Dang, C. Qian, X. Luo, J. Fan, Z. Xie, R. Shi et al., “Multi-agent\ncollaboration via evolving orchestration,” arXiv:2505.19591, 2025.\n[41] J. Zhang, Y. Fan, K. Cai, X. Sun, and K. Wang, “OSC: Cognitive\norchestration through dynamic knowledge alignment in multi-agent\nLLM collaboration,” arXiv:2509.04876, 2025.\n[42] LangChain\nAI,\n“LangGraph,”\n2024.\n[Online].\nAvailable:\nhttps://github.com/langchain-ai/langgraph\n[43] J. Liu, “LlamaIndex,” 2022. [Online]. Available: https://github.com/run-\nllama/llama index\n[44] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. Mcleavey, and I.\nSutskever, “Robust speech recognition via large-scale weak supervision,”\nin Proc. 40th Int. Conf. Mach. Learn., vol. 202. PMLR, 2023, pp.\n28 492–28 518.\n[45] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L.\nLugosch et al., “SpeechBrain: A general-purpose speech toolkit,”\narXiv:2106.04624, 2021.\n[46] A. Abouelenin, A. Ashfaq, A. Atkinson, H. Awadalla, N. Bach, J. Bao\net al., “Phi-4-Mini technical report: Compact yet powerful multimodal\nlanguage models via mixture-of-LoRAs,” arXiv:2503.01743, 2025.\n"}]}