{"doc_id": "arxiv:2601.20674", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.20674.pdf", "meta": {"doc_id": "arxiv:2601.20674", "source": "arxiv", "arxiv_id": "2601.20674", "title": "Harnessing Large Language Models for Precision Querying and Retrieval-Augmented Knowledge Extraction in Clinical Data Science", "authors": ["Juan Jose Rubio Jan", "Jack Wu", "Julia Ive"], "published": "2026-01-28T14:57:36Z", "updated": "2026-01-28T14:57:36Z", "summary": "This study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) data science tasks: structured data querying (using programmatic languages, Python/Pandas) and information extraction from unstructured clinical text via a Retrieval Augmented Generation (RAG) pipeline. We test the ability of LLMs to interact accurately with large structured datasets for analytics and the reliability of LLMs in extracting semantically correct information from free text health records when supported by RAG. To this end, we presented a flexible evaluation framework that automatically generates synthetic question and answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted on a curated subset of MIMIC III, (four structured tables and one clinical note type), using a mix of locally hosted and API-based LLMs. Evaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings demonstrate the potential of LLMs to support precise querying and accurate information extraction in clinical workflows.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.20674v1", "url_pdf": "https://arxiv.org/pdf/2601.20674.pdf", "meta_path": "data/raw/arxiv/meta/2601.20674.json", "sha256": "800d1aef9a9fd6592eb82fc387b1ded5926fbc7a5b5be6694793f18734067413", "status": "ok", "fetched_at": "2026-02-18T02:20:13.210478+00:00"}, "pages": [{"page": 1, "text": "Harnessing Large Language Models for Precision Querying and Retrieval-Augmented \nKnowledge Extraction in Clinical Data Science \nAuthor Information \nAffiliations \nInstitute of Psychiatry, Psychology and Neuroscience King’s College London, London \nJuan Jose Rubio Jan, Jack Wu \nInstitute of Health Informatics, University College London, London \nJulia Ive \nAbstract \nThis study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) \ndata science tasks: structured data querying (using programmatic languages;  Python/Pandas) and \ninformation extraction from unstructured clinical text via a Retrieval-Augmented Generation (RAG) \npipeline.   \nWe test the ability of LLMs to interact accurately with large structured datasets for analytics and the \nreliability of LLMs in extracting semantically correct information from free-text health records when \nsupported by RAG.  \nTo this end, we presented a flexible evaluation framework that automatically generates synthetic \nquestion–answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted \non a curated subset of MIMIC-III, (four structured tables and one clinical note type), using a mix of \nlocally hosted and API-based LLMs.  \nEvaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings \ndemonstrate the potential of LLMs to support precise querying and accurate information-extraction in \nclinical workflows. \nIntroduction \nElectronic Health Records (EHRs) have become the predominant source of patient data in contemporary \nhealthcare, enabled by the digitalisation of clinical workflows and the promise of interoperable \n1 \n \n"}, {"page": 2, "text": "decision-support systems (Gunter et al., 2005). In this context, two capabilities are foundational for \nrealising the value of EHRs: structured querying (e.g., SQL, Python/Pandas) to interrogate large relational \ndatasets typical of systems like MIMIC‑III (Johnson et al., 2016), and information-extraction from \nunstructured clinical notes to identify cohort-defining facts, timelines, and outcomes (Meystre et al., \n2008). Large Language Models (LLMs) offer strong advantages on both fronts: they excel at code \ngeneration (text‑to‑SQL and Python/Pandas) and at zero‑shot information extraction, which is particularly \nvaluable where annotated data is scarce (Ignashina, M. et al., 2025; Guevara et al, 2024; Alsentzer et al., \n2023).  \nHowever, LLMs are also capable of “hallucination”:  producing fluent but unsupported statements (Ji et \nal., 2023). For EHR applications, where clinical accuracy is of critical importance (safeguarding patient \nsafety and research integrity), rigorous evaluation frameworks are essential to ensure outputs are both \nfactually correct and semantically consistent with source data. In this work, we evaluate the consistency \nand clinical validity of LLM-generated responses by focusing on two key aspects: (1) assessing whether \nLLMs can effectively interact with large structured clinical datasets through Python/Pandas to perform \nprecise querying and analytics, and (2) determining how accurately LLMs can extract semantically \ncorrect information from free-text health data. \nPrior work informs both of our research questions. LLMs have shown strong performance in Text-to-SQL \ntasks, with prompt engineering techniques such as in-context learning and Chain-of-Thought reasoning \nsignificantly improving query generation accuracy (Rajkumar et al., 2022). Recent approaches, such as \nDIN-SQ and MAG-SQL, further enhance this capability by separating complex questions into smaller \nsteps (Pourreza & Rafiei et al., 2023; Xie et al., 2024). In the context of EHR applications, smaller \ninstruction‑tuned models such as FLAN‑T5 have demonstrated strong performance in off‑the‑shelf \nevidence detection (e.g., pediatric depression detection or postpartum hemorrhage), surpassing traditional \nbaselines and underscoring their practical utility for cohort selection and analytics pipelines that depend \non precise structured queries (Ignashina et al., 2025; Alsentzer et al., 2023).  Also,  findings from the \n2 \n \n"}, {"page": 3, "text": "broader domains indicate that both smaller and larger models can assist in information extraction tasks, \nwith larger models often achieving  superior results (Ziems et al., 2023). \nAlso, to address the hallucination problem in the medical domain, prior work has introduced metrics such \nas factual consistency scores over QA datasets (Pal et al., 2023). Building on this, Asgari et al. (2025) \nexperiment with various prompting techniques to minimise errors for safer clinical documentation.  To \nfurther reduce hallucinations, we propose creating domain-specific evaluation datasets by synthetically \ngenerating questions with known answers for two key tasks: code generation and information extraction. \nWe follow the approach of Ding et al. (2025), who build silver-standard datasets from NICE guidelines \nfor clinical LLMs. \nOur contributions are twofold: (i) a generalisable method for creating test cases for structured and \nunstructured data applicable to any healthcare use case, and (ii) the application of this framework to the \nMIMIC-III dataset using a range of LLMs, including locally hosted and API-based models. We \ndemonstrate that this approach enables reliable retrieval and summarisation of clinically relevant \ninformation, while also illustrating the practical complexity of deploying such technologies in clinical \ncontexts and assessing the readiness of current models for real-world healthcare applications. \nDatasets and Methods \na.​ Datasets \nFor all tests within the pipeline implementation presented in this work, we used the MIMIC-III database \n(Johnson et al., 2016), which is a large and freely available resource of deidentified health records. The \noriginal dataset contains information on more than forty thousand patients across twenty-six tables. Most \nof the files provided in the database by Johnson et al. (2016) are distributed in comma-separated values \n(CSV) format. \nFor this specific prototype, we focused on a subset of the MIMIC-III database, using only the Patients, \nPrescriptions, Diagnoses, and D_ICD_Diagnoses tables for the structured data component of this \nresearch, and the NoteEvents table for the unstructured data experiments. Additionally, due to the \ndimensionality of the data and constraints related to our token budget for processing, we restricted the \n3 \n \n"}, {"page": 4, "text": "analysis to 101 unique randomly selected patients from the Patients table. The remaining tables were \nsubsequently merged using the SUBJECT_ID and ICD9_CODE fields. \n \nCategory \nDescription \nData source \nMIMIC-III (Johnson et al., 2016) \nData modality \nStructured and unstructured clinical data \nStructured tables used \nPATIENTS, PRESCRIPTIONS, DIAGNOSES_ICD, \nD_ICD_DIAGNOSES \nUnstructured table used \nNOTEEVENTS \nNumber of patients \n101 \nTotal records \n274,022 \nNumber of features \n23 \nJoin keys \nSUBJECT_ID, ICD9_CODE \nStructured data format \nCSV \nUnstructured data format \nFree-text clinical notes \nText preprocessing \nunstructured (RAG) \nChunking (400 tokens, 50-token overlap) \nEmbedding model \nunstructured (RAG) \nMiniLM-L6-v2 \nVector index unstructured \n(RAG) \nFAISS \nDate of birth handling \nSynthetic dates generated using Faker \nRationale for DOB \nsynthesis \nOriginal dates anonymised and not analytically meaningful \nTable 1. Summary of the MIMIC-III data subset and methodological design \n \nb.​ Methods \ni.​\nTest data creation \nBecause all data in MIMIC-III are anonymised, some values in fields such as DOB do not correspond to \nrealistic dates and are therefore not meaningful for analysis. Consequently, we synthetically generated a \nnew field, DOB_Demo, using the faker and datetime libraries, producing random dates within a \npredefined range. After introducing this field and removing variables that were not required for \nsubsequent analyses and would unnecessarily increase tokenisation costs, the resulting dataset had \ndimensions of (274,022, 23), corresponding to 274,022 records and 23 features. Although the cohort \ncomprised only 101 unique subject identifiers, both the Prescriptions and Diagnoses tables contain \n4 \n \n"}, {"page": 5, "text": "multiple entries per subject. Consequently, joining these tables at the subject level yielded a substantially \nlarger dataset due to the one to many relationships inherent in the underlying clinical data. \nFor the unstructured data test, we manually selected one clinical note from the NoteEvents table and \ncreated a generic text (.txt) file for evaluation. This selection was made because the free-text records in \nthe database vary in length and dimensionality across patients, and because of the previously mentioned \nlimitations related to the number of tokens that could be processed. At the same time, the specific clinical \nnote was chosen based on its completeness and representative nature, as it contained sufficient detail of \ncontent to support meaningful testing.  \nFor the structured dataset, we designed 30 prompts of varying complexity, accounting for differences in \ndata preprocessing requirements, aggregation tasks, and the number of operations needed. These factors \nwere treated as shifting variables to enable a comprehensive evaluation of the pipeline. For example, the \nprompt “What is the median age?” represents a relatively simple two-step operation for the model, as the \ndataset contains date of birth rather than age, requiring an initial transformation followed by a median \naggregation. A follow-up prompt with increased complexity was “What is the median age of female \nsubjects?”, which additionally requires conditional filtering prior to aggregation. \nFor the unstructured dataset, we used a more advanced LLM (GPT-5) to segment the original .txt \ndocument into 50 semantically coherent chunks, from which targeted questions were generated. Each \nquestion was directly derived from the content of its corresponding text segment. For example, the \nquestion “Why was the pre-surgical physical exam not obtained?” was generated from the original \nsentence: “Physical exam prior to surgery was not obtained since the patient was intubated and sedated.” \nThis approach ensured that each question was grounded in the source text and allowed for a systematic \nevaluation of the model’s ability to retrieve and reason over unstructured clinical information. \nii.​\nStructured dataset pipeline design \nDue to the nature of structured data, we decided to implement a working prototype based on a real-world \nhealth informatics practice, using an agent-based pipeline and generating questions in a dynamic way \ndepending on the nature of the data. The size of the dataset and the semantic meaning encoded in its row \nand column structure make simple Retrieval-Augmented Generation (RAG) approaches and token based \ningestion unsuitable for this use case. Instead, we adopted an AI agent capable of translating natural \nlanguage queries into executable Python (Pandas) code, which is then run within a controlled execution \nenvironment. This design allows structured queries to be answered programmatically while preserving the \nrelational semantics of the data, making it a more appropriate approach for interacting with tabular \nhealthcare data. \n \nThis approach was implemented for both locally hosted models and proprietary API-based large language \nmodels, specifically Llama 3 8B Instruct and GPT-4o Mini, respectively. Llama 3 8B Instruct was \nselected as one of the smallest viable large language models capable of reliable code generation while \n5 \n \n"}, {"page": 6, "text": "remaining suitable for local deployment. This model provided sufficient instruction following and agent \ncapabilities for the structured querying tasks, while maintaining a computational viability with locally \nhosted experimentation. Llama 3 8B Instruct was accessed via the Hugging Face Transformers interface, \nenabling local inference and integration within the experimental pipeline for structured data querying and \nunstructured text generation tasks. The authors of the MIMIC-III repository emphasise the importance of \nresponsible data usage, particularly when interacting with external or online services such as commercial \nlarge language model APIs. In line with these recommendations, an additional motivation for the \nproposed LLM pipeline design was to ensure compliance with approved data handling practices. \nSpecifically, the use of the Azure OpenAI Service was selected because it is listed as an authorised data \nprocessor for MIMIC data, thereby allowing the integration of proprietary LLMs while adhering to the \nrepository’s data governance requirements. \nBoth models were provided with the same system prompt to ensure consistent generation of executable \ncode for the CSV agent. In this setup, no component of the tabular data is embedded; instead, assumptions \nderived from user queries are validated against column names and resolved through executable filtering, \naggregation, or transformation operations expressed in Pandas code. Once the pandas code is executed, \nthe LLM preprocess the response and generates an output as illustrated in Figure 1.  \n \n \nFigure 1. Agent-based csv Pipeline \n \niii.​\nUnstructured dataset pipeline design \nAs part of a representative pipeline, we implemented a Retrieval-Augmented Generation (RAG) \nframework, originally introduced by Lewis et al. (2021), to extend the effective knowledge of both locally \nhosted and proprietary API-based large language models by grounding their outputs in externally \nretrieved unstructured documents.The models, were  specifically Flan-T5 Large and GPT-4o Mini, by \nproviding access to unstructured data. For the unstructured data experiments, FLAN-T5 Large was \nselected instead of Llama 3 8B Instruct. Despite having a smaller parameter count, FLAN-T5 Large is \nexplicitly trained on a diverse mixture of natural language understanding and instruction-following tasks, \nmaking it well suited for text-based question answering and information extraction from free-text clinical \nnotes. In this context, its architecture and training objectives were more aligned with the unstructured \nextraction task than those of Llama 3 8B, which was prioritised for structured code generation and \nagent-based interaction. As illustrated in Figure 2, the process begins by uploading the pre-processed and \nsampled data described in Table 1 into an embedding model, which transforms the documents into vector \n6 \n \n"}, {"page": 7, "text": "representations. These embeddings are then stored in a vector database that enables retrieval and \ninteraction with the LLM. \nUser prompts are similarly pre-processed and embedded using the same embedding model, in this case \nMiniLM-L6-v2, in order to generate a shared vector space. This allows relevant document chunks to be \nretrieved and passed to the LLM to support query answering. The vector store was constructed using a \nconsistent chunking strategy, with a chunk size of 400 tokens and an overlap of 50 tokens, and was \nsubsequently indexed using FAISS (Facebook AI Similarity Search), a library for efficient similarity \nsearch and clustering of dense vectors (Johnson, Douze, and Jégou, 2017). \nThe Retrieval-Augmented Generation (RAG) paradigm enables the seamless integration of external data \nsources with LLMs, allowing users to query domain-specific information while benefiting from the \nreasoning capabilities of pre-trained models. This approach extends model knowledge without requiring \nfine-tuning, making it a flexible and computationally efficient strategy for incorporating external \nunstructured data. \n \nFigure 2. Retrieval Augmented Generation Pipeline \n \nFor both types of data, model and pipeline design choices were guided by the objective of evaluating a \nminimum viable prototype for LLM-based clinical data interaction. This approach prioritised architectural \nfeasibility, reproducibility, and computational efficiency, allowing assessment of whether accurate \nstructured querying and unstructured information extraction can be achieved under constrained but \nrealistic deployment settings. \n \n7 \n \n"}, {"page": 8, "text": "Results \na.​ Structured data \nFor the structured data experiments, we measured the accuracy of the models based on three categories of \nevaluation. The first was a boolean metric based on an exact match between the expected output and the \nmodel-generated output. The second one was an evaluation of the code generation correctness, based on \nthe model’s python code that interacts with the csv data. Thirdly was a human-annotated metric assessing \nthe correctness of the model’s interpretation, with three grades, allowing for partial matches when the \nunderlying intent or reasoning was correctly captured despite minor differences in format or expression. \n \n \n \n \nContent correct of matches \nModel \n% of Exact Match \nOutputs \nCode \nCorrectness \nSatisfactor\ny \nPartially \nSatisfactor\ny \nNot \nSatisfactory \nllama 3 -8B Instruct \n3% \n60% \n7% \n43% \n50% \ngpt_4o_mini \n50% \n73% \n40% \n33% \n27% \nTable 2. Structured data: NL to python results \nb.​ Unstructured data  \nFor the RAG-based pipeline, extraction accuracy was evaluated using a combination of automated and \nhuman-centred metrics. Lexical overlap between the generated responses and the reference information \nwas quantified using ROUGE scores, which provide a standardised measure of surface similarity. \nHowever, given the known limitations of ROUGE in capturing semantic equivalence, negation, and \nfactual correctness; a complementary human-annotated boolean metric was also employed. This manual \nevaluation assessed whether each generated response was correct with respect to the underlying reference \ninformation, thereby providing a clinically meaningful measure of extraction accuracy that is not \ndependent on lexical overlap alone. \n \n \n \nPrecision \nRecall \nF1 Score \nModel \n% of content \ncorrect of \nmatches \nR1 \nR2 \nRL \nR1 \nR2 \nRL \nR1 \nR2 \nRL \nflan-t5-large \n76% \n0.67 \n0.51 \n0.67 \n0.29 \n0.21 \n0.27 \n0.35 0.26 0.35 \ngpt-4o-mini \n78% \n0.61 \n0.44 \n0.56 \n0.51 \n0.39 \n0.48 \n0.50 0.37 0.49 \nTable 3. Unstructured data: RAG Results \nConclusion \nThis study investigated the applicability of Large Language Models to two core Electronic Health Record \ndata science tasks: querying structured clinical data through programmatic interfaces and extracting \n8 \n \n"}, {"page": 9, "text": "clinically relevant information from unstructured free-text records using a Retrieval-Augmented \nGeneration pipeline. By evaluating both tasks within a unified experimental framework and on a shared \nclinical dataset, this work provides a comparative view of LLM capabilities across fundamentally \ndifferent data modalities. \nFor structured data querying, results indicate that LLMs are capable of generating executable Pandas code \nthat supports non-trivial analytical queries over clinical tables. However, performance varied substantially \nacross models. While GPT-4o-mini achieved higher rates of exact-match outputs and code correctness, \nlocally hosted models exhibited lower exact-match accuracy and a higher proportion of partially or \nunsatisfactory outputs. These findings highlight that, although LLMs can translate natural language \nquestions into programmatic queries, reliability remains sensitive to model choice, and exact correctness \ncannot be assumed without validation. \nFor unstructured clinical text, the RAG-based pipeline demonstrated strong potential for supporting \ninformation extraction when retrieval mechanisms were used to ground model outputs. Human-annotated \nevaluations showed comparable levels of content correctness across models, while automated \nROUGE-based metrics provided complementary insights into lexical overlap and coverage. Importantly, \nthe observed discrepancies between ROUGE scores and human judgments reinforce that surface-level \nsimilarity metrics alone are insufficient for assessing factual correctness in clinical narratives, particularly \nin the presence of negation or partial summarisation. \nTaken together, these results suggest that LLMs, when appropriately integrated with retrieval mechanisms \nand programmatic execution environments, can meaningfully support both structured analytics and \nunstructured information extraction in EHR contexts. At the same time, the findings underscore the \nnecessity of task-aligned evaluation strategies that combine exact-match criteria, semantic similarity \nmeasures, and human judgment to ensure clinically meaningful assessment. Future work should explore \nscaling these approaches to broader clinical datasets, refining evaluation frameworks for domain-specific \ncorrectness, and integrating safeguards to support safe and reliable deployment in real-world clinical \nworkflows. \nReferences \nGuevara, M. et al. (2024). Large language models to identify social determinants of health in electronic \nhealth records. npj digital medicine, 7 (1), p.6.  \n \nJohnson, A. E. W., Pollard, T. J., Shen, L., Lehman, L.-W. H., Feng, M., Ghassemi, M., Moody, B., \nSzolovits, P., Celi, L. A., & Mark, R. G. (2016). MIMIC‑III, a freely accessible critical care database. \nScientific Data, 3, 160035. https://doi.org/10.1038/sdata.2016.35   \n \nGunter, T. D., Terry, N. P. (2005). The emergence of national electronic health record architectures in the \nUnited States and Australia: Models, costs, and questions. Journal of Medical Internet Research, 7(1), e3.   \n \n9 \n \n"}, {"page": 10, "text": "Meystre, S. M., Savova, G. K., Kipper-Schuler, K. C., & Hurdle, J. F. (2008). Extracting information from \ntextual documents in the electronic health record: A review of recent research. IMIA Yearbook of Medical \nInformatics, 47–63. \nAlsentzer, E., Rasmussen, M. J., Fontoura, R., Cull, A. L., Beaulieu‑Jones, B., Gray, K. J., Bates, D. W., \n& Kovacheva, V. P. (2023). Zero‑shot interpretable phenotyping of postpartum hemorrhage using large \nlanguage models. npj Digital Medicine, 6, 212. https://doi.org/10.1038/s41746-023-00957-x \n \nIgnashina, M. et al. (2025). LLM Assistance for Pediatric Depression. arXiv [cs.LG]. arXiv   \n \nZiems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., & Yang, D. (2024). Can large language models \ntransform \ncomputational \nsocial \nscience? \nComputational \nLinguistics, \n50(1), \n237–291. \nhttps://doi.org/10.1162/coli_a_00502   \n \nYu, T., Zhang, R., & Reyna, R. (2018). Spider: A large-scale human-labeled dataset for complex and \ncross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on \nEmpirical Methods in Natural Language Processing. \n \nHui, J., Shi, T., Yu, M., & Lin, C.-Y. (2023). Enhancing text-to-SQL performance through schema \ngrounding and constrained decoding. In Proceedings of the 61st Annual Meeting of the Association for \nComputational Linguistics. \n \nJi, Z., Yu, T., Xu, Y., Lee, N., Ishii, E., & Fung, P. (2023). Towards mitigating LLM hallucination via self \nreflection. In Findings of EMNLP 2023 (pp. 1827–1843). Association for Computational Linguistics. \nhttps://doi.org/10.18653/v1/2023.findings-emnlp.123 \n \nRajkumar, N., Li, R., & Bahdanau, D. (2022). Evaluating the Text‑to‑SQL Capabilities of Large Language \nModels. arXiv:2204.00498.     \n \nPourreza, M., & Rafiei, D. (2023). DIN‑SQL: Decomposed In‑Context Learning of Text‑to‑SQL with \nSelf‑Correction. \nIn \nProceedings \nof \nNeurIPS \n2023. \narXiv:2304.11015. \nhttps://doi.org/10.48550/arXiv.2304.11015  \n \nXie, W., Wu, G., & Zhou, B. (2024). MAG‑SQL: Multi‑Agent Generative Approach with Soft Schema \nLinking \nand \nIterative \nSub‑SQL \nRefinement \nfor \nText‑to‑SQL. \narXiv:2408.07930. \nhttps://doi.org/10.48550/arXiv.2408.07930 \n \n10 \n \n"}, {"page": 11, "text": "Asgari, E. et al. (2025). A framework to assess clinical safety and hallucination rates of LLMs for medical \ntext summarisation. Npj Digital Medicine, 8 (1), p.274. \n \nDing, Q. et al. (2025). Building a silver-standard dataset from NICE guidelines for clinical LLMs. arXiv \n[cs.CL]. arXiv [Online].   \n \nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-T., \nRocktäschel, \nT., \nRiedel, \nS. \nand \nKiela, \nD. \n(2021). \nRetrieval-Augmented \nGeneration \nfor \nKnowledge-Intensive NLP Tasks. [online] Available at: https://arxiv.org/pdf/2005.11401. \n \nDouze, M., Alexandr, M., Zilliz, G., Deng, C., Fair, J., Gergely, M., Fair, S., Pierre-Emmanuel, M., Fair, \nM., Lomeli, M., Hosseini, M., Labs, S. and Kyutai, H. (2025). THE FAISS LIBRARY. [online] Available \nat: https://arxiv.org/pdf/2401.08281. \n11 \n \n"}]}