{"doc_id": "arxiv:2511.16544", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.16544.pdf", "meta": {"doc_id": "arxiv:2511.16544", "source": "arxiv", "arxiv_id": "2511.16544", "title": "WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue", "authors": ["Zachary Ellis", "Jared Joselowitz", "Yash Deo", "Yajie He", "Anna Kalygina", "Aisling Higham", "Mana Rahimzadeh", "Yan Jia", "Ibrahim Habli", "Ernest Lim"], "published": "2025-11-20T16:59:20Z", "updated": "2026-01-19T12:48:12Z", "summary": "As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA through DSPy to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's kappa of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.16544v3", "url_pdf": "https://arxiv.org/pdf/2511.16544.pdf", "meta_path": "data/raw/arxiv/meta/2511.16544.json", "sha256": "0b6a721c5944d44bd20fa3c72522505619544bea4abc34d560627af38a669aef", "status": "ok", "fetched_at": "2026-02-18T02:26:34.369385+00:00"}, "pages": [{"page": 1, "text": "WER is Unaware: Assessing How ASR Errors Distort Clinical\nUnderstanding in Patient Facing Dialogue\nZachary Ellis1*, Jared Joselowitz1*, Yash Deo2, Yajie He1,\nAnna Kalygina1, Aisling Higham1,3, Mana Rahimzadeh4, Yan Jia2,\nIbrahim Habli2, Ernest Lim1,2\n1Ufonia Limited, 2University of York, 3Oxford University Hospitals, 4Moorfields Eye Hospital\nCorrespondence: jj@ufonia.com\nAbstract\nAs Automatic Speech Recognition (ASR) is\nincreasingly deployed in clinical dialogue, stan-\ndard evaluations still rely heavily on Word Er-\nror Rate (WER). This paper challenges that\nstandard, investigating whether WER or other\ncommon metrics correlate with the clinical im-\npact of transcription errors. We establish a\ngold-standard benchmark by having expert clin-\nicians compare ground-truth utterances to their\nASR-generated counterparts, labeling the clin-\nical impact of any discrepancies found in two\ndistinct doctor-patient dialogue datasets. Our\nanalysis reveals that WER and a comprehensive\nsuite of existing metrics correlate poorly with\nthe clinician-assigned risk labels (No, Minimal,\nor Significant Impact). To bridge this evalu-\nation gap, we introduce an LLM-as-a-Judge,\nprogrammatically optimized using GEPA to\nreplicate expert clinical assessment. The opti-\nmized judge (Gemini-2.5-Pro) achieves human-\ncomparable performance, obtaining 90% accu-\nracy and a strong Cohen’s κ of 0.816. This\nwork provides a validated, automated frame-\nwork for moving ASR evaluation beyond sim-\nple textual fidelity to a necessary, scalable as-\nsessment of safety in clinical dialogue.\n1\nIntroduction\nPatient-facing clinical dialogue agents are increas-\ningly being deployed into live clinical environ-\nments, automating tasks from documentation to\ndirect consultations (Teo et al., 2025). Their per-\nformance depends critically on Automatic Speech\nRecognition (ASR), the \"ears\" of these clinical\nagents. While significant research has examined\ntext-level hallucinations in generative models (Kim\net al., 2025), the fidelity of the ASR models that\nfeed these models has received far less scrutiny.\nASR systems are typically benchmarked using\nWord Error Rate (WER). However, WER is context-\nagnostic and ill-suited for safety-critical dialogue.\n*Equal contribution.\nIt treats all word errors equally, failing to distin-\nguish between trivial disfluencies and clinically\nhazardous substitutions. For example, a substitu-\ntion that changes \"there is some extra bleeding\" to\n\"there isn’t some extra bleeding\" minimally affects\nWER yet inverts clinical meaning. Even modern se-\nmantic metrics such as BLEURT or BERTScore re-\nmain blind to such risks, rewarding textual similar-\nity while ignoring potential clinical consequences.\nThis paper argues that ASR evaluation in clin-\nical dialogue must evolve towards assessing real\nclinical impact. To bridge this gap, we make three\ncore contributions (Also illustrated in Figure 1):\nA clinician-annotated benchmark for ASR\nclinical impact. We define a three-point scale for\nclinical distortion and recruit expert clinicians to\nannotate mistranscriptions from two doctor–patient\ndatasets; one proprietary and one open-source span-\nning two ASR systems (Google Chirp and Deep-\ngram Nova-3), yielding a diverse, high-quality\ndataset of clinically rated ASR errors.\nA robust LLM-based turn aligner. We out-\nline that traditional alignment methods fail under\ninconsistent segmentation and semantic ambiguity\nacross ASR providers. Our LLM aligner reasons\njointly over meaning, context, and sequence, en-\nsuring accurate pairing of ground-truth and ASR\nutterances for turn-level comparison.\nA validated LLM-as-a-judge for context-\nsensitive clinical risk assessment.\nUsing our\ndataset, we show that WER and existing semantic\nmetrics correlate poorly with expert-assigned clini-\ncal impact. We then optimize an LLM-based eval-\nuator (Gemini-2.5-Pro) via GEPA, achieving 90%\naccuracy (Cohen’s κ of 0.816), human-comparable\nperformance for scalable clinical safety evaluation.\nThese contributions provide a concrete step to-\nwards risk-informed, context-sensitive evaluations\nfor the development of safer clinical dialogue sys-\ntems.\n1\narXiv:2511.16544v3  [cs.CL]  19 Jan 2026\n"}, {"page": 2, "text": "Figure 1: Overview of the clinical impact evaluation framework. Left: Two examples of ASR errors in patient\nutterances. Middle: We curate a dataset of clinical dialogues and transcriptions, and apply a novel semantically-\naware sentence alignment pipeline to enable contextual clinical evaluation. Expert clinicians annotate a dataset\nof these errors based on our defined scale, labelling the minor change (Ex. 1) as \"Insignificant\" but the clinically\ndangerous negation (Ex. 2) as \"Impactful\". Right: Existing metrics like WER and other semantic scores correlate\npoorly with clinical risk. Our GEPA-optimized LLM-as-a-Judge closely matches clinical expert ratings.\n2\nBackground and Related Works\n2.1\nLimitations of WER in Clinical Contexts\nThe standard metric in ASR evaluation, WER, is\nfundamentally limited for safety-critical domains\nlike clinical dialogue (Sasindran et al., 2024). As a\ncontext-agnostic measure of lexical fidelity (substi-\ntutions, deletions, insertions) (Likhomanenko et al.,\n2021), WER overlooks semantic accuracy which is\ncritical in clinical settings where a single misrec-\nognized negation or medication name can reverse\nmeaning and cause severe clinical harm despite a\nlow WER (Sasindran et al., 2024). Moreover, ASR\nmodels optimized for specific benchmarks often\nshow substantially higher WERs in conversational\nor multi-speaker contexts (Likhomanenko et al.,\n2021), revealing domain variability and the contin-\nued need for post-editing for clinical transcriptions.\nThese findings underscore the need for evaluation\nmethods that capture not just textual accuracy but\nalso preservation of clinical meaning.\n2.2\nBeyond Lexical Fidelity: Semantic and\nHybrid ASR Evaluation\nTo overcome the limitations of lexical fidelity, re-\ncent work focuses on semantic fidelity - measuring\nthe meaning-level distance between reference and\nhypothesis texts. Early embedding-based methods\nlike Semantic Distance (Kim et al., 2021) use vec-\ntor representations to quantify similarity, demon-\nstrating better alignment with human perception of\nquality than WER.\nMore sophisticated hybrid metrics integrate both\nerror quantification and semantic scoring. Clinical\nBERTScore (Shor et al., 2023) conducts utterance-\nlevel analysis validated against clinician prefer-\nences, showing improved performance over stan-\ndard WER in specialized, non-conversational set-\ntings.\nSimilarly, SeMaScore (Sasindran et al.,\n2024) combines error rates with segment-wise se-\nmantic similarity, yielding stronger correlations\nwith expert judgments, even in noisy speech.\nThese semantic and hybrid metrics show\nstronger correlation with human judgments of in-\ntelligibility and correctability. Metrics like Human\nPerceived Accuracy and integrated weighted com-\nbinations (e.g., phonetic, semantic, NLI features)\nachieve better correlations than WER for these do-\nmains (Mishra et al., 2011; Phukon et al., 2025).\nWhile these metrics indicate progress, they prior-\nitize semantic resemblance rather than clinical im-\npact. Changes that alter symptom severity (\"some\nmild pain\" →\"no mild pain\") have vastly differ-\nent clinical implications yet metrics like cosine\nsimilarity may fail to capture the consequences of\nthese differences. Thus, even meaning-aware met-\n2\n"}, {"page": 3, "text": "rics may fail to distinguish clinically consequential\nerrors from inconsequential ones. Furthermore,\npre-trained embeddings are robust to ASR errors\nwhen predicting user ratings of dialogue systems,\nsuggesting limited sensitivity to clinically relevant\nASR distortion (Georgila, 2024). As a result, ex-\nisting metrics (whether edit-distance, n-gram, or\nsemantic) remain poor proxies for clinical impact,\nas they measure textual divergence rather than its ef-\nfect on clinical understanding or decision-making.\nTo address this gap, our work moves beyond\nlinguistic similarity toward clinically aware ASR\nevaluation: quantifying the magnitude of clinical\ndistortion caused by transcription errors. We em-\npirically test how traditional metrics from these\nthree families (edit-distance, overlap-based, and\nsemantic) align with expert clinical judgments and\npropose an LLM-based evaluator that better reflects\nthe actual clinical consequences of misrecognition.\n2.3\nLimitations of Traditional Methods for\nSentence-Level Alignment\nAccurately pairing ground-truth clinical utterances\nwith their ASR outputs is essential for valid evalua-\ntion, yet conventional alignment methods often fail\nunder the messy, overlapping conditions of real-\nworld dialogue and the inconsistent segmentation\nproduced by different ASR systems.\nTimestamp-Proximity Alignment.\nSimple\nproximity matching pairs each gold utterance with\nthe nearest ASR hypothesis, but noisy or drifting\ntimestamps often cause swapped or merged seg-\nments (Bain et al., 2023). Dynamic Time Warping\nmitigates rate differences by minimizing cumula-\ntive time distance, yet non-monotonic or inaccurate\ntimestamps yield locally coherent but semantically\nincorrect alignments, especially when utterances\nare split or merged (Jiang et al., 2020).\nText-Based Alignment.\nEdit-distance algo-\nrithms like Needleman–Wunsch (Needleman and\nWunsch, 1970) and Smith–Waterman (Smith and\nWaterman, 1981) align tokens by lexical similar-\nity alone, ignoring timing and context but they fail\nwhen sentence boundaries diverge or ASR outputs\ncontain paraphrases and disfluencies, producing un-\nstable or crossing alignments (Snover et al., 2006).\nEmbedding-Based Similarity. Sentence em-\nbeddings (e.g., SBERT) align semantically simi-\nlar utterances (Reimers and Gurevych, 2019a), but\nshort backchannels (e.g., \"yes\", \"okay\") merge into\nsimilar vectors, domain-specific terms are under\nrepresented (Zheng et al., 2021), and ignoring se-\nquence order allows semantically plausible yet posi-\ntionally inconsistent matches (Liu and Zhu, 2022).\nThese limitations motivate a context-aware ap-\nproach that integrates semantic and sequential rea-\nsoning. We therefore introduce an LLM-based\naligner that robustly maps utterances across frag-\nmented or merged ASR outputs, forming a reliable\nfoundation for our clinical impact benchmark.\n2.4\nLarge Language Models as Judges for\nClinical Impact\nThe LLM-as-a-Judge framework provides a\npromising solution for providing nuanced and con-\ntextually aware evaluations of transcription quality,\nmoving beyond the limits of static metrics (Gu\net al., 2024; Pulikodan et al., 2025). LLMs have\nbeen leveraged to assess the severity and nature\nof transcription errors, a process that is essential\nfor operational risk monitoring. Domain-specific\nadaptations, like Significant ASR Error Detection\n(SASRED) (Harvill et al., 2024), classify errors as\nSignificant (altering key entities or actions) or Non-\nSignificant (minor surface changes on an Amazon\nAlexa general diaogue dataset). However, whilst\nshowing promise with non-expert human evalua-\ntors (Li et al., 2024), these model have limitations\nand variable validation for expert tasks, particularly\nin healthcare (Szymanski et al., 2025).\nThese LLM-based frameworks are often tailored\nto assess critical healthcare dimensions such as\nfactual correctness, clinical utility, and logical co-\nherence to ensure outputs are safe and align with\nclinical workflow standards (Croxford et al., 2025).\nBuilding on these advances, our methodology tasks\nexpert clinicians, and a subsequent LLM judge,\nwith evaluating transcription errors based on their\ndirect impact on the clinical understanding of a\npatient’s condition, and subsequent risk changes.\n3\nMethods\n3.1\nProgrammatic Alignment of\nGround-Truth and Hypothesis Utterances\nTo handle segmentation and semantic drift across\nASR providers, we employ an LLM-based aligner\nthat performs semantic and structural sentence-\nlevel alignment between gold and ASR utterances,\ninstead of relying on time or token matching.\n3.1.1\nPrompt Design\nEach conversation contained two ordered se-\nquences: (1) a gold transcript of verified patient\n3\n"}, {"page": 4, "text": "utterances with timestamps, and (2) an ASR hy-\npothesis of recognized segments with confidence\nscores. The LLM aligned each gold utterance Gi\nto one or more ASR hypotheses Aj under the fol-\nlowing constraints: each ASR segment could be\nmatched once; consecutive segment could merge\nif forming a single utterance; and consecutive gold\nutterances could map jointly if merged by the rec-\nognizer. The model considered semantic similarity,\nsequential order, and ASR confidence without intro-\nducing new text (prompt provided in Appendix A).\nGemini-2.5-Pro was used with conservative de-\ncoding parameters (temperature = 0.1, top-p = 0.95,\ntop-k = 40) to ensure stable long-context outputs\n(up to 65k tokens). It produced structured JSON\nalignments specifying indices, match types (exact,\nfuzzy, missing), and similarity scores.\n3.1.2\nPost-Processing and Refinement\nRaw alignments were parsed into structured objects\nand refined through deterministic rules to ensure\nvalidity and robustness: (1) duplicate correction\nmerged consecutive gold segments sharing iden-\ntical ASR text; (2) miss recovery re-evaluated\nunmatched gold utterances against unused ASR\nhypotheses (lexical similarity ≥0.65); and (3)\nmulti-fragment reconstruction combined gold\nutterances spanning consecutive ASR fragments,\naveraging confidence and timestamps.\nThis hybrid design combines the LLM’s rea-\nsoning with deterministic corrections, producing\ncontent-aware, sequence-consistent alignments re-\nsilient to real-world ASR behavior (fragmented\nor merged outputs). The final alignments, anno-\ntated with similarity scores, match types, and multi-\nfragment indicators, were saved as structured JSON\nfor downstream evaluation. Worked example of the\nalignment can be seen in Appendix B.\n3.2\nClinician Labelling of Meaning Change\nand Clinical Impact\nTo evaluate the clinical impact of ASR errors, two\nclinician annotators independently labelled a strat-\nified sample of patient utterances (the Clinical\nSubset) from post-operative cataract and general-\npractice consultations. Each example contained a\nshort dialogue segment where only the patient’s\nfinal utterance differed between the ground-truth\nand ASR transcription. Annotators compared these\npaired versions and judged whether the transcrip-\ntion error altered the perceived clinical meaning\nof the exchange. For each instance, clinicians an-\nswered the following question:\n“If uncorrected, and if you could only\nread the transcription alone, would it\nhave changed your understanding of the\npatient’s clinical condition?”\nThey assigned one of three ordinal labels reflect-\ning the magnitude of clinical distortion:\n• 0 – No change in understanding of the pa-\ntient’s clinical condition\n• 1 – Change in understanding with minimal\nclinical impact\n• 2 – Change in understanding with significant\nclinical impact\nFull task instructions and clinician background\nare outlined in Appendix C. Brief justifications\nwere also recorded to capture reasoning and high-\nlight borderline cases. These annotations formed\nthe reference set for subsequent metric develop-\nment and correlation analysis.\n3.2.1\nClinician Inter-Annotator Agreement\nInter-annotator agreement (IAA) was assessed on\nthe full labelled Clinical Subset using Cohen’s κ\nand raw percentage agreement. Figure 2 shows the\nagreement per class and the final adjudicated label\ndistribution. Overall agreement was 79% (κ =\n0.54), indicating moderate agreement. Notably, the\nmajority of disagreements occurred between the\n’No Impact’ (0) and ’Minimal Impact’ (1) classes,\nhighlighting the inherent subjectivity and nuance\nin distinguishing cosmetic errors from those with\nminor clinical significance. Following the initial\nround, the annotators met to resolve disagreements,\nproducing a reconciled gold-standard set.\n3.3\nLLM-as-a-Judge Training\nImplementation.\nThe LLM judge was imple-\nmented using DSPy (Khattab et al., 2024), a frame-\nwork for programmatic prompt optimization. The\njudge is given a ground truth conversation and and\nASR hypothesis as input, and outputs a clinical\nimpact assessment with reasoning.\nPrompt Optimization via GEPA. Rather than\nmanually engineering prompts, we used GEPA\n(Genetic-Pareto) to automatically optimize the\njudge’s instructions (Agrawal et al., 2025). GEPA\nemploys a reflective prompt evolution strategy that\nleverages LLM introspection to iteratively improve\nprompts based on observed failures.\n4\n"}, {"page": 5, "text": "Figure 2: Clinician annotation agreement and final label\ndistribution. Left: IAA between two clinicians on the\nfull labelled subset (n = 298), with most disagreements\nbetween adjacent classes (0 vs. 1), yielding 79% agree-\nment (κ = 0.54). Right: Final adjudicated labels show\na predominance of no-impact cases, with fewer minimal\nand significant-impact examples.\nThe optimization process operates as follows:\n(1) the current prompt is evaluated on minibatches\nof training examples (batch size = 3); (2) incor-\nrect predictions trigger generation of rich textual\nfeedback describing the narure and severity; (3) a\nreflection LM uses the current prompt, failed exam-\nples, and feedback to generate multiple candidate\nimproved prompts; (4) candidates are evaluated\non a validation set and selected via Pareto frontier\noptimization to maintain diverse high-performing\nstrategies; (5) the process iterates until convergence.\nChain-of-Thought approach was used to encourage\nstep-by-step reasoning before classification.\nDataset Split and Evaluation Metric. From\nthe 298 labelled conversation pairs of the Clinical\nSubset, we created a stratified split of 218 train-\ning, 30 validation, and 50 test examples, preserving\nclass distributions. To encourage clinically mean-\ningful optimization, we used a custom cost matrix\nC where C[i, j] represents the reward or penalty\nfor predicting class j when the true class is i. The\nmatrix heavily penalized missed critical errors (e.g.,\nC[2, 0] = −1.2) and applied smaller penalties for\nadjacent-class confusions. Combined with GEPA’s\ntextual feedback, this cost-sensitive setup enabled\nlearning of the relative importance of error types.\nThe cost matrix is shown in Appendix D.\nModel Configuration. We used Gemini-2.5-Pro\nfor both executing clinical assessments and gener-\nating improved prompts during GEPA’s reflection\nphase. This configuration allowed the model to\nboth perform the judgment task and introspect on\nits failures to propose improvements. The GEPA\noptimizer was configured with auto=’medium’,\nPareto-based candidate selection, and was set to\nskip examples achieving perfect scores to focus\ncomputational resources on challenging cases. The\nfinal prompt can be seen in Appendix E.\n3.4\nExisting Metric Evaluation\nWe benchmarked three ASR metric families against\nclinician-assigned risk labels, using aligned ground-\ntruth and hypothesis pairs. A full list of evaluated\nmetrics is provided in Appendix F.\nEdit-distance metrics (e.g., WER, Character\nError Rate (CER)) measure minimal token or\ncharacter-level edits between reference and hypoth-\nesis. The N-gram overlap metrics (e.g., BLEU,\nROUGE) capture lexical overlap through contigu-\nous n-grams. Finally, the learned semantic met-\nrics (e.g., BERTScore, BLEURT) use pretrained\nneural models to assess meaning preservation.\nEnrichment–delta analysis was performed,\nwith all scores normalized as 1 −error rate so\nhigher values indicate better performance. For each\nmetric, we computed the mean score difference be-\ntween clinically safe (y=0) and high-impact (y=2)\ntranscripts, ∆= E[s | y=2] −E[s | y=0], to quan-\ntify sensitivity to clinical severity. This measure\ncaptures whether higher-risk cases receive system-\natically lower quality scores, indicating alignment\nbetween metric sensitivity and clinical relevance.\n4\nData\nWe curated two complementary datasets of real\ndoctor–patient conversations, differing in domain\nand ASR provider, to robustly evaluate how ASR\nmistranscriptions distort clinical meaning.\n4.1\nSources\nBoth datasets contain English-language doctor-\npatient conversations. For the analysis, only the\npatient’s speech is used.\nDora comprises 21 anonymized production calls\nfrom a proprietary telehealth service, Ufonia Lim-\nited (Ufonia Ltd, 2025), capturing naturally occur-\nring mistranscriptions in a live clinical environment\nrather than data engineered for this study. These\nroutine post-operative cataract consultations were\nconducted by an LLM-based conversational agent.\nPrimock57 is an open-source set of 21 mock\nprimary-care dialogues (Sarac et al., 2022). 1\n1All accompanying code and the clinician-labelled Pri-\nmock57 Clinical subsets is publicly released at https://\ngithub.com/Ufonia/wer-is-unaware. The Dora data orig-\ninates from a proprietary internal dataset and cannot be shared.\n5\n"}, {"page": 6, "text": "The combination provides both proprietary and\npublic data analysis, spanning distinct clinical path-\nways, recording conditions, and ASR providers.\n4.2\nTransformation\nAll audio was transcribed to produce ground-truth\n(GT) references and corresponding ASR hypothe-\nses for utterance-level comparison.\nGround-Truth Transcription.\nFor Dora,\nGT transcripts were created using an human–AI\npipeline shown to yield fast, high-accuracy tran-\nscriptions (Liu et al., 2022; Yuan et al., 2021a).\nGemini-2.5-Pro generated initial transcripts from\npatient audio, which human annotators then veri-\nfied and corrected. For Primock57, we used the pro-\nvided human transcriptions as GT. In both datasets,\nadjacent utterances by the same speaker were con-\ncatenated into a single, continuous turn.\nAutomatic Transcription. To capture variation\nacross commercial systems, Dora audio was tran-\nscribed using Google Chirp and Primock57 using\nDeepgram Nova-3, reflecting diversity in ASR out-\nput and segmentation behaviors. Due to cost and\ntime constraints, only one ASR provider was used\nper dataset. Systematic cross-provider comparisons\non the same data are left to future work.\nUtterance Alignment. Each dataset was de-\ncomposed into aligned pairs of patient GT utter-\nances and ASR hypotheses using the LLM-based\nsemantic aligner (Section 3.1), ensuring consistent\npairing despite provider-level segmentation differ-\nences. For each target utterance, the preceding two\ndoctor turns and the most recent patient turn were\nappended to preserve conversational context for\nlater clinical annotation. Summary statistics, in-\ncluding WER distribution and average utterance\nlength, are provided in Appendix G.\n4.3\nCuration\nAligned patient utterances were curated into a\nclinician-labelling sample.\nAfter text cleaning\n(Appendix H), WER was computed between\nground-truth and ASR pairs, and perfect matches\n(WER = 0) were excluded. Random sampling\nfrom both datasets ensured diversity across speak-\ners and call types. To achieve a balanced range of\ntranscription quality, utterances with higher error\nrates (WER ∈[0.4, 1)) were selectively included\nfrom Primock57. Each pair was manually checked\nfor correct alignment, and any misaligned exam-\nples were removed. This curation process yielded\na Clinical Subset dataset of 298 examples which\nwas used for clinician labelling (Sec. 3.2) and the\ntraining and testing of the LLM Judge (Sec. 3.3).\nFor the existing metrics evaluation (Sec. 3.4),\nwe additionally filtered out non-lexical tokens, as\ndetailed in Appendix H. Twenty of the 298 Clinical\nSubset pairs became perfect matches (WER = 0)\ndiffering only by these tokens, and were excluded\nfrom the existing metrics evaluation (Sec. 5.2); this\nyielded the Metrics Subset. The statistics of both\nsubsets are provided in Table 1.\nSubset\nSource\n# Calls\n# Utterances\nAvg. Words/Utt.\nAvg. WER\nClinical\nDora\n21\n123\n9.28\n0.51\nPrimock57\n21\n175\n12.7\n0.50\nTotal\n42\n298\n11.29\n0.51\nMetrics\nDora\n21\n121\n9.03\n0.53\nPrimock57\n21\n157\n12.52\n0.51\nTotal\n42\n278\n10.99\n0.52\nTable 1: Datasets used in this study. Statistics are shown\nfor both the Clinical and Metrics subsets; non-lexical\ntokens are filtered only for the Metrics Subset.\n5\nResults\n5.1\nLLM-Based Aligner\nTo ensure the validity of our downstream clinical\nimpact analysis, we first evaluated the LLM-based\nalignment system. The accuracy of this component\nis critical, as alignment errors would invalidate the\ncomparisons made by clinician annotators.\n5.1.1\nGold-Standard Alignment Dataset\nA human annotator manually aligned patient utter-\nances from a subset of 13 conversations; 7 tran-\nscribed with Google Chirp and 6 with Deepgram\nNova-3. The dataset contains 463 ground-truth\nutterances and 445 ASR hypotheses, with each\ngold utterance mapped to its correct ASR counter-\npart(s). The annotator labeled one-to-one, one-to-\nmany (merges), many-to-one (splits), and zero-to-\none (missed) mappings. This dataset served as the\ngold standard for the alignment evaluations.\n5.1.2\nEvaluation Metrics\nWe evaluate the LLM-based transcript aligner using\ntwo complementary metrics. Classification Accu-\nracy assesses whether the aligner correctly iden-\ntifies if an utterance has a corresponding segment\nin the other transcript. For each of the 463 ground-\ntruth and 445 ASR utterances, this is treated as\na binary classification task: correctly labeling an\nutterance as matched or unmatched (missed). Er-\nrors include false positives (incorrectly labeling\na match as a miss) and false negatives (failing to\n6\n"}, {"page": 7, "text": "detect a true miss). Structural Alignment Accu-\nracy provides a stricter, mapping-level evaluation.\nIt measures the percentage of ground-truth utter-\nances that were mapped to the exact same ASR\nutterance index (or indices) as specified in the gold-\nstandard annotation. This metric is sensitive to\nstructural errors like boundary shifts, mis-merges,\nor the incorrect use of a duplicate ASR segment\n(see Appendix B.2 for a worked example).\n5.1.3\nPerformance Results\nThe LLM-based aligner achieved high, system-\nagnostic performance.\nFor Classification Accu-\nracy, results were 98.9% on gold utterances and\n98.0% on ASR utterances (Figure 3). Misclassifica-\ntions were minimal with one false unmatched case\non the golden side for Google, and five for Deep-\ngram, with similarly low counts for ASR results.\nFor Structural Alignment Accuracy, performance\nremained strong (96.4%). Minor discrepancies\nstemmed from boundary drift in long utterances or\nduplicated ASR fragments, none of which affected\nclinical meaning in downstream comparisons.\nOverall, these results confirm that the LLM-\nbased aligner is robust and accurate enough, pro-\nviding a reliable foundation for subsequent clinical\nimpact annotation.\nFigure 3: Performance of the LLM-based transcript\naligner across Google (Dora) and Deepgram (Primock)\nASR hypotheses. The figure shows high classification\naccuracy (> 98%) and low total error counts for both\ngolden and ASR utterances.\n5.2\nExisting Metric Evaluation\nExisting metrics correlate poorly with clinical la-\nbels for risk.\nAcross all metrics, the enrichment–\ndelta analysis (Figure 4) revealed that score differ-\nences between high–impact (y=2) and safe (y=0)\ntranscriptions were generally small, confirming\nthat most conventional text metrics only weakly\ntrack clinically meaningful errors. Among fam-\nilies, learned semantic metrics (e.g., BLEURT,\nSBERT, NLI models) showed the strongest and\nmost consistent alignment with clinical risk, with\nclearer score separation between safe and high-\nimpact cases and more negative enrichment deltas,\nindicating that lower scores generally corresponded\nto higher clinical severity. Edit-distance metrics\n(WER, CER, etc.) exhibited moderate but less\nstable associations, while N-gram overlap metrics\n(BLEU, ROUGE, METEOR) provided the weakest\ndiscrimination, with high overlap in scores across\nall clinical categories. A table of results, a Kendall\ncorrelation, and an example qualitative error analy-\nsis are provided in Appendix I.\nOverall, the results suggest that while seman-\ntic metrics are relatively better proxies for clinical\nreliability, no existing metric family reliably re-\nflects real clinical impact, underscoring the need\nfor domain-aware evaluation frameworks.\nFigure 4: Mean score difference per metric on the Met-\nrics Subset, coloured by family; more negative bars\nindicate stronger alignment with clinical severity.\n5.3\nLLM-as-a-Judge Automation\nGemini-2.5-Pro outperforms other state-of-the-\nart LLMs.\nTo validate our model selection, we\nbenchmarked the final GEPA-optimized prompt\nacross a suite of leading open source and propri-\netry models. The full comparison, detailed in Ap-\npendix J.1, shows that Gemini-2.5-Pro achieved a\nmean Macro F1 of 0.825 and Cohen’s κ of 0.790.\nThis establishes a clear performance advantage\nover all other tested models, justifying its use.\nThe Judge excels in ambiguous cases.\nA granu-\nlar per-class F1 analysis (Appendix J.1) reveals that\nwhile most models perform adequately on clear-cut\n’No Impact’ (Class 0) or ’Significant Impact’ (Class\n7\n"}, {"page": 8, "text": "2) cases, they consistently fail on the nuanced ’Min-\nimal Impact’ (Class 1) category. This reflects the\ndifficulty of this class, which also proved most chal-\nlenging for human annotators (Figure 5). Gemini-\n2.5-Pro was the only model to achieve an F1 score\n> 0.5 (it got 0.655) for this difficult class, demon-\nstrating a superior capacity for nuanced clinical\nassessment (Figure 8).\nJudge achieves human-comparable perfor-\nmance and agreement.\nThe LLM Judge’s per-\nformance is comparable to its human expert\ncounterparts. From Table 2, its 90% accuracy\n(κ = 0.816), places it between the two expert an-\nnotators (Clinician A: 94%; Clinician B: 80%).\nFurthermore, from Table 11, the Judge’s agree-\nment patterns mirrors expert reliability. Its pairwise\nκ with Clinician A (0.713) and Clinician B (0.497)\nis consistent with the inter-clinician κ of 0.505.\nThis demonstrates the Judge successfully operates\nwithin the same range of expert subjectivity.\nComparison\nAcc (95% CI)\nCohens κ (95% CI)\nLLM Judge vs Gold\n90% [82.0-96.0]\n0.816 [0.649-0.933]\nClinician A vs Gold\n94% [88.0-100.0]\n0.891 [0.764-1.000]\nClinician B vs Gold\n80% [68.0-90.0]\n0.567 [0.336-0.767]\nTable 2: Agreement with gold-standard labels across\n50 cases with 95% confidence interval estimated via\n1,000 bootstrap iterations. The LLM Judge shows high\nalignment with human clinicians.\nThe Judge mirrors the stronger clinician across\nclasses, with greatest uncertainty on the minor-\nity class.\nBeyond aggregate scores, the Judge’s\nper-class F1 performance (Figure 5) closely tracks\nthat of the stronger human annotator. The Judge\nachieves 95.1% on No Impact, 76.9% on Minimal\nImpact, and 84.6% on Significant Impact, com-\npared with Clinician A’s 98.4% / 80.0% / 91.7%\nand Clinician B’s 88.6% / 28.6% / 69.6%. Both\nthe Judge and Clinician A perform nearly perfectly\non clear-cut No Impact cases, show moderate de-\ncline on Significant Impact, and exhibit the greatest\nvariability on the ambiguous Minimal Impact class,\nreflecting its inherent difficulty.\n6\nDiscussion\nOur findings highlight the critical gap between\nexisting ASR evaluation and clinical safety. We\ndemonstrate that metrics must move beyond tex-\ntual fidelity (e.g. WER), and even semantic fidelity,\nboth insufficient proxies for risk for clinical dia-\nlogue tasks, and therefore falling short for required\nFigure 5: Per class Test set results of clinicians and\njudge. 95% confidence interval estimated via 1,000\nbootstrap iterations\nsafety evidence for regulated medical devices (Te-\nferra, 2017). Our LLM-judge closes this gap by\nachieving human-comparable performance in a\nchallenging and nuanced task, using anonymized\nreal-world data from production use of a conversa-\ntional system, supported by an open-sourced pri-\nmary care dataset.\nFurthermore, programmatic optimization via\nGEPA yields not only a high-performing judge\nbut also a reproducible, auditable artefact for the\nprompt tuning process. Unlike manual prompt en-\ngineering, GEPA’s training process enables align-\nment with best-practice AI governance require-\nments in medicine, established for more traditional\nML systems (Gallifant et al., 2025; Ganapathi et al.,\n2022). Additionally, this analysis was enabled by\nthe LLM-based sentence aligner, which ensured ro-\nbust utterance-level pairing between ground-truth\nand ASR transcripts despite segmentation drift or\nmerged utterances.\nLimitations include the benchmark’s moderate\nsize (n = 298), and the initial focus on a smaller set\nof clinical domains. Future work should expand the\nevaluation to more clinical pathways and involve a\nlarger, more diverse group of clinical labellers.\n7\nConclusion\nStandard ASR evaluation fails to ensure patient\nsafety. We show empirically that existing metrics\nlike WER are insufficient, and introduce an expert-\nannotated benchmark and a validated LLM Judge\nthat achieves human-level accuracy in classifying\nclinical risk. Together, these contributions estab-\nlish the first scalable framework for certifying the\nclinical safety of transcription systems in conversa-\ntional clinical dialogues, enabling their responsible\ndevelopment and deployment in healthcare.\n8\n"}, {"page": 9, "text": "References\nLakshya A Agrawal, Shangyin Tan, Dilara Soylu,\nNoah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav\nSinghvi, Herumb Shandilya, Michael J Ryan, Meng\nJiang, Christopher Potts, Koushik Sen, Alexandros G.\nDimakis, Ion Stoica, Dan Klein, Matei Zaharia, and\nOmar Khattab. 2025. Gepa: Reflective prompt evolu-\ntion can outperform reinforcement learning. Preprint,\narXiv:2507.19457.\nMax Bain, Jaesung Huh, Tengda Han, and Andrew Zis-\nserman. 2023. Whisperx: Time-accurate speech tran-\nscription of long-form audio. In Proceedings of In-\nterspeech 2023.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved cor-\nrelation with human judgments. In Proceedings of\nthe acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summariza-\ntion, pages 65–72.\nAsma Ben Abacha, Wen-wai Yim, George Michalopou-\nlos, and Thomas Lin. 2023. An investigation of eval-\nuation methods in automatic medical note generation.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 2575–2588, Toronto,\nCanada. Association for Computational Linguistics.\nEmma Croxford, Yanjun Gao, Elliot First, Nicholas\nPellegrino, Miranda Schnier, John Caskey, Madeline\nOguss, Graham Wills, Guanhua Chen, Dmitriy Dli-\ngach, et al. 2025. Automating evaluation of ai text\ngeneration in healthcare with a large language model\n(llm)-as-a-judge. medRxiv, pages 2025–04.\nJack Gallifant, Majid Afshar, Saleem Ameen, Yindalon\nAphinyanaphongs, Shan Chen, Giovanni Cacciamani,\nDina Demner-Fushman, Dmitriy Dligach, Roxana\nDaneshjou, Chrystinne Fernandes, et al. 2025. The\ntripod-llm reporting guideline for studies using large\nlanguage models. Nature medicine, 31(1):60–69.\nShaswath Ganapathi, Jo Palmer, Joseph E Alderman,\nMelanie Calvert, Cyrus Espinoza, Jacqui Gath,\nMarzyeh Ghassemi, Katherine Heller, Francis Mckay,\nAlan Karthikesalingam, et al. 2022. Tackling bias\nin ai health datasets through the standing together\ninitiative. Nature medicine, 28(11):2232–2233.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. arXiv preprint arXiv:2104.08821.\nKallirroi Georgila. 2024.\nComparing pre-trained\nembeddings and domain-independent features for\nregression-based evaluation of task-oriented dialogue\nsystems. In Proceedings of the 25th Annual Meeting\nof the Special Interest Group on Discourse and Dia-\nlogue, pages 610–623, Kyoto, Japan. Association for\nComputational Linguistics.\nJiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan,\nXuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen,\nShengjie Ma, Honghao Liu, et al. 2024. A survey on\nllm-as-a-judge. arXiv preprint arXiv:2411.15594.\nJohn Harvill, Rinat Khaziev, Scarlett Li, Randy\nCogill, Lidan Wang, Gopinath Chennupati, and Hari\nThadakamalla. 2024.\nSignificant asr error detec-\ntion for conversational voice assistants. In ICASSP\n2024-2024 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n11606–11610. IEEE.\nYihang Jiang, Yuankai Qi, Will Ke Wang, Brinnae Bent,\nRobert Avram, Jeffrey Olgin, and Jessilyn Dunn.\n2020.\nEventDTW: An Improved Dynamic Time\nWarping Algorithm for Aligning Signals with Un-\neven Sampling Frequencies. Sensors, 20(10):2700.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari,\nZhiyuan Zhang, Keshav Santhanam, Sri Vard-\nhamanan, Saiful Haq, Ashutosh Sharma, Thomas T.\nJoshi, Hanna Moazam, Heather Miller, Matei Za-\nharia, and Christopher Potts. 2024. Dspy: Compiling\ndeclarative language model calls into self-improving\npipelines.\nSuyoun Kim, Abhinav Arora, Duc Le, Ching-Feng Yeh,\nChristian Fuegen, Ozlem Kalinli, and Michael L.\nSeltzer. 2021. Semantic distance: A new metric for\nasr performance analysis towards spoken language\nunderstanding. Preprint, arXiv:2104.02138.\nYubin Kim, Hyewon Jeong, Shan Chen, Shuyue Stella\nLi, Mingyu Lu, Kumail Alhamoud, Jimin Mun,\nCristina Grau, Minseok Jung, Rodrigo Gameiro, et al.\n2025. Medical hallucinations in foundation mod-\nels and their impact on healthcare. arXiv preprint\narXiv:2503.05777.\nHaitao Li, Qian Dong, Junjie Chen, Huixue Su, Yu-\njia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu.\n2024.\nLlms-as-judges: a comprehensive survey\non llm-based evaluation methods. arXiv preprint\narXiv:2412.05579.\nTatiana Likhomanenko, Qiantong Xu, Vineel Pratap,\nPaden Tomasello, Jacob Kahn, Gilad Avidov, Ronan\nCollobert, and Gabriel Synnaeve. 2021. Rethinking\nevaluation in asr: Are our models robust enough?\nPreprint, arXiv:2010.11745.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries.\nIn Text summarization\nbranches out, pages 74–81.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022. WANLI: Worker and AI collabora-\ntion for natural language inference dataset creation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 6826–6847, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nWei Liu and Chenhui Zhu. 2022.\nBertalign: High-\nquality sentence alignment for Chinese-English par-\nallel corpora of literary texts. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 164–175, Gyeongju, Republic of Ko-\nrea. International Committee on Computational Lin-\nguistics.\n9\n"}, {"page": 10, "text": "Taniya Mishra, Andrej Ljolje, and Mazin Gilbert. 2011.\nPredicting human perceived accuracy of asr systems.\nIn INTERSPEECH, pages 1945–1948.\nSaul B. Needleman and Christian D. Wunsch. 1970.\nA general method applicable to the search for simi-\nlarities in the amino acid sequence of two proteins.\nJournal of Molecular Biology, 48(3):443–453.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nBornali Phukon, Xiuwen Zheng, and Mark Hasegawa-\nJohnson. 2025. Aligning asr evaluation with human\nand llm judgments: Intelligibility metrics using pho-\nnetic, semantic, and nli approaches. arXiv preprint\narXiv:2506.16528.\nMaja Popovi´c. 2015. chrf: character n-gram f-score for\nautomatic mt evaluation. In Proceedings of the tenth\nworkshop on statistical machine translation, pages\n392–395.\nSujith Pulikodan, Prasanta Kumar Ghosh, Visruth\nSanka, Nihar Desai, et al. 2025. An approach to mea-\nsuring the performance of automatic speech recog-\nnition (asr) models in the context of large language\nmodel (llm) powered applications. arXiv preprint\narXiv:2507.16456.\nNils Reimers and Iryna Gurevych. 2019a. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nNils Reimers and Iryna Gurevych. 2019b. Sentence-\nbert:\nSentence embeddings using siamese bert-\nnetworks. arXiv preprint arXiv:1908.10084.\nSomnath Roy. 2021. Semantic-wer: A unified metric\nfor the evaluation of asr transcript for end usability.\nPreprint, arXiv:2106.02016.\nSam O’Connor Russell, Iona Gessinger, Anna Krason,\nGabriella Vigliocco, and Naomi Harte. 2024. What\nautomatic speech recognition can and cannot do for\nconversational speech transcription. Research Meth-\nods in Applied Linguistics, 3(3):100163.\nRadmila Sarac, Francesco Moramarco, Alex Papadopou-\nlos Korfiatis, and Aleksandar Savkov. 2022.\nPri-\nMock57: A dataset of primary care mock consulta-\ntions. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics.\nZitha Sasindran, Harsha Yelchuri, and T. V. Prabhakar.\n2024. Semascore: A new evaluation metric for auto-\nmatic speech recognition tasks. In Interspeech 2024,\npage 4558–4562. ISCA.\nZitha Sasindran, Harsha Yelchuri, TV Prabhakar, and\nSupreeth Rao. 2023. H eval: A new hybrid evalua-\ntion metric for automatic speech recognition tasks.\nIn 2023 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU), pages 1–7. IEEE.\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\n2020. Bleurt: Learning robust metrics for text gener-\nation. arXiv preprint arXiv:2004.04696.\nJoel Shor, Ruyue Agnes Bi, Subhashini Venugopalan,\nSteven Ibara, Roman Goldenberg, and Ehud Rivlin.\n2023. Clinical bertscore: An improved measure of\nautomatic speech recognition performance in clinical\nsettings. arXiv preprint arXiv:2303.05737.\nTemple F. Smith and Michael S. Waterman. 1981. Iden-\ntification of common molecular subsequences. Jour-\nnal of Molecular Biology, 147(1):195–197.\nMatthew Snover, Bonnie Dorr, Rich Schwartz, Linnea\nMicciulla, and John Makhoul. 2006. A study of trans-\nlation edit rate with targeted human annotation. In\nProceedings of the 7th Conference of the Association\nfor Machine Translation in the Americas (AMTA),\npages 223–231.\nAnnalisa Szymanski, Noah Ziems, Heather A Eicher-\nMiller, Toby Jia-Jun Li, Meng Jiang, and Ronald A\nMetoyer. 2025. Limitations of the llm-as-a-judge\napproach for evaluating llm outputs in expert knowl-\nedge tasks. In Proceedings of the 30th International\nConference on Intelligent User Interfaces, pages 952–\n966.\nMeseret N Teferra. 2017. Iso 14971-medical device\nrisk management standard. International Journal\nof Latest Research in Engineering and Technology\n(IJLRET), 3(3):83–87.\nZhen Ling Teo, Arun James Thirunavukarasu, Kabilan\nElangovan, Haoran Cheng, Prasanth Moova, Brian\nSoetikno, Christopher Nielsen, Andreas Pollreisz,\nDarren Shu Jeng Ting, Robert JT Morris, et al. 2025.\nGenerative artificial intelligence in medicine. Nature\nMedicine, pages 1–13.\nUfonia Ltd. 2025. Ufonia — artificial clinical intel-\nligence.\nhttps://www.ufonia.com/.\nAccessed:\n2025-10-20.\nAnn Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris\nCallison-Burch,\nAndy Coenen,\nand Sebastian\nGehrmann. 2021a. Synthbio: A case study in faster\ncuration of text datasets. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets\nand Benchmarks Track (Round 2).\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021b.\nBartscore: Evaluating generated text as text gener-\nation. Advances in neural information processing\nsystems, 34:27263–27277.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert.\narXiv preprint\narXiv:1904.09675.\n10\n"}, {"page": 11, "text": "Lucia Zheng, Neel Guha, Brandon R. Anderson, Pe-\nter Henderson, and Daniel E. Ho. 2021.\nWhen\ndoes pretraining help?\nassessing self-supervised\nlearning for law and the casehold dataset. Preprint,\narXiv:2104.08671.\n11\n"}, {"page": 12, "text": "A\nLLM Aligner Prompt\nPrompt for aligning sentences from groundtruth to ASR hypothesis\nYou are an expert at aligning speech transcripts. I need you to match patient utterances from a\ngolden transcript with ASR (Automatic Speech Recognition) hypothesis results.\n,→\n{{golden_text}}\n{{asr_text}}\nTASK: Align each golden transcript utterance (G0, G1, etc.) with the most appropriate ASR\nresult(s) (A0, A1, etc.).\n,→\nRULES:\n1. You can only use utterances that exist in the input - DO NOT create new text\n2. Each golden utterance should be matched to one ASR result, multiple ASR results, or marked\nas missing\n,→\n3. Each ASR result can only be used ONCE - no ASR result should appear in multiple alignments\n4. Make reasonable fuzzy matches even if the text isn't perfect - ASR often has errors\n5. Consider semantic similarity, temporal proximity, and confidence scores\n6. Multiple consecutive ASR results can be combined to match one golden utterance if they\nrepresent fragments\n,→\n7. IMPORTANT: If an ASR result contains content that spans multiple consecutive golden\nutterances, those golden utterances should ALL be matched to that same ASR result\n,→\nEXAMPLE of rule 7:\n- Golden G5: \"I know I understand that\"\n- Golden G6: \"but it's different with the cataract\"\n- ASR A3: \"I know I understand that but it's different with the cataract\"\n- CORRECT: G5→A3, G6→A3 (both use same ASR)\n- WRONG: G5→missing, G6→A3 (creates artificial missing)\nOUTPUT FORMAT (JSON):\n{\n\"alignments\": [\n{\n\"golden_index\": 0,\n\"asr_indices\": [0],\n\"match_type\": \"exact|fuzzy|missing\",\n\"similarity_score\": 0.95,\n\"explanation\": \"Brief reason for this alignment\"\n},\n...\n]\n}\nProvide only the JSON response, no other text.\n12\n"}, {"page": 13, "text": "B\nWorked Alignment Examples and\nAlignment Quality Metrics\nB.1\nWorked Alignment Examples Across\nCommon ASR Segmentation Scenarios\nASR systems produce variable-length output seg-\nments that do not correspond reliably to linguistic\nsentences or speaker turns. As a result, alignment is\nperformed between contiguous groups of golden ut-\nterances and contiguous groups of ASR segments,\nallowing one-to-one, many-to-one, one-to-many,\nand partial matches.\nBelow we illustrate the most common alignment\nscenarios observed in practice.\nScenario A: One-to-one alignment (clean seg-\nmentation).\nGolden utterance (index 0):\n“Hello, good morning.”\nASR segment (index 0):\n“hello good morning”\nAlignment:\n• golden_indices = [0]\n• asr_indices = [0]\n• match_type = exact\nThis represents the ideal case where ASR\nsegmentation aligns cleanly with the golden\ntranscript.\nScenario B: Many-to-one alignment (ASR\nsplits a single utterance).\nGolden utterance (index 2):\n“Yes. Uh, my name is John Smith. And I was born on\nthe fifth of April, uh, nineteen seventy three.”\nASR segments (indices 2–3):\n•“yes my name is john smith”\n•“i was born on the fifth of april nineteen”\nAlignment:\n• golden_indices = [2]\n• asr_indices = [2, 3]\n• match_type = multi_fragment\nThis occurs when a single semantic utterance is\nsplit across multiple ASR chunks.\nScenario C: One-to-many alignment (golden\nutterance spans multiple lines).\nGolden utterance (indices 8–9):\n•“Um it’s much more like itchy. And my eczema\nwas more like only in the arm. But now it’s\nalso on the chest. And in the on the, on the\nhands as well.”\n•“Like pretty, yeah when, like I like, for instance\nhiking, during the weekend. And I am, I can’t\nreally do it anymore, because it’s like very\nlike I wanted to do that last weekend. And\nbecause super painful, and I I have to take\nlike showers every day to be able to, cannot\nuh ease this itching part, which is very very\nannoying.”\nASR segments (indices 11):\n“it’s much more like itchy and my eczema was more\nlike only in the arms and now also on the chest and\nin the in the on the hands as well like yeah when\nlike i like for instance hiking during the weekends\nand i i can’t really do it anymore because it’s like\nvery like like i wanted to do that last weekend and it\nwas super painful and i i have to take like showers\nevery day to be able to kind of ease this itching part\nwhich is very very annoying”\nAlignment:\n• golden_indices = [8, 9]\n• asr_indices = [11]\n• match_type = multi_fragment\nHere, a golden sentence that was broken over\ntwo lines (due to the doctor’s interruption), was\ntranscribed as a single sentence by the ASR\nprovider.\nB.2\nWorked Example of Alignment\nEvaluation Metrics\nWe illustrate the computation of the two alignment\nevaluation metrics using a small synthetic example.\nToy example setup.\nConsider a short patient in-\nteraction with the following golden utterances:\n• G0: “Yes, my name is John Smith.”\n• G1: “I was born on the fifth of April.”\n• G2: “I have some swelling on my elbow.”\n• G3: “Bye.”\nThe corresponding ASR output is segmented as:\n• A0: “yes my name is john smith”\n13\n"}, {"page": 14, "text": "• A1: “i was born on the fifth of april”\n• A2: “i have some swelling”\n• A3: “on my elbow”\n• A4: “thank”\nThe gold-standard alignment specifies:\n(0) →[0]\n(1) →[1]\n(2) →[2, 3]\nwith unmatched content:\nUnmatched golden = {3}, Unmatched ASR = {4}.\nAn LLM aligner produces the following predic-\ntion:\n(0) →[0]\n(1) →[1]\n(2) →[2]\nwith predicted unmatched sets:\nUnmatched golden = {3}, Unmatched ASR = {3, 4}.\nThis prediction contains a structural error (miss-\ning ASR index 3) and an unmatched-detection error\n(incorrectly marking ASR index 3 as unused).\nClassification Accuracy.\nClassification Accu-\nracy evaluates whether each utterance or ASR seg-\nment is correctly labeled as matched or unmatched.\nGolden utterances:\nTP = 1, FP = 0, FN = 0, TN = 3,\nyielding an accuracy of:\nTP + TN\nTP + TN + FP + FN = 4\n4 = 1.00.\nASR segments:\nTP = 1, FP = 1, FN = 0, TN = 3,\nyielding an accuracy of:\nTP + TN\nTP + TN + FP + FN = 4\n5 = 0.80.\nThis reflects that the aligner correctly identifies\nunused golden content, but incorrectly discards one\nASR segment that should have been aligned.\nStructural Alignment Accuracy.\nStructural\nAlignment Accuracy evaluates whether the aligner\nproduces the exact same ASR index grouping for\neach golden utterance as in the gold-standard align-\nment.\nIn this example, the alignments for G0 and G1\nare correct, while the alignment for G2 is incorrect\ndue to a missing ASR fragment. With four golden\nutterances in total, this yields:\nStructural Accuracy = 4 −1\n4\n= 0.75.\nInterpretation.\nClassification Accuracy captures\nwhether the aligner correctly determines which con-\ntent should participate in alignment, while Struc-\ntural Alignment Accuracy captures whether the\nresulting fragment groupings are structurally cor-\nrect. Both metrics are required to fully characterize\nalignment quality in the presence of inconsistent\nASR segmentation.\nC\nAnnotation Instructions\nThe following show the full annotation instructions\nthat clinician labellers used for initial labelling,\nand subsequent disagreement resolution. Clinician\nannotators’ backgrounds are also reported here.\nC.1\nDemographic Backgrounds for Clinician\nAnnotators\nClinician\nLabeller\nYears of Post-\ngraduate Ex-\nperience\nPrimary\nSpe-\ncialty\nA\n8 years\nOphthalmology\nB\n7 years\nOphthalmology\nBoth clinicians were ophthalmologists due to\nthe nature of the Dora dataset being primarily in\ncataract surgery follow up, with the remainder of\nthe Primock dataset being primary care clinical\ninteractions.\nC.2\nFull annotation instructions provided to\nlabellers\nObjective\nWe are examining how transcription errors can af-\nfect clinical meaning in a clinical conversational\ncontext, and looking to build better metrics to cap-\nture this.\nYour task is to then assess if your understand-\ning of the patient’s clinical situation is different\nbecause of the transcription errors, and what the\n14\n"}, {"page": 15, "text": "potential clinical impact of that is. All the excerpts\nyou will see are from either post-cataract surgery\nfollow-up calls, or general practice consultations.\nYou’ll be shown short sections of these dialogues.\nFor each one, you will see the Ground Truth (what\nthe patient actually said) and a Transcription (an\nautomatically transcribed version that may contain\nerrors). Only the last sentence is different in these\ntranscription vs ground-truth examples.\nThe Annotation Task\nFor each example, you will be presented with:\nThe patient’s Ground Truth utterance (what was\nactually said)\nThe patient’s Transcription utterance (what was\ntranscribed, with only the last sentence being dif-\nferent)\nSome additional context is provided of the con-\nversation preceding these two statements as well.\nPlease read the two versions of the conversation,\nand then, consider this question:\n“If uncorrected, and if you could only\nread the transcription-alone, would it\nhave changed your understanding of the\npatient’s clinical condition?”\nPlease assign one of the following three labels\nand provide a brief justification for your choice.\n0 - It does not change my understanding of the\npatient’s clinical condition.\n1 - It changes my understanding of the patient’s\nclinical condition, with minimal clinical impact.\n2 - It changes my understanding of the patient’s\nclinical condition, with significant clinical impact.\nBelow shows additional detail for each category\nand some examples.\nDetailed Rating Definitions\n0 - It does not change my understanding of\nthe patient’s clinical condition.\nThis will commonly include situations where\nThe meaning of the utterance is preserved despite\nthe errors\nThe meaning is changed, but not of something re-\nlating to the patient’s clinical condition or situation\n- (e.g. administrative, patient feedback, address or\nlogistical).\nThe ground-truth was not clear, and the transcrip-\ntion didn’t really change it or misrepresent it.\n1 - It changes my understanding of the\npatient’s clinical condition,\nwith minimal\nclinical impact.\nThis is where something is changed, and it is rel-\nevant to the patient’s clinical condition, however, it\nhas either minimal or no clinical significance. This\nwill commonly include situations where\nA patient preference is missing or misunderstood,\nbut this for something administrative or not ex-\ntremely clinically risky.\nA part of an utterance is missed or wrong, and this\nis clinically relevant, (i.e. does relate to the pa-\ntient’s condition, expectations, treatment plan, fam-\nily history) but it is unlikely to affect the overall\noutcomes, or it was not a critical piece of informa-\ntion that was missed or wrong.\n2 - It changes my understanding of the\npatient’s clinical condition, with significant\nclinical impact.\nThis is where something is changed, and it is\nrelevant to the patient’s clinical condition, and it\npotentially leads to significant clinical impact. This\nwill commonly include situations where:\nA patient answers about a symptom but significant\nparts of it are altered or omitted.\nA fact is missed or wrong, especially if it’s clini-\ncally relevant to the scenario, and if its meaning\nhas been totally changed.\nKey history parameters are wrong (e.g. past medi-\ncal history, drug history, family history)\nThe patient could have had relevant questions or\nother points that weren’t captured.\nExamples\nThis is a mock-example:\nNote that in all example pairs only the sentences\nin bold are different between the ground-truth and\ncontext+transcripts.\nExample A\nThis would be labelled 2 - as going off the tran-\nscript alone, my understanding of the situation has\ncompletely flipped from “just a bit gritty” and “not\npainful” to simply “it’s painful”. This is clinically\nsignificant as is a core clinical question.\nExample B\nAlthough the sentence is notably changed, the\nmeaning between both transcript and ground truth\nis not changed and so this would be labelled 0.\n15\n"}, {"page": 16, "text": "Context + Ground\nTruth\nContext + Transcript\n(21) Doctor: Is your\neye red?\n(21) Doctor: Is your\neye red?\n(21) Patient: No\n(21) Patient: No\n(22) Doctor:\nGreat,\nand\nIs\nyour\neye\npainful?\n(22) Doctor:\nGreat,\nand\nIs\nyour\neye\npainful?\n(22) Patient: Well it’s\nnot painful, just a bit\ngritty.\n(22) Patient: Well it’s\npainful\nContext + Ground\nTruth\nContext + Transcript\n(21) Doctor: Is your\neye red?\n(21) Doctor: Is your\neye red?\n(21) Patient: No\n(21) Patient: No\n(22) Doctor:\nGreat,\nand\nIs\nyour\neye\npainful?\n(22) Doctor:\nGreat,\nand\nIs\nyour\neye\npainful?\n(22) Patient: Well it’s\nnot painful, just a bit\ngritty that’s all.\n(22)\nPatient:\nnot\npainful, just bit gritty\nExample C\nContext + Ground\nTruth\nContext + Transcript\n(21) Doctor: Okay and\ndo you drink?\n(21) Doctor: Okay and\ndo you drink?\n(21) Patient: No\n(21) Patient: No\n(22) Doctor: Do you\nsmoke?\n(22) Doctor: Do you\nsmoke?\n(22) Patient: Um, oc-\ncasionally, you know,\njust socially, the odd\ncigarette. But i don’t\nvape or anything.\n(22) Patient:\nUm,\nyeah occasionally, so-\ncial\ncigarettes\nand\nvape anything.\nThis would be labelled 1 - going off the tran-\nscript alone, it sounds like the patient is a social\nsmoker and vapes rather than just a social smoker.\nHowever, this is unlikely to be of significant clini-\ncal impact overall given this is a social history, and\nwe are able to understand in both that they are a\nsmoker.\nExample D\nThis would be labelled 0 - Both transcripts are\nunclear, and the ground truth didn’t misrepresent\nor edit it.\nContext + Ground\nTruth\nContext + Transcript\n(21) Doctor: Okay and\ndo you drink?\n(21) Doctor: Okay and\ndo you drink?\n(21) Patient: No\n(21) Patient: No\n(22) Doctor: Do you\nsmoke?\n(22) Doctor: Do you\nsmoke?\n(22) Patient: um\n(22) Patient: ah\nClarifying Instructions\nYou are comparing between the Ground-truth and\ntranscription. If the original ground-truth transcrip-\ntion is confusing or unclear, please assess any addi-\ntional change in meaning from the Ground Truth.\nD\nGEPA Cost Matrix\nWe employed a cost-sensitive metric using matrix\nC, where C[i, j] is the reward/penalty for predict-\ning class j when the true class is i:\nTable 3: Confusion matrix C\nPred 0\nPred 1\nPred 2\nTrue 0\n1.2\n0.3\n-1.0\nTrue 1\n0.3\n1.5\n0.5\nTrue 2\n-1.2\n0.4\n1.5\nThe matrix encodes three priorities:\n1. reward correct classifications (diagonal > 1.0),\n2. heavily\npenalise\nmissing\ncritical\nerrors\n(C[2, 0] = −1.2),\n3. tolerate adjacent class confusions (small val-\nues for C[i, i ± 1]).\nThis reflects that missing a significant transcrip-\ntion error is more dangerous than over-classifying\na cosmetic error, while acknowledging that bound-\naries between adjacent severity levels contain inher-\nent ambiguity. The aggregate score guides GEPA\noptimization toward clinically safe performance.\n16\n"}, {"page": 17, "text": "E\nGEPA Final Prompt\nPrompt to assess clinical errors\nYou are an expert medical analyst. Your task is to assess the clinical impact of errors in an\nAI-generated transcription of a medical conversation.\n,→\nYou will be given two versions of a conversation:\n1.\n`ground_truth_conversation`: The accurate, human-verified transcript.\n2.\n`transcription_conversation`: The machine-generated transcript, which may contain errors.\nYour goal is to compare the `transcription_conversation` to the `ground_truth_conversation` and\ndetermine if any differences (errors) could negatively impact patient care. You must output\nyour assessment in two parts: a `reasoning` section and a `clinical_impact` classification.\n,→\n,→\n### Core Principle for Assessment\nThe core principle is to determine if a clinician reading the `transcription_conversation`\nwould have a different understanding or make a different medical decision than if they had read\nthe `ground_truth_conversation`. You must be highly sensitive to any changes in clinically\ncritical information, such as symptoms, medications, measurements, or diagnoses.\n,→\n,→\n,→\n### Severity Levels for Clinical Impact\nYou must classify the impact of the transcription errors into one of three distinct severity\nlevels.\n,→\n**Class 0: No Clinical Impact**\nThis is the most common class. Errors in this category are \"cosmetic\" and do **not** change the\nclinical meaning of the conversation in any way. A clinician would arrive at the exact same\nunderstanding and conclusions from both transcripts.\n,→\n,→\nAssign Class 0 for errors such as:\n*\n**Filler words:** Omission or addition of words like \"uh,\" \"um,\" \"like,\" \"you know,\" \"so.\"\n*\n**Punctuation and Capitalization:** Differences in commas, periods, or letter casing.\n*\n**Minor Rephrasing:** The wording is slightly different, but the core clinical fact remains\nidentical and unambiguous. For example, \"probably about six months ago\" vs. \"that's six months\nokay\" both clearly convey the \"six months\" timeframe.\n,→\n,→\n**Class 1: Potential for Minor Clinical Impact**\nErrors in this category introduce ambiguity, omit non-critical context, or alter a piece of\nclinical information in a way that *could* lead to a misunderstanding, but is unlikely to cause\nimmediate harm. These errors might prompt a clinician to seek clarification but do not\nfundamentally alter the patient's current clinical picture.\n,→\n,→\n,→\nAssign Class 1 for errors such as:\n*\n**Ambiguity Requiring Clarification:** The omission of a patient's confirmation that their\nquestions were answered. This leaves the clinician unsure if the patient understands their care\nplan, which requires follow-up but isn't an immediate safety risk.\n,→\n,→\n*\n**Omission of Future Intentions:** The omission of a patient's answer about whether they\nplan to have surgery on their other eye in the future. This creates a gap in planning but does\nnot affect the diagnosis or treatment of the current condition.\n,→\n,→\n*\n**Altered Timelines or Frequencies:** Changing a duration or frequency to a different, but\nplausible, value (e.g., \"two days\" becomes \"two weeks\"; \"once a day\" becomes \"twice a day\").\n,→\n*\n**Ambiguous Symptoms:** Making a symptom's description less clear (e.g., \"sharp pain\"\nbecomes \"some pain\").\n,→\n**Class 2: Significant Clinical Impact**\nErrors in this category are critical and could directly lead to a wrong diagnosis, an incorrect\ntreatment plan, or a serious adverse event. These errors fundamentally change a key clinical\nfact related to the patient's current condition, history, or diagnostic process. **Reserve\nClass 2 for errors that could directly affect diagnosis, treatment, or patient safety.**\n,→\n,→\n,→\nAssign Class 2 for errors such as:\n*\n**Omission/Alteration of Diagnostic Reasoning:** This is a high-priority error. For example,\nomitting a patient's statement where they explicitly connect their symptoms to a known family\nhistory of a specific condition (e.g., \"I'm worried this is a migraine... I know it's genetic\nfrom my mom and sister\"). This information is a critical part of the History of Present Illness\n(HPI) and directly influences the diagnostic workup. Its omission is a significant loss.\n,→\n,→\n,→\n,→\n*\n**Negation Errors:** Changing a positive to a negative, or vice-versa (e.g., \"no chest\npain\" becomes \"chest pain\"; \"patient is not allergic\" becomes \"patient is allergic\").\n,→\n*\n**Critical Value Errors:** Changing a specific, critical number, such as a medication\ndosage (e.g., \"10mg\" becomes \"100mg\") or a vital sign.\n,→\n*\n**Symptom/Condition Errors:** Introducing a new, incorrect symptom or diagnosis, or\nomitting a critical one mentioned in the ground truth.\n,→\n17\n"}, {"page": 18, "text": "*\n**Anatomical Errors:** Changing the location of a symptom (e.g., \"left arm\" becomes \"right\narm\").\n,→\n### Your Response Format\nYour output must include two components:\n1.\n**`reasoning`**: Provide a step-by-step analysis.\n*\nFirst, identify the specific, clinically relevant differences between the ground truth\nand the transcription.\n,→\n*\nSecond, analyze whether these differences alter clinically relevant information\n(symptoms, medications, timelines, diagnostic reasoning, etc.).\n,→\n*\nFinally, justify your choice of `clinical_impact` class by explaining how the error\nwould (or would not) affect a clinician's understanding or decision-making, referencing the\nspecific criteria for the class you have chosen.\n,→\n,→\n2.\n**`clinical_impact`**: Provide the single integer corresponding to your classification\n(`0`, `1`, or `2`).\n,→\n18\n"}, {"page": 19, "text": "F\nEvaluation Metrics Comparison\nF.1\nEdit-Distance Metrics\nThis family of metrics quantifies the dissimilar-\nity between an ASR-generated hypothesis and a\nground-truth reference by calculating the minimum\nnumber of edits required to make them identical.\nThey are fundamentally error rates, where a lower\nscore indicates a better transcription.\n• Word Error Rate (WER) is the de facto\nstandard, measuring word-level substitutions,\ndeletions, and insertions.\n• Character Error Rate (CER) is a variant of\nWER that operates at the character level, use-\nful for morphologically complex languages.\n• Match Error Rate (MER) is a bounded ver-\nsion of WER that includes matches in its de-\nnominator, making it less sensitive to refer-\nence length.\n• Word Information Lost (WIL) is an\ninformation-theoretic extension of WER that\nweighs errors based on their probabilistic im-\npact.\n• Semantic-WER (S-WER) is an enhanced\nWER that adds semantic weights to penalize\nerrors on important words more heavily (Roy,\n2021).\nF.2\nN-gram Overlap Metrics\nBorrowed primarily from the field of machine trans-\nlation, these metrics evaluate quality by measur-\ning the lexical overlap of n-grams (contiguous se-\nquences of items) between the hypothesis and ref-\nerence texts.\n• BLEU (Bilingual Evaluation Understudy)\nis a precision-focused metric that measures\nn-gram overlap with a penalty for overly short\ntranscriptions. (Papineni et al., 2002)\n• ROUGE (Recall-Oriented Understudy for\nGisting Evaluation) is a recall-focused met-\nric for n-gram overlap; variants include\nROUGE-N, ROUGE-L, and ROUGE-W. (Lin,\n2004).\nThe F-Measure is reported for all\nROUGE variants in this work.\n• METEOR (Metric for Evaluation of Trans-\nlation with Explicit ORdering) is an ad-\nvanced metric aligning unigrams using stem-\nming and synonym matching for greater flexi-\nbility. (Banerjee and Lavie, 2005)\n• chrF and chrF++ compute an F-score based\non character n-gram overlap, with the ‘++‘ ver-\nsion also including word n-grams. (Popovi´c,\n2015)\nF.3\nLearned Semantic Metrics\nThis modern class of metrics leverages deep learn-\ning models to move beyond lexical overlap and\ncapture semantic similarity, determining if the core\nmeaning of the text is preserved.\n• BERTScore utilizes contextual embeddings\nto compute a nuanced semantic similar-\nity score between tokens. (Zhang et al.,\n2019) (Shor et al., 2023)\n• BLEURT is a regression-based model trained\non human quality ratings to predict the quality\nof a generated text. (Sellam et al., 2020)\n• ClincalBLEURT is a version of BLEURT\nfine-tuned on family medicine and or-\nthopaedic notes. (Ben Abacha et al., 2023)\n• BARTScore is a generation-based metric\nusing the BART model to assess quality\nbased on conditional probability. (Yuan et al.,\n2021b)\n• SBERT-Similarity and SimCSE compute\nsentence embeddings for the hypothesis and\nreference and measure their cosine similar-\nity. (Reimers and Gurevych, 2019b) (Gao\net al., 2021)\n• Natural Language Inference (NLI) Scores\nrepurpose Natural Language Inference models\nto measure semantic equivalence using bidi-\nrectional entailment (mutual entailment), fol-\nlowing the approach in (Phukon et al., 2025).\n• HEVAL - Hybrid Evaluation Metric for\nAutomatic Speech Recognition Tasks intro-\nduces a hybrid metric combining traditional\nerror-based scoring (e.g. edit distances) on\nnon-keywords with embedding-based seman-\ntic distance for ASR outputs. (Sasindran et al.,\n2023)\n19\n"}, {"page": 20, "text": "• SeMaScore combines phonetic error rates\nwith segment-wise semantic similarity to yield\nstronger correlations with expert judgements\nin noisy speech settings (Sasindran et al.,\n2024).\nIn our experiments the word em-\nbeddings were extracted using deberta-large-\nmnli.\n• Intelligibility Score is a hybrid metric that\nfuses phonetic, semantic, and NLI-based fea-\ntures to align ASR evaluation with human\njudgements of comprehensibility. (Phukon\net al., 2025).In our experiments the word\nembeddings were extracted using RoBERTa-\nlarge fine-tuned on SNLI.\n20\n"}, {"page": 21, "text": "F.4\nEvaluation Metrics Model and Implementation Details\nAdditional information on the implementation of the different evaluation metrics is provided in Table 4.\nMetric\nModel / Implementation\nSource\nEdit-Distance Metrics\nWER\njiwer\n[Link]\nCER\nMER\nWIL\nS-WER\nsentence-transformer\n[Link]\nN-gram Overlap Metrics\nBLEU variants\nNLTK\n[Link]\nROUGE variants\nrouge_score\n[GitHub]\nChrF(++)\nsacrebleu\n[GitHub]\nMETEOR\nNLTK\n[Link]\nLearned Semantic Metrics\nSEMA Score\nmicrosoft/deberta-large-mnli\n[Link]\nIntelligibility Score\nynie/roberta-large-\nsnli_mnli_fever_anli_R1_R2_R3-nli\n[Link]\nHEVAL Score\nroberta-base\n[Link]\nClinical BLEURT\nbleurt-oss-21 (fine-tuned)\n[GitHub]\nBLEURT\nbleurt-oss-21\n[GitHub]\nBART Score\nfacebook/bart-large-cnn\n[HF Link]\nSBERT Similarity\nall-MiniLM-L6-v2\n[HF Link]\nNLI XSmall\ncross-encoder/nli-deberta-v3-xsmall\n[HF Link]\nNLI Base\ncross-encoder/nli-deberta-v3-base\n[HF Link]\nNLI Large\ncross-encoder/nli-deberta-v3-large\n[HF Link]\nBERTScore\nmicrosoft/deberta-large-mnl\n[HF Link]\nSimCSE\nprinceton-nlp/sup-simcse-bert-base-uncased\n[Link]\nTable 4: Model Specifications and Sources for Evaluation Metrics\n21\n"}, {"page": 22, "text": "G\nDataset Distribution Details\nFigure 6: Distribution of WER across utterances on the\ncombined Metrics Subset. A bimodal distribution is\nobserved, with one peak at a low WER and a second\nsmaller peak at high WER.\nFigure 7: Utterance length distribution for Dora and\nPrimock57 on the Metrics Subset. Both datasets are\nskewed toward short utterances, with Primock57 show-\ning a longer tail, reflecting occasional extended patient\nturns.\nH\nDataset Cleaning Process\nPrior to metric calculation, both the reference\n(ground truth) and hypothesis (ASR output) tran-\nscripts underwent standardised text normalization\nto ensure fair comparison.\nThe preprocessing\npipeline, implemented using the Python jiwer li-\nbrary, consisted of the following sequential trans-\nformations:\n1. Number Normalisation: All numeric expres-\nsions were converted to their word equiva-\nlents using British English conventions (e.g.,\n\"1st\" -> \"first\", \"23\" -> \"twenty-three\") via\nthe num2words library.\n2. Case Normalisation: All text was converted\nto lowercase.\n3. Punctuation Standardisation:\nHyphens\nwere replaced with spaces to prevent word\nconcatenation, and all remaining punctuation\nwas removed.\n4. Whitespace Normalisation: Multiple consec-\nutive spaces were collapsed into single spaces,\nand leading/trailing whitespace was removed.\n5. Non-Lexical Token Removal (for the Met-\nrics Subset only, Sec. 4.3): Disfluencies and\nfiller words (e.g., \"um\", \"uh\", \"hmm\") were\nremoved based on a predefined lexicon of 43\nnon-lexical tokens adapted from Speechmat-\nics documentation (Russell et al., 2024).\nThis preprocessing was applied identically to\nboth reference and hypothesis texts immediately\nbefore each metric calculation (WER, BLEU,\nROUGE, etc.), ensuring consistent normalisation\nacross all evaluation metrics.\n22\n"}, {"page": 23, "text": "I\nDetailed Results for Existing Metric\nEvaluation\nI.1\nMean Difference Scores per Metric -\nEnrichment–Delta Analysis\nTable 5: Mean Difference in Score (Condition 2 minus\nCondition 0) Grouped by Metric Family\nMetric\nMean Difference\nEdit-Distance Metrics\nWER\n−0.148\nCER\n−0.062\nMER\n−0.195\nWIL\n−0.202\nS-WER\n−0.144\nN-gram Overlap Metrics\nBLEU-1\n−0.198\nBLEU-2\n−0.170\nBLEU-3\n−0.121\nBLEU-4\n−0.097\nROUGE-L\n−0.195\nROUGE-1\n−0.193\nROUGE-2\n−0.152\nROUGE-W\n−0.195\nChrF\n−0.257\nChrF++\n−0.239\nMETEOR\n−0.216\nLearned Semantic Metrics\nSEMA Score\n−0.216\nIntelligibility Score\n−0.301\nHEVAL Score\n−0.142\nClinical BLEURT\n−0.196\nBLEURT\n−0.294\nBART Score\n−0.160\nSBERT Similarity\n−0.309\nNLI XSmall\n−0.508\nNLI Base\n−0.475\nNLI Large\n−0.463\nBERTScore\n−0.215\nSimCSE\n−0.237\nI.2\nKendalls Correlation between Metrics and\nClinical Labels\nTable 6: Kendall’s τ Correlation Grouped by Metric\nFamily\nMetric\nτ (Kendall’s Tau)\nEdit-Distance Metrics\nWER\n0.206 765\nCER\n0.232 115\nMER\n0.214 383\nWIL\n0.215 302\nS-WER\n0.227 910\nN-gram Overlap Metrics\nBLEU-1\n−0.218 176\nBLEU-2\n−0.188 999\nBLEU-3\n−0.149 426\nBLEU-4\n−0.125 599\nROUGE-L\n−0.224 263\nROUGE-1\n−0.223 657\nROUGE-2\n−0.163 319\nROUGE-W\n−0.224 263\nChrF\n−0.289 068\nChrF++\n−0.261 439\nMETEOR\n−0.235 693\nLearned Semantic Metrics\nSEMA Score\n−0.222 682\nIntelligibility Score\n−0.394 572\nHEVAL Score\n0.280 065\nClinical BLEURT\n−0.381 359\nBLEURT\n−0.372 970\nBART Score\n−0.253 080\nSBERT Similarity\n−0.323 138\nNLI XSmall\n−0.422 054\nNLI Base\n−0.389 810\nNLI Large\n−0.394 935\nBERTScore\n−0.233 922\nSimCSE\n−0.371 572\n23\n"}, {"page": 24, "text": "I.3\nQualitative Error Analysis of Existing Metrics\nTwo examples from the Primock57 portion of the Metrics Subset demonstrate specific scenarios where\ntraditional evaluation metrics fail to detect clinical distortion. Table 7 (Example A) presents a high-risk\nscenario where the ASR system mistranscribed a key word \"throat\" as \"so\". Expert clinicians assigned\nthis a Significant Impact (2) rating, noting that the error transforms a specific symptom denial, \"not\nthroat\", into the vague phrase, \"not so\". In the context of assessing breathing difficulties, this ambiguity is\ndangerous as it fails to clearly rule out the symptom, potentially affecting patient treatment. However,\nmany standard metrics, whether edit-distance, n-gram overlap-based, or learned semantic, failed to capture\nthis dangerous distortion. Table 9 shows that the transcript had a relatively low WER (0.1176) and high\nscores across embedding-based similarity metrics (BERTScore: 0.9656; SimCSE: 0.9523), indicating that\nsuch similarity metrics failed to detect the crucial change in clinical meaning between \"not throat\" and\n\"not so\".\nExample B (Table 8) demonstrates the inverse case of a transcription with elevated WER (0.3333),\nyet No Clinical Impact (0). Despite the ASR substituting \"sitting\" for \"feeling\", core clinical meaning\nwas preserved given the conversational context. Standard context-agnostic metrics, however, penalized\nthe ASR transcript (Table 9), unable to recognize that the core clinical information remained intact. In\ncontrast to existing automated metrics, the LLM-as-a-Judge predictions aligned with the expert clinicians’\nassessment on both examples.\nExample A - Significant Clinical Impact, Low WER\nContext\n(6) Doctor: Mm-hmm.\n(6) Patient: And now I’m, um, just starting to notice a few things. Yeah, I’m having quite shallow\nbreath.\n(7) Doctor: You’re having difficulties breathing since then, yeah? OK. So, is there any swelling of\nyour lips, you feel your throat is closing up?\nGround Truth\nTranscript\n(7) Patient: Not throat, but I can , yeah, I can I\ncan definitely feel something in the lips, yeah.\n(7) Patient: not so but i can i yeah i can i can\ndefinitely feel something in the lips yeah\nTable 7: Example A: High-risk error (Significant Impact) where the crucial word \"throat\" is substituted for \"so\".\nJustification\nClinician: We don’t know if \"not so\" refers to \"not throat\" - \"not so\" could be breathing even.\nLLM-as-a-Judge: The transcription incorrectly captures the patient’s response to a critical question about symptoms of\na severe allergic reaction. The doctor asks about lip swelling and the throat closing up. In the ground truth, the patient\nexplicitly denies throat symptoms (\"Not throat\") while confirming lip symptoms. This is a crucial distinction, as throat\nclosing (laryngeal edema) is a sign of a life-threatening emergency (anaphylaxis) requiring immediate intervention. The\ntranscription replaces the clear denial \"Not throat\" with the ambiguous phrase \"not so.\" A clinician reading \"not so\"\nmight interpret it as \"not really\" or \"not that much,\" which fails to definitively rule out the dangerous symptom of throat\ninvolvement. This ambiguity in the context of a patient reporting \"shallow breath\" could lead to a misassessment of the\nseverity of the reaction and potentially delay or alter the appropriate emergency treatment.\nExample B - No Change In Understanding, Elevated WER\n24\n"}, {"page": 25, "text": "Context\n(9) Doctor: Any abnormal smells or tastes?\n(9) Patient: No.\n(10) Doctor: Any, any nausea, vomiting?\nGround Truth\nTranscript\n(10) Patient: Uh, no, no, been feeling fine actu-\nally.\n(10) Patient: no no it’s sitting fine actually\nTable 8: Example B: Benign error (No Clinical Impact) with elevated WER. Although the transcript discusses\n\"sitting\" rather than \"feeling\", the core clinical meaning is contextually preserved in context of the doctor’s question.\nJustification\nClinician: Same understanding\nLLM-as-a-Judge: The transcription incorrectly captured the patient’s response to the question about nausea and\nvomiting. The ground truth is \"been feeling fine actually,\" while the transcription states, \"it’s sitting fine actually.\" In the\ncontext of a gastrointestinal review of systems, both phrases convey the same clinical meaning: the patient is denying\nsymptoms of nausea and vomiting. A clinician would interpret \"it’s sitting fine\" as a colloquial way of saying they are\nnot experiencing GI upset. The core clinical information (absence of symptoms) is preserved, making this a cosmetic\nerror with no impact on clinical understanding.\n25\n"}, {"page": 26, "text": "Evaluation Metrics Data for Both Examples\nExample A\nExample B\nRisk-Aligned?\nWER ↓\n0.1176\n0.3333\nNo\nCER ↓\n0.0875\n0.2500\nNo\nMER ↓\n0.1111\n0.3333\nNo\nWIL ↓\n0.1634\n0.5556\nNo\nS-WER ↓\n0.0094\n0.1111\nNo\nBLEU-1 ↑\n0.8889\n0.6667\nNo\nBLEU-2 ↑\n0.8245\n0.5164\nNo\nBLEU-3 ↑\n0.7760\n0.0000\nNo\nBLEU-4 ↑\n0.7277\n0.0000\nNo\nROUGE-L ↑\n0.9143\n0.6667\nNo\nROUGE-1 ↑\n0.9143\n0.6667\nNo\nROUGE-2 ↑\n0.7879\n0.4000\nNo\nROUGE-W ↑\n0.9143\n0.6667\nNo\nchrF ↑\n0.8387\n0.5767\nNo\nchrF++ ↑\n0.8459\n0.5659\nNo\nMETEOR ↑\n0.9214\n0.6250\nNo\nSeMaScore ↑\n0.8813\n0.6849\nNo\nIntelligibility ↑\n0.8670\n0.6151\nNo\nHEVAL ↓\n0.0059\n0.0602\nNo\nClinical BLEURT ↑\n0.4967\n0.0982\nNo\nBLEURT ↑\n0.3749\n0.1722\nNo\nBARTScore ↑\n-2.9802\n-3.9300\nNo\nSBERT Sim ↑\n0.7402\n0.5003\nNo\nNLI (XSmall) ↑\n0.8863\n0.1430\nNo\nNLI (Base) ↑\n0.9963\n0.9743\nNo\nNLI (Large) ↑\n0.9680\n0.0433\nNo\nBERTScore ↑\n0.9656\n0.8942\nNo\nSimCSE ↑\n0.9523\n0.5814\nNo\nLLM-as-a-Judge Prediction ↓\n2\n0\nYes\nReconciled Clinician Label ↓\n2\n0\nN/A\nTable 9: Existing metric scores for Example A (Significant Impact) and Example B (No Impact), with LLM-as-a-\nJudge predictions and clinician labels. The ’Risk-Aligned?’ column indicates whether the metric correctly identifies\nA as a more clinically impactful error (lower quality transcript) than B. Arrows indicate the direction of improvement\nfor a metric (↑higher is better quality; ↓lower is better quality).\n26\n"}, {"page": 27, "text": "J\nLLM-as-a-Judge Automation\nJ.1\nModel Comparison\nThis section provides a detailed comparison of\nthe performance of various LLMs on the clinical\nimpact classification task, using the final GEPA-\noptimized prompt. All evaluations were conducted\non the 50-item held-out test set, with results av-\neraged over five independent runs. The results\ndemonstrate that while the optimized prompt is\neffective across models, its performance is maxi-\nmized by Gemini-2.5-Pro, particularly on the most\nclinically nuanced classification tasks.\nFigure 8: While most high-performing models can re-\nliably identify ’No Impact’ (Class 0) and ’Significant\nImpact’ (Class 2) errors, they struggle with the nuanced\n’Minimal Impact’ (Class 1) category. This highlights\nthe difficulty of discerning subtle changes in clinical\nmeaning, a task where Gemini-2.5-Pro demonstrates\nunique proficiency as the only model to achieve an F1\nscore over 0.5, where it got 0.655, for this challenging\nclass. Error bars represent standard deviation across 5\nruns.\nModel\nProvider\nMacro F1\nCohens κ\ngemini-2.5-pro\nVertex AI\n0.825 ± 0.000707\n0.790 ± 0.000472\ngrok-4\nxAI\n0.710 ± 0.001448\n0.638 ± 0.003150\ngrok-4-fast\nxAI\n0.708 ± 0.002754\n0.645 ± 0.003754\ngpt-5-chat\nOpenAI\n0.657 ± 0.003607\n0.588 ± 0.004064\nclaude-sonnet-4.5\nAnthropic\n0.656 ± 0.006194\n0.589 ± 0.010008\nqwen3-235b-a22b-2507\nNebius AI\n0.646 ± 0.001014\n0.592 ± 0.001982\ngpt-5\nOpenAI\n0.562 ± 0.000805\n0.459 ± 0.000790\nqwen3-30b-a3b-instruct-2507\nNebius AI\n0.558 ± 0.003491\n0.428 ± 0.005847\nllama-3.3-70b-instruct\nCrusoe\n0.545 ± 0.002365\n0.451 ± 0.002753\ngemini-2.5-flash\nVertex AI\n0.542 ± 0.021385\n0.450 ± 0.032153\nllama-3.1-8b-instruct\nGroq\n0.345 ± 0.001609\n0.138 ± 0.001388\nTable 10: The table details the aggregate performance\nof each LLM judge. The data shows a consistent trend\nacross both metrics (F1-score and Cohen’s κ), with\nGemini-2.5-Pro establishing a significant lead. Results\nare presented as Mean ± Standard Deviation over 5 runs.\nJ.2\nClinicians and Judge Agreement\nClinician A\nClinician B\nJudge\nClinician A\n—\n0.505 (0.285, 0.708)\n0.713 (0.535, 0.867)\nClinician B\n0.505 (0.285, 0.708)\n—\n0.497 (0.273, 0.702)\nJudge\n0.713 (0.535, 0.867)\n0.497 (0.273, 0.702)\n—\nTable 11: Agreement between clinicians and judge using\nCohen’s κ with 95% bootstrap confidence intervals.\n27\n"}]}