{"doc_id": "arxiv:2601.07935", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.07935.pdf", "meta": {"doc_id": "arxiv:2601.07935", "source": "arxiv", "arxiv_id": "2601.07935", "title": "Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation", "authors": ["Yuxin Yang", "Aoxiong Zeng", "Xiangquan Yang"], "published": "2026-01-12T19:04:58Z", "updated": "2026-01-12T19:04:58Z", "summary": "The rapid evolution of Large Language Models (LLMs) has shifted focus from general-purpose capabilities to domain-specific expertise. However, adapting LLMs to specialized fields such as medicine presents two challenge: (1) the \"Stability-Plasticity Dilemma\", where the model must acquire complex clinical knowledge without suffering from catastrophic forgetting of general world knowledge; and (2) \"Task Interference\", where disparate sub-tasks, such as medical diagnosis, report summarization, and drug-drug interaction prediction, compete for limited low-rank parameter space. In this paper, we propose Med-MoE-LoRA, a novel framework that integrates Mixture-of-Experts (MoE) with Low-Rank Adaptation (LoRA) to enable efficient multi-task domain adaptation, especially for medical scenarios. Drawing inspiration from recent advances, our framework employs an asymmetric expert distribution where deeper layers are equipped with a higher density of LoRA experts to capture complex semantic abstractions. We further introduce a \"Knowledge-Preservation Plugin\", inspired by LoRA MoE, to isolate and protect general-purpose reasoning. By utilizing soft merging with adaptive routing and rank-wise decoupling, Med-MoE-LoRA achieves superior performance in medical benchmarks while reducing interference. Experimental results demonstrate that our approach consistently outperforms standard LoRA and conventional MoE architectures across multiple clinical NLP tasks while retaining the model's general cognitive capabilities.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.07935v1", "url_pdf": "https://arxiv.org/pdf/2601.07935.pdf", "meta_path": "data/raw/arxiv/meta/2601.07935.json", "sha256": "e2d48fba9f1d25bbd2f1f5cd015ad915df5234ab904a1b65aca758bbf6b42fb9", "status": "ok", "fetched_at": "2026-02-18T02:21:39.616966+00:00"}, "pages": [{"page": 1, "text": "Towards Specialized Generalists: A Multi-Task\nMoE-LoRA Framework for Domain-Specific LLM\nAdaptation\nYuxin Yang\nShanghai University\nAoxiong Zeng\nEast China Normal University\nXiangquan Yang\nEast China Normal University\nAbstract\nThe rapid evolution of Large Language Models (LLMs) has shifted focus from\ngeneral-purpose capabilities to domain-specific expertise.\nHowever, adapting\nLLMs to specialized fields such as medicine presents two challenge: (1) the\n“Stability-Plasticity Dilemma”, where the model must acquire complex clinical\nknowledge without suffering from catastrophic forgetting of general world knowl-\nedge; and (2) “Task Interference”, where disparate sub-tasks—such as medical di-\nagnosis, report summarization, and drug-drug interaction prediction—compete for\nlimited low-rank parameter space. In this paper, we propose Med-MoE-LoRA, a\nnovel framework that integrates Mixture-of-Experts (MoE) with Low-Rank Adap-\ntation (LoRA) to enable efficient multi-task domain adaptation, especially for\nmedical scenarios. Drawing inspiration from recent advances, our framework em-\nploys an asymmetric expert distribution where deeper layers are equipped with\na higher density of LoRA experts to capture complex semantic abstractions. We\nfurther introduce a “Knowledge-Preservation Plugin”, inspired by LoRA MoE,\nto isolate and protect general-purpose reasoning. By utilizing soft merging with\nadaptive routing and rank-wise decoupling, Med-MoE-LoRA achieves superior\nperformance in medical benchmarks while reducing interference. Experimental\nresults demonstrate that our approach consistently outperforms standard LoRA\nand conventional MoE architectures across multiple clinical NLP tasks while re-\ntaining the model’s general cognitive capabilities.\n1\nIntroduction\nLarge Language Models (LLMs), such as the Llama [15, 5] and Qwen [1] series, have demonstrated\nremarkable general-purpose intelligence. However, in high-stakes vertical domains like medicine,\ngeneral intelligence alone is often insufficient. Clinical applications require the model to master\nspecialized terminologies, interpret nuanced patient data, and execute diverse tasks ranging from\nmedical question answering (QA) to automated clinical note generation. Parameter-Efficient Fine-\nTuning (PEFT), particularly Low-Rank Adaptation (LoRA) [8], has become the de facto standard for\nsuch adaptation due to its low computational footprint. Yet, as the complexity of the target domain\nincreases, standard LoRA reveals significant structural limitations.\nThe first major hurdle is Catastrophic Forgetting [12] and Knowledge Overlap. As noted in recent\nLoRA MoE literature [4], during domain-specific fine-tuning, new gradients often overwrite the\n“world knowledge” stored in the frozen backbone through shared low-rank updates. For a medical\nLLM, losing general reasoning or linguistic fluency is unacceptable. Standard LoRA lacks a struc-\ntural mechanism to decouple general knowledge from domain-specific expertise. This necessitates\na “plugin-style” architecture that can selectively activate domain knowledge while keeping general\nknowledge intact.\nPreprint.\narXiv:2601.07935v1  [cs.LG]  12 Jan 2026\n"}, {"page": 2, "text": "Secondly, medical adaptation is inherently a Multi-Task Learning (MTL) problem [18]. A medical\nassistant must handle diagnostic reasoning, which requires logical deduction, and medical sum-\nmarization, which requires linguistic compression. Research on Mixture of LoRA Experts [17],\nsuch as MixLoRA [13] and FlyLoRA [19], highlights that when a single LoRA adapter is forced\nto learn multiple conflicting tasks, “parameter contention” occurs, leading to sub-optimal perfor-\nmance. While Mixture-of-Experts (MoE) offers a solution by providing specialized pathways, tra-\nditional MoE models are often prohibitively expensive to train and deploy. The integration of MoE\nwith LoRA, creating a MoE-LoRA architecture, emerges as a promising middle ground, offering the\nsparsity of MoE with the efficiency of LoRA.\nThirdly, the structural efficiency of these adapters remains under-explored. Empirical studies suggest\nthat “higher layers need more LoRA experts” [6] indicating that the traditional uniform distribution\nof experts across all transformer blocks is sub-optimal. The lower layers of LLMs typically process\nsyntactic and foundational features, while higher layers are responsible for high-level task-specific\nsemantics. Furthermore, architectures like HydraLoRA [14] and FlyLoRA [19] suggest that an\nasymmetric and decoupled rank-wise approach can significantly boost the model’s ability to handle\ndiverse tasks without a proportional increase in parameter count. These insights suggest that the\nrouting mechanism and the physical distribution of experts must be carefully engineered rather than\nsimply stacked.\nAddressing these challenges, we present Med-MoE-LoRA, a framework specifically tailored for the\nmedical domain. Our contributions are fourfold:\n• Dual-Path Knowledge Architecture: We implement a dedicated “Base Expert” alongside\na set of “Specialist Experts.” This design ensures that the model’s foundational world\nknowledge remains a primary path, preventing the erosion of general intelligence during\nintense medical training.\n• Asymmetric Layer-Wise Expert Scaling: Following the principle that complex semantic\ntasks require higher capacity in deeper layers, we implement a graduated expert distribu-\ntion. We increase the number of LoRA experts in the top one-third of the transformer\nlayers, allowing for finer-grained task decoupling where it matters most.\n• Adaptive Soft-Merging Router: To address the rigidity of traditional Top-K routing, we\nemploy a soft-merging mechanism with adaptive routing. This allows for a weighted com-\nbination of experts, enabling the model to navigate the “grey areas” between medical tasks,\nsuch as when a diagnostic task requires the summarization of patient history.\n• Parameter Efficiency via Rank-Wise Decoupling: Adopting the philosophy of asymmet-\nric allocation, we utilize a rank-wise mixture-of-experts. This allows us to push the limits\nof instruction tuning by allocating more “rank” to critical experts while keeping the overall\nparameter count extremely low.\nBy evaluating our framework on PubMedQA [10], MedQA [9], and MIMIC-III [11], we demon-\nstrate that Med-MoE-LoRA not only achieves state-of-the-art results in medical tasks but also pre-\nserves the general reasoning capabilities of the base model. This work provides a roadmap for\ntransforming general LLMs into specialized domain experts without sacrificing their original versa-\ntility.\n2\nRelated Work\nEvolution from Static Adapters to Sparse MoE Architectures.\nWhile Low-Rank Adaptation\n(LoRA) [8] has established itself as the de facto standard for parameter-efficient fine-tuning, its static\narchitecture faces inherent limitations in multi-task and complex domain adaptation. Specifically,\nrelying on a shared low-rank subspace for disparate tasks often induces “parameter contention,”\nwhere conflicting gradients lead to sub-optimal performance across tasks [17, 18]. To mitigate this,\nrecent research has pivoted toward integrating the Mixture-of-Experts (MoE) paradigm with LoRA.\nFrameworks such as MixLoRA [13] and LoRA-MoE [4] introduce sparsity into the adapter space,\nallowing models to dynamically route inputs to specialized experts. This decoupling is crucial for\ndomain adaptation: by isolating task-specific knowledge into separate modules, these architectures\ncan significantly reduce interference and alleviate catastrophic forgetting, effectively maintaining\nthe model’s general capabilities while acquiring new expertise [3, 16].\n2\n"}, {"page": 3, "text": "Structural Optimization and Layer-Wise Sensitivity.\nBeyond simply stacking experts, contem-\nporary work focuses on optimizing the topological distribution of these parameters. Emerging em-\npirical evidence suggests that the demand for adaptation capacity is non-uniform across transformer\nblocks; specifically, higher layers—responsible for abstract semantic processing—benefit dispro-\nportionately from a higher density of experts compared to lower, syntactic layers [6]. Advanced\narchitectures like HydraLoRA [14] and FlyLoRA [19] capitalize on this by employing asymmet-\nric designs and rank-wise decoupling, demonstrating that heterogeneous resource allocation yields\nsuperior efficiency. Our work synthesizes these structural insights—specifically the efficacy of top-\nheavy expert distribution and adaptive soft routing—to construct a framework tailored for the rig-\norous demands of medical NLP, addressing the specific stability-plasticity challenges that generic\nMoE-LoRA models often overlook.\n3\nMethodology\nWe propose Med-MoE-LoRA, a structured parameter-efficient fine-tuning framework designed to\ndecouple general cognitive abilities from domain-specific medical expertise. Our design is motivated\nby two key observations: (1) deep neural networks process features hierarchically, necessitating non-\nuniform adaptation capacities; and (2) clinical tasks often require a synergistic blend of capabilities\n(e.g., reasoning and summarization) rather than disjoint expert activation.\n3.1\nProblem Formulation: From Shared Subspace to Sparse Experts\nStandard LoRA adapts a pre-trained Large Language Model (LLM) by injecting rank-decomposition\nmatrices into frozen layers. For a weight matrix W0 ∈Rd×k, the update is constrained to ∆W =\nBA, where B ∈Rd×r, A ∈Rr×k and r ≪d. In multi-task medical domains, a single shared ∆W\nsuffers from gradient interference, where the optimal subspace for one task (e.g., entity extraction)\nmay be orthogonal to another (e.g., logical deduction).\nTo mitigate this, we reformulate the adaptation layer as a Mixture-of-Experts (MoE). For an input\nhidden state x, the output h is computed as the weighted sum of specialized low-rank experts:\nh = W0x +\nN\nX\ni=1\ngi(x) · (BiAi) x\n(1)\nwhere {Bi, Ai}N\ni=1 represents the i-th LoRA expert, and gi(x) denotes the gating network’s routing\nweight.\n3.2\nDual-Path Knowledge Disentanglement\nA critical challenge in domain adaptation is the “Stability-Plasticity Dilemma”—the model must be\nplastic enough to learn new medical protocols while stable enough to retain general world knowl-\nedge. We address this via a physically disentangled expert topology. We partition the expert set E\ninto two distinct functional groups:\n• The Anchor Pathway (Base Experts Ebase): We designate a subset of experts explic-\nitly for knowledge preservation. These experts are initialized to approximate an identity\nmapping or general-domain behavior. During training, their gradients serve as a stabiliz-\ning force, ensuring that the model retains a dedicated pathway for general linguistic and\nreasoning patterns, unaffected by aggressive domain-specific updates.\n• The Adaptation Pathway (Specialist Experts Espec): The remaining experts are free to\nspecialize in divergent medical sub-tasks. This separation ensures that “world knowledge”\nand “clinical knowledge” do not compete for the same low-rank parameter budget.\n3.3\nHierarchical Asymmetric Expert Allocation\nExisting MoE-LoRA methods typically distribute experts uniformly across all transformer layers.\nHowever, prior research on the interpretability of LLMs suggests a feature hierarchy: lower layers\nprocess universal syntactic features, while higher layers handle abstract semantic reasoning. Uni-\nform allocation is therefore suboptimal—wasteful in lower layers and insufficient in higher layers.\n3\n"}, {"page": 4, "text": "We introduce an Asymmetric Layer-Wise Scaling strategy. Let L be the total number of layers.\nThe number of experts Nl at layer l is determined by a non-linear scaling function:\nNl = Nmin +\n\u0016\n(Nmax −Nmin) ·\n\u0012 l\nL\n\u0013γ\u0017\n(2)\nwhere γ ≥1 controls the curvature of the distribution. This design allocates a dense cluster of ex-\nperts to the top layers (l > 2L/3) to capture high-level task variability (e.g., distinguishing between\ndifferential diagnosis and treatment planning), while keeping the lower layers sparse to preserve\nfundamental syntax.\n3.4\nSoft-Merging with Adaptive Routing\nClinical tasks are rarely mutually exclusive; answering a patient query often requires both medi-\ncal knowledge retrieval and conversational summarization. Hard routing (Top-K) forces a discrete\nchoice, potentially severing these synergistic connections.\nWe employ a Soft-Merging Mechanism with a temperature-scaled gating function. The router\ncomputes a logit score s(x) = Wgx, and the routing weights are normalized via:\ngi(x) =\nexp(si/τ)\nPN\nj=1 exp(sj/τ)\n(3)\nHere, τ is a learnable temperature parameter. Unlike standard MoE where τ is fixed, we allow τ to\nadapt. A higher τ smooths the distribution, enabling the model to “blend” experts (soft merging)\nwhen the task boundary is ambiguous, effectively interpolating between specialized skills.\n3.5\nRank-Aware Capacity Decoupling\nTo push the Pareto frontier of parameter efficiency, we challenge the assumption that all experts\nrequire equal rank.\nComplex reasoning tasks require a higher intrinsic dimension than simple\nsurface-level pattern matching. We implement Rank-Wise Decoupling, where each expert ei is\nassigned a specific rank ri based on its functional role. We define a discrete set of allowable ranks\nR = {8, 16, 32}. The adaptation update becomes:\n∆W(x) =\nN\nX\ni=1\ngi(x) ·\n\u0010\nB(d×ri)\ni\nA(ri×k)\ni\n\u0011\n(4)\nThis allows Med-MoE-LoRA to allocate high-capacity experts (r = 32) to critical pathways (e.g.,\nDiagnostic Logic) while using lightweight experts (r = 8) for auxiliary tasks, maximizing the global\nperformance-to-parameter ratio.\n3.6\nOptimization Objective\nThe training objective combines the task-specific cross-entropy loss Ltask with an auxiliary load-\nbalancing loss to prevent expert collapse. The total loss is defined as:\nL = Ltask + λbal · Laux\n(5)\nwhere Laux minimizes the variance of the gating probabilities across a batch, ensuring that the\nspecialized experts in Espec are utilized effectively and that the model does not degenerate into a\nsingle-expert system.\n4\nExperiments\n4.1\nExperimental Setup\nDatasets and Benchmarks.\nTo comprehensively evaluate the efficacy of Med-MoE-LoRA in bal-\nancing domain specialization with general knowledge preservation, we conduct experiments across\ntwo distinct suites of benchmarks:\n4\n"}, {"page": 5, "text": "• Domain-Specific Capabilities: We utilize PubMedQA [10] to assess biomedical reason-\ning in a closed-domain QA setting; MedQA (USMLE) [9] to evaluate complex clinical\nproblem-solving; and Clinical-Sum, a sampled subset of the MIMIC-III dataset [11], to\ntest the model’s ability to synthesize unstructured clinical notes.\n• General Cognitive Capabilities: To quantify the severity of catastrophic forgetting, we\nemploy MMLU (Massive Multitask Language Understanding) [7] as a proxy for broad\nworld knowledge, and GSM8K [2] to monitor the degradation of mathematical reasoning\nlogic.\nBaselines.\nWe compare our proposed framework against a spectrum of PEFT strategies:\n• Full Fine-Tuning (FFT): Updates all backbone parameters, serving as the upper bound for\nplasticity but often suffering from severe forgetting.\n• Standard LoRA [8]: A single low-rank adapter (r = 16) applied to all tasks, representing\nthe standard unified-weight approach.\n• Multi-LoRA: Trains independent LoRA adapters for each task, switching them strictly\nduring inference. This serves as a baseline for task isolation but lacks cross-task synergy.\n• Vanilla MoE-LoRA: A standard mixture of LoRA experts with uniform layer distribu-\ntion and Top-K routing, used to isolate the gains derived specifically from our asymmetric\nallocation and knowledge-preservation mechanisms.\nImplementation Details.\nAll experiments utilize Llama-3-8B [5] as the foundational backbone.\nFor Med-MoE-LoRA, we implement an asymmetric expert scaling strategy: lower layers (L1−10)\ncontain N = 2 experts to handle syntactic features, while deeper layers (L20−32) scale to N = 8\nexperts to capture semantic nuances. The rank r is dynamically decoupled, ranging from r = 8\nto r = 32, with a scaling factor α = 2r. Training is performed on 8×NVIDIA A100 (80GB)\nGPUs using DeepSpeed Zero-2 offloading. We employ the AdamW optimizer with a learning rate\nof 5 × 10−5, a batch size of 128, and a cosine learning rate scheduler with a 3% warm-up ratio.\nEvaluation Metrics.\nFor QA tasks (PubMedQA, MedQA, MMLU, GSM8K), we report Accu-\nracy (%). For the generative summarization task (Clinical-Sum), we report ROUGE-L scores to\ncapture the fluency and information recall of the generated summaries.\n4.2\nResults and Analysis\n4.2.1\nDomain Adaptation Performance\nTable 1 presents the comparative performance across medical benchmarks.\nMed-MoE-LoRA\nachieves superior results among several parameter-efficient methods, consistently outperforming\nStandard LoRA and Vanilla MoE-LoRA. Specifically, on MedQA, our method surpasses Standard\nLoRA by a substantial margin of +6.9%. Crucially, Med-MoE-LoRA outperforms Multi-LoRA\n(65.8% vs. 61.3%), indicating that our soft-merging mechanism effectively exploits the synergy be-\ntween related sub-tasks (e.g., diagnostic reasoning aiding clinical summarization) rather than treat-\ning them in isolation. The improvement over Vanilla MoE-LoRA (+3.3% on average) further vali-\ndates the effectiveness of our asymmetric expert distribution and rank-wise decoupling strategies.\nTable 1: Performance comparison on Medical Benchmarks. We report Accuracy (%) for QA tasks\nand ROUGE-L for Summarization. Best results in bold.\nMethod\nPubMedQA\nMedQA\nClinical-Sum\nAverage\nBase Model (Zero-shot)\n62.4\n48.2\n18.5\n43.0\nFull Fine-Tuning (FFT)\n78.5\n64.1\n32.1\n58.2\nStandard LoRA\n74.2\n58.9\n28.4\n53.8\nMulti-LoRA\n76.1\n61.3\n30.5\n56.0\nVanilla MoE-LoRA\n76.8\n62.5\n31.2\n56.8\nMed-MoE-LoRA (Ours)\n79.2\n65.8\n33.4\n59.5\n5\n"}, {"page": 6, "text": "4.2.2\nAnalysis of Knowledge Retention (Stability-Plasticity Dilemma)\nA critical objective of Med-MoE-LoRA is to mitigate the “learning-forgetting” trade-off. Table 2\nillustrates the impact of domain training on general capabilities. While FFT and Standard LoRA suf-\nfer significant degradation in mathematical reasoning (GSM8K drops by -8.7% and -3.6%, respec-\ntively), Med-MoE-LoRA maintains near-original performance (−0.3% drop). This empirical\nevidence supports the hypothesis that our Dual-Path Knowledge Architecture successfully compart-\nmentalizes domain-specific gradients into Specialist Experts, leaving the Base Expert pathway intact\nto preserve the model’s foundational logic.\nTable 2: Analysis of Catastrophic Forgetting on General Knowledge Benchmarks.\nMethod\nMMLU\n∆\nGSM8K\n∆\nBase Model\n66.4\n-\n45.1\n-\nFull Fine-Tuning\n58.2\n-8.2\n36.4\n-8.7\nStandard LoRA\n62.1\n-4.3\n41.5\n-3.6\nVanilla MoE-LoRA\n63.8\n-2.6\n43.2\n-1.9\nMed-MoE-LoRA\n65.9\n-0.5\n44.8\n-0.3\n4.2.3\nAblation Study: The Efficacy of Asymmetric Allocation\nWe further investigate the architectural prior that “higher layers demand more experts”. As shown\nin Table 3, shifting the expert density to the deeper layers (L23−32) yields a performance gain of\n+2.7% compared to a uniform distribution. Conversely, a bottom-heavy allocation degrades perfor-\nmance. This corroborates the linguistic theory that lower layers in LLMs process universal syntactic\nfeatures (which require less adaptation), while higher layers manage complex, task-specific semantic\nabstractions that benefit from the expanded capacity of our MoE architecture.\nTable 3: Ablation study on Layer-Wise Expert Allocation strategies.\nAllocation Strategy\nLower Layers (L1−10)\nHigher Layers (L23−32)\nMedical Avg.\nUniform Distribution\nN = 4\nN = 4\n56.8\nBottom-Heavy\nN = 8\nN = 2\n54.2\nAsymmetric (Ours)\nN = 2\nN = 8\n59.5\n4.2.4\nParameter Efficiency and Inference Costs\nFinally, we evaluate the trade-off between model capacity and computational cost (Table 4). Al-\nthough Med-MoE-LoRA increases the total trainable parameter count to 88.4M, the active param-\neters during inference remain strictly controlled (22.4M) due to sparse expert activation. This is\ncomparable to the computational footprint of Standard LoRA + Multi-LoRA switching but achieves\nsignificantly higher performance. This result highlights that Med-MoE-LoRA successfully decou-\nples parameter count from computational cost, allowing for “dense training with sparse inference.”\nTable 4: Comparison of Parameter Efficiency and Inference Overhead.\nMethod\nTrainable Params (M)\nActive Params (M)\nMedical Avg.\nStandard LoRA (r = 16)\n12.5\n12.5\n53.8\nMulti-LoRA (3 tasks)\n37.5\n12.5\n56.0\nVanilla MoE-LoRA\n100.2\n25.1\n56.8\nMed-MoE-LoRA\n88.4\n22.4\n59.5\n6\n"}, {"page": 7, "text": "5\nDiscussion\nOur experimental results offer compelling evidence that integrating Mixture-of-Experts with Low-\nRank Adaptation, when guided by specific structural priors, effectively resolves the tension between\ndomain specialization and general knowledge retention.\nDecoupling Knowledge via Gradient Isolation. The most significant insight from our study is the\nvalidation of the “Dual-Path” architecture in mitigating the stability-plasticity dilemma. Standard\nPEFT methods often suffer from weight interference because a shared low-rank subspace is forced\nto accommodate orthogonal gradient updates—simultaneously maintaining world knowledge while\nacquiring conflicting clinical specifics. By physically isolating the “Base Expert” to preserve general\ncapabilities, Med-MoE-LoRA ensures that domain-specific updates do not erode the model’s foun-\ndational reasoning. This structural decoupling explains why our method maintains near-baseline\nperformance on GSM8K and MMLU, whereas standard LoRA exhibits significant degradation. It\nsuggests that future efficient tuning frameworks must explicitly model knowledge separation rather\nthan relying solely on regularization.\nStructural Priors and Semantic Resolution. Our findings also challenge the conventional uniform\ndistribution of adapters. The ablation study on layer-wise allocation corroborates the hypothesis\nthat deep neural networks process information hierarchically—progressing from syntactic features\nin lower layers to abstract semantic reasoning in upper layers. Consequently, concentrating expert\ncapacity in the top one-third of the transformer blocks provides the necessary “semantic resolu-\ntion” to disambiguate complex medical tasks without wasting parameters on foundational layers.\nFurthermore, the success of our rank-wise decoupling strategy demonstrates that task difficulty is\nnon-uniform; assigning higher ranks to diagnostic reasoning while using lower ranks for summa-\nrization allows the model to operate closer to the Pareto frontier of parameter efficiency.\nLimitations and Future Directions. Despite these advancements, our current framework relies on\na static expert topology with a predefined number of experts. This rigidity may limit adaptation\nto unseen sub-domains that emerge dynamically. Future work could explore dynamic expert con-\nstruction, where the model autonomously instantiates new LoRA experts upon detecting novel data\ndistributions. Additionally, given the multimodal nature of clinical practice, extending this sparse\nMoE-LoRA architecture to integrate non-textual modalities, such as medical imaging or time-series\nsensor data, remains a promising avenue for developing truly holistic medical assistants.\n6\nConclusion\nIn this paper, we introduced Med-MoE-LoRA, a parameter-efficient framework designed to trans-\nform general-purpose LLMs into specialized domain experts without sacrificing their broad cogni-\ntive capabilities. By synthesizing an asymmetric expert distribution, adaptive soft-merging routing,\nand a dual-path knowledge preservation mechanism, our approach systematically addresses the chal-\nlenges of catastrophic forgetting and multi-task interference. Extensive evaluations across medical\nand general benchmarks demonstrate that Med-MoE-LoRA achieves superior domain adaptation\nperformance while maintaining a minimal computational footprint during inference. Beyond the\nmedical domain, this work provides a scalable blueprint for “Specialized Generalist”, offering a\ngeneralized methodology for deploying high-performance, domain-aware LLMs in high-stakes en-\nvironments where both precision and general reasoning are indispensable.\n7\n"}, {"page": 8, "text": "References\n[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\nGe, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n[2] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n[3] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng\nXi, Xiao Wang, Xiaoran Fan, et al. Loramoe: Alleviate world knowledge forgetting in large\nlanguage models via moe-style plugin. arXiv preprint arXiv:2312.09979, 2023.\n[4] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Junjie Wang, Wei Shen, Yuhao Zhou, Zhi-\nheng Xi, Xiao Wang, Xiaoran Fan, et al. Loramoe: Alleviate world knowledge forgetting in\nlarge language models via moe-style plugin. arXiv preprint arXiv:2312.09979, 2024.\n[5] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Keshwam, Ahmad Al-dahle,\nAdithya Renduchintala, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783,\n2024.\n[6] Chongyang Gao, Kezhen Chen, Jinmeng Rao, Baochen Sun, Ruibo Liu, Daiyi Peng, Yawen\nZhang, Xiaoyuan Guo, Jie Yang, and VS Subrahmanian. Higher layers need more lora experts.\narXiv preprint arXiv:2402.08562, 2024.\n[7] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. In International Con-\nference on Learning Representations, 2021.\n[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In In-\nternational Conference on Learning Representations, 2022.\n[9] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What\ndisease does this patient have? a large-scale open domain question answering dataset from\nmedical exams. In Applied Sciences, 2021.\n[10] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. Pubmedqa:\nA dataset for biomedical research question answering. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pages 2567–2577, 2019.\n[11] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad\nGhassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-\niii, a freely accessible critical care database. Scientific data, 3(1):1–9, 2016.\n[12] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, An-\ndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.\nOvercoming catastrophic forgetting in neural networks. Proceedings of the national academy\nof sciences, 114(13):3521–3526, 2017.\n[13] Wenzheng Li, Yelong Gong, G Wang, et al. Mixlora: Enhancing large language models fine-\ntuning with lora-based mixture of experts. arXiv preprint arXiv:2404.15159, 2024.\n[14] Chunlin Tian, Zhan Shi, Zhijiang Guo, Li Li, and Cheng-Zhong Xu. Hydralora: An asym-\nmetric lora architecture for efficient fine-tuning. Advances in Neural Information Processing\nSystems, 37:9565–9584, 2024.\n[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[16] Xun Wu, Shaohan Huang, and Furu Wei.\nMixture of lora experts.\narXiv preprint\narXiv:2404.13628, 2024.\n8\n"}, {"page": 9, "text": "[17] Xun Wu, Shaohan Huang, and Furu Wei. Mole: Mixture of lora experts. In International\nConference on Learning Representations, 2024.\n[18] Yu Zhang and Qiang Yang. An overview of multi-task learning. National Science Review, 5\n(1):30–43, 2018.\n[19] Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, and Xiangyang Ji. FlyloRA: Boosting\ntask decoupling and parameter efficiency via implicit rank-wise mixture-of-experts. In The\nThirty-ninth Annual Conference on Neural Information Processing Systems, 2025.\n9\n"}]}