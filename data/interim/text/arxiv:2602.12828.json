{"doc_id": "arxiv:2602.12828", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.12828.pdf", "meta": {"doc_id": "arxiv:2602.12828", "source": "arxiv", "arxiv_id": "2602.12828", "title": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories", "authors": ["Zhan Qu", "Michael Färber"], "published": "2026-02-13T11:30:37Z", "updated": "2026-02-13T11:30:37Z", "summary": "Predicting future clinical events from longitudinal electronic health records (EHRs) is challenging due to sparse multi-type clinical events, hierarchical medical vocabularies, and the tendency of large language models (LLMs) to hallucinate when reasoning over long structured histories. We study next-visit event prediction, which aims to forecast a patient's upcoming clinical events based on prior visits. We propose GRAIL, a framework that models longitudinal EHRs using structured geometric representations and structure-aware retrieval. GRAIL constructs a unified clinical graph by combining deterministic coding-system hierarchies with data-driven temporal associations across event types, embeds this graph in hyperbolic space, and summarizes each visit as a probabilistic Central Event that denoises sparse observations. At inference time, GRAIL retrieves a structured set of clinically plausible future events aligned with hierarchical and temporal progression, and optionally refines their ranking using an LLM as a constrained inference-time reranker. Experiments on MIMIC-IV show that GRAIL consistently improves multi-type next-visit prediction and yields more hierarchy-consistent forecasts.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.12828v1", "url_pdf": "https://arxiv.org/pdf/2602.12828.pdf", "meta_path": "data/raw/arxiv/meta/2602.12828.json", "sha256": "516ac33b329a16f11b011681d7dc7db9be6ec3b77e2c6f201ccab901d40c649c", "status": "ok", "fetched_at": "2026-02-18T02:19:20.500297+00:00"}, "pages": [{"page": 1, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs\nover Hyperbolic Representations of Patient Trajectories\nZhan Qu 1 2 Michael F¨arber 1 2\nAbstract\nPredicting future clinical events from longitu-\ndinal electronic health records (EHRs) is chal-\nlenging due to sparse multi-type clinical events,\nhierarchical medical vocabularies, and the ten-\ndency of large language models (LLMs) to hal-\nlucinate when reasoning over long structured\nhistories. We study next-visit event prediction,\nwhich aims to forecast a patient’s upcoming clin-\nical events based on prior visits. We propose\nGRAIL, a framework that models longitudinal\nEHRs using structured geometric representations\nand structure-aware retrieval. GRAIL constructs\na unified clinical graph by combining determin-\nistic coding-system hierarchies with data-driven\ntemporal associations across event types, em-\nbeds this graph in hyperbolic space, and sum-\nmarizes each visit as a probabilistic Central Event\nthat denoises sparse observations. At inference\ntime, GRAIL retrieves a structured set of clin-\nically plausible future events aligned with hier-\narchical and temporal progression, and option-\nally refines their ranking using an LLM as a con-\nstrained inference-time reranker. Experiments on\nMIMIC-IV show that GRAIL consistently im-\nproves multi-type next-visit prediction and yields\nmore hierarchy-consistent forecasts. Our code is\npublicly available at: GRAIL-1EB1\n1. Introduction\nLongitudinal electronic health records (EHRs) are central\nto clinical decision support, enabling models of disease pro-\ngression, treatment response, and adverse events over time\n(Rajkomar et al., 2018; Harutyunyan et al., 2019; Choi et al.,\n2016a). A large body of work represents patient histories\nas sequences of visits and applies recurrent or transformer-\nbased architectures to predict future clinical events or risks\n1Department of Computer Science, TU Dresden, Dresden, Ger-\nmany 2ScaDS.AI, Dresden, Germany. Correspondence to: Zhan\nQu <zhan.qu@tu-dresden.de>.\nPreprint. February 16, 2026.\n(Choi et al., 2016b; Li et al., 2020). More recently, large\nlanguage models (LLMs) have been explored for clinical\nreasoning due to their strong semantic representations and\nfew-shot generalization capabilities (Singhal et al., 2023;\nNori et al., 2023). However, the effective use of LLMs for\nlongitudinal EHR analysis remains limited by a fundamental\nmismatch between the unique structure of medical data and\nthe assumptions of modern generative models.\nA key difficulty is that EHR trajectories are sparse, irregu-\nlarly sampled, and multi-modal (Lipton et al., 2015; Che\net al., 2018). Within a single admission, information is\nrecorded as multiple unordered sets of entities drawn from\ndifferent coding systems (Johnson et al., 2023), such as di-\nagnoses in ICD and medications in NDC/ATC, rather than\nas a coherent sequence of clinically salient states. This\nfragmentation makes it unclear which elements within a\nvisit are most representative of the patient’s condition, and\nit leaves many clinically meaningful relationships implicit\n(e.g., which diagnoses motivate which treatments, or which\nmedications drive which lab changes).\nOne approach to recovering structure is to incorporate ex-\nternal biomedical knowledge bases such as UMLS (Boden-\nreider, 2004) to provide relations and hierarchies (Qu &\nF¨arber, 2025). However, these resources can be difficult\nto operationalize at scale: they merge overlapping vocab-\nularies, include redundant or weakly specified relations,\nand vary widely in granularity across concept types. In\ncontrast, widely used medical coding systems, such as ICD-\n10 (WHO, 2004) for diagnoses and the Anatomical Ther-\napeutic Chemical (ATC) classification (WHO, 2024) for\nmedications, provide reliable and universally available de-\nterministic hierarchical structure. While typically shallow,\nthese hierarchies encode clinically meaningful abstraction\n(coarse-to-fine groupings) in a way that is consistent across\ninstitutions and datasets.\nYet hierarchies alone do not capture the full complexity of\nlongitudinal EHRs: clinically meaningful dependencies fre-\nquently span modalities and unfold over time. Diagnoses\nprecede treatments, medications affect laboratory values,\nand these interactions are rarely explicit in the data. Im-\nportantly, longitudinal EHRs still encode such relationships\nimplicitly through co-occurrence patterns and temporal or-\n1\narXiv:2602.12828v1  [cs.LG]  13 Feb 2026\n"}, {"page": 2, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\ndering across visits. This motivates complementing deter-\nministic hierarchies with data-driven association signals\ncomputed directly from patient trajectories, which can re-\ncover cross-modal and temporal structure without relying\non noisy external ontologies.\nRepresenting both hierarchical abstraction and data-driven\nassociations in a unified framework raises a central modeling\nquestion: what geometry best supports this structure? Eu-\nclidean embeddings treat distances isotropically and provide\nno native notion of containment or directionality, making\nit difficult to represent abstraction and progression consis-\ntently. Hyperbolic geometry, by contrast, offers an inductive\nbias well-suited to hierarchical organization and asymmetric\nrelationships (Nickel & Kiela, 2017; Ganea et al., 2018). In\nthis work, we embed clinical concepts in hyperbolic space,\nsummarize visits using hyperbolic barycenters, and exploit\ngeometric directionality to support structure-aware retrieval\nof plausible future clinical events.\nFinally, while LLMs offer powerful semantic reasoning ca-\npabilities, their direct application to raw longitudinal EHRs\nis constrained by context limits and a lack of structured\ngrounding (Zhou et al., 2025; Qu & F¨arber, 2025). Naively\nlinearizing long, multi-modal histories can increase halluci-\nnation risk and produce clinically inconsistent predictions.\nWe therefore treat LLMs as inference-time reasoning mod-\nules that operate over a compact, structured set of retrieved\ncandidates rather than over raw EHR sequences. By con-\nstraining LLMs to rerank clinically plausible events derived\nfrom structured representations, we combine hyperbolic\nstructure as a strong prior with LLM semantic reasoning,\nwhile reducing hallucinations by restricting the output space.\nWe introduce GRAIL (Geometric Reasoning Augmented\nInference with LLMs), a framework that integrates determin-\nistic coding-system hierarchies, data-driven cross-modal as-\nsociations, hyperbolic representation learning, and structure-\naware retrieval for longitudinal EHR modeling. GRAIL\nconstructs a unified clinical graph, embeds it in hyperbolic\nspace, compresses each visit into a denoised Central Event,\nand retrieves a structured Risk Horizon of candidate future\nevents. LLMs are optionally used to rerank retrieved candi-\ndates at inference time. Our main contributions are:\n1. Hyperbolic Graph Construction: We construct deter-\nministic trees for disjoint modalities (diagnoses, medi-\ncations, labs) and stitch them into a single connected\ntopology using lagged pointwise mutual information\n(PMI), yielding a statistically grounded, noise-reduced\ngraph that connects heterogeneous clinical inputs.\n2. Hyperbolic Central Event Denoising:\nWe com-\npress sparse, multi-modal admissions by comput-\ning a (tangent-space) Fr´echet-mean approximation of\nevent embeddings in hyperbolic space, producing a\nsemantics-preserving Central Event that drastically re-\nduces sequence length for downstream processing.\n3. Structure-Aware Geometric Prompting: We develop\na retrieval mechanism that uses hyperbolic distance and\ndirectionality to define a Risk Horizon of likely next\nevents. By augmenting LLM prompts with structured,\ntrajectory-consistent candidates, we reduce hallucina-\ntion and improve forecasting accuracy.\n2. Related Work\nDeep Sequential Models for Longitudinal EHRs.\nEarly\nwork modeled longitudinal EHRs as temporal sequences\nusing recurrent neural networks, demonstrating the feasi-\nbility of next-visit prediction from diagnosis codes (Choi\net al., 2016a; Pham et al., 2016).\nRETAIN introduced\nreverse-time attention to improve interpretability (Choi et al.,\n2016b), while subsequent models incorporated uncertainty-\naware and survival objectives (Alaa & van der Schaar, 2019).\nTransformer-based architectures later treated medical codes\nas tokens, enabling long-range dependency modeling (Li\net al., 2020; Rasmy et al., 2021). Recent work has explored\nlong-context and encoder–decoder transformers for EHRs,\nshowing improved robustness to irregular timing and com-\nplex disease trajectories (Wornow et al., 2024; Yang et al.,\n2023). Foundation models such as MOTOR further scale\ntemporal modeling via time-to-event pretraining on large co-\nhorts (Steinberg et al., 2023). While effective for temporal\npattern extraction, these methods typically flatten heteroge-\nneous medical concepts into Euclidean token embeddings,\ndiscarding explicit hierarchical structure and cross-modal\nrelationships that are central to clinical reasoning.\nOntology-Aware and Graph-Based Clinical Represen-\ntation Learning.\nTo incorporate medical knowledge,\nontology-aware methods leverage hierarchical structures\nsuch as ICD and SNOMED-CT. GRAM propagates infor-\nmation from ancestor concepts via attention (Choi et al.,\n2017), while KAME integrates external knowledge graphs\ninto memory networks (Ma et al., 2018). Graph-based ap-\nproaches construct patient–code or code–code graphs and\napply GNNs for representation learning (Shang et al., 2019;\nMao et al., 2022). More recent work models EHRs as tem-\nporal heterogeneous graphs using graph transformers or\ntime-aware message passing (Xu et al., 2022; Chen et al.,\n2024). Although effective, these methods embed ontological\nand relational structure implicitly in Euclidean space, which\nlimits their ability to represent hierarchy, directionality, and\nasymmetric abstraction, and they often rely on external KGs\nthat may introduce noise and scalability challenges.\nHyperbolic Geometry for Medical Representation Learn-\ning\nHyperbolic geometry provides low-distortion embed-\n2\n"}, {"page": 3, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\nFigure 1. Overview of the GRAIL framework. (A) We construct a unified typed graph G by merging deterministic coding hierarchies\nwith data-driven, lagged cross-modal associations. (B) The model learns hyperbolic node embeddings and denoises visits into probabilistic\n“Central Events” (µt) via joint edge reconstruction and masked-visit self-supervision. (C) At inference, geometric “Risk Cones” restrict\nretrieval to hierarchically consistent, downstream candidates to form a Typed Risk Horizon (RT ). (D) An optional LLM component\nrescores these candidates using compressed patient history, combining geometric and semantic signals for final next-visit prediction.\ndings for hierarchical and tree-structured data due to its\nexponential volume growth (Sarkar, 2011; Nickel & Kiela,\n2017). Hyperbolic entailment cones and hyperbolic GNNs\nextend these ideas to neural models operating on non-\nEuclidean manifolds (Ganea et al., 2018; Chami et al.,\n2019; Liu et al., 2019). In the medical domain, hyperbolic\nembeddings have been used to capture latent hierarchies\namong ICD codes and reflect clinically meaningful group-\nings (Beaulieu-Jones et al., 2019). Subsequent work demon-\nstrated benefits for EHR prediction tasks such as mortality\nand readmission prediction (Lu et al., 2019), as well as\nself-supervised and graph-based frameworks for diagnosis\nprediction (Lu et al., 2021; Naseem et al., 2024). Hyper-\nbolic representations have also been applied to automatic\nICD coding, where explicit encoding of ICD hierarchies im-\nproves multi-label classification (Cao et al., 2020; Wu et al.,\n2024). However, existing hyperbolic approaches remain\nlargely diagnosis-centric and focus on classification-style\ntasks, leaving heterogeneous, multi-modal longitudinal tra-\njectories underexplored.\nRetrieval-Augmented Generation in Medicine\nLarge\nlanguage models have demonstrated strong performance\non medical question answering and reasoning benchmarks\n(Singhal et al., 2023; Nori et al., 2023), but hallucination\nremains a critical risk in high-stakes clinical settings (Ji\net al., 2023). Retrieval-augmented generation mitigates this\nrisk by grounding responses in retrieved evidence, typically\nfrom large text corpora (Lewis et al., 2020; Guu et al., 2020).\nIn medicine, recent work has benchmarked RAG pipelines\nand shown that retrieval strategies and corpora substantially\naffect clinical QA performance (Xiong et al., 2024). Be-\nyond text retrieval, knowledge graph–augmented prompting\nretrieves structured context such as entities, relations, or\nsubgraphs (Pan et al., 2024; Zeng et al., 2025), and recent\nsystems integrate graph-based retrieval into medical RAG\npipelines (Wu et al., 2025). However, many retrieval mech-\nanisms rely on lexical overlap or Euclidean similarity, and\nstructured evidence is often linearized before prompting,\ndiscarding relational and hierarchical information (Lewis\net al., 2020; Pan et al., 2024). This motivates structure-aware\nretrieval criteria that leverage the geometry of hierarchical\nmedical knowledge, as pursued in our approach.\n3\n"}, {"page": 4, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\n3. Methodology\nWe propose GRAIL (Geometric Reasoning Augmented In-\nference with LLMs), a framework for longitudinal EHR\nmodeling that integrates (i) deterministic coding-system\nhierarchies, (ii) data-driven cross-modal temporal associ-\nations, (iii) hyperbolic representation learning, and (iv)\nstructure-aware retrieval with optional LLM reranking at\ninference time.\n3.1. Overview of the GRAIL Pipeline\nGRAIL consists of four stages (as in Figure 1): (A) con-\nstructing a unified typed clinical graph from deterministic\nhierarchies and lagged cross-modal associations, (B) learn-\ning hyperbolic node embeddings via typed edge reconstruc-\ntion, and denoising each visit into a probabilistic Central\nEvent using masked-visit self-supervision, (C) performing\nstructure-aware retrieval to form a typed Risk Horizon, and\n(D) optional LLM-based reranking for next-visit prediction.\nAlgorithm 1 summarizes the end-to-end procedure.\n3.2. Problem Setup and Notation\nA patient trajectory is a sequence of visits indexed by time\nt ∈{1, . . . , T}. Each visit is a set-valued, multi-modal\nobservation:\nSt =\nn\nS(dx)\nt\n, S(proc)\nt\n, S(med)\nt\n, S(lab)\nt\no\n,\nS(m)\nt\n⊂V(m).\n(1)\nGiven history S1:t, we aim to predict next-visit events S(m)\nt+1\n(multi-label) and/or a designated primary event. We denote\nthe unified vocabulary by\nV =\n[\nm\nV(m) ∪V(anc),\n(2)\nwhere V(anc) contains deterministic ancestor nodes.\n3.3. Unified Typed Clinical Graph Construction\nWe construct a directed typed graph G = (V, E) with two\nedge families: (i) deterministic hierarchical edges and (ii)\ndata-driven temporal cross-modal edges. Edges are parti-\ntioned by relation types r ∈R:\nE = Ehier ∪Ecross,\nE =\n[\nr∈R\nEr.\n(3)\n3.3.1. DETERMINISTIC VERTICAL HIERARCHIES\nWithin each modality, we build deterministic parent–child\nedges from coding structure (e.g., ICD prefixes, ATC levels).\nWe direct edges from abstract parents to specific children:\nEhier =\nn\n(p →v) : p = parent(v), v ∈V(m)o\n.\n(4)\nAlgorithm 1 GRAIL: Geometric Reasoning Augmented\nInference with LLMs\nRequire: Longitudinal EHR trajectories {S(i)\n1:T }N\ni=1\nEnsure: Next-visit prediction bST +1\n1: Graph Construction\n2: Build deterministic hierarchical edges Ehier\n3: for each modality pair (m, m′) and lag ∆do\n4:\nCompute lagged co-occurrence statistics\n5:\nCompute PMI∆\n6:\nRetain edges using Eq. (7)\n7: end for\n8: Apply stability filtering using Eq. (9)\n9: Construct graph G = (V, E)\n10: Hyperbolic Node Embedding\n11: Initialize embeddings {zv} ⊂Dd\nc\n12: repeat\n13:\nSample typed edges and negatives\n14:\nUpdate embeddings by minimizing Ledge\n15: until convergence\n16: Central Event Pretraining\n17: repeat\n18:\nfor each visit St do\n19:\nSample masked codes Mt\n20:\nCompute barycenter µt\n21:\nMinimize Lmask\n22:\nend for\n23: until convergence\n24: Inference\n25: Compute Central Event CET\n26: Retrieve Risk Horizon RT\n27: Rank candidates using geometric score\n28: if LLM reranking enabled then\n29:\nCombine geometric and LLM scores\n30: end if\n31: Output bST +1\nThis directionality ensures that specialization corresponds\nto moving “downstream” in the hierarchy.\n3.3.2. TEMPORAL CROSS-MODAL STITCHING VIA\nLAGGED PMI\nTo connect modality trees and encode longitudinal depen-\ndencies, we define lagged co-occurrence for a ∈V(m) and\nb ∈V(m′), m ̸= m′:\nP∆(a, b) = Pr\n\u0010\na ∈S(m)\nt\n, b ∈S(m′)\nt+∆\n\u0011\n,\n∆∈{0, . . . , ∆max}.\n(5)\nWe compute lagged PMI (associational, not causal):\nPMI∆(a →b) = log P∆(a, b)\nP(a)P(b).\n(6)\n4\n"}, {"page": 5, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\nWe retain a directed cross-modal edge a →b if\nPMI∆(a →b) > τm,m′,∆\n∧\ncnt∆(a, b) ≥κ,\n(7)\nwhere κ is a minimum support threshold. Each retained\nedge is assigned a type\nr = type(a →b) = (m →m′, ∆),\n(8)\nyielding Ecross = S\nr Er.\n3.3.3. STABILITY FILTERING\nTo reduce dataset-specific artifacts, we apply bootstrap sta-\nbility filtering to PMI-derived edges. Let E(b)\ncross denote\nthe cross-modal edge set recovered on bootstrap resample\nb ∈{1, . . . , B}. We keep edge e iff\nstab(e) = 1\nB\nB\nX\nb=1\nI\nh\ne ∈E(b)\ncross\ni\n≥q,\n(9)\nwhere q ∈(0, 1] controls the stability requirement.\n3.4. Hyperbolic Geometry and Node Embedding\nWe embed nodes into the d-dimensional Poincar´e ball of\ncurvature −c:\nDd\nc =\n\b\nx ∈Rd : ∥x∥< 1/√c\n\t\n,\nzv ∈Dd\nc ∀v ∈V.\n(10)\nFor any two embeddings x, y ∈Dd\nc, we use the hyperbolic\ndistance dD(x, y) (Appendix A.1). For an observed edge\n(u →v) of type r, we define a compatibility score\nsr(u, v) = −dD(zu, zv).\n(11)\nEmbeddings are trained via typed edge reconstruction with\nnegative sampling; the full typed edge reconstruction objec-\ntive and optimization details are provided in Appendix A.2\nand optimized with Riemannian SGD/Adam with projection\nto enforce ∥zv∥< 1/√c.\n3.5. Visit Denoising via Probabilistic Central Events\nLet Ct = S\nm S(m)\nt\nbe the set of all codes observed at visit t.\nWe compute a visit-level hyperbolic barycenter as a closed-\nform approximation of the Fr´echet mean using log–exp\nmaps at the origin:\nµt = expc\n0\n \n1\nP\nci∈Ct wt,i\nX\nci∈Ct\nwt,i logc\n0(zci)\n!\n,\n(12)\nwhere wt,i ≥0 are code weights (uniform by default).\nRather than snapping µt to a single nearest node, we define\na soft assignment over candidate nodes:\npt(v) =\nexp(−β dD(µt, zv))\nP\nv′∈Vt exp(−β dD(µt, zv′)),\n(13)\nwhere Vt is a restricted candidate pool (e.g., same-modality\nnodes and ancestors) for efficiency. The Central Event is\nCEt = (µt, pt(·)).\n3.6. Masked-Visit Reconstruction for Self-Supervised\nDenoising\nTo train Central Events to preserve predictable clinical signal\nwhile filtering noise, we randomly mask a subset of visit\ncodes Mt ⊂Ct and compute µt from Ct \\ Mt. We then\nscore a masked code c ∈Mt by hyperbolic distance:\np(c | µt) ∝exp(−dD(µt, zc)/τ) .\n(14)\nTraining minimizes the sampled-softmax / contrastive re-\nconstruction loss\nLmask = −\nX\nc∈Mt\nlog p(c | µt),\n(15)\nwith an efficient negative-sampling normalization. The com-\nplete sampled-softmax formulation and negative sampling\nstrategy are detailed in Appendix A.3, Eq. (29).\n3.7. Structure-Aware Retrieval via Hyperbolic Risk\nHorizons\nGiven the most recent Central Event at time T, GRAIL per-\nforms structure-aware retrieval to identify a compact set\nof clinically plausible next-visit events. We refer to this\nretrieved, modality-typed candidate set as the Risk Hori-\nzon. Unlike standard nearest-neighbor retrieval in embed-\nding space, our approach explicitly incorporates hierarchi-\ncal structure, cross-modal temporal associations, and direc-\ntional progression.\n3.7.1. CANDIDATE POOL CONSTRUCTION\nLet vT = arg maxv pT (v) denote the representative con-\ncept of the current visit, selected as the maximum-a-\nposteriori estimate under the Central Event distribution. We\nconstruct an initial candidate pool by unifying three struc-\ntured neighborhoods:\nCcand\nT\n= Desc(vT ) ∪Assoc(vT ) ∪Pred(vT ),\n(16)\nwhere: (i) Desc(vT ) denotes hierarchical descendants of\nvT within the deterministic coding-system hierarchy, (ii)\nAssoc(vT ) denotes same-visit cross-modal associates (i.e.,\nedges with ∆= 0), and (iii) Pred(vT ) denotes lagged pre-\ndictive neighbors (i.e., edges with ∆> 0) derived from\ntemporal stitching. This construction ensures that candi-\ndates are grounded in clinically observed hierarchical and\ntemporal dependencies.\n3.7.2. RISK CONES FOR DIRECTIONAL RETRIEVAL\nWhile the candidate pool captures structural relevance, not\nall candidates represent plausible future developments. To\n5\n"}, {"page": 6, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\nenforce directional consistency, we introduce risk cones,\nwhich restrict retrieval to concepts that lie downstream of\nthe current patient state in hyperbolic space.\nLet r(vT ) denote the root (or highest-level ancestor) of vT\nin its modality-specific hierarchy. We define the downstream\ndirection in the tangent space at the origin as\ndT = logc\n0(zvT ) −logc\n0(zr(vT )).\n(17)\nFor a candidate node u, we compute its angular deviation\nfrom this downstream direction:\ncos ∠(u; vT ) =\n⟨logc\n0(zu), dT ⟩\n∥logc\n0(zu)∥∥dT ∥.\n(18)\nWe say that u lies within the risk cone of vT if\n∠(u; vT ) ≤ϕ,\n(19)\nwhere ϕ is a cone aperture parameter controlling the trade-\noff between specificity and recall. Intuitively, risk cones\nfavor specializations and downstream developments over\nunrelated or overly abstract concepts.\nGeometric relevance scoring.\nAfter filtering candidates\nby risk cone membership, we rank retained nodes using a\ngeometric relevance score that balances proximity to the\ncurrent visit representation and directional alignment:\nsgeo(u; T) = −dD(zu, µT ) + η · I[u ∈Cone(vT )] , (20)\nwhere µT is the visit barycenter of the Central Event at time\nT, I[·] is the indicator function, and η ≥0 controls the\nstrength of the directional prior.\nTyped Risk Horizon.\nFinally, we select the top-K ranked\ncandidates per modality according to sgeo(u; T) to form the\ntyped Risk Horizon\nRT =\nn\nR(dx)\nT\n, R(proc)\nT\n, R(med)\nT\n, R(lab)\nT\no\n,\n(21)\nwhich serves as structured, geometry-aware context for\ndownstream inference and LLM-based reranking (Sec-\ntion 3.8).\n3.8. LLM Prompting and Geometric–LLM Reranking\nWe use the Risk Horizon to constrain LLM inference to\nclinically plausible candidates. Let Desc(·) map codes to\nshort text descriptions. The LLM receives (i) a compressed\nhistory from Central Events and (ii) candidates grouped by\nmodality. For each candidate u ∈R(m)\nT\n, we obtain an LLM\nscore sllm(u; T) (e.g., log-probability under multiple-choice\nprompting) and combine it with the geometric score:\ns(u; T) = λ sllm(u; T) + (1 −λ) sgeo(u; T),\n(22)\npredicting top-k events per modality:\nbS(m)\nT +1 = Top-k{s(u; T) : u ∈R(m)\nT\n}.\n(23)\nLLM reranking is used strictly at inference time; we do not\nbackpropagate through the LLM.\n3.9. Training Objective and Optimization\nWe jointly train node embeddings and Central Event denois-\ning using\nL = Ledge + α Lmask,\n(24)\nwhere Ledge is the typed edge reconstruction loss (Ap-\npendix A.2, Eq. (27)) and Lmask is the masked-visit re-\nconstruction loss (Appendix A.3, Eq. (29)). We optimize\non the Poincar´e ball using Riemannian SGD/Adam with\nprojection to maintain zv ∈Dd\nc.\n4. Experimental Setup\nWe evaluate GRAIL on the MIMIC-IV database (Johnson\net al., 2023), focusing on longitudinal forecasting robustness,\nmulti-modal integration, and inference grounding.\nDataset and Cohort. After filtering for patients with se-\nquence length T ≥2, we extract longitudinal trajectories for\na cohort of 14,013 patients comprising 183,965 total visits.\nThe average trajectory length is 3 visits per patient (min = 2,\nmax = 18). The unified vocabulary V contains 25,808 unique\nnodes and 115,0359 edges across four modalities, with an\naverage density of 47 codes per visit (dx/proc/med/lab =\n15/3/12/17). Detailed cohort construction, vocabulary fil-\ntering (≈25 occurrence threshold), and leakage control\nprotocols are detailed in Appendix B.\nTasks. We evaluate on (i) Multi-modal next-visit predic-\ntion, ranking the likelihood of all codes in St+1 given his-\ntory S1:t, and (ii) Primary diagnosis prediction, a single-\nlabel classification of the primary condition at t + 1.\nBaselines. We compare GRAIL against three baseline cat-\negories: (i) Sequential encoders that linearize visits into\nEuclidean token sequences, including Transformer (Eucl.),\nBEHRT, and RETAIN; (ii) Graph and retrieval methods,\nincluding Euclidean RAG (Euclidean embedding kNN re-\ntrieval), Euclidean GRAIL (matched-topology ablation us-\ning Rd embeddings), and Hyperbolic (no stitch), which\nremoves lagged PMI cross-modal edges to isolate the effect\nof temporal stitching; and (iii) LLM baselines, where Llama-\n3.1-8B-Instruct, Llama-3.1-70B-Instruct, and Qwen3-8B\nperform zero-shot next-event selection from large candidate\nspaces, as well as a standard Euclidean RAG+LLM setup.\nAll LLM methods are inference-only and use the same his-\ntory compression for fairness. Hyperparameter details and\nbaseline implementations are provided in Appendix C.\n6\n"}, {"page": 7, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\nTable 1. Multi-modal next-visit prediction on MIMIC-IV (per event type).\nDx\nProc\nMed\nLab\nModel\nR@5\nR@10\nnDCG@10\nR@5\nR@10\nnDCG@10\nR@5\nR@10\nnDCG@10\nR@5\nR@10\nnDCG@10\nSequential Baselines\nTransformer (Eucl.)\n0.214\n0.318\n0.232\n0.121\n0.197\n0.138\n0.066\n0.112\n0.074\n0.058\n0.103\n0.067\nBEHRT\n0.226\n0.334\n0.246\n0.128\n0.206\n0.145\n0.070\n0.118\n0.078\n0.061\n0.109\n0.071\nRETAIN\n0.201\n0.301\n0.217\n0.113\n0.188\n0.132\n0.062\n0.106\n0.070\n0.055\n0.098\n0.064\nGraph & Retrieval\nEuclidean RAG\n0.241\n0.357\n0.266\n0.136\n0.221\n0.156\n0.080\n0.132\n0.090\n0.073\n0.123\n0.083\nEuclidean GRAIL\n0.257\n0.377\n0.284\n0.146\n0.234\n0.167\n0.086\n0.140\n0.097\n0.078\n0.131\n0.088\nHyperbolic (no stitch)\n0.265\n0.387\n0.293\n0.149\n0.240\n0.171\n0.090\n0.146\n0.101\n0.082\n0.138\n0.092\nLarge Language Models (zero-shot)\nLlama-3.1-8B-Instruct\n0.154\n0.243\n0.165\n0.081\n0.142\n0.094\n0.041\n0.075\n0.048\n0.037\n0.070\n0.044\nLlama-3.1-70B-Instruct\n0.376\n0.470\n0.287\n0.294\n0.360\n0.307\n0.249\n0.286\n0.256\n0.143\n0.179\n0.151\nQwen3-8B\n0.160\n0.250\n0.171\n0.084\n0.147\n0.097\n0.043\n0.078\n0.050\n0.039\n0.072\n0.046\nOurs\nGRAIL (w.o. LLM backend)\n0.383\n0.410\n0.315\n0.260\n0.357\n0.283\n0.201\n0.262\n0.214\n0.295\n0.354\n0.308\nGRAIL (+Llama-3.1-8B-Instruct)\n0.598\n0.618\n0.522\n0.594\n0.764\n0.589\n0.508\n0.573\n0.523\n0.602\n0.665\n0.618\nGRAIL (+Qwen3-8B)\n0.587\n0.657\n0.520\n0.563\n0.662\n0.587\n0.506\n0.570\n0.521\n0.500\n0.562\n0.516\nMetrics. For multi-modal next-visit prediction, we report\nRecall@k (k∈{5, 10}) and nDCG@10, computed per visit\nand macro-averaged. For primary diagnosis prediction, we\nreport Top-1, Top-5, and MRR. To assess clinical validity\nunder coding noise, we additionally report hierarchy-aware\nmetrics: average shortest-path distance in the ICD hierarchy\nbetween predictions and ground truth (Tree Distance) and\nan Ancestor Match rate measuring shared higher-level ICD\nancestors.\n5. Results\n5.1. Task 1: Multi-Modal Next-Visit Prediction\nTable 1 summarizes next-visit prediction performance\nacross diagnoses, procedures, medications, and laboratory\ntests. Beyond raw performance gains, the results reveal\nseveral consistent behaviors that help clarify when and why\nstructured geometric retrieval is effective for longitudinal\nEHR modeling.\nRetrieval as a robustness mechanism under clinical noise.\nA first observation is that all retrieval-based approaches out-\nperform sequence-only baselines across modalities. This\nis particularly evident for procedures, medications, and\nlabs, where unordered code sets, sparsity, and institution-\nspecific coding practices introduce substantial noise. Re-\nstricting prediction to a candidate set grounded in historical\nco-occurrence and hierarchy appears to act as an implicit\nregularizer, reducing the burden on the model to discrimi-\nnate among thousands of rarely observed codes. The con-\nsistent improvement of Euclidean RAG over BEHRT and\nTransformer baselines suggests that this effect is largely\nindependent of geometry and instead stems from candidate\nrestriction itself.\nRole of hyperbolic geometry beyond candidate restric-\ntion.\nWhile retrieval alone is beneficial, hyperbolic em-\nbeddings provide additional and systematic gains under\nmatched or comparable structure. The improvements from\nEuclidean GRAIL to GRAIL without the LLM backend\nare modest for diagnoses but much larger for medications\nand labs. This asymmetry is informative: diagnoses are\noften already organized into relatively stable and shallow\nhierarchies, whereas medications and labs exhibit heavier\nlong-tail distributions and more pronounced coarse-to-fine\nabstraction. Hyperbolic geometry appears to better accom-\nmodate these properties by allocating more representational\ncapacity to fine-grained specializations while preserving\ntheir relationship to higher-level concepts. This suggests\nthat the primary benefit of hyperbolic space in this setting\nis not simply better distance preservation, but improved\nalignment between embedding geometry and the abstraction\nstructure imposed by medical coding systems.\nImportance of temporal cross-modal stitching.\nThe ab-\nlation without lagged PMI edges highlights the importance\nof explicitly encoding cross-modal temporal dependencies.\nPerformance drops are largest for medications and labs,\nwhich are rarely predictable from within-modality history\nalone. This aligns with clinical intuition: prescriptions are\ntypically motivated by diagnoses or procedures, and lab\ntests are often ordered to monitor treatment effects. Deter-\nministic hierarchies provide vertical structure within each\nmodality, but temporal stitching provides the horizontal\nlinks needed to model clinical progression across modali-\nties. Without these links, the Risk Horizon becomes less\ntrajectory-consistent, leading to weaker retrieval for down-\nstream modalities.\nLLMs as semantic refiners rather than primary fore-\ncasters.\nThe contrast between zero-shot LLM baselines\n7\n"}, {"page": 8, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\nand LLM reranking over GRAIL is particularly instructive.\nSmall LLMs perform poorly when asked to directly select\nfuture codes from the full vocabulary, even when provided\nwith compressed history. However, when the same mod-\nels are applied as rankers over a structured and trajectory-\nconsistent candidate set, they yield large improvements\nacross all modalities. This suggests that, in this setting,\nLLMs are most effective at resolving fine-grained seman-\ntic ambiguities among plausible candidates rather than at\ndiscovering those candidates in the first place. The similar\nperformance of different small LLM backends under the\nsame retrieval further supports this view: once the candidate\nset is well structured, model choice matters less than the\nquality of the geometric prior.\nImplications for multi-modal forecasting.\nTaken to-\ngether, the multi-modal results suggest a layered model-\ning strategy. Retrieval defines what is plausible, geometry\ndefines how plausibility is organized, and LLMs refine or-\ndering within that space. Improvements in Recall@k indi-\ncate better coverage of true future events, while gains in\nnDCG@k indicate that these events are ranked more con-\nsistently with clinical relevance. The fact that these gains\nare largest for sparse modalities suggests that GRAIL is\nparticularly well suited to settings where data sparsity and\nheterogeneity limit the effectiveness of purely sequential\nmodels.\nTable 2. Primary diagnosis prediction.\nModel\nTop-1\nTop-5\nMRR\nHier. Dist ↓\nAnc. M\nTransformer (Eucl.)\n0.126\n0.362\n0.214\n3.62\n0.581\nEuclidean RAG\n0.139\n0.389\n0.232\n3.34\n0.603\nHyperbolic (no stitch)\n0.146\n0.404\n0.241\n3.18\n0.616\nGRAIL (Ours)\n0.668\n0.748\n0.676\n1.98\n0.972\n5.2. Task 2: Primary Diagnosis Prediction\nTable 2 reports results for primary diagnosis prediction\nalong with hierarchy-aware metrics. In contrast to the multi-\nlabel setting, this task requires selecting a single dominant\nclinical state, making it a useful probe of how well Central\nEvents summarize patient history.\nStability of Central Events for single-label prediction.\nThe large improvement in Top-1 accuracy and MRR indi-\ncates that Central Events provide a stable and discriminative\nrepresentation of patient state for single-label forecasting.\nUnlike sequence encoders that must implicitly aggregate un-\nordered code sets over time, GRAIL explicitly compresses\neach visit into a geometric summary that preserves pre-\ndictable structure while filtering noise. This appears to be\nparticularly beneficial when only one outcome must be cho-\nsen, as small differences in ranking can have a large impact\non Top-1 accuracy.\nHierarchy-aware behavior under label ambiguity.\nThe\nhierarchy-aware metrics provide additional insight into\nmodel behavior when exact matches are not achieved. The\nreduction in hierarchical distance suggests that errors made\nby GRAIL tend to remain within clinically related subtrees\nof the ICD hierarchy. This is an important property in EHR\ndata, where fine-grained diagnosis codes may vary across\nclinicians or institutions, and near-miss predictions often re-\nmain clinically meaningful. The strong ancestor match score\nreported for GRAIL is consistent with this interpretation and\nsupports the claim that the model internalizes hierarchical\nstructure rather than merely memorizing frequent codes.\nOverall interpretation.\nAcross both tasks, the results\nindicate that explicitly modeling hierarchy, temporal asso-\nciation, and geometry yields benefits that go beyond incre-\nmental performance gains. The improvements are largest\nwhere clinical data are most sparse and heterogeneous, and\nwhere purely sequential modeling struggles. The combina-\ntion of structured retrieval and constrained LLM inference\nsuggests a practical pathway for integrating large language\nmodels into longitudinal clinical prediction without relying\non unconstrained generation.\n6. Conclusion\nWe introduced GRAIL, a framework for longitudinal EHR\nforecasting that integrates deterministic coding hierarchies,\ndata-driven temporal stitching, hyperbolic representation\nlearning, and structure-aware retrieval, with optional LLM\nreranking at inference time. By explicitly modeling hierar-\nchical abstraction and cross-modal temporal dependencies,\nGRAIL provides a compact and clinically grounded repre-\nsentation of patient trajectories. Experiments on MIMIC-\nIV demonstrate consistent improvements in multi-modal\nnext-visit prediction, more hierarchy-consistent forecasts,\nand stronger grounding by constraining LLM inference to\nclinically plausible candidates. Together, these results indi-\ncate that hyperbolic geometry offers a principled inductive\nbias for longitudinal clinical modeling, and that retrieval-\nbased integration enables a reliable and scalable pathway for\nincorporating LLM reasoning into high-stakes healthcare\nprediction tasks.\n8\n"}, {"page": 9, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\nImpact Statement\nThis work focuses on improving the reliability and in-\nterpretability of longitudinal modeling over electronic\nhealth records (EHRs), with the goal of supporting clin-\nical decision-making and medical research. By integrating\nexplicit medical hierarchies, data-driven temporal associ-\nations, and structure-aware retrieval with large language\nmodels (LLMs), the proposed framework aims to reduce\nclinically implausible predictions and ungrounded model\noutputs, which are critical risks in high-stakes healthcare set-\ntings. If translated responsibly, such methods could support\ndownstream applications such as clinical forecasting, cohort\nanalysis, and hypothesis generation, particularly in settings\nwhere patient trajectories are sparse and heterogeneous.\nAt the same time, this work is not intended for direct clinical\ndeployment. The models are trained and evaluated retro-\nspectively on observational EHR data and may reflect bi-\nases, missingness, or documentation practices present in the\nsource dataset. Predictions generated by the model should\ntherefore not be interpreted as medical advice, and any real-\nworld use would require prospective validation, regulatory\napproval, and oversight by qualified medical professionals.\nRegarding data accessibility and reproducibility, our ex-\nperiments are conducted on the MIMIC-IV dataset, which\ncontains sensitive patient information and is distributed un-\nder controlled access agreements. As a result, we do not\nrelease the dataset itself. However, MIMIC-IV is publicly\navailable to qualified researchers who complete the required\ndata use training and agreements. To support reproducibil-\nity, we commit to releasing all preprocessing code, model\nimplementations, and fixed experimental configurations. Re-\nsearchers with authorized access to the same dataset will be\nable to reproduce our results and evaluate extensions under\nidentical experimental conditions.\nMore broadly, this work highlights the importance of com-\nbining structured inductive biases with constrained use of\nLLMs in medical domains. We hope it encourages further\nresearch into geometry-aware, retrieval-based, and inter-\npretable approaches that prioritize reliability and grounding\nover unconstrained generation in healthcare applications.\nReferences\nAlaa, A. M. and van der Schaar, M. Attentive state-space\nmodeling of disease progression. Advances in neural\ninformation processing systems, 32, 2019.\nBeaulieu-Jones, B. K., Kohane, I. S., and Beam, A. L. Learn-\ning contextual hierarchical structure of medical concepts\nwith poincair´e embeddings to clarify phenotypes. In Pa-\ncific Symposium on Biocomputing. Pacific Symposium on\nBiocomputing, volume 24, pp. 8, 2019.\nBodenreider, O.\nThe unified medical language system\n(umls): integrating biomedical terminology.\nNucleic\nacids research, 32(suppl 1):D267–D270, 2004.\nCao, P., Chen, Y., Liu, K., Zhao, J., Liu, S., and Chong, W.\nHypercore: Hyperbolic and co-graph representation for\nautomatic icd coding. In Proceedings of the 58th annual\nmeeting of the association for computational linguistics,\npp. 3105–3114, 2020.\nChami, I., Ying, Z., R´e, C., and Leskovec, J. Hyperbolic\ngraph convolutional neural networks. Advances in neural\ninformation processing systems, 32, 2019.\nChe, Z., Purushotham, S., Cho, K., Sontag, D., and Liu, Y.\nRecurrent neural networks for multivariate time series\nwith missing values. Scientific reports, 8(1):6085, 2018.\nChen, J., Yin, C., Wang, Y., and Zhang, P. Predictive model-\ning with temporal graphical representation on electronic\nhealth records. In IJCAI: proceedings of the conference,\nvolume 2024, pp. 5763, 2024.\nChoi, E., Bahadori, M. T., Schuetz, A., Stewart, W. F., and\nSun, J. Doctor ai: Predicting clinical events via recurrent\nneural networks. In Machine learning for healthcare\nconference, pp. 301–318. PMLR, 2016a.\nChoi, E., Bahadori, M. T., Sun, J., Kulas, J., Schuetz, A., and\nStewart, W. Retain: An interpretable predictive model\nfor healthcare using reverse time attention mechanism.\nAdvances in neural information processing systems, 29,\n2016b.\nChoi, E., Bahadori, M. T., Song, L., Stewart, W. F., and\nSun, J. Gram: graph-based attention model for health-\ncare representation learning. In Proceedings of the 23rd\nACM SIGKDD international conference on knowledge\ndiscovery and data mining, pp. 787–795, 2017.\nGanea, O., B´ecigneul, G., and Hofmann, T. Hyperbolic neu-\nral networks. Advances in neural information processing\nsystems, 31, 2018.\nGuu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.\nRetrieval augmented language model pre-training. In\nInternational conference on machine learning, pp. 3929–\n3938. PMLR, 2020.\nHarutyunyan, H., Khachatrian, H., Kale, D. C., Ver Steeg,\nG., and Galstyan, A. Multitask learning and benchmark-\ning with clinical time series data. Scientific data, 6(1):96,\n2019.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E.,\nBang, Y. J., Madotto, A., and Fung, P. Survey of halluci-\nnation in natural language generation. ACM computing\nsurveys, 55(12):1–38, 2023.\n9\n"}, {"page": 10, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\nJohnson, A. E., Bulgarelli, L., Shen, L., Gayles, A., Sham-\nmout, A., Horng, S., Pollard, T. J., Hao, S., Moody, B.,\nGow, B., et al. Mimic-iv, a freely accessible electronic\nhealth record dataset. Scientific data, 10(1):1, 2023.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,\nGoyal, N., K¨uttler, H., Lewis, M., Yih, W.-t., Rockt¨aschel,\nT., et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks. Advances in neural information pro-\ncessing systems, 33:9459–9474, 2020.\nLi, Y., Rao, S., Solares, J. R. A., Hassaine, A., Ramakr-\nishnan, R., Canoy, D., Zhu, Y., Rahimi, K., and Salimi-\nKhorshidi, G. Behrt: transformer for electronic health\nrecords. Scientific reports, 10(1):7155, 2020.\nLipton, Z. C., Kale, D. C., Elkan, C., and Wetzel, R. Learn-\ning to diagnose with lstm recurrent neural networks. arXiv\npreprint arXiv:1511.03677, 2015.\nLiu, Q., Nickel, M., and Kiela, D. Hyperbolic graph neural\nnetworks. Advances in neural information processing\nsystems, 32, 2019.\nLu, C., Reddy, C. K., and Ning, Y. Self-supervised graph\nlearning with hyperbolic embedding for temporal health\nevent prediction. IEEE Transactions on Cybernetics, 53\n(4):2124–2136, 2021.\nLu, Q., De Silva, N., Kafle, S., Cao, J., Dou, D., Nguyen,\nT. H., Sen, P., Hailpern, B., Reinwald, B., and Li, Y.\nLearning electronic health records through hyperbolic\nembedding of medical ontologies. In Proceedings of\nthe 10th acm international conference on bioinformatics,\ncomputational biology and health informatics, pp. 338–\n346, 2019.\nMa, F., You, Q., Xiao, H., Chitta, R., Zhou, J., and Gao, J.\nKame: Knowledge-based attention model for diagnosis\nprediction in healthcare. In Proceedings of the 27th ACM\ninternational conference on information and knowledge\nmanagement, pp. 743–752, 2018.\nMao, C., Yao, L., and Luo, Y. Medgcn: Medication rec-\nommendation and lab test imputation via graph convolu-\ntional networks. Journal of Biomedical Informatics, 127:\n104000, 2022.\nNaseem, U., Thapa, S., Zhang, Q., Wang, S., Rashid, J., Hu,\nL., and Hussain, A. Graph learning with label attention\nand hyperbolic embedding for temporal event prediction\nin healthcare. Neurocomputing, 592:127736, 2024.\nNickel, M. and Kiela, D. Poincar´e embeddings for learning\nhierarchical representations. Advances in neural informa-\ntion processing systems, 30, 2017.\nNori, H., King, N., McKinney, S. M., Carignan, D., and\nHorvitz, E. Capabilities of gpt-4 on medical challenge\nproblems. arXiv preprint arXiv:2303.13375, 2023.\nPan, S., Luo, L., Wang, Y., Chen, C., Wang, J., and Wu, X.\nUnifying large language models and knowledge graphs:\nA roadmap. IEEE Transactions on Knowledge and Data\nEngineering, 36(7):3580–3599, 2024.\nPham, T., Tran, T., Phung, D., and Venkatesh, S. Deepcare:\nA deep dynamic memory model for predictive medicine.\nIn Pacific-Asia conference on knowledge discovery and\ndata mining, pp. 30–41. Springer, 2016.\nQu, Z. and F¨arber, M. Medieval: A unified medical bench-\nmark for patient-contextual and knowledge-grounded rea-\nsoning in llms. arXiv preprint arXiv:2512.20822, 2025.\nRajkomar, A., Oren, E., Chen, K., Dai, A. M., Hajaj, N.,\nHardt, M., Liu, P. J., Liu, X., Marcus, J., Sun, M., et al.\nScalable and accurate deep learning with electronic health\nrecords. NPJ digital medicine, 1(1):18, 2018.\nRasmy, L., Xiang, Y., Xie, Z., Tao, C., and Zhi, D. Med-\nbert: pretrained contextualized embeddings on large-scale\nstructured electronic health records for disease prediction.\nNPJ digital medicine, 4(1):86, 2021.\nSarkar, R. Low distortion delaunay embedding of trees in\nhyperbolic plane. In International symposium on graph\ndrawing, pp. 355–366. Springer, 2011.\nShang, J., Ma, T., Xiao, C., and Sun, J. Pre-training of graph\naugmented transformers for medication recommendation.\narXiv preprint arXiv:1906.00346, 2019.\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung,\nH. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S.,\net al. Large language models encode clinical knowledge.\nNature, 620(7972):172–180, 2023.\nSteinberg, E., Fries, J., Xu, Y., and Shah, N. Motor: A\ntime-to-event foundation model for structured medical\nrecords. arXiv preprint arXiv:2301.03150, 2023.\nWHO. International Statistical Classification of Diseases\nand related health problems: Alphabetical index, vol-\nume 3. World Health Organization, 2004.\nWHO. Anatomical therapeutic chemical (atc) classification,\n2024.\nWornow, M., Bedi, S., Hernandez, M. A. F., Steinberg, E.,\nFries, J. A., R´e, C., Koyejo, S., and Shah, N. H. Context\nclues: Evaluating long context models for clinical pre-\ndiction tasks on ehrs. arXiv preprint arXiv:2412.16178,\n2024.\n10\n"}, {"page": 11, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\nWu, J., Zhu, J., Qi, Y., Chen, J., Xu, M., Menolascina,\nF., Jin, Y., and Grau, V. Medical graph rag: Evidence-\nbased medical large language model via graph retrieval-\naugmented generation. In Proceedings of the 63rd Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 28443–28467, 2025.\nWu, Y., Chen, X., Yao, X., Yu, Y., and Chen, Z. Hyper-\nbolic graph convolutional neural network with contrastive\nlearning for automated icd coding. Computers in Biology\nand Medicine, 168:107797, 2024.\nXiong, G., Jin, Q., Lu, Z., and Zhang, A. Benchmarking\nretrieval-augmented generation for medicine. In Findings\nof the Association for Computational Linguistics ACL\n2024, pp. 6233–6251, 2024.\nXu, Y., Ying, H., Qian, S., Zhuang, F., Zhang, X., Wang,\nD., Wu, J., and Xiong, H. Time-aware context-gated\ngraph attention network for clinical risk prediction. IEEE\nTransactions on Knowledge and Data Engineering, 35\n(7):7557–7568, 2022.\nYang, Z., Mitra, A., Liu, W., Berlowitz, D., and Yu, H.\nTransformehr: transformer-based encoder-decoder gen-\nerative model to enhance prediction of disease outcomes\nusing electronic health records. Nature communications,\n14(1):7857, 2023.\nZeng, Z., Cheng, Q., Hu, X., Zhuang, Y., Liu, X., He,\nK., and Liu, Z. Kosel: Knowledge subgraph enhanced\nlarge language model for medical question answering.\nKnowledge-Based Systems, 309:112837, 2025.\nZhou, Y., Liu, X., Yan, C., Ning, C., Zhang, X., Li, B., Fu,\nX., Wang, S., Hu, G., Wang, Y., and Wu, J. Evaluat-\ning LLMs across multi-cognitive levels: From medical\nknowledge mastery to scenario-based problem solving.\nIn Forty-second International Conference on Machine\nLearning, 2025. URL https://openreview.net/\nforum?id=sgrJs7dbWC.\n11\n"}, {"page": 12, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\nA. Additional Formulas and Optimization Details\nA.1. Hyperbolic Geometry\nWe embed nodes in the d-dimensional Poincar´e ball of curvature −c,\nDd\nc = {x ∈Rd : ∥x∥< 1/√c}.\nThe hyperbolic distance between x, y ∈Dd\nc is\ndD(x, y) = 2\n√c arctanh\n\u0000√c ∥(−x) ⊕c y∥\n\u0001\n,\n(25)\nwhere ⊕c denotes M¨obius addition.\nWe use the logarithmic and exponential maps at the origin:\nlogc\n0(x) = 2\n√c arctanh(√c∥x∥) x\n∥x∥,\nexpc\n0(v) = tanh\n\u0012√c∥v∥\n2\n\u0013\nv\n√c∥v∥.\n(26)\nAfter each gradient update, embeddings are projected to ensure ∥zv∥< 1/√c.\nA.2. Typed Edge Reconstruction Objective\nFor a directed edge (u →v) of type r, the compatibility score is defined as\nsr(u, v) = −dD(zu, zv).\nWe optimize the negative-sampling objective\nLedge = −\nX\nr∈R\nX\n(u→v)∈Er\n\nlog σ(γr + sr(u, v)) +\nX\nv−∼Nr(v)\nlog σ(γr −sr(u, v−))\n\n,\n(27)\nwhere Nr(v) samples negatives from the same target modality and γr is a learned type-specific bias.\nA.3. Masked-Visit Reconstruction Loss\nGiven a masked visit Mt ⊂Ct and Central Event µt, we define the probability of recovering a masked code c as\np(c | µt) =\nexp(−dD(µt, zc)/τ)\nexp(−dD(µt, zc)/τ) + P\nc−∼N (c) exp(−dD(µt, zc−)/τ).\n(28)\nThe reconstruction loss is\nLmask = −\nX\nc∈Mt\nlog p(c | µt),\n(29)\nwhere negatives are sampled from the same modality as c.\nB. Dataset and Cohort Details\nB.1. Cohort Selection and Preprocessing\nWe conduct experiments on the MIMIC-IV database. We construct patient trajectories as time-ordered sequences of visits,\nwhere each visit is a multi-modal set containing diagnoses (ICD-9/10), procedures (ICD-9/10), medications (ATC), and\nlaboratory tests (LOINC).\nInclusion Criteria. We include patients meeting the following criteria:\n• Sequence Length: Minimum of Tmin = 2 visits to ensure sufficient history for longitudinal modeling.\n12\n"}, {"page": 13, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\n• Data Density: We filter rare codes by enforcing a minimum frequency of ≥25 occurrences across the training corpus\nper modality.\nVocabulary and Hierarchy. For each retained leaf code, we include its deterministic ancestors from the standard medical\nontologies (e.g., ICD hierarchy, ATC levels). The final vocabulary V is the union of all retained leaves and their ancestors.\nData Splits. We split the dataset by patient into training (80%), validation (10%), and test (10%) sets. This ensures that no\npatient’s future visits appear in the training set (patient-level leakage control). Crucially, all aggregate statistics used for\ngraph construction—specifically the lagged PMI values for temporal stitching—are computed strictly on the training split to\nprevent label leakage.\nB.2. Task Definitions\nTask A: Multi-modal Next-Visit Prediction. Given history S1:t, the model predicts the set of codes S(m)\nt+1 for each modality\nm ∈{dx, proc, med, lab}. This is formulated as a multi-label ranking problem where the model outputs a score for every\ncode in the vocabulary V(m).\nTask B: Primary Diagnosis Prediction. We predict the single primary diagnosis code for visit t + 1. In MIMIC-IV, explicit\n“primary” diagnosis labels can be ambiguous or inconsistently specified across structured tables. We therefore extract the\nprimary diagnosis directly from discharge summaries, where it is explicitly documented in the majority of cases, and use\nthis as the target label for evaluation.\nC. Baseline Implementations\nWe compare GRAIL against the following categories of baselines. All methods use matched embedding dimensions (d = 64)\nwhere applicable.\nC.1. Sequential Baselines\n• Transformer: A standard Transformer encoder that treats a visit as a “sentence” of flattened code tokens. We add a\nspecialized [SEP] token between visits and use the representation of the final [CLS] token for prediction.\n• BEHRT: A BERT-based model pretrained on large-scale EHR data using masked language modeling (MLM), then\nfine-tuned for next-visit prediction.\n• RETAIN: An interpretable RNN-based model that uses a two-level attention mechanism (visit-level and variable-level)\nto generate risk scores.\nC.2. Ontology and Graph Baselines\n• GRAM / KAME: Ontology-aware methods that use a DAG-based attention mechanism to regularize leaf embeddings\nusing their ancestors. We extend these methods to multi-modal data by learning separate DAGs for each modality and\nconcatenating the final visit representations.\n• GNN Baselines: A Graph Neural Network (GAT) trained on a static co-occurrence graph.\n• Euclidean GRAIL (Ablation): To isolate the benefit of hyperbolic geometry, we implement a version of GRAIL\nwhere:\n1. Nodes are embedded in Euclidean space Rd.\n2. Central Events are computed using standard Euclidean centroids (means).\n3. Retrieval uses Euclidean (L2) distance.\nThe graph topology and noise masking strategies remain identical to the main GRAIL model.\nC.3. LLM Baselines\n• Zero-shot Prompting: We feed the raw code history (converted to text descriptions) to the LLM and ask it to predict\nthe next likely events.\n13\n"}, {"page": 14, "text": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\n• RAG (Euclidean): Standard Retrieval-Augmented Generation where the retriever selects top-k candidates based on\nEuclidean similarity to the history embedding, without GRAIL’s structure-aware risk cones.\nD. Evaluation Metrics\nD.1. Ranking Metrics\nFor multi-label prediction, we report:\n• Recall@k: The proportion of true next-visit codes present in the top-k predicted candidates.\n• nDCG@k: Normalized Discounted Cumulative Gain, which rewards correct predictions ranked higher in the list.\n• Micro-F1: Calculated by thresholding probabilities (threshold tuned on validation set).\nD.2. Hierarchy-Aware Metrics\nTo assess clinical semantic similarity rather than just exact string matching:\n• Tree Distance: The shortest path distance in the ontology tree (e.g., ICD-10) between the predicted code and the\nground truth. A lower distance indicates a ”clinically close” error (e.g., predicting Type 2 Diabetes w/o complications\ninstead of Type 2 Diabetes w/ renal complications).\n• Ancestor Match@L: The accuracy of predicting the correct ancestor at level L of the hierarchy (e.g., matching the\ncorrect ICD Chapter).\nD.3. Grounding Metrics\nFor LLM-generated predictions, we measure:\n• Grounded Mention Rate: The fraction of generated codes that are present in either the patient’s explicit history or the\nretrieval set (Risk Horizon).\n• Unsupported Mention Rate: The fraction of generated codes that appear in neither, indicating potential hallucination.\nE. Implementation Details\nAll models are implemented in PyTorch. Optimization is performed using Riemannian Adam (for hyperbolic parameters)\nand standard Adam (for Euclidean parameters).\nHyperparameters.\n• Embedding Dimension: d = 64 for all methods.\n• Curvature (c): We treat curvature as a trainable parameter, initialized at c = 1.0.\n• Negative Sampling: We use nneg = 50 negatives per positive edge for graph training.\n• Masking: For visit denoising, we use a mask ratio ρ randomly sampled from [0.15, 0.30] during training.\n• Stitching: We retain cross-modal edges with PMI > 0 and support count κ ≥50.\nReproducibility. All dataset preprocessing scripts and train/test splits are released to ensure exact replication of the graph\ntopology and patient cohorts.\n14\n"}]}