{"doc_id": "arxiv:2602.14374", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.14374.pdf", "meta": {"doc_id": "arxiv:2602.14374", "source": "arxiv", "arxiv_id": "2602.14374", "title": "Differentially Private Retrieval-Augmented Generation", "authors": ["Tingting Tang", "James Flemings", "Yongqin Wang", "Murali Annavaram"], "published": "2026-02-16T00:52:57Z", "updated": "2026-02-16T00:52:57Z", "summary": "Retrieval-augmented generation (RAG) is a widely used framework for reducing hallucinations in large language models (LLMs) on domain-specific tasks by retrieving relevant documents from a database to support accurate responses. However, when the database contains sensitive corpora, such as medical records or legal documents, RAG poses serious privacy risks by potentially exposing private information through its outputs. Prior work has demonstrated that one can practically craft adversarial prompts that force an LLM to regurgitate the augmented contexts. A promising direction is to integrate differential privacy (DP), a privacy notion that offers strong formal guarantees, into RAG systems. However, naively applying DP mechanisms into existing systems often leads to significant utility degradation. Particularly for RAG systems, DP can reduce the usefulness of the augmented contexts leading to increase risk of hallucination from the LLMs. Motivated by these challenges, we present DP-KSA, a novel privacy-preserving RAG algorithm that integrates DP using the propose-test-release paradigm. DP-KSA follows from a key observation that most question-answering (QA) queries can be sufficiently answered with a few keywords. Hence, DP-KSA first obtains an ensemble of relevant contexts, each of which will be used to generate a response from an LLM. We utilize these responses to obtain the most frequent keywords in a differentially private manner. Lastly, the keywords are augmented into the prompt for the final output. This approach effectively compresses the semantic space while preserving both utility and privacy. We formally show that DP-KSA provides formal DP guarantees on the generated output with respect to the RAG database. We evaluate DP-KSA on two QA benchmarks using three instruction-tuned LLMs, and our empirical results demonstrate that DP-KSA achieves a strong privacy-utility tradeoff.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.14374v1", "url_pdf": "https://arxiv.org/pdf/2602.14374.pdf", "meta_path": "data/raw/arxiv/meta/2602.14374.json", "sha256": "251912525dad671271a2a4ee57ac30662b0dd6302ea2cdc4f2bd09187be545d4", "status": "ok", "fetched_at": "2026-02-18T02:19:13.517387+00:00"}, "pages": [{"page": 1, "text": "Differentially Private Retrieval-Augmented Generation\nTingting Tang\nUniversity of Southern California\nLos Angeles, California, USA\ntangting@usc.edu\nJames Flemings\nUniversity of Southern California\nLos Angeles, California, USA\njamesf17@usc.edu\nYongqin Wang\nUniversity of Southern California\nLos Angeles, California, USA\nyongqin@usc.edu\nMurali Annavaram\nUniversity of Southern California\nLos Angeles, California, USA\nannavara@usc.edu\nAbstract\nRetrieval-augmented generation (RAG) is a widely used framework\nfor reducing hallucinations in large language models (LLMs) on\ndomain-specific tasks by retrieving relevant documents from a data-\nbase to support accurate responses. However, when the database\ncontains sensitive corpora, such as medical records or legal doc-\numents, RAG poses serious privacy risks by potentially exposing\nprivate information through its outputs. Prior work has demon-\nstrated that one can practically craft adversarial prompts that force\nan LLM to regurgitate the augmented contexts. A promising di-\nrection is to integrate differential privacy (DP), a privacy notion\nthat offers strong formal guarantees, into RAG systems. However,\nnaively applying DP mechanisms into existing systems often leads\nto significant utility degradation. Particularly for RAG systems,\nDP can reduce the usefulness of the augmented contexts lead-\ning to increase risk of hallucination from the LLMs. Motivated\nby these challenges, we present DP-KSA, a novel privacy-preserving\nRAG algorithm that integrates DP using the propose-test-release\n(PTR) paradigm. DP-KSA follows from a key observation that most\nquestion-answering (QA) queries can be sufficiently answered with\na few keywords. Hence, DP-KSA first obtains an ensemble of rele-\nvant contexts, each of which will be used to generate a response\nfrom an LLM. We utilize these responses to obtain the most frequent\nkeywords in a differentially private manner. Lastly, the keywords\nare augmented into the prompt for the final output. This approach\neffectively compresses the semantic space while preserving both\nutility and privacy. We formally show that DP-KSA provides formal\nDP guarantees on the generated output with respect to the RAG\ndatabase. We evaluate DP-KSA on two QA benchmarks using three\ninstruction-tuned LLMs, and our empirical results demonstrate that\nDP-KSA achieves a strong privacy-utility tradeoff.\nKeywords\nDifferential Privacy, Retrieval-Augmented Generation, Large Lan-\nguage Models, Data Privacy\nThis work is licensed under the Creative Commons Attribu-\ntion 4.0 International License. To view a copy of this license\nvisit https://creativecommons.org/licenses/by/4.0/ or send a\nletter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\nProceedings on Privacy Enhancing Technologies YYYY(X), 1â€“15\nÂ© YYYY Copyright held by the owner/author(s).\nhttps://doi.org/XXXXXXX.XXXXXXX\nPrivate External \nDatabase ğ·\nGenerator \nModel ğ¹\nRetrieved \nDocuments\nQuery ğ’™\nResponse ğ’š\nGenerator \nModel ğ¹\nPre-training \nData\nRAG\nPre-training\nAdversarial\nbound\nRetriever \nModel ğ‘…\nFigure 1: Overview of the DP RAG problem setting. The ad-\nversarial bound illustrates what capabilities the adversary\nhas. In this case, the adversary can only query the RAG sys-\ntem and access the answer. The generator model has been\npre-trained on publicly available data, which the adversary\nhas access to. However, we are not concerned about preserv-\ning the privacy of the pre-training data.\n1\nIntroduction\nLarge language models (LLMs) encode copious factual knowledge\nin their parameters through pre-training on internet-scale data [6].\nHence, LLMs leverage its knowledge when prompted to accurately\nanswer queries [28, 30]. However, the knowledge that the LLM was\npre-trained on (1) may not be precisely accessed and utilized and\n(2) can eventually become outdated. This can lead to discrepancies\nbetween generated content by the LLM and verifiable real-world\nfacts, known as factuality hallucinations [16].\nA widely-adopted approach to mitigate hallucinations is to up-\ndate the LLMsâ€™ knowledge base by augmenting prompts with ex-\nternal knowledge retrieved from a database, known as retrieval-\naugmented generation (RAG) [2, 22]. This involves retrieving the\ntop documents from an external database that are semantically rele-\nvant (similar) to a user query. However, a major concern is that the\nexternal database can contain highly-sensitive information, such as\nPersonable Identifiable Information (PII). For example, healthcare\nproviders can leverage internal medical records to offer accurate\ndiagnoses and tailored care recommendations, while law firms may\nrely on their legal case repositories to support clients in conducting\nlegal research and preparing documentation. Several have found\n1\narXiv:2602.14374v1  [cs.CR]  16 Feb 2026\n"}, {"page": 2, "text": "Proceedings on Privacy Enhancing Technologies YYYY(X)\nTang et al.\nthat RAG on a sensitive corpus can leak private information about\nindividual documents in the corpus [17, 29, 36]. An attacker can\ndesign a specific prompt to a RAG system and use the output to\nreveal the information retrieved from the sensitive database.\nIn this work, we focus on preserving the privacy of the external\ndataset with the following problem setup shown in Figure 1. A\nRAG system receives a query x. Our RAG system contains a private\nexternal dataset ğ·, a retriever model ğ‘…that will retrieve the top\ndocuments relevant to the query x, and a generator model ğ¹that\nwill use the top retrieved documents and the query to generate\na response y. Our setup assumes the adversary only has query\naccess to the RAG system to obtain responses. Hence, because the\nresponse can contain information about the retrieved documents,\nthe adversary can adversarially craft prompts such that the response\nreveals information about the external database. Therefore, our goal\nis to design a privacy-preserving RAG system to protect private,\nsensitive information against such attacks.\nOne promising privacy notion is Differential Privacy (DP) [11],\nwhich provides a mathematical guarantee that each individual in a\ndatabase has limited affect on the outcome of a randomized algo-\nrithm. In the context of RAG, each document in the RAG database\nhas limited influence on the output of the LLM. Although DP is the\nstandard privacy safeguard for applications utilizing information-\nsensitive data, it tends to result in substantial utility degradation.\nTo highlight the challenges that privately generating text presents,\nnote that the response from the generator model ğ¹iteratively sam-\nples the next token ğ‘¦ğ‘¡until a stop criteria is met, such as maximum\ntoken length reached. Hence, the range of possible values that ğ‘¦ğ‘¡\ncan take is equal to the vocabulary of the generatorâ€™s tokenizer,\nwhich can be upwards of 50, 000. And if the maximum token length\nis 10, then the output space of possible responses is at most 50, 00010.\nHence, naively applying standard DP mechanisms, such as addi-\ntive Gaussian noise, at every token generation could destroy any\nmeaningful utility from the generated tokens.\nThus, the goal our work is the following:\nHow can we integrate differential privacy into practical\nRAG systems while preserving their utility?\nTo address the above question, we propose DP-KSA, a privacy-\npreserving algorithm based on the propose-test-release (PTR) [12]\nand subsample-and-aggregate [27] paradigm for private text gen-\neration illustrated in Figure 2. DP-KSA derives from a key observa-\ntion that most queries from question-answering datasets can be\nanswered sufficiently with just a few keywords. Hence, we con-\nvert the output space of responses for question-answering tasks\ninto a keyword subspace, then perform differentially privacy to\nextract the keywords. This keyword subspace mostly preserves\nthe relative semantic representation of the original response space,\ni.e. it maintains the utility of the LLM while operating in a low-\ndimensional approximation of the entire sentence space. To achieve\nthis, a retriever model will first retrieve the top-ğ‘documents that\nare most relevant to the query. Next, we partition the documents\ninto prompts, each prompt containing a retrieved document with\nthe corresponding query, and feed these prompts into the generator\nmodel to obtain an ensemble of responses. Then DP-KSA transforms\nthe generated model responses into keywords that preserve relative\nsemantic meaning and privately extracts the frequently occurring\nkeywords in the responses via the propose-test-release paradigm.\nFinally, the privately obtained keywords are augmented to a prompt\ncontaining just the query, which is then fed to the LLM to generate\nthe final output.\nWe summarize the contributions of our work as follows:\n(1) We introduce DP-KSA, a simple RAG framework that pre-\nserves the privacy of the external database by generating a\nresponse for each of the retrieved documents from the re-\ntriever model, then extracting a small set of keywords from\nthe ensemble of responses.\n(2) We formally show that the final outputs generated by DP-KSA\ncan achieve differential privacy, a strong privacy notion that\ngives a probable guarantee of the privacy leakage of the\nexternal database.\n(3) We experimentally demonstrate that DP-KSA can achieve\nstrong privacy guarantees while preserving utility. In partic-\nular, we experimentally evaluated DP-KSA on two standard\nbenchmarking question-answering datasets with modern\ninstruction-tuned LLMs, Qwen 2.5 [34] and Llama 3 [13].\n(4) We provide comprehensive ablation studies for important\nhyperparameters of DP-KSA to demonstrate the robustness\nof DP-KSA as well as provide insights into the inner workings\nof DP-KSA.\n2\nPreliminary\n2.1\nRetrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) is a hybrid architecture\nthat augments a large language model (LLM) with an external\nretrieval mechanism to improve its performance on knowledge-\nintensive tasks. Given a user query x, a retriever model ğ‘…is first\nused to fetch the top-ğ‘most relevant documents from a private\ncorpus ğ·. Formally, the retriever returns a set of documents\nğ·x\n1, . . . , ğ·x\nğ‘â†ğ‘…(x, ğ·; ğ‘)\n(1)\nwhere each ğ·x\nğ‘–âˆˆğ·is deemed semantically similar to the query x.\nSemantic similarity is typically assessed by computing the distance\nbetween the vector representations of a query and a document.\nCommonly used distance functions include the dot product and\ncosine similarity. Each document ğ·x\nğ‘–is then paired with the query\nand passed into a generator model ğ¹, which synthesizes a response:\nğ‘¦ğ‘–â†ğ¹(ğ·x\nğ‘–, x)\n(2)\nThe outputsğ‘¦ğ‘–are natural language responses that reflect the LLMâ€™s\nunderstanding conditioned on both the query and the retrieved\ndocument.\nRAG offers several advantages over standalone language mod-\neling. By retrieving top-ğ‘relevant documents ğ·x\n1, . . . , ğ·x\nğ‘from a\ncorpus ğ·using a retriever ğ‘…, and conditioning generation on these\ndocuments, RAG enables dynamic grounding without retraining\nthe generator ğ¹. This allows models to incorporate up-to-date or\ndomain-specific information at inference time, which is especially\nvaluable when ğ·is frequently updated or too large to encode di-\nrectly into model parameters. A key benefit of RAG is its ability to\nreduce hallucinations. Since outputs ğ‘¦ğ‘–= ğ¹(ğ·x\nğ‘–, x) are generated\nwith direct reference to retrieved content, responses are more likely\nto be factually accurate. RAG is also modular as ğ‘…and ğ¹can be\n2\n"}, {"page": 3, "text": "Differentially Private RAG\nProceedings on Privacy Enhancing Technologies YYYY(X)\nFigure 2: The proposed DP-KSA framework consists of three steps. First, it retrieves the top-ğ‘documents most relevant to the\nquery x from the private database ğ·. Each retrieved document ğ·x\nğ‘–is paired with query x and passed to the generator model ğ¹to\nproduce responses. Next, DP-KSA applies a differentially private mechanism to extract the most frequent keywords from the\nensemble of responses. Finally, the selected keywords are combined with query x and fed back into the generator ğ¹to produce\nthe final output y.\ntrained or fine-tuned independently, enabling flexible adaptation\nacross domains.\n2.2\nDifferential Privacy\nDifferential privacy (DP) is the gold standard for reasoning about\nthe privacy of machine learning algorithms.\nDefinition 2.1 ((ğœ–,ğ›¿)-DP [11]). For ğœ–â‰¥0,ğ›¿âˆˆ[0, 1], a randomized\nalgorithm ğ‘€is (ğœ–,ğ›¿)-differentially private if for any pair of adjacent\ndatasets ğ·and ğ·â€² that differ in only one data point, it holds that\nğ‘ƒğ‘Ÿ[ğ‘€(ğ·) âˆˆğ¸] â‰¤ğ‘’ğœ–Â· ğ‘ƒğ‘Ÿ[ğ‘€(ğ·â€²) âˆˆğ¸] + ğ›¿\nThe above definition indicates that if two datasets are similar, a\nDP algorithm should produce similar output ğ¸with a high proba-\nbility so that attackers cannot infer the difference between them.\nIn our case, ğ‘€functions as a RAG algorithm, producing answers to\nqueries by utilizing private retrieved document as context. If this\nRAG algorithm adheres to differential privacy, it should generate\nsimilar outputs even when the retrieved documents vary. Conse-\nquently, this prohibits the generation of private information, such\nas replicating the retrieved context.\n2.2.1\nPrivate Generation via Sample-and-Aggregate. There has been\na body of work on generating token sequences from an LLM with\nDP, all of which rely on the idea of the sample-and-aggregate frame-\nwork in DP [27]. In this framework, the sensitive dataset is parti-\ntioned into pairwise disjoint subsets. Then, the model will perform\ninference on each subset to generate a response. Lastly, the re-\nsponses are privately aggregated together using a DP mechanism\n[8, 31, 33], such as adding Gaussian noise to the aggregation. The\nreason for using the sample-and-aggregate framework is that it\nhelps with calculating the global sensitivity, which is an important\nproperty needed for DP mechanisms. If we have two neighboring\ndatasets ğ·and ğ·â€² that differ by one document ğ‘‘ğ‘–, then at most one\nsubset will differ since the document can only be contained in at\nmost one subset (due to the pairwise disjoint property of sample-\nand-aggregate). Hence, changing one document changes at most\none response, which we can then use to argue the global sensitivity.\n2.2.2\nPrivate top-ğ‘˜selection. The private top-ğ‘˜selection problem\nis one of the most fundamental problems in privacy-preserving\ndata analysis. The problem is given a set of candidates with corre-\nsponding counts, return the top-ğ‘˜candidates based on the counts\nin a privacy-preserving way [25]. In this work, we adopt an adap-\ntive top-ğ‘˜selection algorithm from Zhu and Wang [39], which is\nbased on the widely-known propose-test-release (PTR) framework\n[12]. The idea is that as long as the count difference between the\nğ‘˜-th and the (ğ‘˜+ 1)-th highest candidates is larger than 2, we can\nnon-privately release the top-ğ‘˜candidates.\n3\nProblem Settings\nIn this section, we describe our problem settings and the threat\nmodel.\nProblem Settings. Following the problem setting in Figure\n1, suppose a RAG system receives an input query x and wants to\ngenerate a response using a generator model ğ¹. Typically ğ¹is a\ndecoder-only model such as LLaMA or Qwen. Also suppose the\n3\n"}, {"page": 4, "text": "Proceedings on Privacy Enhancing Technologies YYYY(X)\nTang et al.\nFigure 3: Histogram of token lengths of ground truth answers\nin NQ dataset.\nFigure 4: Histogram of token lengths of ground truth answers\nin TQA dataset.\nsystem has access to private external database ğ·to assist with\nanswering the query. The goal is to generate the response y such\nthat it is (ğœ–,ğ›¿)-DP (definition 2.1) with respect to a private RAG\ndatabase ğ·.\nThreat Model. Additionally, our work assumes a realistic setup\nwhere an adversary can only query the RAG system with any\nprompt x. The adversary has access only to (1) the response y\nand (2) the generator model ğ¹. Consequently, the adversary does\nnot have access to the external database ğ·. We can assume the\nadversary has access to the generator model, as this is usually\npublicly available to download from the internet. That is, the pre-\ntraining (and fine-tuning) data used to pre-train (and fine-tune)\nthe generator LLM model is considered public and we are only\nconcerned about preserving the privacy of the RAG database ğ·.\n4\nMethodology\nIn this section, we introduce DP-KSA, a DP RAG framework. We\nbegin by discussing the motivation behind the design of DP-KSA\nin Section 4.1. Then we will go through DP-KSA in more detail in\nSection 4.2.\n4.1\nMotivation\nThe challenge of generating differentially private text lies in the\nhigh dimensionality of the output space. The generator model\nğ¹typically generates text autoregressively, i.e. it iteratively sam-\nples the next token ğ‘¦ğ‘¡+1 from ğ¹by using the previously sampled\ntokens ğ‘¦1, ...,ğ‘¦ğ‘¡plus some provided context ğ‘to obtain ğ‘¦ğ‘¡+1 âˆ¼\nğ¹(ğ‘¦ğ‘¡+1|ğ‘,ğ‘¦1, ...,ğ‘¦ğ‘¡). Then we feed ğ‘¦ğ‘¡+1 back to the model to sam-\nple the next token until the desired stop criteria is met (either max\ntoken length reached or eos token sampled). However, the number\nof possible tokens that ğ‘¦ğ‘¡+1 can realize is equal to the vocabulary\nspace of the generator modelâ€™s tokenizer, which could be upwards\nto 50, 000. If the max generation length of the response y is 10, then\nthe output space of possible responses is 50, 00010. Hence, naively\napplying DP noise each time we are generating the next token\nresults in a large magnitude of noise, which can have a deleterious\neffect on the generated tokens and quickly destroy the utility.\nTo overcome the large dimensionality of private text generation,\nthe design of DP-KSA derives from a key observation that most\nquestion-answering datasets can be answered sufficiently with just\na few keywords. To illustrate this, consider the following question-\nanswering example from the Natural Question (NQ) dataset [21], a\nwidely-used RAG benchmarking dataset:\nQuestion: who lives in the imperial palace in tokyo?\nGround Truth Answer: the Imperial Family\nFrom this example, we see that the ground truth only contains\nthree words (i.e., three tokens). Hence, extracting the keywords\n\"Imperial\" and \"Family\" would completely preserve the semantic\nmeaning of the ground truth answer. To demonstrate that this\nobservation holds more generally, Figure 3 and Figure 4 show the\nhistograms of token lengths of ground truth answers in NQ dataset\nand Trivia Question-Answering (TQA) dataset [18], respectively.\nAs we can see, the histograms are rightly skewed where most of\nthe ground-truth answers contain only one to four tokens. The\nmain takeaway here is that we can convert the task of accurately\nanswering these questions into obtaining only a small set of correct\ntokens. Such a conversion effectively converts the QA task into\na keyword space which can be considered as a low-dimensional\napproximation of the entire sequence space of correct answers.\nConsequently, operating in this lower dimensional space will help\nus preserve the utility as we integrate differential privacy into this\nspace.\n4.2\nDP-KSA\nAs shown in Figure 2, at a high level, our algorithm proceeds in three\nsteps: (1) retrieval and partitioning, (2) DP keyword extraction, and\n(3) zero-shot with DP information inference. Algorithm 1 succinctly\ndescribes DP-KSA and we go through the details below.\nRetrieval and partitioning. First, we use a retriever model ğ‘…\nto obtain the top ğ‘documents from the private database ğ·that are\nmost relevant to the query x (line 1). Specifically, each document in\nthe database is encoded into a dense vector representation by the\nretriever ğ‘…. At query time, the retriever encodes the input query\ninto its own dense vector and ranks documents based on a similarity\nmeasure, typically the dot product or cosine similarity, between the\nquery and document vectors. This process identifies the documents\nmost semantically similar to the query.\nThen, for each retrieved document ğ·x\nğ‘–, we partition them into\nchunks with each chunk containing a retrieved document and the\ninput query. The document is augmented with the user query to\n4\n"}, {"page": 5, "text": "Differentially Private RAG\nProceedings on Privacy Enhancing Technologies YYYY(X)\nAlgorithm 1 DP-KSA\nRequire: Generator model ğ¹, retriever model ğ‘…, private database\nğ·, number of top documents ğ‘, user query x, max number of\ngenerated tokens ğ‘‡max\nEnsure: Generated response y\n1: ğ·x\n1, ..., ğ·x\nğ‘â†ğ‘…(x, ğ·; ğ‘) Retrieve ğ‘most relevant documents\nfor query x\n2: for 1 â‰¤ğ‘–â‰¤ğ‘do\n3:\nyğ‘–â†ğ¹(ğ·x\nğ‘–, x)\n4: end for\n5: Form histogram H by counting the number of responses yğ‘–that\ncontain each token.\n6: Ë†ğ‘˜= FindBestK(H) select optimal number of keywords\n7: ğ‘¤1, ...,ğ‘¤Ë†ğ‘˜â†TopKWithPTR( Ë†ğ‘˜, H) obtain keywords\n8: y â†ğ¹(x, {ğ‘¤ğ‘–} Ë†ğ‘˜\nğ‘–=1) zero-shot with keywords inference\n9: Return y\nAlgorithm 2 TopKWithPTR\nRequire: ğ‘˜â€“ the number of top counted tokens to release; H â€“\nhistogram for the counts of each token; ğ›¿â€“ failure probability\n1: Set ğ‘‘ğ‘˜:= H(ğ‘˜) âˆ’H(ğ‘˜+1).\n2: Set bğ‘‘ğ‘˜:= max(2,ğ‘‘ğ‘˜) + N (0, 4ğœ2) âˆ’Î¦(1 âˆ’ğ›¿; 0, 2ğœ).\n3: If bğ‘‘ğ‘˜> 2, Return the exact top-ğ‘˜tokens.\n4: Else Terminate with no keywords.\nAlgorithm 3 FindBestK\nRequire: H â€“ histogram for the counts of each token\n1: Compute histogram gap ğ‘‘ğ‘˜:= H(ğ‘˜) âˆ’H(ğ‘˜+1) for each ğ‘˜=\n1 . . . ğ‘âˆ’1.\n2: Return argmaxğ‘˜{ğ‘‘ğ‘˜+ ğ‘Ÿ(ğ‘˜) + Gumbel(2/ğœ–)}\nproduce a prompt, which is fed to the generator model ğ¹to generate\na response (line 3). Afterwards, we will obtain ğ‘responses.\nDP keyword extraction. Next, we extract keywords from the\nensemble of responses yğ‘–âˆ€ğ‘–to be used for the final prompt. The\nintuition behind this approach is that the answers for QA datasets\ntypically contain a few \"correct\" tokens. Hence, if the retrieved\ndocuments are relevant to the query, it is likely that the correct\ntokens are contained in the ensemble of responses generated based\non different disjoint retrieved documents. This is inspired from\nprior workâ€™s use of PTR for DP text generation [33].\nThe key idea is that we convert keyword extraction into a private\ntop-ğ‘˜selection problem, then use existing solutions in this space.\nTo this end, we form a histogram H by counting the frequency\nof each word token among the responses based on the individual\nretrieved documents. Then we use the histogram to obtain and\nrelease the top-ğ‘˜tokens with the highest counts in a DP-manner.\nSpecifically, first we adaptively estimate the optimal Ë†ğ‘˜to use (line\n6), then extract the top-Ë†ğ‘˜keywords using TopKWithPTR (line 7).\nOne subtle limitation is that applying a private selection algo-\nrithm, such as the exponential mechanism (EM) [25], for every\nkeyword will require applying the EM ğ‘˜times, which will decay\nthe privacy loss ğœ–. Hence, TopKWithPTR only needs to be applied\nonce to obtain the top-Ë†ğ‘˜by testing if H(ğ‘˜) âˆ’H(ğ‘˜+1) > 2. If the test\nholds, then the top-ğ‘˜indices are exactly the same for all the neigh-\nboring datasets, so we can release the exact top-ğ‘˜indices without\nany privatization. However, the test H(ğ‘˜) âˆ’H(ğ‘˜+1) > 2 must be\nperformed in a differentially private way, which is done with PTR\n(Algorithm 2). If the test fails, then no tokens can be release and\nhence the final prompt will resort to zero-shot.\nMoreover, the utility of TopKWithPTR is highest whenğ‘˜is chosen\nto maximize H(ğ‘˜)âˆ’H(ğ‘˜+1). Hence, Ë†ğ‘˜is estimated in a data-dependent\nway by releasing argmaxğ‘˜H(ğ‘˜) âˆ’H(ğ‘˜+1) using EM (Algorithm 3).\nHere, ğ‘Ÿ(ğ‘˜) is a regularizer independent of the dataset, e.g., we can\nset ğ‘Ÿ(ğ‘˜) = âˆ’âˆfor any ğ‘˜> 30 and ğ‘˜< 15, if we donâ€™t want to\nreturn more than 30 or less than 15 tokens.\nZero-shot + DP info inference. Lastly, we use the DP released\ntop-Ë†ğ‘˜tokens along with the user query x to generate the final\nresponse from the generator model y â†ğ¹(x, {ğ‘¤ğ‘–} Ë†ğ‘˜\nğ‘–=1) (line 9).\nNote that the final response does not explicitly use the retrieved\ndocuments for the final response, only the extracted keywords. If\nthe test in the TopKWithPTR fails, then the final response will not\ncontain any information from the external database.\n5\nPrivacy Analysis\nWe now provide a formal guarantee of DP-KSA, in particular that it\nachieves (ğœ–,ğ›¿)-DP. We provide a proof sketch below and defer the\nexact details in Appendix A.\nTheorem 5.1. Let x be a query received. Suppose we generate a\nresponse to x, denoted as y, using DP-KSA (Algorithm 1) with ğ¹as\nthe generator model, ğ‘…as the retrieval model, and ğ·as the private\ndatabase. Then DP-KSA satisfies (ğœ–,ğ›¿)-DP with respect to ğ·.\nProof sketch. The high-level idea of the privacy analysis is that the\nfinal response y is a function of the extracted keywords ğ‘¤1, ...,ğ‘¤Ë†ğ‘˜.\nThe keywords were obtained from the ensemble of responses, which\ndepend on the retrieved documents from the private dataset, and\nan estimate for the number of keywords Ë†ğ‘˜, which also depends\non the responses. Hence, it suffices to show that (1) FindBestK re-\nturn Ë†ğ‘˜that is differentially private with respect to ğ·, then show (2)\nTopKWithPTR is differentially private with respect to ğ·. In regards\nto (1), we use the exponential mechanism to achieve differential\nprivacy on Ë†ğ‘˜. And (2) achieves differential privacy using standard\nanalysis of PTR framework. For both (1) and (2), the global sensitiv-\nity of the utility function ğ‘‘ğ‘˜is 2 because that for two neighboring\nexternal datasets ğ·and ğ·â€² they differ by one document. Subse-\nquently, they will differ by one retrieved document, say the ğ‘–-th\nğ·x\nğ‘–â‰ ğ·â€²x\nğ‘–. Hence the sensitivity of the utility function ğ‘‘ğ‘˜is 2. Once\nwe prove these two claims, then we can invoke the post-processing\ntheorem of DPâ€“ stating that a DP quantity does not leak additional\nprivacy about the datasetâ€“ to argue that the final response is DP.\nRemark 5.2. The formal proof from Appendix A is relatively straight-\nforward, as it relies on mostly well-established properties and theo-\nrems. However, we utilize Renyi Differential Privacy (RDP) [26] to\nperform our privacy analysis. Hence, we formally introduce and\ndefine these properties in terms of RDP in the Appendix due to\nspatial constraints.\n5\n"}, {"page": 6, "text": "Proceedings on Privacy Enhancing Technologies YYYY(X)\nTang et al.\n6\nExperiments\n6.1\nExperimental Setup\nDataset. To evaluate the effectiveness of our proposed method, we\nconduct experiments on two widely used benchmark datasets in the\nRAG literature, Natural Questions (NQ) [21] and Trivia Question\nAnswering (TQA) [18]. Both datasets are designed for open-domain\nquestion answering and consist of a diverse set of real-world ques-\ntions, each associated with multiple reference answers. Following\nstandard RAG evaluations [5, 15, 22], we use the Wikipedia corpus\nas our external knowledge source for retrieval. Due to practical\nconstraints in computational resources, we follow prior work [20]\nand use a subset of 100 questions from each dataset. Note that we\nfilter out questions with empty ground-truth references in both\ndatasets in order to correctly compute the evaluation metrics.\nModel Architectures. Our system follows a standard retrieval-\naugmented generation (RAG) pipeline, composed of a retriever and\na generator. For the retriever component, we adopt the Dense Pas-\nsage Retriever (DPR) [19], a widely used dual-encoder architecture\nbuilt on top of BERT [6]. DPR encodes both the input question\nand the candidate passages into dense vector embeddings and re-\ntrieves the most relevant documents by measuring similarity in\nthe embedding space. In our experiment, we use the inner prod-\nuct as the similarity metric. In addition, given the large size of\nthe Wikipedia corpus (21 million passages), we use FAISS [7] to\naccelerate retrieval. FAISS leverages approximate nearest neigh-\nbor algorithms for efficient similarity search in high-dimensional\nembedding spaces.\nFor the generator, we compare several state-of-the-art large\nlanguage models that have been instruction-tuned to follow user\nprompts. Specifically, we evaluate Qwen 2.5 (3B) [34], Llama 3.2\n(3B), and Llama 3.1 (8B) [13]. The instruction-tuned versions of\nthese models are particularly appropriate for QA tasks, as they are\noptimized to respond effectively to task-specific prompts.\nBaselines. We evaluate the effectiveness of DP-KSA (Algorithm\n1) by comparing it against three representative baselines, each\nreflecting a different level of privacy and retrieval capability.\nThe first baseline, denoted as non-RAG (ğ= 0), represents a\nstrictly private setup where only the question is provided to the\nlanguage model, without any access to external documents. Since no\nretrieval is involved, this configuration guarantees inherent privacy\nand serves as a minimal baseline. For DP-KSA to be considered\npractically useful, it must achieve better performance than this\nno-retrieval baseline under reasonable privacy budgets.\nThe second baseline, labeled as KSA (ğ= âˆ), performs the same\nkeyword extraction as DP-KSA but releases the top-ğ¾keywords\nwithout introducing any noise. This non-private variant effectively\nremoves the privacy constraints while preserving the aggregation\nstructure of our method, making it a strong upper bound for our\nprivacy-preserving approach.\nThe third baseline is the standard retrieval-augmented genera-\ntion pipeline, denoted as RAG (ğ= âˆ). In this setting, the ques-\ntion is paired with the top-2 retrieved documents from the external\nknowledge base, and the full documents are fed directly into the\nlanguage model without any privacy protection. This setting repre-\nsents a theoretical upper bound in terms of utility, as it leverages\nfull retrieval with no noise or low-dimensional approximation.\nSettings. We follow previous work [20] and set ğ›¿= 10âˆ’4. We\nselect ğœ–= {1, 2, 3, 5, 8} to achieve different levels of privacy. We set\nthe number of ensembles to 80 for our method DP-KSA and non-\nprivate KSA method, which is found to be the optimal number of\nensembles and detailed later in the ablation studies in section 6.3.2 .\nMetrics. Following prior works [32, 33], we adopt four widely\nused evaluation metrics, F1, ROUGE-1, ROUGE-L and normalized\nLevenshitein similarity. These metrics collectively capture both lexi-\ncal overlap and semantic alignment between generated outputs and\nground-truth references. The F1 score computes the harmonic mean\nof precision and recall, and is particularly well-suited for QA tasks\nwhere partial correctness is meaningful. It reflects how well the\npredicted answer tokens align with those in the reference, reward-\ning both completeness and accuracy. ROUGE-1 measures unigram\n(i.e., single word) overlap between the prediction and the reference\ntext. This provides a straightforward measure of lexical similarity.\nROUGE-L, on the other hand, incorporates the sequential nature of\nlanguage by computing the longest common subsequence (LCS),\nthus capturing not only which words are shared, but also whether\nthey appear in a similar order, which is an important indicator\nof fluency and coherence in natural language generation. Leven-\nshtein similarity is derived from the Levenshtein distance, which\nquantifies the number of single-character insertions, deletions, or\nsubstitutions required to transform one string into another. By nor-\nmalizing this value, we obtain a score that reflects how closely the\npredicted answer resembles the reference at the character level,\noffering a fine-grained perspective on textual similarity. Across all\nfour metrics, higher scores indicate better alignment between the\ngenerated and ground-truth answers, and thus better performance.\n6.2\nPrivacy-Utility Tradeoff of DP-KSA\nWe show the privacy-utility tradeoff of DP-KSA across four evalu-\nation metrics and three different LLMs on NQ and TQA datasets.\nWe evaluate DP-KSA with different privacy budget ğœ–, ranging from\n1 to 8. Smaller ğœ–indicates stronger privacy constraints. We also\ncompare our method with the non-RAG (ğœ–= 0), non-private KSA\n(ğœ–= âˆ) and non-private RAG (ğœ–= âˆ) baselines.\n6.2.1\nPerformance on NQ dataset. Figure 5 presents the perfor-\nmance of DP-KSA on the NQ dataset. Across all models and metrics,\nwe observe a consistent performance improvement as the privacy\nbudget ğœ–increases from 1 to 8, confirming that DP-KSA effectively\ntrades off privacy for utility. Notably, at moderate privacy levels\n(e.g. ğœ–= 3 or 5), DP-KSA substantially outperforms the strictly pri-\nvate non-RAG baseline, demonstrating its ability to retain useful\nkeywords even under noise. For instance, with Llama 3.2 (3B) as the\ngenerator, the F1 score improves from 21.67 at ğœ–= 0 to 22.55 atğœ–= 2,\nand further to 25.18 at ğœ–= 8, highlighting a strong privacy-utility\ntradeoff. However, at ğœ–= 1, performance tends to drop below or re-\nmain comparable to the non-RAG baseline (ğœ–= 0). This is likely due\nto the higher noise level required under strong privacy constraints,\nwhich hampers the success of the propose-test-release (PTR) con-\ndition H(ğ‘˜) âˆ’H(ğ‘˜+1) > 2, resulting in fewer or no keywords being\nreleased to support final response generation.\nWe find that DP-KSA often matches or even surpasses the perfor-\nmance of the non-private KSA (ğœ–= âˆ) baseline, despite the presence\nof differential privacy noise. For example, using Qwen 2.5 (3B) as\n6\n"}, {"page": 7, "text": "Differentially Private RAG\nProceedings on Privacy Enhancing Technologies YYYY(X)\nFigure 5: Results of DP-KSA on NQ dataset with different generator LLMs: Qwen 2.5 (3B), Llama 3.2 (3B), and Llama 3.1 (8B). We\nuse three baselines including non-RAG (ğœ–= 0), non-private RAG with top-2 retrieved documents (ğœ–= âˆ), and non-private KSA\n(ğœ–= âˆ).\nthe generator, DP-KSA achieves ROUGE-1 and ROUGE-L scores at\na moderate privacy budget (ğœ–= 5) that are comparable to or exceed\nthose of non-private KSA. This suggests that a noisy histogram,\nconstructed across ensembles of multiple responses, effectively\npreserves key semantic contents even in the presence of differ-\nential privacy noise. Additionally, unlike non-private KSA which\ndeterministically selects the most frequent keywords, DP-KSAâ€™s sto-\nchastic keyword release mechanism introduces variation across\nruns, which may help reduce overfitting to overly dominant but po-\ntentially less informative tokens. Overall, these findings underscore\nthat controlled randomness applied over an ensemble of semanti-\ncally coherent outputs enables strong utility-privacy tradeoffs.\nIt is also worth noting that the non-private KSA baseline gen-\nerally falls short of the upper bound set by the non-private RAG\n(ğœ–= âˆ) baseline. This performance gap is likely stems from the\ninformation loss inherent in compressing an ensemble of responses\ninto a fixed set of keywords. Exploring more effective strategies\n7\n"}, {"page": 8, "text": "Proceedings on Privacy Enhancing Technologies YYYY(X)\nTang et al.\nFigure 6: Results of DP-KSA on TQA dataset with different generator LLMs: Qwen 2.5 (3B), Llama 3.2 (3B), and Llama 3.1 (8B). We\nuse three baselines including non-RAG (ğœ–= 0), non-private RAG with top-2 retrieved documents (ğœ–= âˆ), and non-private KSA\n(ğœ–= âˆ).\nfor utilizing these extracted keywords to further narrow the perfor-\nmance gap remains as an important direction for future work.\nInterestingly, we observe that stronger generator models benefit\nmore from DP-KSA. As we move from Qwen 2.5 (3B) to Llama\n3.1 (8B), not only do we observe higher absolute scores, but also\ngreater robustness to DP noise. This supports the intuition that\nlarger models or certain model families are better equipped to infer\ncontext and generate meaningful outputs from partially informative\nor compressed prompts, making them especially well-suited for\nprivacy-preserving QA systems.\nFinally, although all four evaluation metrics follow consistent\nupward trends as ğœ–increases, the degree of improvement varies.\nLevenshtein similarity, which captures fine-grained character-level\ndifferences, improves more noticeably, suggesting that the gener-\nated responses become semantically closer to the references as\n8\n"}, {"page": 9, "text": "Differentially Private RAG\nProceedings on Privacy Enhancing Technologies YYYY(X)\nprivacy budget increases. ROUGE-L and F1 also show steady im-\nprovements, confirming the DP-KSA yields gains across both string\nsimilarity and task-specific QA measures.\n6.2.2\nPerformance on TQA dataset. Figure 6 shows the perfor-\nmance of DP-KSA on the TQA dataset. Similar to the trends ob-\nserved on NQ, we see consistent performance gains as the privacy\nparameter ğœ–increases from 1 to 8, illustrating DP-KSAâ€™s ability to\neffectively trade privacy for utility. Beginning at a privacy budget\nof ğœ–= 2, DP-KSA consistently outperforms the strictly private non-\nRAG (ğœ–= 0) baseline, demonstrating that even under strong privacy\nconstraints, meaningful keywords can still be preserved. One ex-\nample is when using Llama 3.2 (3B) as the generator, the F1 score\nimproves from 55.12 at ğœ–= 0 to 56.92 at ğœ–= 5, indicating a strong\nprivacy-utility tradeoff. Notably, unlike on NQ where performance\nat lower ğœ–values occasionally regresses, DP-KSA on TQA exhibits\nsmoother and more stable improvements even under tight privacy\nbudgets.\nA particular interesting finding is that DP-KSA performs com-\npetitively against the non-private RAG (ğœ–= âˆ) baseline in some\ncases. For instance, with Qwen 2.5 (3B), the ROUGE-1, ROUGE-L,\nand Levenshtein similarity scores at ğœ–= 5 exceed those of the non-\nprivate RAG baseline, despite the added differential privacy noise.\nThis suggests that using keywords from a noisy histogram formed\nacross an ensemble of responses can yield final responses that are\nsemantically rich as those generated with full document contexts,\nparticularly when the retrieved context is noisy or contains more\nredundancy.\nDigging deeper into the baselines, we observe that the non-\nprivate KSA baseline actually outperforms non-private RAG in\nsmaller models such as Qwen 2.5 (3B) and Llama 3.2 (3B), while the\ntrend reverses for larger models like Llama 3.1 (8B). This difference\ncan be attributed to the relative contributions of two information\nsources, namely the consensus of keywords captured by keyword\nfrequency across the ensemble of 80 responses, and the parametric\nknowledge embedded in the generator LLM. For smaller models,\nthe ensemble-driven signal dominates, giving KSA an advantage.\nHowever, as the generator becomes more powerful, its internal\nknowledge plays a larger role in improving generation quality and\nthus shifting the advantage toward full-context RAG.\nIn line with these observations, DP-KSA shows greater benefits\nwith stronger generator models. Moving from Qwen 2.5 (3B) to\nLlama 3.1 (8B), we see not only higher absolute performance but\nalso improved stability across different privacy budgets. This rein-\nforces our earlier conclusion from the NQ results that larger and\nmore capable models are better suited to generalize from sparse or\ncompressed prompts, making them strong candidates for privacy-\npreserving RAG.\nLastly, all four metrics display consistent improvement as ğœ–in-\ncreases, but the extent of gains varies. Levenshtein similarity and\nROUGE-L tend to improve more steeply, indicating that as more\nkeywords are released, the generated final responses become se-\nmantically and structurally closer to the reference answers. F1 and\nROUGE-1 also improve steadily, confirming that DP-KSA remains\neffective across both QA-style metrics and surface-level lexical\noverlap. Compared to NQ, DP-KSA on TQA dataset achieves sig-\nnificantly higher absolute scores, likely due to its more extractive\nanswer structure and the higher redundancy in supporting con-\ntext, which together make it easier for DP-KSA to preserve relevant\ninformation even under privacy constraints.\n(a) NQ\n(b) TQA\nFigure 7: Propose-test-release (PTR) test pass rate of DP-KSA\nwith varying privacy parameters ğœ–on NQ and TQA datasets.\nWe report the results with three different LLMs: Qwen 2.5\n(3B), Llama 3.2 (3B), and Llama 3.1 (8B).\n6.2.3\nPTR test pass rate. We analyze the effect of various privacy\nconstraints of DP-KSA on PTR test pass rates on both NQ and TQA\ndatasets. Following the description of our method DP-KSA in sec-\ntion 4.2, the propose-test-release (PTR) test checks if the condition\nH(ğ‘˜) âˆ’H(ğ‘˜+1) > 2 holds. Figure 7 shows the pass rate changes for\ndifferent privacy parameters ğœ–, ranging from 1 to 8, across three\ndifferent generator LLMs. Across both datasets, we observe a clear\nand consistent trend that the PTR test pass rate increases steadily\nas ğœ–increases. This indicates that relaxing the privacy constraint\nreduces the amount of DP noise added, enabling more privately\nselected keywords to pass the PTR test and being released to aid\nthe final model outputs. In other words, lower noise leads to more\nstable test outcomes and thus higher pass rates. This observation\nalso supports our discussion earlier in the privacy-utility tradeoff\nperformance on both datasets.\n9\n"}, {"page": 10, "text": "Proceedings on Privacy Enhancing Technologies YYYY(X)\nTang et al.\nWhen comparing across models, we find that Qwen 2.5 (3B) trails\nslightly behind both Llama models on NQ and TQA. The Llama\n3.2 (3B) model generally achieves the highest pass rates on the NQ\ndataset, especially showing steep gains between ğœ–= 1 and ğœ–= 5.\nIn contrast, on the TQA dataset, the Llama 3.1 (8B) model slightly\noutperforms the others at higher privacy budgets (ğœ–= 5 and 8),\nsuggesting that its larger capacity helps identify consistent keyword\npatterns even under significant DP noise. These trends highlight\nthat stronger models, particularly those in the Llama family, exhibit\nbetter resilience to DP noise and are more effective at preserving\nsemantic signals necessary for keyword extraction.\nIn terms of dataset differences, we observe that the TQA dataset\nconsistently achieves higher PTR pass rates than NQ across all\nmodels and ğœ–values. Even at ğ‘’ğ‘ğ‘ ğ‘–ğ‘™ğ‘œğ‘›= 1, models on TQA start\nfrom a noticeably higher baseline. This discrepancy likely stems\nfrom differences in dataset structure. TQA answers tend to be more\nextractive and contextually redundant, which helps generate more\nconsistent keyword distributions across ensembles. As a result,\nkeywords in TQA are easier to identify under the added DP noise.\n6.3\nAblation Studies\nWe also conduct ablation studies with varying model sizes and\nnumber of ensembles on NQ dataset to investigate how these hyper-\nparameters affect the performance of our proposed method DP-KSA.\n6.3.1\nEffect of Model Size. Figure 8 illustrates the impact of model\nsize on the performance of DP-KSA across all four metrics, F1,\nROUGE-1, ROUGE-L and Levenshtein similarity, under a fixed pri-\nvacy budget of ğœ–= 3, along with non-private baselines for compar-\nison. In this experiment, we evaluate the Qwen 2.5 model family\nat four scales, 1.5B , 3B, 7B and 32B. To accommodate hardware\nmemory limitations, we use 4-bit quantization to the 32B model\nweights, ensuring that all models can be evaluated under the same\ncomputational constraints.\nAs expected, larger models consistently achieve better perfor-\nmance across most metrics. As model size increases, we observe\nsteady gains in F1 and ROUGE scores under the privacy-preserving\nsetting, indicating that more expressive LLMs are more capable\nof compensating for the restricted input caused by providing only\nkeywords as context in the prompts. Notably, even under a mod-\nerate privacy constraint of ğœ–= 3, the 32B model nearly matches\nthe performance of the fully non-private RAG baseline (ğœ–= âˆ),\ndemonstrating that DP-KSA benefits substantially from scaling to\nlarger model capacities.\nInterestingly, however, we observe a decline in Levenshtein sim-\nilarity as model size increases to 32B. This seemingly counterintu-\nitive trend can be explained by the generative behavior of larger\nmodels. Bigger LLMs, such as Qwen 2.5 (32B), are more likely to\nparaphrase reference answers with greater fluency and lexical diver-\nsity. While these outputs may be semantically accurate, they diverge\nmore from the reference at the character level, which directly low-\ners Levenshtein similarity. Larger models may also produce longer\nor more elaborative responses, introducing more edits even if se-\nmantically valid, while smaller models are more conservative and\nliteral in their generations. Therefore, the drop in Levenshtein sim-\nilarity should be interpreted not as a sign of degraded quality, but\nrather as a side effect of more fluent and paraphrastic generation.\nFigure 8: Ablation studies on model sizes with Qwen 2.5\nmodel family on NQ dataset. Model size axis is in log scale.\nAt the other end of the scale, the smallest model size (1.5B) re-\nveals an unusual pattern. The non-RAG baseline (ğœ–= 0), where only\nthe question is given without any retrieved context, outperforms\nboth the non-private KSA and DP-KSA (ğœ–= 3) settings. This can\nbe explained by the weaker instruction-following capabilities of\nsmaller models. When provided with prompts using keyword-only\ncontext, whether deterministic (KSA) or noisy (DP-KSA), these mod-\nels struggle to interpret and integrate sparse cues into coherent\nanswers. In contrast, the non-RAG setting, which presents only the\nraw question, may align better with the pretraining distribution of\nsmall models and avoid potential confusion from prompts with in-\ncomplete or overly narrow keywords as context. Consequently, the\nmodel relies more heavily on its parametric knowledge, sometimes\nyielding better outputs than poorly constructed context.\nMoreover, between KSA and DP-KSA (ğœ–= 3) at 1.5B, the latter\nsurprisingly performs better. One possible reason is that the sto-\nchastic nature of DP-KSA introduces diversity into the prompts,\noccasionally surfacing less frequent but semantically useful tokens.\nThis noise-driven variation can act as a form of regularization that\nprevents the model from overfitting to dominant but uninformative\nkeywords and this is a vulnerability particularly pronounced in\nsmaller models.\nOverall, these findings underscores that larger models are not\nonly more effective at leveraging prompts with sparse context but\nalso more robust to the limitations imposed by differential privacy.\nThey suggest that DP-KSA is best paired with instruction-following-\ncapable LLMs to achieve strong privacy-utility tradeoffs.\n10\n"}, {"page": 11, "text": "Differentially Private RAG\nProceedings on Privacy Enhancing Technologies YYYY(X)\nFigure 9: Ablation studies on number of ensembles with\nLlama 3.2 (3B) model and ğœ–= 3 on NQ dataset.\n6.3.2\nEffect of Number of Ensembles. Figure 9 shows how the num-\nber of retrieval ensembles impacts the performance of DP-KSA. In\nthis experiment, we use the Llama 3.2 (3B) generator model and\nfix the privacy budget at ğœ–= 3, while varying the number of en-\nsembles from 10 to 100. The number of ensembles reflects how\nmany top-ranked documents are used from retrieval. For instance,\nan ensemble size of 10 uses the top 10 retrieved documents, while\na size of 50 incorporates the top 50. Each ensemble corresponds\nto a different retrieved document used to prompt the generator,\nand their aggregated responses are used to construct a noisy token\nhistogram for differentially private keyword selection.\nWe observe a clear progression in performance trends across all\nfour evaluation metrics as the number of ensembles increases.\nWhen using between 10 and 40 ensembles, performance remains\nrelatively stable. This indicates that aggregating a limited number of\nensembles does not provide sufficient signal strength to overcome\nthe DP noise introduced. With few ensembles, informative tokens\nmay appear sporadically across responses, leading to sparse or\ninconsistent histograms where the DP mechanism struggles to\nidentify high-frequency keywords reliably.\nAs the number of ensembles increases from 40 to 80, performance\nimproves steadily. The addition of more ensembles enriches the\ntoken histogram, making it more representative and resilient to DP\nnoise. Recurrent tokens consistently appearing across semantically\nrelevant responses emerge more clearly, enabling the stochastic\nkeyword release mechanism to retain more meaningful and infor-\nmative content. The growing diversity of responses within this\nrange plays a key role in stabilizing the keyword selection process\nunder DP constraints.\nBetween 80 and 100 ensembles, performance begins to plateau.\nWhile more ensembles continue to be aggregated, the marginal\ngains diminish. Additional documents may contribute less relevant\nor redundant information, increasing the presence of low-utility\nor noisy tokens in the histogram. This can dilute the quality of the\nextracted keywords and limit further improvements in the final\noutput quality.\nThese findings indicate that while increasing the number of en-\nsembles can significantly enhance performance under DP, there\nare diminishing returns beyond a certain point. Once the ensemble\nsize becomes large enough to establish a stable token distribution,\nadding more documents does little to improve and may even slightly\ndegrade the quality of the extracted keywords due to the introduc-\ntion of irrelevant or noisy content. This highlights the need for more\nstrategic methods for selecting the number of ensembles to further\nboost performance without compromising the privacy guarantee.\n7\nRelated Work\n7.1\nPrivacy Attacks on Large Language Models\nPrior work has shown that large language models (LLMs) are vulner-\nable to privacy breaches through a variety of attack vectors. Carlini\net al. [4] demonstrated that adversaries can extract memorized\ntraining examples, such as email addresses, phone numbers, and\nother sensitive content, by carefully crafting input prompts. These\ntraining data extraction attacks reveal that even models trained on\nsupposedly de-identified datasets can still memorize and regurgitate\nspecific private data points. Prompt extraction attacks [9, 24, 38]\nhave also received significant attention. These attacks target de-\nployed systems where users provide prompts that encode propri-\netary logic, credentials, or task-specific templates. Adversaries can\nuse model inversion, gradient leakage, or black-box probing to in-\nfer the original prompts, threatening the confidentiality of model\ncustomization and user inputs in commercial APIs.\nIn the context of retrieval-augmented generation (RAG), recent\nwork has highlighted privacy risks stemming from both the retrieval\nand generation components. Huang et al. [17] analyzed ğ‘˜NN-LMs\nand showed that retrieved neighbors from the training corpus can\nexpose private information, especially when the distance function\nreveals document structure or content characteristics. Zeng et al.\n[36] conducted a systematic investigation into open-source RAG\npipelines and found that sensitive data in the retrieval corpus can\npropagate into generated responses without explicit prompts, par-\nticularly when the retriever ranks memorized or high-sensitivity\ncontent highly. Qi et al. [29] extended these findings to commer-\ncial production-level RAG systems, showing that the configuration\nof RAG, including retrieval granularity, document ranking, and\nprompt construction, can significantly influence the modelâ€™s sus-\nceptibility to data leakage. Together, these studies underline the\nimportance of privacy-aware design to mitigate the growing threat\nof information leakage in retrieval-augmented language generation.\n7.2\nPrivacy-Preserving Large Language Models\nDifferential privacy has been studied in many tasks in large lan-\nguage models. The differentially private pretraining and finetuning\nof LLMs have been studied to address the privacy concern in the\ntraining data by deploying DP-SGD [1]. In this paradigm, noise\nis introduced to the gradient during the modelâ€™s training to en-\nsure privacy. However, as the scale of the large language models\nsignificantly increased, memory becomes a large bottleneck and\nmakes this approach more challenging in practice. Although recent\nmethods have been proposed for efficient per-example gradient\nclipping [23] and parameter-efficient fine-tuning [35], it remains a\ntopic of ongoing research in order to address the engineering and\noptimization problems introduced by DP-SGD. In-context learning\nadapts to different tasks by illustrating some examples in the con-\ntext as the task description. DP in-context learning considers the\n11\n"}, {"page": 12, "text": "Proceedings on Privacy Enhancing Technologies YYYY(X)\nTang et al.\nsituation when the examples are picked from any private dataset.\n[31] tackles this problem by generating synthetic examples with\nDP. [33] instead uses a sample-and-aggregate algorithm to generate\nDP responses.\nTo mitigate privacy risks in RAG systems, [37] proposed an\nempirical privacy-preserving algorithm for RAG through the syn-\nthetic data generation, while our work studies privacy-preserving\nRAG in the framework of differential privacy, which protects the\nprivacy of each individual document with the theoretical guaran-\ntee. Closely-related prior works have proposed DP solutions in\nthe RAG setting by applying DP at every token generation of the\nLLM [14, 20] after retrieving relevant documents. While they offer\nprivacy protection for RAG systems, they come with noticeable\nlimitations. Per-iteration private voting requires composing the\nprivacy loss over the number of tokens being generated, which\nquickly destroys the privacy-utility tradeoff.\n8\nConclusion\nIn this paper, we presented DP-KSA, a novel privacy-preserving al-\ngorithm that ensures differential privacy for sensitive external data\nsource in the RAG system, enabling us to enhance LLMs by domain-\nspecific but sensitive external data source. DP-KSA privatizes RAG\nby extracting the most frequent keywords based on the \"propose-\ntest-release\" paradigm and augmenting them into the prompt to\ngenerate the final output. The privately extracted keywords ef-\nfectively compresses the semantic space while still retaining key\ninformation pertaining to the retrieved contexts for better utility.\nThe experiments on QA benchmarking datasets show that our algo-\nrithm outperforms the non-RAG method under moderate privacy\nbudgets across different models, demonstrating its effectiveness\nfor maintaining high generation quality while providing formal\nprivacy guarantees. We also explored and evaluated the impact of\ngenerator model sizes and number of ensembles on our method.\nIn future work, we will consider a more adaptive scheme for\nselecting the number of ensembles to further improve privacy-\nutility tradeoffs across different generator model families and sizes.\nWhile DP-KSA targets scenarios where the external retrieval corpus\nis private and the generator LLM is trained on publicly available\ndata, commonly seen in real-world RAG systems using open-source\nmodels, it is also worth investigating how DP-KSA affects the train-\ning data leakage risks when the generator LLM is pre-trained or\nfine-tuned on private datasets.\nAcknowledgment\nThe authors used Grammarly and ChatGPT4.o to detect and correct\ngrammatical errors in this paper.\nReferences\n[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov,\nKunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In\nProceedings of the 2016 ACM SIGSAC Conference on Computer and Communications\nSecurity (Vienna, Austria) (CCS â€™16). Association for Computing Machinery, New\nYork, NY, USA, 308â€“318. https://doi.org/10.1145/2976749.2978318\n[2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.\n2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-\nReflection. In The Twelfth International Conference on Learning Representations.\nhttps://openreview.net/forum?id=hSyW5go0v8\n[3] Mark Bun and Thomas Steinke. 2016. Concentrated differential privacy: Simpli-\nfications, extensions, and lower bounds. In Theory of cryptography conference.\nSpringer, 635â€“658.\n[4] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-\nVoss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,\net al. 2021. Extracting training data from large language models. In 30th USENIX\nsecurity symposium (USENIX Security 21). 2633â€“2650.\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading\nWikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers),\nRegina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguis-\ntics, Vancouver, Canada, 1870â€“1879. https://doi.org/10.18653/v1/P17-1171\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association\nfor Computational Linguistics, Minneapolis, Minnesota, 4171â€“4186. https://doi.\norg/10.18653/v1/N19-1423\n[7] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,\nPierre-Emmanuel MazarÃ©, Maria Lomeli, Lucas Hosseini, and HervÃ© JÃ©gou. 2025.\nThe Faiss library. arXiv:2401.08281 [cs.LG] https://arxiv.org/abs/2401.08281\n[8] Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch. 2023.\nFlocks of stochastic parrots: Differentially private prompt learning for large\nlanguage models. Advances in Neural Information Processing Systems 36 (2023),\n76852â€“76871.\n[9] Haonan Duan, Adam Dziedzic, Mohammad Yaghini, Nicolas Papernot, and\nFranziska Boenisch. 2023. On the privacy risk of in-context learning. In The 61st\nAnnual Meeting Of The Association For Computational Linguistics.\n[10] David Durfee and Ryan M Rogers. 2019. Practical differentially private top-k\nselection with pay-what-you-get composition. Advances in Neural Information\nProcessing Systems 32 (2019).\n[11] Cynthia Dwork. 2006. Differential privacy. In International colloquium on au-\ntomata, languages, and programming. Springer, 1â€“12.\n[12] Cynthia Dwork and Jing Lei. 2009. Differential privacy and robust statistics. In\nProceedings of the Forty-First Annual ACM Symposium on Theory of Computing\n(Bethesda, MD, USA) (STOC â€™09). Association for Computing Machinery, New\nYork, NY, USA, 371â€“380. https://doi.org/10.1145/1536414.1536466\n[13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek\nKadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex\nVaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783\n(2024).\n[14] Nicolas Grislain. 2025. RAG with Differential Privacy. arXiv:2412.19291 [cs.LG]\nhttps://arxiv.org/abs/2412.19291\n[15] Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, and Graham Neubig. 2024. Ragged:\nTowards informed design of retrieval augmented generation systems. arXiv\npreprint arXiv:2403.09040 (2024).\n[16] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian\nWang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2025. A\nsurvey on hallucination in large language models: Principles, taxonomy, chal-\nlenges, and open questions. ACM Transactions on Information Systems 43, 2 (2025),\n1â€“55.\n[17] Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, and Danqi Chen. 2023.\nPrivacy Implications of Retrieval-Based Language Models. In EMNLP.\n[18] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A\nLarge Scale Distantly Supervised Challenge Dataset for Reading Comprehension.\nIn Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), Regina Barzilay and Min-Yen Kan (Eds.).\nAssociation for Computational Linguistics, Vancouver, Canada, 1601â€“1611. https:\n//doi.org/10.18653/v1/P17-1147\n[19] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn,\nYulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online,\n6769â€“6781. https://doi.org/10.18653/v1/2020.emnlp-main.550\n[20] Tatsuki Koga, Ruihan Wu, and Kamalika Chaudhuri. 2025.\nPrivacy-\nPreserving\nRetrieval-Augmented\nGeneration\nwith\nDifferential\nPrivacy.\narXiv:2412.04697 [cs.CR] https://arxiv.org/abs/2412.04697\n[21] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,\nKristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M.\nDai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A\nBenchmark for Question Answering Research. Transactions of the Association for\nComputational Linguistics 7 (2019), 452â€“466. https://doi.org/10.1162/tacl_a_00276\n[22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel,\nSebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Proceedings of the 34th International Conference\non Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS â€™20).\n12\n"}, {"page": 13, "text": "Differentially Private RAG\nProceedings on Privacy Enhancing Technologies YYYY(X)\nCurran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages.\n[23] Xuechen Li, Florian TramÃ¨r, Percy Liang, and Tatsunori Hashimoto. 2022. Large\nLanguage Models Can Be Strong Differentially Private Learners. arXiv preprint\narXiv:2110.05679. In International Conference on Learning Representations (ICLR).\nhttps://arxiv.org/abs/2110.05679\n[24] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang,\nTianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, et al. 2023. Prompt Injec-\ntion attack against LLM-integrated Applications. arXiv preprint arXiv:2306.05499\n(2023).\n[25] Frank McSherry and Kunal Talwar. 2007. Mechanism design via differential\nprivacy. In 48th Annual IEEE Symposium on Foundations of Computer Science\n(FOCSâ€™07). IEEE, 94â€“103.\n[26] Ilya Mironov. 2017. RÃ©nyi differential privacy. In 2017 IEEE 30th computer security\nfoundations symposium (CSF). IEEE, 263â€“275.\n[27] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. 2007. Smooth sensitivity\nand sampling in private data analysis. In Proceedings of the Thirty-Ninth Annual\nACM Symposium on Theory of Computing (San Diego, California, USA) (STOC\nâ€™07). Association for Computing Machinery, New York, NY, USA, 75â€“84. https:\n//doi.org/10.1145/1250790.1250803\n[28] Fabio Petroni, Tim RocktÃ¤schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\nAlexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge\nbases? arXiv preprint arXiv:1909.01066 (2019).\n[29] Zhenting Qi, Hanlin Zhang, Eric P. Xing, Sham M. Kakade, and Himabindu\nLakkaraju. 2025.\nFollow My Instruction and Spill the Beans: Scalable Data\nExtraction from Retrieval-Augmented Generation Systems. In The Thirteenth\nInternational Conference on Learning Representations. https://openreview.net/\nforum?id=Y4aWwRh25b\n[30] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\net al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n1, 8 (2019), 9.\n[31] Xinyu Tang, Richard Shin, Huseyin A Inan, Andre Manoel, Fatemehsadat\nMireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, and Robert Sim.\n2024. Privacy-Preserving In-Context Learning with Differentially Private Few-\nShot Generation. In The Twelfth International Conference on Learning Representa-\ntions. https://openreview.net/forum?id=oZtt0pRnOl\n[32] Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong\nWen. 2024. REAR: A Relevance-Aware Retrieval-Augmented Framework for\nOpen-Domain Question Answering. arXiv preprint arXiv:2402.17497 (2024).\n[33] Tong Wu, Ashwinee Panda, Jiachen T Wang, and Prateek Mittal. 2023. Privacy-\npreserving in-context learning for large language models.\narXiv preprint\narXiv:2305.01639 (2023).\n[34] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang\nLin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue,\nPei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia,\nXingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical\nReport. arXiv:2412.15115 [cs.CL] https://arxiv.org/abs/2412.15115\n[35] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam\nKamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al.\n2022. Differentially private fine-tuning of language models. In International\nConference on Learning Representations (ICLR).\n[36] Shenglai Zeng, Jiankun Zhang, Pengfei He, Yiding Liu, Yue Xing, Han Xu, Jie Ren,\nYi Chang, Shuaiqiang Wang, Dawei Yin, and Jiliang Tang. 2024. The Good and\nThe Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG).\nIn Findings of the Association for Computational Linguistics: ACL 2024, Lun-Wei\nKu, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational\nLinguistics, Bangkok, Thailand, 4505â€“4524. https://doi.org/10.18653/v1/2024.\nfindings-acl.267\n[37] Shenglai Zeng, Jiankun Zhang, Pengfei He, Jie Ren, Tianqi Zheng, Hanqing\nLu, Han Xu, Hui Liu, Yue Xing, and Jiliang Tang. 2025. Mitigating the Pri-\nvacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data.\narXiv:2406.14773 [cs.CR] https://arxiv.org/abs/2406.14773\n[38] Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. 2023. Effective prompt\nextraction from language models. arXiv preprint arXiv:2307.06865 (2023).\n[39] Yuqing Zhu and Yu-Xiang Wang. 2022. Adaptive private-k-selection with adaptive\nk and application to multi-label pate. In International Conference on Artificial\nIntelligence and Statistics. PMLR, 5622â€“5635.\n13\n"}, {"page": 14, "text": "Proceedings on Privacy Enhancing Technologies YYYY(X)\nTang et al.\nA\nPrivacy Analysis of DP-KSA\nWe now provide technical details behind the privacy analysis of\nDP-KSA. We first introduce some definitions and theorems to help\nwith the analysis.\nA.1\nRelevant Properties\nIn the main text, we introduced (ğœ–,ğ›¿)-DP (Definition 2.1). Addition-\nally, we will introduce RÃ©nyi Differential Privacy (RDP), a variant\nof (ğœ–,ğ›¿)-DP that uses RÃ©nyi divergence to measure the difference\nbetween ğ‘€(ğ·) and ğ‘€(ğ·â€²).\nDefinition A.1 (RÃ©nyi Divergence). For two probability distribu-\ntions ğ‘ƒand ğ‘„defined over R, the RÃ©nyi divergence of order ğ›¼> 1\nis\nğ·ğ›¼(ğ‘ƒ||ğ‘„) =\n1\nğ›¼âˆ’1 log Eğ‘¥âˆ¼ğ‘„\n\u0014\u0012 ğ‘ƒ(ğ‘¥)\nğ‘„(ğ‘¥)\n\u0013ğ›¼\u0015\n.\nDefinition A.2. ((ğ›¼,ğœ–(ğ›¼))-RDP [26]) A randomized algorithm\nğ‘€: D â†’R is (ğœ–(ğ›¼), ğ›¼)-RDP if for any adjacent datasets ğ·, ğ·â€² âˆˆD\nit holds that ğ·ğ›¼(ğ‘€(ğ·)||ğ‘€(ğ·â€²)) â‰¤ğœ–(ğ›¼).\nAn advantage of RDP is its convenient composition properties,\nwhich states that the privacy loss of the composition of multiple\nRDP algorithms is simply the sum of each RDP algorithm. We state\nthis more formally below.\nTheorem A.3 (Composition [26]). Let ğ´1, ...,ğ´ğ‘˜be a sequence of\n(ğ›¼,ğœ–(ğ›¼))-RDP algorithms. Then the composition ğ´ğ‘˜â—¦ğ´ğ‘˜âˆ’1 â—¦... â—¦ğ´1\nis (ğ›¼,ğ‘˜ğœ–(ğ›¼))-RDP.\nAnother important property of RDP, and more generally DP,\nis that any further operations on the output of an RDP algorithm\ndoes not leak additional information, called the post-processing\nproperty.\nTheorem A.4 (Post-Processing [26]). Let ğ´: D â†’R be (ğ›¼,ğœ–(ğ›¼))-\nRDP, and let ğ¹: R â†’Z be an arbitrary randomized mapping. Then\nğ¹â—¦ğ‘€is (ğ›¼,ğœ–(ğ›¼))-RDP.\nAnother useful relaxation of the RDP definition is approximate\nRDP.\nDefinition A.5 (Approximate RDP [3, 39]). We say a randomized\nalgorithm ğ‘€is ğ›¿-approximately (ğ›¼,ğœ–ğ‘€(ğ›¼))-RDP with order ğ›¼â‰¥1,\nif for all neighboring dataset ğ·, ğ·â€², there exist events ğ¸(depending\non ğ‘€(ğ·)) and ğ¸â€² (depending on ğ‘€(ğ·â€²)) such that Pr[ğ¸] â‰¥1 âˆ’ğ›¿\nand Pr[ğ¸â€²] â‰¥1 âˆ’ğ›¿, and âˆ€ğ›¼â‰¥1, we have\nğ·ğ›¼(ğ‘€(ğ·)|ğ¸âˆ¥ğ‘€(ğ·â€²) |ğ¸â€²) â‰¤ğœ–ğ‘€(ğ›¼)\n(3)\nNote that when ğ›¿= 0, then 0-approximate (ğ›¼,ğœ–(ğ›¼))-RDP is sim-\nply (ğ›¼,ğœ–(ğ‘ğ‘™ğ‘â„ğ‘))-RDP. Finally, we can convert between (ğ›¼,ğœ–(ğ›¼))\nand (ğœ–,ğ›¿)-DP, which is shown below.\nTheorem A.6 (Conversion from approximate RDP to Approximate\nDP [39]). If an algorithm ğ´satisfies ğ›¿1-approximate (ğ›¼,ğœ–(ğ›¼))-RDP,\nthen it is (ğœ–(ğ›¼) + log(1/ğ›¿)\nğ›¼âˆ’1 ,ğ›¿+ ğ›¿1)-DP for any 0 < ğ›¿< 1.\nWe use approximate RDP for a tighter measure of the privacy\ncost under composition. After we obtain the (approximate) RDP\nguarantee for the overall algorithm, we can then convert the privacy\nguarantee back into the standard DP definition (Theorem A.6).\nLastly, we introduce a fundamental DP mechanism, called the\nexponential mechanism [25], which the FindBestK is based on.\nGiven some utility function over outputs, the exponential mech-\nanism samples high-utility outputs with higher probability than\nlow-utility outputs.\nDefinition A.7 (Exponential Mechanism [25]). Given a utility func-\ntionğ‘: Xâˆ—Ã—O â†’R with â„“1 sensitivity Î”(ğ‘) = maxğ·âˆ¼ğ·â€²,ğ‘œâˆˆğ‘‚|ğ‘(ğ·,ğ‘œ)âˆ’\nğ‘(ğ·â€²,ğ‘œ)|, the exponential mechanism ğ‘€has output distribution\nPr[ğ‘€(ğ·) = ğ‘œ] âˆexp\n\u0012ğœ–ğ‘(ğ·,ğ‘œ)\n2Î”(ğ‘)\n\u0013\n.\nwhere âˆelides the normalization factor.\nGiven the above definition, one can show that the exponential\nmechanism satisfies (ğœ–, 0)-DP.\nLemma A.8 ([25]). The exponential mechanism is (ğœ–, 0)-DP.\nSince we want to compose the privacy loss of the exponential\nmechanism and report the loss in terms of (ğœ–,ğ›¿)-DP, we use the\nfollowing property to convert from ğœ–-DP to (ğ›¼,ğœ–(ğ›¼))-RDP.\nTheorem A.9 ([3]). The ğ‘€is ğœ–-DP then it is (ğ›¼,ğœ–EM(ğ›¼))-RDP where\nğœ–EM(ğ›¼) := min\n\u0012ğ›¼\n2 ğœ–2,\n1\nğ›¼âˆ’1 log\n\u0012 sinh(ğ›¼ğœ–) âˆ’sinh((ğ›¼âˆ’1)ğœ–)\nsinh(ğœ–)\n\u0013\u0013\n.\nA.2\nPrivacy Analysis\nNow we will prove that DP-KSA satisfies (ğœ–,ğ›¿)-DP. First, we obtain\na privacy guarantee for TopKWithPTR and FindBestK.\nTheorem A.10. TopKWithPTR (Algorithm 2) is ğ›¿-approximate\nğ›¼\n2ğœ2 -\nRDP.\nProof. The privacy analysis mostly follows from Zhu and Wang\n[39]. Releasing the noisy threshold bğ‘‘ğ‘˜is\nğ›¼\n2ğœ2 -RDP.\nIf ğ‘‘ğ‘˜> 2, then releasing the exact top-ğ‘˜tokens has no privacy\ncost, as its local sensitivity is 0.\nIf ğ‘‘ğ‘˜â‰¤2, then if bğ‘‘ğ‘˜â‰¤2, the program terminates and thereâ€™s no\nprivacy cost.\nIf ğ‘‘ğ‘˜â‰¤2, the failure probability\nPr[bğ‘‘ğ‘˜> 2] = Pr[max(2,ğ‘‘ğ‘˜) + N (0, 4ğœ2) âˆ’Î¦(1 âˆ’ğ›¿; 0, 2ğœ) > 2]\n= Pr[2 + N (0, 4ğœ2) âˆ’Î¦(1 âˆ’ğ›¿; 0, 2ğœ) > 2]\n= Pr[N (0, 4ğœ2) âˆ’Î¦(1 âˆ’ğ›¿; 0, 2ğœ) > 0]\n= ğ›¿\nâ–¡\nTheorem A.11. FindBestK (Algorithm 3) satisfies (ğ›¼,ğœ–EM(ğ›¼))-\nRDP.\nProof. Note that adding Gumbel noise to each outputâ€™s utility\nand releasing the output with the highest noisy utility score is equiv-\nalent to using the exponential mechanism [10]. Hence, FindBestK\nis an EM where the utility function is ğ‘‘ğ‘˜= H(ğ‘˜) âˆ’H(ğ‘˜+1) with\nthe sensitivity Î”(ğ‘‘ğ‘˜) = 2 (this arguement derives from Zhu and\nWang [39]). Therefore, the privacy guarantee follows from Theorem\nA.9.\nâ–¡\nTheorem A.12. DP-KSA (Algorithm 1) satisfies (ğœ–,ğ›¿)-DP.\n14\n"}, {"page": 15, "text": "Differentially Private RAG\nProceedings on Privacy Enhancing Technologies YYYY(X)\nProof. Let ğ·, ğ·â€² be two adjacent datasets the differ by one doc-\nument. Hence, after retrieving the ğ‘most relevant documents for a\nquery x, ğ‘…(x, ğ·; ğ‘) and ğ‘…(x, ğ·â€²; ğ‘) differ by at most one document.\nSuppose it is the ğ‘–-th document. More precisely, ğ·x\nğ‘—= ğ·â€²x\nğ‘—âˆ€ğ‘—â‰ ğ‘–\nand ğ·x\nğ‘–â‰ ğ·â€²x\nğ‘–. Then we obtain the corresponding histograms H\nand Hâ€², which differ by one response yğ‘–â‰ yâ€²\nğ‘–. Hence by Theo-\nrem A.11 Ë†ğ‘˜is (ğ›¼,ğœ–EM(ğ›¼))-RDP. Then using Ë†ğ‘˜we have ğ‘¤1, ...,ğ‘¤Ë†ğ‘˜,\nwhich are ğ›¿1-approximate\nğ›¼\n2ğœ2 -RDP by Theorem A.10. Then by\ncomposition (Theorem A.3) the total privacy loss is ğ›¿1-approximate\n(ğ›¼,ğœ–EM(ğ›¼) +\nğ›¼\n2ğœ2 )-RDP. Moreover, because ğ‘¤1, ...,ğ‘¤Ë†ğ‘˜are RDP, any\nfurther operations on them do not leak additional privacy by post-\nprocessing (Theorem A.4). Hence, using ğ‘¤1, ...,ğ‘¤Ë†ğ‘˜for obtaining the\nfinal output y â†ğ¹(x, {ğ‘¤ğ‘–} Ë†ğ‘˜\nğ‘–=1) is DP. Finally we convert the final\nprivacy loss back to (ğœ–,ğ›¿)-DP using Theorem A.6.\nâ–¡\n15\n"}]}