{"doc_id": "arxiv:2601.06636", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.06636.pdf", "meta": {"doc_id": "arxiv:2601.06636", "source": "arxiv", "arxiv_id": "2601.06636", "title": "MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis", "authors": ["Wenting Chen", "Zhongrui Zhu", "Guolin Huang", "Wenxuan Wang"], "published": "2026-01-10T17:39:25Z", "updated": "2026-01-10T17:39:25Z", "summary": "Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis--relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a \"trap\" case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate--probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.06636v1", "url_pdf": "https://arxiv.org/pdf/2601.06636.pdf", "meta_path": "data/raw/arxiv/meta/2601.06636.json", "sha256": "32b9e540fb2a07e4ed7dc044161e47d60e3885ca4082c4940a8f7bac20226c95", "status": "ok", "fetched_at": "2026-02-18T02:21:52.685552+00:00"}, "pages": [{"page": 1, "text": "MedEinst: Benchmarking the Einstellung Effect in Medical LLMs\nthrough Counterfactual Differential Diagnosis\nWenting Chen1*, Zhongrui Zhu2*, Guolin Huang3, Wenxuan Wang4†\n1Stanford University, 2Xi’an Jiaotong University,\n3Shenzhen University, 4Renmin University of China\nAbstract\nDespite achieving high accuracy on medical\nbenchmarks, LLMs exhibit the Einstellung Ef-\nfect in clinical diagnosis—relying on statistical\nshortcuts rather than patient-specific evidence,\ncausing misdiagnosis in atypical cases. Exist-\ning benchmarks fail to detect this critical fail-\nure mode. We introduce MedEinst, a counter-\nfactual benchmark with 5,383 paired clinical\ncases across 49 diseases. Each pair contains\na control case and a \"trap\" case with altered\ndiscriminative evidence that flips the diagno-\nsis. We measure susceptibility via Bias Trap\nRate—probability of misdiagnosing traps de-\nspite correctly diagnosing controls. Extensive\nEvaluation of 17 LLMs shows frontier mod-\nels achieve high baseline accuracy but severe\nbias trap rates. Thus, we propose ECR-Agent,\naligning LLM reasoning with Evidence-Based\nMedicine standard via two components: (1) Dy-\nnamic Causal Inference (DCI) performs struc-\ntured reasoning through dual-pathway percep-\ntion, dynamic causal graph reasoning across\nthree levels (association, intervention, counter-\nfactual), and evidence audit for final diagnosis;\n(2) Critic-Driven Graph & Memory Evolution\n(CGME) iteratively refines the system by stor-\ning validated reasoning paths in an exemplar\nbase and consolidating disease-specific knowl-\nedge into evolving illness graphs. Source code\nis to be released.\n1\nIntroduction\nLarge Language Models (LLMs) (Achiam et al.,\n2023; Touvron et al., 2023) and LLM-based agents\n(Tang et al., 2024; Kim et al., 2024) achieve high\nperformance on medical benchmarks (Jin et al.,\n2021). However, Kim et al. (2025) show these\nmodels exhibit the Einstellung Effect, relying on\nstatistical shortcuts rather than logical reasoning.\nThis causes models to prioritize common patterns\n*Equal contribution.\n†Corresponding Author.\nFigure 1: (a) Example of Einstellung Effect (b) Dis-\ntribution of failure modes under the Einstellung Effect\nacross reasoning LLMs, including Blindness (missing\nkey evidence), Underthinking (insufficient reasoning),\nand Overthinking (rationalizing incorrect priors).\nover patient-specific evidence when encountering\nmisleading features, ignoring key discriminative\nevidence. This effect is particularly problematic in\ndifferential diagnosis (DDx), where distinguishing\nbetween competing hypotheses depends on sub-\ntle symptomatic differences. Mitigating the Ein-\nstellung Effect in DDx is essential for deploying\ntrustworthy clinical AI systems.\nAlthough various medical benchmarks evaluate\nMed-LLMs(Singhal et al., 2023; Nori et al., 2023;\nYan et al., 2024), they assess general medical ca-\npabilities rather than susceptibility to the Einstel-\nlung Effect. Existing benchmarks focus on knowl-\nedge evaluation (e.g., Medical QA on USMLE(Jin\net al., 2021; Pal et al., 2022)) or clinical task per-\nformance (e.g., Clinical Summarization(Johnson\net al., 2023) and Prognosis Prediction (Jiang et al.,\n2023; Chen et al., 2024)), testing static knowledge\nrecall and standardized procedures. The Einstel-\nlung Effect manifests critically in DDx scenarios\nrequiring identification of subtle discriminative fea-\ntures between similar diseases. Detecting this ef-\nfect requires a counterfactual evaluation design:\n1\narXiv:2601.06636v1  [cs.CL]  10 Jan 2026\n"}, {"page": 2, "text": "presenting cases with similar symptoms but differ-\nent diagnoses to assess whether models override\npattern-based shortcuts for case-specific reasoning.\nHowever, current benchmarks lack such counter-\nfactual scenarios. Thus, a specialized benchmark is\nneeded to evaluate the Einstellung Effect in LLMs.\nWhile current reasoning LLMs demonstrate\nstrong logical capabilities, they remain suscepti-\nble to the Einstellung Effect in differential diagno-\nsis. These models follow a \"think-before-answer\"\nparadigm but primarily establish simple symptom-\ndisease associations rather than identifying dis-\ncriminative evidence to disrupt pattern-based short-\ncuts. In Fig. 1, GPT-5 exhibits blindness in over\n35% of error cases—completely ignoring key dis-\ncriminative symptoms and defaulting to stereotyp-\nical diagnoses. Among cases where key symp-\ntoms are acknowledged, 43% involve underthink-\ning (insufficient analysis) and 22% involve over-\nthinking (motivated reasoning). These patterns\nreveal that current models lack structured mech-\nanisms for rigorous evidence analysis.\nIn con-\ntrast, real-world clinical practice follows Evidence-\nBased Medicine (EBM)(Sackett, 1997) framework:\n(1) Problem Representation—objectively recon-\nstructing patient conditions; (2) Acquire & Ap-\npraise—actively seeking and verifying discrim-\ninative evidence; and (3) Apply—grounding di-\nagnoses in verified evidence. Existing reasoning\nLLMs unfold reasoning linearly based on intu-\nition, forcing a black-box \"Symptoms →Diag-\nnosis\" mapping while neglecting the interpretable\n\"Symptoms →Evidence Verification →Diag-\nnosis\" path. Therefore, constructing a reasoning\nframework grounded in EBM’s cognitive architec-\nture is imperative to mitigate the Einstellung Effect.\nMedEinst: To bridge these gaps, we introduce\nMedEinst, a benchmark for evaluating the Ein-\nstellung Effect in medical LLMs via counterfactual\ndifferential diagnosis. MedEinst contains 5,383\npaired clinical cases spanning 49 diseases across\neight departments. To enable counterfactual eval-\nuation, we employ a rigorous four-stage pipeline\nto generate the paired samples. Each pair consists\nof a control case and a minimally edited trap case:\nthe trap case preserves most contextual evidence\nfrom the control case but replaces only the key dis-\ncriminative evidences so that the correct diagnosis\nflips to a competing disease. This paired design\ncreates counterfactual DDx scenarios in which su-\nperficial pattern matching strongly favors the origi-\nnal label, while correct diagnosis requires attend-\ning to the modified discriminative evidence. Using\nthese pairs, we quantify susceptibility to the Einstel-\nlung Effect with Bias Trap Rate, the probability\nthat a model—despite correctly solving the control\ncase—misdiagnoses the trap case as the control\nlabel. We evaluate a broad set of 10 general and 5\nmedical-domain LLMs, as well as 2 LLM-based\nagents on MedEinst, and observe substantial Ein-\nstellungs Effect errors across different models.\nECR-Agent: To mitigate the Einstellung Effect,\nwe propose ECR-Agent (Evidence-based Causal\nReasoning Agent), an agentic framework that emu-\nlates clinicians’ EBM-grounded reasoning process\nthrough explicit discriminative evidence verifica-\ntion. ECR-Agent comprises two core components:\n(1) Dynamic Causal Inference (DCI) for struc-\ntured diagnostic reasoning, and (2) Critic-driven\nGraph and Memory Evolution (CGME) for ac-\ncumulating clinical experience. The DCI module\noperationalizes the EBM framework through three\nstages. First, dual-pathway perception generates\nboth intuitive differential diagnoses and an objec-\ntive problem representation from patient symptoms,\npreventing premature diagnostic closure. Second,\ndynamic causal graph reasoning systematically\nseeks and verifies discriminative evidence through\nthree progressive steps, each corresponding to a\nlevel in Pearl’s causal hierarchy (Pearl and Macken-\nzie, 2018)—moving from observing patterns to ac-\ntively testing hypotheses to counterfactual verifi-\ncation: (i) Causal Graph Initialization (Associa-\ntion level—observing correlations)—constructs a\ncausal graph connecting observed symptoms, can-\ndidate diseases, and a pre-defined illness graph\nwith prior illness knowledge to establish initial di-\nagnostic hypotheses based on symptom-disease as-\nsociations; (ii) Forward Causal Reasoning (In-\ntervention level—testing what happens if we seek\nnew evidence)—actively retrieves discriminative\nevidence from external knowledge bases as pivot\nnodes while incorporating typical supporting evi-\ndence as general nodes, then evaluates how each\npiece of evidence supports or refutes competing di-\nagnoses to prevent underthinking; (iii) Backward\nCausal Reasoning (Counterfactual level—asking\n\"what if this disease were true?\")—performs coun-\nterfactual verification by identifying what evidence\nwould be missing for each hypothesis, represented\nas shadow nodes that penalize incomplete diag-\nnostic support and prevent overthinking. Third,\nthe evidence audit module computes an evidence-\nbased causal graph score for each candidate disease,\n2\n"}, {"page": 3, "text": "generates graph summary with disease-centric sub-\ngraphs, retrieves similar cases from an exemplar\nbase, and produces the final diagnosis grounded\nin verified evidence rather than pattern matching.\nThe CGME module enables experience accumu-\nlation across cases. Using a critic model, it itera-\ntively refines diagnostic predictions until correct-\nness is achieved, then stores: (1) case-level experi-\nence—the complete reasoning trace in the exemplar\nbase for future retrieval; and (2) illness-level ex-\nperience—merging and refining causal subgraphs\nacross cases into consolidated illness graphs that\ncapture refined discriminative patterns for each dis-\nease. Our contributions are as follows:\n• We propose MedEinst, the first benchmark\nfor evaluating the Einstellung Effect in med-\nical LLMs, and introduce a novel metric re-\nvealing substantial model susceptibility.\n• We propose ECR-Agent, an evidence-based\nframework to systematically verify discrimi-\nnative evidence and accumulate clinical expe-\nrience, mitigating the Einstellung Effect.\n• Through extensive experiments, we demon-\nstrate ECR-Agent’s superiority and reveal cur-\nrent LLMs suffer from Einstellung Effect.\n2\nRelated Work\n2.1\nMedical LLMs and Agents\nLLMs have progressed from general medical as-\nsistants (Singhal et al., 2023; Achiam et al., 2023)\npassing USMLE exams to reasoning models us-\ning \"think-before-answer\" paradigms and LLM-\nbased agents employing collaboration and retrieval.\nAgentic frameworks like MDAgents (Kim et al.,\n2024) and MedAgents (Tang et al., 2024) use\nmulti-role debate, while RAG systems like Med-\nGraphRAG (Wu et al., 2024) and PrimeKG (Chan-\ndak et al., 2023) incorporate Knowledge Graphs\nto reduce hallucinations. However, current models\nsuffer from the Einstellung Effect (Alavi Naeini\net al., 2023; Kim et al., 2025), using associative\n\"Symptoms →Diagnosis\" mappings instead of\nsystematically verifying discriminative evidence.\nThis leads models to favor statistical shortcuts\nover patient-specific evidence, with multi-agent\ncollaboration potentially amplifying Consensus\nBias (Schmidgall et al., 2024a). We therefore intro-\nduce ECR-Agent, an Evidence-Based Medicine\n(EBM) agentic framework (Sackett, 1997) that\nsystematically verifies discriminative evidence\nthrough structured \"Symptoms →Evidence Verifi-\ncation →Diagnosis\" reasoning.\n2.2\nMedical Benchmarks for LLMs\nBenchmarks for medical LLMs have shifted from\nstatic knowledge recall to dynamic reasoning.\nEarly datasets like MedQA (Jin et al., 2021) and\nPubMedQA (Jin et al., 2019) assess factual knowl-\nedge, while DDXPlus (Fansi Tchango et al., 2022)\nand AgentClinic (Schmidgall et al., 2024c) evalu-\nate diagnostic processes. However, existing bench-\nmarks typically employ Independent and Identi-\ncally Distributed (I.I.D.) samples or standard clini-\ncal presentations. They lack adversarial and coun-\nterfactual designs required to expose the Einstel-\nlung Effect. High performance on these datasets\nmay reflect statistical fitting rather than robust rea-\nsoning. Thus, we propose MedEinst, a benchmark\nto evaluate the Einstellung Effect in medical LLMs\nvia counterfactual differential diagnosis.\n3\nMedEinst Benchmark\nOverview. We introduce MedEinst, a benchmark\nto evaluate the Einstellung Effect in medical LLMs\nthrough counterfactual differential diagnosis via a\nfour-stage construction pipeline (Fig. 2). Moreover,\nwe propose the Bias Trap Rate to quantify how of-\nten models solve a control case but fail a minimally\nedited trap case due to superficial reasoning.\n3.1\nProblem Formulation\nWe formalize medical diagnosis as a mapping\nf : X →Y ,where X denotes the patient narrative\nspace and Y is the label space of 49 pathologies.\nWe define a Counterfactual Pair(xc,xt) consisting\nof: (1) Control Case (xc), a typical presentation\nwhere statistical priors align with the ground truth\n(GT) ygt; and (2) Trap Case (xt), an adversarial\nvariant generated via minimal modification. Cru-\ncially, xt remains statistically similar to ygt but\nlogically implies a bias label ybias due to specific\ndiscriminative evidence.\nDefinition 1 (Einstellung Effect).\nA model f\nexhibits the Einstellung Effect if and only if:\nf(xc) = ygt\n∧\nf(xt) = ygt\n(1)\nThis implies that while the model demonstrates\nfundamental diagnostic competence (evidenced by\nsuccess on the control case), it fails to rectify its\n3\n"}, {"page": 4, "text": "Figure 2: Data construction of MedEinst with four-stage process: (1) Data Filtering for hard candidates, (2)\nNarration Conversion to natural language, (3) Differential Features Rewrite for trap case generation, and (4)\nInter-Model Verification for quality control.\nprior intuition when confronted with the discrim-\ninative features in the trap case, rigidly persisting\nwith the original diagnosis.\n3.2\nBenchmark Construction\nData Filtering. We collect 226,814 samples cov-\nering 49 pathologies from the DDXPlus dataset\n(Fansi Tchango et al., 2022) Dsrc and filter for\n\"Hard Candidates\" where evidence-based reason-\ning is strictly necessary. Specifically, we select sam-\nples where the probability gap between the ground\ntruth diagnosis ygt and the top competing diagnosis\nybias is less than 0.5%, ensuring that prior proba-\nbilities alone cannot distinguish between diagnoses\nand forcing the model to perform evidence-based\ndifferential diagnosis.\nNarration Conversion. To simulate real-world\nclinical scenarios, we transform structured feature\nsets s into first-person natural language narratives\nx that capture the unstructured and noisy character-\nistics of actual medical records.\nDifferential Features Rewrite. This module pre-\ncisely induces the Einstellung trap while maintain-\ning clinical validity. To prevent hallucination, we\nground our generation in the DDXPlus Knowl-\nedge Base (K) rather than using standard rewriting.\nSpecifically, we first perform Differential Features\nExtraction to identify the key discriminative fea-\ntures kgt that distinguish ygt from ybias. Second,\nTrap Information generation (ktrap) strictly derives\nmisleading evidence from the bias disease knowl-\nedge base Kbias. Finally, Evidence Substitution\nuses an LLM to replace kgt with ktrap, generating\nxt. This ensures the trap case logically points to\nybias while preserving all other contextual informa-\ntion from the control case.\nInter-Model Verification. To ensure high-quality\npairs, we employ an “LLM-as-a-Judge” commit-\ntee J = {GPT-5, DeepSeek-R1, Gemini-2.5-Pro}\nto assess each pair (xc, xt) across three dimen-\nsions: diagnostic correctness verifies whether xt\nlogically points to ybias, medical plausibility as-\nsesses alignment with real-world medical logic,\nand narrative fluency evaluates text coherence (See\nAppendix B.2 for details). A pair is included in\nMedEinst Sfinal only if at least two judges vote\npositively on diagnostic correctness. As shown\nin Appendix Fig. 7, selected trap cases maintain\nhigh plausibility and fluency comparable to con-\ntrol cases, ensuring performance drops stem from\nreasoning failures rather than textual artifacts.\n3.3\nDataset Statistics\nMedEinst contains 5,383 counterfactual pairs of\nclinical narratives (10,766 cases total) covering 49\npathologies, derived from the DDXPlus test split\nto avoid data leakage. To provide an additional\ntraining set, we process and verify 10,689 pairs\nfrom the DDXPlus training split.\n3.4\nQuality Control\nTo ensure clinical validity in MedEinst, we imple-\nmented a rigorous quality control process involving\nfour board-certified physicians with over 8 years\nof clinical experience. Our evaluation examined a\nstratified random sample of 1,500 counterfactual\npairs (27.9% of the dataset). We developed a stan-\ndardized scoring protocol evaluating seven binary\nquality dimensions: clinical plausibility of both\ncontrol and trap cases, logical consistency of dis-\ncriminative features, appropriateness of diagnoses,\nminimality of edits, and absence of artifactual pat-\nterns. Physicians evaluate each dimension through\nyes/no responses, with pairs satisfying all dimen-\nsions considered valid. The quality assessment\nyielded strong results, with 96.1% of evaluated\npairs meeting our thresholds. Dimension-specific\nquality rates ranged from 94.3% to 98.2%. Inter-\n4\n"}, {"page": 5, "text": "rater reliability analysis produced a Fleiss’ kappa\nof 0.79, indicating substantial agreement. Pairs fail-\ning thresholds (3.9%) were either revised (2.1%) or\nexcluded (1.8%) to maintain benchmark integrity.\n3.5\nEvaluation Metrics\nTo quantify the Einstellung Effect, we first prompt\nthe model to generate diagnostic results for all\ncounterfactual pairs (xc, xt).\nThen, we eval-\nuate performance using three specific metrics\nbased on the set of samples Scorrect_control where\nthe model correctly diagnosed the control case\n(f(xc) = ygt). Baseline Accuracy (Accbase =\n|Scorrect_control|/Ntotal) establishes the model’s\nfundamental diagnostic capability. Robust Accu-\nracy (Accrob = PN\ni=1 I(f(xc\ni) = ygt ∧f(xt\ni) =\nybias)/Ntotal) measures the proportion of pairs\nwhere the model correctly predicts both the control\nand trap cases. Finally, our primary metric, Bias\nTrap Rate (Rbias = P\ni∈Scorrect_control I(f(xt\ni) =\nygt)/|Scorrect_control|), calculates the conditional\nprobability that a capable model fall in the trap\ngiven that the model possesses the fundamental di-\nagnostic capability. Ntotal denotes the number of\ncounterfactual pairs.\n4\nECR-Agent Framework\nOverview. To mitigate the Einstellung Effect, we\npropose ECR-Agent framework to align LLM rea-\nsoning with the rigorous verification standards of\nEBM (Fig. 3). ECR-Agent comprises two syner-\ngistic components: (1) Dynamic Causal Inference\n(DCI), which performs structured diagnostic rea-\nsoning through dual-pathway perception, a three-\nlevel causal graph verification process (spanning as-\nsociation, intervention, and counterfactual levels),\nand evidence audit; and (2) Critic-driven Graph and\nMemory Evolution (CGME), which facilitates con-\ntinuous improvement by refining diagnostic outputs\nand accumulating clinical experience into dynamic\nknowledge bases (Appendix Algorithm 2 and C).\n4.1\nCritic-Driven Graph & Memory\nEvolution\nTo accumulate diagnostic experience, we execute\nthe DCI pipeline on the training set Dtrain and\nintroduce a critic model Mcritic (GPT-5) to orches-\ntrate iterative refinement (Fig. 3 (b)). For each\ntraining case where the base model’s prediction di-\nverges from GT label, Mcritic provides corrective\nfeedback to optimize the reasoning path (maximum\n3 rounds). Upon achieving correct diagnosis, the\nvalidated graph summary is merged with existing\nillness graphs G = {Gy|y ∈Y}—a collection\nof disease-specific causal graphs initialized with\nthe first graph summary—and further refined by\nthe critic model to consolidate disease-level knowl-\nedge. Simultaneously, validated reasoning trajecto-\nries (x, ygt, Path) are stored in an Exemplar Base\n(M) for case-based retrieval during inference.\n4.2\nThe Dynamic Causal Inference (DCI)\n4.2.1\nDual-Pathway Perception\nTo implement EBM’s first principle of objective\nproblem representation, we decouple statistical pri-\nors from factual observation through two parallel\npathways. Firstly, the intuitive pathway generates\nTop-k candidate diagnoses Dset = {d1, ..., dk} via\nChain-of-Thought prompting, capturing pattern-\nbased hypotheses. Secondly, the analytic path-\nway produces a problem representation that ob-\njectively summarizes key case features indepen-\ndent of diagnostic assumptions. From this rep-\nresentation, we extract structured patient obser-\nvations Pobs = {p1, ..., pm} and explicitly cate-\ngorize each observation’s status s(p) as Present\n(affirmed), Absent (negated), or Missing (unmen-\ntioned). This dual-pathway design forces the model\nto acknowledge objective clinical facts before form-\ning diagnostic conclusions, preventing premature\nclosure driven by superficial pattern matching.\n4.2.2\nDynamic Causal Graph Reasoning\n(DCGR)\nDCGR aligns with Pearl’s causal hierarchy through\nthree levels: (1) Causal Graph Initialization (as-\nsociation) connects symptoms Pobs with candidates\nDset via illness graphs G; (2) Forward Causal\nReasoning (intervention) retrieves and evaluates\ndiscriminative evidence; (3) Backward Causal\nReasoning (counterfactual) penalizes hypotheses\nvia expected-but-absent \"shadow nodes\".\nCausal Graph Initialization. To establish initial\ndiagnostic hypotheses based on observed correla-\ntions, we construct a causal graph integrating pa-\ntient observations with disease knowledge. For\neach candidate d ∈Dset, we retrieve its illness\ngraph G(d)\nill = (Vd, Vp, Vk; E) from G, where Vd,\nVp, Vk represent disease, symptom, and knowledge\nnodes, and E denotes their relationships. We per-\nform merge-or-prune based on embedding simi-\nlarity between observations Pobs and Vp, retain-\ning relevant nodes and merging novel observations,\nyielding the contextualized initial graph Gill.\n5\n"}, {"page": 6, "text": "Figure 3: ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine via two parts: (a) Dynamic Causal\nInference (DCI) performs structured reasoning via dual-pathway perception, dynamic causal graph reasoning\nacross three levels, and evidence audit for final diagnosis. (b) Critic-Driven Graph & Memory Evolution\n(CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating\ndisease-specific knowledge into evolving illness graphs.\nForward Causal Reasoning. To prevent under-\nthinking by actively seeking comprehensive dis-\ncriminative evidence, we simulate the intervention:\n\"What happens if we actively seek new evidence\nto differentiate among competing diseases?\" We re-\ntrieve medical knowledge from external sources\n(PubMed, OpenTargets) and extract: (1) pivot\nnodes Va—discriminative evidence differentiating\ndiseases; (2) general nodes Vb—typical support-\ning evidence. We expand Gill with these nodes:\nG′\nill = Gill ∪(Va, Vb). Using Qwen3-32B, we\nidentify 5 causal relations: conflict, matching, rule\nout, support, and penalty. For Vp ↔Vk, we clas-\nsify as conflict or matching; for Vd ↔Vk, as rule\nout or support, producing the refined graph G′\nill.\nBackward Causal Reasoning. To prevent over-\nthinking and motivated reasoning, we perform\ncounterfactual verification asking: \"If disease d\nwere true, what evidence should we observe?\" For\neach d, we trace backward to identify support-\ning knowledge nodes V (d)\nk\nand expected symptom\nnodes V (d)\np\n. When knowledge node vk ∈V (d)\nk\nlacks matching observations in Pobs, we trigger\ncounterfactual verification (purple dashed line), re-\nexamining the case text x. If evidence remains\nunverified, we instantiate a shadow node vs (grey\nnode) with a penalty edge to d, yielding a final\ncausal graph G†\nill. Shadow nodes explicitly penal-\nize hypotheses lacking expected evidence, ensuring\ndiagnoses are grounded in verified evidence.\n4.2.3\nEvidence Audit\nGraph Scoring and Summary:\nTo quantify\nevidential support, we calculate an evidence-\nbased causal graph score S(d) for each candi-\ndate d: S(d) = wmNmatch(d) −wcNconf(d) −\nwsNshadow(d), where Nmatch(d), Nconf(d), and\nNshadow(d) count edges with matching, conflict,\nand penalty relations, respectively, and wm, wc, ws\nare weighting hyperparameters. We then generate a\nGraph Summary by reorganizing the causal graph\nG†\nill into k disease-centric subgraphs, each cen-\ntered on a candidate diagnosis. This reorganization\npreserves all graph information while structuring\nevidence around each hypothesis to facilitate evi-\ndence auditing.\nECR-Agent then integrates three information\nstreams to derive the final diagnosis y∗: (1) in-\ntuition—initial reasoning from dual-pathway per-\nception; (2) evidence—graph summary and scores\nS(d); (3) experience—similar cases retrieved from\nexemplar base M. This holistic audit ensures the\ndiagnosis is grounded in verified evidence rather\nthan pattern-based biases.\n5\nExperiments\n5.1\nEvaluation Baselines\nWe compare ECR-Agent against 3 baseline\ntypes: 1) General LLMs: state-of-the-art propri-\netary (GPT-5, Claude-Sonnet-4.5, Gemini-2.5-Pro)\nand open-source LLMs (DeepSeek-R1, Qwen3-\n6\n"}, {"page": 7, "text": "Table 1: Performance comparison of current LLMs and LLM-based Agents.\nModel\nSize\nBaseline Acc (↑)\nRobust Acc (↑)\nBias Trap Rate (↓)\nOpen-Source LLMs\nKimi-k2\n-\n47.82\n12.46\n47.12\nDeepSeek-R1\n-\n42.20\n11.32\n46.12\nZhipuAI/GLM-4.6\n-\n39.65\n11.25\n47.63\nQwen/Qwen3-14B\n14B\n44.12\n11.28\n54.19\nQwen/QwQ-32B\n32B\n41.05\n11.14\n44.88\nQwen/Qwen3-32B\n32B\n40.25\n11.86\n43.46\nQwen/Qwen3-235B-A22B\n235B\n40.51\n11.40\n43.32\nProprietary LLMs\nGPT-5\n-\n54.30\n15.78\n51.87\nGemini-2.5-pro\n-\n53.58\n10.97\n60.90\nClaude-Sonnet-4.5\n-\n42.09\n12.36\n42.98\nMedical-Specific LLMs\nLingshu-7B\n7B\n14.93\n3.20\n36.74\nLlama3-Med42-8B\n8B\n6.51\n1.19\n44.00\nMedGemma-27B-text-it\n27B\n40.92\n11.68\n53.88\nBaichuan-M2-32B\n32B\n45.03\n7.18\n66.10\nLingshu-32B\n32B\n27.68\n6.17\n54.20\nLLM-based Agents\nMDAgent (Qwen3-32B)\n32B\n29.70\n10.34\n40.34\nDyLAN (Qwen3-32B)\n32B\n32.11\n8.11\n41.69\nECR-Agent (Qwen3-32B)\n32B\n69.49\n24.21\n33.75\n32B, QwQ-32B); 2) Medical LLMs: Lingshu-\n7B, Llama3-Med42-8B, MedGemma-27B-text-it,\nBaichuan-M2-32B and Med42-8B; 3) LLM-based\nAgent: MDAgent (Kim et al., 2024) and Dy-\nLAN (Liu et al., 2024).\n5.2\nOverall Performance Comparison\nTable 1 reveals a striking gap between diagnostic\ncapability and robustness. While frontier models\nlike GPT-5 and Gemini-2.5-Pro achieve the high-\nest baseline accuracy (54.30% and 53.58%), they\nexhibit disproportionately high Bias Trap Rates\n(>50%), indicating a fundamental trade-off where\nmodels that better fit general medical distribu-\ntions develop stronger priors that aggressively fil-\nter out low-probability counter-evidence (Percep-\ntual Blindness, Fig. 1), making them more sus-\nceptible to Einstellung traps than weaker models.\nAgent frameworks like MDAgent (multi-role de-\nbate) and DyLAN (dynamic agent selection) show\nlow robust accuracy (~8-10%) and high trap rates\ndue to noise amplification, where dynamic inter-\naction topologies merely reinforce the dominant\nstatistical prior (Consensus Bias) rather than cor-\nrecting it—DyLAN’s strategy of selecting \"high-\ncontribution\" agents exacerbates this by favoring\nagents that align with the incorrect group consen-\nsus. In contrast, ECR-Agent achieves substantial\nTable 2: Ablation Study on ECR-Agent components.\nDCI CGME\nBase Acc (↑) Rob Acc (↑) Trap Rate (↓)\n40.25\n11.86\n43.46\n✓\n55.49\n19.94\n38.32\n✓\n✓\n69.49\n24.21\n33.75\nimprovements (69.49% baseline accuracy, 24.21%\nrobust accuracy, 33.75% bias trap rate), empiri-\ncally validating that resolving the Einstellung Ef-\nfect requires a paradigm shift from statistical fitting\n(probability) to causal verification (evidence).\n5.3\nAblation Study\nWe conduct an ablation study on ECR-Agent\n(Qwen3-32B as the base model) to evaluate the\nmodule effectiveness. In Table 2, adding DCI sub-\nstantially improves Base Accuracy from 40.25%\nto 55.49%, showing the effectiveness of structured\ncausal reasoning. Further incorporating CGME\nyields additional significant gains to 69.49% Base\nAccuracy and reduces Trap Rate to 33.75%, prov-\ning the critical role of experience accumulation.\n5.4\nDisease-Specific Analysis\nFig. 4 reveals heterogeneity in Bias Trap Rates\nacross diseases, exposing the structural nature of\nthe Einstellung Effect: a systematic “High-Bias\n7\n"}, {"page": 8, "text": "Gemini-2.5-Pro\nKimi-K2\nGLM-4.6\nDeepSeek-R1\nGPT-5\nQwen3-235B-A22B\nQwQ-32B\nQwen3-32B\nClaude-Sonnet-4.5\nQwen3-14B\nHIV (initial infection)\nAcute COPD exacerbation / infection\nPulmonary embolism\nPancreatic neoplasm\nMyasthenia gravis\nPanic attack\nInfluenza\nAnemia\nBoerhaave\nGuillain-Barré syndrome\nGERD\nPulmonary neoplasm\nPSVT\nSLE\nBronchitis\nAtrial fibrillation\nTuberculosis\nAcute laryngitis\nSarcoidosis\n0\n0.2\n0.4\n0.6\n0.8\n1\nBias Trap Rate\nFigure 4: Bias Trap Rate heatmap across diseases. The clustering indicates that models learn spurious correlations\nfor common diseases (e.g., Pneumonia), leading to consistent bias.\nFigure 5: Baseline Accuracy vs. Bias Trap Rate.\nCluster” emerges in diseases like Pulmonary Em-\nbolism and Initial HIV Infection (bottom rows)\nwhose presentations overlap with high-prevalence\ndistractors (e.g., Flu, Anxiety), where LLMs learn\nspurious correlations between generic symptoms\nand statistically probable diagnoses while ignoring\nkey discriminative evidence. This failure persists\nacross all architectures, e.g. reasoning-optimized\n(DeepSeek-R1) and massive-scale LLMs (Qwen3-\n235B), showing CoT capabilities as pattern match-\ners that collapse when diagnosis requires overriding\npriors with specific evidence.\n5.5\nScaling Laws vs. Einstellung Effect\nFig. 5 reveals a failure of Scaling Laws in robust\nmedical reasoning: no meaningful correlation ex-\nists between model size and robustness, with fron-\ntier models like GPT-5 and Gemini-2.5-Pro oc-\ncupying a “High Capability, High Bias” region\nwhere scaling improves baseline diagnostic capabil-\nity (ACCbase) but paradoxically exacerbates Ein-\nstellung susceptibility. We term this “Stronger\nPriors, Stronger Blindness”: larger models cap-\nture statistical regularities so effectively they be-\ncome overconfident in initial intuitions, making it\nharder to override diagnoses when presented with\nsubtle counter-evidence—a trend evident across\nmodel tiers (e.g., Gemini-2.5-Pro achieves superior\nbaseline accuracy yet exhibits a 60.90% bias trap\nrate, significantly higher than less capable mod-\nels). These findings demonstrate the Einstellung\nEffect as a fundamental cognitive failure mode that\npersists with scale, necessitating architectural inter-\nventions like ECR-Agent that decouple evidence\nverification from probabilistic generation.\n6\nConclusion\nWe introduced MedEinst, the first counterfactual\nbenchmark exposing the Einstellung Effect in med-\nical LLMs, revealing that frontier models achieve\nhigh baseline accuracy yet remain severely sus-\nceptible to statistical shortcuts. We proposed ECR-\n8\n"}, {"page": 9, "text": "Agent, which aligns LLM reasoning with Evidence-\nBased Medicine through structured causal infer-\nence and knowledge evolution.\nLimitations\nWhile MedEinst includes 5,383 counterfactual\npairs, it currently covers only 49 common patholo-\ngies across eight departments. Although these dis-\neases represent high-frequency diagnostic scenar-\nios in emergency medicine, they constitute a small\nfraction of the vast medical ontology (e.g., ICD-10).\nConsequently, the manifestation of the Einstellung\nEffect in rare diseases or complex comorbidities re-\nmains to be fully explored. We view MedEinst as a\nfoundational proof-of-concept, paving the way for\nfuture benchmarks to expand into broader disease\ntaxonomies.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, and 1 others. 2023. Gpt-4 techni-\ncal report. arXiv preprint arXiv:2303.08774.\nSaeid Alavi Naeini, Raeid Saqur, Mozhgan Saeidi, John\nGiorgi, and Babak Taati. 2023. Large language mod-\nels are fixated by red herrings: Exploring creative\nproblem solving and einstellung effect using the only\nconnect wall dataset. Advances in Neural Informa-\ntion Processing Systems, 36:5631–5652.\nPayal Chandak, Kexin Huang, and Marinka Zitnik.\n2023. Building a knowledge graph to enable pre-\ncision medicine. Scientific Data, 10(1):67.\nCanyu Chen, Jian Yu, Shan Chen, Che Liu, Zhong-\nwei Wan, Danielle Bitterman, Fei Wang, and Kai\nShu. 2024. Clinicalbench: Can llms beat traditional\nml models in clinical prediction?\narXiv preprint\narXiv:2411.06469.\nArsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien\nMartel, and Joumana Ghosn. 2022. Ddxplus: A new\ndataset for automatic medical diagnosis. Advances\nin neural information processing systems, 35:31306–\n31318.\nLavender Yao Jiang, Xujin Chris Liu, Nima Pour Neja-\ntian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin,\nKevin Eaton, Howard Antony Riina, Ilya Laufer,\nPaawan Punjabi, and 1 others. 2023. Health system-\nscale language models are all-purpose prediction en-\ngines. Nature, 619(7969):357–362.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Co-\nhen, and Xinghua Lu. 2019. Pubmedqa: A dataset for\nbiomedical research question answering. In Proceed-\nings of the 2019 conference on empirical methods\nin natural language processing and the 9th interna-\ntional joint conference on natural language process-\ning (EMNLP-IJCNLP), pages 2567–2577.\nAlistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin\nGayles, Ayad Shammout, Steven Horng, Tom J Pol-\nlard, Sicheng Hao, Benjamin Moody, Brian Gow, and\n1 others. 2023. Mimic-iv, a freely accessible elec-\ntronic health record dataset. Scientific data, 10(1):1.\nJonathan Kim, Anna Podlasek, Kie Shidara, Feng Liu,\nAhmed Alaa, and Danilo Bernardo. 2025. Limita-\ntions of large language models in clinical problem-\nsolving arising from inflexible reasoning. Scientific\nreports, 15(1):39426.\nYubin Kim, Chanwoo Park, Hyewon Jeong, Yik S Chan,\nXuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh\nGhassemi, Cynthia Breazeal, and Hae W Park. 2024.\nMdagents: An adaptive collaboration of llms for med-\nical decision-making. Advances in Neural Informa-\ntion Processing Systems, 37:79410–79452.\nZijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi\nYang. 2024. A dynamic llm-powered agent network\nfor task-oriented agent collaboration. In First Con-\nference on Language Modeling.\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023. Capabili-\nties of gpt-4 on medical challenge problems. arXiv\npreprint arXiv:2303.13375.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on health,\ninference, and learning, pages 248–260. PMLR.\nJudea Pearl and Dana Mackenzie. 2018. The book of\nwhy: the new science of cause and effect. Basic\nbooks.\nJonathan G Richens, Ciarán M Lee, and Saurabh Johri.\n2020. Improving the accuracy of medical diagnosis\nwith causal machine learning. Nature communica-\ntions, 11(1):3923.\nDavid L Sackett. 1997. Evidence-based medicine. Sem-\ninars in perinatology, 21(1):3–5.\nSamuel Schmidgall, Carl Harris, Ime Essien, Daniel Ol-\nshvang, Tawsifur Rahman, Ji Woong Kim, Rojin Zi-\naei, Jason Eshraghian, Peter Abadir, and Rama Chel-\nlappa. 2024a. Addressing cognitive bias in medical\nlanguage models. arXiv preprint arXiv:2402.08113.\nSamuel Schmidgall, Carl Harris, Ime Essien, Daniel\nOlshvang, Tawsifur Rahman, Ji Woong Kim, Rojin\nZiaei, Jason Eshraghian, Peter Abadir, and Rama\nChellappa. 2024b. Evaluation and mitigation of cog-\nnitive biases in medical language models. npj Digital\nMedicine, 7(1):295.\n9\n"}, {"page": 10, "text": "Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo\nReis, Jeffrey Jopling, and Michael Moor. 2024c.\nAgentclinic: a multimodal agent benchmark to eval-\nuate ai in simulated clinical environments. arXiv\npreprint arXiv:2405.07960.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\nand 1 others. 2023. Large language models encode\nclinical knowledge. Nature, 620(7972):172–180.\nXiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming\nLi, Yilun Zhao, Xingyao Zhang, Arman Cohan, and\nMark Gerstein. 2024. Medagents: Large language\nmodels as collaborators for zero-shot medical rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: ACL 2024, pages 599–621.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, and 1 others. 2023. Llama 2: Open foun-\ndation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nJunde Wu, Jiayuan Zhu, Yunli Qi, Jingkun Chen, Min\nXu, Filippo Menolascina, and Vicente Grau. 2024.\nMedical graph rag: Towards safe medical large lan-\nguage model via graph retrieval-augmented genera-\ntion. arXiv preprint arXiv:2408.04187.\nLawrence KQ Yan, Qian Niu, Ming Li, Yichao Zhang,\nCaitlyn Heqi Yin, Cheng Fei, Benji Peng, Ziqian Bi,\nPohsun Feng, Keyu Chen, and 1 others. 2024. Large\nlanguage model benchmarks in medical tasks. arXiv\npreprint arXiv:2410.21348.\n10\n"}, {"page": 11, "text": "Appendix\nAbstract. This appendix provides supplemen-\ntary materials for the MedEinst benchmark and the\nECR-Agent framework.\nAppendix A details the methodological algo-\nrithms for benchmark construction and agent in-\nference, along with the causal graph schema and\nevaluation metrics.\nAppendix B provides a comprehensive analysis\nof the MedEinst benchmark, including clinical spe-\ncialty distribution, quality assurance protocols, and\ndataset statistics.\nAppendix C outlines the implementation details,\nincluding experimental settings and baseline con-\nfigurations.\nAppendix D presents additional empirical anal-\nyses, focusing on detailed failure modes and the\ncapability-robustness gap.\nAppendix E offers a concrete case study (Case\n100473) to qualitatively demonstrate the reasoning\ntrace and interpretability of our approach.\nAppendix F extends the discussion on theo-\nretical grounding, mapping our framework to the\nCausal Hierarchy and contrasting it with existing\nparadigms.\nAppendix G displays raw data samples illustrat-\ning the input format.\nAppendix H lists the detailed prompts used for\ndata construction and the agent reasoning pipeline.\nA\nMethodological Details\nA.1\nMedEinst Construction Algorithm\nAlgorithm 1 outlines the rigorous four-stage\npipeline employed to construct the MedEinst\nbenchmark. The process begins with Data Filtering\nto select \"Hard Candidates\" where statistical short-\ncuts fail. It then proceeds to Narration Conversion\nand Differential Features Rewrite, transforming\nstructured data into natural language and injecting\nadversarial traps based on knowledge base from\nDDXPlus (Fansi Tchango et al., 2022). Finally,\nInter-Model Verification serves as a quality con-\ntrol filter, ensuring that the generated trap cases are\nmedically plausible.\nAlgorithm 1 Construction Pipeline of MedEinst\nInput: Source dataset Dsrc, Knowledge Base K (DDXPlus),\nLLM Judge Committee J ; Threshold ϵ = 0.5%.\nOutput: Paired Counterfactual Benchmark Sfinal.\n1: Initialize Sfinal ←∅\n2: for each sample (s, ygt, P) ∈Dsrc do\n3:\nStep 1: Data Filtering\n4:\nif |P(ygt) −P(ybias)| < ϵ then\n5:\nStep 2: Narration Conversion\n6:\nxc ←LLM(s)\n7:\nStep 3: Differential Features Rewrite\n8:\nRetrieve Kgt, Kbias ←Query(K, {ygt, ybias})\n9:\nkgt ←LLM(xc, Kgt, Kbias)\n10:\nktrap ←LLM(Kbias, kgt)\n11:\nxt ←LLM(xc, ktrap, kgt)\n12:\nStep 4: Inter-Model Verification\n13:\nVscore ←P\nj∈J I(LLMj(xt, ybias) = Correct)\n14:\nif Vscore ≥2 then\n15:\nSfinal ←Sfinal ∪{(xc, xt, ygt, ybias)}\n16:\nend if\n17:\nend if\n18: end for\n19: return Sfinal\nA.2\nECR-Agent Inference Algorithm\nAlgorithm 2 formally describes the complete work-\nflow of the ECR-Agent, integrating both the train-\ning and inference phases. The algorithm first de-\ntails the Critic-Driven Graph & Memory Evolu-\ntion (CGME), where the system iteratively refines\nillness graphs and accumulates an exemplar base\nusing critic feedback on the training set. Subse-\nquently, it presents the Dynamic Causal Inference\n(DCI) pipeline used during inference, which orches-\ntrates Dual-Pathway Perception, Dynamic Causal\nGraph Reasoning (across initialization, forward,\nand backward steps), and the final Evidence Audit\nto derive robust diagnoses for unseen cases.\nA.3\nCausal Reasoning Graph\nGraph Schema Definition:\n• Patient Nodes(VP ): Encode structured clini-\ncal observations extracted from problem repre-\nsentation. Crucially, we distinguish node sta-\ntus s(p) into three states: Present (affirmed),\nAbsent (negated), Missing (unmentioned).\n• Knowledge Nodes(VK):\nEncode disease-\nspecific clinical entities (e.g., symptoms,\nbiomarkers) distilled from literature. They\nare categorized into General (typical features)\nand Pivot (discriminators).\nMerge-or-Prune operation\nAction(pscript) =\n(\nMerge,\nif cos(epscript, epobs) > τ\nPrune,\notherwise\n(2)\n11\n"}, {"page": 12, "text": "where τ = 0.9. This ensures Gcurr only contains the patient’s\nactual data while inheriting relevant causal structures from the\nillness graphs.\nAlgorithm 2 ECR-Agent Evolution & Inference\nPipeline\nInput: Training Set Dtrain; New Case xnew\nOutput: Refined Illness Graphs Grefined; Exemplar Base\nM; Diagnosis d⋆; Causal Reasoning Graph G(d⋆)\nill\n1: /* Critic-Driven Graph & Memory Evolution */\n2: for sample (x, ygt) ∈Dtrain do\n3:\nt ←0\n4:\nwhile t < 3 do\n5:\n(dpred, Gsummary) ←DCI_Pipeline(x)\n6:\nif dpred == ygt then\n7:\nLoad Previous Graph Gprev for ygt\n8:\nGmerged ←Merge(Gprev, Gsummary)\n9:\nbreak\n10:\nelse\n11:\nApplyCriticFeedback(x)\n12:\nt ←t + 1\n13:\nend if\n14:\nend while\n15:\n▷If loop ends without success, sample is discarded.\n16: end for\n17: /* Dynamic Causal Inference (DCI) Pipeline */\n18: function DCI_PIPELINE(x)\n19:\nDual-Pathway Perception\n20:\nDset ←IntuitivePathway(x)\n21:\nPobs ←AnalyticPathway(x)\n22:\nDynamic Causal Graph Reasoning\n23:\nfor candidate d ∈Dtop do\n24:\nLoad G(d)\nill = (Vp, Vk, E)\n25:\nStep 1: Causal Graph Initialization\n26:\nVinit ←{p ∈Vp | Sim(p, Pobs) > τ}\n27:\nGill ←Initialize(Vinit, E)\n28:\nStep 2: Forward Causal Reasoning\n29:\nVk ←LiveSearch(d)\n30:\n▷Expand Pivot/General Nodes\n31:\nG\n′\nill ←Gill ∪Link(Vd, Vk) ∪Link(Vk, Vp)\n32:\nStep 3: Backward Causal Reasoning\n33:\n∆miss\n←\n{k\n∈\nV (d)\nk\n|\nk\n/∈\nPobs ∧\nIsExpected(k)}\n34:\nN (d)\nshadow ←∅\n35:\nfor k ∈∆miss do\n36:\nif ReExamine(x, k) == Found then\n37:\nPobs ←Pobs ∪{k}; Update G†\nill\n38:\nelse\n39:\nN (d)\nshadow ←N (d)\nshadow ∪{k}\n40:\n▷Create Shadow Node\n41:\nend if\n42:\nend for\n43:\n▷This G†\nill serves as the \"Graph Summary\"\n44:\nScore(d) ←CalculateScore(G†\nill, N (d)\nshadow)\n45:\nend for\n46:\nEvidence Audit\n47:\nMsim ←RetrieveExemplars(M, Pobs)\n48:\nd⋆←LLM_Judge(Dtop, {Score(d)}, Msim)\n49:\nreturn (d⋆, G(d⋆)\nill )\n50: end function\nA.4\nEvaluation Metrics\nTo precisely quantify the Einstellung Effect, we classify the\nmodel’s predictions on paired samples (xc, xt) into three\ncategories based on the intersection of their outcomes. Let\nScorrect_control denote the set of samples where the model\ncorrectly diagnoses the Control Case (f(xc) = ygt). We\ndefine the following metrics:\n• Baseline Accuracy (Accbase): Measures the fundamen-\ntal diagnostic capability on standard clinical presenta-\ntions.\nAccbase = |Scorrect_control|\nNtotal\n(3)\n• Robust Accuracy (Accrob): Measures the proportion\nof pairs where the model maintains correctness across\nboth control and trap cases (Robust Success).\nAccrob =\nPN\ni=1 I(f(xc\ni) = ygt ∧f(xt\ni) = ybias)\nNtotal\n(4)\n• Bias Trap Rate (Rbias): The core metric for the Ein-\nstellung Effect. It measures the conditional probability\nof fall in the trap given that the model possesses the\nfundamental diagnostic capability.\nRbias =\nP\ni∈Scorrect_control I(f(xt\ni) = ygt)\n|Scorrect_control|\n(5)\nB\nMedEinst Benchmark Details\nB.1\nClinical Specialty Analysis\nTo assess the clinical breadth and diversity of the MedEinst\nBenchmark, we categorized the 49 target pathologies into 10\ndistinct clinical specialties. Unlike rigid anatomical classifi-\ncations (e.g., ICD-10), we adopted a clinical taxonomy based\non medical specialties and triage departments. This approach\nbetter reflects real-world diagnostic workflows where patholo-\ngies presenting with overlapping symptoms are managed by\nspecific domains.\nFigure 6: Distribution of MedEinst Benchmark Pairs\nby Clinical Specialty. The 5,383 test pairs are grouped\ninto 10 categories based on standard clinical taxonomy.\nThe high representation of Pulmonary and Cardiology\ncases reflects the dataset’s focus on acute care scenarios\nwhere differential diagnosis is most critical.\nB.2\nQuality Assurance\nTo verify that our Differential Features Rewrite (Method\n§3.1.2) does not degrade the linguistic or clinical qual-\nity\nof\nthe\npatient\nnarratives,\nwe\nanalyzed\nthe\ndis-\ntribution\nof\nMedical\nPlausibility\nand\nNarrative\nFlu-\nency scores assigned by the judge committee J\n=\n{GPT-5, DeepSeek-R1, Gemini-2.5-Pro}.\nFigure 7 presents the comparative analysis between GOOD\nCases (successfully generated traps that passed verification)\nand BAD Cases (rejected traps).\n12\n"}, {"page": 13, "text": "• Medical Plausibility: The GOOD cases (green box-\nplots) maintain a high median score (≈8.0/10), statis-\ntically indistinguishable from the original clinical notes.\nThis confirms that the injected trap_info aligns logically\nwith the patient’s context (e.g., age, gender, symptoms,\nantecedents).\n• Narrative Fluency: The rewriting process preserves\nthe natural flow of the text, with GOOD cases achieving\na median fluency score of ≈8.3/10. In contrast, BAD\ncases often exhibit disjointed insertions or grammatical\ninconsistencies, justifying their exclusion.\nThis quality audit confirms that the Einstellung Effect\nobserved in our benchmark stems from the model’s inability\nto process conflicting evidence, rather than poor data quality.\nFigure 7: Distribution of quality metrics (Medical Plau-\nsibility and Narrative Fluency) for accepted (GOOD)\nversus rejected (BAD) trap cases. The high scores of\naccepted cases validate the effectiveness of our isomor-\nphic rewriting protocol.\nB.3\nDataset Statistics\nWe constructed MedEinst based on the DDXPlus dataset,\nstrictly adhering to its original chronological split to prevent\ndata leakage. The benchmark comprises two subsets:\n• Test Set (The Benchmark): Derived from the DDX-\nPlus test split, this set contains 5,383 counterfactual\npairs of clinical narratives (totaling 10,766 cases) cover-\ning 49 pathologies. A unique feature of MedEinst is its\nPaired Counterfactual design: each Control Case (xc)\nis paired with a Trap Case (xt) that differs only in Key\nDiscriminative Features, yet leads to a contradictory\ndiagnosis (ygt vs. ybias). This design strictly decouples\na model’s statistical intuition from its logical reasoning\ncapability.\n• Reference Set (Training Resource): Derived from\nthe DDXPlus training split, we processed and verified\n10,689 pairs. This large-scale set is provided to support\nvarious research paradigms, including fine-tuning, few-\nshot learning, or RAG-based retrieval.\nSelection Criteria. A sample pair is included in the final\nMedEinst benchmark Sfinal if and only if it receives a posi-\ntive vote on Diagnostic Correctness from at least two judges.\nAs shown in Appendix A, the selected trap cases maintain high\nmedical plausibility and narrative fluency comparable to con-\ntrol cases. This rigorous verification ensures that performance\ndrops in MedEinst stem from reasoning failures (Einstellung\nEffect) rather than textual artifacts or data noise.\nC\nImplementation Details\nTo simulate a realistic clinical diagnosis scenario where physi-\ncians encounter unseen cases, all baseline models and agent\nframeworks operate under a Zero-shot Chain-of-Thought\n(CoT) setting. For our ECR-Agent, we maintain the same\nzero-shot input for fair comparison. Specifically, in the Dual-\nPathway Perception phase, we configure the agent to generate\nthe Top-k candidate diagnoses with k = 5. This thresh-\nold was empirically selected to ensure sufficient coverage of\npotential differentials (including the ground truth and trap)\nwhile maintaining computational efficiency for the subsequent\ncausal graph construction. Evidence Expansion is supported\nby structured queries to OpenTargets and PubMed APIs, func-\ntioning as an extension of the agent’s analytic system.\nWe evaluate all methods on the MedEinst benchmark\n(5,383 pairs). To drive the Critic-Driven Graph & Memory\nEvolution, we utilized the MedEinst-Support set. To demon-\nstrate the data efficiency of our framework, we did not employ\nthe full support set. Instead, we curated a compact Balanced\nSeed Subset consisting of only 853 cases (approximately 8%\nof the available training data).This subset was constructed\nusing a Capped Sampling Strategy: we randomly sampled a\nmaximum of N = 20 cases per pathology, while retaining\nall available samples for rare diseases. This lightweight selec-\ntion ensures that the agent can initialize robust Illness Graphs\nand the Exemplar Base with minimal data consumption, high-\nlighting the framework’s capability to generalize from sparse\nbut balanced clinical examples.\nD\nAdditional Experimental Analysis\nTo investigate the microscopic mechanisms and macroscopic\ncharacteristics of the Einstellung Effect, we conduct a multi-\ndimensional empirical analysis.\nD.1\nDetailed Failure Mode Analysis\nTo understand the cognitive failures behind Einstellung Traps,\nwe conducted a fine-grained failure analysis on three represen-\ntative models (DeepSeek-R1, GPT-5, QwQ-32B). We classify\nreasoning failures into three modes based on the model’s in-\nteraction with the Key Discriminative Evidence. The classi-\nfication was performed by a GPT-5 Auditor and verified by\nhuman experts on a subset of data (Cohen’s κ > 0.8).\nAs shown in Figure 1, the distribution reveals distinct cog-\nnitive deficits:\n• Blindness Models completely fail to mention the key\nevidence in their CoT. This suggests that strong statisti-\ncal priors filter out \"unexpected\" symptoms during the\ninitial perception stage. Our Solution (Dual-Pathway\nPerception): We introduce Dual-Track Perception,\nforcing the explicit extraction of a structured Problem\nRepresentation to ensure all evidence is \"seen\".\n• Underthinking Even when evidence is seen, models of-\nten default to the most likely candidate without rigorous\nfalsification. Our Solution (Causal Graph Reasoning):\nWe implement Causal Graph Reasoning. By con-\nstructing a patient-specific graph with Pivot Nodes, we\nstructurally force bidirectional reasoning (Forward Sup-\nport & Backward Exclusion) to prevent the dismissal of\ncontradictory evidence.\n• Overthinking Advanced models (e.g., GPT-5) engage\nin Motivated Reasoning, hallucinating mechanisms\nto force-fit contradictions into the incorrect diagnosis.\nOur Solution (Evidence Audit): We deploy an Evidence\nAudit. By performing Counterfactual Checks, the agent\ndetects and penalizes such non-causal rationalizations,\nbreaking the self-confirming loop.\n13\n"}, {"page": 14, "text": "D.2\nOverall Performance Comparison\nTable 1 presents the performance of various models and agent\nframeworks on MedEinst. We observe three critical phenom-\nena:\n1. The Capability-Robustness Gap.\nWhile frontier\nmodels like GPT-5 and Gemini-2.5-Pro demonstrate supe-\nrior fundamental diagnostic capabilities (Accbase of 54.30%\nand 53.58% respectively), their robustness remains dispro-\nportionately low, with Accrob hovering around 10%–15%.\nAlarmingly, these stronger models often exhibit higher suscep-\ntibility to Einstellung traps (Rbias 51%–61%). For instance,\nGemini-2.5-Pro, despite its high capability, shows a signifi-\ncantly higher bias rate (60.90%) compared to Claude-Sonnet-\n4.5 (42.98%). This implies that in adversarial contexts, high\ncapability can paradoxically increase vulnerability to bias.\nThis result reveals a counter-intuitive conclusion: current\nScaling Laws enhance \"statistical fitting\" but fail to confer\n\"differential diagnostic capability in dynamic contexts\".As\ncorroborated by our failure mode analysis (Figure 1), highly\ncapable models like GPT-5 exhibit a disproportionately high\nrate of Blindness.This suggests that stronger models fit the\nprior distribution of training data so aggressively that they\nliterally filter out low-probability counter-evidence during per-\nception, making it structurally harder to escape the Einstellung\nEffect.\n2.\nExisting Agents Amplify Cognitive Bias.\nCompared to the base model (Qwen3-32B), the multi-agent\nframework MDAgent does not yield the expected improve-\nments and even exhibits degradation. We attribute this to two\nfactors: (1) Noise Amplification: The significant drop in\nAccbase (40.26% →29.70%) suggests that without causal\nconstraints, the diverse viewpoints introduced by multi-agent\ndebate act as noise rather than signal. (2) Bias Amplifica-\ntion: The stagnation in Accrob and high Rbias indicate that\nthe \"debate\" mechanism, when faced with strong Einstellung\ntraps, devolves into Consensus Bias, reinforcing the incorrect\nintuitive consensus rather than correcting it.\n3. Effectiveness of Evidence-Based Architecture.\nIn contrast, ECR-Agent (based on Qwen3-32B) achieves a\nqualitative leap in performance. It significantly boosts fun-\ndamental capability (Accbase →69.49%) while doubling\nrobustness (Accrob →24.21%) and reducing the bias rate\n(Rbias →33.75%). This demonstrates that introducing\nStructural Causal Reasoning and Evidence Audit mecha-\nnisms is key to breaking the Einstellung Effect. Unlike base-\nlines that rely on internal parametric memory, ECR-Agent\nenforces an evidence-based reasoning process that prioritizes\n\"evidence\" over \"probability,\" effectively circumventing the\nEinstellung Traps.\nD.3\nImpact of Scale and Pathology\nScaling Ineffectiveness.\nFigure 5 visualizes the rela-\ntionship between Rbias and Accbase. The results show no\nsignificant linear negative correlation, with data points widely\nscattered. Frontier models like GPT-5, despite possessing\nextreme fundamental capability (right side of X-axis), still\nexhibit very high bias rates (top of Y-axis). This indicates that\nreasoning robustness does not emerge naturally from scale.\nWithout a structured verification mechanism, even advanced\nCoT reasoning remains susceptible to being trapped in the\nEinstellung Effect by strong statistical priors.\nPathology-Dependent Vulnerability.\nThe cluster-\ning patterns in Figure 5 and the heatmap in Figure 4 reveal the\nstructural nature of the Einstellung Effect:\n• Clustering: Pathologies like Pneumonia and Pericardi-\ntis consistently appear in the High Bias Cluster across\nalmost all models. This reveals strong Spurious Corre-\nlations in the training data.\n• Variance: Conversely, pathologies like Influenza show\nhigh variance, suggesting that when statistical priors are\nweaker, some models can successfully reason through\ndistractors.\nThis pathology dependence confirms the systemic vulnera-\nbility of probabilistic models when facing \"High-Confidence\nPrior vs. Low-Confidence Evidence\" conflicts. ECR-Agent\nsucceeds by transforming the \"probability prediction prob-\nlem\" into an \"evidence verification problem\" via Causal In-\ntervention, structurally blocking the propagation of spurious\ncorrelations.\nE\nCase Study\nTo demonstrate the efficacy of MEDEINST in benchmarking\nthe Einstellung Effect and the robustness of ECR-AGENT,\nwe present a detailed analysis of Case 100473. This case\nrepresents a high-stakes emergency scenario where the base-\nline model succumbed to a \"Pattern Matching\" trap, while\nour agent successfully corrected the diagnosis through causal\ngraph reasoning.\nE.1\nCase Overview\n• Ground Truth: Pulmonary Embolism (PE).\n• Trap Type: Distractor Injection (Family History of\nPneumothorax) + Evidence Substitution (History of\nDVT).\n• Baseline Intuition: Spontaneous Pneumothorax.\n• ECR-Agent Verdict: Overturn →Pulmonary Em-\nbolism.\nE.2\nNarrative Comparison\nTable A1 illustrates the minimal yet critical differences be-\ntween the Control and Trap cases. The Trap case introduces a\nstrong \"Red Herring\" (Family History) while subtly embed-\nding the key discriminative evidence (DVT History).\nE.3\nReasoning Trace Analysis\nThe baseline model (intuition) anchored on the \"Young Male\n+ Sudden Chest Pain + Family History\" pattern, incorrectly\ndiagnosing Spontaneous Pneumothorax. Below is the recon-\nstructed audit log from the ECR-AGENT’s Evidence Audit,\ndemonstrating how it utilized the Causal Graph to overturn\nthis error.\nEvidence Audit Log:\nInitial Hypothesis: Spontaneous Pneumothorax\n(Probability: High).\nTier 1: Fatal Conflict Check\n- Check: Pneumothorax typically requires specific\nimaging confirmation (e.g., \"Spontaneous pneu-\nmothorax on imaging\").\n- Result: Missing/Shadow Node. The narrative\ndoes not mention imaging evidence.\nTier 2: Pivot Competition\n- Pivot 1 (DVT History): Strongly supports Pul-\nmonary Embolism (Risk Factor). Matched in\nPatient Narrative.\n- Pivot 2 (Family History): Supports Pneumotho-\nrax, but is a weak \"General\" feature compared to\nthe specific risk factor of DVT.\n- Pivot 3 (Hypoxia + Tachycardia): Supports both,\n14\n"}, {"page": 15, "text": "Feature\nControl Case (xc)\nTrap Case (xt)\nDemographics\nMale, 22 years old\nMale, 22 years old\nChief Complaint\nSudden \"knife-like\" chest pain, Dyspnea\nSudden \"knife-like\" chest pain, Dyspnea\nKey Evidence\nHistory of Spontaneous Pneumothorax\nHistory of Deep Vein Thrombosis (DVT)\nDistractors\nFamily history of Pneumothorax\nFamily history of Pneumothorax\nAssociated Sx\nTachycardia, Hypoxia\nTachycardia, Hypoxia\nModel Diagnosis\nSpontaneous Pneumothorax (✓)\nSpontaneous Pneumothorax (Error)\nTable A1: Comparison of the Control and Trap narratives. The Trap Case replaces the patient’s personal history\nwith DVT (a risk factor for PE) but retains the family history of Pneumothorax, triggering the Einstellung Effect in\nbaseline models.\nbut biologically more severe in PE.\nDecision: The presence of \"History of DVT\" is\na distinct Pivot Node that rules out Pneumotho-\nrax (as a primary cause) and strongly supports\nPE. The initial intuition was biased by the family\nhistory.\nFinal Verdict: OVERTURN →Pulmonary Em-\nbolism.\nE.4\nInterpretability: Evidence Balance Sheet\nThe core of ECR-AGENT’s interpretability lies in its explicit\nCausal Graph. Table A2 details the \"Evidence Balance\nSheet\" for Case 100473.\nThe agent constructs a graph connecting the Patient Ob-\nservations (Pobs) to the Knowledge Nodes (Knodes) of\ncompeting diagnoses.\nThe decision is driven by Pivot\nNodes—features that logically distinguish between the two\nconditions.\nF\nExtended Discussion: Theoretical\nGrounding and Comparative Analysis\nWhile the main text outlines the broad landscape of med-\nical LLMs, this appendix provides a deeper theoretical\nanalysis of why existing paradigms—specifically Multi-\nAgent Collaboration and Retrieval-Augmented Generation\n(RAG)—insufficiently address the Einstellung Effect, and\nhow our ECR-AGENT fundamentally differs by aligning with\nCausal Inference theories.\nF.1\nVerification vs. Consensus: The Limits of\nMulti-Agent Debate\nRecent agentic frameworks like MDAgents (Kim et al., 2024)\nand MedAgents (Tang et al., 2024) rely on \"collaboration\" or\n\"debate\" strategies, assuming that diverse personas will cancel\nout individual errors. However, this assumption holds only\nwhen errors are independent and randomly distributed.\nIn the context of the Einstellung Effect, errors are not\nrandom but systematic. As shown in our experiments (Table\n1), strong statistical priors act as a \"common distractor\" that\nmisleads the majority of models/agents similarly.\n• Consensus Bias: When the \"intuitive but wrong\" di-\nagnosis is statistically dominant, multi-agent debate of-\nten devolves into Consensus Bias (Schmidgall et al.,\n2024a,b). Agents tend to converge on the most likely\nprobabilistic token rather than the ground truth evidence.\n• Our Solution (Veto by Evidence): Unlike debate\nframeworks that optimize for agreement, ECR-AGENT\noptimizes for falsification. By introducing Pivot Nodes\n(Section 4.2.2), our agent grants a single piece of dis-\ncriminative evidence the power to \"veto\" the majority\nconsensus, mirroring the clinical principle that \"one\nproven contradiction outweighs a thousand probabili-\nties\".\nF.2\nDynamic Inference vs. Static Knowledge:\nThe Limits of RAG\nRetrieval-Augmented Generation (RAG) systems, such as\nMedGraphRAG (Wu et al., 2024) and PrimeKG (Chandak\net al., 2023), attempt to mitigate hallucinations by retriev-\ning external knowledge. While effective for factual queries,\nstandard RAG faces structural limitations in Counterfactual\nDifferential Diagnosis:\n• Static vs. Dynamic: RAG retrieves static associa-\ntions (e.g., \"Pulmonary Embolism causes Chest Pain\")\nbut lacks the mechanism to construct a patient-specific\ncausal graph. It cannot dynamically evaluate \"What\nif this specific symptom was absent?\" or \"Why is this\noverlapping symptom non-discriminative in this specific\ncontext?\".\n• Associative vs. Causal: RAG fundamentally enhances\nAssociative Reasoning (Pearl’s Layer 1) by adding more\ncontext to the prompt. It does not perform Intervention\n(Layer 2).\n• Our Solution: ECR-AGENT does not just retrieve\nknowledge; it structures it into a Dynamic Causal\nGraph. By explicitly modeling Match, Conflict, and\nShadow relations, we transform static knowledge into\nactive reasoning tools that can perform logical interven-\ntions on the patient’s narrative.\nF.3\nTheoretical Grounding: Mapping\nDiagnosis to the Causal Hierarchy\nOur framework is theoretically grounded in the integration of\nEvidence-Based Medicine (EBM) (Sackett, 1997) with Pearl’s\nCausal Hierarchy (Pearl and Mackenzie, 2018). We provide a\nformal mapping of these cognitive processes:\n1. Layer 1: Association. Clinical Equivalent: Pattern\nRecognition / Intuition. Implementation: Our Dual-\nPathway Perception module generates initial hypothe-\nses based on P (Diagnosis|Symptoms). This is\nwhere the Einstellung Effect (statistical bias) originates.\n2. Layer 2: Intervention. Clinical Equivalent: Differen-\ntial Diagnosis / Testing. Implementation: Our Forward\nCausal Reasoning simulates the act of \"intervening\"\nto find truth. We define Pivot Nodes as the minimal\nintervention set do(X) required to distinguish between\ncompeting hypotheses di and dj. This aligns with\nRichens et al. (2020), who proved that optimal diagnosis\nrequires maximizing the Information Gain of interven-\ntions.\n15\n"}, {"page": 16, "text": "Diagnosis Candidate\nNode Type\nRelation to Patient\nClinical Feature Content\nDiagnosis A: Pulmonary Embolism (Correct)\nPivot\nMatch\nHistory of Deep Vein Thrombosis (DVT)\nPivot\nMatch\nSudden onset dyspnea with tachycardia & hypoxia\nPivot\nMatch\nSharp pleuritic chest pain exacerbated by inspiration\nGeneral\nMatch\nDyspnea with sudden onset\nDiagnosis B: Spontaneous Pneumothorax (Intuition/Error)\nPivot\nMissing\nSpontaneous pneumothorax on imaging\nGeneral\nMatch\nHistory of spontaneous pneumothorax & family history\nPivot\nMatch\nAcute onset pleuritic chest pain\nPivot\nRule Out\nPresence of prior Deep Vein Thrombosis (DVT)\nTable A2: Evidence Balance Sheet. The table shows why the agent favored PE over Pneumothorax. While\nPneumothorax has matching symptoms (chest pain), it lacks its critical Pivot evidence (Imaging) and is actively\nruled out by the presence of DVT, which is a Pivot Match for PE.\n3. Layer 3: Counterfactuals. Clinical Equivalent: Diag-\nnostic Verification / Audit. Implementation: Our Back-\nward Causal Reasoning and Evidence Audit perform\nthe counterfactual check: \"Given diagnosis d, what\nsymptom s would have been observed?\". The detection\nof Shadow Nodes (missing expected evidence) formally\nrepresents the violation of counterfactual expectations\n(P (smissing|do(d)) ≈0), allowing the model to\nreject high-probability but causally inconsistent traps.\nThis rigorous mapping demonstrates that ECR-AGENT is\nnot merely an engineering improvement but a step towards\nCausal AI in medicine, moving beyond the Curve Fitting\nlimitations of standard LLMs (Richens et al., 2020).\nG\nData Samples\nTo demonstrate the realistic clinical presentation of MEDE-\nINST, Figure 8 displays the raw input narratives for Case\n100473 as they appear to the model.\nWe adopt the structured format from the DDXPlus dataset,\nwhich organizes clinical observations into Symptoms and\nAntecedents with hierarchical indentation. The figure high-\nlights the counterfactual intervention: while the lengthy\nsymptom description and the \"Family history\" distractor re-\nmain identical, the specific patient history in the Antecedents\nsection is surgically altered from \"Spontaneous pneumotho-\nrax\" (Control) to \"Deep vein thrombosis\" (Trap).\nH\nPrompts Details\nTo ensure the reproducibility of our work, we provide the full\nsystem prompts used in both the MEDEINST benchmark con-\nstruction pipeline and the ECR-AGENT reasoning framework.\nH.1\nMedEinst Benchmark Construction\nTables A3, A4, A5, and A6 detail the prompts for the four-\nstage adversarial data construction pipeline.\nH.2\nECR-Agent Reasoning Framework\nTables A7, A8, and A9 detail the prompts for the three-phase\ncausal reasoning engine.\n16\n"}, {"page": 17, "text": "Control Case (xc): Spontaneous Pneumothorax\nSex: M, Age: 22\nGeographical region: North America\nSymptoms:\n---------\n- I feel pain.\n- The pain is:\n* heartbreaking\n* a knife stroke\n- The pain locations are:\n* side of the chest(R)\n* breast(R)\n* breast(L)\n- On a scale of 0-10,\nthe pain intensity is 6\n- The pain radiates to\nthese locations:\n* nowhere\n- On a scale of 0-10,\nthe location precision is 2\n- On a scale of 0-10,\nthe speed of onset is 9\n- I am experiencing shortness\nof breath or\ndifficulty breathing in a significant way.\n- I have pain that is increased\nwhen I breathe\nin deeply.\n- I have tachycardia.\n- I have hypoxia.\nAntecedents:\n------------\n- I have had a spontaneous pneumothorax.\n- I smoke cigarettes.\n- One or more of my family members have had\na pneumothorax.\n- I have not traveled out of the country in\nthe last 4 weeks.\nTrap Case (xt): Pulmonary Embolism\nSex: M, Age: 22\nGeographical region: North America\nSymptoms:\n---------\n- I feel pain.\n- The pain is:\n* heartbreaking\n* a knife stroke\n- The pain locations are:\n* side of the chest(R)\n* breast(R)\n* breast(L)\n- On a scale of 0-10,\nthe pain intensity is 6\n- The pain radiates to\nthese locations:\n* nowhere\n- On a scale of 0-10,\nthe location precision is 2\n- On a scale of 0-10,\nthe speed of onset is 9\n- I am experiencing shortness\nof breath or\ndifficulty breathing in a significant way.\n- I have pain that is increased\nwhen I breathe\nin deeply.\n- I have tachycardia.\n- I have hypoxia.\nAntecedents:\n------------\n- I have had a deep vein thrombosis (DVT). <!!>\n- I smoke cigarettes.\n- One or more of my family members have had\na pneumothorax.\n- I have not traveled out of the country in\nthe last 4 weeks.\nFigure 8: Side-by-side comparison of the raw clinical narratives for Case 100473. The text is presented in the\noriginal DDXPlus format used as input for the LLMs. The Trap Case (Right) contains a minimal edit in the\nAntecedents section (marked with <!!>), replacing the history of pneumothorax with DVT, while retaining the\nmisleading family history.\n17\n"}, {"page": 18, "text": "Discriminative Feature Extraction Prompt (Step 1)\nSystem Prompt: You are a senior medical expert. Your task is to perform a differential diagnosis based on the provided\nreference knowledge.\nCRITICAL INSTRUCTION: You MUST use the ’Reference Knowledge’ below as your ONLY source of truth. Do not use\nyour own internal knowledge.\nReference Knowledge:\n- Typical Symptoms of ’distractor_disease’: distractor_symptoms\n- Typical Symptoms of ’truth_disease’: truth_symptoms\nPatient Narrative: — control_narrative —\nYour Task:\n1. Compare the patient’s narrative with the two lists of typical symptoms.\n2. Identify the SINGLE MOST CRITICAL and ATOMIC phrase within the narrative that is a typical symptom of ’distrac-\ntor_disease’ but NOT a typical symptom of ’truth_disease’.\nResponse Format: If you successfully identify such a phrase, respond in JSON format with one key: {\"narrative_A\": \"the\nexact phrase from the narrative\"}.\nTable A3: Prompt used to extract the key discriminative evidence (kgt) that supports the control diagnosis.\nTrap Information Generation Prompt (Step 2)\nSystem Prompt: You are a medical writer. Your task is to generate an ’isomorphic’ clinical finding, grounded in the provided\nreference knowledge.\nCRITICAL INSTRUCTION: You MUST choose an evidence from the ’Reference Knowledge for truth_disease’ list below.\nReference Knowledge for truth_disease: ...\nThe original phrase (pointing to distractor_disease): \"narrative_A\"\nYour Task:\n1. Review the list of typical evidences for ’truth_disease’.\n2. Select one evidence from those lists that is most ’isomorphic’ to the original phrase in terms of clinical gravity and role.\n3. Rephrase the selected evidence into a concise, ATOMIC phrase a patient would say.\nRespond in JSON format with one key: \"narrative_B\".\nTable A4: Prompt used to generate the misleading trap feature (ktrap) based on the bias disease knowledge.\nDifferential Features Rewrite Prompt (Step 3)\nSystem Prompt: You are an expert medical writer performing a \"Cognitive Surgery\". Your task is to seamlessly rewrite a\nclinical narrative to change its diagnostic direction.\nOriginal Patient Narrative: — control_narrative —\nYour instructions:\n1. Locate the phrase \"narrative_A\" in the text.\n2. Rewrite the single sentence containing this phrase to instead convey the new core information: \"narrative_B\".\n3. Crucially, you MUST ensure the new sentence is grammatically perfect and logically fits the surrounding context.\nThe style and tone must match the original exactly.\n4. Do not change any other part of the narrative.\nRespond with only the complete, rewritten patient narrative text.\nTable A5: Prompt used to inject the trap feature into the patient narrative (xc →xt).\nInter-Model Verification Prompt (Step 4)\nSystem Prompt: You are an expert clinical diagnostician and medical narrative analyst, acting as an impartial judge. Your\ntask is to assess the quality of a synthetically modified \"Trap Case\".\nYour evaluation must be based on three precise criteria:\n1. Diagnostic Correctness (Boolean): When reading the trap case narrative independently, does it provide sufficient evidence\nto make its stated ground truth (’trap_gt’) a plausible and likely diagnosis?\n2. Medical Plausibility (1-10 Scale): How medically believable is the complete trap case narrative?\n3. Narrative Fluency (1-10 Scale): How well was the new information integrated?\n...\nTable A6: LLM-as-a-Judge prompt for verifying the quality and validity of generated trap cases.\n18\n"}, {"page": 19, "text": "Analytic Problem Representation Prompt (Dual-Pathway Perception)\nSystem Prompt: You are a Senior Clinical Diagnostician and Expert Medical Scribe. Your task is to perform \"Problem\nRepresentation\" on a raw patient case.\nOBJECTIVE: Transform the patient’s raw narrative into a structured list of P-Nodes (Patient Features) using precise\nMedical Semantic Qualifiers.\nTHE PROCESS:\n1. Translate Time ... 2. Translate Symptoms ... 3. Filter ... 4. Synthesize ...\nOUTPUT SCHEMA (JSON):\nReturn a single JSON object with two keys: ‘problem_representation_one_liner‘ and ‘p_nodes‘ (containing id, content,\noriginal_text, status).\nRULES: Only mark status: \"Absent\" if the text explicitly says \"no\", \"denies\", or \"without\".\nTable A7: Prompt for extracting structured patient observations (Pobs) from raw text.\nPivot Node Discovery Prompt (Causal Graph Reasoning)\nSystem Prompt: You are an Expert Diagnostician performing a comprehensive Differential Diagnosis.\nStep 1: Disease-by-Disease Analysis ...\nStep 2: Cross-Disease Comparison (Matrix Analysis)\nCreate a mental discrimination matrix: Which features are UNIQUE to one disease? Which features RULE OUT certain\ndiseases?\nOUTPUT JSON SCHEMA:\nYou MUST output a JSON object with: \"k_nodes\": [ { \"content\": \"...\", \"type\": \"Pivot\", \"importance\": \"Pathognomonic\",\n\"supported_candidates\": [...], \"ruled_out_candidates\": [...] } ]\nFIELD DEFINITIONS:\n- Pivot: Discriminating feature that helps distinguish between 2+ diseases.\nTable A8: Prompt for identifying Pivot Nodes to differentiate between competing hypotheses.\nEvidence Audit & Final Decision Prompt (Evidence Audit)\nSystem Prompt: You are the Chief Medical Auditor and Final Decision Maker. Your goal is to audit the reasoning of a\n\"System 1\" (Initial Intuition) agent using a \"System 2\" (Causal Graph) evidence map.\nTHE LOGIC HIERARCHY (Follow Strictly)\nTier 1: The Safety Sentinel (Fatal Conflicts)\nRule: If a Candidate requires a symptom that is ‘Essential‘, but the Patient explicitly has ‘Status: Absent‘, then this Candidate\nis DISQUALIFIED.\nTier 2: The Pivot Competition (Differential Diagnosis)\nRule: A Candidate supported by a matched Pivot Feature is superior to a Candidate supported only by General features.\nTier 3: The Shadow & Coverage Audit (Tie-Breaker)\nSelect the candidate with the highest explanatory coverage and fewest unexplained conflicts.\nTable A9: Prompt for the final evidence audit, applying the Tiered Logic Hierarchy to select the diagnosis.\n19\n"}]}