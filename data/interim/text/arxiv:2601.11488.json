{"doc_id": "arxiv:2601.11488", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.11488.pdf", "meta": {"doc_id": "arxiv:2601.11488", "source": "arxiv", "arxiv_id": "2601.11488", "title": "CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation", "authors": ["Vanshali Sharma", "Andrea Mia Bejar", "Gorkem Durak", "Ulas Bagci"], "published": "2026-01-16T18:09:19Z", "updated": "2026-01-16T18:09:19Z", "summary": "In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 \"disagreement\" cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.11488v1", "url_pdf": "https://arxiv.org/pdf/2601.11488.pdf", "meta_path": "data/raw/arxiv/meta/2601.11488.json", "sha256": "f4d13bd8e6eac52c881cbe92f315a62dea6a5e8b463065ad2c2069dd76567e9c", "status": "ok", "fetched_at": "2026-02-18T02:21:23.400557+00:00"}, "pages": [{"page": 1, "text": "CTEST-METRIC: A UNIFIED FRAMEWORK TO ASSESS CLINICAL VALIDITY OF\nMETRICS FOR CT REPORT GENERATION\nVanshali Sharma, Andrea Mia Bejar, Gorkem Durak, and Ulas Bagci\nMachine and Hybrid Intelligence Lab, Northwestern University, Chicago, IL, USA\nIn the generative AI era, where even critical medical\ntasks are increasingly automated, radiology report generation\n(RRG) continues to rely on suboptimal metrics for quality as-\nsessment. Developing domain-specific metrics has therefore\nbeen an active area of research, yet it remains challenging due\nto the lack of a unified, well-defined framework to assess their\nrobustness and applicability in clinical contexts. To address\nthis, we present CTest-Metric, a first unified metric assess-\nment framework with three modules determining the clinical\nfeasibility of metrics for CT RRG. The modules test: (i) Writ-\ning Style Generalizability (WSG) via LLM-based rephrasing;\n(ii) Synthetic Error Injection (SEI) at graded severities; and\n(iii) Metrics-vs-Expert correlation (MvE) using clinician rat-\nings on 175 “disagreement” cases. Eight widely used metrics\n(BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph,\nRaTEScore, GREEN Score, CRG) are studied across seven\nLLMs built on a CT-CLIP encoder. Using our novel frame-\nwork, we found that lexical NLG metrics are highly sensitive\nto stylistic variations; GREEN Score aligns best with expert\njudgments (Spearman 0.70), while CRG shows negative cor-\nrelation; and BERTScore-F1 is least sensitive to factual error\ninjection. We will release the framework, code, and allowable\nportion of the anonymized evaluation data (rephrased/error-\ninjected CT reports), to facilitate reproducible benchmarking\nand future metric development.\n1. INTRODUCTION\nWith recent breakthroughs in Large Language Models (LLMs)\nfor automated radiology report generation (RRG) [1, 2], a\ncrucial question arises: Do existing metrics capture what ac-\ntually matters to trace the clinical efficacy and acceptability\nof LLM-generated reports?\nA clinically grounded metric\nmust be insensitive to report writing styles and sensitive to\nsubtle yet critical factual mismatches. It should also identify\nsynonymous medical terminologies, and be robust enough\nto capture overfitting in AI-based generative models before\ndeploying them for clinical use.\nDespite the availability of a large number of metrics, most\nare developed for general-domain text [3, 4, 5], and even a\nfew domain-specific metrics overlook deeper clinical rele-\nThis research is partially supported by the NIH grant R01-HL171376.\nvance or ignore synonymous terminologies. Therefore, the\nexisting RRG methods [2, 6] continue to rely on metrics that\nare not reflective of clinical fidelity. Consequently, the de-\nsigning of radiology-specific metrics has been an active area\nof research with the aim of evaluating both clinical aspects\nand linguistic similarity between generated radiology reports\nand the ground truth. However, due to the absence of any\nstandardized tool and well-defined criteria to test these met-\nrics, RRG tasks still rely on inconsistent metrics, resulting in\nmisleading model selections. This underscores the need for a\nsystem that can serve as a well-defined framework for metrics\ndevelopers to assess the clinical applicability of the metrics.\nFew prior studies [7, 8] have addressed this problem; how-\never, their focus has predominantly been on the X-ray RRG.\nSince X-ray images are 2D scans, the corresponding reports\nare limited to a specific anatomical context and span shorter\nsentences. In contrast, 3D CT scans capture volumetric multi-\nslice information, resulting in semantically denser narratives\nand a broader vocabulary that includes richer anatomical de-\ntail, diverse medical terminology, lesion descriptions, and\nmeasurements. Most existing clinical-efficacy (CE) metrics\nwere originally tailored to X-ray-based vocabulary and thus\nstruggle to capture complex and diverse terminologies present\nin CT reports. Despite this limitation, CT-based studies still\nuse these metrics for reporting results and model comparison.\nTherefore, CT RRG requires special attention in terms of\nboth designing appropriate metrics and developing a tool to\nassess their applicability and feasibility.\nIn this paper, we develop CTest-Metric, a framework for\nevaluating the extent to which a given metric satisfies the cri-\nteria for being clinically grounded. This assessment is con-\nducted on eight benchmarking metrics that have been exten-\nsively used in recent CT report generation studies, ensuring\nconsistency with established evaluation practices. The pro-\nposed framework includes three analytical modules: a) Writ-\ning Style Generalizability Test (WSG) examines the metrics’\ngeneralizability across different writing styles, b) Synthetic\nError Injection Test (SEI) introduces factual errors in the re-\nports at three different levels and investigates their impact on\nthe metrics’ outcomes, and c) Metrics-vs-Expert Correlation\nTest (MvE) obtains expert ratings for reports exhibiting dis-\nagreement among the metrics. Further, a correlation is es-\ntablished between the eight metrics and expert ratings for a\narXiv:2601.11488v1  [cs.CL]  16 Jan 2026\n"}, {"page": 2, "text": "Fig. 1: (a) The propsoed framework, CTest-Metric, comprises three modules: (i) Writing Style Generalizability Test (WSG);\n(ii) Synthetic Error Injection Test (SEI); and (iii) Metrics-vs-Expert Correlation Test (MvE). (b) Sample reports are given.\ncomprehensive study. By leveraging reports generated using\nseven different LLMs in conjunction with expert assessment,\nthe proposed framework presents a robust pathway that met-\nrics developers can utilize when designing new metrics. The\npaper’s contributions are summarized below:\n• First framework to assess metrics for CT RRG: We de-\nveloped CTest-Metric, a novel unified framework to as-\nsess metrics for CT RRG. It investigates eight benchmark-\ning metrics on CT reports generated by seven different\nLLMs and analyzes the behavior of both NLG (text-based)\nand CE metrics for CT report evaluation.\n• Expert assessment: Our study selected 175 cases across\npredictions from seven LLMs where the metrics showed\nconflicting assessments. These specific reports were then\nreviewed by clinical experts, and correlations were de-\nrived between expert ratings and the metrics’ scores.\n• Analyzed the impact of stylistic variations and graded\nsynthetic errors: We introduced stylistic variations and\nfactual errors at three severity levels in the CT reports. We\nquantified the effect of these changes on metric sensitivity.\n2. RELATED WORK\nThe RRG literature reveals that various NLG and CE met-\nrics are commonly used to assess predicted CT reports. The\nearlier studies primarily relied on NLG metrics, including\nBLEU [3], ROUGE [4], METEOR [5], and BERTScore-\nF1 [9]. These metrics quantify textual similarity, for exam-\nple, BLEU measures n-gram overlap, ROUGE emphasizes\nsequence-level recall, and METEOR incorporates synonym\nmatches between the generated and the ground truth report.\nSimilarly, BERTScore-F1 computes similarity using contex-\ntual embeddings. To validate the clinical context, CE metrics\nincluding F1-RadGraph [10] and CheXpert [11] were intro-\nduced. While the former extracts entities and relations from\nthe given reports and measures graph-level scores, the latter\nadopts a rule-based labeler for 14 chest X-ray findings. More\nrecently, RaTEScore [12], GREEN Score [13], and CRG [14]\nwere introduced. The RaTEScore compares reports on the\nentity embedding level and the GREEN score uses regu-\nlar expressions to parse error counts from their pre-trained\nmodel output. Unlike other score, the CRG balances penal-\nities based on label distribution in the reports and ignores\nclinically irrelevant true negatives.\nFor the X-ray RRG tasks, prior works [15, 16] followed\na similar trend, relying on the same set of standard metrics\nincluding the four BLEU n-gram levels, METEOR, ROUGE,\nand CheXpert. A largely identical evaluation strategy is also\nobserved in CT RRG, where most studies [1, 2] adopt a sim-\nilar set of NLG and CE metrics, despite the latter being X-\nray-oriented and thus, do not fully capture the semantic and\nanatomical complexity of CT reporting. Although the liter-\nature lacks any unified framework for evaluating CT RRG\nmetrics, some X-ray-focused works such as Yu et al.[8] and\nBanerjee et al. [7] highlighted that even advanced metrics can\nbe inconsistent and poorly correlated with expert ratings.\n3. METHODOLOGY\nOverview.\nGiven a 3D CT scan i ∈I with correspond-\ning ground truth report r ∈R, we employed seven report\ngeneration models to generate CT report pj\ni ∈P, where\nj = 1, 2, .., 7. The seven deep learning models adopt a CT-\nCLIP [17] image encoder to extract image features combined\n"}, {"page": 3, "text": "with seven different LLMs, including variants of GPT (Dis-\ntilgpt, GPT2, GPT2-Medium, LLaMA-3.2-1B) and LLaMA\n(LLaMA-3.2-1B, LLaMA-2-7b-chat-hf), with a biomedical-\ndomain LLM (BioGPT-Large).\nThese variants are paired\nwith two configurations: LLM fine-tuning1 and a frozen-\nLLM setup (R2GenGPT’s shallow alignment [15]).\nThe predictions obtained from each model are assessed\nusing a set of eight metrics M, where M= {BLEU, ROUGE,\nMETEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN\nScore, CRG}. The first four metrics are NLG-based whereas\nthe last four are designed for clinical-efficacy check.\nThe\nproposed CTest-Metric framework is illustrated in Fig. 1,\ncomprising three analytical modules, as detailed below.\n3.1. Writing Style Generalizability Test (WSG)\nIn the WSG module, metrics are tested for sensitivity to re-\nport writing style. It evaluates whether they exhibit signifi-\ncant performance shifts in response to variations in stylistic\nmodifications despite unchanged clinical semantics. This test\nemploys an LLM-based approach using zero-shot instruction-\nbased prompting on LLaMA-3.1-8B-Instruct. This approach\nrephrases the predicted reports P generated by the seven mod-\nels into P ′, while preserving clinical outcomes and semantics.\nEach metric is applied on P and P ′, and the final difference\nis analyzed. A lower difference indicates metrics’ robustness\ntowards stylistic variations.\n3.2. Synthetic Error Injection Test (SEI)\nThe SEI module monitors how the metrics penalizes factual\nerrors of varying severity. We study three levels of errors in-\ntentionally injected using the same prompting technique used\nin Sec 3.1. It involved introducing one, two and multiple syn-\nthetic errors in ground truth reports, followed by evaluation\nat each level using the eight metrics from M. Specifically, it\nexamines the deviation of scores (∆(Sj, SG) = Sj −SG) be-\ntween the ideal case SG (where the ground truth is evaluated\nagainst itself) and report scores Sj with intentionally injected\nerrors at level j. A substantial difference in metrics’ scores for\nvarying levels of discrepancy shows its strong discriminative\nability to capture factual inconsistencies.\n3.3. Metrics-vs-Expert Correlation Test (MvE)\nIn this module, we systematically quantified the correlation\nbetween metrics in M and expert ratings E. We also exam-\nined inter-metric correlations to understand how they agree or\ndisagree in report quality evaluation. We computed this cor-\nrelation on 175 patient reports for which automated metrics\nprovided conflicting assessments. To this end, we initially\nderived per-patient scores for all eight metrics correspond-\ning to each LLM. After normalizing the per-patient scores,\n1https://github.com/fkodom/clip-text-decoder\nwe calculated the standard deviation across all metrics to as-\nsess disagreement. The top 25 cases with the highest stan-\ndard deviation (i.e, highest variable) were selected. These 175\ncases were finally reviewed by clinical experts, yielding ex-\npert ratings E. We then quantified the alignment by comput-\ning Spearman’s rank correlation coefficient (ρ) between every\npair of metrics (for inter-metric correlation) and between each\nmetric in M and the expert ratings E. A higher ρ signifies that\nthe two metrics tend to rank patients in a similar order while\na lower ρ shows disagreement in their assessment.\n4. EXPERIMENTS AND RESULTS\n4.1. Dataset and Training Details\nWe used a publicly available 3D medical imaging dataset, CT-\nRATE [17]. It comprises 50,188 non-contrast chest CT vol-\numes, along with their corresponding radiology reports. We\nused their official training and validation split. All RRG mod-\nels were trained for 10 epochs on NVIDIA A100 GPU using\nthe hyperparameters specified in the publicly released code.\n4.2. Analyzing Writing Style Generalizability Test (WSG)\nThe WSG results are presented in Fig. 2a, where each\nheatmap cell value represents the percentage difference (∆%)\nbetween the original score (S) and the score obtained after\nrephrasing (S\n′) for the corresponding model-metric pair.\nIt can be observed that NLG-based metrics were the most\nimpacted by the stylistic variations (ranging from -48.82%\nto -1.16%) because they primarily measure lexical overlap\nrather than underlying clinical semantics. Among CE met-\nrics, F1-RadGraph experienced the most significant deviation\n(ranging from -42.29% to 68.95%) indicating its high sensi-\ntivity to rephrasing. The best two performing metrics include\nthe CRG and the GREEN Score. Since all CE metrics ex-\ncept CRG were introduced focusing on X-Ray datasets, their\nscores are likely to shift substantially on a vast terminology\nset of CT reports, especially when rephrased. Although built\non X-Ray corpora, the GREEN score was originally tested\non out-of-domain modalities, including CT scans, making\nit relatively robust. Consequently, it performs comparably\nto CRG, which was developed for CT reports. CRG con-\nsiders the presence/absence of multi-label clinical entities\ninstead of sentence-level structure, making it less susceptible\nto performance shift under rephrasing.\n4.3. Analyzing Synthetic Error Injection Test (SEI)\nThe SEI results in Fig. 3 signify that the metric least impacted\nby synthetic errors is the BERTScore-F1 with ∆(SM, SG) of\n-0.02 (approximately -2%), whereas the most affected metrics\ninclude the GREEN Score (∆(S1, SG) = -0.0812, ∆(S2, SG)\n= -0.1277, ∆(SM, SG) = -0.6053) and the F1-RadGraph\n(∆(S1, SG) = -0.0891 , ∆(S2, SG) = -0.1647, ∆(SM, SG) =\n"}, {"page": 4, "text": "(a) Rephrasing robustness (WSG).\n(b) Inter-metric and expert correlation.\nFig. 2: Evaluation of metric reliability and robustness across rephrasing, factual error severity, and metric-expert correlations.\n-0.3970). It can be observed that all metrics exhibit similar\nbehavior when the report has minimal difference from the\nideal case but the performance significantly diverges when\nmultiple errors are injected. Therefore, it is crucial to ex-\namine various error levels to identify metrics’ sensitivity to\ncritical factual errors.\nFig. 3: Metric response across increasing error levels.\n4.4. Analyzing Metrics-vs-Expert Correlation Test (MvE)\nAs discussed in Sec. 3.3, we considered 175 conflicting cases\nfor this test. As shown in Fig. 2b, the GREEN Score closely\nresembles the Expert Rating E with ρ = 0.7 followed by F1-\nRadGraph with ρ = 0.53. The NLG-based metrics demon-\nstrated a similar correlation trend with E, reporting ρ in a\nnarrow range of 0.26 to 0.35. The worst performance was ob-\nserved using the CRG, which presented a negative correlation\nwith E and other metrics. A high correlation range among\nNLP-based metrics indicates strong agreement in their assess-\nment. While F1-RadGraph exhibits consistent alignment to\nother metrics, it shows a slight bias to NLG-based metric out-\ncomes rather than the GREEN Score and Expert Rating.\n5. DISCUSSION AND CONCLUDING REMARKS\nThis study introduces CTest-Metric, a modular framework to\nevaluate automated metrics used in CT RRG, assessing their\nability to capture clinical context in reports. By consider-\ning style robustness (WSG), factual-error sensitivity (SEI),\nand alignment with expert judgment (MvE), the work demon-\nstrates that widely used NLG metrics are brittle to rephras-\ning and incompletely capture factual correctness, whereas CE\nmetrics vary substantially in their agreement with experts.\nIn particular, GREEN Score exhibits the highest association\nwith expert ratings on a curated set of “disagreement” cases,\nwhile CRG remains robust to stylistic changes but correlates\nnegatively with expert scores. This can be attributed to its\nreliance on label-level information which makes it largely in-\nsensitive to rephrasing, yet it remains less aligned with expert\nevaluations. These findings have immediate implications for\nmetric choice in CT RRG and for how future metrics should\nbe stress-tested before deployment.\nExpert validation involved two reviewers, with a second\nopinion sought for ambiguous cases, which limits the diver-\nsity of assessment. WSG and SEI depend on an LLM-based\nprompting, which primarily introduced laterality and nega-\ntion errors, though the fidelity of these edits was not indepen-\ndently validated. Finally, all experiments are conducted on\nCT-RATE (one of the largest in the literature); broader gener-\nalization to other institutions, contrast phases, or body regions\nremains to be established. These constraints highlight oppor-\ntunities for expanded validation and reporting in subsequent\nversions.\n"}, {"page": 5, "text": "6. COMPLIANCE WITH ETHICAL STANDARDS\nThis research study was conducted retrospectively using hu-\nman subject data made available in open access by (CT-Rate).\nEthical approval was not required as confirmed by the license\nattached to the open-access data.\n7. REFERENCES\n[1] Yu Xin, Gorkem Can Ates, Kuang Gong, and Wei\nShao,\n“Med3dvlm:\nAn efficient vision-language\nmodel for 3d medical image analysis,” arXiv preprint\narXiv:2503.20047, 2025.\n[2] Theo Di Piazza, Carole Lazarus, Olivier Nempont, and\nLoic Boussel, “Ct-agrg: Automated abnormality-guided\nreport generation from 3d chest ct volumes,” in 2025\nIEEE 22nd International Symposium on Biomedical\nImaging (ISBI). IEEE, 2025, pp. 01–05.\n[3] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu, “Bleu: a method for automatic evaluation of\nmachine translation,” in Proceedings of the 40th annual\nmeeting of the Association for Computational Linguis-\ntics, 2002, pp. 311–318.\n[4] Chin-Yew Lin, “Rouge: A package for automatic eval-\nuation of summaries,” in Text summarization branches\nout, 2004, pp. 74–81.\n[5] Satanjeev Banerjee and Alon Lavie, “Meteor: An au-\ntomatic metric for mt evaluation with improved correla-\ntion with human judgments,” in Proceedings of the acl\nworkshop on intrinsic and extrinsic evaluation measures\nfor machine translation and/or summarization, 2005,\npp. 65–72.\n[6] Yiming Shi, Xun Zhu, Kaiwen Wang, Ying Hu, Chenyi\nGuo, Miao Li, and Ji Wu, “Med-2e3: A 2d-enhanced\n3d medical multimodal large language model,” arXiv\npreprint arXiv:2411.12783, 2024.\n[7] Oishi Banerjee, Agustina Saenz, Kay Wu, Warren\nClements, Adil Zia, Dominic Buensalido, Helen Kav-\nnoudias, Alain S Abi-Ghanem, Nour El Ghawi, Cibele\nLuna, et al., “Rexamine-global: A framework for un-\ncovering inconsistencies in radiology report generation\nmetrics,” in Biocomputing 2025: Proceedings of the Pa-\ncific Symposium. World Scientific, 2024, pp. 185–198.\n[8] Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan,\nAndy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Uru-\nrahy Nunes Fonseca, Henrique Min Ho Lee, Zahra\nShakeri Hossein Abad, Andrew Y Ng, et al., “Evalu-\nating progress in automatic chest x-ray radiology report\ngeneration,” Patterns, vol. 4, no. 9, 2023.\n[9] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi, “Bertscore: Evaluating text\ngeneration with bert,” arXiv preprint arXiv:1904.09675,\n2019.\n[10] Saahil\nJain,\nAshwin\nAgrawal,\nAdriel\nSaporta,\nSteven QH Truong, Du Nguyen Duong, Tan Bui,\nPierre Chambon, Yuhao Zhang, Matthew P Lungren,\nAndrew Y Ng, et al., “Radgraph: Extracting clinical\nentities and relations from radiology reports,”\narXiv\npreprint arXiv:2106.14463, 2021.\n[11] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu,\nSilviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,\nBehzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al.,\n“Chexpert: A large chest radiograph dataset with uncer-\ntainty labels and expert comparison,”\nin Proceedings\nof the AAAI conference on artificial intelligence, 2019,\nvol. 33, pp. 590–597.\n[12] Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang,\nYanfeng Wang, and Weidi Xie, “Ratescore: A metric\nfor radiology report generation. medrxiv,” 2024.\n[13] Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya\nVarma, Louis Blankemeier, Christian Bluethgen, Arne\nEdward Michalson Md, Michael Moseley, Curtis Lan-\nglotz, Akshay S Chaudhari, et al.,\n“Green: Genera-\ntive radiology report evaluation and error notation,” in\nFindings of the association for computational linguis-\ntics: EMNLP 2024, 2024, pp. 374–390.\n[14] Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit,\nHadrien Reynaud, Bernhard Kainz, and Bjoern Menze,\n“Crg score: A distribution-aware clinical metric for radi-\nology report generation,” in Medical Imaging with Deep\nLearning-Short Papers, 2025.\n[15] Zhanyu Wang, Lingqiao Liu, Lei Wang, and Luping\nZhou,\n“R2gengpt: Radiology report generation with\nfrozen llms,” Meta-Radiology, vol. 1, no. 3, pp. 100033,\n2023.\n[16] Zhixuan Chen, Luyang Luo, Yequan Bie, and Hao Chen,\n“Dia-llama: Towards large language model-driven ct re-\nport generation,” in International Conference on Medi-\ncal Image Computing and Computer-Assisted Interven-\ntion. Springer, 2025, pp. 141–151.\n[17] Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas,\nAyse Gulnihan Simsek, Sevval Nil Esirgun, Irem Do-\ngan, Muhammed Furkan Dasdelen, Bastian Wittmann,\nEnis Simsar, Mehmet Simsar, et al.,\n“A foundation\nmodel utilizing chest ct volumes and radiology reports\nfor supervised-level zero-shot detection of abnormali-\nties,” CoRR, 2024.\n"}]}