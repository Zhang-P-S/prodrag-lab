{"doc_id": "arxiv:2511.11878", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.11878.pdf", "meta": {"doc_id": "arxiv:2511.11878", "source": "arxiv", "arxiv_id": "2511.11878", "title": "MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers", "authors": ["Fernanda Bufon Färber", "Iago Alves Brito", "Julia Soares Dollis", "Pedro Schindler Freire Brasil Ribeiro", "Rafael Teixeira Sousa", "Arlindo Rodrigues Galvão Filho"], "published": "2025-11-14T21:13:28Z", "updated": "2025-11-14T21:13:28Z", "summary": "While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.11878v1", "url_pdf": "https://arxiv.org/pdf/2511.11878.pdf", "meta_path": "data/raw/arxiv/meta/2511.11878.json", "sha256": "ecd8dfbc6f6e43ab2110a96a74ab2a1d2dbdc960f948976f66bae4f9ccfcc743", "status": "ok", "fetched_at": "2026-02-18T02:27:05.492024+00:00"}, "pages": [{"page": 1, "text": "MedPT: A Massive Medical Question Answering Dataset for\nBrazilian-Portuguese Speakers\nFernanda B. Färber1,2, Iago A. Brito1,2, Julia S. Dollis1,2, Pedro S. F. B. Ribeiro1,2\nRafael T. Sousa1,3, Arlindo R. Galvão Filho1,2\nAdvanced Knowledge Center for Immersive Technologies (AKCIT)1\nFederal University of Goiás2\nFederal University of Mato Grosso3\n{fernandabufon, iagoalves, juliadollis, schindler}@discente.ufg.br\nrafaelsousa@ufmt.br, arlindogalvao@ufg.br\nAbstract\nWhile large language models (LLMs) show transformative potential in healthcare, their development remains focused\non high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical\nand cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale,\nreal-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor\ninteractions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative\nanalysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via\nLLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals\nits thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor\ncommunication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter\nmodel achieves an outstanding 94% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows\nmisclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving\nthe dataset’s deep semantic richness. We publicly release MedPT to foster the development of more equitable,\naccurate, and culturally-aware medical technologies for the Portuguese-speaking world.\nKeywords: Medical question answering, healthcare, Brazilian Portuguese\n1.\nIntroduction\nLarge language models (LLMs) are increasingly\ndemonstrating transformative potential across di-\nverse healthcare applications, from online patient\nsupport and clinical text classification to robust med-\nical question answering (Nazi and Peng, 2024; Liu\net al., 2025). However, the performance, safety,\nand clinical reliability of these systems are funda-\nmentally contingent on the scale and quality of their\ntraining data (Ryan Conmy et al., 2023). Conse-\nquently, datasets that lack demographic represen-\ntation, contain inherent biases, or are insufficiently\ncurated to handle complex medical nuances can\nlead to models that produce factually incorrect or\nsystematically skewed outputs, posing significant\nrisks and undermining clinical trust (Khan et al.,\n2021; Schwabe et al., 2024). Compounding this\nchallenge, current progress and resource devel-\nopment are overwhelmingly concentrated in high-\nresource languages, particularly English, a linguis-\ntic bias that creates a critical barrier for other ma-\njor world languages, severely limiting the develop-\nment, validation, and equitable deployment of these\npromising technologies to large global populations\n(Enshaei and Naderkhani, 2024).\nThis linguistic and data-centric gap is clearly re-\nflected in the literature, where significant research\ninvestment in medical text curation has centered\non English (Jin et al., 2019; Pal et al., 2022; Sing-\nhal et al., 2022; Preston et al., 2023), Chinese (He\net al., 2019; Zhang et al., 2018; Zhu et al., 2023), or\nboth (He et al., 2020). Despite valuable initiatives\nhave begun to extend this scope to other languages\nsuch as Arabic, Vietnamese, Persian, and Turkish\n(Abdelhay et al., 2023; Nguyen et al., 2022; Ghas-\nsabi et al., 2025; İncidelen and Aydoğan, 2024),\nthe resource disparity remains particularly acute\nfor Brazilian Portuguese in specialized domains\nsuch as open-domain medical question answering.\nWhile translating existing datasets may seem\nlike a viable shortcut, this approach is fundamen-\ntally limited because it fails to capture essential\nlocal context. For example, the epidemiology of\nendemic conditions such as dengue and Chagas\ndisease is entirely absent from these resources.\nFurthermore, translation erases crucial linguistic\nand cultural nuances, including region-specific clin-\nical expressions and colloquial patient descriptions\nof symptoms (Naveen and Trojovsk`y, 2024). This\ncreates a dual problem: locally, it constrains the\nadvancement of clinically reliable language tech-\nnologies for the population. Globally, it perpetuates\na critical void in the medical data landscape, which\nremains heavily biased towards the health contexts\nprevalent in high-resource language communities.\nTherefore, creating a robust dataset for Brazilian\nPortuguese is not only essential for its speakers\narXiv:2511.11878v1  [cs.CL]  14 Nov 2025\n"}, {"page": 2, "text": "but also represents an opportunity to enrich global\nhealth knowledge with data on otherwise underrep-\nresented diseases and clinical realities.\nTo address this critical resource gap, we intro-\nduce MedPT, the first public large-scale corpus for\nthe Brazilian Portuguese medical domain1, com-\nprising 384,095 authentic question-answer pairs\nfrom real patient-doctor interactions along with\nmetadata detailing the disease, the medical spe-\ncialty, and a label identifying the question type. We\nprovide a comprehensive analysis of the dataset,\ndetailing its linguistic properties, such as the nat-\nural asymmetry of patient and doctor communica-\ntion, and its thematic breadth, which covers over\n3,200 medical topics and 104 professional special-\nizations. To validate its practical utility, we estab-\nlish strong benchmarks on a real-world medical\nsubspecialty classification task. We demonstrate\nthat fine-tuning a modern 1.7B parameter model\non MedPT achieves a 94% F1-score on a chal-\nlenging 20-class setup, confirming the dataset is a\nhigh-quality and effective resource for developing\nand specializing language models for the nuanced\nBrazilian healthcare context.\nThis resource provides a foundational dataset\nfor a range of tasks, including medical question\nanswering, patient intent classification, and linguis-\ntic studies on doctor-patient communication. More\nbroadly, MedPT captures the real-world nuances of\nhealthcare interactions, enabling the development\nof more contextualized, reliable, and accessible\nmodels for Portuguese-speaking populations.\n2.\nRelated Works\nIn recent years, several datasets have been de-\nveloped to support research in medical question\nanswering and health-related natural language pro-\ncessing (NLP). These resources have been funda-\nmental for training models capable of reasoning\nover clinical information, understanding patient in-\ntent, and generating reliable medical responses.\nNatural language processing has been applied\nacross multiple medical fields, including oncology,\npsychiatry, and primary care (Cascella et al., 2024;\nWang et al., 2020; Wu et al., 2019). Within these do-\nmains, the most common NLP tasks are information\nextraction (Mulyar et al., 2021; Cury et al., 2021;\nAlkouz et al., 2019), classification (Magna et al.,\n2020), named entity recognition (Ji et al., 2019)\nand question answering (Singhal et al., 2025).\nHowever, the development and fine-tuning of\nsuch models require large, well-structured, and\nhigh-quality datasets.\nTo address this, several\ndatasets have been proposed to advance research\n1Due storage constraints, during blind review we only\nrelease a 10K sample of the dataset\nin medical natural language processing. For ex-\nample, Johnson et al. (2023) introduced MIMIC-IV,\nwhich focuses on describing a vast collection of\nde-identified electronic health records (EHR) from\nintensive care units, widely used for developing\nclinical prediction models and training NLP sys-\ntems on real-world clinical documentation. Simi-\nlarly, BioRED (Luo et al., 2022) aimed at advanc-\ning biomedical knowledge extraction by annotating\ndocument-level relations between diverse entities\nlike genes, diseases, and chemicals from scientific\npapers abstracts.\nDespite these contributions,\nmost existing\ndatasets consist primarily of clinical or biomedical\nrecords and lack open-ended, real-world patient\nquestions paired with verified medical answers. In\nresponse to this limitation, He et al. (2019) pro-\nposed cMedQA, which includes 54,000 pairs of\nphysician-patient questions and answers in Chi-\nnese, sourced from online health forums, focusing\non non-restricted, open-ended questions. Follow-\ning this line of research, MedRedQA (Nguyen et al.,\n2023) includes 51,000 pairs of open-ended English\nconsumer questions and verified expert answers,\ncollected from online medical consultation forums.\nIn response to this data scarcity, pioneering ef-\nforts have begun to establish benchmark medical\ndatasets for several under-resourced languages,\nemploying a variety of curation strategies. One\ncommon approach involves collecting data from\ntrusted online health portals.\nFor instance, Vi-\nHealthQA (Nguyen et al., 2022) contains over\n10,000 medical question-answering pairs for Viet-\nnamese sourced from credible health websites,\nwhile MAQA (Abdelhay et al., 2023) represents the\nlargest Arabic healthcare Q&A dataset, with over\n430,000 questions from regional health platform.\nOther initiatives have utilized different sources;\nMedTurkQuAD (İncidelen and Aydoğan, 2024), a\nTurkish medical QA dataset, was derived from Turk-\nish Wikipedia and national medical theses. The\neffort for Persian by Ghassabi et al. (2025) was\neven more comprehensive, introducing the MF3QA\ndataset and leveraging it to create Gaokerena-V1,\nthe first open-source Persian medical language\nmodel. Together, these works illustrate a growing\ncross-lingual interest in expanding medical NLP\nresources beyond English, particularly for morpho-\nlogically rich or culturally distinct languages.\nRecent efforts to address the Portuguese data\nshortage include the work of Paiola et al. (2024),\nwho fine-tuned LLMs on an English healthcare cor-\npus translated into Brazilian Portuguese. While this\napproach enabled preliminary evaluations of medi-\ncal dialogue assistants, the resulting dataset was\nnot made publicly available. More fundamentally,\nits reliance on translation means it inherently lacks\nthe authentic, region-specific linguistic nuances\n"}, {"page": 3, "text": "and clinical contexts, such as endemic diseases,\nthat only a native, real-world dataset can capture.\nThis highlights a persistent and critical need for\na publicly accessible, native corpus built from au-\nthentic patient-doctor interactions to advance the\nfield.\nAlongside translation efforts, existing medical\ndatasets for native Brazilian Portuguese have cen-\ntered on formal clinical text from hospital records.\nProminent examples include BRATECA (Consoli\net al., 2022), a large collection of de-identified clini-\ncal notes; SemClinBr (Oliveira et al., 2022), a se-\nmantically annotated corpus for entity recognition;\nand the work of (da Costa Lopes, 2019), identifying\nsymptoms in hospital texts. While valuable, these\nresources are composed of formal medical docu-\nmentation and do not capture the open-ended, con-\nversational dialogues between patients and physi-\ncians. This absence of real-world, patient-initiated\nquestions is a primary barrier to developing and\nevaluating robust interactive healthcare systems in\nPortuguese.\nA recent initiative targeting this conversational\ngap is IaraMed (Färber et al., 2025), a women’s\nhealthcare chatbot developed using data scraped\nfrom online health platform. While a promising\napplication of real-world medical Q&A, this work\nis narrowly focused on women’s health and does\nnot cover the full spectrum of patient-doctor in-\nquiries, and the underlying dataset was not pub-\nlicly released. Thus, a critical need persists for a\nbroad-domain, publicly accessible, native corpus\nbuilt from authentic patient-doctor interactions to\nadvance the field.\n3.\nMedPT\nThis section details the multi-stage process applied\nin the development of MedPT dataset, designed\nto transform raw, user-generated content into a ro-\nbust language resource. We first describe the data\nacquisition from an authentic patient-doctor interac-\ntion platform, followed by our extensive data clean-\ning and curation pipeline, which includes both gen-\neral preprocessing and nuanced, column-specific\nrefinements. We then detail an LLM-driven anno-\ntation step, where we employed a large language\nmodel to classify each query and extract a new,\nstructured feature to enhance the dataset’s utility.\nFinally, to ensure fair comparison and compatibility\nwith modern models, the entire corpus was pro-\ncessed using the sub-word tokenizer employed by\nstate-of-the-art LLMs (Achiam et al., 2023).\n3.1.\nData Acquisition\nTo build the MedPT dataset, we collected real med-\nical questions and answers in Portuguese from\nDoctoralia2, a widely used online platform where\nBrazilian patients ask health-related questions and\nreceive responses from verified doctors. Doctor-\nalia was chosen as data source due both its large\nvolume of questions across a wide range of med-\nical professions (specialties) and its verification\nprocess to ensure that only certified health pro-\nfessionals can respond to users’ inquiries. Addi-\ntionally, all questions are completely anonymous,\nwhich guarantees the privacy and confidentiality\nof users’ personal information. Furthermore, the\ndata collected from this platform are natural and\nuser-generated, reflecting real-life inquiries from\nBrazilian users. This aspect makes the dataset\nparticularly valuable for developing healthcare solu-\ntions tailored to the Brazilian context, as it captures\nthe country’s linguistic and cultural nuances, health-\ncare system characteristics, and the prevalence of\nregion-specific diseases.\nTo collect the questions, answers, and re-\nlated data, we developed a custom web-scraping\npipeline to extract information directly from the web-\nsite. Each sample includes the medical topic of the\nquestion (e.g., hypertension, pregnancy, diabetes),\nthe patient’s textual query, the doctor’s textual re-\nsponse, and metadata about the physician, such as\nprofession and rating. During this collection phase,\nwe gathered approximately 2 million raw samples.\n3.2.\nData Cleaning\nThe initial cleaning phase focused on structural nor-\nmalization and deduplication. A primary source of\ndata redundancy was identified in the platform’s de-\nsign, which allowed a single question-answer pair\nto be associated with multiple medical specialties.\nThis resulted in numerous duplicate entries for the\nsame core text. To address this and consolidate\nthe dataset, we removed the redundant specialty\ncolumn and then applied a strict deduplication pro-\ncess based on the core question-answer text. This\nprocedure, combined with the removal of other ir-\nrelevant columns (e.g., doctor’s ratings), reduced\nthe corpus from approximately 2 million raw sam-\nples to 393,143 unique entries. Subsequently, we\nperformed a textual normalization step across all\ncolumns, removing HTML tags, URLs, special char-\nacters, and extraneous whitespace, which signifi-\ncantly reduced noise as evidenced by a decrease\nin the average token count per question-answer\npair from 33.05 to 26.54 tokens.\n3.2.1.\nColumn-Specific Curation\nTo ensure the quality and suitability of the corpus for\nquestion-answering tasks, we implemented a multi-\nstage, column-specific curation protocol through all\n2https://www.doctoralia.com.br\n"}, {"page": 4, "text": "features of our dataset (Question, Answer Ques-\ntion Topic and Profession). This protocol combined\nquantitative analysis with qualitative manual inspec-\ntion. We first analyzed token distributions such as\nmean, standard deviation (SD) and range, to iden-\ntify statistical outliers, which were then manually\nreviewed to confirm their nature. Based on this\nhybrid analysis, we applied targeted procedures, in-\ncluding the removal of malformed or uninformative\nentries and the contextual enrichment of incomplete\nquestions, as detailed below.\nQuestion.\nThe Question column exhibited high\nlinguistic diversity (mean 42.5, SD 31.0), with to-\nken counts ranging from 3 to 801. Our qualitative\ninspection confirmed that the longest entries were\nvalid, complex patient narratives. Conversely, the\nshortest entries (3–9 tokens) were typically found\nto be context-dependent (e.g., \"What is the treat-\nment?\"), lacking sufficient information to be stan-\ndalone queries. To address this ambiguity with-\nout data loss, we implemented a contextual enrich-\nment procedure: the corresponding Question Topic\nwas prepended to each of these 7,178 incomplete\nqueries. This process rendered each question self-\ncontained and preserved a significant portion of the\ndataset that would have otherwise been discarded.\nAnswer.\nThe Answer column exhibited even\ngreater variability (mean 96.5, SD 75.0), with to-\nken counts spanning from 0 to 2,984. To ensure\nthe corpus contained only direct, high-quality re-\nsponses, a qualitative review of these extremes\nwas performed. This analysis revealed two distinct\ntypes of low-utility data: answers with fewer than 10\ntokens were found to be uninformative (e.g., \"Con-\nsult a specialist\"), while those exceeding 1,000 to-\nkens, though medically valid, were typically generic\ninformational articles rather than specific replies\nto the patient’s query. To optimize the dataset for\ndirect question-answering tasks, we established a\nvalid token range of 10–1,000. Applying this filter\nremoved 8,359 samples, significantly improving the\nsignal-to-noise ratio and focus of the final corpus.\nQuestion Topic.\nFor the Question Topic column,\nwe implemented a token-count-based filtering pro-\ntocol to ensure data quality. An initial analysis pro-\ngrammatically removed all zero-token samples. We\nthen conducted a qualitative review of the distri-\nbution’s extremes. This inspection revealed that\nsingle-token entries (e.g., \"pain\") were valid medical\ntopics and were therefore retained. Conversely, en-\ntries with the highest token counts were found to be\nconsistently malformed or corrupted data. Based\non this manual review, we established a conser-\nvative upper threshold of 23 tokens to precisely\nexclude these erroneous entries while preserving\nall valid topics. This procedure refined the column’s\nconsistency by removing 398 samples.\nProfession.\nThe Profession column presented\na distinct curation challenge. Statistical analysis\nshowed moderate variance (mean 5.7 tokens, SD\n3.4), which our qualitative review attributed to the\nplatform’s feature allowing professionals to list mul-\ntiple specialties. We made the deliberate decision\nto retain these multi-specialty entries to avoid in-\nformation loss, as they represent valid, rich data.\nHowever, a separate filtering process was neces-\nsary to address genuine scraping artifacts. Our\nanalysis identified and removed non-professional\ndata, such as location names (e.g., \"Fortaleza\"),\nnumerical values, and empty strings. This targeted\nprocedure refined the column’s integrity by exclud-\ning 62 invalid samples.\nFollowing this multi-stage cleaning protocol, we\nachieve a final dataset comprising 384,095 high-\nquality question-answer pairs, resulting in a sub-\nstantial and reliable corpus tailored for developing\nlanguage models in the Brazilian Portuguese med-\nical domain.\n3.3.\nQuestion-Type Annotation\nTo provide a fine-grained, semantic understand-\ning of user intent within the corpus, we enriched\nthe dataset with a synthetically generated feature\ncolumn classifying the type of each user question.\nThis structural annotation is critical for analyzing the\ndataset’s composition (e.g., quantifying the types\nof user needs) and enables more targeted model-\ning applications. Following established practices\nin dataset construction (Alhuzali et al., 2024; Guo\net al., 2018), we employed gpt-oss-120b (Agarwal\net al., 2025) to annotate each user query. Through\na structured prompt and definitions, the model as-\nsigned each question to one of the following seven\npredefined classes, which represent distinct facets\nof health-related inquiries:\n• Diagnosis: Questions focused on the inter-\npretation of symptoms, medical exams, or the\nidentification of diseases.\n• Treatment: Encompasses questions about\nmedications, therapies, side effects, and con-\ntraindications.\n• Anatomy and Physiology: Groups questions\nabout the structure and function of body organs\nand systems.\n• Epidemiology: Covers topics such as causes,\nrisk factors, and disease complications.\n• Healthy Lifestyle: Refers to questions about\nhabits that influence health, such as diet, exer-\ncise, or sleep.\n"}, {"page": 5, "text": "• Choosing Healthcare Professionals: Cap-\ntures doubts about which specialist to consult\nor how to access medical services.\n• Other: Includes questions that do not fit into\nthe previous categories but are still related to\nhealth or the healthcare system.\n4.\nDataset Characterization\nTo characterize the final MedPT corpus and validate\nits suitability for training robust language models,\nwe performed an extensive exploratory data analy-\nsis. This section details the dataset’s key statistical\nand structural properties, including an analysis of\nthe distribution across question types and medical\nprofessions, a summary of its linguistic features,\nand a presentation of representative samples. The\nanalysis underscores the corpus’s large scale and\nmulti-domain nature, confirming its value as a com-\nprehensive resource for the Brazilian Portuguese\nmedical NLP community.\n4.1.\nDataset Overview\nThe MedPT dataset is a large-scale corpus for med-\nical question answering in Brazilian Portuguese.\nEach entry comprises a patient’s question paired\nwith one or more verified answers from licensed\nhealthcare professionals, ensuring both authentic-\nity and clinical reliability. The dataset is structured\nwith rich metadata to support a diverse range of\ndownstream NLP tasks. The fields are detailed as\nfollows:\n• Question:\nThe patient’s original inquiry\nin Brazilian Portuguese, typically describing\nsymptoms, test results, or seeking information\nabout medications and treatments.\n• Answer: A response provided by a licensed\nhealthcare professional, containing clinically\ngrounded explanations or recommendations.\n• Question Topic: The primary medical field of\nthe inquiry (e.g., Gynecology, General Prac-\ntice, Dermatology).\n• Subspecialty: A finer-grained topic within the\nmain field, often a specific disease or condition\n(e.g., Diabetes, Acne, Hypertension).\n• Profession: The specialization of the profes-\nsional who authored the answer (e.g., General\nPractitioner, Psychologist, Nutritionist).\n• Question Type: The semantic intent of the\nquestion, classified into one of seven cate-\ngories: Diagnosis, Treatment, Anatomy and\nPhysiology, Epidemiology, Healthy Lifestyle,\nChoosing Healthcare Professionals, or Other.\nCollectively, these fields form a richly structured\ndataset comprising over 384K question-answer\npairs, derived from more than 180K unique ques-\ntions. This composition enables a wide range of\napplications, from specialty prediction and question-\ntype classification to the development and domain\nadaptation of medical language models.\n4.2.\nQuantitative Analysis\nThe scale and structure of MedPT are defined\nby its core statistics. A key characteristic of the\ndataset is its one-to-many mapping, with each ques-\ntion receiving an average of 2.11 answers. While\nmost questions receive between one and three\nresponses, some attract a much larger number,\nwith an observed maximum of 266 answers for the\nsame question. This multiplicity is a central feature\nof the corpus, reflecting the platform’s collabora-\ntive nature where different healthcare professionals\nprovide complementary clinical perspectives on a\nsingle patient inquiry.\nTo characterize the linguistic properties of the\ncorpus, we performed a quantitative analysis of\nits primary textual fields. Together, the question\nand answer columns contain nearly 57 million to-\nkens distributed in a natural asymmetry that re-\nflects the platform’s core pragmatic context. Pa-\ntient questions are typically concise while the cor-\nresponding professional answers are substantially\nmore detailed and explanatory (averaging 51.79\nand 96.49 tokens length, respectively). This di-\nvergence stems from the distinct communicative\ngoals of each role: patients are focused on symp-\ntom description and problem specification, whereas\ndoctors are tasked with providing comprehensive\nexplanations, differential considerations, and es-\nsential risk-management disclaimers.\nFurthermore, the high standard deviations for to-\nken counts in both fields (37.5 for questions and\n82.7 for answers) highlight a strong variability in lin-\nguistic depth. This variability is a key feature of the\nresource, demonstrating that the dataset captures\na wide spectrum of interactions, from simple and\ndirect inquiries to complex narrative-driven case de-\nscriptions, making it a robust resource for training\nmodels that can both understand diverse patient\ncommunication styles and generate appropriately\ndetailed expert responses.\n4.3.\nDiversity and Coverage\nBeyond its scale, MedPT exhibits substantial the-\nmatic diversity, covering over 3,200 unique medical\ninquiry topics. An analysis of the topic distribution\nreveals wide coverage across medical domains and\nhighlights a key characteristic of the dataset. As\nshown in Table 1, the most frequent topics among\n"}, {"page": 6, "text": "Topic\nQuestions\nAnswers\nFrequent - Questions\nSyphilis\n2,336\n2,603\nInguinal Hernia\n1,914\n2,826\nAppendicitis\n1,773\n2,250\nFrequent - Answers\nDepression\n1,140\n11,715\nAnxiety\n1,025\n10,734\nBipolar Disorder\n928\n3,906\nFrequent - Both\nHIV and AIDS\n5,664\n6,162\nCOVID-19\n4,674\n5,339\nTable 1: Most frequent topics in the dataset, com-\nprising the union of the five most frequent by ques-\ntions and answers.\nTopic\nQuestions\nAnswers\nFood desensitization\n1\n39\nNightmares\n1\n38\nChanges After Death\n2\n75\nRelationship conflicts\n3\n112\nMutism\n1\n35\nExistential distress\n2\n66\nDifficulty in decision-making\n3\n96\nDeep and prolonged sadness\n3\n95\nShyness\n10\n311\nTable 2: Topics with few patient questions and a\nhigh number of answers.\npatient questions include infectious and clinical dis-\neases such as HIV/AIDS, COVID-19, and Syphilis,\nwhile the topics with the highest volume of profes-\nsional answers are concentrated in mental health\nfields. This distinction underscores the dataset’s\nunique value, capturing a broad spectrum of both\nurgent public health concerns from patients and\nareas of deep, sustained engagement from health-\ncare professionals.\nFurthermore, Table 2 demonstrates that topics\nwith few questions often received a huge amount\nof answers, suggesting that even rare or highly\nspecific issues tend to stimulate extensive profes-\nsional interaction. This pattern indicates that, al-\nthough these topics represent a smaller portion of\nthe dataset, they attract multiple perspectives and\ndeeper discussions around the same underlying\nquestion.\nTo facilitate a finer-grained analysis of user intent,\neach question was enriched with a Question Type\nlabel. The distribution of these categories, shown\nin Figure 1, reveals that the dataset is dominated\nby action-oriented inquiries. Questions concern-\ning Treatment and Diagnosis collectively account\nTreatment\nDiagnosis\nEpidemiology\nHealthy Lifestyle\nChoice of Health Professionals\nOthers\nAnatomy and Physiology\n0\n10000\n20000\n30000\n40000\n50000\n60000\n70000\n68293 66540\n24788\n8097\n6097\n4982\n2975\nQuestion Type\nQuestions Count per Question Type\nNumber of Questions\nFigure 1: Distribution of medical questions by type.\nfor nearly 70% of the data, indicating a primary fo-\ncus on actionable medical guidance. Concurrently,\nthe presence of informational categories such as\nEpidemiology and Healthy Lifestyle confirms the\ndataset’s thematic breadth. This diversity is also\nreflected in the professional specializations of the\nanswer providers. The dataset includes answers\nfrom professionals across 104 distinct titles, which\nform over 900 unique specialization combinations\ndue to multi-specialty declarations.\nFurthermore, the data exhibits a long-tail distri-\nbution characteristic of real-world: high-frequency\nspecialties such as Psychology, Gynecology, and\nOrthopedics are well-represented (as shown in\nFugure 2), while a wide array of highly specialized\nfields, including Bariatric Surgery and Pediatric Nu-\ntrology, constitute the long tail with few number of\nexamples (8 and 4 samples, respectively). This\nbroad spectrum, spanning both common health\nconcerns and niche medical domains, underscores\nthe dataset’s ecological validity as a resource for\nmodeling authentic healthcare interactions, offering\na broad and realistic coverage.\n5.\nExperiments\nTo test MedPT dataset under downstream appli-\ncations, we formulate a multi-class classification\ntask designed to predict the appropriate medical\nsubspecialty from a patient’s question. This task\nsimulates a critical real-world use case: the au-\ntomated routing of patient inquiries to the correct\nclinical department. To evaluate model robustness\n"}, {"page": 7, "text": "against varying levels of label complexity, we create\nthree experimental tiers based on the Top 5, Top\n10, and Top 20 most frequent subspecialties in the\ndataset.\nPsychologist\nGynecologist\nOrthopedist - Traumatologist\nUrologist\nPsychoanalyst\n0\n20000\n40000\n60000\n80000\n78792\n29308\n22943\n21140\n20527\nProfession\nTop 5 Professions by Number of Questions\nNumber of Samples\nFigure 2: Distribution of the top five most repre-\nsented medical professions by number of answers.\n5.1.\nExperimental Setup\nFor all experiments, we use the Qwen3-1.7B (Yang\net al., 2025) due to its balance between computa-\ntional efficiency and performance, offering compet-\nitive results while maintaining a substantially lower\ntraining and inference cost compared to larger-\nscale models. We then benchmark its performance\nacross three distinct methodological settings cor-\nresponding to the Top 5, Top 10, and Top 20 most\nfrequent subspecialties. Each split as divided into\nan 80% training set and a 20% held-out test set.\nThe evaluation settings are as follows:\n1. Zero-Shot: The base model performs classifi-\ncation using only the task instructions and the\npatient’s question, without any examples. This\ntests the model’s intrinsic ability to generalize\nto the medical domain.\n2. Few-Shot (In-Context Learning): The base\nmodel is prompted with instructions along with\na small number of dynamically-sampled ex-\namples from the training set. We evaluate its\nin-context learning capability with 1, 3, and 5\nexamples to measure performance with vary-\ning levels of contextual information.\n3. Fine-Tuning:\nThe model is fine-tuned on\nthe 80% training split of each respective sub-\ndataset. The training is conducted for 5 epochs\nusing the AdamW optimizer with a learning\nrate of 2e-5 and a batch size of 32. This set-\nting evaluates the dataset’s effectiveness for\nspecializing a model to the task.\nPerformance across all settings is measured us-\ning standard classification metrics: precision, recall,\nand F1-score, calculated with macro averages to\naccount for potential class imbalance.\nStrategy\nPrecision\nRecall\nF1-Score\nTop-5\nZero-Shot\n0.81\n0.60\n0.60\n3-Shot\n0.88\n0.86\n0.86\n5-Shot\n0.88\n0.87\n0.87\nFine-Tuned\n0.98\n0.98\n0.98\nTop-10\nZero-Shot\n0.79\n0.55\n0.54\n3-Shot\n0.84\n0.76\n0.78\n5-Shot\n0.85\n0.77\n0.79\nFine-Tuned\n0.96\n0.96\n0.96\nTop-20\nZero-Shot\n0.75\n0.58\n0.60\n3-Shot\n0.80\n0.71\n0.73\n5-Shot\n0.80\n0.71\n0.73\nFine-Tuned\n0.94\n0.94\n0.94\nTable 3: Results for classification.\n5.2.\nResults\nThe experimental results, summarized in Table 3,\nreveal a clear and consistent performance hierar-\nchy across all tested configurations. Fine-tuning the\nQwen3-1.7B model on the MedPT dataset yields\nthe highest performance by a significant margin,\nestablishing a strong benchmark and demonstrat-\ning the dataset’s value for model specialization. As\nexpected, performance for all methods degrades\ngracefully as the task difficulty increases from 5\nto 20 classes. Nonetheless, the fine-tuned model\nmaintains an exceptional 0.94 F1-score on the most\nchallenging 20-class task, underscoring the quality\nof the learned representations.\nThe efficacy of in-context learning (ICL) is also\nclearly demonstrated. In the 20-class setup, for ex-\nample, the zero-shot baseline achieves a 0.60 F1-\nscore, indicating the task’s inherent difficulty. How-\never, by providing just three examples (3-shot), the\nscore jumps to 0.73. This substantial improvement\nhighlights the model’s ability to quickly leverage the\ncontextual patterns within MedPT to enhance its\npredictive accuracy without full fine-tuning.\nA key finding from the ICL experiments is a clear\npoint of diminishing returns. The performance gain\nfrom zero-shot to 3-shot is substantial across all\nsettings. However, increasing the examples from\n3-shot to 5-shot provides only a marginal benefit\n(e.g., 0.73 F1-score for both in the 20-class setup),\n"}, {"page": 8, "text": "suggesting that a 3-shot prompt may represent the\noptimal balance of performance and efficiency for\nthis task.\n6.\nDiscussion\nThe experimental results confirm the high utility\nof MedPT for developing specialized medical lan-\nguage models. The exceptional performance of\nthe fine-tuned model, achieving a 0.94 F1-score\non the challenging 20-class task, directly validates\nthe dataset’s quality. This demonstrates that the\nlinguistic patterns and class labels within MedPT\nare consistent and highly learnable, providing a ro-\nbust foundation for domain-specific classifiers. Fur-\nthermore, our findings offer practical implications\nfor deployment: the substantial performance gains\nfrom in-context learning confirm MedPT’s value\nfor resource-light applications, while the observed\ndiminishing returns between 3-shot and 5-shot\nprompts suggest an optimal efficiency-performance\ntrade-off.\nA deeper qualitative analysis of the fine-tuned\nmodel’s errors revealed that misclassifications were\nnot random but systematic, occurring between se-\nmantically and clinically related subspecialties. We\nobserved notable confusion between conditions\nwith high comorbidity (e.g., Anxiety and Depres-\nsion), anatomical proximity (e.g., Umbilical Her-\nnia and Inguinal Hernia), or overlapping symp-\ntoms (e.g., Gastritis and Gastroesophageal Reflux).\nThis pattern indicates that the model is capturing\na meaningful semantic space of medical concepts,\nwhere its errors reflect genuine clinical ambiguity.\nThis semantic richness underscores MedPT’s po-\ntential to support future research into more complex\ntasks, such as fine-grained medical entity disam-\nbiguation.\n7.\nConclusion\nIn this work, we introduced MedPT, the first large-\nscale, real-world dataset of patient-doctor inter-\nactions in Brazilian Portuguese, containing over\n384,000 curated question-answer pairs. This re-\nsource was developed to address the critical gap\nof culturally and clinically relevant data for a major\nbut under-resourced language, capturing authentic\nuser inquiries that include endemic diseases and\nlocal health concerns often absent in translated\ndatasets. Our contribution extends beyond collec-\ntion to the rigorous, multi-stage curation protocol\nitself. We applied a hybrid quantitative-qualitative\nanalysis to every feature, using statistical distribu-\ntions to guide deep manual inspection. This metic-\nulous process, coupled with an LLM-driven anno-\ntation of user intent, ensured the dataset’s high\nsignal-to-noise ratio and structural richness.\nThis high utility was then confirmed through multi-\nfaceted validation. Quantitatively, downstream clas-\nsification experiments established a strong bench-\nmark, with models fine-tuned on MedPT achieving\nan outstanding 0.94 F1-score on a 20-class task.\nThis, combined with strong few-shot ICL results,\nproves the dataset’s practical value. By publicly re-\nleasing MedPT, we provide a foundational resource\nto the research community. We aim to foster the\ndevelopment of more equitable, accurate, and safe\nmedical NLP technologies, enabling a new gener-\nation of models that can understand the specific\nclinical and linguistic nuances of the Portuguese-\nspeaking world.\n8.\nLimitations\nWe acknowledge several limitations in this work\nthat also present clear avenues for future research.\nFirst, while our experimental validation focused ex-\nclusively on a multi-class classification task demon-\nstrates the dataset’s utility for a crucial applica-\ntion like inquiry routing, its performance on com-\nplex generative tasks, such as medical question-\nanswering and patient-doctor dialogue, remains to\nbe explored.\nSecond, regarding data quality, although all an-\nswers originate from verified physicians, a sec-\nondary, independent clinical review of the re-\nsponses was not conducted. Such a re-validation,\neven on a representative subset of the data, could\nfurther bolster the dataset’s reliability and ensure\nthe clinical accuracy of the provided information.\nFinally, our deduplication process was based on\nremoving exact-match samples or token-count strat-\negy. Future iterations of this work could employ\nmore sophisticated near-duplication detection tech-\nniques. While this would likely reduce the dataset’s\ntotal size, it could also enhance its cross-sample\nlinguistic diversity by filtering out highly similar, re-\ndundant entries.\n9.\nEthical Considerations\nThe MedPT dataset and any models developed\nusing it are intended to function as assistive tools\nto support, not replace, the judgment of qualified\nhealthcare professionals. We emphasize that AI\nsystems trained on this data should not be used as\na substitute for professional medical consultation,\ndiagnosis, or treatment. Relying solely on LLM-\ngenerated content for medical decisions carries\nsignificant risks, as models may produce outputs\nthat are plausible but clinically inaccurate. The pri-\nmary ethical application of this work is to augment\nclinical workflows, for tasks such as inquiry routing,\ninformation retrieval, preliminary symptom summa-\nrization, while ensuring that the final diagnostic and\n"}, {"page": 9, "text": "therapeutic decisions remain the exclusive respon-\nsibility of a human medical expert.\n10.\nBibliographical References\nMohammed Abdelhay, Ammar Mohammed, and\nHesham A Hefny. 2023. Deep learning for arabic\nhealthcare: Medicalbot. Social Network Analysis\nand Mining, 13(1):71.\nJosh Achiam,\nSteven Adler,\nSandhini Agar-\nwal,\nLama\nAhmad,\nIlge\nAkkaya,\nFloren-\ncia Leoni Aleman, Diogo Almeida, Janko Al-\ntenschmidt, Sam Altman, Shyamal Anadkat, et al.\n2023.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774.\nSandhini Agarwal, Lama Ahmad, Jason Ai, Sam\nAltman, Andy Applebaum, Edwin Arbus, Rahul K\nArora, Yu Bai, Bowen Baker, Haiming Bao, et al.\n2025. gpt-oss-120b & gpt-oss-20b model card.\narXiv preprint arXiv:2508.10925.\nHassan Alhuzali, Ashwag Alasmari, and Hamad Al-\nsaleh. 2024. Mentalqa: An annotated arabic cor-\npus for questions and answers of mental health-\ncare. IEEE Access.\nBalsam Alkouz,\nZaher Al Aghbari,\nand Je-\nmal Hussien Abawajy. 2019. Tweetluenza: Pre-\ndicting flu trends from twitter data. Big Data Min-\ning and Analytics, 2(4):273–287.\nMarco Cascella, Federico Semeraro, Jonathan\nMontomoli, Valentina Bellini, Ornella Piazza, and\nElena Bignami. 2024. The breakthrough of large\nlanguage models release for medical applica-\ntions: 1-year timeline and perspectives. Journal\nof Medical Systems, 48(1):22.\nBernardo Scapini Consoli, Henrique Dias P dos\nSantos, Ana Helena DPS Ulbrich, Renata Vieira,\nand Rafael Heitor Bordini. 2022. Brateca (brazil-\nian tertiary care dataset): a clinical information\ndataset for the portuguese language. In Pro-\nceedings of the 13th Conference on Language\nResources and Evaluation (LREC 2022), Mar-\nseille, 20-25 June, 2022, França.\nRicardo C Cury, Istvan Megyeri, Tony Lindsey, Rob-\nson Macedo, Juan Batlle, Shwan Kim, Brian\nBaker, Robert Harris, and Reese H Clark. 2021.\nNatural language processing and machine learn-\ning for detection of respiratory illness by chest\nct imaging and tracking of covid-19 pandemic\nin the united states. Radiology: Cardiothoracic\nImaging, 3(1):e200596.\nFábio André da Costa Lopes. 2019. Contributions\nto clinical information extraction in portuguese:\nCorpora, named entity recognition, word embed-\ndings. Master’s thesis, Universidade de Coimbra\n(Portugal).\nNastaran Enshaei and Farnoosh Naderkhani. 2024.\nThe role of data quality for reliable ai performance\nin medical applications. IEEE Reliability Maga-\nzine, 1(3):24–28.\nFernanda B Färber, Julia S Dollis, Pedro SFB\nRibeiro, Iago A Brito, Rafael T Sousa, and Ar-\nlindo R Galvão Filho. 2025. Iaramed: A women’s\nhealthcare chatbot for portuguese speakers. In\nSimpósio Brasileiro de Computação Aplicada à\nSaúde (SBCAS), pages 931–942. SBC.\nMehrdad\nGhassabi,\nPedram\nRostami,\nHamidreza Baradaran Kashani, Amirhossein\nPoursina, Zahra Kazemi, and Milad Tavakoli.\n2025.\nLeveraging online data to enhance\nmedical knowledge in a small persian language\nmodel. arXiv preprint arXiv:2505.16000.\nHaihong Guo, Xu Na, and Jiao Li. 2018. Qcorp:\nan annotated classification corpus of chinese\nhealth questions. BMC medical informatics and\ndecision making, 18(Suppl 1):16.\nJunqing He, Mingming Fu, and Manshu Tu. 2019.\nApplying deep matching networks to chinese\nmedical question answering:\na study and a\ndataset. BMC medical informatics and decision\nmaking, 19(Suppl 2):52.\nXuehai He, Shu Chen, Zeqian Ju, Xiangyu Dong,\nHongchao Fang, Sicheng Wang, Yue Yang, Jiaqi\nZeng, Ruisi Zhang, Ruoyu Zhang, et al. 2020.\nMeddialog: Two large-scale medical dialogue\ndatasets. arXiv preprint arXiv:2004.03329.\nBin Ji, Rui Liu, Shasha Li, Jie Yu, Qingbo Wu, Yu-\nsong Tan, and Jiaju Wu. 2019. A hybrid approach\nfor named entity recognition in chinese electronic\nmedical record. BMC medical informatics and\ndecision making, 19(Suppl 2):64.\nQiao\nJin,\nBhuwan\nDhingra,\nZhengping\nLiu,\nWilliam W Cohen, and Xinghua Lu. 2019. Pub-\nmedqa: A dataset for biomedical research ques-\ntion answering. arXiv preprint arXiv:1909.06146.\nAlistair EW Johnson, Lucas Bulgarelli, Lu Shen,\nAlvin Gayles, Ayad Shammout, Steven Horng,\nTom J Pollard, Sicheng Hao, Benjamin Moody,\nBrian Gow, et al. 2023. Mimic-iv, a freely acces-\nsible electronic health record dataset. Scientific\ndata, 10(1):1.\n"}, {"page": 10, "text": "Samir Khan, Seiji Tsutsumi, Takehisa Yairi, and\nShinichi Nakasuka. 2021.\nRobustness of ai-\nbased prognostic and systems health manage-\nment. Annual Reviews in Control, 51:130–152.\nQiming Liu, Ruirong Yang, Qin Gao, Tengxiao\nLiang, Xiuyuan Wang, Shiju Li, Bingyin Lei, and\nKaiye Gao. 2025. A review of applying large\nlanguage models in healthcare. IEEE Access,\n13:6878–6892.\nLing Luo, Po-Ting Lai, Chih-Hsuan Wei, Cecilia N\nArighi, and Zhiyong Lu. 2022. Biored: a rich\nbiomedical relation extraction dataset. Briefings\nin Bioinformatics, 23(5):bbac282.\nAndrés Alejandro Ramos Magna, Héctor Allende-\nCid, Carla Taramasco, Carlos Becerra, and\nRosa L Figueroa. 2020. Application of machine\nlearning and word embeddings in the classifica-\ntion of cancer diagnosis using patient anamnesis.\nIeee Access, 8:106198–106213.\nAndriy Mulyar, Ozlem Uzuner, and Bridget McInnes.\n2021. Mt-clinical bert: scaling clinical informa-\ntion extraction with multitask learning. Journal\nof the American Medical Informatics Association,\n28(10):2108–2115.\nPalanichamy Naveen and Pavel Trojovsk`y. 2024.\nOverview and challenges of machine transla-\ntion for contextually appropriate translations.”\niscience 27 (10): 110878.\nZ Al Nazi and W Peng. 2024.\nLarge language\nmodels in healthcare and medical domain: A\nreview. informatics, 11, 57.\nNhung Thi-Hong Nguyen, Phuong Phan-Dieu Ha,\nLuan Thanh Nguyen, Kiet Van Nguyen, and Ngan\nLuu-Thuy Nguyen. 2022. Spbertqa: A two-stage\nquestion answering system based on sentence\ntransformers for medical texts. In International\nConference on Knowledge Science, Engineering\nand Management, pages 371–382. Springer.\nVincent Nguyen, Sarvnaz Karimi, Maciej Rybin-\nski, and Zhenchang Xing. 2023. Medredqa for\nmedical consumer question answering: Dataset,\ntasks, and neural baselines. In Proceedings of\nthe 13th International Joint Conference on Natu-\nral Language Processing and the 3rd Conference\nof the Asia-Pacific Chapter of the Association for\nComputational Linguistics (Volume 1: Long Pa-\npers), pages 629–648.\nLucas Emanuel Silva e Oliveira, Ana Carolina Pe-\nters, Adalniza Moura Pucca Da Silva, Caroline Pi-\nlatti Gebeluca, Yohan Bonescki Gumiel, Lilian\nMie Mukai Cintho, Deborah Ribeiro Carvalho,\nSadid Al Hasan, and Claudia Maria Cabral Moro.\n2022. Semclinbr-a multi-institutional and multi-\nspecialty semantically annotated corpus for por-\ntuguese clinical nlp tasks. Journal of Biomedical\nSemantics, 13(1):13.\nPedro Henrique Paiola, Gabriel Lino Garcia, Joao\nRenato Ribeiro Manesco, Mateus Roder, Dou-\nglas Rodrigues, and João Paulo Papa. 2024.\nAdapting llms for the medical domain in por-\ntuguese: A study on fine-tuning and model eval-\nuation. arXiv preprint arXiv:2410.00163.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-\nscale multi-subject multi-choice dataset for med-\nical domain question answering. In Proceed-\nings of the Conference on Health, Inference, and\nLearning, volume 174 of Proceedings of Machine\nLearning Research, pages 248–260. PMLR.\nSam Preston, Mu Wei, Rajesh Rao, Robert Tinn,\nNaoto Usuyama, Michael Lucas, Yu Gu, Roshan-\nthi Weerasinghe, Soohee Lee, Brian Piening,\net al. 2023. Toward structuring real-world data:\nDeep learning for extracting oncology information\nfrom clinical text with patient-level supervision.\nPatterns, 4(4).\nPhilippa Ryan Conmy, Berk Ozturk, Tom Lawton,\nand Ibrahim Habli. 2023. The impact of train-\ning data shortfalls on safety of ai-based clinical\ndecision support systems. In Computer Safety,\nReliability, and Security, pages 213–226, Cham.\nSpringer Nature Switzerland.\nDaniel Schwabe, Katinka Becker, Martin Seyferth,\nAndreas Klaß, and Tobias Schaeffter. 2024. The\nmetric-framework for assessing data quality for\ntrustworthy ai in medicine: a systematic review.\nNPJ digital medicine, 7(1):203.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara\nMahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis,\nStephen Pfohl, et al. 2022. Large language mod-\nels encode clinical knowledge. arXiv preprint\narXiv:2212.13138.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\nClark, Stephen R Pfohl, Heather Cole-Lewis,\net al. 2025. Toward expert-level medical question\nanswering with large language models. Nature\nMedicine, 31(3):943–950.\nJing Wang, Huan Deng, Bangtao Liu, Anbin Hu,\nJun Liang, Lingye Fan, Xu Zheng, Tong Wang,\nand Jianbo Lei. 2020. Systematic evaluation of\nresearch progress on natural language process-\ning in medicine over the past 20 years: bibliomet-\nric study on pubmed. Journal of medical Internet\nresearch, 22(1):e16816.\n"}, {"page": 11, "text": "Stephen\nWu,\nKirk\nRoberts,\nSurabhi\nDatta,\nJingcheng Du, Zongcheng Ji, Yuqi Si, Sarvesh\nSoni, Qiong Wang, Qiang Wei, Yang Xiang,\nBo Zhao, and Hua Xu. 2019.\nDeep learning\nin clinical natural language processing: a me-\nthodical review. Journal of the American Medical\nInformatics Association, 27(3):457–470.\nAn Yang, Anfeng Li, Baosong Yang, Beichen\nZhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, et al.\n2025. Qwen3 technical report. arXiv preprint\narXiv:2505.09388.\nSheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo,\nand Shanshan Liu. 2018. Multi-scale attentive in-\nteraction networks for chinese medical question\nanswer selection. IEEE Access, 6:74061–74071.\nEnwei Zhu, Qilin Sheng, Huanwan Yang, Yiyang\nLiu, Ting Cai, and Jinpeng Li. 2023. A unified\nframework of medical information annotation and\nextraction for chinese clinical text. Artificial Intel-\nligence in Medicine, 142:102573.\nMert İncidelen and Murat Aydoğan. 2024. Develop-\ning question-answering models in low-resource\nlanguages: A case study on turkish medical texts\nusing transformer-based approaches. In 2024\n8th International Artificial Intelligence and Data\nProcessing Symposium (IDAP), pages 1–4.\n"}]}