{"doc_id": "arxiv:2601.06519", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.06519.pdf", "meta": {"doc_id": "arxiv:2601.06519", "source": "arxiv", "arxiv_id": "2601.06519", "title": "MedRAGChecker: Claim-Level Verification for Biomedical Retrieval-Augmented Generation", "authors": ["Yuelyu Ji", "Min Gu Kwak", "Hang Zhang", "Xizhi Wu", "Chenyu Li", "Yanshan Wang"], "published": "2026-01-10T10:40:42Z", "updated": "2026-01-10T10:40:42Z", "summary": "Biomedical retrieval-augmented generation (RAG) can ground LLM answers in medical literature, yet long-form outputs often contain isolated unsupported or contradictory claims with safety implications.   We introduce MedRAGChecker, a claim-level verification and diagnostic framework for biomedical RAG.   Given a question, retrieved evidence, and a generated answer, MedRAGChecker decomposes the answer into atomic claims and estimates claim support by combining evidence-grounded natural language inference (NLI) with biomedical knowledge-graph (KG) consistency signals.   Aggregating claim decisions yields answer-level diagnostics that help disentangle retrieval and generation failures, including faithfulness, under-evidence, contradiction, and safety-critical error rates.   To enable scalable evaluation, we distill the pipeline into compact biomedical models and use an ensemble verifier with class-specific reliability weighting.   Experiments on four biomedical QA benchmarks show that MedRAGChecker reliably flags unsupported and contradicted claims and reveals distinct risk profiles across generators, particularly on safety-critical biomedical relations.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.06519v1", "url_pdf": "https://arxiv.org/pdf/2601.06519.pdf", "meta_path": "data/raw/arxiv/meta/2601.06519.json", "sha256": "aa1d1f0a705359f6abccabad0b4dea1ed7ca148d56ad5ab4d3f1054f3e63e53f", "status": "ok", "fetched_at": "2026-02-18T02:21:58.378296+00:00"}, "pages": [{"page": 1, "text": "MedRAGChecker: Claim-Level Verification for Biomedical\nRetrieval-Augmented Generation\nYuelyu Ji\nMin Gu Kwak\nHang Zhang\nXizhi Wu\nChenyu Li\nYanshan Wang\nUniversity of Pittsburgh, Pittsburgh, PA, USA\nyueluji@gmail.com\nAbstract\nBiomedical retrieval-augmented generation\n(RAG) can ground LLM answers in med-\nical literature, yet long-form outputs often\ncontain isolated unsupported or contradictory\nclaims with safety implications. We introduce\nMEDRAGCHECKER, a claim-level verification\nand diagnostic framework for biomedical RAG.\nGiven a question, retrieved evidence, and a gen-\nerated answer, MEDRAGCHECKER decom-\nposes the answer into atomic claims and esti-\nmates claim support by combining evidence-\ngrounded natural language inference (NLI)\nwith biomedical knowledge-graph (KG) con-\nsistency signals. Aggregating claim decisions\nyields answer-level diagnostics that help disen-\ntangle retrieval and generation failures, includ-\ning faithfulness, under-evidence, contradiction,\nand safety-critical error rates. To enable scal-\nable evaluation, we distill the pipeline into com-\npact biomedical models and use an ensemble\nverifier with class-specific reliability weighting.\nExperiments on four biomedical QA bench-\nmarks show that MEDRAGCHECKER reliably\nflags unsupported and contradicted claims and\nreveals distinct risk profiles across generators,\nparticularly on safety-critical biomedical rela-\ntions.\n1\nIntroduction\nLarge language models (LLMs) have shown re-\nmarkable ability in biomedical question answering,\nbut ungrounded generations can be dangerously\nwrong. Hallucination—producing plausible but\nfactually incorrect statements—is a major concern\nin clinical settings (Yu et al., 2025; Singhal et al.,\n2025; Vladika et al., 2024; Pandit et al., 2025; Liu\nand Yu, 2025). Even powerful systems such as\nGPT-4.1 or Med-PaLM2 can provide unsupported\nmedical advice. Retrieval-augmented generation\nGround Truth Claims\nG1 (✓ Covered) \"Propranolol\", \"is \na\", \"non-selective beta-blocker\"\nG2 (✓ Covered) \"Non-selective \nbeta-blockers\", \"can exacerbate\", \n\"bronchoconstriction in patients with \nasthma\"\nG3 (✗ Contradicted) \"Propranolol\", \n\"is not recommended for\", \"patients \nwith asthma due to bronchospasm \nrisk\"\nG4 (△ Missing) \"If a beta-blocker is \nnecessary\", \"consider\", \"a \ncardioselective beta-1 blocker with \nmonitoring\"\nClaims \nflagged as \nContradict\nClaims \nflagged as \nEntail\nMissing \nClaims\nGround \nTruth \nAnswer \nModel \nResponse\nQuestion: Can patients with asthma safely use propranolol?\nModel Response Claims \nC1 (✓ Match GT) \nPropranolol is a  non-selective \nbeta-blocker\nC2 (✓ Match GT) \nNon-selective beta-blockers\", \n\"can exacerbate\", \n\"bronchoconstriction in \npatients with asthma\"\nC3 (✗ Contradict GT) \n\"Propranolol\", \"is generally \nconsidered safe for\", \"patients \nwith asthma when used at low \ndoses\"\nFigure 1: Example of claim-level verification in biomed-\nical RAG. We decompose both the model response and\nthe reference answer into atomic claims. Each model\nclaim is labeled as ENTAIL (supported) or CONTRA-\nDICT given retrieved evidence, and reference claims not\ncovered by the model are marked as Missing.\n(RAG) partly addresses this by conditioning an-\nswers on up-to-date medical literature (Achiam\net al., 2023; Tu et al., 2024; Lewis et al., 2020).\nHowever, noisy retrieval and weak grounding still\nlead to errors: when irrelevant or contradictory doc-\numents are retrieved, the model may latch onto mis-\nleading snippets and hallucinate. MedTrust-RAG\n(Ning et al., 2025) even shows that adding retrieved\ntext can flip a correct answer into an incorrect one.\nTherefore, factual reliability has become a central\nconcern in recent LLM factuality studies (Wang\net al., 2023, 2024).\nLong-form biomedical answers contain many\ndistinct factual claims, and whole-answer scoring\ncan hide isolated but clinically important mistakes.\n1\narXiv:2601.06519v1  [cs.CL]  10 Jan 2026\n"}, {"page": 2, "text": "Recent work argues for atomic, claim-level evalu-\nation: Med-PaLM’s verifier decomposes answers\ninto statements and searches evidence for each,\nwhile FActScore measures factual precision over\natomic claims (Singhal et al., 2025; Min et al.,\n2023). Figure 1 illustrates claim-level verification\noutcomes in biomedical RAG, including supported\n(Entail), contradicted (Contradict), and missing ref-\nerence claims.\nRagChecker further uses claims to diagnose be-\nhaviors such as hallucination and self-knowledge\n(Ru et al., 2024). However, in medical settings,\nretrieved passages may contain correlational state-\nments that look supportive but do not justify\ncausal/treatment claims; claims can be linguisti-\ncally supported yet violate known drug–disease\ncontraindication relations. This motivates augment-\ning claim verification with structured biomedical\nknowledge graphs (KGs).\nWe introduce MEDRAGCHECKER, a claim-\nlevel verification framework for biomedical RAG\n(Figure 2). Given a question, retrieved evidence,\nand a long-form answer, MEDRAGCHECKER de-\ncomposes the answer into atomic claims and as-\nsigns each claim a calibrated support confidence by\ncombining (i) textual natural language inference\n(NLI) verification supervised by LLM supervision\n(GPT-4.1 or GPT-4o) and (ii) KG-based support\ncomputed by linking entities to a Drug Repurposing\nKnowledge Graph (DRKG)-style biomedical graph\n(Ioannidis et al., 2020). To make the pipeline prac-\ntical, we distill teacher supervision into compact\nbiomedical student models (Li et al., 2025c,b,a;\nMa et al., 2025b,a) and validate the resulting di-\nagnostics with human judgments, complementing\nLLM-as-judge approaches (Wang et al., 2023; Li\net al., 2024; Min et al., 2023).\nIn summary, our contributions are: (1) A claim-\nlevel diagnostic framework for biomedical RAG.\nWe extract atomic claims from long-form answers\nand verify each claim to produce per-claim confi-\ndence scores and answer-level hallucination diag-\nnostics. (2) KG-enhanced biomedical verifica-\ntion. We augment text-only claim verification by\nanchoring claims to a DRKG-style biomedical KG\nand computing a soft KG support signal (Ioanni-\ndis et al., 2020). (3) Teacher-distilled, efficient\nchecking. We distill GPT-4.1 supervision into com-\npact biomedical student models for claim extrac-\ntion and verification (Li et al., 2025c,b,a; Ma et al.,\n2025b,a). (4) Human-aligned evaluation. We\nevaluate on multiple biomedical QA benchmarks\nand calibrate MedRAGChecker diagnostics against\nhuman judgments, complementing prior factual-\nity evaluation work (Wang et al., 2023; Li et al.,\n2024; Min et al., 2023). Our code is available at\nhttps://anonymous.4open.science/r/Medica\nlRagChecker-752E/.\n2\nRelated Work\nBiomedical QA.\nBiomedical QA benchmarks\n(e.g., MedQA, MedMCQA, PubMedQA) drove\ndomain-specific LMs, and systems such as Med-\nPaLM2 achieved strong performance on standard-\nized medical exams (Jin et al., 2020; Pal et al.,\n2022; Jin et al., 2019).\nHowever, long-form\nbiomedical answers remained prone to hallucina-\ntions (Asgari et al., 2025; Huang et al., 2025), mak-\ning safety-critical validation necessary.\nRAG Evaluation and Fact-Checking.\nRAG fac-\ntuality checking was often formulated as an NLI-\nstyle claim-evidence problem, where systems split\ngenerations into statements and verified them by\nretrieving supporting evidence (Lewis et al., 2020;\nFu et al., 2024; Xin et al., 2025a,b). Related works,\nsuch as RAGAS (Es et al., 2024) and RAGChecker\n(Ru et al., 2024), evaluated the quality of re-\ntrieval and grounding at the answer-level scores.\nIn contrast, MEDRAGCHECKER operated at the\nclaim level and then aggregated verified claims\ninto answer-level diagnostics, integrating an ad-\nditional structured KG signal to assess claim ve-\nracity and evidence sufficiency. Domain-specific\nfact-checkers like HealthFC (Vladika et al., 2024)\noffered strong baselines for clinical validation, but\ntypically assumed short, well-formed statements.\nOur work, targeting multi-sentence RAG answers,\nprovides a more fine-grained approach.\nKnowledge Graphs for QA.\nBiomedical KGs\nwere applied in QA to support reasoning and\ndecision-making, such as in diagnostic KGs for\ndisease-centric applications (Himmelstein et al.,\n2017). We built on this by using a biomedical KG\nas an external verifier, mapping claims to KG en-\ntities and relations to complement text-based NLI,\nimproving robustness. Because biomedical KGs\nwere incomplete (Chen et al., 2022), fusing KG evi-\ndence with textual entailment enhanced consistency\nin answering safety-critical biomedical questions.\n2\n"}, {"page": 3, "text": "Question\nRetrieved Context \nLLM \nGenerate \nAnswer\nClaim \nExtractor \nC 1\nC 2\nC n\n…\nGround \nTruth \nAnswer\nClaim \nExtractor \nC 1\nC 2\nC m\n…\nTextural NLI Checker\nTextual entailment under retrieved evidence\nAtomic \nClaim  C\nRetrieved \nContext \nTeacher \nNLI  \nVerifier \nNLI Score \n& Label\n(Entail/Neutral/Con\ntradict)\nMulti-signal \nClaim \nJudgement\nEntity \nLinking\nKG Lookup \n& Path \nBiomedical KG\nKG Support \nScore\nKG-Based Verification\nGraph-based factual grounding \nStudent Model Training  \nStudent \nModels \nSFT training\nFinal \nClaim \nTeacher Claim for Training\n[\"subject\", \"relation\", \"object\"]\nTeacher Label for \nTraining  \n(NLI: Entail, Contradict \nNeutral)\nFigure 2: Overview of MEDRAGCHECKER. For each atomic claim, textual NLI checking (purple) and KG-based\nverification (orange) provide complementary signals—semantic entailment from retrieved context versus structured\nbiomedical constraints—which are fused and distilled into a student verifier (grey).\n3\nMedRAGChecker\nFigure 2 overviews the pipeline. MedRAGChecker\ncombines a textual NLI signal from teacher-\ndistilled student checkers with a KG-based con-\nsistency signal from DRKG, and fuses them into\na single calibrated support score used by all down-\nstream diagnostics.\n3.1\nTask Definition and Outputs\nWe cast biomedical RAG evaluation as claim-\ncentric verification.\nGiven a RAG instance\n(q, D, a),\nwhere\nq\nis\nthe\nquestion,\na\nis\nthe generated answer,\nand D\n=\n{dj}k\nj=1\ndenotes\nthe\ntop-k\nretrieved\nevidence\npas-\nsages/documents, MEDRAGCHECKER decom-\nposes a into atomic claims C = {c1, . . . , cn}\nand assigns each claim (1) a discrete verdict\nˆyi ∈{ENTAIL, NEUTRAL, CONTRADICT} and\n(2) a calibrated support score s(ci)\n∈\n[0, 1]\nindicating how likely the claim is supported under\nthe available evidence.\nIn our implementation, the textual checker pro-\nvides pNLI(ci) ≜P(ENTAIL | ci, D) from the\nensemble distribution. When KG evidence is avail-\nable, we further fuse this textual signal with KG\nsupport to obtain the final support score (Sec-\ntion 3.5).\n3.2\nClaim Extraction (Teacher →Student)\n3.2.1\nTeacher supervision\nWe use GPT-4.1 as a teacher to generate pseudo-\nlabels for both claim extraction and claim verifi-\ncation. For each (q, D, a) triple, we first prompt\nteacher model to extract atomic claims (Sec-\ntion 3.2.1).\nWe then obtain NLI-style verifica-\ntion labels and entailment probabilities for each\nextracted claim using teacher model (Section 3.3).\nClaim extraction: teacher model decomposes\nthe answer into a list of short, atomic statements\nc1, . . . , cn that are intended to capture the factual\ncontent of a, following the spirit of FActScore-style\natomic evaluation.(Min et al., 2023)\nThese teacher outputs give us supervision sig-\nnal for training both the claim extractor and the\nchecker. We treat teacher labels as noisy but strong\nteacher claim and labels compared to automatic\nheuristics, and use them on the training and devel-\nopment splits rather than as definitive human labels.\nCombined with a small human study (Section 4.5),\nthis allows us to approximate claim-level supervi-\nsion at scale while acknowledging that conclusions\nare conditional on the choice of teacher model.\n3.2.2\nStudent claim extractor\nTo avoid calling teacher model at inference time\ndue to its costs and time, we distill a student claim\n3\n"}, {"page": 4, "text": "extractor. The student is a biomedical LLM (e.g.,\nMeditron3-8B or Med42-Llama3-8B) fine-tuned to\nmap the question-answer pair (q, a) to a list of tex-\ntual spans that closely match the teacher’s claims.\nTraining data consists of teacher claim lists, lin-\nearized as a sequence of numbered bullet points.\nWe fine-tune the student with a standard sequence-\nto-sequence SFT objective on (q, a) →c1, . . . , cn.\nAt evaluation time, we compare the extracted\nclaims with the output of the teacher model us-\ning precision, recall, and F1 at the span-level, and\nwe also ask human annotators to rate the overall\nquality of the claim set on a 1–5 Likert scale (Sec-\ntion G). Empirically, the distilled extractor recovers\nmost atomic facts while introducing few halluci-\nnated claims; this makes it suitable as the front-end\nfor MedRAGChecker when the teacher model is\nnot available.\n3.3\nTextual Verification via NLI (Student\nCheckers + Ensemble)\nFor\neach\nextracted\nclaim\nci,\nthe\nteacher\nmodel\nis\nprompted\nwith\nthe\nretrieved\nev-\nidence\nD\nand\nasked\nto\nassign\na\nlabel\nyi\n∈\n{ENTAIL, NEUTRAL, CONTRADICT}\ntogether with an entailment probability pNLI(ci),\nframed as a natural language inference task\n(evidence as premise, claim as hypothesis). These\nteacher outputs provide teacher label supervision\nfor distilling student claim checkers.\n3.3.1\nStudent claim checkers\nWe similarly distill the claim verifier into com-\npact student checkers. Each training example con-\nsists of an extracted claim ci, the associated re-\ntrieved context D, and the teacher model verdict\nyi ∈{Entail, Neutral, Contradict}.\nWe consider several biomedical LLMs as stu-\ndents, including Meditron3-8B (Sallinen et al.,\n2025), PMC-LLaMA-13B (Wu et al., 2024), Med-\nQwen2-7B (Bai et al., 2023), and Med42-Llama3-\n8B (Christophe et al., 2024) using a standard cross-\nentropy loss over the three NLI labels.This super-\nvised fine-tuning already brings all students close\nto the teacher model in overall accuracy (Table 1),\nand we use these SFT checkers throughout the main\nexperiments. We also experimented with GRPO-\nbased refinement of the SFT checkpoints; how-\never, the improvements were not consistent across\nmodels and often came with drops in overall ac-\ncuracy. We therefore report GRPO results only in\nAppendix F and keep SFT-only checkers as our\ndefault.\n3.3.2\nF1-weighted ensemble\nBecause different student checkers specialize in\ndifferent classes, we build a lightweight ensemble\nthat combines their predictions using per-class F1\nscores as reliability weights.\nLet pm(y | ci, D) be the predicted probability\nthat the student checker m assigns the label y ∈\n{Entail, Neutral, Contradict} to claim ci. From the\ndev set, we compute per-class F1 scores F1(y)\nm for\neach checker. We then define class-specific weights\nw(y)\nm =\nF1(y)\nm\nP\nm′ F1(y)\nm′\n.\n(1)\nAt inference time, the ensemble score for label y is\ns(y | ci, D) =\nX\nm\nw(y)\nm pm(y | ci, D),\n(2)\nand\nthe\nfinal\ndiscrete\nverdict\nis\nˆyi\n=\narg maxy s(y | ci, D).\nIntuitively, the ensemble allows models that are\nstrong on Neutral to dominate that class, while\nmodels that are better at spotting contradictions\n(e.g., Med-Qwen2-7B or Med42-Llama3-8B) con-\ntribute more to CONTRADICT} decisions.\nDefault checker.\nUnless otherwise stated, we use\nthe F1-weighted ensemble as the default checker to\nproduce pNLI(ci); when KG fusion is enabled, we\nthen combine it with the KG signal to obtain the\nfinal support score (Section 3.5).\n3.4\nKG Support via DRKG\n3.4.1\nEntity/relation linking\nFor each claim c, we construct a candidate set of\naligned KG triples A(c) = {(hj, rj, tj)}j. We\nfirst identify a subject mention and an object men-\ntion in c using simple heuristics over the extracted\nSPO form (Appendix C), and perform string-based\nmatching against DRKG entity names to obtain can-\ndidate heads hj and tails tj. We similarly map the\ncanonical claim relation (e.g., TREATS, CAUSES) to\nDRKG edge types to obtain candidate relations rj.\nFor each candidate triple (hj, rj, tj) we compute\na text alignment score stext(c, j) ∈[0, 1] by com-\nbining normalized string similarity for the subject,\nrelation, and object (details in Appendix C), and\ndefine the claim-level alignment score as\nstext(c) = max\nj\nstext(c, j).\n(3)\n4\n"}, {"page": 5, "text": "3.4.2\nSoft support with TransE\nWe score each aligned candidate triple (h, r, t) ∈\nA(c) using TransE (Bordes et al., 2013):\ndTransE(h, r, t) = ∥eh + rr −et∥2\n(4)\npKGE(h, r, t) = σ(−dTransE(h, r, t)) .\n(5)\nWe then aggregate a claim-level embedding plausi-\nbility score by\npKGE(c) =\nmax\n(h,r,t)∈A(c) pKGE(h, r, t).\n(6)\nIf A(c) is empty, the claim is treated as KG-\nuncovered.\n3.5\nSignal Fusion and Calibrated Support\nScore P ⋆\nFor claims that can be aligned to DRKG, we com-\npute a KG consistency score as a soft prior, com-\nbining embedding-based plausibility and text-level\nalignment:\nsKG(ci) = (1 −α) pKGE(ci) + α stext(ci),\n(7)\nwhere α ∈[0, 1] is tuned on a development set. We\nthen fuse the textual NLI probability pNLI(ci) from\nthe ensemble checker with the KG score using a\nlogistic mixture:\nP ⋆(ci) = σ\n\u0010\nβ · logit\n\u0000pNLI(ci)\n\u0001\n+ (1 −β) · logit\n\u0000sKG(ci)\n\u0001\u0011\n,\n(8)\nwhere β controls the relative weight of text vs. KG\nevidence and σ is the sigmoid function. For claims\nthat cannot be mapped to DRKG entities, we fall\nback to P ⋆(ci) = pNLI(ci). We fuse the two sig-\nnals in logit space to avoid one score dominating\nthe mixture purely due to scale mismatch in proba-\nbility space. This fusion treats both components as\nsupport proxies and yields a single support score\nused consistently across diagnostics.\nAll downstream MedRAGChecker diagnos-\ntics (claim faithfulness, hallucination rate, con-\ntext precision, and safety-critical error rate) are\ncomputed from P ⋆(ci) and the predicted En-\ntail/Neutral/Contradict labels. In experiments we\nfocus on safety-critical subsets where DRKG cover-\nage is reasonably high, such as drug–disease treat-\nment, drug–side-effect and gene–disease claims. In\nall experiments we tune the fusion weights α and\nβ and the decision threshold τ by grid search on\na held-out development split, maximizing macro-\nF1 on claim-level verification. Once selected, the\nsame (α, β, τ) configuration is fixed across all test\nsets. For ablations comparing NLI-only vs. Fused,\nwe keep the same dev-tuned support threshold τ\nwhen computing supported/unsupported statistics\n(e.g., CLAIMREC and CTXPREC) to ensure com-\nparability. SELFKNOW is computed with KG fu-\nsion disabled and uses the NLI-only threshold τNLI\n(Section 3.6).\n3.6\nMetrics\nNotation.\nFor an example (q, D, a), the extractor\noutputs claims C = {ci}n\ni=1. The checker predicts\na label ˆyi ∈{ENTAIL, NEUTRAL, CONTRADICT}\nand a fused confidence P ⋆(ci) ∈[0, 1] (Eq. 8). In\nthe fused setting, we propagate the fused entail-\nment signal to the 3-way decision: we replace the\ntextual NLI ENTAIL logit with the fused entail logit\nimplied by P ⋆(ci) (Eq. 8), while keeping the NEU-\nTRAL and CONTRADICT logits from the textual\nNLI checker unchanged. We then renormalize with\na softmax to obtain a 3-way distribution and predict\nˆyi = arg maxy p(y | ci, D).\nWhen computing binary supported/unsupported\nstatistics, we use a threshold τ (tuned on dev) and\ndefine Isup(ci) = I[P ⋆(ci) ≥τ].\nClaim extraction quality.\nAgainst teacher-\nextracted claims (GPT-4.1), we compute soft span-\nlevel precision/recall/F1 by matching each pre-\ndicted claim to the most similar teacher claim using\na token-overlap similarity (details in Appendix C).\nWe also report the average number of extracted\nclaims per answer.\nVerification metrics (claim-level).\nGiven claim-\nlevel teacher labels yi from the teacher model (or\nhuman labels when available), we report accuracy\nand Macro-F1 over the three NLI classes.\nDiagnostic metrics (answer-level).\nWe aggre-\ngate claim outputs to characterize generator and\nretrieval behaviors:\nFaith(a) = 1\nn\nn\nX\ni=1\nI[ˆyi = ENTAIL],\n(9)\nHalluc(a) = 1\nn\nn\nX\ni=1\nI[ˆyi = CONTRADICT]. (10)\nRetrieval diagnostics.\nLet Cref be reference\n(gold) claims extracted from the dataset reference\n5\n"}, {"page": 6, "text": "answer. We define claim recall as the fraction of\nreference claims that are supported by the retrieved\nevidence:\nClaimRec =\n1\n|Cref|\nX\nc∈Cref\nIsup(c).\n(11)\nClaimRec only reported where references are avail-\nable.\nWe define context precision as the fraction of\nretrieved passages that are used as evidence for at\nleast one supported claim (teacher provides evi-\ndence spans):\nCtxPrec = 1\nk\nk\nX\nj=1\nI\nh\n∃i : dj is cited for ci\n∧P ⋆(ci) ≥τ\ni\n.\n(12)\nClaimF1 (answer-level, reference-claim overlap).\nLet Cref be reference claims extracted from the\ndataset reference answer, and let Cgen be claims\nextracted from the generated answer. We compute\nPrecision/Recall by matching each generated claim\nto the most similar reference claim (token-overlap\nmatching as in Appendix C) and counting a match\nas correct if it is supported (P ⋆≥τ). ClaimF1 is\nthe harmonic mean of this Precision and Recall.\nSelf-knowledge (parametric, NLI-only).\nTo es-\ntimate parametric knowledge usage, we disable\nKG fusion and rerun the textual NLI checker with\nan empty context ∅. Let pNLI(ci | ∅) denote the\nensemble entailment probability when no retrieved\npassages are provided. We compute\nSelfKnow = 1\nn\nn\nX\ni=1\nI[pNLI(ci | ∅) ≥τNLI] ,\n(13)\nwhere τNLI is tuned on the dev split using NLI-\nonly scores. This isolates parametric support from\nexternal structured evidence (KG).\nSafety-critical error rate.\nLet Csafety ⊆C be\nclaims mapped to safety-critical biomedical rela-\ntions (e.g., drug–disease, drug–side-effect) by our\nDRKG linker. We report\nSafetyErr =\n1\n|Csafety|\nX\nc∈Csafety\nI[ˆy(c) = CONTRADICT],\n(14)\nand optionally the same metric under the fused\nscore threshold τ.\n4\nExperiments\n4.1\nExperimental Setup\nData and RAG setup.\nWe evaluated on four\nbiomedical QA benchmarks:\nPubMedQA (Jin\net al., 2019), MedQuAD (Ben Abacha and Demner-\nFushman, 2019), LiveQA (Yang et al., 2017), and\nMedRedQA (Nguyen et al., 2023). We used a\nPubMed-based RAG pipeline with fixed retrieval\nsettings across runs and compared four biomed-\nical generators (Meditron3-8B, PMC-LLaMA-\n13B, Med-Qwen2-7B, Med42-Llama3-8B) plus a\ngeneral-domain baseline (LLaMA-3-8B-Instruct).\nDataset and retrieval details are in Appendix A and\nAppendix J.\nChecker training and evaluation protocol.\nWe\nused GPT-4.1 to provide claims and NLI labels for\nclaim extraction and NLI verification on train/dev,\nand distilled (i) a student claim extractor and (ii)\nstudent NLI checkers; unless stated otherwise, we\nused the F1-weighted ensemble (Section 3.3.2)\nfor downstream diagnostics.\nWe report three\nverification settings that differ only in who per-\nforms extraction/checking: teacher-checker (GPT-\n4.1), single-student, and student-ensemble. We\nsplit the teacher-labeled data into train/dev/test\n(80/10/10) and fine-tuned open-source students\nwith LoRA; training details are in Appendix L.\nFor KG-enhanced verification, we computed a\nDRKG-based support score for aligned claims (Sec-\ntion 3.4.1) and fused it with textual NLI via Eq. 8;\n(α, β, τ) were tuned on dev and then fixed for all\ntest results. GRPO refinements were exploratory\nand are reported in Appendix F; unless noted, all\ntest results use student models only (no GPT-4.1\ncalls at inference).\nWe computed a DRKG-based support score for\naligned claims (Section 3.4.1) and fused it with\ntextual NLI via Eq. 8. Hyperparameters (α, β, τ)\nwere tuned on dev and then fixed for all test re-\nsults. In RQ2, we focused on KG-aligned safety-\ncritical claim subsets (e.g., drug–disease, drug–\nadverse event, gene–disease), where KG coverage\nwas meaningful.\n4.2\nResearch Questions\nOur experiments addressed: RQ1: How well did\ndistilled extractors/checkers match the teacher?\nRQ2: Did DRKG fusion improve verification, es-\npecially for safety-critical claims? RQ3: Did end-\nto-end diagnostics meaningfully differentiate gen-\n6\n"}, {"page": 7, "text": "Model\nPanel A: Extractor\nPanel B: Checker\nP ↑\nR ↑\nF1 ↑\n#Claims\nTraining\nAcc (%)\nMacro-F1 (%)\nF1-E (%)\nF1-N (%)\nF1-C (%)\nMed-Qwen2-7B\n11.9\n12.5\n12.2\n3.25\nSFT\n83.7\n55.0\n40.1\n94.9\n42.4\nMed42-Llama3-8B\n21.4\n26.7\n23.7\n4.70\nSFT\n85.6\n67.3\n50.3\n94.5\n27.3\nMeditron3-8B\n20.5\n25.1\n22.6\n5.28\nSFT\n81.2\n49.7\n46.3\n92.9\n23.4\nPMC-LLaMA-13B\n0.3\n0.2\n0.2\n1.19\nSFT\n84.5\n43.8\n34.1\n97.2\n53.1\nEnsemble (F1-weighted)\n87.4\n60.5\n52.8\n95.0\n41.3\nTable 1: Distilled claim extractor and checker fidelity to the GPT-4.1 teacher. Panel A: Soft span-level P/R/F1 under\ntoken-overlap matching to teacher atomic claims; absolute values are conservative because the teacher produces\nhighly granular claims and matching requires near-surface overlap. Panel B: Checker accuracy and per-class F1 on\nthe 3-way NLI task. Unless stated otherwise, we use the F1-weighted ensemble as the default checker in downstream\nexperiments.\nTeacher-checker (GPT-4.1)\nStudent-checker (SFT ensemble)\nModel\nSetting\nFaith. ↑\nHalluc. ↓\nSafetyErr ↓\nFaith. ↑\nHalluc. ↓\nSafetyErr ↓\nMed-Qwen2-7B\nNLI-only\n70.1\n21.8\n29.9\n62.3\n20.7\n19.6\nFused (ours)\n75.6\n18.6\n24.4\n68.6\n10.4\n12.6\nMed42-Llama3-8B\nNLI-only\n70.2\n18.1\n29.8\n67.3\n24.3\n28.9\nFused (ours)\n77.0\n23.0\n17.5\n58.4\n25.7\n19.6\nMeditron3-8B\nNLI-only\n58.2\n20.6\n41.8\n52.7\n14.6\n29.0\nFused (ours)\n66.8\n33.2\n23.7\n59.6\n27.3\n27.2\nPMC-LLaMA-13B\nNLI-only\n23.2\n38.8\n76.8\n15.3\n57.3\n63.2\nFused (ours)\n22.2\n77.8\n67.3\n21.3\n73.4\n54.3\nTable 2: The results for the generation models in the overall seting. KG fusion ablation on KG-aligned claims. For\nboth the teacher-checker and student-checker columns, we evaluate on the same set of atomic claims extracted by\nthe teacher model and restrict to claims that are aligned to DRKG. Teacher-checker uses the teacher model for NLI\nlabeling, while student-checker uses the SFT ensemble to label the same claims. All numbers are macro-averaged\nover four datasets on the KG-aligned subset.\nModel\nClaim qual.\nCorr.\nCompl.\nOverall\nMed-Qwen2-7B\n4.97±0.18\n2.29±0.98\n3.06±1.31\n2.66±1.05\nMed42-Llama3-8B\n4.73±0.99\n2.35±0.95\n4.10±1.12\n3.16±0.92\nMeditron3-8B\n4.62±0.18\n2.42±1.04\n3.21±1.25\n2.77±1.09\nPMC-LLaMA-13B\n3.12±0.18\n1.35±1.03\n2.86±1.07\n2.13±0.95\nTable 3: RQ3: Human ratings (1–5) averaged over two\nannotators on 100 questions. Claim quality measures\nwhether the extracted claim set faithfully reflects the\nanswer without introducing new facts.\nerators and align with human ratings?\n4.3\nRQ1: Distillation Quality\nExtractor.\nPanel A of Table 1 reported span-\nlevel precision/recall/F1 of the distilled extractors\nagainst GPT-4.1 teacher claims, as well as the\naverage number of claims per answer. Med42-\nLlama3-8B and Meditron3-8B obtained the high-\nest F1 while producing 4–5 claims per answer,\nclosely matching the teacher’s atomic decompo-\nGenerator\nFaith. ↑\nHalluc. ↓\nSafetyErr ↓\nMed-Qwen2-7B\n81.4\n8.0\n7.7\nMed42-Llama3-8B\n85.3\n6.3\n6.8\nMeditron3-8B\n71.5\n7.6\n8.2\nPMC-LLaMA-13B\n60.1\n10.7\n11.3\nTable 4: Representative end-to-end MedRAGChecker\ndiagnostics, macro-averaged across four biomedical\nQA datasets. Full per-dataset results are reported in\nAppendix B.\nsition. Because teacher claims were intentionally\nhighly granular and evaluation used conservative\ntoken-overlap matching, absolute extractor F1 val-\nues were lower-bounded; human ratings in RQ3 fur-\nther validated that extracted claim sets were faithful\nto the original answers.\nChecker.\nPanel B of Table 1 showed that SFT\nbrought all biomedical checkers to strong overall\naccuracy on the three-way NLI task, but with clear\nclass-wise specialization (e.g., stronger CONTRA-\n7\n"}, {"page": 8, "text": "Setting\nAcc. ↑\nMacro-F1 ↑\nNLI-only\n63.4\n59.2\nFused (NLI+KG)\n69.8\n64.7\nTable 5: Human agreement on KG-induced decision\nflips (KG-aligned safety claims). Corrected flips: 31%.\nDICT F1 for Med42-Llama3-8B). The F1-weighted\nensemble improved overall accuracy and robust-\nness on the minority CONTRADICT class, and was\nused as the default checker for all downstream di-\nagnostics. GRPO variants were deferred to Ap-\npendix F.\n4.4\nRQ2: Effect of KG-Enhanced Verification\nWe isolated the impact of the KG signal (DRKG)\nby comparing NLI-only vs. Fused (NLI+KG) un-\nder identical teacher and student checkers. Ta-\nble 2 reported results on KG-aligned safety-critical\nclaims. Overall, fusion changed decisions mainly\nwithin the KG-covered subset, indicating that KG\nevidence acted as a complementary support proxy\nrather than perturbing unrelated claims. Additional\nanalyses on fusion sensitivity were provided in\nAppendix E. While Table 2 showed that KG fu-\nsion systematically altered verification outcomes\non KG-aligned claims, this alone did not establish\nthat such changes were more reliable. To directly\ntest whether KG integration improved trustwor-\nthiness, we conducted a targeted human study on\nKG-aligned claims where the final decision dif-\nfered between NLI-only and Fused (NLI+KG)\nsettings.\nSpecifically,\nwe\nfocused\non\ndecision-flip\ncases\nwhere\nKG\nfusion\nchanged\nthe\nsup-\nported/unsupported status or the predicted EN-\nTAIL/CONTRADICT label. These cases concen-\ntrated annotation effort on scenarios where the KG\nsignal actively intervened, providing the most di-\nrect test of its benefit. Annotators were asked to\njudge claim veracity given the retrieved passages,\nand we compared agreement with human labels\nbetween NLI-only and Fused predictions.\nAs shown in Table 5, KG fusion achieved\nhigher agreement with human judgments on these\ndecision-flip claims, indicating that structured\nbiomedical constraints helped correct a non-trivial\nsubset of text-only over-entailment errors, espe-\ncially for safety-critical relations such as drug–\ndisease treatment and drug–adverse effect claims.\nCalibration and Sensitivity On the dev split\n(KG-aligned claims), P ⋆is stable over a wide range\nof (β, τ). A β sweep shows that uncalibrated KG\nscores make Eq. 8 overly β-sensitive and increase\ndecision flips, while min–max calibration (rescal-\ning KG scores to [0, 1] on dev) substantially flat-\ntens both supported-rate and flip-rate curves (Ap-\npendix E, Fig. 4; Appendix E.1).\n4.5\nRQ3: End-to-End Diagnostics and\nHuman Alignment\nHuman study.\nWe conducted a small-scale hu-\nman evaluation on 100 questions sampled from\nthe four datasets, covering long-form answers pro-\nduced by four biomedical generators. Two anno-\ntators with biomedical/NLP background indepen-\ndently rated, for each (question, model) pair, (1) the\nquality of the extracted claim set, (2) answer cor-\nrectness, (3) completeness, and (4) overall quality\non a 1–5 Likert scale.\nTable 3 summarized the results. All systems\nobtained near-ceiling claim-set quality scores, in-\ndicating that the distilled extractor generally pro-\nduced faithful and non-hallucinated atomic claims.\nAcross models, answer correctness was modest,\nwith Med42-Llama3-8B achieving higher com-\npleteness and overall quality than the others. We\nreported agreement statistics and KG-alignment\nvalidation in Appendix H. We also reported inter-\nannotator agreement (quadratic-weighted κ) and\ncorrelations between MedRAGChecker diagnos-\ntics and human ratings in Appendix H.6.\nEnd-to-end diagnostics.\nWe applied the full\nMedRAGChecker pipeline (distilled extractor,\nstudent-ensemble checker, and KG fusion) to gener-\nator outputs to obtain answer-level diagnostics (e.g.,\nfaithfulness/NotSupported rate, context precision,\nself-knowledge, safety-critical error rate). Repre-\nsentative end-to-end diagnostics, macro-averaged\nacross datasets, were summarized in Table 4; the\nfull per-dataset breakdown was reported in Ap-\npendix B.\n5\nConclusion\nWe introduced MEDRAGCHECKER, a claim-\nlevel diagnostic framework for biomedical RAG.\nMedRAGChecker decomposes long-form answers\ninto atomic claims, verifies each claim with a dis-\ntilled textual NLI checker and a DRKG-based con-\nsistency signal, and fuses them into a calibrated sup-\nport score P ⋆. Across four biomedical QA bench-\nmarks, MedRAGChecker reveals distinct error pro-\nfiles across generators (e.g., under-evidenced vs.\n8\n"}, {"page": 9, "text": "contradicted claims) and supports actionable diag-\nnosis via retrieval- and safety-oriented metrics.\nLimitations\nTeacher supervision as pseudo-ground truth.\nOur distillation and evaluation rely on teacher la-\nbels for claim decomposition and NLI judgments,\nwhich may contain biases or systematic errors, es-\npecially for rare biomedical conditions or ambigu-\nous questions. As shown by our teacher-sensitivity\nanalysis (Appendix Table 10), agreement between\nGPT-4.1 and GPT-4o is high on research-style\ndatasets but substantially lower on consumer-health\nquestions (MedRedQA), so conclusions on those\ndatasets should be interpreted with caution and as\nconditional on the chosen teacher.\nChecker calibration and class imbalance.\nContradiction is typically a minority class; even\nwith ensembling, performance can vary across\ndatasets and may under-detect subtle contradic-\ntions. MedRedQA exhibits low inter-teacher agree-\nment (Appendix Table 10), so we interpret absolute\nFaith/Halluc values on MedRedQA with caution\nand primarily focus on within-teacher relative com-\nparisons and trends. KG coverage and linker\nerrors. DRKG does not cover all biomedical en-\ntities/relations, and our entity/relation linking re-\nlies on surface-form matching without ontology-\nlevel normalization. This can fail for paraphrases,\nnegations, or multi-hop statements, and KG-based\nsignals may therefore be sparse or noisy outside\ncovered subsets. We use DRKG because it is a\nlarge, public biomedical KG with broad coverage\nof the safety-critical relation families we analyze,\nenabling scalable and reproducible KG-based con-\nsistency scoring.\nGeneral-purpose LLM-based evaluators may be\nless reliable in high-stakes medical settings with-\nout domain calibration, and standardized evalua-\ntion remains an open challenge for medical LLM\napplications.\nEthics Statement\nMedRAGChecker is designed to detect unsup-\nported or contradictory claims in biomedical RAG\noutputs and to surface safety-critical error patterns.\nNevertheless, automatic verification can be wrong;\nfalse negatives may miss harmful claims and false\npositives may over-warn. We recommend using\nMedRAGChecker as a screening and debugging\naid, with human oversight for high-stakes settings.\nOur experiments use publicly available datasets\nand retrieved biomedical literature. If human an-\nnotation is used, annotators are informed of the\nstudy purpose, compensated appropriately, and in-\nstructed not to provide medical advice. We do\nnot process private patient data. We will release\nprompts, model checkpoints (where licenses allow),\nand evaluation code to support transparency and\nreproducibility.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, and 1 others. 2023. Gpt-4 techni-\ncal report. arXiv preprint arXiv:2303.08774.\nElham Asgari, Nina Montaña-Brown, Magda Dubois,\nSaleh Khalil, Jasmine Balloch, Joshua Au Yeung, and\nDominic Pimenta. 2025. A framework to assess clin-\nical safety and hallucination rates of llms for medical\ntext summarisation. npj Digital Medicine, 8(1):274.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, and 1 others. 2023. Qwen technical report.\narXiv preprint arXiv:2309.16609.\nAsma Ben Abacha and Dina Demner-Fushman. 2019. A\nquestion-entailment approach to question answering.\nBMC Bioinform., 20(1):511:1–511:23.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. Advances in neural information pro-\ncessing systems, 26.\nWeijian Chen, Yixin Cao, Fuli Feng, Xiangnan He, and\nYongdong Zhang. 2022. Explainable sparse knowl-\nedge graph completion via high-order graph reason-\ning network. arXiv preprint arXiv:2207.07503.\nClément Christophe, Praveen K Kanithi, Tathagata\nRaha, Shadab Khan, and Marco AF Pimentel. 2024.\nMed42-v2: A suite of clinical llms.\nGordon V Cormack, Charles LA Clarke, and Stefan\nBuettcher. 2009. Reciprocal rank fusion outperforms\ncondorcet and individual rank learning methods. In\nProceedings of the 32nd international ACM SIGIR\nconference on Research and development in informa-\ntion retrieval, pages 758–759.\nShahul Es, Jithin James, Luis Espinosa Anke, and\nSteven Schockaert. 2024. RAGAs: Automated evalu-\nation of retrieval augmented generation. In Proceed-\nings of the 18th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nSystem Demonstrations, pages 150–158, St. Julians,\nMalta. Association for Computational Linguistics.\n9\n"}, {"page": 10, "text": "Jinlan Fu, See Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2024. Gptscore: Evaluate as you desire. In\nProceedings of the 2024 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 6556–6576.\nDaniel Scott Himmelstein, Antoine Lizee, Christine\nHessler, Leo Brueggeman, Sabrina L Chen, Dexter\nHadley, Ari Green, Pouya Khankhanian, and Sergio E\nBaranzini. 2017. Systematic integration of biomedi-\ncal knowledge prioritizes drugs for repurposing. elife,\n6:e26726.\nBolin Huang, Yuanzhou Wei, Jun Xiao, Yuanhao\nTian, Yeyubei Zhang, and Changyang Zheng. 2025.\nProactive reliability governance in complex systems:\nLeveraging pattern mining for scalable solutions. In\n2025 10th International Conference on Information\nand Network Technologies (ICINT), pages 180–185.\nIEEE.\nVassilis N. Ioannidis, Xiang Song, Saurav Manchanda,\nMufei Li, Xiaoqin Pan, Da Zheng, Xia Ning, Xi-\nangxiang Zeng, and George Karypis. 2020. Drkg\n- drug repurposing knowledge graph for covid-19.\nhttps://github.com/gnn4dr/DRKG/.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2020. What dis-\nease does this patient have? a large-scale open do-\nmain question answering dataset from medical exams.\narXiv preprint arXiv:2009.13081.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019.\nPubMedQA: A\ndataset for biomedical research question answering.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2567–\n2577, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with gpus.\nIEEE\nTransactions on Big Data, 7(3):535–547.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, and 1 others. 2020. Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks. Advances\nin neural information processing systems, 33:9459–\n9474.\nHaitao Li, Qian Dong, Junjie Chen, Huixue Su, Yu-\njia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu.\n2024.\nLlms-as-judges: a comprehensive survey\non llm-based evaluation methods. arXiv preprint\narXiv:2412.05579.\nYuqi Li, Junhao Dong, Chuanguang Yang, Shiping\nWen, Piotr Koniusz, Tingwen Huang, Yingli Tian,\nand Yew-Soon Ong. 2025a. Mmt-ard: Multimodal\nmulti-teacher adversarial distillation for robust vision-\nlanguage models. arXiv preprint arXiv:2511.17448.\nYuqi Li, Kai Li, Xin Yin, Zhifei Yang, Junhao Dong,\nZeyu Dong, Chuanguang Yang, Yingli Tian, and\nYao Lu. 2025b. Sepprune: Structured pruning for\nefficient deep speech separation.\narXiv preprint\narXiv:2505.12079.\nYuqi Li, Chuanguang Yang, Hansheng Zeng, Zeyu\nDong, Zhulin An, Yongjun Xu, Yingli Tian, and Hao\nWu. 2025c. Frequency-aligned knowledge distilla-\ntion for lightweight spatiotemporal forecasting. In\nProceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 7262–7272.\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021. Pyserini: A Python toolkit for reproducible\ninformation retrieval research with sparse and dense\nrepresentations. In Proceedings of the 44th Annual\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR\n2021), pages 2356–2362.\nDong Liu and Yanxuan Yu. 2025. Cxl-speckv: A dis-\naggregated fpga speculative kv-cache for datacenter\nllm serving. Preprint, arXiv:2512.11920.\nZhichao Ma, Yutong Luo, Zheyu Zhang, Aijia Sun,\nYinuo Yang, and Hao Liu. 2025a. Reinforcement\nlearning approach for highway lane-changing: Ppo-\nbased strategy design. In 2025 10th International\nConference on Electronic Technology and Informa-\ntion Science (ICETIS), pages 298–301.\nZhichao Ma, Aijia Sun, Zheyu Zhang, Yinuo Yang, Zi-\njun Gao, and Hao Liu. 2025b. Energy-constrained\nmotion planning and scheduling for autonomous\nrobots in complex environments. In 2025 5th In-\nternational Conference on Advanced Algorithms and\nNeural Networks (AANN), pages 591–594.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. Factscore:\nFine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 12076–12100.\nVincent Nguyen, Sarvnaz Karimi, Maciej Rybinski, and\nZhenchang Xing. 2023. MedRedQA for medical con-\nsumer question answering: Dataset, tasks, and neural\nbaselines. In Proceedings of the 13th International\nJoint Conference on Natural Language Processing\nand the 3rd Conference of the Asia-Pacific Chapter of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 629–648, Nusa Dua,\nBali. Association for Computational Linguistics.\nYingpeng Ning, Yuanyuan Sun, Ling Luo, Yanhua\nWang, Yuchen Pan, and Hongfei Lin. 2025. Medtrust-\nrag: Evidence verification and trust alignment for\nbiomedical question answering.\narXiv preprint\narXiv:2510.14400.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\n10\n"}, {"page": 11, "text": "Sandhini Agarwal, Katarina Slama, Alex Ray, and 1\nothers. 2022. Training language models to follow in-\nstructions with human feedback. Advances in neural\ninformation processing systems, 35:27730–27744.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan\nSankarasubbu. 2022. Medmcqa: A large-scale multi-\nsubject multi-choice dataset for medical domain ques-\ntion answering. In Proceedings of the Conference\non Health, Inference, and Learning, volume 174 of\nProceedings of Machine Learning Research, pages\n248–260. PMLR.\nShrey Pandit, Jiawei Xu, Junyuan Hong, Zhangyang\nWang, Tianlong Chen, Kaidi Xu, and Ying Ding.\n2025. Medhallu: A comprehensive benchmark for\ndetecting medical hallucinations in large language\nmodels. arXiv preprint arXiv:2502.14302.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\npher D Manning, Stefano Ermon, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. Advances in neural\ninformation processing systems, 36:53728–53741.\nStephen Robertson, Hugo Zaragoza, and 1 others. 2009.\nThe probabilistic relevance framework: Bm25 and\nbeyond. Foundations and Trends® in Information\nRetrieval, 3(4):333–389.\nDongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang,\nPeng Shi, Shuaichen Chang, Cheng Jiayang, Cunx-\niang Wang, Shichao Sun, Huanyu Li, and 1 others.\n2024. Ragchecker: A fine-grained framework for di-\nagnosing retrieval-augmented generation. Advances\nin Neural Information Processing Systems, 37:21999–\n22027.\nAlexandre Sallinen, Antoni-Joan Solergibert, Michael\nZhang, Guillaume Boyé, Maud Dupont-Roc, Xavier\nTheimer-Lienhard, Etienne Boisson, Bastien Bernath,\nHichem Hadhri, Antoine Tran, and 1 others. 2025.\nLlama-3-meditron: An open-weight suite of medical\nllms based on llama-3.1. In Workshop on Large Lan-\nguage Models and Generative AI for Health at AAAI\n2025.\nArchit Sharma, Sedrick Scott Keh, Eric Mitchell,\nChelsea Finn, Kushal Arora, and Thomas Kollar.\n2024. A critical evaluation of ai feedback for aligning\nlarge language models. Advances in Neural Informa-\ntion Processing Systems, 37:29166–29190.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\nClark, Stephen R Pfohl, Heather Cole-Lewis, and\n1 others. 2025. Toward expert-level medical ques-\ntion answering with large language models. Nature\nMedicine, 31(3):943–950.\nTao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaek-\nermann, Mohamed Amin, Pi-Chuan Chang, Andrew\nCarroll, Charles Lau, Ryutaro Tanno, Ira Ktena, and\n1 others. 2024. Towards generalist biomedical ai.\nNejm Ai, 1(3):AIoa2300138.\nJuraj Vladika, Phillip Schneider, and Florian Matthes.\n2024.\nHealthfc:\nVerifying health claims with\nevidence-based medical fact-checking. In Proceed-\nings of the 2024 Joint International Conference\non Computational Linguistics, Language Resources\nand Evaluation (LREC-COLING 2024), pages 8095–\n8107.\nCunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru\nTang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao,\nWenyang Gao, Xuming Hu, Zehan Qi, and 1 others.\n2023. Survey on factuality in large language models:\nKnowledge, retrieval and domain-specificity. arXiv\npreprint arXiv:2310.07521.\nYuxia Wang, Minghan Wang, Muhammad Arslan Man-\nzoor, Fei Liu, Georgi Nenkov Georgiev, Rocktim Jy-\noti Das, and Preslav Nakov. 2024. Factuality of large\nlanguage models: A survey. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 19519–19529, Miami,\nFlorida, USA. Association for Computational Lin-\nguistics.\nChaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang,\nWeidi Xie, and Yanfeng Wang. 2024. Pmc-llama:\ntoward building open-source language models for\nmedicine. Journal of the American Medical Infor-\nmatics Association, 31(9):1833–1843.\nWangjiaxuan Xin, Kanlun Wang, Zhe Fu, and Lina\nZhou. 2025a. Let community rules be reflected in\nonline content moderation. In The 18th China Sum-\nmer Workshop on Information Management (CSWIM)\n2025.\nWangjiaxuan Xin, Shuhua Yin, Shi Chen, and Yaorong\nGe. 2025b. Improving topic modeling of social me-\ndia short texts with rephrasing: A case study of covid-\n19 related tweets. arXiv preprint arXiv:2510.18908.\nYuan Yang, Jingcheng Yu, Ye Hu, Xiaoyao Xu, and Eric\nNyberg. 2017. Cmu livemedqa at trec 2017 liveqa: A\nconsumer health question answering system. arXiv\npreprint arXiv:1711.05789.\nErlan Yu, Xuehong Chu, Wanwan Zhang, Xiangbin\nMeng, Yaodong Yang, Xunming Ji, and Chuanjie Wu.\n2025. Large language models in medicine: Applica-\ntions, challenges, and future directions. International\nJournal of Medical Sciences, 22(11):2792.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences.\narXiv\npreprint arXiv:1909.08593.\nA\nDataset\nThis section summarizes the four biomedical QA\nbenchmarks used in our experiments. Table 6 re-\nports length and retrieval-context statistics, and\nTable 7 provides a qualitative overview of question\nstyle and evidence source.\n11\n"}, {"page": 12, "text": "Dataset\n#Q Median |q| Max |q| Median |a| Max |a| Median #Doc Max #Doc Median/Max |d|\nMedRedQA (CSIRO)\n799\n167\n3732\n56\n1187\n8\n8\n123 / 187\nTREC LiveQA Medical\n104\n12\n66\n88\n486\n8\n8\n205 / 972\nMedQuAD\n1000\n9\n18\n86\n731\n10\n10\n123 / 203\nPubMedQA\n1000\n15\n43\n35\n263\n8\n8\n239 / 735\nTable 6: Dataset statistics for MedRAGChecker RAG inputs. |q| and |a| denote tokenized question and (ground-\ntruth) answer lengths (whitespace tokens). |d| denotes retrieved document length; median/max refer to median and\nmaximum values across all retrieved contexts.\nDataset\nDomain\nAnswer type\nQuestion style / evidence\nMedRedQA\n(CSIRO)\nConsumer health (Red-\ndit r/AskDocs)\nFree-text answers by\nverified clinicians\nMedium–long layperson questions covering di-\nverse symptoms and concerns; thread content\nand PubMed-style references.\nTREC\nLiveQA\nMedical\nConsumer health ques-\ntions to NLM\nFree-text answers with\nquality judgments\nLong, noisy real-world user questions; reference\nanswers plus retrieved passages with human rel-\nevance labels.\nMedQuAD\nNIH consumer health\nwebsites\nFree-text factoid / defi-\nnition QA\nShort–medium consumer questions about dis-\neases, drugs, and procedures; long snippets from\nNIH pages with source URLs.\nPubMedQA\nBiomedical\nresearch\n(PubMed abstracts)\nYes/No/Maybe + long-\nanswer rationale\nShort research questions derived from article\ntitles; abstract body used as context, conclusion\nparagraph as gold answer.\nTable 7: Qualitative overview of the biomedical QA datasets used with MedRAGChecker, including domain, answer\ntype, and typical question/evidence style.\nB\nFull Diagnostic Tables\n12\n"}, {"page": 13, "text": "Dataset\nGenerator\nFaith. ↑Halluc. ↓CtxPrec ↑SelfKnow. ↑SafetyErr ↓\nPubMedQA\nMed-Qwen2-7B\n83.6\n7.4\n41.8\n28.2\n7.0\nMed42-Llama3-8B\n88.9\n4.7\n41.4\n32.7\n6.5\nMeditron3-8B\n69.2\n3.3\n53.2\n30.4\n5.9\nPMC-LLaMA-13B\n57.9\n11.7\n35.6\n21.6\n12.4\nMedQuAD\nMed-Qwen2-7B\n85.3\n6.0\n42.4\n32.4\n8.5\nMed42-Llama3-8B\n87.6\n5.5\n46.3\n31.2\n7.8\nMeditron3-8B\n73.1\n7.6\n49.4\n24.0\n8.2\nPMC-LLaMA-13B\n63.2\n8.7\n44.6\n20.0\n9.7\nLiveQA\nMed-Qwen2-7B\n72.4\n11.3\n43.2\n23.1\n7.6\nMed42-Llama3-8B\n82.5\n8.6\n42.7\n28.6\n6.2\nMeditron3-8B\n72.3\n11.3\n48.6\n31.4\n9.5\nPMC-LLaMA-13B\n55.7\n12.8\n38.1\n22.6\n10.4\nMedRedQA\nMed-Qwen2-7B\n84.2\n7.3\n41.7\n26.4\n7.6\nMed42-Llama3-8B\n82.4\n6.4\n42.7\n27.2\n6.7\nMeditron3-8B\n71.2\n8.3\n44.3\n28.0\n9.3\nPMC-LLaMA-13B\n63.4\n9.5\n41.3\n22.5\n12.7\nTable 8: RQ3: Full end-to-end MedRAGChecker diagnostics across all datasets and generators (teacher = GPT-4.1 ).\nTeacher = GPT-4.1\nTeacher = GPT-4o\nDataset\nGenerator\nClaimF1 ClaimRec CtxPrec Faith. Halluc. ClaimF1 ClaimRec CtxPrec Faith. Halluc.\nPubMedQA\nMed-Qwen2-7B\n26.7\n97.6\n49.9\n88.6\n7.4\n23.2\n96.9\n41.8\n83.6\n7.4\nMed42-Llama3-8B\n27.6\n97.9\n49.8\n92.4\n5.2\n21.1\n99.0\n41.4\n88.9\n4.7\nMeditron3-8B\n23.5\n97.2\n32.9\n69.2\n3.3\n21.0\n97.5\n41.9\n63.7\n1.0\nPMC-LLaMA-13B\n14.6\n98.3\n50.0\n58.0\n29.6\n14.7\n97.4\n50.7\n60.5\n29.5\nMedQuAD\nMed-Qwen2-7B\n39.3\n65.7\n51.4\n88.1\n9.4\n30.1\n62.3\n47.0\n82.9\n6.2\nMed42-Llama3-8B\n43.4\n61.5\n51.4\n86.1\n6.4\n39.5\n62.8\n46.0\n80.2\n8.5\nMeditron3-8B\n31.8\n63.3\n52.8\n83.1\n4.9\n30.1\n63.9\n49.4\n73.1\n7.6\nPMC-LLaMA-13B\n0.0\n63.5\n51.9\n0.0\n5.2\n0.0\n61.3\n47.2\n4.0\n6.0\nLiveQA\nMed-Qwen2-7B\n6.3\n26.8\n43.1\n72.4\n18.3\n5.3\n23.8\n37.6\n65.9\n22.4\nMed42-Llama3-8B\n10.8\n27.5\n42.2\n70.3\n12.6\n8.8\n24.8\n38.1\n63.7\n7.5\nMeditron3-8B\n5.7\n26.4\n41.1\n53.9\n22.9\n4.6\n26.3\n41.9\n29.6\n13.5\nPMC-LLaMA-13B\n0.0\n30.1\n45.6\n27.3\n45.2\n0.0\n15.6\n27.5\n0.0\n0.0\nMedRedQA\nMed-Qwen2-7B\n10.7\n16.2\n12.3\n31.4\n52.1\n1.5\n13.9\n9.5\n12.7\n5.6\nMed42-Llama3-8B\n11.5\n14.6\n11.2\n32.0\n48.0\n2.0\n13.2\n9.8\n14.0\n4.5\nMeditron3-8B\n7.4\n14.5\n11.3\n26.7\n51.4\n1.2\n13.5\n10.1\n9.7\n6.0\nPMC-LLaMA-13B\n9.8\n14.4\n10.5\n7.4\n75.0\n0.0\n9.2\n7.5\n3.7\n5.3\nTable 9: Teacher-checker results (teacher = GPT-4.1 vs GPT-4o).\nC\nPrompt Templates\nC.1\nTeacher claim extraction prompt\nYou are a claim extraction assistant.\nTask: Given an answer, extract a list of\nATOMIC factual claims as SPO triples.\nOutput format (STRICT): Return ONLY\na valid JSON array of triples: [ [\"sub-\nject\", \"relation\", \"object\"], ... ] - No\nprose, no markdown, no preface, no trail-\ning commas. - Use double quotes for\nall strings. - Each triple must contain\nexactly 3 strings.\nAtomicity & faithfulness constraints: -\nEach triple must express a single, check-\nable fact stated in the answer. - Do NOT\nparaphrase the whole sentence; split con-\njunctions into separate triples. - Do NOT\nintroduce any new facts that are not ex-\nplicitly stated in the answer. - Keep entity\nnames as they appear in the answer when\npossible.\nNegation / uncertainty / condition han-\ndling: - If the answer negates a fact, re-\nflect negation in the RELATION (pre-\nferred) or OBJECT. Example: [\"Drug\nA\", \"is not recommended for\", \"Con-\ndition B\"] - If the answer is uncer-\ntain/probabilistic (may/can/likely), re-\nflect it in RELATION. Example: [\"Drug\nA\", \"may cause\", \"Side effect X\"] - If the\nclaim is conditional (if/when), include\nthe condition in RELATION. Example:\n[\"Drug A\", \"reduces X when\", \"taken\n13\n"}, {"page": 14, "text": "with food\"]\nNow extract triples from the answer be-\nlow.\nANSWER: answer\nC.2\nTeacher NLI verification prompt\nYou are a strict NLI verifier for biomedi-\ncal QA.\nGoal: Decide whether the CLAIM is sup-\nported by the provided PASSAGES.\nInput: - CLAIM: a single atomic claim\n(hypothesis). - PASSAGES: retrieved\ntext snippets (premises). Each passage\nincludes a doc_id.\nDecision labels: - Entail: at least one\npassage explicitly supports the claim. -\nContradict: at least one passage explic-\nitly states the opposite of the claim. -\nNeutral: the passages do not provide suf-\nficient information to entail or contra-\ndict. Neutral includes: (a) insufficient\nevidence (relevant but incomplete) (b) ir-\nrelevant evidence (not about the claim)\nConstraints: - Use ONLY the informa-\ntion in PASSAGES. Do NOT use exter-\nnal knowledge. - If evidence is missing,\nchoose Neutral (insufficient). - Prefer En-\ntail/Contradict only with explicit textual\nsupport.\nOutput format (STRICT JSON): Re-\nturn ONLY: \"label\": \"Entail\" | \"Neu-\ntral\" | \"Contradict\", \"prob\": \"Entail\":\n<float>, \"Neutral\": <float>, \"Contradict\":\n<float>, \"neutral_type\": \"insufficient\" |\n\"irrelevant\" | null, \"rationale\": <string>,\n\"spans\": [ \"doc_id\": <string>, \"quote\":\n<string> ] - prob values must sum to\n1. - quote should be a short support-\ning/contradicting span (<= 25 words). -\nIf label is Neutral, spans can be [].\nNow verify.\nCLAIM: claim\nPASSAGES:\ntopk_passages_with_doc_id\nTeacher sensitivity and RAG alignment.\nTa-\nble 10 shows that GPT-4.1 and GPT-4o agree\non most claim–evidence labels, suggesting that\nour conclusions are not overly sensitive to the\nDataset\nLabel agreement ↑\nκ ↑\nPubMedQA\n96.2\n0.84\nMedQuAD\n91.9\n0.74\nLiveQA\n68.9\n0.53\nMedRedQA\n32.1\n0.13\nTable 10: Teacher sensitivity: GPT-4o vs GPT-4.1 on\nthe same claim–evidence pairs.\nteacher choice. Recent RAG evaluation tools (e.g.,\nRAGAS (Es et al., 2024)) provide answer-level,\nreference-free signals such as faithfulness and con-\ntext quality; we conceptually align with this line\nby treating each extracted claim as a minimal eval-\nuation unit and scoring its support from retrieved\nevidence. We do not report RAGAS as a main quan-\ntitative baseline because it is a general-purpose\nframework rather than a biomedical claim-level\nchecker, and its behaviour in our setting is highly\nsensitive to configuration choices (e.g., claim seg-\nmentation and judge prompts).\nD\nKG Case Study\nD.1\nRating Scales and Guidelines\nThis appendix provides the full guidelines given\nto annotators when rating model answers and ex-\ntracted claims.\nD.2\nGoal of the Annotation\nWe evaluate (i) the quality of long-form answers\nproduced by different LLM/RAG generators to\nmedical questions, and (ii) the quality of an au-\ntomatic claim extraction step applied to those an-\nswers. Each row in the annotation sheet corre-\nsponds to a single question, and contains:\n• The original question.\n• Model-generated answers from each student\nmodel (e.g., Meditron3-8B, Med42-Llama3-\n8B, Med-Qwen2-7B, PMC-LlaMA-13B).\n• Automatically extracted claims for each\nmodel.\nFor each model, annotators rate both the claim\nset and the answer text according to the scales de-\nscribed below.\nWe quantify how often generated claims can be\naligned to DRKG entities and edges. Table 11\nreports node/pair coverage and KG-subset faith-\nfulness/hallucination behavior under the pair-hit\ndefinition.\n14\n"}, {"page": 15, "text": "Claim + Evidence Snippet\nClaim: “Aspirin is recommended to treat fever \nin children with viral infections.”\nRetrieved snippet (partial/ambiguous):\n “Aspirin is effective for reducing fever and pain. \n… children … viral infections … Reye’s \nsyndrome … (avoid aspirin).”\nTextual NLI/Checker \nEntailment score: 0.93 (ENTAIL)\nNote: Negation/contraindication under-attended \n→ over-confident entailment.\nKG +Fusion\nKG\nAspirin —(associated_with/adverse_event)—> Reye \nsyndrome\nReye syndrome —(risk_group)—> Children\nReye syndrome —(triggered_by)—> Viral infection\nFusion\ntext_entail = 0.93\nkg_support = -0.70 (contradiction)\nfused = down-weighted to 0.20 → label: \nNOT-ENTAILED (safety-critical)\nAspirin\ndrug\nViral infection\nReye syndrome\ndisease\nsymptoms\nFigure 3: Example where KG support corrects an over-confident textual entailment decision.\nDataset\nGenerator\nKG-Covnode (%)\nKG-Covpair (%)\nFaith\nFaithKG\nHallKG\nLiveQA\nMed-Qwen2-7B\n84.2\n51.3\n72.4\n86.3\n8.8\nMed42-Llama3-8B\n68.5\n56.4\n63.7\n87.8\n9.4\nMeditron3-8B\n70.0\n52.6\n53.9\n65.7\n26.1\nPMC-LLaMA-13B\n65.0\n30.0\n27.3\n25.6\n57.7\nPubMedQA\nMed-Qwen2-7B\n83.0\n49.0\n88.6\n96.1\n3.9\nMed42-Llama3-8B\n95.0\n83.0\n92.4\n94.5\n5.0\nMeditron3-8B\n88.3\n62.7\n69.2\n85.8\n6.2\nPMC-LLaMA-13B\n72.0\n35.0\n60.5\n57.1\n42.9\nMedQuAD\nMed-Qwen2-7B\n93.0\n69.0\n88.1\n87.2\n11.2\nMed42-Llama3-8B\n94.0\n88.0\n86.1\n89.0\n7.2\nMeditron3-8B\n84.0\n66.0\n83.1\n89.5\n7.4\nPMC-LLaMA-13B\n8.0\n4.0\n4.0\n0.0\n92.3\nMedRedQA\nMed-Qwen2-7B\n90.0\n53.4\n37.8\n32.8\n50.3\nMed42-Llama3-8B\n92.0\n74.3\n31.3\n36.7\n48.5\nMeditron3-8B\n90.2\n46.3\n26.7\n26.0\n55.1\nPMC-LLaMA-13B\n92.7\n86.6\n7.4\n6.1\n76.2\nTable 11: DRKG coverage and KG-subset behavior (pair-hit definition). We report node/pair coverage and KG-\nsubset faithfulness/hallucination metrics.\nDataset\nGenerator\nAvg. Resp. Claims\nAvg. GT Claims\nEnt (%)\nNeu (%)\nCon (%)\nPubMedQA\nMed-Qwen2-7B\n2.10\n4.90\n88.6\n4.0\n7.4\nMed42-Llama3-8B\n2.00\n4.90\n92.3\n2.4\n5.2\nMeditron3-8B\n1.95\n4.90\n29.1\n69.0\n1.8\nPMC-LLaMA-13B\n6.80\n4.90\n58.0\n12.4\n29.6\nMedQuAD\nMed-Qwen2-7B\n4.80\n10.20\n88.1\n2.4\n9.4\nMed42-Llama3-8B\n5.10\n10.20\n86.1\n7.5\n6.4\nMeditron3-8B\n4.70\n10.20\n83.1\n12.0\n4.9\nPMC-LLaMA-13B\n12.50\n10.20\n4.0\n90.0\n6.0\nLiveQA\nMed-Qwen2-7B\n5.60\n14.00\n72.4\n9.3\n18.3\nMed42-Llama3-8B\n6.81\n13.06\n12.3\n86.1\n1.7\nMeditron3-8B\n5.20\n14.12\n10.6\n87.9\n1.6\nPMC-LLaMA-13B\n17.50\n14.00\n27.3\n27.5\n45.2\nMedRedQA\nMed-Qwen2-7B\n3.92\n5.81\n15.5\n82.1\n2.3\nMed42-Llama3-8B\n3.82\n5.80\n19.3\n78.2\n2.5\nMeditron3-8B\n7.98\n7.71\n54.6\n45.1\n0.3\nPMC-LLaMA-13B\n21.01\n9.23\n28.1\n71.1\n0.8\nTable 12: Text-level claim verification summary (Entailment/Neutral/Contradiction histograms) across runs.\nEnt/Neu/Con denote the percentage of generated claims assigned to each NLI label.\n15\n"}, {"page": 16, "text": "E\nCalibration Details\nE.1\nFusion-weight sensitivity (β-sweep)\nWe study how the fused support score P ⋆and down-\nstream supported/not-supported decisions change\nas we vary the mixing weight β in Eq. 8. Unless\notherwise specified, we fix α to the dev-tuned value\nand sweep β ∈{0, 0.1, . . . , 1.0}.\nMetrics.\nWe report (i) the fused supported rate\n(fraction of claims with P ⋆≥τ) and (ii) the deci-\nsion flip rate relative to a near-NLI reference setting\n(e.g., β = 0.9).\nSupported rate and flip rate.\nFor a given mixing\nweight β, we compute fused scores P ⋆\nβ(c) via Eq. 8\nand binarize them with the dev-tuned threshold τ:\nIsup,β(c) ≜I\n\u0002\nP ⋆\nβ(c) ≥τ\n\u0003\n.\nWe summarize fusion behavior on a claim set C\nusing\nfused_E(β) = 100 · 1\n|C|\nX\nc∈C\nIsup,β(c),\nand the decision flip rate relative to a near-text-only\nreference β0 (we use β0=0.9):\nflip_rate(β) =\n100\n|C|\nX\nc∈C\nI [Isup,β(c) ̸= Isup,β0(c)] . (15)\nA lower flip_rate means fusion decisions are less\ndominated by the KG score scale and thus more\nstable across β.\nMin–max calibration of KG scores.\nRaw KG\nplausibility scores (e.g., TransE-based pKGE and\nthe derived sKG) can be poorly scaled and concen-\ntrated in a narrow range. To reduce scale mismatch\nin the logit-mixture fusion (Eq. 8), we optionally\napply min–max calibration to the KG score on the\ndevelopment split:\n˜sKG(c) = clip\n\u0012sKG(c) −smin\nsmax −smin\n, ϵ, 1 −ϵ\n\u0013\n, (16)\nwhere smin and smax are the minimum and maxi-\nmum KG scores observed on the dev set (restricted\nto KG-aligned claims), and ϵ is a small constant to\navoid infinite logits. We then replace sKG(c) with\n˜sKG(c) when computing Eq. 8.\nWhy calibrating KG scores matters.\nRaw KG\nplausibility scores can be poorly calibrated and\nconcentrated in a narrow range, which makes the\nlogit term logit(sKG) disproportionately large (in\nmagnitude) and causes fusion decisions to change\nsharply with β. Min–max calibration aligns the\nKG score scale to the NLI probability range while\npreserving ranking, substantially reducing flip rates\nand yielding flatter supported-rate curves in Fig. 4.\nE.2\nThreshold robustness (τ-sweep)\nWe additionally sweep the support threshold τ\naround the dev-tuned value and observe stable\ntrends in the supported rate and SafetyErr within a\nreasonable range.\nFor completeness, we report teacher-checker re-\nsults for all dataset/generator configurations in Ta-\nble 9, complementing the representative results in\nthe main text.\nF\nGRPO Refinement of Student Checkers\nSetup.\nBeyond supervised fine-tuning (SFT), we\nalso explored Group Relative Policy Optimization\n(GRPO; (Sharma et al., 2024; Rafailov et al., 2023;\nZiegler et al., 2019)) to further align the student\n16\n"}, {"page": 17, "text": "0\n5\n10\n15\n20\n25\n30\n35\nfused_E\nMed-Qwen2-7B\n10\n20\n30\n40\n50\n60\n70\n80\nMed42-Llama3-8B\n0\n10\n20\n30\n40\nMeditron3-8B\n0\n10\n20\n30\n40\n50\nPMC-LLaMA-13B\n0.2\n0.4\n0.6\n0.8\n0.9\n0\n5\n10\n15\n20\n25\n30\nflip_rate\n0.2\n0.4\n0.6\n0.8\n0.9\n0\n10\n20\n30\n40\n50\n0.2\n0.4\n0.6\n0.8\n0.9\n0\n5\n10\n15\n20\n25\n30\n35\n0.2\n0.4\n0.6\n0.8\n0.9\n5\n0\n5\n10\n15\n20\n25\nMetrics vs  (mean ± SEM)\ngrpo | minmax\ngrpo | none\nsft | minmax\nsft | none\nFigure 4: Sensitivity of KG–NLI fusion to the mixing weight β. Top: fused entailment rate (fused_E), i.e., the\nfraction of claims classified as supported after fusion and thresholding. Bottom: decision flip rate (flip_rate)\nrelative to a near-NLI reference setting (β=0.9), measuring how often KG changes the supported/not-supported\ndecision. We compare SFT and GRPO checkers, with and without KG score calibration (minmax vs. none).\ncheckers with the GPT-4.1 teacher. Starting from\nthe SFT checkpoints, GRPO generates multiple ver-\ndict candidates per (claim, evidence) pair and nor-\nmalizes rewards within each group. The reward is\n+1 if the candidate label matches the teacher verdict\nand 0 otherwise, with a KL penalty to the SFT pol-\nicy. We run one additional GRPO epoch for each\nbiomedical backbone considered in the main text\n(Med-Qwen2-7B, Med42-Llama3-8B, Meditron3-\n8B, PMC-LLaMA-13B) using a smaller learning\nrate than SFT (Ouyang et al., 2022; Sharma et al.,\n2024; Rafailov et al., 2023; Ziegler et al., 2019).\nResults.\nOverall, GRPO did not consistently im-\nprove checker quality. For some backbones (e.g.,\nMed-Qwen2-7B), GRPO increased CONTRADICT\nF1 but substantially reduced ENTAIL F1 and overall\naccuracy, leading to overly aggressive contradiction\npredictions. For other backbones, GRPO produced\nonly marginal changes relative to the SFT base-\nlines. When plugged into MedRAGChecker, these\nGRPO-tuned checkers did not yield clear gains on\ndownstream diagnostics such as answer-level faith-\nfulness, hallucination, or safety-critical error rate.\nGiven the extra complexity and computational cost\nof GRPO, and the lack of consistent improvements,\nwe therefore report SFT-only student checkers in\nthe main paper and treat GRPO as an exploratory\nnegative result.\nModel\nTrain / Setting\nFaith. ↑\nHalluc. ↓\nSafetyErr ↓\nMed-Qwen2-7B\nSFT / NLI-only\n62.3\n20.7\n19.6\nSFT / Fused (ours)\n68.6\n10.4\n12.6\nGRPO / NLI-only\n24.0\n76.0\n13.2\nGRPO / Fused (ours)\n20.1\n79.9\n13.8\nMed42-Llama3-8B\nSFT / NLI-only\n67.3\n24.3\n28.9\nSFT / Fused (ours)\n58.4\n25.7\n19.6\nGRPO / NLI-only\n75.1\n24.9\n49.9\nGRPO / Fused (ours)\n73.8\n26.2\n50.0\nMeditron3-8B\nSFT / NLI-only\n52.7\n14.6\n29.0\nSFT / Fused (ours)\n59.6\n27.3\n27.2\nGRPO / NLI-only\n32.6\n66.7\n20.8\nGRPO / Fused (ours)\n37.0\n63.0\n22.0\nPMC-LLaMA-13B\nSFT / NLI-only\n15.3\n57.3\n63.2\nSFT / Fused (ours)\n21.3\n73.4\n54.3\nGRPO / NLI-only\n16.2\n73.1\n18.9\nGRPO / Fused (ours)\n24.6\n68.7\n21.2\nTable 13: RQ2: KG fusion ablation by checker back-\nbone (SFT vs. GRPO). GRPO variants are deferred to\nAppendix F and do not consistently improve over SFT.\nG\nHuman Evaluation: Protocol\nWe conduct human evaluation to assess (i) answer\nquality and (ii) claim extraction quality for model\noutputs.\nG.1\nSetup\nSampling.\nWe sample 100 questions from\nMedQuAD, LiveQA, MedRedQA, and Pub-\nMedQA. For each question, we evaluate up to\nfour generators’ answers (Meditron3-8B, PMC-\nLLaMA-13B, Med-Qwen2-7B, Med42-Llama3-\n8B) and the extracted claim sets.\nAnnotators.\nAnnotators are graduate students\nwith training in biomedical or health sciences. Each\n17\n"}, {"page": 18, "text": "Score\nAnchor rubric (applies to all four dimen-\nsions)\n5\nExcellent: correct and reliable; for claims,\ncovers key factual points without adding un-\nsupported content.\n3\nMixed: partially correct but with noticeable\nomissions or distortions; could mislead with-\nout caution.\n1\nPoor: largely incorrect/off-topic or unusable;\nfor claims, fails to represent the answer or\nintroduces clear hallucinations.\nTable 14: Collapsed anchor rubric for 1–5 ratings (2 and\n4 are intermediate).\nmodel output is rated independently; annotators do\nnot rank models.\nUnit of annotation.\nFor each (question, model)\npair, annotators score the answer and its extracted\nclaim set. When reference answers/evidence are\navailable, annotators rely primarily on them; other-\nwise they use domain knowledge.\nG.2\nDimensions and scoring\nAnnotators assign 1–5 Likert scores for four dimen-\nsions: Claim extraction quality (claims faithfully\nreflect the answer without hallucinations), Answer\ncorrectness, Answer completeness, and Overall\nanswer quality. Scores 5/3/1 correspond to excel-\nlent / mixed / poor, with 2 and 4 as intermediate\nlevels. Table 16 include the results for the kappa\nvalue for correctness and claim quality\nWhen to comment.\nAnnotators provide a brief\ncomment if (i) any answer score is 1–2, or (ii) claim\nquality is ≤3.\nG.3\nAgreement and reporting\nWe report mean±std over samples (averaging two\nannotators per sample) and compute quadratic-\nweighted Cohen’s κ for inter-annotator agreement.\nFull guidelines.\nDetailed examples and interface\nschemas are provided in our anonymized reposi-\ntory.\nH\nHuman Validation of KG Alignment\nGoal.\nWe assess whether the KG-alignment mod-\nule yields a semantically faithful structured ren-\ndering of a claim. This evaluates alignment cor-\nrectness (entity identity and relation meaning), not\nbiomedical truth.\nH.1\nSampling and annotation unit\nWe perform stratified sampling over evaluation\nfolders (dataset/model/config), and only sample\nfrom claims where the aligner outputs at least one\nDRKG triple after filtering. Each annotated item\nis a claim-level instance consisting of: claim text,\noptional supporting context, and the top-1 aligned\ntriple (h, r, t) returned by the aligner (ties broken\ndeterministically). We annotate n=100 aligned\nclaims for correctness, and additionally double-\nannotate a subset of size nκ for inter-annotator\nagreement.\nH.2\nAnnotation fields\nAnnotators provide four binary labels and optional\nnotes:\n• h_subj_entity_ok: subject entity refers to\nthe same biomedical concept as the claim sub-\nject;\n• h_obj_entity_ok: object entity refers to the\nsame biomedical concept as the claim object;\n• h_relation_map_ok: relation matches the\nclaim predicate and directionality;\n• h_triple_semantic_ok: overall triple ex-\npresses the claim semantics.\nH.3\nDecision criteria\nEntities are marked OK if they map to the same con-\ncept (synonyms/standard variants allowed); NOT\nOK for wrong concept/type or overly broad/narrow\nmappings that change meaning.\nRelations are\nmarked OK only if predicate meaning and direction\nmatch (e.g., treats vs. associated with; contraindi-\ncation vs. adverse event).\nCollapsed triple criterion.\nTo avoid ambiguity,\nwe define:\nhtriple ≜hsubj ∧hobj ∧hrel,\nwhere\nhsubj\n=\nh_subj_entity_ok,\nhobj\n=\nh_obj_entity_ok,\nand\nhrel\n=\nh_relation_map_ok. This yields a conservative\nestimate of end-to-end alignment correctness.\nH.4\nMetrics\nWe report claim-level correctness rates on the an-\nnotated\nsubset:\nPr[h_subj_entity_ok=1],\nPr[h_obj_entity_ok=1],\nPr[h_relation_map_ok=1],\n18\n"}, {"page": 19, "text": "Metric\nValue (%)\nSubject entity accuracy\n78\nObject entity accuracy\n76\nRelation mapping accuracy\n89.2\nTriple plausibility rate\n82\nTable 15: Human validation results for DRKG align-\nment on a stratified sample, aggregating two annotators.\nPr[h_triple_semantic_ok=1]. On the double-\nannotated subset, we report Cohen’s κ for each\nfield (or for h_triple_semantic_ok).\nH.5\nCoverage of KG alignment\nWe also report claim-level coverage: a claim is\ncounted as “aligned” if the module outputs at least\none retained DRKG triple (h, r, t) after filtering.\nWe stratify coverage by coarse relation families\nused in our safety analysis (e.g., Drug–Disease,\nDrug–Adverse Event, Gene–Disease).\nH.6\nHuman Evaluation: Agreement and\nCorrelation\nCorrelations between MedRAGChecker diagnos-\ntics and human ratings. See in Table 16.\nDimension\nκ (quadratic)\n#Items\nClaim quality\n0.67\n100\nCorrectness\n0.32\n100\nCompleteness\n0.27\n100\nTable 16: Inter-annotator agreement for the human\nstudy.\nMetric vs Human\nρ (Spearman)\nFaith vs Correctness\n0.47\nHalluc vs Correctness\n0.23\nSafetyErr vs Correctness\n0.36\nCtxPrec vs Overall\n0.53\nTable 17: Correlation between MedRAGChecker diag-\nnostics and human ratings, since we have 100 questions\nfor annotation and each question at least 4 claims gener-\nated.\nI\nHuman Evaluation on KG-Induced\nDecision Flips\nSampling.\nWe sample claims from the KG-\naligned subset where the final verification decision\ndiffers between NLI-only and Fused (NLI+KG)\nsettings. Claims are stratified by relation families\nused in the safety analysis (e.g., drug–disease, drug–\nadverse effect, gene–disease).\nAnnotation unit.\nEach annotation instance con-\nsists of a single atomic claim, the retrieved pas-\nsages used for verification, and (optionally) the\ntop-1 aligned KG triple for reference. Annotators\nare instructed to judge claim veracity only based\non the retrieved passages, without using external\nknowledge.\nLabels.\nAnnotators assign one of {ENTAIL, NEU-\nTRAL, CONTRADICT} to each claim. We then\ncompare human labels against NLI-only and Fused\npredictions to compute accuracy, Macro-F1, and\ncorrected flip rates.\nJ\nRetrieval settings and indices\nOverview.\nWe\nstandardize\nretrieval\nacross\ndatasets by using (i) a FAISS index for CSIRO\nruns, (ii) a pre-retrieved pickle for MedQuAD, and\n(iii) the MedRAG retrieval system for the remain-\ning datasets. Table 18 summarizes the mapping\nfrom dataset to retriever and the corresponding\nindex artifacts. Table 19 lists the index files and\nwhere they are stored.\n19\n"}, {"page": 20, "text": "Dataset\nRetriever\nIndex / evidence source\nMedRedQA\nCsiroFaissRetriever / gold\nFAISS mode (Johnson et al., 2019): FAISS ANN index built over\nCSIRO corpus (faiss.index + texts.jsonl + meta.jsonl under\n–csiro_index_dir); gold mode: use provided contexts directly\n(no external index).\nMedQuAD\nMedQuADRetriever\nPre-retrieved contexts loaded from –medquad_pickle (a pickle file\ncontaining query-to-context mappings).\nPubMedQA / LiveQA /\nMedRedQA (non-CSIRO)\nMedRAGRetriever\nMedRAG retrieval over a selected corpus (e.g., PubMed / MedCorp)\nusing BM25 (Robertson et al., 2009) (–retriever bm25), dense\n(contriever/specter/medcpt), or hybrid (RRF (Cormack et al.,\n2009)). Indices are stored under –medrag_db_dir.\nTable 18: Dataset-to-retriever mapping and the evidence source used for retrieval.\nRetriever family\nKey artifacts\nWhere it lives / how it is used\nCSIRO FAISS\nfaiss.index, texts.jsonl, meta.jsonl\nAll under –csiro_index_dir. Re-\ntrieved hits are mapped to text via\ntexts.jsonl (with optional meta-\ndata in meta.jsonl).\nMedQuAD pre-retrieval\nretrieved_*.pkl\n–medquad_pickle. No ANN/BM25\nsearch at runtime;\ndirectly loads\ncached retrieval results.\nMedRAG BM25\nLucene/Pyserini (Lin et al., 2021) BM25 index (directory)\nStored under –medrag_db_dir (per\ncorpus).\nUsed when –retriever\nbm25.\nMedRAG dense / hybrid\nfaiss.index (+ metadata file such as metadatas.jsonl)\nStored\nunder\n–medrag_db_dir\n(per corpus and retriever).\nUsed\nwhen –retriever is dense (e.g.,\ncontriever/specter/medcpt)\nor\nhybrid (RRF fusion).\nTable 19: Index artifacts used by each retriever family.\nK\nExtractor Results\nBackbone ablation.\nTable 13 compares GRPO\ncheckers under calibrated β fusion. med42-llama3-\n8b yields the most stable fused distribution, while\nother backbones exhibit degenerate or highly\nskewed fused label rates, motivating our choice of\nmed42-llama3-8b as the default checker backbone.\nFusion form and calibration.\nWe compare\nadditive-style fusion (alpha) with logit-mixture fu-\nsion (beta). Beta fusion benefits from calibrating\nKG scores, as raw KGE plausibility often concen-\ntrates in a narrow low range; calibration preserves\nKG ranking while aligning its scale to NLI proba-\nbilities, preventing the KG term from dominating\nthe mixture purely due to scale mismatch. We use\nGPU A100 80G for the training.\nL\nStudent Configurations\nDataset\nGenerator\nClaimF1 ↑\nFaith ↑\nHalluc ↓\nPubMedQA\nMed42-Llama3-8B\n21.1\n88.9\n4.7\nMeditron3-8B (GPT-4o)\n21.0\n63.0\n1.0\nMed-Qwen2-7B\n23.2\n83.6\n7.4\nPMC-LLaMA-13B\n19.2\n56.3\n20.6\nMedQuAD\nMed42-Llama3-8B\n27.2\n72.5\n4.2\nMeditron3-8B\n30.1\n73.1\n7.6\nMed-Qwen2-7B\n34.2\n82.4\n5.4\nPMC-LLaMA-13B\n22.8\n63.6\n12.8\nLiveQA\nMed42-Llama3-8B\n14.3\n77.3\n7.9\nMeditron3-8B\n10.4\n67.4\n10.6\nMed-Qwen2-7B\n12.6\n76.7\n21.6\nPMC-LLaMA-13B\n7.4\n43.6\n26.4\nMedRedQA\nMed42-Llama3-8B\n11.6\n33.2\n63.2\nMeditron3-8B\n12.4\n20.9\n57.2\nMed-Qwen2-7B\n15.7\n37.4\n18.9\nPMC-LLaMA-13B\n8.0\n18.3\n74.6\nTable 20: Teacher-based MedRAGChecker metrics\n(placeholder version). ClaimF1 is claim overlap F1;\nFaith is entailment rate; Halluc is contradiction rate\n(Contradict-only).\n20\n"}, {"page": 21, "text": "Stage\nBase models\nlr\nep\nbs\nacc\nL\nGen\nLoRA / Extra\nExtractor SFT\nMed42-Llama3-8B\nMeditron3-8B\nMed-Qwen2-7B\nPMC-LLaMA-13B\n1e−4\n3\n2\n16\n2048\nmax_new_tokens=256\nr=16, α=32, drop=0\ntargets: q,k,v,o,gate,up,down\nChecker SFT\nMed42-Llama3-8B\nMeditron3-8B\nMed-Qwen2-7B\nPMC-LLaMA-13B\n5e−5\n3\n2\n16\n1024\neval: max_new_tokens=4\neval_subset=1000\nr=8, α=16, drop=0\ntargets: q,k,v,o,gate,up,down\nChecker GRPO\nMed42-Llama3-8B\nMeditron3-8B\nMed-Qwen2-7B\nPMC-LLaMA-13B\n5e−6\n1\n2\n8\n1024\nsample: max_new_tokens=3, T =0.7, top_p=0.9\neval: max_new_tokens=4\neval_subset=800\nr=8, α=16, drop=0\nK=4 samples/prompt\ngrad clip=1.0\ntargets: q,k,v,o,gate,up,down\nTable 21: Training configurations for the distilled extractor and checker models. L denotes tokenizer truncation\nlength.\n21\n"}]}