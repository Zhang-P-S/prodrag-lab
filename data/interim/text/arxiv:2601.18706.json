{"doc_id": "arxiv:2601.18706", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.18706.pdf", "meta": {"doc_id": "arxiv:2601.18706", "source": "arxiv", "arxiv_id": "2601.18706", "title": "Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs", "authors": ["Zhichao Yang", "Sepehr Janghorbani", "Dongxu Zhang", "Jun Han", "Qian Qian", "Andrew Ressler", "Gregory D. Lyng", "Sanjit Singh Batra", "Robert E. Tillman"], "published": "2026-01-26T17:34:10Z", "updated": "2026-01-26T17:34:10Z", "summary": "Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.18706v1", "url_pdf": "https://arxiv.org/pdf/2601.18706.pdf", "meta_path": "data/raw/arxiv/meta/2601.18706.json", "sha256": "9c031cd7546bccd70a2face735cf6d624425ab9bb0b99ab4af539fdab1da885f", "status": "ok", "fetched_at": "2026-02-18T02:20:29.424822+00:00"}, "pages": [{"page": 1, "text": "Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs\nZhichao Yang* †, Sepehr Janghorbani* †,\nDongxu Zhang, Jun Han, Qian Qian, Andrew Ressler II, Gregory D. Lyng,\nSanjit Singh Batra*, Robert E. Tillman*\nOptum AI\nAbstract\nRubrics are essential for evaluating open-ended\nLLM responses, especially in safety-critical do-\nmains such as healthcare. However, creating\nhigh-quality and domain-specific rubrics typi-\ncally requires significant human expertise time\nand development cost, making rubric-based\nevaluation and training difficult to scale. In this\nwork, we introduce Health-SCORE, a general-\nizable and scalable rubric-based training and\nevaluation framework that substantially reduces\nrubric development costs without sacrificing\nperformance. We show that Health-SCORE\nprovides two practical benefits beyond stan-\ndalone evaluation: it can be used as a structured\nreward signal to guide reinforcement learning\nwith safety-aware supervision, and it can be\nincorporated directly into prompts to improve\nresponse quality through in-context learning.\nAcross open-ended healthcare tasks, Health-\nSCORE achieves evaluation quality compara-\nble to human-created rubrics while significantly\nlowering development effort, making rubric-\nbased evaluation and training more scalable.\n1\nIntroduction\nThe widespread adoption of large language models\n(LLMs) in safety-critical domains such as health-\ncare has highlighted the importance of develop-\ning trustworthy models.\nLLMs are sometimes\ncapable of producing hallucinations–outputs that\nsound plausible but are factually incorrect, and\nin high-stakes domains, such as healthcare, the\nability to identify these types of errors is criti-\ncal (Ji et al., 2023). Early efforts to make LLMs\nclinically-grounded have largely relied on standard-\nized, examination-style multiple-choice questions\n(MCQs), such as those used in the United States\nMedical Licensing Examination (USMLE) (Jin\net al., 2019; Pal et al., 2022). However, MCQ-based\n†These authors contributed equally.\n*Correspondence to: {zhichao_yang, sepehr.janghorbani,\nsanjit.batra, rob.tillman}@optum.com\nevaluations reduce complex clinical reasoning to\na single discrete answer and fail to capture key\ncompetencies required in real-world clinical prac-\ntice, including synthesizing clinical notes, reason-\ning through differential diagnoses, and recommend-\ning appropriate treatment plans (Van Der Vleuten\nand Schuwirth, 2005). Moreover, although mod-\nern LLMs now achieve near-expert performance on\nUSMLE-style benchmarks, substantial gaps in real-\nworld clinical capability remain (Van Der Vleuten\nand Schuwirth, 2005).\nMore recent evaluation\napproaches have shifted toward open-ended re-\nsponse evaluation, in which multiple solutions may\nbe valid (Arora et al., 2025; Wang et al., 2025b).\nRubric-based evaluations offer greater represen-\ntational power by enabling more diverse and nu-\nanced assessments through a structured framework\nof diverse criteria. These criteria can span multi-\nple dimensions relevant to real-world clinical prac-\ntice, such as clinical safety, relevance, and com-\npleteness. A rubric typically consists of a set of\nevaluation criteria, explicit descriptions of pass/fail\nor performance-level conditions for each criterion,\nand an associated scoring scheme.\nRubric-based evaluation frameworks can vary\nwidely in both the dimensions they assess and the\ngranularity at which evaluation is performed. Fig-\nure 1 illustrates different categories of rubrics along\nthis spectrum. At one end are generalized rubrics,\nsuch as Anthropic’s HHH framework, which define\nbroad and universal criteria for response “good-\nness” (Bai et al., 2022a). These rubrics are sim-\nple, general, and relatively easy to scale, making\nthem well-suited for high-level alignment. How-\never, their coarse design limits their representa-\ntional power, often causing them to be less applica-\nble to context-specific errors and failure modes.\nAt the opposite end of the spectrum are instance-\nlevel rubrics, (e.g.\nPaperBench (Starace et al.,\n2025) and HealthBench (Arora et al., 2025)), which\nprovide detailed, context-specific criteria for each\n1\narXiv:2601.18706v1  [cs.AI]  26 Jan 2026\n"}, {"page": 2, "text": "Figure 1: Specificity vs. cost tradeoff. Both Instance-\nlevel and Health-SCORE rubrics exhibit high specificity,\nbut Health-SCORE is far less costly to develop.\nexample. These rubrics enable highly granular and\nprecise assessments, capturing subtle reasoning er-\nrors and misalignments. However, they are difficult\nto scale due to their high development cost and\nlimited transferability across domains and exam-\nples. Moreover, because instance-level rubrics are\ntightly coupled to examples, they cannot be used\nduring inference for an in-context learning setting,\nlimiting their potential (Min et al., 2022).\nA third class lies between these extremes, balanc-\ning specificity, generalizability, and development\ncost. This category includes domain-specific cri-\nteria that are sufficiently detailed to capture subtle\nmodel errors, yet broad enough to remain practical\nfor large-scale, real-world deployment. Moreover,\nbecause these rubrics generalize across examples,\nthey can also be used at inference time as in-context\nlearning signals to improve model response qual-\nity. In this study, we introduce Health-SCORE\n(Healthcare Scalable COmprehensive Rubric Eval-\nuation), a generalized rubric framework designed\nfor healthcare applications that integrates the scala-\nbility and generalizability of coarse-grained rubrics\nwith the precision of instance-level approaches. In\nsummary, our main contributions are:\n1. We introduce Health-SCORE, a scalable and\ngeneralizable rubric-based evaluation frame-\nwork for open-ended medical LLM evaluation,\nreducing rubric development cost.\n2. We propose an adaptive rubric selection mech-\nanism that identifies relevant rubrics for indi-\nvidual prompts, enabling on-the-fly personal-\nized criterion selection.\n3. Through in-domain and out-of-distribution ex-\nperiments, we show that using Health-SCORE\nas a reward signal during RL training leads\nto improved model performance and greater\ntraining stability when evaluated against in-\ndependent, human-authored instance-level\nrubrics, establishing Health-SCORE as an ef-\nfective surrogate for expert supervision.\n4. We show that Health-SCORE can also serve\nas an effective in-context learning solution to\nguide LLMs toward higher-quality responses\nat inference time without additional training.\n2\nRelated Works\nAlthough earlier work primarily focused on us-\ning USMLE-style questions to evaluate healthcare-\noriented language models (Singhal et al., 2025;\nLiu et al., 2024), recent studies have increasingly\nshifted toward open-ended evaluation paradigms.\nFor example, MultiMedQA demonstrated that mod-\nels such as Med-PaLM (Tu et al., 2024) achieve\nstrong performance on MCQs but still exhibit sub-\nstantial gaps on free-response tasks. Beyond ques-\ntion answering, several studies have examined\nmodel performance in simulated clinical interac-\ntions. Frameworks such as CRAFT-MD (Johri\net al., 2025), AMIE (Tu et al., 2024), and Agent-\nClinic (Schmidgall et al., 2024) focus on model-\ning doctor–patient dialogue and clinical reasoning.\nCRAFT-MD evaluates LLM responses in conver-\nsational scenarios by measuring diagnosis (Johri\net al., 2025). However, these approaches have\nshown that model accuracy can degrade on suf-\nficiently complex tasks (Schmidgall et al., 2024).\nRubric-based evaluation frameworks have also\nbeen explored extensively outside the healthcare\ndomain. For example, Constitutional AI (Bai et al.,\n2022b) introduced 16 guiding “Constitutional Prin-\nciples” aimed at improving model alignment by\nemphasizing helpfulness, honesty, and harmless-\nness. However, the high-level nature of these princi-\nples limits their ability to capture subtle or context-\ndependent errors. Building on this idea, Rubicon\n(Biyani et al., 2024) proposed a method for auto-\nmatically generating domain-specific rubrics that\nare both informative and discriminative, particu-\nlarly for evaluating multi-turn technical problem-\nsolving dialogues. A key limitation of Rubicon is\nits reliance on an initial set of manually labeled\nconversations, which restricts scalability and gener-\nalization. FLASK (Ye et al., 2023) further reframes\n2\n"}, {"page": 3, "text": "Figure 2: Health-SCORE rubric creation process: First, original rubrics are clustered using high-dimensional\nembeddings. Then the clusters are refined and a Health-SCORE rubric is proposed for each cluster, reducing\nredundancy while preserving core evaluative dimensions.\nevaluation as the assessment of a model’s underly-\ning skills. Its dataset comprises 1,740 diverse in-\nstructions spanning STEM reasoning, factual ques-\ntion answering, and safety, with each task mapped\nto 12 fundamental skills such as logical reasoning,\ncommonsense understanding, and problem-solving.\nHowever, FLASK depends on reference answers\nrather than flexible, rubric-based criteria, which\nlimits its applicability in open-ended or inference-\ntime evaluation settings. More granular, instance-\nlevel evaluation frameworks have also emerged in\nrecent work. Prometheus-v1 (Kim et al., 2023)\nand Prometheus-v2 (Kim et al., 2024) introduce\ndatasets of approximately 20,000 instructions span-\nning tasks such as question answering, summariza-\ntion, reasoning, and dialogue, paired with around\n1,000 fine-grained rubrics. These rubrics define de-\ntailed, example-specific evaluation criteria, improv-\ning assessment precision while retaining partial\ngeneralizability across instances. Similarly, Paper-\nBench (Starace et al., 2025) evaluates LLM agents\non their ability to reproduce findings from aca-\ndemic papers, providing 8,316 hierarchical rubric\nitems derived from 20 AI research studies. Al-\nthough highly precise and context-sensitive, this\napproach is labor-intensive and difficult to scale.\nWithin the healthcare domain, OpenAI’s Health-\nBench represents one of the most comprehensive\nbenchmarks to date. It comprises over 5,000 multi-\nturn medical conversations between clinicians and\nLLMs, evaluated using more than 48,000 physician-\nauthored criteria. Developed with input from 262\ndoctors across 60 countries and 26 medical spe-\ncialties, HealthBench captures the complexity, nu-\nance, and safety-critical aspects of real-world clin-\nical interactions.\nUnlike traditional exam-style\nbenchmarks, it emphasizes open-ended reasoning,\nuncertainty management, contextual understand-\ning, and communication quality, marking a signifi-\ncant step toward trustworthy and clinically aligned\nAI systems (Arora et al., 2025). Related rubric-\nbased healthcare benchmarks have also emerged\nin other languages. For example, CSEDB pro-\nvides a Chinese evaluation framework for clini-\ncal safety and effectiveness, consisting of 2,069\nopen-ended synthetic clinical scenarios and ques-\ntions spanning 26 medical departments. It includes\ninstance-level rubric criteria that assess multiple\ndimensions of safety and effectiveness in LLM-\ngenerated responses (Wang et al., 2025b). Several\nrecent works have explored using rubrics as reward\nsignal for RL, enabling more structured and inter-\npretable optimization (Gunjal et al., 2025; Huang\net al., 2025; Zhou et al., 2025; Chen et al., 2025).\n3\nMethods\n3.1\nHealth-SCORE Rubric Creation Process\nWe use the set of rubrics from HealthBench to de-\nvelop the initial set of criteria for Health-SCORE.\nIn HealthBench, each conversation is organized\ninto seven high-level themes: expertise-tailored\ncommunication, response depth, emergency refer-\nrals, health data tasks, global health, context seek-\ning, and responding under uncertainty. As shown in\nHealthBench, frontier models have the lowest per-\nformance on health data tasks. This is probably due\nto the level of task complexity, as it require models\nto interpret and reason over structured numerical\nand categorical clinical data (e.g., laboratory pan-\n3\n"}, {"page": 4, "text": "Figure 3: Health-SCORE rubric Selection Process: Given a user query, an LLM-based selector scores each Health-\nSCORE rubric for contextual relevance. Rubrics whose scores exceed a threshold are selected. Then the adaptive\nHealth-SCORE rubrics will be added to the system prompt and used as training rewards during RL post-training.\nels, medication lists, and comorbidity tables), and\nto generate structured or semi-structured clinical\noutputs such as medical notes or lab summaries\n(Arora et al., 2025). We adopt this theme and fol-\nlow a structured, multi-step methodology to de-\nvelop the initial set of criteria for Health-SCORE.\nFirst, each rubric was embedded using OpenAI’s\ntext-embedding-3, producing high-dimensional se-\nmantic representations of the rubric. These embed-\ndings enabled the clustering of rubrics with similar\nevaluative criteria despite differences in phrasing.\nFor instance, as illustrated in Figure 2, Rubrics 1\nand 3 both assess whether responses contain fabri-\ncations. Rubric 1 verifies patient details, whereas\nRubric 3 evaluates fabricated laboratory results;\nhowever, both target the same underlying failure\nmode: making unsupported assumptions or intro-\nducing information not stated in the prompt. We ap-\nplied clustering to rubric embeddings to obtain an\ninitial set of clusters. While some rubrics formed in-\ndividual clusters due to their high specificity, others\nnaturally grouped together based on shared evalua-\ntive intent. We then conducted a quality assurance\nstep involving manual inspection and refinement\nof the clusters to reduce noise and remove outliers,\nensuring coherence within each group. Following\nrefinement, we derived a final set of 29 Health-\nSCORE criteria provided in Appendix F.\n3.2\nAdaptive Criteria Selection Mechanism\nMedical conversations can span a wide range of\ntopics and tasks, such as clinical recommendations,\nsearching for medical guidelines, or summarizing\nclinical documentation. As such, Health-SCORE\naims to include a broad set of evaluation criteria\ndesigned to cover this diversity. However, since\neach conversation typically centers around a spe-\ncific task, applying the full set of rubrics indiscrim-\ninately to every conversation can introduce noise.\nTo address this challenge, an essential component\nof our framework is an automated adaptive selec-\ntion mechanism that identifies the subset of con-\ntextually relevant criteria. For instance, a rubric\ncriterion related to the formatting of a SOAP note\nshould only be applied when the prompt explicitly\nrequests generating structured clinical documen-\ntation. To implement this, we adopt an LLM-as-\na-judge approach that assigns each rubric item a\nrelevance score on a five-point scale, based on its\nsemantic and contextual alignment with the task\n(Zheng et al., 2023). Rubrics receiving higher rele-\nvance scores are then selectively retained, ensuring\nthat only the appropriate criteria contribute to the\nfinal evaluation. Figure 3 illustrates this adaptive\nHealth-SCORE selection process. Similar adaptive\nevaluation strategies have been shown to be bene-\nficial in prior work (Mallinar et al., 2025). More\ndetails are provided in Appendix G\n3.3\nHealth-SCORE for In-Context Learning\nRubrics can be especially useful for in-context\nlearning with health-focused LLMs because they\ngive the model a clear target for what a good re-\nsponse should look like. When prompts include\nexplicit criteria that emphasize accuracy, complete-\nness, and safety, the rubric acts like a lightweight\n4\n"}, {"page": 5, "text": "policy the model can follow in real time, improving\nthe quality and consistency of its outputs. Thus,\nrubrics also function as an evaluation checklist, en-\ncouraging the model to verify each requirement\nas it generates the response. We will demonstrate\nthe applicability of Health-SCORE for in-context\nlearning by using an adaptive selection mechanism\nthat identifies the most relevant Health-SCORE\ncriteria for each prompt and inserts them directly\ninto the context. We show that this approach im-\nproves the quality of off-the-shelf models and also\nmakes training more sample-efficient and stable by\nguiding the learned policy in the right direction.\n3.4\nHealth-SCORE Rubrics as RL Rewards\nWe adopt a reinforcement learning (RL) formula-\ntion for our post-training pipeline. The objective\nis to optimize a policy model to generate outputs\naligned with a custom reward function over a set\nof n prompts D = {x(i)\n1:Ti}n\ni=1. This setup is simi-\nlar to prior work in RL-based alignment and post-\ntraining (Shao et al., 2024). Our approach is based\non the Group Relative Policy Optimization (GRPO)\nframework and consists of three main steps:\nIn the first step, for a given user prompt x(i)\n1:Ti ∈\nD, multiple candidate outputs are sampled from the\ncurrent policy model {y(j)}O\nj=1 ∼πθ(· | xi). Each\noutput y(j)\n1:Tj consists of intermediate thinking to-\nkens y(j)\n1:CoT and the final response y(j)\nr\n= y(j)\nCoT:Tj.\n{y(j)}O\nj=1 ∼πθ(· | xi)\ny(j) = {<think> y(j)\n1:CoT </think> y(j)\nCoT:Tj}\nThe\nsampling\nprocess\nuses\ntemperature-\ncontrolled decoding to promote diversity among\nthe CoT tokens. This is essential for estimating\nrelative response quality and effectively guiding\npolicy updates. In the second step, for each group\nof generated outputs, we compute a sequence-level\nreward that reflects the relative quality of each\nresponse candidate. This reward is derived from a\ncomparative evaluation using a heuristic scoring\nfunction R({y(j)\nr }G\nj=1). In the final step , the policy\nmodel parameters are updated using a variant of\npolicy gradient optimization that incorporates\nthe group-relative rewards. GRPO extends PPO\nby adjusting the advantage estimates ˆr(x, y)\nbased on within-group comparisons,\nthereby\nencouraging the model to shift probability mass\ntoward higher-ranked outputs while avoiding large,\ndestabilizing updates with reference model πref.\nThis is achieved by maximizing objective J (θ):\nJGRPO(θ) = Ex∼D Ey∼πθ(·|x)\nh\nˆr(x, y) log πθ(y|x)\ni\n−β Ex∼D\nh\nKL(πθ(·|x) ∥πref(·|x))\ni\nDifferent from the reinforcement learning with\nverifiable rewards (RLVR) setting (Guo et al.,\n2025), where the outcome reward can be easily\ncomputed from hard-coded rule-based functions\n(e.g. string matching for math answers), Rubric-\nbased RL requires more sophisticated reward sys-\ntem to get the sequence-level reward. For each\nprompt, the adaptive selection mechanism first\nidentifies the relevant set of rubrics; then an LLM\njudges the model’s response against each selected\nrubric to determine whether it satisfies the criteria.\nIf a positive rubric is satisfied, it receives a posi-\ntive point (+1). If a negative rubric is satisfied, it\nreceives a negative point (-1). If the rubric is not\nsatisfied, it receives no points (0). The points across\nall selected rubrics are then summed and normal-\nized to produce a sequence-level reward, which is\nused to update the policy model.\nFigure 4: Training using Health-SCORE as the learning\nobjective and evaluation using human-authored rubrics\nas the gold standard.\n4\nExperimental Setup\n4.1\nEvaluation Methodology\nIn this study, we aim to evaluate whether Health-\nSCORE can (1) act as a reward signal for improving\nmodel training and (2) support in-context learning,\nrather than serving solely as a standalone evalua-\ntion framework. To this end, we have designed 3\nsets of separate experiments (described in Section\n5\n"}, {"page": 6, "text": "5) to assess Health-SCORE from these perspec-\ntives. Section 5.1 examines the benefits of using\nHealth-SCORE as a reward for training. Section\n5.2 examines the benefits of Health-SCORE in In-\nContext learning and section 5.3 demonstrates the\nbenefit of Health-SCORE on training efficiency\nand stability. Furthermore, in order to measure the\ngeneralizability of Health-SCORE, we run exper-\niments on both in-domain and out-of-distribution\n(OOD) data. For in-domain evaluation, we mea-\nsure performance on held-out conversations from\nHealthBench–HealthData. These examples match\nthe training split in task categories, data source,\nand physician-authored rubric construction proto-\ncol, but are disjoint at the conversation level. We\ntest generalization using two OOD settings:\n• OOD-Difficulty: HealthBench-Hard (Arora\net al., 2025), which remains in the same health-\ncare domain as HealthBench–HealthData but\ncontains more challenging instances that re-\nquire deeper reasoning, involve greater ambi-\nguity, and exhibit stricter failure modes.\n• OOD-Dataset: CSEDB (Wang et al., 2025b),\nwhich differs from HealthBench not only in\ndata source but also in task formulation and\nrubric ontology.\nIts rubrics emphasize re-\nsponse safety and effectiveness, rather than\nthe HealthBench evaluation axes. The dataset\nis originally in Chinese and has been trans-\nlated into English.\n4.2\nEvaluation Rubrics\nFigure 4 shows our evaluation pipeline. We eval-\nuate all models using human-authored, instance-\nlevel rubrics from HealthBench and CSEDB, rather\nthan Health-SCORE itself. This design choice\navoids circular evaluation and enables a direct as-\nsessment of whether models trained or guided by\nHealth-SCORE better align with independent ex-\npert judgment. In this framework, Health-SCORE\nfunctions as a scalable surrogate supervision signal,\nwhile human-authored rubrics serve as the gold\nstandard for evaluation. Following HealthBench\n(Arora et al., 2025), we use GPT-4.1 as the judge\nto apply these expert-crafted rubrics. All reported\nscores are normalized to the range [0.0,1.0]. Exper-\niment details are available in Appendix D.\n4.3\nBaselines\nTo measure the effect of our design components,\nwe will evaluate our method against the following\nFigure 5: In-Domain and out-of-distribution (OOD)\nevaluation when models are trained with different rubric\ntypes. Dotted/dashed lines correspond to lower/upper\nbounds.\nbaselines (Statistics available in Appendix C):\nSingle-Axis\nSingle-Axis Instruction-only rubrics\nuse a single-axis, generic rubric applied across\ndomains: \"You\nare\na\nhelpful\nassistant.\nPlease generate a response that follows\nuser\ninstructions.\" This rubric focuses on\ninstruction-following, without domain-specific con-\nstraints or multi-dimensional axes. It is lightweight\nand easy to scale, but lacks task-specific guidance.\nMulti-Axes\nNon-Adaptive Multi-Axis rubrics\nare a set of fixed, multi-axis evaluation criteria cov-\nering five axes from HealthBench: communication\nquality, instruction following, accuracy, context\nawareness, and completeness. This baseline im-\nproves relevance compared to Single-Axis, but is\n6\n"}, {"page": 7, "text": "Figure 6: In-Domain and out-of-distribution evaluation when models are prompted with different rubric types.\nnon-adaptive with limited representational power.\nLLM-Generated\nInspired by Gunjal et al.\n(2025); Wang et al. (2025a), rubrics are generated\nby LLM and specific to each prompt. Given each\nuser prompt, we first asked GPT-4.1 to generate a\nlist of appropriate rubrics in natural language, and\nthen applied those rubrics as reward functions.\nInstance-Specific\nHealthBench Instance-specific\nrubrics are the physician-authored criteria provided\nin HealthBench, designed specifically for each con-\nversation instance. They yield the most precise and\ncontextually relevant reward signals, but require\nsubstantial development effort and domain knowl-\nedge, making them costly, hard to scale, and with\nlimited use for in-context learning.\n5\nResults\nWe evaluate the effectiveness of Health-SCORE\nfrom three complementary perspectives: (i) its\nvalue as a reward signal during reinforcement learn-\ning, (ii) its effect on training dynamics when in-\ncluded directly in prompts, and (iii) its utility as\ntest-time guidance via in-context learning without\nadditional fine-tuning. First, we show that mod-\nels trained with Adaptive Health-SCORE achieve\nstronger alignment with physician-authored evalu-\nation criteria, outperforming baselines in both in-\ndomain and out-of-distribution settings. Second,\nwe analyze training dynamics and find that incorpo-\nrating Health-SCORE into prompts during learning\nimproves sample efficiency and stabilizes policy\noptimization by steering exploration toward human-\nrelevant criteria. Finally, we demonstrate that using\nAdaptive Health-SCORE purely at inference time\nalso yields consistent performance gains. Together,\nthese results establish Adaptive Health-SCORE as\na unified mechanism for scalable evaluation, effi-\ncient training, and effective test-time improvement\nof healthcare LLMs.\n5.1\nHealth-SCORE as RL Reward Signal\nFigure 5 illustrates performance differences among\nmodels trained with various rubric-based reward\nformulations.\nAll models are evaluated using\nHealthBench instance-specific rubrics in accor-\ndance with the HealthBench evaluation protocol.\nOverall, Health-SCORE consistently outperforms\nalternative reward formulations across evaluation\nsetups and model sizes.\nIn the in-domain set-\nting, Health-SCORE achieves the highest scores,\nsurpassing the Single-Axis, Multi-Axis, and Gen-\nerated Rubrics baselines, while attaining perfor-\nmance comparable to training directly with Health-\nBench instance-specific rubrics. This result indi-\ncates that adaptively selecting relevant evaluation\ncriteria provides a more effective training signal\nthan either fixed or automatically generated rubrics.\nImportantly, these performance gains extend to out-\nof-distribution evaluations. Health-SCORE demon-\nstrates substantial improvements even in the most\nchallenging setting, HealthBench-Hard, particu-\nlarly for the larger model, suggesting increased\nrobustness under distribution shift. Similar trends\nare observed on CSEDB, where Health-SCORE\nachieves the best performance for both model sizes.\nThese results collectively demonstrate that poli-\ncies trained with adaptive reward signals generalize\nmore effectively across domains and datasets.\n5.2\nIn-Context Learning: Better Generation\nIn addition to improving training, Health-SCORE\ncan also be used at inference time to generate\nhigher-quality responses when incorporated di-\nrectly into prompts. This addresses one of the\n7\n"}, {"page": 8, "text": "key limitations of using predefined instance-based\nRubrics such as in HealthBench. Figure 6 demon-\nstrates that Health-SCORE can improve perfor-\nmance even for frontier models such as GPT-5, o3,\nand GPT-4.1, which were not trained with Health-\nSCORE, in both in-domain and out-of-distribution\nsettings. This result is consistent with prior findings\nshowing that rubric-based prompting can enhance\nmodel performance by making evaluation criteria\nexplicit at generation time (Wang et al., 2025b).\nFigure 7: Left: Health-SCORE speeds up RL train-\ning convergence, leading to higher ∆early in training.\nRight: Health-SCORE improves KL stability during\ntraining.\n5.3\nTraining Efficiency Improvement\nIn addition to overall performance gains, we an-\nalyze how incorporating Health-SCORE rubrics\ndirectly into the prompts influences the learning\ndynamics during training. As shown in Figure 7,\nmodels trained with Health-SCORE prompting con-\nsistently achieve higher evaluation scores earlier\nin training, suggesting that Health-SCORE guides\nthe policy toward desirable behaviors and reduces\nwasteful exploration. This is further supported by\nthe training stability analysis in Figure 7. Health-\nSCORE prompting results in less volatile PPO KL\ndivergence compared to training without rubric con-\nditioning, indicating smoother, less noisy, and more\ncontrolled policy updates. These results suggest\nthat Health-SCORE is beneficial not only as a re-\ninforcement learning reward signal, but also as an\nexplicit conditioning mechanism during generation.\nBy shaping the exploration space, Health-SCORE\nenables faster and more stable convergence without\ncompromising final performance.\n5.4\nAblation: Adaptive Selection Mechanism\nTo evaluate the impact of the adaptive selection\nmechanism, we further conduct ablation experi-\nments.\nTable 1 reports the results of these ex-\nperiments.\nAs shown, the adaptive selection\nSetup\nModel\nNon-Adaptive\nAdaptive\nHealthBench:\nHealth Data\nQwen3-8B\n0.051\n0.345\nQwen3-32B\n0.279\n0.416\nGPT-4.1\n0.328\n0.445\no3\n0.391\n0.509\nGPT-5\n0.486\n0.597\nHealthBench:\nHard (OOD)\nQwen3-8B\n0.000\n0.102\nQwen3-32B\n0.073\n0.196\nGPT-4.1\n0.136\n0.198\no3\n0.291\n0.368\nGPT-5\n0.397\n0.429\nCSEDB (OOD)\nQwen3-8B\n0.263\n0.418\nQwen3-32B\n0.393\n0.476\nGPT-4.1\n0.244\n0.388\no3\n0.445\n0.615\nGPT-5\n0.491\n0.568\nTable 1: Selection mechanism ablation study.\nmechanism consistently improves evaluation scores\nacross all settings and models. This holds both\nwhen the mechanism is applied during training (as\nin HealthBench:HealthData) and when it is used\nonly at inference time through in-context learn-\ning. The effect is particularly stronger for weaker\nmodels, e.g. Qwen3-8B. In these cases, indiscrim-\ninately including all rubric criteria can dilute the\nlearning/guidance signal by introducing many con-\nstraints, some of which may not be relevant to the\nspecific conversation. Adaptive selection mitigates\nthis issue by focusing on the most pertinent criteria.\n6\nConclusion\nRubric-based evaluation is essential for assess-\ning open-ended LLM outputs in safety-critical do-\nmains such as healthcare. However, designing high-\nquality rubrics is time-consuming and resource-\nintensive, which limits the scalability of rubric-\nbased evaluation in real-world settings. In this\nwork, we introduce Health-SCORE, a scalable and\ngeneralizable framework that reduces rubric de-\nvelopment effort while maintaining strong evalua-\ntion performance. Through extensive experiments,\nwe show that Health-SCORE serves as an effec-\ntive alternative to human-authored rubrics, both as\nan inference-time guidance mechanism and as a\nstructured reward signal for reinforcement learning.\nModels trained or guided using Health-SCORE\nachieve improved performance and training effi-\nciency when evaluated against expert instance-level\ncriteria. These results highlight Health-SCORE’s\npotential as a practical and scalable solution for\nevaluating and optimizing healthcare LLMs.\n8\n"}, {"page": 9, "text": "7\nLimitations\nThis work makes several assumptions and has lim-\nitations that should be considered when interpret-\ning the results. Health-SCORE relies on LLMs for\nmultiple stages of the pipeline, including rubric em-\nbedding, relevance-based selection, and rubric eval-\nuation during reward computation. While LLM-as-\na-judge approaches have been shown to correlate\nwell with human judgments in prior work, auto-\nmated evaluation may still exhibit biases or incon-\nsistencies, particularly in safety-critical domains\nsuch as healthcare. Although our final evaluation is\nconducted using human expert–authored, instance-\nlevel rubrics, we do not fully eliminate reliance\non automated judges during training. Future work\ncould further improve this by incorporating human-\nin-the-loop validation or multi-judge consensus\nmechanisms to further improve robustness.\nOur approach assumes that human-authored\nrubrics encode reusable, higher-level evaluation\ncriteria that can be meaningfully abstracted across\ninstances. The clustering-based Health-SCORE\nrubric construction process, while effective in prac-\ntice, involves heuristic choices as well as a manual\nrefinement and outlier-detection step to refine clus-\nters and remove case-specific or noisy rubric items.\nThese design decisions may introduce some level of\nminor subjectivity. We try to minimize this by vali-\ndating the resulting Health-SCORE rubrics through\ndownstream performance and axis-level analysis,\nbut alternative abstraction strategies may also yield\ndifferent Health-SCORE rubric representations.\nIn our reward formulation, all selected rubric\nitems contribute equally, and rubric satisfaction is\ntreated as a discrete outcome. This simplifies re-\nward computation and stabilizes training but does\nnot capture differences in criterion severity or im-\nportance that may be reflected in human evaluation.\nMore expressive reward formulations that incorpo-\nrate graded satisfaction or learned rubric weights\nare a promising direction for future work.\nWe focus our experiments on the health data\ntasks subset of HealthBench, motivated by its com-\nplexity and relevance to structured clinical rea-\nsoning. Although we evaluate generalization on\nHealthBench-Hard and CSEDB, our findings may\nnot imply universal applicability across all health-\ncare tasks or non-medical domains. Extending\nHealth-SCORE to broader task families and vali-\ndating them under substantially different evaluation\nontologies remain important future directions.\nReferences\nRahul K Arora, Jason Wei, Rebecca Soskin Hicks, Pre-\nston Bowman, Joaquin Quiñonero-Candela, Foivos\nTsimpourlas, Michael Sharman, Meghan Shah, An-\ndrea Vallone, Alex Beutel, and 1 others. 2025.\nHealthbench:\nEvaluating large language models\ntowards improved human health.\narXiv preprint\narXiv:2505.08775.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, and\n1 others. 2022a. Training a helpful and harmless\nassistant with reinforcement learning from human\nfeedback. arXiv preprint arXiv:2204.05862.\nYuntao Bai,\nSaurav Kadavath,\nSandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, and 1 others. 2022b.\nConstitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nParam Biyani, Yasharth Bajpai, Arjun Radhakrishna,\nGustavo Soares, and Sumit Gulwani. 2024. Rubicon:\nRubric-based evaluation of domain-specific human\nai conversations. In Proceedings of the 1st ACM\nInternational Conference on AI-Powered Software,\npages 161–169.\nXiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng\nQian, Yu Wang, Hongru Wang, Yu Zhang, Denghui\nZhang, Tong Zhang, and 1 others. 2025. Rm-r1:\nReward modeling as reasoning.\narXiv preprint\narXiv:2505.02387.\nAnisha Gunjal, Anthony Wang, Elaine Lau, Vaskar\nNath, Bing Liu, and Sean Hendryx. 2025. Rubrics as\nrewards: Reinforcement learning beyond verifiable\ndomains. arXiv preprint arXiv:2507.17746.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nPeiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang,\nShirong Ma, Xiao Bi, and 1 others. 2025. Deepseek-\nr1 incentivizes reasoning in llms through reinforce-\nment learning. Nature, 645(8081):633–638.\nZenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu\nQin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu,\nZhanming Shen, Xiaomeng Hu, and 1 others. 2025.\nReinforcement learning with rubric anchors. URL\nhttps://arxiv. org/abs/2508.12790.\nZiwei Ji, YU Tiezheng, Yan Xu, Nayeon Lee, Etsuko\nIshii, and Pascale Fung. 2023. Towards mitigating\nllm hallucination via self reflection. In The 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\n9\n"}, {"page": 10, "text": "Shreya Johri, Jaehwan Jeong, Benjamin A Tran, Daniel I\nSchlessinger, Shannon Wongvibulsin, Leandra A\nBarnes, Hong-Yu Zhou, Zhuo Ran Cai, Eliezer M\nVan Allen, David Kim, and 1 others. 2025. An eval-\nuation framework for clinical use of large language\nmodels in patient interaction tasks. Nature medicine,\n31(1):77–86.\nSeungone Kim, Jamin Shin, Yejin Cho, Joel Jang,\nShayne Longpre,\nHwaran Lee,\nSangdoo Yun,\nSeongjin Shin, Sungdong Kim, James Thorne, and\n1 others. 2023. Prometheus: Inducing fine-grained\nevaluation capability in language models. In The\nTwelfth International Conference on Learning Repre-\nsentations.\nSeungone Kim,\nJuyoung Suk,\nShayne Longpre,\nBill Yuchen Lin, Jamin Shin, Sean Welleck, Graham\nNeubig, Moontae Lee, Kyungjae Lee, and Minjoon\nSeo. 2024. Prometheus 2: An open source language\nmodel specialized in evaluating other language mod-\nels. arXiv preprint arXiv:2405.01535.\nMianxin Liu, Weiguo Hu, Jinru Ding, Jie Xu, Xiaoyang\nLi, Lifeng Zhu, Zhian Bai, Xiaoming Shi, Benyou\nWang, Haitao Song, and 1 others. 2024. Medbench:\nA comprehensive, standardized, and reliable bench-\nmarking system for evaluating chinese medical large\nlanguage models. Big Data Mining and Analytics,\n7(4):1116–1128.\nNeil Mallinar, A Ali Heydari, Xin Liu, Anthony Z\nFaranesh, Brent Winslow, Nova Hammerquist, Ben-\njamin Graef, Cathy Speed, Mark Malhotra, Shwetak\nPatel, and 1 others. 2025. A scalable framework for\nevaluating health language models. arXiv preprint\narXiv:2503.23339.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on health,\ninference, and learning, pages 248–260. PMLR.\nSamuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo\nReis, Jeffrey Jopling, and Michael Moor. 2024.\nAgentclinic: a multimodal agent benchmark to eval-\nuate ai in simulated clinical environments. arXiv\npreprint arXiv:2405.07960.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Yang Wu, and 1 others. 2024.\nDeepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint\narXiv:2402.03300.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\nClark, Stephen R Pfohl, Heather Cole-Lewis, and\n1 others. 2025. Toward expert-level medical ques-\ntion answering with large language models. Nature\nMedicine, 31(3):943–950.\nGiulio Starace, Oliver Jaffe, Dane Sherburn, James\nAung, Jun Shern Chan, Leon Maksin, Rachel Dias,\nEvan Mays, Benjamin Kinsella, Wyatt Thompson,\nand 1 others. 2025. Paperbench: Evaluating ai’s\nability to replicate ai research.\narXiv preprint\narXiv:2504.01848.\nTao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaek-\nermann, Mohamed Amin, Pi-Chuan Chang, Andrew\nCarroll, Charles Lau, Ryutaro Tanno, Ira Ktena, and\n1 others. 2024. Towards generalist biomedical ai.\nNejm Ai, 1(3):AIoa2300138.\nCees PM Van Der Vleuten and Lambert WT Schuwirth.\n2005.\nAssessing professional competence: from\nmethods to programmes.\nMedical education,\n39(3):309–317.\nPengkai Wang, Pengwei Liu, Zhijie Sang, Congkai\nXie, Hongxia Yang, and 1 others. 2025a. Infimed-\norbit: Aligning llms on open-ended complex tasks\nvia rubric-based incremental training. arXiv preprint\narXiv:2510.15859.\nShirui Wang, Zhihui Tang, Huaxia Yang, Qiuhong Gong,\nTiantian Gu, Hongyang Ma, Yongxin Wang, Wubin\nSun, Zeliang Lian, Kehang Mao, and 1 others. 2025b.\nA novel evaluation benchmark for medical llms: Illu-\nminating safety and effectiveness in clinical domains.\narXiv preprint arXiv:2507.23486.\nSeonghyeon Ye, Doyoung Kim, Sungdong Kim,\nHyeonbin Hwang, Seungone Kim, Yongrae Jo,\nJames Thorne, Juho Kim, and Minjoon Seo. 2023.\nFlask:\nFine-grained language model evaluation\nbased on alignment skill sets.\narXiv preprint\narXiv:2307.10928.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, and 1 others.\n2023. Judging llm-as-a-judge with mt-bench and\nchatbot arena. Advances in neural information pro-\ncessing systems, 36:46595–46623.\nYang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang,\nKongcheng Zhang, Jiale Zhao, Jingwen Yang, Yihe\nZhou, Jianwei Lv, Tongya Zheng, and 1 others.\n2025. Breaking the exploration bottleneck: Rubric-\nscaffolded reinforcement learning for general llm\nreasoning. arXiv preprint arXiv:2508.16949.\n10\n"}, {"page": 11, "text": "A\nPotential Risks\nOur work introduces adaptive Health-SCORE to en-\nable scalable rubric-based evaluation and training\nof large language models, particularly for health-\ncare tasks. A primary risk is overgeneralization,\nwhere abstracted rubrics may be applied beyond\ncontexts that require instance-specific or domain-\nexpert judgment, potentially leading to misplaced\ntrust in model outputs. Because Health-SCORE\nare derived from existing human-authored criteria,\nthey may also propagate or amplify implicit biases\npresent in the original benchmarks, which could\ndisproportionately affect underrepresented popula-\ntions if deployed without auditing. Additionally,\nwhen used as reward functions for reinforcement\nlearning, rubric-based supervision may incentivize\nmodels to optimize for measured criteria while ne-\nglecting unmeasured but clinically relevant factors.\nWe emphasize that Health-SCORE are intended\nto complement rather than replace human evalu-\nation, and our experiments explicitly rely on in-\ndependent, expert-authored instance-level rubrics\nfor final assessment. To mitigate these risks, we\nrecommend human-in-the-loop oversight, periodic\nbias audits of abstracted criteria, careful scoping of\napplicability, and efficiency-aware implementation\nto limit unnecessary computational and environ-\nmental costs.\nB\nRubric statistics\nTable 2 summarizes the structural properties of dif-\nferent types of rubric used in our experiments. We\nreport three statistics, each averaged over all con-\nversations in the evaluation set. Number of rubrics\ndenotes the average count of individual rubric cri-\nteria applied to a single conversation. For non-\nadaptive methods, such as Single-axis, Multi-axes,\nHealth-SCORE (Non-adaptive), this value is fixed\nby design and identical across examples. For adap-\ntive methods, such as LLM-Generated, Instance-\nspecific, Health-SCORE (Adaptive), the number\nvaries per prompt and is computed by counting\nonly the rubric items used for that conversation,\nthen averaging across the dataset. Number of to-\nkens measures the average length of all rubric text\nfor each conversation. Adaptive column indicates\nwhether the rubric set varies across conversations.\nC\nPer-Axis Performance Analysis\nIn addition to overall benchmark scores, we also ex-\namine model performance across individual Health-\nBench evaluation axes.\nFigure 8 demonstrates\nthat Health-SCORE improves model performance\nalong most expert-defined rubric dimensions, in-\ncluding Accuracy, Instruction Following, and Com-\npleteness. Notably, gains are not uniform across all\naxes. Communication Quality exhibits similar per-\nformance across methods. This axis-level analysis\nprovides further evidence that Health-SCORE re-\ntains the structure of human evaluation criteria and\nthat adaptive rubric selection enables more targeted\nimprovements than either fixed task-level rubrics\nor generic instruction-following objectives.\nFigure 8: Qwen3-32B performance on HealthBench\nHealth data task. Bars show mean scores for three eval-\nuation strategies: Off-the-shelf Baseline (blue), Multi-\nAxes (orange), and Health-SCORE (green). Dimensions\ninclude Accuracy (n=129), Context Awareness (n=145),\nInstruction Following (n=177), Completeness (n=142),\nand Communication Quality (n=47).\nFigure 9: GPT-5 performance on HealthBench Health\ndata task. Bars show mean scores for three evaluation\nstrategies: Off-the-shelf Baseline (blue), Multi-Axes\n(orange), and Health-SCORE (green). Dimensions in-\nclude Accuracy (n=129), Context Awareness (n=145),\nInstruction Following (n=177), Completeness (n=142),\nand Communication Quality (n=47).\nD\nImplementation Details\nTo implement the GRPO algorithm policy update,\nwe construct prompts by placing the rubric con-\ntent in the system message, followed by the task-\nspecific user prompt, at each training step. For\neach prompt, eight rollouts are generated.\nTo\n11\n"}, {"page": 12, "text": "Rubric Method\nNumber of Rubrics\nNumber of Tokens\nAdaptive\nSingle-Axis\n1\n15\nNo\nMulti-Axes\n5\n158\nNo\nLLM-Generated\n7.8\n242.5\nYes\nInstance-Specific\n10.5\n452.7\nYes\nHealth-SCORE (Non-Adaptive)\n29\n1117\nNo\nHealth-SCORE (Adaptive)\n11.5\n431.4\nYes\nTable 2: Rubric and token statistics for different methods. Numbers are averaged over conversations.\nmake optimization with large models feasible, we\nmicro-batch the rollouts together with the reference\nmodel’s log-probabilities. The reference model\nprovides log-probs for KL computation and helps\nstabilize updates. We apply an adaptive KL con-\ntroller with a low-variance KL penalty (kl_coef =\n1e-4) and a target KL of 0.001 to keep the fine-\ntuned policy close to the reference. Finally, the\nGRPO estimator uses the judge-provided rewards\nand reference log-probs to compute advantages and\nperform mini-batch policy updates over multiple\nepochs. To speed up reward computation, we in-\nvoke the judge asynchronously with a concurrency\nthrottle, allowing many <prompt, response, rubric>\ntriplets to be evaluated in parallel without overload-\ning the judge.\nFollowing the HealthBench setup, we ensure\nthat each rubric is applied to the final decision,\nfollowing the last </think> token, rather than any\nintermediate chain-of-thought content. Rewards\nare computed with conversation context in mind:\nmodel input is a list (system + user messages) and\nthe judge receives rubric text alongside the full\nprompt and final response, enabling it to assess\nsystem-prompt steerability and multi-turn consis-\ntency. This enforces global constraints (tone, per-\nsona, forbidden content) rather than treating each\nmodel turn as an independent checklist.\nModel comparisons are conducted using the\nsame evaluation protocol and metrics across all\nbaselines.\nWhere reinforcement learning is in-\nvolved, each configuration is trained once due to\ncomputational constraints; however, uncertainty\nin evaluation is captured through bootstrap resam-\npling of test examples rather than repeated training\nruns. Bootstrap standard deviation is computed\nby resampling with replacement 1000 times, com-\nputing the mean for each resample, then taking\nstandard deviation of those bootstrap means.\nAll post-training experiments were conducted\non a single node equipped with 8 NVIDIA A100\nGPUs. The total GPU usage for training and evalu-\nation was approximately 30 GPU-hours.\nE\nUse of Generative AI\nTo ensure linguistic precision and clarity, Chat-\nGPT 5.2 was used to improve grammar, enhance\nreadability, and assist with generating visualization\ncode. ChatGPT was not used to generate experi-\nmental results, perform data analysis, or introduce\nnew technical claims. All reported numbers and\nstatements were verified by the authors against the\nunderlying experiments and sources.\nF\nHealth-SCORE Criteria List\nThe following list includes the list of all the criteria\nincluded in Health-SCORE:\nG\nAdaptive Selection Mechanism\nWe used the following prompt for the adaptive\ncriteria-selection mechanism.\n12\n"}, {"page": 13, "text": "13\n"}, {"page": 14, "text": "14\n"}, {"page": 15, "text": "15\n"}]}