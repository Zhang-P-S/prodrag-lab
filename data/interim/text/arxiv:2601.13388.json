{"doc_id": "arxiv:2601.13388", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.13388.pdf", "meta": {"doc_id": "arxiv:2601.13388", "source": "arxiv", "arxiv_id": "2601.13388", "title": "Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction", "authors": ["Sasha Ronaghi", "Prerit Choudhary", "David H Rehkopf", "Bryant Lin"], "published": "2026-01-19T20:53:09Z", "updated": "2026-01-19T20:53:09Z", "summary": "Social determinants of health (SDOH) play a critical role in Type 2 Diabetes (T2D) management but are often absent from electronic health records and risk prediction models. Most individual-level SDOH data is collected through structured screening tools, which lack the flexibility to capture the complexity of patient experiences and unique needs of a clinic's population. This study explores the use of large language models (LLMs) to extract structured SDOH information from unstructured patient life stories and evaluate the predictive value of both the extracted features and the narratives themselves for assessing diabetes control. We collected unstructured interviews from 65 T2D patients aged 65 and older, focused on their lived experiences, social context, and diabetes management. These narratives were analyzed using LLMs with retrieval-augmented generation to produce concise, actionable qualitative summaries for clinical interpretation and structured quantitative SDOH ratings for risk prediction modeling. The structured SDOH ratings were used independently and in combination with traditional laboratory biomarkers as inputs to linear and tree-based machine learning models (Ridge, Lasso, Random Forest, and XGBoost) to demonstrate how unstructured narrative data can be applied in conventional risk prediction workflows. Finally, we evaluated several LLMs on their ability to predict a patient's level of diabetes control (low, medium, high) directly from interview text with A1C values redacted. LLMs achieved 60% accuracy in predicting diabetes control levels from interview text. This work demonstrates how LLMs can translate unstructured SDOH-related data into structured insights, offering a scalable approach to augment clinical risk models and decision-making.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.13388v1", "url_pdf": "https://arxiv.org/pdf/2601.13388.pdf", "meta_path": "data/raw/arxiv/meta/2601.13388.json", "sha256": "274af4219e6f65bdab05a53f73432520936b389c085b28cae8fa65535b597469", "status": "ok", "fetched_at": "2026-02-18T02:21:04.120910+00:00"}, "pages": [{"page": 1, "text": "Structured Insight from Unstructured Data: Large Language Models\nfor SDOH-Driven Diabetes Risk Prediction\nSasha Ronaghi1, Prerit Choudhary1, David H. Rehkopf2, and Bryant Lin3\nAbstract— Social determinants of health (SDOH) play a\ncritical role in Type 2 Diabetes (T2D) management but are\noften absent from electronic health records and risk prediction\nmodels. Most individual-level SDOH data is collected through\nstructured screening tools, which lack the flexibility to capture\nthe complexity of patient experiences and unique needs of a\nclinic’s population. This study explores the use of large language\nmodels (LLMs) to extract structured SDOH information from\nunstructured patient life stories and evaluate the predictive\nvalue of both the extracted features and the narratives them-\nselves for assessing diabetes control. We collected unstructured\ninterviews from 65 T2D patients aged 65 and older, focused\non their lived experiences, social context, and diabetes man-\nagement. These narratives were analyzed using LLMs with\nretrieval-augmented generation to produce concise, actionable\nqualitative summaries for clinical interpretation and structured\nquantitative SDOH ratings for risk prediction modeling. The\nstructured SDOH ratings were used independently and in\ncombination with traditional laboratory biomarkers as inputs\nto linear and tree-based machine learning models (Ridge,\nLasso, Random Forest, and XGBoost) to demonstrate how\nunstructured narrative data can be applied in conventional risk\nprediction workflows. Finally, we evaluated several LLMs on\ntheir ability to predict a patient’s level of diabetes control (low,\nmedium, high) directly from interview text with A1C values\nredacted. LLMs achieved 60% accuracy in predicting diabetes\ncontrol levels from interview text. This work demonstrates\nhow LLMs can translate unstructured SDOH-related data into\nstructured insights, offering a scalable approach to augment\nclinical risk models and decision-making.\nI. INTRODUCTION\nSocial determinants of health (SDOH) such as financial\nstress, access to transportation, and social support—play a\nsignificant role in shaping diabetes experience and outcomes\n[1]. However, social determinants are often missing from\nelectronic health record (EHR) data, which serves as the\nfoundation for most risk prediction models and provides\nessential context for clinicians delivering care [2, 3]. Truong\net al show that only 1.9% of admissions include ICD-10\nZ-codes, which are the only diagnosis codes that identify\nnon-medical factors that may affect a patient’s health [4].\n*This work was supported by the Stanford Center for Asian Health\nResearch and Education (CARE).\n1Sasha\nRonaghi,\nPrerit\nChoudhary\nare\nwith\nthe\nDepartment\nof\nComputer\nScience,\nStanford\nUniversity,\nStanford,\nCA\n94305\nUSA\nsronaghi@stanford.edu, preritc@stanford.edu\n2David H. Rekhopf is with the Department of Epidemiology and Popula-\ntion Health, Department of Medicine (Division of Primary Care and Popu-\nlation Health), Department of Pediatrics, and Department of Health Policy,\nStanford University School of Medicine, and Department of Sociology, Stan-\nford University, Stanford, CA 94305. drehkopf@stanford.edu\n3Bryant\nLin\nis\nwith\nthe\nDepartment\nof\nMedicine,\nStanford\nUniversity\nSchool\nof\nMedicine,\nStanford,\nCA\n94305\nUSA.\nbylin@stanford.edu\nTraditionally, questionnaire-style screenings have been the\nprimary method for collecting individual-level SDOH data\n[5] as they allow for large-scale data collection and potential\nintegration with EHR models. Commonly used tools include\nPRAPARE [6] and the Center for Medicare and Medicaid\nServices’ Accountable Health Communities screening tool\n[7]. The limited structure and pre-defined categories within\nthese tools often fail to capture the richness of patients’\nlived experiences and the unique needs and circumstances of\ndifferent patient demographics. To overcome this challenge,\nclinics are increasingly utilizing home-grown, customized\nquestionnaires tailored to a clinic’s patient demographics,\nwhich can present data interoperability challenges for larger-\nscale population health analysis [5]. This trend highlights\nthe need for a more flexible mode of SDOH data collection.\nAdditionally, although diabetes is influenced by behaviors,\npsychological characteristics, and socioeconomic factors [8],\nexisting risk prediction models do not integrate these do-\nmains [9]. This underscores the need for larger-scale data\ncollection methodologies that capture behaviors, psycholog-\nical characteristics, and socioeconomic factors in order to\ndevelop more holistic diabetes risk prediction tools.\nCollecting social determinant data in an unstructured man-\nner, such as through clinical encounters or free-text boxes,\npresents an opportunity for capturing richer, more nuanced\ndetails about patients’ lives, including aspects of their disease\nmanagement and social context that are rarely addressed\nin structured data collection methods [10]. However, until\nrecently, the analysis of unstructured data using traditional\nmachine learning models was constrained by the need for\nextensive manual preprocessing and feature extraction [11].\nLarge language models (LLMs) now present a transformative\nopportunity to process and analyze unstructured language\nefficiently and at scale [12]. LLMs excel at understanding\ncontext, identifying patterns, and extracting key insights from\nfree-text narratives, enabling the discovery of novel risk\nfactors and relationships that were previously inaccessible.\nThis study leverages the capabilities of LLMs to analyze\nunstructured life stories from 65 T2D patients aged 65 years\nand older, focusing on their day-to-day disease management,\ndiabetes literacy, and SDOH-related challenges. Although we\napply these tools to a relatively small sample size, this work\nis aimed to serve as a proof of concept to illustrate how\nLLMs can analyze large quantities of unstructured qualitative\ndata quickly for use within the clinical setting and for\nquantitative risk prediction modeling.\nImportantly, the study was conducted in a cohort with\nhigh Asian American and Pacific Islanders (AAPI) repre-\narXiv:2601.13388v1  [cs.CL]  19 Jan 2026\n"}, {"page": 2, "text": "sentation (46.2% of participants), providing valuable insight\ninto this population. AAPI have a higher type 2 diabetes\n(T2D) prevalence and are less likely to take anti-diabetic\nmedications than non-Hispanic White individuals [13]. De-\nspite these disparities, AAPI are underrepresented in both\nepidemiological and genomic T2D research and the inherent\ndiversity of these populations is often overlooked [14]. To our\nknowledge, this is one of the first times, across any disease\noutcome, that new methodologies for prediction are being\ndeveloped with a focus on AAPI populations first, rather\nthan as an afterthought.\nII. METHODS\nOur study population consisted of 65 participants. These\nparticipants were recruited directly from a Stanford Health-\ncare clinic. Participants were primarily aged 65–69 (28.6%),\n70–74 (25.4%), and 75–79 (27.0%), with smaller proportions\naged 80–84 (9.5%) and 85–89 (9.5%), and 95–99 (1.6%).\n46.2% of participants identify as Asian American and Pacific\nIslander, 32.3% as White, 12.3% as Hispanic, 7.7% as Black,\nand 6.2% as Middle Eastern/North African. The gender\ndistribution of the population includes 70.5% identifying as\nmale, 29.5% as female. The data for the project was obtained\nunder IRB approval from Stanford University.\nWe conducted unstructured interviews asking general con-\nversational questions about patients’ life stories and ex-\nperiences with diabetes, enabling participants to discuss\ntopics they find significant. This approach ensures visibility\ninto their lived experiences and highlights self-identified\nchallenges and priorities. Interview lengths varied, with a\nminimum word count of 236, a maximum of 7,366, and\nmost under 1,500 words. Alongside these narratives, we\ncollected the full electronic health record clinical data for\neach participant. The mean most recent A1C level among\nparticipants was 6.83, with a median of 6.6, a standard\ndeviation of 1.05, and a range from 4.5 to 10.3.\nWe employ Stanford SecureGPT [15], which enables se-\ncure utilization of ChatGPT models for high-risk, protected\nhealth information. Using this platform, we applied LLMs\nto our dataset through three methods:\n1) Extracting structured, free-form SDOH data: Con-\nverting unstructured narrative content into structured,\nactionable insights.\n2) Using extracted SDOH data for risk prediction:\nGenerating quantitative variables from qualitative nar-\nratives to serve as inputs for machine learning-based\nrisk prediction models.\n3) Predicting level of diabetes control: Asking LLMs\nto predict a patient’s level of diabetes control from the\nunstructured narratives.\nA. Extracting structured, free-form SDOH data\nBecause the interviews were unstructured, they covered\na wide range of topics. To identify and organize themes\nwithin the dataset, we prompted ChatGPT-4o to read through\neach interview transcript, identify main themes and sub-\nthemes, and provide three supportive quotes for each theme\n(Prompt 1 in Appendix). To further consolidate the dataset,\nwe used sentence transformers to generate embeddings for\nthe codes and applied agglomerative clustering to group\nsimilar codes. These clustered codes were manually reviewed\nand consolidated into representative codes, resulting in 16\nkey risk factors: level of knowledge about T2D management,\nsocial support, outlook towards their condition, and their day-\nto-day habits, socioeconomic status, education, occupation,\nimmigration status, smoking status, alcohol consumption,\nphysical activity, diet, air quality, social support, and access\nto green space, public transport, health services.\nNext, we aimed to systematically identify the presence\nof these risk factors across all interviews. We used retrieval-\naugmented generation (RAG) as our approach, leveraging its\nproven ability to reduce hallucinations and improve factual\naccuracy in responses [17]. First, we generated embeddings\nfor each interview using OpenAI’s text-embedding-3-large\nembedding model [18]. Using the embeddings, our workflow\nretrieved the most relevant excerpts for each risk factor and\npassed the excerpts to ChatGPT-4o, which was prompted to\nprovide bullet-point summaries of each patient’s relationship\nto the risk factor, along with supporting quotes from the\ntranscript (Prompt 2 in Appendix). From this process, we\nidentified five topics, each with three sub-topics, as the most\nfrequently discussed across all interviews:\n1) Socioeconomic Status: Income Level, Housing, Finan-\ncial Stress\n2) Diet: Diet Type, Food Preferences, Dietary Restrictions\n3) Social Support: Family Support, Friends, Social Net-\nworks\n4) Health Services: Healthcare Utilization, Satisfaction\nwith Services, Barriers to Care\n5) Information on Diabetes Management: Knowledge\nLevel, Self-Care Practices, Medication Adherence\nFor each of these five topics and their subtopics, we devel-\noped a 1-to-5 rating scale (in Appendix), where higher scores\nindicated more positive outcomes or stronger alignment with\nbest practices. The LLM was prompted to assign a rating\nbased on the scale, provide a justification in concise bullet\npoints, and include supporting quotes from the transcript\n(Prompt 3 in Appendix). If the topic was not in the interview,\nthe LLM was asked to provide a rating of -1.\nTo validate these ratings and ensure their accuracy, we\nmanually reviewed five interviews, comparing the LLM-\ngenerated ratings and justifications against human interpre-\ntations of the interviews.\nB. Using extracted SDOH data for risk prediction\nIn this section, we build risk models for predicting a\npatient’s A1C level using three feature sets of data from the\n65 patients:\n• Social Determinants of Health (SDOH): We use the\nnumerical ratings of each patient’s SDOH risk factors,\nextracted by ChatGPT-4o from the interviews as de-\nscribed in Method A.\n• Patient laboratory results: For each patient, we collect\ntheir most recent values for triglycerides, high-density\n"}, {"page": 3, "text": "lipoprotein (HDL), low-density lipoprotein (LDL), glu-\ncose, and creatinine. This serve as a standard control\ndataset for the risk prediction task, as prior studies have\ndemonstrated that glucose, lipid panels, and creatinine\nare key predictors of diabetes onset, progression, and\nrelated complications [19, 20, 21, 22].\n• Combined SDOH + labs: We combine both datasets\nto simulate the most realistic use case, where SDOH\ndata augments existing information in electronic health\nrecords to improve predictive performance.\nWe considered aiming to predict hospitalization risk based\non frequency of ED-to-hospitalizations and ED visits, how-\never, given the dataset size, the hospitalization data was\nsparse (73.9% of patients did not have hospitalizations since\nthe study start date in 2022).\nPrior to model development, the SDOH dataset underwent\nseveral preprocessing steps to ensure quality and robustness\nof the predictive models. Given that the interviews are free-\nform and cover a wide range of topics, not all themes\nnecessarily overlap across patient narratives. This is expected\nand naturally results in missing entries in the extracted\nSDOH factors for some interviews. Approximately 17%\nof the SDOH factors were missing across all patients and\nfactors. Because our dataset is small, removing these missing\nvalues would significantly reduce the available data. We\nuse K-Nearest Neighbors to impute the missing missing\nentries based on the similarity between observations. Then,\nto mitigate the impact of outliers and ensure that the features\ncontributed comparably to the model, we scale the input data\nto a range of 0–1 to allow for predictable regularization\nbehavior as well as preventing any sensitivity to the scale\nof inputs from the models.\nEach feature set was randomly divided into training and\ntesting sets with an 80/20 split. The training set was used for\nmodel development and hyperparameter tuning. The testing\nset was reserved for evaluating the model performance.\nFor all feature sets, we explored linear and tree-based\nregression approaches to predict A1C levels, modeled as a\ncontinuous target variable. The models developed include:\n• Random Forest Regressor: A tree-based ensemble\nmodel known for handling both linear and nonlinear\nrelationships and high-dimensional effectively.\n• XGBoost\nRegressor: Another tree-based ensemble\nmodel that optimizes for prediction accuracy through\niterative refinement.\n• Lasso Regression: A linear regression model with L1\nregularization to encourage sparsity in feature selection\nthrough mitigation of overfitting.\n• Ridge Regression: A linear regression model with\nL2 regularization to also prevent overfitting and better\nhandle potential multicollinearity.\nLinear regression was chosen for its effectiveness and\nsimplicity in regression tasks involving a single dependent\nvariable (A1C levels), while tree-based regression models\nwere employed for feature importance analysis. Feature\nimportance helps identify which variables most strongly\ninfluence model predictions. It is calculated using the Gini\nImportance metric, which measures the total reduction in\nerror or impurity (such as mean squared error) attributed to\neach feature across all trees in the ensemble.\nModel performance was assessed using R2 score as the\nprimary metric. The R2 score measures how well the model\nfits the data, in other words, the proportion of variance in\nthe dependent variable (A1C levels) that is predictable from\nthe independent variables (SDOH factors and/ or lab data).\nFor the tree-based models (Random Forest and XGBoost),\nhyperparameter tuning was performed using grid search with\ncross-validation, optimizing for the R2 score.\nC. Predicting level of diabetes control\nUnlike traditional regression models, which rely solely on\nstructured numerical inputs as training data, LLMs are built\non a foundation of extensive pretraining on diverse textual\ndata and can extract and synthesize information from un-\nstructured data. We tested the ability for OpenAI’s ChatGPT-\n4o, o1, and o1-mini models, as well as DeepSeek’s r1 model\nto predict the A1C level of the patient directly from patient\ninterview. We first asked ChatGPT-4o to remove all mentions\nof A1C from the interview by replacing A1C numerical\nvalues with \"[REMOVED]\" (Prompt 4 in Appendix) in order\nin order to prevent models from relying on explicit A1C\ninformation and instead assess their ability to infer control\nfrom the interview content. The models were then prompted\nwith the interview transcript and tasked with determining the\nA1C level of the patient and providing a justification with\nrelevant quotes (Prompt 4 in Appendix).\nWe categorized the A1C predictions and the patient’s true\nA1C values into three control levels (low, medium, high)\nbased on the distribution of the patient’s true A1C values as\nthe following:\n• Low: A1C < 6.0 (16.9% of patients)\n• Medium: 6.0 ≤A1C ≤7.5 (66.2% of patients)\n• High: A1C > 7.5 (16.9% of patients)\nTo evaluate the accuracy of each model, we compared the\ncontrol level of the model’s prediction with the true control\nlevel of the patient.\nIII. RESULTS AND DISCUSSION\nA. Extracting structured, free-form SDOH data\nFigure 1 illustrates the coverage of each subtopic across\ninterviews. Among the most frequently discussed topics, in-\ncome level, family support, social networks, and medication\nadherence were present in 90% or more of the interviews.\nThrough qualitative analysis, we found that most sum-\nmaries were relevant to the main topic. When analyz-\ning subtopics within the same domain, we observed that\nthe LLM sometimes repeated information across similar\nsubtopics (e.g., Dietary Restrictions and Diet Type). Separat-\ning subtopics allowed us to guide the LLM to retrieve more\ncomprehensive and specific insights for each area, though\nat the cost of some duplication. Figure 2 is an example of\na qualitative summary for one patient’s \"Social Support -\nSocial Networks\" factor.\n"}, {"page": 4, "text": "Fig. 1.\nPercentage distribution of how frequently a specific topic was\ndiscussed across patient interviews.\nBy asking the model to include quotes, we are able to\nverify the accuracy of extracted information and reduce the\nlikelihood of hallucination by the LLM. This method is\nparticularly valuable in clinical settings, where healthcare\nproviders often lack time to deeply explore patients’ social\ndeterminants. Through our method, providers can quickly\nunderstand aspects of a patient’s social context that influ-\nence their health. Furthermore, by being able to quantify a\npatient’s risk related to an SDOH factor, we can develop\npopulation-level data for risk modeling and public health\nstudies, as demonstrated in Method B.\nB. Using extracted SDOH data for risk prediction\nFigure 3 presents the performance of the four models in\npredicting A1C levels. Among them, only Ridge Regression\non the SDOH-only dataset (R2 = 0.046) and Random Forest\non the combined dataset (R2 = 0.013) achieved positive\ncross-validated R2 values. A positive R2 indicates that the\nmodel explains more variance in A1C levels than a baseline\nmodel that predicts the mean, reflecting modest predictive\nvalue on unseen data.\nThe low R2 values, along with the disparity between\nthe training and cross-validated R2 values, suggest that the\nmodels are overfitting to the training data, likely due to the\nsmall sample size relative to the number of predictors. With\nrelatively few observations and many variables, the models\nsuffer from low degrees of freedom, making it difficult to\nreliably estimate residual variance and resulting in large\nstandard errors in linear regression.\nWhen we augmented the SDOH-only models with tra-\nditional blood biomarker predictors, model performance\nfurther deteriorated, highlighting the challenges of high-\ndimensional inference in small samples. Since the blood\nbiomarkers within the lab data are highly correlated both\nwith one another and with the SDOH scores, introducing\nthem inflates variance in parameter estimates and amplifies\nmeasurement error. Although ensemble tree methods are\ndesigned to reduce overfitting, they also tend to struggle\nwith small datasets, producing overly specific splits that\nfail to generalize. In our case, this manifested in models\nwhose Residual Sum of Squares (RSS) exceeded the Total\nSum of Squares (TSS), i.e. worse than predicting the mean\nRating: 4\nJustification: The interviewee describes having a\nstrong network of friends with whom they have\nconsistent and meaningful interactions, indicating a\nhigh level of social support.\n- The interviewee has a strong connection with long-\ntime friends who provide consistent social interaction.\n[Quote]: \"I can reach out to any of my friends, and\nwe talk on the phone for an hour, whatever”\n- The interviewee values their friendships despite dif-\nfering political views, indicating a deep and accepting\nrelationship.\n[Quote]: \"I’ve known her for 46 years. I know exactly\nwhat she’s going to say. I know how her feelings\nare, and you know about this and that, and she’s a\nconservative Republican, but I love her anyway.\"\n- The interviewee frequently invites friends to enjoy\ntheir garden, showing an active effort to maintain and\nshare their interests with friends.\n[Quote]: \"I always invite my friends to my garden,\nbecause I can’t go out and work in it anymore.\"\n- Despite enjoying solitude, the interviewee empha-\nsizes that they are never lonely due to their strong\nsocial connections and personal interests.\n[Quote]: \"I like to be in my silence and and alone.\nAnd so people sometimes say to me, oh, you know,\nI don’t want you to be lonely. It’s never lonely. I am\nalone, but I’m never lonely, and I have my books, I\nhave my artwork around me.\"\nFig. 2.\nLLM generated summary for \"Social Support - Social Networks\"\nanalysis for one patient\nSDOH\nLabs\nCombined\nModel\nTrain\nCV\nTrain\nCV\nTrain\nCV\nLasso\n0.000\n–0.038\n0.001\n–0.008\n0.001\n–0.090\nRidge\n0.248\n0.046\n0.028\n–0.027\n0.217\n–0.015\nRandom Forest\n0.704\n–0.053\n0.713\n–0.063\n0.782\n0.013\nXGBoost\n0.995\n–0.194\n0.948\n–0.112\n0.999\n–0.103\nFig. 3.\nTraining and cross-validated R2 by model and feature set.\nfor every observation. These findings suggest that, without\na substantially larger sample size or careful dimensionality\nreduction, adding more health-related variables may worsen\nrather than improve model validity.\nFigure 4 shows the feature importance analysis of the\nRandom-Forest model on the combined dataset. Glucose,\ncreatinine, and triglyceride levels have the most importance\nfollowed by Financial Stress and Diet Type in the com-\nbined model. This indicates that in combination, objective\nbiomarkers remain powerful predictors, but are limited with\na small sample size when assessing overall model prediction\ncapability. Although biomarkers such as glucose, creatinine,\nand triglycerides are known to be correlated with A1C levels\nin the broader literature, our data-constrained study shows a\ncollapse in their predictive power, likely due to the very few\n"}, {"page": 5, "text": "Rank\nFeature\nImportance\n1\nGlucose\n0.216\n2\nCreatinine\n0.187\n3\nTriglyceride\n0.124\n4\nSocioeconomic Status – Financial Stress\n0.098\n5\nDiet – Diet Type\n0.064\n6\nHDL\n0.056\n7\nLDL\n0.050\n8\nDiet – Food Preferences\n0.047\n9\nSocial Support – Friends\n0.032\n10\nSocial Support – Family Support\n0.027\nFig. 4.\nFeature Importance for Combined Feature Set, Random-Forest\neffective degrees of freedom for the models, given the large\nfeature space in contrast to a small dataset. As a result, even\nthough these features have higher importance than the SDOH\nfeatures, they still aren’t able to provide enough predictive\nvalue within our models, but still provide insight into the\ngeneral standing of the different features within our model.\nC. Predicting level of diabetes control\nOur results indicate that the LLM can reasonably process\nand contextualize patient narratives. As shown in figure 5, the\ncomparative evaluation demonstrates that GPT-4o attains the\nhighest overall accuracy (60%), outperforming o1 (53.85%),\nDeepSeek (50.77%), and o1-mini (50.82%).\nModel\nLow\nMedium\nHigh\nOverall Accuracy\nGPT4-O\n0.0% (0/11)\n75.6% (34/43)\n38.5% (5/11)\n60.0% (39/65)\nO1\n11.1% (1/11)\n68.2% (30/43)\n33.3% (4/11)\n53.8% (35/65)\nO1-Mini\n0.0% (0/10)\n64.2% (26/40)\n37.0% (5/11)\n50.8% (31/61)\nDeepSeek\n0.0% (0/11)\n67.4% (29/43)\n32.0% (4/11)\n50.8% (33/65)\nFig. 5.\nCategorical and overall accuracy for each model across 65 patients.\no1-mini did not provide a response for 4 patients.\nThrough qualitatively analyzing A1C prediction justifica-\ntions, we found that every justification included commentary\non the person’s lifestyle, whereas fewer mention strictly\nclinical markers. This pattern indicates that the models rely\nmore on social- and behavioral-oriented cues when judging\na patient’s level of diabetes control. This suggests that, even\nwithout prompting LLMs to extract SDOH factors as in\nMethod 1, LLMs can be used to generate concise accounts\nof the patient’s behavioral and environmental contexts.\nChatGPT-4o has unique behaviors that could explain its\nperformance advantage. ChatGPT-4o justifications typically\ninclude exactly one quote about the patient’s risk factors,\nlifestyle, and clinical experience. The quotes tend to be\nfrom three different parts of the transcript, whereas the other\nmodels tended to pick quotes that are from within the same\nsection of the interview. This results in ChatGPT-4o having\nmore balanced responses that potentially capture contrasting\nelements. In contrast, the reasoning models (o1, o1-mini,\nDeepSeek R1) have quotes that fit a more cohesive narrative.\nFor example, for a medium-control patient, ChatGPT-4o uses\nthe quotes “I walk the dog every morning” and “doctor said\npre-diabetic two years ago,” citing both improvement and\nhistorical warning to accurately determine medium control.\nFor the same patient, o1 chooses \"I walk the dog every\nmorning\" and \"cutting carbs\" which suggest lifestyle im-\nprovement and lead the model to determine low control. Ad-\nditionally, ChatGPT-4o is more literal. ChatGPT-4o takes “no\ndifficulty” and “controlled” statements at face value, whereas\no1 cross-checks self-report with context (e.g., continued\nmedication). ChatGPT-4o also tends to be more conservative\nin risk weighting. For example, any unresolved risk (e.g.,\nmedication lapses, comorbidities) nudges ChatGPT-4o into\na higher A1C level, whereas o1 is more optimistic and, if\na patient speaks about self-improvement, the prediction is a\nlower A1C level.\nNone of the models handle the low category well.\nFor ChatGPT-4o, all patients were incorrectly classified as\nmedium likely due to mentions of medication use and past\ndiagnosis of diabetes. The models performed equally well\nfor the high category, although for different cases. We found\nthat high-A1C patients often discussed both past crises and\nrecent lifestyle improvements or medication changes and,\nwithout temporal anchoring, models typically averaged the\ntwo narratives to determine medium.\nWhile ChatGPT-4o demonstrated moderate accuracy over-\nall, certain edge cases revealed scenarios where the model\nwas \"fooled\" by the contextual complexity of patient nar-\nratives. These cases provide interesting insights into the\nmodel’s strengths and limitations:\n• Patient 1: The model incorrectly classified this patient\nas having severe diabetes. This was likely due to the\npatient’s mention of near-blindness, a symptom often\nassociated with advanced-stage diabetes. However, their\nactual A1C level was low, suggesting that the blindness\nwas unrelated to their diabetes condition.\n• Patient 2: The model predicted a lower A1C level for\nthis patient based on their statement, \"I am prediabetic\nand shouldn’t be classified as diabetic.\" In reality, their\ntrue A1C level was higher, indicating that the patient’s\nself-reported status misled the model.\n• Patient 3: This patient had a low A1C level, which\nmight suggest better diabetes control. However, their\ninterview revealed that they had been hospitalized mul-\ntiple times and had been on insulin therapy for over\n20 years. This context indicates a much more severe\ndiabetes condition than their A1C alone would suggest.\nThese findings highlight the potential and challenges of\nusing LLMs for nuanced healthcare tasks, and the need\nfor manual verification of the LLM’s assessments using the\nquotes and justification provided by the LLM.\nIV. CONCLUSIONS\nThis work serves as a proof of concept for integrating un-\nstructured patient narratives into clinical workflows through\nthe use of large language models (LLMs). We envision a\nworkflow in which patients provide unstructured information\nabout their lives through a variety of convenient options\n– pre-appointment interviews, free-form text boxes, virtual\nrecordings, etc – and large language models (LLMs) generate\n"}, {"page": 6, "text": "actionable, verifiable summaries, quantitative risk scores, and\nassessments of health management. This approach highlights\nthe potential for transforming how social determinants of\nhealth (SDOH) and other qualitative data are utilized in\nhealthcare to provide more personalized and effective care.\nThe main limitation of this study is the relatively small\nsample size of 65 participants, which affected the ability\nof machine learning models to find strong correlations or\nachieve robust predictive performance. Additionally, the re-\nliance on self-reported narratives introduces the potential for\nsocial desirability bias, where patients might unintentionally\nreport their experiences and behaviors in a more favorable\nlight. We also compare LLM output to each patient’s most\nrecent A1C level and attempt to predict that value using ma-\nchine learning models. Incorporating longitudinal A1C data\nmay offer a more meaningful comparison because it reflects\ntrends and variability in control over time, aligning more\nclosely with the narrative nature of the patient interviews.\nIn order to validate the accuracy compared to a physician,\nfuture work involves comparing A1C predictions and justifi-\ncations with at least two physician experts. Such a compari-\nson would provide deeper insights into the model’s accuracy\nand limitations. The reliance on Stanford SecureGPT, while\nensuring secure handling of protected health information,\nalso poses challenges related to accessibility and scalability.\nLocally hosted LLMs could address this issue, though they\nmay introduce additional costs.\nFurther research is required to determine how these tools\ncan be seamlessly integrated into clinical workflows. Under-\nstanding the practical requirements of physicians and tailor-\ning the tool to meet their specific needs will be critical for\nmaximizing its impact and adoption. Developing strategies\nto balance efficiency with transparency will be essential to\nbuilding trust among clinicians and patients.\nThrough these advancements, we aim to move beyond\na proof of concept towards a clinically validated, scalable\nsystem. Such a system has the potential to enhance diabetes\nmanagement by incorporating key social and contextual\nfactors into the clinical workflow and risk prediction models.\nWe also hope to expand this approach to other chronic\ndisease areas, leveraging the flexibility of general LLMs to\naddress broader healthcare challenges.\nREFERENCES\n[1] Hill-Briggs, F., Adler, N. E., Berkowitz, S. A., Chin, M. H., Gary-\nWebb, T. L., Navas-Acien, A., Thornton, P. L., & Haire-Joshu, D.\n(2020). Social Determinants of Health and Diabetes: A Scientific\nReview. Diabetes care, 44(1), 258–279. Advance online publication.\nhttps://doi.org/10.2337/dci20-0053\n[2] Goldstein, B. A., Navar, A. M., Pencina, M. J., & Ioannidis, J. P.\n(2017). Opportunities and challenges in developing risk prediction\nmodels with electronic health records data: a systematic review.\nJournal of the American Medical Informatics Association : JAMIA,\n24(1), 198–208. https://doi.org/10.1093/jamia/ocw042\n[3] Office of the National Coordinator for Health Information Technology.\n‘National Trends in Hospital and Physician Adoption of Electronic\nHealth Records,’ Health IT Quick-Stat #61.\n[4] Truong, Hannah P. BA*; Luke, Alina A. MPH*; Hammond, Gmerice\nMD, MPH*; Wadhera, Rishi K. MD, MPP†; Reidhead, Mat MA‡;\nJoynt Maddox, Karen E. MD, MPH*,§. Utilization of Social De-\nterminants of Health ICD-10 Z-Codes Among Hospitalized Patients\nin the United States, 2016–2017. Medical Care 58(12):p 1037-1043,\nDecember 2020. | DOI: 10.1097/MLR.0000000000001418\n[5] Li, C., Mowery, D. L., Ma, X., Yang, R., Vurgun, U., Hwang, S.,\nDonnelly, H. K., Bandhey, H., Akhtar, Z., Senathirajah, Y., Sadhu,\nE. M., Getzen, E., Freda, P. J., Long, Q., & Becich, M. J. (2024).\nRealizing the Potential of Social Determinants Data: A Scoping\nReview of Approaches for Screening, Linkage, Extraction, Analysis\nand Interventions. medRxiv : the preprint server for health sciences,\n2024.02.04.24302242. https://doi.org/10.1101/2024.02.04.24302242\n[6] National Association of Community Health Centers, Association of\nAsian Pacific Community Health Organizations, Association OPC,\nInstitute for Alternative Futures. 2016. The Protocol for Responding to\nand Assessing Patients’ Assets, Risks, and Experiences (PRAPARE)\n[Internet]. Available at: www.nachc.org/prapare.\n[7] \"AHCM\nScreening\nTool\nCompanion.\"\nCenters\nfor\nMedicare\n&\nMedicaid\nServices\n(CMS),\nn.d.,\nhttps://www.cms.gov/priorities/innovation/media/document/ahcm-\nscreeningtool-companion.\n[8] Ismail, Leila, Huned Materwala, and Juma Al Kaabi. \"Association of\nrisk factors with type 2 diabetes: A systematic review.\" Computational\nand structural biotechnology journal 19 (2021): 1759-1785.\n[9] Collins, Gary S., et al. \"Developing risk prediction models for type\n2 diabetes: a systematic review of methodology and reporting.\" BMC\nmedicine 9 (2011): 1-14.\n[10] Edin KJ, Fields CD, Grusky DB, Leskovec J, Mattingly MJ, Olson K,\nVarner C. Listening to the Voices of America. RSF: The Russell Sage\nFoundation Journal of the Social Sciences. 2024 Sep 1;10(4):1-31.\n[11] Hossain\nE,\nRana\nR,\nHiggins\nN,\nSoar\nJ,\nBarua\nPD,\nPisani\nAR, Turner K. Natural Language Processing in Electronic Health\nRecords\nin\nrelation\nto\nhealthcare\ndecision-making:\nA\nsystem-\natic\nreview.\nComput\nBiol\nMed.\n2023\nMar;155:106649.\ndoi:\n10.1016/j.compbiomed.2023.106649. Epub 2023 Feb 10. PMID:\n36805219.\n[12] Humza Naveed and Asad Ullah Khan and Shi Qiu and Muham-\nmad\nSaqib\nand\nSaeed\nAnwar\nand\nMuhammad\nUsman\nand\nNick\nBarnes\nand\nAjmal\nS.\nMian\n(2023),\nA\nComprehensive\nOverview\nof\nLarge\nLanguage\nModels,\nArXiv,\nabs/2307.06435,\nhttps://api.semanticscholar.org/CorpusID:259847443\n[13] Yoshida, Yilin, and Vivian A. Fonseca. \"Diabetes control in Asian\nAmericans—Disparities and the role of acculturation.\" Primary Care\nDiabetes 15.1 (2021): 187-190.\n[14] Sentell, T. L., et al. \"The burden of diagnosed and undiagnosed dia-\nbetes in Native Hawaiian and Asian American hospitalized patients.\"\nJournal of clinical & translational endocrinology 2.4 (2015): 115-124.\n[15] Stanford\nHealth\nCare\nand\nStanford\nSchool\nof\nMedicine.\n(n.d.). Secure GPT (beta). Retrieved October 22, 2024, from\nhttps://securegpt.stanfordhealthcare.org/chat\n[16] Hugging Face. (n.d.). all-MiniLM-L6-v2 [Computer software]. Re-\ntrieved\nJanuary\n27,\n2025,\nfrom\nhttps://huggingface.co/sentence-\ntransformers/all-MiniLM-L6-v2/blob/main/README.md\n[17] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J.,\nGuo, Q., Wang, M., & Wang, H. (2023). Retrieval-Augmented Gener-\nation for Large Language Models: A Survey. ArXiv, abs/2312.10997.\n[18] OpenAI. (n.d.). Embeddings guide. Retrieved January 27, 2025, from\nhttps://platform.openai.com/docs/guides/\n[19] Wilson, P. W. F., Meigs, J. B., Sullivan, L., Fox, C. S., Nathan,\nD.\nM.,\nand\nD’Agostino,\nR.\nB.\n\"Prediction\nof\nincident\ndia-\nbetes mellitus in middle-aged adults: The Framingham Offspring\nStudy.\" Archives of Internal Medicine 167.10 (2007): 1068–1074.\nhttps://doi.org/10.1001/archinte.167.10.1068\n[20] American Diabetes Association. \"Standards of medical care in\ndiabetes—2024.\" Diabetes Care 47.Supplement_1 (2024): S1–S2.\nhttps://doi.org/10.2337/dc24-S001\n[21] Levey, A. S., Stevens, L. A., Schmid, C. H., Zhang, Y. L., Castro,\nA. F., Feldman, H. I., Kusek, J. W., Eggers, P., Van Lente, F.,\nGreene, T., and Coresh, J. \"A new equation to estimate glomerular\nfiltration rate.\" Annals of Internal Medicine 150.9 (2009): 604–612.\nhttps://doi.org/10.7326/0003-4819-150-9-200905050-00006\n[22] Fox, C. S., Coady, S., Sorlie, P. D., Levy, D., Meigs, J. B.,\nD’Agostino, R. B., Wilson, P. W., and Savage, P. J. \"Increas-\ning cardiovascular disease burden due to diabetes mellitus: The\nFramingham Heart Study.\" Circulation 109.5 (2004): 724–730.\nhttps://doi.org/10.1161/01.CIR.0000115901.39823.18\n"}, {"page": 7, "text": "APPENDIX\nA. LLM prompts\nPrompt 1: You are an AI assistant specializing in qualitative analysis. Read the\nfollowing interview transcript with a diabetes patient and identify the main themes\nand sub-themes discussed. For each theme, provide: - A concise code (theme) that\nrepresents the key concept. - Up to three relevant direct quotes from the transcript\nthat illustrate the theme. Provide the output in valid JSON format as a list of objects,\nwhere each object has the following structure: \"code\": \"Code representing the theme\",\n\"quotes\": [\"Quote 1\", \"Quote 2\", \"Quote 3\"]\nDo not include any additional text or\nexplanations outside the JSON. Interview Transcript: (interview text) Extracted Codes\nand Quotes:\nPrompt 2:\nYou are an expert analyst extracting key information from an interview with a\ndiabetes patient about their life story. The patients are all older than 50. **Task**:\nExtract and list only the key pieces of information specifically about \"topic\" from the\nfollowing interview excerpts. **Instructions**: - Provide the information as a bullet-\npoint list. - Start each bullet point with a standardized format: \"- [Keyword][Time\nFrame]: Detail\" - Use only the following predefined keywords for \"{topic}\": {key-\nwords} - Pay close attention to the time frames mentioned, and accurately reflect\nwhether the information is about the past, present, or future. - Indicate the time\nframe in each bullet point using [Past], [Present], or [Future]. - Focus exclusively\non \"{topic}\" and exclude any unrelated details. - If there is no information about\n\"topic\" in the interview excerpts, respond exactly with \"No information available.\"\n- Do not include introductions, explanations, summaries, or conclusions. - Limit the\nlist to a maximum of 5 bullet points. **Example**: - [Exercise][Past]: Used to play\nbasketball in college. - [Diet][Present]: Currently avoids sweets and limits rice intake.\n- [Work Routine][Future]: Plans to retire next year. **Interview Excerpts**: context\n**Answer**:\nPrompt\n3: Subtopic: {subtopic_label} Below is the rating scale for this\nsubtopic:{scale_text} Here is the relevant content from the interview: {subtopic_text}\nTASK: 1) Decide on an integer rating for the subtopic based on the scale above from\n1..5, or -1 if not present. 2) Provide a short justification or rationale for why you chose\nthat rating. 3) Then follow these instructions for summarizing and quoting material\nfrom the interview: Provide a concise and specific answer in 3-5 bullet points. Each\nbullet point must include: 1. A clear summary of the insight or opinion in a single\nsentence. 2. A full supporting quote from the interviews, written verbatim, with no\nomissions or ellipses. Ensure the quote is at least 2-3 sentences long and provides\nenough context for the reader to understand its meaning fully. 3. If contradictory\nopinions exist, provide them in a separate bullet point with a full quote. 4. Ensure\nthe entire output adheres to the above criteria. Formatting: <your summary> Quote:\n<verbatim quote> Return your answer in the following structure: Rating: <numeric>\nJustification: <1-2 sentence reason> Summary And Quotes: <Bullet points following\ninstructions>\nPrompt 4: Please remove all explicit mentions of A1C levels from the following\ninterview text. Replace any specific A1C numbers or ranges with ’[REMOVED]’ while\npreserving the rest of the context and meaning of the sentences. Keep all other diabetes-\nrelated information intact. Interview Content: {interview_text} Please return only the\nmodified text without any additional commentary.\nPrompt 5: Based on the following interview content, please: 1. Predict the person’s\nA1C level to .1 accuracy and only use one number, not a range. 2. Do not use\nmentions of A1C in the text to determine the level of diabetes control. 3. Provide\na detailed justification with relevant quotes from the interview Interview Content:\n{interview_text} Please format your response exactly as follows: A1C Level: [A1C\nlevel prediction] Justification: [Your detailed analysis] Supporting Quotes: [At least\n2-3 relevant quotes from the interview]\nB. Scale for Rating Patients on SDOH factors\nSocioeconomic Status - Income Level\n•\n1: Extreme financial hardship; below the poverty line; unable to meet basic\nneeds (food, shelter).\n•\n2: Low income; struggles with consistent basic needs but has some access to\nsocial support or assistance.\n•\n3: Moderate income; meets basic needs but has no financial flexibility.\n•\n4: Stable income; meets needs and has moderate financial security.\n•\n5: High income; strong financial security with savings/investments.\n•\n-1: Not mentioned or irrelevant in the interview.\nSocioeconomic Status - Housing\n•\n1: Homeless or living in unstable, temporary housing.\n•\n2: Inconsistent housing; risk of eviction or unsafe conditions.\n•\n3: Stable housing but low-quality or unsafe environment.\n•\n4: Stable, safe housing; moderate quality.\n•\n5: High-quality, stable housing in a safe neighborhood.\n•\n-1: Not mentioned or irrelevant in the interview.\nSocioeconomic Status - Financial Stress\n•\n1: Severe financial stress; constant worry about affording essentials.\n•\n2: Frequent financial stress; periodic struggle meeting expenses.\n•\n3: Moderate financial stress; occasional worries but generally manageable.\n•\n4: Minimal financial stress; able to pay bills comfortably most of the time.\n•\n5: No financial stress at all; feels fully secure financially.\n•\n-1: Not mentioned or irrelevant in the interview.\nDiet - Diet Type\n•\n1: Diet dominated by unhealthy, processed foods.\n•\n2: Primarily unhealthy with occasional healthy items.\n•\n3: Balanced mix but with some indulgences.\n•\n4: Mostly healthy with rare unhealthy moments.\n•\n5: Consistently healthy, nutrient-rich diet.\n•\n-1: Not mentioned or irrelevant in the interview.\nDiet - Food Preferences\n•\n1: Strong preference for high-calorie, low-nutrient foods.\n•\n2: Limited interest in healthy foods.\n•\n3: Moderately open to healthy options.\n•\n4: Prefer healthier foods, tries to avoid junk.\n•\n5: Actively seeks out nutrient-rich, healthy foods.\n•\n-1: Not mentioned or irrelevant in the interview.\nDiet - Dietary Restrictions\n•\n1: Ignores recommended restrictions entirely.\n•\n2: Very quick to lapse or cheat on them.\n•\n3: Moderate adherence to dietary restrictions, but not perfectly consistent.\n•\n4: Generally follows restrictions with occasional slips.\n•\n5: Fully compliant; no major slips.\n•\n-1: Not mentioned or irrelevant in the interview.\nSocial Support - Family Support\n•\n1: No family support; may feel isolated.\n•\n2: Minimal or inconsistent family assistance.\n•\n3: Some reliable help, but not comprehensive.\n•\n4: Reliable and significant family support.\n•\n5: Very strong or exceptional family network.\n•\n-1: Not mentioned or irrelevant in the interview.\nSocial Support - Friends\n•\n1: No friends; isolated socially.\n•\n2: Minimal friend support; rare interactions.\n•\n3: Moderate friend support; some emotional or practical help.\n•\n4: Strong friend network; consistent help.\n•\n5: Exceptional friend support; highly dependable and engaged.\n•\n-1: Not mentioned or irrelevant in the interview.\nSocial Support - Social Networks\n•\n1: Completely disconnected; no broader community ties.\n•\n2: Limited social involvement; rare group activities.\n•\n3: Some involvement with local community or social groups.\n•\n4: Active in community or group settings regularly.\n•\n5: Exceptionally integrated into multiple networks.\n•\n-1: Not mentioned or irrelevant in the interview.\nHealth Services - Healthcare Utilization\n•\n1: No regular healthcare visits; major gaps in care.\n•\n2: Rare or only emergency visits.\n•\n3: Irregular but some routine check-ups.\n•\n4: Consistent appointments and fairly comprehensive care.\n•\n5: Excellent utilization; very proactive approach.\n•\n-1: Not mentioned or irrelevant in the interview.\nHealth Services - Satisfaction with Services\n•\n1: Deeply dissatisfied; ongoing issues.\n•\n2: Mostly dissatisfied; some minor positive points.\n•\n3: Neutral or mixed feelings; basics met.\n•\n4: Generally satisfied; no major issues.\n•\n5: Extremely satisfied; fully trusts providers.\n•\n-1: Not mentioned or irrelevant in the interview.\nHealth Services - Barriers to Care\n•\n1: Intense barriers (cost, transport, discrimination).\n•\n2: Multiple major barriers limiting access.\n•\n3: Some barriers but partially manageable.\n•\n4: Few barriers; mostly minor.\n•\n5: No barriers to care.\n•\n-1: Not mentioned or irrelevant in the interview.\nInformation on Diabetes Management - Knowledge Level\n•\n1: No knowledge of diabetes management.\n•\n2: Very little awareness of key concepts.\n•\n3: Moderate knowledge; some gaps remain.\n•\n4: Good knowledge base; fairly solid understanding.\n•\n5: Excellent knowledge, possibly well-educated about diabetes.\n•\n-1: Not mentioned or irrelevant in the interview.\nInformation on Diabetes Management - Self-Care Practices\n•\n1: No self-care for diabetes management.\n•\n2: Rare or minimal attempts at self-management.\n•\n3: Some consistent efforts but not comprehensive.\n•\n4: Regular, reliable self-care habits.\n•\n5: Exemplary self-care; very proactive and thorough.\n•\n-1: Not mentioned or irrelevant in the interview.\nInformation on Diabetes Management - Medication Adherence\n•\n1: Never follows medication schedule.\n•\n2: Very poor adherence; frequent lapses.\n•\n3: Moderate adherence; occasional misses.\n•\n4: Strong adherence; minor lapses only.\n•\n5: Perfect adherence; no missed doses.\n•\n-1: Not mentioned or irrelevant in the interview.\n"}]}