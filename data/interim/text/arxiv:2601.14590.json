{"doc_id": "arxiv:2601.14590", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.14590.pdf", "meta": {"doc_id": "arxiv:2601.14590", "source": "arxiv", "arxiv_id": "2601.14590", "title": "Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design and Sensor Data Augmentation", "authors": ["Shovito Barua Soumma", "Asiful Arefeen", "Stephanie M. Carpenter", "Melanie Hingle", "Hassan Ghasemzadeh"], "published": "2026-01-21T02:04:08Z", "updated": "2026-01-21T02:04:08Z", "summary": "Counterfactual explanations (CFEs) provide human-centric interpretability by identifying the minimal, actionable changes required to alter a machine learning model's prediction. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. We conduct a comprehensive evaluation of CF generation using large language models (LLMs), including GPT-4 (zero-shot and few-shot) and two open-source models-BioMistral-7B and LLaMA-3.1-8B, in both pretrained and fine-tuned configurations. Using the multimodal AI-READI clinical dataset, we assess CFs across three dimensions: intervention quality, feature diversity, and augmentation effectiveness. Fine-tuned LLMs, particularly LLaMA-3.1-8B, produce CFs with high plausibility (up to 99%), strong validity (up to 0.99), and realistic, behaviorally modifiable feature adjustments. When used for data augmentation under controlled label-scarcity settings, LLM-generated CFs substantially restore classifier performance, yielding an average 20% F1 recovery across three scarcity scenarios. Compared with optimization-based baselines such as DiCE, CFNOW, and NICE, LLMs offer a flexible, model-agnostic approach that generates more clinically actionable and semantically coherent counterfactuals. Overall, this work demonstrates the promise of LLM-driven counterfactuals for both interpretable intervention design and data-efficient model training in sensor-based digital health.   Impact: SenseCF fine-tunes an LLM to generate valid, representative counterfactual explanations and supplement minority class in an imbalanced dataset for improving model training and boosting model robustness and predictive performance", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.14590v1", "url_pdf": "https://arxiv.org/pdf/2601.14590.pdf", "meta_path": "data/raw/arxiv/meta/2601.14590.json", "sha256": "3fc313562c1c8fd9aeb80d48e06f5c964eb09a9ea102061121bb51913d47fa31", "status": "ok", "fetched_at": "2026-02-18T02:20:56.034515+00:00"}, "pages": [{"page": 1, "text": "Technology\nCounterfactual Modeling with Fine-Tuned LLMs for\nHealth Intervention Design and Sensor Data\nAugmentation\nShovito Barua Soumma1,2, Student Member, IEEE, Asiful Arefeen1,2, Stephanie M. Carpenter1, Melanie Hingle3,\nand Hassan Ghasemzadeh1, Senior Member, IEEE\nAbstract‚ÄîCounterfactual explanations (CFEs) provide human-\ncentric interpretability by identifying the minimal, actionable\nchanges required to alter a machine learning model‚Äôs prediction.\nTherefore, CFs can be used as (i) interventions for abnormality\nprevention and (ii) augmented data for training robust models. we\nconduct a comprehensive evaluation of CF generation using large\nlanguage models (LLMs), including GPT-4 (zero-shot and few-\nshot) and two open-source models‚ÄîBioMistral-7B and LLaMA-\n3.1-8B‚Äîin both pretrained and fine-tuned configurations. Using\nthe multimodal AI-READI clinical dataset, we assess CFs across\nthree dimensions: intervention quality, feature diversity, and aug-\nmentation effectiveness. Fine-tuned LLMs, particularly LLaMA-\n3.1-8B, produce CFs with high plausibility (up to 99%), strong\nvalidity (up to 0.99), and realistic, behaviorally modifiable feature\nadjustments. When used for data augmentation under controlled\nlabel-scarcity settings, LLM-generated CFs substantially restore\nclassifier performance, yielding an average\n20% F1 recovery\nacross three scarcity scenarios Compared with optimization-\nbased baselines such as DiCE, CFNOW, and NICE, LLMs\noffer a flexible, model-agnostic approach that generates more\nclinically actionable and semantically coherent counterfactuals.\nOverall, this work demonstrates the promise of LLM-driven\ncounterfactuals for both interpretable intervention design and\ndata-efficient model training in sensor-based digital health.\nIndex Terms‚ÄîCounterfactual explanations, Digital health, Ex-\nplainable AI, Label Scarcity, Large Language Model (LLM)\nImpact\nStatement-\nSenseCF\nfine-tunes\nan\nLLM\nto\ngenerate valid, representative counterfactual explanations\nand supplement minority class in an imbalanced dataset for\nimproving model training and boosting model robustness\nand predictive performance.\nI. INTRODUCTION\nA\nCCURATE and interpretable predictions from machine\nlearning (ML) models are increasingly vital in healthcare\n1College of Health Solutions, Arizona State University, Phoenix, AZ 85004,\nUSA. Emails: {shovito, aarefeen, stephanie.m.carpenter, hghasemz}@asu.edu.\n2School of Computing and Augmented Intelligence, Arizona State Univer-\nsity, Tempe, AZ 85281, USA.\n3School of Nutritional Sciences and Wellness, University of Arizona,\nTucson, AZ 85721, USA. Email: hinglem@arizona.edu.\nThis work was supported in part by the National Science Foundation (NSF)\nunder Grant IIS-2402650. A. Arefeen was supported in part by the National\nInstitute of Diabetes and Digestive and Kidney Diseases (NIDDK) of the\nNational Institutes of Health (NIH) under Award T32DK137525. The content\nis solely the responsibility of the authors and does not necessarily represent\nthe official views of the NSF or NIH.\napplications such as disease risk forecasting and sleep effi-\nciency estimation using physiological and sensor data. While\nthese models excel at outcome prediction, they often fall\nshort in guiding actionable interventions to reverse adverse\noutcomes- specially in black-box settings.\nCounterfactual explanations (CFEs) offer a powerful ap-\nproach for model interpretability by identifying the minimal,\nactionable changes needed to reverse an undesirable pre-\ndiction, a concept first formalized in the seminal work of\nWachter et al [1]. Subsequent surveys and methodological\nframeworks [2]‚Äì[4] have emphasized CFs‚Äô central role in\nactionable recourse, feasibility constraints, and user-centered\ninterpretability. Traditional optimization-driven CF methods\nsuch as DiCE [5], CFNOW [6], and NICE [7] often require\naccess to model gradients or internal structure, limiting their\nreal-world applicability and frequently struggling with cat-\negorical coherence or clinically plausible modifications. In\ncontrast, large language models (LLMs) provide a promising\nalternative: leveraging zero- and few-shot prompting, they can\ngenerate realistic, semantically consistent counterfactuals using\nonly input‚Äìoutput context [8], [9]. This paradigm not only\neliminates the dependency on gradients or model access but\nalso opens the door for scalable, human-centered explanations\nacross diverse datasets.\nRecent work highlights that LLMs possess strong innate\ncounterfactual reasoning abilities even without task-specific\nfine-tuning [9]‚Äì[12], building on earlier foundations show-\ning that counterfactual reasoning supports actionable model\ntransparency [1]‚Äì[4]. However, their application to structured\nand multimodal health data‚Äîwhere features arise from het-\nerogeneous physiological, behavioral, and environmental sen-\nsors‚Äîremains largely underexplored. Beyond interpretabil-\nity, counterfactual explanations also serve as label-flipping\nsynthetic samples that generate corner cases and enhance\nmodel robustness in imbalanced medical datasets, advanc-\ning prior work demonstrating the utility of counterfactuals\nfor recourse, robustness, and data augmentation [13]. When\nused for augmentation, CFs introduce controlled perturbations\nthat preserve the underlying data manifold while enriching\ndecision-boundary diversity, improving resilience in low-data\nclinical scenarios. Although optimization-based CF approaches\nhave shown effectiveness in decision-support settings [5],\n[6], they often struggle with categorical constraints, nonlin-\narXiv:2601.14590v1  [cs.LG]  21 Jan 2026\n"}, {"page": 2, "text": "Technology\near physiological dependencies, and plausible semantic ad-\njustments‚Äîlimitations that LLMs can mitigate through their\ncontextual reasoning and stronger distributional priors [7], [10].\n0\n10\n20\n30\n40\n50\n60\n70\nData Reduction (%)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nF1-Score\nSVC\nRandomForest\nXGBoost\nNeuralNetwork\nFig. 1. F1-Score Decline Across Models as Training Data is Reduced.\nHowever, several critical gaps persist in the current lit-\nerature: first, the effectiveness of LLM-based CFs have not\nbeen comprehensively evaluated on large, multimodal clinical\ndatasets; second, standardized evaluation metrics comparing\noptimization-based and generative approaches remain limited;\nthird, CFs‚Äô potential as data augmenters in healthcare scenarios\nremains underexplored. As illustrated in Fig. 1, all classifiers\nexperience marked declines in F1-score under increasing data\nreduction, highlighting the vulnerability of standard models to\nlabel scarcity and motivating the need for principled synthetic\naugmentation via LLM-generated counterfactuals.\nTo address these gaps, we introduce a systematic evalua-\ntion of LLM-generated counterfactuals using zero-shot, few-\nshot, pretrained, and fine-tuned models across the multimodal\nAI-READI clinical dataset. This paper extends our earlier\nwork, SenseCF [9], which focused primarily on GPT-4o based\nprompting, by incorporating a broader suite of open source\nLLMs and performing a more comprehensive assessment of\ntheir counterfactual capabilities. Our contributions extend be-\nyond existing LLM-focused studies that primarily evaluate nat-\nural language processing (NLP) tasks, providing a rigorous and\nquantitative comparison in multimodal clinical settings [10],\n[11]. In this work, (i) We systematically compare GPT-4o with\ntwo open-source LLMs (BioMistral-7B and LLaMA-3.1-8B)\nevaluated in both pretrained and fine-tuned configurations. We\nassess their performance across three core dimensions: (a) ac-\ntionable intervention quality, (b) feature diversity and realism,\nand (c) data augmentation effectiveness under controlled label-\nscarcity scenarios. (ii) We further benchmark their plausibility,\ndiversity, and impact on model performance against state-of-\nthe-art baselines.\nTo the best of our knowledge, this is the first study1 to fine-\ntune LLMs and explore them as counterfactual generators for\nboth actionable explanation and augmentation in sensor-driven\nhealth contexts, moving toward AI systems that can inform not\njust prediction, but intervention.\n1A preliminary version of this work has been reported [9].\nII. MATERIALS AND METHODS\nIn this section, we detail our approach for generating and\nevaluating counterfactual explanations (CFs) using large lan-\nguage models (LLMs). As illustrated in Fig 3 and Fig 2, our\nmethodology aims at: (1) producing actionable counterfactual\n(CF) interventions by reversing the predictions of trained ML\nmodels, and (2) leveraging these CFs as augmented training\ndata to enhance model robustness, specifically under label\nscarcity.\nA. Study Design\nWe represent our input data as a set of tuples (xi,yi), where\nxi ‚ààX is a feature vector representing either clinical or phys-\niological data and yi ‚àà{0,1} denotes the ground truth label.\nA trained predictive model f(¬∑) outputs predictions ÀÜyi = f(xi).\nOur preprocessing stage (discussed in Section II-C) transforms\nraw data into structured, tabular feature vectors suitable for\nprompting the LLM.\n1) Counterfactual Generation: To generate counterfactuals,\nwe used three LLM families under four generative modes:\n1) GPT-4o: Zero-shot (ZS) and few-shot (FS) prompting\n2) BioMistral-7B: Zero-shot and fine-tuned (FT) on the\nstructured training data\n3) LLaMA-3.1-8B: Zero-shot and fine-tuned on the struc-\ntured training data\nProblem Formulation: Given a feature vector xi and the\nprediction ÀÜyi, each LLM generates a modified vector x\n‚Ä≤\ni, where\nthe model‚Äôs prediction changes from ÀÜyi to a desired opposite\noutcome yi Ã∏= ÀÜyi. We also explicitly constrain the LLM from\naltering immutable or clinically fixed features (e.g., age, sex,\nor medication type), ensuring that generated counterfactuals\nremain actionable and plausible within domain constraints. The\ngeneration of CFs can be described as:\nx‚Ä≤\ni = LLM(xi,prompt),\ns.t. f(x‚Ä≤\ni) Ã∏= f(xi)\nThe instructional prompt explicitly constrains LLM to min-\nimally alter feature values to achieve a realistic and action-\nable counterfactual, ensuring the plausibility and feasibility of\ngenerated CFs. This multi-model setup enables us to compare\nthe semantic precision, boundary awareness, and distributional\nalignment of counterfactuals produced by proprietary vs. open-\nsource models and by zero-shot prompting vs. supervised fine-\ntuning.\n2) Intervention, Diversity and Data Augmentation: Gener-\nated CFs serve three purposes in our analysis:\n(i) Intervention: We assess whether CFs provide action-\nable and clinically meaningful adjustments to move a sample\nfrom an undesired label (e.g., stressed, disease-positive) to a\ndesirable outcome. Validity, sparsity, and plausibility metrics\nquantify intervention realism.\n(ii) Diversity: We also analyze how different LLMs modify\nfeature distributions, highlighting whether fine-tuned models\nproduce semantically grounded and structurally consistent CFs.\n(iii) Data Augmentation: CFs that successfully alter the\nmodel prediction are added to the training dataset to form an\naugmented set:\nXaug = X ‚à™{(x‚Ä≤\ni,y‚Ä≤\ni) | f(x‚Ä≤\ni) Ã∏= f(xi)}\n"}, {"page": 3, "text": "Technology\nBiased Risk \nClassifier\nRisk  Prediction\nImbalanced \ndataset\nFactual Samples\nCounterfactuals\nLLM\nFine \nTune\nRobust Classifier\nCounterfactual \nExplanation\nEarly Intervention\n1. Traditional Training\n2. CF Based Augmentation\nRobust Model Training\nFig. 2. SenseCF pipeline: LLM-generated counterfactuals are used both for augmenting imbalanced training data (left) and for model\ninterpretability (right).\nPreprocessing\nExtracted \nFeature\n(ùíô, ùíö)\nInstruction \nPrompt\n(ùíô!, ùíö!)\nCounterfactuals\n with  same or different \nlabel\nSame label\nùíö! = ùíö\nDifferent label\nùíö!! = ùíö\nData Augmentation & Intervention\nFig. 3. Counterfactual generation using LLMs from sensor-derived\nfeatures.\nYou are an expert in generating counterfactual explanations to understand model \npredictions.\nTask: Given a structured tabular instance where a trained black-box model has predicted \nlabel = 1 (undesired outcome), generate a counterfactual explanation by minimally \nmodifying the instance so that the predicted label flips to 0 (desired outcome).\nDefinition: A counterfactual explanation reveals what should have been different in an \ninstance to observe a different outcome.\nFixed Features: Do not modify these features: Age , Sex, Medication Type, [Add or remove \nbased on domain]\nHere are a few examples with desired outcome {outcome label}:\n---\nExample 1: Feature 1:{}, - Feature 2:{}\n---\nExample 2: Feature 1:{}, - Feature 2:{}\n---\nNow consider the following instance where the model predicted label {}:\n- Feature 1:{}\n- Feature 2:{}\n...\nInstruction: Modify the instance minimally to flip the label from {outcome label}  to {desired \nlabel}:. Only adjust features not listed under \\\"Fixed Features.\\\" Enclose your output within \n<new> tags as shown:\n<new>\n- Modified Feature 1: {}\n- Modified Feature 2: {}  \n</new>\nFig. 4. Prompt template for counterfactual generation.\nWe evaluate the impact of CF augmentation under three dif-\nferent label-scarcity settings (Positive-Class Scarcity, Negative-\nClass Scarcity, Dual-Class Scarcity) and across multiple aug-\nmentation ratios. This allows us to quantify how CF quantity\nand LLM fine-tuning quality improve downstream classifier\nperformance.\nB. Dataset\nAIREADI data:\nSenseCF utilizes the publicly available\nAIREADI Flagship Dataset [14], [15], a resource developed\nto support artificial intelligence and machine-learning research\nin Type 2 Diabetes Mellitus (T2DM). The dataset contains\ninformation from 1,067 participants recruited across three U.S.\nlocations: the University of Alabama at Birmingham (UAB),\nthe University of California San Diego (UCSD), and the\nUniversity of Washington (UW). AIREADI includes individ-\nuals both with and without T2DM, with careful balancing\nacross demographic groups and diabetes status. Participants\nfall into four categories: healthy controls, individuals with\nprediabetes, individuals with T2DM treated with oral agents,\nand individuals with T2DM using insulin.\nA major strength of the dataset is its rich multimodal\ncomposition. Each participant was monitored over a ten-day\nperiod using multiple wearable sensors: a Dexcom G6 con-\ntinuous glucose monitor for frequent glucose measurements,\na Garmin Vivosmart 5 for physical activity and heart-rate-\nvariability‚Äìderived stress indices, and an Anura environmental\nsensor capturing factors such as air quality and temperature.\nAdditional components include self-reported surveys, clinical\nexaminations, and retinal images. Daily step counts were\nobtained via an accelerometer, with occasional missing values\ndue to device charging.\nC. Preprocessing and Feature Extraction\nFigure 5 illustrates the overall preprocessing pipeline used\nin this study. To avoid data leakage across samples from the\nsame individual, we perform the train‚Äìtest split at the patient\nlevel, assigning 80% of patients to the training set and the\nremaining 20% to the test set. This ensures that no temporal\nwindows, physiological patterns, or behavioral characteristics\nfrom a given patient appear in both sets, thereby preserving\nthe integrity of model evaluation.\nFrom each night, the Awake, Light Sleep, Deep Sleep and\nREM Sleep percentages are computed. A two-hour long moving\n"}, {"page": 4, "text": "Technology\nTrain-Test Split\nTrain 80%\nTest 20%\nSleep \nActivity\nCGM\nDemogra\nphic\nWindowing\nOverlap Ratio [50%]\nProgression\nWindow Size [2Hr]\nExtracted Features\nFig. 5. Overview of the feature extraction pipeline. Patients are first\nsplit into train and test sets to avoid data leakage. A two-hour sliding\nwindow with 50% overlap is applied to continuous signals, followed\nby extraction of sleep, CGM, activity, and demographic features.\nwindow is used to extract mean step counts, mean glucose\nlevel, number of hyperglycemic events, percentage of time-\nin-range for glucose levels (%TIR). Based on high (mean\ndaily stress > 75) and moderate stress (mean daily stress 75)\nlevels\n[16], the target label is set. Therefore, a total of 12\nfeatures, including 4 immutable features: Age, Gender, Medi-\ncation and Patient Subgroup, are used to classify a sample for\nhigh/moderate stress and generate the CFEs for supplementing\nthe minority class.\nD. Experimental Setup\nAll experiments were conducted using a single NVIDIA\nA100 GPU (40 GB), which provides sufficient memory band-\nwidth and tensor-core acceleration for finetuning 7‚Äì8B pa-\nrameter large language models. We fine-tuned two models:\nLlama-3.1-8B-Instruct and BioMistral-7B, using parameter-\nefficient LoRA adaptation rather than full-model training to\nreduce computational cost and prevent catastrophic forgetting\nof medical knowledge. Both models were trained for 2 epochs,\nas we found this to provide the best trade-off between learning\nstability and overfitting; additional epochs tended to yield\ndiminishing returns on validation loss. We applied 4-bit NF4\nTABLE I. Training Configuration for LLM Finetuning\nComponent\nSetting\nGPU\nNVIDIA A100\nModels\nLlama-3.1-8B, BioMistral-7B\nFinetuning Strategy\nLoRA (PEFT)\nLoRA Rank (r)\n16\nLoRA Alpha\n32\nLoRA Dropout\n0.03\nQuantization\n4-bit NF4 (BitsAndBytes)\nEpochs\n2\nBatch Size\n4 (effective 16 with GA=4)\nLearning Rate\n2e-4\nMax Sequence Length\n512 tokens\nPrecision\nFP16\nEval Frequency\nEvery 50 steps\nCheckpoint Frequency\nEvery 100 steps\nquantization (BitsAndBytes) only to the Llama-3.1-8B model\nin order to substantially reduce its memory footprint on the\nA100 GPU. LLaMA models have larger attention projections\nand higher activation memory compared to BioMistral-7B,\nmaking them more memory-intensive during finetuning. Quan-\ntizing LLaMA to NF4 allows the model to fit comfortably\nin GPU memory, enables larger effective batch sizes, and\navoids out-of-memory (OOM) errors during training‚Äîwhile\npreserving model quality and maintaining stable optimization\nunder LoRA.\nE. Baselines\nWe have identified the following techniques to compare\nagainst SenseCF.\n(i) DiCE [5] generates a diverse set of CFs to maximize\nvariability across solutions while also optimizing for proximity,\nand feasibility across local regions of the decision boundary.\n(ii) CFNOW [6] is a model-agnostic method that employs\na two-step search algorithm to explore the search space and\ngenerate valid and minimal CFs.\n(iii) NICE [7] iteratively constructs CFs by replacing feature\nvalues with those from the nearest instance having a different\nprediction\nF. Evaluation Metrics\nWe assess the CFs using some standard metrics found in the\nliterature:\nValidity assesses whether the produced CFs genuinely be-\nlong to the desired class [17]. High validity indicates the\ntechnique‚Äôs effectiveness in generating valid CF examples.\nvalidity = #|f(X‚àó\nT) Ã∏= f(XT)|\n‚à•CF‚à•\nDistance between the CF and the factual sample is calculated\nfrom the L2 normalized distance of the continuous features and\nthe hamming distance of the categorical features [3].\nSparsity is the average number of feature changes per CF\n[18]. A low sparsity ensures better user understanding of the\nCFs.\nsparsity =\n‚àëX‚àó\nT ‚ààCF ‚àëd\ni=1 1(x‚àói\nT Ã∏= xi\nT)\n‚à•CF‚à•\n(1)\nPlausibility quantifies the fraction of explanations that fall\nwithin the feature ranges derived from the data [19]-\nplausibility =\n‚àëX‚àó\nT ‚ààCF 1(dist(X‚àó\nT) ‚äÜdist(X))\n‚à•CF‚à•\nwhere, dist(X‚àó\nT) and dist(X) represent the distribution of feature\nvalues in the CF instances X‚àó\nT and in the training data,\nrespectively. ‚à•CF‚à•is the total number of CF instances.\nIII. RESULTS\nOur evaluation highlights the dual role of LLM-generated\nCFs as highly plausible interventions and as impactful data\naugmenters for robust model training in digital health contexts.\nA. Intervention\nThe counterfactual intervention in Table II illustrates how\nLLMs can propose clinically meaningful and physiologically\ngrounded modifications for a high-stress patient. In this exam-\nple, the model identifies low deep sleep, moderate REM sleep,\nelevated glucose (210.8 mg/dL), and low activity as major\ncontributors to the stress prediction.\n"}, {"page": 5, "text": "Technology\nCondition\nAn 81-year-old patient labeled as ‚Äústressed‚Äù showed low deep sleep (30.1%), moderate REM (15.4%), high blood glucose (210.8 mg/dL),\nand limited activity (5.95 steps). Stress level was high (85.25), with poor glucose control (TIR: 12.5%, 1 hyper event).\nIntervention\nThe LLM suggests increasing deep sleep to ‚Üë35% and REM sleep to ‚Üë20%, which could help reduce physiological and emotional stress.\nIt also recommends lowering blood glucose from 210.8 to ‚Üì180 mg/dL, aligning with better metabolic control. These changes reflect\nclinically actionable strategies such as sleep hygiene improvement and tighter glucose management.\nTABLE II. Example of LLM-suggested counterfactual intervention for a high-stress patient\nMethod\nClass 0 Metrics\nClass 1 Metrics\nValidity\n‚Üë\nDistance\n‚Üì\nSparsity\n‚Üì\nPlausibility\n‚Üë\nValidity\n‚Üë\nDistance\n‚Üì\nSparsity\n‚Üì\nPlausibility\n‚Üë\nDiCE\n0.67\n0.2\n2.27\n100\n0.58\n0.41\n2.4\n99\nCFNOW\n0.85\n0.1\n2.9\n100\n0.84\n0.25\n3\n99\nNICE\n0.44\n0.02\n1.12\n33\n0.53\n0.04\n1.31\n35\nGPT-4 (ZS)\n0.91\n1.1\n3.6\n85\n0.89\n1.5\n3.8\n82\nGPT-4 (FS)\n0.99\n1.2\n4.4\n99\n0.92\n1.82\n4\n96\nBioMistral\n0.51\n1.4\n5.2\n77\n0.47\n1.5\n4.1\n70\nLlama\n0.62\n1.6\n4.6\n91\n0.68\n1.3\n3.8\n78\nBioMistral*\n0.93\n0.92\n2.27\n90\n0.91\n1.0\n2.1\n95\nLlama*\n0.99\n0.41\n1.8\n99\n0.98\n0.2\n1.9\n99\nTABLE III. Evaluating CFs on AI-READI Dataset for Class 0 and Class 1. ZS: Zero-shot, FS: Few-shot, *: Fine-tuned, Red: best, Blue:\nsecond-best.\nTable III provides a comparison of counterfactual quality\nacross baseline optimization methods, GPT-4, and open-source\nLLMs. While traditional methods such as DiCE, CFNOW,\nand NICE achieve strong plausibility and occasionally lower\ndistances, they often propose unrealistic or non-actionable\nfeature shifts. GPT-4 performs reliably in both zero-shot and\nfew-shot modes, but its counterfactuals still exhibit larger\nfeature deviations than desired for sensor-derived tabular data.\nFine-tuned BioMistral-7B and LLaMA-3.1-8B substantially\nimprove validity, sparsity, and distance relative to their pre-\ntrained versions‚Äîshowing gains of 20‚Äì40% points in validity\nand reductions of more than 50% in feature distance. Although\nfine-tuned LLMs do not always outperform every state-of-the-\nart baseline across all metrics, they remain highly competitive\noverall. Importantly, fine-tuned LLaMA provides the strongest\nbalance across metrics, achieving near-perfect validity with\nminimal, clinically realistic modifications.\nUnlike optimization-based methods (e.g., DICE, CFNOW),\nwhich rely on access to model internals, our LLM-based\napproach generates CFs in a model-agnostic fashion while\nremaining interpretable and actionable.\nFeature Diversity: The radar plot (Fig 6) highlights im-\nportant differences in actionability and realism across coun-\nterfactual generation methods. Traditional CF methods such\nas DICE and CFNOW often propose large shifts in physi-\nological features- specially sleep-stage percentages and total\nsleep duration-that are not immediately modifiable in real-\nlife settings and therefore have lower practical utility. NICE\nperforms better by recommending smaller, more localized\nchanges, but it still occasionally alters features like REM or\ndeep sleep that individuals cannot directly control in the short\nDeep \nSleep (%)\nTIR (%)\nHyper Count\nFig. 6. Feature diversity in the generated CFs for AI-Readi data. Avg:\nAverage, Hyper: No. of hyperglycemia\nterm. In contrast, the fine-tuned BioMistral and Llama models\nproduce counterfactuals concentrated around highly actionable\nvariables, such as average steps, glucose levels (Avg CGM,\nTIR%), and hyperglycemia frequency‚Äîfactors that can be\nmodified through short-term behavioral or treatment adjust-\nments. This shift toward modifiable lifestyle and metabolic\nvariables makes the LLM-based counterfactuals more clinically\nrealistic and better aligned with interventions that individuals\n"}, {"page": 6, "text": "Technology\nTABLE IV. Performance Impact of LLM-Generated Counterfactuals Under Three Label-Scarcity Scenarios. Baseline performance is measured\non the full training set. Negative values indicate the performance drop after undersampling. CF-added rows report the improvement relative\nto the reduced dataset.\nScenario\nMethod\nACC\nPRE\nREC\nF1\nAUC\nBaseline (Using Full Training Set)\nNN\n71.8\n0.68\n0.68\n0.68\n0.78\nScenario A ‚Äî Positive-Class Undersampling (Class 1 Reduced by 50%)\nModel trained on reduced data\nNN\n-12.87%‚Üì\n-11.76%‚Üì\n-16.18%‚Üì\n-14.71%‚Üì\n-14.10%‚Üì\nCF Added (Recovery over Reduced)\nBioMistral\n7.10%‚Üë\n5.00%‚Üë\n8.77%‚Üë\n6.90%‚Üë\n7.46%‚Üë\nLLaMA\n8.54%‚Üë\n6.67%‚Üë\n10.53%‚Üë\n8.62%‚Üë\n7.46%‚Üë\nGPT-4\n10.29%‚Üë\n10.00%‚Üë\n12.28%‚Üë\n12.07%‚Üë\n11.94%‚Üë\nBioMistral*\n14.93%‚Üë\n18.33%‚Üë\n22.81%‚Üë\n20.69%‚Üë\n20.90%‚Üë\nLLaMA*\n21.00%‚Üë\n20.00%‚Üë\n24.56%‚Üë\n22.41%‚Üë\n25.37%‚Üë\nScenario B ‚Äî Negative-Class Undersampling (Class 0 Reduced by 50%)\nModel trained on reduced data\nNN\n-10.72%‚Üì\n-8.82%‚Üì\n-10.29%‚Üì\n-10.29%‚Üì\n-12.82%‚Üì\nCF Added (Recovery over Reduced)\nBioMistral\n4.52%‚Üë\n1.61%‚Üë\n1.64%‚Üë\n1.64%‚Üë\n4.41%‚Üë\nLLaMA\n5.93%‚Üë\n3.23%‚Üë\n3.28%‚Üë\n3.28%‚Üë\n5.88%‚Üë\nGPT-4\n6.86%‚Üë\n4.84%‚Üë\n4.92%‚Üë\n4.92%‚Üë\n8.82%‚Üë\nBioMistral*\n14.82%‚Üë\n14.52%‚Üë\n13.11%‚Üë\n14.75%‚Üë\n20.59%‚Üë\nLLaMA*\n17.16%‚Üë\n16.13%‚Üë\n16.39%‚Üë\n16.39%‚Üë\n23.53%‚Üë\nScenario C ‚Äî Dual-Class Undersampling (Both Classes Reduced by 50%)\nModel trained on reduced data\nNN\n-11.14%‚Üì\n-10.29%‚Üì\n-13.24%‚Üì\n-11.76%‚Üì\n-14.10%‚Üì\nCF Added (Recovery over Reduced)\nBioMistral\n4.55%‚Üë\n1.64%‚Üë\n3.39%‚Üë\n1.67%‚Üë\n4.48%‚Üë\nLLaMA\n5.90%‚Üë\n4.92%‚Üë\n6.78%‚Üë\n5.00%‚Üë\n7.46%‚Üë\nGPT-4\n6.27%‚Üë\n4.92%‚Üë\n5.08%‚Üë\n5.00%‚Üë\n8.96%‚Üë\nBioMistral*\n15.99%‚Üë\n16.39%‚Üë\n16.95%‚Üë\n16.67%‚Üë\n22.39%‚Üë\nLLaMA*\n21.47%‚Üë\n19.67%‚Üë\n22.03%‚Üë\n20.00%‚Üë\n26.87%‚Üë\ncan adopt immediately. Overall, the Llama model generates the\nmost actionable and plausible counterfactual recommendations,\navoiding unrealistic alterations to intrinsic sleep architecture\nwhile still achieving high validity in flipping the prediction.\nB. Augmentation\n1) Performance Recovery: Table IV summarizes the impact\nof LLM-generated counterfactual samples on classifier perfor-\nmance under three controlled label-scarcity settings. We first\ntrained a neural network on the full training set to establish\na baseline. Next, we constructed three undersampling scenar-\nios: (1) Positive-Class Scarcity (Class 1 undersampled), (2)\nNegative-Class Scarcity (Class 0 undersampled), and (3) Dual-\nClass Scarcity (both classes undersampled). In each case, re-\nmoving 50% of the samples from the targeted class resulted in\na substantial performance drop across all metrics, as reflected\nby negative deltas relative to the baseline neural network.\nTo recover the lost information, we generated counterfactual\nsamples using several LLMs (BioMistral-7B, LLaMA-3.1-\n8B, and GPT-4o) in both zero-shot and fine-tuned configu-\nrations, and added only the counterfactuals corresponding to\nthe class that was undersampled. The ‚ÄúCF Added‚Äù rows report\nthe performance recovery relative to the reduced-data model,\nshowing that fine-tuned LLMs consistently provide the largest\nimprovement, with fine-tuned LLaMA delivering the strongest\ngains across all three scarcity scenarios.\nLLaMA-3.1-8B (FT) restores more than 22% of F1 in\nthe positive-class scarcity setting and roughly 20% under\ndual-class scarcity‚Äîfully reversing the degradation introduced\nby undersampling. Even in the more challenging negative-\nclass scarcity case, fine-tuned LLMs recover 16‚Äì17% of F1,\nclearly outperforming their zero-shot counterparts. These large\npercentage-point gains highlight the strong corrective effect of\nCF-based augmentation when guided by fine-tuned LLMs.\n2) Effect of Augmentation Ratio: Figure 7 further explores\nhow the amount of counterfactual augmentation influences\nperformance by varying the augmentation ratio from 20% to\n100%. Each subplot begins with the reduced-data model in Sce-\nnario A, B, or C, where the neural network exhibits markedly\ndegraded performance due to undersampling; this is shown\nas the horizontal dashed baseline in Figure 7 (A-C). Starting\nfrom these weakened models, we incrementally add different\nquantities of LLM-generated counterfactuals and observe how\nperformance recovers as augmentation increases. Across all\nthree scarcity scenarios, the F1 score improves steadily as\nmore counterfactuals are injected into the training set. The\neffect is most pronounced for the fine-tuned LLaMA-3.1-8B\nand BioMistral-7B models, which show clear monotonic gains\nand achieve the highest F1 scores at large augmentation ratios.\nZero-shot LLMs (dashed lines) still provide measurable\nimprovement over the undersampled baseline, but their gains\nplateau earlier, suggesting limited ability to mimic the true\nclass manifold without task-specific adaptation. Another con-\nsistent observation is that Positive-Class Scarcity and Dual-\nClass Scarcity benefit more strongly from CF augmentation\nthan Negative-Class Scarcity, indicating that the model is par-\n"}, {"page": 7, "text": "Technology\n(a) Positive-Class Scarcity (Class 1 Undersampled)\n(b) Negative-Class Scarcity (Class 0 Undersampled)\n(c) Dual-Class Scarcity \nFig. 7. Impact of LLM-Generated Counterfactual Augmentation Under Class-Specific Label Scarcity\nClass 0:  \nClass 1: \nClass 0 CF:  \nClass 1 CF: \n(a) Without Finetuning\nDimension-1\nDimension-2\n(b) After Finetuning\nClass 0:  \nClass 1: \nClass 0 CF:  \nClass 1 CF: \nDimension-2\nDimension-1\nFig. 8. Latent Space Distribution of Factual and Counterfactual\nSamples Generated by LLaMA-3.1-8B on the AI-READI Dataset\n(Before vs. After Fine-Tuning). The amount of overlap between two\nclasses is much lesser in the dataset augmented using fine-Tuned\nLLM.\nticularly sensitive to missing minority-class examples. Overall,\nthe F1 curves demonstrate that counterfactuals function as an\neffective and controllable data augmenter, with augmentation\nmagnitude and LLM fine-tuning both playing significant roles\nin performance recovery.\nC. Ablation Study\nFine-tuning LLaMA-3.1-8B leads to markedly improved\ncounterfactual behavior, as shown in Figure 8. Using a ran-\ndomly selected subset of 200 factual samples, the zero-shot\nmodel generates diffused and often misaligned CFs; the fine-\ntuned model produces CFs that cluster tightly within the ap-\npropriate class region and maintain local structural coherence.\nThis improved spatial organization highlights the effectiveness\nof finetuning in producing plausible, semantically meaningful,\nand distribution-consistent counterfactuals.\nIV. DISCUSSION\nThis study presents the first systematic evaluation of LLM-\ngenerated counterfactuals for intervention design and data aug-\nmentation in sensor-derived health datasets. We evaluate multi-\nple LLM families‚ÄîGPT-4o, BioMistral-7B, and LLaMA-3.1-\n8B‚Äîin both pretrained and fine-tuned configurations, demon-\nstrating that fine-tuned LLaMA achieves an average 19.9%\nF1 recovery across three scarcity conditions when 50% of\nlabels are removed. This represents a substantial improvement\nin classifier robustness under label scarcity while providing\ninterpretable, domain-aligned explanations that traditional CF\nmethods cannot offer.\nOur comparative analysis reveals a critical distinction be-\ntween LLM-based and optimization-based approaches: while\ntraditional methods (DiCE, CFNOW, NICE) frequently modify\nclinically intractable features such as sleep architecture, fine-\ntuned LLMs prioritize actionable variables‚Äîaverage steps,\nglucose metrics (Avg CGM, TIR%), and hyperglycemia fre-\nquency‚Äîthat reflect realistic behavioral interventions. Latent-\nspace visualizations confirm that fine-tuned LLaMA produces\nsemantically coherent CFs aligned with the target class mani-\nfold, addressing a key limitation of existing methods.\nA. Clinical Implications\nThese findings highlight the potential for LLM-generated\ncounterfactuals to support personalized health guidance. By\nsuggesting small, clinically plausible adjustments grounded\nin sensor-derived physiology, CFs can help identify early\nbehavioral or metabolic interventions for individuals at risk.\nMoreover, CF-based augmentation offers a scalable alternative\nto collecting more sensor data, helping improve classifier\nrobustness in health settings where labeled data are scarce.\nB. Dataset & Limitation\nWe selected the AI-READI dataset because publicly avail-\nable clinical datasets suitable for LLM fine-tuning on structured\ntabular health features are extremely limited. Most open-\nsource biomedical datasets are too small, lack multimodal\ncoverage, or contain insufficient feature richness for effective\n"}, {"page": 8, "text": "Technology\nLLM alignment. AI-READI, by contrast, offers a large, well-\ncurated combination of CGM, activity, sleep, demographic, and\nstress features‚Äîmaking it one of the few datasets capable of\nsupporting robust LLM fine-tuning and CF evaluation.\nDespite strong overall performance, fine-tuned LLMs do\nnot outperform all state-of-the-art CF baselines across every\nmetric, particularly in distance or sparsity for certain classes.\nSome CFs may still exceed clinically achievable ranges, and\nfine-tuning requires labeled data that may be scarce for some\nconditions. Finally, our work is limited to structured tabular\nfeatures and does not yet incorporate multimodal raw sensor\nstreams or clinical text.\nC. Future Directions\nSeveral promising avenues remain for improving LLM-based\ncounterfactual reasoning. First, integrating LLMs directly into\nthe machine learning training loop could enable iterative, CF-\nguided model updates (‚ÄúLLM-in-the-loop learning‚Äù). Second,\nincorporating clinical knowledge graphs or causal structures\ninto the fine-tuning pipeline may reduce unrealistic or physio-\nlogically implausible feature changes. Third, expanding beyond\ntabular data to multimodal LLMs could enable CF generation\nfrom raw sensor traces, free-text clinical notes, or imaging.\nLastly, deploying CF-driven augmentation in real-world digital\nhealth ecosystems may help assess the longer-term impact of\nCF-based guidance on early intervention and patient outcomes.\nV. CONCLUSION\nIn this work, we introduce a novel framework for generating\nCFs using large language models (LLMs) and extend it to\nevaluate both proprietary and open-source models across mul-\ntiple generative settings. Our results show that LLM-generated\ncounterfactuals are semantically coherent, clinically plausible,\nand capable of improving downstream robustness when used\nfor data augmentation‚Äîrestoring, on average, 20% F1 under\nsevere label scarcity. Fine-tuned LLaMA and BioMistral mod-\nels, in particular, produce compact and actionable CFs that out-\nperform their pretrained counterparts and remain competitive\nwith state-of-the-art optimization methods.\nTo the best of our knowledge, this is the first systematic\nexploration of LLM-based CFs in sensor-driven data under\nboth zero- and few-shot settings. We believe this opens a\npromising direction for integrating generative AI into trust-\nworthy, intervention-oriented healthcare ML pipelines.\nREFERENCES\n[1] S. Wachter, B. D. Mittelstadt, and C. Russell, ‚ÄúCounterfactual explana-\ntions without opening the black box: Automated decisions and the gdpr,‚Äù\nCybersecurity, 2017.\n[2] R. Guidotti, A. Monreale, F. Turini, D. Pedreschi, and F. Giannotti, ‚ÄúA\nsurvey of methods for explaining black box models,‚Äù ACM Computing\nSurveys (CSUR), vol. 51, pp. 1 ‚Äì 42, 2018.\n[3] A.-H. Karimi, B. Scholkopf, and I. Valera, ‚ÄúAlgorithmic recourse: from\ncounterfactual explanations to interventions,‚Äù Proceedings of the 2021\nACM Conference on Fairness, Accountability, and Transparency, 2020.\n[4] A.-H. Karimi, G. Barthe, B. Balle, and I. Valera, ‚ÄúModel-agnostic\ncounterfactual explanations for consequential decisions,‚Äù ArXiv, vol.\nabs/1905.11190, 2019.\n[5] R. K. Mothilal, A. Sharma, and C. Tan, ‚ÄúExplaining machine learning\nclassifiers through diverse counterfactual explanations,‚Äù in Proceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency,\n2020, pp. 607‚Äì617.\n[6] R. M. B. de Oliveira, K. S¬®orensen, and D. Martens, ‚ÄúA model-agnostic\nand data-independent tabu search algorithm to generate counterfactuals\nfor tabular, image, and text data,‚Äù European Journal of Operational\nResearch, 2023.\n[7] D. Brughmans and D. Martens, ‚ÄúNice: an algorithm for nearest instance\ncounterfactual explanations,‚Äù Data Mining and Knowledge Discovery, pp.\n1‚Äì39, 2021.\n[8] B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal et al., ‚ÄúLanguage models are\nfew-shot learners,‚Äù arXiv preprint arXiv:2005.14165, vol. 1, p. 3, 2020.\n[9] S. B. Soumma, A. Arefeen, S. M. Carpenter, M. Hingle, and\nH.\nGhasemzadeh,\n‚ÄúSenseCF:\nLLM-prompted\ncounterfactuals\nfor\nintervention and sensor data augmentation,‚Äù in IEEE-EMBS International\nConference on Body Sensor Networks 2025, 2025. [Online]. Available:\nhttps://openreview.net/forum?id=8qqMeF9EmT\n[10] A. Bhattacharjee, R. Moraffah, J. Garland, and H. Liu, ‚Äú Zero-\nshot LLM-guided Counterfactual Generation: A Case Study on NLP\nModel Evaluation ,‚Äù in 2024 IEEE International Conference on\nBig Data (BigData).\nLos Alamitos, CA, USA: IEEE Computer\nSociety,\nDec.\n2024,\npp.\n1243‚Äì1248.\n[Online].\nAvailable:\nhttps:\n//doi.ieeecomputersociety.org/10.1109/BigData62323.2024.10825537\n[11] Y.\nLi,\nM.\nXu,\nX.\nMiao,\nS.\nZhou,\nand\nT.\nQian,\n‚ÄúPrompting\nlarge language models for counterfactual generation: An empirical\nstudy,‚Äù in Proceedings of the 2024 Joint International Conference\non Computational Linguistics, Language Resources and Evaluation\n(LREC-COLING 2024), N. Calzolari, M.-Y. Kan, V. Hoste, A. Lenci,\nS. Sakti, and N. Xue, Eds.\nTorino, Italia: ELRA and ICCL, May\n2024, pp. 13 201‚Äì13 221. [Online]. Available: https://aclanthology.org/\n2024.lrec-main.1156/\n[12] Y. Chen, V. K. Singh, J. Ma, and R. Tang, ‚ÄúCounterbench: A benchmark\nfor counterfactuals reasoning in large language models,‚Äù ArXiv, vol.\nabs/2502.11008, 2025.\n[13] C. Russell, ‚ÄúEfficient search for diverse coherent explanations,‚Äù Proceed-\nings of the Conference on Fairness, Accountability, and Transparency,\n2019.\n[14] P.\nM.\nhttp://orcid.\norg/0000-0001-6343-2140\nDrolet\nCaroline\n4\nhttp://orcid. org/0000-0003-2287-4190 Lucero Abigail 8 Matthies Dawn\n7 http://orcid. org/0009 0003-4909-6058 Pittock Hanna 3 Watkins Kate\n3 York Brittany 1 and N. P. S. W. X. 11, ‚ÄúAi-readi: rethinking ai data\ncollection, preparation and sharing in diabetes research and beyond,‚Äù\nNature metabolism, vol. 6, no. 12, pp. 2210‚Äì2212, 2024.\n[15] S. L. Baxter, V. R. de Sa, K. S. Ferryman, P. Jain, C. S. Lee, J. Li-\nPook-Than, T. Y. A. Liu, J. P. Owen, B. Patel, Q. Yu, L. M. Zangwill,\nA. Bahmani, C. G. Chute, J. C. Edberg, S. Hurst, H. Ishikawa, A. Y.\nLee, G. McGwin, S. K. McWeeney, C. Nebeker, C. Owsley, S. J. Singer,\nR. Adib, M. Adibuzzaman, A. Alavi, C. Ashley, A. Baer, E. Benton,\nM. Blazes, A. Cohen, B. A. Cordier, K. Crist, C. Cuddy, A. Gasimova,\nN. Gim, S. S. Hong, T. Kim, W.-C. Lin, J. Mitchell, C. Ngadisastra,\nV. Patronilo, J. Shaffer, S. Soundarajan, K. Zhao, C. Drolet, A. Lucero,\nD. S. Matthies, H. Pittock, K. Watkins, B. York, C. E. Amankwa,\nM. Bangudi, N. Haboudal, S. Hallaj, A. Heinke, L. Huang, F. G. P.\nKalaw, A. Karsolia, H. Khazaei, M. Mohammed, K. U. Simpkins, and\nX. Wang, ‚ÄúAi-readi: rethinking ai data collection, preparation and sharing\nin diabetes research and beyond,‚Äù Nature Metabolism, vol. 6, pp. 2210\n‚Äì 2212, 2024.\n[16] Garmin,\n‚ÄúWhat\ndo\nthe\nstress\nlevel\nnumbers\nmean?‚Äù\nhttps:\n//support.garmin.com/en-US/?faq=WT9BmhjacO4ZpxbCc0EKn9,\naccessed: 2025-12-02.\n[17] F. Hamman, E. Noorani, S. Mishra, D. Magazzeni, and S. Dutta, ‚ÄúRo-\nbust counterfactual explanations for neural networks with probabilistic\nguarantees,‚Äù in International Conference on Machine Learning, 2023.\n[18] H. Guo, T. H. Nguyen, and A. Yadav, ‚ÄúCounternet: End-to-end training\nof prediction aware counterfactual explanations,‚Äù Proceedings of the 29th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining,\n2021.\n[19] R. Guidotti, ‚ÄúCounterfactual explanations and how to find them: literature\nreview and benchmarking,‚Äù Data Mining and Knowledge Discovery,\nvol. 38, pp. 2770 ‚Äì 2824, 2022.\n"}]}