{"doc_id": "arxiv:2602.15677", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.15677.pdf", "meta": {"doc_id": "arxiv:2602.15677", "source": "arxiv", "arxiv_id": "2602.15677", "title": "CAMEL: An ECG Language Model for Forecasting Cardiac Events", "authors": ["Neelay Velingker", "Alaia Solko-Breslin", "Mayank Keoliya", "Seewon Choi", "Jiayi Xin", "Anika Marathe", "Alireza Oraii", "Rajat Deo", "Sameed Khatana", "Rajeev Alur", "Mayur Naik", "Eric Wong"], "published": "2026-02-17T16:02:52Z", "updated": "2026-02-17T16:02:52Z", "summary": "Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.15677v1", "url_pdf": "https://arxiv.org/pdf/2602.15677.pdf", "meta_path": "data/raw/arxiv/meta/2602.15677.json", "sha256": "d03c65e499cf809f2a18f799c71f000509e5ee88bf71e865af7a7961a3836516", "status": "ok", "fetched_at": "2026-02-18T02:19:09.043612+00:00"}, "pages": [{"page": 1, "text": "CAMEL: An ECG Language Model\nfor Forecasting Cardiac Events\nNeelay Velingker∗, Alaia Solko-Breslin∗, Mayank Keoliya∗, Seewon Choi∗\nJiayi Xin, Anika Marathe, Alireza Oraii,\nRajat Deo†, Sameed Khatana†, Rajeev Alur†, Mayur Naik†, Eric Wong†\nUniversity of Pennsylvania\n{neelay,alaia,mkeoliya,seewon}@seas.upenn.edu\nAbstract\nElectrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing\ncardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising\nframework for ECG classification accompanied by report generation. However, current models\ncannot forecast future cardiac events despite the immense clinical value for planning earlier\nintervention. To address this gap, we propose CAMEL, the first ELM that is capable of\ninference over longer signal durations which enables its forecasting capability. Our key insight\nis a specialized ECG encoder which enables cross-understanding of ECG signals with text.\nWe train CAMEL using established LLM training procedures, combining LoRA adaptation\nwith a curriculum learning pipeline. Our curriculum includes ECG classification, metrics\ncalculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong\nzero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new\nbenchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses\nELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on\nECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully\nsupervised models and +21.1% over zero-shot ELMs). 1\n1\nIntroduction\nElectrocardiograms (ECG) are multi-dimensional recordings of the heart’s electrical activity and serve as a primary\ntool for diagnosing and triaging conditions such as heart attacks, arrhythmias, and other cardiac abnormalities\n(Kaplan Berkaya et al., 2018; Savonitto et al., 1999). From symbolic algorithms and statistical models to CNNs,\nautomated ECG classification has moved from extensive academic study to widespread deployment in both\nambulatory and in-hospital environments, as seen in systems like GE’s Marquette 12SL (GE Healthcare, 2019).\nMore recently, foundation models have emerged for jointly processing ECG and text, which we call ECG Language\nModels (ELMs). ELMs combine ECG representation learning with natural language generation to produce\ninterpretable classifications and reports (Liu et al., 2024b,a; Wang et al., 2025; Lan et al., 2025). Despite their\npromise, existing ELMs only target classification and do not predict a patient’s future state, thus offering limited\nsupport for early intervention.\nForecasting cardiac events from ECG signals is a key challenge for AI in cardiac care. Unlike classification,\nforecasting requires detecting subtle, prognostic patterns in ECGs to anticipate future adverse events. Such early\nwarning of cardiac events, such as ventricular tachycardia, could allow clinicians to intervene to improve patient\noutcomes (Pollack et al., 2016; Soar et al., 2021). While classical ML models and CNNs have been applied to\nthis task (Kenet et al., 2023; Rooney et al., 2023), they rely on fully supervised training for fixed-length inputs\nand offer interpretability only through post-hoc explanations, limiting their ability to generalize across tasks and\n∗Equal contribution and shared first authorship\n†Equal contribution and shared senior authorship\n1Code and datasets are available at github.com/CAMEL-ECG/CAMEL-inference and huggingface.co/CAMEL-ECG.\narXiv:2602.15677v1  [cs.LG]  17 Feb 2026\n"}, {"page": 2, "text": "Figure 1: Example of CAMEL’s forecasting capability. In the top example, CAMEL takes as input normal sinus\nrhythm ECG at time T and correctly forecasts AFIB at T + 3 minutes by reasoning over the RMSSD, RR-interval,\nand PAC count (reasoning highlighted). In the bottom example, CAMEL correctly predicts a normal outcome\nbased on accurately extracted statistics.\nclinical contexts. In contrast, ELMs contain an LLM backbone trained on clinical knowledge, allowing them to\ngeneralize across tasks and generate natural language explanations along with their predictions.\nTo meet this challenge, we propose CAMEL (Cardiac Autoregressive Model for ECG Language-Modeling), the\nfirst general-purpose ELM designed to support long temporal context windows of ECG signals. While existing\nbenchmarks are largely restricted to the classification of 10-second snippets, we introduce ECGForecastBench,\na new benchmark for predicting future arrhythmias from baseline normal sinus rhythms as input. Our model\ngenerates forecasting reports by leveraging ECG signal statistics with established clinical associations (Zhang et al.,\n2025). These statistics provide physiologically grounded explanations for the risk of a future cardiac event.\nLike prior ELMs, CAMEL builds on a pre-trained large language model backbone, namely MedGemma-4B\n(Sellergren et al., 2025), to support reasoning and natural language generation. The core insight that allows\nCAMEL to reason over long temporal contexts of ECG signals is how the integration of signal embeddings with\ntext embeddings operates at the token level. By encoding each one-second segment of each lead in an ECG as an\nindividual token, CAMEL can interleave multiple signal sequences of any duration with textual prompts. This\ndesign supports flexibility in both input length and lead configuration, enabling CAMEL to reason over long ECG\ncontexts and variable, potentially incomplete sets of leads common in real-world settings. This is in contrast to\nprior ELMs, whose contexts are generally restricted to 10-second, 12-lead ECGs (Table 1).\nTo train CAMEL, we introduce a 5-stage curriculum that gradually builds the model’s reasoning and forecasting\ncapabilities. Training starts with an autoencoder stage to learn robust ECG representations. Subsequent stages\nteach the model multiple-choice and short-answer tasks, understanding of ECG statistics, multi-turn conversational\nreasoning, and finally the generation of forecasting reports. This curriculum enables CAMEL to compute ECG\nstatistics from long contexts and use them as evidence for clinically grounded forecasts (Fig. 1).\nIn summary, the main contributions of this paper are as follows. First, we introduce the architecture for CAMEL,\nwhich enables its unique capability to reason over long-duration ECG signals and identify predictive markers of\nfuture events. Next, we present a large-scale data generation pipeline that supports curriculum learning for ECG\ncomprehension, including the development of a novel benchmark, ECGForecastBench, for evaluating forecasting\nof future adverse cardiac events. We then present a staged training algorithm that progressively builds CAMEL’s\nECG grounding, reasoning, and forecasting capabilities. Finally, we demonstrate that CAMEL achieves strong\nzero-shot performance, matching or surpassing fully supervised in- and out-of-distribution baselines.\n2\nCAMEL Architecture\nIn this section, we describe CAMEL’s architecture and detail the characteristics that enable its unique capabilities.\nAs Figure 2, illustrates, CAMEL consists of two main components: 1) a trainable tokenizing encoder that\nmaps ECG segments into vector representations, and 2) a LoRA-adapted LLM that processes the aligned ECG\n2\n"}, {"page": 3, "text": "Table 1: Comparison of ECG foundation models across capabilities. The duration field indicates the ECG lengths\nsupported during training and evaluation for each model. CAMEL is the first ELM to support forecasting, multi-\nECG inputs, and long-context signals.\nTasks\nInputs\nModel\nInput\nClassification\nReport\nGeneration\nForecasting\nModality\n# ECGs\nDuration (s)\nECGFounder (2025b)\n✓\n✗\n✗\nwaveform\nsingle\n10\nECG-FM (2025)\n✓\n✗\n✗\nwaveform\nsingle\n10\nMERL (2024a)\n✓\n✗\n✗\nwaveform\nsingle\n10\nECG-Byte (2025)\n✓\n✗\n✗\nwaveform\nsingle\n10\nMELP (2025)\n✓\n✓\n✗\nwaveform\nsingle\n10\nPULSE (2024b)\n✓\n✓\n✗\nimage, text\nmulti\n10\nGEM (2025)\n✓\n✓\n✗\nimage, waveform, text\nmulti\n10\nCAMEL (Ours)\n✓\n✓\n✓\nwaveform, text\nmulti\n0–10K\nembeddings along with text. Specifically, CAMEL is built upon MedGemma-4B (Sellergren et al., 2025), leveraging\nits medical language understanding. Together, these jointly trained components produce an end-to-end late-fusion\npipeline for ECG-centric multi-turn conversation.\nECG encoder.\nWe consider a single-lead ECG signal sampled at N Hz. A 1-second ECG segment is represented\nas xecg ∈RN. To encode the raw waveform, we use a 3-layer CNN fθ : RN →Rd which maps each 1-second\nsegment into a latent representation:\nzecg ∈Rd = fθ(xecg).\nIn our implementation, we use d = 64, and all signals are resampled to the same frequency N = 256. This design\nyields a compact, continuous representation of ECG segments, preserving clinically meaningful features while\nrequiring relatively few tokens to represent long signals.\nTo integrate ECG representations with the LLM, ECG embeddings must align with the LLM’s hidden token space\nwith dimension h. Therefore, we apply a linear projection layer to the ECG embeddings, implemented as a single\nlinear layer SLPϕ : Rd →Rh\neecg ∈Rh = SLPϕ(zecg).\nIntegrating ECG and text.\nGiven an ECG signal of T seconds with L leads, we encode each lead ℓ∈{1, . . . , L}\nas a sequence of 1-second embeddings:\nH(ℓ)\necg =\n\u0002\ne(ℓ,1)\necg , . . . , e(ℓ,T )\necg\n\u0003\n∈RT ×h.\nTo explicitly encode structure, we introduce lead start and end special tokens for each input lead, between which\nthe encoded segments are placed:\n˜H\n(ℓ)\necg = Concat\n\u0010\ne⟨leadℓ⟩, H(ℓ)\necg, e⟨/leadℓ⟩\n\u0011\n.\nWith this, we are ready to integrate the ECG with text. Given input text xtxt, MedGemma’s tokenizer maps\neach text token to an embedding in the same hidden space. Without loss of generality, CAMEL combines these\nembeddings with ECG start and end special tokens to form a unified sequence suitable for LLM input,\nH = Concat\n\u0010\nEmbed(xtxt), e⟨ecg⟩,\nL\nM\nℓ=1\n˜H\n(ℓ)\necg, e⟨/ecg⟩\n\u0011\n,\nwhere L denotes concatenation. Note that text may also be placed between ECGs or at the end of the sequence.\nFurthermore, lead-specific special tokens may appear multiple times, allowing users to provide more than one ECG\nin a conversation, which can be placed anywhere in a user turn. This design allows for comparative reasoning\nacross ECGs, supporting richer ECG comprehension.\nLead-aware attention masking. Unlike text, multi-lead ECG signals are not strictly causal when flattened.\nSpecifically, all leads at a given time t measure simultaneous projections of the same underlying cardiac electrical\nactivity. To leverage this insight, and enable cross-lead learning, CAMEL uses a custom attention mask that allows\n3\n"}, {"page": 4, "text": "Figure 2: An overview of the CAMEL architecture. 1-second, single-lead ECG segments from two patients are\nencoded and combined with text token embeddings. The resulting sequence is processed by an LLM backbone\n(MedGemma-4B with LoRA adapters) to generate a clinical report. Fixed models are shown in blue, and trainable\nmodels are shown in green.\nthe tokens of 1-second ECG segments at the same position (time t) to attend to each other bidirectionally, regardless\nof prompt lead order. Additionally, ECG embeddings can attend to their corresponding lead boundary markers\nand all preceding special tokens, while lead boundary markers can attend to all ECG embeddings within their\ncorresponding leads. This design allows the model to learn inter- and intra-lead relationships while preserving the\nsequential dependencies required for autoregressive text generation.\n3\nTraining\nIn this section, we describe the 5-stage training pipeline we use to learn ECG representations and translate this to\ngeneral ECG reasoning and forecasting.\nData preprocessing.\nPrior to training, we perform minimal preprocessing to preserve the information in raw data\nas much as possible while allowing the model to generalize across diverse clinical settings. We apply 50/60 and\n0.3Hz band-pass filters and resample each ECG to 256 Hz. We exclude data with persistent (> 5 seconds) zero or\nNaN values and replace any remaining NaN values with zero.\nStage 1: Autoencoder training.\nIn the first stage, we use self-supervised learning to train the the CNN-based\nencoder (i.e., the mapping from raw ECG to zecg), which will later be used as part of the full CNN+SLP encoder.\nGiven 1-second, single-lead ECG segments, the encoder-decoder pair is optimized with MSE loss to reconstruct\nthe input signal. The training dataset includes over 1 billion ECG segments collected from 13 public datasets\n(Appendix A).\nFor subsequent training stages, we connect the CNN-based encoder trained in stage 1 to the LLM via a linear\nlayer, allowing ECG embeddings to be mixed in with text and special token embeddings. We train using ECG-text\ninstruction tuning using a masked autoregressive cross-entropy loss. Unless otherwise specified, we apply lead-wise\nshuffling and random lead dropping during training to improve robustness to varying input leads. See Appendix A\nfor further training details.\nStage 2: Multiple choice and short answer.\nIn this next stage, we begin curriculum learning with simpler\ninstructional formats and less challenging ECG-related content than latter stages. Specifically, Stage 2 is restricted\nto short-answer and multiple-choice questions, primarily targeting high-level ECG classification tasks. The data for\nStage 2 are comprised of 20 million samples derived from the Harvard-Emory dataset (Koscova et al., 2024).\nStage 3: Adding statistics questions.\nIn the third stage, curriculum training incorporates question formats\nthat require fine-grained understanding of ECG statistics. Specifically, we leverage the ECGDeli library (Pilia\net al., 2021) to compute lead-specific and global statistics for each sample of the datasets from Stage 2. We then\ncreate short-answer questions based on these statistics, including statistics computation such as extracting T-wave\namplitude, step-by-step reasoning e.g to extract beat-wise R-R interval and compute heart rate, and multi-ECG\n4\n"}, {"page": 5, "text": "comparisons to compare e.g. longer QRS durations. These questions teach clinically meaningful statistics that\nunderlie the diagnoses introduced in Stage 2. Further details about the Stage 3 data are in Appendix C.1.\nStage 4: Multi-turn conversations.\nIn Stage 4, we unify the classification and statistical reasoning capabilities\nby training CAMEL with multi-turn conversations with 1 or 2 ECGs. We build on the GEM framework, which first\nintroduced the generation of ECG reports grounded in ECG statistics. (Lan et al., 2025). Using the GE Marquette\n12SL manual (GE Healthcare, 2019), we identify clinically important global and lead-specific statistics associated\nwith each diagnosis, which guides the selection of statistics for conversations. To ensure there is substantial diversity,\neach conversation randomly samples a task type, audience, response format, and dialogue arc (detailed in Appendix\nC.2). We pass these criteria, along with the selected statistics and diagnosis, to gemini-3-flash-preview to\ngenerate the dialogues. Further details about the Stage 4 data are in Appendix C.2. We refer to the model trained\nup to Stage 4 as CAMEL-Base.\nStage 5: Forecasting.\nIn the final stage, we train the model to produce evidence-based narratives that reason\nabout future cardiac rhythm transitions over specified time horizons. To construct a forecasting dataset, we use the\nIcentia11k dataset containing single-lead tracings, spanning multiple days per patient, with beats labeled as normal,\natrial fibrillation (AFIB), or atrial flutter (AFL) (Tan et al., 2022). We compile literature-grounded rubrics covering\nknown electrophysiologic substrates, triggers, and temporal risk factors for atrial arrhythmias. These features are\nverified with cardiologists to ensure clinical plausibility. We generate samples across various input lengths, forecast\ntime horizons, and label transitions, e.g., normal →normal or normal →AFIB. Using the validated features, we\ngenerate supervised forecasting examples with gemini-3-flash-preview consisting of a predicted transition\nlabel and a structured clinical explanation that links observed temporal trends to future risk (Fig. 1). Further details\nabout the Stage 5 data are in Appendix C.3. We refer to the model trained up to Stage 5 as CAMEL-Forecast.\n4\nEvaluation\nIn this section, we evaluate CAMEL against various baselines on downstream tasks such as forecasting, classifica-\ntion, and report generation.\n4.1\nExperimental Setup\nWe use the MedGemma-4B-instruct model (Sellergren et al., 2025) as the pretrained LLM backbone. The ECG\nencoder is comprised of a 3-layer CNN which projects each 256 Hz 1-lead, 1-second ECG segment to a d = 64\ndimension latent space, which is then passed through an SLP to the LLM’s latent token space (h = 2560). All\ntraining is done on 32 B200 NVIDIA GPUs. We fit the LLM backbone with LoRA adapters, each with a rank of\nr = 128. For further compute timing details and hyperparameters, see Appendix A.1. Unless otherwise specified,\nwe evaluate using CAMEL-Base on each task. Given CAMEL’s relatively small parameter size, fine-tuning\non the forecasting dataset prioritizes task-specific performance and results in a modest tradeoff in open-ended\nconversational ability, rather than a loss of core reasoning ability.\n4.2\nDownstream Tasks & Baselines\nForecasting.\nWe evaluate forecasting performance on the Icentia11k (Tan et al., 2022) dataset. Icentia11k\ncontains single-lead ECG recordings with NORM, AFIB, and AFL beat-level annotations. We report the F1\nscore for varying forecasting horizons (time-to-event) and input ECG signal duration. For this task, we use the\nCAMEL-Forecast model.\nClassification.\nWe evaluate on classifying diagnostic codes using seven datasets: PTB-XL (Wagner et al., 2020),\nCSN (Zheng et al., 2020), CODE-15% (Ribeiro et al., 2021), CPSC-2018 (Liu et al., 2018), HEEDB (Koscova\net al., 2024), Icentia (Tan et al., 2022), and Penn. Penn is a privately sourced ECG dataset from the University\nof Pennsylvania hospital system, containing 1K 8-lead ECG recordings from ICU units with NORM, Ventricular\nFibrillation (VT), and Ventricular Tachycardia (VF) labels. See Appendix A.2 for more details. We report F1 score\nfor zero-shot evaluation and AUROC for linear probing results. Like other baselines, we finetune CAMEL-Base on\nECGInstruct (Liu et al., 2024b) for this task.\nReport generation.\nWe evaluate report generation and conversation performance on PTB-XL (Wagner et al.,\n2020) and MIMIC-IV-ECG (Gow et al., 2023). We utilize GPT-5 to score the responses on diagnostic accuracy,\nanalysis completeness, and relevance based on the predefined criteria used for GEM Lan et al. (2025). We also\n5\n"}, {"page": 6, "text": "report traditional NLP metrics: BLEU-1, BLEU-4, METEOR, Rouge, and BERT-F1. Like other baselines, we\nfinetune CAMEL-Base on ECGInstruct (Liu et al., 2024b) for this task.\nQuestion answering.\nWe use multiple-choice questions from ECGBench (Liu et al., 2024b), sourced from the\nG12 (Perez Alday et al., 2020) and CSN (Zheng et al., 2020) datasets and ECG-QA Oh et al. (2023), sourced\nfrom PTB-XL and MIMIC-IV-ECG. ECG-QA comprises single-ECG questions for general interpretation and\nmulti-ECG questions for comparative analysis of two ECGs. We use accuracy and hamming score.\nMulti-turn conversations.\nThis benchmark comprises two multi-turn task types curated from PTB-XL reports.\nThe first is a two-turn task that requires interpreting a single ECG and subsequently answering questions about\nit. The second is a three-turn task that requires interpreting two ECG recordings and performing comparative\nanalysis. Following PULSE (Liu et al., 2024b), we use GPT-5 to score the conversation based on diagnostic\naccuracy, analysis completeness, and instruction adherence.\nGrounding.\nWe create a benchmark of 2K ECG statistics (RR interval, HR, QRS duration, QTc interval, PR\ninterval, QRS amplitude) from CPSC-2018 samples. We report RMSE between the ground truth values computed\nusing ECGDeli (Pilia et al., 2021) and the predicted values.\nBaselines.\nFor baselines, we use MELP (Wang et al., 2025), MERL (Liu et al., 2024a), PULSE (Liu et al., 2024b),\nand GEM (Lan et al., 2025). MELP and MERL are capable of zero-shot classification of ECG waveforms. Both\ntake a list of classes and ECG as input and return the most likely class based on the cosine similarity. PULSE and\nGEM are multi-modal LLMs finetuned for ECG interpretation. PULSE uses ECG images as inputs whereas GEM\nuses both images and waveforms. We use all four baselines for classification tasks, but exclude MERL and MELP\non others where a prompt has to be supplied. On the forecasting task we also evaluate on GPT-5.2, augmented with\na Code Interpreter, and access to the raw ECG stored as a CSV file.\n4.3\nForecasting\nWe study the effectiveness of CAMEL on forecasting adverse cardiac events. We compare the performance as\nwe vary the forecasting horizon h ∈{1, 3, 5, 10} minutes and ECG signal duration l ∈{10, 30, 60, 120, 300, 600}\nseconds in Table 8. Note that unlike CAMEL, baseline ELMs do not support ECG inputs longer than 10 seconds.\nThe results demonstrate that CAMEL (evaluated on a subset of the test data), outperforms all baselines, including\nGPT-5.2 by over 10%. Across all models and datasets, we observe that the model performance drops as we increase\nthe forecasting horizon. However, we observe a notable increase in F1 score with longer ECG signals. For 10\nminutes inputs, we are providing 40 times more information compared to the standard 10 second ECGs, and the\ntemporal variability in signal aids forecasting.\n4.4\nExisting Benchmarks\nWe compare zero-shot performance of CAMEL against baseline models on previously studied tasks: classification,\nreport generation, question answering, and grounded ECG understanding.\nClassification.\nWe present zero-shot classification results using 10-second, full-lead inputs in Tables 2 and 9.\nFor Icentia11k and Penn datasets, we provide 1 and 8 leads respectively, and 12 leads for all other datasets. We\nadditionally evaluate reduced-lead settings to mimic scenarios where fewer leads are available, with results reported\nin Table 11. For Icentia11k AFIB classification, we vary the input duration l ∈{10, 30, 60, 120, 300, 600} seconds\nto assess the effect of temporal context (Table 10).\nIn the single-lead telemetry analysis, the sensitivity for classifying AFIB increased as the recording duration\nincreased from 10 to 60 seconds. After 60 seconds, there were only modest improvements in diagnostic metrics,\nsuggesting that longer contexts lengths can help, must most discriminative information is captured within the first\nminute of telemetry. For full-lead classification, CAMEL achieves the best performance on 7 of 17 tasks, including\nthe out-of-domain CSN and Penn datasets. Notably, CAMEL performs best on 3 of 4 PTB-XL tasks and attains\ncompetitive F1 scores on Code15 and HEEDB.\nA consistent trend across datasets is that LLM-based models (PULSE, GEM, CAMEL) struggle on tasks with\nlarge label spaces, such as HEEDB and CSN (Table 9), whereas non-LLM architectures (e.g., MELP and MERL)\nmaintain strong performance in these multi-class settings.\n6\n"}, {"page": 7, "text": "10\n30\n60\n120\n300\n600\n0\n20\n40\n60\n80\nw (s)\nF1 (%)\nh = 60s\nPULSE\nGEM\nGPT-5.2\nCAMEL\nXGB\nCNN\nCAMEL Probe\n10\n30\n60\n120\n300\n600\n0\n20\n40\n60\n80\nw (s)\nh = 180s\n10\n30\n60\n120\n300\n600\n0\n20\n40\n60\n80\nw (s)\nF1 (%)\nh = 300s\n10\n30\n60\n120\n300\n600\n0\n20\n40\n60\n80\nw (s)\nh = 600s\nFigure 3: Forecasting performance (Macro-F1) in predicting AFib, AFlutter or Sinus Rhythm across input window\nw (s) and horizon h (s). We report zero-shot results from GPT-5.2 (with Code Interpreter and high effort), PULSE,\nGEM, and CAMEL, supervised training results for XGB and CNN, and linear probing results for CAMEL Probe.\nPULSE and GEM only support 10-second ECG inputs (w = 10). CAMEL outperforms all baselines, with increased\nperformance with higher input windows, highlighting the importance of supporting longer ECG recordings.\nLinear probing.\nTo evaluate the quality of the latent representation learned by CAMEL, we conduct linear\nprobing classification experiments where we freeze the parameters of the model and train a linear layer with 1% of\nthe dataset’s training data. We report the results in Table 3. We compare CAMEL-Base to the standard baselines as\nwell as the CAMEL architecture trained solely on ECGInstruct. CAMEL outperforms baselines on both in-domain\nand out-domain datasets across all tasks, demonstrating that it has learned discriminative representation of the ECG\nsignals. CAMEL’s superior performance to the ECGInstruct-only finetuned version demonstrates the usefulness of\nthe curriculum training for learning representations.\nReport generation.\nWe present the full report generation results in Table 14, and provide an example report\ngenerated by CAMEL in Figures 4 and 5. Using LLM-as-a-judge, CAMEL achieves superior performance on\nMIMIC-IV. CAMEL also comes within 1 point of GEM, the top-performing model on PTB-XL on a 30-point scale.\nCAMEL also attains competitive scores on the 5 NLP metrics for both MIMIC-IV and PTB-XL, demonstrating\nstrong conversational ability.\nSingle and Multi-ECG QA.\nWe report results for the multiple choice and single-ECG questions from ECG-QA,\nas well as multi-ECG questions, in Table 2. Because Stages 3 and 4 of the curriculum contain diverse multi-ECG\nquestions, CAMEL achieves superior accuracy on multi-ECG questions compared to GEM and PULSE. CAMEL\nalso maintains strong performance on single-ECG questions in ECG-QA, indicating that training on comparative\nmulti-ECG reasoning does not degrade performance in the single-ECG setting.\nStatistics.\nWe report performance on the ECG grounding dataset in Table 2. Compared to the baselines, CAMEL\nachieves the lowest average RMSE on statistics computations. Notably, CAMEL’s RMSE is nearly half that of\nGEM, a model trained on ECG reasoning grounded in statistics. This improvement likely stems from our staged\ncurriculum: Stage 3 introduces focused, direct statistical computation questions, while Stage 4 integrates these\n7\n"}, {"page": 8, "text": "Table 2: Comprehensive evaluation results across three tasks: Forecasting, Classification, and Report Generation.\nBest results are highlighted in “-” indicates method not evaluated on task/dataset.\nClassification\nECG-QA\nStat\nReport\nPTB-XL Code15 CPSC CSN HEEDB Icentia Penn Single Multi\nCPSC\nPTB-XL MIMIC\nMethod\nF1\nAccuracy\nRMSE\nLLM\nMERL\n19.22\n19.72\n24.86\n6.38\n16.33\n15.80\n2.53\n-\n-\n-\n-\n-\nMELP\n18.62\n26.07\n20.75\n5.00\n19.03\n10.80\n2.53\n-\n-\n-\n-\n-\nPULSE\n28.06\n75.90\n57.08\n12.62\n18.62\n25.15\n34.12\n79.07\n64.41 144148\n18.40\n22.05\nGEM\n29.82\n80.83\n61.06\n8.19\n19.40\n18.70\n33.33\n80.80\n65.89\n304\n20.45\n44.65\nCAMEL\n40.37\n78.20\n56.12\n12.84\n20.89\n41.14\n67.53\n72.99\n69.23\n109\n19.45\n62.59\nTable 3: Linear probing performance (AUC [%]) of ECG classification across multiple datasets, 12-leads. Results\nare reported for different training data proportions (1%, 10%, 100%) CAMEL (ECGInstruct) refers to CAMEL\ntrained only on ECGInstruct data Liu et al. (2024b), whereas CAMEL refers to the model trained on our curriculum\nspecified in § 3.\nMethods\nPTBXL-Rhythm\nPTBXL-Sub\nPTBXL-Form\nPTBXL-Super\nCPSC2018\nCSN\nTraining Ratio\n1%\n10% 100%\n1%\n10% 100%\n1%\n10% 100%\n1%\n10% 100%\n1%\n10% 100%\n1%\n10% 100%\nMERL (2024a)\n53.33 82.88 88.34 64.90 80.56 84.72 58.70 72.43 79.65 82.39 86.27 88.67 70.33 85.22 90.57 66.60 82.74 87.95\nMELP (2025)\n88.83 94.65 96.91 79.22 84.40 87.46 63.44 76.71 83.30 85.82 87.61 87.87 88.54 91.75 94.32 78.25 84.83 90.17\nPULSE (2024b)\n98.38 99.07 99.25 89.21 92.44 93.88 82.38 90.19 91.97 89.88 92.32 93.08 67.81 73.90 82.48 87.25 89.49 91.87\nGEM (2025)\n97.55 98.78 99.12 89.48 92.57 93.19 83.32 88.07 90.77 87.61 91.88 93.03 74.58 82.41 91.38 87.12 90.33 91.69\nCAMEL (ECGInstruct) 97.74 98.98 99.07 90.07 92.38 93.75 81.28 89.47 92.17 87.59 91.58 92.33 78.54 93.37 98.52 90.00 95.05 97.11\nCAMEL\n98.13 99.17 99.39 90.52 93.41 94.14 85.98 91.14 93.27 91.40 93.73 94.73 86.04 96.15 99.40 93.52 97.17 98.36\ncomputations into diagnosis tasks. In contrast, GEM encounters statistics only the context of diagnostic reasoning,\nwithout an intermediate phase dedicated to learning the underlying computations.\n4.5\nAblation\nWe study the design components present in the architecture of CAMEL. We vary LoRA adapter usage, special ECG\ntokens, attentions masks, and train with different combinations on ECGInstruct and evaluate on its counterpart test\nset ECGBench.\nAs shown in Table 4, the removal of LoRA significantly negatively impacts the performance. We posit that, due to\nCAMEL’s relatively simple encoding network, most of the trainable parameters stem from LoRA. LoRA allows\npretrained LLM’s transformer blocks to learn deeper and more complex relationships betwen text and ECG.\nFinally, we evaluate CAMEL’s lead-aware attention masking scheme against full bi-directional attention within\neach ECG block, and vanilla causal attention throughout all tokens. As shown in Table 4, our ECG attention\nmasking scheme outperforms all other options. We attribute the benefits of this scheme primarily due to the nature\nof ECGs: at time t, all leads represent the same state of the heart.\n5\nRelated Work\nIn this section, we discuss the field of ML-based ECG comprehension and divide it into (i) unimodal ECG\nfoundation models, (ii) multimodal ECG models, including ELMs, (iii) general natural-language-based ECG\nmodels, and (iv) long-horizon cardiac event forecasting.\nUnimodal ECG foundation models.\nRecent advances in foundation models have transformed ECG analysis\nthrough large-scale pretraining. HuBERT-ECG (Coppola et al., 2024) uses masked signal reconstruction, predicting\nmasked ECG segments from unmasked context. ECG-FM (McKeen et al., 2025) combines signal masking\nwith contrastive learning across ECG segments. ECGFounder (Li et al., 2025b) employs supervised multi-label\nclassification on expert diagnostic annotations. These foundation models are unimodal, i.e. trained solely on ECG\nsignals during training, and require fine-tuning for downstream tasks.\n8\n"}, {"page": 9, "text": "Table 4: Architecture ablations (F1 [%]). Training without LoRA adapters leads to poor ECG representations,\nas evidenced by the zero F-1 scores on PTB-XL tasks. Lead-aware masking, which enables bidirectionality in\nattention computation for ECG leads, proves be better, or at-par with full masking. Both LoRA adapters and\nlead-aware masking are necessary for achieving SOTA accuracy.\nMethod\nPTBXL-Super\nPTBXL-Sub\nPTBXL-Rhythm\nPTBXL-Form\nCode15\nCPSC-2018\nCSN\nNo LoRA + Lead-aware masking\n0.00\n0.00\n0.00\n0.00\n27.35\n1.18\n0.00\nLoRA + Full masking\n54.39\n17.22\n21.34\n16.98\n75.13\n46.69\n10.15\nLoRA + Causal masking\n69.15\n22.13\n21.08\n14.72\n77.34\n42.30\n10.02\nLoRA + Lead-aware masking\n75.91\n26.49\n34.27\n19.03\n81.54\n51.10\n14.30\nMultimodal ECG models.\nMultimodal models such as MERL (Liu et al., 2024a) enable zero-shot classification\nthrough joint learning on ECG records and clinical reports. MELP (Wang et al., 2025) also trains on both modalities,\nbut adds hierarchical supervision at the token, beat, and rhythm-level to align ECG with reports at different time-\nscales. PULSE (Liu et al., 2024b) is another ECG-text model that performs LLaVA-style finetuning for ECG\nimage analysis. GEM (Lan et al., 2025) jointly trains ECG and text with a conversational dataset grounded in ECG\nstatistics. As noted earlier, the inputs to these models are limited to at most 10 seconds.\nNatural language generation & understanding.\nNatural language generation-based methods are more powerful,\nsince they implicitly include classification, and can also support report generation and general QA. Recent\napproaches are trained in a multi-stage fashion, either via self-supervised or contrastive learning. For example,\nECG-ReGen (Tang et al., 2024) uses a self-supervised encoder, ECG-Chat (Zhao et al., 2025) employs contrastive\nlearning for feature alignment, MEIT (Wan et al., 2025) focuses on multimodal instruction tuning, and anyECG-chat\n(Li et al., 2025a) utilizes three-step curriculum training for variable length inputs. Additionally, METS (Li et al.,\n2024) uses self-supervised learning to train an ECG encoder paired with a frozen language model. A notable\nexception is ECG-Byte (Han et al., 2025), which bypasses a specialized encoder by directly tokenizing the ECG\nsignal, enabling end-to-end LLM training.\nForecasting cardiac events.\nDeep learning and classical ML techniques have also demonstrated potential for\nlong-horizon forecasting of adverse cardiac events. Kenet et al. (2023) apply XGBoost to forecast pediatric cardiac\narrest from clinical data, reporting high auROC and auPRC. Kim et al. (2022) also use XGBoost in addition to\nother classical ML models to forecast in-hospital cardiac arrest for emergency department (ED) patients, reporting\nhigh auROC. However, both of these works use clinical features beyond raw ECG, e.g., ED occupancy, limiting\ntheir broader applicability. Conversely, Rooney et al. (2023) use convolutional-transformer models for forecasting\nimminent atrial fibrillation events using only long-term ECG waveforms, demonstrating that such models can learn\npredictive features from ECG without additional clinical features.\n6\nLimitations and Future Work\nThere are two main limitations with CAMEL’s tokenization strategy. While CAMEL can handle ECG segments\nlonger than 10 seconds, the maximum input ECG segment is limited by the context length of the backbone LLM.\nAdditionally, segmenting ECG into 1-second chunks may truncate the QRS complex or fail to capture subtle\nmorphological changes in the signal. For future work, we plan to explore different tokenization strategies such as\nsegmenting based on QRS intervals, which could serve as semantic unit of explanation, or using 5 second ECG\nsegments, which would allow longer duration ECG to fit into the context window.\n7\nConclusion\nIn this work, we introduce CAMEL, an ECG language model for forecasting cardiac events. Unlike prior\nworks, CAMEL supports flexible ECG context windows exceeding 10 seconds, enabling reasoning over longer\nsignals. Its multimodal architecture combines a CNN-based ECG encoder and an LLM with LoRA adapters,\nand is trained using a 5-stage curriculum that progressively builds ECG understanding and forecasting capability.\nCAMEL achieves SOTA results on prior benchmarks as well as our newly introduced forecasting benchmark,\nECGForecastBench.\n9\n"}, {"page": 10, "text": "Acknowledgments and Disclosure of Funding\nThis research was supported by ARPA-H program on Safe and Explainable AI under the award D24AC00253-00\nand by a gift from AWS AI to ASSET (Penn Engineering Center on Trustworthy AI).\nReferences\nR. Bousseljot, D. Kreiseler, and A. Schnabel. Nutzung der ekg-signaldatenbank cardiodat der ptb ¨uber das internet,\nJuly 2009. ISSN 1862-278X. URL http://dx.doi.org/10.1515/bmte.1995.40.s1.317.\nSusan Cheng, Michelle J Keyes, Martin G Larson, Elizabeth L McCabe, Christopher Newton-Cheh, Daniel Levy,\nEmelia J Benjamin, Ramachandran S Vasan, and Thomas J Wang. Long-term outcomes in individuals with\nprolonged PR interval or first-degree atrioventricular block. JAMA, 301(24):2571–2577, June 2009.\nEdoardo Coppola, Mattia Savardi, Mauro Massussi, Marianna Adamo, Marco Metra, and Alberto Signoroni.\nHubert-ecg as a self-supervised foundation model for broad and scalable cardiac applications, November 2024.\nURL http://dx.doi.org/10.1101/2024.11.14.24317328.\nJos´e Maria Farinha, Dhiraj Gupta, and Gregory Y H Lip. Frequent premature atrial contractions as a signalling\nmarker of atrial cardiomyopathy, incident atrial fibrillation, and stroke. Cardiovascular Research, 119(2):\n429–439, March 2023.\nGE Healthcare. Marquette 12SL ECG Analysis Program: Physician’s Guide. General Electric Company, 2019.\nURL https://www.gehealthcare.com.\nSven Geurts, Martijn J Tilly, Banafsheh Arshi, Bruno H C Stricker, Jan A Kors, Jaap W Deckers, Natasja M S\nde Groot, M Arfan Ikram, and Maryam Kavousi. Heart rate variability and atrial fibrillation in the general\npopulation: a longitudinal and mendelian randomization study. Clin. Res. Cardiol., 112(6):747–758, June 2023.\nBrian Gow, Tom Pollard, Larry A Nathanson, Alistair Johnson, Benjamin Moody, Chrystinne Fernandes, Nathaniel\nGreenbaum, Jonathan W Waks, Parastou Eslami, Tanner Carbonati, Ashish Chaudhari, Elizabeth Herbst, Dana\nMoukheiber, Seth Berkowitz, Roger Mark, and Steven Horng. Mimic-iv-ecg: Diagnostic electrocardiogram\nmatched subset, 2023. URL https://physionet.org/content/mimic-iv-ecg/1.0/.\nWilliam Han, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, and Ding Zhao. Ecg-byte: A tokenizer for end-\nto-end generative electrocardiogram language modeling, 2025. URL https://arxiv.org/abs/2412.14373.\nChien Hua Huang, Randy Su, Hui Chun Huang, Katie Lin, Nick Foster, Nathan Juergens, Josh Risley, Katy Haynes,\nand Ziad Obermeyer. Subtyping cardiac arrest with ecg waveforms: A nightingale open science dataset, 2021.\nURL https://doi.org/10.48815/N5WC7D.\nSelcan Kaplan Berkaya, Alper Kursat Uysal, Efnan Sora Gunal, Semih Ergin, Serkan Gunal, and M. Bilginer\nGulmezoglu. A survey on ecg analysis. Biomedical Signal Processing and Control, 43:216–235, 2018.\nISSN 1746-8094. doi: https://doi.org/10.1016/j.bspc.2018.03.003. URL https://www.sciencedirect.com/\nscience/article/pii/S1746809418300636.\nAdam L. Kenet, Rahul Pemmaraju, Sejal Ghate, Shreeya Raghunath, Yifan Zhang, Mordred Yuan, Tony Y. Wei,\nJacob M. Desman, Joseph L. Greenstein, Casey O. Taylor, Timothy Ruchti, James Fackler, and Jules Bergmann.\nA pilot study to predict cardiac arrest in the pediatric intensive care unit. Resuscitation, 185:109740, April\n2023. ISSN 0300-9572. doi: 10.1016/j.resuscitation.2023.109740. URL http://dx.doi.org/10.1016/j.\nresuscitation.2023.109740.\nJi Hoon Kim, Arom Choi, Min Joung Kim, Heejung Hyun, Sunhee Kim, and Hyuk-Jae Chang. Development of a\nmachine-learning algorithm to predict in-hospital cardiac arrest for emergency department patients using a nation-\nwide database. Scientific Reports, 12(1), December 2022. ISSN 2045-2322. doi: 10.1038/s41598-022-26167-1.\nURL http://dx.doi.org/10.1038/s41598-022-26167-1.\nZuzana Koscova, Qiao Li, Chad Robichaux, Valdery Moura Junior, Manohar Ghanta, Aditya Gupta, Jonathan\nRosand, Aaron Aguirre, Shenda Hong, David E. Albert, Joel Xue, Aarya Parekh, Reza Sameni, Matthew A.\nReyna, M. Brandon Westover, and Gari D. Cliford. The harvard-emory ecg database. medRxiv, 2024. doi:\n10.1101/2024.09.27.24314503. URL https://www.medrxiv.org/content/early/2024/10/01/2024.09.\n27.24314503.\n10\n"}, {"page": 11, "text": "P. Laguna, R.G. Mark, A. Goldberg, and G.B. Moody. A database for evaluation of algorithms for measurement\nof qt and other waveform intervals in the ecg. In Computers in Cardiology 1997, pp. 673–676, 1997. doi:\n10.1109/CIC.1997.648140. URL http://dx.doi.org/10.1109/CIC.1997.648140.\nXiang Lan, Feng Wu, Kai He, Qinghao Zhao, Shenda Hong, and Mengling Feng. Gem: Empowering mllm for\ngrounded ecg understanding with time series and images, 2025. URL https://arxiv.org/abs/2503.06073.\nHaitao Li, Ziyu Li, Yiheng Mao, Ziyi Liu, Zhoujian Sun, and Zhengxing Huang. anyecg-chat: A generalist ecg-mllm\nfor flexible ecg input and multi-task understanding, 2025a. URL https://arxiv.org/abs/2506.00942.\nJun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. Frozen language model helps ecg zero-shot\nlearning. In Ipek Oguz, Jack Noble, Xiaoxiao Li, Martin Styner, Christian Baumgartner, Mirabela Rusu,\nTobias Heinmann, Despina Kontos, Bennett Landman, and Benoit Dawant (eds.), Medical Imaging with Deep\nLearning, volume 227 of Proceedings of Machine Learning Research, pp. 402–415. PMLR, 10–12 Jul 2024.\nURL https://proceedings.mlr.press/v227/li24a.html.\nJun Li, Aaron D. Aguirre, Valdery Moura Junior, Jiarui Jin, Che Liu, Lanhai Zhong, Chenxi Sun, Gari Clifford,\nM. Brandon Westover, and Shenda Hong. An electrocardiogram foundation model built on over 10 million\nrecordings. NEJM AI, 2(7), June 2025b. ISSN 2836-9386. doi: 10.1056/aioa2401033. URL http://dx.doi.\norg/10.1056/AIoa2401033.\nChe Liu, Zhongwei Wan, Cheng Ouyang, Anand Shah, Wenjia Bai, and Rossella Arcucci.\nZero-shot ecg\nclassification with multimodal learning and test-time clinical knowledge enhancement, 2024a. URL https:\n//arxiv.org/abs/2403.06659.\nFeifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan Xu, Yulin Liu, Caiyun Ma, Shoushui\nWei, Zhiqiang He, Jianqing Li, and Eddie Ng Yin Kwee. An open access database for evaluating the algorithms\nof electrocardiogram rhythm and morphology abnormality detection. Journal of Medical Imaging and Health\nInformatics, 8(7):1368–1373, September 2018.\nISSN 2156-7018.\ndoi: 10.1166/jmihi.2018.2442.\nURL\nhttp://dx.doi.org/10.1166/jmihi.2018.2442.\nHui Liu, Dan Chen, Da Chen, Xiyu Zhang, Huijie Li, Lipan Bian, Minglei Shu, and Yinglong Wang. A large-scale\nmulti-label 12-lead electrocardiogram database with standardized diagnostic statements, June 2022. ISSN\n2052-4463. URL http://dx.doi.org/10.1038/s41597-022-01403-5.\nRuoqi Liu, Yuelin Bai, Xiang Yue, and Ping Zhang. Teach multimodal llms to comprehend electrocardiographic\nimages, 2024b. URL https://arxiv.org/abs/2410.19008.\nKaden McKeen, Sameer Masood, Augustin Toma, Barry Rubin, and Bo Wang. Ecg-fm: an open electrocardiogram\nfoundation model, September 2025. ISSN 2574-2531. URL http://dx.doi.org/10.1093/jamiaopen/\nooaf122.\nG.B. Moody and R.G. Mark. The impact of the mit-bih arrhythmia database. IEEE Engineering in Medicine and\nBiology Magazine, 20(3):45–50, 2001. ISSN 0739-5175. doi: 10.1109/51.932724. URL http://dx.doi.org/\n10.1109/51.932724.\nJungwoo Oh, Gyubok Lee, Seongsu Bae, Joon myoung Kwon, and Edward Choi. ECG-QA: A comprehensive\nquestion answering dataset combined with electrocardiogram. In Thirty-seventh Conference on Neural Infor-\nmation Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?\nid=YWJ7Yi4OtH.\nErick A Perez Alday, Annie Gu, Amit J Shah, Chad Robichaux, An-Kwok Ian Wong, Chengyu Liu, Feifei Liu,\nAli Bahrami Rad, Andoni Elola, Salman Seyedi, Qiao Li, Ashish Sharma, Gari D Clifford, and Matthew A\nReyna. Classification of 12-lead ecgs: the physionet/computing in cardiology challenge 2020. Physiological\nMeasurement, 41(12):124003, December 2020. ISSN 1361-6579. doi: 10.1088/1361-6579/abc960. URL\nhttp://dx.doi.org/10.1088/1361-6579/abc960.\nNicolas Pilia, Claudia Nagel, Gustavo Lenis, Silvia Becker, Olaf D¨ossel, and Axel Loewe. Ecgdeli - an open\nsource ecg delineation toolbox for matlab. SoftwareX, 13:100639, January 2021. ISSN 2352-7110. doi:\n10.1016/j.softx.2020.100639. URL http://dx.doi.org/10.1016/j.softx.2020.100639.\n11\n"}, {"page": 12, "text": "Murray M. Pollack, Richard Holubkov, Tomohiko Funai, J. Michael Dean, John T. Berger, David L. Wessel,\nKathleen Meert, Robert A. Berg, Christopher J. L. Newth, Rick E. Harrison, Joseph Carcillo, Heidi Dalton,\nThomas Shanley, Tammara L. Jenkins, and Robert Tamburro. The pediatric risk of mortality score: Update\n2015*. Pediatric Critical Care Medicine, 17(1):2–9, January 2016. ISSN 1529-7535. doi: 10.1097/pcc.\n0000000000000558. URL http://dx.doi.org/10.1097/PCC.0000000000000558.\nAntˆonio H. Ribeiro, Gabriela M.M. Paixao, Emilly M. Lima, Manoel Horta Ribeiro, Marcelo M. Pinto Filho,\nPaulo R. Gomes, Derick M. Oliveira, Wagner Meira Jr, Th¨omas B Schon, and Antonio Luiz P. Ribeiro. Code-15\nURL https://zenodo.org/doi/10.5281/zenodo.4916206.\nSydney R. Rooney, Roman Kaufman, Raghavan Murugan, Kianoush B. Kashani, Michael R. Pinsky, Salah Al-Zaiti,\nArtur Dubrawski, Gilles Clermont, and J. Kyle Miller. Forecasting imminent atrial fibrillation in long-term\nelectrocardiogram recordings. Journal of Electrocardiology, 81:111–116, November 2023. ISSN 0022-0736. doi:\n10.1016/j.jelectrocard.2023.08.011. URL http://dx.doi.org/10.1016/j.jelectrocard.2023.08.011.\nStefano Savonitto, Diego Ardissino, Christopher B. Granger, Giorgio Morando, Maria D. Prando, Antonio Mafrici,\nClaudio Cavallini, Giovanni Melandri, Trevor D. Thompson, Alec Vahanian, E. Magnus Ohman, Robert M.\nCaliff, Frans Van de Werf, and Eric J. Topol. Prognostic value of the admission electrocardiogram in acute\ncoronary syndromes. JAMA, 281(8):707–713, 02 1999. ISSN 0098-7484. doi: 10.1001/jama.281.8.707. URL\nhttps://doi.org/10.1001/jama.281.8.707.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, C´ıan Hughes, Charles Lau, Justin Chen, Fereshteh Mahvar, Liron Yatziv, Tiffany\nChen, Bram Sterling, Stefanie Anna Baby, Susanna Maria Baby, Jeremy Lai, Samuel Schmidgall, Lu Yang,\nKejia Chen, Per Bjornsson, Shashir Reddy, Ryan Brush, Kenneth Philbrick, Mercy Asiedu, Ines Mezerreg,\nHoward Hu, Howard Yang, Richa Tiwari, Sunny Jansen, Preeti Singh, Yun Liu, Shekoofeh Azizi, Aishwarya\nKamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova,\nAlexandre Ram´e, Morgane Riviere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill,\nSabela Ramos, Edouard Yvinec, Michelle Casbon, Elena Buchatskaya, Jean-Baptiste Alayrac, Dmitry Lepikhin,\nVlad Feinberg, Sebastian Borgeaud, Alek Andreev, Cassidy Hardin, Robert Dadashi, L´eonard Hussenot, Armand\nJoulin, Olivier Bachem, Yossi Matias, Katherine Chou, Avinatan Hassidim, Kavi Goel, Clement Farabet, Joelle\nBarral, Tris Warkentin, Jonathon Shlens, David Fleet, Victor Cotruta, Omar Sanseviero, Gus Martins, Phoebe\nKirk, Anand Rao, Shravya Shetty, David F. Steiner, Can Kirmizibayrak, Rory Pilgrim, Daniel Golden, and Lin\nYang. Medgemma technical report, 2025. URL https://arxiv.org/abs/2507.05201.\nJustin W Smith, Wesley T O’Neal, M Benjamin Shoemaker, Lin Y Chen, Alvaro Alonso, S Patrick Whalen,\nand Elsayed Z Soliman. PR-interval components and atrial fibrillation risk (from the atherosclerosis risk in\ncommunities study). Am. J. Cardiol., 119(3):466–472, February 2017.\nJasmeet Soar, Bernd W. B¨ottiger, Pierre Carli, Keith Couper, Charles D. Deakin, Therese Dj¨arv, Carsten Lott,\nTheresa Olasveengen, Peter Paal, Tommaso Pellis, Gavin D. Perkins, Claudio Sandroni, and Jerry P. Nolan.\nEuropean resuscitation council guidelines 2021: Adult advanced life support. Resuscitation, 161:115–151, April\n2021. ISSN 0300-9572. doi: 10.1016/j.resuscitation.2021.02.010. URL http://dx.doi.org/10.1016/j.\nresuscitation.2021.02.010.\nShawn Tan, Satya Ortiz-Gagn´e, Nicolas Beaudoin-Gagnon, Pierre Fecteau, Aaron Courville, Yoshua Bengio,\nand Joseph Paul Cohen. Icentia11k single lead continuous raw electrocardiogram dataset, 2022. URL https:\n//physionet.org/content/icentia11k-continuous-ecg/1.0/.\nJialu Tang, Tong Xia, Yuan Lu, Cecilia Mascolo, and Aaqib Saeed. Electrocardiogram report generation and\nquestion answering via retrieval-augmented self-supervised modeling, 2024. URL https://arxiv.org/abs/\n2409.08788.\nPatrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I. Lunze, Wojciech Samek,\nand Tobias Schaeffter. Ptb-xl, a large publicly available electrocardiography dataset. Scientific Data, 7(1),\nMay 2020. ISSN 2052-4463. doi: 10.1038/s41597-020-0495-6. URL http://dx.doi.org/10.1038/\ns41597-020-0495-6.\nZhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Jing Xiong, Rossella Arcucci, Huaxiu Yao, and\nMi Zhang. Meit: Multimodal electrocardiogram instruction tuning on large language models for report generation,\n2025. URL https://arxiv.org/abs/2403.04945.\n12\n"}, {"page": 13, "text": "Fuying Wang, Jiacheng Xu, and Lequan Yu. From token to rhythm: A multi-scale approach for ecg-language\npretraining, 2025. URL https://arxiv.org/abs/2506.21803.\nFons J Wesselius, Mathijs S van Schie, Natasja M S De Groot, and Richard C Hendriks. Digital biomarkers and\nalgorithms for detection of atrial fibrillation using surface electrocardiograms: A systematic review. Comput.\nBiol. Med., 133(104404):104404, June 2021.\nLifan Zhang, Bingxun Li, and Lin Wu. Heart rate variability in patients with atrial fibrillation of sinus rhythm or\natrial fibrillation: chaos or merit? Ann. Med., 57(1):2478474, December 2025.\nYubao Zhao, Jiaju Kang, Tian Zhang, Puyu Han, and Tong Chen. Ecg-chat: A large ecg-language model for\ncardiac disease diagnosis, 2025. URL https://arxiv.org/abs/2408.08849.\nJianwei Zheng, Jianming Zhang, Sidy Danioko, Hai Yao, Hangyuan Guo, and Cyril Rakovski. A 12-lead\nelectrocardiogram database for arrhythmia research covering more than 10, 000 patients. Scientific Data, 7(1),\nFebruary 2020. ISSN 2052-4463. doi: 10.1038/s41597-020-0386-x. URL http://dx.doi.org/10.1038/\ns41597-020-0386-x.\n13\n"}, {"page": 14, "text": "A\nExperimental Set-up\nTable 5: Overview of the ECG Datasets.\nSupervision\nDataset Name\nSource/Collection\nRecords\nDuration\nLeads\nSignal-only\nMITDB (2001)\nBeth Israel\n48\n30 m\n2\nNTUH (2021)\nNational Taiwan University Hospital\n1,000\n10 s\n12\nPTB (2009)\nUniversity Clinic Benjamin Franklin\n549\n30–120 s\n12\nQTDB (1997)\nBeth Israel, Multi-center Europe\n105\n15 m\n2\nSPH (2022)\nShandong Provincial Hospital\n25,770\n10–60 s\n12\nG12 (2020)\nEmory University\n10,344\n10 s\n12\nLabels\n(in-distribution)\nCode-15% (2021)\nTelehealth Network (Brazil)\n345,779\n7/10 s\n12\nHEEDB (2024)\nHarvard University, Emory University\n11,440,211\n10 s\n12\nIcentia11k (2022)\nCardioSTAT (Canada)\n11,000\n1–2 weeks\n1\nMIMIC-IV (2023)\nBeth Israel\n795,546\n10 s\n12\nPTB-XL (2020)\nSchiller AG (Germany)\n21,837\n10 s\n12\nLabels\n(out-of-distribution)\nCPSC-2018 (2018)\nMulti-center China\n13,256\n6–144 s\n12\nCSN (2020)\nChapman University, Shaoxing People’s\nHospital, Ningbo First Hospital\n80,938\n10 s\n12\nPenn\nPenn hospital system\n191\n10 m - 48 hrs\n8\nWe collect 1 private and 14 public ECG datasets (listed in Table 5). The signal-only datasets are used only in\ntraining the autoencoder, i.e. Stage 1. The in-distribution datasets are used in Stage 2, 3 and 4; we only use the\nHarvard-Emory dataset in Stage 2 and 3 due to data use policies prohibiting LLM API calls. For each in-distribution\ndataset, we set aside 20% of the records for validation and evaluation and use the remaining records for training.\nFor the out-of-distribution datasets, we do not use any samples for training.\nA.1\nHyperparameters\nWe list all global and stage-specific hyperparameters in Tables 6 and 7.\nTable 6: Hyperparameters for Training Stages 2-5\nStage\nLearning Rate\nEpochs\nBatch Size\nWarmup steps\nStage 2\n1e-4\n3\n8\n500\nStage 3\n1e-4\n1\n8\n500\nStage 4\n1e-4\n9\n4\n256\nStage 5\n1e-4\n3\n2\n256\nTable 7: Global Hyperparameters\nHyperparameter\nValue\nTotal Gradient Accumulation Target\n4\nGradient Clipping\n1.0\nOptimizer\nAdamW\nLearning rate scheduler\nlinear\nLoRA Rank\n128\nMask strategy\nsemantic\n14\n"}, {"page": 15, "text": "A.2\nBaselines and Tasks\nFor MERL, we evaluate using ncbi/MedCPT-Query-Encoder text encoder and 1D ResNet18 ECG encoder\nwith embedding dimension 256. For MELP, we evaluate using fuyingw/heart bert text encoder and 8-layer\nECG encoder with 12 attention heads, embedding dimension 768, and feed-forward network with embedding\ndimension 3072. For PULSE, we use PULSE-ECG/PULSE-7B model. For CAMEL, we set max new tokens =\n1000, temperature = 1.0, and top p = 0.95.\nWe divide PTB-XL classification task into superclass, subclass, rhythm, and form, following prior work (Wagner\net al., 2020). Similarly, for HEEDB classification, we divide into overall interpretation, conduction, ectopy,\nischmeia, rhythm, hypertrophy, and infarct sub-tasks. The ECG-QA dataset contains 3 types of single-ECG\nquestions and 4 types of multi-ECG questions which are sourced from PTB-XL and MIMIC-IV-ECG respectively.\nB\nAdditional Experimental Results\nB.1\nForecasting\nTable 8: Forecasting results (F1 [%]) on Icentia. The task is forecasting atrial fibrillation or flutter (abnormal) or\nnormal sinus rhythm at varying horizons h (s) and input-windows w (s). We report supervised training results for\nXGB and CNN, zero-shot results for GPT-5.2, PULSE, GEM, and CAMEL, and linear probing result for CAMEL\nProbe.\nh = 60\nh = 180\nMethod\n10\n30\n60\n120\n300\n600\n10\n30\n60\n120\n300\n600\nXGB\n59.55\n55.58\n56.89\n56.09\n59.64\n55.20\n56.85\n56.60\n58.18\n57.09\n58.07\n56.17\nCNN\n46.89\n54.62\n53.59\n50.79\n52.35\n53.10\n49.59\n54.72\n57.08\n54.40\n52.76\n50.21\nCAMEL Probe\n69.83\n72.52\n69.43\n74.30\n72.56\n75.14\n69.81\n67.38\n69.91\n69.85\n75.46\n74.64\nGPT-5.2\n43.26\n47.54\n48.00\n48.24\n54.36\n51.28\n37.67\n46.71\n51.22\n54.43\n49.29\n52.01\nPULSE\n43.21\n-\n-\n-\n-\n-\n40.50\n-\n-\n-\n-\n-\nGEM\n32.54\n-\n-\n-\n-\n-\n32.95\n-\n-\n-\n-\n-\nCAMEL\n63.28\n68.88\n69.23\n71.37\n73.42\n73.18\n60.04\n61.27\n65.44\n69.59\n71.34\n67.53\nh = 300\nh = 600\nMethod\n10\n30\n60\n120\n300\n600\n10\n30\n60\n120\n300\n600\nXGB\n54.08\n54.97\n54.35\n57.05\n56.86\n53.37\n57.93\n57.12\n53.84\n55.06\n56.03\n57.07\nCNN\n48.47\n53.13\n46.54\n46.32\n57.72\n48.99\n48.85\n51.96\n53.69\n51.32\n49.90\n56.19\nCAMEL Probe\n68.54\n71.05\n69.31\n70.30\n73.36\n72.70\n69.30\n69.53\n68.49\n72.04\n73.54\n76.02\nGPT-5.2\n43.14\n53.42\n52.07\n51.19\n58.16\n56.37\n46.11\n56.04\n47.55\n55.80\n50.01\n55.41\nPULSE\n36.67\n-\n-\n-\n-\n-\n40.12\n-\n-\n-\n-\n-\nGEM\n33.33\n-\n-\n-\n-\n-\n31.49\n-\n-\n-\n-\n-\nCAMEL\n58.07\n63.58\n70.15\n67.15\n70.73\n70.48\n58.54\n63.91\n64.28\n65.99\n71.37\n66.90\nB.2\nInput Classification\nWe report per-task classification results for 10-second 12-lead ECG signals in Table 9. Additionally, we compare\nthe performance of classifying AF across single lead rhythm strips ranging in duration from 10 seconds to 15\nminutes (Table 10) and using only a fixed selection of 6, 4, 3, 2, and 1 leads (Table 11).\n15\n"}, {"page": 16, "text": "Table 9: Full classification results (F1 [%]).\nMethod\nPTBXL\nSuper\nPTBXL\nSub\nPTBXL\nRhythm\nPTBXL\nForm\nCode15\nBinary\nCode15\nMulti\nMERL\n41.39\n12.36\n10.84\n12.31\n3.09\n36.35\nMELP\n40.45\n10.45\n11.43\n12.16\n20.50\n31.63\nPULSE\n73.47\n8.11\n18.07\n12.59\n62.28\n89.52\nGEM\n75.77\n8.01\n20.87\n14.64\n71.12\n90.53\nCAMEL\n67.75\n28.89\n39.29\n25.54\n69.45\n86.95\nMethod\nHEEDB\nBinary\nHEEDB\nConduction\nHEEDB\nEctopy\nHEEDB\nIschemia\nHEEDB\nRhythm\nHEEDB\nHypertrophy\nHEEDB\nInfarct\nMERL\n4.27\n18.05\n17.41\n17.21\n10.86\n20.67\n25.81\nMELP\n24.96\n16.56\n18.26\n16.97\n10.62\n20.42\n25.43\nPULSE\n86.36\n14.15\n3.33\n4.59\n5.74\n7.73\n8.44\nGEM\n88.95\n14.15\n3.48\n6.41\n5.97\n6.79\n10.05\nCAMEL\n96.41\n13.78\n3.61\n6.69\n10.05\n11.45\n4.21\nTable 10: AFIB classification results (F1 [%]) with varying ECG durations (s)\nMethod\n10\n30\n60\n120\n300\n600\n900\nCAMEL 46.19 41.32 40.45 45.10 58.28 64.95 64.15\nTable 11: Classification results (F1 [%]) with fewer leads at test time.\n(a) Leads I, II, III, aVR, aVL, and aVF.\nMethod\nPTBXL-Super\nPTBXL-Sub\nPTBXL-Rhythm\nPTBXL-Form\nCPSC-2018\nCSN\nCode15-Binary\nCode15-Multi\nMERL\n43.15\n12.48\n11.23\n12.95\n22.52\n4.87\n45.31\n33.14\nMELP\n41.35\n12.09\n10.81\n12.52\n20.90\n5.67\n41.57\n33.65\nPULSE\n48.48\n8.29\n16.47\n11.48\n54.71\n6.65\n71.34\n78.97\nGEM\n47.75\n8.96\n11.69\n12.03\n54.86\n6.67\n74.00\n83.67\nCAMEL\n60.79\n24.35\n41.89\n21.42\n55.46\n10.58\n69.48\n87.99\n(b) Leads I, II, III, and V2.\nMethod\nPTBXL-Super\nPTBXL-Sub\nPTBXL-Rhythm\nPTBXL-Form\nCPSC-2018\nCSN\nCode15-Binary\nCode15-Multi\nMERL\n40.60\n11.96\n10.96\n13.02\n24.16\n4.15\n33.54\n34.56\nMELP\n40.14\n10.87\n11.00\n12.91\n21.44\n4.46\n34.58\n32.48\nPULSE\n52.71\n6.82\n13.07\n11.91\n57.03\n9.05\n69.41\n83.64\nGEM\n44.60\n4.76\n11.08\n12.09\n50.96\n6.40\n72.02\n78.94\nCAMEL\n67.39\n19.15\n39.75\n23.30\n57.78\n11.33\n70.21\n89.15\n(c) Leads I, II, and III.\nMethod\nPTBXL-Super\nPTBXL-Sub\nPTBXL-Rhythm\nPTBXL-Form\nCPSC-2018\nCSN\nCode15-Binary\nCode15-Multi\nMERL\n40.47\n12.18\n10.91\n12.95\n24.18\n3.95\n33.46\n34.76\nMELP\n40.31\n11.28\n11.17\n12.05\n22.04\n5.10\n37.40\n34.34\nPULSE\n51.97\n6.35\n13.26\n14.39\n54.89\n8.71\n68.24\n84.14\nGEM\n43.18\n7.17\n8.74\n13.92\n50.26\n7.09\n72.24\n77.21\nCAMEL\n67.47\n18.85\n38.03\n25.00\n59.33\n10.28\n69.38\n89.48\n16\n"}, {"page": 17, "text": "(d) Leads I and II.\nMethod\nPTBXL-Super\nPTBXL-Sub\nPTBXL-Rhythm\nPTBXL-Form\nCPSC-2018\nCSN\nCode15-Binary\nCode15-Multi\nMERL\n44.24\n13.09\n11.20\n13.32\n23.81\n5.55\n46.39\n35.21\nMELP\n40.30\n11.33\n10.76\n13.31\n20.83\n5.01\n31.53\n33.23\nPULSE\n43.91\n5.32\n13.90\n11.23\n51.82\n4.25\n66.50\n72.63\nGEM\n43.71\n9.06\n5.65\n13.25\n43.99\n7.52\n71.22\n57.07\nCAMEL\n62.24\n17.80\n38.96\n20.80\n56.76\n11.93\n69.86\n88.02\n(e) Lead II.\nMethod\nPTBXL-Super\nPTBXL-Sub\nPTBXL-Rhythm\nPTBXL-Form\nCPSC-2018\nCSN\nCode15-Binary\nCode15-Multi\nMERL\n45.06\n12.98\n11.39\n12.71\n23.64\n4.66\n52.29\n35.49\nMELP\n42.09\n13.08\n10.88\n13.73\n21.06\n4.89\n46.39\n33.20\nPULSE\n38.77\n5.34\n12.05\n11.68\n41.65\n5.83\n67.59\n59.52\nGEM\n44.26\n6.59\n5.97\n10.33\n34.90\n8.48\n70.27\n50.06\nCAMEL\n52.79\n12.40\n35.31\n19.09\n53.54\n7.97\n69.25\n82.03\nB.3\nQuestion Answering\nTable 13: Full ECG-QA results (Accuracy [%]) on PTB-XL dataset.\nMethod\nSingle\nChoose\nSingle\nQuery\nSingle\nVerify\nMulti\nCQ\nMulti\nCV\nMulti\nIQ\nMulti\nIV\nPULSE\n82.22\n78.07\n76.91\n67.96\n53.75\n84.71\n51.21\nGEM\n83.67\n78.19\n80.54\n67.94\n57.36\n84.61\n53.64\nCAMEL\n70.69\n72.38\n75.89\n67.83\n56.85\n84.17\n68.06\nB.4\nReport Generation\nWe present results on evaluating reports using GPT-5.2 as a judge and NLP metrics in Table 14. We show examples\nof reports generated by CAMEL for datasets PTB-XL (Figure 4) and MIMIC-IV (Figure 5).\nTable 14: Full report generation result.\n(a) PTB-XL\nMethod\nLLM\nBLEU-1\nBLEU-4\nMETEOR\nROUGE\nBERT F1\nMELP\n†\n13.02\n1.87\n11.28\n18.50\n44.08\nPULSE\n18.40\n9.37\n1.44\n24.53\n9.91\n83.95\nGEM\n20.45\n6.05\n1.30\n23.70\n8.02\n83.47\nCAMEL\n19.45\n14.28\n1.52\n31.31\n10.64\n84.00\n(b) MIMIC-IV.\nMethod\nLLM\nBLEU-1\nBLEU-4\nMETEOR\nROUGE\nBERT F1\nPULSE\n40.43\n6.09\n0.93\n20.74\n8.02\n83.37\nGEM\n44.65\n50.81\n11.60\n35.06\n23.81\n87.78\nCAMEL\n62.59\n23.11\n3.30\n17.57\n15.56\n82.10\n17\n"}, {"page": 18, "text": "Prompt\nWrite a diagnostic report for this ECG waveform.\nResponse\nThe mean RR interval is 819 ms, resulting in a heart rate of 73 bpm (60,000 / 819). The PR interval is 227 ms and\nthe QRS duration is 113 ms. Diagnosis: Sinus rhythm with incomplete right bundle branch block.\nFigure 4: Example report generated by CAMEL for a sample from the PTB-XL dataset. The ground truth la-\nbel is Sinus rhythm.\nIncomplete right bundle branch block.\nPR interval is at the upper\nlimit.\nOtherwise, normal ECG.\nPrompt\nInterpret the provided ECG waveform, identify key features and abnormalities in each\nlead, and generate a clinical diagnosis that is supported by the observed evidence.\nResponse\nTo determine the heart rate, we calculate the mean of the RR intervals: (648 + 652 + 652 + 652 + 652 + 652 + 652\n+ 652 + 648 + 652 + 656 + 652 + 652 + 656 + 652) / 15 = 652 ms. The heart rate is 60,000 / 652 ≈92 bpm. The\nrhythm is sinus, with a PR interval of 188 ms and a P-wave duration of 152 ms. The QRS duration is significantly\nprolonged at 152 ms globally, with Lead V1 specifically showing a duration of 172 ms. The QRS axis is shifted to\nthe left. In the precordial leads, there is a notable loss of R-wave progression and deep negative deflections in V1\n(-0.72 mV), V2 (-1.00 mV), and V3 (-1.22 mV), which is indicative of an anteroseptal infarct of undetermined age.\nFurthermore, the T-wave amplitudes in the lateral leads are negative (V5: -0.07 mV, V6: -0.05 mV), suggesting\nlateral ischemia.\n| Feature | Value/Interpretation |\n| Heart Rate | 92 bpm (Sinus) |\n| PR Interval | 188 ms (Normal) |\n| QRS Duration | 152 ms (Widened) |\n| QRS Axis | Left Axis Deviation |\n| V1-V3 Morphology | Anteroseptal infarct (Undetermined age) |\n| Lateral T-waves | Inverted in V5-V6 (Ischemia) |\nDiagnosis: Sinus rhythm, Left axis deviation, Anteroseptal infarct - age undetermined, Lateral T wave changes\nmay be due to myocardial ischemia, Abnormal ECG.\nFigure 5: Example of a report generated by CAMEL for a sample from the MIMIC-IV dataset.\n†Unable to reproduce the result.\n18\n"}, {"page": 19, "text": "C\nTraining Dataset\nC.1\nStage 3\nFor Stage 3, we generate conversations that require understanding ECG statistics. We use the ECGDeli library\n(Pilia et al., 2021) to compute lead-specific and global statistics for each sample of the datasets from Stage 2.\nThese statistics include P-QRS-T amplitudes, beat-wise R-R intervals, QRS durations, PR-intervals, ST-segment\ndeviations, ST-slope, and global heart rate metrics.\nWith these statistics, we use a diverse set of question types to reflect real-world clinical workflows. Example\nquestion types include beat-wise statistics retrieval (e.g., verifying specific wave durations), temporal anomaly\ndetection (e.g, identifying premature beats), and comparative analysis (e.g., comparing feature across two ECGs).\nAnswer formats include multiple choice, short-answer, and step-by-step explanations. We provide several examples\nin Figure 6. We generate approximately 30 million question-answer pairs from ECG samples spanning the HEEDB,\nCode-15%, PTBXL, and MIMIC-IV datasets.\nFigure 6: Stage 3 conversation examples across the 5 task types.\nC.2\nStage 4\nThe goal of Stage 4 is to generate multi-turn conversational data that has either single-ECG analysis questions\nor comparison questions. These conversations use statistics learned in Stage 3 as a part of their reasoning. We\ngenerate approximately 420K samples with ECGs sourced from MIMIC, PTBXL, and Code15. To generate the\nfirst 20K of these conversations, we follow a similar approach as GEM (Lan et al., 2025). Specifically, we take the\ntask types and label formats from ECGInstruct (Liu et al., 2024b) and use gemini-3-flash-preview to generate\nreasoning (grounded in ECG statistics) along with the final answer. To generate the remaining 400K of these\nconversations, we again use gemini-3-flash-preview with the following system prompt (abridged):\nSystem Prompt\nYou are an expert cardiologist. Generate a realistic clinical dialogue interpreting an ECG according to the\nspecifications below. Always include a clinically supported diagnosis consistent with the report and provided\nstatistics.\nObjectives\n1. Realistic clinical reasoning: Reflect how a cardiologist visually analyzes an ECG and arrives at a\ndiagnosis.\n2. Evidence-based diagnosis: Support conclusions with ECG findings and statistics when applicable.\nDo not cite statistics that contradict the diagnosis.\n3. Logical flow: Present reasoning from basic observations to final diagnosis.\nDialogue constraints\n19\n"}, {"page": 20, "text": "• Knowledge asymmetry: The human can see the ECG image only and does not know the statistics or\ndiagnosis.\n• ECG-dependent: The conversation must require the ECG; avoid questions answerable from text\nalone.\n• Role discipline: The human asks questions; the gpt role ONLY answers and explains. GPT must\nNEVER ask questions.\n• Human limitations: The human must NOT describe ECG features, morphology, or measurements.\n• Multi-turn continuity: Each turn should build on prior discussion.\nClinical reasoning rules\n• Reasoning before conclusions: Each gpt turn must include clinical reasoning before any final\nanswer.\n• Numeric fidelity: All values must exactly match ground truth, including units and lead-specific\ndetails when relevant.\n• Stepwise computation: When interpreting metrics (e.g., heart rate, intervals), show individual\nmeasurements and math before the final value.\n• No patient history assumptions: Discuss only what is observable from the ECG.\n• No software references: Interpretation must be framed as human visual analysis only.\n• Avoid filler language; be direct and clinical.\nFormatting rules\n• Return ONLY a JSON list of turns using DOUBLE QUOTES.\n• Valid elements: {”from”:”human”,”value”:”...”} or {”from”:”gpt”,”value”:”...”}.\n• Do not refer to the assistant as “doctor” or “doc”.\n• If a specific output format (e.g., SOAP) is required, it must be explicitly requested by the human.\nGround truth inputs\nStats: {stats str}\nDiagnosis: {condition}\n{blueprint spec}\nGenerate the conversation now:\nIn order to promote greater diversity in the data, we sample from a set of blueprints, including 28 task types, 10\naudiences, 13 formats, and 13 dialogue arcs. We list several examples below:\nBlueprints\nTask Types\n1. Concise summary: provide a brief clinical summary of the ECG findings\n2. Structured report: generate a structured clinical report with specific fields\n3. QA probing: answer specific questions about ECG features\n4. Clinical explanation: explain the clinical significance of findings\n5. Teaching: teach ECG interpretation concepts using this example\nAudiences\n1. Clinician-to-clinician: attending physician to fellow/resident\n2. Clinician to patient: doctor explaining to a patient\n3. Teaching trainee: attending teaching a medical student or junior resident\n4. Paramedic handoff: paramedic to ED physician handoff\n5. Researcher note: research annotation or case documentation\n20\n"}, {"page": 21, "text": "Format\n1. Narrative: free-form narrative text\n2. Bullet list: bulleted list of findings\n3. Soap note: SOAP note structure\n4. Impression findings: findings then impression format\n5. Short answer rationale: evidence and rationale followed by a brief answer\nDialogue Arcs\n1. Direct response: human asks, GPT answers directly\n2. Context volunteered: human volunteers asks whether the diagnosis or explanation would change\nwith additional clinical context\n3. Challenge response: human challenges GPT’s interpretation\n4. Lay translation: human asks for simpler explanation\n5. Next steps: human asks about clinical next steps\nWe list several examples of conversations produced in Stage 4 below.\nConversation\nUser: How does this ECG’s rhythm compare to a typical normal baseline rhythm, and what is your diagnosis?\nAssistant: Reasoning: RR intervals (727, 531, 879, 926, 828, 484, 930, 703, 527, 859, 504, 562, 875 ms)\nshow extreme variability (std 168 ms) and absent P waves compared to a steady normal baseline.\nComparison: This rhythm is irregular with a heart rate of 84 bpm, unlike a normal P-wave-driven sinus\nrhythm. Diagnosis: Atrial Fibrillation.\nUser: Based on these findings, categorize the clinical priorities for the next shift into emergent, urgent, and\nroutine categories.\nAssistant: Reasoning: The rhythm is stable Atrial Fibrillation with a controlled rate (84 bpm) and a QRS\nduration of 115 ms.\n1. Emergent: None; monitor for rapid ventricular response or hypotension.\n2. Urgent: Cardiology consult for anticoagulation and rate/rhythm strategy.\n3. Routine: Continuous telemetry and daily 12-lead ECGs.\nConversation\nUser: I need a rapid interpretation of this ECG for an emergency department handoff. Please provide a\nnumbered reasoning walkthrough covering rate, rhythm, axis, and intervals. Use a focused findings and\nimpression format.\nAssistant: Reasoning:\n1. Rate: The individual R-R intervals are 762, 770, 590, 863, 773, 762, 770, 754, and 773 ms. The\naverage R-R interval is 757 ms. Calculating the heart rate: 60,000 / 757 = 79 beats per minute.\n2. Rhythm: The rhythm is sinus with a single Premature Atrial Contraction (PAC) identified by an\nearly beat at 590 ms and a compensatory pause of 863 ms.\n3. Axis/Morphology: The QRS shows a deep negative deflection in V1 (amplitude -4.85 mV) and a\nprominent positive R-wave in V6 (amplitude 4.61 mV).\n4. Intervals: The PR interval is 193 ms. The QRS duration is markedly prolonged at 160 ms (exceeding\nthe 120 ms threshold for a complete block).\n21\n"}, {"page": 22, "text": "5. Conclusion: The combination of a wide QRS and the specific morphology in V1 and V6 indicates a\ndelay in left ventricular depolarization.\nFindings:\n• Heart Rate: 79 bpm\n• Rhythm: Sinus with 1 PAC\n• PR Interval: 193 ms\n• QRS Duration: 160 ms\n• Lead V1 QRS Amplitude: -4.85 mV\n• Lead V6 QRS Amplitude: 4.61 mV\nImpression: Left Bundle Branch Block.\nUser: So the heart rate is about 80 beats per minute with one extra beat. Since the QRS is wide and the main\ndeflection is pointing down in V1, this confirms a Right Bundle Branch Block, correct?\nAssistant: Reasoning: In a Right Bundle Branch Block (RBBB), lead V1 typically shows a terminal upward\ndeflection (RSR’ pattern) because the final electrical forces move toward the right ventricle. In this ECG,\nlead V1 shows a deep, wide S-wave with a negative amplitude of -4.85 mV. The broad, positive R-wave in\nV6 (4.61 mV) and the wide QRS (160 ms) indicate that the impulse is traveling normally through the right\nbundle but is delayed reaching the left ventricle, which characterizes a left-sided conduction delay rather than\na right-sided one.\nFindings:\n• QRS morphology: Deep S-wave in V1, broad R-wave in V6\n• QRS duration: 160 ms\nImpression: Left Bundle Branch Block (Correction: This is not RBBB).\nUser: I understand now; the deep downward wave in V1 actually points to a left-sided delay. Can you\nprovide the specific measurements for the P-wave and confirm the final PR interval for the clinical report?\nAssistant: Reasoning: The P-wave duration is measured globally at 164 ms with an amplitude of 0.08 mV.\nSpecifically looking at Lead II, the P-wave duration is 113 ms with an amplitude of 0.12 mV. The PR interval\nis 193 ms, which is within the upper limits of normal (less than 200 ms). These findings, alongside the 160\nms QRS duration, confirm the diagnosis of a significant ventricular conduction delay without a primary AV\nblock.\nFindings:\n• Global P-duration: 164 ms\n• Lead II P-duration: 113 ms\n• PR interval: 193 ms\n• QRS duration: 160 ms\nImpression: Left Bundle Branch Block.\nC.3\nStage 5\nThe Stage 5 dataset contains 17K samples, with ECGs sourced from the Icentia11k dataset. Like Stage 4, we use\nECGDeli-computed statistics to generate conversations with gemini-3-flash-preview. However, in Stage 5\nwe focus on single-ECG, single-turn conversations (one question and one response). We first compile a rubric\nconsisting of the most clinically relevant features for forecasting AFIB and AFL, including PR interval prolongation\n(Smith et al., 2017; Cheng et al., 2009), heightened premature atrial contractions (Farinha et al., 2023), RR\nirregularity (Wesselius et al., 2021), and increased standard deviation of normal RR interval (SDNN) (Geurts et al.,\n2023). We verify these features with cardiologists to ensure clinical relevance. We use this rubric to craft a Gemini\n22\n"}, {"page": 23, "text": "prompt and select statistics to pass along with the prompt. We provide an abridged system prompt for generating\nStage 5 data below.\nSystem Prompt\nYou are an expert cardiologist specializing in arrhythmia risk prediction. Using ONLY the ECG statistics\nprovided, produce evidence-based clinical reasoning to forecast whether an AFib or AFlutter event will occur\nwithin the next {horizon seconds} seconds.\nInterpret the values as if you personally reviewed the ECG and measured them. Do not assume patient history,\ndo not diagnose the current rhythm, and do not reference software, models, or automated interpretation.\nTask\nGiven ECG-derived metrics (including beat-level sequences such as RR, PR, QRS, and ectopy\nmarkers), produce:\n1. A concise, evidence-grounded clinical reasoning narrative describing near-term rhythm stability.\n2. A forecast of AFib/AFlutter occurrence within {horizon seconds} seconds.\nCore rules\n• Use ONLY the provided statistics; do not invent values or features.\n• Cite AT MOST 3–4 relevant metrics. Discuss evidence BEFORE forming any opinion.\n• Ground every claim in specific metrics; reference individual beats explicitly when applicable\n(1-based).\n• Numeric fidelity is mandatory. Show math for any derived values.\n• Do not analyze metrics that contradict the ground-truth event.\n• No filler, hedging clich´es, patient history, or diagnostic statements.\nReasoning objectives (in order)\n1. Evidence extraction: Identify metrics relevant to short-horizon AFib/AFlutter risk (e.g., RR variabil-\nity, ectopy, atrial surrogates, conduction instability).\n2. Beat-referenced reasoning: Tie claims to specific beats when possible (e.g., “short RR at beat 7”).\n3. Mechanistic interpretation: Explain how observed findings plausibly precede AFib/AFlutter, using\nonly supported relationships.\n4. Forecast formation: Provide a final forecast only after reasoning.\nOutput requirements\n• Return ONLY a JSON list of chat turns using double quotes.\n• Include reasoning before the forecast.\n• End with exactly one of:\n– Forecast:\nNORM\n– Forecast:\nABNORMAL\n• If ABNORMAL, state whether AFib or AFlutter is more likely, or explicitly say uncertain.\nClinical phrasing constraints\n• Do NOT refer to inputs as arrays, indices, fields, JSON, or statistics.\n• Describe missing data clinically (e.g., “no premature atrial beats identified”).\n• If PAC count is zero or label is NORM, do NOT mention PACs.\n• Beat references must remain explicit and 1-based (e.g., “beat 5”).\nGround truth inputs\nECG STATISTICS: {stats str}\nGround-truth Event after {horizon seconds} seconds: {forecast label}\nGenerate the response now:\nWe list several examples of conversations produced in Stage 5 below.\n23\n"}, {"page": 24, "text": "Conversation\nHuman: Analyze the ECG signal and predict the cardiac rhythm for the next 60.0 seconds.\nNORM: Normal ECG\nABNORM: Atrial Fibrillation or Atrial Flutter\nOutput one of: NORM or ABNORM.\nAssistant: In reviewing the rhythm for short-term risk, I first note the significant irregularity indicated by a\nhigh RMSSD of 194 ms and an RR Interval IQR of 128.91 ms. Examining the beat-to-beat timing reveals a\nsevere shortening of the cycle length at beat 21, where the RR interval drops to 348 ms, compared to the mean\nRR interval of 793 ms. Furthermore, there are six premature atrial contractions identified in the segment,\nspecifically occurring at beat 22, beat 23, beat 27, beat 30, and beat 36. The presence of frequent atrial\nectopy, combined with a P-wave duration of 160.16 ms, suggests a high degree of atrial conduction delay\nand electrical instability. This pattern of frequent premature triggers on a background of high autonomic\nvariability is a common precursor for paroxysmal atrial fibrillation. Given the high RMSSD and frequent\nectopy, a transition into atrial fibrillation is likely within the next minute. Forecast: ABNORMAL\nConversation\nHuman: Analyze the ECG signal and predict the cardiac rhythm for the next 300.0 seconds.\nNORM: Normal ECG\nABNORM: Atrial Fibrillation or Atrial Flutter\nOutput one of: NORM or ABNORM.\nAssistant: In assessing this patient’s rhythm for potential near-term transition into an arrhythmia, I am\nevaluating the stability of conduction and heart rate variability over the next 300.0 seconds. First, I noticed\nthat the mean RR interval of 1030 ms, which correlates to a heart rate of 57.4 bpm, demonstrates a stable\nbaseline. This is evident in the beat-to-beat consistency, for example, beat 1 at 1066 ms and beat 2 at 1043\nms. Second, the PR interval remains consistent with a mean value of 156 ms, and specific measurements such\nas beat 1 and beat 4 showing 156 ms and 152 ms respectively, indicating well-maintained atrioventricular\nconduction without the variations often associated with atrial instability. Third, the RR interval IQR of\n71.29 ms, combined with an estimated atrial rate of 58, suggests that while there is physiological variability,\nthe rhythm lacks the chaotic irregularity or rapid atrial activity that would precede a paroxysm of atrial\nfibrillation. Clinically, the uniformity of the QRS duration at 105.47 ms further supports a stable ventricular\nresponse. Given the absence of progressive shortening in the RR intervals or erratic shifts in the PR timing,\nthe risk of a conversion to a tachyarrhythmia in the coming minutes is low. Forecast: NORM\n24\n"}]}