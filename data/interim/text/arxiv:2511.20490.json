{"doc_id": "arxiv:2511.20490", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.20490.pdf", "meta": {"doc_id": "arxiv:2511.20490", "source": "arxiv", "arxiv_id": "2511.20490", "title": "MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology", "authors": ["Kiril Vasilev", "Alexandre Misrahi", "Eeshaan Jain", "Phil F Cheng", "Petros Liakopoulos", "Olivier Michielin", "Michael Moor", "Charlotte Bunne"], "published": "2025-11-25T16:56:25Z", "updated": "2025-11-25T16:56:25Z", "summary": "Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.20490v1", "url_pdf": "https://arxiv.org/pdf/2511.20490.pdf", "meta_path": "data/raw/arxiv/meta/2511.20490.json", "sha256": "be981dd4e76144f1d64d6c35c8700afe450edbe60af0eaae4e6af93daa140813", "status": "ok", "fetched_at": "2026-02-18T02:26:17.199338+00:00"}, "pages": [{"page": 1, "text": "MTBBench: A Multimodal Sequential Clinical\nDecision-Making Benchmark in Oncology\nKiril Vasilev1∗\nAlexandre Misrahi2∗\nEeshaan Jain2∗†\nPhil Cheng3\nPetros Liakopoulos3\nOlivier Michielin3\nMichael Moor1‡\nCharlotte Bunne2‡\n1ETH Zürich\n2EPFL\n3HUG\ngithub.com/bunnelab/MTBBench\nhuggingface.co/datasets/EeshaanJain/MTBBench\nAbstract\nMultimodal Large Language Models (LLMs) hold promise for biomedical reason-\ning, but current benchmarks fail to capture the complexity of real-world clinical\nworkflows. Existing evaluations primarily assess unimodal, decontextualized\nquestion-answering, overlooking multi-agent decision-making environments such\nas Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in\noncology, where diagnostic and prognostic tasks require integrating heterogeneous\ndata and evolving insights over time. Current benchmarks lack this longitudinal\nand multimodal complexity. We introduce MTBBench, an agentic benchmark\nsimulating MTB-style decision-making through clinically challenging, multimodal,\nand longitudinal oncology questions. Ground truth annotations are validated by\nclinicians via a co-developed app, ensuring clinical relevance. We benchmark\nmultiple open and closed-source LLMs and show that, even at scale, they lack\nreliability—frequently hallucinating, struggling with reasoning from time-resolved\ndata, and failing to reconcile conflicting evidence or different modalities. To ad-\ndress these limitations, MTBBench goes beyond benchmarking by providing an\nagentic framework with foundation model-based tools that enhance multi-modal\nand longitudinal reasoning, leading to task-level performance gains of up to 9.0%\nand 11.2%, respectively. Overall, MTBBench offers a challenging and realistic\ntestbed for advancing multimodal LLM reasoning, reliability, and tool-use with a\nfocus on MTB environments in precision oncology.\n1\nIntroduction\nRecent advances in large multi-modal and language models have opened the door to general-purpose\nclinical agents capable of reasoning across diverse biomedical tasks (Moor et al., 2023). Vision-\nlanguage models can describe pathology images (Lu et al., 2024a; Dai et al., 2025; Lu et al., 2024b),\nLLMs can summarize clinical notes (Choudhuri et al., 2025; Yang et al., 2024), and medical agents are\nincreasingly able to query tools, retrieve knowledge, and even hold multi-turn clinical conversations\n(Schmidgall et al., 2024; Wang et al., 2025a). These developments have prompted growing interest\nin using agents to support complex workflows (Wang et al., 2024a, 2025b; Gao et al., 2024; Lee\net al., 2024; Yue et al., 2024; Fallahpour et al., 2025) like those seen in molecular tumor boards\n(MTBs) (Tsimberidou et al., 2023), where oncologists, radiologists, pathologists, and geneticists\njointly analyze a patient’s evolving case (Fig. 4).\n∗These authors contributed equally.\n†Correspondence to: eeshaan.jain@epfl.ch\n‡Co-last authorship.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\narXiv:2511.20490v1  [cs.LG]  25 Nov 2025\n"}, {"page": 2, "text": "a.\npatient\ndigital pathology\nradiology\nhematology\ngenomics\nIHC\nH&E\netc.\nmulti-modal \npatient data\nlongitudinal  \npatient \nhistories\nSequencing  \nTreatment \nDiagnosis \nLabtests \nPathology\ntime\nmolecular \ntumor \nboard\nincl. oncologists, \ngeneticists, \npathologists, etc.\nb.\nknowledge  \ndatabases\nbioinformatics \ntools\nresources \nand tools\nexpert  \nconsensus \ncriteria\nongoing \nclinical trials \ndatabase\nfoundation \nmodels\nVision \nLMs\nmulti-modal \nlongitudinal  \nbenchmark\nIs there evidence of perineural \ninvasion in the primary tumor site?\nWhat can be concluded about T-cell \n(CD3+) infiltration in the tumor center?\nWill the patient’s cancer recur within \ntwo years, given all available data?\nTRUE\nFALSE\nA\nB\nC\nD\nA\nB\nC\nD\nE\nMTBBench\nFM\n… for digital pathology \n… incl. H&E and IHC\n… for reports, literature, \n… etc.\ndown- \nstream tasks\nFigure 1: The MTBBench benchmark and agent framework. a. MTBBench simulates molecular tumor\nboard workflows, presenting agents with longitudinal, multi-modal patient data (H&E, IHC, hematology, and\ngenomics) along with temporally distributed clinical events. Agents are tasked with integrating this information\nto support complex decision-making. b. MTBBench allows benchmarking agents on their ability to reason\nacross modalities and time in order to accurately tackle clinical questions concerning diagnosis, prognosis, and\nbiomarker interpretation. Lastly, we introduce an agentic framework that enables querying both external tools\nand pretrained foundation models, allowing agents to more effectively reason over complex, multi-modal and\ntemporally resolved clinical information.\nHowever, the evaluation of such agents remains underdeveloped. Existing benchmarks typically\nframe tasks as static, uni-modal, single-turn question-answering problems, where the model is given\nall necessary inputs at once and evaluated on its ability to predict a discrete answer. This setup\ndiverges sharply from how clinical decisions are made in practice. Real-world oncology reasoning\nis interactive, temporal, and multi-modal: physicians accumulate information over time, integrate\nfindings from multiple data types (e.g., hematoxylin and eosin (H&E) staining, immunohistochemistry\n(IHC) staining, radiology, blood values, genomics), and make provisional decisions that are updated\nas new evidence emerges (Fig. 1a). To be useful in these settings, AI agents must not only understand\neach modality, but also query, contextualize, and reconcile information across modalities and time—\ncapabilities rarely assessed in current evaluations.\nRecent works such as MedAgentBench (Jiang et al., 2025), MediQ (Li et al., 2024), and MedJourney\n(Wu et al., 2024) take steps toward interactive or longitudinal evaluation, but typically in limited or\nuni-modal contexts (e.g., textual EHRs) (Kweon et al., 2024) (Table 1). Likewise, emerging studies on\nmulti-modal agents demonstrate strong promise but lack standardized evaluation across longitudinal\npatient trajectories (Li et al., 2024). Most importantly, these agents are not tested under the cognitive\ndemands of tasks that mirror MTB decision-making: questions involving partial data, sequential\nupdates, conflicting information, highly heterogeneous and different modalities, and high-stakes\noutcomes.\nTo address this gap, we introduce MTBBench, an oncology-focused benchmark for evaluating AI\nagents in longitudinal, multi-modal clinical reasoning. Inspired by the structure and decision flow of\nreal molecular tumor boards, MTBBench simulates patient case reviews where agents must process\nheterogeneous patient data across time—including pathology slides, lab data, pathological, surgical\nand genomic reports—and answer clinically meaningful questions at each step. Questions span\ndiverse task types, including diagnostic classification, spatial biomarker interpretation, and outcome,\nprogression, or recurrence prediction (Fig. 1b). Importantly, the benchmark is validated by clinicians\nusing a custom-built expert annotation platform (Fig. 5, for details see Appendix C.1), ensuring both\nthe clinical plausibility of the data and the correctness of model evaluation.\nBeyond benchmark construction, we also introduce a modular agentic framework designed to interface\nwith different tools as well as pretrained foundation models (Fig. 1b). These include models trained\n2\n"}, {"page": 3, "text": "Table 1: Comparison of MTBBench with existing clinical and biomedical benchmarks.\nBenchmark\nMulti-Modal\nLongitudinal\nInteractive / Multi-Agent\nClinician-Annotated\nDomain\nMC-BEC (Chen et al., 2023)\n✓\n✓\n✗\n✗\nEmergency Medicine\nAsclepius (Wang et al., 2024b)\n✓\n✗\n✗\n✗\nGeneral Medicine\nMedJourney (Wu et al., 2024)\n✗\n✓\n✗\n✗\nGeneral Medicine\nEHRNoteQA (Kweon et al., 2024)\n✗\n✓\n✗\n✓\nGeneral Medicine\nMediQ (Li et al., 2024)\n✗\n✗\n✓\n✗\nGeneral Medicine\nClinicBench (Liu et al., 2024)\n✗\n✓\n✗\n✗\nGeneral Medicine\nHEST-1k (Jaume et al., 2024)\n✓\n✗\n✗\n✓\nPathology\nBixBench (Mitchener et al., 2025)\n✗\n✗\n✗\n✗\nBioinformatics\nMTBBench (ours)\n✓\n✓\n✓\n✓\nPrecision Oncology\non large-scale digital pathology datasets, reports, literature, and other domain-specific modalities.\nAgents can query these foundation models as part of their reasoning process—invoking them when\nneeded to interpret image regions, extract genomic signatures, or cross-reference trial data—thus\nmirroring how expert clinicians rely on specialized resources in practice. This framework enables\nflexible, multi-step decision-making and substantially enhances the agent’s ability to synthesize\ninformation across modalities and time.\nConcretely, our main contributions are:\ni.\nA benchmark for longitudinal, multi-modal clinical reasoning. MTBBench simulates MTB-\nstyle decision-making with temporally evolving patient data across modalities—H&E, IHC,\nhematology, and genomics—and includes complex, expert-curated questions reflecting real\ntumor board workflows.\nii.\nSystematic evaluation of vision-language models. We benchmark diverse open and closed-\nsource models and find that performance improves with exposure to more modalities, emphasiz-\ning the importance of integrated multi-modal context.\niii.\nAn agent framework for tool and foundation model integration. Our agent framework\nenables dynamic access to external tools and pretrained modality-specific foundation models,\nsignificantly boosting reasoning accuracy across tasks.\niv.\nExpert-validated data and reproducible tools. We release curated benchmark data, expert-\nreviewed annotations, agent logs, and tools to support rigorous and reproducible evaluation of\nclinical agents.\n2\nMTBBench: A Multimodal Sequential Clinical Decision-Making\nBenchmark in Oncology\n2.1\nMotivation and Positioning\nLLMs have shown increasing promise across medical domains, but current benchmarks remain\ndisconnected from the realities of clinical workflows. Ferber et al. (2024) introduce an agent for\nclinical decision making, however it is limited to general tool frameworks and to single-image inputs.\nOthers evaluate unimodal, static question-answering tasks without requiring the model to gather\ninformation, reconcile conflicting inputs, or reason over time (Li et al., 2025). In contrast, real clinical\ndecision-making—especially in molecular tumor boards—is inherently multimodal, interactive, and\nlongitudinal.\nMTBBench is a benchmark designed to close this gap. It evaluates how well AI agents can simulate\nan MTB-style setting, where patient cases evolve across time and require integration of imaging, lab,\npathology, genomic, and textual information. MTBBench captures three essential dimensions lacking\nin prior work:\nMultimodality: Clinical data spans digital pathology (H&E, IHC), hematology, radiology, and\ngenomics.\nLongitudinality: Patient histories unfold over multiple timepoints, with temporally ordered\nupdates.\nAgent workflow: Models must actively request relevant information, access tools, and answer in\nmulti-turn settings.\nAs shown in Table 1, MTBBench is the first benchmark to jointly address these three dimensions. All\nquestions are expert-validated through a co-developed application, which we further describe below.\n3\n"}, {"page": 4, "text": "Together, these components establish MTBBench as a framework for evaluating the capabilities of\nAI agents in clinical settings that require multi-modal and longitudinal understanding. For further\nrelated works, see Appendix B.\n2.2\nA Benchmark for Molecular Tumor Boards\nMTBBench comprises both multiple-choice and true/false questions designed to be answered within\na clinically grounded, agent-based framework. Questions span multimodal and longitudinal patient\ndata, requiring the agent to retrieve, interpret, and reason over diverse evidence sources under realistic\nconstraints that mirror the sequential and evolving nature of clinical decision-making.\nCompanion application for expert validation. To support expert review of the benchmark, we\ndeveloped a web-based interface allowing clinicians to inspect the clinical context, browse pathology\nand IHC images, and annotate feedback for each question. The interface presents structured case\ndescriptions alongside image thumbnails of different modalities that open full-resolution slides on\ndemand (Fig. 5). Clinicians can view grouped images by region and marker (e.g., CD3, CD163) and\nprovide detailed assessments directly linked to individual Q&A items. This tool enabled efficient,\nstructured validation of questions and answers by domain experts.\n2.2.1\nMTBBench-Multimodal\nDataset. We curated a subset of 26 patient cases from the HANCOCK dataset (CC BY 4.0) (Dörrich\net al., 2024), a multimodal repository of head and neck cancer patients that includes demographic,\npathological, hematological, surgical, and histological data. For each selected patient, an average\nof 40 modality-specific files are available, including 1.2 H&E slides, 26.2 IHC images, and one\nhematology report. Among these cases, 32 include a primary tumor H&E slide, and 17 contain at\nleast one lymph node slide. Notably, 27 of the primary tumor slides are accompanied by annotated\nregions of interest.\nQ&A design. We generate 390 multimodal question-answer pairs (15 per patient) using GPT-4o,\nwith select questions verified through expert-in-the-loop review through the companion app (Fig. 5,\nAppendix C.1). The questions span a range of modalities and clinical reasoning tasks: 2 H&E-based,\n4 IHC-based, 3 joint H&E+IHC, 4 hematology-based, and one question each related to clinical\noutcome and cancer recurrence.\nTasks. The multi-modal track unfolds in three clinically grounded stages. The first focuses on\npathological image interpretation (e.g., of H&E, IHC images): agents must infer histologic sub-\ntypes and assess spatial patterns of immune infiltration (e.g., CD3+, CD8+ T cells, CD68+, CD163+\nmacrophages) across tumor subregions such as the invasion front and tumor center. Intermediate\nquestions probe immune correlates of pathology, such as whether lymphovascular or perineural\ninvasion associates with distinct immune profiles. The second stage evaluates hematologic reasoning\nin a preoperative context—models analyze lab parameters (e.g., CRP, MPV, leukocyte subtypes,\ncreatinine) to infer infection risk, bleeding tendency, renal impairment, and thromboembolic predispo-\nsition. Finally, in the post-surgical stage, agents must integrate pathology and lab findings to predict\nhigh-level outcomes, including 5-year survival and 2-year recurrence, simulating tumor board-style\nprognostic deliberation.\n2.2.2\nMTBBench-Longitudinal\nDataset. We curated a subset of 40 patient cases from the MSK-CHORD dataset (CC BY-NC-ND\n4.0) (Jee et al., 2024), a clinicogenomic resource of cancer patients linking genomic profiles with\nstructured clinical timelines, each with an average of five associated files, including copy-number\nalterations, somatic mutations, specimen pathology reports, and clinical timelines. The timelines\ncapture key clinical events such as diagnostic procedures and treatment transitions, and are segmented\ninto decision-relevant timepoints to support temporally grounded evaluation.\nQ&A design. We manually construct 183 question-answer pairs (i.e., on average 4.6 questions\nper patient), with clinical feedback, targeting outcome prediction, recurrence risk, and treatment\nprogression across clinically actionable stages.\n4\n"}, {"page": 5, "text": "Tasks. The longitudinal track challenges agents to reason over temporally structured patient data\nsegmented into decision-relevant timepoints. Initial questions assess diagnosis and disease trajectory,\nfollowed by outcome prediction (e.g., survival), recurrence forecasting, and treatment progression\nmapping. Genomic data—such as somatic mutations and copy—number alterations—are introduced\nat key stages, enabling reasoning about resistance patterns or post-treatment stratification. Agents\nmust align treatment regimens with outcomes and integrate evolving context (e.g., updated timelines,\nnew genomic tests, surgical pathology) to justify predictions. This setup mirrors the longitudinal de-\nliberations of MTBs, where clinicians revise hypotheses in light of new events and cumulative history.\n2.3\nAgent System\nCurrent LLM-based systems struggle to reason across multiple modalities and timepoints (Hager\net al., 2024; AlSaad et al., 2024)—a critical requirement in real-world clinical decision-making. In\nparticular, tasks encountered in molecular tumor boards involve dynamic access to evolving patient\ndata, integration of heterogeneous sources such as pathology, lab tests, and genomics, and the ability\nto contextualize findings over time. Static, single-shot prompting falls short in such settings.\nTo overcome these limitations, MTBBench implements an agentic framework that enables interactive,\nmulti-turn decision-making. Agents must actively select which files to access, manage evolving\nmemory across turns. A key novelty of our setup is the integration of domain-specific foundation\nmodels (FMs) as callable tools besides structured biomedical resources used as tool (e.g., PubMed,\nDrugBank). These models—trained on large corpora of pathology slides, IHC images, or clinical\ntexts—offer rich, pretrained representations that complement the LLM’s general capabilities. Rather\nthan evaluating FMs in isolation, MTBBench enables agents to selectively invoke them as part of\na decision-making process, simulating how clinicians consult expert resources. This design of an\nagentic framework reflects how expert clinicians reason iteratively and selectively, and allows us\nto benchmark not only factual accuracy but also the agent’s ability to gather and use evidence in a\nrealistic clinical workflow.\nAgentic workflow. In MTBBench, the agent engages in a multi-turn decision-making process over a\ntemporally evolving patient trajectory. At each turn t, the agent receives a clinical query qt along with\naccess to a set of modality-specific files Ft = {f 1\nt , f 2\nt , . . . , f k\nt }, which may include digital pathology\nimages, lab results, clinical notes, or structured genomic and temporal data. The agent may issue a\nrequest Rt ⊆Ft to retrieve any subset of these files, which remain accessible only within the current\nturn. Namely, they do not persist across turns. However, any file from Ft may be re-requested at\na future turn t′ > t, simulating realistic constraints in clinical workflows where information must\nbe actively re-accessed. The agent’s internal memory consists of its reasoning history ht and a\nrecord of previously accessed files R≤t, forming the basis for answering downstream queries. In\nthe longitudinal track, clinical context is further enriched by an evolving timeline Tt = St\ni=1 τi,\nincrementally summarizing patient history. This setup enforces non-persistent access patterns while\nencouraging deliberate information gathering and reasoning over temporally non-stationary data. An\nextensive overview of this workflow is provided in Appendix D.1.\nOverview of models. We select a wide range of models with varying sizes. For the multimodal part\nof our benchmark, we evaluate the vision-text models (including some models with reasoning ca-\npabilities): gemma-3-12b, gemma-3-27b, gpt4o, o4-mini (reasoning), internvl3-38b, internvl3-78b,\nllama90b, mistralsmall, qwen25-7b, and qwen25-32b. For the longitudinal part, we evaluate a\nmix of text-only and vision-text models: gemma-3-12b, gemma-3-27b, gpt4o, o4-mini (reasoning),\nllama31-8b, llama33-70b, qwen3-8b (reasoning), and qwen3-32b (reasoning).\n2.3.1\nFoundation Model-based Tools\nWhile large language models excel at reasoning over textual inputs, they exhibit well-known limi-\ntations in visual understanding—especially when interpreting high-resolution biomedical imagery\nsuch as histopathology slides (Lu et al., 2024c). In clinical contexts like MTBs, however, the ability\nto analyze and contextualize pathology images is essential. At the same time, recent advances in\nvision-language foundation models have shown that pretrained models trained on large-scale medical\nimaging corpora can capture powerful, domain-specific visual representations (Vaidya et al., 2025a).\nTo harness these capabilities, MTBBench integrates foundation models as external tools (taking\ninspiration from (Schick et al., 2023; Yao et al., 2023)) that LLM agents can call on-demand. These\n5\n"}, {"page": 6, "text": "Table 2: Mean accuracy and 95% confidence intervals of various LLMs by task, estimated via bootstrap\nresampling. Each cell reports the model’s mean accuracy, with confidence intervals computed by resampling\n(with replacement) 1,000 times from the set of patient–question pairs within each task.\nMulti-Modal Analysis\nDigital Pathology\nHematology\nOutcome and Recurrence\nOverall\ngemma-3-12b\n55.9 ± 6.4\n74.9 ± 8.7\n53.6 ± 13.5\n61.5 ± 10.1\ngemma-3-27b\n51.8 ± 6.4\n76.9 ± 8.2\n42.1 ± 13.5\n56.9 ± 16.5\ngpt4o\n63.2 ± 6.0\n76.9 ± 7.7\n59.9 ± 13.5\n66.7 ± 8.1\no4-mini\n59.5 ± 6.4\n77.8 ± 8.2\n55.7 ± 14.4\n64.3 ± 10.5\ninternvl3-38b\n54.7 ± 6.4\n79.7 ± 8.2\n55.9 ± 13.5\n63.5 ± 11.9\ninternvl3-78b\n62.0 ± 6.4\n79.7 ± 7.7\n65.6 ± 11.5\n69.1 ± 8.4\nllama90b\n54.6 ± 6.2\n82.8 ± 7.2\n51.7 ± 13.5\n63.0 ± 14.8\nmistralsmall\n62.4 ± 6.2\n75.8 ± 8.7\n51.7 ± 13.5\n63.3 ± 11.5\nqwen25-7b\n42.3 ± 6.2\n61.1 ± 9.1\n53.9 ± 12.5\n52.4 ± 9.0\nqwen25-32b\n53.3 ± 6.2\n73.0 ± 8.7\n63.6 ± 12.5\n63.3 ± 9.3\nLongitudinal Analysis\nOutcome\nProgression\nRecurrence\nOverall\ngemma-3-12b\n63.3 ± 11.3\n55.9 ± 13.2\n54.6 ± 12.8\n58.0 ± 4.1\ngemma-3-27b\n57.7 ± 11.3\n50.7 ± 14.0\n47.4 ± 13.6\n51.9 ± 4.9\ngpt4o\n72.9 ± 10.6\n64.8 ± 13.2\n54.8 ± 13.6\n64.2 ± 8.6\no4-mini\n66.0 ± 10.6\n63.1 ± 12.3\n51.1 ± 13.6\n60.0 ± 7.1\nllama31-8b\n60.4 ± 11.3\n49.0 ± 12.3\n45.5 ± 13.6\n51.6 ± 7.1\nllama33-70b\n73.2 ± 9.9\n68.2 ± 13.2\n56.7 ± 13.6\n66.0 ± 7.8\nqwen3-8b\n63.1 ± 11.3\n57.6 ± 13.2\n47.4 ± 12.7\n56.0 ± 7.5\nqwen3-32b\n83.0 ± 9.2\n63.3 ± 12.3\n54.6 ± 13.6\n67.0 ± 13.5\nmodels are not used passively; instead, agents learn to query them selectively as part of a broader rea-\nsoning process. This setup reflects real-world clinical workflows, where specialists consult diagnostic\nsystems or reference image atlases to refine decisions. By exposing FMs as callable components,\nMTBBench enables systematic evaluation of how agents can leverage visual expertise to complement\ntheir textual reasoning.\n2.3.2\nDigital Pathology Foundation Models\nFor H&E images, we integrate CONCH (Lu et al., 2024d), a vision-language model pretrained on\nover 1.17 million H&E image–caption pairs. CONCH generates dual visual and textual embeddings,\nallowing image–text similarity computations. We expose this capability to the LLM by framing it as\na tool: given an image and a list of candidate textual descriptors, the tool returns the one with highest\nembedding similarity to the image, based on dot product in the shared representation space.\nFor IHC images, we develop a custom tool that combines foundation model embeddings with weakly\nsupervised learning to support quantification of marker-specific staining. Tissue regions are segmented\nand tiled into fixed-size patches (256 × 256), each embedded using the UNI2 foundation model\n(Chen et al., 2024a) to produce 1536-dimensional representations. These embeddings are aggregated\nusing an attention-based multiple instance learning (ABMIL) (Ilse et al., 2018) model trained to\nregress the percentage of positively stained cells. The ABMIL model is trained on a manually curated\ndataset of IHC images annotated via QuPath (see Appendix E.1), providing marker-level supervision\nwithout requiring single-cell labels.\n2.3.3\nAnalysis and Knowledge Database Tools\nTo support reasoning over temporal sequences of clinical events, we introduce two tools that provide\nexternal biomedical knowledge for answering longitudinal questions more accurately: a literature\nsearch tool and a pharmacological knowledge base.\nPubMed. The tool enables the LLM to issue natural language queries to retrieve biomedical literature\nrelevant to a patient’s clinical trajectory. The LLM issues natural language queries, which are used\nto retrieve the top 30 PubMed articles. These are reranked using the BAAI-bge-reranker-v2-m3\nmodel (Li et al., 2023), and the top 3 abstracts are returned to the LLM, supporting evidence-grounded\nreasoning for questions involving treatment effectiveness, sequencing, or disease progression.\n6\n"}, {"page": 7, "text": "with tools\nMulti-Modal  \nAnalysis\nTask: Digital Pathology\na.\nc.\nb.\nTask: Hematology\nTask: Outcome and Recurrence\nTask: Recurrence\nd.\nf.\ne.\nTask: Progression\nTask: Outcome\nLongitudinal  \nAnalysis\nFigure 2: Accuracy vs. average number of files accessed per question. Analyzed across tasks for multi-modal\nunderstanding (a–c) and longitudinal reasoning (d–f). Each point represents a model evaluated on a specific\ntask across all patients. Dots indicate model sizes (gpt-4o’s size has been reduced for visibility). Higher file\naccess generally correlates with increased accuracy, highlighting the importance of cross-modality and temporal\nintegration for performance.\nDrugBank. To augment drug-related knowledge, we integrate information from DrugBank (Wishart,\n2006; Knox et al., 2023). When processing a patient’s clinical timeline, drug mentions are automati-\ncally linked to corresponding DrugBank entries. Relevant metadata such as therapeutic indications,\nmechanisms of action, and known drug interactions is incorporated into the model’s context. This\nenrichment enables the language model to reason about treatment sequences with greater specificity,\nespecially in scenarios involving therapeutic decision making and longitudinal disease management.\n3\nEmpirical Evaluation\nWe evaluate models on both the multi-modal and longitudinal tracks of MTBBench under two\nconditions: (i) baseline inference without tool support and (ii) augmented inference with access\nto domain-specific tools, across several metrics, including their accuracy, analysis on multi-modal\nunderstanding and ability to reason across temporally-resolved data. Each model acts as an agent\ninteracting with the benchmark via multi-turn dialogues, selectively retrieving and reasoning over\navailable patient files to answer clinical questions. Models are provided only with patient metadata\nand a list of modality-specific files at each turn. They must request specific files and construct their\nanswers from retrieved content.\n3.1\nResults on MTBBench without Tools\nSettings. We evaluate a diverse set of LLMs and VLMs across all tasks in both the multimodal\nand longitudinal tracks of MTBBench. To simulate realistic tumor board conditions, we adjust the\ncontext and available files for each question type, ensuring that only data plausibly accessible at the\ncorresponding clinical stage is provided. No tools or external resources are available in this setting.\nModels receive only demographic details, pathology reports, imaging references, and structured\nclinical information. The core task remains multiple-choice question answering, but success requires\nmultimodal reasoning, data retrieval, and longitudinal inference rather than simple pattern recognition.\nEvaluation metrics. We report mean accuracy per model and task. To quantify uncertainty, we\nestimate 95% confidence intervals using bootstrap resampling with 1000 iterations. For each task, we\nsample with replacement from the set of question outcomes per model, compute the mean accuracy\nper sample, and extract the 2.5th and 97.5th percentiles to define the confidence interval. To assess\n7\n"}, {"page": 8, "text": "their ability to incorporate findings from several modalities, we analyze the number of modalities\nqueried compared to the resulting achieved accuracy. The analysis is conducted across 26 patients for\nMTBBench-Multimodal and 40 for MTBBench-Longitudinal. For details, see Appendix E.\nResults for MTBBench-Multimodal. Accuracy across all models (of different parameter sizes) are\ndisplayed in Table 2. Model performance varies substantially across the multimodal tasks. Digital\npathology, despite being visually complex, does not show a consistent benefit from model size—for\ninstance, gemma-3-12b outperforms its larger counterpart gemma-3-27b. Hematology emerges as\nthe most approachable task, likely due to its structured and interpretable inputs. In contrast, outcome\nand recurrence prediction remain the most difficult, with accuracies near random (50%), even for\nleading models. The strongest overall performance is achieved by internvl3-78b at 69.1% accuracy,\noutperforming closed-source baselines like gpt4o by 2.5%. Nevertheless, large performance gaps\npersist: up to 36.7% in digital pathology, 17.2% in hematology, and 34.6% in outcome and recurrence\nprediction—highlighting the need for more robust multimodal reasoning.\nInstead of model size, a stronger signal emerges in the relationship between performance and the\nnumber of files accessed (Fig. 2a-c), in both MTBBench-Multimodal and MTBBench-Longitudinal.\nThis suggests that effective information gathering, rather than raw scale, is a key determinant of\naccuracy. This is also demonstrated in Example 1: Compared to qwen25-7b, gpt4o accesses more\nmodalities including higher resolution H&E regions-of-interest, resulting in the correct cancer\nsubtype identification. In Example 2, mistralsmall requests file access to more IHCs and the H&E\nslide compared to gemma-3-27b, resulting in correct cancer subtype identification. However, this\ntrend does not hold for outcome and recurrence tasks, where high error rates persist across models. We\nhypothesize that these questions require contextual grounding and biomarker interpretation beyond the\ncurrent capabilities of uni-modal or tool-free agents. For further results, see Appendix G and Figs. 6).\nResults for MTBBench-Longitudinal. The longitudinal track reveals persistent weaknesses in\nbaseline LLMs. While outcome prediction shows some promise—qwen3-32b reaches 83.1% accu-\nracy—recurrence and progression tasks remain near chance (Table 2). Similarly, querying multiple\nmodalities improves model performance (Fig. 2d-f). This is also demonstrated in Example 3: com-\npared to gemma-3-27b, qwen3-32b re-accesses part of the patient timeline of events as well as\npathological data, resulting in better cancer progression prediction. This disparity suggests models\ncan detect coarse survival signals but struggle with more nuanced temporal reasoning, reflecting\nvaried evidence complexity across tasks.\n3.2\nResults on MTBBench with Tools\nSettings. In this setup, we augment the baseline LLMs with access to external tools, including\nfoundation model-based modules (UNI, Chen et al. (2024a) and CONCH, Lu et al. (2024b)) and\nclassical biomedical resources (PubMed and DrugBank). These tools are accessible via API-style\ninterfaces, allowing agents to retrieve structured outputs during multi-turn reasoning. Visual tools are\nprimarily used in multimodal tasks, while longitudinal tools support reasoning over treatment history,\ndrug interactions, and literature-based evidence.\nEvaluation metrics. We use the same accuracy metrics described in Section 3.1. Results are reported\nper task and model, and improvements are measured relative to tool-free baselines.\nResults for MTBBench-Multimodal. As shown in Figure 3a–c (exact numbers in Fig. 8), access to\nvisual foundation model tools significantly improves performance on all multimodal tasks. Digital\npathology, in particular, benefits from integration of the FMs, with models like gemma-3-12b\nand qwen25-7b showing improvements of up to 9%. Notably, tool augmentation also improves\nperformance on hematology tasks, despite the absence of dedicated tools for lab analysis. We\nattribute this to better contextual grounding: agents are more effective at integrating diverse file types\nwhen given richer information from related modalities. Outcome and recurrence tasks, which showed\nlow baseline performance, also benefit modestly from the improved visual reasoning capabilities.\nThis behavior is also demonstrated through an example: in Example 4, mistralsmall + TOOLS in\ncontrast to mistralsmall without tools accesses a digital pathology FM for IHC analysis. Despite both\nmodels accessing the same number of modalities, mistralsmall + TOOLS is able to properly address\nthe question, i.e., which marker is critical in a given pathological analysis.\n8\n"}, {"page": 9, "text": "Task: Digital Pathology\na.\nc.\nb.\nTask: Hematology\nTask: Outcome and Recurrence\nMulti-Modal  \nAnalysis\nwith tools\nwith tools\nTask: Recurrence\nd.\nf.\ne.\nTask: Progression\nTask: Outcome\nLongitudinal  \nAnalysis\nwith tools\nLongitudinal  \nAnalysis\nFigure 3: Accuracy across models and tasks for naive and tool-augmented agents. For multi-modal (a.–c.)\nand longitudinal (d.–f.) evaluation. Models equipped with tool access (hatched bars) generally show improved\naccuracy, highlighting the benefit of querying external resources in both multi-modal and temporal settings.\nResults for MTBBench-Longitudinal. Figures 3d–f show that tool access improves performance\nacross most longitudinal tasks, though gains are generally modest. This is expected, as no specialized\nfoundation model currently exists for longitudinal clinical reasoning—a key limitation in this track.\nInstead, agents rely on general-purpose tools like DrugBank and PubMed, which still provide some\nbenefit by enriching context and supporting evidence-based decisions. For example, progression and\nrecurrence predictions improve by over 5% in selected models. Outcome prediction—which already\nexhibited strong baseline performance—also sees incremental gains, underscoring the potential of\neven generic tools to enhance structured clinical reasoning.\n4\nConclusion\nMTBBench introduces a benchmark and agentic framework for evaluating AI agents in longitudi-\nnal, multi-modal oncology workflows, modeled on the structure of real molecular tumor boards.\nBy combining temporally evolving patient data, expert-validated clinical questions, and access\nto external tools and pretrained foundation models, MTBBench enables a rigorous assessment of\nagents’ ability to reason across modalities and time. Our evaluation shows that agents perform\nsignificantly better when equipped to query diverse modalities and leverage domain-specific models,\nunderscoring the need for flexible, tool-augmented reasoning in clinical AI. MTBBench shifts the\nfield from static, uni-modal evaluation toward dynamic, decision-centric assessment grounded in\nclinical complexity. While MTBBench simulates realistic decision-making, it remains a controlled\noffline benchmark—agents are not yet tested in interactive, real-world clinical workflows or exposed\nto ambiguous or incomplete inputs requiring clarification or adaptive strategies. Looking forward,\nfuture work will explore extending MTBBench to more diverse clinical domains and incorporating\ninteractive elements—paving the way for evaluating agents not only as reasoning tools but as potential\ncollaborators in real-world precision oncology. A particularly promising direction we would like\nto explore involves the integration of medical foundation models with capabilities for analyzing\ncomplex longitudinal data, enabling deeper temporal reasoning and personalized decision support.\n9\n"}, {"page": 10, "text": "Societal impacts. MTBBench offers clear benefits by promoting clinically relevant evaluation of AI\nagents, potentially advancing more trustworthy decision support in oncology. It encourages models to\nreason over multimodal, longitudinal data, closer to real-world needs. However, risks include misuse\nof the benchmark to suggest clinical readiness, and reduced transparency when agents rely heavily on\ntools. We emphasize that MTBBench is for research only, not clinical deployment, and should be\nused with proper oversight and ethical safeguards.\n5\nAcknowledgements\nThe authors would like to thank Stavros Pantelakos, Martha Nifora, Jan Rüschoff, and Claudio de\nVito for their valuable feedback for validation of the benchmark.\nReferences\nMichael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec,\nEric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence.\nNature, 616(7956):259–265, 2023.\nMing Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Melissa Zhao, Aaron K. Chow,\nKenji Ikemura, Ahrong Kim, Dimitra Pouli, Ankush Patel, Amr Soliman, Chengkuan Chen, Tong\nDing, Judy J. Wang, Georg Gerber, Ivy Liang, Long Phi Le, Anil V. Parwani, Luca L. Weishaupt,\nand Faisal Mahmood. A multimodal generative ai copilot for human pathology. Nature, 634(8033):\n466–473, Oct 2024a. ISSN 1476-4687. doi: 10.1038/s41586-024-07618-3.\nDawei Dai, Yuanhui Zhang, Qianlan Yang, Long Xu, Xiaojing Shen, Shuyin Xia, and Guoyin\nWang. Pathologyvlm: a large vision-language model for pathology image understanding. Artificial\nIntelligence Review, 58(6):186, Mar 2025. ISSN 1573-7462. doi: 10.1007/s10462-025-11190-1.\nMing Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Ivy Liang, Tong Ding, Guillaume\nJaume, Igor Odintsov, Long Phi Le, Georg Gerber, Anil V. Parwani, Andrew Zhang, and Faisal\nMahmood. A visual-language foundation model for computational pathology. Nature Medicine,\n30(3):863–874, Mar 2024b. ISSN 1546-170X. doi: 10.1038/s41591-024-02856-4.\nAkash Choudhuri, Philip Polgreen, Alberto Segre, and Bijaya Adhikari. Summarizing clinical notes\nusing llms for icu bounceback and length-of-stay prediction. medRxiv, 2025. doi: 10.1101/2025.\n01.19.25320797.\nZhichao Yang, Avijit Mitra, Sunjae Kwon, and Hong Yu. Clinicalmamba: A generative clinical\nlanguage model on longitudinal clinical notes. arXiv preprint arXiv:2403.05795, 2024.\nSamuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor.\nAgentclinic: a multimodal agent benchmark to evaluate ai in simulated clinical environments.\narXiv preprint arXiv:2405.07960, 2024.\nWenxuan Wang, Zizhan Ma, Zheng Wang, Chenghan Wu, Wenting Chen, Xiang Li, and Yixuan\nYuan. A survey of llm-based agents in medicine: How far are we from baymax? arXiv preprint\narXiv:2502.11211, 2025a.\nZifeng Wang, Hanyin Wang, Benjamin Danek, Ying Li, Christina Mack, Hoifung Poon, Yajuan\nWang, Pranav Rajpurkar, and Jimeng Sun. A perspective for adapting generalist ai to specialized\nmedical ai applications and their challenges. arXiv preprint arXiv:2411.00024, 2024a.\nEric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral,\nDavid Fleet, and Shekoofeh Azizi. Txgemma: Efficient and agentic llms for therapeutics. arXiv\npreprint arXiv:2504.06196, 2025b.\nShanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard\nSchwarz, Yasha Ektefaie, Jovana Kondic, and Marinka Zitnik. Empowering biomedical discovery\nwith ai agents. arXiv preprint arXiv:2404.02831, 2024.\n10\n"}, {"page": 11, "text": "Yongju Lee, Dyke Ferber, Jennifer E. Rood, Aviv Regev, and Jakob Nikolas Kather. How ai agents\nwill change cancer research and oncology. Nature Cancer, 5(12):1765–1767, Dec 2024. ISSN\n2662-1347. doi: 10.1038/s43018-024-00861-7.\nLing Yue, Sixue Xing, Jintai Chen, and Tianfan Fu. Clinicalagent: Clinical trial multi-agent system\nwith large language model-based reasoning. arXiv preprint arXiv:2404.14777, 2024.\nAdibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, and Bo Wang. Medrax: Medical reasoning\nagent for chest x-ray. arXiv preprint arXiv:2502.02673, 2025.\nApostolia M Tsimberidou, Michael Kahle, Henry Hiep Vo, Mehmet A Baysal, Amber Johnson, and\nFunda Meric-Bernstam. Molecular tumour boards—current and future considerations for precision\noncology. Nature Reviews Clinical Oncology, 20(12):843–863, 2023.\nYixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, James Zou, Andrew Y. Ng, and\nJonathan H. Chen. Medagentbench: A realistic virtual ehr environment to benchmark medical llm\nagents. arXiv preprint arXiv:2501.14654, 2025.\nShuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan S Ilgen, Emma Pierson, Pang Wei\nKoh, and Yulia Tsvetkov. Mediq: Question-asking llms and a benchmark for reliable interactive\nclinical reasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 38,\n2024.\nXian Wu, Yutian Zhao, Yunyan Zhang, Jiageng Wu, Zhihong Zhu, Yingying Zhang, Yi Ouyang,\nZiheng Zhang, Huimin Wang, Zhenxi Lin, Jie Yang, Shuang Zhao, and Yefeng Zheng. Medjourney:\nBenchmark and evaluation of large language models over patient clinical journey. In A. Globerson,\nL. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural\nInformation Processing Systems, volume 37, pages 87621–87646. Curran Associates, Inc., 2024.\nSunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim,\nJeewon Yang, Seunghyun Won, and Edward Choi. Ehrnoteqa: An llm benchmark for real-world\nclinical practice using discharge summaries. arXiv preprint arXiv:2402.16040, 2024.\nEmma Chen, Aman Kansal, Julie Chen, Boyang Tom Jin, Julia Rachel Reisler, David A. Kim,\nand Pranav Rajpurkar. Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A\nComprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine. arXiv\npreprint arXiv:2311.04937, 2023.\nWenxuan Wang, Yihang Su, Jingyuan Huan, Jie Liu, Wenting Chen, Yudi Zhang, Cheng-Yi Li,\nKao-Jung Chang, Xiaohan Xin, Linlin Shen, et al. Asclepius: A spectrum evaluation benchmark\nfor medical multi-modal large language models. arXiv preprint arXiv:2402.11217, 2024b.\nFenglin Liu, Zheng Li, Hongjian Zhou, Qingyu Yin, Jingfeng Yang, Xianfeng Tang, Chen Luo, Ming\nZeng, Haoming Jiang, Yifan Gao, Priyanka Nigam, Sreyashi Nag, Bing Yin, Yining Hua, Xuan\nZhou, Omid Rohanian, Anshul Thakur, Lei Clifton, and David A. Clifton. Large language models\nare poor clinical decision-makers: A comprehensive benchmark. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2024.\nGuillaume Jaume, Paul Doucet, Andrew H. Song, Ming Y. Lu, Cristina Almagro-Perez, Sophia J.\nWagner, Anurag J. Vaidya, Richard J. Chen, Drew F. K. Williamson, Ahrong Kim, and Faisal\nMahmood. Hest-1k: A dataset for spatial transcriptomics and histology image analysis. In\nAdvances in Neural Information Processing Systems, December 2024.\nLudovico Mitchener, Jon M Laurent, Benjamin Tenmann, Siddharth Narayanan, Geemi P Wellawatte,\nAndrew White, Lorenzo Sani, and Samuel G Rodriques. Bixbench: a comprehensive benchmark\nfor llm-based agents in computational biology. arXiv preprint arXiv:2503.00096, 2025.\nDyke Ferber, Omar S. M. El Nahhas, Georg Wölflein, Isabella C. Wiest, Jan Clusmann, Marie-\nElisabeth Leßman, Sebastian Foersch, Jacqueline Lammert, Maximilian Tschochohei, Dirk Jäger,\nManuel Salto-Tellez, Nikolaus Schultz, Daniel Truhn, and Jakob Nikolas Kather. Autonomous arti-\nficial intelligence agents for clinical decision making in oncology. arXiv preprint arXiv:2404.04667,\n2024.\n11\n"}, {"page": 12, "text": "Junkai Li, Yunghwei Lai, Weitao Li, Jingyi Ren, Meng Zhang, Xinhui Kang, Siyu Wang, Peng Li,\nYa-Qin Zhang, Weizhi Ma, and Yang Liu. Agent hospital: A simulacrum of hospital with evolvable\nmedical agents. arXiv preprint arXiv:2405.02957, 2025.\nMarion Dörrich, Matthias Balk, Tatjana Heusinger, Sandra Beyer, Hassan Kanso, Christian Matek,\nArndt Hartmann, Heinrich Iro, Markus Eckstein, Antoniu-Oreste Gostian, and Andreas M. Kist.\nA multimodal dataset for precision oncology in head and neck cancer. medRxiv, 2024. doi:\n10.1101/2024.05.29.24308141.\nJustin Jee, Christopher Fong, Karl Pichotta, Thinh Ngoc Tran, Anisha Luthra, Michele Waters,\nChenlian Fu, Mirella Altoe, Si-Yang Liu, Steven B Maron, et al. Automated real-world data\nintegration improves cancer outcome prediction. Nature, pages 1–9, 2024.\nPaul Hager, Friederike Jungmann, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer,\nRobbie Holland, Rickmer Braren, Marcus Makowski, Georgios Kaisis, and Daniel Rueckert. Eval-\nuating and mitigating limitations of large language models in clinical decision making. medRxiv,\n2024. doi: 10.1101/2024.01.26.24301810.\nRawan AlSaad, Alaa Abd-Alrazaq, Sabri Boughorbel, Arfan Ahmed, Max-Antoine Renault, Rafat\nDamseh, and Javaid Sheikh. Multimodal large language models in healthcare: Applications,\nchallenges, and future outlook. Journal of Medical Internet Research, 26:e59505, August 2024.\ndoi: 10.2196/59505.\nMing Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Melissa Zhao, Aaron K. Chow,\nKenji Ikemura, Ahrong Kim, Dimitra Pouli, Ankush Patel, Amr Soliman, Chengkuan Chen, Tong\nDing, Judy J. Wang, Georg Gerber, Ivy Liang, Long Phi Le, Anil V. Parwani, Luca L. Weishaupt,\nand Faisal Mahmood. A multimodal generative AI copilot for human pathology. Nature, 634\n(8033):466–473, October 2024c. ISSN 1476-4687. doi: 10.1038/s41586-024-07618-3. Publisher:\nNature Publishing Group.\nAnurag Vaidya, Andrew Zhang, Guillaume Jaume, Andrew H. Song, Tong Ding, Sophia J. Wagner,\nMing Y. Lu, Paul Doucet, Harry Robertson, Cristina Almagro-Perez, Richard J. Chen, Dina\nElHarouni, Georges Ayoub, Connor Bossi, Keith L. Ligon, Georg Gerber, Long Phi Le, and\nFaisal Mahmood. H&E, DNA, scRNA-seq: Molecular-driven Foundation Model for Oncologic\nPathology. arXiv preprint arXiv:2501.16652, January 2025a.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2023.\nMing Y Lu, Bowen Chen, Drew FK Williamson, Richard J Chen, Ivy Liang, Tong Ding, Guillaume\nJaume, Igor Odintsov, Long Phi Le, Georg Gerber, et al. A visual-language foundation model for\ncomputational pathology. Nature Medicine, 30:863–874, 2024d.\nRichard J Chen, Tong Ding, Ming Y Lu, Drew FK Williamson, Guillaume Jaume, Bowen Chen,\nAndrew Zhang, Daniel Shao, Andrew H Song, Muhammad Shaban, et al. Towards a general-\npurpose foundation model for computational pathology. Nature Medicine, 2024a.\nMaximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning.\nIn Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on\nMachine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2127–2136.\nPMLR, 10–15 Jul 2018.\nChaofan Li, Zheng Liu, Shitao Xiao, and Yingxia Shao. Making large language models a better\nfoundation for dense retrieval. arXiv preprint arXiv:2312.15503, 2023.\nD. S. Wishart. DrugBank: a comprehensive resource for in silico drug discovery and exploration.\nNucleic Acids Research, 34(90001):D668–D672, January 2006. ISSN 0305-1048, 1362-4962. doi:\n10.1093/nar/gkj067.\n12\n"}, {"page": 13, "text": "Craig Knox, Mike Wilson, Christen M Klinger, Mark Franklin, Eponine Oler, Alex Wilson, Allison\nPon, Jordan Cox, Na Eun Chin, Seth A Strawbridge, Marysol Garcia-Patino, Ray Kruger, Aadhavya\nSivakumaran, Selena Sanford, Rahil Doshi, Nitya Khetarpal, Omolola Fatokun, Daphnee Doucet,\nAshley Zubkowski, Dorsa Yahya Rayat, Hayley Jackson, Karxena Harford, Afia Anjum, Mahi\nZakir, Fei Wang, et al. Drugbank 6.0: the drugbank knowledgebase for 2024. Nucleic Acids\nResearch, 52(D1):D1265–D1275, November 2023. doi: 10.1093/nar/gkad976.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What\ndisease does this patient have? a large-scale open domain question answering dataset from medical\nexams. arXiv preprint arXiv:2009.13081, 2020.\nPengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong\nDuan, Ziyan Huang, Yanzhou Su, Benyou Wang, Shaoting Zhang, Bin Fu, Jianfei Cai, Bohan\nZhuang, Eric J Seibel, Junjun He, and Yu Qiao. Gmai-mmbench: A comprehensive multimodal\nevaluation benchmark towards general medical ai. arXiv preprint arXiv:2408.03361, 2024b.\nYunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing,\nJames Zou, Cihang Xie, and Yuyin Zhou. Medtrinity-25m: A large-scale multimodal dataset with\nmultigranular annotations for medicine. arXiv preprint arXiv:2408.02900, 2024.\nWouter Bulten, Kimmo Kartasalo, Po-Hsuan Cameron Chen, Peter Ström, Hans Pinckaers, Kunal\nNagpal, Yuannan Cai, David F. Steiner, Hester Van Boven, Robert Vink, Christina Hulsbergen-Van\nDe Kaa, Jeroen Van Der Laak, Mahul B. Amin, Andrew J. Evans, Theodorus Van Der Kwast,\nRobert Allan, Peter A. Humphrey, Henrik Grönberg, et al. Artificial intelligence for diagnosis and\ngleason grading of prostate cancer: the panda challenge. Nature Medicine, 28(1):154–163, January\n2022. doi: 10.1038/s41591-021-01620-2.\nZiyue Wang, Junde Wu, Linghan Cai, Chang Han Low, Xihong Yang, Qiaxuan Li, and Yueming\nJin. Medagent-pro: Towards evidence-based multi-modal medical diagnosis via reasoning agentic\nworkflow. arXiv preprint arXiv:2503.18968, 2025c.\nQiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, and Weidi\nXie. How well can modern llms act as agent cores in radiology environments? arXiv preprint\narXiv:2412.09529, 2025.\nFatemeh Ghezloo, Mehmet Saygin Seyfioglu, Rustin Soraki, Wisdom O Ikezogwo, Beibin Li, Tejoram\nVivekanandan, Joann G Elmore, Ranjay Krishna, and Linda Shapiro. Pathfinder: A multi-modal\nmulti-agent system for medical diagnostic decision-making applied to histopathology. arXiv\npreprint arXiv:2502.08916, 2025.\nZiyue Wang, Junde Wu, Chang Han Low, and Yueming Jin. Medagent-pro: Towards evidence-based\nmulti-modal medical diagnosis via reasoning agentic workflow. 2025d.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu.\nBiogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings\nin Bioinformatics, 23(6):bbac409, 09 2022. ISSN 1477-4054. doi: 10.1093/bib/bbac409. URL\nhttps://doi.org/10.1093/bib/bbac409.\nElliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Rox-\nana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, and Christopher D. Man-\nning. Biomedlm: A 2.7b parameter language model trained on biomedical text, 2024. URL\nhttps://arxiv.org/abs/2403.18421.\nCharlie Saillard, Rodolphe Jenatton, Felipe Llinares-López, Zelda Mariet, David Cahané, Eric\nDurand, and Jean-Philippe Vert. H-optimus-0, 2024. URL https://github.com/bioptimus/\nreleases/tree/main/models/h-optimus/v0.\nAlexandre Filiot, Paul Jacob, Alice Mac Kain, and Charlie Saillard. Phikon-v2, a large and public\nfeature extractor for biomarker prediction, 2024. URL https://arxiv.org/abs/2409.09173.\nEugene Vorontsov, Alican Bozkurt, Adam Casson, George Shaikovski, Michal Zelechowski, Kristen\nSeverson, Eric Zimmermann, James Hall, Neil Tenenholtz, Nicolo Fusi, Ellen Yang, Philippe\nMathieu, Alexander van Eck, Donghun Lee, Julian Viret, Eric Robert, Yi Kan Wang, Jeremy D.\n13\n"}, {"page": 14, "text": "Kunz, Matthew C. H. Lee, Jan H. Bernhard, Ran A. Godrich, Gerard Oakley, Ewan Millar,\nMatthew Hanna, Hannah Wen, Juan A. Retamero, William A. Moye, Razik Yousfi, Christopher\nKanan, David S. Klimstra, Brandon Rothrock, Siqi Liu, and Thomas J. Fuchs. A foundation\nmodel for clinical-grade computational pathology and rare cancers detection. Nature Medicine,\n30(10):2924–2935, Oct 2024. ISSN 1546-170X. doi: 10.1038/s41591-024-03141-0. URL\nhttps://doi.org/10.1038/s41591-024-03141-0.\nFang Yan, Jianfeng Wu, Jiawen Li, Wei Wang, Jiaxuan Lu, Wen Chen, Zizhao Gao, Jianan Li,\nHong Yan, Jiabo Ma, Minda Chen, Yang Lu, Qing Chen, Yizhi Wang, Xitong Ling, Xuenian\nWang, Zihan Wang, Qiang Huang, Shengyi Hua, Mianxin Liu, Lei Ma, Tian Shen, Xiaofan Zhang,\nYonghong He, Hao Chen, Shaoting Zhang, and Zhe Wang. Pathorchestra: A comprehensive\nfoundation model for computational pathology with over 100 diverse clinical-grade tasks, 2025.\nURL https://arxiv.org/abs/2503.24345.\nJinxi Xiang, Xiyue Wang, Xiaoming Zhang, Yinghua Xi, Feyisope Eweje, Yijiang Chen, Yuchen Li,\nColin Bergstrom, Matthew Gopaulchan, Ted Kim, Kun-Hsing Yu, Sierra Willens, Francesca Maria\nOlguin, Jeffrey J. Nirschl, Joel Neal, Maximilian Diehn, Sen Yang, and Ruijiang Li. A vision–\nlanguage foundation model for precision oncology.\nNature, 638(8051):769–778, Feb 2025.\nISSN 1476-4687. doi: 10.1038/s41586-024-08378-w. URL https://doi.org/10.1038/\ns41586-024-08378-w.\nJacqueline Lammert, Tobias Dreyer, Sonja Mathes, Leonid Kuligin, Kai J Borm, Ulrich A Schatz,\nMarion Kiechle, Alisa M Lörsch, Johannes Jung, Sebastian Lange, et al. Expert-guided large\nlanguage models for clinical decision support in precision oncology. JCO precision oncology, 8:\ne2400478, 2024.\nJacob Cohen. A coefficient of agreement for nominal scales. Educational and Psychological\nMeasurement, 20(1):37–46, 1960.\nJoseph L. Fleiss. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76\n(5):378–382, 1971.\nTed Byrt, Janet Bishop, and John B. Carlin. Bias, prevalence and kappa. Journal of Clinical\nEpidemiology, 46(5):423–429, 1993.\nKilem Li Gwet. Computing inter-rater reliability and its variance in the presence of high agreement.\nBritish Journal of Mathematical and Statistical Psychology, 61(1):29–48, 2008.\nKlaus Krippendorff. Computing krippendorff’s alpha-reliability. https://repository.upenn.\nedu/asc_papers/43, 2011. University of Pennsylvania ScholarlyCommons.\nPeter Bankhead, Maurice B Loughrey, José A Fernández, Yvonne Dombrowski, Darragh G McArt,\nPhilip D Dunne, Stephen McQuaid, Ronan T Gray, Liam J Murray, Helen G Coleman, et al.\nQupath: Open source software for digital pathology image analysis. Scientific reports, 7(1):1–7,\n2017.\nAnurag Vaidya, Andrew Zhang, Guillaume Jaume, Andrew H Song, Tong Ding, Sophia J Wagner,\nMing Y Lu, Paul Doucet, Harry Robertson, Cristina Almagro-Perez, et al. Molecular-driven\nfoundation model for oncologic pathology. arXiv preprint arXiv:2501.16652, 2025b.\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej,\nSarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas\nMesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon,\nEtienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai\nZhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman,\nYi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, et al. Gemma 3 technical report. arXiv preprint\narXiv:2503.19786, 2025.\nOpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan\nClark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander M ˛adry, Alex Baker-\nWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex\nPaino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau,\nAli Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin\n14\n"}, {"page": 15, "text": "Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew\nCann, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.\nJinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen\nDuan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu,\nXingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng,\nSongze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang,\nWenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, et al. Internvl3: Exploring advanced training\nand test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem\nKorenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava\nSpataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya\nNayak, Chloe Bi, Chris Marra, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783,\n2024.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan,\nPengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,\nHang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv\npreprint arXiv:2502.13923, 2025.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\nGao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu,\nHao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang,\nLe Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu,\nRui Men, Ruize Gao, Shixuan Liu, Shuang Luo, et al. Qwen3 technical report. arXiv preprint\narXiv:2505.09388, 2025.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023.\n15\n"}, {"page": 16, "text": "Appendix\nA\nBackground on Molecular Tumor Boards\nTreating physician to make \nthe inal treatment decision\n•  The meetings are usually held \nweekly, biweekly or monthly\n•  Ad hoc meetings are held \nto provide treatment \nrecommendations in\n   a timely manner\n•  The treating physician presents \n   the case of the patient\n•  Provides data on the\n   demographics, pathological \ndiagnosis, clinical characteristics \nand any previous therapies of the \npatient\nMTB members and patient \nadvocates should provide input \nregarding treatment \nrecommendations\nConsensus recommendations \nshould be provided in writing by \nthe chair of the MTB, and they \nshould be clearly communicated \nto the treating physician\nParticipants: oncologists, physician \nscientists, precision oncology \nscientists, cancer geneticists, clinical \nfellows and/or residents, studies \ncoordinators, haematologists, \npathologists, patient advocates, \nradiation oncologists, surgeons and \nclinical genetics counsellors\n•  MTB members with clinical \nexpertise should propose matched \ntargeted therapy\n•  Non-molecular matched approved \nand investigational therapy options \nshould also be considered\nThe scientists provide the tumour \ngenetic pro\nile of the patient (e.g., \nTMB, MSI status, fusion, the status of \nbiomarkers or immune markers)\nThe conclusion should be \nsummarized by the chair of the MTB\nFigure 4: Overview of the Molecular Tumor Board process. Meetings are held at regular intervals or on\ndemand, bringing together multidisciplinary experts who jointly review patient history, molecular profiling\nresults, and clinical evidence to recommend personalized treatment strategies. Final decisions are communicated\nin writing to the treating physician. Figure adapted from Tsimberidou et al. (2023).\nMolecular Tumor Boards are structured, multidisciplinary forums where complex cancer cases are\nevaluated through the integration of clinical, pathological, and molecular data. These boards include\noncologists, pathologists, geneticists, and other specialists who collectively interpret diagnostic and\ngenomic findings to formulate personalized treatment recommendations. The process begins with\nthe treating physician presenting the patient’s case, including demographic and clinical background,\nprior therapies, and pathological findings. Scientists contribute molecular profiling data—such as\ntumor mutational burden (TMB), microsatellite instability (MSI) status, fusion events, and biomarker\nexpression—that are critical for matching patients to targeted therapies (Tsimberidou et al., 2023).\nTreatment options are proposed by the MTB members based on this integrated evidence, and\nconsensus recommendations are recorded and relayed to the treating physician for final decision-\nmaking. The process is iterative, often revisiting cases as new data becomes available, and increasingly\ninvolves patient advocates and real-time access to clinical trial data and treatment databases.\nFigure 4 visualizes this workflow, emphasizing how the flow of structured information and expert\ninput leads to treatment recommendations that are tailored, evidence-driven, and context-aware. This\nstructured, evolving nature of clinical decision-making underpins the MTBBench benchmark design,\nwhich mirrors MTB dynamics through sequential, multimodal question-answering tasks grounded in\nexpert-reviewed patient cases.\n16\n"}, {"page": 17, "text": "B\nRelated Work\nBenchmarks for clinical AI. Numerous benchmarks have been developed to evaluate clinical\nquestion-answering and medical reasoning in large language models. Early efforts focused on\ntextual data, such as EHRNoteQA (Kweon et al., 2024), which uses discharge summaries to evaluate\nclinical understanding, and MedQA (Jin et al., 2020), which tests medical licensing exam-style\nquestions. While these benchmarks provide foundational testbeds, they are largely unimodal and do\nnot evaluate temporal reasoning or interaction. ClinicBench (Liu et al., 2024) and MedJourney (Wu\net al., 2024) introduced more realistic clinical tasks with timeline structures, but still lack multimodal\nintegration and agentic file access. MTBBench addresses these gaps by combining longitudinal\nstructure, multimodal data, and interactive information retrieval within a single benchmark.\nMultimodal biomedical benchmarks. Multimodal benchmarks such as MC-BEC (Chen et al.,\n2023), Asclepius (Wang et al., 2024b), GMAI-MMBench (Chen et al., 2024b), and MedTrinity-\n25M (Xie et al., 2024) have broadened evaluation to include clinical text, waveforms, and images,\ntesting foundation models across diverse modalities. However, these tasks are typically static and\ndo not assess models’ ability to retrieve, interpret, or combine information across time. Pathology-\nspecific datasets like HEST-1k (Jaume et al., 2024) and PANDA (Bulten et al., 2022) enable visual\nclassification, but lack the clinical reasoning components required for decision-making. MTBBench\nadvances this line of work by evaluating whether models can synthesize insights across pathology,\nhematology, and genomics within evolving case contexts.\nAgentic clinical systems. Recent interest in LLM-based clinical agents has led to new frameworks\nsuch as MedAgentBench (Jiang et al., 2025), MediQ (Li et al., 2024), MedAgent-Pro (Wang et al.,\n2025c), RadA-BenchPlat (Zheng et al., 2025), and AgentClinic (Schmidgall et al., 2024). These\nsystems evaluate agents in interactive or dialogue-based environments, but focus primarily on textual\ndata or synthetic tasks. There has been effort to develop LLM-based agents for chest X-rays (Fal-\nlahpour et al., 2025), histopathology (Ghezloo et al., 2025), and multiple imaging modalities (Wang\net al., 2025d). MTBBench complements these efforts by embedding tool-use into clinically realistic\nworkflows and measuring how tool-augmented agents reason in complex, multimodal scenarios.\nFoundation models in healthcare. Foundation models trained on biomedical corpora or medical\nimaging datasets have demonstrated promising capabilities in generalization and zero-shot reason-\ning (Moor et al., 2023). Biomedical language models trained on literature and structured data\n(e.g., BioGPT (Luo et al., 2022), BioMedLM (Bolton et al., 2024)) support evidence grounding.\nIn pathology, several vision foundation models have emerged: H-optimus-0 (Saillard et al., 2024),\nPhikon-v2 (Filiot et al., 2024), Virchow (Vorontsov et al., 2024), and PathOrchestra (Yan et al., 2025).\nVision-language models like CONCH (Lu et al., 2024b), UNI-2 (Chen et al., 2024a), and MUSK (Xi-\nang et al., 2025) integrate histopathology images with clinical text to enhance slide interpretation and\nsupport precision oncology applications. However, these models are typically benchmarked in isola-\ntion. MTBBench instead evaluates how foundation models function as tools within agentic systems,\ntesting not just their predictive accuracy but also their integration into sequential decision-making\nprocesses.\nPrecision oncology and Molecular Tumor Boards. MTBs represent a high-stakes, information-\ndense setting in which multimodal and longitudinal reasoning is essential (Tsimberidou et al., 2023).\nLammert et al. (2024) introduces a domain-specific LLM system for oncology treatment recommen-\ndations. Prior datasets in this space are often limited to structured formats or single-modality use\ncases. MTBBench is among the first benchmarks to simulate MTB workflows comprehensively, cap-\nturing the clinical sequencing, agent interaction, and data integration that define real-world oncology\ndecision-making (Jee et al., 2024).\nC\nClinical Validation\nC.1\nCompanion App for Clinical Validation\nTo ensure the clinical validity of the benchmark and facilitate expert-in-the-loop review, we de-\nveloped a custom web-based application to support question and answer annotation (https:\n//share.streamlit.io/app/oncoform/ - private). The companion app allows clinicians to\n17\n"}, {"page": 18, "text": "…\n…\nFigure 5: Companion app interface for clinical validation. The platform displays clinical context, reference\nimages grouped by region and marker, and multiple-choice questions for expert review. Full-resolution slide\nviewers and inline feedback fields allow for efficient validation of benchmark items.\nbrowse detailed patient cases, including demographic data, clinical summaries, reference images\n(H&E and IHC), and the associated question-answer pairs (Fig. 5).\nEach patient case is presented with a structured clinical context, followed by a grid of labeled image\nthumbnails categorized by tissue region and marker (e.g., \"Tumor Center CD8\", \"Invasion Front\nH&E\"). Clicking a thumbnail loads the full-resolution slide. Besides the image panel, the user is\npresented with multiple-choice questions linked to the case, and input fields to provide feedback or\ncorrections for each answer.\nThis interface was designed to mirror the decision-making process of molecular tumor boards,\noffering domain experts an intuitive environment for validating multimodal reasoning tasks. It\nsupports both qualitative feedback and quantitative validation, and was used extensively during\nbenchmark construction to curate expert-reviewed question sets for both MTBBench-Multimodal\nand MTBBench-Longitudinal.\nC.2\nExpert Validation\nAfter multiple iterations of internal expert-in-the-loop auditing, the final set of questions in\nMTBBench were sent for external manual review by domain specialists via the companion ap-\nplication, where we aimed to judge the soundness and relevance of the QA-pairs. Due to the diverse\nnature of the benchmark and different categories of questions, reviews were conducted both indepen-\ndently and with overlapping sets to assess consistency. In total, 10 experts from multiple countries,\nhospitals, and levels of expertise were involved in the review process. For the overlapping sets, the\nexternal reviewers received 45 questions to review via the app, and the QA pairs were rated as Good\nas is, Needs minor rewording, or Inappropriate. We report the average inter-rater agreement metrics\nover these reviews in Table 3. We observe that there is a high annotation consistency and confidence\nin question quality.\n18\n"}, {"page": 19, "text": "Table 3: Average inter-rater agreement metrics of ten experts over 45 questions.\nOverall\nMean pairwise\nMean Cohen’s κ (Cohen, 1960)\nFleiss’ κ (Fleiss, 1971)\nMean PABAK (Byrt et al., 1993)\nGwet’s AC1 (Gwet, 2008)\nKrippendorff’s α (Krippendorff, 2011)\n0.91\n0.94\n0.81\n0.79\n0.91\n0.91\n0.79\nD\nAdditional Details on MTBBench\nD.1\nAgentic Workflow\nInteractive dialogue setup. To simulate realistic clinical scenarios, MTBBench employs an inter-\nactive, multi-turn dialogue setup in which a Doctor agent is tasked with responding to several user\nquestions. These questions are grounded in access to several modalities grouped in a comprehensive\npatient case folder.\nConversation initialization. Both evaluation datasets follow the same general approach, where the\nconversation begins with a short patient introduction, which contains patient demographic information\nsuch as age, gender, as well as symptoms the patient is currently experiencing about their oncological\ncondition. The agent is then guided on the availability and structure of files that contain additional\npatient-specific information.\nDifferences between datasets. The available files vary between the two MTBBench datasets. In the\nmultimodal dataset, patient data can include digital pathology, hematological, and general clinical\nnotes. In contrast, the longitudinal dataset presents information in temporal or tabular format,\nreflecting the patient’s medical history over time.\nAgent actions and memory constraints. The agent is equipped with two main actions during its\nreasoning: it can either request one or several files or provide a final answer once it has gathered\nsufficient information. Files accessed by the agent remain available only during the context of the\ncurrent question being answered. For instance, if the model receives several images or text files while\nresponding to a query, those files will no longer be accessible when answering subsequent questions,\nunless the model requests them again. This setup simulates a realistic clinical workflow, where an\nagent opens several clinical files from a patient folder, processes their information, and closes them\nupon completion of the current task. As a result, only the agent’s reasoning steps alongside the record\nof which files were accessed persist across turns, reinforcing the need for information gathering and\nmemory management.\nProgressive contextual disclosure. As the conversation progresses, with multiple questions ad-\ndressed, new contextual information and corresponding patient files are made available to the agent.\nThis design mirrors a real-life clinical workflow, where the initial diagnosis is performed, and sev-\neral additional tests are performed over time. By introducing new stages, the agent is required to\ncontinuously reason over evolving patient information.\nMTBBench-Multimodal structure. In the multi-modal part of our benchmark, the conversation\nbegins with initial patient information, accompanied by multiple H&E and IHC slides, provided to\nsupport reasoning around oncological image interpretation. With the initial assessment performed\nand primary cancer type identified, the simulated environment continues with a pre-surgical stage.\nIn preparation for a surgery, the patient undergoes several lab tests aimed at assessing the overall\nhealth status and fitness for surgery and future treatments. In this stage, the agent has access to the\npatient’s blood tests, together with a reference table specifying normal ranges for male and female\npatients. Following this, the surgery would be carried out, and a summary of the outcome would be\nprovided in the context. The full surgery report and a short list of interventions would be accessible\nin the patient’s case folder. At this stage, the questions asked would have a prognostic nature, with\nthe agent tasked to predict the 5-year survival outcome and 2-year cancer recurrence, based on the\npatient data gathered throughout the entire case.\nMTBBench-Longitudinal structure. In the longitudinal evaluation track, the conversation begins\nwith baseline clinical information and the introduction of the primary cancer type. The questions\nasked target outcome prediction, recurrence, and cancer progression for a given period. To support\nthe agent in its reasoning, a timeline file is available in the patient’s case folder with important events\nsorted by the age at which they occurred. If a sample has been taken and sequenced, additional\n19\n"}, {"page": 20, "text": "patient data is provided in tabular format, such as gene mutations, copy number alterations, and\nstructural variants. After the agent has answered several questions, a new context may be provided\ncontaining the outcome of all questions asked, with a new timeline file capturing the additional\npatient history. The agent can then combine all timeline files together to create a complete and\ncomprehensive medical history to answer additional questions.\nEvaluation and reproducibility. To facilitate reproducibility and streamline model evaluation, each\nrun stores the complete conversation history, including the model’s final answers for each question,\nthe set of files accessed per query, and any hallucinated file names. We provide comprehensive logs\nfor all evaluated models, both with and without tool access, in the project repository.\nE\nDetails on Experiments\nE.1\nDetails on Foundation Models and Downstream Tasks\nH&E foundation model. For histopathology image encoding, we employ the CONCH model\ndirectly, without any task-specific fine-tuning on H&E slides. Model weights are obtained from the\nofficial HuggingFace repository. To ensure compatibility and optimal performance, we adhere strictly\nto the preprocessing and usage guidelines outlined in the model card, including image normalization\nand text-token preparation for the dual-encoder architecture.\nCONCH downstream tasks. The CONCH foundation model is utilized for zero-shot region-of-\ninterest (ROI) classification, where candidate labels are supplied by the LLM. Classification is\nperformed by computing the dot product similarity between image and text embeddings. The label\nwith the highest similarity is selected and returned to the LLM, along with a confidence estimate. To\nimprove interpretability and account for potential label ambiguity, the raw softmax score is discretized\ninto confidence bins: very low (0–20%), low (20–40%), medium (40–60%), high (60–80%), and very\nhigh (80–100%). Because the LLM-generated label set may not always include the most semantically\naccurate class, we avoid exposing exact probabilities and instead provide only the corresponding bin.\nTissue extraction using QuPath (Bankhead et al., 2017). To develop our ABMIL tool, which\npredicts the percentage of positively stained cells, we manually curated a dataset of immunohisto-\nchemistry (IHC) images using a semi-automated annotation pipeline in QuPath. For each patient,\ntissue microarray (TMA) cores were segmented to isolate regions containing tissue, and each region\nwas mapped to the corresponding patient metadata. Within these regions, individual cells were\nidentified through a combination of optical density transformation, background correction, and mor-\nphological segmentation. Cells were then classified as positively or negatively-stained based on DAB\nstaining intensity. The resulting cell-level annotations formed the basis for training and evaluating\nour ABMIL model.\nIHC foundation model. We extract UNIv2 embeddings using the TRIDENT framework (Vaidya\net al., 2025b). Each whole-slide image is first loaded at the appropriate microns-per-pixel (mpp)\nresolution, followed by tissue segmentation using the HEST model (Jaume et al., 2024). The\nsegmented tissue regions are then divided into fixed-size patches of 256 × 256 pixels. The UniV2\nfoundation model is applied to each patch to generate feature embeddings, which are subsequently\naggregated using the ABMIL.\nOur Gated ABMIL model architecture consists of two attention heads, each with a dimensionality of\n512. The model incorporates a dropout rate of 0.3 and an overall model dimensionality of 1536. The\nregression component of the IHC tool is implemented as a five-layer fully connected neural network,\nutilizing ReLU activation functions and a dropout rate of 0.2. Training was conducted over 70 epochs\nusing a batch size of 64, the Adam optimizer, and a learning rate of 0.0004. All experiments were\ncarried out on a single NVIDIA A100 GPU with 80 GB of memory.\nWe release the pretrained weights of our IHC foundation model as part of the project’s GitHub\nrepository.\n20\n"}, {"page": 21, "text": "E.2\nDetails on Resource and Knowledge Database Tools\nPubMed querying tool. To retrieve biomedical literature, we integrate a PubMed querying tool using\nthe Biopython library (https://biopython.org/). Search queries are generated by the calling\nLLM and may include advanced formatting, such as boolean operators (e.g., lung carcinoma OR\nlung adenocarcinoma). The tool retrieves the top 30 articles from PubMed based on the provided\nquery and reranks them using the BAAI-bge-reranker-v2-m3 model. The reranker jointly embeds\nthe query and each article’s abstract as a text pair and computes a logit-based relevance score. The\ntop 3 abstracts with the highest scores are returned to the LLM for downstream reasoning.\nDrugBank querying tool. To integrate drug-related knowledge, we obtained a non-commercial\nresearch license for DrugBank and accessed the platform’s API. We retrieved a snapshot containing\napproximately 21,000 drug names along with their corresponding descriptions. Our DrugBank\nquerying tool performs string-based lookups on file contents requested by the LLM. When a drug\nname match is identified, the corresponding description is returned to the LLM, enabling enhanced\ncontextual understanding and more informed clinical reasoning.\nE.3\nDetails on Large Language and Vision Language Models\nLLM specifications. We benchmark the following models: gemma-3-12b, gemma-3-27b (Team\net al., 2025), gpt4o (OpenAI et al., 2024), o4-mini, internvl3-38b, internvl3-78b (Zhu et al., 2025),\nllama90b, llama31-8b, llama33-70b (Grattafiori et al., 2024), mistralsmall, qwen25-7b, qwen25-\n32b (Bai et al., 2025), qwen3-8b, qwen3-32b (Yang et al., 2025). To optimize GPU memory\nusage while preserving model performance, we apply 4-bit quantization to the following models:\nllama90b, mistralsmall, llama31-8b, llama33-70b, qwen3-8b (reasoning), and qwen3-32b (reason-\ning). These models are served using the VLLM inference engine (Kwon et al., 2023), with weights\nsourced from HuggingFace. For gemma-3-12b, gemma-3-27b, qwen25-7b, and qwen25-32b, we\nemploy 8-bit quantization and use the HuggingFace implementation and associated pretrained\nweights. For OpenAI-based models, we use the gpt-4o-2024-08-06 checkpoint for gpt4o and the\no4-mini-2025-04-16 checkpoint for o4-mini.\nE.4\nDetails on Evaluation Metrics\nAnswer accuracy. We evaluate the agent system primarily using accuracy, measured on a set of\ntrue/false and multiple-choice questions (each with six answer options). This formulation enables\nobjective evaluation without relying on human annotators or oracle LLMs, thereby ensuring re-\nproducibility and consistency across models. Model outputs are parsed using regular expressions\nto extract answers. We attempt to identify whether the output is a single letter (e.g., [ANSWER:\nA]) or a letter with the corresponding option (e.g., [ANSWER: A) Squamous Cell Carcinoma,\nKeratizing]). If the model does not follow this format, we prompt it again to extract a valid answer.\nAfter three failed attempts, the response is marked as incorrect.\nFile access count. In addition to accuracy, we track the number of files accessed per question. The\nmodel is allowed to access files by explicitly specifying the filename and extension in the prescribed\nformat (i.e., [REQUEST: primary_tumor_roi.jpg]). This metric captures how actively the model\nexplores the available patient data, serving as a proxy for information-seeking behavior. A lower\nfile access count may indicate superficial reasoning or hallucination, whereas higher counts suggest\ninformation retrieval and more grounded decision-making. Thus, this metric provides valuable insight\ninto the agent’s interpretability and alignment with real-world clinical workflows.\nE.5\nDetails on Computational Resources\nAll agentic experiments were conducted using NVIDIA A100 80GB GPUs. Specifically, the models\nqwen3-8b (reasoning), gemma-3-12b, llama31-8b, and qwen25-7b were run on a single GPU. We\nused two GPUs for gemma-3-27b, qwen25-32b, qwen3-32b (reasoning), and mistralsmall. The most\nresource-intensive model, llama33-70b, required seven GPUs for inference. For the OpenAI models\ngpt4o and o4-mini, all evaluations were performed using CPU-only inference via API access. Across\nall configurations, the full execution of the MTBBench benchmark for a given model was completed\nwithin a maximum of two days.\n21\n"}, {"page": 22, "text": "F\nExamples of MTBBench Agentic Interactions\nIn 2016, a 52-year-old male presented for evaluation. The patient has a history of smoking. At the \ntime of initial assessment, diagnostic workup was initiated, including imaging studies and a sample \ncollection for further analysis. Hematoxylin and eosin (H&E) staining, as well as immunohistochemical \n(IHC) staining, have been performed on the collected sample to assist in determining the underlying \ncondition. Further evaluation and clinical correlation are ongoing. \nNew files available: [List of H&E slides at Primary-Tumor and Lymph Nodes, and 28 IHC tumor microarrays \nspanning 7 different markers] \nQuestion: Based on the histopathological features visible on the H&E image, what is the most likely \nhistologic type of the tumor? \nA) SCC_Conventional-Keratinizing \nB) Adenocarcinoma \nC) Small cell neuroendocrine carcinoma \nD) Non-keratinizing SCC \nE) Clear cell carcinoma \nF) Synovial sarcoma\nTo determine the most likely \nhistologic type of the tumor, I \nneed to analyze the H&E-stained \nimages provided. Specifically, I \nwill examine the primary tumor H&E \nimage and its region of interest \n(ROI).\nGPT-4o\nTo determine the most likely \nhistologic type of the tumor based \non the H&E images, I would need to \nreview the H&E stained slides. \nPlease provide the H&E stained image \nso I can analyze it.\nQwen2.5-7b\nSystem\nPrimary \nTumor\nThank you for providing the \nimages. I will now analyze the \nhistopathological features \nvisible in the H&E-stained \nimages to determine the most \nlikely histologic type of the \ntumor. \nThe images show the following \nfeatures: \n- Keratinization: Evidence of \nkeratin pearls or individual \ncell keratinization. \n- Cellular arrangement: The \norganization of cells, including \nglandular structures or sheets \nof cells. \n- Nuclear morphology: The size, \nshape, and chromatin pattern of \nnuclei. \n- Stromal interaction: The \nrelationship between the tumor \ncells and the surrounding \nstroma. \nBased on these features, I will \nprovide the most likely \ndiagnosis.\nThank you for providing the H&E \nstained image. Based on the \nappearance of the tissue, the \ntumor shows glandular structures \nwith a cribriform pattern, which \nis characteristic of \nadenocarcinoma. The presence of \nglandular structures and the \nabsence of squamous \ndifferentiation suggest that \nthis is not a squamous cell \ncarcinoma (SCC). Therefore, the \nmost likely histologic type of \nthe tumor is: \n[ANSWER: B) Adenocarcinoma]\nRegion-\nof-Interest\n❌\nBased on the histopathological \nfeatures visible in the provided \nH&E-stained images, the most likely \nhistologic type of the tumor is: \n[ANSWER: A) SCC_Conventional-\nKeratinizing]\n✅\nPlease provide the final \nanswer in [ANSWER: LETTER) \nanswer] specifying the \nanswer choice letter you \npicked (A, B, C, D, E, or \nF) or ask for files with \n[REQUEST: \nfilename.extension]. Make \nsure you have both the \nopening an closing brackets\nExample 1: In this digital pathology question, gpt4o requests not only the low-resolution primary tumor slide,\nbut also the high resolution region-of-interest (ROI), which allows to identify the correct cancer subtype. On\nthe other hand, qwen25-7b cannot identify the correct subtype because it only requested the primary tumor\nslide, which is too high level to identify the critical features for cancer subtype identification. Note that the\nraw histology slides are high resolution (which allows to extract ROIs) but were resized for compatibility with\nVLMs.\n22\n"}, {"page": 23, "text": "H&E + H&E ROI\n[... previous questions on H&E slides and specific TMA markers...] \nQuestion: Based on both H&E and IHC slides, is perineural invasion present, and how does this correlate \nwith immune cell infiltration? \nA) Perineural invasion is present, with dense T-cell infiltration around nerves.   \nB) Perineural invasion is absent, with sparse T-cell infiltration in perineural areas.   \nC) Perineural invasion is present, with sparse T-cell infiltration around nerves.   \nD) Perineural invasion is absent, with dense macrophage infiltration in perineural areas.   \nE) Perineural invasion is absent, with dense NK cell infiltration in perineural areas.   \nF) Perineural invasion is present, with dense NK cell infiltration around nerves.  \nTo determine the presence of \nperineural invasion and its \ncorrelation with immune cell \ninfiltration, I need to examine the \nrelevant H&E and IHC slides. \n[Request for H&E slide, H&E Region-of-\ninterest, and all IHCs for markers \nCD3, CD8, CD56, CD68 at Invasion Front \nand Tumor Center]\nMistral-Small\nTo assess for perineural invasion, I \nneed to carefully review the H&E \nslides looking for tumor cells \ninfiltrating around nerves. I will \nalso examine the CD3 IHC slides to \nassess T-cell infiltration in \nperineural spaces. \n[REQUEST: \nPrimaryTumor_HE_706_roi.jpg] \n[REQUEST: \nTMA_IHC_InvasionFront_CD3_0.png]\nGemma3-27b\nSystem\nAfter careful review of the H&E and \nCD3 IHC slides, perineural invasion \nis present. I can identify tumor \ncells closely associated with and \nsurrounding a nerve bundle. \nFurthermore, there is sparse T-cell \ninfiltration around the affected \nnerve. The CD3 staining shows only a \nlimited number of T cells in the \nperineural space. \n[ANSWER: C) Perineural invasion is \npresent, with sparse T-cell \ninfiltration around nerves.]\nBased on the provided H&E and IHC \nslides, we can evaluate the presence \nof perineural invasion and its \ncorrelation with immune cell \ninfiltration. \n1. H&E Slides: \n   - The H&E slides \n(PrimaryTumor_HE_706.jpg and \nPrimaryTumor_HE_706_roi.jpg) show \nareas where tumor cells are in close \nproximity to nerves. However, \ndefinitive perineural invasion is \nnot clearly evident in these images. \nPerineural invasion typically \nappears as tumor cells tracking \nalong the nerve sheath, which is not \ndistinctly visible here. \n❌\n2. IHC Slides: \n   - CD3 Staining: The CD3 stained images (TMA_IHC_TumorCenter_CD3_0.png, \nTMA_IHC_TumorCenter_CD3_1.png, TMA_IHC_InvasionFront_CD3_0.png, \nTMA_IHC_InvasionFront_CD3_1.png) show T-cell infiltration. The invasion front appears to have \na higher density of CD3-positive T-cells compared to the tumor center. \n   - CD8 Staining: The CD8 stained images (TMA_IHC_TumorCenter_CD8_0.png, \nTMA_IHC_TumorCenter_CD8_1.png, TMA_IHC_InvasionFront_CD8_0.png, \nTMA_IHC_InvasionFront_CD8_1.png) also show a higher density of CD8-positive T-cells at the \ninvasion front. \n   - CD56 Staining: The CD56 stained images (TMA_IHC_TumorCenter_CD56_0.png, \nTMA_IHC_TumorCenter_CD56_1.png, TMA_IHC_InvasionFront_CD56_0.png, \nTMA_IHC_InvasionFront_CD56_1.png) show sparse NK cell infiltration in both the tumor center \nand the invasion front. \n   - CD68 Staining: The CD68 stained images (TMA_IHC_TumorCenter_CD68_0.png, \nTMA_IHC_TumorCenter_CD68_1.png, TMA_IHC_InvasionFront_CD68_0.png, \nTMA_IHC_InvasionFront_CD68_1.png) indicate higher macrophage infiltration at the invasion \nfront compared to the tumor center. \nGiven these observations, there is no definitive evidence of perineural invasion in the \nprovided H&E slides. The immune cell infiltration, particularly T-cells (CD3 and CD8), is \nhigher at the invasion front, but there is no dense T-cell infiltration specifically around \nnerves. Macrophage infiltration is also higher at the invasion front. \nTherefore, the answer is: \n[ANSWER: B) Perineural invasion is absent, with sparse T-cell infiltration in perineural \nareas.]\n✅\nTumorCenter_CD3_0\n…\nInvasionFront_CD68_1 \nInvasionFront_CD68_0 \nH&E ROI + \nInvasionFront_CD3_0\nExample 2: In this digital pathology question, mistralsmall requests the histology slide and ROI, as well as 16\nIHC images corresponding to two images for each of 4 markers observed at Tumor Center and Invasion Front.\nOn the other hand, gemma-3-27b only requests the histology ROI and one IHC, which leads to a wrong analysis.\n23\n"}, {"page": 24, "text": "The patient is a 60.5-year-old White female of Non-Spanish, Non-Hispanic ethnicity with a diagnosis of \npancreatic adenocarcinoma. A sample from the patient has been sequenced, and mutation data is available \nfor analysis. Additionally, detailed clinical history, including information on diagnosis, treatments, \nand lab tests, is documented for the period between 60.5 and 61.6 years of age. \n[... previous question about recurrence in the next 12 months ...] \nQuestion: Given that the patient is 61.6 years old, will the cancer progress in the next 12 months? \nA) Yes \nB) No\n<think> \nOkay, let's tackle this question. The user is \nasking if the cancer will progress in the next \n12 months for a 61.6-year-old patient with \npancreatic adenocarcinoma. They provided two \nfiles: timeline0.txt and specimen.txt. \nFirst, I need to recall what information those \nfiles contain. From the previous interaction, \nthe timeline file probably details the patient's \nclinical history, including diagnosis dates, \ntreatments, and any lab results. The specimen \nfile likely has the mutation data from the \nsequenced sample.  \nPancreatic adenocarcinoma is an aggressive \ncancer, but progression can depend on factors \nlike treatment response, genetic mutations, and \noverall health. The user wants to know if \nprogression is expected in the next year.  \nI should check the timeline for recent \ntreatments. If the patient has been on effective \ntherapy, progression might be slower. Also, \nlooking at lab tests around 61.6 years old could \nshow if there are signs of progression like \nrising tumor markers. The specimen data might \nreveal mutations associated with aggressive \nbehavior or resistance to treatment.  \nWait, in the previous question about recurrence, \nthe answer was \"No.\" Maybe the data showed no \nsigns of recurrence yet. For progression, even \nif there's no recurrence, the cancer might still \nprogress. But I need to check the files again.  \n[...] \nLet me proceed to request the files again to \nverify. \n</think> \n[REQUEST: timeline0.txt] [REQUEST: specimen.txt]\nQwen3-32b\nConsidering the patient's current clinical status \nand the information from the provided files, I \nbelieve the cancer will likely progress in the \nnext 12 months. \nReasoning: \nAs previously established, the patient initially \nresponded to chemotherapy, but there are \nconcerning signs suggesting potential disease \nprogression. Specifically: \n*   CA 19-9 Trend: While the CA 19-9 level \ndecreased from 121 to 52 between ages 61.293 and \n61.523, it remains elevated above the initial \ndiagnostic level of 70. This suggests ongoing \ndisease activity. \n*   Indeterminate Scan: The \"indeterminate\" \nfinding on the MR scan at age 61.284 raises \nsuspicion of early recurrence, even though the \nsubsequent scan at 61.523 was reported as showing \nno progression. Indeterminate findings often \nwarrant close monitoring as they can precede \ndefinitive progression. \n*   Molecular Profile: The MSS status and low TMB \nindicate a limited likelihood of response to \nimmunotherapy, reducing treatment options if the \ndisease progresses. \n*   Stage: The patient was diagnosed with stage \n1-3 pancreatic adenocarcinoma. Even with initial \ntreatment response, recurrence rates are \nsignificant in this disease. \nGiven these factors, it is more probable than not \nthat the cancer will progress within the next 12 \nmonths, even if the progression is slow or \ninitially subtle. \n[ANSWER: A) Yes]\nGemma3-27b\n❌\n<think> \n[...] \nThe CA 19-9 levels fluctuate but don't show a \nconsistent upward trend. The CEA levels are also \nrelatively stable. The most recent data point is \nat 61.523, which is about 0.9 months before the \ncurrent age of 61.6. Since the question is about \nthe next 12 months, the available data only \ncovers up to 0.9 months post-61.6. However, \nbased on the trend up to 61.523, there's no \nevidence of progression.  \n[...] \n</think> \n[ANSWER: B) No]\n✅\n{\"SAMPLE_ID\": \"P-0042530-T01-IM6\", \"PATIENT_ID\": \n\"P-0042530\", \"CANCER_TYPE\": \"Pancreatic Cancer\", \n\"SAMPLE_TYPE\": \"Primary\", \"SAMPLE_CLASS\": \"Tumor\", \n\"METASTATIC_SITE\": \"Not Applicable\", \"PRIMARY_SITE\": \n\"Pancreas\", \"CANCER_TYPE_DETAILED\": \"Pancreatic \nAdenocarcinoma\", \"GENE_PANEL\": \"IMPACT468\", \n\"SAMPLE_COVERAGE\": 570, \"TUMOR_PURITY\": 10.0, \n\"ONCOTREE_CODE\": \"PAAD\", \"MSI_COMMENT\": \n\"MICROSATELLITE STABLE (MSS). See MSI note below.\", \n\"MSI_SCORE\": 0.0, \"MSI_TYPE\": \"Stable\", \n\"SOMATIC_STATUS\": \"Matched\", \"CLINICAL_GROUP_x\": \n\"1B\", \"PATHOLOGICAL_GROUP\": 2.0, \n\"CLINICAL_SUMMARY\": \"Regional Both 2 and 3\", \n\"ICD_O_HISTOLOGY_DESCRIPTION\": \"Adenocarcinoma, \nNos\", \"DIAGNOSIS_DESCRIPTION\": \"Pancreas\", \n\"TMB_NONSYNONYMOUS\": 2.594094285, \n\"DX_DESCRIPTION\": \"ADENOCARCINOMA, NOS\", \n\"START_AGE\": 60.5172, \"EVENT_TYPE\": \"Diagnosis\", \n\"SUBTYPE\": \"Primary\", \"SOURCE\": \"Tumor Registry\", \n\"CLINICAL_GROUP_y\": \"1B\", \"PATH_GROUP\": \"2B\", \n\"STAGE_CDM_DERIVED\": \"Stage 1-3\", \"SUMMARY\": \n\"Regional both 2and 3                              \", \"ICD_10\": \n\"C25.1\"}\nAGE: 60.517, samples… \nAGE: 60.517, diagnosis > diagnosis --> SUBTYPE: … \nAGE: 60.556, diagnosis > cancer_presence --> … \nAGE: 60.556, diagnosis > progression --> CT scan … \nAGE: 60.556, diagnosis > tumor_sites --> SOURCE: … \nAGE: 60.591, surgery --> SUBTYPE: PROCEDURE,  \nAGE: 60.591, specimen_surgery --> SAMPLE_ID: … \nAGE: 60.624, diagnosis > cancer_presence --> … \nAGE: 60.624, diagnosis > progression --> CT scan … \nAGE: 60.627, diagnosis > cancer_presence --> … \nAGE: 60.627, diagnosis > progression --> CT scan … \nAGE: 60.682, diagnosis > performance --> ECOG: 1,  \nAGE: 60.684, treatment > prior_meds --> … \nAGE: 60.684, labtest > ca_19-9_labs --> … \nAGE: 60.684, labtest > cea_labs --> CEA: 2.3000 ng/ml          \nAGE: 60.701, specimen --> SAMPLE_ID: … \nAGE: 60.739, treatment > treatment --> STOP_AGE: … \nAGE: 60.739, treatment > treatment --> STOP_AGE: … \nAGE: 60.739, treatment > treatment --> STOP_AGE: … \nAGE: 60.739, treatment > treatment --> STOP_AGE: … \nAGE: 60.895, labtest > ca_19-9_labs --> RESULT: 39… \nAGE: 60.895, labtest > cea_labs --> CEA: 2.7000 ng/ml \n…\nspecimen.txt\ntimeline0.txt\nExample 3: In this longitudinal case, the question asks about cancer progression in the next months. However,\nprevious questions already asked about cancer recurrence, so gemma-3-27b re-uses the data analysis from\nthe previous question, without requesting for the timeline data or pathological data again. On the other hand,\nqwen3-32b reasons that progression may not require the same analysis than recurrence and requests for the files\nagain, which leads to the correct analysis.\n24\n"}, {"page": 25, "text": "[… previous questions on H&E slides and other tumour markers…] \nQuestion: Which marker shows the lowest proportion of positive cells in both the tumor center and \ninvasion front? \nA) CD3   \nB) CD8   \nC) CD56   \nD) CD68   \nE) PDL1   \nF) MHC1\nTo determine which marker shows the \nlowest proportion of positive cells \nin both the tumor center and the \ninvasion front, I need to examine \nthe IHC stained images for each of \nthe specified markers in both \nregions. \n[Request for all pairs of IHCs for \nall 6 markers]\nMistral-Small\nTo determine which marker shows the \nlowest proportion of positive cells \nin both the tumor center and \ninvasion front, I need to analyze \nthe provided IHC images. \n[Request for all pairs of IHCs for \nall 6 markers] \nI will wait for the images to be \nprovided before proceeding with the \nanalysis.\nMistral-Small (with tools)\nSystem\nTumorCenter_CD3_0, \nTumorCenter_CD3_1, \nInvasionFront_CD3_0, \nInvasionFront_CD3_1 \nAssistant: Based on the analysis \nof the IHC images and the data \nprovided by the IHCTool, the \nmarker that shows the lowest \nproportion of positive cells in \nboth the tumor center and \ninvasion front is MHC1. \n[ANSWER: F]\nBased on the provided IHC images, \nthe marker that shows the lowest \nproportion of positive cells in \nboth the tumor center and the \ninvasion front is PDL1. The images \nfor PDL1 (both _0 and _1) in both \nregions exhibit minimal brown \nstaining, indicating a low \nproportion of positive cells \ncompared to the other markers. \nTherefore, the answer is: \n[ANSWER: E) PDL1]\n✅\n❌\n…\nInvasionFront_MHC1_1 \n[IHCTool: According to IHC \ntool around 23.98% of the \ncells in \nTMA_IHC_TumorCenter_CD3_0.p\nng are positively stained.] \n...\nExample 4: Augmenting LLMs with tools such as foundation models allows for grounded and explainable\npredictions. In this case, the tool-augmented model uses the output of the ABMIL model that quantifies the\npercent of positive cells in a IHC core, which, in contrast to the zero-shot LLM, leads to the correct answer.\n25\n"}, {"page": 26, "text": "G\nFurther Experimental Results\nTask: Digital Pathology\na.\nb.\nTask: Outcome and Recurrence\nc.\nNumber of Files across Modalities Accessed\nTask: Hematology\nMulti-Modal  \nAnalysis\nNumber of Files across Modalities Accessed\nTask: Recurrence\nd.\ne.\nTask: Progression\nTask: Outcome\nf.\nLongitudinal  \nAnalysis\nFigure 6: Distribution of number of files requested for a given backbone LLM per task.\na.\nb.\nMulti-Modal  \nAnalysis\nLongitudinal  \nAnalysis\nFigure 7: Distribution of the number of files accessed per patient. Across models for MTBBench-Multimodal\na. and MTBBench-Longitudinal b. tasks.\nFigure 6 shows the distribution of requested files per model and task. Some tasks show consistent\nfile requests across models: for example, hematology usually requires two files to answer questions,\nnamely patient hematology data and hematology reference ranges. However, some harder tasks like\nOutcome & Recurrence show more variability across models. For example, mistralsmall tends to ask\nfor more files than any other model, which could explain how it reaches a similar accuracy in this\ntask as llama90b, a model that contains almost four times as many parameters (see Fig. 2).\nFigure 7 shows the distribution of number of files accessed per patient. Stronger models tend to access\nmore files per case, reflecting greater use of available modalities, especially in multi-modal settings.\nThis especially supports the analysis of the Example 3, where models benefit from requesting again\ndetached files in the same conversation.\nFigure 8 shows the difference in bootstrap accuracies + TOOLS minus without tools. Smaller models\ntend to benefit more from tool use (e.g. qwen25-7b, llama33-8b, gemma-3-12b, gemma-3-27b,\nqwen25-32b, mistralsmall) especially in Digital Pathology, Outcome/Recurrence, and Progression\ntasks. Notice that mistralsmall does not benefit that much from tool use in Digital Pathology (+0.6),\nwhich could be due to strong vision capabilities. Another notable outlier is qwen25-32b which\nsignificantly decreases performance with tool use in Outcome/Recurrence tasks.\n26\n"}, {"page": 27, "text": "a.\nb.\nMulti-Modal  \nAnalysis\nLongitudinal  \nAnalysis\nFigure 8: Effect (difference) of tool use on bootstrap accuracy for a.\nMTBBench-Multimodal and b.\nMTBBench-Longitudinal.\nH\nOther Prompts\nSystem prompt used at the beginning of each conversation:\nSystem: You are a pathologist AI assistant expert at analyzing patient data and answering user \nquestions. \nYou will be provided with files that you are allowed to read. \nMore files may become available as the conversation progresses. \nTo ask for files, include in your reasoning [REQUEST: filename.extension] for each file you \nneedExample: [REQUEST: image1.jpg] [REQUEST: image2.jpg] \nYou may request multiple files at once if necessary. If you ask for a file wait to receive it from the \nuser. \nTo provide a final answer to a question, include [ANSWER: your answer] in your response. \nYou MUST ONLY provide [ANSWER] when you have all necessary information.\nSystem prompt when tool calling is enabled:\nSystem: You are a pathologist AI assistant expert at analyzing patient data and answering user \nquestions. \nYou will be provided with files that you are allowed to read. \nTo ask for files, include in your reasoning [REQUEST: filename.extension] for each file you \nneedExample: [REQUEST: image1.jpg] [REQUEST: image2.jpg] \nTo provide a final answer to a question, include [ANSWER: LETTER) your answer] in your response, \nspecifying the answer choice you picked (A, B, C, D, E, or F). \nYou MUST ONLY provide [ANSWER] when you have all necessary information.You also have access to a H&E \nfoundation model CONCH that can be used to determine cancer type and NOTHING ELSE. \nTo use CONCH you must provide the H&E image name and extension and a list of options in the format \n[CONCH: filename.extension, (option1 text), (option2 text), ...] with each option surrounded by () \nExample: [CONCH: image1.jpg, (melanoma), (squamous cell carcinoma)] [CONCH: image2.jpg, (option 1), \n(option 2)] \nThe model will then tell you which option resembles the image the most.\n27\n"}]}