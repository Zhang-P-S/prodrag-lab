{"doc_id": "arxiv:2601.22324", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.22324.pdf", "meta": {"doc_id": "arxiv:2601.22324", "source": "arxiv", "arxiv_id": "2601.22324", "title": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems", "authors": ["Silas Ruhrberg Estévez", "Christopher Chiu", "Mihaela van der Schaar"], "published": "2026-01-29T21:11:06Z", "updated": "2026-01-29T21:11:06Z", "summary": "Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.22324v1", "url_pdf": "https://arxiv.org/pdf/2601.22324.pdf", "meta_path": "data/raw/arxiv/meta/2601.22324.json", "sha256": "2aaba3abd3c44fe293672ce8aec5e9fac35c313c00c93efad38c0d4eaa4d32bd", "status": "ok", "fetched_at": "2026-02-18T02:20:07.706943+00:00"}, "pages": [{"page": 1, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nSilas Ruhrberg Est´evez 1 Christopher Chiu 1 Mihaela van der Schaar 1\nAbstract\nModern clinical practice relies on evidence-based\nguidelines implemented as compact scoring sys-\ntems composed of a small number of interpretable\ndecision rules. While machine-learning models\nachieve strong performance, many fail to trans-\nlate into routine clinical use due to misalignment\nwith workflow constraints such as memorabil-\nity, auditability, and bedside execution. We ar-\ngue that this gap arises not from insufficient pre-\ndictive power, but from optimizing over model\nclasses that are incompatible with guideline de-\nployment. Deployable guidelines often take the\nform of unit-weighted clinical checklists, formed\nby thresholding the sum of binary rules, but learn-\ning such scores requires searching an exponen-\ntially large discrete space of possible rule sets.\nWe introduce AgentScore, which performs\nsemantically guided optimization in this space\nby using LLMs to propose candidate rules and\na deterministic, data-grounded verification-and-\nselection loop to enforce statistical validity and de-\nployability constraints. Across eight clinical pre-\ndiction tasks, AgentScore outperforms exist-\ning score-generation methods and achieves AUC\ncomparable to more flexible interpretable models\ndespite operating under stronger structural con-\nstraints. On two additional externally validated\ntasks, AgentScore achieves higher discrimina-\ntion than established guideline-based scores.\n1. Introduction\nClinical decision-making is inherently difficult, requiring\nclinicians to act under uncertainty, time pressure, and incom-\nplete information (Patton, 1978). Over recent decades, clini-\ncal guidelines have pushed medicine toward evidence-based\ncare, moving beyond the opinions of individual clinicians\n(Sur & Dahm, 2011; Wieten, 2018). A central instrument\nin this shift is the clinical scoring system: a compact set of\n1DAMTP, University of Cambridge, Cambridge, UK. Corre-\nspondence to: Silas Ruhrberg Estevez <sr933@cam.ac.uk>.\nPreprint. February 2, 2026.\nexplicit rules mapping a small number of routinely available\npatient measurements to risk strata or management recom-\nmendations (Challener et al., 2019). When well designed,\nsuch scores standardize decisions, support resource allo-\ncation, and facilitate communication across care settings\n(Woolf et al., 1999). From a machine learning perspective,\nthese artifacts are best viewed not as approximate regressors,\nbut as a deliberately constrained model family optimized for\nbedside execution, recall, and auditability (Ustun & Rudin,\n2015).\nDespite their ubiquity, effective scoring systems such as\nCURB-65 (Lim et al., 2003) must satisfy stringent practical\nrequirements (Desai & Gross, 2019; Moons et al., 2015).\nThey must rely on routinely available inputs, generalize\nacross institutions despite missingness and measurement\nshift (Dambha-Miller et al., 2020), and remain interpretable\nand memorable for reliable bedside recall and audit without\ncomputational aids (Graham et al., 2011). Meeting these\nrequirements in practice remains challenging. Most widely\nused scores are derived from expert consensus or manual\nanalysis of observational studies (Woolf et al., 1999), often\nvia regression models discretized for bedside use (see Fig. 1)\n(Sullivan et al., 2004). This process is labor-intensive, slow\nto update, and can yield brittle feature and threshold choices\nthat are difficult to revise (Wasylewicz & Scheepers-Hoeks,\n2018; Woolf et al., 1999).\nClinical data\nDevelopment methods\nClinical scoring system\nConfusion\nUrea > 7 mmol/l\nRespiratory rate > 30\nBlood pressure under 90/60 mmHg\nAge > 65 years\n1\nManual curation\n2\n3\nClassical ML\nAgentScore\nAI\nProposed rule\nCURB-65 \nScore: 3\nExpert consensus\nDownstream\nvalidation\nLLM Agent\nAnalysis \ntools\nIncreased\nmortality risk\nHospital\nadmission\nValidation\nCURB-65 Score\nAI\nBedside usage\nt-SNE1\nt-SNE2\n-30\n-30\n-20\n-20\n-10\n-10\n0\n  0\n10\n 10\n20\n 20\n30\n 30\nManual\ncuration\nFigure 1. Clinical scoring systems: Guideline artifacts are com-\npact, explicit checklists intended for reliable manual use.\nIn parallel, increasingly complex machine learning mod-\nels have achieved strong performance on clinical predic-\ntion tasks (Takita et al., 2025; Killock, 2020; Shickel et al.,\n2018). However, even when accurate and ostensibly inter-\npretable, many such models remain poorly matched to guide-\nline deployment: they rely on inputs that are inconsistently\navailable, require preprocessing and software-mediated in-\n1\narXiv:2601.22324v1  [cs.LG]  29 Jan 2026\n"}, {"page": 2, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nference, and produce continuous-weight computations or\ndecision thresholds that are difficult to execute and audit\nreliably at the bedside (Topol, 2019; Chen et al., 2021). This\nmisalignment is reinforced by optimization incentives. Un-\nconstrained models admit smooth parameterizations and\nefficient gradient-based training, whereas enforcing check-\nlist structure induces a discrete subset selection problem\nwith hard constraints on size and rule form, yielding a non-\nconvex and often NP-hard objective (Ustun & Rudin, 2015;\nBertsimas & Stellato, 2020). As a result, much of modern\nclinical ML implicitly optimizes over model classes that are\nconvenient to train but incompatible with guideline work-\nflows (Shortliffe & Sepulveda, 2018), while simple scores\nintegrate directly into clinical practice (Graham et al., 2011).\nGap. Current clinical ML pipelines often treat deployability\nas a downstream engineering consideration rather than a\nfirst-class modeling constraint (Kelly et al., 2019). While\nrecent work emphasizes interpretability, interpretability\ndoes not guarantee deployability (Lipton, 2018): a de-\ncision tree or sparse logistic regression may be human-\nunderstandable, but in its native form it typically cannot be\nexecuted and audited reliably at the bedside (e.g., floating-\npoint arithmetic, non-memorable thresholds). Conversely,\nexisting integer- and point-score learning methods largely\noptimize over a fixed, static feature matrix, and therefore\nlack a mechanism to systematically construct and search\nover derived guideline-style rules (e.g., temporal trends or\nphysiologic ratios) that can materially improve checklist per-\nformance. A gap therefore remains for methods that jointly\noptimize rule construction and checklist-compatibility con-\nstraints by design.\nContributions\nConceptual. We formalize deployable clinical scoring\nsystems as a constrained model class defined by unit-\nweighted checklists and a clinically motivated rule lan-\nguage supporting temporal patterns, physiologic ratios,\nand shallow compositions, explicitly encoding cognitive\nand operational constraints of bedside decision-making.\nAlgorithmic. We introduce AgentScore, a framework\nthat bridges LLMs and discrete optimization: LLMs nav-\nigate the combinatorial space of semantic rule proposals,\nwhile a deterministic, data-grounded verification loop\nenforces statistical validity and sparsity.\nEmpirical. Across eight clinical prediction tasks span-\nning MIMIC-IV and eICU, AgentScore matches or\nexceeds state-of-the-art score-learning baselines under\nstricter structural constraints, and on two externally vali-\ndated tasks it achieves higher discrimination than estab-\nlished guideline-based scores while remaining suitable\nfor manual execution.\n2. Deployable Clinical Scoring Systems\nWe treat bedside deployability as a hard modeling primi-\ntive rather than an auxiliary constraint. Accordingly, we\nrestrict the hypothesis class itself to scoring systems that\nare natively compatible with clinical guidelines and routine\nbedside execution. Under this view, a clinical score is not\nan approximation to a continuous predictor, but a discrete\ndecision object: a sparse, unit-weighted collection of hu-\nman interpretable binary rules. Its structure is dictated by\noperational and cognitive constraints, including memora-\nbility, auditability, and arithmetic-free use, rather than by\npredictive expressiveness alone.\nProblem setting. Let D = {(Xi, yi)}N\ni=1 denote a dataset\nof patient trajectories, where Xi ∈Rp×Ti is a matrix of p\nclinical variables observed over Ti timepoints for patient i,\nand yi ∈{0, 1} is a binary outcome associated with each\ntrajectory. For deployment, we compute a fixed, deployable\nrepresentation ˜xi = ϕ(Xi) ∈Rp′ that includes both static\nsummaries and predefined temporal transformations; rules\noperate on ˜xi.\nRule-based feature construction.\nA rule is a binary-\nvalued predicate r(˜x) : Rp′ →{0, 1}, corresponding to\na clinical statement, drawn from a restricted, clinically\nmotivated rule language. The candidate rule dictionary is\nR = {rj(˜x)}|R|\nj=1. The clinical checklists and the allowed\nrule families reflect constructs recurring in guideline-based\nscoring systems and bedside decision rules (see Appendix\nA).\nWhy unit-weighted checklists? Clinical scoring systems\nare typically applied under time pressure, interruption, and\nincomplete information, where cognitive load and memora-\nbility are primary bottlenecks (Miller, 1956). Empirically,\nwidely adopted clinical guidelines almost universally take\nthe form of short additive checklists composed of binary\nconditions and small total score ranges. Classical results on\nimproper linear models further show that equal-weighted\nadditive models often perform comparably to optimally\nweighted linear predictors (Dawes, 1979), aligning with\nevidence that simple, transparent heuristics are particularly\neffective under time pressure and uncertainty (Gigerenzer\n& Gaissmaier, 2011). Allowing non-unit weights increases\nmental arithmetic burden and reduces transparency, often\nnecessitating calculators or electronic support. Similarly,\ndeeply nested logic trees require tracking multiple contin-\ngent branches and intermediate states, which exceeds cog-\nnitive limits in bedside settings; as a result, such models\nare rarely adopted without computational mediation. We\ntherefore adopt simplicity as an explicit design objective\nrather than a byproduct of regularization. By deliberately\nrestricting the model class to the simplest structure, unit\nweighted N-of-M checklists, we maximize memorability,\nauditability, and reliable bedside execution.\n2\n"}, {"page": 3, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nRule Language for Deployable Scoring Systems\n(i) Numeric threshold rules.\nr(˜x) = I[˜xk ⊙c],\n⊙∈{>, ≥, <, ≤}, c ∈R.\n(ii) Numeric range rules.\nr(˜x) = I[clow ≤˜xk ≤chigh].\n(iii) Categorical inclusion rules.\nr(˜x) = I[˜xk ∈A].\n(iv) Binary presence rules.\nr(˜x) = I[˜xk = 1].\n(v) Physiological ratio and contrast rules.\nr(˜x) = I[g(˜x) ⊙c],\ng(˜x) ∈\n\u001a ˜xa\n˜xb\n, ˜xa −˜xb\n\u001b\n,\n(vi) Count-based rules.\nr(˜x) = I\n\nX\nj∈J\nrj(˜x) ≥m′\n\n.\n(vii) Logical composition rules. Given base rules rℓL\nℓ=1,\nwe allow shallow Boolean compositions (AND/OR). We\ndefine Depth(r) as the number of logical operations and\nrestrict it to preserve bedside memorability:\nr(˜x) =\n(VL\nℓ=1 rℓ(˜x)\n(AND),\nWL\nℓ=1 rℓ(˜x)\n(OR).\n(viii) Temporal and distributional rules. Temporal\nrules operate on predefined summaries of longitudi-\nnal measurements included in ˜xi = ϕ(Xi): r(˜xi) =\nI[h(Xi) ⊙c].\nh(Xi) ∈\n(\n∆x(t)\nk , x(t)\nk −x(0)\nk\nx(0)\nk\n, max\nt\nx(t)\nk −min\nt\nx(t)\nk\n)\n,\nIn addition, we allow distributional normalizations, in-\ncluding quantile-based rules I[˜xk ⊙Qk(q)], standard-\nized rules I[(˜xk−µk)/σk ⊙z], and percent-change rules\nI[∆xk/xk ⊙γ].\nScore definition. A clinical scoring system selects a subset\nS ⊆R of at most m rules and assigns each a unit weight:\nS(˜x) = P\nrj∈S rj(˜x), yielding a discrete score S(˜x) ∈\n{0, 1, . . . , m}. An integer threshold τ ∈{0, . . . , m} in-\nduces a binary clinical decision rule:\nby(˜x) =\n(\n1,\nS(˜x) ≥τ,\n0,\notherwise.\nThe resulting model is a unit-weighted N-of-M checklist,\nwith M = |S| denoting the number of rules and N = τ\nthe decision threshold. If a required feature ˜xk is miss-\ning at prediction time, the rule evaluates to false, ensuring\nconservative risk assessment.\nOptimization objective. We seek guideline-style scoring\nsystems that maximize empirical clinical utility under de-\nployability constraints:\nmax\nS⊆R\nU(S(˜x), y; Dval)\ns.t.\n|S| ≤m, Depth(r) ≤d\n∀r ∈S.\n(1)\nHere U denotes an empirical, non-convex utility (e.g., AUC,\nnet benefit, or decision-curve utility) evaluated on held-out\ndata. This optimization is NP-hard and non-differentiable\ndue to discrete rule selection and combinatorial structural\nconstraints (Ustun & Rudin, 2015). Unlike continuous re-\nlaxations or surrogate losses, we directly optimize the target\nutility via constrained, agent-guided search, ensuring that\ndeployability constraints are enforced throughout.\nDefinition 2.1 (Deployable Clinical Scoring System). A\ndeployable clinical scoring system is a tuple G = (S, S, τ),\nwhere S ⊆R is a finite set of binary, human-interpretable\nrules, S(˜x) = P\nrj∈S rj(˜x) is an integer-valued score, and\nτ is a decision threshold. The system is deployable if it\nsatisfies:\n1. Parsimony: |S| ≤m for small m.\n2. Interpretability: Each rule corresponds to a clini-\ncally meaningful statement over routinely collected\nvariables.\n3. Memorability: Rules are binary, unit-weighted, and\nshallowly composed, enabling reliable recall and man-\nual application.\n4. Operational deployability: Evaluation requires no\nspecialized computational infrastructure.\n5. Predictive adequacy: The model achieves acceptable\ndiscrimination and calibration on the target population,\nsubject to the above constraints.\nExample: UK CF Registry 1-year Mortality Score.\nAgentScore produces compact, guideline-style check-\nlists. Table 1 shows one representative instantiation of a\none-year mortality score. The resulting score instantiates\nmultiple allowed rule families. Importantly, the listed rules\nare derived constructs rather than raw dataset columns; in-\nstead, they are constructed by AgentScore.\nTable 1. UK CF Registry one-year mortality checklist generated\nby AgentScore. Each satisfied rule contributes one point.\nChecklist rule (satisfied?)\nPoints\nFEV1 predicted ≥15% decline from 5-year best\n+1\nCurrent FEV1 predicted ≤50%\n+1\nCurrent FEV1 predicted ≤30%\n+1\nIV antibiotics: ≥15 days (hospital) or ≥20\ndays (home)\n+1\nHigh-risk threshold\nS(˜x) ≥2\n3\n"}, {"page": 4, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nTable 2. Comparison of representative interpretable score-learning approaches. Prior methods optimize linear, additive, or sequential\nrule models over fixed feature encodings. In contrast, AgentScore targets deployable-by-design, unit-weighted checklist scores by\nsearching a guideline-compatible rule language under hard structural constraints.\nMethod\nScore form\nWeights\nRule language\nSearch / construction\nDerived rule construction\nUnit-weighted unordered checklist\nRiskSLIM\nlinear score\nnon-unit integers\nbinned thresholds / fixed binaries\nMIP over coefficients\n×\n×\nFasterRisk\nlinear score\nnon-unit integers\nbinned thresholds / fixed binaries\napproximate integer optimization\n×\n×\nAutoScore\nadditive score\nnon-negative integers\nbinned thresholds\npipeline (rank →bin →score)\n×\n×\nCORELS\nrule list\nN/A (sequential logic)\nbinarized features / thresholds\nbranch-and-bound search\n×\n×\nAgentScore\nN-of-M checklist\nunit (0/1)\nclinical rules / thresholds\nconstrained search + validation\n✓\n✓\n3. Related Works\nOur work lies at the intersection of (i) interpretable machine\nlearning, (ii) sparse clinical score learning, and (iii) LLM-\nguided rule generation. Extended comparisons are provided\nin Appendix C.\nGeneral interpretable models in healthcare.\nA broad\nclass of methods targets predictive performance while main-\ntaining transparency. Generalized additive models (GAMs)\nsuch as Explainable Boosting Machines (EBMs) (Hastie\n& Tibshirani, 1986; Nori et al., 2019) and sparse linear\nmodels (e.g., LASSO) provide intelligibility via shape func-\ntions or feature selection, and are often used as pragmatic\nbaselines on tabular clinical data. However, interpretability\nalone does not ensure reliable, unaided bedside deployment\n(Lipton, 2018). These models typically yield continuous\ncoefficients (e.g., 0.41×Age), require non-trivial arithmetic,\nor rely on look-up tables for non-linear terms, making un-\naided manual use difficult at the point of care. Moreover,\nthey do not typically enforce the operational constraints\nof guideline artifacts, such as bounded score ranges, unit\nweights, and unordered checklist execution. Rule-based\nmodels such as CORELS (Angelino et al., 2018), optimal\ndecision trees including GOSDT (Hu et al., 2019), and\nBayesian Rule Lists (Letham et al., 2015) offer alternative\nforms of interpretability. However, these models rely on or-\ndered or hierarchical evaluation, in contrast to the unordered,\nadditive checklist structure typical of clinical scoring sys-\ntems. We also include heuristic scoring pipelines such as\nAutoScore (Xie et al., 2020). While AutoScore produces\ninteger-valued point systems, it often produces multi-bin\npoint systems with wider score ranges and non-unit weights,\nwhich can increase cognitive load compared with short unit-\nweighted checklists (Graham et al., 2011).\nSparse clinical score learning.\nDistinct from general in-\nterpretable ML, score-learning methods explicitly construct\ncompact points-based scores intended for manual execu-\ntion. Integer-weighted methods such as RiskSLIM (Ustun\n& Rudin, 2015) and FasterRisk (Liu et al., 2022) formulate\nlearning as integer optimization over sparse models with\nsmall integer coefficients, producing concise linear point\nsystems with strong face validity. This line complements\nclassical regression-to-points pipelines that discretize and\nround coefficients to obtain integer scores, but still typi-\ncally retain non-unit weights and arithmetic burden. How-\never, even short scores can remain operationally challenging\nwhen they use non-unit or signed weights (e.g., +5, −3), in-\ncreasing mental arithmetic burden and error risk under time\npressure. More importantly, these methods typically opti-\nmize over a fixed feature matrix: they can select among pre-\ncomputed columns, but do not systematically construct new\nsemantic rules such as physiologic ratios, temporal changes,\npercentile-based thresholds, or logical compositions unless\nthese are manually engineered a priori. Consequently, se-\nmantic feature engineering and rule design remain manual\nprerequisites, limiting coverage of the combinatorial rule\nspace that clinical guideline authors routinely explore. In\ncontrast to points-based coefficient learning, our target ob-\nject is an unordered clinical checklist with unit weights,\nwhere the primary challenge is discovering and validating\nthe rules themselves rather than optimizing coefficients.\nLLM-guided rule and feature generation.\nLarge lan-\nguage models have been explored for structured discovery,\nincluding feature generation, program synthesis, and guid-\nance in combinatorial search spaces (Nam et al., 2024; Balek\net al., 2025; Liu et al., 2025). In clinical settings, prior work\nreports limitations in robustness, calibration, and guideline\nadherence when LLMs are used as standalone decision-\nmakers (Williams et al., 2024; Artsi et al., 2025). We in-\nstead constrain LLMs to a narrow, verifiable role: proposing\ncandidate rules within a restricted, guideline-compatible\nlanguage that encodes admissible rule families and shallow\ncompositions. All proposals are subsequently screened and\nselected using held-out patient data under explicit deploya-\nbility constraints (unit weights, bounded size, limited depth),\nso LLMs act as semantic proposal mechanisms embedded in\na deterministic, data-grounded optimization loop rather than\nas autonomous decision-makers. This separation of seman-\ntic proposal from statistical verification yields reproducible\nrule selection and ensures that every retained rule is both\ninterpretable and empirically supported. As longitudinal\nEHR signals become increasingly available, the bottleneck\nshifts from data scarcity to synthesizing deployable rules\nfrom routine measurements; we address this by combining\nLLM-guided proposal over a guideline-compatible rule lan-\nguage with deterministic, data-grounded validation under\nhard checklist constraints.\n4\n"}, {"page": 5, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\n1\n2\n3\n4\nDataset initialization\nRule proposal\nRule validation\nScore assembly\nClinical Dataset\nDataset Description\nAnalysis Toolbox\nAI\nRepeat \nfor N \niterations\nHeart Rate > 100 bpm\nPositive PCR\nWeight / Height > 30\nAI\nAI\nAI\nHeart Rate > 100 bpm\nPositive PCR\nWeight / Height > 30\nTool \nselection\nAnalysis \nresults\nRule \nProposal\nRule \nValidation\nAI\nCandidate Rule\nDatabase\nScore Construction\nAgent\nInitial Clinical\nScoring System\nInitial Clinical\nScoring System\nScore Refinement\nAgent\nFinal Clinical\nScoring System\nFigure 2. Overview of AgentScore. An LLM-based proposal agent generates candidate rules from a dataset description and tool-\nmediated aggregate statistics; it never receives patient-level records. Proposed rules are screened by a deterministic validation module\nenforcing statistical performance and grammar-level deployability constraints (e.g., complexity limits, unit weights). Statistically\nadmissible rules are reviewed by a clinical plausibility agent to filter semantically incoherent proposals, after which a score assembly\nagent selects a compact clinical checklist from the retained pool and the evaluation tools choose the operating threshold.\n4. AgentScore\nProblem setup. Learning deployable clinical scoring sys-\ntems imposes three competing requirements. First, candi-\ndate rules must express clinically meaningful semantics,\nincluding derived quantities, temporal patterns, and shallow\ncompositional constructs that are rarely available as explicit\nfeatures in clinical datasets. Second, rule generation must be\nstrictly data-grounded and robust to hallucination, bias, and\nspurious correlations, with every decision evaluated under\nexplicit, verifiable metrics. Third, the resulting scores must\nsatisfy hard deployability constraints while retaining accept-\nable predictive utility. Formally, we seek to learn a unit-\nweighted checklist rule set S = {r1, . . . , r|S|} ⊆R from\na structured rule space R that maximizes predictive utility\nU(S) subject to hard constraints on rule structure and check-\nlist size, i.e., |S| ≤m. This induces a constrained subset\nselection problem over R that is NP-hard, rendering exhaus-\ntive or purely optimization-based approaches intractable at\nthe required level of expressiveness. This intractability is\ndriven by the combinatorial growth of the rule space under\neven modest grammars and depth limits (see Appendix E).\nLLM usage: proposal vs. verification. Injecting semantic\nunderstanding and domain knowledge into rule construction\nnaturally motivates the use of LLMs. However, direct and\nunconstrained application of LLMs is unsuitable in medi-\ncal settings: free-form generation provides no guarantees\nof validity or empirical grounding and introduces signifi-\ncant hallucination risks. Conversely, existing score-learning\npipelines operating over fixed feature encodings are lim-\nited to pre-specified thresholds and binned variables, and\ncannot systematically discover higher-level clinical rules\nwithout exponential enumeration or extensive manual fea-\nture engineering. We therefore introduce AgentScore, a\nframework that treats LLMs not as a decision-maker, but as a\nstructured semantic proposal mechanism embedded within\na deterministic, tool-mediated evaluation loop. This de-\nsign enables tractable search over an expressive, guideline-\ncompatible hypothesis class while enforcing data grounding\nvia tool verification and deployability by construction. Al-\ngorithmically, AgentScore instantiates an LLM-assisted\ncombinatorial optimization procedure in which semantic\nexploration is decoupled from acceptance and optimization,\nyielding auditable, data-driven rule induction.\nOverview.\nAgentScore approximates the resulting\nchecklist learning problem via a decomposition into three\nstages: rule proposal, rule filtering, and score assembly\n(Figure 2). Each stage enforces a disjoint subset of con-\nstraints, enabling tractable optimization over the guideline-\ncompatible rule space. Complete algorithmic specifications,\nincluding pseudocode, tool interfaces, and prompts, are pro-\nvided in Appendix F and Appendix G.\nRule proposal. Candidate rules are proposed by a language-\nmodel agent constrained to emit structured rules from a pre-\ndefined grammar (Appendix A). The agent does not observe\npatient-level data and interacts with the dataset exclusively\nthrough a fixed tool interface exposing feature metadata\nand aggregate evaluation statistics. Proposals are generated\nin small batches per iteration to control exploration of the\ncombinatorial rule space.\nDeterministic validation module. Each proposed rule is\ndeterministically evaluated using the tool interface. The\nvalidation module enforces both statistical performance and\ngrammar-level deployability constraints. A rule r is retained\nonly if it satisfies a minimum discrimination threshold τrule\nand passes redundancy checks with respect to previously\nretained rules:\nAUROC(r) ≥τrule,\nmax\nr′∈P J+(r, r′) ≤δ,\nwhere P denotes the current pool of retained rules, and\nJ+ denotes Jaccard similarity computed over positive-class\ncoverage vectors on Dval. Rules failing either criterion\nare discarded without further consideration. We set τrule\ndeliberately low: because checklists are compact and each\nrule increases cognitive burden, we require every retained\nrule to be individually meaningful while allowing checklist-\nlevel evaluation to select complementary weak signals.\n5\n"}, {"page": 6, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nClinical plausibility agent. All rules passing deterministic\nvalidation are subsequently reviewed by a clinical plausibil-\nity agent, an independent LLM-based self-check. This agent\nevaluates whether a rule corresponds to a coherent and de-\nfensible clinical statement given the variable semantics and\ntransformations used. Rules judged implausible or clinically\nnonsensical are rejected prior to final retention, ensuring\nthat only rules satisfying both statistical and semantic crite-\nria enter the candidate pool. Importantly, this filter is purely\neliminative: it does not propose, modify, or rank rules, and\ncannot introduce new structure into the score.\nScore construction agent. Given the retained pool P, a\nthird agent proposes a checklist S ⊆P with |S| ≤m. We\nemploy an agent for this step rather than an exact solver (e.g.,\nMIP) to encourage semantic diversity. While exact solvers\nmaximize statistical utility, they can select highly correlated\nproxies for the same underlying signal (e.g., multiple vari-\nants of blood pressure). The agent can construct checklists\nthat cover distinct physiological domains (e.g., respiratory,\ncardiovascular, renal), aligning the final score with clini-\ncal reasoning principles for robust multi-organ assessment.\nEach proposed checklist is evaluated deterministically via\nthe tool interface to yield discrimination and coverage met-\nrics. Across proposals and refinement steps, we retain the\nbest-performing checklist on Dval under the target utility U,\nensuring that semantic guidance affects which subsets are\nexplored but not how the final checklist is selected.\nIterative refinement. The score construction agent may\niteratively refine its proposal over a fixed number of steps,\nreceiving updated evaluation metrics after each revision.\nRefinement operations are limited to rule inclusion or exclu-\nsion within the retained pool P. Importantly, the decision\nthreshold τ defining the positive class (i.e., S(˜x) ≥τ) is\noptimized automatically by the evaluation tools by maxi-\nmizing Youden’s J statistic on validation data and is not\nselected by the agent.\n5. Experiments\nWe evaluate the clinical scoring systems learned by\nAgentScore across a diverse set of real-world clinical\nprediction tasks. Our evaluation is structured around four\nquestions:\n(i) Predictive adequacy: Can deployable, unit-weighted\nchecklists learned by AgentScore achieve discrimi-\nnation comparable to interpretable but less deployable\nmachine learning models?\n(ii) Guideline competitiveness:\nHow do the learned\nscores compare to established clinical guidelines under\nrealistic cross-institutional evaluation?\n(iii) Practical deployability: Do the resulting scoring sys-\ntems satisfy the cognitive and operational constraints\nrequired for bedside use?\n(iv) Ablation study: Why is iterative, multi-step agent-\nbased search necessary to achieve effective perfor-\nmance under these constraints?\nUnless otherwise stated, all main experiments use GPT-5\nas the agent backbone. Additional analyses of the learned\nguidelines and comparisons across different LLM back-\nbones are provided in Appendix E.\n5.1. Predictive Performance\nDatasets and Tasks. We evaluate AgentScore on eight\nreal-world clinical prediction tasks derived from the publicly\navailable MIMIC-IV and eICU EHR datasets, spanning mor-\ntality, length-of-stay, and clinically actionable intervention\nprediction. All tasks use only variables routinely available\nat the time of clinical decision-making. We perform 5-fold\ncross-validation, holding out 20% of patients in each fold\nas a test set used exclusively for final evaluation. Reported\nmetrics are computed on held-out test splits and aggregated\nacross folds. For AgentScore, we further split the train-\ning portion, using 20% of the training patients as validation.\nAdditional details are provided in Appendix D.\nBaselines. We compare AgentScore against a compre-\nhensive set of interpretable and score-learning approaches\nrepresenting strong alternatives for clinical risk prediction\nunder varying degrees of structural constraint. These in-\nclude state-of-the-art integer-valued scoring methods such\nas RiskSLIM and FasterRisk, as well as pooled penalized\nlogistic regression (PLR) baselines (Liu et al., 2022). While\nthese methods produce compact and interpretable models,\nthey do not enforce unit weighting, checklist-style execu-\ntion, or bounded logical complexity during learning. As\ninterpretable upper bounds, we additionally evaluate logis-\ntic regression, decision trees, and the AutoScore framework.\nThese models often achieve strong discrimination but rely\non flexible coefficients and inference pipelines that violate\nguideline-style deployability constraints.\nResults. Among score-generation baselines that learn small\ninteger-weighted scores (RiskSLIM, FasterRisk, and PLR\nvariants), AgentScore is competitive and improves over\nthese integer-score methods despite operating under stricter\nchecklist-deployability constraints. We include pooled PLR\nbaselines primarily to illustrate the brittleness of post-hoc\ncoefficient modification in logistic models (e.g., threshold\npooling and rounding), which can substantially degrade\ndiscrimination; by contrast, RiskSLIM and FasterRisk miti-\ngate this brittleness via dedicated integer optimization over\ncoefficients.\nWe additionally report logistic regression, decision trees,\nand the AutoScore pipeline. These methods can achieve\n6\n"}, {"page": 7, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nTable 3. Model performances. AgentScore enforces unit weights, bounded size, and checklist structure, while competing methods do\nnot. Entries are mean ± std. For each dataset, the best-performing score-based method (highest mean AUROC) is shown in bold.\nMethod\nMIMIC AF\nMIMIC COPD\nMIMIC HF\nMIMIC AKI\nMIMIC Cancer\nMIMIC Lung\neICU LOS\neICU Vaso\nMean\nDecision Tree\n0.79 ± 0.01\n0.69 ± 0.01\n0.77 ± 0.02\n0.85 ± 0.00\n0.64 ± 0.01\n0.66 ± 0.00\n0.69 ± 0.00\n0.77 ± 0.00\n0.73 ± 0.07\nLogistic (L1)\n0.77 ± 0.01\n0.68 ± 0.01\n0.76 ± 0.01\n0.78 ± 0.00\n0.64 ± 0.02\n0.65 ± 0.01\n0.69 ± 0.00\n0.75 ± 0.00\n0.72 ± 0.05\nAutoScore\n0.82 ± 0.00\n0.66 ± 0.01\n0.81 ± 0.01\n0.84 ± 0.00\n0.60 ± 0.01\n0.66 ± 0.00\n0.63 ± 0.00\n0.75 ± 0.00\n0.72 ± 0.09\nFasterRisk\n0.75 ± 0.02\n0.62 ± 0.00\n0.73 ± 0.01\n0.76 ± 0.00\n0.57 ± 0.01\n0.54 ± 0.00\n0.70 ± 0.00\n0.75 ± 0.00\n0.68 ± 0.08\nRiskSLIM\n0.75 ± 0.01\n0.62 ± 0.00\n0.74 ± 0.01\n0.80 ± 0.00\n0.58 ± 0.01\n0.56 ± 0.03\n0.62 ± 0.00\n0.64 ± 0.01\n0.66 ± 0.08\nPooled PLR (RDU)\n0.70 ± 0.01\n0.58 ± 0.00\n0.73 ± 0.03\n0.73 ± 0.00\n0.55 ± 0.01\n0.61 ± 0.01\n0.67 ± 0.00\n0.64 ± 0.00\n0.65 ± 0.06\nPooled PLR (RSRD)\n0.70 ± 0.01\n0.58 ± 0.00\n0.70 ± 0.01\n0.76 ± 0.00\n0.55 ± 0.01\n0.59 ± 0.02\n0.62 ± 0.00\n0.64 ± 0.00\n0.64 ± 0.07\nPooled PLR (Rand)\n0.70 ± 0.01\n0.58 ± 0.00\n0.70 ± 0.01\n0.76 ± 0.00\n0.55 ± 0.01\n0.50 ± 0.00\n0.62 ± 0.00\n0.50 ± 0.00\n0.62 ± 0.09\nPooled PLR (RDP)\n0.70 ± 0.01\n0.58 ± 0.00\n0.70 ± 0.01\n0.76 ± 0.00\n0.55 ± 0.01\n0.50 ± 0.00\n0.62 ± 0.00\n0.50 ± 0.00\n0.62 ± 0.09\nPooled PLR (RDSP)\n0.70 ± 0.01\n0.58 ± 0.00\n0.70 ± 0.01\n0.76 ± 0.00\n0.55 ± 0.01\n0.50 ± 0.00\n0.62 ± 0.00\n0.50 ± 0.00\n0.62 ± 0.09\nPooled PLR (RD)\n0.70 ± 0.01\n0.58 ± 0.00\n0.70 ± 0.01\n0.76 ± 0.00\n0.55 ± 0.01\n0.50 ± 0.00\n0.50 ± 0.00\n0.50 ± 0.00\n0.60 ± 0.10\nAgentScore\n0.81 ± 0.01\n0.63 ± 0.03\n0.79 ± 0.01\n0.79 ± 0.02\n0.59 ± 0.01\n0.63 ± 0.00\n0.67 ± 0.00\n0.76 ± 0.00\n0.71 ± 0.08\nhigher AUROC, but they are not checklist-deployable by\ndesign: they rely on learned (non-unit) integer coefficients,\npost-hoc discretization, or model execution requirements\nthat impose arithmetic and operational overhead relative to\nunit-weighted clinical checklists.\nAveraged across all eight tasks, AgentScore significantly\nimproves AUROC over integer score-learning baselines,\nwith mean fold-level gains of +0.046 vs. RiskSLIM and\n+0.031 vs. FasterRisk. Two-sided paired tests on fold-level\nAUROCs (n = 40; Holm–Bonferroni corrected) reject\nequality for all score-based baselines (p < 0.001). Full\nresults are included in Appendix E.\n5.2. Guideline Competitiveness\nIn the previous section, we compared AgentScore\nagainst strong interpretable machine-learning baselines that\ndo not satisfy guideline-style deployability constraints. We\nnow evaluate a different and clinically relevant question:\nwhether data-driven, deployable scoring systems learned by\nAgentScore can compete with established clinical guide-\nlines under out-of-distribution evaluation settings that mimic\nthe validation procedures used prior to clinical deployment.\nEvaluation protocol.\nWe evaluate guideline competitive-\nness under a cross-institutional setting. AgentScore is\ntrained on data from one institution and evaluated on an\nindependent external cohort. For fairness, existing clinical\nguidelines are applied without modification to their rule\nstructure; only the decision threshold is calibrated on the\nsame training data used for AgentScore, isolating the\nquality of the underlying rule sets rather than threshold\nselection.\nResults.\nUsing a PhysioNet ICU 2012 mortality bench-\nmark, we compare AgentScore against the SOFA (Vin-\ncent et al., 1996) and SAPS-I scores (Gall et al., 1984), two\nwidely used ICU risk stratification tools. To further assess\nrobustness across healthcare systems and disease domains,\nwe train AgentScore on a UK cystic fibrosis cohort and\nevaluate performance on an independent Canadian cohort.\nWe compare against commonly used lung-transplantation\neligibility guidelines for identifying high-risk individuals\n(Ramos et al., 2019) and a simple, widely used FEV1-based\nthreshold (FEV1 < 30%) (Ramos et al., 2017). Figure 3\nshows that AgentScore achieves higher discrimination\nthan the established guidelines under external validation.\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nAgentScore\nSAPS-I\nSOFA\n0.701 ± 0.033\n0.660\n0.657\n(A) ICU High Risk Patients\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nAgentScore\nCF Guideline\nFEV  < 30%\n0.677 ± 0.049\n0.539\n0.598\n(B) CF High Risk Patients\nFigure 3. External validation against guidelines. AgentScore\noutperforms clinical guidelines in AUROC (mean ± std over 5\nseeds; guidelines deterministic).\n5.3. Practical Deployability\nWe conducted a structured expert review of score deployabil-\nity with a panel of N = 18 practicing clinicians (89% with\n≥6 years of clinical experience) from six countries. Partic-\nipants evaluated four representative AgentScore check-\nlists alongside matched outputs from FasterRisk and\nparticipants were instructed to assume equal predictive per-\nformance. Across 72 pairwise judgments per question, par-\nticipants significantly preferred AgentScore-generated\nchecklists for alignment with guideline-style reasoning (85%\nvs. 15%; binomial p < 10−9, Cohen’s h = 0.77), ease of\nbedside application (71% vs. 29%; p < 10−3, h = 0.43),\nand deployment preference (81% vs. 19%; p < 10−7,\nh = 0.66). For overall model preference, 67% of clini-\ncians selected AgentScore, 6% selected FasterRisk,\nand 28% reported no preference (χ2(2) = 10.3, p = 0.006).\nFull methodology, question wording, and per-question re-\nsponse distributions are provided in Appendix E.6.\n5.4. Ablation Studies\nWe conduct targeted ablation studies to isolate the contribu-\ntion of key components of AgentScore (see Table 4). In\nthe LLM Only ablation, we replace the constrained proposal–\nevaluation loop with a single unconstrained language model,\n7\n"}, {"page": 8, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nremoving tool-mediated validation and iterative feedback.\nIn the Single-Pass ablation, we disable iterative refinement\nwhile retaining full evaluation tools. We further ablate\nstructural constraints. The No Jaccard variant disables re-\ndundancy filtering based on positive-class Jaccard overlap,\nallowing highly overlapping rules to co-exist. The No Di-\nversity variant disables rule-type diversity enforcement that\nencourages diverse rule family sampling.\nTable 4. Ablation study of AgentScore components. Mean\nAUROC is averaged across datasets, with standard deviation com-\nputed across tasks.\nAblation\nMean AUROC\nStd\nFull AgentScore\n0.71\n0.08\nLLM Only (unconstrained)\n0.59\n0.07\nSingle-Pass (no refinement)\n0.69\n0.07\nNo Jaccard\n0.69\n0.08\nNo Rule Diversity\n0.70\n0.08\n6. Discussion\nDespite high predictive accuracy in retrospective evalua-\ntions, many clinical machine-learning models fail to trans-\nlate into real-world impact. A central reason is persistent\nmisalignment with clinical workflows: complex models\noften depend on dense or non-routinely available inputs,\nspecialized computational infrastructure, and opaque infer-\nence procedures. These barriers are particularly pronounced\nin resource-constrained settings.\nIn this work, we argue that meaningful clinical impact does\nnot always require increasingly large or expressive models.\nInstead, it requires leveraging machine learning to improve\nexisting clinical workflows while respecting the structural,\ncognitive, and operational constraints under which medical\ndecisions are made. From this perspective, clinical scor-\ning systems are not a legacy artifact to be replaced, but\na deliberately constrained hypothesis class optimized for\ndeployment.\nWe introduce AgentScore, a constrained learning frame-\nwork that automatically constructs unit-weighted clinical\nchecklists under explicit deployability constraints. Com-\nputational resources are required only during development;\ndeployment reduces to evaluating a small set of binary rules\nand summing their outputs, enabling bedside use without\ncalculators, servers, or continuous model maintenance. Be-\nyond deployability, AgentScore addresses a key barrier\nto clinical ML adoption: data governance. Rule generation\nis mediated through a restricted tool interface and decou-\npled from direct data access. Large language models never\nobserve raw patient records and interact only through a re-\nstricted tool interface that returns aggregate statistics. This\ncan simplify governance and audit in collaborations where\ndirect access to patient-level data is limited, although aggre-\ngate interfaces still require careful privacy controls.\nMore broadly, our results suggest that much of the pre-\ndictive signal exploited by flexible interpretable models\ncan be retained within a tightly constrained, deployable\nhypothesis class when semantic rule construction, deter-\nministic evaluation, and structured selection are jointly en-\nforced. This challenges the common implicit assumption\nthat clinical machine learning must trade deployability for\nperformance, and highlights constrained optimization over\nguideline-compatible model classes as a promising direction\nfor future research.\nLimitations. Clinical scoring systems necessarily trade\nexpressivity for simplicity. While such systems can stan-\ndardize care and improve outcomes at the population level,\nthey cannot capture all nuances of individual patient tra-\njectories, and clinical judgment remains essential. Since\nAgentScore is less expressive than flexible statistical or\nblack-box models, it is not expected to consistently outper-\nform them in raw predictive performance. Rule discovery\nis also knowledge-bounded: although acceptance is data-\ngrounded via deterministic evaluation, the candidate rules\nare mediated by the LLM and limited by the constructs it\ncan surface; thus, predictive relationships may be missed if\nthey are not semantically accessible to the agent. Further-\nmore, AgentScore prioritizes semantic meaningfulness\nand deployability over formal optimization guarantees and\ndoes not guarantee convergence to a globally optimal check-\nlist, which is generally intractable for this problem class\nanyway. While our framework enforces deployability con-\nstraints, it does not explicitly enforce fairness constraints\nacross demographic subgroups. Like all data-driven meth-\nods, AgentScore may learn rules that reflect historical\nbiases in clinical practice or documentation patterns; formal\nfairness auditing and subgroup validation remain essential\nprior to deployment. Finally, while our framework reduces\nreliance on direct data access during learning, it does not\neliminate the need for careful dataset curation and does not\nby itself guarantee robustness under distribution shift.\nConclusion and clinical perspective.\nAgentScore\ndemonstrates that machine learning can be used not to\nreplace clinical guidelines, but to systematically improve\nthem. By learning compact, interpretable, and auditable\nunit-weighted checklists that align with real clinical work-\nflows, our approach provides a pathway to generate can-\ndidate scoring systems for prospective validation. More\nbroadly, AgentScore exemplifies a framework for con-\nstrained semantic optimization, in which hypothesis class de-\nsign, deployability, and verification are treated as first-class\nmodeling concerns. Similar additive checklist structures\nalready appear in other safety-critical settings, including\naviation safety checklists (e.g., pre-flight procedures) and\nfinancial lending decisions (e.g., credit scoring), suggesting\nbroader applicability beyond healthcare.\n8\n"}, {"page": 9, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nImpact statement\nThis paper presents work whose goal is to advance the field\nof machine learning by proposing a framework for construct-\ning interpretable checklist models under explicit deployabil-\nity constraints. The scoring systems produced in this paper\nare research artifacts only and are not intended for clinical\nuse. As with any model trained on observational data, they\nmay reflect bias or dataset artifacts and could cause harm\nif deployed without expert review, external validation, and\nregulatory oversight. Our aim is to contribute a methodolog-\nical perspective on clinically aligned model design, not to\npromote real-world deployment.\nReferences\nAdebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt,\nM., and Kim, B. Sanity checks for saliency maps. In\nProceedings of the 32nd International Conference on\nNeural Information Processing Systems, NIPS’18, pp.\n9525–9536, Red Hook, NY, USA, 2018. Curran Asso-\nciates Inc.\nAngelino, E., Larus-Stone, N., Alabi, D., Seltzer, M.,\nand Rudin, C. Learning certifiably optimal rule lists\nfor categorical data. Journal of Machine Learning Re-\nsearch, 18(234):1–78, 2018.\nURL http://jmlr.\norg/papers/v18/17-716.html.\nArtsi, Y., Sorin, V., Glicksberg, B. S., Korfiatis, P., Free-\nman, R., Nadkarni, G. N., and Klang, E. Challenges of\nimplementing llms in clinical practice: Perspectives. Jour-\nnal of Clinical Medicine, 14(17):6169, September 2025.\nISSN 2077-0383. doi: 10.3390/jcm14176169. URL\nhttp://dx.doi.org/10.3390/jcm14176169.\nBalek, V., S´ykora, L., Sklen´ak, V., and Kliegr, T. Llm-\nbased feature generation from text for interpretable\nmachine learning.\nMachine Learning, 114(11), Oc-\ntober 2025.\nISSN 1573-0565.\ndoi:\n10.1007/\ns10994-025-06867-1. URL http://dx.doi.org/\n10.1007/s10994-025-06867-1.\nBellomo, R., Ronco, C., Kellum, J. A., Mehta, R. L., and\nPalevsky, P. Acute renal failure – definition, outcome\nmeasures, animal models, fluid therapy and information\ntechnology needs: the second international consensus\nconference of the acute dialysis quality initiative (adqi)\ngroup. Critical Care, 8(4), May 2004. ISSN 1364-8535.\ndoi: 10.1186/cc2872. URL http://dx.doi.org/\n10.1186/cc2872.\nBertsimas, D. and Stellato, B.\nThe voice of op-\ntimization.\nMachine Learning,\n110(2):249–277,\nJuly 2020.\nISSN 1573-0565.\ndoi:\n10.1007/\ns10994-020-05893-5. URL http://dx.doi.org/\n10.1007/s10994-020-05893-5.\nBishop, E. H. Pelvic scoring for elective induction. Obstet-\nrics & Gynecology, 24:266–268, 1964.\nBlatchford, O., Murray, W. R., and Blatchford, M.\nA\nrisk score to predict need for treatment for uppergas-\ntrointestinal haemorrhage.\nThe Lancet, 356(9238):\n1318–1321, October 2000. ISSN 0140-6736. doi: 10.\n1016/s0140-6736(00)02816-6. URL http://dx.doi.\norg/10.1016/S0140-6736(00)02816-6.\nBone, R. C., Balk, R. A., Cerra, F. B., Dellinger, R. P., Fein,\nA. M., Knaus, W. A., Schein, R. M., and Sibbald, W. J.\nDefinitions for sepsis and organ failure and guidelines\nfor the use of innovative therapies in sepsis. Chest, 101\n(6):1644–1655, June 1992. ISSN 0012-3692. doi: 10.\n1378/chest.101.6.1644. URL http://dx.doi.org/\n10.1378/chest.101.6.1644.\nBrunton, S. L., Proctor, J. L., and Kutz, J. N.\nDiscov-\nering governing equations from data by sparse iden-\ntification of nonlinear dynamical systems.\nProceed-\nings of the National Academy of Sciences, 113(15):\n3932–3937, March 2016. ISSN 1091-6490. doi: 10.1073/\npnas.1517384113. URL http://dx.doi.org/10.\n1073/pnas.1517384113.\nCao, K., Xia, Y., Yao, J., Han, X., Lambert, L., Zhang, T.,\nTang, W., Jin, G., Jiang, H., Fang, X., Nogues, I., Li, X.,\nGuo, W., Wang, Y., Fang, W., Qiu, M., Hou, Y., Kovarnik,\nT., Vocka, M., Lu, Y., Chen, Y., Chen, X., Liu, Z., Zhou,\nJ., Xie, C., Zhang, R., Lu, H., Hager, G. D., Yuille, A. L.,\nLu, L., Shao, C., Shi, Y., Zhang, Q., Liang, T., Zhang,\nL., and Lu, J. Large-scale pancreatic cancer detection\nvia non-contrast ct and deep learning. Nature Medicine,\n29(12):3033–3043, November 2023. ISSN 1546-170X.\ndoi: 10.1038/s41591-023-02640-w. URL http://dx.\ndoi.org/10.1038/s41591-023-02640-w.\nCentor, R. M., Witherspoon, J. M., Dalton, H. P., Brody,\nC. E., and Link, K. The diagnosis of strep throat in adults\nin the emergency room. Medical Decision Making, 1(3):\n239–246, August 1981. ISSN 1552-681X. doi: 10.1177/\n0272989x8100100304. URL http://dx.doi.org/\n10.1177/0272989X8100100304.\nChallener, D. W., Prokop, L. J., and Abu-Saleh, O. The\nproliferation of reports on clinical scoring systems: Is-\nsues about uptake and clinical utility. JAMA, 321(24):\n2405, June 2019. ISSN 0098-7484. doi: 10.1001/jama.\n2019.5284. URL http://dx.doi.org/10.1001/\njama.2019.5284.\nChampion, H. R., Sacco, W. J., Copes, W. S., Gann,\nD. S., Gennarelli, T. A., and Flanagan, M. E.\nA re-\nvision of the trauma score.\nThe Journal of Trauma:\nInjury, Infection, and Critical Care, 29(5):623–629,\nMay 1989.\nISSN 0022-5282.\ndoi:\n10.1097/\n9\n"}, {"page": 10, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\n00005373-198905000-00017. URL http://dx.doi.\norg/10.1097/00005373-198905000-00017.\nChen, I. Y., Pierson, E., Rose, S., Joshi, S., Ferryman,\nK., and Ghassemi, M.\nEthical machine learning\nin healthcare.\nAnnual Review of Biomedical Data\nScience, 4(1):123–144, July 2021.\nISSN 2574-3414.\ndoi:\n10.1146/annurev-biodatasci-092820-114757.\nURL\nhttp://dx.doi.org/10.1146/\nannurev-biodatasci-092820-114757.\nChen, L.\nOverview of clinical prediction models.\nAn-\nnals of Translational Medicine, 8(4):71–71, February\n2020. ISSN 2305-5847. doi: 10.21037/atm.2019.11.\n121. URL http://dx.doi.org/10.21037/atm.\n2019.11.121.\nChen, R. T. Q., Rubanova, Y., Bettencourt, J., and\nDuvenaud, D. K.\nNeural ordinary differential equa-\ntions.\nIn Bengio, S., Wallach, H., Larochelle, H.,\nGrauman, K., Cesa-Bianchi, N., and Garnett, R.\n(eds.),\nAdvances\nin\nNeural\nInformation\nProcess-\ning Systems, volume 31. Curran Associates, Inc.,\n2018.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2018/file/\n69386f6bb1dfed68692a24c8686939b9-Paper.\npdf.\nCollins, G. S., Reitsma, J. B., Altman, D. G., and Moons,\nK. G. Transparent reporting of a multivariable prediction\nmodel for individual prognosis or diagnosis (tripod): The\ntripod statement. Annals of Internal Medicine, 162(1):\n55–63, January 2015. ISSN 1539-3704. doi: 10.7326/\nm14-0697. URL http://dx.doi.org/10.7326/\nM14-0697.\nCranmer, M. Interpretable machine learning for science\nwith pysr and symbolicregression.jl, 2023. URL https:\n//arxiv.org/abs/2305.01582.\nDambha-Miller, H., Everitt, H., and Little, P. Clinical scores\nin primary care. British Journal of General Practice, 70\n(693):163–163, March 2020. ISSN 1478-5242. doi: 10.\n3399/bjgp20x708941. URL http://dx.doi.org/\n10.3399/bjgp20X708941.\nDash, M. and Liu, H.\nFeature selection for classi-\nfication.\nIntelligent Data Analysis, 1(1–4):131–156,\n1997. ISSN 1088-467X. doi: 10.1016/s1088-467x(97)\n00008-5.\nURL http://dx.doi.org/10.1016/\nS1088-467X(97)00008-5.\nDawes, R. M.\nThe robust beauty of improper linear\nmodels in decision making.\nAmerican Psychologist,\n34(7):571–582, July 1979.\nISSN 0003-066X.\ndoi:\n10.1037/0003-066x.34.7.571. URL http://dx.doi.\norg/10.1037/0003-066X.34.7.571.\nDesai, N. and Gross, J. Scoring systems in the critically\nill: uses, cautions, and future directions. BJA Education,\n19(7):212–218, 2019. ISSN 2058-5349. doi: 10.1016/\nj.bjae.2019.03.002. URL http://dx.doi.org/10.\n1016/j.bjae.2019.03.002.\nDimmer, A., Baird, R., and Puligandla, P.\nRole of\npractice standardization in outcome optimization for\ncdh. World Journal of Pediatric Surgery, 7(2):e000783,\nMarch 2024.\nISSN 2516-5410.\ndoi:\n10.1136/\nwjps-2024-000783. URL http://dx.doi.org/10.\n1136/wjps-2024-000783.\nDurack, D. T., Lukes, A. S., Bright, D. K., and Service,\nD. E. New criteria for diagnosis of infective endocarditis:\nutilization of specific echocardiographic findings. The\nAmerican Journal of Medicine, 96(3):200–209, March\n1994. ISSN 0002-9343. doi: 10.1016/0002-9343(94)\n90143-0.\nURL http://dx.doi.org/10.1016/\n0002-9343(94)90143-0.\nD’Agostino, R. B., Vasan, R. S., Pencina, M. J., Wolf,\nP. A., Cobain, M., Massaro, J. M., and Kannel, W. B.\nGeneral cardiovascular risk profile for use in primary\ncare: The framingham heart study. Circulation, 117(6):\n743–753, February 2008. ISSN 1524-4539. doi: 10.1161/\ncirculationaha.107.699579.\nURL http://dx.doi.\norg/10.1161/CIRCULATIONAHA.107.699579.\nEsteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter,\nS. M., Blau, H. M., and Thrun, S.\nDermatologist-\nlevel classification of skin cancer with deep neural net-\nworks. Nature, 542(7639):115–118, January 2017. ISSN\n1476-4687.\ndoi: 10.1038/nature21056.\nURL http:\n//dx.doi.org/10.1038/nature21056.\nGall, J.-R. L., Loirat, P., Alperovitch, A., Glaser, P.,\nGranthil, C., Mathieu, D., Mercier, P., Thomas, R., and\nVillers, D.\nA simplified acute physiology score for\nicu patients. Critical Care Medicine, 12(11):975–977,\nNovember 1984.\nISSN 0090-3493.\ndoi: 10.1097/\n00003246-198411000-00012. URL http://dx.doi.\norg/10.1097/00003246-198411000-00012.\nGigerenzer, G. and Gaissmaier, W.\nHeuristic deci-\nsion making.\nAnnual Review of Psychology, 62\n(1):451–482,\nJanuary\n2011.\nISSN\n1545-2085.\ndoi:\n10.1146/annurev-psych-120709-145346.\nURL\nhttp://dx.doi.org/10.1146/\nannurev-psych-120709-145346.\nGoldstein, B. A., Navar, A. M., Pencina, M. J., and Ioanni-\ndis, J. P. A. Opportunities and challenges in developing\nrisk prediction models with electronic health records data:\na systematic review. Journal of the American Medical In-\nformatics Association, 24(1):198–208, May 2016. ISSN\n10\n"}, {"page": 11, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\n1527-974X. doi: 10.1093/jamia/ocw042. URL http:\n//dx.doi.org/10.1093/jamia/ocw042.\nGraham, R., Mancher, M., Miller Wolman, D., Greenfield,\nS., and Steinberg, E. Clinical Practice Guidelines We\nCan Trust. National Academies Press, Washington, DC,\n2011. ISBN 9780309164232.\nGranger, C. B. Predictors of hospital mortality in the global\nregistry of acute coronary events. Archives of Internal\nMedicine, 163(19):2345, October 2003. ISSN 0003-9926.\ndoi: 10.1001/archinte.163.19.2345. URL http://dx.\ndoi.org/10.1001/archinte.163.19.2345.\nHager, P., Jungmann, F., Holland, R., Bhagat, K., Hubrecht,\nI., Knauer, M., Vielhauer, J., Makowski, M., Braren,\nR., Kaissis, G., and Rueckert, D.\nEvaluation and\nmitigation of the limitations of large language mod-\nels in clinical decision-making. Nature Medicine, 30\n(9):2613–2622, July 2024.\nISSN 1546-170X.\ndoi:\n10.1038/s41591-024-03097-1. URL http://dx.doi.\norg/10.1038/s41591-024-03097-1.\nHastie, T. and Tibshirani, R. Generalized additive models.\nStatistical Science, 1(3), August 1986. ISSN 0883-4237.\ndoi: 10.1214/ss/1177013604. URL http://dx.doi.\norg/10.1214/ss/1177013604.\nHofer, I. S., Burns, M., Kendale, S., and Wanderer, J. P.\nRealistically integrating machine learning into clinical\npractice: A road map of opportunities, challenges, and\na potential future. Anesthesia &amp; Analgesia, 130\n(5):1115–1118, May 2020. ISSN 0003-2999. doi: 10.\n1213/ane.0000000000004575. URL http://dx.doi.\norg/10.1213/ANE.0000000000004575.\nHu, X., Rudin, C., and Seltzer, M.\nOptimal sparse\ndecision trees. In Wallach, H., Larochelle, H., Beygelz-\nimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R.\n(eds.),\nAdvances\nin\nNeural\nInformation\nProcess-\ning Systems, volume 32. Curran Associates, Inc.,\n2019.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2019/file/\nac52c626afc10d4075708ac4c778ddfc-Paper.\npdf.\nJohnson, A. E. W., Bulgarelli, L., Shen, L., Gayles,\nA., Shammout, A., Horng, S., Pollard, T. J., Hao,\nS., Moody, B., Gow, B., Lehman, L.-w. H., Celi,\nL. A., and Mark, R. G. Mimic-iv, a freely accessible\nelectronic health record dataset.\nScientific Data, 10\n(1), January 2023.\nISSN 2052-4463.\ndoi: 10.1038/\ns41597-022-01899-x. URL http://dx.doi.org/\n10.1038/s41597-022-01899-x.\nKather, J. N., Heij, L. R., Grabsch, H. I., Loeffler, C., Echle,\nA., Muti, H. S., Krause, J., Niehues, J. M., Sommer, K.\nA. J., Bankhead, P., Kooreman, L. F. S., Schulte, J. J.,\nCipriani, N. A., Buelow, R. D., Boor, P., Ortiz-Br¨uchle,\nN., Hanby, A. M., Speirs, V., Kochanny, S., Patnaik,\nA., Srisuwananukorn, A., Brenner, H., Hoffmeister, M.,\nvan den Brandt, P. A., J¨ager, D., Trautwein, C., Pearson,\nA. T., and Luedde, T. Pan-cancer image-based detection\nof clinically actionable genetic alterations. Nature Can-\ncer, 1(8):789–799, July 2020. ISSN 2662-1347. doi:\n10.1038/s43018-020-0087-6. URL http://dx.doi.\norg/10.1038/s43018-020-0087-6.\nKelly, C. J., Karthikesalingam, A., Suleyman, M., Cor-\nrado, G., and King, D.\nKey challenges for deliver-\ning clinical impact with artificial intelligence.\nBMC\nMedicine, 17(1), October 2019. ISSN 1741-7015. doi:\n10.1186/s12916-019-1426-2. URL http://dx.doi.\norg/10.1186/s12916-019-1426-2.\nKillock, D. Ai outperforms radiologists in mammographic\nscreening. Nature Reviews Clinical Oncology, 17(3):\n134–134, January 2020. ISSN 1759-4782. doi: 10.1038/\ns41571-020-0329-7.\nURL http://dx.doi.org/\n10.1038/s41571-020-0329-7.\nKroenke, K., Spitzer, R. L., and Williams, J. B. W. The phq-\n9: Validity of a brief depression severity measure. Journal\nof General Internal Medicine, 16(9):606–613, Septem-\nber 2001. ISSN 1525-1497. doi: 10.1046/j.1525-1497.\n2001.016009606.x. URL http://dx.doi.org/10.\n1046/j.1525-1497.2001.016009606.x.\nKwong, J. C. C., Erdman, L., Khondker, A., Skreta, M.,\nGoldenberg, A., McCradden, M. D., Lorenzo, A. J., and\nRickard, M. The silent trial - the bridge between bench-\nto-bedside clinical ai applications. Frontiers in Digital\nHealth, 4, August 2022. ISSN 2673-253X. doi: 10.3389/\nfdgth.2022.929508. URL http://dx.doi.org/10.\n3389/fdgth.2022.929508.\nLetham, B., Rudin, C., McCormick, T. H., and Madigan, D.\nInterpretable classifiers using rules and bayesian analysis:\nBuilding a better stroke prediction model. The Annals\nof Applied Statistics, 9(3), September 2015. ISSN 1932-\n6157. doi: 10.1214/15-aoas848. URL http://dx.\ndoi.org/10.1214/15-AOAS848.\nLim, W S an van der Eerden, M. M., Laing, R., Boersma,\nW. G., Karalus, N., Town, G. I., Lewis, S. A., and Mac-\nfarlane, J. T. Defining community acquired pneumo-\nnia severity on presentation to hospital: an international\nderivation and validation study. Thorax, 58(5):377–382,\nMay 2003.\nISSN 0040-6376.\ndoi: 10.1136/thorax.\n58.5.377. URL http://dx.doi.org/10.1136/\nthorax.58.5.377.\nLip, G. Y., Nieuwlaat, R., Pisters, R., Lane, D. A., and\nCrijns, H. J. Refining clinical risk stratification for pre-\n11\n"}, {"page": 12, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\ndicting stroke and thromboembolism in atrial fibrillation\nusing a novel risk factor-based approach. Chest, 137\n(2):263–272, February 2010.\nISSN 0012-3692.\ndoi:\n10.1378/chest.09-1584. URL http://dx.doi.org/\n10.1378/chest.09-1584.\nLipton, Z. C. The mythos of model interpretability. Com-\nmunications of the ACM, 61(10):36–43, September 2018.\nISSN 1557-7317. doi: 10.1145/3233231. URL http:\n//dx.doi.org/10.1145/3233231.\nLiu, J., Zhong, C., Li, B., Seltzer, M., and Rudin, C. Faster-\nrisk: Fast and accurate interpretable risk scores. In Oh,\nA. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),\nAdvances in Neural Information Processing Systems,\n2022. URL https://openreview.net/forum?\nid=xTYL1J6Xt-z.\nLiu, T., Huynh, N., and van der Schaar, M. Decision tree in-\nduction through LLMs via semantically-aware evolution.\nIn The Thirteenth International Conference on Learning\nRepresentations, 2025. URL https://openreview.\nnet/forum?id=UyhRtB4hjN.\nLundberg, S. M. and Lee, S.-I. A unified approach to in-\nterpreting model predictions.\nIn Advances in Neural\nInformation Processing Systems (NeurIPS), 2017.\nMiller, G. A. The magical number seven, plus or minus two:\nSome limits on our capacity for processing information.\nPsychological Review, 63(2):81–97, March 1956. ISSN\n0033-295X. doi: 10.1037/h0043158. URL http://\ndx.doi.org/10.1037/h0043158.\nMolnar, C., K¨onig, G., Herbinger, J., Freiesleben, T.,\nDandl, S., Scholbeck, C. A., Casalicchio, G., Grosse-\nWentrup, M., and Bischl, B. General Pitfalls of Model-\nAgnostic Interpretation Methods for Machine Learn-\ning Models, pp. 39–68.\nSpringer International Pub-\nlishing, 2022. ISBN 9783031040832. doi: 10.1007/\n978-3-031-04083-2 4. URL http://dx.doi.org/\n10.1007/978-3-031-04083-2_4.\nMoons, K. G. M., Altman, D. G., Reitsma, J. B., and\nCollins, G. S.\nNew guideline for the reporting of\nstudies developing, validating, or updating a multi-\nvariable clinical prediction model: The tripod state-\nment. Advances in Anatomic Pathology, 22(5):303–305,\nSeptember 2015. ISSN 1072-4109. doi: 10.1097/pap.\n0000000000000072.\nURL http://dx.doi.org/\n10.1097/PAP.0000000000000072.\nMoons, K. G. M., Damen, J. A. A., Kaul, T., Hooft, L.,\nAndaur Navarro, C., Dhiman, P., Beam, A. L., Van Cal-\nster, B., Celi, L. A., Denaxas, S., Denniston, A. K.,\nGhassemi, M., Heinze, G., Kengne, A. P., Maier-Hein,\nL., Liu, X., Logullo, P., McCradden, M. D., Liu, N.,\nOakden-Rayner, L., Singh, K., Ting, D. S., Wynants, L.,\nYang, B., Reitsma, J. B., Riley, R. D., Collins, G. S.,\nand van Smeden, M. Probast+ai: an updated quality,\nrisk of bias, and applicability assessment tool for pre-\ndiction models using regression or artificial intelligence\nmethods. BMJ, 388:e082505, March 2025. ISSN 1756-\n1833.\ndoi: 10.1136/bmj-2024-082505.\nURL http:\n//dx.doi.org/10.1136/bmj-2024-082505.\nNam, J., Kim, K., Oh, S., Tack, J., Kim, J., and Shin, J.\nOptimized feature generation for tabular data via LLMs\nwith decision tree reasoning. In The Thirty-eighth Annual\nConference on Neural Information Processing Systems,\n2024. URL https://openreview.net/forum?\nid=APSBwuMopO.\nNori, H., Jenkins, S., Koch, P., and Caruana, R. Interpretml:\nA unified framework for machine learning interpretabil-\nity, 2019. URL https://arxiv.org/abs/1909.\n09223.\nOlesen, J., Torp-Pedersen, C., Hansen, M., and Lip, G.\nThe value of the cha2ds2-vasc score for refining stroke\nrisk stratification in patients with atrial fibrillation with a\nchads2 score 0–1: A nationwide cohort study. Thrombosis\nand Haemostasis, 107(06):1172–1179, 2012. ISSN 2567-\n689X. doi: 10.1160/th12-03-0175. URL http://dx.\ndoi.org/10.1160/TH12-03-0175.\nPal, A., Umapathi, L. K., and Sankarasubbu, M. Medm-\ncqa: A large-scale multi-subject multi-choice dataset\nfor medical domain question answering. In Flores, G.,\nChen, G. H., Pollard, T., Ho, J. C., and Naumann, T.\n(eds.), Proceedings of the Conference on Health, In-\nference, and Learning, volume 174 of Proceedings of\nMachine Learning Research, pp. 248–260. PMLR, 07–\n08 Apr 2022. URL https://proceedings.mlr.\npress/v174/pal22a.html.\nPatton, D. D.\nIntroduction to clinical decision making.\nSeminars in Nuclear Medicine, 8(4):273–282, October\n1978. ISSN 0001-2998. doi: 10.1016/s0001-2998(78)\n80013-0.\nURL http://dx.doi.org/10.1016/\ns0001-2998(78)80013-0.\nPollard, T. J., Johnson, A. E. W., Raffa, J. D., Celi, L. A.,\nMark, R. G., and Badawi, O.\nThe eicu collabora-\ntive research database, a freely available multi-center\ndatabase for critical care research. Scientific Data, 5(1),\nSeptember 2018. ISSN 2052-4463. doi: 10.1038/sdata.\n2018.178. URL http://dx.doi.org/10.1038/\nsdata.2018.178.\nRamos, K. J., Quon, B. S., Heltshe, S. L., Mayer-Hamblett,\nN., Lease, E. D., Aitken, M. L., Weiss, N. S., and\nGoss, C. H.\nHeterogeneity in survival in adult pa-\ntients with cystic fibrosis with fev1 under 30 Chest,\n12\n"}, {"page": 13, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\n151(6):1320–1328, June 2017. ISSN 0012-3692. doi:\n10.1016/j.chest.2017.01.019. URL http://dx.doi.\norg/10.1016/j.chest.2017.01.019.\nRamos, K. J., Smith, P. J., McKone, E. F., Pilewski, J. M.,\nLucy, A., Hempstead, S. E., Tallarico, E., Faro, A., Rosen-\nbluth, D. B., Gray, A. L., and Dunitz, J. M. Lung trans-\nplant referral for individuals with cystic fibrosis: Cys-\ntic fibrosis foundation consensus guidelines. Journal of\nCystic Fibrosis, 18(3):321–333, May 2019. ISSN 1569-\n1993. doi: 10.1016/j.jcf.2019.03.002. URL http://\ndx.doi.org/10.1016/j.jcf.2019.03.002.\nRibeiro, M. T., Singh, S., and Guestrin, C.\n”why\nshould i trust you?”:\nExplaining the predictions of\nany classifier.\nIn Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge Dis-\ncovery and Data Mining, KDD ’16, pp. 1135–1144,\nNew York, NY, USA, 2016. Association for Comput-\ning Machinery. ISBN 9781450342322. doi: 10.1145/\n2939672.2939778.\nURL https://doi.org/10.\n1145/2939672.2939778.\nRoyal\nCollege\nof\nPhysicians.\nNational\nearly\nwarning\nscore\n(news)\n2.\nOfficial\nguideline\nreport,\n2017.\nURL\nhttps://www.rcp.\nac.uk/improving-care/resources/\nnational-early-warning-score-news-2/.\nRudin, C. Stop explaining black box machine learning\nmodels for high stakes decisions and use interpretable\nmodels instead.\nNature Machine Intelligence, 1(5):\n206–215, May 2019. ISSN 2522-5839. doi: 10.1038/\ns42256-019-0048-x.\nURL http://dx.doi.org/\n10.1038/s42256-019-0048-x.\nShell, I. G.\nDecision rules for the use of radio-\ngraphy in acute ankle injuries:\nRefinement and\nprospective validation.\nJAMA, 269(9):1127, March\n1993.\nISSN 0098-7484.\ndoi:\n10.1001/jama.1993.\n03500090063034. URL http://dx.doi.org/10.\n1001/jama.1993.03500090063034.\nShickel, B., Tighe, P. J., Bihorac, A., and Rashidi, P. Deep\nehr: A survey of recent advances in deep learning tech-\nniques for electronic health record (ehr) analysis. IEEE\nJournal of Biomedical and Health Informatics, 22(5):\n1589–1604, September 2018. ISSN 2168-2208. doi:\n10.1109/jbhi.2017.2767063. URL http://dx.doi.\norg/10.1109/JBHI.2017.2767063.\nShortliffe, E. H. and Sepulveda, M. J. Clinical decision\nsupport in the era of artificial intelligence. JAMA, 320\n(21):2199, December 2018. ISSN 0098-7484. doi: 10.\n1001/jama.2018.17163. URL http://dx.doi.org/\n10.1001/jama.2018.17163.\nSilva, I., Moody, G., Scott, D. J., Celi, L. A., and\nMark, R. G. Predicting in-hospital mortality of icu pa-\ntients: The physionet/computing in cardiology challenge\n2012.\nComputing in cardiology, 39:245—248, 2012.\nISSN 2325-8861. URL https://europepmc.org/\narticles/PMC3965265.\nSilverman, W. A. and Andersen, D. H. A controlled clinical\ntrial of effects of water mist on obstructive respiratory\nsigns, death rate and necropsy findings among premature\ninfants. Pediatrics, 17(1):1–10, 1956.\nSinger, M., Deutschman, C. S., Seymour, C. W., Shankar-\nHari, M., Annane, D., Bauer, M., Bellomo, R., Bernard,\nG. R., Chiche, J.-D., Coopersmith, C. M., Hotchkiss,\nR. S., Levy, M. M., Marshall, J. C., Martin, G. S., Opal,\nS. M., Rubenfeld, G. D., van der Poll, T., Vincent, J.-\nL., and Angus, D. C. The third international consensus\ndefinitions for sepsis and septic shock (sepsis-3). JAMA,\n315(8):801, February 2016. ISSN 0098-7484. doi: 10.\n1001/jama.2016.0287. URL http://dx.doi.org/\n10.1001/jama.2016.0287.\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J.,\nChung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H.,\nPfohl, S., Payne, P., Seneviratne, M., Gamble, P., Kelly,\nC., Babiker, A., Sch¨arli, N., Chowdhery, A., Mansfield,\nP., Demner-Fushman, D., Ag¨uera y Arcas, B., Webster,\nD., Corrado, G. S., Matias, Y., Chou, K., Gottweis, J.,\nTomasev, N., Liu, Y., Rajkomar, A., Barral, J., Sem-\nturs, C., Karthikesalingam, A., and Natarajan, V. Large\nlanguage models encode clinical knowledge. Nature,\n620(7972):172–180, July 2023. ISSN 1476-4687. doi:\n10.1038/s41586-023-06291-2. URL http://dx.doi.\norg/10.1038/s41586-023-06291-2.\nSteyerberg, E. W., Vickers, A. J., Cook, N. R., Gerds, T.,\nGonen, M., Obuchowski, N., Pencina, M. J., and Kattan,\nM. W. Assessing the performance of prediction models: A\nframework for traditional and novel measures. Epidemiol-\nogy, 21(1):128–138, January 2010. ISSN 1044-3983. doi:\n10.1097/ede.0b013e3181c30fb2.\nURL http://dx.\ndoi.org/10.1097/EDE.0b013e3181c30fb2.\nSullivan, L. M., Massaro, J. M., and D’Agostino, R. B. Pre-\nsentation of multivariate data for clinical use: The fram-\ningham study risk score functions. Statistics in Medicine,\n23(10):1631–1660, April 2004. ISSN 1097-0258. doi:\n10.1002/sim.1742. URL http://dx.doi.org/10.\n1002/sim.1742.\nSur, R. and Dahm, P. History of evidence-based medicine.\nIndian Journal of Urology, 27(4):487, 2011. ISSN 0970-\n1591. doi: 10.4103/0970-1591.91438. URL http://\ndx.doi.org/10.4103/0970-1591.91438.\n13\n"}, {"page": 14, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nTakita, H., Kabata, D., Walston, S. L., Tatekawa, H., Saito,\nK., Tsujimoto, Y., Miki, Y., and Ueda, D. A system-\natic review and meta-analysis of diagnostic performance\ncomparison between generative ai and physicians. npj\nDigital Medicine, 8(1), March 2025. ISSN 2398-6352.\ndoi: 10.1038/s41746-025-01543-z. URL http://dx.\ndoi.org/10.1038/s41746-025-01543-z.\nTeasdale, G. and Jennett, B. Assessment of coma and im-\npaired consciousness. The Lancet, 304(7872):81–84, July\n1974. ISSN 0140-6736. doi: 10.1016/s0140-6736(74)\n91639-0.\nURL http://dx.doi.org/10.1016/\ns0140-6736(74)91639-0.\nTopol, E. J. High-performance medicine: the convergence\nof human and artificial intelligence. Nature Medicine,\n25(1):44–56, January 2019.\nISSN 1546-170X.\ndoi:\n10.1038/s41591-018-0300-7. URL http://dx.doi.\norg/10.1038/s41591-018-0300-7.\nUstun, B. and Rudin, C. Supersparse linear integer models\nfor optimized medical scoring systems. Machine Learn-\ning, 102(3):349–391, November 2015. ISSN 1573-0565.\ndoi: 10.1007/s10994-015-5528-6. URL http://dx.\ndoi.org/10.1007/s10994-015-5528-6.\nUstun, B. and Rudin, C. Learning optimized risk scores.\nJournal of Machine Learning Research, 20(150):1–\n75, 2019. URL http://jmlr.org/papers/v20/\n18-615.html.\nVincent, J.-L. and Moreno, R. Clinical review: Scoring\nsystems in the critically ill. Critical Care, 14(2):207,\n2010. ISSN 1364-8535. doi: 10.1186/cc8204. URL\nhttp://dx.doi.org/10.1186/cc8204.\nVincent, J. L., Moreno, R., Takala, J., Willatts, S.,\nDe Mendonc¸a, A., Bruining, H., Reinhart, C. K., Suter,\nP. M., and Thijs, L. G.\nThe sofa (sepsis-related or-\ngan failure assessment) score to describe organ dysfunc-\ntion/failure: On behalf of the working group on sepsis-\nrelated problems of the european society of intensive\ncare medicine (see contributors to the project in the ap-\npendix). Intensive Care Medicine, 22(7):707–710, July\n1996. ISSN 1432-1238. doi: 10.1007/bf01709751. URL\nhttp://dx.doi.org/10.1007/BF01709751.\nWang, F.\nThe crisis of biomedical foundation models.\nJournal of Biomedical Informatics, 171:104917, Novem-\nber 2025. ISSN 1532-0464. doi: 10.1016/j.jbi.2025.\n104917. URL http://dx.doi.org/10.1016/j.\njbi.2025.104917.\nWasylewicz, A. T. M. and Scheepers-Hoeks, A. M. J. W.\nClinical Decision Support Systems. Springer International\nPublishing, December 2018. ISBN 9783319997131. doi:\n10.1007/978-3-319-99713-1 11.\nURL http://dx.\ndoi.org/10.1007/978-3-319-99713-1_11.\nWelch, J., Dean, J., and Hartin, J. Using news2: an essen-\ntial component of reliable clinical assessment. Clinical\nMedicine, 22(6):509–513, November 2022. ISSN 1470-\n2118. doi: 10.7861/clinmed.2022-0435. URL http://\ndx.doi.org/10.7861/clinmed.2022-0435.\nWells, P., Anderson, D., Rodger, M., Ginsberg, J., Kearon,\nC., Gent, M., Turpie, A., Bormanis, J., Weitz, J., Cham-\nberlain, M., Bowie, D., Barnes, D., and Hirsh, J. Deriva-\ntion of a simple clinical model to categorize patients\nprobability of pulmonary embolism: Increasing the mod-\nels utility with the simplired d-dimer. Thrombosis and\nHaemostasis, 83(03):416–420, 2000. ISSN 2567-689X.\ndoi: 10.1055/s-0037-1613830. URL http://dx.doi.\norg/10.1055/s-0037-1613830.\nWells, P. S., Anderson, D. R., Bormanis, J., Guy, F.,\nMitchell, M., Gray, L., Clement, C., Robinson, K. S.,\nand Lewandowski, B. Value of assessment of pretest\nprobability of deep-vein thrombosis in clinical manage-\nment.\nThe Lancet, 350(9094):1795–1798, December\n1997. ISSN 0140-6736. doi: 10.1016/s0140-6736(97)\n08140-3.\nURL http://dx.doi.org/10.1016/\nS0140-6736(97)08140-3.\nWieten, S. Expertise in evidence-based medicine: a tale\nof three models. Philosophy, Ethics, and Humanities in\nMedicine, 13(1), February 2018. ISSN 1747-5341. doi:\n10.1186/s13010-018-0055-2. URL http://dx.doi.\norg/10.1186/s13010-018-0055-2.\nWilliams, C. Y. K., Miao, B. Y., Kornblith, A. E., and\nButte, A. J.\nEvaluating the use of large language\nmodels to provide clinical recommendations in the\nemergency department.\nNature Communications, 15\n(1), October 2024.\nISSN 2041-1723.\ndoi: 10.1038/\ns41467-024-52415-1. URL http://dx.doi.org/\n10.1038/s41467-024-52415-1.\nWolff, R. F., Moons, K. G., Riley, R. D., Whiting, P. F.,\nWestwood, M., Collins, G. S., Reitsma, J. B., Kleijnen, J.,\nand Mallett, S. Probast: A tool to assess the risk of bias\nand applicability of prediction model studies. Annals of\nInternal Medicine, 170(1):51–58, January 2019. ISSN\n1539-3704. doi: 10.7326/m18-1376. URL http://dx.\ndoi.org/10.7326/M18-1376.\nWoolf, S. H., Grol, R., Hutchinson, A., Eccles, M., and\nGrimshaw, J. Clinical guidelines: Potential benefits, lim-\nitations, and harms of clinical guidelines.\nBMJ, 318\n(7182):527–530, February 1999. ISSN 1468-5833. doi:\n10.1136/bmj.318.7182.527.\nURL http://dx.doi.\norg/10.1136/bmj.318.7182.527.\nXie, F., Chakraborty, B., Ong, M. E. H., Goldstein, B. A.,\nand Liu, N.\nAutoscore: A machine learning–based\nautomatic clinical score generator and its application\n14\n"}, {"page": 15, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nto mortality prediction using electronic health records.\nJMIR Medical Informatics, 8(10):e21798, October 2020.\nISSN 2291-9694. doi: 10.2196/21798. URL http:\n//dx.doi.org/10.2196/21798.\nZhong, D., Wu, Y., Aarons, G. A., Hutchinson, A. M.,\nWong, W. C., Lv, S., Song, Z., Wu, Y., Bishai, D. M.,\nChen, K., Yang, N., Chen, Y., Liu, Z., Yan, L., Zhou,\nP., and Xu, D. R.\nImplementability of clinical prac-\ntice guidelines: the review and development of a com-\nprehensive framework for guideline implementability\n(cfgi).\nImplementation Science Communications, 6\n(1), October 2025.\nISSN 2662-2211.\ndoi: 10.1186/\ns43058-025-00780-3. URL http://dx.doi.org/\n10.1186/s43058-025-00780-3.\n15\n"}, {"page": 16, "text": "Supplementary Material for AgentScore\nA. Clinical scoring systems\nA.1. Why clinical scoring systems matter\nRole in evidence-based medicine and workflow standardization. In routine clinical practice, guidelines function as\noperational coordination mechanisms that determine how evidence is applied across heterogeneous clinicians, settings, and\ntime horizons, often under conditions of uncertainty and time pressure (Welch et al., 2022; Dimmer et al., 2024). Their\neffectiveness depends not only on statistical validity, but on whether recommendations can be executed reliably at the\nbedside using routinely available information, frequently by clinicians with differing levels of training (Graham et al., 2011).\nAs a result, guideline impact in real-world workflows is driven less by marginal improvements in predictive accuracy than\nby the ability to produce decisions that are consistent, auditable, and directly actionable (Zhong et al., 2025).\nClinical scoring systems instantiate these principles by mapping a small number of routinely collected patient features to\ndiscrete risk strata or management recommendations. By coupling bounded integer scores to explicit action thresholds\n(e.g., hospital admission, advanced imaging, antibiotic administration), they provide a shared decision representation that\nsupports triage, escalation, and communication across care settings (Dambha-Miller et al., 2020; Olesen et al., 2012; Vincent\n& Moreno, 2010). Crucially, these scores are embedded directly into care pathways rather than interpreted as unconstrained\npredictive outputs.\nWhy small, explicit rules survive deployment. From a learning-theoretic perspective, clinical deployment induces a\nmismatch between hypothesis classes that are easy to optimize and those that are viable in practice (Wang, 2025). High-\ncapacity models benefit from smooth parameterizations and weak structural constraints, enabling efficient optimization and\nstrong retrospective performance. In contrast, bedside deployment restricts models to discrete, low-capacity, and tightly\nstructured forms that must be executed reliably under time pressure and partial information (Graham et al., 2011).\nUnder these constraints, additional model flexibility often yields diminishing returns with respect to clinically actionable\ndecision quality (Chen, 2020; Steyerberg et al., 2010). Instead, compact rule-based models operate near the boundary of\nminimal sufficient complexity: by excluding degrees of freedom that cannot be supported at deployment time, they exhibit\nimproved robustness to sampling variability, distribution shift, and execution noise (Dash & Liu, 1997; Wasylewicz &\nScheepers-Hoeks, 2018). Their enduring use in clinical practice reflects an implicit optimization objective—maximize\nperformance subject to hard constraints on executability and stability, rather than unconstrained predictive accuracy.\nA.2. Case studies of clinical impact\nClinical scoring systems achieve impact not merely by ranking risk, but by reliably changing clinical decisions through\nexplicit, thresholded actions. Table 5 summarizes the checklist structure of four widely adopted decision rules that illustrate\ncomplementary mechanisms of clinical impact. Across all cases, clinical impact arises from the presence of trusted, explicit\naction thresholds that deterministically link observed findings to management decisions. Their enduring adoption reflects the\nvalue of compact, unit-weighted checklists in converting uncertainty into reliable action, rather than marginal improvements\nin discriminative accuracy alone.\nAcute care escalation tools such as qSOFA (Singer et al., 2016) and CURB-65 (Lim et al., 2003) provide rapid bedside\nidentification of patients at elevated risk of deterioration or mortality. By mapping a small number of routinely available\nobservations to discrete severity thresholds, these scores support timely escalation, admission decisions, and resource\nallocation under conditions of uncertainty. In primary care, the Centor criteria illustrate a complementary mechanism: by\naggregating a small set of binary clinical findings into a bounded checklist, the score supports rational antibiotic prescribing\nand targeted diagnostic testing, reducing unnecessary treatment while maintaining safety. The Ottawa Ankle Rules (Shell,\n1993) demonstrate a distinct but equally important form of impact in emergency medicine, where a conservative binary\nchecklist enables the safe exclusion of fracture and avoids unnecessary imaging, reducing cost and patient burden without\nincreasing missed injuries. For the Ottawa Ankle Rules, the checklist is operationalized as a conservative OR-rule.\n16\n"}, {"page": 17, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nTable 5. Binary checklist structure of representative clinical decision rules. Each system consists of unit-weighted binary conditions\n(+1 if satisfied) whose total score is mapped to an explicit guideline-recommended clinical action.\nScore\nItem\nBinary rule (satisfied?)\nPoints\nqSOFA\nRespiratory rate\nRespiratory rate ≥22 breaths/min\n+1\nBlood pressure\nSystolic blood pressure ≤100 mmHg\n+1\nMental status\nAltered mental status (GCS < 15)\n+1\nThreshold & action\nScore ≥2 ⇒high risk of sepsis; urgent evaluation\nCURB-65\nConfusion\nNew onset confusion (disorientation to person, place, or time)\n+1\nUrea\nBlood urea nitrogen > 7 mmol/L\n+1\nRespiratory rate\nRespiratory rate ≥30 breaths/min\n+1\nBlood pressure\nSystolic < 90 mmHg or diastolic ≤60 mmHg\n+1\nAge\nAge ≥65 years\n+1\nThreshold & action\nScore ≥3 ⇒inpatient management recommended\nCentor\nFever\nHistory of fever or measured temperature ≥38◦C\n+1\nTonsillar exudates\nTonsillar exudates or swelling present\n+1\nCervical lymphadenopathy\nTender anterior cervical lymph nodes\n+1\nCough absence\nAbsence of cough\n+1\nThreshold & action\nScore ≥3 ⇒consider antibiotics or rapid strep testing\nOttawa Ankle Rules\nMalleolar pain\nPain in the malleolar zone\n+1\nBone tenderness (lateral)\nTenderness at posterior edge or tip of lateral malleolus\n+1\nBone tenderness (medial)\nTenderness at posterior edge or tip of medial malleolus\n+1\nWeight bearing (immediate)\nInability to bear weight immediately after injury\n+1\nWeight bearing (ED)\nInability to bear weight for four steps in the hospital\n+1\nThreshold & action\nScore ≥1 ⇒ankle radiography indicated\nA.3. Survey of established guideline scores\nScope. To contextualize our modeling choices, we examined a range of widely adopted clinical scoring systems spanning\nacute care, cardiology, respiratory disease, obstetrics, psychiatry, and trauma (Table 6). We focused on scores that are\nguideline-embedded, widely cited, and routinely used in clinical practice. Despite their diverse clinical contexts and\nhistorical origins, these systems exhibit a remarkably consistent design philosophy, suggesting the presence of shared,\ndomain-agnostic deployment constraints.\nCommon structural properties. Across domains, successful clinical scoring systems share several recurring features: (i) a\nsmall number of easily recalled items (typically 5–10); (ii) reliance on routine, low-cost inputs; (iii) integer or low-cardinality\nscoring that enables manual computation; (iv) bounded score ranges that support clear risk stratification; and (v) explicit\nthresholds tied to concrete management actions. Together, these properties promote robustness to missingness, reduce\ncognitive burden, and enable reliable execution without specialized infrastructure.\nImplications for hypothesis class design. These empirical regularities motivate learning directly within the checklist\nstructures observed in deployed guidelines, rather than approximating them post hoc. In particular, across surveyed systems\nthe median checklist length is five, with the majority of widely deployed scores operating at six or fewer items (Table 6).\nThis concentration at small rule counts supports treating a limited rule budget as a deployability prior rather than a tunable\nperformance parameter. Restricting the rule budget regularizes the hypothesis class, limits cognitive load, and aligns model\nstructure with real-world guideline practice. This is also consistent with classic results on the limits of human working\nmemory, suggesting that compact checklists better match the cognitive constraints of bedside decision-making (Miller,\n1956).\nSimilarly, restricting attention to unit-weighted rules is supported by both clinical convention and classical results on\nimproper linear models, which show that equal-weighted predictors often perform competitively in noisy, low-signal regimes\nwhile exhibiting greater robustness to estimation error (Dawes, 1979). While some successful scores employ non-unit\nweights, their increased arithmetic complexity frequently necessitates reference aids. When a unit-weighted formulation is\nfeasible, it is therefore preferable due to reduced arithmetic burden and more reliable bedside execution.\nTaken together, these considerations justify focusing on compact, unit-weighted checklists as the most expressive hypothesis\nclass that remains reliably deployable under realistic clinical conditions.\n17\n"}, {"page": 18, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nTable 6. Overview of established clinical scoring systems. Widely adopted scores rely on a small number of routine features, integer\nor categorical point allocations, and explicit decision thresholds, enabling memorability and bedside deployment without specialized\ninfrastructure.\nDomain\nScore\n# Items\nRange\nTypical use / threshold\nReference\nInfection / Sepsis\nSIRS\n4\n0–4\n≥2: systemic inflammation\n(Bone et al., 1992)\nThrombosis\nWells (DVT)\n9\n0–9\n≥2: high DVT probability\n(Wells et al., 1997)\nWells (PE)\n7\n0–12.5\n≥4: high PE probability\n(Wells et al., 2000)\nCardiology\nCHA2DS2-VASc\n8\n0–9\n≥2: anticoagulation\n(Lip et al., 2010)\nGRACE\n8\n0–15\nACS mortality risk\n(Granger, 2003)\nFramingham\n9\n0–30\n10-year cardiovascular risk\n(D’Agostino et al., 2008)\nRespiratory\nCURB-65\n5\n0–5\n≥3: inpatient management\n(Lim et al., 2003)\nCentor\n4\n0–4\n≥3: consider antibiotics\n(Centor et al., 1981)\nGastrointestinal\nGlasgow–Blatchford\n8\n0–23\n≥1: urgent endoscopy\n(Blatchford et al., 2000)\nNeurology\nGlasgow Coma Scale\n3\n3–15\n≤8: severe head injury\n(Teasdale & Jennett, 1974)\nPsychiatry\nPHQ-9\n9\n0–27\n≥10: moderate depression\n(Kroenke et al., 2001)\nObstetrics / Neonatal\nBishop Score\n5\n0–13\n≥8: induction readiness\n(Bishop, 1964)\nSilverman–Andersen\n5\n0–10\n≥7: respiratory distress\n(Silverman & Andersen, 1956)\nCritical Care\nSOFA\n6\n0–24\n≥2: organ dysfunction\n(Vincent et al., 1996)\nNEWS2\n6\n0–20\n≥5: urgent clinical review\n(Royal College of Physicians, 2017)\nqSOFA\n3\n0–3\n≥2: sepsis risk trigger\n(Singer et al., 2016)\nTrauma / Imaging\nRTS\n4\n0–8\n≥2: trauma team activation\n(Champion et al., 1989)\nOttawa Ankle Rules\n5\n0–5\n≥1: ankle radiography\n(Shell, 1993)\nB. The Clinical Scoring System Rule Grammar\nOur rule language is designed to capture the forms of reasoning that already survive deployment in clinical guidelines: rules\nmust be actionable (paired with clear thresholds), auditable (each condition can be inspected and contested), compute-free\nat the bedside (no continuous inference pipeline), and compatible with routine documentation and measurement practices\n(Woolf et al., 1999; Graham et al., 2011; Shortliffe & Sepulveda, 2018; Rudin, 2019; Moons et al., 2015). Accordingly, we\nrestrict attention to a small set of rule families that recur across widely adopted scoring systems.\n(i) Numeric thresholds: cutpoints as operational triggers. Clinical guidelines frequently operationalize physiologic risk\nthrough thresholded cutpoints that map directly to escalation actions. For example, CURB-65 uses a urea cutpoint (blood\nurea nitrogen > 7 mmol/L) and a respiratory rate cutpoint (≥30 breaths/min) to define higher-severity pneumonia and\nguide inpatient management (Lim et al., 2003). Similarly, qSOFA uses binary thresholds (e.g., respiratory rate ≥22/min;\nsystolic blood pressure ≤100 mmHg) as a sepsis risk trigger (Singer et al., 2016). In practice, such thresholds are robust to\nmoderate measurement noise and are easy to execute at the bedside, motivating the inclusion of single-variable threshold\nrules as a primitive in R.\n(ii) Numeric ranges: normality bands and safety windows. Many physiologic variables exhibit both low- and high-risk\nregimes, and guidelines often reason in terms of safe bands or abnormality windows rather than monotone risk. Early\nwarning scores, including NEWS2, discretize vital signs into clinically meaningful ranges (e.g., oxygen saturation bands,\ntemperature bands, and systolic blood pressure bands) that reflect nonlinear risk and guide escalation (Royal College of\nPhysicians, 2017). Range rules therefore provide a compact mechanism to encode “normal vs abnormal” intervals without\nintroducing nontransparent nonlinearities.\n(iii) Categorical inclusion and (iv) binary presence: yes/no clinical facts. Guideline logic often depends on discrete\nclinical facts—diagnoses, comorbidities, and prior events—that are naturally represented as categorical inclusion or binary\npresence rules. For instance, CHA2DS2-VASc assigns points for discrete risk factors such as diabetes mellitus, prior\n18\n"}, {"page": 19, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nstroke/TIA/thromboembolism, vascular disease, and heart failure (Lip et al., 2010; Olesen et al., 2012). Similarly, the\nWells criteria include categorical items such as active cancer and recent immobilization/surgery (Wells et al., 1997). These\nvariables are typically well documented in routine care and map cleanly to checklist items, motivating explicit support for\ncategorical and binary predicates.\n(v) Ratios and contrasts: derived physiology with long precedent. Clinicians frequently reason in derived indices that\nnormalize or compare measurements rather than relying on raw values alone. Although many guideline scores encode\nderived reasoning implicitly via multiple thresholded items, the underlying clinical logic often corresponds to ratios or\ncontrasts (e.g., oxygenation indices such as PaO2/FiO2, perfusion indices, or relative changes between related labs). Our\nlanguage therefore permits a restricted allowlist of simple transformations (ratios and differences) to capture such reasoning\nwhile preserving interpretability and auditability. Importantly, these derived quantities are typically available at no additional\ncognitive or computational cost, as they can be precomputed automatically when the relevant laboratory measurements are\nordered and results returned. We do not allow arbitrary algebraic expressions: only clinically interpretable forms (e.g., xa/xb\nor xa −xb) are permitted, and all derived rules are validated empirically and screened for plausibility by the eliminative\nplausibility agent.\n(vi) Count-based rules: syndromic criteria and cumulative burden. Many clinical definitions and escalation triggers are\ninherently M-of-N criteria: the presence of multiple abnormal findings jointly increases suspicion or defines a syndrome.\nClassic examples include SIRS-style criteria, which combine several abnormal vitals/labs, and organ dysfunction frameworks\nsuch as SOFA, which aggregate multiple organ-specific components (Bone et al., 1992; Vincent et al., 1996). A widely\ntaught diagnostic example is the Duke criteria for infective endocarditis, which require combinations of major and minor\ncriteria to establish or exclude diagnosis, illustrating how guideline reasoning often formalizes evidence accumulation as\ntransparent count-based logic (Durack et al., 1994). Count-based rules preserve transparency because the contributing items\nremain explicit and auditable; they also align naturally with unit-weighted checklist execution by capturing cumulative\nburden without complex arithmetic.\n(vii) Shallow logical composition: small conjunctions/disjunctions mirror guideline logic. Guidelines often specify\nshort conjunctions and disjunctions that define an abnormality or escalation trigger. For example, CURB-65 assigns a point\nfor hypotension defined as systolic blood pressure < 90 or diastolic ≤60 mmHg (Lim et al., 2003), and the Wells criteria\ninclude a competing-diagnosis clause (“alternative diagnosis at least as likely”) that explicitly modifies the risk assessment\n(Wells et al., 1997). We therefore allow shallow AND/OR compositions over base predicates while restricting logical depth\nto preserve memorability and reduce execution error under time pressure (Miller, 1956; Gigerenzer & Gaissmaier, 2011).\n(viii) Temporal and distributional summaries: capturing acuity and trend without black-box time-series. Clinical\ndeterioration is often defined by trends rather than static values (e.g., rapid worsening, sustained abnormalities, or recent\nchanges). In practice, many guideline tools operationalize time indirectly through repeated observations and escalation\nthresholds (e.g., repeated NEWS2 scoring over time) (Royal College of Physicians, 2017). To capture such reasoning while\nremaining compute-free at deployment, we restrict temporal constructs to simple, auditable summaries over a pre-index\nwindow (e.g., delta, percent change, range, or max–min). Canonical examples include acute kidney injury (AKI), where\ndiagnostic criteria depend on changes in serum creatinine over time (e.g., a rise from baseline within a short window)\nrather than a single absolute measurement (Bellomo et al., 2004). Similar trend-based reasoning appears in myocardial\ninfarction evaluation, where serial troponin measurements are interpreted through rises and/or falls over time, and in\nsepsis management, where lactate clearance (a decrease in lactate over a specified interval) is used to assess response\nto resuscitation and ongoing risk. More broadly, clinicians routinely monitor temporal patterns such as the fever curve\n(persistence or trajectory of temperature) when assessing infection progression or treatment response. All temporal features\nare computed using only measurements available prior to the prediction time to avoid label leakage.\nClinically meaningful discretization. To avoid spurious cutpoints and improve interpretability, continuous thresholds\nare quantized to clinically meaningful values (e.g., integer vitals, common lab units, or guideline-style bands), consistent\nwith classical regression-to-points approaches used in clinical score construction (Sullivan et al., 2004). This discretization\nstabilizes rules under sampling variability and supports reliable bedside execution.\nMissingness and measurement frequency. EHR data are characterized by irregular sampling and informative missingness.\nTo ensure deployability and avoid hidden imputation effects, we define explicit missingness semantics for each rule family\nand report them alongside learned checklists. Concretely, if a required input is missing at prediction time, the rule evaluates\nto false by default, avoiding hidden imputation and preserving conservative, audit-friendly behavior under missingness.\n19\n"}, {"page": 20, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nC. Extended Related Work\nC.1. Clinical scoring systems and score construction\nExpert-designed scoring systems.\nA large fraction of widely used clinical scores are developed through expert consensus\nand guideline processes rather than direct optimization on large datasets. Such scores typically encode domain knowledge\nand pragmatic workflow constraints (e.g., reliance on routinely available measurements, bounded item counts, and actionable\nthresholds) and thus benefit from strong face validity, institutional trust, and straightforward bedside execution. In evidence-\nbased medicine, these properties are not incidental: guidelines function as coordination mechanisms that standardize\ndecisions across heterogeneous clinicians and settings, and therefore prioritize auditability and actionability alongside\npredictive utility (Woolf et al., 1999). The limitations are equally well known: expert-derived scores can be conservative,\nslow to update as evidence evolves, and may underperform in new populations or under distribution shift, especially when\nderived from small cohorts or historical practice patterns (Challener et al., 2019).These concerns are now formalized in\nclinical prediction model methodology through reporting and bias-assessment frameworks such as TRIPOD and PROBAST\nwhich emphasize transparency, applicability, and robustness under dataset shift (Collins et al., 2015; Wolff et al., 2019).\nRegression-based score derivation and rounding.\nA classical approach to score construction fits a regression model\n(often logistic regression) and converts continuous coefficients into an integer point system via scaling and rounding, as\nexemplified by Framingham-style risk scores (Sullivan et al., 2004). This pipeline preserves the additive structure of\nlinear predictors while producing a human-usable point tally, and it is often accompanied by calibration to produce risk\nestimates. However, coefficient-to-point conversion introduces additional approximation error, and the resulting point\nweights are typically non-unit and heterogeneous, imposing arithmetic burden and reducing memorability. Moreover,\nestimated coefficients may be unstable under sampling variability, collinearity, and missingness mechanisms, leading to\nsensitivity in the derived point system and thresholds across cohorts (Sullivan et al., 2004; Moons et al., 2015). These issues\nare particularly salient when scores are intended for manual computation or when guideline governance requires stable,\ntransparent rules that remain sensible across institutions.\nOptimization-based integer scoring methods.\nRecent work has formalized score construction as an optimization\nproblem over sparse integer-weighted linear models, enabling direct control over sparsity, coefficient bounds, and (in some\ncases) monotonicity or sign constraints. Representative examples include AutoScore, which combines feature ranking and\ndiscretization with scorecard-style point assignment; RiskSLIM, which solves a mixed-integer program to optimize a sparse\ninteger score; and FasterRisk, which provides a scalable approximation procedure for learning sparse risk scores under\ncoefficient and sparsity constraints (Xie et al., 2020; Ustun & Rudin, 2015; Liu et al., 2022). These methods offer substantial\nadvantages over ad hoc rounding: they search over discrete coefficient spaces directly and provide clearer objective-driven\ntrade-offs. Nonetheless, they remain fundamentally selectors over a pre-specified feature matrix; in practice, this requires\ncommitting to a fixed set of binned threshold indicators and does not address the generation-selection gap when the clinically\nmeaningful hypothesis class includes derived quantities, temporal summaries, or shallow compositional logic that are not\nexplicitly available as features. Further, even when coefficients are small integers, heterogeneous weights typically require\nnontrivial arithmetic at the bedside and do not inherently yield checklist-style N-of-M decision procedures.\nWhy integer coefficients alone are insufficient.\nWhile integer coefficients increase interpretability relative to continuous\nweights, integer does not imply deployable-by-design. These human-factors limitations are increasingly recognized in\nclinical AI evaluation guidelines, which stress that executable decision procedures must be assessed in context of use\nrather than solely via retrospective accuracy metrics (Moons et al., 2025; Kwong et al., 2022). Heterogeneous point values\nstill impose cognitive load and increase the risk of arithmetic errors under interruption and time pressure, and they often\nrequire external aids (charts, calculators, or EHR integration) for reliable execution. There are, however, settings where\nheterogeneous integer weights are entirely appropriate: for example, in longitudinal risk stratification and care planning\nwhere decisions are made with more time, or when scores are routinely pre-computed by software (e.g., EHR dashboards\nor clinical decision support systems), so the human does not perform arithmetic at the point of care. In these contexts,\nthe added flexibility of non-unit integer weights can improve calibration or risk separation without materially increasing\nworkflow burden. The central issue is therefore not whether weights are integer, but whether the computation and decision\nprocedure are bedside-executable under realistic operating conditions.\nIn contrast, unit-weighted checklists align with how clinicians frequently reason in practice: as short sets of explicit triggers\nwhere the contribution of each rule is identical and the decision reduces to a count exceeding a threshold. This design is\n20\n"}, {"page": 21, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nalso supported by classical results on so-called improper linear models, which show that equal-weighted predictors can\nbe competitive in noisy regimes and may be more robust to estimation error than finely tuned weights (Dawes, 1979).\nFrom a human-factors standpoint, bounded item counts and simple aggregation rules (including N-of-M procedures) help\naccommodate working-memory constraints and facilitate consistent application in high-stakes environments (Miller, 1956;\nGigerenzer & Gaissmaier, 2011). Finally, integer-score learners typically inherit the expressivity limits of their pre-binning:\nif clinically meaningful constructs are absent from the initial feature representation, coefficient optimization alone cannot\nrecover them.\nC.2. Interpretable machine learning\nInherently interpretable models.\nInherently interpretable models aim to make the full predictive mechanism transparent,\ncommonly including generalized linear models, decision trees, sparse linear models, and rule-based classifiers. Logistic\nregression and linear models offer additive structure and straightforward coefficient interpretation; shallow decision trees\nprovide explicit decision paths that can be inspected and audited. More specialized rule learners include Bayesian Rule\nLists (Letham et al., 2015) and constraint-based rule sets (e.g., CORELS (Angelino et al., 2018)), which produce compact\nif–then structures with probabilistic or combinatorial learning procedures. These approaches provide important baselines for\ninterpretability and can be audited post hoc, but they often remain misaligned with bedside execution requirements: even\nsparse models may depend on dense feature sets, require preprocessing pipelines, or produce real-valued computations that\nare not manually executable. Moreover, many interpretable ML methods optimize within model classes (linear scores, trees,\nrule lists) that do not directly correspond to guideline-style checklist workflows, and thus may improve transparency without\ndelivering deployability (Rudin, 2019; Molnar et al., 2022).\nPost-hoc explanations and their limitations.\nSimilarly, post-hoc explanation methods (e.g., SHAP, LIME) can provide\nuseful diagnostic insight into individual predictions, but they do not constrain model behavior or guarantee stability\nunder distribution shift (Ribeiro et al., 2016; Lundberg & Lee, 2017; Adebayo et al., 2018). More fundamentally, post-\nhoc explanations can be faithful only locally, can vary across equivalent representations, and may give a false sense\nof transparency when the underlying decision boundary remains complex (Lipton, 2018; Rudin, 2019). For guideline\ndeployment, the central requirement is not merely that a prediction can be explained, but that the decision procedure is itself\nsimple, auditable, and reliably executable under real-world constraints.\nSymbolic regression and rule learning.\nSymbolic regression and equation discovery methods learn structured mathemat-\nical expressions from data, including frameworks such as SINDy (Brunton et al., 2016) and modern genetic-programming\nsystems (e.g., PySR) (Cranmer, 2023). These methods aim to recover compact functional forms with scientific interpretabil-\nity and have been successful in low-dimensional dynamical systems discovery. However, their objectives and constraints\ntypically differ from clinical score design: they target continuous functional relationships rather than discrete, actionable\ndecision rules, and they can exhibit instability in high-dimensional, noisy, and heavily confounded observational data. More\nbroadly, black-box time-series models (including neural ODE variants and deep sequence models) can capture rich temporal\nstructure but are rarely compatible with bedside execution and often require substantial computational infrastructure, making\nthem poor substitutes for guideline-style scoring systems when the deployment target is a human-executable protocol (Chen\net al., 2018). Our work is complementary: we borrow the idea of structured hypothesis classes, but we focus on a constrained,\nchecklist-oriented rule language that is explicitly aligned with clinical workflows.\nLarge Language Models in Clinical Applications.\nA rapidly growing literature studies LLMs for clinical documentation\n(note/discharge summary drafting), information extraction, triage and diagnostic suggestion, and medical question answering\n(Singhal et al., 2023; Pal et al., 2022). While these results indicate substantial medical knowledge and reasoning capacity,\nmultiple studies emphasize that benchmark accuracy alone is insufficient for autonomous clinical decision-making: LLMs\ncan hallucinate, exhibit instruction and context-order sensitivity, and remain difficult to calibrate and integrate into real\nworkflows, with additional constraints from privacy and governance when patient data must be transmitted to external\nservices (Hager et al., 2024; Williams et al., 2024). AgentScore differs fundamentally from direct LLM-based prediction:\nthe LLM is used only during development as a constrained proposal mechanism to explore a semantic rule space, while all\nacceptance is determined by deterministic, data-grounded verification (and a conservative eliminative plausibility filter).\nThe deployed artifact is a static, deterministic unit-weighted checklist that requires no LLM at inference time, avoiding the\nlatency, cost, and reliability risks of bedside LLM invocation and ensuring that final decisions are induced by validated rules\non data rather than free-form model generation.\n21\n"}, {"page": 22, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nC.3. Clinical model deployment and workflow integration\nDeployment challenges in clinical ML.\nA recurring theme in clinical machine learning is the gap between retrospective\nperformance and real-world impact. Predictors trained on EHR data can fail under dataset shift, evolving clinical practice,\nmissingness, and changes in coding or documentation. Practical deployment further requires robust integration into clinical\nworkflows, governance, monitoring, and often regulatory review, all of which impose constraints beyond standard ML\nbenchmarks. These challenges are widely documented and have motivated calls for rigorous evaluation, transparent reporting,\nand human-centered design in clinical decision support (Goldstein et al., 2016; Kelly et al., 2019; Hofer et al., 2020).\nModels aligned with guideline workflows.\nA promising direction is to align model outputs with existing clinical decision\npoints rather than attempting to replace workflows end-to-end. In several domains, ML systems act as perception or\nmeasurement enhancers (e.g., image-based classification in dermatology, computational pathology, or radiology) (Kather\net al., 2020; Esteva et al., 2017; Cao et al., 2023), while the downstream decision logic remains anchored in established\nclinical pathways and guidelines. This separation between measurement and decision can improve adoption: the model\naugments an upstream signal, and clinicians retain control over guideline-aligned thresholds and actions. This paradigm\nsuggests that the most deployable ML systems may be those that explicitly target the model classes and interfaces used in\nroutine practice.\nInterpretability versus deployability.\nFinally, it is important to distinguish interpretability from deployability. Inter-\npretability concerns whether humans can understand the rationale for a model’s outputs; deployability concerns whether\nthe model can be executed reliably within the operational constraints of the target setting. A model can be interpretable\nyet non-deployable if it requires non-routine inputs, complex computations, or specialized infrastructure; conversely, a\ndeployable guideline score may be less expressive yet succeed because it is auditable, memorable, and tightly coupled to\nactionable thresholds. This distinction motivates constraining the hypothesis class to guideline-compatible forms rather than\noptimizing flexible predictors and attempting to explain or simplify them post hoc (Rudin, 2019). AgentScore opera-\ntionalizes this principle by searching over a clinically motivated rule language while enforcing hard structural constraints\nthat ensure the learned artifact is executable at the bedside. This distinction is increasingly reflected in clinical AI reporting\nguidance, which separates transparency of model internals from suitability for real-world execution (Moons et al., 2025).\nIn summary, prior approaches are typically either deployable but manually designed (e.g., CURB-65), or data-driven but\nnot checklist-deployable and not generative over semantic rules (e.g., regression-to-points, RiskSLIM/FasterRisk, trees),\nwhereas AgentScore is both deployable-by-design and learns the rules themselves (see Table 7).\nTable 7. At-a-glance comparison. Interpretable vs. deployable vs. rule-generation capability, with representative examples.\nApproach\nInterpretable\nChecklist-deployable\nGenerates rules\nExample\nExpert design\n✓\n✓\n✓(manual)\nCURB-65\nRegression + rounding\n✓\npartial\n×\nFramingham\nRiskSLIM / FasterRisk\n✓\npartial\n×\n—\nDecision trees\n✓\n×\n×\nCART\nAgentScore\n✓\n✓\n✓(automated)\nThis work\nD. Experimental details\nD.1. Datasets\nWe evaluate AgentScore on ten prediction tasks spanning five data sources:\n1. MIMIC-IV (v3.1) (Johnson et al., 2023): A de-identified electronic health record (EHR) database from Beth Israel\nDeaconess Medical Center containing over 400,000 hospital admissions.\n2. eICU Collaborative Research Database (v2.0) (Pollard et al., 2018): A multicenter critical care database comprising\nICU stays from 208 hospitals across the United States.\n22\n"}, {"page": 23, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\n3. UK Cystic Fibrosis (CF) Registry: A national registry containing annual longitudinal follow-up records for individuals\nwith cystic fibrosis in the United Kingdom.\n4. Canadian Cystic Fibrosis Registry: A national population-based registry used for external validation of CF mortality\nprediction.\n5. PhysioNet Challenge 2012 (Silva et al., 2012): A publicly available ICU mortality benchmark comprising 8,000\npatient episodes from two hospitals.\nTask definitions.\nTable 8 summarizes outcome definitions, prediction horizons, index times, and inclusion criteria. We\nprovide additional clarifications below to ensure precise reproducibility.\nObservation windows.\nFor all time-series tasks, only measurements recorded strictly before the prediction horizon are\nused to prevent information leakage:\n• MIMIC AF, HF, AKI, COPD, pneumonia (lung), lung cancer (cancer), and ICU mortality tasks use laboratory and\nvital-sign measurements up to 245 hours from hospital admission or ICU intime.\n• MIMIC length-of-stay tasks use measurements from the first 24 hours only.\n• eICU tasks use measurements from the first 6 hours of ICU admission.\nOutcome construction.\nMortality outcomes correspond to binary in-hospital death unless otherwise specified. Length-of-\nstay (LOS) outcomes are defined using clinically meaningful thresholds: ICU LOS >3 days; LOS >5 days for COPD and\nlung cancer; and LOS >7 days for pneumonia. For cystic fibrosis cohorts, the outcome is defined as death within a fixed\nprediction horizon following the index annual visit.\nMissing data handling.\nTime-series variables are imputed using forward fill; if no prior measurement exists within the\nobservation window, backward fill is applied when later measurements are available. For baselines that do not operate on\ntemporal data, the final observed value per variable is used with median imputation. For AgentScore, if a required input\nis missing at prediction time, the rule evaluates to false.\nTable 8. Task definitions. Outcome definitions, prediction horizons, index times, and key inclusion criteria for each prediction task.\nTask\nOutcome\nHorizon\nIndex Time\nInclusion Criteria\nMIMIC-AF\nIn-hospital mortality\nStay\nAdmission\nAdults with AF (ICD-9: 42731; ICD-10: I48.*)\nMIMIC-HF\nIn-hospital mortality\nStay\nAdmission\nAdults with heart failure (ICD-9: 428.*; ICD-10: I50.*)\nMIMIC-AKI\nAKI development\nStay\nAdmission\nAdults; AKI per ICD-9: 584.* or ICD-10: N17.*\nMIMIC-COPD\nLOS >5 days\nStay\nAdmission\nAdults with COPD (ICD-9: 491.*, 492.*, 496; ICD-10:\nJ43.*, J44.*)\nMIMIC-Lung\nLOS >7 days\nStay\nAdmission\nAdults with pneumonia (ICD-9: 480–486; ICD-10: J12–\nJ18)\nMIMIC-Cancer\nLOS >5 days\nStay\nAdmission\nAdults with lung cancer (ICD-9: 162; ICD-10: C34.*)\neICU-Vaso\nVasopressor initiation\nDelayed\nICU admission\nAdults; excludes stays <1h\neICU-LOS\nICU LOS >3 days\nStay\nICU admission\nAdults; excludes stays <1h\nCF-UK\nMortality\n3 years\nAnnual visit\nPatients with annual follow-up records\nCF-CA\nMortality\nHorizon-based\nAnnual visit\nPatients with ≥2 annual records\nICU-Mort.\nIn-hospital mortality\nStay\nICU admission\nPhysioNet 2012 cohort\nTrain/validation/test splits.\nWe use 5-fold stratified cross-validation over trajectories (hospital admissions or ICU stays).\nStratification preserves outcome prevalence across folds, and all splits are deterministic with a fixed random seed. For the\nCF cohort comparison, models are trained on the UK CF Registry and externally validated on the Canadian CF Registry to\nassess cross-population generalization. For ICU mortality external validation, models are trained on hospital A and evaluated\non hospital B. Because fold-level splits are unavailable in this setting, results are aggregated over five independent random\ntraining seeds. For AgentScore, the training portion of each fold is further split into 80% used for rule construction and\n20% used for internal validation during score selection.\n23\n"}, {"page": 24, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nStatistical analysis.\nWe evaluate predictive performance using AUROC. For AgentScore, predicted probabilities are\ncomputed as\nˆp = count\nnrules\n,\nwhere count denotes the number of satisfied rules. Statistical significance is assessed using paired tests across cross-validation\nfolds, including both a paired t-test and a Wilcoxon signed-rank test. One-sided alternatives are used with hypothesis H1:\nAgentScore > baseline. In some folds, heavily regularized or discretized baselines fail to produce a non-degenerate\npredictor (e.g., all coefficients collapse to zero). In these cases, AUROC is conservatively set to 0.5 to enable paired\ncomparisons without discarding folds.\nTable 9. Dataset summary statistics. Patients denote unique individuals; observations denote total time-indexed records.\nStatistic\nMIMIC AF\neICU Vaso\neICU LOS\nMIMIC HF\nMIMIC Lung\nMIMIC Cancer\nMIMIC COPD\nMIMIC AKI\nCF-CA\nCF-UK\nICU-Mort.\nPatients\n79,779\n195,339\n195,339\n80,611\n29,180\n8,245\n44,246\n546,028\n7,582\n11,741\n8,000\nFeatures\n74\n60\n60\n74\n74\n74\n74\n75\n107\n132\n42\nObservations\n8.5M\n195K\n195K\n9.2M\n451K\n114K\n614K\n38.3M\n144K\n58.7K\n381K\nPos. (%)\n5.4\n10.7\n25.3\n5.0\n46.2\n44.6\n42.0\n13.4\n32.0\n4.9\n14.0\nFeature binarization.\nScore-learning methods (RiskSLIM, FasterRisk, and PLR variants) require binary indicator features\nto learn integer-valued scoring systems (Liu et al., 2022). For each continuous variable x·,j, we construct a set of binary\nthreshold indicators ˜x·,j,θ = I[x·,j ≤θ]. When the number of unique values in column j is small, we use all unique values as\nthresholds (excluding the maximum to avoid constant predictors). When the number of unique values exceeds a configurable\ncap (default: 1000), we instead use quantile-based thresholds at probabilities {1/q, 2/q, . . . , (q −1)/q} for q = 1000 to\nmaintain computational tractability. Constant dummy columns (all zeros or all ones on the training set) are dropped. This\nthreshold-dummy transform is fitted on training data and applied identically to test data to prevent leakage.\nD.2. Baselines\nWe note that none of the existing score-learning or interpretable modeling approaches natively support the construction\nof unit-weighted N-of-M clinical checklists as produced by AgentScore. Methods such as RiskSLIM, FasterRisk, and\npooled piecewise-linear rule (PLR) variants optimize over sparse linear models with integer or real-valued coefficients, but\ndo not enforce unit weights or checklist-style aggregation. AutoScore can generate integer-valued point systems, but the\nresulting scores are not bounded and may assign large cumulative point totals, which complicates memorability and bedside\nuse in practice.\nTo ensure a fair and meaningful comparison, we restrict all integer-based baselines to the coefficient set {−1, 0, +1} and\nlimit model sparsity to match the maximum number of rules used by AgentScore. This aligns the hypothesis classes as\nclosely as possible while preserving each method’s native optimization procedure.\nSmall integer-based score-learning baselines.\nWe compare against established methods for learning sparse, interpretable\nscoring systems:\n• RiskSLIM (Ustun & Rudin, 2015): A mixed-integer programming (MIP) approach for learning sparse linear models\nwith integer coefficients. We use the CPLEX solver with a maximum runtime of 3000 seconds and an L0 penalty\nc0 = 10−6. Features are standardized to zero mean and unit variance, and the threshold-dummy transform is optionally\napplied. Coefficients are constrained to {−1, 0, +1}, and the sparsity budget is matched to AgentScore.\n• FasterRisk (Liu et al., 2022): A fast coordinate-descent–based algorithm for learning sparse risk scores. We use\nthe official Python implementation with sparsity constraint k = 6 and coefficient bounds {−1, 0, +1}. Features are\nstandardized prior to fitting, and the threshold-dummy transform is optionally applied via configuration.\nAutoScore.\nAutoScore (Xie et al., 2020) is a framework for automatically constructing point-based clinical scores. We\nuse the reference R implementation with random forest–based feature selection (100 trees), equal-frequency discretization\ninto four bins for continuous variables, and a maximum score of 100 points. AutoScore performs its own internal binning\nand does not rely on the shared threshold-dummy transform. While AutoScore produces integer-valued scores, the resulting\npoint totals are not bounded to unit weights, leading to less compact and less checklist-like models.\n24\n"}, {"page": 25, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nLinear and tree-based baselines.\nWe include standard interpretable machine-learning comparators operating on continu-\nous features without binarization. Logistic regression is trained with feature standardization (zero mean, unit variance),\nmedian imputation for missing values, and the SAGA optimizer with a maximum of 2000 iterations. As a non-linear but still\ninterpretable comparator, we include a CART decision tree with maximum depth four and a minimum of 20 samples per\nleaf.\nPLR baselines.\nFollowing Liu et al. (2022), we include pooled piecewise-linear rule (PLR) baselines, which expand\ncontinuous variables into collections of binary threshold indicators prior to fitting sparse linear models. This expansion uses\nthe same threshold-dummy transform applied to other baselines.\nEach PLR model is trained by solving an ElasticNet-regularized logistic regression problem over the expanded feature space,\nwith mixing parameter α ∈{0, 0.1, . . . , 1.0} and regularization strength C ∈[10−4, 102] on a logarithmic scale, yielding\nreal-valued coefficients.\nTo obtain integer-valued scoring systems, we apply the rounding procedures introduced by Ustun & Rudin (2019), which\nconvert real-valued PLR solutions into discrete coefficients in {−1, 0, +1}. For each dataset, we generate a pool of\napproximately 1,100 candidate PLR models (11 α values × 8 regularization settings × multiple random seeds) and apply\nmultiple rounding strategies. The final model is selected as the rounded solution with the lowest logistic loss.\nSpecifically, we consider:\n• PLR-RD: Direct rounding after truncating coefficients to [−1, +1].\n• PLR-RDU: Unit-weighted rounding (Burgess method), assigning coefficients based solely on sign.\n• PLR-RSRD: Rescaling coefficients to unit magnitude prior to rounding.\n• PLR-Rand: Randomized rounding based on fractional coefficient values.\n• PLR-RDP: Sequential, loss-aware rounding that locally minimizes logistic loss.\n• PLR-RDSP: Sequential rounding followed by discrete coordinate descent (25 iterations) to further reduce loss.\nD.3. AgentScore Hyperparameters\nTable 10 summarizes the hyperparameters used for AgentScore across all experiments.\nWe set auc threshold= 0.60 empirically as the lowest univariate AUROC that still justifies including a rule under a\nsparse rule budget. For diversification, jaccard threshold= 0.9 filters near-duplicate rules (high overlap in covered\npositives); we allow an exception when a candidate improves AUROC by at least 0.01 over the incumbent. For scoring, we\nuse a deterministic Youden-J thresholding objective by default, but other objectives (e.g., prioritizing sensitivity or PPV)\ncan be substituted depending on the desired operating point.\nTable 10. AgentScore hyperparameters. Default values used across all experiments unless otherwise specified.\nParameter\nValue\nDescription\nFeature Agent\nmax rules\n6\nMaximum number of rules in final score\niterations\n100\nNumber of LLM proposal iterations\nauc threshold\n0.60\nMinimum univariate AUROC to retain a candidate rule\ntemperature\n1.0\nLLM sampling temperature for diversity\nlogic depth\n1\nMaximum logical composition depth (atomic single-condition rules)\nScore Diversification\njaccard threshold\n0.9\nMaximum Jaccard similarity on positive cases\nmin pos gain\n0.01\nMinimum AUROC gain to accept a similar rule\nScoring Agent\nrefine steps\n10\nNumber of refinement iterations\nobjective\nYouden\nThreshold selection objective (Youden’s J)\n25\n"}, {"page": 26, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nD.4. Compute resources\nExperiments were conducted on a shared Linux workstation with dual AMD EPYC 7713 CPUs (128 physical cores), 1 TB\nof system memory, and a single NVIDIA RTX 6000 Ada GPU (48 GB VRAM), using CUDA 11.5. We use GPT-5 (version\n2025-08-07), GPT-4o (version 2024-11-20) and Deepseek V3.2 (version 2026-05-01-preview) via API calls as provided on\nMicrosoft Azure.\nE. Extended results\nE.1. Formal Optimization Landscape and Complexity\nProblem Formulation.\nLet D = {(xi, yi)}N\ni=1 be a dataset where xi ∈Rd and yi ∈{0, 1}. We define a rule grammar G\ngenerating a universe of logical predicates Runiv. The objective is to select a subset S ⊂Runiv to form a unit-weighted score\nfS(x) = P\nr∈S r(x) that maximizes a utility metric J .\nmax\nw∈{0,1}|Runiv|,k∈Z J (w, k; D)\ns.t.\n∥w∥0 ≤Mmax,\nˆy = I\n\nX\nj\nwjrj(x) ≥k\n\n.\n(2)\nHere J is a utility metric (e.g., AUROC) and ∥w∥0 is the ℓ0 pseudo-norm.\nWhy Standard Solvers Fail.\n1. The “Generation–Selection” Gap: State-of-the-art interpretable solvers such as RiskSLIM and FasterRisk act\nas selectors, requiring a pre-computed feature matrix X ∈{0, 1}N×|Runiv|. They cannot generate semantic rules\ndynamically; any clinically meaningful derived constructs (ratios, trends, shallow logic) must be manually engineered\nand materialized as columns a priori.\n2. Failure of Continuous Relaxation (e.g., Lasso): Relaxing w ∈{0, 1}|Runiv| to continuous weights w ∈R|Runiv|\nintroduces a substantial integrality gap: continuous relaxations induce fractional solutions, and rounding them can\nchange the induced decision boundary and utility relative to the discrete optimum. In checklist learning, where both the\nscore and the operating threshold are discrete, such rounding effects are often amplified.\n3. Failure of Classical Heuristics (Genetic Algorithms, SA): Standard heuristic searches struggle with the semantic\nstructure of the rule space:\n• Undefined Metric Space: Crossover operators in Genetic Algorithms require a meaningful metric space. It is\nunclear how to interpolate between “Age > 65” and “Lactate < 2.0”.\n• Sparse Fitness Landscape: A random mutation to a complex rule (e.g., changing a temporal window from 24h to\n1h) often yields a rule with AUROC ≈0.5. Without semantic guidance, random search (and simulated annealing)\nwastes the large majority of evaluations on statistically irrelevant candidates.\nCombinatorial and Physical Intractability (order-of-magnitude).\nThe necessity of AgentScore is driven by the\nsheer scale of the grammar-induced search space. Below we provide an order-of-magnitude breakdown of the candidate\nspace size for a representative clinical task (p = 50 variables, N = 5 × 105 patients). For clarity, we omit several additional\nrule families (e.g., some higher-arity compositions and additional temporal operators); including them would only increase\nthe space further.\n1. Primitive Rules: With T = 20 quantile thresholds and range constraints, a crude count gives\n|Rprim| ≈p × (2T + T 2) ≈50 × 440 ≈2.2 × 104.\n2. Compositional Rules: Allowing depth-1 logical operators (AND/OR) between pairs of primitives yields, up to constants,\n|Rcomp| ≈2 ·\n\u0012|Rprim|\n2\n\u0013\n≈O\n\u0000|Rprim|2\u0001\n≈(2.2 × 104)2 ≈4.8 × 108.\n26\n"}, {"page": 27, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\n3. Additional Variants (Temporal + Ratios): Introducing simple temporal summaries (e.g., W = 4 windows × 3 stats =\n12 variants) and a restricted set of arithmetic ratios/differences over variable pairs (\n\u000050\n2\n\u0001\n≈1225) increases the candidate\nuniverse by large multiplicative factors. An order-of-magnitude approximation is\n|Runiv| ≈|Rcomp| × (1 + 12temporal) × (1 + 1225ratios) ≈(4.8 × 108) × 13 × 1226 ≈7.6 × 1012.\nThe Physical Barriers.\nThe Memory Wall: Instantiating a full binary feature matrix X ∈{0, 1}N×|Runiv| for matrix-based\nsolvers would require storing on the order of\nN × |Runiv| ≈(5 × 105) × (7.6 × 1012) ≈3.8 × 1018\nbinary entries. Even under ideal bit-packing (1 bit per entry), this is\n3.8 × 1018 bits ≈4.75 × 1017 bytes ≈475 PB.\nIn practice, sparse/dense representations and metadata overhead would increase this further. This creates a hard physical\nconstraint: the full rule matrix cannot be pre-computed; features must be generated on-the-fly.\nThe Time Wall: Even with on-the-fly generation, assuming a realistic evaluation time of τ = 0.1s per rule (including\ntemporal feature extraction over N = 5 × 105 patients),\nTime ≈|Runiv| × τ ≈7.6 × 1012 × 0.1s = 7.6 × 1011s ≈24,000 years.\nConclusion.\nThe problem space is too large for enumeration (Time Wall), too large for standard matrix-based solvers\n(Memory Wall), and ill-suited for gradient-based or random heuristics (integrality and semantic gaps). This motivates\nAgentScore: using an LLM to learn a proposal distribution Pθ(r) that navigates the semantic structure of the rule space,\nalleviating the “cold start” problem of finding high-utility rules in a sparse combinatorial landscape.\nE.2. Effect of different LLM backbones\nAgentScore relies on a language model to navigate the rule space by proposing semantically plausible candidates.\nWhile the downstream evaluation, acceptance, and selection procedures are fully deterministic, the quality of the proposal\ndistribution depends on the underlying LLM. We therefore assess the sensitivity of AgentScore to the choice of language\nmodel backbone. For the main experiments, we use GPT-5, a frontier closed-source model. To evaluate robustness to\nmodel choice, cost, and openness, we additionally consider a strong open-source alternative (DeepSeek V3.2) as well as a\ncomputationally cheaper proprietary model (GPT-4o).\nAcross all datasets, GPT-5 yields the strongest overall performance, however, the performance gap to GPT-4o and DeepSeek\nV3.2 is small, with all models achieving competitive average AUROC (Figure 4). These results highlight two practical\nimplications. First, the framework remains effective under substantially lower-cost or open-source model choices, improving\naccessibility and reproducibility. Second, because the LLM is used solely as a proposal mechanism and never bypasses\ndeterministic statistical validation, the overall system is expected to improve monotonically as language models advance.\nFuture gains in LLM reasoning quality are therefore likely to translate directly into more efficient search and higher-quality\nguideline-style scoring systems, without requiring changes to the underlying optimization framework.\nE.3. Risk calibration and score monotonicity\nFor clinical deployment, performance metrics such as AUROC are necessary but not sufficient. In practice, clinicians do\nnot treat scoring systems as purely binary classifiers. Instead, scores are used to contextualize risk: borderline scores may\nprompt individualized clinical judgment, while very high scores often trigger heightened vigilance, escalation, or additional\nreview even when the clinician’s prior concern is low. Consequently, a clinically usable scoring system must exhibit risk\nmonotonicity, such that higher scores correspond to systematically higher event rates.\nTo assess this property, we analyze outcome prevalence as a function of the discrete score value produced by AgentScore.\nAcross all datasets, we observe a consistent and approximately monotonic increase in empirical risk with increasing score\n(Figure 5). This indicates that the learned unit-weighted checklists preserve ordinal risk structure and support meaningful\nrisk stratification beyond a single operating threshold.\n27\n"}, {"page": 28, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nGPT-5\nGPT-4o\nDeepSeek-V3.2\nModel\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUC\nAUC Distribution by Model (All Datasets)\nMIMIC AF\neICU LOS\neICU Vaso\nMIMIC HF\nMIMIC Cancer\nMIMIC AKI\nMIMIC COPD\nMIMIC Lung\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUC\nPer-Dataset AUC Comparison\nGPT-5\nGPT-4o\nDeepSeek-V3.2\nFigure 4. Effect of LLM backbone choice. Performance of AgentScore across different language-model backbones. While GPT-5\nattains the strongest results on average, GPT-4o and DeepSeek V3.2 exhibit comparable performance trends, suggesting limited sensitivity\nto the specific LLM used for rule proposal.\nThe only apparent deviation from strict monotonicity occurs in the MIMIC Lung dataset, where the score bin of zero exhibits\na slightly higher positive rate than the score bin of one. This effect is attributable to extreme data sparsity: only 13 patients\nfall into the zero-score bin, compared to several thousand patients in all other bins. When accounting for sampling variability,\nthe overall risk trend remains monotone.\n0\n1\n2\n3\n4\n5\nAssigned Score\n0\n20\n40\n60\n80\nPositive Rate (%)\nn=17459 n=38669\nn=16595\nn=4814\nn=1535\nn=707\nMIMIC AF\n0\n1\n2\n3\n4\n5\nAssigned Score\n0\n10\n20\n30\n40\n50\n60\nPositive Rate (%)\nn=77140\nn=51157\nn=37031\nn=20484\nn=7547\nn=1980\neICU Vaso\n0\n1\n2\n3\nAssigned Score\n0\n10\n20\n30\n40\n50\n60\n70\nPositive Rate (%)\nn=118377\nn=41619\nn=21556\nn=13787\neICU LOS\n0\n1\n2\n3\n4\n5\n6\nAssigned Score\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPositive Rate (%)\nn=18021n=31878\nn=19896\nn=6638\nn=2666\nn=1313\nn=199\nMIMIC HF\n0\n1\n2\n3\n4\n5\n6\nAssigned Score\n0\n20\n40\n60\n80\n100\nPositive Rate (%)\nn=13\nn=3969\nn=8951\nn=6671\nn=5313\nn=3369\nn=894\nMIMIC Lung\n0\n1\n2\n3\n4\n5\nAssigned Score\n0\n20\n40\n60\n80\nPositive Rate (%)\nn=2\nn=2191\nn=2760\nn=2343\nn=848\nn=101\nMIMIC Cancer\n0\n1\n2\n3\n4\nAssigned Score\n0\n20\n40\n60\n80\nPositive Rate (%)\nn=12218\nn=14227\nn=12457\nn=4834\nn=510\nMIMIC COPD\n0\n1\n2\n3\n4\n5\nAssigned Score\n0\n20\n40\n60\n80\nPositive Rate (%)\nn=189758\nn=178070\nn=103367\nn=43180\nn=25312\nn=6341\nMIMIC AKI\nRisk (% Positive) by Assigned Score\nFigure 5. Risk monotonicity across score values. Empirical outcome prevalence (percentage positive) as a function of the discrete\nAgentScore value.\nE.4. Effect of guideline size\nThroughout the paper, we restrict the maximum number of rules to Mmax = 6, reflecting a conservative upper bound for\nchecklists that remain easily memorizable and executable at the bedside while achieving competitive predictive performance.\nIn practice, however, clinical stakeholders may prefer even smaller scores, motivating an explicit trade-off between model\n28\n"}, {"page": 29, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\ncomplexity and accuracy.\nTo characterize this trade-off, we vary the maximum allowed number of rules and evaluate predictive performance as a\nfunction of checklist size. Across all datasets, performance increases smoothly and predictably with the number of included\nrules (Figure 6), yielding a clear accuracy–complexity Pareto frontier. Notably, in many tasks, strong performance is already\nachieved with only four or five rules, with diminishing returns beyond this point.\nThese results provide clinicians and guideline developers with direct control over deployability: additional rules can be\nretained when modest gains in accuracy are clinically meaningful, or omitted to maximize simplicity, memorability, and ease\nof use. This flexibility highlights the practical advantage of learning within a strictly constrained, unit-weighted checklist\nmodel class.\n1\n2\n3\n4\n5\n6\nMax Rules\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\n0.825\nAUC\nMIMIC AF\n1\n2\n3\n4\n5\n6\nMax Rules\n0.62\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\nAUC\neICU Vaso\n1\n2\n3\n4\n5\n6\nMax Rules\n0.62\n0.64\n0.66\n0.68\nAUC\neICU LOS\n1\n2\n3\n4\n5\n6\nMax Rules\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nAUC\nMIMIC HF\n1\n2\n3\n4\n5\n6\nMax Rules\n0.56\n0.58\n0.60\n0.62\n0.64\nAUC\nMIMIC Lung\n1\n2\n3\n4\n5\n6\nMax Rules\n0.52\n0.54\n0.56\n0.58\n0.60\n0.62\nAUC\nMIMIC Cancer\n1\n2\n3\n4\n5\n6\nMax Rules\n0.56\n0.58\n0.60\n0.62\n0.64\nAUC\nMIMIC COPD\n1\n2\n3\n4\n5\n6\nMax Rules\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nAUC\nMIMIC AKI\nAgentScore: AUC vs Model Complexity (Max Rules)\nFigure 6. Accuracy–complexity trade-off. AUROC as a function of the maximum number of rules allowed in the checklist. Performance\nimproves monotonically with rule set size, illustrating a clear deployability–accuracy Pareto frontier.\nStatistical analysis.\nWe evaluate predictive performance using AUROC. For AgentScore, predicted probabilities are\ncomputed as\nˆp = count\nnrules\n,\nwhere count denotes the number of satisfied rules. Statistical significance is assessed using paired tests on fold-level\nAUROC values across the 8 selected datasets (5 folds each; n = 40 paired observations per comparison), including a paired\nt-test and a Wilcoxon signed-rank test. Tests are two-sided, and p-values are corrected for multiple comparisons using\nHolm–Bonferroni. We additionally report bootstrap 95% confidence intervals for mean differences and Cohen’s d effect\nsizes. If a method is missing for a fold/dataset, AUROC is set to 0.5 to preserve pairing.\nE.5. Scoring system analysis\nWe analyze the structure of the scoring systems produced by AgentScore across a full cross-validation run. Concretely,\nfor each dataset we collect the final checklist generated in each of the five folds and summarize the distribution of rule\nfamilies induced by the rule grammar (Appendix A). Figure 7 reports the resulting composition statistics, quantifying\nhow often the learned checklists rely on different rule types, demonstrating that the framework consistently leverages\nmultiple rule families rather than collapsing to a single template. To illustrate the learned artifacts, Table 12 presents four\nrepresentative AgentScore checklists sampled. Each checklist is unit-weighted and executable as an N-of-M procedure,\nwhere the total score is the number of satisfied rules and the decision threshold K is selected on validation data. These\n29\n"}, {"page": 30, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nTable 11. Statistical comparison of AgentScore vs. score-based baselines. Two-sided paired tests on fold-level AUC values (n = tasks\n× 5 folds). p-values corrected using Holm-Bonferroni.\nBaseline\nn\n∆AUC\n95% CI\np (t-test)\np (Wilcoxon)\nCohen’s d\nPLR (rd)\n40\n+0.109\n[+0.087, +0.133]\n0.000∗∗\n0.000∗∗\n+1.47\nPLR (rand)\n40\n+0.094\n[+0.073, +0.118]\n0.000∗∗\n0.000∗∗\n+1.26\nPLR (rdp)\n40\n+0.094\n[+0.073, +0.118]\n0.000∗∗\n0.000∗∗\n+1.26\nPLR (rdsp)\n40\n+0.094\n[+0.073, +0.118]\n0.000∗∗\n0.000∗∗\n+1.26\nPLR (rsrd)\n40\n+0.066\n[+0.054, +0.079]\n0.000∗∗\n0.000∗∗\n+1.67\nPLR (rdu)\n40\n+0.058\n[+0.044, +0.072]\n0.000∗∗\n0.000∗∗\n+1.26\nRiskSLIM\n40\n+0.046\n[+0.033, +0.060]\n0.000∗∗\n0.000∗∗\n+1.07\nFasterRisk\n40\n+0.031\n[+0.019, +0.042]\n0.000∗∗\n0.000∗∗\n+0.81\nexamples highlight the intended output form: compact, auditable rule sets that combine simple threshold predicates with\noccasional shallow conjunctions and clinically interpretable derived rules (e.g., ratios), while remaining compatible with\nbedside execution.\n0\n20\n40\n60\n80\n100\nProportion of Rules (%)\nMIMIC COPD\nMIMIC Lung\nMIMIC Cancer\nMIMIC AKI\neICU LOS\nMIMIC HF\nMIMIC AF\neICU Vaso\nMortality\nCF Cohort\nRule Type\nThreshold\nRange\nCategorical\nDerived\nComposite\nFigure 7. Rule type diversity across cross-validation. Distribution of rule families (as defined by the rule grammar) appearing in the\nfinal AgentScore checklists across five folds per dataset. Bars report the frequency with which each rule type is used, showing that\nlearned scores draw on multiple rule families rather than collapsing to a single template.\nE.6. Clinician review of scoring system deployability\nWe conducted a structured expert review to assess the face validity, deployability, and usability of scoring systems generated\nby AgentScore compared to a strong baseline. This section provides full methodological details, statistical analyses, and\nresponse distributions to complement the summary in Section 5.3.\nParticipants\nA panel of N = 18 practicing clinicians participated in the study. Participants were recruited from six\ncountries across three continents and represented diverse clinical specialties. The sample was predominantly experienced: 8\nparticipants (44.4%) reported 10+ years of clinical experience, 8 (44.4%) reported 6–10 years, and 2 (11.1%) reported 0–2\nyears. Overall, 89% of participants had at least 6 years of clinical experience.\nStudy design\nWe employed a blinded, randomized paired-comparison design. Participants evaluated four representative\nscoring systems generated by AgentScore (labeled “Model B”) alongside matched outputs from FasterRisk (labeled\n“Model A”), a state-of-the-art integer score-learning baseline. To isolate structural and usability considerations from\npredictive performance, participants were explicitly instructed to assume equal discrimination and to focus exclusively on\nrule structure, feature choice, and bedside deployability.\n30\n"}, {"page": 31, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nTable 12. Representative AgentScore checklists (2×2 panel). Each satisfied rule contributes +1 point. The total score is the number\nof satisfied rules; predict positive if S(x) ≥τ, where τ is selected on validation data. Quantile thresholds. Quantile-based cutpoints\nare common in clinical medicine (e.g., troponin above the 99th percentile upper reference limit; percentile-based definitions of “normal”\nranges). For deployment, any quantile expression Qq(·) is converted to a fixed numeric threshold estimated on a reference cohort (or\nprecomputed and reported in guideline units); we retain the symbolic Qq notation here to show the original rule form produced by the\nsystem.\neICU Prolonged ICU LOS (N-of-6)\n(Prolonged ICU Stay > 3 days Checklist)\nChecklist rule (satisfied?)\nPoints\nGCS Verbal < 5\n+1\nGCS Eyes < 4\n+1\nGCS Motor < 6\n+1\nHematocrit < 35%\n+1\nHeart rate ≥85 bpm\n+1\nMean BP < 65 mmHg\n+1\nTotal score\n0–6\nHigh-risk threshold\nS(x) ≥3\nMIMIC-IV HF Mortality (N-of-6)\n(HF In-Hospital Mortality N-of-M)\nChecklist rule (satisfied?)\nPoints\nHCO−\n3 /Cl−< 0.2\n+1\nWBC ≥10 and HCO−\n3 ≤25\n+1\nBUN ≥30\n+1\n1.5 ≤Cr ≤5.0\n+1\nAdmission is emergency or urgent\n+1\n10 ≤WBC ≤20\n+1\nTotal score\n0–6\nHigh-risk threshold\nS(x) ≥3\neICU Vasopressor (N-of-4)\n(Shock & Organ Dysfunction Checklist)\nChecklist rule (satisfied?)\nPoints\nIntubated or mechanical ventilation\n+1\nHeart rate / mean BP ≥1.5\n+1\nBUN ≥25 mg/dL\n+1\nWBC ≥Q0.75(WBC)\n+1\nTotal score\n0–4\nHigh-risk threshold\nS(x) ≥2\nMIMIC-IV AKI (N-of-4)\n(Renal–Anemia–Acuity Checklist)\nChecklist rule (satisfied?)\nPoints\nCr ≥Q0.75(Cr)\n+1\n0 ≤Hematocrit ≤30\n+1\nER admission and BUN ≥20\n+1\nBUN/Cr ≥20\n+1\nTotal score\n0–4\nHigh-risk threshold\nS(x) ≥2\nSurvey instrument\nThe survey comprised 10 questions organized into three blocks:\nBlock 1: Demographics and general attitudes (Q1–Q6).\n• Q1 (Demographics): Years of clinical experience (0–2, 3–5, 6–10, 10+).\n• Q2 (5-point Likert): “I would trust a clinical scoring system generated by AI.”\n• Q3 (5-point Likert): “I would trust a clinical scoring system generated by AI if it had been externally validated on\n50,000 patients.”\n• Q4 (5-point Likert): “Memorability is important for clinical scoring systems used at the bedside.”\n• Q5 (5-point Likert): “I would be willing to use a unit-weighted checklist (each item worth 1 point) in clinical practice.”\n• Q6 (5-point Likert): “I would be willing to use a scoring system that requires mental arithmetic (e.g., multiplying\nvalues by 2, 3, or 5) at the bedside.”\nBlock 2: Paired comparisons across four clinical tasks (Q7–Q9). For each of four clinical prediction tasks (ICU mortality,\nsepsis risk, respiratory failure, cardiac events), participants were shown two scoring systems (Model A and Model B) and\nasked:\n• Q7: “Which scoring system better matches guideline-style clinical reasoning?”\n• Q8: “Which scoring system would be easier to apply at the bedside under time pressure?”\n• Q9: “Assuming both have been externally validated, which would you prefer to deploy?”\nBlock 3: Overall preference (Q10).\n• Q10: “Overall, which scoring-system style would you prefer for clinical deployment?” (Model A / Model B / Neither).\n31\n"}, {"page": 32, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nStatistical analysis\nAll analyses were pre-specified. For Likert-scale items (Q2–Q6), we report means, 95% confidence\nintervals, and one-sample t-tests against the neutral midpoint (3). For the trust comparison (Q2 vs. Q3), we used a paired\nt-test and report Cohen’s d as an effect size. For pairwise preference questions (Q7–Q9), we aggregated responses across\nthe 4 clinical tasks (n = 72 judgments per question) and tested whether the proportion preferring Model B exceeded 50%\nusing one-sided binomial tests; we report Cohen’s h as an effect size for proportions. For overall preference (Q10), we\nused a chi-square goodness-of-fit test against a uniform distribution and a binomial test comparing Model B to Model A\n(excluding “Neither” responses). All p-values are reported without correction for multiple comparisons; conclusions are\nrobust to Bonferroni correction.\nTrust in AI-generated scores.\nBaseline trust in AI-generated scores was neutral to low (M = 2.72, 95% CI [2.13, 3.31],\nt(17) = −0.92, p = 0.37). However, trust increased substantially when large-scale external validation was specified\n(M = 4.39, 95% CI [4.16, 4.62], t(17) = 11.75, p < 10−8). The within-subject increase was statistically significant\n(∆M = +1.67, SD = 1.37; paired t(17) = 5.15, p < 10−4) with a large effect size (Cohen’s d = 1.21).\nDeployability preferences.\nClinicians expressed strong willingness to use unit-weighted checklists (M = 4.33, 95% CI\n[3.91, 4.75], t(17) = 6.23, p < 10−5) and moderate willingness to use scores requiring mental arithmetic (M = 3.89, 95%\nCI [3.58, 4.20], t(17) = 5.58, p < 10−4). Importance of memorability was rated as neutral to high on average (M = 3.22,\np = 0.45).\nPairwise comparisons.\nAcross 72 judgments per question (18 clinicians × 4 tasks), participants significantly preferred\nAgentScore (Model B) over FasterRisk (Model A) on all three dimensions:\n• Q7 (Guideline reasoning): 85% preferred Model B (95% CI [76%, 92%]; binomial p < 10−9, Cohen’s h = 0.77).\n• Q8 (Bedside ease): 71% preferred Model B (95% CI [61%, 80%]; binomial p < 10−3, Cohen’s h = 0.43).\n• Q9 (Deployment preference): 81% preferred Model B (95% CI [71%, 89%]; binomial p < 10−7, Cohen’s h = 0.66).\nWithin-respondent consistency.\nTo assess reliability, we computed the proportion of respondents who gave the same\nanswer across all four clinical tasks: 61% for Q7, 33% for Q8, and 50% for Q9. The moderate consistency for Q8 suggests\nthat bedside-ease judgments are more task-dependent than guideline-alignment judgments.\nOverall preference.\nWhen asked for their overall preferred scoring-system style for clinical deployment, 12/18 clinicians\n(67%) selected Model B (AgentScore), 1/18 (6%) selected Model A (FasterRisk), and 5/18 (28%) reported no\npreference. The distribution differed significantly from uniform (χ2(2) = 10.33, p = 0.006). Excluding “Neither”\nresponses, 12/13 (92%) preferred Model B over Model A (binomial p = 0.002).\nQualitative observations.\nIn free-text comments, clinicians noted that non-unit weights (e.g., +3, +5) in FasterRisk\noutputs would require mental arithmetic or electronic assistance, reducing reliability under time pressure. In contrast, the\nunit-weighted checklist structure of AgentScore was viewed as immediately executable. Participants particularly valued\nderived features aligned with clinical reasoning patterns (e.g., physiologic ratios such as shock index), which are standard\nconstructs in existing guidelines but are not discoverable by fixed-feature baselines.\nLimitations\nThe sample size (N = 18) is modest and precludes subgroup analyses by specialty or experience level.\nAlthough participants were blinded to model identity, the structural differences between unit-weighted checklists and\ninteger-weighted scores may have been recognizable. This study assesses face validity and usability preferences; it does not\nmeasure actual bedside performance or patient outcomes. Finally, the convenience sampling approach limits generalizability\nto the broader clinical population.\nSummary\nClinicians with substantial experience expressed strong and statistically significant preferences for\nAgentScore-generated unit-weighted checklists over integer-weighted baselines across guideline alignment, bedside us-\nability, and deployment preference. Trust in AI-generated scores increased substantially when large-scale external validation\nwas specified. These findings support the practical deployability of the scoring systems produced by AgentScore.\n32\n"}, {"page": 33, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\n0-2\n3-5\n6-10\n10+\nYears of Experience\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nCount\n2\n8\n8\nQ1: Experience\n1\n2\n3\n4\n5\nTrust Level\n0\n1\n2\n3\n4\n5\n6\n7\n8\nCount\n5\n2\n4\n7\nQ2: AI Trust\n1\n2\n3\n4\n5\nTrust Level\n0\n2\n4\n6\n8\n10\n12\nCount\n11\n7\nQ3: Trust (Validated)\n1\n2\n3\n4\n5\nImportance\n0\n1\n2\n3\n4\n5\n6\nCount\n1\n5\n4\n5\n3\nQ4: Memorability\n1\n2\n3\n4\n5\nWillingness\n0\n2\n4\n6\n8\n10\nCount\n1\n2\n5\n10\nQ5: Unit-Weight\n1\n2\n3\n4\n5\nWillingness\n0\n2\n4\n6\n8\n10\nCount\n5\n10\n3\nQ6: Arithmetic\nA\nB\n0\n8\n16\n24\n32\n40\n48\n56\n64\nVotes\n11\n(15%)\n61\n(85%)\nQ7: Guideline Match\nA\nB\n0\n6\n12\n18\n24\n30\n36\n42\n48\n54\nVotes\n21\n(29%)\n51\n(71%)\nQ8: Bedside Ease\nA\nB\n0\n8\n16\n24\n32\n40\n48\n56\nVotes\n14\n(19%)\n58\n(81%)\nQ9: Deploy Pref.\nA\nB\nNeither\n0\n2\n4\n6\n8\n10\n12\nCount\n1\n12\n5\nQ10: Overall Pref.\nClinical Evaluation of AgentScore (N=18 Clinicians)\nA: FasterRisk\nB: AgentScore\nFigure 8. Clinician evaluation of scoring system deployability (N = 18). Top row: Experience distribution (Q1) and Likert-scale\nresponses for trust (Q2–Q3) and deployability preferences (Q4–Q6). Bottom row: Aggregated pairwise preferences across 4 clinical tasks\n(Q7–Q9; 72 judgments each) and overall model preference (Q10). Model A: FasterRisk; Model B: AgentScore.\nE.7. Wall-Clock Time\nWe additionally report wall-clock training time for AgentScore and all baselines. In absolute terms, AgentScore is\noften slower than standard baselines, primarily due to external API latency associated with language-model queries. Its\nruntime is nevertheless comparable to optimization-based score learning methods such as FasterRisk and RiskSLIM.\nTo make this cost explicit, for each cross-validation fold AgentScore issues a bounded number of language-model calls.\nSpecifically, we perform up to 100 API calls for candidate rule generation. Each statistically valid candidate rule is then\nsubjected to a single self-consistency check by the language model, resulting in at most an additional 100 calls. Checklist\nconstruction involves one initial guideline proposal call, followed by two refinement phases of 10 steps each, yielding at\nmost 21 further calls. In total, the number of API calls per fold is therefore capped at approximately 220, independent of\ndataset size.\nMore importantly, training-time compute and deployment-time compute differ fundamentally across methods. For most\nmachine-learning models, increased training complexity is typically coupled with nontrivial inference cost at test time,\nrequiring computational infrastructure for deployment. In contrast, AgentScore incurs computational cost only during\ndevelopment. At deployment, inference is effectively zero-cost: the learned N-of-M unit-weighted checklist can be\nevaluated manually by clinicians without calculators, servers, or model execution.\nSince score construction is performed offline and only once per task, differences in wall-clock training time on the order of\nminutes are acceptable in practice and do not affect real-world usability. These results highlight that AgentScore trades\nmodest additional development-time compute for negligible deployment-time cost, aligning with the constraints of bedside\nclinical use.\nF. Algorithmic Details\nThis section summarizes the AgentScore learning procedure and provides concise implementation details to support\nreproducibility. Full code will be released upon acceptance.\n33\n"}, {"page": 34, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nTable 13. Wall-clock training time per fold (seconds). All PLR variants (RD, RDU, RSRD, Rand, RDP, RDSP) share identical training\ntime and are reported jointly.\nMethod\nMIMIC AF\nMIMIC AKI\nMIMIC COPD\nMIMIC HF\nMIMIC CANCER\nMIMIC LUNG\neICU LOS\neICU Vaso.\nAgentScore\n3918\n4096\n3594\n2380\n2244\n3330\n3145\n1382\nDecision Tree\n2.9\n6.0\n1.9\n3.7\n0.4\n1.3\n3.3\n3.2\nLogistic\n3.6\n6.3\n2.7\n4.4\n0.8\n1.7\n4.2\n3.6\nFasterRisk\n1360\n699\n1045\n1514\n55.7\n588\n806\n820\nRiskSLIM\n1292\n193\n205\n1176\n439\n652\n1934\n2942\nAutoScore\n156\n362\n92\n153\n18.3\n51.9\n236\n201\nPLR\n61\n90\n45.2\n80\n8.9\n32.1\n151\n96\nF.1. AgentScore Algorithm (High-Level)\nAlgorithm 1 AgentScore Framework\nRequire: Training data (X, y), task description T , rule budget M\nRequire: Validity threshold τrule, redundancy threshold δ\nEnsure: Unit-weighted checklist S and decision threshold τ\nConstruct tool interface I exposing metadata and evaluation only\nInitialize rule pool P ←∅\n{Phase 1: Rule Pool Generation}\nwhile generation budget not exhausted do\nPropose candidate batch C from grammar R using Rule Proposal Agent\nfor each proposed rule r ∈C do\nCompute AUROC(r) and overlap metrics via I\nif AUROC(r) ≥τrule and maxr′∈P J+(r, r′) ≤δ then\nplausible ←Clinical Plausibility Agent.review(r)\nif plausible then\nP ←P ∪{r}\nend if\nend if\nend for\nend while\n{Phase 2: Score Construction & Refinement}\nS ←Score Construction Agent.select(P, M, I)\nwhile refinement criteria not met do\nS ←Score Construction Agent.refine(S, P, I)\nend while\nSelect final decision threshold τ on validation data\nreturn (S, τ)\nF.2. Notation, Splits, and Caching\nWithin each outer cross-validation fold, the training data is further split into a construction set Dcon and an internal\nvalidation set Dval. Rule evaluation, acceptance, and redundancy filtering are performed on Dcon, while checklist selection\nand decision-threshold optimization are performed exclusively on Dval. Held-out test folds are never accessed during rule\ngeneration or score construction.\nWe denote by P ⊂R the pool of retained candidate rules constructed during Phase 1, and by S ⊆P the final checklist of\nsize at most M assembled during Phase 2. All scores are unit-weighted by construction.\nFor each retained rule r ∈P, we cache its positive-class coverage mask on Dcon:\n(cr)i = 1{r(xi) = 1 ∧yi = 1},\n(xi, yi) ∈Dcon,\n34\n"}, {"page": 35, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nand write C+ = {cr : r ∈P}. Redundancy is measured using the positive-class Jaccard similarity\nJ+(r, r′) = |cr ∩cr′|\n|cr ∪cr′|,\nr, r′ ∈P.\nF.3. Tool-Mediated Data Access\nAll interactions with the dataset are mediated by a fixed tool interface I. The language model never observes raw patient-level\nvalues. Instead, it interacts exclusively through deterministic tools that expose: (i) feature names and inferred types; (ii)\naggregate statistics computed on training data only (e.g., quantiles, missingness); (iii) evaluation outputs such as AUROC\nand coverage. No tool returns individual samples, identifiers, or free-form data.\nF.4. Typed Rule Proposal and Validation\nRules are proposed as structured objects drawn from a typed grammar R supporting thresholds, ranges, derived expressions,\ntemporal summaries, and shallow logical compositions. All proposals must satisfy a strict schema; malformed rules,\nunknown features, or type violations are deterministically rejected prior to evaluation.\nThe Clinical Plausibility Agent functions as a binary gate applied only after statistical screening. It receives the symbolic\nrule definition and aggregate feature summaries (but no raw data) and returns a binary accept/reject decision based on\nclinical coherence and interpretability.\nF.5. Deterministic Rule Evaluation and Retention\nEach syntactically valid rule is evaluated on the construction split Dcon and retained into the rule pool P if and only if it\nsatisfies all of the following criteria: (i) its AUROC exceeds a minimum acceptance threshold τrule; (ii) its redundancy\nwith previously retained rules, measured by the positive-class Jaccard similarity J+ computed over Dcon, is below a fixed\nthreshold δ, unless the rule increases positive-class coverage by at least ∆min; and (iii) rule-family diversity constraints\nare satisfied when feasible (i.e., enforcing a minimum mix across rule types such as thresholds, derived rules, and logical\ncompositions when sufficient candidates exist; otherwise skipped).\nFrom the pool of retained rules P, a final checklist S of size at most M is assembled. The decision threshold K (predict\npositive if count ≥K) is selected on Dval according to a specified operating objective (e.g., Youden’s J, balanced accuracy,\nor F1). Unless otherwise stated, we use a fixed minimum acceptance threshold of τrule = 0.6 AUROC.\nComputational Complexity. Conditional on a fixed stream of language-model proposals, the AgentScore pipeline\nis fully deterministic. All rule validation, acceptance, redundancy checks, and checklist assembly are performed via\ndeterministic tools with no access to raw patient-level data. Let N denote the number of samples and T the total number\nof proposed rules. Rule evaluation scales linearly as O(T · N). Redundancy filtering compares each candidate against\nthe retained pool P and scales as O(T · |P|) in the worst case. Coverage masks are stored as compact bitsets of word\nsize w, yielding memory usage O(|P| · N/w) for cached rule coverage, in addition to the data matrix storage. In practice,\nredundancy filtering dominates runtime, while memory remains modest due to bitset compression.\nG. Agent prompts\nThis section documents the exact prompting templates used to instantiate the AgentScore framework.\nFeature Proposal Prompt (AgentScore)\nYou are a feature selection agent for clinical risk modeling.\nTask context:\n$task_description\nPropose binary (0/1) clinical features as JSON lines. All rules must follow one of the schemas below.\nAllowed rule types:\nnumeric_threshold:\n{\"type\":\"numeric_threshold\",\"feature\":str,\"op\":\">=\"|\">\"|\"<=\"|\"<\",\"threshold\":number}\n35\n"}, {"page": 36, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nnumeric_range:\n{\"type\":\"numeric_range\",\"feature\":str,\"low\":number,\"high\":number}\ncategorical_in:\n{\"type\":\"categorical_in\",\"feature\":str,\"in\":[category,...]}\nbinary_true:\n{\"type\":\"binary_true\",\"feature\":str}\nderived_numeric_threshold:\n{\"type\":\"derived_numeric_threshold\",\"expr\":str,\"op\":\">=\"|\">\"|\"<=\"|\"<\",\"threshold\":number}\ncount_present:\n{\"type\":\"count_present\",\"features\":[str,...],\"min_count\":int}\nlogical:\n{\"type\":\"logical\",\"op\":\"and\"|\"or\",\"rules\":[<rule>,...]}\npercent_change:\n{\"type\":\"percent_change\",\"feature_t0\":str,\"feature_t1\":str,\n\"pct\":number,\"op\":\">=\"|\">\"|\"<=\"|\"<\",\"direction\":\"increase\"|\"decrease\"}\nzscore_threshold:\n{\"type\":\"zscore_threshold\",\"feature\":str,\"op\":\">=\"|\">\"|\"<=\"|\"<\",\"z\":number}\nquantile_threshold:\n{\"type\":\"quantile_threshold\",\"feature\":str,\"op\":\">=\"|\">\"|\"<=\"|\"<\",\"q\":float}\nRule diversity guidelines:\n• Use a mix of rule types (thresholds, derived rules, logic).\n• Include both high-value (≥) and low-value (<) thresholds.\n• Prefer rules that capture distinct patient subgroups.\nClinical interpretability constraints:\n• Prefer features with suffixes\nlast,\nfirst,\nmin,\nmax.\n• Use round, clinically meaningful thresholds (e.g. HR ≥100).\n• Derived expressions should reflect standard clinical concepts.\nHard constraints:\n• Output only strict JSON.\n• One rule per line.\n• Use only provided variable names.\n• Minimum acceptable AUROC when evaluated: auc threshold.\nAvailable variables:\n$variable_list\nAggregate analysis insights (optional):\n$analysis_context\nTool-derived guidance (optional):\n$tool_summaries\nSuggest 1–3 candidate rules.\n36\n"}, {"page": 37, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nClinical Plausibility Review Prompt\nYou are a clinical expert reviewing a proposed risk prediction rule.\nAssess whether the rule is clinically plausible and meaningful.\nConsider:\n• Whether the direction of risk makes clinical sense.\n• Whether thresholds are physiologically reasonable.\n• Whether the rule is interpretable and actionable at the bedside.\nRule under review:\n$rule_json\nRespond with only a JSON object of the form:\n{\"plausible\": true|false, \"reason\": \"brief explanation\"}\nScore Construction Prompt (N-of-M Checklist)\nYou are a clinical scoring system designer.\nTask context:\n$task_description\nYou are given a set of binary (0/1) clinical rules.\nConstruct an interpretable N-of-M checklist score with the following properties:\n• Each rule contributes exactly 1 point.\n• The score equals the count of satisfied rules.\n• Predict positive if score ≥K (threshold selected automatically).\n• Do not introduce new rules.\nConstraints:\n• Use only the provided rules.\n• Maximum number of rules: max rules.\n• Output strict JSON with fields:\n– name\n– description\n– rules (list of {\"rule\":\n<rule json>})\nCandidate rules (with AUROC):\n$retained_rules_with_auc\nReturn only the JSON specification.\n37\n"}, {"page": 38, "text": "AgentScore: Autoformulation of Deployable Clinical Scoring Systems\nScore Refinement Prompt\nYou are refining an existing clinical checklist score.\nCurrent score specification:\n$current_score_json\nEvaluation metrics:\n$score_metrics\nObjective: Improve discrimination and calibration while preserving interpretability.\nConstraints:\n• Same schema as before.\n• Maximum number of rules: max rules.\n• All rules are unit-weighted.\n• No new rules may be introduced.\nReturn only the updated JSON specification.\n38\n"}]}