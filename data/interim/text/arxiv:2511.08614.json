{"doc_id": "arxiv:2511.08614", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.08614.pdf", "meta": {"doc_id": "arxiv:2511.08614", "source": "arxiv", "arxiv_id": "2511.08614", "title": "A Super-Learner with Large Language Models for Medical Emergency Advising", "authors": ["Sergey K. Aityan", "Abdolreza Mosaddegh", "Rolando Herrero", "Haitham Tayyar", "Jiang Han", "Vikram Sawant", "Qi Chen", "Rishabh Jain", "Aruna Senthamaraikannan", "Stephen Wood", "Manuel Mersini", "Rita Lazzaro", "Mario Balzanelli", "Nicola Iacovazzo", "Ciro Gargiulo Isacco"], "published": "2025-11-05T19:30:51Z", "updated": "2025-11-15T00:42:26Z", "summary": "Medical decision-support and advising systems are critical for emergency physicians to quickly and accurately assess patients' conditions and make diagnosis. Artificial Intelligence (AI) has emerged as a transformative force in healthcare in recent years and Large Language Models (LLMs) have been employed in various fields of medical decision-support systems. We studied responses of a group of different LLMs to real cases in emergency medicine. The results of our study on five most renown LLMs showed significant differences in capabilities of Large Language Models for diagnostics acute diseases in medical emergencies with accuracy ranging between 58% and 65%. This accuracy significantly exceeds the reported accuracy of human doctors. We built a super-learner MEDAS (Medical Emergency Diagnostic Advising System) of five major LLMs - Gemini, Llama, Grok, GPT, and Claude). The super-learner produces higher diagnostic accuracy, 70%, even with a quite basic meta-learner. However, at least one of the integrated LLMs in the same super-learner produces 85% correct diagnoses. The super-learner integrates a cluster of LLMs using a meta-learner capable of learning different capabilities of each LLM to leverage diagnostic accuracy of the model by collective capabilities of all LLMs in the cluster. The results of our study showed that aggregated diagnostic accuracy provided by a meta-learning approach exceeds that of any individual LLM, suggesting that the super-learner can take advantage of the combined knowledge of the medical datasets used to train the group of LLMs.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.08614v2", "url_pdf": "https://arxiv.org/pdf/2511.08614.pdf", "meta_path": "data/raw/arxiv/meta/2511.08614.json", "sha256": "fcf1a27fc3002f7c4949c1dad34054f06fd00de4f3e0449e2aa0f2b6aed3ba00", "status": "ok", "fetched_at": "2026-02-18T02:28:25.996414+00:00"}, "pages": [{"page": 1, "text": "arXiv.2511.08614, Nov. 5, 2025 \nhttps://doi.org/10.48550/arXiv.2511.08614 \n \n1 of 12 \n \nA Super-Learner with Large Language Models  \nfor Medical Emergency Advising \n \nSergey K. Aityan¹, Abdolreza Mosaddegh¹, Rolando Herrero², Haitham Tayyar³, \n Jiang Han¹, Vikram Sawant¹, Qi Chen¹, Rishabh Jain², Aruna Senthamaraikannan²,  \nStephen Wood⁴, Manuel Mersini⁵, Rita Lazzaro⁶, Mario Balzanelli⁶, \n Nicola Iacovazzo⁷, Ciro Gargiulo Isacco⁸ \n \n¹ Department of Multidisciplinary Engineering, Northeastern University, Oakland, CA, USA \n² Department of Multidisciplinary Engineering, Northeastern University, Boston, MA, USA \n³ Department of Multidisciplinary Engineering, Northeastern University, Toronto, ON, Canada \n⁴ Bouvé College of Health Sciences, Northeastern University, Boston, MA, USA \n⁵ Biovitalage S.R.L., 70010 Valenzano, Italy \n⁶ Territorial Emergency System SET 118, 74121 Taranto, Italy \n⁷ Territorial Center for Medical Assistance, 74121 Taranto, Italy  \n⁸ Department of Interdisciplinary Medicine (DIM), Aldo Moro University of Bari, 70121 Bari, Italy \n \nAbstract \nMedical decision-support and advising systems are critical for emergency physicians to quickly and \naccurately assess patients' conditions and make diagnosis. Artificial Intelligence (AI) has emerged \nas a transformative force in healthcare in recent years and Large Language Models (LLMs) have been \nemployed in various fields of medical decision-support systems. We studied responses of a group of \ndifferent LLMs to real cases in emergency medicine. The results of our study on five most renown \nLLMs showed significant differences in capabilities of Large Language Models for diagnostics acute \ndiseases in medical emergencies with accuracy ranging between 58% and 65%. This accuracy \nsignificantly exceeds the reported accuracy of human doctors. We built a super-learner MEDAS \n(Medical Emergency Diagnostic Advising System) of five major LLMs - Gemini, Llama, Grok, GPT, and \nClaude). The super-learner produces higher diagnostic accuracy, 70%, even with a quite basic meta-\nlearner. However, at least one of the integrated LLMs in the same super-learner produces 85% correct \ndiagnoses. The super-learner integrates a cluster of LLMs using a meta-learner capable of learning \ndifferent capabilities of each LLM to leverage diagnostic accuracy of the model by collective \ncapabilities of all LLMs in the cluster. The results of our study showed that aggregated diagnostic \naccuracy provided by a meta-learning approach exceeds that of any individual LLM, suggesting that \nthe super-learner can take advantage of the combined knowledge of the medical datasets used to \ntrain the group of LLMs.  \n \nKeywords: Super-learner, Meta-learner, Large Language Models, AI-Powered Diagnostics, \nMultiagent systems, Emergency Medicine \n1 Introduction \nReal-time accurate diagnostics is critically important in emergency medicine because of its time \nsensitive nature when delays can lead to severe complications or even to patient death. Emergency \ncare practitioners typically work under intense time and resource constraints. The urgency of making \n"}, {"page": 2, "text": "arXiv.2511.08614, Nov. 5, 2025  \nhttps://doi.org/10.48550/arXiv.2511.08614 \n2 of 12 \n \na immediate correct diagnosis, inability to conduct extensive observations and collect second \nopinion, negatively impacts on the ability of making a balanced diagnostic decision (Fleischmann et \nal, 2016). The elevated level of stress imposed on emergency clinicians adds to the accuracy of \ndiagnostic decisions too (Dias & Neto, 2017; Garcia-Tudela et al, 2022). All above mentioned factors \nresult in burnout syndrome among emergency medical physicians that also reduces their diagnostic \naccuracy (Boutou, 2019). \nAs it is reported in American Systematic Review (Newman-Toker et al, 2022), the level of \nmisdiagnoses of traumatic injuries in emergency medicine is quite low ranking about 5%-6% while \nthe rate of misdiagnoses in acute internal emergencies is significantly elevated. For some life \nthreatening and time critical diseases such as stroke, myocardial infarction, aortic \naneurysm/dissection, venous thromboembolism, spinal cord compression/injury, and spinal \nabscess misdiagnosis reaches up to 56% cases. Missed diagnoses for subarachnoid hemorrhage \nmay vary between 0% and 100% for different emergency hospitals. The level of 56% diagnostic errors \nin spinal epidural abscess was also reported by Bhise et al (2017). The assessment of the level of \ndiagnostic errors in the publications cited above (Newman-Toker et al, 2022; Bhise et al, 2017) was \nmade based on false-positive diagnoses. Also, the misdiagnoses do not include delayed diagnoses. \nThus, the number of timely unavailable correct diagnoses may be even higher than reported in the \nabove studies. The initial misdiagnosis and diagnostic delays have been historically described as \nhigh as 75%-89% (Tetsuka et al, 2020), which is quite high. Delayed diagnoses in emergency \nmedicine almost equal wrong diagnoses because medical actions to save patients in emergency \nmedicine should be taken without delays. \nDiagnostic accuracy is a complement of misdiagnoses to hundred percent. Misdiagnoses \nmay include delayed diagnoses or do not include delayed diagnoses. Such a difference in diagnostic \naccuracy assessment plays an important role in emergency medicine. Thus, the diagnostic accuracy \nwithout accounting for the delayed diagnoses can be assessed as about 40% while the diagnostic \naccuracy including the delayed diagnoses as failed is in the range of 11%-25% (Tetsuka et al, 2020) \nwhich can be averaged up as about 18%. A similar level of 20% diagnostic accuracy by human \ndoctors is reported by King & Nori, (2025).  \nQuite often, emergency physicians have no time and no practical ability to consult with other \nmedical professionals. Availability of real-time second opinion and practical advice would \nsignificantly improve quality of diagnostics and survival rate of emergency patients. Diagnostic errors \npropagate to clinical decision making, including imaging or test requests, the results interpretation, \nclinical assessment and illness treatment, particularly in atypical cases or in uncommon \npathological conditions (Ronicke et al, 2019).  \nThus, the specifics of emergency medicine suggest the need for a medical decision-support \ndiagnostic and advising system that will play a role of a real-time second opinion to enable medical \nprofessionals to quickly and accurately assess patients' conditions and make right decisions under \nstress and time limitations typical in emergency medicine. \nArtificial Intelligence has emerged as a transformative force in healthcare, particularly in \nmedical diagnostics. Applying advanced artificial intelligence, including LLMs in emergency \nmedicine provides an opportunity to improve diagnostics, predictive actions, treatment planning, \nand medication prescription using AI-based decision-support systems. In this paper, we evaluated \nthe performance of multiple top LLMs in medical advising for emergency medicine and introduced a \nsuper-learner of LLMs aimed at improving their accuracy and reliability. \n"}, {"page": 3, "text": "Aityan et al. (2025)  \nSuper-Learner for Medical Emergency Advising \n \n3 of 12 \n \n2 LLMs for Medical Applications \nRecent studies have demonstrated the effectiveness of LLMs to aid healthcare professionals in \ngenerating comprehensive lists of possible diagnoses based on patient’s symptoms, clinical \nobservations, medical history, and other relevant data (Huang et al, 2024; Meng et al, 2024; Nazi & \nPeng, 2024; Zhou et al, 2025). The utilization of LLMs for medical diagnostics has enhanced accuracy, \naccelerated decision-making processes, and improved treatment planning. A comparison of \nphysician and AI chatbot responses to public medical questions (Ayers et al, 2023) showed that \nevaluators preferred chatbot responses (78%) vs responses by human physicians (22%). Singhal et \nal (2025) reported that Med-PaLM2, a version of PaLM2 fine-tuned on medical data, has achieved a \nreasonably high accuracy rate in answering medical questions which was close to the accuracy \nachieved by human clinicians. \nThe integration of LLMs with medical imaging data is an emerging area of research. Multi-\nmodal models that combine text and image inputs can enhance the interpretation of radiology \nreports and improve diagnostic accuracy. Brin et al. (2025) assessed performance of a multimodal \nLLM that integrates visual and textual information to assist radiologists in interpreting medical \nimages. The study shows the potential of zero-shot generative AI in enhancing diagnostic processes \nin radiology. Huang et al (2024) showed that their LLM chatbot outperformed glaucoma specialists \nand matched retina specialists in diagnostic and treatment accuracy. \nAlthough pretrained LLMs take advantage of huge training datasets, they may lack \nspecialization in specific domains. Fine-tuning addresses this limitation by allowing the model to \nlearn from domain-specific knowledge to make it more accurate and effective for target application. \nEmergency medicine has its own unique goals, patterns, and context. Fine-tuning of pre-trained \nLLMs allows tailoring these models meeting the specific requirements of emergency medicine. \nFine-tuning large LLMs can adapt these pretrained general models on large datasets to \nspecific tasks in medical domain by further training it on clinical data. Yadav et al. (2024) evaluated \nfour LLMs (BART-base, BART-large-CNN, T5 large, and BART-large-xsum-samsum) with and without \nfine-tuning for diagnostics of mental disorders. The results highlighted that fine tuning significantly \nimprove their performance. and fine-tuned LLMs significantly outperform the same LLMs without \nfine-tuning. \nThere are different approaches for fine-tuning of LLMs. Full fine-tuning involves updating all \nthe parameters of a pretrained model on a new task-specific dataset. Devlin et al. (2019) \ndemonstrated this approach demonstrating that the model pre-trained on a large dataset was then \nfine-tuned on a specific task by adjusting all its weights. This method usually provides a higher level \nof alignment but can be computationally expensive. On the other hand, quite a few studies focused \non parameter-efficient fine-tuning methods that aim to reduce the number of parameters updated \nduring fine-tuning. Rebuffi et al. (2017) proposed fine-tuning only for a small subset of parameters, \nsuch as bias terms, which significantly reduced the computational overhead. \nPrefix-tuning and prompt-based fine-tuning adjust the input prompts or prefixes fed to the \nmodel rather than modifying the model parameters directly. Li & Liang (2021) proposed prefix- tuning, \nwhich adds prefixes to the input sequence, allowing the model to adapt to specific tasks. Unlike \ntraditional fine-tuning where the model's parameters are adjusted, prompt-based fine-tuning \ninvolves minimal or no changes to the model's weights. Instead, it relies on designing effective \nqueries which result in desired outputs from the model. This method leverages the pretrained \nmodel's existing knowledge and can be efficient for LLMs trained in various tasks and \nknowledgebases. \nSome modern LLMs allow fine tuning, however the most advanced commercial LLMs have \nrecently terminated this functionality because they claim to be trained on all available information. \n"}, {"page": 4, "text": "arXiv.2511.08614, Nov. 5, 2025  \nhttps://doi.org/10.48550/arXiv.2511.08614 \n4 of 12 \n \n3 Evaluating LLMs on Diagnostics in Emergency Medicine \nSignificant investments of big tech companies in commercial LLMs resulted in models that are \ngenerally more powerful and not comparable to their academic or open-source counterparts. \nHowever, despite huge investments and technical advantages in LLMs, each of commercial LLMs \nshows some flaws and limitations in medical diagnosis and treatment specific to each LLM. Fine-\ntuning reduces these problems but does not eliminate them. Thus, each commercial LLM has its \nstrengths and weaknesses. These shortcomings stem from the complexity of medical diagnostics, \nlimited scope of medical datasets used during pre-training and fine-tuning, which fail to capture the \nextremely broad variety of diseases, symptoms, test parameters and their correlations, patient \nconditions, apparent similarity between some diseases, and patient demographic variability. \nWe have developed a real-time solution for medical emergency diagnostics using \ncommercially available LLMs. For the evaluation purpose, we used GPT, Claude, and Gemini. \nA free format inquiry with a case description from an emergency physician was submitted to \nour system. Automatic prompt generation component processed the inquiry, generated the \nrespective prompts specific to each LLM, and submitted them to respective LLMs. The responses \nfrom each LLM were evaluated by a group of medical emergency physicians for diagnosis accuracy, \ntreatment advice, urgency detection, alternative diagnosis, and medical image interpretation using \na dataset of 120 real emergency cases. Each response was evaluated using a score from 0 through 4 \n(0 is performing baseline, 4 is the highest AI performance on current dataset). The final scores were \ncalculated as the average of these scores. \nThe results of the evaluation of LLM performance on emergency medicine diagnostics are \npresented in Table 1. As is evident from the table, ChatGPT got the highest score in diagnostic \naccuracy and was the best but not perfect in treatment advice, and image interpretation, while \nClaude was the best in urgency detection. All the three LLMs were equally good but not perfect in \nsuggesting alternative diagnoses. On the other hand, Gemini overperforms Claude in image \ninterpretation. \nTable 1: Comparing medical capabilities of three LLMs \nFeature \nChatGPT Gemini Claude \nDiagnostic Accuracy  \n4    \n2 \n3 \nTreatment Advice \n3.2 \n1 \n2 \nImage Interpretation \n3.3 \n1.7 \n1.3 \nUrgency detection  \n3 \n2 \n3.8 \nAlternative diagnoses \n3 \n3 \n3 \n \nThe overall better responses received from ChatGPT reflect fine-tuning on emergency cases \nwhile Gemini and Claude did not allow fine-tuning and have been used as is. \nThe results presented in Table 1 do not pretend to represent a comprehensive comparative \nstudy of LLMs but used only for the purpose of illustration that different LLMs perform differently on \ndifferent tasks. We are currently conducting a comprehensive study of variety of commercially \navailable LLMs in medical diagnostics and will report on our findings as soon as this study concludes. \nThus, different LLMs perform differently on different tasks. They also show different \nperformances on diagnostics of different diseases. The results of our comparative study were shown \nto illustrate that each LLMs has its strength and weaknesses. Thus, if used together as an integrated \nensemble, they may provide better results than each LLM alone. The collective capabilities of LLMs \n"}, {"page": 5, "text": "Aityan et al. (2025)  \nSuper-Learner for Medical Emergency Advising \n \n5 of 12 \n \nin medical diagnosis are expected to be stronger than each individual LLM. This identifies a need for \ndecision support solutions which can take advantage of collective capabilities of LLMs. \n4 A Super-learner of Large Language Models \nWe have developed a real-time super-learner solution “MEDAS” (Medical Emergency Decision \nAdvising System) for advising physicians in medical emergency diagnostics. MEDAS stands for \nMedical Emergency Diagnostics Advising System. The goal of the system is to provide in real time \npossible diagnoses for an emergency case. The possible diagnoses are generated together with the \nrespective probabilities and urgencies of intervention. \nThe MEDAS super-learner integrates a number of AI engines which are represented in the \ncurrent version by several commercially available LLMs such as ChatGPT, Gemini, Claude, \nNemotron, and Llama. Any additional AI agent can be easily integrated into the system. \nThe Super-learner contains a meta-learner which receives responses from different LLMs \nand learns how to produce optimum results and employ capabilities of each individual LLM (Figure \n1). \n \nFigure 1: Super-learner of LLMs \nIn case description inquiry is entered by an emergency physician using the MEDAS \ncommunication component via a web browser or the respective app on a smartphone. The inquiry \nconsists of a free format case description that includes all information the physician wants to \ncommunicate to the MEDAS advising system. The inquiry can be made as a text or voice entry or a \nmix of them. \nThe super-learner prompt generation and dispatching component processes the inquiry and \ngenerates the appropriate prompts for each of the integrated LLMs. Then the prompts are sent in \nparallel to the respective LLMs. Each engaged LLM generates a response to the inquiry that consists \nof possible diagnoses together with their respective probabilities and urgencies. \nAll generated responses are submitted to the meta learner component of the super-learner. \nThe meta learner consolidate all responses into one combined response to the emergency physician. \nSuper Learner \n \nPrompt generation and dispatching \nLLM₁ \nLLM₂ \nLLM₃ \nLLMK \nLLM₄ \n… \nPatient case description query \nEmergency doctor’s final decision \nWeb- and Mobile- \nMEDAS \ncommunication \ncomponent \ni\n/\nResp₁ Resp₂ Resp₃ \nRespK \nResp₄ … \nMeta Learner \nResponse aggregation \nDiagnostic advising \nresponse \n"}, {"page": 6, "text": "arXiv.2511.08614, Nov. 5, 2025  \nhttps://doi.org/10.48550/arXiv.2511.08614 \n6 of 12 \n \nThe consolidated response is communicated back to the emergency physician who \nconsiders it as second opinion advice. Finally, the emergency physician makes the diagnostic and \ntreatment decision on the case using the MEDAS response to the case. \nMEDAS is not making any decisions for the emergency physician but provides all possible \ndiagnoses leaving the final decision on the actual physician. \nMEDAS super-learner benefits from all medical datasets used for training a group of LLMs, \nhence, it can provide more comprehensive information and cover a wider range of health conditions \nand possible diagnoses. The super-learner also benefits from different capabilities of LLMs in various \naspects of emergency medicine. \n \nMeta-Learner \nDiagnostic capabilities of commercial LLMs are not the same for different diseases. Some LLMs are \nbetter or worse in diagnosing some diseases (Aityan et al, 2024). There are also specialized AI agents \ndesigned for diagnosing some specific diseases, for example, early diagnostic of sepsis (Aityan et al, \n2025). This is due to the differences in their specific focus and differences in medical datasets used \nfor training. The range and content of disorders and symptoms present in those training datasets \ndiffer, which can significantly affect the diagnostic capabilities of the LLM. Also, one LLM can provide \nthe best results in diagnostics by symptoms while another LLM might be superior in interpretation of \nmedical conditions. Thus, different LLMs may provide complementary diagnostic results. \nThe role of a meta-learner is to provide an aggregated response from the super-learner to the \nphysician who asked for diagnostic advice. The meta-learner takes advantage of the best capabilities \nof each individual LLM. It can learn which LLM is best suited for a specific disease and suggesting a \nset of more reliable possible diagnoses to optimally combine outputs of the engaged LLMs to achieve \nthe best outcome. \nThe meta-learner is trained on a dataset containing the responses from the LLMs integrated \ninto the super-learner and confirmed diagnoses provided by medical emergency doctors for patient \ncases once the diagnoses are finalized. A trained meta-learner can produce optimal results based \non collective capabilities of LLMs engaged in the super-learner.  \nWhile LLMs are black-box models, the meta-learner model can employ a more \ncomprehensive approach to provide more explicitly understood results. The combination of LLMs as \nblack-box models with the meta-learner provides the super-learner with a certain degree of \ntransparency and explainability valuable for the medical decision-support system.  \nThe meta-learner generates the final response to the diagnostic inquiry by aggregating the \nresults generated by the LLMs integrated into the super-learner. The aggregation mechanism selects \nthe best results generated by the ensemble of LLMs integrated in the supe learner.  \nIn the meta-learner, overall performance in disease diagnostics of each integrated LLM is \nevaluated, and the respective weights are assigned for the selection of the best responses. These \nweights are learned through the ongoing training process based on evaluation of the LLMs responses \nby comparing them with the confirmed diagnoses by medical emergency doctors on real data of \nmedical emergency cases. \nFor advising on a specific health condition, each individual LLM in the super-learner model \nreceives the same diagnostic inquiry specially configured for each integrated LLM by the automatic \nprompt generation and generates a response to the received query. Each response from each LLM \nincludes a list (vector) of possible diagnoses, together with the list (vector) of respective probabilities \nof the suggested diagnoses. \n"}, {"page": 7, "text": "Aityan et al. (2025)  \nSuper-Learner for Medical Emergency Advising \n \n7 of 12 \n \n5 Super-learner Performance  \nThe performance of the super-learner MEDAS solution was tested with the conventional majority vote \nused by the meta-learner for each individual LLM integrated in the super-learner, GPT-4o, Claude \nOpus 4, Llama Maverick 4, Grok 4, and Google Gemini 2.5 Pro. \nWe tested the accuracy of each of the engaged LLMs on a dataset of 420 real acute internal \nemergencies cases, which included information on patients’ health conditions, medical records, test \nresults, and the corresponding confirmed diagnoses. Then we evaluated the accuracy of the meta-\nlearner aggregated response using the majority vote based on weights adjusted proportionally to the \naccuracies of the LLMs on the same dataset.  \nEmergency physicians assessed the responses based on Pass@1 accuracy (the most \nprobable diagnosis) by comparing them to the confirmed diagnoses (Table 2). Only exact matches \nwith the confirmed diagnoses were considered correct while partial alignments were considered \nincorrect, in accordance with requirements of emergency medicine. \nTable 2: Number of correct responses (Pass@1) from 420 emergency cases \nLLMs and Super-learner \nAccuracy \nGemini \n58% \nLlama \n59% \nGrok \n60% \nGPT \n65% \nClaude \n65% \nSuper-learner on majority votes \n70% \nSuper-learner, at least one LLM \n85% \n \nTable 2 shows that accuracy of the individual LLMs vary between 58% and 65% while the \nmeta-learner based on a simple Pass@1 majority vote mechanism responds with the accuracy, 67%, \nwhich is higher than any individual LLM alone. However, at least one of the integrated LLMs generates \ncorrect response in 85% of cases which is substantially higher. This explicitly demonstrates that a \nsuper-learner approach has great potential in advising physicians in emergency diagnostics with \nmuch higher accuracy than any individual LLM. A more sophisticated meta-learner model that can \nlearn to choose the best results from the individual LLMs integrated into a super-learner is expected \nto provide aggregated diagnostic accuracy closer to the above-mentioned 85%. This indicates great \npotential for the super-learner approach. \nTo compare the super-learner performance with human physicians, we randomly selected 10 \nacute internal emergencies cases from the same dataset and asked a group of 12 experienced \nemergency medicine physicians to provide diagnoses by reading and analyzing acute internal \nemergenciy cases. This has allowed us to obtain a rough estimate of human doctors’ diagnostic \naccuracy compared to the accuracy of the MEDAS AI solution. Figure 2 shows the results of this \nexperiment with human doctors.  \n \n"}, {"page": 8, "text": "arXiv.2511.08614, Nov. 5, 2025  \nhttps://doi.org/10.48550/arXiv.2511.08614 \n8 of 12 \n \n \nFigure 2: Diagnostic accuracy of 12 human doctors in emergency medicine \nThe average diagnostic accuracy across all twelve emergency physicians was 41%. With \nstandard deviation 9%. The diagnostic accuracy by human doctors varied between 20% and 50%. \nThese results do not represent a rigorous study but were intended for an estimate of human doctors’ \ndiagnostic accuracy. However, our estimates are quite consistent with the diagnostic accuracy by \nhuman physicians reported by other research groups (Tetsuka et al, 2020; Newman-Toker et al, 2022; \nNori et al, 2025). \nFigure 3 shows a comparison of diagnostic accuracy achieved by general medicine doctors \n(18% on average reported by Tetsuka et al, 2020; 20% reported by King & Nori, 2025), and 43% \nreported by Newman-Toker et al, 2022), 41% on average by emergency medicine doctors in our own \nresearch (Figure 2), individual LLMs, and the super-learner with the same LLMs (Table 2). \n \nFigure 3: Diagnostic accuracy by general medicine doctors, emergency doctors, individual \nLLMs, and the super-learner \nAs is evident from Figure 3, individual LLMs provide higher diagnostic accuracy than human \ndoctors because they have been trained on a much broader volume of clinical information and \npossess huge amount of knowledge which no individual human is capable to acquire. A super-\nlearner with the same LLMs integrated into a single solution produces higher diagnostic accuracy \neven with a quite basic meta-learner (~70%). However, at least one of the integrated LLMs in the same \nsuper-learner produces 85% correct diagnoses. This clearly suggests that a more sophisticated \nmeta-earner will be capable of significantly improving the diagnostic accuracy of the super-learner \nby more accurately selecting the best diagnoses generated by the integrated LLMs. \nThe architecture of the super-learner of commercially available LLMs for medical advising in \nemergency medicine has evolved from our previous integrated ensemble of AI-based diagnostic \n58%\n18%\n59%\n20%\n41%\n60%\n70%\n85%\n43%\n65%65%\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nGeneral\nmedicine\ndoctors\nEmergency\nmedicine\ndoctors\nIndividual\nLLMs\n(Pass@1)\nSuper-learner\non majority\nvotes\nSuper-learner\nat least one\ncorrect\nAccuracy\nDiagnostic Sources\nDiagnostic Accuracy\nAverage 61%\n"}, {"page": 9, "text": "Aityan et al. (2025)  \nSuper-Learner for Medical Emergency Advising \n \n9 of 12 \n \nsolutions including LLMs for emergency medicine (Aityan et al, 2024, 2025a). The super-learner \nsolution may be expanded beyond commercially available LLMs by engaging some other specialized \nAI-agents in the super-learner solution. For example, we also integrated our proprietary AI-based \nearly sepsis prediction system to the diagnostic advising super-learner as one of the integrated \nagents (Aityan et al, 2025b).  \nMicrosoft has recently claimed that their AI-based general purpose medical diagnostic \nsolution (King & Nori, 2025) achieves 85% diagnostic accuracy. However, according to their own \ninformation, this accuracy is shown as an extreme point on the frontier curve of their performance \nchart. Such accuracy can be achieved as a target for high-cost diagnostics, which is most likely \nassociated with comprehensive testing and extended time of patient observation. This is not typical \nfor regular emergency medicine. In typical emergency medicine settings, cost, time, and resources \nfor diagnostics in emergencies are normally limited that corresponds to the low-cost range of \naccuracy of the Microsoft diagnostic frontier which does not exceed 50% or at most 60% as clearly \nindicated in their performance chart. \n6 Discussion \nWe aimed to evaluate the performance of the proposed super-learner approach in real-world \nemergency medicine settings. To do so, the majority vote, weighted consolidated responses and \neach individual LLM were assessed separately using the same test dataset. \nThe results show significant differences in the diagnostic capabilities of LLMs for diagnosis \nof patients’ health conditions in emergency cases. \nIn this regard, we proposed a super-learner to perform meta-learning over the capabilities of \nLLMs integrated into the model and train weights based on the performance of the LLMs to improve \naccuracy. The results highlighted that weighted majority voting can improve the diagnostic outcome. \nHowever, the comparison between the accuracy of the aggregated response and that of each \nindividual model shows that the aggregated response exceeds the performance of any individual \nLLM. This suggests that the super-learner can benefit from the combined knowledge of the medical \ndatasets used to train the engaged LLMs. Thus, it offers more precise advice and covers a wider range \nof health conditions. \nAlthough the performance of each individual LLM exceeds the performance of human \ndoctors, the collective capabilities of integrated LLMs leveraged by the super-learner can \nsignificantly improve overall performance of individual LLMs. \n7 Limitations and Future Research Directions \nIn this study, we employed five commercial LLMs for the performance test of the white-box meta-\nlearning approach. Adding more AI agents can leverage the performance of the meta-learning and \nenrich the results of the study. Also, the weights used in the current study can be fine-tuned to \nprovide better performance.   \nOur study was primary focused on diagnostics in emergency cases. However, medical \nadvising covers many other aspects including planning treatment and complementary clinical tests. \nCooperative multi-agent AI can leverage our super-learner model. Each agent can be a Super-learner \nitself specialized in one specific task in emergency medicine including diagnosis, treatment, \ninterpretation of medical images and laboratory tests. \n Each agent also takes advantage of the expertise of other agents. The agents specialized in \ndiagnostics based on textual or voice notes from doctors may benefit from interpretation of medical \nimages analyzed by other agents and provide inputs for other agents specialized in advising \n"}, {"page": 10, "text": "arXiv.2511.08614, Nov. 5, 2025  \nhttps://doi.org/10.48550/arXiv.2511.08614 \n10 of 12 \n \ntreatments. Such cooperative AI agents enable medical professionals to quickly and accurately \nassess patients' conditions and make the right decisions under stress and time limitations typical for \nemergency medicine. Each agent collaborates with other agents using multi-agent coordination \ntechniques based on MCP (Model Context Protocol) design principles which ensure that agents can \nwork together and make joint decisions effectively. \nMoreover, some specialized AI models including ad-hoc models for prediction life-\nthreatening diseases and models for analysis patterns from wearables can be added to this agentic \nframework for leveraging the capabilities of the model. \nOur future directions also include work on an efficient self-learning meta-learner which can \nperform a more comprehensive aggregation of the responses from LLMs and other AI-agents \nintegrated into a super-learner to provide high accuracy combined response to emergency doctors. \nThe quest for higher diagnostic accuracy may hit fundamental limits of medical diagnostics \nrelated to immanent uncertainty caused by complexity of human body. Thus, it is extremely \nimportant to investigate and find out such limitations to understand the natural limits of AI approach \nin medicine. \n8 Conclusion \nMedical diagnostics and treatment in emergency medicine is a complex process characterized by \nmultiple uncertainties, ambiguous symptoms, limited time as well as limited human and clinical \nresources during emergency. In many cases, even experienced physicians may not arrive at the \ncorrect diagnosis based on initial observations. For this reason, a real-time, precise AI-based \nmedical advising system can be lifesaving. \nIn this regard, we introduced a super-learner to take advantage of collective capabilities of \nmultiple LLMs. The framework can employ both white-box and black-box meta-learner to produce \noptimal results from outputs of LLMs fine-tuned for emergency medicine.  \nThe results show that the super-learner can take advantage of different capabilities of LLMs \nin various aspects of emergency medicine since it utilizes the strengths and knowledgebases of \nmultiple LLMs to benefit from their different specializations for diagnostic advising. This approach is \napplicable to many other decision-support systems dealing with a list of possibilities (e.g., financial \ndecisions) since the accuracy provided by the super-learner model exceeds the accuracy of each \nindividual LLM. The super-learner also provides more reliability for impactful decisions compared to \nan individual AI agent, since it considers multiple advising responses. Additionally, it avoids \nhallucinated diagnoses, which are common in LLMs, since it is highly implausible that several LLMs \nwould generate the same hallucinated diagnosis for a single case. \nThe introduced model takes advantage of the massive investments by big tech companies in \ncommercial LLMs, without needing to build an individual LLM from scratch. It is also dynamically \nupgradable by integrating new or upgraded LLMs that add capabilities to the system. Moreover, the \naggregation mechanism is scalable, as each integrated LLM processes queries independently and in \nparallel. This makes it possible to add many new LLMs without significant computational cost or \naggregation overhead. These characteristics make the super-learner a less expensive yet scalable \nand more efficient alternative to the huge commercial models for decision support systems. Finally, \nthe successful integration of AI-based medical advising in emergency medicine paves the way for the \nnext generation of autonomous medical agents, which can diagnose diseases and prescribe real-\ntime treatments without human intervention. \n"}, {"page": 11, "text": "Aityan et al. (2025)  \nSuper-Learner for Medical Emergency Advising \n \n11 of 12 \n \nReferences \nAityan, S. K.; Mosaddegh, A.; Herrero, R.; Inchingolo, F.; Nguyen, K. C.; Balzanelli, M.; Lazzaro, R.; \nIacovazzo, N.; Cefalo, A.; Carriero, L.; Mersini, M.; Legramante, J.M.; Minieri, M.; \nSantacroce,L.; and Gargiulo Isacco, C. (2024). Integrated AI Medical Emergency Diagnostics \nAdvising System. Electronics, 13(22), 4389. doi.org/10.3390/electronics13224389 \nAityan, S. K.; Mosaddegh, A.; Herrero, R.;  Gargiulo, C.; Lazzaro, R.; and Iacovazzo, N. (2025). \nSpeech-Driven Medical Emergency Decision-Support Systems in Constrained \nEnvironments, in Engineering Cyber-Physical Systems and Critical Infrastructures \nNetworking Data Integrity and Manipulation, in Cyber-Physical and Communication \nSystems, Springer, ISSN: 2731-5002, p. 119-140. doi: 10.1007/978-3-031-83149-2_6 \nAityan, S.; Herrero, R.; Mosaddegh, A.; Tayyar, H.; Adebesin, E.; Jeedigunta, S.P.; Kim, H. Mersini, M.;  \nLazzaro, R.; Iacovazzo, N; and Isacco, C.G.;  (2025). AI-Powered Early Detection of Sepsis in \nEmergency Medicine, Life 15(10): 1576. Doi: 10.3390/life15101576 \nAyers, J.W.; Poliak. A.; Dredze, M.; Leas, E.C.; Zhu, Z.; Kelley, J.B.; Faix, D.J.; Goodman, A.M.; \nLonghurst, C.A.; Hogarth, M.; Smith, D.M. (2023). Comparing physician and artificial \nintelligence chatbot responses to patient questions posted to a public social media forum. \nJAMA Intern. Med., 183, 589–596. doi:10.1001/jamainternmed.2023.1838 \nBhise, V.; Meyer, A.N.D.; Singh, H.; Russo, E.; Al-Mutairi, A; Murphy, D.R.: and Wei, L.  (2017). Errors \nin Diagnosis of Spinal Epidural Abscesses in the Era of Electronic Health Records, The \nAmerican Journal of Medicine, 130(8), doi: 10.1016/j.amjmed.2017.03.009 \nBoutou, A; Pitsiou, G.; Sourla, E.; Kioumis, I (2019). Burnout syndrome among emergency medicine \nphysicians: an update on its prevalence and risk factors, European Review for Medical and \nPharmacological Sciences, 23: 9058-9065. \nBrin, D., Sorin, V.; Barash, Y.; Konen, E.; Glicksberg, B. S.; Nadkarni, G. N.; and Klang, E. (2025). \nAssessing GPT-4 multimodal performance in radiological image analysis. European \nRadiology, (4):1959-1965. doi: 10.1007/s00330-024-11035-5. \nDevlin, J.; Chang, M. W.; Lee, K., & Toutanova, K. (2019). BERT: Pre- training of Deep Bidirectional \nTransformers for Language Understanding. Proceedings of NAACL-HLT, (1):4171-4186. doi = \n10.18653/v1/N19-1423 \nDias, R.D. and Neto, A.S. (2017). Acute stress in residents during emergency care: a study of \npersonal and situational factors, The International Journal on the Biology of Stress, 20 (3): \n241-248. doi.org/10.1080/10253890.2017.1325866 \nFleischmann, C.; Scherag, A.; Adhikari, N.K J; Hartog, C.S; Tsaganos, T.; Schlattmann, P.; Angus, \nD.C; Reinhart, K. (2015). Assessment of global incidence and mortality of hospital-treated \nsepsis. Current estimates and limitations, Am J Respir Crit Care Med., 2016; 193(3), 259-\n272. \nGarcía-Tudela, Á.; Simonelli-Muñoz, A.J.; Rivera-Caravaca, J.M.; Fortea, M.I.; Simón-Sánchez, L.; \nGonzález-Moro, M.T.R.; González-Moro, J.M.R.; Jiménez-Rodríguez, D.; Gallego-Gómez, J.I. \n(2022). Stress in Emergency Healthcare Professionals: The Stress Factors and \nManifestations Scale, Int J Environ Res Public Health, Apr 5,19(7), 4342. doi: \n10.3390/ijerph19074342 \nHuang, A.S.; Hinabayashi, K.; Barna, L.; Parikh, D.; Pasquale, L.R. (2024). Assessment of a Large \nlanguage model’s Responses to Questions and Cases About Glaucoma and Retina \nManagement, JAMA Ophtalmology, 2024 Apr 1;142(4):371-375. doi: \n10.1001/jamaophthalmol.2023.6917. \nKing, D. and & Nori, H. (2025). The Path to Medical Superintelligence, Microsoft press release, June \n30, 2025, https://microsoft.ai/news/the-path-to-medical-superintelligence/ \n"}, {"page": 12, "text": "arXiv.2511.08614, Nov. 5, 2025  \nhttps://doi.org/10.48550/arXiv.2511.08614 \n12 of 12 \n \nLi, X. L. and Liang, P. (2021). Prefix-Tuning: Optimizing Continuous Prompts for Generation. \nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics, \n(Volume 1: Long Papers), 4582-4597. doi.org/10.48550/arXiv.2101.00190 \nMeng, X.;  Yan, X.; Zhang, K.; Liu, D.; Cui, X.; Yang, Y.; Zhang, M.; Cao, C.; Wang, J.; Wang, X.; Gao, J.; \nWang, Y.G.; Ji, JM.; Qiu, Z.; Li, M.; Qian, C.; Guo, T.; Ma, S.; Wang, Z.; Guo, Z.; Lei, Y.; Shao, \nC.; Wang, W.; Fan, H.; and Tang, YD. (2024). The application of large language models in \nmedicine: A scoping review. iScience. 2024 Apr 23;27(5):109713. doi: \n10.1016/j.isci.2024.109713. PMID: 38746668; PMCID: PMC11091685. \nNazi, Z. A. and Peng, W. (2024). Large language models in healthcare and medical domain: A \nreview, Informatics, Vol. 11, No. 3, p. 57). doi.org/10.3390/informatics11030057 \nNewman-Toker, D.E.; Peterson, S.M.; Badihian, S.; Hassoon, A.; Nassery, N.; Parizadeh, D.; Wilson \nL.M.; Jia, Y.; Omron, R.; Tharmarajah, S.; Guerin, L.; Bastani, P.B.; Fracica, E.A.; Kotwal, S.; \nRobinson, K.A. (2022). Diagnostic Errors in the Emergency Department: A Systematic \nReview [Internet], Rockville (MD): Agency for Healthcare Research and Quality (US), Dec. \nReport No.: 22(23)-EHC043. PMID: 36574484. doi: 10.23970/AHRQEPCCER258, \nhttps://www.ncbi.nlm.nih.gov/books/NBK588123/ \nRebuffi, S. A.; Bilen, H.; and Vedaldi, A. (2017). Learning multiple visual domains with residual \nadapters. Advances in neural information processing systems, 30. \ndoi.org/10.48550/arXiv.1705.08045 \nRonicke, S.; Hirsch, M. C.; Türk, E.; Larionov, K.; Tientcheu, D.; and Wagner, A. D. (2019). Can a \ndecision support system accelerate rare disease diagnosis? Evaluating the potential impact \nof Ada DX in a retrospective study. Orphanet journal of rare diseases, 14(1): 69. doi: \n10.1186/s13023-019-1040-6  \nShortliffe, E. H.; Axline, S. G.; Buchanan, B. G. ; Cohen, S. N.; and Lederberg, J. (1973). \"An artificial \nintelligence program to advise physicians regarding antimicrobial therapy,\" Proceedings of \nthe 3rd International Joint Conference on Artificial Intelligence (IJCAI-73), pp. 466–470. \nSinghal, K.; Tu, T.; Gottweis, J.; Sayres, R.; Wulczyn, E.; Amin, M.; Hou, L.; Clark, K.; Pfohl, S.R.; \nCole-Lewis, H.; Neal, D.; Rashid, Q.M.; Schaekermann, M.; Wang, A.; Dash, D.; Chen, J.H.; \nShah, N.H.; Lachgar, S.; Mansfield, P.A.; Prakash, S.; Green, B.; Dominowska, E.; Agüera, Y. \nA. B.; Tomašev, N.; Liu, Y.; Wong, R.; Semturs, C.; Mahdavi, S.S.; Barral, J.K.; Webster, D.R.; \nCorrado, G.S.; Matias, Y.; Azizi, S.; Karthikesalingam, A.; Natarajan, V. (2025). Toward expert-\nlevel medical question answering with large language models. Nat Med., 31(3):943-950. doi: \n10.1038/s41591-024-03423-7. Epub 2025 Jan 8. PMID: 39779926; PMCID: PMC11922739. \nTetsuka, S.; Suzuki, T.; Tomoko Ogawa, T.; Hashimoto, R.; and Kato, H. (2020). Spinal Epidural \nAbscess: A Review Highlighting Early Diagnosis and Management, JMA J. 2020 Jan \n15;3(1):29-40. doi: 10.31662/jmaj.2019-0038. \nYadav, M.; Sahu, N. K.; Chaturvedi, M.; Gupta, S.; and Lone, H. R. (2024). Fine-tuning Large \nLanguage Models for Automated Diagnostic Screening Summaries. arXiv preprint \narXiv:2403.20145. doi.org/10.48550/arXiv.2403.20145.  \nZhou, Z.; Xu, Z.; Zhang, M.; Xu, C.; Guo, Y.; Zhan, Z.; Fang, Y.; Dong, S.; Wang, J.; Xu, K.; Xia, L.; \nYeung, J.; Zha, D,; Cal, D.; Melton, G.B.; Lim, M. and Zhang, R. (2025). Large language \nmodels for disease diagnostics; a scoping review, Cornel Univdersity, arXiv:2409.00097, \nhttps://arxiv.org/pdf/2409.00097 \n \n \n \n \n"}]}