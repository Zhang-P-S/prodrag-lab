{"doc_id": "arxiv:2602.00981", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.00981.pdf", "meta": {"doc_id": "arxiv:2602.00981", "source": "arxiv", "arxiv_id": "2602.00981", "title": "MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA", "authors": ["Yutong Song", "Shiva Shrestha", "Chenhan Lyu", "Elahe Khatibi", "Pengfei Zhang", "Honghui Xu", "Nikil Dutt", "Amir Rahmani"], "published": "2026-02-01T02:44:08Z", "updated": "2026-02-01T02:44:08Z", "summary": "Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.00981v1", "url_pdf": "https://arxiv.org/pdf/2602.00981.pdf", "meta_path": "data/raw/arxiv/meta/2602.00981.json", "sha256": "4e1c70a2f5eda8e69c395725331c2f22c44bf56cd7e9814908004f7c4d4f5ce4", "status": "ok", "fetched_at": "2026-02-18T02:20:05.876772+00:00"}, "pages": [{"page": 1, "text": "MEDSPEAK: A KNOWLEDGE GRAPH-AIDED ASR ERROR CORRECTION FRAMEWORK\nFOR SPOKEN MEDICAL QA\nYutong Song⋆, Shiva Shrestha†, Chenhan Lyu⋆, Elahe Khatibi⋆,\nPengfei Zhang⋆, Honghui Xu†, Nikil Dutt⋆, Amir Rahmani⋆\n⋆University of California, Irvine, Computer Science, Irvine, CA, USA\n†Kennesaw State University, Computer Science, Kennesaw, GA, USA\nABSTRACT\nSpoken question-answering (SQA) systems relying on au-\ntomatic speech recognition (ASR) often struggle with ac-\ncurately recognizing medical terminology. To this end, we\npropose MedSpeak, a novel knowledge graph-aided ASR\nerror correction framework that refines noisy transcripts and\nimproves downstream answer prediction by leveraging both\nsemantic relationships and phonetic information encoded\nin a medical knowledge graph, together with the reasoning\npower of LLMs.\nComprehensive experimental results on\nbenchmarks demonstrate that MedSpeak significantly im-\nproves the accuracy of medical term recognition and overall\nmedical SQA performance, establishing MedSpeak as a state-\nof-the-art solution for medical SQA. The code is available at\nhttps://github.com/RainieLLM/MedSpeak.\nIndex Terms— Knowledge Graph, Automatic Speech Recog-\nnition, Spoken Question Answering, Large Language Models\n1. INTRODUCTION\nThe increasing adoption of speech-based AI systems in medi-\ncal applications has led to the growing need for high-accuracy\nspoken medical question answering (SQA) systems. These\nsystems rely on Automatic Speech Recognition (ASR) to\ntranscribe spoken medical queries before feeding them into\nretrieval-augmented generation (RAG) models or large lan-\nguage models (LLMs) for answer generation.\nHowever,\ndespite advancements in ASR models, such as Whisper and\nWav2Vec2, their performance remains suboptimal in medi-\ncal settings due to several domain-specific challenges [1, 2].\nThese challenges include complex medical terminology, pho-\nnetic ambiguities, and contextual dependencies, which de-\ngrade ASR transcription accuracy and impact downstream\nmedical reasoning [3, 4]. In other words, traditional ASR\nmodels, trained on general-domain corpora, often struggle\nto accurately recognize specialized medical terminology,\nleading to frequent misrecognitions of critical medical en-\ntities [5, 6]. Although some domain-specific finetuning ap-\nproaches have been attempted [2, 7, 8], they are restricted by\ndata insufficiency and lack of generalization. Additionally,\n[SPEECH]\nA neonate presented with cicatrizing skin lesions all over the body with \nhypoplasia of all limbs. An MRI of the brain revealed diffuse cerebral \natrophy. An ophthalmologic evaluation reveals chorioretinitis. Which of \nthese tests is most likely to show a positive result in this patient? \n[TEXT]\nOption A: Anti-HCMV antibodies Option B: Anti-toxoplasma antibodies \nOption C: Anti-VZV antibody Option D: Anti-rubella antibody \n[OUTPUT] The correct answer is option C\nExample\nFig. 1. An Example of SQA.\nthese domain-specific models still face challenges with pho-\nnetic confusability between terms like ”hypertension” and\n”hypotension,” further exacerbating ASR errors [9]. While\ndistinct in their medical implications, those words can be\nconfusingly similar to ASR systems, leading to errors in tran-\nscription. These deficiencies have negative impacts in clinical\napplications where transcription errors cascade to incorrect\nreasoning with resulting threats to patient safety. This under-\nscores the need for a robust ASR with a reliable QA system\nto support trustworthy healthcare decisions.\nTo address these challenges, in this paper, we propose\nMedSpeak, a knowledge graph-aided ASR error correction\nframework designed specifically for spoken medical QA\ntasks. By harnessing the power of knowledge graphs and the\nreasoning capabilities of LLMs, MedSpeak integrates struc-\ntured domain knowledge, phonetic and semantic similarity\nconstraints, and context-aware reasoning to significantly im-\nprove medical ASR correction and QA ability, as shown in\nFig. 2. Our contributions include: (1) An automated medical\nknowledge graph (KG) construction method encodes seman-\ntic and phonetic relationships between terms, enabling robust\nerror correction and reasoning.\n(2) A multi-constraint re-\ntrieval mechanism leverages phonetic and semantic features\nto generate accurate ASR hypotheses for medical terminol-\nogy. (3) A fine-tuned LLM integrates retrieved knowledge\nwith answer constraints, producing precise transcriptions and\nreliable answers. (4) Extensive experiments on spoken medi-\ncal QA datasets show that MedSpeak achieves state-of-the-art\nASR error correction in the medical domain.\narXiv:2602.00981v1  [cs.CL]  1 Feb 2026\n"}, {"page": 2, "text": "2. RELATED WORKS\nTraditional ASR models, trained on general-domain corpora,\noften struggle to accurately recognize specialized medical ter-\nminology, leading to frequent misrecognitions of critical med-\nical entities [1, 5, 6, 7]. While fine-tuning ASR models on\ndomain-specific datasets has been explored, these efforts are\nstill limited by data scarcity and poor generalization [2, 7, 8].\nAdditionally, these models face challenges with phonetic con-\nfusability between terms like ”hypertension” and ”hypoten-\nsion,” further exacerbating ASR errors [3, 4, 9].\nTo address these challanges, RAG-based methods aim to\nimprove ASR accuracy and named entity recognition [10]\nby incorporating external domain specific knowledge re-\ntrieval [11, 12, 13]. Techniques such as GEC-RAG [2] and\nIRAG [14] utilize contextually retrieved text snippets to refine\nASR hypotheses. However, these approaches relies on text-\ncentric evidences and fail to leverage phonetic knowledge,\nlimiting their ability to resolve confusion between phoneti-\ncally similar medical entities. Structured knowledge graphs\nhave been explored for domain-specific ASR correction, par-\nticularly in medical QA systems. MedRAG [15] incorporates\na medical KG into retrieval-based ASR correction but does\nnot explicitly address phonetic similarity for disambiguation.\nSimilarly, RANEC [3] leverages named entity correction for\nASR refinement but overlooks the integration of multiple-\nchoice context for better accuracy.\n3. MEDSPEAK\nOur MedSpeak framework combines static KG context injec-\ntion with a fine-tuned LLM, explicitly integrating both se-\nmantic relationship and phonetic similarity into LLM fine-\ntuning. This combined representation allows for robust ASR\nerror correction and QA reasoning within a unified two-line\nsupervision format, achieving great performance across mul-\ntiple medical QA benchmarks. Fig. 1 presents an example of\nour spoken question-answering (SQA) framework.\nASR text + options\nNodes: medical entities\nEdges: sematic and phonetic\nrelations\nKG retrieval\n<prompt>\nContext Construction\nQuery extraction\nLLM training\nASR text + relations in KG\nFig. 2. Overview of MedSpeak framework.\nThe system receives two distinct inputs: (1) a clinical case\ndescription in speech form (containing key diagnostic terms\nhighlighted in yellow such as ”hypoplasia,” ”cerebral atro-\nphy,” and ”chorioretinitis”), and (2) four possible diagnostic\ntest options provided as text (Options A-D). The system then\nproduces two outputs: the accurately transcribed speech and\nthe correct option (Option C). This example demonstrates the\ncritical challenge of accurately recognizing specialized med-\nical terminology in speech, as mis-recognition of terms like\n”chorioretinitis” as ”chorioamnionitis” would lead to incor-\nrect evidence retrieval and subsequent diagnostic errors. Our\nproposed MedSpeak leverages knowledge graph-aided error\ncorrection to improve recognition of these critical diagnostic\nterms, ensuring that the speech-to-text conversion preserves\nthe medical meaning necessary for selecting the appropriate\ndiagnostic test. In the following, we detail two main compo-\nnents in MedSpeak.\n3.1. Knowledge Graph Building\nWe construct our knowledge graph using the Unified Medical\nLanguage System (UMLS) dataset from the National Insti-\ntutes of Health (NIH) [16], establishing relationships between\nmedical terms through the MRREL table. During construc-\ntion, we iteratively select medical keywords from the table\nwhile filtering out generic relationships, duplicate entries, and\nrelationship pairs that lack specific semantic information. Af-\nter each keyword is selected, we use The CMU Pronouncing\nDictionary to generate phonetically similar words and append\nthem to our knowledge graph. By removing these uninfor-\nmative relationships and adding pronunciation information,\nwe streamline both the knowledge graph construction process\nand improve retrieval efficiency.\nCongenital VZV Infection\nHypoplasia\nHyperplasia\nAplasia\nCerebral atrophy\nCerebral dystrophy\nChorioretinitis\nChorioamnionitis\nCicatrizing skin lesions\nDysphagia\nDysphasia\nRubella virus\nToxoplasma\nVZV\nHCMV\nMRI\nAnti-HCMV antibodies\nOphthalmologic evaluation\nAnti-rubella antibody\nAnti-VZV antibody\nAnti-toxoplasma antibodies\nNode Types\nDisease\nSymptom\nPathogen\nTest\nSimilarly Terms\nFig. 3. Medical KG Constructed from SQA.\nAs outlined in Fig. 2, in the ”Relations in the Knowl-\nedge Graph” step, we deploy the Double Metaphone algo-\nrithm and Levenshtein Distance metric to identify English\nwords with similar pronunciation to medical entities. Since\nDouble Metaphone alone produces excessive false positives,\ncombining both metrics enhances phonetic matching accu-\nracy. We then integrate these entities into a unified knowl-\nedge graph for retrieval. Fig.3 depicts a semantic knowledge\ngraph for the QA example in Fig.1. Each node represents\ndifferential diagnostic considerations (HCMV, Toxoplasma,\nVZV, and Rubella virus infections) with corresponding anti-\nbody tests. The graph incorporates phonetically similar terms\n"}, {"page": 3, "text": "often misrecognized in speech systems, such as ”Hypopla-\nsia”/”Hyperplasia” and ”atrophy”/”hypertrophy.” In the final\nKG, nodes contain unique identifiers and term strings, while\nedges have relationship tags including ”classifies,” ”consti-\ntutes,” ”due to,” and ”plays role” to help the LLM understand\nrelationships between question terms. A specific ”phonetic”\nedge tag aids the LLM in identifying potential speech tran-\nscription errors.\n3.2. KG-Aided ASR Error Correction Framework\nIn our MedSpeak, the workflow begins with an existing\nspeech recognition model to generate transcripts, followed by\nidentification of medical terminology for knowledge graph\nretrieval. The retrieved medical information is then embed-\nded alongside contextual prompts to enhance LLM reasoning\nand correction capabilities.\n3.2.1. Training Process\nOur training methodology primarily fine-tunes the parame-\nters of a base LLM model under the full model supervision.\nEach training sample is formatted as a dialogue with follow-\ning three parts: i) System Messages consists of a fixed rules\nonly instruction that enforced a strict two-line output from\nthe LLM. ii) User Input included three items:(a) the noisy\nASR transcript, (b) the multiple-choice answer options, and\n(c) the semantic and phonetic Knowledge-Graph (KG). The\nKG section is truncated to a fixed budget(900 tokens) in order\nto prevent gold labels from being dropped. Finally, iii) As-\nsistant Target is defined as a structured two-line completion\nformat:\nCorrected Text: [Original GT Text]\nCorrect Option: [Option Letter<A|B|C|D>]\nTraining Objective: Given an input–output pair (x, y), the\nmodel is optimized with a single causal language model-\ning objective.\nFormally, the input sequence is defined as\nx =\n\u0002\ns ; u(ASR, Options, KG)\n\u0003\n, where s is the fixed sys-\ntem instruction, and u(·) represents the user input consisting\nof the ASR transcript, multiple-choice options, and trun-\ncated KG context. The target sequence is expressed as y =\nCorrected Text: t∗⊕Correct Option: o∗, where t∗is the gold-\nstandard transcript (ground truth) and o∗∈{A, B, C, D} is\nthe correct answer option.\nThe model parameters θ are\ntrained to minimize\nL(θ) = −\n|y|\nX\ni=1\nlog Pθ(yi | x, y<i),\n(1)\nwhere yi is the i-th token in the target sequence, y<i are all\nprevious tokens, and Pθ(yi | x, y<i) is the probability of\npredicting the correct token given the input x and prior con-\ntext. This formulation unifies ASR error correction (via t∗)\nand multiple-choice reasoning (via o∗) into a single genera-\ntion task. The KG ensures domain knowledge remains avail-\nable, enabling the model to produce clinically accurate and\ncontext-aware outputs.\n3.2.2. Inference Process\nThe acoustic input a is transcribed into a noisy text by the\npre-trained ASR model which is given as ˆt = ASR(a). The\nuser input is then constructed in the same format as during\ntraining: u = u(ˆt, O, K), where ˆt is the ASR transcript,\nO = {A, B, C, D} denotes the multiple-choice options, and\nK is the KG context (Ksem, Kphon). The overall model input\nis given by x = [s; u], where s is the fixed system instruction.\nThe fine-tuned LLM generates the structured two-line output:\ny = LLMθ(x) ⇒\n\u001a\nCorrected Text:\n˜t\nCorrect Option:\no∗\n(2)\nwith ˜t denoting the corrected transcript and o∗∈{A, B, C, D}\nthe predicted option.\n4. EXPERIMENT AND RESULTS\nIn this section, we first describe the data preparation and ex-\nperimental settings. We then evaluate our MedSpeak frame-\nwork on spoken medical QA against various baselines.\n4.1. Data Preparation\nTo systematically evaluate MedSpeak, we use a diverse med-\nical SQA benchmark by synthesizing spoken data from three\nwell-established multiple-choice QA datasets [1, 6, 17]. The\nMMLU Medical Tasks dataset includes 1,089 questions cov-\nering six medical subjects, including biology, anatomy, and\nclinical medicine. The MedQA (USMLE-Based) dataset con-\nsists of 1,273 questions derived from the United States Med-\nical Licensing Examination (USMLE), incorporating real-\nworld diagnostic reasoning tasks. The MedMCQA dataset\ncomprises 4,183 questions spanning multi-specialty medical\nassessments used in Indian medical entrance exams. Each\ndataset is synthesized at 16,000 Hz, mono WAV format using\nlocal pyttsx3 text-to-speech(TTS) engine for speech genera-\ntion. This benchmark provides over 47 hours of spoken medi-\ncal QA data, ensuring rigorous evaluation of ASR correction,\nretrieval-driven reasoning, and knowledge integration.\n4.2. Experimental Settings\nConfigurations: Our MedSpeak framework is implemented\nby fine-tuning all parameters of Llama-3.1-8B-Instruct. We\nconduct training across approximately ∼10k of vocalized\nmedical QA pairs drawn from MedMCQA, MedQA, and\nMMLU-Medical, totaling about 6.79M tokens. The model is\noptimized for 10 epochs with a batch size of 8 and gradient\n"}, {"page": 4, "text": "the building up of 16, using a learning rate multiplier of 2 rel-\native to the cosine schedule baseline. The length of sequence\nis constrained to at most 2048 tokens, To prevent truncation\nof gold labels, we budget semantic and phonetic knowledge-\ngraph partitions of 600 and 300 tokens respectively.\nThe\nobjective of training is the standard causal language model-\ning loss across the whole rendered dialogue, so that the model\njointly learns ASR correction and answer prediction. This ex-\nperiment is conducted on a cluster of 8x NVIDIA A100 GPUs\nwith 80 GB memory each, paired with AMD EPYC 7742 64-\nCore Processor. We enable bfloat16 precision, gradient\ncheckpointing, and TF32 matrix multiplication for efficiency\nenhancement. The training runs are distributed across mul-\ntiple GPUs with the help of the HuggingFace Trainer\nframework. The Whisper Small Model (244M parameters)\nis used as the front-end ASR system to generate the noisy\ntranscript for all our experiments.\nBaselines: We compare our MedSpeak framework against a\nset of baseline systems to evaluate the contribution of fine-\ntuning, ASR correction and KG integration: (1) Zero-shot\nASR (ZS-ASR) is the model using the raw whisper gener-\nated transcript without fine-tuning or KG-context. (2) Zero-\nshot GT (Zero-Shot) is the model using the pure base LLM\nwhen provided with the ground truth transcripts, and with-\nout fine-tuning or KG-context. (3) Fine-Tuned LLM + Whis-\nper (FT+Whisp) is the model where the fine-tuned LLM is\ngiven the whisper generated transcripts without KG-context.\n(4) Fine-Tuned LLM + GT (FT-LLM) is the fully fine-tuned\nLLM evaluated with the ground truth texts. (5) MedSpeak: is\nour proposed model that incorporates noisy transcripts gener-\nated by whisper with the budgeted KG context and the fined-\ntuned LLM.\nEvaluation Metrics: We evaluate MedSpeak performance on\ntwo metrics: (1) QA Accuracy (%): the fraction of correctly\npredicted multiple choice answers, and (2) Word Error Rate\n(WER) (%): Punctuation-insensitive Levenshtein distance be-\ntween reference and corrected transcripts, which quantifies\nthe ASR correction quality, calculated as: WER = S+D+I\nN\n×\n100%, where S are substitutions, D deletions, I insertions,\nand N is the total number of words in the reference transcript.\n4.3. Results and Analysis\nWe compare the QA accuracy and WER of our proposed\nframework MedSpeak against different baselines for different\ndatasets. Accuracy results are listed in Table 1. Indicatively,\nthe best performance is recorded by MedSpeak overall on the\nbenchmarks, recording an overall accuracy of 93.4%, very\nclose to the performance of the FT-LLM baseline paired with\nthe ground-truth texts (92.5%) and considerably higher than\nzero-shot setups in the 50-56% range. The biggest boosts\nare recorded in Virology and in MedQA, where MedSpeak\nreduces the gap by over four percentage points compared to\nthe FT-LLM baseline, demonstrating our knowledge graph\ncontext integration to be highly beneficial in coping with\nchallenging terminology and noisy ASR transcriptions. The\nexception is in the case of MedMCQA where the LLM fine-\ntuned on ground-truth transcriptions performs by a very slim\nmargin (92.5% vs. 91.5%).\nEven then though, MedSpeak\noutperforms baselines in all other tasks.\nMoreover, WER\nTable 1. QA Accuracy of MedSpeak and Baselines.\nTask\nZS-ASR Zero-Shot FT+Whisp FT-LLM MedSpeak\nMMLU\nClinical\n62.2\n66.3\n85.6\n94.3\n95.4\nAnatomy\n57.0\n64.4\n85.2\n94.1\n93.3\nCollege\n62.1\n67.5\n84.2\n92.7\n95.6\nVirology\n47.6\n48.8\n85.5\n91.0\n95.8\nProf. Med.\n74.6\n76.1\n88.6\n94.9\n97.8\nMedQA\n54.9\n58.9\n86.6\n91.8\n97.5\nMedMCQA\n45.5\n52.8\n82.3\n92.5\n91.5\nAvg.\n50.2\n56.3\n83.7\n92.5\n93.4\nresults are presented in Table 2. As WER is defined only over\nASR-processed inputs, ground-truth baselines are omitted; if\nused as input, their error rate would necessarily be zero. Med-\nSpeak shows outright benefits. The overall WER decreases\nto 29.9%, versus 77.2% on ZS-ASR generated transcripts\nwith non finetuned LLM and 35.8% on FT+Whisp. Gains\nare particularly significant in Anatomy and Virology, where\nMedSpeak decreases error by 5–7 percentage points over the\nnext-best baseline. These decreases are significant to how\nwell combining semantic and phonetic knowledge graph in-\nformation with fine-tuning helps the model to discriminate\nphonetically similar but semantically distinct medical entities\nand to correct domain-specific transcription mistakes with\ngreater reliability. These results in tandem confirm that Med-\nSpeak is equally very accurate on spoken medical QA and\nimproves transcription quality.\nTable 2. WER of MedSpeak and Baselines.\nTask\nZS-ASR\nFT+Whisp\nMedSpeak\nMMLU\nClinical\n76.7\n27.3\n22.1\nAnatomy\n87.0\n25.8\n19.8\nCollege\n67.5\n21.0\n15.7\nVirology\n78.9\n24.6\n17.5\nProf. Med.\n61.9\n39.3\n37.8\nMedQA\n63.9\n44.9\n43.5\nMedMCQA\n81.4\n35.4\n28.1\nAvg.\n77.2\n35.8\n29.9\n5. CONCLUSION\nIn this paper, we proposed MedSpeak, a framework integrates\nsemantic and phonetic context from a medical knowledge\ngraph with the reasoning ability of LLMs, enabling robust\ncorrection of transcription errors and reliable clinical reason-\ning. Through extensive evaluation, MedSpeak demonstrated\nconsistent improvements in both transcription accuracy and\ndownstream QA reliability, offering a scalable and effective\nsolution for enhancing ASR systems in high-stakes medical\napplications.\n"}, {"page": 5, "text": "6. REFERENCES\n[1] Yanis Labrak, Adel Moumen, Richard Dufour, and\nMickael Rouvier, “Zero-shot end-to-end spoken ques-\ntion answering in medical domain,”\narXiv preprint,\n2025.\n[2] Amin\nRobatian,\nMohammad\nHajipour,\nMoham-\nmad Reza Peyghan, Fatemeh Rajabi, Sajjad Amini,\nShahrokh Ghaemmaghami, and Iman Gholampour,\n“Gec-rag:\nImproving generative error correction via\nretrieval-augmented generation for automatic speech\nrecognition systems,” arXiv preprint, 2025.\n[3] Ernest Pusateri, Anmol Walia, Anirudh Kashi, Bor-\ntik Bandyopadhyay, Nadia Hyder, Sayantan Mahinder,\nRaviteja Anantha, Daben Liu, and Sashank Gondala,\n“Retrieval-augmented correction of named entity speech\nrecognition errors,” IEEE, 2025.\n[4] Mingqiu Wang, Izhak Shafran, Hagen Soltau, Wei Han,\nYuan Cao, Dian Yu, and Laurent El Shafey, “Retrieval-\naugmented end-to-end spoken dialog models,” ICASSP,\n2025.\n[5] Do June Min, Karel Mundnich, Andy Lapastora, Erfan\nSoltanmohammadi, Srikanth Ronanki, and Kyu Han,\n“Speech retrieval-augmented generation without auto-\nmatic speech recognition,” ICASSP, 2025.\n[6] Xuejiao Zhao, Siyan Liu, Su-Yin Yang, and Chun-\nyan Miao,\n“Medrag: Enhancing retrieval-augmented\ngeneration with knowledge graph-elicited reasoning for\nhealthcare copilot,” arXiv preprint, 2025.\n[7] Hao Yang, Min Zhang, Minghan Wang, and Jiaxin\nGuo, “Rasu: Retrieval-augmented speech understand-\ning through generative modeling,” IEEE, 2025.\n[8] Chunyu Sun, Bingyu Liu, Zhichao Cui, Anbin Qi, Tian-\nHao Zhang, Dinghao Zhou, and Lewei Lu,\n“Seal:\nSpeech embedding alignment learning for speech large\nlanguage model with retrieval-augmented generation,”\narXiv preprint, 2025.\n[9] Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.\nPlumbley, and Wenwu Wang,\n“Retrieval-augmented\ntext-to-audio generation,” ICASSP, 2025.\n[10] Zhihong Lei, Xingyu Na, Mingbin Xu, Ernest Pusateri,\nChristophe Van Gysel, Yuanyuan Zhang, Shiyi Han, and\nZhen Huang, “Contextualization of asr with llm using\nphonetic retrieval-based augmentation,” arXiv preprint\narXiv:2409.15353, 2024.\n[11] Hao Yang, Min Zhang, Minghan Wang, and Jiaxin\nGuo, “Rasu: Retrieval augmented speech understand-\ning through generative modeling,” in Interspeech, 2024,\nvol. 2024, pp. 3510–3514.\n[12] Jinlong Xue,\nYayue Deng,\nYingming Gao,\nand\nYa Li,\n“Retrieval augmented generation in prompt-\nbased text-to-speech synthesis with context-aware con-\ntrastive language-audio pretraining,”\narXiv preprint\narXiv:2406.03714, 2024.\n[13] Cihan Xiao, Zejiang Hou, Daniel Garcia-Romero, and\nKyu J Han, “Contextual asr with retrieval augmented\nlarge language model,” in ICASSP 2025-2025 IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2025, pp. 1–5.\n[14] Hao Yang, Min Zhang, and Daimeng Wei, “Irag: Itera-\ntive retrieval-augmented generation for spoken language\nunderstanding,” IEEE, 2025.\n[15] Xuejiao Zhao, Siyan Liu, Su-Yin Yang, and Chun-\nyan Miao,\n“Medrag: Enhancing retrieval-augmented\ngeneration with knowledge graph-elicited reasoning for\nhealthcare copilot,” arXiv preprint, 2025.\n[16] National Library of Medicine,\n“Unified medical lan-\nguage system (umls),” National Institutes of Health,\n2024.\n[17] Rui Liu, Zhenqi Jia, Feilong Bao, and Haizhou Li,\n“Retrieval-augmented dialogue knowledge aggregation\nfor expressive conversational speech synthesis,” Infor-\nmation Fusion, 2025.\n"}]}