{"doc_id": "arxiv:2601.20890", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.20890.pdf", "meta": {"doc_id": "arxiv:2601.20890", "source": "arxiv", "arxiv_id": "2601.20890", "title": "SW-ASR: A Context-Aware Hybrid ASR Pipeline for Robust Single Word Speech Recognition", "authors": ["Manali Sharma", "Riya Naik", "Buvaneshwari G"], "published": "2026-01-28T04:50:04Z", "updated": "2026-01-28T04:50:04Z", "summary": "Single-word Automatic Speech Recognition (ASR) is a challenging task due to the lack of linguistic context and sensitivity to noise, pronunciation variation, and channel artifacts, especially in low-resource, communication-critical domains such as healthcare and emergency response. This paper reviews recent deep learning approaches and proposes a modular framework for robust single-word detection. The system combines denoising and normalization with a hybrid ASR front end (Whisper + Vosk) and a verification layer designed to handle out-of-vocabulary words and degraded audio. The verification layer supports multiple matching strategies, including embedding similarity, edit distance, and LLM-based matching with optional contextual guidance. We evaluate the framework on the Google Speech Commands dataset and a curated real-world dataset collected from telephony and messaging platforms under bandwidth-limited conditions. Results show that while the hybrid ASR front end performs well on clean audio, the verification layer significantly improves accuracy on noisy and compressed channels. Context-guided and LLM-based matching yield the largest gains, demonstrating that lightweight verification and context mechanisms can substantially improve single-word ASR robustness without sacrificing latency required for real-time telephony applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.20890v1", "url_pdf": "https://arxiv.org/pdf/2601.20890.pdf", "meta_path": "data/raw/arxiv/meta/2601.20890.json", "sha256": "a990ea5746bc5cb27d8c4db7234bf0e1cfe45794ff391f787534c9b7ee5e1173", "status": "ok", "fetched_at": "2026-02-18T02:20:17.768956+00:00"}, "pages": [{"page": 1, "text": "SW-ASR: A Context-Aware Hybrid ASR Pipeline for Robust Single Word\nSpeech Recognition\nManali Sharma*, Riya Naik* and Buvaneshwari G\nmanali,riya.n,buvaneshwari@tetranetics.com\nTetranetics Private Limited\nMumbai, India\nAbstract\nAutomatic Speech Recognition (ASR) for\nsingle-word detection is a specialized task\nthat focuses on accurately identifying isolated\nspoken words.\nIt is especially critical for\nconstrained and open-vocabulary applications\nin low-resource, communication-sensitive do-\nmains such as healthcare and emergency re-\nsponse. Unlike continuous speech ASR, which\nbenefits from surrounding linguistic context,\nsingle-word ASR faces challenges due to its\nminimal context. This is in addition to the\nissues that are common among continuous\nspeech and single-word ASR such as pronunci-\nation variability, background noise, and speaker\ndiversity. This paper reviews current deep learn-\ning approaches and presents a modular frame-\nwork designed to enhance single-word detec-\ntion accuracy. The pipeline first applies de-\nnoising and volume normalization, then uses\na hybrid ASR front end (Whisper + Vosk)\nwith confidence-weighted selection to produce\nan initial transcription.\nTo address out-of-\nvocabulary words and low-quality channels,\nwe add a verification layer that can operate in\nfour modes: cosine-embedding similarity, Lev-\nenshtein distance, LLM-based matching, and\ncontext-guided matching (cosine/LLM with\nsurrounding context). This architecture then\nintegrates with SIP-based telephony stacks, en-\nabling intent-driven functionalities that can be\nused for cases such as blind call transfers and\nemergency alerts.\nFor evaluation we com-\nbine a public benchmark with platform-specific\nrecordings. We use Google Speech Commands\n(65k one-second clips across 30 words), and\nwe curate a supplementary dataset that records\nthe same 30 words across real-world channels:\ncellular voice calls and voice-messaging on\nWhatsApp, WebChat, and Facebook Messen-\nger. To mirror deployment conditions, we tar-\nget bandwidth-limited and compressed audio\ntypical of telecommunication networks (e.g., 8\nkHz sampling, codec artifacts, noise). Across\n*These authors contributed equally to this work.\nGSC and the curated platform recordings, the\nhybrid Whisper + Vosk front end performs\nbest on high-quality audio, while the verifica-\ntion layer yields clear gains on noisier chan-\nnels. LLM-based matching with contextual\nprompts consistently reduces word error rate\non telephony and WeChat audio, with few-shot\nprompting providing the strongest improve-\nments among low-quality signals.\nContext-\nguided cosine narrows the gap with LLMs\nin many conditions and can outperform non-\ncontextual methods, offering a favorable accu-\nracy–latency trade-off. Timing analyses show\nthat cosine with context has latency comparable\nto hybrid/Levenshtein pipelines, while a naïve\nLLM prompt is costlier; however, adding con-\ntext + instructions (and few-shot) focuses the\nLLM enough that its average time approaches\ncosine in practice. These results suggest that\nmodest verification and context mechanisms\ncan deliver robustness in single-word detection\nwithout sacrificing responsiveness required for\nlive telephony actions.\n1\nIntroduction and Related Work\nAutomatic Speech Recognition systems have be-\ncome integral to modern communication plat-\nforms, powering voice assistants, customer ser-\nvice automation, emergency response triggers, and\ncommand-and-control interfaces (Atayero et al.,\n2009). Although most ASR research efforts fo-\ncus on continuous speech recognition, an equally\nimportant but underexplored area is single-word\nrecognition, where the system must identify and\ninterpret speech input consisting of just one word.\nSuch scenarios are common in voice command\ninterfaces, helpline bots, etc. The recognition of\nsingle words has been studied in the context of key-\nword identification and isolated word recognition\n(Michaely et al., 2017; Paul et al., 2025). Tradi-\ntional approaches rely on mixture models(Dines\net al., 2010; Nainan and Kulkarni, 2016) con-\nstrained by domain-specific vocabularies and lack-\narXiv:2601.20890v1  [cs.SD]  28 Jan 2026\n"}, {"page": 2, "text": "ing the adaptability required for open vocabulary.\nThese systems depend on cloud-based inference\nwith massive compute availability, limiting their\napplicability in resource-constrained or latency-\nsensitive environments.\nWith the advent of deep learning, neural net-\nwork techniques have improved recognition perfor-\nmance. CNNs(Pan et al., 2020) have been applied\nto spectrogram-based word classification, while\nLSTM and GRUs have been used to model tem-\nporal dependencies and enhance robustness (Pass-\nricha and Aggarwal, 2019; Tonk et al., 2024).\nTransformer architectures using Wav2Vec and\nWhisper have shown state-of-the-art performance\nby learning contextual representations (Radford\net al., 2023; Baevski et al., 2020). While these\nhave addressed many challenges, such as pronun-\nciation variation, background noise, and speaker\ndiversity, single-word signals still face a unique lim-\nitation: the absence of surrounding context. Our\nsingle word (SW-ASR) operates in isolation, with-\nout grammatical or semantic cues to support infer-\nence, making accurate detection difficult and even\nworse when dealing with low-quality audio signals.\nIn this paper, we propose a modular frame-\nwork for SW-ASR for both constrained- and open-\nvocabulary scenarios. Our architecture integrates\ndeep learning-based acoustic models with SIP-\ncompatible telephony systems, enabling accurate\nrecognition and immediate response actions, such\nas blind call transfers, emergency alerts, and role-\nbased routing. We evaluate our system on real-\nworld platforms, including cellular networks and\nvoice messaging applications such as WhatsApp1,\nWeChat2, and Facebook Messenger3, and demon-\nstrate substantial improvements in recognition ac-\ncuracy, even under noisy and resource-limited con-\nditions.\nCase Study:\nReal-World Applications of\nSingle-Word ASR The SW-ASR framework pro-\nposed in this paper is developed to address the\nchallenges observed in various public-facing de-\nployments, such as\n(1) A citizen-facing chatbot was deployed on What-\nsApp and social media platforms to report power\noutages in a region with more than 130 million\npeople and a literacy rate of only 63%. Many users,\noften unable to type in their native script, chose\nto record voice messages with short phrases like\n1https://www.whatsapp.com/\n2https://www.wechat.com/\n3https://www.facebook.com/messenger/\n’bijli gayi’ (’electricity gone’). Initial deployments\nshowed that the system underperformed in detect-\ning such informal speech without adaptation to mul-\ntilingual, noisy, and informal inputs, requirements\nthat directly shaped the improvements discussed in\nthis document.\n(2) An emergency helpline received over 130,000\ncalls, with fewer than 0.03% being genuine. Most\nwere pocket dials, pranks, or non-emergencies. To\nmanage this, an ASR-based filtration system was\ndeployed to detect urgent keywords like \"help,\"\n\"fire,\" or \"ambulance.\" It had to operate reliably\nin noisy, high-stress conditions to ensure genuine\nemergencies weren’t missed.\n(3) A secure telephony environment in which per-\nsonnel could initiate call transfers simply by speak-\ning the name of a colleague. The ASR system\nneeds to accurately recognize a finite but dialect-\nsensitive set of proper nouns, enabling spoken in-\nputs to replace memorized extensions in complex,\ndistributed environments. Although we quantita-\ntively evaluated the model using publicly avail-\nable datasets, the system architecture and training\nchoices were directly informed by the demands and\nfailure points from these real-world applications.\n2\nExperimental Setup\n2.1\nData\nIn our experimental investigation, we focus on the\npublically available Google Speech Commands4\n(GSC) dataset (Sainath and Parada, 2015). The\nGSC dataset consists of approximately 65,000 one-\nsecond audio recordings that cover 30 distinct\nwords. To evaluate generalization across platforms,\nwe recorded the same 30 words on multiple chan-\nnels, each paired with its textual label for training\nand evaluation.\n2.1.1\nData Curation\nGSC consists of exceptionally high-quality audio\nrecordings characterized by articulate speech and\nminimal periods of silence or distortion, a level of\nquality that is uncommon in real-world datasets.\nIn contrast, speech data in real-world applications\noften originates from bandwidth-limited environ-\nments, such as telecommunication networks, where\naudio is sampled at 8 kHz and subject to com-\npression and noise.\nTo reflect this variability,\nwe curated a supplementary dataset by recording\n4https://research.google/blog/\nlaunching-the-speech-commands-dataset/\n"}, {"page": 3, "text": "Figure 1: SW-ASR: (1) The single-word speech audio\nprocessed to improve quality. (2)Hybrid: Whisper &\nVosk used to generate initial transcript. (3) Candidate\nmatching performed using three approaches. (4) Con-\ntextual Concatenation for Generalization.\nthe same 30 words via WhatsApp, cellular calls,\nWeChat, and Facebook voice messaging. Our anno-\ntation team of native speakers meticulously crafted\nspeech samples for every word on these diverse\nplatforms to encapsulate nuanced differences in au-\ndio quality. We gathered ten audio recordings per\nword from each participant, accumulating a total\nof 300 audio recordings per platform, which collec-\ntively amounted to 1,200 speech samples spanning\nvaried real-world conditions.\n2.2\nSystem Model and Design\nThe system framework is designed with key com-\nponents depicted in Figure 1. The raw audio input\nundergoes preprocessing consisting noise reduc-\ntion and volume normalization. A hybrid combina-\ntion of OpenAI’s Whisper5 and Vosk6 is used for\nASR. The order of processing through this hybrid\nmodel begins with Whisper for accuracy followed\nby Vosk for improved single-word detection via\nits phoneme-trained backbone. To further improve\nrecognition context is injected into each matching\nalternative. The working and formulation of each\nkey component is detailed below:\n2.3\nInitial Audio Transcription\nThe audio input X is first pre-processed and tran-\nscribed using a hybrid approach that integrates the\noutputs of Whisper and Vosk, weighted by their\nrespective confidence scores. TW , CW = fW (X)\nand TV , CV = fV (X) where TW is the Whisper\ntranscription output, and fW represents the Whis-\nper model and TV is Vosk’s transcription and fV\nrepresents the Vosk model. TM = TW if CW ≥\nCV and CW ≥τ otherwise TV . Here τ is a prede-\nfined confidence threshold. Our selection strategy\nensures that the transcription with the highest con-\nfidence across the ensemble is chosen.\n5https://huggingface.co/openai/whisper-base\n6https://alphacephei.com/vosk/models\n2.4\nSimilarity Matching\nRelying solely on the TM as the final output can be\nproblematic in scenarios where the word in ques-\ntion is unique, falls outside the training vocabulary,\nor is derived from low-quality audio signals. To ad-\ndress this and improve the overall robustness of the\nsystem, we incorporate a post-processing step for\nknown vocabulary cases. This involves comparing\nthe initial transcription against a predefined set of\ntarget words using a suitable similarity metric. This\napproach helps refine the transcription output by\naligning it accurately with the expected vocabulary.\n2.4.1\nCosine Similarity\nIn the first approach, we convert both the initial\ntranscription and target words into vectors using\na pre-trained embedding model and match them\nusing cosine similarity(Laskar et al., 2020). These\nembeddings capture semantic and syntactic rela-\ntionships between words. We compute cosine simi-\nlarity between the vector of the initial transcription\nand each target word vector, and map the initial\ntranscription to the target word with the highest\nsimilarity.\n2.4.2\nLevenshtein Distance\nCosine similarity relies on semantic embeddings of\na pre-trained model and may not work reliably on\nout-of-vocabulary or domain-specific words. For\nthis reason, we test the similarity matching using\nLevenshtein distance (Zhang et al., 2017). It oper-\nates on a character level, directly measuring how\nmany edits (insertions, deletions, substitutions) are\nneeded to turn one word into another. This feature\nmakes it efficient for single-word matching without\nusing any embedding model, handling orthographi-\ncally different words.\n2.4.3\nLLM Matching\nCosine and Levenshtein are effective to some ex-\ntent, but they do not account for phonetic similar-\nity. They can fail when two words sound alike but\ndiffer in terms of spelling and semantics, such as\n\"phone\" and \"fone\". To improve on this for the\nknown vocabulary cases, we prompt LLM with\ninitial transcription and a list of target words to\nselect the most plausible match based on semantic\nunderstanding and phonetics. This enables intel-\nligent corrections, especially in cases where the\ntranscription is ambiguous or incomplete.\n"}, {"page": 4, "text": "2.4.4\nContext Guided Matching\nThis approach combines cosine similarity and\nLLMs with contextual information to improve accu-\nracy further and reduce the word error rate. Rather\nthan processing words in isolation, embeddings are\ncalculated with surrounding context for better se-\nmantic relevance. Although cosine with context\nshows substantial improvement, it can sometimes\nfail in cases where there can be two alternatives.\nFor example, if the context is “I have two pets, a\ndog and a cat” and the target word is “cat”, cosine\nsimilarity may incorrectly match it to “dog”. In\nsuch cases, LLMs perform better by using contex-\ntual prompts to understand broader discourse, en-\nabling more accurate selection based on meaning,\ngrammar, and coherence. This hybrid approach\nwith similarity matching ensures a balanced trade-\noff between accuracy and computational efficiency.\n3\nResults and Discussion\nWe now discuss the evaluation and comparison of\npipelines drawn in section 2 for the GSC dataset\nalongside platform-specific augmentations. We la-\nbel each pipeline as: Hybrid for initial hybrid tran-\nscription using Whisper & Vosk, CS for cosine\nsimilarity, LS for Levenshtein similarity, LLM\nfor LLM-based similarity, CS + C for context-\nembedded cosine similarity, LLM + C for context-\nembedded LLM similarity, and LLM + C + FS for\ncontext-embedded LLM similarity with few-shot\nprompting.\nWe assess the single word recognition capabili-\nties of each pipeline across datasets using accuracy,\nword error rate (WER), and time taken. As tabu-\nlated in Tables 1 and 2, the initial hybrid method\nperforms better on the GSC dataset due to its high\nquality as compared to other platforms with low-\nquality signals. We observe the advantages of using\nsimilarity matching in such scenarios, specifically\non Facebook data. Using standard similarity mea-\nsures such as 2.4.1 and 2.4.2 is computationally\nefficient, but these methods have limitations: Al-\nthough the Levenshtein distance is similar to co-\nsine matching, it has the potential to deviate as the\ncontext increases. Cosine distance takes semantic\nsimilarity, but it does not inherently take phonetics\ninto account. Thus, if the initial transcription is\nsemantically distinct but phonetically overlapping,\nit can be mismatched. LLMs are trained on a huge\namount of data, which can be leveraged for similar-\nity matching. We observed Llama-4-Scout7 here to\nmeasure the matching performance. We first test it\nwith a naive prompt that instructs the LLM to map\nthe transcription to the word in the target set. LLM\nmatching shows a significant improvement over\nstandard measures; even in the telephony setting,\nthe WER is substantially reduced. Unlike standard\nmeasures, LLMs take into account various features\nof a word (syntactic, semantic, and phonetic) to\nperform the mapping.\nDataset\nHybrid\nCS\nLS\nLLM\nCS\nLLM\nLLM\n+ C\n+ C\n+ C + FS\nGSC\n0.49\n0.59\n0.61\n0.66\n0.65\n0.77\n0.86\nGSC-Wh\n0.28\n0.38\n0.31\n0.42\n0.47\n0.68\n0.69\nGSC-T\n0.11\n0.26\n0.23\n0.35\n0.36\n0.51\n0.51\nGSC-We\n0.20\n0.34\n0.43\n0.53\n0.49\n0.67\n0.73\nGSC-FB\n0.42\n0.44\n0.58\n0.61\n0.47\n0.70\n0.71\nTable 1: Accuracy Comparison across datasets\nDataset\nHybrid\nCS\nLS\nLLM\nCS\nLLM\nLLM\n+ C\n+ C\n+ C + FS\nGSC\n0.59\n0.41\n0.39\n0.35\n0.35\n0.24\n0.15\nGSC-Wh\n0.96\n0.65\n0.70\n0.58\n0.53\n0.33\n0.32\nGSC-T\n1.00\n0.77\n0.80\n0.65\n0.64\n0.49\n0.48\nGSC-We\n1.00\n0.67\n0.57\n0.48\n0.52\n0.33\n0.27\nGSC-FB\n0.81\n0.57\n0.44\n0.41\n0.53\n0.30\n0.29\nTable 2: WER Comparison across datasets\nContext. The ASR models are trained on long-\ncontext sentences and thus fail to recognize isolated\nsingle-word queries. But these SW queries are of-\nten instructions/commands to perform an action\nor an answer or choice to a question. This ob-\nservation makes it suitable for SW transcriptions\nto be concatenated with relevant context to boost\nthe performance. We observe that having addi-\ntional context for matching enhances the perfor-\nmance. CS + C gives comparable results to using\nLLM with context. And marginally better on What-\nsApp and Telephony signals. We also see that LLM\nwith a contextual prompt with additional instruc-\ntions understands the context and maps the target\nword more efficiently with at least 10% in WER\nand, achieves the best results for low-quality tele-\nphony and WeChat signals. This approach can be\nfurther improved using state-of-the-art prompting\nstrategies. For our experiment, we adopt few-shot\nprompting. It is evident from the results that few-\nshot helps LLMs to understand the data and handle\nmarginal cases, specifically when multiple single-\nword answers/commands are possible.\nIt’s always a tradeoff to prioritize between perfor-\nmance and latency when we incorporate transform-\ners and LLMs. Figure 2 shows the average time\n7https://ai.meta.com/blog/\nllama-4-multimodal-intelligence/\n"}, {"page": 5, "text": "taken by each pipeline alternative, and we can wit-\nness that cosine similarity with context amounts to\nsimilar time as compared to hybrid and LS. LLM\nwith naive prompt takes the highest, this is due to\nthe nature of LLM to consider a broad range of fea-\ntures and generalization. When clubbed with better\nprompt and context, LLM interestingly consumes a\nsimilar amount as CS. Since, based on instructions,\ncontext, and a few shots, it can narrow its focus and\nperform mapping easily.\nFigure 2: Average time taken across datasets for each\napproach\n4\nConclusion\nIn this paper, we present a modular framework\nfor Single-Word Automatic Speech Recognition\n(SW-ASR) applicable to both constrained and open-\nvocabulary settings. We explore multiple strate-\ngies to enhance the accuracy and robustness of\nSW-ASR systems. To ensure generalizability and\npractical relevance, we evaluate our methods on\naudio data across diverse platforms. Our hybrid\napproach, incorporating similarity-based match-\ning, demonstrates consistent performance improve-\nments across these environments. The findings\nhighlight the feasibility of deploying SW-ASR sys-\ntems in real-world scenarios, particularly where\ncomputational or data limitations preclude exten-\nsive fine-tuning or retraining of acoustic models.\nLimitations\nOur single-word (SW-ASR) framework was tested\non WhatsApp, WeChat, Facebook Messenger, cel-\nlular calls, and recorded audio, covering both\nsynchronous and asynchronous voice interactions.\nHowever, several constraints limit its generalizabil-\nity across platforms and contexts. First, platform-\nspecific audio characteristics (e.g., compression\nrates, codecs, noise artifacts) introduce variability.\nWhatsApp voice notes may suffer from compres-\nsion distortions, WeChat uses different sampling\nrates, and phone calls experience telecom-induced\nbandwidth constraints and channel noise. Each plat-\nform required targeted model adaptation. Second,\ndialect and language diversity significantly affect\nperformance. While the system supports multilin-\ngual and dialect-sensitive inputs, generalization to\nlow-resource or code-switched languages may re-\nquire domain-specific tuning and additional data.\nThird, speaker conditions affect generalizability.\nThe framework performs best on single-speaker ut-\nterances, with reduced accuracy on multi-speaker\nor conversational inputs. Finally, despite strong\nevaluation on public datasets, edge cases and atyp-\nical conditions (e.g., extreme background noise,\noverlapped speech, novel dialects) remain under-\nrepresented. This work can be enhanced for lan-\nguage coverage, introduce lightweight on-device\nadaptation, and fine-tune for cross-platform deploy-\nment.\nEthics Statement\nOur proposed research adheres to the ACL Code\nof Ethics and involves no risk to individuals or\ncommunities. All supplementary speech data was\ngenerated in-house. The primary data collector and\nannotator is one of the authors. Recordings were\nlimited to predefined, non-identifying single-word\nutterances. Annotators were fairly compensated.\nNo personal or sensitive information was collected.\nGiven the intended use in public service domains\nlike emergency response, we acknowledge dual-use\nconcerns and have designed the system with human\nfallback mechanisms to prevent over-reliance on\nautomation. No trained models are released pub-\nlicly. By curating multilingual, platform-specific\ndata, this work aims to advance fairness and gener-\nalizability in speech recognition for low-resource\nand real-world deployment settings.\nReferences\nAderemi A Atayero, Charles K Ayo, Ikhu-Omoregbe\nNicholas, and Azeta Ambrose. 2009. Implementa-\ntion of ‘asr4crm’: An automated speech-enabled cus-\ntomer care service system. In IEEE EUROCON 2009,\npages 1712–1715. IEEE.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nAdvances in neural information processing systems,\n33:12449–12460.\nJohn Dines, Junichi Yamagishi, and Simon King. 2010.\nMeasuring the gap between hmm-based asr and tts.\n"}, {"page": 6, "text": "IEEE Journal of Selected Topics in Signal Processing,\n4(6):1046–1058.\nMd Tahmid Rahman Laskar, Xiangji Huang, and Ena-\nmul Hoque. 2020. Contextualized embeddings based\ntransformer encoder for sentence similarity model-\ning in answer selection task. In Proceedings of the\ntwelfth language resources and evaluation confer-\nence, pages 5505–5514.\nAssaf Hurwitz Michaely, Xuedong Zhang, Gabor\nSimko, Carolina Parada, and Petar Aleksic. 2017.\nKeyword spotting for google assistant using contex-\ntual speech recognition. In 2017 IEEE Automatic\nSpeech Recognition and Understanding Workshop\n(ASRU), pages 272–278. IEEE.\nSumita Nainan and Vaishali Kulkarni. 2016. A compar-\nison of performance evaluation of asr for noisy and\nenhanced signal using gmm. In 2016 International\nConference on Computing, Analytics and Security\nTrends (CAST), pages 489–494. IEEE.\nJing Pan, Joshua Shapiro, Jeremy Wohlwend, Kyu J\nHan, Tao Lei, and Tao Ma. 2020. Asapp-asr: Mul-\ntistream cnn and self-attentive sru for sota speech\nrecognition. arXiv preprint arXiv:2005.10469.\nVishal Passricha and Rajesh Kumar Aggarwal. 2019.\nA hybrid of deep cnn and bidirectional lstm for au-\ntomatic speech recognition. Journal of Intelligent\nSystems, 29(1):1261–1274.\nBachchu Paul, Santanu Phadikar, Somnath Bera,\nTanushree Dey, and Utpal Nandi. 2025. Isolated\nword recognition based on a hyper-tuned cross-\nvalidated cnn-bilstm from mel frequency cepstral\ncoefficients.\nMultimedia Tools and Applications,\n84(17):17309–17328.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2023.\nRobust speech recognition via large-scale weak su-\npervision. In International conference on machine\nlearning, pages 28492–28518. PMLR.\nTara N Sainath and Carolina Parada. 2015. Convolu-\ntional neural networks for small-footprint keyword\nspotting. Interspeech 2015.\nAnushka Tonk, Arsh Akhtar, and P Pankaja Lakshmi.\n2024. Automatic speech recognition with customized\nbi-gru hybrid model for real-time speech analysis. In\nInternational Conference on Artificial Intelligence on\nTextile and Apparel, pages 233–251. Springer.\nShengnan Zhang, Yan Hu, and Guangrong Bian. 2017.\nResearch on string similarity algorithm based on lev-\nenshtein distance. In 2017 IEEE 2nd Advanced Infor-\nmation Technology, Electronic and Automation Con-\ntrol Conference (IAEAC), pages 2247–2251. IEEE.\n"}]}