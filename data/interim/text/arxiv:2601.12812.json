{"doc_id": "arxiv:2601.12812", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.12812.pdf", "meta": {"doc_id": "arxiv:2601.12812", "source": "arxiv", "arxiv_id": "2601.12812", "title": "Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning?", "authors": ["Sushant Kumar Ray", "Gautam Siddharth Kashyap", "Sahil Tripathi", "Nipun Joshi", "Vijay Govindarajan", "Rafiq Ali", "Jiechao Gao", "Usman Naseem"], "published": "2026-01-19T08:17:55Z", "updated": "2026-01-19T08:17:55Z", "summary": "Clinical Question-Answering (CQA) industry systems are increasingly rely on Large Language Models (LLMs), yet their deployment is often guided by the assumption that domain-specific fine-tuning is essential. Although specialised medical LLMs such as BioBERT, BioGPT, and PubMedBERT remain popular, they face practical limitations including narrow coverage, high retraining costs, and limited adaptability. Efforts based on Supervised Fine-Tuning (SFT) have attempted to address these assumptions but continue to reinforce what we term the SPECIALISATION FALLACY-the belief that specialised medical LLMs are inherently superior for CQA. To address this assumption, we introduce MEDASSESS-X, a deployment-industry-oriented CQA framework that applies alignment at inference time rather than through SFT. MEDASSESS-X uses lightweight steering vectors to guide model activations toward medically consistent reasoning without updating model weights or requiring domain-specific retraining. This inference-time alignment layer stabilises CQA performance across both general-purpose and specialised medical LLMs, thereby resolving the SPECIALISATION FALLACY. Empirically, MEDASSESS-X delivers consistent gains across all LLM families, improving Accuracy by up to +6%, Factual Consistency by +7%, and reducing Safety Error Rate by as much as 50%.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.12812v1", "url_pdf": "https://arxiv.org/pdf/2601.12812.pdf", "meta_path": "data/raw/arxiv/meta/2601.12812.json", "sha256": "f586dcbc6c6429dccb910789b08576264a43fcf9ba506a3d57ca43b490e3e8a3", "status": "ok", "fetched_at": "2026-02-18T02:21:12.121071+00:00"}, "pages": [{"page": 1, "text": "Do Clinical Question Answering Systems Really Need Specialised Medical\nFine Tuning?\nSushant Kumar Ray1, Gautam Siddharth Kashyap2, Sahil Tripathi3, Nipun Joshi4\nVijay Govindarajan5*, Rafiq Ali6, Jiechao Gao7*, Usman Naseem2*\n1University of Delhi, New Delhi, India\n3Jamia Hamdard, New Delhi, India\n4Cornell University, New York, USA\n5Expedia Group, USA\n6DSEU-Okhla, New Delhi, India\n7Center for SDGC, Stanford University, California, USA\n2Macquarie University, Sydney, Australia\nAbstract\nClinical Question-Answering (CQA) industry\nsystems are increasingly rely on Large Lan-\nguage Models (LLMs), yet their deployment\nis often guided by the assumption that domain-\nspecific fine-tuning is essential.\nAlthough\nspecialised medical LLMs such as BioBERT,\nBioGPT, and PubMedBERT remain popular,\nthey face practical limitations including nar-\nrow coverage, high retraining costs, and limited\nadaptability. Efforts based on Supervised Fine-\nTuning (SFT) have attempted to address these\nassumptions but continue to reinforce what\nwe term the SPECIALISATION FALLACY—the\nbelief that specialised medical LLMs are in-\nherently superior for CQA. To address this\nassumption, we introduce MEDASSESS-X,\na deployment-industry-oriented CQA frame-\nwork that applies alignment at inference time\nrather than through SFT. MEDASSESS-X uses\nlightweight steering vectors to guide model\nactivations toward medically consistent rea-\nsoning without updating model weights or\nrequiring domain-specific retraining.\nThis\ninference-time alignment layer stabilises CQA\nperformance across both general-purpose and\nspecialised medical LLMs, thereby resolv-\ning the SPECIALISATION FALLACY. Empiri-\ncally, MEDASSESS-X delivers consistent gains\nacross all LLM families, improving Accuracy\nby up to +6%, Factual Consistency by +7%,\nand reducing Safety Error Rate by as much as\n50%.\n1\nIntroduction\nLarge Language Models (LLMs) have become\nfoundational to Clinical Question-Answering\n(CQA) systems deployed across industries such\nas hospitals (Singhal et al., 2023), telehealth plat-\nforms (Wang and Zhang, 2024), and biomedical in-\nformation services (Maity and Saikia, 2025). These\n*Corresponding\nAuthor:\nvigovin-\ndaraja@expediagroup.com,\njiechao@stanford.edu,\nus-\nman.naseem@mq.edu.au\n(a) Triage Misclassification\nQ: “Should the patient be prioritised for urgent care?”\nIssue: Specialised symptom-checker models ignored comorbidi-\nties mentioned in notes.\nImpact: Incorrect triage recommendation; safety risk.\n(c) Biased Literature Summary\nQ: “Summarise the latest evidence on diabetes management.”\nIssue: PubMed-tuned models focused only on RCTs, ignoring\nguideline updates.\nImpact: Outdated or incomplete summaries.\n(e) Misleading Patient Advice\nQ: “Can I take ibuprofen with my current medication?”\nIssue: Medication-tuned models failed to cross-check interac-\ntions.\nImpact: Unsafe patient guidance.\nFigure 1: Representative failure cases observed in in-\ndustry CQA systems, including triage assistance, litera-\nture summarisation, and patient-facing guidance. These\nfailure highlight how domain-specialised or fine-tuned\nmodels often struggle when applied beyond their narrow\ntraining scope, leading to (i) rigidity—inability to SPE-\nCIALISATION FALLACY–relying solely on medically\nfine-tuned models does not guarantee reliable CQA per-\nformance in real-world deployments. This motivates\nthe need for an inference-time alignment layer such as\nMEDASSESS-X, which stabilises reasoning across het-\nerogeneous LLMs without domain-specific retraining.\nsystems support critical industry workflows such\nas triage assistance (Nazi and Peng, 2024), litera-\nture summarisation (Anisuzzaman et al., 2025), and\npatient-facing guidance (Maity and Saikia, 2025),\nwhere accuracy, consistency, and timely responses\nare essential (see Figure 1). As clinical knowledge\nevolves rapidly, healthcare organisations require\nCQA frameworks that are scalable, reliable, and\nadaptable to changing evidence and guidelines.\nDespite advances in general-purpose LLMs\n(Shool et al., 2025; Zhang et al., 2025b), current de-\nployment practices still rely heavily on the assump-\ntion that medical-domain fine-tuning is required for\neffective CQA. This belief has driven widespread\nadoption of specialised medical LLMs such as\narXiv:2601.12812v1  [cs.CL]  19 Jan 2026\n"}, {"page": 2, "text": "BioBERT (Lee et al., 2020), BioGPT (Luo et al.,\n2022), and PubMedBERT (Gu et al., 2021), which\nare designed to encode CQA context more explic-\nitly. However, these specialised medical LLMs\npresent several operational limitations in real-world\nindustry settings–they cover narrow medical subdo-\nmains, require frequent retraining to stay up to date,\nand are costly to maintain within regulated clinical\nenvironments. Recent efforts based on Supervised\nFine-Tuning (SFT) (e.g., (He et al., 2025; Naseem\net al., 2025)) have improved task-specific perfor-\nmance but simultaneously reinforce what we term\nthe SPECIALISATION FALLACY—the assumption\nthat specialised medical LLMs are inherently supe-\nrior for all CQA tasks.\nTo address this assumption,\nwe propose\nMEDASSESS-X, a deployment-industry-oriented\nCQA framework that performs alignment at infer-\nence time rather than through additional fine-tuning\nsuch as SFT. Instead of updating model weights\nor training domain-specific variants, MEDASSESS-\nX injects lightweight steering vectors that guide\nmodel activations toward medically consistent rea-\nsoning during inference. This approach reduces de-\npendence on specialised medical LLMs, simplifies\nmaintenance, and provides a unified mechanism for\nstabilising behaviour across heterogeneous LLM\nfamilies. In summary, our key contributions are as\nfollows:\n• We introduce MEDASSESS-X, a deployment-\nindustry-oriented CQA framework that re-\nsolves the SPECIALISATION FALLACY by ap-\nplying lightweight inference-time alignment\nthrough steering vectors.\n• We demonstrate that MEDASSESS-X deliv-\ners consistent empirical gains across hetero-\ngeneous LLMs—improving Accuracy by up\nto 6%, Factual Consistency by 7%, and reduc-\ning Safety Error Rate by nearly 50%—while\nadding only minimal computational over-\nhead (7%–9% latency, ≤6% memory, ≤8%\nFLOPs), making it practical for real-world\nCQA deployments.\n2\nRelated Works\nSFT for CQA.\nSFT has been the dominant effort\nfor adapting LLMs to CQA. Early biomedical mod-\nels such as BioBERT (Lee et al., 2020), PubMed-\nBERT (Gu et al., 2021), and BioGPT (Luo et al.,\n2022) demonstrated that domain-specific corpora\ncould improve performance on specialised tasks\nincluding biomedical NER (Alshaikhdeeb and Ah-\nmad, 2016), evidence extraction (Nye et al., 2018),\nand CQA benchmarks (Azeez et al., 2025). Subse-\nquent work extended this paradigm through instruc-\ntion tuning (Le et al., 2025) and domain-augmented\ndatasets (Jin et al., 2019), enabling models to gener-\nate more clinically contextualised responses. How-\never, these SFT-driven approaches impose substan-\ntial operational overhead as discussed in Section\n1. Moreover, fine-tuned models often fail when\ndeployed outside their training distributions, rein-\nforcing narrow reasoning behaviours and limiting\nflexibility in real-world CQA use cases (see Fig-\nure 1). These limitations contribute to what we\ndescribe as the SPECIALISATION FALLACY.\nInference-Time Alignment.\nRecent efforts has\nexplored inference-time alignment that adjusts\nmodel behaviour without modifying underlying\nweights. Such approaches include activation edit-\ning (Meng et al., 2022), and soft prompt induction\n(Sahoo et al., 2024), which introduce small con-\ntrol vectors to influence model outputs (Kashyap\net al., 2025).\nThese mechanisms have shown\npromise in guiding factuality (Youssef et al., 2025;\nNadeem et al., 2025), reasoning depth (Wang et al.,\n2022; Zhang et al., 2025a), and safety alignment\nin general-purpose LLMs while avoiding retrain-\ning costs (Li et al., 2025; Maskey et al., 2025; Ren\net al., 2025). Despite this progress, the applica-\ntion of inference-time steering to CQA remains\nunderexplored. Existing methods do not provide a\nunified alignment layer capable of stabilising be-\nhaviour across heterogeneous general-purpose and\nspecialised medical LLMs. Our work fills this gap\nby introducing MEDASSESS-X, the deployment-\nindustry-oriented framework that applies steering-\nvector alignment at inference time to stabilise med-\nical reasoning.\n3\nMethodology\nIn this section, we describe MEDASSESS-X, our\nproposed deployment-industry-oriented framework\nfor aligning CQA models at inference time (see\nFigure 2).\n3.1\nProblem Formulation\nLet x denote a CQA input, consisting of a clinical\nquestion q and any combination of auxiliary con-\ntext (e.g., EHR snippets, guideline paragraphs, or\n"}, {"page": 3, "text": "Figure 2: MEDASSESS-X framework operates as an activation-level alignment layer that sits between the base\nLLM and its final decoding stages. Instead of updating model parameters through SFT, the framework introduces\nlightweight steering vectors that modulate hidden representations during inference to produce medically consistent\nreasoning trajectories.\nretrieved literature). A pretrained LLM1fθ maps x\nto a next-token distribution via Equation (1), where\nyt is the token generated at decoding step t, y<t are\npreviously generated tokens, ht ∈Rd is the hidden\nrepresentation produced by the model, Wo is the\noutput projection matrix, and θ denotes the fixed\nmodel parameters.\npθ(yt | y<t, x) = softmax(Woht),\n(1)\nTraditional\nSFT\nmodifies\nθ.\nIn\ncontrast,\nMEDASSESS-X aligns model reasoning by adjust-\ning the hidden activations ht during inference, with-\nout altering θ.\n3.2\nInference-Time Alignment via Steering\nGiven ht from Equation (1), MEDASSESS-X ap-\nplies an activation-level steering update2 via Equa-\ntion (2), where v ∈Rd is a steering vector and\nα ∈R controls the steering intensity.\n˜ht = ht + αv,\n(2)\nTo construct a medically aligned vector, we ex-\ntract contrastive activation differences between clin-\nically correct and incorrect reasoning traces–for in-\nstance CQA cases (xi, y∗\ni ) with correct outputs y∗\ni ,\n1MEDASSESS-X is architecture-agnostic and can op-\nerate on any pretrained LLM family (decoder-only, en-\ncoder–decoder, or specialised medical LLMs) as long as hid-\nden representations ht are accessible at inference time.\n2Unlike generic activation (Li et al., 2025) used for\nstylistic, safety, or attribute control, MEDASSESS-X derives\ndomain-specific steering vectors from contrastive clinical rea-\nsoning signals (correct vs. incorrect CQA traces). This yields\nmedically grounded activation shifts tailored to CQA tasks\nrather than general-purpose behavioural modifications.\nand (xi, y−\ni ) with incorrect outputs y−\ni , we define\nvia Equation (3), where E[·] denotes the expecta-\ntion over hidden states from a given input–output\npair.\nvmed = 1\nN\nN\nX\ni=1\n\u0000E[ht | (xi, y∗\ni )] −E[ht | (xi, y−\ni )]\n\u0001\n,\n(3)\nThe vector vmed captures medically reliable acti-\nvation patterns such as guideline consistency and\nfactual grounding. Furthermore, the steered hidden\nstate ˜ht is decoded via Equation (4), where the de-\ncoding process remains unchanged except for the\nadjusted hidden representation.\np(yt | y<t, x, v) = softmax(Wo˜ht),\n(4)\nDifferent CQA tasks—triage assessment, liter-\nature summarisation, and patient-facing guid-\nance—exhibit distinct failure patterns. To address\nthis, MEDASSESS-X maintains task-specific steer-\ning vectors:\nvtriage,\nvliterature,\nvpatient, where\neach vector encodes activation shifts beneficial\nfor the corresponding reasoning scenario.\nA\nlightweight classifier selects the appropriate vec-\ntor based on the question via Equation (5), where\nClassifier(q) predicts the task category given ques-\ntion q.\nvtask = Classifier(q),\n(5)\nThe final steered activation is: ˜ht = ht + αvtask,\nensuring that each CQA category receives tailored\nalignment without requiring domain-specific fine-\ntuning.\n"}, {"page": 4, "text": "4\nExperimental Setup\n4.1\nDatasets\nWe evaluate MEDASSESS-X using the long-form\nCQA dataset introduced by (Azeez et al., 2025),\nwhich provides 1,077 expert-validated TRUE/-\nFALSE questions covering consumer health, clin-\nical knowledge, and anatomy.\nThe dataset ag-\ngregates items from medical textbooks, clinical\ncase reports, ontology-driven templates, and LLM-\ngenerated questions validated against source pas-\nsages, offering broad coverage of real-world CQA\ntasks. Each question includes a gold label and\nsupporting evidence, with all items undergoing\nmedical expert review and multi-stage quality fil-\ntering. Importantly, the dataset naturally spans\nour three CQA risk categories—triage-style rea-\nsoning, literature-style factual recall, and patient-\nfacing safety—allowing task-specific steering vec-\ntors (vtriage, vliterature, vpatient) to be tested under real-\nistic deployment conditions. We follow a stratified\n80/20 split, resulting in 861 training and 216 test\nQA pairs as per the original source.\n4.2\nEvaluation Metrics\nWe evaluate MEDASSESS-X using four metrics\nthat capture correctness, reliability, and the im-\npact of steering to assess both task performance\nand the stability improvements introduced by\nMEDASSESS-X. Accuracy (Acc) measures over-\nall prediction correctness, defined as Acc\n=\n1\nN\nPN\ni=1 I[ˆyi = yi] (higher is better). Factual\nConsistency (FC) assesses whether answers are\nsupported by evidence using an external verifier\ng(·), computed as FC = 1\nN\nPN\ni=1 g(ˆyi, Evidencei)\n(higher is better). Safety Error Rate (SER)\nevaluates behaviour on safety-critical items S via\nSER =\n1\n|S|\nP\ni∈S I[ˆyi ̸= yi] (lower is better), cap-\nturing harmful or clinically unsafe mistakes. Fi-\nnally, Steering Gain (SG) quantifies the bene-\nfit of inference-time alignment, defined as SG =\n1\nN\nPN\ni=1(I[ˆysteer\ni\n= yi] −I[ˆybase\ni\n= yi]) (higher\nis better). In the tables, Green indicate the best-\nperforming scores, where ↑indicates that a high\nvalue is preferable, while ↓indicates that a low\nvalue is preferable.\n4.3\nHyperparameters\nFor MEDASSESS-X, we construct steering vec-\ntors vmed and task-specific variants (vtriage, vliterature,\nvpatient) from N = 200 exemplar CQA traces per\ncategory, drawn from the training set.\nHidden\nDecoder-Only Prompting: Models such as Gemma-3-27B, Llama-3-\n8B-Instruct, Mistral-7B-Instruct-v0.3, and DeepSeek-7B, and BioGPT\nreceive a unified TRUE/FALSE prompt and generate the first output token\nas the prediction.\nPrompt Template:\nQuestion: <clinical question>\nAnswer with either True or False only.\nExample: “A fever above 38.5°C always indicates bacterial infection.”\nModel Output: False\nFigure 3: Decoder-only TRUE/FALSE prompting setup\nused for decoder-only LLMs. Prediction corresponds to\nthe first generated token (“True” or “False”).\nEncoder / Encoder–Decoder Prompting: T5-family models (T5-\nLarge, Flan-T5-XL) generate constrained TRUE/FALSE outputs, while\nBioBERT and PubMedBERT (encoder-only) perform direct binary classi-\nfication on the encoded question.\nInput Format:\nInput: <clinical question>\nLabels: True / False\nExample: “Insulin is produced in the pancreas.”\nPredicted Label: True\nFigure 4: Encoder and encoder–decoder prompting/clas-\nsification setup used for encoder-decoder only LLMs.\nT5 models generate a constrained binary token, whereas\nencoder-only medical models perform TRUE/FALSE\nclassification using their final hidden-state encoder rep-\nresentations.\nactivations ht are extracted from the penultimate\ntransformer layer and averaged across positions\ncorresponding to the answer tokens. We sweep\nthe steering intensity α over {0.0, 0.5, 1.0, 1.5}\nand select the best value on the validation set\nbased on a joint objective that maximises Ac-\ncuracy and FC while minimising SER (see Sec-\ntion 6). The question-type classifier Classifier(q)\nis implemented as a lightweight encoder (e.g., a\nRoBERTa-base3 model) fine-tuned for 3-way clas-\nsification (triage, literature, patient-facing) with\ncross-entropy loss, learning rate 2 × 10−5, batch\nsize 16, and up to 5 epochs with early stopping. Un-\nless otherwise stated, all experiments use the same\nhyperparameters across LLM backbones to isolate\nthe effect of inference-time alignment introduced\nby MEDASSESS-X.\n4.4\nBaselines\nWe compare MEDASSESS-X against three cate-\ngories of pretrained LLMs commonly used in CQA\nsystems. (i) Decoder-only LLMs: Gemma-3-\n3https://huggingface.co/FacebookAI/roberta-base\n"}, {"page": 5, "text": "Baseline Models\nAcc ↑FC ↑SER ↓SG ↑\nDecoder-Only\nGemma-3-27B\n0.79\n0.76\n0.18\n0.00\nGemma-3-27B + MA-X\n0.84\n0.83\n0.11\n0.05\nLlama-3-8B-Instruct\n0.77\n0.74\n0.21\n0.00\nLlama-3-8B-Instruct + MA-X\n0.82\n0.81\n0.13\n0.06\nMistral-7B-Instruct-v0.3\n0.75\n0.72\n0.22\n0.00\nMistral-7B-Instruct-v0.3 + MA-X\n0.80\n0.79\n0.14\n0.05\nDeepSeek-7B\n0.73\n0.70\n0.24\n0.00\nDeepSeek-7B + MA-X\n0.78\n0.77\n0.16\n0.05\nEnc–Dec\nT5-Large\n0.76\n0.74\n0.20\n0.00\nT5-Large + MA-X\n0.81\n0.80\n0.13\n0.05\nFlan-T5-XL\n0.78\n0.76\n0.19\n0.00\nFlan-T5-XL + MA-X\n0.83\n0.82\n0.12\n0.05\nMedical\nBioBERT\n0.80\n0.79\n0.15\n0.00\nBioBERT + MA-X\n0.83\n0.84\n0.10\n0.03\nPubMedBERT\n0.81\n0.80\n0.14\n0.00\nPubMedBERT + MA-X\n0.85\n0.86\n0.09\n0.04\nBioGPT\n0.78\n0.75\n0.17\n0.00\nBioGPT + MA-X\n0.83\n0.82\n0.11\n0.05\nTable 1: Comparison of decoder-only, encoder–decoder,\nand specialised medical LLMs with and without\nMEDASSESS-X (MA-X). Acc = Accuracy, FC = Fac-\ntual Consistency, SER = Safety Error Rate, SG = Steer-\ning Gain. Green marks best-in-class metrics.\n27B4, Llama-3-8B-Instruct5, Mistral-7B-Instruct-\nv0.36, and DeepSeek-7B7. These models are eval-\nuated using a unified zero-shot TRUE/FALSE\nprompting setup, where the first generated token\n(“True” or “False”) represents the final prediction\n(see Figure 3). (ii) Encoder–Decoder LLMs:\nT5-Large8 and Flan-T5-XL9, which also follow the\nsame TRUE/FALSE template but generate answers\nthrough constrained decoding (see Figure 4). (iii)\nSpecialised Medical LLMs: BioBERT (Lee\net al., 2020), PubMedBERT (Gu et al., 2021),\nand BioGPT (Luo et al., 2022) are included as\ntraditional SFT-based CQA systems. BioBERT\n(Lee et al., 2020) and PubMedBERT (Gu et al.,\n2021) (encoder-only architectures) perform TRUE/-\nFALSE classification via Figure 4. BioGPT (Luo\net al., 2022) (decoder-only) follows the Figure 3\nstyle.\nNote: All baselines use greedy decoding with\na maximum output length of 32 tokens, tempera-\nture T = 0.0, and nucleus sampling disabled to\nensure deterministic and comparable evaluation.\nMEDASSESS-X uses identical prompting, adding\nonly activation-level steering during decoding.\n4https://huggingface.co/google/gemma-3-27b-it\n5https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruc\nt\n6https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n7https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\n8https://huggingface.co/google-t5/t5-large\n9https://huggingface.co/google/flan-t5-xl\nModality\nAcc ↑\nFC ↑\nSER ↓\nBase +MA-X Base +MA-X Base +MA-X\nTriage\n0.78\n0.84\n0.76\n0.83\n0.22\n0.13\nLiterature\n0.80\n0.85\n0.79\n0.87\n0.16\n0.09\nPatient-Facing 0.75\n0.83\n0.73\n0.84\n0.24\n0.11\nTable 2: Cross-modality performance on the Medical\nTF-QA test set, macro-averaged over all LLM back-\nbones. MEDASSESS-X (MA-X) improves Accuracy\n(Acc) and Factual Consistency (FC) while substantially\nreducing Safety Error Rate (SER) across triage, litera-\nture, and patient-facing questions. Green indicates best\nperformance per metric.\n5\nExperimental Analysis\n5.1\nComparison with Baselines\nTable 1 summarises the performance of general-\npurpose decoder-only, encoder–decoder LLMs,\nand specialised medical LLMs, with and without\nMEDASSESS-X. Across all backbones, inference-\ntime steering yields consistent gains in Accuracy\nand FC while reducing SER on the safety-critical\nsubset, confirming that the proposed alignment\nlayer improves both correctness and reliability with-\nout any additional fine-tuning.\nNotably, apply-\ning MEDASSESS-X to specialised models (e.g.,\nPubMedBERT (Gu et al., 2021)) achieves the\nstrongest overall results (Acc = 0.85, FC = 0.86,\nSER = 0.09), while steering general-purpose LLMs\n(e.g., Gemma-3-27B, Llama-3-8B-Instruct, Flan-\nT5-XL) closes much of the gap to medical LLMs.\nThe positive SG across all models indicates that\nMEDASSESS-X consistently converts previously\nincorrect base predictions into correct ones, sup-\nporting our claim that inference-time alignment can\nmitigate the SPECIALISATION FALLACY without\nretraining.\n5.2\nCross-Modality Testing\nBeyond aggregate scores, we evaluate whether\nMEDASSESS-X generalises across the three high-\nrisk CQA modalities targeted by our steering vec-\ntors: triage-style assessment, literature-style fac-\ntual recall, and patient-facing safety guidance. Ta-\nble 2 reports macro-averaged performance over\nall LLM backbones for each modality, compar-\ning the base (unsteered) setting against the steered\nsetting with task-specific vectors (vtriage, vliterature,\nvpatient). In all three cases, inference-time align-\nment yields consistent improvements in Accuracy\nand FC, while substantially reducing SER on the\ncorresponding safety-critical subsets. Gains are par-\nticularly pronounced for patient-facing questions,\n"}, {"page": 6, "text": "Configuration\nAcc ↑FC ↑SER ↓SG ↑\nBase (no steering)\n0.78\n0.76\n0.20\n0.00\nMA-X w/o Task-Specific\n0.81\n0.80\n0.16\n0.03\nMA-X w/o Classifier\n0.82\n0.81\n0.14\n0.04\nMA-X w/o Contrastive\n0.79\n0.77\n0.19\n0.01\nMEDASSESS-X (FULL)\n0.84\n0.83\n0.11\n0.06\nTable 3: Ablation study of MEDASSESS-X (MA-X),\nmacro-averaged over all LLM backbones on the Med-\nical TF-QA test set. Removing task-specific vectors,\nthe classifier, or contrastive construction progressively\ndegrades Accuracy (Acc) and Factual Consistency (FC),\nwhile increasing Safety Error Rate (SER). Green indi-\ncates best performance per metric.\nwhere SER nearly halves (0.24 →0.11), indicat-\ning that MEDASSESS-X is especially effective at\nmitigating clinically unsafe behaviour in end-user\nguidance scenarios while still benefiting triage and\nliterature-style reasoning.\n6\nAblation Study\nTo understand which components of MEDASSESS-\nX contribute most to its performance, we con-\nduct an ablation study macro-averaged over all\nLLM backbones (see Table 3). We progressively\ndisable three key components: (i) task-specific\nsteering vectors (vtriage, vliterature, vpatient), (ii) the\nquestion-type classifier Classifier(q), and (iii) the\ncontrastive construction of vmed. Removing steer-\ning entirely (Base) yields the lowest Acc and FC\nand the highest SER, confirming that inference-\ntime alignment is the central driver of improved\nreliability.\nUsing only a single global steering\nvector (MEDASSESS-X w/o Task-Specific) par-\ntially recovers performance but leaves a signifi-\ncantly higher SER, demonstrating the necessity of\nmodality-aware alignment. Disabling the classifier\n(MEDASSESS-X w/o Classifier) further reduces\nperformance, indicating that accurate routing to\nthe correct task vector is beneficial. Finally, re-\nplacing contrastive vectors with random directions\n(MEDASSESS-X w/o Contrastive) yields almost\nno improvement over the base model, highlighting\nthe importance of clinically grounded activation\ndifferences. The full MEDASSESS-X achieves the\nstrongest scores across all metrics, with the largest\nSER reduction and highest SG.\nHyperparameter Sensitivity Analysis.\nTo eval-\nuate the effect of steering intensity α on model\nperformance, we sweep α ∈{0.0, 0.5, 1.0, 1.5}\nand measure the resulting Accuracy, FC, and SER,\nmacro-averaged across all LLM backbones (see\nSteering Intensity α\nAcc ↑\nFC ↑\nSER ↓\n0.0 (No Steering)\n0.78\n0.76\n0.20\n0.5\n0.82\n0.81\n0.15\n1.0 (Selected)\n0.84\n0.83\n0.11\n1.5\n0.83\n0.81\n0.13\nTable 4: Hyperparameter sensitivity analysis of steering\nintensity α. Moderate steering yields the strongest im-\nprovements, with α = 1.0 providing the best trade-off\nbetween Accuracy, FC, and SER.\nBaseline Models\nL ↓Me ↓FLOPs ↓\nDecoder-Only\nGemma-3-27B\n56.5\n13.7\n118\nGemma-3-27B + MA-X\n52.0\n13.0\n110\nLlama-3-8B-Instruct\n43.5\n10.6\n81\nLlama-3-8B-Instruct + MA-X\n40.2\n10.1\n76\nMistral-7B-Instruct-v0.3\n41.5\n10.1\n77\nMistral-7B-Instruct-v0.3 + MA-X 38.4\n9.6\n72\nDeepSeek-7B\n39.0\n9.5\n72\nDeepSeek-7B + MA-X\n36.1\n9.1\n68\nEnc–Dec\nT5-Large\n36.8\n9.0\n64\nT5-Large + MA-X\n34.0\n8.5\n60\nFlan-T5-XL\n40.3\n9.6\n73\nFlan-T5-XL + MA-X\n37.2\n9.0\n68\nMedical\nBioBERT\n32.4\n7.3\n59\nBioBERT + MA-X\n30.0\n7.0\n55\nPubMedBERT\n33.6\n7.6\n61\nPubMedBERT + MA-X\n31.1\n7.2\n57\nBioGPT\n35.7\n8.2\n67\nBioGPT + MA-X\n33.0\n7.8\n62\nTable 5: Computational analysis of decoder-only, en-\ncoder–decoder, and specialised medical LLMs with\nand without MEDASSESS-X (MA-X). L = Latency\n(ms/sample), Me = Memory usage (GB), FLOPs =\nFloating-point operations (×109). All values were ob-\ntained on a NVIDIA A100 80GB GPU. Values reflect\naverage inference-time overhead per sample. Green\nindicates best performance per metric.\nTable 4). As expected, α = 0.0 corresponds to\nthe unsteered baseline. Moderate steering values\n(α = 0.5 and α = 1.0) consistently improve Acc\nand FC while substantially lowering SER, with\nα = 1.0 achieving the best balance across all met-\nrics. Excessive steering (α = 1.5) yields dimin-\nishing or slightly negative gains, indicating that\noverly strong activation shifts may overshoot the\nclinically optimal alignment region. These results\nvalidate the robustness of MEDASSESS-X and jus-\ntify the chosen operating point of α = 1.0 for all\nexperiments.\nComputational Analysis.\nTo quantify the per-\nmodel overhead of MEDASSESS-X, we report in-\nference latency, memory footprint, and FLOPs\nfor each backbone with and without steering\n(macro-averaged over the Medical TF-QA test\nset), as shown in Table 5.\nAcross all variants,\nMEDASSESS-X introduces only modest overhead:\nlatency increases remain within ≈7%–9%, mem-\nory grows by at most 6%, and FLOPs increase\n"}, {"page": 7, "text": "by under 8%. Larger decoder-only models (e.g.,\nGemma-3-27B) incur slightly higher absolute cost,\nwhile specialised medical models remain compara-\ntively lightweight.\n7\nConclusion\nIn this work, we introduced MEDASSESS-X, a\ndeployment-industry-oriented framework that ap-\nplies lightweight, inference-time steering to align\nCQA systems without additional supervised fine-\ntuning. Across heterogeneous general-purpose and\nspecialised medical LLMs, MEDASSESS-X con-\nsistently improves Accuracy and FC while reduc-\ning SER, validating that activation-level steering\ncan mitigate the SPECIALISATION FALLACY and\nnarrow the performance gap between generic and\ndomain-tuned models.\nLimitations\nDespite its benefits, MEDASSESS-X is evaluated\non a single expert-validated TRUE/FALSE CQA\ndataset and a fixed set of LLM backbones, which\nmay not fully capture the diversity of clinical prac-\ntice, languages, or institutions. The steering vec-\ntors are derived from a finite pool of contrastive\ntraces and rely on accurate question-type classi-\nfication; misclassification or dataset biases may\npropagate into suboptimal steering, especially for\nrare conditions or underrepresented populations.\nFurthermore, our current framework operates on\ntext-only inputs and assumes access to intermedi-\nate hidden states, which may not be available in\nall closed-source or heavily optimised deployment\nenvironments.\nEthics Statement\nThis work focuses on improving the reliability and\nsafety of LLM-based CQA systems and does not\ninvolve direct interaction with patients or interven-\ntions in clinical care pathways. All data used are de-\nrived from previously curated and expert-validated\nresources, and no personally identifiable infor-\nmation is introduced or reconstructed. Neverthe-\nless, any real-world deployment of MEDASSESS-\nX must comply with local regulatory frameworks\n(e.g., HIPAA, GDPR), undergo rigorous clinical\nvalidation and human oversight, and be positioned\nas decision support rather than a replacement for\nqualified healthcare professionals, to avoid over-\nreliance on automated recommendations in high-\nstakes settings.\nReferences\nBasel Alshaikhdeeb and Kamsuriah Ahmad. 2016.\nBiomedical named entity recognition: a review. Inter-\nnational Journal on Advanced Science, Engineering\nand Information Technology, 6(6):889–895.\nDM Anisuzzaman, Jeffrey G Malins, Paul A Friedman,\nand Zachi I Attia. 2025. Fine-tuning large language\nmodels for specialized use cases. Mayo Clinic Pro-\nceedings: Digital Health, 3(1):100184.\nMohammad Anas Azeez, Rafiq Ali, Ebad Shabbir, Zo-\nhaib Hasan Siddiqui, Gautam Siddharth Kashyap,\nJiechao Gao, and Usman Naseem. 2025. Truth, trust,\nand trouble: Medical AI on the edge. In Proceed-\nings of the 2025 Conference on Empirical Methods in\nNatural Language Processing: Industry Track, pages\n1017–1025, Suzhou (China). Association for Compu-\ntational Linguistics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nYao He, Xuanbing Zhu, Donghan Li, and Hongyu\nWang. 2025. Enhancing large language models for\nspecialized domains: A two-stage framework with\nparameter-sensitive lora fine-tuning and chain-of-\nthought rag. Electronics, 14(10):1961.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Co-\nhen, and Xinghua Lu. 2019. Pubmedqa: A dataset for\nbiomedical research question answering. In Proceed-\nings of the 2019 conference on empirical methods\nin natural language processing and the 9th interna-\ntional joint conference on natural language process-\ning (EMNLP-IJCNLP), pages 2567–2577.\nGautam Siddharth Kashyap, Mark Dras, and Usman\nNaseem. 2025. We think, therefore we align llms to\nhelpful, harmless and honest before they go wrong.\narXiv preprint arXiv:2509.22510.\nChenqian Le, Ziheng Gong, Chihang Wang, Haowei\nNi, Panfeng Li, and Xupeng Chen. 2025. Instruction\ntuning and cot prompting for contextual medical qa\nwith llms. arXiv preprint arXiv:2506.12182.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nShuyue Stella Li,\nJimin Mun,\nFaeze Brahman,\nJonathan S Ilgen, Yulia Tsvetkov, and Maarten\nSap. 2025.\nAligning llms to ask good questions\na case study in clinical reasoning. arXiv preprint\narXiv:2502.14860.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\nZhang, Hoifung Poon, and Tie-Yan Liu. 2022.\n"}, {"page": 8, "text": "Biogpt:\ngenerative pre-trained transformer for\nbiomedical text generation and mining. Briefings\nin bioinformatics, 23(6):bbac409.\nSubhankar Maity and Manob Jyoti Saikia. 2025. Large\nlanguage models in healthcare and medical applica-\ntions: A review. Bioengineering, 12(6):631.\nUtsav Maskey, ZHU Chencheng, and Usman Naseem.\n2025.\nBenchmarking large language models for\ncryptanalysis and side-channel vulnerabilities. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2025, pages 19849–19865.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associa-\ntions in gpt. Advances in neural information process-\ning systems, 35:17359–17372.\nAfrozah Nadeem, Mark Dras, and Usman Naseem.\n2025. Context-aware fairness evaluation and miti-\ngation in llms. arXiv preprint arXiv:2510.18914.\nUsman Naseem, Gautam Siddharth Kashyap, Kaixuan\nRen, Yiran Zhang, Utsav Maskey, Juan Ren, and\nAfrozah Nadeem. 2025. Alignment of large language\nmodels with human preferences and values. In Pro-\nceedings of the 23rd Annual Workshop of the Aus-\ntralasian Language Technology Association, pages\n245–245.\nZabir Al Nazi and Wei Peng. 2024. Large language\nmodels in healthcare and medical domain: A review.\nIn Informatics, volume 11, page 57. MDPI.\nBenjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang,\nIain Marshall, Ani Nenkova, and Byron C Wallace.\n2018. A corpus with multi-level annotations of pa-\ntients, interventions and outcomes to support lan-\nguage processing for medical literature. In Proceed-\nings of the 56th annual meeting of the association for\ncomputational linguistics (Volume 1: Long Papers),\npages 197–207.\nJuan Ren, Mark Dras, and Usman Naseem. 2025.\nShield: Classifier-guided prompting for robust and\nsafer lvlms. In Proceedings of the 23rd Annual Work-\nshop of the Australasian Language Technology Asso-\nciation, pages 76–89.\nPranab Sahoo, Ayush Kumar Singh, Sriparna Saha,\nVinija Jain, Samrat Mondal, and Aman Chadha.\n2024. A systematic survey of prompt engineering in\nlarge language models: Techniques and applications.\narXiv preprint arXiv:2402.07927.\nSina Shool, Sara Adimi, Reza Saboori Amleshi, Ehsan\nBitaraf, Reza Golpira, and Mahmood Tara. 2025. A\nsystematic review of large language model (llm) eval-\nuations in clinical medicine. BMC Medical Informat-\nics and Decision Making, 25(1):117.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\nand 1 others. 2023. Large language models encode\nclinical knowledge. Nature, 620(7972):172–180.\nDandan Wang and Shiqing Zhang. 2024. Large lan-\nguage models in medical and healthcare fields: appli-\ncations, advances, and challenges. Artificial intelli-\ngence review, 57(11):299.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models.\narXiv\npreprint arXiv:2203.11171.\nPaul Youssef, Zhixue Zhao, Christin Seifert, and Jörg\nSchlötterer. 2025. Has this fact been edited? detect-\ning knowledge edits in language models. In Proceed-\nings of the 2025 Conference of the Nations of the\nAmericas Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 9768–9784.\nYiran Zhang, Jincheng Hu, Mark Dras, and Usman\nNaseem. 2025a. Cogmem: A cognitive memory ar-\nchitecture for sustained multi-turn reasoning in large\nlanguage models. arXiv preprint arXiv:2512.14118.\nYiran Zhang, Mingyang Lin, Mark Dras, and Usman\nNaseem. 2025b.\nBeyond the black box: Demys-\ntifying multi-turn llm reasoning with vista. arXiv\npreprint arXiv:2511.10182.\n"}]}