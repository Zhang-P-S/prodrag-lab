{"doc_id": "arxiv:2602.13321", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.13321.pdf", "meta": {"doc_id": "arxiv:2602.13321", "source": "arxiv", "arxiv_id": "2602.13321", "title": "Detecting Jailbreak Attempts in Clinical Training LLMs Through Automated Linguistic Feature Extraction", "authors": ["Tri Nguyen", "Huy Hoang Bao Le", "Lohith Srikanth Pentapalli", "Laurah Turner", "Kelly Cohen"], "published": "2026-02-10T21:57:55Z", "updated": "2026-02-10T21:57:55Z", "summary": "Detecting jailbreak attempts in clinical training large language models (LLMs) requires accurate modeling of linguistic deviations that signal unsafe or off-task user behavior. Prior work on the 2-Sigma clinical simulation platform showed that manually annotated linguistic features could support jailbreak detection. However, reliance on manual annotation limited both scalability and expressiveness. In this study, we extend this framework by using experts' annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction) and training multiple general-domain and medical-domain BERT-based LLM models to predict these features directly from text. The most reliable feature regressor for each dimension was selected and used as the feature extractor in a second layer of classifiers. We evaluate a suite of predictive models, including tree-based, linear, probabilistic, and ensemble methods, to determine jailbreak likelihood from the extracted features. Across cross-validation and held-out evaluations, the system achieves strong overall performance, indicating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis further highlights key limitations in current annotations and feature representations, pointing toward future improvements such as richer annotation schemes, finer-grained feature extraction, and methods that capture the evolving risk of jailbreak behavior over the course of a dialogue. This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.13321v1", "url_pdf": "https://arxiv.org/pdf/2602.13321.pdf", "meta_path": "data/raw/arxiv/meta/2602.13321.json", "sha256": "2285095aa0604e94e74309e41762d653b8d64d36159bc4c03e041c84d919d1d4", "status": "ok", "fetched_at": "2026-02-18T02:19:26.353658+00:00"}, "pages": [{"page": 1, "text": "Detecting Jailbreak Attempts in Clinical Training\nLLMs Through Automated Linguistic Feature\nExtraction\nTri Nguyen , Huy Hoang Bao Le, Lohith Srikanth Pentapalli , Laurah\nTurner , and Kelly Cohen\nUniversity of Cincinnati, Cincinnati OH 45219, USA\nnguye3hr@mail.uc.edu, lebu@mail.uc.edu, pentapl5@mail.uc.edu,\nturnela@ucmail.uc.edu, cohenky@ucmail.uc.edu\nAbstract. Detecting jailbreak attempts in clinical training large lan-\nguage models (LLMs) requires accurate modeling of linguistic deviations\nthat signal unsafe or off-task user behavior. Prior work on the 2-Sigma\nclinical simulation platform showed that manually annotated linguistic\nfeatures could support jailbreak detection. However, reliance on manual\nannotation limited both scalability and expressiveness. In this study, we\nextend this framework by using experts’ annotations of four core lin-\nguistic features (Professionalism, Medical Relevance, Ethical Behavior,\nand Contextual Distraction) and training multiple general-domain and\nmedical-domain BERT-based LLM models to predict these features di-\nrectly from text. The most reliable feature regressor for each dimension\nwas selected and used as the feature extractor in a second layer of clas-\nsifiers. We evaluate a suite of predictive models, including tree-based,\nlinear, probabilistic, and ensemble methods, to determine jailbreak like-\nlihood from the extracted features. Across cross-validation and held-out\nevaluations, the system achieves strong overall performance, indicating\nthat LLM-derived linguistic features provide an effective basis for auto-\nmated jailbreak detection. Error analysis further highlights key limita-\ntions in current annotations and feature representations, pointing toward\nfuture improvements such as richer annotation schemes, finer-grained\nfeature extraction, and methods that capture the evolving risk of jail-\nbreak behavior over the course of a dialogue. This work demonstrates a\nscalable and interpretable approach for detecting jailbreak behavior in\nsafety-critical clinical dialogue systems.\nKeywords: Jailbreak Detection · Clinical Training LLMs · LLM Fea-\nture Extraction · Interpretability.\n1\nIntroduction\n2-Sigma is a clinical training platform that leverages large language models to\nsimulate patient interactions, enabling medical students to practice clinical rea-\nsoning, diagnosis, and decision-making through conversational, case-based sce-\nnarios. While this approach offers substantial educational benefits, it also intro-\nduces new safety challenges, as students may attempt to manipulate the LLM\narXiv:2602.13321v1  [cs.AI]  10 Feb 2026\n"}, {"page": 2, "text": "2\nT. Nguyen et al.\nthrough jailbreak behaviors that degrade professionalism, ethical standards, or\nalignment with the intended clinical task. Prior work on 2-Sigma demonstrated\nthat such jailbreak attempts are strongly correlated with specific linguistic pat-\nterns and showed that manually engineered linguistic features could be used\nto detect jailbreak behavior with high accuracy [9]. However, this feature-based\nframework relied on labor-intensive human annotation and expanded categorical\nencodings, which limited both scalability and expressiveness. Building on this\nfoundation, the present study seeks to automate linguistic feature extraction us-\ning fine-tuned LLMs while preserving interpretability, enabling a scalable and\nrobust approach to jailbreak detection in clinical training environments.\nThis study adopts a two-layer modeling architecture for jailbreak detection in\nclinical LLM interactions, as shown in Fig. 1. In the first layer, four independently\nfine-tuned transformer-based regressors map each user message—using either\nsingle-turn or multi-turn context—to continuous scores across linguistic dimen-\nsions shown to correlate with jailbreak behavior: Professionalism, Medical Rel-\nevance, Ethical Behavior, and Contextual Distraction. These LLM-derived fea-\ntures provide a compact, semantically informed representation of each prompt.\nIn the second layer, multiple downstream classifiers, spanning linear, probabilis-\ntic, and ensemble methods, use the resulting four-dimensional feature vector\nto predict jailbreak likelihood. This modular design separates linguistic feature\nextraction from classification, enabling automated annotation, improved inter-\npretability, and flexible substitution of models at either stage. Experimental\nresults show that the proposed two-layer framework achieves strong and stable\nperformance across multiple classifiers, providing an automated yet interpretable\nfoundation for jailbreak detection through explicit linguistic features.\nFig. 1: System overview of the proposed framework.\n2\nRelated Work\nLarge Language Models have demonstrated strong performance across natural\nlanguage understanding and generation tasks, yet they remain vulnerable to\njailbreak attacks. Jailbreak attacks involve designing adversarial prompts that\ncircumvent LLMs’ safety constraints and bypass alignment. With the increas-\ning sophistication of jailbreaking strategies, it is paramount to develop reliable\ndetection mechanisms. Existing research spans surface-level classifiers, prompt\n"}, {"page": 3, "text": "Detecting Jailbreak in Clinical Training LLMs\n3\nperturbation techniques, internal signal monitoring, and gradient-based intro-\nspection, each offering complementary strengths and limitations.\nEarly jailbreak detection methods are primarily based on classifier-based and\nheuristic approaches. Many LLM providers deploy moderation filters (e.g. Ope-\nnAI’s Moderation API) to flag toxic or policy-violating content, but these generic\nclassifiers may miss the nuanced strategies of jailbreak prompts [12]. Zero-shot\nsafety judgments performed internally by LLMs themselves have also proven\nunreliable [12]. To improve robustness and performance, supervised detectors\ntrained on labeled jailbreak data have been proposed. Hawkins et al. showed\nthat a fine-tuned BERT classifier outperforms generic moderation systems by\nexploiting lexical and syntactic cues such as explicit reflexivity in prompt struc-\nture and role-play framing that correlate strongly with jailbreak intent [3]. Lin-\nguistically motivated approaches such as \"Prompter Says\" further emphasize\nsyntactic structure and phrasing as indicators of adversarial prompts [7]. In\naddition, statistical techniques such as prompt perplexity have been explored,\nwith evidence that malicious prompts often exhibit anomalous perplexity rela-\ntive to benign inputs [1]. While effective against known attack patterns, these\napproaches highlight the importance of richer data and adaptive modeling to\nenhance robustness against evolving jailbreaking strategies.\nTo handle increasingly obfuscated attacks, researchers have introduced\naugmentation-based and multi-prompt detection strategies. SmoothLLM ap-\nplies randomized smoothing by perturbing user prompts and aggregating model\nresponses, exploiting instability near decision boundaries characteristic of jail-\nbreak prompts [10]. There has also been an increase in hybrid approaches, which\naid in reducing false positives. Self-Reminder mechanisms reinforce safety by\nprepending alignment-focused system prompts [11], while multi-agent frame-\nworks such as AutoDefense introduce gatekeeper agents that analyze prompts\nbefore execution, improving detection of complex jailbreak strategies at the cost\nof increased latency and system complexity [14].\nMore recent work has shifted toward internal signal monitoring, leverag-\ning the observation that aligned LLMs internally recognize disallowed intent\neven when coerced into compliance. JBShield operationalizes this idea by using\nconcept-activation vectors representing toxic content and jailbreak manipulation,\nenabling accurate detection when both concepts are simultaneously activated\n[15]. Beyond detection, JBShield can intervene by modifying latent representa-\ntions to restore refusal behavior. HiddenDetect introduces a training-free method\nthat monitors hidden-layer activations for refusal-related semantics by measur-\ning similarity to embeddings of refusal phrases [5]. This method generalizes to\nmultimodal models, where delayed or suppressed refusal signals correlate with\nhigher jailbreak success. These findings demonstrate that LLMs encode intrinsic\nsafety-relevant signals in their internal representations, which can be leveraged\nfor jailbreak detection.\nIn parallel, gradient-based detection methods have emerged as another intro-\nspective strategy. GradSafe analyzes gradients of the next-token loss with respect\nto safety-critical parameters and identifies characteristic patterns shared across\n"}, {"page": 4, "text": "4\nT. Nguyen et al.\nunsafe prompts, operating directly on pre-trained models without additional\nfine-tuning [12]. Gradient Cuff further refines this idea by explicitly modeling\nrefusal loss and jointly examining its magnitude and gradient norm, where a\nlow refusal loss combined with a high gradient norm signals adversarial pressure\naway from refusal [4]. Complementary work has shown that analysis of hidden-\nlayer representations using tensor decomposition can reveal latent factors that\nnaturally separate jailbreak prompts from benign inputs, even without labeled\ndata [6].\nAlongside jailbreak detection, LLMs have increasingly been used for fea-\nture extraction and representation learning. Transformer-based models such as\nBERT and GPT provide dense semantic embeddings that outperform manu-\nally engineered features in many NLP tasks. In the medical domain, LLM-based\nsystems have achieved near-expert performance in extracting structured clinical\ninformation from unstructured electronic health records, significantly surpass-\ning traditional regex-based and n-gram approaches [13]. Beyond text analytics,\nLLMs have also been applied to feature selection in tabular datasets by lever-\naging semantic knowledge of feature descriptions rather than relying solely on\nstatistical criteria [8]. More advanced systems such as Rogue One extend these\nideas through collaborative multi-agent architectures that autonomously gener-\nate, evaluate, and refine new features, producing both performance gains and\nsemantically meaningful representations [2].\nIn summary, jailbreak detection has evolved from surface-level classifiers to\nintrospective methods that exploit hidden states, gradients, and latent repre-\nsentations within LLMs. While modern techniques achieve strong detection per-\nformance, they vary in computational cost, interpretability, and robustness to\nunseen attacks. Concurrently, advances in LLM-based feature extraction high-\nlight the models’ capacity to generate rich, high-level representations. This dual\nrole of LLMs as both vulnerable targets and powerful feature extractors moti-\nvates hybrid detection frameworks. In particular, fuzzy-logic-based approaches\nprovide a principled mechanism for integrating heterogeneous signals under un-\ncertainty, making them well suited for robust jailbreak detection.\n3\nMethods\n3.1\nData Preparation\nThe dataset used in this study originates from 2-Sigma’s initial evaluation phase,\nwhich collected 158 conversations comprising 2,302 total prompts containing a\nmixture of jailbreak (JB) and non-jailbreak interactions. In this context, a jail-\nbreak refers to instances where a user’s communication deteriorates in profes-\nsionalism, relevance, ethical behavior, or contextual alignment as they attempt\nto steer the clinical LLM toward unsafe, inappropriate, or off-task behavior. Each\nprompt was independently labeled as JB or non-JB by two annotators, forming\nthe binary ground-truth labels used for downstream classification.\nIn addition to the binary jailbreak labels, the previous study [9] provided a\nset of human annotations evaluating four conversational linguistic features (Pro-\n"}, {"page": 5, "text": "Detecting Jailbreak in Clinical Training LLMs\n5\nfessionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction)\nthat were shown to correlate strongly with jailbreak attempts. Six annotators\nrated every prompt using structured ordinal scales: unprofessional →borderline\n→appropriate for Professionalism; irrelevant →partially relevant →relevant\nfor Medical Relevance; dangerous →unsafe →questionable →mostly safe →\nsafe for Ethical Behavior; and highly distracting →moderately distracting →\nquestionable →not distracting for Contextual Distraction.\nTo support the goal of automating the annotation process, each linguistic\nfeature was converted into a continuous regression target. The categorical ratings\nfrom all annotators were first mapped to ordinal numeric values, after which the\naverage rating for each prompt was computed. The resulting continuous targets\nfall within the numeric range of 1 to the maximum level of their respective rating\nscales, preserving the ordinal meaning of the annotations. This procedure yielded\nfour continuous variables that capture the consensus severity of each linguistic\ndeviation while retaining inter-annotator variability. These continuous targets\nwere then used to fine-tune a set of pretrained LLMs, enabling them to act as\nfeature extractors capable of predicting the four linguistic dimensions for unseen\nprompts.\nFollowing LLM fine-tuning, each prompt was represented by a four-\ndimensional vector containing the model-predicted levels of Professionalism,\nMedical Relevance, Ethical Behavior, and Contextual Distraction. These learned\nfeatures served as the input to a suite of downstream predictive models trained\nto classify whether a user message constitutes a jailbreak attempt. To prevent\nconversation-level leakage and ensure realistic evaluation, the dataset was split\nby conversation: 147 conversations (2,021 prompts) were used for training and\nvalidation of both the LLM regressors and the predictive classifiers, while the\nremaining 11 conversations (281 prompts) were held out as an unseen test set\nfor final evaluation.\n3.2\nLLM Feature Extraction\nTo construct semantic features capable of capturing subtle linguistic deviations\nassociated with jailbreak attempts, we fine-tuned a diverse suite of transformer-\nbased encoders on four continuous expert-derived features: Professionalism,\nMedical Relevance, Ethical Behavior, and Contextual Distraction. These pre-\ntrained encoder architectures includes BERT-Base, BERT-Large, DistilBERT,\nRoBERTa-Base, DeBERTa-v3-Large, BioBERT, and PubMedBERT. Detailed\ndescriptions of these models and their pretraining corpora are provided in the\nAppendix.\nEach model was fine-tuned as a regression predictor, using either single-\nturn messages or cumulative multi-turn context for features that are sensitive\nto conversational flow (particularly Contextual Distraction), as demonstrated in\nTable 1. Inputs were tokenized using each model’s native tokenizer and encoded\nby the transformer encoder, followed by a single-output regression head that\npredicts a continuous score. Training optimized the root mean squared error\n"}, {"page": 6, "text": "6\nT. Nguyen et al.\nTable 1: Example of single-turn and multi-turn LLM training inputs, human\nannotations, and corresponding numerical regression targets.\nLLM Training Inputs\nHuman\nAnnotation\n(6 annotators)\nLLM Training Targets\nSingle-turn\nMessage\nMulti-turn\nMessage\nProf.\nMed.\nRel.\nEth.\nBeh.\nCont.\nDist.\nI am a second\nyear\nmedical\nstudent. Could\nI have a case\nabout urology?\nI am a second\nyear\nmedical\nstudent. Could\nI have a case\nabout urology?\n(6) Professional\n(6) Relevant\n(6) Safe\n(6)\nNot\nDis-\ntracting\n3\n3\n5\n4\nHi\nsir,\nwhat\nbrings\nyou\nin\ntoday?\nI am a second\nyear\nmedical\nstudent. Could\nI have a case\nabout urology?\nHi\nsir,\nwhat\nbrings\nyou\nin\ntoday?\n(6) Professional\n(6) Relevant\n(6) Safe\n(6)\nNot\nDis-\ntracting\n3\n3\n5\n4\n(RMSE), defined as\nRMSE =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n(ˆyi −yi)2,\nwhile performance was additionally evaluated using the coefficient of determina-\ntion,\nR2 = 1 −\nPn\ni=1 (yi −ˆyi)2\nPn\ni=1 (yi −¯y)2 ,\nwhere yi denotes the expert-annotated target score, ˆyi the model prediction, and\n¯y the mean of all target scores. To quantify prediction bias, we also report the\nmean error,\nMean Error = 1\nn\nn\nX\ni=1\n(ˆyi −yi) ,\nand the standard deviation of error,\nSD Error =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n[(ˆyi −yi) −Mean Error]2.\nA complete summary of all regression metrics (RMSE, R2, mean error, and\nstandard deviation of error) for every model-feature pair is provided in Table 6.\nTraining stability across model scales was supported by small per-device batch\n"}, {"page": 7, "text": "Detecting Jailbreak in Clinical Training LLMs\n7\nTable 2: Best-performing pretrained model for each linguistic feature based on\nRMSE.\nLinguistic Feature\nBest Model RMSE\nR2\nProfessionalism\nDistilBERT\n0.4441 0.5939\nMedical Relevance\nBERT\n0.4630 0.5983\nEthical Behavior\nBioBERT\n0.5751 0.4965\nContextual Distraction\nBioBERT\n0.6701 0.6147\nsizes with gradient accumulation, weight decay regularization, and early stopping\nbased on validation RMSE, with the checkpoint achieving the lowest validation\nRMSE retained for each linguistic feature.\nAfter fine-tuning, each encoder produced task-specific contextual embeddings\nthat operationalize the feature it was trained to predict. Because the architec-\ntures differ in representational capacity, pretraining objectives, and domain spe-\ncialization, the resulting embeddings capture complementary linguistic informa-\ntion: general semantic patterns (e.g., BERT, DistilBERT), deeper contextual and\nsyntactic cues (e.g., RoBERTa, DeBERTa), and clinically grounded biomedical\nsemantics (e.g., BioBERT, PubMedBERT). Based on the regression outcomes,\ndifferent pretrained encoders emerged as the top-performing models for differ-\nent linguistic features. Specifically, DistilBERT achieved the lowest RMSE for\nProfessionalism, BERT performed best for Medical Relevance, and BioBERT\nwas the most effective model for both Ethical Behavior and Contextual Distrac-\ntion, as shown in Table 2. Accordingly, the downstream prediction layer uses the\nmodel fine-tuned on its corresponding feature, ensuring that the feature repre-\nsentations are optimally aligned with the linguistic dimension they are intended\nto capture.\n3.3\nPredictive Modeling\nThe final jailbreak classification layer was evaluated using a diverse set of pre-\ndictive models including tree-based, linear, probabilistic, and ensemble learning\napproaches. Specifically, we trained Decision Trees (DT), Fuzzy Decision Trees\n(FDT), Random Forests (RF), Light Gradient Boosting Machine (LGBM), Ex-\ntreme Gradient Boosting (XGBoost), Logistic Regression (LR), and Gaussian\nNaïve Bayes (NB). All models operated on the four LLM-extracted linguistic\nfeatures generated by the best-performing LLM for each dimension (Table 2).\nThe classifiers were implemented using the scikit-learn, lightgbm, xgboost,\nand skfuzzy libraries, with default parameter settings. 5-fold cross-validation\nwas performed on the training set to evaluate model stability and generalization.\nThe performance metrics averaged across folds and their standard deviation are\nsummarized in Table 4.\n"}, {"page": 8, "text": "8\nT. Nguyen et al.\n4\nResults and Discussion\nTable 3 reports the test-set performance of all downstream jailbreak-detection\nclassifiers across five evaluation metrics: accuracy, precision, recall, F1-score,\nand ROC-AUC. Each model was evaluated on the four LLM-extracted linguistic\nfeatures to generate a final prediction for each user message. Table 4 presents the\ncorresponding cross-validation results, providing mean and standard deviation\nvalues for the same metrics.\nTable 3: Test set performance of downstream jailbreak-detection classifiers across\nmultiple evaluation metrics.\nModel\nAccuracy Precision Recall\nF1\nROC-AUC\nDT\n0.8256\n0.5789\n0.7213 0.6423\n0.7879\nFDT\n0.8932\n0.7246\n0.8197 0.7692\n0.8667\nRF\n0.9004\n0.7463\n0.8197 0.7812\n0.8712\nLGBM\n0.8790\n0.6901\n0.8033 0.7424\n0.8516\nXGBoost\n0.8861\n0.6986\n0.8361 0.7612\n0.8680\nLR\n0.8897\n0.7027\n0.8525 0.7704\n0.8762\nNB\n0.8968\n0.7286\n0.8361 0.7786\n0.8749\nCompared to the prior study [9], the two-layer approach proposed here of-\nfers two key advantages: it automates the annotation of linguistic features and\nprovides interpretable insight into how those features contribute to the final jail-\nbreak prediction. As shown in Table 3, the system achieves strong performance\nacross multiple metrics, demonstrating that LLM-extracted linguistic cues can\nreliably support jailbreak detection. Although performance is slightly lower than\nclassifiers trained directly on ground-truth annotations (Table 7), this difference\nis expected due to the propagation of regression error from the first layer. Im-\nportantly, the overall results remain robust, as supported by cross-validation\noutcomes in Table 4, indicating that the feature extraction stage effectively cap-\ntures the linguistic signals most predictive of jailbreak behavior.\nAcross\nclassifiers,\nDecision\nTrees\nshow\nthe\nweakest\nperformance\non\nboth cross-validation and the test set, with marked drops across all met-\nrics—suggesting overfitting and limited ability to model the structure of the\nfeature space. In contrast, ensemble methods such as Random Forest, Light-\nGBM, and XGBoost leverage aggregated weak learners to capture more complex\npatterns, yielding consistently strong results; among them, Random Forest per-\nforms best on the held-out test set. Fuzzy Decision Trees, Logistic Regression,\nand Naïve Bayes also perform competitively in cross-validation, in some cases\nslightly outperforming the ensemble models. Their success suggests that the\nLLM-extracted features form a representation that is close to linearly separa-\nble. Notably, the substantial gap between standard and fuzzy Decision Trees\ndemonstrates that incorporating soft decision boundaries improves robustness\nwhen modeling subtle linguistic deviations.\n"}, {"page": 9, "text": "Detecting Jailbreak in Clinical Training LLMs\n9\nTable 4: Cross-validation performance comparison of classifiers (µ ± σ). Bold =\nbest; bold italic = second-best.\n(a) Accuracy, F1-Score, Precision\nModel\nAccuracy\nF1-Score\nPrecision\nDT\n0.8745 ± 0.0168\n0.8752 ± 0.0163\n0.8738 ± 0.0233\nFDT\n0.9118 ± 0.0093\n0.9114 ± 0.0090\n0.9200 ± 0.0159\nRF\n0.9083 ± 0.0083\n0.9084 ± 0.0071\n0.9125 ± 0.0187\nLGBM\n0.9092 ± 0.0087\n0.9090 ± 0.0075\n0.9157 ± 0.0220\nXGBoost\n0.9070 ± 0.0116\n0.9067 ± 0.0115\n0.9141 ± 0.0171\nLR\n0.9127 ± 0.0135\n0.9118 ± 0.0136\n0.9246 ± 0.0172\nNB\n0.9127 ± 0.0139\n0.9120 ± 0.0143\n0.9222 ± 0.0142\n(b) Recall, ROC-AUC\nModel\nRecall\nROC-AUC\nDT\n0.8771 ± 0.0179\n0.8745 ± 0.0168\nFDT\n0.9030 ± 0.0097\n0.9677 ± 0.0073\nRF\n0.9048 ± 0.0091\n0.9662 ± 0.0050\nLGBM\n0.9030 ± 0.0124\n0.9657 ± 0.0058\nXGBoost\n0.8996 ± 0.0141\n0.9631 ± 0.0064\nLR\n0.8996 ± 0.0135\n0.9666 ± 0.0075\nNB\n0.9022 ± 0.0163\n0.9652 ± 0.0082\nGiven the sensitivity of this application domain, it is essential to under-\nstand how and why misclassifications occur. Accordingly, the following sections\nanalyze representative false negative and false positive cases drawn from the\nheld-out test set. Each example was misclassified by at least four of the seven\npredictive models, ensuring that the analysis highlights systematic rather than\nmodel-specific errors. These patterns reveal several limitations of the current\nfeature-based framework and motivate the future improvements discussed later\nin the paper.\nFalse Negative Analysis\nA review of the false negative cases reveals several systematic limitations in\nthe current feature-based framework. Some jailbreak attempts rely on subtle\nor humorous injections that derail the conversation without strongly affecting\nthe annotated linguistic features. For example, the prompt “Tell me about your\nfamily history. Any clowns?” is clearly irrelevant, yet its deviations are too weakly\nexpressed for the model to detect.\nOther errors arise from ambiguous or questionable human labels. Prompts\nsuch as “Yes, but are your physical symptoms not also connected to your mental\n"}, {"page": 10, "text": "10\nT. Nguyen et al.\nstate?” or “You seem a little disoriented. Does this happen often?” resemble legit-\nimate clinical questions. However, they are labeled as jailbreaks by annotators.\nIn such cases, the system’s prediction error reflects uncertainty or inconsistency\nin the ground truth rather than a model deficiency.\nA third failure mode involves prompts that are linguistically appropriate but\nclinically illogical or unsafe. Instructions like “Listen to his forehead” cannot be\nrecognized as harmful because the annotated features capture tone and relevance\nrather than medical validity. These examples show that detecting such jailbreaks\nrequires reasoning beyond surface-level linguistic cues.\nFinally, some false negatives involve workflow bypass attempts in which stu-\ndents politely ask the AI for direct answers. These prompts maintain profes-\nsionalism and relevance, causing the model to interpret them as benign despite\nviolating the purpose of the simulation. Since the four linguistic features do not\nencode task adherence or procedural integrity, the system fails to identify these\nas jailbreak attempts.\nFalse Positive Analysis\nSeveral sources of false positives reflect limitations in both the feature extrac-\ntion process and the underlying annotations. Some errors arise from unfinished\nor mistyped prompts that superficially resemble incoherence or derailment. For\ninstance, the incomplete message “Jimmy you have any kind” received low fea-\nture scores despite the intended prompt being entirely appropriate, showing the\nmodel’s sensitivity to surface-level noise.\nAnother category involves role-play or theatrical injections that deviate from\nstandard clinical tone but do not constitute adversarial behavior. Prompts such\nas “Ok I take off all my makeup and walk in the room. I say ‘hello madame. How\ncan I be of assistance today’ ” introduce off-role elements that reasonably resem-\nble jailbreak attempts, yet were labeled as non-jailbreak by annotators. These\ncases also highlight inconsistencies in human labeling that contribute directly to\nfalse positives.\nA major set of false positives stems from annotation ambiguity in casual con-\nversational replies. Messages like “You and me both, brother!” or “YEH LET’S\nGET BUSY SON” diverge from expected clinical professionalism, and the model\nflags them accordingly, but annotators classified them as benign. Such discrep-\nancies illustrate how differing interpretations of tone lead to inconsistent super-\nvision signals.\nThe system also struggles with mixed-quality prompts that blend clinically\nrelevant content with unrelated commentary. Examples include prompts such as\n“I think you’ve got a case against them here. I’ve got a great lawyer you should\ncall. But let’s get back to your exam first.” The prompt switches between casual\nconversation and medical diagnosing, causing distracting segments to dominate\nthe message-level features. Because the model cannot perfectly capture sentence-\nlevel nuance, it disproportionately penalizes these mixed intents.\nFinally, several false positives involve benign conversational affirmations that\ncontain little or no clinical content. Short responses such as “Good for you.” or\n"}, {"page": 11, "text": "Detecting Jailbreak in Clinical Training LLMs\n11\n“Edward, I think you’re a really great guy.” are harmless but fall near the decision\nboundary due to low medical relevance. This leads the classifier to misinterpret\nthem as indicators of distraction or derailment in the clinical workflow.\nSuggestions and Future Directions\nThe patterns observed in both the false positive and false negative cases highlight\nseveral limitations in the current feature-based jailbreak detection framework.\nMany errors arise from missing contextual nuance, annotation inconsistencies,\nmixed-content prompts, or workflow-related behaviors that the four linguistic\nfeatures were not designed to capture. These findings indicate that jailbreak be-\nhavior in clinical simulations often manifests in subtle or indirect ways, requiring\nmethods that extend beyond surface-level linguistic cues.\nOne avenue for improvement is the use of larger and more capable LLM anno-\ntators during the feature extraction stage. Stronger models may better interpret\ncontext, reduce susceptibility to injection-style distractions, and improve the ac-\ncuracy of the regression outputs on which downstream classifiers depend. This\ncould help mitigate several false predictions stemming from weak feature signals.\nAnother strategy involves segmenting prompts into finer-grained units so\nthat clinically relevant statements can be analyzed independently from irrelevant\nor casual language. This approach may reduce false positives in mixed-quality\nprompts and help the system distinguish clinical reasoning from conversational\npadding. Finer granularity could also strengthen detection of subtle jailbreak at-\ntempts that rely on embedding misleading content within otherwise appropriate\nmessages.\nNonetheless, system performance will remain constrained by the quality and\nconsistency of human annotations. Several misclassifications arise directly from\nambiguous or questionable labels, suggesting the need for expanded datasets with\nmore detailed annotation schemes. Additional features related to clinical actions,\nworkflow adherence, reasoning steps, or safety violations may help operationalize\ndistinctions that annotators currently make inconsistently.\nFinally, future work may explore temporal or conversation-level measures of\njailbreak likelihood rather than relying solely on isolated message predictions.\nTracking how jailbreak probability evolves across a dialogue could capture grad-\nual deviations or manipulative patterns that are overlooked when evaluating\nprompts independently. Together, these improvements can support a more ro-\nbust and context-aware jailbreak detection system.\n5\nConclusion\nIn this study, we present a two-layer framework for detecting jailbreak attempts\nin clinical LLM interactions by combining automated linguistic feature extrac-\ntion with downstream classification models. In the first layer, four fine-tuned\nLLM regressors generate continuous scores for the linguistic constructs shown\n"}, {"page": 12, "text": "12\nT. Nguyen et al.\nto correlate with jailbreak behavior: Professionalism, Medical Relevance, Ethi-\ncal Behavior, and Contextual Distraction. These model-derived features provide\nan interpretable and scalable alternative to manual annotation. In the second\nlayer, a diverse suite of classifiers leverages the four-dimensional feature repre-\nsentation to predict jailbreak likelihood. The system demonstrates strong per-\nformance across both cross-validation and held-out test sets, indicating that\nLLM-extracted linguistic cues form a reliable foundation for downstream jail-\nbreak detection.\nAn analysis of failure modes reveals that misclassifications often arise from\nlimitations in both the feature representation and the underlying human anno-\ntations. False negatives are frequently driven by subtle injections, ambiguous\nlabels, clinically illogical prompts, or workflow bypass attempts that fall outside\nthe expressive capacity of the four linguistic features. False positives, in con-\ntrast, frequently stem from annotation inconsistencies, mixed-quality prompts,\nconversational informality, or benign statements with low medical content that\nlie near the classifier’s decision boundary. These observations underscore the\nneed for richer and more consistent annotations, finer-grained feature extraction\nmethods, and potential integration of clinical reasoning or workflow modeling.\nFuture work should explore the use of more capable LLM annotators,\nsentence-level or segment-level feature extraction, expanded annotation schemes,\nand temporal modeling of jailbreak probability across an entire conversation.\nTogether, these enhancements could improve robustness and address the nu-\nanced ways in which jailbreak behavior emerges in clinical simulations. Overall,\nthe findings demonstrate that automated linguistic feature extraction coupled\nwith downstream classifiers provides a promising and interpretable path toward\nscalable jailbreak detection in safety-critical clinical LLM systems.\nReferences\n1. Alon, G., Kamfonas, M.: Detecting Language Model Attacks with Perplexity (Nov\n2023). https://doi.org/10.48550/arXiv.2308.14132\n2. Bradland, H., Goodwin, M., Zadorozhny, V.I., Andersen, P.A.: Knowledge-\nInformed Automatic Feature Extraction via Collaborative Large Language Model\nAgents (Nov 2025). https://doi.org/10.48550/arXiv.2511.15074\n3. Hawkins, J., Pramar, A., Beard, R., Chandra, R.: Machine Learning for Detection\nand Analysis of Novel LLM Jailbreaks (Oct 2025). https://doi.org/10.48550/a\nrXiv.2510.01644\n4. Hu, X., Chen, P.Y., Ho, T.Y.: Gradient Cuff: Detecting Jailbreak Attacks on Large\nLanguage Models by Exploring Refusal Loss Landscapes (Nov 2024). https://do\ni.org/10.48550/arXiv.2403.00867\n5. Jiang, Y., Gao, X., Peng, T., Tan, Y., Zhu, X., Zheng, B., Yue, X.: HiddenDetect:\nDetecting Jailbreak Attacks against Large Vision-Language Models via Monitoring\nHidden States (Jun 2025). https://doi.org/10.48550/arXiv.2502.14744\n6. Kadali, S.D.S.S., Papalexakis, E.E.: Do Internal Layers of LLMs Reveal Patterns\nfor Jailbreak Detection? (Oct 2025). https://doi.org/10.48550/arXiv.2510.06\n594\n"}, {"page": 13, "text": "Detecting Jailbreak in Clinical Training LLMs\n13\n7. Lee, D., Xie, S., Rahman, S., Pat, K., Lee, D., Chen, Q.A.: \"Prompter Says\": A\nLinguistic Approach to Understanding and Detecting Jailbreak Attacks Against\nLarge-Language Models. In: Proceedings of the 1st ACM Workshop on Large AI\nSystems and Models with Privacy and Safety Analysis. pp. 77–87. LAMPS ’24,\nAssociation for Computing Machinery, New York, NY, USA (Nov 2024). https:\n//doi.org/10.1145/3689217.3690618\n8. Li, D., Tan, Z., Liu, H.: Exploring Large Language Models for Feature Selection:\nA Data-centric Perspective (Oct 2024). https://doi.org/10.48550/arXiv.2408.\n12025\n9. Nguyen, T., Pentapalli, L.S., Sieverding, M., Turner, L., Overla, S., Zheng, W.,\nZhou, C., Furniss, D., Weber, D., Gharib, M., Kelleher, M., Shukis, M., Pawlik,\nC., Cohen, K.: Jailbreak detection in clinical training llms using feature-based\npredictive models (2025), https://arxiv.org/abs/2505.00010\n10. Robey, A., Wong, E., Hassani, H., Pappas, G.J.: SmoothLLM: Defending Large\nLanguage Models Against Jailbreaking Attacks (Jun 2024). https://doi.org/10\n.48550/arXiv.2310.03684\n11. Wu, F., Xie, Y., Yi, J., Shao, J., Curl, J., Lyu, L., Chen, Q., Xie, X.: Defending\nChatGPT against Jailbreak Attack via Self-Reminder (Apr 2023). https://doi.\norg/10.21203/rs.3.rs-2873090/v1\n12. Xie, Y., Fang, M., Pi, R., Gong, N.: GradSafe: Detecting Jailbreak Prompts for\nLLMs via Safety-Critical Gradient Analysis (May 2024)\n13. Yuan, K., Yoon, C.H., Gu, Q., Munby, H., Walker, A.S., Zhu, T., Eyre, D.W.:\nTransformers and large language models are efficient feature extractors for elec-\ntronic health record studies. Communications Medicine 5(1),\n83 (Mar 2025).\nhttps://doi.org/10.1038/s43856-025-00790-1\n14. Zeng, Y., Wu, Y., Zhang, X., Wang, H., Wu, Q.: AutoDefense: Multi-Agent LLM\nDefense against Jailbreak Attacks (Nov 2024). https://doi.org/10.48550/arXiv\n.2403.04783\n15. Zhang, S., Zhai, Y., Guo, K., Hu, H., Guo, S., Fang, Z., Zhao, L., Shen, C., Wang,\nC., Wang, Q.: JBShield: Defending Large Language Models from Jailbreak Attacks\nthrough Activated Concept Analysis and Manipulation (Feb 2025). https://doi.\norg/10.48550/arXiv.2502.07557\n"}, {"page": 14, "text": "14\nT. Nguyen et al.\nAppendix\nThis appendix provides supplementary tables and figures that support the results\npresented in the main text. These materials include detailed descriptions of all\npretrained transformer models used in this study, along with comprehensive\nregression performance tables for each linguistic feature. Additional tables also\nreport the test-set performance of classification models trained on ground-truth\nannotated features.\nTable 5: Summary of Pre-trained Transformer Models Used in This Study\nModel\nParams\nRelease\nTraining Data\nBERT (Base) google-\nbert/bert-base-uncased\n110M\nOct.\n2018\nBooksCorpus (800M words),\nEnglish Wikipedia (2.5B words)\nBERT (Large) google-\nbert/bert-large-uncased\n340M\nOct.\n2018\nBooksCorpus, English Wikipedia\nDistilBERT\ndistilbert/distilbert-base-\nuncased\n66M\nOct.\n2019\nBooksCorpus, English Wikipedia\nRoBERTa\nFacebookAI/roberta-base\n125M\nJul.\n2019\nBookCorpus, CC-News,\nOpenWebText, Stories (∼160GB)\nDeBERTa (Large)\nmicrosoft/deberta-v3-\nlarge\n304M\n2021\nLarge-scale web corpora\n(∼160GB)\nBioBERT (Base)\ndmis-lab/biobert-base-\ncased-v1.1\n110M\nJan.\n2019\nBERT + PubMed abstracts\n(4.5B), PMC full text (13.5B)\nPubMedBERT\nmicrosoft/BiomedNLP-\nBiomedBERT-base-\nuncased-abstract\n110M\nJul.\n2020\n14M PubMed abstracts (∼3.1B\nwords)\n"}, {"page": 15, "text": "Detecting Jailbreak in Clinical Training LLMs\n15\nTable 6: Regression results across all four dimensions evaluated in this study.\n(a) Professionalism and Medical Relevance.\nProfessionalism\nMedical Relevance\nModel\nRMSE\nR2\nError (µ ± σ)\nRMSE\nR2\nError (µ ± σ)\nBERT\n0.4571 0.5697 0.0246±0.4565 0.4630 0.5983 0.0056±0.4630\nBERT-Large\n0.4700 0.5450 0.0044±0.4700 0.4996 0.5324 -0.0923±0.4910\nDistilBERT\n0.4441 0.5939 -0.0037±0.4441 0.5098 0.5132 0.1294±0.4931\nRoBERTa\n0.4534 0.5766 -0.0304±0.4524 0.5656 0.4007 0.0058±0.5655\nDeBERTa-v3\n0.4667 0.5514 -0.0158±0.4665 0.5289 0.4760 0.0634±0.5251\nBioBERT\n0.4536 0.5763 -0.0315±0.4525 0.5306 0.4725 -0.0564±0.5276\nPubMedBERT 0.4608 0.5627 -0.0484±0.4583 0.5051 0.5221 -0.0624±0.5012\n(b) Ethical Behavior and Contextual Distraction.\nEthical Behavior\nContextual Distraction\nModel\nRMSE\nR2\nError (µ ± σ)\nRMSE\nR2\nError (µ ± σ)\nBERT\n0.6774 0.3015 -0.2707±0.6210 0.7112 0.5661 0.0094±0.7111\nBERT-Large\n0.6164 0.4216 -0.0402±0.6151 0.8078 0.4401 -0.0489±0.8063\nDistilBERT\n0.5982 0.4552 -0.1951±0.5655 0.7560 0.5097 0.0017±0.7560\nRoBERTa\n0.6757 0.3051 -0.0996±0.6683 0.8406 0.3938 0.0677±0.8378\nDeBERTa-v3\n0.6056 0.4417 0.0104±0.6056 0.6783 0.6053 0.2049±0.6466\nBioBERT\n0.5751 0.4965 -0.1355±0.5589 0.6701 0.6147 0.0039±0.6701\nPubMedBERT 0.5971 0.4573 0.0217±0.5967 0.9042 0.2986 -0.2277±0.8751\nTable 7: Test set performance of classification models using annotated features.\nModel\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\nDT\n0.9359\n0.8308\n0.8852\n0.8571\n0.9176\nFDT\n0.9431\n0.8571\n0.8852\n0.8710\n0.9222\nRF\n0.9288\n0.7971\n0.9016\n0.8462\n0.9190\nLGBM\n0.9253\n0.7857\n0.9016\n0.8397\n0.9167\nXGBoost\n0.9217\n0.7671\n0.9180\n0.8358\n0.9204\nLR\n0.9359\n0.8413\n0.8689\n0.8548\n0.9117\nNB\n0.9395\n0.8438\n0.8852\n0.8640\n0.9199\n"}]}