{"doc_id": "arxiv:2511.08245", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.08245.pdf", "meta": {"doc_id": "arxiv:2511.08245", "source": "arxiv", "arxiv_id": "2511.08245", "title": "Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG", "authors": ["Jisoo Jang", "Tien-Cuong Bui", "Yunjun Choi", "Wen-Syan Li"], "published": "2025-11-11T13:41:13Z", "updated": "2025-11-11T13:41:13Z", "summary": "This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.08245v1", "url_pdf": "https://arxiv.org/pdf/2511.08245.pdf", "meta_path": "data/raw/arxiv/meta/2511.08245.json", "sha256": "d5bf8709d0fdcc397125aeb8fa11a924fb09cdb09981afe53eb11822b6431608", "status": "ok", "fetched_at": "2026-02-18T02:27:23.691470+00:00"}, "pages": [{"page": 1, "text": "Prompt Tuning for Natural Language to SQL\nwith Embedding Fine-Tuning and RAG\nJisoo Jang, Tien-Cuong Bui, Yunjun Choi, and Wen-Syan Li\nGraduate School of Data Science, Seoul National University, Seoul, South Korea\n{simonjisu,cuongbt91,datajun77,wensyanli}@snu.ac.kr\nAbstract. This paper introduces an Error Correction through Prompt\nTuning for NL-to-SQL, leveraging the latest advancements in genera-\ntive pre-training-based LLMs and RAG. Our work addresses the crucial\nneed for efficient and accurate translation of natural language queries\ninto SQL expressions in various settings with the growing use of natu-\nral language interfaces. We explore the evolution of NLIDBs from early\nrule-based systems to advanced neural network-driven approaches. Draw-\ning inspiration from the medical diagnostic process, we propose a novel\nframework integrating an error correction mechanism that diagnoses er-\nror types, identifies their causes, provides fixing instructions, and applies\nthese corrections to SQL queries. This approach is further enriched by\nembedding fine-tuning and RAG, which harnesses external knowledge\nbases for improved accuracy and transparency. Through comprehensive\nexperiments, we demonstrate that our framework achieves a significant\n12 percent accuracy improvement over existing baselines, highlighting\nits potential to revolutionize data access and handling in contemporary\ndata-driven environments.\nKeywords: NL-to-SQL · Embedding Fine-Tuning · Large Language\nModels · Retrieval-Augmented Generation\n1\nIntroduction\nNatural language interfaces (NLIs) offer a convenient means for querying and\ninteracting with various data types. Increasingly, individuals are using everyday\nlanguage to access online and offline information. For instance, people might\ninquire about today’s weather or discuss tomorrow’s agenda on their personal\ncalendars with NLIs. A notable example is Moveworks, a company that primarily\ndevelops an AI copilot to boost employee productivity. This technology enables\nemployees to effortlessly access personal or company data using text, which em-\nploys natural language. As generative AI continues evolving, natural language\ninterfaces seamlessly integrate into our everyday lives.\nNatural Language Interfaces to Databases (NLIDBs) [1] facilitate data access\nin the Natural Language Processing (NLP) domain by allowing users to query\nPresented at the Workshop on Robust ML in Open Environments (PAKDD 2024)\narXiv:2511.08245v1  [cs.CL]  11 Nov 2025\n"}, {"page": 2, "text": "2\nJisoo et al.\ndatabases using natural language. These systems aim to translate a natural lan-\nguage question qnl into an equivalent SQL expression qsql (NL-to-SQL) in a given\ndatabase system D. Various approaches have been explored for NLIDBs. Early\nmethods like LUNAR [2] and Athena [3] employed rule-based systems to parse\nor map NL queries to an ontology syntactically and then to SQL. With advance-\nments in NLP and neural networks, intermediate representation systems trans-\nform NL queries into vector representations before generating SQL. For instance,\nSQLova [4] and HydraNet [5] predict SQL keyword sections, while RATSQL [6]\nand BRIDGE [7] utilize encoder-decoder frameworks for SQL generation.\nRecent advancements in generative pre-training-based large language mod-\nels (LLMs) [8][9] address NL-to-SQL challenges. These models, trained on vast\ninternet-sourced data, excel in understanding human language. They operate\nthrough sequence-to-sequence tasks, where users provide a prompt to gener-\nate conditional answers. Research shows that LLMs’ effectiveness in NL-to-SQL\ntasks is significantly influenced by the design of these prompts. Notably, in-\ncluding schema and foreign critical information in prompts led to a 51.6% im-\nprovement in execution accuracy on the Spider development dataset [10][11], a\nbenchmark for large-scale NL-to-SQL tasks.\nTwo fundamental approaches for improving LLM-based NL-to-SQL are fine-\ntuning [12][13], adapting LLMs to structured tabular data, and inference-only\n[14] with either prompt engineering or few-shot in-context learning. For instance,\nPourreza et al. [15] improved the generation performance by decomposing prob-\nlems and employing chain-of-thought [16] with few-shot prompting. Despite nu-\nmerous advancements, the few-shot approach depends on the quality and quan-\ntity of examples [17]. The Retrieval-Augmented Generation (RAG) approach\n[18] can mitigate the few-shot approach drawbacks, which enhances LLMs by\nintegrating facts from external knowledge bases. It ensures access to the most\ncurrent and accurate information and offers users transparency in the model’s\ngenerative process. However, optimizing content augmentation within LLMs’\nconstrained input size remains a complex issue in the NL-to-SQL field.\nThis paper proposes an Error Correction framework based on Prompt Tuning\nwith embedding fine-tuning and RAG for NL-to-SQL. The motivation for this\nwork stems from medical diagnosis processes. As depicted in Fig. 1, our frame-\nwork incorporates an error correction process that diagnoses error types, finds the\nreasons, provides fixing instructions based on RAG, and applies the instructions\nto fix SQL errors. To improve retrieval processes, we fine-tune a pre-trained Sen-\ntence Transformer model with a customized error correction dataset. We evaluate\nthe efficiency and correctness of our framework through extensive experiments.\nExperimental results demonstrate that our framework achieves a 12% accuracy\nimprovement compared to baselines.\nThe rest of the paper is organized as follows: Section 2 introduces background.\nSection 3 presents error analysis. Section 4 explains our framework. Section 5\nshows our experiment results. Finally, Section 6 summarizes the findings and\ndiscusses the limitations and future works.\n"}, {"page": 3, "text": "Prompt Tuning for NL2SQL with Embedding Fine-Tuning and RAG\n3\nLegend\nNew Error Case\nclassiﬁes\nCorrection Cases\nError Types \nKnowledge Base\n(Error id, Error name, \nand simple explanation)\nLLM\nClassiﬁed Error Types\nCase\nCorrection INFO\nCase\nVector DB\nrequest \nrelevant cases\nRelevant Cases\nLLM\nLLM\ngenerate\ngenerate\n1. Diagnose\n2. Write Prescription\n3. Apply Treatment\nText type object\nCase object\nTable description\nNL question\nGenerated SQL\nExecution result\nTable description\nNL question\nGenerated SQL\nExecution result\nError Type\nCorrect SQL\nReason\nInstruction\nReason\nInstruction\nCorrect SQL\nDiagnosis\nPrompt\nPrescription\nPrompt\nTreatment\nPrompt\ne0\ne1\n…\ne13\nNo error\nOther:DISTINCT\n…\nGroup-by:Wrong Cols\n[            ,            ]\nto text\nto text\ne4\ne9\nto text\nFig. 1. Overview of Error Correction through Prompt Tuning (ECPT)\n2\nBackground\n2.1\nLarge language model (LLM)\nLLMs like GPT-3.5 and GPT-4, PaLM [19], and LLaMa [20] achieve compre-\nhensive language understanding and generation via pre-training with an enor-\nmous amount of data and fine-tuning with RLHF [21]. Additionally, domain-\nspecific LLMs like BloombergGPT [22], trained with domain-specific and pro-\nprietary data, are also prevalent. LLMs excel in numerous NLP tasks since they\ncan understand diverse input contexts through billions of trained parameters.\n2.2\nFine-tuning Language Models\nFoundation models can be fine-tuned for specific tasks through primary meth-\nods: model fine-tuning, prompt tuning, and prompt engineering. Model\nfine-tuning involves adjusting the pre-trained model’s weights using supplemen-\ntary labeled data, which can be both time-intensive and costly. Conversely,\nprompt tuning or prompt engineering [14] is more cost-efficient. In prompt tun-\ning, AI-generated numerical sequences, known as soft prompts, alter a separate\ntuneable model’s embedding space to guide LLM to perform specific tasks. Ad-\nditionally, human engineers can employ hard prompts or engineered prompts,\nwhich are instructions or examples integrated with the input prompt, to steer\nthe model toward specific tasks. These hard prompts, considered few-shot learn-\ning examples, are more interpretable but generally less effective than the AI-\ngenerated soft prompts. While soft prompts offer better performance, they lack\ninterpretability, resembling a ‘black box’ similar to deep learning models, whereas\nhard prompts, being engineer-generated, are easier to understand.\n"}, {"page": 4, "text": "4\nJisoo et al.\n2.3\nRetrieval-Augmented Generation\nFine-tuning a model alone often fails to equip it with the comprehensive\nknowledge necessary to answer particular, contextually evolving questions. Since\nLLMs are trained with Internet data, they may suffer from inconsistencies and\noutdated information. Retrieval-Augmented Generation (RAG) [18], en-\nhancing LLMs by retrieving facts from external knowledge bases is a solution\nfor these issues. It reduces the need for continuous retraining and fine-tuning\nof new data, resulting in better costs of maintaining high-quality Q&A perfor-\nmance. Additionally, RAG can be used together with fine-tuning approaches to\nmaximize the benefits of both approaches.\n3\nError Analysis\nTo gain a clearer insight into the limitations of LLMs in a zero-shot scenario,\nwe randomly selected a subset of 16 databases and 959 NL questions in the\nSpider [11] training dataset. The basic prompt was utilized in conjunction with\nforeign keys to assist LLMs in formulating accurate responses to NL questions,\nas suggested in [23]. After executing the generated queries, we classified the\nexecution results as follows:\n• Success: The SQL executed successfully compared to the ground-truth\nqueries.\n• Execution Error: An SQL system error occurs when executing the gener-\nated SQL.\n• Empty Table: The SQL result returns an empty result. Usually, it happens\nwhen there are no matching values in the WHERE clause.\n• Undesired Result: The SQL executed well but did not match the user’s\nexpectation compared to the ground-truth queries.\nBy double-checking the execution results and the generated queries manually,\nwe excluded 14 questions that the natural language question did not match the\nground-truth query. For example, it is hard to consider selecting rID from the\ntable in the question ‘Find the title and star rating of the movie that got the\nleast rating star for each reviewer.’ The SELECT clause in the ground-truth\nquery was SELECT T2.title, T1.rID, T1.stars, min(T1.stars).\nTo further study the execution results, our team manually classified them into\nsix categories, similar to [15] but with some modification, as shown in Fig. 2. One\nof the different categories is ‘Other:Not Enough Value Information’. It has been\ndiscovered that many SQL queries lack value information, resulting in an empty\ntable. E.g., in the question ‘What are all the different food allergies?’ and the\nground-truth query of WHERE clause is WHERE allergytype = \"food\". LLM\ngenerates \"Food\" instead of \"food\".\nSince LLMs are trained on extensive data and code for general purposes\nbut not trained on tasks on structured tabular data, they might struggle with\nknowledge of enterprise-scaled data lakes. They can craft queries from basic\n"}, {"page": 5, "text": "Prompt Tuning for NL2SQL with Embedding Fine-Tuning and RAG\n5\nGroup by\n7.8%\nOther\n21%\nNested\n21%\nWrong Cols \n4.8%\nAlias 4%\nWrong Cols \n7.6%\nWrong Keyword \n7.6%\nWrong Tables/Cols \n9.6%\nWrong Sub Query \n11.2%\nSet Operation \n9.8%\nNot Enough \nValue Information \n12.3%\nDESC \n3.8%\nDISTINCT \n4.9%\nCond \n17%\nWrong Cols \n10.5%\nSchema-Linking\n27.5%\nInvalid \n5.5%\nJoin\n17.2%\nNot Detected \n3%\nFig. 2. Statistics of error classification\ntable descriptions containing only the table, column names, and foreign keys\nbut often miss crucial value information, leading to inaccuracies. This guides\nus in developing an idea to construct a knowledge base for error correction and\nutilizing RAG to solve the problem.\nWhen categorizing, we note reasons for LLM-generated failures and suggest\ninstructions. Ambiguous SQL is labeled as a failure against the ground-truth\nstandard. For example, the question ‘Which major has the most number of\nstudents?’ yields two similar but different SQL queries: ground-truth (SELECT\nmajor) and generated query (SELECT Major, COUNT(*) AS StudentCount).\nAlthough both are technically correct, we set the ground-truth query as the\ndesired answer to the NL question to resolve these ambiguities.\nAll the categories are described in Table 1. Detailed descriptions of each\ncategory are in the supplementary material.\n4\nMethodology\nError Correction through Prompt Tuning (ECPT) is a three-step\napproach for fixing SQL query errors. It starts with the LLM identifying error\ntypes using a Diagnosis Prompt. Relevant cases are then fetched from a vector\ndatabase. The LLM explains the error and generates instructions on correcting\nit via the Prescription Prompt. Finally, the Treatment Prompt generates the\ncorrected SQL with instructions. It is similar to how doctors make a diagnosis\nand try to treat a patient. Given error information, we decompose the error\ncorrection process into three steps as shown in Fig. 1: (1) Diagnose, (2) Write\nPrescription, and (3) Apply Treatment.\n"}, {"page": 6, "text": "6\nJisoo et al.\n4.1\nTerminology Definitions\nThe term case refers to the outcome of zero-shot SQL generation and ex-\necution, encompassing a table description (including table, column names, and\nforeign keys), a natural language question, generated SQL, and its execution\nresult. This information, formatted as structured text, aids in prompt creation\nor vector conversion for similarity searches in a vector database.\nTable 1. Error Types\nError ID\nError Name\nShort Explanations\ne1\nOther:DISTINCT\nDidn’t use or use keyword DISTINCT properly.\ne2\nOther:DESC\nDidn’t use or use keyword DESC properly.\ne3\nOther:Not Enough Value Information\nWrong value in the WHERE clause.\ne4\nSchema-Linking:Wrong Cols\nUnnecessary or wrong columns in SELECT clause refer to question.\ne5\nSchema-Linking:Cond\nMissing or used wrong logic in the conditions.\ne6\nNested:Wrong Sub Query\nUnnecessary or wrong sub query.\ne7\nNested:Set Operation\nDidn’t used set operation.\ne8\nJoin:Wrong Tables/Cols\nJoined unnecessary or wrong tables or columns.\ne9\nJoin:Wrong Keyword\nDidn’t use JOIN keyword where it should be used or misuse LEFT/RIGHT JOIN.\ne10\nInvalid:Wrong Cols\nUse columns that do not exist in the table.\ne11\nInvalid:Alias\nUsed same column name in a single statement without any alias.\ne12\nGroup-by:Not Detected\nDidn’t use GROUP BY keyword where it should be used.\ne13\nGroup-by:Wrong Cols\nGroup by wrong columns or unnecessary group by.\nIn our error analysis (Section 3), we manually identified various Error Types\nand outlined them in Table 1, each comprising multiple correction cases. These\ncases provide details like the error type, correct SQL, reasons for failure, and\ninstructions. Additionally, we transform ’Case’ information from these cases into\nembedding vectors using the Sentence Transformer model [24], storing them in\nFAISS [25] for similarity searches with new error case query vectors.\nStep 1: Diagnosing. LLM performs well in the classification problem with a\ndiagnostic reasoning process [26]. We developed a Diagnosis Prompt for LLMs\nto classify error types. When an error arises, case details and an error types table\nare input as text to the prompt (indicated by red and yellow lines in Fig. 3). The\nLLM outputs one or more error types, ranked in descending order of severity,\nhighlighting the most critical error first.\nStep 2: Writing Prescription. In our experiment (Section 5), using the self-\ngeneric correction prompt from [15], we found that LLMs can’t correct SQL\nerrors due to limited knowledge. To address this, we introduced a Prescription\nPrompt for LLMs. This prompt generates reasoning and instructions for the\nnew error case. It starts by converting the error case into structured text and\nusing the Sentence Transformer model to embed it as a vector. Then, it finds\nthe most relevant cases from the vector database via similarity searching and\ntransforms them into text for the Prescription Prompt, as shown in the blue\nblocks of Fig. 3.\nStep 3: Applying Treatment. Given the instructions on fixing the generated\nSQL, the final step is to serve it as input to the Treatment Prompt for LLM\n"}, {"page": 7, "text": "Prompt Tuning for NL2SQL with Embedding Fine-Tuning and RAG\n7\nDiagnosis Prompt\n…(omit the format instructions)… \n1. Table description(`table_desc`: str): the description of the table with format(Table [table \nname]: columns=[*, column names]\\n…\\nForeign Keys=[ ... ]).\n2. Question(`question`: str): a natural language query about the table.\n3. Generated SQL(`generated_sql`: str): the generated SQL by the model.\n4. Execution result(`exec_res`: str): the execution result of the generated SQL, there are \nfour types of errors (1) Execution Error: SQLite Error when executing the SQL (2) Empty \nTable: The SQL result returns empty table(or empty list) (3) Undesired Result: Executed well \nbut doesn't match user's expectation (4) Success: the SQL executed well.\nIf the `exec_res` is not 'Success', you need to diagnose the error id(or multiple error ids) with \nthe given information above as much as you can. If there exists multiple error ids, you MUST \nrank the most possible error id in descending order.\nElse, you need to output 'e0' for `error_ids`.\nHere is the new error case:\n```\n\"table_desc\": \"\"\"{table_desc}\"\"\"\n\"question\": \"{question}\"\n\"generated_sql\": \"\"\"{generated_sql}\"\"\"\n\"exec_res\": \"{exec_res}\"\n\"error_ids\": #fillin\n```\nHere is the error types refer to the new error case, empty in the column 'case_example' \nmeans there is no existing example avaliable in our database, however it doesn't mean it \nhas the error.\n{error_table}\nBegin to diagnose with no explanation! \n…(omit the format instructions)… \nPrescription Prompt\n…(omit the format instructions)… \n…(omit the context: 1. Table description ~ 4. Execution result)… \n5. Error Name(`error_names`: list[str]): the error names of the generated SQL.\n6. Reason(`reason`: str): the reason why the generated SQL has the error.\n7. Instruction(`instruction`: list[str]): the instruction to generate a correct SQL. It consists of \nthe smallest unit of modification. Each amendment shall describe the modification of the \ncontent of 'generated_sql'.\nWe have several error cases that have the same error name and execution result as the \ncurrent error case. Please refer to the following error cases and write down the reasons and \ninstructions for the current error case.\n{prescription_examples}\n…(omit the format instructions)… \nBegin! Remind to keep format instructions.\nNew error case:\n```\n\"table_desc\": \"\"\"{table_desc}\"\"\"\n\"question\": \"{question}\"\n\"generated_sql\": \"\"\"{generated_sql}\"\"\"\n\"exec_res\": \"{exec_res}\"\n\"error_names\": {error_names}\n\"reason\": #fillin ,\n\"instruction\": #fillin\n```\nFig. 3. Diagnosis Prompt and Prescription Prompt are designed to diagnose and write\nprescriptions. Red blocks refer to the new error case information, yellow blocks are\nerror type information, blue blocks are relevant case examples, and green blocks are\nanswers that LLM should fill in.\nto generate the correct SQL. The system will attempt this up to three times\nuntil a ‘Success’ is achieved in the execution result.\n4.2\nEmbedding Fine-Tuning\nSince LLMs are trained on vast and diverse data sources, they can understand\ndiverse contexts. Intuitively, words with similar meanings are closer to each other\nin the embedding space, and those that are not are further apart. However, in\nour case, embedding vectors for correction cases should be organized by case\ntypes rather than token meanings. Thus, we fine-tuned a Sentence Transformer\nmodel with MPNet [27] base architecture using a dataset of correction cases\nlabeled with 14 labels (13 error types and one success). We employed triplet loss\n[28] for 20 epochs to enhance relevance in case retrieval.\n5\nExperiments and Results\n5.1\nEvaluations, Models, and Metrics\nOur evaluation was conducted on the Spider [11] development set, where\nground-truth queries can be easily accessible. We used OpenAI’s models(GPT3.5-\nturbo, GPT4-turbo) with 0.01 temperature and a different number of max to-\nkens(100, 1,024, and 600 for each step in ECPT). The performance was measured\nby execution accuracy in most of the experiments. To assess the effectiveness of\n"}, {"page": 8, "text": "8\nJisoo et al.\nthe diagnosis, we utilize a straightforward hit rate metric: the number of trials\nthat succeed in fixing errors divided by the number of total trials.\n5.2\nEmbedding Fine-Tuning\n100\n50\n0\n50\n100\nx1\n100\n50\n0\n50\n100\nx2\nw/o Fine-tuning\n100\n50\n0\n50\n100\nx1\n100\n50\n0\n50\n100\nx2\nw/ Fine-tuning (10% Cases)\n100\n50\n0\n50\n100\nx1\n100\n50\n0\n50\n100\nx2\nw/ Fine-tuning (50% Cases)\n100\n50\n0\n50\n100\nx1\n100\n50\n0\n50\n100\nx2\nw/ Fine-tuning (100% Cases)\nCorrection Cases(n=945) Embedding Space\nFig. 4. Embedding vector spaces of correction cases visualized via T-SNE[29]. Each\npoint represents an embedded vector for each correction case. Success cases are colored\ngreen, and other colors are failed cases.\nOptimizing tasks with RAG (Retriever-Augmented Generation) involves com-\nplex challenges, particularly in providing relevant examples and augmentations.\nWe aim to enhance the semantic understanding of success and error types by\nrefining the embedded vectors of correction cases, as illustrated in our results\n(Fig. 4). We evaluated the impact of fine-tuning on RAG by testing various sizes\nof correction cases, ensuring an equal distribution of each error type. This process\nrevealed that while additional correction cases help distinctly separate success\nand error values in the embedding space, error cases remain closely clustered\ndue to their multiple types.\n5.3\nResults on Execution Accuracy\nFig. 5 shows that using different models and case numbers for RAG sig-\nnificantly improves execution accuracy on the development set. Switching to\nGPT4-turbo and incorporating an embedding fine-tuned model yielded about a\n12% gain compared to the baseline, the GPT3.5-turbo model with a generic self-\ncorrection prompt and no RAG. Both GPT3.5-turbo and GPT4-turbo models\nsaw improvements with RAG from 76.07% to 78.78% and from 83.04%(GPT4-\nturbo in Table 2) to 84.88%, respectively. Also, further benefits were gained from\nthe embedding fine-tuned model (increasing by about 1% and 3%). Interestingly,\nthe size of the correction cases had minimal impact, suggesting that correcting\nSQL errors might require fewer examples. This aspect needs more exploration\nin the future.\n"}, {"page": 9, "text": "Prompt Tuning for NL2SQL with Embedding Fine-Tuning and RAG\n9\nGeneric\nSelf-Correction\n(W/O RAG)\nGPT3.5t\n(100%)\nGPT3.5t-EFT\n(10%)\nGPT3.5t-EFT\n(50%)\nGPT3.5t-EFT\n(100%)\nGPT4t\n(100%)\nGPT4t-EFT\n(10%)\nGPT4t-EFT\n(50%)\nGPT4t-EFT\n(100%)\nModels(% of Correction Cases(n=264))\n65\n70\n75\n80\n85\n90\n95\n100\nSpider Dev-set Accuracy(%)\n76.07\n78.78\n79.84\n78.97\n79.46\n84.88\n87.69\n87.31\n88.08\nSpider Dev-set Accuracy with different Models\nGPT3.5t\nGPT4t\n \nW/O RAG\nW/O Embedding Fine-Tuning\nW/ Embedding Fine-Tuning\nFig. 5. Experiments results. Percentage values in the x-axis mean accuracies of different\nconfigurations with LLM models.\nTable 2. Spider Dev-Set Performance Summary. Correction accuracy is determined by\ndividing successful fixes by 247 error cases. Execution accuracy is obtained by dividing\nthe total of fixed cases and zero-shot prompting successes by 1,032 cases.\nPrompt Type\nModel\nOption A\nFine-Tuned\nEmbeddings\nOption B\nProvide Example\nin Diagnosis\nOption C\nResolve all\nat once\nCorrection\nAccuracy\n(247 cases)\nSpider Dev-set\nExecution Accuracy\n(1032 cases)\n-\nGPT4\n-\n-\n-\n-\n77.33%\n-\nGPT4-turbo\n-\n-\n-\n-\n83.04%\nGeneric\nGPT3.5-turbo\nFalse\nFalse\nFalse\n0.00%\n76.07%\nECPT\nGPT3.5-turbo\nFalse\nFalse\nFalse\n11.34%\n78.78%\nECPT\nGPT3.5-turbo\nTrue\nFalse\nFalse\n14.17%\n79.46%\nECPT\nGPT3.5-turbo\nTrue\nTrue\nFalse\n14.57%\n79.55%\nECPT\nGPT4-turbo\nFalse\nFalse\nFalse\n36.84%\n84.88%\nECPT\nGPT4-turbo\nTrue\nFalse\nFalse\n50.20%\n88.08%\nECPT\nGPT4-turbo\nTrue\nTrue\nFalse\n44.13%\n86.63%\nECPT\nGPT4-turbo\nTrue\nFalse\nTrue\n48.18%\n87.60%\nECPT\nGPT4-turbo\nTrue\nTrue\nTrue\n50.61%\n88.18%\nTable 2 presents the execution accuracy in an ablation study. Zero-shot NL-\nto-SQL results are shown in the first two rows. Option A uses a fine-tuned\nembedding model, while Option B employs a diagnostic prompt with a new er-\nror case alongside each error type. Option C selects only the top error types\npost-diagnosis. Embedding fine-tuning improved error correction, but other op-\ntions didn’t enhance performance. Compared with other works as a reference,\nDin-SQL [15] achieved 74.2% execution accuracy on the dev set of Spider. Our\napproach emphasizes error correction and needs feedback based on ground-truth\nSQL queries, especially for execution result: “Undesired Result.”\n"}, {"page": 10, "text": "10\nJisoo et al.\n5.4\nHit Rate and Cost Usage\nTable 3 reports the hit rate, token usage, and costs while using API, a novel\nanalysis not previously undertaken. We investigate the cost-effectiveness of accu-\nracy improvements in three key experiments. The accuracy gain per dollar spent\ncompared to the baseline (76.07%) is 0.52%, 0.59%, and 0.38% for each exper-\niment. Notably, despite its higher hit rate, the third experiment incurs higher\ncosts due to excessive token usage in input prompts, approximately 1.6 times\nmore than the GPT4t-EFT experiment.\nTable 3. Hit Rate and Cost Usage. Prompt tokens are the number of tokens used for\ninput prompts. Completion tokens are the number of tokens that LLM generates.\nExperiments\nPrompt Tokens\nCompletion Tokens\nTotal Cost($)\nHit Rate\n# of Trials\nExecution Accuracy\nGPT3.5t-EFT\n2,020,555\n129,455\n6.58\n5.09%\n687\n79.46%\nGPT4t-EFT\n1,665,553\n119,112\n20.23\n23.01%\n539\n88.08%\nGPT4t-EFT w/ options B, C\n2,808,228\n120,913\n31.71\n23.81%\n525\n88.18%\n6\nConclusion\nThis paper proposed a novel approach to NL-to-SQL, addressing a critical gap\nin current systems. By integrating error correction with prompt tuning, embed-\nding fine-tuning, and RAG, we addressed the crucial need for accurate translation\nof natural language questions into SQL expressions. Our method drew inspira-\ntion from medical diagnosis processes and went beyond simple query translation;\nit intelligently diagnoses and corrects errors, leveraging external knowledge bases\nto refine its outputs. The notable 12% accuracy improvement over existing base-\nlines underscores the effectiveness of our framework, marking a substantial ad-\nvancement in data access and management. This breakthrough has far-reaching\nimplications, offering a powerful tool for diverse users, particularly in decision-\nmaking roles, and sets a new benchmark for future research and development in\nNL-to-SQL.\nWe see much room for improvement in our framework. First, since LLM-\ngenerated queries need to be verified with ground-truth ones, we can integrate\nHuman-in-the-loop approaches to address this challenge. Second, manual error\ntype initialization can be automatic by utilizing LLM agents. Finally, RAG-based\nprompt tuning remains resource-intensive due to the nature of the decomposed\nerror correction process.\n"}, {"page": 11, "text": "Prompt Tuning for NL2SQL with Embedding Fine-Tuning and RAG\n11\nReferences\n1. I. Androutsopoulos, G. D. Ritchie, and P. Thanisch. Natural language interfaces\nto databases - an introduction, 1995.\n2. William Woods. The lunar sciences natural language information system. BBN\nreport, 1972.\n3. Diptikalyan Saha, Avrilia Floratou, Karthik Sankaranarayanan, Umar Farooq Min-\nhas, Ashish R Mittal, and Fatma Özcan. Athena: an ontology-driven system for\nnatural language querying over relational data stores. Proceedings of the VLDB\nEndowment, 9(12):1209–1220, 2016.\n4. Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehen-\nsive exploration on wikisql with table-aware word contextualization. arXiv preprint\narXiv:1902.01069, 2019.\n5. Qin Lyu, Kaushik Chakrabarti, Shobhit Hathi, Souvik Kundu, Jianwen Zhang,\nand Zheng Chen.\nHybrid ranking network for text-to-sql.\narXiv preprint\narXiv:2008.04759, 2020.\n6. Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew\nRichardson. Rat-sql: Relation-aware schema encoding and linking for text-to-sql\nparsers. arXiv preprint arXiv:1911.04942, 2019.\n7. Xi Victoria Lin, Richard Socher, and Caiming Xiong.\nBridging textual and\ntabular data for cross-domain text-to-sql semantic parsing.\narXiv preprint\narXiv:2012.12627, 2020.\n8. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. Language models are unsupervised multitask learners. OpenAI\nblog, 1(8):9, 2019.\n9. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al. Language models are few-shot learners. Advances in neural information pro-\ncessing systems, 33:1877–1901, 2020.\n10. Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. Evaluating the text-\nto-sql capabilities of large language models. arXiv preprint arXiv:2204.00498, 2022.\n11. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James\nMa, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-\nlabeled dataset for complex and cross-domain semantic parsing and text-to-sql\ntask. arXiv preprint arXiv:1809.08887, 2018.\n12. Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema\nlinking and skeleton parsing for text-to-sql. In AAAI, 2023.\n13. Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. Picard: Parsing in-\ncrementally for constrained auto-regressive decoding from language models. arXiv\npreprint arXiv:2109.05093, 2021.\n14. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Gra-\nham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting\nmethods in natural language processing. arXiv preprint arXiv:2107.13586, 2021.\n15. Mohammadreza Pourreza and Davood Rafiei.\nDin-sql: Decomposed in-context\nlearning of text-to-sql with self-correction. arXiv preprint arXiv:2304.11015, 2023.\n16. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,\nEd Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning\nin large language models. arXiv preprint arXiv:2201.11903, 2022.\n17. Yisheng Song, Ting Wang, SK Mondal, and JP Sahoo. A comprehensive survey of\nfew-shot learning: Evolution, applications, challenges, and opportunities (2022).\n"}, {"page": 12, "text": "12\nJisoo et al.\n18. Patrick\nLewis,\nEthan\nPerez,\nAleksandra\nPiktus,\nFabio\nPetroni,\nVladimir\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.\narXiv preprint arXiv:2005.11401, 2020.\n19. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022.\n20. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023.\n21. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Train-\ning language models to follow instructions with human feedback.\nAdvances in\nNeural Information Processing Systems, 35:27730–27744, 2022.\n22. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Se-\nbastian\nGehrmann,\nPrabhanjan\nKambadur,\nDavid\nRosenberg,\nand\nGideon\nMann.\nBloomberggpt: A large language model for finance.\narXiv preprint\narXiv:2303.17564, 2023.\n23. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and\nJingren Zhou.\nText-to-sql empowered by large language models: A benchmark\nevaluation. arXiv preprint arXiv:2308.15363, 2023.\n24. Nils Reimers and Iryna Gurevych.\nSentence-bert: Sentence embeddings using\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\n25. Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with\ngpus. IEEE Transactions on Big Data, 7(3):535–547, 2019.\n26. Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and\nGuoyin Wang.\nText classification via large language models.\narXiv preprint\narXiv:2305.08377, 2023.\n27. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked\nand permuted pre-training for language understanding. Advances in Neural Infor-\nmation Processing Systems, 33:16857–16867, 2020.\n28. Elad Hoffer and Nir Ailon.\nDeep metric learning using triplet network.\narXiv\npreprint arXiv:1412.6622, 2014.\n29. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Jour-\nnal of machine learning research, 9(11), 2008.\n"}]}