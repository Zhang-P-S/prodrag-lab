{"doc_id": "arxiv:2512.02024", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.02024.pdf", "meta": {"doc_id": "arxiv:2512.02024", "source": "arxiv", "arxiv_id": "2512.02024", "title": "Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review", "authors": ["Yan Yang", "Mouxiao Bian", "Peiling Li", "Bingjian Wen", "Ruiyao Chen", "Kangkun Mao", "Xiaojun Ye", "Tianbin Li", "Pengcheng Chen", "Bing Han", "Jie Xu", "Kaifeng Qiu", "Junyan Wu"], "published": "2025-11-17T08:36:53Z", "updated": "2025-11-17T08:36:53Z", "summary": "The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.02024v1", "url_pdf": "https://arxiv.org/pdf/2512.02024.pdf", "meta_path": "data/raw/arxiv/meta/2512.02024.json", "sha256": "0d4806407df945cad43a6be4989fd61526d02acb36eef629c0e7920d5d0e8d0e", "status": "ok", "fetched_at": "2026-02-18T02:26:52.295325+00:00"}, "pages": [{"page": 1, "text": "HUMAN-LEVEL AND BEYOND: BENCHMARKING LARGE\nLANGUAGE MODELS AGAINST CLINICAL PHARMACISTS IN\nPRESCRIPTION REVIEW\nYan Yang1,†, Mouxiao Bian2,†, Peiling Li1, Bingjian Wen1, Ruiyao Chen2, Kangkun Mao2, Xiaojun Ye1, Tianbin Li2,\nPengcheng Chen2,3, Bing Han2, Jie Xu2,*, Kaifeng Qiu1,*, and Junyan Wu1,*\n1 SUN YAT-SEN MEMORIAL HOSPITAL , Guangdong, China\n2 Shanghai Artificial Intelligence Laboratory, Shanghai, China\n3 University of Washington, Washington, USA\nABSTRACT\nThe rapid advancement of large language models (LLMs) has accelerated their integration into\nclinical decision support, particularly in prescription review. To enable systematic and fine-grained\nevaluation, we developed RxBench, a comprehensive benchmark that covers common prescription\nreview categories and consolidates 14 frequent types of prescription errors drawn from authoritative\npharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879\nshort-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-\nof-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-\n2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier,\noutperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists\nindicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore,\nbuilding on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier\nmodel, resulting in a specialized model that rivals leading general-purpose LLMs in performance on\nshort-answer question tasks. The main contribution of RxBench lies in establishing a standardized,\nerror-type–oriented framework that not only reveals the capabilities and limitations of frontier LLMs\nin prescription review but also provides a foundational resource for building more reliable and\nspecialized clinical tools.\nKeywords Benchmark · Prescription Review · Large Language Model · Clinical Pharmacists · Evaluation\n1\nIntroduction\nMedication errors remain a leading cause of patient harm and healthcare costs. In the United States alone, 7,000-9,000\ndeaths and over $40 billion in annual costs are attributed to preventable medication errorsNaseralallah et al. [2025a]. The\nprescribing stage is particularly error-prone. It’s reported that over 75% of medication errors occur during prescribing\nor administrationPais et al. [2024]. A study from a pediatric hospital found that clinical pharmacist review detected\nat least one prescribing error in 81% of discharge prescriptionsChristiansen et al. [2008]. Thus, rigorous review of\nprescriptions is critical.\nPrescription checking by clinical pharmacists has been shown to improve medication safety and rationality, reduce\ninappropriate drug use and wasteFan et al. [2023]Naseralallah et al. [2025b]Skains et al. [2025], and lower rates of\nhospital readmissionsCostello et al. [2025]Ravn-Nielsen et al. [2018]. However, the availability of trained pharmacists\noften lags behind growing clinical demand, especially with the increasing burden of multimorbidity and polypharmacy.\nIn addition, pharmacist-led reviews are time-consuming, highly dependent on individual expertiseCheng et al. [2020],\n1†These authors contributed equally.\n2*Correspondence: Junyan Wu(wujunyan@mail.sysu.edu.cn), Kaifeng Qiu(feng.qk@163.com), Jie Xu (xujie@pjlab.org.cn)\narXiv:2512.02024v1  [cs.CL]  17 Nov 2025\n"}, {"page": 2, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nFigure 1: RxBench: End-to-End Pipeline for Prescription Review Benchmarking\nand difficult to scale across diverse clinical settings. These constraints contribute to variability in review quality\nand leave gaps in ensuring medication safety. Collectively, these challenges highlight the urgent need for innovative\nsolutions that can enhance both the efficiency and accuracy of prescription review.\nLLMs have opened new opportunities for prescription review. With their advanced natural language understanding\nand multi-step reasoning capabilities, LLMs can analyze clinical records and prescriptions while considering the\ncomplex relationships among drugs, diseases, and patient-specific characteristics. Recent studies have started to explore\ntheir potential in clinical pharmacy tasks. For instance, Huang et al.Huang et al. [2024] compared GPT-4 (ChatGPT)\nwith licensed pharmacists on pharmacy practice questions, and found that while ChatGPT performed comparably in\n“medication consultation” (mean scores 8.77 vs. 9.50), it lagged significantly in “prescription review” (5.23 vs. 9.90,\np = 0.0089). In another study, a retrieval-augmented generation (RAG)-based clinical decision support system was\ndeveloped to detect prescription errors, using 23 complex cases comprising 61 erroneous prescription scenarios. Its co-\npilot mode achieved 54.1% accuracy in the taskOng et al. [2024]. Stansfield et al. Bull and Okaygoun [2024]evaluated\nGPT-4 on the UK Prescribing Safety Assessment and reported an overall accuracy of 79.7% (153/192) in the prescription\nreview module. Similarly, Li et al. Li et al. [2025]compared eight generative AI systems across key clinical pharmacy\ntasks, including prescription review, using 48 real-world problems covering ten categories of prescription errors.\nDespite these promising findings, most existing studies treat prescription review merely as a subset of general medical\nquestion answering. Evaluations often rely on small-scale, non-specialized datasets, with limited coverage of critical\nerror categories such as off-label use, Inappropriate diluent selection, and skin test requirement labeling. Moreover,\ncurrent evaluation metrics mostly emphasize binary correctness. There is no standardized benchmark dedicated to\nsystematically assessing LLM performance in prescription review. This absence hinders fair model comparison,\nobscures the identification of model-specific error patterns.\nTo address this gap, this work propose RxBench (Figure 1), a comprehensive benchmark specifically designed to\nevaluate LLMs in prescription review. Our work makes three main contributions: (1) A pharmacist-verified dataset was\nconstructed, comprising diverse prescription cases, including antineoplastic drugs, gynecological endocrine diseases\ndrugs, high-alert medications, high-risk, allergenic drugs, immunosuppressive agents, intravenous medications, off-label\ndrug use, traditional chinese medicine and pharmaceutical compounding; (2) we conducted baseline evaluations of\nseveral state-of-the-art LLMs, providing new insights into their capabilities and limitations; and (3) These evaluation\nresults directly informed the development of a fine-tuned model for prescription review, which demonstrated enhanced\nperformance in detecting complex medication errors and improves practical applicability. It is believed that RxBench will\nserve as a foundational resource for advancing research on LLMs in clinical pharmacy and supporting their safe\nintegration into real-world prescription review workflows.\n2\n"}, {"page": 3, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\n2\nMethods\n2.1\nCompeting Models\nEighteen representative large language models released from November 2024 to May 2025 were evaluated, spanning\nthe OpenAI, Qwen, Gemini, Claude, and LLaMA families. These models capture a broad spectrum of recent advances\n(Table 1).\nTable 1: Introduction of Large Language Models for Evaluation\nModel Name\nParameters Open-Source Organization\nRelease Date Model Type\nBaichuan2-13B-chat\n13B\nYes\nBaichuan AI\nDec 2024\nText\nDeepseek-R1-0528\n671B\nYes\nDeepSeek\nMay 2025\nText\nDeepSeek-V3\n671B\nYes\nDeepSeek\nDec 2024\nText\nGemma-3-27B\n27B\nYes\nGoogle\nApr 2025\nMultimodal\nLlama-4-maverick\n400B\nYes\nMeta\nMar 2025\nMultimodal\nMistral-small-3.1-24B-instruct 24B\nYes\nMistral AI\nMar 2025\nMultimodal\nQwen2.5-72B-Instruct\n72B\nYes\nAlibaba\nNov 2024\nText\nQwen3-14B\n14B\nYes\nAlibaba\nApr 2025\nText\nQwen3-32B\n32B\nYes\nAlibaba\nApr 2025\nText\nQwen3-235B-A22B\n235B\nYes\nAlibaba\nApr 2025\nText\nClaude-3-7-sonnet-20250219\nN/A\nNo\nAnthropic\nFeb 2025\nMultimodal\nClaude-sonnet-4-20250514\nN/A\nNo\nAnthropic\nMay 2025\nMultimodal\nGemini-2.0-flash\nN/A\nNo\nGoogle DeepMind\nDec 2024\nMultimodal\nGemini-2.5-flash\nN/A\nNo\nGoogle DeepMind\nMay 2025\nMultimodal\nGPT-4o-2024-11-20\n200B\nNo\nOpenAI\nNov 2024\nMultimodal\nGPT-4.1-2025-04-14\nN/A\nNo\nOpenAI\nApr 2025\nMultimodal\nGrok-4\n100–175B\nNo\nxAI\nMar 2025\nMultimodal\no4-mini-2025-04-16\nN/A\nNo\nOpenAI\nApr 2025\nMultimodal\n2.2\nClinical Pharmacists Test\nTo compare model performance with clinical pharmacists across professional levels, we constructed a standardized\n100-item test from RxBench, comprising 51 single-choice, 10 multiple-choice, and 39 short-answer questions. Twenty-\nseven pharmacists from the pharmacy departments of three tertiary hospitals were recruited: 18 pharmacists, 6 senior\npharmacists, and 3 principle pharmacists. The assessment was conducted as a two-hour, closed-book, double-blind\nexamination, with all participants completing the test independently. This design ensured rigor and direct comparability\nbetween human and model performance.\n2.3\nPrescription Error Type\nA standardized assessment framework was developed by reviewing national and international guidelines, clinical\npractice standards, and regulatory documents, supplemented by practical considerations from clinical care. Fourteen\ncommon categories of prescription errors were identified (Table 2).\n3\n"}, {"page": 4, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nTable 2: Classification and Description of Prescription Errors\nError Type\nDescription\nInappropriate Dosing Regi-\nmen\nThe prescribed dose, frequency, or duration of therapy deviates from estab-\nlished guidelines, potentially compromising therapeutic efficacy, increasing\nthe risk of adverse drug events (ADEs), or leading to other negative clinical\noutcomes. Examples include exceeding single or daily dose limits, inap-\npropriate dosing intervals, failure to transition from a loading dose to a\nmaintenance dose, and suboptimal or excessive duration of treatment.\nUnsuitable Patient Population\nThe medication is contraindicated or requires caution in the patient’s specific\ndemographic or physiological state, such as in pediatric, geriatric, pregnant,\nor lactating populations, or in patients with significant hepatic or renal\nimpairment.\nUnwarranted Indication\nThe therapeutic indication for the prescribed medication is inconsistent with\nthe patient’s clinical diagnosis, treatment goals, or disease state.\nInappropriate Dosage Form or\nRoute of Administration\nThe selected dosage form or route of administration is not aligned with the\nproduct’s labeling, clinical practice guidelines, or the patient’s condition,\nwhich may lead to diminished efficacy or adverse reactions. Examples\ninclude prescribing solid oral dosage forms to patients with dysphagia\nor selecting an intravenous route for a drug intended for intramuscular\nadministration.\nIncompatibility or Clinically\nSignificant Interaction\nCo-administration of two or more drugs results in physical or chemical\nincompatibility (e.g., precipitation, degradation) or a clinically significant\ndrug-drug interaction that alters the pharmacokinetics or pharmacodynamics\nof one or more agents, leading to reduced efficacy or increased toxicity.\nInappropriate Solvent or Vehi-\ncle\nThe selection or volume of the solvent or vehicle used for drug reconstitution\nor dilution does not adhere to the manufacturer’s instructions or established\npharmaceutical standards, posing a risk of altered drug stability, precipita-\ntion, or adverse reactions.\nTherapeutic Duplication\nThe concurrent, unjustifiable prescribing of two or more medications with\nidentical or overlapping pharmacological mechanisms of action. For in-\nstance, prescribing both acetaminophen and a combination cold product\ncontaining acetaminophen.\nInappropriate Timing of Ad-\nministration\nThe scheduling of drug administration is suboptimal, potentially affecting\nbioavailability, efficacy, or tolerability, or fails to adhere to critical timing\nrequirements. Examples include administering a gastrointestinal irritant on\nan empty stomach or failing to administer surgical antibiotic prophylaxis\nwithin the recommended 0.5-2 hour pre-incisional window.\nContraindicated Drug Use\nThe patient’s clinical condition is a contraindication to the drug as listed in\nthe prescribing information. For example, prescribing aspirin to a patient\nwith an active peptic ulcer.\nOmission of Required Allergy\nTest Documentation\nFor medications necessitating a preliminary allergy or skin test, the medical\nrecord lacks documentation confirming that the test was performed and its\noutcome.\nPrescribing to a Patient with a\nKnown Allergy\nA medication is prescribed to a patient with a documented history of allergy\nto the drug itself or to a structurally related compound with a known risk of\ncross-reactivity.\nSuboptimal Drug Selection\nWhile the drug is indicated for the patient’s diagnosis, a more appropriate\nagent exists based on patient-specific factors (e.g., comorbidities, concomi-\ntant medications, genetic profile), and the current choice may result in\ninferior outcomes or higher risk.\n4\n"}, {"page": 5, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nError Type\nDescription\nPrescription\nNon-\nconformance\nThe prescription fails to meet formal requirements as stipulated by regula-\ntory standards (e.g., \"Management Specification for Hospital Prescription\nReview\"). This includes, but is not limited to: 1. Omissions, non-standard\nformatting, or illegibility in prescription components; 2.Non-compliant or\ninconsistent physician signature/seal; 3.Absence of a pharmacist’s appro-\npriateness review; 4. Omission of age in days/months for neonatal/infant\nprescriptions; 5.Failure to issue separate prescriptions for different drug\ncategories; 6.Use of non-proprietary or non-standard drug names; 7.Am-\nbiguous or non-standard notation for dose, strength, quantity, or units; 8.Use\nof vague instructions such as \"as directed\" or \"for personal use.\"\nOff-label Use\nThe use of a drug outside the scope of its marketing authorization as ap-\nproved by the national regulatory authority. This encompasses use for an\nunapproved indication, patient population, dosage, route of administration,\nfrequency, or duration of therapy.\n2.4\nLoRA Fine-tuning Method\nTo adapt the pre-trained language model for the specialized domain of prescription review, we employ the Low-Rank\nAdaptation (LoRA) method for parameter-efficient fine-tuning. LoRA introduces low-rank decomposed adapter modules\nwhile keeping the pre-trained weights frozen, enabling targeted adjustment of model behavior.\n2.4.1\nModel Configuration\nWe implement LoRA fine-tuning within the Megatron-LM framework with the following specifications:\n• Rank Parameters: LoRA rank is set to 32 with a scaling factor α of 64, ensuring a balance between parameter\nefficiency and representational capacity.\n• Target Modules: LoRA adapters are injected into all linear layers (all-linear), including key transformation\nmatrices in attention mechanisms and feed-forward networks.\n• Training Strategy: We employ a gradient accumulation strategy with global batch size of 16 and micro batch\nsize of 8, combined with sequence parallelism and Flash Attention backend to optimize memory usage.\n2.4.2\nTraining Parameters\nThe model is trained for 10 epochs using a constant learning rate of 10−4 with 5% warmup proportion. To prevent\noverfitting, the learning rate linearly decays to a minimum of 10−5. We adopt a full recomputation strategy with uniform\ngradient checkpointing per layer, effectively balancing computational overhead and memory consumption.\n2.5\nStatistical Analysis\nAll statistical analyses were conducted using Python. To assess overall performance differences among multiple\nmodels, the Friedman test was employed as a non-parametric alternative to repeated-measures ANOVA. Pairwise\ncomparisons of classification outcomes, particularly in error-type analyses visualized through heatmaps, were evaluated\nusing McNemar’s test. A two-tailed P value of 0.05 was considered the threshold for statistical significance. Unless\notherwise specified, all reported results include exact P values when significant differences were observed.For the\npharmacist groups, results were aggregated and reported as mean scores within each professional rank (pharmacist,\nsenior pharmacist, and principle pharmacist) to ensure comparability with model outputs.\n3\nDataset and Experiment\n3.1\nData Source and Expert Review\nAll test and training items were derived from authoritative clinical pharmacist training textbooks. The scope of prescrip-\ntion types was broad and clinically representative, covering medications used in cardiovascular diseases, gastrointestinal\n5\n"}, {"page": 6, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\ndisorders, otorhinolaryngologic conditions, infectious diseases, chronic diseases in older adults, pregnancy and lacta-\ntion, pain management, pediatrics, renal disorders, respiratory diseases, endocrine and metabolic diseases, as well as\nneurological and psychiatric conditions.\nThe test dataset consisted of both single-choice questions, multiple-choice questions and short-answer questions (Figure\n1), whereas the training dataset included only short-answer questions, totaling 2,547 items. All items, regardless\nof disease system or question format, were mapped to a unified set of error categories as defined in Table 2 and\nsubsequently underwent manual review.\nTo ensure data quality, a review committee was convened, consisting of three senior pharmacists with over five years of\nexperience in prescription review and one principal pharmacist with more than ten years of experience. Inclusion criteria\nrequired completeness of both questions and reference answers, clinical representativeness, educational relevance,\nand reflection of common or high-risk prescription errors. Exclusion criteria included missing information, outdated\nregimens inconsistent with current guidelines, duplicate or highly similar entries, and items limited to rote memorization\nwithout clinical relevance.\nEach item was independently reviewed and cross-checked against reference answers, followed by group discussion to\nresolve discrepancies. This process ensured that the dataset accurately captured the reasoning and judgment required in\nreal-world prescription review, thereby guaranteeing its scientific validity and consistency.\nIn addition, to facilitate automated evaluation, clinical pharmacists extracted key answer points from the reference\nsolutions to generate predefined scoring rubrics.\n3.2\nEvaluation Tasks Design\nTo objectively assess model performance, a zero-shot evaluation paradigm was adopted. For multiple-choice questions,\nprompts were restricted to elicit only the final answer to ensure accuracy in subsequent analysis. For short-answer\nquestions, where established paradigms are lacking, prompts explicitly defined error categories and their descriptions,\nwhile enforcing a standardized output format. Detailed prompt designs for each item type are provided in Table 3.\nTable 3: Prompts of Different Tasks\nCategory\nPrompt\nSingle-choice question\nYou are a clinical pharmacist with extensive expertise. Your task is to answer\nthe following single-choice question based on your knowledge of evidence-\nbased pharmacy. Output only the letter of the most appropriate answer; do\nnot include any other content. Example output: A\nMultiple-choice question\nYou are a clinical pharmacist with extensive expertise. Your task is to\nanswer the following multiple-choice question based on your knowledge of\nevidence-based pharmacy. Output all letters of the correct answers; do not\ninclude any other content. Example output: ABC\n6\n"}, {"page": 7, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nCategory\nPrompt\nShort-answer question\nBackground:\nYou are a clinical pharmacist with extensive expertise. Your task is to\nconduct a systematic review of electronic prescriptions based on evidence-\nbased pharmacy principles to identify, prevent, and resolve potential or\nactual drug-related problems, ensuring patient medication is safe, effective,\nand rational.\nCore Task:\nPlease analyze the provided [Prescription Information], strictly ad-\nhering to the [Problem Type Definitions and Explanations] to\nidentify and classify any issues. Finally, generate a professional and rigorous\nprescription review report according to the specified [Output Format].\n[Problem Type Definitions and Explanations]\nSee Table 2 for error types and their definitions.\n[Output Format] Strictly adhere to this format for your response. Do\nnot output any extraneous content or explanations.\n–If the review identifies problems, use this format:\nProblem Type: [Select the most accurate classification(s) from the list\nabove. Separate multiple types with a semicolon. Example: Inappropriate\nDosing Regimen; Off-label Use]\nIntervention Suggestion: [Briefly describe the specific issues in the pre-\nscription and the proposed interventions.]\n–If the review finds no issues, use this format:\nPrescription without Discrepancies\n3.3\nEvaluation Framework and Procedure\nFor each task, we constructed a complete input consisting of the problem and its prompt: single-choice questions\nincluded the stem and options, while short-answer questions included the patient’s baseline information and prescription\ndetails. These inputs, combined with task-specific prompts, were submitted to each model via API calls to obtain the\noutputs, which were then stored in structured JSON files for subsequent analysis. To ensure stable outputs that accurately\nreflected the models’ intrinsic capabilities, the temperature parameter was fixed at 0, while all other parameters remained\nat their default settings.For the pharmacist cohort, we collected responses from 18 pharmacists, 6 senior pharmacists,\nand 3 principle pharmacists. A total of 25 valid responses were obtained for the single- and multiple-choice tasks, and\n27 valid responses were collected for the short-answer tasks.\n3.4\nData Post-processing\nBecause the prompts enforced a standardized response format, model outputs required minimal post-processing, except\nfor occasional issues such as redundant line breaks. Clinical pharmacists’ responses were collected using Excel\nspreadsheets; we performed light normalization by correcting mixed Chinese–English punctuation and concatenating\ncontent into structured formats. No additional processing was applied beyond these steps.\n3.5\nEvaluation Metrics\nDifferent evaluation methods were adopted for different types of questions, as detailed in Table4 .\n3.5.1\nSingle-choice questions\nPerformance was evaluated using accuracy (Equation 1 ), calculated from true positives (TP), true negatives (TN), false\npositives (FP), and false negatives (FN).\nAccuracy =\nTP + TN\nTP + TN + FP + FN\n(1)\n7\n"}, {"page": 8, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nTable 4: Evaluation Metrics for Different Task Types\nTask Type\nEvaluation Metric(s)\nSingle-choice questions\nAccuracy (based on TP, TN, FP, FN)\nMultiple-choice questions\nF1 score (harmonic mean of Precision and Recall)&Accuracy\n(based on TP, TN, FP, FN)\nShort-answer questions\nF1 score (error-type classification); Macro Recall (coverage of\nintervention points),BERTScore (semantic similarity of interven-\ntion); final weighted score\n3.5.2\nMultiple-choice questions\nPerformance was measured by the F1 score (Equation 2), combining precision and recall and accuracy (Equation 1 ).\nF1 = 2 × Precision × Recall\nPrecision + Recall\n(2)\nWhere:\n- Precision represents the proportion of samples predicted as positive that are actually positive(Equation 3):\nPrecision =\nTP\nTP + FP\n(3)\n- Recall represents the proportion of actual positive samples that are correctly predicted as positive(Equation 4):\nRecall =\nTP\nTP + FN\n(4)\nIn the above formulas, TP stands for True Positives, FP stands for False Positives, and FN stands for False Negatives.\n3.5.3\nShort-answer questions\nA multidimensional evaluation framework was employed. Error-type classification was assessed using the F1 score\n(Equation 2), while the quality of intervention recommendations was evaluated by Macro Recall (calculating the average\nrecall rate of all answer points within each question by Python) and BERTScore (semantic similarity with references).\nA weighted integration of these metrics yielded the final score (Equation 5). Specially, Incorrect error-type classification\nresulted in a total score of zero.\nTotal = 0.4 × TotalError Type + 0.6 × TotalIntervention\n(5)\n4\nResults\n4.1\nComposition of RxBench\nDue to the lack of evaluation criteria related to prescription review, we selected relevant content from the prescription\nreview training textbooks for clinical pharmacists and developed RxBench, which is also the first of its kind. RxBench\nincludes two types of items: multiple-choice questions and short-answer questions, covering nine common prescription\nreview types. The multiple-choice section contains 1,150 single-choice and 230 multiple-choice items, each with a\nstem and five options (A–E). The short-answer section comprises 879 items integrating patient demographics and\nprescription details. An overview is shown in Figure 2.\n4.2\nLarge Language Models Comparison\n4.2.1\nSingle-Choice Question Task\nIn the single-choice prescription review task, the 18 large language models exhibited substantial variability in accuracy\n(range: 0.509–0.881, P < 0.001; Figure3A). DeepSeek-R1-0528 achieved the highest accuracy (0.881), significantly\nsurpassing all other models (McNemar test, all P < 0.05). The next tier comprised Grok-4-0709 (0.857) and\nGemini-2.5-pro-preview-05-06 (0.855), followed by DeepSeek-V3 (0.848), Qwen3-235B-A22B-thinking (0.837), and\nGemini-2.5-flash (0.825). A mid-range cluster—including Qwen3-32B, Gemini-2.0-flash, LLaMA-4-maverick, Claude,\n8\n"}, {"page": 9, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nFigure 2: Percentage Distribution of Categories by Task Type\nFigure 3: Comparison of LLMs in Single-Choice Question Task\nand GPT-4 series—achieved accuracies of 0.75–0.80. Performance declined markedly for Mistral-small-3.1-24B-\ninstruct (0.626) and Gemma-3-27B (0.592), with Baichuan2-13B-chat (0.509) performing the worst. Within the Qwen\nseries, “thinking” variants consistently outperformed their standard counterparts. Pairwise McNemar tests showed\nnon-significant differences among the top models (e.g., Grok-4-0709 vs. Gemini-2.5-pro-preview-05-06; Figure3B),\nindicating shared error tendencies at the highest performance level.\n9\n"}, {"page": 10, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nFigure 4: Comparison of LLMs in Multiple-Choice Question Task\n4.2.2\nMultiple-Choice Question Task\nIn the multiple-choice prescription review task, models displayed pronounced divergence in precision, recall, and F1\nperformance (P < 0.001,Figure 4A). The strongest overall performers were Gemini-2.5-pro-preview-05-06, Grok-4-\n0709, and Qwen2.5-72B-instruct. Gemini-2.5-pro-preview-05-06 led across all three metrics, demonstrating balanced\naccuracy and coverage. Grok-4-0709 achieved a comparable F1 score, reflecting a favorable balance between recall and\nprecision. Qwen2.5-72B-instruct., although marginally lower, maintained consistently strong performance, underscoring\nits robustness.\nDistinct optimization strategies were evident. Models such as DeepSeek-R1-0528 and DeepSeek-V3 emphasized\nprecision at the expense of recall, reflecting a conservative error-avoidance approach. In contrast, Grok-4-0709 and\nQwen3-235B-A22B-thinking prioritized recall, capturing a broader range of correct options while tolerating modest\nprecision losses. These divergent strategies highlight context-dependent trade-offs in model design.\nAccuracy results (Figure4B) reinforced these trends: Gemini-2.5-pro-preview-05-06 achieved the highest accuracy\n(0.639), closely followed by Grok-4-0709 (0.635). By contrast, Baichuan2-13B-chat performed poorly (0.165),\nindicating profound limitations in multi-answer contexts. Several models in the Qwen series achieved mid-range\naccuracy but demonstrated stable F1 scores, suggesting moderate generalizability and transfer potential.In multiple-\nchoice question tasks, thinking models did not seem to absolutely outperform non-thinking models as expected.\n4.2.3\nShort-Answer Question Task\nIn the short-answer prescription review task, models exhibited substantial heterogeneity in overall scoring and error-type\nrecognition (P < 0.001). As shown in the boxplots (Figure5A), Gemini-2.5-pro-preview-05-06, Grok-4-0709, and\nDeepSeek-R1-0528 consistently achieved higher mean scores with narrower distributions, indicating greater stability\nand reliability. Conversely, Baichuan2-13B-Chat and Mistral-small-3.1-24B-instruct displayed lower mean scores and\ngreater variability, reflecting limited robustness and inconsistent task adaptation.\nHeatmap analyses of F1 scores across error categories (Figure5B) further confirmed these trends. Gemini-2.5-pro-\npreview-05-06 achieved high values across precision, recall, and F1 score (with its F1 score being the highest among\nall models), reflecting balanced strengths in identifying error types while minimizing over-prediction. Grok-4-0709\ndemonstrated the highest recall among all models, alongside a relatively high F1 score, though its precision was\nmoderate — indicating heightened sensitivity to detecting errors but reduced precision in predicting errors. DeepSeek-\nR1-0528 maintained balanced and consistently strong performance across precision, recall, and F1 metrics. In contrast,\nBaichuan2-13B-Chat showed poor results across all three indicators, underscoring its limited applicability in specialized\nprescription review contexts.\n10\n"}, {"page": 11, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nFigure 5: Comparison of LLMs in Short-Answer Question Task\n4.3\nComparison Between Large Language Models and Pharmacists\n4.3.1\nSingle-Choice Question Task\nIn the single-choice prescription review task, clear performance stratification was observed across language models\nand pharmacist groups. As shown in Figure 6A, pharmacist accuracy ranged from 0.833 (pharmacists) to 0.902\n(principle pharmacists). Several advanced models, including Grok-4-0709 (0.941), Qwen3-235B-A22B (0.922), and\nGemini-2.5-flash (0.922), exceeded the highest pharmacist benchmark, suggesting superior adaptability to this task.\nA number of other frontier models (e.g., Gemini-2.5-pro-preview-05-06, DeepSeek-R1-0528, o4-mini-2025-04-16)\nalso achieved comparable accuracy to senior pharmacists (0.858–0.902). In contrast, mid-tier and smaller models,\nsuch as Baichuan2-13B-chat (0.588), Gemma-3-27b (0.627), and Mistral-small-3.1-24b-instruct (0.686), performed\nsubstantially worse, indicating limited robustness for complex clinical decision-making.\nFigure 6B presents the McNemar’s test results for pairwise comparisons. Compared with pharmacists, Grok-4-0709\ndemonstrated statistically significant superiority over all three ranks (principle pharmacists, senior pharmacists, and\npharmacists, p ≤0.05). Qwen3-235B-A22B and Gemini-2.5-flash also significantly outperformed pharmacists\nand senior pharmacists, and achieved parity with principle pharmacists, showing no significant difference in that\ncomparison (p > 0.05). Models in the intermediate range (e.g., GPT-4.1-2025-04-14, Qwen3-32B) generally aligned\nwith pharmacist performance, often showing no significant difference from senior pharmacists or pharmacists, but still\ntrailing behind principle pharmacists. In contrast, low-performing models (e.g., Baichuan2-13B-chat, Gemma-3-27b)\nwere significantly worse than all pharmacist groups. Importantly, no significant difference was observed among\npharmacist groups themselves, confirming consistent performance across levels of professional seniority.\n4.3.2\nMultiple-Choice Question Task\nIn the multiple-choice prescription review task, large performance gaps emerged between models and pharmacist groups.\nAs shown in Figure 7A, top-performing models such as Qwen3-235B-A22B, Gemini-2.0-flash, and GPT-4.1-2025-04-\n14 achieved F1 scores above 0.94, with most leading models maintaining scores above 0.92. In contrast, pharmacist\ngroups performed substantially worse, with F1 scores of 0.757 (principle pharmacists), 0.740 (senior pharmacists),\nand 0.774 (pharmacists), all well below the mid-tier model range (approximately 0.85–0.90). Notably, the expected\ngradient by professional rank was absent, as pharmacists slightly outperformed senior pharmacists, suggesting that\ngreater clinical experience did not translate into superior task performance.\nAccuracy analysis (Figure 7B) reinforced this pattern. Qwen3-235B-A22B achieved the highest accuracy (0.800),\nfollowed by a cluster of high-performing models such as Gemini-2.0-flash, LLaMA-4-maverick, and GPT-4.1-2025-04-\n14, all at 0.700. Pharmacist groups, however, were positioned near the bottom: 0.233 for principle pharmacists, 0.224\n11\n"}, {"page": 12, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nFigure 6: Comparison Between Large Language Models and Pharmacists in Single-Choice Question Task\nFigure 7: Comparison Between Large Language Models and Pharmacists in Multiple-Choice Question Task\nfor pharmacists, and 0.200 for senior pharmacists. These values were close to or even below some lower-tier models\n(e.g., Claude-3.7-sonnet, Mistral-small-3.1), underscoring the striking magnitude of the human–model disparity.\nTogether, these findings highlight two key insights: (1) state-of-the-art large language models exhibit strong robustness\nin handling knowledge-intensive, multi-solution tasks, consistently outperforming pharmacists across both F1 and\naccuracy; and (2) performance stratification by professional seniority was not evident, indicating that pharmacist\nexperience alone was insufficient to achieve higher performance in this evaluation.\n4.3.3\nShort-Answer Question Task\nIn the short-answer prescription review task, boxplot comparisons (Figure 8A) revealed substantial variation across\nmodels and pharmacist groups. The best-performing models included Gemini-2.5-pro-preview-05-06 (mean score\n0.431), DeepSeek-R1-0528 (0.391), and Grok-4-0709 (0.390), which consistently achieved higher averages with\nrelatively stable distributions. By contrast, models such as Baichuan2-13B-chat (0.222) and GPT-4o-2024-11-20 (0.230)\nscored lowest, showing both weaker central tendencies and higher variability. Pharmacist groups—principle pharmacist\n(0.322), senior pharmacist (0.316), and pharmacist (0.245)—were positioned in the mid-to-lower range, outperforming\nsome underperforming models but falling behind the leading systems, suggesting that clinical experience provided\nlimited advantage in this task.\n12\n"}, {"page": 13, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nFigure 8: Comparison Between Large Language Models and Pharmacists in Short-Answer Question Task\nThe heatmap of precision, recall, and F1 score (Figure 8B) further illustrated this disparity. Gemini-2.5-pro-preview-05-\n06 achieved the highest F1 (0.487), followed by Grok-4-0709 (0.435), Qwen3-32B-thinking (0.412), and DeepSeek-\nR1-0528 (0.410). In comparison, pharmacists scored notably lower, with F1 values of 0.312 (principle pharmacist),\n0.321 (senior pharmacist), and 0.240 (pharmacist). The weakest performers, including Baichuan2-13B-chat (0.220) and\nGPT-4o-2024-11-20 (0.248), showed deficiencies across all three metrics.\nOverall, these findings indicate that only the most advanced models demonstrated clear superiority over pharmacists in\nshort-answer tasks, whereas lower- and mid-tier models often underperformed or showed comparable results. This\nsuggests that while cutting-edge systems can enhance error recognition and response accuracy, pharmacist expertise\nremains competitive relative to many existing models.\n4.4\nPerformance Improvement After LoRA Fine-Tuning in Short-Answer Question Task\nWe selected the moderately performing Qwen3-32B as the base model for fine-tuning using the training dataset. After\nLoRA adaptation, Qwen3-32B-LoRA achieved a score nearly 30% higher than the non–fine-tuned Qwen3-32B and 17%\nhigher than Qwen3-32B equipped only with the fine-tuned thinking mode (Figure 5A). Notably, the LoRA-enhanced\nmodel also surpassed several leading large-parameter models, ranking first overall in the short-answer evaluation.\nQwen3-32B-LoRA demonstrated a substantial improvement in recognition accuracy of error-type, with its F1 score\nrising to a level second only to Gemini-2.5-pro-preview-05-06 (Figure 5B).\nSimilarly, in the comparison with clinical pharmacists, Qwen3-32B-LoRA outperformed the non–fine-tuned Qwen3-\n32B in both total score and F1 score of error-type(Figure 8). These results collectively indicate that the model retains\nconsiderable room for further improvement in prescription-review tasks, underscoring the importance of continued\ntask-specific optimization in future work.\n5\nDiscussion\nOur study provides a comprehensive evaluation of LLMs and pharmacists in prescription review involving single-choice,\nmultiple-choice, and short-answer question tasks. The results consistently demonstrate that state-of-the-art LLMs not\nonly reach but often exceed the accuracy and stability of human experts, revealing both opportunities and challenges in\nintegrating these systems into clinical pharmacy practice.\n5.1\nPerformance Superiority of LLMs Over Pharmacists\nAcross all tasks, advanced LLMs such as Grok-4 and Gemini-2.5-pro-preview-05-06 significantly outperformed\npharmacists in accuracy, F1 scores, and overall stability. Particularly in the multiple-choice task, the gap was striking,\nwith models achieving F1 scores exceeding 0.92 while pharmacists remained below 0.78, and accuracy differences\nreaching nearly fourfold. These findings align with prior research indicating that LLMs are capable of integrating vast\n13\n"}, {"page": 14, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nknowledge bases and reasoning across complex contexts, often surpassing domain specialists in specific constrained\ntasks [Kung et al., 2023, Gilson et al., 2023, Singhal et al., 2023]. A plausible explanation for this performance gap lies\nin the specialized nature of clinical pharmacy practice. Unlike LLMs, which apply a uniform reasoning process across\nall domains, clinical pharmacists typically undergo training focused on specific therapeutic areas. While this cultivates\ndeep expertise within their specialty, it may lead to less familiarity or reduced confidence when addressing prescription\nissues outside their immediate domain, thereby constraining their performance on a broad and diverse question set.\n5.2\nClinical Experience Versus Computational Reasoning\nOne unexpected observation was the inverted effect among pharmacists: senior pharmacists did not consistently\noutperform their junior counterparts. This result suggests that prescription review ability is not determined solely by\nyears of experience, but may also be shaped by task type, evaluation criteria, and cognitive or environmental factors. For\ninstance, different question formats may elicit distinct reasoning strategies. When confronted with complex prescription\nscenarios—particularly those involving off-label drug use—senior or clinical pharmacists tend to engage in more\nprofound and specialized reasoning, conducting comprehensive risk assessments and evidence-based analyses. However,\nthis cautious and thorough approach may lead to more conservative or hesitant response strategies in a standardized\nmultiple-choice testing format, potentially impacting their scores. By contrast, less experienced pharmacists may rely\nmore strictly on guideline adherence or test-oriented reasoning, which can be advantageous in standardized assessments.\nLLMs further underscore this divergence, as they excel in structured question-answering tasks by leveraging pattern\nrecognition and probabilistic reasoning, unaffected by clinical experience or contextual nuance. In addition, potential\ninfluences such as cognitive bias, workload, or contextual distractions cannot be ruled out. These findings highlight the\ncomplexity of evaluating pharmacists’ prescription review competence. Future assessment designs should therefore\nadopt more diverse and context-oriented tasks, integrating both standardized tests and real-world scenarios, to more\ncomprehensively capture professional performance and provide a basis for targeted training and decision-support system\ndevelopment.\n5.3\nModel Strategies and Error Profiles\nOur analyses also revealed divergent optimization strategies among LLMs. Some models (e.g., Claude-sonnet series)\nprioritized precision at the expense of recall, adopting a conservative stance that minimized false positives, while others\n(e.g., Grok-4, Qwen3-235B-A22B) emphasized broader recall, achieving higher sensitivity but reduced specificity.\nThis reflects trade-offs similar to those reported in diagnostic AI systems, where performance varies according to the\nweighting of false-positive versus false-negative risks [Rajpurkar et al., 2022, Miotto et al., 2018]. Notably, heatmap\nanalyses confirmed that models surpassed pharmacists in error-type recognition, albeit with universally modest F1\nscores, suggesting an area where both human and machine performance require improvement.\n5.4\nEffectiveness of Domain-Specific Fine-Tuning\nOur study demonstrates that domain-specific fine-tuning is a highly effective strategy for overcoming the limitations\nof general-purpose LLMs in clinical reasoning tasks. Leading LLMs showed significant weaknesses in short-answer\nprescription review, exposing a core bottleneck in complex clinical reasoning. To address this gap, we conducted\nsupervised fine-tuning of Qwen3-32B, a mid-performing model in our initial assessment, using a rigorously curated\ndataset of 2,457 high-quality prescription review cases. These cases, sourced from multiple medical centers across China,\ncover a wide range of clinical departments and have been thoroughly validated by pharmaceutical experts. Each case\nincludes detailed reasoning logic, forming a \"textbook-grade\" training resource. The fine-tuned model demonstrated\nremarkable improvement. Notably, it not only significantly surpassed its base version but also outperformed most\nleading general-purpose models that lacked specialized optimization. This finding strongly suggests that for knowledge-\nintensive and safety-critical tasks like prescription review, the construction of high-quality, logically-annotated training\ndata may be as important as simply scaling up base model parameters.Furthermore, the suboptimal performance of\ngeneral-purpose models underscores their lack of targeted training in this specific domain, highlighting the need for\nfuture development of specialized LLMs dedicated to prescription review.\n5.5\nImplications for Clinical Practice\nThe consistent outperformance of pharmacists by LLMs raises important implications for the future role of AI in\npharmacy practice. On the one hand, LLMs can serve as powerful decision-support tools, offering enhanced error\ndetection and reducing cognitive load for human pharmacists. Prior work has shown that AI-assisted prescription review\ncan lower dispensing errors and improve workflow efficiency [Shoaran et al., 2020, Grzybowski et al., 2020]. On the\nother hand, caution is warranted. LLMs remain prone to hallucination, lack explainability, and may underperform in\n14\n"}, {"page": 15, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nreal-world contexts requiring integration of patient-specific factors such as comorbidities or social determinants of\nhealth [Wang et al., 2024]. Therefore, careful implementation is essential to ensure that AI systems complement, rather\nthan replace, pharmacists’ professional judgment, thereby supporting clinical decision-making and safeguarding patient\nsafety.\n5.6\nLimitations and Future Directions\nSeveral limitations should be acknowledged. First, the study is constrained by its reliance on cases sourced from eight\nauthoritative clinical pharmacist training textbooks. While this multi-center collection from 29 clinical centers ensures\na high standard of accuracy and credibility, the standardized nature of textbook cases may not fully capture the nuances\nand heterogeneity of real-world clinical practice.the tasks employed here were knowledge- and text-based, which\nmay not fully capture the complexity of real-world clinical decision-making. Second, the evaluation was limited to\ncurrent benchmark models and may not reflect future iterations with improved reasoning or multimodal capabilities\n[Thirunavukarasu et al., 2023]. Finally, while statistical superiority of LLMs was established, their long-term reliability,\ninterpretability, and integration into pharmacy workflows require further investigation in prospective clinical trials.\nFuture work therefore prioritize validation using real-world data to assess model performance in authentic scenarios.\nMoreover, research should focus on hybrid human–AI systems that combine the contextual awareness and ethical\nreasoning of pharmacists with the computational breadth and consistency of LLMs. This complementarity could\noptimize prescription safety, enhance efficiency, and preserve the pharmacist’s central role in patient care.\n5.7\nConclusion\nIn summary, this study demonstrates that cutting-edge LLMs outperform pharmacists across a range of prescription\nreview tasks, with particularly pronounced advantages in complex multiple-choice and error-recognition scenarios.\nThese findings highlight the transformative potential of LLMs as decision-support tools in clinical pharmacy, while\nunderscoring the importance of careful integration, interpretability, and collaborative practice models to ensure safe and\neffective implementation.\n6\nData Availability Statement\nThe dataset supporting the findings of this study has been made publicly available through the MedBench repository. It\ncan be accessed at https://medbench.opencompass.org.cn/home.\nReferences\nD. Bull and D. Okaygoun. Evaluating the performance of chatgpt in the prescribing safety assessment: Implications\nfor artificial intelligence-assisted prescribing. Cureus, 16(11):e73003, 2024. doi: 10.7759/cureus.73003. URL\nhttps://doi.org/10.7759/cureus.73003.\nWei Cheng, Chen Wang, Jing Ma, Wen Ji, Xiangli Yang, Bei Wu, and Ruigang Hou. Satisfaction and needs of\npharmacists in prescription-checking training: a cross-sectional survey. Journal of International Medical Research,\n48(11):0300060520965810, 2020.\nS. R. Christiansen, J. A. Morgan, E. Hilmas, and A. Shepardson. Impact of a prescription review program on the\naccuracy and safety of discharge prescriptions in a pediatric hospital setting. J Pediatr Pharmacol Ther, 13(4):\n226–232, 2008. doi: 10.5863/1551-6776-13.4.226. URL https://doi.org/10.5863/1551-6776-13.4.226.\nJaclyn Costello, Michael Barras, Centaine L Snoswell, and Holly Foot. A post-discharge pharmacist clinic to reduce\nhospital readmissions: a retrospective cohort study. International Journal of Clinical Pharmacy, pages 1–9, 2025.\nXiucong Fan, Danxia Chen, Siwei Bao, Xiaohui Dong, Fang Fang, Rong Bai, Yuyi Zhang, Xiaogang Zhang, Weijun\nTang, Yabin Ma, et al. Prospective prescription review system correlated with more rational ppi medication use,\nbetter clinical outcomes and reduced ppi costs: experience from a retrospective cohort study. BMC Health Services\nResearch, 23(1):1014, 2023.\nA. Gilson, C. W. Safranek, T. Huang, V. Socrates, L. Chi, R. A. Taylor, and D. Chartash. Does chatgpt have the potential\nto support clinical decision-making? Journal of the American Medical Informatics Association, 30(9):1724–1732,\n2023. doi: 10.1093/jamia/ocad083. URL https://doi.org/10.1093/jamia/ocad083.\nA. Grzybowski, P. Brona, and G. Lim. Artificial intelligence for medical decision support in ophthalmology. Trans-\nlational Vision Science & Technology, 9(2):14–14, 2020. doi: 10.1167/tvst.9.2.14. URL https://doi.org/10.\n1167/tvst.9.2.14.\n15\n"}, {"page": 16, "text": "Benchmarking Large Language Models Against Pharmacists in Prescription Review Tasks\nX. Huang, D. Estau, X. Liu, Y. Yu, J. Qin, and Z. Li. Evaluating the performance of chatgpt in clinical pharmacy:\nA comparative study of chatgpt and clinical pharmacists.\nBr J Clin Pharmacol, 90(1):232–238, 2024.\ndoi:\n10.1111/bcp.15896. URL https://doi.org/10.1111/bcp.15896.\nT. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon, C. Elepaño, others, and V. Tseng. Performance of\nchatgpt on usmle: Potential for ai-assisted medical education using large language models. PLOS Digital Health,\n2(2):e0000198, 2023. doi: 10.1371/journal.pdig.0000198. URL https://doi.org/10.1371/journal.pdig.\n0000198.\nL. Li, P. Du, X. Huang, H. Zhao, M. Ni, M. Yan, and A. Wang. Comparative analysis of generative artificial intelligence\nsystems in solving clinical pharmacy problems: Mixed methods study. JMIR Med Inform, 13:e76128, 2025. doi:\n10.2196/76128. URL https://doi.org/10.2196/76128.\nR. Miotto, F. Wang, S. Wang, X. Jiang, and J. T. Dudley. Deep learning for healthcare: Review, opportunities\nand challenges. Briefings in Bioinformatics, 19(6):1236–1246, 2018. doi: 10.1093/bib/bbx044. URL https:\n//doi.org/10.1093/bib/bbx044.\nL. Naseralallah, S. Koraysh, M. Alasmar, and B. Aboujabal. The role of pharmacists in mitigating medication\nerrors in the perioperative setting: a systematic review. Systematic Reviews, 14(1):12, 2025a. doi: 10.1186/\ns13643-024-02710-1. URL https://doi.org/10.1186/s13643-024-02710-1.\nLina Naseralallah, Somaya Koraysh, May Alasmar, and Bodoor Aboujabal. The role of pharmacists in mitigating\nmedication errors in the perioperative setting: a systematic review. Systematic Reviews, 14(1):12, 2025b.\nJ. C. L. Ong, L. Jin, K. Elangovan, G. Y. S. Lim, D. Y. Z. Lim, G. G. R. Sng, Y. Ke, J. Y. M. Tung, R. J. Zhong, and\nC. M. Y. Koh. Development and testing of a novel large language model-based clinical decision support systems for\nmedication safety in 12 clinical specialties. arXiv Preprint, 2024. URL https://arxiv.org/abs/2402.01741.\nC. Pais, J. Liu, R. Voigt, V. Gupta, E. Wade, and M. Bayati. Large language models for preventing medication direction\nerrors in online pharmacies. Nature Medicine, 30(6):1574–1582, 2024. doi: 10.1038/s41591-024-02933-8. URL\nhttps://doi.org/10.1038/s41591-024-02933-8.\nP. Rajpurkar, E. Chen, O. Banerjee, and E. J. Topol. Ai in health and medicine. Nature Medicine, 28(1):31–38, 2022.\ndoi: 10.1038/s41591-021-01614-0. URL https://doi.org/10.1038/s41591-021-01614-0.\nLene Vestergaard Ravn-Nielsen, Marie-Louise Duckert, Mia Lolk Lund, Jolene Pilegaard Henriksen, Michelle Lyn-\ndgaard Nielsen, Christina Skovsende Eriksen, Thomas Croft Buck, Anton Pottegård, Morten Rix Hansen, and Jesper\nHallas. Effect of an in-hospital multifaceted clinical pharmacist intervention on the risk of readmission: a randomized\nclinical trial. JAMA internal medicine, 178(3):375–382, 2018.\nM. Shoaran, J. G. Makin, others, and J. M. Carmena. Ai in pharmacovigilance and prescription safety: A review.\nDrug Safety, 43(5):421–432, 2020. doi: 10.1007/s40264-020-00932-9. URL https://doi.org/10.1007/\ns40264-020-00932-9.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani,\nHeather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):\n172–180, 2023.\nRachel M Skains, Jane M Hayes, Katherine Selman, Yue Zhang, Phraewa Thatphet, Kazuki Toda, Bryan D Hayes,\nCarla Tayes, Martin F Casey, Elizabeth Moreton, et al. Emergency department programs to support medication safety\nin older adults: a systematic review and meta-analysis. JAMA network open, 8(3):e250814–e250814, 2025.\nA. J. Thirunavukarasu, D. S. Ting, K. Elangovan, L. Gutierrez, and P. A. Keane. Large language models in medicine.\nNature Medicine, 29(8):1930–1940, 2023. doi: 10.1038/s41591-023-02448-8. URL https://doi.org/10.1038/\ns41591-023-02448-8.\nF. Wang, Y. Wang, M. Rastegar-Mojarad, and S. Liu.\nOpportunities and challenges of llms in healthcare.\nnpj Digital Medicine, 7:55, 2024.\ndoi: 10.1038/s41746-024-01055-w.\nURL https://doi.org/10.1038/\ns41746-024-01055-w.\n16\n"}]}