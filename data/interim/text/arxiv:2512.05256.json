{"doc_id": "arxiv:2512.05256", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.05256.pdf", "meta": {"doc_id": "arxiv:2512.05256", "source": "arxiv", "arxiv_id": "2512.05256", "title": "Enhancing Clinical Note Generation with ICD-10, Clinical Ontology Knowledge Graphs, and Chain-of-Thought Prompting Using GPT-4", "authors": ["Ivan Makohon", "Mohamad Najafi", "Jian Wu", "Mathias Brochhausen", "Yaohang Li"], "published": "2025-12-04T21:12:21Z", "updated": "2025-12-04T21:12:21Z", "summary": "In the past decade a surge in the amount of electronic health record (EHR) data in the United States, attributed to a favorable policy environment created by the Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009 and the 21st Century Cures Act of 2016. Clinical notes for patients' assessments, diagnoses, and treatments are captured in these EHRs in free-form text by physicians, who spend a considerable amount of time entering and editing them. Manually writing clinical notes takes a considerable amount of a doctor's valuable time, increasing the patient's waiting time and possibly delaying diagnoses. Large language models (LLMs) possess the ability to generate news articles that closely resemble human-written ones. We investigate the usage of Chain-of-Thought (CoT) prompt engineering to improve the LLM's response in clinical note generation. In our prompts, we use as input International Classification of Diseases (ICD) codes and basic patient information. We investigate a strategy that combines the traditional CoT with semantic search results to improve the quality of generated clinical notes. Additionally, we infuse a knowledge graph (KG) built from clinical ontology to further enrich the domain-specific knowledge of generated clinical notes. We test our prompting technique on six clinical cases from the CodiEsp test dataset using GPT-4 and our results show that it outperformed the clinical notes generated by standard one-shot prompts.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.05256v1", "url_pdf": "https://arxiv.org/pdf/2512.05256.pdf", "meta_path": "data/raw/arxiv/meta/2512.05256.json", "sha256": "e8f6811e5dd1f74cb2a08989a84a72eea01735893942865836e96b9135230e1d", "status": "ok", "fetched_at": "2026-02-18T02:25:20.698345+00:00"}, "pages": [{"page": 1, "text": "Enhancing Clinical Note Generation with ICD-10,\nClinical Ontology Knowledge Graphs, and\nChain-of-Thought Prompting Using GPT-4\nIvan Makohon,1 Mohamad Najafi,1 Jian Wu,1\nMathias Brochhausen,2 Yaohang Li1∗\n1Computer Science, Old Dominion University\n4700 Elkhorn Ave, Norfolk, VA 23529, USA\n2Biomedical Informatics, University of Arkansas for Medical Sciences\n4301 W. Markham St. Little Rock, AR 72205, USA\n∗To whom correspondence should be addressed;\nE-mail: yaohang@cs.odu.edu\nKeywords:\nLarge language models, generative AI, chain-of-thought (CoT),\nknowledge graph, clinical note generation, and International Classification\nof Diseases (ICD) codes\nAbstract\nIn the past decade a surge in the amount of electronic health record\n(EHR) data in the United States, attributed to a favorable policy envi-\nronment created by the Health Information Technology for Economic\nand Clinical Health (HITECH) Act of 2009 and the 21st Century Cures\nAct of 2016. Clinical notes for patients’ assessments, diagnoses, and\ntreatments are captured in these EHRs in free-form text by physi-\ncians, who spend a considerable amount of time entering and editing\nthem. Manually writing clinical notes takes a considerable amount of\na doctor’s valuable time, increasing the patient’s waiting time and pos-\nsibly delaying diagnoses. Large language models (LLMs) possess the\nability to generate news articles that closely resemble human-written\nones. We investigate the usage of Chain-of-Thought (CoT) prompt en-\ngineering to improve the LLM’s response in clinical note generation. In\nour prompts, we use as input International Classification of Diseases\n(ICD) codes and basic patient information. We investigate a strat-\negy that combines the traditional CoT with semantic search results\nto improve the quality of generated clinical notes. Additionally, we\n1\narXiv:2512.05256v1  [cs.CL]  4 Dec 2025\n"}, {"page": 2, "text": "infuse a knowledge graph (KG) built from clinical ontology to further\nenrich the domain-specific knowledge of generated clinical notes. We\ntest our prompting technique on six clinical cases from the CodiEsp\ntest dataset using GPT-4 and our results show that it outperformed\nthe clinical notes generated by standard one-shot prompts.\n1\nIntroduction\nIn the past decade, there has been a surge in the amount of electronic health\nrecord (EHR) data in the United States. In 2008, only 42% of physicians had\naccess to an EHR (Office of the National Coordinator for Health Information\nTechnology). This figure has now risen to 88% as reported in 2021 (Office of\nthe National Coordinator for Health Information Technology). This increase\nmay be attributed to a favorable policy environment created by the Health\nInformation Technology for Economic and Clinical Health (HITECH) Act\nof 2009 (Burde, 2011) and the 21st Century Cures Act of 2016 (U.S. Food\n& Drug Administration, 2020).\nClinical notes for patients’ assessments, diagnoses, and treatments are\ncaptured in these EHRs in free-form text by physicians, who spend a con-\nsiderable amount of time entering them into computers. These notes offer\nvaluable insights based on real-time observed data, which have shown to en-\nhance the predictive capabilities of medical decision-making models (Classen\net al., 2018).\nDespite the rich information contained in these notes, it is likely some\ndetails are excluded from publicly available LLMs due to restrictions on ac-\ncess to their content, a consequence of the Health Insurance Portability and\nAccountability Act (HIPAA) of 1996 (Moore and Frye, 2019). HIPAA plays\na crucial role in safeguarding the privacy and security of patients’ protected\nhealth information (PHI) in the context of clinical notes. One key outcome of\nHIPAA is the standardization of clinical text de-identification, which allows\nsensitive patient information to be stripped from records so the data can\nbe safely reused. This de-identification process itself is a complex natural\nlanguage processing (NLP) task, often requiring substantial time (Negash\net al., 2023). As a result, datasets used for pretraining LLMs are typically\nassumed to exclude PHI.\nDespite these technological and regulatory advancements, the effective-\nness of clinical note documentation has not kept pace (Arndt et al., 2017;\nGellert et al., 2015; Kroth et al., 2019). Among the factors contributing to\nthis stagnation is physician burnout, a growing concern that impacts both\nprovider well-being and patient care. Physicians often experience emotional\n2\n"}, {"page": 3, "text": "exhaustion, reduced motivation, and a sense of detachment from their pa-\ntients, largely due to the demanding and high-pressure nature of their work.\nOne significant driver of this burnout is the administrative burden associ-\nated with EHRs, particularly the excessive time spent on data entry and\nclinical documentation (Arndt et al., 2017; Gellert et al., 2015; Kroth et al.,\n2019; Liu, 2018).\nThe medical scribe industry has emerged to handle the burdensome doc-\numentation tasks behind the scenes (Gellert et al., 2015). However, relying\non non-professional scribes presents challenges, as these unlicensed individ-\nuals, working under clinician supervision, may lack the medical expertise\nnecessary for accurate EHR documentation, reflecting broader gaps in un-\nderstanding of their impact and effectiveness (Ash et al., 2021). To address\nthe physician burnout challenge, we focus our attention on LLMs given re-\nmarkable progress has been made in recent years with some observations sug-\ngesting that they exhibit more powerful reasoning abilities as the model size\nincreases (Brown et al., 2020). Clinicians show growing interest in adopting\nLLMs for medical question answering, clinical note summarization, diagnos-\ntic assistance, and clinical note documentation, appreciating their potential\nto reduce documentation burden and improve efficiency, as highlighted in\nrecent reviews (Maity and Saikia, 2025; Busch et al., 2025; Vrdoljak et al.,\n2025).\nIn this paper, we investigate the use of large language models (LLMs) to\ngenerate medical notes. We begin by leveraging International Classification\nof Diseases (ICD) codes as initial prompts for the LLM. To enhance the\nquality and relevance of the generated notes, we apply Chain-of-Thought\n(CoT) prompting, using example clinical cases to guide GPT-4’s reasoning\nprocess.\nIn addition, we explore the integration of SNOMED CT OWL\nexpressions as a clinical ontology knowledge graph (KG) prompt. The KG\noffers a structured, machine-readable representation of medical concepts and\ntheir interrelationships, such as disease hierarchies, symptom-diagnosis as-\nsociations, treatment pathways, and comorbid conditions. We evaluated the\nperformance of GPT-4 in generating the patient’s current history of present\nillness (HPI) based on specific task instructions, using diagnosis codes and\nrelevant patient information as input. To benchmark our approach, we ap-\nply the CoT prompting technique to six clinical cases from the CodiEsp test\ndataset.\n3\n"}, {"page": 4, "text": "2\nRelated Works\nThe rapid advancements in LLMs have greatly enhanced their ability to\ncomprehend patterns and relationships between words and phrases more ef-\nfectively by developing a general understanding of grammar, syntax, and se-\nmantic relationships to generate text, bringing their output closer to human-\nlevel quality in areas of news compositions, story generation, and code gener-\nation (Wu et al., 2023). LLMs like GPT-3 (Brown et al., 2020) and GPT-4\n(OpenAI, 2023) have demonstrated impressive performance on downstream\nNLP tasks, even in zero-shot and few-shot settings. With its substantial\ncapacity, it possesses the ability to generate news articles that closely resem-\nble human-written ones, making it difficult to distinguish between the two\n(Brown et al., 2020). This poses a particular challenge in detecting LLM-\ngenerated text, which is crucial for ensuring responsible AI governance (Wu\net al., 2023). GPT-4 is said to adhere more closely to guardrails, ensuring a\nhigher level of responsible text generation.\nPrompt engineering (or In-Context Prompting)\n(Ye et al., 2023; Wei\net al., 2022) emerged as a recent field focused on crafting and refining\nprompts to effectively harness techniques aimed at interacting with LLM\nto guide its behavior towards specific goals, without making changes to the\nmodel weights. Since the recent releases of LLMs, Google researchers re-\ncently revolutionized a prompting strategy in solving word problems across\nfive different LLMs (Zhang et al., 2023). Several prompt engineering tech-\nniques (Li et al., 2022; Sahoo et al., 2024) have emerged and significantly im-\nproves the performance of LLMs on many natural language generation tasks.\nRecent studies, such as CoT (Wei et al., 2022; Zhang et al., 2023), Tree-\nof-Thought (ToT) (Yao et al., 2023a; Long, 2023), and Graph-of-Thought\n(GoT)\n(Besta et al., 2024; Yao et al., 2023b) have shown to improve the\nreasoning and accuracy performance of LLMs by providing rationales for a\ngiven word or phrase (Wei et al., 2022; Zhang et al., 2023; Yao et al., 2023a;\nLong, 2023; Besta et al., 2024; Yao et al., 2023b). Although self-verification\n(Weng et al., 2023) and self-consistency (Wang et al., 2022) have enhanced\nperformance in CoT prompting, recent prompting techniques such as ToT\n(Yao et al., 2023a; Long, 2023) and GoT\n(Besta et al., 2024; Yao et al.,\n2023b) have shown improvement, though their effectiveness is still being\nassessed. CoT (Li’evin et al., 2022) has demonstrated that LLMs are capa-\nble of reasoning through multiple-choice questions on medical board exams.\nFor our purposes, can it reason about ICD codes along with some patient\ninformation to generate clinical notes?\nPrevious endeavors have demonstrated that employing an attention mech-\n4\n"}, {"page": 5, "text": "anism in a multi-label classification task can effectively yield ICD codes from\nclinical notes (Makohon and Li, 2021) and shows that numerous prior re-\nsearch endeavors have revolved around classifying ICD codes using clinical\nnotes as their primary input data (Li et al., 2019; Niu et al., 2023; Wu\net al., 2022). Our work in this paper is to reverse this process by generating\ncomprehensive clinical notes, guided by provided ICD codes and supple-\nmented basic patient information using instructional prompting techniques.\nIn a recent study (Lee and Lindsey, 2024), LLMs were investigated using\nzero-shot prompting to predict ICD-10 codes. ICD-10 codes were provided\nin their prompt: “Predict these ICD-10 codes to the best of your ability”\nwithout any patient information or a clear instruction task to generate clini-\ncal notes. Based on their outputs, the LLMs outputs predicted just the ICD\ncodes titles, not actual patient clinical notes.\nAdditionally, recent studies have explored the use of LLMs for generat-\ning clinical notes with the use of prompt engineering. These include leverag-\ning LLMs to convert transcribed interactions into structured notes through\nstructured prompting and integration of supplementary data for improved\nquality (Biswas and Talukdar, 2024), developing a specialized medical LLM\nto understand and summarize medical conversations using zero-shot prompt\nfor note generation (Yuan et al., 2024), and providing rapid access to med-\nical information via a chatbot that utilizes a predefined system prompt to\nperform contextual searches and Retrieval-Augmented Generation (RAG)\ntechniques\n(Leong et al., 2024).\nSome of the challenges in clinical note\ngeneration through use of LLMs are captured in this study (Wang et al.,\n2024), which highlights the feasibility of training efficient open-source LLMs\nfor clinical note generation, with opportunities for further exploration in\ndomain adaptation, data selection, and reinforcement learning from human\nfeedback (RLHF). RLHF helps align LLMs with human preferences and can\nbe applied in two ways: outcome-supervised, which focuses on improving\nthe overall quality of the text, and process-supervised, which provides more\ndetailed guidance on specific text components, such as reasoning steps, as\nseen in approaches like InstructGPT (Li et al., 2022).\nWe conduct experiments on the closed-source GPT-4 using semantic\nsearches and the CoT prompting technique to query similar clinical cases\nbased on the given ICD codes or text references. We further explore the\nintegration of clinical ontology KG into the prompts to enrich the LLM\ninput with well-defined medical relationships and constraints. To the best\nof our knowledge, we are the first to perform experiments of this kind using\ndiagnosis codes (ICD codes) as input along with basic patient information\nto generate clinical notes using LLMs and CoT prompting instructions. We\n5\n"}, {"page": 6, "text": "seek to answer our research question: Can LLMs reason about ICD codes\nto guide the generation of clinical notes using instruction prompting?\n3\nMethodology\nThis paper explores a method for guiding the generation of clinical notes\nusing an LLM while providing patient information, ICD codes, and task\ninstructions as input, medical relationships between ICD codes and con-\nstraints derived from a KG, utilizing CoT with examples of clinical notes\ndiagnosis returned by semantic search.\n3.1\nSemantic Search & Clinical Cases\nCodiEsp, introduced during the CodiEsp track for CLEF eHealth 2020, is\nrecognized as a gold-standard annotated data source (Miranda-Escalada1\net al., 2020). The dataset is comprised of 1,000 clinical cases, where the\nclinical notes are translated from Spanish to English. It encompasses both\nICD-10 CM and PCS codes, distributed across three randomly sampled\ndatasets: the Training set contains 500 clinical cases.\nThe Development\n(validation) and the Test set each contains 250 clinical cases.\nThe text-\nreference column consists of text used during the annotated process using\nthe Brat visualization tool (Stenetorp et al., 2012). Hereafter, we will refer\nto text-reference column as the Text Reference.\nWe combine CodiEsp’s training and validation datasets (750) for our\nsemantic search embedding query, while reserving the test dataset (250)\nfor selecting six clinical cases samples for evaluating ground-truth against\ngenerative text.\nWe converted the texts in the combined dataset of 750\nclinical cases into numerical vector representations with OpenAI’s text em-\nbedding model (text-embedding-3-small). Our objective is to leverage the\nembedding-driven retrieval to tap into the rich semantic features present in\nother clinical cases. This is achieved through the use of query ICD codes or\ntext references, which facilitates the provision of clinical case examples for\nuse as “thoughts” in our CoT prompt. As we will demonstrate, these seman-\ntic searches provide an efficient approach to identifying examples resembling\nthe examples in the prompts. The embedded query (ICD code or text refer-\nence) is used to pinpoint the most relevant clinical cases by assessing their\nproximity within the embedding space, utilizing document similarity to rank\nand present the top-n most suitable clinical cases. For each query, the cosine\nsimilarity is used to identify the top-n most similar clinical cases. Figure 1\n6\n"}, {"page": 7, "text": "shows an illustration of the Semantic Search query used to search for sim-\nilarities within the embedding space. The top illustration shows the ICD\nCode query. The bottom illustration shows the Text References query. The\nviolet highlight depicts the type of inputs for semantic search query using\nexamples. The query result provides the top-10 similar clinical cases for use\nas a prompt thought.\nFigure 1: An illustration of the Semantic Search query used to search for\nsimilarities within the embedding space.\nWe randomly selected six clinical cases with less than 200 words from the\ntest dataset that contain 1 or more ICD codes or text references. The break-\ndown of the CodiEsp samples along with their ICD codes, text references\nand word count for each clinical case are shown in Table 1 and Table 2.\n3.2\nKnowledge Graph & Clinical Cases\nWe utilized the SNOMED CT to ICD-10-CM map\n(National Library of\nMedicine, 2021) as the foundation for constructing our KG prompt.\nTo\nenrich this mapping, we curated data from the SNOMED CT OWL ontol-\nogy reference set (National Library of Medicine, 2019), which provides the\nOWL expressions and the accompanying description file (National Library\nof Medicine, 2019), which provides human-readable terms that convey the\nintended meaning of each concept. By integrating SNOMED CT concepts\n7\n"}, {"page": 8, "text": "Table 1: Clinical Case Samples (Counts)\nClinical\nCase\nCodiEsp ArticleID\nICD\nCode\nText\nReference\nClinical\nNote\nA\nS0213-12852003000600002-1\n2\n2\n99\nB\nS1130-05582017000100031-1\n1\n1\n113\nC\nS1130-01082008001000008-1\n4\n5\n128\nD\nS1130-01082009000500011-1\n9\n9\n107\nE\nS1130-01082008000100009-1\n10\n11\n107\nF\nS1130-01082006001000017-1\n9\n9\n90\nwith their corresponding ICD-10-CM codes, OWL expressions, and descrip-\ntive labels, we established clinically meaningful links between the ICD codes,\nontological definitions, and semantic representations.\nFigure 2: An example of the OWL expression prompt for CoT KG prompt-\ning.\nFigure 2 illustrates the OWL expression derived from SNOMED CT\nand corresponds to the ICD-10 code K08.109 (Complete loss of teeth, un-\nspecified cause) within Clinical Case B. It is used to formalize the concept\nEdentulous (finding) (278650002) using a combination of ontological rela-\ntionships that capture its clinical semantics.\nSpecifically, the expression\nindicates the absence (Absent [2667000]) of the observable entity Tooth\npresence (278652005), located at the finding site All teeth (1162715001).\nThese attributes are grouped under the Role group (609096000), reflecting\na normalized structure for reasoning over clinical findings.\nThe OWL axioms capture logical relationships and attributes among\nSNOMED CT concepts, enabling more advanced, ontology-driven reason-\ning. This alignment results in a structured and semantically rich prompt\nfor GPT-4, facilitating the generation of a clinically accurate and logically\ncoherent knowledge graph that connects diagnoses, procedures, and broader\nbiomedical relationships. From the listing of ICD-10 codes within our cu-\nrated data, we were able to match the ICD Code: K08.109 (Complete loss of\nteeth, unspecified cause, unspecified class) to the only ICD code associated\n8\n"}, {"page": 9, "text": "Table 2: Clinical Case Samples (Details)\nClinical\nCase\nICD Code\nText Reference\nAge\nGender\nA\nR52, K08.89\npain, toothache\n56\nfemale\nB\nK08.109\nedentulism\n54\nfemale\nC\nE11.9 M79.81\nR05 T14.8\nabdominal wall\nhematoma, cough, DM\nID, hematoma, right\nanterior rectal hematoma\n61\nmale\nD\nB00.9 B99.9\nC15.9 F17.210\nK22.2 R13.10\nR50.9 R60.9\nR63.4\ndysphagia, edema,\nesophageal carcinoma,\nfever, herpes simplex,\ninfection, partial narrowing\nof the esophageal\nlumen, smoker, weightloss\n67\nmale\nE\nB99.9 C81.90\nG93.40 K72.00\nK72.90 K92.1\nK92.2 R58\nR69 R74.0\nacute liver failure,\ndigestive bleeding, disease,\nelevated transaminases,\nencephalopathy, hemorrhage,\nhepatic encephalopathy,\nHodgkin’s disease, infection,\nliver failure, manes\n16\nmale\nF\nI10 K42.9\nK56.60 K92.2\nN80.5 R10.32\nR10.814\nR19.8 R58\nAHT, colon endometriosis,\ncolon hemorrhage,\nhemorrhage, pain in FII,\npain on deep palpation\nin FII, rectal urgency,\nsigma level stricture\nimaging, umbilical hernia\n42\nfemale\n9\n"}, {"page": 10, "text": "to Clinical Case B in Table 2.\n3.3\nPrompt Format\nOur standard prompt, referred to as the baseline, is formatted as task in-\nstructions with one-shot prompt to generate the HPI clinical notes based\non the given ICD codes shown in Figure 3. Our CoT semantic search (CoT\nprompting) is formatted as instructions to guide the output of language\nmodel by controlling its generated text shown in Figure 4. In the CoT in-\nstruction prompt, each experiment contains the ground-truth clinical case’s\nICD codes along with basic patient information.\nFigure 3: Baseline (One-Shot) Prompt.\nIn addition, the top-10 similar clinical cases are provided by seman-\ntic search query (based on the ground-truth ICD codes or text references),\nwhich uses contextualized word embeddings and the cosine similarity func-\ntion to find related clinical cases, are provided as prompts. These inputs act\nas rationales, enabling the LLM to learn and generate the intended clinical\nnotes based on the provided ICD codes.\nOur CoT prompting takes inputs, such as: task instruction, ICD codes\nfor the diagnosis and/or procedure, examples of similar clinical cases using\nthe semantic search (ICD codes or text references) query, and Basic patient\ninformation (age and gender). Both CoT KG (Figure 5) and CoT Seman-\ntic Search KG (Figure 6) prompting techniques utilize the SNOMED-CT\nKnowledge Graph derived from OWL expressions, as illustrated in Figure 2.\nThe key difference between the two lies in the use of semantic search: CoT\nKG does not incorporate semantic search, whereas CoT SS KG does.\n3.4\nMetrics\nCosine distance is the complement of cosine similarity, which measures the\nangular difference between two vector representations in a multi-dimensional\n10\n"}, {"page": 11, "text": "Figure 4: CoT Prompt (leveraging the ICD Code semantic search query).\nspace. It is a mathematical function that quantifies the degree of dissimilar-\nity between two vectors based on their orientation rather than their magni-\ntude. The cosine distance formula is defined as:\nCosine Distance = 1 −\nA · B\n∥A∥∥B∥= 1 −\nPn\ni=1 AiBi\nqPn\ni=1 A2\ni\nqPn\ni=1 B2\ni\n(1)\nwhere, A · B is the dot product of the sentence vectors A and B, ∥A∥\nand ∥B∥are the magnitudes (norms) of the vectors, and the result gives a\nmeasure of the angular distance between the vectors.\nTransformer-based models, such as Bidirectional Encoder Representa-\ntions from Transformers (BERT) (Devlin et al., 2019), capture both syn-\ntactic and semantic relationships between words by generating contextual-\nized word embeddings. To assess the similarity between machine-generated\nand ground-truth documents, we use BERT. Both documents are processed\nthrough the bert-large-cased model to obtain embeddings, which are then\nused to calculate sentence similarities. The two steps are the following.\n11\n"}, {"page": 12, "text": "Figure 5: CoT Prompt (leveraging the knowledge graph).\n1. Using the special “classification” [CLS] token of each sentence. The\n[CLS] token in BERT serves as a holistic representation of the input\nsequence. The output of [CLS] is inferred by all other words in this\nsentence. This implies that the [CLS] contains all information in other\nwords, which makes [CLS] a representation for sentence-level classifi-\ncation.\n2. Calculating the MEAN of the sentence embeddings provides a way to\nquantify the overall cosine distance between the sentences based on\nsemantic meaning.\n3.5\nExperiments\nFor our experiments, we use OpenAI’s GPT-4 (gpt-4) model for all exper-\niments, with the following parameters: seed (123), temperature (0), top p\n(0.000001), frequency penalty (0), and presence penalty (0). We establish a\nbaseline for our results using the standard one-shot prompt and compare it\nagainst the results from our CoT prompts, which utilize a semantic search\nquery based on the provided ICD codes or text references from ground-truth\nclinical case samples. Basic patient information is provided as supplemen-\ntary prompts. Our semantic search query introduces an extra prompt, which\nincludes the top-10 most similar clinical cases based on the given ICD codes\nor text references.\n12\n"}, {"page": 13, "text": "Figure 6: CoT Prompt (leveraging the semantic search query and knowledge\ngraph).\nFor each clinical case sample, we collect results from 100 API calls to\nGPT-4, with each call treated as an independent interaction. This ensures\nthere is no memory or history from previous interactions, making each re-\nsponse independent. The clinical case’s top-10 relatedness scores from the\nsemantic search query are presented in Table 3.\nThese scores are calcu-\nlated using cosine distance, which evaluates spatial proximity to identify\nthe top-10 most similar clinical cases based on the provided ICD codes or\ntext references.\n4\nResults and Discussions\nWe evaluate our prompting technique using cosine distance as the primary\nmetric, comparing the generated text against the ground truth. We utilized\n13\n"}, {"page": 14, "text": "Table 3: Semantic Search Query (Top-10 Relatedness Scores)\nCase#\nICD Code Relatedness\nText Reference Relatedness\nA\n0.762, 0.754, 0.729, 0.724, 0.718,\n0.715, 0.708, 0.695, 0.687, 0.683\n0.563, 0.483, 0.478, 0.469, 0.462,\n0.458, 0.433, 0.431, 0.429, 0.417\nB\n0.720, 0.717, 0.711, 0.701, 0.677,\n0.672, 0.653, 0.644, 0.635, 0.630\n0.469, 0.434, 0.411, 0.387, 0.380,\n0.361, 0.359, 0.338, 0.336, 0.334\nC\n0.812, 0.780, 0.775, 0.768, 0.768,\n0.760, 0.759, 0.754, 0.754, 0.751\n0.601, 0.553, 0.545, 0.522, 0.521,\n0.520, 0.520, 0.517, 0.512, 0.511\nD\n0.803, 0.802, 0.798, 0.796, 0.787,\n0.783, 0.783, 0.782, 0.782, 0.782\n0.630, 0.613, 0.606, 0.568, 0.565,\n0.551, 0.547, 0.537, 0.524, 0.522\nE\n0.846, 0.807, 0.805, 0.795, 0.786,\n0.774, 0.770, 0.769, 0.767, 0.757\n0.637, 0.632, 0.623, 0.613, 0.610,\n0.609, 0.601, 0.598, 0.595, 0.594\nF\n0.797, 0.775, 0.764, 0.763, 0.759,\n0.752, 0.744, 0.741, 0.739, 0.739\n0.654, 0.646, 0.645, 0.629, 0.627,\n0.625, 0.624, 0.617, 0.615, 0.610\nKernel Density Estimation (KDE) plots to visualize the distribution of our\nresults and support our discussion, Box plots to show the comparison of\nour CoT prompting against the various CoT KG approach, and Uniform\nManifold Approximation and Projection (UMAP)\n(McInnes et al., 2018)\nplots to show a visual comparison of noun distributions between Ground\nTruth and various generation methods.\n4.1\nKDE Results and Discussions\nWe present the results using KDE plots, which include Bootstrap Confidence\nIntervals (BCIs) for both sentence-level [CLS] and mean scores. BCIs were\nused to estimate the uncertainty of results by resampling the data with re-\nplacement, allowing for robust, distribution-free confidence estimates. This\nprovides a more reliable measure of variability, especially when parametric\nassumptions may not hold. KDE plots offer a smooth, continuous repre-\nsentation of the underlying data distribution, allowing for clearer identifica-\ntion of patterns, peaks, and differences between groups. Unlike histograms,\nwhich can be sensitive to bin size and may obscure subtle features in the\ndata, KDE provides a more refined view that enhances interpretability. This\nvisualization enables a direct comparison between our CoT prompting ap-\nproach and the baseline one-shot prompt. The distributional analysis shows\nthat CoT prompting, enhanced with semantic search queries such as ICD\ncodes and relevant text references, consistently improves the model’s ability\n14\n"}, {"page": 15, "text": "Figure 7: Illustration of the six clinical cases using KDE with BCIs to\ncompare the Baseline and CoT ICD code semantic search, based on cosine\ndistance score of sentence-level [CLS].\nto capture underlying clinical reasoning. This includes better representation\nof structured information from ICD codes and contextual patient details,\nleading to superior performance compared to the baseline.\nThe KDE with BCI plots (Figures 7- 10) reveal a leftward shift in the\npeaks for the CoT semantic search prompting technique, indicating that its\ndistribution has lower values compared to the baseline prompts. This shift\nhighlights notable differences in semantic alignment with ground-truth clin-\nical cases. The inclusion of BCIs provides statistical validation, enhancing\nthe robustness and interpretability of these findings. However, as shown in\nFigure 10 , we observe that in clinical case A, the baseline prompt works\nbetter than the CoT text reference prompt. This is due to the presence of\ntwo text references (pain, toothache). In contrast, in Figure\n8 , the two\nICD codes (K08.89, R52) clinical case A perform better than the baseline\nprompt.\nOne limitation we observed with CoT prompting is its sensitivity to\nambiguous or underspecified clinical terms, such as “pain” or “toothache.”\nThese terms are semantically broad and often lack contextual qualifiers like\n15\n"}, {"page": 16, "text": "Figure 8: Illustration of the six clinical cases using KDE with BCIs to\ncompare the Baseline and CoT text reference semantic search, based on\ncosine distance score of sentence-level [CLS].\nlocation, severity, or duration, which are essential for generating accurate\nand clinically useful notes. For example, the ICD-10 code R52 (Pain, unspec-\nified) is generally considered too broad for clinical documentation because it\nfails to capture the underlying source or etiology of the pain and may even\nraise compliance concerns. In contrast, when the record specifies dental pain\nor pain due to a tooth disorder, K08.89 (Other specified disorders of teeth\nand supporting structures) is the more clinically meaningful choice, making\nR52 redundant unless no other ICD code provides greater specificity. In\nsuch edge cases, CoT prompting can either propagate the ambiguity of the\ninput or reflect limitations in the annotated data, defaulting to overly broad\ncodes when faced with vague terms.\n4.2\nCoT KG Results and Discussion\nTo summarize and compare the performance across the baseline and CoT\nprompting methods, we use box plots to visualize the results collected from\n250 API calls to GPT-4 for our CoT KG setup.\nThese plots provide a\n16\n"}, {"page": 17, "text": "Figure 9: Illustration of the six clinical cases using KDE with BCIs to\ncompare the Baseline and CoT ICD code semantic search, based on cosine\ndistance score of sentence-level MEAN.\nconcise representation of the output distribution, highlighting key statisti-\ncal features such as the median, interquartile range (IQR), and potential\noutliers, thereby facilitating an informative comparison of variability and\ncentral tendency across prompting methods.\nIn Figure 11, the results of [CLS] (left) show that CoT prompting yields\nthe lowest cosine distances, while CoT KG leads to the highest distances\nwith the greatest variance. In the [MEAN] results (right), CoT prompting\nachieves the lowest distances, and all prompts exhibit tighter distributions\nand improved semantic similarity. These findings suggest that, although the\nontology-based KG introduces greater output diversity, it does not signifi-\ncantly enhance the precision or semantic accuracy of the generated medical\nnotes.\n17\n"}, {"page": 18, "text": "Figure 10: Illustration of the six clinical cases using KDE with BCIs to\ncompare the baseline and CoT text reference semantic search, based on\ncosine distance score of sentence-level MEAN.\n4.3\nPart-of-Speech (PoS) Tagging (Nouns and PROPN) Re-\nsults and Discussion\nTo better understand why CoT prompting outperforms both CoT-KG ap-\nproaches in aligning ground-truth and generated outputs, we applied UMAP\n(McInnes et al., 2018) along with the transformer-based model SciBERT\n(Beltagy et al., 2019) to visualize noun embeddings across different prompt-\ning strategies. For each of five sampled instances, we generated a separate\nplot comparing all four prompting techniques.\nEach technique’s visual-\nization was based on a single randomly selected run from its 250 genera-\ntions. The UMAP dimensionality reduction technique allows us to project\nhigh-dimensional token-level embeddings into a 2D-space, facilitating di-\nrect visual comparison of distributions.\nWe focused specifically on PoS\ntagging (NOUN and PROPN), as they often carry the most critical com-\nmon and proper noun meaning within the tokens. The UMAP parameters:\nn neighbors (15), min dist (0.1), and random state (1) were used to empha-\nsize local structure and ensuring reproducibility.\n18\n"}, {"page": 19, "text": "Figure 11: Illustration of the KG results against the Baseline and CoT\nprompting for sentence-level CLS and MEAN.\nIn Figures 12 13 14 15 16, the illustrations shows the UMAP distribu-\ntion of NOUN and PROPN (proper noun) tokens from ground-truth and\nmachine-generated outputs, where each subplot shows one representative\nresult for a different prompt technique. In each figure, the Baseline out-\nput displays broader dispersion and limited overlap between generated and\nground-truth tokens, indicating greater semantic divergence. In contrast,\nthe CoT-based prompting techniques demonstrate tighter clustering and in-\ncreased overlap, suggesting improved semantic alignment of the knowledge.\nLike the Baseline, the CoT KG plots shows moderate overlap but exhibits\nsome spatial drift between token clusters, which may reflect subtle content\nhallucinations shifts. The CoT prompting technique appears to leverage the\nclinical knowledge from the semantic search examples.\n4.4\nClinical Case Example\nFigure 17 shows that some words are present in the ground-truth but miss-\ning from the generated text. It also reveals that ICD-10 codes K08.89 (pain)\nand R52 (toothache) in Table 2 are linked to terms like “oral,” “cavity,” and\n“discomfort,” which appear in the generated text, all related to oral health.\nBoth texts include “A 54-year-old female,” confirming alignment in basic pa-\ntient information. Figure 18 presents a pairwise token-level cosine similarity\nanalysis between the ground truth text and generated text, with similarity\ndistinguished by height and color. Each bar represents the cosine similarity\nof a token from the ground truth (x-axis) with its corresponding token in\n19\n"}, {"page": 20, "text": "Figure 12: UMAP Token Projection for Sample Instance #1.\nthe generated output. Bar colors indicate similarity levels: blue for high\nsimilarity, gray for moderate similarity, and red for low similarity. Higher\nbars reflect a stronger lexical or semantic alignment between the ground\ntruth and the generated text.\nSeveral tokens such as “woman”, “mild”,\nand “pain” fall within moderate to high similarity (0.3-0.6), suggesting that\nthe model captured clinical terms. Tokens, such as “inspection”, “crown”,\nand “exam” in the (0.2-0.4) range, indicate partial alignment. The red bars\n(right) correspond to lower similarity scores, indicating a greater deviation\nfrom the ground-truth text.\n4.5\nResults Summary\nOverall, by using visual comparisons, such as overlaying KDE plots for all\nclinical cases and prompting techniques, we were able to assess the extent\nof the observed shifts. These plots, along with the cosine distance measure-\nments, confirm that the differences in semantic similarity performance be-\n20\n"}, {"page": 21, "text": "Figure 13: UMAP Token Projection for Sample Instance #2.\ntween the techniques are both statistically significant and practically mean-\ningful. For instance, clinical cases A and B exhibit a significant leftward\nshift in the peaks (Figures 7 and 10) for the ICD code semantic search, indi-\ncating that their distributions have lower values compared with the baseline.\nThe ICD code prompting technique appears to outperform the Text Refer-\nence, likely due to higher relatedness scores for the clinical case examples in\nTable 3. From observation, these visual insights combined with the preci-\nsion offered by the BCIs show that both CoT prompting technique results\ndistributions differ from the baseline standard prompt, which suggest that\nthis technique can guide the generation of clinical notes through instruction\nprompt using similar clinical cases.\n21\n"}, {"page": 22, "text": "Figure 14: UMAP Token Projection for Sample Instance #3.\n5\nConclusion and Future Work\nOur study constructs the HPI clinical notes using CoT prompting with ICD\ncodes, clinical case examples, clinical ontology KG, and basic patient infor-\nmation. Experiments were conducted across various clinical cases (clinical\ncases with 1 ICD code, 2 ICD codes, and several cases with multiple ICD\ncodes), comparing results obtained from a baseline one-shot prompt and\ntwo CoT prompting templates. Through cosine distance analysis, we com-\npared the generated text with ground-truth text, addressing whether LLMs\ncan effectively reason about ICD codes to produce clinical notes using CoT\nprompting.\nOur analysis concludes that the GPT-4 LLM is capable of reasoning\nabout ICD codes using our CoT semantic search prompting techniques over\nthe baseline one-shot prompt to produce clinical notes. Also, comparing\nan EHR to a single ground-truth may not be effective, as different doctors\n22\n"}, {"page": 23, "text": "Figure 15: UMAP Token Projection for Sample Instance #4.\nwrite EHRs differently. For this reason, human evaluation, either through\nexpert review, structured validation protocols, or human-in-the-loop feed-\nback, should complement automatic metrics to ensure clinical relevance,\nfactual accuracy, and real-world utility. This perspective is supported by\nrecent work. (Tam et al., 2024) propose a framework for human evaluation\nof LLMs in healthcare, highlighting that automatic metrics alone cannot\ncapture critical dimensions such as safety and clinical appropriateness. Sim-\nilarly, (Chiu and Chung, 2025) outline a protocol for human evaluation of\ngenerative AI chatbots in clinical consultations, stressing the importance of\ndomain-specific, structured human validation. Designing a human-in-the-\nloop evaluation phase to complement our current quantitative analyses will\nbe one of our future directions.\nAlthough our results show that the clinical ontology KG can improve\nover the baseline, it does not further contribute to more precise generation\nof medical notes when COT with semantic search is incorporated, mainly due\n23\n"}, {"page": 24, "text": "Figure 16: UMAP Token Projection for Sample Instance #5.\nto the increased variability of output generated. How to effectively incorpo-\nrate clinical ontology KG into prompting and how to optimally extract and\nalign model outputs with respect to the KG will be the key focus of our future\nresearch. The code and materials used for model replication are available at\nour GitHub repository (https://github.com/Moh-najafi14/COT-KG).\nTo enhance the prompting techniques, we propose some follow up studies\nnot only in areas of using other instruction prompting techniques, but in\nthese areas as well to enrich LLMs reasoning:\n1. CoT Prompting using Patient’s Past Medical History\nThe Medical Information Mart for Intensive Care (MIMIC-III) (John-\nson et al., 2016) offers clinical data on 30-day ICU readmissions, al-\nlowing past medical history and admission notes to guide model pre-\ndictions for future visits.\n2. Fine-Tune an LLM to become more biased towards the Physi-\n24\n"}, {"page": 25, "text": "Figure 17: A sample from Clinical Case A showing ground-truth and the\nmachine-generated text.\nFigure 18: Pairwise token-level cosine similarity between ground truth and\ngenerated text from Clinical Case A.\ncian’s output\nLLMs are prone to biases from training data, but they can be fine-\ntuned for individual physicians using personalized data. This adjust-\nment, achieved through instruction prompting, allows the model to\nbetter meet specific needs. Physician notes can be extracted for this\nfine-tuning from the MIMIC-III dataset.\n3. Use RAG in conjunction with CoT prompting\nRAG enables retrieval of relevant information, such as patient data\nfrom medical databases, to inform the instruction prompting process.\nOur semantic search embeddings identify the most relevant documents\nbased on query similarity (e.g., patients with similar ICD codes). This\n25\n"}, {"page": 26, "text": "helps guide LLM text generation, while RAG minimizes “hallucina-\ntions” by feeding relevant facts into the model, improving the accuracy\nand relevance of clinical note generation.\n4. Better Integrating Disease and Clinical Ontologies as KG\nReasoners for Clinical Inference\nIncorporating structured and standardized representations of clinical\nand biomedical knowledge through disease ontologies (e.g., SNOMED\nCT (National Library of Medicine, 2021, 2019), MONDO (Vasilevsky\net al., 2022)) has the potential to enhance an LLM’s reasoning by\nproviding a formal framework of relational disease concepts. We plan\nto investigate more effective methods for embedding ontology KGs\ninto CoT prompting, such as translating them into natural language\ndescriptions or designing more precise graph-based representations,\nenabling the LLMs to better infer latent relationships between condi-\ntions, symptoms, and comorbidities for clinical note generation. Fur-\nthermore, optimally aligning model outputs with patient-specific de-\ntails and related conditions in the medical notes could further improve\nthe quality of the generated content.\n6\nAcknowledgments\nThe project is partially supported by the AI Initiative funded by Old Do-\nminion University.\n7\nContribution\nIvan Makohon: Conceptualization, Methodology, Software, Data Curation,\nFormal Analysis, Writing – Original Draft Preparation, Visualization. Mo-\nhamad Najafi: Software, Writing – Reviewing and Editing. Jian Wu: Method-\nology, Writing – Reviewing and Editing, Funding Acquisition.\nMathias\nBrochhausen: Writing – Reviewing and Editing.\nYaohang Li: Supervi-\nsion, Project Administration, Conceptualization, Methodology, Writing –\nReviewing and Editing, Funding Acquisition.\n8\nConflict of Interest\nNone.\n26\n"}, {"page": 27, "text": "References\nB.G. Arndt et al. Tethered to the EHR: Primary Care Physician Workload\nAssessment Using EHR Event Log Data and Time-Motion Observations.\nAnnals of Family Medicine, 15:419–426, 2017.\nJoan S. Ash, Sky Corby, Vishnu Mohan, Nicholas Solberg, James Becton,\nRobby Bergstrom, Benjamin Orwoll, Christopher Hoekstra, and Jeffrey A.\nGold.\nSafe use of the ehr by medical scribes: a qualitative study.\nJ\nAmer Med Inform Assoc, 28:294–302, 08/2020 2021.\nISSN 1067-5027.\ndoi: 10.1093/jamia/ocaa199.\nIz Beltagy, Kyle Lo, and Arman Cohan.\nSciBERT: A pretrained lan-\nguage model for scientific text.\nIn Kentaro Inui, Jing Jiang, Vincent\nNg, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 3615–3620, Hong Kong, China, November 2019. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/D19-1371. URL\nhttps://aclanthology.org/D19-1371/.\nM. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gian-\ninazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk, and T. Hoe-\nfler. Graph of thoughts: Solving elaborate problems with large language\nmodels. In AAAI, 2024.\nA. Biswas and W. Talukdar. Intelligent clinical documentation: Harness-\ning generative ai for patient-centric clinical note generation, 2024. arXiv\npreprint arXiv:240X.\nT.B. Brown et al.\nLanguage models are few-shot learners.\nIn NeurIPS,\nVancouver, Canada, 2020.\nH. Burde. Health law the hitech act - an overview. Virtual Mentor, 13(3):\n172–175, 2011.\nFelix Busch, Lena Hoffmann, Christopher Rueger, {Elon H.C.} {van Dijk},\nRawen Kader, Esteban Ortiz-Prado, {Marcus R.} Makowski, Luca Saba,\nMartin Hadamitzky, {Jakob Nikolas} Kather, Daniel Truhn, Renato Cuo-\ncolo, {Lisa C.} Adams, and {Keno K.} Bressem. Current applications and\nchallenges in large language models for patient care: a systematic review.\nCommunications Medicine, 5(1), December 2025. ISSN 2730-664X. doi:\n10.1038/s43856-024-00717-2.\n27\n"}, {"page": 28, "text": "E. K. Chiu and T. W. Chung. Protocol for human evaluation of generative\nartificial intelligence chatbots in clinical consultations. PLoS One, 20(3):\ne0300487, 2025.\nDavid Classen, Michael Li, Suzanne Miller, and Drew Ladner. An electronic\nhealth record–based real-time analytics program for patient safety surveil-\nlance and improvement.\nHealth Affairs, 37(11):1805–1812, 2018.\ndoi:\n10.1377/hlthaff.2018.0728.\nURL https://doi.org/10.1377/hlthaff.\n2018.0728. PMID: 30395491.\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL: HLT,\npages 4171–4186, Minneapolis, Minnesota, 2019.\nG.A. Gellert, R. Ramirez, and S.L. Webster. The rise of the medical scribe\nindustry: Implications for the advancement of electronic health records.\nJAMA, 313(13):1315–1316, 2015.\nA. E.W. Johnson, T. J. Pollard, L. Shen, L. H. Lehman, M. Feng, M. Ghas-\nsemi, B. Moody, P. Szolovits, L. Anthony Celi, and R. G. Mark. MIMIC-\nIII, a Freely Accessible Critical Care Database. Scientific Data, 3:160035,\n2016.\nP. J. Kroth, N. Morioka-Douglas, S. Veres, S. Babbott, S. Poplau,\nF. Qeadan, C. Parshall, K. Corrigan, and M. Linzer.\nAssociation of\nelectronic health record design and use factors with clinician stress and\nburnout. JAMA Network Open, 2(8):e199609, 2019.\nS.A. Lee and T. Lindsey. Do large language models understand medical\ncodes?, 2024. arXiv preprint arXiv:2406.X.\nH. Y. Leong, Y. F. Gao, S. Ji, B. Kalaycioglu, and U. Pamuksuz. A gen ai\nframework for medical note generation, 2024.\nJ. Li, T. Tang, W.X. Zhao, J. Nie, and J. Wen. Pre-trained language models\nfor text generation: A survey. ACM Computing Surveys, 56:1–39, 2022.\nMin Li, Zhihui Fei, Min Zeng, Fang-Xiang Wu, Yaohang Li, Yi Pan, and\nJianxin Wang.\nAutomated icd-9 coding via a deep learning approach.\nIEEE/ACM Transactions on Computational Biology and Bioinformatics,\n16(4):1193–1202, 2019.\nV. Li’evin, C.E. Hother, and O. Winther. Can large language models reason\nabout medical questions? Patterns, 5, 2022.\n28\n"}, {"page": 29, "text": "P.J. Liu. Learning to write notes in electronic health records, 2018.\nJ. Long. Large language model guided tree-of-thought, 2023. arXiv preprint\narXiv:230X.\nSubhankar Maity and Manob Jyoti Saikia.\nLarge language models in\nhealthcare and medical applications: A review.\nBioengineering, 12(6),\n2025.\nISSN 2306-5354.\ndoi:\n10.3390/bioengineering12060631.\nURL\nhttps://www.mdpi.com/2306-5354/12/6/631.\nIvan Makohon and Yaohang Li.\nMulti-label classification of icd-10 cod-\ning clinical notes using mimic codiesp.\nIn 2021 IEEE EMBS Interna-\ntional Conference on Biomedical and Health Informatics (BHI), pages\n1–4. IEEE, 2021. doi: 10.1109/BHI50953.2021.9508542.\nLeland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger.\nUMAP: Uniform Manifold Approximation and Projection.\nJournal of\nOpen Source Software, 3(29):861, 2018. doi: 10.21105/joss.00861. URL\nhttps://doi.org/10.21105/joss.00861.\nA. Miranda-Escalada1,\nA. Gonzalez-Agirre1,\nArmengol-Estap´J.,\nand\nM. Krallinger1.\nOverview of automatic clinical coding:\nAnnotations,\nguidelines, and solutions for non-english clinical cases at codiesp track\nof clef ehealth 2020. In Conference and Labs of the Evaluation Forum\n(CLEF), 2020.\nW. Moore and S.A. Frye. Review of hipaa, part 1: History, protected health\ninformation, and privacy and security rules. Journal of Nuclear Medicine\nTechnology, 47:269–272, 2019.\nNational Library of Medicine. SNOMED CT. https://www.nlm.nih.gov/\nhealthit/snomedct/index.html, 2019. U.S. Department of Health and\nHuman Services, accessed May 31, 2025.\nNational\nLibrary\nof\nMedicine.\nSNOMED\nCT\nto\nICD-10-CM\nMap. https://www.nlm.nih.gov/research/umls/mapping_projects/\nsnomedct_to_icd10cm.html, 2021. U.S. Department of Health and Hu-\nman Services, accessed May 31, 2025.\nBekelu Negash, Alan Katz, Christine J. Neilson, Moniruzzaman Moni, Marc\nNesca, Alexander Singer, and Jennifer Emily Enns. De-identification of\nfree text data containing personal health information: a scoping review of\nreviews. International Journal of Population Data Science, 8, 2023. URL\nhttps://api.semanticscholar.org/CorpusID:266214589.\n29\n"}, {"page": 30, "text": "Kunying Niu, Yifan Wu, Yaohang Li, and Min Li. Retrieve and rerank for\nautomated ICD coding via Contrastive Learning. Journal of Biomedical\nInformatics, 143:104396, 2023. ISSN 1532-0464.\nOffice of the National Coordinator for Health Information Technol-\nogy.\nOffice-based physician electronic health record adoption, health\nit quick-stat #50.\nhttps://www.healthit.gov/data/quickstats/\noffice-based-physician-electronic-health-record-adoption.\nOpenAI. GPT-4 Technical Report. Technical report, OpenAI, 2023. URL\nhttps://doi.org/10.48550/arXiv.2303.08774. arXiv:2303.08774.\nP. Sahoo, P. others Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and\nA. Chadha. A systematic survey of prompt engineering in large language\nmodels: Techniques and applications, 2024. arXiv preprint arXiv:240X.\nP. Stenetorp, S. Pyysalo, G. Topi´c, T. Ohta, S. Ananiadou, and J. Tsujii.\nbrat: a web-based tool for nlp-assisted text annotation. In Proceedings\nof the Demonstrations at the 13th Conference of the European Chapter of\nthe Association for Computational Linguistics, 2012.\nThomas Yu Chow Tam, Sonish Sivarajkumar, Sumit Kapoor, Alisa V Stol-\nyar, Katelyn Polanska, Karleigh R McCarthy, Hunter Osterhoudt, Xizhi\nWu, Shyam Visweswaran, Sunyang Fu, Piyush Mathur, Giovanni E. Cac-\nciamani, Cong Sun, Yifan Peng, and Yanshan Wang. A framework for\nhuman evaluation of large language models in healthcare derived from\nliterature review. NPJ Digit Med., 7(1):258, 2024.\nU.S. Food & Drug Administration. 21st century cures act. https://www.\nfda.gov/regulatory-information/selected-amendments-fdc-act/\n21st-century-cures-act, 2020.\nNicole\nA.\nVasilevsky\net\nal.\nMondo:\nUnifying\ndiseases\nfor\nthe\nworld,\nby the world.\nmedRxiv,\n2022.\ndoi:\n10.1101/2022.04.13.\n22273750.\nURL https://www.medrxiv.org/content/10.1101/2022.\n04.13.22273750v3. Preprint.\nJosip Vrdoljak, Zvonimir Boban, Marino Vilovi´c, Marko Kumri´c, and Joˇsko\nBoˇzi´c. A review of large language models in medical education, clinical\ndecision support, and healthcare administration. Healthcare, 13(6), 2025.\nISSN 2227-9032. doi: 10.3390/healthcare13060603. URL https://www.\nmdpi.com/2227-9032/13/6/603.\n30\n"}, {"page": 31, "text": "H. Wang, C. Gao, B. Liu, Q. Xu, G. Hussein, M. El Labban, K. Iheasirim,\nH. Korsapati, C. Outcalt, and J. Sun. Adapting open-source large lan-\nguage models for cost-effective, expert-level clinical note generation with\non-policy reinforcement learning, 2024. arXiv preprint arXiv:240X.\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E.H. Chi, and D. Zhou.\nSelf-\nconsistency improves chain of thought reasoning in language models, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,\nFei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou.\nChain-of-thought\nprompting elicits reasoning in large language models. In Proceedings of\nthe 36th International Conference on Neural Information Processing Sys-\ntems, NIPS ’22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN\n9781713871088.\nY. Weng, M. Zhu, F. Xia, B. Li, S. He, K. Liu, and J. Zhao. Large language\nmodels are better reasoners with self-verification. In Findings of the ACL:\nEMNLP, pages 2550–2575, 2023.\nJ. Wu, S. Yang, R. Zhan, Y. Yuan, D.F. Wong, and L.S. Chao. A Sur-\nvey on LLM-generated Text Detection: Necessity, Methods, and Future\nDirections, 2023. arXiv preprint arXiv:230X.\nYifan Wu, Min Zeng, Ying Yu, Yaohang Li, and Min Li. A Pseudo Label-\nWise Attention Network for Automatic ICD Coding. IEEE Journal of\nBiomedical and Health Informatics, 26(10):5201–5212, 2022.\nS. Yao,\nD. Yu,\nJ. Zhao,\nI. Shafran,\nT.L. Griffiths,\nY. Cao,\nand\nK. Narasimhan. Tree of thoughts: Deliberate problem solving with large\nlanguage models. In NeurIPS, 2023a.\nY. Yao, Z. Li, and H. Zhao.\nBeyond chain-of-thought, effective graph-\nof-thought reasoning in large language models, 2023b.\narXiv preprint\narXiv:230X.\nSeonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim,\nand Minjoon Seo. Investigating the effectiveness of task-agnostic prefix\nprompt for instruction following, 2023. URL https://arxiv.org/abs/\n2302.14691.\nDong Yuan, Eti Rastogi, Gautam Naik, Sree Prasanna Rajagopal, Sagar\nGoyal, Fen Zhao, Bharath Chintagunta, and Jeff Ward.\nA continued\npretrained LLM approach for automatic medical note generation.\nIn\n31\n"}, {"page": 32, "text": "Kevin Duh, Helena G´omez-Adorno, and Steven Bethard, editors, Pro-\nceedings of the 2024 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technolo-\ngies: Short Papers, NAACL 2024, Mexico City, Mexico, June 16-21,\n2024, pages 565–571. Association for Computational Linguistics, 2024.\ndoi: 10.18653/V1/2024.NAACL-SHORT.47. URL https://doi.org/10.\n18653/v1/2024.naacl-short.47.\nZ. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A.J. Smola. Mul-\ntimodal chain-of-thought reasoning in language models, 2023.\narXiv\npreprint arXiv:2305.04798.\n32\n"}]}