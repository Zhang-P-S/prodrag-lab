{"doc_id": "arxiv:2601.06300", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.06300.pdf", "meta": {"doc_id": "arxiv:2601.06300", "source": "arxiv", "arxiv_id": "2601.06300", "title": "$\\texttt{AMEND++}$: Benchmarking Eligibility Criteria Amendments in Clinical Trials", "authors": ["Trisha Das", "Mandis Beigi", "Jacob Aptekar", "Jimeng Sun"], "published": "2026-01-09T20:32:04Z", "updated": "2026-01-09T20:32:04Z", "summary": "Clinical trial amendments frequently introduce delays, increased costs, and administrative burden, with eligibility criteria being the most commonly amended component. We introduce \\textit{eligibility criteria amendment prediction}, a novel NLP task that aims to forecast whether the eligibility criteria of an initial trial protocol will undergo future amendments. To support this task, we release $\\texttt{AMEND++}$, a benchmark suite comprising two datasets: $\\texttt{AMEND}$, which captures eligibility-criteria version histories and amendment labels from public clinical trials, and $\\verb|AMEND_LLM|$, a refined subset curated using an LLM-based denoising pipeline to isolate substantive changes. We further propose $\\textit{Change-Aware Masked Language Modeling}$ (CAMLM), a revision-aware pretraining strategy that leverages historical edits to learn amendment-sensitive representations. Experiments across diverse baselines show that CAMLM consistently improves amendment prediction, enabling more robust and cost-effective clinical trial design.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.06300v1", "url_pdf": "https://arxiv.org/pdf/2601.06300.pdf", "meta_path": "data/raw/arxiv/meta/2601.06300.json", "sha256": "230f33f3ee0d5d2ab2073a012f75d4599cc1a240790cec1473ade19d89ba4cbe", "status": "ok", "fetched_at": "2026-02-18T02:22:21.296683+00:00"}, "pages": [{"page": 1, "text": "AMEND++: Benchmarking Eligibility Criteria Amendments in\nClinical Trials\nTrisha Das1, Mandis Beigi2, Jacob Aptekar2, Jimeng Sun1,\n1University of Illinois Urbana-Champaign, 2Medidata Solutions,\nCorrespondence: trishad2@illinois.edu\nAbstract\nClinical trial amendments frequently in-\ntroduce delays, increased costs, and ad-\nministrative burden, with eligibility crite-\nria being the most commonly amended\ncomponent.\nWe introduce eligibility cri-\nteria amendment prediction, a novel NLP\ntask that aims to forecast whether the el-\nigibility criteria of an initial trial proto-\ncol will undergo future amendments. To\nsupport this task, we release AMEND++, a\nbenchmark suite comprising two datasets:\nAMEND, which captures eligibility-criteria\nversion histories and amendment labels\nfrom public clinical trials, and AMEND_LLM,\na refined subset curated using an LLM-\nbased denoising pipeline to isolate sub-\nstantive changes.\nWe further propose\nChange-Aware Masked Language Modeling\n(CAMLM), a revision-aware pretraining\nstrategy that leverages historical edits to\nlearn amendment-sensitive representations.\nExperiments across diverse baselines show\nthat CAMLM consistently improves amend-\nment prediction, enabling more robust and\ncost-effective clinical trial design. 1\n1\nIntroduction\nClinical trial protocols define the scientific and\noperational blueprint of a study and must be\nfollowed exactly as approved by regulatory au-\nthorities. In practice, however, many trials un-\ndergo post-approval modifications (i.e., proto-\ncol amendments) to address emerging scientific\nquestions, operational challenges, or regulatory\nfeedback. While often necessary, amendments\nintroduce substantial burdens by delaying time-\nlines, requiring extensive coordination across\ninvestigators and sites, and generating high\nadditional costs for sponsors and regulatory\nbodies.\n1We will publicly release code and data once the\npaper is accepted.\nA prior study from the Tufts Center for the\nStudy of Drug Development has shown that\namendments are both frequent and increasingly\ncomplex, with a growing proportion triggered\nby design deficiencies or changes in eligibility\ncriteria (Getz et al., 2016). More recent anal-\nyses indicate a rise in amendment prevalence\nand longer implementation cycles (Getz et al.,\n2024), reflecting the intensification of regula-\ntory expectations and operational demands.\nRegulatory agencies face similar pressures, pro-\ncessing large volumes of amendments each year,\noften with multi-week review periods. These\nfindings underscore that many amendments\ncould be anticipated earlier if potential issues\nin the initial protocol were detected proactively.\nEligibility criteria (EC)2 are a particularly\ncommon driver of protocol amendments (Getz\net al., 2024). Changes to inclusion and exclu-\nsion criteria can arise for multiple reasons, as\nnoted in ICH E9 and prior clinical research\n(Lewis, 1999; Cleophas et al., 2009). New medi-\ncal insights may necessitate amendments, while\npersistent violations of entry criteria or inad-\nequate recruitment can also prompt revisions.\nBecause modifications to EC directly affect\nwho can enroll, changes introduced mid-trial\ncan yield meaningful differences between par-\nticipant populations before and after an amend-\nment, with implications for study integrity and\ninterpretability. These factors make EC amend-\nments both impactful and important to antici-\npate early in the design process.\nDespite the central role of protocol design in\nclinical trial efficiency, the computational lit-\nerature lacks large-scale resources and models\nto predict whether EC in an initial protocol\nwill be modified later (Table 1). EC define\n2Throughout this paper, we use EC to refer to the\neligibility criteria section as a single textual unit.\n1\narXiv:2601.06300v1  [cs.CL]  9 Jan 2026\n"}, {"page": 2, "text": "Table 1: Summary of existing studies on clinical trial protocol amendments and dataset availability. None\nof the prior studies released a public dataset suitable for machine learning tasks.\nStudy\nData Source\n# of Trials\nFocus / Contribution\nPublic\nDataset\n(Getz et al., 2016)\n15 pharmaceutical com-\npanies and CROs (Tufts\nCSDD collaboration)\n836\nQuantified prevalence, causes,\nand costs of protocol amend-\nments across commercial trials.\n✗\n(Getz et al., 2024)\n16 pharmaceutical com-\npanies and CROs\n950\nUpdated benchmark analysis\non amendment frequency and\nimpact on study performance.\n✗\n(Botto et al., 2024)\n(Getz et al., 2024)\n950\nCompared amendment preva-\nlence and completion outcomes\nin oncology vs. non-oncology\ntrials.\n✗\n(Joshi, 2023)\n53 NHS-sponsored trials\n(UK)\n53\nMixed-methods study identify-\ning common amendment rea-\nsons and avoidability patterns.\n✗\nAMEND (ours)\nhttps://\nclinicaltrials.gov\n161970\nPrepared an ML-ready bench-\nmark dataset containing all ver-\nsions of eligibility criteria and\namendment labels.\n✓\nAMEND_LLM (ours)\nhttps://\nclinicaltrials.gov\n64641\nSubset of AMEND dataset with\ndenoised amendment labels us-\ning LLMs.\n✓\nthe study population and are among the most\nfrequently amended components of trial pro-\ntocols, making the initial EC text a strong\nsignal for predicting future eligibility-criteria\namendments. Automatically identifying EC\nat elevated risk of future amendments could\nhelp sponsors improve protocol quality, reduce\navoidable revisions, and streamline trial execu-\ntion.\nTo support research on clinical trial design,\nwe introduce AMEND++, a benchmark suite for\nstudying eligibility-criteria (EC) amendment\nprediction from initial trial protocols. AMEND++\nincludes two datasets capturing different trade-\noffs between scale and label fidelity: AMEND, a\nlarge-scale collection derived from public pro-\ntocol version histories, and AMEND_LLM, a re-\nfined subset emphasizing substantive eligibility\nchanges. Using this benchmark, we show that\nthe information of initial eligibility criteria con-\ntains predictive signals of future amendment\nrisk.\nWe make three primary contributions:\n1. We formulate eligibility criteria amend-\nment prediction as a new NLP task\ngrounded in real-world clinical trial de-\nsign, where the objective is to forecast\nwhether an initial protocol will later un-\ndergo eligibility-related amendments.\n2. We release AMEND++, a benchmark suite\nconsisting of two large-scale datasets:\nAMEND, which provides eligibility criteria\nversion histories with amendment labels\nderived from public trial records, and\nAMEND_LLM, a refined subset constructed\nusing an LLM-based label denoising algo-\nrithm.\nThis algorithm decomposes EC\namendments into added, removed, and\nmodified components to isolate substan-\ntive eligibility changes, resulting in high-\nquality amendment labels with substan-\ntially higher agreement with human an-\nnotations than alternative labeling ap-\nproaches.\n3. We propose Change-Aware Masked Lan-\nguage Modeling (CAMLM), a revision-\naware pretraining strategy that lever-\nages historical eligibility-criteria edits\nto learn amendment-sensitive representa-\ntions.\nThrough extensive benchmarks\nand ablations, we show that CAMLM\nconsistently improves amendment predic-\ntion across multiple datasets, backbone\nencoders, and downstream classifiers.\n2\nRelated Work\nIn recent years, a variety of machine learn-\ning and deep learning approaches have been\n2\n"}, {"page": 3, "text": "Figure 1: Overview of the proposed framework for clinical trial eligibility criteria (EC) amendment\nprediction, from data collection to downstream task application. LR: logistic regression, RF: random\nforest, TH: trial head used for predicting amendment. Human annotation is performed only on the test\ndata.\napplied throughout the clinical-trial lifecycle,\nincluding outcome prediction (Gao et al., 2025;\nFu et al., 2022), generation of digital twins or\nsimulated trial participants (Das et al., 2023,\n2025a), and trial-document similarity search\n(Das et al., 2025b; Luo et al., 2024). These\nstudies demonstrate the growing potential of\ndata-driven methods to support trial design,\nanalysis, and operational decision-making.\nFew NLP systems have focused on extracting\nand normalizing eligibility criteria into struc-\ntured representations, enabling cohort query-\ning and downstream analysis but not mod-\neling protocol evolution or amendment risk\n(Tseo et al., 2020; Yuan et al., 2019). More\nrecently, multi-task benchmarks such as Tri-\nalBench have released standardized datasets\nspanning trial design and prediction tasks, in-\ncluding eligibility-criteria design, but do not\nconsider revision-aware pretraining or forecast-\ning protocol amendments from initial drafts\n(Chen et al., 2024).\nIn contrast, research on protocol amend-\nments has been primarily descriptive and sta-\ntistical.\nPrior work has characterized the\nfrequency, causes, and operational burden of\namendments (Getz et al., 2016; Botto et al.,\n2024; Joshi, 2023), but none have framed\namendment forecasting as a predictive mod-\neling problem or released ML-ready datasets.\nTo our knowledge, no machine learning study\nhas examined whether the text of initial proto-\ncols can predict future amendments. A closely\nrelated line of research examines the down-\nstream statistical consequences of EC amend-\nments. One study proposes conducting sepa-\nrate hypothesis tests for participants enrolled\nbefore and after an EC amendment and com-\nbining them using Fisher’s combination test,\nhighlighting that population shifts induced by\namendments can meaningfully affect inference\n(Lösch and Neuhäuser, 2008). While this work\nfocuses on post hoc analysis rather than pre-\ndiction, it reinforces the broader importance\nof EC amendments and the need for method-\nological tools that account for changes in study\npopulations.\n3\nMethods\nOur pipeline (Figure 1) builds the AMEND++\nbenchmark from longitudinal eligibility-criteria\nhistories, combines human annotation and\nLLM-based denoising for label construction,\nformulates amendment prediction as a su-\npervised task, and introduces CAMLM for\nrevision-aware pretraining.\n3.1\nData Collection and Processing\nWe constructed AMEND++, a benchmark suite of\ninterventional drug trials curated from Clini-\ncalTrials.gov. We first filtered for pharmaco-\nlogical interventional studies and programmat-\nically scraped the Record History tab using\nBeautifulSoup to reconstruct longitudinal EC\n3\n"}, {"page": 4, "text": "version histories, which are otherwise unavail-\nable via standard API or direct downloads.\nThe resulting suite comprises two datasets:\nAMEND, a large-scale collection capturing raw\nEC version histories and amendment labels di-\nrectly from public records; and AMEND_LLM, a\nrefined subset where labels are denoised via\nan LLM-based pipeline to isolate substantive\nchanges from minor administrative edits (e.g.,\nformatting or typos). For each trial, we also\nextracted baseline metadata (e.g., titles, de-\nscriptions, disease indications, investigational\ndrugs, MeSH terms, and study phases, etc.) to\nprovide a comprehensive feature set for amend-\nment prediction.\nAfter scraping, we saved each version sep-\narately, preserving the full content and order\nof the original ECs. Disease names were stan-\ndardized using MeSH terms to allow consistent\ncomparisons across trials. For label denoising\n(see Section 3.3), we only used the first and final\nEC versions. However, all versions are released\nfor each clinical trial with EC amendments.\n3.2\nTask Definition: Eligibility Criteria\nAmendment Prediction\nWe formulate eligibility criteria amendment\nprediction as a supervised binary classification\ntask. For each trial i, the model takes as input\nthe initial eligibility criteria text E(0)\ni\nand trial-\nlevel metadata Mi available at trial initiation,\nand predicts ˆyi ∈{0, 1}, where ˆyi = 1 indicates\nthat the eligibility criteria are amended in later\nprotocol versions and ˆyi = 0 otherwise.\n3.3\nLabel Denoising\nHuman Annotation\nTo generate high-quality labels for EC amend-\nments, we relied on human annotation only for\nthe test split. For these trials, human annota-\ntors examined the first and final versions of the\nECs and applied a set of systematic rules (Al-\ngorithm 1) to determine whether a substantive\namendment had occurred. This split serves as\na gold-standard evaluation set.\nLabel Denoising using LLM\nFor the training and validation splits, large-\nscale manual annotation is prohibitively costly.\nInstead, we generate amendment labels using\nlarge language models (LLMs), resulting in\nthe AMEND_LLM dataset, a refined subset of the\nAMEND dataset.\nWe decompose amendment detection into\nthree complementary, criterion-level checks,\neach handled by a separate LLM instance:\n1. Criteria Added: identifying whether any\nnew inclusion or exclusion criteria are in-\ntroduced in the final version.\n2. Criteria Removed: detecting whether\nany criteria present in the initial version\nare absent from the final version.\n3. Criteria Modified: identifying modifi-\ncations to existing criteria that alter the\neligible population, such as changes in nu-\nmeric thresholds, medical entities, logical\noperators, or temporal constraints.\nThis decomposition allows each LLM to fo-\ncus on a single, well-defined type of change,\nreducing ambiguity and improving robustness\ncompared to a single, monolithic amendment\njudgment.\nFor each check, we use a 1-shot prompting\nstrategy, where the LLM is provided with a\nsingle illustrative example followed by the tar-\nget input. To reduce context length and focus\nthe LLMs on semantically meaningful changes,\nwe compute textual differences between the\nfirst and final EC versions and provide only\nthe extracted differences as input to the LLM.\nInstructions provided to the LLM also abide\nby the rules in Algorithm 1.\nThe prompts\nfor LLMs are available in the Appendix (see\nprompts 4, 5, 6).\nFormally, for a given trial i, let E(0)\ni\nand\nE(T)\ni\ndenote the initial and final EC texts, re-\nspectively. We compute the differences:\n∆Ei = Diff(E(0)\ni\n, E(T)\ni\n),\n(1)\nwhich serves as the LLM input for all amend-\nment checks.\nEach LLM check produces a\nbinary output:\ny(k)\ni\n= fk(∆Ei),\nk ∈{add, remove, modify},\n(2)\nwhere fk(·) denotes the LLM corresponding to\nthe k-th amendment type.\nThe final amendment label is obtained by\naggregating the outputs of the three checks\n4\n"}, {"page": 5, "text": "using a logical OR:\nyi = I\n _\nk\ny(k)\ni\n= 1\n!\n,\n(3)\nwhere I(·) is the indicator function.\nBased\non\nvalidation\nagainst\nhuman-\nannotated test data, the LLM-generated labels\nshow strong agreement with human judgments\nand are therefore used for the training and\nvalidation splits of AMEND_LLM. This strategy\nenables scalable and consistent denoising of\namendment labels while reserving human\nannotation exclusively for evaluation.\n3.4\nChange-Aware Masked Language\nModeling (CAMLM)\nStandard masked language modeling (MLM)\ntreats all tokens in a document as equally in-\nformative during pretraining. In contrast, EC\namendments in clinical trial protocols are typi-\ncally sparse and localized: only specific criteria\nare added, removed, or modified across versions.\nTo exploit this structure, we propose a pretrain-\ning strategy, Change-Aware Masked Language\nModeling (CAMLM), that biases representation\nlearning toward historically unstable regions\nof EC text.\nCAMLM does not introduce a\nnew model architecture; instead, it modifies\nthe masking policy used during MLM pretrain-\ning by leveraging historical versions of EC. The\nobjective is to encourage the encoder to learn\nrepresentations that are sensitive to content\nthat has been revised in prior protocols, which\nmay signal fragility or ambiguity in the initial\ntrial design.\nFor each clinical trial i with recorded EC\nversion history, we construct ordered version\npairs\n\u0010\nE(t)\ni , E(T)\ni\n\u0011\n, where E(t)\ni\ndenotes an ear-\nlier EC version (t < T) and E(T)\ni\ndenotes the\nfinal EC version. If multiple historical versions\nare available, each earlier version is paired with\nthe final version, yielding multiple training in-\nstances that capture long-horizon protocol evo-\nlution. For trials without intermediate versions,\nwe construct a single pair\n\u0010\nE(0)\ni\n, E(T)\ni\n\u0011\n, allow-\ning both amended and non-amended trials to\ncontribute to pretraining.\nGiven a version pair\n\u0010\nE(t)\ni , E(T)\ni\n\u0011\n, we com-\npute a token-level diff\n∆E(t)\ni\n= Diff\n\u0010\nE(t)\ni , E(T)\ni\n\u0011\n,\n(4)\nwhich identifies EC content that is deleted or\nreplaced in the final version. Tokens in E(t)\ni\nthat correspond to deletions or substitutions\nin ∆E(t)\ni\nare treated as unstable spans, repre-\nsenting eligibility criteria that were ultimately\nmodified or removed.\nLet x(t)\ni\n= (x(t)\ni,1, . . . , x(t)\ni,Ti) denote the token\nsequence of E(t)\ni . During pretraining, CAMLM\napplies a span-aware masking strategy in which\ntokens within unstable spans are masked with\na high probability pspan, while tokens outside\nthese spans are masked with a lower back-\nground probability plow. The model is trained\nusing the standard MLM objective:\nLCAMLM = −\nX\n(i,t)\nX\nm∈M(t)\ni\nlog p\n\u0010\nx(t)\ni,m | x(t)\ni,\\m\n\u0011\n,\n(5)\nwhere M(t)\ni\ndenotes the set of masked token\npositions for trial i and version t.\nAfter CAMLM pretraining, the resulting en-\ncoder can be used either as a frozen feature ex-\ntractor or fine-tuned end-to-end for eligibility-\ncriteria amendment prediction as defined in\nSection 3.2. Because CAMLM introduces no\nadditional parameters and makes no architec-\ntural changes, it can be applied directly to ex-\nisting transformer encoder models like BERT,\nBioBERT, etc. In Section 4.3, we show that\nCAMLM consistently improves amendment\nprediction performance across multiple model\nclasses and datasets, with gains that persist\nunder extensive ablation studies.\nFigure 2: Clinical trial eligibility criteria amend-\nment rates across phases and trial status in the\nAMEND_LLM dataset (n=64,641) dataset.\nDashed\nlines indicate mean amendment rates for each di-\nmension.\n5\n"}, {"page": 6, "text": "Table 2: Dataset statistics for AMEND and AMEND_LLM.\nProperty\nAMEND\nAMEND_LLM\nTrain/Validation/Test Sizes\nTrain trials (1 / 0)\n140,312 (52,071 / 88,241)\n49,678 (17,178 / 32,500)\nValidation trials (1 / 0)\n15,591 (5,846 / 9,745)\n8,896 (3,240 / 5,656)\nTest trials (1 / 0)\n6,067 (2,786 / 3,281)\n6,067 (2,786 / 3,281) (same as AMEND)\nLabel Sources\nTrain + validation labels\nCTGov labels\nLLM-denoised amendment labels\nTest labels\nHuman annotations\nHuman annotations\nAmendment Count Statistics (amended trials only)\nMin / Avg / Median / Max\n1 / 2.92 / 2 / 38\n1 / 3.07 / 2 / 38\nTable 3: Performance comparison of different models on the AMEND dataset for eligibility criteria amendment\nprediction. Results are reported as mean ± standard deviation over multiple runs. Bold indicates the\nbest score for each metric, and underlining marks the better-performing model between CAMLM and its\ncorresponding non-CAMLM variant.\nModel\nAUROC\nAUPRC\nAccuracy\nBioBERT + LR\n0.676 ± 0.007\n0.609 ± 0.010\n0.629 ± 0.006\nBioBERT_CAMLM + LR\n0.689 ± 0.007\n0.637 ± 0.010\n0.643 ± 0.006\nBioBERT + RF\n0.683 ± 0.007\n0.616 ± 0.011\n0.624 ± 0.006\nBioBERT_CAMLM + RF\n0.684 ± 0.007\n0.633 ± 0.010\n0.633 ± 0.006\nFinetuned BioBERT\n0.704 ± 0.006\n0.656 ± 0.009\n0.631 ± 0.005\nFinetuned BioBERT_CAMLM\n0.714 ± 0.007\n0.670 ± 0.009\n0.659 ± 0.006\nFigure 3: Clinical trial eligibility criteria amend-\nment rates across phases and trial status in the\nAMEND (n=161,970) dataset. Dashed lines indicate\nmean amendment rates for each dimension.\n4\nExperiments and Results\n4.1\nSummary of Datasets\nFor this work, we focused exclusively on inter-\nventional drug trials. After filtering out tri-\nals with missing values, the AMEND_LLM dataset\ncontains 49,678 training, 8,896 validation, and\n6,067 test trials, while the larger AMEND dataset\nincludes 140,312 training, 15,591 validation,\nand the same 6,067 test trials. Both datasets\nshare the test split to enable fair evaluation\nagainst human-labeled annotations. In addi-\ntion to multiple versions of EC, both datasets\nprovide rich trial-level metadata (i.e., disease\narea, intervention drugs, study phase, trial ti-\ntles) that can serve as additional input features\nfor amendment prediction models, helping to\nimprove performance and contextual under-\nstanding. Detailed statistics of the datasets\nare shown in Table 2. Figure 2 and Figure 3\nshow EC amendment rates across phases and\ntrial status in AMEND_LLM and AMEND datasets\nrespectively.\n4.2\nImplementation Details\nFor LLM-based denoising, we utilized the\nQwen/Qwen2.5-7B-Instruct-1M model.\nAll\ngenerations were performed with a tempera-\nture of 0.1 to ensure deterministic and stable\noutputs. The exact prompts used for denoising\nare provided in Appendix 5. For extracting\ndifferences between two versions of EC, we\nutilized difflib python package (Foundation,\n2025). For benchmarking experiments, we em-\nployed BioBERT using the dmis-lab/biobert-\nbase-cased-v1.1 checkpoint as the backbone\nencoder. We also used BERT and longformer\n6\n"}, {"page": 7, "text": "Table 4: Performance comparison of different models on the AMEND_LLM dataset for eligibility criteria\namendment prediction. Results are reported as mean ± standard deviation over multiple runs. Bold\nindicates the best score for each metric, and underlining marks the better-performing model between\nCAMLM and its corresponding non-CAMLM variant.\nModel\nAUROC\nAUPRC\nAccuracy\nBioBERT + LR\n0.681 ± 0.007\n0.610 ± 0.011\n0.633 ± 0.006\nBioBERT_CAMLM + LR\n0.691 ± 0.007\n0.632 ± 0.010\n0.639 ± 0.006\nBioBERT + RF\n0.676 ± 0.007\n0.603 ± 0.011\n0.617 ± 0.006\nBioBERT_CAMLM + RF\n0.686 ± 0.007\n0.628 ± 0.010\n0.633 ± 0.006\nFinetuned BioBERT\n0.687 ± 0.007\n0.621 ± 0.010\n0.619 ± 0.006\nFinetuned BioBERT_CAMLM\n0.697 ± 0.006\n0.634 ± 0.010\n0.644 ± 0.006\nTable 5: Ablations on pretraining strategies on AMEND_LLM. Results shown for BioBERT + LR model.\nMethod\nAUROC\nAUPRC\nAccuracy\nNo pretraining\n0.681 ± 0.007\n0.610 ± 0.011\n0.633 ± 0.006\nMLM\n0.687 ± 0.007\n0.618 ± 0.010\n0.635 ± 0.006\nSpan MLM\n0.688 ± 0.007\n0.628 ± 0.010\n0.636 ± 0.006\nCAMLM\n0.691 ± 0.007\n0.632 ± 0.010\n0.639 ± 0.006\nfor showing results using general and long con-\ntext encoders respectively in the Appendix.\nAll evaluation metrics are reported using non-\nparametric bootstrap resampling with 1,000\niterations over the test set, where we report\nthe mean and standard deviation. Additional\nimplementation details, including training con-\nfigurations, hyperparameters and feature im-\nportance analysis are provided in Appendix.\n4.3\nEligibility Criteria Amendment\nPrediction\nWe evaluate eligibility criteria amendment pre-\ndiction, where models use the initial EC and\nother trial metadata available at trial initiation\nto predict whether EC will be amended later.\nAs shown in Table 3 and Table 4, frozen-\nembedding baselines achieve moderate perfor-\nmance on both datasets, indicating that static\nrepresentations of initial eligibility criteria al-\nready capture some amendment-relevant sig-\nnals. Among these baselines, BioBERT + Lo-\ngistic Regression (LR) and BioBERT + Ran-\ndom Forest (RF) exhibit comparable perfor-\nmance when representations are fixed.\nEnd-to-end fine-tuning with a BioBERT\nclassifier\nconsistently\noutperforms\nfrozen-\nembedding baselines across both datasets, high-\nlighting the importance of task-specific rep-\nresentation learning. While absolute perfor-\nmance is higher on the full AMEND dataset\nin most of the cases, incorporating CAMLM\nyields consistent gains across all model classes\nand metrics.\nFor the fine-tuned models,\nCAMLM improves AUROC by approximately\n1.4–1.5%, AUPRC by about 2.1%, and accu-\nracy by 4.0–4.4% relative on both datasets. 3\nNotably, these improvements are observed both\non AMEND and AMEND_LLM.\nWe evaluated the statistical significance of\nthe AUROC differences using the DeLong’s\ntest.\nAs shown in Table 9, CAMLM pre-\ntraining yields statistically significant improve-\nments over standard BioBERT for logistic re-\ngression and fine-tuned BioBERT models on\nboth AMEND and AMEND_LLM (p < 0.05).\nNo\nsignificant difference is observed for Random\nForest on AMEND, while a significant gain is ob-\nserved in AMEND_LLM. As shown in Appendix\nTable 10, CAMLM pretraining consistently im-\nproves AUROC, AUPRC, and accuracy across\ndifferent backbones (BERT, Longformer) and\nclassifiers (LR, RF), demonstrating its gen-\neralizability beyond BioBERT. We focus on\nBioBERT-based baselines in the main paper\nbecause BioBERT is pretrained on biomedi-\ncal text and consistently outperforms general-\n3Relative\nimprovements\nare\ncomputed\nas\n(CAMLM −baseline)/baseline × 100.\n7\n"}, {"page": 8, "text": "domain BERT and long-context Longformer\nmodels (see Appendix).\n4.4\nAblations\nTo evaluate the impact of revision-aware pre-\ntraining, we compare four settings.\n(1) No\npretraining: BioBERT is the off-the-shelf en-\ncoder. (2) MLM: further pretrains BioBERT\nwith standard masked language modeling on\nthe first EC version, capturing in-domain ter-\nminology but ignoring protocol evolution. (3)\nSpan-aware MLM: masks spans corresponding\nto textual changes between consecutive EC ver-\nsions while lightly masking unchanged tokens,\ncapturing short-range revision patterns. (4)\nCAMLM: extends this to long-horizon model-\ning by pairing intermediate EC versions with\nthe final protocol, focusing on regions predic-\ntive of eventual amendments. Table 5 shows\nthat MLM on the first EC provides modest\ngains, span-aware MLM improves further by\nleveraging local changes, and CAMLM consis-\ntently outperforms all baselines, demonstrating\nthat modeling EC evolution yields richer rep-\nresentations for amendment prediction than\nstatic text alone.\n4.5\nValidity and Impact of\nLLM-Denoised Amendment Labels\nGiven the substantially higher agreement be-\ntween our LLM-generated labels and human\nannotations compared to both raw CTGov-\nderived labels and a baseline chain-of-thought\n(CoT) approach (Table 6), we directly use\nour LLM-generated amendment labels for the\ntraining and validation splits of the AMEND_LLM\ndataset.\nUnlike the baseline CoT method,\nwhich treats amendment detection as a single-\nstep prediction problem, our approach de-\ncomposes eligibility-criteria amendments into\nadded, removed, and modified components, en-\nabling more precise isolation of substantive\neligibility changes. This structured decompo-\nsition yields markedly improved label fidelity\nwhile avoiding the prohibitive cost of large-scale\nhuman annotation, allowing us to scale reliable\nsupervision beyond what manual labeling alone\ncould support. To ensure a high-quality eval-\nuation benchmark, the test split was indepen-\ndently annotated by two clinical trial protocol\nexperts (Algorithm 1), with disagreements re-\nsolved through consensus discussion.\nTable 6: Agreement with Human-Annotated EC\nAmendment Labels (N=6067)\nMethod\nMismatches\nMatch Rate\nCTGov Label\n230\n96.21%\nBaseline CoT\n257\n95.76%\nOur Approach\n40\n99.34%\nTable 7: Performance comparison of models trained\non raw CTGov labels vs. LLM-denoised amend-\nment labels, evaluated on human-labeled test data.\nThese are BioBERT+LR results for showing the\nutility of the denoising step. Models are trained on\nAMEND_LLM train split.\nTrained On\nAUROC\nAUPRC\nCTGov labels\n0.672 ± 0.007\n0.606 ± 0.011\nLLM labels\n0.681 ± 0.007\n0.610 ± 0.011\nTable 7 shows that replacing noisy CTGov4\nlabels with LLM-denoised labels consistently\nimproves all evaluation metrics when tested\nagainst human annotations. The training and\nvalidation trials are from AMEND_LLM as those\nhave both CTGov labels and LLM-denoised\nlabels. The test is the same test set annotated\nby human. This shows that denoising substan-\ntially enhances the quality of the label and\nyields better prediction performance.\n5\nConclusion\nWe introduce eligibility criteria amendment\nprediction, a novel NLP task addressing the\nfrequent and impactful problem of clinical trial\nprotocol amendments. To support this task, we\nrelease AMEND++, a benchmark suite comprising\ntwo datasets: AMEND, which captures eligibility-\ncriteria version histories and amendment labels\nfrom public clinical trials, and AMEND_LLM, a\nrefined subset curated using an LLM-based de-\nnoising pipeline to isolate substantive changes.\nIn addition, we propose CAMLM, a revision-\naware pretraining strategy that takes advan-\ntage of historical edits to learn amendment-\nsensitive representations. Experiments demon-\nstrate that CAMLM consistently improves pre-\ndiction performance across datasets and ar-\nchitectures. Together, our datasets and pre-\ntraining approach offer a scalable framework\nfor anticipating protocol amendments, enabling\nmore efficient, cost-effective, and robust clinical\ntrial design.\n4CTGov is short for ClinicalTrials.gov.\n8\n"}, {"page": 9, "text": "Limitations\nWhile we are the first to construct a machine-\nlearning–ready dataset for protocol amend-\nments, our work focuses exclusively on EC.\nEC is one of the most frequently amended and\nconceptually complex protocol sections, mak-\ning it a natural starting point. However, other\nsections (such as outcomes, study design ele-\nments, and interventions) are also commonly\namended, and extending our dataset and meth-\nods to these areas is an important direction\nfor future work. A second limitation is our\nassumption that ClinicalTrials.gov labels indi-\ncating no eligibility-criteria amendment (label\n0) are correct. We do not explicitly account for\npotential false negatives, which are expected\nto be rare in curated trial registries but could\nintroduce residual label noise.\nReferences\nEmily Botto, Zachary Smith, and Kenneth Getz.\n2024. New benchmarks on protocol amendment\nexperience in oncology clinical trials. Therapeutic\nInnovation & Regulatory Science, 58(4):645–654.\nJintai Chen, Yaojun Hu, Mingchen Cai, Yingzhou\nLu, Yue Wang, Xu Cao, Miao Lin, Hongxia\nXu, Jian Wu, Cao Xiao, and 1 others. 2024.\nTrialbench: Multi-modal artificial intelligence-\nready clinical trial datasets.\narXiv preprint\narXiv:2407.00631.\nTon J Cleophas, Aeilko H Zwinderman, Toine F\nCleophas, and Eugene P Cleophas. 2009. Statis-\ntics applied to clinical trials. Springer.\nTrisha Das, Afrah Shafquat, Mandis Beigi, Jacob\nAptekar, Jason Mezey, and Jimeng Sun. 2025a.\nSeqtrial: Utility preserving sequential clinical\ntrial data generator. In AMIA Annual Sympo-\nsium Proceedings, volume 2024, page 329.\nTrisha Das, Afrah Shafquat, Mandis Beigi, Ja-\ncob Aptekar, and Jimeng Sun. 2025b. secret:\nSemi-supervised clinical trial document similarity\nsearch. In Proceedings of the 63rd Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 5278–5291,\nVienna, Austria. Association for Computational\nLinguistics.\nTrisha Das, Zifeng Wang, and Jimeng Sun. 2023.\nTwin:\nPersonalized clinical trial digital twin\ngeneration.\nIn Proceedings of the 29th ACM\nSIGKDD Conference on Knowledge Discovery\nand Data Mining, pages 402–413.\nPython Software Foundation. 2025. The ‘difflib‘\nmodule — ‘difflib‘ — python 3.12.0 documen-\ntation.\nhttps://docs.python.org/3/library/\ndifflib.html. Accessed: 2025-11-25.\nTianfan Fu, Kexin Huang, Cao Xiao, Lucas M\nGlass, and Jimeng Sun. 2022. Hint: Hierarchi-\ncal interaction network for clinical-trial-outcome\npredictions. Patterns, 3(4).\nChufan Gao, Jathurshan Pradeepkumar, Trisha\nDas, Shivashankar Thati, and Jimeng Sun. 2025.\nAutomatically labeling clinical trial outcomes:\nA large-scale benchmark for drug development.\nPreprint, arXiv:2406.10292.\nKenneth Getz, Zachary Smith, Emily Botto, Elisa-\nbeth Murphy, and Arnaud Dauchy. 2024. New\nbenchmarks on protocol amendment practices,\ntrends and their impact on clinical trial perfor-\nmance. Therapeutic Innovation & Regulatory\nScience, 58(3):539–548.\nKenneth A Getz, Stella Stergiopoulos, Mary Short,\nLeon Surgeon, Randy Krauss, Sybrand Pretorius,\nJulian Desmond, and Derek Dunn. 2016. The\nimpact of protocol amendments on clinical trial\nperformance and cost. Therapeutic innovation\n& regulatory science, 50(4):436–441.\nShivam Joshi. 2023. Common clinical trial amend-\nments, why they are submitted and how they\ncan be avoided: a mixed methods study on nhs\nuk sponsored research (amendments assemble).\nTrials, 24(1):10.\nJohn A. Lewis. 1999. Statistical principles for clin-\nical trials (ich e9): an introductory note on an\ninternational guideline. Statistics in medicine,\n18 15:1903–42.\nChristian Lösch and Markus Neuhäuser. 2008. The\nstatistical analysis of a clinical trial when a pro-\ntocol amendment changed the inclusion criteria.\nBMC medical research methodology, 8(1):16.\nJunyu Luo, Cheng Qian, Lucas Glass, and Feng-\nlong Ma. 2024. Clinical trial retrieval via multi-\ngrained similarity learning. In Proceedings of\nthe 47th International ACM SIGIR Conference\non Research and Development in Information\nRetrieval, pages 2950–2954.\nYitong Tseo, MI Salkola, Ahmed Mohamed, Anuj\nKumar, and Freddy Abnousi. 2020.\nInforma-\ntion extraction of clinical trial eligibility criteria.\narXiv preprint arXiv:2006.07296.\nChi Yuan, Patrick B Ryan, Casey Ta, Yixuan Guo,\nZiran Li, Jill Hardin, Rupa Makadia, Peng Jin,\nNing Shang, Tian Kang, and 1 others. 2019. Cri-\nteria2query: a natural language interface to clin-\nical databases for cohort definition. Journal of\nthe American Medical Informatics Association,\n26(4):294–305.\n9\n"}, {"page": 10, "text": "Appendix A\nHyperparameter Tuning\nFor the BioBERT + LR baseline, we tuned C\n(inverse of the regularization strength) from\n[0.01, 0.1, 1, 10, 100]. For the BioBERT +\nRF baseline, we tuned n_estimators from [100,\n200, 300] and max_depth from [5, 10]. We\ndid not let any weight updates of BioBERT for\nthese baselines. For finetuning the BioBERT\nmodel, we used 10 epochs for training. We se-\nlected a model based on the validation AUROC\nevaluated after each epoch.\nFor CAMLM, MLM and Span MLM, we\ntrained BioBERT for 3 epochs. For CAMLM\npretraining, we use BioBERT’s WordPiece to-\nkenizer. Change spans are computed at the to-\nken level using difflib.SequenceMatcher be-\ntween the tokenized initial and final EC ver-\nsions, where replace and delete operations\ndefine unstable spans.\nWe use a change-\naware masked language modeling (MLM) ob-\njective that emphasizes amendment-prone re-\ngions, masking tokens within detected change\nspans with probability pspan = 0.8 and mask-\ning remaining tokens with a lower background\nprobability plow = 0.05. All masked tokens\nare replaced with the standard [MASK] token,\nand supervision is provided only at masked\npositions following standard MLM training.\nFeature Importance\nWe tried to understand feature importance on\nthe BioBERT + LR model. We assume that\nthe findings from the results will be similar for\nother models too. Table 8 shows that using\nthe first version of EC, disease/condition, inter-\nvention and phase combined gives the highest\nscores.\nLLM Prompts\nFigure 6, figure 5 and figure 4 are prompts used\nfor denoising amendment labels.\nUsage of AI Assistants\nWe utilized chatgpt for paper drafting (e.g.,\nsummarizing, paraphrasing, etc.) assistance.\nPrompt for \"Criteria Removed\" Task\nBelow is an example diff between two\nversions of eligibility criteria (EC), with\ncorrect reasoning.\n{example}\nNow apply the same reasoning to the new case\nbelow.\nHere are the differences between two\nversions of eligibility criteria:\n{difference}\nYour thoughts:\nYou are a clinical trial analyst. Think\nstep-by-step about the differences between\ntwo versions of eligibility criteria (EC)\nand determine if any existing criteria have\nbeen removed.\nStep-by-step Instructions:\n- Read all lines starting with ’-’ as a\nsingle block. These represent the original\neligibility criteria.\n- Read all lines starting with ’+’ as a\nsingle block. These represent the updated\neligibility criteria.\n- Compare the meaning of the ’-’ block with\nthe ’+’ block as a whole.\n- Ignore formatting changes (e.g., line\nbreaks, indentation) and rewordings that\npreserve the original meaning.\n- Focus only on whether the ’-’ block\ncontains any criteria that are completely\nmissing in the ’+’ block.\n- If any requirement, rule, or constraint\nwas present in the ’-’ block but is no\nlonger found in the ’+’ block, then a\ncriterion has been removed.\nIf at least one criterion has been removed:\nFinal output: 1\nIf no criteria have been removed:\nFinal output: 0\nThink step-by-step.\nYour final answer must be a single line at\nthe end in the following format\n(case-sensitive):\nFinal output: 0\nor\nFinal output: 1\nFigure 4: Prompt given to the LLM for the “Criteria\nRemoved” task.\n10\n"}, {"page": 11, "text": "Prompt for \"Criteria Modified\" Task\nBelow is an example diff between two versions of eligibility criteria (EC), with correct\nreasoning.\n{example}\nNow apply the same reasoning to the new case below.\nHere are the differences between two versions of eligibility criteria:\n{difference}\nYour thoughts:\nYou are a clinical trial analyst. Your task is to detect whether any existing eligibility\ncriteria have been *modified* in a way that changes who is eligible. You are not responsible for\ndetecting new criteria or removed criteria - those are handled separately.\nInstructions:\n- Treat ’-’ lines as the original version and ’+’ lines as the updated version of the *same*\ncriterion.\n- Compare carefully for semantic differences.\nCount as meaningful changes (Final output: 1):\n- Numeric changes (age limits, lab thresholds, dosage values).\n- Severity qualifiers (e.g., \"liver disease\" –> \"severe liver disease\").\n- Logical operator changes (e.g., \"and\" <–> \"or\").\n- Added or removed biomedical entities within the same criterion:\n* Drugs (e.g., \"warfarin\" –> \"warfarin or heparin\").\n* Diseases/conditions (e.g., \"HIV\" –> \"HIV or HBV\").\n* Enzymes/transporters/biomarkers (e.g., \"CYP3A4\" –> \"CYP3A4 or P-gp\").\n- Time window changes (e.g., \"within 6 months\" –> \"within 3 months\").\n- Condition scope changes (e.g., \"cancer\" –> \"breast cancer\", \"diabetes\" –> \"type 2 diabetes\").\n- Entity substitutions (e.g., \"rituximab\" –> \"adalimumab\").\nIgnore as non-meaningful (Final output: 0):\n- Rearrangement of criteria or bullet points.\n- Capitalization differences (e.g., \"BOTH\" –> \"both\").\n- Formatting or indentation changes.\n- Punctuation only (commas, semicolons, periods).\n- Abbreviation expansion or contraction (e.g., \"ART\" <–> \"antiretroviral therapy (ART)\", \"CYP3A4\"\n<–> \"cytochrome P450 3A4\").\n- Synonyms with no scope change (e.g., \"high blood pressure\" <–> \"hypertension\").\n- Spelling corrections or typos (e.g., \"diabtes\" –> \"diabetes\").\n- Minor stylistic rephrasing without meaning change (e.g., \"patients who have\" –> \"patients\nwith\").\n- Splitting or merging sentences without altering meaning.\n- Reordering of biomedical terms without additions/removals (e.g., \"HIV, HBV, HCV\" <–> \"HBV, HCV,\nHIV\").\nOutput format (case sensitive):\n- If at least one meaningful modification is found:\nFinal output: 1\n- If no meaningful modification is found:\nFinal output: 0\nFigure 5: Prompt given to the LLM for the “Criteria Modified” task.\n11\n"}, {"page": 12, "text": "Prompt for \"Criteria Added\" Task\nBelow is an example diff between two versions of eligibility criteria (EC), with correct\nreasoning.\n{example}\nNow apply the same reasoning to the new case below. Here are the differences between two versions\nof eligibility criteria:\n{difference}\nYour thoughts:\nYou are a clinical trial analyst. Think step-by-step about the differences between two versions\nof eligibility criteria (EC) and determine if any new criteria have been added.\nStep-by-step Instructions:\n- Read all lines starting with ’-’ as a single block. These represent the original eligibility\ncriteria.\n- Read all lines starting with ’+’ as a single block. These represent the updated eligibility\ncriteria.\n- Compare the meaning of the ’+’ block with the ’-’ block as a whole.\n- Ignore formatting changes (e.g., line breaks, indentation) and rewordings that preserve the\noriginal meaning.\n- Focus only on whether the ’+’ block introduces any new criterion that is not already present in\nthe ’-’ block.\n- If any new requirement, rule, or constraint appears in the ’+’ block that is absent from the ’-’\nblock, it is a new criterion and constitutes an amendment.\nIf at least one new criterion is added:\nFinal output: 1\nIf no new criteria are added:\nFinal output: 0\nThink step-by-step.\nYour final answer must be a single line at the end in the following format (case-sensitive):\nFinal output: 0\nor\nFinal output: 1\nFigure 6: Prompt given to the LLM for the “Criteria Added” task.\n12\n"}, {"page": 13, "text": "Table 8: Feature Importance Exploration\nFeatures Used\nAUROC\nAUPRC\nAccuracy\nF1\nFirst_EC\n0.675 ± 0.007\n0.607 ± 0.011\n0.630 ± 0.006\n0.563 ± 0.008\nFirst_EC, disease\n0.676 ± 0.007\n0.609 ± 0.011\n0.630 ± 0.006\n0.563 ± 0.008\nFirst_EC, disease, intervention\n0.680 ± 0.007\n0.606 ± 0.011\n0.633 ± 0.006\n0.567 ± 0.008\nFirst_EC, disease, intervention, phase\n0.681 ± 0.007\n0.610 ± 0.011\n0.633 ± 0.006\n0.567 ± 0.008\nAlgorithm 1: Human Annotation Procedure for Determining EC Amendments\nInput: First version of eligibility criteria ECfirst,\nFinal version of eligibility criteria ECfinal,\nctgov_label ∈{0, 1}\nOutput: amendment ∈{0, 1}\nif ctgov_label = 0 then\nreturn 0\n// Use ctgov label as ground truth\nInitialize: amendment ←0\nRule 1: Addition Check\nif there exists a criterion in ECfinal not present in ECfirst then\namendment ←1\nRule 2: Removal Check\nif there exists a criterion in ECfirst not present in ECfinal then\namendment ←1\nRule 3: Significant Modification Check\nif any criterion is modified such that: then\nnumeric thresholds change or medical entities are added/removed or\nthe meaning of the criterion changes or the affected population differs\namendment ←1\nIgnored non-amendments:\nFormatting differences (bullets, numbering, indentation)\nMinor spelling or grammar corrections\nSynonyms or wording changes with preserved meaning\nAbbreviation expansion or contraction (e.g., ECG →electrocardiogram)\nReordering of criteria without semantic change\nText added or removed that does not affect eligibility or population\netc\nreturn amendment\nComparison\nAMEND\nAMEND_LLM\np-value\nSig.\np-value\nSig.\nBioBERT (LR) vs. BioBERT-CAMLM (LR)\n3.24 × 10−3\n✓\n3.03 × 10−2\n✓\nBioBERT (RF) vs. BioBERT-CAMLM (RF)\n9.24 × 10−1\n–\n4.87 × 10−3\n✓\nFinetuned BioBERT vs. Finetuned BioBERT-CAMLM\n1.23 × 10−3\n✓\n5.05 × 10−3\n✓\nTable 9: Statistical significance of AUROC differences between BioBERT and BioBERT-CAMLM variants,\nevaluated using DeLong’s test.\n13\n"}, {"page": 14, "text": "Table 10: Performance comparison of all baseline models with and without CAMLM on the AMEND_LLM\ndataset. Results are reported as mean ± standard deviation over multiple runs. Bold indicates the\nbest score for each metric, and underlining marks the better-performing model between CAMLM and its\ncorresponding non-CAMLM variant.\nModel\nAUROC\nAUPRC\nAccuracy\nBERT + LR\n0.669 ± 0.007\n0.602 ± 0.010\n0.616 ± 0.006\nBERT_CAMLM + LR\n0.675 ± 0.007\n0.620 ± 0.010\n0.624 ± 0.006\nBERT + RF\n0.660 ± 0.007\n0.598 ± 0.010\n0.605 ± 0.006\nBERT_CAMLM + RF\n0.674 ± 0.007\n0.616 ± 0.010\n0.618 ± 0.006\nLongformer + LR\n0.670 ± 0.007\n0.611 ± 0.010\n0.622 ± 0.006\nLongformer_CAMLM + LR\n0.674 ± 0.007\n0.618 ± 0.010\n0.626 ± 0.006\nLongformer + RF\n0.666 ± 0.007\n0.602 ± 0.010\n0.597 ± 0.006\nLongformer_CAMLM + RF\n0.673 ± 0.007\n0.627 ± 0.010\n0.619 ± 0.006\nBioBERT + LR\n0.681 ± 0.007\n0.610 ± 0.011\n0.633 ± 0.006\nBioBERT_CAMLM + LR\n0.691 ± 0.007\n0.632 ± 0.010\n0.639 ± 0.006\nBioBERT + RF\n0.676 ± 0.007\n0.603 ± 0.011\n0.617 ± 0.006\nBioBERT_CAMLM + RF\n0.686 ± 0.007\n0.628 ± 0.010\n0.633 ± 0.006\nFinetuned BioBERT\n0.687 ± 0.007\n0.621 ± 0.010\n0.619 ± 0.006\nFinetuned BioBERT_CAMLM\n0.697 ± 0.006\n0.634 ± 0.010\n0.644 ± 0.006\n14\n"}]}