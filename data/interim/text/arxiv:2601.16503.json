{"doc_id": "arxiv:2601.16503", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.16503.pdf", "meta": {"doc_id": "arxiv:2601.16503", "source": "arxiv", "arxiv_id": "2601.16503", "title": "MRAG: Benchmarking Retrieval-Augmented Generation for Bio-medicine", "authors": ["Liz Li", "Wei Zhu"], "published": "2026-01-23T07:07:13Z", "updated": "2026-02-11T03:33:28Z", "summary": "While Retrieval-Augmented Generation (RAG) has been swiftly adopted in scientific and clinical QA systems, a comprehensive evaluation benchmark in the medical domain is lacking. To address this gap, we introduce the Medical Retrieval-Augmented Generation (MRAG) benchmark, covering various tasks in English and Chinese languages, and building a corpus with Wikipedia and Pubmed. Additionally, we develop the MRAG-Toolkit, facilitating systematic exploration of different RAG components. Our experiments reveal that: (a) RAG enhances LLM reliability across MRAG tasks. (b) the performance of RAG systems is influenced by retrieval approaches, model sizes, and prompting strategies. (c) While RAG improves usefulness and reasoning quality, LLM responses may become slightly less readable for long-form questions. We will release the MRAG-Bench's dataset and toolkit with CCBY-4.0 license upon acceptance, to facilitate applications from both academia and industry.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.16503v2", "url_pdf": "https://arxiv.org/pdf/2601.16503.pdf", "meta_path": "data/raw/arxiv/meta/2601.16503.json", "sha256": "e6169b89d6d5dddfc68ec000f91105ad3cd29f68a3bb61485ff4590fd82e7498", "status": "ok", "fetched_at": "2026-02-18T02:20:43.006070+00:00"}, "pages": [{"page": 1, "text": "MRAG: Benchmarking Retrieval-Augmented Generation for Bio-medicine\nLiz Li1,, Wei Zhu2,*\n1 DataSelect AI, Xuhui, Shanghai, China\n2University of Hong Kong, Hong Kong, HK, China\nAbstract\nWhile Retrieval-Augmented Generation (RAG)\nhas been swiftly adopted in scientific and clin-\nical QA systems, a comprehensive evaluation\nbenchmark in the medical domain is lacking.\nTo address this gap, we introduce the Medi-\ncal Retrieval-Augmented Generation (MRAG)\nbenchmark, covering various tasks in English\nand Chinese languages, and building a corpus\nwith Wikipedia and Pubmed. Additionally, we\ndevelop the MRAG-Toolkit, facilitating system-\natic exploration of different RAG components.\nOur experiments reveal that: (a) RAG enhances\nLLM reliability across MRAG tasks. (b) the\nperformance of RAG systems is influenced by\nretrieval approaches, model sizes, and prompt-\ning strategies. (c) While RAG improves use-\nfulness and reasoning quality, LLM responses\nmay become slightly less readable for long-\nform questions. We will release the MRAG-\nBench’s dataset and toolkit with CCBY-4.0 li-\ncense upon acceptance, to facilitate applica-\ntions from both academia and industry.\n1\nIntroduction\nLarge Language Models (LLMs) have trans-\nformed how people seek information online, shift-\ning from searching through websites to directly\nasking chatbots for answers. Recent studies have\ndemonstrated their state-of-the-art capabilities in\nquestion answering (QA) across both general and\nmedical domains (Achiam et al., 2023; Anil et al.,\n2023; Singhal et al., 2023a,b; Nori et al., 2023;\nHuang et al., 2023; Li et al., 2023a; Cui et al., 2023;\nWang et al., 2024; Wenjing Yue and Wang, 2023;\nZhang et al., 2023e; Zhao et al., 2023; Xu et al.,\n2023; Ding et al., 2022; Xin et al., 2024; Qin et al.,\n2023; Zhu et al., 2023; Zhu et al., 2023a,b, 2021a;\nLi et al., 2023b; Zhu et al., 2023c; Zhang et al.,\n2023b; Zhu et al., 2023e; Guo et al., 2021; Zhu\n* Corresponding author. For any inquiries, please contact:\nmichaelwzhu91@gmail.com.\net al., 2021b; Zheng et al., 2023; Sun et al., 2020;\nZhang et al., 2023c,d; Wang et al., 2023c; Zhu et al.,\n2019a; Zhu, 2021a; Zhang et al., 2021; Wang et al.,\n2020; Li et al., 2025; Leong et al., 2025; Zhang\net al., 2025; Yin et al., 2024). However, LLMs\noften produce plausible but factually incorrect re-\nsponses, a phenomenon known as hallucination\n(Ji et al., 2023; Rawte et al., 2023). Additionally,\nthe training data for LLMs may not encompass\nthe latest knowledge, such as recent medical liter-\nature in PubMed1 or the latest updates to clinical\nguidelines. These issues pose significant risks in\nhigh-stakes scenarios like healthcare (Tian et al.,\n2024b; Hersh, 2024; Zhu et al., 2024; Zhu and Tan,\n2023; Liu et al., 2022; Xie et al., 2024; Cui et al.,\n2023; Zheng et al., 2024a; Zhu et al., 2023d; Gao\net al., 2023a; Zuo et al., 2022; Zhang et al., 2022;\nSun et al., 2022; Zhu et al., 2021c; Zhu, 2021b; Li\net al., 2019; Zhu et al., 2019c,b; Zhou et al., 2019;\nZhang et al., 2025; Wang et al., 2025; Liu et al.,\n2025; Yi-Ge et al., 2024; Tian et al., 2024a).\nRetrieval-Augmented Generation (RAG) lever-\nages up-to-date and reliable document collections\nto enhance the capabilities of Large Language\nModels (LLMs), potentially resolving various chal-\nlenges in the field (Lewis et al., 2020; Gao et al.,\n2023b; Zhao et al., 2024). By grounding the reason-\ning of LLMs in the retrieved documents, RAG also\nenhances their transparency. Consequently, RAG\nhas rapidly been adopted in numerous scientific and\nclinical question-answering systems (Lála et al.,\n2023; Jin et al., 2024; Zakka et al., 2024). A com-\nplete RAG system comprises several flexible mod-\nules, including document collections (corpora), re-\ntrieval algorithms (retrievers), and backbone LLMs.\nGiven the diverse tasks within the medical domain,\nRAG’s roles can vary significantly. Therefore, a\ncomprehensive evaluation of RAG in the medical\ndomain is critically important.\n1https://pubmed.ncbi.nlm.nih.gov/.\narXiv:2601.16503v2  [cs.CL]  11 Feb 2026\n"}, {"page": 2, "text": "We first construct a comprehensive Medi-\ncal Retrieval-Augmented Generation benchmark\n(MRAG-Bench) to evaluate the LLM-based RAG\nsystems systematically. MRAG covers 4 task co-\nhorts, two major languages, English and Chinese,\nand a total of 13 test datasets and 14,816 test sam-\nples (see Figure 1 for visualization of task com-\npositions). We also develop and open-source the\nMRAG-Toolkit, an off-the-shelf toolkit (see Fig-\nure 2) that supports (a) three different retrieval\napproaches, sparse retrieval, semantic retrieval,\nand webpage search, (b) different retrieval algo-\nrithms or models, (c) various API based or lo-\ncally deployed LLMs, and (d) different prompt\nstrategies. Extensive experiments are conducted on\nthe MRAG benchmark using the MRAG-Toolkit,\nwhich results in the following observations: (a)\nRAG indeed helps the LLMs to become more reli-\nable on all four types of MRAG tasks. (b) LLMs’\nperformance is directly affected by the referen-\ntial corpus, the retrieval approaches/models, and\nthe prompting strategies. (c) LLMs’ performance\nis log-linearly related to the model’s sizes, and\nlarger LLMs tend to benefit more from RAG. (d)\nAlthough benefiting from referential documents re-\ngarding reasoning, medical knowledge, and overall\nusefulness, LLMs’ responses become slightly less\nreadable when answering long-form questions.\nIn summary, our contributions are three-fold:\n• We introduce a comprehensive RAG evalua-\ntion benchmark, MRAG-Bench, for large lan-\nguage models in the medical domain. Our\nbenchmark provides a suitable testbed for the\nacademic and industrial RAG systems, espe-\ncially those focused on the bio-medical do-\nmain.\n• We provide an accompanying toolkit, MRAG-\nToolkit, for systematically investigating how\ndifferent components of the MRAG system\naffect performance.\n• We have conducted extensive experiments\nwhich reveal how to improve an LLM’s per-\nformance on MRAG tasks.\n2\nRelated work\nDue to limited length, more related works on\nbio-medical question answering are presented in\nAppendix A.\nRetrieval-Augmented Generation (RAG) was\nproposed by (Lewis et al., 2020) to enhance the\ngeneration performance on knowledge-intensive\ntasks by integrating retrieved relevant information.\nIn the LLM era led by OpenAI’s ChatGPT and\nGPT-4, RAG not only mitigates the problem of\nhallucinations as LLMs are grounded on given con-\ntexts but can also provide up-to-date knowledge\nthat the LLMs might not encode (Gao et al., 2023b;\nZhao et al., 2024). Many recent studies have been\ndevoted to improving upon the vanilla RAG work-\nflow by either designing novel retrieval and gener-\nation mechanisms (Borgeaud et al., 2022; Zhang\net al., 2023a; Ram et al., 2023; Jiang et al., 2023),\nor incorporating pre-training and fine-tuning for im-\nproving LLMs’ capabilities in RAG (Zhang et al.,\n2024b; Siriwardhana et al., 2023; Xue et al., 2024).\nIn the bio-medicine domain, current systematic\nevaluations of LLMs typically focus on the vanilla\nLLMs without RAG (Zhu et al., 2023; Singhal et al.,\n2023a,b; Nori et al., 2023; Chen et al., 2023a; Saab\net al., 2024). There has been a series of works on\nhow RAG can help to improve LLMs’ capabilities\nin tasks like clinical decision-making, literature\nanalysis, and information extraction (Frisoni et al.,\n2022; Naik et al., 2021; Xiong et al., 2024; Lála\net al., 2023; Jin et al., 2024; Zakka et al., 2024;\nJeong et al., 2024; Wang et al., 2023d). However,\n(a) a comprehensive evaluation benchmark that con-\ntains a variety of tasks is lacking, and (b) systematic\ninvestigations on how to build a RAG system, such\nas the prompt strategies, in the medical domain is\nlacking. Our work compliments the existing liter-\nature by constructing a comprehensive evaluation\nbenchmark for the LLM-based RAG system.\n3\nThe MRAG Benchmark\n3.1\nConstituting tasks\nTo better evaluate LLMs’ capabilities in med-\nical RAG, we consider a variety of task types in\nthe medical domain, including multi-choice ques-\ntion answering (MCQA), information extraction\n(IE), link prediction (LP), and long-form question\nanswering (LFQA). The task compositions are pre-\nsented as a pie chart in Figure 1, where each task’s\ncorresponding area is proportional to its test set\nsize.\nMulti-choice question-answering (MCQA).\nTo be consistent with existing literature on LLMs’\nevaluation (Hendrycks et al., 2020; Suzgun et al.,\n2022; Wang et al., 2023b; Yue et al., 2024), we\ninclude five commonly used English MCQA tasks,\nincluding three medical examination QA datasets\n"}, {"page": 3, "text": "Figure 1: Composition of tasks in our MRAG-Bench.\n(MMLU-Med (Hendrycks et al., 2020), MedQA-\nUS (Jin et al., 2020), MedMCQA (Pal et al., 2022))\nand two biomedical research QA datasets (Pub-\nMedQA (Jin et al., 2019), BioASQ-Y/N (Krithara\net al., 2023)).\nFor Chinese MCQA, since the exam questions of\nWestern medicine are well covered by the MMLU-\nMed, MedQA-US, and MedMCQA tasks, we con-\nstruct an MCQA dataset containing 1200 test sam-\nples for Traditional Chinese Medicine (TCM) (Xue\nand Roy, 2003), and refer to this dataset as MRAG-\nTCM. This dataset is a robust benchmark for testing\nthe efficacy and accuracy of language models (with\nor without RAG) in understanding and generating\nresponses pertinent to TCM.\nLong-form question answering (LFQA). To\nbetter reflect how online users obtain medical in-\nformation, we also include long-form question-\nanswering in our MRAG benchmark, in which a\nquestion does not have a precise answer like the\nmulti-choice setting. Instead, the answer is a text\nparagraph. For English LFQA, we utilize the Multi-\nMedQA dataset from (Singhal et al., 2023b), which\ncontains 1066 questions curated from the Health-\nSearchQA (Singhal et al., 2023a), LiveQA (Abacha\net al., 2017), MedicationQA (Abacha et al., 2019)\ndatasets. For Chinese LFQA, we collect 1,253\nuser queries from an online medical consultation\nplatform2. An expert panel ensures the safety of\nthe dataset. This dataset is referred to as MRAG-\nCLFQA.\nInformation extraction (IE). To evaluate how\nLLMs powered by the RAG mechanism can per-\nform in the medical information extraction tasks,\n2The name of the online medical consultation platform\nwill be revealed upon acceptance.\nwe consider the following three tasks: (a) DDI\n(Herrero-Zazo et al., 2013), which asks a model\nto extract triplets that reflects how drugs interact\nwith one another. (b) ChemProt (Taboureau et al.,\n2010), extracting the relationships among diseases,\ndrugs, and genes from medical articles. (c) CMeIE\n(Guan et al., 2020), which asks a model to extract\ntriplets of 43 different relation types. The first two\ntasks are in English, and the last in Chinese.\nLink prediction (LP). The link prediction task\n(Kumar et al., 2020) is suitable for evaluating\nLLMs since it directly checks whether LLMs cor-\nrectly grasp the world knowledge and is of great\nimportance for applications like drug repurposing\n(Aruna et al., 2022). In MRAG, we consider the\nfollowing two tasks: (a) ADInt (Xiao et al., 2024),\nwhich is a dataset for identifying new pharmaceu-\ntical interventions (PI) for Alzheimer’s Disease\n(AD). Our MRAG-Bench randomly selects 1,500\nsamples from the ADInt testing set. (b) DRKG\n(Ioannidis et al., 2020) is a knowledge graph inves-\ntigating graph-based drug repurposing algorithms\nfor COVID-19. We randomly select 1,500 drug-\ndisease triplets. We reformulate ADInt and DRKG\nas multiple-choice QA tasks in which two medi-\ncal entities are given in the prompt, and the model\nneeds to determine the relation types.\nIn summary, MRAG investigates how LLM\nRAG systems perform on four cohorts of tasks,\n13 different datasets, and a total of 14,816 test sam-\nples.\n3.2\nEvaluation metrics\nWe use objective metrics can evaluate models\non the MCQA, IE, and LP task cohorts. For the\nLFQA tasks, since there are no standard answers,\nwe conducted a series of model- and human-based\nevaluations.\n4\nRAG system\nTo comprehensively evaluate how different\nLLM-based RAG systems perform on our MRAG\nbenchmark, we propose MRAG-Toolkit, a toolkit\nwith systematic implementations of RAG for medi-\ncal QA. As shown in Figure 2, the MRAG-Toolkit\nconsists of the following major components: (a) the\nCorpus from which the supporting documents are\nretrieved; (b) Retriever, the model or method for\ncoducting efficient retrieval on the retrieval corpus;\n(c) Response generator, the LLM used to summa-\nrize the supporting document and generate the final\n"}, {"page": 4, "text": "Figure 2: Framework of the MRAG toolkit, demonstrating each of its components.\nresponse; (d) Prompting strategies, the strategies\nthat instruct the response generator how to summa-\nrize, reason, reflect, and organize the final response.\n5\nResults\nWe systematically evaluate LLMs on our\nMRAG-Bench benchmark, providing a multi-\ndimensional analysis of different components in\nRAG for medicine.\n5.1\nResults on the closed-form tasks\nWe first benchmark various LLMs on the\nMCQA, IE, and LP tasks in the MRAG bench,\nboth w/o. RAG and w. RAG. The COT-Refine\nstrategy is used to elicit responses. All the LLMs\nutilize the nucleus sampling strategy for decoding.\nThe temperature parameter is set to 0.7, the top_p\nis set to 0.8, and the repetition penalty is set to\n1.05. The combined corpus is used as the source of\nreferential documents. The retriever is BGE-base,\nand eight snippets are retrieved for each query text\nand concatenated to the prompt if using RAG.\nAs shown in Table 1, without RAG, GPT-4 sig-\nnificantly outperforms other competitors, with an\naverage score of 80.2% on the MCQA cohort and\n10.5% on the IE cohort, 47.7% on the LP cohort.\nEven the strongest open-sourced LLMs, Mixtral-\n8x22B, can only achieve about 74.2% of the GPT-\n4’s performance on the MCQA tasks. In com-\nparison, RAG helps all the models improve sig-\nnificantly on the MRAG-Bench regarding the av-\nerage scores on the three cohorts. These results\nsuggest RAG’s great potential to enhance LLMs’\nzero-shot capability to answer users’ medical ques-\ntions or conduct medical knowledge discovery and\nmake the LLMs more reliable. In addition, the\nfollowing observations can be made: (a) Without\nRAG, MEDITRON, the domain-specific LLM, out-\nperforms the open-domain LLM, Qwen2.5-72B,\non the MCQA tasks. However, with the help of\nMRAG, Qwen2.5-72B outperforms MEDITRON\non all three MRAG task cohorts. The intuition\nis that Qwen2.5-72B is better at following the in-\nstructions in MRAG prompts and incorporating the\ninformation retrieved from the documents in the\nreasoning steps. (b) MRAG is not beneficial on all\nof the MRAG tasks. MRAG negatively impacts\nthe MMNLU-Med task for most of the LLMs eval-\nuated. And relatively smaller LLMs, LlaMA-3,\nMEDITRON, and PMC-LlaMA, can not utilize the\nretrieved documents to improve the performance\non MedQA. The results show that the retrieved doc-\numents may distract an LLM if it pays too much\nattention to the wrong document.\n5.2\nResults on the LFQA tasks\nFor the LFQA task, we first ask GPT-4 and GPT-\n3.5 to generate responses, with or without RAG.\nThen, we put these four models into an arena where\neach match randomly selects a pair of responses\nfor the same query. They are judged by GPT-4 to\ndetermine which model’s response wins, loses, or\nis a draw with the other in terms of the evaluation\naxes described in Section 3.2. These matches will\ncontinue until all the model pairs have at least 80\nmatches for each evaluation axis. In this work,\nwe will ask GPT-4 to serve as an unbiased judge\n(Zheng et al., 2024c,b; Zhang et al., 2024a) and ask\nmedical experts to annotate a part of the matches\nto ensure the quality.\nAfter the models conduct matches in the arena,\nwe use the Elo rating system to rank the models\nalong the four axes. The Elo rating system (Glick-\nman and Doan, 2010) is a method for calculating\nthe relative skill levels of players in competitive\ngames. In this work, we set the initial score of each\nLLM as 1000 and the K factor to 40 in the arena.\nThe Elo scores are presented in Table 2.\nFrom Table 2, we can see that RAG can effec-\n"}, {"page": 5, "text": "Method\nPrompting\nmethod\nMCQA tasks in MRAG-Bench\nAvg.\nMMLU-Med\nMedQA\nMedMCQA\nPubMedQA\nBioASQ\nGPT-4\n(gpt-4o)\nw/o. RAG\n93.1 ± 1.2\n86.5 ± 0.9\n76.2 ± 0.8\n57.2 ± 1.4\n88.1 ± 1.1\n80.2\nw. RAG\n92.5 ± 0.9\n89.3 ± 1.0\n75.8 ± 0.7\n78.3 ± 1.2\n90.5 ± 0.9\n85.2\nGPT-3.5\n(gpt-3.5-turbo)\nw/o. RAG\n78.9 ± 1.3\n61.5 ± 0.8\n56.2 ± 1.1\n36.7 ± 1.9\n76.9 ± 1.0\n62.0\nw. RAG\n78.2 ± 1.0\n67.3 ± 0.7\n57.8 ± 0.9\n68.7 ± 1.7\n86.4 ± 0.8\n71.7\nTongyi Qwen\n(qwen_max)\nw/o. RAG\n77.7 ± 1.1\n62.6 ± 1.1\n59.7 ± 0.9\n37.1 ± 1.6\n76.3 ± 1.2\n62.7\nw. RAG\n77.1 ± 1.2\n66.7 ± 0.9\n60.2 ± 0.8\n75.3 ± 1.8\n88.1 ± 1.3\n73.5\nMixtral\n(8 * 22B)\nw/o. RAG\n75.5 ± 1.5\n59.3 ± 1.2\n53.4 ± 0.7\n35.8 ± 1.9\n73.8 ± 1.4\n59.5\nw. RAG\n75.1 ± 1.4\n62.5 ± 1.1\n54.0 ± 0.8\n74.1 ± 1.7\n84.2 ± 1.2\n69.9\nLlaMA-3\n(70B)\nw/o. RAG\n61.6 ± 1.2\n54.1 ± 1.1\n44.5 ± 0.9\n32.6 ± 1.7\n61.6 ± 1.7\n50.9\nw. RAG\n61.4 ± 1.2\n53.6 ± 1.0\n45.2 ± 0.7\n63.5 ± 1.5\n70.2 ± 1.6\n58.8\nQwen2.5\n(72B)\nw/o. RAG\n70.6 ± 0.9\n55.6 ± 0.9\n43.9 ± 0.8\n30.9 ± 1.6\n64.3 ± 1.6\n53.1\nw. RAG\n68.5 ± 1.0\n56.9 ± 0.7\n43.0 ± 0.8\n69.2 ± 1.9\n81.7 ± 1.6\n63.9\nMEDITRON\n(70B)\nw/o. RAG\n64.9 ± 1.6\n51.6 ± 1.1\n46.7 ± 1.0\n43.4 ± 1.7\n68.4 ± 1.9\n55.0\nw. RAG\n65.4 ± 1.5\n49.5 ± 1.1\n45.9 ± 0.9\n53.4 ± 1.6\n76.8 ± 1.6\n58.2\nPMC-LlaMA\n(13B)\nw/o. RAG\n52.2 ± 1.7\n44.3 ± 1.2\n46.5 ± 1.1\n35.8 ± 2.2\n63.1 ± 1.7\n48.4\nw. RAG\n52.5 ± 1.4\n42.6 ± 1.3\n48.3 ± 1.0\n54.0 ± 2.1\n65.2 ± 1.5\n52.5\nTable 1: Benchmark results of different backbone LLMs on the multi-choice QA tasks in MRAG-Bench. We report\nthe average accuracy in percentages on five different runs, along with the standard deviations in the light-grey color.\nModel\nPrompting method\nUsefulness\nReadability\nKnowledge\nReasoning\nGPT-4\n(gpt-4o)\nw/o. RAG\n931.7\n1179.5\n1062.4\n990.9\nw. RAG\n1270.9\n1082.4\n696.1\n1259.4\nGPT-3.5\n(gpt-3.5-turbo)\nw/o. RAG\n811.7\n873.4\n1345.8\n773.5\nw. RAG\n985.5\n864.6\n895.6\n976.0\nTable 2: Results of different backbone LLMs on the LFQA task in MRAG-Bench. The Elo rating scores on each\nevaluation axis are reported. The highest scores are in bold.\nRetriever\nPubMedQA\nMedQA\nDRKG\nBGE-base\n68.7 ± 1.7\n67.3 ± 0.7\n35.4 ± 1.7\nBM25\n61.3 ± 2.1\n62.1 ± 0.9\n31.2 ± 1.4\nMedCPT\n67.9 ± 1.6\n65.8 ± 1.1\n34.4 ± 1.5\nE5-Mistral-7B\n68.6 ± 1.9\n67.4 ± 0.7\n35.3 ± 1.8\nRRF\n68.9 ± 1.6\n67.5 ± 0.8\n35.1 ± 1.6\nTable 3: Comparison of different retrievers for MRAG.\nThe LLM is GPT-3.5.\ntively improve the LLM’s usefulness, knowledge,\nand reasoning in the LFQA task. However, the\nreadability of LLMs declines with RAG. The intu-\nition is that the LLMs tend to answer more formally\nverbatim with RAG, making it slightly more diffi-\ncult for the layer-persons to read.\n5.3\nAblation studies\nComparison of different retrievers\nIn this\nsubsection, we now investigate how different re-\ntrievers affect the performances of RAG systems.\nTable 3 reports the performance of GPT-3.5 with\nRAG on PubmedQA, MedQA, and DRKG with the\nhelp of different retrievers. The domain-specific\nretrievers, MedCPT, perform slightly worse than\nBGE-base. The heavier retriever, E5-mistral-7B,\nCorpus\nPubMedQA\nMedQA\nDRKG\nCombined corpus\n68.7 ± 1.7\n67.3 ± 0.7\n35.4 ± 1.7\nMedical corpus\n68.5 ± 1.9\n67.0 ± 1.1\n35.5 ± 1.3\nOpen-domain corpus\n57.4 ± 1.9\n63.6 ± 1.5\n31.9 ± 1.3\nWorld Wide Web\n60.2 ± 2.1\n66.3 ± 1.1\n33.8 ± 1.8\nTable 4: Comparison of different corpora for MRAG.\nThe LLM is GPT-3.5.\nperforms comparably to the BGE base. The results\nshow that: (a) by large-scale contrastive learning,\nBGE-base is also effective in retrieving domain-\nspecific documents. (b) domain-specific pretrain-\ning does not provide clear advantages against the\nopen-domain retrievers. Table 3 also demonstrates\nthat RRF, the fusion of BGE-base and MedCPT,\noutperforms the two component retrievers on two\nof the three tasks. However, employing RRF in\napplications means multiple retrievers have to be\ndeployed.\nEffects of different corpora\nTable 4 reports\nthe performance of GPT-3.5 w.\nRAG on Pub-\nmedQA, MedQA, and DRKG, with different cor-\npus for document retrieval. The results show that:\n(a) The performance of LLMs on the MRAG tasks\nis highly related to the referential corpus. For ex-\n"}, {"page": 6, "text": "Prompting\nPubMedQA\nMedQA\nDRKG\nCOT-Refine\n68.7 ± 1.7\n67.3 ± 0.7\n35.4 ± 1.7\nChain-of-thought\n67.3 ± 1.6\n66.3 ± 0.9\n35.2 ± 1.6\nDirect answer\n63.1 ± 2.1\n64.5 ± 1.3\n30.8 ± 1.8\nTable 5: Comparison of different prompting strategies\nfor eliciting responses. The LLM is GPT-3.5.\nFigure 3: Effects of #documents retrieved.\nample, the open-domain corpus is significantly less\nhelpful than the medical corpus for the PubMedQA\ntask. (b) A simple combination of the two corpora\nof different domains does not affect the average\naccuracy on the three medical tasks. (c) The World\nWide Web is helpful for the MedQA task but is less\nbeneficial for the other two tasks since these two\ntasks rely on the medical literature.\nComparison of different prompting strate-\ngies\nTable 5 reports the performance of GPT-3.5\nw. RAG on PubmedQA, MedQA, and DRKG, with\nprompting strategies for eliciting responses. The\nresults show that: (a) compared with the chain-\nof-thought strategy and direct answer strategy, the\nCOT-Refine improves the LLM’s accuracy by re-\nflecting on its previous answer and improving by\nbetter incorporating the referential documents and\nchanging the reasoning steps. (b) The direct answer\nstrategy results show that directly answering a ques-\ntion without reasoning steps leads to performance\ndegradation.\nInvestigating the scaling laws in RAG\nWe\nfirst explore how the performance of MRAG scales\nwith the increase in the number of documents re-\ntrieved and concatenated for medical QA tasks. In\nthese experiments, we use GPT-3.5 as the back-\nbone LLM and utilize the RRF retriever and the\ncombined corpus for retrieval. Figure 3 shows the\nscaling curves of MRAG on the PubMedQA and\nMedQA tasks with different numbers of snippets\nk ∈{1, 2, 4, ..., 64}. The scaling curves are quite\ndifferent for different tasks. On the MedQA task,\nwe see roughly log-linear curves in the scaling plot\nfor k ≤32. However, on the PubMedQA task,\nthe ground truth documents can be accurately re-\nFigure 4: The scaling laws of LLMs on the MRAG\ntasks, with or without RAG.\ntrieved, and MRAG presents higher performance\nwhen k ≤2. Moreover, with k increases, more\nirrelevant documents are included in the prompt,\nhurting the accuracy.\nTo investigate the scaling law of model sizes, we\nuse the Qwen2.5 models of different sizes (0.5B,\n1.8B, 7B, 14B, 32B, 72B) while keeping the other\nsettings fixed. Figure 4 shows the scaling curves of\nQwen model sizes on the PubMedQA and MedQA\ntasks, with or without RAG. The four curves are\nroughly log-linear, demonstrating that the scaling\nlaw of LLMs (Kaplan et al., 2020) applies under\nRAG. In addition, the performance gaps between\nthe RAG and non-RAG curves increase slightly as\nthe model scales up since larger models are better\nat incorporating the referential documents into the\nreasoning steps.\n6\nConclusions\nIn this work, we presented the Medical Retrieval-\nAugmented\nGeneration\nbenchmark\n(MRAG-\nBench) and the MRAG-Toolkit, designed to sys-\ntematically evaluate and enhance the performance\nof LLMs through Retrieval-Augmented Genera-\ntion (RAG). Our MRAG-Bench spans four task\ncohorts across English and Chinese, providing a\nrobust evaluation framework for LLM-based RAG\nsystems.\nThe MRAG-Toolkit supports various\nretrieval approaches, algorithms, and LLMs, en-\nabling a detailed investigation of how different\ncomponents influence performance. Our exper-\niments demonstrated that RAG significantly im-\nproves LLM reliability in all MRAG tasks. We\nobserved that the choice of the referential corpus,\nretrieval methods, and prompting strategies heavily\ninfluence LLM performance. Additionally, larger\nLLMs benefit more from RAG, although there\nis a trade-off in readability for long-form ques-\ntion answering. The MRAG-Bench and MRAG-\nToolkit will serve as valuable resources for the re-\n"}, {"page": 7, "text": "search community, fostering further advancements\nin RAG and its applications in the medical industry.\nLimitations\nDespite the fact that we provide extensive ex-\nperiments of the MRAG benchmark in this work,\nthe following limitations remain: (a) Powerful\nlanguage models like Gemini (Reid et al., 2024),\nClaude-33, Grok4 are not evaluated due to resource\nlimitation. (b) There are literature in RAG that\nadopt more complicated workflow than our MRAG\nsystem (in Figure 2), such as iterative retrieval\n(Zhang et al., 2023a; Jiang et al., 2023). These\nmore advanced RAG strategies have not been eval-\nuated in our current version, but we will address\nthis aspect in our updated version.\nEthical considerations\nThe advancement of Large Language Mod-\nels (LLMs) and their integration with Retrieval-\nAugmented Generation (RAG) systems have signif-\nicant implications for various domains, particularly\nin high-stakes fields like healthcare. The Medical\nRetrieval-Augmented Generation (MRAG) bench-\nmark and MRAG-Toolkit developed in this study\naim to enhance the reliability and accuracy of\nLLMs in medical question answering (QA). Our\nwork leads to the following positive or negative\nsociatal impacts:\n• Positive Societal Impacts:\n– Enhanced Medical Information Access:\nBy\nintegrating\nRetrieval-Augmented\nGeneration (RAG) with Large Language\nModels (LLMs), our work can signifi-\ncantly improve access to up-to-date and\nreliable medical information.\nThis is\nparticularly valuable in clinical settings\nwhere timely and accurate information\nis crucial for patient care. The MRAG\nsystem can assist healthcare profession-\nals in making more informed decisions,\npotentially leading to better patient out-\ncomes.\n– Transparency and Accountability: The\nRAG approach enhances the trans-\nparency of LLMs by grounding their re-\nsponses in retrieved documents. This can\n3https://www.anthropic.com/news/\nclaude-3-family.\n4https://github.com/xai-org/grok-1\nfoster trust in AI systems as users can\ntrace back the source of the information\nprovided. Such transparency is essen-\ntial in the medical field where the prove-\nnance of information can impact clinical\ndecisions.\n– Open-Sourced Toolkit:\nThe MRAG-\nToolkit we have developed is open-\nsourced, promoting collaboration and\nfurther research in the field.\nBy pro-\nviding a standardized evaluation frame-\nwork, we enable other researchers to\nbuild upon our work, accelerating ad-\nvancements in medical AI and contribut-\ning to the broader scientific community.\n• Negative Societal Impacts:\n– Reliance on AI Systems: While RAG\nenhances the reliability of LLMs, there\nis a risk that over-reliance on AI sys-\ntems might emerge, potentially leading\nto complacency among healthcare pro-\nfessionals. It is crucial to maintain a\nbalance where AI serves as a supportive\ntool rather than a replacement for profes-\nsional judgment.\n– Impact on Healthcare Workforce: The\nintroduction of advanced AI systems like\nMRAG may impact the job market for\ncertain roles within the healthcare sector.\nWhile AI can augment human capabili-\nties, it may also lead to job displacement,\nnecessitating a focus on retraining and\nupskilling affected workers.\n– Potential for Bias: The retrieved docu-\nments and underlying datasets may con-\ntain biases that could be propagated or\neven amplified by the MRAG system.\nEnsuring that the sources used for re-\ntrieval are diverse and unbiased is essen-\ntial to mitigate this risk. It is our duty\nto further study the bias issue of MRAG-\nBench.\nBy carefully considering these positive and neg-\native impacts, our work aims to contribute to the\ndevelopment and deployment of responsible LLM-\nbased technologies in the medical domain.\n"}, {"page": 8, "text": "References\nAsma Ben Abacha, Eugene Agichtein, Yuval Pinter,\nand Dina Demner-Fushman. 2017. Overview of the\nmedical question answering task at trec 2017 liveqa.\nIn TREC, pages 1–12.\nAsma Ben Abacha, Yassine Mrabet, Mark Sharp,\nTravis R Goodwin, Sonya E Shooshan, and Dina\nDemner-Fushman. 2019. Bridging the gap between\nconsumers’ medication questions and trusted an-\nswers. In MedInfo, pages 25–29.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nAS Aruna, KR Remesh Babu, and K Deepthi. 2022. A\nsurvey of recent techniques in computational drug re-\npurposing. In International Conference on Intelligent\nSystems Design and Applications, pages 565–575.\nSpringer.\nSofia J Athenikos and Hyoil Han. 2010. Biomedical\nquestion answering: A survey. Computer methods\nand programs in biomedicine, 99(1):1–24.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. 2023. Qwen technical report. arXiv\npreprint arXiv:2309.16609.\nZhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao\nWu, Cheng Zhong, Jiajie Peng, Xuanjing Huang, and\nZhongyu Wei. 2023. Disc-medllm: Bridging gen-\neral large language models and real-world medical\nconsultation. arXiv preprint arXiv:2308.14346.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206–2240. PMLR.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. arXiv preprint arXiv:1704.00051.\nQingyu Chen, Jingcheng Du, Yan Hu, Vipina Kuttichi\nKeloth, Xueqing Peng, Kalpana Raja, Rui Zhang,\nZhiyong Lu, and Hua Xu. 2023a. Large language\nmodels in biomedical natural language processing:\nbenchmarks, baselines, and recommendations. arXiv\npreprint arXiv:2305.16326.\nZeming Chen, Alejandro Hernández Cano, Angelika\nRomanou, Antoine Bonnet, Kyle Matoba, Francesco\nSalvi, Matteo Pagliardini, Simin Fan, Andreas Köpf,\nAmirkeivan Mohtashami, et al. 2023b. Meditron-\n70b: Scaling medical pretraining for large language\nmodels. arXiv preprint arXiv:2311.16079.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anasta-\nsios Nikolas Angelopoulos, Tianle Li, Dacheng Li,\nHao Zhang, Banghua Zhu, Michael Jordan, Joseph E\nGonzalez, et al. 2024. Chatbot arena: An open plat-\nform for evaluating llms by human preference. arXiv\npreprint arXiv:2403.04132.\nGordon V Cormack, Charles LA Clarke, and Stefan\nBuettcher. 2009. Reciprocal rank fusion outperforms\ncondorcet and individual rank learning methods. In\nProceedings of the 32nd international ACM SIGIR\nconference on Research and development in informa-\ntion retrieval, pages 758–759.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,\nWei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and\nMaosong Sun. 2023. Ultrafeedback: Boosting lan-\nguage models with high-quality feedback. ArXiv,\nabs/2310.01377.\nNing Ding, Yujia Qin, Guang Yang, Fu Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Haitao Zheng, Jianfei\nChen, Yang Liu, Jie Tang, Juan Li, and Maosong\nSun. 2022. Delta tuning: A comprehensive study of\nparameter efficient methods for pre-trained language\nmodels. ArXiv, abs/2203.06904.\nGiacomo Frisoni, Miki Mizutani, Gianluca Moro, and\nLorenzo Valgimigli. 2022. Bioreader: a retrieval-\nenhanced text-to-text transformer for biomedical lit-\nerature. In Proceedings of the 2022 conference on\nempirical methods in natural language processing,\npages 5770–5793.\nXiangxiang Gao, Wei Zhu, Jiasheng Gao, and Congrui\nYin. 2023a. F-pabee: Flexible-patience-based early\nexiting for single-label and multi-label text classifica-\ntion tasks. In ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 1–5. IEEE.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023b. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\nMark E Glickman and Thomas Doan. 2010.\nThe\nuscf rating system. URL http://www. glicko. net/rat-\nings/rating. system. pdf.\nTongfeng Guan, Hongying Zan, Xiabing Zhou, Hongfei\nXu, and Kunli Zhang. 2020. Cmeie: construction\nand evaluation of chinese medical information extrac-\ntion dataset. In Natural Language Processing and\nChinese Computing: 9th CCF International Con-\nference, NLPCC 2020, Zhengzhou, China, October\n"}, {"page": 9, "text": "14–18, 2020, Proceedings, Part I 9, pages 270–282.\nSpringer.\nZhao Guo, Yuan Ni, Keqiang Wang, Wei Zhu, and\nGuotong Xie. 2021. Global attention decoder for\nChinese spelling error correction. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 1419–1428, Online. Association\nfor Computational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nMaría Herrero-Zazo, Isabel Segura-Bedmar, Paloma\nMartínez, and Thierry Declerck. 2013.\nThe ddi\ncorpus: An annotated corpus with pharmacological\nsubstances and drug–drug interactions. Journal of\nbiomedical informatics, 46(5):914–920.\nWilliam Hersh. 2024. Search still matters: informa-\ntion retrieval in the era of generative ai. Journal of\nthe American Medical Informatics Association, page\nocae014.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei\nZhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, et al. 2023.\nC-eval: A multi-level multi-discipline chinese eval-\nuation suite for foundation models. arXiv preprint\narXiv:2305.08322.\nVassilis N Ioannidis, Xiang Song, Saurav Manchanda,\nMufei Li, Xiaoqin Pan, Da Zheng, Xia Ning, Xiangx-\niang Zeng, and George Karypis. 2020. Drkg-drug\nrepurposing knowledge graph for covid-19. arXiv\npreprint arXiv:2010.09600.\nMinbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jae-\nwoo Kang. 2024.\nImproving medical reasoning\nthrough retrieval and self-reflection with retrieval-\naugmented large language models. arXiv preprint\narXiv:2401.15269.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. 2023.\nAc-\ntive retrieval augmented generation. arXiv preprint\narXiv:2305.06983.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2020. What disease\ndoes this patient have. A Large-scale Open Domain\nQuestion Answering Dataset from Medical Exams.\narXiv [cs. CL].\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\nQiao Jin, Won Kim, Qingyu Chen, Donald C Comeau,\nLana Yeganova, W John Wilbur, and Zhiyong Lu.\n2023. Medcpt: Contrastive pre-trained transformers\nwith large-scale pubmed search logs for zero-shot\nbiomedical information retrieval.\nBioinformatics,\n39(11):btad651.\nQiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu,\nDonald Wright, Thomas Huang, W John Wilbur,\nZhe He, Andrew Taylor, Qingyu Chen, et al. 2024.\nAgentmd: Empowering language agents for risk pre-\ndiction with large-scale clinical tool learning. arXiv\npreprint arXiv:2402.13225.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nAnastasia Krithara, Anastasios Nentidis, Konstantinos\nBougiatiotis, and Georgios Paliouras. 2023. Bioasq-\nqa: A manually curated corpus for biomedical ques-\ntion answering. Scientific Data, 10(1):170.\nAjay Kumar, Shashank Sheshar Singh, Kuldeep Singh,\nand Bhaskar Biswas. 2020. Link prediction tech-\nniques, applications, and performance: A survey.\nPhysica A: Statistical Mechanics and its Applications,\n553:124289.\nJakub Lála, Odhran O’Donoghue, Aleksandar Shtedrit-\nski, Sam Cox, Samuel G Rodriques, and Andrew D\nWhite. 2023. Paperqa: Retrieval-augmented gener-\native agent for scientific research. arXiv preprint\narXiv:2312.07559.\nHui Yi Leong, Yuheng Li, Yuqing Wu, Wenwen Ouyang,\nWei Zhu, Jiechao Gao, and Wei Han. 2025. Amas:\nAdaptively determining communication topology for\nllm-based multi-agent system. In Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing: Industry Track, pages 2061–\n2070.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai\nZhao, Yeyun Gong, Nan Duan, and Timothy Bald-\nwin. 2023a. Cmmlu: Measuring massive multitask\nlanguage understanding in chinese. arXiv preprint\narXiv:2306.09212.\nXiaonan Li, Kai Lv, Hang Yan, Tianya Lin, Wei Zhu,\nYuan Ni, Guo Tong Xie, Xiaoling Wang, and Xipeng\nQiu. 2023b. Unified demonstration retriever for in-\ncontext learning. ArXiv, abs/2305.04320.\n"}, {"page": 10, "text": "Xiepeng Li, Zhexi Zhang, Wei Zhu, Zheng Li, Yuan\nNi, Peng Gao, Junchi Yan, and Guotong Xie. 2019.\nPingan smart health and SJTU at COIN - shared task:\nutilizing pre-trained language models and common-\nsense knowledge in machine reading tasks. In Pro-\nceedings of the First Workshop on Commonsense\nInference in Natural Language Processing, pages\n93–98, Hong Kong, China. Association for Computa-\ntional Linguistics.\nYuheng Li, Jiechao Gao, Wei Han, Wenwen Ouyang,\nWei Zhu, and Hui Yi Leong. 2025. Ft-mdt: Extract-\ning decision trees from medical texts via a novel\nlow-rank adaptation method. In Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing: Industry Track, pages 65–76.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin A Raf-\nfel. 2022. Few-shot parameter-efficient fine-tuning\nis better and cheaper than in-context learning. Ad-\nvances in Neural Information Processing Systems,\n35:1950–1965.\nZequan Liu, Yi Zhao, Ming Tan, Wei Zhu, and\nAaron Xuxiang Tian. 2025. Para: Parameter-efficient\nfine-tuning with prompt aware representation adjust-\nment. arXiv preprint arXiv:2502.01033.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2024. Self-refine: Iterative refinement with\nself-feedback. Advances in Neural Information Pro-\ncessing Systems, 36.\nAakanksha Naik, Sravanthi Parasa, Sergey Feldman,\nLucy Lu Wang, and Tom Hope. 2021. Literature-\naugmented clinical outcome prediction.\narXiv\npreprint arXiv:2111.08374.\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023. Capabili-\nties of gpt-4 on medical challenge problems. arXiv\npreprint arXiv:2303.13375.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on health,\ninference, and learning, pages 248–260. PMLR.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 11:1316–1331.\nVipula Rawte, Amit Sheth, and Amitava Das. 2023. A\nsurvey of hallucination in large foundation models.\narXiv preprint arXiv:2309.05922.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-\nrat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-\nlocking multimodal understanding across millions of\ntokens of context. arXiv preprint arXiv:2403.05530.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nKhaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno,\nDavid Stutz, Ellery Wulczyn, Fan Zhang, Tim\nStrother, Chunjong Park, Elahe Vedadi, et al. 2024.\nCapabilities of gemini models in medicine. arXiv\npreprint arXiv:2404.18416.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\net al. 2023a. Large language models encode clinical\nknowledge. Nature, pages 1–9.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Le Hou, Kevin Clark, Stephen\nPfohl, Heather Cole-Lewis, Darlene Neal, et al.\n2023b. Towards expert-level medical question an-\nswering with large language models. arXiv preprint\narXiv:2305.09617.\nShamane Siriwardhana, Rivindu Weerasekera, Elliott\nWen, Tharindu Kaluarachchi, Rajib Rana, and\nSuranga Nanayakkara. 2023. Improving the domain\nadaptation of retrieval augmented generation (rag)\nmodels for open domain question answering. Trans-\nactions of the Association for Computational Linguis-\ntics, 11:1–17.\nHaixia Sun, Jin Xiao, Wei Zhu, Yilong He, Sheng\nZhang, Xiaowei Xu, Li Hou, Jiao Li, Yuan Ni, and\nGuotong Xie. 2020. Medical knowledge graph to\nenhance fraud, waste, and abuse detection on claim\ndata: Model development and performance evalua-\ntion. JMIR Med Inform, 8(7):e17653.\nTianxiang Sun, Xiangyang Liu, Wei Zhu, Zhichao Geng,\nLingling Wu, Yilong He, Yuan Ni, Guotong Xie, Xu-\nanjing Huang, and Xipeng Qiu. 2022.\nA simple\nhash-based early exiting approach for language un-\nderstanding and generation. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 2409–2421, Dublin, Ireland. Association for\nComputational Linguistics.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, et al. 2022. Challenging big-bench tasks and\nwhether chain-of-thought can solve them.\narXiv\npreprint arXiv:2210.09261.\nOlivier Taboureau, Sonny Kim Nielsen, Karine Au-\ndouze, Nils Weinhold, Daniel Edsgärd, Francisco S\nRoque, Irene Kouskoumvekaki, Alina Bora, Ramona\nCurpan, Thomas Skøt Jensen, et al. 2010. Chemprot:\n"}, {"page": 11, "text": "a disease chemical biology database. Nucleic acids\nresearch, 39(suppl_1):D367–D372.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogenous benchmark for zero-shot evalua-\ntion of information retrieval models. arXiv preprint\narXiv:2104.08663.\nAaron Tian, Yi Zhao, Congrui Yin, Wei Zhu, Xing Tian,\nand Yi Ge. 2024a. Fanlora: Fantastic loras and where\nto find them in large language model fine-tuning. In\nProceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing: Industry\nTrack, pages 515–528.\nShubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai,\nQingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu\nChen, Won Kim, Donald C Comeau, et al. 2024b.\nOpportunities and challenges for chatgpt and large\nlanguage models in biomedicine and health. Brief-\nings in Bioinformatics, 25(1):bbad493.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Cantón Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel M. Kloumann, A. V.\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aure-\nlien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv, abs/2307.09288.\nLi Wang, Wei Zhu, Sihang Jiang, Sheng Zhang, Ke-\nqiang Wang, Yuan Ni, Guo Tong Xie, and Yanghua\nXiao. 2020. Mining infrequent high-quality phrases\nfrom domain-specific corpora. Proceedings of the\n29th ACM International Conference on Information\n& Knowledge Management.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2023a. Improving\ntext embeddings with large language models. arXiv\npreprint arXiv:2401.00368.\nPengfei Wang, Huanran Zheng, Silong Dai, Wenjing\nYue, Wei Zhu, and Xiaoling Wang. 2024. Ts-tcd:\nTriplet-level cross-modal distillation for time-series\nforecasting using large language models.\narXiv\npreprint arXiv:2409.14978.\nPengfei Wang, Huanran Zheng, Qi’ao Xu, Silong Dai,\nYiqiao Wang, Wenjing Yue, Wei Zhu, Tianwen Qian,\nand Liang Zhao. 2025. Ts-htfa: Advancing time-\nseries forecasting via hierarchical text-free alignment\nwith large language models. Symmetry, 17(3):401.\nXidong Wang, Guiming Hardy Chen, Dingjie Song,\nZhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng\nJiang, Jianquan Li, Xiang Wan, Benyou Wang, et al.\n2023b. Cmb: A comprehensive medical benchmark\nin chinese. arXiv preprint arXiv:2308.08833.\nXuwu Wang, Lihan Chen, Wei Zhu, Yuan Ni, Guo Tong\nXie, Deqing Yang, and Yanghua Xiao. 2023c. Multi-\ntask entity linking with supervision from a taxon-\nomy. Knowledge and Information Systems, 65:4335\n– 4358.\nYubo Wang, Xueguang Ma, and Wenhu Chen. 2023d.\nAugmenting black-box llms with medical textbooks\nfor clinical question answering.\narXiv preprint\narXiv:2309.02233.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and\nDenny Zhou. 2022. Chain of thought prompting\nelicits reasoning in large language models. ArXiv,\nabs/2201.11903.\nWei Zhu Wenjing Yue and Xiaoling Wang. 2023.\nTcmeb: Performance evaluation of large language\nmodels based on traditional chinese medicine\nbenchmarks.\nhttps://github.com/ywjawmw/\nShenNong-TCM-Evaluation-BenchMark.\nChaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang,\nand Weidi Xie. 2023.\nPmc-llama: Further fine-\ntuning llama on medical papers.\narXiv preprint\narXiv:2304.14454.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighof. 2023. C-pack: Packaged resources to\nadvance general chinese embedding. arXiv preprint\narXiv:2309.07597.\nYongkang Xiao, Yu Hou, Huixue Zhou, Gayo Di-\nallo, Marcelo Fiszman, Julian Wolfson, Li Zhou,\nHalil Kilicoglu, You Chen, Chang Su, et al. 2024.\nRepurposing non-pharmacological interventions for\nalzheimer’s disease through link prediction on\nbiomedical literature. Scientific Reports, 14(1):8693.\nTianfang Xie, Tianjing Li, Wei Zhu, Wei Han, and\nYi Zhao. 2024.\nPedro: Parameter-efficient fine-\ntuning with prompt dependent representation modifi-\ncation. arXiv preprint arXiv:2409.17834.\nYi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiao-\nhong Liu, Yue Fan, Qing Li, and Yuntao Du. 2024.\nParameter-efficient fine-tuning for pre-trained vision\nmodels: A survey. ArXiv, abs/2402.02242.\nGuangzhi\nXiong,\nQiao\nJin,\nZhiyong\nLu,\nand\nAidong Zhang. 2024.\nBenchmarking retrieval-\naugmented generation for medicine. arXiv preprint\narXiv:2402.13178.\n"}, {"page": 12, "text": "Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui\nTao, and Fu Lee Wang. 2023. Parameter-efficient\nfine-tuning methods for pretrained language mod-\nels:\nA critical review and assessment.\nArXiv,\nabs/2312.12148.\nJiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun\nChen, and Qian Lou. 2024. Badrag: Identifying vul-\nnerabilities in retrieval augmented generation of large\nlanguage models. arXiv preprint arXiv:2406.00083.\nTianhan Xue and Rustum Roy. 2003. Studying tradi-\ntional chinese medicine. Science, 300(5620):740–\n741.\nEllen Yi-Ge, Jiechao Gao, Wei Han, and Wei Zhu.\n2024.\nDrum: Learning demonstration retriever\nfor large multi-modal models.\narXiv preprint\narXiv:2412.07619.\nHuiming Yin, Kun Wang, Ruyu Yang, Yanfang Tan,\nQiang Li, Wei Zhu, and Suzi Sung. 2024. A ma-\nchine learning model for predicting acute exacerba-\ntion of in-home chronic obstructive pulmonary dis-\nease patients. Computer Methods and Programs in\nBiomedicine, 246:108005.\nWenjing Yue, Xiaoling Wang, Wei Zhu, Ming Guan,\nHuanran Zheng, Pengfei Wang, Changzhi Sun, and\nXin Ma. 2024.\nTcmbench:\nA comprehensive\nbenchmark for evaluating large language models\nin traditional chinese medicine.\narXiv preprint\narXiv:2406.01126.\nCyril Zakka, Rohan Shad, Akash Chaurasia, Alex R\nDalal, Jennifer L Kim, Michael Moor, Robyn Fong,\nCurran Phillips, Kevin Alexander, Euan Ashley,\net al. 2024.\nAlmanac—retrieval-augmented lan-\nguage models for clinical medicine.\nNEJM AI,\n1(2):AIoa2300068.\nFengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin\nLiu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and\nWeizhu Chen. 2023a. Repocoder: Repository-level\ncode completion through iterative retrieval and gen-\neration. arXiv preprint arXiv:2303.12570.\nJingfan Zhang, Yi Zhao, Dan Chen, Xing Tian, Huanran\nZheng, and Wei Zhu. 2024a. Milora: Efficient mix-\nture of low-rank adaptation for large language models\nfine-tuning. arXiv preprint arXiv:2410.18035.\nJingfang Zhang, Ming Tan, Pengyu Dai, and Wei-Guo\nZhu. 2023b.\nLeco: Improving early exiting via\nlearned exits and comparison-based exiting mech-\nanism. In Annual Meeting of the Association for\nComputational Linguistics.\nJuyuan Zhang, Jiechao Gao, Wenwen Ouyang, Wei Zhu,\nand Hui Yi Leong. 2025. Time-llama: Adapting\nlarge language models for time series modeling via\ndynamic low-rank adaptation. In Proceedings of the\n63rd Annual Meeting of the Association for Com-\nputational Linguistics (Volume 4: Student Research\nWorkshop), pages 1145–1157.\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng\nShen, Matei Zaharia, Ion Stoica, and Joseph E Gon-\nzalez. 2024b. Raft: Adapting language model to do-\nmain specific rag. arXiv preprint arXiv:2403.10131.\nXinpeng Zhang, Ming Tan, Jingfan Zhang, and Wei\nZhu. 2023c. Nag-ner: a unified non-autoregressive\ngeneration framework for various ner tasks. In An-\nnual Meeting of the Association for Computational\nLinguistics.\nYuming Zhang, Xiangxiang Gao, Wei Zhu, and Xiaol-\ning Wang. 2023d. Fastner: Speeding up inferences\nfor named entity recognition tasks. In International\nConference on Advanced Data Mining and Applica-\ntions.\nYuming Zhang, Peng Wang, Ming Tan, and Wei-Guo\nZhu. 2023e. Learned adapters are better than man-\nually designed adapters. In Annual Meeting of the\nAssociation for Computational Linguistics.\nZhen Zhang, Wei Zhu, Jinfan Zhang, Peng Wang, Rize\nJin, and Tae-Sun Chung. 2022. PCEE-BERT: Ac-\ncelerating BERT inference via patient and confident\nearly exiting. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022, pages 327–338,\nSeattle, United States. Association for Computational\nLinguistics.\nZhexi Zhang, Wei Zhu, Junchi Yan, Peng Gao, and\nGuowang Xie. 2021. Automatic student network\nsearch for knowledge distillation. 2020 25th Inter-\nnational Conference on Pattern Recognition (ICPR),\npages 2446–2453.\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren\nWang, Yunteng Geng, Fangcheng Fu, Ling Yang,\nWentao Zhang, and Bin Cui. 2024.\nRetrieval-\naugmented generation for ai-generated content: A\nsurvey. arXiv preprint arXiv:2402.19473.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nSurvey of Large Language Models. arXiv e-prints,\narXiv:2303.18223.\nHuanran Zheng, Wei Zhu, Pengfei Wang, and Xiaol-\ning Wang. 2023. Candidate soups: Fusing candi-\ndate results improves translation quality for non-\nautoregressive translation. ArXiv, abs/2301.11503.\nHuanran Zheng, Wei Zhu, and Xiaoling Wang. 2024a.\nNat4at: Using non-autoregressive translation makes\nautoregressive translation faster and better. In Pro-\nceedings of the ACM on Web Conference 2024, pages\n4181–4192.\nHuanran Zheng, Wei Zhu, and Xiaoling Wang. 2024b.\nSca: Selective compression attention for efficiently\nextending the context window of large language mod-\nels. In Findings of the Association for Computational\nLinguistics: EMNLP 2024, pages 6166–6178.\n"}, {"page": 13, "text": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2024c.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. Advances in Neural Information Processing\nSystems, 36.\nXiaofeng Zhou, Yuan Ni, Guotong Xie, Wei Zhu, Cai\nChen, Tianhao Wang, and Zhigang Pan. 2019. Anal-\nysis of the health information needs of diabetics in\nchina. In MEDINFO 2019: Health and Wellbeing\ne-Networks for All, pages 487–491. IOS Press.\nWei Zhu. 2021a. Leebert: Learned early exit for bert\nwith cross-level optimization. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 2968–2980.\nWei Zhu. 2021b. Mvp-bert: Multi-vocab pre-training\nfor chinese bert. In Annual Meeting of the Associa-\ntion for Computational Linguistics.\nWei Zhu, Yilong He, Ling Chai, Yuanchun Fan, Yuan Ni,\nGuo Tong Xie, and Xiaoling Wang. 2021a. paht_nlp\n@ mediqa 2021: Multi-grained query focused multi-\nanswer summarization. In Workshop on Biomedical\nNatural Language Processing.\nWei Zhu, Wenfeng Li, Xiaoling Wang, Wendi Ji, Yuan-\nbin Wu, Jin Chen, Liang Chen, and Buzhou Tang.\n2023a. Extracting decision trees from medical texts:\nAn overview of the text2dt track in chip2022. In\nHealth Information Processing. Evaluation Track Pa-\npers, pages 89–102, Singapore. Springer Nature Sin-\ngapore.\nWei Zhu, Wenfeng Li, Xiaoling Wang, Wendi Ji, Yuan-\nbin Wu, Jin Chen, Liang Chen, and Buzhou Tang.\n2023b. Extracting decision trees from medical texts:\nAn overview of the text2dt track in chip2022. In\nHealth Information Processing. Evaluation Track Pa-\npers, pages 89–102, Singapore. Springer Nature Sin-\ngapore.\nWei Zhu, Yuan Ni, Xiaoling Wang, and Guotong Xie.\n2021b. Discovering better model architectures for\nmedical query understanding. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies: Industry Papers, pages\n230–237, Online. Association for Computational Lin-\nguistics.\nWei Zhu, Yuan Ni, Guo Tong Xie, Xiaofeng Zhou,\nand Cai Chen. 2019a. The dr-kgqa system for au-\ntomatically answering medication related questions\nin chinese. 2019 IEEE International Conference on\nHealthcare Informatics (ICHI), pages 1–6.\nWei Zhu, Yuan Ni, Guotong Xie, Xiaofeng Zhou, and\nCai Chen. 2019b. The dr-kgqa system for automati-\ncally answering medication related questions in chi-\nnese. In 2019 IEEE International Conference on\nHealthcare Informatics (ICHI), pages 1–6. IEEE.\nWei Zhu and Ming Tan. 2023. SPT: Learning to se-\nlectively insert prompts for better prompt tuning.\nIn Proceedings of the 2023 Conference on Empir-\nical Methods in Natural Language Processing, pages\n11862–11878, Singapore. Association for Computa-\ntional Linguistics.\nWei Zhu, Aaron Xuxiang Tian, Congrui Yin, Yuan\nNi, Xiaoling Wang, and Guotong Xie. 2024. Iapt:\nInstruction-aware prompt tuning for large language\nmodels. arXiv preprint arXiv:2405.18203.\nWei Zhu, Peifeng Wang, Yuan Ni, Guo Tong Xie, and\nXiaoling Wang. 2023c. Badge: Speeding up bert\ninference after deployment via block-wise bypasses\nand divergence-based early exiting. In Annual Meet-\ning of the Association for Computational Linguistics.\nWei Zhu, Peng Wang, Xiaoling Wang, Yuan Ni, and\nGuotong Xie. 2023d. Acf: aligned contrastive fine-\ntuning for language and vision tasks. In ICASSP\n2023-2023 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n1–5. IEEE.\nWei Zhu, Xiaoling Wang, Mosha Chen, and Buzhou\nTang. 2023e. Overview of the promptcblue shared\ntask in chip2023. ArXiv, abs/2312.17522.\nWei Zhu, Xiaoling Wang, Yuan Ni, and Guotong Xie.\n2021c. GAML-BERT: Improving BERT early exit-\ning by gradient aligned mutual learning. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 3033–3044,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nWei Zhu, Xiaoling Wang, Huanran Zheng, Mosha Chen,\nand Buzhou Tang. 2023. PromptCBLUE: A Chinese\nPrompt Tuning Benchmark for the Medical Domain.\narXiv e-prints, arXiv:2310.14151.\nWei Zhu, Xiaofeng Zhou, Keqiang Wang, Xun Luo,\nXiepeng Li, Yuan Ni, and Guotong Xie. 2019c. Panlp\nat mediqa 2019: Pre-trained language models, trans-\nfer learning and knowledge distillation. In Proceed-\nings of the 18th BioNLP Workshop and Shared Task,\npages 380–388.\nYuhui Zuo, Wei Zhu, and Guoyong GUET Cai. 2022.\nContinually detection, rapidly react: Unseen rumors\ndetection based on continual prompt-tuning.\nIn\nProceedings of the 29th International Conference\non Computational Linguistics, pages 3029–3041,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nPierre Zweigenbaum. 2003.\nQuestion answering in\nbiomedicine. In Proceedings Workshop on Natural\nLanguage Processing for Question Answering, EACL,\nvolume 2005, pages 1–4. Citeseer.\n"}, {"page": 14, "text": "A\nAppendix: Related works\nA.1\nQuestion answering in bio-medicine\nIn the LLM era, almost all the bio-medical infor-\nmation needs are expressed as natural language\nquestions (Zweigenbaum, 2003; Athenikos and\nHan, 2010; Zhu et al., 2023; Zhu et al., 2023b),\nsuch as user queries about healthcare, or searches\nfor specific knowledge from medical practitioners,\nor the need to make the knowledge structural from\nan unstructured document for bio-medical knowl-\nedge graph construction. LLMs, both open-domain\nand domain-specific, have demonstrated outstand-\ning potential for medical QA tasks (Achiam et al.,\n2023; Anil et al., 2023; Singhal et al., 2023a,b;\nNori et al., 2023; Chen et al., 2023a; Saab et al.,\n2024), with chain-of-thought prompting or in-\ncontext learning. However, due to the knowledge-\nintensive nature, intuitively, RAG could help the\nLLMs achieve better performance and be more\nreliable by grounding their responses to the re-\ntrieved referential documents. In this work, our\nproposed MRAG-Bench consists of four cohorts of\nbio-medical tasks, reflecting different information-\nseeking needs in this domain. Through experi-\nments, we can see that some of the MRAG tasks\nare challenging for LLMs, even with the help of\nRAG.\nB\nMRAG-Bench datasets\nIn this section, we provide detailed introductions,\nstatistics, filtering procedures, and licensing infor-\nmation for all the tasks in the MRAG-Bench. Our\nMRAG-Bench consists of 13 tasks, 9 of which\nwere previously open-sourced by their original au-\nthors. We construct two link prediction datasets\nfrom open-sourced knowledge graphs and curate\ntwo novel Chinese datasets.\nB.1\nPreviously open-sourced datasets\nMMLU-Med.\nMassive Multitask Language\nUnderstanding (MMLU) (Hendrycks et al., 2020)\nis a benchmark for the evaluation of the multi-\ntask learning capability of language models. The\ndataset is released under the MIT License and\nin https://github.com/hendrycks/test. The bench-\nmark contains a variety of 57 different tasks. To\nmeasure the performance of medical RAG systems,\nwe select six tasks related to biomedicine following\n(Singhal et al., 2023a), including anatomy, clinical\nknowledge, professional medicine, human genetics,\ncollege medicine, and college biology. The sub-\nset is collectively denoted as MMLU-Med. Only\nthe test set of each task is used in our benchmark,\nwhich contains 1089 questions in total.\nMedQA-US. MedQA (Jin et al., 2020) is a multi-\nchoice question answering (MCQA) dataset col-\nlected from professional medical board exams. The\ndataset is released under the CC BY 4.0 (Creative\nCommons Attribution 4.0 International) license\nand released in https://github.com/jind11/MedQA.\nSpecifically, we focus on the English part, which\nincludes real-world questions from the US Med-\nical Licensing Examination (MedQA-US). Thus,\nthe subset of 1273 four-option test questions are\nincluded in our MRAG-Bench.\nMedMCQA. MedMCQA (Pal et al., 2022) con-\ntains 194k multi-choice questions collected from\nIndian medical entrance exams. The dataset is\nreleased under the MIT License and in https://\nmedmcqa.github.io/. The questions cover a range\nof 2.4k healthcare topics and 21 medical subjects.\nSince the ground truth of its test set is not pro-\nvided, the dev set of the original MedMCQA is\nchosen for MRAG-Bench, including 4183 medical\nquestions. To ensure balance in the task composi-\ntion, we randomly selected 1,500 test samples from\nMedMCQA’s dev set.\nPubMedQA. PubMedQA (Jin et al., 2019) is\na biomedical research QA dataset. The dataset is\nreleased under the CC BY 4.0 (Creative Commons\nAttribution 4.0 International) license and released\nin https://pubmedqa.github.io/. It has 1k manually\nannotated questions constructed from PubMed ab-\nstracts. To test the capability of RAG systems to\nfind related documents and answer the question\naccordingly, we discard the relevant context for\neach question originally included in the dataset.\nA test set of 500 PubMedQA samples is adopted\nfor MRAG-Bench following (Lála et al., 2023).\nThe possible answer to a PubMedQA question can\nbe yes/no/maybe, reflecting the authenticity of the\nquestion statement based on biomedical literature.\nBioASQ. BioASQ (Krithara et al., 2023) is an\nannual competition for biomedical QA, which in-\ncludes both the information retrieval track (Task\nA) and the machine reading comprehension track\n(Task B). The dataset is released under the\nCreative Commons Attribution-NonCommercial-\nShareAlike 4.0 International License (CC BY-NC-\nSA 4.0) and released in http://bioasq.org/. To\nleverage the resources of BioASQ for our medi-\ncal RAG benchmark, we select the Yes/No ques-\n"}, {"page": 15, "text": "tions in the ground truth test set of Task B from\nthe most recent five years (2019-2023), including\n618 questions. In the original task, questions are\nconstructed based on biomedical literature, and the\nground truth document snippets are provided as a\nbasis for machine reading comprehension. We dis-\ncard the provided document snippets and only keep\nthe questions and answer choices for the MRAG-\nBench.\nChemProt.\nChemProt (Taboureau et al.,\n2010) is a specialized task in the field of bio-\nmedical information extraction (IE), focusing\non\nextracting\ninteractions\namong\ndiseases,\nchemical compounds, and genes from medical\narticles. This task is essential for understanding\nbiochemical\nprocesses\nand\nadvancing\ndrug\ndiscovery and development.\nThe ChemProt\ncorpus is distributed under a Creative Commons\nAttribution-NonCommercial-ShareAlike\n3.0\nUnported License, and it is publicly available\nat\nhttps://biocreative.bioinformatics.udel.edu/\nnews/corpora/chemprot-corpus-biocreative-vi/.\nFor MRAG-Bench, we include its test set, which\nconsists of 800 samples.\nDDI. The DDI Extraction (DDI) 2013 task\n(Herrero-Zazo et al., 2013) is a significant bench-\nmark in the field of bio-medical natural language\nprocessing (NLP), focusing on the extraction of\ndrug-drug interactions (DDIs) from textual data.\nThe task is structured to challenge participants\nto develop and refine algorithms capable of ac-\ncurately parsing complex biomedical texts to de-\ntect and categorize these interactions. This task\nis crucial for improving our understanding of how\ndifferent drugs interact, vital for drug safety, pa-\ntient care, and the development of new pharma-\nceutical treatments. This task relies on the DDI\ncorpus, which includes MedLine abstracts and doc-\numents from the DrugBank database. These docu-\nments have been manually annotated with pharma-\ncological substances and drug-drug interactions.\nThe DDI corpus is licensed under the Creative\nCommons Attribution-NonCommercial 4.0 Inter-\nnational (CC BY-NC 4.0) license and is available\nat https://github.com/isegura/DDICorpus.\nCMeIE. The Chinese Medical Information Ex-\ntraction (CMeIE) dataset (Guan et al., 2020), part\nof the CHIP-2020 shared tasks5, is a crucial re-\nsource for advancing Chinese natural language\nprocessing (NLP) in the medical domain. This\n5http://cips-chip.org.cn/2020/eval2\ndataset is designed to facilitate medical informa-\ntion extraction by identifying entities and their rela-\ntionships within clinical text, following predefined\nschema constraints. This dataset is released un-\nder the Creative Commons Attribution 4.0 Interna-\ntional (CC BY 4.0) License and is available at http:\n//biendata.com/competition/chip_2020_2. Since the\noriginal authors keep its test set private, we ran-\ndomly sample a 1,500-sample subset from its dev\nset for MRAG-Bench.\nMultiMedQA. The MultiMedQA dataset (Sing-\nhal et al., 2023b) is a comprehensive collection\nof medical question sets designed to support the\ndevelopment and evaluation of long-form question-\nanswering (LFQA) systems in the healthcare do-\nmain.\nThis dataset is constructed by combin-\ning 1066 questions curated from three distinct\nsources: HealthSearchQA (Singhal et al., 2023a),\nLiveQA (Abacha et al., 2017), and MedicationQA\n(Abacha et al., 2019).\nThe diverse nature of\nthese sources ensures a wide coverage of medi-\ncal topics, ranging from general health inquiries\nto specific medication-related questions. The Mul-\ntiMedQA dataset is released under the Creative\nCommons Attribution-NonCommercial 4.0 Inter-\nnational (CC BY-NC 4.0) license and is avail-\nable at https://huggingface.co/datasets/katielink/\nhealthsearchqa/tree/main.\nB.2\nConstruction of the two link prediction\ntasks\nSource. Our link prediction (LP) tasks are con-\nstructed based on two open-sourced knowledge\ngraphs (KGs), ADInt (Xiao et al., 2024) and DRKG\n(Ioannidis et al., 2020).\n• ADInt. ADInt is a comprehensive knowledge\ngraph designed to facilitate the identification\nof novel pharmaceutical interventions (PIs)\nfor Alzheimer’s Disease (AD). ADInt aims\nto accelerate research and discovery in AD\ntherapeutics by integrating and organizing di-\nverse biomedical data. ADInt is distributed\nunder the Creative Commons Attribution 4.0\nInternational (CC BY 4.0) license and is avail-\nable at https://github.com/zhang-informatics/\nADInt. ADInt harnesses the power of a knowl-\nedge graph to map complex relationships be-\ntween various entities related to Alzheimer’s\nDisease. These entities include genes, pro-\nteins, biological pathways, drugs, clinical tri-\nals, and research publications. Researchers\n"}, {"page": 16, "text": "can identify novel drug candidates and repur-\nposing opportunities by exploring connections\nbetween known compounds and AD-related\ntargets.\n• DRKG. The Drug Repurposing Knowledge\nGraph (DRKG) is a comprehensive drug dis-\ncovery and repurposing research resource.\nDRKG integrates information from various\nbiological databases to create a unified knowl-\nedge graph that includes genes, diseases,\ndrugs, biological processes, and molecular\nfunctions.\nBy representing these relation-\nships in a structured format, DRKG enables\nresearchers to identify potential drug repurpos-\ning opportunities, uncover novel therapeutic\ntargets, and better understand the complex in-\nteractions within biological systems. DRKG\nis released under the MIT License and is avail-\nable at https://github.com/gnn4dr/DRKG.\nDataset Collection. In both ADInt and DRKG,\na piece of knowledge is expressed as a triplet (sub-\nject, predicate, object), in which the subject and ob-\nject are entities from the knowledge graphs, and the\npredicate represents the type of relation between\nthe two entities. The relation type is pre-defined as\nthe schema of the KG. We randomly select 1,500\ntriplets from each of the two KGs to evaluate large\nlanguage models in MRAG-Bench.\nFormatting. For constructing a link prediction\ntask for large language models, a triplet is then\nformatted into a standardized multi-choice format.\nThe question uses the following template to include\nthe subject and object’s names,\n1 How are the following entity pairs\nconnected?\n2 Entity 1: <ent1 >\n3 Entity 2: <ent2 >\nMoreover, the correct answer is the predicate name\nof the triplet. The distractors (incorrect options) are\nthe other relation types in the knowledge graphs.\nQuality Assurance. The dataset undergoes a\nfinal review for quality assurance. A pool of 15\nmedical experts from the united states with medical\ndoctoral degrees and different fields is divided into\nfive groups, each containing three experts. A group\nwill be given a link prediction question in the multi-\nchoice format. These experts participate in this\nproject as volunteers and are paid 10 US dollars per\nhour. They will check whether (a) the whole ques-\ntion is correctly formatted, (b) the answer choices\nare plausible, and (c) the correct answers are accu-\nrate. The screenshot of the annotation webpage is\npresented in Figure 5. If not all of the experts in\nthe group agree that the question is valid in all the\nabove three aspects, this question will be filtered\nout. After the review process, no question is dis-\ncarded. This result also reflects the high quality of\nthe source KGs.\nDataset Compilation. After the quality assur-\nance process, the questions are compiled into a\nstructured dataset. Each entry in the dataset in-\ncludes the question text, the answer choices, and\nthe correct answer label.\nB.3\nCuration of MRAG-TCM\nFor Chinese MCQA, since the exam questions of\nWestern medicine are well covered by the MMLU-\nMed, MedQA-US, and MedMCQA tasks, we con-\nstruct an MCQA dataset containing 1200 test sam-\nples for Traditional Chinese Medicine (TCM) (Xue\nand Roy, 2003), and refer to this dataset as MRAG-\nTCM. We now describe how we construct the\nMRAG-TCM dataset.\nSource. The MRAG-TCM dataset is meticu-\nlously designed to evaluate the performance of\nlarge-scale language models in the context of the\nTraditional Chinese Medicine Practitioners Qualifi-\ncation Examination (TCMPQE)6. This exam is an\nimportant test that evaluates candidates’ theoretical\nknowledge, comprehension, and comprehensive\napplication abilities in TCM. The exam content is\nmainly divided into the following 12 topics cov-\nering Basic TCM knowledge, classical literature,\nclinical TCM, basic Western medicine comprehen-\nsive, and basics in medical humanities:\n• TCM basic theory, which covers basic con-\ncepts such as Yin-Yang, Five Elements, Zang-\nFu theory, Qi, Blood, and Body Fluids.\n• TCM diagnostics, including the four diag-\nnostic methods: observation, listening and\nsmelling, inquiry, and palpation, as well as\nthe fundamental theories of differentiation and\ntreatment.\n• Chinese Materia Medica, which studies the\nproperties, channels, effects, and compatibil-\nity of Chinese medicinal herbs.\n• Formulae of TCM, which focuses on the com-\nposition, effects, indications, and applications\n6https://www.tcmtest.org.cn/.\n"}, {"page": 17, "text": "Figure 5: The screenshot of the annotation web-page for quality assurance of the link prediction tasks.\n"}, {"page": 18, "text": "of commonly used TCM formulas.\n• TCM classics, which cover content from clas-\nsical TCM texts, including the \"Huangdi\nNeijing\" (Yellow Emperor’s Inner Canon),\n\"Shang Han Lun\" (Treatise on Cold Damage),\n\"Jingui Yaolue\" (Essential Prescriptions of the\nGolden Cabinet), and Warm Diseases Theory,\nproviding theoretical and clinical foundations\nfor TCM practice.\n• TCM internal medicine, which studies the\nTCM diagnosis and treatment of internal dis-\neases.\n• TCM surgery, which studies the TCM diagno-\nsis and treatment of surgical diseases.\n• TCM gynecology, which studies the TCM di-\nagnosis and treatment of gynecological dis-\neases.\n• TCM pediatrics, which studies the TCM diag-\nnosis and treatment of pediatric diseases.\n• Acupuncture, which studies acupuncture treat-\nment methods and their clinical applications.\n• Western medicine comprehensive, which\nassesses the basic knowledge of Western\nmedicine, clinical professional knowledge,\nand infectious disease knowledge required for\nclinical practice, including basics of diagnos-\ntics, internal medicine (not tested for appren-\ntice or specialty practitioners), and infectious\ndiseases.\n• Medical humanities, which assesses the legal\nregulations and ethical knowledge necessary\nfor clinical practice, including Medical Ethics\nand Health Laws.\nWe include multi-choice questions collected\nfrom publicly available TCM qualification exami-\nnation question sets provided by (Yue et al., 2024),\nreleased under the CC BY 4.0 license, allowing for\nconverting the original exam questions into a new\nformat.\nQuestion Collection. Multi-choice questions\nare gathered from the above sources. To ensure\nthe diversity of topics and difficulty levels, we se-\nlect 100 questions from the above 12 topics. The\nquestions are then formatted into a standardized\nmulti-choice format. Each question includes a stem\n(the main question), several distractors (incorrect\noptions), and one correct answer. This formatting\naligns with the structure commonly used in medi-\ncal board exams. We then convert this structured\nformat into a text sequence, allowing LLMs to read\nthe questions.\nQuality Assurance. The dataset undergoes a\nreview process for quality assurance. A pool of 15\nTCM medical experts from China is divided into\nfive groups, each containing three experts. These\nannotators participate in this project as volunteers\nand are paid 60 RMB per hour. A group will be\ngiven a TCM exam question in the multi-choice\nformat. They will check whether (a) the content\nof the question is not outdated, irrelevant, or not\nsuitable for a multi-choice format, (b) the whole\nsample is correctly formatted, and (c) the answers\nare accurate. The screenshot of the annotation web-\npage is presented in Figure 6. If not all of the\nexperts in the group agree that the question is valid\nin all the above three aspects, this question will be\nfiltered out. If a question is filtered out, another\nquestion with the same TCM topic is sampled from\nthe sources and will go through the same review\nprocess.\nDataset Compilation.\nAfter the quality as-\nsurance process, the questions are compiled into\na structured dataset.\nEach entry in the dataset\nincludes the question text, the possible answer\nchoices, and the correct answer label.\nB.4\nCuration of MRAG-CLFQA\nFor the Chinese long-form question-answering\ntask, we collected questions suitable for the re-\ntrieval augmented generation (RAG) system.\nSource. For the Chinese long-form question-\nanswering task, we collect 1,943 user queries from\nan online medical consultation platform7. Each\nuser is prompted to consent to data collection when\ncollecting the queries. Furthermore, we ensure that\nno personal information is included in the dataset\nreviewing step.\nDataset collection and filtering. The collected\ndataset covers a wide range of topics, including (a)\nSymptoms, diagnosis, and treatment of illnesses.\n(b) prevention, vaccinations, or quarantine for in-\nfectious diseases. (c) lifestyle and wellness ad-\nvice. A pool of 15 medical experts from China\nwith medical doctoral degrees is divided into five\ngroups, each containing three experts. These ex-\nperts participate in this project as volunteers and\n7Due to the company policy, the name of the online medi-\ncal consultation platform will be revealed upon acceptance.\n"}, {"page": 19, "text": "Figure 6: The screenshot of the annotation webpage for quality assurance of the MRAG-TCM tasks.\n"}, {"page": 20, "text": "Figure 7: The screenshot of the annotation webpage for quality assurance of the MRAG-CLFQA tasks.\nare paid 10 US dollars per hour. Each group is\nrandomly assigned a question, and the experts will\ncheck whether (a) the question contains no per-\nsonal information. (b) a single medical fact can\nnot answer the question. Moreover, one should\nrefer to multiple documents to organize a proper\nresponse to the question. (c) The question does\nnot contain any harmful questions related to drug\nabuse or other toxic content. The screenshot of the\nannotation webpage is presented in Figure 7. The\nquestion will not be included if the three experts\ndo not unanimously agree upon any of the above\naspects. Moreover, the remaining dataset contains\n1253 medical queries.\nFormatting. The medical queries are organized\nas a list of samples containing the query ID and the\nquery’s text string.\nB.5\nDataset statistics\nTo summarize the datasets of the MRAG-Bench,\nwe present the statistics of the datasets in Table 6.\nTo ensure the balance of the dataset composition,\nthe sizes of the included datasets will not exceed\n1,500 samples.\nB.6\nEvaluation metrics\nWe use objective evaluation metrics for the\nMCQA, IE, and LP task cohorts. We use post-\nprocessing scripts to transform the LLM’s re-\nsponses to structured data formats and use the\nfollowing metrics: (a) we calculate the accuracy\nscores for the multi-choice questions. (b) The LP\ntasks use the same metric as MCQA. (c) For the\nIE tasks, we adopt the instance-level strict micro-\nF1 (Zhu et al., 2023); that is, the model predicts a\ntriplet correctly if and only if it correctly predicts\nall its components.\nFor the LFQA tasks, we conducted a series of\nmodel- and human-based evaluations to assess the\nperformance of LLM-based RAG systems. Since\nwe are focusing on consumer health-related ques-\ntions in which the audience is usually a layperson\nof average reading comprehension and no specific\nclinical context. Thus, we evaluate the LLM re-\nsponses on the following aspects:\n• Usefulness: The response should be helpful,\nsafe, and informative and answer the query.\n• Readability: The response should be well-\norganized and easy to follow for laypersons.\n"}, {"page": 21, "text": "Dataset\nSize\n#Options\nAvg. Length\nTask type\nLanguage\nMMLU-Med\n1,089\n4\n63.5\nMCQA\nEnglish\nMedQA-US\n1,273\n4\n177.3\nMCQA\nEnglish\nMedMCQA\n1,500\n4\n26.1\nMCQA\nEnglish\nPubMedQA\n500\n3\n24.5\nMCQA\nEnglish\nBioASQ\n618\n2\n17.7\nMCQA\nEnglish\nMRAG-TCM\n1,200\n4\n29.8\nMCQA\nChinese\nChemProt\n800\n-\n384.2\nIE\nEnglish\nDDI\n1,017\n-\n299.7\nIE\nEnglish\nCMeIE\n1,500\n-\n293.7\nIE\nChinese\nADInt\n1,500\n-\n22.9\nLP\nEnglish\nDRKG\n1,500\n-\n28.2\nLP\nEnglish\nMultiMedQA\n1,066\n-\n10.2\nLFQA\nEnglish\nMRAG-CLFQA\n1,253\n-\n70.9\nLFQA\nChinese\nTable 6: Statistics of MRAG-Bench tasks. #Options: numbers of options; Avg. Length: average token counts in\neach question.\n• Knowledge: The response should reflect the\ncurrent consensus well and mention relevant\nand correct medical facts for answering the\nquery. Moreover, no irrelevant information is\ndiscussed.\n• Reasoning: The response presents clear, cor-\nrect reasoning steps.\nIn this work, LLMs (w/o. or w. RAG) are eval-\nuated and ranked via pairwise matches over the\nabove four axes. Our evaluation protocol is similar\nto Chatbot Arena (Chiang et al., 2024; Zheng et al.,\n2024c).\nB.7\nMore information\nHosting.\nThe MRAG-Bench’s datasets\nare hosted in\nhttps://huggingface.co/datasets/\nmichaelwzhu/MRAG. This URL is a permanent link\nand can be accessed by the community.\nlicense. The MRAG-Bench is released under\nthe Creative Commons Attribution (CC BY 4.0)\nlicense.\nAuthor Responsibility Statement. The authors\nof this dataset bear all responsibility for its con-\ntent. The dataset has been created and shared to\nprovide accurate and valuable data for research pur-\nposes. However, the authors do not assume liability\nfor any errors, omissions, or inaccuracies in the\ndataset.\nUser Responsibility. By using this dataset, users\nagree to:\n• Properly attribute the authors in any derivative\nworks or publications that utilize the dataset.\n• Comply with all applicable laws and regula-\ntions, including data privacy and intellectual\nproperty rights.\n• Do Not use the dataset for any unlawful or\nunethical purposes.\nThe authors reserve the right to update or modify\nthe dataset and its terms of use at any time. Users\nare encouraged to review the dataset and license\nperiodically to ensure compliance with the current\nterms.\nIf anyone has any questions or requires further\nclarification regarding the use of this dataset, please\ncontact wzhu91@connect.hku.hk.\nC\nDetailed Descriptions of\nMRAG-Toolkit\nIn the main contents, we introduce the MRAG\nToolkit to comprehensively evaluate how different\nLLM-based RAG systems perform on our MRAG\nBench. As shown in Figure 2, the MRAG Toolkit\nconsists of three major components: corpora, re-\ntrievers, and response generators. We now intro-\nduce these components in detail.\nC.1\nCorpora\nIn this work, we utilize four different corpora\nfor the English tasks: the medical corpus, the\nopen-domain corpus, the combined corpus, and\nthe World Wide Web. The combined corpus is the\ncombination of the first two. The medical corpus\nis the combination of the following resources for\nmedical literature or textbooks:\n"}, {"page": 22, "text": "• PubMed8 is the most widely used literature\nresource, containing millions of biomedical\narticles. Many relevant studies use PubMed\nas the retrieval corpus (Frisoni et al., 2022;\nNaik et al., 2021). We use a PubMed subset\nof 23.9 million articles with valid titles and ab-\nstracts for this work. The processed dataset is\navailable at https://huggingface.co/datasets/\nmichaelwzhu/MRAG/tree/main.\n• StatPearls9 is a point-of-the-care clinical deci-\nsion support tool similar to UpToDate10. We\nuse the 9,330 publicly available StatPearl arti-\ncles through NCBI Bookshelf14 to construct\nthe StatPearls corpus. We chunked StatPearls\naccording to the hierarchical structure, treat-\ning each paragraph in an article is a snippet,\nand all the relevant hierarchical headings are\nspliced as the corresponding title.\n• Textbooks11 is a collection of 18 widely\nused medical textbooks, which are impor-\ntant references for medical students taking\nthe United States Medical Licensing Exami-\nnation (USLME). In MRAG, the textbooks are\nprocessed as chunks with no more than 1000\ncharacters. We used the RecursiveCharac-\nterTextSplitter from LangChain12 to perform\nthe chunking.\nFor constructing an open-domain corpus, we\nutilize the Wikipedia (English) corpus13. As a\nlarge-scale open-source encyclopedia, Wikipedia\nis frequently used as a corpus in information re-\ntrieval tasks (Thakur et al., 2021) and open-domain\nquestion-answering tasks (Chen et al., 2017). We\nselect Wikipedia as one of the corpora to see if\nthe general domain database can be used to im-\nprove the ability of medical QA. We also chunked\nWikipedia’s documents with LangChain.\nThe World Wide Web can also serve as an ex-\ntensive and dynamic retrieval corpus for Retrieval-\nAugmented Generation (RAG), offering a vast and\ndiverse repository of information across virtually\nall knowledge domains. Leveraging the web as a\nretrieval corpus enables RAG systems to access\n8https://pubmed.ncbi.nlm.nih.gov/\n9https://www.statpearls.com/\n10https://www.wolterskluwer.com/en/solutions/\nuptodate\n11https://github.com/jind11/MedQA\n12https://github.com/langchain-ai/langchain\n13https://en.wikipedia.org/wiki/Wikipedia:\nDatabase_download\nup-to-date content, providing rich context and com-\nprehensive data sources for generating accurate\nand relevant responses.\nThis expansive corpus\nincludes various formats, from scholarly articles\nand news reports to blogs, forums, and multime-\ndia content, ensuring a breadth of perspectives and\ninsights. The web’s continuously evolving nature\ncould enhance the RAG system’s ability to produce\ninformed and current outputs. It is an invaluable\nresource for applications requiring real-time infor-\nmation retrieval and generation. However, the web\npage contents could also introduce noise or false\ninformation to the RAG system. In this work, we\nutilize the Bing Search API14 to access and retrieve\nrelevant documents from the web.\nFor the Chinese tasks, we utilize (a) the World\nWide Web and (b) a proprietary medical corpus and\nopen-domain corpus owned by a company. The\ncompany’s name and detailed information on the\ncorpus will be revealed upon acceptance.\nTo summarize the retrieval corpora, we present\ntheir statistics in Table 7.\nC.2\nRetrievers\nIn this work, we consider the following retrievers\nfor the English MRAG-Bench tasks:\n• Best Matching 25 (BM25) (Robertson et al.,\n2009). BM25 is a highly effective lexicon-\nbased sparse retrieval algorithm commonly\nutilized for information retrieval tasks, such\nas in Retrieval-Augmented Generation (RAG)\nfor large language models. BM25 scores the\nrelevance of documents by considering the fre-\nquency and distribution of query terms within\nthose documents. Specifically, it enhances\ntraditional term frequency-inverse document\nfrequency (TF-IDF) methods by incorporat-\ning term saturation and document length nor-\nmalization. BM25 ensures that the relevance\nscore increases logarithmically with term fre-\nquency, avoiding excessive influence from\noverly common terms, and adjusts for docu-\nment length to prevent bias toward longer doc-\numents. By weighing query terms according\nto their inverse document frequency and ac-\ncounting for term saturation, BM25 provides\na robust and scalable approach for retrieving\npertinent documents in RAG, enhancing the\n14https://www.microsoft.com/en-us/bing/apis/\nbing-web-search-api\n"}, {"page": 23, "text": "Corpus\nSource\n#Docs\n#Snippets\nAvg. Length\nDomain\nLanguage\nMedical corpus\nPubMed\n23.9M\n23.9M\n296\nBiomedicine\nEnglish\nStatPearls\n9.3k\n301.2k\n119\nClinics\nEnglish\nTextbooks\n18\n125.8k\n182\nMedicine\nEnglish\nOpen-domain corpus\nWikiPedia\n6.5M\n29.9M\n162\nGeneral\nEnglish\nWorld wide web\nInternet\n-\n-\n-\nGeneral\nEnglish & Chinese\nProprietary medical corpus\nproprietary\n3.6M\n13.5M\n348\nMedical\nChinese\nTable 7: Statistics of the retrieval corpora. #Docs: the number of documents contained in the corpus. #Snippets:\nthe number of document snippets contained in the corpus. Avg. Length: average token counts in each document\nsnippets.\ncontextual accuracy and informativeness of\nthe generated responses.\n• MedCPT (Jin et al., 2023). MedCPT is a\nbiomedical embedding model pre-trained with\ncontrastive loss on 255 million user clicks\nfrom PubMed search logs. It achieved state-\nof-the-art performance on several biomedical\nIR tasks. For our experiments, we use the\nembedding model to transform the document\nsnippets to vectors and build a vector index\nwith the help of Faiss15. Upon receiving a\nuser query, the embedding model embeds the\nquery to a vector and leverages the efficient\nnearest neighbor search techniques (also im-\nplemented in Faiss) on vectors. Vector-based\nsearch is highly efficient since a search can be\ndone in 3 ms with a vector index of sizes in\nbillions.\n• BGE-base (Xiao et al., 2023). The BGE-base\nmodel is a sophisticated sentence embedding\nmodel designed to transform sentences into\nhigh-dimensional vector representations, en-\nabling efficient and meaningful comparison\nof textual data. This model leverages pre-\ntraining on large-scale corpora to deeply un-\nderstand language and provide high-quality\nsemantic representations for input documents.\n• E5-Mistral-7B (Wang et al., 2023a), a LLM\nbased retriever. This model uses the Mixtral-\n7B as the document encoder and is further\npre-trained on a large-scale synthetic dataset\nvia the contrastive learning loss function.\n• RRF (Cormack et al., 2009) proposed to\nmerge results from different retrievers with\nReciprocal Rank Fusion (RRF), which effec-\ntively fuses the information from different\n15https://github.com/facebookresearch/faiss\nsources by selecting shared predictions. In\nthis work, we utilize this approach to combine\nresults from BGE-base and MedCPT.\nFor the Chinese tasks, we utilize the BGE-base\nChinese model16 as the retriever, if the local corpus\nis used.\nWe summarize the basic information of the re-\ntrievers in Table 8.\nC.3\nLLMs as response generator\nIn this work, we select the most frequently used\nor recently released LLMs with excellent perfor-\nmance in the open-domain evaluation benchmarks\nto evaluate RAG systems.\n• Commercial LLMs developed by the OpenAI,\nGPT-3.5 (gpt-3.5-turbo), and GPT-4 (gpt-4o).\nThese two models are popular commercial\nLLMs, which have already shown great ca-\npabilities in directly answering medical ques-\ntions (Singhal et al., 2023a; Nori et al., 2023).\nWe access these two models via the APIs pro-\nvided by OpenAI17.\n• The Tongyi Qwen (qwen_max) model18 is an\nadvanced language model developed by Al-\nibaba Cloud, designed to push the boundaries\nof natural language processing and generation\ncapabilities. This model, built upon exten-\nsive datasets and cutting-edge deep learning\nalgorithms, aims to excel in a wide range of\ntasks in both Chinese and English, including\ntext generation, conversation, summarization,\ntranslation, and more.\n• The Mixtral-8x22B (-Instruct-v0.1) model19\nis one of the latest open-sourced LLM. It\n16https://huggingface.co/BAAI/bge-base-zh\n17https://platform.openai.com/docs/models\n18https://tongyi.aliyun.com/qianwen/\n19https://mistral.ai/news/mixtral-8x22b/\n"}, {"page": 24, "text": "Retriever\nType\nSize\nMetric\nDomain\nLanguage\nBM25\nlexical\n-\nBM25\nGeneral\n-\nMedCPT\nSemantic\n109M\ncosine similarity\nBiomedicine\nEnglish\nBGE-base\nSemantic\n110M\ncosine similarity\nGeneral\nEnglish\nE5-Mistral-7B\nSemantic\n7B\ncosine similarity\nGeneral\nEnglish\nBGE-base Chinese\nSemantic\n110M\ncosine similarity\nGeneral\nChinese\nTable 8: Statistics of the retrievers.\nsets a new standard for performance and effi-\nciency within the AI community. It is a sparse\nMixture-of-Experts (SMoE) model that uses\nonly 39B active parameters out of 141B, of-\nfering unparalleled cost efficiency for its size.\nMixtral-8x22B has the following strengths:\n(a) It is fluent in English, French, Italian, Ger-\nman, and Spanish. (b) It has strong mathemat-\nics and coding capabilities. (c) It is natively\ncapable of function calling. (d) Its 64K tokens\ncontext window allows precise information to\nbe recalled from large documents. This model\nis released under Apache 2.0, the most permis-\nsive open-source license, allowing anyone to\nuse the model anywhere without restrictions.\n• Qwen2.5 (Bai et al., 2023) is a language\nmodel series including decoder language mod-\nels of different sizes. It is based on the Trans-\nformer architecture with SwiGLU activation,\nattention QKV bias, group query attention, a\nmixture of sliding window attention and full\nattention. Additionally, it has an improved\ntokenizer that is adaptive to multiple natural\nlanguages and codes. In this work, unless oth-\nerwise specified, we use the Qwen-1.5-72B\n(-chat) 20 model.\n• Meta developed and released the Meta Llama\n3 family21 of large language models (LLMs),\na collection of pretrained and instruction-\ntuned generative text models in 8 and 70B\nsizes. The Llama 3 instruction-tuned models\nare optimized for dialogue use cases and out-\nperform many of the available open-source\nchat models on standard industry benchmarks.\nFurther, in developing these models, Meta sig-\nnificantly optimized helpfulness and safety.\nUnless otherwise specified, we use the Llama-\n3-70B (-Instruct) model in this work.\n20https://huggingface.co/Qwen/Qwen2.5-72B\n21https://llama.meta.com/llama3/\n• MEDITRON (Chen et al., 2023b) is a series\nof biomedical LLMs built based on Llama2\n(Touvron et al., 2023) and fine-tuned on open-\nsource biomedical literature. In this work, we\nuse its 70B version model.\n• PMC-LlaMA (13B) (Wu et al., 2023) is fine-\ntuned based on LLaMA (Touvron et al., 2023),\nusing the medical literature from PubMed.\nFor the Chinese tasks, the following LLMs will\nbe evaluated:\n• GPT-3.5 (gpt-3.5-turbo).\n• GPT-4 (gpt-4o).\n• Tongyi Qwen (qwen_max).\n• Qwen-1.5 72B.\n• DISC-MedLLM (Bao et al., 2023) leverages\nLarge Language Models (LLMs) to provide\naccurate and truthful medical responses in\nend-to-end conversational healthcare services.\nIt constructs high-quality Supervised Fine-\nTuning (SFT) datasets by utilizing medical\nknowledge graphs, reconstructing real-world\ndialogues, and incorporating human-guided\npreference rephrasing. With the constructed\nhigh-quality dataset, DISC-MedLLM is fine-\ntuned from Baichuan-13B-Base22 model and\nsurpasses many Chinese medical LLMs in\nboth single-turn and multi-turn consultation\nscenarios.\nUnless otherwise specified, all the LLMs uti-\nlize the nucleus sampling strategy (Holtzman et al.,\n2019) for decoding. The temperature parameter is\nset to 0.7, and the top_p parameter is set to 0.8.\nWe summarize the basic information of the LLM\nresponse generators in Table 9.\n22https://huggingface.co/baichuan-inc/\nBaichuan-13B-Base\n"}, {"page": 25, "text": "LLM\nSize\nContext size\nOpen-source\nDomain\nLanguage\nGPT-3.5\n-\n16,385\nFalse\nGeneral\nEnglish & Chinese\nGPT-4\n-\n128,000\nFalse\nGeneral\nEnglish & Chinese\nTongyi Qwen\n-\n8,000\nFalse\nGeneral\nEnglish & Chinese\nMixtral-8x22B\n141B (39B activated)\n65,536\nTrue\nGeneral\nEnglish\nQwen2.5-72B\n72B\n32,768\nTrue\nGeneral\nEnglish & Chinese\nLlaMA-3-70B\n70B\n8,192\nTrue\nGeneral\nEnglish\nMEDITRON\n70B\n4,096\nTrue\nBiomedicine\nEnglish\nPMC-LlaMA\n13B\n4,096\nTrue\nBiomedicine\nEnglish\nDISC-MedLLM\n13B\n4,096\nTrue\nMedicine\nChinese\nTable 9: Statistics of the LLM response generators.\nC.4\nPrompting strategies\nWe now describe the prompting strategies used\nwhen evaluating the LLMs on the MRAG bench.\nFollowing (Xiong et al., 2024), all the RAG sys-\ntems should be evaluated in a zero-shot setting\nwhere in-context few-shot learning is not permit-\nted.\nThe prompting strategy can be classified as ei-\nther (a) w/o. RAG or (b) w. RAG, based on whether\nan LLM retrieves referential documents and con-\ncatenates them to the prompt. In this work, we only\nconsider the framework where the retrieved knowl-\nedge/contents are concatenated in the prompt, and\nno other approaches, like memory augmentation,\nare applied to insert external information into the\nLLMs.\nBased on how the response is elicited, the\nprompting strategy can be classified as: (a) Direct\nanswer (DA): given the question, the prompt asks\nthe LLM to output the answer directly. (b) Chain-\nof-thought (COT) (Wei et al., 2022) explicitly asks\nthe LLM to think step by step and demonstrate\nthe intermediate outputs. (c) COT-Refine. Build-\ning on COT and Self-Refine(Madaan et al., 2024),\nwe developed a simple prompting strategy called\nCOT-refine. This strategy involves a two-stage pro-\ncess: first, given a COT prompt and a question,\nthe model produces a response (R0). Then, in the\nsecond stage, the model is conditioned on the orig-\ninal prompt, question, and R0 and is prompted to\nproduce a refined answer with detailed explana-\ntions. This strategy allows the LLM to reflect on\nthe previous answer and make necessary correc-\ntions. Regarding the prompt strategy for eliciting\nresponses, COT-Refine is used by default.\n"}]}