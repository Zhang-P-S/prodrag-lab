{"doc_id": "arxiv:2601.15558", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.15558.pdf", "meta": {"doc_id": "arxiv:2601.15558", "source": "arxiv", "arxiv_id": "2601.15558", "title": "From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare", "authors": ["Man Luo", "Bahareh Harandizadeh", "Amara Tariq", "Halim Abbas", "Umar Ghaffar", "Christopher J Warren", "Segun O. Kolade", "Haidar M. Abdul-Muhsin"], "published": "2026-01-22T00:56:33Z", "updated": "2026-01-22T00:56:33Z", "summary": "Clinical empathy is essential for patient care, but physicians need continually balance emotional warmth with factual precision under the cognitive and emotional constraints of clinical practice. This study investigates how large language models (LLMs) can function as empathy editors, refining physicians' written responses to enhance empathetic tone while preserving underlying medical information. More importantly, we introduce novel quantitative metrics, an Empathy Ranking Score and a MedFactChecking Score to systematically assess both emotional and factual quality of the responses. Experimental results show that LLM edited responses significantly increase perceived empathy while preserving factual accuracy compared with fully LLM generated outputs. These findings suggest that using LLMs as editorial assistants, rather than autonomous generators, offers a safer, more effective pathway to empathetic and trustworthy AI-assisted healthcare communication.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.15558v1", "url_pdf": "https://arxiv.org/pdf/2601.15558.pdf", "meta_path": "data/raw/arxiv/meta/2601.15558.json", "sha256": "3cf9b2c4a6d264595baf6ca8f7aeda96d5cb4c447c57daacd6d31e465297d748", "status": "ok", "fetched_at": "2026-02-18T02:20:48.783576+00:00"}, "pages": [{"page": 1, "text": "From Generation to Collaboration: Using LLMs to\nEdit for Empathy in Healthcare\nMan Luo1*†, Bahareh Harandizadeh2†, Amara Tariq3,\nHalim Abbas2, Umar Ghaffar4, Christopher J Warren4,\nSegun O. Kolade2, Haidar M. Abdul-Muhsin4\n1*Science team, Abridge, 470 Alabama, San Francisco, 94110, CA, U.S.\n2Platform, Mayo Clinic, 200 First St. S.W., Rochester, 10587, MN, U.S.\n3Department of AI & Informatics, Mayo Clinic, 6161 E Mayo Blvd,\nPhoenix, 85054, AZ, U.S.\n4Urology Department, Mayo Clinic, 5777 E Mayo Blvd, Phoenix, 85054,\nAZ, U.S.\n*Corresponding author(s). E-mail(s): luoman.cs@gmail.com;\nContributing authors: Harandizadeh.Bahareh@mayo.edu;\nTariq.Amara@mayo.edu; Abbas.Halim@mayo.edu;\nGhaffar.Umar@mayo.edu; warren.christopher@mayo.edu;\nKolade.John@mayo.edu; Abdul-Muhsin.Haidar@mayo.edu;\n†These authors contributed equally to this work.\nAbstract\nClinical empathy is essential for patient care, but physicians need continually\nbalance emotional warmth with factual precision under the cognitive and emo-\ntional constraints of clinical practice. This study investigates how large language\nmodels (LLMs) can function as empathy editors, refining physicians’ written\nresponses to enhance empathetic tone while preserving underlying medical infor-\nmation. More importantly, we introduce novel quantitative metrics, an Empathy\nRanking Score and a MedFactChecking Score to systematically assess both emo-\ntional and factual quality of the responses. Experimental results show that\nLLM-edited responses significantly increase perceived empathy while preserving\nfactual accuracy compared with fully LLM-generated outputs. These findings\nsuggest that using LLMs as editorial assistants, rather than autonomous gen-\nerators, offers a safer, more effective pathway to empathetic and trustworthy\nAI-assisted healthcare communication.\n1\narXiv:2601.15558v1  [cs.CL]  22 Jan 2026\n"}, {"page": 2, "text": "Keywords: Large Language Models (LLMs), Empathy Ranking, Factuality Evaluation\n1 Introduction\nEmpathy encompasses cognitive understanding of others’ emotional states and affec-\ntive emotional responses when witnessing others’ emotions [5, 6]. In healthcare, clinical\nempathy involves understanding patient pain and suffering, communicating this under-\nstanding, and having an intention to help [14]. Clinical empathy plays a crucial role in\npatient care, as it fosters trust, encourages patient engagement, and ultimately leads\nto better treatment outcomes [24, 15, 37]. Studies have shown that empathetic com-\nmunication improves adherence to medical advice and enhances patient satisfaction.\nDespite its importance, empathy is often difficult to sustain in routine clinical prac-\ntice [7, 23]. Physicians face heavy cognitive and emotional demands, including time\npressure, complex decision making, and repeated exposure to distressing situations.\nTo safeguard their own mental well-being and maintain sound clinical judgment, they\nmust balance empathy with a degree of professional detachment [26, 11, 33]. This cre-\nates a persistent tension between factual precision and emotional warmth, posing a key\nchallenge in preserving clinical accuracy and objectivity while ensuring compassionate\nphysician–patient communication.\nRecent advances in large language models (LLMs) have revolutionized healthcare\napplications [45, 27]. Clinical chatbots and decision support tools have shown potential\nto improve accessibility for patients and reduce the workload of healthcare profes-\nsionals [1]. However, their deployment in clinical settings raises important concerns\nregarding patient privacy [46, 30], data security, and the factual accuracy of AI-\ngenerated content [43, 34]. Consequently, most prior studies have focused primarily on\nclinical accuracy and safety, often at the expense of empathetic communication. More-\nover, many current systems rely on end-to-end text generation, which increases the\nrisk of hallucination and reduces control over tone, intent, and emotional expression\n[21, 18].\nTo address the lack of empathy in AI-generated medical responses, researchers have\nexplored fine-tuning LLMs on empathetic medical dialogues and patient-physician\nconversations [51, 55, 48]. While fine-tuning can enhance empathetic language gener-\nation, it requires substantial computational resources and domain expertise, making\nit costly and time-intensive [50]. More critically, studies have shown that fine-tuning\nLLMs for stylistic attributes such as empathy can inadvertently increase the risk of\nfactual errors, fabrications, and “phony empathy” [19]. In the medical domain, where\nfactual precision is paramount, such trade-offs pose significant safety concerns. These\nlimitations highlight the need for alternative approaches that can enhance empathy\nwithout compromising clinical accuracy or requiring extensive model retraining.\nTo address these limitations, we propose reframing the role of LLMs in clinical com-\nmunication: from autonomous generators to collaborative editors that refine physician\nresponses to enhance empathy while maintaining factual integrity. This perspective\nemphasizes augmentation rather than automation, positioning LLMs as partners that\n2\n"}, {"page": 3, "text": "Fig. 1: AI editing versus generation for clinical responses. Direct AI generation pro-\nduces empathetic but factually inaccurate responses with hallucinated medical details,\nwhile AI editing of physician responses maintains factual accuracy while adding empa-\nthy.\nenhance, rather than replace, human expertise. Figure 1 illustrates our proposed col-\nlaborative editing approach. In this example, a patient asks about resuming bike riding\nafter a prostatectomy, the physician provides an accurate but terse response. An end-\nto-end AI chat system produces a more empathetic message but introduces factual\nerrors. Our editing approach preserves the physician’s recommendation while adding\nempathetic acknowledgment. This demonstrates how LLMs can enhance physician-\npatient communication without introducing hallucinations or compromising medical\nprecision.\nTo systematically evaluate this editing approach, we explore three research\nquestions:\n• RQ1: How can we add empathy to physician responses and measure the improve-\nment?\n• RQ2: How can we ensure empathetic edits preserve clinical facts?\n• RQ3: How do empathy and factuality relate to each other?\nWe developed two complementary evaluation protocols. For i) empathy evaluation,\nwe introduce a three-way empathy comparison which achieves decent alignment score\nwith human evaluators. For ii) factuality evaluation, we introduce the MedFactCheck-\ning Score to measure Fact-Recall (preservation of original medical information) and\nFact-Precision (grounding of enhanced content in the original response). Through\n3\n"}, {"page": 4, "text": "bidirectional entailment checks, we detect both information loss and hallucinated addi-\ntions. Domain experts validated both metrics and reviewed responses across multiple\nmodels and empathy levels, informing iterative prompt refinement to balance empathy\nwith factual integrity.\nBased on our investigations of RQ1–RQ3, our experiments demonstrate that the\nproposed empathy editing framework, equipped with a refined editing instruction\nprompt, achieves the most favorable trade-off between empathy and factual accuracy,\nmaking it well suited for practical clinical use. Overall, this work reframes LLMs as\nsafe, empathetic, and trustworthy editorial assistants that support clinicians in pro-\nducing emotionally resonant yet factually sound communication, which we consider as\nan essential step toward responsible AI integration in healthcare.\n2 Related Work\nClinical empathy improves trust, treatment adherence, and patient satisfaction [15, 7],\nyet physicians must balance emotional warmth with clinical detachment to preserve\nfactual precision under time pressure [26, 11]. This tension between empathy and accu-\nracy remains underexplored in NLP research, which lacks concrete conceptualization\nof empathy in text [41, 25].\nCentral to this challenge is the question of how empathy should be measured.\nEmpathy measurement in healthcare relies on validated psychometric instruments such\nas the Jefferson Scale of Empathy (JSE) [13] and the Consultation and Relational\nEmpathy (CARE) scale [12]. While these instruments reliably assess human empathy,\nthey were designed for human interactions rather than algorithmic text generation,\nmaking their direct application to LLM-generated text problematic [42].\nTo address this gap, researchers have developed computational approaches for\nempathy evaluation. These include rule-based frameworks, machine learning, and con-\ntext grammar approaches [9, 8, 36]. For AI-generated text specifically, methods include\nRoBERTa-based frameworks [40] and LLM-based approaches like EM-Rank [29] and\nESC-Rank [58]. In-context learning has also been used to evaluate LLM therapists,\nfinding they tend to mimic low-quality therapy [3]. However, reliability and alignment\nwith human judgment remain concerns for these automated methods [44].\nBeyond evaluation methods, a separate line of research has examined the inher-\nent empathetic capabilities of LLMs. Studies show that while LLMs are generally\nperceived as empathetic, they exhibit notable limitations [44, 17], including repeti-\ntive use of empathetic phrases [17] and lack of cultural understanding compared to\nhuman counselors [20]. Additionally, highly educated patients report greater skepti-\ncism toward chatbot performance [2], suggesting that perceived empathy may vary\nacross user populations.\nGiven these limitations, researchers have developed various approaches to enhance\nempathy in LLM-generated text. These include augmenting LLMs with small-scale\nempathetic models [53], integrating psychotherapy and psychological principles into\nLLM frameworks [28, 47], and leveraging human-AI collaboration to improve empathic\nconversations [39]. LLM-generated feedback has also been shown to improve empathy\n4\n"}, {"page": 5, "text": "expression among healthcare trainees [54]. However, fine-tuning LLMs for empa-\nthy requires substantial computational resources and extensive labeled datasets [49],\nposing practical limitations for widespread implementation.\nWhile these enhancement efforts focus on improving empathy, they rarely consider\npotential trade-offs with factual accuracy. There is growing concern regarding the\nbalance between empathy enhancement and maintaining factuality in AI-generated\ntext [19], especially for sensitive domains like medicine and legal proceedings [35, 57,\n56]. However, systematic investigation of their relationship in clinical contexts remains\nlimited, particularly regarding which specific medical information is vulnerable to\ndegradation during empathy enhancement.\n3 Methods\n3.1 Empathy Edition\nPrevious work has explored using LLMs to generate an empathetic responses by just\ngiving the patient question. While these LLM-generated responses tend to be more\nempathetic than those written by physicians [29], this approach presents two major\nchallenges in real-world applications. First, there are privacy concerns, as patient\ndata must be carefully handled to prevent unintended disclosure. Prior studies have\naddressed this by performing manual de-identification, which is labor-intensive. Sec-\nond, there is a factuality concern, as LLMs can generate responses that contain factual\nerrors, potentially leading to misinformation in medical communication.\nTo mitigate these issues, we propose an alternative approach: instead of generating\nresponses purling by LLMs, we use LLMs to edit physician responses. This ensures that\nthe editing responses are grounded by factual contents while enhancing empathetic\nexpression. Our method positions LLMs as controlled editors, focusing on linguistic\nempathy rather than medical inference, thereby minimizing the risk of hallucinated or\nspeculative statements.\nSimple Editing Prompt\nWe design an instruction prompt that explicitly directs the model to revise physician\nresponses to enhance empathy while maintaining the original meaning and writing\nstyle (see Appendix A.1 for full prompt templates). The prompt provides the patient’s\nquestion and the physician’s response, ensuring the model has sufficient context for\neffective editing. Additionally, we experiment with a minimal-editing prompt, which\ninstructs the model to make only essential modifications, preserving as much of the\noriginal response as possible. This setup allows us to isolate the linguistic effects of\nempathy editing from those of content generation.\nRefine Editing Prompt\nUsing the simple editing prompt, we obtained model outputs and engaged a domain\nexpert to evaluate the quality of these responses. The expert received all pairs of\nresponses, consisting of the original patient message, the original physician response,\nand the model-edited version. The initially generated editing responses occasionally\n5\n"}, {"page": 6, "text": "included factual inaccuracies and speculative statements (more details are discussed\nin 3.6). To mitigate these risks, we designed a refined prompt by adding explicit\nbehavioral constraints to the simple editing prompt (see Appendix A.1). These editing\nprinciples define the operational boundaries for empathy enhancement, ensuring that\nmodel interventions remain clinically grounded and ethically appropriate. Throughout\nour experiments in §4, we employ both simple and refined prompts and compare the\nresulting levels of empathy and factuality.\n3.2 Empathy Evaluation\nMost prior studies have assessed clinical empathy through human annotation [38, 16,\n40], while a recent work, EMRanknk [29] has introduced automated evaluation using\nLLMs as clinical empathy raters. Although these LLM-based assessments show mod-\nerate agreement with human judgments, their reliability remains limited. In this work,\nwe develop a new empathy metric based on LLMs-judge to align better with human\npreference. In this section, we provide an overview of EMRank and our proposed\n3-EMRank, which achieves a higher degree of alignment with human judgments.\nOverview of the EMRank framework. EMRank is a prompt-based evaluation\nmethod inspired by the LLMs-as-judge paradigm [59]. It utilizes a large language\nmodel (LLM) as a judge to determine which response is more empathetic. The eval-\nuation is conducted in zero-shot, one-shot, and few-shot settings, with an ensemble\napproach that aggregates majority votes from these settings. Among these methods,\nthe ensemble approach has been shown to achieve the highest alignment with human\njudgments. For more details, we refer the readers to the original paper. A key lim-\nitation of the original EMRanknk framework is that it relies on binary comparison,\nforcing the model to always choose one response as more empathetic than the other.\nHowever, in real-world scenarios, two responses may exhibit similar levels of empathy,\nmaking binary comparisons inadequate and potentially contributing to discrepancies\nbetween human and model evaluations.\nImplementation of Three-way EMRank. To address the issue of EMRank, we\nintroduce a three-way ranking, 3EM-Ranker, which allows for an additional judg-\nment option: both responses are equally empathetic. This modification enables more\nnuanced differentiation among comparable responses and better reflects the subjectiv-\nity inherent in empathy perception. The three-way extension is achieved by modifying\nthe evaluation prompt to include a third response category, “Both responses are\nequally empathetic.” This adjustment improves the flexibility of the model judgment\nand improves the consistency with human ratings. We also test multiple LLMs as\nempathy judges to ensure the cross-model reliability of the EMRank outputs.\nThe modified prompts shown in Supplementary Fig. 3 in Appendix A.2 explic-\nitly accommodate equivalence judgments, enabling the metric to capture subtle\ndistinctions between near-identical empathetic tones.\n3.3 Factuality Evaluation\nTo systematically evaluate whether empathy enhancement preserves medical accu-\nracy, we introduce the MedFactChecking Score, an automated evaluation framework\n6\n"}, {"page": 7, "text": "Fig. 2: Bidirectional fact-checking framework for measuring factual accuracy in AI-\nedited clinical responses. Fact-Recall (green arrow) quantifies information loss from the\noriginal physician response, while Fact-Precision (yellow arrow) detects hallucinated\nadditions in the edited response. Red text indicates facts that fail entailment checking:\nlost information (left) and unsupported additions (right).\nadapted from FactEHR [32]. Unlike FactEHR, which evaluates factuality within a sin-\ngle clinical note, our approach performs bidirectional evaluation across two distinct\ntexts: the original physician response and its empathy-enhanced version (Figure 2).\nThis bidirectional design is necessary because empathy enhancement can both lose\ninformation from the original response and introduce unsupported additions.\nThe framework quantifies two complementary dimensions: Fact-Recall measures\nwhat proportion of medical facts from the original response are preserved in the\nenhanced version, while Fact-Precision measures what proportion of facts in the\nenhanced response are grounded in the original. By extracting medical facts from both\nresponses and performing entailment checks in both directions, we separately quantify\nomission and hallucination errors.\nWe validate our automated metrics through human annotation studies: ML experts\nverify the accuracy of fact extraction and entailment judgments, while two clinical\nexperts (urologists) assesses whether MedFactChecking captures clinically significant\nerrors and identifies patterns in what medical information is systematically lost or\nadded.\nFormal Definition. Let d denote the original physician response and d′ denote\nthe empathy-enhanced response. Let C = {c1, c2, . . . , cn} represent the set of medical\nfacts produced by decomposing d, and let C′ = {c′\n1, c′\n2, . . . , c′\nm} represent the set of\nmedical facts produced by decomposing d′.\nWe define the entailment indicator function as:\n• [d′ |= c] = 1 if a fact c ∈C is completely entailed by the empathy-enhanced response\nd′, and 0 otherwise.\n• [d |= c′] = 1 if a fact c′ ∈C′ from the enhanced response is entailed by the original\nresponse d, and 0 otherwise.\n7\n"}, {"page": 8, "text": "Fact-Recall extracts facts from the original physician response and verifies their\nentailment in the enhanced response, thereby measuring information preservation. We\ndefine:\nFact-Recall =\n1\n|C|\nX\nc∈C\n[d′ |= c]\n(1)\nA higher recall indicates better preservation of the original medical informa-\ntion (i.e., less information loss during editing). Fact-Precision extracts facts from\nthe enhanced response and verifies their entailment in the original response, thereby\ndetecting unsupported additions or hallucinations. We define:\nFact-Precision =\n1\n|C′|\nX\nc′∈C′\n[d |= c′]\n(2)\nA higher precision indicates fewer hallucinated medical facts added without basis in the\noriginal response. Together, these two measures provide complementary perspectives\non factual reliability - recall captures retention, while precision captures restraint.\nMicro and Macro Averaging. To provide a comprehensive evaluation, we report\nboth averaging schemes. Macro-averaging computes metrics for each question-response\npair independently, then averages across all pairs, treating each exchange equally\nregardless of fact count. Micro-averaging aggregates all fact counts across the corpus\nbefore computing metrics, giving more weight to responses with more facts. Formally:\nMacro-Rec = 1\nN\nN\nX\ni=1\nP\nc∈Ci [d′\ni |= c]\n|Ci|\n,\nMicro-Rec =\nPN\ni=1\nP\nc∈Ci I[d′\ni |= c]\nPN\ni=1 |Ci|\n(3)\nMacro-Prec = 1\nN\nN\nX\ni=1\nP\nc′∈C′\ni [di |= c′]\n|C′\ni|\n,\nMicro-Prec =\nPN\ni=1\nP\nc′∈C′\ni [di |= c′]\nPN\ni=1 |C′\ni|\n(4)\nwhere N is the total number of question-response pairs. For edge cases where C\nor C′ is empty, we define the metric as 1.0 if both sets are empty, and 0.0 otherwise.\nThese conventions ensure metric stability even for brief or minimally factual responses.\nPrompts for Fact Extraction and Entailment Checking\nWe employ two distinct prompts adapted from prior work: one for medical fact decom-\nposition and another for entailment evaluation. This modular structure allows the\nsame LLM to perform both extraction and verification tasks using clear, role-specific\ninstructions.\nFact Decomposition. Our decomposition prompt, adapted from FActScore [31],\nwhich is later also used by FactEHR [32], includes 2 in-context examples and instructs\nthe LLM to extract medical facts only, explicitly excluding empathetic expressions\nand conversational acknowledgments. For example, from “I’m sorry you’re worried.\nYour PSA is 4.2,” we extract only “PSA is 4.2”. This distinction is critical because\nempathy enhancement primarily affects emotional tone rather than medical content,\n8\n"}, {"page": 9, "text": "and including non-medical statements would conflate empathy with factuality. The\nstructured prompt shown in Supplementary Fig. 5 in Appendix A.2\nEntailment Evaluation Prompt. For entailment verification, we use the prompt\ndeveloped in FactEHR [32], which was optimized using 40 premise-hypothesis pairs\nsampled from clinical notes and tuned to maximize F1 score on entailment classifica-\ntion. This prompt instructs the LLM to determine whether a given fact is completely\nentailed by, contradicted by, or neutral with respect to a source text, providing robust\nentailment judgments for medical statements. We maintain strict output formatting\nrequirements to facilitate automated parsing of entailment predictions (exact prompt\nis given in Supplementary Fig. 4 in Appendix A.2).\n3.4 QA Dataset Collection\nOur dataset comprises 163 patient-physician message pairs collected from a patient\nportal at a hospital. We focus on messages from individuals diagnosed with prostate\ncancer who underwent radical prostatectomy between December 2018 and October\n2023. Messages were gathered under the category of “Patient advice request” and\nrandomly selected from the pool of available conversations (and we only use the first\nQA pair in the conversation). IRB approval was obtained to use the data.\nDe-identification Process\nThe de-identification process was meticulously carried out by a team of three: two\nmedical students and one physician (postgraduate year 3 urology resident), with\nthe primary goal of ensuring privacy and confidentiality. This process involved a\ndetailed review of each patient message and physician response to identify and remove\nor anonymize any personally identifiable information (PII) including patient names,\nphysician names, dates, phone numbers, and addresses.\nDataset Characteristics\nPhysician responses in our dataset are notably concise (mean 65.8±50.4 words, 4.7±3.0\nsentences), reflecting the time-constrained nature of asynchronous clinical commu-\nnication in patient portals. Patient questions tend to be longer and more variable\n(mean 82.2±56.2 words, 6.4±4.1 sentences), with some patients providing extensive\nclinical context while others pose brief queries (see Supplementary Fig. 6 for full dis-\ntributions). This heterogeneity in response length motivates our investigation of how\nbrevity impacts factual preservation during empathy enhancement (see §4).\nWe also analyzed whether questions could be answered using only general medi-\ncal knowledge or required patient-specific information from electronic health records\n(EHR). Using Gemini-2.5-Flash for classification, we found that 91.4% (149/163) of\nquestions require access to patient-specific EHR data (e.g., lab results, imaging find-\nings, treatment history), while only 8.6% (14/163) represent general medical inquiries.\nThis finding indicates that the task is largely evidence-dependent and simply using\nan end-to-end QA model without retrieving the correct EHR data is insufficient. In\ncontrast, our proposed approach of editing existing physician responses offers a more\nrealistic and clinically grounded framework.\n9\n"}, {"page": 10, "text": "3.5 Models\nWe evaluate five state-of-the-art LLMs spanning both open-source and proprietary\narchitectures: Qwen3-7/14B [52], Mistral-7B-Instruct [22], Llama-3.1-8B-Instruct [10],\nGemini-2.5-Flash [4]. All models are hosted internally on the Mayo Clinic Plat-\nform within HIPAA-compliant compute environments to ensure patient data privacy\nand security. We use four models for empathy-based editing and employ Llama-\n3.1 and Qwen3 for EM-Ranker evaluation. For fact decomposition and entailment\nverification—the core components of factuality evaluation, we use Gemini-2.5-Flash\nexclusively, following FactEHR’s findings that Gemini achieves superior precision and\nrecall on these tasks compared to alternative models.\n3.6 Metric Validation\n3EM-Ranker Validation\nTo validate our three-way annotation approach, we conducted a human evaluation as a\nbaseline for comparison against 3EM-Ranker. We provided annotators with a patient\nquestion and two responses: one from a physician and one from an AI model (Gemini-\n2.5-Flash). Annotators selected from three options: response 1 is more empathetic,\nresponse 2 is more empathetic, or they are equally empathetic. The patient anno-\ntators included three male patients with prostate cancer who had undergone radical\nprostatectomy within the same timeframe as our collected patient message dataset.\nWe compared the results produced by human annotators and 3EM-Ranker (see\nSupplementary Table 1 for full results). When using Qwen-3 and LLaMA-3 as the\nunderlying LLMs for 3EM-Ranker, the alignment scores reach 0.57 and 0.55 respec-\ntively, both substantially higher than the previously reported 0.23 agreement between\nhumans and EM-Rank [29]. These results demonstrate that 3EM-Ranker achieves\nconsiderably stronger alignment with human judgments compared to prior work.\nMedFactChecking Validation\nTo validate the MedFactChecking Score’s ability to detect factual preservation in\nempathy-enhanced responses, we conducted a multi-stage evaluation combining auto-\nmated fact extraction with expert human annotation. We edited physician responses\nusing the Simple Editing prompt (see Appendix A.1 or §3.1) with Llama-3.1 to enhance\nempathy, then analyzed the responses using the MedFactChecking Score framework,\nML researchers, and a medical expert.\nAutomated Fact Extraction Validation. Two machine learning scientists inde-\npendently verified the accuracy of facts flagged as added (hallucinated, [d |= c′] = 0) or\nnot preserved (missed, [d′ |= c] = 0) by the algorithm. For original responses, 934 facts\nwere extracted, of which 191 were flagged as “not preserved” in the edited versions;\nannotators confirmed 139 of these (72.7%) as genuinely missing. For edited responses,\n1230 facts were extracted, of which 262 were flagged as “added”; annotators confirmed\n219 of these (83.6%) as actual hallucinations (see Supplementary Table 2).\nThe moderate results, particularly for not-preserved facts (72.6%), warrant further\ninterpretation. Many extraction errors involved semantically similar facts where the\n10\n"}, {"page": 11, "text": "algorithm made overly conservative entailment judgments. While the semantic con-\ntent was largely preserved between original and edited responses, some variations in\nphrasing led the algorithm to flag facts as “not preserved” or “added.” Annotators\njudged these conservative flaggings as incorrect since the core meaning remained intact.\nHowever, such errors do not necessarily indicate that the algorithm misses clinically\nmeaningful changes. To assess whether the algorithm successfully identifies clinically\nimpactful errors despite this conservative behavior, we conducted a complementary\nexpert evaluation in the next section.\nClinical Expert Evaluation. To understand whether the algorithm could capture\nerrors that are medically impactful, two board-certified urologists from Mayo Clinic\nreviewed the empathy-enhanced responses to identify factual errors, fabrications, and\nclinically problematic statements without access to the automated analysis. The expert\nfocused on medically significant errors that could impact patient care or safety.\nOf 163 edited responses, 32 (19%) were flagged as containing potential fabrications\nor clinical inaccuracies. The expert categorized these errors into six patterns shown in\nTable 3. From these patterns, four key observations emerged:\n1. Follow-up Recommendations (13 cases): Unprompted follow-up suggestions\nthat risk inflating healthcare utilization and creating unnecessary patient anxiety.\n2. Clinical Assumptions and Inaccuracies (11 cases): Speculative statements or\nfactual errors that can undermine patient trust and potentially compromise safety.\n3. Inconsistent Tone Management (4 cases): False assurance or unnecessary fear\ninduction that carry disproportionate emotional impact on patient decision-making.\n4. Adds Unnecessary Advice (4 cases): Unsolicited advice unrelated to the\npatient’s query that can overwhelm patients and shift focus from their primary\nconcern.\nTo measure alignment between MedFactChecking and expert annotations for these\n36 responses, ML researchers mapped the automatically extracted ”missed” or ”Not\npreserved” facts ([d |= c′] = 0 and [d′ |= c] = 0) to the expert’s six fabrication\ncategories. For example, the added fact “A seat with a bit more cushioning can\nbe beneficial” was mapped to “Adds unnecessary advice.” We then computed cat-\negory coverage: the proportion of expert-identified fabrications where at least one\nextracted fact from that response was mapped to the same category. MedFactChecking\nachieved 90.62% overall coverage, with perfect detection (100%) for clinical assump-\ntions, unnecessary advice, and unnecessary doubt, and strong detection for follow-up\nrecommendations (92.3%). Only false assurance showed lower coverage (50%), though\nthis category had few cases (see Supplementary Table 3 for full breakdown). These\nresults demonstrate that despite moderate precision in granular fact-level validation,\nthe algorithm successfully captures the majority of clinically impactful errors.\n4 Results\nOur experimental framework addresses three research questions through a system-\natic pipeline: we use LLMs to edit physician responses for empathy, then evaluate\n11\n"}, {"page": 12, "text": "both empathy enhancement (using 3EM-Ranker) and factual preservation (using\nthe MedFactChecking Score).\n4.1 RQ1: Empathy Evaluation Results and Analysis\nTo evaluate empathetic quality, 3EM-Ranker performs pairwise comparisons\nbetween two responses. Accordingly, we conduct five comparison experiments: physi-\ncian vs. direct AI-generated; physician vs. simple-prompt edited; physician vs.\nrefined-prompt edited; direct AI-generated vs. simple-prompt edited; simple-prompt\nedited vs. refined-prompt edited. Table 1 summarizes the results using two LLM\njudges. We highlight four key observations.\nOverall Performances\nObservation 1: LLM-generated responses exhibit higher empathy than physician\nresponses. In all comparisons involving physicians (physician vs. direct AI; physician\nvs. simple-prompt edited; physician vs. refined-prompt edited), LLM responses are\njudged to be more empathetic in over 90% of cases. This trend is consistent across\nboth judge models. The finding aligns with previous work that LLMs produce more\nsupportive language than physicians.\nObservation 2: Increasing prompt restrictions reduces the empathy ranking. The\nrefined-prompt edited responses are more constrained than the simple-prompt edited\nresponses, which are themselves more constrained than direct AI outputs. In the\nLLM–LLM comparison experiments (direct vs. simple-prompt; simple-prompt vs.\nrefined-prompt), the more constrained version is consistently judged as less empa-\nthetic. This is expected since stricter prompts limit the model’s freedom to generate\nexpressive affective language. However, these same constraints help reduce hallucina-\ntions and improve factuality. We expand on this empathy and factuality trade-off in\n§4.3.\nObservation 3: Judges disagree more when comparing two LLM-generated\nresponses. When comparing physician responses against LLM outputs, Qwen3 and\nLLaMA3 judges produce closely aligned rankings. However, their disagreement\nincreases when evaluating two LLM-generated responses. This suggests that the two\nLLM responses tend to be very similar in empathetic tone, making the distinction\nharder and more sensitive to judge-model variation.\nObservation 4: Qwen3 judges label ties more frequently than LLaMA3. In the\nLLM–LLM comparison experiments, Qwen3 more frequently assigns both responses\nan equal empathy score compared to LLaMA3 (indicated by the lower sum of non-\nequal rankings). Our human analysis confirms that these cases indeed involve responses\nwith very similar levels of empathetic expression, indicating Qwen3 a better-calibrated\nempathy judge compared to LLaMA3.\nAnalysis of Equal-Empathy Cases\nWhen applying LLM-based editing to physician responses, we observe a small number\nof cases in which the physician and model-edited responses are judged to have equal\nlevels of empathy. These typically occur when the original physician response is already\nempathetic; in such instances, the edited version generally preserves the original tone\n12\n"}, {"page": 13, "text": "Table 1: 3EM-Ranker comparison results: percentage of pairs where system A is\njudged more empathic than system B, and vice versa, under two LLM judges.\nQwen3 Judge\nLLaMA3 Judge\nComparison (A vs B)\nModel\nA > B (%)\nB > A (%)\nA > B (%)\nB > A (%)\nPhysician (A) vs Direct AI (B)\nLLaMA3-8B\n0.0\n95.7\n2.4\n97.5\nMistral-7B-Instruct\n0.0\n90.8\n6.7\n93.3\nQwen3-14B\n0.6\n96.3\n3.1\n96.3\nGemini-2.5-Flash\n0.0\n96.9\n1.2\n98.1\nPhysician (A) vs Simple-Prompt Edited (B)\nLLaMA3-8B\n0.0\n94.5\n0.0\n99.4\nMistral-7B-Instruct\n0.0\n96.3\n0.0\n98.8\nQwen3-14B\n0.0\n96.3\n0.6\n99.4\nGemini-2.5-Flash\n0.0\n95.1\n1.2\n95.6\nPhysician (A) vs Refined-Prompt Edited (B)\nLLaMA3-8B\n0.0\n95.7\n3.1\n90.6\nMistral-7B-Instruct\n0.0\n94.5\n0.0\n99.6\nQwen3-14B\n0.0\n93.3\n0.6\n95.1\nGemini-2.5-Flash\n0.0\n93.3\n1.2\n92.6\nDirect AI (A) vs Simple-Prompt Edited (B)\nLLaMA3-8B\n27.0\n27.0\n67.5\n32.5\nMistral-7B-Instruct\n6.7\n54.0\n74.9\n25.1\nQwen3-14B\n11.0\n52.1\n40.5\n58.9\nGemini-2.5-Flash\n53.4\n6.1\n90.8\n7.9\nSimple-Prompt Edited (A) vs Refined-Prompt Edited (B)\nLLaMA3-8B\n63.4\n8.7\n95.7\n3.7\nMistral-7B-Instruct\n17.8\n24.5\n75.4\n23.9\nQwen3-14B\n55.8\n2.5\n96.9\n2.5\nGemini-2.5-Flash\n17.2\n27.6\n65.0\n27.6\nA and B denote the two systems in each comparison. Percentages may not sum to 100 due\nto ties or unclassified cases.\nand may add only mild validation phrases (e.g., “I understand your concerns”), which\ndo not substantially improve the perceived empathy. We also identify a few cases in\nwhich the physician response is ranked as more empathetic than the model-edited\nversion. This is primarily associated with two scenarios. First, the richness of physician\nresponses: some physician-authored replies are more narrative and highly personalized,\nnaturally conveying a stronger empathetic presence. Second, loss of personal touch\nduring model editing: the editing process occasionally removes individualized elements,\nsuch as references to prior interactions or subtle acknowledgments, which reduces the\npersonal warmth and empathy of the response.\n4.2 RQ2: Factuality Evaluation Results and Analysis\nTo evaluate factual preservation during empathy enhancement, we applied our Med-\nFactChecking Score to edited responses from all four models. All models used the\nsimple-prompt to edit physician responses, and Gemini-2.5-Flash was used for fact\nextraction and entailment.\nOverall Performance.\nFigure 3 presents factuality metrics and fact flow analysis across models. Gemini-2.5-\nFlash achieves the highest factual preservation with micro-recall of 0.92 and micro-\nprecision of 0.91, substantially outperforming other models. LLaMA, Mistral, and\nQwen3-14B demonstrate similar performance with F1 scores ranging from 0.77 to 0.79,\nsuggesting that among open-source models of varying sizes, model scale alone does\nnot substantially impact factual preservation during empathy enhancement. The close\n13\n"}, {"page": 14, "text": "alignment between micro and macro metrics across all models (maximum difference\n≤0.03) indicates consistent performance regardless of individual response complexity.\nThe fact flow analysis (Figure 3, bottom left) quantifies fact transformation during\nempathy editing. All models add substantial content: from 934 original facts, edited\nresponses contain 1194–1429 facts (28–53% increase), but models differ dramatically\nin how much content is grounded in the original response. The results reveal two dis-\ntinct failure modes. First, information loss (lower recall): non-Gemini models preserve\nonly 732–744 of 934 original facts (78–80%), potentially losing clinical information.\nGemini-2.5-Flash preserves 855 facts (91.5%). Second, hallucinated additions (lower\nprecision): non-Gemini models introduce 260–355 ungrounded facts (21–25% of edited\ncontent), risking patient safety through fabricated medical claims. Gemini-2.5-Flash’s\nsuperior performance stems from both minimal loss (8.5%) and controlled addition\n(9.5% hallucination rate). The distribution of fact counts per response (Figure 3, bot-\ntom right) shows that edited responses consistently contain more facts than originals\nacross all models (see Supplementary Table 4 for detailed breakdown).\nResponse Length and Factuality Trade-offs.\nFigure 4 reveals a systematic relationship between original response length and factual\npreservation during empathy enhancement. All models show a negative correlation\nbetween physician response length and fact ratio ( # edited facts\n# original facts): for short responses\n(<200 characters), mean fact ratios range from 1.45 (Gemini) to 1.90 (Qwen), while\nlonger responses (>600 characters) approach a ratio of 1.0. This pattern suggests that\nmodels interpret brevity as incompleteness and attempt to “fill in” missing context.\nHowever, models differ dramatically in whether this added content is factually\ngrounded. Non-Gemini models exhibit 12–23 percentage point precision drops for\nshort versus long responses, with Mistral showing the most severe degradation (0.65\nfor short vs. 0.88 for long). In contrast, Gemini-2.5-Flash maintains stable precision\nacross all length categories (0.90–0.95), successfully grounding its added content even\nwhen editing brief inputs. This length-dependent hallucination pattern has significant\nclinical implications: brief physician responses, common in asynchronous patient por-\ntals for straightforward updates, are precisely where non-Gemini models introduce the\nmost fabricated content, suggesting deployment-ready systems require either length-\naware safeguards or models like Gemini that maintain factual integrity regardless of\nresponse brevity.\n4.3 RQ3: Relation between Empathy and Factuality\nImpact of Empathy Intensity on Factual Preservation.\nTo investigate the relation between empathy enhancement and factual preservation,\nwe modified the simple-prompt’s empathy descriptor across three levels—“Empathic,”\n“High Empathic,” and “Extreme Empathic”—and used Gemini-2.5-Flash to gener-\nate edited responses for each condition. Figure 5 reveals a clear inverse relationship\nbetween empathy level and factual accuracy. While recall remains stable across\nall three conditions, precision demonstrates substantial degradation as empathy\n14\n"}, {"page": 15, "text": "Fig. 3: MedFactChecking Score analysis across models. Top: Micro (left) and macro\n(right) metrics for recall, precision, and F1. Bottom: Fact flow showing preserved and\ngrounded facts (left); fact count distributions (right). Gemini-2.5-Flash substantially\noutperforms other models.\nintensifies, declining from 0.91 (standard) to 0.80 (high) to 0.78 (extreme), a 13-\npercentage-point drop. This pattern indicates that aggressive empathy enhancement\nprimarily increases hallucinated additions rather than causing information loss from\nthe original response. The length-based analysis illustrates the mechanism underly-\ning this trade-off. Higher empathy levels systematically inflate fact ratios, with mean\nratios increasing from 1.45 (standard) to 2.24 (high) to 2.41 (extreme), indicating\nprogressively more aggressive content expansion. Critically, this expansion is accom-\npanied by declining precision: as empathy increases, more yellow and orange points\nappear throughout the distribution, with the most pronounced degradation occurring\nfor short responses.\nThese findings demonstrate that empathy and factuality exist in tension: while\nmodest empathy enhancement preserves >90% factual integrity, aggressive empathiza-\ntion nearly doubles content volume while degrading precision by 13 percentage points,\nrequiring careful calibration of empathy intensity for safe clinical deployment.\n15\n"}, {"page": 16, "text": "Fig. 4: Relationship between original response length and factual preservation. Top:\nFact ratio (edited/original facts) vs. response length, with precision indicated by color.\nBottom: Precision distribution across response length tertiles (Short: <231, Medium:\n231–393, Long: >393 characters).\nFig. 5: Impact of empathy intensity on factual preservation using Gemini-2.5-Flash.\nLeft: Micro metrics across three empathy levels showing stable recall but declining\nprecision. Right: Fact ratio vs. response length for each empathy level, with precision\nindicated by color. Higher empathy levels increase content expansion, while degrading\nprecision\nEmpathy-Factuality Trade-off\nTo investigate whether empathy enhancement can be achieved without compromising\nfactual accuracy, we evaluated three response generation strategies using Gemini-2.5-\nFlash on the general medical queries subsample (see §3.4): (1) direct AI-generated, (2)\nsimple-prompt, and (3) refined-prompt.\nThe results reveal a stark contrast between editing-based and generation-based\napproaches. The refined editing approach achieves near-perfect scores across both\nmicro (recall: 0.99, precision: 0.93, F1: 0.96) and macro metrics (recall: 0.93, precision:\n16\n"}, {"page": 17, "text": "0.93, F1: 0.93), demonstrating that carefully constrained empathy enhancement can\npreserve factual integrity. Simple editing shows modest degradation (micro recall: 0.95,\nprecision: 0.89, F1: 0.92; macro recall: 0.84, precision: 0.92, F1: 0.88), indicating that\nunconstrained empathy instructions lead to some fact omissions and additions.\nIn dramatic contrast, direct AI generation fails catastrophically on factual ground-\ning, achieving only 0.39 micro recall and 0.27 macro recall, meaning it preserves fewer\nthan 40% of facts that physicians would include. Precision remains low at 0.40, with\nF1 scores of 0.39 (micro) and 0.32 (macro). These low scores reflect that AI-generated\nresponses operate in a fundamentally different information space: they introduce\nextensive medical details, recommendations, and contextual information drawn from\nthe model’s training data rather than the physician’s specific response. Without access\nto the original physician response, the model cannot preserve its facts, resulting in\nresponses that may be medically sound but factually divergent from what the physician\nwould communicate (see Supplementary Fig. 7 for full comparison).\nIn addition, we use 3EM-Ranker to compare the empathy level of these responses,\nwe run three evaluation: direct responses v.s. simple editing responses, simple editing\nresponses v.s. refined-editing responses, and refined-editing responses v.s. physician\nresponses. The results in Table 5 in Appendix reveals the inverse ordering for empa-\nthy: direct AI-generated responses score highest, followed by simple editing, then\nrefined editing, with original physician responses scoring lowest. This demonstrates a\nfundamental tension:\nFactuality ranking: physician response > refined editing > simple editing > direct\ngeneration\nEmpathy ranking: direct generation > simple editing > refined editing > physician\nresponse\nThese findings suggest that editing physician responses is fundamentally superior\nto generating responses de novo for clinical deployment. The refined editing approach\nachieves 93–99% factual preservation while meaningfully improving empathy. Direct\ngeneration, despite maximizing perceived empathy, cannot preserve physician-specific\nfacts and introduces unverifiable content, making it unsuitable for clinical deployment\nwithout physician review.\n5 Discussion\nIn this work, we propose to utilize LLMs as editing to improve the empathetic degree\nof physician responses along with a empathy evaluation and fact checking scores to\ncompare the trade off between empathy and factuality. However, there are some future\nwork can further improve the work. Our evaluation dataset is relatively small due to\nthe substantial human effort required at multiple stages of data preparation, includ-\ning manual de-identification of patient questions and physician responses, as well as\nthe collection of high-quality annotations, particularly those from patients, which are\ninherently challenging to obtain. Moreover, the current dataset is restricted to the\nurology domain. As future work, we plan to extend our evaluation framework to addi-\ntional clinical specialties to assess its robustness and broader applicability. Another\nlimitation is that the current 3EM-Ranker prompt does not incorporate an explicit\n17\n"}, {"page": 18, "text": "definition of empathy. The absence of a clearly specified rubric and illustrative instruc-\ntion examples may constrain the model’s ability to consistently identify or evaluate\nempathetic behaviors. For future work, we intend to collaborate with psychologists or\npsychiatry experts to formalize a domain-informed empathy definition and develop a\nmore detailed rubric for comparison. Such specifications are likely to further strengthen\nthe reliability and discriminative capacity of the 3EM-Ranker metric.\nReferences\n[1] John W Ayers, Adam Poliak, Mark Dredze, Eric C Leas, Zechariah Zhu, Jes-\nsica B Kelley, Dennis J Faix, Aaron M Goodman, Christopher A Longhurst,\nMichael Hogarth, et al. Comparing physician and artificial intelligence chatbot\nresponses to patient questions posted to a public social media forum. JAMA\ninternal medicine, 183(6):589–596, 2023.\n[2] Nicolas Carl, Sarah Haggenm¨uller, Jana Theres Winterstein, Lisa Nguyen,\nChristoph Wies, Martin Joachim Hetz, Maurin Helen Mangold, Britta Gr¨une,\nMaurice Stephan Michel, Titus Josef Brinker, et al. Patient insights into empathy,\ncompassion and self-disclosure in medical large language models: Results from\nthe ipallm iii study. World Journal of Urology, 43(1):1–9, 2025.\n[3] Yu Ying Chiu, Ashish Sharma, Inna Wanyin Lin, and Tim Althoff. A compu-\ntational framework for behavioral assessment of llm therapists. arXiv preprint\narXiv:2401.00820, 2024.\n[4] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen\nSachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen,\net al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodal-\nity, long context, and next generation agentic capabilities.\narXiv preprint\narXiv:2507.06261, 2025.\n[5] Benjamin MP Cuff, Sarah J Brown, Laura Taylor, and Douglas J Howat.\nEmpathy: A review of the concept. Emotion review, 8(2):144–153, 2016.\n[6] Mark H Davis. Measuring individual differences in empathy: evidence for a mul-\ntidimensional approach. Journal of personality and social psychology, 44(1):113,\n1983.\n[7] Frans Derksen, Jozien Bensing, and Antoine Lagro-Janssen.\nEffectiveness of\nempathy in general practice: a systematic review. The British Journal of General\nPractice, 63(606):e76, 2012.\n[8] Priyanka Dey and Roxana Girju. Enriching deep learning with frame seman-\ntics for empathy classification in medical narrative essays. In Proceedings of the\n13th International Workshop on Health Text Mining and Information Analysis\n(LOUHI), pages 207–217, 2022.\n[9] Priyanka Dey and Roxana Girju.\nInvestigating stylistic profiles for the\ntask of empathy classification in medical narrative essays.\narXiv preprint\narXiv:2302.01839, 2023.\n[10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,\net al. The llama 3 herd of models. arXiv e-prints, pages arXiv–2407, 2024.\n18\n"}, {"page": 19, "text": "[11] Ezequiel Gleichgerrcht and Jean Decety. Empathy in clinical practice: how indi-\nvidual dispositions, gender, and experience moderate empathic concern, burnout,\nand emotional distress in physicians. PloS one, 8(4):e61526, 2013.\n[12] Joanne M Hemmerdinger, Samuel DR Stoddart, and Richard J Lilford. A sys-\ntematic review of tests of empathy in medicine. BMC medical education, 7(1):24,\n2007.\n[13] Mohammadreza\nHojat,\nJennifer\nDeSantis,\nStephen\nC\nShannon,\nLuke\nH\nMortensen, Mark R Speicher, Lynn Bragan, Marianna LaNoue, and Leonard H\nCalabrese. The jefferson scale of empathy: a nationwide study of measurement\nproperties, underlying components, latent variable structure, and national norms\nin medical students. Advances in Health Sciences Education, 23(5):899–920, 2018.\n[14] Mohammadreza Hojat et al. Empathy in patient care: antecedents, development,\nmeasurement, and outcomes, volume 77. Springer, 2007.\n[15] Mohammadreza Hojat, Daniel Z Louis, Fred W Markham, Richard Wender, Carol\nRabinowitz, and Joseph S Gonnella. Physicians’ empathy and clinical outcomes\nfor diabetic patients. Academic medicine, 86(3):359–364, 2011.\n[16] Mahshid Hosseini and Cornelia Caragea.\nIt takes two to empathize: One to\nseek and one to provide. In Proceedings of the AAAI conference on artificial\nintelligence, volume 35, pages 13018–13026, 2021.\n[17] Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxi-\nang Jiao, Zhaopeng Tu, and Michael R Lyu. Apathetic or empathetic? evaluating\nllms’ emotional alignments with humans.\nAdvances in Neural Information\nProcessing Systems, 37:97053–97087, 2024.\n[18] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian\nWang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey\non hallucination in large language models: Principles, taxonomy, challenges, and\nopen questions. ACM Transactions on Information Systems, 43(2):1–55, 2025.\n[19] Lujain Ibrahim, Franziska Sofia Hafner, and Luc Rocher. Training language mod-\nels to be warm and empathetic makes them less reliable and more sycophantic.\narXiv preprint arXiv:2507.21919, 2025.\n[20] Zainab Iftikhar, Sean Ransom, Amy Xiao, Nicole Nugent, and Jeff Huang. Ther-\napy as an nlp task: psychologists’ comparison of llms and human peers in cbt.\narXiv preprint arXiv:2409.02244, 2024.\n[21] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung.\nSurvey of hallucination in\nnatural language generation. ACM computing surveys, 55(12):1–38, 2023.\n[22] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-\ndra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L´elio Renard Lavaud, Marie-Anne Lachaux,\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth´ee Lacroix,\nand William El Sayed. Mistral 7b, 2023.\n[23] John M Kelley, Gordon Kraft-Todd, Lidia Schapira, Joe Kossowsky, and Helen\nRiess. The influence of the patient-clinician relationship on healthcare outcomes:\na systematic review and meta-analysis of randomized controlled trials. PloS one,\n9(4):e94207, 2014.\n19\n"}, {"page": 20, "text": "[24] Sung Soo Kim, Stan Kaplowitz, and Mark V Johnston. The effects of physi-\ncian empathy on patient satisfaction and compliance. Evaluation & the health\nprofessions, 27(3):237–251, 2004.\n[25] Allison Lahnala, Charles Welch, David Jurgens, and Lucie Flek. A critical reflec-\ntion and forward perspective on empathy and natural language processing. arXiv\npreprint arXiv:2210.16604, 2022.\n[26] Eric B Larson and Xin Yao. Clinical empathy as emotional labor in the patient-\nphysician relationship. Jama, 293(9):1100–1106, 2005.\n[27] Peter Lee, Sebastien Bubeck, and Joseph Petro. Benefits, limits, and risks of gpt-4\nas an ai chatbot for medicine. New England Journal of Medicine, 388(13):1233–\n1239, 2023.\n[28] Yoon Kyung Lee, Inju Lee, Minjung Shin, Seoyeon Bae, and Sowon Hahn.\nEnhancing empathic reasoning of large language models based on psychotherapy\nmodels for ai-assisted social support. Korean Journal of Cognitive Science, 35(1),\n2024.\n[29] Man Luo, Christopher J Warren, Lu Cheng, Haidar M Abdul-Muhsin, and\nImon Banerjee.\nAssessing empathy in large language models with real-world\nphysician-patient interactions. In 2024 IEEE International Conference on Big\nData (BigData), pages 6510–6519. IEEE, 2024.\n[30] MM Mello and N Guha. Chatgpt and physicians’ malpractice risk. jama health\nforum 4 (5): e231938, 1938.\n[31] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang\nKoh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.\nFActScore:\nFine-grained atomic evaluation of factual precision in long form text genera-\ntion.\nIn Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing,\npages 12076–12100, Singapore, December 2023. Association for Computational\nLinguistics.\n[32] Monica Munnangi, Akshay Swaminathan, Jason Alan Fries, Jenelle Jindal, San-\njana Narayanan, Ivan Lopez, Lucia Tu, Philip Chung, Jesutofunmi A Omiye,\nMehr Kashyap, et al. Factehr: A dataset for evaluating factuality in clinical notes\nusing llms. In Machine Learning for Healthcare Conference. PMLR, 2025.\n[33] Melanie Neumann, Friedrich Edelh¨auser, Diethard Tauschel, Martin R Fischer,\nMarkus Wirtz, Christiane Woopen, Aviad Haramati, and Christian Scheffer.\nEmpathy decline and its reasons: a systematic review of studies with medical\nstudents and residents. Academic medicine, 86(8):996–1009, 2011.\n[34] Jesutofunmi A Omiye, Jenna C Lester, Simon Spichak, Veronica Rotemberg, and\nRoxana Daneshjou. Large language models propagate race-based medicine. NPJ\nDigital Medicine, 6(1):195, 2023.\n[35] Desmond Ong, Amit Goldenberg, Michael Inzlicht, and Anat Perry. Ai-generated\nempathy: Opportunities, limits, and future directions.\n[36] Scott Provence, Alyssa A Forcehimes, and Alyssa Forcehimes. Algorithms for\nempathy: Using machine learning to categorize common empathetic traits across\nprofessional and peer-based conversations. Cureus, 16(4), 2024.\n[37] David Rakel, Bruce Barrett, Zhengjun Zhang, Theresa Hoeft, Betty Chewning,\n20\n"}, {"page": 21, "text": "Lucille Marchand, and Jo Scheder.\nPerception of empathy in the therapeu-\ntic encounter: effects on the common cold.\nPatient education and counseling,\n85(3):390–397, 2011.\n[38] Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. Towards\nempathetic open-domain conversation models: A new benchmark and dataset.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 5370–5381, 2019.\n[39] Ashish Sharma, Inna W Lin, Adam S Miner, David C Atkins, and Tim Althoff.\nHuman–ai collaboration enables more empathic conversations in text-based peer-\nto-peer mental health support. Nature Machine Intelligence, 5(1):46–57, 2023.\n[40] Ashish Sharma, Adam Miner, David Atkins, and Tim Althoff. A computational\napproach to understanding empathy expressed in text-based mental health sup-\nport. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5263–5276, 2020.\n[41] Vishal Anand Shetty, Shauna Durbin, Meghan S Weyrich, Air´ın Denise Mart´ınez,\nJing Qian, and David L Chin. A scoping review of empathy recognition in text\nusing natural language processing. Journal of the American Medical Informatics\nAssociation, 31(3):762–775, 2024.\n[42] Garriy Shteynberg, Jodi Halpern, Amir Sadovnik, Jon Garthoff, Anat Perry,\nJessica Hay, Carlos Montemayor, Michael A Olson, Tim L Hulsey, and Abrol\nFairweather. Does it matter if empathic ai has no empathy? Nature Machine\nIntelligence, 6(5):496–497, 2024.\n[43] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won\nChung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.\nLarge language models encode clinical knowledge. Nature, 620(7972):172–180,\n2023.\n[44] Vera Sorin, Dana Brin, Yiftach Barash, Eli Konen, Alexander Charney, Girish\nNadkarni, and Eyal Klang.\nLarge language models and empathy: systematic\nreview. Journal of medical Internet research, 26:e52597, 2024.\n[45] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura\nGutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in\nmedicine. Nature medicine, 29(8):1930–1940, 2023.\n[46] Changyu Wang, Siru Liu, Hao Yang, Jiulin Guo, Yuxuan Wu, and Jialin Liu.\nEthical considerations of using chatgpt in health care. Journal of Medical Internet\nResearch, 25:e48009, 2023.\n[47] Yinuo Wang, Huaping Zhang, and Jianyun Shang. Empllm: Enhancing empa-\nthy in llms through psychologist simulation.\nIn International Conference on\nIntelligent Multilingual Information Processing, pages 205–218. Springer, 2024.\n[48] Anuradha Welivita and Pearl Pu. A taxonomy of empathetic response intents in\nhuman social conversations. arXiv preprint arXiv:2012.04080, 2020.\n[49] Walter F Wiggins and Ali S Tejani. On the opportunities and risks of founda-\ntion models for natural language processing in radiology. Radiology: Artificial\nIntelligence, 4(4):e220119, 2022.\n[50] Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott\n21\n"}, {"page": 22, "text": "Fleming, Michael A Pfeffer, Jason Fries, and Nigam H Shah. The shaky foun-\ndations of large language models and foundation models for electronic health\nrecords. npj digital medicine, 6(1):135, 2023.\n[51] Jia Xu, Tianyi Wei, Bojian Hou, George Demiris, and Li Shen. Revolutionizing\ndementia care: Enhancing talk therapy with fine-tuned large language models\nusing gpt self-generated data. Alzheimer’s & Dementia, 20:e093496, 2024.\n[52] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al.\nQwen3 technical\nreport. arXiv preprint arXiv:2505.09388, 2025.\n[53] Zhou Yang, Zhaochun Ren, Wang Yufeng, Shizhong Peng, Haizhou Sun, Xiaofei\nZhu, and Xiangwen Liao. Enhancing empathetic response generation by augment-\ning llms with small-scale empathetic models. arXiv preprint arXiv:2402.11801,\n2024.\n[54] Heng Yao, Alexandre Gomes de Siqueira, Margeaux Johnson, Roberta Pileggi,\nAmy Blue, Michael D Bumbach, Rene Love, and Benjamin Lok.\nEnhancing\nempathic communication in healthcare education through virtual conversations:\nLeveraging large language models for real-time feedback. In Proceedings of the\n26th Symposium on Virtual and Augmented Reality, pages 41–50, 2024.\n[55] Stav Yosef, Moreah Zisquit, Ben Cohen, Anat Brunstein Klomek, Kfir Bar, and\nDoron Friedman. The impact of fine-tuning llms on the quality of automated\ntherapy assessed by digital patients. npj Mental Health Research, 4(1):43, 2025.\n[56] Yifan Zhang, Christopher Radishian, Sabine Brunswicker, Dan Whitenack, and\nDaniel W Linna Jr. Empathetic language in llms under prompt engineering: A\ncomparative study in the legal field. Procedia Computer Science, 244:308–317,\n2024.\n[57] Yimeng Zhang, Xu Li, Junfeng Zhu, Zhongzhen Sheng, and Tony Rousmaniere. ”\nwhat happens, what helps, what hurts:” a qualitative analysis of user experiences\nwith large language models for mental health support.\n[58] Haiquan Zhao, Lingyu Li, Shisong Chen, Shuqi Kong, Jiaan Wang, Kexin Huang,\nTianle Gu, Yixu Wang, Jian Wang, Liang Dandan, et al. Esc-eval: Evaluating\nemotion support conversations in large language models. In Proceedings of the\n2024 Conference on Empirical Methods in Natural Language Processing, pages\n15785–15810, 2024.\n[59] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-\nas-a-judge with mt-bench and chatbot arena. Advances in neural information\nprocessing systems, 36:46595–46623, 2023.\n22\n"}, {"page": 23, "text": "A Prompt Templates\nAll prompts used in this study are presented below.\nA.1 Empathy Enhancement Prompts\nSimple Editing Prompt\nYou will be provided with a patient’s question and a physician’s response. Your\ntask is to revise the physician’s response to make it more empathetic while pre-\nserving its original meaning and writing style. Patient Question: {PQ} Physician’s\nresponse: {PR}\nSupplementary Fig. 1: Editing prompt for empathy enhancement. PQ refers to the\npatient question and PR refers to the physician’s response.\nRefine Editing Prompt\nKey editing principles: 1. Preserve factual and clinical accuracy.\n- Do not introduce or infer medical facts or conditions not explicitly stated by the\nphysician.\n- Do not make any clinical assumptions or diagnoses that are not present in the\noriginal response.\n2. Respect the physician’s intent.\n- Do not add follow-up recommendations or next-step suggestions unless they\nalready appear in the physician’s original response.\n- Do not add new advice, warnings, or treatment instructions.\n3. Maintain emotional balance.\n- Do not add false reassurance or overconfidence.\n- Do not introduce unnecessary doubt, fear, or alarming language.\n- Empathy should be expressed through tone, acknowledgment, and understand-\ning—not through added medical content.\n4. Preserve structure and style.\n- Keep the response roughly the same length as the original.\n- Maintain the physician’s professional tone and sentence structure where possible.\n- Revise only what’s necessary to make the tone warmer, more understanding, or\nmore supportive.\nSupplementary Fig. 2: The refined editing prompt with explicit behavioral con-\nstraints.\n23\n"}, {"page": 24, "text": "A.2 Evaluation Prompts\nEmpathy Evaluation Prompt\nYou are an expert in comparing the empathy level of two responses to a patient\nquestion. You will be given a patient question, and two responses, your task is to\nevaluate which answer is more empathetic.\nPatient Question: {PQ}\nResponse 1: {R1}\nResponse 2:{R2}\nWhich response is more empathetic? Response 1 or 2?\n- Response 1: More empathetic\n- Response 2: More empathetic\n- Both responses are equally empathetic\nSupplementary Fig. 3: 3-EMRank metric Prompt.\nFactuality Evaluation Prompt\nYou are an expert on natural language entailment.\nYour task is to deduce whether premise statements entail hypotheses.\nReturn only ’1’ if the hypothesis can be fully entailed by the premise.\nReturn only ’0’ if the hypothesis contains information that cannot be entailed by\nthe premise.\nGenerate the answer in JSON format with the following keys:\n‘entailment prediction’: 1 or 0, whether the claim can be entailed.\nOnly return the JSON-formatted answer and nothing else.\nPremise: {premise}\nHypothesis: {hypothesis}\nHere is the JSON-formatted answer:\nSupplementary Fig. 4: Prompt for entailment evaluation\n24\n"}, {"page": 25, "text": "You are extracting medical facts from a physician’s response to a patient question.\nPlease breakdown the PHYSICIAN’S RESPONSE into independent MEDICAL\nfacts as a string delimited by “//” to separate the facts.\nDO NOT include as facts:\n- Pure emotional support (e.g., ”I understand”, ”I’m here to help”)\n- Non-medical expressions of care (e.g., ”We care about you”)\n- General conversational elements (e.g., ”Glad you’re doing well”)\n- Repetitions or acknowledgments of patient’s reported symptoms (e.g., ”I under-\nstand you have pain”)\nExample 1:\nNote: “Don’t worry, we’ll get through this together. The chest X-ray shows\npneumonia in the right lung. Start antibiotics twice daily for 7 days. You’re in\ngood hands.”\nAtomic facts:\nThe chest X-ray shows pneumonia. // The pneumonia is in the right lung. //\nStart antibiotics. // Take antibiotics twice daily. // Continue antibiotics for 7\ndays.\nExample 2:\nNote: “There is a dense consolidation in the left lower lobe.”\nAtomic facts:\nThere is a consolidation. // The consolidation is dense. // The consolidation is\non the left. // The consolidation is in a lobe. // The consolidation is in the lower\nportion of the left lobe.\nDo not include any other text, or say “Here is the list...”\nNote: {text}\nSupplementary Fig. 5: Prompt for medical fact extraction\n25\n"}, {"page": 26, "text": "B Data\nPhysician responses are concise (mean 65.8±50.4 words) while patient questions\nare longer and more variable (mean 82.2±56.2 words; Supplementary Fig. 6). Since\n91.4% of questions require patient-specific EHR data, our approach of editing existing\nphysician responses offers a more realistic alternative to end-to-end QA models.\nSupplementary Fig. 6: Statistical characteristics of the patient-physician QA\ndataset (N=163). Physician responses are notably concise while patient questions show\ngreater length variability. Question classification reveals that 91% require patient-\nspecific EHR data rather than general medical knowledge.\nC Metric Validation\nC.1 3EM-Ranker Validation\nWe validated 3EM-Ranker through human evaluation with three prostate cancer\npatients comparing empathy between physician and AI responses. Alignment scores\nof 0.57 (Qwen-3) and 0.55 (LLaMA-3) substantially exceed the 0.23 reported for EM-\nRank [29].\nSupplementary\nTable\n1: Alignment\nscores between 3-EMRank and human\nannotators.\nModel\nAlignment Score\n3-EMRank (LLaMA-3)\n0.55\n3-EMRank (Qwen-3)\n0.57\nPrior work (EM-Rank)\n0.23\n26\n"}, {"page": 27, "text": "C.2 MedFactChecking Validation\nAutomated Fact Extraction Validation\nTwo machine learning scientists independently verified the accuracy of facts flagged\nas added (hallucinated, [d |= c′] = 0) or not preserved (missed, [d′ |= c] = 0) by the\nalgorithm. Validation achieved 83.6% precision for detecting hallucinated facts and\n72.7% for not-preserved facts (see Supplementary Table 2).\nSupplementary Table 2: Precision of automated fact extraction validated\nby ML expert annotators.\nResponse Type\nTotal Facts\nFlagged\nConfirmed\nPrecision\nOriginal Response\n934\n191 (Not Preserved)\n139\n72.7%\nEdited Response\n1230\n262 (Added)\n219\n83.6%\nClinical Expert Evaluation\nTwo\nboard-certified\nurologists\nfrom\nMayo\nClinic\nreviewed\nempathy-enhanced\nresponses without access to the automated analysis, flagging 32 of 163 (19%) as con-\ntaining potential fabrications or clinical inaccuracies. These errors fell into six patterns:\nfollow-up recommendations (13 cases), clinical assumptions/speculation (7), clinical\ninaccuracies (4), unnecessary advice (4), unnecessary doubt/fear (2), and false assur-\nance (2). To measure alignment, ML researchers mapped automatically extracted\nfacts ([d |= c′] = 0 and [d′ |= c] = 0) to these categories and computed coverage:\nthe proportion of expert-identified fabrications where the algorithm detected at least\none corresponding fact. MedFactChecking achieved 90.62% overall coverage (29/32),\nwith perfect detection for clinical assumptions, unnecessary advice, and unnecessary\ndoubt/fear, indicating that despite moderate precision in granular fact-level validation,\nthe algorithm captures the majority of clinically impactful errors (see Supplementary\nTable 3).\nSupplementary Table 3: Expert-identified fabrication patterns and Med-\nFactChecking coverage.\nPattern\nTotal\n(Expert)\nDetected\n(Algorithm)\nMissed\nCoverage\nAdded follow-up recommendation\n13\n12\n1\n92.3%\nClinical assumption/speculation\n7\n7\n0\n100%\nClinical inaccuracy\n4\n3\n1\n75.0%\nAdds unnecessary advice\n4\n4\n0\n100%\nAdds unnecessary doubt/fear\n2\n2\n0\n100%\nFalse assurance\n2\n1\n1\n50.0%\nOverall\n32\n29\n3\n90.62%\n27\n"}, {"page": 28, "text": "D Supplementary Results\nD.1 Factuality Evaluation Results\nTable 4 quantifies fact transformation during empathy editing. All models add sub-\nstantial content (28–53% increase in fact count), but differ dramatically in how much\nof this content is grounded in the original response. The results reveal two distinct fail-\nure modes. First, information loss (lower recall): non-Gemini models fail to preserve\n20–22% of original medical facts, that might also include critical clinical informa-\ntion. Second, hallucinated additions (lower precision): these same models introduce\n260–355 unsupported facts (21–25% of edited content), risking patient safety through\nfabricated medical claims. Gemini-2.5-Flash’s superior performance stems from both\nminimal loss (8.5%) and controlled addition (9.5% hallucination rate).\nSupplementary Table 4: Fact flow analysis across empathy-editing models. Loss Rate\nmeasures information omission from original responses; Hallucination Rate measures\nunsupported additions in edited responses. Metrics are computed as: New Facts = Edited\n−Grounded; Loss Rate = (Original −Preserved) / Original; Hallucination Rate = New\n/ Edited.\nModel\nOriginal\n|C|\nPreserved\nP I[d′ |= c]\nEdited\n|C′|\nGrounded\nP I[d |= c′]\nNew\nFacts\nLoss\nRate\nHalluc.\nRate\nGemini-2.5-Flash\n934\n855\n1,194\n1,081\n113\n8.5%\n9.5%\nLlama-3.1-70B\n934\n743\n1,230\n970\n260\n20.4%\n21.1%\nMistral-7B\n934\n732\n1,373\n1,053\n320\n21.6%\n23.3%\nQwen3-14B\n934\n744\n1,429\n1,074\n355\n20.3%\n24.8%\nD.2 Relation of Empathy and Factuality\nWe evaluated three strategies using Gemini-2.5-Flash on general medical queries:\n(1) direct AI-generated, (2) simple-prompt, and (3) refined-prompt. Refined-prompt\nachieves near-perfect factual preservation (micro/macro F1: 0.96/0.93), simple-prompt\nshows modest degradation (F1: 0.92/0.88), while direct AI-generated fails catastroph-\nically (F1: 0.39/0.32), preserving fewer than 40% of physician facts (Supplementary\nFig. 7). Empathy evaluation using 3EM-Ranker reveals an inverse pattern (Supple-\nmentary Table 5): Factuality: physician > refined-prompt > simple-prompt > direct\nAI-generated; Empathy: direct AI-generated > simple-prompt > refined-prompt\n> physician. This demonstrates that editing physician responses achieves 93–99%\nfactual preservation while improving empathy, whereas direct generation introduces\nunverifiable content unsuitable for clinical deployment.\n28\n"}, {"page": 29, "text": "Supplementary Table 5: 3EM-Ranker comparison results: percentage of pairs\nwhere system A is judged more empathic than system B, and vice versa, under two\nLLM judges. Use Gemini to generate the patient responses.\nQwen3 Judge\nLLaMA3 Judge\nComparison (A vs B)\nA > B (%)\nB > A (%)\nA > B (%)\nB > A (%)\nDirect AI (A) vs Simple-Prompt Edited (B)\n71.4\n0.0\n85.7\n14.3\nSimple-Prompt Edited (A) vs Refined-Prompt Edited (B)\n71.5\n21.4\n64.2\n35.7\nRefined-Prompt Edited (A) vs Physician (B)\n57.1\n0.0\n92.8\n0.0\nA and B denote the two systems in each comparison. Percentages may not sum to 100 due\nto ties or unclassified cases.\nSupplementary Fig. 7: MedFactChecking scores comparing three response gener-\nation strategies on general medical questions (n=14). Refined editing maintains high\nrecall and precision (0.93–0.99), while direct AI generation fails to preserve physician\nfacts (recall: 0.27–0.39), demonstrating the superiority of editing over generation for\nfactual grounding.\n29\n"}]}