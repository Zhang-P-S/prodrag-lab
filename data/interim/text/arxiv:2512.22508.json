{"doc_id": "arxiv:2512.22508", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.22508.pdf", "meta": {"doc_id": "arxiv:2512.22508", "source": "arxiv", "arxiv_id": "2512.22508", "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals", "authors": ["Lucky Susanto", "Anasta Pranawijayana", "Cortino Sukotjo", "Soni Prasad", "Derry Wijaya"], "published": "2025-12-27T07:51:50Z", "updated": "2025-12-27T07:51:50Z", "summary": "Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.22508v1", "url_pdf": "https://arxiv.org/pdf/2512.22508.pdf", "meta_path": "data/raw/arxiv/meta/2512.22508.json", "sha256": "0887ecc6b5b0a95cfef15556153f5b2da338bef0e12688b74c7c04358e6e8a49", "status": "ok", "fetched_at": "2026-02-18T02:23:46.410761+00:00"}, "pages": [{"page": 1, "text": "Predicting LLM Correctness in Prosthodontics Using Metadata and\nHallucination Signals\nLucky Susanto1\na, Anasta Pranawiyana2, Cortino Sukotjo3\nb, Soni Prasad4\nc, Derry Wijaya1,5\nd\n1Department of Data Science, Monash University Indonesia, Tangerang, Indonesia\n2Independent Researcher\n3Department of Prosthodontics, University of Pittsburgh, Pittsburgh, Pennsylvania\n4Department of Restorative Sciences, University of North Carolina Adams School of Dentistry, Chapel Hill, North Carolina\n5Department of Computer Science, Boston University, Boston, Massachusetts\nlucky.susanto@monash.edu, adhibaye@gmail.com, COS162@pitt.edu, prasadso@unc.edu, derry.wijaya@monash.edu\nKeywords:\nLarge Language Models, Hallucination, Correctness Prediction, Model Reliability, Prompting Strategies\nAbstract:\nLarge language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and med-\nical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major con-\ncern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether\nan LLM’s response is correct remains a critical yet underexplored problem. This study investigates the feasi-\nbility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model\n(OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across\nthree distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our find-\nings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a\nprecision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual\nhallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallu-\ncination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter\nthe models’ internal behaviors and the predictive utility of their metadata. These results present a promising\ndirection for developing reliability signals in LLMs but also highlight that the methods explored in this paper\nare not yet robust enough for critical, high-stakes deployment.\n1\nINTRODUCTION\nLarge language models (LLMs) have seen an in-\ncrease in adoption, even in high-stakes domains such\nas healthcare. Despite their capabilities, LLMs are\nstill prone to generating hallucinated content. This\nis particularly dangerous, as these hallucinations of-\nten sound logical yet contain non-factual information\n(Huang et al., 2025). This issue is particularly con-\ncerning in medical education, where LLMs are in-\ncreasingly used as learning aids or evaluators. In such\nsettings, hallucinated yet plausible explanations can\nmisinform students or reinforce misconceptions, in-\ndirectly affecting clinical competence (Abd-alrazaq\net al., 2023).\na\nhttps://orcid.org/0009-0003-3494-9249\nb\nhttps://orcid.org/0000-0002-2171-004X\nc\nhttps://orcid.org/0000-0002-4297-5012\nd\nhttps://orcid.org/0000-0002-0848-4703\nDue to the danger of hallucinations, many efforts\nhave been made to detect and mitigate them (Man-\nakul et al., 2023; Kadavath et al., 2022). These works\ndetect hallucinations by utilizing information such as\nconsistency, log probability, entropy distribution, and\nmany more. However, to our knowledge, previous\nwork has not directly attempted to predict model cor-\nrectness. As previously mentioned, predicting cor-\nrectness is vital in medical education, where inaccu-\nrate or misleading model responses could negatively\ninfluence learners’ understanding or clinical reason-\ning.\nIn this study, we analyze two distinct LLMs on\na specialized, non-reasoning task (i.e., a multiple-\nchoice question answering problem) in the field of\nprosthodontics. We examine how metadata, specif-\nically response consistency and answer token log\nprobability, can be used to predict whether the model\nwill answer the question correctly or not. Moreover,\nwe then compare different prompting strategies and\narXiv:2512.22508v1  [cs.LG]  27 Dec 2025\n"}, {"page": 2, "text": "how they affect these two different models. Through\nthis work, we bring the following contributions:\n1. We demonstrate that it is feasible to predict model\ncorrectness by only looking at its consistency and\nlog probabilities, achieving an increased accuracy\nof up to +7.14% when compared to simply assum-\ning LLMs are always correct, alongside a maxi-\nmum precision of 83.12%.\n2. We show that under an idealized setup, hallucina-\ntion signals are strong indicators of model correct-\nness. However, this requires a powerful hallucina-\ntion detection model.\n3. In relation to the previous finding, we also show\nthat consistency and token log probabilities are\nnot predictive of hallucination signals.\n4. Lastly, we found that even though differing\nprompting strategies have no significant effect on\nmodel accuracy, different prompting strategies af-\nfect GPT-4o’s behavior more than OSS-120B’s, as\nreflected in the predictiveness of metadata.\n2\nRELATED WORKS\n2.1\nReasoning LLMs\nReasoning LLMs are a relatively new technology.\nPopularized by DeepSeek-R1 (DeepSeek-AI et al.,\n2025), these LLMs show incredible promise in tasks\nsuch as math, coding, and other reasoning-heavy tasks\n(Comanici, 2025; Yang et al., 2025). Perhaps due to\ntheir novelty, there is only a minimal amount of work\nanalyzing the effect of reasoning on non-reasoning\ntasks. A recently released paper notes that we should\ninduce reasoning only for specific tasks requiring rea-\nsoning, as they found that a general LLM would be\nmore efficient otherwise (Boizard et al., 2025).\n2.2\nPrompting Strategies\nAfter the rise of LLMs, a particular prompting strat-\negy rose above the rest: Chain-of-Thought prompt-\ning. Shortened to CoT, this prompting strategy has\nbeen shown to induce reasoning capabilities in LLMs\n(Wei et al., 2022).\nIn fact, reasoning LLMs such\nas DeepSeek-R1 are trained through post-training\non reasoning-oriented datasets containing CoT exem-\nplars, designed to incentivize step-by-step reasoning.\nHowever, recent work has cast doubt on this ap-\nproach, as it has been shown that CoT prompting can\nbe unfaithful (Turpin et al., 2023) and that the rea-\nsoning induced by CoT is potentially a mirage (Zhao\net al., 2025). Specifically, it seems that CoT is used\nby the model to match the pattern of reasoning, not\nto perform reasoning itself. In contrast, Answer-then-\nExplain (post-hoc explanation) is less popular, though\nthere exist works that utilize it to improve model per-\nformance (Krishna et al., 2023).\nLastly, it has been reported that despite differ-\ning prompting strategies having the same average\nfaithfulness score, they can actually have a high de-\ngree of disagreement in the explanations they gener-\nate (Huang et al., 2023). This shows that different\nprompting strategies affect the behavior of how LLMs\nproduce their answers. However, this work has only\nbeen done on general LLMs, leaving room to explore\nthis for reasoning LLMs.\n2.3\nLLM-as-a-Judge\nDue to the impressive capabilities of modern LLMs,\na new paradigm has recently emerged.\nKnown as\n”LLM-as-a-judge” (Li et al., 2025), this method uti-\nlizes an LLM to act as a judge that rates the output of\nanother model based on specific metrics. Of course, it\nis possible that the judge itself is prone to error, thus\nchoosing a reliable model is a must. Even when a re-\nliable model is chosen, it is imperative to understand\nthat LLMs are not perfect.\n3\nMETHODOLOGY\n3.1\nDataset\nWe use the 2025 National Prosthodontics Resident\nExam questions in this work. The exam originally\nconsists of 150 multiple-choice questions. We remove\nquestions that use books as the source of reference\ndue to the difficulty of extracting ground truth expla-\nnations. Furthermore, we exclude questions that in-\nclude images. After filtering, we end up with a final\ndataset of 98 questions.\nTo obtain the ground truth explanation, we use\nGPT-4o to extract an explanatory paragraph with re-\nspect to a given question and answer. The extracted\nparagraph is then evaluated by two experts in the field\nof prosthodontics, ensuring its quality.\nAny incor-\nrectly extracted paragraphs were then manually fixed\nby the authors. Refer to Figure 1’s Primary Data Field\nfor an overview.\n3.2\nModels\nWe evaluate two models in this work:\n"}, {"page": 3, "text": "Figure 1: Dataset Overview. First row in grey denotes Primary Data Field. Second row in blue denotes Inference Data Field.\nThird row in cyan denotes LLM-as-a-Judge Data Field.\n1. GPT-4o (OpenAI et al., 2024): A proprietary gen-\neralist LLM\n2. OSS-120B (OpenAI et al., 2025):\nAn open-\nsource reasoning LLM\nWe prompt each model five times for each promting\nstrategy. Refer to Figure 1’s Inference Data Field for\nan overview.\n3.3\nPrompting Strategy\nWe use three types of prompting in this work:\n1. Answer only (A): Provided a question, the model\nanswers directly.\n2. Answer then Explain (AE): Provided a question,\nthe model answers and then provides a justifica-\ntion.\n3. Explain then Answer (EA): Commonly known\nas Chain-of-Thought; provided a question, the\nmodel reasons to reach an answer.\nWe use the following system prompt to enforce a\nconsistent answer format for both models:\nInstruction: You must answer this question using the exact\ntemplate below\nResponse:\nAnswer: Only the selected option (e.g., A [without anything\nelse])\nExplanation: Your explanation\nTo change the prompting strategy, we modify the sys-\ntem prompt as follows:\n1. Leave it unchanged for Answer then Explain.\n2. Move the ”Explanation” line above the ”Answer”\nline for Explain then Answer.\n3. Delete the ”Explanation” line for the Answer\nonly strategy.\n3.4\nEvaluation\nFor each question in the dataset, we prompt each\nmodel five times. We then take the majority answer\nas the model’s final answer.\n3.5\nHallucinations\nWe use LLM-as-a-judge (Li et al., 2025), with\nGemini-2.5-flash (Comanici, 2025) as a judge. This\nmodel is chosen due to its capabilities, with a reported\nhallucination rate of 1.3% on the Vectara hallucina-\ntion benchmark (Bao et al., 2024)1. The prompt is\navailable in Appendix 7. Note that the Vectara hallu-\ncination benchmark is not specific to the prosthodon-\ntics domain and thus this number may be an overesti-\nmation of the model’s capabilities.\n1As reported on the Vectara LLM Hallucination Leader-\nboard.\n"}, {"page": 4, "text": "In this work, we focus on three types of hallucina-\ntions:\n1. Intrinsic Hallucination: The explanation pro-\nvided by the model directly contradicts the refer-\nence explanation.\n2. Extrinsic Hallucination: The explanation pro-\nvided by the model contains unverifiable informa-\ntion with respect to the reference explanation.\n3. Logical Hallucination:\nThe explanation pro-\nvided by the model does not support its final an-\nswer.\nWe further refine the definition for Extrinsic Hal-\nlucination:\n1. Safe: There is no unverifiable information from\nthe model’s generated explanation.\n2. Benign: The unverifiable information generated\nby the model is superfluous and the core premise\ndoes not differ from the reference explanation.\n3. Harmful: The unverifiable information generated\nby the model has a core premise that differs from\nthe reference explanation.\nExamples illustrating the difference between classes\nof extrinsic hallucination are provided in Appendix\n??. In this work, we group extrinsic hallucinations\ninto two categories: harmful and non-harmful (i.e.,\nsafe or benign). We made this choice because our\nsetup limits the reference explanation to a single para-\ngraph. However, this simplification introduces a lim-\nitation: a model’s explanation may be factual yet un-\nverifiable within the relatively short reference. This\nresults in a higher occurrence of benign extrinsic\nhallucinations that would have been deemed safe if\nthe reference explanation were more complete. Re-\nfer to Figure 1’s LLM-as-a-Judge Data Field for an\noverview.\n3.6\nMetadata\nThis work chooses two types of metadata as the focus:\n1. Consistency: How often each model outputs its\nmost common answer across five attempts.\n2. Answer token log probability: A proxy for the\nmodel’s confidence when deciding which choice\nit should answer with (i.e., X in ”Answer: [X]”).\nA log probability of 0 denotes absolute confi-\ndence.\nFigure 2:\nDifferent prompting strategies have a non-\nsignificant effect on model accuracy.\n4\nEXPLORATORY ANALYSIS\n4.1\nModel Performance\nDespite previous works reporting Reasoning LLMs’\nhigher capabilities in domains such as math, cod-\ning, and other reasoning-heavy tasks, we found re-\nsults indicating that this ability does not help in a\nnon-reasoning task, supporting results from previous\nworks (Boizard et al., 2025).\n4.2\nConsistency\nWe find that GPT-4o is relatively more consistent\nacross the board when compared to OSS-120B. How-\never, differences in prompting methods do not signif-\nicantly affect consistency.\n4.3\nDisagreement\nDespite having similar accuracy, each prompting\nstrategy impacts what each model eventually re-\nsponds with.\nFigure 3:\nDifferent prompting strategies have a non-\nsignificant effect on model consistency.\n"}, {"page": 5, "text": "Figure 4: Despite similar performance, each pair of prompt-\ning strategies shows some degree of disagreement.\n4.4\nCorrelation\nThe correlation plot shows that consistency has a pos-\nitive correlation with the model’s correctness for both\nGPT-4o and OSS-120B. However, except in the case\nof Explain then Answer, answer token log probability\nhas near-zero correlation for OSS-120B. We note that\nOSS-120B is extremely confident in most cases, with\nan answer token log probability near zero (indicating\nabsolute confidence).\nWe define hallucination rate as the average oc-\ncurrence of each hallucination for a specific question\nwhen prompted five times. As previously mentioned,\nwe group safe and benign extrinsic hallucinations into\none group, treating them as if no extrinsic hallucina-\ntion occurred, with harmful extrinsic hallucination be-\ning the other case.\n5\nRESULT: PREDICTING\nCORRECTNESS\nThis section addresses the main research question:\n”Can we predict correctness based on available meta-\ndata?” To answer this question, we use leave-one-out\nFigure 5: Correlation of each metadata and hallucination\nrate with respect to each model’s correctness. We group safe\nand benign extrinsic hallucinations into one group (non-\nharmful extrinsic hallucination) and compare it to harmful\nextrinsic hallucination.\ncross validation (LOOCV)2 due to our limited num-\nber of data points. Table 1 reports the results for all\nexperimental setups and a visualizes the experiment\nflow can be found in the Appendix, Figure 8.\n5.1\nUsing Metadata\nWe initialize this experiment using only two meta-\ndata features: consistency and the log probability of\nthe answer token. The results are shown in Table 1,\nspecifically in rows 1, 2, 7, 12, 13, and 18. Although\nprevious analyses indicate that prompting strategy has\nno statistically significant effect on model accuracy,\nthe table shows that the Explain then Answer strat-\negy produces metadata that is more predictive of cor-\nrectness. For GPT-4o, Explain then Answer yields\na +5.12% improvement in accuracy over the base-\nline. For OSS-120B, while Explain then Answer pro-\nvides a +7.14% increase, other prompting strategies\nperform comparably, suggesting that prompting order\nhas a smaller impact on metadata predictiveness for\nthis model.\n5.2\nAdding Predicted Hallucination\nLabels\nObtaining labels for extrinsic and intrinsic hallucina-\ntions necessitates the availability of a golden refer-\nence, which is typically unattainable in practical sce-\nnarios. Consequently, we employ metadata as input\nto train two predictor models: (1) a classifier that\npredicts a binary label (i.e., whether a hallucination\noccurs) and (2) a regressor that estimates the hallu-\ncination rate (i.e., the average proportion of halluci-\nnated content across five attempts). Under the present\nconfiguration, the consistency and answer token log\nprobability are not predictive of hallucination occur-\nrence. The outcomes of these two models are pro-\nvided in Appendix ??, Table 2.\nAlthough Table 1 indicates that incorporating pre-\ndicted hallucination labels marginally improves the\ncorrectness predictor model, we attribute this effect\nto random noise. In most cases, the inclusion of hal-\nlucination labels results in either negligible improve-\nments (see rows 7–9 and 13–15) or no improvement\nat all (see rows 18–20). The apparent performance\ngains observed in rows 2–4 are likewise interpreted as\nartifacts of random fluctuations, compounded by the\nweak baseline performance reported in row 2.\n2https://scikit-learn.org/stable/modules/generated/\nsklearn.model selection.LeaveOneOut.html\n"}, {"page": 6, "text": "Table 1: Performance comparison of GPT-4o and OSS-120B across prompting strategies. Gray rows (Oracle) denote upper-\nbound potential, not comparable results. Value inside brackets indicates improvement compared to baseline (assuming that all\noutput made by an LLM is always correct). Under the baseline scenario, recall achieve a perfect score of 1. REG (regression)\nand CLS (classification) refers to the addition of predicted hallucination signals. Oracle refers to the addition of actual\nhallucination signals, where Rate denotes the average occurence of each hallucination and Maj denotes whether or not each\nhallucination occurs at least three times when the question is prompted five times.\n#\nModel\nPrompting\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n1\nGPT-4o\nAnswer Only\n78.57% (+4.08%)\n78.89% (+4.40%)\n97.26% (-2.74%)\n87.12% (+1.74%)\n0.573\n2\nGPT-4o\nAnswer then Explain\n74.49% (-2.04%)\n76.60% (+0.07%)\n96.00% (-4.00%)\n85.21% (-1.50%)\n0.484\n3\nGPT-4o\n+ REG\n77.55% (+1.02%)\n78.49% (+1.96%)\n97.33% (-2.67%)\n86.90% (+0.19%)\n0.763\n4\nGPT-4o\n+ CLS\n78.57% (+2.04%)\n79.35% (+2.82%)\n97.33% (-2.67%)\n87.43% (+0.72%)\n0.710\n5\nGPT-4o\n+ (Oracle) Rate\n85.71% (+9.18%)\n89.61% (+13.08%)\n92.00% (-8.00%)\n90.79% (+4.08%)\n0.897\n6\nGPT-4o\n+ (Oracle) Maj\n84.69% (+8.16%)\n87.50% (+10.97%)\n93.33% (-6.67%)\n90.32% (+3.61%)\n0.868\n7\nGPT-4o\nExplain then Answer\n79.59% (+5.10%)\n80.46% (+5.97%)\n95.89% (-4.11%)\n87.50% (+2.12%)\n0.822\n8\nGPT-4o\n+ REG\n80.61% (+6.12%)\n81.40% (+6.91%)\n95.89% (-4.11%)\n88.05% (+2.67%)\n0.815\n9\nGPT-4o\n+ CLS\n80.61% (+6.12%)\n82.14% (+7.65%)\n94.52% (-5.48%)\n87.90% (+2.52%)\n0.803\n10\nGPT-4o\n+ (Oracle) Rate\n78.57% (+4.08%)\n83.33% (+8.84%)\n89.04% (-10.96%)\n86.09% (+0.71%)\n0.889\n11\nGPT-4o\n+ (Oracle) Maj\n82.65% (+8.16%)\n85.90% (+11.41%)\n91.78% (-8.22%)\n88.74% (+3.36%)\n0.890\n12\nOSS-120B\nAnswer Only\n77.55% (+4.08%)\n81.25% (+7.78%)\n90.28% (-9.72%)\n85.53% (+0.82%)\n0.643\n13\nOSS-120B\nAnswer then Explain\n78.57% (+5.10%)\n80.72% (+7.25%)\n93.06% (-6.94%)\n86.45% (+1.74%)\n0.591\n14\nOSS-120B\n+ REG\n79.59% (+6.12%)\n80.95% (+7.48%)\n94.44% (-5.56%)\n87.18% (+2.47%)\n0.594\n15\nOSS-120B\n+ CLS\n79.59% (+6.12%)\n80.95% (+7.48%)\n94.44% (-5.56%)\n87.18% (+2.47%)\n0.613\n16\nOSS-120B\n+ (Oracle) Rate\n84.69% (+11.22%)\n89.04% (+15.57%)\n90.28% (-9.72%)\n89.66% (+4.95%)\n0.929\n17\nOSS-120B\n+ (Oracle) Maj\n89.80% (+16.33%)\n91.89% (+18.42%)\n94.44% (-5.56%)\n93.15% (+8.44%)\n0.939\n18\nOSS-120B\nExplain then Answer\n79.59% (+7.14%)\n83.12% (+10.67%)\n90.14% (-9.86%)\n86.49% (+2.47%)\n0.736\n19\nOSS-120B\n+ REG\n79.59% (+7.14%)\n83.12% (+10.67%)\n90.14% (-9.86%)\n86.49% (+2.47%)\n0.740\n20\nOSS-120B\n+ CLS\n79.59% (+7.14%)\n83.12% (+10.67%)\n90.14% (-9.86%)\n86.49% (+2.47%)\n0.733\n21\nOSS-120B\n+ (Oracle) Rate\n88.78% (+16.33%)\n92.86% (+20.41%)\n91.55% (-8.45%)\n92.20% (+8.18%)\n0.890\n22\nOSS-120B\n+ (Oracle) Maj\n89.80% (+17.35%)\n92.96% (+20.51%)\n92.96% (-7.04%)\n92.96% (+8.94%)\n0.894\n5.3\nOracle Setting\nThe oracle setting represents the upper-bound perfor-\nmance of the correctness predictor when actual hal-\nlucination labels are provided. As shown in the gray\nrows of Table 1, there remains substantial room for\nimprovement. In this idealized setup, GPT-4o with\nthe Answer then Explain strategy provides the highest\npredictability of correctness. For OSS-120B, while\nthe potential for improvement remains large, the up-\nper bounds for Answer then Explain and Explain then\nAnswer are identical.\nFigure 6 and 7 showcase the correctness predic-\ntor’s feature weights. For GPT-4o, the Answer then\nExplain strategy makes harmful extrinsic and intrinsic\nhallucinations the strongest predictors of correctness,\nwhereas under Explain then Answer, consistency be-\ncomes the most important feature.\nFor OSS-120B, both prompting strategies high-\nlight consistency and hallucination signals as predic-\ntive, though the answer token log probability is only\nuseful under Explain then Answer.\nThis observa-\ntion shows that prompting strategy has an existing\neffect on the predictiveness of correctness through\nmetadata, despite the same upper-bound performance.\nWe hypothesize that the choice of prompting strat-\negy for OSS-120B still affects the model’s internal\nmechanism, as indicated by the disagreement be-\ntween strategies, though we leave this question for fu-\nture work.\n6\nDISCUSSION\nThe use of generalist and reasoning LLMs is still\ntoo risky in high-stakes domains\nBoth GPT-4o\nand OSS-120B achieve only around 75% and 73%\naccuracy on the exam dataset, respectively. This is\nexcessively low for high-stakes domains that require\nabsolute precision. Our work shows that both consis-\ntency and answer token log probability are useful for\nflagging potentially erroneous responses, achieving a\nmaximum precision of 83.12%, which is still unreli-\nable for high-stakes applications.\nThe effect of prompting strategy is still unclear\nWhile we find that different prompting strategies have\nno effect on model performance for this specific task,\nthey still alter the predictive power of metadata fea-\ntures, which we attribute to differences in the model’s\ninternal mechanisms. This is especially surprising for\n"}, {"page": 7, "text": "Figure 6: Feature importance weights of the correctness\npredictor when provided with metadata and oracle hallu-\ncination majority signal. Cons. = consistency, Ext. H.\n= extrinsic hallucination, textbfInt. H. = intrinsic halluci-\nnation,Log. H. = logical hallucination, LogP = predicted\nanswer token log probability.\nOSS-120B, a reasoning model designed to innately\nreason. Specifically, we found that Explain then An-\nswer may result in different final answers compared\nto Answer then Explain. This is particularly intrigu-\ning since the task itself does not require reasoning and\nis primarily factual.\nHow different prompting strategies affect LLMs\nand reasoning LLMs is still understudied\nOur re-\nsults show that even on factual, non-reasoning tasks,\nprompting strategies can change which features are\nmost predictive of correctness, and can even lead to\ndifferent final answers for the same model.\nWhile\nGPT-4o shows more pronounced differences between\nAnswer then Explain and Explain then Answer, OSS-\n120B exhibits subtler changes in metadata predic-\ntiveness despite identical upper-bound performance.\nThese findings suggest that prompting can influence\ninternal decision processes in ways not captured by\naccuracy alone, highlighting the need for further re-\nsearch into how prompts interact with reasoning mod-\nels’ internal mechanisms across domains and tasks.\nFigure 7: Feature importance weights of the correctness\npredictor when provided with metadata and oracle hallu-\ncination rate signal, identical abbreviations as Figure 6.\nImplications for medical education\nThe exam\ndataset we use reflects real-world medical knowledge\nassessments, where accuracy is critical. Our results\nindicate that even strong LLMs like GPT-4o and OSS-\n120B achieve only 75–79% accuracy, corresponding\nroughly to a B– to B grade—insufficient for high-\nstakes decisions. While metadata such as consistency\nand answer log probability can help flag potentially\nincorrect responses, the maximum precision of 83%\nstill leaves substantial risk. Even under the oracle set-\nting, accuracy reaches only 89% (B+), highlighting\nthe gap between current LLM capabilities and safe\ndeployment in medical education contexts. This un-\nderscores the importance of cautious, supervised use\nof LLMs as educational aids rather than as unsuper-\nvised decision-makers.\n7\nLIMITATIONS AND FUTURE\nWORKS\nLimited Scope and Small Dataset\nThe primary\nlimtation of this work lies in the small score\n(Prosthodontics) and limited dataset size (n=98),\nwhich restrict the generalizability and statistical\npower of our results.\nThe limited scope of\nprosthodontics is chosen due to the authors field of\n"}, {"page": 8, "text": "expertise. Due to the limited dataset size, marginal\nperformance increases should be interpreted with cau-\ntion. Despite this, the key insight of our work comes\nfrom the oracle setting, which demonstrates that cor-\nrectness is predictable with the help of accurate hal-\nlucination detection. We urge future work to replicate\nthis work across other domains and explore more so-\nphisticated and less restrictive hallucination detection\nmethods (i.e., those not requiring a reference explana-\ntion).\nNo Specialized LLMs\nTo our knowledge, there are\nno LLMs specialized in Prosthodontics. Thus, this\nwork uses LLMs trained on broad domains, which\nmay limit domain-specific accuracy and clinical nu-\nance.\nClassification of extrinsic hallucination\nOur work\nclassifies extrinsic hallucination using a single refer-\nence explanation per question. While this simplifies\nthe task, it may misclassify valid alternative explana-\ntions as ”benign extrinsic hallucinations” when they\ndeviate from the reference. We mitigate this issue by\nstandardizing the annotation into a binary classifica-\ntion: harmful versus non-harmful (i.e., safe or benign)\nextrinsic hallucinations. This allows us to focus on\nmisleading and factually incorrect explanations.\nACKNOWLEDGEMENTS\nThis research was funded by the Ministry of Educa-\ntion, Culture, Research and Technology of the Repub-\nlic of Indonesia through the Indonesia-US Research\nCollaboration in Open Digital Technology program,\nGoogle Research Scholar Award, and Monash Uni-\nversity’s Action Lab. The findings and conclusions\npresented in this publication are those of the authors\nand do not necessarily reflect the views of the spon-\nsors.\nThis work utilizes Gemini-2.5-Pro to help proof-\nread the writing of the authors and provide sugges-\ntions and codes for the visualization shown in many\nsections of this work. Concretely, for each paragraph\nwe deemed hard to read, we prompt Gemini-2.5-Pro\nto focus on fixing any grammatical error and increase\nits readability, which is then re-reviewed by the au-\nthors. We also prompt the model to help create the\nvisualization for Figure 5, Figure 6, and Figure 7.\nREFERENCES\nAbd-alrazaq, A., AlSaad, R., Alhuwail, D., Ahmed, A.,\nHealy, P. M., Latifi, S., Aziz, S., Damseh, R., Al-\nabed Alrazak, S., and Sheikh, J. (2023). Large lan-\nguage models in medical education: Opportunities,\nchallenges, and future directions. JMIR Med Educ,\n9:e48291.\nBao, F., Li, M., Luo, R., and Mendelevitch, O. (2024).\nHHEM-2.1-Open.\nBoizard, N., Gisserot-Boukhlef, H., El-Haddad, K., Hude-\nlot, C., and Colombo, P. (2025).\nWhen does rea-\nsoning matter?\na controlled study of reasoning’s\ncontribution to model performance.\narXiv preprint\narxiv:2509.22193.\nComanici, G. e. a. (2025). Gemini 2.5: Pushing the frontier\nwith advanced reasoning, multimodality, long con-\ntext, and next generation agentic capabilities. arXiv\npreprint arxiv:2507.06261.\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J.,\nZhang, R., Xu, R., et al. (2025). Deepseek-r1: Incen-\ntivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arxiv:2501.12948.\nHuang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang,\nH., Chen, Q., Peng, W., Feng, X., Qin, B., and Liu, T.\n(2025). A survey on hallucination in large language\nmodels: Principles, taxonomy, challenges, and open\nquestions. ACM Transactions on Information Systems,\n43(2):1–55.\nHuang, S., Mamidanna, S., Jangam, S., Zhou, Y., and\nGilpin, L. H. (2023).\nCan large language models\nexplain themselves?\na study of llm-generated self-\nexplanations. arXiv preprint arxiv:2310.11207.\nKadavath, S., Conerly, T., Askell, A., Henighan, T., Drain,\nD., Perez, E., Schiefer, N., et al. (2022). Language\nmodels (mostly) know what they know. arXiv preprint\narxiv:2207.05221.\nKrishna, S., Ma, J., Slack, D., Ghandeharioun, A., Singh,\nS., and Lakkaraju, H. (2023). Post hoc explanations\nof language models can improve language models.\nIn Proceedings of the 37th International Conference\non Neural Information Processing Systems, NIPS ’23,\nRed Hook, NY, USA. Curran Associates Inc.\nLi, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan,\nZ., Bhattacharjee, A., et al. (2025). From generation\nto judgment: Opportunities and challenges of LLM-\nas-a-judge. In Christodoulopoulos, C., Chakraborty,\nT., Rose, C., and Peng, V., editors, Proceedings of\nthe 2025 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2757–2791, Suzhou,\nChina. Association for Computational Linguistics.\nManakul, P., Liusie, A., and Gales, M. (2023). SelfCheck-\nGPT: Zero-resource black-box hallucination detection\nfor generative large language models. In Bouamor,\nH., Pino, J., and Bali, K., editors, Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9004–9017, Singapore.\nAssociation for Computational Linguistics.\nOpenAI, Agarwal, S., Ahmad, L., Ai, J., Altman, S.,\nApplebaum, A., Arbus, E., et al. (2025).\ngpt-oss-\n"}, {"page": 9, "text": "120b & gpt-oss-20b model card.\narXiv preprint\narxiv:2508.10925.\nOpenAI, Hurst, A., Lerer, A., Goucher, A. P., Perelman, A.,\nRamesh, A., Clark, A., et al. (2024). Gpt-4o system\ncard. arXiv preprint arxiv:2410.21276.\nTurpin, M., Michael, J., Perez, E., and Bowman, S. R.\n(2023). Language models don’t always say what they\nthink:\nunfaithful explanations in chain-of-thought\nprompting. In Proceedings of the 37th International\nConference on Neural Information Processing Sys-\ntems, NIPS ’23, Red Hook, NY, USA. Curran Asso-\nciates Inc.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,\nXia, F., Chi, E. H., Le, Q. V., and Zhou, D. (2022).\nChain-of-thought prompting elicits reasoning in large\nlanguage models. In Proceedings of the 36th Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS ’22, Red Hook, NY, USA. Curran As-\nsociates Inc.\nYang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B.,\nYu, B., et al. (2025). Qwen3 technical report. arXiv\npreprint arxiv:2505.09388.\nZhao, C., Tan, Z., Ma, P., Li, D., Jiang, B., Wang, Y., Yang,\nY., and Liu, H. (2025). Is chain-of-thought reason-\ning of llms a mirage? a data distribution lens. arXiv\npreprint arxiv:2508.01191.\nAPPENDIX\nHallucination Predictor Model\nPerformance\nThis appendix shows the performance of the halluci-\nnation predictor model, which is available in Table 2.\nThe metrics used for the regressor are the following:\n1. R2: The coefficient of the determinator. A value\nof 0 indicates that the model explains none of the\nvariance in the target variable (the Target column)\n2. MAE: Mean absolute error. A value of 0 indicates\nperfect model performance.\n3. RMSE: Root mean squared error. A value of 0\nindicates perfect model performance.\nNote that both the classifier and the regressor perform\nclose to the baseline, suggesting that metadata offers\nlimited predictive power in our current setup. How-\never, this inefficacy is likely driven by severe class\nimbalance, which may be masking the signal.\nExperiment Workflow\nRefer to Figure 8 for the experiment workflow. The\nperformance of the hallucination predictor model can\nbe seen in Table 2.\nHallucination Extraction Prompt\nWe use the following prompt to extract the judgement\nfrom Gemini-2.5-flash.\nYou are a meticulous, expert evaluator in the field of\nprosthodontics. Your task is to perform a three−part\nevaluation of the ”Candidate Explanation” using the ”\nGround Truth Context” and the detailed ”Evaluation\nRubric” below.\n**Evaluation Rubric:**\n1. **Intrinsic Hallucination:** Does the Candidate\nExplanation directly CONTRADICT a fact in the\nReference Explanation?\n−”Yes” or ”No”.\n2. **Extrinsic Hallucination:** Does the Candidate\nExplanation FABRICATE new information not present\nin the Reference Explanation? Classify any\nfabrication based on its **harm to the explanation’s\ncore argument**:\n−**”Harmful”:** The fabricated information is an **\nessential detail** to support the final answer. It\nintroduces a new cause or reason that is\nessential to the logic, deviating from the core\npremise of the ground truth.\n−**”Benign”:** The fabricated information is a **\nsuperfluous detail**. It does not deviate from the\ncore premise of the ground truth.\n−**”None”:** The explanation contains no new\ninformation.\n3. **Logical Hallucination:** Does the reasoning in the\nCandidate Explanation logically support the\nCandidate’s Final Answer?\n−”Aligned” or ”Misaligned”.\n**Your Task:**\nProvide your evaluation ONLY in the following JSON format\n:\n{{\n”intrinsic hallucination”: ”Yes or No”,\n”extrinsic hallucination”: ”None, Benign, or Harmful”,\n”logical rating”: ”Aligned or Misaligned”,\n”reasoning”: ”An explanation for your evaluation,\nreferencing the rubric.”\n}}\n−−−\n**Ground Truth Context:**\n**Question:** {question text}\n**Reference Explanation:**\n{ground truth explanation}\n−−−\n**Candidate to Evaluate:**\n**Candidate’s Final Answer:** {prediction answer}\n**Candidate Explanation:**\n{prediction explanation}\n−−−\n**JSON Output:**\n"}, {"page": 10, "text": "Table 2: Performance of hallucination predictors using metadata. Left: classification of hallucination class. Right: regression\nof hallucination rate.\nClassifier\nRegressor\nModel\nPrompting\nTarget\nBaseline Acc\nAccuracy\nPrecision\nF1\nR2\nMAE\nRMSE\nGPT-4o\nA→E\nextrinsic\n0.81\n0.81\n0.00\n0.00\n-0.01\n0.282\n0.361\nGPT-4o\nA→E\nintrinsic\n0.70\n0.69\n0.00\n0.00\n-0.03\n0.342\n0.392\nGPT-4o\nA→E\nlogical\n0.98\n0.98\n0.00\n0.00\n-0.02\n0.051\n0.117\nGPT-4o\nE→A\nextrinsic\n0.83\n0.84\n0.60\n0.27\n0.07\n0.210\n0.304\nGPT-4o\nE→A\nintrinsic\n0.74\n0.72\n0.00\n0.00\n0.01\n0.299\n0.364\nGPT-4o\nE→A\nlogical\n0.90\n0.90\n0.00\n0.00\n-0.00\n0.178\n0.236\nOSS-120B\nA→E\nextrinsic\n0.78\n0.78\n0.00\n0.00\n0.05\n0.295\n0.367\nOSS-120B\nA→E\nintrinsic\n0.64\n0.65\n0.53\n0.32\n0.01\n0.342\n0.388\nOSS-120B\nA→E\nlogical\n0.98\n0.98\n0.00\n0.00\n0.05\n0.071\n0.115\nOSS-120B\nE→A\nextrinsic\n0.69\n0.77\n0.67\n0.55\n0.16\n0.274\n0.353\nOSS-120B\nE→A\nintrinsic\n0.54\n0.67\n0.72\n0.57\n0.02\n0.352\n0.399\nOSS-120B\nE→A\nlogical\n0.98\n0.98\n0.00\n0.00\n-0.05\n0.080\n0.124\nFigure 8: Experiment Workflow. Correctness Predictor is trained using differing Input Features dependant on the experiment\nsetup. The base setup utilizes only Consistency and Average Answer Token Logprob. The Oracle setup additionally uses\nGemini-2.5-flash’s LLM-as-a-Judge results to add three new features to the correctness predictor. The +REG and +CLS\nsetup trains a model to predict Gemini-2.5-flash’s LLM-as-a-Judge result and uses these predicted features to train the\ncorrectness predictor.\n"}]}