{"doc_id": "arxiv:2602.10345", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.10345.pdf", "meta": {"doc_id": "arxiv:2602.10345", "source": "arxiv", "arxiv_id": "2602.10345", "title": "Identifying Evidence-Based Nudges in Biomedical Literature with Large Language Models", "authors": ["Jaydeep Chauhan", "Mark Seidman", "Pezhman Raeisian Parvari", "Zhi Zheng", "Zina Ben-Miled", "Cristina Barboi", "Andrew Gonzalez", "Malaz Boustani"], "published": "2026-02-10T22:36:07Z", "updated": "2026-02-10T22:36:07Z", "summary": "We present a scalable, AI-powered system that identifies and extracts evidence-based behavioral nudges from unstructured biomedical literature. Nudges are subtle, non-coercive interventions that influence behavior without limiting choice, showing strong impact on health outcomes like medication adherence. However, identifying these interventions from PubMed's 8 million+ articles is a bottleneck. Our system uses a novel multi-stage pipeline: first, hybrid filtering (keywords, TF-IDF, cosine similarity, and a \"nudge-term bonus\") reduces the corpus to about 81,000 candidates. Second, we use OpenScholar (quantized LLaMA 3.1 8B) to classify papers and extract structured fields like nudge type and target behavior in a single pass, validated against a JSON schema.   We evaluated four configurations on a labeled test set (N=197). The best setup (Title/Abstract/Intro) achieved a 67.0% F1 score and 72.0% recall, ideal for discovery. A high-precision variant using self-consistency (7 randomized passes) achieved 100% precision with 12% recall, demonstrating a tunable trade-off for high-trust use cases. This system is being integrated into Agile Nudge+, a real-world platform, to ground LLM-generated interventions in peer-reviewed evidence. This work demonstrates interpretable, domain-specific retrieval pipelines for evidence synthesis and personalized healthcare.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.10345v1", "url_pdf": "https://arxiv.org/pdf/2602.10345.pdf", "meta_path": "data/raw/arxiv/meta/2602.10345.json", "sha256": "31ec12ed1849bdf11c8bfdeaed6c124a28c98fb4d31ea39c0adee8409d0cfda2", "status": "ok", "fetched_at": "2026-02-18T02:19:26.174550+00:00"}, "pages": [{"page": 1, "text": "Identifying Evidence-Based Nudges in Biomedical Literature with Large\nLanguage Models\nJaydeep Chauhan1, Mark Seidman1, Pezhman Raeisian Parvari2, Zhi Zheng3, Zina Ben-Miled4,\nCristina Barboi1,5, Andrew Gonzalez1,5, Malaz Boustani1,5,6,\n1 Center for Health Innovation and Implementation Science, Indiana University School of Medicine, Indianapolis, IN, USA\n2 Indiana University, Bloomington, IN, USA\n3 University of Notre Dame, Notre Dame, IN, USA\n4 Phillip M. Drayer Department of Electrical and Computer Engineering, Lamar University, TX, USA\n5 Regenstrief Institute, Inc., Indianapolis, IN, USA\n6 Sandra Eskanazi Center for Brain Care Innovation, Eskanazi Health, Indianapolis, IN, USA\n{jtchauha, mjseidma, praeisia, mboustan, cbarboi, andrewg}@iu.edu,\nzzheng3@nd.edu, zbenmiled@lamar.edu\nAbstract\nWe present a scalable, AI-powered system that identifies\nand extracts evidence-based behavioral nudges from the un-\nstructured biomedical literature to support positive behav-\nioral decision-making in healthcare. Nudges, which are sub-\ntle, noncoercive interventions that influence behavior without\nlimiting choice, have demonstrated a strong impact on im-\nproving health outcomes (e.g., medication adherence, vacci-\nnation uptake). However, identifying these interventions from\nlarge scientific corpora remains a bottleneck; PubMed alone\ncontains more than 8 million unstructured articles.\nOur system addresses this challenge via a novel multi-stage\npipeline. First, we apply hybrid filtering using keyword\nheuristics, Term Frequency - Inverse Document Frequency\n(TF-IDF), cosine similarity, and a domain-specific “nudge-\nterm bonus” reducing the corpus to ∼81,000 candidate arti-\ncles. In the second stage, we use OpenScholar, a quantized\nopen-source version of LLaMA 3.1 8B fine-tuned on scien-\ntific texts, to classify nudge-relevant papers and extract struc-\ntured fields (e.g., nudge type, cognitive bias, target behavior)\nin a single forward pass. The outputs are validated against\na constrained JSON schema to ensure consistency and inter-\npretability.\nWe evaluated four inference configurations on a manually\nlabeled test set (N=197). The best-performing set-up (Ti-\ntle/Abstract/Intro) achieved 67.0% F1 score and 72.0% re-\ncall, ideal for high-recall discovery. A high-precision vari-\nant using self-consistency (7 randomized passes with high-\ntemperature sampling) achieved 100% precision with reduced\nrecall (12%), demonstrating a tunable trade-off between the\nexploratory and high-trust use cases.\nThis system is currently being validated and integrated\ninto Agile Nudge+, a real-world nudge design platform.\nOnce deployed, it will ground LLM-generated behavioral\ninterventions in peer-reviewed evidence. This work demon-\nstrates a new class of interpretable, domain-specific retrieval\npipelines, with practical applications in cross-domain evi-\ndence synthesis, meta-analysis acceleration, and personalized\nhealthcare.\nIntroduction\nBehavioral nudges, which are subtle, non-coercive inter-\nventions that guide decision making without restricting\nchoice, have demonstrated a measurable impact in health-\ncare, from improving medication adherence to increasing\nvaccination uptake (Thaler and Sunstein 2008; Patel et al.\n2023; Boustani, Azar, and Solid 2020). During the COVID-\n19 pandemic, low-cost strategies such as default vaccina-\ntion appointments and risk-framing messages measurably\nincreased uptake at the population level (Banerjee et al.\n2024; Kasting et al. 2019).\nHowever, the evidence base for effective nudges is buried\nin the biomedical literature, dispersed across unstructured\ntext, and inconsistently labeled. As of July 2025, PubMed\ncontained approximately 8.1 million full-text articles, which\nmakes manual discovery prohibitively slow for researchers\nconducting systematic reviews or designing interventions.\nKeyword search is insufficient, retrieving irrelevant results\n(for example, ’nudging a neuron’) while missing valid\nnudges described in contextually different terms.\nTo address this challenge, we present a two-stage AI\npipeline that combines classical information retrieval with\nlarge-language model (LLM) reasoning to identify and ex-\ntract evidence-based behavioral nudges from biomedical lit-\nerature at scale. The system first narrows the search space\nusing a hybrid keyword, TF-IDF, and semantic similarity fil-\ntering. It then applies an LLM fine-tuned for scientific text\nto classify candidate articles using behavioral science inclu-\nsion criteria and to extract structured metadata (nudge type,\ntargeted cognitive bias, and intervention goal).\nOur key contributions are as follows:\n1. Scalable nudge discovery — A pipeline capable of pro-\ncessing millions of biomedical articles to surface high-\nquality behavioral interventions.\n2. LLM-based classification and structured extraction —\nIntegrating precision filtering with interpretable metadata\ncapture in a single pass.\n3. The system is being actively incorporated into Agile\nNudge+ (Shojaei et al. 2024), a real-world platform to\narXiv:2602.10345v1  [cs.LG]  10 Feb 2026\n"}, {"page": 2, "text": "design personalized behavioral interventions. By ground-\ning its recommendations in a vetted, structured evidence\nbase, the platform can improve reliability, transparency,\nand trust in AI-generated behavioral recommendations.\nTo our knowledge, this is the first system to combine\nlarge-scale biomedical filtering with behavioral science-\naware classification and extraction, creating an evidence\nbase suitable for direct integration into healthcare decision\nsupport.\nBackground and Related Work\nAgile Science\nAgile science is a rapidly evolving adaptive process for\nknowledge discovery and acquisition within the dynamic,\nconstantly changing real-world. The field integrates insights\nfrom behavioral economics, complexity science, and net-\nwork science to understand, predict, and steer the behav-\niors of both an individual human and a social organiza-\ntion. The practice enables scalable and effective human-\ncentered strategies, processes and tools, including nudges,\nto be implemented into routine care and subsequently dif-\nfused across various social networks (Mehta et al. 2022).\nBehavioral Nudges in Healthcare\nNudges are rooted in behavioral economics and cognitive\npsychology, influencing behavioral change by altering the\ndecision environment without limiting freedom of choice\n(Thaler and Sunstein 2008). In healthcare, they have been\nused to promote preventive screening, improve appointment\nattendance, and support chronic disease management, often\nwith minimal cost and high scalability. Their success has\ndriven inclusion in public health guidelines and implemen-\ntation science frameworks.\nLiterature Mining and Information Extraction\nwith LLMs\nRecent advances in LLMs such as GPT (Achiam et al. 2023),\nBERT (Devlin et al. 2018), and LLaMA (Grattafiori et al.\n2024) have transformed the literature mining, enabling doc-\nument classification, summarization, and information ex-\ntraction at scale. Biomedical NLP has also benefited from\ndomain-specific adaptations such as BioBERT (Lee et al.\n2020) and PubMedBERT (Gu et al. 2021), which improve\nnamed entity recognition and retrieval for scientific texts.\nTools such as Elicit and ASReview apply transformer mod-\nels to accelerate systematic review screening, but these ap-\nproaches typically focus on general textual relevance rather\nthan on behavioral science constructs central to nudge re-\nsearch, such as choice architecture elements (e.g., defaults,\nframing, salience, ordering effects) and underlying cognitive\nbiases (e.g., loss aversion, status quo bias, anchoring).\nLimitations of Existing Tools\nMining behavioral interventions from the biomedical liter-\nature is fundamentally an information retrieval challenge:\nthe goal is to surface a small, high-value subset of docu-\nments from a massive corpus, where relevance is defined by\ndomain-specific constructs rather than broad topical simi-\nlarity. In our case, these constructs include choice architec-\nture (e.g., altering the environment in which decisions are\nmade) and cognitive biases (e.g., loss aversion, present bias)\nthat underlie behavioral nudges. These concepts are often\ndescribed implicitly, scattered across different sections of a\npaper, and not captured by standard indexing methods such\nas keyword matching or MeSH terms.\nTraditional tools like PubMed, Cochrane Review Man-\nager, and Publish or Perish excel at retrieving documents\nbased on keyword or citation metrics, but are over- and\nunder-inclusive when the target concepts are abstract, in-\nterdisciplinary, or expressed without consistent terminology.\nPrior biomedical NLP pipelines (e.g., BERT-based screen-\ning models) improve general relevance ranking, but rarely\nincorporate the semantic reasoning and contextual inference\nneeded to detect nudges embedded in complex behavioral or\nexperimental designs.\nOur approach addresses this gap by combining large-scale\ncandidate filtering with domain-aware semantic classifica-\ntion and structured metadata extraction, bridging established\nIR techniques with behavioral science–specific reasoning.\nThis positions the system as both an applied solution for be-\nhavioral health and a transferable framework for other spe-\ncialized domains where fine-grained, concept-level retrieval\nis essential.\nSystem Overview\nOur system automates the discovery and structuring of\nevidence-based behavioral nudges from the large-scale\nbiomedical literature, enabling rapid evidence synthesis for\nhealthcare decision making and behavioral change design.\nIt follows a two-stage architecture designed for scalability,\nprecision, and downstream integration.\nStage 1 — Hybrid Filtering:\nWe first narrowed the search space from millions of PubMed\narticles to a manageable set of candidates using a hybrid re-\ntrieval strategy. This combines:\n• Broad keyword matching to maximize recall.\n• TF-IDF and cosine similarity to capture semantic rele-\nvance beyond exact keyword matches.\n• A domain-specific “nudge-term bonus” to upweight doc-\numents more likely to describe behavioral interventions.\nThis stage reduces the corpus size by ∼99% while maintain-\ning high coverage of potential nudge-relevant studies.\nStage 2 — LLM-Based Classification and\nStructured Extraction:\nThe filtered set is passed to an open-source LLaMA 3.1 8B\nmodel fine-tuned on the scientific text. In a single forward\npass, the model:\n1. Classifies whether the article describes a valid behav-\nioral nudge based on strict inclusion and exclusion cri-\nteria rooted in agile science.\n2. Extracts structured metadata, including:\n"}, {"page": 3, "text": "• Nudge type (e.g., default, framing, reminder)\n• Targeted cognitive bias (e.g., loss aversion, social\nnorms)\nThe output is validated against a constrained JSON\nschema, ensuring interpretable and machine-readable results\nsuitable for direct integration into applications.\nFrom an IR perspective, this stage acts as a seman-\ntic reranker, operating on a high-recall candidate set from\nStage 1 and applying fine-grained, domain-specific rele-\nvance modeling that goes beyond surface lexical similar-\nity. The result is a high-precision, semantically curated\nsubset (∼12,000 documents) ready for evidence synthe-\nsis, database construction, or retrieval-augmented genera-\ntion (RAG) applications.\nMethods\nHybrid Filtering Stage (TF-IDF + Keyword Bonus)\nOur first stage filter reduces the 8 million articles in PubMed\nto a high-recall and manageable subset suitable for compu-\ntationally expensive downstream classification. This stage\nwas implemented and executed on the Jetstream2 academic\ncloud platform, which provided the scalability needed to\nprocess the entire corpus under resource constraints.\nData Source and Initial Keyword Expansion\nWe started by downloading the PubMed Open Access subset\nto Jetstream2 storage, containing titles, abstracts, introduc-\ntions, and metadata in structured JSON format. The initial\nretrieval step used a curated set of keywords drawn from\nthe behavioral science literature (Thaler and Sunstein 2008;\nMehta et al. 2023), covering only some common nudge re-\nlated keywords (e.g. agile science, nudge theory, behavioral\ninterventions). This step retrieved approximately 440,000\narticles (∼5.5% of PubMed), prioritizing the recall over pre-\ncision.\nTF-IDF Vectorization and Semantic Matching\nWe computed TF-IDF representations using unigrams, bi-\ngrams, and trigrams (ngram range=(1,3)) while exclud-\ning extremely rare terms (min df=2) and overly common\nterms (max df=0.85). A domain-specific reference vector\nwas constructed from the expanded set of keywords list cov-\nering:\n• Core nudge terms (e.g., nudge, choice architecture, loss\naversion),\n• Intervention terms (e.g., randomized, controlled trial, im-\npact),\n• Domain-specific behavioral concepts (e.g., reminders,\nsocial proof, present bias).\nThe complete keyword lists are provided in Appendix A to\nensure reproducibility.\nCosine similarity was computed between each article vec-\ntor and the reference vector using sparse matrix operations,\nenabling millions of comparisons to be efficiently executed\nwithin the Jetstream2 environment.\nKeyword Bonus Heuristic\nAlthough TF-IDF similarity captures broad lexical overlap,\nit can dilute the impact of rare but highly discriminative do-\nmain terms (e.g., ’choice architecture’, ’default option’, ’loss\naversion’) that are crucial for identifying behavioral nudges.\nTo address this, we introduce a ’nudge-term bonus’ that ex-\nplicitly increases the retrieval score for documents whose ti-\ntle or abstract contains high-value behavioral science terms.\nThe bonus is computed as:\nbonus(d) = min\n\u0000|Td| × 0.1, 0.3\n\u0001\n(1)\nwhere Td is the set of kaywords matching in the document\nd.\nThe final hybrid score is then:\nhybrid score(d) = cos sim(d, q) + bonus(d)\n(2)\nwhere cos sim(d, q) is the cosine similarity between the\ndocument vector d and the reference vector q.\nScaling factor 0.1 and cap 0.3 were empirically deter-\nmined by iterative trial and error on a 100 document de-\nvelopment set sampled from PubMed. If the bonus was set\ntoo high, the system disproportionately favored documents\nwith explicit nudge-related terminology, leading to precision\ngains but substantial recall loss (e.g. missing articles that de-\nscribed nudges implicitly). In contrast, setting the bonus too\nlow caused high recall but poor ranking quality, allowing\ngeneric biomedical articles with minimal nudge relevance to\ndominate the top results.\nScalability and Filtering Outcome\nThis stage was implemented for streaming batch process-\ning with checkpointing, enabling the entire PubMed corpus\nto be processed in parallelizable chunks without exceeding\nnode memory limits. Using a threshold of 0.12 on the hybrid\nscore reduced the candidate pool to approximately 81,000\narticles (∼1% of PubMed), a 98% corpus size reduction.\nThis ensured that the second stage LLM classification could\nrun efficiently while maintaining a broad coverage of poten-\ntial nudge-related literature.\nLLM-Based Classification and Extraction\nModel Setup and Deployment\nThe second stage of our pipeline applies a large language\nmodel to the reduced candidate set from Stage 1. We use\nOpenScholar (Asai et al. 2024), an open-source LLaMA 3.1\n8B parameter model fine-tuned for scientific literature un-\nderstanding. The model is deployed on a Jetstream2 GPU\nnode (16 vCPUs, 60 GB RAM, 20 GB A100 GPU), using\nAWQ (Lin et al. 2024) INT4 quantization to reduce the foot-\nprint of GPU memory without significant degradation in ac-\ncuracy.\nPrompt and Input Structure\nFor each retained article,\nwe pass the title, abstract, and introduction into a structured,\ninstruction-following prompt directing the model to:\n1. Classify whether the article describes a behavioral nudge\nintervention.\n2. If relevant, extract the following structured fields:\n"}, {"page": 4, "text": "• Nudge type(s) (e.g., default, framing, reminders)\n• Targeted cognitive bias(es) (e.g., loss aversion, an-\nchoring)\n• Problem behavior addressed\n• Target behavior influenced\n• Reasoning trace for interpretability\nThe complete prompt template, including both task instruc-\ntions and few-shot examples, is provided in Appendix B for\ntransparency and reproducibility. The prompt explicitly in-\nstructs the model to return its output in a strict ’JSON’ for-\nmat. The outputs are validated using Python’s ’json’ module,\nwith up to two re-attempts for malformed generations.\nIllustrative examples of model outputs for both nudge-\nrelated and non-nudge-related articles are included in Ap-\npendix C.\nInclusion and Exclusion Criteria\nAn article is labeled\n’positive’ if it describes a non-coercive, choice-architectural\nintervention that alters the digital, social, or physical envi-\nronment to influence individual behavior without restricting\nchoice and without relying on medical, clinical, or policy-\nlevel mechanisms. Qualifying interventions target behaviors\nsuch as healthy eating, smoking reduction, physical activ-\nity, or mental health improvement, and operate through au-\ntomatic cognitive processes (e.g., heuristics, biases) rather\nthan only informational or educational means. Appendix B\ncontains the full prompt along with detailed inclusion and\nexclusion criteria. ensure reproducibility.\nFew-shot examples embedded in the classification prompt\nillustrate these boundaries, helping to distinguish valid be-\nhavioral nudges from general health education campaigns,\nmedical treatments, or systemic policy changes.\nSelf-Consistency Voting\nTo improve the reliability of\nthe method, we also evaluated the self-consistency method\n(Wang et al. 2023; Chen et al. 2024), in which each docu-\nment is evaluated seven times with high-temperature sam-\npling (T = 0.8). The final decision is obtained by major-\nity vote. This approach reduces the influence of stochastic\nmodel outputs and promotes high-confidence predictions,\nalthough it may reduce recall in exchange for greater pre-\ncision.\nLLM-as-a-Judge Verification\nIn selected configurations,\nwe introduce an LLM-as-a-Judge step (Li et al. 2024), where\nthe model re-assesses its own classification in a secondary\nprompt that explicitly asks whether the initial output meets\nthe defined inclusion criteria. This meta-evaluation is partic-\nularly effective for flagging ambiguous or borderline cases,\nimproving trust and interpretability in the final classification\nresults. The full judge verification prompt is provided in Ap-\npendix B.\nExperimental Setup\nDataset and Preprocessing\nWe used the PubMed database (∼8M articles) as our pri-\nmary corpus. The hybrid filtering stage, as described in the\nprevious section, reduced this to ∼81,000 candidate doc-\numents (∼1% of PubMed) while preserving broad topical\ncoverage. The title, abstract, and introduction text were ex-\ntracted for each article; full-text parsing was enabled for se-\nlect runs.\nLLM Processing\nThe LLaMA 3.1 8B OpenScholar model was applied to all\n81,000 documents using a continuous batching. A single for-\nward pass jointly:\n1. Classified articles as nudge-relevant or not.\n2. Extracted structured fields: nudge type, cognitive bias,\nproblem behavior, and target behavior.\nThis yielded ∼12,000 high-confidence positives for further\nevaluation.\nEvaluation Set\nA gold standard set of 197 articles was randomly sampled\nfrom the 12,000 positives and manually labeled by a domain\nexpert, blinded to model output. Labels followed the same\ninclusion criteria as the classification prompt: non-coercive\nintervention, environmental/choice-architecture design, and\nexclusion of purely medical or policy mechanisms. The set\ncontained 86 positives and 111 negatives.\nModel Configurations\nWe tested four configurations with LLM as a judge verifica-\ntion prompt:\n• Title+Abstract+Intro: LLaMA 3.1 8B, T = 0.1, single\npass.\n• Full Text: Same model with complete article input.\n• Self-Consistency (k = 7): Seven runs at T = 0.8; label\nby majority vote:\nˆy = arg max\nc\nk\nX\ni=1\n1[yi = c]\n• Gemini 2.5 Pro: API-based, T = 0.1, full-text classifi-\ncation.\nMetrics\nWe report standard classification metrics—Precision, Re-\ncall, F1 score, and Accuracy, to evaluate the performance of\nour filtering approach. These metrics capture the trade-off\nbetween recall and precision, which is particularly impor-\ntant when distinguishing between exploratory analysis and\nhigh-trust applications.\nResults\nClassification Performance\nWe tested four model configurations on a manually labeled\ntest set of 197 articles, with the results shown in Table 1.\nThe configuration using only the title, abstract, and introduc-\ntion achieved the highest recall (72%) and F1 score (67%),\nmaking it the most effective for applications requiring com-\nprehensive coverage, such as literature review phases or ex-\nploratory research. Its relatively high accuracy (69%) and\n"}, {"page": 5, "text": "Method\nPrecision Recall F1 Score Accuracy\nLLaMA 3.1 8B (Ti-\ntle, Abstract, Intro)\n0.63\n0.72\n0.67\n0.69\nLLaMA 3.1 8B (Full\nDocument)\n0.72\n0.51\n0.60\n0.70\nLLaMA\n3.1\n8B\n(Self-Consistency\nx7)\n1.00\n0.12\n0.21\n0.61\nGemini 2.5 Pro (Full\nDocument)\n0.61\n0.65\n0.63\n0.66\nTable 1: Performance of classification models on labeled test\nset (N = 197).\nbalanced precision (63%) indicate strong general perfor-\nmance in surfacing a broad range of potentially relevant\nnudge interventions.\nBy contrast, the self-consistency configuration, in which\neach document was classified seven times and the final la-\nbel determined by majority vote, delivered perfect precision\n(100%) but at the cost of severely reduced recall (12%). This\nmakes it well-suited for generating gold-standard corpora\nor other high-trust datasets where false positives must be\navoided, but impractical for large-scale discovery. The full-\ndocument configuration of LLaMA 3.1 8B improved preci-\nsion to 72% compared to the truncated input version, but\nrecall dropped to 51%, suggesting that additional context\nsometimes introduces noise or distracts the model from key\nclassification cues. The Gemini 2.5 Pro configuration, which\noperates on full documents, achieved balanced performance\nacross all metrics, making it a viable alternative in settings\nwhere open-source deployment is not feasible.\nError Analysis\nAnalysis of false positives revealed that models occasionally\nhallucinated the presence of behavioral interventions based\non loosely related descriptions, particularly when interven-\ntions were vaguely defined or mentioned primarily in the\ndiscussion section. Systematic reviews were another com-\nmon source of error; despite explicit instructions to include\nonly empirical evaluations of nudge interventions, the struc-\ntured and concept-rich format of reviews often led to mis-\nclassification.\nFalse negatives most often resulted from non-standard ter-\nminology or unconventional framing of nudges, which pre-\nvented the model from recognizing legitimate interventions.\nSome cases involved interventions embedded within broader\nclinical or population-level programs, making it difficult for\nthe model to isolate and classify the behavioral component.\nStructured information extraction generally succeeded for\nprimary fields such as nudge type and cognitive bias, but\nconsistency decreased for complex studies involving multi-\nple nudges or multiple experimental arms.\nWe also observed that the LLM-as-a-judge mechanism,\nused to verify classifications, was highly sensitive to prompt\nwording and occasionally biased toward over-conservative\ndecisions. The self-consistency approach, while effective in\neliminating false positives, came with high computational\ncost and low throughput, making it unsuitable for large-scale\napplication. In some cases, both verification strategies ap-\npeared to produce post hoc justifications for incorrect pre-\ndictions rather than genuine reasoning traces, a phenomenon\nof unfaithfulness observed in prior work (Turpin et al. 2024).\nFinally, as noted in prior studies (Liu et al. 2023; Kand-\npal et al. 2022), providing full-text input can sometimes re-\nduce model performance compared to truncated input, likely\nbecause longer documents contain distracting or redundant\ncontent that diminishes the model’s focus on relevant evi-\ndence.\nDetailed examples of both false positive and false negative\ncases, illustrating the types of errors described above, are\nprovided in Appendix D.\nDiscussion\nApplied Impact for Evidence-Based Healthcare\nThe results demonstrate that large language models, when\ncombined with a scalable two-stage filtering pipeline, can ef-\nficiently identify behavioral nudge interventions within mas-\nsive biomedical corpora. Our system reduced the PubMed\ndatabase of more than eight million articles to approximately\n12,000 high-relevance studies, offering a valuable resource\nfor healthcare strategy design, rapid evidence synthesis, and\nintegration into decision-support tools. The curated corpus\nis being incorporated into Agile Nudge+. This integration\nwill enable retrieval-augmented generation, grounding AI-\ngenerated recommendations in verified literature to enhance\nboth transparency and trustworthiness.\nLimitations and Risks\nDespite these benefits, there are limitations. Large language\nmodels are prone to both false positives and false nega-\ntives, especially when confronted with atypical terminology\nor ambiguous descriptions of interventions. Overly strict in-\nclusion criteria, as seen in the self-consistency and judge-\nmode configurations, can limit recall and exclude borderline\nbut relevant studies. Furthermore, full-text processing can\ndilute model performance, suggesting that targeted context\nwindows may be more effective for classification. Computa-\ntional costs, particularly for high-precision modes, also limit\nscalability in resource-constrained environments.\nEthical risks must be considered when automating the dis-\ncovery of behavioral interventions. Misclassifications could\npropagate misleading evidence into downstream systems,\nand reliance on automated classification without expert over-\nsight risks bias amplification.\nEthical Considerations\nAutomating the identification and recommendation of be-\nhavioral interventions introduces ethical concerns. There is\na risk of amplifying the biases present in the training data,\nmisrepresenting interventions, or overrelying on model out-\nputs without sufficient expert oversight. To mitigate these\nconcerns, LLM-generated output will be subjected to down-\nstream validation layers, including human-in-the-loop re-\nview, transparency in traceable reasoning, and strict adher-\nence to inclusion criteria rooted in behavioral science.\n"}, {"page": 6, "text": "Ultimately, this system represents a step towards respon-\nsible and transparent AI-assisted behavioral intervention de-\nsign, offering an important tool for evidence-based health-\ncare and digital policy development.\nConclusion\nWe presented a scalable, fully automated pipeline for iden-\ntifying and classifying evidence-based behavioral nudges\nfrom the biomedical literature. The system integrates a\ntwo-phase filtering approach – combining TF-IDF and co-\nsine similarity with LLM-based classification — to reduce\nthe PubMed corpus of more than eight million articles to\napproximately 12,000 highly relevant studies. Evaluation\nacross multiple model configurations revealed clear preci-\nsion–recall trade-offs, offering flexible modes for different\nuse cases. The LLaMA 3.1 8B model using only titles, ab-\nstracts, and introductions achieved the highest recall and\nF1 score, making it optimal for broad discovery, while the\nself-consistency mode achieved perfect precision for high-\ntrust applications. This corpus will be integrated into Agile\nNudge+ to enhance the reliability and transparency of AI-\ngenerated nudge recommendations, forming the basis for\na Retrieval-Augmented Generation (RAG) framework that\nlinks user-specified behaviors with verified, structured evi-\ndence.\nReferences\nAchiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;\nAleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;\nAnadkat, S.; and et al. 2023. GPT-4 Technical Report. arXiv\npreprint arXiv:2303.08774.\nAsai, A.; He, J.; Shao, R.; Shi, W.; Singh, A.; Chang,\nJ. C.; Lo, K.; Soldaini, L.; Feldman, S.; D’arcy, M.;\net al. 2024.\nOpenScholar: Synthesizing Scientific Lit-\nerature with Retrieval-Augmented LMs.\narXiv preprint\narXiv:2411.14199.\nBanerjee, S.; John, P.; Nyhan, B.; Hunter, A.; Koenig, R.;\nLee-Whiting, B.; Loewen, P. J.; McAndrews, J.; and Savani,\nM. 2024. Thinking about Default Enrollment Lowers Vacci-\nnation Intentions and Public Support in G7 Countries. PNAS\nNexus, page093.\nBoustani, M.; Azar, J.; and Solid, C. 2020. Agile Implemen-\ntation: A Model for Implementing Evidence-Based Health-\ncare Solutions into Real-world Practice to Achieve Sustain-\nable Change. Morgan James.\nChen, X.; Aksitov, R.; Alon, U.; Ren, J.; Xiao, K.; Yin, P.;\nPrakash, S.; Sutton, C.; Wang, X.; and Zhou, D. 2024. Uni-\nversal self-consistency for large language models. In ICML\n2024 Workshop on In-Context Learning.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. arXiv preprint arXiv:1810.04805.\nGrattafiori, A.; Dubey, A.; Jauhri, A.; Pandey, A.; Kadian,\nA.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.;\nVaughan, A.; et al. 2024. The LLaMA 3 Herd of Models.\narXiv preprint arXiv:2407.21783.\nGu, Y.; Tinn, R.; Cheng, H.; Lucas, M.; Usuyama, N.; Liu,\nX.; Naumann, T.; Gao, J.; and Poon, H. 2021.\nDomain-\nspecific language model pretraining for biomedical natural\nlanguage processing. ACM Transactions on Computing for\nHealthcare (HEALTH), 3(1): 1–23.\nKandpal, N.; Deng, H.; Roberts, A.; Wallace, E.; and Raffel,\nC. 2022. Large language models struggle to learn long-tail\nknowledge. arXiv preprint arXiv:2211.08411.\nKasting, M. L.; Head, K. J.; Cox, D.; Cox, A. D.; and Zimet,\nG. D. 2019.\nThe effects of message framing and health-\ncare provider recommendation on adult hepatitis B vacci-\nnation: a randomized controlled trial. Preventive Medicine,\n127: 105798.\nLee, J.; Yoon, W.; Kim, S.; Kim, D.; So, C. H.; and Kang, J.\n2020. BioBERT: a pre-trained biomedical language repre-\nsentation model for biomedical text mining. Bioinformatics,\n36(4): 1234–1240.\nLi, H.; et al. 2024.\nLLMs-as-Judges: A Comprehensive\nSurvey on LLM-based Evaluation Methods. arXiv preprint\narXiv:2412.05579.\nLin, J.; Tang, J.; Tang, H.; Yang, S.; Chen, W.-M.; Wang, W.-\nC.; Xiao, G.; Dang, X.; Gan, C.; and Han, S. 2024. AWQ:\nActivation-aware weight quantization for on-device LLM\ncompression and acceleration. In Proceedings of Machine\nLearning and Systems, volume 6, 87–100.\nLiu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua,\nM.; Petroni, F.; and Liang, P. 2023.\nLost in the middle:\nHow language models use long contexts.\narXiv preprint\narXiv:2307.03172.\nMehta, J.; Aalsma, M. C.; O’brien, A.; Boyer, T. J.; Ahmed,\nR. A.; Summanwar, D.; and Boustani, M. 2022. Becoming\nan Agile Change Conductor. Front Publick Health.\nMehta, J.; Williams, C.; Holden, R. J.; Taylor, B.; Fowler,\nN. R.; and Boustani, M. 2023. The methodology of the Agile\nnudge university. Frontiers in Health Services, 3: 1212787.\nPatel, M. S.; Milkman, K. L.; Gandhi, L.; Graci, H. N.;\nGromet, D.; Ho, H.; Kay, S. J.; Lee, T. W.; Rothschild, J.;\nAkinola, M.; et al. 2023. A randomized trial of behavioral\nnudges delivered through text messages to increase influenza\nvaccination among patients with an upcoming primary care\nvisit. American Journal of Health Promotion, 37: 324–332.\nShojaei, F.; Shojaei, F.; Desai, A. P.; Long, E.; Mehta, J.;\nFowler, N. R.; et al. 2024. The feasibility of AgileNudge+\nsoftware to facilitate positive behavioral change: a mixed\nmethods design. JMIR Formative Research.\nThaler, R. H.; and Sunstein, C. R. 2008. Nudge: Improving\nDecisions About Health, Wealth, and Happiness. Penguin\nBooks.\nTurpin, M.; Michael, J.; Perez, E.; and Bowman, S. 2024.\nLanguage models don’t always say what they think: unfaith-\nful explanations in chain-of-thought prompting. In Advances\nin Neural Information Processing Systems, volume 36.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q. V.; Chi, E. H.;\nNarang, S.; Chowdhery, A.; and Zhou, D. 2023.\nSelf-\nconsistency improves chain of thought reasoning in lan-\nguage models. In Proceedings of the 11th International Con-\nference on Learning Representations (ICLR).\n"}, {"page": 7, "text": "Appendix A: TF-IDF Keyword Lists\nBelow are the complete keyword sets used for the TF-IDF\nfiltering stage.\nNudge Keywords\n1\nnudge, nudge theory, behavioral\nintervention, choice architecture,\n2\ndecision making, behavioral economics,\nbehavior change,\n3\nbehavioral insights, default option, opt\nout, opt in,\n4\nsocial norm, anchor, anchoring, framing\neffect,\n5\nchoice design, behavioral science,\ndecision architecture,\n6\ncognitive bias, heuristic, behavioral\ndesign, loss aversion,\n7\nincentive, priming, salience, decision\nfatigue, status quo bias,\n8\nbehavioral insight, behavioral science,\nchoice design, choice framing\nIntervention-Related Keywords\n1\nrandomized, controlled trial,\nintervention, experiment,\n2\nstudy design, implementation, evaluation\n, outcome,\n3\neffectiveness, efficacy, impact, effect\nDomain-Specific Keywords\n1\nincentives, message framing, reminders,\nsalience,\n2\nsimplification, defaults, commitment\ndevices, priming,\n3\nego, messenger effect, social proof,\nloss aversion,\n4\npublic policy, health behavior,\nfinancial decision,\n5\nenvironmental choice, pro-social\nbehavior, persuasive design,\n6\nchoice overload, scarcity, present bias,\nfeedback,\n7\ntransparency, disclosure, evidence-based\npolicy\nAppendix B: LLM Prompts\nMain Prompt\n1\nmain_prompt: str = (\n2\n\"\"\"\n3\nYou are an expert in behavioral science\nand AI-driven text analysis. Your\ntask is to analyze research papers\nand extract structured information\nabout **evidenced-based nudge\ninterventions**.\n4\n5\n### **Step 1: Classify the Paper**\n6\nDetermine if the paper describes an **\nevidence-based nudge intervention**\nbased on the following criteria:\n7\n8\n**Inclusion Criteria (ALL must be met\nfor a paper to be classified as \"\nNudge-Related\"):\\n**:\n9\n1. The study describes an **intervention\n** that modifies **choice\narchitecture** rather than simply\ninforming or educating.\n10\n2. The intervention **targets individual\nbehavior change** in areas such as:\n11\n- Improving sleep schedules\n12\n- Reducing smoking, alcohol\nconsumption\n13\n- Combating obesity, unhealthy eating\n14\n- Addressing mental health challenges\n15\n- Encouraging healthy lifestyle\nchanges\n16\n3. The intervention is **non-coercive,\nvoluntary, and does not involve\nfinancial rewards or punishments**.\n17\n4. The nudge mechanism aligns with at\nleast one recognized nudge types (see\nStep 2).\n18\n19\n**Exclusion Criteria (If ANY are met,\nclassify as \"Not Nudge-Related\")**:\n20\n1. The study describes **a medical\ntreatment, drug, or clinical\nprocedure**.\n21\n2. The intervention relies on **\nfinancial incentives, taxes,\npenalties, or subsidies**.\n22\n3. The study focuses on **policy, law,\nor government-level interventions**\nrather than individual choice\nmodification.\n23\n4. The study is a **review article or\ntheoretical discussion** without an\nactual intervention.\n24\n5. The study does **not describe a\nspecific intervention applied to\nparticipants**.\n25\n26\n---\n27\n28\n### **Step 2: Extract Information**\n29\nIf the paper is **nudge-related**,\nextract the following structured\ndetails:\n30\n31\n1. **Nudge Types** -> Select one or more\nrelevant nudge mechanisms from this\nlist:\n32\n- **Messenger** -> Influences based\non who delivers the information.\n33\n- **Incentives** -> Leverages loss\naversion but without monetary\nrewards/punishments.\n34\n- **Norms** -> Uses social proof (e.g\n., \"90% of people do X\").\n35\n- **Default** -> Pre-selects an\noption to encourage a behavior.\n36\n- **Salience** -> Highlights\ninformation to make it more\nnoticeable.\n37\n- **Priming** -> Uses subconscious\n"}, {"page": 8, "text": "cues (e.g., healthy food placement\n).\n38\n- **Affect** -> Evokes emotional\nresponses to drive action.\n39\n- **Commitment** -> Encourages self-\nimposed goals.\n40\n- **Ego** -> Aligns behavior with\nself-identity.\n41\n42\n2. **Associated Cognitive Biases** ->\nIdentify biases influencing the\nintervention (e.g., loss aversion,\nstatus quo bias, anchoring).\n43\n44\n3. **Problem Behavior** -> The undesired\nbehavior targeted by the\nintervention.\n45\n46\n4. **Target Behavior** -> The intended\nbehavioral outcome.\n47\n48\n---\n49\n50\n### **Step 3: Final Decision:\\n**\n51\n- If Step 2 fails to extract relevant\ndetails, the paper should be\nclassified as not-nudge related and\nexcluded\n52\n- Otherwise, confirm it as **Nudge-\nRelated**\n53\n54\n### **JSON Output Format**\n55\nYour response must be in **valid JSON\nformat** with the following structure\n:\n56\n57\n‘‘‘json\n58\n{{\n59\n\"is_nudge_related\": true/false,\n60\n\"nudge_types\": [],\n61\n\"cognitive_biases\": [],\n62\n\"problem_behavior\": \"\",\n63\n\"target_behavior\": \"\",\n64\n\"Reason\": \"\"\n65\n}}\n66\n‘‘‘\n67\nDo not generate unnecessary explanations\nbeyond this structured output.\n68\n69\n### **Input:**\n70\n**Title**: {title}\n71\n**Abstract**: {abstract}\n72\n**Introduction**: {intro}\n73\n\"\"\".strip()\n74\n)\nVerification Prompt\n1\nverify_nudge_cls_prompt: str = (\n2\n\"\"\"\n3\nYou are an expert in behavioral science\nand AI-assisted systematic reviews.\nYour task is to verify the following\nclaim about a research paper.\n4\n5\nCarefully analyze the paper’s **title**,\n**abstract**, and **full text**, and\nassess whether the claim is **\nsupported by the content**. If the\nclaim is accurate, respond with **\"\nYes\"**. If the claim is not supported\nor contradicted by the text, respond\nwith **\"No\"**.\n6\n7\n### The claim:\n8\nThis paper **{claim}**.\n9\n10\n### Use the following criteria to\nevaluate the claim:\n11\n12\n#### Inclusion Criteria (ALL must be met\nto classify as \"Nudge-Related\"):\n13\n1. The study describes an **intervention\n** that modifies **choice\narchitecture** altering the digital,\nsocial, or physical environment to\nfacilitate behavior change **without\nrestricting choice**.\n14\n2. The intervention targets **individual\nbehavior change**, such as:\n15\n- Improving sleep\n16\n- Reducing smoking or alcohol use\n17\n- Encouraging healthier eating or\nweight loss\n18\n- Addressing mental health\n19\n- Promoting a healthier lifestyle\n20\n3. The intervention is **non-coercive**\n(no force, mandates, or excessive\nfinancial rewards or penalties).\n21\n4. The behavior change mechanism relies\non **automatic, subconscious\ncognitive processes**, such as\nheuristics or biases.\n22\n5. The intervention aligns with at least\none known **nudge type** (e.g.,\ndefaults, salience, reminders, social\nnorms, framing).\n23\n6. Has expirical evidence from actual\nimplementation or experiment\n24\n25\n#### Exclusion Criteria (ANY of these\nmakes the claim false):\n26\n1. The study focuses on **medical\ntreatments**, **drugs**, or **\nclinical procedures**.\n27\n2. The intervention is based on **\nfinancial incentives**, **penalties\n**, **subsidies**, or **taxes**.\n28\n3. The focus is on **policy or systemic\ninterventions**, not individual-level\nbehavior.\n29\n4. The paper is a **review**, **meta-\nanalysis**, or **theoretical piece**\nwith no implemented intervention.\n30\n5. There is **no specific intervention**\ndescribed or applied to participants\n.\n31\n32\n### Paper:\n33\n**Title**: {title}\n"}, {"page": 9, "text": "34\n**Abstract**: {abstract}\n35\n**Full Text**: {full_text}\n36\n37\n### Give your answer in the following\nformat:\n38\n{{\n39\n\"Reasoning\": \"give your reasoning here\nexplaining your decision\"\n40\n\"Supported\": [Yes/No]\n41\n}}\n42\n\"\"\".strip()\n43\n)\nAppendix C: Sample Model Outputs\nTo illustrate how the classification and extraction pipeline\noperates in practice, we provide sample outputs generated\nby the LLaMA 3.1 8B model. These examples demonstrate\nboth positive and negative classifications, highlighting the\nstructured JSON schema returned by the system.\nIdentified Articles with Nudge Interventions\nExample 1\nPaper title: A Multisite Randomized Trial of a Motivational\nIntervention Targeting Multiple Risks in Youth Living with\nHIV: Initial effects on Motivation, Self-Efficacy and Depres-\nsion\n1\n{\n2\n\"is_nudge_related\": true,\n3\n\"nudge_types\": [\"Messenger\", \"\nCommitment\"],\n4\n\"cognitive_biases\": [\"Loss aversion\"],\n5\n\"problem_behavior\": \"Multiple risk\nbehaviors (HIV medication adherence\n, sexual risk behavior, and\nsubstance use)\",\n6\n\"target_behavior\": \"Improved\nmotivation, self-efficacy, and\nreduced depression\",\n7\n\"Reason\": \"The study describes an\nintervention that modifies choice\narchitecture by using motivational\ninterviewing (MI) to increase\nmotivational readiness to change,\nbuild self-efficacy, and reduce\npsychological distress.\"\n8\n}\nExample 2\nPaper title: ProsCan for Men: Randomised controlled trial\nof a decision support intervention for men with localised\nprostate cancer\n1\n{\n2\n\"is_nudge_related\": true,\n3\n\"nudge_types\": [\"Messenger\", \"Affect\n\"],\n4\n\"cognitive_biases\": [\"Loss aversion\",\n\"Status quo bias\"],\n5\n\"problem_behavior\": \"Decision-related\ndistress\",\n6\n\"target_behavior\": \"Improved decision-\nmaking and adjustment to cancer\",\n7\n\"Reason\": \"The study describes a\ndecision support intervention that\ntargets individual behavior change,\nuses messenger and affect nudge\ntypes, and addresses cognitive\nbiases such as loss aversion and\nstatus quo bias.\"\n8\n}\nArticles Identified as Not Meeting Nudge Criteria\nExample 3\nPaper title: Clinician and cancer patient views on patient\nparticipation in treatment decision-making: a quantitative\nand qualitative exploration\n1\n{\n2\n\"is_nudge_related\": false,\n3\n\"Reason\": \"The study does not describe\nan intervention that modifies\nchoice architecture, nor does it\ntarget individual behavior change.\nThe study is a qualitative and\nquantitative exploration of\nclinician and cancer patient views\non treatment decision-making,\nwithout an actual intervention.\"\n4\n}\nExample 4\nPaper title: Olfactory mechanisms of stereotyped behavior:\non the scent of specialized circuits\n1\n{\n2\n\"is_nudge_related\": false,\n3\n\"nudge_types\": [],\n4\n\"cognitive_biases\": [],\n5\n\"problem_behavior\": \"\",\n6\n\"target_behavior\": \"\",\n7\n\"Reason\": \"The paper does not describe\nan evidence-based nudge\nintervention. It is a review\narticle discussing mechanisms of\nolfactory behavior and\nidentification of specialized\nolfactory circuits.\"\n8\n}\nAppendix D: Illustrative Error Cases\nFalse Positives\nExample 1 – Misclassified Systematic Review\nPaper Title: Can nudge-interventions address health\nservice overuse and underuse? Protocol for a systematic\nreview\nGround Truth: Not a nudge intervention study (systematic\nreview protocol).\nModel Configuration: Title + Abstract + Introduction.\nModel Prediction: Classified as nudge study.\nModel Reasoning (excerpt):\nThe paper describes nudge-interventions aimed at\nhealth professionals to reduce overuse and underuse\nof health services. The interventions modify choice\n"}, {"page": 10, "text": "architecture, target individual behavior change, are\nnon-coercive, voluntary, and do not involve exces-\nsive financial rewards or punishments. They attempt\nto predictably influence health professionals’ behav-\nior by targeting subconscious routines and biases\npresent in decision-making. The paper mentions vari-\nous nudge types, including default, framing, salience,\nnorms, and commitment, and cognitive biases such as\ndefault bias, framing effects, loss aversion, order ef-\nfects, salience effect, and norms. The target behav-\niors include reducing inappropriate prescribing of an-\ntibiotics, reducing inappropriate total knee replace-\nment, reducing inappropriate surgery for low back\npain, reducing unnecessary screening tests for cancer,\nimproving prescribing of generic medications instead\nof brand name medications, improving anticoagula-\ntion in patients with atrial fibrillation, improving beta\nblockers for patients who have had a myocardial in-\nfarction, improving exercise prescription for chronic\nconditions, and reducing wasteful misallocation of re-\nsources.\nError Type: Misclassification of a systematic review.\nNotes: Also illustrates post hoc justification — the model\nfabricated detailed reasoning to match its incorrect label.\nExample 2 – Misclassified Theoretical Study\nPaper Title: Designing an Internet Intervention for Emerg-\ning Adults Who Experience Troubled Relationships\nGround Truth: Not a nudge intervention (conceptual/theo-\nretical design).\nModel Configuration: Title + Abstract + Introduction.\nModel Prediction: Classified as nudge study.\nModel Reasoning (excerpt):\nThe paper describes an intervention that modifies\nchoice architecture by targeting individual behav-\nior change in emerging adults experiencing trou-\nbled intimate partner relationships. The intervention\nuses narrative therapy practices as mechanisms of\nchange and employs a human-supported approach\nwith peer counselors to provide personalized re-\nsponses to users’ stories. The target behaviors are spe-\ncific and actionable, such as writing stories that ex-\nplore and challenge problematic relational schemas,\nengaging with the WISER program, and participat-\ning in a human-supported approach. The interven-\ntion aligns with recognized nudge types, including\nmessenger and default, and leverages cognitive biases\nsuch as loss aversion and framing.\nError Type: False positive due to hallucinated nudge fram-\ning.\nNotes: Shows unfaithful reasoning traces — model gener-\nated a nudge framing where none existed.\nExample 3 – System-Level Initiative Misclassified\nas Nudge\nPaper Title: Creating the Exceptional Patient Experience in\nOne Academic Health System\nGround Truth: Not a nudge intervention at the individual\nlevel (organizational/system-level initiative).\nModel Configuration: Title + Abstract + Introduction.\nModel Prediction: Classified as nudge study.\nModel Reasoning (excerpt):\nThe University of Utah Health Care system’s Excep-\ntional Patient Experience (EPE) initiative uses various\nnudge types to improve patient satisfaction and ex-\nperience. The initiative includes posting patient satis-\nfaction scores and comments on the university’s pub-\nlic Web site, which serves as a messenger nudge. The\nsystem also uses default nudge by pre-selecting an op-\ntion to improve patient satisfaction. The initiative also\nuses loss aversion and framing cognitive biases to in-\nfluence behavior. The target behaviors include post-\ning patient satisfaction scores and comments, sharing\npatient feedback and best practices, providing values-\nbased employment and retention, reward, and recog-\nnition, improving communication and empathy be-\ntween patients and providers, and enhancing profes-\nsionalism and reducing variability in performance.\nError Type: False positive due to misclassification of a\nsystem-level initiative as an individual-level nudge.\nNotes: The nudge described is not applicable at the indi-\nvidual level, but the model inferred detailed individual-level\neffects.\nExample 4 – Population-Level Policy Misclassified\nas Nudge\nPaper Title: Dynamics of Mask Use as a Prevention Strat-\negy against SARS-CoV-2 in Panama\nGround Truth: Not a nudge intervention at the individual\nlevel (population-level policy).\nModel Configuration: Title + Abstract + Introduction.\nModel Prediction: Classified as nudge study.\nModel Reasoning (excerpt):\nThe study describes an intervention that modifies\nchoice architecture by promoting the use of surgi-\ncal masks over cloth masks and encouraging mask\nusage in suburban neighborhoods through targeted\nmessaging. The intervention targets individual behav-\nior change by reducing the transmission of SARS-\nCoV-2. The study does not rely on direct financial\nincentives, taxes, penalties, or subsidies. The inter-\nvention aligns with the nudge type ’Messenger’ and\n’Salience’ and attempts to predictably influence be-\nhavior by targeting subconscious routines and biases\npresent in decision-making. The study focuses on in-\ndividual behavior modification rather than policy or\ngovernment-level interventions.\nError Type: False positive due to misclassification of a\npopulation-level policy as an individual-level nudge.\nNotes: The intervention is primarily a public health pol-\nicy; the model incorrectly framed it as an individual nudge.\n"}, {"page": 11, "text": "False Negatives\nExample 5 – Digital Environment Gamification\nMisclassified as Nudge\nPaper Title: Competitive gamification in crowdsourcing-\nbased contextual-aware recommender systems\nGround Truth: Nudge intervention (targets engagement\nbehavior via gamification).\nModel Configuration: Gemini 2.5 pro with full docu-\nment\nModel Prediction: Classified as non nudge study.\nModel Reasoning (excerpt):\nThe paper describes an intervention using competitive\ngamification to encourage users to perform crowd-\nsourcing tasks for a restaurant recommender sys-\ntem during COVID-19. While the system’s output\n(restaurant recommendations) could indirectly relate\nto health by helping users avoid risky places, the inter-\nvention itself (gamification) is applied to the crowd-\nsourcers to influence their task engagement behav-\nior, not to the end-users to influence their health-\nrelated choices. Specifically, Inclusion Criterion 2\nstates: The intervention targets individual behavior\nchange, such as: Improving sleep, Reducing smoking\nor alcohol use, Encouraging healthier eating or weight\nloss, Addressing mental health, Promoting a health-\nier lifestyle. The behavior targeted by the gamification\nintervention in this paper is users performing crowd-\nsourcing tasks and engage more in contributing data.\nThis behavior, while valuable for the system, does not\ndirectly align with the examples of individual health\nor lifestyle behaviors listed in Criterion 2. Therefore,\nInclusion Criterion 2 is not met, making the claim un-\nsupported.\nError Type: False negative due to conservative interpreta-\ntion of digital gamification as a non-health-related nudge.\nNotes: Model was over-conservative; sensitive to prompt\nwording and classified the digital gamification intervention\nas non-nudge despite it influencing individual engagement\nbehavior.\nExample 6 – Digital Environment Salience and\nPriming Misclassified as Non-Nudge\nPaper Title: [Efficacy and Moderation of Mobile App-\nBased Programs for Mindfulness-Based Training, Self-\nCompassion Training, and Cognitive Behavioral Psychoe-\nducation on Mental Health: Randomized Controlled Nonin-\nferiority Trial]\nGround Truth: Nudge intervention (digital environment\nsalience and priming).\nModel Configuration: Gemini 2.5 Pro with full docu-\nment\nModel Prediction: Classified as non-nudge study.\nModel Reasoning (excerpt): The paper describes mobile\napp-based programs involving mindfulness-based training,\nself-compassion training, and cognitive behavioral psychoe-\nducation. While the intervention targets individual men-\ntal health and uses digital prompts, the core content in-\nvolves conscious learning, reflection, and therapeutic exer-\ncises rather than automatic, subconscious processes. The in-\ntervention does not align with recognized nudge types such\nas defaults, salience, reminders, social norms, or framing.\nAlthough the study is an RCT with empirical evidence, it\nalso meets an exclusion criterion as the interventions are es-\ntablished psychological treatments in a clinical/therapeutic\ncontext.\nError Type: False negative due to conservative interpre-\ntation of therapeutic app-based interventions as non-nudge,\ndespite features of digital salience and priming.\nNotes: The model is highly sensitive to prompt wording\nand showing over-conservative behavior in identifying digi-\ntal environment salience and priming interventions.\nExample 7 – Digital Engagement Misclassified as\nNon-Nudge\nPaper Title: Understanding rural women’s preferences\nfor telephone call engagement with primary health care\nproviders in Nigeria: a discrete choice experiment\nGround Truth: Nudge intervention (behavioral interven-\ntion modifying digital engagement environment).\nModel Configuration: Llama 3.1 8B with full document\nModel Prediction: Classified as non-nudge study.\nModel Reasoning (excerpt): The paper describes an in-\ntervention that modifies the digital environment to facilitate\nbehavior change by providing telephone call engagement\nwith primary health care providers. While the intervention\nfacilitates behavior change, it does not alter choice archi-\ntecture in a subtle way without restricting choice, which is\na key characteristic of classical nudges. Instead, it provides\na service based on user preferences. The study focuses on\nunderstanding these preferences rather than shaping behav-\nior via environmental modifications. Therefore, the model\njudged this as not an evidence-based nudge intervention.\nError Type: False negative due to non-standard terminol-\nogy and unconventional framing of the intervention.\nNotes: The model failed to identify the intervention as a\nnudge due to unconventional terminology\nExample 8 – Decision Board Intervention Misclassified\nas Non-Nudge\nPaper Title: Developing and pre-testing a\ndecision board to facilitate informed choice about delivery\napproach in uncomplicated pregnancy\nGround Truth: Nudge intervention (physical environ-\nment priming).\nModel Configuration: Llama 3.1 8B with full text\nModel Prediction: Classified as non-nudge study.\nModel Reasoning (excerpt): The paper describes an\nintervention that modifies choice architecture by provid-\ning systematic evidence-based information to facilitate in-\nformed decision-making about delivery approach in uncom-\nplicated pregnancy. The intervention targets individual be-\nhavior change, specifically enhancing informed decision-\nmaking related to delivery approach. The intervention is\nnon-coercive, as it does not restrict choice or impose penal-\nties. The behavior change mechanism relies on automatic,\nsubconscious cognitive processes, such as heuristics or bi-\nases, as the decision board aims to present medical risks\n"}, {"page": 12, "text": "and benefits in a clear and systematic way. The interven-\ntion aligns with the nudge type of default or framing, as it\npresents information in a way that influences the decision-\nmaking process. The study provides empirical evidence\nfrom actual implementation or experiment, as it reports the\nresults of a pre-test of the decision board with 40 women.\nHowever, the focus is on policy, not individual-level behav-\nior, as the goal is to inform clinical discussions and enhance\ninformed decision-making related to delivery approach in\nuncomplicated pregnancy. Therefore, the claim that the pa-\nper is an evidence-based nudge intervention is not fully sup-\nported.\nError Type: False negative due to the intervention being\npart of a broader clinical or population-level program, which\nmade it difficult for the model to isolate and classify the be-\nhavioral component.\nNotes: The decision board uses physical environment\npriming to guide decision-making, but the model failed to\nrecognize it as a nudge due to its embedding within a clini-\ncal setting.\n"}]}