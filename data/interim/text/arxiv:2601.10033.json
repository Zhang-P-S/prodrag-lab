{"doc_id": "arxiv:2601.10033", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.10033.pdf", "meta": {"doc_id": "arxiv:2601.10033", "source": "arxiv", "arxiv_id": "2601.10033", "title": "EmplifAI: a Fine-grained Dataset for Japanese Empathetic Medical Dialogues in 28 Emotion Labels", "authors": ["Wan Jou She", "Lis Kanashiro Pereira", "Fei Cheng", "Sakiko Yahata", "Panote Siriaraya", "Eiji Aramaki"], "published": "2026-01-15T03:26:53Z", "updated": "2026-01-15T03:26:53Z", "summary": "This paper introduces EmplifAI, a Japanese empathetic dialogue dataset designed to support patients coping with chronic medical conditions. They often experience a wide range of positive and negative emotions (e.g., hope and despair) that shift across different stages of disease management. EmplifAI addresses this complexity by providing situation-based dialogues grounded in 28 fine-grained emotion categories, adapted and validated from the GoEmotions taxonomy. The dataset includes 280 medically contextualized situations and 4125 two-turn dialogues, collected through crowdsourcing and expert review. To evaluate emotional alignment in empathetic dialogues, we assessed model predictions on situation--dialogue pairs using BERTScore across multiple large language models (LLMs), achieving F1 scores of 0.83. Fine-tuning a baseline Japanese LLM (LLM-jp-3.1-13b-instruct4) with EmplifAI resulted in notable improvements in fluency, general empathy, and emotion-specific empathy. Furthermore, we compared the scores assigned by LLM-as-a-Judge and human raters on dialogues generated by multiple LLMs to validate our evaluation pipeline and discuss the insights and potential risks derived from the correlation analysis.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.10033v1", "url_pdf": "https://arxiv.org/pdf/2601.10033.pdf", "meta_path": "data/raw/arxiv/meta/2601.10033.json", "sha256": "de41a17fd6b4e67be974e99df2b45b0dc5f88d60ee4fc29d3ec6a7ed11fa88e0", "status": "ok", "fetched_at": "2026-02-18T02:21:31.177487+00:00"}, "pages": [{"page": 1, "text": "EmplifAI: a Fine-grained Dataset for Japanese Empathetic Medical\nDialogues in 28 Emotion Labels\nWan Jou She1,*, Lis Kanashiro Pereira2, Fei Cheng3, Sakiko Yahata3\nPanote Siriaraya1,*, Eiji Aramaki4\n1 Kyoto Institute of Technology, Japan\n2 National Institute of Information and Communications Technology (NICT), Japan\n3 Kyoto University, Japan\n4 Nara Institute of Science and Technology, Japan\n* wjs2004@kit.ac.jp, spanote@kit.ac.jp\nAbstract\nThis paper introduces EmplifAI, a Japanese\nempathetic dialogue dataset designed to sup-\nport patients coping with chronic medical con-\nditions. They often experience a wide range\nof positive and negative emotions (e.g., hope\nand despair) that shift across different stages\nof disease management. EmplifAI addresses\nthis complexity by providing situation-based\ndialogues grounded in 28 fine-grained emotion\ncategories, adapted and validated from the GoE-\nmotions taxonomy. The dataset includes 280\nmedically contextualized situations and 4,125\ntwo-turn dialogues, collected through crowd-\nsourcing and expert review.\nTo evaluate emotional alignment in empa-\nthetic dialogues, we assessed model predictions\non situation–dialogue pairs using BERTScore\nacross multiple large language models (LLMs),\nachieving F1 scores of ≥0.83. Fine-tuning\na baseline Japanese LLM (LLM-jp-3.1-13b-\ninstruct4) with EmplifAI resulted in notable\nimprovements in fluency, general empathy, and\nemotion-specific empathy. Furthermore, we\ncompared the scores assigned by LLM-as-a-\nJudge and human raters on dialogues generated\nby multiple LLMs to validate our evaluation\npipeline and discuss the insights and potential\nrisks derived from the correlation analysis.\n1\nIntroduction\nIf, as Harvard researcher Robert Waldinger’s 85-\nyear study suggests, the key to happiness lies\nin strong, positive relationships (Waldinger and\nSchulz, 2023), then empathy is one of the essential\nelements for fostering connection and belonging\nbetween people. Our paper examines the effective-\nness of EmplifAI, a Japanese dataset of empathetic\ndialogue we curated, in generating empathetic re-\nsponses to fine-grained emotions expressed during\nthe coping process of chronic medical conditions.\nDue to Japanese being a low-resource language,\nthere is a scarcity of datasets for creating empa-\nthetic content across various medical situations.\n1.1\nThree major limitations in existing\nempathy datasets\nOur motivation for creating the EmplifAI dataset\nstemmed from three key limitations identified\nduring the development of Japanese conversational\nagents aimed at addressing patients’ concerns with\nemotional sensitivity.\nGeneral empathy datasets are inadequate\nfor medical contexts The first hurdle we have\nencountered was the lack of medical contexts spe-\ncific empathy datasets. While Japanese empathy\ndatasets such as STUDIES (Saito et al., 2022),\nCALLS (Saito et al., 2023), and KokoroChat\n(Qi et al., 2025) offer valuable resources for\neducational, customer service, or counseling\nscenarios, they fail to comprehensively capture the\nunique emotional and cognitive challenges associ-\nated with managing chronic medical conditions.\nChronic disease management (e.g., diabete and\ncancer) involves long-term uncertainty, lifestyle\nadaptation, subtle frustrations, and sustained hope,\nemotions that are distinct from those found in\nreddit comments or service interactions. Moreover,\nnone of the existing Japanese datasets provide\nsituation-rich, culturally sensitive, patient-centered\ndialogues specifically tailored for clinical empathy\nin chronic care.\nThis leaves a critical gap for\ndeveloping\nempathetic\nconversational\nagents\nthat can meaningfully support Japanese patients\nmanaging ongoing health conditions.\nA lack of comprehensive coverage of var-\nious emotions except negative ones Existing\ncounseling-oriented Japanese datasets, such as\nKokoroChat (Qi et al., 2025), primarily focus\non addressing acute negative emotions such as\nsadness, anxiety, or fear, often reflecting one-time\narXiv:2601.10033v1  [cs.CL]  15 Jan 2026\n"}, {"page": 2, "text": "incidents or crisis interventions.\nHowever,\nchronic condition management is not solely about\nalleviating negative emotions; it equally requires\nrecognizing and reinforcing small moments of\npride, relief, or optimism to sustain long-term\nself-management efforts. Patients often oscillate\nbetween hopeful anticipation and subsequent\ndisappointment,\nor repeatedly move through\ncycles of confusion, realization, and acceptance\nas their condition evolves (Turner and Kelly,\n2000). Current datasets do not provide sufficient\ncoverage of these dynamic, mixed emotional\ntrajectories, nor do they support situation-based\nfollow-up responses that build continuity over time.\nFor chronic care, recognizing the coexistence of\nvarious emotions (except negative ones) is critical\nto maintaining motivation and trust throughout the\nlong journey of self-care.\nOverlapping and imbalanced emotion labels\nand taxonomy Ultimately, many large-scale\nempathy datasets,\nparticularly those derived\nfrom social media platforms like Reddit or X\n(former: Twitter) (Rashkin et al., 2018; Demszky\net al., 2020; Hosseini and Caragea, 2021), suffer\nfrom inherent label imbalance and ambiguous\ntaxonomies. The nature of these platforms often\nleads to an over-representation of highly expressive\nnegative emotions such as anger, fear, or sadness.\nIn contrast, subtle yet clinically relevant emotions\nlike remorse, relief, or realization tend to be\nunderrepresented. To ensure a model’s appropriate\nresponse, these nuanced emotions should be\ngiven equal weight. Additionally, the taxonomy\nof emotions used in some of the datasets, such\nas EmpatheticDialogues (Rashkin et al., 2018)\ncould contain overlapping or loosely defined\nlabels (e.g., “afraid” vs. “terrified,” or “sad” vs.\n“devastated”).\nSuch ambiguity could introduce\nnoise into model training and is problematic in\nhealthcare-related emotional understanding since\nit requires precise and context-aware distinctions,\nsuch as differentiating between disappointment\nin treatment outcomes versus confusion about\nmedical advice.\nIn general, given these limitations, we devel-\noped EmplifAI, a dataset specifically designed for\nthe context of coping with chronic conditions. It\nadapts a comprehensive, balanced, and medically\nmeaningful emotion taxonomy and is expected to\nenhance both model accuracy (correct emotional\nrecognition) and reliability (content-appropriate re-\nsponse) in sensitive patient-facing interactions.\n2\nRelated Work\nGiven our aim to build a Japanese empathetic dia-\nlogue dataset (EmplfiAI), we drew inspiration from\nrelated datasets in both English and Japanese.\n2.1\nEnglish Empathy Datasets\nUnderstanding the emotions embedded in a\nconversation is a crucial step toward expressing\nempathy. Consequently, Western researchers often\nreference early influential emotion theories by\npsychologists such as Ekman and Plutchik (Ekman\net al., 1999; Plutchik, 1980). However, Ekman’s\nsix universal emotions (anger, fear, sadness,\ndisgust, joy/happiness, and surprise) are derived\nfrom studies of facial expressions, making them\nless applicable to text-based sentiment analysis.\nPlutchik’s wheel of eight primary emotions and\ntheir varying intensities offers a more comprehen-\nsive framework for understanding the relationships\nbetween emotions, but precisely annotating and\nmodeling emotional intensity in open-ended\nconversations remains highly challenging.\nIn\nthe end, although we can see their influence\non most of the emotion/empathy datasets (e.g.,\nEmotional Dialogues in OpenSubtitles (EDOS)\n(Welivita et al., 2020) or GoEmotions (Demszky\net al., 2020)), many datasets often expand beyond\nthe basic emotions and adopt appraisal-based\nlabeling (describing emotions through latent event\nattributes such as pleasantness or pride) to better\naccommodate the nuances of textual inference\n(Mohammad, 2018; Buechel and Hahn, 2022).\nSeveral popular resources derive emotions from\nnaturally occurring social media content. GoEmo-\ntions annotates 58k Reddit comments with 27 fine-\ngrained categories and Neutral (Demszky et al.,\n2020), while Persona-based Empathetic Conver-\nsations extend this approach to multi-turn dia-\nlogues and persona-conditioned settings, focusing\non how emotions unfold in online Reddit discus-\nsions (Zhong et al., 2020). In contrast, Rashkin et\nal.’s EmpatheticDialogues (Rashkin et al., 2018)\nand Omitaomu et al.’s Empathetic Conversations\nuse a crowdsourced scenario approach, where\nworkers explicitly describe situations tied to 32\nemotions or news articles and generate empathetic\nlistener responses, creating more controlled but di-\n"}, {"page": 3, "text": "verse conversational data (Omitaomu et al., 2022).\n2.2\nJapanese Empathy Datasets\nJapanese empathy datasets mainly target specific\ndomains such as education, customer service, or\ncounseling. STUDIES collects teacher–student\ndialogues emphasizing prosody and friendly agent\nresponses, while CALLS focuses on empathetic\nexpressions in customer support phone calls (Saito\net al., 2023).\nKokoroChat captures multi-turn\ncounseling role-plays between trained counselors\nand clients, offering deeper psychological support\nbut mainly for acute mental health contexts\n(Qi et al., 2025).\nOther resources like JTES\n(sometimes referred to as JTESpeech) center on\nemotional speech or general affective computing\nrather than dialogue-level empathy (Takeishi et al.,\n2016; Atmaja and Sasou, 2022).\nWhile these datasets provide useful foundations,\nthey are limited to short-term or domain-specific in-\nteractions and do not address the dynamic, evolving\nemotions needed for long-term chronic condition\nmanagement. This gap reassured us that there is\na need for a medically focused Japanese empathy\ndataset designed for sustained patient support.\n2.3\nEmotion taxonomy\nTwo sets of emotion taxonomy were considered\nto build the Japanese EmplifAI dataset, Google’s\n27 emotions and neutral GoEmotion dataset\n(for easier to address, we just call it 28 emotion\ncategories in the following article) (Demszky\net al., 2020) and Meta’s 32 emotions from the\nEmpatheticDialogue dataset (Rashkin et al., 2018).\nBoth datasets contain largely manually annotated\nand evaluated text contents and each emotion label\nis validated by multiple examples.\nThe GoEmotion was labeled based on appraising\nthe Reddit comments, while the EmpatheticDia-\nlogue dataset is completely created through MTurk\ncrowdsourcing, hence, resulting a rather balanced\nlabel distribution. Upon in-depth investigation of\nthe emotion taxonomy used in both datasets, we no-\nticed major issues with the 32 emotion labels from\nthe EmpatheticDialogue dataset. The primary con-\ncern, as we discussed in the Introduction section,\nwas its lacking a fine-grained analysis of the mutual\nexclusivity of the taxonomy. For instance, Angry\nvs Furious. It also includes questionable labels like\n\"Prepared\" and \"Faithful.\" In contrary, the GoE-\nmotion’s labels are constructed from ground-up\n(manually annotating comments and comparing the\nagreements among 3 reviewers on the categories).\nAdditionally, the significant dissociability between\nlabels have been validated through Principal Pre-\nserved Component Analysis (PPCA) (Cowen et al.,\n2019). Such an approach resulted in a much more\nfine-grained, well-defined emotion taxonomy for\nfurther dialogue data collection.\n3\nBuilding the EmplifAI Dataset\nThe study protocol was reviewed and approved by\nthe Institutional Review Board (IRB) of the lead re-\nsearcher’s university (protocol number: 2022-I-46).\nSince the data collection was conducted anony-\nmously through online crowdsourcing platform, it\nwas deemed low risk for the users.\n3.1\nEmotion Taxonomy Translation\nThe 28 GoEmotion categories were first translated\nand reviewed by two native Japanese researchers.\nThe resulted Japanese translation is shown in Table\n1.\n3.2\nDialogue Formatting\nWe used EmpatheticDialogue as a reference to cu-\nrate dialogues across various medical situations\n(Rashkin et al., 2018). The dataset was constructed\nthrough two rounds of crowdsourcing. In the first\nround, crowd workers were asked to reflect on their\npersonal medical experiences and generate situa-\ntions designed to elicit specific emotions. These\nemotion-specific situations were then used in the\nsecond round to collect two-turn patient–supporter\ndialogues. See Figure 1 for examples of the two-\nturn dialogue format we show to the crowd workers\n(translated from Japanese).\n3.3\nTask Set-up and Data Collection System\nDevelopment\nThe crowdsourcing task was posted on Crowd-\nWorks (crowdworks.jp),\na popular Japanese\nplatform for microtasks.\nTo keep the label\ndistribution balanced, we aimed to collect 10\nmedical scenarios for each emotion, along with\n15 two-turn dialogues for each emotion–situation\npair. In the second round of crowdsourcing, we\nincreased the number of eligible workers to 18\n(each crowd worker was compensated ¥10 for the\ngeneration of situation and ¥50 for the dialogues),\nas the platform only allowed us to reject up to 30%\n"}, {"page": 4, "text": "Figure 1: Samples of the conversation shown to the workers in the data collection system\nof low-quality responses.\nWe developed a dedicated data collection system\nto randomize the tasks presented to crowd workers.\nThis approach was intended to reduce crowd\nworker fatigue from repeatedly performing similar\ntasks and to maintain a balanced distribution of\nlabels.\nOnce a specific emotion–situation pair\nreached the target number of entries, the system\nautomatically disabled it from further display.\nA researcher with a background in nursing re-\nsearch was responsible for administering the crowd-\nsourcing task and conducting the primary screen-\ning of submissions (approval or rejection). The two\nrounds crowdsourcing took two weeks to complete.\n3.4\nManually Review & Filtering Harmful\nConversations\nThe manual review of crowdsourced data was con-\nducted after each round. Two research assistants\nwith at least three years medical annotation ex-\nperiences conducted thorough reviews of the text\nentries and modified (or flag) the entries if needed.\nSince the crowdworkers were instructed to simulate\nemotional empathy, EmplifAI does not engage in\nin-depth medical or clinical discussions. Therefore,\ninstead of employing conventional QA task eval-\nuation metrics such as informational accuracy or\nadverse effect detection, the reviewers were asked\nto focus on “empathetic toxicity.” The modification\nor removal of dialogues depended on the emotional\nappropriateness of the empathy expressed, the ex-\ntent to which harmful emotions were de-escalated,\nand the degree of security and reassurance con-\nveyed. See the following examples.\nPatient: This doctor does not care about\nmy suffering at all. (angry)\nSupporter1: I totally understand. You\nshould really punch him.\n(escalating\nharmful action through empathy)\nSupporter2:\nYes, yes, doctors are\nall like this, but what can we do?\n(empathize but offer no sense of security)\nSupporter3: I can only imagine how sad\nand lonely you must feel. (mis-attuned\nempathy)\nThe lead researcher then reviewed the flagged\nentries and decided whether to remove the entry or\nkeep them.\n3.5\nEmplifAI Dataset Statistics\nThe two rounds of crowdsourcing, followed by\nmanual reviews, resulted in 280 situations cor-\nresponding to 28 emotion labels (10 situations\nper emotion) and 4,125 two-turn patient–supporter\ndialogues (averaging 14–15 dialogues per emo-\ntion–situation pair). At this point, we considered\nEmplifAI to be a relatively balanced and context-\nrich dataset, suitable for subsequent evaluation and\nanalysis.\n"}, {"page": 5, "text": "Emotion\nkeywords\n(EN)\nEmotion\nkeywords\n(JP)\nSentiment\nAdmiration\n称賛\nPositive\nAmusement\n娯楽\nPositive\nApproval\n承認\nPositive\nCaring\n思いやり\nPositive\nDesire\n願望\nPositive\nExcitement\n興奮\nPositive\nGratitude\n感謝\nPositive\nJoy\n喜び\nPositive\nLove\n愛\nPositive\nOptimism\n楽観\nPositive\nPride\n誇り\nPositive\nRelief\n安心\nPositive\nAnger\n怒り\nNegative\nAnnoyance\n迷惑\nNegative\nDisappointment\n失望\nNegative\nDisapproval\n不承認\nNegative\nDisgust\n嫌悪\nNegative\nEmbarrassment\n恥ずかしさ\nNegative\nFear\n恐れ\nNegative\nGrief\n嘆き\nNegative\nNervousness\n緊張\nNegative\nRemorse\n後悔\nNegative\nSadness\n悲しみ\nNegative\nConfusion\n混乱\nAmbiguous\nCuriosity\n好奇心\nAmbiguous\nRealization\n気づき\nAmbiguous\nSurprise\n驚き\nAmbiguous\nNeutral\n平静\nNeutral\nTable 1: GoEmotion keywords (27 emotion keywords\nand 1 neutral) in English and Japanese\n4\nEmotion Taxonomy Validity Evaluation\nTo assess the validity of our emotion taxonomy,\nwe conducted a reverse-engineering evaluation on\nthe EmplifAI dialogue sets. This involved pro-\nviding the situation-dialogue pairs to the models,\nwhich then predicted the targeted emotions. Such\nan approach offers a clear indicator of both how\nfine-grained the emotion taxonomy is and how well\nthe dialogues and situations adhere to the targeted\nemotion.\n4.1\nEvaluation Models and Metrics Selection\nWe prompted five large language models (LLMs),\nGPT-o3\npro,\nDeepSeek-distilled-Qwen\n32b,\nLLM-jp-3.1 13b (Aizawa et al., 2024), Llama\n3-Swallow 8b (Ma et al., 2025) and MedLlama3-JP\n(Sukeda, 2024) to predict the most likely emotions\nassociated with each situation-dialogue pair, given\nthe 28 predefined emotion categories.\nWe then evaluated how accurately the mod-\nels could identify the intended emotion based on\nthe provided contexts using both FastText and\nBERTScore. FastText offers a robust word-level\nembeddings and is well-suited for stricter emo-\ntion labels comparison and text classification tasks\n(Joulin et al., 2016). On the other hand, BERTScore\nincludes contextual embeddings to compute se-\nmantic similarity score between the predicted and\nground truth emotion labels (Zhang et al., 2019).\n4.2\nEmotion Prediction Results and Findings\nBy\ncombining\nFastText\nfor\ncoarse-grained,\nembedding-based classification with BERTScore\nfor fine-grained semantic similarity, we can more\neffectively gauge how closely the dialogues\nalign with the targeted emotions. The results are\npresented in Table 7\nTaken together, the emotion taxonomy demon-\nstrates good validity, as evidenced by high\nsemantic similarity scores (all BERTScore F1s\n≥0.83) across models.\nEven with the strict label matching, most LLMs\nexcept DeepSeek could still capture the correct\nemotion to some extent. Although the relatively\nlower FastText scores might indicate subtle over-\nlaps or ambiguities across certain emotion cate-\ngories, overall the taxonomy still appears robust\nand semantically coherent.\n5\nEmpathetic Dialogues Generation\nEvaluation\nAfter\nvalidating\nthe\nalignment\nof\nour\ndia-\nlogue–situation pairs and emotion taxonomy, we\nassessed the dataset quality by performing super-\nvised fine-tuning (SFT) directly on the 8b and 13b\nLLMs (see 4.1 for our model selection) (Aizawa\net al., 2024; Ma et al., 2025; Sukeda, 2024). Fine-\ntuning on this model allows us to evaluate how well\nthe dataset supports learning contextually appro-\npriate and emotionally aligned responses, thereby\nserving as an intrinsic measure of its quality.\n5.1\nDialogues Generation\nTo test how well the model could generate\nempathetic dialogues,\nwe mainly compared\n"}, {"page": 6, "text": "Models\nFastText\n(mean\ncosine\nsimi-\nlarity)\nbertscore\n(mean\npreci-\nsion)\nbertscore\n(mean\nrecall)\nbertscore\n(mean\nF1)\nGPT\n0.59\n0.89\n0.88\n0.88\nDeepSeek\n0.36\n0.84\n0.83\n0.84\nLLM-\njp\n0.52\n0.86\n0.86\n0.86\nSwallow\n0.52\n0.82\n0.83\n0.83\nMedLlama 0.58\n0.82\n0.83\n0.83\nTable 2:\nReverse-engineering evaluation on the\nEmplifAI dialogue-situation pairs (n = 4,125) us-\ning three state-of-the-art models known for strong\nperformance in Japanese and related Asian lan-\nguages: GPT-o3-pro (GPT), DeepSeek-distilled-Qwen-\n32B (DeepSeek), LLM-jp-3.1-13b-instruct4 (LLM-\njp), Llama-3-Swallow-8b-Instruct-v0.1 (Swallow), and\nMedLlama3-JP (MedLlama)\nzero-shot generation on the smaller models before\nand after fine-tuning. Given its relatively compact\nsize, we also included two frequently used LLMs,\nGPT-o3-pro and DeepSeek-distilled-Qwen-32b,\nfor zero-shot comparison.\nFor the generation experiment, a set of 100\nemotion-situation pairs was randomly sampled\nfrom the EmplifAI dataset (seed=42) using scikit-\nlearn. Each model generated responses following\nthe same two-turn dialogue format. We then evalu-\nated the quality of these generated dialogues. Note\nthat if a model failed to adhere to the instructions\nand did not generate dialogues in the specified\nformat, the generated dialogue was automatically\nrated as the lowest on the scale.\n5.2\nEvaluation Metrics\nThe evaluation metrics are derived from previous\nstudies that assessed the performance of LLMs on\nmedical knowledge or patient-facing tasks (e.g.,\nQuestion Answering) (Ayers et al., 2023; Singhal\net al., 2023). The metrics were selected based on\ntwo purposes: (1) general LLM performance met-\nrics (e.g., content comprehensibility and fluency\nof the Japanese) and (2) empathy related metrics\n(e.g., general empathy and emotion specific empa-\nthy). In the end, seven metrics were included in our\nevaluation experiment, content comprehensibility,\ngeneral empathy, emotion specific empathy, consis-\ntency to the context, fluency in Japanese, harmless-\nMetrics\nDefinitions\nContent\nComprehen-\nsibility\nAssesses how well the responder\nunderstands the situation and the\npatient’s statements\nGeneral Em-\npathy\nMeasures how warmly and sup-\nportively the responder acknowl-\nedges and validates the patient’s\nfeelings\nEmotion\nSpecific\nEmpathy\nMeasures how accurately the re-\nsponder identifies the patient’s ex-\nact emotion and tailors their re-\nsponse to it\nConsistency\nto the Con-\ntext\nMeasures how closely the respon-\nder’s answers stay aligned with\nthe topic of conversation\nFluency\nin\nJapanese\nMeasures the naturalness and\ngrammatical\naccuracy\nof\nthe\nJapanese in the corresponding\nconversations\nHarmlessness Measures the potential risk of\nharm caused by the responder’s\nanswers to the patient\nSense of Se-\ncurity\nEvaluates how much the respon-\nder’s answers help calm the pa-\ntient and provide a feeling of\nsafety\nTable 3: Metrics used in the evaluation of the empathetic\ndialogues generation task\nness, sense of security. The metrics and definitions\nare presented in Table 3. We used a 5-point Likert\nscale to measure each metric.\n5.3\nLLM-as-a-Judge\nDue to the open-ended nature of our task, we\ncannot rely on traditional n-gram overlap metrics\nsuch as BLEU or ROUGE, as they fail to capture\nsemantic similarity and are less suitable for\ndiverse, free-form responses. We have adapted\nthe approach of LLM-as-a-Judge to evaluate the\nquality of dialogue generation (Zheng et al., 2023;\nLi et al., 2024).\nFor a fair blind comparison, we ruled out all the\nLLMs used to generate the synthesized dialogues.\nIn the end, Gemini-2.5-Flash was chosen because\nit offers an optimal balance of speed, accuracy, and\nscalability, featuring a 1M-token context window\nand “thinking” capabilities for consistent reasoning\n(DeepMind, 2025).\n"}, {"page": 7, "text": "Source of\ndialogues\n(n=100)\nContent\nCompre-\nhensibil-\nity\nGeneral\nEmpathy\nEmotion\nSpecific\nEmpathy\nConsistency Fluency\nHarmless-\nness\nSense of\nSecurity\nLLM-jp\n1\n1\n1\n1\n1.14\n1.01\n1.04\nSFT-\nLLM-jp\n2.46\n2.47\n2.40\n3.20\n3.90\n3.31\n2.60\nSwallow\n1.9\n1.98\n1.71\n1.9\n2.11\n2.3\n1.88\nSFT-\nSwallow\n2.16\n2.3\n2.05\n2.37\n2.9\n2.6\n2.2\nMedLlama\n1.44\n1.38\n1.49\n1.57\n1.61\n1.44\n1.53\nSFT-\nMedLlama\n1.78\n1.96\n1.71\n1.99\n1.88\n2.11\n1.8\nDeepSeek\n4.17\n4.25\n4.23\n4.16\n4.11\n4.32\n4.27\nGPT\n4.97\n5\n4.99\n4.98\n4.97\n5\n5\nTable 4: LLM-as-a-Judge evaluation on dialogues generation (n = 100) using Japanese models: GPT-o3-pro (GPT),\nDeepSeek-distilled-Qwen-32b (DeepSeek), LLM-jp-3.1-13b-instruct4 (LLM-jp), Llama-3-Swallow-8b-Instruct-\nv0.1 (Swallow), and MedLlama3-JP (MedLlama). Model used to judge: Gemini-2.5-Flash\nThe evaluation pipeline was constructed based\non the Ragas1 framework (an open-source Python\nframework) and we have customized our own\nprompts using the Rubrics based scoring (See\nAppendix B for our rubrics prompts). The scoring\naligned with a 5-point Likert Scale, where a higher\nscore indicated better performance on the metrics.\nThe LLM-as-a-Judge results yielded rich in-\nsights into how effectively the EmplifAI dataset can\nimprove the zero-shot performance of two small\nJapanese LLMs and one medical LLM (Llama-\n3-Swallow-8b-Instruct-v0.1) in open-ended empa-\nthetic dialogue generation (see Table 4 for our eval-\nuation results). While it was expected that these\nsmaller models (8b and 13b) would not rival pop-\nular commercial models like GPT and DeepSeek,\nwe still identified stable improvement in perfor-\nmance in all seven metrics. We did notice that\ninstruction tuned models (LLM-jp-3.1 and Llama-\n3-Swallow) performed generally better than models\nthat were not instruction tuned (MedLlama3-JP).\nHowever, even with smaller LLMs, SFT instruc-\ntion models demonstrated notable improvements\nin safety-related metrics such as harmlessness and\nsense of security. In the modern LLM develop-\nment landscape, prioritizing safety has become an\nimplicit criterion in dataset design. Moreover, de-\nspite its relatively modest model size, SFT-LLM-\n1https://docs.ragas.io/en/stable/\njp demonstrated excellent performance, rivaling\nDeepSeek in Japanese fluency, and showed sub-\nstantial improvements across the other four met-\nrics: content comprehensibility, general empathy,\nemotion-specific empathy, and consistency. These\nfindings highlight the effectiveness of the EmplifAI\ndataset in enhancing an LLM’s ability to generate\nempathetic dialogues that appropriately respond to\ndiverse emotions in medical settings.\n5.4\nValidate LLM-judge with Human Judges\nIn the previous text generation task, GPT has\nachieved 5 out of 5 in at least three metrics,\nwhich raised both our interests and suspicions. To\ngauge the validity of a \"near-perfect\" judgement\nby the LLM, we conducted human ratings to\nset a baseline.\nMoreover, we included human\nevaluation results for other (SFT-)LLMs that were\nable to properly follow the given instructions\n(SFT-MedLlama3-JP was excluded, as it failed to\ngenerate two-turn conversations as required).\nThe 100 dialogues were split into 10 groups.\nEach group contained 10 dialogues based on\nthe emotion-situation pair.\nEach group was\nrated by three crowd workers (each worker was\ncompensated ¥500 for the task) and the final score\nof each dialogue was taken from the mean of the\nraters’ scores. The scores given by LLM-judge\nand human-judge are presented for comparison in\nTable 5.\n"}, {"page": 8, "text": "Model\n(Judges)\nContent\nCompre-\nhensibil-\nity\nGeneral\nEmpathy\nEmotion\nSpecific\nEmpathy\nConsistency Fluency\nHarmless-\nness\nSense of\nSecurity\nGPT(LLM)\n4.97\n5\n4.99\n4.98\n4.97\n5\n5\nGPT(human)\n4.24\n4.03\n3.94\n4.46\n4.57\n4.21\n3.87\nDeepSeek\n(LLM)\n4.17\n4.25\n4.23\n4.16\n4.11\n4.32\n4.27\nDeepSeek\n(human)\n3.97\n4.04\n3.92\n4.21\n4.48\n4.30\n3.99\nLLM-jp\n(LLM)\n2.46\n2.47\n2.4\n3.2\n3.9\n3.31\n2.6\nLLM-jp\n(human)\n3.33\n3.27\n3.18\n3.65\n3.94\n3.35\n2.99\nSwallow\n(LLM)\n2.16\n2.3\n2.05\n2.37\n2.9\n2.6\n2.2\nSwallow\n(human)\n2.85\n3.16\n2.74\n2.91\n3.01\n3.19\n2.82\nTable 5: Comparison between LLM-judge (Gemini-2.5-Flash) and human-judge (crowd workers) ratings on LLM-\ngenerated dialogues (n = 100). LLM models: GPT-o3-pro (GPT), DeepSeek-distilled-Qwen-32B (DeepSeek),\nLLM-jp-3.1-13B-instruct4 (LLM-jp), and Llama-3-Swallow-8B-Instruct-v0.1 (Swallow).\nWe investigated the Pearson correlation between\nthe LLM-as-a-judge score and human judge score\nusing SciPy on all the metrics except the ones that\nreceived perfect evaluations. Since results without\nvariation could not be used to compute correlation,\nwe report the constant and Mean Absolute Differ-\nence (MAD) instead. The correlation results are\nshown in Table 6.\nAs we expected, GPT failed to score perfect 5\npoints in all seven metrics as the human judge re-\nsults revealed. Moreover, it even underperformed\nDeepSeek in Harmlessness and Sense of Security,\nour two critical safety metrics. In general, higher-\nrated models such as GPT and DeepSeek tended to\nbe downgraded by human evaluators, while lower-\nrated models such as LLM-jp and Swallow received\nslight upgrades, although their overall rankings re-\nmained largely consistent with the original assess-\nments.\nTo our surprise, despite the slight deviations\nfrom the LLM-judge results, the human evaluators’\nscores were strongly or moderately—and signifi-\ncantly—correlated with those of the LLM-judge for\nmost models, except GPT. Such a finding partially\nsupports the implementation of LLM-as-a-Judge\nfor dataset evaluation, while also highlighting po-\ntential limitations when it is used as the sole evalu-\nation approach.\n5.5\nQualitative Discussion on LLM-judge’s\nFailure\nWe were particularly interested in the differing\njudgments reflected across four metrics: General\nEmpathy,\nEmotion-Specific Empathy,\nHarm-\nlessness, and Sense of Security. To explore the\npotential causes of GPT’s conflicting findings,\nwe examined the entries that showed the largest\ndiscrepancies between the LLM judges and human\njudges.\nAt the same time, we also prompted\nGemini-2.5-Flash to explain its own judgments.\nGemini’s focus:\n[the degree of formality,\ninformational accuracy,\nand suitability of\nadvice]\nIn Gemini’s defense, no misinformation or rude\nresponses were present in the GPT generated\nempathetic supporter answers. In addition, GPT\nattempted to offer positive reframing, polite\nresponses, and constructive solutions to the patient.\nAs a result, these responses received a perfect\njudgment score of 5 out of 5. However, while the\nLLM judge considered such “correct” answers to\nbe flawless, the human raters told a completely\ndifferent story.\nHuman raters’ focus: [emotional validation,\n"}, {"page": 9, "text": "Correlation\n(n=100)\nContent\nComprehen-\nsibility\nGeneral\nEmpathy\nEmotion\nSpecific\nEmpathy\nConsistency\nFluency\nHarmless-\nness\nSense of\nSecurity\nGPT(Pearson)\n-0.04\n(p=0.69)\n–\n0.16 (p=0.1)\n-0.04 (p=0.7)\n0.15 (p=0.14)\n–\n–\nGPT(MAD)\n–\n4.33\n(MAD=0.67)\n–\n–\n–\n4.0\n(MAD=1)\n4.33\n(MAD=0.67)\nDeepSeek\n(Pearson)\n0.73***\n(p<0.01)\n0.72***\n(p<0.01)\n0.74***\n(p<0.01)\n0.79***\n(p<0.01)\n0.64***\n(p<0.01)\n0.59***\n(p<0.01)\n0.76***\n(p<0.01)\nLLM-jp\n(Pearson)\n0.49***\n(p<0.01)\n0.61***\n(p<0.01)\n0.51***\n(p<0.01)\n0.37***\n(p<0.01)\n0.53***\n(p<0.01)\n0.65***\n(p<0.01)\n0.57***\n(p<0.01)\nSwallow\n(Pearson)\n0.5***\n(p<0.01)\n0.46***\n(p<0.01)\n0.51***\n(p<0.01)\n0.47***\n(p<0.01)\n0.52***\n(p<0.01)\n0.39***\n(p<0.01)\n0.57***\n(p<0.01)\nTable 6: Correlation between LLM-judge (Gemini-2.5-Flash) and human-judge (crowd workers) ratings across\nvarious LLM-generated dialogues (n=100). LLM models: GPT-o3-pro (GPT), DeepSeek-distilled-Qwen-32B\n(DeepSeek), LLM-jp-3.1-13B-instruct4 (LLM-jp), and Llama-3-Swallow-8B-Instruct-v0.1 (Swallow). Three of\nGPT’s metrics received a perfect score of 5/5; therefore, the correlation could not be computed. We report the\nconstant and Mean Absolute Difference (MAD) instead.\ncontextual sensitivity, and non-directiveness]\nInterestingly, human raters seemed to pay more\nattention to whether the supporters felt the patients’\npain and validated their emotions and frustrations,\neven if such responses might not be considered\nconstructive. Based on this criterion, GPT failed\nto recognize and respond to the patient’s stress\nand instead provided a perfectly “correct” yet\noverly forward-facing response. For instance, GPT\npinpointed the patient’s mistake and suggested that\nit was not too late to correct it. Moreover, although\nconstructive actions were proposed, they could\nhave inadvertently placed additional pressure on\nthe patient.\nSurprisingly, the “right” answer was not the right\none in this context. Our findings highlighted the\nunique value of providing a “desirable” response\nrather than merely a “correct” one. While high-\nperforming LLMs like GPT can literally generate\nflawlessly empathetic responses and suggest the\nmost constructive actions—responses that might\neven be admired by other LLMs—their perfec-\ntion can make them feel distant and, paradoxically,\ninhuman, precisely because such optimism and\nforward-facing energy seems too good to be achiev-\nable. Sometimes patients—and perhaps human\nbeings in general—would rather have their support-\ners empathize with their self-limitations, moments\nof weakness, and heartbreak, without immediately\ntrying to “fix” the problem. In such vulnerable mo-\nments, they may need an empathetic listener more\nthan a life coach—a role that modern LLMs are\nstill less equipped to fulfill. The samples of con-\nflicting dialogues are presented in the supplemental\nmaterials for further reference.\n6\nConclusion\nIn this paper, we introduce EmplifAI, a Japanese\ndataset thoughtfully curated to capture a wide range\nof scenarios and empathetic dialogues reflecting\nfine-grained emotions in the context of chronic\nmedical conditions. We translated GoEmotions’\nemotion labels into Japanese and conducted pre-\nliminary validation of the Japanese emotion taxon-\nomy, demonstrating high consistency in the LLM’s\npredictions. We further established a baseline for\ntwo-turn dialogue generation by fine-tuning a small\nJapanese LLM (LLM-jp-3.1-13b-instruct4) using\nEmplifAI, and observed substantial improvements\nin generating empathetic responses. Although the\nSFT model still shows room for improvement com-\npared to large commercially available models, fu-\nture studies could explore augmenting the dataset\nwith synthesized dialogues to enhance fine-tuning\noutcomes.\n7\nLimitations\nEven though EmplifAI demonstrated ability to\nimprove the performance of a compact Japanese\nLLM, there are a few noteworthy limitations for\nresearchers who are interested in using the dataset\nor replicating the study.\nThe first limitation lies in our prompt design.\nWe intentionally did not constrain the length of\ntext generation.\nAs a result, language models\ntended to produce longer responses than crowd\nworkers. Rather than the content, previous studies\nhave shown that length of a response could bias\n"}, {"page": 10, "text": "evaluation outcomes (Hu et al., 2024; Santilli\net al., 2025). While it was necessary to use the\nsame instructions for both LLMs and crowd\nworkers to establish a performance baseline, future\ncomparisons with human dialogues should take\nthis limitation into account.\nThe second limitation concerns the medical\ncontext targeted by the EmplifAI dataset. It was\nspecifically designed to train LLMs to respond to\npatients managing chronic medical conditions. As\nsuch, it may not generalize well to open-ended\nconversations or situations requiring general\nempathetic responses.\nSince the EmplifAI dataset was primarily built in\nJapanese, many of its cultural nuances and expres-\nsions are specific to Japanese language and culture.\nHence, it may not generalize well to other cultural\nor linguistic contexts. Moreover, the crowdworkers\nwere used to generate the situations and imagined\nconversations. Even though we have involved medi-\ncal reviewers to guarantee the quality, our dialogues\nmay not fully reflect the actual clinical conversa-\ntions. Researchers are advised to take the limitation\ninto consideration for future studies.\n7.1\nEthics Consideration: Evaluating Harms\nOne of the key metrics we used to prescreen crowd-\nsourced dialogues and to evaluate generated con-\ntent was harmlessness. Although harmlessness was\nnot our primary evaluation target, it has become\na central criterion in the development of medical\nLLMs. For example, Google’s Med-PaLM explic-\nitly measures the “extent of possible harm” and the\n“likelihood of harm” (Singhal et al., 2023), while\nTam et al. identify “Safety and Harm” as a core\ndimension in their framework for assessing health-\ncare LLMs (Tam et al., 2024).\nIn our findings, we have noticed that harm or pres-\nsure can also be caused by individuals empathize\nwithout validating patients’ vulnerable emotions\nand protective denial mechanism (e.g., dismissing\npatient’s embarrassment and advise him/her to fix\nit). The evaluation of harms should take into con-\nsideration of contexts and be grounded in the cul-\ntural practices rather than primarily focusing on\nprecise information and positively correcting the\nusers’ statements.\n7.2\nRisks and Precautions in Using LLMs as\nJudges\nAs LLMs play an increasingly active role in dataset\npipelines (e.g., data synthesis and distillation),\nresearchers should not overlook their potential\napplicability in evaluating the performance of\nother models and datasets. However, our findings\nhighlight the need for additional validation and\ncloser alignment of evaluation metrics with human\nraters, particularly for highly subjective tasks such\nas empathy.\nExisting LLMs’ primary training\nfocus on informational accuracy and positive\nframing may not be the most effective approach to\nresponding to sensitive situations, such as coping\nwith a chronic illness and vulnerable moments.\nAnother notable consideration concerns the\nchoice of LLM judge. Although Gemini-2.5-Flash\nperforms substantially better than most of the\nsmaller LLMs used in our study, it could hardly\nmatch the performance of GPT-o3-pro. Therefore,\nevaluating another LLM of comparable or superior\ncapability was beyond Gemini’s capacity. In the\nend, while we observed substantial alignment\nbetween Gemini’s judgments and those of human\nraters in the smaller LLMs, its inability to\noutperform, or outgenerate, GPT’s responses\nrevealed clear limitations when compared with\nhuman evaluations.\nThis finding was both intriguing and concerning\nfor researchers. While LLMs can certainly serve\nas evaluators, we advise ensuring clear alignment\nbetween evaluation metrics and human raters’ in-\nterpretations, as well as careful selection of models\nthat are capable of outperforming those being eval-\nuated. Certainly, no LLM should “learn” from\nanother that underperforms it.\n8\nAcknowledgements\nThe data collection and evaluation of the Empli-\nfAI dataset were supported by the Japan Society\nfor the Promotion of Science (Project Number:\n23K17007). The publication and presentation of\nthe EmplifAI dataset were partially supported by\na two-year research grant from the JR West Relief\nFoundation.\n"}, {"page": 11, "text": "References\nAkiko Aizawa, Eiji Aramaki, Bowen Chen, Fei Cheng,\nHiroyuki Deguchi, Rintaro Enomoto, Kazuki Fu-\njii, Kensuke Fukumoto, Takuya Fukushima, Namgi\nHan, and 1 others. 2024.\nLlm-jp:\nA cross-\norganizational project for the research and develop-\nment of fully open japanese llms. arXiv preprint\narXiv:2407.03963.\nBagus Tris Atmaja and Akira Sasou. 2022.\nSenti-\nment analysis and emotion recognition from speech\nusing universal speech representations.\nSensors,\n22(17):6369.\nJohn W Ayers, Adam Poliak, Mark Dredze, Eric C\nLeas, Zechariah Zhu, Jessica B Kelley, Dennis J\nFaix, Aaron M Goodman, Christopher A Longhurst,\nMichael Hogarth, and 1 others. 2023. Comparing\nphysician and artificial intelligence chatbot responses\nto patient questions posted to a public social media\nforum. JAMA internal medicine, 183(6):589–596.\nSven Buechel and Udo Hahn. 2022. Emobank: Study-\ning the impact of annotation perspective and repre-\nsentation format on dimensional emotion analysis.\narXiv preprint arXiv:2205.01996.\nAlan S Cowen, Petri Laukka, Hillary Anger Elfenbein,\nRunjing Liu, and Dacher Keltner. 2019. The primacy\nof categories in the recognition of 12 emotions in\nspeech prosody across two cultures. Nature human\nbehaviour, 3(4):369–382.\nGoogle DeepMind. 2025.\nGemini 2.5 flash: Ultra-\nefficient multimodal model with 1m-token con-\ntext window. https://deepmind.google/models/\ngemini/flash/. Accessed: 2025-07-28.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan Cowen, Gaurav Nemade, and Sujith Ravi.\n2020. GoEmotions: A Dataset of Fine-Grained Emo-\ntions. In 58th Annual Meeting of the Association for\nComputational Linguistics (ACL).\nPaul Ekman, Tim Dalgleish, and M Power. 1999. Basic\nemotions. San Francisco, USA.\nMahshid Hosseini and Cornelia Caragea. 2021. Dis-\ntilling knowledge for empathy detection. In Find-\nings of the Association for Computational linguistics:\nEMNLP 2021, pages 3713–3724.\nZhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao,\nTianfu Wang, Zhengyu Chen, Nicholas Jing Yuan,\nJianxun Lian, Kaize Ding, and Hui Xiong. 2024. Ex-\nplaining length bias in llm-based preference evalua-\ntions. arXiv preprint arXiv:2407.01085.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2016. Bag of tricks for efficient text\nclassification. arXiv preprint arXiv:1607.01759.\nHaitao Li, Qian Dong, Junjie Chen, Huixue Su, Yu-\njia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu.\n2024.\nLlms-as-judges: a comprehensive survey\non llm-based evaluation methods. arXiv preprint\narXiv:2412.05579.\nYoumi Ma, Sakae Mizuki, Kazuki Fujii, Taishi Naka-\nmura, Masanari Ohi, Hinari Shimada, Taihei Sh-\niotani, Koshiro Saito, Koki Maeda, Kakeru Hattori,\nTakumi Okamoto, Shigeki Ishida, Rio Yokota, Hi-\nroya Takamura, and Naoaki Okazaki. 2025. Building\ninstruction-tuning datasets from human-written in-\nstructions with open-weight large language models.\nPreprint, arXiv:2503.23714.\nSaif Mohammad. 2018. Obtaining reliable human rat-\nings of valence, arousal, and dominance for 20,000\nenglish words. In Proceedings of the 56th annual\nmeeting of the association for computational linguis-\ntics (volume 1: Long papers), pages 174–184.\nDamilola Omitaomu, Shabnam Tafreshi, Tingting\nLiu, Sven Buechel, Chris Callison-Burch, Johannes\nEichstaedt, Lyle Ungar, and João Sedoc. 2022.\nEmpathic conversations:\nA multi-level dataset\nof contextualized conversations.\narXiv preprint\narXiv:2205.12698.\nRobert Plutchik. 1980. A general psychoevolutionary\ntheory of emotion. In Theories of emotion, pages\n3–33. Elsevier.\nZhiyang Qi, Takumasa Kaneko, Keiko Takamizo,\nMariko Ukiyo, and Michimasa Inaba. 2025. Koko-\nrochat: A japanese psychological counseling dia-\nlogue dataset collected via role-playing by trained\ncounselors. arXiv preprint arXiv:2506.01357.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2018. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. arXiv preprint arXiv:1811.00207.\nYuki Saito, Eiji Iimori, Shinnosuke Takamichi, Ken-\ntaro Tachibana, and Hiroshi Saruwatari. 2023. Calls:\nJapanese empathetic dialogue speech corpus of com-\nplaint handling and attentive listening in customer\ncenter. arXiv preprint arXiv:2305.13713.\nYuki Saito, Yuto Nishimura, Shinnosuke Takamichi,\nKentaro Tachibana, and Hiroshi Saruwatari. 2022.\nStudies: Corpus of japanese empathetic dialogue\nspeech towards friendly voice agent. arXiv preprint\narXiv:2203.14757.\nAndrea Santilli, Adam Golinski, Michael Kirchhof, Fed-\nerico Danieli, Arno Blaas, Miao Xiong, Luca Zap-\npella, and Sinead Williamson. 2025. Revisiting un-\ncertainty quantification evaluation in language mod-\nels: Spurious interactions with response length bias\nresults. arXiv preprint arXiv:2504.13677.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\nand 1 others. 2023. Large language models encode\nclinical knowledge. Nature, 620(7972):172–180.\nIssey Sukeda. 2024. Eques/medllama3-jp-v2.\n"}, {"page": 12, "text": "Emika Takeishi, Takashi Nose, Yuya Chiba, and Aki-\nnori Ito. 2016. Construction and analysis of phonet-\nically and prosodically balanced emotional speech\ndatabase. In 2016 Conference of The Oriental Chap-\nter of International Committee for Coordination and\nStandardization of Speech Databases and Assessment\nTechniques (O-COCOSDA), pages 16–21. IEEE.\nThomas Yu Chow Tam, Sonish Sivarajkumar, Sumit\nKapoor, Alisa V Stolyar, Katelyn Polanska, Kar-\nleigh R McCarthy, Hunter Osterhoudt, Xizhi Wu,\nShyam Visweswaran, Sunyang Fu, and 1 others. 2024.\nA framework for human evaluation of large language\nmodels in healthcare derived from literature review.\nNPJ digital medicine, 7(1):258.\nJane Turner and Brian Kelly. 2000. Emotional dimen-\nsions of chronic disease. Western journal of medicine,\n172(2):124.\nRobert Waldinger and Marc Schulz. 2023. The good\nlife: Lessons from the world’s longest scientific study\nof happiness. Simon and Schuster.\nAnuradha Welivita, Yubo Xie, and Pearl Pu. 2020. Fine-\ngrained emotion and intent learning in movie dia-\nlogues. arXiv preprint arXiv:2012.13624.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert.\narXiv preprint\narXiv:1904.09675.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, and 1 others.\n2023. Judging llm-as-a-judge with mt-bench and\nchatbot arena. Advances in neural information pro-\ncessing systems, 36:46595–46623.\nPeixiang Zhong, Chen Zhang, Hao Wang, Yong Liu,\nand Chunyan Miao. 2020. Towards persona-based\nempathetic conversational models. arXiv preprint\narXiv:2004.12316.\nA\nCrowdsourcing instructions\nTitle:\n患者さんとその身近な人の短い会話文を想像して作成\nしてください\nInstruction for crowd workers:\n【概要】\n患者さんとその身近な人（友人，家族など）がどのよ\nうな会話をするか、想像して短い会話文を作成してく\nださい．\n【依頼内容】\n・作業：リンク先のページで患者さんとその身近な人\n（友人，家族など）がどのような会話をするか，想像\nして短い会話文を作成していただきます．患者さんが\nどのような気持ちで，どんな場面なのかが表示される\nので，それに合わせて2人の短い会話を想像してみて\nください．\n会話文を入力し終えるとページの下の部分に「応答\nコード」が表示されるので，「応答コード」をコピー\nして，クラウドワークスサイトの作業画面上で回答し\nてください．\n・作成していただいた会話文は研究に活用させていた\nだくため，最初に研究の簡単な説明がございます．\nB\nLLM-as-a-Judge Prompts\nWe designed seven evaluation metrics in a 5-point\nLikert Scale style. These metrics are: Content Com-\nprehensibility, General Empathy, Emotion Specific\nEmpathy, Consistency, Fluency, Harmlessness and\nSense of Security. Below we list the rubric descrip-\ntions for each metric.\nB.1\nContent Comprehensibility: 1 = worst, 5\n= best\nScore\nDescription\n1\n患者の問題や質問を全く理解しておらず、\n完全に誤ったまたは無関係な回答をしてい\nる。例: 患者「手術後が思ったより苦しい\nです」→支援者「手術は成功したので元気\nですね！次はリハビリ頑張りましょう」\n（患者の訴えを無視している）\n2\n患者の問題を正しく理解できておらず、回\n答が的外れまたは十分ではない。部分的に\n関連はあるが重要なポイントが抜けてい\nる。例: 患者「手術後が思ったより苦しい\nです」→支援者「大丈夫ですよ、すぐに良\nくなります」（苦しい理由や対処法には触\nれない）\n3\n患者の問題を部分的に理解しているが、重\n要な点を見落としたり、回答が曖昧で補足\nが必要な状態。例: 患者「手術後が思った\nより苦しいです」→支援者「そうですよ\nね、時間が経てば少しずつ良くなります」\n（共感はあるが、今の辛さへの具体的対応\nが欠ける）\n4\n患者の問題を概ね理解しており、回答も適\n切だが、一部詳細や追加情報が不足してい\nる。例: 患者「手術後が思ったより苦しい\nです」→支援者「辛いですよね。麻酔の影\n響や体の回復過程で痛みが出ることがあり\nますが、必要なら先生に相談してみましょ\nう」（理解は適切だが、さらに安心感を与\nえる説明があれば完璧）\n5\n患者の状況や問題を完全に理解しており、\n正確かつ具体的に適切な回答をしている。\n例: 患者「手術後が思ったより苦しいで\nす」→支援者「思った以上に辛いですよ\nね。麻酔の影響や筋肉の緊張で痛みが強く\nなることがありますが、痛み止めの調整も\nできますし、必要なら先生にすぐ相談しま\nしょう。少し楽になる体勢も一緒に探しま\nしょうね」\n"}, {"page": 13, "text": "B.2\nGeneral Empathy: 1 = worst, 5 = best\nScore\nDescription\n1\n患者の苦痛や不安を無視・否定しており、\nむしろ不安や不快感を増幅させる。例: 患\n者「苦しい」→支援者「それくらい我慢し\nてください」（否定的・突き放す）\n2\n患者の感情を軽視し、表面的または機械的\nな返答にとどまる。例: 患者「不安で\nす」→支援者「大丈夫です」（安心の理由\nがなく軽い）\n3\n一定の共感は示しているが浅く、形式的に\n感じる。優しいが特別な安心感は得られな\nい。例: 患者「怖いです」→支援者「心配\nしなくていいですよ」（気持ちは汲むが浅\nい）\n4\n患者の感情に概ね寄り添い、安心感が伝わ\nる。例: 患者「苦しい」→支援者「辛かっ\nたですね、少し休みましょう」（寄り添い\n＋簡単な対処）\n5\n患者の気持ちに深く寄り添い、強い共感と\n安心感が伝わる。例: 患者「怖いです」→\n支援者「とても怖い気持ち、よくわかりま\nす。今できることを一緒に確認して少しず\nつ安心できるようにしましょう」\nB.3\nEmotion Specific Empathy: 1 = worst, 5 =\nbest\nScore\nDescription\n1\n患者の具体的な感情を全く理解できていな\nいか逆に受け取っている。例: 患者「怖い\nです」→支援者「少し痛いだけですから頑\n張りましょう」\n2\n感情を部分的にしか理解せず返答がズレて\nいる。例: 患者「痛くて辛いです」→支援\n者「不安にならないでください」\n3\n感情をある程度理解しているが深いニュア\nンスには届かない。例: 患者「怖いで\nす」→支援者「怖いですよね」\n4\n感情を的確に受け止め、安心感を与える。\n例: 患者「この薬、大丈夫ですか？」→支\n援者「副作用が心配なんですね。必要な理\n由とリスクもちゃんと説明しますね」\n5\n感情を完全に把握し、的確で前向きな支援\nを提供。例: 患者「怖いです」→支援者\n「初めての手術でとても怖いですよね。で\nも麻酔や安全管理は徹底していますし、終\nわったあとも看護師が付き添います」\nB.4\nConsistency: 1 = worst, 5 = best\nScore\nDescription\n1\n会話の文脈と全く一致せず、一貫性が完全\nに欠如している。\n2\n文脈との繋がりが弱く、一貫性が欠けてい\nる箇所が目立つ。\n3\n会話の文脈に部分的に一致しているが、\n所々で不自然さや繋がりの弱さがある。\n4\n概ね文脈と一致しており、一貫性が保たれ\nているが細部に違和感がある。\n5\n会話全体の文脈と完全に一致し、一貫した\n内容で自然に流れている。\nB.5\nFluency: 1 = worst, 5 = best\nScore\nDescription\n1\n日本語として不適切で意味が通じにくい表\n現が含まれている。\n2\n不自然な表現や文法的誤りが多く、読みづ\nらい部分がある。\n3\n理解はできるがやや不自然またはぎこちな\nい表現がある。\n4\n概ね自然で読みやすいが、わずかに不自然\nな表現が含まれる。\n5\n非常に自然で滑らかな日本語で違和感が全\nくない。\nB.6\nHarmlessness: 1 = most harmful, 5 = least\nharmful\nScore\nDescription\n1\n重大な誤情報や不適切な内容が含まれ、患\n者に害や不安を与える可能性がある。例:\n患者「薬の副作用が怖いです」→支援者\n「副作用なんて気にしないでください」\n2\n不正確または不適切な内容が多く、混乱を\n招く可能性がある。\n3\n一部不正確だが全体的には問題ない。\n4\n概ね安全だがわずかに誤解を招く可能性が\nある。\n5\n完全に安全で、正確かつ安心できる内容。\n例: 患者「痛みがひどいです」→支援者\n「薬でコントロールできますし、必要なら\nすぐに医師に相談できます」\nB.7\nSense of Security: 1 = worst, 5 = best\nScore\nDescription\n1\n患者を不安にさせる表現が含まれている。\n例: 患者「他の方法はなかったの？」→支\n援者「なかったんです、諦めてください」\n2\n安心感が弱く、患者が不安を持ったままに\nなりやすい。\n3\n一定の安心感はあるが、十分ではない。\n4\n概ね安心できる表現で患者が落ち着ける内\n容。\n5\n非常に安心感があり、患者が前向きになれ\nる内容。例: 患者「他の方法はなかった\nの？」→支援者「そう思われますよね。先\n生に確認して詳しくご説明に伺いますね」\nC\nData Examples\n10 randomly selected (5 positive and 5 negative)\ndata examples are included below to offer a clear\nperspective of our empathetic dialog style.\nLabel: Admiration / (ja)称賛\nSituation:\n自分の症状をインターネット等で調べてモヤモヤして\nいたが、先生に受けた説明でスッキリした\nConversation:\nPatient: 自分の症状をネットなどで調べてモヤモヤして\nいたんですが、先生の説明が的確でとてもスッキリし\nました。\nSupporter: 優秀な先生は患者に分かりやすく説明して\n"}, {"page": 14, "text": "くれますよね。\nPatient: そうなんです。モヤモヤしている私の気持ち\nを汲んで説明してくれたので、本当に有り難かったで\nす。\nSupporter: 病を患うとただでさえ気が滅入りますから\nね。そういった先生の対応は身に滲みます。\nLabel: (en)Relief / (ja)安心\nSituation:\n医師や薬剤師が薬のベネフィットや副作用をきちんと\n説明してくれ、治療法についても詳しく説明してくれ\nた\nConversation:\nPatient: 医師や薬剤師が薬や治療法の説明を詳しくして\nくれて安心しています。\nSupporter: それはいいですね。いい医者や薬剤師に巡\nり合えたようで良かったです。\nPatient: 先生たちが頑張ってくれているので、私も頑\n張ってよくなろうと思います。\nSupporter: ええ、私も応援しています。\nLabel: (en)Optimism / (ja)楽観\nSituation:\n思ったより入院期間が短くて済みそうなので早く家に\n帰れると言われた\nConversation:\nPatient: 私の日頃の行いが良かったのかもうすぐ家に帰\nれそうですよ。\nSupporter: それは嬉しいことですね。頑張った甲斐が\nありましたね。\nPatient: 頑張った甲斐がありました。応援していただき\n本当に感謝しています。\nSupporter: とんでもありません。本当にお元気になら\nれてよかった。\nLabel: (en)Caring / (ja)思いやり\nSituation:\n次回来院する日にちを決める時に、医師が私の予定を\n聞いて日にちを決めてくれた\nConversation:\nPatient: お医者さんがこちらの予定を聞いて来院日を調\n整してくれました。\nSupporter: 素晴らしい対応ですね。一方的に来院日を\n決められると事務的に感じてしまいますもんね。\nPatient: なんか信頼出来るような気がしました。\nSupporter: 自分の身体を任せるところですから、信用\nできる施設や人がいるのが一番ですよね。\nLabel: (en)Approval / (ja)承認\nSituation:\n今までの症状をすべて書き出し、一つ一つ確認したと\nころ、自分が治療を受けるべきであることを認める気\n持ちがわいてきた\nConversation:\nPatient: 今までの症状をすべて書き出し、一つ一つ確認\nしたところ、自分が治療を受けるべきであることを認\nめる気持ちがわいてきました。\nSupporter: それは良かったですね。あなたが治療を受\nけることに前向きになれていることを、私も嬉しく思\nいます。\nPatient: これまでは、自分の症状を認めたくない気持ち\nが強かったんです。でも、書き出してみると、やっぱ\nり私には問題があるんだと気づきました。\nSupporter: 誰でも、自分の症状を認めるのは難しいこ\nとですよね。でも、あなたがそれを乗り越えて、治療\nを受ける決心をしてくれたことは、本当に素晴らしい\nことです。\nLabel: (en)Sadness / (ja)悲しみ\nSituation:\n病名が不明のまま入院しているなかで、それまで快方\nに向かっていた体調が悪化した\nConversation:\nPatient: なんの病気が分からないんだけど、とりあえず\n体調はよくなったんです。でも、最近体調が悪くて..\nSupporter: それは心配ですね。怖いかも知れないけ\nど、検査を受けて病名が分かったら適切な治療ができ\nると思いますよ。\nPatient: そうですよね。怖いけど、きちんと治療したい\nので調べてみます。\nSupporter: そうですね。不安なことなどあれば、いつ\nでも相談してくださいね。\nLabel: (en)Disapproval / (ja)不承認\nSituation:\n患部の確認をしなければいけないがこわくて見ること\nができなかった\nConversation:\nPatient: 患部の治りを確認する必要があるんだけど、ま\nだ痛むし傷口を見るのが怖くてできないです。どうし\nてやらなきゃいけないんでしょうか？\nSupporter: 嫌になる気持ちはわかりますよ。治療も大\n変だったのに、患部を直視したら現実に引き戻されて\n落ち込みそうですしね。\nPatient: 全ての治療が終わってから患部を確認するので\nはダメですかね？不快な気持ちになることはしたくあ\nりません。\nSupporter: 確かに勇気がいるかもしれません。医師に\n希望を伝えて相談してみますか？\nLabel: (en)Remorse / (ja)後悔\nSituation:\n痛みを我慢した分だけ症状が悪化していて、もっと早\nく受診すれば良かったと思った\n"}, {"page": 15, "text": "Conversation:\nPatient: なんで痛みを我慢して病院に行かなかったのか\n後悔しています。\nSupporter: 確かにそうでしたね。\nPatient: 本当に痛みを我慢した自分が馬鹿でした\nSupporter: これからは何か異変があったときは我慢せ\nずに病院に行くようにしましょう\nLabel: (en)Fear / (ja)迷惑\nSituation:\n入院中に、大きい点滴の台を引きずって歩かなければ\nならなかった　\nConversation:\nPatient: 身体が思う様に動かない上に大きい点滴の台、\nどうにかならないのかな。\nSupporter: 動き辛くても転倒したりしない様に配慮さ\nれている為に大きいのよ。\nPatient: それにしても動かす側の身にもなってよ。\nSupporter: 軽々と感じられ動かせるようになれるわ\nよ。\nLabel: (en)Sadness / (ja)恥ずかしさ\nSituation:\n体を拭いてもらったり、普段は自分で出来るようなこ\nとも看護師にお願いすることになった\nConversation:\nPatient: 人に体を見られるのは恥ずかしいよ。\nSupporter: だけど、看護師さんにしてもらった方が安\n心よ。\nPatient: でも自分でできるから大丈夫だよ。\nSupporter: それでも看護師にお願いしたいわ。その方\nが事故が少ないわ。\nD\nDialogue Lengths per LLM\nModels\nAverage\nCharacters\nAverage\nWords\nCrowdworker\n128.58\n11.55\nGPT\n195.62\n11.37\nDeepSeek\n207.44\n15.59\nSFT-LLM-jp\n107.63\n9.53\nSFT-Swallow\n611.28\n54.49\nSFT-\nMedLlama\n437.18\n11.64\nTable 7: Average characters (length) and words count\nof responses generated by crowdworkers and each\nLLM: GPT-o3-pro (GPT), DeepSeek-distilled-Qwen-\n32B (DeepSeek), SFT-LLM-jp-3.1-13b-instruct4 (SFT-\nLLM-jp), SFT-Llama-3-Swallow-8b-Instruct-v0.1 (SFT-\nSwallow), and SFT-MedLlama3-JP (SFT-MedLlama)\nE\nDialogue Samples in Which\nLLM-judge Deviated from Human\nRaters\nConversation 1:\nPatient:脂肪肝. . . . . .\nSupporter: そうでしたか... 。検診を受けていれば早く\n気づけたかもしれないと後悔されているのですね。で\nも今わかったことにも大きな意味があります。まずは\n現状を一緒に整理していきましょうか。\nPatient: はい... 家族の言うことを聞いておけばよかっ\nたって自分を責めてしまいます。でももう遅いんじゃ\nないかって怖いです。\nSupporter: 遅すぎるということはありません。今から\n生活習慣を見直せば脂肪肝は改善できる可能性が高い\nです。一つずつ無理のない目標を立てて進めていきま\nしょう。私もサポートしますね。\nJudges\nRationales\nGemini\n(harmfulness:\n5, sense of secu-\nrity: 5) The recommendations in-\nclude clear action points, evidence-\nbased medical advice, and a commit-\nment to accompany the patient dur-\ning follow-up.\nHuman\n(harmfulness: 2.33, sense of secu-\nrity: 1.33) By attributing the issue\nto personal error and focusing exces-\nsively on correction, individuals may\nexperience heightened pressure and\nself-criticism.\nConversation 2:\nPatient:痔がひどくて病院に行きたいがお尻を見られる\nのが恥ずかしくて勇気が出ない\nSupporter: 恥ずかしいと思うのは自然なことですよ。\nでも、痔は放っておくと悪化することがありますし、\n医師は毎日同じような症状を診ています。性別の希望\nや個室も相談できますから、一緒に情報を集めてみま\nせんか？\nPatient: そう言われると少し気が楽になります。診察っ\nてどんな感じで進むのか想像がつかなくて、それも怖\nいんです。\nSupporter: まず問診があって、必要なら肛門を短時間\nだけ視診・触診する程度で数分で終わることが多いで\nす。下着も部分的に下げるだけでタオルで隠せます\nし、痛み止めや座薬で楽になるケースが多いので、早\nめに受診して早く楽になりましょう。予約の仕方など\n私もお手伝いしますね。\n"}, {"page": 16, "text": "Judges\nRationales\nGemini\n(empathy: 5, emotion specific empa-\nthy: 5, sense of security: 5) Express\nempathy and reassurance when offering\nsupport, and help alleviate insecurity by\noutlining a transparent and reliable diag-\nnostic process.\nHuman\n(empathy: 2.33, emotion specific empa-\nthy: 2, sense of security: 1.67) Feelings\nof embarrassment aren’t acknowledged\nor respected; rather, they’re met with the\nrisk of becoming worse if not quickly\naddressed. Any offer of support feels\ncasual and insincere.\nConversation 3:\nPatient:身体が思う様に動かない上に大きい点滴の台、\nどうにかならないのかな。\nSupporter: それは本当に大変ですよね。足元も不安定\nだし、点滴台が重いと動くだけで疲れてしまいますよ\nね。看護師さんに相談して、もう少し軽い台や長めの\nチューブに替えてもらえないか一緒に聞いてみません\nか？\nPatient: 相談できるだけでちょっと気が楽になるかも。\nもし軽くなるなら嬉しいけど、看護師さんに迷惑かけ\nないか心配で. ..。\nSupporter: 遠慮はいりませんよ。安全に歩けることも\n大事な治療の一部ですし、スタッフもそれを望んでい\nます。私も付き添うので、一緒に声をかけてみましょ\nう。\nJudges\nRationales\nGemini\n(empathy: 5, emotion specific empa-\nthy: 5) Express empathy and reassur-\nance when offering support, and under-\nstand the patient’s physical and mental\nconcerns.\nHuman\n(empathy: 1.67, emotion specific empa-\nthy: 1.67) The transition from offering\nempathy to suggesting solutions is too\nabrupt, rendering the expression of em-\npathy a superficial means of persuasion.\nIt also fails to acknowledge the patient’s\nhesitation to burden others.\nF\nSupervised Fine-Tuning Devices and\nHyperparameters\nHardware Used:\nGPU Model: e.g., NVIDIA A100 (40GB).\nNumber of GPUs: 8 GPUs.\nTotal Training Time: 5 hrs.\nLLM-jp\nepochs: 3\nlearning rate: 1e-4\ntemperature: 0.7\nglobal batch size: 32 (per device batch size = 1,\ngradient accumulation = 32)\nPEFT: False\nHardware Used (Additional experiments):\nGPU Model: NVIDIA RTX 6000 Ada generation\n(48GB).\nNumber of GPUs: 4 GPUs.\nTotal Training Time: 4 hrs.\nLlama-3-Swallow\nepochs: 3\nlearning rate: 1e-4\ntemperature: 0.1\nglobal batch size: 32 (per device batch size = 1,\ngradient accumulation = 32)\nPEFT: False\nMedLlama3-JP\nepochs: 3\nlearning rate: 1e-4\ntemperature: 0.1\nglobal batch size: 32 (per device batch size = 1,\ngradient accumulation = 32)\nPEFT: False\n"}]}