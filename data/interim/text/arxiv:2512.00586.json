{"doc_id": "arxiv:2512.00586", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.00586.pdf", "meta": {"doc_id": "arxiv:2512.00586", "source": "arxiv", "arxiv_id": "2512.00586", "title": "Statistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical R&D", "authors": ["Michael R. Doane"], "published": "2025-11-29T18:40:42Z", "updated": "2025-11-29T18:40:42Z", "summary": "This work presents the development and evaluation of an NLP-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. While pharmaceutical R&D is plagued by high attrition rates and enormous costs, particularly within neuroscience, where success rates are below 10%, timely identification of promising programs can streamline resource allocation and reduce financial risk. Leveraging data from the ClinicalTrials.gov database and success labels from the recently developed Clinical Trial Outcome dataset, the classifier extracts text-based clinical trial features using statistical NLP techniques. These features were integrated into several non-LLM frameworks (logistic regression, gradient boosting, and random forest) to generate calibrated probability scores. Model performance was assessed on a retrospective dataset of 101,145 completed clinical trials spanning 1976-2024, achieving an overall ROC-AUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-specific language representation encoder. The BioBERT-based model achieved an overall ROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, 40% less squared error than would be observed using industry benchmarks. The BioBERT-based model also made trial outcome predictions that were superior to benchmark values 70% of the time overall. By integrating NLP-driven insights into drug development decision-making, this work aims to enhance strategic planning and optimize investment allocation in neuroscience programs.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.00586v1", "url_pdf": "https://arxiv.org/pdf/2512.00586.pdf", "meta_path": "data/raw/arxiv/meta/2512.00586.json", "sha256": "4a8983c81d14886bc43f6f3766d30468c091ea87612c23fd8d3c2a8baaaed500", "status": "ok", "fetched_at": "2026-02-18T02:25:43.346134+00:00"}, "pages": [{"page": 1, "text": " \nStatistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical \nR&D \n \n \nby Michael Robert Doane \n \n \n \nB.S. in Chemical Engineering, May 2019, University of Massachusetts – Lowell \nM.S. in Bioengineering, December 2021, Stanford University \n \n \nA Praxis submitted to \n \n \n \nThe Faculty of \nThe School of Engineering and Applied Science \nof The George Washington University \nin partial fulfillment of the requirements \nfor the degree of Doctor of Engineering \n \n \n \nAugust 15, 2025 \n \n \n \n \nPraxis directed by \n \nEverett Oliver \nProfessorial Lecturer of Engineering Management and Systems Engineering \n \n"}, {"page": 2, "text": "ii \n \nThe School of Engineering and Applied Science of The George Washington University \ncertifies that Michael Robert Doane has passed the Final Examination for the degree of \nDoctor of Engineering as of July 23, 2025. This is the final and approved form of the \nPraxis. \n \n \nStatistical NLP for Optimization of Clinical Trial Success Prediction in \nPharmaceutical R&D  \nMichael R. Doane \n \n \n \n \nPraxis Research Committee: \n \n \nEverett Oliver, Professorial Lecturer of Engineering and Applied Science, Praxis \nDirector \n \nStanley Small, Professorial Lecturer of Engineering and Applied Science, \nCommittee Chair \n \nDaniel Veit, Professorial Lecturer of Engineering and Applied Science, \nCommittee Member \n \n \n \n \n \n"}, {"page": 3, "text": "iii \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n© Copyright 2025 by Michael R. Doane \nAll rights reserved \n \n \n"}, {"page": 4, "text": "iv \n \nDedication \n \nFirst and foremost, this work is dedicated to my loving wife, Amanda, whose \npatience and belief in me enabled me to persevere through the challenges of this program. \nMay all who read this be fortunate enough to know someone as kind and compassionate \nas her. \n \nTo my mother and father, Catherine and Dennis, who instilled in me from a very \nearly age the value of education. They recognized academia as a path that could lead to a \nrich and fulfilling life, and I am eternally grateful for their sacrifices and encouragement. \n \nTo my closest friends, Ian, Josh, Ag Mohd, Steven, and Chris, who have been \nthere to celebrate with me in the good times, and shepherd me through the tough times. \n \nFinally, I have been fortunate enough to have several great teachers and mentors \nin my life. Scott Hildreth and Bruce Mayer of Chabot College taught me to push myself \nacademically beyond what I had thought I was capable of. Dr. Prakash Rai of UMass \nLowell believed in me before I knew whether I should believe in myself. Finally, in \nbusiness, Kundini Amin inspired in me the professional confidence to pursue a shift to \nmy current career path, which brought me to the subject matter explored in this praxis.  \n \nWithout the people whose names are mentioned above, this work would not exist. \n \n \n"}, {"page": 5, "text": "v \n \nAcknowledgements \n \nI would like to acknowledge the tireless efforts of my advisor, Dr. Everett Oliver, \nwhose guidance and encouragement was critical in the development of this work. Always \nthere to make a suggestion, be a sounding board for new ideas, or just to lend an \nempathetic ear to frustration, he exemplifies what academic advising should be. \n \nThis scholastic milestone would not have been possible without generous \nfinancial assistance from my employer, Biogen. I also greatly appreciate the wonderful \ntechnical inspiration I had from speaking about this work with my brilliant colleagues, \nDr. Joseph Charest and Cedric Liu. \n \nThe genesis of this work was not straightforward, and along that circuitous path I \nencountered a great many individuals who helped to shape my appreciation and intuition \naround technical inquiry, probability estimation, and machine learning. Though there \nmay be too many of you to name, please know that I do cherish your immeasurable \ncontribution to my educational and professional journey.  \n \n \n \n"}, {"page": 6, "text": "vi \n \nAbstract of Praxis \n \nStatistical NLP for Optimization of Clinical Trial Success Prediction in \nPharmaceutical R&D \n    \n  \nPharmaceutical research and development is plagued by high attrition rates and \nenormous clinical trial costs, particularly within neuroscience, where overall industry \nsuccess rates are below 10%. Timely identification of promising programs can streamline \nresource allocation and reduce financial risk. This praxis presents the development and \nevaluation of an NLP‐enabled probabilistic classifier designed to estimate the probability \nof technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. \nLeveraging data from the ClinicalTrials.gov database and success labels from the recently \ndeveloped Clinical Trial Outcome dataset, the classifier extracts text‐based clinical trial \nfeatures using statistical NLP techniques. These features were integrated into several non-\nLLM frameworks (logistic regression, gradient boosting, and random forest) to generate \ncalibrated probability scores. Model performance was assessed on a retrospective dataset \nof 101,145 completed clinical trials spanning 1976–2024, achieving an overall ROC-\nAUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-\nspecific language representation encoder. The BioBERT-based model achieved an overall \nROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, \n40% less squared error than would be observed using industry benchmarks. The \nBioBERT-based model also made trial outcome predictions that were superior than \nbenchmark values 70% of the time, overall. By integrating NLP‐driven insights into drug \ndevelopment decision‐making, this work aims to enhance strategic planning and optimize \ninvestment allocation in neuroscience programs.  \n"}, {"page": 7, "text": "vii \n \nTable of Contents \n \n \nDedication .......................................................................................................................... iv \nAcknowledgements ............................................................................................................. v \nAbstract of Praxis ............................................................................................................... vi \nTable of Contents .............................................................................................................. vii \nList of Figures ..................................................................................................................... x \nList of Tables ................................................................................................................... xiii \nList of Acronyms ............................................................................................................. xiv \nChapter 1—Introduction ..................................................................................................... 1 \n1.1 Background ....................................................................................................... 1 \n1.2 Research Motivation ......................................................................................... 3 \n1.3 Problem Statement ............................................................................................ 3 \n1.4 Thesis Statement ............................................................................................... 4 \n1.5 Research Objectives .......................................................................................... 4 \n1.6 Research Questions and Hypotheses ................................................................ 5 \n1.7 Scope of Research ............................................................................................. 6 \n1.8 Research Limitations ........................................................................................ 6 \n1.9 Organization of Praxis ...................................................................................... 7 \nChapter 2—Literature Review ............................................................................................ 8 \n2.1 Introduction ....................................................................................................... 8 \n2.2 Overview of Current Approaches to pTRS Estimation .................................. 10 \n2.3 NLP Models and LLMs .................................................................................. 14 \n"}, {"page": 8, "text": "viii \n \n2.4 AI-based Methods for Clinical Trials ............................................................. 15 \n2.5 AI-based Methods of pTRS Estimation .......................................................... 17 \n2.6 Prediction Validation Methods ....................................................................... 22 \n2.7 Feature Importance Analysis .......................................................................... 25 \n2.8 Summary and Conclusion ............................................................................... 25 \nChapter 3—Methodology ................................................................................................. 28 \n3.1 Introduction ..................................................................................................... 28 \n3.2 Data Collection ............................................................................................... 28 \n3.3 Data Preprocessing .......................................................................................... 30 \n3.4 Basic (non-LLM) NLP Models ....................................................................... 33 \n3.5 LLM-Enabled NLP Model Screening ............................................................. 36 \n3.6 BioBERT Models............................................................................................ 37 \n3.7 Implementation ............................................................................................... 40 \nChapter 4—Results ........................................................................................................... 43 \n4.1 Introduction ..................................................................................................... 43 \n4.2 Overall Results – non-LLM Models ............................................................... 43 \n4.3 LLM Transformer Comparison....................................................................... 52 \n4.4 BioBERT Initial Model Performance ............................................................. 53 \n4.5 BioBERT Final Model Performance .............................................................. 56 \n4.6 BioBERT Final Model Performance vs. Benchmarks .................................... 59 \n4.7 Comparison of BioBERT Final Model to Published Models ......................... 61 \n4.8 Summary ......................................................................................................... 62 \nChapter 5—Discussion and Conclusions .......................................................................... 64 \n"}, {"page": 9, "text": "ix \n \n5.1 Discussion ....................................................................................................... 64 \n5.2 Conclusions ..................................................................................................... 68 \n5.3 Contributions to Body of Knowledge ............................................................. 69 \n5.4 Recommendations for Future Research .......................................................... 69 \nReferences ......................................................................................................................... 73 \nAppendix A ....................................................................................................................... 85 \nAppendix B ....................................................................................................................... 96 \n \n \n \n \n"}, {"page": 10, "text": "x \n \nList of Figures \n \nFigure 2-1. R&D Productivity Model: Parametric Sensitivity Analysis. Error! Bookmark \nnot defined. \nFigure 4-1. Comparisons of Mean Performance for each non-LLM Model Type. ... Error! \nBookmark not defined. \nFigure 4-2. SHAP Values for Phase 1. ............................................................................. 46 \nFigure 4-3. SHAP Values for Phase 2. ............................................................................ .48 \nFigure 4-4. SHAP Values for Phase 3. ............................................................................. 49 \nFigure 4-5. Performance Metrics for All Indications and Neuroscience-only Training Sets\n............................................................................................................................... 51 \nFigure 4-6. Non-LLM Performance Metrics by Phase  .................................................... 52 \nFigure 4-7. LLM Performance Metrics by Transformer  .................................................. 53 \nFigure 4-8. BioBERT Initial Model Performance  ........................................................... 55 \nFigure 4-9. BioBERT Binary Model Performance Metrics by Training/Test Set. ........... 56 \nFigure 4-10. BioBERT Final Model Performance Metrics .............................................. 57 \nFigure 4-11. ROC Curves for BioBERT Model Predictions ............................................ 58 \nFigure 4-12. BioBERT Final Model Additional Metrics .................................................. 59 \nFigure 4-13. BioBERT Performance Metrics vs. Benchmarks ......................................... 61 \nFigure 4-14. Proportion of Trials Where Model Outperformed Benchmarks .................. 61 \nFigure A-1. Full Logistic Regression Results ................................................................... 85 \nFigure A-2. Full Gradient Boosting Results ..................................................................... 86 \nFigure A-3. Full Random Forest Results .......................................................................... 86 \n"}, {"page": 11, "text": "xi \n \nFigure A-4. ROC Curve for Phase 1 Logistic Regression Model Training on \nNeuroscience Trials .............................................................................................. 87 \nFigure A-5. ROC Curve for Phase 2 Logistic Regression Model Training on \nNeuroscience Trials .............................................................................................. 87 \nFigure A-6. ROC Curve for Phase 3 Logistic Regression Model Training on \nNeuroscience Trials .............................................................................................. 88 \nFigure A-7. ROC Curve for Phase 1 Logistic Regression Model Training on All \nIndications ............................................................................................................. 88 \nFigure A-8. ROC Curve for Phase 2 Logistic Regression Model Training on All \nIndications ............................................................................................................. 89 \nFigure A-9. ROC Curve for Phase 3 Logistic Regression Model Training on All \nIndications ............................................................................................................. 89 \nFigure A-10. ROC Curve for Phase 1 Gradient Boosting Model Training on \nNeuroscience Trials .............................................................................................. 90 \nFigure A-11. ROC Curve for Phase 2 Gradient Boosting Model Training on \nNeuroscience Trials .............................................................................................. 90 \nFigure A-12. ROC Curve for Phase 3 Gradient Boosting Model Training on \nNeuroscience Trials .............................................................................................. 91 \nFigure A-13. ROC Curve for Phase 1 Gradient Boosting Model Training on All \nIndications ............................................................................................................. 91 \nFigure A-14. ROC Curve for Phase 2 Gradient Boosting Model Training on All \nIndications ............................................................................................................. 92 \n"}, {"page": 12, "text": "xii \n \nFigure A-15. ROC Curve for Phase 3 Gradient Boosting Model Training on All \nIndications ............................................................................................................. 92 \nFigure A-16. ROC Curve for Phase 1 Random Forest Model Training on Neuroscience \nTrials ..................................................................................................................... 93 \nFigure A-17. ROC Curve for Phase 2 Random Forest Model Training on Neuroscience \nTrials ..................................................................................................................... 93 \nFigure A-18. ROC Curve for Phase 3 Random Forest Model Training on Neuroscience \nTrials ..................................................................................................................... 94 \nFigure A-19. ROC Curve for Phase 1 Random Forest Model Training on All Indications\n............................................................................................................................... 94 \nFigure A-20. ROC Curve for Phase 2 Random Forest Model Training on All Indications\n............................................................................................................................... 95 \nFigure A-21. ROC Curve for Phase 3 Random Forest Model Training on All Indications\n............................................................................................................................... 95 \n \n \n \n \n \n \n"}, {"page": 13, "text": "xiii \n \nList of Tables \n \nTable 2-1. Areas of Machine Learning Contribution to Clinical Research ...................... 16 \nTable 2-2. Comparison of Phase 2 Predictive Performance Across Models in Literature 21 \nTable 3-1. Industry Benchmarks ....................................................................................... 38 \nTable 3-2. Dataset-Specific Benchmarks .......................................................................... 39 \nTable 4-1. Performance Metric Means Across All Train/Test Sets .................................. 44 \nTable 4-2. Means of non-LLM Model Metrics by Training/Test Sets ............................. 50 \nTable 4-3. Non-LLM Performance Metrics by Phase ...................................................... 51 \nTable 4-4. LLM Performance Metrics by Transformer .................................................... 52 \nTable 4-5. BioBERT Initial Model Performance .............................................................. 54 \nTable 4-6. BioBERT Model Performance Metrics by Training/Test Set ......................... 55 \nTable 4-7. BioBERT Final Model Performance Metrics .................................................. 57 \nTable 4-8. BioBERT Final Model Additional Metrics ..................................................... 59 \nTable 4-9. BioBERT Performance Metrics vs. Benchmarks ............................................ 60 \nTable 4-10. Comparison of Model ROC-AUC Relative to Published Methods............... 62 \nTable A-1. Full Logistic Regression Results .................................................................... 85 \nTable A-2. Full Gradient Boosting Results....................................................................... 85 \nTable A-3. Full Random Forest Results ........................................................................... 86 \n \n \n \n \n \n \n"}, {"page": 14, "text": "xiv \n \nList of Acronyms \n \nBERT  \nBidirectional Encoder Representations from Transformers \nCTO  \nClinical Trial Outcome \nPR-AUC \nPrecision Recall Area Under Curve \npRS \n \nProbability of Regulatory Success \npTRS  \nProbability of Technical and Regulatory Success \npTS \n \nProbability of Technical Success \nROC-AUC \nReceiver Operating Characteristic Area Under Curve \nR&D  \nResearch and Development \n \n \n \n  \n \n \n \n \n \n \n \n \n \n"}, {"page": 15, "text": "1 \n \nChapter 1—Introduction \n \n1.1 Background \nPharmaceutical research and development represents a potentially lucrative, but \nvery costly endeavor. When accounting for cost of capital and failed therapeutic \ncandidates, the cost of developing a drug had risen at an annual rate 8.5% above inflation \nfrom 2003 to 2013, to $3.88 billion per new approved compound in 2024 dollars \n(DiMasi, et al., 2016). These cost constraints make drug failures particularly difficult for \npharmaceutical firms to endure, while preventing those resources from being used for \ndevelopment of other potential therapeutic candidates. \nThis work will concern itself primarily with clinical development phases (Phase 1, \n2, and 3). Drug therapies require a particularly long development timeline, with risks and \nunknowns becoming resolved slowly over time as studies read out. New drugs may \nbehave very differently in human subjects than in in vitro or animal testing during \npreclinical phases, so it is very difficult for firms to assess the probabilities of technical \nand regulatory success (pTRS) for emerging molecular entities.  \nThe uncertainties around these evaluations include: \n Patient safety with respect to the active ingredient(s) (the molecules that give rise to \nclinical therapeutic effect) and its chosen formulation (the overall composition of the \ndrug product, including inactive ingredients; particularly relevant in Phase 1) \n Effectiveness (efficacy) of the active ingredient(s) in the clinical indication(s) for \nwhich approval is being sought (particularly relevant in Phase 2 and 3) \n"}, {"page": 16, "text": "2 \n \n Competitors may have positive trial readouts (interim or completed trial results) for \ndrugs treating the same clinical indication, imposing a higher bar for follow-on drugs \nto surpass in order to gain approval (relevant in all phases) \n Dosage uncertainty presents a risk until the appropriate dosing regimen for a drug has \nbeen established in humans (Phases 1 and 2) \n Biomarker-related risks: not having a biomarker (a measurable molecule in patients \nthat relates to disease state or outcome) to help identify ideal patient populations and \nevaluate molecular function can impede trial success (all phases) \nAs drugs progress through preclinical and clinical stages, more is learned about \ntheir scientific and clinical properties. Thus, drug development results in the discharge of \ntechnical risks such as those surrounding safety, efficacy, dosage, etc., but resolving this \nuncertainty requires enormous capital investment.  \nA chief aim of R&D organizations is to accurately characterize their drug \ncandidate’s probability of technical and regulatory success (pTRS) so that program value \nproposition, and value for patients, can be appropriately assessed. This can then guide \nenterprise decision making (whether to progress, accelerate, or terminate a program). \nSeveral methods exist for pTRS estimation, often involving the use of industry-wide \nsuccess rate benchmarks. This can be combined with some form of expert elicitation, a \nprocess in which the technical experts in a product development team provide subjective \ninput related to a program’s likelihood of success, based on a myriad of factors that are \ndifficult to quantify. However, this process is time consuming, and subject to the biases \nof the experts being interviewed. \n \n"}, {"page": 17, "text": "3 \n \n1.2 Research Motivation \n When a program that progresses to a new phase eventually fails, this \nunsuccessful candidate represents a poor result for both companies and patients:  \n1. The opportunity cost of having invested in an asset that did not succeed rather \nthan one that might have \n2. Patients are exposed to a compound that is not efficacious, and/or does not have \nan appropriate safety and tolerability profile \nBecause of this, even small improvements in the quality of pTRS predictions can \nyield enormous benefits for the health of both patients and the industry. \n \n1.3 Problem Statement \nThe inability of pharmaceutical companies to accurately scope the technical risks \nof R&D product development scenarios diminishes R&D investment returns, with overall \ndrug success rates of approximately 10%. (Zhou, 2018). \n \nThese drug failures generate capitalized cost burdens that contribute to the overall \ncost of modern drugs, and are passed on to payers and patients. The situation is \nparticularly difficult in the field of neuroscience, which is notorious for having low \nclinical trial success rates and a lower innovation index compared to other innovative \nfields like immunology and oncology (Zaragoza Domingo, et al., 2024).  \n \nA challenge for predicting future outcomes of drug trials is that there have not \nhistorically been large accurate labels of whether past trials were successful. Recently, \nresearchers at the University of Illinois – Urbana-Champaign developed a post-hoc \nmethod of assessing with high accuracy whether clinical trials that have been completed \n"}, {"page": 18, "text": "4 \n \nwere successful based on a number of publicly available sources of information. (Gao, \n2024). The database they have built, the CTO Dataset, now can be used as a single source \nof truth, permitting the building of supervised learning models to generate predictions of \nclinical trial success for ongoing or future trials. \n \n1.4 Thesis Statement \nA predictive model using Natural Language Processing is needed to improve \nProbability of Success (pTRS) predictions for neuroscience programs in pharmaceutical \nR&D.  \nCurrent means for generating pTRS are long, cumbersome, and vulnerable to the \nsubjectivity of project teams with a vested interest in the success of the program. The \nability to make better, faster, objective predictions about the success of neuroscience \nprograms would enable enormous improvements in R&D.  \n \n1.5 Research Objectives \nInitial steps for this work will involve extracting data from ClinicalTrials.gov \n(n.d.) and the trial success estimates from the CTO Dataset built by Gao, et al. (2024). \nThese data will be integrated, filtered, and cleaned, before implementing the BERT \n(Bidirectional Encoder Representations from Transformers) deep learning model from \nGoogle, which will allow for the construction of a model trained on pre-2019 \nneuroscience trial data that is capable of generating pTRS estimates ranging from 0 to 1. \nEvaluation of this model will be accomplished by testing its ability to predict the outcome \nof neuroscience trials from after 2019 and quantifying accuracy using log loss, ROC-\n"}, {"page": 19, "text": "5 \n \nAUC, Brier score, and other metrics. These accuracy scores will be compared to the \nscores that would be seen by using industry benchmarks alone. Separate models will be \nbuilt and validated for trial Phases 1, 2, and 3, and words and phrases that have a \nparticularly strong correlation with trial outcomes will be examined. \n \n1.6 Research Questions and Hypotheses \n \nRQ1: Can a non-LLM NLP model be built that can generate >5% better-than-\nrandom pTRS predictions for neuroscience indications? \n \nRQ2: Does the use of an LLM-enhanced NLP model lead to a >5% improvement \nin pTRS prediction over non-LLM for neuroscience indications? \nRQ3: Can a model trained solely on ClinicalTrials.gov data and outcome labels \nsurpass the predictive performance of industry benchmarks by >5% for neuroscience \nindications? \nRQ4: Does use of training data from all indications (not just neuroscience) \nimprove neuroscience pTRS prediction >5%? \n \nH1: A non-LLM NLP model can be built that can generate >5% better-than-\nrandom pTRS predictions for neuroscience indications. \n \nH2: The use of an LLM-enhanced NLP model leads to a >5% improvement in \npTRS prediction over non-LLM for neuroscience indications. \n \nH3: A model trained solely on ClinicalTrials.gov data and outcome labels can \nsurpass the predictive performance of industry benchmarks by >5% for neuroscience \nindications. \n"}, {"page": 20, "text": "6 \n \nH4: Use of training data from all indications can improve neuroscience pTRS \nprediction by >5%. \n \n1.7 Scope of Research \n \nThe predictive model described here will be trained on all clinical trials in the \nClinicalTrials.gov database that existed prior to 2019, and validated on trials from 2019 \nto June 1, 2024. While some models will be built from trials for all disease indications, \nattention will be paid to whether training specifically on neuroscience programs increases \nor decreases the predictive validity of the pTRS estimates for neuroscience programs. \nSeparate models will exist for Phase 1, 2, and 3 trials. This model approach will be \napplicable for any R&D group looking to improve its pTRS estimates using analysis of \ntheir own clinical trial properties, or looking to determine which textual attributes of trial \ndescription data most closely correlate with success or failure. \n \n1.8 Research Limitations \n \nWhile the predictive model built in this research will be trained on all data within \nthe ClinicalTrials.gov database within the specified date ranges, there may be trial data \nfrom older studies that is missing or incomplete. While BERT is an advanced LLM \nmodel that is capable of ingesting and interpreting a variety of language styles, clinical \ntrial data contains medical language that the model may not be optimized for. The model \nmay not be equally applicable across all indications, and it may be better at making \npredictions for some phases than others, due to the types of information available at the \ntime that the clinical trial description is written. While the model shown here will be able \n"}, {"page": 21, "text": "7 \n \nto incorporate the content of a clinical study’s design, it cannot directly capture nuanced, \nundescribed characteristics of the program itself, or the efficacy or safety demonstrated \nby the program in its previous clinical trials. Conversely, expert elicitation processes for \npTRS estimation can easily take these factors into consideration. \n \n1.9 Organization of Praxis \n \nThis praxis is divided into five chapters. Following this introduction, Chapter 2 \nsurveys the relevant literature regarding pharmaceutical R&D risks, and the various \nmethods employed to develop pTRS estimates. These include both LLM- and non-LLM-\nbased approaches that researchers have utilized to estimate pTRS. Chapter 3 provides an \noverview of the methodology employed in this research, and in particular, the \nconstruction and implementation of the predictive models. Chapter 4 covers results of \nmodel validation, comparison with industry benchmarks and other research in this field, \nand exploration of the most significant factors identified by the model. Chapter 5 \nsummarizes the work, providing potential use cases as well as avenues for future work. \n \n \n \n \n \n \n \n \n"}, {"page": 22, "text": "8 \n \nChapter 2—Literature Review \n \n2.1 Introduction \n \nPublicly traded pharmaceutical companies dedicate more than 25% of their net \nincome to R&D, compared to 2-3% for companies within the S&P 500 as a whole \n(Austin & Hayford, 2021). This level of investment intensity is needed because of the \nsheer cost to bring a drug to market, median estimates of which range from $1.14 billion \nto $2.87 billion (DiMasi, Grabowski, and Hansen, 2016; Wouters & Kesselheim, 2024). \nThese high costs of development require pharmaceutical firms to be as judicious as \npossible when determining which drug development programs to pursue and which to \nterminate. \n \nMultiple metrics must be considered when determining the viability of \nprospective programs. These include tradeoffs between development costs, commercial \nrevenue projections (with consideration of competitive landscape), time/duration of \ndevelopment, and risk (Grudzinskas, 2022). Risk is often characterized by examining the \nprobability of success, which can include technical success (pTS), regulatory success \n(pRS) or the probability of both technical and regulatory success (pTRS) (Stalder, 2022).  \n \nOnce a pTRS estimate is available, a company may then value a program asset by \nrisk-adjusting discounted cash flows to generate an expected net present value, or eNPV. \nSince pTRS dictates the likelihood that a drug candidate molecule will progress forward \nand be launched, it is a critical parameter in the determination of program value (Harpum, \n2010). Improvements in estimates of pTRS allow firms to better identify and reject \n"}, {"page": 23, "text": "9 \n \nprograms that are unlikely to succeed, and invest in programs that are likely to help \npatients. (Wong et al., 2019) \n \nParticular attention is also paid to pTRS because of the sensitivity of program \nvalue analysis to this variable as an input. As Peck (2017) points out, “relatively small \nimprovements in success rates, especially in expensive clinical development can \nsubstantially decrease the total cost of drug development.” A sensitivity analysis by Paul, \net al. (2010) demonstrates this, showing that success probabilities represent four of the \nsix most sensitive inputs when computing capitalized cost per launch (see Figure 2-1, \nbelow). \n \nFigure 2-1. R&D Productivity Model: Parametric Sensitivity Analysis from Paul, et \nal., 2010. Reproduced with permission. \n \nNeuroscience is a particularly resource-strained area of drug development. More \nmoney is spent for CNS drugs than any other therapeutic area, but only 8% of these drugs \n"}, {"page": 24, "text": "10 \n \neven make it to clinical trials, about half the rate of other indication categories (Berger, et \nal., 2013). Despite enormous unmet need, some prominent CNS disease indications have \nno viable drug treatments available (Zhu, 2021). Some reasons for this difficulty include \nthe sensitivity of the nervous system to toxicity and the presence of the blood brain \nbarrier, which prevents 95% of potential neuromodulatory molecules from being viable \ndrug candidates (Dong, 2018). \n \n2.2 Overview of Current Approaches to pTRS Estimation \n \n \nVarious methods have been developed by which companies may attempt to \nestimate the probabilities of technical and regulatory success for their products. These \ninclude: \n1. Industry or internal benchmark pTRS, based on past successes and failures \n2. Expert elicitation, which leverages the opinions of knowledgeable individuals  \n3. Statistical analysis, which can involve many types of information sources \n(Vergetis, 2021) \nEach of these methods will now be elaborated upon further. \nBenchmarks \n \n \nPharmaceutical pTRS benchmarks are the literal percentages of programs that \nsucceed in a given phase. These values may be generated deterministically based upon \npast trial and registration successes and failures, either from the industry as a whole \n(industry benchmarks), or from a company’s internal historical data (an internal or \ncompany benchmark). This can be broken down by phase, clinical indication, or both, so \nlong as sufficient data exists to generate meaningful probabilities. (Hampson et al., 2022) \n"}, {"page": 25, "text": "11 \n \nHowever, benchmarks do not include any specific information or about, or customization \nfor, any one trial or drug candidate. This method is simple and objective, but blind to a \ndrug’s clinical development history in previous trials, as well as the intrinsic properties of \nthe individual molecule. Zhou and Johnson (2018) reviewed eight papers from 2010-2018 \nusing different methodologies to generate benchmarks, resulting in overall success \nestimates ranging from 8.3% to 13.4%. \n \n \nWhile many benchmark approaches examine a phase-by-phase approach, which \nestimates the probability of phase success from a random sampling of phase transitions, \nWong, et al. (2019) were able to implement a path-by-path approach that “traces the \nproportion of development paths that make it from one phase to the next.” By composing \ntrials as chains of distinct development pathways and inferring terminations from lack of \nsuccessive trials, they were able to estimate aggregate success rates for the over 400,000 \ntrials available at the time of publication. While this method does not involve manual \ncuration of study results to develop these aggregates, it does eliminate the selection bias \ninvolved in picking a sampling of trials from which an aggregate estimate could be made \n(the phase-by-phase approach). \nExpert Elicitation \n \n \nTo build greater sophistication and specificity into the pTRS estimation process, \nsome researchers turned to expert elicitation, which involves recruiting and questioning \nsubject matter experts (SMEs) about the merits of the therapeutics under examination. Eli \nLilly and Company was one of the early adopters of this method, and has used an \nindependent review board develop pTRS estimates since 1997. (Andersen, 2012). In \n2018, Dallow, et al., of GlaxoSmithKline, implemented a structured Bayesian approach \n"}, {"page": 26, "text": "12 \n \nfor this known as the SHeffield ELicitation Framework (SHELF) (Dallow et al., 2018). \nThis approach had been originally developed at the University of Sheffield for microbial \nrisk assessment (Oakley & O'Hagan, 2022). The broader approach can be summarized as \nfollows: Experts are selected and trained on the process before being provided with an \nevidence dossier detailing relevant information about subject at hand. These experts then \nindividually answer questions about their beliefs about one or more uncertain quantities \n(for example, expected drug safety profile), and responses are used to generate a \nprobability distribution representing the experts’ beliefs and uncertainties about those \nbeliefs. SMEs are then allowed to share and discuss their responses, and ideally, come to \nsome group consensus. \n \n \nOther firms in the pharmaceutical industry, such as Novartis, now also use a \nsimilar SHELF-based approach (Hampson, et al., 2021; Holzhauer, et al., 2022). Benefits \nof this approach include transparency of calculation, the ability for SMEs to adjust their \nimpression of a drug based on clinical study characteristics, and the ability to leverage the \ncollective depth of expertise within the SME group (Dallow et al., 2018). But the use of \nhuman subjects also inherently introduces bias, including over-optimism among opinion \nleaders, or who may misunderstand the probabilities being asked of them.  \n \n \nIn addition to the SHELF method, another means of deriving expert elicitations \ninclude Cooke’s method, which aggregates the probability density functions from each \nexpert opinion into a single one, while the Delphi method utilizes an interactive survey \napproach to enable structured interactions to promote the integration of new information \nfrom expert peers (European Food Safety Authority, 2014). However, each of the \nelicitation methods described here are hampered by the fact that significant time must be \n"}, {"page": 27, "text": "13 \n \nsiphoned from busy, valuable members of organizations to enable these interactions. That \ntime investment, as well as the biases of experts, have led many to look for faster, more \nobjective means of pTRS estimation. \nStatistical Methods \n \n \nStatistical methods for PTRS estimation can take on many forms. For purposes of \nthis review, these methods will include any attempt to use objective data to improve \npTRS estimation, beyond what would be provided by a simple benchmark. These tend to \nfall into two categories, big data approaches (examining large datasets for attributes that \ncorrelate with success) and mechanistic approaches, which use the biophysics of the \ndrug’s interaction with biological systems to predict efficacy, toxicity, etc. \n \n \nAttribute analysis of the drug molecule and/or target are correlated to the success \nof drugs of the past to generate predictions of pTRS. Yamaguchi, et al. (2020) focused on \ndrug target (enzyme, receptor, etc.), action (agonist, antagonist, etc.) modality \n(monoclonal antibody, small molecule, etc.), and application (cardiovascular, nervous \nsystem, etc.) for 3999 molecules. They were able to determine success rates within each \ncategory, enabling the construction of multivariate predictive models of pTRS based on \nthese factors. These factors, and others, can then be used to generated more specific, \ntailored industry benchmarks (Hampson, et al., 2022). Big data analysis such as this has \nthe ability to leverage decades of real-world clinical results, but is sensitive to incorrect \nor misattributed information within that data. \n \n \nWith the advancement of artificial intelligence and in-silico quantitative \npharmacology approaches, it is also possible to directly interrogate and predict the \nefficacy and off-target effects of novel compounds based on molecular physics (VeriSIM \n"}, {"page": 28, "text": "14 \n \nLife, 2024). Some evidence exists that this may give rise to improved success predictions. \nThese mechanistic approaches do not rely on existing (potentially flawed) datasets, and \nare more generalizable in that can make predictions for highly novel interventional \napproaches (Geerts, et al., 2018). \n \n2.3 NLP Models and LLMs \nNLP Models \n \n \nRecent developments in natural language processing (NLP) and large language \nmodels (LLM) have expanded the capability to ingest and utilize large text-based data \nsets. NLP can be divided into the areas of NLU (natural language understanding) and \nNLG (natural language generation) (Khurana et al., 2023). Highly functional NLU \nrequires that a system be able to extract the morphology, syntax, semantics, and context \nof textual data up to a sentence long. Beyond one sentence in length is known as \ndiscourse, the level at which NLP deals with longer segments of text.  \n \n \nA significant problem in NLP is known as the “curse of dimensionality” – the fact \nthat the sequence of tokens upon which a model will be tested will usually not be \nincluded in its training set. (Pappas & Meyer, 2012). Neural language modeling (NLM) \nwas proposed in the early 2000s as a way to predict a succeeding word based on a \nprobabilistic model taking into account the previous n words (Bengio et al., 2003). \nDespite these levels at which NLP can break down text to constituent elements to extract \nmeaning, ambiguity of meaning can still exist, a problem tackled by Gao and others in \nthe 2010s (Gao et al., 2015; Umber & Bajwa, 2011).  \nLLMs \n"}, {"page": 29, "text": "15 \n \n \n \nIn recent years, large language models (LLMs) have taken on a rapidly expanding \nrole in the NLP ecosystem. According to Naveed, et al. (2024), the number of published \npapers containing the term “large language model” has increased from only 60 in 2019 to \nover 28,000 as of 2024.  \n \n \nMuch of this progress has been related to the development of transformers, \nunderlying algorithms that power many modern LLMs (Luitse & Denkena, 2021; \nVaswani et al., 2017). Raiaan et al., (2024) state that, “The transformer model has had a \nsignificant impact on the field of NLP and has played a crucial role in the development of \nlanguage models such as Bidirectional Encoder Representations from Transformers \n(BERT).” The pre-trained BERT model can easily be tuned to accomplish a range of \ntasks without requiring changes to its core architecture (Devlin, et al., 2019). The \nbidirectionality of this transformer-based encoder means that it can take into account \ntokens both before and after a word in order to account for semantics (Li et al., 2022). \nLLMs have also proven to be useful tools for Biomedical Natural Language Processing \n(BioNLP) applications, which require the ability to process nuanced and often obscure \nmedical text (Chen et al., 2023). Because of their power and ease of deployment, the \nBERT family of LLMs will be used for the work described in this praxis. \n \n2.4 AI-based Methods for Clinical Trials \n \nPharmaceutical firms employ a range of technical functions to execute the design \nand development of clinical trials, a task which implicitly requires examination of steps \nto improve the risk profile of clinical trials for candidate drugs. These areas include \nClinical Development and Operations, Regulatory Affairs, Medical Affairs, Biostatistics, \n"}, {"page": 30, "text": "16 \n \nand Program & Portfolio Analytics. Each of these groups can influence clinical trial \ndesign, and thus, clinical risk profile, and artificial intelligence has shown promise in \ndelivering value for many of these functions. \nTable 2-1: Areas of Machine Learning Contribution to Clinical Research. Adapted \nfrom Weissler, et al. (2021). \nClinical Trials and Observational Research \nPretrial Planning \nParticipant Management \nData Management \nProtocol Development \nCohort Selection \nAutomate Data Collection \nDrug Regimen Selection \nPatient Identification \nMonitor Data Quality \nSite Selection \nParticipant Retention \nAdjudicate Outcome \nEvents \n \n \nAnalyze large, highly \ndimensional, or sparse \ndatasets \n \n \nUnlock novel biological \nfeatures \n \n \nAs shown in Table 2-1, above, there are a number of ways in which machine \nlearning can aid the clinical development process. In particular, significant strides have \nbeen made in using ML to aid in the design and execution of clinical trials (Kolluri et al., \n2022). Li, et al. (2015) developed a dose-response framework that could predict \nclinically-relevant translational biomarkers, a critical asset that development teams need \nto demonstrate efficacy. Li, et al. (2020) and Liu, et al. (2020) were able to use Bayesian \nlearning to construct models to aid design of both adaptive and modified toxicity \nprobability interval (mTPI) dose finding studies, respectively.  \n \nStrides have also been made in clinical trial execution, a critical step in getting \ntherapies to market in a timely manner. Beck, et al. (2020) developed an AI-based tool to \nhelp determine patient eligibility, an otherwise cumbersome and manual process, by \n"}, {"page": 31, "text": "17 \n \nscreening patient medical records. In 2021, Haddad, et al., evaluated an AI-based CDSS \n(clinical decision support system) and found an overall accuracy of 87.6% in screening \npatients for a cancer trial. Analysis of imaging also plays a role, as researchers were \nrecently able to develop AI-based tools to identify patients potentially eligible for clinical \ntrials based on their OCT (optical coherence tomography) retinal scans (Williamson, et \nal., 2024). \n \n2.5 AI-based Methods of pTRS Estimation \nThis praxis approaches the problem of pTRS estimation by leveraging large \ndatasets, employing natural language processing. This section will describe the work that \nhas been done in this area, and the existing gaps. \nApproaches to estimating pTRS can be developed either as chemical component \nmodels (for example, predicting efficacy or safety based on molecular structure), or as \ncomposite models that incorporate text and other elements to predict pTRS directly.  \nComponent Models and Techniques \nSeveral estimable properties of molecules themselves are known significantly \naffect pTRS, especially safety and efficacy. Gayvert et al. (2016) developed an approach \ncalled PrOCTOR that uses a compound’s structure and molecular targets, identifies \nproperties that could give rise to safety concerns, and used this information to the \nprobability of such safety concerns preventing drug candidate success. Their model \nachieved an ROC (receiver operator curve) AUC of 0.826. \n"}, {"page": 32, "text": "18 \n \nSimilar approaches have been taken on the efficacy side of the analysis. \nComputational approaches to understanding structure-activity relationships have been \ngoing on for decades and have greatly accelerated as a result of recent developments in \nartificial intelligence. In 2015, Wallach, et al., developed AtomNet, a deep convolutional \nneural network to predict biological activity of drug molecules. Murali, et al., (2022) used \nan ML-based approach to predict pTRS based on predicted biological activity of drug \ncandidates. Other work further developed these techniques, which are now used as tools \nto help design drugs with improved pTRS (Staszak et al., 2022). Approaches such as \nthese are also used in many composite pTRS prediction tools, which use structural data \nalong with other inputs to inform pTRS estimates. \nText-Based and Composite pTRS Models and Techniques \n \nComposite models are built to generate an overall prediction of success for a \ncandidate drug, be it for a single trial or phase, or its overall development. These \napproaches may utilize clinical trial descriptions, trial design parameters, and/or \nstructural/molecular data, to generate predictions for success. Such models that utilize \nAI/ML/NLP will be described here. \n \nA significant foray into this approach was accomplished by Lo, et al. (2017), \nusing 6344 drugs, derived from the databases Pharmaprojects and Trialtrove. 31 drug \nattributes and 113 clinical trial characteristics were used as features for ML-based \nprediction using R. Their analysis resulted in a ROC-AUC of 0.78 for Phase 2 and 0.81 \nfor Phase 3 trials. While the data was limited in time frame (1990-2015) and excluded \nPhase 1 prediction, the ability of this algorithm to outperform complete-case analysis was \nnoteworthy. \n"}, {"page": 33, "text": "19 \n \n \n Subsequent ML-based approaches utilized different combinations of source data \nand core methodology. Feijoo, et al. (2020) used a combination of ClinicalTrials.gov and \nBiomedtracker data to generate a dataset of 6417 individual trials that initiated between \n1993 and 2004. A supervised random forest model was implemented, which led to a \nreported all-disease accuracy, sensitivity, and specificity of (0.743, 0.740, 0.746) for \nPhase 2 trials and (0.672, 0.692, 0.648) for Phase 3 trials. \n \nFu, et al. (2022) introduced HINT (Hierarchical Interaction Network) as a graph \nneural network method for clinical trial outcome prediction. The method uses an \ninteraction graph to connect knowledge-embedding modules containing information on \nthe drug, disease, etc. 9045 total trials were used for training and validation, and tested on \n3420 total trials, achieving ROC-AUC of (.576, .645, and .723) on Phases 1, 2, and 3, \nrespectively. Updated publications on this method are available from Lu, et al. (2024). \n \nMeanwhile, Aliper, et al. (2023) developed inClinico, a multi-modal AI prediction \nplatform designed specifically to predict Phase 2->Phase 3 transitions. 55,653 unique \nPhase 2 clinical trials were used in the proprietary training set and 2849 trials were used \nfor validation, achieving a robust ROC-AUC of 0.88 and 79% accuracy for real-world \ntrials that have read out. \n \nReinisch, et al. (2024) created CTP-LLM, a GPT 3.5-based model designed to \npredict clinical phase transitions. Using a combination of ClinicalTrials.gov and \nBiomedtracker data, they obtained a training data set of 20,000 trials and based on \npublished trial protocols generated five models, with their best model achieving a ROC-\nAUC of 0.667 and an F1 score of 0.665. \n"}, {"page": 34, "text": "20 \n \n \nZheng, et al. (2024) also used a multimodal LLM approach they dubbed LIFTED \nas an approach for clinical trial outcome prediction. Their mixture-of-experts approach \ntransforms modality data into natural language descriptions for LLM models. The \nmixture-of-experts framework can then identify common information patterns across \nmodalities. The composite LIFTED method was able to achieve ROC-AUC scores of \n(.649, .651, .735) for Phases 1, 2, and 3.  \n \nA dissertation by Jung Won Choi in (2024) laid out a comprehensive approach to \nthe use of machine learning for clinical trial analysis, and included some outcome \nprediction. Choi used a random forest model in R to make predictions based on trials in \nwhich only a single country was used for trial sites (n = 83,852). Across all phases, a \nmean ROC-AUC of 0.85 and mean F1 of 0.95 was achieved. \n \nIn order to address the challenges of prediction on non-small molecule entities as \nwell as the problems inherent to graph neural network approaches, Gao et al. (2024) \nintroduced LINT (Language Interaction Network), a method requiring only clinical trial \ntext input. Descriptions, pharmacodynamic and pharmacokinetic data were retrieved from \nDrugBank, and LINT uses a classifier and transformer to generate a learning model. \nDespite tackling only hard-to-predict biologics candidates, the model achieved ROC-\nAUC scores of (.770, .740, .748) in Phases 1, 2, and 3, respectively. \n \n \n \n"}, {"page": 35, "text": "21 \n \nTable 2-2. Comparison of Phase 2 Predictive Performance Across Models in \nLiterature \nPaper \nN \nScope of Training \nData \nPerformance \nLo, et al. (2017) \n6344 drugs \nTrial characteristics \n+ drug attributes \nROC-AUC:  \nPh2 = 0.78; Ph3 = \n0.81 \nFeijoo, et al. (2020) \n6417 trials \nTrial characteristics \n+ Biomedtracker \ndata \nROC-AUC: \nPh2 = 0.76 for \nNeuro \nGao, et al. (2024) \n23,519 trials \nTrial characteristics \n+ drug attributes \nROC-AUC: \nPh2 = 0.74 \nFu, et al. (2022) \n9045 trials \nTrial \ncharacteristics, \nsome drug \nattributes \nROC-AUC:  \nPh2 = 0.65 \nAliper, et al. (2023) \n55,653 trials \nMultimodal \nROC-AUC: \nPh2 = 0.88 \nReinisch, et al. \n(2024) \n20,000 trials \nTrial characteristics \n+ Biomedtracker \ndata \nPh2 Accuracy = \n0.604; F1 = 0.600 \nZheng, et al. (2024) \n17,538 trials \nTrial characteristics \n+ drug attributes \nROC-AUC: \nPh2 = 0.651 \n \n \nGao, et al. (2024) also released a paper introducing CTO – the Clinical Trial \nOutcomes dataset. Much of the limitation in generating large training sets using the \ncomplete index of trials on ClinicalTrials.gov came from the lack of a single source of \ntruth regarding the actual success/failure outcomes of those trials. Manual curation by \nsome companies has yielded proprietary datasets comprising only a small (and possibly \nnonrepresentative) fraction of those trials. Gao and colleagues were able to develop a \nmachine learning model using publications, news reports, trial linking, etc., to generate a \ndataset that predicts with remarkable accuracy whether a completed trial was successful \nor not. The random forest model they created has a 91% overall accuracy, sufficient for \n"}, {"page": 36, "text": "22 \n \nuse in validating clinical trial predictive models. Most importantly, its predictions are \navailable for nearly all interventional trials contained within ClinicalTrials.gov, \nrepresenting an enormous leap in scope for predictive validation. The CTO Database will \nbe used as the source of truth for trial successes and failures in model validation for this \npraxis. \n \n2.6 Prediction Validation Methods \n \nTrained NLP-based models require formal evaluation before they can be trusted \nfor implementation. Because different metrics focus on different aspects of performance, \nseveral should be used in order to understand the strengths and weaknesses of a model, \nunderstand its performance relative to existing models, and identify areas for \nimprovement.  \nThis praxis concerns itself with the estimation of the probability of success for \nprogram phases. This could be validated using binary predictions of clinical trial \nsuccesses and failures and evaluated relative to the true outcomes. However, it may be \nmore prudent for the model to generate a pTRS value for each clinical trial and evaluate \nthose continuous values relative to the binary outcomes. Both general approaches to \nvalidation will be described here. \nBinary Prediction Validation Methods \n \nBinary classification results in binary predictions, and is often used when an \noutcome is all-or-nothing, such as detecting disease, patient survival, etc. The true state \n"}, {"page": 37, "text": "23 \n \nof whether an event occurs is classified as positive or negative, while the prediction may \nbe classified as true or false. \n \nThe most commonly applied metrics for binary classification models are \naccuracy, precision, recall, and F1 score (Naidu et al., 2023). These are derived from the \nvalues contained within the confusion matrix of true positive (TP), true negative (TN), \nfalse positive (FP) and false negative (FN) (Hossain, et al., 2023).  \nAccuracy is the proportion of all predictions that were true (Equation 1): \n𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦=\n்௉ା்ே\n்௉ା்ேାி௉ାிே                                            (1) \nPrecision is the proportion of all positive predictions that were true (Equation 2): \n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛=\n்௉\n்௉ାி௉                                                   (2) \nRecall is the proportion of true predictions out of all of the positive events (Equation 3): \n𝑅𝑒𝑐𝑎𝑙𝑙=\n்௉\n்௉ା\n                                                     (3) \nF1 Score is the harmonic mean of Precision and Recall (Equation 4; Hand et al., 2021):  \n𝐹1 = 2 ∗\n௉௥௘௖௜௦௜௢௡∗ோ௘௖௔௟௟\n௉௥௘௖௜௦௜௢௡ାோ௘௖௔௟௟                                            (4) \n \nBalanced accuracy (sometimes referred to as the “index of balanced accuracy”) is \nanother means by which model and prediction accuracy can be gauged, which can \naccount for imbalances in data. A balanced accuracy score of 0.50 represents that a model \nis no better than random guessing. (García et al., 2009) \n"}, {"page": 38, "text": "24 \n \nAnother useful method in model development is K-fold cross validation, which \ncan prevent overfitting. This technique requires that a model be tested on k different \nsubsets, or folds, of the test data, and trained on the rest (Wilimitis & Walsh, 2023). By \npreventing overfitting, the researcher may reduce the chance of a model performing well \non training data, but being unable to make accurate predictions when presented with new \ndata (Hossain, et al., 2023). K-fold cross validation may be useful for either binary or \ncontinuous classification schemes. \nContinuous prediction validation methods \nReceiver operating characteristic (ROC) curves plot true positive rate against false \npositive rate across a series of thresholds (Nahm, 2022). The area under the curve (ROC-\nAUC) can then be measured as a way of characterizing the relative predictive \nperformance of the model. \nPrecision-Recall curves are another tool for understanding the interplay between \nvarious elements of the confusion matrix. Specifically, PR-AUC can be very valuable \nwhen there is a stark imbalance in data, for example if positive events are rare, for \nexample in fraud detection (Cook & Ramadas, 2020). Because the outcome data in this \npraxis is imbalanced, PR-AUC will be examined.  \nCohen’s kappa is a metric that can be used to measure agreement between two \nsets of predictions, but is able to account for prediction agreement that would result from \nrandom chance (Grandini et al., 2020). \nBrier Score measures the mean squared error between predictions and outcomes \n(Zhu, et al., 2024). It is an overall accuracy metric, and thus can evaluate both \n"}, {"page": 39, "text": "25 \n \ndiscrimination (discerning the positive and negative) and calibration (probabilities vs. \nactual event frequencies). \nLog Loss Score attempts to measure the distance between a predicted distribution \nand its true distribution, and penalizes incorrect classifications (Aggarwal et al., 2020). \n \n2.7 Feature Importance Analysis \n \nA common challenge with machine learning models is understanding why an \naccurate model is making the predictions that it makes. Tools and techniques are \navailable to allow NLP models to be more transparent, by lending insight into the \ncharacteristics that lead to certain categories of predictions. \n \nIn the work described in Chapter 3, a technique called SHAP (SHapely Additive \nexPlanations) is implemented to improve understanding of the reasons for model \npredictions. This allows for the identification of features that most strongly impact the \nmodel’s predictions, as well as a measure of the magnitude and direction of that \ninfluence, increasing or decreasing a predicted pTRS, in the case of this work (Mosca et \nal., 2022). \n \n2.8 Summary and Conclusion \n \n \n Pharmaceutical research and development involves enormous capital expenditure \nand risk, so appropriate identification of a program’s likelihood of achieving technical \nand regulatory success is of paramount importance. Not only does pursuing low-viability \ndrug candidates destroy value and expose trial participants to potentially toxic or non-\n"}, {"page": 40, "text": "26 \n \nefficacious treatments, there is also considerable opportunity cost, as companies must \nforego other potentially promising therapies. Because of this importance, efforts for \npredicting trial outcomes has been of keen interest in both industry and academia. \n \n \nApproaches that firms currently employ range from having no formal system of \nestimating pTRS at all, to using benchmarks alone, benchmarks predicated on indication \nor other factors, expert elicitation to develop more bespoke risk profiles, or any \ncombination thereof. Each of these approaches involves a different level of time, effort, \nand potential reward in terms of accuracy. \n \n \nThe evolution of machine learning, natural language processing, and large \nlanguage models have over time provided a variety of new tools with potential for \nimproving clinical development. Some of this includes machine learning based \napplications to help researchers recruit and select appropriate patients for clinical trials, \nor identify the most appropriate biomarkers for clinical analysis.  \n \n \nSimilar tools have been developed to analyze drug candidates themselves. \nNumerous attempts have been made to develop means for predicting the toxicological \nprofile of new drugs, based on their structure, targets, and known databases of existing \ndrugs. Similar attempts have been made to estimate potential efficacy. While these can \ninform some preclinical and clinical decision making, they do not serve, by themselves, \nto quantify pTRS for a drug candidate. \n \n \nThe latest generation of AI-based methods for pTRS estimation take advantage of \ntools that can help extract insights from enormous amounts of data. Some of these \nmethods are largely numerically-based and utilize feature extraction to define estimates \nbased on any number of properties of a trial or molecule, from patient count, to binding \n"}, {"page": 41, "text": "27 \n \nsite characteristics. Other, more straightforward tools may rely solely upon text and \nutilize NLP/LLM models to derive predictions from subscription or publicly available \ntrial data. \n \n \nA major limiting factor has been the fact that trials listed in major comprehensive \ndatabases such as ClinicalTrials.gov do not contain information on whether the trial was a \n“success” or “failure.” Efforts for manual curation of these trials has resulted in smaller \n(<20,000 study) databases for training and validating predictive models. However, there \nis opportunity for bias, as the trials for which outcomes have been curated may be more \nspecific to a particular time, region, indication, or other property that could skew results. \nRecent work by Gao, et al., to generate robust success assessments for completed clinical \ntrials has afforded success labeling for trials in model training sets. This has made it \npossible to utilize the full canon of clinical information on ClinicalTrials.gov, enabling \nmore powerful LLM-based development of pTRS prediction models based solely on \npublicly available information. \n \n \n  \n \n \n"}, {"page": 42, "text": "28 \n \nChapter 3—Methodology \n \n3.1 Introduction \n \nThis work approaches the problem of pTRS similarly to many of the papers \ndetailed in Section 2.6 – deployment of machine learning as a tool to improve \npredictions. Openly-available ClinicalTrials.gov data for 101,145 clinical trials was used \nto train and test the model, using the success predictions of Gao, et al. (2024) as the \nsource of truth, which enabled use of the entire canon of past trials rather than a small \ncurated fraction. This much larger starting dataset provided a sufficiently large set of \nneurology programs for neurology-specific NLP training.  \n \nThis chapter will provide the step-by-step overview of the methods implemented \nto develop and validate these predictions. Section 3.2 provides information on data \ncollection and format. Section 3.3 provides the preprocessing framework that was \nemployed to prepare data for analysis. Section 3.4 details the non-LLM models. Section \n3.5 details the screening of the different LLM-enabled models. Section 3.6 provides \ninformation on the BioBERT LLM-based final model. Finally, Section 3.7 provides a \ngeneral description of appropriate implementation. \n \n3.2 Data Collection \n \nThere were three primary data components used for this analysis. The \nClinicalTrials.gov data provided the trial information from which the model was built. \nThe Clinical Trial Outcomes Dataset from Gao, et al. (2024) provided the source of truth \nfor the trial outcomes. Finally, clinical benchmarks for overall and neurological study \n"}, {"page": 43, "text": "29 \n \nindications were used as a comparison to determine whether the predictive model was \nsuperior to non-program-specific, naïve benchmarking. \nClinical Trial Data \n \nClinical Trial Data was downloaded directly from ClinicalTrials.gov as a CSV \nfile. From 525,418 initial database entries, a downloadable set of 235,200 after filtering \nfor “Interventional” trials, selecting for only those labeled “Completed” or “Terminated”, \nand selecting only those for which the study completion date was earlier than 1 April \n2024. The date selection was to remove those trials for which the Clinical Trial Outcomes \ndatabase would not have a trial outcome prediction. \n \nThe download itself allows for selection of individual columns that the user wants \nfor the CSV. Columns believed to be irrelevant were unselected to remove noise and size \nfrom the data. The removed columns included “Study URL”, “Acronym”, “Study \nResults”, “Other IDs”, “First Posted”, “Results First Posted”, “Last Update Posted”, and \n“Study Documents”. Remaining columns included “NCT Number”, “Study Title”, \n“Study Status”, “Brief Summary”, “Conditions”, “Interventions”, “Primary Outcome \nMeasures”, “Secondary Outcome Measures”, “Other Outcome Measures”, “Sponsor”, \n“Collaborators”, “Sex”, “Age”, “Phases”, “Enrollment”, “Funder Type”, “Study Type”, \n“Study Design”, “Start Date”, “Primary Completion Date”, “Completion Date”, and \n“Locations.” Note that the model was not trained on “Study Status”, “Start Date”, \n“Primary Completion Date”, or “Completion Date”, as these were used for sorting and \npreprocessing but omitted prior to training. \n \n \n"}, {"page": 44, "text": "30 \n \nClinical Trial Outcomes Database (CTOD) Data \n \nThe Gao paper (2024) provides links to the CTOD data used as the source of truth \nfor clinical trial outcomes. The CSV files containing this data are differentiated by Phase \nnumber, and contain the NCT Number (a clinical trial ID number) for each trial \nevaluated, along with the predictions, both binary and continuous. \nBenchmark Data \n \nTwo benchmarks were generated for model evaluation. The first was data from \nCiteline’s Pharmapremia Pharma Intelligence (n.d.) database, which provides current \ninformation on clinical trial success rates from 2015-2024. This was used as an industry \nbenchmark for Phases 1, 2 and 3. The second was the calculated success rate for Phases \n1-3 in the overall CTOD dataset itself (see Section 3.6 for additional details). \n \n3.3 Data Preprocessing \nA number of steps were undertaken to address missing data, merge predictions \nwith the clinical data, reformat entries, do basic duration calculations, properly assign \nphases, and eliminate trials for which key pieces of information were missing. \nMerging Data \n \nA python script was written to ingest the CTO dataset files (one for each of Phase \n1, Phase 2, and Phase 3) and the exported ClinicalTrials.gov (“CT.gov”) dataset. Since \nboth files differentiate trials by NCT Number, the script merges the predictions with the \n"}, {"page": 45, "text": "31 \n \nrows containing the trial data, and adds new cells within the CSV containing the clinical \ntrial data.  \nDate Processing \nDate Reformatting \n \nDates were not supplied in a uniform format across the CT.gov dataset, so a \nPython script was used to convert the dates to YYYY-MM-DD format, regardless of the \ninput format. If only a month and year were supplied, the date was assumed as the 1st. \nPhase Formatting \n \nCT.gov data comes with phase labels for each row/trial. These include \n“EARLY_PHASE1”, “PHASE1”, “PHASE1|PHASE2”, “PHASE2”, \n“PHASE2|PHASE3”, “PHASE3”, AND “PHASE4”. In keeping with the sorting rules \nused by Gao in the CTO dataset, “EARLY_PHASE1”, “PHASE1”, and \n“PHASE1|PHASE2” were labeled as “PHASE1”, “PHASE2|PHASE3” was labeled \n“PHASE2”, and all trials labeled “PHASE4” were eliminated.  \n \nThese phase assignments were selected in order to sort the trials as accurately as \npossible. Since Phase 1/2 trials still have not discharged Phase 1 risks, they are treated as \nPhase 1 trials still for purposes of pTRS estimation, and likewise with Phase 2/3 trials \nbeing treated as Phase 2. Phase 4 trials are post-approval trials and thus are outside the \nscope of typical pTRS estimation analysis. \nPurging Incomplete Data \n"}, {"page": 46, "text": "32 \n \n \nRows that were missing data for phase, prediction, or completion date were \nremoved from the dataset. After all steps in data preprocessing were completed, 101,145 \nclinical trials remained, spread across the three phases of interest. \nFiltering for Neuroscience Trials \n \nClinicalTrials.gov data does not designate the therapeutic area (e.g., oncology, \ndermatology, etc.) for a given clinical trial. However, in industry, pTRS values are \ndifferentiated not just by phase but also by therapeutic area, and the area of interest for \nthis praxis is neuroscience. This work therefore required that a standalone neuroscience \ntraining/test set be constructed. \n \nA list of indications and disease areas in neuroscience developed through use of \nvarious databases (see Appendix B). The decision was made to be as comprehensive as \npossible, intending to capture all of the possible neuroscience programs while \nacknowledging the possibility that some non-neuroscience indication trials might leak \nthrough the filter. This allows the neuroscience model to train on rare neurological \nconditions, and ensures that the resulting train/test set is large enough to facilitate \nappropriate model validation. \nSome indications may be named a number of different ways (e.g., ‘Alzheimers \nDisease’, ‘Alzheimer’s Disease’, AD, etc.). To remedy this, ChatGPT-4o was used to \nexpand upon the list and amplify the indication terms by including plural and abbreviated \nnames for each, as well as synonyms. Then, a Python scripts was written that filtered \nclinical trials for which the “Conditions” data field contained any of the terms. In total, \n"}, {"page": 47, "text": "33 \n \n32,106 trials met the inclusion criteria for the neuroscience training/test sets, across all 3 \nphases. \nOverall Data Flowchart \n \n \n3.4 Basic (non-LLM) NLP Models \n \nThree non-LLM NLP models were constructed in order to understand whether \naccurate inferences about pTRS could be made based on a relatively simple model \nwithout the use of sophisticated LLMs. These relatively simple Python models were \noperated from the PyCharm IDE console, using an ASUS Vivobook laptop PC with an \nIntel Core i5-1235U processor and 40 GB of RAM.  \nLogistic Regression Model Description \nOverall CT.gov \nDatabase\n•525,418 trials\n•Some non-interventional\nInitial \nDownload\n•235,200 trials\n•Filtered for interventional trials, Phases 1-3 only, prior to 1 April 2024\nMaster File\n•101,145 trials\n•Follows removal of trials missing critical data, or for which there was no CTOD \napproval/failure listed\nWorking Files\n•All indications Phases 1-3\n•Neuroscience only Phases 1-3\n"}, {"page": 48, "text": "34 \n \n \nA Python program was written to generate a logistic regression model. The \nprogram read in the CSV file of interest, trained on clinical trials that completed before \n01 January 2019 and testing on those that completed on or after that date, with the binary \nsuccess predictions from the CTO Database. The LogisticRegression function within the \nScikit-learn machine learning library was used, and metrics included a ROC curve, ROC-\nAUC, confusion matrix, accuracy, precision, recall, F1, balanced accuracy score, average \nprecision score, and Cohen’s kappa. A pilot evaluation run was made for each phase in \norder to generate an optimal decision threshold value for binary prediction (done by \nmaximizing Youden’s J statistic). Then each train/test set (all indications vs. \nneuroscience only, for each of the three phases) was trained and evaluated under that \ndecision threshold. \nRandom Forest Model Description \n \nA Python program was written to generate a logistic random forest model. The \nprogram read in the CSV file of interest, trained on clinical trials that completed before \n01 January 2019 and testing on those that completed on or after that date, with the binary \nsuccess predictions from the CTO Database. The RandomForestClassifier function within \nthe Scikit-learn machine learning library was used, and metrics included a ROC curve, \nROC-AUC, confusion matrix, accuracy, precision, recall, F1, balanced accuracy score, \naverage precision score, and Cohen’s kappa. A pilot evaluation run was made for each \nphase in order to generate an optimal decision threshold value for binary prediction (done \nby maximizing Youden’s J statistic). Then each train/test set (all indications vs. \nneuroscience only, for each of the three phases) was trained and evaluated under that \ndecision threshold. \n"}, {"page": 49, "text": "35 \n \nGradient Boosting Model Description \n \nA Python program was written to generate a gradient boosting model. The \nprogram read in the CSV file of interest, trained on clinical trials that completed before \n01 January 2019 and testing on those that completed on or after that date, with the binary \nsuccess predictions from the CTO Database. The GradientBoostingClassifier function \nwithin the Scikit-learn machine learning library was used, and metrics included a ROC \ncurve, ROC-AUC, confusion matrix, accuracy, precision, recall, F1, balanced accuracy \nscore, average precision score, and Cohen’s kappa. A pilot evaluation run was made for \neach phase in order to generate an optimal decision threshold value for binary prediction \n(done by maximizing Youden’s J statistic). Then each train/test set (all indications vs. \nneuroscience only, for each of the three phases) was trained and evaluated under that \ndecision threshold. \nFeature Importance Analysis \n \nIn order to gain a better understanding of the individual training set features that \nthe model considers most important, it was necessary to carry out feature importance \nanalysis. This was done using the gradient boosting model since it was the best \nperforming non-LLM model, and because for LLMs in general, it is considerably more \ndifficult to understand the rationale underlying a given model. \n \nFor this work the 20 most important features in the gradient boosting model were \nextracted and SHAP values were generated to indicate the direction of the influence (i.e., \nwhether a given token increases or decreases predicted probability of success. \nPseudocode for the Gradient Boosting Model \n"}, {"page": 50, "text": "36 \n \n \nThe script loads the training and test datasets from CSV files and performs data \npreprocessing to ensure that all rows and contain valid entries. It filters out rows that are \nmissing the target value (“pred_proba”) and invalid dates, then splits the datasets using a \ncutoff date (pre‑2019 for training, 2019 and onward for testing). All text columns \n(excluding “pred_proba” and the date column) are combined into a single \n“combined_text” field and renamed from “pred_proba” to “label,” converting it to a float \nwhile also producing a binary version (by rounding) for metrics like accuracy that require \nclassification. Next, a TF‑IDF vectorizer is used to transform the combined text into \nnumerical features, which are then used along with the binary labels to train a gradient \nboosting classifier. \nAfter training, the script evaluates model performance on the test set, computing \nvarious metrics such as ROC‑AUC, accuracy, precision, recall, F1 score, and confusion \nmatrix. It then extracts built‑in feature importances from the gradient boosting model and \nlogs the top 20 features. In addition, a SHAP analysis is performed: the test set features \nare converted to a dense float array, and SHAP’s TreeExplainer computes the SHAP \nvalues, which provide a directional/signed contribution for each feature. The script logs \nthe top 20 features by mean SHAP value (indicating whether they tend to increase or \ndecrease the predicted value). Finally, it computes the ROC curve from the predictions, \ncompares the model’s performance with benchmark predictions, calculates the proportion \nof test cases where the model outperforms these benchmarks, and writes all logs to an \noutput. \n \n3.5 LLM-Enabled NLP Model Screening \n"}, {"page": 51, "text": "37 \n \n \nIn order to generate models with greater accuracy, several LLMs from the BERT \n(Bidirectional Encoder Representations from Transformers) family were evaluated. These \nincluded BERT, BioBERT, SciBERT, and PubMedBERT. A single train/test set was \nchosen (Phase 1, Neuroscience-only) and run for only three training epochs due to the \ncomputational intensiveness of training all models on all data sets. \n \n3.6 BioBERT Models \n \nOnce the BioBERT model had been selected, additional steps were taken to \nenhance the predictive power of the model. Additional data became available within the \nCTO dataset that included continuous predictions of success for those trials where it was \nnot definitively clear whether a clinical trial had succeeded or failed. Moving forward, all \nmodel training and evaluation was done using the prediction dataset that included \ncontinuous values, to help produce more nuanced distinguishment of auspicious trials. \n \nIn order to properly validate these continuous predictions, additional metrics were \nincorporated into the evaluation process, specifically log loss and Brier score. These are \nuseful in evaluating comparisons between continuous predictions and true values. The \noriginal binary comparison metrics relating to confusion matrix elements (e.g., ROC-\nAUC, balanced accuracy, F1, etc.) remain. \n \nThere is also value in understanding not just the overall relative accuracy of a \npTRS prediction model, but also the proportion of the time that a model prediction is \nsuperior to a benchmark. This analysis was included as well. \nBenchmark Comparisons \n"}, {"page": 52, "text": "38 \n \nIndustry Benchmarks \n \nTwo benchmarks were generated for this project. The industry benchmark is a \nweighted average of the industry benchmarks for success rates in phases 1, 2, and 3, for \npsychiatry and neurology These were weighted by number of studies used to make the \nbenchmark (the constituent trials were 76% neurology and 24% psychiatry) to generate a \nbenchmark for neuroscience as a whole. Performance superior to this benchmark \nindicates the model predictions are better than simply assigning the published industry \nbenchmarks for each study in each phase of the test set. These benchmarks were \ncompiled by Citeline, Inc., and represent the alternative pTRS that a company would \nhave to calibrate their asset valuations if they relied on benchmarks alone. Predictive \nability superior to the industry benchmark would be considered a minimum \nrequirement for a pTRS prediction model to be viable. \nTable 3-1. Industry Benchmarks (derived from Citeline data) \nPhase 1 \nPhase 2 \nPhase 3 \n48.2% \n29.0% \n50.5% \n \nDataset-specific Benchmarks \n \nDataset-specific benchmarks were also generated. This benchmark represents the \npercentage of successes predicted in the CTO Dataset used for success labeling. For each \nphase in the neuroscience-filtered test sets, the ‘pred_proba’ column was averaged to \ngenerate an average success rate across the specific dataset being used. Performance \nsuperior to this benchmark indicates the model predictions are better than simply \n"}, {"page": 53, "text": "39 \n \nassigning the dataset’s mean phase success probability to each trial. It should be noted \nthat the dataset-specific benchmark also included the outcomes for studies in the test set, \nwhich the model is not trained on, making it significantly more challenging to beat, since \nthe benchmark is predicated on outcome data that is not used to inform the model. This is \na less realistic comparator than the industry benchmark, since this information would not \nbe available to a firm at the time it is developing program analysis and making decisions \non pursuing drug candidates. However, this benchmark was used to further characterize \nthe predictive ability of the model. Predictive ability superior to the dataset benchmark \nwould be considered aspirational for a pTRS prediction model. \nTable 3-2. Dataset-Specific Benchmarks (derived from the predictions of success in \nthe CTO Dataset) \nPhase 1 \nPhase 2 \nPhase 3 \n63.2% \n56.7% \n65.4% \n \nEpoch Control \n \nIn order to prevent over or underfitting, it is important to control the number of \ntraining epochs in each model. This was accomplished with respect to a variable called \ntraining loss. The training script continued to initiate consecutive training epochs until the \ntraining loss had failed to decrease for three consecutive training epochs. At that time, \ntraining stops and the model is evaluated. \nBioBERT Code Description \nThe script first loads training and test data from CSV files and filters them based \non the completion dates (using trials before 2019 for training and trials from 2019 onward \n"}, {"page": 54, "text": "40 \n \nfor testing). To ensure that no invalid entries remain from initial preprocessing, the files \nare preprocessed once again. Samples missing the target column (“pred_proba”) are \ndropped, and dates are converted to proper date types. Next, the script combines all text \ncolumns (except the target and date fields) into a single “combined_text” string for each \nsample. The “pred_proba” column is then renamed to “label” and converted to float (with \na binary label version created via rounding for metrics such as accuracy that require \nbinary values). \nAfter preprocessing, the script initializes a BioBERT model by loading the “dmis-\nlab/biobert-base-cased-v1.1” checkpoint. It trains the model using a training loop that \nminimizes binary cross‑entropy loss with early stopping (if the loss does not improve for \na three epochs). After training the model, the script evaluates the model on both the \ntraining and test sets. Various metrics (ROC‑AUC, balanced accuracy, precision, recall, \nF1 score, etc.) are computed by rounding the continuous labels (except for the final \nmodels in sections 4.5-4.7, which were trained using the continuous labels). The script \nalso compares the model’s predictions to two fixed benchmark values (an “industry \nbenchmark” and a “dataset benchmark”) by creating constant prediction arrays and \ncomputing evaluation metrics as well as the proportion of cases where the model’s \nprediction is closer to the true value than the benchmark’s. Finally, the script outputs the \nROC curve as an image file and logs all the evaluation details to a text file. \n \n3.7 Implementation \n \nIn order to implement the model described in this praxis, the user will need to \nascertain some basic information about the clinical trial(s) in question. For example, if a \n"}, {"page": 55, "text": "41 \n \nuser intends to use the model to predict the probability of Phase 2 success for a program, \nthey will need to implement the model based on the trial(s) that comprise Phase 2 for that \nprogram. \n \nThe model described here is trained on the majority of clinical trial data that \naccompanies descriptions in ClinicalTrials.gov datasets. Therefore, inputs to any \nimplementation will include those types of data, which include the following: “NCT \nNumber”, “Study Title”, “Brief Summary”, “Conditions”, “Interventions”, “Primary \nOutcome Measures”, “Secondary Outcome Measures”, “Other Outcome Measures”, \n“Sponsor”, “Collaborators”, “Sex”, “Age”, “Phases”, “Enrollment”, “Funder Type”, \n“Study Type”, “Study Design”, and “Locations.” \n \nFor trials that have already begun enrolling patients, or which are registered in \nClinicalTrials.gov, acquiring these inputs will not be a particularly difficult. For those for \nwhich this information is not yet available, it may be quite difficult to use this model for \npredictions. Since the model relies on properties of the clinical trial itself, generating \nsurrogate inputs for these fields, for a clinical trial that has yet to be designed, would \nlikely not result in quality predictions. Once all inputs for a single trial are available, they \nshould be loaded into a single row of a CSV file and used as an input for the model, with \ncode implemented to output the prediction to console or append to an unused column in \nthe row. \n \nPractical applications that drug development professionals will have for this \nmodel fall into two primary areas:  \n1. Analysis of internal programs in development  \n"}, {"page": 56, "text": "42 \n \n2. Analysis of external competitor programs  \nThe former is typically the business of internal finance and portfolio management groups, \nwhile the latter tends to be the domain of commercial development experts who wish to \nunderstand an internal asset’s chances of being first-in-class, an early follower, a late \nfollower, etc., as this can have ramifications for revenue projections. Regardless of \napplication, however, the above-described inputs will be needed in order to generate a \nhigh quality pTRS estimate. \n \n \n \n"}, {"page": 57, "text": "43 \n \nChapter 4—Results \n \n4.1 Introduction \n \nThe datasets were preprocessed, and the code scripts were prepared in Python as \ndescribed in Chapter 3. The metrics of interest were then compiled and visualized using \nExcel®. This chapter will describe the results from the following: \n Non-LLM models \no Logistic Regression \no Gradient Boosting \n Feature extraction/SHAP \no Random Forest \n LLM model comparison \no BERT  \no SciBERT  \no BioBERT  \no PubMedBERT \n BioBERT intermediate binary model \n BioBERT final model \no Evaluation against benchmarks \no Comparison to published models \n \n4.2 Overall Results – non-LLM Models \n \nAfter preprocessing train/test data, the logistic regression, random forest, and \ngradient boosting models were built and implemented as described in Chapter 3. The \nresults are shown in Table 4-1 and Figure 4-1, below.  \n \n"}, {"page": 58, "text": "44 \n \nTable 4-1. Performance Metric Means Across All Train/Test Sets  \n \nModels \nROC-\nAUC \nBal'd \nAcc \nPrecision\nRecall  \nF1 \nPR-\nAUC \nLogistic Regression \n0.639 \n0.598 \n0.877 \n0.496 \n0.623 \n0.875 \nRandom Forest \n0.636 \n0.595 \n0.866 \n0.570 \n0.681 \n0.873 \nGradient Boosting \n0.639 \n0.597 \n0.864 \n0.596 \n0.698 \n0.876 \n \n \nFigure 4-1. Comparisons of Mean Performance for each non-LLM Model Type \n \n \nEach of the three models achieved scores above 0.50 in balanced accuracy, ROC-\nAUC, and PR-AUC, indicating that they performed better than would be expected from \nrandom guessing. This also appears to satisfy Hypothesis 1 (Chapter 1). The logistic \nregression model performed marginally better in balanced accuracy and precision, while \nthe gradient boosting model demonstrated significantly better performance in recall \n(0.596, vs. 0.496 for logistic regression) and F1 (0.698, vs. 0.623 for logistic regression), \nand marginally better in PR-AUC. Whereas both ROC-AUC (for which both gradient \n0.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\n1.000\nROC-AUC\nBal'd Acc\nPrecision\nRecall\nF1\nPR-AUC\nComparisons Across All Non-LLM Model Types\nLogistic Regression\nRandom Forest\nGradient Boosting\n"}, {"page": 59, "text": "45 \n \nboosting and logistic regression scored a 0.639) is a more common metric used in \npredictive model evaluation, PR-AUC is better at addressing the issue of imbalanced \ndatasets. While these results do not indicate overwhelming superiority for the gradient \nboosting model, it does appear as though it was the better of the three. Because of this, \nthe gradient boosting model was selected for further interrogation using feature \nextraction. \nGradient Boosting Feature Extraction \n \nThe purpose of feature extraction is to define the input parameters or tokens \n(features) which are most impactful for model prediction, and quantify that impact \nthrough a SHAP value, as explained in Chapter 2. \n \nFor Phase 1, all SHAP values were negative, indicating that each of the 20 most \nprominent features were associated with a reduction in the predicted pTRS for the \nprogram. This indicates that penalizing trial descriptions for key elements may be a more \neffective means by which to make good predictions, as opposed to rewarding them for \nmore auspicious elements. \n \nAmong the most impactful terms were ‘survival,’ perhaps because trials in the test \nset with descriptions that included survival as a clinical endpoint were less likely to be \nsuccessful. “Cancer” was another inauspicious term, as well as “ctcae.” CTCAE is a \ngrading system used to evaluate adverse events in cancer interventions, and thus bears \ntight correlation with the “cancer” token (Trotti, et al., 2003). A diagram showing the top \n20 impactful tokens and their SHAP values can be seen in Figure 4-2. \n"}, {"page": 60, "text": "46 \n \n \nFigure 4-2. SHAP Values for Phase 1 \n \n \nFigure 4-3, below, shows the feature contributions for the 20 most impactful \ntokens in Phase 2. In this case, one term, “terminated,” was found to have a positive \ncorrelation with a higher success prediction. “Free survival,” the most inauspicious token \nfor Phase 2, is most likely a fragment of the parent term “progression-free survival,” a \ncommon clinical endpoint which refers to the length of time that a patient lives with a \n"}, {"page": 61, "text": "47 \n \ndisease but it does not progress (Gyawali, et al., 2022). This seems to indicate that trials \nfor which this endpoint is important may be correlated with lower probabilities of success \n(at least, in this model).  \n“Allocation intervention” appears to be referring to how trial participants are \nallocated into different interventional groupings, while “mechanical ventilation” could be \na significant negative factor because of the number of COVID-19 treatment candidates \nthat were in development, many unsuccessfully, from 2020 to 2024.  \n"}, {"page": 62, "text": "48 \n \n \nFigure 4-3. SHAP Values for Phase 2 \n \n \n \nThe results of the feature extraction analysis for Phase 3 are shown in Figure 4-4. \nHere, we see that six of the 20 most prominent tokens carry positive values, indicating \nthat they are correlated with a higher predicted pTRS. Several of these, such as “pfizer \ninvestigational,” “vaccine,” and “vaccination,” may be related to COVID-19 vaccines, \nseveral of which were successfully approved and launched. \n"}, {"page": 63, "text": "49 \n \n \nInterestingly, “progression” is listed as a token correlated with lower pTRS \npredictions, potentially because of its use in the phrase “progression free survival,” as \nseen in the Phase 2 feature extraction. This would indicate that this term is potentially a \npowerful negative predictor across multiple phases. More of the tokens in Phase 3 appear \nto be associated with locations in which studies were conducted, perhaps due to \nsignificant historical differences in study outcomes in these locations.  \n \n \nFigure 4-4. SHAP Values for Phase 3 \n"}, {"page": 64, "text": "50 \n \n \nComparison of All Indication vs. Neuroscience Only Train/Test Sets \n \nOne goal for this praxis was to evaluate the validity of Hypothesis 3, which \npertains to which training set is better to use, one containing trial descriptions for all \nindications, or one containing only those for neuroscience indications. This is difficult to \nsurmise intuitively, since the ‘all indication’ dataset contains more trials to train on, but \nthe neuroscience-only dataset contains training data that may be more concentrated in the \nmost pertinent features for pTRS prediction. \n \nAs the data in Table 4-2 and Figure 4-5 demonstrate, when averaging across all \nphases and models, the use of all trial indications to train the model results in better \nperformance metrics across the board, and particularly for recall and F1. This appears to \nindicate that the additional trials available result in a more robust overall model, even if \nthe training set is no longer specific to the neuroscience therapeutic area. \nTable 4-2. Means of non-LLM Model Metrics by Training/Test Sets \nTraining Set \nROC-\nAUC \nBal'd \nAcc \nPrecision\nRecall  \nF1 \nPR-\nAUC \nAll Indications Mean \n0.641 \n0.600 \n0.875 \n0.592 \n0.698 \n0.882 \nNeuroscience Only \nMean \n0.635 \n0.593 \n0.863 \n0.516 \n0.637 \n0.868 \n \n"}, {"page": 65, "text": "51 \n \n \nFigure 4-5. Performance Metrics for All Indications and Neuroscience-only \nTraining Sets \n \nComparisons by Phase \n \nAnother important question is how applicable the predictive model is in different \nphases. Each phase has different criteria for success, and a model that is predictive in \nonly some of those phases will have a significant weakness. As shown in Table 4-3 and \nFigure 4-6, the model fared slightly better in predicting the success of Phase 1 trials than \nPhase 3 trials, but both of those were far better than the predictions for Phase 2 trials. In \nfact, it appears that Phase 2 predictive validity is enough of an outlier to be dragging \ndown the means across the board. \nTable 4-3. Non-LLM Performance Metrics by Phase \nPhase \nROC-\nAUC \nBal'd \nAcc \nPrecision\nRecall  \nF1 \nPR-\nAUC \nPhase 1 \n0.662 \n0.613 \n0.886 \n0.668 \n0.760 \n0.899 \nPhase 2 \n0.595 \n0.564 \n0.846 \n0.384 \n0.525 \n0.838 \nPhase 3 \n0.657 \n0.614 \n0.874 \n0.610 \n0.717 \n0.886 \n \n0.000\n0.200\n0.400\n0.600\n0.800\n1.000\nROC-AUC\nBal'd Acc\nPrecision\nRecall\nF1\nPR-AUC\nComparisons of All-Indication vs. \nNeuroscience-only Training\nAll Indications Mean\nNeuroscience Only Mean\n"}, {"page": 66, "text": "52 \n \n \nFigure 4-6. Non-LLM Performance Metrics by Phase \nAdditional data and figures from the non-LLM models can be found in Appendix A \n \n4.3 LLM Transformer Comparison \n \nThe next step in this work was to determine whether LLM/transformer-based \napproaches could surpass the performance of the non-LLM models. The first step was to \nidentify the most suitable LLM/transformer model to use. Table 4-4 and Figure 4-7 show \nthe performance metrics for an initial evaluation in which four related but different LLMs \nwere evaluated: BERT, BioBERT, SciBERT, and PubMedBERT.  \nTable 4-4. LLM Performance Metrics by Transformer \nROC-\nAUC \nBal'd \nAcc \nPrecision\nRecall \nF1 \nPR-\nAUC \nBERT \n0.595 \n0.537 \n0.833 \n0.860 \n0.846 \n0.871 \nBioBERT \n0.604 \n0.530 \n0.835 \n0.908 \n0.87 \n0.87 \nSciBERT \n0.637 \n0.554 \n0.839 \n0.868 \n0.853 \n0.9 \nPubMedBERT\n0.628 \n0.537 \n0.833 \n0.868 \n0.85 \n0.885 \n \n0.000\n0.200\n0.400\n0.600\n0.800\n1.000\nROC-AUC\nBal'd Acc\nPrecision\nRecall\nF1\nPR-AUC\nComparison of non-LLM Model Performance \nby Phase\nPhase 1\nPhase 2\nPhase 3\n"}, {"page": 67, "text": "53 \n \n \nFigure 4-7. LLM Performance Metrics by Transformer \n \nAs shown, BioBERT and SciBERT each performed well in these metrics. \nUltimately, because BioBERT was trained specifically on biomedical corpora (narrower \nin scope and better aligned with clinical trials than SciBERT), BioBERT was selected as \nthe LLM to move forward with. All models in the succeeding sections were built using \nBioBERT. \n \n4.4 BioBERT Initial Model Performance \n \nInitially, the BioBERT models were built by training and testing on the entire \ndataset, for all indications, for each phase. The models were then trained and tested again \nusing the smaller, neuroscience-only datasets. As shown in Table 4-5 and Figure 4-8, the \nall-phase means of ‘all indication’ train/test models outperformed the ‘neuroscience only’ \ntrain test models in every metric except for recall (which was quite close). \n \n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\nROC-AUC\nBal'd Acc\nPrecision\nRecall\nF1\nPR-AUC\nComparison of Preliminary LLM Models - 15 \nepochs each (evaluated using Ph1 data set)\nBERT\nBioBERT\nSciBERT\nPubMedBERT\n"}, {"page": 68, "text": "54 \n \n \nTable 4-5. BioBERT Initial Model Performance \nModel \nROC-\nAUC \nBal'd \nAcc \nPrecision\nRecall  \nF1 \nPR-\nAUC \nAll Indications Phase 1 0.743 \n0.559 \n0.864 \n0.905 \n0.884\n0.94 \nAll Indications Phase 2 0.671 \n0.554 \n0.811 \n0.847 \n0.829\n0.894 \nAll Indications Phase 3 0.707 \n0.583 \n0.855 \n0.872 \n0.864\n0.923 \nNeuroscience Only \nPhase 1 \n0.604 \n0.531 \n0.835 \n0.908 \n0.870\n0.870 \nNeuroscience Only \nPhase 2 \n0.592 \n0.550 \n0.790 \n0.743 \n0.766\n0.819 \nNeuroscience Only \nPhase 3 \n0.639 \n0.567 \n0.824 \n0.822 \n0.823\n0.869 \nOverall Mean \n0.659 \n0.557 \n0.830 \n0.850 \n0.839\n0.886 \n \n"}, {"page": 69, "text": "55 \n \n \nFigure 4-8. BioBERT Initial Model Performance \nThe next step was to compare the all indication and neuroscience-only model \nperformance means to models which were trained on all clinical trials but tested only on \nneuroscience. This comparison, shown in Table 4-6 and Figure 4-9, shows that the all-\nphase mean of the “Train All Test Neuro” combination was superior to the other model \nmeans in most metrics, and very close in precision and PR-AUC.  \nTable 4-6. BioBERT Model Performance Metrics by Training/Test Set \nTrain/Test Set \nROC-\nAUC \nBal'd \nAcc \nPrecision\nRecall  \nF1 \nPR-\nAUC \nAll Indications \n0.707 \n0.565 \n0.843 \n0.875 \n0.859\n0.919 \nNeuroscience Only  \n0.612 \n0.549 \n0.816 \n0.824 \n0.820\n0.853 \nTrain All Test Neuro \n0.740 \n0.573 \n0.832 \n0.900 \n0.864\n0.904 \n \n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nROC-AUC\nBal'd Acc\nPrecision\nRecall\nF1\nPR-AUC\nBioBERT Initial Model Performance\nAll Indications Phase 1\nAll Indications Phase 2\nAll Indications Phase 3\nNeuroscience Only Phase 1\nNeuroscience Only Phase 2\nNeuroscience Only Phase 3\n"}, {"page": 70, "text": "56 \n \n \nFigure 4-9. BioBERT Binary Model Performance Metrics by Training/Test Set  \n \n4.5 BioBERT Final Model Performance \nAll final models were evaluated using the BioBERT LLM, with models trained on \nall clinical trials in the dataset and tested on only neuroscience trials. As described in \nSection 3.6 of the Methodology, the final models used continuous probability labels from \nthe CTO Dataset, rather than rounding to a binary label. The Phase 1-3 breakdown is \nshown below in Table 4-7 and Figure 4-10. Phase 1 shows significantly superior \nperformance metrics to phases 2 and 3 in ROC-AUC, balanced accuracy, and PR-AUC, \nsimilar to the trend that was seen in the non-LLM models.  \nFigure 4-11 shows the ROC-AUC curves which correspond to the scores in Table \n4-7. Here, the dashed lines indicate model performance akin to guessing, and greater area \nunder the black curve corresponds to a model that is both sensitive enough to capture true \npositives and specific enough to avoid false positives.  \n0.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\n1.000\nROC-AUC\nBal'd Acc\nPrecision\nRecall\nF1\nPR-AUC\nComparison of Train/Test Sets for BioBERT Model\nAll Indications\nNeuroscience Only\nTrain All Test Neuro\n"}, {"page": 71, "text": "57 \n \nTable 4-7. BioBERT Final Model Performance Metrics \nModel \nROC-\nAUC \nBal'd \nAcc \nPrecision \nRecall F1 \nPR-\nAUC \nTrain All Test Neuro \nPhase 1 \n0.843 \n0.601 \n0.840 \n0.900 \n0.869 \n0.949 \nTrain All Test Neuro \nPhase 2 \n0.733 \n0.597 \n0.815 \n0.829 \n0.822 \n0.884 \nTrain All Test Neuro \nPhase 3 \n0.644 \n0.520 \n0.840 \n0.970 \n0.900 \n0.879 \nMean Across Phases \n0.740 \n0.573 \n0.832 \n0.900 \n0.864 \n0.904 \n \n \nFigure 4-10. BioBERT Final Model Performance Metrics \n0.5\n0.6\n0.7\n0.8\n0.9\n1\nROC-AUC\nBal'd Acc\nPrecision\nRecall\nF1\nPR-AUC\nBioBERT Final Model \nTraditional Performance Metrics Across Phases\nTrain All Test Neuro Phase 1\nTrain All Test Neuro Phase 2\nTrain All Test Neuro Phase 3\n"}, {"page": 72, "text": "58 \n \n \n \nFigure 4-11. ROC Curves for BioBERT Model Predictions \nTop Left: Phase 1. Top Right: Phase 2. Bottom Left: Phase 3. \n \nAdditional Metrics of interest for comparing the distance between predicted \n(model) and actual (label) values are log loss and Brier score, described in greater depth \nin Chapter 2. For these metrics, lower values represent superior performance. As shown \nin Table 4-8 and Figure 4-12, Phase 1 and 3 performance was quite similar, while phase 2 \npredictions appeared to fare worse by these metrics. \n \n \n"}, {"page": 73, "text": "59 \n \n \nTable 4-8. BioBERT Final Model Additional Metrics \nModel \nLog Loss \nBrier Score \nTrain All Test Neuro Phase 1 \n0.637 \n0.163 \nTrain All Test Neuro Phase 2 \n0.956 \n0.238 \nTrain All Test Neuro Phase 3 \n0.634 \n0.155 \nNote: For Log Loss and Brier Score, lower values represent superior performance. \n \n \nFigure 4-12. BioBERT Final Model Additional Metrics \n \n4.6 BioBERT Final Model Performance vs. Benchmarks \n \nFinally, the final models for Phases 1-3 were evaluated with respect to the \nindustry and dataset benchmarks. As a reminder, the industry benchmark is the one \nprovided by Citeline, as described in Sections 3.2 and 3.6, and provides industry average \ntrial outcome pTRS values. The dataset benchmark, also described in Chapter 3, is the \nactual mean outcome across all trials within the dataset that the model was trained on \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTrain All Test Neuro Phase 1 Train All Test Neuro Phase 2 Train All Test Neuro Phase 3\nAdditional Performance Metrics \nLogLoss\nBrier Score\n"}, {"page": 74, "text": "60 \n \n(this disadvantages the model, since this mean includes outcomes of trials that the model \nwas tested against, not just that it was trained on). Superiority to the industry benchmark \nindicates an improvement over common industry practice, whereas superiority to the \ndataset benchmark would represent excellent or aspirational performance. The evaluation \nwas conducted to see if the model’s individual trial predictions would be superior to \nsimply using one of the benchmarks instead for all trials. \n \nAs shown in Table 4-9 and Figure 4-13, the log loss and Brier score for the final \nmodel were both lower (superior) to the dataset benchmark in all three phases. \nAdditionally, the proportion of trials in which the model made a superior prediction (one \nthat was closer to the outcome label) was measured, and compared to the benchmarks as \nshown in Table 4-9 and Figure 4-14. The industry benchmark was surpassed by the \nmodel for all phases, and only for Phase 3 did the model fail to surpass the aspirational \nbar of the dataset benchmark, although Phase 3 all-phase mean performance for the \nmodel was still superior to the dataset benchmark. \nTable 4-9. BioBERT Performance Metrics vs. Benchmarks \nModel \nLog \nLoss \nBrier \nScore \n% of time model beats \nbenchmark \nModel Phase 1 \n0.637\n0.163\nn/a\nIndustry Benchmark Ph 1 \n0.716\n0.261\n76%\nDataset Benchmark Ph 1 \n0.565\n0.187\n79%\nModel Phase 2 \n0.956\n0.238\nn/a\nIndustry Benchmark Ph 2 \n1.034\n0.408\n71%\nDataset Benchmark Ph 2 \n0.629\n0.218\n62%\nModel Phase 3 \n0.634\n0.155\nn/a\nIndustry Benchmark Ph 3 \n0.687\n0.247\n64%\nDataset Benchmark Ph 3 \n0.53\n0.17\n48%\nMean Using Model \n0.742\n0.185\nn/a\nMean Using Industry \nBenchmark \n0.812\n0.305\n70%\n"}, {"page": 75, "text": "61 \n \nMean Using Dataset \nBenchmark \n0.575\n0.192\n63%\n \n \nFigure 4-13. BioBERT Performance Metrics vs. Benchmarks \n \nFigure 4-14. Proportion of Trials Where Model Outperformed Benchmarks \n \n4.7 Comparison of BioBERT Final Model to Published Models \n0.742\n0.812\n0.575\n0.185\n0.305\n0.192\n0.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\nMean Using Model\nMean Using Industry\nBenchmark\nMean Using Dataset\nBenchmark\nComparison of Metrics Across Phase Means when \nusing Model vs. Industry and Dataset Benchmarks \nLogLoss\nBrier Score\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\nIndustry Bench'k Ph 1\nDataset Bench'k Ph 1\nIndustry Bench'k Ph 2\nDataset Bench'k Ph 2\nIndustry Bench'k Ph 3\nDataset Bench'k Ph 3\nMean Industry Benchmark\nMean Dataset Benchmark\nProportion of test trials for which the BioBERT \nmodel predictions are superior to benchmark\n"}, {"page": 76, "text": "62 \n \n \nIn order to understand the performance of the final BioBERT LLM model relative \nto those described in published work with similar aims, Table 4-10 was assembled. ROC-\nAUC was the most consistently available metric that would allow for a comparison of \nboth the sensitivity and specificity of the models. The model described here outperformed \nmost of the other models reporting Phase 1 scores, and across all phases, the mean ROC-\nAUC score for this model comes in tied for #4 out of 7, despite being the only one to use \nno drug characteristic information other than what is contained in ClinicalTrials.gov.  \nTable 4-10. Comparison of Model ROC-AUC Relative to Published Methods \n \n \n4.8 Summary \nPhase 1\nPhase 2\nPhase 3\nMean\nThis \nWork\n0.74\n0.67\n0.71\n0.71\nLo, et al. \n(2017)\nn/a\n0.78\n0.81\n0.8\nGao, et \nal \n(2024)\n0.77\n0.74\n0.75\n0.75\nFeijoo, \net al. \n(2020)\nn/a\n0.74\n0.67\n0.71\nFu, et al. \n(2020)\n0.58\n0.65\n0.72\n0.65\nAliper, \net al. \n(2023)\nn/a\n0.88\nn/a\n0.88\nZheng, et \nal. \n(2024)\n0.65\n0.65\n0.74\n0.68\nROC-AUC Score\n"}, {"page": 77, "text": "63 \n \n \nThe results of this praxis were able to satisfactorily answer all three of the \nresearch questions from Section 1.6: \nRQ1: Can a non-LLM NLP model be built that can generate >5% better-than-random \npTRS predictions for neuroscience indications? \n \nYes. Accuracy relative to random predictions is best measured by balanced \naccuracy. As demonstrated in Section 4.2, balanced accuracy metrics for all three non-\nLLM models were above 59% (50% would correspond to random guessing).  \nRQ2: Does the use of an LLM-enhanced NLP model lead to a >5% improvement in \npTRS prediction over non-LLM for neuroscience indications? \n \nYes. In terms of quality of prediction, as measured by ROC-AUC, the all-phase \nmean ROC-AUC of 0.74 for the BioBERT model is a 15.8% improvement over the \ngradient boosting ROC-AUC of 0.639. \nRQ3: Can a model trained solely on ClinicalTrials.gov data and outcome labels surpass \nthe predictive performance of industry benchmarks by >5% for neuroscience indications? \n \nYes. As shown in Section 4.6, the final BioBERT model made better predictions \nof trial outcome than industry benchmarks for 70% of trials in the test set, with a mean \nBrier score 39.9% lower (better) than when benchmarks were used for the pTRS \npredictions. \nRQ4: Does use of training data from all indications (not just neuroscience) improve \nneuroscience pTRS prediction >5%? \n \nYes, training the BioBERT model on trials from all indications resulted in a mean \nROC-AUC score of 0.740, a 20.9% improvement over the 0.612 ROC-AUC seen when \ntraining only on trials for neuroscience indications (See Table 4-6).  \n"}, {"page": 78, "text": "64 \n \nChapter 5—Discussion and Conclusions \n \n5.1 Discussion \n \nPharmaceutical R&D involves the meticulous curation of programs within \ncompanies’ development pipelines. Extensive analysis is completed in order to ensure \nthat the most promising drug candidates are provided with the financial resources to \ncomplete costly clinical trials. This analysis includes the calculation of expected net \npresent value (eNPV), and other metrics to define the risk adjusted value of programs. \nThese risk adjusted values ultimately hinge on one extraordinarily important estimate: \nProbability of Success, or pTRS. Because of trial expense (hundreds of millions of \ndollars) and enormity of potential returns (billions of dollars annually), even small \nchanges to a program’s pTRS estimates may result in large shifts in the expected value of \nthese programs, and thus alter decisions on whether to proceed with or terminate a drug \ncandidate’s development. Because of this importance, drug companies spend significant \ntime (developing estimates), and money (investing in proprietary datasets) to improve \ntheir predictions to allow for better decisions to be made. \n \nIn order to build a model to generate predictions superior to industry benchmarks, \nthis praxis leveraged publicly available data from ClinicalTrials.gov. The data was \ndownloaded, sorted, and cleaned to prepare it for analysis. Any categories of information \n(such as patient number) that could have leaked data about trial status or outcome during \ntraining was removed. Clinical Trial Outcomes Database (CTOD) data from Gao, et al. \n(2024) was then also downloaded, and NCT (clinical trial ID) numbers were used to link \nclinical trial outcome labels with their corresponding trials in the clinical trial data CSVs. \n"}, {"page": 79, "text": "65 \n \nA subset of the comprehensive “all indication” train/test set was created containing only \nneuroscience trials. Industry benchmarks, providing the mean probabilities of trial \nsuccess for different clinical phases, were downloaded from Citeline, and dataset \nbenchmark means were calculated based on the labels within the train/test sets used. \n \nNon-LLM models were built in order to evaluate random forest, gradient \nboosting, and logistic regression approaches. These were evaluated in terms of ROC-\nAUC, PR-AUC, balanced accuracy, recall, precision, and F1. Each was able to make \npredictions far superior to random guessing, with the gradient boosting model being the \nbest across many metrics.  \n \nAn LLM-based approach was then implemented, centering around the BERT \n(Bidirectional Encoder Representations from Transformers) family of transformer \nmodels. Several BERT types were evaluated, with little clear difference among them, and \nBioBERT was chosen for use in building a refined LLM-based model. The final model \nwas able to produce a better prediction than the industry benchmark in 70% of trials, with \na mean Brier score 40% lower (better) than the industry benchmark. \nThis praxis sought to determine whether a useful model could be built to make \nsuccessful predictions of clinical trial outcomes based solely upon their descriptions in \nClinicalTrials.gov. The criteria for this evaluation (Section 4.8) included greater than 5% \nimprovement over random guessing and improvement over industry benchmarks. The \nfinal model described in this praxis was able to successfully surpass these thresholds.\n \nAs illustrated in Section 4.7, the model described in this praxis was outperformed \nby those built by some research teams However, those teams leveraged data such as \nmolecular profiles from manually curated databases of bioactive molecules, and/or drug \n"}, {"page": 80, "text": "66 \n \ncompound attributes from Pharmaprojrcts profiles database. Both of these databases \nwere used in order to train models on information about the drug and/or target molecules \nthemselves to enhance predictions. Likewise, Feijoo, et al., used proprietary \nBiomedtracker data to supply information about disease area and drug class for each trial \nin their training set. Fu, et al., used molecular and pharmacokinetic information about the \ndrugs used in trials from MoleculeNet for their training sets, and Zheng, et al, used \nSMILES, string-format information about molecular structures. Since the aim of this \npraxis was to build a model that did not use databases other than ClinicalTrials.gov and \nthe outcome labels, the model described herein was surprisingly competitive against these \nother models that leveraged additional molecular data. \nLimitations \nA key limitation of this work is that the clinical trial outcome dataset (CTOD) \nlabels are not, themselves, manually curated. Though they leverage large amounts of data \nthat appears after clinical trial completion, inaccuracies can exist in that dataset which \ncould cost overall model accuracy. In this way, there exists a potential tradeoff between \nusing a large set of outcome labels (like CTOD) that may have inaccuracies, or instead \ntraining a model with human-curated train/test datasets where the confidence in the label \naccuracy is greater, as other authors have done.  \nClinical trial descriptions from ClinicalTrials.gov that were missing critical \nelements needed for processing were omitted. If those omissions fit some pattern (e.g., if \npoorly funded trials positively correlate with incomplete trial data) this could potentially \nbias the model.  \n"}, {"page": 81, "text": "67 \n \nThe models described in this praxis were also trained on clinical trial data in a \nvery specific format – the fields associated with a clinical trial description published \nwithin the database. A company would therefore need to have a description set that aligns \nvery closely with this in order for the model to be able to make appropriate predictions, \nand that is not possible until a drug candidate’s clinical trial has been designed (see \nSection 3.7). If this information is not available from ClinicalTrials.gov or a similar \ndatabase, but a trial has been designed, a team could choose to request that the necessary \ndata fields (e.g., “Summary,” “Conditions,” “Interventions”) be drafted by their clinical \ndevelopment colleagues in order to facilitate pTRS estimation. However, it should be \nnoted that for the model’s predictions to be valid and applicable, the individuals \nproviding content for these data fields must have no knowledge of trial properties and \ndescriptors that correlate with higher or lower success predictions (e.g., the SHAP values \nfrom Section 4.2). This could very easily bias the model inputs and invalidate the \npredicted pTRS. \nAlong a similar line, it may be of interest to clinical development teams to \nunderstand the specific tokens that were correlated with higher outcome predictions. This \nmight lead them to focus on clinical approaches that appeared to be more auspicious. But \nthey should understand that the interactions between tokens and outcomes may be very \ncomplex, and correlation does not necessarily imply causation. Currently, it would be \ndifficult to ascertain SHAP-like values from the BioBERT model, since it is based on a \ncomplex neural network transformer, which makes it more difficult to reverse engineer \npotentially fortuitous trial properties. \n"}, {"page": 82, "text": "68 \n \nPotential users of this model should also be aware that while the clinical trial data \nit was trained on contains significant information about the trial, its endpoints, location, \ntarget patient population, etc., there are simply many variables that could result in the \ntermination of a trial or drug development program that are not ever going to be known at \nthe time the clinical trial description is written. Some safety concerns cannot be identified \nin any way until a drug has been administered to a sufficiently large human patient \npopulation, and drug efficacy is dependent on a myriad of molecular and systemic \ninteractions that the trial descriptions simply cannot shed light on. For these reasons, no \npredictive pTRS model will ever be able to approach 100% accuracy. \n \n5.2 Conclusions \n \nPharmaceutical innovation is inherently limited by risk, and the enormous \nexpense of failed clinical trials. Because of this, it is imperative that drug developers are \nable to scope probability of technical and regulatory success as accurately as possible. \nThis work demonstrates that non-LLM NLP models can provide better than random \npredictions of pTRS based only on ClinicalTrials.gov data. This allows for useful models \nto be used by companies and organizations who may not have access to expensive and \nspecialized technical data or GPU hardware to train pTRS prediction models. An LLM-\nbased model was then built that generated even better predictions, able to significantly \nsurpass the predictive validity of industry benchmarks for neuroscience trials. Since \nindustry benchmarks are often used by companies to help them understand technical risk \nand make go/no-go decisions, the predictive improvements demonstrated in this work can \nenable better asset allocation decisions. \n"}, {"page": 83, "text": "69 \n \n5.3 Contributions to Body of Knowledge  \n1. This praxis describes a novel use of the Clinical Trial Outcome Dataset to aid \npTRS prediction, demonstrating the utility of highly comprehensive outcome \nlabel sets. \n2. This praxis utilizes only ClinicalTrials.gov for training data, demonstrating that \nsignificant insight can be gained without deep background on drug molecular \nstructure. This underlying training data framework can then be scaled with \nwhatever additional data a team does have access to in order to further optimize \npredictions. \n3. This praxis demonstrates, using neuroscience as an example indication, that \ntraining a single-indication predictive pTRS model with clinical trials from all \nindications may produce better results than a narrower training approach using \nonly the target indication. \n4. This praxis showed that pTRS predictions for neuroscience, a therapeutic area \nwith unique drug delivery and toxicological challenges, could be improved \nimmensely with a model trained on open-source data. \n \n5.4 Recommendations for Future Research \nThe following activities could enhance the predictive power and utility of the final \nmodel: \n1. Evaluate additional LLM transformer models \nOther LLMs, such as BioGPT, or GatorTron from the University of Florida, \ncould exceed the predictive performance of the BioBERT model. \n"}, {"page": 84, "text": "70 \n \n2. Test the model against one of the smaller, human-curated trial outcome label \nsets \nThis could provide information on the impact of any potential inaccuracies in the \nlabels from the CTOD dataset. \n3. Identify key tokens from the BioBERT or other LLM model found to be \ncorrelated with higher or lower pTRS estimates \nWhile this was done for the gradient boosting model, this is more challenging for \na neural network based LLM. However, these insights could be particularly \nhelpful, aiding development teams in identifying auspicious trial features. \n4. Evaluate the model across all individual therapeutic areas \nSince neuroscience was the only indication evaluated, additional insights could \nbe gleaned from other therapeutic areas such as cardiovascular or oncology \ndrugs. This would also help to evaluate whether the finding that predictions are \nimproved by using all indications to train the model are generalizable to other \ntherapeutic areas. \n5. Evaluate a version of the model that uses all phases, with phase number as a \npredictive feature \nCurrently, there is a separate model for all three phases. By building one large \ncomprehensive model and then having Phase number be a selectable feature, it \nwould be possible to leverage the information from trials in other phases, which \ncould potentially improve predictions. \n6. Evaluate how the model performs on data from different time periods, and \nidentify trends \n"}, {"page": 85, "text": "71 \n \nIt may be that the model performs better on trials from a certain period, or \nperforms better when not trained on much older clinical trial data. Adjusting time \nparameters for train/test sets should be done in a way that maintains the \nsequential fidelity of training and testing, so that the model is not trained on any \ntrials that occurred after any of the test set trials. \n7. Identify additional open-source data that could be easily integrated into the \ntrain/test data \nWhile no predictive pTRS model will be able to make perfect predictions, adding \nfeatures to the current model, such as pharmacological data, could enhance its \npredictive utility. \n8. Conduct a post-hoc analysis on the best and worst predictions  \nAcross the test set, it would be possible to identify the best predictions (those \npredictions for which the model’s success probability prediction values were \nclosest to their corresponding 0/1 binary success/fail outcome) and the worst. \nExploratory data analysis could then be conducted to identify patterns that could \nlead to model improvements. \n9. Enable data pipelines that allow for continuous updating of model training \nThe biotechnology regulatory landscape can evolve relatively quickly, so an \nagile model that can be re-trained at a regular cadence would enhance pTRS \nprediction fidelity. Gao, et al., have included tools in their GitHub to facilitate \nuser-updating of clinical trial outcome estimates for emerging trials, meaning \nthat labels could be available for regular re-training of the model over time. \n \n"}, {"page": 86, "text": "72 \n \nContinual, incremental improvements to this model may pave the way for a predictive \ntool that allows companies to make better allocation of capital resources, so that they can \nimprove the arsenal of therapeutic tools available to help patients. \n \n"}, {"page": 87, "text": "73 \n \nReferences \n \nAggarwal, A., Xu, Z., Feyisetan, O., & Teissier, N. (2020). On Log-Loss Scores and (No) \nPrivacy. Proceedings of the Second Workshop on Privacy in NLP, 1–6. \nhttps://doi.org/10.18653/v1/2020.privatenlp-1.1 \nAliper, A., Kudrin, R., Polykovskiy, D., Kamya, P., Tutubalina, E., Chen, S., Ren, F., & \nZhavoronkov, A. (2023). Prediction of Clinical Trials Outcomes Based on Target \nChoice and Clinical Trial Design with Multi‐Modal Artificial Intelligence. \nClinical Pharmacology & Therapeutics, 114(5), 972–980. \nhttps://doi.org/10.1002/cpt.3008 \nAndersen, J. (2012). Probability Elicitation and Calibration in a Research & \nDevelopment Portfolio: A 13-Year Case Study. [PowerPoint Slides]. Eli Lilly and \nCompany. https://slideplayer.com/slide/8394454/ \nAustin, D. & Hayford, T. (2021). Research and Development in the Pharmaceutical \nIndustry. Congressional Budget Office. www.cbo.gov/publication/57025 \nBeck, J. T., Rammage, M., Jackson, G. P., Preininger, A. M., Dankwa-Mullan, I., \nRoebuck, M. C., Torres, A., Holtzen, H., Coverdill, S. E., Williamson, M. P., \nChau, Q., Rhee, K., & Vinegra, M. (2020). Artificial Intelligence Tool for \nOptimizing Eligibility Screening for Clinical Trials in a Large Community Cancer \nCenter. JCO Clinical Cancer Informatics, 4, 50–59. \nhttps://doi.org/10.1200/CCI.19.00079 \n"}, {"page": 88, "text": "74 \n \nBengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A Neural Probabilistic \nLanguage Model. Journal of Machine Learning Language 3. 1137-1155. \nhttps://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf \nBerger, J. R., Choi, D., Kaminski, H. J., Gordon, M. F., Hurko, O., D’Cruz, O., Pleasure, \nS. J., & Feldman, E. L. (2013). Importance and hurdles to drug discovery for \nneurological disease. Annals of Neurology, 74(3), 441–446. \nhttps://doi.org/10.1002/ana.23997 \nChen, Q., Hu, Y., Peng, X., Xie, Q., Jin, Q., Gilson, A., Singer, M. B., Ai, X., Lai, P.-T., \nWang, Z., Keloth, V. K., Raja, K., Huang, J., He, H., Lin, F., Du, J., Zhang, R., \nZheng, W. J., Adelman, R. A., … Xu, H. (2023). A systematic evaluation of large \nlanguage models for biomedical natural language processing: Benchmarks, \nbaselines, and recommendations (Version 4). arXiv. \nhttps://doi.org/10.48550/ARXIV.2305.16326 \nChoi, J. W. (2024). Analysis of Clinical Trial Design and Prediction of Success. \n[Doctoral dissertation, University College London]. UCL Discovery. \nClinicalTrials.gov. (n.d.) National Library of Medicine. National Institutes of Health. \nRetrieved September 3, 2024, from https://clinicaltrials.gov/ \nCook, J., & Ramadas, V. (2020). When to consult precision-recall curves. The Stata \nJournal: Promoting Communications on Statistics and Stata, 20(1), 131–148. \nhttps://doi.org/10.1177/1536867X20909693 \nDallow, N., Best, N., & Montague, T. H. (2018). Better decision making in drug \ndevelopment through adoption of formal prior elicitation. Pharmaceutical \nStatistics, 17(4), 301–316. https://doi.org/10.1002/pst.1854 \n"}, {"page": 89, "text": "75 \n \nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep \nBidirectional Transformers for Language Understanding (Version 2). arXiv. \nhttps://doi.org/10.48550/ARXIV.1810.04805 \nDiMasi, J. A., Grabowski, H. G., & Hansen, R. W. (2016). Innovation in the \npharmaceutical industry: New estimates of R&D costs. Journal of Health \nEconomics, 47, 20–33. https://doi.org/10.1016/j.jhealeco.2016.01.012 \nDong, X. (2018). Current Strategies for Brain Drug Delivery. Theranostics, 8(6), 1481–\n1493. https://doi.org/10.7150/thno.21254 \nEuropean Food Safety Authority. (2014). Guidance on Expert Knowledge Elicitation in \nFood and Feed Safety Risk Assessment. EFSA Journal, 12(6). \nhttps://doi.org/10.2903/j.efsa.2014.3734 \nFeijoo, F., Palopoli, M., Bernstein, J., Siddiqui, S., & Albright, T. E. (2020). Key \nindicators of phase transition for clinical trials through machine learning. Drug \nDiscovery Today, 25(2), 414–421. https://doi.org/10.1016/j.drudis.2019.12.014 \nFu, T., Huang, K., Xiao, C., Glass, L. M., & Sun, J. (2022). HINT: Hierarchical \ninteraction network for clinical-trial-outcome predictions. Patterns (New York, \nN.Y.), 3(4), 100445. https://doi.org/10.1016/j.patter.2022.100445 \nGao, T., Dontcheva, M., Adar, E., Liu, Z., & Karahalios, K. G. (2015). DataTone: \nManaging Ambiguity in Natural Language Interfaces for Data Visualization. \nProceedings of the 28th Annual ACM Symposium on User Interface Software & \nTechnology, 489–500. https://doi.org/10.1145/2807442.2807478 \n"}, {"page": 90, "text": "76 \n \nGao, C., Fu, T., & Sun, J. (2024). Language Interaction Network for Clinical Trial \nApproval Estimation (Version 1). arXiv. \nhttps://doi.org/10.48550/ARXIV.2405.06662 \nGao, C., Pradeepkumar, J., Das, T., Thati, S., & Sun, J. (2024). Automatically Labeling \n$200B Life-Saving Datasets: A Large Clinical Trial Outcome Benchmark (No. \narXiv:2406.10292). arXiv. http://arxiv.org/abs/2406.10292 \nGarcía, V., Mollineda, R. A., & Sánchez, J. S. (2009). Index of Balanced Accuracy: A \nPerformance Measure for Skewed Class Distributions. In H. Araujo, A. M. \nMendonça, A. J. Pinho, & M. I. Torres (Eds.), Pattern Recognition and Image \nAnalysis (Vol. 5524, pp. 441–448). Springer Berlin Heidelberg. \nhttps://doi.org/10.1007/978-3-642-02172-5_57 \nGayvert, K. M., Madhukar, N. S., & Elemento, O. (2016). A Data-Driven Approach to \nPredicting Successes and Failures of Clinical Trials. Cell Chemical Biology, \n23(10), 1294–1301. https://doi.org/10.1016/j.chembiol.2016.07.023 \nGeerts, H., Gieschke, R., & Peck, R. (2018). Use of quantitative clinical pharmacology to \nimprove early clinical development success in neurodegenerative diseases. Expert \nReview of Clinical Pharmacology, 11(8), 789–795. \nhttps://doi.org/10.1080/17512433.2018.1501555 \nGrandini, M., Bagli, E., & Visani, G. (2020). Metrics for Multi-Class Classification: An \nOverview (Version 1). arXiv. https://doi.org/10.48550/ARXIV.2008.05756 \nGrudzinskas, C., Dyszel, M., Sharma, K., & Gombar, C. T. (2022). Portfolio and project \nplanning and management in the drug discovery, evaluation, development, and \n"}, {"page": 91, "text": "77 \n \nregulatory review process. In Atkinson's Principles of Clinical Pharmacology (pp. \n537-562). Academic Press. \nGyawali, B., Eisenhauer, E., Tregear, M., & Booth, C. M. (2022). Progression-free \nsurvival: It is time for a new name. The Lancet Oncology, 23(3), 328–330. \nhttps://doi.org/10.1016/S1470-2045(22)00015-8 \nHaddad, T., Helgeson, J. M., Pomerleau, K. E., Preininger, A. M., Roebuck, M. C., \nDankwa-Mullan, I., Jackson, G. P., & Goetz, M. P. (2021). Accuracy of an \nArtificial Intelligence System for Cancer Clinical Trial Eligibility Screening: \nRetrospective Pilot Study. JMIR Medical Informatics, 9(3), e27767. \nhttps://doi.org/10.2196/27767 \nHampson, L. V., Holzhauer, B., Bornkamp, B., Kahn, J., Lange, M. R., Luo, W., Singh, \nP., Ballerstedt, S., & Cioppa, G. D. (2022). A New Comprehensive Approach to \nAssess the Probability of Success of Development Programs Before Pivotal \nTrials. Clinical Pharmacology & Therapeutics, 111(5), 1050–1060. \nhttps://doi.org/10.1002/cpt.2488 \nHampson, L. V., Bornkamp, B., Holzhauer, B., Kahn, J., Lange, M. R., Luo, W.-L., \nCioppa, G. D., Stott, K., & Ballerstedt, S. (2021). Improving the assessment of the \nprobability of success in late stage drug development (No. arXiv:2102.02752). \narXiv. http://arxiv.org/abs/2102.02752 \nHand, D. J., Christen, P., & Kirielle, N. (2021). F*: An interpretable transformation of the \nF-measure. Machine Learning, 110(3), 451–456. https://doi.org/10.1007/s10994-\n021-05964-1 \n"}, {"page": 92, "text": "78 \n \nHarpum, P. (2010). Portfolio, program, and project management in the pharmaceutical \nand biotechnology industries. John Wiley & Sons. \nhttps://doi.org/10.1002/9780470603789 \nHolzhauer, B., Hampson, L. V., Gosling, J. P., Bornkamp, B., Kahn, J., Lange, M. R., \nLuo, W., Brindicci, C., Lawrence, D., Ballerstedt, S., & O’Hagan, A. (2022). \nEliciting judgements about dependent quantities of interest: The SHeffield \nELicitation Framework extension and copula methods illustrated using an asthma \ncase study. Pharmaceutical Statistics, 21(5), 1005–1021. \nhttps://doi.org/10.1002/pst.2212 \nHossain, E., Rana, R., Higgins, N., Soar, J., Barua, P. D., Pisani, A. R., & Turner, K. \n(2023). Natural Language Processing in Electronic Health Records in relation to \nhealthcare decision-making: A systematic review. Computers in Biology and \nMedicine, 155, 106649. https://doi.org/10.1016/j.compbiomed.2023.106649 \nKhurana, D., Koli, A., Khatter, K., & Singh, S. (2023). Natural language processing: \nState of the art, current trends and challenges. Multimedia Tools and Applications, \n82(3), 3713–3744. https://doi.org/10.1007/s11042-022-13428-4 \nKolluri, S., Lin, J., Liu, R., Zhang, Y., & Zhang, W. (2022). Machine Learning and \nArtificial Intelligence in Pharmaceutical Research and Development: A Review. \nThe AAPS Journal, 24(1), 19. https://doi.org/10.1208/s12248-021-00644-3 \nLi, B., Shin, H., Gulbekyan, G., Pustovalova, O., Nikolsky, Y., Hope, A., Bessarabova, \nM., Schu, M., Kolpakova-Hart, E., Merberg, D., Dorner, A., & Trepicchio, W. L. \n(2015). Development of a Drug-Response Modeling Framework to Identify Cell \nLine Derived Translational Biomarkers That Can Predict Treatment Outcome to \n"}, {"page": 93, "text": "79 \n \nErlotinib or Sorafenib. PLOS ONE, 10(6), e0130700. \nhttps://doi.org/10.1371/journal.pone.0130700 \nLi, M., Liu, R., Lin, J., Bunn, V., & Zhao, H. (2020). Bayesian Semi-parametric Design \n(BSD) for adaptive dose-finding with multiple strata. Journal of \nBiopharmaceutical Statistics, 30(5), 806–820. \nhttps://doi.org/10.1080/10543406.2020.1730870 \nLi, X., Lei, Y., & Ji, S. (2022). BERT- and BiLSTM-Based Sentiment Analysis of Online \nChinese Buzzwords. Future Internet, 14(11), 332. \nhttps://doi.org/10.3390/fi14110332 \nLiu, R., Lin, J., & Li, P. (2020). Design considerations for phase I/II dose finding clinical \ntrials in Immuno-oncology and cell therapy. Contemporary Clinical Trials, 96, \n106083. https://doi.org/10.1016/j.cct.2020.106083 \nLo, A. W., Siah, K. W., & Wong, C. H. (2017). Machine-Learning Models for Predicting \nDrug Approvals and Clinical-Phase Transitions. SSRN Electronic Journal. \nhttps://doi.org/10.2139/ssrn.2973611 \nLu, Y., Chen, T., Hao, N., Van Rechem, C., Chen, J., & Fu, T. (2024). Uncertainty \nQuantification and Interpretability for Clinical Trial Approval Prediction. Health \nData Science, 4, 0126. https://doi.org/10.34133/hds.0126 \nLuitse, D., & Denkena, W. (2021). The great Transformer: Examining the role of large \nlanguage models in the political economy of AI. Big Data & Society, 8(2), \n20539517211047734. https://doi.org/10.1177/20539517211047734 \nMosca, E., Szigeti, F., Tragianni, S., Gallagher, D., & Groh, G. (2022). SHAP-Based \nExplanation Methods: A Review for NLP Interpretability. In N. Calzolari, C.-R. \n"}, {"page": 94, "text": "80 \n \nHuang, H. Kim, J. Pustejovsky, L. Wanner, K.-S. Choi, … S.-H. Na (Eds.), \nProceedings of the 29th International Conference on Computational Linguistics \n(pp. 4593–4603). Retrieved from https://aclanthology.org/2022.coling-1.406/ \nMurali, V., Muralidhar, Y. P., Königs, C., Nair, M., Madhu, S., Nedungadi, P., Srinivasa, \nG., & Athri, P. (2022). Predicting clinical trial outcomes using drug bioactivities \nthrough graph database integration and machine learning. Chemical Biology & \nDrug Design, 100(2), 169–184. https://doi.org/10.1111/cbdd.14092 \nNahm, F. S. (2022). Receiver operating characteristic curve: Overview and practical use \nfor clinicians. Korean Journal of Anesthesiology, 75(1), 25–36. \nhttps://doi.org/10.4097/kja.21209 \nNaidu, G., Zuva, T., & Sibanda, E. M. (2023). A Review of Evaluation Metrics in \nMachine Learning Algorithms. In R. Silhavy & P. Silhavy (Eds.), Artificial \nIntelligence Application in Networks and Systems (Vol. 724, pp. 15–25). Springer \nInternational Publishing. https://doi.org/10.1007/978-3-031-35314-7_2 \nNaveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, \nN., & Mian, A. (2024). A Comprehensive Overview of Large Language Models \n(No. arXiv:2307.06435). arXiv. http://arxiv.org/abs/2307.06435 \nOakley, J., & O’Hagan, T. (2022). The Sheffield Elicitation Framework (SHELF). \nhttps://shelf.sites.sheffield.ac.uk/  \nPappas, N., & Meyer, T. (2012). A Survey on Language Modeling Using Neural \nNetworks. IDIAP Research Institute. \nhttps://infoscience.epfl.ch/server/api/core/bitstreams/066718a4-2e03-4697-a1e0-\n623e185d95ee/content \n"}, {"page": 95, "text": "81 \n \nPeck, R. (2017). The pharmaceutical industry needs more clinical pharmacologists. \nBritish Journal of Clinical Pharmacology, 83(11), 2343–2346. \nhttps://doi.org/10.1111/bcp.13370 \nPaul, S. M., Mytelka, D. S., Dunwiddie, C. T., Persinger, C. C., Munos, B. H., Lindborg, \nS. R., & Schacht, A. L. (2010). How to improve R&D productivity: The \npharmaceutical industry’s grand challenge. Nature Reviews Drug Discovery, 9(3), \n203–214. https://doi.org/10.1038/nrd3078 \nPharmapremia Pharma Intelligence (n.d.) Sample Selection. Citeline. Retrieved March 3, \n2025, from https://www.pharmapremiasolutions.com/table \nRaiaan, M. A. K., Mukta, Md. S. H., Fatema, K., Fahad, N. M., Sakib, S., Mim, M. M. J., \nAhmad, J., Ali, M. E., & Azam, S. (2024). A Review on Large Language Models: \nArchitectures, Applications, Taxonomies, Open Issues and Challenges. IEEE \nAccess, 12, 26839–26874. https://doi.org/10.1109/ACCESS.2024.3365742 \nReinisch, M., He, J., Liao, C., Siddiqui, S. A., & Xiao, B. (2024). CTP-LLM: Clinical \nTrial Phase Transition Prediction Using Large Language Models (No. \narXiv:2408.10995). arXiv. http://arxiv.org/abs/2408.10995 \nStalder, J. P. (2022). Creating a Clinical Development Plan. In Project Management for \nDrug Developers (pp. 185-201). CRC Press. \nStaszak, M., Staszak, K., Wieszczycka, K., Bajek, A., Roszkowski, K., & Tylkowski, B. \n(2022). Machine learning in drug design: Use of artificial intelligence to explore \nthe chemical structure–biological activity relationship. WIREs Computational \nMolecular Science, 12(2), e1568. https://doi.org/10.1002/wcms.1568 \n"}, {"page": 96, "text": "82 \n \nTrotti, A., Colevas, A., Setser, A., Rusch, V., Jaques, D., Budach, V., Langer, C., \nMurphy, B., Cumberlin, R., & Coleman, C. (2003). CTCAE v3.0: Development \nof a comprehensive grading system for the adverse effects of cancer treatment. \nSeminars in Radiation Oncology, 13(3), 176–181. https://doi.org/10.1016/S1053-\n4296(03)00031-6 \nUmber, A., & Bajwa, I. S. (2011). Minimizing ambiguity in natural language software \nrequirements specification. 2011 Sixth International Conference on Digital \nInformation Management, 102–107. \nhttps://doi.org/10.1109/ICDIM.2011.6093363 \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., \n& Polosukhin, I. (2017). Attention Is All You Need (Version 7). arXiv. \nhttps://doi.org/10.48550/ARXIV.1706.03762 \nVergetis, V., Skaltsas, D., Gorgoulis, V. G., & Tsirigos, A. (2021). Assessing Drug \nDevelopment Risk Using Big Data and Machine Learning. Cancer Research, \n81(4), 816–819. https://doi.org/10.1158/0008-5472.CAN-20-0866 \nVeriSIM Life. (2024, August 6). How AI Assists Small Molecule Drug Design and \nDevelopment. November 11, 2024, https://www.verisimlife.com/publications-\nblog/how-ai-assists-small-molecule-drug-design-and-development  \nWallach, I., Dzamba, M., & Heifets, A. (2015). AtomNet: A Deep Convolutional Neural \nNetwork for Bioactivity Prediction in Structure-based Drug Discovery (Version \n1). arXiv. https://doi.org/10.48550/ARXIV.1510.02855 \nWeissler, E. H., Naumann, T., Andersson, T., Ranganath, R., Elemento, O., Luo, Y., \nFreitag, D. F., Benoit, J., Hughes, M. C., Khan, F., Slater, P., Shameer, K., Roe, \n"}, {"page": 97, "text": "83 \n \nM., Hutchison, E., Kollins, S. H., Broedl, U., Meng, Z., Wong, J. L., Curtis, L., … \nGhassemi, M. (2021). The role of machine learning in clinical research: \nTransforming the future of evidence generation. Trials, 22(1), 537. \nhttps://doi.org/10.1186/s13063-021-05489-x \nWilimitis, D., & Walsh, C. G. (2023). Practical Considerations and Applied Examples of \nCross-Validation for Model Development and Evaluation in Health Care: \nTutorial. JMIR AI, 2, e49023. https://doi.org/10.2196/49023 \nWilliamson, D. J., Struyven, R. R., Antaki, F., Chia, M. A., Wagner, S. K., Jhingan, M., \nWu, Z., Guymer, R., Skene, S. S., Tammuz, N., Thomson, B., Chopra, R., & \nKeane, P. A. (2024). Artificial Intelligence to Facilitate Clinical Trial Recruitment \nin Age-Related Macular Degeneration. Ophthalmology Science, 4(6), 100566. \nhttps://doi.org/10.1016/j.xops.2024.100566 \nWong, C. H., Siah, K. W., & Lo, A. W. (2019). Estimation of clinical trial success rates \nand related parameters. Biostatistics, 20(2), 273–286. \nhttps://doi.org/10.1093/biostatistics/kxx069 \nWouters, O. J., McKee, M., & Luyten, J. (2020). Estimated research and development \ninvestment needed to bring a new medicine to market, 2009-2018. JAMA, 323(9), \n844-853. https://doi.org/10.1001/jama.2020.1166 \nWouters, O. J., & Kesselheim, A. S. (2024). Quantifying Research and Development \nExpenditures in the Drug Industry. JAMA Network Open, 7(6), e2415407. \nhttps://doi.org/10.1001/jamanetworkopen.2024.15407 \nYamaguchi, S., Kaneko, M., & Narukawa, M. (2021). Approval success rates of drug \ncandidates based on target, action, modality, application, and their combinations. \n"}, {"page": 98, "text": "84 \n \nClinical and Translational Science, 14(3), 1113–1122. \nhttps://doi.org/10.1111/cts.12980 \nZaragoza Domingo, S., Alonso, J., Ferrer, M., Acosta, M. T., Alphs, L., Annas, P., \nBalabanov, P., Berger, A.-K., Bishop, K. I., Butlen-Ducuing, F., Dorffner, G., \nEdgar, C., De Gracia Blanco, M., Harel, B., Harrison, J., Horan, W. P., Jaeger, J., \nKottner, J., Pinkham, A., … Yavorsky, C. (2024). Methods for Neuroscience \nDrug Development: Guidance on Standardization of the Process for Defining \nClinical Outcome Strategies in Clinical Trials. European \nNeuropsychopharmacology, 83, 32–42. \nhttps://doi.org/10.1016/j.euroneuro.2024.02.009 \nZheng, W., Peng, D., Xu, H., Li, Y., Zhu, H., Fu, T., & Yao, H. (2024). Multimodal \nClinical Trial Outcome Prediction with Large Language Models (Version 3). \narXiv. https://doi.org/10.48550/ARXIV.2402.06512 \nZhou, S., & Johnson, R. (2018). Pharmaceutical Probability of Success. Alacrita. \nhttps://alacrita.com/wp-content/uploads/2018/12/Pharmaceutical-Probability-of-\nSuccess.pdf \nZhu, K., Zheng, Y., & Chan, K. C. G. (2024). Weighted Brier Score—An Overall \nSummary Measure for Risk Prediction Models with Clinical Utility Consideration \n(No. arXiv:2408.01626). arXiv. https://doi.org/10.48550/arXiv.2408.01626 \nZhu, T. (2021). Challenges of Psychiatry Drug Development and the Role of Human \nPharmacology Models in Early Development—A Drug Developer’s Perspective. \nFrontiers in Psychiatry, 11, 562660. https://doi.org/10.3389/fpsyt.2020.562660 \n \n"}, {"page": 99, "text": "85 \n \nAppendix A \nTable A-1. Full Logistic Regression Results \n \nTN = true negative. FP = false positive. FN = false negative. TP = true positive. \n \n \nFigure A-1. Full Logistic Regression Results \n \nTable A-2. Full Gradient Boosting Results\n \nTraining/Testing\nROC-AUC TN\nFP\nFN\nTP\nAccuracy Bal'd Acc Precision Recall \nF1\nCohen-k\nPR-AUC\nAll Indications Phase 1\n0.665\n1174\n862\n3665\n7636\n0.661\n0.626\n0.899\n0.676\n0.771\n0.161\n0.905\nAll Indications Phase 2\n0.602\n1672\n500\n5385\n2972\n0.441\n0.563\n0.856\n0.356\n0.502\n0.0685\n0.844\nAll Indications Phase 3\n0.661\n660\n341\n2028\n2739\n0.589\n0.617\n0.889\n0.575\n0.698\n0.14\n0.896\nNeuroscience Only Phase 1\n0.652\n522\n261\n1662\n2024\n0.57\n0.607\n0.886\n0.549\n0.678\n0.127\n0.887\nNeuroscience Only Phase 2\n0.593\n673\n134\n2013\n779\n0.403\n0.556\n0.853\n0.279\n0.421\n0.062\n0.831\nNeuroscience Only Phase 3\n0.66\n268\n113\n695\n815\n0.573\n0.621\n0.878\n0.54\n0.669\n0.155\n0.885\nMean\n0.639\n828\n369\n2575\n2828\n0.540\n0.598\n0.877\n0.496\n0.623\n0.119\n0.875\n0\n0.2\n0.4\n0.6\n0.8\n1\nROC-AUC Accuracy\nBal'd Acc\nPrecision\nRecall\nF1\nCohen-k\nPR-AUC\nLogsitic Regression Model Performance Across \nTrain/Test Sets\nAll Indications Phase 1\nAll Indications Phase 2\nAll Indications Phase 3\nNeuroscience Only Phase 1\nNeuroscience Only Phase 2\nNeuroscience Only Phase 3\nTraining/Testing\nROC-AUC TN\nFP\nFN\nTP\nAccuracy Bal'd Acc Precision Recall \nF1\nCohen-k\nPR-AUC\nAll Indications Phase 1\n0.67\n926\n1110\n2683\n8618\n0.716\n0.609\n0.886\n0.763\n0.82\n0.165\n0.909\nAll Indications Phase 2\n0.598\n1491\n681\n4567\n3790\n0.501\n0.57\n0.848\n0.454\n0.591\n0.0842\n0.848\nAll Indications Phase 3\n0.662\n583\n418\n1568\n3199\n0.656\n0.627\n0.884\n0.671\n0.763\n0.174\n0.893\nNeuroscience Only Phase 1\n0.662\n446\n337\n1289\n2397\n0.636\n0.61\n0.877\n0.65\n0.747\n0.149\n0.895\nNeuroscience Only Phase 2\n0.591\n597\n210\n1716\n1076\n0.465\n0.563\n0.837\n0.385\n0.528\n0.075\n0.835\nNeuroscience Only Phase 3\n0.652\n208\n173\n521\n989\n0.633\n0.6\n0.851\n0.655\n0.74\n0.15\n0.878\nMean \n0.639\n708.500\n488.167\n2057.333\n3344.833\n0.601\n0.597\n0.864\n0.596\n0.698\n0.133\n0.876\n"}, {"page": 100, "text": "86 \n \n \nFigure A-2. Full Gradient Boosting Results \n \nTable A-3. Full Random Forest Results \n \n \nFigure A-3. Full Random Forest Results \n0\n0.2\n0.4\n0.6\n0.8\n1\nROC-AUC Accuracy\nBal'd Acc\nPrecision\nRecall\nF1\nCohen-k\nPR-AUC\nGradient Boosting Model Performance Across \nTrain/Test Sets\nAll Indications Phase 1\nAll Indications Phase 2\nAll Indications Phase 3\nNeuroscience Only Phase 1\nNeuroscience Only Phase 2\nNeuroscience Only Phase 3\nTraining/Testing\nROC-AUC TN\nFP\nFN\nTP\nAccuracy Bal'd Acc Precision Recall \nF1\nCohen-k\nPR-AUC\nAll Indications Phase 1\n0.664\n957\n1079\n2830\n8471\n0.707\n0.61\n0.887\n0.75\n0.813\n0.162\n0.907\nAll Indications Phase 2\n0.599\n1493\n679\n4596\n3761\n0.499\n0.569\n0.847\n0.45\n0.588\n0.0824\n0.847\nAll Indications Phase 3\n0.651\n586\n415\n1743\n3024\n0.626\n0.609\n0.879\n0.634\n0.737\n0.144\n0.886\nNeuroscience Only Phase 1\n0.658\n475\n308\n1400\n2286\n0.618\n0.613\n0.881\n0.62\n0.728\n0.146\n0.893\nNeuroscience Only Phase 2\n0.588\n600\n207\n1734\n1058\n0.461\n0.561\n0.836\n0.379\n0.522\n0.0732\n0.825\nNeuroscience Only Phase 3\n0.657\n242\n139\n627\n883\n0.595\n0.61\n0.864\n0.585\n0.697\n0.149\n0.88\nMean\n0.636\n726\n471\n2155\n3247\n0.584\n0.595\n0.866\n0.570\n0.681\n0.126\n0.873\n0\n0.2\n0.4\n0.6\n0.8\n1\nROC-AUC Accuracy\nBal'd Acc\nPrecision\nRecall\nF1\nCohen-k\nPR-AUC\nRandom Forest Model Performance Across \nTrain/Test Sets\nAll Indications Phase 1\nAll Indications Phase 2\nAll Indications Phase 3\nNeuroscience Only Phase 1\nNeuroscience Only Phase 2\nNeuroscience Only Phase 3\n"}, {"page": 101, "text": "87 \n \n \n \n \nFigure A4. ROC Curve for Phase 1 Logistic Regression Model Training on \nNeuroscience Trials \n \nFigure A5. ROC Curve for Phase 2 Logistic Regression Model Training on \nNeuroscience Trials \n"}, {"page": 102, "text": "88 \n \n \nFigure A6. ROC Curve for Phase 3 Logistic Regression Model Training on \nNeuroscience Trials \n \nFigure A7. ROC Curve for Phase 1 Logistic Regression Model Training on All \nIndications \n"}, {"page": 103, "text": "89 \n \n \nFigure A8. ROC Curve for Phase 2 Logistic Regression Model Training on All \nIndications \n \nFigure A9. ROC Curve for Phase 3 Logistic Regression Model Training on All \nIndications \n"}, {"page": 104, "text": "90 \n \n \nFigure A10. ROC Curve for Phase 1 Gradient Boosting Model Training on \nNeuroscience Trials \n \nFigure A11. ROC Curve for Phase 2 Gradient Boosting Model Training on \nNeuroscience Trials \n"}, {"page": 105, "text": "91 \n \n \nFigure A12. ROC Curve for Phase 3 Gradient Boosting Model Training on \nNeuroscience Trials \n \nFigure A13. ROC Curve for Phase 1 Gradient Boosting Model Training on All \nIndications \n"}, {"page": 106, "text": "92 \n \n \nFigure A14. ROC Curve for Phase 2 Gradient Boosting Model Training on All \nIndications \n \nFigure A15. ROC Curve for Phase 3 Gradient Boosting Model Training on All \nIndications \n"}, {"page": 107, "text": "93 \n \n \nFigure A16. ROC Curve for Phase 1 Random Forest Model Training on \nNeuroscience Trials \n \nFigure A17. ROC Curve for Phase 2 Random Forest Model Training on \nNeuroscience Trials \n"}, {"page": 108, "text": "94 \n \n \nFigure A18. ROC Curve for Phase 3 Random Forest Model Training on \nNeuroscience Trials \n \n \nFigure A19. ROC Curve for Phase 1 Random Forest Model Training on All \nIndications \n"}, {"page": 109, "text": "95 \n \n \nFigure A20. ROC Curve for Phase 2 Random Forest Model Training on All \nIndications \n \nFigure A21. ROC Curve for Phase 3 Random Forest Model Training on All \nIndications \n \n \n \n"}, {"page": 110, "text": "96 \n \nAppendix B \nNeuroscience Filter Strings \nDementias and related  \n        \"Alzheimer's disease\", \"Alzheimer's\", \"Alzheimers disease\", \"Alzheimers\", \"AD\", \n        \"Parkinson's disease\", \"Parkinson's\", \"Parkinsons disease\", \"Parkinsons\", \"PD\", \n        \"Huntington's disease\", \"Huntingtons disease\", \"Huntington\", \"HD\", \n        \"Amyotrophic lateral sclerosis\", \"ALS\", \"Lou Gehrig's disease\", \n        \"Frontotemporal dementia\", \"FTD\", \"fronto-temporal dementia\", \"frontotemporal \nlobar degeneration\", \"FTLD\", \n        \"Lewy body dementia\", \"LBD\", \"dementia with Lewy bodies\", \"DLB\", \n        \"Vascular dementia\", \"multi-infarct dementia\", \n        \"Creutzfeldt-Jakob disease\", \"CJD\", \n        \"Progressive supranuclear palsy\", \"PSP\", \"Steele-Richardson-Olszewski syndrome\", \n        \"Corticobasal degeneration\", \"CBD\", \"corticobasal syndrome\", \"CBS\", \n \n  Multiple sclerosis & related \n        \"Multiple sclerosis\", \"MS\", \n        \"Neuromyelitis optica\", \"NMO\", \"Devic's disease\", \n        \"Acute disseminated encephalomyelitis\", \"ADEM\", \n        \"Transverse myelitis\", \n \n  Neuropathies, demyelinating, peripheral nerve \n        \"Guillain-Barré syndrome\", \"GBS\", \n        \"Chronic inflammatory demyelinating polyneuropathy\", \"CIDP\", \n        \"Peripheral neuropathy\", \"peripheral neuropathic\", \"neuropathy\", \"neuropathic\", \n        \"Diabetic neuropathy\", \"diabetic neuropathic\", \n        \"Charcot-Marie-Tooth disease\", \"CMT\", \"hereditary motor and sensory \nneuropathy\", \"HMSN\", \n        \"Friedreich's ataxia\", \"FRDA\", \n        \"Spinocerebellar ataxia\", \"SCA\", \"ataxia\", \"ataxic disorders\", \"ataxic\", \n \n    Movement disorders \n        \"Essential tremor\", \"familial tremor\", \n        \"Dystonia\", \"dystonic\", \n        \"Tourette syndrome\", \"Tourette's\", \"Tourette\", \"TS\", \n        \"Restless legs syndrome\", \"RLS\", \"Willis-Ekbom disease\", \n        \"Multiple system atrophy\", \"MSA\", \"Shy-Drager syndrome\", \n        \"Ataxia-telangiectasia\", \"Louis–Bar syndrome\", \n        \"Myasthenia gravis\", \"MG\", \n        \"Lambert-Eaton myasthenic syndrome\", \"LEMS\", \n \n"}, {"page": 111, "text": "97 \n \n     Muscular dystrophies & SMA \n        \"Muscular dystrophy\", \"MD\", \n        \"Duchenne muscular dystrophy\", \"DMD\", \n        \"Becker muscular dystrophy\", \"BMD\", \n        \"Spinal muscular atrophy\", \"SMA\", \n \n     Myopathies, inflammatory muscle diseases \n        \"Polymyositis\", \n        \"Dermatomyositis\", \n \n     Epilepsies & seizure disorders \n        \"Epilepsy\", \"epileptic disorders\", \"seizure disorder\", \"seizures\", \n        \"Absence seizures\", \"petit mal seizures\", \n        \"Tonic-clonic seizures\", \"grand mal seizures\", \n        \"Temporal lobe epilepsy\", \n        \"Dravet syndrome\", \"severe myoclonic epilepsy of infancy\", \n        \"Lennox-Gastaut syndrome\", \"LGS\", \n        \"West syndrome\", \"infantile spasms\", \n        \"Landau-Kleffner syndrome\", \n        \"Benign rolandic epilepsy\", \"benign childhood epilepsy with centrotemporal \nspikes\", \n \n    Headaches & facial pain \n        \"Migraine\", \"migraines\", \"migraine headache\", \n        \"Tension-type headache\", \"tension headache\", \n        \"Cluster headache\", \"cluster headaches\", \n        \"Trigeminal neuralgia\", \"tic douloureux\", \n        \"Occipital neuralgia\", \n        \"Hemicrania continua\", \n        \"Postherpetic neuralgia\", \"PHN\", \n        \"Neuropathic pain\", \n        \"Complex regional pain syndrome\", \"CRPS\", \"reflex sympathetic dystrophy\", \n\"RSD\", \n \n     Stroke & cerebrovascular \n        \"Stroke\", \"cerebrovascular accident\", \"CVA\", \n        \"Transient ischemic attack\", \"TIA\", \n        \"Subarachnoid hemorrhage\", \"SAH\", \n        \"Cerebral aneurysm\", \"intracranial aneurysm\", \n        \"Arteriovenous malformation\", \"AVM\", \n        \"Intracerebral hemorrhage\", \"ICH\", \n        \"Vascular malformation\", \n \n"}, {"page": 112, "text": "98 \n \n     Infections of the CNS \n        \"Meningitis\", \n        \"Encephalitis\", \n        \"Brain abscess\", \"intracranial abscess\", \n        \"Poliomyelitis\", \"polio\", \n        \"Rabies\", \n        \"Tetanus\", \n        \"Progressive multifocal leukoencephalopathy\", \"PML\", \n        \"Lyme neuroborreliosis\", \"neurological Lyme disease\", \n        \"Herpes simplex encephalitis\", \n        \"Prion disease\", \"prion disorders\", \n \n     Brain & spinal tumors \n        \"Brain tumor\", \"intracranial neoplasm\", \n        \"Glioblastoma\", \"GBM\", \"glioblastoma multiforme\", \n        \"Astrocytoma\", \n        \"Meningioma\", \n        \"Oligodendroglioma\", \n        \"Spinal cord tumor\", \"spinal neoplasm\", \n \n     Structural/anatomical brain & spine \n        \"Hydrocephalus\", \n        \"Chiari malformation\", \"Arnold-Chiari malformation\", \n        \"Syringomyelia\", \n        \"Tethered cord syndrome\", \n \n     Conversion / functional disorders \n        \"Conversion disorder\", \"functional neurological symptom disorder\", \n        \"Psychogenic nonepileptic seizures\", \"PNES\", \n \n     Autonomic dysfunction \n        \"Dysautonomia\", \"autonomic dysfunction\", \n        \"Postural orthostatic tachycardia syndrome\", \"POTS\", \n \n     Sleep disorders (neurological basis) \n        \"Narcolepsy\", \"narcoleptic\", \n        \"REM sleep behavior disorder\", \"RBD\", \n        \"Idiopathic hypersomnia\", \n \n     Developmental & pediatric \n        \"Autism spectrum disorder\", \"ASD\", \"autistic disorder\", \n        \"Cerebral palsy\", \"CP\", \n        \"Rett syndrome\", \"RTT\", \n"}, {"page": 113, "text": "99 \n \n        \"Fragile X syndrome\", \n        \"Angelman syndrome\", \n        \"Developmental coordination disorder\", \"DCD\", \n        \"Attention deficit hyperactivity disorder\", \"ADHD\", \n \n     Rare/Other \n        \"Guam disease\", \"lytico-bodig disease\", \n        \"Familial hemiplegic migraine\", \"FHM\", \n        \"CADASIL\", \"Cerebral autosomal dominant arteriopathy with subcortical infarcts \nand leukoencephalopathy\", \n        \"Moyamoya disease\", \n        \"Stiff-person syndrome\", \"SPS\", \"stiff-man syndrome\", \n        \"Opsoclonus-myoclonus syndrome\", \"OMS\", \"dancing eyes-dancing feet \nsyndrome\", \n        \"Neuromyopathy\", \"neuromuscular disorder\", \n        \"Bell's palsy\", \"idiopathic facial nerve palsy\", \n        \"Neurofibromatosis\", \"NF\", \n        \"Tuberous sclerosis\", \"Bourneville disease\", \"TSC\", \n        \"Sturge-Weber syndrome\", \"encephalotrigeminal angiomatosis\", \n        \"Alexander disease\", \n        \"Canavan disease\", \n        \"Krabbe disease\", \"globoid cell leukodystrophy\", \n        \"Metachromatic leukodystrophy\", \"MLD\", \n        \"Adrenoleukodystrophy\", \"ALD\", \n        \"Werdnig-Hoffmann disease\",  “SMA Type I” \n        \"Kennedy's disease\", \"spinal and bulbar muscular atrophy\", \"SBMA\", \n        \"Wilson's disease\", \"hepatolenticular degeneration\", \n        \"Neuroacanthocytosis\", \"Levine-Critchley syndrome\", \n        \"Subacute sclerosing panencephalitis\", \"SSPE\", \n        \"Progressive encephalopathy\", \n        \"Aphasia\", \"dysphasia\", \n        \"Agnosia\", \n        \"Apraxia\", \"dyspraxia\", \n        \"Locked-in syndrome\", \n        \"Guillain-Barré variants\", \"Miller Fisher syndrome\", \"MFS\", \n        \"Sarcoidosis\", \"neurosarcoidosis\", \n        \"Behçet's disease\", \"neuro-Behçet's\", \n        \"Bickerstaff brainstem encephalitis\", \n        \"Acoustic neuroma\", \"vestibular schwannoma\", \n        \"Atypical parkinsonism\", \"Parkinson-plus syndrome\", \n        \"Essential palatal tremor\", \n        \"Orthostatic tremor\", \n        \"Hereditary spastic paraplegia\", \"HSP\", \"familial spastic paraparesis\", \n"}, {"page": 114, "text": "100 \n \n        \"Paramyotonia congenita\", \n        \"Periodic paralysis\", \"hypokalemic periodic paralysis\", \"hyperkalemic periodic \nparalysis\", \n        \"Myotonic dystrophy\", \"DM\", \"Steinert's disease\", \n        \"Paraneoplastic neurological syndromes\", \"PNS\", \n        \"Gerstmann-Sträussler-Scheinker syndrome\", \"GSS\", \n        \"Leukoencephalopathy\", \n        \"Niemann-Pick disease type C\", \n        \"Tay-Sachs disease\", \n        \"Sandhoff disease\", \n        \"Neurodegeneration with brain iron accumulation\", \"NBIA\", \"Hallervorden-Spatz \ndisease\", \n        \"Phenylketonuria\", \"PKU\", \n        \"Subacute combined degeneration\", \n        \"Toxic encephalopathy\", \n        \"Wernicke encephalopathy\", \n        \"Korsakoff syndrome\", \"Wernicke-Korsakoff syndrome\", \n        \"Marchiafava-Bignami disease\", \n        \"Hashimoto's encephalopathy\", \n        \"Posterior reversible encephalopathy syndrome\", \"PRES\", \n        \"Fibromyalgia\", \n        \"Chronic fatigue syndrome\", \"ME/CFS\", \"myalgic encephalomyelitis\", \n        \"Hyperekplexia\", \"startle disease\", \n        \"Pudendal neuropathy\", \n        \"Temporal arteritis\", \"giant cell arteritis\", \n        \"Reversible cerebral vasoconstriction syndrome\", \"RCVS\", \n        \"Carpal tunnel syndrome\", \n        \"Cubital tunnel syndrome\", \n        \"Thoracic outlet syndrome (neurogenic)\", \n        \"Meralgia paresthetica\", \"lateral femoral cutaneous neuropathy\", \n        \"Spinal stenosis\", \"neurogenic claudication\", \n        \"Radiculopathy\", \"radicular pain\", \n        \"Facet arthropathy\", \n        \"Syringobulbia\", \n        \"Brown-Séquard syndrome\", \n        \"Central cord syndrome\", \n        \"Anterior cord syndrome\", \n        \"Subacute sclerosing encephalitis\",   \n        \"Botulism\", \n        \"Toxic neuropathy\", \n        \"Heavy metal neuropathy\", \"lead neuropathy\", \"mercury neuropathy\", \n        \"Reye's syndrome\", \n        \"Vogt-Koyanagi-Harada disease\", \n"}, {"page": 115, "text": "101 \n \n        \"Susac syndrome\", \n        \"Stargardt disease (neurological implications)\", \n        \"Usher syndrome\", \n        \"Optic neuritis\", \n        \"Alpers’ disease\", \"Alpers’ syndrome\", \n        \"Refsum disease\", \n        \"Canal dehiscence syndrome\", \"superior semicircular canal dehiscence\", \n        \"Barre-Lieou syndrome\", \n        \"Ehlers-Danlos syndrome (neurological complications)\", \n        \"Marfan syndrome (neurological complications)\", \n        \"Zellweger syndrome\", \n        \"Agenesis of the corpus callosum\", \n        \"Encephalocele\", \n        \"Anencephaly\", \n        \"Spina bifida\", \n        \"Subdural hematoma\", \n        \"Epidural hematoma\", \n        \"Concussion\", \"mild traumatic brain injury\", \"TBI\", \n        \"Chronic traumatic encephalopathy\", \"CTE\", \n        \"Diffuse axonal injury\", \n        \"Pseudotumor cerebri\", \"idiopathic intracranial hypertension\", \"IIH\", \n        \"Alice in Wonderland syndrome\", \"AIWS\", \n        \"Synesthesia\", \"synaesthesia\", \n        \"Phantom limb pain\", \"phantom limb syndrome\", \n        \"Fetal alcohol syndrome (neurological defects)\", \n        \"Chemotherapy-induced neuropathy\", \n        \"Radiation-induced neuropathy\", \"radiation encephalopathy\", \n        \"Neurotoxicity\", \n        \"Neuroleptic malignant syndrome\", \"NMS\", \n        \"Serotonin syndrome\", \n        \"Tardive dyskinesia\", \n        \"Akathisia\", \n        \"Antiphospholipid syndrome (neurological manifestations)\", \n        \"Fabry disease (neurological involvement)\", \n        \"Cerebrotendinous xanthomatosis\", \"CTX\", \n        \"MELAS (Mitochondrial encephalomyopathy, lactic acidosis, and stroke-like \nepisodes)\", \n        \"MERRF (Myoclonic epilepsy with ragged red fibers)\", \n        \"Leigh syndrome\", \"subacute necrotizing encephalomyelopathy\", \n        \"Kearns-Sayre syndrome\", \n        \"SANDO (Sensory ataxic neuropathy, dysarthria, and ophthalmoparesis)\", \n        \"Wolfram syndrome\", \"DIDMOAD\", \n        \"Basal ganglia calcification\", \"Fahr's disease\", \n"}, {"page": 116, "text": "102 \n \n        \"Neurocysticercosis\", \n        \"Cerebral malaria\", \n        \"Sleeping sickness\", \"African trypanosomiasis\", \n        \"Kuru\", \n        \"New variant CJD\", \"vCJD\", \n        \"Foix-Chavany-Marie syndrome\", \n        \"Rasmussen's encephalitis\", \n        \"Hemimegalencephaly\", \n        \"Neonatal encephalopathy\", \"hypoxic-ischemic encephalopathy\", \n        \"Neuroblastoma (paraneoplastic)\", \n        \"Opsoclonus-myoclonus-ataxia\", \n        \"Neurodegeneration with Lewy bodies\", \n        \"Hypomyelination disorders\", \n        \"Paroxysmal kinesigenic dyskinesia\", \"PKD\", \n        \"Paroxysmal nonkinesigenic dyskinesia\", \"PNKD\", \n        \"Alternating hemiplegia of childhood\", \n        \"Abetalipoproteinemia (neurological complications)\", \n        \"Foix-Alajouanine syndrome\", \"subacute necrotic myelopathy\", \n        \"Hirayama disease\", \"monomelic amyotrophy\", \n        \"Floppy baby syndrome\", \n        \"Pompe disease\", \"GSD II\", \n        \"Mitochondrial myopathy\", \n        \"Glycogen storage disease type IV\", \"Andersen's disease\", \n        \"Hyperammonemia (neurological manifestations)\", \n        \"Maple syrup urine disease\", \n        \"Urea cycle disorders (neurological manifestations)\", \n        \"TRAPS syndrome\", \"Tumor necrosis factor receptor–associated periodic \nsyndrome\", \n        \"Cogan's syndrome (neurological involvement)\", \n        \"Retinal vasculopathy with cerebral leukodystrophy\", \"RVCL\", \n        \"Sneddon's syndrome\", \n        \"Takayasu arteritis (neurological complications)\", \n        \"Cerebral amyloid angiopathy\", \n        \"Binswanger's disease\", \"subcortical leukoencephalopathy\", \n        \"Neuro-ichthyosis\", \n        \"Gaucher disease (neuronopathic forms)\", \n        \"Parkinsonism-dementia complex of Guam\", \"Lytico-Bodig disease\", \n        \"Farber disease (neurologic involvement)\", \n        \"Adrenomyeloneuropathy\", \"AMN\", \n        \"Pelizaeus-Merzbacher disease\", \"PMD\", \n        \"Hereditary diffuse leukoencephalopathy with axonal spheroids\", \n        \"Aicardi syndrome\", \n        \"Goldenhar syndrome (neurologic involvement)\", \n"}, {"page": 117, "text": "103 \n \n        \"Moebius syndrome (facial nerve involvement)\", \n        \"Klippel-Feil syndrome\", \n        \"CHARGE syndrome\", \n        \"Pitt-Hopkins syndrome\", \n        \"Phelan-McDermid syndrome\", \"22q13 deletion syndrome\", \n        \"Smith-Magenis syndrome\", \n        \"Prader-Willi syndrome\", \n        \"Velo-cardio-facial syndrome\", \"22q11.2 deletion syndrome\", \n        \"Williams syndrome\", \n        \"Down syndrome (neurological aspects)\", \n        \"Trisomy 18\", \"Edwards syndrome\", \n        \"Trisomy 13\", \"Patau syndrome\", \n        \"Hyperekplexia (startle disease)\",  \n        \"Neonatal seizures\", \n        \"Infantile spasms (West syndrome)\",  \n        \"Benign myoclonus of early infancy\", \n        \"Benign familial neonatal convulsions\", \n        \"Ohtahara syndrome\", \"early infantile epileptic encephalopathy\", \n        \"Doose syndrome\", \"myoclonic-astatic epilepsy\", \n        \"Eclampsia\", \n        \"Transient global amnesia\", \n        \"Amnestic syndromes\", \n        \"Demyelinating polyneuropathy\", \n        \"Glossopharyngeal neuralgia\", \n        \"Post-polio syndrome\", \n        \"Hereditary spastic paraplegia (repeated synonyms)\", \n        \"Motor neuron disease\",   \n        \"Primary lateral sclerosis\", \"PLS\", \n        \"Progressive muscular atrophy\", \"PMA\", \n        \"Bulbar palsy\", \"pseudobulbar palsy\", \n        \"Polyradiculopathy\", \n        \"Cauda equina syndrome\", \n        \"Conus medullaris syndrome\", \n        \"Brain ischemia\", \"cerebral ischemia\", \n        \"White matter disease\", \n        \"Leukodystrophy\", \n        \"Subcortical arteriosclerotic encephalopathy\", \n        \"Neurodegenerative disease\",  \n        \"Movement disorder\",  \n        \"Spastic paraplegia\",   \n        \"Neurodevelopmental disorder\",  \n        \"Neurocutaneous syndrome\",  \n        \"Inborn error of metabolism (neurological involvement)\", \n"}, {"page": 118, "text": "104 \n \n        \"Lysosomal storage disease (neurological involvement)\", \n        \"Ulnar neuropathy\", \n        \"Tarsal tunnel syndrome\", \n        \"Meralgia paresthetica (lateral femoral cutaneous neuropathy)\",   \n        \"Complex migraine\", \"basilar-type migraine\", \"hemiplegic migraine\", \n        \"Basilar migraine\", \"basilar-type migraine\",  \n        \"Ophthalmoplegic migraine\", \n        \"Retinal migraine\", \n        \"Post-traumatic headache\", \n        \"Medication overuse headache\", \"MOH\", \"rebound headache\", \n        \"Opioid-induced hyperalgesia\", \n        \"Sensorineural hearing loss (neurological basis)\", \n        \"Vertigo\", \"labyrinthitis\", \"vestibular neuritis\", \n        \"Meniere's disease\", \"endolymphatic hydrops\", \n        \"Cholesteatoma (intracranial extension)\", \n        \"Neuro-ophthalmologic disorders\", \n        \"Leber's hereditary optic neuropathy\", \"LHON\", \n        \"Batten disease\", \"neuronal ceroid lipofuscinosis\", \"NCL\", \"CLN1 disease\", \"CLN2 \ndisease\", \n        \"Galactosemia (neurological complications)\", \n        \"Dejerine-Sottas disease\", \"hereditary motor and sensory neuropathy type III\", \n        \"Acute flaccid myelitis\", \"AFM\", \n        \"West Nile neurological syndrome\", \n        \"Zika-associated neuropathy\", \"microcephaly\", \n        \"COVID-19 neurological complications\", \n        \"MERS neurological complications\", \n        \"SARS neurological complications\", \n        \"HIV-associated neurocognitive disorder\", \"HAND\", \n        \"Neurosyphilis\", \n        \"Neuro-Behçet's disease\",  \n        \"Whipple's disease (neurological)\", \n        \"Celiac disease (neurological manifestations)\", \"gluten ataxia\", \"celiac neuropathy\", \n        \"Isaacs' syndrome\", \"continuous muscle fiber activity syndrome\", \n        \"Neuromyotonia\", \n        \"Rippling muscle disease\", \n        \"Dermatome pain\", \"radicular syndrome\", \n        \"Herpes zoster\", \"shingles (neurological complications)\", \n        \"Von Hippel-Lindau disease\", \"hemangioblastomas\", \n        \"Marinesco-Sjögren syndrome\", \n        \"Cogan’s syndrome\", \n        \"Neurotuberculosis\", \"tuberculous meningitis\", \n        \"Brucellosis (neurobrucellosis)\", \n        \"Coccidioidomycosis (neurological)\", \n"}, {"page": 119, "text": "105 \n \n        \"Histoplasmosis (neurohistoplasmosis)\", \n        \"Fatal familial insomnia\", \"FFI\", \n        \"Neurolymphomatosis\", \n        \"Lymphomatoid granulomatosis (CNS involvement)\", \n        \"Progressive rubella panencephalitis\", \n        \"Parry-Romberg syndrome\", \"hemifacial atrophy\", \n        \"Hemifacial spasm\", \n        \"Trigeminal autonomic cephalalgias\", \"TACs\", \n        \"SUNCT syndrome\", \"Short-lasting unilateral neuralgiform headache with \nconjunctival injection and tearing\", \n        \"SUNA syndrome\", \"Short-lasting unilateral neuralgiform headache attacks with \ncranial autonomic symptoms\", \n        \"Paroxysmal hemicrania\", \n        \"Hypnic headache\", \n        \"Vestibular migraine\", \n        \"Abdominal migraine\", \n        \"Status migrainosus\", \n        \"New daily persistent headache\", \"NDPH\", \n        \"Psychomotor retardation\", \n        \"Catatonia\", \n        \"Tropical spastic paraparesis\", \"HTLV-1 associated myelopathy\", \n        \"Diabetic amyotrophy\", \n        \"Diabetic autonomic neuropathy\", \n        \"Diabetic radiculoplexus neuropathy\", \n        \"Acute hemorrhagic leukoencephalitis\", \"Hurst disease\", \n        \"Neurologic paraneoplastic syndromes\", \n        \"Omenn syndrome (neurological complications)\", \n        \"Acute flaccid paralysis\", \n        \"Spinal cord injury\", \"SCI\", \n        \"Anoxic brain injury\", \"global hypoxic-ischemic injury\", \n        \"Cerebral edema\", \n        \"Brain herniation syndromes\", \n        \"Sundowning (Alzheimer's-related)\", \n        \"Pellagra (niacin deficiency encephalopathy)\", \n        \"Leprosy (Hansen’s disease, neuropathy)\", \n        \"Tropical ataxic neuropathy\", \n        \"Konzo (epidemic spastic paraparesis)\", \n        \"Lathyrism\", \n        \"Pesticide-induced neuropathy\", \n        \"Organophosphate neuropathy\", \n        \"Carcinomatous meningitis\", \"leptomeningeal carcinomatosis\", \n        \"Gliomatosis cerebri\", \n"}, {"page": 120, "text": "106 \n \n        \"POEMS syndrome\", \"Polyneuropathy, Organomegaly, Endocrinopathy, M-protein, \nSkin changes\", \n        \"Waldenström macroglobulinemia (neurological)\", \n        \"Monoclonal gammopathy of undetermined significance neuropathy\", \"MGUS \nneuropathy\", \n        \"CRION (chronic relapsing inflammatory optic neuropathy)\", \n        \"Neuromyelitis optica spectrum disorder\", \"NMOSD\", \n        \"MOG antibody disease\", \"myelin oligodendrocyte glycoprotein disorder\", \n        \"Autoimmune encephalitis\", \"anti-NMDA receptor encephalitis\", \n        \"Limbic encephalitis\", \n        \"Basal ganglia encephalitis\", \n        \"Voltage-gated potassium channel complex antibody encephalitis\", \"VGKC \nencephalitis\", \n        \"Glutamic acid decarboxylase antibody neurologic syndromes\", \"GAD antibody \nsyndromes\", \n        \"CASPR2 antibody encephalitis\", \"Contactin-associated protein-like 2 encephalitis\", \n        \"Morvan syndrome\", \n        \"PANDAS (pediatric autoimmune neuropsychiatric disorders associated with \nstrep)\", \n        \"Sydenham chorea\", \"rheumatic chorea\", \n        \"Chorea gravidarum\", \n        \"Hemiballismus\", \n        \"Neuroferritinopathy\", \n        \"Benign hereditary chorea\", \n        \"Dentatorubral-pallidoluysian atrophy\", \"DRPLA\", \n        \"Spastic dystonia\", \n        \"Kayser-Fleischer ring (Wilson's disease sign)\",  \n        \"Oromandibular dystonia\", \n        \"Blepharospasm\", \n        \"Cervical dystonia\", \"torticollis\", \n        \"Writer's cramp\", \"focal hand dystonia\", \n        \"Alien hand syndrome\", \n        \"Conduction aphasia\", \n        \"Broca's aphasia\", \"expressive aphasia\", \n        \"Wernicke's aphasia\", \"receptive aphasia\", \n        \"Global aphasia\", \n        \"Prosopagnosia\", \"face blindness\", \n        \"Balint's syndrome\", \n        \"Gerstmann syndrome\", \n        \"Phineas Gage syndrome\", \"frontal lobe syndrome\", \n        \"Temporal lobe syndrome\", \"Kluver-Bucy syndrome\", \n        \"Papez circuit lesions\", \"amnestic syndromes\", \n        \"Transient epileptic amnesia\", \n"}, {"page": 121, "text": "107 \n \n        \"Psychogenic amnesia\", \n        \"Electroconvulsive therapy-induced amnesia\", \n        \"Mild cognitive impairment\", \"MCI\", \n        \"Disorders of consciousness\", \"coma\", \"vegetative state\", \"minimally conscious \nstate\", \n        \"Brain death\", \n        \"Hypersomnia\", \n        \"Kleine-Levin syndrome\", \"recurrent hypersomnia\", \n        \"Periodic limb movement disorder\", \"PLMD\", \n        \"Sleep apnea\", \"obstructive sleep apnea\", \"OSA\", \"central sleep apnea\", \"CSA\", \n        \"Cataplexy\", \n        \"Circadian rhythm sleep disorder\", \n        \"Somnambulism\", \"sleepwalking\", \n        \"Night terrors\", \"pavor nocturnus\", \n        \"Insomnia\", \n        \"Parasomnias\", \n        \"X-linked adrenoleukodystrophy\", \"X-ALD\", \n        \"Spinal cord infarction\", \n        \"Spinal arteriovenous malformation\", \n        \"Scoliosis (neurological involvement)\", \n        \"Hydromyelia\", \n        \"Diastematomyelia\", \n        \"Basilar invagination\", \n        \"Foramen magnum stenosis\", \n        \"Kugelberg-Welander disease (SMA type III)\", \n        \"Aran-Duchenne muscular atrophy\", \n        \"Scapuloperoneal spinal muscular atrophy\", \n        \"Emery-Dreifuss muscular dystrophy\", \n        \"Myotonia congenita\", \"Thomsen disease\", \"Becker type\", \n        \"Facioscapulohumeral muscular dystrophy\", \"FSHD\", \n        \"Limb-girdle muscular dystrophy\", \"LGMD\", \n        \"Distal muscular dystrophy\", \n        \"Oculopharyngeal muscular dystrophy\", \"OPMD\", \n        \"Inclusion body myositis\", \"IBM\", \n        \"Necrotizing myopathy\", \n        \"Infectious myositis\", \n        \"Acute rhabdomyolysis with neuropathy\", \n        \"Episodic ataxia\", \n        \"Spasmodic dysphonia\", \"laryngeal dystonia\", \n        \"Tardive dystonia\", \n        \"Septo-optic dysplasia\", \n        \"Joubert syndrome\", \n        \"Dandy-Walker syndrome\", \n"}, {"page": 122, "text": "108 \n \n        \"Arachnoid cyst\", \n        \"Colloid cyst\", \n        \"Craniopharyngioma\", \n        \"Pineal tumor\", \"pinealoma\", \n        \"Pituitary adenoma (neurological involvement)\", \n        \"Hypothalamic hamartoma\", \n        \"Choroid plexus papilloma\", \n        \"Hydranencephaly\", \n        \"Holoprosencephaly\", \n        \"Schizencephaly\", \n        \"Lissencephaly\", \n        \"Pachygyria\", \n        \"Polymicrogyria\", \n        \"Heterotopia (neuronal migration disorder)\", \n        \"Dural arteriovenous fistula\", \n        \"Cerebral venous sinus thrombosis\", \"CVST\", \n        \"Budd-Chiari syndrome (neurological involvement)\", \n        \"Central pontine myelinolysis\", \"osmotic demyelination syndrome\", \n        \"Extrapontine myelinolysis\", \n        \"Radiation myelopathy\", \n        \"Iatrogenic neurological disorders\", \n        \"Hemicraniectomy complications\", \n        \"Hyperperfusion syndrome (post-carotid endarterectomy)\", \n        \"Scorpion venom neuropathy\", \n        \"Snake bite neurotoxicity\", \n        \"Tick paralysis\", \n        \"Spider envenomation (e.g., black widow neurotoxicity)\", \n        \"Shellfish toxin-induced neuropathy\", \"paralytic shellfish poisoning\", \n        \"Ciguatera poisoning (neurological)\", \n        \"Scombroid poisoning\", \n        \"Thrombotic thrombocytopenic purpura (neurological involvement)\", \"TTP\", \n        \"Shaken baby syndrome\", \"abusive head trauma\" \n     \n \n"}]}