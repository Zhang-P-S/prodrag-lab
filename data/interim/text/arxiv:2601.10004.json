{"doc_id": "arxiv:2601.10004", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.10004.pdf", "meta": {"doc_id": "arxiv:2601.10004", "source": "arxiv", "arxiv_id": "2601.10004", "title": "SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations", "authors": ["Mohoshin Ara Tahera", "Karamveer Singh Sidhu", "Shuvalaxmi Dass", "Sajal Saha"], "published": "2026-01-15T02:28:57Z", "updated": "2026-01-15T02:28:57Z", "summary": "Large Language Models (LLMs) are increasingly adopted in healthcare to support clinical decision-making, summarize electronic health records (EHRs), and enhance patient care. However, this integration introduces significant privacy and security challenges, driven by the sensitivity of clinical data and the high-stakes nature of medical workflows. These risks become even more pronounced across heterogeneous deployment environments, ranging from small on-premise hospital systems to regional health networks, each with unique resource limitations and regulatory demands. This Systematization of Knowledge (SoK) examines the evolving threat landscape across the three core LLM phases: Data preprocessing, Fine-tuning, and Inference within realistic healthcare settings. We present a detailed threat model that characterizes adversaries, capabilities, and attack surfaces at each phase, and we systematize how existing privacy-preserving techniques (PPTs) attempt to mitigate these vulnerabilities. While existing defenses show promise, our analysis identifies persistent limitations in securing sensitive clinical data across diverse operational tiers. We conclude with phase-aware recommendations and future research directions aimed at strengthening privacy guarantees for LLMs in regulated environments. This work provides a foundation for understanding the intersection of LLMs, threats, and privacy in healthcare, offering a roadmap toward more robust and clinically trustworthy AI systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.10004v1", "url_pdf": "https://arxiv.org/pdf/2601.10004.pdf", "meta_path": "data/raw/arxiv/meta/2601.10004.json", "sha256": "4577b9446ac83995facf36c7809d5a20a91bddb5530013bb5bfc52fbeec07dcb", "status": "ok", "fetched_at": "2026-02-18T02:21:31.797086+00:00"}, "pages": [{"page": 1, "text": "SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques,\nChallenges and Recommendations\nMohoshin Ara Tahera∗, Karamveer Singh Sidhu†, Shuvalaxmi Dass∗, Sajal Saha†\n∗University of Louisiana at Lafayette, Lafayette, LA, USA\n†University of Northern British Columbia, Canada\nEmails: mohoshin-ara.tahera1,shuvalaxmi.dass@louisiana.edu\nksidhu,sajal.saha@unbc.ca\nAbstract—Large Language Models (LLMs) are increasingly\nadopted in healthcare to support clinical decision-making,\nsummarize electronic health records (EHRs), and enhance\npatient care. However, this integration introduces significant\nprivacy and security challenges, driven by the sensitivity of\nclinical data and the high-stakes nature of medical work-\nflows. These risks become even more pronounced across\nheterogeneous deployment environments, ranging from small\non-premise hospital systems to regional health networks,\neach with unique resource limitations and regulatory de-\nmands. This Systematization of Knowledge (SoK) exam-\nines the evolving threat landscape across the three core\nLLM phases: Data preprocessing, Fine-tuning, and Inference\nwithin realistic healthcare settings. We present a detailed\nthreat model that characterizes adversaries, capabilities,\nand attack surfaces at each phase, and we systematize\nhow existing privacy-preserving techniques (PPTs) attempt\nto mitigate these vulnerabilities. While existing defenses\nshow promise, our analysis identifies persistent limitations\nin securing sensitive clinical data across diverse operational\ntiers. We conclude with phase-aware recommendations and\nfuture research directions aimed at strengthening privacy\nguarantees for LLMs in regulated environments. This work\nprovides a foundation for understanding the intersection\nof LLMs, threats, and privacy in healthcare, offering a\nroadmap toward more robust and clinically trustworthy AI\nsystems.\nIndex Terms—LLM, healthcare, Privacy-preserving tech-\nniques, Threat model\n1. Introduction\nLLMs are increasingly integrated into healthcare for\nclinical documentation, decision support, radiology and\npathology report summarization, and clinician–patient\ncommunication (representative applications in Table 1).\nDeployments span radiology/report summarization [1],\n[2], triage and clinical decision support (CDS) systems [3],\n[4], and multilingual dialogue assistants [5], [6]. While\nthese applications demonstrate substantial utility, they also\nintroduce significant privacy risks due to the sensitivity of\nProtected Health Information (PHI) and stringent regula-\ntory constraints.\nThis SoK examines privacy concerns across the three\ncore operational phases of LLMs: data preprocessing, fine-\ntuning, and inference, with a specific focus on text-centric\nhealthcare applications (e.g., EHR notes, discharge sum-\nmaries, DICOM-derived reports, pathology and radiology\nnarratives, and clinician–patient transcripts). We intention-\nally scope the SoK to text-generating or text-consuming\nLLM pipelines, excluding imaging-only models unless\nthey interact with PHI-bearing textual artifacts.\nUnlike prior surveys [7]–[12], which primarily cat-\nalog techniques or discuss privacy at a high level, our\nwork provides a phase-aligned systematization. We unify\nterminology and explicitly map: attack surfaces→enabled\nattacks→defenses→remaining limitations grounded in\nhealthcare data artifacts such as EHR/HL7/FHIR struc-\ntures, DICOM text fields, and clinical transcripts. Prior\nwork does not systematically link adversary capabilities\nto specific vulnerabilities across the three phases; Table\n2 highlights these gaps. Our corpus spans peer-reviewed\nstudies and authoritative preprints from 2020–2025 involv-\ning clinical datasets, hospital deployments, or consortium-\nbased learning; details appear in section 2.\nThis SoK answers three guiding research questions in\neach phase:\n• RQ1.How should adversaries in healthcare LLMs\nbe categorized (e.g., internal vs. external), and how\ndo their capabilities and prior knowledge shape the\nattack vectors that emerge at each phase of the LLM\nlifecycle?\n• RQ2. How effectively do current privacy-preserving\ntechniques mitigate phase-specific vulnerabilities?\n• RQ3. What limitations remain in current defenses,\nand what phase-aware strategies can guide future\nprivacy-enhanced LLM development?\nTo address RQ1, we construct a detailed threat model\nfor each phase, identifying adversary capabilities, prior\nknowledge, and corresponding vulnerabilities. For RQ2,\nwe evaluate privacy-preserving techniques and analyze\ntheir effectiveness relative to the identified phase-specific\nthreats. For RQ3, we synthesize limitations and propose\nfuture directions and recommendations for phase-aware,\nthreat-resilient, healthcare-specific privacy enhancements.\nGiven that healthcare deployments commonly rely on\ndistributed architectures, our analysis of the fine-tuning\nstage adopts a Federated Learning (FL) framework, em-\nphasizing vulnerabilities and defenses in the Federated\nFine-Tuning Phase. By answering these questions, we\nmake the following three contributions:\n1) Phase-Specific Threat Model: We develop a com-\nprehensive threat model for healthcare LLMs, catego-\narXiv:2601.10004v1  [cs.CR]  15 Jan 2026\n"}, {"page": 2, "text": "rizing adversaries by location (internal vs. external)\nand capability and identifying attack surfaces and key\nvulnerabilities across data preprocessing, federated\nfine-tuning, and inference (e.g., gradient leakage, in-\nternal update exposure, model extraction).\n2) Evaluation of Privacy-Preserving Techniques: We\nsystematically analyze defenses such as differential\nprivacy, secure aggregation, inference-time mitiga-\ntions and evaluate how effectively they address the\nvulnerabilities surfaced in each phase. Our findings\nshow that existing techniques provide partial protec-\ntion but are often phase-agnostic or misaligned with\nhealthcare-specific threats.\n3) Limitations and Future Directions: We identify\ncritical limitations in current privacy strategies and\npropose future research directions aimed at de-\nveloping phase-aware, threat-resilient, and privacy-\nenhanced LLMs tailored for healthcare environments.\nThese include recommendations such as standard-\nized data anonymization, adaptive differential privacy\nto mitigate gradient leakage while preserving rare-\ndisease fidelity in federated fine-tuning.\nFollowing the literature collection methodology (Sec-\ntion 2), our SoK is structured into three sections, each\nfocused on a distinct phase of the LLM lifecycle: Data\nPreprocessing (Section 3), Federated Fine-Tuning (Section\n4), and Inference (Section 5). Each section includes two\nsubsections: The Threat Model subsection addresses RQ1,\noutlining adversaries, capabilities, and attack vectors. The\nPrivacy Preserving Defenses subsection covers the rest of\nthe RQs: RQ2 with Takeaways on how existing privacy-\npreserving techniques mitigate vulnerabilities, and RQ3\nwith Limitations of current defenses and Recommenda-\ntions. To synthesize insights across phases, we present\nTables 3, 4 and 5, which systematically maps the threats\nand vulnerabilities of each phase to their corresponding\ndefenses, limitations, and recommendations, offering a\nconcise, phase-specific roadmap for privacy-aware LLMs.\n2. Literature Collection\nThis SoK follows a structured but targeted methodol-\nogy aimed at systematizing privacy and security risks for\nhealthcare LLMs across the three operational phases: data\npreprocessing, federated fine-tuning, and inference, priori-\ntizing works that directly engage with privacy, security, or\ncompliance rather than providing exhaustive coverage of\nall clinical LLM applications. We searched IEEE Xplore,\nACM DL, SpringerLink, ScienceDirect, PubMed, and ma-\njor preprint repositories (arXiv, medRxiv) for the years\n2019–2025 using combined terms spanning model types\n(“LLM,” “large language model,” “foundation model”),\nhealthcare domains (“medical,” “clinical,” “EHR,” “ra-\ndiology,” “pathology,” “telemedicine”), and privacy/secu-\nrity mechanisms (“privacy,” “differential privacy,” “feder-\nated learning,” “secure aggregation,” “homomorphic en-\ncryption,” “access control”). Searches were supplemented\nwith backward and forward snowballing from influential\nsurveys on LLM privacy and healthcare AI. We included\npapers that (i) involve LLMs or closely related foundation\nmodels or core privacy-preserving mechanisms relevant\nto healthcare (e.g., DP, FL, HE/MPC, anonymization,\naccess control); (ii) use or explicitly target healthcare data\nor clinical workflows such as EHRs, imaging, pathol-\nogy, telemedicine, or medical QA; and (iii) provide a\nsubstantive treatment of privacy, security, or regulatory\nconstraints, including threat models, attacks, defenses, or\nevaluations. We excluded purely application-focused clin-\nical LLM papers with only passing references to privacy,\nas well as generic privacy/security works lacking a clear\nconnection to healthcare or LLMs.\n3. Data Preprocessing Phase\nThe preprocessing phase involves cleaning, structur-\ning, and transforming raw clinical data to address issues\nlike missing values, feature normalization, class imbal-\nance, and skewed distributions [13], [14]. In healthcare,\nthis phase is critical due to the sensitivity and diversity\nof data from EHRs, medical imaging, claims, and patient-\ngenerated sources.\n3.1. Threat Model\nThe data preprocessing stage ingests heterogeneous\nclinical data including EHR tables, radiology metadata,\nlab results, claims records, and patient-generated con-\ntent—where identifiers and quasi-identifiers remain intact\n[34], [35]. Because adversaries interact with the raw sub-\nstrate from which LLM datasets are formed, this phase\npresents uniquely powerful opportunities for privacy com-\npromise and data manipulation [36], [37].\n3.1.1. Attacker Landscape and Incentives. Preprocess-\ning threats stem from both internal and external adver-\nsaries, whose incentives and privileges shape distinct pri-\nvacy risks.\nInternal adversaries are the most operationally im-\npactful, as they handle raw data throughout daily work-\nflows. (1) Clinical staff (nurses, residents, and attending\nphysicians) may export EHR data for shift handovers or\naudits through informal channels, unintentionally expos-\ning unmasked identifiers and clinical notes [35]. (2) Data\nengineers and ML researchers maintaining ETL and data-\ncleaning pipelines often retain identifiers for debugging or\nconvenience, propagating PHI leakage into intermediate\ntables and logs [38]–[41]. (3) System administrators and\nIT personnel possess privileged access to storage servers,\nbackups, and preprocessing nodes. Misconfigured access\ncontrols or network shares can silently expose PHI to\nunauthorized internal users or external exploits [39].\nExternal adversaries target the same infrastructures\nbut with financial, strategic, or competitive motives. (1)\nRansomware groups exploit unpatched or misconfigured\nETL servers and data lakes, as illustrated by the WannaCry\nattack on the NHS [42]. (2) Corporate or state-linked\nattackers infiltrate healthcare data lakes to harvest PHI for\nanalytics or commercial leverage [9], [36]. (3) Research\ncompetitors may compromise preprocessing APIs or stor-\nage connectors to reconstruct cohorts or infer institutional\npractices [43]–[45].\n3.1.2. Prior Knowledge and Capability Gradient. Ad-\nversaries in this phase often hold strong prior knowl-\nedge of healthcare data systems, making even limited\naccess highly dangerous, such as: (1) They understand\n"}, {"page": 3, "text": "TABLE 1: Representative Healthcare Applications of LLMs with example tasks\nApplication Type\nExample Tasks\nElectronic Health Records (EHR)\nDemographics, diagnoses, labs, ICU notes & pathology (MIMIC, n2c2) [13]–[15]; insurance\nclaims [3]; rare cohorts [16]; synthetic EHRs for comorbidity/survival modeling [17]–[19]\nClinical Summaries & Documentation\nDischarge-note assistants [4], [20]; cardiac/oncology summaries [16], [21]; multilingual notes\n[5], [22]; ICU timelines [23]; leakage risks (telemedicine) [24], [25]\nMedical Imaging & Signal Data\nRadiology summarization (CT, X-ray) [1], [2]; pathology summarizers [6], [21]; ChestX-\nray14 [8]; cardiology data (ECG, BP, cholesterol, EF) [2], [20]\nClinical Decision Support (CDS)\nRule-based CDS (guidelines) [26]; oncology classification [21], [27]; cardiovascular outcome\nprediction [3], [4]; comorbidity prediction (graph prompting, RAG) [19]; multimodal support\n[27], [28]\nTelemedicine & Patient Interaction\nTriage chatbots [29], [30]; telemedicine dialogues [24]; multilingual transcripts [5], [6]; DSS\nfor rare tumor queries [31], [32]; mobile health apps (chest pain triage) [20], [33]\nTABLE 2: Comparison of healthcare-focused surveys vs. our SoK. Legend: ★= strong/unique/comprehensive; ✓=\npresent/addressed; G = Partial; # = Narrow/absent; Full = Pre-Processing →Federated Fine-Tuning →Inference\nCriteria\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\nOur SoK\nStages Covered\nGPartial\nGPartial\nGPartial\nGPartial\nGPartial\nGPartial\n★Full\nThreat Model\n# Narrow\n# Narrow\n# Narrow\n# Narrow\nG Partial\nG Partial\n★Comprehensive,\nphase-\nmapped\nDefenses / PPTs\n✓Partial\n✓Partial\n# Narrow\n✓Partial\n✓Partial\n✓Partial\n★Tailored + Phase-mapped\nLimits of Defenses\n✓Present\n# Absent\n# Absent\n# Absent\n✓Present\n✓Present\n★Phase-tied\nRecommendations\n✓Present\n✓Present\n✓Present\n# Absent\n✓Present\n✓Present\n★Gap-tied, phase-specific\nEHR schemas and identifiers (HL7/FHIR fields, MRNs,\ntimestamps, and ICD codes), enabling precise targeting\nof PHI within raw exports [13], [14], [35]. (2) They\nexploit quasi-identifiers such as age, location, ethnicity,\nor rare conditions for re-identification and linkage be-\nfore anonymization [34], [46]. (3) They are familiar with\nETL workflows and data handling practices, including\nwhere staging files, failed job outputs, and temporary\nexports are stored [38], [39]. (4) Such insider knowledge\ndirectly enables exploitation of weakly protected APIs\nand misconfigured file systems that connect preprocessing\ntools to hospital databases. (5) High-resource externals\nalso combine auxiliary datasets such as insurance claims,\nleaked registries, and prior hospital breaches to strengthen\nre-identification or model poisoning efforts [47].\nThe capability spectrum spans: (1) low-resource insid-\ners (e.g., nurses, clerks) can unintentionally leak identifiers\nthrough manual exports or shift handover lists. [34], [35];\n(2) moderate-resource insiders (e.g., ETL engineers, ML\nstaff) handle staging data and debugging logs containing\nunmasked PHI. [40], [41]; and (3) high-resource externals\n(e.g., ransomware groups, APTs) exploit insecure APIs,\nexfiltrate backups, or inject poisoned HL7/FHIR records\nto compromise downstream model integrity. [48], [49].\n3.1.3. Attack Surface Vulnerabilities and Enabled At-\ntacks. These layered attacker capabilities map directly to\nthe core vulnerabilities of the preprocessing stage:\n• Quasi-identifiers: Demographic and clinical combi-\nnations exploited by insiders and externals for re-\nidentification [34].\n• Data leakage via logs: Debug outputs and ETL failure\nlogs containing PHI accessible to authorized insiders or\ncompromised systems [41].\n• Weak access controls and APIs: Misconfigured per-\nmissions, exposed endpoints, or insecure ETL connec-\ntors exploited by administrators or external attackers to\naccess raw datasets [38], [39].\nThese vulnerabilities directly enable several concrete\nattacks: (1) Re-identification/linkage attacks, where ad-\nversaries combine quasi-identifiers with auxiliary records\nto recover patient identities [46]. (2) Log-based PHI leak-\nage, where unmasked identifiers in ETL traces or debug\noutputs expose raw clinical details [40]. (3) Unauthorized\ndataset access, where weak permissions or misconfigured\nAPIs allow direct retrieval of raw EHR exports or staging\nfiles [39]. (4) Data poisoning, where attackers inject\nmanipulated or fabricated clinical records into HL7/FHIR\nstreams to distort downstream fine-tuning behavior [49].\nTogether, these attacks operationalize the incentives,\ncapabilities, and prior knowledge described in the pre-\nprocessing threat model, demonstrating how seemingly\nroutine workflow weaknesses become attack vectors.\n3.2. Privacy-Preserving Defenses\nGiven the risks of handling PHI, implementing robust\nprivacy-preserving techniques in this phase is crucial [50].\nCommon methods like data anonymization, synthetic data\ngeneration, and noise addition help safeguard patient pri-\nvacy while enabling LLM training.\n"}, {"page": 4, "text": "TABLE 3: Data Preprocessing Phase — Summary of Threats, Defenses, Limitations, and Recommendations.\nThreat Model\nDefenses (PPTs)\nLimitations\nRecommendations\n• Adversaries: Internal\n(clinicians, data engineers, IT\nadmins) and external\n(ransomware, state-linked,\ncompetitors).\n• Capabilities: Exploit\nquasi-identifiers, extract PHI\nfrom ETL logs/debug traces,\nabuse weak APIs, or inject\npoisoned HL7/FHIR data.\n• Key Vulnerabilities:\nQuasi-identifiers, PHI-bearing\nlogs, weak access control, raw\nexports/backups.\n• Anonymization: Masking,\ntokenization, k-anonymity,\nl-diversity, pseudonym vaults.\n• Synthetic Data:\nGAN/VAE/LLM-based\ngeneration replacing real EHR\ninputs.\n• Differential Privacy: DP-SGD,\nlocal DP, stochastic embedding\nnoise for pre-training or tabular\nfeatures.\n• Quasi-identifiers: Generators\nmay reproduce unique attribute\npatterns, enabling linkage.\n• Logs / APIs: PHI may persist\nin debug traces; DP\nmisconfiguration leaks\nidentifiers.\n• Poisoning: Defenses fail\nagainst manipulated records\ninjected pre-noise or masking.\n• Governance: Weak auditing or\ninconsistent DP parameters\nreintroduce vulnerabilities.\n• Quasi-identifiers: Structured\nanonymization within ETL\n(tokenization, suppression).\n• Log leakage: LLM-assisted\nscrubbing of debug/staging\ndata.\n• Access control:\nLeast-privilege, vault-based\ntokens and synthetic sandboxes.\n• Poisoning: Pre-ingestion\nanomaly detection and\nHL7/FHIR integrity checks.\n3.2.1. Data Anonymization. It is the first operational de-\nfense in preprocessing, directly countering insider misuse\nand quasi-identifiers exposure before clinical data enter\nmodeling pipelines. During cleaning, raw EHR tables,\nradiology metadata, and lab files still carry names, MRNs,\ntimestamps, and geocodes, which are the prime targets\nfor careless or malicious insiders [35], [38], [40], [41].\nAs outlined in the threat model, low-resource insiders\n(nurses, clerks) leak unmasked exports in handovers, and\nmoderate-resource insiders (ETL/ML staff) surface iden-\ntifiers in debug logs and staging data [39]. Early masking\nor tokenization interrupts these leakage paths so exports,\nlogs, and cached views hold only pseudonymous values\neven when systems are misconfigured.\nClassical frameworks such as k-anonymity, l-diversity,\nt-closeness generalize or perturb demographic/clinical\nfields so attribute combinations cannot uniquely identify\na patient [34], [51], [52]. This directly mitigates the link-\nage risks noted in the threat model, where adversaries\ncombine auxiliary attributes (age, ZIP, rare conditions)\nto re-identify patients [34], [46]. Hospitals commonly\ntruncate postal codes or merge rare diagnoses (e.g., oncol-\nogy registries), blunting the advantage of external actors\nwho leverage demographic priors or leaked registries [47].\nComplementary pseudonymization and encryption replace\nidentifiers with reversible tokens kept in audited vaults\n[53], [54], constraining the practical privileges of ETL\npersonnel and admins: if staging/backup files are accessed,\nonly encrypted tokens, not PHI, are exposed, cutting off\nthe same audit log and backup channels exploited by\ninsider error or lateral movement [40], [41].\nUsing anonymized datasets. Most healthcare-LLM\nstudies lean on pre-anonymized corpora (MIMIC-III/IV,\nMIMIC-CXR) de-identified for HIPAA/GDPR [13], [14],\n[53], [55], enabling clinical IE [3], privacy-preserving\nlocal deployment [56], and federated pretraining [15]. Yet\ndataset-level compliance does not eliminate pipeline-level\nexposure: identifiers can resurface in temporary exports,\nlogs, or monitoring dashboards during hospital preprocess-\ning [40], and quasi-identifiers can still support attribute\ninference or record linkage [43], [57]. Some studies in-\nstead use closed-source anonymized sets [58] or “PII-free\nby design” resources (ChatDoctor, MMQS, IMCS-21) [6],\n[59], which avoid direct identifiers but rarely provide end-\nto-end audit evidence that PHI never entered the pipeline.\nUsing LLMs for anonymization. An emerging miti-\ngation embeds anonymization inside preprocessing itself:\nLLMs fine-tuned for medical de-identification redact PHI\nwhile preserving clinical semantics [60]. These tools ad-\ndress the free-text/logging vulnerabilities highlighted in\nthe threat model, e.g., identifiers buried in notes or times-\ntamps that leak via ETL logs and debug outputs [38], [40]\nand, when paired with hybrid NER–DL pipelines, sanitize\ntext streams before storage or model ingestion, closing\ninsider and scraper pathways that rely on residual identi-\nfiers for linkage [47]. Implementation quality, however, is\nuneven: some datasets report manual redaction (e.g., Ha-\njjHealthQA) [27], others assume PHI absence in synthetic\ndialogues [61], [62], and several open resources still show\nexplicit identifiers without automated scrubbing [63]. This\ninconsistency reinforces the need to treat anonymization\nas an engineered control in the preprocessing pipeline, not\nmerely a property of upstream datasets.\nTakeaway. Anonymization directly mitigates the pre-\nprocessing threats by disrupting how insiders and ex-\nternals exploit raw clinical data before transformation.\nMasking or encrypting identifiers before ETL prevents re-\nidentification and linkage attacks via quasi-identifiers such\nas age or rare conditions, while also blocking log-based\nPHI leakage from debug outputs [34], [40]. For high-\nresource external adversaries, it removes identifiers that\nenable cross-record matching through their prior knowl-\nedge and auxiliary datasets, thereby reducing the utility\nof stolen clinical data. Consistent application of this tech-\nnique substantially reduces exposure to core preprocessing\nvulnerabilities: quasi-identifiers misuse, log leakage, and\nweak access controls, forming a crucial first layer of\ndefense for privacy-preserving LLM pipelines.\nLimitation. While anonymization mitigates key pre-\nprocessing threats, it leaves several vulnerabilities partially\nexposed. (1) Quasi-identifiers:it cannot fully prevent re-\nidentification or linkage attacks by high-resource externals\nleveraging prior knowledge and auxiliary datasets; (2)\nData leakage via logs: when applied only at the dataset\nlevel, it fails to mitigate PHI exposure through debug\noutputs, temporary exports, or audit traces, allowing low-\nand moderate-resource insiders to access identifiers left in\nsystem logs or misconfigured dashboards. (3) Weak access\ncontrols: anonymization does not prevent data poisoning\nor unauthorized input manipulation, as moderate-resource\ninsiders can still inject or alter records before masking\nthrough insecure ETL access points.\n"}, {"page": 5, "text": "3.2.2. Synthetic Data Generation. This operates as a\nproactive privacy defense in the preprocessing phase,\ndirectly addressing the insider misuse and external re-\nidentification threats outlined in the threat model. Whereas\nanonymization masks identifiers after data collection, syn-\nthetic generation removes reliance on raw PHI altogether,\neliminating exposure points before they arise. By sim-\nulating clinical distributions from learned or rule-based\npatterns, it dismantles the economic and operational in-\ncentives for insiders who might leak raw EHR exports and\nrenders external ransomware operators unable to monetize\nstolen backups or staging snapshots [36], [48]. Because\nsynthetic records contain no true identifiers or one-to-one\nmappings to real patients, they nullify quasi-identifier risks\nand schema-specific leak paths that low- and moderate-\nresource insiders can trigger during data preparation [35],\n[40]. For high-resource external adversaries, the value\nof exfiltrated data collapses; synthetic ETL snapshots or\nexported samples cannot be used for re-identification,\nblackmail, or population linkage [46], [47].\nRule-based methods: Rule-driven and simulation ap-\nproaches generate synthetic data from explicit clinical\nlogic or population statistics, providing auditable and de-\nterministic privacy guarantees. Monte Carlo and discrete-\nevent models replicate hospital workflows—admissions,\nlab requests, and comorbidities, without using real patient\nidentifiers [64], [65]. For instance, the Dismed dataset\nrandomized annotated entities using biomedical ontologies\nto maintain diagnostic structure while erasing sensitive\nidentifiers [60], [66]. Such systems are particularly ef-\nfective against insider leakage because their generation\nlogic never touches PHI; even if preprocessing pipelines\nor exports are compromised, the data itself is synthetic\nand devoid of re-identifiable features.\nUsing LLMs: Learning-based methods extend this\nprotection to complex multimodal data such as radiology\nreports, clinical narratives, and tabular EHRs. Genera-\ntive networks (GANs, VAEs, and diffusion models) and\nLLMs capture nonlinear dependencies across diagnoses,\nmedications, and outcomes [67], [68]. LLM-based gen-\nerators—such as GPT-4 and LLaMA 3.1-70B, have been\nused to produce synthetic case studies and comorbidity\ngraphs for models like ComLLM and Asclepius, support-\ning clinical QA and CDS tasks without direct patient data\naccess [18], [19], [69]. This approach directly mitigates\nthe text-based leakage vulnerabilities identified in the\nthreat analysis, where identifiers persist in unstructured\nnotes or debugging logs [38], [40]. By inserting a synthetic\nlayer between hospital data and LLM pipelines, healthcare\ninstitutions convert high-risk preprocessing workflows into\nauditable, privacy-preserving sandboxes.\nTakeaway. Synthetic data generation directly miti-\ngates the vulnerabilities identified in the preprocessing\nthreat model by removing dependence on raw PHI before\ntraining. By replacing true records with statistically valid\nbut fictitious samples, it disrupts the economic and oper-\national incentives of low- and moderate-resource insiders\nwho rely on understanding of EHR schemas and identi-\nfiers and access to staging data to exploit quasi-identifiers\nor export unmasked EHRs. At the same time, synthetic\ndata eliminates the informational value that high-resource\nexternal adversaries derive from auxiliary datasets, ren-\ndering re-identification, linkage attacks, and exploitation\nof EHR exports or staging leaks ineffective. By removing\nthe real identifiers that make these vulnerabilities action-\nable, synthetic data closes the same attack surfaces de-\nscribed in the threat model and reshapes the preprocessing\npipeline into a controlled environment where both insider\nmisuse and external infiltration have substantially reduced\nimpact.\nLimitation. While synthetic data generation reduces\ndependence on real PHI, it does not eliminate pre-\nprocessing vulnerabilities.(1) Quasi-identifiers: generators\nmay inadvertently reproduce statistical or quasi-identifier\npatterns, leaving the system exposed to the same re-\nidentification and linkage risks exploited by high-resource\nadversaries with strong prior knowledge and auxiliary\nrecords.(2) Weak access controls: synthetic pipelines re-\nmain vulnerable to data poisoning introduced before gen-\neration, allowing moderate-resource insiders or malicious\ncollaborators to manipulate raw PHI and exploit their\naccess privileges. (3) Data leakage via logs: if generation\nquality is inconsistent or insufficiently audited, synthetic\ndatasets may recreate structural cues that allow adversaries\nto exploit previously identified attack surfaces such as\nEHR exports, staging leaks, or API exposures. Thus, while\nsynthetic generation reduces reliance on real PHI, its\neffectiveness depends on strict governance and validation\nto prevent threat-model vulnerabilities from re-emerging\nthrough statistical artifacts.\n3.2.3. Differential Privacy (DP). provides a formal de-\nfense against the inference and memorization risks that\npersist in preprocessing, particularly when partial iden-\ntifiers or structured embeddings are exposed. Unlike\nanonymization or synthetic generation, DP offers quan-\ntifiable privacy guarantees by injecting calibrated noise\ninto data values, gradients, or outputs [70]. This directly\naddresses the moderate-resource insiders and external\nattackers identified in the threat model, those capable\nof inspecting gradients, intermediate logs, or fine-tuning\ncheckpoints [38], [40]. By applying Gaussian or Laplacian\nnoise during aggregation or embedding generation, DP\nensures that individual patient records remain statistically\nindistinguishable, even to privileged ETL engineers or\nmodel developers [71], [72]. Healthcare frameworks such\nas DisLLM, MedMCQ, and PubMedQA demonstrate how\nDP-SGD and local DP protect sensitive EHR and clinical\ntext embeddings while retaining acceptable task accuracy\n[72]–[74]. Local DP, used in resource-limited hospital\npreprocessing, adds noise directly at the feature extraction\nstage, obscuring PHI before it reaches shared systems,\nwhile centralized DP reduces cross-phase linkage between\npreprocessing and downstream fine-tuning [8]. Beyond\nstrict DP, stochastic embedding methods such as NEFtune\nand SHADE-AD [23], [75] introduce non-determinism\nto reduce overfitting and memorization.Although lacking\nformal guarantees, these strategies complement DP by\nfurther diminishing the information advantage of internal\nor external adversaries during preprocessing.\nTakeaway. DP directly mitigates the inference and\nreconstruction risks described in the preprocessing threat\nmodel by protecting against the misuse of gradients, in-\ntermediate logs, and structured embeddings. By adding\ncalibrated noise during feature extraction or aggregation,\nDP reduces the ability of moderate-resource insiders, who\n"}, {"page": 6, "text": "have access to debugging logs, staging data, or model\ncheckpoints, to recover patient attributes or exploit quasi-\nidentifiers. The same noise also weakens external ad-\nversaries who rely on auxiliary datasets to perform re-\nidentification or record reconstruction. In this way, DP\nconverts preprocessing from a trust-dependent stage into\none that enforces quantifiable privacy guarantees, directly\naddressing the vulnerabilities associated with linkage at-\ntacks and log leakage identified in the threat model.\nLimitation. Despite its benefits, DP does not address\nall vulnerabilities in the preprocessing threat model. (1)\nWeak access controls/APIs: DP cannot stop data poison-\ning, since adversaries can inject or manipulate raw clinical\nrecords before noise is applied, exploiting insecure ETL\nworkflows. (2) Quasi-identifiers: the privacy–utility trade-\noff can distort sensitive fields, degrading fidelity in down-\nstream clinical tasks while leaving partial re-identification\nrisks. (3) Data leakage via logs: DP’s effectiveness de-\npends on correct configuration; mis-set noise budgets or\nimplementation flaws can still expose PHI through debug\ntraces or unsecured system logs. Thus, while DP mitigates\nre-identification and log exposure, it cannot fully prevent\npoisoning or misconfiguration exploits in preprocessing.\nRecommendation 1: The preprocessing phase ex-\nposes the most sensitive attack surface of the health-\ncare LLM pipeline, where adversaries exploit raw\nidentifiers, staging artifacts, and weak system con-\nfigurations. To mitigate these vulnerabilities, de-\nfenses must directly align with the threat model. (1)\nQuasi-identifiers Exposure. Implement task-specific\nanonymization combined with differential privacy au-\ndits to remove or mask high-risk identifiers before\nETL execution. This reduces re-identification and\nlinkage attacks that exploit demographic and clinical\nquasi-identifiers using prior knowledge and auxiliary\ndatasets (2) Data leakage via logs.Adopt privacy-\naware logging frameworks with automatic redaction,\nhashed identifiers, and secure audit retention poli-\ncies. This ensures ETL traces and debugging outputs\ncannot expose PHI through intermediate tables or\nsystem logs accessible to insiders or compromised\nsystems. (3) Weak access controls and API expo-\nsures. Deploy role-based access control (RBAC) and\nzero-trust authentication across preprocessing nodes,\nensuring that only verified users and processes can\nquery or export data. Integrate secure API gateways\nand encryption-in-use (TEE) mechanisms to protect\nintermediate exports and prevent unauthorized dataset\nretrieval (4) Data poisoning. Incorporate data in-\ntegrity validation and schema-constrained ingestion\nto detect anomalous or manipulated records injected\nbefore anonymization. Combine this with input prove-\nnance tracking and hash-based cross-validation to en-\nsure the authenticity of clinical feeds (HL7/FHIR)\nbefore preprocessing pipelines ingest\nTable 3 summarizes the preprocessing-phase taxon-\nomy, consolidating internal/external actors, attack sur-\nfaces, defenses, limitations, and actionable recommenda-\ntions derived from this section.\n4. Federated Fine-Tuning Phase\nBuilding on the vulnerabilities introduced during pre-\nprocessing, the fine-tuning phase exposes a new class of\ndistributed risks: even when raw PHI is masked, residual\nartifacts, cohort structures, and poisoned records propa-\ngate into gradients and client updates across federated\nhospitals. We focus specifically on federated fine-tuning\nbecause, in healthcare, centralized fine-tuning is rarely\nfeasible in cross-institutional raw-data pooling violates\nHIPAA/GDPR, institutional policies, and multi-site data-\nsharing agreements [8], [9], [34], [35]. As a result, fed-\nerated fine-tuning is not an architectural preference but\nthe only legally and operationally viable mechanism for\nadapting LLMs to clinical data in real deployments [4],\n[10], [15], [76]. This makes FL the principal attack surface\nand hence the appropriate scope for a phase-aware SoK.\nFine-tuning LLMs in healthcare enables models to\nadapt to domain-specific terminology and tasks, improving\naccuracy and clinical relevance. However, fine-tuning on\nsensitive, institution-specific datasets introduces distinct\nprivacy risks. While techniques such as Differential Pri-\nvacy (DP) and adapter tuning offer partial protection [77],\nthe majority of practical deployments and research indi-\ncate that Federated Learning (FL) is the most comprehen-\nsive privacy-preserving framework for regulated health-\ncare environments [4], [21], [78]. In this phase, we outline\nthe threat landscape specific to FL, focusing on the privacy\nchallenges arising from distributed training infrastructures\nconnecting hospitals.\n4.1. Threat Model\nIn the fine-tuning phase, threats arise from adversaries,\ninternal or external, who interact with the FL infrastruc-\nture connecting hospitals. Unlike the preprocessing stage,\nwhere unmasked raw data is exposed, here the primary\nprivacy risks emerge from access to local gradients, client\nupdates, and communication buffers. These artifacts im-\nplicitly encode sensitive clinical attributes and thus create\na distinct attack surface.\nInternal adversaries pose the most operationally im-\npactful risks because they operate inside trusted hospi-\ntal boundaries and interact directly with local training\nworkflows: (1) Clinicians or annotators may inadvertently\nexpose local patient records while validating model pre-\ndictions or providing feedback on draft outputs [2]. (2)\nML engineers and data scientists routinely inspect gradient\nsnapshots or checkpoints during debugging, unintention-\nally accessing encoded EHR information [20]. (3) System\nadministrators and IT staff managing synchronization or\nstorage systems have privileged visibility into local model\ncaches, communication buffers, or server logs, making\nsilent extraction or export of update data feasible [21].\nThese internal actions rarely trigger alarms because they\noccur within expected operational workflows.\nExternal adversaries target the distributed nature of\nFL and the heterogeneity of hospital networks: (1) Net-\nwork intruders intercept parameter updates or partial gra-\ndients in transit, enabling reconstruction of patient details\nvia gradient inversion [43]. (2) Corporate or state-linked\nactors inject malicious updates to bias shared clinical\nmodels, impairing diagnostic consistency across sites [5].\n"}, {"page": 7, "text": "(3) Ransomware and extortion groups target aggregation\nservers to exfiltrate multi-institutional model checkpoints\nencoding sensitive patterns across cohorts [4]. These ad-\nversaries are driven by financial, strategic, or competitive\nmotives and exploit FL’s distributed communication as a\nlarge-scale leakage vector.\n4.1.1. Prior Knowledge and Capability Gradient. Ad-\nversaries in this phase vary in both domain knowledge and\ncomputational capacity, which determine how effectively\nthey can exploit gradients, checkpoints, or update buffers:\n(1) Low-resource insiders (e.g., clinicians, annotators)\nunderstand how patient-level features are represented in\nmodel behavior and may unintentionally expose encoded\nEHR details during validation or feedback. (2) Moderate-\nresource insiders (data scientists, ML engineers) possess\ndeeper knowledge of model architectures and storage\nlayouts, knowing where gradient files, synchronization\nbuffers, or checkpoints are stored, allowing silent ac-\ncess or export during debugging or maintenance [21].\n(3) High-resource external adversaries, such as coordi-\nnated ransomware groups or corporate actors, leverage\nprior knowledge of biomedical embeddings and auxiliary\nEHR datasets to align intercepted gradients with public\ncheckpoints, reconstructing rare conditions or site-specific\nvocabulary [43], [70].\nThis knowledge gradient corresponds directly to their\noperational capabilities: (1) Low-resource insiders leak\nupdates through logs and validation workflows; (2)\nModerate-resource insiders or externals perform gradi-\nent inversion and membership inference using intercepted\ntraffic or open biomedical corpora [5], [79]; and (3) High-\nresource actors conduct model alignment or poisoning\nacross hospitals, exploiting the distributed update-sharing\nand aggregation layers [4], [20]. Together, these escalating\ncapabilities explain how prior knowledge, from local sys-\ntem familiarity to cross-institutional data access, translates\ndirectly into exploitation of fine-tuning vulnerabilities\nsuch as gradient leakage, client update interception, and\npoisoning within federated healthcare networks.\n4.1.2. Attack Surface Vulnerabilities and Enabled At-\ntacks. Distinct attack surface vulnerabilities emerge dur-\ning fine-tuning of healthcare LLMs, each tied to the nature\nof clinical datasets and federated infrastructures.\n• Model gradient leakage: During fine-tuning, gradi-\nents or model weights stored locally may inadver-\ntently memorize sensitive clinical data such as struc-\ntured EHR variables (e.g., blood pressure, choles-\nterol levels, ICD-10 codes) and unstructured dis-\ncharge narratives. Even partial leakage can expose\nindividual patient trajectories, particularly for rare\ndiseases recorded in small data sets. For example, in\ncardiology, the leakage of gradients can reveal critical\npatient outcomes like ejection fraction or surgical\ninterventions—easily re-identifiable in small cohorts\n[16], [20]\n• Client update leakage: In federated fine-tuning, hos-\npitals share model updates instead of raw client data.\nHowever, these updates still carry sensitive EHR pat-\nterns, such as lab panels, clinical notes, or medication\nhistories, which can still be leaked. Adversaries may\nuse gradient inversion to reconstruct detailed narra-\ntives (e.g., ICU notes, pathology reports). Moreover,\npoisoning attacks may skew diagnoses, such as for\ndiabetes or heart failure [4], [15]. This makes feder-\nated updates particularly sensitive, as partial feature\nleakage can compromise longitudinal EHR timelines.\n• Communication\nchannel\ninterception:\nDuring\nmodel synchronization, adversaries who intercept up-\ndates can recover latent embeddings associated with\nsensitive clinical data such as EHR-derived phe-\nnotypes, lab trajectories for oncology patients, or\ncardiology monitoring logs. In multilingual hospital\nsystems, intercepted updates can also expose doc-\ntor–patient transcripts integrated with EHRs, poten-\ntially revealing sensitive lifestyle or family history\ndetails tied to EHRs [5], [76].\n• Misconfigured audit and logging systems: Many\nlogging mechanisms often capture sensitive traces\npresent in model training data, including PHI tokens\nfrom EHR fields such as medication lists, allergies,\nor comorbidities. If these logs are misconfigured or\nimproperly secured, adversaries can gain access to\nsensitive metadata such as genetic risk markers or\nrare disease cohorts, which could then be cross-linked\nwith external datasets [3], [6].\nThese vulnerabilities directly enable several concrete at-\ntack types in federated fine-tuning: (1) Gradient in-\nversion attacks, where adversaries reconstruct sensitive\nclinical details, such as ICU notes, pathology descriptions,\nor lab trajectories from leaked or intercepted gradients. (2)\nMembership inference attacks, which allow adversaries\nto determine whether a specific patient’s records con-\ntributed to the fine-tuning process, particularly for rare dis-\neases or small-site cohorts. (3) Update leakage attacks,\nwhere model updates shared across hospitals reveal struc-\ntured or unstructured EHR patterns embedded in weight\ndeltas or optimizer states. (4) Model poisoning and\nbackdoor attacks, where malicious insiders or externals\ninject crafted updates that bias diagnosis-related outputs\nor embed hidden triggers into clinical prediction tasks.\n(5) Communication-layer interception attacks, where\nman-in-the-middle adversaries extract latent representa-\ntions from synchronization streams exchanged between\nhospitals and the central aggregator.\n4.2. Privacy-Preserving Defenses\nThis section reviews core privacy defenses integrated\ninto federated learning (FL) fine-tuning workflows cover-\ning client-side, secure update sharing, and communication\nsafeguards followed by limitations and recommendations.\n4.2.1. Client Side. These cover the privacy-preserving\ntechniques employed at the client side during training in\nthe FL setup.\nDifferential Privacy (DP) adds calibrated noise to\nmodel updates before leaving the hospital, preventing\npatient-level re-identification in sensitive datasets such as\nMIMIC-IV ICU notes or oncology records [8]. DP-LoRA\napplies noise only to adapter layers, preserving accuracy\n[80], while selective DP targets the most sensitive pa-\nrameters [78], [81]. Despite potential signal loss in rare\ndisease cohorts [16], DP remains a practical safeguard\n"}, {"page": 8, "text": "for HIPAA/GDPR compliance and multi-hospital training\n[82].\nSecure Multi-Party Computation (SMPC) enables\nhospitals to train jointly without sharing raw data. Each\nsite encrypts and splits its updates, which are only usable\nwhen aggregated [82]. This supports legally restricted\nstudies like cancer survival or cardiovascular prediction\n[3]. While secure against reconstruction, SMPC’s high\ncompute cost limits real-time applications [8], though it\nremains vital for regulated hospital consortia [4].\nSplit Learning (SL)\npartitions the model between\nclient and server—local layers process raw data, and\nonly intermediate features are shared [8], [82]. This pro-\ntects imaging and textual data (e.g., ChestX-ray14, ECG\ndatasets) while supporting multimodal tasks. Though in-\ntermediate features may leak partial identity traces, SL’s\nlightweight setup enables participation from smaller hos-\npitals with limited infrastructure.\nRandomized Low-Rank Adaptation (LoRA) fine-\ntunes only low-rank parameters with added randomiza-\ntion, hindering inversion attacks on sensitive text (e.g.,\noncology notes, psychiatric transcripts) [80], [81]. It offers\nbetter accuracy than DP and can be combined with it for\nstronger protection [28]. Its low compute footprint makes\nit ideal for hospitals with limited GPUs, balancing privacy,\nefficiency, and performance.\nQuantization reduces weight precision (e.g., 32-bit →\n8/4-bit), limiting exploitable detail while cutting memory\nand bandwidth needs [3], [82]. Effective on structured\nmedical data, quantized models maintain accuracy and\nresist gradient inversion [83]. When paired with DP or\nLoRA, it forms a layered defense which is resource-\nefficient and scalable for diverse healthcare networks [4],\n[78], [84].\nTakeaway. Client-side mechanisms directly address\nprivacy risks arising from local training gradients and\nintermediate checkpoints. DP mitigates memorization and\ngradient inversion by adding calibrated noise, reducing\nthe ability of internal or external adversaries to recon-\nstruct structured or narrative EHR data from local model\nstates. SMPC encrypts local gradients before aggrega-\ntion, preventing adversaries from extracting embeddings\nor conversational features from intercepted updates. SL\nminimizes raw data transmission by keeping sensitive\nfeatures (e.g., imaging, ECG traces, pathology text) local\nand sharing only intermediate activations. Randomized\nLoRA introduces stochasticity in parameter updates, weak-\nening the consistency needed for gradient correlation or\npoisoning. Quantization further obscures precise gradient\nvalues, reducing leakage in logs and audit traces while\nlowering communication overheads. Together, these de-\nfenses collectively address gradient leakage, poisoning,\nand metadata exposure across distributed clients.\nLimitation. Despite reducing risks, client-side de-\nfenses still leave healthcare FL systems exposed across\nidentified attack surfaces. (1) Model Gradient Leakage:\nDP reduces signal strength but the privacy–utility trade-off\ncan suppress rare-disease features while leaving residual\nexposure in local artifacts (e.g., verbose checkpoints or\ncached tensors), enabling gradient inversion and mem-\nbership inference on small cohorts. (2) Client Update\nLeakage: SMPC hides individual updates from the aggre-\ngator but does not protect against leakage at the endpoints\n(misconfigured clients/servers, optimizer state dumps) and\ndoes not prevent update reconstruction from artifacts cap-\ntured outside the secure aggregation step; it also does not\naddress poisoning/backdoor manipulation of valid shares.\n(3) Communication Channel Interception: Split learning\nkeeps raw inputs local, yet exchanged intermediate rep-\nresentations and synchronization streams remain linkable;\nadvanced adversaries can perform re-identification/linkage\non intercepted communication buffers or server-visible ac-\ntivations. (4) Misconfigured Audits: Randomized LoRA or\nquantization does not sanitize logs; if audit systems record\nlocal gradients, client updates, or intermediate tensors,\ncorrelation across rounds/sites can still expose sensitive\ndialogues or discharge notes. These limitations show that\ncurrent defenses partially address risks like memorization\nand poisoning, leaving vulnerabilities for adversaries to\nexploit in federated healthcare training.\n4.2.2. Client Update Sharing and Secure Aggrega-\ntion. This stage targets vulnerabilities in update trans-\nmission and aggregation, where even privacy-preserving\nlocal training can expose sensitive patterns. Because the\naggregator is a central risk point, mechanisms such as\nsecure aggregation and blockchain-based FL are critical\nin healthcare to maintain both confidentiality and institu-\ntional accountability.\nSecure Aggregation ensures that hospital updates re-\nmain confidential during cross-silo fine-tuning. Each client\nmasks its gradients or adapter updates with random values;\nonly the summed result reveals the true aggregate [4], [43],\n[81]. This prevents reconstruction of sensitive features\nsuch as medication histories or lab trajectories. To reduce\noverhead, recent work masks only LoRA adapter weights\ninstead of full gradients [78], [80], while anomaly de-\ntection helps flag poisoned updates targeting rare-disease\ncohorts [3]. Though computationally demanding for small\nhospitals [82], secure aggregation remains essential in\nfederated healthcare networks, balancing privacy, compli-\nance, and trust.\nWeight Delta Sharing. transmits only parameter dif-\nferences between local and global models, minimizing\nexposure of raw patient data [83]. This approach improves\nefficiency for EHR-based LLMs such as MIMIC-IV dis-\ncharge models [23]. LoRA-only delta sharing compresses\nupdates into low-rank matrices for multilingual hospital\nnetworks [5], while selective layer freezing focuses on\nclinically relevant upper layers [16], [21]. However, re-\npeated deltas can leak sensitive trends; combining delta\nsharing with DP or gradient clipping mitigates this risk\n[81].\nBlockchain for Update Integrity and Unlearning.\nprovides traceability and auditability by recording en-\ncrypted update hashes, timestamps, and institutional IDs\n[79]. This deters tampering and enables federated unlearn-\ning, removing a client’s contributions without full retrain-\ning [28], [79]. Provenance records further enhance insti-\ntutional trust in federated healthcare deployments [82].\nWhile added infrastructure and latency pose challenges,\nblockchain remains valuable for high-stakes domains such\nas oncology or ICU consortia [21], [78].\nTakeaway. At the aggregation layer, update-sharing\nmechanisms mitigate vulnerabilities associated with cross-\nsite synchronization and multi-round leakage. Secure ag-\n"}, {"page": 9, "text": "TABLE 4: Federated Fine-Tuning Phase — Summary of Threats, Defenses, Limitations, and Recommendations.\nThreat Model\nPrivacy-preserving Defenses\nLimitations\nRecommendations\n• Internal adversaries:\nclinicians/annotators (validation\nleaks); ML engineers\n(gradients, checkpoints); system\nadmins (caches, sync buffers).\n• External adversaries: network\nintruders, ransomware groups,\nstate-linked or corporate actors.\n• Capabilities: gradient\ninversion, membership\ninference, poisoning/backdoors,\nalignment with auxiliary\ncorpora, client-update\nreconstruction.\n• Key Vulnerabilities: local\ngradients, client updates,\noptimizer states,\ncommunication buffers,\nsynchronization traffic,\nmisconfigured logs/audits.\n• Client-side: DP-SGD,\nDP-LoRA, selective/local DP;\nSMPC for encrypted/split\nupdates; Split Learning (local\nearly layers); randomized\nLoRA; quantization for low\nprecision (8/4-bit).\n• Client update sharing: Secure\nAggregation (masking,\nLoRA-only); Weight-Delta\nsharing (adapter updates only);\nBlockchain for integrity,\ntamper-evidence, and\nunlearning.\n• Communication channel:\nAdapter-based compression\n(LoRA), few-shot learning\n(small gradients), and RTIR\n(lightweight reasoning\nfine-tuning).\n• Model gradient leakage: DP\ndegrades rare-disease fidelity;\ncached tensors persist; Split\nLearning leaks intermediate\nactivations.\n• Client update leakage: SMPC\nprotects only at aggregation;\npoisoning and backdoors\npersist; deltas reconstructable\nacross rounds.\n• Communication interception:\nLoRA lowers size but not local\nleakage; few-shot gradients\nreconstructable; RTIR depends\non secure retrieval.\n• Misconfigured logs/audits:\nlogs capture\ngradients/activations;\nblockchain metadata can reveal\ninstitutional identifiers.\n• Gradient leakage: use\nadaptive DP with randomized\nLoRA to reduce inversion and\nmembership inference.\n• Update leakage: combine\nSMPC with clipping and\nanomaly detection; selectively\nfreeze sensitive layers.\n• Communication interception:\nadopt authenticated encryption,\nquantized aggregation, and\nTEEs; pair LoRA with DP.\n• Audit exposure: use\nprivacy-aware provenance;\nminimize logged gradients;\nobfuscate ledger metadata.\n• Combined mitigation:\nintegrate quantization +\nadaptive DP + randomized\nLoRA for fidelity, low\nbandwidth, and poisoning\nresistance.\ngregation limits client update leakage at the aggregator\nand weakens gradient inversion and membership infer-\nence on per-site contributions by preventing inspection\nof individual updates. Weight-delta sharing narrows ex-\nposure but must be paired with DP or clipping to resist\nmulti-round leakage and subsequent update reconstruc-\ntion. Blockchain-based provenance strengthens protection\nagainst misconfigured audit and logging systems and helps\nsurface poisoning/backdoors by providing tamper-evident\nintegrity and traceability for updates. Quantized aggre-\ngation alleviates communication-channel bottlenecks by\ncompressing updates, though aggressive compression can\naffect rare-disease fidelity and bias.\nLimitation. Update-sharing defenses reduce some\nrisks but still leave critical gaps when mapped to the attack\nsurfaces. (1) Client update leakage: Secure aggregation\nconceals individual hospital updates at the aggregator, but\nit does not prevent poisoning/backdoors adversaries can\ninject crafted, valid-looking shares, and endpoint artifacts\n(caches, optimizer states) can still leak per-site infor-\nmation. (2) Multi-round leakage (client update leakage):\nWeight-delta sharing lowers bandwidth per round, yet\ndeltas accumulated across rounds enable reconstruction\nand membership inference over pathology or ICU trajec-\ntories, sustaining the multi-round leakage surface without\nadditional DP or clipping. (3) Misconfigured audit and\nlogging systems: Blockchain provides tamper evidence,\nnot confidentiality; if ledger or audit metadata (insti-\ntution IDs, timestamps, cohort counts) are exposed or\nlogs are unsanitized, cross-linking and profiling of rare-\ndisease cohorts remain possible on the audit surface. (4)\nCommunication channel interception: Quantized aggre-\ngation compresses updates but does not secure transport\nor endpoints; intercepted compressed streams can still be\nanalyzed, and aggressive rounding degrades rare-disease\nfidelity, opening room for biased predictions in oncology\nor cardiology tasks.\n4.2.3. Communication Channel. As LLMs integrate\ninto federated learning (FL), communication overhead\nbecomes a major constraint, distinct from classical FL\nthreats. In healthcare, transmitting full model updates is\nimpractical; hence, reducing transmission size and fre-\nquency is crucial for scalability, privacy, and performance.\nAdapter-based Compression such as LoRA mini-\nmizes communication by updating only low-rank adapter\nparameters rather than full model weights [80]. This al-\nlows hospitals to share compact updates while keeping\nbillions of base parameters frozen, cutting bandwidth use\n[28], [83], [85]. Studies such as Med42 confirm LoRA-\nbased fine-tuning maintains accuracy for oncology classi-\nfication and discharge summary generation while signifi-\ncantly reducing parameter size [16]. LoRA-only updates\nalso enable multilingual medical transcript training across\nsilos [5] and limit PHI exposure since fewer parameters\nleave the institution [10], [81]. Additionally, adapter-level\nsharing supports selective unlearning, removing hospital-\nspecific contributions without retraining [21]. However,\nLoRA may underperform on deep multimodal tasks like\nClipsyntel summarization that require richer representa-\ntions [27].\nFew-shot Learning reduces communication by fine-\ntuning on limited local samples (10–50), yielding sparse,\nlightweight gradients [76], [84]. Applied to ICU discharge\nnotes and oncology reports, this method transmits only\ntask-relevant updates while maintaining accuracy [23].\nFew-shot FL further supports multilingual transcripts for\nhospitals with small datasets [5], [22]. Smaller updates\ninherently reduce inversion risk [81], and combining few-\nshot training with DP or adapter tuning enhances stability\n[86]. Still, performance drops in multimodal settings like\nClipsyntel-based question summarization, which demands\ndeeper contextual learning [27].\nReal-Time Information Retrieval (RTIR) decouples\nknowledge from model weights, retrieving external data\nduring inference instead of embedding it in updates [5],\n[78]. In healthcare, this allows querying clinical knowl-\nedge bases for oncology guidelines, multilingual symptom\ndata, or ICU protocols without transmitting sensitive gra-\ndients. Hospitals then fine-tune only lightweight reasoning\n"}, {"page": 10, "text": "layers, dramatically shrinking update size and prevent-\ning patient data memorization [6]. RTIR integrated with\nadapter tuning supports efficient and privacy-preserving\nfederated pipelines while sustaining diagnostic accuracy\n[87]. The trade-off lies in reliance on secure retrieval\ninfrastructure, which may challenge smaller hospitals, and\nits limited autonomy in decision-making. Nonetheless,\nRTIR offers a dynamic, privacy-resilient strategy for fast-\nevolving healthcare domains.\nTakeaway. Channel-level defenses map to the vul-\nnerabilities identified in the threat model. Adapter-based\nCompression (LoRA) reduces bandwidth usage and nar-\nrows client update leakage by transmitting only adapter\nupdates, weakening opportunities for gradient inversion\nand membership inference over communication buffers.\nFew-shot Learning yields sparse, smaller updates from\nlimited local examples, further reducing the signal avail-\nable for update reconstruction on sensitive EHR trajecto-\nries and pathology timelines and lowering exposure dur-\ning communication channel interception. RTIR decouples\nknowledge from model weights so entire clinical histories\nare not embedded in checkpoints, shrinking the footprint\nof client updates and the attack surface for intercep-\ntion and downstream leakage. Collectively, these methods\ntarget interception, leakage, and channel vulnerabilities,\nreducing adversarial opportunities in federated healthcare\nsynchronization streams.\nLimitation. Communication-efficient methods reduce\nbandwidth but still leave key threat surfaces exposed\nin healthcare FL. (1) Model gradient leakage: Adapter-\nbased compression (LoRA) restricts parameter sharing\nbut does not sanitize local artifacts; reduced capacity can\nweaken rare-disease fidelity while leaving residual signal\nfor gradient inversion and membership inference on the\ngradient/memorization leakage surface. (2) Client update\nleakage: Few-shot learning minimizes exchanged updates,\nyet sparse gradients can omit subtle clinical signals (e.g.,\ndrug–drug interactions) and remain vulnerable to multi-\nround reconstruction and poisoning/backdoors, sustaining\nexposure on the client update surface and risking biased\npredictions for under-represented tasks. (3) Communica-\ntion channel interception: RTIR avoids embedding PHI in\nweights, but insecure retrieval paths and cached queries\nkeep the communication buffers susceptible to intercep-\ntion and cross-linking (e.g., transcripts, sensitive queries\nfor HIV or psychiatric histories).\nRecommendation 2: To close the defensive gaps\nidentified in the threat model, future federated health-\ncare systems should employ layered, phase-specific\ndefenses explicitly mapped to the key vulnerabilities.\n(1) Model gradient leakage. Adaptive DP should be\ncombined with Randomized LoRA to mitigate gradi-\nent inversion and memorization risks while preserving\nfidelity for rare-disease or small-cohort training. This\npairing directly strengthens protection against internal\nengineers or external interceptors who exploit gradi-\nent sensitivity.(2) Client update leakage. Augment\nSecure Aggregation and Weight-Delta Sharing with\ngradient clipping and anomaly detection to detect\npoisoning and reconstruction attempts before aggre-\ngation. These measures counter moderate-resource\nadversaries who manipulate or infer sensitive EHR\npatterns from multi-round updates. (3) Communi-\ncation channel interception. Deploy hybrid crypto-\ngraphic schemes combining lightweight SMPC and\nTEE-based aggregation to encrypt and isolate update\nstreams while maintaining acceptable latency. This\nlimits interception and timing analysis by external or\ncross-institutional adversaries during synchronization\nover WAN links. (4) Misconfigured audits/logging.\nIntegrate blockchain-based provenance tracking with\nmetadata obfuscation and differentially private log-\nging to ensure traceability without exposing partici-\npation frequency or institutional identifiers. This re-\nduces internal leakage risks from logs and external\ndeanonymization via audit metadata.\nTable 4 provides the fine-tuning-phase taxonomy, cap-\nturing the federated update pathway, client/server adver-\nsaries, surface-specific threats, and the layered defenses\ndiscussed here.\n5. Inference Phase\nThe vulnerabilities embedded during preprocessing\nand compounded through federated fine-tuning surface\nmost clearly during inference, where PHI can leak through\nprompts, hidden states, caches, and outputs even when\nmodel parameters remain protected. In healthcare, LLM\ninferencing is the process where a trained LLM uses pa-\ntient data to generate predictions, diagnoses, and person-\nalized treatment recommendations in real time. However,\nthis phase poses a distinct threat in LLM deployments, as\nclinical inputs can leak through outputs, embeddings, or\nmemory without exposing model parameters. Protecting\npatient data during inference is essential for regulatory\ncompliance and ethical AI.\n5.1. Threat Model\nInference in healthcare LLM deployments introduces\nrisks that differ from training. Here, PHI can leak via\nprompts, embeddings/hidden states, KV caches, and re-\nturned text, even when model weights and training data\nare protected. This matters for real clinical tools like\ndischarge-note assistants, triage chatbots, radiology and\npathology summarizers, and multilingual transcript sys-\ntems—often deployed on hybrid edge–cloud stacks under\ntight latency constraints [2], [5], [6], [20], [27].\n5.1.1. Attacker Landscape and Incentives. Inference\nintroduces distinct adversarial incentives because sensitive\ninformation appears in runtime artifacts, not in train-\ning data or gradients. These artifacts persist in logs,\ncaches, monitoring traces, and activation buffers main-\ntained across large, distributed clinical infrastructures.\nInternal adversaries pose the most persistent risk, as\nthey interact directly with operational LLM systems de-\nployed inside hospitals: (1) IT operators, SRE teams, and\nplatform engineers often access telemetry dashboards, run-\ntime logs, and GPU memory during debugging. Because\nmulti-turn systems maintain KV caches across generations,\nthese memory snapshots may reveal full conversations, in-\ncluding telemedicine notes or oncology assessments [24],\n[25]. (2) Clinicians and data scientists frequently input\n"}, {"page": 11, "text": "full EHR excerpts (e.g., ICU timelines, staging notes,\nsurgical histories) into prompts during validation or A/B\ntesting. These prompts can resurface in logging, error\ntraces, or monitoring pipelines [6], [30]. (3) Teams oper-\nating domain-specific inference pipelines, such as digital\npathology or endocrine-cancer extraction systems, may\nstore or inspect intermediate embeddings during perfor-\nmance tuning, inadvertently exposing PHI embedded in\nfeature vectors [2]. Internal threats are especially severe\nbecause these activities occur under legitimate operational\nworkflows and rarely trigger intrusion detection. External\nadversaries\nExternal adversaries raise threats to exploit LLM\ndeployment APIs, cross-hospital WAN links, or cloud\ninference infrastructure: (1) API-based attackers can per-\nform user inference attacks, probing outputs to determine\nwhether the model was adapted on rare clinical cohorts\n(e.g., rare oncology patients), enabling deanonymization\nof small hospitals [31], [32]. (2) Cloud-side or semi-\ntrusted platform adversaries can apply black-box inver-\nsion, reconstructing sensitive spans from intermediate\nactivations or returned outputs [88], [89]. (3) Network\nadversaries exploit WAN traffic between local hospitals\nand cloud inference nodes; timing and packet-size pat-\nterns can reveal PHI density or prompt structure, even\nwhen content is encrypted [90]. (4) Distributed inference\narchitectures especially those using adapter offloading or\nsplit execution expand the attack surface when activations\ncross trust boundaries [33], [91]. These attackers operate\noutside institutional boundaries, motivated by financial,\ncompetitive, or strategic objectives.\n5.1.2. Prior Knowledge and Capability Gradient. Ad-\nversaries at inference time vary in domain awareness and\ntechnical capability, shaping how effectively they exploit\nruntime artifacts such as prompts, KV caches, and hidden\nstates. (1) Low-resource insiders, clinicians, analysts, or\nIT operators understand prompt formats and may access\nlogs or traces containing raw PHI. Their local familiarity\nwith EHR content and dashboard telemetry enables unin-\ntentional exposure through debugging or monitoring sys-\ntems [25], [30]. (2) Moderate-capability actors, including\nengineers or cloud operators, know where GPU dumps,\nactivation buffers, and inference logs are stored, allowing\nsilent extraction or correlation of prompts, embeddings,\nand conversational states [24]. (3) High-resource adver-\nsaries such as external API probers or state-linked entities,\npossess auxiliary clinical datasets and use prior statistical\nknowledge to align output distributions or hidden-state\nsignatures with real cohorts, revealing rare conditions or\nsite-specific attributes [31], [32].\nThis knowledge gradient translates directly into ex-\nploitability: (1) insiders cause accidental PHI leakage via\nlogs and KV caches; (2) mid-level actors conduct em-\nbedding inversion or output-correlation attacks using API\nor infrastructure access [29], [88]; and (3) high-resource\nadversaries perform cross-dataset alignment and timing\nanalysis across WAN links to infer sensitive patterns or\nprompt structures [90], [91].\n5.1.3. Attack Surface Vulnerabilities and Enabled At-\ntacks. The key vulnerabilities include:\n• Prompt channel and Hidden-State Leakage: Clin-\nical prompts carry PHI such as diagnoses, medica-\ntions, lab timelines, and family histories. Even when\ntransmitted over TLS, prompts are plaintext at the\nservice node, and simple redaction can distort clinical\nmeaning. Hidden states store rich patterns from EHR\nnotes or pathology text, which can be reconstructed\nby embedding inversion in semitrusted clouds or de-\nbug pipelines [30], [88], [89], [91]. Edge–cloud splits\nor adapter offloads can leak intermediate activations\nif not properly protected [33], [91].\n• KV cache leakage: Multi-turn KV caches retain\ncontextual information for efficiency. If snapshots of\nGPU memory or caches are exposed, entire patient\ndialogues, such as telemedicine or oncology sessions,\ncan be recovered [24], [25]. Hybrid-cloud monitor-\ning and scaling layers introduce additional points of\nexposure.\n• Returned text (outputs): Outputs may leak PHI\nvia over-specific recommendations (dose names, rare\ncomorbidity patterns) or verbatim regurgitation of\nearlier prompts. Even with input obfuscation, outputs\nmay re-expose sensitive facts if protections are not\nend-to-end [27], [29], [30].\n• Delegated / hybrid inference (edge–cloud): When\nembeddings or adapters are computed locally and\nlater processed in the cloud, activations and metadata\ntraverse WAN links. Without secure partitioning or\nlocal DP, these streams can be profiled or linked to\npatient data [33], [91], [92]. WAN conditions can\nalso create timing side channels that leak information\n[90].\n• Restoration/meta-vector channels: Pipelines that\nremove sensitive spans and send noised restoration\nvectors risk leaking protected attributes (e.g., HIV\nstatus, hereditary cancer risk) if noise schedules\nare mismanaged [29]. Reusing obfuscation mappings\nacross sessions increases this risk [30].\n• Operational logging and provenance: LLM stacks\nwith tracing, A/B testing, or auditing can inadver-\ntently capture PHI in prompts, activations, or outputs.\nWithout strict log minimization, this creates long-\nterm leakage vulnerabilities [8], [25], [87].\nThese vulnerabilities enable concrete inference-time\nattack types highly relevant to healthcare: (1) Prompt re-\nconstruction attacks, extracting PHI from hidden states,\nKV caches, or traced activations. (2) Embedding in-\nversion attacks, reconstructing clinical notes, lab sum-\nmaries, or pathology descriptions via inversion of hidden\nstates or adapter outputs. (3) Output-based inference\nattacks, inferring patient cohort membership, rare disease\nparticipation, or underlying text from over-specific model\nresponses. (4) Split-inference intercept attacks, recover-\ning sensitive activations or adapter states crossing WAN\nlinks in hybrid edge–cloud deployments. (5) Restoration-\nvector attacks, recovering masked identifiers or sensi-\ntive attributes from poorly noised reconstruction vectors.\n(6) Log-correlation attacks, linking traces across clin-\nical sessions to reconstruct longitudinal care histories.\nTogether, these attacks operationalize the incentives and\ncapabilities outlined above, turning routine inference path-\nways into high-value exploitation channels.\n"}, {"page": 12, "text": "TABLE 5: Inference Phase — Summary of Threats, Defenses, Limitations, and Recommendations.\nThreat Model\nPrivacy-preserving Defenses\nLimitations\nPhase-Tied Recommendations\n• Internal adversaries: IT/SRE\nteams with access to logs, GPU\nmemory, KV caches; clinicians\nor data scientists entering full\nEHR prompts; teams inspecting\nintermediate embeddings.\n• External adversaries:\nAPI-based attackers, cloud-side\ninversion agents, WAN\ninterceptors, and adapter/split\nexecution exploiters.\n• Capabilities: prompt\nreconstruction, embedding\ninversion, output-based\ninference, cross-session\ncorrelation, KV-cache scraping,\ntiming and metadata profiling.\n• Key Vulnerabilities:\nprompts/inputs, hidden states,\nKV caches, returned text,\nedge–cloud activations,\nrestoration/meta-vectors,\noperational logs/provenance\ntraces.\n• Local DP: DP-Forward,\nsplit-and-denoise, token\nperturbations (InferDPT).\n• Input Obfuscation/Span\nRemoval: PrivacyRestore,\ndynamic substitution/hashing,\nselective masking.\n• Cryptographic Protocols:\nMPC-minimized, PermLLM\n(HE), GPU-accelerated FHE.\n• KV Cache Protection:\nKV-Shield, PFID TEEs.\n• Model\nPartitioning/Edge–Cloud:\nPFID local\nembeddings/adapters;\nPrivateLoRA for local adapter\nexecution.\n• Federated Inference:\neFedLLM for distributed\ninference across institutions.\n• Prompt/Hidden-State\nLeakage: LDP reduces fidelity;\nstatic obfuscation bypassed by\ncross-session correlation.\n• KV Cache Leakage: relies on\ntrusted hardware; persistent or\nmisconfigured caches expose\nfull dialogues.\n• Returned Text/Restoration:\nspan removal fails under\nrepeated probing; restoration\nvectors reveal patient traits.\n• Hybrid Inference Leakage:\nMPC/HE latency exposes\ntiming channels; WAN\nactivations profiled by network\nadversaries.\n• Operational Logging: traces\nretain sensitive\nprompts/activations, creating\npersistent PHI leakage.\n• Prompt/Hidden-State\nProtection: use span-level\nadaptive DP with contextual\nobfuscation for clinical entities.\n• Cross-Session Defense:\nemploy session-specific token\nrandomization and dynamic\nobfuscation schedules.\n• KV Cache Security: enforce\nautomated expiration,\nTEE-based isolation, and strict\nmemory hygiene.\n• Secure Hybrid Inference:\ncombine MPC with\nGPU-accelerated HE; run early\nlayers locally before\ntransmission.\n• Operational\nLogging/Provenance:\nminimize trace capture;\nanonymize logs; use\nprivacy-aware provenance\ntracking.\n5.2. Privacy-Preserving Defenses\nInference-time attacks differ from training risks by tar-\ngeting live system outputs, embeddings, and caches - often\nassumed non-sensitive. In healthcare, this underscores the\nneed for holistic, privacy-by-design defenses protecting\nboth user inputs and transient data. These mechanisms\naim to prevent leakage during runtime execution, when\nPHI is most exposed.\nLocal Differential Privacy (LDP). introduces cali-\nbrated noise to inputs or embeddings before model access,\nlimiting adversarial inference. DP-Forward adds noise dur-\ning the forward pass to obscure subtle prompt differences\n[89], while Split-and-Denoise combines token masking\nwith local DP to resist reconstruction [93]. InferDPT\nadapts this for black-box inference with token-level pertur-\nbations [88]. For instance, a query like “blood in stool with\nrecurring headaches” is perturbed locally, preserving ICD-\nprediction utility while masking identity cues. Excessive\nnoise, however, may weaken rare-disease fidelity.\nInput Obfuscation and Privacy Span Removal.\nselectively mask or substitute sensitive spans like names,\ndosages, or diagnoses before transmission. PrivacyRestore\nremoves PHI locally, transmitting noised meta-vectors\nfor server-side reconstruction [29]; Instance Obfuscation\nhashes tokens dynamically per session [30]. In clinical\nchatbots, such obfuscation ensures confidential prompts\n(e.g., “tested positive for HIV”) never leave the device.\nWhen paired with local DP, these lightweight methods\nreinforce cross-session privacy while preserving reasoning\nquality, ideal for telemedicine and triage.\nCryptographic Protocols (MPC and HE). protect\ninference over untrusted clouds. MPC-minimized secret\nshares only early layers to hide embeddings efficiently\n[94]; PermLLM uses HE with lightweight permutations\nfor secure attention under WAN latency [90]; and GPU-\naccelerated FHE supports encrypted inference for radi-\nology or pathology batch tasks [1]. Though resource-\nintensive, such methods suit regulated domains like on-\ncology where security outweighs delay.\nKV Cache Protection. addresses persistent memory\nrisks. KV-Shield permutes attention matrices to render\nstolen caches useless [24], while PFID confines cache-\nsensitive operations within Trusted Execution Environ-\nments (TEEs) [91]. These safeguards prevent full-session\ntranscript recovery in telemedicine settings with minimal\nlatency impact, assuming secure hardware is available.\nModel Partitioning and Edge-Cloud Collaboration.\nlocalize sensitive processing by executing early layers or\nadapters on secure devices. PFID runs embeddings within\nTEEs before delegating deeper computation [91], while\nPrivateLoRA fine-tunes and executes low-rank adapters\nlocally [33]. This allows mobile clinical apps to handle ini-\ntial contexts (e.g., “47-year-old diabetic with chest pain”)\nprivately before cloud inference. While device demands\nincrease, emerging mobile accelerators make shallow in-\nference viable.\nFederated Inference and Decentralized Deploy-\nment. extend FL principles to inference-time privacy.\neFedLLM distributes inference across multiple nodes so\nno single server processes full prompts [92]. Used in\noncology and multilingual telemedicine networks, each\ninstitution executes partial inference and contributes to\naggregate predictions [4]. This decentralization reduces\ncentral memory risk but introduces synchronization chal-\nlenges across heterogeneous hospital systems.\nTakeaway. Inference-time threats arise not from raw\ndata or gradients but from runtime artifacts, including\nprompts, hidden states, KV caches, intermediate activa-\ntions, and returned text, where PHI may persist across ses-\nsions. Defenses such as Local DP, privacy-span removal,\ncryptographic inference (MPC/HE), KV-cache shielding,\nand edge–cloud partitioning align directly with the at-\ntack surfaces identified in the threat model. Prompt and\nHidden-State Leakage: Local DP and input obfuscation\nprevent direct re-identification from plaintext prompts and\nhidden activations, reducing the risk of embedding in-\n"}, {"page": 13, "text": "version or membership inference attacks. KV-Cache Ex-\nposure: TEEs and KV-Shield protect multi-turn dialogue\ncaches stored in GPU memory, securing patient–clinician\ntranscripts in telemedicine and ICU chat scenarios. Out-\nput and Restoration Risks: Privacy restore mechanisms\nremove or noise sensitive spans in returned text, lim-\niting PHI resurfacing across prompts. Delegated/Hybrid\nInference: Cryptographic protocols such as MPC and HE\nsecure embeddings and activations traversing WAN links,\nshielding clinical data during cloud-based or split infer-\nence. Together, these defenses provide partial containment\nagainst prompt-level leakage, hidden-state reconstruction,\nand output correlation, enhancing privacy assurance across\ndeployed healthcare LLMs.\nLimitation. Despite their value, inference defenses\nremain fragile when mapped to real-world healthcare de-\nployments: (1) Prompt channel and Hidden-State Leak-\nage): Local DP blurs sensitive fields but diminishes\nclinical fidelity; static obfuscation can still be bypassed\nvia cross-session correlation. (2) KV cache leakage: KV\nshielding depends on trusted hardware; misconfigured or\npersistent caches allow recovery of multi-turn patient con-\nversations. (3) Returned text (outputs) and Restoration:\nSpan-level removal fails under repeated queries, allowing\nre-identification through prompt–output correlation. (4)\nDelegated / hybrid inference: MPC and HE secure content\nbut introduce high latency, enabling timing-based infer-\nence of prompt complexity or PHI density. (5) Operational\nlogging and provenance: Tracing and debugging pipelines\noften retain raw prompts and activations, creating durable\nPHI leakage paths even after session termination.\nRecommendation 3: To address these shortcomings,\ninference-time privacy must adopt dynamic, context-\naware protection tied to each identified surface:\n(1) Prompt and Hidden-State Protection: Ap-\nply span-level adaptive DP and contextual obfus-\ncation that selectively perturb only sensitive en-\ntities (e.g., HIV status, rare conditions), preserv-\ning clinical accuracy while blocking re-identification.\n(2) Cross-session correlation Defense: Introduce\nsession-specific randomization of token mappings\nand obfuscation schedules to prevent linkage of\nrepeated prompts across oncology or chronic-care\ncases. (3) KV-Cache Security: Enforce automated\ncache expiration and TEE-based isolation for multi-\nturn dialogue memory, preventing GPU scraping\nor reuse of prior session states. (4) Secure Hy-\nbrid Inference:Use hybrid cryptographic inference\n(MPC + GPU-accelerated HE) to balance latency and\nconfidentiality across WAN-based deployments. (5)\nOperational Logging and Provenance: Minimize\ntrace capture, anonymize diagnostic logs, and em-\nploy privacy-aware provenance tracking to limit long-\nterm PHI persistence. Together, these measures form\na phase-aware, layered strategy explicitly mapped\nto inference-time attack surfaces, closing the resid-\nual gaps between theoretical privacy and real-world\nhealthcare deployment safety.\nTable 5 presents the inference-phase taxonomy, detail-\ning model-, prompt-, and system-level threats alongside\ntheir corresponding defensive strategies, limitations, and\nrecommendations.\n6. Cross-Phase Propagation of Privacy Risks\nAlthough prior sections analyze vulnerabilities within\npreprocessing, federated fine-tuning, and inference indi-\nvidually, real-world healthcare deployments rarely operate\nin isolation. Privacy failures accumulate across phases,\nallowing seemingly minor leaks in one stage to am-\nplify downstream. Preprocessing artifacts such as resid-\nual identifiers, demographic signals, or mislabeled data\ncan be memorized during fine-tuning and later resurfaced\nthrough inference-time extraction attacks. Conversely, in-\nsecure inference interfaces (e.g., prompt injection, KV-\ncache probing) can exfiltrate model representations that\nencode sensitive patterns originating from earlier phases.\nThis lifecycle coupling explains why phase-specific de-\nfenses: differential privacy in training, anonymization in\npreprocessing, or filtering in inference often fail when\ndeployed independently.\nPropagation also creates indirect vulnerabilities. Poi-\nsoned or synthetic records introduced during preprocess-\ning can bias model gradients during fine-tuning, which\nthen distort inference behavior in clinically consequen-\ntial ways. Similarly, unsecured logs, caching mecha-\nnisms, or checkpoint reuse allow auxiliary metadata from\nlater stages to re-identify samples that were previously\nanonymized. Threats therefore move bidirectionally: up-\nstream preprocessing influences representational leakage\ndownstream, while downstream inference interfaces ex-\npose training-time weaknesses upstream. This intercon-\nnectedness underscores a key observation of this SoK:\nprotecting any single phase without accounting for its\ninteractions with the others is structurally insufficient.\nThis insight motivates the need for lifecycle-aware\nprivacy mechanisms, which the conclusion elaborates by\nintegrating cross-phase findings into a coherent set of\ndeployment-ready recommendations.\n7. Conclusion\nThis work presents the first phase-aware systematiza-\ntion of privacy and security in healthcare LLMs, tracing\nrisks and defenses across data preprocessing, fine-tuning,\nand inference. By mapping adversaries, capabilities, attack\nsurfaces, and defenses at each stage, we illustrate how\nprivacy risks manifest uniquely across the LLM lifecycle\nand why phase-agnostic approaches fall short in clinical\nsettings\nOur analysis highlights a central gap: existing tech-\nniques, from anonymization and DP to secure aggrega-\ntion and cryptographic inference, provide protection only\nin isolation. In practice, they struggle with rare-disease\nexposure, multi-round reconstruction, poisoning, and\nruntime leakage through logs, caches, and distributed\ninfrastructure. Effective privacy in healthcare LLMs there-\nfore requires defenses that are context-aware, workflow-\naligned, and robust across phases, not just individually\nstrong. We identify three priorities for future research: (1)\nlayered, adaptive defenses that combine DP, randomized\nadapters, and hardware enclaves with phase-specific tun-\ning; (2) standardized evaluation and auditing protocols\nfor jointly assessing privacy and clinical fidelity; and (3)\n"}, {"page": 14, "text": "lightweight secure computation and federated infer-\nence that meet the latency and reliability demands of real\nclinical deployments.\nBy structuring a fragmented landscape into a uni-\nfied, phase-aware framework, this SoK provides a clear\nroadmap\nfor\ndeveloping\nprivacy-resilient,\nregulation-\naligned, clinically safe LLM systems that can earn trust\nin high-stakes healthcare settings.\nReferences\n[1]\nL. de Castro, A. Polychroniadou, and D. Escudero, “Privacy-\npreserving large language model inference via gpu-accelerated\nfully homomorphic encryption,” Neurips Safe Generative AI Work-\nshop 2024, 2024.\n[2]\nD. T. Lee, A. Vaid, K. M. Menon, R. Freeman, D. S. Matteson,\nM. P. Marin, and G. N. Nadkarni, “Development of a privacy\npreserving large language model for automated data extraction\nfrom thyroid cancer pathology reports,” 2023, medRxiv, 2023-11.\n[3]\nI. C. Wiest, D. Ferber, J. Zhu, M. van Treeck, S. K. Meyer,\nR. Juglan, and J. N. Kather, “Privacy-preserving large language\nmodels for structured medical information retrieval,” NPJ Digital\nMedicine, vol. 7, no. 1, p. 257, 2024.\n[4]\nR. Ye, W. Wang, J. Chai, D. Li, Z. Li, Y. Xu, Y. Du, Y. Wang,\nand S. Chen, “Openfedllm: Training large language models on\ndecentralized private data via federated learning,” Association for\nComputing Machinery, 2024.\n[5]\nA. Manoel, M. Garcia, T. Baumel, S. Su, J. Chen, R. Sim,\nD. Miller, D. Karmon, and D. Dimitriadis, “Federated multilingual\nmodels for medical transcript analysis,” Conference on Health,\nInference, and Learning, 2023, 2023.\n[6]\nZ. Wang, H. Li, D. Huang, and A. M. Rahmani, “Healthq: Unveil-\ning questioning capabilities of llm chains in healthcare conversa-\ntions,” 2024, arXiv preprint arXiv:2409.19487.\n[7]\nN. Khalid, A. Qayyum, M. Bilal, A. Al-Fuqaha, and J. Qadir,\n“Privacy-preserving artificial intelligence in healthcare: Tech-\nniques, challenges and future research directions,” Computers in\nBiology and Medicine, 2023.\n[8]\nJ. Jonnagaddala and Z. S. Y. Wong, “Privacy preserving strategies\nfor electronic health records in the era of large language models,”\nnpj Digital Medicine, vol. 8, no. 1, p. 34, 2025.\n[9]\nA. K. Conduah, S. Ofoe, and D. Siaw-Marfo, “Data privacy in\nhealthcare: Global challenges and solutions,” Frontiers in Digital\nHealth, 2025.\n[10] O. Shoham and N. Rappoport, “Federated learning of medical\nconcepts embedding using behrt,” JAMIA Open. 2024, 2024.\n[11] M. Aljohani, J. Hou, S. Kommu, and X. Wang, “A comprehensive\nsurvey on the trustworthiness of large language models in health-\ncare,” 2025.\n[12] M. S and S. MJ., “Large language models in healthcare and medical\napplications: A review,” Bioengineering, 2025.\n[13] Supervised learning.\nElsevier eBooks, 2022.\n[14] J. Thomas, Preprocessing.\nInforma, 2023, pp. 196–210.\n[15] D. Liu and T. Miller, “Federated pretraining and fine tuning of\nbert using clinical notes from multiple silos,” 2020, arXiv preprint\narXiv:2002.08562.\n[16] C. Christophe, P. K. Kanithi, P. Munjal, T. Raha, N. Hayat,\nR. Rajan, A. Al-Mahrooqi, A. Gupta, M. U. Salman, G. Gosal\net al., “Med42–evaluating fine-tuning strategies for medical llms:\nfull-parameter vs. parameter-efficient approaches,” arXiv preprint\narXiv:2404.14779, 2024.\n[17] F. Lucini, “The real deal about synthetic data,” MIT Sloan Man-\nagement Review, vol. 63, no. 1, pp. 1–4, 2021.\n[18] H. Yang, H. Chen, H. Guo, Y. Chen, C. S. Lin, S. Hu, and\nX. Wang, “Llm-medqa: Enhancing medical question answering\nthrough case studies in large language models,” 2024, arXiv\npreprint arXiv:2501.05464.\n[19] H. Lu and U. Naseem, “Can large language models enhance\npredictions of disease progression? investigating through disease\nnetwork link prediction,” in Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing, 2024, pp.\n17 703–17 715.\n[20] H. Jung, Y. Kim, H. Choi, H. Seo, M. Kim, J. Han, and Y. H. Kim,\n“Enhancing clinical efficiency through llm: Discharge note gener-\nation for cardiac patients,” 2024, arXiv preprint arXiv:2404.05144.\n[21] Gagan, N, Sanand, and Sasidharan, “Enhancing oncology care with\nfederated learning and foundation models,” 2024 ITU Kaleido-\nscope: Innovation and Digital Transformation for a Sustainable\nWorld (ITU K), 2024.\n[22] Y. Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y. Zhang, “A medical\nchat model fine-tuned on a large language model meta-ai (llama)\nusing medical domain knowledge.” Cureus. 2023, 2023.\n[23] A. Anaissi, A. Braytee, and J. Akram, “Fine-tuning llms for\nreliable medical question-answering services,” 2024, arXiv preprint\narXiv:2410.16088.\n[24] H. Yang, D. Zhang, Y. Zhao, Y. Li, and Y. Liu, “A first look at\nefficient and secure on-device llm inference against kv leakage,”\nProceedings of the 19th Workshop on Mobility in the Evolving\nInternet Architecture, 2024, 2024.\n[25] D. Chen, A. Youssef, R. Pendse, A. Schleife, B. K. Clark,\nH. Hamann, J. He et al., “Transforming the hybrid cloud for\nemerging ai workloads,” arXiv, 2025.\n[26] D. Oniani, X. Wu, S. Visweswaran, S. Kapoor, S. Kooragayalu,\nK. Polanska, and Y. Wang, “Enhancing large language models for\nclinical decision support by incorporating clinical practice guide-\nlines,” in 2024 IEEE 12th International Conference on Healthcare\nInformatics (ICHI).\nIEEE, 2024, pp. 694–702.\n[27] A. Ghosh, A. Acharya, R. Jain, S. Saha, A. Chadha, and S. Sinha,\n“Clipsyntel: clip and llm synergy for multimodal question summa-\nrization in healthcare,” in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 38, no. 20, 2024, pp. 22 031–22 039.\n[28] Z. Wang, Z. Shen, Y. He, G. Sun, H. Wang, L. Lyu, and A. Li,\n“Flora: Federated fine-tuning large language models with hetero-\ngeneous low-rank adaptations,” arXiv preprint arXiv:2409.05976,\n2024.\n[29] Z. Zeng, J. Wang, J. Yang, Z. Lu, H. Zhuang, and C. Chen,\n“Privacyrestore: Privacy-preserving inference in large language\nmodels via privacy removal and restoration,” arXiv preprint\narXiv:2406.01394, 2024, 2024.\n[30] Y. Yao, F. Wang, S. Ravi, and M. Chen, “Privacy-preserving lan-\nguage model inference with instance obfuscation,” arXiv preprint\narXiv:2402.08227, 2024, 2024.\n[31] N. Kandpal, K. Pillutla, A. Oprea, P. Kairouz, C. A. Choquette-\nChoo, and Z. Xu, “User inference attacks on large language\nmodels,” ICASSP 2024 - 2024 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP),, 2024.\n[32] R. Staab, M. Vero, M. Balunovi´c, and M. Vechev, “Beyond memo-\nrization: Violating privacy via inference with large language,” arXiv\npreprint arXiv:2310.07298, 2023, 2023.\n[33] Y. Wang, Y. Lin, X. Zeng, and G. Zhang, “Privatelora for efficient\nprivacy preserving llm,” arXiv preprint arXiv:2311.14030, 2023,\n2023.\n[34] A. Gadotti, L. Rocher, F. Houssiau, A. M. Cret¸u, and Y. A. D.\nMontjoye, “Anonymization: The imperfect science of using data\nwhile preserving privacy,” Science Advances, vol. 10, no. 29, p.\neadn7053, 2024.\n[35] R. Tertulino, N. Antunes, and H. Morais, “Privacy in electronic\nhealth records: a systematic mapping study,” Journal of Public\nHealth, vol. 32, no. 3, pp. 435–454, 2024.\n[36] M. Hussain, N. Akhtar, and R. Hasan, “A robust framework for\nensuring data confidentiality and security in modern healthcare\nnetworks,” International Journal of Intelligent Systems and Ap-\nplications in Engineering, vol. 12, no. 3, pp. 128–137, 2024.\n[37] R. Zhang, R. Xue, and L. Liu, “Searchable encryption for health-\ncare clouds: A survey,” IEEE Transactions on Services Computing,\nvol. 11, no. 6, pp. 978–996, 2017.\n"}, {"page": 15, "text": "[38] S. Abdali, R. Anarfi, C. J. Barberan, and J. He, “Securing large lan-\nguage models: Threats, vulnerabilities and responsible practices,”\narXiv preprint arXiv:2403.12503, 2024.\n[39] Anonymous, “Ensuring data security and compliance in etl\nprocesses for healthcare and financial services,” ResearchGate\n(preprint), 2024.\n[40] J. Cˆandido, M. Aniche, and A. V. Deursen, “Log-based software\nmonitoring: a systematic mapping study,” PeerJ Computer Science,\nvol. 7, p. e489, 2021.\n[41] H. Bahsi and A. Levi, “Preserving organizational privacy in intru-\nsion detection log sharing,” in 2011 3rd International Conference\non Cyber Conflict.\nIEEE, June 2011, pp. 1–14.\n[42] National Audit Office, “Investigation: Wannacry cyber attack and\nthe nhs,” National Audit Office, UK, Tech. Rep., 2018.\n[43] F. Wang and B. Li, “Data reconstruction and protection in federated\nlearning for fine-tuning large language models,” IEEE Transactions\non Big Data, 2024, 2024.\n[44] H. Li, Y. Chen, J. Luo, J. Wang, H. Peng, Y. Kang, and Y. Song,\n“Privacy in large language models: Attacks, defenses and future\ndirections,” 2023, arXiv preprint arXiv:2310.10383.\n[45] E. Shayegani, M. A. A. Mamun, Y. Fu, P. Zaree, Y. Dong,\nand N. Abu-Ghazaleh, “Survey of vulnerabilities in large lan-\nguage models revealed by adversarial attacks,” arXiv preprint\narXiv:2310.10844, 2023.\n[46] A. Narayanan and V. Shmatikov, “Robust de-anonymization of\nlarge sparse datasets,” in 2008 IEEE Symposium on Security and\nPrivacy (sp 2008).\nIEEE, 2008, pp. 111–125.\n[47] T. Stadler, B. Oprisanu, and C. Troncoso, “Synthetic data–\nanonymisation groundhog day,” in 31st USENIX Security Sympo-\nsium (USENIX Security 22).\nUSENIX Association, 2022, pp.\n1451–1468.\n[48] “Cyber threat modeling of an llm-based healthcare system,” Inter-\nnational Conference on Information Systems Security and Privacy,\n2025.\n[49] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and\nB. Li, “Manipulating machine learning: Poisoning attacks and\ncountermeasures,” in Proceedings of the IEEE Symposium on\nSecurity and Privacy (SP), 2018, pp. 19–35.\n[50] W. F. Shah, “Data preprocessing in healthcare: A vital step towards\ninformed decision-making,” 2024.\n[51] E. Shamsinejad, T. Banirostam, M. M. Pedram, and A. M. Rah-\nmani, “A review of anonymization algorithms and methods in big\ndata,” Annals of Data Science, pp. 1–27, 2024.\n[52] O. Vovk, G. Piho, and P. Ross, “Methods and tools for healthcare\ndata anonymization: a literature review,” International Journal of\nGeneral Systems, vol. 52, no. 3, pp. 326–342, 2023.\n[53] A. Aminifar, Y. Lamo, K. I. Pun, and F. Rabbi, “A practical\nmethodology for anonymization of structured health data,” 2019.\n[54] S. L. Ribeiro and E. T. Nakamura, “Privacy protection with\npseudonymization and anonymization in a health iot system: results\nfrom ocariot,” in 2019 IEEE 19th International Conference on\nBioinformatics and Bioengineering (BIBE). IEEE, 2019, pp. 904–\n908.\n[55] I. E. Olatunji, J. Rauch, M. Katzensteiner, and M. Khosla, “A\nreview of anonymization for healthcare data,” Big data, vol. 12,\nno. 6, pp. 538–555, 2024.\n[56] Y. Qu, Y. Dai, S. Yu, P. Tanikella, T. Schrank, T. Hackman, and\nD. Wu, “A novel compact llm framework for local, high-privacy\nehr data applications,” 2024, arXiv preprint arXiv:2412.02868.\n[57] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks\nthat exploit confidence information and basic countermeasures,” in\nProceedings of the 22nd ACM SIGSAC Conference on Computer\nand Communications Security (CCS), 2015, pp. 1322–1333.\n[58] D. R. Alattal, Z. Wang, P. Myles, and A. Tucker, “Creating\nsynthetic geospatial patient data to mimic real data whilst pre-\nserving privacy,” in 2023 IEEE 36th International Symposium on\nComputer-Based Medical Systems (CBMS). IEEE, 2023, pp. 7–12.\n[59] M. Jin, Q. Yu, C. Zhang, D. Shu, S. Zhu, M. Du, and Y. Meng,\n“Health-llm: Personalized retrieval-augmented disease prediction\nmodel,” 2024, arXiv preprint arXiv:2402.00746.\n[60] J. A. Alzate-Grisales, J. Bernal-Salcedo, J. M. Saborit-Torres,\nA. Mora-Rubio, J. M. Serrano, F. Garc´ıa-Garc´ıa, and M. D. L.\nIglesia-Vay´a, “Dismed-llm: De-identifying spanish medical text\nwith large language models,” 2025.\n[61] M. Abbasian, I. Azimi, A. M. Rahmani, and R. Jain, “Conver-\nsational health agents: A personalized llm-powered agent frame-\nwork,” 2023, arXiv preprint arXiv:2310.02374.\n[62] M. A. Roshani, X. Zhou, Y. Qiang, S. Suresh, S. Hicks, U. Sethu-\nraman, and D. Zhu, “Generative llm powered conversational ai\napplication for personalized risk assessment: A case study in covid-\n19,” 2024, arXiv preprint arXiv:2409.15027.\n[63] M. Naji, M. Masmoudi, and H. B. Zghal, “Towards an llm based\napproach for medical e-consent,” Procedia Computer Science, vol.\n246, pp. 3694–3701, 2024.\n[64] A. Goncalves, P. Ray, B. Soper, J. Stevens, L. Coyle, and A. P.\nSales, “Generation and evaluation of synthetic patient data,” BMC\nmedical research methodology, vol. 20, pp. 1–40, 2020.\n[65] X. Chen, Z. Wu, X. Shi, H. Cho, and B. Mukherjee, “Generat-\ning synthetic electronic health record (ehr) data: A review with\nbenchmarking,” 2024, arXiv preprint arXiv:2411.04281.\n[66] I. P´erez-D´ıez, R. P´erez-Moraga, A. L´opez-Cerd´an, J. M. Salinas-\nSerrano, and M. D. la Iglesia-Vay´a, “De-identifying spanish med-\nical texts-named entity recognition applied to radiology reports,”\nJournal of Biomedical Semantics, vol. 12, pp. 1–13, 2021.\n[67] M. Ibrahim, Y. A. Khalil, S. Amirrajab, C. Sun, M. Breeuwer,\nJ. Pluim, and M. Dumontier, “Generative ai for synthetic\ndata across multiple medical modalities: A systematic review\nof recent developments and challenges,” 2024, arXiv preprint\narXiv:2407.00116.\n[68] M. Ali, M. Ali, M. Hussain, and D. Koundal, “Generative adver-\nsarial networks (gans) for medical image processing: Recent ad-\nvancements,” Archives of Computational Methods in Engineering,\npp. 1–14, 2024.\n[69] S. Kweon, J. Kim, J. Kim, S. Im, E. Cho, S. Bae, and E. Choi,\n“Publicly shareable clinical large language model built on synthetic\nclinical notes,” 2023, arXiv preprint arXiv:2309.00237.\n[70] Z. Wang, P. Myles, and A. Tucker, “Generating and evaluating\ncross-sectional synthetic electronic healthcare data: preserving data\nutility and patient privacy,” Computational Intelligence, vol. 37,\nno. 2, pp. 819–851, 2021.\n[71] K. M. Babu, E. Bhavitha, M. Mythri, A. Anusha, B. S. Chan-\ndana, and C. G. Akhtar, “Privacy-preserving federated learning for\nhealthcare: A synergistic approach using differential privacy and\nhomomorphic encryption,” 2024, available at SSRN 5088943.\n[72] S. Sadeepa, K. Kavinda, E. Hashika, C. Sandeepa, T. Gamage,\nand M. Liyanage, “Disllm: Distributed llms for privacy assurance\nin resource-constrained environments,” in 2024 IEEE Conference\non Communications and Network Security (CNS).\nIEEE, 2024,\npp. 1–9.\n[73] T. Bossy, J. Vignoud, T. Rabbani, J. R. T. Pastoriza, and M. Jaggi,\n“Mitigating unintended memorization with lora in federated learn-\ning for llms,” 2025, arXiv preprint arXiv:2502.05087.\n[74] Kokala and Abhilash, “Scalable large language models for the\nhealthcare domain: A research perspective,” 2025.\n[75] H. Fu, H. Chen, S. Lin, and G. Xing, “Shade-ad: An llm-based\nframework for synthesizing activity data of alzheimer’s patients,”\n2025, arXiv preprint arXiv:2503.01768.\n[76] F. Piccialli, D. Chiaro, P. Qi, V. Bellandi, and E. Damiani, “Fed-\nerated and edge learning for large language models.” Information\nFusion, 2025.\n[77] Liu, Xiao-Yang, Zhu, Rongyi, Zha, Daochen, Gao, Jiechao, Zhong,\nShan, White, Matt, Qiu, and Meikang, “Differentially private low-\nrank adaptation of large language model using federated learning,”\nAssociation for Computing Machinery, 2024.\n[78] C. Li, B. Gu, Z. Zhao, Y. Qu, G. Xin, J. Huo, and L. Gao,\n“Federated transfer learning for on-device llms efficient fine tuning\noptimization,” Big Data Mining and Analytics, 2025.\n"}, {"page": 16, "text": "[79] X. Zuo, M. Wang, T. Zhu, S. Yu, and W. Zhou, “Large language\nmodel federated learning with blockchain and unlearning for cross-\norganizational collaboration,” arXiv preprint, 2024.\n[80] Z. Lin, X. Hu, Y. Zhang, Z. Chen, Z. Fang, X. Chen, A. Li,\nP. Vepakomma, and Y. Gao, “Splitlora: A split parameter-efficient\nfine-tuning framework for large language models,” arXiv preprint,\n2024.\n[81] J. Zhao, “Privacy-preserving fine-tuning of artificial intelligence\n(ai) foundation models with federated learning, differential privacy,\noffsite tuning, and parameter-efficient fine-tuning (peft),” Authorea\nPreprints, 2023, 2023.\n[82] B. C. Das, M. H. Amini, and Y. Wu, “Security and privacy\nchallenges of large language models: A survey,” ACM Computing\nSurveys, vol. 57, no. 6, pp. 1–39, 2025.\n[83] J. Jiang, H. Jiang, Y. Ma, X. Liu, and C. Fan, “Low-parameter\nfederated learning with large language models,” International Con-\nference on Web Information Systems and Applications, 2024.\n[84] F. Jiang, L. Dong, S. Tu, Y. Peng, K. Wang, K. Yang, C. Pan,\nand D. Niyato, “Personalized wireless federated learning for large\nlanguage models,” arXiv preprint, 2024.\n[85] F. Wu, Z. Li, Y. Li, B. Ding, and J. Gao, “Fedbiot: Llm local fine-\ntuning in federated learning without full model,” Association for\nComputing Machinery, 2024.\n[86] J. Qi, Z. Luan, S. Huang, C. Fung, H. Yang, and D. Qian, “Fdlora:\nPersonalized federated learning of large language model via dual\nlora tuning,” arXiv preprint arXiv:2406.07925, 2024.\n[87] Z. Fang, Z. Lin, Z. Chen, X. Chen, Y. Gao, and Y. Fang, “Auto-\nmated federated pipeline for parameter-efficient fine-tuning of large\nlanguage models,” arXiv preprint, 2024.\n[88] M. Tong, K. Chen, J. Zhang, Y. Qi, W. Zhang, N. Yu, T. Zhang,\nand Z. Zhang, “Inferdpt: Privacy-preserving inference for black-\nbox large language models,” IEEE Transactions on Dependable\nand Secure Computing, 2025, 2025.\n[89] M. Du, X. Yue, S. Chow, T. Wang, C. Huang, and H. Sun,\n“Dp-forward: Fine-tuning and inference on language models with\ndifferential privacy in forward pass,” Proceedings of the 2023 ACM\nSIGSAC Conference on Computer and Communications, 2023,\n2023.\n[90] F. Zheng, C. Chen, Z. Han, and X. Zheng, “Permllm: Private\ninference of large language models within 3 seconds under wan,”\narXiv preprint arXiv:2405.18744, 2024, 2024.\n[91] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao,\n“Pfid: Privacy first inference delegation framework for llms,” arXiv\npreprint arXiv:2406.12238, 2024, 2024.\n[92] S. Ding and C. Hu, “efedllm: Efficient llm inference based on\nfederated learning,” arXiv preprint arXiv:2411.16003, 2024, 2024.\n[93] P. Mai, R. Yan, Z. Huang, Y. Yang, and Y. Pang, “Split-and-\ndenoise: Protect large language model inference with local dif-\nferential privacy,” arXiv preprint arXiv:2310.09130, 2023, 2023.\n[94] D. Rathee, D. Li, I. Stoica, H. Zhang, and R. Popa, “Mpc-\nminimized secure llm inference,” arXiv preprint arXiv:2408.03561,\n2024, 2024.\n"}]}