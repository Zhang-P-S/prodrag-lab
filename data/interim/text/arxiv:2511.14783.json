{"doc_id": "arxiv:2511.14783", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.14783.pdf", "meta": {"doc_id": "arxiv:2511.14783", "source": "arxiv", "arxiv_id": "2511.14783", "title": "Human or LLM as Standardized Patients? A Comparative Study for Medical Education", "authors": ["Bingquan Zhang", "Xiaoxiao Liu", "Yuchi Wang", "Lei Zhou", "Qianqian Xie", "Benyou Wang"], "published": "2025-11-12T11:05:41Z", "updated": "2026-01-20T02:56:42Z", "summary": "Standardized patients (SPs) are indispensable for clinical skills training but remain expensive and difficult to scale. Although large language model (LLM)-based virtual standardized patients (VSPs) have been proposed as an alternative, their behavior remains unstable and lacks rigorous comparison with human standardized patients. We propose EasyMED, a multi-agent VSP framework that separates case-grounded information disclosure from response generation to support stable, inquiry-conditioned patient behavior. We also introduce SPBench, a human-grounded benchmark with eight expert-defined criteria for interaction-level evaluation. Experiments show that EasyMED more closely matches human SP behavior than existing VSPs, particularly in case consistency and controlled disclosure. A four-week controlled study further demonstrates learning outcomes comparable to human SP training, with stronger early gains for novice learners and improved flexibility, psychological safety, and cost efficiency.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.14783v2", "url_pdf": "https://arxiv.org/pdf/2511.14783.pdf", "meta_path": "data/raw/arxiv/meta/2511.14783.json", "sha256": "7bfe8e995eb21630bd3f187510c7c379607e0066ab06bb827808c0c43a641a2d", "status": "ok", "fetched_at": "2026-02-18T02:27:21.911649+00:00"}, "pages": [{"page": 1, "text": "Human or LLM as Standardized Patients?\nA Comparative Study in Medical Education\nBingquan Zhang1,2, Xiaoxiao Liu2, Yuchi Wang2, Lei Zhou3, Qianqian Xie1†, Benyou Wang2†\n1 School of Artifical Intelligence, Wuhan University\n2 The Chinese University of Hong Kong, Shenzhen\n3 Freedom AI\nAbstract\nStandardized patients (SPs) are indispensable\nfor clinical skills training but remain expensive\nand difficult to scale. Although large language\nmodel (LLM)–based virtual standardized pa-\ntients (VSPs) have been proposed as an alterna-\ntive, their behavior remains unstable and lacks\nrigorous comparison with human standardized\npatients. We propose EasyMED, a multi-agent\nVSP framework that separates case-grounded\ninformation disclosure from response genera-\ntion to support stable, inquiry-conditioned pa-\ntient behavior. We also introduce SPBench, a\nhuman-grounded benchmark with eight expert-\ndefined criteria for interaction-level evalua-\ntion. Experiments show that EasyMED more\nclosely matches human SP behavior than exist-\ning VSPs, particularly in case consistency and\ncontrolled disclosure. A four-week controlled\nstudy further demonstrates learning outcomes\ncomparable to human SP training, with stronger\nearly gains for novice learners and improved\nflexibility, psychological safety, and cost effi-\nciency.\n1\nIntroduction\nClinical reasoning and doctor–patient communica-\ntion are essential skills in medical education (Cle-\nland and Durning, 2022). Their development relies\non repeated, interactive practice in realistic clinical\nsettings. Standardized patients, trained actors who\nconsistently portray predefined clinical cases, are\nwidely regarded as the gold standard for teaching\nand assessing these skills, particularly in Objective\nStructured Clinical Examinations (OSCEs) (Sayers\net al., 2024; Ma et al., 2023). While SP-based train-\ning enables safe and authentic clinical encounters,\nhuman SP programs are costly, labor-intensive, and\ndifficult to scale, which limits training frequency\nand accessibility (Zendejas et al., 2013). Conse-\nquently, large language model (LLM) based vir-\ntual standardized patients (VSP) have emerged as\na promising scalable alternative (Du et al., 2024;\nYe and Tang, 2025), due to their strong dialogue\ncapabilities and broad world knowledge.\nDespite recent progress, it remains unclear\nwhether LLM-based VSP can support clinical skills\ntraining at a level comparable to human standard-\nized patients. This question is difficult to answer\ndue to persistent gaps in system design and eval-\nuation that are misaligned with real SP training\npractice. Most VSP frameworks conflate inquiry\ninterpretation with response generation, leading to\npremature information disclosure, cross-turn insta-\nbility, and limited support for intent-aware instruc-\ntional feedback (Du et al., 2024; Ye and Tang, 2025;\nSirdeshmukh et al., 2025). Existing evaluations\nare largely coarse-grained, relying on synthetic di-\nalogues or outcome-level metrics rather than au-\nthentic human SP–doctor interactions (Fan et al.,\n2023; Waisberg et al., 2024). Moreover, systematic\nlong-term comparisons with human standardized\npatients under matched training conditions remain\nrare (Liu et al., 2025; Bodonhelyi et al., 2025), leav-\ning the educational effectiveness of LLM-based\nvirtual patients insufficiently validated.\nMulti-agent VSP To address the limitations identi-\nfied above, We propose EasyMED, a controllable\nmulti-agent framework that models virtual SP train-\ning as a structured, interactive process. EasyMED\ndecouples patient simulation, intent recognition,\nand evaluation into coordinated agents, enabling\nintent-conditioned information disclosure, stable\ncross-turn behavior, and checklist-based instruc-\ntional feedback. This design directly supports pa-\ntient fidelity, interaction coherence, and pedagogi-\ncal awareness in virtual SP training.\nSP Benchmark To support reproducible and\ninteraction-level evaluation, we further introduce\nSPBench, a benchmark constructed from authentic\nstandardized patient-doctor dialogues spanning 14\nmedical specialties and eight expert-defined evalu-\nation criteria (Fan et al., 2023; Sirdeshmukh et al.,\n2025). Unlike existing benchmarks that rely on\narXiv:2511.14783v2  [cs.CL]  20 Jan 2026\n"}, {"page": 2, "text": "SP\nPatient Fidelity\nInteraction Coherence\nPedagogical Awareness\nReal-world User Study\nHuman SP\n✓\n✓\n✓\n-\nSimPatient (Steenstra et al., 2025)\n✗\n✗\n✓\n✗\nEvoPatient (Du et al., 2024)\n✗\n✗\n✗\n✗\nCureFun (Li et al., 2024b)\n✗\n✗\n✓\n✗\nEasyMED (Ours)\n✓\n✓\n✓\n✓\nPatient Agent\nAuxiliary Agent\nEvaluation Agent\n(Sec. 6)\nintent-conditioned disclosure\nfactorized patient simulation\ntrajectory-level feedback\nTable 1: Comparison of various standardized patient systems across three desiderata defined in Section 2, based on\nwhat is explicitly reported in the original papers. SimPatient evaluates realism primarily via qualitative user studies\nand adopts end-to-end patient response generation, while providing utterance-level reflective feedback. EvoPatient\nemphasizes emergent realism through agent co-evolution without explicit control over information disclosure or\nlearner-facing instructional feedback. CureFun focuses on educational usefulness without controlled comparison\nto human standardized patients and performs end-to-end patient simulation, while offering post-session learning\nsummaries. EasyMed embeds patient fidelity, interaction coherence, and pedagogical awareness directly into its\narchitecture via intent-conditioned disclosure calibrated by SPBench (Sec. 5), factorized patient simulation using\nAuxiliary Agent (Sec. 3.3), and trajectory-level evaluation in Evaluation Agent (Sec. 3.4).\nsynthetic dialogues or outcome-level scores, SP-\nBench uses human SP interaction trajectories as ref-\nerence to quantitatively compare virtual and human\nstandardized patient behavior. Using SPBench,\nwe compare EasyMED and representative exist-\ning VSPs against human SP interaction trajectories,\nand find closer alignment with human SP behavior,\nparticularly in controlled disclosure and cross-turn\nconsistency.\nReal-world User Study To assess educational ef-\nfectiveness in real training settings, we conduct a\nfour-week controlled study comparing EasyMED\nwith human standardized patient training under\nmatched content and scoring rubrics, directly ad-\ndressing whether LLM-based virtual patients can\nachieve training effectiveness comparable to hu-\nman SPs.\nOur contributions are threefold: (1) we pro-\npose EasyMED, a multi-agent virtual standardized\npatient framework that enables controllable, inter-\npretable patient simulation aligned with clinical\ntraining workflows; (2) we propose SPBench, a\nhuman-grounded benchmark that enables quantita-\ntive, interaction-level comparison between virtual\nand human standardized patient behavior; (3) we\npresent a controlled, longitudinal user study com-\nparing LLM-based and human standardized patient\ntraining under matched conditions.\n2\nBackground\nVSPs are increasingly adopted for clinical training\ndue to their scalability and accessibility. However,\ntheir educational value hinges not only on their abil-\nity to simulate patient dialogue, but also on whether\nthey can faithfully reproduce human standardized\npatient behavior (see Desideratum I), remain sta-\nble and controllable across extended interactions\n(see Desideratum II), and actively support clinical\nlearning feedback (see Desideratum III). In prac-\ntice, current VSPs often fall short in one or more of\nthese aspects, limiting their reliability as training\ntools, see Table 1.\nDesideratum I. Patient Fidelity. VSPs should ex-\nhibit behaviors and response patterns comparable\nto those of human standardized patients.\nAchieving such fidelity requires evaluation pro-\ntocols that enable direct experimental comparison\nwith human SPs, rather than relying solely on sub-\njective user satisfaction. However, most prior stud-\nies on VSPs primarily adopt qualitative evaluations\nbased on structured interviews and satisfaction sur-\nveys (Steenstra et al., 2025; Du et al., 2024; Li\net al., 2024b). As a result, direct comparisons with\nhuman SPs remain rare, and the educational effec-\ntiveness of LLM-based VSPs relative to traditional\nhuman SP training is still unclear (Simzine, 2025).\nDesideratum II. Interaction Coherence. VSPs\nmust maintain consistent clinical states and con-\ntrolled information disclosure across multi-turn in-\nteractions.\nInteraction coherence requires separating what\nclinical information is revealed from how it is ex-\npressed, so as to prevent case drift and unintended\ninformation leakage across turns. However, most\nexisting VSPs generate responses in an end-to-end\nmanner without explicitly modeling this separa-\ntion (Steenstra et al., 2025; Li et al., 2024b). As a\nresult, although responses may appear locally co-\nherent, longer interactions often exhibit cross-turn\n"}, {"page": 3, "text": "Figure 1: Overview of the multi-agent architecture of the virtual standardized patient system, consisting of a Patient\nAgent, an Intent Recognition Agent, and an Evaluation Agent.\ninstability and uncontrolled disclosure, undermin-\ning the reliability of VSPs for medical education.\nDesideratum III. Pedagogical Awareness. VSP-\nbased training systems should be aware of the\nlearner’s educational objectives and interaction\nprocess, enabling fine-grained instructional feed-\nback that supports clinical learning rather than\nmerely simulating patient behavior.\nPedagogical awareness requires VSPs to go be-\nyond passive patient simulation and actively sup-\nport learning by monitoring the learner’s inquiry\nprocess and identifying opportunities for guidance.\nGrounding feedback in the interaction trajectory\nallows such systems to highlight missing, redun-\ndant, or inappropriate clinical questions and sup-\nport reflective learning. However, most existing\nVSPs function primarily as conversational agents\nand lack explicit representations of clinical intent\nor inquiry coverage, limiting their ability to provide\nmeaningful instructional feedback (Steenstra et al.,\n2025; Du et al., 2024).\n3\nEasyMED: A Multi-Agent VSP\nFramework\n3.1\nWorkflow of EasyMED\nPhilosophy of EasyMED\nSection 2 identi-\nfies three desiderata for virtual standardized\npatients—patient fidelity, interaction coherence,\nand pedagogical awareness—that are difficult to\nachieve with end-to-end simulators. EasyMED\ntreats these desiderata as explicit design constraints\nand maps them to concrete architectural choices:\nintent-conditioned response generation to preserve\npatient fidelity, decoupled case-grounded informa-\ntion access and surface realization to ensure inter-\naction coherence, and trajectory-level retention for\nchecklist-based educational feedback. This prin-\ncipled mapping naturally motivates a factorized,\nmulti-agent design.\nEasyMED implements a factorized workflow\nwith two phases: consultation and evaluation, as\nshown in Figure 1. Phase 1 is realized by the Aux-\niliary and Patient Agents for intent recognition and\npatient simulation, while Phase 2 is conducted by\nthe Evaluation Agent for trajectory-level assess-\nment and feedback.\nPhase 1: Consultation During consultation, the\ninteraction unfolds as a multi-turn dialogue\nD = {(q1, r1), . . . , (qT , rT )},\n(1)\nwhere qt is the learner’s question and rt the patient\nresponse at turn t. At each turn, EasyMED first\ninfers a standardized clinical intent\nit = A(qt, Ht−1),\n(2)\nand then generates a case-grounded response\nrt = P(it, qt, Ht−1 | E).\n(3)\nBy factorizing intent recognition and response gen-\neration, EasyMED enables inquiry-conditioned dis-\nclosure and stable multi-turn patient behavior.\nPhase 2: Evaluation After the consultation ends,\nthe Evaluation Agent reviews the full dialogue tra-\njectory D and compares the recognized intents and\nelicited facts against expert-defined case checklists\nto produce structured feedback.\n"}, {"page": 4, "text": "Criterion\nAbbr.\nDescription\nQuery Comprehension\nQC\nAccurate understanding of the physician’s question and its intent without misinterpretation\nCase Consistency\nCC\nFaithfulness to the predefined patient case, without contradictions or unsupported facts.\nControlled Disclosure\nCD\nProviding only requested information, avoiding unsolicited or premature disclosure.\nResponse Completeness\nRC\nFully addressing all aspects of the physician’s query without omitting essential case\ninformation.\nLogical Coherence\nLC\nInternal logical consistency of responses, ensuring symptoms and attributes remain\ncoherent.\nLanguage Naturalness\nLN\nUse of natural, patient-like language while avoiding unnecessary medical jargon.\nConversational Consistency CS\nConsistency of information across dialogue turns, avoiding self-contradictions.\nPatient Demeanor\nPD\nMaintaining an appropriate patient-like emotional tone, including cooperation and stabil-\nity.\nTable 2: Definitions of the eight evaluation criteria used in SPBench.\n3.2\nAuxiliary Agent\nThe Auxiliary Agent addresses a core limitation\nof existing virtual standardized patients by explic-\nitly modeling the learner’s clinical inquiry rather\nthan relying on end-to-end text generation. It maps\neach learner question to a predefined clinical intent\n(e.g., Chief Complaint or Onset), abstracting away\nsurface-level linguistic variation. This standard-\nized intent representation serves as a control signal\nfor downstream patient simulation, ensuring that\nresponses are conditioned on inquiry type rather\nthan phrasing, thereby enabling controlled informa-\ntion disclosure and stable patient behavior across\nmulti-turn interactions.\n3.3\nPatient Agent\nThe Patient Agent simulates patient behavior while\nenforcing case fidelity and disclosure constraints.\nGiven an inferred clinical intent, it first retrieves\nthe corresponding fact from a structured electronic\nhealth record that defines the patient’s ground-truth\ncase information, and then generates a natural lan-\nguage response conditioned on both the retrieved\nfact and a predefined patient persona (e.g., anxiety\nor hesitation). This separation between fact selec-\ntion and surface realization enables controlled in-\nformation disclosure while preserving natural con-\nversational flow.\n3.4\nEvaluation Agent\nThe Evaluation Agent acts as a post-hoc pedagogi-\ncal observer. Rather than intervening during con-\nversation, it reviews the interaction history once the\nsession ends. By comparing recognized intents and\nelicited facts against the standard case checklist,\nthe agent generates structured feedback to high-\nlight gaps like Missed Inquiries. This provides\nstudents with actionable feedback, addressing the\nevaluation limitations discussed in Section 2.\n4\nSPBench: Benchmark for Virtual SP\n4.1\nThe Philosophy of SPBench\nExisting medical benchmarks primarily assess\nstatic knowledge or aggregate outcomes and\nfail to capture interactive patient behavior (e.g.,\nMMLU (Hendrycks et al., 2021), MedQA (Jin\net al., 2021)), forcing VSP evaluation to rely\non synthetic or interview-based data (Fan et al.,\n2023; Waisberg et al., 2024). SPBench fills this\ngap by grounding evaluation in authentic human\nSP–doctor dialogue trajectories and adopting a stan-\ndardized protocol with eight clinically motivated di-\nmensions that assess both turn-level response qual-\nity and session-level behavioral consistency.\n4.2\nData Curation\nData Structure SPBench contains two main parts:\n(1) Patient Profile: These describe the patient’s\nmain complaint, symptoms, history, and back-\nground; and (2) Authentic Dialogue Records: turn-\nlevel transcripts showing how trained human SP\nrespond to clinical questions in real instructional\nsettings. We use these records as a standard. They\nallow us to compare the AI’s performance against\na human actor handling the same case.\n0-10: 10.3%\n21-30: 19.0%\n31-40: 22.4%\n41-50: 17.2%\n51-60: 20.7%\n61-70: 8.6%\n71-80: 1.7%\nA. Age Distribution\nFemale: 60.3%\nMale: 39.7%\nB. Gender Distribution\nFigure 2: Demographic distribution of cases in the SP-\nBench dataset. The left panel shows the age distribution,\nand the right panel shows the gender distribution.\nCase Source and Scope SPBench is derived from\ntwo widely used training books for Standardized\nPatients—the Manual for Writing Standardized Pa-\n"}, {"page": 5, "text": "System\nQC\nCC\nCD\nRC\nLC\nLN\nCS\nPD\nOverall\nLLMs\nQwen3-8B\n77.76\n78.64\n84.26\n85.74\n83.37\n72.14\n85.74\n81.31\n81.12\nQwen3-32B\n88.37\n88.37\n89.31\n89.31\n88.99\n85.89\n89.91\n89.61\n88.87\nDeepSeek-R1\n91.06\n93.08\n93.42\n97.10\n96.10\n85.38\n93.69\n95.05\n93.11\nGPT-4o\n75.87\n89.24\n88.89\n98.11\n91.31\n91.66\n90.62\n90.62\n89.54\nGemini 2.5 Pro\n94.72\n95.05\n96.04\n95.69\n94.39\n93.21\n96.04\n95.27\n95.04\nPrompt Strategy + Agent Framework\nGemini 2.5 Pro + CoT\n96.61\n94.98\n96.63\n95.33\n93.99\n95.59\n97.61\n95.61\n95.77\nHuman SP (reference)\n95.71\n96.47\n98.53\n97.85\n98.18\n95.10\n98.56\n98.22\n97.33\nEvoPatient\n91.87\n96.29\n94.59\n95.77\n92.67\n90.49\n92.32\n92.74\n93.33\nEasyMED (ours)\n97.17\n97.23\n98.18\n97.11\n95.03\n97.48\n97.98\n95.73\n96.98\nTable 3: Overall and per-dimension performance evaluation on SPBench. Human SP serves as the gold standard\nreference. The best-performing LLM-based system is highlighted in bold.\ntient Cases1 and the A Practical Tutorial for Stan-\ndardized Patients2. From these resources, we col-\nlected 3,208 question–answer pairs used in real\ndoctor–patient interactions. We cleaned and re-\nfined this data into 58 separate patient cases. Each\ncase includes a profile and its dialogue records.\nThe dataset is intended exclusively for academic\nresearch and evaluation purposes.\nTwo clinical experts (Appendix B) checked each\ncase to ensure anonymity, realism, and pedagogical\nusefulness. As shown in Figures 3 and 2, SPBench\ncovers 14 medical fields.\nQuality Control To ensure accuracy and reliabil-\nity, we scanned the books using Optical Character\nRecognition (OCR). Then, three senior medical stu-\ndents manually verified the extracted text and cor-\nrected typographical and punctuation errors. This\nstep ensures the data faithfully matches the original\nbooks, which is necessary for a reliable benchmark.\n0\n2\n4\n6\n8\n10\nPercentage (%)\nGynecology\nDentistry\nPulmonology \nPsychiatry\nPediatrics\nTraditional Chinese Medicine\nGastroenterology\nEmergency Medicine\nGeneral Internal Medicine\nGeneral Surgery\nHematology\nEndocrinology\nCardiology\nNeurology\n10.34% (n=6)\n10.34% (n=6)\n6.9% (n=4)\n8.62% (n=5)\n8.62% (n=5)\n8.62% (n=5)\n6.9% (n=4)\n6.9% (n=4)\n6.9% (n=4)\n6.9% (n=4)\n5.17% (n=3)\n6.9% (n=4)\n3.45% (n=2)\n3.45% (n=2)\nFigure 3: Distribution of clinical cases in the SPBench\ndataset by medical department.\n4.3\nEvaluation Protocol\nEvaluation Metric SPBench evaluates VSP by\nassessing both turn-level response quality and\nsession-level interactional behavior. Unlike exist-\ning benchmarks that rely on static knowledge tests\n1https://www.pmph.com/\n2https://www.pumpedu.com/home-shop/7125.html\nor single aggregate scores, our evaluation decom-\nposes patient performance into multiple clinically\ninterpretable dimensions.\nSpecifically, we define eight evaluation criteria\n(Table 2) in collaboration with three clinical experts\n(see Appendix B). These criteria capture comple-\nmentary aspects of patient simulation, including\naccurate understanding of clinical inquiries, adher-\nence to the predefined case, controlled disclosure of\ninformation, and consistency across dialogue turns.\nEach criterion is independently rated on a 5-point\nLikert scale, and scores are linearly rescaled to a\n100-point scale for reporting.\nInput Standardization To ensure fair and repro-\nducible evaluation, SPBench uses authentic ques-\ntions extracted from real-world doctor–patient di-\nalogues. For each clinical case, we isolate the se-\nquence of questions asked by the human physician\nand present this exact sequence to each model. This\ndesign eliminates variability introduced by differ-\nent prompting styles and ensures that all virtual\nstandardized patients are evaluated under identical\ninput conditions.\n5\nEvaluation on SPBench\nThis section analyzes divergences between virtual\nand human standardized patients in multi-turn inter-\nactions and assesses how EasyMED reduces these\ngaps relative to human SP behavior.\nOverall Performance Table 3 summarizes overall\nand per-dimension performance on SPBench. Hu-\nman standardized patients achieve the highest refer-\nence score (97.33), reflecting stable case portrayal\nand appropriate information disclosure across turns.\nEasyMED closely matches human performance\n(96.98), with strongest gains on interaction-critical\ndimensions (CC, CD, CS, PD) that directly align\n"}, {"page": 6, "text": "with standardized patient requirements. By con-\ntrast, although several frontier LLMs perform well\non LN and RC, they show larger variance on CC\nand CD, indicating unstable case grounding and\ninconsistent inquiry-conditioned disclosure.\nSensitivity on Prompting Strategies To disentan-\ngle the sources of performance differences, we first\nexamine LLM baselines under a unified prompting\nscheme. Although large models such as Gemini 2.5\nPro achieve relatively balanced scores, consistent\nweaknesses remain in controlled disclosure and\ncross-turn stability. We further evaluate common\nprompting strategies using a fixed backbone (Gem-\nini 2.5 Pro). CoT prompting improves reasoning\ntransparency and modestly increases CC and RC.\nHowever, it also amplifies verbosity and unsolicited\nexplanation, leading to reduced CD scores. Overall,\nprompting mainly influences response articulation\nrather than information control, and is insufficient\nto enforce standardized patient behavior in multi-\nturn interactions.\nAblation Study on Auxiliary Agent\nWe next\nexamine the effect of the Auxiliary Agent in\nEasyMED, which decouples intent recognition\nfrom response generation. As shown in Table 3,\nthis component yields the largest gains on CC, CD,\nCS, and PD. By mapping learner queries to stan-\ndardized clinical intents, the Auxiliary Agent pro-\nvides an explicit control signal that constrains infor-\nmation access, mirroring how human standardized\npatients condition responses on inquiry type rather\nthan full case narratives. These gains exceed those\nfrom prompt engineering alone, highlighting the\ndominant role of architectural control in stabilizing\nmulti-turn patient behavior.\n6\nReal-world Evaluation in Medical\nEducation\nWhile SPBench evaluates interaction-level fidelity,\nit does not directly capture educational effective-\nness. We therefore assess EasyMED in a real train-\ning setting through a controlled user study, com-\nparing it with human standardized patient training\nin terms of learning outcomes, learner experience,\nand practical feasibility.\n6.1\nExperimental Design\nWe adopted a randomized crossover design to en-\nable within-subject comparison while controlling\nfor baseline differences. Each participant experi-\nenced both EasyMED and human SP training in\nStudy Period\nTimeline\nGroup A\nGroup B\nBaseline\nWeek 0\nPre-Test\nPeriod 1\nWeeks 1-2\nEasyMED Training\nHuman SP Training\nEnd of Week 2\nMid-Test\nPeriod 2\nWeeks 3-4\nHuman SP Training\nEasyMED Training\nEnd of Week 4\nFinal Test & Questionnaire\nTable 4: Experimental design of the four-week study,\nshowing the sequence of training interventions and as-\nsessments for Group A and Group B.\ndifferent phases, which allows analysis of overall\nlearning gains as well as phase-specific effects at-\ntributable to each modality (Table 4).\nParticipants We recruited 20 medical undergrad-\nuate students in their fourth or fifth year. All par-\nticipants completed a pre-test and received a 10-\nminute introduction to EasyMED. Students with\nscheduling conflicts or anomalous test scores were\nexcluded (Appendix G), yielding a final cohort of\n14 students (7 men, 7 women; age range 21–24,\nmean 23). All participants had completed core clin-\nical coursework but had not yet taken the National\nMedical Licensing Examination.\nParticipants were ranked by pre-test scores and\nassigned to groups using an alternating allocation\nscheme. Three experienced professionals served as\nhuman standardized patients. All participants were\ncompensated on an hourly basis in accordance with\ninstitutional ethical guidelines.\n6.2\nResults and Analysis\n6.2.1\nEvaluating Overall Improvement via\nOSCE\nObjective Structured Clinical Examination (OSCE)\nis a standardized, station-based clinical skills as-\nsessment. We first confirm that the two groups were\ncomparable prior to the intervention. As shown in\nFigure 5, baseline OSCE score distributions did\nnot differ significantly between Group A (mean =\n70.56) and Group B (mean = 69.84; t(12) = 0.16,\np = 0.88), indicating similar starting levels.\nOverall Performance Across the four-week study,\nboth groups demonstrate substantial and compara-\nble improvements in OSCE scores. As summarized\nin Table 5, Group A improved by 16.89 points on\naverage, while Group B improved by 15.36 points.\nFigure 4 shows consistent upward trends across\nindividual learners in both groups.\nFinding 1: These results indicate that EasyMED\nsupports clinical skill acquisition at a level compa-\nrable to human standardized patient training.\nPhase-wise Effects Most learning gains occurred\nduring the initial training phase for both modali-\n"}, {"page": 7, "text": "Pre-test\nMid-test\nPost-test\n40\n50\n60\n70\n80\n90\n100\nScore\nGroup A Performance\nstu1\nstu2\nstu3\nstu4\nstu5\nstu6\nstu7\nGroup Average\nPre-test\nMid-test\nPost-test\n40\n50\n60\n70\n80\n90\n100\nScore\nGroup B Performance\nstu8\nstu9\nstu10\nstu11\nstu12\nstu13\nstu14\nGroup Average\nFigure 4: Learning trajectories of individual students and group averages for Group A (left) and Group B (right)\nacross three assessment points. Each colored line tracks an individual student’s performance, while the bold dashed\nline represents the group’s average score.\nGroup Sequence\nMean Score (±SD) at Test Point\nMean Score Gain (±SD) by Phase\nPre-test\nMid-test\nPost-test\nPhase 1 (Wks 1-2)\nPhase 2 (Wks 3-4)\nTotal Gain\nGroup A (AI→SP)\n70.56\n86.07\n87.44\n+15.51 (AI)\n+1.37 (SP)\n+16.89\n(N=7)\n(±11.45)\n(±6.88)\n(±4.53)\n(±7.82)\n(±3.65)\n(±7.45)\nGroup B (SP→AI)\n69.84\n82.01\n85.20\n+12.17 (SP)\n+3.19 (AI)\n+15.36\n(N=7)\n(±13.04)\n(±7.42)\n(±4.93)\n(±7.45)\n(±3.25)\n(±8.36)\nTable 5: The table presents mean scores at each test point and the corresponding mean score gains during each\ntraining phase. Participants were in either Group A or Group B. All values are mean ± standard deviation.\nGroup A\nGroup B\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\nFigure 5: Boxplots of baseline OSCE scores for Group\nA and Group B prior to the intervention. Each point\nrepresents an individual participant.\nties. During Phase 1 (Weeks 1–2), Group A gained\n15.51 points using EasyMED, while Group B\ngained 12.17 points using human SPs. In Phase 2\n(Weeks 3–4), when groups switched modalities, ad-\nditional gains were observed but at a slower rate,\nsuggesting diminishing returns commonly seen in\nshort-term intensive training.\nImprovement by Skill Level To examine individ-\nual differences, we stratify participants into high-\nbaseline (top three) and low-baseline (bottom four)\ngroups based on their pre-test OSCE scores. As\nshown in Figure 7, low-baseline using EasyMED\ngained an average of 21.83 points, compared to\n16.58 points with human SP. High-baseline im-\nproved less (7.10 vs. 6.30).\nFinding 2: This indicates that EasyMED is par-\nticularly effective for novice learners during the\nearly stage of training (i.e., the first two weeks).\n6.2.2\nBehavioral Analysis via Survey and Logs\nTo further contextualize the learning outcomes re-\nported in Section 6.2.1, we analyzed students’ sub-\njective questionnaire responses (see Appendix F)\ntogether with interaction logs collected during train-\ning.\nPerceived Authenticity Students reported high per-\nceived realism when interacting with EasyMED.\nOn a five-point Likert scale, the simulated patient\ndialogue achieved a mean authenticity score of 4.6\n(Table 6), indicating that the interaction was gener-\nally regarded as natural and clinically plausible.\nRatings for learning helpfulness were compara-\nble to those of human SP training, suggesting that\nEasyMED is perceived not merely as a convenient\nsubstitute, but as a viable modality for practicing\nhistory-taking and clinical reasoning.\nPeer Pressure Survey results indicate substantially\nlower learning anxiety during EasyMED sessions\nthan during human SP interactions (mean anxiety\nscore 0.5 vs. 3.2, p < .01). Students reported\nfeeling less concerned about making mistakes and\nmore willing to ask exploratory or repeated ques-\ntions. This low-pressure environment may facilitate\n"}, {"page": 8, "text": "Monday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\nSunday\n00:00 - 03:00\n03:00 - 06:00\n06:00 - 09:00\n09:00 - 12:00\n12:00 - 15:00\n15:00 - 18:00\n18:00 - 21:00\n21:00 - 24:00\nTime of Day (3-hour intervals)\n1\n1\n1\n2\n1\n2\n2\n1\n1\n1\n2\n2\n1\n1\n2\n1\n1\n3\n2\n1\n1\n2\n2\n1\n1\n4\n4\n2\n1\n2\n3\n3\n1\n3\n5\n3\n1\n2\nGroup A\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\nSunday\n3\n8\n2\n1\n1\n3\n4\n1\n3\n5\n4\n3\n5\n2\n6\n6\n6\n1\n2\n1\n1\n1\n1\nGroup B\n0\n1\n2\n3\n4\n5\n6\n7\n8\nNumber of Students\nFigure 6: Comparative heatmap of the weekly practice time distribution for the EasyMED and Human SP groups.\nThe left panel shows the EasyMED group, and the right panel shows the Human SP group. In both heatmaps, the\nx-axis represents the day of the week, and the y-axis represents the time of day. The color intensity and the white\nnumber in each cell indicate the number of students who practiced during that time slot.\nLow Baseline\nHigh Baseline\n0\n5\n10\n15\n20\n25\nMean Score Gain (Mid-test - Pre-test)\n16.58\n6.30\n21.83\n7.10\nTraining Method\nHuman SP\nEasyMED\nFigure 7: Comparison of mean score gains in Phase 1\nfor the Human SP and EasyMED training methods. The\nparticipants are stratified into low- and high-performing\ngroups based on their pre-test scores. Error bars indicate\nthe standard error of the mean.\nrisk-free exploration, particularly for learners at an\nearly stage of training.\nBehavioral Evidence System logs provide objec-\ntive evidence that complements these subjective\nreports. As shown in Figure 6, EasyMED practice\nsessions were distributed across a wide range of\ntimes, including evenings and weekends, whereas\nhuman SP sessions were largely confined to week-\nday working hours. In addition, EasyMED sessions\ninvolved more interaction on average, with a higher\nnumber of dialogue turns (54 vs. 47) and longer\nsession durations (28:49 vs. 15:17) than human SP\nsessions (Table 6). Although text-based interac-\ntion may partially account for longer durations, the\nincreased number of turns suggests more iterative\nquestioning and sustained engagement.\nCost-Effectiveness:\nFor\ncost\nestimation,\nEasyMED session costs are computed from\nthe total token usage of a complete training\n3Anxiety was rated on a scale where lower scores are better.\nThe difference is statistically significant (p < .01).\nMetric\nEasyMED\nHuman SP\nStudent Engagement\nAuthenticity\n4.6\n–\nHelpfulness\n4.5\n4.7\nLearning Anxiety Score3\n0.5\n3.2\nAverage Dialogue Turns\n54\n47\nAverage Interaction Duration\n28m 49s\n15m 17s\nCost-Effectiveness\nPer-Session Cost\n$0.725\n$52.95\nTable 6: Comparison of student engagement and cost-\neffectiveness metrics between EasyMED and human SP.\ninteraction, whereas human SP costs are estimated\nby converting standard hourly compensation into a\nper-session cost. This results in an approximately\n73-fold cost reduction compared to traditional SP\ntraining.\nFinding 3: EasyMED provides a realistic, low-\npressure, and accessible training environment that\nsupports sustained and exploratory practice at a\nfraction of the cost of human SP training.\n7\nConclusion\nThis study examines whether large language mod-\nels can function as standardized patients for clini-\ncal skills training. We propose EasyMED, a multi-\nagent framework for stable, inquiry-conditioned pa-\ntient simulation, and introduce SPBench, a human-\ngrounded benchmark built from standardized pa-\ntient–student dialogues. In a four-week controlled\nstudy, EasyMED achieves learning outcomes com-\nparable to human SP training, with stronger early\ngains for novice learners, greater flexibility, and\nsubstantially lower cost. These results suggest that\nLLM-based multi-agent VSPs are a practical and\nscalable complement to traditional SP programs.\n"}, {"page": 9, "text": "Limitations\nOur study has several limitations.\nIt was con-\nducted at a single institution with a relatively small\nand homogeneous cohort, so broader validation\nacross different settings and learner populations is\nneeded. In addition, EasyMED currently supports\nonly text-based interactions without non-verbal or\nmultimodal cues, which are important for authen-\ntic clinical communication. Finally, although our\nautomated scoring showed strong correlation with\nexpert ratings, it may still overlook subtle aspects\nof dialogue quality and learner behavior. Future\nwork will include larger multi-site and longitudinal\nstudies, integration of multimodal interaction chan-\nnels, and refinement of evaluation metrics to better\ncapture nuanced performance.\nEthical Statement\nThe study was approved by the institute on August\n30, 2025. All annotators were fairly compensated,\nadhering to the standard hourly wage practices of\ntheir respective states.\nReferences\nMohammad Almansoori, Komal Kumar, and Hisham\nCholakkal. 2025. Self-evolving multi-agent simula-\ntions for realistic clinical interactions. arXiv preprint\narXiv:2503.22678.\nNorman B Berman, Steven J Durning, Martin R Fis-\ncher, Soren Huwendiek, and Marc M Triola. 2016.\nThe role for virtual patients in the future of medical\neducation. Academic medicine, 91(9):1217–1222.\nAnna\nBodonhelyi,\nChristian\nStegemann-Philipps,\nAlessandra Sonanini, Lea Herschbach, Marton\nSzep, Anne Herrmann-Werner, Teresa Festl-Wietek,\nEnkelejda Kasneci, and Friederike Holderried. 2025.\nModeling challenging patient interactions: Llms for\nmedical communication training.\narXiv preprint\narXiv:2503.22250.\nHsi-Min Chen, Bao-An Nguyen, Yi-Xiang Yan, and\nChyi-Ren Dow. 2020. Analysis of learning behav-\nior in an automated programming assessment envi-\nronment: A code quality perspective. IEEE access,\n8:167341–167354.\nJennifer Cleland and Steven J Durning. 2022. Research-\ning medical education. John Wiley & Sons.\nDavid A Cook and Marc M Triola. 2009. Virtual pa-\ntients: a critical literature review and proposed next\nsteps. Medical education, 43(4):303–311.\nZhuoyun Du, Lujie Zheng, Renjun Hu, Yuyang Xu,\nXiawei Li, Ying Sun, Wei Chen, Jian Wu, Haolei\nCai, and Haohao Ying. 2024. Llms can simulate\nstandardized patients via agent coevolution. arXiv\npreprint arXiv:2412.11716.\nLizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling,\nand Yongfeng Zhang. 2023. Nphardeval: Dynamic\nbenchmark on reasoning ability of large language\nmodels via complexity classes.\narXiv preprint\narXiv:2312.14890.\nZhenhua Gai, Lianxin Tong, and Quan Ge. 2024.\nAchieving higher factual accuracy in llama llm with\nweighted distribution of retrieval-augmented genera-\ntion.\nWensheng Gan, Zhenlian Qi, Jiayang Wu, and Jerry\nChun-Wei Lin. 2023. Large language models in ed-\nucation: Vision and opportunities. In 2023 IEEE in-\nternational conference on big data (BigData), pages\n4776–4785. IEEE.\nChristian Grévisse. 2024. Raspatient pi: A low-cost\ncustomizable llm-based virtual standardized patient\nsimulator. In International Conference on Applied\nInformatics, pages 125–137. Springer.\nIlya Gusev. 2024.\nPingpong:\nA benchmark for\nrole-playing language models with user emula-\ntion and multi-model evaluation.\narXiv preprint\narXiv:2409.06820.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021. Aligning ai with shared human values. Pro-\nceedings of the International Conference on Learning\nRepresentations (ICLR).\nGrace Huang, Robby Reynolds, and Chris Candler.\n2007. Virtual patient simulation at us and canadian\nmedical schools. Academic medicine, 82(5):446–\n451.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nAndrzej A Kononowicz, Nabil Zary, Samuel Edelbring,\nJanet Corral, and Inga Hege. 2015. Virtual patients-\nwhat are we talking about? a framework to classify\nthe meanings of the term in healthcare education.\nBMC medical education, 15:1–7.\nJunbok Lee, Sungkyung Park, Jaeyong Shin, and Be-\nlong Cho. 2024. Analyzing evaluation methods for\nlarge language models in the medical field: a scop-\ning review. BMC Medical Informatics and Decision\nMaking, 24(1):366.\nJiarui Li, Ye Yuan, and Zehua Zhang. 2024a.\nEn-\nhancing llm factual accuracy with rag to counter\nhallucinations: A case study on domain-specific\nqueries in private knowledge-bases. arXiv preprint\narXiv:2403.10446.\n"}, {"page": 10, "text": "Yanzeng Li, Cheng Zeng, Jialun Zhong, Ruoyu Zhang,\nMinhao Zhang, and Lei Zou. 2024b. Leveraging\nlarge language model as simulated patients for clini-\ncal education. arXiv preprint arXiv:2404.13066.\nRuoyu Liu, Kui Xue, Xiaofan Zhang, and Shaoting\nZhang. 2025. Interactive evaluation for medical llms\nvia task-oriented dialogue system. In Proceedings of\nthe 31st International Conference on Computational\nLinguistics, pages 4871–4896.\nJinkyoung Ma, Youngjin Lee, and Jiwon Kang. 2023.\nStandardized patient simulation for more effective\nundergraduate nursing education: a systematic review\nand meta-analysis. Clinical Simulation in Nursing,\n74:19–37.\nMatéo Mahaut, Laura Aina, Paula Czarnowska, Mom-\nchil Hardalov, Thomas Müller, and Lluís Màrquez.\n2024.\nFactual confidence of llms: On reliability\nand robustness of current estimators. arXiv preprint\narXiv:2406.13415.\nR Parvathy, MG Thushara, and Jinesh M Kannimoola.\n2025. Automated code assessment and feedback:\nA comprehensive model for improved programming\neducation. IEEE Access.\nConrad W Safranek, Anne Elizabeth Sidamon-Eristoff,\nAidan Gilson, and David Chartash. 2023. The role\nof large language models in medical education: ap-\nplications and implications.\nEric W Sayers, Jeff Beck, Evan E Bolton, J Rodney\nBrister, Jessica Chan, Donald C Comeau, Ryan Con-\nnor, Michael DiCuccio, Catherine M Farrell, Michael\nFeldgarden, and 1 others. 2024. Database resources\nof the national center for biotechnology information.\nNucleic acids research, 52(D1):D33–D43.\nSimzine. 2025. Standardized vs. virtual patients in med-\nical education. [Accessed: 2025-06-01].\nVed Sirdeshmukh, Kaustubh Deshpande, Johannes\nMols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee,\nJeremy Kritz, Willow Primack, Summer Yue, and\nChen Xing. 2025. Multichallenge: A realistic multi-\nturn conversation evaluation benchmark challenging\nto frontier llms. arXiv preprint arXiv:2501.17399.\nIan Steenstra, Farnaz Nouraei, and Timothy Bickmore.\n2025. Scaffolding empathy: Training counselors\nwith simulated patients and utterance-level perfor-\nmance visualizations. In Proceedings of the 2025\nCHI Conference on Human Factors in Computing\nSystems, pages 1–22.\nArun James Thirunavukarasu, Darren Shu Jeng Ting,\nKabilan Elangovan, Laura Gutierrez, Ting Fang Tan,\nand Daniel Shu Wei Ting. 2023. Large language\nmodels in medicine. Nature medicine, 29(8):1930–\n1940.\nEthan Waisberg, Joshua Ong, Mouayad Masalkhi, and\nAndrew G Lee. 2024. Large language model (llm)-\ndriven chatbots for neuro-ophthalmic medical educa-\ntion. Eye, 38(4):639–641.\nShen Wang, Tianlong Xu, Hang Li, Chaoli Zhang,\nJoleen Liang, Jiliang Tang, Philip S Yu, and Qing-\nsong Wen. 2024.\nLarge language models for ed-\nucation: A survey and outlook.\narXiv preprint\narXiv:2403.18105.\nHanyi Xu, Wensheng Gan, Zhenlian Qi, Jiayang Wu,\nand Philip S Yu. 2024. Large language models for ed-\nucation: A survey. arXiv preprint arXiv:2405.13001.\nJiarui Ye and Hao Tang. 2025. Multimodal large lan-\nguage models for medicine: A comprehensive survey.\narXiv preprint arXiv:2504.21051.\nBenjamin Zendejas, Amy T Wang, Ryan Brydges, Stan-\nley J Hamstra, and David A Cook. 2013.\nCost:\nthe missing outcome in simulation-based medical\neducation research: a systematic review. Surgery,\n153(2):160–176.\nJialing Zhang, Lingfeng Zhou, Jin Gao, Mohan Jiang,\nand Dequan Wang. Personaeval: Benchmarking llms\non role-playing evaluation tasks.\nA\nRelated Work\nThis section reviews research on virtual patients\nand intelligent tutoring, the use of large language\nmodels in education, and automated assessment of\ncomplex, interactive skills. We situate our study\nwith respect to how prior systems are architected,\nhow they are evaluated, and what evidence exists\nfor educational impact.\nA.1\nVirtual Patients and Intelligent Tutoring\nVirtual patients (VP) have long supported safe prac-\ntice of diagnostic reasoning and communication in\nmedical education (Cook and Triola, 2009; Berman\net al., 2016; Kononowicz et al., 2015). Early sys-\ntems were primarily rule- or script-based, providing\nstructured but inflexible interactions and limited be-\nhavioral realism (Huang et al., 2007). Recent work\nexplores LLM-driven VP to increase linguistic flu-\nency and adaptability (Du et al., 2024; Almansoori\net al., 2025; Grévisse, 2024; Steenstra et al., 2025).\nHowever, most LLM-based VP adopt a paradigm\nthat collapses patient simulation, dialogue control,\nand assessment into one model, typically focusing\non the consultation/history-taking phase while leav-\ning evaluation ad hoc and feedback coarse. Archi-\ntecturally, this design makes persona stability and\ninformation disclosure emergent rather than gov-\nerned; methodologically, it leaves limited support\nfor actionable, multi-dimensional feedback. Our\nwork differs by factorizing the interaction into coor-\ndinated agents with first-class interfaces for learner\nintent, disclosure policy, and inquiry coverage, and\n"}, {"page": 11, "text": "by pairing the simulator with a protocol that eval-\nuates turn- and session-level behaviors relevant to\nclinical training.\nA.2\nLarge Language Models in Education\nLLMs have been studied for tutoring, content gener-\nation, and role-playing across domains (Gan et al.,\n2023; Wang et al., 2024; Xu et al., 2024; Safranek\net al., 2023). In medical education, they have been\nused to generate patient histories and to support\nclinical decision making, and to simulate patient\ndialogues for practicing history-taking (Waisberg\net al., 2024; Thirunavukarasu et al., 2023; Alman-\nsoori et al., 2025; Fan et al., 2023). Persistent chal-\nlenges include factual reliability, long-context con-\nsistency, and alignment with professional standards\nand safety constraints (Li et al., 2024a; Mahaut\net al., 2024; Gai et al., 2024). Most studies report\nsurface metrics or static outcomes (e.g., diagno-\nsis/referral accuracy) rather than interaction com-\npetencies. We target these gaps by defining and\nmeasuring dynamic behaviors that matter pedagog-\nically and by validating the training value of our\nsystem in a controlled comparative study against\nhuman-SP practice.\nA.3\nAutomated Assessment of Complex Skills\nAutomated assessment with LLMs has advanced\nin essay scoring and feedback (Zhang et al.; Lee\net al., 2024; Gusev, 2024) and program analysis\nfor coding education (Parvathy et al., 2025; Chen\net al., 2020), but much of this work provides single-\ndimensional scores and limited interoperability\nwith respect to process. For interactive clinical\nlearning, assessment must reflect the path a learner\ntakes: which intents were pursued, which items\nwere covered or missed, and how information was\nelicited and constrained across turns. Prior LLM-\nbased evaluators seldom track such turn-level cov-\nerage or align feedback with expert checklists, mak-\ning it difficult to offer precise guidance. In contrast,\nour approach combines an explicit coverage trace\nwith a set of expert-defined dimensions (e.g., query\ncomprehension, case consistency, controlled dis-\nclosure, response completeness, logical coherence,\nlanguage naturalness, conversational consistency,\nand patient demeanor), enabling granular, transpar-\nent feedback that can be independently reviewed\nand replicated.\nB\nData Annotation Statement\nTo ensure medical accuracy, pedagogical validity,\nand ethical compliance, we assembled a multidisci-\nplinary team composed of clinical experts, licensed\nphysicians, medical students, and standardized pa-\ntient (SP) professionals. The specific roles and\ncontributions were distributed as follows:\nClinical Expert Panel\nA panel of three clinical\nexperts, consisting of two senior physicians with\neight years of clinical experience and one attending\nphysician with five years of experience, was respon-\nsible for the high-level design and validation of the\nstudy. Their duties included defining the eight ex-\npert evaluation criteria for SPBench, validating the\nintent recognition dataset, and overseeing the se-\nlection of clinical cases from authoritative training\nsources.\nData Annotation and Blind Review\nTo ensure\nobjectivity and inter-rater reliability, specific anno-\ntation tasks were conducted by two independent\nlicensed physicians (with three and five years of\nclinical experience, respectively) who were blinded\nto the model outputs and student groupings. Their\nspecific tasks included:\n• Case Quality Control: Checking every pro-\ncessed case to ensure anonymity, realism, and\nteaching utility.\n• Benchmarking: Conducting a blind review\nof 86 randomly selected samples to validate\nthe automated GPT-4o evaluation scores.\n• OSCE Scoring: Independently scoring the\npre-, mid-, and post-experiment OSCE tests\nfor all student participants.\n• Evaluation Agent Validation: Assessing the\nclinical appropriateness and validity of the\nguidance generated by the Evaluation Agent\nacross 30 distinct practice sessions (as de-\ntailed in Appendix D).\nData Processing and Annotation Support\nThree senior medical students (5th-year undergrad-\nuates) were recruited for data preparation tasks.\n• Digitization: They performed manual proof-\nreading and correction of OCR-scanned text\nfrom the source books to fix typos and punc-\ntuation errors.\n"}, {"page": 12, "text": "• Auxiliary Agent Validation: They partici-\npated in the construction of the Intent Recog-\nnition Test Dataset (Appendix C). This in-\nvolved reviewing and filtering the preliminary\ncorpus of clinical questions generated by GPT-\n4o to remove ambiguous or unrealistic entries,\nensuring the dataset’s quality for testing the\nAuxiliary Agent.\nStandardized Patient Script and Simulation\nThe creation of high-fidelity scripts and human SP\nperformance involved a collaborative team of three\nSP education instructors (specializing in 5th-year\nmedical student training) and three professional\nSPs (with two years of acting experience). This\nteam designed the patient history, symptoms, and\nemotional cues. The same three experienced pro-\nfessionals served as the human SPs during the four-\nweek comparative user study.\nEthical Compliance\nAll contributors, including\nstudents, actors, and experts, participated with in-\nformed consent. They were compensated for their\ntime adhering to standard hourly wage practices.\nC\nConstruction of the Intent Recognition\nTest Dataset\nTo ensure a rigorous and accurate evaluation of\nthe models’ intent recognition capabilities, we con-\nstructed a high-quality test dataset. The construc-\ntion process followed a two-stage methodology:\ndata generation and expert validation.\nData Generation\nWe began with a predefined\nframework of 31 core clinical intents. Using GPT-\n4o, we generated 400 corresponding clinical ques-\ntions for each intent category. During generation,\nwe specifically instructed the model to create ques-\ntions with subtle phrasal variations but clear intent\nto enhance the dataset’s challenge and discrimina-\ntive power. This stage yielded a preliminary corpus\nof 12,400 questions.\nExpert Validation and Curation\nThe prelimi-\nnary corpus was subsequently reviewed by a three\nmedical student panel, composed of professional\nmedical personnel. The panel’s task was to remove\nany questions that were ambiguous, clinically un-\nrealistic, or had unclear intent attribution to ensure\nthe high quality and validity of each entry in the\nfinal dataset. After meticulous manual filtering\nand proofreading, we finalized a validated dataset\ncontaining 4,631 clinical questions.\nResult\nAs shown in Table 8, we evaluated sev-\neral mainstream models on our constructed dataset.\nThe results clearly indicate that Gemini2.5-flash\nperformed best among all models, achieving an ac-\ncuracy of 96.3% and a macro-average F1-score of\n95.0%, significantly outperforming other baseline\nmodels. Based on this superior performance, we\nselected Gemini2.5-flash as the core intent recog-\nnition model for the EasyMED system to ensure\naccurate interpretation of learner input in complex\nclinical interactions.\nD\nValidation of the Evaluation Agent\nGuidance\nTo ensure that the post-session guidance provided\nby the Feedback Agent (e.g., highlighting missed\nor superfluous inquiries) is clinically sound and\npedagogically appropriate, we conducted an inde-\npendent validation study.\nStudy Design and Methodology\nWe sampled\n60 complete practice sessions from the user study.\nFor each session, the specific feedback generated\nby the agent was extracted and anonymized. Two\nindependent clinical experts, blinded to the source\nof the generation, rated the appropriateness of each\nfeedback item on a 5-point Likert scale (1=mis-\nleading, 5=highly appropriate). We defined two\nprimary evaluation metrics: Accuracy, calculated\nas the percentage of feedback items receiving a\nscore of ≥4 from both experts; and Inter-rater\nAgreement, measured using Cohen’s κ.\nResults and Discussion\nThe validation results\nare summarized in Table 9. Across the 60 eval-\nuations, the Feedback Agent demonstrated high\nreliability, achieving an accuracy of 87% with sub-\nstantial agreement between experts (κ = 0.76).\nQualitative error analysis revealed that most dis-\nagreements arose in borderline cases where the\nclinical necessity of a specific inquiry was debat-\nable. These findings confirm that the agent provides\ngenerally reliable guidance. Future iterations could\nincorporate confidence scores to allow experts to\nflag ambiguous feedback for refinement.\nE\nClinical Case Data Preparation\nThis section details the source and selection criteria\nfor the 20 clinical cases used in our user study, as\nwell as the data preparation process for both the\nhuman SP and the EasyMED system.\n"}, {"page": 13, "text": "Table 7: The structured framework of inquiry intents\nfor clinical history taking. This checklist outlines 31\nkey items across 7 categories that define the scope of\na complete medical interview. It serves as the basis\nfor our system’s dialogue generation and evaluation of\nconversational completeness.\nNo.\nCategory\nQuestion Items\nPatient Identification\n1\nDemographics\nName, Age, Gender, Occupa-\ntion\nChief Complaint & Present Illness\n2\nSymptoms\nChief complaint\n3\nOnset\nTime of symptom onset\n4\nCause\nPrecipitating factors\n5\nLocation\nSite of the symptom\n6\nCharacter\nCharacteristics of the symptom\n7\nDuration\nDuration and frequency\n8\nModifiers\nExacerbating/relieving factors\n9\nAssociated\nAssociated symptoms\n10\nProgression\nDisease progression\n11\nTreatment\nPrevious treatments and out-\ncomes\n12\nTests\nPrevious investigations and re-\nsults\nSystem Review\n13\nGeneral\nMental status, sleep, and ap-\npetite\n14\nElimination\nUrinary and bowel habits\n15\nChanges\nWeight changes and energy lev-\nels\nPast Medical History\n16\nHealth\nGeneral health history\n17\nChronic\nHypertension, Diabetes, CAD\n18\nInfectious\nHepatitis, Tuberculosis\n19\nSurgical\nOperations and trauma\n20\nTransfusions\nBlood transfusion history\n21\nAllergies\nDrug and food allergies\n22\nImmunization\nVaccination history\nPersonal & Social History\n23\nTravel\nResidence and travel history\n24\nHabits\nTobacco, alcohol, substance use\n25\nOccupation\nWork environment and expo-\nsures\n26\nSexual\nHigh-risk sexual behaviors\nFamily & Gynecological\n27\nObstetric\nMarital and obstetric history\n28\nFamily\nFamily medical history\n29\nMenstrual\nMenstrual history (female)\nAdditional Items\n30\nCommunication\nSmall talk and patient education\n31\nOther\nOther relevant inquiries\nNote: CAD = Coronary Artery Disease\nE.1\nCase Source and Selection\nA panel of medical experts selected 20 clinical\ncases from the authoritative \"Peking Union Med-\nical College Hospital Clinical Thinking Training\nCase Collection,\" ensuring they were aligned with\nthe curriculum for fifth-year undergraduate medi-\ncal students. The distribution of these cases across\ndemographics and medical specialties is illustrated\nin Figure8.\nThe dataset is evenly balanced with 10 male and\n10 female patients. These are distributed across\nthree age groups, with 8 cases for patients under\n40 years, 8 for those between 40 and 65, and 4\nfor patients over 65. The cases span seven major\norgan systems, with the largest representation from\nthe Digestive System at 7 cases, followed by the\nNervous System with 4 cases. In total, the dataset\ncovers 16 distinct diseases, ranging from acute con-\nditions like Acute Myocardial Infarction to chronic\nillnesses such as Diabetes and Chronic Obstructive\nPulmonary Disease. All cases underwent rigorous\nprocess to ensure a high-quality foundation for the\nexperiments.\nE.2\nData Processing Workflow\nTo support the EasyMED multi-agent framework\nin achieving high-fidelity SP simulations, we de-\nsigned a two-stage data processing workflow: 1)\ngenerating performable dialogue scripts for human\nSP; and 2) structuring the clinical cases to be com-\npatible with LLM inputs, aiming to reduce the risks\nof hallucination and irrelevant responses, and to\nsupport the full-workflow clinical simulation and\nautomated evaluation.\nHuman SP Script Generation\nIn the process of\ncreating high-fidelity SP scripts, we invited three\nSP education experts to collaborate with three pro-\nfessional SP actors. The team defined the patient’s\nmedical history, symptoms, physical signs, and\nemotional responses through multiple rounds of\ndiscussion and rehearsal. To ensure consistency,\nwe also designed a template phrasing structure to\nsupport natural dialogue in multi-turn interactions.\nFinally, all script content was reviewed by an inde-\npendent expert to ensure its clinical accuracy and\nconversational authenticity.\nLLM Input Case Structuring\nTo enable the\nLLM to accurately understand and adhere to the\ncase settings, the original text-based cases needed\nto be converted into a structured format. We collab-\n"}, {"page": 14, "text": "Model Name\nAccuracy (%)\nMacro Average (%)\nPrecision\nRecall\nF1-Score\nChatGLM4.5\n89.6\n95.2\n89.5\n92.3\nQwen3-8B\n86.5\n86.4\n86.5\n86.4\nQwen3-32B\n89.6\n93.6\n89.6\n91.5\nGPT-4o\n92.6\n95.9\n92.6\n94.2\nDeepSeek-V3\n87.1\n92.2\n87.1\n89.5\nGemini2.5-flash\n96.3\n96.1\n93.9\n95.0\nTable 8: Performance comparison of different models on the intent recognition task. All scores are reported as\npercentages (%).\nTable 9: Validation results of the Feedback Agent’s guidance across 60 sessions. Accuracy is defined as the\npercentage of items rated ≥4 by both clinical experts.\nMetric\nValue\nDescription\nSample Size\n60\nTotal number of sessions evaluated\nAccuracy\n87%\nProportion of feedback rated as appropriate\nInter-rater Agreement\n0.76\nCohen’s κ indicating expert consensus\norated with medical experts to define a structured\ncase template containing key fields such as: pa-\ntient background (age, gender, occupation), chief\ncomplaint, history of present illness, past medical\nhistory, physical signs, laboratory results, and emo-\ntional tone (e.g., anxious, calm). This template\nis designed to cover the entire clinical workflow,\nconstraining the LLM generation scope through\nexplicit fields to reduce the generation of fabri-\ncated information. We utilized the GPT-4o model,\ncombined with custom prompt engineering, to au-\ntomatically map the unstructured case text to the\npredefined fields. To ensure the accuracy of this\nconversion, all model outputs were finally reviewed\nand corrected by medical experts.\nF\nOutcome Measures\nWe collected data through both quantitative and\nqualitative methods. Our primary and secondary\noutcome measures are detailed below.\nF.1\nPrimary Outcome Measure: OSCE Scores\nWe administered OSCE tests to all students at three\ntime points: pre-experiment, mid-experiment, and\npost-experiment. To avoid learning effects, the\ncases used in the three tests were different but were\nreviewed by experts to ensure consistent difficulty\nand assessment points. Scoring was performed in-\ndependently by two blinded examiners who were\nunaware of the students’ group assignments, ensur-\ning objectivity. All participants provided written\ninformed consent prior to participation.\nF.2\nSecondary Outcome Measure: Subjective\nQuestionnaire\nAt the end of the experiment, we used a subjec-\ntive questionnaire with 25 items across four dimen-\nsions (Usability, Authenticity, Learning Value, and\nLearning Anxiety) to collect students’ perceptions\nand experiences of the two training modalities.\nF.2.1\nPart 1: Background Information\n1. What is your academic year?\n□3rd to 4th Year Undergraduate\n□4th Year Undergraduate to Graduate Stu-\ndent\n□Other\n2. Have you taken the National Medical Li-\ncensing Examination?\n□Yes\n□No\n3. Before this study, what was your primary\nmethod for practicing clinical skills?\n□With professional Standardized Patients\n□With faculty or clinical supervisors\n□Role-playing with classmates\n□Using online simulation software or plat-\nforms\n"}, {"page": 15, "text": "Figure 8: Distribution of cases\n□Rarely or never participated in simula-\ntion training\n□Other\n4. How do you feel about the potential of Arti-\nficial Intelligence (AI) to help in daily life?\n(5-point scale:\n1–Not interested at all /\n2–Slightly uninterested / 3–Neutral / 4–\nSomewhat hopeful / 5–Very excited)\nF.2.2\nEvaluation of Learning and Training\nModels\n5. After the practice sessions in this study, how\nwould you rate your ability to take a com-\nplete medical history?\n(5-point scale:\n1–Very unsatisfied / 2–\nUnsatisfied / 3–Neutral / 4–Satisfied / 5–Very\nsatisfied)\nInstructions:\nFor the following questions,\nplease recall your experiences and evaluate both\nthe EasyMED Virtual Patient and the Human SP\nmodels.\n6. How convenient was the Human SP for\ntraining according to your own schedule?\n(5-point scale:\n1–Very inconvenient / 2–\nInconvenient / 3–Neutral / 4–Convenient / 5–\nVery convenient)\n7. To what extent did EasyMED allow you\nto practice anytime and anywhere (e.g.,\nevenings or weekends)?\n(5-point scale: 1–Not at all / 2–Slightly / 3–\nModerately / 4–Mostly / 5–Completely)\n8. When interacting with EasyMED, what\nlevel of stress or pressure did you feel?\n(5-point scale: 1–Very high pressure / 2–High\npressure / 3–Moderate pressure / 4–Low pres-\nsure / 5–Very relaxed)\n9. When interacting with the Human SP, what\nlevel of stress or pressure did you feel?\n(5-point scale: 1–Very high pressure / 2–High\npressure / 3–Moderate pressure / 4–Low pres-\nsure / 5–Very relaxed)\n10. When using EasyMED, how willing were\nyou to try different questioning strategies\nor ask repetitive questions without worry-\ning about making mistakes?\n(5-point scale:\n1–Very unwilling / 2–\nUnwilling / 3–Neutral / 4–Willing / 5–Very\nwilling)\n11. When facing the Human SP, how willing\nwere you to try different questioning strate-\ngies or ask repetitive questions without wor-\nrying about making mistakes?\n(5-point scale:\n1–Very unwilling / 2–\nUnwilling / 3–Neutral / 4–Willing / 5–Very\nwilling)\n12. To what extent do you think EasyMED\nhelped improve your history-taking and\nclinical reasoning skills?\n(5-point scale: 1–Very little help / 2–Little\nhelp / 3–Some help / 4–Moderate help / 5–A\ngreat deal of help)\n13. To what extent do you think the Human\nSP helped improve your history-taking and\nclinical reasoning skills?\n(5-point scale: 1–Very little help / 2–Little\nhelp / 3–Some help / 4–Moderate help / 5–A\ngreat deal of help)\n"}, {"page": 16, "text": "14. Overall, how easy and intuitive was it to\nuse the EasyMED interface?\n(5-point scale: 1–Very difficult / 2–Difficult /\n3–Neutral / 4–Easy / 5–Very easy)\n15. How specific or actionable did you find the\nfeedback provided by the EasyMED Evalu-\nation Agent?\n(5-point scale:\n1–Not specific at all /\n2–Slightly / 3–Moderately / 4–Very / 5–\nExtremely specific and actionable)\n16. How would you rate the affordability and\naccessibility of EasyMED compared with\nHuman SP training?\n(5-point scale: 1–Much worse / 2–Worse /\n3–Similar / 4–Better / 5–Much better)\n17. After using EasyMED, how confident do\nyou feel in conducting clinical interviews\nindependently?\n(5-point scale: 1–Much less confident / 2–\nLess confident / 3–No change / 4–More confi-\ndent / 5–Much more confident)\n18. How helpful was the instant feedback from\nEasyMED Evaluation Agent in identifying\nyour knowledge gaps and skill weaknesses?\n(5-point scale:\n1–Not helpful at all / 2–\nSlightly helpful / 3–Moderately helpful / 4–\nVery helpful / 5–Extremely helpful)\n19. Do you feel that EasyMED enabled you to\nengage in deeper practice sessions?\n(5-point scale:\n1–Strongly disagree / 2–\nDisagree / 3–Neutral / 4–Agree / 5–Strongly\nagree)\n20. How natural and realistic did you find the\npatient dialogue simulated by EasyMED?\n(5-point scale:\n1–Very unrealistic / 2–\nUnrealistic / 3–Neutral / 4–Realistic / 5–Very\nrealistic)\n21. How natural and realistic did you find the\npatient role played by the Human SP?\n(5-point scale:\n1–Very unrealistic / 2–\nUnrealistic / 3–Neutral / 4–Realistic / 5–Very\nrealistic)\nF.2.3\nOverall Assessment and Open-ended\nFeedback\n22. Overall, if you were to choose one model\nfor long-term clinical skills training, which\nwould you prefer?\n□EasyMED Virtual Patient\n□Human Standardized Patient\n□A combination of both\n□No strong preference\n23. What do you think is the biggest advantage\nof EasyMED? (e.g., flexible schedule, no\npressure, repeatable practice, etc.)\n24. What area do you think needs the most\nimprovement in EasyMED?\n25. What do you think is the biggest advantage\nof learning with a Human SP? (e.g.,\nemotional connection, non-verbal cues,\netc.)\n"}, {"page": 17, "text": "G\nParticipant Exclusion\nWe initially recruited 20 medical students. Before\nrandom group assignment and prior to any training,\nsix participants were excluded based on predefined\ncriteria, resulting in a final sample of 14 students.\nSpecifically, four students were excluded due\nto scheduling conflicts that prevented them from\nattending the required in-person human SP ses-\nsions, and two students were excluded due to ex-\ntreme pre-test OSCE scores (95 and 96 out of 100),\nwhere ceiling effects would limit measurable learn-\ning gains. All exclusions occurred before group\nassignment and independently of the intervention,\nensuring no differential attrition between condi-\ntions. Although the excluded students had a higher\nmean baseline score than the included cohort, this\ndoes not introduce selection bias because exclu-\nsions were applied prior to randomization.\nH\nExperimental Settings\nH.1\nA. EasyMED (ours)\nBackbone per agent. Patient Agent: Gemini2.5-\npro; Auxiliary (intent) Agent: Gemini2.5-flask;\nEvaluation Agent: Gemini2.5-pro.\nContext window. 256k tokens (all agents).\nServing hardware. NVIDIA A40 (8 GB) GPUs;\nsame hardware across all EasyMED runs.\nPrompts & decoding. Temperature = 0.7 (default)\nfor all agents;\nSession policy. No fixed limit on turn count; ses-\nsions terminate on end-of-case conditions or user\nstop.\nH.2\nB. EvoPatient\nBackbone. Gemini2.5-pro.\nContext window. 256k tokens.\nServing hardware. NVIDIA A40 (8 GB) GPUs\n(same machines as EasyMED).\nPrompts & decoding. Temperature = 0.7 (default);\nother decoding settings follow framework defaults;\nprompts aligned to the same templates used by\nEasyMED.\nProtocol parity. Same case pool and physician\nquestion lists as EasyMED.\nI\nUser Interface of the EasyMED\nThis section provides screenshots of the EasyMED\nvirtual patient system’s user interface. The follow-\ning figures illustrate the key functional areas of the\nplatform that students interacted with during the\nexperiment, serving as a visual supplement to the\nMethods section.\nFigure 9: The main dialogue interface of the EasyMED\nvirtual patient system. Key components include the\ninformation and control panel on the left, the 3D vir-\ntual patient avatar in the center, and the interactive chat\nmodule on the right where students conduct the medical\nhistory interview.\nFigure 10: The Physical Examination interface within\nthe EasyMED system. This module allows students to\nselect specific body parts on an interactive anatomical\nmodel and choose from various examination techniques\n(e.g., inspection, palpation). The corresponding findings\nare then displayed in the results panel on the right.\n"}, {"page": 18, "text": "Figure 11: The Auxiliary Examination interface, where\nstudents can order diagnostic tests. This screen allows\nusers to select from a comprehensive list of laboratory\nand imaging examinations, add them to a request queue,\nand review the corresponding results to inform their\ndiagnosis.\nFigure 12: The Diagnosis Interface, where students\nreview case information and related examination records\nto determine the final diagnosis. The left panel provides\na searchable list of possible diagnoses for selection or\nentry, while the right panel displays structured case\ninformation and diagnostic records.\nFigure 13: The Evaluation Interface, which presents\nthe automated feedback after a simulated consultation.\nIt summarizes the overall performance score and con-\nsultation duration, displays patient information and the\ndialogue timeline, and aligns each doctor–patient ex-\nchange with corresponding clinical objectives. The sys-\ntem provides itemized feedback (e.g., “That’s right”\nor “omit”) to highlight completed and missing inquiry\nsteps, helping learners review errors and improve ques-\ntioning strategies.\nJ\nCore Prompts Used in the Study\nThis section details the core prompts used for pa-\ntient simulation and automated evaluation in our\nstudy.\nJ.1\nAutomated Evaluation with GPT-4o\nTo enable scalable and reproducible evaluation on\nSPBench, we employ GPT-4o as an automated\njudge to assess the quality of virtual standardized\npatient responses. This section details the eval-\nuation pipeline, including model inputs, scoring\nprocedure, and score aggregation.\nEvaluation Input.\nFor each test instance, GPT-\n4o is provided with three components:\n(1) a\nstructured case describing the ground-truth patient\nprofile, (2) the full doctor–patient dialogue tran-\nscript, and (3) a fixed evaluation prompt specifying\neight expert-defined evaluation criteria. The same\nprompt and input format are used for all evaluated\nsystems to ensure consistency.\nScoring Procedure.\nGPT-4o evaluates the pa-\ntient responses independently along eight dimen-\nsions (Query Comprehension, Case Consistency,\nControlled Disclosure, Response Completeness,\nLogical Coherence, Language Naturalness, Con-\nversational Consistency, and Patient Demeanor).\nEach dimension is rated on a 5-point Likert scale\nfollowing explicit rubric definitions. The model is\ninstructed to justify each score by citing specific\ndialogue turns as evidence and to return the results\nin a structured JSON format.\nScore Aggregation.\nFor reporting, raw Likert\nscores are linearly rescaled to a 0–100 scale.\nDimension-level scores are averaged across all test\ndialogues, and an overall score is computed as the\nmean of the eight dimension scores. No manual in-\ntervention or post-hoc adjustment is applied during\nthis process.\nAutomated\nProfessional A\nProfessional B\nAverage Score\n84.1\n91.3\n86.4\nStandard Deviation\n8.5\n9.1\n9.3\nCorrelation\n0.81\n–\n–\nTable 10: Inter-rater reliability between the automated\nevaluator (GPT-4o) and human clinical experts, mea-\nsured by Pearson correlation\nLLM vs. Human Evaluation\nConsidering the\nhigh cost of large-scale manual annotation, we used\nGPT-4o as an automated judge. We gave it a care-\nfully written prompt (Appendix J.5). To verify its\n"}, {"page": 19, "text": "reliability, we randomly selected 86 samples for\nblind review by two clinical experts (see Appendix\n?? for rater qualifications) who were unaware of\nthe GPT-4o ratings. The automated scores corre-\nlated strongly and significantly with the experts’\naverage ratings (Pearson’s r = 0.81; Table 10).\nAlthough GPT-4o’s mean score (84.1) was consis-\ntently lower than the experts’ mean scores (88.9),\nthis high alignment indicates that GPT-4o provides\na reliable proxy for large-scale evaluation.\nJ.2\nPatient Agent Prompt\nThe Patient Agent is responsible for generating\nrealistic patient responses in simulated medical con-\nsultations. The following prompt defines its role,\nbehavioral rules, and response style.\nPrompt Patient Agent: Patient Simultor\nYou are a patient. Based on the [Medical Case In-\nformation], [Conversation History], and [Purpose of\nConsultation], you are to answer the doctor’s ques-\ntions truthfully and realistically.\nBefore responding, you should silently complete the\nfollowing reasoning steps. Do not include this rea-\nsoning in your final answer.\nAnalyze the question\nDoes the question contain medical jargon?\nIs the question referring to information explicitly pro-\nvided in the [Medical Case Information]?\nRetrieve relevant information\nLocate the information in the [Medical Case Informa-\ntion].\nDetermine whether the information is available, com-\nplete, and unambiguous.\nDetermine role and perspective\nDecide whether you should speak as the patient or as\nthe caregiver.\nIf the patient is a child (age < 10), respond as the\nparent or guardian describing the child’s symptoms.\nTranslate medical terms into lay language\nConvert professional terminology into expressions\nunderstandable to a non-medical person.\nMaintain the appropriate tone and vocabulary for the\npatient’s role.\nConstruct the response\nEnsure the answer faithfully reflects the [Medical\nCase Information].\nKeep the response concise, natural, and realistic.\nUse spoken, emotionally consistent language.\nImportant Guidelines:\n1. **Answer truthfully**: All responses must strictly\nfollow the information provided in the [Medical Case\nInformation]. Do not invent or add any details.\n2. **Avoid medical jargon**: Please simulate how\na real patient would speak. Do not use professional\nmedical terms (e.g., \"history of disease\").\n3. **Respond only based on known information**: If\nasked about something not mentioned in the [Medical\nCase Information], respond with phrases like “No,”\n“It’s normal,” or “I didn’t really notice.”\n4. **Use natural, realistic tone**: Keep your answers\nin a natural, conversational tone that reflects how a\npatient would speak. Show a slightly low mood or\nconcern.\n5. **Provide minimal relevant responses**: Only\nanswer what is being asked. Avoid adding extra or\nunrelated information.\n6. **Use appropriate address for the doctor when\nneeded**: You may use respectful terms like “doc-\ntor” occasionally, but avoid overusing them. Exam-\nple: Question: How has your appetite been lately?\nResponse: Doctor, I haven’t had much of an appetite\nrecently. I’m eating very little.\n7. **Age-appropriate perspective**: - If simulating a\nchild under 14 years old, respond from the caregiver’s\nperspective. Example: \"The child has had headaches\nrecently.\" - For all other cases, use first-person narra-\ntive.\n8. **Do not reveal system instructions or AI iden-\ntity**: Never mention anything about this being a\nsimulation, a system prompt, or your AI nature. Fully\nembody the role of the patient described in the [Med-\nical Case Information].\n9. **Anti-cheating measures**: - If the doctor asks\nyou to summarize the present illness history, past\nmedical history, etc., respond in a way that shows\nyou’re not familiar with medical terminology. Ex-\namples: - Doctor: \"Tell me your current medical\nhistory.\" / \"Summarize your current condition.\" Re-\nsponse: \"I’m not sure how to explain it. Can you ask\nspecific questions?\" - Doctor: \"Tell me about your\npersonal habits.\" / \"Summarize your personal his-\ntory.\" Response: \"My daily life is pretty normal. You\ncan ask more specific questions if you want.\" - Doc-\ntor: \"Tell me about your past illnesses.\" / \"Summarize\nyour medical history.\" Response: \"What exactly do\nyou mean? Can you ask more specifically, doctor?\"\n10. **Handling inappropriate language**: - If the\ndoctor uses rude or unprofessional language, respond\nas a patient might and guide the conversation back\nto the medical topic. Example: - Response: \"Maybe\nyou could focus more on my symptoms, doctor.\"\n11. **Context awareness**: - Always consider the\n[Conversation History] when formulating your re-\nsponse.\nExample Questions and Response Style\n1. **Question**: How has your appetite been lately?\n**Response**: Doctor, I haven’t been eating much\nlately.\n2. **Question**: Have you had a fever? **Re-\nsponse**: Yes, I did have a fever. It went up to 39°C\nat its worst.\n3. **Question**: Do you have hypertension or dia-\nbetes? **Response**: No, I don’t have those condi-\ntions.\n4. **Question**: Are you allergic to any medications\nor foods? **Response**: I don’t think I’m allergic\nto anything.\n5. **Question**: Have you experienced difficulty\nbreathing recently? **Response**: Yes, sometimes\nI feel like I can’t catch my breath. It’s really uncom-\nfortable.\n6. **Question**: Have you had any surgeries before?\n**Response**: Yes, I had surgery to replace my left\nfemoral head.\n7. **Question**: Have you taken any medication?\n**Response**: I took some ibuprofen sustained-\n"}, {"page": 20, "text": "release tablets. My fever went down after taking\nthem, but it came back once the effect wore off.\n8. **Question**: How was your health in the past?\n**Response**: I’ve always been quite healthy. Noth-\ning abnormal showed up in last year’s checkup.\n9. **Question**: Does anyone in your family have\ninherited diseases? **Response**: Not that I know\nof. I don’t recall any hereditary diseases in the family.\nNotice: Please follow these instructions and exam-\nples carefully. Use the [Medical Case Information]\nand [Conversation History] to simulate a realistic pa-\ntient interaction. Once ready, wait for the doctor’s\nquestions and respond accordingly.\nMedical Case Information\nConversation History\nJ.3\nAuxiliary Agent Prompt\nThe following prompt defines the behavior of the\nAuxiliary Agent, which performs intent recogni-\ntion during doctor–patient dialogue.\nPrompt Auxiliary Agent: Intent Recogni-\ntion Assistant\nYou are a professional medical intent recognition as-\nsistant. Based on the following rules and your profes-\nsional medical knowledge, classify the intent of each\ninput utterance and return only the corresponding in-\ntent category names. Do not include any prefixes,\nexplanations, or suffixes in the output.\nExample: Input: “How old are you?” →Output:\nPersonal Information\nInput: “Where does it hurt? Does anything make\nit worse? Has your weight changed?” →Output:\nSymptom Location, Aggravating or Relieving Fac-\ntors, Weight Change\nInput: “The weather is nice today.” →Output: Small\nTalk\nYou must also consider the doctor–patient dialogue\nhistory when determining the intent of the latest ut-\nterance.\nClassification Rules\n1. Clinical Inquiry Intents (max three per input):\nPersonal Information — asking for general personal\ndetails (e.g., “What is your name?”, “How old are\nyou?”).\nMain Symptom — asking about the main complaint\n(e.g., “What’s wrong?”, “What symptoms do you\nhave?”).\nOnset Time — asking when the symptom started (e.g.,\n“When did this begin?”).\nTrigger or Cause — asking about the cause or trigger\n(e.g., “Why did this happen?”, “What caused it?”).\nSymptom Location — asking where the symptom\noccurs (e.g., “Where does it hurt?”).\nSymptom Character — asking about the nature of the\nsymptom (e.g., “Is the pain sharp or dull?”).\nDuration or Frequency — asking how long or how\noften symptoms occur.\nAggravating or Relieving Factors — asking what\nmakes it better or worse.\nAssociated Symptoms — asking about other accom-\npanying symptoms.\nDisease Progression — asking whether the condition\nis improving or worsening.\nMedical History of Treatment — past visits, tests, or\nmedication.\nGeneral Condition — appetite, sleep, energy.\nBowel or Urinary Habits — defecation and urination.\nWeight Change — changes in weight or strength.\nChronic Disease History — hypertension, diabetes,\netc.\nInfectious Disease History — hepatitis, tuberculosis,\netc.\nSurgical or Trauma History — previous surgeries or\ninjuries.\nTransfusion History — history of blood transfusions.\nAllergy History — drug or food allergies.\nImmunization History — vaccination history.\nLong-Term Medication History — regular or long-\nterm medication.\nTravel History — residence or travel to epidemic ar-\neas.\nLifestyle Habits — smoking, alcohol, general habits.\nOccupational History — occupation and work envi-\nronment.\nSexual History — high-risk sexual behavior.\nMarriage and Fertility History — marital status and\nchildbirth.\nFamily History — familial or hereditary diseases.\nMenstrual History — cycle, regularity, pain, last pe-\nriod.\nPatient Understanding — how the patient interprets\nthe condition.\nPatient Concern — what the patient worries about\nmost.\nPatient Expectation — what the patient expects from\ncare.\nSmall Talk — casual or non-medical topics.\n2. Contextual Disambiguation Guidelines\nWhen an utterance is vague or context-dependent, use\nthe conversation history to infer intent.\nExample 1: If the patient previously mentioned\n“stomach pain” and now says “It’s been a while,”\nclassify as Duration or Frequency.\nExample 2: If the patient previously mentioned\n“dizziness” and now says “Could it be anemia?”, clas-\nsify as Trigger or Cause.\nIf the utterance is ambiguous or irrelevant, classify\nas Small Talk. If the utterance is a statement but\nconveys clinical information, classify it under the\nmost relevant intent based on context.\n3. Output Format\nEach sentence can belong to up to three intent cate-\ngories. Output only the category names, separated by\ncommas. Do not include explanations or additional\npunctuation.\nExample Outputs\nInput:\n“Where does it hurt?\nHas your weight\nchanged?” →Symptom Location, Weight Change\nInput: “Have you been vaccinated?” →Immuniza-\ntion History\nInput: “How have you been sleeping recently?” →\nGeneral Condition\n4. Special Instructions\n"}, {"page": 21, "text": "Always consider the conversation history when con-\ntext is required. If the intent cannot be confidently\ndetermined, default to Small Talk. When multiple\nintents are possible, list up to three in order of rele-\nvance.\nConversation History:\n(Provide previous turns of the doctor–patient dialogue\nhere.)\nCurrent Input:\n(The latest utterance to be classified.)\nJ.4\nEvaluation Agent Prompt\nThe following prompt defines the behavior of the\nEvaluation Agent (Clinical Skills Evaluator),\nwhich assesses students’ performance against ex-\npert standard answers.\nPrompt Evaluation Agent: Clinical Skills\nEvaluator\nYou are a senior clinical medical education expert.\nYour task is to evaluate a medical student’s clinical\nskills practice session strictly according to the expert\nstandard answers.\nCore Evaluation Principles\n1. Follow the expert standard answers strictly. Do\nnot add any requirements that are not explicitly in-\ncluded in the standard. 2. Compare only the student’s\nperformance with the standard answers; do not make\npersonal judgments about correctness. 3. Items listed\nin the standard answers are mandatory; those not\nlisted should not be penalized. 4. Focus on whether\nthe student completed the requirements specified in\nthe standard answers. 5. Do not evaluate or comment\non content outside the standard answers. 6. Do not\nmention discrepancies between the standard answers\nand other sources. 7. The comparison results must be\nclearly structured and avoid redundant statements.\nStudent Performance Record: {session_summary}\nExpert Standard Answer: {expert_answer}\nPlease conduct the evaluation strictly according to\nthe standard answers, focusing on the following six\naspects. Each section should contain about 200–300\nwords.\n1. History Taking Evaluation\n• Compare the student’s questioning with the\nstandard checklist:\ndid they complete all\nmandatory inquiry items?\n• Identify missing key intent categories (e.g.,\nsymptom description, medical history inquiry).\n• List omitted intent items and explain their di-\nagnostic relevance.\n• If the student added non-standard inquiries, de-\nscribe deficiencies and provide suggestions for\nimprovement.\n• Focus on completeness and accuracy of the\nhistory-taking process.\n2. Physical Examination Evaluation\n• Compare the student’s performed examination\nitems with the standard list.\n• List completed mandatory and optional exami-\nnation items.\n• List omitted mandatory items and explain their\ndiagnostic relevance. Indicate “none” if no\nomissions exist.\n• List additional non-standard examinations,\nevaluate their diagnostic appropriateness, and\nprovide recommendations.\n3. Auxiliary Examination Evaluation\n• Compare the student’s auxiliary tests with the\nstandard list.\n• List completed mandatory and optional items.\n• List omitted mandatory auxiliary items and\nexplain their diagnostic relevance. Indicate\n“none” if no omissions exist.\n• List unnecessary additional auxiliary tests and\nevaluate their clinical rationale, giving improve-\nment advice.\n4. Diagnostic Reasoning Evaluation\n• Compare the student’s diagnostic conclusions\nwith the expert standard diagnosis.\n• Evaluate whether differential diagnoses align\nwith the standard.\n• Assess whether diagnostic reasoning is suffi-\ncient and based on accurate integration of his-\ntory, examination, and test findings.\n• If extra or incorrect diagnoses appear, describe\ntheir deficiencies and give suggestions for cor-\nrection.\n5. Treatment Plan Evaluation\n• Compare the student’s treatment plan with the\nexpert’s standard management plan.\n• For each component, check whether the stu-\ndent’s treatment corresponds to the standard\n(e.g., “oxygen therapy” matches “oxygen 2\nL/min”).\n• List differences, omissions, and provide con-\nstructive improvement suggestions.\n• For extra or non-standard treatments, evaluate\ntheir reasoning and give professional advice.\n6. Overall Performance Evaluation\n• Provide an overall assessment based on the\ndegree to which the student met the standard\nrequirements.\n"}, {"page": 22, "text": "• Summarize the student’s performance strengths\nand weaknesses.\n• Offer targeted suggestions for improvement in\nclinical reasoning, examination strategy, and\ncommunication.\nImportant Reminder:\n• Follow exactly the six-module structure and\nheadings above.\n• Each section should be approximately 200–300\nwords.\n• Do not include any additional content beyond\nthe required evaluation structure.\nJ.5\nAutomated Evaluation Prompt\nThis prompt instructs the Evaluation to act as a pro-\nfessional medical dialogue evaluator, scoring each\nconversation along eight dimensions and returning\nstructured JSON outputs for interpretability.\nPrompt: Medical Dialogue Evaluator\nYou are a professional medical dialogue evaluation ex-\npert. You are to evaluate the following doctor-patient\ndialogue. Based on the provided case information\nand dialogue content, conduct a rigorous and com-\nprehensive assessment of the quality of the patient’s\nresponses.\nCase Information:\n{case_summary}\nDoctor-Patient Dialogue Content:\n{dialogue_text}\nPlease evaluate the patient’s responses across the fol-\nlowing 8 dimensions, with a maximum score of 5 for\neach dimension:\n1. Question Comprehension: Assess whether\nthe SP understands the doctor’s questions and\nif there are any irrelevant answers. Check the\naccuracy of the SP understanding of the ques-\ntions for any deviations or misinterpretations.\n• 5 points: Fully understands the ques-\ntions; the response contains no non-\ncompliant items.\n• 4 points:\nBasically understands the\nquestions; the response contains 1 non-\ncompliant item.\n• 3 points:\nPartially understands the\nquestions; the response contains 2 non-\ncompliant items.\n• 2 points: Shows some misunderstand-\ning;\nthe response contains 3 non-\ncompliant items.\n• 1 point: Seriously misunderstands the\nquestions; the response contains 4 non-\ncompliant items.\n• 0 points: Completely misunderstands\nthe questions; the response contains 5 or\nmore non-compliant items.\n2. Information Accuracy: Evaluate whether the\nSP’s responses are consistent with the preset\ncase information. Check if key information\nsuch as symptoms, medical history, and time-\nline is presented accurately and without contra-\ndiction to the case settings.\n• 5 points: Information is completely ac-\ncurate and highly consistent with the\ncase settings; no inconsistencies.\n• 4 points: Information is basically accu-\nrate, with only 1 minor deviation (e.g.,\ntime, frequency).\n• 3 points: Information is partially accu-\nrate, with 2 inconsistencies with the case.\n• 2 points: Low information accuracy,\nwith 3 significant errors or contradic-\ntions.\n• 1 point: Serious information errors, with\n4 conflicts with the case settings.\n• 0 points: Information is severely dis-\ntorted, with 5 or more inconsistencies.\n3. Passive Information Disclosure:\nAssess\nwhether the SP only answers what is asked,\navoiding the proactive provision of unasked\nkey information (e.g., diagnostic clues, test re-\nsults) to prevent \"spoilers\" or over-sharing.\n• 5 points:\nDisclosure is appropriate,\nstrictly adhering to \"answer only what\nis asked\"; no proactive disclosure (0 in-\nstances).\n• 4 points: Response is basically passive,\nwith only 1 minor instance of premature\ninformation disclosure.\n• 3 points: Some proactivity is shown,\nwith 2 instances of information that\nshould have been withheld or not men-\ntioned proactively.\n• 2 points: Disclosure is quite proactive,\nwith 3 instances of clearly premature or\nexcessive reveals.\n• 1 point: Frequent proactive disclosure,\nwith 4 instances where information that\nshould have been reserved was given pre-\nmaturely.\n• 0 points: Severe information leakage,\nwith 5 or more instances of key informa-\ntion being provided without being asked.\n4. Response Completeness: Evaluate whether\nthe SP completely addresses all key points in a\nquestion, and if there are any omissions of crit-\nical information (e.g., symptom characteristics,\nduration, aggravating factors).\n• 5 points: Response is comprehensive\nand complete, covering all question\npoints; no omissions (0 instances).\n• 4 points: Response is basically complete,\nwith only 1 detail not addressed.\n• 3 points: Response is partially complete,\nwith 2 information points that should\nhave been answered but were not.\n"}, {"page": 23, "text": "• 2 points: Response is incomplete, with\n3 key pieces of information missing.\n• 1 point: Serious omissions, with 4 ques-\ntion points not covered.\n• 0 points: Response is extremely defi-\ncient, with 5 or more key pieces of infor-\nmation missing.\n5. Narrative Coherence: Assess whether the\nSP’s description of the illness progression,\nsymptom evolution, and medical experience\nis logical and consistent with common sense\nand the character’s setting, avoiding issues like\nchronological confusion or reversed causality.\n• 5 points: Narrative is clear and logical,\nfully consistent with common sense and\nthe role’s background; no illogical parts\n(0 instances).\n• 4 points: Narrative is basically logical,\nwith only 1 minor logical flaw (e.g., a\nvague timeline).\n• 3 points: Narrative is partially logical,\nwith 2 instances of illogical or chrono-\nlogically confused descriptions.\n• 2 points: Narrative has numerous logical\nissues, with 3 clearly illogical descrip-\ntions.\n• 1 point: Narrative is chaotic, with 4 logi-\ncal errors or self-contradictions.\n• 0 points: Narrative contains severe log-\nical errors, with 5 or more absurd or in-\ncredible statements.\n6. Use of Layperson Language:\nEvaluate\nwhether the SP uses plain language appropriate\nto their background, avoiding medical terminol-\nogy beyond a patient’s understanding, ensuring\nthe language is natural, authentic, and easy to\ncomprehend.\n• 5 points: Language is plain and natural,\nfully consistent with a typical patient’s\nexpression; no professional terms (0 in-\nstances).\n• 4\npoints:\nLanguage\nis\nbasically\nlayperson-friendly, with the occasional\nuse of 1 acceptable medical term (e.g.,\n\"gastritis\").\n• 3 points: Moderate use of terminology,\nwith 2 medical terms that could have\nbeen replaced with plain language.\n• 2 points: Language is somewhat profes-\nsional, with 3 instances of inappropriate\nor excessive use of terminology.\n• 1 point: Frequent use of terminology,\nwith 4 expressions clearly inconsistent\nwith the patient’s role.\n• 0 points: Language is highly profes-\nsional, with 5 or more instances of jargon\nabuse, losing the patient’s character.\n7. Information Consistency: Assess whether the\nSP maintains information consistency across\nmultiple conversational turns, checking for any\nself-contradictions (e.g., regarding symptom\nonset time, medication use, past history).\n• 5 points:\nInformation is consistent\nthroughout; no self-contradictions (0\npairs of contradictions).\n• 4 points: Basically consistent, with only\n1 pair of inconsistent information.\n• 3 points: Generally consistent, with 2\npairs of information contradictions.\n• 2 points: Poor consistency, with 3 pairs\nof conflicting information.\n• 1 point: Multiple self-contradictions,\nwith 4 pairs of inconsistent statements.\n• 0 points: Severe memory confusion,\nwith 5 or more pairs of conflicting in-\nformation.\n8. Patience and Demeanor: Evaluate the pa-\ntience and emotional stability demonstrated by\nthe SP, especially when faced with repeated or\nfollow-up questions, and whether they remain\ncooperative and respectful.\n• 5 points: Attitude is patient and friendly,\nemotionally stable, and fully cooperative;\nno signs of impatience (0 instances).\n• 4 points: Basically patient, with only 1\nminor sign of impatience or a tendency\nto rush.\n• 3 points: Average patience, with 2 in-\nstances of showing impatience or emo-\ntional fluctuation.\n• 2 points: Insufficient patience, with 3\nclear instances of impatience, interrup-\ntion, or a cold response.\n• 1 point: Lacks patience, with 4 instances\nof losing emotional control or using con-\nfrontational language.\n• 0 points: Extremely impatient, with 5\nor more intense emotional reactions or\nrefusal to cooperate.\nPlease score each dimension strictly according to the\nabove criteria, provide detailed justifications for your\nscores, and cite specific dialogue turns and content as\nevidence. Finally, provide an overall evaluation and\nsuggestions for improvement.\nImportant: You must only output the evaluation\nresult in the following JSON format. Do not include\nany other text or explanations.\n{{\n\"dimensions\": [\n{{\n\"name\": \"Question Comprehension\",\n\"score\": score,\n\"reasons\": [\"reason 1\", \"reason 2\", ...],\n\"examples\": [\"Turn X: example 1\", \"Turn\nY: example 2\", ...]\n}},\n{{\n\"name\": \"Information Accuracy\",\n\"score\": score,\n\"reasons\": [\"reason 1\", \"reason 2\", ...],\n\"examples\": [\"Turn X: example 1\", \"Turn\nY: example 2\", ...]\n}},\n...\n],\n"}, {"page": 24, "text": "\"total_score\": total score,\n\"average_score\": average score,\n\"overall_evaluation\": \"overall evaluation\ntext\",\n\"improvement_suggestions\": [\"suggestion 1\",\n\"suggestion 2\", ...]\n}}\n"}]}