{"doc_id": "arxiv:2512.22966", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.22966.pdf", "meta": {"doc_id": "arxiv:2512.22966", "source": "arxiv", "arxiv_id": "2512.22966", "title": "Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks", "authors": ["Mengdi Chai", "Ali R. Zomorrodi"], "published": "2025-12-28T15:15:51Z", "updated": "2025-12-28T15:15:51Z", "summary": "Large Language Models (LLMs) have demonstrated promise in medical knowledge assessments, yet their practical utility in real-world clinical decision-making remains underexplored. In this study, we evaluated the performance of three state-of-the-art LLMs-ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70B-in clinical decision support across the entire clinical reasoning workflow of a typical patient encounter. Using 36 case studies, we first assessed LLM's out-of-the-box performance across five key sequential clinical decision-making tasks under two temperature settings (default vs. zero): differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. All models showed high variability by task, achieving near-perfect accuracy in final diagnosis, poor performance in relevant diagnostic testing, and moderate performance in remaining tasks. Furthermore, ChatGPT performed better under the zero temperature, whereas LIama showed stronger performance under the default temperature. Next, we assessed whether prompt engineering could enhance LLM performance by applying variations of the MedPrompt framework, incorporating targeted and random dynamic few-shot learning. The results demonstrate that prompt engineering is not a one-size-fit-all solution. While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others. Another key finding was that the targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity. These findings suggest that the impact of prompt engineering is highly model and task-dependent, highlighting the need for tailored, context-aware strategies for integrating LLMs into healthcare.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.22966v1", "url_pdf": "https://arxiv.org/pdf/2512.22966.pdf", "meta_path": "data/raw/arxiv/meta/2512.22966.json", "sha256": "2edb65d54796316598e69298f9b76dc38bb85053623c4216c793f83e6749dee4", "status": "ok", "fetched_at": "2026-02-18T02:23:44.134491+00:00"}, "pages": [{"page": 1, "text": "Prompt engineering does not universally improve Large \nLanguage Model performance across clinical decision-\nmaking tasks \n \nMengdi Chai 1,2 and Ali R. Zomorrodi 2,3,4* \n \n1 Department of Biostatistics, Harvard School of Public Health, Boston, MA \n2 Mucosal Immunology and Biology Research Center, Department of Pediatrics, Massachuse<s \nGeneral Hospital, Boston, MA \n3 Harvard Medical School \n4 Broad Institute of MIT and Harvard \n \n*Corresponding author \nE-mail: azommorrodi@mgh.harvard.edu \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 2, "text": "Abstract \nLarge Language Models (LLMs) have demonstrated promise in medical knowledge \nassessments, yet their practical utility in real-world clinical decision-making remains \nunderexplored. In this study, we evaluated the performance of three state-of-the-art \nLLMsâ€”ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70Bâ€”in clinical decision support \nacross the entire clinical reasoning workï¬‚ow of a typical patient encounter. Using 36 case \nstudies, we ï¬rst assessed LLMâ€™s out-of-the-box performance across ï¬ve key sequential \nclinical decision-making tasks under two temperature settings (default vs. zero): \ndi]erential diagnosis, essential immediate steps, relevant diagnostic testing, ï¬nal \ndiagnosis, and treatment recommendation. All models showed high variability by task, \nachieving near-perfect accuracy in ï¬nal diagnosis, poor performance in relevant diagnostic \ntesting, and moderate performance in remaining tasks. Furthermore, ChatGPT performed \nbetter under the zero temperature, whereas LIama showed stronger performance under the \ndefault temperature. Next, we assessed whether prompt engineering could enhance LLM \nperformance by applying variations of the MedPrompt framework, incorporating targeted \nand random dynamic few-shot learning. The results demonstrate that prompt engineering \nis not a one-size-ï¬t-all solution. While it signiï¬cantly improved the performance on the task \nwith lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for \nothers. Another key ï¬nding was that the targeted dynamic few-shot prompting did not \nconsistently outperform random selection, indicating that the presumed beneï¬ts of closely \nmatched examples may be counterbalanced by loss of broader contextual diversity. These \nï¬ndings suggest that the impact of prompt engineering is highly model and task-\ndependent, highlighting the need for tailored, context-aware strategies for integrating LLMs \ninto healthcare.  \nKeywords: Clinical decision-making; Large Language Models; Prompt engineering; AI in \nhealthcare \n \n \n \n \n \n"}, {"page": 3, "text": "Introduction \nClinical decision-making is recognized as a cognitively demanding process in safe and \ne]ective patient care. Yet diagnostic and therapeutic errors remain alarmingly prevalent: \napproximately 795,000 Americans su]er permanent disability or death annually across \nhealthcare systems due to misdiagnosed conditions 1. Such failures may arise from time \npressures, the overwhelming volume of information clinicians must process in real time, \ncliniciansâ€™ overconï¬dence, or miscommunication 2,3. General-purpose LLMs have emerged \nas a potentially transformative paradigm that can help mitigate these challenges by \naugmenting provider judgment. Trained on vast corpora of text and ï¬ne-tuned through \nmethods such as reinforcement learning from human feedback (RLHF), these models have \ndemonstrated prominent capabilities in understanding and generating natural language 4. \nLLMs have also exhibited emergent behaviors such as in-context learning and \ncompositional reasoning that extend well beyond mere memorization and both traditional \nand modern Computerized Clinical Decision Support Systems (CDS) 5-7. These properties \nmake them uniquely suited to the inherently ambiguous and multifaceted nature of clinical \ndecision-making, where structured reasoning must frequently be applied under conditions \nof uncertainty. \nRecent studies have highlighted this potential by documenting the strong performance of \nLLMs on standardized medical knowledge assessment tests. Models such as OpenAIâ€™s \nGPT-4 8, Metaâ€™s LIama9, and Googleâ€™s Gemini 10 have achieved expert-level accuracy on \nbenchmarks including the United States Medical Licensing Examination (USMLE) and \nMedQA, in some cases matching or surpassing the performance of trained physicians in \nmultiple-choice testing environments 11,12. These ï¬ndings indicate the breadth of medical \nknowledge encoded within modern LLMs. \nPrompt engineering has been recognized as a means of reï¬ning LLM behavior. Structured \nprompting strategiesâ€”such as chain-of-thought (CoT) reasoning and few-shot promptingâ€”\nhave been shown to enhance accuracy, reduce hallucinations, and improve logical \ncoherence in both medical and non-medical tasks13,14. Notably, the MedPrompt framework \nhas demonstrated promising results in complex medical exam settings by dynamically \nselecting CoT exemplars and applying ensembling techniques 15. These methods represent \na model-agnostic, low-cost strategy for improving reasoning without additional training. \nHowever, success on knowledge-based examinations does not equate to proï¬ciency in \nreal-world clinical reasoning. E]ective decision-making at the bedside requires the \nintegration of patient-speciï¬c data, prioritization under uncertainty, and the timing and \narrangement of diagnostic test and therapeutic interventionsâ€”skills that are highly \ncontext-dependent, temporally sensitive, and not amenable to factual recall 16-19. Thus, \n"}, {"page": 4, "text": "while strong exam performance represents a notable milestone, it is insu]icient to \nestablish clinical safety or utility. Most evaluations of prompt engineering to date remain \nconï¬ned to these standard medical benchmarks and a narrow range of tasks, focusing \nprimarily on di]erential or ï¬nal diagnoses. Moreover, approaches such as MedPrompt have \nbeen tested almost exclusively on OpenAI models like ChatGPT 15, and while these \nmethods are model agnostic, their generalizability to other LLM architectures has not been \nexplored. This underscores the pressing need for systematic, task-level evaluations that \nassess how di]erent prompting strategies inï¬‚uence LLM performance across the full \nspectrum of clinical workï¬‚ow. \nIn this study, we aimed to address this gap by systematically evaluating the performance of \nthree state-of-the-art LLMsâ€”ChatGPT-4o, LIama 3.3 70B, and Gemini 1.5 Proâ€”across all \nkey stages of clinical reasoning using both baseline prompting and prompt engineering \napproaches. Through this analysis, we sought to elucidate the strengths and limitations of \nLLMs in clinical reasoning and to explore where prompt engineering can enhance their \nperformance in complex healthcare scenarios. \nResults \nA total of 36 publicly available clinical cases were extracted from the Merck Sharpe & \nDohme (MSD) Clinical Manual, also known as the MSD Manual 20. These clinical vignettes \nare designed to reï¬‚ect real-world clinical scenarios that healthcare professionals \ncommonly encounter and involve a structured clinical reasoning workï¬‚ow that begins with \npatientâ€™s history of present and past illness (HPI), review of systems (ROS), and physical \nexamination (PE); it then proceeds through ï¬ve sequential decision-making tasks: \ndi]erential diagnosis, essential immediate steps, relevant diagnostic testing, ï¬nal \ndiagnosis, and treatment recommendations (Figure 1). For each decision-making task, \nLLMs were provided with progressively richer clinical input: initially, HPI, ROS, and PE for \ndi]erential diagnosis; subsequently, HPI, ROS, and PE along with the ground truth for \ndi]erential diagnosis (available from clinical vignettes) for essential immediate steps; then, \nall prior information plus ground truth for essential immediate steps for relevant diagnostic \ntesting; and so forth for each subsequent task. This ensured the model had access to all \npertinent context as it moved through the clinical reasoning sequence. We evaluated \nmodel performance on each of the ï¬ve clinical reasoning tasks with this information \npresented to them using both baseline prompting (without engineering) and prompts \nenhanced by engineering strategies. \n"}, {"page": 5, "text": "  \n  \nFigure 1. Workï¬‚ow for assessing LLM performance across diverse clinical decision-making tasks. \nFor each task, the HPI, ROS, and PE results, the ground truth answers from all previous tasks, and \nthe current set of answer choices relevant to the task were provided as inputs to the LLM. \nEvaluating out-of-the-box LLM performance across clinical decision-making \nworkï¬‚ow and di=erent temperature settings \nWe ï¬rst aimed to comprehensively characterize the out-of-the-box performance of leading \nLLMs, ChatGPT-4o (Open AI), Gemini 1.5 Pro (Google), and Llama 3.3 70B (Meta), across \nthe entire clinical reasoning workï¬‚ow and default vs. zero temperature settings. To this end, \nwe evaluated each LLMâ€™s clinical reasoning accuracy on the full set of 36 clinical vignettes \nfrom the MSD database using basic prompting. For each reasoning task. relevant patient \ninformation was presented to the models without additional prompt engineering strategies \nto observe how each model processed and responded to standard clinical input in a \nstraightforward manner. We also investigated whether the performance may di]er between \nthe default temperature for each model and zero temperature settings. Each clinical \nreasoning task involves multiple answer options and more than one may be correct. Model \naccuracy is calculated as the percentage of correct selections by the model among the \ntotal correct options. For instance, if a di]erential diagnosis question presents 10 possible \nconditions and 6 are correct (i.e., 6 cannot be excluded based on the presented patient \ninformation), and the model identiï¬es 4 of these, its accuracy for that vignette is 4/6 \n(66.7%). All evaluations for each clinical vignette, every clinical task, and under each \nTemperature = Default\nvs. \nTemperature = 0\nHistory of Present \nand Past Illness\nReview of \nSystem\nPhysical \nExamination\nDifferential \nDiagnosis\nEssential \nImmediate Steps\nRelevant \nDiagnostic Testing\nFinal \nDiagnosis\nTreatment \nRecommendation\nChatGPT 4o\nGemini 1.5 Pro\nLIama 3.3 70B\n"}, {"page": 6, "text": "temperature setting were conducted three times to capture variability in LLMs responses. \nThe accuracy for each vignette was calculated as the average of these three repeated trials \nand the overall model performance for each decision-making task was then determined by \naveraging the mean accuracies across all clinical vignettes. The ï¬ndings of this analysis are \nsummarized in Figure 2, with LLM accuracy for individual cases at default and zero \ntemperature settings are provided in Supplementary File 1. \nDi8erential Diagnosis: For each of the 36 clinical vignettes, the LLMs were provided with \nthe HPI, ROS, and PE results, along with a list of potential conditions for di]erential \ndiagnosis question. The models were then tasked with identifying which of the presented \nconditions could not be excluded based on the available patient information. The best \nperformance was delivered by LIama 3.3 70B and ChatGPT 4o both at default temperature \nsetting, with accuracies of 71.6% and 71.0%, respectively. Gemini 1.5 Pro exhibited the \nlowest accuracy at 61.9% as its best performance achieved at zero temperature (Figure 2).  \nEssential Immediate Steps: In this task, each LLM was provided with the HPI, ROS, and \nPE, as well as the ground truth for the di]erential diagnosis, together with essential \nimmediate step questions. The models were prompted to select which actions were \nessential to take immediately. Out of the 36 clinical vignettes, 12 included this task. LIama \n3.3 70B demonstrated the highest accuracy for this task, achieving 77.5% at default \ntemperature and a comparable accuracy of 77.3% at zero temperature. This outperformed \nboth ChatGPT 4o and Gemini 1.5 Pro at both zero (72.0% for ChatGPT 4o and 70.2% for \nGemini 1.5 Pro) and default (68.9% for ChatGPT 4o and 70.8% for Gemini 1.5 Pro) \ntemperature settings (Figure 2).  \nRelevant Diagnostic Testing: For this task, each LLM was provided with the HPI, ROS, and \nPE results, as well as the ground truth for prior clinical tasks (di]erential diagnosis and \nessential immediate steps), along with a list of possible relevant diagnostic testing. The \nmodels were then prompted to identify which tests were the most appropriate initial \ndiagnostic studies to guide ï¬nal diagnosis and treatment. Of the 36 clinical vignettes, 21 \nincluded this task. All models performed poorly on this task with LIama 3.3 70B once again \nachieving the highest accuracy, scoring 50.8% at default temperature and a comparable \nperformance (50.2%) at zero temperature. Gemini 1.5 Pro followed, with accuracies of \n48.1% at zero temperature, while ChatGPT 4o recorded the lowest score, achieving 39.4% \naccuracy at zero temperature as its highest performance (Figure 2).  \nFinal Diagnosis: Here, the LLMs were provided with the HPI, ROS, and PE results, the \nground truth for all prior reasoning steps (di]erential diagnosis, essential immediate steps, \nand relevant diagnostic testing), the results of diagnostic tests, and a list of choices for \nï¬nal diagnosis. Each model was then tasked to identify which of the presented conditions \n"}, {"page": 7, "text": "represented the most likely ï¬nal diagnosis for the patient. Of the 36 clinical vignettes, 33 \nincluded this task. All three LLMs performed strongly on this task: ChatGPT 4o achieved a \nnear-perfect accuracy of 98.4% at zero temperature, with Gemini 1.5 Pro and LIama 3.3 \n70B following closely both with an accuracy of 96.8% (Figure 2).  \n \nFigure 2. Evaluation of the three LLMsâ€™ performance under default and zero temperature settings \nusing baseline prompting. Each bar indicates the average accuracy achieved for each clinical \ndecision-making task across all available clinical vignettes. \nTreatment Recommendation: For this task, all patient information, the ground truth from \nall previous clinical tasks, diagnostic test results, and a list of treatment options were \npresented to the models. Each model was then prompted to identify which treatment \noptions were most appropriate for the patients based on their ï¬nal diagnosis. Of the 36 \nclinical vignettes, 32 included this task. For this task, LIama 3.3 70B again demonstrated \n71.0%\n57.7%\n71.6%\n70.6%\n61.9%\n68.4%\n0\n20\n40\n60\n80\n100\nChatGPT 4o\nGemini 1.5 Pro\nLIama 3.3 70B\nAccuracy (%)\nDifferential Diagnosis\n79.2%\n73.6%\n82.0%\n81.3%\n72.7%\n81.7%\n0\n20\n40\n60\n80\n100\nChatGPT 4o\nGemini 1.5 Pro\nLIama 3.3 70B\nAccuracy (%)\nTreatment Recommendation\n94.1%\n96.8%\n96.8%\n98.4%\n96.8%\n96.8%\n0\n20\n40\n60\n80\n100\nChatGPT 4o\nGemini 1.5 Pro\nLIama 3.3 70B\nAccuracy (%)\nFinal Diagnosis\n32.3%\n43.7%\n50.8%\n39.4%\n48.1%\n50.2%\n0\n20\n40\n60\n80\n100\nChatGPT 4o\nGemini 1.5 Pro\nLIama 3.3 70B\nAccuracy (%)\nRelevant Diagnostic Testing\n68.9%\n70.8%\n77.5%\n72.0%\n70.2%\n77.3%\n0\n20\n40\n60\n80\n100\nChatGPT 4o\nGemini 1.5 Pro\nLIama 3.3 70B\nAccuracy (%)\nEssential Immediate Steps\nDifferential Diagnosis\nEssential Immediate Steps\nRelevant Diagnostic Testing\nTreatment Recommendation\nFinal Diagnosis\nTemperature = 0\nTemperature = Default\n"}, {"page": 8, "text": "the strongest performance, with accuracies of 82% at default temperature and a \ncomparable accuracy of 81.7% at zero temperature. ChatGPT 4o followed with accuracies \nof 81.3% at zero temperature, while Gemini 1.5 Pro achieved an accuracy of 73.6% at \ndefault temperatures (Figure 2).  \nE8ect of temperature on LLM clinical reasoning accuracy:  Although slight di]erences \nwere observed in LLM performance between the default and zero temperature settings \nacross all ï¬ve clinical reasoning tasks and for all three models (Figure 2), statistical \nanalysis conï¬rmed that none of the observed di]erences in model accuracy between \ntemperature settings were statistically signiï¬cant (paired Mann-Whitney U, p < 0.05; \nSupplementary Figure 1).    \nEvaluating the impact of prompt engineering strategies on LLM performance \nacross clinical decision-making tasks \nFollowing our baseline evaluation, we sought to determine whether prompt engineering \ncould further enhance the clinical reasoning capabilities of the LLMs. To this end, we \nleveraged prompt engineering strategies from MedPrompt 15, a structured framework \ndesigned to optimize model performance by integrating self-generated chain-of-thought \n(CoT) reasoning with dynamic few-shot learning and answer choice shu]ling ensemble. \nSelf-generated CoT and choice shu]ling ensemble approaches were the same as in \nMedPrompt 15. For dynamic few-shot learning, we explored two di]erent example selection \napproaches. The ï¬rst utilized a targeted dynamic few-shot selection scheme based on the \nð¾ nearest neighbors (ð¾NN), where ð¾ examples are chosen based on their semantic \nsimilarity to the test question, as originally proposed in MedPrompt 15. The second \napproach substituted this with random dynamic few-shot selection, where ð¾ exemplars \nwere chosen at random from the training set (Figure 3).  \nWe excluded ï¬nal diagnosis from our analysis as all three LLMs showed near-complete \naccuracy for this task using basic prompting (Figure 2), which left little room for further \nimprovement. For the remaining question types, cases were partitioned into training and \ntest sets; 30% of cases were allocated to the training set for the generation of few-shot \nexamples with self-generated CoT reasoning, while the remaining cases formed the test set \nfor model evaluation. For each task, ð¾= 3 cases from the training set were selected as \nexamples using ð¾NN or random dynamic few-shot prompting (see Methods for details). \nThe distribution of clinical vignettes across training and test subsets for each clinical \nreasoning task is summarized in Supplementary File 1. The LLM performance before and \nafter applying prompt engineering was compared for the test dataset, with the baseline for \nthis analysis being the performance on the test dataset (not on the entire dataset as \ndiscussed in the previous section), to ensure a valid comparison. Notably, given that our \n"}, {"page": 9, "text": "analysis presented in the previous section revealed no statistically signiï¬cant performance \ndi]erences between temperature settings, all prompt engineering evaluations were \nconducted using each model's default temperature. The results of this analysis is shown in \nFigure 4. \n \nFigure 3. Overview of structured prompt engineering strategies evaluated in this study across \nclinical reasoning tasks. The MedPrompt framework 15 was assessed using both the original ð¾NN \nand random dynamic few-shot selection methods. \nDi8erential Diagnosis: The application of prompt engineering for essential di]erential \ndiagnosis yielded varied and often counterproductive results across the models. LIama 3.3 \n70B, which established the strongest baseline accuracy on the test set at 69.9%, showed a \nmodest improvement to 72.3% with CoT reasoning + ð¾NN dynamic few-shots + choice \nshu]ling ensemble (2.4% performance improvement). Conversely, the use of MedPrompt \nwith the originally proposed ð¾NN dynamic few-shots led to a slight degradation in its \nperformance to 69.0%. For the other two models, both of the prompt engineering strategies \nwere largely detrimental. ChatGPT 4oâ€™s accuracy declined substantially from 68.6% to \n56.9% and 59.2% with MedPrompt using random and ð¾NN dynamic few-shot prompting, \nrespectively, while Gemini 1.5 Pro experienced a notable drop in performance from 62.3% \nat baseline to 47.6% when ð¾NN dynamic few-shot examples were used. These outcomes \nsuggest that for this speciï¬c clinical task, advanced prompting did not o]er a clear \nadvantage and frequently impaired reasoning. \nEssential Immediate Steps: The prompt engineering strategies had a notably divergent \nimpact on the models' ability to identify essential immediate steps (Figure 4). For Llama \n3.3 70B, both prompt engineering strategies yielded substantial gains; its accuracy rose \nfrom a baseline of 62.2% to 70.2% (8.0% improvement) using MedPrompt with random \nSelf-generated CoT (Chain of Thought)\nEncourage the model to explain its \nreasoning step-by-step \nKNN Dynamic Few-Shot Learning\nCreate embeddings for questions in the \ntraining and test set. Use cosine similarities \nto find the top-K most similar examples\nAnswer choice Shuffling Ensemble\nShuffle answer choices to avoid \nposition bias\nRandom Dynamic Few-Shot Learning\nFor each test question, choose examples \nrandomly from the training dataset.\n+\n+\nvs.\nMedPrompt with KNN vs. Random Dynamic Few-Shot Learning \n"}, {"page": 10, "text": "dynamic few-shot examples and climbed even higher to 74.1% with ð¾NN dynamic few-\nshot selection (11.9% improvement). Gemini 1.5 Pro, which scored the highest \nperformance with an accuracy of 75.3% at baseline, showed no change in performance \nwith prompt engineering using random dynamic few-shots (75.3%) and a slight \nperformance drop to 74.3% with ð¾NN dynamic few-shots. ChatGPT 4oâ€™s performance \ndegraded with both methods, dropping from its baseline of 61.8% to 57.4%, and 58.9% for \nrandom and ð¾NN dynamic few-shot prompting, respectively.  \nRelevant Diagnostic Testing: The task of identifying relevant diagnostic tests proved to be \nthe area where prompt engineering delivered the most dramatic improvements, although \nthe beneï¬ts were not uniform across all models (Figure 4). Gemini 1.5 Pro, in particular, \nexperienced a remarkable increase in performance. Scoring 46.5% at baseline, its \naccuracy rose to 66.8% using MedPrompt with random dynamic few-shots examples \n(20.3% improvement) and surged to 71.5% with ð¾NN dynamic few-shots (25.0 % \nimprovement). LIama 3.3 70B also beneï¬ted, with highest gains achieved using \nMedPrompt with random dynamic few-shot prompting, with its accuracy increased from \n39.1% at baseline to 62.7% (23.6% improvement), while the more targeted ð¾NN dynamic \nselection method was less e]ective for Llama, resulting in a much smaller ï¬nal accuracy of \n41.8% (2.7% improvement). Likdwise, ChatGPT 4o began with the lowest baseline accuracy \nat 24.8%, and both prompting strategies provided a noticeable rise, increasing its accuracy \nto 32.9%, and 36.9% (8.1% and 12.1% improvement) for random and ð¾NN dynamic few-\nshots, respectively. \nTreatment Recommendation: When tasked with treatment recommendation, the \napplication of prompt engineering strategies yielded counterproductive results for the two \nstrongest baseline models (Figure 4). LIama 3.3 70B, which began with the highest \nbaseline accuracy at 72.8%, experienced an identical performance drop to 69.5% using \nMedPrompt with both random and ð¾NN dynamic few-shot prompting (3.3% performance \ndrop). A more pronounced decline was observed for ChatGPT 40: its performance reduced \nfrom a strong baseline accuracy of 71.0% to 66.1% with random dynamic few-shots (4.9% \ndecrease) and then sharply to 50.2% with ð¾NN dynamic few-shots (20.8% decrease). \nConversely, Gemini 1.5 Proâ€™s performance remained stable, showing only small ï¬‚uctuation \nfrom its baseline accuracy of 67.6%, with accuracies of 68.4%, and 67.6% with random and \nð¾NN dynamic few-shot examples, respectively. These results indicate that for treatment \nrecommendation, the structured reasoning imposed by prompt engineering hindered, \nrather than helped, the models' decision-making processes. \n"}, {"page": 11, "text": "  \n \nFigure 4. Assessment of the eLect of prompt engineering strategies on LLM performance across \nclinical decision-making tasks. Performance was evaluated for the test dataset using the \nMedPrompt framework 15, which integrates CoT reasoning, ð¾NN dynamic few-shot learning, and \nanswer choice shuLling assemble, as well as a variant of MedPrompt in which ð¾NN dynamic few-\nshot learning was substituted with random dynamic few-shot learning. \nDiscussion \nIn this study, we comprehensively evaluated the capabilities of three leading LLMs, \nChatGPT 4o, Gemini 1.5 Pro, and LIama 3.3 70B, for knowledge recall and clinical \nreasoning across ï¬ve core, sequential decision-making tasks typical of a clinical \nencounter. This analysis aimed to establish a comprehensive out-of-the-box performance \nproï¬le for each model across these clinical reasoning tasks in order to better understand \ntheir inherent strengths and weaknesses in a realistic clinical context. This analysis \nrevealed a crucial gap between the theoretical knowledge of LLMs and their practical \napplication in dynamic clinical workï¬‚ows. While all three models excelled in ï¬nal \ndiagnosis, their baseline performance varied signiï¬cantly across other tasks in the clinical \nworkï¬‚ow. More importantly, our ï¬ndings demonstrate that advanced prompt engineering is \n71.0%\n67.6%\n72.8%\n50.2%\n67.6%\n69.5%\n66.1%\n68.4%\n69.5%\n0\n20\n40\n60\n80\n100\nChatGPT 4o\nGemini 1.5 Pro\nLIama 3.3 70B\nAccuracy (%)\nTreatment Recommendation\n61.8%\n75.3%\n62.2%\n58.9%\n74.3%\n74.1%\n57.4%\n75.3%\n70.2%\n0\n20\n40\n60\n80\n100\nChatGPT 4o\nGemini 1.5 Pro\nLIama 3.3 70B\nAccuracy (%)\nEssential Immediate Steps\n24.8%\n46.5%\n39.1%\n36.9%\n71.5%\n41.8%\n32.9%\n66.8%\n62.7%\n0\n20\n40\n60\n80\n100\nChatGPT 4o\nGemini 1.5 Pro\nLIama 3.3 70B\nAccuracy (%)\nRelevant Diagnostic Testing\n68.6%\n62.3%\n69.9%\n59.2%\n47.6%\n69.0%\n56.9%\n62.6%\n72.3%\n0\n20\n40\n60\n80\n100\nChatGPT 4o\nGemini 1.5 Pro\nLIama 3.3 70B\nAccuracy (%)\nDifferential Diagnosis\nDifferential Diagnosis\nEssential Immediate Steps\nBaseline\nCoT + KNN Dynamic Few-shot + Choice \nShuffling Ensemble\nCoT + Random Dynamic Few-\nshot + Choice Shuffling Ensemble\nRelevant Diagnostic Testing\nTreatment Recommendation\n"}, {"page": 12, "text": "not a one-size-ï¬ts-all solution to improve LLM performance. The application of structured \nprompting based on the MedPrompt framework yielded highly inconsistent and sometimes \ncounterproductive outcomes, suggesting that these techniques must be tailored to both \nthe speciï¬c model and the clinical task at hand, rather than being applied as a universal \nstrategy for improvement. \nOut-of-the-box LLM performance varies by clinical task and model: Our foundational \nanalysis of out-of-the-box LLM performance revealed signiï¬cant variability, not only \nbetween the models but also across the di]erent clinical tasks. LIama 3.3 70B consistently \nmatched or outperformed its peers across most evaluated tasks, demonstrating the best \nperformance in di]erential diagnosis, essential immediate steps, relevant diagnostic \ntesting, and treatment recommendation. On the other hand, while all three models \nperformed strongly on ï¬nal diagnosis, they all performed poorly on relevant testing and \nachieved moderate to moderately high accuracy in the remaining tasks. This performance \nspectrum appears to reï¬‚ect the inherent cognitive demand and complexity of each task. \nThe near-perfect accuracy achieved in ï¬nal diagnosis, for instance, can be likely attributed \nto the fact that the models were provided with a complete set of information, including all \nprior ground truth and deï¬nitive diagnostic test results, which signiï¬cantly constrains the \ndiagnosis problem and minimizes uncertainty. Conversely, relevant diagnostic testing likely \nproved the most di]icult because it requires a higher cognitive load: the model must \nreason under uncertainty, weigh the potential diagnostic yield of multiple test options, and \nstrategize the most e]icient path to a diagnosis. This highlights the di]iculty of a \nfundamentally di]erent type of reasoning compared to ï¬nal diagnosis: making decisions \nunder uncertainty. \nModel accuracy was robust to temperature changes, though the choice remains \ncritical for clinical applications: Our initial rationale for including the zero temperature \nsetting in our analysis was that reduced variability in model outputs might help ensure \nmore consistent and reproducible clinical reasoning, which is particularly valuable in high-\nstakes healthcare environments. On the other hand, zero temperature may also \ncompromise the performance as it could potentially favor more conservative or \nprototypical responses, impacting the breadth or creativity of the model's clinical \nreasoning. Examining both settings allowed us to explore whether certain clinical reasoning \ntasks beneï¬t from more predictable, consistent responses (high determinism) versus \nallowing for a more ï¬‚exible and broader range of potential solutions (greater adaptability). \nOur analysis identiï¬ed subtle model-speciï¬c trends between temperature settings. For \nexample, ChatGPT-4o exhibited a slight decline in performance under default temperature \nacross all tasks except di]erential diagnosis, whereas LIama 3.3 70B performed slightly \nbetter under default temperature for all tasks excepts ï¬nal diagnosis. Nonetheless, none of \n"}, {"page": 13, "text": "the observed di]erences in accuracy between the default and zero temperature settings \nwere statistically signiï¬cant (paired Mannh-Whitney U, p < 0.05) across any of the tasks \nand the observed changes were all marginal. This suggests that model accuracy is mostly \nrobust to this hyperparameter within the scope of the MSD dataset. However, the choice of \ntemperature setting remains clinically signiï¬cant; while subtle performance trends were \nobserved in our study, the decision to prioritize deterministic, reproducible outputs over \nmore varied responses will be a critical consideration for the safe and reliable deployment \nof these models in clinical workï¬‚ows. This warrants further explorations across larger and \nmore diverse datasets. \nStructured prompting can degrade performance by over-constraining a modelâ€™s \nreasoning process: A key ï¬nding of our study is that while MedPromptâ€™s structured prompt \nengineering aims to guide models toward more systematic reasoning, it does not \nuniversally improve model performance, and in several cases, it can paradoxically be \ncounterproductive, particularly when the prompts impose elaborate reasoning sca]olds or \ndemand tight alignment between example semantics. For instance, ChatGPT 4oâ€™s \nperformance degraded under MedPrompt with both ð¾NN and random dynamic few-shots \nfor all tasks except relevant diagnostic testing, while Gemini 1.5 Pro and LIama 3.3 70B \nresponded more favorably to prompt engineering (Figure 4).  Llama 3.3 70B especially \nbeneï¬ted from prompt engineering for all tasks except for treatment recommendation.  \nPerformance drops when subjected to prompt engineering was most evident for ChatGPT-\n4o.  For instance, its accuracy in treatment recommendation fell sharply from 71.0% to \n50.2% with MedPrompt using ð¾NN dynamic few-shot prompting (Figure 4). This e]ect is \nnot unique to one model or one task; Llama 3.3 70B's performance also degraded in \ntreatment recommendation with prompting strategies, and Gemini 1.5 Pro experienced a \nsharp accuracy drop in di]erential diagnosis when using MedPrompt with ð¾NN dynamic \nfew-shot examples. This indicates that even models that beneï¬t from advanced prompting \nin some scenarios, can be impaired by it in others. These observations suggest a \nphenomenon of \"cognitive forcing\" e]ect, where the structured, step-by-step reasoning \nimposed by the MedPrompt framework may interfere with and override a model's e]ective, \npre-trained heuristics. This hypothesis is supported by emerging ï¬ndings in the broader \nLLM literature that forcing models into rigid, structured reasoning paths can undermine \ntheir natural reasoning process 21,22. For example, both Shao et al 23 and Kim et. al 21 argue \nthat CoT functions as a structural constraint that drives models to mimic reasoning \npatterns rather than engage in genuine, ï¬‚exible, and novel inference pathways.  \nAnother notable observation was that MedPrompt with the more targeted ð¾NN dynamic \nfew-shots did not always outperform that with random dynamic few-shots. This ï¬nding \n"}, {"page": 14, "text": "indicates that relying solely on semantic similarity to select few-shot examples may not \nalways yield the best results for model performance. The reason for this unexpected \nobservation could be that the presumed advantage of closely matched examples can be \no]set in some cases by the loss of broader contextual diversity or alternative reasoning \nstrategies. These mixed outcomes are in contrast to the consistent improvements reported \nin the original MedPrompt paper for medical knowledge exams 15 and highlight the intricate \ninterplay between model architecture, the speciï¬c clinical reasoning task, and the chosen \nprompting strategy in real-world clinical scenarios. \nPrompt engineering acted as e8ective sca8olding for tasks with high uncertainty: \nPrompt engineering provided substantial beneï¬ts for the single most challenging task: \nrelevant diagnostic testing. Across all three models, which showed very low baseline \nscores on this task, the prompting frameworksâ€”MedPrompt with both ð¾NN and random \ndynamic few-shotsâ€”acted as a form of â€œcognitive sca]oldingâ€, guiding the models through \na reasoning process where their baseline capabilities were weakest. For example, the \naccuracy of Gemini 1.5 Pro for relevant diagnostic testing surged from 46.5% to 71.5% with \nMedPrompt using ð¾NN few-shots (a 25.0% improvement). This suggests that when a model \nlacks a robust internal strategy for a high-uncertainty task, the explicit structure of a \nprompt is highly beneï¬cial.  \nTogether, the results of our evaluation of LLMs performance when subjected to prompt \nengineering highlight that prompting strategies must be tailoredâ€”both to the speciï¬c \nmodel and the clinical task at hand.  \nThe study's ï¬ndings should be interpreted in the context of several key limitations: It is \nimportant to consider several limitations when interpreting out ï¬ndings in this study. First, \nthe evaluation was conducted on a modest dataset of 36 clinical vignettes, with the \nsample size for some tasks, such as essential immediate steps, being as low as 12 cases. \nWhile su]icient for this exploratory analysis, this small sample size limits the statistical \npower of our comparisons and increases the chance of Type II errors, which may obscure \nsubtle but clinically meaningful performance di]erences. This reinforces the need for \nlarger-scale evaluations. Second, the clinical cases were sourced from the MSD Manual, \nwhich is an evidence-based educational resource. Although this ensures high-quality and \nwell-structured data, these vignettes do not fully represent the diversity, complexity, \nambiguity, and noise of real-world clinical practice, which often involves incomplete or \ncontradictory information from electronic health records. Third, model performance was \nassessed using a performance metric deï¬ned for the degree of overlap between predicted \nmultiple-choice answers and ground truth. This quantitative approach, while objective, \ndoes not capture the qualitative aspects of the models' outputs, such as the coherence, \n"}, {"page": 15, "text": "safety, or clinical appropriateness of their underlying reasoning. Finally, our prompt \nengineering framework relied on static LLM-generated CoT examples and did not include \nreal-time clinician feedback or reinforcement mechanisms, which may limit safety and \nadaptability in dynamic clinical environments. \nConclusions \nThis study demonstrates that strong LLM performance on medical knowledge exams does \nnot necessarily translate to e]ective application in real-world clinical encounters. It also \npresents a critical insight for the integration of LLMs into clinical workï¬‚ows: there is no \nâ€œone-size-ï¬ts-allâ€ solution for optimizing LLM performance in clinical decision-making.  \nOur results indicate that the e]ectiveness of both the language models and prompting \nstrategies to guide them are highly dependent on the speciï¬c clinical task at hand. This \nimplies that the path toward safe and e]ective clinical AI involves the development of \ntailored approaches that pair the right model with a relevant prompting strategy to the right \ntask, rather than a search for a single, universally optimal model or prompting technique. \nThese insights can guide healthcare systems, AI developers, and regulatory bodies in \noptimizing LLM deployment for safe and e]ective use in medicine. Future research can \nexpand on this work by incorporating larger-scale and more diverse datasets to better \nreï¬‚ect real-world clinical complexity. Additionally, integrating human-in-the-loop \nreï¬nement and reinforcement learning from human feedback (RLHF) are essential for \nbuilding clinician trust and ensuring safe implementation of LLMs in healthcare.  \nMethods \nData sources: The dataset used in this study was sourced from the Merck Sharpe & \nDohme (MSD) Clinical Manual Professional Version 20. The MSD Manual is a widely \nrecognized, evidence-based medical resource authored by subject matter experts and \npeer-reviewed prior to publication. It provides comprehensive clinical information across a \nwide range of specialties and is commonly used by healthcare professionals for education \nand decision support. A total of 36 clinical cases were available in this database at the time \nthe study was conducted, each representing an individual patient case. The cases were \nextracted using web-scrapping in Python. These cases are structured to reï¬‚ect realistic \nclinical scenarios and include key components of a standard patient encounter, such as \nHPI, ROS, PE results, and, where applicable, laboratory data. \nLLMs examined: We evaluated the performance of three leading LLMs: two commercial \nmodelsâ€”ChatGPT-4o (OpenAI) and Gemini 1.5 Pro (Google)â€”and one open-source \nmodelâ€”LIama 3.3 70B (Meta). Each model was accessed through its respective API. For \nChatGPT-4o and Gemini 1.5 Pro, we used o]icial API endpoints provided by OpenAI and \n"}, {"page": 16, "text": "Google, respectively. Requests were made using Python scripts through the openai and \ngoogle.generativeai libraries. For the open-source LIama 3.1 70B model, we employed \nthe LIamaapi Python SDK, accessing the model via the LIamaAPI platform. API requests \nwere structured as JSON objects specifying the modelâ€™s name, prompt messages, and \nfunction-calling parameters.  \nPrompting LLMs: Each clinical vignette was input into the model as a single continuous \nsession to preserve contextual continuity. A new session was initialized for each case to \navoid cross-contamination of responses between vignettes. Questions involving visual \ndata, such as medical images, were excluded from analysis. Clinical decision-making \ntasks were ordered as follows: di]erential diagnosis, essential immediate steps, relevant \ndiagnostic testing, ï¬nal diagnosis, and treatment recommendation. These tasks were \npresented to the models as a sequential series of prompts, where the input to the model \nfor each clinical decision-making task included HPI, ROS, PE results, along with the ground \ntruth for all preceding tasks, to simulate the stepwise reasoning process typical of real-\nworld clinical encounters. For example, model inputs for di]erential diagnosis were HPI, \nROS, and PE results. The inputs to the next clinical task, essential immediate steps, \nincluded HPI, ROS, and PE results as well as the ground truth for di]erential diagnosis. \nEach question was tested three times (each in a new session) to account for potential \nvariability in responses. The modelâ€™s output for each trial was compared against a \npredeï¬ned set of correct answers extracted from the MSD database. Notably, not all case \nstudies included all ï¬ve question types; in such instances, the model was directed to \nproceed to the next available question type. The number of clinical vignettes for each \ncategory of questions is: 36 in essential di]erential diagnosis, 12 in essential immediate \nsteps, 21 in relevant diagnostic testing, 33 in ï¬nal diagnosis, and 32 in treatment ordering. \nPrompt engineering: Prior to applying prompt engineering, we ï¬rst constructed a training \ndataset by identifying question instances for which the modelâ€™s self-generated CoT \nreasoning produced an answer that matched the ground truth answer. Speciï¬cally, we \nprompted each LLM with the instruction â€œLetâ€™s think step by step.â€ to elicit CoT reasoning. \nFrom the resulting output, we extracted the modelâ€™s ï¬nal answer and compared it to the \ncorrect answer for each question type. If the modelâ€™s answer matched the ground truth, the \ninstance was included in the training set. Otherwise, it was assigned to the test set. For \neach question type, approximately 30% of the cases were allocated to the training set, \nwhile the remaining cases were reserved for testing. In situations where the self-generated \nCoT did not yield a su]icient number of high-quality training examples, we supplied the \ncorrect answer to the model and prompted it to generate a corresponding CoT explanation. \nThese synthetically augmented CoT examples were included in the training set until the \n"}, {"page": 17, "text": "desired sample size was achieved. Once the training set was ï¬nalized, we applied the \nstructured MedPrompt prompting techniques to evaluate performance on the test set. \nFor ð¾NN dynamic few-shot learning introduced and utilized in MedPrompt 15, we ï¬rst \ncomputed vector representations of both training and test set questions using the OpenAIâ€™s \nclient.embeddings.create() function. Three few-shot examples were then selected for \neach test question based on cosine similarity, prioritizing examples that were semantically \nsimilar to the target test question in terms of clinical context. Answer to these questions \nalready involved CoT explanation generated by the respective LLM under evaluation.  \nIn addition to the original MedPrompt framework, which involved ð¾NN dynamic few-short \nprompting 15, we also examined a variant of MedPrompt, where we replaced the ð¾NN \ndynamic few-shot prompting with random dynamic-shot prompting, where the examples \nfrom the training dataset for each test question were selected randomly for few-shot \nlearning. For ensemble shu]ling ensemble, another key component of MedPrompt, answer \nchoices were shu]led to minimize answer-order bias. Three prompt variants with shu]led \nanswer choices were generated and aggregated in an ensemble framework for robust \nevaluation.  \nPerformance evaluation: Model performance for questions with multiple answer choices \n(di]erential diagnosis, essential immediate steps, relevant diagnostic testing, treatment \nrecommendation) was calculated based on the percent overlap between the model-\ngenerated responses and the ground truth answers in the MSD database. This allowed us to \naward partial credit to LLMs when the modelâ€™s answer intersected with the correct set but \nwas not fully correct. For example, in di]erential diagnosis questions, if the ground truth \nlisted ï¬ve possible conditions (e.g., pneumonia, pulmonary embolism, asthma, heart \nfailure, and bronchitis) and the model correctly identiï¬ed three of them (e.g., pneumonia, \nasthma, and heart failure), then the performance score for that instance would be \ncalculated as the proportion of correct predictions: 3 out of 5, or 60%. Each clinical \nquestion was evaluated over three independent runs to account for potential variability in \nmodel responses. The individual question score was computed as the average accuracy \nacross these three runs. To derive the overall performance for each question type, we \ncalculated the mean accuracy across all relevant questions within that category.  \nFor ï¬nal diagnosis, where there was a single correct answer choice per case, the modelâ€™s \nanswer for each clinical case was directly compared with MSD correct answers, labeling 0 \n(incorrect) or 1(correct). The average score was calculated for the result accuracy.  \n"}, {"page": 18, "text": "All baseline prompting, prompt engineering, and performance evaluation implementations \nwere conducted within Google Colaboratory and can be accessed in Supplementary File \n2. \nStatistical methods: To evaluate the statistical signiï¬cance of performance di]erences \nbetween LLMs under di]erent prompting strategies or temperature settings, we conducted \nstatistical hypothesis testing using the paired Mann-Whitney U (Wilcoxon Signed-Rank) test \nbetween the evaluation results for both zero and default temperature. All tests were two-\nsided, and statistical signiï¬cance was based on a p-value of < 0.05. All statistical analyses \nwere performed using the tidyverse, ggsignif packages in R. \nDeclarations \nData availability \nAll data generated in this study along with the Google Colaboratory notebooks detailing all \nmodel evaluations for each LLM are provided in the supplementary materials. \nAuthor contributions \nARZ conceived the study. MC performed all model evaluations and analyses. CM and ARZ \nanalyzed the results. CM and ARZ wrote the manuscript. All authors have read and \napproved the ï¬nal manuscript. \nConï¬‚ict of interests \nThe authors declare no competing interests. \nFunding \nThis project was unfunded. \n \nReferences \n1. \nNewman-Toker DE, Nassery N, Scha]er AC, et al. Burden of serious harms from \ndiagnostic error in the USA. BMJ Qual Saf. Jan 19 2024;33(2):109-120. doi:10.1136/bmjqs-\n2021-014130 \n2. \nBerner ES, Graber ML. Overconï¬dence as a cause of diagnostic error in medicine. \nAm J Med. May 2008;121(5 Suppl):S2-23. doi:10.1016/j.amjmed.2008.01.001 \n3. \nSacco AY, Self QR, Worswick EL, et al. Patients' Perspectives of Diagnostic Error: A \nQualitative Study. J Patient Saf. Dec 01 2021;17(8):e1759-e1764. \ndoi:10.1097/PTS.0000000000000642 \n4. \nOuyang L, Wu J, Jiang X, et al. Training language models to follow instructions with \nhuman feedback. 2022. \n"}, {"page": 19, "text": "5. \nOshero] J, Teich J, Levick D, et al. Improving Outcomes with Clinical Decision \nSupport: An Implementer's Guide, Second Edition. 2012. \n6. \nConstantinou AC, Fenton N, Marsh W, Radlinski L. From complex questionnaire and \ninterviewing data to intelligent Bayesian network models for medical decision support. Artif \nIntell Med. Feb 2016;67:75-93. doi:10.1016/j.artmed.2016.01.002 \n7. \nRotmensch M, Halpern Y, Tlimat A, Horng S, Sontag D. Learning a Health Knowledge \nGraph from Electronic Medical Records. Sci Rep. Jul 20 2017;7(1):5994. \ndoi:10.1038/s41598-017-05778-z \n8. \nOpenAi, Achiam J, Adler S, et al. GPT-4 Technical Report. 2023. \n9. \nTouvron H, Lavril T, Izacard G, et al. LLaMA: Open and ELicient Foundation Language \nModels. 2023. \n10. \nAnil R, Borgeaud S, Alayrac J-B, et al. Gemini: A Family of Highly Capable Multimodal \nModels. 12/19 2023;doi:10.48550/arXiv.2312.11805 \n11. \nKung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on USMLE: \nPotential for AI-assisted medical education using large language models. PLOS Digit \nHealth. Feb 2023;2(2):e0000198. doi:10.1371/journal.pdig.0000198 \n12. \nSinghal K, Tu T, Gottweis J, et al. Toward expert-level medical question answering \nwith large language models. Nature Medicine. 01/08 2025;31:943-950. \ndoi:10.1038/s41591-024-03423-7 \n13. \nLu P, Mishra S, Xia T, et al. Learn to Explain: Multimodal Reasoning via Thought \nChains for Science Question Answering. 2022. \n14. \nBrown T, Mann B, Ryder N, et al. Language models are few-shot learners. Advances \nin neural information processing systems. 2020;33:1877-1901.  \n15. \nNori H, Lee YT, Zhang S, et al. Can Generalist Foundation Models Outcompete \nSpecial-Purpose Tuning? Case Study in Medicine. arXiv e-prints. 2023:arXiv:2311.16452. \ndoi:10.48550/arXiv.2311.16452 \n16. \nTsaneva-Atanasova K, Pederzanil G, Laviola M. Decoding uncertainty for clinical \ndecision-making. Philosophical Transactions A. 03/13 \n2025;383doi:10.1098/rsta.2024.0207 \n17. \nHelou MA, DiazGranados D, Ryan MS, Cyrus JW. Uncertainty in Decision Making in \nMedicine: A Scoping Review and Thematic Analysis of Conceptual Models. Acad Med. Jan \n2020;95(1):157-165. doi:10.1097/ACM.0000000000002902 \n18. \nNorman GR, Monteiro SD, Sherbino J, Ilgen JS, Schmidt HG, Mamede S. The Causes \nof Errors in Clinical Reasoning: Cognitive Biases, Knowledge Deï¬cits, and Dual Process \nThinking. Acad Med. Jan 2017;92(1):23-30. doi:10.1097/ACM.0000000000001421 \n19. \nCroskerry P. A universal model of diagnostic reasoning. Acad Med. Aug \n2009;84(8):1022-8. doi:10.1097/ACM.0b013e3181ace703 \n20. \nManuals M. Case Studies. Merck Manual, Professional Version. 2024. \n21. \nKim J, Podlasek A, Shidara K, Liu F, Alaa A, Bernardo D. Limitations of large language \nmodels in clinical problem-solving arising from inï¬‚exible reasoning. arXiv preprint \narXiv:250204381. 2025; \n22. \nStechly K, Valmeekam K, Kambhampati S. Chain of thoughtlessness? an analysis of \ncot in planning. Advances in Neural Information Processing Systems. 2024;37:29106-\n29141.  \n"}, {"page": 20, "text": "23. \nShao J, Cheng Y. CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A \nTheory Perspective. arXiv preprint arXiv:250602878. 2025; \n \n \n"}]}