{"doc_id": "arxiv:2511.05696", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.05696.pdf", "meta": {"doc_id": "arxiv:2511.05696", "source": "arxiv", "arxiv_id": "2511.05696", "title": "AI-assisted workflow enables rapid, high-fidelity breast cancer clinical trial eligibility prescreening", "authors": ["Jacob T. Rosenthal", "Emma Hahesy", "Sulov Chalise", "Menglei Zhu", "Mert R. Sabuncu", "Lior Z. Braunstein", "Anyi Li"], "published": "2025-11-07T20:27:05Z", "updated": "2025-11-07T20:27:05Z", "summary": "Clinical trials play an important role in cancer care and research, yet participation rates remain low. We developed MSK-MATCH (Memorial Sloan Kettering Multi-Agent Trial Coordination Hub), an AI system for automated eligibility screening from clinical text. MSK-MATCH integrates a large language model with a curated oncology trial knowledge base and retrieval-augmented architecture providing explanations for all AI predictions grounded in source text. In a retrospective dataset of 88,518 clinical documents from 731 patients across six breast cancer trials, MSK-MATCH automatically resolved 61.9% of cases and triaged 38.1% for human review. This AI-assisted workflow achieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity for patient-level eligibility classification, matching or exceeding performance of the human-only and AI-only comparisons. For the triaged cases requiring manual review, prepopulating eligibility screens with AI-generated explanations reduced screening time from 20 minutes to 43 seconds at an average cost of $0.96 per patient-trial pair.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.05696v1", "url_pdf": "https://arxiv.org/pdf/2511.05696.pdf", "meta_path": "data/raw/arxiv/meta/2511.05696.json", "sha256": "d9116c097cc1bcbbf79891647ecc0f8a579250ad89539342bb3c9ec43a4a8368", "status": "ok", "fetched_at": "2026-02-18T02:28:21.922794+00:00"}, "pages": [{"page": 1, "text": " \n1 \nAI-assisted workflow enables rapid, high-\nfidelity breast cancer clinical trial eligibility \nprescreening \n \nJacob T. Rosenthal1, Emma Hahesy2, Sulov Chalise3, Menglei Zhu3, Mert R. \nSabuncu4,5*, Lior Z. Braunstein2*†, Anyi Li6*† \n \n \n1. Weill Cornell Medicine/Rockefeller University/Memorial Sloan Kettering Cancer \nCenter Tri-Institutional MD-PhD Program \n2. Department of Radiation Oncology, Memorial Sloan Kettering Cancer Center \n3. Department of Pathology and Laboratory Medicine, Memorial Sloan Kettering \nCancer Center \n4. School of Electrical and Computer Engineering, Cornell University and Cornell \nTech \n5. Department of Radiology, Weill Cornell Medicine \n6. Department of Medical Physics, Memorial Sloan Kettering Cancer Center \n \n \n*Equal contribution \n \n†Correspondence to: \nLior Braunstein, MD \nDepartment of Radiation Oncology \nMemorial Sloan Kettering Cancer Center \nBraunstL@mskcc.org \n \nAnyi Li, PhD \nDepartment of Medical Physics \nMemorial Sloan Kettering Cancer Center \nLiA5@mskcc.org  \n \n \n \nWord count (main text): 4509 \n \n \n \n"}, {"page": 2, "text": " \n2 \nAbstract \nClinical trials play an important role in cancer care and research, yet participation rates \nremain low. We developed MSK-MATCH (Memorial Sloan Kettering Multi-Agent Trial \nCoordination Hub), an AI system for automated eligibility screening from clinical text. \nMSK-MATCH integrates a large language model with a curated oncology trial \nknowledge base and retrieval-augmented architecture providing explanations for all AI \npredictions grounded in source text. In a retrospective dataset of 88,518 clinical \ndocuments from 731 patients across six breast cancer trials, MSK-MATCH automatically \nresolved 61.9% of cases and triaged 38.1% for human review. This AI-assisted workflow \nachieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity for patient-level \neligibility classification, matching or exceeding performance of the human-only and AI-\nonly comparisons. For the triaged cases requiring manual review, prepopulating \neligibility screens with AI-generated explanations reduced screening time from 20 \nminutes to 43 seconds at an average cost of $0.96 per patient-trial pair. \n \n \n \n \n \n"}, {"page": 3, "text": " \n3 \nIntroduction \nClinical trials are central to the advancement of cancer care and research, yet only 7–\n8% of eligible patients participate nationally1. This is despite studies showing that 44% \nof cancer patients have trials available to them at their treating institutions2, and more \nthan half of adults with cancer are likely to participate in a clinical trial when offered3,4. A \nkey barrier is the complexity of eligibility determination, requiring information retrieval \nfrom longitudinal clinical documents spanning notes, imaging and pathology reports, \nand biomarkers. \n \nEligibility screening today is typically a labor-intensive manual process: screening a \nbreast cancer patient for a single trial routinely takes longer than 30 minutes, requiring \nover an hour in more than 25% of cases, with overall cost estimates upwards of $330 \nper enrolled patient5.  \n \nWith systematic trial eligibility prescreening infeasible, current practice is mainly reliant \non individual oncologists to stay informed about relevant trials and to advise patients of \ntheir options for clinical trial participation6. At large academic medical centers, which \nmay have more than 1000 clinical trials recruiting at any given time, clinicians are \nunlikely to be able to stay up to date on all of them. Consequently, most oncology \npatients are never systematically screened for trials – many potential trial matches go \nundiscovered, patients are not offered the option of trial participation, and accrual rates \nremain low. \n \nThe emergence in recent years of large language models (LLMs), a type of AI model \nspecialized for processing and generating textual data, has provided a powerful new \ntool for processing unstructured text including clinical notes and reports. Rather than \nbeing trained for a single task, current LLMs have general-purpose capabilities and \nshow promise in many applications across cancer research and oncology10–12. Beyond \nstandalone use, multiple LLMs can be orchestrated into multi-agent systems, with each \nagent performing a specialized role and communicating with others to perform complex, \nknowledge-intensive biomedical tasks13–17.  \n \nPrevious approaches for applying LLMs to automate clinical trial matching have been \nlargely limited by the reliance on small datasets that are not representative of the \ncomplexity of longitudinal documentation in real-world data. For example, the 2018 n2c2 \ncohort selection challenge, with a maximum of 5 notes per patient18, has been widely \nused for the development and evaluation of LLM-based eligibility screening systems, \ndespite its limited representativeness of the scale of data seen in real clinical practice19–\n22. The performance of the Synapsis LLM for clinical trial eligibility assessment was \nevaluated on a set of 50 melanoma patients23. Other studies have relied on synthetic \ndata, such as paragraph-length patient vignettes written by medical professionals24–28, \nLLM-generated summaries of patient records29, or even fully synthetic EHR data \ncreated by an LLM30. \n \nA second category of existing work adopts a two-step approach, using LLMs to extract \nprespecified sets of structured attributes, either biomarker based31,32 or curated for trials \n"}, {"page": 4, "text": " \n4 \nof interest23,33, before then applying rules-based logic to make a final eligibility \ndetermination. While this intermediate step could be automated by using LLMs to \nautomatically extract relevant entities for trials29,34, it is overall less scalable across \ndiverse protocols than an end-to-end approach. \n \nSome works have investigated using retrieval-augmentation to incorporate real-world \nclinical notes. For instance, the RECTIFIER tool showed promising performance when \nused for adjudicating 13 criteria in a single heart failure trial35,36, but has not yet been \nextended to the oncology domain, where clinical trials are the most complex37. \nOncoLLM relies on an LLM fine-tuned on notes from a single institution and was \nevaluated on 98 patients, potentially limiting generalizability and adoption38. \n \nBuilding upon these previous efforts, the objective of our present work is to develop an \nAI-assisted clinical trial eligibility prescreening system which can be integrated with \nexisting clinical workflows in a high-volume breast radiation oncology service at a large \nacademic medical center to enable efficient, reliable, and systematic prescreening of \nnew patients for relevant trials. \nMethods \nCohort identiﬁcation \nWe constructed a retrospective cohort of all patients referred for enrollment in an \ninterventional clinical trial by the breast cancer radiation oncology service at Memorial \nSloan Kettering Cancer Center (MSK) between 2016 and 2024. Once referred by a \nphysician for potential enrollment in a particular trial, patients are then screened by \ntrained staff at the hospital’s centralized clinical trials office, who perform an in-depth \nreview to determine the patient’s eligibility status for the specific trial for which they were \nreferred. These patient-level eligibility labels, representing the gold standard for trial \neligibility determination in current standard of care at our institution, were used as \nground truth. Criterion-level labels were not available, so for patients ultimately \ndetermined to be ineligible, we do not know the specific reason they were disqualified. \nAfter removing trials with fewer than 30 patients screened and fewer than two negative \nlabels (i.e., patients determined to be ineligible after review), we were left with 731 \npatients spanning 6 trials (Table 1).  \n \nWe obtained complete EHR data for each patient, including all available reports, notes, \nand other documents from all services. Documents created on or after the eligibility \ndetermination date were excluded to avoid information leakage. This yielded an average \nof 121 documents per patient, corresponding to 126,346 tokens as processed by the \nGPT-4o tokenizer39 (Table 2). \n \nThis non-interventional research was approved by the MSK Institutional Review Board \nunder protocol 16-114 and a waiver of informed consent was granted. \n \n"}, {"page": 5, "text": " \n5 \nCross-trial negative labels \nThe data-generating process selected for patients presumed to be eligible, as patients \nwere only referred for enrollment if the clinician believed they were very likely to qualify. \nConsequently, the original dataset was heavily skewed toward cases with a high pre-\ntest probability of being eligible, and the negative labels it did contain likely represent \nthe most difficult cases selected from a general population of largely ineligible patients, \nwhere the referring clinician was incorrect in their initial assessment. In reality, a \nclinician likely considers each new patient for all six of the possible trials, quickly ruling \nthem out for the majority before identifying the trial(s) where they are likely to be eligible. \n \nTo mitigate class imbalance and capture a broader range of ineligible cases that better \nreflect real clinical decision-making, we therefore augmented our dataset with additional \nnegative labels by leveraging mutually exclusive trials. Trials were classified as having \nmetastatic disease as either an inclusion criterion (18-486, 22-259) or an exclusion \ncriterion (16-323, 19-300, 19-410, 21-283), with patients eligible for a trial in one group \nbeing ineligible for the trials in the other group by definition. These cross-trial exclusions \nprovided additional negative labels for the patients in our cohort at the same standard of \nevidence. After adding 1564 cross-trial negative labels to the original dataset of 731 \npatients, we obtained a total of 2295 {patient, trial} pairs (Table 1). The dataset was split \ninto 50% training and 50% testing sets according to stratified random sampling to \nmaintain label balance. \nProtocol data \nThe 6 clinical trials comprised a total of 73 inclusion criteria and 62 exclusion criteria \n(Table 3). The text for each criterion was taken verbatim from trial documentation, \nwithout any further processing.  \n \nIn initial experimentation, we found that certain criteria were consistently difficult for the \nAI system to determine correctly. Some were written in such a way that they were \nalways satisfied regardless of the patient, yet the AI model often made the incorrect \ndetermination (e.g., “bilateral breast cancer is permitted”). Others were judged by \nclinicians to be outside the scope of what could be automatically determined by AI and \ninstead required clinical judgment by an oncologist on a case-by-case basis (e.g., \nwhether a patient’s disease is “technically amenable” to a particular therapy). This is \nreflective of the fact that while automated decision-making is desirable in many cases, \nthere are certain instances where human expertise and input is required for high-stakes \ndecisions. We flagged these criteria as “vacuous” and “requires human review” \nrespectively, directing the AI to skip them and instead provide a predetermined answer \n(“met” or “unable to determine”). In total, we identified six vacuous criteria and 15 \ncriteria requiring human review (Table 3, Table S1, Table S2). \nAI system design \nOur objective was to design an AI system for automated trial eligibility determination that \nis effective and practical for use in real-world clinical workflows. To that end, the system \nmust satisfy several criteria. First, predictions must be explainable and auditable to \n"}, {"page": 6, "text": " \n6 \nallow clinicians to verify the correctness of AI outputs, building trust and ensuring \ntransparency. Secondly, the system should use general-purpose, commodity LLMs. \nWith the rapid pace of technological advancement and the regular release of new \nmodels, we consider it essential to avoid being locked into a single model or provider, \nand to minimize the costs and complexity associated with training or fine-tuning LLMs \non our data. Finally, the system must have the capacity to work with the scale and \nheterogeneity of real-world EHR data, with up to hundreds of notes per patient and \ndozens of criteria per trial, without requiring any manual curation. \n \nThe MSK-MATCH system comprises multiple LLM agents with distinct roles (Figure 1). \nWe begin by creating expert agents emulating the composition of a multidisciplinary \nbreast cancer care team, including a pathologist, radiologist, surgical oncologist, \nmedical oncologist, radiation oncologist, and general medicine practitioner. Each agent \nis given access to a vector store containing clinical notes relevant to their specialty. \nNotes are assigned by matching their type to a predefined set of keywords and using \nprompts tailored to each specialty’s domain. The generalist agent is assigned \ndocuments that do not match keywords for any of the other specialist agents. The set of \nexpert agents is dynamically created for each patient based on their available \ndocuments, such that, for example, if a particular patient has no surgical notes, there \nwill be no surgical oncology agent. In comparison to a single-expert design, the multi-\nexpert framework showed comparable classification performance (Figure S1), but \noffered multiple domain-specific explanations which may improve overall interpretability. \nNotes from nursing, rehabilitation, and survivorship categories were excluded. Next, we \ncreate a trial coordinator agent responsible for routing each eligibility criterion to the \nappropriate expert agent(s) for evaluation. Finally, a principal investigator agent reviews \nthe experts’ explanations, resolves any discrepancies, and makes the final \ndetermination for each criterion (met, not met, or unable to determine). While this \nprocess could in principle be repeated in multiple rounds of deliberation, we found that a \nsingle pass was sufficient to achieve strong performance and did not explore further \niterations. All prompts used are made available in the accompanying code release. \n \nAfter all criteria have been assessed, a rules-based approach is applied to make the \nfinal patient-level eligibility determination. If any inclusion criteria are not met or any \nexclusion criteria are met, then the patient is classified as not eligible; otherwise, the \npatient is potentially eligible. Criteria labeled as “unable to determine” do not factor into \nthis. \n \nThe workflow is defined as a graph structure using the LangGraph framework40. All \nagents used GPT-4o, version 2024-08-0639. The temperature was set to 0 to reduce \ncreativity and encourage focused responses41. To protect patient data, all computation \nwas conducted on an institutionally approved, HIPAA-compliant cloud computing \ninstance on Microsoft Azure. \nRetrieval-augmented generation \nA known shortcoming of LLMs is their tendency to generate responses that appear \nplausible yet contain references to nonexistent facts or sources: this phenomenon is \n"}, {"page": 7, "text": " \n7 \nknown as hallucination42–45. Retrieval-augmented generation (RAG) is a technique \ncommonly used to mitigate this problem by adding an additional step to LLM generation \nin which they first retrieve relevant snippets from a provided corpus of documents, \nwhich are then used to ground responses46,47.  \n \nRetrieval augmentation was used to ground AI responses, allowing the LLM agents to \nsearch through a patient-specific vector store of clinical documents to identify relevant \nsnippets before answering a query48. This facilitates transparency and auditability, \nimproving response quality and increasing trust among users while reducing the risk of \nhallucination46,47. The notes are processed into a vector database as follows. First, each \nnote is tokenized and split into chunks of 500 tokens, with an overlap of 50 tokens. \nNext, each chunk is passed through the Text-Embedding-3-Large model (OpenAI) to \nproduce an embedding vector designed to capture the semantic meaning of the text \nwithin the chunk. Retrieval augmentation is then used to allow agents to access \ndocuments from the vector store during generation. When the LLM receives a query, the \nprompt is first converted into an embedding vector. A similarity search is then performed \nto retrieve the k documents from the vector store whose embeddings are closest to the \nquery embedding. In our implementation, we set k = 10 to select the top 10 most similar \ndocuments. These retrieved documents, containing the most relevant information, are \nthen incorporated directly into the prompt, allowing the LLM to reference them when \ngenerating a response. \nError analysis and knowledge base construction \nIn initial experiments, we observed overall poor performance. Experimentation \ndemonstrated that individual errors could be rectified by modifying the system prompt, \nproviding the LLMs with precise instructions to make the appropriate determination. \nHowever, we realized that hand-crafted prompting would not be a scalable solution. We \ntherefore developed a human-in-the-loop framework to systematically analyze errors \nand solicit domain-specific feedback.  \n \nAI inference was run for the entire training set, and each misclassified case was \nindividually reviewed by a clinical research coordinator (CRC) with experience \nscreening patients for breast radiation oncology trials, who provided feedback in the \nform of clarification or instruction. The CRC was instructed to provide generally \napplicable feedback, rather than patient-specific. We collected all feedback in an \noncology trial knowledge base (KB) (Figure 1b). During inference, the entire KB is \nconcatenated together and injected directly into the prompts for each expert agent. \nAI-assisted workﬂow \nWe sought to develop a workflow leveraging the MSK-MATCH automated predictions to \nimprove the efficiency of human review without compromising performance. During \ndevelopment, we observed that 1) AI predictions tend to have lower sensitivity than \nspecificity, meaning that there are more false negatives than false positives; and 2) a \npredicted negative can be quickly confirmed by reviewing only the subset of criteria \nwhich have been flagged as disqualifying, whereas confirming a predicted positive \nrequires checking every single criterion to verify eligibility.  \n"}, {"page": 8, "text": " \n8 \n \nTo take advantage of these asymmetries, we created an AI-assisted workflow which \nconducts an automated first pass eligibility prediction and then triages a subset of the \npredicted negative cases for secondary human review. By sorting the predicted \nnegatives according to the total number of disqualifying criteria (i.e., inclusion criteria \nnot met, and exclusion criteria met), we are able to identify cases with only one or two \ndisqualifying criteria. Limiting the review to these “almost matches” enables efficient use \nof humans’ time on cases where their review has a higher likelihood of quickly \nidentifying errors that change the final eligibility prediction. Furthermore, with the AI \nsystem having already extracted relevant information and prepopulated criterion-level \nexplanations for each case, the secondary human review can be conducted faster.  \nExperimental design \nWe conducted an experiment to compare three different workflows: a human only \nmanual workflow, representing the current state of the art in clinical practice; a fully \nautomated AI-only workflow; and a human-AI collaborative workflow in which the AI \nsystem prepopulates eligibility determinations and triages a subset for review by a \nhuman (Figure 2a).  \n \nFirst, AI inference was run on all n=1160 {patient, trial} pairs in the test set. Next, to \nmeasure the performance of the human-only workflow, we selected a random sample \nfrom the test set, stratified by protocol and class-balanced to create a sample of 5 \npositives and 5 negatives per protocol, for a total of 60. This subsampling was \nnecessary due to the time-intensive nature of the manual workflow, which prohibited a \nmanual evaluation of the entire test set. Samples were drawn from the original labels \nwhen possible and supplemented with cross-trial negatives only when there were fewer \nthan 5 negative cases for a trial in the original dataset. The CRC was then asked to \nconduct a “gold-standard” exhaustive review for each of these cases, replicating the \nprocess they would use in the ordinary practice of their job, and free to use any \nresources apart from MSK-MATCH. The time duration was measured and recorded.   \n \nIn the human-AI collaboration experimental arm, cases in the human-only baseline were \nexcluded to ensure that none of the cases had been previously seen by the human \ncurator, thereby avoiding any effects due to memorization or recall. This resulted in a \ntotal of n=1100 {patient, trial} pairs remaining. AI predictions were then sorted by the \nnumber of disqualifying criteria. The CRC was then asked to review all predicted \nineligible cases with 1 or 2 disqualifying criteria. The time duration was measured and \nrecorded. Performance statistics are calculated using the full set of 1100 predictions, \nincluding those which were triaged and did not receive human review, to capture the \noverall performance of the workflow. \nUser interface \nWe developed a custom user interface to support both the human-in-the-loop error \nanalysis and the human-AI collaborative screening workflow (Figure S2). The interface \nallows users to select a patient and protocol, then displays the patient-level result as \nwell as individual criterion-level explanations. AI-generated explanations can be further \n"}, {"page": 9, "text": " \n9 \ninspected to view the verbatim snippets of text retrieved from the EHR data and used in \nthe generation process. Dialogs for entering feedback allow for on-the-fly error analysis \nand correction. The web application is hosted behind the hospital’s virtual private \nnetwork (VPN) and password protected to ensure data security. The application runs on \nKubernetes infrastructure and uses a Redis database on the backend. \nStatistical analysis \nThe primary outcome metrics were accuracy, sensitivity, and specificity. The Wilson \nscore interval was used to compute confidence intervals for each metric. Due to our \nexperimental setup, the experimental arms did not have an equal baseline prevalence \nof eligibility. To directly compare accuracy between groups with different prevalences, \nwe created subsets of matched cases (Figure S3, Figure S4). \nResults \nError analysis  \nAnalysis of incorrect AI-only predictions identified four common failure modes: 1) gaps \nin domain knowledge, where the AI makes factual mistakes about breast cancer; 2) \nlogical mistakes, where the AI misinterprets information or makes incorrect deductions; \n3) incorrect handling of missing information, where the AI makes inappropriate \ninferences when elements are missing from a patient’s EHR data; and 4) irrelevant \ncriterion misclassification, where the model incorrectly applies or misinterprets a \ncriterion that is only relevant in a specific context. Examples of each category are given \nin Table 4. Importantly, we did not observe any instances of hallucination, i.e. inclusion \nof factually incorrect information which is confabulated or otherwise not grounded in the \nunderlying source documents. We conducted a single pass through the training set, \ncollecting a total of 89 feedback elements capturing all error modes observed in the \ntraining set, forming a domain-specific oncology trial KB. \nDomain knowledge injection \nWe conducted an ablation study to assess the effect of including the domain knowledge \nin the KB, finding that injecting the KB directly into the prompts for each of the specialist \nagents increased accuracy from 64.1% (95% CI: 61.3-66.8%) to 87.5% (95% CI: 85.5-\n89.3%), increased sensitivity from 21.6% (95% CI: 17.6-26.2%) to 62.8% (95% CI: 57.6-\n67.7%), and increased specificity from 82.7% (95% CI: 79.9-85.1%) to 98.3% (95% CI: \n97.1-99.0%) (Figure 3a). Addition of the KB into the prompt increased the number of \ntokens per LLM call, resulting in an increase in average cost per run of $0.58 (from \n$0.38 to $0.96) (Figure 3b). The KB only contained information obtained through review \nof errors in the training dataset, while performance was measured on the held-out \ntesting dataset. \nHuman-AI collaborative workﬂow \nThe human-AI collaborative workflow met or exceeded overall performance of the other \nworkflows across all metrics, achieving accuracy of 98.6% (95% CI: 97.8-99.2%), \n"}, {"page": 10, "text": " \n10 \nsensitivity of 98.4% (95% CI: 96.4-99.3%), and specificity of 98.7% (95% CI: 97.7-\n99.3%) (Figure 2b). Full results are shown in Figure S5.  \n \nApplying a cutoff of two disqualifying criteria allowed us to capture nearly all the false \nnegatives, while only requiring review of 38.1% of the AI predictions (Figure S6). The AI-\nassisted human workflow required an average of only 43 seconds to make a final \neligibility determination, a 96.5% reduction from the manual baseline which required an \naverage of 20 minutes and 25 seconds per case (Figure 2c). Confirmation of predicted \npositives, however, requires a full assessment of all criteria to identify any potential \nerrors. Furthermore, the AI predictions have a higher positive predictive value than \nnegative predictive value (94.04% vs. 85.84% respectively; Figure S5b) meaning that \nreviewing the predicted negatives is likely to be more fruitful in finding mistakes. Among \nthe subset of cases that the human reviewed, the final classification performance was \nnearly perfect, with only a single false positive in 419 cases (Figure S5d). \nDiscussion \nMSK-MATCH demonstrates how a retrieval-grounded AI system can operationalize \nhuman-AI collaboration for high-stakes clinical decision-making. Applied here to breast \ncancer trial eligibility prescreening, the system achieves clinical-grade accuracy while \ndramatically reducing manual human effort and cost. We developed and evaluated \nMSK-MATCH using a large retrospective dataset of more than 88,000 clinical \ndocuments from 731 breast cancer patients across 6 interventional breast cancer trials. \nIn comparison to related works, our evaluation is conducted using a dataset which more \nfaithfully captures the challenges of a busy clinical service at a large academic cancer \ncenter, with full longitudinal medical records, no synthetic data, and multiple clinical \ntrials (Table S3). \n \nBecause eligibility determinations produced by our screening system are intended to be \nused as inputs for downstream steps in the clinical workflow, it is important for real-\nworld utility that the performance is sufficiently good to not overwhelm the system with \nfalse positives. The positive predictive value (PPV) of our human-AI collaborative \nworkflow, as measured on the test set, is not significantly different from the PPV of the \ncurrent state-of-the-art as measured by the distribution of labels in our original dataset \n(96.9% vs. 95.2%, two-tailed normal test for proportions, P=0.196). Therefore, the use \nof the AI-assisted system is not expected to result in substantially more false positives \nthan current practice, further highlighting its readiness for integration with existing \nworkflows in real-world clinical use. With an average cost of less than one dollar to \nevaluate a patient for a single trial, implementation of broad prescreening programs for \ntrial eligibility is feasible, and there is opportunity for future work to investigate reducing \ncosts further. \n \nThe paradigm of human-AI collaboration that first uses AI to autonomously and rapidly \nproduce predictions and explanations and then incorporates human experts to verify \nand refine these outputs has shown success in other medical applications such as \ngenerating radiology reports49, giving feedback to surgical trainees50, and responding to \n"}, {"page": 11, "text": " \n11 \npatient messages51. In addition to boosting predictive accuracy, such approaches also \nhelp build trust among users, smoothing the path toward widespread adoption52. \nHowever, there have been concerns that reliance on humans to review LLM-generated \ntext could result in added cognitive burden due to the tedious nature of the work, \npossibly inducing cognitive biases (e.g. automation bias)53. To the contrary, we showed \nthat our proposed workflow can largely automate what is today a highly labor-intensive \nand tedious manual process, with the net result of decreased cognitive load and \nincreased efficiency. Future work to further optimize workflows and user interfaces may \nbe able to further improve human-AI collaboration. \n \nMSK-MATCH is designed to incorporate multiple layers of explainability and \ntransparency with increasing levels of granularity, thereby facilitating the verification of \nAI outputs. At the highest level, the final eligibility determination is highlighted along with \nthe counts of inclusion and exclusion criteria by status. Eligibility status can then be \nbroken down by criterion-level labels, which is especially helpful in enabling users to \nquickly identify which specific criteria disqualify a patient from a given trial. For each \ncriterion, integrative explanations synthesize multiple sources of evidence to provide a \nnarrative justification for the AI model’s determination with references to source \ndocuments as evidence. Finally, citations in explanations can be traced back to \nverbatim snippets of text from original source documents in the medical record. \nTogether, these mechanisms help build trust with clinical users while reducing the \noccurrence of AI model hallucinations or factual errors. This trust translates into practical \ngains, with the CRC taking less than one minute on average to review AI predictions in \nour experiment, suggesting that they are reviewing only a subset of criteria for \ncorrectness and trusting the majority of the AI outputs – leading to substantial efficiency \ngains.  \n \nHow best to leverage general-purpose LLMs in highly specialized medical domains \nremains an active area of inquiry. Previous works have built domain-specific models by \nconducting additional fine-tuning or retraining entirely from scratch with a narrowly \ncurated dataset of clinically relevant text, such as EHR data38,54–58; however, such \ndomain adaptation can be costly and logistically difficult. Other work has cast doubt on \nthe value of specialized models, positing that general-purpose LLMs can achieve \ncomparable performance even on narrow medical tasks59,60. Our work takes a middle \npath, using a commodity LLM while imbuing it with specialized domain knowledge \nthrough its prompt, achieving strong performance in complex information extraction and \nreasoning without the need to invest in model training or fine-tuning. This approach is \npragmatic, allowing the base LLM to be swapped for newer versions as vendors release \nthem. Furthermore, the KB can continue to be dynamically updated while the system is \ndeployed in clinical practice, allowing the AI system to continually improve and adapt in \nresponse to user feedback61. In summary, we emphasize the importance of \nincorporating specialized domain knowledge to unlock the full potential of generalist AI \nmodels for oncology applications, noting that this can be done through prompting \nstrategies without requiring fine-tuning. \n \n"}, {"page": 12, "text": " \n12 \nOur study does have limitations. Firstly, while our dataset of real longitudinal patient \nrecords and gold-standard eligibility labels allows us to comprehensively evaluate our \nproposed workflow in scenarios similar to how it would be used in real clinical practice, it \nstill represents a single service at a single academic center and may not capture \ndifferences in population and practice patterns in other cancer types, institutions, or \nregions. Secondly, we did not systematically evaluate the performance of different \nLLMs, deciding to instead focus on human-AI collaboration and clinical workflow \nintegration. Further work should assess how changing the base LLM from GPT-4o to \nother models, either from commercial vendors or local LLMs from the open-source \ncommunity, could affect performance and cost. Additionally, our method of directly \ninjecting the entire KB into the LLM prompts, while effective, did increase inference \ncosts. A targeted approach to retrieve relevant information from the KB could preserve \nperformance gains while reducing prompt lengths, improving scalability and efficiency. \nConclusion \nWe developed a workflow that effectively leverages AI for automated first-pass eligibility \nscreening for breast cancer clinical trials, prepopulating explanations and identifying \nrelevant documents while triaging only a subset of cases for secondary human review. \nBy reducing eligibility prescreening as a barrier to trial entry, more patients can be \noffered the option of clinical trial participation, increasing access to experimental \ntherapies and accelerating the pace of clinical oncology research to advance the \nstandard of care for future patients. MSK-MATCH demonstrates a pragmatic and \neffective approach to harnessing AI in clinical workflows which could serve as a \nblueprint for other applications beyond clinical trial eligibility screening. \nAcknowledgements \nJ.T.R. was supported by a Medical Scientist Training Program grant from the National \nInstitute of General Medical Sciences of the National Institutes of Health under award \nnumber T32GM152349 to the Weill Cornell/Rockefeller/Sloan Kettering Tri-Institutional \nMD-PhD Program. This study was supported in part by the Achar Meyohas Family and \nNIH/NCI Cancer Center Support Grant No. P30CA008748. \nAuthor contributions \nJ.T.R. contributed project conceptualization, methods and experimental design, \nimplementation, and analysis of results. E.H. reviewed AI predictions and contributed to \nexperimental design. S.C. and M.Z. contributed to methods and experimental design \nand analysis of results. M.R.S, L.Z.B, and A.L. contributed to project conceptualization, \nmethods and experimental design, analysis of results, and project oversight. All authors \ncontributed to writing and review of the manuscript. \n"}, {"page": 13, "text": " \n13 \nData availability \nThe patient-level longitudinal medical record data cannot be shared publicly due to \npatient privacy protections. Eligibility criteria for the protocols used in this study are \npublicly available at https://github.com/msk-mph/msk-match. \nCode availability \nCode to reproduce the methods is publicly available at https://github.com/msk-\nmph/msk-match. \n \n \n \n \n"}, {"page": 14, "text": " \n14 \nFigures \n \n \nFigure 1. Schematic diagram of the MSK-MATCH system and human-in-the-loop \nknowledge base curation. a) Clinical documents from EHR are categorized by domain, \nthen split into chunks and embedded to create vector stores (solid blue arrow). Each \ntrial eligibility criterion is sent through the multi-expert workflow (solid black arrows). \nExpert agents have access to the vector store of documents from their specialty for \nretrieval-augmented generation (dashed blue arrows). After all criteria have been \nassessed, a final report with a patient-level eligibility determination and explanations for \neach criterion is produced (dashed black arrow). b) The AI system was evaluated on all \n{patient, trial} pairs in the training dataset. Every incorrect prediction was manually \nreviewed by a clinical research coordinator, and feedback was given in the form of \ninstruction for the LLMs on how to avoid making the same error in the future. All \nfeedback was collected into an oncology trial knowledge base. When evaluating on the \ntest dataset, the knowledge base is injected directly into the prompts of the expert \nagents to guide their outputs (green arrows). Figure created with icons from \nBiorender.com.  \n"}, {"page": 15, "text": " \n15 \n \nFigure 2. Experimental design and overall results. a) Schematic of the experimental \ndesign. All {patient, trial} pairs are evaluated by the MSK-MATCH AI system (“AI alone”, \nn=1160). All cases are also assigned to human review, either with the AI-assisted triage \nand workflow (“Human + AI”, n=1100) or with the unassisted manual workflow (“Human \nalone”, n=60). b) Comparison of performance metrics on the test set, by experimental \ngroup. c) Comparison of the time duration required to complete eligibility screening for a \nsingle {patient, trial} pair, with and without AI-assistance. \n \n \nFigure 3. Effect of including the oncology trial knowledge base (KB). a) Comparison of \nperformance metrics on the test set, with and without inclusion of the KB. b) \nComparison of cost per {patient, trial} pair, with and without inclusion of the KB. \n \n \n \n \n \n \n"}, {"page": 16, "text": " \n16 \nTables \nTable 1. Dataset summary.  \nProtocol \nEligible \nNot Eligible \n(Original) \nNot Eligible \n(Cross-Trial) \nTotal \n16-323 \n387 \n13 \n86 \n486 \n18-486 \n56 \n5 \n610 \n671 \n19-300 \n167 \n5 \n86 \n258 \n19-410 \n29 \n3 \n86 \n118 \n21-283 \n27 \n6 \n86 \n119 \n22-259 \n30 \n3 \n610 \n643 \nTotal \n696 \n35 \n1564 \n2295 \n \nTable 2. Summary statistics of clinical documents. Token count is calculated using the \nGPT-4o tokenizer39. \nStatistic \nDocuments \nTokens \nMean \n121.09 \n126345.88 \nTotal \n88518 \n92358837 \n \nTable 3. Details of protocols included in this study. Inclusion and exclusion criteria are \ndrawn directly from clinical trial documentation. A subset of criteria is labeled as \n“vacuous” or “requires human review” based on manual inspection (see Table S1 and \nTable S2). NCT ID: National Clinical Trial Identifier.  \nProtocol NCT ID \nInclusion \nCriteria \nExclusion \nCriteria \nTotal \nVacuous Requires \nHuman \nReview \n16-323 \nNCT02603341 \n12 \n5 \n17 \n3 \n2 \n18-486 \nNCT03808337 \n12 \n6 \n18 \n0 \n2 \n19-300 \nNCT04084730 \n9 \n15 \n24 \n0 \n2 \n19-410 \nNCT03488693 \n16 \n10 \n26 \n2 \n4 \n21-283 \nNCT04852887 \n17 \n22 \n39 \n1 \n3 \n22-259 \nNCT05534438 \n7 \n4 \n11 \n0 \n2 \nTotal \n \n73 \n62 \n135 \n6 \n15 \n \n"}, {"page": 17, "text": " \n17 \nTable 4. Representative examples of AI error modes. \nError Type \nCriterion \nAI Output: Explanation and Determination \nError Explanation \nGaps in \nDomain \nKnowledge \nInclusion: Metastatic \ndisease detected on \nimaging and \nhistologically confirmed. \n“The report from the FDG PET/CT scan on [date] \nindicates a hypermetabolic left breast neoplasm \nmetastatic to the left axilla, which is consistent with \nmetastatic disease.” \n \nDetermination: Met \nSpread to axillary nodes, although \ndescribed as “metastatic” in the imaging \nreports, does not count as “metastatic \ndisease” for the purposes of this \nexclusion criterion. \nExclusion: History of \ncosmetic or \nreconstructive breast \nsurgery. \n“The patient has a history of breast surgery, specifically \na partial mastectomy, as indicated in the follow-up \nreport. This is considered a form of reconstructive \nbreast surgery. Therefore, the patient has a history of \ncosmetic or reconstructive breast surgery.” \n \nDetermination: Met \nMastectomies and lumpectomies are not \nconsidered cosmetic or reconstructive \nbreast surgery. \n \nLogical \nMistakes \nInclusion: The patient \nmust be ≥50 years and \n<70 years of age. \n“The criterion requires a patient to be a female between \nthe ages of 50 and 69. The progress notes from [date] \nstate that the patient is a 65-year-old female. \nTherefore, the criterion is Met.”  \n \nDetermination: Not Met \nFinal determination is incongruent with \nexplanation. \nExclusion: Prior \nradiation therapy to the \nipsilateral or \ncontralateral breast or \nthorax. \n“The treatment plan from [date] mentions radiation \ntherapy to the breast and axilla. Therefore, the \nexclusion criterion is met.” \n \nDetermination: Met \nDiscussion of a planned future \ntreatment, mistaken for treatment history. \n \n \nExclusion: \nDermatomyositis with a \nCPK level above \nnormal or with an active \nskin rash or \nscleroderma. \n“The patient has a rash, as noted in both the initial \nconsult and follow-up visits. Thus, the patient meets \nthis criterion” \n \nDetermination: Met \nCriterion as written in protocol is logically \nambiguous. Logic of the form “A with B \nor C or D” interpreted by LLM as “(A with \nB) or C or D” when it likely was intended \nto be “(A with B or C) or D”. \n"}, {"page": 18, "text": " \n18 \nIncorrect \nHandling of \nMissing \nInformation \nInclusion: ECOG \nPerformance Status of \n0 or 1. \n“The document does not mention ECOG performance \nstatus.”  \n \nDetermination: Not Met \nIf a criterion requires specific concrete \ndata elements such as ECOG status, \nBMI, or age, and the data element is not \nexplicitly mentioned in the patient's chart, \nthe criterion should be marked as \n'unable to determine' and flagged for \nhuman review. \nInclusion: Patients who \nare HIV positive are \neligible, provided they \nare under treatment \nwith highly active \nantiretroviral therapy \nand have a CD4 count \n≥ 200 cells/microliter \nwithin 180 days prior to \nregistration. \n“The patient has no mention of HIV in their chart. \nWithout explicit confirmation of their HIV status, I am \nunable to determine whether they ‘meet’ the criterion.” \n \nDetermination: Not Met \nIf there is no mention of HIV status or \nCD4 count tests, then it is reasonable to \nassume that the patient is not HIV \npositive. \nIrrelevant \nCriterion \nMisclassific\nation \nInclusion criteria for \nintermediate risk sub-\nstudy: Invasive lobular \ncarcinoma. \n \n“The criterion specifies that a patient must have biopsy-\nproven invasive lobular carcinoma to be eligible for the \nintermediate risk sub-study group. According to the \nsurgical pathology on [date], the patient has a \ndiagnosis of invasive ductal carcinoma. Therefore, she \ndoes not meet the eligibility criterion.” \n \nDetermination: Not Met \nCriteria describing sub-studies should be \nmarked as “Met” or “Unable to \ndetermine” if not relevant to the patient, \nso as to not adversely affect their overall \neligibility assessment for the main study. \nInclusion: Patients with \nbrain metastases are \neligible if these lesions \nhave been treated prior \nto enrollment. \n“Radiology report from [date] shows no evidence of \nmetastatic disease.” \n \nDetermination: Not Met  \nFor criteria in the form 'patients with X \nare eligible if Y,' showing that a patient \ndoes not have X is sufficient to answer \nthe criterion. \n"}, {"page": 19, "text": " \n19 \nReferences \n1. Unger, J. M., Shulman, L. N., Facktor, M. A., Nelson, H. & Fleury, M. E. National \nEstimates of the Participation of Patients With Cancer in Clinical Research Studies \nBased on Commission on Cancer Accreditation Data. J. Clin. Oncol. 42, 2139–\n2148 (2024). \n2. Unger, J. M., Vaidya, R., Hershman, D. L., Minasian, L. M. & Fleury, M. E. Systematic \nReview and Meta-Analysis of the Magnitude of Structural, Clinical, and Physician \nand Patient Barriers to Cancer Clinical Trial Participation. JNCI J. Natl. Cancer Inst. \n111, 245–255 (2019). \n3. Comis, R. L., Miller, J. D., Aldige, C. R., Krebs, L. & Stoval, E. Public Attitudes Toward \nParticipation in Cancer Clinical Trials. J. Clin. Oncol. 21, 830–835 (2003). \n4. Unger, J. M. et al. “When Offered to Participate”: A Systematic Review and Meta-\nAnalysis of Patient Agreement to Participate in Cancer Clinical Trials. JNCI J. Natl. \nCancer Inst. 113, 244–257 (2021). \n5. Penberthy, L. T., Dahman, B. A., Petkov, V. I. & DeShazo, J. P. Effort Required in \nEligibility Screening for Clinical Trials. J. Oncol. Pract. 8, 365–370 (2012). \n6. Comis, R. L., Miller, J. D., Colaizzi, D. D. & Kimmel, L. G. Physician-Related Factors \nInvolved in Patient Decisions to Enroll Onto Cancer Clinical Trials. J. Oncol. Pract. \n5, 50–56 (2009). \n7. Wiest, I. C. et al. Privacy-preserving large language models for structured medical \ninformation retrieval. Npj Digit. Med. 7, 1–9 (2024). \n8. Hu, Y. et al. Information Extraction from Clinical Notes: Are We Ready to Switch to \nLarge Language Models? Preprint at http://arxiv.org/abs/2411.10020 (2024). \n9. Liu, L. et al. Human-level information extraction from clinical reports with fine-tuned \nlanguage models. Preprint at https://doi.org/10.1101/2024.11.18.24317466 (2024). \n10. Elemento, O., Khozin, S. & Sternberg, C. N. The Use of Artificial Intelligence for \nCancer Therapeutic Decision-Making. NEJM AI 2, AIra2401164 (2025). \n11. Lotter, W. et al. Artificial Intelligence in Oncology: Current Landscape, Challenges, \nand Future Directions. Cancer Discov. 14, 711–726 (2024). \n12. Benary, M. et al. Leveraging Large Language Models for Decision Support in \nPersonalized Oncology. JAMA Netw. Open 6, e2343689 (2023). \n13. Zou, J. & Topol, E. J. The rise of agentic AI teammates in medicine. The Lancet \n405, 457 (2025). \n14. Lee, Y., Ferber, D., Rood, J. E., Regev, A. & Kather, J. N. How AI agents will \nchange cancer research and oncology. Nat. Cancer 5, 1765–1767 (2024). \n15. Qiu, J. et al. LLM-based agentic systems in medicine and healthcare. Nat. Mach. \nIntell. 1–3 (2024) doi:10.1038/s42256-024-00944-1. \n16. Gao, S. et al. Empowering biomedical discovery with AI agents. Cell 187, 6125–\n6151 (2024). \n17. Nori, H. et al. Sequential Diagnosis with Language Models. Preprint at \nhttps://doi.org/10.48550/arXiv.2506.22405 (2025). \n18. Stubbs, A., Filannino, M., Soysal, E., Henry, S. & Uzuner, Ö. Cohort selection for \nclinical trials: n2c2 2018 shared task track 1. J. Am. Med. Inform. Assoc. 26, 1163–\n1171 (2019). \n"}, {"page": 20, "text": " \n20 \n19. Tai, C. A. & Tannier, X. Clinical trial cohort selection using Large Language Models \non n2c2 Challenges. Preprint at https://doi.org/10.48550/arXiv.2501.11114 (2025). \n20. Beattie, J. et al. Utilizing Large Language Models for Enhanced Clinical Trial \nMatching: A Study on Automation in Patient Screening. Cureus \nhttps://doi.org/10.7759/cureus.60044 (2024) doi:10.7759/cureus.60044. \n21. Wornow, M. et al. Zero-Shot Clinical Trial Patient Matching with LLMs. NEJM AI 0, \nAIcs2400360 (2024). \n22. Callies, A., Bodinier, Q., Ravaud, P. & Davarpanah, K. Real-world validation of a \nmultimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching \nleveraging EHR data. Preprint at https://doi.org/10.48550/arXiv.2503.15374 (2025). \n23. Vecchio, C. et al. Analysis of a large language model-based system versus manual \nreview in clinical data abstraction and deduction from real-world medical records of \npatients with melanoma for clinical trial eligibility assessment. J. Clin. Oncol. \nhttps://doi.org/10.1200/JCO.2025.43.16_suppl.1571 (2025) \ndoi:10.1200/JCO.2025.43.16_suppl.1571. \n24. Koopman, B. & Zuccon, G. A Test Collection for Matching Patients to Clinical Trials. \nin Proceedings of the 39th International ACM SIGIR conference on Research and \nDevelopment in Information Retrieval 669–672 (ACM, Pisa Italy, 2016). \ndoi:10.1145/2911451.2914672. \n25. Roberts, K., Demner-Fushman, D., Voorhees, E. M., Bedrick, S. & Hersh, W. R. \nOverview of the TREC 2022 Clinical Trials Track. in The Thirty-First Text REtrieval \nConference (TREC 2022) Proceedings (NIST, 2022). \n26. TREC 2021 Clinical Trials Track. https://www.trec-cds.org/2021.html (2021). \n27. Jin, Q. et al. Matching patients to clinical trials with large language models. Nat. \nCommun. 15, 9074 (2024). \n28. Nievas, M., Basu, A., Wang, Y. & Singh, H. Distilling large language models for \nmatching patients to clinical trials. J. Am. Med. Inform. Assoc. 31, 1953–1963 \n(2024). \n29. Cerami, E. et al. MatchMiner-AI: An Open-Source Solution for Cancer Clinical Trial \nMatching. Preprint at https://doi.org/10.48550/arXiv.2412.17228 (2024). \n30. Ferber, D. et al. End-To-End Clinical Trial Matching with Large Language Models. \nPreprint at http://arxiv.org/abs/2407.13463 (2024). \n31. Zeng, J. et al. OCTANE: Oncology Clinical Trial Annotation Engine. JCO Clin. \nCancer Inform. 1–11 (2019) doi:10.1200/CCI.18.00145. \n32. Klein, H. et al. MatchMiner: an open-source platform for cancer precision medicine. \nNpj Precis. Oncol. 6, 1–9 (2022). \n33. Parikh, R. B. et al. Human-AI teams to improve accuracy and timeliness of \noncology trial prescreening: Preplanned interim analysis of a randomized trial. J. \nClin. Oncol. 42, 1524–1524 (2024). \n34. Datta, S. et al. AutoCriteria: a generalizable clinical trial eligibility criteria extraction \nsystem powered by large language models. J. Am. Med. Inform. Assoc. 31, 375–\n385 (2024). \n35. Unlu, O. et al. Retrieval-Augmented Generation–Enabled GPT-4 for Clinical Trial \nScreening. NEJM AI 1, AIoa2400181 (2024). \n"}, {"page": 21, "text": " \n21 \n36. Unlu, O. et al. Manual vs AI-Assisted Prescreening for Trial Eligibility Using Large \nLanguage Models—A Randomized Clinical Trial. JAMA \nhttps://doi.org/10.1001/jama.2024.28047 (2025) doi:10.1001/jama.2024.28047. \n37. Markey, N. et al. Clinical trials are becoming more complex: a machine learning \nanalysis of data from over 16,000 trials. Sci. Rep. 14, 3514 (2024). \n38. Gupta, S. et al. PRISM: Patient Records Interpretation for Semantic clinical trial \nMatching system using large language models. Npj Digit. Med. 7, 1–12 (2024). \n39. OpenAI et al. GPT-4o System Card. Preprint at \nhttps://doi.org/10.48550/arXiv.2410.21276 (2024). \n40. LangGraph. LangChain Inc. (2024). \n41. Davis, J., Van Bulck, L., Durieux, B. N. & Lindvall, C. The Temperature Feature of \nChatGPT: Modifying Creativity for Clinical Research. JMIR Hum. Factors 11, \ne53559 (2024). \n42. Huang, L. et al. A Survey on Hallucination in Large Language Models: Principles, \nTaxonomy, Challenges, and Open Questions. ACM Trans Inf Syst 43, 42:1-42:55 \n(2025). \n43. Asgari, E. et al. A framework to assess clinical safety and hallucination rates of \nLLMs for medical text summarisation. Npj Digit. Med. 8, 1–15 (2025). \n44. Shah, S. V. Accuracy, Consistency, and Hallucination of Large Language Models \nWhen Analyzing Unstructured Clinical Notes in Electronic Medical Records. JAMA \nNetw. Open 7, e2425953 (2024). \n45. Ji, Z. et al. Survey of Hallucination in Natural Language Generation. ACM Comput \nSurv 55, 248:1-248:38 (2023). \n46. Zakka, C. et al. Almanac — Retrieval-Augmented Language Models for Clinical \nMedicine. NEJM AI 1, AIoa2300068 (2024). \n47. Ng, K. K. Y., Matsuba, I. & Zhang, P. C. RAG in Health Care: A Novel Framework \nfor Improving Communication and Decision-Making by Addressing LLM Limitations. \nNEJM AI 2, AIra2400380 (2025). \n48. Lewis, P. et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP \nTasks. Preprint at https://doi.org/10.48550/arXiv.2005.11401 (2021). \n49. Tanno, R. et al. Collaboration between clinicians and vision–language models in \nradiology report generation. Nat. Med. https://doi.org/10.1038/s41591-024-03302-1 \n(2024) doi:10.1038/s41591-024-03302-1. \n50. Kocielnik, R. et al. Human AI collaboration for unsupervised categorization of live \nsurgical feedback. Npj Digit. Med. 7, 1–12 (2024). \n51. English, E., Laughlin, J., Sippel, J., DeCamp, M. & Lin, C.-T. Utility of Artificial \nIntelligence–Generative Draft Replies to Patient Messages. JAMA Netw. Open 7, \ne2438573 (2024). \n52. Budd, S., Robinson, E. C. & Kainz, B. A survey on active learning and human-in-\nthe-loop deep learning for medical image analysis. Med. Image Anal. 71, 102062 \n(2021). \n53. Ohde, J. W., Rost, L. M. & Overgaard, J. D. The Burden of Reviewing LLM-\nGenerated Content. NEJM AI 2, AIp2400979 (2025). \n54. Lehman, E. et al. Do We Still Need Clinical Language Models? Preprint at \nhttps://doi.org/10.48550/arXiv.2302.08091 (2023). \n"}, {"page": 22, "text": " \n22 \n55. Pal, S., Bhattacharya, M., Lee, S.-S. & Chakraborty, C. A Domain-Specific Next-\nGeneration Large Language Model (LLM) or ChatGPT is Required for Biomedical \nEngineering and Research. Ann. Biomed. Eng. 52, 451–454 (2024). \n56. Guevara, M. et al. Large language models to identify social determinants of health \nin electronic health records. Npj Digit. Med. 7, 6 (2024). \n57. Jiang, L. Y. et al. Health system-scale language models are all-purpose prediction \nengines. Nature 619, 357–362 (2023). \n58. Wang, G., Yang, G., Du, Z., Fan, L. & Li, X. ClinicalGPT: Large Language Models \nFinetuned with Diverse Medical Data and Comprehensive Evaluation. Preprint at \nhttp://arxiv.org/abs/2306.09968 (2023). \n59. Jeong, D. P., Garg, S., Lipton, Z. C. & Oberst, M. Medical Adaptation of Large \nLanguage and Vision-Language Models: Are We Making Progress? in Proceedings \nof the 2024 Conference on Empirical Methods in Natural Language Processing \n12143–12170 (Association for Computational Linguistics, Miami, Florida, USA, \n2024). doi:10.18653/v1/2024.emnlp-main.677. \n60. Nori, H. et al. Can Generalist Foundation Models Outcompete Special-Purpose \nTuning? Case Study in Medicine. Preprint at \nhttps://doi.org/10.48550/arXiv.2311.16452 (2023). \n61. Rosenthal, J. T., Beecy, A. & Sabuncu, M. R. Rethinking clinical trials for medical AI \nwith dynamic deployments of adaptive systems. Npj Digit. Med. 8, 1–6 (2025). \n \n \n \n"}, {"page": 23, "text": " \n23 \nSupplementary Information \nSupplementary Figures \n \nSupplementary Figure S1. Comparison of performance metrics on the test set, single-\nexpert vs. multi-expert system architecture.  \n \n \n"}, {"page": 24, "text": " \n24 \n \n \nSupplementary Figure S2. MSK-MATCH web application human-AI interface. a) The \ntop row has controls for selecting protocol and patient-specific medical record number \n(MRN). When data are loaded for the selected {patient, protocol} pair, the overall \neligibility status is shown, along with counts of criteria by status (qualifying, \ndisqualifying, and unable to determine). b) Example of interface for one criterion. On the \nleft column, the criterion label is colored by status to provide a visual cue for users \nscrolling through. The second column gives the verbatim text of the criterion. The third \ncolumn displays the AI-generated explanations, one for each of the expert agents \nconsulted. In the final column, a dropdown menu allows the user to override the AI-\ngenerated explanation based on their human expert judgement. In that case, a modal \ndialog box will prompt the user to provide feedback or explanation of their decision, \nwhich is then added to the oncology trial knowledge base. c) When the “inspect \nevidence” button is clicked, users are directed to a separate page (depicted here in an \ninset) where they can view the verbatim snippets from the clinical documents which \nwere retrieved from the vector stores and used by the expert agent. Each snippet is \nlabeled with the note type and date to facilitate users being able to quickly navigate to it \nin the external EHR system for further inspection and verification. Dates and other \nidentifying information have been redacted to protect privacy.  \n"}, {"page": 25, "text": " \n25 \n \n \n \nSupplementary Figure S3. Performance on matched cases in the test set. a) AI only \nvs. human-AI collaborative workflow. b) Human only vs. AI only. \n \nSupplementary Figure S4. Schematic diagram of dataset resulting from experimental \ndesign shown in Figure 2a. \n"}, {"page": 26, "text": " \n26 \n \nSupplementary Figure S5. Detailed results of the classification performance of four \ndifferent conditions on the test set. a) Human-only workflow, unassisted. b) AI \npredictions only, with no human review. c) Human-AI collaborative workflow, with triage \nand human review of a subset of AI predictions. d) Performance on the subset of cases \nfrom (c) that were reviewed by a human.  \n"}, {"page": 27, "text": " \n27 \n \nSupplementary Figure S6. Distribution of the number of disqualifying criteria among \npredicted negatives in the test set. A disqualifying criterion is an inclusion criterion that is \nnot met or an exclusion criterion that is met. Colors indicate the counts of true negatives \nand false negatives.  \n \n \n \n"}, {"page": 28, "text": " \n28 \nSupplementary Tables \n \nSupplementary Table S1. Text of criteria identified as vacuous. \n \nCriterion \nProtocol \nFor patients who have undergone lumpectomy, any type of mastectomy and any \ntype of reconstruction (including no reconstruction) are allowed. Metallic \ncomponents of some tissue expanders may complicate delivery of proton therapy; \nany concerns should be discussed with the Breast Committee Study Chairs prior to \nregistration. \n16-323 \nFor patients who have undergone lumpectomy, there are no breast size limitations. \n16-323 \nBilateral breast cancer is permitted. Patients receiving treatment to both breasts for \nbilateral breast cancer will be stratified as left-sided. \n16-323 \nPatients may or may not have had adjuvant chemotherapy. \n19-410 \nPatients with T3N0 disease are eligible. \n19-410 \nThe trial is open to female and male patients. \n21-283 \n \nSupplementary Table S2. Text of criteria identified as requiring clinician input. \nCriterion \nProtocol \nConfirmation that the patient's health insurance will pay for the treatment in this study (patients may \nstill be responsible for some costs, such as co-pays and deductibles). If the patient's insurance will \nnot cover a specific treatment in this study and the patient still wants to participate, confirmation \nthat the patient would be responsible for paying for any treatment received. \n16-323 \nThe patient must provide study-specific informed consent prior to study entry. \n16-323 \nAble to provide informed consent. \n18-486 \nPatients whose entry to the trial will cause unacceptable clinical delays in their planned \nmanagement. \n18-486 \nWritten informed consent obtained from subject and ability to comply with the requirements of the \nstudy. \n19-300 \nFor female subjects of childbearing potential, patient is willing to use 2 methods of birth control or \nbe surgically sterile or abstain from heterosexual activity for the duration of study participation. \nNote: Should a woman become pregnant while participating on study, she should inform the \ntreating physician immediately. \n19-300 \nPatient consent must be appropriately obtained in accordance with applicable loca`l and regulatory \nrequirements. Each patient must sign a consent form prior to enrollment in the trial to document \ntheir willingness to participate. A similar process must be followed for sites outside of Canada as \nper their respective cooperative group's procedures. \n19-410 \nPatients must be accessible for treatment and follow-up. Investigators must assure themselves that \npatients randomized on this trial will be available for complete documentation of the treatment, \nadverse events, and follow-up. \n19-410 \nPatients must have had endocrine therapy initiated or planned for ≥ 5 years. Premenopausal \nwomen will receive ovarian ablation plus aromatase inhibitor therapy or tamoxifen if adjuvant \nchemotherapy was not administered. For all patients, endocrine therapy can be given concurrently \nor following RT. \n19-410 \nHas the patient seen their Medical Oncologist? \n19-410 \nWomen of childbearing potential must have agreed to use an effective contraceptive method. A \nwoman is considered to be of 'childbearing potential' if she has had menses at any time in the \n19-410 \n"}, {"page": 29, "text": " \n29 \npreceding 12 consecutive months. In addition to routine contraceptive methods, 'effective \ncontraception' also includes heterosexual celibacy and surgery intended to prevent pregnancy (or \nwith a side-effect of pregnancy prevention) defined as a hysterectomy, bilateral oophorectomy or \nbilateral tubal ligation, or vasectomy/vasectomized partner. However, if at any point a previously \ncelibate patient chooses to become heterosexually active during the time period for use of \ncontraceptive measures outlined in the protocol, she is responsible for beginning contraceptive \nmeasures. Women of childbearing potential will have a pregnancy test to determine eligibility as \npart of the Pre-Study Evaluation (see Section 4.0); this may include an ultrasound to rule-out \npregnancy if a false-positive is suspected. For example, when beta-human chorionic gonadotropin \nis high and partner is vasectomized, it may be associated with tumour production of hCG, as seen \nwith some cancers. Patient will be considered eligible if an ultrasound is negative for pregnancy. \nThe patient or a legally authorized representative must provide study-specific informed consent \nprior to pre entry /step 1 and, for patients treated in the U.S., authorization permitting release of \npersonal health information. \n21-283 \nPatients must be intending to take endocrine therapy for a minimum 5 years duration (tamoxifen or \naromatase inhibitor). The specific regimen of endocrine therapy is at the treating physician's \ndiscretion. \n21-283 \nWilling and able to provide informed consent. \n22-259 \nConsented to 12-245. \n22-259 \n \n \n \n \n \n \n \n \n \n"}, {"page": 30, "text": " \n30 \nSupplementary Table S3: Comparison of datasets used to evaluate performance in \nrelated works. ◇: pre-specified set of criterion-level attributes; ◈: single trial; ◆: multiple \ntrials. \n \nReference \nNumber of \npatients \nDocuments \nper patient \nReal EHR \ndata \nClinical \ntrial data \n2018 n2c2 cohort selection \nchallenge18 (used in 19–22) \n288 \n2-5 \n✓ \n◇ \nTREC 2021 Clinical Trials \nTrack26 \n75 \n1 \n✗ \n◆ \nTREC 2022 Clinical Trials \nTrack25 \n50 \n1 \n✗ \n◆ \nFerber et al.30 \n51 \n1 \n✗ \n◆ \nTrialGPT27 \n183 \n1 \n✗ \n◆ \nTrial-LLAMA28 \n313 \n1 \n✗ \n◆ \nSynapsis23 \n50 \n? \n✓ \n◇ \nParikh et al.33 \n74 \n? \n✓ \n◇ \nMatchMiner-AI29 \n13086 \n1† \n✗† \n◆ \nOncoLLM38 \n98 \n74 \n✓ \n◆ \nRECTIFIER35 \n1509 \n127* \n✓ \n◈ \nMSK-MATCH (ours) \n731 \n121 \n✓ \n◆ \n \n† Synthetic summaries derived from real EHR data \n* Estimated using values from Figure S2 C. \n \n \n"}]}