{"doc_id": "arxiv:2512.19864", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.19864.pdf", "meta": {"doc_id": "arxiv:2512.19864", "source": "arxiv", "arxiv_id": "2512.19864", "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data", "authors": ["Shashi Kant Gupta", "Arijeet Pramanik", "Jerrin John Thomas", "Regina Schwind", "Lauren Wiener", "Avi Raju", "Jeremy Kornbluth", "Yanshan Wang", "Zhaohui Su", "Hrituraj Singh"], "published": "2025-12-22T20:38:30Z", "updated": "2025-12-26T11:32:02Z", "summary": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.19864v2", "url_pdf": "https://arxiv.org/pdf/2512.19864.pdf", "meta_path": "data/raw/arxiv/meta/2512.19864.json", "sha256": "09b05ca3f15959446b6118c691ae8eae37da39f3f7acf4a7d331638b003b6b2f", "status": "ok", "fetched_at": "2026-02-18T02:23:57.391588+00:00"}, "pages": [{"page": 1, "text": "HARMON-E : Hierarchical Agentic Reasoning for\nMulti-modal Oncology Notes to Extract\nStructured Data\nShashi Kant Gupta1, Arijeet Pramanik1, Jerrin John Thomas1,\nRegina Schwind1, Lauren Wiener3, Avi Raju3, Jeremy Kornbluth3,\nYanshan Wang2†, Zhaohui Su3†, Hrituraj Singh1*†\n1 Triomics, New York, USA.\n2 University of Pittsburgh, Pittsburgh, USA.\n3 Ontada, Boston, USA.\n*Corresponding author(s). E-mail(s): hrituraj@triomics.com;\nContributing authors: shashi.gupta@triomics.com;\narijeet.pramanik@triomics.com; jerrin.thomas@triomics.com;\nregina@triomics.com; lauren.wiener@mckesson.com;\navi.raju@mckesson.com; jeremy.kornbluth@mckesson.com;\nyanshan.wang@pitt.edu; zhaohui.Su@mckesson.com;\n†These authors contributed equally to this work.\nAbstract\nUnstructured notes within the electronic health record (EHR) contain rich clin-\nical information vital for cancer treatment decision making and research, yet\nreliably extracting structured oncology data remains challenging due to extensive\nvariability, specialized terminology, and inconsistent document formats. Manual\nabstraction, although accurate, is prohibitively costly and unscalable. Existing\nautomated approaches typically address narrow scenarios—either using synthetic\ndatasets, restricting focus to document-level extraction, or isolating specific clini-\ncal variables (e.g., staging, biomarkers, histology)—and do not adequately handle\npatient-level synthesis across the large number of clinical documents containing\ncontradictory information. In this study, we propose an agentic framework that\nsystematically decomposes complex oncology data extraction into modular, adap-\ntive tasks. Specifically, we use large language models (LLMs) as reasoning agents,\nequipped with context-sensitive retrieval and iterative synthesis capabilities,\nto exhaustively and comprehensively extract structured clinical variables from\n1\narXiv:2512.19864v2  [cs.CL]  26 Dec 2025\n"}, {"page": 2, "text": "real-world oncology notes. Evaluated on a large-scale dataset of over 400, 000\nunstructured clinical notes and scanned PDF reports spanning 2, 250 cancer\npatients, our method achieves an average F1-score of 0.93, with 100 out of\n103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g.,\nbiomarkers and medications) surpassing 0.95. Moreover, integration of the agen-\ntic system into a data curation workflow resulted in 0.94 direct manual approval\nrate, significantly reducing annotation costs. To our knowledge, this consti-\ntutes the first exhaustive, end-to-end application of LLM-based agents\nfor structured oncology data extraction at scale.\nKeywords: electronic health records, large language models, oncology data\nextraction, artificial intelligence, clinical natural language processing, agentic language\nmodeling, cancer, structured data, automated abstraction, clinical informatics,\nbiomarkers, healthcare data curation, medical text mining, computational oncology,\nEHR notes, unstructured clinical data, patient records, medical document processing,\nclinical decision support, information extraction\n1 Introduction\nTraditional efforts toward automating the extraction of structured data from clinical\ntext have relied heavily on rule-based systems or shallow machine learning models\n(e.g., Conditional Random Fields, Support Vector Machines), each requiring extensive\ndomain-specific feature engineering [1–4]. In the last decade, however, transformer-\nbased models [5], such as Bidirectional Encoder Representations from Transformers\n(BERT)[6] and its domain-specific variants like ClinicalBERT [7–9], have significantly\nimproved upon classical approaches across a range of clinical natural language pro-\ncessing (NLP) tasks. More recently, the approach has shifted from fine-tuning domain\nspecific models [10, 11] to using the generalized abilities of frontier large language mod-\nels (LLMs) like GPT-4 and GPT-5[12, 13] to extract key concepts from EHR records\n[14].\nNevertheless, prior works often assume relatively uniform data (such as a single\ncancer diagnosis or one type of note), single-document inputs, or a very limited set of\nconcepts. Published studies have demonstrated entity extraction for discrete variables\nsuch as tumor stage from pathology reports [15, 16], receptor or genomic biomarkers\nfrom pathology or genomic reports [17, 18], and initial regimens or lines of therapy\nfor select cancers [19, 20]. However, these approaches extract variables independently\nwithout synthesizing findings across multiple documents to resolve contradictions or\ncomplete partial information. This study is distinct in that it targets patient-level\nsynthesis across heterogeneous, longitudinal records, where oncology-specific concepts\nmust be inferred by collating and contextualizing information scattered across multiple\nnotes and data types, rather than relying on single-step extraction from a uniform\ndocument.\nTo address these challenges, we introduce HARMON-E : Hierarchical Agentic\nReasoning for Multi-source Oncology Notes to Extract structured data\n2\n"}, {"page": 3, "text": "Our framework systematically decomposes oncology data extraction into modular,\niterative steps that leverage large language models (LLMs) as “reasoning agents”\nthat interleave retrieval with multi-step inference to reconcile conflicting evidence,\nnormalize temporal references, and derive implicit variables not explicitly stated in the\ntext - such as inferring treatment discontinuation dates from adverse event narratives\n(Figure 1). Concretely, HARMON-E combines context-aware indexing, adaptive retrieval\nmethods (including vector-based search and rule-based actions), and LLM-driven data\nsynthesis into a unified workflow. By adopting a hierarchical, agentic approach, we\nmitigate common pitfalls in extracting complex, interdependent oncology data and\nachieve end-to-end alignment with standardized pre-defined set of clinical variables\ncomprising 103 attributes across 16 entity types-including Biomarker, Medication,\nDiagnosis, Staging, Surgery and Radiation entities as outlined in Table 1 and Figure 3.\nWe validate HARMON-E on a large-scale real-world dataset of over 400,000 unstruc-\ntured clinical notes and scanned PDFs belonging to 2,250 cancer patients. Our\nevaluation covers 103 oncology-specific variables—ranging from histopathological find-\nings and biomarker statuses to treatment patterns and disease progression. HARMON-E\nconsistently delivers high accuracy, with 100 out of 103 variables exceeding an F1-\nscore of 80%, and crucial fields such as biomarker assessments and medication data\nsurpassing 95% accuracy. Beyond these metrics, when integrated into a data cura-\ntion platform, our system demonstrates a direct manual approval rate of 94.1%,\nsubstantially reducing human annotation burden without sacrificing data quality.\nOur contributions can be summarized as follows -\n1. High-accuracy LLM-based agentic workflows. We provide the first evidence\nthat large language models, organized into an agentic workflow, can exceed 95%\nF1-score on key oncology concepts at scale, validated against a corpus of 940,923\ndata points derived from 400,000 clinical notes covering 2,250 patients.\n2. Real-world integration and user acceptance. We also demonstrate that\nwhen integrated into an interactive data curation platform, 94% of the extracted\ndata points get directly accepted by professional oncology abstractors without\nmodification, substantially reducing manual review time.\n3. Novel evaluation framework. We propose an evaluation methodology which\nmoves beyond traditional named-entity recognition metrics by aligning performance\nassessment with the real-world requirements of oncology data curation and quality\nmonitoring.\n2 Related Work\nEarly Clinical NLP and Rule-Based Methods: Early clinical NLP relied primar-\nily on rule-based systems, utilizing domain-specific lexicons and hand-crafted patterns\n[3, 21, 22]. These systems, though precise in certain scenarios, required substantial\nmanual effort and were brittle when encountering variability [1, 23]. Notable examples\ninclude MedXN for medication extraction [24], comprehensive clinical NLP toolkits\nlike CLAMP [25], fracture identification from radiology reports [26], and sudden car-\ndiac death risk factor extraction [27]. Statistical machine learning models, notably\n3\n"}, {"page": 4, "text": "Fig. 1: Comparative analysis of HARMON-E versus traditional Named Entity\nRecognition (NER) approaches for medical document processing. A tradi-\ntional NER system (lower left, red box) would incorrectly extract the planned end\ndate (09/17/2020) without contextual understanding. In contrast, our agentic system\n(lower right, green box) performs multi-step reasoning: first identifying the planned\ntreatment duration, then detecting the adverse event (pneumonitis), cross-referencing\ninformation about completed treatment cycles, and finally concluding that treatment\nactually ended on December 12, 2019 due to the adverse event—a conclusion impos-\nsible with traditional single-pass methods. (This is not real patient data and is for\nillustrative purposes only.)\nConditional Random Fields (CRFs) and Support Vector Machines (SVMs), subse-\nquently improved performance on clinical text tasks like de-identification and entity\nextraction [1]. Early language modeling approaches showed promise for identifying\nrelevant information in clinical notes [28], though still required task-specific adapta-\ntion. Nevertheless, adapting these models to heterogeneous oncology datasets posed\nsignificant challenges due to diverse terminologies and complex documentation [29].\nThe evolution of clinical information extraction methods has been comprehensively\nreviewed by Wang et al. [30], documenting the systematic transition from rule-based\nto neural approaches.\n4\n"}, {"page": 5, "text": "Deep Learning and Transformer Models in Clinical NLP: Transformer\narchitectures, particularly BERT [5, 6], significantly advanced clinical NLP by pro-\nviding powerful language representations. Domain-specific models such as BioBERT\n[8], ClinicalBERT [7], and SciBERT [11] further enhanced clinical NLP tasks, achiev-\ning state-of-the-art performance in clinical entity recognition and relation extraction\n[31, 32]. Comprehensive evaluations of these models on biomedical text-mining tasks\ndemonstrated their superiority over traditional approaches [33]. However, oncology\ndata, characterized by extensive and contextually complex documentation, poses\nunique challenges inadequately addressed by traditional single-pass transformers.\nApproaches employing hierarchical transformers and multi-document summarization\nhave attempted to mitigate these limitations [9, 34–36] but still fall short of fully\nreplicating human abstraction.\nLarge Language Models in Healthcare: Recent advances in large language\nmodels (LLMs), including GPT-4, and GPT-5[12, 13], have demonstrated impressive\ncapabilities in summarization, medical question answering, and clinical decision sup-\nport [37, 38]. This has led to several research works exploring the capabilities of these\nmodels in extracting structured data from notes [14, 39, 40]. Recent multi-institutional\nefforts have shown promise in extracting social determinants of health from clinical\nnotes using LLMs, achieving F1 scores over 0.9 [41]. Work on extracting functional\nstatus information, including mobility assessments, from clinical notes has demon-\nstrated the potential of LLMs for capturing complex clinical concepts [42, 43]. Despite\nthis progress, single-pass LLM approaches struggle with multi-document EHR data,\npotentially losing critical context or misinterpreting conflicting information. More-\nover, most research work so far has been limited to a limited set of oncology concepts\n[9, 36, 39, 44] instead of scaling it to a comprehensive RWD dataset dictionary.\nAgentic and Iterative NLP Frameworks: Recent NLP frameworks have\nincreasingly adopted iterative reasoning approaches. The ReAct framework introduced\nby Yao et al. [45] enables LLMs to interleave reasoning and retrieval actions, improving\naccuracy on complex reasoning tasks. Techniques such as chain-of-thought prompting\n[46] and self-consistency decoding [47] further enhance multi-step reasoning capa-\nbilities. Additionally, integrating external tools or retrieval systems, as explored in\nToolformer [48] and web-augmented retrieval methods [49], has shown promise. Yet,\nsystematically integrating these approaches for clinical NLP, particularly oncology-\nspecific data extraction, remains an open challenge due to accuracy and validation\nconstraints.\nAutonomous & Agentic LLM Systems in Oncology: Oncology specific AI\nresearch has also begun moving from single-prompt LLM use toward agentic pipelines\nthat interleave reasoning and tool calls. Ferber et al. developed an autonomous GPT-\n4–driven clinical-decision agent that orchestrates vision models, knowledge bases, and\nweb search to solve multimodal oncology cases, raising accuracy from 30% (plain\nGPT-4) to over 87% on complex vignettes [50]. Sandhu et al. proposed an open-\nsource modular framework that couples rule-based components with an agentic layer\nto generate comprehensive breast-cancer notes and benchmark treatment recom-\nmendations against NCCN guidelines [51]. Outside healthcare, Zhang and Elhamod\n5\n"}, {"page": 6, "text": "proposed Data-to-Dashboard, a multi-agent architecture that detects domain con-\ntext, extracts concepts, performs iterative self-reflection, and automatically builds\nanalytic dashboards [52]. While these works demonstrate the promise of hierarchical\nagents, they tackle relatively narrow tasks (tens of test cases, disease-specific notes, or\ngeneric analytics) rather than end-to-end patient-level abstraction across thousands\nof heterogeneous documents.\nDomain-Specific LLMs for Oncology. Parallel efforts tailor foundation models\nand pipelines to oncology data. CancerBERT pretrains BERT on cancer corpora with\na cancer-specific vocabulary and reports significant gains for extracting breast-cancer\nphenotypes (e.g., receptor status, site/laterality) over general and clinical BERT\nbaselines from EHR notes and pathology reports[53]. OncoBERT explores transfer\nlearning on oncology notes for outcome prediction and phenotype structuring, report-\ning improvements on site and clinical T-staging tasks and highlighting interpretability\nconsiderations for radiation oncology workflows [9, 10]. Beyond encoders, registry-\noriented systems deliver patient-level abstraction: the original DeepPhe extracts\nphenotypes across entire EMRs, while DeepPhe-CR exposes API services integrated\ninto cancer-registrar tools for computer-assisted abstraction, with usability studies\nand deployment guidance for registry workflows [4, 54]. Patient-level staging at treat-\nment initiation has also been demonstrated in Veterans Affairs data using rule-based\nNLP with validated roll-up for multiple myeloma [55]. In parallel, registry–EHR\nfusion cohorts constructed via NLP underscore the need for multi-source abstraction\n[56]. A complementary line targets narrow modalities or procedures—e.g., Woollie,\na radiology-focused LLM trained on 39k impressions that attains strong progression-\nprediction AUROC of 0.97 for progression prediction and outperforms general LLMs\non medical benchmarks, and hybrid agentic pipelines coupling rules with GPT-4-\nTurbo for detailed spine-surgery variable extraction [57, 58]. Orthogonal oncology\nIE efforts focus on single-entity extraction from pathology and clinical notes, includ-\ning TNM staging and receptor status [15, 16, 18], systemic treatment identification\nand line-of-therapy inference [19, 20], and broader RWD curation reviews and tooling\n[17, 59].\nMost oncology LLM studies target narrow concept sets rather than comprehensive\ndata dictionaries [9, 36, 39, 44]. These systems typically optimize for task- or modality-\nspecific accuracy using single-pass encoders or rule-based components; they provide\nlimited mechanisms for cross-entity dependency resolution, large-scale deduplication\nacross documents, and curator-ready validation—gaps our agentic workflow aims to\nclose.\nTo overcome this, HARMON-E aim to mirror the iterative reasoning process of human\nabstractors. By combining state-of-the-art linguistic models and structured retrieval\nactions, our proposed approach promises robust, scalable extraction from heteroge-\nneous oncology records. To our knowledge, this is the first agentic LLM framework\nthat delivers both breadth (comprehensive data dictionary) and depth (patient-level\nconsistency) at scale, bridging the gap between prototype agents or niche LLMs and\nproduction-grade oncology data curation.\n6\n"}, {"page": 7, "text": "Table 1: Clinical Entities, Definitions, and Corresponding Attributes\nEntity Name\nDefinition of Entity\nAttributes\nBiomarker\nConsolidated\nentity\nfor\nall\nbiomarker testing, including genetic\nbiomarkers,\nmicrosatellite\ninsta-\nbility,\ncopy\nnumber\nalterations,\nrearrangements,\ntumor\nmarker\ntests, and tumor mutation burden.\nbiomarker test date,\nresult date,\nbiomarker tested,\ngene studied,\ngene1,\ncopy number type,\nmethod,\ncode,\naminoacid change,\naminoacid changetype,\nstain percent,\nvalue,\nvalue quantity,\nvalue unit, interpretation, molecular abnormal\nBiopsy\nDescribes the biopsy specimen (e.g.,\nliquid or tissue) and dates.\ntype, result date, ordered date, collect date,\ninsufficient tissue\nClinical Trial\nDerived\nfrom\nclinical\ntrial\nattributes. Indicates patient enroll-\nment in a trial.\nct flag, ct start date, ct end date\nComorbidities\nCaptures health conditions (Charl-\nson Comorbidity Index) co-existing\nwith primary cancer.\ncomorb date,\ncomorb condition,\ncomorb condition present\nDistant Metasta-\nsis\nIndicates metastatic spread beyond\nthe primary site after initial diag-\nnosis.\nstatus date,\nstatus,\nassociated diagnosis,\nbody site\nDiagnosis\nDetails about the primary diagno-\nsis, including histology and site.\ndiag date, condition, body site, histology\nFamily History\nDetails the family cancer history for\na patient.\nrelationship, condition, onset\nImaging\nDescribes imaging services a patient\nhas received.\nstart date, modality, body site\nMedication\nCaptures\nsystemic\ntreatments\n(drugs, therapies), start/end dates,\nroutes, etc.\nstatus, start date, end date, treatment intent,\nmedication,\nroute,\ntermination reason,\nbaseline dosage,\nbaseline dosage units,\nbaseline dosage quantity,\nbaseline dosage freq,\nbaseline dosage duration,\nbaseline cycle length,\ndose change reason,\nchanged dosage,\nchanged dosage units,\nchanged dosage quantity,\nchanged dosage freq,\nchanged dosage duration, changed cycle length,\ndose formula,\ndose formula unit,\ntreatment sequence\nNicotine\nUse\nStatus\nDescribes patient’s smoking or nico-\ntine usage.\ncode,\ntype,\nuse,\nuse unit,\nuse frequency,\nstart date, end date\nPatient Status\nCovers demographics, vital status,\nand disposition.\nvital status, last contact date, hospice date,\nrelapse flag\nRadiation\nCaptures radiation therapy details.\nmodality,\nstart date,\nend date,\ntotal dose delivered value,\ntotal dose delivered unit,\nfractions delivered, body site\nRecurrence\nSta-\ntus\nTracks local or metastatic recur-\nrence of disease.\nstatus date,\nstatus,\nassociated diagnosis,\nbody site\nStaging\nDescribes TNM staging at diagno-\nsis.\nstage date,\nstage type,\ntumor category,\nnodes category,\nmetastases category,\nstage value\nSurgery\nDates and types of surgical proce-\ndures received.\nsurgery date, surgery type, outcome\nSurgery\nObser-\nvations\nSpecific to surgical procedures and\noutcomes.\nobserve date, code, value, value units\n7\n"}, {"page": 8, "text": "Fig. 2: The HARMON-E transformation pipeline. Unstructured clinical doc-\numents from multiple sources (left) containing medications, biomarkers, staging\ninformation, and other oncology data are processed through the HARMON-E agen-\ntic extraction pipeline (center) to produce standardized, structured database entries\n(right). Each piece of clinically relevant information is extracted, validated, and orga-\nnized into predefined entity-attribute pairs suitable for clinical research and decision\nsupport. The system processes heterogeneous inputs including progress notes, pathol-\nogy reports, radiology impressions, and scanned PDFs, transforming approximately\n180 documents per patient into comprehensive structured records.\n3 Methods\n3.1 Problem Formulation\nWe consider a set of patients\nP = {p1, p2, . . . , pn}\nand a predefined collection of clinical entities relevant to oncology (e.g., Biomarker,\nCancer Related Medication, TNM Stage Group). Each entity Ej consists of a\ncollection of attributes\nA(Ej) = { aj,1, aj,2, . . . , aj,kj},\nkj := |A(Ej)|.\nwhere each attribute can be a categorical label (with a fixed value set), a date, a\nnumeric value, or unstructured text. Here, kj denotes the number of attributes defined\nfor entity Ej and can differ by entity (see Table 1). Given a corpus of unstructured or\nsemi-structured oncology documents\nDi = { d1, d2, . . . , dm}\nfor each patient pi,\n8\n"}, {"page": 9, "text": "Fig. 3: The tree structure illustrates the decomposition of a patient record into six\nprimary entity categories (Medication, Biomarker, Diagnosis, Staging, Surgery, and\nRadiation), each containing multiple typed attributes. Representative examples from\nreal oncology data are provided in italics below each attribute. This structured schema\nensures consistency across heterogeneous clinical documentation and enables valida-\ntion of extracted data against clinical standards. The complete model encompasses 16\nentity types with 103 distinct attributes (subset shown for clarity).\nour goal is to automatically extract all valid instances of these entities. Equivalently,\nwe want a structured output Si for every patient pi containing all detected entities\n{ E1, . . . , EJ} and their attribute values. Thus,\nSi =\nJ[\nj=1\nˆEj,\nwhere J is the total number of clinical entity types considered, and each ˆEj is a set\nof extracted instances of type Ej. Table 1 lists 16 oncology entity types considered in\nthis study.\nWe evaluate the quality of extraction by comparing the system outputs against a\ngold standard dataset. For each entity type, we assess:\n• Entity-Level Recall: Do we capture all correct entity instances mentioned in\npatients’ notes?\n• Entity-Level\nPrecision: Do we avoid producing extra or incorrect entity\ninstances?\n• Attribute-Level Accuracy: Conditioned on having a correct entity instance, are\nall attributes (e.g., dates, biomarker results) accurate?\n9\n"}, {"page": 10, "text": "Fig. 4: System workflow of HARMON-E. The system accepts raw, unstructured\npatient documents (HTML notes and/or scanned PDFs normalized as in Sec. 3.2)\nand processes them through three main components: Retrievers, LLM Synthesizers,\nand Collators. Retrievers extract relevant segments for each targeted oncology entity;\nLLM Synthesizers transform these segments into structured attribute-value pairs; and\nCollators merge, validate, and resolve any dependencies among the extracted entities.\nThe framework supports multiple pipeline strategies—from single-step to multi-step\nor topical extraction—accommodating a diverse range of real-world workflows. The\nfinal output is a patient-level structured data record spanning multiple oncology con-\ncepts (e.g., biomarkers, medications, TNM staging).\nWe further perform alignment between predicted entity instances and ground-truth\nreferences (e.g., via root-based or weighted matching) to compute these metrics\nsystematically as discussed in the Section 4.\n3.2 Document ingestion and normalization\nScanned PDFs are transcribed to page-level Markdown using a vision–language\nmodel fine-tuned on clinical documents. We then invoke two LLM calls for\n(i) page-to-document segmentation and doc typing, and (ii) metadata extraction\n(encounter date, report title, identifiers). The resulting normalized text feeds the\nretrieval–synthesis–collation pipeline.\n10\n"}, {"page": 11, "text": "3.3 Architecture\nWe design a modular, domain-focused architecture for extracting oncology-specific\ndata variables at scale. The architecture decomposes the problem into three compo-\nnents and integrates them into one of several pipeline strategies, each specializing in\ndifferent document-processing scenarios.\n3.3.1 Key Features\n• Modular Component Architecture: We expose interchangeable building blocks\n(retrievers, LLM synthesizers, and collators) to accommodate different data formats\nand use cases.\n• Multiple Pipeline Strategies: We implement multiple component combinations\n(Single-Step, Multi-Step, Topical Extraction, Sequential Document Analysis), each\nwith distinct retrieval-extraction flows.\n• Flexible Configuration System: Users can seamlessly modify pipeline parame-\nters (e.g., queries, prompts, pattern matchers) without altering core logic.\n• Strong Typing & Validation: Entity attributes are rigorously typed (categorical,\ndate, numeric, etc.) to ensure consistent outputs and validate data integrity.\n• Dependency Management for Complex Extractions: Certain extractions rely\non previously resolved attributes (e.g., a medication collator may need a confirmed\ndiagnosis date). Our architecture manages these dependencies explicitly.\n3.3.2 Core Data Models\nAt the heart of the system are entities and attributes. Each entity Ej includes:\nA(Ej) = { (nj,i, tj,i, Vj,i) }kj\ni=1,\nwhere nj,i is an attribute name (e.g., biomarker tested), tj,i is its type (e.g., Date,\nInteger, or Categorical), and Vj,i is an optional finite set of valid values for\ncategorical attributes (e.g., {Positive, Negative}).\nWe also incorporate strong typing by rejecting any extractions that violate the\ndeclared schema (e.g., a numeric field with an invalid string). In practice, this ensures\nconsistent representation across different pipeline stages.\n3.3.3 Main Components\n1. Retrievers\nA retriever receives text input and yields a list of candidate chunks (snippets) relevant\nto the extraction goal. Clinical notes are chunked deterministically into sentence-\nbounded windows, each capped at M characters, with a one-sentence overlap between\nconsecutive windows. We consider two broad categories:\n• Vector Retriever: Embedding-based similarity search to identify chunks that\nsemantically match a query (e.g., a disease name or biomarker).\n• Regex Retriever: Regex-based pattern matching to capture well-defined textual\ncues (e.g., specific drug names, standard biomarkers).\n11\n"}, {"page": 12, "text": "How queries are defined. Retrieval queries are entity-conditioned. For a target\nentity Ej and attribute subset S, the user configures {q1, . . . , qm}, where m is the\nnumber of queries generated for that extraction call.\nExample queries. Diagnosis (surgery): “Has the patient undergone any resection?”,\n“Is there a mention of mastectomy?”, “What is the surgery date?”\nBiomarker (BRAF): “When was BRAF last tested?”, “What was the BRAF test\nresult?”, “How did the lab interpret the BRAF result?”\nAlgorithmically, a Vector Retriever can be summarized as:\nAlgorithm 1 Vector Retriever (Abstract)\nRequire: A set of chunk texts, an embedding function ϕ(·), a user query q, and\nretrieval size k.\nCompute the embedding ϕ(q) for the query.\nFor each chunk text d, compute ϕ(d).\nRank chunks by cosine similarity ⟨ϕ(q), ϕ(d)⟩.\nReturn top k most similar chunks.\nFor each query qr, top k (kr) is the number of highest-scoring chunks that is\nretained after ranking; larger kr increases recall at the cost of more noise and latency.\nExamples: surgery-identification queries use kr=16 to catch sparse mentions;\nsurgery-detail queries (e.g., date, margins) use kr=8; biomarker result-line queries use\nsmall kr (e.g., 4) since evidence is localized.\n2. LLM Synthesizer\nGiven a chunk or chunks of text, the LLM Synthesizer (e.g., an LLM such as GPT-\n4) is prompted to extract a structured representation. The prompt typically includes\ninstructions about the schema to be returned. For instance, we might request that\nthe LLM produce a JSON-like object with name, dosage, and start date fields for a\nCancerMedication entity. Concretely:\n1. Concatenate: The chunk text plus any instructions (e.g., templates, format\nspecification).\n2. Infer: The LLM extracts attribute values from the chunk’s content.\n3. Emit: A structured representation that respects the required fields and data types.\n3. Collators\nA collator accepts multiple partial extractions and merges or filters them into a final,\nvalidated structure. Here, a collator is a deterministic post-processing module (not\nan LLM unless explicitly stated) that canonicalizes values, enforces type/value-set\nconstraints, deduplicates, and resolves conflicts via simple precedence rules. Typical\ncollator functions include:\n• Deduplication: If multiple extractions have the same name and date, unify them.\n• Validation: Check whether attributes match type constraints or known value sets.\n12\n"}, {"page": 13, "text": "• Conflict Resolution: If two extractions have contradictory attributes, apply domain\nlogic to keep the most recent or plausible instance.\nCollation can be formalized as:\nCollate\n\u0000{R1, . . . , Rm}\n\u0001\n= bR,\nwhere Ri are sets of attribute-value pairs from the LLM Synthesizer, and bR is a single\nconsolidated set of entity instances. If the collator depends on other entity data (e.g.,\na known diagnosis date), it can be passed in as an auxiliary input, ensuring consistent\ndomain constraints across different entity types. Collators can be chained together to\nproduce complex logical strategies.\n3.3.4 Pipeline Types\nWe integrate these components into four canonical pipelines for this specific project,\neach reflecting a distinct approach to retrieving, synthesizing, and merging entity\ninformation. The modular components, however, allows us to create even more complex\npipelines which are out of the scope of this work:\n(a) Single-Step Pipeline\nA straightforward, single-pass strategy. The pipeline first retrieves all relevant chunks\nusing either vector or regex methods. The LLM Synthesizer then processes each chunk\nin one shot to produce the entity attributes, and a collator merges the results. This\nis well suited for entities that appear in well-defined contexts or are consistently\nmentioned in the text such as biomarkers.\n(b) Multi-Step Pipeline\nA multi-stage approach for more complex or heterogeneous entities. First, we identify\nwhich variants or subtypes are mentioned (e.g., enumerating possible medications).\nNext, for each identified subtype, we extract more detailed attributes with a tar-\ngeted LLM prompt. The collator then merges these partial extractions. This approach\nreduces confusion for the LLM when multiple entity types are possible.\n(c) Topical Extraction\nWe split a patient’s documents into coherent topics (e.g., radiology reports, molecu-\nlar testing, social history) and systematically process each topic with a specialized\nprompt. This is particularly useful when distinct sections of text require different\ndomain knowledge or extraction strategies, but must ultimately be combined into a\nsingle patient-level record.\n(d) Sequential Documents Analysis\nWe treat each document as an individual unit, performing retrieval or chunking within\nthat document, then calling the LLM. The collator merges the partial outputs across\nall documents, preserving the document lineage. This is valuable when the timing and\n13\n"}, {"page": 14, "text": "source of information is critical, such as for pathology or surgical reports that must\nbe chronologically tracked.\nPipeline selection policy\nWe choose the pipeline by entity complexity and context: Single-Step for localized\ncues with few attributes (e.g., Biomarker); Multi-Step for entities with variants and\nmany attributes (e.g., Medication); Topical when sections require specialized prompts\n(e.g., radiology vs. molecular testing); Sequential Documents when provenance and\nchronology are critical (e.g., Staging, surgery timelines).\n3.3.5 Dependency Management\nWhen the extraction for an entity (e.g., Cancer Medication) requires context from\nan upstream entity (e.g., Primary Cancer Condition), the pipeline enforces a depen-\ndency order among collators. For instance, if the medication collator depends on a\nconfirmed diagnosis date, the pipeline first finalizes the Primary Cancer Condition\ncollations before generating medication instances. This ensures consistent references\n(e.g., no medication entry without a corresponding diagnosis period).\nIn summary, our methods combine modular retrieval (via patterns or embed-\ndings), LLMs (for robust text-to-structure synthesis), and domain-driven collation (for\nvalidation, deduplication, and dependency resolution). This general design accommo-\ndates a wide array of oncology-specific extractions, from straightforward biomarkers\nto multi-document treatments and staging workflows. Implementation specifics,\nincluding representative prompt templates and retrieval parameters, are detailed in\nSupplementary Sections S-2 and S-3.\n4 Evaluation\nIn this section, we outline our approach to assessing the performance of HARMON-E in\nthe extraction of oncology variables from clinical notes. Our evaluation targets three\ncore objectives:\n1. Entity-Level Recall: Confirming that the pipeline correctly identifies all entities\nrecorded in the ground truth.\n2. Precision: Ensuring the pipeline does not introduce false positives, i.e., entities\nnot present in the ground truth.\n3. Attribute-Level Accuracy: Verifying that all attribute values match the ground\ntruth for each correctly extracted entity.\nTogether, these objectives confirm that the pipeline not only captures all relevant\ninformation but does so accurately at both the entity and attribute levels.\n4.1 Entity Alignment Methods\nAfter running the pipeline, we must determine whether the extracted entities corre-\nspond to those in the ground truth data. This alignment step is crucial for measuring\nentity-level recall, precision, and attribute-level accuracy. Specifically, we employ two\nmethods: Root-based alignment and Weighted alignment.\n14\n"}, {"page": 15, "text": "Fig. 5: Overview of Two Entity Alignment Methods. (A) Root-Based Align-\nment: An alignment is established only if both entities share the same root attribute\n(e.g., “medication” with value “Trastuzumab”), making other attributes (such as\nstart dates) irrelevant for basic alignment. (B) Weighted Alignment: Each attribute\n(e.g., surgery type, surgery date, body site) contributes a partial score based on\na predefined weight. If the sum of matching attributes meets or exceeds a threshold\n(e.g., 0.9), the ground-truth and predicted entities are aligned.\n4.1.1 Root-Based Entity Alignment\nFor certain entity types, a specific attribute—the root attribute—uniquely identifies\nthat entity. We thus enforce the condition that if these root attributes do not match,\nthe entities cannot be aligned.\nalignroot(e1, e2) =\n(\n1\nif root(e1) = root(e2),\n0\notherwise.\n(1)\nRoot-based alignment applies to entities whose identity is fixed by a single field\n(Table 2; e.g., biomarker tested for Biomarker Summary, medication for Cancer\nRelated Medication).\n4.1.2 Weighted Entity Alignment\nFor entities where the root attribute may be ambiguous or prone to lexical variation,\nwe adopt a weighted alignment strategy. Each attribute is assigned a weight reflecting\nits importance in identifying that entity. The alignment score between a ground truth\nentity e1 and an extracted entity e2 is computed by:\n15\n"}, {"page": 16, "text": "Table 2: Alignment schemes by entity (anchors and implications).\nEntity\nScheme\nAnchor / decisive fields\nImplication\nBiomarker Summary\nRoot\nbiomarker tested\nDifferent biomarkers never\nalign even if dates match\n(e.g., BRAF vs NRAS).\nCancer Related\nMedication\nRoot\nmedication\nDrug names must match;\nother fields are irrelevant for\nthe root check (e.g.,\nTrastuzumab vs Paclitaxel do\nnot align).\nCancer Related Surgery\nWeighted\nsurgery date, body site >\nsurgery type\nDecisive fields dominate;\nlexical variants of type may\nstill align (e.g., “wide local\nexcision” vs “re-excision”).\nStaging\nWeighted\nstage date, stage value,\nstage type\nPrioritize date and value;\nresolves minor notation\ndifferences (e.g., pT2N0M0 vs\npT2 N0 M0 align; cT2N0M0 vs\npT2N0M0 do not).\nalignweighted(e1, e2) =\nk\nX\ni=1\nwi · match(a1i, a2i),\n(2)\nwhere wi is the importance weight of attribute i, and match(a1i, a2i) indicates a match\nof attribute values. A threshold τ then determines alignment:\naligned(e1, e2) =\n(\n1\nif alignweighted(e1, e2) ≥τ,\n0\notherwise.\n(3)\nFor entities with lexical variation, we use weighted alignment: attributes receive\nweights under the guidance of oncology experts, with higher weights on clinically deci-\nsive fields (Table 2; e.g., surgery date, body site) and lower weights on descriptive\nfields (e.g., surgery type). The pairwise score is Eq. (2); alignment holds when it\nexceeds a threshold τ.\n4.1.3 Entity Alignment Process\nThe entity alignment process follows specific constraints regardless of whether root-\nbased or weighted alignment is employed. The fundamental requirement is the\nuniqueness constraint, which ensures that each ground truth entity aligns with at most\none extracted entity, and vice versa. This one-to-one mapping is essential to prevent\nartificial inflation of recall metrics.\nOnce entities are aligned, the process identifies a driver attribute that serves as the\nanchor for computing entity-level recall and precision metrics. The selection of this\ndriver attribute depends on the alignment method used. In root-based alignment, the\nroot attribute itself—such as biomarker tested or medication—naturally serves as\nthe driver. For weighted alignment, the system selects the attribute that received the\nhighest weight assignment as the driver attribute. This driver attribute then becomes\nthe primary reference point for evaluating how well the extraction system captures\nthe essential characteristics of each entity.\n16\n"}, {"page": 17, "text": "4.2 Manual Evaluation\nBeyond automated metrics, we developed a manual evaluation protocol to assess real-\nworld pipeline performance through clinical expert review. This approach acknowl-\nedges that divergences between model outputs and ground truth may stem from either\ngenuine model errors or inconsistencies in the original manual abstractions.\nGiven the resource-intensive nature of comprehensive manual review, we imple-\nmented a strategic sampling method that prioritizes cases of disagreement. We\nquantified divergence through a disagreement scoring mechanism, formally defined\nas DS(p) in Supplementary Methods S4, that counts mismatched attributes between\npipeline outputs and ground truth for each patient. Patients were ranked by their dis-\nagreement scores, with the top 50 highest-scoring cases per entity type selected for\nexpert review. This targeted approach concentrates human expertise where it provides\nmaximum insight into system performance limitations.\nClinical abstraction specialists with oncology domain expertise reviewed selected\npatient records following a standardized protocol. The review process minimized bias\nby presenting complete patient records without indicators of data source (pipeline\nvs. ground truth). Reviewers classified each extracted item as correct, incorrect, or\nmissing—categories detailed in Supplementary Methods S4 along with the complete\nevaluation workflow.\nTwo complementary performance metrics were derived from this classification:\nan Acceptance Score and a Missing Rate. These metrics, whose mathematical for-\nmulations are provided in Supplementary Methods S4, characterize both extraction\naccuracy and completeness—critical dimensions for clinical deployment.\n4.3 Dataset\nDataset Description: Our study draws upon a real-world dataset of 2,250 melanoma\npatients, each contributing an average of approximately 180 unstructured clinical doc-\numents. These documents originated from hundreds of distinct healthcare institutions,\nreflecting considerable variability in document formatting, language use, and clinical\nnotation styles. In total, the corpus is composed of roughly 50% HTML-based notes\nexported directly from the EHR systems, with the remaining 50% comprising scanned\nPDF files. Even though the system was tested on Melanoma patients, the same sys-\ntem can be used to get these results by only modifying the valuesets while using same\nprompts.\nTo streamline data preparation, all HTML-based documents were treated as single-\npage entities, irrespective of their actual text length or content density. In contrast,\nscanned PDF documents were processed at the physical page level, preserving the\npagination structure that closely mirrors real-world clinical workflows. In Figure 6, we\nplot the frequency distribution of per-patient page counts (combining HTML “pages”\nand PDF pages) for the testing subset. Notably, the distribution exhibits a wide range,\nwith some patients having as few as tens of pages while others have more than a thou-\nsand. This spread underscores the variable nature of oncology documentation: some\npatients receive only a limited number of reports (e.g., routine follow-up or localized\n17\n"}, {"page": 18, "text": "Fig. 6: Distribution of the Number of Pages per Patient. Each bar represents\nthe number of patients grouped by their total page count, where HTML-based docu-\nments are treated as single pages while PDF pages are counted individually.\ntreatment), whereas others undergo extensive diagnostic workups, multi-line thera-\npies, and second opinions, producing voluminous records. Such variability in document\nlength and format is precisely what makes the automation of data abstraction both\nchallenging and clinically valuable.\nGround Truth: The dataset was curated by a team of qualified oncology data\nabstractors (ODS-Cs), employing standardized abstraction guidelines refined over a\ntwo-year span. The abstractors recorded over one hundred oncology-specific data ele-\nments at the patient level, including (but not limited to) primary cancer diagnoses,\ntreatment regimens (e.g., chemotherapies, targeted therapies, immunotherapies),\ndiagnostic test results (e.g., biomarker findings), disease progression events, and\ncomorbidities. This labor-intensive manual curation process allowed for the creation\nof a rich, high-fidelity “ground truth” dataset, forming the reference standard against\nwhich we benchmark our extraction pipeline.\n4.4 Cohort Selection\nWe analyzed a retrospective melanoma cohort assembled from participating practices\nunder active data-use agreements. The analytic set comprised 2,250 adult patients\nwith histologically confirmed melanoma and available longitudinal unstructured\ndocumentation (clinical notes and scanned reports).\nDevelopment–test split.\nThe final eligible population (n = 2 250 unique patients) was randomly partitioned\ninto 50 % development (n = 1 125) and 50 % hold-out test (n = 1 125). Randomi-\nsation was stratified by (i) health-system cluster, (ii) year of index diagnosis, and (iii)\nAJCC stage at diagnosis to ensure balanced distribution of site-specific coding styles\nand disease severity.\n18\n"}, {"page": 19, "text": "The first half was used iteratively for:\n• Prompt Development and Refinement: Crafting and optimizing large language\nmodel (LLM) prompts to ensure comprehensive coverage of target oncology entities\nand minimize ambiguity in free-text interpretation.\n• Pipeline Configuration and Tuning: Adjusting individual components—such as\nretrievers, entity collators, and conflict-resolution logic—to accommodate the\ndiverse formats and terminologies present in both HTML notes and PDF scans.\n• Model Selection: Comparing the performance of candidate large language models,\nembedded retrieval methods, and specialized domain rules to choose the pipeline\nconfiguration that maximized extraction accuracy while preserving computational\nefficiency.\nOnce these elements were established, we used the remaining 1,125 patients as our\nheld-out test cohort for final evaluation. This two-phase approach (development and\ntest) was designed to prevent data leakage and ensure that performance metrics\naccurately reflect the system’s ability to generalize to new patients and institutions.\nTable S1 compares key baseline characteristics of the hold-out cohort against the\nSEER melanoma registry.\n5 Results\nWe summarize in Table 3 the automated evaluation results for each Entity averaged\nover all its attributes. For each entity, we report Precision, Recall, and F1-score,\ncomputed according to the alignment methods described in Section 4.1. Furthermore,\nwe present attribute level results in Fig 7. Overall, the pipeline demonstrates robust\nperformance across a wide range of oncology-related attributes, frequently achieving\nF1-scores above 90%. Below, we highlight notable trends and address a few areas with\nrelatively lower scores.\nTable 3: Averaged Results For Different Entities\nEntity Name\nPrecision\nRecall\nF1\nBiomarker\n0.9890\n0.9722\n0.9806\nBiopsy\n0.8953\n0.8631\n0.8789\nClinical Trial\n1.0000\n0.9615\n0.9800\nComorbidities\n0.8000\n1.0000\n0.8889\nDistant Metastasis\n1.0000\n0.8182\n0.9000\nDiagnosis\n0.9846\n0.9600\n0.9722\nFamily History\n1.0000\n0.8738\n0.9323\nImaging\n0.6940\n0.9337\n0.7962\nMedication\n0.9722\n0.9525\n0.9620\nNicotine Use Status\n1.0000\n1.0000\n1.0000\nPatient Status\n1.0000\n1.0000\n1.0000\nRadiation\n0.9481\n0.9778\n0.9627\nRecurrence Status\n0.9651\n0.7445\n0.8405\nStaging\n0.8771\n0.9625\n0.9178\nSurgery\n0.9937\n0.9633\n0.9782\nSurgery Observations\n0.9939\n0.9899\n0.9919\n19\n"}, {"page": 20, "text": "Table 4: Patient-level performance on the hold-out cohort (n = 1 125). Values are\nmacro-averages across the 16 entities; ± denotes 95% confidence intervals were com-\nputed using the Bag-of-Little-Bootstraps (BLB): 10 subsets of size m=128 and 100\nbootstrap replicates per subset\nConfiguration\nPrecision(%)\nRecall(%)\nF1 (%)\nGPT-4o Single-Step\n78.2±0.6\n70.7±0.7\n74.3 ±0.6\nHARMON-E (w/o collator)\n82.1±0.5\n90.7±0.6\n86.2±0.5\nHARMON-E (full)\n94.1±0.4\n92.4 ±0.5\n93.2±0.4\n5.1 Baselines and Ablation Analysis\nRobust benchmarking of patient-level extraction is challenging because most prior\nwork in clinical NLP reports note- or sentence-level metrics. Our gold standard dataset\nenables the patient-level validation, which is closer to real-world use cases. We\nconstructed three reference configurations that satisfy the same output schema and\nunit-of-analysis:\n1. GPT-4o Single-Step. All notes for a patient are concatenated (truncated to\n32 k tokens) and passed to GPT-4o (June-2024 snapshot) with a single prompt\nrequesting the full JSON schema. No retrieval, no self-reflection, no collator.\n2. No-Collator Ablation. Identical to HARMON-E except consolidation, valida-\ntion, and dependency-resolution rules are disabled; LLM generations are used\n“as-is.”\n3. HARMON-E. Full HARMON-E pipeline with distinct configuration for each\nentity\nAll baselines were run on the same 1 125-patient hold-out cohort. Metrics are\nmacro-averaged across the 16 entities, 95% confidence intervals were computed using\nthe Bag-of-Little-Bootstraps (BLB) due to compute constraints: 10 subsets of size\nm=128 (≈N 0.7) and 100 bootstrap replicates per subset with multinomial weights to\nsize N; percentile intervals aggregated across subsets.\nTable 4 presents precision, recall and F1. HARMON-E exceeds the GPT-4o\nsingle-step by +20 percentage points (pp) in F1 (p < 10−3), driven mainly by\nimprovements in date-sensitive attributes (Medication +28.6 pp). Disabling the col-\nlator degrades macro-F1 by 6.3 pp, confirming the value of our dependency-aware\npost-processing.\nThe GPT-4o baseline required a ∼5× larger average prompt (29.4 k vs 6.1 k tokens\nper patient) and 11.2× higher inference-time latency (median = 94 s vs 8.4 s). Token\nsavings are principally from targeted retrieval.\n20\n"}, {"page": 21, "text": "Fig. 7: F1-Scores of the evaluated attributes\n21\n"}, {"page": 22, "text": "5.2 Entity-Level Performance\nDiagnosis-Related Entities\nFor Diagnosis (diagnosisType, primaryCancer, diagnosisDate, tumorLocation,\nhistology), the pipeline achieves F1-scores of 95–98%. Diagnosis dates are occa-\nsionally challenging, but the model still maintains over 95% F1, a testament to the\npipeline’s date normalization and validation steps.\nThe Biomarker entity, including attributes such as name, testingMethod, date,\ninterpretation, categoricalValue, and numericalValue, consistently shows F1-\nscores in the mid-to-upper 90%s. Amino acid change details (aminoAcidChange,\naminoAcidChangeType) also approach or exceed 98%. This underscores the pipeline’s\nrobustness in capturing structured molecular test findings from sometimes lengthy\npathology or genetic reports.\nTNM staging attributes (tStage, nStage, mStage, groupStage) have F1-scores\ngenerally between 90% and 95%. stagingDate, however, is relatively lower (88.42%\nF1). We observe that temporal references to staging can appear in summary\nparagraphs or at multiple time points, leading to partial confusion.\nRecurrence Status\nattributes\n(statusType,\nassociatedDiagnosis,\ndiagnosisDate, bodySite) exhibit moderate-to-high performance, with statusType\nreaching 86.87% F1 and diagnosisDate at 76.77%. Distant Metastasis attributes\n(e.g., associatedDiagnosis, diagnosisDate, bodySite) consistently outperform\nrecurrence, hovering near 90%. The slight drop in diagnosisDate for recurrence\nstems from complexities in distinguishing the actual date of recurrence which can be\ninferred from imaging, biopsy or even clinician judgement. If the margin of error is\nincreased to +-14 days, the system achieves ≈89% F-1 score.\nTreatment-Related Entities\nThe Cancer Related Medication entity exhibits consistently strong results, with\nmany attributes (name, treatmentSequence, route, dosageQuantity, etc.) achieving\nF1-scores of 95% or higher. Several fields (changedDosageQuantity, changedDosage,\nchangedDosageUnits, and related attributes) reach or exceed 99% F1, indicating that\nthe pipeline accurately captures even nuanced treatment modifications.\nBy contrast, date-related attributes (e.g., startDate, endDate) exhibit slightly\nlower performance. For instance, endDate achieves an F1-score of 85.47%. While still\nhigh, This aligns with known difficulties in extracting date information from unstruc-\ntured text, potentially due to ambiguous or partial documentation. If the margin of\nerror is increased to +-7 days, the system achieves ≈92% highlighting the ability of\nthe system to land at a nearby date, even if not completely correct. This is due to\nthe difficulties in analyzing the date of the last dosage of the medication which is very\noften derived and not explicitly stated.\nThe pipeline excels in Surgery, where surgeryType, surgeryName, surgeryDate,\nand surgeryOutcome all exceed 95% F1, indicating robust detection of procedu-\nral information. surgerySite stands at 95.34% F1, which still signifies strong\nperformance.\nUnder Surgery Observations, most attributes (e.g., tumorThickness, tumorPCR,\nlymphNodePCR) are extracted with near-perfect precision and recall. Even for more\n22\n"}, {"page": 23, "text": "complex fields like lymphNodeSampled (∼96% F1), the pipeline demonstrates minimal\nerror rates, suggesting that specialized prompts for pathology details are effective.\nRadiation attributes (radiationModality, radiationSite, dosageDelivered,\netc.) frequently approach 98–99% in F1, demonstrating high precision and recall. The\ndate-related fields (startDate, endDate) again pose slight challenges (both at 88.89%\nF1), but remain in a range considered acceptable for large-scale automated extractions.\nOther Entities\nThe attributes name, status, dateOfOnset under the Comorbidities entity exhibit\nsolid performance near 88.89% F1. Although lower than some other entities, the\npipeline still captures the majority of comorbidity data correctly despite variability in\nhow clinical notes reference chronic conditions.\nFamily History attributes—relationship, condition, and onset—achieve F1-\nscores in the low-to-mid 90% range. The pipeline occasionally struggles with ambigu-\nous family relationships or missing explicit onset dates, but it still yields a high level\nof accuracy overall.\n5.3 Summary of Automated Evaluation\nIn conclusion, the 73 out of 103 Entity–Attribute pairs surpass 90% F1-score, high-\nlighting the effectiveness of the agentic multi-step approach outlined in Section 3.3.\nEntities with more complex or date-dependent attributes (e.g., certain Biopsy fields,\nsome Imaging details) see moderate performance dips, underscoring the inherent chal-\nlenges of inconsistent or ambiguous date references in clinical text. Nonetheless, these\nresults illustrate that the proposed HARMON-E pipeline provides robust and compre-\nhensive extraction of oncology-specific data, setting the stage for efficient downstream\ncuration and analysis.\n6 Discussion\nOur proposed HARMON-E pipeline systematically addresses the challenge of extracting\nfine-grained oncology data from heterogeneous EHR notes. The strong performance\nreported in Table 3, with a approximately 75% Entity–Attribute pairs exceeding 90%\nF1-score, underscores the effectiveness of a multi-step, agentic approach to clini-\ncal information extraction. Here, we examine the factors behind these results, the\nlimitations inherent to current methods, and potential avenues for future refinement.\n6.1 Robustness of the Agentic Approach\nThe crux of our pipeline’s success lies in the hierarchical and modular decomposition\nof complex tasks. Rather than forcing a single model to infer all oncology attributes\nfrom large, fragmented documents, we divide the workload into discrete steps such\nas retrieval, LLM-based synthesis, and collation. This strategy closely mimics human\nworkflows where different abstraction tasks (e.g., medications vs. biomarkers) demand\ndifferent retrieval contexts and domain-specific prompts.\n23\n"}, {"page": 24, "text": "The high scores across most medication attributes, notably around dosage adjust-\nments and medication status, point to the efficacy of using agentic iterative prompts.\nIn addition, the pipeline’s ability to detect multiple lines of therapy and handle par-\ntial or conflicting data suggests that multi-step retrieval—where each pass narrows\nthe focus—reduces confusion that might otherwise arise in single-shot extractions.\n6.2 Why not ClinicalBERT/SciBERT?\nThose models (and their fine-tuned variants) emit token-level BIO tags or short rela-\ntion triples per document; transforming such outputs into patient-level, longitudinal\nabstractions requires bespoke heuristics (e.g., cross-document clustering, conflict res-\nolution, temporal alignment) that vary across data partners and are not publicly\nstandardised. Any comparison would therefore conflate intrinsic model performance\nwith pipeline engineering choices and is unlikely to offer actionable insights. Despite\nthat, we ran additional experiments with self-designed postprocessing script and have\nsummarized the results in Supplementary S7.\n6.3 Manual Adjudication Results\nTo complement the automated evaluations, we integrated the pipeline outputs into a\ndata curation platform (as shown in Fig. 8) and conducted a manual review, or adju-\ndication, of 13,609 individual data points spanning Radiation, Surgery, Medication,\nStaging, and Diagnosis entities. As shown in Figure 9 (a representative dashboard\nexcerpt), reviewers with oncology data abstraction expertise assessed each extracted\nentry in a patient’s record. Specifically, they had three options for each item:\n• Approve (Correct): The pipeline output required no edits.\n• Edit (Incorrect): The pipeline output existed but needed to be modified (e.g., a\nwrong date or attribute value).\n• Add (Missing): The pipeline missed an item or field that reviewers deemed\nrelevant.\nThe adjudication process demonstrated that 94.1% of extracted items were directly\napproved without changes. This high approval rate highlights the real-world robust-\nness of the pipeline. In contrast, 4.2% of data points were categorized as missing\nentries, signifying that the pipeline had overlooked relevant details. An additional\n1.7% required edits to correct partially incorrect information.\nFigure 9 also breaks down the adjudication outcomes by entity type. Medication,\nRadiation, and Diagnosis exhibit particularly high direct approvals (routinely over\n90%). Where missing data did occur, it was often tied to ambiguous dates or undocu-\nmented changes. Edits tended to involve small discrepancies such as inaccurate route\nor dosage in medication entries or minor staging detail mismatches.\n6.4 Challenges with Date Fields and Context Ambiguity\nDespite the overall strong performance, several date-related attributes (startDate,\nendDate, stagingDate, etc.) manifest lower recall or precision relative to simpler\n24\n"}, {"page": 25, "text": "Fig. 8: Manual adjudication interface for expert validation of HARMON-\nE extractions. The data curation platform displays extracted clinical entities in\na three-panel layout: (left) hierarchical list of all extracted entities organized by\ntype (Diagnosis, Staging, Distant Metastasis, etc.) with entity counts and comple-\ntion indicators; (center) detailed view of the selected entity showing all extracted\nattributes, with the example showing a Primary Melanoma diagnosis dated February\n11, 2019 (Deidentified/Shifted date), at anatomical location “Upper Extremity/Shoul-\nder”; (right) source clinical note with automatic highlighting of the relevant text\npassage from which the information was extracted, enabling traceable validation. The\nhighlighted yellow section shows the exact source text supporting the extraction, while\nthe orange boxes indicate the specific data points under review. Clinical experts can\ndirectly approve extractions using the “Mark as complete” button, edit incorrect values\ninline, or add missing information not captured by the automated pipeline. This inter-\nface facilitated the review of 13,609 data points across 50 high-disagreement patients,\nachieving a 94.1% direct approval rate as described in Section 6.3.\ncategorical fields. This is partly because real-world oncology notes often contain mul-\ntiple, sometimes conflicting date references: for instance, the medication’s original\nprescription date may differ from the administration start date, and not all providers\nconsistently document time points in an unambiguous format.\nSimilarly, entities like Imaging can appear sporadically across radiology reports,\ndischarge summaries, and referral letters, each containing references to different imag-\ning sessions. The pipeline may over-detect or conflate sessions if a single retrieval chunk\ncontains synonyms or abbreviations pointing to multiple studies. Further refinement\nof chunking strategies and more sophisticated context tracking could address these\nlimitations, for example by leveraging specialized date resolution modules or advanced\ntemporal reasoning prompts within the LLM.\n25\n"}, {"page": 26, "text": "Fig. 9: HARMON-E demonstrates high direct-approval rates (94.1%), with low missing\n(4.2%) and required-edit (1.7%) rates further corrobarting the automated metrics.\n6.5 Implications for Clinical Research and Real-World\nEvidence\nThe breadth and depth of high-accuracy extraction demonstrated by HARMON-E show\npromise for scaling up real-world data (RWD) curation in oncology. Automating\nthe abstraction of TNM staging, biomarker statuses, and multi-line therapies has\nthe potential to expedite clinical trial matching, pharmacovigilance, and precision\nmedicine initiatives. Indeed, if integrated into routine EHR workflows, such an agen-\ntic pipeline could accelerate retrospective analyses of large cancer cohorts while\nmaintaining data fidelity akin to manual chart reviews.\nMoreover, by capturing a broad range of attributes (e.g., familyHistory,\ncomorbidities, nicotineUseStatus), the system can support comprehensive epi-\ndemiological studies that hinge on robust phenotypic representations. The precision\nseen in medication changes (e.g., dosageQuantity, doseChangeReason) also paves the\nway for granular analysis of treatment patterns over time.\n7 Conclusion\nWe presented HARMON-E, a modular, agentic framework for extracting structured\noncology data from heterogeneous, often voluminous EHR notes. By combining\ndomain-aware retrieval, large language models for context-sensitive synthesis, and\nrobust collation, HARMON-E demonstrates state-of-the-art performance on a compre-\nhensive range of oncology related clinical attributes. Automated evaluation highlights\nF1-scores exceeding 90% for most attributes, and our multi-step strategy effectively\naddresses the challenges of conflicting or fragmented documentation.\nIn automating the generation of curated, patient-level oncology data, HARMON-E\noffers an impactful tool for both clinical research and operational workflows. Our\n26\n"}, {"page": 27, "text": "ongoing efforts focus on extending the pipeline’s applicability to other solid tumors,\nrefining date extraction modules, and incorporating active learning mechanisms for\ncontinuous improvement. Ultimately, by reducing the resource-intensive burden of\nmanual abstraction, this framework stands to accelerate real-world evidence generation\nand support more personalized cancer care at scale.\nDeclarations\nData Availability\nThis study is retrospective, and no new data was generated. Due to access restrictions\nand the risk of re-identification, study data will not be shared externally.\nCode Availability\nPipeline configuration files, pseudo-code for helper scripts and representative prompt\ntemplates are released in the Supplementary. The authors agree to provide code\nsnippets at a reasonable request by non-competing entity.\nAcknowledgements\nThe authors gratefully acknowledge the oncology data abstraction and informatics\nteams at Ontada for their assistance in data access and clinical validation. The authors\nalso appreciate the engineering and data operations teams at Triomics for their work\nin implementing the HARMON-E infrastructure, including large-scale data ingestion,\nretrieval optimization, and validation pipelines.\nAuthor Contributions\nHrituraj Singh and Yanshan Wang contributed to the conceptualization of the study.\nMethodology was developed by Hrituraj Singh, and Shashi kant Gupta. Software and\nimplementation were carried out by Shashikant Gupta, Jerrin John Thomas, and\nArijeet Pramanik.. All authors reviewed and approved the final manuscript.\nFunding\nThis research was conducted with institutional support from Triomics and Ontada.\nNo external funding was specifically dedicated to this study.\nCompeting Interests\nHrituraj Singh, Shashikant Gupta, Arijeet Pramanik, Regina Schwind and Jerrin John\nThomas are employees of Triomics, Inc.\nLauren Wiener, Avi Raju, Jeremy Kornbluth, and Zhaohui Su are employees of\nOntada LLC.\nYanshan Wang declares no competing interests.\n27\n"}, {"page": 28, "text": "References\n[1] Uzuner, O., South, B.R., Shen, S., DuVall, S.L.: 2010 i2b2/va challenge on con-\ncepts, assertions, and relations in clinical text. Journal of the American Medical\nInformatics Association 18, 552–556 (2011)\n[2] Eftimov, T., Korouˇsi´c Seljak, B., Koroˇsec, P.: A rule-based named-entity recog-\nnition method for knowledge extraction of evidence-based dietary recommenda-\ntions. PloS one 12(6), 0179488 (2017)\n[3] Savova, G.K., Masanz, J.J., Ogren, P.V., Zheng, J., Sohn, S., Kipper-Schuler,\nK.C., Chute, C.G.: Mayo clinical text analysis and knowledge extraction sys-\ntem (ctakes): architecture, component evaluation and applications. Journal of the\nAmerican Medical Informatics Association 17(5), 507–513 (2010)\n[4] Savova, G.K., Tseytlin, E., Finan, S., Castine, M., Miller, T., Medvedeva, O.,\nHarris, D., Hochheiser, H., Lin, C., Chavan, G., Jacobson, R.S.: DeepPhe: A\nnatural language processing system for extracting cancer phenotypes from clin-\nical records. Cancer Research 77(21), 115–118 (2017) https://doi.org/10.1158/\n0008-5472.CAN-17-0615\n[5] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, L.u., Polosukhin, I.: Attention is all you need. Advances in Neural\nInformation Processing Systems (2017)\n[6] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 (2018)\n[7] Alsentzer, E., Murphy, J., Boag, W., Weng, W.-H., Jin, D., Naumann, T.,\nMcDermott, M.: Publicly available clinical bert embeddings. (2019)\n[8] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: Biobert: a\npre-trained biomedical language representation model for biomedical text mining.\nBioinformatics 36(4), 1234–1240 (2020)\n[9] Preston, S., Wei, M., Rao, R., Tinn, R., Usuyama, N., Lucas, M., Gu, Y., Weeras-\ninghe, R., Lee, S., Piening, B., Tittel, P., Valluri, N., Naumann, T., Bifulco, C.,\nPoon, H.: Toward structuring real-world data: Deep learning for extracting oncol-\nogy information from clinical text with patient-level supervision. Patterns 4(4),\n100726 (2023) https://doi.org/10.1016/j.patter.2023.100726\n[10] Lin, H., Ginart, J.B., Chen, W., Interian, Y., Gong, H., Liu, B., Upadhaya,\nT., Lupo, J., Hong, J., Braunstein, S., et al.: Oncobert: building an inter-\npretable transfer learning bidirectional encoder representations from transformers\nframework for longitudinal survival prediction of cancer patients (2023)\n28\n"}, {"page": 29, "text": "[11] Beltagy, I., Lo, K., Cohan, A.: Scibert: A pretrained language model for scientific\ntext. In: EMNLP (2019)\n[12] OpenAI: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)\n[13] OpenAI: Introducing GPT – 5. Accessed: 2025-10-18 (2025). https://openai.com/\nindex/introducing-gpt-5/\n[14] Bhattarai, K., Oh, I.Y., Sierra, J.M., Tang, J., Payne, P.R.O., Abrams, Z., Lai,\nA.M.: Leveraging gpt-4 for identifying cancer phenotypes in electronic health\nrecords: A performance comparison between gpt-4, gpt-3.5-turbo, flan-t5, llama-\n3-8b, and spacy’s rule-based and machine learning-based methods. JAMIA Open\n7(3), 060 (2024) https://doi.org/10.1093/jamiaopen/ooae060\n[15] Abedian, S., Sholle, E.T., Adekkanattu, P.M., Cusick, M.M., Weiner, S.E., Shoag,\nJ.E., Hu, J.C., Campion, T.R.J.: Automated extraction of tumor staging and\ndiagnosis information from surgical pathology reports. JCO Clinical Cancer\nInformatics 5, 1054–1061 (2021) https://doi.org/10.1200/CCI.21.00065\n[16] Kefeli, J., Berkowitz, J., Acitores Cortina, J.M., Tsang, K.K., Tatonetti, N.P.:\nGeneralizable and automated classification of TNM stage from pathology reports\nwith external validation. Nature Communications 15(1), 8916 (2024) https://doi.\norg/10.1038/s41467-024-53190-9\n[17] Gauthier, M.-P., Law, J.H., Le, L.W., Li, J.J.N., Zahir, S., Nirmalakumar, S.,\nSung, M., Pettengell, C., Aviv, S., Chu, R., Sacher, A., Liu, G., Bradbury, P.,\nShepherd, F.A., Leighl, N.B.: Automating access to real-world evidence. JTO\nClinical and Research Reports 3(6), 100340 (2022) https://doi.org/10.1016/j.\njtocrr.2022.100340\n[18] Pironet, A., Poirel, H.A., Tambuyzer, T., De Schutter, H., Walle, L., Matthei-\njssens, J., Henau, K., Van Eycken, L., Van Damme, N.: Machine learning-based\nextraction of breast cancer receptor status from bilingual free-text pathology\nreports. Frontiers in Digital Health 3, 692077 (2021) https://doi.org/10.3389/\nfdgth.2021.692077\n[19] Zeng, J., Banerjee, I., Henry, A.S., Wood, D.J., Shachter, R.D., Gensheimer,\nM.F., Rubin, D.L.: Natural language processing to identify cancer treatments with\nelectronic medical records. JCO Clinical Cancer Informatics 5, 379–393 (2021)\nhttps://doi.org/10.1200/CCI.20.00173\n[20] Meng, W., Mosesso, K.M., Lane, K.A., Roberts, A.R., Griffith, A., Ou, W., Dex-\nter, P.R.: An automated line-of-therapy algorithm for adults with metastatic\nnon–small cell lung cancer: Validation study using blinded manual chart review.\nJMIR Medical Informatics 9(10), 29017 (2021) https://doi.org/10.2196/29017\n29\n"}, {"page": 30, "text": "[21] Friedman, C., Shagina, L., Lussier, Y.: Automated encoding of clinical docu-\nments based on natural language processing. Journal of the American Medical\nInformatics Association 11, 392–402 (2004)\n[22] Chapman, W.W., Bridewell, W., Hanbury, P., Cooper, G.F., Buchanan, B.G.:\nA simple algorithm for identifying negated findings and diseases in discharge\nsummaries. Journal of Biomedical Informatics 34, 301–310 (2001)\n[23] Uzuner, O., Bodnari, A., Shen, S., Forbush, T., Pestian, J., South, B.R.: Evalu-\nating the state of the art in coreference resolution for electronic medical records.\nJournal of the American Medical Informatics Association 19(5), 786–791 (2012)\n[24] Sohn, S., Clark, C., Halgrim, S.R., Murphy, S.P., Chute, C.G., Liu, H.: Medxn:\nan open source medication extraction and normalization tool for clinical text.\nJournal of the American Medical Informatics Association 21(5), 858–865 (2014)\n[25] Soysal, E., Wang, J., Jiang, M., Wu, Y., Pakhomov, S., Liu, H., Xu, H.: Clamp–\na toolkit for efficiently building customized clinical natural language processing\npipelines. Journal of the American Medical Informatics Association 25(3), 331–\n336 (2018)\n[26] Tibbo, M.E., Wyles, C.C., Fu, S., Sohn, S., Lewallen, D.G., Berry, D.J.,\nMaradit Kremers, H.: Natural language processing of radiology reports for iden-\ntification of skeletal site-specific fractures. BMC medical informatics and decision\nmaking 19(1), 73 (2019)\n[27] Moon, S., Liu, S., Scott, C.G., Samudrala, S., Abidian, M.M., Geske, J.B., Nose-\nworthy, P.A., Shellum, J.L., Chaudhry, R., Ommen, S.R., et al.: Automated\nextraction of sudden cardiac death risk factors in hypertrophic cardiomyopa-\nthy patients by natural language processing. International journal of medical\ninformatics 128, 32–38 (2019)\n[28] Zhang, R., Pakhomov, S., Melton, G.B.: Automated identification of relevant new\ninformation in clinical narrative using language modeling. Journal of biomedical\ninformatics 49, 255–261 (2014)\n[29] Murff, H.J., FitzHenry, F., Matheny, M.E.: Automated identification of postop-\nerative complications within an electronic medical record using natural language\nprocessing. JAMA 306(8), 848–855 (2011)\n[30] Wang, Y., Wang, L., Rastegar-Mojarad, M., Moon, S., Shen, F., Afzal, N., Liu,\nS., Zeng, Y., Mehrabi, S., Sohn, S., Liu, H.: Clinical information extraction\napplications: a literature review. Journal of biomedical informatics 77, 34–49\n(2018)\n[31] Peng, N., Zhang, Y., Jin, W.: Transfer learning-based approach for clinical named\nentity recognition with limited training data. Journal of Biomedical Informatics\n30\n"}, {"page": 31, "text": "95, 103218 (2019)\n[32] Si, Y., Sun, Y., Li, H., Zhang, Z.: Deep learning for clinical information extraction:\nA survey. Journal of Biomedical Informatics 94, 103196 (2019)\n[33] Peng, Y., Chen, Q., Lu, Z.: An empirical study of multi-task learning on bert for\nbiomedical text mining. arXiv preprint arXiv:1908.11692 (2019)\n[34] Ding, Y., Shen, D., Huang, Z., Chen, W.: Cogran: A hierarchical transformer\nmodel for multi-document summarization. arXiv preprint arXiv:2004.07840\n(2020)\n[35] Fabbri, A., Fan, P., Gupta, R., Sedoc, J., Khalid, B.: Multi-document summa-\nrization. arXiv preprint arXiv:1908.08376 (2019)\n[36] Adamson, B., Waskom, M., Blarre, A., Kelly, J., Krismer, K., Nemeth, S., Gip-\npetti, J., Ritten, J., Harrison, K., Ho, G., Linzmayer, R., Bansal, T., Wilkinson,\nS.C., Amster, G., Estola, E., Benedum, C.M., Fidyk, E., Est´evez, M., Shapiro,\nW., Cohen, A.B.: Approach to machine learning for extraction of real-world data\nvariables from electronic health records. Frontiers in Pharmacology 14, 1180962\n(2023) https://doi.org/10.3389/fphar.2023.1180962\n[37] Kung, A.Y., Li, J.X., Liu, M.Y., Yuan, M., Luo, Y., Zhang, Y., Zhang, Y., Lu, Y.,\nLi, J., Sun, Y., et al.: Performance of a large language model at medical question\nanswering. arXiv preprint arXiv:2303.13257 (2023)\n[38] Agrawal, A., Chen, Y., Zhang, Y., Li, Y., Zhang, Y., Lu, Y., Li, J., Sun, Y.,\nLi, J.X., Kung, A.Y.: Large language models for clinical decision support: A\nsystematic review. arXiv preprint arXiv:2303.13258 (2023)\n[39] Wong, C., Preston, S., Liu, Q., Gero, Z., Bagga, J., Zhang, S., Jain, S., Zhao,\nT., Gu, Y., Xu, Y., et al.: Universal abstraction: Harnessing frontier models to\nstructure real-world data at scale. arXiv preprint arXiv:2502.00943 (2025)\n[40] Porter, R., Diehl, A., Pastel, B., Hinnefeld, J.H., Nerenberg, L., Maung, P., Ker-\nbrat, S., Hanson, G., Astorino, T., Tarsa, S.J.: Llmd: A large language model for\ninterpreting longitudinal medical records. arXiv preprint arXiv:2410.12860 (2024)\n[41] Keloth, V.K., Selek, S., Chen, Q., Gilman, C., Fu, S., Dang, Y., Chen, X., Hu,\nX., Zhou, Y., He, H., Fan, J.W., Wang, K., Brandt, C., Tao, C., Liu, H., Xu, H.:\nSocial determinants of health extraction from clinical notes across institutions\nusing large language models. npj Digital Medicine 8, 287 (2025)\n[42] Fu, S., Jia, H., Vassilaki, M., Keloth, V.K., Dang, Y., Zhou, Y., Garg, M.,\nPetersen, R.C., St Sauver, J., Moon, S., Wang, L., Wen, A., Li, F., Xu, H., Tao,\nC., Fan, J., Liu, H., Sohn, S.: Fedfsa: Hybrid and federated framework for func-\ntional status ascertainment across institutions. Journal of Biomedical Informatics\n31\n"}, {"page": 32, "text": "152, 104623 (2024)\n[43] Kaster, L., Hillis, E., Oh, I.Y., Aravamuthan, B.R., Lanzotti, V.C., Vickstrom,\nC.R., Brain Gene Registry Consortium, Gurnett, C.A., Payne, P.R.O., Gupta, A.:\nAutomated extraction of functional biomarkers of verbal and ambulatory ability\nfrom multi-institutional clinical notes using large language models. Journal of\nNeurodevelopmental Disorders 17, 24 (2025)\n[44] Stuhlmiller, T.J., Rabe, A., Rapp, J., Manasco, P., Awawda, A., Kouser, H.,\nSalamon, H., Chuyka, D., Mahoney, W., Wong, K.K., et al.: A scalable method\nfor validated data extraction from electronic health records with large language\nmodels. medRxiv, 2025–02 (2025)\n[45] Yao, S., Zhao, J., Yu, D.: React: Synergizing reasoning and acting in lan-\nguage models. In: Advances in Neural Information Processing Systems (NeurIPS)\n(2022)\n[46] Wei, J., Xiong, D., Ma, Z., Huang, D., Mihaylov, T., Gong, C., Zhang, S., Yang,\nW., Wang, X., Liu, Y., et al.: Chain-of-thought prompting elicits reasoning in\nlarge language models. arXiv preprint arXiv:2201.11903 (2022)\n[47] Wang, X., Wei, J., Schuurmans, D., Le, Q.V., Chi, E.H., Narang, S., Chowdhery,\nA., Zhou, D.: Self-consistency improves chain of thought reasoning in language\nmodels. In: ACL 2023 (2022)\n[48] Schick, T., Hoffmann, Y., Borgeaud, S., Hennigan, T., Wojak, G., Rhodes, A.R.,\nZemel, R., Sutskever, I.: Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.03328 (2023)\n[49] Nakano, R., Choi, Y., Lee, K., Choi, J., Lee, K.: Webgpt: Browser as an oracle.\narXiv preprint arXiv:2112.09622 (2021)\n[50] Ferber, D., Nahhas, O.S.M.E., W¨olflein, G., Wiest, I.C., et al.: Develop-\nment and validation of an autonomous artificial intelligence agent for clinical\ndecision-making in oncology. Nature Cancer 6, (2025) https://doi.org/10.1038/\ns43018-025-00991-6\n[51] Sandhu, A., Kim, E.J., colleagues: Open-source modular ai coupled with agentic ai\nfor comprehensive breast cancer note generation and guideline-directed treatment\ncomparison. In: Proceedings of the 2025 American Society of Clinical Oncology\nAnnual Meeting. Journal of Clinical Oncology, vol. 43, p. 13685 (2025). https:\n//doi.org/10.1200/JCO.2025.43.16 suppl.e13685\n[52] Zhang, R., Elhamod, M.: Data-to-dashboard: Multi-agent llm framework for\ninsightful visualization in enterprise analytics. arXiv preprint arXiv:2505.23695\n(2025)\n32\n"}, {"page": 33, "text": "[53] Zhou, S., Wang, N., Wang, L., Liu, H., Zhang, R.: Cancerbert: a cancer domain-\nspecific language model for extracting breast cancer phenotypes from electronic\nhealth records. Journal of the American Medical Informatics Association 29(7),\n1208–1216 (2022) https://doi.org/10.1093/jamia/ocac040\n[54] Hochheiser, H., Finan, S., Yuan, Z., Durbin, E.B., Jeong, J.C., Hands, I., Rust,\nD., Kavuluru, R., Wu, X.-C., Warner, J.L., Savova, G.K.: Deepphe-cr: Natural\nlanguage processing software services for cancer registrar case abstraction. JCO\nClinical Cancer Informatics (2023) https://doi.org/10.1200/CCI.23.00156\n[55] Goryachev, S.D., et al.: Natural language processing algorithm to extract multiple\nmyeloma stage from oncology notes in the veterans affairs healthcare system. JCO\nClinical Cancer Informatics 8, 2300197 (2024) https://doi.org/10.1200/CCI.23.\n00197\n[56] Ling, A.Y., Kurian, A.W., Caswell-Jin, J.L., Sledge, G.W., Shah, N.H., Tamang,\nS.R.: Using natural language processing to construct a metastatic breast cancer\ncohort from linked cancer registry and electronic medical records data. JAMIA\nOpen 2(4), 528–537 (2019) https://doi.org/10.1093/jamiaopen/ooz040\n[57] Zhu, M., Lin, H., Jiang, J., et al.: Large language model trained on clinical\noncology data predicts cancer progression. npj Digital Medicine 8, 397 (2025)\nhttps://doi.org/10.1038/s41746-025-01780-2\n[58] Dagli, M.M., Ghenbot, Y., Ahmad, H.S., et al.: Development and validation of\na novel ai framework using nlp with llm integration for relevant clinical data\nextraction through automated chart review. Scientific Reports 14, 26783 (2024)\nhttps://doi.org/10.1038/s41598-024-77535-y\n[59] Zeng, Z., Deng, Y., Li, X., Naumann, T.J., Luo, Y.: Natural language processing\nfor ehr-based computational phenotyping. IEEE/ACM Transactions on Compu-\ntational Biology and Bioinformatics 16(1), 139–153 (2019) https://doi.org/10.\n1109/TCBB.2018.2810898\n33\n"}, {"page": 34, "text": "Supplementary Information\nS-1\nEnvironment and Dependencies\nLanguage models :\ngpt-4o-2024-05-13, Qwen2-7B, o1-preview-2024-09-12\nEmbeddings\n:\ntext-embedding-3-large\nVector store\n:\nFAISS v1.8.1\n(Inner Product)\nPython\n:\n3.11.4\nopenai\n:\n1.26.0\npydantic\n:\n2.8.1\ntiktoken\n:\n0.6.0\nrich\n:\n13.7.0\nfaiss-cpu\n:\n1.8.1\nAll LLM calls were executed on an Azure OpenAI private endpoint inside a HIPAA-\ncompliant VNet; no PHI left the secure boundary.\nS-2\nPrompt Templates\nStage\nRepresentative Template Fragment1\nBiomarker\nSingle-Step\nsystem: You are an expert molecular pathologist ...\nuser\n: <<SNIPPET>>\nassistant: Return JSON {biomarker_tested,...,test_date}.\nMedication\nMulti-Step–1\n(enumerate drugs)\nsystem: You are an oncology pharmacist ...\nList all distinct systemic agents mentioned in the snippet.\nMedication\nMulti-Step–2\n(attributes\nper\ndrug)\nsystem: For the drug \"{{DRUG}}\" extract\n{start_date,end_date,route,dose_change_reason}.\nCollation /\nDeduplication\nsystem: Merge entries if medication + start_date\ndiffer by <=7 days OR share identical event_id.\nSelf-reflection\nassistant: If any required attribute is NULL,\nre-read context and attempt one retry.\nTo ensure methodological transparency while respecting intellectual property considerations,\nabbreviated versions of the prompts used in this study are provided in the supplementary\nmaterials. These abbreviated prompts capture the essential structure and intent of the inter-\nactions with the large language model, sufficient for understanding our methodology. Each\nentity in our synthesis pipeline underwent multiple processing stages, each requiring distinct\nprompting strategies. Complete prompt templates with full instructions, examples, and spe-\ncific parameters can be made available to researchers seeking to replicate our findings upon\nreasonable request and execution of a non-disclosure agreement. Interested parties may con-\ntact the corresponding author for access to these materials.\n34\n"}, {"page": 35, "text": "S-3\nEnd-to-End Pipeline Configuration\n1\n{\n2\n\"pipeline_name\": \"harmon-e_melanoma_v1\",\n3\n\"entities\": [\n4\n{\n5\n\"name\": \"Biomarker\",\n6\n\"retriever\": {\n7\n\"type\": \"vector\",\n8\n\"embedding_model\": \"text-embedding-3-large\",\n9\n\"k\": 12,\n10\n\"query_template\":\n11\n\"Find passages describing laboratory or genomic tests for melanoma.\"\n12\n},\n13\n\"synthesizer\": {\n14\n\"llm\": \"gpt-4o-2024-05-13\",\n15\n\"prompt_file\": \"prompts/biomarker_single_step.txt\",\n16\n\"max_tokens\": 600\n17\n},\n18\n\"collator\": {\n19\n\"rules\": [\"deduplicate_by_root: biomarker_tested\",\n20\n\"prefer_latest: result_date\"]\n21\n}\n22\n},\n23\n24\n{\n25\n\"name\": \"CancerRelatedMedication\",\n26\n\"retriever\": {\n27\n\"type\": \"regex+vector\",\n28\n\"patterns\":\n29\n[\"(?i)(nivolumab|pembrolizumab|ipilimumab|vemurafenib)\"],\n30\n\"k\": 20\n31\n},\n32\n\"synthesizer\": [\n33\n{\n34\n\"stage\": \"enumerate\",\n35\n\"prompt_file\": \"prompts/medication_stage1_list.txt\"\n36\n},\n37\n{\n38\n\"stage\": \"detail\",\n39\n\"prompt_file\": \"prompts/medication_stage2_detail.txt\",\n40\n\"loop_over\": \"{{ENUMERATED_DRUGS}}\"\n41\n}\n42\n],\n43\n\"collator\": {\n44\n\"rules\": [\n45\n\"merge_if_name_and_start<=7d\",\n46\n\"infer_end_date_from_last_administration\",\n47\n\"set_status_discontinued_if_end_date<today-28d\"\n48\n]\n49\n}\n50\n}\n51\n],\n52\n53\n\"post_processors\": [\n54\n\"validate_against_schema\",\n55\n\"iso8601_date_normalizer\",\n56\n\"convert_units\"\n57\n],\n58\n59\n\"evaluation\": {\n60\n\"alignment_method\": \"root_or_weighted\",\n61\n\"metrics\": [\"precision\", \"recall\", \"f1\"],\n62\n\"date_tolerance_days\": 7\n1Temperature = 0, top p = 0.1 for all calls.\n35\n"}, {"page": 36, "text": "63\n}\n64\n}\n1\n\"\"\"\n2\nrun_harmone.py - Minimal driver to execute the JSON pipeline.\n3\n\"\"\"\n4\nimport json, pathlib\n5\nfrom harmone.engine import Pipeline\n# lightweight wrapper in SI\n6\n7\ncfg_path = pathlib.Path(\"harmon-e_pipeline.json\")\n8\npipe\n= Pipeline.from_config(json.loads(cfg_path.read_text()))\n9\n10\nfor patient_dir in pathlib.Path(\"/data/melanoma_notes/\").iterdir():\n11\nresult = pipe.run(\n12\npatient_id = patient_dir.stem,\n13\nnote_paths = list(patient_dir.glob(\"*.txt\"))\n14\n)\n15\npipe.save_json(result, out_dir=\"outputs/\")\nThis sample configuration file illustrates the structure of a typical HARMON-E\npipeline, showing how different modules can be connected and parameterized to cre-\nate a complete workflow. The accompanying driver script provides a straightforward\nexample of how to load and execute such a pipeline configuration programmatically.\nS-4\nManual Evaluation Protocol\nThis section provides the detailed methodology for the manual evaluation protocol\nsummarized in Section 4.2.\nDisagreement-Based Sampling.\nFor each patient p, we calculated a Disagreement Score DS(p) representing the total\nnumber of mismatched attributes between pipeline outputs and ground truth:\nDS(p) =\nN\nX\nj=1\nδj\n(1)\nwhere δj = 1 if attribute j is mismatched and 0 otherwise across N total attributes.\nPatients were ranked by DS(p) in descending order, with the top 50 highest-scoring\npatients selected per entity type for expert review.\nReview Categories.\nClinical experts classified each extracted item into one of three categories:\n• Correct: Pipeline output accurately reflects clinical documentation and requires no\nmodification\n• Incorrect: Pipeline output contains errors requiring editing or deletion\n• Missing: Clinically relevant information present in the patient record but absent\nfrom pipeline output\n36\n"}, {"page": 37, "text": "Table 1: Step-by-step implementation of the manual evaluation protocol\nStep\nAction\nImplementation Details\n1\nCalculate Disagreement\nScore\n• Compare\neach\nattribute\nacross\nall\nentity\ninstances\n• Count mismatches between pipeline and ground\ntruth\n• Compute DS(p) = PN\nj=1 δj\n2\nSelect Review Cohort\n• Sort patients by DS(p) in descending order\n• Select top 50 patients per entity type E ∈E\n• Ensure minimum 5 instances per entity type per\npatient\n3\nConduct Blinded Review\n• Present complete patient EHR via curation plat-\nform\n• Remove all indicators of data source (pipeline vs.\nground truth)\n• Expert classifies each item: Correct / Incorrect\n/ Missing\n• Expert adds any clinically relevant missing infor-\nmation\n4\nCompute Metrics\n• Tally classifications per patient-entity pair\n• Calculate Acceptance Score (Eq. 2)\n• Calculate Missing Rate (Eq. 3)\nPerformance Metrics.\nThe Acceptance Score quantifies the proportion of pipeline-extracted items requir-\ning no modification:\nAcceptance Score =\nX\np∈P\nX\nE∈E\nncorrect(p, E)\nX\np∈P\nX\nE∈E\nnextracted(p, E)\n(2)\n37\n"}, {"page": 38, "text": "The Missing Rate measures the completeness of extraction:\nMissing Rate =\nX\np∈P\nX\nE∈E\nnmissing(p, E)\nX\np∈P\nX\nE∈E\n[nextracted(p, E) + nmissing(p, E)]\n(3)\nVariable definitions:\n• P ⊆P: Set of patients selected for review\n• E: Set of entity types evaluated\n• ncorrect(p, E): Count of correct extractions for patient p, entity type E\n• nincorrect(p, E): Count of incorrect extractions\n• nmissing(p, E): Count of missing items identified by reviewers\n• nextracted(p, E) = ncorrect(p, E) + nincorrect(p, E): Total extractions\nS-5\nDataset Statistics\nTable 2: Comparison of baseline characteristics between\nthe SEER 2025 melanoma cohort (2018–2022 diagnoses)\nand the values reported for the HARMON-E hold-out set.\nDifferences |∆|>3 percentage-points are typeset in bold.\nSome numbers are not exact and have been derived from\nvarious sources\nDistribution (%)\nCharacteristic\nSEER 2025\nDataset\n∆(pp)\nMedian age, years a\n66 (IQR 59–74)\n67\n–\nSex – Male\n57.7\n61.1\n+3.4\nAJCC Stage I & II\n77.0\n48.9\n–28.1\nAJCC Stage III\n9.5\n42.5\n+33.0\nAJCC Stage IV\n4.7\n1.8\n-2.9\nNotes.\na Age summarised as median (inter-quartile range); no ∆computed.\nAJCC = American Joint Committee on Cancer; IQR = inter-quartile\nrange; pp = percentage-points.\nWe extracted reference statistics from SEER public database along with few\nresearch papers that summarize melanoma specific statistics. The table 2 summarizes\nthe statistics.\n38\n"}, {"page": 39, "text": "Table 3: Macro-averaged performance of prior baselines on the hold-out melanoma cohort.\nNot all pipelines could be modified to approximate all data points in the schema\nPipeline\nPrec. (%)\nRec. (%)\nF1 (%)\nDate Err.∗(%)\ncTAKES 4.0 + rule post-proc\n63.7\n54.2\n58.6\n27.8\nSciSpacy + CRF aggregation\n67.3\n61.8\n64.4\n30.3\nClinicalBERT\n(fine-tuned,\nnote-\nlevel)\n66.5\n68.9\n67.7\n26.2\nNotes. Prec. = macro-precision; Rec. = macro-recall. (*) Date Err. = percentage of extracted date attributes devi-\nating by >±14 d from the reference.\nS-6\nComparison with Prior Baseline Systems\nTo contextualise HARMON-E’s performance, we re-implemented three representative\nextraction pipelines that are commonly cited in clinical-NLP literature. Not all vari-\nables were supported - so we just averaged over whatever could be processed using\neach method. All baselines were executed on the identical 1 125-patient hold-out\ncohort described in Section 4.4. Because none of the legacy systems natively consoli-\ndate information across hundreds of notes, we added a lightweight post-processor that\n(i) clusters identical concepts within a 14-day window and (ii) propagates the most fre-\nquent attribute value. The configurations and macro-level results are summarised in\nTable 3.\n39\n"}]}