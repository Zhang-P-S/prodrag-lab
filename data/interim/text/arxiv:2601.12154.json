{"doc_id": "arxiv:2601.12154", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.12154.pdf", "meta": {"doc_id": "arxiv:2601.12154", "source": "arxiv", "arxiv_id": "2601.12154", "title": "Analyzing Cancer Patients' Experiences with Embedding-based Topic Modeling and LLMs", "authors": ["Teodor-Călin Ionescu", "Lifeng Han", "Jan Heijdra Suasnabar", "Anne Stiggelbout", "Suzan Verberne"], "published": "2026-01-17T20:06:14Z", "updated": "2026-01-17T20:06:14Z", "summary": "This study investigates the use of neural topic modeling and LLMs to uncover meaningful themes from patient storytelling data, to offer insights that could contribute to more patient-oriented healthcare practices. We analyze a collection of transcribed interviews with cancer patients (132,722 words in 13 interviews). We first evaluate BERTopic and Top2Vec for individual interview summarization by using similar preprocessing, chunking, and clustering configurations to ensure a fair comparison on Keyword Extraction. LLMs (GPT4) are then used for the next step topic labeling. Their outputs for a single interview (I0) are rated through a small-scale human evaluation, focusing on {coherence}, {clarity}, and {relevance}. Based on the preliminary results and evaluation, BERTopic shows stronger performance and is selected for further experimentation using three {clinically oriented embedding} models. We then analyzed the full interview collection with the best model setting. Results show that domain-specific embeddings improved topic \\textit{precision} and \\textit{interpretability}, with BioClinicalBERT producing the most consistent results across transcripts. The global analysis of the full dataset of 13 interviews, using the BioClinicalBERT embedding model, reveals the most dominant topics throughout all 13 interviews, namely ``Coordination and Communication in Cancer Care Management\" and ``Patient Decision-Making in Cancer Treatment Journey''. Although the interviews are machine translations from Dutch to English, and clinical professionals are not involved in this evaluation, the findings suggest that neural topic modeling, particularly BERTopic, can help provide useful feedback to clinicians from patient interviews. This pipeline could support more efficient document navigation and strengthen the role of patients' voices in healthcare workflows.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.12154v1", "url_pdf": "https://arxiv.org/pdf/2601.12154.pdf", "meta_path": "data/raw/arxiv/meta/2601.12154.json", "sha256": "63c6b02752ed2f21e464f8afbdcb5c50da889efed138417364685ef6de8812d3", "status": "ok", "fetched_at": "2026-02-18T02:21:21.856132+00:00"}, "pages": [{"page": 1, "text": "Analyzing Cancer Patients’ Experiences with\nEmbedding-based Topic Modeling and LLMs\nTeodor-C˘alin Ionescu∗\nLifeng Han∗†\nl.han@lumc.nl\nJan Heijdra Suasnabar†\nj.m.heijdra suasnabar@lumc.nl\nAnne Stiggelbout†\na.m.stiggelbout@lumc.nl\nSuzan Verberne∗\ns.verberne@liacs.leidenuniv.nl\n∗Leiden Institute of Advanced Computer Science (LIACS), Leiden, The Netherlands\n†Leiden University Medical Center (LUMC), Leiden, The Netherlands\nCorresponding authors: LH and SV\nAbstract\nThis study investigates the use of neural topic modeling and LLMs to uncover meaningful themes\nfrom patient storytelling data, with the goal of offering insights that could contribute to more\npatient-oriented healthcare practices. We analyze a collection of transcribed interviews with cancer\npatients (132,722 words in 13 interviews).\nWe first evaluate BERTopic and Top2Vec for the\npurpose of individual interview summarization, by using similar preprocessing, chunking, and\nclustering configurations to ensure a fair comparison on Keyword Extraction.\nLLMs (GPT4)\nare then used for next step topic labeling. Their outputs for a single interview (I0) are rated\nthrough a small-scale human evaluation, focusing on coherence, clarity, and relevance. Based on\nthe preliminary results and evaluation, BERTopic shows stronger performance and is selected for\nfurther experimentation using three clinically oriented embedding models. We then analyzed the\nfull interview collection with the best model setting. Results show that domain-specific embeddings\nimproved topic precision and interpretability, with BioClinicalBERT producing the most consistent\nresults across transcripts.\nThe global analysis of the full dataset of 13 interviews, using the\nBioClinicalBERT embedding model, reveals the most dominant topics throughout all 13 interviews,\nnamely “Coordination and Communication in Cancer Care Management” and “Patient Decision-\nMaking in Cancer Treatment Journey”. Although the interviews are machine translations from\nDutch to English, and clinical professionals are not involved in this evaluation, the findings suggest\nthat neural topic modeling, particularly BERTopic, can help provide useful feedback to clinicians\nfrom patient interviews.\nThis pipeline could support more efficient document navigation and\nstrengthen the role of patients’ voices in healthcare workflows. Affiliated resources created in this\nwork will be shared publicly at https://github.com/4dpicture/TM4health including codes on\npreprocessing and stopword list prepared.\n1. Introduction\nCancer is one of the most challenging global health issues, affecting not only individuals but also\nentire families and communities. People are subjected to intense physical and mental difficulties\nto the point where their quality of life changes forever, even after recovering from the disease.\nIn modern healthcare, understanding patient experiences is crucial for improving treatment and\ncare (Ren et al. 2025).\nWhile clinical research traditionally relies on structured medical data,\npatient feedback, such as storytelling data, provides valuable insights that should not be overlooked.\nHealthcare should not only focus on treating the disease, but also on the emotional and psychological\nneeds of patients, recognizing them as individuals in need of comfort and support, rather than just\nsubjects in a medical process. These have been reflected by recent work on shared decision making\narXiv:2601.12154v1  [cs.CL]  17 Jan 2026\n"}, {"page": 2, "text": "(SDM) and patient-centered carepath design (Griffioen et al. 2021, Kidanemariam et al. 2024, Bak\net al. 2025).\nNatural Language Processing (NLP) is a powerful approach to analyzing patient narratives,\nenabling the extraction of meaningful topics from a large volume of text. In this paper, we use\nneural topic modelling techniques to extract relevant topics from cancer patient storytelling data and\nanalyze them systematically. We aim to see what kind of valuable insights we can offer to healthcare\nproviders in order to make the patients’ cancer treatment journeys more bearable.\nThis could\ngive professionals a deeper understanding of patient needs, enabling a more patient-oriented care\nstrategy. We compare two topic modeling algorithms, BERTopic (Grootendorst 2022) and Top2Vec\n(Angelov 2020, Angelov and Inkpen 2024), to determine which one performs better at extracting\nrelevant topics from patient storytelling data. By optimizing these models, we determine which\ntopic modeling approach yields more interpretable and coherent topics within the context of cancer\ncare. Ultimately, this work lays the foundation for a potential feedback tool that allows clinicians to\nautomatically analyze patient files, scanning through large bodies of text easily and focusing on key\nthemes and concerns patients raise. For instance, for a collection of patient consultation transcripts,\nthis methods can help to identify cross-patient population level concerns effectively.\nWe address two research questions in this paper:\n1. What key themes can current neural topic modeling models extract from patient\nstorytelling data, which model is the best choice for this task, and if the integration\nof clinical domain embeddings help?\n2. Based on the LLM-labeled topics using key word lists from the first step, what\nfeedback can we offer to current healthcare frameworks or procedures to improve\npatient care?\nThis study aims to connect patient feedback with clinical decision-making by addressing these\nquestions. The dataset used in this study was provided by Erasmus Medical Center (Erasmus MC)\nin Rotterdam, the Netherlands (Erasmus MC 2025) through an EU healthcare research project. It\noriginates from the Metro Mapping Project, a design-driven initiative to support cancer patients in\nnavigating their care journey and preparing for significant medical decisions (Griffioen et al. 2017, TU\nDelft 2021). By analyzing the cancer patient storytelling data, we aim to contribute to that mission,\nproving insights that reflect the patients’ lived experiences and can be used to further improve\npatient-centered healthcare practices.\n2. Background and Related Work\n2.1 Model Evolvement of Topic modeling\nTopic modeling is an unsupervised technique for identifying themes in large text collections and\nforms the foundation of this research. A widely used traditional method is Latent Dirichlet Allo-\ncation (LDA) (Blei et al. 2003), a Bayesian algorithm for extracting topics from text. LDA has\nbeen applied in various contexts, such as analyzing topic evolution during the COVID-19 pandemic\nin Swedish newspapers (Grici¯ut˙e et al. 2023). However, LDA suffers from limitations, including\ndifficulty capturing semantic relationships, sensitivity to common words, and issues with short-text\ndata (Albalawi et al. 2020). These shortcomings include non-deterministic outputs, the need for a\npredefined number of topics, data sparsity, and inability to model topic relations.\nMore recent research introduced embedding-based models like BERTopic (Grootendorst 2022)\nand Top2Vec (Angelov 2020) including its contextual-embedding variation from Angelov and\nInkpen (2024), which overcome many LDA limitations by using dense vector representations, di-\nmensionality reduction, and clustering. These models require minimal preprocessing, automatically\ndetermine topic numbers, and excel with short or context-rich texts. Studies show they outper-\nform classical methods in coherence and interpretability: Egger and Yu (2022) found BERTopic\n"}, {"page": 3, "text": "produced the most distinct topics on COVID-19 Twitter data, while Top2Vec also surpassed LDA.\nSimilarly, Wahbeh et al. (2025) reported BERTopic achieving the highest coherence scores across\ndatasets, with Top2Vec ranking slightly lower but still ahead of LDA. Overall, embedding-based\nmodels consistently deliver more coherent and interpretable topics than traditional approaches.\n2.2 Patient-Narrative Clinical NLP\nClinical Natural Language Processing (NLP) applies NLP techniques, such as topic modeling, to ex-\ntract and analyze medical text from sources like unstructured health data, discharge summaries,\nsurveys, and patient files.\nModern research highlights its integration into healthcare systems,\nparticularly for clinical decision support and clinician-patient communications (Demner-Fushman\net al. 2009, Wang et al. 2018). For instance, in the statistical era, the review by Demner-Fushman\net al. (2009) documents the use of the Linguistic Inquiry and Word Count (LIWC) tool for ana-\nlyzing patient personality through linguistic style, enabling applications such as predicting cancer\nadjustment, mental health outcomes after bereavement, and differentiating suicidal from non-suicidal\npatients. Complementing these clinician-facing applications, Van Buchem et al. (2022) introduced\nthe AI-PREM, an open-ended patient experience questionnaire paired with an NLP pipeline that\ncombines topic modeling and sentiment analysis to thematically cluster short patient narratives into\nactionable summaries for clinicians. These examples underscore the role of linguistic feature analy-\nsis in diagnosis, management, and prognosis. However, Clinical NLP faces challenges in clustering\ntasks, as noted by Sheikhalishahi et al. (2019) and Ghosh (2024), including difficulties in processing\nclinical notes, ambiguities, context-dependent, and domain specific knowledge.\nRecent advancements emphasize patient-narrative data, aligning with this study’s focus. Ohno\net al. (2025) developed a BERT-based model for monitoring symptoms from patient interviews\nin a Japanese hospital, achieving superior performance despite limitations like data scarcity and\nsingle-source training. Furthermore, Alon et al. (2025) analyzed Hebrew clinician speech using word\nfrequency and Large Language Models (LLMs) to uncover cognitive paradigms, finding reliance on\nheuristics and intuitive reasoning, though limited by small datasets and lack of multilingual frame-\nworks. Similarly, Somani et al. (2023) used an AI pipeline combining BERTopic, dimensionality\nreduction, and spectral clustering to organize 10,233 patients’ statin-related Reddit posts into six\noverarching themes, and then applied a pretrained BERT sentiment model, which found the dis-\ncussions were predominantly neutral to negative. Overall, literature shows Clinical NLP’s potential\nto transform healthcare by extracting insights from both patient and clinician narratives, yet chal-\nlenges persist due to privacy concerns, linguistic variability, and limited annotated datasets (Raj\net al. 2023, Han et al. 2024, Bak et al. 2025). This study addresses these issues by exploring neu-\nral topic modeling for coherent, patient-centered topics, promoting reproducibility through shared\nand anonymized datasets and models, and investigating multilingual approaches using specialized\nembeddings and translation tools to advance linguistically inclusive Clinical NLP.\n3. The Dataset - Patient Storytelling\nThe data we use consists of 13 anonymized transcribed .docx files (I0 to I12), each corresponding\nto a different cancer patient. As mentioned in the introduction section, the dataset is provided\nby the 4D PICTURE consortium, an EU research project across multiple institutes from several\ncountries1, which aims to improve the cancer patient journey and ensure personal preferences are\nrespected. The data originates from the Metro Mapping Project (TU Delft 2021) pubished in the\nwork of Griffioen et al. (2021). They contain multiple interviews in which the patients discuss their\nexperiences with the disease, such as how they were diagnosed, their emotional struggles, coping\nmechanisms during treatment, and other personal reflections. The length of each document varies,\n1. https://4dpicture.eu “Design-based Data-Driven Decision-support Tools: Producing Improved Cancer Out-\ncomes Through User-Centred Research”\n"}, {"page": 4, "text": "Figure 1: Snippets of the I0 interview in Dutch (left image) and English translation (right image)\nwith the shortest document containing 5,596 words (approximately 34,000 characters), and the\nlongest document containing 12,875 words (approximately 58,000 characters). The total collection\nsize is 132,772 words.\nEach document features three different speakers: the patient, the researcher who conducts the\ninterview, and the “naaste”, a Dutch word which may be roughly translated in English to “loved\none” or “close relation”, whose role during the interviews is to offer an outside perspective on the\npatient’s cancer journey and emotional support. Each line within the document is marked with a\ncapital letter which represents who is speaking: P for the patient, N for the “naaste”, and O for the\ninterviewer. An example is shown in Figure 1.\nMoreover, all texts preserve marks of orality, such as hesitations, repetitions, and informal speech.\nWhile this format reflects the emotional nature of the interviews and provides a better insight into\nthe patient’s experiences, it also introduces potential complications in the preprocessing and analysis\nstages, which are thoroughly discussed and analyzed in the upcoming dedicated section.\nAnother challenge lies in the language of the documents: all interviews are fully conducted in\nDutch. This presents a potential barrier, as many pre-trained domain-specific embedding models\ntend to perform best on English-language data or are even exclusively English (especially in the\nbiomedical domain), which may impact the quality and consistency of the results when working\nwith Dutch transcripts.\nWe examine structural and linguistic challenges in detail in the upcoming dedicated sections,\nwhich explore workarounds and compromises for not only preserving the main essence and important\naspects of the original transcripts but also for producing accurate and easily interpretable results by\nshowcasing various experiments and alternative options.\n4. Design of Methods\nThe goal of this study is to identify key topics and themes present in patient storytelling data\nand examine their relevance within the context of the interview, in order to determine whether\nstate-of-the-art neural topic modeling techniques can be useful for providing patient feedback to\n"}, {"page": 5, "text": "BERTtopic & \nTop2vec\nModel \ninitialisation\nIterative parameter \ntuning & \nEmbedding selections\nOptimised topic \nidentification \nparameters &\nSelected \nembeddings\npilot data\nI(0)\nFull data\n(13 interviews)\nData \npreprocessing\nHuman \nevaluations\n(coherence)\n(fidelity)\n(interpretability)\nBERT-topic \nTop2Vec\nOut: Keyword list\nLLM topic labeling\nselected \nTopic \nmodel\nclinical \nembedding\nselection\nGlobal \nanalysis\nPilot study:\nFigure 2: Framework Design of Overall Experimental Investigation\nmedical staff. To achieve this, BERTopic and Top2Vec are applied on a single anonymized cancer\npatient interview at a time, with the pilot-study interview being Interview I0, and their outputs\nare evaluated in order to determine their coherence and relevancy in a medical context, as shown in\nthe system development framework in Figure 2.\nOut of available topic modeling methods, we first select BERTopic because of its flexibility in\nchoosing an embedding model, as well as its capability to capture context and synonyms, which\nis important for medical jargon and marks of orality.2\nFor example, the word “patient” has a\ndifferent meaning when used as a noun, referring to a person receiving medical care, compared\nto its use as an adjective, where it describes the quality of being tolerant.\nThis distinction is\nespecially important in medical interviews, where context determines whether terms describe people,\nconditions, or behaviors. Moreover, BERTopic does not require manually setting the number of\ntopics in advance, which is useful if the number of themes that may emerge from a corpus is unknown.\nThis is achieved through its use of HDBSCAN as clustering algorithm, which determines the number\nof clusters based on the distribution and density of the embedded documents.\nFor comparison\npurpose, we select Top2vec as an alternative neural topic model.3 While BERTopic is designed to\nuse transformer-based models to better capture contextual nuance, Top2Vec originally used Doc2Vec\nembeddings exclusively, and was later extended to support more powerful models like Universal\nSentence Encoder (Cer et al. 2018).\nAs shown in Figure 2, from bottom-left to right, the dataset is firstly preprocessed and chunked\ninto segments (the exact number varies per experiment and file) in order to preserve narrative\ncoherence while also ensuring compatibility with the embedding models in terms of context length.\nIn the Pilot-study, both BERTopic and Top2Vec are tested on the default settings in order to check for\npreliminary issues and to further experiment with the parameters of the models based on the initial\noutputs. Moreover, we tested several different embedding models as backbones at the “iterative\nparameter tuning and embedding selections” phase, but due to the differences between BERTopic\nand Top2Vec, the focus is placed on a common embedding model, namely all-mpnet-base-v2\n(Reimers and Gurevych 2020), in order to fairly evaluate their performance on as much common\nground as possible. During this phase, the parameters of each model, as well as the parameters that\ncontrol the clustering and dimensionality reduction processes, are experimented with and modified\nacross several runs to determine which settings yield the best results.\nSubsequently, the optimized topic identification parameters and selected embeddings (available\nwithin the two neural models) will be used for both neural topic models to produce keyword lists.\nWe use an LLM to summarize topics from the keyword list into a descriptive label. Then human\nevaluations will be carried out on topic model selection by looking into features of coherence, fidelity,\nand interpretability.\n2. available at https://github.com/MaartenGr/BERTopic\n3. available at https://github.com/ddangelov/Top2Vec\n"}, {"page": 6, "text": "Table 1: Preprocessing Pipeline\nPreprocessing Step\nPurpose\nConverstion to plain text\nConverting the document to a clean .txt file by using the\npython-docx package (Foundation 2024)\nDocument Translation\nTranslation from Dutch to English with DeepL (DeepL GmbH\n2024) for the application of domain-specific backbone models\nand interpretation by non-Dutch speakers.\nSpeaker Label Removal\nRemove speaker tags (e.g., P:, N:) that can confuse embedding\nmodels.\nSection Header Removal\nRemove structural markers (e.g., I0-1) that do not carry any\nsemantic meaning.\nContraction Expansion\nPrevent broken tokens (e.g., wasn, t) by expanding contrac-\ntions (e.g., wasn’t to was not).\nCustom Stop Words List\nRemove uninformative and meaningless words (e.g., “uh”,\n“yeah”, ”says”).\nFor analyzing the entire interview data, we will further explore clinical domain embedding models,\nbeyond the default embeddings from BERT-topic and Top2Vec themselves, together with the selected\ntopic model framework from the pilot study, for a global analysis, in Section 7.\n5. Pilot-study and Optimizing Topic Models\n5.1 Data Preprocessing\nAs stated in Section 3, the source data consists of 13 anonymized cancer patient interviews stored as\ndocx files, which contain speaker labels (P for patient, N for “loved one”, and O for the interviewer),\nstructural markings such as headers and internal identifiers, as well as marks of orality, such as\nstutters in speech and repetitions. Our preprocessing pipeline is listed in Table 1 with examples,\nincluding conversation to plain text conversion, document translation, speaker label removal, section\nheader removal, contraction expansion, and removing custom stop words list.\nFor model development and pilot-testing, we first apply each model to a single preprocessed\nstorytelling interview (I0) and test various configurations to explore how different parameter choices\nand data representations influence the quality and interoperability of the resulting topics, detailed\nin next sections separately for two neural models.\n5.2 Model Development and Optimization: BERTopic\nTo manage the length of storytelling transcripts and ensure compatibility with embedding model\ninput limits, we apply a sentence-based chunking strategy. After processing, the text is split into\nsentences using a regex-based approach, chosen for its simplicity and sufficient accuracy given the\nwell-punctuated nature of the transcripts. These sentences are then grouped into fixed-size chunks,\nand different chunk sizes are tested to assess their impact on topic generation and coherence. Table\n2 shows that the number of chunks directly influences the number of topics produced.\nWe start with BERTopic’s default settings and the all-mpnet-base-v2 embedding model, briefly\ntesting alternatives like MiniLM-L6-v2, which we discard due to token limits and inconsistent out-\nputs. We adjust UMAP parameters (n neighbors, min dist, n components) to balance topic sep-\naration and cohesion, and tune HDBSCAN settings such as min cluster size and min samples\nto control granularity and noise. The vectorizer is optimized to include bigrams, improving inter-\n"}, {"page": 7, "text": "Table 2: Effect of Sentence Chunk Size on Number of Chunks and Topics for Interview I0\nSentences Per Chunk\nChunks\nTopics\n5\n172\n17\n6\n144\n16\n7\n123\n12\n8\n108\n9\npretability by capturing multi-word expressions common in clinical narratives. Finally, we exper-\niment with min topic size, finding the default value (10) best preserves both broad and specific\nthemes without excessive fragmentation.\n5.3 Model Development and Optimization: Top2Vec\nTo compare BERTopic and Top2Vec fairly, we consider both the original Top2Vec and its contextu-\nalized variant (C-Top2Vec) from Angelov and Inkpen (2024), which incorporates transformer-based\nembeddings such as all-mpnet-base-v2 for richer, context-aware topics. However, we exclude C-\nTop2Vec from the final comparison because it lacks support for viewing representative documents\n(document-to-topic mapping), which is crucial for interpretability, manual verification, and the envi-\nsioned clinical support. We apply the same sentence-based chunking strategy to ensure both models\nreceive identical input.\nSimilar to the tuning process employed for BERTopic, Top2Vec is optimized through iterative\nrefinements of key parameters. This time, however, we start from the parameters of the already-\ntuned BERTopic model in order to see how Top2Vec behaves differently under close-to-identical\nconditions to BERTopic. Most of the tuning process that follows involves altering the parameters\npassed to the UMAP and HDBSCAN components that are internally used during the dimensionality\nreduction and clustering.\nWe evaluate the same UMAP parameters explored during BERTopic\ntuning, namely n neighbors min dist and metric, to observe their influence on topic formation.\nSimilarly, we modify the HDBSCAN parameters such as min cluster size and min samples in\norder to balance specificity and generalization within the topic clusters.\nIn our experiments, the default Top2Vec settings cause clustering errors, but tuning UMAP and\nHDBSCAN parameters resolves this and produces meaningful topics. Unlike BERTopic, Top2Vec\ndoes not allow custom n-gram or stop-word settings, limiting keyword flexibility, though contextual\nembeddings still yield relevant terms. After iterative refinements, the optimized Top2Vec models\ngenerate topics suitable for comparative analysis with BERTopic.\n5.4 Topic Labeling with an LLM on Keyword Lists\nBecause the output topics from both models are essentially lists of keywords, they do not hold an\ninterpretable meaning at first glance. To make sense of them, each topic must be labeled according\nto its semantic coherence. Traditionally, this involves manually looking at the top keywords for\neach topic, along with the sample documents that determined the formation of the topic. Labels\nare usually chosen based on the literal meaning of the keywords, along with the context found in\nthe actual text, in order to make them interpretable and meaningful for human readers. The goal\nis for the label to be as meaningful as possible, especially in a healthcare setting where clarity and\nrelevance are essential.\nIn order to achieve this, we use a large language model (LLM) to label the topics in order to\nautomate the process. The specific LLM model that we use for this task is OpenAI’s GPT-4o\nmini model, because it is a powerful yet cost-efficient and lightweight model that shows strong\nperformance across a range of evaluation metrics (OpenAI 2024). At first, we only pass the topic\n"}, {"page": 8, "text": "keywords to the model to see if the resulting topic labels would be cohesive enough without the need\nfor representative documents.\nThe model generates descriptive labels only based on the topic’s keywords, which produces mixed\nresults. While some topic labels are good representations of the actual topics, others are unreliable\nbecause they lack the context behind the actual keywords. For example, one of the output topics\nproduced by BERTopic, with some of the top keywords being “size 19, tricky, 25” received the label\n“Size Discussion in Medical Context”, which has nothing to do with the actual context behind the\ntopic. The absence of document context substantially impacts the quality of the generated labels,\nwhich is an impediment for individual-interview analysis. We therefore adapt the prompt including\ndocument snippets as additional contextual evidence to help interpret the topics more accurately.\nThe final prompt is the following:\n“You are an AI that labels discussion topics, from a cancer storytelling interview, for a\nsoftware that allows doctors to browse through medical files without the need to read\nthem from start to finish. Given the following keywords and sample documents, provide a\nclear and specific topic label, focusing mainly on the keyword list and using the document\nsnippets as supporting context rather than a baseline. Only type the topic label and\nnothing else:”.\n6. Pilot-study Results and analysis\n6.1 BERTopic Results\nWe first evaluate BERTopic on the preprocessed content of interview I0. The final model uses the\nall-mpnet-base-v2 embedding model for semantic encoding, UMAP for dimensionality reduction,\nand HDBSCAN for clustering. Using the optimized configuration, BERTopic generates a total of 17\ntopics, each representing a relatively coherent cluster of semantically related segments within the\ninterview. These topics span a wide range of themes, from procedural experiences and emotional\nreflections to logistical concerns and treatment decision-making, deeming this output suitable for a\nclinically-oriented software that clinical staff can use to quickly analyze patient data. A compre-\nhensive overview of the topics, along with their automatically generated labels and keywords, is\npresented in Table 3. The all-mpnet-base-v2 embedding model manages to produce rich semantic\nrepresentations that help to distinguish nuanced narratives.\nThe model output results in topics that are not only clinically relevant but also reflective of\nthe patient’s emotional and experiential journey through cancer treatment. For instance, Topic\n16 describes the patient’s experiences during their FOLFIRINOX (National Cancer Institute 2023)\nchemotherapy treatment, highlighting concerns such as neuropathy and treatment planning. Insights\nlike these can serve as valuable feedback for clinical staff, offering a patient-centered perspective on\nhow individuals are coping with the physical and psychological effects of specific therapies. On the\nother side of the spectrum, Topic 8 offers the patient’s logistical challenges and communication-\nrelated experiences, particularly in coordinating appointments and interactions with medical staff\nat Erasmus Hospital in Rotterdam. These types of topics can offer useful feedback to the hospital\nitself for improving internal processes, such as appointment coordination, patient communication,\nand overall administrative support. By surfacing these issues from patient narratives, the model\nprovides actionable insights that can contribute to a more patient-oriented care experience.\nThe pilot study also shows limitations of BERTtopic. 1) Some topic keywords contain duplicate\nwords due to the ngrams parameter. For example,in the keywords list of Topic 7, the word “port\ncath” appears as three different entries: “port”, “cath”, and “port cath”. While the inclusion of\nbigrams increases contextual richness, it can also lead to partial redundancy when both a phrase and\nits constituent words appear independently in the keyword list. Although this does not substantially\nimpact interpretability, it can affect visual clarity and compactness of the topic’s summary. On the\nother hand, this overlap may be beneficial when dealing with noisy or varied language, as it may\n"}, {"page": 9, "text": "Table 3:\nOutput of the Optimized BERTopic Model (Interview I0): pilot study\nTopic ID\nTopic Label\nTop 15 Keywords\n0\nExperiences and Challenges Navigating\nPatient Passes at Daniel den Hoed Can-\ncer Center\nweird, white, pass, understand, den, hoed, daniel, daniel den, den\nhoed, patient, outside, room room, team, notice, waiting\n1\nExperiencing Fear and Anxiety During\nMedical Examinations Involving Tubes\nand MRI Scans\nperiod, easy, lying, scary, tubes, throat, mri, tumor marker,\nmarker, examination, prepared, tumor, rest, sorry, ultrasound\n2\nRadiation Treatment Timeline:\nDelays,\nScheduling,\nand\nScans\nfrom\nJune\nto\nNovember\nradiotherapist, radiation treatments, june, months, months scan,\nend november, guus, guus meeuwis, meeuwis, radiation, december,\n21, follow, november, radio\n3\nTimeline\nof\nMalignant\nDiagnosis\nand\nHospital Visits Including MRI and Ultra-\nsound Examinations\nmalignant, april, write, timeline, place hospital, pretty, mri, walk-\ning, town, ultrasound, start, work, recording, raise, hospital hos-\npital\n4\nChallenges and Experiences with Blood\nDraws and Port-a-Cath Access in Cancer\nTreatment\nprick, cath, port, port cath, blood, needle, markers placed, hand,\nday, placed, markers, poked, difficult, puncture, puncture room\n5\nDiscussion\non\nCommunication\nand\nDecision-Making in Pancreatic Cancer\nTreatment Conversations\neating, talk, cancer, list, clear, pancreatic cancer, pancreatic,\ncertainly, eventually, conversation, surgeon, helps, time exciting,\nmoney, evening\n6\nEncouraging Patients to Ask Questions\nand Address Concerns During Appoint-\nments\nespecially, calls, questions, concerns, monday, head, difficult,\nmean, appointments, ask, involved, asked things, doctor going,\nreach, regular\n7\nChallenges and gaps in understanding\nport-a-cath placement and related surgi-\ncal procedures.\nbrand, surgery, kind information, size, port, port cath, cath, ex-\nample, information, 21, rotterdam, work, happens, eligible, drive\n8\nPatient’s\nexperience\ncoordinating\nap-\npointments and communication with doc-\ntors at Erasmus Hospital in Rotterdam.\nforget, wednesday, doctor doctor, hair, date, friday, send, rotter-\ndam, surgeon, erasmus, parking, specialized, hospital came, hos-\npital want, went hospital\n9\nDiscussion of tumor markers and the\ntimeline\nof\nmetastases\ndetection\nand\nmonitoring.\nmetastases, months, heard, tumor marker, marker, lot, year, tu-\nmor, spots, normal, lot googled, information, months blood, sam-\npling, blood sampling\n10\nImpact of 2017 Cancer Cure on Patient’s\nLife and Recovery Experience\n2017, cure, year, hands, true course, opinion, gee, intense, 11,\nhusband, home, took long, clear, certainly, rest\n11\nBowel Test Results and Persistent Pain\nLeading to Further Medical Referral\ntaken, test, feeling, bowel test, piece, bowel, poking, worked, re-\nferred, pain, monday, showed, away, read, touch\n12\nFamily Doctor Interactions and Patient\nConcerns During Cancer Treatment In-\nterviews\nfamily doctor, family, interview, grumpy, doctor family, worry,\nguys, time time, time doctor, long hair, hair, surgeon, nurse, def-\ninitely, puncture\n13\nAwkward Appointment Experiences and\nScan Discussions Over Two and a Half\nYears\nappointments, sigh, showing, half years, appointments new, years,\nlook, scans, came scan, awkward, learned, exactly question, ex-\nperts, cd, forward\n14\nDiscussion\nof\ntreatment\noptions\nand\nchoices made with the doctor during can-\ncer care.\nask doctor, woolly, place place, ask, goodbye, familiar, doctor\nactually, choice, discussed, options, effective, hospital ask, time\nheard, kept, want want\n15\nDiscussion\non\nmanaging\ninterruptions\nand planning during cancer treatment\nconversations.\nprepared, stop, coming, clearly, time speak, stop moment, plan,\nmention, fine, going, feel, moment, right, people, happy\n16\nConcerns\nabout\nneuropathy\nduring\nFOLFIRINOX\ntreatment\nprocess\nand\ncourse options discussed.\nneuropathy, concerns, folfirinox, told, treat, courses, left, process,\ntreatment process, folfiri, disease, huge, want want, write, treat-\nment\nhelp ensure that key terms are captured even when they appear in different forms across the corpus.\nThis error could potentially be fixed in a post-processing process, or even within the vectorizer\nsettings with extra tuning. Future iterations could involve filtering out redundant unigrams when a\nhigh-confidence bigram is present, though this would need to be balanced against the risk of losing\nrelevant variations in phrasing. 2) Another issue is overlapping themes spanning different topics. For\nexample, port-a-cath procedures are discussed in two different topics: Topic 4 and Topic 7. While\nthe lists of keywords are different, meaning that while port-a-cath is discussed, this could reflect a\ntrue conversational shift around the same object of interest, they still share a common theme, which\ncould be a sign of over-segmentation driven by overly sensitive clustering parameters. Although this\ncould become an issue, manual document analysis proves that the two topics are distinct enough to\nbe classified in individual clusters. Therefore, this occurrence is not a substantial issue, especially\nfor this preliminary comparison phase, but rather proof that the model is capable of identifying\nnuanced themes, even from within the same broader topic.\n"}, {"page": 10, "text": "6.2 Top2Vec Results\nThe final configuration of the model, including the UMAP and the HDBSCAN parameters, largely\nmirrors those used for BERTopic. The only major difference is the use of min count parameter\nin place of min df, which serves a similar function by controlling the minimum frequency a word\nmust appear in the corpus to be considered for the clustering process. The embedding model used\nby default in Top2Vec is all-MiniLM-L6-v2, as confirmed by the verbose output during the fitting\nprocess.\nUsing the optimal configuration, Top2Vec generates a total of 18 topics, which is two topics\nmore than the output generated by BERTopic. These topics are generally cohesive and interpretable,\nand could be reliably mapped back to the original chunks used during the model training. The final\nmodel output, along with labels and keyword lists, is presented in Table 4. In several cases, the\nmodel produces topic clusters that closely resemble those found in the BERTopic output.\nFor\ninstance, two distinct topics relating to port-a-cath procedures emerge, similar to those identified\nby BERTopic, with keywords reflecting different aspects of the same clinical theme. However, not\nall topic clusters capture the broader context with the same accuracy. In one case, a conversation\nsurrounding needle sizes is isolated into a broader, more general topic, namely Topic 18, but the\nmodel fails to recognize that the discussion is part of a larger conversation about needle types and\nprocedures, as the interview transcripts suggest. As a result, the topic is reduced to a generic theme\nabout “experiences and information”, with keywords focusing on different medical and non-medical\nterminology and procedural words, such as “asked” and “conversation”, rather than a cohesive and\nconcrete theme.\nIn general, the model produces interpretable results that align reasonably well\nwith human-labeled themes. The ability to trace topic clusters back to specific document chunks\nmakes manual evaluation possible and useful.\n4 While both C-Top2Vec and the original Top2Vec\nimplementations are explored during the experimentation process, we only use the latter in the final\ncomparative analysis. This decision is due to the fact that the original Top2Vec model supports\ndocument-to-topic mapping.\n6.3 Model Evaluation and Comparison with Human Perspectives\nTo evaluate the coherence, relevance, and clinical interpretability of the topics produced by BERTopic\nand Top2Vec, we conducted a small-scale user study involving three volunteer participants. Each\nparticipant was provided with the anonymized cancer interview I0 and asked to read it carefully.\nThey were then asked to answer five questions for each model output. The full survey can be found\nin Appendix A. While the volunteers are not clinical experts, the task of judging the coherence and\ncontextual relevance of the topics does not require specialized expertise and can be meaningfully\nperformed by general readers with enough context from the source material. Participants rated each\ntopic on a scale from 1 to 5 based on its coherence and usefulness (Q1), and separately evaluated\nthe associated top 15 keywords for how well they described the topic (Q3). They were also asked\nto identify any essential themes that the models may have missed. The outputs that they were\nprovided with were Tables 3 and 4. They did not have access to the representative documents from\neach topic, but they were allowed to go back to the interview in order to check whether the topic\nlabel was, in fact, describing a topic that was talked about during the interview.\nThe results for Q1 (topic coherence and usefulness) are in Figure 3. It shows that the ratings for\nBERTopic are on average higher and there are fewer topics that score very low compared to Top2vec.\nOverall, BERTopic performed strongly, with 12 out of the 17 topics receiving a coherence score of\n4.0 or higher. The results for Q3 (keyword descriptiveness) are in Figure 4. While Top2Vec achieved\n4. Using the same parameters as the final BERTopic model with six-sentence chunks, C-Top2Vec produces up to\n86 topics, many of which are overly fragmented or completely redundant.\nWhile increasing min samples and\nmin cluster size parameters does reduce the topic count, to around 30 topics on average at best, the resulting\nclusters often lack coherence and show heavy overlap in keywords. In addition, without the ability to trace topics\nback to the original text segments, evaluating the quality of topics becomes nearly impossible.\n"}, {"page": 11, "text": "Table 4: Output of the Optimized Top2Vec Model (Interview I0): pilot study\nTopic ID\nTopic Label\nTop 15 Keywords\n0\nPatient Experiences and Emotions Dur-\ning Cancer Appointments and Treatment\nEnvironments\nsuddenly, conversation, room, happened, exciting, talk, appoint-\nment, story, hadn, later, move, cry, experiences, felt, was\n1\nChallenges and Experiences During Can-\ncer Diagnosis and Treatment Journey\nmri, surgeon, surgery, appointment, patient, doctor, ultrasound,\ntumor, hospital, appointments, metastases, scan, cancer, exami-\nnation, malignant\n2\nPatient Experiences with Cancer Treat-\nments, Cures, and Recovery Challenges\ncure, cures, treatments, patient, treatment, pain, surgery, doctor,\nmetastases, rest, puncture, appointment, no, yes, mri\n3\nPatient Experiences and Challenges Dur-\ning Cancer Treatment,\nAppointments,\nand Surgical Procedures\ntreatment, treatments, patient, appointment, doctor, surgery,\nchemo, cure, cures, experiences, metastases, cancer, appoint-\nments, concerns, experienced\n4\nInteractions with Healthcare Profession-\nals During Cancer Diagnosis and Treat-\nment\ndoctor, surgeon, hospital, patient, appointment, nurse, nurses,\nsurgery,\nappointments,\ninterview,\nultrasound,\nexamination,\nmaybe, secretary, coach\n5\nPatient Experiences with Doctor Com-\nmunication and Appointment Manage-\nment in Cancer Care\nappointment, patient, hospital, doctor, appointments, nurse, call,\nconversation, calls, nurses, surgery, suddenly, treatment, talk, sur-\ngeon\n6\nTimeline of Cancer Diagnosis and Treat-\nment: Metastases, Tumor Markers, and\nPatient Experience\nmetastases, tumor, cancer, malignant, chemo, mri, patient, ap-\npointment, surgery, treatments, months, march, timeline, treat-\nment, weeks\n7\nPatient Experiences with Doctors and\nHospitals During Cancer Diagnosis and\nTreatment\ndoctor, hospital, appointment, patient, surgeon, appointments,\nnurse, nurses, surgery, ultrasound, mri, tumor, malignant, metas-\ntases, examination\n8\nPatient Experiences and Communication\nwith Medical Professionals at Erasmus\nHospital for Cancer Treatment\nerasmus, appointment, doctor, surgeon, surgery, hospital, patient,\nappointments, treatment, examination, tumor, nurse, mri, punc-\nture, treatments\n9\nPatient\nExperience\nwith\nRadiation\nTreatment and Follow-Up Appointments\nfor Cancer Management\nradiation, radiotherapist, radio, patient, treatment, treatments,\nappointment, surgery, cancer, metastases, tumor, doctor, cure,\nmri, surgeon\n10\nChallenges and Experiences with Blood\nDraws and Port-a-Cath Procedures in\nCancer Treatment\nneedle, puncture, poked, patient, blood, ultrasound, mri, prick,\ntubes, appointment, scan, port, treatments, surgery, examination\n11\nExploring the Role of Turmeric and Pa-\ntient Questions in Cancer Treatment De-\ncisions\nturmeric, cures, treatments, cure, certainly, only, no, concerns,\ndefinitely, doctor, maybe, treatment, always, probably, surgeon\n12\nNavigating Hope and Treatments in Pan-\ncreatic Cancer: A Patient’s Journey and\nConversations\ncancer, pancreatic, tumor, chemo, metastases, malignant, patient,\ncure, treatments, cures, treatment, doctor, hope, surgery, bowel\n13\nExperiences and Concerns Surrounding\nPort-a-Cath Placement and Blood Draw\nProcedures\nsurgery, port, puncture, patient, needle, cath, surgeon, tubes,\nnurse, hospital, obviously, fact, appointment, nurses, operation\n14\nBowel and Pancreatic Cancer Diagnosis\nJourney: Pain, Tests, and Treatment Ex-\nperiences\nbowel, pancreatic, patient, puncture, hospital, ultrasound, doc-\ntor, pain, appointment, mri, surgery, examination, scan, tumor,\nsurgeon\n15\nPatient Experiences with Chemotherapy\nOptions and Neuropathy Management in\nCancer Treatment\ntreatments, neuropathy, treatment, cure, doctor, patient, surgery,\ncures, surgeon, folfirinox, mri, chemo, tumor, cancer, pain\n16\nNavigating Appointments and Treatment\nfor Pancreatic Cancer at Rotterdam Hos-\npital\nrotterdam, hospital, appointment, erasmus, patient, appoint-\nments, surgery, doctor, pancreatic, surgeon, tumor, interview,\ntreatment, experiences, examination\n17\nPatient Experiences and Concerns Re-\ngarding MRI and Scan Examinations in\nCancer Care\nscan, mri, ultrasound, certainly, examination, appointment, yes,\npatient, radio, no, ask, probably, radiotherapist, well, asked\n18\nPatient\nExperiences\nand\nInformation\nGaps During Surgical Appointments and\nExaminations in Oncology\nsize, patient, surgery, questions, doctor, information, experiences,\nexamination, nurse, nurses, surgeon, experienced, appointment,\nconversation, asked\nlower topic scores on average than BERTopic, it performed better in terms of keyword ratings.\n15 out of 19 topics received a keyword score of 4 or above, indicating that participants generally\nfound the keywords descriptive and relevant to the topic content. A common point of feedback from\nthe participants was that some topics appeared to overlap in content, such as between Topics 4,\n5, and 7. This, alongside the more thematically general topic labels, led the volunteers to rank\nthe BERTopic output higher than the Top2Vec model overall, despite the high keyword list scores,\nfurther supporting the idea that keywords are less relevant when extracting helpful information from\nclinical interviews. This shows that while Top2Vec extracted more potential topics than BERTopic,\nit might fail to capture nuanced themes, which are crucial for the primary goal of this research.\n"}, {"page": 12, "text": "Figure 3: Distribution of ratings for Top2Vec and BERTopic topics (Q1), averaged over 3 raters.\nNote that Top2Vec has generated 19 topics and BERTopic has generated 17 topics.\nFigure 4: Distribution of ratings for Top2Vec and BERTopic keywords (Q3), averaged over 3 raters.\nNote that Top2Vec has generated 19 topics and BERTopic has generated 17 topics.\nFor Question 2, which asks how well the extracted topics represent the content of the interview\noverall, the BERTopic model was ranked higher than the Top2Vec model, with the added mention\nfrom all three of the evaluation participants that, while both models manage to extract mostly cohe-\n"}, {"page": 13, "text": "pilot data\nI(0) / I(2)\nFull data\n(13 interviews)\nPre-processed\nData\nBERT-topic \nframe\nclinical \nembedding\nselection\nGlobal analysis \n(topic prevalence \n& Approximate \ndistribution)\nBioClinicalBERT\nClinicalBERT\nBiomedBERT\nselected \nclinical \nembeddings\nPilot \nstudy \nwinner\nBERT-topic \nFrame\n(param. adaptation)\nKeyword list\nLLM \nprompting\nFigure 5: BERT-topic Deployment on all Interviews with Clinical LM Embeddings\nsive topics from the I0 interview, the BERTopic model’s output is way more precise and contained\nless overlap.\nReviewing the questions we designed:\n• Q1/Q2 is on the topic label (from LLMs) to the interview - Bert-topic wins, meaning Bert-topic\nis better at reflecting the interview topic.\n• Q3 is on how well the extracted key word list (from TModel) supports the labeled topics\n(from LLM) - Top2Vec wins, meaning Top2Vec extracted keyword lists better connect with\nthe summarized topic label.\nFrom the overlapping (phenomenon) and cognitive (human reasoning effort) perspectives, we hy-\npothesis that the more over-lapped keyword/theme list from Top2Vec can help annotators under-\nstand/infer the labeled topic better with less inference effort, because more isolated topic keywords\n(e.g. from BERT-topic) would need annotators to carry out more reasoning themselves to connect to\nthe labeled topics. This is comparison-wise. On the other hand, however, this does not necessarily\nmean that the extracted keyword list by Top2Vec are better reflecting the interview document in\ncomparison to BERT-topic.\nTaking the evaluation process, as well as the experimentation and results, into consideration, we\ndecide to focus the analysis on BERTopic further. Not only did BERTopic demonstrate that it can\nconsistently produce topics of high overall quality, but it also offers greater flexibility and extensibility\nbecause it is compatible with a wide range of publicly available embedding models. This opens up\nthe possibility of integrating clinically-oriented embedding models that may further enhance the\nmodel’s ability to extract relevant, interpretable, and context-sensitive information from patient\nnarratives.\n7. Investigating the Generalizability with Global Analysis\n7.1 Domain-specific Clinical/Medical Embedding Model Selection\nTo explore whether clinically oriented embeddings improve BERTopic’s performance on patient\nstorytelling data, we experimented with three widely used models on I(0) interview: BioClini-\ncalBERT (Alsentzer et al. 2019), ClinicalBERT (Liu et al. 2025), and MSR BiomedBERT\n(formerly PubMedBERT) (Gu et al. 2020) for domain-specific embedding model selection, as shown\nin Figure 5. All experiments used the same BERTopic configuration as in Section 5.2 to ensure\nconsistency. ClinicalBERT produced 17 topics initially, but coherence issues persisted even after\n"}, {"page": 14, "text": "tuning parameters such as chunk size and dimensionality reduction. Moreover, MSR BiomedBERT\nalso struggles with topic coherence to a greater extent than the previous models. For example, one\nof the topics, incorrectly labeled as “Monitoring Eye Health and Treatment Progress in Cancer Care\nDiscussions”, includes representative document chunks that are not only short and uninformative,\nbut one of the documents also includes the expression “to keep an eye on”, which is the reason for\nthe misleading topic label. This means that the model struggles to create meaningful clusters, which\nleads to the LLM mislabeling the topic due to the lack of context and coherence within the represen-\ntative documents and the list of keywords. This relatively weaker performance may be attributed to\nthe nature of the dataset used to pretrain MSR BiomedBERT, which is probably less aligned with\nnarrative-style patient data.\nIn contrast, BioClinicalBERT consistently generated more interpretable and clinically meaningful\ntopics, likely due to its pretraining on real clinical notes. Its outputs captured both technical medical\nexperiences and patient perspectives with greater nuance, outperforming the other models in terms of\ncoherence and relevance. These findings suggest that embedding models trained on clinical narratives\nprovide stronger semantic representations for this type of data.\nFurther tests on shorter interviews (such as I(2), which is the shortest in the dataset at approx-\nimately 5596 words) revealed that chunk size also plays a critical role: Reducing the chunk size to\n6 sentences increases the number of topics to I(2), with a noticeable improvement in nuance and\nprecision. The resulting topics capture more specific clinical moments and emotionally substantial\nstatements, enhancing the interpretability and relevance of the output. This indicates that a dy-\nnamic chunking strategy, combined with clinically oriented embeddings such as BioClinicalBERT,\nmay be essential for optimizing topic modeling across datasets with varying lengths and complexities.\n7.2 Global Analysis on All Interview Data\n7.2.1 Model Setup\nAs shown in Figure 5 (middle-horizontal), to gain deeper insights into the full dataset of 13 interviews,\nwe applied topic modeling to the entire corpus rather than individual interviews, aiming to uncover\noverarching themes and recurring patterns. For this global analysis, we used BioClinicalBERT,\nidentified in Section 7.1 as the most effective domain embedding model for producing coherent\nand clinically meaningful topics. Its domain-specific training on biomedical and clinical text makes\nit well-suited for capturing general themes. Unlike the per-interview approach, which prioritized\nfine-grained topics, this phase focused on broader themes.\nFor model configuration, we increased the chunk size from 6 to 7 sentences to provide more context\nper document, as larger chunks tend to yield fewer but more generalized topics. The BERTopic\npipeline was tuned/adapted accordingly: UMAP parameters were adjusted to promote broader\nclustering (n neighbors = 16, min dist = 0.2, n components = 4), and HDBSCAN settings were\nmodified to favor stable clusters (min cluster size = 11, cluster selection = eom). These changes\nbias the model toward extracting higher-level themes that recur across interviews. We also revised\nthe LLM-prompt labeling process to suit global themes. Instead of using representative documents,\nthe new LLM prompt focuses exclusively on keywords for each topic:\n“You are an AI that labels discussion topics, from a collection of cancer storytelling\ninterviews, for a software that allows doctors to browse through medical files without\nthe need to read them from start to finish. Given the following keywords, provide a\nclear and specific topic label, with enough context to be interpretable, focusing on the\nkeyword list. Be general. Keep it short. Only type the topic label and nothing else.”\nThis approach ensures more objective and consistent labels for overarching themes. Additionally,\nwe expanded the stop word list to reduce noise, though translation errors and linked words across\ninterviews made perfect filtering impractical without manual review.\n"}, {"page": 15, "text": "7.2.2 Model Output: Topic Analysis\nUsing the above configuration and parameters, we fitted the topic model on the entire corpus of 13\ninterviews. The resulting output (Table 5) reveals overarching themes that span all interviews, of-\nfering a broad view of the cancer treatment experience. Unlike granular per-interview models, which\ncapture individual nuances, the global model identifies recurrent concerns and shared points across\npatients, enabling detection of systemic issues and common emotional or physical pain points. For\nexample, Topic 0 includes keywords such as “oxycontin”, “medications”, and “nausea”, highlight-\ning widespread struggles with medication side effects. Topic 2 reflects how patients recall surgical\ninterventions, specifically keyhole surgery, while Topic 7 clusters references to timelines and appoint-\nments, suggesting opportunities for visual treatment timelines. Similarly, Topic 4 (Sleep Patterns\nand Nighttime Activities) emphasizes recurring sleep-related discussions, indicating that sleep is a\nsignificant aspect of the cancer journey and warrants deeper analysis to uncover common issues\naffecting rest and well-being.\nOther topics capture institution-specific and emotional insights. Topic 9 focuses on “Support\nand Resources for Cancer Care at Erasmus MC”, referencing amenities like food, lighting, and\nbuildings. These are elements that, while non-clinical, influence patient experience and could inform\nfacility improvements. Emotional responses also emerge prominently: Topic 8, labeled “Coping\nwith Treatment Setbacks and Emotional Reactions”, groups keywords related to coping mecha-\nnisms, underscoring the need for mental health integration in oncology care. Finally, Topic 12\n(Navigating Treatment Decisions with Specialist Nurses) highlights the critical role of nurses in\nguiding patients through complex choices, providing both clinical clarity and emotional reassurance.\nOverall, the globally tuned model captures high-level patterns across medical procedures, emotional\nresilience, logistics, and support systems, complementing per-interview analyses. While minor issues\nlike repetitive keywords (e.g., “day day” in Topic 0) persist, they are less pronounced, confirming\nthe effectiveness of this broader modeling approach.\n7.2.3 Topic Distribution in the whole corpus\nFigure 6 shows the distribution of topics over interviews. Rather than assigning a single topic to\neach chunk, the distribution is based on the soft output of the BERTopic model.5 This allows for\nthe possibility that a chunk touches on multiple themes to varying degrees. By averaging these\nprobability distributions across all chunks in a given interview, an interpretable vector is produced\nthat captures how strongly each topic is present throughout the interview as a whole. This approach\nis useful for capturing thematic overlap and uncertainty that may be missed with hard topic\nassignments. Table 6 lists the 5 highest probability topics across all 13 interviews.\nThe results indicate that Topic 14 (Coordination and Communication in Cancer Care Man-\nagement) appears predominantly across several interviews (e.g. Interviews 0, 1, 3, 4, 8), even in\nthose where it was not one of the top three in prevalence view (most notably, Interview 3). This\nsuggests that while it may not have been the main focus in any one interview, it is a persistent\ntheme that underlies many conversations, with the coordination of the medical team likely being a\ncommon underlying conversational topic among all patients. Another shift of this sort occurs with\nTopics 12 (Navigating Treatment Decisions with Specialist Nurses) and 13 (CyberKnife Treatment\nProgram in Rotterdam), which are rarely top topics by prevalence, but frequently show up in the\napproximate distribution. This implies these themes may appear in shorter or more subtle forms,\nmentioned briefly, but across a wide range of patients. Meanwhile, the sharply focused topics (such\nas Topics 0 or 2) remain central only to select individuals.\n5. This functionality is provided directly by BERTopic through its approximate distribution() method, which\nestimates the topic probabilities for each chunk without requiring re-fitting the model.\n"}, {"page": 16, "text": "Table 5: BERT-topic Global Analysis using BioClinicalBERT embedding on All 13 Interviews\nTopic ID\nTopic Label\nTop 15 Keywords\n0\nMedication Management and Symptom Relief in\nCancer Care\nknee, day day, pills, oxycodone, medications, diarrhea, medica-\ntion, symptoms, times day, went doctor, nausea, stomach, pre-\nscribed, consultantdoctor, ones\n1\nImpact of Chemotherapy on Patient Experience\nand Expectations\nchemo, chemotherapy, intense, effects, tomorrow, paper, oncol-\nogist, does thats, start chemo, took long, oncologist oncologist,\ndoor, meeting, cells, drive\n2\nPlacement of Portacath via Keyhole Surgery\nportacath, placed, arm, keyhole surgery, keyhole, anesthesia,\nsurgery, portacath portacath, probe, puncture, puts, run, shower,\nsedated, poked\n3\nNutrition and Dietary Habits in Cancer Care\ncook, drinking, eating, sandwich, food drink, taste, eat, eating\ndrinking, food, dietician, weight, eaten, brother, soup, fat\n4\nSleep Patterns and Nighttime Activities\nsleep, downstairs, bed, couch, awake, lie, bathroom, watch, single,\nwash, groceries, rest rest, outside, upstairs, cup\n5\nFamily Support and Life Impact in Cancer Jour-\nneys\nson, sister, joint, mother, project, children, twice, life, lives, times\ntime, live, kind thing, older, child, large\n6\nDiagnostic Imaging and Tests for Abdominal\nConditions\nbowel, ultrasound, mri, stomach, examination, appendix, tests,\nct, biopsy, admission, ct scan, medium, pain clinic, scan hospital,\ntaken\n7\nRadiation Therapy Treatment Experiences and\nSide Effects\nradiotherapist, radiation, courses, treatments, poked, december,\noctober, thats possible, abdominal pain, abdominal, operate,\nmarkers, september, november, placed\n8\nCoping with Treatment Setbacks and Emotional\nReactions\nfailed, plan, weeks later, wall, reactions, success, calmly, tremen-\ndous, dirty, alive, face, march, nerves, cells, tried\n9\nSupport and Resources for Cancer Care at Eras-\nmus MC Hospital\neuros, erasmus, erasmus mc, mc, places, hospitals, hospital eras-\nmus, light, food drink, support, hair, approach, lot people, possi-\nbly, building\n10\nPatient Experience with Doctor Appointments\nand Risk Assessment\nrisk, doctor hospital, date, wonder, gosh, appointment doctor, for-\nget, data, rotterdam, touch, tomorrow, space, ended, wife, march\n11\nPatient Experience with Medical Equipment and\nCare Delays\npump, ticket, broken, waited, burden, air, walked, nurses, hours,\ndoes work, minutes, outpatient, decisions, early, nursing\n12\nNavigating Treatment Decisions with Specialist\nNurses\ndecisions, treatment process, experiences, trajectory, open, calls,\nimportant decision, advise, negative, specialist nurse, shes, cries,\nreal, super, metastatic\n13\nCyberKnife Treatment Program in Rotterdam\nrotterdam, program, cab, stone, cyberknife, quarter past, button,\nfile, quarter, puncture, push, examination, family doctor, liver,\nrecord\n14\nCoordination and Communication in Cancer Care\nManagement\nresponsible, team, secretary, order, number, personal, doctor\ncome, creon, surgeon, clear, scary annoying, short, conversations,\nturn, knows\nTable 6: Most Occurring Topics Overall (Approx. Distribution)\nTopic ID\nMean Avg. Probability\nTopic Label\n14\n0.118\nCoordination of Medical Team and Patient Care\n12\n0.079\nPatient Decision-Making in Cancer Treatment Journey\n3\n0.071\nNutrition and Dietary Habits in Cancer Care\n13\n0.070\nCyberKnife Treatment Program in Rotterdam\n2\n0.053\nPlacement of Portacath via Keyhole Surgery\n8. Discussion\nThe experiments conducted in this study provide insight into the capabilities of BERTopic and\nTop2Vec for extracting clinically relevant themes from cancer patient interviews. Overall, BERTopic\n– particularly when configured with clinically oriented embeddings and sentence-based chunking –\ndemonstrated a stronger ability to capture nuanced topics. These findings suggest that topic mod-\neling could serve as the basis for clinical feedback tools, enabling medical staff to navigate lengthy,\nunstructured patient documents efficiently and focus on patient-centered care. Comparing BERTopic\nand Top2Vec revealed clear differences: while both produced coherent topics, BERTopic consistently\ngenerated more precise and less overlapping themes, which is critical in medical contexts, while\nTop2Vec generated more descriptive key terms. The ability to leverage domain-specific embeddings\nsuch as BioClinicalBERT further improved performance, confirming that embedding selection plays\na key role in topic coherence and relevance.\n"}, {"page": 17, "text": "Figure 6: Approximate distribution across all 13 interviews, where each color represents a topic, and\neach bar represents one of the 13 interviews.\nWe would like to point out a few limitations. First, topic modeling proved highly sensitive to\nchunking strategies: smaller chunks increased granularity but fragmented narratives, while larger\nchunks preserved context but reduced specificity. This trade-off between granularity and context\npreservation proves to be a recurring theme in the experimental setup for both BERTopic and\nTop2Vec. Second, while substantial effort was made to optimize the processing of the data, it is\npossible that alternative approaches or configurations, particularly in areas such as preprocessing,\nembedding selection, or clustering strategies, could have yielded improved or different results. Third,\nour evaluation survey was small-scale and lacked clinical experts, meaning that judgments were based\non interpretability rather than professional applicability. Dataset characteristics introduced further\nchallenges: interviews were originally in Dutch and translated into English, which may have altered\nmeaning. These issues underscore broader resource limitations for non-English clinical NLP. Despite\nthese challenges, the study demonstrates the potential of topic modeling as a foundation for clinical\nsupport tools that streamline workflows and amplify patient voices.\n9. Conclusions and Future Work\nOverall, this study highlights the value of embedding-based topic modeling for clinical applications\non patient care. By automating the extraction of meaningful themes from patient narratives, tools\nlike BERTopic could enhance workflow efficiency, awareness of patient concerns, and strengthen\npatient-doctor communication.\n"}, {"page": 18, "text": "In response to the first research question, the results show that current neural topic\nmodeling techniques, namely Top2Vec and BERTopic, can extract a variety of relevant themes\nfrom patient interviews. These include emotional experiences, treatment details, personal struggles,\nand reflections on the treatment processes. While both techniques produce fairly coherent and easily\ninterpretable topics, BERTopic, especially when paired with an embedding model pretrained on large\namounts of clinical data, such as BioMedicalBERT, and a sentence-based chunking strategy, delivers\nmore refined and focused results, which better represent the patients’ experiences and concerns\nexpressed during the interviews. This makes BERTopic more promising for practical use in clinical\nenvironments, as opposed to Top2Vec, given the specific experimental setup and dataset.\nAs for the second research question, the extracted topics suggest several ways in which\ntopic modeling could support and improve healthcare processes. Most importantly, they could help\nclinicians identify and understand key moments in a patient’s narrative without having to read ev-\nery transcript manually. This could save time, reduce the workload of clinical staff, and give more\nvisibility to the patient’s voice, especially in cases where emotional or psychological concerns might\notherwise be overlooked.\nAlthough not empirically tested in this study, this type of automated\nworkflow for analyzing patient documents could also help reduce the risk of overlooking impor-\ntant details in a patient’s history. By eliminating the need for clinicians to read through lengthy\ntranscripts manually, the system may lessen the chance of missing key information due to time con-\nstraints or fatigue. Although this study did not involve a clinical trial or professional assistance, the\nstructure of the output, combined with feedback from the conducted small-scale human evaluation,\nshows clear potential for integrating topic modeling into tools that support patient-centered care.\nMoreover, the global analysis of all 13 interviews reveals recurring themes discussed by all patients,\nwhich could potentially assist in identifying patterns in cancer patient treatment journeys in order\nto mitigate common issues.\nWhile this work remains a proof of concept, it lays the foundation for future research focused\non multilingual capabilities, contextual models, improved evaluation frameworks, and integration\ninto clinical decision-support systems. Such advancements could help transform narrative data into\nactionable insights, placing patient voices at the center of healthcare delivery.\nFor future work, it will be important to test the system with clinical experts to better understand\nhow the generated topics can be applied/integrated in a real-world healthcare setting. Beyond that,\nmoving past translated text is an important next step. This includes not only exploring Dutch-\nlanguage embedding models suitable for processing the original interviews, but also gathering new\ndatasets in English and other languages.\nDoing so would serve the purpose of supporting the\ndevelopment and training of more robust multilingual, domain-specific embedding models, but it\nwould also allow this approach to be tested on native language data to see how it performs without\nthe distortions that come with translation. Additionally, future experiments could explore more\nadaptive or dynamic chunking strategies, which might better balance granularity and contextual\ncoherence, especially across interviews of different lengths and structures. Lastly, the global dataset\nanalysis could be expanded to track how the identified global themes evolve across different patient\npopulations over time (temporal dimension), and offer solutions to help clinical staff solve or minimize\ncommon complaints from cancer patients.\n10. Acknowledgement\nWe thank Ida Korfage and Sheila Payne for the valuable comments on the Abstract version of this\nwork (presented at CLIN2025). We thank Anne Kuld-Nielsen for valuable feedback and encourage-\nment from reading the manuscript.\nThis work has been funded by the European Union under Horizon Europe Work Programme\n101057332.\nViews and opinions expressed are however those of the author(s) only and do not\nnecessarily reflect those of the European Union or the European Health and Digital Executive\nAgency (HaDEA). Neither the European Union nor the granting authority can be held responsible\n"}, {"page": 19, "text": "for them. The UK team are funded under the Innovate UK Horizon Europe Guarantee Programme,\nUKRI Reference Number: 10041120.\nReferences\nAlbalawi, Rania, Tet Hin Yeap, and Morad Benyoucef (2020), Using topic modeling methods\nfor short-text data:\nA comparative analysis, Frontiers in Artificial Intelligence 3, pp. 42.\nhttps://www.frontiersin.org/articles/10.3389/frai.2020.00042/full.\nAlon,\nYaniv,\nEtti Naimi,\nChedva Levin,\nHila Videl,\nand Mor Saban (2025),\nLeveraging\nnatural language processing to elucidate real-world clinical decision-making paradigms:\nA\nproof\nof\nconcept\nstudy,\nJournal\nof\nBiomedical\nInformatics\n136,\npp.\n104829.\nhttps://doi.org/10.1016/j.jbi.2025.104829.\nAlsentzer, Emily, John Murphy, William Boag, Wei-Hung Weng, Yuhao Jin, Tristan Naumann,\nand Matthew McDermott (2019), Publicly available clinical bert embeddings, arXiv preprint\narXiv:1904.03323.\nAngelov,\nDimo\n(2020),\nTop2vec:\nDistributed\nrepresentations\nof\ntopics,\narXiv\npreprint\narXiv:2008.09470.\nAngelov, Dimo and Diana Inkpen (2024), Topic modeling: Contextual token embeddings are all you\nneed, in Al-Onaizan, Yaser, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Associ-\nation for Computational Linguistics: EMNLP 2024, Association for Computational Linguistics,\nMiami, Florida, USA, pp. 13528–13539. https://aclanthology.org/2024.findings-emnlp.790/.\nBak, Marieke, Laura Hartman, Charlotte Graafland, Ida J Korfage, Alena Buyx, Maartje Schermer,\n4D PICTURE Consortium, et al. (2025), Ethical design of data-driven decision support tools\nfor improving cancer care: embedded ethics review of the 4d picture project, JMIR cancer 11\n(1), pp. e65566, JMIR Publications Inc., Toronto, Canada.\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan (2003), Latent dirichlet allocation, J. Mach.\nLearn. Res. 3 (null), pp. 993–1022, JMLR.org.\nCer, Daniel, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-\nstant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and\nRay Kurzweil (2018), Universal sentence encoder, arXiv preprint arXiv:1803.11175.\nDeepL GmbH (2024), DeepL Translator. Accessed: 2025-05-13.\nDemner-Fushman,\nDina,\nWendy W. Chapman,\nand Clement J. McDonald (2009),\nWhat\ncan natural language processing do for clinical decision support?, Journal of Biomed-\nical\nInformatics\n42\n(5),\npp.\n760–772.\nBiomedical\nNatural\nLanguage\nProcessing.\nhttps://www.sciencedirect.com/science/article/pii/S1532046409001087.\nEgger, Roman and Jie Yu (2022), Interpretable topic modeling for social media analysis: Covid-19\nand travel on twitter, Frontiers in Sociology 7, pp. 850586.\nErasmus MC (2025), Erasmus university medical center. Accessed: 2025-05-12.\nFoundation, Python Software (2024), python-docx 0.8.11 documentation. Accessed: 2025-05-13.\nhttps://python-docx.readthedocs.io/en/latest/.\nGhosh, Subhajit (2024), Natural language processing: Basics, challenges, and clustering applica-\ntions, Federated learning for Internet of Vehicles: IoV Image Processing, Vision and Intelligent\nSystems, Bentham Science Publishers, pp. 61–82.\n"}, {"page": 20, "text": "Grici¯ut˙e, Bernadeta, Lifeng Han, and Goran Nenadic (2023), Topic modelling of swedish newspaper\narticles about coronavirus: a case study using latent dirichlet allocation method, 2023 IEEE\n11th International Conference on Healthcare Informatics (ICHI), pp. 627–636.\nGriffioen, Ingeborg, Marijke Melles, Anne Stiggelbout, and Dirk Snelders (2017), The potential of\nservice design for improving the implementation of shared decision-making, Design for Health\n1 (2), pp. 194–209, Taylor & Francis.\nGriffioen, Ingeborg PM, Judith AC Rietjens, Marijke Melles, Dirk Snelders, Marjolein YV Homs,\nCasper H van Eijck, and Anne M Stiggelbout (2021), The bigger picture of shared decision\nmaking: a service design perspective using the care path of locally advanced pancreatic cancer\nas a case, Cancer Medicine 10 (17), pp. 5907–5916, Wiley Online Library.\nGrootendorst, Maarten (2022), Bertopic:\nNeural topic modeling with class-based tf-idf, arXiv\npreprint arXiv:2203.05794.\nGu, Yu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Nau-\nmann, Jianfeng Gao, and Hoifung Poon (2020), Domain-specific language model pretraining\nfor biomedical natural language processing.\nHan, Lifeng, Serge Gladkoff, Gleb Erofeev, Irina Sorokina, Betty Galiano, and Goran Nenadic\n(2024), Neural machine translation of clinical text: an empirical investigation into multilingual\npre-trained language models and transfer-learning, Frontiers in Digital Health 6, pp. 1211564,\nFrontiers Media SA.\nKidanemariam, Martha, Matthijs A Graner, Willem Jan W Bos, Marielle A Schroijen, Eelco JP\nde Koning, Anne M Stiggelbout, Arwen H Pieterse, and Marleen Kunneman (2024), Patient-\nclinician collaboration in making care fit: A qualitative analysis of clinical consultations in\ndiabetes care, Patient Education and Counseling 125, pp. 108295, Elsevier.\nLiu, Xiaohong, Hao Liu, Guoxing Yang, Zeyu Jiang, Shuguang Cui, Zhaoze Zhang, Huan Wang,\nLiyuan Tao, Yongchang Sun, Zhu Song, et al. (2025), A generalist medical language model for\ndisease diagnosis assistance, Nature medicine 31 (3), pp. 932–942, Nature Publishing Group\nUS New York.\nNational Cancer Institute (2023), Folfirinox. Accessed: 2025-05-17. https://www.cancer.gov/about-\ncancer/treatment/drugs/folfirinox.\nOhno, Yukiko, Tohru Aomori, Tomohiro Nishiyama, Riri Kato, Reina Fujiki, Haruki Ishikawa,\nKeisuke Kiyomiya, Minae Isawa, Mayumi Mochizuki, Eiji Aramaki, and Hisakazu Ohtani\n(2025), Performance improvement of a natural language processing tool for extracting patient\nnarratives related to medical states from japanese pharmaceutical care records by increasing\nthe amount of training data: Nlp analysis and validation study, JMIR Medical Informatics,\nJMIR Publications. https://www.jmir.org/2025/1/e68863.\nOpenAI (2024), Gpt-4o mini:\nadvancing cost-efficient intelligence.\nAccessed:\n2025-05-12.\nhttps://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/.\nRaj, Minakshi, Kerry Ryan, Philip Sahr Amara, Paige Nong, Karen Calhoun, M Grace Trinidad,\nDaniel Thiel, Kayte Spector-Bagdady, Raymond De Vries, Sharon Kardia, et al. (2023), Policy\npreferences regarding health data sharing among patients with cancer: public deliberations,\nJMIR cancer 9 (1), pp. e39631, JMIR Publications Inc., Toronto, Canada.\nReimers, Nils and Iryna Gurevych (2020), Sentence-bert: all-mpnet-base-v2. Accessed: 2025-05-17.\n"}, {"page": 21, "text": "Ren, Libo, Yee Man Ng, and Lifeng Han (2025), Malei at multiclinsum: Summarisation of clinical\ndocuments using perspective-aware iterative self-prompting with llms, BioASQ WS at CLEF\n2025.\nSheikhalishahi, Sara, Riccardo Miotto, Joel Dudley, Alberto Lavelli, Fabio Rinaldi, and Venet Os-\nmani (2019), Natural language processing of clinical notes on chronic diseases: Systematic re-\nview, JMIR Medical Informatics 7 (2), pp. e12239. https://medinform.jmir.org/2019/2/e12239.\nSomani,\nSulaiman,\nMarieke\nVan\nBuchem,\nAshish\nSarraju,\nTina\nHernandez-Boussard,\nand\nFatima\nRodriguez\n(2023),\nAnalyzing\npatient\nexperiences\nusing\nnatural\nlan-\nguage\nprocessing:\ndevelopment\nand\nvalidation\nof\nthe\nartificial\nintelligence\npatient\nreported\nexperience\nmeasure\n(ai-prem),\nJAMA\nNetw\nOpen\n6\n(4),\npp.\ne239747.\nhttps://jamanetwork.com/journals/jamanetworkopen/fullarticle/2803988.\nTU Delft (2021), Better prepared for big decisions in healthcare. Accessed: 2025-05-12.\nVan Buchem, Marieke, Olaf M. Neve, Ilse M. J. Kant, Ewout W. Steyerberg, Hileen Boos-\nman, and Erik F. Hensen (2022), Artificial intelligence–enabled analysis of statin-related\ntopics and sentiments on social media, BMC Medical Informatics and Decision Making.\nhttps://doi.org/10.1186/s12911-022-01923-5.\nWahbeh, Abdullah, Mohammad Al-Ramahi, Omar El-gayar, Ahmed Elnoshokaty, and Tareq Nas-\nralah (2025), Evaluating topic models with openai embeddings, Technical report, University\nof Hawai‘i at M¯anoa. https://scholarspace.manoa.hawaii.edu/items/4fab27e3-5fad-412c-8f31-\n0c6f4de3c25d.\nWang, Yanshan, Liwei Wang, Majid Rastegar-Mojarad, et al. (2018), Clinical information extraction\napplications: A literature review, Journal of Biomedical Informatics.\nAppendix A. Volunteer Survey Questionnaire\nThis section contains the volunteer survey questionnaire, analyzed in Section 6.3, which we used\nin order to evaluate the outputs of the two chosen topic modeling techniques. The volunteers were\npresented with interview I0, along with the following survey to complete:\nContext: This survey is part of a research project investigating the use of automated\ntopic modeling techniques on cancer patient interview transcripts. The broader goal of\nthis thesis is to explore how topic modeling can help clinical staff quickly extract relevant\ninsights, such as emotional responses, symptoms, experiences, etc., from lengthy patient\nnarratives without having to read entire files. You will be presented with one anonymized\npatient interview and the resulting topic outputs from each model. Your feedback will\nhelp evaluate the clarity, relevance, and usefulness of the topics, contributing to an as-\nsessment of how well these models could support future clinical decision-making tools.\nOn the next page, you will find the extracted topics and keywords generated by each\nmodel. Please read the interview first before proceeding with the questions.\n1. Please rate each topic on a scale from 1 to 5, where:\n• 1 = Not coherent / Not useful\n• 5 = Very coherent / Very useful\n"}, {"page": 22, "text": "For each rating, please add a brief explanation if needed.\nExample: T1: 4 – Useful topic, but title could be more precise.\nT2: 1 - The topic makes no sense, this was never talked about during the interview.\n2. Overall, how well do the extracted topics represent the content of the interview\nyou read?\n(1 = Not at all, 5 = Very accurately)\n3. How helpful/accurate were the keywords under each topic for understanding what\nthe topic was about?\n(1 = Not helpful, 5 = Very helpful)\nExample: T1: 4 - Useful keywords, but it has one irrelevant word in it: ”the”)\n4. Were there any important themes, ideas, or aspects of the interview that were\nmissing from the extracted topics?\n• Yes\n• No\nIf yes, please specify.\n5. Do you have any additional feedback or suggestions about the topics or the overall\nexperience?\nEach participant received one survey document for each model, which had the same questions,\nwith the only difference being the topic output.\n"}]}