{"doc_id": "arxiv:2602.07529", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.07529.pdf", "meta": {"doc_id": "arxiv:2602.07529", "source": "arxiv", "arxiv_id": "2602.07529", "title": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution", "authors": ["Jianwen Chen", "Xinyu Yang", "Peng Xia", "Arian Azarang", "Yueh Z Lee", "Gang Li", "Hongtu Zhu", "Yun Li", "Beidi Chen", "Huaxiu Yao"], "published": "2026-02-07T12:54:01Z", "updated": "2026-02-10T03:03:52Z", "summary": "Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability. Code is available at https://github.com/aiming-lab/MedVerse.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.07529v2", "url_pdf": "https://arxiv.org/pdf/2602.07529.pdf", "meta_path": "data/raw/arxiv/meta/2602.07529.json", "sha256": "2a9e906022130ad8053a61e626e9d6bc9b0039b8654a2f1f3249a5e1b02d1a7e", "status": "ok", "fetched_at": "2026-02-18T02:19:31.111979+00:00"}, "pages": [{"page": 1, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured\nParallel Execution\nJianwen Chen1*,\nXinyu Yang2*,\nPeng Xia1\nArian Azarang1\nYueh Z Lee1\nGang Li1\nHongtu Zhu1\nYun Li1\nBeidi Chen2\nHuaxiu Yao1\n1UNC-Chapel Hill\n2Carnegie Mellon University\n*Equal contribution\nAbstract\nLarge language models (LLMs) have demon-\nstrated strong performance and rapid progress\nin a wide range of medical reasoning tasks.\nHowever, their sequential autoregressive de-\ncoding forces inherently parallel clinical rea-\nsoning, such as differential diagnosis, into a\nsingle linear reasoning path, limiting both ef-\nficiency and reliability for complex medical\nproblems. To address this, we propose Med-\nVerse, a reasoning framework for complex med-\nical inference that reformulates medical reason-\ning as a parallelizable directed acyclic graph\n(DAG) process based on Petri Net theory. The\nframework adopts a full-stack design across\ndata, model architecture, and system execution.\nFor data creation, we introduce the MedVerse\nCurator, an automated pipeline that synthesizes\nknowledge-grounded medical reasoning path\nand transforms them into Petri Net–structured\nrepresentations. At the architectural level, we\npropose a topology-aware attention mechanism\nwith adaptive position indices that supports par-\nallel reasoning while preserving logical con-\nsistency. Systematically, we develop a cus-\ntomized inference engine that supports parallel\nexecution without additional overhead. Empiri-\ncal evaluations show that MedVerse improves\nstrong general-purpose LLMs by up to 8.9%.\nCompared to specialized medical LLMs, Med-\nVerse achieves comparable performance while\ndelivering a 1.3× reduction in inference latency\nand a 1.7× increase in generation through-\nput, enabled by its parallel decoding capability.\nCode is available at https://github.com/aiming-\nlab/MedVerse.\n1\nIntroduction\nRecent advancements in Large Reasoning Models\n(LRMs) (OpenAI, 2024; Guo et al., 2025) have\nbroadened the capabilities of medical artificial in-\ntelligence (Moor et al., 2023a; Thirunavukarasu\net al., 2023; Nori et al., 2023; Xia et al., 2024a),\nenabling a transition beyond simple information\nretrieval toward complex clinical reasoning (Xia\net al., 2024b, 2025b; Alam et al., 2025; Zhang\net al., 2025). In particular, state-of-the-art med-\nical LRMs, such as MedReason (Wu et al., 2025),\nHuatuoGPT-o1 (Chen et al., 2024), and m1 (Huang\net al., 2025a), have shown that Chain-of-Thought\n(CoT) (Wei et al., 2022) reasoning can significantly\nenhance diagnostic accuracy. However, it remains\nunknown whether such models can be reliably and\nefficiently deployed in real-world clinical settings.\nIn practice, physicians increasingly employ these\nmodels to assist with clinical decision-making. Yet,\nthe sequential nature of autoregressive (AR) mod-\nels is misaligned with human cognitive processes,\nwhich naturally considers multiple differential di-\nagnoses simultaneously (Kassirer and Gorry, 1978).\nAccordingly, this mismatch leads to three funda-\nmental limitations: (i) Accuracy: linear reason-\ning narrows the diagnostic hypothesis space, limit-\ning the exploration of alternative diagnostic path-\nways; (ii) Efficiency: serialized decoding increases\nresponse latency, posing challenges for real-time\nclinical decision support; (iii) Interpretability: un-\nstructured CoT lacks explicit causal relationships,\nhindering clinical interpretation and validation. An\noverview of these challenges is shown in Figure 1.\nTo address these bottlenecks, prior work on\ngeneral-purpose LRMs has proposed parallel think-\ning (Luong et al., 2025; Wang et al., 2025; Ding\net al., 2025), enabling multiple reasoning branches\nto be explored concurrently before being synthe-\nsized into a final conclusion. However, most ex-\nisting parallel generation frameworks induce par-\nallelism primarily through brute-force repeat sam-\npling at the early stages of generation (Brown et al.,\n2024). Such approaches fail to capture the complex\nstructure in clinical inference that involves multiple\ncompeting hypotheses and conditional dependen-\ncies (Elstein et al., 1978), resulting in suboptimal\ntoken efficiency and limited structural interpretabil-\nity. Moreover, these inference-only solutions can-\n1\narXiv:2602.07529v2  [cs.LG]  10 Feb 2026\n"}, {"page": 2, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nnot internalize parallel thinking into the model, due\nto the absence of data-centric, end-to-end learning.\nWhile recent studies (Yang et al., 2025; Pan et al.,\n2025) enables recursive and consecutive parallel\nthinking patterns, they are limited to serial–parallel\ngraph structures, failing to adapt either structure or\nknowledge required for complex clinical reasoning.\nIn this work, we overcome these challenges with\nMedVerse, a novel modeling framework that en-\nables native directed acyclic graph (DAG)-based\nparallel generation tailored for medical reasoning.\nSpecifically, we formalize the entire differential\ndiagnosis process with Petri Net (Petri, 1962), a\nbipartite graph composed of places and transitions.\nIn this formulation, each place corresponds to a\nclinical entity (e.g., symptoms), while each transi-\ntion encodes a directed relation between entities.\nBuilt upon this structure, MedVerse is realized\nthrough a co-design of data, algorithm, and system:\n• Data Curation. We propose MedVerse Curator,\nan automated LLM-assisted pipeline that syn-\nthesizes knowledge-grounded medical reasoning\ntrajectories. Starting from an entity-level knowl-\nedge graph extracted for each query, our curator\nleverages the Petri Net representation to trans-\nform it into a transition-level DAG, in which\neach node refers to an atomic reasoning step be-\ntween entities. In practice, this process yields\nMedVerse-14K, a training corpus of 13,904 high-\nquality structured medical reasoning examples.\n• Algorithm Design. We propose MedVerse Atten-\ntion, a new attention mechanism for DAG-based\nexecution.This is achieved by modifying atten-\ntion masks and position embeddings to strictly\nensure the DAG-structured dependency. This de-\nsign also excels in data efficiency: since these\nchanges are minor, pre-trained AR models can be\nrapidly fine-tuned from causal attention to Med-\nVerse attention through a few thousand examples.\n• System Implementation. We propose MedVerse\nEngine, a high-performance serving engine built\nupon the Multiverse Engine (Yang et al., 2025).\nOur inference starts from a linear planning stage\nthat generates the DAG-structured plan. Next,\nour engine dynamically extract this plan to en-\nable true parallel decoding while respecting the\nunderlying topological dependencies. Our en-\ngine enables parallel generation with negligible\ncost via continuous batching and radix attention.\nQuestion: A 55-year-old male\nwith a history of chronic alcohol\nuse presents with fatigue and\ntingling in his hands and feet\n(neuropathy). Labs show\nmacrocytic anemia (MCV 110),\nelevated Homocysteine, and\nsigniﬁcantly elevated\nMethylmalonic Acid (MMA). What\nis the primary nutritional\ndeﬁciency?\nOptions: A) Folate (Folic Acid) B)\nVitamin B12 (Cobalamin)\nPath A: Investigate the patient's\nhistory of chronic alcohol use and\nits association with macrocytic\nanemia.\nPath B: Analyze the signiﬁcance\nof elevated Methylmalonic Acid\nand Homocysteine levels.\nPath C: Evaluate the cause of the\ntingling in hands and feet.\nShared Reasoning Path\nExecuting Path A: The patient has chronic\nalcohol use, a risk factor for nutritional\ndeﬁciency and macrocytic anemia … alcohol\nuse alone is not diagnostic.\nExecuting Path B: The labs show elevated\nhomocysteine … signiﬁcantly elevated MMA,\nwhich is speciﬁc for vitamin B12 deﬁciency.\nExecuting Path C: The patient has peripheral\nneuropathy … neurological involvement\nsupports vitamin B12 deﬁciency rather than\nfolate deﬁciency.\nConclusion: Macrocytic\nanemia with elevated\nMMA and neurological\nsymptoms indicates\nvitamin B12 deﬁciency,\nwith alcohol use as a\ncontributing\nbackground factor.\nAnswer: (B) Vitamin\nB12 (Cobalamin)\nExecuting Path A: Chronic alcohol use …\nstrongly associated with folate deﬁciency …\nexplains macrocytic anemia.\nExecuting Path B: Elevated homocysteine\nand MMA …  MMA may also be elevated in\nrenal insufﬁciency … given the strong risk\nfactor for folate deﬁciency identiﬁed in Path A,\nthe MMA ﬁnding may be less speciﬁc.\nExecuting Path C: Peripheral neuropathy …\ncan be explained by chronic alcohol use.\nConclusion: Chronic\nalcoholism provides a\nunifying explanation for\nmacrocytic anemia and\nneuropathy … most\nconsistent with folate\ndeﬁciency.\nAnswer: (A) Folate\nDeﬁciency\nLinear Execution Generation\nParallel Execution Generation\na\nb\nLinear Execution Generation\nReasoning Path: Path A: **Chronic alcohol use\n(cirrhosis)** \n Thiamine deﬁciency \n Wernicke-\nKorsakoff syndrome. Path B: **Confusion and\ndisorientation** \n Thiamine deﬁciency \n Wernicke-\nKorsakoff syndrome.\nReasoning Process:  Executing Path A: ... further\nsupports the likelihood of thiamine deﬁciency. Thiamine\ndeﬁciency leads to ... , resulting in the characteristic\ncognitive deﬁcits seen in Wernicke-Korsakoﬀ\nsyndrome. Executing Path B: ... Wernicke-Korsakoﬀ\nsyndrome, a condition caused by thiamine deﬁciency... \nc\nLinear Execution Generation\nReasoning Path: Path A： Psychiatric disorders →\nInsula changes → Inter-hemispheric\nconnectivity↓ Path B： Substance abuse → Insula\nchanges → Inter-hemispheric connectivity↓ \nReasoning Process:  ... Chronic substance abuse can\nhave profound effects on mental health... These\npsychiatric conditions are known to involve structural\nand functional alterations in the brain, ... insula ... As\ninsular structure and function become compromised, ...\nleading to decreased inter-hemispheric connectivity.\nFigure 1: Limitations of sequential chain-of-thought\nreasoning in medical diagnosis. (a) Accuracy: Lin-\near execution suffers from contextual pollution due to\nearly incorrect hypotheses (red), whereas parallel rea-\nsoning preserves correct inference paths (green). (b)\nEfficiency: Sequential reasoning repeatedly processes\noverlapping evidence, leading to redundant computation\n(orange). (c) Interpretability: Unstructured chain-of-\nthought (red) obscures explicit causal dependencies due\nto a structural mismatch with parallel clinical reasoning.\nEmpirical evaluations on clinical benchmarks\nshow that MedVerse improves accuracy by 4.8%\non Qwen2.5-7B and 8.9% on Llama-3.1-8B, match-\ning the performance of specialized reasoning mod-\nels such as MedReason and HuatuoGPT-o1. Be-\nyond accuracy, MedVerse overcomes the serial\nconstraints of autoregression, achieving a 1.3×\nspeedup in inference latency and a 69.3% increase\nin peak throughput.\n2\nRelated Work\nLLMs in Medical Reasoning. The application\nof LLMs in healthcare has evolved from general\ndomain adaptation to specialized clinical reasoning.\nEarly efforts, such as Med-PaLM (Singhal et al.,\n2023) and PMC-LLaMA (Wu et al., 2023), fo-\ncused on injecting medical knowledge via continual\npre-training on biomedical corpora. Subsequently,\ninstruction-tuned models like HuatuoGPT (Zhang\net al., 2023) and ChatDoctor (Li et al., 2023b) lever-\naged real-world dialogue data to align models with\nphysician behaviors (Li et al., 2023a; Moor et al.,\n"}, {"page": 3, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\n2023b; Nath et al., 2025). More recently, the focus\nhas shifted towards enhancing diagnostic logic, e.g.,\nmodels trained on MedReason are explicitly super-\nvised to generate CoT rationales (Wu et al., 2025;\nXia et al., 2025a; Zhu et al., 2025; Lai et al., 2025;\nHuang et al., 2025b). Despite these advances, ex-\nisting models remain fundamentally constrained by\nthe autoregressive decoding mechanism (Vaswani\net al., 2017). They generate rationales as a rigid,\nserial token sequence, yielding an inference com-\nplexity of O(N). This computational linearity not\nonly imposes high latency in real-time scenarios\nbut also restricts the model to a single narrative\nthread, making it inefficient to maintain context\nacross long, complex reasoning trajectories.\nParallel Generation and Efficient Inference To\nbreak the serial decoding barrier, architectural\napproaches like Non-Autoregressive (NAR) de-\ncoding (Gu et al., 2017) and Speculative Decod-\ning (Leviathan et al., 2023) attempt simultane-\nous token generation, though often facing coher-\nence or verification bottlenecks. Notably, Multi-\nverse introduces a MapReduce paradigm, decom-\nposing generation into parallel “Map” branches\nmerged via “Reduce” steps, supported by a spe-\ncialized engine leveraging Radix Attention (Zheng\net al., 2024) for efficiency. However, these general-\npurpose paradigms lack medical grounding, and\ntheir topologies are constrained to simple fork-join\npatterns. This structural simplicity fails to capture\nthe complex, non-linear branching networks of dif-\nferential diagnosis (Bowen, 2006). MedVerse ad-\ndresses this limitation by co-designing a medically-\ngrounded architecture with a custom vLLM-based\nengine (Kwon et al., 2023), utilizing Radix Atten-\ntion to enable zero-copy forking specifically tai-\nlored for complex medical reasoning topologies.\n3\nMedVerse Modeling\nThis section introduces MedVerse, a novel model-\ning framework that reimagines the entire clinical\nreasoning process as DAG-based execution rather\nthan sequential generation via a Petri Net abstrac-\ntion, thereby improving reliability and efficiency.\n3.1\nDAG Structure for Medical Reasoning\nAs illustrated in Figure 2, multiple diagnostic\nhypotheses may share intermediate evidence or\nconverge on common pathological mechanisms,\nleading to complex DAG-structured dependencies\nacross different entities for medical reasoning.\nSuch structures extend beyond the capabilities of\nsequential CoT reasoning or the tree-based exten-\nsions (e.g., Tree-of-Thoughts (Yao et al., 2023)).\nMotivated by this observation, we formalize medi-\ncal reasoning using a DAG G = (V, E) as follows:\n• Nodes as Reasoning States (V ): Each node rep-\nresents an intermediate reasoning state. Specif-\nically, we distinguish three types of nodes: (i)\nsource nodes, corresponding to clinical entities\ngrounded in the input question, which only have\noutgoing edges; (ii) hypothesis nodes, repre-\nsenting diagnostic hypotheses or pathological\nstates that may both split into multiple down-\nstream paths and merge evidence from multiple\nupstream paths; and (3) conclusion nodes, re-\nferring to final diagnostic outcomes, which only\nadmit incoming edges and serve as unique con-\nvergence points of the entire reasoning process.\n• Edges as Reasoning Steps (E): Each (directed)\nedges encode conditional dependencies between\nreasoning states. An edge (u, v) indicates an\nadmissible reasoning step from state u to state\nv. To ensure structural acyclicity, all edges are\nrestricted to forward dependencies that follow the\ntemporal and causal order of clinical reasoning.\nFigure 2 showcases that our formulation natu-\nrally captures DAG structures in clinical reasoning.\n3.2\nExtension to Petri Nets\nWhile the DAG structure provides a topological\nblueprint, its static representation of solitary states\nfails to capture the generative transition logic essen-\ntial for contextualizing history-dependent LLM in-\nference. To realize this structure into an executable\nparallel process, we formalize our framework using\nPetri Nets. This mathematical grounding bridges\nthe gap between the logical graph G and the physi-\ncal execution of LLM inference, enabling reason-\ning states to be instantiated as places and causal\ndependencies as transitions for parallel execution\n(Figure 2).\nFormal Definition.\nWe define the execution\nmodel as a tuple N = (P, T, F, M0), explicitly\nmapping to the DAG components:\n• P = {p1, . . . , pm} is a finite set of places, corre-\nsponding to the nodes (V ) in the DAG. Function-\nally, a place serves as a state container, holding\nthe context or belief state waiting to be processed.\n• T = {t1, . . . , tn} is a finite set of transitions,\nrepresenting reasoning steps. Edges are mapped\n"}, {"page": 4, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nClinical Topological DAG\nFamily history\nof cancer\nLynch\nsyndrome\nMSH2 defect\nMLH1 defect\nColon mass\nMSH6 defect\nMismatch\nrepair\nExecutable Petri Net (MedVerse Framework)\nQuestion: A 47-year-old man presents to his primary care\nphysician for fatigue... His past medical history is notable for\nobesity, type II diabetes mellitus, and hypertension. He takes\nmetformin and enalapril. His family history is notable for\ncolorectal cancer... and endometrial cancer... A colonoscopy\nreveals a small hemorrhagic mass ... Which of the following\nprocesses is likely impaired in this patient?\nAnswer: Mismatch repair\nClinical Question & Answer Pair\nP1: Family \nhistory of cancer\nP2: Colon Mass\nP3: Lynch\nsyndrome\nP4: MLH1 defect\nP5: MSH2 defect\nP6: MSH6 defect\nP7: Mismatch \nrepair\nTransition1: \nP1, P2 -> P3\nTransition2: \nP1, P3 -> P4\nTransition3: \nP3 -> P5\nTransition4: \nP3 -> P6\nTransition5: \nP4, P5, P6 -> P7\n∅\n∅\n∅\nLegend\nPlace\nTransition\nRunning Transition\nFlow Run\n∅\nFigure 2: Illustration of the topological modeling process. The framework first extracts a structured clinical\ntopological DAG from an unstructured question-answer pair, capturing causal dependencies. This DAG is then\nformally mapped into an Executable Petri Net, where reasoning states are instantiated as places and their dependency\nrelations are realized through transitions.\nvia many-to-one aggregation: converging edges\n(e.g., A, B →C) form a single transition, while\ndivergent edges (A →B, A →C) instantiate\ndistinct transitions.\n• F ⊆(P ×T)∪(T ×P) defines the flow arcs that\nadhere to the DAG’s topological direction. We\ndenote the set of input places for a transition t as\nthe pre-set •t = {p | (p, t) ∈F} and the output\nplaces as the post-set t• = {q | (t, q) ∈F}.\n• M0 is the initial marking such that for any\nplace p ∈P corresponding to a DAG node\nwith in-degree zero, M0(p) is non-empty, while\nM0(p) = ∅for all other places.\nMedVerse Token Semantics.\nStandard Petri nets\ntreat tokens as simple counters. To support the rich\ncontext of medical reasoning, we instantiate N as\na Colored Petri Net (CPN). We define a token not\nas a scalar, but as a semantic tuple τ = (h, k):\n• h: Encapsulates the textual history generated\nalong the current path.\n• k: Denotes the KV-cache indices associated with\nthat history.\nThis definition transforms tokens into computa-\ntional state carriers, enabling the system to pass\nmemory references rather than copying full text,\nwhich is pivotal for efficiency.\nExecution as Inference.\nInference is modeled\nas token flow through the Petri Net. A transition t\nis enabled when tokens are present in all its input\nplaces and its output places are empty, ensuring\neach reasoning step executes exactly once. When\nfired, t invokes the LLM to generate the correspond-\ning reasoning output, reading from the input tokens\nand producing new tokens at its output places. Each\nresulting token inherits and extends both the textual\nhistory h and KV-cache references k: the engine\nappends the newly generated text to h and maps\nthe corresponding memory blocks to k. Multiple\nenabled transitions may fire concurrently, yielding\nparallel decoding streams.\n3.3\nExecution Semantics\nGiven the executable Petri Net constructed in Sec-\ntion 3.2, we specifies the execution semantics of\nMedVerse. We decouple scheduling from execu-\ntion: scheduling identifies the set of causally en-\nabled transitions ready for execution, while execu-\ntion operates on this set via two abstract primitives,\nFork and Join, to realize parallel generation with\nexplicit causal synchronization.\nScheduling via Enabled-Transition Frontier. At\nruntime, execution is governed by the current Petri\nNet marking Mk, which encodes token availability\nat each place. A transition is schedulable if tokens\nare present in all its input places and all output\nplaces are empty. The enabled-transition frontier\n"}, {"page": 5, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nat step k is defined as\nFk =\nn\nt ∈T\n\f\f\f\n\u0000∀p ∈•t, Mk(p) ̸= ∅\n\u0001\n∧\n\u0000∀q ∈t•, Mk(q) = ∅\n\u0001o\n,\n(1)\nwhich represents the maximal set of transitions\nthat can be executed concurrently without violating\ncausal dependencies.\nFork and Join Primitives. Given the frontier Fk,\nthe engine executes transitions based on their topo-\nlogical dependencies. Fork is applied to transitions\nsharing a common predecessor, initiating parallel\ndecoding streams that inherit the exact same con-\ntext. Conversely, Join is applied to transitions ag-\ngregating multiple reasoning branches, performing\nlogical synchronization by merging distinct prede-\ncessor histories before generation proceeds.\nExecution Loop. After all transitions in Fk com-\nplete, the marking advances to Mk+1, the frontier\nis recomputed, and the scheduling–execution cycle\nrepeats until no further transitions are enabled.\n3.4\nStructured Generation Flow\nBuilding on the structured representations and ex-\necution semantics introduced in Sections 3.1–3.3,\nMedVerse organizes LLM generation into a struc-\ntured three-stage flow: planning, execution, and\nconclusion (see Figure 3). The process first con-\nstructs an explicit reasoning plan, executes it under\ngraph-based constraints, and finally aggregates the\nresults into a unified conclusion.\nPlanning Stage. Generation begins with a plan-\nning stage that reconciles sequential generation\nwith non-linear reasoning via a Think–then–Map\nstrategy. The model first generates multiple lin-\near reasoning paths connecting clinical evidence\nto candidate diagnoses. These paths collectively\nencode overlapping dependencies that implicitly\nform a logical DAG. The model then consolidates\nthem into a structured <Plan> block, where rea-\nsoning steps are organized using <Outline> tags\nannotated with dependency information. These\nannotations instantiate the flow relation F of the\nPetri Net, specifying the reasoning topology while\ndeferring detailed inference to execution.\nExecution Stage. Given the specified plan, gen-\neration proceeds within the <Execution> block\nfollowing the execution paradigm in Section 3.3.\nAt each iteration, the enabled-transition frontier is\nidentified from the current marking. The engine\nthen concurrently executes the transitions within\n... ﬁrst ﬁnd a reasoning path, then transform that path into an outlined plan... \nFinding Reasoning Path: \n1. Thyrotoxicosis -> High Dose Potassium iodide -> Reduced thyroid vascularity.\n2. Thyrotoxicosis -> High Dose Therapeutic Iodine -> Reduced thyroid vascularity.\n...\n<Plan> <Outline> Transient Step 1: Thyrotoxicosis -> High Dose Potassium iodide;\nDependency: [] </Outline> <Outline> Transient Step 2: Thyrotoxicosis -> High\nDose Therapeutic Iodine; Dependency: [] </Outline> ... <Outline> Transient Step\n4: High Dose Potassium iodide, High Dose Therapeutic Iodine -> Reduced thyroid\nvascularity; Dependency: [1, 2] </Outline> ...\nPlanning Stage\n<Execution> <Step> Transient Step 1: Thyrotoxicosis -> High Dose Potassium iodide\nHigh-dose potassium iodide is administered as a therapeutic intervention to acutely\ninhibit thyroid hormone release and reduce thyroid blood ﬂow. </Step>\n<Step> Transient Step 2: Thyrotoxicosis -> High Dose Therapeutic Iodine\nHigh-dose therapeutic iodine is administered to transiently suppress thyroid hormone\nrelease and decrease thyroid blood ﬂow. </Step> ...\n<Step> Transient Step 4: High Dose Potassium iodide, High Dose Therapeutic Iodine \n -> Reduced thyroid vascularity\nHigh-dose potassium iodide reduces thyroid vascularity through acute suppression of\nhormone release, while high-dose therapeutic iodine reduces thyroid vascularity by\ntransiently inhibiting thyroid hormone release.</Step> ...\nExecution Stage\n<Conclusion> Explanation: Transient Steps 1 and 2 show that high-dose potassium\niodide or therapeutic iodine can be used in thyrotoxicosis... As summarized in Transient\nStep 4, either agent independently reduces thyroid vascularity by acutely suppressing\nthyroid hormone release... Among the options, potassium iodide is the appropriate\nchoice.\nAnswer: a) Potassium iodide\n</Conclusion>\nConclusion Stage\nFigure 3: Example of the structured generation flow in\nMedVerse. The reasoning process is explicitly decom-\nposed into planning, execution, and conclusion stages.\nthis frontier by applying Fork and Join, encapsu-\nlating the generated reasoning for each transition\nwithin <Step> tags.\nConclusion Stage. After all executable reasoning\nsteps have completed, the process transitions to a\nfinal synthesis phase marked by the <Conclusion>\nblock. In this phase, the outcomes of all completed\nreasoning branches are aggregated to produce a\nunified diagnostic conclusion.\n4\nMedVerse Instantiation\nTo instantiate our MedVerse model discussed in\nSection 3, we present a comprehensive suite includ-\ning three core components: the MedVerse Cura-\ntor for structured data generation, the MedVerse\nAttention for DAG-structured modeling, and the\nMedVerse Engine to enable parallel execution.\n4.1\nData Curation: MedVerse Curator\nWe introduce the MedVerse Curator, an LLM-\nassisted pipeline that extracts knowledge-grounded\nreasoning paths from a medical knowledge graph\nand compiles them into structured training in-\nstances executable under the MedVerse framework.\nSpecifically, it includes four steps.\nI. Knowledge-Grounded Retrieval. The curator\ngrounds medical reasoning in established clinical\nknowledge by first mapping questions and answer\n"}, {"page": 6, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\ncandidates to standardized medical concepts. It\nthen retrieves plausible reasoning paths connecting\nthese concepts through a medical knowledge graph,\nand prunes irrelevant or low-confidence branches.\nII. Topological Planning. The resulting linear rea-\nsoning skeletons are refined into executable plans\nusing a specialized prompt that removes redun-\ndancy and enforces logical correctness and coher-\nence. A DAG validity check ensures that all depen-\ndencies are acyclic; paths with invalid structures\nare discarded or re-routed.\nIII. Structural Synthesis. Given a topology, the\ncurator generates structured data in the MedVerse\nformat. An LLM produces step-by-step reasoning\nfor each execution transition, followed by an auto-\nmated refinement module that smooths transitions\nacross branches and ensures reasoning correctness.\nFinally, a coherent conclusion is synthesized from\nthe refined execution trajectory.\nIV. Dual-Layer Verification. Finally, syntax-level\nvalidation and model-based logical evaluation are\napplied. Samples failing either check are iteratively\nregenerated until all constraints are satisfied.\nMedVerse-14K Dataset.\nIn practice, we ap-\nply our automated pipeline to the training sub-\nset of multiple medical training datasets, includ-\ning MedQA (Jin et al., 2021), MedMCQA (Pal\net al., 2022), PubMedQA (Jin et al., 2019), MMLU-\nMedical (Hendrycks et al., 2021a,b), HuatuoGPT-\no1, MedXpert (Zuo et al., 2025), and Humanity’s\nLast Exam (HLE) (Phan et al., 2025). This pro-\ncess yields MedVerse-14K that comprises 13,904\nhigh-quality and structured reasoning trajectories,\ncovering complex long-context medical problems.\n4.2\nAlgorithm Design: MedVerse Attention\nNext, we introduce MedVerse Attention to replace\nstandard causal attention (Vaswani et al., 2017).\nThe causal attention computes the i-th token’s out-\nput with query qi, and keys kj, values vj (j ≤i):\naij = Softmax\n\u0010\n(qi · P(i))⊤(kj · P(j)) + Mij\n\u0011\n.\n(2)\nwhere Mij =\n(\n0,\nj ≤i\n−∞,\notherwise is casual atten-\ntion mask and P(i) is embedding for position i.\nDAG-based Attention Mask. Transitions within\nthe same enabled-transition frontier may execute\nconcurrently and must remain causally indepen-\ndent. To enforce this property, we construct a layer-\nwise mutual exclusion mask M. During training,\nthe input sequence is segmented into frontier layers\naccording to the execution plan, where each layer\ncontains a set of parallel reasoning steps S1, S2, . . ..\nFor two tokens i and j associated with steps Su and\nSv, the attention bias is defined as:\nMij =\n\n\n\n\n\n−∞\nif j > i\n−∞\nif Layer(i) = Layer(j) ∧Su ̸= Sv\n0\notherwise\n(3)\nThe first condition preserves standard autoregres-\nsive causality, while the second prevents informa-\ntion leakage between parallel transitions.\nAdaptive Position Indices. While masking en-\nforces causal isolation, standard monotonic posi-\ntion indices cannot represent Petri Net execution\nsynchronization. We therefore assign position in-\ndices based on the logical execution timeline rather\nthan linear token order. Tokens generated by tran-\nsitions within the same enabled-transition frontier\nare assigned an identical starting index (fork align-\nment). For transitions that join multiple branches in\na subsequent frontier, the position index is set to the\nmaximum index among all predecessor branches,\nallowing the model to attend to the complete causal\nhistory.\nTogether, topology-aware masking and adaptive\nposition indices implement the execution semantics\nof Section 3.3 within standard autoregressive Trans-\nformers, without modifying the training pipeline.\n4.3\nInference Engine: MedVerse Engine\nTo translate the execution semantics of the Petri\nNet into practical inference speedups, we develop\nthe MedVerse Engine, a high-performance serving\nsystem built upon the Multiverse Engine. Unlike\nstandard engines that treat generation as a uniform\nstream, our engine implements a Hybrid Execu-\ntion Pipeline that seamlessly transitions from linear\nLLM-driven planning to system-guided parallel ex-\necution. This engine includes two phases, which\nare detailed as follows:\nPhase I: Linear Planning & Graph Initialization.\nThe engine first employs standard autoregressive\ndecoding to perform linear planning, during which\nthe LLM translates the input into multiple linear\nreasoning paths followed by a topological <Plan>\nblock. This phase is managed strictly as a con-\nventional linear generation process to ensure log-\nical correctness and coherence. Upon detecting\nthe </Plan> tag, the engine pauses generation and\n"}, {"page": 7, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nTable 1: Performance comparison on medical reasoning benchmarks. We report accuracy (%) across different\ndatasets. Bold indicates the best performance.\nQwen2.5-7B-Instruct\nLLaMA-3.1-8B-Instruct\nBenchmark\nOriginal MedReason MedVerse Original MedReason Huatuo-o1 MedVerse\nHLE (Medical)\n18.4\n20.8\n19.6\n13.6\n20.2\n14.6\n20.6\nMedBullets (op4)\n45.8\n49.7\n55.2\n48.7\n57.1\n55.8\n62.3\nMedBullets (op5)\n39.6\n44.2\n48.0\n42.5\n51.0\n53.9\n53.6\nMedQA\n56.2\n56.2\n58.6\n58.7\n63.9\n72.4\n66.4\nMedXpert\n12.3\n14.5\n15.3\n13.2\n18.4\n16.8\n19.3\nAverage\n34.5\n37.1\n39.3\n35.3\n42.2\n42.7\n44.2\nparses the dependency annotations specified in the\n<Outline> tags to instantiate the in-memory Petri\nNet structure N and initialize the token marking\nM0, thereby triggering an immediate transition to\ngraph-based execution mode.\nPhase II: Frontier-Based Graph Execution.\nGuided by the parsed topology, the scheduler iden-\ntifies the enabled-transition frontier Fk (Sec. 3.3) at\neach step and executes all transitions in the frontier\nconcurrently using two memory-optimized primi-\ntives. For transitions that branch from a common\npredecessor, the engine applies Fork execution,\nspawning multiple parallel decoding streams that\nshare the same prefix KV cache via Radix Atten-\ntion, thereby enabling zero-copy prefix reuse until\neach corresponding <Step> block completes. Con-\nversely, for transitions with multiple predecessors,\nexecution is deferred until all upstream reasoning\npaths finish, after which the engine applies Join\nexecution by merging the KV states of all prede-\ncessor paths together with the preceding context\nto construct a unified KV cache. Leveraging the\nflexible radix cache layout, this merge is performed\nwithout padding or physical memory copying, and\ngeneration then proceeds from the merged KV state\nalong the newly constructed sequence.\n5\nExperiments\n5.1\nSetup\nTraining. We fine-tune MedVerse variants from\nthe Qwen2.5-7B-Instruct (Team et al., 2024) and\nLlama-3.1-8B-Instruct (Dubey et al., 2024) check-\npoints, incorporating our proposed MedVerse at-\ntention mechanism. Training is conducted on the\ncurated MedVerse-14K dataset. All base models\nare trained for 3 epochs with a learning rate of 10−5\nand a batch size of 128. Fine-tuning is conducted\nusing 4 NVIDIA H200 GPUs with PyTorch FSDP.\nEvaluation. We evaluate all MedVerse variants\nacross five standard medical reasoning bench-\nmarks, including MedQA, MedXpert, MedBul-\nlets (Chen et al., 2025), and Humanity’s Last Exam\n(HLE) (Phan et al., 2025). All experiments are con-\nducted using our SGLang-based MedVerse Engine.\nBaselines. We compare MedVerse against the orig-\ninal base models and other medical LLMs, includ-\ning MedReason-8B and HuatuoGPT-o1-RL-8B. In\naddition to accuracy, we report both latency and\nthroughput to assess efficiency gains of MedVerse.\nFor fair comparisons, MedReason-8B is fine-tuned\nusing the same number of examples as MedVerse-\n14K, while following its official training recipe.\n5.2\nPerformance Evaluation\nTable 1 reports the performance of MedVerse on\nmedical reasoning benchmarks. Across both back-\nbones, MedVerse consistently outperforms the base\nmodels and strong autoregressive medical LLM\nbaselines. For Qwen2.5-7B-Instruct, MedVerse im-\nproves the average accuracy from 34.5% to 39.3%,\nexceeding MedReason (37.1%). For LLaMA-3.1-\n8B-Instruct, MedVerse achieves the highest aver-\nage accuracy of 44.2%, surpassing both MedRea-\nson (42.2%) and HuatuoGPT-o1-RL-8B (42.7%).\n5.3\nEfficiency Analysis\nTo validate the efficiency of our system, we con-\nducted a rigorous efficiency evaluation comparing\nMedVerse against standard autoregressive base-\nlines on NVIDIA H200 GPUs. Our analysis fo-\ncuses on two key dimensions: the latency for\ngenerating full reasoning chains and the engine’s\nthroughput under controlled output lengths.\nEnd-to-End CoT Latency. First, we measured the\ntotal wall-clock time required to answer medical\nqueries across five diverse datasets (e.g., MedX-\n"}, {"page": 8, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nMedBullets(op4)\nMedQA\nMedXpertQA\nMedBullets(op5)\nHLE(Biomed)\n0\n2\n4\n6\n8\n10\n12\n14\nAvg. Latency (s)\n(a) End-to-End Latency & Speedup\nSpeedup Rate (Right axis)\nAutoregressive Baseline (Left axis)\nMultiverse-Med (Left axis)\n1.0\n1.1\n1.2\n1.3\n1.4\nSpeedup Rate (x Baseline)\n1.33x\n1.29x\n1.30x\n1.25x\n1.30x\n128\n256\n512\n1024\n2048\nToken Limit (Sequence Length)\n8\n10\n12\n14\n16\n18\n20\nThroughput (Tokens/Sec)\n(b) Iso-Length Throughput Scaling\nAutoregressive Baseline\nMultiverse-Med\nFigure 4: Efficiency Metrics.\n(a) Average latency\nand relative speedup (orange line) across five datasets.\nMedVerse consistently outperforms the baseline. (b)\nThroughput vs. Sequence Length. Our method exhibits\nbetter scaling properties, maintaining higher throughput\nas token complexity increases.\npertQA, MedQA) with a batch size of 64. As\nshown in Figure 4(a), MedVerse consistently out-\nperforms the sequential AR baseline, achieving a\nstable speedup ranging from 1.25× to 1.33× (in-\ndicated by the orange trend line). Theoretically, in\nstandard AR models, generating a comprehensive\ndiagnosis with multiple branches requires linear\ntime O(N), where N is the total token count. In\ncontrast, MedVerse leverages its topological struc-\nture to generate independent reasoning paths si-\nmultaneously. This shifts the latency complexity\nfrom total length to the topological depth of the\nreasoning graph O(D), drastically reducing user\nwait time for complex, multi-step queries.\nIso-Length Throughput Comparison. To iso-\nlate the system’s raw generation capability, we\nconducted an “Iso-Length” stress test on the HLE\ndataset with a batch size of 1, where both models\nwere constrained to generate identical sequence\nlengths ranging from 128 to 2048 tokens. Fig-\nure 4(b) illustrates the throughput divergence.\nWhile the AR baseline maintains a relatively static\nthroughput (∼10 tokens/sec) limited by mem-\nModel Variant\nLinear\nParallel\nAccuracy (%)\nLatency (s)\nAutoregressive\n✓\n×\n18.4\n5.1\nDirect Petri Net\n×\n✓\n17.4\n4.5\nMedVerse\n✓\n✓\n19.3\n4.0\nTable 2: Efficacy of Linear-to-Parallel Hybridization.\nAblation study on the MedXpert benchmark.\nory bandwidth and serial dependency, MedVerse\ndemonstrates superior parallel scalability.\nThe\nperformance gap widens significantly as the se-\nquence length increases: at 2048 tokens, our sys-\ntem achieves a peak throughput of ∼17.1 token-\ns/sec, representing a +69.3% gain over the baseline.\nThis confirms that MedVerse effectively converts\nthe GPU’s parallel compute capacity into valid to-\nken throughput, making it increasingly efficient for\nlong-context medical reasoning tasks.\n5.4\nAblation Study\nWe conduct a targeted ablation to assess the neces-\nsity of linear-to-parallel hybrid reasoning by com-\nparing MedVerse with a Direct Petri Net variant\nthat directly generates topological structures with-\nout linear planning. As shown in Table 2, when\nevaluated on MedXpert (batch size 1), the Direct\nPetri Net variant exhibits a substantial accuracy\ndrop and underperforms even the autoregressive\nbaseline, indicating that standard LLMs struggle\nto construct sound execution graphs from scratch.\nIn contrast, MedVerse achieves both the highest\naccuracy (19.3%) and the lowest latency (4.0s),\ndemonstrating that linear planning is essential for\nreliable graph construction and that its combina-\ntion with parallel execution yields the best accu-\nracy–efficiency trade-off.\n6\nConclusion\nThis work addresses the fundamental mismatch\nbetween sequential autoregressive decoding and\nthe inherently parallel nature of clinical reason-\ning. By reformulating medical inference as a DAG-\nstructured process grounded in Petri Net theory,\nMedVerse enables large language models to reason\nover multiple diagnostic hypotheses concurrently\nwhile preserving causal consistency. This paradigm\nalleviates key limitations of linear chain-of-thought\nreasoning in accuracy, efficiency, and interpretabil-\nity, and offers a principled path toward structurally\naligned medical reasoning systems suitable for real-\nworld clinical decision support.\n"}, {"page": 9, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nAcknowledgement\nThis work is partially supported by NSF #2449442\nand\nNIH\nR01AG085581,\nR01AG079291,\nR01AR083790 and P50HD103573. The Authors\nacknowledge the National Artificial Intelligence\nResearch Resource (NAIRR) Pilot, NCSA DeltaAI\nand OpenAI API for contributing to this research\nresult.\nReferences\nHasan Md Tusfiqur Alam, Devansh Srivastav, Md Ab-\ndul Kadir, and Daniel Sonntag. 2025. Towards in-\nterpretable radiology report generation via concept\nbottlenecks using a multi-agentic rag. In European\nConference on Information Retrieval, pages 201–209.\nSpringer.\nJudith L Bowen. 2006. Educational strategies to pro-\nmote clinical diagnostic reasoning. New England\nJournal of Medicine, 355(21):2217–2225.\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald\nClark, Quoc V Le, Christopher Ré, and Azalia Mirho-\nseini. 2024. Large language monkeys: Scaling infer-\nence compute with repeated sampling. arXiv preprint\narXiv:2407.21787.\nHanjie Chen, Zhouxiang Fang, Yash Singla, and Mark\nDredze. 2025. Benchmarking large language mod-\nels on answering and explaining challenging medical\nquestions. In Proceedings of the 2025 Conference\nof the Nations of the Americas Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers), pages\n3563–3599.\nJunying Chen, Zhenyang Cai, Ke Ji, Xidong Wang,\nWanlong Liu, Rongsheng Wang, Jianye Hou, and\nBenyou Wang. 2024. Huatuogpt-o1, towards med-\nical complex reasoning with llms. arXiv preprint\narXiv:2412.18925.\nYifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing,\nJinyang Guo, Yingjie Wang, Jing Zhang, Zengmao\nWang, Ziwei Liu, Bo Du, and 1 others. 2025. Dy-\nnamic parallel tree search for efficient llm reasoning.\narXiv preprint arXiv:2502.16235.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and 1 others. 2024. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783.\nArthur S Elstein, Lee S Shulman, and Sarah A Sprafka.\n1978. Medical problem solving: An analysis of clini-\ncal reasoning. Harvard University Press.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor OK Li, and Richard Socher. 2017.\nNon-\nautoregressive neural machine translation.\narXiv\npreprint arXiv:1711.02281.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao\nSong, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning.\narXiv preprint\narXiv:2501.12948.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021a. Aligning ai with shared human values. Pro-\nceedings of the International Conference on Learning\nRepresentations (ICLR).\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021b. Measuring massive multitask language\nunderstanding. Proceedings of the International Con-\nference on Learning Representations (ICLR).\nXiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang,\nand Yuyin Zhou. 2025a. m1: Unleash the potential\nof test-time scaling for medical reasoning with large\nlanguage models. arXiv preprint arXiv:2504.00869.\nXiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang,\nand Yuyin Zhou. 2025b.\nMedvlthinker: Simple\nbaselines for multimodal medical reasoning. arXiv\npreprint arXiv:2508.02669.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Co-\nhen, and Xinghua Lu. 2019. Pubmedqa: A dataset for\nbiomedical research question answering. In Proceed-\nings of the 2019 conference on empirical methods\nin natural language processing and the 9th interna-\ntional joint conference on natural language process-\ning (EMNLP-IJCNLP), pages 2567–2577.\nJerome P Kassirer and G Anthony Gorry. 1978. Clinical\nproblem solving: a behavioral analysis. Annals of\nInternal Medicine, 89(2):245–255.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serv-\ning with pagedattention. In Proceedings of the 29th\nsymposium on operating systems principles, pages\n611–626.\nYuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao,\nYuheng Li, Konstantinos Psounis, and Xiaofeng Yang.\n2025. Med-r1: Reinforcement learning for general-\nizable medical reasoning in vision-language models.\narXiv preprint arXiv:2503.13939.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2023. Fast inference from transformers via spec-\nulative decoding. In International Conference on\nMachine Learning, pages 19274–19286. PMLR.\n"}, {"page": 10, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto\nUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-\nmann, Hoifung Poon, and Jianfeng Gao. 2023a.\nLlava-med: Training a large language-and-vision\nassistant for biomedicine in one day. Advances in\nNeural Information Processing Systems, 36:28541–\n28564.\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve\nJiang, and You Zhang. 2023b. Chatdoctor: A medical\nchat model fine-tuned on a large language model\nmeta-ai (llama) using medical domain knowledge.\nCureus, 15(6).\nThang Luong, Edward Lockhart, and 1 others. 2025.\nAdvanced version of gemini with deep think offi-\ncially achieves gold-medal standard at the interna-\ntional mathematical olympiad. Google DeepMind\nBlog, 1.\nMichael Moor, Oishi Banerjee, Zahra Shakeri Hossein\nAbad, Harlan M Krumholz, Jure Leskovec, Eric J\nTopol, and Pranav Rajpurkar. 2023a. Foundation\nmodels for generalist medical artificial intelligence.\nNature, 616(7956):259–265.\nMichael Moor, Qian Huang, Shirley Wu, Michihiro\nYasunaga, Yash Dalmia, Jure Leskovec, Cyril Za-\nkka, Eduardo Pontes Reis, and Pranav Rajpurkar.\n2023b. Med-flamingo: a multimodal medical few-\nshot learner.\nIn Machine Learning for Health\n(ML4H), pages 353–367. PMLR.\nVishwesh Nath, Wenqi Li, Dong Yang, Andriy Myro-\nnenko, Mingxin Zheng, Yao Lu, Zhijian Liu, Hongxu\nYin, Yee Man Law, Yucheng Tang, and 1 others. 2025.\nVila-m3: Enhancing vision-language models with\nmedical expert knowledge. In Proceedings of the\nComputer Vision and Pattern Recognition Confer-\nence, pages 14788–14798.\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023. Capabili-\nties of gpt-4 on medical challenge problems. arXiv\npreprint arXiv:2303.13375.\nOpenAI. 2024. Learning to reason with LLMs. Ac-\ncessed: 2025-01-04.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on health,\ninference, and learning, pages 248–260. PMLR.\nJiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei\nZhou, Adam Yala, Trevor Darrell, Kurt Keutzer,\nand Alane Suhr. 2025. Learning adaptive parallel\nreasoning with language models.\narXiv preprint\narXiv:2504.15466.\nCarl Adam Petri. 1962. Kommunikation mit automaten.\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li,\nJosephina Hu, Hugh Zhang, Chen Bo Calvin Zhang,\nMohamed Shaaban, John Ling, Sean Shi, and 1 oth-\ners. 2025. Humanity’s last exam. arXiv preprint\narXiv:2501.14249.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\nand 1 others. 2023. Large language models encode\nclinical knowledge. Nature, 620(7972):172–180.\nQwen Team and 1 others. 2024. Qwen2 technical report.\narXiv preprint arXiv:2407.10671, 2(3).\nArun James Thirunavukarasu, Darren Shu Jeng Ting,\nKabilan Elangovan, Laura Gutierrez, Ting Fang Tan,\nand Daniel Shu Wei Ting. 2023. Large language\nmodels in medicine. Nature medicine, 29(8):1930–\n1940.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nZiqi Wang, Boye Niu, Zipeng Gao, Zhi Zheng, Tong Xu,\nLinghui Meng, Zhongli Li, Jing Liu, Yilong Chen,\nChen Zhu, and 1 others. 2025. A survey on parallel\nreasoning. arXiv preprint arXiv:2510.12164.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\nand 1 others. 2022. Chain-of-thought prompting elic-\nits reasoning in large language models. Advances\nin neural information processing systems, 35:24824–\n24837.\nChaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang,\nand Weidi Xie. 2023.\nPmc-llama: Further fine-\ntuning llama on medical papers.\narXiv preprint\narXiv:2304.14454, 2(5):6.\nJuncheng Wu, Wenlong Deng, Xingxuan Li, Sheng\nLiu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu,\nHyunjin Cho, Chang-In Choi, and 1 others. 2025.\nMedreason:\nEliciting factual medical reasoning\nsteps in llms via knowledge graphs. arXiv preprint\narXiv:2504.00993.\nPeng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo\nHou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang\nZhou, Kangyu Zhu, and 1 others. 2024a. Cares: A\ncomprehensive benchmark of trustworthiness in med-\nical vision language models. Advances in Neural\nInformation Processing Systems, 37:140334–140365.\nPeng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng,\nXian Wu, Xiangru Tang, Hongtu Zhu, Yun Li,\nShujie Liu, Yan Lu, and Huaxiu Yao. 2025a.\nMmedagent-rl: Optimizing multi-agent collaboration\nfor multimodal medical reasoning. arXiv preprint\narXiv:2506.00555.\nPeng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Wei-\njia Shi, Sheng Wang, Linjun Zhang, James Zou, and\n"}, {"page": 11, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nHuaxiu Yao. 2025b.\nMmed-rag: Versatile multi-\nmodal rag system for medical vision language mod-\nels. In The Thirteen International Conference on\nLearning Representations.\nPeng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun\nLi, Gang Li, Linjun Zhang, and Huaxiu Yao. 2024b.\nRule: Reliable multimodal rag for factuality in med-\nical vision language models. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1081–1093.\nXinyu Yang, Yuwei An, Hongyi Liu, Tianqi Chen, and\nBeidi Chen. 2025. Multiverse: Your language models\nsecretly decide how to parallelize and merge genera-\ntion. arXiv preprint arXiv:2506.09991.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\n2023. Tree of thoughts: Deliberate problem solving\nwith large language models.\nAdvances in neural\ninformation processing systems, 36:11809–11822.\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhi-\nhong Chen, Guiming Chen, Jianquan Li, Xiangbo\nWu, Zhang Zhiyi, Qingying Xiao, and 1 others. 2023.\nHuatuogpt, towards taming language model to be\na doctor. In Findings of the association for com-\nputational linguistics: EMNLP 2023, pages 10859–\n10885.\nWenchuan Zhang, Jingru Guo, Hengzhe Zhang, Peng-\nhao Zhang, Jie Chen, Shuwan Zhang, Zhang Zhang,\nYuhao Yi, and Hong Bu. 2025. Patho-agenticrag: To-\nwards multimodal agentic retrieval-augmented gener-\nation for pathology vlms via reinforcement learning.\narXiv preprint arXiv:2508.02258.\nLianmin Zheng,\nLiangsheng Yin,\nZhiqiang Xie,\nChuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi\nCao, Christos Kozyrakis, Ion Stoica, Joseph E Gonza-\nlez, and 1 others. 2024. Sglang: Efficient execution\nof structured language model programs. Advances\nin neural information processing systems, 37:62557–\n62583.\nKangyu Zhu, Peng Xia, Yun Li, Hongtu Zhu, Sheng\nWang, and Huaxiu Yao. 2025. Mmedpo: Aligning\nmedical vision-language models with clinical-aware\nmultimodal preference optimization. Forty-Second\nInternational Conference on Machine Learning.\nYuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai\nZhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and\nBowen Zhou. 2025. Medxpertqa: Benchmarking\nexpert-level medical reasoning and understanding.\narXiv preprint arXiv:2501.18362.\n"}, {"page": 12, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nA\nAdditional Analysis\nA.1\nEffect of Training Data Scale\nSamples\n1k\n2k\n5k\n8k\n14k\nSubset Ratio\n∼7%\n∼14%\n∼36%\n∼57%\n100%\nAverage Accuracy\n29.2%\n32.5%\n37.5%\n38.7%\n39.2%\nTable 3: Scalability Analysis. The monotonic improve-\nment in average accuracy across all benchmarks con-\nfirms the effectiveness of our data scaling strategy.\nWe investigate the scalability of our approach\nby fine-tuning the base model on varying sub-\nsets of the MedVerse-14K dataset (ranging from\n1k to 14k samples).\nAs presented in Table 3,\nwe observe a clear monotonic positive correlation\nbetween dataset size and the average reasoning\naccuracy across all evaluation benchmarks. No-\ntably, the model exhibits exceptional data effi-\nciency: with only 5k samples (∼36% of the total\ndata), it achieves an accuracy of 37.5%, recovering\nover 95% of the peak performance. Furthermore,\nthe performance does not plateau even at the 14k\nmark.This validates the high quality of our synthe-\nsized data and suggests that further scaling could\nyield even greater improvements.\nA.2\nAnalysis of Training Strategies and\nInference Modes\nTo further investigate the source of MedVerse’s per-\nformance gains, we conducted a comprehensive\nablation study decoupling the training strategies\n(standard autoregressive vs. MedVerse attention)\nfrom the inference execution mode (serial vs. par-\nallel). We evaluated four configurations on the\nQwen2.5-7B-Instruct backbone across our bench-\nmarks. The aggregated results are summarized in\nTable 4.\nThe four configurations are defined as follows:\n• Auto-Ser: Standard autoregressive training\nexecuted with a standard serial engine (base-\nline).\n• Auto-Par: Standard autoregressive training\nexecuted with our DAG-based parallel engine.\n• Mask-Ser: MedVerse topology-aware train-\ning (masked attention) executed serially.\n• Mask-Par: MedVerse topology-aware train-\ning executed with our DAG-based parallel en-\ngine.\nMetric\nAuto-Ser\nAuto-Par\nMask-Ser\nMask-Par (Ours)\nAverage Accuracy\n0.3690\n0.3792\n0.3856\n0.3934\nTable 4: Ablation study on Training Strategies and In-\nference Modes. We report the average accuracy across\nfive medical reasoning benchmarks (HLE, MedBullets-\nop4/op5, MedQA, MedXpert).\nDiscussion.\nThe results demonstrate a strong\nsynergistic effect, with Mask-Par achieving the\nhighest accuracy (39.34%, +2.44% over baseline).\nThis improvement highlights two critical insights.\nFirst, the superiority of Mask-Ser over the baseline\n(+1.66%) confirms that topology-aware masking\nenhances learning; by “hiding” irrelevant parallel\nbranches, the mechanism prevents reliance on spu-\nrious positional correlations and forces the model to\nfocus on genuine causal dependencies. Second, re-\ngarding topological alignment, the consistent gains\nfrom parallel structures in both training and in-\nference indicate that medical reasoning inherently\nfollows a DAG topology rather than a linear chain.\nMedVerse succeeds precisely by aligning the com-\nputational framework with this non-linear cognitive\nstructure.\nB\nDetailed MedVerse Curator Pipeline\nThis appendix provides a detailed description of\nthe MedVerse Curator, including phase-wise proce-\ndures for knowledge grounding, topological plan-\nning, structural synthesis, and data verification.\nPhase 1: Knowledge-Grounded Path Initializa-\ntion.\nThe pipeline begins by anchoring the rea-\nsoning in established medical knowledge, lever-\naging the retrieval methodology from MedReason\n(Steps 1–3):\n(i) Knowledge Retrieval: We first retrieve po-\ntential reasoning paths connecting the ques-\ntion entities to answer candidates from a large-\nscale medical Knowledge Graph (KG).\n(ii) Entity Mapping: We perform medical entity\nextraction to map unstructured query terms to\nstandardized KG nodes.\n(iii) Path Pruning: We search for and prune ir-\nrelevant branches to isolate the original, raw\nreasoning paths.\nThis phase provides a \"ground truth\" skeleton,\nensuring the subsequent generation is factually\ngrounded rather than hallucinated.\nPhase 2: Topological Planning and Filtering.\nWe then transform these linear skeletons into the\n"}, {"page": 13, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nMedVerse architecture (Steps 4–5). We employ\na specialized prompt to filter and edit the raw\npaths, removing redundancy and ensuring logical\ncoherence. Crucially, we perform a DAG Valid-\nity Check: the refined path is analyzed to ensure\nit forms a valid Directed Acyclic Graph. If the\ndependencies form a cycle, the path is rejected or\nre-routed.\nPhase 3: Structural Synthesis and Refinement.\nThis core phase generates the XML-structured data\n(Steps 6–8):\n• Plan Generation (<Plan>): We iteratively de-\ncompose the verified path, utilizing regex-based\nformatting to outline the Petri Net structure,\ndefining transitions and explicit dependency lists.\n• Parallel Execution (<Execution>): The teacher\nLLM generates the \"transient step\" content for\neach transition.\n• Iterative Refinement: We implement a Refine-\nment Module to knit these independent steps\ninto a cohesive narrative. This module dedupli-\ncates overlapping logic across parallel branches,\nremoves non-contributory details, and smooths\nthe transition flow to ensure the reasoning natu-\nrally bridges the gap from the problem descrip-\ntion to the final entity.\n• Conclusion Synthesis: Finally, conditioned on\nthe refined execution trajectory, the model gen-\nerates the <Conclusion>, providing the final an-\nswer and a holistic explanation.\nPhase 4: Dual-Layer Verification Loop.\nTo\nguarantee data quality, we append two supplemen-\ntary validation stages:\n(a) Syntax Verification: Ensures strict adherence\nto the XML schema and Petri Net definition\n(e.g., matching <Step> indices to <Outline>\nplans).\n(b) Logic & Completeness Evaluation: An eval-\nuator model assesses the reasoning chain for\nlogical gaps and verifies that the conclusion\ncorrectly addresses the user goal.\nData failing either check triggers an iterative regen-\neration loop until all criteria are met.\nC\nPrompting Protocol for the MedVerse\nCurator\nIn this section, we present the complete five-stage\nprompting protocol used by the MedVerse Curator\nto construct the MedVerse-14K dataset, powered\nby the GPT-5.1 model accessed via the ChatGPT\nAPI. The goal of this protocol is to systematically\ntransform question–answer pairs into structured,\nparallel medical reasoning trajectories suitable for\ntopology-aware execution.\nThe protocol is implemented as an offline data\nconstruction pipeline consisting of five sequential\nphases. Phase 1 adopts the knowledge-grounded re-\ntrieval procedure proposed in MedReason to initial-\nize medically valid reasoning paths; as this phase\ndirectly reuses an existing method without modifi-\ncation, we do not reproduce the original prompts.\nPhases 2–4 introduce new structured transforma-\ntions that progressively induce parallel reasoning\nstructure, repopulate detailed clinical content, and\nenforce execution-ready constraints. The full spec-\nifications and prompts for Phases 2–4 are listed\nbelow.\n"}, {"page": 14, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nPhase 2: Reasoning Chain Filtering Template\nSYSTEM PROMPT\nYou are a strict reasoning chain filter. Given a question, a list of candidate reasoning chains (original_reasoning), and\nthe correct answer, select only the reasoning chains that are directly relevant for deriving the answer from the question.\nFiltering Rules (Follow All Exactly):\n1) Relevance: Keep only chains that directly or critically contribute to deriving the answer from the question. Discard\nany chain that is unrelated or unnecessary for reaching the answer.\n2) Consistency: Remove chains that contradict the facts stated in the question or that lead to conclusions conflicting\nwith the answer.\n3) Duplicate Removal: If multiple chains are textually identical, keep only the first occurrence.\n4) Order & Priority: Preserve the original order of appearance in original_reasoning. If more than 10 chains\nremain, output the 10 most useful ones for deriving the answer (strongest/direct connection).\n5) Text Integrity: Do not modify any retained reasoning chain text. Each chain must remain exactly identical to its\noriginal text (from ’A->B->C->...’ to the end). Only reassign new indices starting from 1 in ascending order.\n6) Empty Case: If no chains satisfy the rules, output nothing (no text, no comment).\nUSER INPUT\nInput:\n• question: {question}\n• answer: {answer}\n• original_reasoning (each line formatted as \"<index>: A->B->C->...\"): {original_reasoning}\nOutput:\nReturn only the filtered reasoning chains in this exact format and nothing else:\n1: reasoning chain text (identical to original)\n2: reasoning chain text (identical to original)\n...\n(Up to 10 lines total.)\nPhase 2: Reasoning Chain Refinement Template\nSYSTEM PROMPT\nYou are an expert in the medical domain.\nGoal: Generate reasoning chains where the Start Node is strongly correlated with a key entity in the Question, and the\nEnd Node is strongly correlated with the Answer entity.\nSTRONG PRIORITY ON USING PROVIDED PATHS:\n• Maximize reuse of entities and links that appear in the provided Paths.\n• Prefer chains composed entirely of nodes from the Paths.\n• If you reuse nodes from the Paths, keep their strings EXACTLY as written (same casing, no edits).\n• Select Start/End nodes from the provided Paths that have the highest semantic or clinical correlation to the Ques-\ntion/Answer context.\nSTRICT OUTPUT RULES:\n1) Format: Each line: ’<index>: A->B->C->...’ (Index starts at 1, exactly one space after colon).\n2) Delimiter: Use ’->’ as the ONLY delimiter with no spaces around it.\n3) Start Node: Must have a STRONG CORRELATION (semantic or clinical) to a key entity in the Question (does\nNOT need to be an exact string match).\n4) Final Node: Must have a STRONG CORRELATION to the Answer entity (does NOT need to be the exact Answer\nstring).\n5) Validity: Each link A→B must be a medically valid causal/inferential relation.\n6) Cleanliness: No headers, no explanations, no extra text. Do not rewrite/rename nodes taken from the Paths.\n7) Quantity: Output up to 6 chains; output nothing if no valid chain can be formed.\n8) Preference: If multiple valid options exist, prefer chains that (a) maximize coverage of nodes from the provided\nPaths, and (b) require zero new nodes (or at most one new medically sound bridge).\n9) Diversity: Avoid producing only a single reasoning chain unless only one medically valid path exists; whenever\npossible, output two or more distinct valid reasoning chains.\nUSER INPUT\n• Question: {question}\n• Answer: {answer}\n• Paths (filtered reasoning paths to reuse): {filter_reasoning_path}\nOutput:\n1: A->B->...\n2: A->B->...\n...\n"}, {"page": 15, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nPhase 2: Reasoning Chain Editing Template\nSYSTEM PROMPT\nYou are a medical-domain reasoning chain editor.\nEdit policy (hard constraints):\n• Identity-by-default: If a chain is already complete and logically sound, output it UNCHANGED.\n• Only-if-incomplete: Modify a chain ONLY when it is logically incomplete between Question-entity and Answer-\nentity/synonym.\n• Preserve-original-entities: Do NOT alter, delete, paraphrase, or reorder ANY existing entities or links.\n• Insert-only-new-bridge: When a fix is necessary, INSERT the FEWEST possible concise medical entities as bridges;\ndo not modify existing tokens. Prefer ≤2 new entities per chain.\n• Medical validity: Each hop A→B must be a clinically valid causal/inferential relation.\n• Uncertainty: If uncertain whether a change is required, leave the chain UNCHANGED.\nUSER INPUT\nRequirements:\n1) Only add new entities to chains that are logically incomplete; keep ALL original entities exactly as given (no edits,\nno reordering, no deletions).\n2) Use ’->’ only; no spaces around it. Output one line per chain as ’<index>: A->B->C->...’; indices start at 1,\nincrement by 1, and have exactly one space after ’:’.\nInput Data:\n• Question: {question}\n• Answer: {answer}\n• new_reasoning_path: {new_reasoning_path}\nOutput:\n1: A->B->...\n2: A->B->...\n...\nPhase 3: Atomic Reasoning Step Template\nSYSTEM PROMPT\nYou are an expert in the medical domain. Your task is to perform Chain-of-Thought (CoT) reasoning for a single step\nindependently, strictly based on prior dependency results and current step keywords.\nCore Task: Reason through only the current step. Rely solely on: 1) The CoT results from the directly dependent\nprevious steps. 2) The entity keywords specific to the current step.\nStrict Constraints (Do NOT):\n• Do not explain the overall goal or speculate about future steps.\n• Do not explicitly mention \"dependencies\" or reference previous steps by name (e.g., \"based on step 1\").\n• Do not introduce extraneous definitions or general knowledge unrelated to the specific structural logic.\n• Do not state functions or importance; stick to factual, anatomy-based structural logic.\nMandatory Requirements:\n• Conciseness: Output only a single, short CoT paragraph.\n• Entity Detail: All entities mentioned in the current step AND its dependencies MUST have their factual details\nexplicitly included (e.g., structural, anatomical, or pathological attributes).\n• completeness: The reasoning is considered incorrect if any entity detail is omitted or vaguely referenced.\nUSER INPUT\nInput Data:\n1. Goal: {goal}\n2. Plan (Steps & Dependencies): {plan}\n3. Executed Steps (CoT results from dependencies only): {executed_step}\n4. Current Step (Focus of this reasoning): {current_step}\nOutput Instruction: CoT Paragraph: (Provide only one short paragraph of factual reasoning. Do not reference the\nanswer or future steps.)\n"}, {"page": 16, "text": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution\nPhase 3: Reasoning Refinement Template\nSYSTEM PROMPT\nYou are an expert in extracting concise, non-redundant, factual reasoning from multi-step medical analyses.\nInput Context: You will be given: 1) A goal, describing the final medical question. 2) A series of step-wise reasoning\noutputs, where each step includes a label (Entity A -> Entity B) and a detailed reasoning paragraph.\nTask: Eliminate any redundant content that has already appeared in previous steps. Keep only factual details necessary\nfor reasoning toward the goal.\nStrict Constraints:\n• No Redundancy: Remove information repeated from previous steps.\n• Facts Only: Contain only medical facts; exclude definitions, purpose, significance, or usefulness.\n• Logic Preservation: Must retain the logical relationship between Entity A and Entity B.\n• Format: Return the original step label followed by your revised concise reasoning.\n• No Meta-Content: Do not include explanations, summaries, or overall conclusions.\nUSER INPUT\n• Goal: {goal}\n• Stepwise Reasoning: {multistep_reasoning}\nPhase 3: Conclusion Synthesis Template\nSYSTEM PROMPT\nYou are an expert in medical reasoning. You will be given three inputs: 1) Reasoning Paragraphs (a sequence of\n’Transient Step N: ...’ logical steps), 2) Question (a medical multiple-choice question), 3) Options (possible\nanswers).\nYour Task:\n• Use ONLY the Reasoning Paragraphs to determine the correct answer.\n• First, output the final answer to the Question.\n• Then, output one concise paragraph explaining why this is the correct answer, referencing the relevant steps.\n• Constraint: Do not add external knowledge.\nOutput Format:\nExplanation: <one paragraph justification>\nAnswer: <the correct option>\nUSER INPUT\n• Reasoning Paragraphs: {total_final}\n• Question: {question}\n• Options: {option}\nPhase 4: Answer & Logic Verification Template\nSYSTEM PROMPT\nYou are an expert evaluator of medical reasoning consistency.\nInput Context: You will be given: 1) A goal (the correct answer). 2) A question and its options. 3) A conclusion (final\nanswer + explanation). 4) A set of reasoning steps (the chain of thought).\nYour Tasks:\n1. Answer Verification: Verify whether the conclusion’s final answer matches the correct answer specified in the goal.\n2. Logic Verification: Verify whether the explanation in the conclusion is logically consistent with the reasoning steps.\n• The explanation must be directly derivable from the given reasoning steps.\n• It must not rely on external facts or background knowledge not present in the reasoning steps.\nOutput Requirements:\n• Output \"Consistent\" if AND ONLY IF: (a) the answer matches the goal AND (b) the explanation is logically derived\nsolely from the steps.\n• Otherwise, output \"Inconsistent\".\n• Do not output anything else.\nUSER INPUT\n• Goal (correct answer): {goal}\n• Question: {question}\n• Options: {options}\n• Conclusion (explanation + answer): {conclusion}\n• Reasoning Steps: {reasoning_steps}\n"}]}