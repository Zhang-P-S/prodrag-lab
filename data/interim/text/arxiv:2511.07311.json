{"doc_id": "arxiv:2511.07311", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.07311.pdf", "meta": {"doc_id": "arxiv:2511.07311", "source": "arxiv", "arxiv_id": "2511.07311", "title": "ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding", "authors": ["Tuan-Dung Le", "Shohreh Haddadan", "Thanh Q. Thieu"], "published": "2025-11-10T17:11:20Z", "updated": "2025-11-10T17:11:20Z", "summary": "Automatic ICD coding, the task of assigning disease and procedure codes to electronic medical records, is crucial for clinical documentation and billing. While existing methods primarily enhance model understanding of code hierarchies and synonyms, they often overlook the pervasive use of medical acronyms in clinical notes, a key factor in ICD code inference. To address this gap, we propose a novel effective data augmentation technique that leverages large language models to expand medical acronyms, allowing models to be trained on their full form representations. Moreover, we incorporate consistency training to regularize predictions by enforcing agreement between the original and augmented documents. Extensive experiments on the MIMIC-III dataset demonstrate that our approach, ACE-ICD establishes new state-of-the-art performance across multiple settings, including common codes, rare codes, and full-code assignments. Our code is publicly available.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.07311v1", "url_pdf": "https://arxiv.org/pdf/2511.07311.pdf", "meta_path": "data/raw/arxiv/meta/2511.07311.json", "sha256": "846000d6761299b6efb55397ea06d2f232093937690d2def06d186060b5f53b6", "status": "ok", "fetched_at": "2026-02-18T02:27:26.798601+00:00"}, "pages": [{"page": 1, "text": "ACE-ICD: Acronym Expansion As Data Augmentation For Automated\nICD Coding\nTuan-Dung Le1,2, Shohreh Haddadan1, Thanh Q. Thieu1,2\n1Moffitt Cancer Center and Research Institute, 2University of South Florida\n{tuandung.le, shohreh.haddadan, thanh.thieu}@moffitt.org\nAbstract\nAutomatic ICD coding, the task of assigning\ndisease and procedure codes to electronic med-\nical records, is crucial for clinical documenta-\ntion and billing. While existing methods pri-\nmarily enhance model understanding of code\nhierarchies and synonyms, they often over-\nlook the pervasive use of medical acronyms\nin clinical notes, a key factor in ICD code in-\nference. To address this gap, we propose a\nnovel effective data augmentation technique\nthat leverages large language models to ex-\npand medical acronyms, allowing models to\nbe trained on their full form representations.\nMoreover, we incorporate consistency training\nto regularize predictions by enforcing agree-\nment between the original and augmented doc-\numents. Extensive experiments on the MIMIC-\nIII dataset demonstrate that our approach, ACE-\nICD establishes new state-of-the-art perfor-\nmance across multiple settings, including com-\nmon codes, rare codes, and full-code assign-\nments. Our code is publicly available 1.\n1\nIntroduction\nAssigning standardized codes based on the Inter-\nnational Classification of Diseases (ICD2) known\nas ICD Coding is essential for efficient medical\nrecord management, accurate billing processes, and\nstreamlined insurance reimbursements (Park et al.,\n2000; Sonabend et al., 2020). However, traditional\nICD coding relies on manual effort, making it time-\nintensive and error-prone driving the development\nof automated coding methods.\nAccurate ICD code assignment, whether man-\nual or automated, requires a comprehensive un-\nderstanding of clinical notes, which include de-\ntailed information such as symptoms, diagnoses,\nand test results. However, healthcare profession-\nals often rely on acronyms and abbreviations to\n1https://github.com/LangIntLab/ACE-ICD\n2who.int/standards/classifications/\nclassification-of-diseases\nDischarge Summary\n... history of present illness: ortho hpi: 86m w/ severe\nb/l oa , admitted to ortho for sequential bilateral tka ...\nicu hpi: 86 y/o m with pmhx of arthritis, bph &\nosteoporosis s/p elective right total knee replacement ...\npast medical history: osteoporosis anemia (family h/o\ng6pd deficiency) bph osteoarthritis cataracts ...\nempiric vancomycin and ceftriaxone for possible uti\nwere initiated ...\nLabel\nKEPT ACE-ICD\n599.0 urinary tract infection, site not\nspecified\n✗\n✓\n715.36 osteoarthrosis, localized, not\nspecified whether primary or sec-\nondary, lower leg\n✗\n✓\n81.54 total knee replacement\n✓\n✓\n285.9 anemia, unspecified\n✓\n✓\n...\n...\n...\nTable 1: Example predictions from the MIMIC-III-full\ndataset (HADM_ID = 108519) using KEPT(Yang et al.,\n2022) and our ACE-ICD models.\nreduce documentation effort (Amosa et al., 2023).\nThis shorthand introduces significant ambiguity,\nmaking it difficult for both human coders and lan-\nguage models to interpret the clinical text correctly.\nTo better understand the impact of acronyms on\nICD code assignment, we analyze expert-annotated\nevidence spans from the MDACE dataset (Cheng\net al., 2023), which is derived from MIMIC-III\n(Johnson et al., 2016). Our analysis shows that\n22% of the evidence spans either consist entirely\nof acronyms and abbreviations or include at least\none such term, highlighting that acronyms often\ncarry useful information for assigning ICD codes.\nPrevious studies improved model understanding\nof medical acronyms by leveraging their appear-\nance in code synonyms (Yuan et al., 2022; Yang\net al., 2022; Gomes et al., 2024). However, they\nstill fail to predict codes which contain acronyms\nin their synonym description. Table 1 shows sev-\neral instances of this shortcoming in previous mod-\narXiv:2511.07311v1  [cs.CL]  10 Nov 2025\n"}, {"page": 2, "text": "Figure 1: Our training pipeline incorporating acronym-expanded data augmentation and consistency training.\nels. These failures underscore the need for new ap-\nproaches to effectively address medical acronyms\nto consequently improve the ICD coding system.\nIn this paper, we propose ACronym Expansion\nfor ICD coding (ACE-ICD) to investigate the im-\npact of expanding medical acronyms to full terms\non coding models. Our system harnesses the strong\ncapability of large language models in disambiguat-\ning clinical acronyms (Kugic et al., 2024; Liu et al.,\n2024), as a data augmentation method that expands\nacronyms using open-source LLM prompting. We\nfurther apply KL divergence consistency regulariza-\ntion to enforce alignment between predictions from\nthe original and augmented examples. Finally, we\nconduct experiments on three standard ICD coding\ntasks using the MIMIC-III dataset, demonstrating\nthat our approach outperforms previous state-of-\nthe-art methods across all tasks.\n2\nMethods\nPrevious methods frame the ICD coding task as\na multi-label classification problem, as a single\nclinical note can contain multiple diagnosis or pro-\ncedure codes. Given a clinical note t, the task is to\nassign a binary label yi ∈{0, 1} for each ICD code\ni (where i = 1, 2, ..., Nc and Nc is the total num-\nber of ICD codes). A label of 1 denotes relevance,\nwhile 0 indicates irrelevance.\nWe followed the prompt-based fine-tuning ap-\nproach by (Yang et al., 2022) for the ICD coding\ntask, reformulating the multi-label classification\ntask as a cloze task (Schick and Schütze, 2021;\nGao et al., 2021). Specifically, we construct a\nprompt template by concatenating each ICD code\ndescription ci, appending a [MASK] token after\neach description, and adding the clinical note t.\nThe prompt P is given by: P = c1 [MASK] c2\n[MASK] ... cNc [MASK] t. The model is trained\nvia a masked language modeling objective to pre-\ndict “yes” or “no” in each [MASK] position, corre-\nsponding to a label of 1 or 0, respectively. For tasks\nwhere Nc is large (e.g., thousands of codes), this\napproach is typically used as a reranker (Tsai et al.,\n2021; Yang et al., 2022, 2023b; Kailas et al., 2023),\nas including all code descriptions in the prompt is\ninfeasible.\n2.1\nAcronym-expansion as data augmentation\nMotivated by capabilities of LLMs in clinical\nacronym disambiguation (Liu et al., 2024; Kugic\net al., 2024), we use the open-source Llama 3\nmodel (Dubey et al., 2024) to generate acronym-\nexpanded version of clinical notes, denoted as ta,\nfrom the original notes t. Due to the considerable\nlength of the MIMIC-III notes, we first split each\ndischarge summary based on the headers identified\nthrough automatic section-based segmentation (Lu\net al., 2023). We then prompt the instruction-tuned\nLlama 3 models to generate augmented sections,\nwhich are concatenated into the final acronym-\nexpanded note, using the following prompt:\n<|begin_of_text|>\n<|start_header_id|> system <|end_header_id>\nYou are a helpful assistant. <|eot_id|>\n<|start_header_id|> user <|end_header_id|>\nExpand all acronyms to their full forms while\npreserving all the details in the following paragraph,\ndo not mention the acronyms again. Paragraph: ___\n<|eot_id|>\n<|start_header_id|> assistant <|end_header_id|>\nHere is the paragraph with all acronyms expanded to\ntheir full forms:\n"}, {"page": 3, "text": "2.2\nConsistency training\nInspired by (Shen et al., 2020; Wu et al., 2021),\nwe incorporate a consistency loss into the train-\ning objective to encourage the model to generate\nsimilar predictions for the original clinical note t\nand its acronym-expanded counterpart ta. We first\nconstruct a second prompt template using the aug-\nmented note ta and a synonym si for each ICD code\ndescription collected from UMLS from previous\nstudies (Yuan et al., 2022; Gomes et al., 2024): Pa\n= s1 [MASK] s2 [MASK] ... sN [MASK] ta. The\ntraining objective can be written as the following:\nL = 1\n2(Lce(P, y) + Lce(Pa, y))\n+ αLcons(P, Pa, y)\nwhere Lce is cross-entropy loss for masked lan-\nguage modeling, and Lcons enforces consistency\nby minimizing the bidirectional KL-divergence:\nLcons = 1\n2(KL[p(y|P)||p(y|Pa)]\n+ KL[p(y|Pa)||p(y|P)])\n3\nExperiments\n3.1\nAcronym expansion evaluation\nDataset.\nSeveral annotated datasets for med-\nical abbreviations and acronyms have been intro-\nduced in prior work (Moon et al., 2012; Rajkomar\net al., 2022). To evaluate our zero-shot prompt-\ning approach on the MIMIC-III corpus, we utilize\nthe annotated dataset developed by Rajkomar et al.\n(2022), which we consider most suitable for our\ntask. This dataset was constructed using a reverse-\nsubstitution technique applied to MIMIC-III dis-\ncharge summaries, resulting in a large-scale, high-\nquality resource for acronym expansion. As our ex-\nperiments are also based on MIMIC-III, this dataset\noffers a consistent and reliable benchmark for eval-\nuation.\nMetrics.\nWe perform zero-shot prompting\nexperiments using four instruction-tuned variants\nof the Llama model. We first report detection pre-\ncision and recall (Rajkomar et al., 2022), which\nassess the model’s ability to identify abbreviations\nin text, regardless of whether the corresponding\nexpansions are correct. We then compute total ac-\ncuracy, defined as the proportion of abbreviations\nin the gold standard that are correctly expanded\nto their full forms by the model. To assess the\nquality of expansions, we consider two evaluation\nsettings. Strict accuracy requires an exact string\nmatch between the model-generated expansion and\nthe gold-standard full form. However, as illustrated\nin Table 3, certain expansions may differ lexically\nyet convey the same meaning. To accommodate\nsuch cases, we introduce lenient accuracy, which\ncomputes a similarity score based on the normal-\nized inverse edit distance between the generated\nand reference expansions. Specifically, we normal-\nize the edit distance by the length of the reference\nand consider expansions with a similarity score of\nat least 70% as correct. Although some semanti-\ncally valid expansions may fall below this thresh-\nold, we adopt the 70% cutoff to balance recall and\nprecision while avoiding acceptance of clearly in-\ncorrect outputs.\nImplementation Details.\nTo extract abbre-\nviations and their corresponding expansions from\nmodel outputs, we employ the Python difflib li-\nbrary3, which aligns two sequences.\n3.2\nICD coding evaluation\nWe evaluate our methods on three MIMIC-III tasks,\nfollowing (Mullenbach et al., 2018) for MIMIC-\nIII-50 and MIMIC-III-full dataset splits and (Yang\net al., 2022) for constructing MIMIC-III-rare50,\nwhich focuses on the top 50 codes with fewer than\n10 occurrences (See Table 2).\nDataset\nTrain\nDev\nTest\nNC\nMIMIC-III-50\n8066\n1573\n1729\n50\nMIMIC-III-full\n47723\n1631\n3372\n8922\nMIMIC-III-rare50\n249\n20\n142\n50\nTable 2: Statistics of the MIMIC-III ICD-9 datasets.\nMetrics.\nWe evaluate performance using\nmacro-AUC, micro-AUC, macro-F1, micro-F1,\nand precision@k (k = 5 for MIMIC-III-50, k = 8\nand 15 for MIMIC-III-full). We determine the opti-\nmal threshold for micro-F1 on the development set\nand report test metrics using the best-performing\ncheckpoint. To ensure robustness, we run each ex-\nperiment with five random seeds and report mean\nresults.\nImplementation Details.\nWe initialize our\nmodel with two pre-trained language models:\n3https://docs.python.org/3/library/difflib.\nhtml\n"}, {"page": 4, "text": "Acronym\nExpanded form by LLM\nFull form\nInverse Edit distance\npod#15\npost-operative day #15\npost-operative day #15\n100.0\nintrabd\nintrabdominal\nintra-abdominal\n86.67\np/w\npresented with\npresents with\n84.61\ndvt\ndeep vein thrombosis\ndeep venous thrombosis\n81.82\nco\ncomplained about\ncomplained of\n69.23\nangio\nangiography\nangiogram\n66.67\ndec\ndead\ndeceased\n50.0\nx3\nthree\nthree times\n45.45\np.a.\npersonal assistant\nphysician assistant\n68.42\npms\nprevious medical symptoms.\npremenstrual syndrome\n28.57\nvma\nvisual motor assessment\nvanillylmandelic acid\n0.0\nopa\noffice of personnel administration\noropharyngeal airway\n-25.0\nTable 3: Examples of expanded abbreviations, their correct full-forms and the value of the length-normalized inverse\nedit distance. The threshold is considered 70%, thus the expanded terms with a lower threshold are considered\nincorrect in our evaluation.\nKEPTLongformer4 and KEPT-PMM35. Follow-\ning (Yang et al., 2022), we preprocess MIMIC\ndischarge summary by removing de-identification\ntokens, replacing non-alphanumeric characters (ex-\ncept punctuation) with whitespace, and truncating\nat 8,192 tokens. If the length exceeded this limit, ir-\nrelevant sections were removed prior to truncation\nto retain the most relevant sections. The MIMIC-\nIII full dataset contains 8,922 ICD codes, making it\ninfeasible to include all descriptions in one prompt.\nWe re-rank the top 300 candidates predicted by\nMSMN (Yuan et al., 2022) and process 50 candi-\ndates at a time, as described in (Yang et al., 2022;\nKailas et al., 2023).\nTo determine the consistency loss weight (α),\nwe perform an ablation study on the MIMIC-III-\n50 dataset by varying α ∈{0.02, 0.05, 0.1, 0.2}.\nα = 0.05 yields the best performance across all\nevaluation metrics (see Table 6). This value is ap-\nplied to all other experiments, as tuning on the\nlarger MIMIC-III-full dataset is computationally\nexpensive. For the top-50 and rare-code datasets,\nwe randomly select 4 synonyms to construct the\naugmented prompt Pa, following findings from\n(Yuan et al., 2022; Gomes et al., 2024) that using\n4 or 8 synonyms improves ICD coding model per-\nformance. For MIMIC-III-full dataset, we use the\nsame code descriptions for both P and Pa, as we\nobserved that incorporating synonyms increases\ntraining time and slows convergence on this larger\ndataset.\n4https://huggingface.co/whaleloops/\nkeptlongformer\n5https://huggingface.co/whaleloops/\nKEPTlongformer-PMM3\nAll experiments are conducted on a single\nNVIDIA H100 80GB GPU, with training time and\nhyperparameters detailed in Appendix A.2. We\nuse the Llama-3.1-70B-Instruct model to perform\nzero-shot acronym expansion for all ICD coding\nexperiments, except for the ablation study reported\nin Table 7.\n4\nResults\n4.1\nAcronym expansion performance\nOur experimental results indicate that the size of\nthe large language model (LLM) used for zero-shot\nprompting has minimal impact on acronym detec-\ntion. Detection precision ranges from 93.7% with\nthe smallest model to 96.6% with the largest, while\nrecall increases from 84.5% to 90.5%. In contrast,\nmodel size has a substantial effect on acronym ex-\npansion accuracy, showing a notable performance\nimprovement of +42 percentage points, from 18.8%\nwith the 1B-parameter model to 60.8% with the\n70B-parameter model (Table 7). Evaluating expan-\nsions under the lenient accuracy criterion yields\nadditional gains of up to 4%.\nTable 3 illustrates example outputs from\nacronym expansion using the Llama-3.1-70B-\nInstruct model, along with their corresponding in-\nverse edit distance scores used for lenient accuracy\nevaluation. As shown in the second section of the\ntable, several expansions remain marked as incor-\nrect even under the lenient 70% similarity thresh-\nold, demonstrating our effort to avoid misclassify-\ning incorrect expansions as correct (for example,\nthe first example in the third section). As such, le-\nnient accuracy should be viewed as a conservative,\n"}, {"page": 5, "text": "lower-bound estimate of the model’s true acronym\nexpansion performance.\n4.2\nICD coding performance\nResults show that ACE-ICD outperforms previous\nstate-of-the-art methods across all three MIMIC-\nIII datasets (Table 4 and 5), regardless of whether\nKEPTLongformer or KEPT-PMM3 is used for ini-\ntialization. From now on, we refer to ACE-ICD\n(PMM3) as ACE-ICD. To assess statistical signif-\nicance, we conduct 1,000 rounds of permutation\ntesting comparing the predictions of ACE-ICD and\nthe KEPT baseline. The resulting p-values are all\nbelow 0.05 across evaluation metrics and datasets,\nindicating that our proposed approach yields sta-\ntistically significant improvements in ICD coding\nperformance.\nOn the MIMIC-III-50 task (Table 4), ACE-ICD\nachieves a macro AUC of 94.4 (+0.6), micro AUC\nof 95.9 (+0.5), macro F1 of 71.6 (+1.2), and mi-\ncro F1 of 75.0 (+1.0) and precision@5 of 70.0\n(+1.1), with values in parentheses indicate im-\nprovements over the previous best results. For the\nMIMIC-III-full task (Table 4), ACE-ICD outper-\nforms prior methods on most metrics except macro\nF1, achieving a macro F1 of 13.2 (-0.8), micro F1\nof 62.7 (+2.0), precision@8 of 79.4 (+1.0), and\nprecision@15 of 63.9 (+0.2). We manage to im-\nprove the macro F1 score to 14.3 (+0.3), by apply-\ning code-specific threshold optimization (see Ap-\npendix A.1). Under the MIMIC-III-rare50 setting\n(Table 5), ACE-ICD achieves a macro AUC of 91.1\n(+2.2), micro AUC of 91.9 (+2.0), macro F1 of 54.0\n(+13.7), and micro F1 of 55.8 (+13.2) when fine-\ntuned from the MIMIC-III-50 checkpoint. Notably,\nACE-ICD fine-tuned from KEPT-PMM3 outper-\nforms prior methods initialized from the MIMIC-\nIII-50 checkpoint.\n5\nDiscussion\nEffectiveness of our proposed training frame-\nwork.\nWe evaluate the impact of acronym aug-\nmentation and consistency training in ACE-ICD\nthrough an ablation study by adding each compo-\nnent separately and assessing performance on the\nMIMIC-III-50 dataset (Table 6). For data augmen-\ntation only, we include the augmented data into the\noriginal dataset, doubling the training set size while\nhalving the number of training epochs. For consis-\ntency training only, we apply R-Drop (Wu et al.,\n2021) directly to the original data. Both approaches\nimprove performance over the reproduced KEPT\nbaseline, with consistency training yields better\ngains. We attribute this to the fact that zero-shot\nacronym expansion may introduce translation er-\nrors, adding noise to the training data. In contrast,\nR-Drop acts as dropout augmentation and has been\nshown to enhance ICD coding performance by pre-\nventing overfitting (Yuan et al., 2022; Luo et al.,\n2024). However, the performance further improves\nwhen both strategies are applied together.\nImpact of acronym expansion quality in im-\nproving ICD coding performance.\nTo eval-\nuate how the quality of the acronym expansion\nprocess affects ICD coding performance, we con-\nduct an ablation study using Llama 3 models of\nvarying sizes and compare them to a baseline with-\nout acronym expansion, as shown in Table 7. We\nobserve that poor expansion quality can hurt perfor-\nmance: the 1B model, with less than 20% accuracy,\ndegrades coding results. Moderate-size models (3B\nand 8B), achieving 30–50% accuracy, yield only\nminor gains. In contrast, the 70B model, with over\n60% exact match accuracy, provides the most sig-\nnificant performance gains. By observing a few\nincorrectly expanded acronyms illustrated in Ta-\nble 3, we hypothesize that expansion errors may\nhave minimal impact if the expanded acronyms are\nunrelated to any ICD code.\nEffect of acronym expansion as data augmen-\ntation on different ICD codes.\nFigure 2 illus-\ntrates the F1-score improvement for each ICD code,\ncomparing our ACE-ICD model to the baseline\nKEPT model in the MIMIC-III-50 settings. The\nresults show that ACE-ICD outperforms the base-\nline on 39 out of 50 codes, with only marginal\ndecreases observed for the remaining ones. Our\nmethod effectively improves the performance of\nlower-performing codes, including 99.04 (22.8 →\n29.9), 285.9 (24.5 →35.9), 38.91 (28.0 →43.6),\n37.23 (35.2 →51.4), and V15.82 (12.7 →33.4).\nA Pearson correlation of –0.25 (p = 0.08) between\nabsolute F1 improvement and the number of train-\ning examples per code suggests that rarer codes\ntend to benefit more from our method, although the\nevidence is not statistically significant.\nMoreover, although our approach is designed to\nimprove model robustness to the use of acronyms in\nclinical texts, the impact of acronym expansion on\nICD coding performance varies depending on the\npresence of code-relevant evidence in abbreviated\nor expanded form across different codes. If several\n"}, {"page": 6, "text": "Methods\nMIMIC-III-50\nMIMIC-III-full\nAUC\nF1\nPre\nF1\nPre\nMacro\nMicro\nMacro\nMicro\nP@5\nMacro\nMicro\nP@8\nP@15\nMSMN(Yuan et al., 2022)\n92.8\n94.7\n68.3\n72.5\n68.0\n10.3\n58.4\n75.2\n59.9\nPLM-ICD(Huang et al., 2022)\n-\n-\n-\n-\n-\n10.4\n59.8\n77.1\n61.3\nDiscNet+RE(Zhang et al., 2022)\n-\n-\n-\n-\n-\n14.0\n58.8\n76.5\n61.1\nKEPT (KL)(Yang et al., 2022) †\n92.6\n94.8\n68.9\n72.9\n67.3\n11.8\n59.9\n77.1\n61.5\nCoRelation (Luo et al., 2024)\n93.3\n95.1\n69.3\n73.1\n68.3\n10.2\n59.1\n76.2\n60.7\nMSAM(Gomes et al., 2024)\n93.7\n95.4\n70.4\n74.0\n68.9\n-\n-\n-\n-\nExtra human annotations\nNoteContrast(Kailas et al., 2023) †\n93.8\n95.4\n69.2\n73.6\n68.6\n11.9\n60.7\n77.8\n62.2\nMRR (Wang et al., 2024a)\n92.7\n94.7\n68.7\n73.2\n68.5\n11.4\n60.3\n77.5\n62.3\nAKIL (Wang et al., 2024b)\n92.8\n95.0\n69.2\n73.4\n68.3\n11.2\n60.5\n78.4\n63.7\nOurs\nKEPT (KL) †*\n93.1\n94.9\n68.5\n72.7\n67.6\n11.3\n60.3\n77.5\n61.6\nKEPT (PMM3) †*\n93.6\n95.2\n69.6\n73.5\n68.3\n12.9\n61.5\n78.4\n62.7\nACE-ICD (KL) †\n93.9\n95.6\n70.9\n74.5\n69.2\n11.7\n61.8\n78.6\n63.0\nACE-ICD (PMM3) †\n94.4\n95.9\n71.6\n75.0\n70.0\n13.2\n62.7\n79.4\n63.9\nGPT4-based\nLLM-codex (Yang et al., 2023a)\n92.9\n94.8\n67.4\n71.5\n-\n-\n-\n-\n-\nMulti-Agents (Li et al., 2024)\n-\n-\n74.8\n58.9\n-\n-\n-\n-\n-\nTable 4: Results on MIMIC-III-50 and MIMIC-III-full datasets, using KEPTLongformer (KL) and KEPT-PMM3\n(PMM3). Our reproduced KEPT results (marked with *) closely align with those reported by (Yang et al., 2022).\nMethods marked with † indicate approaches that re-rank the top 300 predictions from MSMN (Yuan et al., 2022)\nunder the MIMIC-III-full setting.\nMethods\nTrained\nAUC\nF1\nfrom\nMacro Micro Macro Micro\nMSMN\n75.4\n77.4\n15.3\n16.6\nKEPT (KL)\n79.4\n80.7\n24.6\n23.3\nNoteContrast\npre-trained 85.7\n86.7\n39.0\n41.8\nACE-ICD (KL)\n86.8\n89.1\n37.9\n37.7\nACE-ICD (PMM3)\n92.2\n90.9\n49.1\n51.1\nMSMN\n59.0\n58.9\n3.5\n5.5\nKEPT (KL)\nMIMIC\n82.3\n83.7\n29.0\n31.4\nNoteContrast\nIII-50\n88.9\n89.9\n40.3\n42.6\nACE-ICD (KL)\ncheckpoint 90.0\n90.9\n45.3\n48.1\nACE-ICD (PMM3)\n91.1\n91.9\n54.0\n55.8\nGPT4-based\nLLM-codex\n82.5\n83.2\n27.9\n30.2\nMulti-Agents\n-\n-\n71.5\n37.6\nTable 5: Results on the MIMIC-III-rare50 dataset.\nfull-form expressions are already present in the\ntext, acronym expansion may not be necessary for\naccurate code prediction (e.g., “tka” and “total knee\nreplacement” as evidence for code 81.54 in the\nexample from Table 1).\nA case study is shown in Table 1. Our ACE-\nICD demonstrate a better understanding of medi-\ncal acronyms, correctly predicting codes such as\n599.0 and 715.36.\nIn this discharge summary\n(HADM_ID=108519), the only evidence for in-\nModel\nMacro F1 Micro F1 P@5\nKEPT (baseline)\n69.6\n73.5\n68.3\n+ data augmentation only\n70.0\n73.8\n68.6\n+ consistency training only\n71.2\n74.7\n69.6\nACE-ICD (+ both)\n71.6\n75.0\n70.0\nα = 0.2\n71.0\n74.6\n69.4\nα = 0.1\n71.6\n75.0\n69.7\nα = 0.05\n71.6\n75.0\n70.0\nα = 0.02\n71.4\n74.7\n69.4\nTable 6: Ablation study on MIMIC-III-50.\nferring code 599.0 is the acronym \"uti\", as other\nmentions of \"urine\" in the text are not relevant.\nSimilarly, code 715.36 can be inferred from \"oa\"\nor its synonym \"osteoarthritis\". Despite the KEPT\nmodel being pre-trained to incorporate knowledge\nfrom UMLS terms, it still fails to correctly predict\nthese codes, highlighting the effectiveness of our\napproach in handling medical acronyms.\nComparison with recent methods.\nOur\nmethod outperforms previous state-of-the-art mod-\nels across all three MIMIC-III datasets, including\nthose using additional human annotations or auxil-\niary clinical knowledge (Kailas et al., 2023; Wang\net al., 2024a,b). Notably, our approach signifi-\ncantly improves over the KEPT baseline using a\nsmaller model, KEPTLongformer (149M parame-\n"}, {"page": 7, "text": "Figure 2: F1 improvement per code in MIMIC-III-50 dataset (sorted by number of training examples in descending\norder).\nExpansion models\nAcronym Expansion Performance\nICD coding Performance\nDetection\nAccuracy\nAUC\nF1\nPre\nPrecision\nRecall\nStrict\nLenient\nMacro\nMicro\nMacro\nMicro\nP@5\nLlama-3.2-1B-Instruct\n93.7\n84.5\n18.8\n19.7\n94.0\n95.6\n70.5\n73.9\n69.2\nLlama-3.2-3B-Instruct\n95.3\n84.3\n31.0\n33.1\n94.2\n95.7\n71.4\n74.7\n69.6\nLlama-3.1-8B-Instruct\n96.6\n86.9\n46.3\n49.5\n94.1\n95.7\n71.5\n74.8\n69.6\nLlama-3.1-70B-Instruct\n96.6\n90.5\n60.8\n64.7\n94.4\n95.9\n71.6\n75.0\n70.0\nWithout acronym expansion (consistency training only)\n94.2\n95.7\n71.2\n74.7\n69.6\nTable 7: Performance of instruction-tuned Llama model variants on acronym expansion and ICD coding. Expansion\naccuracy is evaluated on the reverse-substituted dataset from Rajkomar et al. (2022), and ICD coding results are\nreported on MIMIC-III-50.\nters), and even outperforms MSAM(Gomes et al.,\n2024), built on the larger GatorTron model (345M\nparameters), under the common code setting.\nEven though, GPT-4-based approaches show\npromising results in ICD coding (Yang et al.,\n2023a; Li et al., 2024), they still underperform\ncompared to fine-tuned encoder-based models, par-\nticularly in terms of micro-F1. The multi-agent\nframework proposed by Li et al. (2024) achieves a\nhigher macro-F1, which suggests better handling of\nrare codes due to GPT-4’s extensive medical knowl-\nedge, but it exhibits lower micro-F1, reflecting chal-\nlenges in accurately predicting frequent codes.(See\ntables 4 and 5). Additionally, these methods require\nrepeatedly sending clinical notes to third-party ser-\nvices to access proprietary LLMs, raising concerns\naround privacy, cost, and scalability. In contrast,\nour method delivers strong performance on both\nfrequent and rare codes through a lightweight, tar-\ngeted data augmentation strategy that uses open-\nsource LLMs locally in a one-time preprocessing\nstep, preserving privacy and efficiency.\n6\nRelated works\n6.1\nAutomatic ICD coding\nAutomatic ICD coding is a multi-label classifica-\ntion task that assigns diagnosis and procedure codes\nto clinical notes (Perotte et al., 2014; Nguyen et al.,\n2023a). Early approaches use CNNs (Mullenbach\net al., 2018; Xie et al., 2019), LSTMs (Vu et al.,\n2020; Nguyen et al., 2023b), and Transformers\n(Huang et al., 2022) to encode clinical notes, while\nincorporating label attention mechanisms to cap-\nture relationships between the notes and ICD codes.\nRecent studies further improve code representa-\ntion by integrating multiple code synonyms (Yuan\net al., 2022; Gomes et al., 2024) or code relation\ngraph learning (Luo et al., 2024). Contrastive learn-\ning has also been applied to improve model capa-\nbilities, either between medical entities in UMLS\n"}, {"page": 8, "text": "(Yang et al., 2022) or between clinical notes and\nICD codes (Kailas et al., 2023). Several studies en-\nhance ICD coding performance by leveraging the\ndiscourse structure of clinical notes, such as using\nsection type embeddings (Zhang et al., 2022) or\ncontrastive pre-training between sections (Lu et al.,\n2023).\nWhile most methods rely solely on the provided\nclinical notes and code descriptions, some studies\nfocuses on enhancing ICD coding performance us-\ning extra human annotations or data augmentation.\nKailas et al. (2023) pre-train a model on temporal\nsequences of diagnostic codes using proprietary\ndata from a large patient cohort, where clinical\nnotes are paired with ICD-10 codes, to provide a\nstrong initialization for finetuning coding model.\nWang et al. (2024a,b) incorporate auxiliary informa-\ntion such as diagnosis-related group (DRG) codes,\ncurrent procedural terminology (CPT) codes, and\nprescribed medications to improve performance.\nLu et al. (2023) introduce masked section training\nwith small ratio as a data augmentation strategy,\nfollowing contrastive pre-training between note’s\nsections to boost model performance. Falis et al.\n(2022) propose ontology-guided synonym augmen-\ntation and sibling-code replacement to generate\nsilver training examples. However, their method\nrequires a pretrained named entity recognition and\nlinking system to identify code-relevant text spans.\nLLMs have demonstrated remarkable capabili-\nties across various general-domain tasks and have\nrecently been explored for ICD coding (Boyle et al.,\n2023; Soroush et al., 2024). Yang et al. (2023a) in-\ntroduced LLM-Codex, which generates ICD codes\nand evidence with GPT-4, followed by LSTM-\nbased verification. Li et al. (2024) use GPT-4 to\nconvert discharge summaries into Subjective, Ob-\njective, Assessment, and Plan (SOAP) format, al-\nlowing multiple agents to perform ICD coding via\npredefined workflows. While promising, these ap-\nproaches still lag behind fully fine-tuned non-LLM\nmodels on MIMIC-III common and rare datasets.\nTo the best of our knowledge, no prior work\nhas explored augmenting data with acronym expan-\nsions and incorporating them via consistency train-\ning to enhance ICD coding performance. Moreover\nunlike previous approaches, our approach does not\nrely on additional annotations or specialized pre-\ntraining. Instead, given the evidence of the zero-\nshot capabilities of general-purpose LLMs to ex-\npand medical acronyms, our approach provides a\nsimple yet effective data augmentation strategy to\nimprove ICD coding performance.\n6.2\nClinical acronyms disambiguation\nAccurately disambiguating clinical acronyms and\nabbreviations enhances automated clinical note\nprocessing which include medical information re-\ntrieval and analysis. Several studies focus on train-\ning deep learning models with large amounts of\nannotated data, including word-embeddings (Jaber\nand Martínez, 2021; Wu et al., 2015), convolutional\nneural networks (CNNs) (Skreta et al., 2021), fine-\ntuning transformer-based models such as BioBERT\n(Li et al., 2024) and BlueBERT (Hosseini et al.,\n2024), and fine-tuned encoder-decoder architec-\ntures like T5 (Rajkomar et al., 2022). To address\nthe scarcity of annotated data and the data-hungry\nnature of deep learning models, prior work has\nexplored various data augmentation strategies, in-\ncluding reverse substitution (Rajkomar et al., 2022;\nLiu et al., 2024; Skreta et al., 2021), UMLS-based\nsimilar concept retrieval (Skreta et al., 2021), in-\ntegration of clinical note metadata (Kugic et al.,\n2024), and the use of generative clinical models\n(Hosseini et al., 2024). The potential of LLMs\nin medical context understanding, coupled with\ntheir reduced reliance on large annotated datasets,\nhas driven research toward zero-shot and few-shot\nacronym disambiguation in clinical text requiring\nless training cost and effort. Kugic et al. (2024),\nLiu et al. (2024) and Hosseini et al. (2024) eval-\nuate various LLMs on the CASI dataset (Moon\net al., 2012), showing that LLM-based prompting\nachieves performance comparable to supervised\nmodels, even in zero-shot settings.\n7\nConclusion\nIn this paper, we introduce ACE-ICD, a system\nthat advances ICD coding performance by utilizing\nacronym expansion as an innovative data augmen-\ntation technique. Furthermore, we incorporate con-\nsistency training, a regularization strategy that en-\nforces alignment between original and augmented\ndocuments to enhance model predictions. Our ap-\nproach also outperforms studies which rely on ex-\nternal annotations or proprietary resources. Our\nextensive experiments reveal that the combination\nof LLM-based acronym expansion and consistency\ntraining elevates ICD coding accuracy, outperform-\ning existing methods and establishing new state-of-\nthe-art benchmarks across various settings.\n"}, {"page": 9, "text": "Limitations\nOur data augmentation approach relies on zero-\nshot prompting to disambiguate medical acronyms\nin clinical notes, making its effectiveness depen-\ndent on the performance of the selected LLMs. We\nchose Llama 3.1 70B as it was the best-performing\nopen-source model available at the time of our ex-\nperiments and aligned with our computational re-\nsources. More advanced LLMs or prompting tech-\nniques could potentially reduce translation errors\nand generate higher-quality augmented data.\nOur work uses KEPT as the base method, but we\nargue that acronym expansion as a data augmen-\ntation technique, combined with consistency train-\ning, can benefit other existing ICD coding systems.\nAdditional experiments are needed to thoroughly\nassess the effectiveness of our proposed strategy\nacross various models.\n8\nEthics Statement\nThis work uses the publicly available MIMIC-III\nclinical dataset, which contains de-identified pa-\ntient information in compliance with HIPAA stan-\ndards. Access to the dataset requires completion\nof a data use agreement and training in responsible\nresearch conduct. Acronym expansion was per-\nformed using open-source LLMs on a secure local\ncluster, and no patient data were transmitted to any\nthird-party services. Our method is intended to\nsupport clinical NLP research and is not designed\nfor direct clinical deployment without expert over-\nsight. We do not anticipate any ethical concerns\nassociated with this study.\nReferences\nTemitope Ibrahim Amosa, Lila Iznita Bt Izhar, Patrick\nSebastian, Idris B Ismail, Oladimeji Ibrahim, and\nShehu Lukman Ayinla. 2023. Clinical errors from\nacronym use in electronic health record: A review of\nnlp-based disambiguation techniques. IEEE Access,\n11:59297–59316.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nJoseph S Boyle, Antanas Kascenas, Pat Lok, Maria\nLiakata, and Alison Q O’Neil. 2023. Automated clin-\nical coding using off-the-shelf large language models.\nIn Deep Generative Models for Health Workshop\nNeurIPS 2023.\nHua Cheng, Rana Jafari, April Russell, Russell Klopfer,\nEdmond Lu, Benjamin Striner, and Matthew Gorm-\nley. 2023. MDACE: MIMIC documents annotated\nwith code evidence. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7534–\n7550, Toronto, Canada. Association for Computa-\ntional Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nMatúš Falis, Hang Dong, Alexandra Birch, and Beat-\nrice Alex. 2022. Horses to zebras: Ontology-guided\ndata augmentation and synthesis for ICD-9 coding.\nIn Proceedings of the 21st Workshop on Biomedi-\ncal Language Processing, pages 389–401, Dublin,\nIreland. Association for Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nGoncalo Gomes, Isabel Coutinho, and Bruno Martins.\n2024. Accurate and well-calibrated ICD code as-\nsignment through attention over diverse label embed-\ndings. In Proceedings of the 18th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages\n2302–2315, St. Julian’s, Malta. Association for Com-\nputational Linguistics.\nManda Hosseini, Mandana Hosseini, and Reza Javidan.\n2024. Leveraging large language models for clinical\nabbreviation disambiguation.\nJournal of medical\nsystems, 48(1):27.\nChao-Wei Huang, Shang-Chi Tsai, and Yun-Nung Chen.\n2022. PLM-ICD: Automatic ICD coding with pre-\ntrained language models. In Proceedings of the 4th\nClinical Natural Language Processing Workshop,\npages 10–20, Seattle, WA. Association for Computa-\ntional Linguistics.\nAreej Jaber and Paloma Martínez. 2021. Disambiguat-\ning clinical abbreviations using pre-trained word em-\nbeddings. In Healthinf, pages 501–508.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H\nLehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi,\nand Roger G Mark. 2016. Mimic-iii, a freely accessi-\nble critical care database. Scientific data, 3(1):1–9.\nPrajwal Kailas, Max Homilius, Rahul C. Deo, and\nCalum A. MacRae. 2023. Notecontrast: Contrastive\nlanguage-diagnostic pretraining for medical text. In\nProceedings of the 3rd Machine Learning for Health\nSymposium, volume 225 of Proceedings of Machine\nLearning Research, pages 201–216. PMLR.\n"}, {"page": 10, "text": "Amila Kugic, Stefan Schulz, and Markus Kreuzthaler.\n2024.\nDisambiguation of acronyms in clinical\nnarratives with large language models.\nJournal\nof the American Medical Informatics Association,\n31(9):2040–2046.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomedi-\ncal and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n146–157, Online. Association for Computational Lin-\nguistics.\nRumeng Li, Xun Wang, and Hong Yu. 2024. Explor-\ning llm multi-agents for icd coding. arXiv preprint\narXiv:2406.15363.\nYikuan Li, Ramsey M Wehbe, Faraz S Ahmad, Hanyin\nWang, and Yuan Luo. 2023. A comparative study\nof pretrained language models for long clinical text.\nJournal of the American Medical Informatics Associ-\nation, 30(2):340–347.\nYing Liu, Genevieve B Melton, and Rui Zhang. 2024.\nExploring large language models for acronym, sym-\nbol sense disambiguation, and semantic similarity\nand relatedness assessment. AMIA Summits on Trans-\nlational Science Proceedings, 2024:324.\nChang Lu, Chandan Reddy, Ping Wang, and Yue Ning.\n2023. Towards semi-structured automatic icd cod-\ning via tree-based contrastive learning. Advances in\nNeural Information Processing Systems, 36:68300–\n68315.\nJunyu Luo, Xiaochen Wang, Jiaqi Wang, Aofei Chang,\nYaqing Wang, and Fenglong Ma. 2024. CoRelation:\nBoosting automatic ICD coding through contextual-\nized code relation learning. In Proceedings of the\n2024 Joint International Conference on Computa-\ntional Linguistics, Language Resources and Eval-\nuation (LREC-COLING 2024), pages 3997–4007,\nTorino, Italia. ELRA and ICCL.\nSungrim Moon, Serguei Pakhomov, and Genevieve\nMelton. 2012. Clinical abbreviation sense inventory.\nJames Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng\nSun, and Jacob Eisenstein. 2018. Explainable predic-\ntion of medical codes from clinical text. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1101–1111, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThanh-Tung\nNguyen,\nViktor\nSchlegel,\nAbhinav\nKashyap, Stefan Winkler, Shao-Syuan Huang, Jie-\nJyun Liu, and Chih-Jen Lin. 2023a. Mimic-iv-icd: A\nnew benchmark for extreme multilabel classification.\narXiv preprint arXiv:2304.13998.\nThanh-Tung\nNguyen,\nViktor\nSchlegel,\nAbhinav\nRamesh Kashyap, and Stefan Winkler. 2023b. A\ntwo-stage decoder for efficient ICD coding. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2023, pages 4658–4665, Toronto, Canada.\nAssociation for Computational Linguistics.\nJong-Ku Park, Ki-Soon Kim, Tae-Yong Lee, Kang-Sook\nLee, Duk-Hee Lee, Sun-Hee Lee, Sun-Ha Jee, Il Suh,\nKwang-Wook Koh, So-Yeon Ryu, et al. 2000. The\naccuracy of icd codes for cerebrovascular diseases\nin medical insurance claims. Journal of Preventive\nMedicine and Public Health, 33(1):76–82.\nAdler Perotte, Rimma Pivovarov, Karthik Natarajan,\nNicole Weiskopf, Frank Wood, and Noémie Elhadad.\n2014. Diagnosis code assignment: models and eval-\nuation metrics. Journal of the American Medical\nInformatics Association, 21(2):231–237.\nAlvin Rajkomar, Eric Loreaux, Yuchen Liu, Jonas\nKemp, Benny Li, Ming-Jun Chen, Yi Zhang, Afroz\nMohiuddin, and Juraj Gottweis. 2022.\nDecipher-\ning clinical abbreviations with a privacy protecting\nmachine learning system. Nature Communications,\n13(1):7456.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nDinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru\nQu, and Weizhu Chen. 2020. A simple but tough-\nto-beat data augmentation approach for natural lan-\nguage understanding and generation. arXiv preprint\narXiv:2009.13818.\nMarta Skreta, Aryan Arbabi, Jixuan Wang, Erik\nDrysdale, Jacob Kelly, Devin Singh, and Michael\nBrudno. 2021. Automatically disambiguating med-\nical acronyms with ontology-aware deep learning.\nNature communications, 12(1):5319.\nAaron Sonabend, Winston Cai, Yuri Ahuja, Ashwin\nAnanthakrishnan, Zongqi Xia, Sheng Yu, and Chuan\nHong. 2020. Automated icd coding via unsupervised\nknowledge integration (unite). International journal\nof medical informatics, 139:104135.\nAli Soroush, Benjamin S Glicksberg, Eyal Zimlich-\nman, Yiftach Barash, Robert Freeman, Alexan-\nder W Charney, Girish N Nadkarni, and Eyal Klang.\n2024.\nLarge language models are poor medical\ncoders—benchmarking of medical code querying.\nNEJM AI, 1(5):AIdbp2300040.\nShang-Chi Tsai, Chao-Wei Huang, and Yun-Nung Chen.\n2021. Modeling diagnostic label correlation for au-\ntomatic ICD coding. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\n"}, {"page": 11, "text": "Language Technologies, pages 4043–4052, Online.\nAssociation for Computational Linguistics.\nThanh Vu, Dat Quoc Nguyen, and Anthony Nguyen.\n2020. A label attention model for icd coding from\nclinical text. In Proceedings of the Twenty-Ninth\nInternational Joint Conference on Artificial Intel-\nligence, IJCAI-20, pages 3335–3341. International\nJoint Conferences on Artificial Intelligence Organi-\nzation. Main track.\nXindi Wang, Robert Mercer, and Frank Rudzicz. 2024a.\nMulti-stage retrieve and re-rank model for automatic\nmedical coding recommendation. In Proceedings of\nthe 2024 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long\nPapers), pages 4881–4891, Mexico City, Mexico. As-\nsociation for Computational Linguistics.\nXindi Wang, Robert E. Mercer, and Frank Rudzicz.\n2024b. Auxiliary knowledge-induced learning for\nautomatic multi-label medical document classifica-\ntion. In Proceedings of the 2024 Joint International\nConference on Computational Linguistics, Language\nResources and Evaluation (LREC-COLING 2024),\npages 2006–2016, Torino, Italia. ELRA and ICCL.\nLijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei\nChen, Min Zhang, Tie-Yan Liu, et al. 2021. R-drop:\nRegularized dropout for neural networks. Advances\nin Neural Information Processing Systems, 34:10890–\n10905.\nYonghui Wu, Jun Xu, Yaoyun Zhang, and Hua Xu. 2015.\nClinical abbreviation disambiguation using neural\nword embeddings. In Proceedings of BioNLP 15,\npages 171–176.\nXiancheng Xie, Yun Xiong, Philip S. Yu, and Yangyong\nZhu. 2019. Ehr coding with multi-scale feature at-\ntention and structured knowledge graph propagation.\nIn Proceedings of the 28th ACM International Con-\nference on Information and Knowledge Management,\nCIKM ’19, page 649–658, New York, NY, USA. As-\nsociation for Computing Machinery.\nZhichao Yang, Sanjit Singh Batra, Joel Stremmel, and\nEran Halperin. 2023a.\nSurpassing gpt-4 medical\ncoding with a two-stage approach. arXiv preprint\narXiv:2311.13735.\nZhichao Yang, Sunjae Kwon, Zonghai Yao, and Hong\nYu. 2023b. Multi-label few-shot icd coding as au-\ntoregressive generation with prompt. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 37, pages 5366–5374.\nZhichao Yang, Shufan Wang, Bhanu Pratap Singh\nRawat, Avijit Mitra, and Hong Yu. 2022. Knowledge\ninjected prompt based fine-tuning for multi-label few-\nshot icd coding. Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing.\nConference on Empirical Methods in Natural Lan-\nguage Processing, 2022:1767–1781.\nMethods\nDev F1\nTest F1\nMacro Micro Macro Micro\nACE-ICD (PMM3)\nw/ single threshold\n10.5\n62.9\n13.2\n62.7\nw/ code-specific thresholds\n14.9\n66.0\n14.3\n61.5\nDiscNet+RE\n-\n-\n14.0\n58.8\nTable 8: Results of different threshold optimization\napproaches on MIMIC-III-full dataset.\nZheng Yuan, Chuanqi Tan, and Songfang Huang. 2022.\nCode synonyms do matter:\nMultiple synonyms\nmatching network for automatic ICD coding.\nIn\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 808–814, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nShurui Zhang, Bozheng Zhang, Fuxin Zhang, Bo Sang,\nand Wanchun Yang. 2022.\nAutomatic ICD cod-\ning exploiting discourse structure and reconciled\ncode embeddings. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\npages 2883–2891, Gyeongju, Republic of Korea. In-\nternational Committee on Computational Linguistics.\nA\nAppendix\nA.1\nThreshold Optimization\nAll results presented in the main paper are obtained\nusing a single threshold for all ICD codes, opti-\nmized for the micro F1. An alternative approach\nis to determine a specific threshold for each ICD\ncode. This method effectively lowers the threshold\nfor ICD codes with less training data compared\nto a single-threshold approach, leading to an im-\nprovement in the macro F1 and surpassing (Zhang\net al., 2022) on the MIMIC-III-full dataset. How-\never, this strategy tends to overfit the development\nset, increasing the performance gap between the\ndevelopment and test sets.\nA.2\nMore implementation details\nPretrained models.\nWe initialize our model\nwith two pretrained variants provided by (Yang\net al., 2022). KEPTLongformer is based on Clin-\nical Longformer (Li et al., 2023), while KEPT-\nPMM3 builds upon RoBERTa-base-PM-M3-Voc-\ndistill (Lewis et al., 2020), a distilled variant of\nRoBERTa-large pre-trained on PubMed, PMC, and\nMIMIC-III corpus. These models adapt the Long-\nformer sparse attention mechanism (Beltagy et al.,\n2020) to handle longer sequences and incorporate\nmedical knowledge through contrastive learning.\nTraining Details.\nTable 9 summarizes the\ntraining hyperparameters for the three MIMIC-III\n"}, {"page": 12, "text": "datasets. Training ACE-ICD (PMM3) for 8 epochs\non a single NVIDIA H100 GPU takes approxi-\nmately 25 minutes for MIMIC-III-rare, 7 hours for\nMIMIC-III-50, and 5 days for MIMIC-III-full.\nInference.\nEvaluating 1,573 examples\nfrom the development set of MIMIC-III-50 takes\napproximately 1 minute and 36 seconds on a sin-\ngle H100 GPU, achieving a throughput of around\n16 examples per second. For MIMIC-III-full, the\nmodel requires six runs to re-rank 300 candidates,\nresulting in a throughput of 2.67 examples per sec-\nond.\n"}, {"page": 13, "text": "Configuration\nMIMIC-III-50\nMIMIC-III-rare50\nMIMIC-III-full\nglobal attention on\ncode descriptions + masks\ncode descriptions + masks\ncode descriptions + masks\nglobal attention stride\n1\n1\n3\nsynonyms in prompt\nyes\nyes\nno\nmax length\n8192\n8192\n8192\nnum epochs\n8\n8\n4\nbatch size\n1\n1\n1\ngradient accumulation steps\n1\n1\n6\nlearning rate\n1.5e-5\n1.5e-5\n1.5e-5\nlearning rate scheduler\ncosine\ncosine\ncosine\nmax grad norm\n1\n1\n1\nwarm up ratio\n0\n0\n0.1\nAdamW epsilon\n1e-6\n1e-6\n1e-7\nAdamW betas\n(0.9, 0.999)\n(0.9, 0.999)\n(0.9, 0.999)\nweight decay\n0.01\n0.01\n1e-4\nTable 9: Training hyperparameters used in our experiments for the three ICD coding tasks on the MIMIC-III dataset.\n"}]}