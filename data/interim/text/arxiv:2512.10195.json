{"doc_id": "arxiv:2512.10195", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.10195.pdf", "meta": {"doc_id": "arxiv:2512.10195", "source": "arxiv", "arxiv_id": "2512.10195", "title": "AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding", "authors": ["Gyutaek Oh", "Sangjoon Park", "Byung-Hoon Kim"], "published": "2025-12-11T01:25:36Z", "updated": "2025-12-11T01:25:36Z", "summary": "Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.10195v1", "url_pdf": "https://arxiv.org/pdf/2512.10195.pdf", "meta_path": "data/raw/arxiv/meta/2512.10195.json", "sha256": "f68f3fb59ad89048c277079facd406b4804df53cc62ebcca5d9212bed14973a6", "status": "ok", "fetched_at": "2026-02-18T02:24:26.506353+00:00"}, "pages": [{"page": 1, "text": "AutoMedic: An Automated Evaluation Framework for Clinical\nConversational Agents with Medical Dataset Grounding\nGyutaek Oh1,2, Sangjoon Park*,2,3, Byung-Hoon Kim*,1,2,4,5\n1Department of Biomedical Systems Informatics, Yonsei University College of Medicine,\n2Yonsei Institute for Digital Health, Yonsei University\n3Department of Radiation Oncology, Yonsei University College of Medicine,\n4Department of Psychiatry, Yonsei University College of Medicine,\n5Institute of Behavioral Sciences in Medicine, Yonsei University College of Medicine\nCorrespondence: depecher@yuhs.ac, egyptdj@yonsei.ac.kr\nAbstract\nEvaluating large language models (LLMs) has\nrecently emerged as a critical issue for safe and\ntrustworthy application of LLMs in the medi-\ncal domain. Although a variety of static medi-\ncal question-answering (QA) benchmarks have\nbeen proposed, many aspects remain underex-\nplored, such as the effectiveness of LLMs in\ngenerating responses in dynamic, interactive\nclinical multi-turn conversation situations and\nthe identification of multi-faceted evaluation\nstrategies beyond simple accuracy. However,\nformally evaluating a dynamic, interactive clin-\nical situation is hindered by its vast combinato-\nrial space of possible patient states and interac-\ntion trajectories, making it difficult to standard-\nize and quantitatively measure such scenarios.\nHere, we introduce AutoMedic, a multi-agent\nsimulation framework that enables automated\nevaluation of LLMs as clinical conversational\nagents. AutoMedic transforms off-the-shelf\nstatic QA datasets into virtual patient profiles,\nenabling realistic and clinically grounded multi-\nturn clinical dialogues between LLM agents.\nThe performance of various clinical conver-\nsational agents is then assessed based on our\nCARE metric, which provides a multi-faceted\nevaluation standard of clinical conversational\naccuracy, efficiency/strategy, empathy, and ro-\nbustness. Our findings, validated by human ex-\nperts, demonstrate the validity of AutoMedic as\nan automated evaluation framework for clinical\nconversational agents, offering practical guide-\nlines for the effective development of LLMs in\nconversational medical applications.\n1\nIntroduction\nThe rapid advancement of large language models\n(LLMs) has spurred the development of sophisti-\ncated foundation models across both proprietary\nand open-source landscapes (Achiam et al., 2023;\nHurst et al., 2024; Jaech et al., 2024; Grattafiori\net al., 2024; Guo et al., 2025; Comanici et al., 2025;\nYang et al., 2025; OpenAI, 2025). This has also\nled to a significant interest in their medical applica-\ntions, resulting in a variety of specialized models\nfine-tuned on clinical data (Chen et al., 2023, 2024;\nChristophe et al., 2024; Ankit Pal, 2024; Zhang\net al., 2024; Jiang et al., 2025; Sellergren et al.,\n2025). Notably, researchers have found from exper-\niments that the knowledge level of these LLMs on\nstandard medical question-answering (QA) bench-\nmarks has become comparable to that of human\nexperts (Singhal et al., 2025).\nHowever, a crucial limitation of evaluations\nbased on existing medical QA benchmarks is their\nfailure to assess the dynamic and interactive na-\nture of real-world clinical encounters. There are\ntwo primary shortcomings. First, the evaluation\nprocess is inherently static. Unlike a real clinical\ncare scenario where the clinician, who LLMs are\nexpected to take the role in, must interact with the\npatient to elicit information, these benchmarks pro-\nvide no measure of conversational and diagnostic\ninquiry skills. Effective patient-doctor communi-\ncation is fundamental to clinical care; it is the pri-\nmary method for gathering nuanced patient history,\nbuilding trust, and ensuring patient adherence to\ntreatment plans. An agent’s ability to communicate\nwith empathy and clarity is not just a feature but a\ncore requirement for safe and effective deployment.\nFurthermore, given the susceptibility of LLMs to\nuser-driven misinformation or leading inputs (Zi-\naei and Schmidgall, 2023; Lim et al., 2025), the\nability to maintain robust and accurate dialogue\nis critical for safe clinical deployment. Second,\nthe queries in these benchmarks are typically self-\ncontained, providing all the necessary information\nupfront. In actual clinical practice, however, criti-\ncal information must be actively gathered through\npatient dialogue and diagnostic tests, which is a\nvital capability overlooked by current evaluation\nmethods.\nTo address these limitations, recent studies have\nmade attempts in evaluating LLMs as clinical con-\n1\narXiv:2512.10195v1  [cs.CL]  11 Dec 2025\n"}, {"page": 2, "text": "Figure 1: Overview of the proposed AutoMedic framework. AutoMedic framework automatically converts a\nmedical QA dataset into a virtual patient scenario, facilitates a multi-agent clinical simulation, and evaluates the\nLLM agent’s performance based on our proposed metrics.\nversational agents (Schmidgall et al., 2024; Alman-\nsoori et al., 2025; Arora et al., 2025; Lee et al.,\n2025; Nori et al., 2025; Tu et al., 2025). These\nframeworks have demonstrated the feasibility of\nevaluating LLMs as clinical conversational agents\nby introducing novel datasets and simulating clin-\nical scenarios. However, these approaches still\npresent challenges for full automation, as their\nworkflows 1) require specially designed datasets, 2)\nnecessitate substantial human labor or rely on non-\nrealistic clinical simulations, and 3) heavily rely\non human evaluators for key qualitative metrics.\nMoreover, these methods have notable drawbacks\nin their extensiveness, often limited to diagnostic\nscenarios, and lack clear quantitative evaluation\nmetrics.\nTo address these shortcomings, this paper intro-\nduces AutoMedic, a fully automated framework for\nevaluating LLMs as clinical conversational agents\nin clinical care scenarios, with the overall scheme\nillustrated in Fig. 1. Our key contributions are:\n• An automatic method for converting static\nmedical QA datasets into interactive vir-\ntual patient profiles. This approach trans-\nforms any standard off-the-shelf QA bench-\nmark into a conversational evaluation suite, re-\nmoving the need for specialized, hand-crafted\ndatasets. Moreover, our framework includes a\nfiltering mechanism that assesses the suitabil-\nity of source QA items, thereby also enabling\nan automatic analysis of which datasets are\nmost appropriate for conversion into realistic\nconversational simulations.\n• A multi-agent simulation framework that\nmodels a realistic clinical dialogue. In this\nframework, the LLM-based clinical conversa-\ntional agent must actively gather information\nby conversing with a virtual patient and re-\nquesting test results from a clinical staff agent\nbefore making a final judgment. The entire\nevaluation process is fully automated, requir-\ning no human intervention.\n• A novel, multi-faceted metric for quantita-\ntive evaluation. We propose the CARE metric\nto assess performance across four key rubrics:\naccuracy, conversational efficiency and strat-\negy, empathy, and robustness. By quantify-\ning these crucial aspects, our metric offers\na comprehensive guideline for selecting and\ndeveloping LLMs for real-world clinical de-\nployment.\nThe remainder of this paper is organized as fol-\nlows. Section 2 reviews related works on medical\nLLMs and conversational agent evaluation. Sec-\ntions 3, 4 detail our proposed framework and exper-\nimental setup. Section 5 and Section 6 present the\nexperimental results and discussion, and Section 7\nconcludes the paper.\n2\nRelated Works\n2.1\nLLMs for the Medical Domain\nLeading LLMs have consistently demonstrated\nhigh performance on a range of medical tasks\nand benchmarks.\nOpenAI’s GPT-4, for exam-\nple, surpassed the passing score of the United\nStates Medical Licensing Examination (USMLE)\nby over 20 points (Nori et al., 2023). Its successor,\nGPT-5 (OpenAI, 2025), continued this trend by\n2\n"}, {"page": 3, "text": "achieving the highest score to date on the Health-\nBench benchmark (Arora et al., 2025).\nSimi-\nlarly, DeepSeek-R1 (Guo et al., 2025) has shown\nstrong performance in clinical decision-making\ntasks (Sandmann et al., 2025) and has achieved the\ntop win-rate on several medical benchmarks, out-\nperforming other prominent proprietary and open-\nsource LLMs (Bedi et al., 2025).\nBeyond the aforementioned general-purpose\nLLMs, a significant amount of research has fo-\ncused on developing LLMs specifically tailored for\nthe medical domain. Models such as HuatuoGPT-\no1 (Chen et al., 2024) and m1 (Huang et al.,\n2025) are fine-tuned on medical problem sets to en-\nhance their clinical reasoning abilities. In parallel,\nGoogle has introduced a series of powerful multi-\nmodal medical LLMs, including Med-PaLM M (Tu\net al., 2024), Med-Gemini (Saab et al., 2024), and\nMedGemma (Sellergren et al., 2025). These mod-\nels have demonstrated exceptional performance\nacross a diverse range of medical benchmarks.\nHowever, since most current evaluations pri-\nmarily assess the knowledge of LLMs via simple\nquestion-answering tasks, concerns arise regarding\ntheir ecological validity. Specifically, these meth-\nods may fail to adequately gauge a model’s utility\nin real-world clinical settings, which inherently\nnecessitate explorative information gathering for\naccurate diagnosis and treatment planning (Hager\net al., 2024; Williams et al., 2024).\n2.2\nEvaluation of Clinical Conversational\nAgents\nSeveral pioneering efforts have sought to evaluate\nLLMs as clinical conversational agents by creat-\ning specialized benchmarks and systems. For in-\nstance, Google’s Articulate Medical Intelligence\nExplorer (AMIE), a system for diagnostic dialogue,\nwas evaluated through interactions with human ac-\ntors playing the role of simulated patients (Tu et al.,\n2025). Similarly, OpenAI developed HealthBench,\na benchmark comprising 5,000 realistic health con-\nversations created in collaboration with physicians,\ncomplete with detailed scoring rubrics (Arora et al.,\n2025). More recently, Microsoft introduced the\nMAI Diagnostic Orchestrator (MAI-DxO), a sys-\ntem where multiple LLM agents collaboratively\ndebate to make an accurate diagnosis. To assess it,\nthey also released the Sequential Diagnosis Bench-\nmark (SDBench), derived from complex clinico-\npathological cases from the NEJM (Nori et al.,\n2025). A common limitation of these approaches\nis their dependence on bespoke datasets. The cre-\nation of such resources is a time-consuming and\nlabor-intensive process, which helps explain why\nthe number of conversational benchmarks remains\nlimited compared to the abundance of static medi-\ncal QA datasets. Moreover, these datasets are often\nlimited in conversational depth, lacking a signifi-\ncant number of multi-turn exchanges. Instead of\nserving as active agents that drive the dialogue,\nmodels in these settings are relegated to passive\nroles, evaluating fixed conversation histories rather\nthan engaging in actual multi-turn interactions.\nTo overcome these limitations, several studies\nhave specifically focused on evaluating the inter-\nactive capabilities of LLM agents in patient dia-\nlogue. A notable example is AgentClinic, a mul-\ntimodal agent benchmark for evaluating LLMs in\nsimulated clinical environments (Schmidgall et al.,\n2024). The AgentClinic framework generates vir-\ntual patient and measurement agents from medi-\ncal QA data. A doctor agent must then interact\nwith these simulated agents to gather the necessary\ninformation for a medical decision. The authors\nuse this simulation to compare the diagnostic ac-\ncuracy of LLM agents across various biases and\nlanguages, supplementing the results with human\nevaluations of the conversation quality. Similarly,\nMedAgentSim (Almansoori et al., 2025) simulates\ndoctor-patient interactions but distinguishes itself\nby utilizing multiple doctor agents to collabora-\ntively derive the final clinical decision. Despite\ntheir comprehensive approach, these approaches\nhas two key limitations relevant to our work. First,\nit does not provide a method for assessing whether\nleveraging an existing medical QA benchmark is\nsuitable for conversion into a realistic conversa-\ntional simulation.\nSecond, it lacks a quantita-\ntive framework for automatically evaluating an\nagent’s performance across multiple conversational\naspects, relying instead on subjective manual hu-\nman assessment or solely on the accuracy of the\nQA task.\n3\nAutoMedic Framework\nThe AutoMedic framework, illustrated in Fig. 1, is\ncomposed of three primary stages: 1) patient profile\ngeneration, 2) multi-agent conversation simulation,\nand 3) automated evaluation. This entire process\nis orchestrated by four specialized LLM agents: a\nprofile generator, a doctor, a patient, and a clinical\nstaff. The following sections provide the specific\n3\n"}, {"page": 4, "text": "Figure 2: The Automedic framework, detailing (a) the automatic generation of a virtual patient profile from a\nmedical QA query, and (b) the subsequent multi-agent conversation simulation.\nroles of these agents and a detailed description of\neach stage.\n3.1\nAgents\nThe AutoMedic framework operates using four dis-\ntinct LLM agents, each with a specialized role. The\nprimary agent is the doctor agent, which emulates\na physician and serves as the sole subject of our\nevaluation. The other three agents, the profile gen-\nerator, patient, and clinical staff, act in supporting\nroles to create the environment for this evaluation.\nThe process begins with the profile generator,\nwhich performs two sequential tasks. First, it filters\na set of medical QA queries to select items suitable\nfor simulation. Second, it transforms each valid\nquery into a structured virtual patient profile.\nThis profile is then used by the patient agent,\nwhich instantiates the patient’s persona. It dynam-\nically responds to inquiries about symptoms and\nmedical history, and can adapt to a caregiver role\nin scenarios where a patient cannot self-represent\n(e.g., due to age or cognitive impairment).\nTo simulate the broader clinical environment, the\nclinical staff agent acts as a gatekeeper for technical\ndata. When the doctor agent requests a test, this\nagent provides the corresponding results from the\nprofile. To ensure factual grounding, it only returns\npre-existing information and will state that a result\nis unavailable if it is not in the profile, preventing\ndata hallucination.\nWithin this environment, the doctor agent’s ob-\njective is to solve the medical problem by strategi-\ncally gathering information from both the patient\nand clinical staff agents. For baseline comparison,\nthis agent also directly answers the original static\nQA query without any interaction.\n3.2\nPatient Profile Generation\nThe first stage of the AutoMedic framework is pa-\ntient profile generation (Fig. 2(a)). This process\n4\n"}, {"page": 5, "text": "transforms a static medical QA query into a struc-\ntured virtual patient profile, which then serves as\nthe foundation for the subsequent conversation sim-\nulation. The process begins when the profile gener-\nator agent receives a medical QA query, comprising\na medical context, a question, several options, and\na correct answer, and first assesses its suitability.\nAs our framework is designed for patient-specific\ndecision-making scenarios, the following types of\nqueries are automatically filtered out and excluded\nfrom the dataset by the profile generator agent:\n• Queries describing research participants or\nexperimental settings rather than clinical care.\n• Those focusing on abstract concepts (e.g.,\npathophysiology, molecular mechanisms) in-\nstead of a specific patient case.\n• General knowledge or fact-recall questions\n(e.g., definitions, classifications).\n• Image-dependent queries that do not describe\nthe visual findings in the text.\n• Queries where critical patient details (e.g.,\nage, chief complaint) are only present in the\nanswer choices.\nOnce a query is determined to be suitable, the\nprofile generator agent proceeds to create the vir-\ntual patient profile. It extracts relevant information\nfrom the medical context and organizes it into three\ndistinct categories:\n• Demographics: Basic patient data such as\nage and sex.\n• Basic Information: Core clinical details in-\ncluding the chief complaint, duration of symp-\ntoms, family and past medical history, and\nrelevant lifestyle factors (e.g., smoking, alco-\nhol use).\n• Optional Information: Data that may not\nbe present in all cases, such as vital signs,\nlaboratory tests, physical examinations, and\nimaging results.\nTo enhance the realism of the simulation, the pro-\nfile generator is instructed to impute any missing\nbasic information items with plausible, randomly\ngenerated values, ensuring these additions do not\nalter the correct answer to the original query. Con-\nversely, optional information is never generated if\nit is absent. This is a crucial constraint, as many\nqueries test the doctor agent’s ability to determine\nwhich tests are necessary. Generating these results\nupfront would interfere with the evaluation. Finally,\nthe agent isolates the specific medical question and\nits corresponding response options from the orig-\ninal query, excluding the clinical vignette. This\nextracted content is presented to the doctor agent\nupon the conclusion of the conversation simulation.\nOnce the patient profile is generated, the infor-\nmation is selectively distributed among the agents\naccording to their role, to simulate realistic knowl-\nedge boundaries (Fig.2(b)). The doctor agent re-\nceives only the demographics as the baseline in-\nformation of the given patient. The patient agent\nis provided with both demographics and basic in-\nformation, enabling it to accurately represent the\npatient’s history and symptoms to answer the query\nfrom the doctor agent. Lastly, the clinical staff\nagent receives optional information (e.g., exami-\nnation results) that it provides upon request from\nthe doctor agent. By isolating technical data until\nspecifically requested, this design maintains the\nsimulation’s integrity and ensures the proper evalu-\nation of the doctor agent’s information-gathering\nskills.\n3.3\nMulti-Agent Conversation Simulation\nThe multi-agent conversation simulation begins\nafter the patient profile has been generated, as\nillustrated in Fig. 2(b).\nThe doctor agent ini-\ntiates and directs the entire dialogue using spe-\ncific tags.\nQuestions for the patient agent are\nwrapped in <patient></patient> tags, while re-\nquests for the clinical staff agent are framed within\n<clinical></clinical> tags. The patient and\nclinical staff agents automatically respond to any\ntext directed to them within these respective tags.\nThis interactive process continues until one of\ntwo termination conditions is met: either the doctor\nagent concludes the dialogue by issuing an </end>\ntag, or the conversation reaches a predefined max-\nimum length. We define a single turn as a query\nfrom the doctor agent and the corresponding re-\nsponse from another agent. For our experiments,\nwe set the maximum number of turns to 20, which\nis sufficient if the doctor agent gathers relevant\ninformation efficiently.\nUpon the conversation’s conclusion, the doctor\nagent is presented with the final medical question\nthat was extracted from the original query. The doc-\ntor agent then formulates an answer based solely\n5\n"}, {"page": 6, "text": "on the information it gathered during the simulated\ndialogue. To establish a performance baseline, we\nalso collect responses in a standard QA setting. As\ndepicted at the top of Fig. 2, the doctor agent is pre-\nsented with the complete original medical query,\ncontaining full patient context upfront. In this static\nconfiguration, the agent must generate an answer\nwithout any conversational interaction, mirroring\ntraditional QA tasks. Both the conversational and\nstatic QA responses are subsequently compared\nduring the evaluation phase.\n3.4\nAutomated Evaluation\nThe final stage of the AutoMedic framework is the\nautomated evaluation of the doctor agent. This\nevaluation is based on the entire transcript of the\nmulti-agent conversation as well as the final answer\nprovided. To conduct this assessment automatically\nand without human intervention, we introduce the\nCARE (Conversation efficiency and strategy, Ac-\ncuracy, Robustness, and Empathy) metric. This\nmetric quantitatively assesses the agent’s perfor-\nmance across the following four distinct aspects:\n1. Accuracy: The primary goal of this score, SACC,\nis to evaluate the diagnostic accuracy of the doctor\nagent, while also accounting for any performance\ndegradation that occurs when moving from a static\nto a conversational setting.\nFirst, we independently calculate the accuracy\nfor the conversational setting (AccConv) and the\nstatic QA setting (AccQA):\nAccConv = nConv\nN\n,\nAccQA = nQA\nN ,\n(1)\nwhere N is the total number of queries, and nConv\nand nQA are the number of correct answers in the\nconversational and static QA settings, respectively.\nThe final accuracy score, SACC, is then defined\nas:\nSACC = AccConv × AccConv\nACCQA\n.\n(2)\nThis formula incorporates two components. The\nfirst term, AccConv, directly measures the agent’s\nperformance in the interactive simulation. The\nsecond term, AccConv/ACCQA, acts as a penalty\nfactor. It quantifies the drop-off in performance\nfrom the ideal-information (static QA) setting to\nthe more challenging conversational setting. An\nagent that maintains high accuracy in the conver-\nsation relative to its baseline QA performance will\nscore higher. This ensures that the metric rewards\nnot just correct answers, but also robust conversa-\ntional information-gathering skills.\n2. Conversational Efficiency & Strategy: This\nscore, SCES, evaluates the doctor agent’s conver-\nsational efficiency and its adherence to a natural,\nstrategic dialogue flow. In real clinical encoun-\nters, physicians typically ask one or two focused\nquestions at a time, iteratively building informa-\ntion based on patient responses. Conversely, some\nLLM agents may adopt an unnatural “checklist\"\napproach, asking numerous questions simultane-\nously to gather information quickly. While this\nmight appear efficient in terms of raw information\nacquisition, it is not representative of an effective\nclinical strategy.\nTo quantify this, we calculate the average num-\nber of words per turn (wturn) used by the doctor\nagent. An excessively high wturn indicates a less\nnatural, multi-question approach within a single\nturn. For each sample i, the conversational ef-\nficiency and strategy score, si\nCES, is assigned as\n1/wturn if the doctor agent correctly answers the\nfinal question, and 0 if the answer is incorrect. This\nties conversational style directly to diagnostic suc-\ncess.\nThe final SCES score is then computed as the\nscaled average across all N samples:\nSCES = 1\nN\nN\nX\nn=1\nsi\nCES × 100.\n(3)\nThe scaling factor of 100 is applied for clarity and\nease of interpretation.\n3. Empathy: The third score, SEMP, assesses the\nempathetic quality of the doctor agent’s communi-\ncation. Since LLMs can sometimes interact in a\nrobotic or emotionally detached manner, evaluat-\ning their ability to convey empathy is crucial for\npatient-centric applications.\nTo quantify this, we use the patient agent as a\nproxy evaluator. After each conversation concludes,\nthe patient agent is prompted to rate the doctor\nagent’s empathy on a 5-point scale (where 1 is low\nand 5 is high). The final empathy score, SEmp, is\nthe average of these ratings, normalized across all\nN queries:\nSEMP =\n1\n5N\nN\nX\nn=1\nsi\nEMP,\n(4)\nwhere si\nEMP is the empathy score for the ith sample.\n4. Robustness: The final score, SROB, evaluates\nthe conversational robustness of the doctor agent.\n6\n"}, {"page": 7, "text": "A robust agent must consistently adhere to its des-\nignated role and interaction protocols. Deviations,\nsuch as misdirecting questions or breaking charac-\nter, can compromise the simulation’s integrity and\nlead the conversation astray.\nWe quantify robustness by identifying and count-\ning specific failure cases during the simulation. A\ndialogue is considered a failure if any of the follow-\ning occur:\n• Role-Breaking:\nThe doctor agent self-\nsimulates the entire interaction, playing the\nroles of other agents instead of interacting\nwith them.\n• Abrupt Termination: The conversation ends\nprematurely (defined as ≤3 turns) due to\nthe doctor agent’s failure to use the required\n<patient> or <clinical> tags, or due to an\nimproperly placed </end> tag.\n• Invalid Answer: After a seemingly normal\nconversation, the doctor agent fails to provide\na valid answer to the final question (e.g., by\nproviding an answer not listed in the options).\nThe robustness score, SROB, is then calculated\nas the proportion of successful conversations:\nSROB = 1 −nFail\nN\n(5)\nwhere nFail is the total count of the failure cases\ndescribed above.\n4\nMethods\n4.1\nDatasets\nTo evaluate the performance of each LLM as a\nclinical conversational agent using the AutoMedic\nframework, we utilized six distinct medical QA\ndatasets.\nMedBullets: This dataset (Chen et al., 2025)\ncomprises 308 multiple-choice questions formatted\nin the style of the USMLE, each with five answer\noptions.\nMedQA: Another USMLE-style medical QA\nbenchmark, MedQA (Jin et al., 2021) contains\n1,273 multiple-choice questions, each presenting\nfive answer options.\nMedXpertQA: MedXpertQA (Zuo et al., 2025)\nis a medical QA benchmark featuring both textual\nand multimodal questions. Our evaluation focused\nexclusively on its text-based subset, which consists\nof 2,450 questions, each with ten answer choices.\nTable 1: List of LLMs included in our experiments\nName\nType\nDomain\nSize\nLlama 3\nOpen-Source\nGeneral\n70B\nQwen3\n32B\nDeepSeek-R1\n70B\ngpt-oss\n120B\nMed42-v2\nBiomedical\n70B\nOpenBioLLM\n70B\nHuatuoGPT-o1\n72B\nGPT-4o\nProprietary\nGeneral\n-\nClaude Sonnet 4\n-\nMedMCQA: MedMCQA (Pal et al., 2022) is a\nmultiple-choice question answering dataset derived\nfrom real-world medical entrance exams. For our\nexperiments, we used its validation split, which\nprovides the correct answers. After filtering to\ninclude only single-choice questions, our subset\ncontained 2,816 questions, each with four answer\noptions.\nHEAD-QA:\nThis\nmulti-choice\nhealthcare\ndataset (Vilares and Gómez-Rodríguez, 2019)\nincludes medical questions in both Spanish and\nEnglish. We specifically used the test split of its\nEnglish subset, which provides 2,742 questions,\neach with four answer options.\nMMLU-Pro: MMLU-Pro (Wang et al., 2024)\nis a multi-task understanding dataset designed for\nrigorous LLM benchmarking. From this dataset,\nwe extracted questions pertaining only to the health\nand biology categories, totalling 1,535 questions\nwith three to ten answer options each.\n4.2\nModels\nFor the doctor agent, we evaluate the performance\nof 11 distinct LLMs, which are detailed in TA-\nBLE 1. These models are grouped into three cate-\ngories.\nOpen-Source General LLMs: We selected a\nrange of general-purpose open-source models. We\nutilized the instruction-tuned versions of Llama\n3 (Grattafiori et al., 2024) and Qwen3 (Yang et al.,\n2025). As a representative reasoning model, we\nincluded the 70B distilled version of DeepSeek-\nR1 (Guo et al., 2025). We also used gpt-oss (Agar-\nwal et al., 2025), another open-source reasoning\nmodel from OpenAI, in its 120B parameter version.\nOpen Source Biomedical LLMs:\nTo as-\nsess\ndomain-specific\nmodels,\nwe\nemployed\ntwo LLMs designed for the biomedical field:\nMed42-v2 (Christophe et al., 2024) and OpenBi-\noLLM (Ankit Pal, 2024), both with 70B parame-\nters. We also included HuatuoGPT-o1 (Chen et al.,\n7\n"}, {"page": 8, "text": "2024), a 72B parameter model specifically tailored\nfor medical reasoning tasks.\nProprietary General LLMs: Finally, we evalu-\nated two high-performing proprietary models: GPT-\n4o (Hurst et al., 2024) and Claude Sonnet 4 (An-\nthropic, 2025).\nFor the other agents (profile generator, patient,\nand clinical staff), we employed GPT-4o. Although\nother state-of-the-art LLMs could be used for these\nroles and their impact could be analyzed, the pri-\nmary aim of this study is to evaluate the perfor-\nmance of the doctor agent. Therefore, GPT-4o was\nused for all supporting agents to ensure a stable\nand consistent conversation simulation.\n4.3\nHuman Evaluation\nTo validate the accuracy and effectiveness of our\nframework, we conducted three distinct human ex-\npert evaluations with the participation of four li-\ncensed medical professionals. These evaluations\nvalidate the clinical validity of the AutoMedic\nframework and the CARE metric by demonstrat-\ning their alignment with the judgments of medical\nexperts. The institutional review board approved\nthese human evaluation studies (IRB No. 4-2025-\n0982).\nThe first study assessed the accuracy of the pro-\nfile generator’s filtering mechanism. We randomly\nsampled 24 medical queries from the six datasets,\ncomprising 12 queries classified as \"appropriate\"\nand 12 as \"inappropriate\" by the profile genera-\ntor agent. The human experts then independently\njudged the suitability of each sample for patient\nprofile generation. We measured inter-rater reliabil-\nity among the experts using Fleiss’ Kappa and the\nagreement between our agent’s classification and\nthe experts’ majority vote using Cohen’s Kappa.\nThe second study evaluated the quality of the\ngenerated patient profiles. Experts were asked to\nrate 21 randomly sampled profiles on a 4-point\nscale. The evaluation criteria included faithfulness\nto the original medical query, completeness of in-\nformation, and the clinical plausibility of any im-\nputed data. To measure inter-rater reliability, we\ncalculated both the percent agreement and Gwet’s\nAC2 coefficient, a metric robust to chance agree-\nment. We then determined the average quality\nscore across all profiles.\nThe final study aimed to correlate our automated\nCARE metric with human judgment. We generated\n30 simulation results (15 each for Llama 3-70B and\nClaude Sonnet 4 as the doctor agent on the MedQA\nTable 2: Human Expert Evaluation Results for Patient\nProfile Generation.\nFiltering Accuracy of the Profile Generator\nFleiss’ Kappa\n0.7630\nCohen’s Kappa\n0.8197\nQuality of Generated Patient Profiles\nPercent Agreement\n0.9140\nGwet’s AC2\n0.8313\nAverage Score\n3.464\nNotes: For the filtering accuracy assessment, Fleiss’ Kappa\nquantifies inter-rater agreement among human experts,\nwhile Cohen’s Kappa measures agreement between the\nexperts’ majority vote and the profile generator. For the\nquality evaluation of generated patient profiles, the per-\ncent agreement and Gwet’s AC2 indicate human expert\nreliability, and the “Average Score” represents the mean\nexpert rating across all samples (maximum possible score:\n4 points).\ndataset). In a blinded setup, the human experts\nrated each result on a 3-point scale across the four\nCARE dimensions: accuracy, conversational effi-\nciency and strategy, empathy, and robustness. We\nmeasured inter-rater reliability using both percent\nagreement and Gwet’s AC2 coefficient. Finally, we\ncompared the trend of the experts’ average scores\nagainst the trend of the calculated CARE metric\nscores for each model.\n5\nResults\n5.1\nHuman Evaluation of Patient Profile\nGeneration.\nTo justify the patient profile generation process,\nwe conducted two human expert evaluations. The\nfirst specifically assessed the profile generator’s\nfiltering accuracy. Human experts demonstrated\nsubstantial agreement on the suitability of queries\nfor profile generation, indicated by a Fleiss’ Kappa\nof 0.7630 (TABLE 2). This high inter-rater relia-\nbility confirms that our filtering criteria are well-\ndefined and consistently applied. Furthermore, the\nagreement between the profile generator’s classi-\nfications and the human experts’ majority vote\nwas almost perfect, yielding a Cohen’s Kappa of\n0.8197 (excluding two tied cases). This evaluation\nrobustly demonstrates that our profile generator\naccurately identifies and filters out inappropriate\nmedical queries before patient profile generation.\nNext, the evaluation of the generated patient pro-\nfiles further supported our framework’s design. Hu-\nman expert ratings showed strong reliability, ev-\nidenced by a percent agreement of 0.9140 and a\n8\n"}, {"page": 9, "text": "Figure 3: Analysis of Medical QA Benchmark Datasets\nfor Clinical Simulation.\n(a) A bar chart illustrates\nthe proportion of “Appropriate Samples” within each\ndataset (calculated from the total samples). (b) A radar\nchart illustrates the percentage of appropriate samples\nthat contain specific demographic, basic, and optional\ninformation for patient profile generation (calculated\nfrom the appropriate samples).\nGwet’s AC2 coefficient of 0.8313 (TABLE 2). This\nindicates consistent expert judgment. Additionally,\nthe profiles received an average score of 3.464 out\nof a possible 4 points. This high average score\nsuggests that the patient profiles generated by our\nframework are generally accurate and clinically\nreliable. These evaluations collectively validate\nthe effectiveness of our patient profile generation\nprocess.\n5.2\nDataset Suitability for Clinical Simulation\nBefore evaluating the performance of LLMs with\nour AutoMedic framework, we conducted a pre-\nanalysis of the selected medical QA benchmarks\nto assess their suitability for simulating realistic\nclinical situations. Leveraging our validated pa-\ntient profile generation process, we analyzed each\ndataset based on its capacity to provide sufficient\nand relevant information for virtual patient profiles.\nThe detailed findings are presented in Fig. 3.\nOur analysis first focused on the proportion of\nsamples deemed appropriate for generating a vir-\ntual patient profile. As illustrated in Fig. 3(a), Med-\nBullets and MedQA exhibited the highest propor-\ntion, with over 90% of their samples being suitable.\nMedXeprtQA, while having a slightly lower ratio\nof appropriate samples (79.8%), still contributed\nthe largest absolute number of appropriate sam-\nples (1,955). In contrast, MedMCQA, HEAD-QA,\nand MMLU-Pro showed a significantly lower pro-\nportion of appropriate samples, ranging from 10%\nto 15%. This initial filter is crucial, as only ap-\npropriate queries can lead to meaningful clinical\nsimulations.\nNext, we assessed the informational richness\nof the appropriate samples, specifically evaluat-\ning whether the datasets provided sufficient details\nto construct comprehensive patient profiles. As\nshown in Fig. 3(b), demographics, such as age\nand sex, were generally well-covered across most\ndatasets, though MedMCQA provided this informa-\ntion for only about half of its appropriate samples.\nFor basic clinical information like symptom dura-\ntion and past medical history, MedBullets, MedQA,\nand MedXpertQA consistently included these de-\ntails for over 70% of their samples. Conversely,\nMedMCQA and HEAD-QA showed noticeable\ngaps, providing this information for a relatively\nsmaller fraction of their queries. This disparity\nbecame even more pronounced when examining\noptional information, such as vital signs or physical\nexamination findings. MedMCQA and HEAD-QA\nagain presented the lowest ratios of inclusion. In-\nterestingly, MMLU-Pro, despite its low proportion\nof appropriate samples, provided patient informa-\ntion with a frequency similar to the more suitable\ndatasets once a sample was deemed appropriate.\nIn summary, MedBullets, MedQA, and MedX-\npertQA emerge as the most suitable datasets for\nthe Automedic framework. They not only offer\na high proportion (or a large absolute number) of\nappropriate samples for patient profile generation\nbut also provide abundant and comprehensive in-\nformation to create detailed virtual patient profiles,\nessential for accurate clinical situation simulation.\nMedBullets and MedQA stand out for their high\nproportional suitability, while MedXpertQA excels\nin the total volume of usable samples.\nConversely, MedMCQA and HEAD-QA, despite\npotentially having large total numbers of medical\nqueries, are less optimal for our framework due to\ntheir low proportion of appropriate samples and\ntheir subsequent lack of detailed information neces-\nsary for robust patient profile generation. MMLU-\n9\n"}, {"page": 10, "text": "Table 3: Alignment of Human Expert and CARE Metric Evaluation for LLMs as Doctor Agents.\nAccuracy\nConversational\nEfficiency & Strategy\nEmpathy\nRobustness\nPercent Agreement\n0.9722\n0.8185\n0.9185\n0.7426\nGwet’s AC2\n0.9460\n0.6002\n0.8571\n0.6209\nAverage Human Score\nClaude Sonnet 4\n2.2333\n2.5000\n2.9333\n2.6500\nLlama 3-70B\n1.6333\n2.0167\n2.3667\n2.7333\nCARE Metric\nClaude Sonnet 4\n0.4699\n1.0192\n0.8520\n0.9333\nLlama 3-70B\n0.2866\n0.6703\n0.7260\n1.0000\nNotes: This table presents inter-rater reliability metrics (Percent Agreement and Gwet’s AC2), average human expert\nscores (3-point Likert scale), and corresponding CARE metric scores for Claude Sonnet 4 and Llama 3-70B across four\nevaluation dimensions when acting as a doctor agent on the MedBullets dataset.\nPro, while also having a low proportion of appropri-\nate samples, is somewhat mitigated by the relatively\ngood information completeness when a sample is\nappropriate, suggesting its suitable samples are of\nhigher quality.\n5.3\nCorrelation of CARE Metric with Human\nExpert Evaluations\nBefore deploying the CARE metric for LLM eval-\nuation, we first established its alignment with the\nhuman expert judgment. TABLE 3 presents the re-\nsults of human expert evaluation for Cluade Sonnet\n4 and Llama 3-70B acting as doctor agents on a ran-\ndom sample from the MedQA dataset, alongside\ntheir corresponding CARE metric scores.\nFor accuracy, human experts demonstrated very\nhigh reliability, with a Gwet’s AC2 coefficient of\n0.9460. This high agreement likely stems from the\nrelatively objective nature of judging the correct-\nness of a medical answer. Claude Sonnet 4 received\na notably higher average score of 2.2333 compared\nto Llama 3-70B’s 1.6333. This trend is directly re-\nflected in the CARE metric, where Claude Sonnet\n4’s SACC (0.4699) is also substantially higher than\nLlama 3-70B’s (0.2866), indicating strong align-\nment.\nRegarding conversation efficiency and strategy,\nthe human expert agreement, while lower than for\naccuracy, remained substantial with a Gwet’s AC2\nof 0.6002. Human evaluators rated Claude Sonnet\n4 at 2.5000, slightly higher than Llama 3-70B’s\n2.0167. This performance hierarchy is consistently\nmirrored by the SCES score of the CARE metric,\nwith Claude Sonnet 4 scoring 1.0192 and Llama\n3-70B scoring 0.6703. This suggests that the au-\ntomated metric effectively captures the nuances\nof conversational flow and strategy perceived by\nhuman experts.\nFor empathy, human evaluators exhibited high\nagreement (0.9185 percent agreement and 0.8571\nGwet’s AC2 coefficient). Claude Sonnet 4 achieved\na near-perfect average human score of 2.9333,\nsignificantly outperforming Llama 3-70B, which\nscored 2.3667. The SEMP score of the CARE met-\nric also reflects this hierarchy, with Claude Sonnet\n4 (0.8520) demonstrating higher empathy scores\nthan Llama 3-70B (0.7260), further reinforcing the\nalignment.\nFinally, for robustness, human experts showed\nsubstantial agreement, with a Gwet’s AC2 coeffi-\ncient of 0.6209. Interestingly, both models received\nsimilarly high average human scores, with Llama\n3-70B (2.7333) slightly edging out Claude Son-\nnet 4 (2.6500). This trend is compellingly aligned\nwith SROB score, where Llama 3-70B achieved a\nperfect score of 1.0000, and Claude Sonnet 4 also\nscored highly at 0.9333. This high performance for\nboth models suggests that even under potentially\nchallenging conversational conditions, both LLMs\nmaintained a high degree of consistency in their\nresponses.\nIn summary, the high inter-rater reliability ob-\nserved across all four CARE categories underscores\nthe robustness and consistency of the human evalua-\ntion results. Crucially, the trends in average human\nexpert scores show strong alignment with the corre-\nsponding CARE metric values regardless of LLMs.\nThis robust correlation validated the CARE metric\nas an appropriate and reliable automated tool for\nevaluating the multifaceted capabilities of LLMs\nas clinical conversational agents.\n5.4\nAccuracy in Static and Conversational\nSettings\nFirst, we comprehensively compare the perfor-\nmance of LLMs by examining their accuracy in\nboth static QA and conversational settings. Fig. 4\nillustrates these accuracies across the various medi-\ncal QA datasets.\nA clear trend emerges whereby models exhibit-\n10\n"}, {"page": 11, "text": "Figure 4: Static and conversational QA accuracy of LLMs across medical QA benchmarks. This figure presents the\nstatic QA accuracy (solid bars) and conversational QA accuracy (striped bars) for each LLM across the six medical\nQA benchmark datasets.\ning high QA accuracy generally maintain relatively\nhigh conversational accuracy. However, a consis-\ntent observation across all experiments is that ev-\nery model demonstrates lower performance in the\nconversational setting compared to its static QA\ncounterpart. This performance drop is particularly\npronounced for DeepSeek-R1-70B and Med42-v2-\n70B, as evident in their significant accuracy decline\nin conversational scenarios across most datasets.\nThis universal struggle suggests that models, re-\ngardless of their underlying knowledge, face sub-\nstantial challenges in effectively gathering all nec-\nessary information within a dynamic conversational\nexchange.\nThis finding provides a critical insight that an\nLLM’s raw knowledge in medicine (reflected in QA\naccuracy) does not directly translate to effective\nclinical conversational ability. The conversational\ncontext introduces complexities such as managing\ndialogue flow, interpreting implicit cues, maintain-\ning coherence, and adapting to user responses, all\nof which are distinct from simply answering a well-\nposed, single-turn question. Therefore, evaluating\nLLMs specifically as clinical conversational agents,\nrather than solely on static QA, is essential. This ap-\nproach reveals whether models can effectively com-\nmunicate with users, even if they possess extensive\ndomain knowledge, highlighting the importance of\nmetrics that capture interactive performance.\n5.5\nClinical Conversation Performance\nAnalysis using the CARE Metric\nNext, we evaluated the capabilities of various\nLLMs as clinical conversational agents using the\ncomprehensive CARE metric. Fig. 5(a) visually\nsummarizes these evaluation results across the med-\nical QA datasets.\nAmong the open-source general models, Llama\n3-70B and Qwen3-32B exhibited average perfor-\nmance across the overall CARE metric. Llama3-\n70B notably achieved the highest SROB across the\nboard, though the inter-model differences in robust-\nness were generally minor. Qwen3-32B demon-\nstrated strong SACC, yet it did not surpass larger\nmodels like gpt-oss-120B or the proprietary models.\nDeepSeek-R1-70B consistently performed poorly\nacross most experiments. As illustrated in Figure 7\nin the Appendix, DeepSeek-R1-70B struggled to\nfunction appropriately within a virtual clinical set-\nting (e.g., ended the conversation inappropriately).\nWe hypothesize this degradation in performance\nstems from its distilled nature, where supervised\nfine-tuning on synthetic data, rather than reinforce-\nment learning, may have impaired both its reason-\ning capabilities and its ability to engage in proper\ncommunication with other agents, a phenomenon\nobserved in prior work (Oh et al., 2025). gpt-oss-\n120B achieved high SACC due to strong QA and\nconversational accuracy, but received low scores\nfor both SCES and SEMP. This deficiency in conver-\nsational strategy and empathy is due to the model’s\nreliance on uncharacteristic formats, such as num-\nbered lists (Figure 7 in the Appendix), a style that\nemphasizes objective communication over human\ninteraction. This detached, objective approach in-\nherently ensures the absence of sycophancy.\nTurning to open-source biomedical LLMs,\nMed42-v2-70B generally performed poorly across\nmost CARE metrics. Despite being fine-tuned on\nmedical data, it showed no significant improve-\n11\n"}, {"page": 12, "text": "Figure 5: (a) LLM performance across medical QA benchmarks via CARE metric. These radar charts display\nthe performance of various LLMs on each medical QA dataset, illustrating scores across the four CARE metric\ndimensions: SACC (accuracy), SCES (conversational efficiency & strategy), SEMP (empathy), and SROB (robustness).\n(b) Average normalized CARE metric across medical QA benchmarks. This bar plot illustrates the average\nnormalized scores for each component of the CARE metric (SACC, SCES, SEMP, SROB) across all evaluated medical\nQA datasets. Scores are min-max normalized per dataset, with 1 representing the highest score and 0 the lowest for\neach metric.\nment in SACC and underperformed in other dimen-\nsions. Moreover, as shown in Figure 8 in the Ap-\npendix, it exhibited abnormal conversational behav-\nior, such as answering its own questions without\nwaiting for input form the patient or clinical staff.\nOpenBioLLM-70B achieved the best SCES and bet-\nter SROB compared to Med42-v2-70B, but it too\nexhibited degraded performance in accuracy and\nempathy. In contrast, HuatuoGPT-o1-72B demon-\nstrated superior performance across all CARE met-\nrics. Its SACC scores were comparable to those\nof gpt-oss-120B and proprietary models, which\nwe attribute to its combination of medical domain-\nspecific tuning and robust reasoning abilities.\nThe two proprietary general LLMs presented dif-\nferent strengths. Claude Sonnet 4 achieved a high\nSACC due to its excellent QA and conversational\naccuracy, with a minimal gap between the two. Fur-\nthermore, it recorded the highest SEMP across all\nmodels, consistently exceeding 0.85 on all datasets.\nThis superior empathetic performance is likely due\nto its tendency to check patient status and express\nempathy, as shown in Figure 8 in the Appendix,\nunlike other LLMs that primarily focus on informa-\ntion gathering. GPT-4o, while showing relatively\nlower SACC and SEMP compared to Claude Sonnet\n4, slightly outperformed it in SROB. This indicates\nthat GPT-4o possesses superior robustness, effec-\ntively maintaining its assigned persona and adher-\ning to role constraints throughout extended multi-\nturn conversations. Moreover, GPT-4o exhibited\nsuperior SCES, indicating its efficiency in gathering\nnecessary patient information with minimal turns\nand words.\nTo enable a comprehensive comparison of model\nperformance across diverse datasets, we applied\nmin-max normalization for each metric within each\ndataset (where 1 represents the highest score and 0\nthe lowest). Subsequently, these normalized scores\nwere averaged across all datasets for each metric.\n12\n"}, {"page": 13, "text": "Figure 6: The relationship between static and conversational QA accuracy. The scatter plots display the correlation\nfor: (a) All models, (b) Open-source general models, (c) Open-source biomedical models, and (d) Proprietary\nmodels. For each category, the Pearson correlation coefficient (r), p-value, and linear regression equation are\nprovided in the top-left corner. The grey dashed line represents the identity line (y = x), while the red dashed line\nindicates the fitted regression line.\nFig. 5(b) presents these average normalized overall\nCARE metric scores.\nFor SACC,\ngpt-oss-120B demonstrates the\nstrongest performance, closely followed by Claude\nSonnet 4 and HuatuoGPT-o1-72B. In terms of\nSCES, OpenBioLLM achieves the highest average\nscore, with GPT-4o securing the second position.\nRegarding SEMP, Claude Sonnet 4 stands out with\nan overwhelmingly superior performance, indicat-\ning its exceptional ability in this dimension. For\nSROB, Llama 3-70B leads, although the perfor-\nmance differences among models in this category\nare relatively small. Notably, DeepSeek-R1-70B\nand Med42-v2-70B consistently exhibit poor per-\nformance across most CARE metrics. These spe-\ncific trends will be discussed in greater detail in the\nsubsequent discussion section.\n6\nDiscussion\nIn this study, we introduced AutoMedic, a novel\nagentic framework designed for the fully auto-\nmated evaluation of LLMs as clinical conversa-\ntional agents. To enable accurate and automated\nevaluation of LLMs, we proposed the CARE met-\nric, which enables a multi-faceted assessment of\nLLMs across four critical dimensions: accuracy,\nconversational efficiency and strategy, empathy,\nand robustness. Through extensive human expert\nevaluations, we validated the reliability of the vir-\ntual patient profile generation process within Au-\ntoMedic and demonstrated that our CARE metric\nclosely aligns with the clinical judgment of human\nexperts.\nLeveraging the patient profile generation capa-\nbilities of AutoMedic, we first established guide-\nlines for assessing the suitability of medical QA\ndatasets for clinical simulation. Our analysis re-\nvealed significant disparities among datasets regard-\ning the volume of clinically appropriate samples\nand the depth of patient information provided. We\nfound that datasets such as MedBullets, MedQA,\nand MedXpertQA offer a substantial number of ap-\npropriate samples enriched with sufficient patient\ndetails. In contrast, benchmarks like MedMCQA\nand HEAD-QA exhibit a low proportion of suitable\nsamples and frequently lack basic patient informa-\ntion. These results imply that not all medical QA\ndatasets contain questions relevant to realistic clin-\nical situations. Therefore, relying solely on them\nto assess the clinical capabilities of LLMs may be\ninadequate. Consequently, our framework serves\nas a vital guideline for determining the applica-\nbility of medical QA datasets to clinical scenario\nsimulations. Furthermore, a key advantage of our\nframework is its generalizability. Although we uti-\nlized six specific datasets in this study, AutoMedic\nis designed to automatically adapt to and process\nany new off-the-shelf medical QA dataset without\nmanual modification.\nWe compared the performance in both static\nQA and conversational settings, revealing that the\ndynamic conversational environment presents a\nsignificantly greater challenge than static QA. To\nquantify the relationship between these two per-\nformance modes, we performed a linear regres-\nsion analysis and calculated the Pearson corre-\nlation coefficient (r). As shown in Fig. 6(a), a\nstrong positive correlation exists between static\nand conversational accuracy across all models\n(r=0.7547). When examining open-source gen-\neral models (Fig. 6(b)), a similar strong correlation\nis observed, although DeepSeek-R1-70B notably\ndeviates from the general trend. In contrast, open-\nsource biomedical models (Fig. 6(c)) exhibit the\n13\n"}, {"page": 14, "text": "weakest correlation (r = 0.6260). These results\nsuggest that while static medical knowledge serves\nas a foundation for conversational competence, sub-\noptimal fine-tuning strategies may hinder the effec-\ntive translation of this knowledge into clinical dia-\nlogue. Conversely, proprietary models (Fig. 6(d))\ndemonstrate the highest correlation (r = 0.9350),\nindicating a consistent balance between their static\nclinical knowledge and conversational capabilities.\nIn conclusion, for clinical conversational settings, it\nis advantageous to prioritize proprietary models or\ncarefully select open-source models that demon-\nstrate both high static QA accuracy and robust\ninstruction-following tuning.\nOur evaluation using the CARE metric revealed\ndistinct performance profiles for each LLM, high-\nlighting unique strengths and weaknesses. One par-\nticularly striking finding was that domain-specific\nmedical tuning did not confer superiority in con-\nversational settings. With the notable exception\nof HuatuoGPT-o1-72B, most medical-tuned mod-\nels performed worse than their general-domain\ncounterparts. We conjecture that this performance\ndeficit arises because these models were primar-\nily fine-tuned for static medical QA tasks, not for\ndynamic, interactive communication. During such\nspecialized fine-tuning, the models may inadver-\ntently lose their broader communicative capabil-\nities. Consequently, when deployed in realistic\nclinical scenarios, they fail to leverage their spe-\ncialized knowledge effectively because they cannot\nmanage the conversational interaction. This re-\nsult strongly indicates that fine-tuning LLMs with\ndomain-specific data must be approached carefully,\nensuring that foundational communication skills\nare preserved.\nA comparison between open-source and propri-\netary models revealed significant differences in\ntheir performance profiles. Open-source models\noften demonstrated “spiky” capabilities, excelling\nin specific dimensions while underperforming in\nothers. For instance, gpt-oss-120B achieved the\nhighest SACC and OpenBioLLM-70B led in SCES,\nbut both showed considerable deficits in other met-\nrics. In contrast, the proprietary models, GPT-4o\nand Claude Sonnet 4, exhibited a more balanced\nand consistently high performance across all CARE\ndimensions. Although they did not always secure\nthe highest score in every individual metric, their\nstrong, well-rounded profiles suggest they are cur-\nrently the most reliable choice for a comprehen-\nsive clinical conversational agent. However, in\nscenarios where proprietary models are unfeasible\ndue to data privacy constraints, data export limi-\ntations, or other restrictions, our findings indicate\nthat HuatuoGPT-o1-72B presents itself as the most\nviable open-source alternative.\n7\nConclusion\nIn this paper, we introduced AutoMedic, a novel,\nautomated multi-agent framework that enables\nthe evaluation of LLMs as clinical conversa-\ntional agents through realistic, simulated dialogues.\nTo quantitatively assess performance within this\nframework, we also proposed the CARE metric,\na multi-faceted tool that measures accuracy, con-\nversational efficiency and strategy, empathy, and\nrobustness.\nOur investigation yielded several key findings.\nHuman expert evaluations confirmed the reliabil-\nity of AutoMedic’s patient profile generation and\nthe strong correlation of our CARE metric with\nhuman clinical judgment. We found that only a\nsubset of medical QA datasets, such as MedBul-\nlets, MedQA, and MedXpertQA, are suitable for\nclinical simulation. Crucially, we observed that\nall LLMs suffer a performance drop when moving\nfrom static QA to conversational settings. This\nfinding confirms that existing static medical QA\nbenchmarks, which primarily assess knowledge,\ndo not sufficiently reflect the actual performance of\nLLMs in dynamic medical field applications. The\nCARE metric further revealed distinct LLM per-\nformance profiles. Proprietary models like Claude\nSonnet 4 and GPT-4o showed balanced, high-level\nperformance, while most medical-tuned models un-\nderperformed, suggesting fine-tuning may erode\ncommunication skills. HuatuoGPT-o1-72B, how-\never, emerged as a strong open-source alternative.\nThis work provides a robust, automated method-\nology for the comprehensive evaluation of conver-\nsational medical AI, offering deeper insights than\nstatic metrics alone. AutoMedic and the CARE\nmetric can guide researchers and developers in\nbuilding and selecting LLMs that are not only\nknowledgeable but also effective, empathetic, and\nreliable in practice.\nLimitations\nWhile our framework provides a comprehensive\nevaluation of LLMs as clinical conversational\nagents, several limitations must be addressed. First,\nour current evaluation is limited to text-based in-\n14\n"}, {"page": 15, "text": "teractions and does not directly incorporate other\nmodalities. Given that many real-world medical\nscenarios are inherently multimodal and involve\ndata such as medical imaging, a critical area for\nfuture work is extending the AutoMedic frame-\nwork to evaluate vision-language and other mul-\ntimodal models for broader impact. Second, al-\nthough the CARE metric provides a multi-faceted\nquantitative analysis, we did not define a single,\naggregated score for overall performance. A sim-\nple sum or average is insufficient, as the four di-\nmensions have different scales and their relative\nimportance can vary based on the specific clini-\ncal application. Future research could address this\nby developing a weighted composite score, per-\nhaps calibrated against human expert preferences\nderived from head-to-head model comparisons. Fi-\nnally, this paper focused exclusively on the evalu-\nation of the doctor agent. To ensure stability and\nconsistency, we utilized a single proprietary LLM\nfor the patient and clinical staff roles. However,\nthe performance and behavior of these supporting\nagents can influence the doctor agent’s actions. An-\nalyzing the effect of different LLMs in these patient\nand staff roles on the overall simulation and doctor\nagent performance remains a valuable direction for\nfuture investigation.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, and 1 others. 2023. GPT-4 techni-\ncal report. arXiv preprint arXiv:2303.08774.\nSandhini Agarwal, Lama Ahmad, Jason Ai, Sam Alt-\nman, Andy Applebaum, Edwin Arbus, Rahul K\nArora, Yu Bai, Bowen Baker, Haiming Bao, and 1\nothers. 2025. gpt-oss-120b & gpt-oss-20b model\ncard. arXiv preprint arXiv:2508.10925.\nMohammad Almansoori, Komal Kumar, and Hisham\nCholakkal. 2025. Self-evolving multi-agent simula-\ntions for realistic clinical interactions. In Interna-\ntional Conference on Medical Image Computing and\nComputer-Assisted Intervention.\nMalaikannan\nSankarasubbu\nAnkit\nPal.\n2024.\nOpenBioLLMs:\nAdvancing open-source large\nlanguage models for healthcare and life sci-\nences.\nhttps://huggingface.co/aaditya/\nOpenBioLLM-Llama3-70B.\nAnthropic. 2025.\nSystem card: Claude Opus 4 &\nClaude Sonnet 4.\nRahul K Arora, Jason Wei, Rebecca Soskin Hicks, Pre-\nston Bowman, Joaquin Quiñonero-Candela, Foivos\nTsimpourlas, Michael Sharman, Meghan Shah, An-\ndrea Vallone, Alex Beutel, and 1 others. 2025.\nHealthBench: Evaluating large language models\ntowards improved human health.\narXiv preprint\narXiv:2505.08775.\nSuhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell,\nMichael Wornow, Juan M Banda, Nikesh Kotecha,\nTimothy Keyes, Yifan Mai, Mert Oez, and 1 oth-\ners. 2025. MedHELM: Holistic evaluation of large\nlanguage models for medical tasks. arXiv preprint\narXiv:2505.23802.\nHanjie Chen, Zhouxiang Fang, Yash Singla, and Mark\nDredze. 2025. Benchmarking large language mod-\nels on answering and explaining challenging medical\nquestions. In Proceedings of the 2025 Conference\nof the Nations of the Americas Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers), pages\n3563–3599.\nJunying Chen, Zhenyang Cai, Ke Ji, Xidong Wang,\nWanlong Liu, Rongsheng Wang, Jianye Hou, and\nBenyou Wang. 2024. HuatuoGPT-o1, towards med-\nical complex reasoning with LLMs. arXiv preprint\narXiv:2412.18925.\nZeming Chen, Alejandro Hernández Cano, Angelika\nRomanou, Antoine Bonnet, Kyle Matoba, Francesco\nSalvi, Matteo Pagliardini, Simin Fan, Andreas\nKöpf, Amirkeivan Mohtashami, and 1 others. 2023.\nMeditron-70b: Scaling medical pretraining for large\nlanguage models. arXiv preprint arXiv:2311.16079.\nClément Christophe, Praveen K Kanithi, Tathagata\nRaha, Shadab Khan, and Marco AF Pimentel. 2024.\nMed42-v2: A suite of clinical LLMs. arXiv preprint\narXiv:2408.06142.\nGheorghe Comanici, Eric Bieber, Mike Schaekermann,\nIce Pasupat, Noveen Sachdeva, Inderjit Dhillon, Mar-\ncel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and\n1 others. 2025. Gemini 2.5: Pushing the frontier with\nadvanced reasoning, multimodality, long context, and\nnext generation agentic capabilities. arXiv preprint\narXiv:2507.06261.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, and 1 others. 2024. The Llama 3 herd\nof models. arXiv e-prints, pages arXiv–2407.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao\nSong, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\nDeepSeek-R1: Incentivizing reasoning capability in\nLLMs via reinforcement learning. arXiv preprint\narXiv:2501.12948.\nPaul Hager, Friederike Jungmann, Robbie Holland, Ku-\nnal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob\n15\n"}, {"page": 16, "text": "Vielhauer, Marcus Makowski, Rickmer Braren, Geor-\ngios Kaissis, and 1 others. 2024. Evaluation and\nmitigation of the limitations of large language mod-\nels in clinical decision-making. Nature medicine,\n30(9):2613–2622.\nXiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang,\nand Yuyin Zhou. 2025. m1: Unleash the potential\nof test-time scaling for medical reasoning with large\nlanguage models. arXiv preprint arXiv:2504.00869.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, and 1\nothers. 2024. GPT-4o system card. arXiv preprint\narXiv:2410.21276.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-\nson, Ahmed El-Kishky, Aiden Low, Alec Helyar,\nAleksander Madry, Alex Beutel, Alex Carney, and 1\nothers. 2024. OpenAI o1 system card. arXiv preprint\narXiv:2412.16720.\nShuyang Jiang, Yusheng Liao, Zhe Chen, Ya Zhang,\nYanfeng Wang, and Yu Wang. 2025. Meds3: Towards\nmedical small language models with self-evolved\nslow thinking. arXiv preprint arXiv:2501.12051.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? A large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nJingoo Lee, Kyungho Lim, Young-Chul Jung, and\nByung-Hoon Kim. 2025. Psyche: A multi-faceted\npatient simulation framework for evaluation of psy-\nchiatric assessment conversational agents.\narXiv\npreprint arXiv:2501.01594.\nKyung Ho Lim, Ujin Kang, Xiang Li, Jin Sung Kim,\nYoung-Chul Jung, Sangjoon Park, and Byung-Hoon\nKim. 2025. Susceptibility of large language mod-\nels to user-driven factors in medical queries. arXiv\npreprint arXiv:2503.22746.\nHarsha Nori, Mayank Daswani, Christopher Kelly, Scott\nLundberg, Marco Tulio Ribeiro, Marc Wilson, Xi-\naoxuan Liu, Viknesh Sounderajah, Jonathan Carlson,\nMatthew P Lungren, and 1 others. 2025. Sequen-\ntial diagnosis with language models. arXiv preprint\narXiv:2506.22405.\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023. Capabilities\nof GPT-4 on medical challenge problems.\narXiv\npreprint arXiv:2303.13375.\nGyutaek Oh, Seoyeon Kim, Sangjoon Park, and Byung-\nHoon Kim. 2025. Rethinking test-time scaling for\nmedical AI: Model and task-aware strategies for\nLLMs and VLMs. arXiv preprint arXiv:2506.13102.\nOpenAI. 2025. Introducing GPT-5.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. MedMCQA: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on health,\ninference, and learning, pages 248–260. PMLR.\nKhaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno,\nDavid Stutz, Ellery Wulczyn, Fan Zhang, Tim\nStrother, Chunjong Park, Elahe Vedadi, and 1 others.\n2024. Capabilities of Gemini models in medicine.\narXiv preprint arXiv:2404.18416.\nSarah Sandmann, Stefan Hegselmann, Michael Fujarski,\nLucas Bickmann, Benjamin Wild, Roland Eils, and\nJulian Varghese. 2025.\nBenchmark evaluation of\nDeepSeek large language models in clinical decision-\nmaking. Nature Medicine, pages 1–1.\nSamuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo\nReis, Jeffrey Jopling, and Michael Moor. 2024.\nAgentClinic: A multimodal agent benchmark to eval-\nuate ai in simulated clinical environments. arXiv\npreprint arXiv:2405.07960.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri,\nAtilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau,\nand 1 others. 2025. MedGemma technical report.\narXiv preprint arXiv:2507.05201.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\nClark, Stephen R Pfohl, Heather Cole-Lewis, and\n1 others. 2025. Toward expert-level medical ques-\ntion answering with large language models. Nature\nMedicine, 31(3):943–950.\nTao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaek-\nermann, Mohamed Amin, Pi-Chuan Chang, Andrew\nCarroll, Charles Lau, Ryutaro Tanno, Ira Ktena, and\n1 others. 2024. Towards generalist biomedical ai.\nNejm Ai, 1(3):AIoa2300138.\nTao Tu, Mike Schaekermann, Anil Palepu, Khaled Saab,\nJan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li,\nMohamed Amin, Yong Cheng, and 1 others. 2025.\nTowards conversational diagnostic artificial intelli-\ngence. Nature, pages 1–9.\nDavid Vilares and Carlos Gómez-Rodríguez. 2019.\nHEAD-QA: A healthcare dataset for complex reason-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n960–966, Florence, Italy. Association for Computa-\ntional Linguistics.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nAaran Arulraj, Xuan He, Ziyan Jiang, and 1 others.\n2024. MMLU-Pro: A more robust and challenging\nmulti-task language understanding benchmark. Ad-\nvances in Neural Information Processing Systems,\n37:95266–95290.\nChristopher YK Williams, Brenda Y Miao, Aaron E\nKornblith, and Atul J Butte. 2024. Evaluating the\n16\n"}, {"page": 17, "text": "use of large language models to provide clinical rec-\nommendations in the emergency department. Nature\ncommunications, 15(1):8236.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui,\nBo Zheng,\nBowen Yu,\nChang\nGao, Chengen Huang, Chenxu Lv, and 1 others.\n2025.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388.\nKaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding,\nZhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu\nCui, Biqing Qi, Xuekai Zhu, and 1 others. 2024.\nUltraMedical: Building specialized generalists in\nbiomedicine. Advances in Neural Information Pro-\ncessing Systems, 37:26045–26081.\nRojin Ziaei and Samuel Schmidgall. 2023. Language\nmodels are susceptible to incorrect patient self-\ndiagnosis in medical applications. In Deep Genera-\ntive Models for Health Workshop NeurIPS 2023.\nYuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai\nZhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and\nBowen Zhou. 2025. MedXpertQA: Benchmarking\nexpert-level medical reasoning and understanding.\narXiv preprint arXiv:2501.18362.\nA\nAppendix\n17\n"}, {"page": 18, "text": "Figure 7: An example conversation generated by the DeepSeek-R1-70B and gpt-oss-120B doctor agent.\n18\n"}, {"page": 19, "text": "Figure 8: An example conversation generated by the Med42-v2-70B and Claude Sonnet 4 doctor agent.\n19\n"}]}