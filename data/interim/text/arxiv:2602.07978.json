{"doc_id": "arxiv:2602.07978", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.07978.pdf", "meta": {"doc_id": "arxiv:2602.07978", "source": "arxiv", "arxiv_id": "2602.07978", "title": "Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection", "authors": ["Rui Feng", "Zhiyao Luo", "Liuyu Wu", "Wei Wang", "Yuting Song", "Yong Liu", "Kok Pin Ng", "Jianqing Li", "Xingyao Wang"], "published": "2026-02-08T14:10:05Z", "updated": "2026-02-08T14:10:05Z", "summary": "Speech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive Impairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity and a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to provide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel framework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning. Specifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data scarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages, effectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large Language Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a CoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on black-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited clinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and 78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin cohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute a critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.07978v1", "url_pdf": "https://arxiv.org/pdf/2602.07978.pdf", "meta_path": "data/raw/arxiv/meta/2602.07978.json", "sha256": "ea0cf0ef6ca34c5cf1e571cffd14a5de8d52ce295370abd0dff978986c491fda", "status": "ok", "fetched_at": "2026-02-18T02:19:29.569052+00:00"}, "pages": [{"page": 1, "text": "Cross-Linguistic Persona-Driven Data Synthesis for\nRobust Multimodal Cognitive Decline Detection\nRui Feng1,‚Ä†, Zhiyao Luo2,‚Ä†, Liuyu Wu1, Wei Wang1, Yuting Song3, Yong Liu3, Kok Pin Ng4, 5,\nJianqing Li1,*, and Xingyao Wang3,*\n1Engineering Research Center of Intelligent Theranostics Technology and Instruments, Ministry of Education,\nSchool of Biomedical Engineering and Informatics, Nanjing Medical University, Nanjing, 211166, China\n2Institute of Biomedical Engineering, University of Oxford, Oxford, OX1 2JD, United Kingdom\n3Institute of High Performance Computing, Agency for Science, Technology and Research (A*STAR), Singapore,\n138632, Singapore\n4Department of Neurology, National Neuroscience Institute, Singapore, 308433, Singapore\n5Duke-NUS Medical School, Singapore, 169857, Singapore\n‚Ä†These authors contributed equally to this work\n*Corresponding author: jqli@njmu.edu.cn, wang xingyao@a-star.edu.sg\nABSTRACT\nSpeech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive\nImpairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity\nand a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to\nprovide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel\nframework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning.\nSpecifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data\nscarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages,\neffectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large\nLanguage Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a\nCoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on\nblack-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited\nclinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and\n78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin\ncohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute\na critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.\nIntroduction\nAlzheimer‚Äôs disease (AD) is a progressive neurodegenerative disorder characterized by the gradual erosion of cognitive\nfunctions, memory, and activities of daily living. Driven by the global demographic shift toward an aging population, the\nprevalence of AD has escalated significantly, emerging as a critical public health priority1,2. As a critical transitional phase,\nMild Cognitive Impairment (MCI) represents a vital clinical window for the early identification of AD. Identifying individuals at\nthis stage and implementing timely interventions are essential to slow disease progression and secure better cognitive outcomes.\nCompared to standard clinical diagnoses (e.g., comprehensive neuropsychological assessments, fluid biomarker analysis,\nand neuroimaging techniques)3,4, speech and language production has been recognized as sensitive indicators of cognitive\ndecline5, as the complex process of formulating speech requires the integrity of multiple cognitive domains, including episodic\nmemory, executive function, and lexical retrieval. In particular, picture description tasks (e.g., the ‚ÄúCookie Theft‚Äù task) provide\na unique and clinically robust paradigm for eliciting speech6. Picture description constrains the semantic context, requiring the\nsubject to visually process a scene and retrieve specific lexical items to describe simultaneous actions. Leveraging this paradigm,\nAI-based automated assessment techniques have gained significant traction. By utilizing Natural Language Processing (NLP)\nand acoustic analysis to evaluate recordings from picture description tasks, these systems offer a scalable, cost-effective, and\nobjective alternative for mass screening and longitudinal monitoring7,8.\nRecently, Multimodal Large Language Models (MLLMs) have demonstrated transformative potential in biomedical tasks,\nowing to their capacity to process multimodal inputs and encode vast amounts of semantic knowledge9,10, which offer a\npromising avenue for scalable cognitive assessment. However, deploying MLLMs for clinical MCI detection currently faces\narXiv:2602.07978v1  [cs.CL]  8 Feb 2026\n"}, {"page": 2, "text": "(a) Data Synthesis\nvisual stimulus\n<|im_start|>user\nGiven the image, produce one \nnatural, conversational utterance \nthat could plausibly be said by \nthis participant. <|im_end|>\n<|im_start|> assistant\n<description text>\n< persona >\nOkay. The little boy is on the stool, which\nis tipping over, and he's getting into the\ncookie jar, which is up in the cabinet. The\nlid is off the jar‚Ä¶\nNormal\nImpaired\nIndexTTS 2\nText Tokenizer\nTimbre Bank\n<Target>\n<Target>\n1\n1\n2\nPersona Construction\nAudio Encoder\nLarge Language Model (LLM)\nThe subject appears to \nbe experiencing some \nlevel of difficulty with \nlanguage and memory. \nHere is a step-by-step \nanalysis of the subject's \nvocal behavior and \nlinguistic expression:\n...\n<|im_start|> user\nAnalyze the subject's vocal \nbehavior and linguistic \nexpression during cognitive \nimpairment assessments, \nand find clues that support \ntheir cognitive \nlabels.<|im_end|>\n<|im_start|> assistant\n(b) CoT Deduction\nSpeech Synthesis\n(c) LoRA Fine-Tuning\nLLM\n‚Ñù!√ó#\n‚Ñù#√ó$\nPretrained\nweight ùëæùíê\nPred\nTarget\nAudio \nTokens\nText \nTokens\nCE Loss\nFrozen\nTrainable\nForward\nBackward\nMom was washing the dishes in\nthe kitchen. and she forgot to\nturn off the faucet‚Ä¶\n20\"\n\"Rationale\": \"‚Ä¶\",\n\"Cognitive Function Status\": \"‚Ä¶\"\n<|im_start|> user\nAnalyze the provided data \nand infer which cognitive \nfunction status the subject \nis most likely to \nhave.<|im_end|>\n<|im_start|>user\n<|im_start|> assistant\nelderly voices\n\"label\": \"‚Ä¶.\"\n\"gender\": \"‚Ä¶\"\n\"age\": ‚Ä¶\n\"edu_level\": \"‚Ä¶\"\n<diagnostic text>\n(d) Diagnostic Inference\nFigure 1. Overview of the proposed SynCog framework. The pipeline consists of four phases: (a) Data Synthesis, which\ninvolves simulating subjects via LLMs to generate high-fidelity multimodal narratives comprising both audio recordings and\ntheir corresponding transcripts, conditioned on demographic and cognitive attributes; (b) Chain-of-Thought Distillation, where\ndiagnostic rationales are systematically derived to bridge raw multimodal evidence with clinical labels; (c) CoT deduction\nFine-Tuning, utilizing Low-Rank Adaptation (LoRA) to optimize the model M based on the diagnostic reasoning sequences\nderived during CoT Distillation; and (d) Diagnostic Inference, where the fine-tuned model M first articulates a heuristic\nreasoning sequence r j to ground the final assessment in specific pathological markers. By synthesizing evidence from acoustic\nprosody and linguistic content, this transparent inference pathway effectively mitigates the risk of shortcut learning. The\nresulting diagnostic process not only enhances the robustness of results across different linguistic contexts but also provides\nclinicians with interpretable evidentiary support to validate the final assessment.\nthree critical challenges. First, data scarcity creates a significant bottleneck. High-quality speech-text MCI datasets are often\nlimited in size due to privacy concerns and collection costs. For instance, the widely used ADReSSo dataset comprises only\n237 participants11. Applying Supervised Fine-Tuning (SFT) on such small-scale datasets often leads to overfitting, where\nmodels may hallucinate non-existent linguistic cues or capture spurious correlations from background noise rather than genuine\ncognitive markers12. Second, there is a lack of explicit diagnostic reasoning. Clinical MCI assessment relies on subjective and\ncomplex decision-making processes, resulting in a scarcity of standardized clinical diagnostic thought processes recorded as\nstep-by-step reasoning annotations. This absence hinders the interpretability of AI models, rendering them black boxes that fail\nto engender clinical trust. Third, cross-lingual generalization remains a critical bottleneck. Existing research is predominantly\nconfined to English-centric datasets, resulting in a severe scarcity of multilingual clinical corpora. Consequently, models often\nfail to generalize across different languages and dialects, leading to significant performance degradation when deployed in\ndiverse global populations13.\nTo address these challenges, we propose SynCog, a novel framework that overcomes the inherent constraints of data-scarce\nclinical supervision by integrating a controllable phenotypic simulation framework with a logic-driven Chain-of-Thought (CoT)\ndeduction strategy. Specifically, to mitigate data scarcity and imbalance, we introduce a controllable data synthesis pipeline.\nUnlike previous augmentation methods14 that operate on a single modality, our approach constructs comprehensive digital\npersonas defined by demographic attributes and linguistic styles. We leverage MLLMs to generate diverse, persona-consistent\npicture description narratives and employ advanced voice cloning techniques to synthesize corresponding speech signals. This\nprocess yields a large-scale, privacy-compliant, and fully labeled multimodal dataset that mirrors the variability of real-world\npatient populations. Furthermore, to compensate for the lack of expert reasoning annotations, we integrate a CoT deduction\nFine-Tuning strategy. Rather than training the model solely to predict diagnostic labels, we guide it to generate explicit CoT\nreasoning traces. By distilling high-quality self-generated rationales, we enforce a learning process that prioritizes diagnostic\n2/14\n"}, {"page": 3, "text": "logic over shallow surface patterns. Finally, to validate the clinical robustness and scalability of our framework, we conducted\nextensive experiments on both standardized English benchmarks and an independently collected real-world Mandarin clinical\ncohort. The results demonstrate that SynCog achieves superior performance compared to existing baselines. Crucially, our\nfindings confirm the framework‚Äôs ability to bridge the linguistic gap in cognitive assessment, proving that synthesis-augmented\napproaches can generalize effectively across diverse languages and clinical environments.\nResults\nOverview of SynCog\nTo address the dual challenges of data scarcity and the lack of interpretability in AI-based cognitive assessment, we developed\na novel framework that integrates generative data synthesis with CoT deduction fine-tuning. Specifically, we instantiate our\nframework using Qwen2-Audio-7B-Instruct15 as the multimodal backbone. We employ this architecture due to its native\naudio-text alignment and robust instruction-following capabilities, which have established it as a reliable foundation for\ncomparative multimodal research. The overall workflow is illustrated in Fig. 1. Unlike conventional deep learning approaches\nthat rely solely on discriminative features, our system first constructs a stratified synthetic cohort. This cohort comprises both\ncognitively normal and impaired digital subjects, enabling the model to learn robust pathological representations. Subsequently,\nthrough reasoning self-distillation, the model is trained to elucidate its diagnostic logic. This process maps multimodal inputs,\nincluding speech audio and transcripts, to a predicted clinical status while providing a corresponding clinical rationale.\nTable 1. Detailed clinical and demographic profiles of the public datasets, real-world, and synthetic cohorts.\nCohorts\nLanguage\nGroup\nSubjects (n)\nAge\nSex (n)\nEducation level\nMMSE\nMoCA\nPublic Datasets\nADReSS\nEnglish\nAD\n78\n66.6 (6.6)\n35 / 43\n‚Äì\n17.8 (5.6)\n‚Äì\nnon-AD\n78\n66.8 (6.3)\n35 / 43\n‚Äì\n29.0 (1.2)\n‚Äì\nADReSSo\nEnglish\nAD\n122\n‚Äì\n‚Äì\n‚Äì\n17.8 (5.5)\n‚Äì\nnon-AD\n115\n‚Äì\n‚Äì\n‚Äì\n29.0 (1.2)\n‚Äì\nReal-world Cohort\nCIR-E\nMandarin\nAD\n46\n75.1 (5.4)\n29 / 17\n3 / 24 / 16 / 3\n23.2 (2.6) 16.3 (2.0)\nMCI\n74\n73.6 (5.4)\n44 / 30\n3 / 18 / 51 / 2\n27.0 (2.1) 22.2 (2.0)\nHC\n33\n71.1 (4.2)\n18 / 15\n2 / 5 / 23 / 3\n28.2 (1.1) 27.0 (1.5)\nSynthetic Cohorts\nSYN-EN\nEnglish\nAD\n500\n73.1 (7.2) 250 / 250 192 / 164 / 104 / 40\n‚Äì\n‚Äì\nnon-AD\n500\n73.1 (7.0) 250 / 250 205 / 146 / 103 / 46\n‚Äì\n‚Äì\nSYN-ZH\nMandarin\nAD\n500\n72.4 (6.8) 250 / 250 189 / 162 / 102 / 47\n‚Äì\n‚Äì\nMCI\n500\n71.9 (7.3) 250 / 250\n198 / 159 / 98 / 45\n‚Äì\n‚Äì\nHC\n500\n72.4 (7.4) 250 / 250\n204 / 151 / 90 / 55\n‚Äì\n‚Äì\n1 Age, MMSE, and MoCA are reported as mean (sd).\n2 Gender is reported as Female / Male counts.\n3 Education: Primary or below / Junior high / High school / University or above.\nData description\nIn this study, we aim to construct multi-dimensional patient personas and generate synthetic speech data across varying cognitive\nlevels. To this end, we utilize three datasets to comprehensively evaluate the proposed framework: two publicly available\nEnglish benchmarks, ADReSS and ADReSSo, and an independently collected Mandarin corpus, CIR-E. The detailed clinical\nand demographic profiles of these cohorts, alongside their synthetic counterparts, are summarized in Table 1. Leveraging these\ndiverse data sources, which vary in diagnostic labeling schemes and disease severity, allows us to validate the cross-lingual\ntransferability and robustness of our framework, ensuring the model accurately captures and generalizes linguistic biomarkers\nacross diverse real-world scenarios.\nADReSS was introduced as part of the INTERSPEECH 2020 ADReSS Challenge16. It consists of speech recordings\nand corresponding transcripts collected from spontaneous picture description tasks based on the ‚ÄúCookie Theft‚Äù picture from\nthe Boston Diagnostic Aphasia Examination. The dataset includes an equal number of participants diagnosed with AD and\ncognitively normal controls, balanced for gender and age to minimize demographic bias. Each audio sample has a duration of\napproximately one to two minutes, with an accompanying transcript. The ADReSS dataset is designed for binary classification\n(AD and non-AD) and has been preprocessed to remove personally identifiable information and background noise.\n3/14\n"}, {"page": 4, "text": "ADReSSo was released for the INTERSPEECH 2021 ADReSSo Challenge11 as an extension of the original ADReSS\ndataset. Unlike ADReSS, which includes both audio and transcripts, ADReSSo focuses solely on the acoustic modality,\nproviding only raw audio recordings for training and evaluation. The dataset contains recordings collected in naturalistic\nsettings, emphasizing robustness against variations in recording conditions and spontaneous speech behaviors. All recordings\nwere normalized and manually screened to ensure consistent audio quality and speaker intelligibility.\nTo assess real-world generalization, we collected an independent Mandarin corpus, CIR-E17, from community-dwelling\nolder adults in Jiangsu, China. CIR-E captures spontaneous speech during a one-minute picture description task. This corpus\ncomprises 153 samples with labels assigned by neurologists according to a ternary scheme of Healthy Control (HC), MCI, and\nAD. For individuals meeting the preliminary screening criteria, the research protocol was explained in detail, and informed\nconsent was obtained. Subsequently, experienced neurologists conducted a comprehensive clinical evaluation, which included\nstandardized cognitive assessments (e.g., MMSE, MoCA) and evaluation of activities of daily living (ADL). Group inclusion\nwas then determined based on the results of these assessments, alongside medical history and physical examination. Specifically,\nparticipants with cognitive impairment were identified through this clinical assessment, while HC were defined as individuals\nwith no subjective memory complaints and no history of major neurological, psychiatric, or metabolic disorders. Strict exclusion\ncriteria (e.g., other neurological or psychiatric disorders, systemic organ failure, cognition-affecting medications) were applied\nto ensure that observed speech characteristics primarily reflect AD-related cognitive decline.\nFigure 2. Distribution of key linguistic biomarkers across synthetic cohorts generated by SynCog. Box plots quantify\nfour representative features mapped to the assessment dimensions and scoring criteria: Total Word Count, Frequency of Spatial\nTerms, Frequency of Filler Words, and Frequency of Vague Terms. These metrics characterize the linguistic profiles of the\ngenerated cohorts across the diagnostic spectrum. (a) Synthetic English dataset contrasting patients with Alzheimer‚Äôs disease\n(AD) and non-AD individuals. (b) Synthetic Mandarin dataset across three diagnostic categories: healthy controls (HC), mild\ncognitive impairment (MCI), and AD. The distinct separation between neurodegenerative groups and healthy controls confirms\nthat the generated text preserves pathological patterns consistent with the established scoring criteria and clinical diagnostic\nstandards.\nSynthetic Cohorts Reproduce Clinical Linguistic and Acoustic Phenotypes\nBefore assessing diagnostic performance, we rigorously validated the quality and clinical fidelity of the multimodal data\ngenerated by SynCog. To systematically investigate the scalability of our framework, we constructed stratified synthetic cohorts\ndesignated as SYN-EN for the English domain and SYN-ZH for the Mandarin domain by progressively increasing the data\nvolume. Specifically, we applied five synthetic data scales, where each diagnostic category within the cohorts was expanded\nfrom 50 to 500 subjects for both the English and Mandarin cohorts. This design yielded a maximum synthetic corpus of 1,000\nsubjects for the binary SYN-EN cohort (AD and non-AD) and 1,500 subjects for the ternary SYN-ZH cohort (AD, MCI,\nand HC). Such proportional generation preserves a balanced distribution across all diagnostic phenotypes while enabling a\nsystematic evaluation of model performance under increasing data scales.\n4/14\n"}, {"page": 5, "text": "60\n40\n20\n0\n20\n40\n30\n20\n10\n0\n10\n20\n30\n40\nADReSSo\nADReSS\nCIR-E\nSYN-EN\nSYN-ZH\nFigure 3. Distributional alignment of synthetic and clinical acoustic embeddings. The t-SNE visualization displays\nhigh-dimensional feature vectors extracted from speech samples using the wav2vec2-base-960h model. The plot reveals two\ndistinct linguistic clusters where the synthetic data shares a substantial manifold overlap with real-world recordings. The\nEnglish cluster comprises the ADReSS and ADReSSo clinical baselines aligned with the synthetic English dataset. The\nMandarin cluster includes the CIR-E real-world cohort aligned with the synthetic Mandarin cohort. The dashed boundaries\nindicate the distributional extent of each subgroup and highlight the preservation of language-specific acoustic phenotypes by\nthe SynCog framework.\nSynthesis of Linguistic Biomarkers\nWe analyzed key linguistic biomarkers associated with neurodegeneration to validate the clinical fidelity of our synthetic\ncohorts. As illustrated in Fig. 2, the generated data successfully recapitulates clinically established pathological trends across\nboth English and Mandarin languages. Specifically, the synthetic AD and MCI groups exhibited a statistically significant\nreduction in total word count and the usage of spatial terms compared to HC (p < 0.001). Conversely, indicators of disfluency\nand anomia, quantified by the frequency of filler words and vague terms, showed a significant increase proportional to cognitive\nseverity (p < 0.001). These distinct distributional shifts confirm that our phenotype modeling captures the progressive nature\nof linguistic impairment rather than merely generating generic descriptions.\nConsistency of Acoustic Distributions\nIn the acoustic domain, the t-SNE visualization shown in Fig. 3 reveals that the embeddings of synthetic speech share a\nsubstantial distributional overlap with real clinical recordings. We observe two distinct primary clusters corresponding to the\nlinguistic domains: an English cluster comprising ADReSS, ADReSSo, and SYN-EN, and a Mandarin cluster containing\nCIR-E and SYN-ZH. Crucially, within each linguistic domain, the synthetic samples exhibit substantial manifold alignment\nwith their respective real-world clinical counterparts. We acknowledge that a slight distributional shift persists, which is likely\nattributable to the idealized acoustic environment of the synthetic generation compared to the background noise inherent in\nclinical recordings. Nevertheless, the extensive overlap indicates that our voice cloning pipeline successfully preserves the\nessential acoustic phenotypes and paralinguistic features required for robust, language-specific model training.\nDiagnostic Performance and Cross-Linguistic Generalization\nTo comprehensively evaluate the efficacy of the proposed framework, we conducted a rigorous comparative analysis against a di-\nverse array of established and contemporary baselines. The evaluated open-source models include Qwen2-Audio-7B-Instruct15,\nMiniCPM-o-2.618, the Llama-3.1-based Ultravox series (v0.5 and v0.6), Phi-4-Multimodal-Instruct19, SeaLLMs-Audio-7B,\nR1-AQA20, as well as the Qwen-Omni series comprising Qwen2.5-Omni-3B, Qwen2.5-Omni-7B21, and Qwen3-Omni-30B-\nInstruct22. Furthermore, to benchmark against the upper bound of current capabilities, we incorporated leading closed-source\nmodels, namely GPT-4o-Audio-Preview23, Gemini-2.5-Flash24, Gemini-3-Flash, and Gemini-3-Pro. A comprehensive sum-\nmary of the model cards for these baselines is presented in Table S1. Table 2 summarizes the diagnostic performance across the\nthree diverse cohorts.\nPerformance on English Benchmarks\nTo evaluate the core efficacy of SynCog in a well-studied linguistic context, we first benchmarked its performance on the\nstandardized English ADReSS and ADReSSo datasets. When trained exclusively on synthetic data, SynCog achieved F1-scores\nof 77.05 (¬± 2.95)% and 73.04 (¬± 2.84)% on these benchmarks, respectively, as detailed in Table 2. Furthermore, SynCog\n5/14\n"}, {"page": 6, "text": "Table 2. Performance comparison of different diagnostic models (%)\nMethod\nEN: ADReSS\nEN: ADReSSo\nZH: CIR-E\nAVG\nMacro-F1\nAVS\nBo8\nMacro-F1\nAVS\nBo8\nMacro-F1\nAVS\nBo8\nMacro-F1\nQwen2-Audio-7B-Instruct\n49.35 (4.38) 54.69 (4.13) 58.33 46.62 (7.63) 53.52 (4.88) 60.56 30.07 (3.96) 46.98 (2.93) 52.94 42.01 (5.32)\nMiniCPM-o-2.6\n40.66 (5.29) 51.82 (3.37) 58.33 41.34 (4.67) 51.41 (2.99) 56.34 28.56 (2.19) 45.67 (2.68) 50.98 36.85 (4.05)\nUltravox-v0.5-Llama-3.1-8b 46.87 (8.91) 55.73 (5.39) 66.67 43.75 (7.97) 52.99 (5.12) 63.38 26.07 (3.43) 45.92 (1.90) 49.02 38.90 (6.77)\nPhi-4-Multimodal-Instruct\n55.43 (5.08) 56.51 (4.47) 62.50 48.91 (4.11) 49.30 (3.98) 53.52 24.73 (1.60) 46.57 (0.91) 48.37 43.02 (3.60)\nQwen2.5-Omni-3B\n60.05 (6.38) 60.94 (6.48) 68.75 58.09 (3.52) 59.33 (3.62) 64.79 28.25 (2.11) 40.52 (3.17) 44.44 48.80 (4.00)\nQwen2.5-Omni-7B\n57.50 (8.39) 61.72 (5.60) 72.92 59.43 (7.98) 61.97 (5.32) 73.24 31.39 (2.18) 42.97 (2.79) 49.02 49.44 (6.18)\nSeaLLMs-Audio-7B\n56.76 (8.27) 58.85 (6.97) 70.83 50.79 (6.80) 53.17 (5.40) 61.97 28.45 (2.43) 33.42 (2.99) 39.22 45.33 (5.83)\nR1-AQA\n49.77 (5.73) 56.25 (3.13) 62.50 50.18 (3.93) 56.34 (2.82) 61.97 27.51 (3.53) 45.10 (3.50) 49.67 42.49 (4.40)\nUltravox-v0.6-Llama-3.1-8b 43.20 (6.12) 52.08 (3.15) 58.33 42.26 (4.52) 50.70 (2.34) 53.52 24.29 (2.20) 46.00 (1.70) 48.37 36.58 (4.28)\nQwen3-Omni-30B-Instruct\n74.35 (2.69) 74.40 (2.66) 77.08 58.83 (1.88) 58.98 (1.92) 61.97 33.12 (2.64) 44.12 (2.19) 46.41 55.44 (2.41)\nGPT-4o-Audio-Preview ‚Ä†\n51.60 (2.98) 58.85 (2.02) 62.50 50.41 (2.12) 59.15 (1.22) 61.97 26.17 (1.33) 33.25 (1.65) 35.29 42.73 (2.14)\nGemini-2.5-Flash ‚Ä†\n63.39 (5.92) 63.54 (5.98) 75.00 67.39 (2.93) 67.43 (2.94) 71.83 32.96 (3.33) 46.98 (1.89) 50.98 54.58 (4.06)\nGemini-3-Flash ‚Ä†\n73.50 (2.82) 73.96 (2.76) 77.08 69.82 (3.45) 70.77 (3.28) 76.06 24.09 (2.27) 29.00 (2.24) 32.03 55.80 (2.89)\nGemini-3-Pro ‚Ä†\n75.79 (4.06) 76.04 (4.03) 83.33 72.17 (4.01) 72.71 (3.85) 80.28 44.72 (1.90) 44.61 (2.14) 47.71 64.23 (3.32)\nSynCog (Ours)\n77.05 (2.95) 77.34 (2.84) 83.33 73.04 (2.29) 73.59 (1.96) 77.46 48.71 (1.55) 49.51 (1.38) 51.63 66.27 (2.34)\n1 ‚Ä† denotes closed-source models.\n2 Metrics are reported as mean (sd) over eight runs with different random seeds.\n3 The best results are bolded, and the second-best are underlined.\nexhibited superior diagnostic efficacy and potential, recording AVS scores of 77.34 (¬± 2.84)% and 73.59 (¬± 1.96)%, alongside\na peak Bo8 performance of 83.33% and 77.46% on the two benchmarks. Notably, these zero-real-data results surpass the\nstrongest closed-source baseline, Gemini-3-Pro, by margins of 1.26% and 0.87%. This achievement directly validates the\ncapacity of our data synthesis pipeline to construct robust diagnostic models for English-speaking populations without relying\non any real-world training samples.\nPerformance on Mandarin Real-world Cohorts\nDetecting fine-grained cognitive decline is inherently challenging due to the subtle, often imperceptible nature of early\nsymptoms. Nevertheless, experimental results on the CIR-E cohort substantiate the robustness of our approach (Table 2).\nSpecifically, SynCog attained superior diagnostic efficacy, securing an overall Macro-F1 score of 48.71 (¬± 1.55)% and an\nAVS of 49.51 (¬± 1.38)%. These results outperform leading closed-source models, including Gemini-3-Pro which recorded a\nMacro-F1 of 44.72 (¬± 1.90)% and an AVS of 44.61 (¬± 2.14)%, and Gemini-2.5-Flash which reached a Macro-F1 of 32.96 (¬±\n3.33)% and an AVS of 46.98 (¬± 1.89)%. While Qwen2-Audio-7B-Instruct exhibited a higher Bo8 score of 52.94%, SynCog\ndelivered a more balanced and consistent diagnostic performance across all evaluated metrics, validating the cross-linguistic\nefficacy of the pipeline.\nTable 3. Stratified diagnostic performance on the CIR-E cohort utilizing\nSYN-ZH (%)\nDiagnostic Objective\nClinical Transition\nSensitivity\nSpecificity\nF1-score\nEarly Detection\nMCI vs. HC\n40.03 (5.19) 76.52 (5.19) 53.02 (4.73)\nDiagnostic Verification AD vs. HC\n67.12 (4.41) 69.70 (7.73) 71.07 (3.74)\nClinical Screening\n(MCI + AD) vs. HC 83.54 (5.80) 46.21 (9.91) 84.15 (2.52)\n1 Metrics are reported as mean (sd) over eight runs with different random seeds.\nTo address the clinical imperative for early identification, we stratified the diagnostic performance across specific clinical\ntransitions, as summarized in Table 3. SynCog demonstrated a robust capability in differentiating early-stage impairment,\nachieving an F1-score of 53.02 (¬± 4.73)% in the discrimination of MCI from HC. While detecting prodromal markers is\nchallenging, the model maintained a high specificity of 76.52 (¬± 5.19)%, ensuring reliable exclusion of healthy controls.\nThis finding is particularly significant given the heterogeneous linguistic markers characterizing the prodromal phase of\ndementia. In the more pronounced AD vs. HC transition, where pathological speech markers are typically more pervasive, the\nmodel delivered a strong F1-score of 71.07 (¬± 3.74)% with balanced sensitivity and specificity. Most notably, in a simulated\nclinical screening scenario, defined as the binary classification of cognitively impaired individuals (comprising MCI and AD)\n6/14\n"}, {"page": 7, "text": "against healthy controls, SynCog achieved a high F1-score of 84.15 (¬± 2.52)%. This performance was underpinned by a high\nsensitivity of 83.54 (¬± 5.80)%, demonstrating the model‚Äôs efficacy as a potent screening tool capable of capturing the vast\nmajority of cognitively impaired cases. These stratified results underscore the diagnostic robustness of SynCog across the entire\ndisease continuum. By explicitly modeling diverse demographic and cognitive profiles during synthesis, SynCog successfully\nbridges the linguistic gap, demonstrating that specialized diagnostic models for under-represented languages can be effectively\nconstructed without relying on scarce real-world annotations.\nImpact of Synthetic Data Scale\nTo explicitly quantify the contribution of the synthetic data generation pipeline, an ablation study was conducted focusing on\ntwo aspects: the scaling ratio of synthetic augmentation and the impact of integrating different data sources.\nFig. 4 illustrates the Average F1-score trajectories across the three datasets under increasing synthetic data scales. We\nobserve a consistent pattern of rapid initial improvement followed by diminishing returns. Specifically, expanding the synthetic\ndata from the baseline (1√ó) to a moderate scale (2√ó) leads to a substantial performance gain, validating the effectiveness of\ndata augmentation. However, a saturation point becomes apparent as the data scale continues to increase; beyond moderate\nexpansion levels, additional synthetic samples result in marginal improvements or even slight performance degradation. These\nresults suggest that while synthetic data is beneficial for model adaptation, identifying an appropriate data scale is crucial to\nbalance enhanced feature diversity against potential distribution shifts introduced by the generative process.\n0√ó\n1√ó\n2√ó\n4√ó\n6√ó\n10√ó\nScale of Synthetic Data\n20\n30\n40\n50\n60\n70\n80\nAvg-F1 (%)\nADReSS\nADReSSo\nCIR-E\nFigure 4. Impact of data augmentation scaling on diagnostic performance. The line graph illustrates the Average F1 score\ntrajectories for the ADReSS, ADReSSo, and CIR-E datasets as the augmentation ratio scales from zero to five times the\nbaseline. The error bars represent the standard deviation across experimental runs. The observed trends demonstrate a rapid\ninitial performance improvement followed by a plateau or slight decline, indicating the existence of an optimal threshold for\nsynthetic data integration.\nThe ablation study further demonstrates that leveraging synthetic phenotypes to augment existing real-world English\nsamples yields significant synergistic improvements across all metrics. Table 4 presents the performance comparison across\ndifferent data source configurations. Notably, models trained exclusively on synthetic samples (w/ Syn Data) consistently\noutperformed those trained on limited real-world datasets (w/ Real Data), with F1-scores improving from 75.96 (¬± 3.51)% and\n72.62 (¬± 2.46)% to 77.05 (¬± 2.95)% and 73.04 (¬± 2.29)% on ADReSS and ADReSSo, respectively. This performance gain\nhighlights the superior quality and diagnostic relevance of the generated synthetic phenotypes. Building upon this foundation,\nthe integration of mixed data (w/ Mix Data), which was constructed by combining the original ADReSS and ADReSSo training\nsets with an optimized proportion of synthetic samples, further elevated diagnostic performance to peak levels, achieving\nF1-scores of 80.67 (¬± 3.26)% and 78.46 (¬± 2.56)%. Simultaneously, this synergistic approach drove substantial gains in\ndiagnostic accuracy, with AVS rising to 80.73% and 78.52%, while the performance ceiling was significantly expanded, as\nevidenced by the elevated Bo8 scores of 87.50% and 83.10%. These findings demonstrate that synthetic data serves not only as\na high-fidelity alternative but also as a catalyst that significantly enhances diagnostic precision in data-scarce clinical settings.\nThis finding suggests that real and synthetic data provide complementary supervision signals. The real data anchors the model to\nthe ground-truth clinical distribution, while the diverse synthetic samples cover the long-tail pathological features and decision\nboundaries that are sparsely represented in limited clinical datasets.\n7/14\n"}, {"page": 8, "text": "Table 4. Ablation study on data source configurations for the SynCog (%)\nMethod\nEN: ADReSS\nEN: ADReSSo\nMacro-F1\nAVS\nBo8\nMacro-F1\nAVS\nBo8\nZero-Shot\n49.35 (4.38) 54.69 (4.13)\n58.33\n46.62 (7.63) 53.52 (4.88)\n60.56\n‚àÜ\n+26.61\n+21.35\n+22.92\n+26.00\n+19.37\n+15.50\nw/ Real Data 75.96 (3.51) 76.04 (3.45)\n81.25\n72.62 (2.46) 72.89 (2.41)\n76.06\n‚àÜ\n+1.09\n+1.30\n+2.08\n+0.42\n+0.70\n+1.40\nw/ Syn Data\n77.05 (2.95) 77.34 (2.84)\n83.33\n73.04 (2.29) 73.59 (1.96)\n77.46\n‚àÜ\n+3.62\n+3.39\n+4.17\n+5.42\n+4.93\n+5.64\nw/ Mix Data\n80.67 (3.26) 80.73 (3.25)\n87.50\n78.46 (2.56) 78.52 (2.51)\n83.10\n1 Metrics are reported as mean (sd) over eight runs with different random seeds.\nDiscussion\nThis study introduces SynCog, a novel framework that bridges the gap between generative AI and clinical utility by integrating\npersona-driven phenotypic simulation with reasoning self-distillation. Diverging from prior studies constrained by surface-level\ndata augmentation, our approach constructs clinically high-fidelity digital subjects anchored in established pathological profiles.\nThis innovation enables the training of robust diagnostic models that exhibit superior diagnostic efficacy, outperforming leading\nclosed-source baselines such as Gemini-3-Pro, while effectively mitigating the privacy risks and scarcity bottlenecks inherent\nin sensitive clinical data. By demonstrating that generative simulation can serve as a primary source of reliable supervision,\nSynCog offers a transformative perspective on digital phenotyping and challenges the conventional dependency on large-scale,\nmanually annotated clinical datasets.\nThe superior performance of our framework stems from the synergistic supervision provided by the integration of real\nand synthetic data. As evidenced by our ablation studies, while synthetic data alone enables robust generalization, the hybrid\napproach yields the most accurate diagnostic boundaries. We attribute this to the complementary roles of the two data sources:\nreal data serves as a distributional anchor to stabilize the model against artifacts, while diverse synthetic samples densify the\nfeature space. This densification covers long-tail pathological patterns, such as specific syntactic deficits or subtle paralinguistic\ncues, which are often sparsely represented in limited clinical cohorts. Furthermore, the distinct overlap of acoustic feature\ndistributions confirms that our voice cloning pipeline successfully preserves essential disease-specific phenotypes. However,\nwe observed a performance plateau and subsequent decline with excessive synthetic augmentation. This suggests that data\nbalance is critical; an optimal ratio is required to maximize feature diversity while minimizing the risk of introducing synthetic\nartifacts that could skew the clinical distribution.\nBeyond diagnostic accuracy, the clinical utility of an AI tool relies heavily on trust and transparency. Traditional deep\nlearning models often function as opaque systems that provide probability scores without actionable rationale. Through\nreasoning self-distillation, SynCog transforms this paradigm by explicating the diagnostic pathway. By learning to articulate\nspecific linguistic evidence, such as empty speech or circumlocution, consistent with the final prediction, our model mimics the\ncognitive process of a clinician. This interpretability is not merely a technical feature but a prerequisite for integration into\nclinical workflows, as it empowers physicians to validate AI-generated assessments and mitigates the risk of unsubstantiated or\nhallucinated diagnoses.\nCurrent digital health research is predominantly centered on English-speaking populations, exacerbating global health\ndisparities. The robust performance of SynCog on the Mandarin CIR-E cohort highlights a scalable pathway to bridge this\nlinguistic gap. Unlike traditional methods that require collecting expensive clinical datasets for every target language, our\nframework facilitates rapid adaptation through persona-based synthesis. By simply adjusting the demographic and linguistic\nparameters of the generative prompt, we can construct specialized diagnostic models for under-represented languages or dialects.\nThis capability is instrumental in fostering global health equity, offering a cost-effective solution to deploy standardized\ncognitive screening tools in low-resource settings.\nThe non-invasive and cost-effective nature of SynCog positions it as an ideal candidate for large-scale community screening\nand telemedicine. As a digital biomarker, speech can be collected remotely via ubiquitous mobile devices, significantly\nlowering accessibility barriers compared to invasive biomarkers or neuroimaging. In a tiered healthcare system, our framework\ncould serve as an accessible gatekeeper to identify individuals with early cognitive decline who warrant further specialized\nexamination, such as positron emission tomography (PET) scans or cerebrospinal fluid (CSF) analysis. This proactive screening\ncapability is particularly vital for aging societies, enabling timely intervention and more efficient clinical resource allocation.\nDespite the promising results, our study has several limitations that warrant consideration.\nOne significant limitation lies in the potential biological disconnect within the acoustic synthesis pipeline. Neurodegenerative\n8/14\n"}, {"page": 9, "text": "disorders entail not only linguistic simplification but also profound deficits in neuromuscular control. These impairments\noften manifest as micro-tremors, irregular articulatory breakdowns, and hesitancy induced by cognitive load. However, our\ncurrent two-stage framework generates narratives via large language models followed by a separate acoustic synthesis step.\nThis separation may inadvertently decouple linguistic content from these biological constraints. While the employed voice\ncloning technique effectively captures the static timbre and general voice quality of elderly speakers, it may not fully replicate\nthe dynamic pathological prosody stemming from actual brain lesions. Consequently, the synthetic speech might lack specific\nfine-grained motor speech markers and potentially creates a domain gap when the model is applied to real-world patients\nexhibiting complex neuro-acoustic symptoms.\nFurthermore, the scope of our evaluation is currently confined to the picture description paradigm. While this task\nsignificantly simplifies the assessment workflow, relying on a single neuropsychological probe restricts the holistic evaluation\nof cognitive function. A comprehensive diagnosis typically encompasses multiple cognitive domains, including verbal fluency,\nepisodic memory recall, and executive function tasks. Future iterations of SynCog will aim to extend the generative framework\nto encompass a broader battery of cognitive tests and thereby enhance the ecological validity of the screening tool.\nA final challenge pertains to clinical safety and the risk of reasoning hallucination. Although CoT fine-tuning significantly\nimproves interpretability, there remains a possibility that the model could generate a plausible-sounding medical rationale\nthat does not accurately reflect the internal features driving its prediction. This phenomenon is known as the unfaithfulness of\nexplanation and poses a challenge for clinical integration. Future work will focus on integrating mechanistic interpretability\ntechniques to ensure that the generated natural language explanations are causally aligned with the internal decision-making\nprocesses of the model.\nIn conclusion, SynCog provides a unified, privacy-preserving, and interpretable framework for AI-driven cognitive\nassessment. By harmonizing generative simulation with clinical reasoning, we demonstrate that high-performance diagnostic\ntools can be built with minimal reliance on real-world data. This work lays the foundation for the next generation of ethical and\nscalable digital health solutions, moving us closer to the goal of accessible cognitive healthcare for all.\nMethods\nDiagnostic Task Formulation\nWe formulate the cognitive assessment of picture description tasks as a supervised classification problem. Given D =\n{(xi,yi)}M\ni=1, each sample xi = (ai,ti) contains an audio recording ai and its transcript ti, with label yi indicating the cognitive\nstatus. Our primary objective is to learn a mapping function f(xi) ‚ÜíÀÜyi to assess cognitive status.\nTo adapt this classification task to MLLMs, we recast it as a structured text generation problem. Given a prompt template Pcls\nthat embeds the sample xi, the MLLM generates a response Si = LLM(Pcls(xi)). The output Si is constrained to a predefined\nformat, from which the predicted label ÀÜyi can be parsed.\nData Synthesis\nPersona-Based Digital Phenotyping\nTo account for inter-subject variability in cognitive assessments, we construct a controllable persona model that conditions the\ngenerative process. Each persona is defined by demographic attributes, including gender, age, and education level, together\nwith the cognitive status label. These factors are motivated by clinical evidence indicating that demographic and educational\ncharacteristics substantially influence linguistic and cognitive performance25‚Äì28.\nBeyond demographics, each persona is characterized by a five-dimensional linguistic style vector v = (v1,v2,v3,v4,v5),\nencompassing narrative length, syntactic complexity, spatial reference, fluency, and clarity. Specifically, v is formulated as a\nquantized style embedding that encapsulates cognitive-functional signatures into a latent representation. To enable granular\ncontrol, each dimension is discretized into three ordinal levels: poor, normal, and good. The style parameters are sampled\nfrom truncated Gaussian distributions, where the latent means are anchored to the cognitive status and further modulated by\ndemographic factors. Formally, the continuous latent style Àúvk is sampled as:\nÀúvk ‚àºN\n\u0010\n¬µ(y)\nk\n‚àíŒ±k g(age)+Œ≤k h(edu), œÉ2\nk\n\u0011\n,\n(1)\nThe final discrete style level vk is obtained through clipping and rounding operations:\nvk = Round(Clip(Àúvk,1,3)),\n(2)\nwhere Àúvk denotes the continuous latent variable, ¬µ(y)\nk\nrepresents the baseline mean for cognitive status y, and Œ±k and Œ≤k are\nsensitivity coefficients controlling the influence of age and education respectively. The functions g(age) and h(edu) represent\nthe mappings of age and education level, while œÉ2\nk is the variance capturing natural variability. The Clip(¬∑) function constrains\n9/14\n"}, {"page": 10, "text": "the value to the range [1,3], and Round(¬∑) quantizes the continuous value into a discrete integer in {1,2,3}, corresponding to\nthe three ordinal levels.\nThe standardized scoring rubrics governing each linguistic dimension are detailed in Table 5. To ensure that the synthetic\nsamples reflect realistic variability across subjects while remaining consistent with established clinical observations, the\nresulting persona specification is embedded into a structured prompt template for LLM generation (See Supplementary Fig. S1\nfor prompt details).\nText Generation\nThe synthesized persona specification is integrated into a structured prompt to guide the LLM-based text generation, ensuring\nthat the outputs capture the demographic and cognitive heterogeneity inherent in clinical populations. In this framework, the\nadvanced MLLM GPT-4o is utilized to generate picture description narratives conditioned jointly on the visual stimulus I and\nthe constructed persona pi.\nFormally, given an image I and its associated persona specification pi, the generation process can be formulated as\nTi = LLMgpt\n\u0000Psyn(I, pi)\n\u0001\n,\n(3)\nwhere Psyn denotes the prompt construction function that embeds demographic attributes and linguistic style parameters into a\ntextual instruction, LLMgpt(¬∑) represents the GPT-4o, and Ti is the generated picture description.\nTo ensure clinical fidelity, the prompt design explicitly incorporates the five-dimensional linguistic style vector to guide\nthe model toward emulating colloquial oral discourse rather than formal, structured summaries. This formulation enables\ncontrollable text generation that accurately reflects the spontaneous linguistic variability and pathological markers observed in\nreal-world subjects.\nSpeech Synthesis\nTo transform the generated narratives into acoustic signals, a specialized geriatric timbre library was curated specifically to\npreserve the acoustic integrity of elderly speech. This library was constructed by selecting reference audios from high-quality\nclinical corpora and public datasets29,30, ensuring an age-range match with the target persona to encapsulate representative\ngeriatric vocal traits. To ensure clean and uniform audio quality, a rigorous preprocessing pipeline was implemented, which\nincorporated adaptive noise reduction, silence removal, and sampling-rate normalization. The resulting repository comprises\n551 female and 436 male distinctive timbre profiles, capturing a wide spectrum of geriatric acoustic characteristics, such as\nincreased jitter, shimmer, and reduced fundamental frequency, which are representative of the target demographic.\nThe objective of this stage is to ensure that the synthesized audio not only conveys the linguistic content Ti but also adheres\nto the demographic constraints defined in the persona specification pi, particularly regarding gender and age-specific vocal\nsignatures. IndexTTS231 is employed for reference-based voice cloning. Given the generated text and a sex-matched reference\naudio sample aref\ni\nfrom the library, the synthetic waveform ÀÜai is generated as:\nÀÜai = TIndex(Ti,aref\ni ),\n(4)\nHere T (¬∑) denotes the IndexTTS2 inference function, which performs reference-based voice cloning: it replicates the speaker‚Äôs\ntimbre from the reference audio aref\ni\nwhile preserving the prosody and linguistic content of Ti.\nCoT deduction Fine-Tuning\nDirect diagnostic label prediction using MLLMs often exhibits instability and limited generalization, as models may gravitate\ntoward superficial linguistic cues rather than cognitively substantive markers. To mitigate this risk of shortcut learning, we\nimplement a reasoning distillation framework that integrates CoT paradigm with SFT (see Supplementary Fig. S2 for details).\nThis approach guides the model to develop interpretable and robust diagnostic pathways by mimicking clinical deliberative\nprocesses.\nWithin this framework, the model M is tasked with generating explicit diagnostic rationales that bridge multimodal\nevidence comprising both linguistic and acoustic features to the final assessment. Given a multimodal input (ai,ti) and its\ncorresponding ground-truth label yi, the model is prompted to infer a reasoning sequence ri that elucidates the underlying\ndecision logic:\nri = LLM\n\u0000Pcot(ai,ti,yi)\n\u0001\n.\n(5)\nThe generated reasoning traces ri are then paired with their corresponding inputs and labels to construct a reasoning-\naugmented dataset:\nDself-cot = {(ai,ti,ri,yi)}M\ni=1.\n(6)\n10/14\n"}, {"page": 11, "text": "Table 5. Assessment dimensions and scoring criteria\nScore Narrative Length\nSyntactic Complexity\nSpatial Expressions\nSpeech Fluency\nClarity of Expression\n1\n60‚Äì110 words; only 2‚Äì\n3 key elements; minimal\ndescriptive detail.\nPredominantly short, sim-\nple sentences; rare con-\njunctions; absence of sub-\nordinate clauses.\n0‚Äì1\nspatial\nreference;\nvague or imprecise ex-\npression.\n‚â•4\nfillers\nor\nword-\nsearch events; frequent\npauses/elongations;\ndisrupted speech flow.\n‚â•4 vague terms; ‚â§1 key\nnoun; frequent pronoun\nreliance.\n2\n90‚Äì150 words;\ncovers\nmain content with 3‚Äì4 de-\nscriptive details.\nMix of short and medium-\nlength sentences; occa-\nsional conjunctions; may\ncontain simple subordi-\nnate clauses.\n1‚Äì2 explicit spatial refer-\nences; basic relative posi-\ntioning expressed.\n2‚Äì3 disfluency events; oc-\ncasional pauses or self-\nrepairs;\noverall under-\nstandable.\n2‚Äì3 vague terms; 2‚Äì3 key\nnouns; occasional pro-\nnoun substitution.\n3\n120‚Äì200\nwords;\nrela-\ntively complete descrip-\ntion with\n‚â•4 details;\nwell-organized structure.\nAlternation of short and\nlong sentences; inclusion\nof causal, temporal, or\nconditional relations; co-\nherent cohesion.\n‚â•3 explicit spatial refer-\nences; comprehensive ex-\npression of spatial rela-\ntions.\n‚â§1 disfluency event; nat-\nural speech rate; min-\nimal pauses or word-\nsearching.\n‚â§1 vague term; ‚â•3 key\nnouns;\npredominantly\nspecific references.\nThis dataset serves as structured supervision, enabling the model to learn a mapping from complex multimodal features to\nclinical rationales. By fine-tuning on Dself-cot, the foundational model transitions into the specialized diagnostic model M,\nevolving from a black-box predictor into a transparent tool capable of articulating an evidentiary basis. The comprehensive\ntraining pipeline is detailed in Algorithm 1.\nDiagnostic Inference\nDuring the inference phase, the fine-tuned model M is presented with multimodal inputs consisting of an audio recording\naj and its corresponding transcript tj. Rather than producing a solitary diagnostic category, the model is prompted to first\narticulate a reasoning sequence rj. This heuristic reasoning ensures that the final assessment is grounded in specific pathological\nmarkers, such as reduced syntactic complexity, frequent speech disfluency, or diminished clarity of expression. The integration\nof CoT deduction effectively mitigates the risk of shortcut learning by forcing the model to align multimodal evidence with\nthe standardized scoring rubrics. By synthesizing evidence from both acoustic prosody and linguistic content, the model\ngenerates a structured response Sj, from which the final cognitive status ÀÜy j is parsed (Supplementary Fig. S3). This transparent\ninference pathway not only enhances the robustness of the diagnostic results across different linguistic contexts but also provides\nclinicians with interpretable evidence to support the final assessment.\nData Pre-processing\nA unified preprocessing pipeline was implemented to ensure consistent data quality and the preservation of diagnostic\nmarkers across all evaluation cohorts. To standardize the acoustic input, raw audio recordings were first converted to a\nmono-channel format and resampled to 16 kHz. The participant‚Äôs speech was isolated from the original dialogues using the\nspeaker-diarization-3.1 pipeline32, which effectively removed interviewer prompts and instructions throughout the task. Since\ninterviewer involvement was primarily limited to providing instructions, the extraction of participant-specific segments ensures\nthe clinical integrity and completeness of the diagnostic content.\nThe isolated acoustic sequences were subsequently transcribed using the faster-whisper model to generate verbatim\ntranscripts. During this process, we applied standard punctuation and case normalization while explicitly retaining all\ndisfluencies and filler words (e.g., ‚Äúuh‚Äù, ‚Äúum‚Äù). Preserving these hesitation markers is crucial for capturing cognitive deficits\ncharacterized by speech fragmentation, as they serve as vital linguistic biomarkers for neurodegenerative assessment. To\naccommodate the input constraints of LLMs, the total duration of each concatenated audio sample was capped at 90 seconds.\nEach processed sample was ultimately represented as a synchronized multimodal pair (ai,ti), comprising the participant‚Äôs full\nconcatenated acoustic sequence and the corresponding verbatim transcript.\nEvaluation Metrics\nTo comprehensively evaluate the robustness and efficacy of the proposed method, each model is executed for N independent\nstochastic rollouts. We employ three distinct metrics to quantify performance: the Macro F1-score (Macro-F1), the Average\nScore (AVS), and the Best-of-N performance (BoN).\nFirst, we define the standard classification statistics for each sample: True Positives (TP), False Positives (FP), True\nNegatives (TN), and False Negatives (FN). Based on these, the per-class precision (Pc) and recall (Rc) are computed as:\n11/14\n"}, {"page": 12, "text": "Algorithm 1 Pipeline of the SynCog Framework\nRequire: Dataset D = {((ai,ti),yi)}N\ni=1; image stimuli {I},V; LLM M; prompts {Psyn,Pcls,Pcot}\nEnsure: Fine-tuned model M; predictions ÀÜy\n1: Step 1: Persona-Based Data Synthesis\n2: for i = 1 to Nsyn do\n3:\nSample demographics & style pi conditioned on target yi\n4:\nTi ‚ÜêLLMgpt(Psyn(Ii, pi))\n5:\nSelect rref\ni\n‚ààV (sex-matched)\n6:\nÀÜai ‚ÜêTIndex(Ti,rref\ni )\n7:\nD‚Ä≤ ‚ÜêD ‚à™{(( ÀÜai,Ti),yi)}\n8: end for\n9: Step 2: CoT deduction Fine-Tuning\n10: Initialize Dself-cot ‚Üê/0\n11: for each (xi,yi) ‚ààD‚Ä≤ do\n12:\nri ‚ÜêM(Pcot(ai,ti,yi))\n13:\nDself-cot ‚ÜêD‚Ä≤ ‚à™{(ai,ti,ri,yi)}\n14: end for\n15: Fine-tune M on Dself-cot with SFT\n16: Step 3: Inference for Cognitive Impairment Assessment\n17: for each test sample xj = (a j,t j) do\n18:\nSj ‚ÜêM(Pcls(xj))\n19:\nParse ÀÜy j from Sj\n20:\nreturn ÀÜy\n21: end for\nPc =\nTPc\nTPc +FPc\n,\nRc =\nTPc\nTPc +FNc\n,\n(7)\nand the per-sample accuracy is:\nAcci =\nTPi +TNi\nTPi +TNi +FPi +FNi\n.\n(8)\nMacro F1-score (Macro-F1)\nTo assess the classification performance of the model while accounting for potential class imbalance, we utilize the Macro\nF1-score. It is calculated as the arithmetic mean of the per-class F1 scores over all N rollouts:\nF1 = 1\nN\nN\n‚àë\nn=1\n \n1\nC\nC\n‚àë\nc=1\n2¬∑P(n)\nc\n¬∑R(n)\nc\nP(n)\nc\n+R(n)\nc\n!\n,\n(9)\nwhere C is the number of classes, and P(n)\nc\nand R(n)\nc\ndenote the precision and recall for class c in rollout n.\nAverage Score (AVS)\nThe Average Score serves as a measure of the expected performance and stability of the model across repeated trials. Let Acc(n)\ni\ndenote the accuracy of the i-th sample in rollout n, where i ‚àà{1,...,M} and n ‚àà{1,...,N}. The AVS is computed as the mean\naccuracy over all samples and all N rollouts:\nAVS = 1\nN\nN\n‚àë\nn=1\n \n1\nM\nM\n‚àë\ni=1\nAcc(n)\ni\n!\n,\n(10)\nwhere M is the total number of samples.\n12/14\n"}, {"page": 13, "text": "Best-of-N (BoN)\nTo evaluate the peak potential of the generation capability of the model, we report the BoN metric. This metric captures the\nupper bound of performance by selecting the maximum accuracy achieved among the N rollouts:\nBoN =\nmax\nn=1,2,...,N\n \n1\nM\nM\n‚àë\ni=1\nAcc(n)\ni\n!\n,\n(11)\nwhere Acc(n)\ni\ndenotes the accuracy of the i-th sample in rollout n. BoN provides an estimate of the maximum achievable\nperformance of the model.\nImplementation Details\nFor all LLM-based inferences, decoding parameters were standardized across models to ensure a fair comparison. Inference\nwas conducted in bfloat16 precision to reduce memory consumption, with a rollout performed N = 8 times.\nDuring fine-tuning, we employed the AdamW optimizer. The model was trained for 5 epochs, with the learning rate\ngradually decayed following a cosine annealing schedule. The batch size per device was set to 1, and gradient accumulation\nsteps were set to 8, resulting in an effective batch size of 8. The dropout rate was set to 0.1. To determine the optimal\nhyperparameters, we conducted a grid search over the learning rate and the LoRA rank r. The learning rate candidates were\n{1e‚àí3, 5e‚àí4, 1e‚àí4, 5e‚àí5, 1e‚àí5}, and the candidate ranks were r ‚àà{8, 16, 32}. The scaling factor Œ± in LoRA was set to\ntwice the selected rank r, and LoRA was applied to all attention projection layers, including the query, key, value, and output\nprojections. All experiments were implemented on 8 NVIDIA RTX 3090 GPUs.\nCode availability\nThe source code supporting this study is publicly available at https://github.com/FengRui1998/SynCog.\nReferences\n1. Gustavsson, A. et al. Global estimates on the number of persons across the alzheimer‚Äôs disease continuum. Alzheimer‚Äôs &\nDementia 19, 658‚Äì670 (2023).\n2. Better, M. A. Alzheimer‚Äôs disease facts and figures. Alzheimers Dement 19, 1598‚Äì1695 (2023).\n3. Valletta, M. et al. Blood biomarkers of alzheimer‚Äôs disease and progression across different stages of cognitive decline in\nthe community. Nat. Commun. (2025).\n4. Arevalo-Rodriguez, I. et al. Mini-mental state examination (mmse) for the detection of alzheimer‚Äôs disease and other\ndementias in people with mild cognitive impairment (mci). Cochrane database systematic reviews (2015).\n5. S√°nchez-Benavides, G. et al. Identification of underlying ad pathology in cognitively unimpaired individuals with subjective\ncognitive decline by the means of automated speech and language processing. Alzheimer‚Äôs & Dementia 20, e094373\n(2024).\n6. Shafiyan, S. et al. Revisiting the cookie theft picture for cognitive impairment: Assessing its relevance for discourse\nanalysis after four decades. Alzheimer‚Äôs & Dementia 21, e106522 (2025).\n7. Lima, M. R. et al. Evaluating spoken language as a biomarker for automated screening of cognitive impairment. Commun.\nMedicine (2025).\n8. Casu, F., Lagorio, A., Ruiu, P., Trunfio, G. A. & Grosso, E. Integrating fine-tuned llm with acoustic features for enhanced\ndetection of alzheimer‚Äôs disease. IEEE J. Biomed. Heal. Informatics (2025).\n9. McDuff, D. et al. Towards accurate differential diagnosis with large language models. Nature 1‚Äì7 (2025).\n10. Du, X. et al. Enhancing early detection of cognitive decline in the elderly: a comparative study utilizing large language\nmodels in clinical notes. EBioMedicine 109 (2024).\n11. Luz, S., Haider, F., De la Fuente, S., Fromm, D. & MacWhinney, B. Detecting cognitive decline using speech only: The\nadresso challenge. arXiv preprint arXiv:2104.09356 (2021).\n12. Huang, L. et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.\nACM Transactions on Inf. Syst. 43, 1‚Äì55 (2025).\n13. Wang, P. et al. Llm-autoda: Large language model-driven automatic data augmentation for long-tailed problems. Adv.\nNeural Inf. Process. Syst. 37, 64915‚Äì64941 (2024).\n13/14\n"}, {"page": 14, "text": "14. Wu, S.-L. et al. Improving audio captioning models with fine-grained audio features, text embedding supervision, and llm\nmix-up augmentation. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 316‚Äì320 (IEEE, 2024).\n15. Chu, Y. et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759 (2024).\n16. Luz, S., Haider, F., de la Fuente, S., Fromm, D. & MacWhinney, B. Alzheimer‚Äôs dementia recognition through spontaneous\nspeech: The adress challenge. arXiv preprint arXiv:2004.06833 (2020).\n17. Feng, R. et al. Cogbench: A large language model benchmark for multilingual speech-based cognitive impairment\nassessment. arXiv preprint arXiv:2508.03360 (2025).\n18. Yao, Y. et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800 (2024).\n19. Abdin, M. et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905 (2024).\n20. Li, G. et al. Reinforcement learning outperforms supervised fine-tuning: A case study on audio question answering. arXiv\npreprint arXiv:2503.11197 (2025).\n21. Xu, J. et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215 (2025).\n22. Xu, J. et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765 (2025).\n23. Hurst, A. et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024).\n24. Comanici, G. et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next\ngeneration agentic capabilities. arXiv preprint arXiv:2507.06261 (2025).\n25. Long, M. R., Vega-Mendoza, M., Rohde, H., Sorace, A. & Bak, T. H. Understudied factors contributing to variability in\ncognitive performance related to language learning. Biling. Lang. Cogn. 23, 801‚Äì811 (2020).\n26. Kim, M. & Park, J.-M. Factors affecting cognitive function according to gender in community-dwelling elderly individuals.\nEpidemiol. health 39, e2017054 (2017).\n27. Bennett, D. A. et al. Education modifies the relation of ad pathology to level of cognitive function in older persons.\nNeurology 60, 1909‚Äì1915 (2003).\n28. Podcasy, J. L. & Epperson, C. N. Considering sex and gender in alzheimer disease and other dementias. Dialogues clinical\nneuroscience 18, 437‚Äì446 (2016).\n29. Liang, T. et al. Construction and evaluation of an emotion-inducing video dataset towards chinese elderly healthy controls\nand individuals with mild cognitive impairment. Cogn. neurodynamics 19, 154 (2025).\n30. Chen, X., Zhang, W.-Q. & Ma, Y. Raw waveform-based end-to-end alzheimer‚Äôs disease detection method. Acta Electron.\nSinica 51, 3582‚Äì3590 (2023).\n31. Zhou, S. et al. Indextts2: A breakthrough in emotionally expressive and duration-controlled auto-regressive zero-shot\ntext-to-speech. arXiv preprint arXiv:2506.21619 (2025).\n32. Plaquet, A. & Bredin, H.\nPowerset multi-class cross entropy loss for neural speaker diarization.\narXiv preprint\narXiv:2310.13025 (2023).\n14/14\n"}, {"page": 15, "text": "Supplementary Information:\nCross-Linguistic Persona-Driven Data Synthesis for Robust Multi-\nmodal Cognitive Decline Detection\nA Prompt Design\nIn this section, we provide the detailed prompt templates employed in the SynCog framework. The prompt engineering strategy\nis divided into three distinct stages: persona-based data synthesis, reasoning extraction, and diagnostic inference.\n<|im_start|>system\nYou will simulate an elderly, community-dwelling participant performing the picture description task\nin a cognitive assessment. Given the image, produce one natural, conversational utterance that could\nplausibly be said by this participant.\nYou must strictly follow the participant characteristics and language parameters that will be provided\nlater. <|im_end|>\n<|im_start|>user\n<img_start>image.png [Cookie Theft picture]<img_end>\n## Persona\nGender: {sex}\nAge: {age}\nEducation: {edu_level}\nCognitive status: {label}\n## Language parameters\n[Based on the scoring criteria table]\nAs the participant described above, describe the given image following all persona and language\nparameters. <|im_end|>\n<|im_start|>assistant\nThe Prompt Template for Persona-Based Text Generation\nFigure S1. Prompt Template for Persona-Based Text Generation. The prompt conditions the text generation on specific\ndemographic attributes and a discrete linguistic style vector. This mechanism enforces the production of narratives that reflect\nspecific cognitive deficits rather than generic descriptions.\nTo generate diverse and clinically plausible synthetic phenotypes, we designed a persona-conditioned generation prompt\n(Psyn), as illustrated in Fig. S1. This template acts as a bridge between the structured persona specifications and natural language\nnarratives. It explicitly integrates the sampled demographic attributes (e.g., age, education) and the five-dimensional linguistic\nstyle vector into the instruction. By conditioning the LLM on these fine-grained parameters, we ensure that the generated\npicture descriptions strictly adhere to the simulated cognitive profile rather than reverting to the model‚Äôs default, high-quality\nwriting style.\nFor the construction of the reasoning-augmented dataset, we utilized a label-conditioned prompt (Pcot), shown in Fig. S2.\nUnlike the inference stage, this template incorporates the ground-truth diagnosis to guide the model in a \"hindsight analysis\"\nprocess. The prompt instructs the model to reverse-engineer the diagnostic logic, explicitly articulating the specific linguistic\nand acoustic evidence that supports the known label. These high-quality, evidence-based reasoning traces are subsequently\nused as supervision targets for the CoT fine-tuning.\nFinally, to facilitate the actual diagnostic task during the testing phase, we designed a zero-shot inference prompt (Pcls),\nas illustrated in Fig. S3. This template instructs the LLM to act as a cognitive assessment specialist. It is provided with the\nmultimodal input without the clinical label. The model is tasked with analyzing the linguistic patterns to predict the cognitive\nstatus and, thanks to the prior CoT fine-tuning, spontaneously generating the supporting rationale.\nS1/S2\n"}, {"page": 16, "text": "<|im_start|>system\nYou are an experienced cognitive assessment expert with a profound background in linguistics and\nneuropsychology.\nYour role is to analyze the subject's vocal behavior and linguistic expression during cognitive\nimpairment assessments, and comprehensively judge their cognitive function status.\nYou will receive the following information:\n1. cognitive status: for internal use only to guide analysis toward this status (without inventing or\naltering observations). Do not disclose the known cognitive status.\n2. raw audio: The audio recording of the subject performing an image description task (may contain\nbackground noise).\n3. ASR transcript: The automatic speech recognition text of the raw audio (may include recognition\nerrors or missing fillers). <|im_end|>\n<|im_start|>user\nThe known cognitive function statu of this subject: {Label}.\nThe following is this subject's raw audio:\n<|audio_start|>audio.wav [audio file of the subject's picture description task]<|audio_end|>\nThe following is this subject's ASR transcript:\n{ASR transcript}\nPlease combine the subject's acoustic and linguistic features in the audio and transcript, analyze and\nreason step-by-step, explain the rationale for your judgment, and ultimately output the cognitive\nstatus result.\nList the acoustic and linguistic evidence that supports the conclusion first, and then derive a\nconclusion.\n<|im_end|>\n<|im_start|>assistant\nThe Prompt Template for Chain-of-Thought Generation\nFigure S2. Prompt template for label-conditioned reasoning generation. This template is utilized during the training data\nconstruction phase. By incorporating the ground-truth diagnostic label, the prompt guides the model to perform hindsight\nanalysis, explicitly articulating the linguistic and logical evidence that supports the correct diagnosis. The resulting output\nserves as the CoT supervision.\nB Model Card\nTo ensure benchmarking transparency and reproducibility, we provide specifications for the Multimodal Large Language\nModels (MLLMs) evaluated in this study (Table S1). The selection encompasses a representative spectrum of architectures:\n‚Ä¢ Open-source Models: Includes the Qwen, Ultravox, Phi-4, and MiniCPM series, ranging from 3B to 30B parameters. These\nmodels facilitate assessments of diagnostic scaling and cross-architectural generalizability.\n‚Ä¢ Closed-source Models: Includes GPT and the Gemini series. These models represent current leading benchmarks in\nmultimodal inference. Due to their commercial nature, their internal architectural parameters are undisclosed, and they are\naccessed exclusively via API interfaces.\nS2/S2\n"}, {"page": 17, "text": "<|im_start|>system\nYou are an experienced cognitive assessment expert with a profound background in linguistics and\nneuropsychology.\nYour role is to analyze the subject's vocal behavior and linguistic expression during cognitive\nimpairment assessments, and comprehensively judge their cognitive function status.<|im_end|>\n<|im_start|>user\nYou will receive the raw audio and its corresponding ASR transcript from an elderly subject\nperforming a cognitive assessment task.\n1. raw audio: The audio recording of the subject performing an image description task (may contain\nbackground noise).\n2. ASR transcript: The automatic speech recognition text of the raw audio (may include recognition\nerrors or missing fillers).\nThe following is this subject's raw audio:\n<|audio_start|>audio.wav [audio file of the subject's picture description task]<|audio_end|>\nThe following is this subject's ASR transcript:\n{ASR transcript}\nAnalyze the provided data and infer which cognitive function status the subject is most likely to have:\nA. Normal Cognitive Function\nB. Alzheimer's Disease\nPlease combine the subject's acoustic and linguistic features in the audio and transcript, analyze and\nreason step-by-step, explain the rationale for your judgment, and ultimately output the categorical\nresult.\nPlease strictly follow the JSON output format below. In '<Reasoning>' , write out the reasoning\nprocess. '<Option>' must be one of the letters 'A' or 'B', each corresponding to one of the two\ncognitive function statuses.\n{\n\"Rationale\": \"<Reasoning>\",\n\"Cognitive Function Status\": \"<Option>\"\n}\n<|im_end|>\n<|im_start|>assistant\nThe Prompt Template for Inference in Cognitive Assessment\nFigure S3. Prompt template for multimodal diagnostic inference. The LLM receives the task instruction along with the\nparticipant‚Äôs audio and text records. The model is instructed to analyze linguistic patterns and generate a predicted cognitive\nstatus.\nS3/S2\n"}, {"page": 18, "text": "Table S1. Model cards for MLLMs\nModel\n# Params\nLink\nMiniCPM-o-2.6\n8B\nhttps://huggingface.co/openbmb/MiniCPM-o-2_6\nUltravox-v0.5-llama-3.1-8b\n8B\nhttps://huggingface.co/fixie-ai/ultravox-v0_5-llama-3_1-8b\nUltravox-v0.6-Llama-3.1-8b\n8B\nhttps://huggingface.co/fixie-ai/ultravox-v0_6-llama-3_1-8b\nPhi-4-Multimodal-Instruct\n5B\nhttps://huggingface.co/microsoft/Phi-4-multimodal-instruct\nR1-AQA\n7B\nhttps://huggingface.co/mispeech/r1-aqa\nSeaLLMs-Audio-7B\n7B\nhttps://huggingface.co/SeaLLMs/SeaLLMs-Audio-7B\nQwen2-Audio-7B-Instruct\n7B\nhttps://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\nQwen2.5-Omni-3B\n3B\nhttps://huggingface.co/Qwen/Qwen2.5-Omni-3B\nQwen2.5-Omni-7B\n7B\nhttps://huggingface.co/Qwen/Qwen2.5-Omni-7B\nQwen3-Omni-30B-Instruct\n30B\nhttps://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct\nGPT-4o-Audio-Preview\n‚Äì\nhttps://platform.openai.com/docs/models/gpt-4o\nGemini-2.5-Flash\n‚Äì\nhttps://deepmind.google/technologies/gemini/\nGemini-3-Flash\n‚Äì\nhttps://deepmind.google/technologies/gemini/\nGemini-3-Pro\n‚Äì\nhttps://deepmind.google/technologies/gemini/\nS4/S2\n"}]}