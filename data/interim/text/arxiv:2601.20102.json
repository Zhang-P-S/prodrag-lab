{"doc_id": "arxiv:2601.20102", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.20102.pdf", "meta": {"doc_id": "arxiv:2601.20102", "source": "arxiv", "arxiv_id": "2601.20102", "title": "Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs: Identifier vs Context Effects", "authors": ["Amirhossein Haji Mohammad Rezaei", "Zahra Shakeri"], "published": "2026-01-27T22:44:07Z", "updated": "2026-01-27T22:44:07Z", "summary": "Engineering sustainable and equitable healthcare requires medical language models that do not change clinically correct diagnoses when presented with non-decisive cultural information. We introduce a counterfactual benchmark that expands 150 MedQA test items into 1650 variants by inserting culture-related (i) identifier tokens, (ii) contextual cues, or (iii) their combination for three groups (Indigenous Canadian, Middle-Eastern Muslim, Southeast Asian), plus a length-matched neutral control, where a clinician verified that the gold answer remains invariant in all variants. We evaluate GPT-5.2, Llama-3.1-8B, DeepSeek-R1, and MedGemma (4B/27B) under option-only and brief-explanation prompting. Across models, cultural cues significantly affect accuracy (Cochran's Q, $p<10^-14$), with the largest degradation when identifier and context co-occur (up to 3-7 percentage points under option-only prompting), while neutral edits produce smaller, non-systematic changes. A human-validated rubric ($κ=0.76$) applied via an LLM-as-judge shows that more than half of culturally grounded explanations end in an incorrect answer, linking culture-referential reasoning to diagnostic failure. We release prompts and augmentations to support evaluation and mitigation of culturally induced diagnostic errors.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.20102v1", "url_pdf": "https://arxiv.org/pdf/2601.20102.pdf", "meta_path": "data/raw/arxiv/meta/2601.20102.json", "sha256": "e81f4062d73782377d4f4bf5efcb12409cd6233623aaabb35a46a22c372f42f4", "status": "ok", "fetched_at": "2026-02-18T02:20:18.370308+00:00"}, "pages": [{"page": 1, "text": "Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs:\nIdentifier vs Context Effects\nAmirhossein Haji Mohammad Rezaei1 and Zahra Shakeri1,2,3\nAbstract— Engineering sustainable and equitable healthcare\nrequires medical language models that do not change clinically\ncorrect diagnoses when presented with non-decisive cultural\ninformation. We introduce a counterfactual benchmark that\nexpands 150 MedQA test items into 1,650 variants by inserting\nculture-related (i) identifier tokens, (ii) contextual cues, or\n(iii) their combination for three groups (Indigenous Cana-\ndian, Middle-Eastern Muslim, Southeast Asian), plus a length-\nmatched neutral control, where a clinician verified that the gold\nanswer remains invariant in all variants. We evaluate GPT-\n5.2, Llama-3.1-8B, DeepSeek-R1, and MedGemma (4B/27B)\nunder option-only and brief-explanation prompting. Across\nmodels, cultural cues significantly affect accuracy (Cochran's Q,\np < 10−14), with the largest degradation when identifier and\ncontext co-occur (up to 3-7 percentage points under option-\nonly prompting), while neutral edits produce smaller, non-\nsystematic changes. A human-validated rubric (κ = 0.76)\napplied via an LLM-as-judge shows that more than half of\nculturally grounded explanations end in an incorrect answer,\nlinking culture-referential reasoning to diagnostic failure. We\nrelease prompts and augmentations to support evaluation and\nmitigation of culturally induced diagnostic errors.\nI. INTRODUCTION\nImagine a scenario where an AI doctor gives two different\ndiagnoses for the same symptoms, just because one patient is\ndescribed as coming from a particular cultural background.\nThis is no longer a hypothetical concern. A recent study\nanalyzing over 1.7 million AI-generated emergency room\nrecommendations found that simply changing a patient's\ndemographic descriptors (e.g., race, gender, housing status,\netc.) led to markedly different medical advice [1], [2], [3].\nFor example, cases labeled as Black, LGBTQIA+, or un-\nhoused were far more likely to be triaged as urgent or sent for\ninvasive tests. These cases also received invasive testing more\noften without clear clinical indication, whereas high-income\ncues prompted more offers of advanced imaging [1]. Biases\nin large language model (LLM) outputs can compromise\npatient safety in clinical contexts. Communication failures\ncontribute to about 27% of medical malpractice cases, and\ncultural misunderstandings can worsen them [4]. As LLM\ntools support millions of healthcare encounters, cultural\ncompetence and impartiality must remain core design re-\nquirements.\n1\nAmirhossein\nHaji\nMohammad\nRezaei\nis\nwith\nthe\nInstitute\nof\nHealth\nPolicy,\nManagement,\nand\nEvaluation\n(IHPME),\nDalla\nLana\nSchool\nof\nPublic\nHealth,\nUniversity\nof\nToronto,\nCanada.\namirhossein.haji@mail.utoronto.ca\n1,2,3Zahra Shakeri is with IHPME; the Dalla Lana School of Public\nHealth; the Faculty of Information; and the Schwartz Reisman Institute at\nthe University of Toronto, Canada.\nLarge language models can generate and interpret human-\nlike text, which has accelerated interest in clinical use\n[5]. Hospitals and health organizations are evaluating these\nmodels for decision support and evidence-based information\nin settings with limited clinicians [6], [7]. The World Health\nOrganization projects a shortfall of more than 10 million\nhealth workers by 2030 as healthcare demand increases\n[8], [9]. AI assistants could reduce workload for clinical\nteams and expand access to care in settings with limited\ncapacity. However, reliability is still a central concern be-\ncause these systems can produce incorrect outputs under\nplausible clinical prompts. Factual hallucinations, in which\nan LLM fabricates credible medical statements, and bias\nin reasoning both threaten safe decision support [10], [11].\nSuch failures can lead to incorrect diagnoses, harm patients,\nand weaken trust in AI-assisted care [12], [13]. For clinical\ndeployment, rigorous auditing and mitigation of these failure\nmodes should precede use in high-stakes settings.\nPrior work has examined several bias types in medical\nLLMs, including cognition-related bias [14] and race and\ngender bias [11], [15]. In contrast, cultural bias in med-\nical tasks has garnered disproportionately less investiga-\ntion. The DiversityMedQA dataset evaluates ethnicity and\ngender bias by adding an identifier sentence to MedQA\nitems, for example, 'The patient is of African descent'\n[15]. Current benchmarks often miss the interaction between\ncultural context and identity cues, which limits evaluation\nquality. The Africa Health Check study examines cultural\nbias using a curated dataset on African traditional herbal\nmedicine [16]. That study focuses on one cultural group and\ndoes not address other low-resource cultural or Indigenous\npopulations. In general, current evaluation approaches lack\nsufficient granularity to determine whether errors stem from\ncultural identifiers (e.g., ethnicity, religion) or contextual\ndetails in case histories. This gap limits our understanding\nof cultural bias in medical LLMs today. To test robustness,\ncan a cultural cue in a patient narrative, such as Ramadan\nfasting or a traditional festival, shift the model from the\ncorrect diagnosis? No benchmark currently evaluates this\neffect across multiple cultures in realistic clinical settings.\nIn this study, we respond to the above gap by introduc-\ning a counterfactual evaluation of cultural bias in medical\nquestion-answering. To test cultural robustness, we augment\neach MedQA item with culture-related identity or context\ntext that is non-medically-decisive, so the gold answer re-\nmains unchanged. Answer changes or lower accuracy in this\nsetting indicate spurious sensitivity to cultural cues, such as\nunsupported prevalence assumptions or stereotypes.\narXiv:2601.20102v1  [cs.CL]  27 Jan 2026\n"}, {"page": 2, "text": "To explore cultural edits that should not affect clinical de-\ncisions, we examine counterfactual cultural cues designed\nto be clinically non-decisive. We define cultural identifiers\nas explicit references to a cultural group (e.g., 'a 35-year\nold Muslim man living in a large Middle Eastern city').\nWe define cultural contextual cues as culturally related\nsituational details embedded in the narrative (e.g., 'symptoms\nbegan during evening prayer at a mosque'). Because clinician\nreview and study design hold the clinically correct answer\ninvariant, any systematic accuracy drop or answer flipping\nunder these edits indicates reliance on cultural cues, not\nclinically grounded personalization.\nFigure 1 presents our evaluation pipeline for cultural bias\nin medical diagnosis. We sample multiple-choice questions\nfrom MedQA [10] and create culturally augmented variants\nthrough LLM-based augmentation. The augmentation targets\nthree cultural groups: Indigenous Canadian, Middle-Eastern\nMuslim, and Southeast Asian. Our results show that cultural\naugmentation increases diagnostic errors, with lower perfor-\nmance than the original setting. The analysis also shows\nthat both identifiers and context can elicit more culture-\nreferential reasoning, while identifiers contribute more to\nthe culturally induced error rate. Our main contributions\nare: (1) a clinician-verified counterfactual benchmark that\nisolates cultural identifiers vs context while preserving gold\ndiagnoses; (2) a multi-model evaluation under two clinically\nplausible prompting regimes; and (3) an analysis framework\nthat connects culture-referential rationales to diagnostic er-\nrors via a human-validated rubric. The rest of the paper is\nstructured as follows. Section II describes the methodology,\nincluding data preparation, model selection, and prompt\ndesign. Section III reports results on diagnostic accuracy,\ncontrasts error rates for identifiers and contextual informa-\ntion, and discusses limitations. Section IV concludes with\nthe main findings.\nII. METHODS\nA. Data Collection and Preparation\nDatasets: We used the MedQA dataset as the primary\nsource of medical queries, which contains multiple-choice\nproblems from professional medical licensing boards [10].\nEach item includes a question, the answer options, and the\ncorrect label. MedQA provides widely recognized clinical\nscenarios, which supports a stable baseline for testing how\nnon-medical (cultural) perturbations affect expert-level rea-\nsoning.\nPreparation: To add information about cultural back-\nground, we augmented MedQA question text with an LLM\nunder few-shot prompting. We randomly selected 150 items\nfrom the MedQA test set. The augmentation targeted three\nminority cultural backgrounds that often face biased and\ninequitable treatment in society and in AI systems: Middle-\nEastern Muslims [17], Southeast Asians [18], and Indigenous\nCanadians [19]. The first two groups may be less familiar in\nWestern settings, and distinct traditions and lifestyles can\nshape context in everyday decision-making, including care\nseeking and communication. Indigenous Canadians face sub-\nstantial discrimination and inequitable treatment in Canadian\nhealth systems [19], which motivates an analysis of model\nbehaviour in these clinical contexts. Few-shot prompting\nused augmentation examples within the prompt.\nIn addition to cultural augmentation, we separated identity\nphrases from contextual details to support a more granular\nanalysis of model behaviour. Each question was augmented\nfor each culture in three ways:\nChanging the identifier only (Id): We modified only the first\nsentence and the patient identifier (e.g., The patient is a 31-\nyear-old Muslim man from a Middle Eastern city.)\nChanging the Context only (Ctx): We modified only the\nsituational context, which can vary with cultural background\n(e.g., The symptoms started while doing evening prayer at a\nmosque.)\nChanging the Identifier and Context (Id + Ctx): We applied\nboth modifications to the question. Figure 2 presents an\nexample of the augmentation procedure. To attribute output\nshifts to cultural text rather than question length, we also\nincluded a neutral augmentation setting. In this setting, we\ninserted the sentence 'The patient arrived with a family\nmember and provided ID at registration' at the start of each\nquestion. For all cultural augmentations, the augmentation\nmodel received instructions not to introduce new clinical\ninformation. The edits remained limited to surface-level\nidentity phrasing, such as cultural self-identification, and to\ncontext details unrelated to the gold answer. A clinician\nreviewed each augmented item and confirmed that the added\ntext did not change the clinically correct answer. Subtle\ncontext can still imply differences in access or lifestyle, so\nwe treat residual confounding as a study limitation. To reduce\nthis risk, we designed examples and prompts so that cultural\ndetails remain descriptive rather than clinically informative.\nFinally, the design produced 150×(3×3) = 1350 culturally\naugmented samples. With the 150 original questions and 150\nneutrally augmented questions, the dataset contains 1650\nsamples. Each original question produces 10 augmented\nvariants, and this sample size balances statistical reliability\nwith inference cost.\nTo confirm that the test set covers diverse contextual\nscenarios, we extracted the added sentences from each aug-\nmented question relative to its original form. We embedded\nthese sentences with all-MiniLM-L6-v2 1. We then computed\nthe average pairwise cosine similarity within each cultural\ngroup. Each group achieved an average similarity of 0.3142,\nwhich suggests substantial contextual diversity across the\ngenerated scenarios.\nB. Counterfactual evaluation metrics\nWe use a counterfactual experimental design in this study\nfor each MedQA item i. For each item, we create augmented\nvariants that add non-medically decisive cultural identifiers\n(Id), cultural context cues (Ctx), or both (Id+Ctx). A clinician\n1https://huggingface.co/sentence-transformers/\nall-MiniLM-L6-v2\n"}, {"page": 3, "text": "Fig. 1: Overview of the pipeline for evaluation of cultural bias in medical question answering. MedQA questions are augmented using\nidentity-based, context-based, and identity + context methods across three cultural backgrounds (Indigenous Canadian, Middle-Eastern Muslim,\nand Southeast Asian). Augmented questions are answered by multiple large language models (LLaMA-3.1, GPT-5.2, MedGemma-4B/27B, and\nDeepSeek-R1) under option-only and brief-explanation prompting. Model outputs are analyzed using statistical testing (Cochran’s Q) and an LLM-\nas-judge framework to detect culturally-induced errors. The code and data of this pipeline are available at https://github.com/HIVE-UofT/\nEvaluatiog-Cultural-Clues-Medical-LLMs.\nORIGINAL QUESTION\n\"A 39-year-old man presents to the emergency department\nbecause of progressively worsening chest pain and\nnausea that started at a local bar 30 minutes prior...\"\nIdentifier Only\nContext Only\nIdentifier + Context\nA 39-year-old man\nfrom a First Nations\ncommunity in Canada\npresents with chest\npain...\nA 39-year-old man\npresents with chest\npain... started\nduring a community\ngathering...\nA 39-year-old man\nfrom a remote First\nNations community\npresents... started\nduring a community\ngathering...\nIndigenous\nCanadian\nA 39-year-old\nSoutheast Asian man\npresents with chest\npain...\nA 39-year-old\nman presents...\nstarted at a family\nwedding...\nA 39-year-old\nSoutheast Asian\nman presents...\nstarted at a family\nwedding...\nSoutheast\nAsian\nA 39-year-old\nMuslim man presents\nwith chest pain...\nA 39-year-old\nman presents...\nstarted after\neating a heavy meal\nduring a religious\ngathering...\nA 39-year-old\nMuslim man\npresents... started\nduring a religious\ngathering...\nMiddle-Eastern\nMuslim\nFig. 2: Culturally augmented MedQA questions. Each original question is transformed into identifier-only (Teal), context-only (Sage), and combined\n(Lavender) variants. Bold text shows the inserted cultural identifiers or contextual cues in the clinical scenario.\nverifies that the clinically correct reference answer yi remains\nthe same for all variants. Therefore, shifts in accuracy or\noption choice indicate spurious sensitivity to cultural text,\nnot clinically grounded personalization.\nLet i ∈{1, . . . , N} index base questions with gold\noption yi ∈{A, B, C, D, E}. Let x(0)\ni\ndenote the original\nquestion stem, x(Neutral)\ni\nthe length-matched neutral control,\nand x(c,t)\ni\na culturally augmented variant for culture c ∈C\nand perturbation type t ∈{Id, Ctx, Id+Ctx}. For model m,\nlet ˆym(x) be the predicted option parsed from the model\noutput.\nWe report condition accuracy as:\nAccm,s = 1\nNs\nX\ni∈Is\nI\nh\nˆym\n\u0010\nx(s)\ni\n\u0011\n= yi\ni\n,\n(1)\nwhere Is is the set of questions retained for condition s\n(e.g., after filtering incomplete outputs under explanation\nprompting), and Ns = |Is|.\nTo align with Table I/II, we summarize the average cultural\n"}, {"page": 4, "text": "TABLE I: Model accuracy and average cultural impact across cultural augmentations for option prompting. ∆Avg values\nare reported as ∆Orig/∆Neutral. Grey cells indicate values of interest (highest accuracy, large drops, or best robustness).\nCochran’s Q test shows statistically significant differences across all cultural conditions for both original and neutral settings\n(p < 10−14).\nModel\nBaselines\nC1: Indigenous\nC2: Mid-Eastern\nC3: SE Asian\n∆Avg (C1–C3)\nCochran's Q\nOrig. Neut.\nId\nCtx\nId+Ctx\nId\nCtx\nId+Ctx\nId\nCtx\nId+Ctx\nId\nCtx\nId+Ctx\nId\nCtx\nId+Ctx\nLLaMA\n64.67 60.67 60.00 60.00\n58.67\n62.00 60.67\n58.00\n58.00 63.33\n57.33\n-4.67 / -0.67\n-3.34 / +0.66 -6.67 / -2.67 ✓/✓✓/✓\n✓/✓\nGPT-5.2\n92.67 90.67 90.00 89.33\n87.33\n90.00 90.67\n85.33\n91.33 92.00\n86.67\n-2.23 / -0.23\n-1.67 / +0.33 -6.23 / -4.23 ✓/✓✓/✓\n✓/✓\nMedGemma-4B\n54.00 52.67 51.33 52.67\n49.33\n52.00 54.00\n50.67\n52.67 53.33\n50.00\n-2.00 / -0.67\n-0.56 / +0.78 -3.67 / -2.34 ✓/✓✓/✓\n✓/✓\nMedGemma-27B 72.00 72.67 68.00 70.67\n66.67\n70.00 70.00\n69.33\n70.67 72.00\n70.67\n-2.44 / -3.11\n-1.11 / -1.78\n-3.11 / -3.78 ✓/✓✓/✓\n✓/✓\nDeepSeek-R1\n92.67 94.00 92.00 92.67\n87.33\n94.67 94.00\n86.67\n94.67 94.00\n86.67\n+1.11 / -0.22 +0.44 / -0.67 -5.78 / -7.11 ✓/✓✓/✓\n✓/✓\nimpact for a perturbation type t by averaging over cultures:\nAccm,t = 1\n|C|\nX\nc∈C\nAccm,(c,t),\n∆Orig\nm,t = Accm,t −Accm,0,\n∆Neutral\nm,t\n= Accm,t −Accm,Neutral\n(2)\nAccuracy captures clinical correctness, but does not dis-\ntinguish whether cultural edits cause the model to flip its\nchosen option even when both options are wrong. To quantify\ncounterfactual instability directly, we also compute the flip\nrate:\nFlipm,s = 1\nNs\nX\ni∈Is\nI\nh\nˆym\n\u0010\nx(s)\ni\n\u0011\n̸= ˆym\n\u0010\nx(0)\ni\n\u0011i\n(3)\nFinally, to measure clinically harmful instability, we com-\npute the rate at which a previously correct original prediction\nbecomes incorrect after augmentation:\nHFlipm,s =\nP\ni∈Is I[ˆym(x(0)\ni ) = yi] · I[ˆym(x(s)\ni ) ̸= yi]\nP\ni∈Is I[ˆym(x(0)\ni ) = yi]\n(4)\nIn this benchmark, elevated Flip or HFlip under cultural\nconditions relative to Neutral indicates that culture-related\ntext changes the model's diagnostic decision despite invariant\ngold labels.\nC. Large Language Model Setup\n1) Model Selection: We selected commercial and open-\nsource LLMs that differ in alignment approach, reasoning\ntraining, and intended use in general or medical tasks:\n• OpenAI-GPT5.2 [20]: This model is described as Ope-\nnAI's strongest general-purpose system for broad rea-\nsoning tasks. Its generalization capacity motivated its\nuse for medical diagnosis and as the LLM-as-Judge for\ncultural impact evaluation.\n• MedGemma [21]: MedGemma is an open-source model\nfrom Google that is fine-tuned for medical and clinical\nreasoning. We included it as a model tuned for medical\ndiagnosis.\n• DeepSeek-R1 [22]: We included DeepSeek models be-\ncause reinforcement learning-based fine-tuning has pro-\nduced strong reasoning performance relative to earlier\nsystems.\n• Llama3.1-8B [23]: We included Llama models as\nwidely adopted open-source instruction-tuned LLMs,\nwhich motivates evaluation of cultural bias in medical\ncontexts.\n2) Prompt design: We designed system prompts for three\ntasks: generating culturally augmented questions, answering\nthe medical questions, and detecting cultural cues with an\nLLM-as-Judge (GPT-5.2). For medical question answering,\neach item is a multiple-choice question, so we used two\nprompting formats. The first format is option prompting,\nwhere the model outputs only the selected option. The second\nformat is brief explanation plus option prompting, where the\nmodel provides a 2–3 sentence rationale and then outputs the\nfinal option as 'The answer is X'.\nMedical Diagnosis Prompt\nSystem prompt: You are a medical expert\nassistant. You are given multiple-choice\nmedical questions. Your task is to analyze\nthe question and the provided answer options,\napply correct medical reasoning, and choose the\nsingle best answer.\nOUTPUT CONSTRAINTS (STRICT): - The explanation\nmust be exactly 2-3 sentences. - Do NOT\nuse bullet points, numbering, or headings.\n- Do NOT include disclaimers, preambles,\nor meta-comments. - Do NOT include extra\nwhitespace or blank lines. - The response must\nfollow the exact format described below.\nFINAL FORMAT (MANDATORY): - After the\nexplanation, output exactly one final line:\n\"The answer is X\" where X is one of (A, B, C,\nD, or E).\nUser prompt: Question: {question}\nOptions: {options}\nProvide a medical explanation in exactly 2-3\nsentences. Strictly follow all formatting\nrules. Then end the response with exactly this\nline and nothing else: \"The answer is X\" where\nX is the correct option letter.\nIII. RESULTS AND DISCUSSION\nA. Medical Diagnosis Accuracy\nTable I presents medical diagnosis accuracy across set-\ntings under option prompting. The results show that models\nlose accuracy under cultural augmentations, especially when\nidentity and context appear together. All models perform\nworse in the combined setting than in the original and\n"}, {"page": 5, "text": "TABLE II: Model accuracy and average cultural impact across cultural augmentations for explanation + option prompting.\n∆Avg values are reported as ∆Orig/∆Neutral. Grey cells indicate values of interest (highest accuracy, significant drops, or\nbest robustness). Cochran’s Q test shows statistically significant differences across all cultural conditions for both original\nand neutral settings (p < 10−14).\nModel\nBaselines\nC1: Indigenous\nC2: Mid-Eastern\nC3: SE Asian\n∆Avg (C1–C3)\nCochran's Q\nOrig. Neut.\nId\nCtx\nId+Ctx\nId\nCtx\nId+Ctx\nId\nCtx\nId+Ctx\nId\nCtx\nId+Ctx\nId\nCtx\nId+Ctx\nLLaMA\n65.99 65.99 66.67 70.07\n64.63\n69.39 64.63\n63.95\n62.59 67.35\n65.31\n+0.23 / +0.23 +1.36 / +1.36 -1.15 / -1.15 ✓/✓✓/✓\n✓/✓\nGPT-5.2\n91.16 91.16 91.16 89.80\n87.76\n92.52 91.84\n87.07\n91.84 93.20\n90.48\n+0.68 / +0.68 +0.45 / +0.45 -2.72 / -2.72 ✓/✓✓/✓\n✓/✓\nMedGemma-4B\n59.18 59.18 57.82 56.46\n53.74\n58.50 57.14\n57.14\n57.14 57.82\n53.06\n-1.36 / -1.36\n-1.70 / -1.70\n-4.53 / -4.53 ✓/✓✓/✓\n✓/✓\nMedGemma-27B 75.51 75.51 73.47 76.87\n73.47\n74.15 77.55\n75.51\n73.47 77.55\n76.87\n-1.98 / -1.98\n+1.15 / +1.15 -0.57 / -0.57 ✓/✓✓/✓\n✓/✓\nDeepSeek-R1\n92.52 93.88 91.84 92.52\n87.76\n94.56 93.88\n87.07\n94.56 93.88\n87.07\n+1.14 / -0.23\n+0.73 / -0.64\n-5.22 / -6.81 ✓/✓✓/✓\n✓/✓\nLlama­3.1\nGPT­5.2\nMedGemma­4B\nMedGemma­27B\nDeepSeek­R1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy (95% CI)\nCondition\nOriginal\nNeutral\nIndigenous\nMid­East\nSE Asian\nFig. 3: Model robustness across cultural scenarios. Bar heights represent absolute accuracy, and error bars denote bootstrapped 95% confidence intervals\n(n = 2000). The overlap between the Original and Neutral conditions across all models shows that prompt length alone does not reduce performance.\nThe Identity + Context settings show statistically significant accuracy declines for DeepSeek-R1 and MedGemma-4B, with larger effects in Indigenous\ncontexts.\nneutral baselines. Because MedGemma models are fine-tuned\nfor medicine, their limited robustness motivates targeted\nmitigation in medical LLM development. We use Cochran’s\nQ test, a non-parametric test for repeated binary outcomes, to\ntest whether accuracy differs across cultural variants. The test\nindicates statistically significant differences for all models\nacross cultural conditions in both original and neutral set-\ntings. These results indicate that cultural cues systematically\ninfluence diagnostic accuracy.\nTable II reports the same analysis for explanation + option\nprompting. Some outputs omitted the selected option, so we\nremoved partial answers and retained 147 questions per set-\nting. The results show that explanation prompting improves\nrobustness in identifier-only and context-only conditions.\nThis pattern suggests that explicit reasoning reduces bias\nwhen a single cultural cue is present. However, accuracy\ndeclines for every model when cultural identifier and context\nappear together. This pattern indicates that the combined\nedits reliably induce medical errors across models.\nFor both prompting strategies, original and neutral baseline\naccuracies are nearly identical. This similarity indicates that\nperformance changes arise from cultural background cues\nrather than prompt length. Figure 3 shows the same pattern\nusing bootstrapped 95% confidence intervals. The Original\nand Neutral conditions overlap closely for all models. Cul-\nturally augmented settings show lower accuracy, with the\nlargest drops for DeepSeek-R1 and MedGemma-4B.\nLlama­3.1\nGPT­5.2\nMedGemma­4B\nMedGemma­27B\nDeepSeek­R1\n8\n6\n4\n2\n0\n2\n4\nChange in Accuracy (pp)\nAugmentation\nIdentity Only\nContext Only\nIdentity + Context\nFig. 4: Component analysis of culturally induced errors. Bars show the\nmean percentage-point drop in accuracy relative to the baseline. For most\nmodels, the Identity Only condition produces a drop similar to Identity +\nContext, whereas Context Only has a smaller effect.\nTABLE III: Aggregated Error Rates Across Cultural Variants (C1-C3)\nModel\nIdentity\nContext\nIdentity + Context\nLLaMA\n42 / 63 (66.67%)\n27 / 40 (67.50%)\n63 / 82 (76.83%)\nGPT-5.2\n18 / 35 (51.43%)\n21 / 39 (53.85%)\n24 / 43 (55.81%)\nMedGemma-4B\n42 / 62 (67.74%)\n15 / 34 (44.12%)\n55 / 85 (64.71%)\nMedGemma-27B\n47 / 75 (62.67%)\n25 / 46 (54.35%)\n52 / 85 (61.18%)\nDeepSeek-R1\n193 / 331 (58.31%)\n75 / 143 (52.45%)\n210 / 335 (62.69%)\nB. Comparison of Cultural Identifiers and Context on\nCulturally-induced Errors\nTo compare the role of identity tokens and contextual cues,\nwe analyze which component more often triggers diagnostic\nerrors. We detect culturally grounded reasoning using an\nLLM-as-judge (GPT-5.2), which flags explanations that ref-\n"}, {"page": 6, "text": "10\n5\n0\n5\n10\n15\n20\n25\nChange in Accuracy (Percentage Points)\nLlama­3.1\nGPT­5.2\nMedGemma­4B\nMedGemma­27B\nDeepSeek­R1\n-1.4 [-8.8, +6.1]\n-2.0 [-9.5, +4.8]\n-0.7 [-8.8, +6.8]\n-3.4 [-8.2, +1.4]\n-4.1 [-8.2, +0.0]\n-0.7 [-4.8, +3.4]\n-5.4 [-10.9, +0.0]\n-2.0 [-8.2, +3.4]\n-6.1 [-10.9, -0.7]\n-2.0 [-7.5, +3.4]\n+0.0 [-5.4, +5.4]\n+1.4 [-3.4, +6.1]\n-4.8 [-9.5, +0.7]\n-5.4 [-9.5, -1.4]\n-5.4 [-10.2, -0.7]\nEffect Size [95% CI]\nChange in Accuracy (%)\nCultural Context\nIndigenous\nMiddle­Eastern\nSE Asian\nFig. 5: Forest plot of model sensitivity to cultural contexts. Points\nrepresent the effect size, defined as the mean accuracy difference in\npercentage points between Identity + Context and the original baseline. The\nright-hand column reports point estimates with bootstrapped 95% confidence\nintervals. Bolded values indicate statistically significant bias because the\nconfidence interval does not include 0. DeepSeek-R1 shows the largest\nsensitivity to Indigenous contexts, whereas MedGemma-27B remains robust\nacross groups.\nerence identity or region-specific context. Two independent\nhuman annotators labeled a random subset of 50 explanations\nto validate the rubric. Cohen's κ = 0.76 indicates substantial\nagreement between annotators. We then apply the same\nrubric at scale with the LLM-as-judge for consistent labeling\nacross models and settings.\nTable III aggregates results across the three cultural vari-\nants. For each setting, the denominator is the number of\nculturally induced explanations, and the numerator is the\nnumber with incorrect final answers. The combined identity\nand context setting produces the largest number of culturally\ninduced explanations for all models. The identifier-only\nsetting yields an error rate comparable to the identity +\ncontext setting for all models except LLaMA. For most\nmodels except GPT-5.2, the number of errors remains similar\nbetween identifier-only and identity + context conditions.\nThese results indicate that identity phrases play a major role\nin triggering diagnostic errors.\nFigure 4 provides a complementary view by decomposing\naccuracy changes relative to the original baseline for Id, Ctx,\nand Id+Ctx. Across most models, Id produces a drop similar\nto Id+Ctx, while Ctx produces a smaller change. This pattern\nsuggests that identity tokens can activate spurious priors\nunder controlled edits that preserve the correct answer. In\nterms of error rates, more than half of culturally grounded\nexplanations end with incorrect answers for every model.\nDeepSeek-R1 shows the highest counts of culturally induced\nexplanations and culturally linked errors. This behaviour sug-\ngests that the reasoning process incorporates patient profile\ncues more strongly than other models.\nTABLE IV: Flip rates (%) across models and cultural augmentations for\nexplanation + option prompting. The bold values indicates the highest flip\namong augmentation types.\nModel\nNeutral\nC1: Indigenous\nC2: Mid-Eastern\nC3: SE Asian\nI\nC\nId+Ctx\nI\nC\nId+Ctx\nI\nC\nId+Ctx\nLLaMA\n0.00\n23.13 25.85\n32.65\n21.77 19.73\n30.61\n27.89 23.81\n30.61\nGPT\n0.00\n3.40\n3.40\n9.52\n3.40\n2.72\n7.48\n2.72\n2.04\n6.80\nMedGemma-4B\n0.00\n11.56\n8.84\n18.37\n6.80\n10.88\n16.33\n10.88 10.20\n19.05\nMedGemma-27B\n0.00\n10.88\n5.44\n14.29\n7.48\n9.52\n12.93\n6.12\n10.88\n14.97\nDeepSeek\n4.76\n7.48\n3.40\n12.24\n4.76\n4.08\n8.84\n4.08\n3.40\n11.56\nTABLE V: Harmful flip rates (%) across models and cultural augmen-\ntations for explanation + option prompting. The bold values indicates the\nhighest flip among augmentation types.\nModel\nNeutral\nC1: Indigenous\nC2: Mid-Eastern\nC3: SE Asian\nI\nC\nId+Ctx\nI\nC\nId+Ctx\nI\nC\nId+Ctx\nLLaMA\n0.00\n12.37 11.34\n18.56\n6.19 11.34\n16.49\n17.53 12.37\n16.49\nGPT\n0.00\n1.49\n2.24\n6.72\n0.75\n0.75\n5.97\n0.75\n0.00\n3.73\nMedGemma-4B\n0.00\n6.90\n5.75\n14.94\n3.45\n8.05\n12.64\n6.90\n5.75\n13.79\nMedGemma-27B\n0.00\n6.31\n2.70\n9.91\n5.41\n4.50\n7.21\n4.50\n4.50\n6.31\nDeepSeek\n1.47\n3.68\n1.47\n8.82\n1.47\n1.47\n7.35\n0.74\n0.74\n8.82\nC. Counterfactual Analysis of Culturally-augmented Settings\nTable IV presents the result for flip rates for each model,\ncultural group, and augmentation type. The results reveal that\nall models have a higher flip rate compared to the neutral\nsetting, and Id+Ctx augmentation causes the highest flip rate\namong augmentation types. For the DeepSeek model, the\nflip rate for the neutral setting is comparable with the flip\nrates for Identity-only and Context-only settings, and the\nneutral flip rate for all models except the DeepSeek is zero,\nindicating the effectiveness of our neutral augmentation to\nremain stable compared to the original setting. The LLama\nand MedGemma-4B models demonstrate the highest values\nfor flip rates among the models.\nTable V shows the result for the harmful flip rate for\nour experiments. The results show a consistent pattern with\nthe flip rate results, as the Id+Ctx setting causes significant\nanswer changes for questions correctly answered in the\noriginal setting.\nD. Comparison of Models Performance Changes Across\nCultures\nWe compare model sensitivity across cultural groups un-\nder the Id+Ctx setting. Figure 5 reports effect sizes as\npercentage-point accuracy differences between Id+Ctx and\nthe Original baseline. Bootstrapped 95% confidence intervals\naccompany each cultural condition. DeepSeek-R1 shows the\nlargest negative shifts across groups at about 5-6%. The\nIndigenous condition shows a statistically reliable decline\nbecause its interval excludes 0. MedGemma-27B is more\nstable, and its intervals include 0 for most groups.\nE. Manual Review of Culturally Induced Reasoning Failures\nWe manually reviewed cases flagged by the judge to\ncharacterize how culture-related cues shaped reasoning. One\ncommon failure mode involved unsupported generalization\nabout higher prevalence in a group. Several cases attributed\nsymptoms to presumed diet or lifestyle without support in the\nquestion stem. These assumptions displaced higher-salience\n"}, {"page": 7, "text": "clinical features from the original MedQA item and produced\nan incorrect option choice. These results indicate that culture-\nrelated tokens can activate ungrounded priors in model\nreasoning rather than clinically grounded personalization.\nF. Limitations & Future Work\nWe note several limitations that affect the scope of our\nbenchmark and the interpretation of its results. First, our\npipeline produces nine culture-specific augmentations per\nquestion (three cultures × three augmentation types) and one\nneutral variant. This expansion increases inference time and\ncost for medical diagnosis and LLM-as-Judge runs, which\nconstrained the amount of source data we could include. Sec-\nond, LLM-based cultural augmentation can reduce scenario\ndiversity, and the model can repeat contexts across similar\nquestions. For example, multiple items describe symptoms\nthat begin during a family gathering setting. Third, we\nevaluated models that differ in scale and tuning methods,\nwhich limited coverage of other LLM categories. To address\nthese constraints, future work will expand the model set\nand incorporate additional diagnosis datasets from diverse\nclinical settings.\nIV. CONCLUSION\nIn this work, we evaluated whether medical language\nmodels remain clinically consistent when exposed to cultural\ncues that are designed to be non-decisive. Our results show\nsystematic sensitivity to cultural identifiers and context,\nwhich can undermine trustworthy, equitable, and scalable\nuse of language models in clinical workflows. Moreover, we\ncompared the error rate for generated explanations among\nsettings based on the identifier and contextual information\nadded to the questions. Our findings highlight the fact that\nthe models are more sensitive to identifier phrases for cultural\ngroups rather than the contextual information.\nThis study motivates the need for future research for\nthe mitigation of cultural bias in the context of medical\ndiagnosis. The development procedures for medical LLMs\nneed to consider the cultural background information of\ndifferent population groups to align the models' knowledge\nwith the needs of more diverse user profiles. This line\nof research requires further investigation for more accurate\nbenchmarks to evaluate LLMs on more diverse and in-depth\ncultural proxies evident in medical diagnosis cases. The\nmedical LLMs should be reliable and unbiased when facing\nthe cultural background and clues in the medical history of\npatients, as this would lead to the increased reliability and\ntrust of medical professionals and patients worldwide for\ncritical applications, including medical and clinical tasks.\nREFERENCES\n[1]\nM. Omar et al., “Sociodemographic biases in medical decision\nmaking by large language models,” Nature Medicine, pp. 1–9, 2025.\n[2]\nY. Artsi, V. Sorin, B. S. Glicksberg, P. Korfiatis, G. N. Nadkarni, and\nE. Klang, “Large language models in real-world clinical workflows:\nA systematic review of applications and implementation,” Frontiers\nin Digital Health, vol. 7, p. 1 659 134, 2025.\n[3]\nY. Artsi et al., “Challenges of implementing llms in clinical practice:\nPerspectives,” Journal of Clinical Medicine, vol. 14, no. 17, p. 6169,\n2025.\n[4]\nA. Tiwary, A. Rimal, B. Paudyal, K. R. Sigdel, and B. Basnyat,\n“Poor communication by health care professionals may lead to\nlife-threatening complications: Examples from two case reports,”\nWellcome open research, vol. 4, p. 7, 2019.\n[5]\nA. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez,\nT. F. Tan, and D. S. W. Ting, “Large language models in medicine,”\nNature medicine, vol. 29, no. 8, pp. 1930–1940, 2023.\n[6]\nE. J. Topol, “High-performance medicine: The convergence of human\nand artificial intelligence,” Nature medicine, vol. 25, no. 1, pp. 44–56,\n2019.\n[7]\nL. Builtjes, J. Bosma, M. Prokop, B. van Ginneken, and A. Hering,\n“Leveraging open-source large language models for clinical informa-\ntion extraction in resource-constrained settings,” JAMIA open, vol. 8,\nno. 5, ooaf109, 2025.\n[8]\nH. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz,\n“Capabilities of gpt-4 on medical challenge problems,” arXiv\npreprint arXiv:2303.13375, 2023.\n[9]\nM. Boniol, T. Kunjumen, T. S. Nair, A. Siyam, J. Campbell, and\nK. Diallo, “The global health workforce stock and distribution in\n2020 and 2030: A threat to equity and ‘universal’health coverage?”\nBMJ global health, vol. 7, no. 6, 2022.\n[10]\nD. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits,\n“What disease does this patient have? a large-scale open domain\nquestion answering dataset from medical exams,” Applied Sciences,\nvol. 11, no. 14, p. 6421, 2021.\n[11]\nS. R. Pfohl et al., “A toolbox for surfacing health equity harms and\nbiases in large language models,” Nature Medicine, vol. 30, no. 12,\npp. 3590–3600, 2024.\n[12]\nD. Roustan, F. Bastardot, et al., “The clinicians’ guide to large lan-\nguage models: A general perspective with a focus on hallucinations,”\nInteractive journal of medical research, vol. 14, no. 1, e59823, 2025.\n[13]\nL. Moura et al., “Implications of large language models for quality\nand efficiency of neurologic care: Emerging issues in neurology,”\nNeurology, vol. 102, no. 11, e209497, 2024.\n[14]\nS. Schmidgall et al., “Evaluation and mitigation of cognitive biases\nin medical language models,” npj Digital Medicine, vol. 7, no. 1,\np. 295, 2024.\n[15]\nR. Rawat et al., “Diversitymedqa: A benchmark for assessing demo-\ngraphic biases in medical diagnosis using large language models,”\nin Proceedings of the Third Workshop on NLP for Positive Impact,\n2024, pp. 334–348.\n[16]\nC. Nimo, S. Liu, I. Essa, and M. L. Best, “Africa health check:\nProbing cultural bias in medical llms,” in Proceedings of the 2025\nConference on Empirical Methods in Natural Language Processing,\n2025, pp. 32 207–32 220.\n[17]\nB. Asseri, E. Abdelaziz, and A. Al-Wabil, “Prompt engineering\ntechniques for mitigating cultural bias against arabs and muslims\nin large language models: A systematic review,” arXiv preprint\narXiv:2506.18199, 2025.\n[18]\nM. Rinki, C. Raj, A. Mukherjee, and Z. Zhu, “Measuring south asian\nbiases in large language models,” arXiv preprint arXiv:2505.18466,\n2025.\n[19]\nA. Khan, V. McKinney, and I. Mendez, “Two-eyed seeing and\nartificial intelligence: Enhancing healthcare delivery in indigenous\ncommunities requires an ethical and culturally relevant public health\nframework,” Canadian Journal of Public Health, pp. 1–4, 2025.\n[20]\nOpenAI, Gpt-5.2, https://platform.openai.com/docs/\nguides/latest-model, Accessed: 2025-12-23.\n[21]\nGoogle, Medgemma, https://developers.google.com/\nhealth - ai - developer - foundations / medgemma, Ac-\ncessed: 2025-12-23.\n[22]\nD. Guo et al., “Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning,” arXiv preprint arXiv:2501.12948,\n2025.\n[23]\nA. Dubey et al., “The llama 3 herd of models,” arXiv e-prints, arXiv–\n2407, 2024.\n"}]}