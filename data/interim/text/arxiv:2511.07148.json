{"doc_id": "arxiv:2511.07148", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.07148.pdf", "meta": {"doc_id": "arxiv:2511.07148", "source": "arxiv", "arxiv_id": "2511.07148", "title": "TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine", "authors": ["Zihao Cheng", "Yuheng Lu", "Huaiqian Ye", "Zeming Liu", "Minqi Wang", "Jingjing Liu", "Zihan Li", "Wei Fan", "Yuanfang Guo", "Ruiji Fu", "Shifeng She", "Gang Wang", "Yunhong Wang"], "published": "2025-11-10T14:35:25Z", "updated": "2025-12-26T11:02:50Z", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in modern medicine, yet their application in Traditional Chinese Medicine (TCM) remains severely limited by the absence of standardized benchmarks and the scarcity of high-quality training data. To address these challenges, we introduce TCM-Eval, the first dynamic and extensible benchmark for TCM, meticulously curated from national medical licensing examinations and validated by TCM experts. Furthermore, we construct a large-scale training corpus and propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously enrich question-answer pairs with validated reasoning chains through rejection sampling, establishing a virtuous cycle of data and model co-evolution. Using this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art LLM specifically designed for TCM, which significantly exceeds the passing threshold for human practitioners. To encourage future research and development, we release a public leaderboard, fostering community engagement and continuous improvement.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.07148v2", "url_pdf": "https://arxiv.org/pdf/2511.07148.pdf", "meta_path": "data/raw/arxiv/meta/2511.07148.json", "sha256": "084aee8aae1acce283d676bcab0644b752bd04184ad1cedbcfbf2c7043d1ffac", "status": "ok", "fetched_at": "2026-02-18T02:27:27.306295+00:00"}, "pages": [{"page": 1, "text": "TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark\nfor Traditional Chinese Medicine\nZihao Cheng1∗, Yuheng Lu1∗, Huaiqian Ye1, Zeming Liu1† , Minqi Wang2,\nJingjing Liu1, Zihan Li1, Wei Fan2, Yuanfang Guo1, Ruiji Fu3‡ , Shifeng She2,4,\nGang Wang2, Yunhong Wang1,\n1School of Computer Science and Engineering, Beihang University\n2Beijing Zhimingtang Technology Co., Ltd. 3Beijing Zhiyan AI Technology Co., Ltd.\n4Guangzhou University of Chinese Medicine\n https://tcmeval.bamaidical.com\nAbstract\nLarge Language Models (LLMs) have demon-\nstrated remarkable capabilities in modern\nmedicine, yet their application in Traditional\nChinese Medicine (TCM) remains severely\nlimited by the absence of standardized bench-\nmarks and the scarcity of high-quality training\ndata. To address these challenges, we intro-\nduce TCM-Eval, the first dynamic and exten-\nsible benchmark for TCM, meticulously cu-\nrated from national medical licensing examina-\ntions and validated by TCM experts. Further-\nmore, we construct a large-scale training corpus\nand propose Self-Iterative Chain-of-Thought\nEnhancement (SI-CoTE) to autonomously en-\nrich question-answer pairs with validated rea-\nsoning chains through rejection sampling, es-\ntablishing a virtuous cycle of data and model\nco-evolution. Using this enriched training data,\nwe develop ZhiMingTang-M1 (ZMT-M1), a\nstate-of-the-art LLM specifically designed for\nTCM, which significantly exceeds the passing\nthreshold for human practitioners. To encour-\nage future research and development, we re-\nlease a public leaderboard, fostering commu-\nnity engagement and continuous improvement.\n1\nIntroduction\nThe rapid advancement of Large Language Mod-\nels (LLMs) has catalyzed a paradigm shift across\nnumerous domains (Zhao et al., 2023; Gao\net al., 2025; Wang et al., 2024), with health-\ncare emerging as a particularly promising frontier\n(Thirunavukarasu et al., 2023; Zhou et al., 2023).\nThese models have demonstrated remarkable capa-\nbilities in tasks such as clinical decision support (Li\net al., 2025; Garza et al., 2025), medical text sum-\nmarization (Van Veen et al., 2024; Aali et al., 2025),\nand patient-facing conversational AI (Mukherjee\n∗Equal contribution and the order is determined alphabet-\nically by last name.\n†Corresponding author: Zeming Liu.\n‡Project leader: Ruiji Fu.\nFundamentals of TCM\nTCM Clinical Practice\nIntegrated Western Medicine\nMedical Humanities\nQuestion:\n   人体作为一个有机整体，其“中心”是\n   (What is considered the “center” of\nthe human body as an organic whole?)\nOptions:\n   A. 脑 (Brain)\n   B. 心 (Heart)\n   C. 五脏 (Five Zang organs)\n   D. 经络 (Meridians)\n   E. 脏腑 (Zang-fu organs)\nCorrect Answer: C\nQuestion:\n   头痛，刺痛，日久，伴有失眠\n   (Headache, stabbing in nature,\nchronic, accompanied by insomnia)\nOptions:\n   A. 瘀阻脑窍 (Blood stasis obstructing\nthe orifices of the brain)\n   B. 寒凝气滞 (Cold congealing with Qi\nstagnation)\n   C. 痰瘀交阻 (Phlegm and blood stasis\nobstructing each other)\n   D. 阳气虚衰 (Yang Qi deficiency)\n   E. 风寒侵袭 (Wind-cold invasion)\nCorrect Answer: A\nQuestion:\n   用于治疗肺炎链球菌肺炎的抗生素\n   (Antibiotics used for treating\nStreptococcus pneumoniae pneumonia)\nOptions:\n   A. 青霉素G (Penicillin G)\n   B. 左氧氟沙星 (Levofloxacin)\n   C. 庆大霉素 (Gentamicin)\n   D. 阿奇霉素 (Azithromycin)\n   E. 亚胺培南 (Imipenem)\nCorrect Answer: A\nQuestion:\n   下列哪项属于假药\n   (Which of the following belongs to\ncounterfeit drugs)\nOptions:\n   A. 药品超过有效期 (Drugs past their\nexpiration date)\n   B. 药品不注明或者更改生产批号\n(Drugs without labeled or altered batch\nnumbers)\n   C. 药品未标明有效期或者更改有效期\n(Drugs without labeled expiration date\nor altered expiration date) ...\nCorrect Answer: B\nFigure 1: Examples of TCM-Eval, illustrating four\naspects: Fundamentals of TCM, TCM Clinical Practice,\nIntegrated Western Medicine, Medical Humanities.\net al., 2024), significantly accelerating innovation\nin the medical field.\nHowever, despite these advancements, the focus\nof current medical LLMs and open-source datasets\nhas been overwhelmingly skewed towards mod-\nern medicine (Liu et al., 2024b; Wu et al., 2024;\nChen et al., 2024; Zhang et al., 2025). This has\ncreated a significant gap in the domain of Tradi-\ntional Chinese Medicine (TCM). The development\nof powerful LLMs for TCM is severely hampered\nby two fundamental challenges: the absence of a\nunified, authoritative platform for model evaluation\n(Chen et al., 2025), and the scarcity of high-quality,\nlarge-scale training data (Qiu et al., 2024; Zhang\net al., 2024). This lack of foundational resources\nrestricts progress and prevents a systematic under-\nstanding of current models’ capabilities within the\nTCM domain.\nTo address these challenges, we introduce TCM-\n1\narXiv:2511.07148v2  [cs.CL]  26 Dec 2025\n"}, {"page": 2, "text": "Eval, the first dynamic and extensible benchmark\nspecifically designed for evaluating LLMs in the\ndomain of TCM. TCM-Eval consists of 6,099\nhigh-quality questions, carefully selected from a\ndecade’s worth of the National Medical Licensing\nExamination for TCM Practitioners. Each question\nhas been validated by experts in TCM to ensure\nits accuracy and relevance. As shown in Figure 1,\nTCM-Eval comprehensively tests the models’ capa-\nbilities across four key dimensions: (1) Fundamen-\ntals of TCM, (2) TCM Clinical Practice, (3) Inte-\ngrated Western Medicine, and (4) Medical Humani-\nties. All questions are formatted as multiple-choice,\nensuring the uniqueness of answers and the verifia-\nbility of responses. To maintain the benchmark’s\nintegrity and prevent data leakage, TCM-Eval is\ndynamic, with continuous updates to incorporate\nnew questions.\nTo address the challenge of scarce high-quality\ntraining data, we have compiled a comprehensive\ncorpus from 18 authoritative TCM textbooks and\nthousands of mock examinations, which we used to\ncreate a domain-specific dataset consisting of over\n384,807 question-answer pairs. To leverage this\ndata and enhance reasoning abilities, we introduce\nSelf-Iterative Chain-of-Thought Enhancement (SI-\nCoTE), designed to autonomously augment simple\nQA pairs with high-quality, step-by-step reason-\ning processes. The SI-CoTE process operates it-\neratively: the model generates potential reasoning\nchains for a subset of the data, and through Re-\njection Sampling (Liu et al., 2023), we retain only\nthose chains that lead to the correct answer. This\nnewly generated high-quality CoT data is then used\nto fine-tune the model, yielding a more capable ver-\nsion. This enhanced model, in turn, processes the\nnext subset of data, creating a virtuous cycle where\nboth the training data quality and the model’s rea-\nsoning abilities evolve synergistically. Using this\nefficient self-improvement strategy, we developed\nZhiMingTang-M1 (ZMT-M1) by fine-tuning the\npowerful Deepseek-R1 (DeepSeek-AI et al., 2025)\nfoundation model.\nWe conducted extensive experiments on TCM-\nEval using both open-source and closed-source\nmodels. ZMT-M1 achieved an impressive aver-\nage score of 96.32, significantly outperforming all\n34 competing models. It also far exceeds the 60-\npoint passing score required for human practition-\ners in the official examination. This demonstrates\nthat ZMT-M1 has attained expert-level knowledge\nand reasoning abilities in TCM, matching or even\nsurpassing the capabilities of qualified human pro-\nfessionals. Additionally, we observed a significant\ndata leakage issue in current models, where perfor-\nmance on newly added questions was notably lower\ncompared to older questions. In contrast, ZMT-M1\nshowed no such decline, maintaining consistent\nperformance across both old and new questions.\nIn summary, our key contributions are threefold:\n• We introduce TCM-Eval, the first dynamic,\nextensible, and expert-validated benchmark\nspecifically designed for TCM.\n• We build a high-quality QA dataset from mock\nexams and authoritative TCM textbooks. Us-\ning the SI-CoTE method, we expanded it into\nQA pairs with reasoning chains. This enriched\ndataset is used to train ZMT-M1, establishing\na new SOTA in the TCM domain.\n• We establish the first comprehensive evalua-\ntion platform for TCM LLMs, offering a stan-\ndardized testbed and a public leaderboard to\ndrive research in the field.\n2\nRelated Work\n2.1\nLarge Language Models for Traditional\nChinese Medicine\nIn recent years, a series of Large Language Mod-\nels for TCM have emerged, which can be catego-\nrized into three types: (1) Large language models\nbased on external TCM knowledge bases; (2) Large\nlanguage models with basic conversational capa-\nbilities in TCM; (3) Large language models with\nexpert-level reasoning capabilities in TCM.\nOne type of research integrates knowledge from\nexternal Traditional Chinese Medicine (TCM)\nknowledge bases into open-source large language\nmodels to assist the models in generating responses.\nTCM-KLLaMA (Zhuang et al., 2025) extracts\nknowledge from knowledge graphs and injects it\ninto the input text of the model; BenTsao (Wang\net al., 2023), OpenTCM (He et al., 2025) utilizes\nknowledge graph based retrieval enhanced gener-\nation technology to explicitly utilize knowledge\nfrom the knowledge graph bases during inference.\nHowever, they remain confined to guidance via\nsimple prompt engineering or reliance on retrieval\nfrom external knowledge bases to assist decision-\nmaking, and these approaches have not truly ex-\npanded the inherent knowledge boundaries of TCM\nLLMs.\n2\n"}, {"page": 3, "text": "Datasets\nDynamic\nTCMLE-\nTextbook\nReal\nMock\nExpert-\nScale\nSpecific\nExam\nExam\nReviewed\nTCM-Bench (Yue et al., 2024)\n✗\n✔\n✗\n✔\n✗\n✔\n✗\n5,473\nTCM-3CEval (Huang et al., 2025)\n✗\n✗\n✔\n✗\n✗\n✔\n450\nMTCMB (Kong et al., 2025)\n✗\n✗\n✗\n✔\n✗\n✔\n7,100\nTCMEval-SDT (Wang et al., 2025)\n✗\n✗\n✔\n✗\n✗\n✔\n300\nTCM-Ladder (Xie et al., 2025)\n✗\n✗\n✔\n✔\n✗\n✔\n52K\nOphthBench (Zhou et al., 2025)\n✗\n✔\n✗\n✔\n✔\n✗\n✔\n591\nTCM-Eval(Ours)\n✔\n✔\n✔\n✔\n✔\n✔\n6,099\nTable 1: Overview of Datasets in the TCM Domain. TCMLE-Specific refers to datasets specifically designed for\nthe TCM Licensing Examination.\nOne type of research applies simple question-\nanswer pairs for Supervised Fine-Tuning (SFT)\n(Ding et al., 2023) on open-source large language\nmodels, enabling the models to acquire TCM\ndomain knowledge.\nTCMLLM (Haoyu et al.,\n2024), TCM-FTP (Zhou et al., 2024), ShenNong\nTCM-LLM (Zhu et al., 2023), BianCang (Sibo\net al., 2024), TCMChat (Dai et al., 2024) are fine-\ntuned on instructions based on TCM knowledge\nand Chinese question answering; BianQue (Chen\net al., 2023) , Qibo (Jia et al., 2025) and CMLM-\nZhongJing (Kang et al., 2025) build multi-turn di-\nalogue datasets based on doctor-patient roles and\ninquiry scenarios to enhance their consultation ca-\npabilities.\nHowever, these models only suitable for simple\ndialogue scenarios. Plagued by low-quality fine-\ntuning data, they are prone to hallucinations and\nthus cannot truly match the reasoning capabilities\nof human TCM practitioners. In contrast, through\nRejection Sampling, our SI-CoTE automatically\nvalidate and retain the correct ground-truth answer,\nthereby augmenting simple QA pairs with high-\nquality, step-by-step reasoning processes.\nThe other type of research adopts chain-of-\nthought (CoT) (Wei et al., 2023) data for SFT\non open-source large language models, allowing\nthe models to not only grasp the basic knowledge\nof the TCM domain but also possess the reason-\ning capabilities of TCM domain experts. Qibo\n(Jia et al., 2025) and Lingdan (Hua et al., 2024)\nconstruct Chain-of-Thought for TCM consultation\nprompts, endowing the models with preliminary\nthinking logic and reasoning capabilities; JingFang\n(Yang et al., 2025b) improves the model’s abil-\nity in comprehensive clinical consultation and pre-\ncise syndrome differentiation through the design\nof Multi-Agent Collaborative Chain-of-Thought\nMechanism (MACCTM) and Dual-Stage Recovery\nScheme (DSRS).\nHowever, these methods only apply static CoT\ndata to prompts for model reasoning, failing to\nform dynamic iteration of CoT data.In contrast, our\nZMT-M1 autonomously enriches question-answer\npairs with validated reasoning chains through re-\njection sampling, establishing a virtuous cycle of\ndata and model co-evolution.\n2.2\nDatasets for Traditional Chinese Medicine\nIn recent years, evaluation benchmarks specifi-\ncally designed for the TCM domain have emerged.\nTCMBench (Yue et al., 2024) targets practitioner\nexams across 16 knowledge areas, while TCM-\n3CEval (Huang et al., 2025) and MTCMB (Kong\net al., 2025) broaden the scope to include litera-\nture understanding, diagnostic reasoning, and pre-\nscription recommendation. Benchmarks such as\nTCMEval-SDT (Wang et al., 2025) and TCM-\nLadder (Xie et al., 2025) emphasize structured and\nmultimodal reasoning, with the latter incorporat-\ning hierarchical difficulty and text–image inputs.\nComprehensive frameworks like TCMBench (Yue\net al., 2024) cover both theory and clinical decision-\nmaking, and cross-domain efforts such as Ophth-\nBench (Zhou et al., 2025) demonstrate adaptability\nto specialized fields. However, most existing bench-\nmarks are static and narrow in coverage limiting\ntheir long-term applicability. In contrast, the TCM-\nEval dataset introduces a dynamic, expert-validated\nbenchmark based on NMLE-TCM, with a system-\natically constructed training corpus derived from\n18 authoritative textbooks and mock exams.\n3\nData Collection\nOur proposed dataset can be divided into two\nparts: TCM-Eval, a benchmark for Traditional\n3\n"}, {"page": 4, "text": "TCM-Eval\n     Generate practice exercises\n based on \nHigh quality\nHard to Utilize\nText Documents\nData Cleaning\nInspection\nKnowledge 1\nKnowledge 2\nKnowledge 3\n: Sure,  here are...\nTCM-Corpus\nTCM Doctor\nTeams\nHigh quality\nEasy to Utilize\nForums & \nplatforms \nfrom medical\n universities\nthe National\nQualification \nExamination\n18 Authoritative Paper\nTextbooks\nWebsite\nOnline Q&A\nLow Quality\nEasy to Utilize\nMock\nExam\nReal\nExam\nInspection\nDynamic\nRefresh\nDataset\nnovel\nexam\nFigure 2: Data Collection Pipeline. Training Data is used for model training, and Test Data is continuously updated\nfor model evaluation. Ordinary websites and online Q&A were excluded to ensure data quality.\nChinese Medicine (TCM) developed with expert\noversight and inter-rater consistency to ensure ac-\ncuracy(Section 3.1); and TCM-Corpus, a training\ncorpus constructed via an expert–LLM collabora-\ntive pipeline enhanced by our Self-Iterative Chain-\nof-Thought Enhancement (SI-CoTE) framework,\nwhich was used to train our model (Section 3.2).\nWe apply rigorous quality control protocols to both\nparts (Section 3.3), and report their distributions,\ndomain coverage, and key statistics (Section 3.4).\n3.1\nTCM-Eval\nThe National Qualification Examination for Tra-\nditional Chinese Medicine (TCM) Practitioners,\nrecognized as the authoritative professional certifi-\ncation in China, integrates the full scope of TCM\nacross 18 official textbooks, making it the field’s\nde facto gold standard. TCM-Eval is designed to\nrigorously evaluate the reasoning, memorization,\nand application abilities of models in the TCM do-\nmain by leveraging this gold standard. Unlike the\nTCM-Corpus, which emphasizes knowledge acqui-\nsition and exposure to diverse question formats,\nTCM-Eval exclusively prioritizes authenticity, re-\nliability, and dynamic renewal to ensure fair and\nrepresentative benchmarking.\nSource of Examination Items\nTo reflect real-\nworld competence requirements, the test items\nwere derived from ten years of authentic questions\nin the National Qualification Examination for TCM\nPractitioners. This examination is recognized as the\ngold standard for evaluating practitioners’ mastery\nof TCM knowledge and clinical reasoning skills.\nThe coverage spans classical theories, diagnostic\nmethodologies, herbal prescriptions, acupuncture\ntechniques, and integrative applications with mod-\nern medicine, thereby offering a comprehensive\nbasis for evaluating model performance.\nDynamic Refresh Mechanism\nA potential con-\ncern for long-term benchmarks in domain-specific\nlarge models is the risk of data leakage, which\ncould compromise the validity of test results. To\naddress this, we collaborated with a panel of li-\ncensed TCM professionals to periodically design\nand release novel examination sets that closely fol-\nlow the style and difficulty of the official exam-\nination, yet are never exposed in public training\ncorpora. This dynamic update mechanism ensures\nthat the benchmark remains robust against mem-\norization and provides a continually challenging\nenvironment for evaluating genuine reasoning abil-\nity.\n3.2\nTCM-Corpus\nThe domain of Traditional Chinese Medicine\n(TCM) encompasses not only a broad spectrum\nof knowledge ( including both classical TCM the-\nories and modern medical concepts ) but also con-\nstitutes a self-contained system of dialectical rea-\nsoning, which introduces unique challenges for\nmodel training. Drawing inspiration from the au-\nthentic learning process of human students, we\ndesign a data collection pipeline that systemati-\ncally extracts knowledge-oriented question–answer\npairs from textbooks and acquires mock examina-\ntion items through automated procedures, thereby\nemulating the dual processes of study and practice\nin TCM education. Furthermore, we propose a\nself-iterative framework to generate high-quality\nChain-of-Thought (CoT) data.\n4\n"}, {"page": 5, "text": "Unit\n2003\n2004\n2007\n2008\n2009\n2012\n2013\n2016\n2022\n2024\nHC\nSubtotal\nUnit 1\n135\n131\n150\n149\n100\n150\n150\n150\n150\n142\n150\n1415\nUnit 2\n135\n142\n150\n150\n100\n150\n150\n150\n150\n105\n150\n1427\nUnit 3\n135\n135\n150\n150\n136\n148\n150\n150\n150\n52\n150\n1454\nUnit 4\n135\n133\n150\n145\n120\n150\n150\n150\n150\n71\n150\n1433\nTotal\n540\n541\n600\n594\n456\n598\n600\n600\n600\n370\n600\n6099\nTable 2: Statistics of the test set. The test set comprises 10 years of authentic TCM examination papers, with each\nyear covering 4 units that span the full spectrum of knowledge in traditional Chinese medicine. All data have been\nrigorously annotated and quality-controlled by a team of professional physicians.\nTextbook Extraction\nDue to the scarcity of high-\nquality datasets in the TCM domain and the unre-\nliability of online sources, only the 18 authorita-\ntive textbooks were utilized to ensure data quality.\nHigh-quality text data were extracted using auto-\nmated scanning tools and OCR. Since the raw OCR\noutput contained numerous irrelevant and noisy\ncharacters, manually designed rules were applied\nto remove extraneous elements such as headers,\nfooters, and other artifacts.\nUsing the collected textbook data, we segmented\nthe textbook data into coherent blocks. to achieve\ncomprehensive coverage of knowledge points. The\nsegmentation was guided by chapter titles and con-\ntrolled by text length to maintain readability. Subse-\nquently, manual inspection was performed to refine\nthe boundaries, ensuring that individual knowledge\npoints remained intact and were not fragmented\nacross multiple segments.\nTo enable the model to acquire TCM knowl-\nedge progressively, we designed prompts to guide\nDeepSeek-v3 in generating question–answer pairs\nof varying difficulty from textbook segments. Since\nTCM knowledge includes a substantial proportion\nof memorization-based content, fill-in-the-blank\nquestions were used as the primary format. To en-\nhance diversity and align with examination styles,\nwe additionally generated multiple-choice ques-\ntions.\nMock Exam Harvesting\nTo familiarize the\nmodel with the format and style of China’s TCM\nPractitioner Qualification Examination, automated\nweb crawlers were employed to collect over 60,000\ncandidate mock exam questions from university-\nhosted forums and platforms. The raw data con-\ntained invalid options, invalid answers, and dupli-\ncates. Rule-based filtering was applied to remove\nincomplete or malformed items, while deduplica-\ntion based on question-stem similarity was used to\nreduce redundancy.\nSI-CoTE\nTo training our reasoning model —\nZhiMingTang, we proposed Self-Iterative Chain-\nof-Thought Enhancement (SI-CoTE) framework.\nThis framework is built upon the Deepseek-R1\n(Guo et al., 2025) base model, denoted as M0.\nAt its core, the SI-CoTE methodology employs\na phased, iterative process to efficiently convert\nthe original Question-Answer (QA) pair dataset,\nDQA, into an enhanced dataset containing high-\nquality CoT data, thereby driving the evolution of\nthe model’s capabilities.\nInitially, we partition the original QA dataset\ninto K disjoint subsets:\nDQA = D1 ∪D2 ∪· · · ∪DK\n(1)\nOur training process then unfolds iteratively\nover these subsets. In the k-th iteration (for k =\n1, . . . , K), we focus on generating high-quality\nCoT for the QA pairs (Qi, A∗\ni ) ∈Dk.\nSpecifically, in the k-th iteration, we leverage the\nmodel from the previous iteration, Mk−1, where\nM0 is the initial base model, to generate candidate\nCoT and answer pairs (Ti, Ai) for each question\nQi in subset Dk. The generated answers are then\nfiltered using a verification function. This Rejec-\ntion Sampling step retains a candidate if and only\nif its generated answer Ai is consistent with the\nground-truth answer A∗\ni . This condition is formally\nexpressed as:\nV (Ai, A∗\ni ) = 1\n(2)\nFor questions where the model fails to produce\na correct answer after multiple attempts, these\n\"hard cases\" are identified and annotated with high-\nquality CoT by medical experts. Through this com-\nbined approach of machine generation-verification\nand human-in-the-loop assistance, we construct a\nhigh-quality CoT dataset, DCoT,k, for the corre-\nsponding data subset Dk.\n5\n"}, {"page": 6, "text": "Dataset\nDescription\nSource\nItems\nTokens\nTCM Internal Medicine\nDiagnosis and treatment of internal diseases\n+\n46,599\n20.6M\nTCM Surgery\nDiagnosis and treatment of surgical diseases\n+\n45,031\n18.2M\nInfectious Diseases\nPrevention and treatment of infectious diseases\n+\n39,953\n15.4M\nTCM Pediatrics\nDiagnosis and treatment of pediatric diseases\n+\n35,609\n14.6M\nChinese Materia Medica\nKnowledge of Chinese medicinal herbs\n+\n31,620\n13.1M\nHealth Law and Regulations\nMedical laws and regulations\n+\n29,110\n11.1M\nTCM Diagnostics\nDiagnostic methods in TCM\n+\n28,389\n12.0M\nBasic Theory of TCM\nFundamentals of Traditional Chinese Medicine\n+\n24,690\n9.6M\nAcupuncture and Moxibustion\nAcupuncture, moxibustion, and Tuina\n+\n22,974\n9.7M\nChinese Herbal Formulas\nFormula composition and compatibility\n+\n15,051\n6.6M\nTCM Ethics\nMedical ethics in TCM\n+\n14,688\n5.6M\nTCM Gynecology\nGynecological diseases\n+\n11,952\n5.6M\nWarm-Febrile Diseases\nTheory and treatment of warm-febrile diseases\n10,106\n3.7M\nShang Han Lun\nTheory of cold damage diseases\n7,823\n2.8M\nJin Gui Yao Lue\nTheory of miscellaneous diseases\n6,465\n2.4M\nHuangdi Neijing\nThe Yellow Emperor’s Inner Canon\n5,688\n2.2M\nOther\nOther subjects\n9,059\n5.8M\nTotal\n384,807\n159M\nTable 3: Statistics of the training set. This dataset provides comprehensive knowledge coverage across 16\nTraditional Chinese Medicine (TCM) domains, constructed from 18 authoritative textbooks and 1601 manually\ncollected mock exams. In the \"Source\" column, the\nicon indicates data derived from textbooks, while the\nicon represents data from mock exams.\nThe key to this framework lies in the accumula-\ntion of data and the iterative evolution of the model.\nThe newly generated CoT data from each iteration\nis aggregated into a cumulative SFT training set.\nAt the conclusion of the k-th iteration, this set is\nupdated as follows:\nD(k)\nSFT =\nk[\nj=1\nDCoT,j\n(3)\nWe then fine-tune the base model M0 on this\ncontinuously expanding and refined dataset to yield\na more capable next-generation model, Mk:\nMk = SFT(M0, D(k)\nSFT)\n(4)\nHaving learned the reasoning patterns from the\nfirst k subsets, the enhanced model Mk exhibits\nstronger reasoning capabilities and a higher suc-\ncess rate when processing the subsequent subset,\nDk+1. This process continues until all K subsets\nhave been processed.\nThe final model, ZhiM-\ningTang, denoted as MK, is trained on the com-\nplete, high-quality CoT dataset D(K)\nSFT. Through this\nbatch-wise, iterative enhancement approach, the SI-\nCoTE framework achieves a synergistic evolution\nof model capability and data quality.\n3.3\nQuality Control\nTo ensure correctness of TCM-Eval and to improve\nthe coherence of TCM-Corpus, we implement a\nmulti-stage quality control protocol, supporting re-\nliable model evaluation and training.\nFor TCM-Eval, each test item was manually col-\nlected and curated by a dedicated team of medical\nexperts. Every question underwent multi-round an-\nnotation and verification by independent reviewers\nto guarantee accuracy in both stems and answers.\nItems with ambiguous wording or multiple plau-\nsible answers were excluded to avoid confound-\ning evaluation outcomes. This strict quality assur-\nance process ensures that the final test dataset is\ncomposed of high-quality, unambiguous, and exam-\nstandard questions, suitable for serving as a reliable\nyardstick of model performance.\nFor TCM-Corpus, a model-in-the-loop valida-\ntion procedure was introduced (Liu et al., 2020;\nCheng et al., 2025; Liu et al., 2025). Items con-\nsistently answered correctly by the model were re-\ntained as high-confidence samples, while items an-\nswered incorrectly were flagged for stricter review,\nincluding both automated heuristics and human in-\nspection. This hybrid validation process yielded a\nfinal curated dataset.\n6\n"}, {"page": 7, "text": "3.4\nData Statistics\nWe conducted a detailed statistical analysis of the\ntraining and test sets for TCM-Eval, as shown in Ta-\nble 3 and Table 2. The training set covers 16 knowl-\nedge domains in traditional Chinese medicine, con-\nstructed from 1,601 sets of practice questions and\n18 authoritative official textbooks, comprising a to-\ntal of 384,807 instruction-tuning samples and 159\nmillion tokens. These data enable the model to ac-\nquire foundational knowledge and key examination\ntopics in TCM during the post-training phase.\nThe test set consists of 6,099 questions, span-\nning 10 years of official exam questions, uniformly\ndistributed across four modules to assess different\nmodel capabilities. To prevent data leakage and\nensure the integrity of model evaluation, a team of\nprofessional TCM practitioners manually curated\n600 high-quality questions. This dynamic test set\nwill be continuously maintained and updated.\n4\nExperiment\n4.1\nSetup\nModels\nFollowing previous work (Chen et al.,\n2025; Cheng et al., 2025), we conducted extensive\nexperiments on a wide range of open-source and\nAPI-based models.\nSpecifically, the open-sourced models encom-\npass various series with different parameter scales.\nFor general-purpose large language models, our\nselection includes Qwen3 series (8B, 14B, 32B)\n(Yang et al., 2025a), Llama-3 series (8B, 70B)\n(Grattafiori et al., 2024), Deepseek series (V3.1,\nR1) (Liu et al., 2024a; Guo et al., 2025), Baichuan-\nM2 (32B) (Dou et al., 2025), and Mistral-8B1, GPT-\noss series (20B, 120B) (OpenAI, 2025), the DS-\nQwen series (7B, 14B, 32B), and the DS-Llama\nseries (8B, 70B). To account for domain-specific\napplications, we also incorporated models known\nfor their performance in the medical field, namely\nthe MedGemma series (4B, 27B) (Sellergren et al.,\n2025) and the ShiZhenGPT series (7B, 32B) (Chen\net al., 2025).\nFor the API-based models, our evaluation in-\ncludes services from various providers.\nThese\ninclude GLM-4 series (GLM-4.5, GLM-4.5-Air)\n(Team et al., 2025a), Kimi-K2-Instruct (Team et al.,\n2025b), Baichuan42, Ernie-x1-turbo-32k (), Spark-\n4.0-Ultra3, MiniMax-M1 (MiniMax et al., 2025),\n1https://mistral.ai/news/ministraux\n2https://platform.baichuan-ai.com/\n3https://xinghuo.xfyun.cn/\nLongCat-Flash-Chat (Team, 2025), and GPT-4 se-\nries4 (GPT-4o, GPT-4.1).\nBenchmarks\nAs introduced in Section 3.1, we\nconstructed TCM-Eval to comprehensively evaluate\nthe knowledge and reasoning capabilities of LLMs\nin the field of TCM. We use this as our primary\nbenchmark, testing the accuracy of each model on\ndifferent question sets.\nImplemental Details\nTo ensure the reproducibil-\nity and fairness of our results, we standardized\nthe inference and fine-tuning procedures for all\nmodels. For inference, we employed the efficient\nvLLM framework (Kwon et al., 2023) as the uni-\nfied engine for all open-sourced models. For non-\nreasoning or deterministic generation tasks, we\nset the decoding temperature to 0 with both top-\np and top-k configured to 0.1, whereas for tasks\nrequiring reasoning, the temperature was set to\n0.6. In the Supervised Fine-Tuning (SFT) phase,\nwe customized the SFTTrainer from the Hugging\nFace Transformers library (Wolf et al., 2020) and\nadopted the parameter-efficient Low-Rank Adap-\ntation (LoRA) (Hu et al., 2022). All experiments\nwere conducted on a single node equipped with\neight H20 GPUs, each with 141 GB of VRAM, and\nthe entire training phase consumed approximately\n7,000 GPU-hours.\n4.2\nMain Results\nAs shown in Table 4, we conducted experiments\non a wide range of models using the past ten\nyears of official examination questions as well as\nHuman-Crafted items, and subsequently computed\nthe Overall score. From these results, we can draw\nthe following conclusions:\nZMT-M1 demonstrated the most outstanding per-\nformance in the qualification examination for\nTCM practitioners.\nAs shown in Table 4, it\nachieved the highest scores in 10 out of 11 test sub-\nsets. With the inclusion of updated test items (such\nas the 2024 official exam questions and Human-\nCrafted items), ZMT-M1’s advantage has further\nexpanded. Because these items carry a lower risk\nof data leakage, they provide a more authentic re-\nflection of the model’s true capabilities, thereby\nunderscoring ZMT-M1’s robustness and authority\nin the field of TCM.\n4https://chatgpt.com/\n7\n"}, {"page": 8, "text": "Model\nSize\n2003\n2004\n2007\n2008\n2009\n2012\n2013\n2016\n2022\n2024\nHC\nOverall\nOpen-Sourced\nQwen3-8B\n8B\n69.46\n67.16\n64.36\n61.47\n71.05\n63.82\n64.66\n65.77\n63.38\n76.76\n68.17\n66.45\nQwen3-14B\n14B\n79.33\n80.60\n78.89\n76.88\n77.19\n76.88\n78.89\n81.98\n77.42\n80.00\n75.33\n78.35\nQwen3-32B\n32B\n85.29\n86.01\n84.97\n86.13\n85.09\n85.26\n83.42\n88.51\n86.29\n83.51\n81.50\n85.04\nQwen3-30B-A3B\n30B\n57.36\n61.75\n60.47\n65.24\n62.28\n62.98\n59.90\n68.92\n55.69\n68.11\n53.83\n61.07\nQwen3-235B-A22B\n235B\n95.34\n95.52\n92.40\n95.55\n96.05\n92.63\n94.64\n92.57\n93.81\n87.84\n91.00\n93.52\nGPT-oss-20B\n20B\n35.57\n37.69\n36.82\n39.90\n36.40\n37.19\n37.19\n40.32\n41.47\n45.14\n40.17\n38.72\nGPT-oss-120B\n120B\n54.38\n58.40\n52.87\n54.28\n55.70\n54.44\n57.45\n59.23\n60.20\n59.19\n56.56\n56.49\nDS-Qwen-7B\n7B\n27.75\n33.21\n28.21\n30.14\n28.29\n30.49\n32.83\n32.43\n33.11\n33.78\n33.83\n31.25\nDS-Qwen-14B\n14B\n74.30\n77.24\n77.20\n75.00\n75.88\n74.54\n73.53\n78.38\n77.42\n72.43\n75.33\n75.60\nDS-Qwen-32B\n32B\n77.09\n82.28\n78.55\n79.11\n80.92\n75.38\n79.40\n83.78\n80.94\n74.05\n79.17\n79.17\nDS-Llama-8B\n8B\n28.12\n27.61\n25.34\n27.74\n27.19\n30.49\n32.33\n32.33\n27.70\n30.27\n30.17\n28.74\nDS-Llama-70B\n70B\n56.42\n58.96\n56.42\n58.90\n55.92\n55.28\n58.29\n56.98\n61.04\n55.41\n58.00\n57.54\nDeepseek-V3.1\n685B\n91.43\n93.66\n91.05\n91.78\n91.67\n90.79\n90.62\n90.54\n88.96\n88.65\n86.83\n90.54\nDeepseek-R1\n671B\n80.91\n93.28\n90.88\n90.41\n88.82\n89.45\n89.78\n90.32\n89.30\n85.68\n90.56\n89.04\nLlama-3.1-8B\n8B\n47.11\n51.49\n44.76\n48.46\n47.59\n48.58\n48.58\n48.87\n50.50\n48.11\n50.83\n48.66\nLlama-3.3-70B\n70B\n65.74\n71.08\n69.43\n73.12\n70.39\n65.66\n65.16\n68.24\n69.23\n67.03\n67.17\n68.38\nMinistral-8B\n8B\n33.40\n34.96\n33.85\n34.49\n33.19\n35.81\n34.68\n31.53\n36.35\n28.92\n35.68\n33.90\nMedGemma-4B\n4B\n30.04\n34.33\n29.47\n31.56\n32.16\n30.81\n31.64\n32.35\n32.09\n35.89\n34.39\n32.25\nMedGemma-27B\n27B\n44.67\n51.96\n43.05\n48.37\n49.45\n44.46\n48.24\n48.75\n50.08\n57.53\n50.17\n48.79\nBaichuan-M2\n32B\n51.58\n55.04\n60.30\n63.18\n56.36\n60.13\n54.61\n63.06\n50.00\n54.32\n44.67\n55.63\nShiZhenGPT-7B\n7B\n79.58\n83.69\n82.69\n82.77\n81.43\n81.50\n83.74\n81.92\n82.53\n75.64\n82.64\n81.65\nShiZhenGPT-32B\n32B\n89.33\n93.22\n90.14\n93.85\n90.62\n88.89\n90.02\n91.16\n89.57\n87.29\n87.78\n90.17\nAPI-Based\nGLM-4.5\n358B\n85.29\n86.19\n84.12\n86.64\n85.53\n86.93\n85.76\n89.64\n82.27\n85.14\n79.33\n85.03\nGLM-4.5-Air\n110B\n75.98\n79.66\n77.87\n79.62\n79.61\n78.39\n79.40\n82.66\n75.42\n77.84\n73.00\n77.99\nMiniMax-M1\n456B\n71.32\n76.12\n74.66\n76.88\n75.88\n75.04\n73.70\n76.58\n71.91\n74.32\n73.32\n74.52\nGPT-4o\n–\n71.14\n77.24\n71.62\n73.46\n75.00\n73.20\n77.89\n77.48\n76.25\n67.30\n75.32\n74.17\nGPT-4.1\n–\n73.18\n78.17\n71.62\n72.09\n73.46\n72.70\n75.21\n78.83\n75.92\n72.43\n72.83\n74.22\nBaichuan4\n–\n92.92\n93.10\n92.91\n93.32\n92.54\n92.29\n91.44\n91.67\n91.14\n83.78\n88.33\n91.22\nLongCat-Flash-Chat\n560B\n91.25\n95.71\n92.23\n92.98\n91.89\n91.96\n91.46\n92.12\n91.25\n88.89\n87.83\n91.60\nHunyuan-T1\n–\n91.25\n92.91\n92.74\n95.38\n93.86\n92.96\n92.80\n94.37\n92.14\n91.08\n88.00\n92.47\nKimi-K2-Instruct\n1T\n94.79\n97.76\n95.10\n95.38\n96.05\n94.47\n94.97\n95.27\n92.14\n92.16\n79.70\n93.30\nSpark-4.0-Ultra\n–\n88.83\n93.10\n91.05\n90.58\n89.91\n89.78\n91.29\n91.44\n90.47\n85.41\n87.17\n90.02\nErnie-x1-turbo-32k\n–\n87.90\n90.11\n86.82\n85.96\n91.45\n85.43\n84.59\n84.68\n83.11\n90.54\n89.43\n87.27\nZMT-M1 (Ours)\n671B\n97.02\n96.27\n96.96\n96.58\n96.93\n95.98\n95.98\n95.95\n94.48\n97.03\n95.67\n96.26\nTable 4: Comparison of open-sourced and API-based models across years, with an HC representing the Hand-\nCrafted set and Overall column reporting the average score. The best , second-best , and third-best results in\neach column are marked with purple, orange, and gray backgrounds, respectively.\nCompared with Deepseek-R1, our improvements\nare substantial, fully demonstrating the high\nquality of the training corpus.\nAlthough the un-\ntrained Deepseek-R1 already achieved an Overall\nscore of 89.04, it still lagged behind the state-of-\nthe-art models. After training, however, ZMT-M1’s\nscore increased to 96.17, representing a gain of\n7.13 points. This indicates that our training cor-\npus—comprising textbooks and mock exam ques-\ntions—provides comprehensive coverage of knowl-\nedge points in Traditional Chinese Medicine and\ndelivers targeted enhancement of the model’s ex-\namination performance.\nModels developed by Chinese enterprises or aca-\ndemic institutions tend to perform better in the\nfield of TCM.\nFor instance, as shown in the ta-\nble, although GPT-4o and GPT-4.1 are among the\n8\n"}, {"page": 9, "text": "strongest models across most other domains, their\nperformance in TCM is even inferior to that of\nQwen3-8B. We attribute this primarily to differ-\nences in the proportion of TCM-related content\nwithin the training corpora, as well as variations in\nthe models’ capabilities for processing Chinese.\n5\nAnalysis\nIn this section, we conduct a comprehensive analy-\nsis to answer the three research questions RQ1:\nHow does the model perform across different\nsub-domains or tasks within traditional Chinese\nmedicine? (Sec 5.1) RQ2: Is the SI-CoTE train-\ning approach universally effective across different\nmodel architectures? (Sec 5.2) RQ3: Through case\nstudies, does the model genuinely acquire knowl-\nedge specific to traditional Chinese medicine after\ntraining? (Sec 5.3)\n5.1\nAnalysis of Model Performance Across\nVarious Aspects of TCM\nFigure 3: Performance distribution of ZMT-M1 and\nbaseline models across four evaluation units. ZMT-\nM1 exhibits the most balanced performance profile with\nstable scores on all units. In contrast, baseline models\ndemonstrate significant performance variations, particu-\nlarly weaker on Units 3 and 4 compared to Unit 2.\nAs shown in Table 2, TCM-Eval is divided into\nfour units, each designed to assess a distinct capa-\nbility within the domain of TCM. We have sepa-\nrately evaluated and compared the performance of\nvarious models across these four units. Figure 3 re-\nveals a significant variance in the models’ abilities\nacross different dimensions of TCM. Specifically,\nthe scores in Unit 4 are substantially lower than\nthose in Unit 2, indicating that existing models\nhave a notable deficiency in clinical application\nskills. In contrast, our proposed ZMT-M1 model\nnot only surpasses other models in every unit but\nalso demonstrates the most balanced proficiency,\nachieving the lowest score variance. This supe-\nrior and well-rounded performance is attributed to\nthe comprehensive coverage of knowledge points\nduring our model’s training phase.\n5.2\nGeneralizability of the SI-CoTE\nModel\nSize\nOA (original)\nOA (w/ fine-tuning)\n∆%\nrel\nQwen3-8B\n8B\n66.45\n82.91\n↑24.77\nQwen3-14B\n14B\n78.35\n86.79\n↑10.77\nQwen3-32B\n32B\n85.04\n88.21\n↑3.73\nDS-Qwen-7B\n7B\n31.25\n81.52\n↑160.86\nDS-Llama-8B\n8B\n28.74\n77.35\n↑169.14\nDS-Qwen-14B\n14B\n75.60\n86.31\n↑14.17\nDS-Qwen-32B\n32B\n79.17\n87.18\n↑10.12\nTable 5: Comparison of overall accuracy before and\nafter fine-tuning models on Chain-of-Thought train-\ning data generated with SI-CoTE.\nTo ascertain the efficacy of the CoT augmented\ntraining data from Section ??, we conducted fine-\ntuning experiments on open-source models with\nknown deficiencies in the TCM domain. As pre-\nsented in Table 5, fine-tuning the Qwen3 and\nDeepseek-Distill series with our data yielded sig-\nnificant performance enhancements. The improve-\nments were particularly pronounced for smaller\nmodels, which saw performance boosts of up to\n135%, rivaling the capabilities of API-based mod-\nels. These findings underscore the high quality\nand broad coverage of the data produced by our\nSI-CoTE iterative method, confirming its effective-\nness in advancing model performance in the TCM\nfield.\n5.3\nCase Study\nThis section presents a qualitative, side-by-side\ncomparison of DeepSeek-R1 and ZMT-M1 (Fig-\nure 4), highlighting key differences in their ap-\nproach to and understanding of TCM. The ques-\ntion shown in the figure combines textbook knowl-\nedge with clinical diagnostic experience. It is evi-\ndent that ZMT-M1 has a broader knowledge base\nwithin the scope of TCM compared to DeepSeek-\nR1. ZMT-M1 demonstrates a better grasp of text-\nbook knowledge and integrates real clinical experi-\nence. Additionally, ZMT-M1 provides more con-\ncise and accurate answers, reflecting its greater\nexpertise in the field of TCM.\n9\n"}, {"page": 10, "text": "Query：对于流行性脑脊髓膜炎，以下哪项检查最常用于确诊\nFor epidemic cerebrospinal meningitis, which of the following examinations is most commonly used for definitive diagnosis?\nA.瘀点挤液涂片染色镜检\nB.脑脊液培养\nC.血培养\nD.特异性抗体检测 \nE.骨髓培养\nA. Petechial aspiration smear staining and microscopy\nB. Cerebrospinal fluid culture\nC. Blood culture\nD. Specific antibody detection\nE. Bone marrow culture\nExplanation: The definitive diagnosis of epidemic cerebrospinal\nmeningitis depends on the isolation and culture of the pathogen.\nCerebrospinal fluid (CSF) culture (Option B) can directly isolate\nNeisseria meningitidis from the site of infection and is considered the\ngold standard for diagnosis. Although petechial smear (Option A) is\nrapid and has a relatively high early positive rate, it may yield false\nnegatives and must be interpreted alongside clinical findings. Blood\nculture (Option C) has a low positive rate during the bacteremic\nphase, specific antibody detection (Option D) is mainly used for\nretrospective analysis, and bone marrow culture (Option E) is not\napplicable for diagnosing meningococcal meningitis. Therefore, CSF\nculture remains the most definitive diagnostic method.\nExplanation: In the diagnosis of epidemic cerebrospinal\nmeningitis, smear microscopy of petechial or ecchymotic\nlesions has a higher positive rate and provides faster\nresults. Therefore, the most commonly used method for\ndefinitive diagnosis of epidemic cerebrospinal meningitis is\nstained smear examination of petechial exudate (Option A).\nAlthough methods such as CSF culture, blood culture,\nspecific antibody detection, and bone marrow culture can\nalso aid in diagnosis, they are not the most commonly used\nconfirmatory tests. Hence, the correct answer is Option A. \nAnswer：A\nExplanation：流脑诊断时，皮肤瘀点、瘀斑涂片具有更高\n的阳性率及更加快速，所以最常用于确诊流行性脑脊髓膜炎\n的方法是瘀点挤液涂片染色镜检。而脑脊液培养、血培养、\n特异性抗体检测、骨髓培养等方法虽然也可用于诊断，但不\n是最常用的确诊方法。答案选A。\nAnswer：B\nExplanation：流行性脑脊髓膜炎的确诊依赖于病原体的分离培\n养。脑脊液培养（选项B）可直接从感染部位获取脑膜炎奈瑟\n菌，是诊断的金标准。虽然瘀点涂片（选项A）快速且早期阳性\n率较高，但可能存在假阴性，且需结合临床表现。血培养（选项\nC）在菌血症期阳性率低，特异性抗体检测（选项D）多用于回\n顾性分析，骨髓培养（选项E）不适用于流脑诊断。因此，脑脊\n液培养是最具确诊价值的检查方法。\n(a) Deepseek-R1\n(b) ZhiMingTang\nThe factual errors are caused by a lack of\nknowledge in the field of TCM!!!\nAfter specialized training in the field of TCM, the\nmodel becomes proficient in TCM knowledge.\nFigure 4: Case Study. This study demonstrates that after specialized training in TCM using the SI-CoTE method,\nthe ZhiMingTang exhibits a more comprehensive knowledge base than the general-purpose Deepseek-R1. Critically,\nit effectively integrates clinical case experience to provide more precise and professionally accurate responses.\n6\nConclusion\nThis work addresses critical gaps in LLM appli-\ncations for TCM. We introduce TCM-Eval, the\nfirst dynamic, expert-validated benchmark sourced\nfrom national licensing examinations, establishing\na rigorous ’gold standard’ for robust LLM eval-\nuation. To foster advanced reasoning, our novel\nSelf-Iterative Chain-of-Thought Enhancement (SI-\nCoTE) framework enables autonomous CoT gen-\neration and validation, driving a virtuous cycle\nof data and model co-evolution. Leveraging this,\nour SOTA LLM, ZMT-M1, achieves an unprece-\ndented 96.32% on TCM-Eval, far surpassing hu-\nman practitioner pass rates and setting a new bench-\nmark for expert-level knowledge and reasoning. By\nopen-sourcing these resources and maintaining a\ndynamic public leaderboard, we aim to catalyze\nfuture research and accelerate TCM’s AI-driven\nmodernization.\nReferences\nAsad Aali, Dave Van Veen, Yamin Ishraq Arefeen, Ja-\nson Hom, Christian Bluethgen, Eduardo Pontes Reis,\nSergios Gatidis, Namuun Clifford, Joseph Daws,\nArash S Tehrani, and 1 others. 2025. A dataset and\nbenchmark for hospital course summarization with\nadapted large language models. Journal of the Ameri-\ncan Medical Informatics Association, 32(3):470–479.\nCanyu Chen, Jian Yu, Shan Chen, Che Liu, Zhong-\nwei Wan, Danielle Bitterman, Fei Wang, and Kai\nShu. 2024. Clinicalbench: Can llms beat traditional\nml models in clinical prediction?\narXiv preprint\narXiv:2411.06469.\nJunying Chen, Zhenyang Cai, Zhiheng Liu, Yunjin\nYang, Rongsheng Wang, Qingying Xiao, Xiangyi\nFeng, Zhan Su, Jing Guo, Xiang Wan, Guangjun Yu,\nHaizhou Li, and Benyou Wang. 2025. Shizhengpt:\nTowards multimodal llms for traditional chinese\nmedicine. Preprint, arXiv:2508.14706.\nYirong Chen, Zhenyu Wang, Xiaofen Xing, huimin\nzheng, Zhipei Xu, Kai Fang, Junhong Wang, Si-\nhang Li, Jieling Wu, Qi Liu, and Xiangmin Xu.\n2023.\nBianque: Balancing the questioning and\nsuggestion ability of health llms with multi-turn\nhealth conversations polished by chatgpt. Preprint,\narXiv:2310.15896.\nZihao Cheng, Hongru Wang, Zeming Liu, Yuhang Guo,\nYuanfang Guo, Yunhong Wang, and Haifeng Wang.\n2025. ToolSpectrum: Towards personalized tool uti-\nlization for large language models. In Findings of\nthe Association for Computational Linguistics: ACL\n10\n"}, {"page": 11, "text": "2025, pages 20679–20699, Vienna, Austria. Associa-\ntion for Computational Linguistics.\nYizheng Dai, Xin Shao, Jinlu Zhang, Yulong Chen, Qian\nChen, Jie Liao, Fei Chi, Junhua Zhang, and Xiaohui\nFan. 2024. Tcmchat: A generative large language\nmodel for traditional chinese medicine. Pharmaco-\nlogical Research, 210:107530.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,\nXingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-\nhong Shao, Zhuoshu Li, Ziyi Gao, and 181 others.\n2025. Deepseek-r1: Incentivizing reasoning capa-\nbility in llms via reinforcement learning. Preprint,\narXiv:2501.12948.\nNing Ding, Yujia Qin, Guang Yang, Fu Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Haitao Zheng, Jianfei\nChen, Y. Liu, Jie Tang, Juanzi Li, and Maosong Sun.\n2023. Parameter-efficient fine-tuning of large-scale\npre-trained language models. Nature Machine Intel-\nligence, 5:220–235.\nChengfeng Dou, Chong Liu, Fan Yang, Fei Li, Jiyuan\nJia, Mingyang Chen, Qiang Ju, Shuai Wang, Shunya\nDang, Tianpeng Li, and 1 others. 2025. Baichuan-\nm2: Scaling medical capability with large verifier\nsystem. arXiv preprint arXiv:2509.02208.\nHuan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu,\nXinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao\nQiu, Xuan Qi, Yiran Wu, and 1 others. 2025. A\nsurvey of self-evolving agents: On path to artificial\nsuper intelligence. arXiv preprint arXiv:2507.21046.\nLeon Garza, Anantaa Kotal, Michael A. Grasso, and\nEmre Umucu. 2025. Retrieval-augmented framework\nfor llm-based clinical decision support. Preprint,\narXiv:2510.01363.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, and 1 others. 2024. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao\nSong, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning.\narXiv preprint\narXiv:2501.12948.\nTIAN Haoyu, YANG Kuo, DONG Xin, ZHAO\nChenxi, YE Mingwei, WANG Hongyan, LIU Yiming,\nHU Minjie, ZHU Qiang, YU Jian, and 1 others. 2024.\nTcmllm-pr: evaluation of large language models for\nprescription recommendation in traditional chinese\nmedicine. Digital Chinese Medicine, 7(4):343–355.\nJinglin He, Yunqi Guo, Lai Kwan Lam, Waikei Le-\nung, Lixing He, Yuanan Jiang, Chi Chiu Wang, Guo-\nliang Xing, and Hongkai Chen. 2025.\nOpentcm:\nA graphrag-empowered llm-based system for tradi-\ntional chinese medicine knowledge retrieval and di-\nagnosis. Preprint, arXiv:2504.20118.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, and 1 others. 2022. Lora: Low-rank\nadaptation of large language models. ICLR, 1(2):3.\nRui Hua, Xin Dong, Yu Wei, Zixin Shu, Pengcheng\nYang, Yunhui Hu, Shuiping Zhou, He Sun, Kaijing\nYan, Xijun Yan, and 1 others. 2024. Lingdan: enhanc-\ning encoding of traditional chinese medicine knowl-\nedge for clinical reasoning tasks with large language\nmodels. Journal of the American Medical Informat-\nics Association, page ocae087.\nTianai Huang, Lu Lu, Jiayuan Chen, Lihao Liu, Junjun\nHe, Yuping Zhao, Wenchao Tang, and Jie Xu. 2025.\nTcm-3ceval: A triaxial benchmark for assessing re-\nsponses from large language models in traditional\nchinese medicine. arXiv preprint.\nYongzhe Jia, Xiangyu Ji, Xin Wang, Heyi Zhang,\nZhaopeng Meng, Junhua Zhang, Zhe Chen, Pengwei\nZhuang, Dawei Xu, Wenbin Guo, Yuting Yan, and\nJianguo Wei. 2025. Qibo: A large language model\nfor traditional chinese medicine. Expert Systems with\nApplications, 284:127672.\nYanlan Kang, Yang Chang, Sunsi Wu, Xuening Wu,\nYuqi Jiao, Jiyuan Fu, Qingshan Ma, Yide Fang,\nYue Chen, Xue Zhao, Xukun Zhang, Jingyi Zhu,\nXiyu Liu, Yan Wang, Haofen Wang, William Cheng-\nChung Chu, and Wenqiang Zhang. 2025. Zhongjing-\ngpt: An expert knowledge-guided language model\nfor traditional chinese medicine. Tsinghua Science\nand Technology.\nShufeng Kong, Xingru Yang, Yuanyuan Wei, Zijie\nWang, Hao Tang, Jiuqi Qin, Shuting Lan, Yingheng\nWang, Junwen Bai, Zhuangbin Chen, Zibin Zheng,\nCaihua Liu, and Hao Liang. 2025. Mtcmb: A multi-\ntask benchmark framework for evaluating llms on\nknowledge, reasoning, and safety in traditional chi-\nnese medicine. arXiv preprint.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Ef-\nficient memory management for large language\nmodel serving with pagedattention.\nPreprint,\narXiv:2309.06180.\nDubai Li, Nan Jiang, Kangping Huang, Ruiqi Tu, Shuyu\nOuyang, Huayu Yu, Lin Qiao, Chen Yu, Tianshu\nZhou, Danyang Tong, Qian Wang, Mengtao Li, Xi-\naofeng Zeng, Yu Tian, Xinping Tian, and Jingsong Li.\n2025. From questions to clinical recommendations:\nLarge language models driving evidence-based clini-\ncal decision making. Preprint, arXiv:2505.10282.\n11\n"}, {"page": 12, "text": "Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,\nBochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, and 1 others.\n2024a. Deepseek-v3 technical report. arXiv preprint\narXiv:2412.19437.\nJingjing Liu, Zeming Liu, Zihao Cheng, Mengliang\nHe, Xiaoming Shi, Yuhang Guo, Xiangrong Zhu,\nYuanfang Guo, Yunhong Wang, and Haifeng Wang.\n2025. Repodebug: Repository-level multi-task and\nmulti-language debugging evaluation of large lan-\nguage models. Preprint, arXiv:2509.04078.\nMianxin Liu, Jinru Ding, Jie Xu, Weiguo Hu, Xi-\naoyang Li, Lifeng Zhu, Zhian Bai, Xiaoming Shi,\nBenyou Wang, Haitao Song, Pengfei Liu, Xiaofan\nZhang, Shanshan Wang, Kang Li, Haofen Wang,\nTong Ruan, Xuanjing Huang, Xin Sun, and Shaot-\ning Zhang. 2024b. Medbench: A comprehensive,\nstandardized, and reliable benchmarking system for\nevaluating chinese medical large language models.\nPreprint, arXiv:2407.10990.\nTianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman,\nMohammad Saleh, Peter J Liu, and Jialu Liu. 2023.\nStatistical rejection sampling improves preference\noptimization. arXiv preprint arXiv:2309.06657.\nZeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu,\nWanxiang Che, and Ting Liu. 2020. Towards conver-\nsational recommendation over multi-type dialogs. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1036–\n1049, Online. Association for Computational Linguis-\ntics.\nMiniMax, :, Aili Chen, Aonian Li, Bangwei Gong,\nBinyang Jiang, Bo Fei, Bo Yang, Boji Shan,\nChangqing Yu, Chao Wang, Cheng Zhu, Chengjun\nXiao, Chengyu Du, Chi Zhang, Chu Qiao, Chun-\nhao Zhang, Chunhui Du, Congchao Guo, and 109\nothers. 2025. Minimax-m1: Scaling test-time com-\npute efficiently with lightning attention. Preprint,\narXiv:2506.13585.\nSubhabrata Mukherjee, Paul Gamble, Markel Sanz\nAusin, Neel Kant, Kriti Aggarwal, Neha Manju-\nnath, Debajyoti Datta, Zhengliang Liu, Jiayuan\nDing, Sophia Busacca, and 1 others. 2024. Polaris:\nA safety-focused llm constellation architecture for\nhealthcare. arXiv preprint arXiv:2403.13313.\nOpenAI. 2025. gpt-oss-120b & gpt-oss-20b model card.\nPreprint, arXiv:2508.10925.\nPengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weix-\niong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang,\nand Weidi Xie. 2024.\nTowards building multi-\nlingual language model for medicine.\nPreprint,\narXiv:2402.13963.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri,\nAtilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, Cían Hughes, Charles\nLau, Justin Chen, Fereshteh Mahvar, Liron Yatziv,\nTiffany Chen, Bram Sterling, Stefanie Anna Baby, Su-\nsanna Maria Baby, Jeremy Lai, Samuel Schmidgall,\nand 62 others. 2025. Medgemma technical report.\nPreprint, arXiv:2507.05201.\nWei Sibo, Peng Xueping, Wang Yi-fei, Si Jiasheng,\nZhang Weiyu, Lu Wenpeng, Wu Xiaoming, and\nWang Yinglong. 2024. Biancang: A traditional chi-\nnese medicine large language model. arXiv preprint\narXiv:2411.11027.\n5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu\nHou, Bin Chen, Chengxing Xie, Cunxiang Wang,\nDa Yin, Hao Zeng, Jiajie Zhang, Kedong Wang,\nLucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao,\nXiaohan Zhang, Xuancheng Huang, Yao Wei, and\n152 others. 2025a.\nGlm-4.5:\nAgentic, reason-\ning, and coding (arc) foundation models. Preprint,\narXiv:2508.06471.\nKimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jia-\nhao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen,\nYuankun Chen, Yutian Chen, Zhuofu Chen, Jialei\nCui, Hao Ding, Mengnan Dong, Angang Du, Chen-\nzhuang Du, Dikang Du, Yulun Du, Yu Fan, and 150\nothers. 2025b. Kimi k2: Open agentic intelligence.\nPreprint, arXiv:2507.20534.\nMeituan LongCat Team. 2025. Longcat-flash technical\nreport. Preprint, arXiv:2509.01322.\nArun James Thirunavukarasu, Darren Shu Jeng Ting,\nKabilan Elangovan, Laura Gutierrez, Ting Fang Tan,\nand Daniel Shu Wei Ting. 2023. Large language\nmodels in medicine. Nature medicine, 29(8):1930–\n1940.\nDave Van Veen, Cara Van Uden, Louis Blanke-\nmeier, Jean-Benoit Delbrouck, Asad Aali, Christian\nBluethgen, Anuj Pareek, Malgorzata Polacin, Ed-\nuardo Pontes Reis, Anna Seehofnerová, and 1 others.\n2024. Adapted large language models can outper-\nform medical experts in clinical text summarization.\nNature medicine, 30(4):1134–1142.\nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang,\nSendong Zhao, Bing Qin, and Ting Liu. 2023. Hu-\natuo: Tuning llama model with chinese medical\nknowledge. Preprint, arXiv:2304.06975.\nKe Wang, Jiahui Zhu, Minjie Ren, Zeming Liu, Shiwei\nLi, Zongye Zhang, Chenkai Zhang, Xiaoyu Wu, Qiqi\nZhan, Qingjie Liu, and 1 others. 2024. A survey on\ndata synthesis and augmentation for large language\nmodels. arXiv preprint arXiv:2410.12896.\nZhe Wang, Meng Hao, Suyuan Peng, Yuyan Huang, Yi-\nwei Lu, Keyu Yao, Xiaolin Yang, and Yan Zhu. 2025.\nTcmeval-sdt: A benchmark dataset for syndrome dif-\nferentiation thought of traditional chinese medicine.\nScientific Data, 12(1):437.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models. Preprint,\narXiv:2201.11903.\n12\n"}, {"page": 13, "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, and 3 others. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nXian Wu, Yutian Zhao, Yunyan Zhang, Jiageng Wu,\nZhihong Zhu, Yingying Zhang, Yi Ouyang, Ziheng\nZhang, Huimin Wang, Jie Yang, and 1 others. 2024.\nMedjourney: Benchmark and evaluation of large lan-\nguage models over patient clinical journey. Advances\nin Neural Information Processing Systems, 37:87621–\n87646.\nJiacheng Xie, Yang Yu, Ziyang Zhang, Shuai Zeng, Ji-\naxuan He, Ayush Vasireddy, Xiaoting Tang, Congyu\nGuo, Lening Zhao, Congcong Jing, Guanghui An,\nand Dong Xu. 2025. Tcm-ladder: A benchmark for\nmultimodal question answering on traditional chinese\nmedicine. arXiv preprint.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui,\nBo Zheng,\nBowen Yu,\nChang\nGao, Chengen Huang, Chenxu Lv, and 1 others.\n2025a.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388.\nYehan Yang, Tianhao Ma, Ruotai Li, Xinhan Zheng,\nGuodong Shan, and Chisheng Li. 2025b.\nJing-\nfang: An expert-level large language model for tra-\nditional chinese medicine clinical consultation and\nsyndrome differentiation-based treatment. Preprint,\narXiv:2502.04345.\nWenjing Yue, Xiaoling Wang, Wei Zhu, Ming Guan,\nHuanran Zheng, Pengfei Wang, Changzhi Sun,\nand Xin Ma. 2024.\nTcmbench: A comprehen-\nsive benchmark for evaluating large language mod-\nels in traditional chinese medicine.\nPreprint,\narXiv:2406.01126.\nHeyi Zhang, Xin Wang, Zhaopeng Meng, Zhe Chen,\nPengwei Zhuang, Yongzhe Jia, Dawei Xu, and\nWenbin Guo. 2024.\nQibo:\nA large language\nmodel for traditional chinese medicine. Preprint,\narXiv:2403.16056.\nMing Zhang, Yujiong Shen, Zelin Li, Huayu Sha,\nBinze Hu, Yuhui Wang, Chenhao Huang, Shichun\nLiu, Jingqi Tong, Changhao Jiang, and 1 others.\n2025. Llmeval-med: A real-world clinical bench-\nmark for medical llms with physician validation.\narXiv preprint arXiv:2506.04078.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, and 1 others. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223, 1(2).\nChengfeng Zhou, Ji Wang, Juanjuan Qin, Yining Wang,\nLing Sun, and Weiwei Dai. 2025.\nOphthbench:\nA comprehensive benchmark for evaluating large\nlanguage models in chinese ophthalmology. arXiv\npreprint.\nHongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu\nZou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S\nChen, Peilin Zhou, Junling Liu, and 1 others. 2023.\nA survey of large language models in medicine:\nProgress, application, and challenge. arXiv preprint\narXiv:2311.05112.\nXingzhi Zhou, Xin Dong, Chunhao Li, Yuning Bai,\nYulong Xu, Ka Chun Cheung, Simon See, Xinpeng\nSong, Runshun Zhang, Xuezhong Zhou, and Nevin L.\nZhang. 2024. Tcm-ftp: Fine-tuning large language\nmodels for herbal prescription prediction. Preprint,\narXiv:2407.10510.\nWei Zhu, Wenjing Yue, and Xiaoling Wang. 2023.\nShennong-tcm:\nA traditional chinese medicine\nlarge language model.\nhttps://github.com/\nmichael-wzhu/ShenNong-TCM-LLM.\nYi Zhuang, Lingkai Yu, Nan Jiang, and Yujia Ge. 2025.\nTcm-kllama: Intelligent generation model for tradi-\ntional chinese medicine prescriptions based on knowl-\nedge graph and large language model. Computers in\nBiology and Medicine, 189:109887.\n13\n"}]}