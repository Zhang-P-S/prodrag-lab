{"doc_id": "arxiv:2511.19877", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.19877.pdf", "meta": {"doc_id": "arxiv:2511.19877", "source": "arxiv", "arxiv_id": "2511.19877", "title": "It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models", "authors": ["Xiangyu Zhao", "Yaling Shen", "Yiwen Jiang", "Zimu Wang", "Jiahe Liu", "Maxmartwell H Cheng", "Guilherme C Oliveira", "Robert Desimone", "Dominic Dwyer", "Zongyuan Ge"], "published": "2025-11-25T03:38:05Z", "updated": "2025-12-11T03:40:00Z", "summary": "Depression is one of the most prevalent mental health disorders globally. In recent years, multi-modal data, such as speech, video, and transcripts, has been increasingly used to develop AI-assisted depression assessment systems. Large language models have further advanced this field due to their strong language understanding and generalization capabilities. However, conventional LLMs remain text-centric and cannot process the rich non-verbal cues found in audio and visual modalities, which are critical components in mental health evaluation. While multi-modal LLMs offer a promising direction, few are tailored for psychological applications. In this study, we propose a novel multi-modal LLM framework for depression detection. Our approach augments an audio language model with visual understanding and aligns audio-visual features at the timestamp level. This fine-grained alignment improves modeling of temporal dynamics across modalities while reducing the need for extensive training data and computational resources. Experiments on the DAIC-WoZ dataset demonstrate that our model outperforms both single-modality approaches and previous multi-modal methods. Moreover, the proposed framework can be extended to incorporate additional physiological signals, paving the way for broader clinical applications beyond mental health.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.19877v2", "url_pdf": "https://arxiv.org/pdf/2511.19877.pdf", "meta_path": "data/raw/arxiv/meta/2511.19877.json", "sha256": "867d39603439b9c4dff021277281695810c328db713c42ba376ce46e977bf1ef", "status": "ok", "fetched_at": "2026-02-18T02:26:24.052479+00:00"}, "pages": [{"page": 1, "text": "Preprint\nIT HEARS, IT SEES TOO: MULTI-MODAL LLM FOR\nDEPRESSION DETECTION BY INTEGRATING VISUAL\nUNDERSTANDING INTO AUDIO LANGUAGE MODELS\nXiangyu Zhao1 Yaling Shen1 Yiwen Jiang1 Zimu Wang1 Jiahe Liu1 Maxmartwell H Cheng1\nGuilherme C Oliveira1 Robert Desimone2 Dominic Dwyer3 Zongyuan Ge1\n1Monash University, 2Massachusetts Institute of Technology, 3The University of Melbourne\nABSTRACT\nDepression is one of the most prevalent mental health disorders globally. In recent\nyears, multi-modal data, such as speech, video, and transcripts, has been increas-\ningly used to develop AI-assisted depression assessment systems. Large language\nmodels have further advanced this field due to their strong language understanding\nand generalization capabilities. However, conventional LLMs remain text-centric\nand cannot process the rich non-verbal cues found in audio and visual modalities,\nwhich are critical components in mental health evaluation. While multi-modal\nLLMs offer a promising direction, few are tailored for psychological applications.\nIn this study, we propose a novel multi-modal LLM framework for depression de-\ntection. Our approach augments an audio language model with visual understand-\ning and aligns audio-visual features at the timestamp level. This fine-grained align-\nment improves modeling of temporal dynamics across modalities while reducing\nthe need for extensive training data and computational resources. Experiments\non the DAIC-WoZ dataset demonstrate that our model outperforms both single-\nmodality approaches and previous multi-modal methods. Moreover, the proposed\nframework can be extended to incorporate additional physiological signals, paving\nthe way for broader clinical applications beyond mental health.\n1\nINTRODUCTION\nDepression has emerged as a critical concern in the field of mental health, affecting a broad pop-\nulation across various age groups. Particularly, the incidence of depression among adolescents has\nsurged over the past decade, raising significant social and public health concerns (Thapar et al.,\n2022). Diagnosing and treating depression often entails substantial labor and financial costs for\nboth families and healthcare systems. With the advancement of natural language processing (NLP),\nincreasing attention has been given to automated approaches for depression detection, reducing hu-\nman intervention. Large language models (LLMs) have demonstrated remarkable capabilities across\na wide array of NLP tasks (Naveed et al., 2023), which has sparked interest in their application to\nmental health screening (Hengle et al., 2024; Xu et al., 2024). Despite their success, a fundamen-\ntal limitation of conventional LLMs lies in their confinement to textual inputs, lacking the capacity\nto interpret multi-modal signals such as speech and facial expressions that are also indicative of\ndepressive symptoms (Koops et al., 2023; Krause et al., 2021).\nMulti-modal data, including acoustic and visual cues, can significantly enhance the accuracy of de-\npression detection. Prior studies have shown that individuals at high risk of depression often exhibit\nreduced facial expressiveness, diminished vitality, and weakened responses to external stimuli such\nas decreased eye contact (Perez & Riggio, 2003; Waxer, 1974). Similarly, specific acoustic features,\nsuch as monotonous tone, slow speech rate, disfluency, and low vocal energy, have been linked to\ndepressive states (Koops et al., 2023). These behavioral signals offer valuable complementary infor-\nmation beyond what can be derived from text alone. Multi-modal large language models (MLLMs)\noffer an ideal solution to the integration of text and multi-modal data, which shows great promise in a\nlot of downstream tasks (Zhang et al., 2024a). However, current MLLMs face several limitations that\nhinder their application to depression detection. First, depression detection relies heavily on tempo-\nral data such as audio and video, yet most existing MLLMs are limited to static images (Caffagni\n1\narXiv:2511.19877v2  [cs.MM]  11 Dec 2025\n"}, {"page": 2, "text": "Preprint\nVisual\nEncoder\nüî•\nAudio\nEncoder\n‚ùÑ\nDecoder\nüî•\nVisual\nEncoder\nüî•\nProjection\nPooling\nProjection\nPooling\n‚Ä¶\n‚Ä¶\n‚Ä¶ ‚Ä¶\n‚Ä¶\nùê°!\nùê°\"\nProjection\nüî•\n‚ùÑ\nüî•\nLLM\n‚ùÑ\nAudio\nEncoder\n‚ùÑ\nVisual\nEncoder\n‚ùÑ\nDepressed: Yes/No\nTraining Phase 1:\nSelf-Supervised Visual Pretraining\nTraining Phase 2:\nUtterance-Level Audio-Visual Alignment\nTraining Phase 3:\nMulti-Modal Instruction Tuning\nFigure 1: The training scheme of the proposed multi-modal LLM for depression detection.\net al., 2024). Furthermore, due to the relatively small size of depression-related datasets compared to\nstandard NLP corpora, developing MLLMs for this domain demands careful consideration of model\ncomplexity to mitigate overfitting and ensure training efficiency.\nTo address these limitations, we propose a simple yet effective framework that adapts a multi-modal\nlarge language model for depression detection. Our method builds upon a pretrained audio language\nmodel (ALM) and augments it with visual understanding capabilities, forming a truly multi-modal\nsystem. This design leverages the shared temporal structure of audio and visual modalities, allowing\nfor the alignment at the timestamp level. By incrementally integrating visual modules into the ALM\nwith self-supervised visual pretraining and parameter-efficient fine-tuning (PEFT) (Hu et al., 2022),\nour approach maintains the efficiency and modularity of the base model while enhancing its multi-\nmodal capacity. This strategy also reduces the number of trainable parameters and mitigates the\nneed for large-scale pretraining, making it efficient in data usage and computational requirements.\nExperiments on the public depression detection dataset, DAIC-WoZ, confirm the effectiveness of\nour approach, highlighting its potential for practical applications in mental health assessment.\nIn summary, the contributions of this work consist of the following aspects:\n‚Ä¢ We develop a multi-modal large language model for depression detection based on the\nQwen2-Audio (Chu et al., 2024) model by integrating a self-supervised vision encoder\nwith parameter-efficient fine-tuning. To the best of our knowledge, this is the first study\nto propose multi-modal depression detection using LLM across text, audio, and video\nmodalities;\n‚Ä¢ We implement a timestamp-level alignment strategy that enables fine-grained temporal fu-\nsion across modalities. This design leverages the inherent temporal characteristics of both\naudio and video signals, enhancing the model‚Äôs capacity to capture subtle behavioral cues\nindicative of depression.\n‚Ä¢ We validate our approach by the comparison with single-modality methods and previous\nLLM-based state-of-the-art methods on the DAIC-WoZ database (Gratch et al., 2014). The\nexperimental results demonstrate that our approach yields superior performance at a smaller\nmodel scale (7B versus 13B), compared with pioneering multi-modal LLMs.\n2\nRELATED WORKS\n2.1\nAUTOMATED DEPRESSION DETECTION\nDeep learning has been widely adopted for automated depression detection using speech, text, and\nvideo modalities. Earlier works focused on single modality, such as self-supervised speech mod-\nels (Wu et al., 2023), hierarchical acoustic representations (Chen et al., 2022), or mobile speech\ndata (Kim et al., 2023). Visual features like facial expressions and eye movements have also shown\npromise, with methods leveraging weakly supervised learning (Shangguan et al., 2022), gaze pat-\nterns (Zheng et al., 2024), and combined facial-gaze analysis (Stolicyn et al., 2022). Recent studies\nhave explored multi-modal fusion to capture richer cues, incorporating audio, video, and text (Zhang\net al., 2024c; Shen et al., 2022; Xue et al., 2024). However, most rely on late fusion strategies with-\n2\n"}, {"page": 3, "text": "Preprint\nout joint pretraining, limiting their ability to fully exploit temporal and semantic correlations across\nmodalities.\n2.2\nLARGE LANGUAGE MODELS IN DEPRESSION\nLarge language models have been applied to depression detection due to their strong ability to model\nlong-range dependencies in dialogue, which is an essential feature for analyzing clinical interviews.\nFor example, Liu et al. (Liu et al., 2023b) introduced ChatCounselor, which leverages LLMs to as-\nsess depressive symptoms and provide mental health support. Other studies have employed LLMs\nto analyze social media content; Hengle et al. (Hengle et al., 2024) constructed a benchmark for\ndepression-stress classification from online posts, while Xu et al. (Xu et al., 2024) used LLMs to in-\nfer depression status from various web-based sources. Recent efforts have extended LLMs to multi-\nmodal settings for improved diagnostic accuracy. Sadeghi et al. (Sadeghi et al., 2024) combined\nLLMs with facial expression analysis to estimate depression severity, and Zhang et al. (Zhang et al.,\n2024b) incorporated acoustic landmarks into LLMs to build an audio-text model for depression de-\ntection. While these approaches demonstrate the potential of LLMs in mental health applications,\nthey remain limited to textual inputs or approximations thereof (e.g., acoustic landmarks). The in-\nability to directly process rich multi-modal signals restricts their overall effectiveness.\n2.3\nMULTI-MODAL LARGE LANGUAGE MODELS\nIntegrating textual inputs with audio and visual modalities represents a major advancement in the\ndevelopment of generative AI. The fusion of LLMs with visual encoders has enabled impressive\nperformance on tasks such as visual dialogue, visual question answering, and image captioning (Liu\net al., 2023a; Zhu et al., 2023; Dai et al., 2023; Wang et al., 2024; Lu et al., 2024). Similarly, audio\nlanguage models have emerged to jointly process speech and text. For instance, Chu et al. (Chu et al.,\n2024) introduced Qwen2-Audio, extending the Qwen2-7B backbone (Qwen et al., 2025), while Ding\net al. (Ding et al., 2025) proposed Kimi-Audio, which incorporates both discrete acoustic tokens\nand continuous audio embeddings into an LLM framework. Despite their success, these models\nare generally not well-suited for mental health applications due to substantial domain gaps in both\ntraining data and pretraining objectives. Moreover, most vision-language models lack the capacity\nto handle continuous video input, further limiting their applicability to tasks such as depression\ndetection, where temporal visual cues are crucial.\n3\nMETHOD\n3.1\nOVERVIEW OF THE FRAMEWORK\nWe propose a multi-modal large language model (MLLM) for depression detection, constructed\nupon a pretrained audio language model (ALM) as the backbone. As depicted in Figure 2, the frame-\nwork consists of three key components: (1) an audio encoder that processes raw audio signals and\nextracts temporal embeddings; (2) a visual encoder that receives video frames and produces visual\nembeddings aligned with the audio stream at the timestamp level; (3) a large language model that\nintegrates the audio-visual features along with textual inputs to perform depression classification.\nThe training process is divided into three sequential stages. First, the visual encoder is pretrained\nusing a self-supervised learning strategy inspired by masked autoencoders (He et al., 2022), which\nenhances its capacity to capture rich visual representations. In the second stage, the visual encoder\nis fine-tuned on a contrastive alignment task designed to match visual and audio embeddings at the\nutterance level, thereby improving cross-modal temporal synchronization. Finally, the projection\nlayer and LLM are trained using parameter-efficient fine-tuning (PEFT) techniques to effectively\nincorporate the visual modality while minimizing additional computational overhead.\n3.2\nMODEL COMPONENTS\n3.2.1\nAUDIO LANGUAGE MODEL\nWe adopt Qwen2-Audio (Chu et al., 2024) as the foundation of our framework. This model integrates\nWhisper-large-v3 (Radford et al., 2023) as the audio encoder and Qwen2-7B as the language model.\n3\n"}, {"page": 4, "text": "Preprint\nLarge Language Model (LLM)\nProjection\nüî•\n‚ùÑ\nüî•\n‚ùÑ\nAudio Encoder\n‚ùÑ\nVisual Encoder\n‚ùÑ\nDepressed / Non-Depressed\nMulti-Modal Token\nAudio Token\nVisual Token\nText Token\nüî•PEFT Module (LoRA)\nInterviewer: What's the hardest thing about being a parent?\nParticipant: You worry all the time you worry for 'em.\n‚Ä¶‚Ä¶\nInterviewer: When was the last time that happened?\nParticipant: I saw my children about a week ago.\nFigure 2: The framework of the proposed multi-modal large language model. The model includes\nan audio encoder, a visual encoder, and an LLM for detection.\nThe audio encoder processes raw waveforms resampled to 16 kHz and converts them into 128-\nchannel Mel-spectrograms, with each frame representing a 10 ms segment. These spectrograms\nare subsequently downsampled via strided convolutions and average pooling, resulting in encoder\noutputs where each frame corresponds to a 40 ms segment of the original waveform. To ensure the\nuniversality of our method, we retain the pretrained weights of Qwen2-Audio throughout the initial\nstages and apply PEFT-based adaptation only in the final training phase. Notably, our framework is\nmodular and can be extended to other audio language models, provided their audio encoders output\nsequences aligned with fixed temporal intervals.\n3.2.2\nVISUAL ENCODER\nThe visual encoder is designed to extract visual embeddings that align temporally with the audio\nencoder outputs. To ensure architectural compatibility and ease of alignment, its design mirrors\nthe Whisper encoder, comprising a strided convolutional embedding layer, a stack of Transformer\nencoder layers, and an output average pooling layer. Initially, visual features are resampled to match\nthe temporal resolution of the audio Mel-spectrograms and are projected into the embedding space\nvia 1D convolutions. This embedding process includes striding, reducing the temporal resolution to\n20 ms per token. The resulting features are then processed by the Transformer layers and further\ndownsampled through average pooling to match the final 40 ms resolution of the audio encoder\noutputs. As a result, both audio and visual embeddings are temporally synchronized, as illustrated\nin Figure 3.\n3.2.3\nAUDIO-VISUAL PROJECTION\nAfter obtaining audio and visual embeddings, the next step is to fuse them into a unified represen-\ntation for input into the LLM. While a common fusion strategy involves concatenating modality\nembeddings along the sequence dimension (Xu et al., 2025), this approach is suboptimal for inte-\ngrating new modalities into pretrained LLMs, as it disrupts the expected sequence length and can\ninterfere with positional encoding. To preserve compatibility with pretrained LLMs, we propose a\nsimple yet effective fusion method‚Äîelement-wise addition of audio and visual embeddings, which\nis illustrated in Figure 2. This is feasible due to our explicit timestamp-level synchronization, ensur-\ning both sequences share the same temporal structure. Moreover, our three-stage training strategy\nprogressively aligns the modalities, enabling effective fusion without representation collapse.\n3.3\nTIMESTAMP-SYNCHRONIZED DATA AUGMENTATION\nDepression corpora typically consist of participant‚Äìinterviewer interviews, which present two chal-\nlenges: (1) severe class imbalance, as healthy controls far outnumber depressed individuals, and\n(2) limited data volume, despite long session durations. To alleviate these issues, we adopt subdia-\nlogue shuffling based on Wu et al. (2023), segmenting lengthy interviews into shorter, contiguous\n4\n"}, {"page": 5, "text": "Preprint\nexchanges. This increases sample size per participant and enables flexible resampling for class bal-\nancing.\nBuilding on Wu et al. (2023), we enhance the method by ensuring timestamp alignment across\ntranscript, audio, and visual modalities. Each subdialogue is constrained to start with an interviewer‚Äôs\n2 x Conv1D + GeLU\nSelf-A3n\nFFN\nSelf-A3n\nFFN\n‚Ä¶‚Ä¶\nPooling\n2 x Conv1D + GeLU\nSelf-A3n\nFFN\nSelf-A3n\nFFN\n‚Ä¶‚Ä¶\nPooling\nSinusoidal \nPE\nMel-Spectrogram Frames\nVisual Feature Sequence\nAudio Embeddings\nVisual Embeddings\nSinusoidal \nPE\nProjecIon\nProjection\nTemporal Pooling\nTemporal Pooling\nùê°!\nùê°\"\nu3erance level \naudio-visual alignment\nFigure 3: The scheme of utterance-level audio-\nvisual alignment. Audio and visual inputs are\nstrided simultaneously, ensuring synchronization\non timestamps.\nutterance and end with the participant‚Äôs re-\nsponse, maintaining contextual coherence and\nnarrowing the domain gap between LLM pre-\ntraining and depression detection. We then dis-\ncard interviewer audio and corresponding vi-\nsual frames, retaining only participant seg-\nments, while preserving interviewer transcripts.\nThis choice reflects two considerations: in-\nterviewer speech carries little acoustic value\nfor mental state assessment, yet their utter-\nances are essential for conversational coher-\nence. Although removing interviewer segments\ninevitably discards some multimodal informa-\ntion, the trade-off between information reduc-\ntion and coherence is analyzed in Section 4.3.3.\nFurther augmentation details are provided in\nthe Appendix.\n3.4\nTRAINING\nThe training pipeline of our framework is di-\nvided into three sequential stages, as shown in\nFigure 1. The first two stages focus on train-\ning the visual encoder, while the final stage in-\nvolves fine-tuning the LLM.\n3.4.1\nSELF-SUPERVISED VISUAL\nPRETRAINING\nTo enhance the visual representation capability of the encoder, we first conduct self-supervised pre-\ntraining. Instead of learning directly from raw video data, we opt to pretrain on pre-extracted visual\nfeatures, as raw video files may contain sensitive content and are often unavailable in commonly\nused depression-related corpora. This not only addresses potential privacy concerns but also reduces\ncomputational overhead, making the approach more generalizable to other time-series modalities\nsuch as physiological signals (e.g., rPPG and ECG).\nInspired by the masked autoencoder (MAE) framework (He et al., 2022), we design a reconstruction\ntask where the encoder learns to recover masked portions of the input time series. Specifically, given\na sequence input x = (x1, x2, ..., xT ) ‚ààRT √ód, we randomly mask K frames of the input and use\na learnable token xmask ‚ààRd shared across all masked frames. The indices for masked tokens\nare denoted as M, and the indices for unmasked tokens are denoted as V. Obviously V ‚à™M =\n{1, 2, . . . , T}. The unmasked sequence xin = {xi|i ‚ààV} ‚ààRK√ód are fed to the visual encoder to\nacquire the latent representation h ‚ààRT ‚àíK√ód. Then, the latent representation h and masked frames\nare concatenated together to acquire the input sequence z ‚ààRT √ód, which are fed to the decoder to\nobtain the reconstructed input sequence ÀÜx ‚ààRT √ód. The objective is to minimize the mean squared\nerror (MSE) between the reconstructed and original sequences within the masked regions:\nmin\n1\n|M|\nX\ni‚ààM\n||ÀÜxi ‚àíxi||2\n2\n(1)\nThis approach allows the model to capture temporal dependencies and improve robustness in down-\nstream tasks.\n5\n"}, {"page": 6, "text": "Preprint\n3.4.2\nUTTERANCE LEVEL AUDIO-VISUAL ALIGNMENT\nAfter the visual pretraining in the first stage, the visual encoder is enabled to extract visual embed-\ndings from input visual feature sequences for downstream tasks. However, the visual comprehension\nof the visual encoder is not aligned with the audio encoder. To reduce the training gap between both\nencoders, we design a proxy downstream task with contrastive learning to align the visual encoder\nwith the audio encoder at the utterance level. As illustrated in Figure 3, we add a projection layer to\neach encoder, respectively, and pool the outputs in the time dimension to obtain the utterance level\nrepresentations. Given a mini-batch of audio outputs ha ‚ààRN√ód and visual outputs hv ‚ààRN√ód,\nwhere N denotes the batch size, we obtain a similarity matrix Sim = hahT\nv ‚ààRN√óN. The learn-\ning objective is to find the correct match of each audio-visual pair for utterance level audio-visual\nalignment:\nmin Lce(Sim/tau, IN)\n(2)\nwhere Lce denotes cross-entropy loss, IN denotes the identity matrix, and œÑ is the temperature\nparameter.\nDuring this stage, we freeze the entire audio encoder and the lower layers of the visual encoder to\npreserve the representations learned in the initial stage. Only the upper layers of the visual encoder\nreceive gradient updates, ensuring stability and preventing catastrophic forgetting.\n3.4.3\nMULTI-MODAL INSTRUCTION TUNING\nIn the final stage, we integrate the pretrained visual encoder with the audio language model to con-\nstruct a multi-modal large language model tailored for depression detection, which is illustrated in\nFigure 2. Since traditional LLMs are not inherently designed to process visual information, addi-\ntional instruction tuning is required to adapt the model to this task. We employ Low-Rank Adapta-\ntion (LoRA) (Hu et al., 2022) to update the parameters of both the LLM and the modality projection\nlayer. As audio and visual features have been temporally synchronized and aligned at the utterance\nlevel in previous stages, the complexity of cross-modal fusion is substantially reduced.\n3.5\nMULTI-SCALE SLIDING-WINDOW INFERENCE\nSince our model is trained on subdialogues rather than entire conversations, we adopt a multi-scale\nsliding-window inference strategy to derive a final prediction for each full conversation. This ap-\nproach aggregates predictions from multiple subdialogue segments extracted at different temporal\nscales. Specifically, for each conversation, we generate a fixed number (200) of subdialogues at three\npredefined durations: 30s, 75s, and 120s. This multi-scale design ensures that each temporal reso-\nlution contributes equally to the final decision, capturing both short-term and long-term behavioral\ncues. The overlap between adjacent subdialogues is dynamically adjusted based on the conversa-\ntion length and the total number of segments per setting. Each time-scale configuration yields an\nindependent conversation-level prediction, and the final prediction is determined by majority voting\nacross the three settings.\n4\nEXPERIMENTS\n4.1\nDATABASE AND IMPLEMENTATION DETAILS\nWe utilize the DAIC-WoZ database (Gratch et al., 2014), one of the most popular datasets for depres-\nsion detection, to develop and evaluate our proposed multi-modal LLM in depression detection. The\nDAIC-WoZ database contains interview transcripts, speech records, and visual features from 189\nparticipants, including healthy controls and depression cases. The golden labels of the dataset are\nbased on PHQ-8 scores, where a PHQ-8 score higher than 10 is recognized as a depressed case. The\ntraining set contains 107 participants, 30 of whom are labeled as depressed, while the development\nset contains 35 participants, 12 of whom are labeled as depressed. Following our previous works (Wu\net al., 2023; Zhang et al., 2024b), we report the evaluation results on the development set for compar-\nison. In addition to the training set and development set, we also evaluated our method on the test set,\nwhere 14 out of the 47 subjects are labeled as depressed. For timestamp-synchronized data augmen-\ntation, we set the maximum length of each subdialogue to 120 seconds, generate 1,000 subdialogues\nper conversation with depression, which achieves a trade-off between data diversity and the risk of\n6\n"}, {"page": 7, "text": "Preprint\noverfitting. The visual features generated by data augmentation are utilized for self-supervised vi-\nsual pretraining and utterance level audio-visual alignment. Then the augmented transcripts, audio\nclips, and visual features are used for multi-modal instruction finetuning. Our multi-modal LLM for\ndepression detection is developed on Qwen2-Audio-7B-Instruct model. We utilize 2 NVIDIA H200\n141G GPUs during training. The detailed training hyperparameters have been demonstrated in the\nAppendix.\n4.2\nRESULTS\nWe compare our methods with previous methods, including single-modal approaches, conventional\nmulti-modal approaches, and multi-modal LLMs, on both the development set and test set of the\nDAIC-WoZ database. The detailed comparison results are illustrated in Table 1, Table 2, and Table\n3, respectively. Following previous works, we adopt the F1 score for evaluation.\nModality\nModels\nF1\nText\nRoBERTa 2022\n0.602\nLlama2-7B 2024b\n0.578\nLlama2-13B 2024b\n0.636\nQwen2-7B 2024\n0.564\nGPT4 2024b\n0.571\nAudio\nHuBERT 2023\n0.640\nWavLM 2023\n0.720\nSpeechFormer 2022\n0.694\nSpeechFormer++ 2023\n0.709\nWhisper-v3 2023\n0.694\nVideo\nGSM 2016\n0.530\nSSL + CLS\n0.668\nA+T\nAudiBERT 2021\n0.709\nTOAT 2022\n0.741\nLSTM 2018\n0.770\nA+T+V\nC-CNN 2018\n0.769\nConvBiLSTM 2022\n0.70*\nOurs w/o MS\n0.789\nOurs\n0.844\nTable 1: The performance compar-\nison of our method and other ap-\nproaches on DAIC-WoZ develop-\nment set. ‚Äú*‚Äù denotes that the origi-\nnal results are reported with 2 sig-\nnificant digits. ‚ÄúMS‚Äù denoting the\nmulti-scale strategy in our infer-\nence.\nEvaluation on DAIC-WoZ Dev Set\nWe present a compre-\nhensive comparison between our proposed multi-modal LLM\nand previous methods on the DAIC-WoZ development set in\nTable 1. Additionally, we evaluate the contribution of each in-\ndividual module in our framework, including the Qwen2-7B\nmodel, the Whisper-v3 audio encoder, and a self-supervised\nvision encoder. Overall, our multi-modal model achieves su-\nperior classification performance on the development set of\nthe DAIC-WoZ dataset, consistently outperforming all single-\nmodality baselines.\nText-based models show that Llama2-13B (Touvron et al.,\n2023; Zhang et al., 2024b) performs best among text-only\nmodels, likely due to its larger parameter scale. Among\nsmaller models, Qwen2-7B and Llama2-7B exhibit similar\nperformance but fall short of the 13B variant. Interestingly,\nGPT-4, despite its scale and zero-shot capabilities, underper-\nforms relative to Llama2-13B. Likewise, RoBERTa surpasses\nGPT-4 despite its significantly smaller size as well. A simi-\nlar phenomenon has been observed in Zhang et al. (2024b).\nThis performance gap may be attributed to the nature of de-\npression detection, which emphasizes representation learn-\ning over generative modeling, making encoder-based models\nmore suitable.\nAudio-based models generally outperform text-only models,\nsuggesting that acoustic cues carry richer information for de-\ntecting depressive symptoms. In addition, the performance of\naudio models could benefit from downstream tasks such as speech recognition or emotion recog-\nnition (Wu et al., 2023). Notably, WavLM fine-tuned for emotion recognition shows superior per-\nformance, surpassing even Whisper-v3-large. This suggests that tasks closely related to depression,\nsuch as emotion recognition and ASR, provide transferable knowledge useful for this application.\nFor video models, our finetuned visual encoder with a classification head achieves the best perfor-\nmance. The main factor that could affect video-based models is the choice of visual feature sets.\nSince raw videos are not available at the DAIC-WoZ database, only facial feature sets, such as land-\nmarks and action units, are available for depression detection. As the feature set could be rather\nredundant, the performance of video models could even deteriorate if the feature set selection is\ninappropriate. Self-supervised pretraining alleviates the issue significantly, as masked autoencoders\nare designed for images, which possess a redundant nature, and are suitable in our scenario.\nMulti-modal approaches that incorporate both audio and text, or integrate all three modalities, gen-\nerally outperform single-modal baselines. In particular, the inclusion of audio features often leads\nto significant performance improvements, highlighting the importance of acoustic information in\ndepression detection. Compared with other multi-modal methods, our proposed framework consis-\n7\n"}, {"page": 8, "text": "Preprint\ntently achieves superior results, demonstrating the effectiveness of timestamp-level alignment and\nthe synergy of modality-specific encoders in capturing clinically relevant cues.\nComparison with Multi-Modal LLMs\nTable 2 presents the performance comparison between\nour method and existing multi-modal LLMs. Together with Table 1, the results demonstrate that\nincorporating audio significantly enhances the classification performance of LLMs. For instance,\naugmenting Llama2-13B with acoustic landmarks improves its F1 score from 0.636 to 0.695. A\nsimilar trend is observed with Qwen2-7B, where the inclusion of audio elevates the F1 score from\n0.578 to 0.720. Our proposed multi-modal framework, which jointly models text, audio, and vi-\nsual signals, achieves the highest F1 score of 0.789, validating the benefit of integrating visual\ncues alongside audio and language inputs. This underscores the advantage of leveraging comple-\nmentary modalities for capturing the complex and multi-faceted nature of depressive symptoms.\nModel\nBase Model\nF1\nAcoustic LLM\n(Zhang et al., 2024b)\n7B\n0.545\n7B-Chat\n0.500\n13B\n0.695\n13B-Chat\n0.666\nQwen2-Audio\n(Chu et al., 2024)\n7B\n0.650\n7B-Instruct\n0.720\nOurs w/o audio\n7B\n0.617\n7B-Instruct\n0.643\nOurs\n7B\n0.709\n7B-Instruct\n0.789\nTable 2: The performance compari-\nson of our method and multi-modal\nLLMs on DAIC-WoZ development\nset. Note that for fair comparison we\ndo not employ model ensemble or\nmulti-scale inference.\nNotably, both our approach and Qwen2-Audio variants out-\nperform LLMs with acoustic landmarks, despite relying on\nsmaller language backbones (7B vs 13B). This suggests that\nnative multi-modal architectures might be more adept at\ninterpreting raw sensory inputs. While acoustic landmarks\nserve as a lightweight representation of audio, they may omit\nsubtle prosodic or emotional cues that are preserved in the\noriginal waveforms. In contrast, models trained end-to-end\non raw audio exhibit stronger modality comprehension and\nmore effective feature fusion.\nEvaluation on DAIC-WoZ Test Set\nIn addition, since the\ngolden labels of the DAIC-WoZ test set have been released,\nwe compare our method with previous state-of-the-art ap-\nproaches on this benchmark. The quantitative results are pre-\nsented in Table 3. It can be observed that single-modal ap-\nproaches yield similar or slightly lower F1 scores on the test\nset compared to their performance on the development set.\nIn contrast, a recent multi-modal approach that integrates au-\ndio, video, and textual information (Jung et al., 2024) achieves significantly better results than single-\nmodal methods. Overall, our method outperforms both previous single-modal and multi-modal ap-\nproaches on the test set, demonstrating its effectiveness and robustness.\nDataset\nModels\nModality\nF1\nDAIC-WoZ\nGloVe-CNN (Campbell et al., 2022)\nText\n0.68*\nTOAT (Guo et al., 2022)\nAudio\n0.647\nEmoAudioNet (Othmani et al., 2021)\nAudio\n0.66*\nHiQuE (Jung et al., 2024)\nA+T+V\n0.79*\nMultiDepNet ()\nA+T\n0.785\nOurs\nA+T+V\n0.825\nTable 3: The performance comparison of our method and previous approaches on DAIC-WoZ test\nset. ‚Äú*‚Äù denotes that the original results are reported with 2 significant digits.\n4.3\nABLATION STUDIES AND DISCUSSION\nIn this section, we analyze the source of performance gain in our framework, including the contribu-\ntion of each modality and the selection of the base model. In addition, we discuss the effectiveness\nof our proposed timestamp-synchronized data augmentation upon the removal of the interviewer‚Äôs\nutterance and context length in subdialogues. The experiments are all conducted on the development\nset of DAIC-WoZ.\n4.3.1\nTHE CONTRIBUTION OF EACH MODALITY\nWe further investigate the individual contribution of each modality within our framework. As shown\nin Table 1, both audio and video modalities enhance depression detection performance. The baseline\n8\n"}, {"page": 9, "text": "Preprint\nQwen2-7B model achieves an F1 score of 0.564 using text alone. Introducing audio features leads\nto a substantial improvement, raising the F1 score to 0.720. Further incorporation of video features\nelevates the performance to 0.789. Additionally, our proposed multi-scale sliding-window strategy\ncontributes to model performance significantly, improving the F1 score to 0.844.\nAn interesting observation is that the addition of audio yields a greater performance gain compared\nto the inclusion of video, in both instruction-tuned and pre-trained variants. This discrepancy can be\nattributed to two primary factors. First, as pre-extracted visual features rather than raw video data are\nutilized in our framework, the model may face information loss, leading to reduced expressive power.\nSecond, our model is fundamentally built upon an audio language modeling architecture. Removing\naudio embeddings may disrupt the alignment mechanism across modalities, thereby compromising\nthe model‚Äôs ability to integrate non-verbal cues effectively.\n4.3.2\nTHE CHOICE OF BASE MODEL\nSince both pretrained model and instruction-tuned model are available in Qwen2-Audio families,\nwe compare the performance of these two model variants as the base model. The results in Table\n2 indicate that the instruction-tuned model provides higher detection performance. The findings in\nour research are different from previous work (Zhang et al., 2024b), where instruction tuning leads\nto significant performance deterioration compared with the pretrained model. The reasons for the\ninconsistency could be the difference in instruction tuning in general LLMs and audio language\nmodels. Depression detection involves the analysis of both audio and text; a similar task has been\nused to finetune the model in instruction tuning. Thus, the instruction-tuned model could be better\nat the audio analysis task.\n4.3.3\nTHE EFFECT OF CONTEXT LENGTH AND INTERVIEWER UTTERANCE REMOVAL\nFigure 4: The depression detection performance\non subdialogue length with or without inter-\nviewer‚Äôs utterances.\nThe length of subdialogues plays a crucial role\nin our framework, as longer contexts gener-\nally provide richer cues for depression detec-\ntion. However, longer subdialogues do not nec-\nessarily improve the performance for detec-\ntion, as the audio records for the interviewer\ndo not contribute to the decision, but even in-\nterfere with the depression detection. To ad-\ndress this constraint, we propose to remove the\ninterviewer‚Äôs utterances during data augmenta-\ntion, allowing more content from the partici-\npant to be retained within the fixed audio win-\ndow. While this enhances the availability of\nparticipant-specific acoustic cues, it also results\nin the loss of visual information associated with\nthe removed segments. To explore this trade-\noff, we conduct an ablation study under varying subdialogue lengths, as shown in Figure 4. When\nthe maximum subdialogue length is constrained to 30 seconds, removing the interviewer‚Äôs speech\nleads to degraded performance. In this setting, the entire subdialogue can be encoded without trun-\ncation, and discarding the interviewer‚Äôs turns causes unnecessary loss of visual cues, thus impairing\nmulti-modal inference. In contrast, as the subdialogue length increases beyond the model‚Äôs audio\ncapacity, the removal of interviewer utterances proves beneficial. By prioritizing participant speech\nwithin the fixed input window, the model gains access to more relevant acoustic information, leading\nto improved detection accuracy. However, when the context length becomes excessively long, the\nperformance gain diminishes. This is likely due to reduced dialogue diversity and increased risk of\noverfitting, as longer subdialogues tend to be less variable.\n5\nCONCLUSION\nIn this study, we propose a multi-modal large language model for depression detection, built upon\naudio-based language models and augmented with visual understanding capabilities. Experiments\non the DAIC-WoZ dataset demonstrate the superiority of our framework over existing multi-modal\n9\n"}, {"page": 10, "text": "Preprint\nLLMs. To our knowledge, this is the first work to develop a multi-modal LLM for depression detec-\ntion that simultaneously integrates textual, audio, and visual modalities. We further provide detailed\nanalyses of how model design and data augmentation strategies affect performance. Overall, our\nmethod offers an effective solution for adapting multi-modal LLMs to mental health applications,\nwith potential for broader extension to other domains.\nETHICS STATEMENT\nThis study is conducted using the DAIC-WoZ database, a publicly available resource accessible\nto qualified researchers upon request. All data collection procedures for this dataset were carried\nout with informed consent from participants, and the data have been fully anonymized to protect\nindividual privacy. We have obtained proper authorization by signing the DAIC-WoZ End-User\nLicense Agreement and strictly adhere to its terms of use. Our model is built upon the Qwen2-Audio\narchitecture, and all research activities related to it comply with the Apache-2.0 license under which\nthe model is released. While our method achieves state-of-the-art performance on the DAIC-WoZ\nbenchmark, it is intended for research purposes only and should not be used for clinical diagnosis,\ntreatment, or intervention of depression. We further acknowledge that, like many large language\nmodels, our framework may be vulnerable to hallucinations, harmful outputs, or systemic biases.\nWe disclaim responsibility for any misuse, misinterpretation, or unintended consequences resulting\nfrom the deployment of this model outside its intended research context.\nREFERENCES\nTuka Al Hanai, Mohammad M Ghassemi, and James R Glass. Detecting depression with audio/text\nsequence modeling of interviews∆í. In Interspeech, pp. 1716‚Äì1720, 2018.\nDavide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo\nBaraldi, Marcella Cornia, and Rita Cucchiara. The revolution of multimodal large language mod-\nels: a survey. arXiv preprint arXiv:2402.12451, 2024.\nEdward L Campbell, Laura Docƒ±o-Fern¬¥andez, Nicholas Cummins, and Carmen Garcƒ±a-Mateo.\nSpeech and text processing for major depressive disorder detection. Training, 31:76, 2022.\nWeidong Chen, Xiaofen Xing, Xiangmin Xu, Jianxin Pang, and Lan Du. Speechformer: A hierar-\nchical efficient framework incorporating the characteristics of speech. In Proc. Interspeech 2022,\npp. 346‚Äì350, 2022.\nWeidong Chen, Xiaofen Xing, Xiangmin Xu, Jianxin Pang, and Lan Du. Speechformer++: A hi-\nerarchical efficient framework for paralinguistic speech processing. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 31:775‚Äì788, 2023.\nYunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv,\nJinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759,\n2024.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li,\nPascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models\nwith instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems,\n2023. URL https://openreview.net/forum?id=vvoWPYqZJA.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. Advances in neural information processing systems, 36:10088‚Äì10115, 2023.\nDing Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song,\nXu Tan, Heyi Tang, et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025.\nJonathan Gratch, Ron Artstein, Gale M Lucas, Giota Stratou, Stefan Scherer, Angela Nazarian,\nRachel Wood, Jill Boberg, David DeVault, Stacy Marsella, et al. The distress analysis interview\ncorpus of human and computer interviews. In LREC, volume 14, pp. 3123‚Äì3128. Reykjavik,\n2014.\n10\n"}, {"page": 11, "text": "Preprint\nYanrong Guo, Chenyang Zhu, Shijie Hao, and Richang Hong. A topic-attentive transformer-based\nmodel for multimodal depression detection. arXiv preprint arXiv:2206.13256, 2022.\nAlbert Haque, Michelle Guo, Adam S Miner, and Li Fei-Fei. Measuring depression symptom sever-\nity from spoken language and 3d facial expressions. arXiv preprint arXiv:1811.08592, 2018.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll¬¥ar, and Ross Girshick. Masked au-\ntoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 16000‚Äì16009, 2022.\nAmey Hengle, Atharva Kulkarni, Shantanu Patankar, Madhumitha Chandrasekaran, Sneha D‚Äôsilva,\nJemima Jacob, and Rashmi Gupta. Still not quite there! evaluating large language models for\ncomorbid mental health diagnosis. In Proceedings of the 2024 Conference on Empirical Methods\nin Natural Language Processing, pp. 16698‚Äì16721, 2024.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022.\nJuho Jung, Chaewon Kang, Jeewoo Yoon, Seungbae Kim, and Jinyoung Han. Hique: Hierarchical\nquestion embedding network for multimodal depression detection. In Proceedings of the 33rd\nACM International Conference on Information and Knowledge Management, pp. 1049‚Äì1059,\n2024.\nAh Young Kim, Eun Hye Jang, Seung-Hwan Lee, Kwang-Yeon Choi, Jeon Gue Park, and Hyun-\nChool Shin. Automatic depression detection using smartphone-based text-dependent speech sig-\nnals: deep convolutional neural network approach.\nJournal of medical Internet research, 25:\ne34474, 2023.\nSanne Koops, Sanne G Brederoo, Janna N de Boer, Femke G Nadema, Alban E Voppel, and Iris E\nSommer. Speech as a biomarker for depression. CNS & Neurological Disorders-Drug Targets-\nCNS & Neurological Disorders), 22(2):152‚Äì160, 2023.\nFernando C Krause, Eftihia Linardatos, David M Fresco, and Michael T Moore. Facial emotion\nrecognition in major depressive disorder: A meta-analytic review. Journal of affective disorders,\n293:320‚Äì328, 2021.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances\nin neural information processing systems, 36:34892‚Äì34916, 2023a.\nJune M Liu, Donghao Li, He Cao, Tianhe Ren, Zeyi Liao, and Jiamin Wu. Chatcounselor: A large\nlanguage models for mental health support. arXiv preprint arXiv:2309.15461, 2023b.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nHaoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren,\nZhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding.\narXiv preprint arXiv:2403.05525, 2024.\nHumza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman,\nNaveed Akhtar, Nick Barnes, and Ajmal Mian. A comprehensive overview of large language\nmodels. arXiv preprint arXiv:2307.06435, 2023.\nAlice Othmani, Daoud Kadoch, Kamil Bentounes, Emna Rejaibi, Romain Alfred, and Abdenour\nHadid. Towards robust deep neural networks for affect and depression recognition from speech.\nIn International conference on pattern recognition, pp. 5‚Äì19. Springer, 2021.\nJohn E Perez and Ronald E Riggio. Nonverbal social skills and psychopathology. Nonverbal behav-\nior in clinical settings, pp. 17‚Äì44, 2003.\n11\n"}, {"page": 12, "text": "Preprint\nRafa≈Ç Po¬¥swiata and Micha≈Ç Pere≈Çkiewicz.\nOPI@LT-EDI-ACL2022: Detecting signs of depres-\nsion from social media text using RoBERTa pre-trained language models.\nIn Bharathi Raja\nChakravarthi, B Bharathi, John P McCrae, Manel Zarrouk, Kalika Bali, and Paul Buitelaar (eds.),\nProceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclu-\nsion, pp. 276‚Äì282, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:\n10.18653/v1/2022.ltedi-1.40. URL https://aclanthology.org/2022.ltedi-1.40/.\nQwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin\nYang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi\nTang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL\nhttps://arxiv.org/abs/2412.15115.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. In International conference on ma-\nchine learning, pp. 28492‚Äì28518. PMLR, 2023.\nMisha Sadeghi, Robert Richer, Bernhard Egger, Lena Schindler-Gmelch, Lydia Helene Rupp, Far-\nnaz Rahimi, Matthias Berking, and Bjoern M Eskofier. Harnessing multimodal approaches for\ndepression detection using large language models and facial expressions. npj Mental Health Re-\nsearch, 3(1):66, 2024.\nZixuan Shangguan, Zhenyu Liu, Gang Li, Qiongqiong Chen, Zhijie Ding, and Bin Hu. Dual-stream\nmultiple instance learning for depression detection with facial expression videos. IEEE Transac-\ntions on Neural Systems and Rehabilitation Engineering, 31:554‚Äì563, 2022.\nYing Shen, Huiyu Yang, and Lin Lin. Automatic depression detection: An emotional audio-textual\ncorpus and a gru/bilstm-based model. In ICASSP 2022-2022 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp. 6247‚Äì6251. IEEE, 2022.\nAleks Stolicyn, J Douglas Steele, and Peggy Seri`es. Prediction of depression symptoms in individual\nsubjects with face and eye movement tracking. Psychological medicine, 52(9):1784‚Äì1792, 2022.\nAnita Thapar, Olga Eyre, Vikram Patel, and David Brent. Depression in young people. The Lancet,\n400(10352):617‚Äì631, 2022.\nErmal Toto, ML Tlachac, and Elke A Rundensteiner. Audibert: A deep transfer learning multimodal\nclassification framework for depression screening. In Proceedings of the 30th ACM international\nconference on information & knowledge management, pp. 4145‚Äì4154, 2021.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu,\nJialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model‚Äôs perception of the\nworld at any resolution. arXiv preprint arXiv:2409.12191, 2024.\nPeter Waxer. Nonverbal cues for depression. Journal of Abnormal Psychology, 83(3):319, 1974.\nPing-Cheng Wei, Kunyu Peng, Alina Roitberg, Kailun Yang, Jiaming Zhang, and Rainer Stiefelha-\ngen. Multi-modal depression estimation based on sub-attentional fusion. In European Conference\non Computer Vision, pp. 623‚Äì639. Springer, 2022.\nJames R Williamson, Elizabeth Godoy, Miriam Cha, Adrianne Schwarzentruber, Pooya Khorrami,\nYoungjune Gwon, Hsiang-Tsung Kung, Charlie Dagli, and Thomas F Quatieri. Detecting depres-\nsion using vocal, facial and semantic communication cues. In Proceedings of the 6th international\nworkshop on audio/visual emotion challenge, pp. 11‚Äì18, 2016.\nWen Wu, Chao Zhang, and Philip C Woodland. Self-supervised representations in speech-based\ndepression detection. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pp. 1‚Äì5. IEEE, 2023.\n12\n"}, {"page": 13, "text": "Preprint\nJin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang\nFan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025.\nXuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James Hendler, Marzyeh Ghas-\nsemi, Anind K Dey, and Dakuo Wang. Mental-llm: Leveraging large language models for mental\nhealth prediction via online text data. Proceedings of the ACM on Interactive, Mobile, Wearable\nand Ubiquitous Technologies, 8(1):1‚Äì32, 2024.\nJunqi Xue, Ruihan Qin, Xinxu Zhou, Honghai Liu, Min Zhang, and Zhiguo Zhang. Fusing multi-\nlevel features from audio and contextual sentence embedding from text for interview-based de-\npression detection. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pp. 6790‚Äì6794. IEEE, 2024.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengping Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, et al. Qwen2 technical report. arXiv\npreprint arXiv:2407.10671, 2024.\nDuzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-\nllms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601,\n2024a.\nXiangyu Zhang, Hexin Liu, Kaishuai Xu, Qiquan Zhang, Daijiao Liu, Beena Ahmed, and Julien\nEpps. When llms meets acoustic landmarks: An efficient approach to integrate speech into large\nlanguage models for depression detection. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, pp. 146‚Äì158, 2024b.\nZhenwei Zhang, Shengming Zhang, Dong Ni, Zhaoguo Wei, Kongjun Yang, Shan Jin, Gan Huang,\nZhen Liang, Li Zhang, Linling Li, et al. Multimodal sensing for depression risk detection: Inte-\ngrating audio, video, and text data. Sensors, 24(12):3714, 2024c.\nZhiguo Zheng, Lijuan Liang, Xiong Luo, Jie Chen, Meirong Lin, Guanjun Wang, and Chenyang\nXue. Diagnosing and tracking depression based on eye movement in response to virtual reality.\nFrontiers in Psychiatry, 15:1280935, 2024.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\nA\nIMPLEMENTATION DETAILS\nOur framework is implemented using the HuggingFace transformers library with PyTorch 2.1. The\nfull hyperparameter configurations used during training are summarized in Table 4. We adopt the\nAdamW optimizer (Loshchilov & Hutter, 2017) for model optimization. To improve training speed\nwithout compromising performance, we enable TensorFloat32 (TF32) computation and apply auto-\nmatic mixed-precision training using BFloat16 (BF16). For parameter-efficient fine-tuning (PEFT)\nof the Qwen2-Audio model on the depression detection task, we employ QLoRA (Dettmers et al.,\n2023), which compresses the base model to 4-bit precision to reduce memory usage and improve\ncomputational efficiency. The full training process requires approximately 90+ GPU hours on an\nNVIDIA H200 141GB GPU. This includes around 40 hours for self-supervised visual pretraining,\n20 hours for utterance-level audio-visual alignment, and 30 hours for multimodal instruction tuning.\nEarly stopping is applied in all stages when training loss plateaus.\nB\nDETAILS OF TIMESTAMP-SYNCHRONIZED DATA AUGMENTATION\nFollowing the approach of Wu et al. (2023), we generate subdialogues from the original interview\ntranscripts to mitigate class imbalance and expand the size of the training set. In our data augmenta-\ntion pipeline, we enforce strict synchronization among transcripts, audio, and video to ensure precise\ntimestamp-level alignment. However, due to varying frame rates across modalities, achieving syn-\nchronization presents a technical challenge. For example, audio recordings are typically captured at\n13\n"}, {"page": 14, "text": "Preprint\nStage I\nStage II\nStage III\nOptimizer\nAdamW\nLearning Rate\n1.5e-4\n1e-6\n3e-6\nŒ≤1\n0.9\nŒ≤2\n0.95\n0.999\nWeight Decay\n0\n0.001\nBatch Size\n128\n64\n8\nGrad Accum Steps\n8\n16\n8\nScheduler\nCosine LR\nNum Epochs\n50\n20\n3\nWarm Up Epochs\n5\n2\n0.1\nMax Grad Norm\n1.0\n0.5\nBF16\nTrue\nTF32\nTrue\nTable 4: Training hyperparameters.\nAlgorithm 1 Time-Sync Data Augmentation\n1: N + ‚ÜêNumber of positive samples in the training set\n2: N ‚àí‚ÜêNumber of negative samples in the training set\n3: Set number of subdialogues per positive sample M +\n4: Set minimum length of subdialogue in seconds dmin\n5: Set maximum length of subdialogue in seconds dmax\n6: M ‚àí= N ‚àí/N + √ó M + ‚ÜêNumber of sub-dialogues per negative sample\n7: for Dialogue X(n) = (T n, An, V n), n = 1, 2, ..., N do\n8:\nD ‚ÜêDialogue length in seconds\n9:\n{Œµi} ‚ÜêInterviewer utterance start timestamps\n10:\n{Œµp} ‚ÜêParticipant utterance end timestamps\n11:\nif X(n) is positive then\n12:\nM ‚ÜêM +\n13:\nelse\n14:\nM ‚ÜêM ‚àí\n15:\nend if\n16:\nfor Sub-dialogue X(n)m, m = 1 to M do\n17:\nSample length d uniformly from (dmin, dmax)\n18:\nSample start timestamp Œµ‚Ä≤\ns ‚àà{Œµi} from range (0, D ‚àíd)\n19:\nRound the start timestamp to its closet integer second Œµs ‚Üê‚åäŒµ‚Ä≤\ns‚åã\n20:\nŒµtmp = Œµs + d ‚ÜêRaw end timestamp\n21:\nSample end timestamp Œµ‚Ä≤\ne ‚àà{Œµp} and min |Œµ‚Ä≤\ne ‚àíŒµtmp|\n22:\nRound the end timestamp to its closet integer second Œµe ‚Üê‚åàŒµ‚Ä≤\ne‚åâ\n23:\nGenerate subdialogue T (n)m ‚ÜêT (n)\nŒµs:Œµe\n24:\nObtain the raw audio segment A‚Ä≤(n)m ‚ÜêA(n)\nŒµs:Œµe\n25:\nObtain the raw visual segment V ‚Ä≤(n)m ‚ÜêV (n)\nŒµs:Œµe\n26:\nRemove the interviewer utterances A(n)m ‚ÜêA‚Ä≤(n)m and V (n)m ‚ÜêV ‚Ä≤(n)m\n27:\nSubdialogue X(n)m = (T (n)m, A(n)m, V (n)m)\n28:\nend for\n29: end for\na 16,000 Hz sampling rate and later converted into Mel-spectrograms with a frame rate of 100 Hz,\nwhile video recordings are collected at 30 frames per second (FPS). To address this discrepancy,\nwe constrain the start and end timestamps of each subdialogue to align with whole seconds (i.e.,\ninteger-second boundaries).\nAdditionally, we require each subdialogue to begin with an utterance from the interviewer and con-\nclude with a response from the participant. This design choice ensures that each subdialogue forms\na complete and contextually coherent conversational unit, with a clear initiation and response struc-\nture. Such a constraint preserves the semantic continuity and logical flow within each segment,\nmaking them more suitable for downstream tasks that rely on natural discourse patterns. Moreover,\nthis structure aligns with the training paradigm of large language models, which are typically pre-\n14\n"}, {"page": 15, "text": "Preprint\ntrained on large-scale dialogue corpora. By maintaining this dialogue consistency, we enhance the\nmodel‚Äôs ability to interpret the subdialogues effectively within a familiar conversational framework.\nC\nTHE PROMPT DESIGN FOR INSTRUCTION TUNING\nDuring instruction tuning, we design a system prompt to guide the behavior of the language model.\nGiven that the Qwen2-Audio-Instruct model has been fine-tuned on audio analysis tasks, we adopt\na chat-based prompt template to elicit model responses. Notably, we use the same prompt design\nfor both the Qwen2-Audio family and our multi-modal LLM. This consistency is based on our\nintegration strategy, where visual embeddings are directly added to the audio embeddings without\nmodifying the model architecture. Therefore, we assume that the model can still function effectively\neven without explicitly referencing visual information in the prompt.\nSystem\nPrompt\nBelow is a conversation between an interviewer and a\nparticipant. Please analyze the transcripts and audio, and find\nwhether the participant is affected by depression.\nInstructions\nAudio: {audio} \\n Interview conversation: {transcripts}\n\\n Response: \\n\n15\n"}]}