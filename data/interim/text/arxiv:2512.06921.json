{"doc_id": "arxiv:2512.06921", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.06921.pdf", "meta": {"doc_id": "arxiv:2512.06921", "source": "arxiv", "arxiv_id": "2512.06921", "title": "NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification", "authors": ["Ziyang Song", "Zelin Zang", "Xiaofan Ye", "Boqiang Xu", "Long Bai", "Jinlin Wu", "Hongliang Ren", "Hongbin Liu", "Jiebo Luo", "Zhen Lei"], "published": "2025-12-07T17:00:25Z", "updated": "2025-12-07T17:00:25Z", "summary": "Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.06921v1", "url_pdf": "https://arxiv.org/pdf/2512.06921.pdf", "meta_path": "data/raw/arxiv/meta/2512.06921.json", "sha256": "5e8e55cef25aed8cc1dce1973994d83a4c9dc313ca1c7e4c06280afee4b06fb9", "status": "ok", "fetched_at": "2026-02-18T02:24:59.074122+00:00"}, "pages": [{"page": 1, "text": "NeuroABench: A Multimodal Evaluation\nBenchmark for Neurosurgical Anatomy\nIdentification\nZiyang Song1, †, Zelin Zang1, †, Xiaofan Ye2, Boqiang Xu1, Long Bai3, Jinlin Wu1,*,\nHongliang Ren3, Hongbin Liu1, Jiebo Luo1, Zhen Lei1\nAbstract—Multimodal Large Language Models (MLLMs) have\nshown significant potential in surgical video understanding. With\nimproved zero-shot performance and more effective human-\nmachine interaction, they provide a strong foundation for ad-\nvancing surgical education and assistance. However, existing\nresearch and datasets primarily focus on understanding surgical\nprocedures and workflows, while paying limited attention to the\ncritical role of anatomical comprehension. In clinical practice,\nsurgeons rely heavily on precise anatomical understanding to\ninterpret, review, and learn from surgical videos. To fill this\ngap, we introduce the Neurosurgical Anatomy Benchmark (Neu-\nroABench), the first multimodal benchmark explicitly created to\nevaluate anatomical understanding in the neurosurgical domain.\nNeuroABench consists of 9 hours of annotated neurosurgical\nvideos covering 89 distinct procedures and is developed using\na novel multimodal annotation pipeline with multiple review\ncycles. The benchmark evaluates the identification of 68 clinical\nanatomical structures, providing a rigorous and standardized\nframework for assessing model performance. Experiments on\nover 10 state-of-the-art MLLMs reveal significant limitations,\nwith the best-performing model achieving only 40.87% accu-\nracy in anatomical identification tasks. To further evaluate the\nbenchmark, we extract a subset of the dataset and conduct an\ninformative test with four neurosurgical trainees. The results\nshow that the best-performing student achieves 56% accuracy,\nwith the lowest scores of 28% and an average score of 46.5%.\nWhile the best MLLM performs comparably to the lowest-scoring\nstudent, it still lags significantly behind the group’s average\nperformance. This comparison underscores both the progress\nof MLLMs in anatomical understanding and the substantial\ngap that remains in achieving human-level performance. These\nfindings highlight the importance of optimizing MLLMs for\nneurosurgical applications.\nIndex Terms—Large Language Models, Multimodal LLM,\nSurgery Understanding, Neurosurgery.\nI. INTRODUCTION\nIn\nrecent\nyears,\nmultimodal\nlarge\nlanguage\nmodels\n(MLLMs) have demonstrated remarkable progress in surgical\nvideo understanding, laying a potential foundation for advanc-\ning both surgical education and intraoperative assistance [1],\nThis work was supported in part by the National Natural Science Foundation\nof China (Grant No.#62306313) and the InnoHK Program by the Hong Kong\nSAR Government.\n† Equal contribution.\n* Corresponding author.\n1 Hong Kong Institute of Science and Innovation, Hong Kong SAR, China.\n2 The University of Hong Kong-Shenzhen Hospital.\n3 Department of Electronic Engineering, The Chinese University of Hong\nKong, Hong Kong SAR, China.\n[2]. Existing studies [3]–[5] and datasets [6], [7] primarily\nfocus on the recognition of surgical actions, workflows [8], [9],\nor tool usage [10]–[12]. However, these works tend to overlook\nthe critical aspect of anatomical understanding. In clinical\npractice, surgeons often rely heavily on the identification and\ncomprehension of anatomical structures to interpret, review,\nand learn from surgical videos. The lack of datasets and\nbenchmarks centered on anatomical understanding limits the\ndevelopment and evaluation of advanced AI models tailored\nfor real clinical needs.\nAs shown in Table I, existing medical visual question\nanswering (VQA) datasets pay limited attention to fine-\ngrained anatomical understanding in realistic clinical scenar-\nios. Datasets such as OmniMedVQA [13], VQA-RAD [14],\nand SLAKE [15] include some questions related to anatomy,\nbut their focus is typically on broad organ-level identification\nusing static imaging modalities like MRI and CT. These\nsettings do not capture the dynamic, detailed anatomical\ncontext crucial for intraoperative operation. While more re-\ncent benchmarks, such as GMAI-MMBench [16], attempt to\ninclude anatomical tasks in clinical environments, they still\nsuffer from two key limitations: 1) potential data leakage due\nto reusing and relabeling from previous datasets, and 2) a\nlack of comprehensive, fine-grained anatomical classification\nrelevant to surgery. Therefore, currently available multimodal\nbenchmarks fall short of meeting the specific and nuanced\nneeds of surgical AI, particularly for evaluating models’ true\nanatomical comprehension in operative settings.\nNeurosurgical procedures are inherently anatomy-driven:\nsurgeons interpret, review, and learn from surgical videos pri-\nmarily by recognizing anatomical structures and understanding\ntheir relationship to surgical maneuvers. Precise anatomical\ncomprehension underpins intraoperative operation, postopera-\ntive assessment, and the continuous improvement of surgical\nskills. In this context, MLLMs’ ability to accurately identify\nfine-grained anatomical landmarks and associate them with\ncorresponding operative actions is critical for effective surgical\nvideo understanding and support. However, despite their strong\nperformance in general multimodal tasks [17], current MLLMs\nremain limited in their anatomical reasoning capabilities\nwithin neurosurgical settings. They often struggle to capture\nnuanced anatomical features, spatial relationships, and their\ndynamic changes during surgery, which are elements essential\narXiv:2512.06921v1  [cs.CV]  7 Dec 2025\n"}, {"page": 2, "text": "Which specific anatomy is \nvisible in this image?\nA: Temporal Fossa B: Temporal Lobe \nC: Precuneus          D: Skull  \nE: None of the above\nWhich anatomy is \nshown in the image?\nA: Skin B: Mastoid C: Muscles D: Tentorium \nE: None of the above\nWhich anatomy is currently \nbeing operated on?\nA: Cerebellum B: Interhemispheric Fissure \nC: Arachnoid Membranes D: Skull \nE: None of the above\nWhat anatomy can be \nseen in this visual?\nA: Foramen spinosum        B: Skull \nC: Internal Auditory Canal D: Dura Mater              \nE: None of the above\nWhich specific anatomy \nis visible in this image?\nA: Meningeal Orbital Band B: Skull C: Skin \nD: Arachnoid of Interpeduncular System\nE: None of the above\nWhich anatomy is currently \nbeing operated on?\nA: Skin     B: Anterior Cerebral Artery \nC: Lateral Ventricle  D: Anterior Cavernous Sinus\nE: None of the above\nFig. 1.\nExamples of NeuroABench. This benchmark is constructed using neurosurgical anatomical content derived from publicly available educational\nvideos. Each video is paired with a question designed to query the identification of specific neuroanatomical structures. For every question, a set of candidate\nanatomical structures (e.g., skin, anterior cerebral artery, temporal lobe) is provided as multiple-choice options.\nTABLE I\nCOMPARISON BETWEEN NEUROABENCH AND OTHER EXISTING MEDICAL VISUAL-ANSWER BENCHMARKS. HERE, WE ONLY SELECT AND COUNT\nTHE NUMBER RELATED TO ANATOMICAL UNDERSTANDING FROM THOSE DATASETS. *AC DENOTES THE ANATOMY CATEGORY.\nDataset\n#Size\n#AC∗\nSource\nVQA-RAD\n106\n31\nMedpix\nSLAKE\n507\n21\nMSD, Chestx-ray8, CHAOS\nGMAI-MMBench\n2098\n18\n284 datasets from public & hospital\nOmniMedVQA\n16448\n48\n73 classification datasets\nNeuroABench(ours)\n1079\n68\nNeurosurgical Atlas Website\nfor surgical quality assessment and skill development [18].\nThus, there is an urgent need for dedicated benchmarks that\nsystematically evaluate and drive the development of MLLMs\ntowards robust anatomical understanding in neurosurgery.\nTo address current gaps in anatomical understanding for\nneurosurgery, we introduce NeuroABench (Figure 1), a com-\nprehensive, domain-specific, multimodal benchmark that pro-\nvides a standardized framework to evaluate MLLMs’ abilities\nto comprehend anatomical structures and support intraop-\nerative decisions in real-world neurosurgical contexts. Neu-\nroABench builds upon a rigorously curated collection of 89\nhigh-quality neurosurgical videos and 32 clinician-reviewed\nteaching manuals, selected after filtering hundreds of resources\nfrom the Atlas. Using Gemini-1.5-Pro, each video receives\nannotations with structured progressions extracted from the\nmanuals and validated by expert surgeons; the resulting images\nundergo further expert review to generate precise question-\nand-answer pairs for every landmark anatomy depicted. This\nmeticulous pipeline ensures that NeuroABench captures a\nwide spectrum of clinical approaches and anatomical varia-\ntions, enabling consistent and reproducible evaluations that\nfoster the development of clinically valuable AI systems\nfor neurosurgery. Building upon this foundation, we conduct\ncomprehensive experiments to highlight the deficiencies of\ncurrent MLLMs in anatomical recognition. Additionally, we\nengage four neurosurgical specialist trainees to participate in\ntesting a subset of our dataset, enabling a comparative analysis\nbetween the capabilities of state-of-the-art models and those\nof medical professionals at a general proficiency level.\nIn summary, our main contributions are as follows: 1) We\ncollect and curate a specialized corpus of neurosurgical videos,\ncomprising 89 approach recordings with a total duration of 9\nhours, all annotated and rigorously validated by senior neuro-\nsurgeons according to intraoperative guidance standards. 2) We\npropose NeuroABench, the first benchmark specifically de-\nsigned to evaluate the anatomical understanding of MLLMs in\nreal-world neurosurgical scenarios, systematically covering 32\nneurosurgical approaches and 68 core anatomical structures.\n3) Through extensive evaluation of over 10 state-of-the-art\nMLLMs (spanning both general-purpose and medical-specific\nMLLMs), we reveal a substantial performance gap between\nMLLMs and even neurosurgical trainees in neurosurgical\nanatomical understanding: even the most advanced models\nachieve only 40.87% accuracy in anatomical identification\n"}, {"page": 3, "text": "Senior Doctor\nCheck\nMulti-round checks\nWhich anatomy is \nshown in the image?\nA: Precuneus   B: Inner Hemispheric Fissure\nC:Septal Vein   D: Skull  E: None of the above\nATLAS\nNeurosurgical\nMultimodal \nDatabase\nCollect \n&\nFilter\nTeaching \nManuals\nStructured\nProcess\nLLM\nGemini-1.5-Pro \nAtlas Videos\nAnnoated Images\nExperts\nAnnoated Images\nGenerate\nData Collection\nVideo Annotation\nVQA Generation \nFig. 2.\nPipeline illustration of NeuroABench. The data collection can be divided into three main steps: 1) We search hundreds of videos and teaching\nmanuals from the Neurosurgical Atlas, then keep 89 high-quality videos and 32 teaching manuals after filtering. 2) We use Gemini-1.5-Pro to annotate videos\nwith the instructions of clinician-reviewed structured progress extracted from teaching manuals. 3) The annotated images go through additional validation and\nexperts’ selection. From these images, we generate question-and-answer pairs for each landmark anatomy featured in the videos.\ntasks, highlighting the urgent need for further research in this\ndomain.\nII. NEUROSURIGCAL ANATOMY BENCH\nWe develop NeuroABench, an innovative benchmark metic-\nulously designed for the neurosurgery field, capable of pro-\nviding comprehensive evaluations of MLLMs in anatomy\nidentification during the neurosurgical approach. A total of\n886 neurosurgical videos are sourced from the Neurosurgical\nAtlas1, a globally recognized platform that offers authoritative\nneurosurgical tutorials presented by expert clinicians. We\nsubsequently filter 89 videos focused on the neurosurgical\napproach. Based on the data foundation, we design a reli-\nable pipeline to generate question-answer pairs. The steps in\nconstructing our NeuroABench can be divided into three main\nsteps as shown in Figure 2.\nA. Multimodal Data Collection and Curation\nTo establish a strong foundation for evaluating the under-\nstanding of anatomy, we carefully design the data collection\nphase. First, we systematically curate video resources from\nAtlas. Second, to assist with labeling, we acquire neurosurgical\napproach manuals from Atlas. These manuals provide detailed,\nstep-by-step protocols and critical precautions for various\nneurosurgical techniques. As a result of this process, we have\ncurated 89 high-quality videos and 32 teaching manuals.\nB. Video Annotation and Labeling Pipeline\nIn this stage, we design a multi-modal pipeline to annotate\nthe videos. First, OpenAI-o1 [31] is employed to analyze\nthe neurosurgical approach manuals obtained from ATLAS,\nautomatically summarizing procedural workflows based on\nanatomical landmarks. These summaries are then reviewed\nand corrected by a neurosurgical expert to ensure clini-\ncal validity and terminological precision. Subsequently, we\n1https://www.neurosurgicalatlas.com/\nemploy Gemini-1.5-Pro [25] to annotate pre-selected high-\nquality neurosurgical videos. During annotation, Gemini-1.5-\nPro utilizes both visual content and speech from experienced\nsurgeons within the videos to refine understanding. For each\nvideo, Gemini-1.5-Pro generates a JSON file containing three\nstructured keys for each video: (1) “step number” indicating\nthe sequential order of procedural phases, (2) “key anatomy”\nspecifying the landmark anatomy identified in each phase,\nand (3) “time period” documenting the temporal boundaries\nof critical procedural steps through timestamp annotations.\nFor label standardization, our standardization approach has\ntwo primary objectives: First, we expand medical abbrevia-\ntions into their full clinical terms, such as converting “IAC”\nto “Internal Auditory Canal” for better clarity. Second, we\naim to unify synonymous terms for anatomy, exemplified by\nstandardizing variations like “Dura” and “Dura Mater” into\nthe unified term “Dura Mater.” By implementing these two\nstrategies, terminological expansion and synonym unification,\nwe achieve consistency of the label that highlights crucial\nmedical semantics while reducing interpretive biases caused\nby variations in vocabulary.\nC. QA Generation and Selection\nBased on the annotated videos, we extract frames at one-\nsecond intervals and align them with anatomy annotations.\nNext, we organize a multi-round examination process in-\nvolving clinical doctors to verify these image-anatomy pairs.\nDuring quality control, we remove three types of cases: (1)\nimage-anatomy pair mismatches where annotations contradict\nthe visual evidence; (2) low-quality images lacking clear\nanatomical details; and (3) frames containing multiple land-\nmark anatomies that could introduce ambiguity.\nFor question formulation, we predefine a curated question\npool consisting of five distinct questions. For each image-\nanatomy pair, one question is randomly selected from this\npool for each case. The answer options follow a standardized\nformat: options A-D present specific anatomical names, while\noption E serves as a fallback choice (“None of the above”).\n"}, {"page": 4, "text": "TABLE II\nRESULTS OF THE ADVANCED MLLMS ON NEUROABENCH. THE BEST-PERFORMING MODEL IN EACH CATEGORY IS IN-BOLD, AND THE SECOND BEST\nIS UNDERLINED. ∗DEEPSEEK-VL2 IS BASED ON DEEPSEEKMOE-27B, WITH 4.5B PARAMETERS ACTIVATED DURING INFERENCE.\nModel\nParams\nInstance-Level\nAnatomy-Level\nAcc\nPrecision\nRecall\nF1-Score\nPrecision\nRecall\nF1-Score\nRandom\n-\n19.48\n19.99\n20.01\n19.96\n16.86\n12.10\n1.68\nOpen-Source MLLMs\nmPLUG-Owl3 [19]\n8B\n25.76\n20.37\n25.36\n22.55\n19.53\n17.96\n15.80\nDeepseek-VL2∗[20]\n4.5B\n22.15\n27.54\n23.35\n16.93\n17.53\n14.96\n14.65\nLLaVA-NeXT [21]\n7B\n28.82\n24.06\n28.90\n25.67\n23.75\n18.82\n18.64\nBaichuan-Omni-1.5 [22]\n7B\n27.25\n27.90\n27.22\n27.29\n20.45\n15.07\n15.10\nQwen2.5-VL [23]\n7B\n34.11\n36.21\n33.69\n32.18\n21.00\n20.16\n18.31\nLLaVA-NeXT [21]\n72B\n27.53\n26.70\n27.43\n25.45\n16.72\n16.62\n14.60\nQwen2.5-VL [23]\n72B\n37.44\n40.57\n37.27\n37.52\n25.91\n20.20\n20.17\nProprietary MLLMs\nGPT-4o [24]\n-\n30.21\n38.19\n29.59\n29.93\n25.11\n20.48\n19.56\nGemini-1.5-Pro\n-\n38.83\n41.68\n38.92\n35.88\n22.54\n23.35\n20.57\nGemini-2.0-Flash [25]\n-\n40.87\n46.61\n41.07\n38.56\n29.68\n27.02\n25.52\nQwen-VL-MAX [26]\n-\n18.26\n19.46\n18.26\n18.31\n12.13\n11.52\n9.29\nClaude-3.5-Sonnet [27]\n-\n40.41\n42.52\n40.44\n38.12\n29.44\n27.03\n24.52\nClaude-3.7-Sonnet [28]\n-\n33.27\n37.63\n33.11\n30.98\n25.29\n20.00\n20.35\nMedical Special Models\nLLaVA-Med-v1.5 [29]\n7B\n15.01\n12.54\n15.09\n7.66\n13.89\n10.53\n10.79\nHuatuoGPT-Vision\n7B\n30.31\n34.28\n30.87\n28.31\n22.26\n20.98\n18.85\nHuatuoGPT-Vision [30]\n34B\n36.52\n36.40\n35.48\n32.71\n24.17\n23.76\n21.49\nTo maintain a balanced answer distribution, we implement\nan equal probability allocation strategy where each option\nhas an identical 20% chance of being designated as the\ncorrect answer. As a result, we finalize 1079 QA pairs for\nthe NeuroABench.\nIII. EXPERIMENT\nA. Experiment Setup\nIn this study, we conduct comprehensive evaluations of\ndiverse MLLMs spanning medical-specialized, open-source,\nand proprietary API-based general models. The assessment\nemploys a zero-shot paradigm. To evaluate the model’s per-\nformance, we use macro-averaged Precision, Recall, and F1-\nscore as the evaluation metrics, similar to EndoNet [32]. If a\nmodel’s output does not include clearly followed instructions\nto select an answer or letter options, it is treated as an error. To\naddress the potential influence of uneven data distribution in\nour benchmark, we report performance scores at two distinct\nlevels: the instance-level, which represents the average of the\nperformance metrics computed across all individual images,\nand the anatomy-level, which is derived as the average of the\nperformance metrics across all anatomical structures.\nB. Models\nFor completeness, we conduct evaluations using several\nstate-of-the-art MLLMs to benchmark their performance on\nSABench, including both general models and medical-specific\nmodels that are meticulously trained for clinical medicine.\nOur testing encompasses architectures ranging from 7 bil-\nlion to 72 billion parameters to examine scale-related per-\nformance variations. Model weights are rigorously sourced\nfrom their official Hugging Face repositories, maintaining\nversion integrity across all implementations. The assess-\nment employs a strict zero-shot paradigm where evalua-\ntion prompts contain no exemplar demonstrations, requiring\nmodels to solve novel problems through pure comprehen-\nsion without task-specific training or contextual examples.\nFor medical-specific models, we include 3 latest powerful\nMLLMs: LLaVA-Med-v1.5 [29], HuatuoGPT-Vision-7B, and\nHuatuoGPT-Vision-34B [30]. For general models, we test 7\nof the latest and most advanced MLLMs currently available:\nmPLUG-Owl3 [19], Deepseek-VL2 [20], LLaVA-NeXT-7B,\nLLaVA-NeXT-72B [21], Baichuan-Omni-1.5 [22], Qwen2.5-\nVL-7B, Qwen2.5-VL-72B [23]. In addition, we also evalu-\nate 6 proprietary MLLMs via API: GPT-4o [24], Gemini-\n1.5-pro [25], Gemini-2.0-Flash [33], Qwen-VL-MAX [26],\nClaude-3.5-Sonnet [27], and Claude-3.7-Sonnet [28]. We also\ninvite four medical trainees to participate in our benchmark\nassessment.\nC. Results\nAfter reviewing the evaluation results, we have 3 key\nfindings for developing future MLLMs in the medical domain:\nFinding\n1:\nMedical\ntasks\nremain\nchallenging\nfor\nMLLMs. Our surgical anatomy identification benchmark,\nNeuroABench, presents deceptively simple yet clinically de-\nmanding tasks. The evaluation reveals that even state-of-the-art\nmodels, such as Gemini-2.0-Flash, achieve only 40.87% preci-\nsion (Table II), falling short of clinical applicability standards.\nThis performance gap underscores the need for substantial\nimprovements in current MLLMs’ medical capabilities.\nFinding 2: The most advanced models still lag medical\n"}, {"page": 5, "text": "Based on the image provided, the identifiable anatomical structure appears to \nbe the Tentorium. The tentorium is a fold of dura mater that separates the \ncerebrum from the cerebellum within the cranial cavity. The presence of this \nsheet-like structure indicated by the tools in the surgical approach is a strong \nindication. Therefore, the correct option in this case would be B: Tentorium\nThe image appears to show Internal Cerebral Veins (option D) during what \nlooks like a neurosurgical procedure. This can be identified by the \ncharacteristic deep red vascular structures visible in the surgical field. Internal \ncerebral veins are important deep venous structures within the brain that are \nsometimes encountered during certain neurosurgical approaches. The image \nquality and presence of surgical instrumentation suggests this is from an \nintraoperative view during a neurosurgical procedure.\nAs a note, when analyzing surgical images, it's important to consider the \ncontext and multiple anatomical landmarks. However, in this case, the \nprominent vascular structures most clearly align with option D (Internal \nCerebral Veins) among the given choices.\nFig. 3. A case on the influence of anatomical deformation. Here, we select the responses of Claude-3.5-Sonnet to two closely similar frames of anatomical\nimages from the same surgical procedure to demonstrate the impact of anatomical deformation on the model’s anatomical recognition.\nTABLE III\nPERFORMANCE COMPARISON OF MODELS AND NEUROSURGICAL\nSPECIALIST TRAINEES. WE SELECT THE BEST-PERFORMING\nGEMINI-2.0-FLASH FOR COMPARISON WITH HUMANS (TRAINEES).\nParticipant\nAcc\nNeurosurgical Specialist Trainees 1\n0.48\nNeurosurgical Specialist Trainees 2\n0.28\nNeurosurgical Specialist Trainees 3\n0.56\nNeurosurgical Specialist Trainees 4\n0.54\nGemini-2.0-Flash\n0.36\ntrainees. As shown in Table III, the accuracies of the four\nparticipants are 48%, 28%, 56%, and 54%, respectively. This\nindicates that most models can reach the level of the worst\nmedical trainee. However, the best-performing model, Gemini-\n2.0-Flash, still lags behind most medical trainees by a sig-\nnificant margin (average -10.50%). Additionally, through our\ntesting, the performance variance of MLLMs is far lower than\nthe variability observed among humans. This demonstrates that\nwhile current MLLMs still underperform compared to human\npractitioners, their future development could enable them to\nachieve comparable proficiency to doctors while maintaining\nstability in clinical applications.\nFinding 3: There is a deficiency in current medical\ntraining data. Our analysis shows that while medically-\nspecialized models (e.g., Huatuo-Vision-34B) achieve modest\nperformance improvements (average +7.52% accuracy) over\ngeneral-purpose MLLMs in anatomy reasoning tasks, their\nabsolute performance remains unsatisfying (ceiling accuracy:\n36.52% in Table II). We suspect that current medical special\nmodels neglect clinical surgical anatomy comprehension when\ntraining with medical data.\nInstance-Level Metric = 1\nN\nN\nX\ni=1\n\n1\nK\nK\nX\nj=1\nMi,j\n\n\nD. Discussion\nTo gain deeper insight into the reasoning processes of\nMLLMs, we require these models to produce free-form re-\nsponses and explicitly articulate their reasoning steps, rather\nthan merely selecting from a set of predefined options. This\napproach facilitates a more nuanced and comprehensive anal-\nysis of how models arrive at their conclusions, allowing us to\ndissect their decision-making paths and uncover specific error\npatterns. By examining the explanations provided in these\nopen-ended responses, we can better identify the limitations\nand strengths of current models in complex clinical scenarios.\nThe task of clinical anatomical identification presents sig-\nnificant challenges for MLLMs, stemming not only from their\nlimited exposure to detailed anatomical information during\npre-training but also from the complex morphological changes\ninduced by intraoperative procedures. These challenges are\nevident when anatomical structures undergo transformation\ndue to manipulation or surgical intervention, which frequently\noccurs in real-world neurosurgical settings. For instance, as\nillustrated in Figure 3, in the first image, Claude-3.5-sonnet\nwas able to accurately identify and describe the anatomical\nfeatures presented, as the anatomy was in the exposure phase\nand retained its standard morphology, unaffected by surgi-\ncal manipulation. However, in the second image, where the\nanatomical structure was actively manipulated and had experi-\nenced noticeable deformation, Claude-3.5-sonnet’s description\ndeviated from the correct answer, leading to an erroneous\nfinal response. This example highlights how morphological\nalterations can significantly impact model performance.\nSuch findings underscore the fact that current MLLMs\npossess an understanding of anatomy that is largely limited to\nstatic, textbook representations, rather than the dynamic and\noften altered presentations encountered in clinical practice. As\na result, these models struggle to generalize their anatomical\nknowledge to the complexities of real-world surgical environ-\nments, demonstrating the urgent need for more robust and\ncontext-aware training paradigms that can bridge this critical\ngap between theoretical knowledge and practical application\n"}, {"page": 6, "text": "in neurosurgery.\nIV. CONCLUSION\nWe collect a diverse set of neurosurgical videos from the\nNeurosurgical Atlas and establish a systematic pipeline to\nannotate this new video library. Building on this rigorously\ncurated database, we develop NeuroABench, a benchmark for\nevaluating neurosurgical anatomy identification by AI models.\nOur annotation process is overseen by experienced neuro-\nsurgeons to ensure clinical accuracy and reliability. Through\ncomprehensive experiments with over 10 advanced MLLMs,\nwe observe notable performance gaps between the MLLMs\nand neurological trainees. The highest anatomical identifi-\ncation accuracy achieved is only 40.87%, highlighting the\nlimitations of existing models and emphasizing the need for\nAI solutions tailored to the unique demands of neurosurgical\npractice. NeuroABench helps address this gap by offering a\nstandardized, clinically aligned benchmark for evaluating and\nguiding the development of MLLMs in neurosurgery.\nREFERENCES\n[1] A. Guduguntla, A. Al-Khanaty, C. E. Davey, O. Patel, A. Ta, and\nJ. Ischia, “A review of the intraoperative use of artificial intelligence\nin urologic surgery,” Soci´et´e Internationale d’Urologie Journal, vol. 6,\nno. 1, p. 5, 2025.\n[2] L. Bai, M. Islam, L. Seenivasan, and H. Ren, “Surgical-vqla: Trans-\nformer with gated vision-language embedding for visual question\nlocalized-answering in robotic surgery,” in 2023 IEEE International\nConference on Robotics and Automation (ICRA).\nIEEE, 2023, pp.\n6859–6865.\n[3] L. Bai, G. Wang, M. Islam, L. Seenivasan, A. Wang, and H. Ren,\n“Surgical-vqla++: Adversarial contrastive learning for calibrated robust\nvisual question-localized answering in robotic surgery,” Information\nFusion, vol. 113, p. 102602, 2025.\n[4] Z. Chen, X. Luo, J. Wu, D. T. Chan, Z. Lei, S. Ourselin, and H. Liu,\n“Surgfc: Multimodal surgical function calling framework on the demand\nof surgeons,” in 2024 IEEE International Conference on Bioinformatics\nand Biomedicine (BIBM).\nIEEE, 2024, pp. 3076–3081.\n[5] P. Hao, S. Li, H. Wang, Z. Kou, J. Zhang, G. Yang, and L. Zhu, “Surgery-\nr1: Advancing surgical-vqla with reasoning multimodal large language\nmodel via reinforcement learning,” arXiv preprint arXiv:2506.19469,\n2025.\n[6] A. Twinanda, S. Shehata, D. Mutter, J. Marescaux, M. D. Mathelin,\nand N. Padoy, “Endonet: A deep architecture for recognition tasks on\nlaparoscopic videos,” IEEE Transactions on Medical Imaging, vol. 36,\n02 2016.\n[7] G. Wang, L. Bai, J. Wang, K. Yuan, Z. Li, T. Jiang, X. He, J. Wu,\nZ. Chen, Z. Lei et al., “Endochat: Grounded multimodal large language\nmodel for endoscopic surgery,” arXiv preprint arXiv:2501.11347, 2025.\n[8] Z. Chen, X. Luo, J. Wu, L. Bai, Z. Lei, H. Ren, S. Ourselin, and H. Liu,\n“Surgplan++: Universal surgical phase localization network for online\nand offline inference,” arXiv preprint arXiv:2409.12467, 2024.\n[9] X. Luo, Y. Pang, Z. Chen, J. Wu, Z. Zhang, Z. Lei, and H. Liu,\n“Surgplan: Surgical phase localization network for phase recognition,”\nin 2024 IEEE International Symposium on Biomedical Imaging (ISBI).\nIEEE, 2024, pp. 1–5.\n[10] J. Li, G. Skinner, G. Yang, B. R. Quaranto, S. D. Schwaitzberg, P. C.\nKim, and J. Xiong, “Llava-surg: towards multimodal surgical assistant\nvia structured surgical video learning,” arXiv preprint arXiv:2408.07981,\n2024.\n[11] G. Wang, L. Bai, W. J. Nah, J. Wang, Z. Zhang, Z. Chen, J. Wu,\nM. Islam, H. Liu, and H. Ren, “Surgical-lvlm: Learning to adapt large\nvision-language model for grounded visual question answering in robotic\nsurgery,” arXiv preprint arXiv:2405.10948, 2024.\n[12] P. Hao, H. Wang, S. Li, Z. Xing, G. Yang, K. Wu, and L. Zhu, “Surgical-\nmamballm: Mamba2-enhanced multimodal large language model for\nvqla in robotic surgery,” in International Conference on Medical Image\nComputing and Computer-Assisted Intervention.\nSpringer, 2025, pp.\n573–583.\n[13] Y. Hu, T. Li, Q. Lu, W. Shao, J. He, Y. Qiao, and P. Luo, “Omnimedvqa:\nA new large-scale comprehensive evaluation benchmark for medical\nlvlm,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2024, pp. 22 170–22 183.\n[14] J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman, “A dataset\nof clinically generated visual questions and answers about radiology\nimages,” Scientific data, vol. 5, no. 1, pp. 1–10, 2018.\n[15] B. Liu, L.-M. Zhan, L. Xu, L. Ma, Y. Yang, and X.-M. Wu, “Slake:\nA semantically-labeled knowledge-enhanced dataset for medical visual\nquestion answering,” in 2021 IEEE 18th international symposium on\nbiomedical imaging (ISBI).\nIEEE, 2021, pp. 1650–1654.\n[16] J. Ye, G. Wang, Y. Li, Z. Deng, W. Li, T. Li, H. Duan, Z. Huang,\nY. Su, B. Wang et al., “Gmai-mmbench: A comprehensive multimodal\nevaluation benchmark towards general medical ai,” Advances in Neural\nInformation Processing Systems, vol. 37, pp. 94 327–94 427, 2024.\n[17] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, “A survey on\nmultimodal large language models,” National Science Review, vol. 11,\nno. 12, Nov. 2024.\n[18] Z. Chen, Z. Zhang, W. Guo, X. Luo, L. Bai, J. Wu, H. Ren, and H. Liu,\n“Asi-seg: Audio-driven surgical instrument segmentation with surgeon\nintention understanding,” in 2024 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS). IEEE, 2024, pp. 13 773–13 779.\n[19] J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang, and\nJ. Zhou, “mplug-owl3: Towards long image-sequence understanding in\nmulti-modal large language models,” arXiv preprint arXiv:2408.04840,\n2024.\n[20] Z. Wu, X. Chen, Z. Pan, X. Liu, W. Liu, D. Dai, H. Gao,\nY. Ma, C. Wu, B. Wang et al., “Deepseek-vl2: Mixture-of-experts\nvision-language models for advanced multimodal understanding,” arXiv\npreprint arXiv:2412.10302, 2024.\n[21] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee, “Llava-next:\nImproved reasoning, ocr, and world knowledge,” January 2024. [Online].\nAvailable: https://llava-vl.github.io/blog/2024-01-30-llava-next/\n[22] Y. Li, J. Liu, T. Zhang, S. Chen, T. Li, Z. Li, L. Liu, L. Ming, G. Dong,\nD. Pan et al., “Baichuan-omni-1.5 technical report,” arXiv preprint\narXiv:2501.15368, 2025.\n[23] Q. Team, “Qwen2.5-vl,” January 2025. [Online]. Available: https:\n//qwenlm.github.io/blog/qwen2.5-vl/\n[24] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark,\nA. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., “Gpt-4o system\ncard,” arXiv preprint arXiv:2410.21276, 2024.\n[25] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer,\nD. Vincent, Z. Pan, S. Wang et al., “Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context,” arXiv preprint\narXiv:2403.05530, 2024.\n[26] Q. Team, “Introducing qwen1.5,” February 2024. [Online]. Available:\nhttps://qwenlm.github.io/blog/qwen1.5/\n[27] A. Team, “Claude 3.5 sonnet model documentation,” https://www.\nanthropic.com/news/claude-3-5-sonnet, 2024, accessed: 2024-06-21.\n[28] ——, “Claude 3.7 sonnet model documentation,” https://www.anthropic.\ncom/news/claude-3-7-sonnet, 2025, accessed: 2025-02-25.\n[29] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,”\nAdvances in neural information processing systems, vol. 36, pp. 34 892–\n34 916, 2023.\n[30] J. Chen, C. Gui, R. Ouyang, A. Gao, S. Chen, G. H. Chen, X. Wang,\nR. Zhang, Z. Cai, K. Ji et al., “Huatuogpt-vision, towards injecting\nmedical visual knowledge into multimodal llms at scale,” arXiv preprint\narXiv:2406.19280, 2024.\n[31] OpenAI, “Openai model versioning system,” 2025, model updates\noccur every 3 months with backward compatibility. [Online]. Available:\nhttps://platform.openai.com/docs/model-versioning\n[32] A. P. Twinanda, S. Shehata, D. Mutter, J. Marescaux, M. De Mathelin,\nand N. Padoy, “Endonet: a deep architecture for recognition tasks on\nlaparoscopic videos,” IEEE transactions on medical imaging, vol. 36,\nno. 1, pp. 86–97, 2016.\n[33] Google DeepMind, “Gemini 2.0 pro model documentation,” https://\nai.google.dev/gemini-api/docs/models/gemini, 2025, accessed: 2025-02-\n19.\n"}]}