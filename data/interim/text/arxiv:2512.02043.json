{"doc_id": "arxiv:2512.02043", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.02043.pdf", "meta": {"doc_id": "arxiv:2512.02043", "source": "arxiv", "arxiv_id": "2512.02043", "title": "Mirror, Mirror on the Wall -- Which is the Best Model of Them All?", "authors": ["Dina Sayed", "Heiko Schuldt"], "published": "2025-11-25T20:52:45Z", "updated": "2025-11-25T20:52:45Z", "summary": "Large Language Models (LLMs) have become one of the most transformative tools across many applications, as they have significantly boosted productivity and achieved impressive results in various domains such as finance, healthcare, education, telecommunications, and law, among others. Typically, state-of-the-art (SOTA) foundation models are developed by large corporations based on large data collections and substantial computational and financial resources required to pretrain such models from scratch. These foundation models then serve as the basis for further development and domain adaptation for specific use cases or tasks. However, given the dynamic and fast-paced nature of launching new foundation models, the process of selecting the most suitable model for a particular use case, application, or domain becomes increasingly complex. We argue that there are two main dimensions that need to be taken into consideration when selecting a model for further training: a qualitative dimension (which model is best suited for a task based on information, for instance, taken from model cards) and a quantitative dimension (which is the best performing model). The quantitative performance of models is assessed through leaderboards, which rank models based on standardized benchmarks and provide a consistent framework for comparing different LLMs. In this work, we address the analysis of the quantitative dimension by exploring the current leaderboards and benchmarks. To illustrate this analysis, we focus on the medical domain as a case study, demonstrating the evolution, current landscape, and practical significance of this quantitative evaluation dimension. Finally, we propose a Model Selection Methodology (MSM), a systematic approach designed to guide the navigation, prioritization, and selection of the model that best aligns with a given use case.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.02043v1", "url_pdf": "https://arxiv.org/pdf/2512.02043.pdf", "meta_path": "data/raw/arxiv/meta/2512.02043.json", "sha256": "c8b0d05b7f92e73ad817e8d4b4f69739abe8355b8537c5797654e9b2fa2ae6f6", "status": "ok", "fetched_at": "2026-02-18T02:26:16.433023+00:00"}, "pages": [{"page": 1, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n1\nMirror, Mirror on the Wall – Which is the Best\nModel of Them All?\nDina Sayed\nHeiko Schuldt\nDatabases and Information Systems Research Group\nUniversity of Basel, Switzerland\n{dina.awad|heiko.schuldt}@unibas.ch\nAbstract—Large Language Models (LLMs) have become one\nof the most transformative tools across many applications, as they\nhave significantly boosted productivity and achieved impressive\nresults in various domains such as finance, healthcare, education,\ntelecommunications, and law, among others. Typically, state-\nof-the-art (SOTA) foundation models are developed by large\ncorporations based on large data collections and substantial\ncomputational and financial resources required to pretrain such\nmodels from scratch. These foundation models then serve as\nthe basis for further development and domain adaptation for\nspecific use cases or tasks. However, given the dynamic and fast-\npaced nature of launching new foundation models, the process\nof selecting the most suitable model for a particular use case,\napplication, or domain becomes increasingly complex. We argue\nthat there are two main dimensions that need to be taken into\nconsideration when selecting a model for further training: a\nqualitative dimension (which model is best suited for a task –\nbased on information, for instance, taken from model cards [1])\nand a quantitative dimension (which is the best performing\nmodel). The quantitative performance of models is assessed\nthrough leaderboards, which rank models based on standardized\nbenchmarks and provide a consistent framework for comparing\ndifferent LLMs. In this work, we address the analysis of the\nquantitative dimension by exploring the current leaderboards\nand benchmarks. To illustrate this analysis, we focus on the\nmedical domain as a case study, demonstrating the evolution,\ncurrent landscape, and practical significance of this quantitative\nevaluation dimension. Finally, we propose a Model Selection\nMethodology (MSM), a systematic approach designed to guide\nthe navigation, prioritization, and selection of the model that\nbest aligns with a given use case.\nIndex\nTerms—Medical\nBenchmarks,\nLLM\nLeaderboards,\nMedical Datasets.\nI. INTRODUCTION\nC\nONSIDER a scenario in which a professional seeks to\nperform a task with greater efficiency; for example, a\nmedical expert may wish to produce a concise summary of\na case or generate a structured report. In previous decades,\nsuch tasks often required significant time and effort. However,\nwith the advancement of natural language processing, and in\nparticular the emergence of large language models (LLMs),\nthese tasks have become more manageable and can now be\ncompleted in a fast and effective manner. LLMs have trans-\nformed numerous domains and are considered indispensable\ntools across a wide range of professions. In response to\nThis work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may\nno longer be accessible.\nthis growth, many organizations and research institutions are\nreleasing their own LLMs at a rapid pace, in ever increasing\nquality.\nThis raises the key question: Which model is most ap-\npropriate and best performing for a given task and set of\nrequirements? Selecting a suitable model for subsequent fine-\ntuning necessitates the evaluation of two primary criteria:\na quantitative dimension and a qualitative dimension, as\nfollows:\n• Qualitative dimension: involves human-centric evaluation\nof a model’s functionality. This includes carefully review-\ning the model card for the authors’ reported limitations\nand intended use cases [1]. It can also include methods\nsuch as Report Card, which focus on generating natural\nlanguage reports that analyze the model’s specificity,\nfaithfulness, and interpretability [2], [3].\n• Quantitative dimension: focuses on answering how well\na model performs numerically. We examine the perfor-\nmance aspect by reviewing current model rankings across\nvarious leaderboards and benchmarks. These rankings\nprovide a standardized quantitative measure of how dif-\nferent LLMs perform, enabling consistent comparisons\nacross models.\nIn this paper, we mainly address the analysis of the quantita-\ntive dimension. To further illustrate this analysis, we focus on\nthe medical domain as an example, which helps to highlight\nthe benefits of using such metrics, shed light on the challenges\nof maintaining these leaderboards, and emphasize that this\ncannot be the sole method for model selection. Finally, based\non our analysis, we propose a systematic approach for model\nselection.\nThe contributions of this paper are threefold: First, we\nexplore the quantitative dimension by introducing various\nleaderboards and delving deeper into the medical domain as an\nexample of a domain-specific case, navigating through medical\nleaderboards and benchmarks. Second, we shed light on the\ninsights that can be derived from community endorsements\nof different models. Third, we propose the Model Selection\nMethodology (MSM), an intuitive and systematic approach\nthat supports the selection and shortlisting of the most suitable\nmodels for specific use cases.\nThe remainder of this paper is structured as follows: Sec-\ntion II introduces various leaderboards as the quantitative di-\nmension and Section III surveys existing medical leaderboards.\nSection IV illustrates commonly used benchmarks in the\narXiv:2512.02043v1  [cs.CL]  25 Nov 2025\n"}, {"page": 2, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n2\nmedical domain and the discussion in Section V outlines how\nto address the most pertinent challenges in LLM leaderboards.\nSection VI explores the community voting metrics for mod-\nels. Section VII proposes the Model Selection Methodology\n(MSM) to help in selecting the most suitable models using\na structured approach. Section VIII discusses the need for\nstandardizing the model selection process. Finally, Section IX\nconcludes the paper.\nII. LEADERBOARDS AND BENCHMARKS – AN OVERVIEW\nLLM leaderboards have been introduced as a mechanism to\nprovide systematic comparative insights into various LLMs.\nLLM leaderboards are popular within the research community\nas they provide a quick overview of the latest top-performing\nmodels and their rankings. Model performance is usually\npresented by a quantitative metric such as accuracy, F1 score,\nBLEU, or ROUGE depending on the benchmark/task under\nassessment, whereas model rank represents the position of the\nmodel compared to others on the same benchmark/task.\nLLM leaderboards present a common ground for comparing\nvarious models against the same benchmarks across different\nskill sets, such as machine translation, summarization, and\nquestion answering. For instance, the Open LLM Leaderboard\n[4] was one of the early leaderboards, with the objective of\nenabling the evaluation of any model under the exact same\nsetup (same questions, same order, etc.). It covers bench-\nmarks such as ARC [5], MMLU [6], HellaSwag [7], and\nTruthfulQA [8]. Another example is the Holistic Evaluation\nof Language Models (HELM) [9], which is composed of a set\nof leaderboards, each focusing on specific capabilities, such\nas Helm-Safety [10], which focuses on a collection of safety\nand risk benchmarks (e.g., violence, fraud, discrimination,\nharassment), VHELM [11], which focuses on assessing vision-\nlanguage models, and MedHELM [12] for medical tasks.\nThere are also leaderboards that are based on user votes,\nsuch as LMArena, a web-based platform that evaluates LLMs\nthrough crowd-sourced, pairwise comparisons from anony-\nmous users. Users enter prompts for two anonymous models\nand, according to each model’s response, vote for their pre-\nferred response. After the vote is cast, the two model identities\nare revealed. LMArena [13] was established by Large Model\nSystems (LMSYS) [13], a non-profit organization, and the\nrankings on the leaderboard are based on an ELO rating\nsystem [14]. The spread of leaderboards has become more\nfocused on different NLP tasks, such as TTS-Arena [15] for\ntext-to-speech models, the Open ASR Leaderboard [16] for\nspeech-to-text models, and the Open VLM Leaderboard [17]\nfor vision-language models.\nIII. EXISTING MEDICAL LEADERBOARDS\nIn recent years, many leaderboards have been developed to\nevaluate LLMs in the medical domain. This section focuses\non leaderboards that include multiple benchmarks to compre-\nhensively assess LLM performance. Many of these platforms\nare openly accessible, enabling both public inspection and\nreproducibility of results. A notable recent trend is the shift\nfrom traditional QA-style evaluations toward more diverse,\nskill-based assessments. These assessments are designed to test\na range of capabilities, such as clinical reasoning, information\nextraction, and summarization skills, rather than relying only\non multiple-choice or short-answer formats. Table I provides\nan example illustrating how different skill sets can be mapped\nto corresponding medical benchmarks. This broader evaluation\napproach has been adopted by several leaderboards, including\nfor instance ClinicBench [18] and MedHELM [19] leader-\nboards, which incorporate varied benchmarks targeting distinct\nskill categories. The remainder of this section provides an\noverview of selected medical LLM leaderboards and examines\nthe scope of the benchmarks they cover.\nA. Open Medical-LLM\nThe Open Medical-LLM Leaderboard [36] is a board that\nlists different LLM performances on medical question an-\nswering tasks. The main objective is to provide a broad\nassessment of each model’s medical knowledge, medical rea-\nsoning capabilities, and question answering abilities [37]. The\ndatasets covered in the leaderboard are MedQA (USMLE),\nPubMedQA, MedMCQA, and parts of MMLU that relate to\nmedicine and biology. It is open for the research community to\nsubmit their models, and the evaluation metric used is accuracy\n(ACC). Only models that are actively accessible are included\nin the leaderboard, enabling submission result validation and\nmaintaining the leaderboard’s integrity and transparency.\nB. MIRAGE\nMedical Information Retrieval-Augmented Generation Eval-\nuation (MIRAGE) [38], [39] is a leaderboard that covers\n7,663 questions from five medical QA datasets: MMLU-\nMed, MedQA-US, MedMCQA, PubMedQA, and BioASQ-\nY/N [40]. The core idea behind MIRAGE is to tackle LLMs’\nhallucination challenges and outdated knowledge by employ-\ning the Retrieval-Augmented Generation (RAG) method for\nthe medical domain [41]. RAG is a method that improves\nLLM outputs by combining them with information retrieval\nsystems. Thus, in RAG, the outcome does not rely only on\nthe LLMs’ pre-trained knowledge; instead, it accesses and\nextracts information from external, specific sources, which\nmakes the results more accurate and relevant. The MIRAGE\napproach utilizes the RAG technique by introducing MedRAG,\na toolkit that combines different retrieval methods, corpora,\nand LLMs [42]. Retrievers used include lexical-based retriev-\ners like BM25 [43], [44], and semantic retrievers such as\na general-domain semantic retriever (Contriever) [45], [46],\na scientific-domain retriever (SPECTER) [47], [48], and a\nbiomedical-domain retriever (MedCPT) [49], [50].\nFor the corpora used in MedRAG, the data is gathered from\nPubMed (all biomedical abstracts, totaling 23.9 million docu-\nments) [51], StatPearls [52] for clinical decision support (cov-\nering 9.3k documents and 301.2k snippets), medical textbooks\nfor domain-specific knowledge (covering 18 documents and\n125.8k snippets) [53], and Wikipedia for general knowledge\n(covering 6.5 million documents and 29.9 million snippets).\nLastly, the MedCorp corpus includes 30.4 million documents\nand 54.2 million snippets. All snippets from these corpora\n"}, {"page": 3, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n3\nTABLE I\nEXAMPLES OF SKILL SETS AND CORRESPONDING MEDICAL BENCHMARKS\nSkill Category\nSpecific Skill\nBenchmark Examples\nQuestion & Answering\nMultiple-Choice QA\nMedQA [20], MedMCQA [21], MMLU [6]\nOpen-Ended QA\nLiveQA [22], MedicationQA [23]\nReasoning\nDecision Making\nPubMedQA [24]\nDifferential Diagnosis\nDDXPlus [25]\nVision\nQuestion Answering\nVQA-RAD [26]\nSummarization\nConsumer Long Inquiry\nMeQSum [27]\nReporting\nNote Generation\nAci-bench [28], CliniKnote [29]\nInformation Extraction\nNamed Entity Recognition (NER)\nBC5CDR [30]\nRelation Extraction (RE)\nGAD [31]\nInformation Retrieval\nDocument Retrieval\nTREC Clinical Trials [32]\nEthical Decision Making\nFairness & Bias\nMIMIC-IV [33], EquityMedQA [34]\nSafety\nMedsafetybench [35]\nContriever\nGPT-4\nBM 25\nMedCPT\nSPECTER\nPubMed\nMEDITRON\nMixtral\nLlama-2\nGPT-3.5\nWikipedia\nStatesPearls\nMedCorp\nTextBooks\nPMC-Llama\nQuestion\nAnswer\nRetrievers\nLLMs\nCorpora\nIndexing\nRetrieval\nGeneration\nFig. 1. MedRAG Toolkit Workflow.\nenhance cross-source retrieval for different inquiries. Figure 1\ndemonstrates the workflow of the MedRAG toolkit.\nC. MedHELM\nMedHELM [19] is one of the HELM leaderboards, which\nfocuses exclusively on evaluating LLM medical tasks. It\nconsists of 5 categories, 22 subcategories, and 121 clinical\ntasks covered by 35 benchmarks. The five categories are: Clin-\nical Decision Support; Patient Communication; Clinical Note\nGeneration; Medical Research Assistance; and Administration\nand Workflow. An example of the taxonomy covered is shown\nin Figure 2. The objective of the MedHELM leaderboard is to\nconduct evaluations of practical applications of language mod-\nels in healthcare. Unlike the Open LLM Leaderboard, HELM\nis not open for public submissions. It is maintained by the\nStanford Center for Research on Foundation Models (CRFM),\nwhich evaluates predefined state-of-the-art (SOTA) models\nfrom major research labs and companies. The 35 benchmarks\nused are distributed as follows: 14 private, 7 gated-access\n(i.e., requiring approval), and 14 public. MedHELM collab-\norated with various medical and technology institutions to\nbuild the leaderboard, which brought diverse coverage in the\nbenchmarks used. It also covers a set of LLMs from some\nof the pioneers in the field. While MedHELM provides open\ndocumentation to facilitate leaderboard reproduction, some\nbenchmarks remain private because they are proprietary or\nrestricted datasets available only to the respective organiza-\ntions [19]. Closed leaderboards may be discouraging for the\nbroader research community, as smaller labs or open-source\nprojects might struggle to gain fair recognition there.\nSeveral other medical leaderboards target different aspects\nof the domain. ClinicBench [18] supports a diverse set of\nbenchmarks across various medical tasks [54], [55]. MMed-\nBench [56] focuses on medical multiple-choice questions\nspanning six languages [57]. HealthBench focuses on realistic\nhealth conversations, each graded using custom, physician-\ncreated criteria [58].\nIV. MEDICAL BENCHMARKS USED IN LEADERBOARDS\nIn this section, we explore a selected set of medical bench-\nmarks that are commonly used on leaderboards, as summa-\nrized in Table II, also used in evaluating many models like\nMedPrompt [59], GPT4 [60], MedPaLM [61], Claude 3 [62]\nand MEDITRON [63], summarized in Table III. By analyz-\ning these benchmarks, we aim to uncover how leaderboard\nrankings translate into measurable capabilities and identify\nthe specific skills they evaluate, thereby helping to build\nadditional benchmarks that address any gaps in assessing\nmedical models.\nA. MedQA\nThe MedQA dataset benchmark [20] is considered one of\nthe first open-domain QA (OpenQA) datasets aimed at finding\n"}, {"page": 4, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n4\nClinical Note\nGeneration\nClinical Decision\nSupport\nMedHELM\nPatient\nCommunication &\nEducation\nMedical Research\nAssistance\nAdministration &\nWorkflow\nSupporting\nDiagnostic\nDecisions\nPlanning\nTreatments\nDocumenting\nPatient Visits\nRecording\nProcedures\nDeliver\nPersonalized Care\ninstruct\nPatient Provider\nMessaging\nRecording\nResearch Process\nAnalyze Clinical\nResearch Data\nOverseeing\nFinancial Activites\nScheduling\nResources & Staff\nEx: Generate\ndifferential\ndiagnoses\nEx: Generate visit\nprogress notes\nEx: Triage and route\npatient messages\nEx: Statistically\nanalyze\ntrial data\nEx: Generate\nbilling codes\nFig. 2. Overview of MeHELM Categories, Subcategories and tasks [19].\nTABLE II\nMEDICAL BENCHMARKS USED IN VARIOUS LEADERBOARDS\nBenchmark\nOpen Medical-LLM\nMIRAGE\nMedHELM\nClinicBench\nMedQA\n✓\n✓\n×\n✓\nMMLU\n✓\n✓\n×\n✓\nMedMCQA\n✓\n✓\n×\n✓\nPubMedQA\n✓\n✓\n✓\n✓\nBIOASQ\n×\n✓\n×\n×\nTABLE III\nMEDICAL BENCHMARKS USED IN EVALUATION FOR VARIOUS MODELS\nBenchmark\nMedPrompt\nGPT4\nMed-PaLM2\nClaude3\nMEDITRON\nMedQA\n✓\n✓\n✓\n✓\n✓\nMMLU\n✓\n✓\n✓\n×\n✓\nMedMCQA\n✓\n✓\n✓\n✓\n✓\nPubMedQA\n✓\n✓\n✓\n✓\n✓\nBIOASQ\n×\n×\n×\n✓\n×\nanswers to medical questions based on large-scale information\nsources such as search engines or Wikipedia. Examples of\nOpenQA datasets include ARC [5] and OpenBookQA [64].\nMedQA was developed from professional national board ex-\nams in the USA, China, and Taiwan, known in the literature as\nUSMLE, MCMLE, and TWMLE, respectively. MedQA covers\nthree main languages: English with 12,723 questions, Simpli-\nfied Chinese with 34,251 questions, and Traditional Chinese\nwith 14,123 questions. The main objective of these exams is\nto evaluate doctors’ ability to apply their acquired knowledge,\nunderstand concepts, and demonstrate decision-making skills.\nThe exam questions typically begin by describing a medical\nuse case involving a patient’s condition, followed by multiple-\nchoice answers targeting diagnosis or treatment options.\nB. MMLU\nMMLU (Massive Multitask Language Understanding) [6] is\na dataset designed to assess the knowledge acquired by LLMs\nduring pretraining. It contains a medical part covering topics\nlike -clinical knowledge, professional medicine, anatomy and\nmore. It evaluates models in both zero-shot and few-shot\nsettings. The benchmark aims to align more closely with hu-\nman evaluation by including a wide range of difficulty levels,\nfrom elementary to advanced professional, and by assessing\nboth world knowledge and problem-solving abilities. MMLU\nincludes 15,908 multiple-choice questions [65], covering 57\nsubjects across STEM, the humanities, the social sciences, and\nother domains.\nC. MedMCQA\nThe MedMCQA dataset is a large-scale multiple-choice QA\nbenchmark [21]. It consists of more than 194K questions\nextracted from two medical entrance exams: the All India\nInstitute of Medical Sciences (AIIMS PG) and the National\nEligibility cum Entrance Test (NEET PG). The primary objec-\ntive of these exams is to evaluate doctors’ ability to apply their\nacquired knowledge and use reasoning to diagnose presented\ncases and make appropriate treatment decisions. MedMCQA\nincludes several medical topics such as Anesthesia, Anatomy,\nBiochemistry, Dentistry, Medicine, and others. The exams\ncover approximately 2.4K healthcare topics and 21 medical\nsubjects. In MedMCQA, each question presents a medical\nquery associated with four possible answer choices, some\nof which include an explanation section. Such explanations\ncan be used as contextual information, as referenced by the\nleaderboard [66]. The testing subset of MedMCQA comprises\n4,183 records.\nD. PubMedQA\nPubMedQA [24] is a healthcare and biomedical question-\nanswering (QA) dataset compiled from PubMed abstracts\nand articles. The objective of PubMedQA is to assess sys-\ntem capability in reasoning by answering medical research\nquestions with one of three possible responses: yes, no, or\n"}, {"page": 5, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n5\nmaybe (e.g., Do mitochondria play a role in remodeling lace\nplant leaves during programmed cell death?). The answer\ndepends on the corresponding article abstract. The benchmark\nleaderboard [67] is based on a 500-question expert-annotated\ntest set. PubMedQA also released a 1,000-question expert-\nannotated dataset named PQA-L(Labeled). Additionally, it\nprovides PQA-U(Unlabeled), a collection of 61.2K PubMed\narticles without labeled answers, and PQA-A(Artificial), a\ncollection of 211.3K PubMed articles that include artificially\ngenerated labeled answers.\nE. BIOASQ\nBioASQ is an annual large-scale competition in biomedical\nsemantic indexing and question answering (QA). It has been\nrunning since 2013, with a focus on assessing the capability of\nsystems to semantically index large-scale collections of medi-\ncal scientific articles and make them available for natural lan-\nguage QA systems, enabling them to return relevant answers\nto users. Two tasks were initially in scope. The first was the\nautomatic classification of biomedical documents using con-\ncepts from knowledge bases (e.g., Medical Subject Headings).\nThe second task involved answering biomedical questions by\nextracting text fragments from scientific articles [68]. The\ncompetition has expanded over the past decade and currently\ncovers diverse tasks such as biomedical text classification,\ninformation retrieval, machine learning, QA from texts and\nstructured data, and multi-document summarization [69]–[71].\nThere is no single standard leaderboard as in the Open Medical\nLeaderboard and MIRAGE; instead, there are detailed results\npages listing the competition outcomes for each year, along\nwith additional subcategories based on that year’s tasks [72].\nV. BEYOND LEADERBOARD AND BENCHMARKS\nIn the previous sections, we have introduced and exem-\nplified leaderboards and benchmarks for the assessment of\nLLMs in the healthcare domain. However, the main findings\nare independent of this domain and can be transferred and\ngeneralized to other domains:\nBenchmark Contamination: Dataset or benchmark contam-\nination occurs when LLMs unintentionally include evaluation\nbenchmark information in their training or fine-tuning data,\nleading to inaccurate, false, or unreliable results during model\nevaluation. A possible solution is to move toward private\nor gated benchmarks, or to use dynamic benchmarks with\ncontinuously refreshed content [73], [74].\nBenchmark Saturation: Some benchmarks are no longer\nchallenging and have become too easy, reaching baseline\nhuman performance. This has occurred in the Open LLM\nLeaderboard, leading to a refactoring of the leaderboard to\nreplace those benchmarks [75].\nBenchmark Selection Bias: Deciding which benchmarks to\ninclude can be subjective, as leaderboards may misrepresent\nthe practicality and usefulness of certain benchmarks for real-\nworld applications, especially in domains such as medicine.\nThe continuous participation of domain-specific subject matter\nexperts is therefore essential [76].\nTABLE IV\nTOP DOWNLOADED HUGGING FACE LLMS (SNAPSHOT: 30 SEP 2025).\nL\nModel\nDownloads\nLikes\nSize\nopenai-community/gpt2\n11,567,187\n2,958\n137M\nfacebook/opt-125m\n8,105,841\n218\n125M\nQwen/Qwen2.5-3B-\nInstruct\n8,073,896\n311\n3.0B\nmeta-llama/Llama-3.1-8B-\nInstruct\n7,192,139\n4,689\n8.0B\nQwen/Qwen2.5-7B-\nInstruct\n7,184,004\n806\n7.0B\nmeta-llama/Llama-3.2-1B-\nInstruct\n6,849,770\n1,087\n1.0B\nQwen/Qwen3-0.6B\n6,748,616\n652\n600M\nopenai/gpt-oss-20b\n6,637,334\n3,632\n20.0B\nQwen/Qwen3-32B\n5,660,241\n541\n32.0B\ngoogle/gemma-3-1b-it\n5,636,363\n633\n1.0B\nLegend:\nPermissive\nNon-comm.\nComputation and Maintenance Cost: Running an eval-\nuation for a model requires significant computational re-\nsources, which explains why current leaderboards are typically\nsupported by large organizations. Maintaining and keeping\nleaderboards relevant requires continuous updates to bench-\nmarks, evaluation methods, and infrastructure. Actively up-\ndated leaderboards consistently gain the community’s trust.\nEvaluation Metrics: Many leaderboards rely on one or two\nscores, such as accuracy, average performance, or efficiency\n(e.g., inference time in seconds), which may oversimplify\nmodel capabilities. A model might perform very well on\ncertain tasks but poorly on others, yet aggregation hides these\nweaknesses. Furthermore, automated statistical metrics such\nas BLEU, ROUGE, or exact match do not effectively measure\nreasoning quality, truthfulness, or usefulness of outputs. Some\nleaderboards have addressed this by using LLMs as judges;\nwhile widely adopted, this approach is still under investiga-\ntion [77]–[79].\nOverfitting in Arena-type Leaderboards: For leaderboards\nbased on Elo ratings and user votes, such as Chatbot Arena,\ncertain regulations are needed to prevent overfitting, as detailed\nin The Leaderboard Illusion [80]. Some of the recommenda-\ntions include: prohibiting score withdrawal after submission,\nsetting transparent limits on the number of private model\nvariants per provider, establishing clear and auditable model\ndeprecation criteria, and enhance sampling fairness.\nVI. COMMUNITY ENDORSEMENT\nBesides navigating leaderboards, there are other ways to\ngain insights into model performance. This can be done by\nobtaining insights from platforms like Hugging Face [81], the\nmost popular platform that provides access to thousands of\npre-trained models, datasets, and libraries. It enables devel-\nopers and researchers to build, share, and deploy AI models\neasily. Hugging Face provides filtering options for users to\nnavigate models and datasets, for example, sorting the models\nby top downloads, likes, or trending status. Downloads, for in-\nstance, can reflect how often a model has been pulled, offering\ninsights into its popularity and adoption. Thus, the higher the\nnumber of downloads, the stronger the indication that such\n"}, {"page": 6, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n6\nmodels are widely used and trusted by the community. It can\nsuggest that the model is well-documented, easy to integrate,\nor reliable across tasks. However, downloads can be inflated\nby automated pipelines or repeated pulls, so they may not\nalways measure user satisfaction or quality. Sorting models\nby likes in Hugging Face can also shed light on community\napproval and endorsement of a model. The more likes a model\nhas, the more indicative it is that users appreciate its quality,\nreliability, usefulness, ease of use, innovative architecture, or\ngood documentation. Unlike downloads, likes do not necessar-\nily indicate usage volume. A niche model could have fewer\ndownloads but a high like-to-download ratio if the people who\ntried it liked it a lot. Nevertheless, likes can also be subjective,\nsomeone could like a model for its documentation, speed,\nor even novelty, not purely for its accuracy or performance.\nTherefore, to build a more confident understanding of current\nmodels, it is better to consider likes along with downloads,\ntrends, and evaluation metrics on benchmarks before choosing\na model. We have extracted a snapshot (covering 30 days) from\nHugging Face to view the top downloaded and liked models\nfor the text-generation task, as shown in Table IV and Table V,\nrespectively, and we noticed the following:\n• The models that are highly downloaded and highly liked\nare mostly in the permissive license spectrum. We define\npermissive in this paper as a license that allows usage\nin both commercial and research settings (though some\nlicenses have detailed restrictions), such as Apache 2.0,\nMIT, Gemma, Llama 2, and Llama 3.\n• The models that are highly downloaded usually favor\nsmall models in terms of parameter size. For Table IV, the\nrange of models is mostly under 3B parameters, which is\nexpected, as such models are more affordable to try and\nexplore.\n• When comparing the content of Table IV versus Table V,\nthe models that are highly downloaded are not necessarily\nthe most liked models, which encourages maintaining\nmore options in the model selection decision.\n• The models that are highly downloaded and/or liked are\ndominated by the leading organizations in the field, which\nis expected given the cost of pretraining these models\nfrom scratch.\nThese insights can motivate the selection process, even if only\nfor one task (text generation), but they do not necessarily\nimply that the most downloaded or liked model is the best.\nThe process of selecting the right model for a particular use\ncase is sophisticated and customizable, depending on many\nother factors, as will be detailed in Section VII.\nVII. MODEL SELECTION METHODOLOGY (MSM)\nIt is clear that LLMs have become integral to everyday\nlife in recent years. Market research has anticipated rapid\ngrowth in the LLM sector. For instance, Grand View Research\nestimated the global market for LLMs at USD 5.61 billion in\n2024, with a projected compound annual growth rate (CAGR)\nof 36.9% from 2025 to 2030 [82], [83]. This underscores the\nincreasing importance of selecting the appropriate model, and\nhighlights the need for a more structured selection process\nTABLE V\nTOP HUGGING FACE LLMS BY USER LIKES (SNAPSHOT: 30 SEP 2025).\nL\nModel\nDownloads\nLikes\nSize\nmeta-llama/Meta-Llama-3-\n8B\n2,120,284\n6,332\n8.0B\nmeta-llama/Llama-3.1-8B-\nInstruct\n7,192,139\n4,689\n8.0B\nopenai/gpt-oss-120b\n3,572,640\n3,912\n120.0B\nopenai/gpt-oss-20b\n6,637,334\n3,632\n20.0B\nopenai-community/gpt2\n11,567,187\n2,958\n137M\nmeta-llama/Llama-2-7b-hf\n2,133,119\n2,158\n7.0B\nmeta-llama/Llama-3.2-1B\n2,293,832\n2,093\n1.0B\nmeta-llama/Llama-3.2-3B-\nInstruct\n1,832,773\n1,739\n3.0B\ndeepseek-ai/DeepSeek-R1-\nDistill-Qwen-32B\n3,207,520\n1,446\n32.0B\nTinyLlama/TinyLlama-\n1.1B-Chat-v1.0\n2,404,466\n1,416\n1.1B\nLegend:\nPermissive\nNon-comm.\nfor practitioners. While leaderboards and benchmarks provide\nuseful guidance in comparing models for research or business\napplications, other factors are equally important in narrowing\ndown choices. To address this, we present the MSM (Model\nSelection Methodology), an intuitive methodology for navigat-\ning different SOTA models and guiding the model decision-\nmaking process. MSM is organized into three stages, reflecting\ndifferent types of considerations, as illustrated in Figure 3. The\nearlier stages focus on critical requirements, while the later\nstages address important factors and contextual validation.\nAlthough MSM is represented as a three-stage pyramid, these\nstages should not be interpreted as strict levels of importance.\nMSM is inherently contextual, and some criteria may shift in\ncriticality or relevance depending on the specific use case.\nA. Stage 1: (Critical)\nThis stage defines the prerequisite conditions. If these\nconditions are not met, it will likely be necessary to build\nand pretrain a model from scratch or adjust the project\nrequirements significantly.\n1) Task Specification: The first step is to specify the objec-\ntive task (e.g., text generation, text-to-speech, image-to-text,\netc.). Hugging Face lists over 40 tasks, which can be filtered\nto identify relevant models as seen in Table VI. This step\nnarrows the scope to what is available and avoids considering\nirrelevant use cases.\n2) Model Size and Computational Cost: Model size directly\nimpacts training and inference costs, hardware requirements,\nand latency. Many institutions are promoting compact yet\nhigh-performing models. Model size is so central that it is\noften embedded in model names. Additionally, smaller models\nare generally more energy efficient and are downloaded more\nfrequently, reflecting practical usability [84]–[86].\n3) License and User Rights: Licensing defines whether\na model can be used commercially or only for research.\nIt also constrains whether modifications, redistribution, or\nintegration with proprietary systems are allowed. Well-known\nlicenses include MIT, Creative Commons, and Apache 2, while\nspecialized licenses such as LLaMA 2 and Gemma impose\n"}, {"page": 7, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n7\nTask Specification\nModel Size\nInstruction vs. Base\nModels\nLanguage Support\nLibrary & Ecosystem\nSupport\nModel Maintenance &\nUpdates\nAccess\nMethod\nCommunity\nSupport\nModel Architecture\nTransparency\nAvailability of\nCode Base\nPerformance on\nBenchmarks\nCommunity\nVotes\nLightweight &\nPortable Versions\nStage 1\nStage 2\nStage 3\nEarly\nPrototyping\nLicense & User Rights\nDomain Specialization\nFig. 3. Model Selection Methodology\nTABLE VI\nEXAMPLES OF TASKS LISTED IN HUGGINGFACE PLATFORM\nTask Category\nTasks\nMultimodal\nAudio-Text-to-Text • Image-Text-to-Text • Visual Question Answering • Document Question Answering\n• Video-Text-to-Text • Visual Document Retrieval • Any-to-Any\nComputer Vision\nDepth Estimation • Image Classification • Object Detection • Image Segmentation • Text-to-Image •\nImage-to-Text • Image-to-Image • Image-to-Video • Unconditional Image Generation • Video Classifica-\ntion • Text-to-Video • Zero-Shot Image Classification • Mask Generation • Zero-Shot Object Detection\n• Text-to-3D • Image-to-3D • Image Feature Extraction • Keypoint Detection • Video-to-Video\nNatural Language Processing\nText Classification • Token Classification • Table Question Answering • Question Answering • Zero-Shot\nClassification • Translation • Summarization • Feature Extraction • Text Generation • Fill-Mask • Sentence\nSimilarity • Text Ranking\nAudio\nText-to-Speech • Text-to-Audio • Automatic Speech Recognition • Audio-to-Audio • Audio Classification\n• Voice Activity Detection\nTabular\nTabular Classification • Tabular Regression • Time Series Forecasting\nReinforcement Learning\nReinforcement Learning • Robotics\nstricter conditions. This factor requires careful alignment with\nthe intended use case. Performing an assessment of bias,\nfairness, interpretability, and data privacy is also important.\nEven technically strong models may be unsuitable if they\nintroduce unacceptable risks or fail to meet compliance re-\nquirements [87]–[89].\nB. Stage 2: (Important)\nThis stage covers secondary yet important conditions. If\nthese conditions are unmet, they may not prevent model use\nentirely, but they will require additional adjustments, such as\nfine-tuning, adaptation, or further development to close the\ngaps.\n1) Domain Specialization: Certain applications demand\ndomain adaptation (e.g., biomedical, finance, telecom) [90].\nSeveral ready-made domain-adapted models are available. For\nexample, in the medical domain [91]–[93], selecting a medical\nfoundation model can considerably accelerate the development\nof medical use cases compared to training from a general-\npurpose model [94].\n2) Instruction vs. Base Models: Models are often released\nin two variants: base and instruction-tuned. Base models serve\nas general-purpose backbones but usually require fine-tuning\nor prompting strategies to perform well on downstream tasks.\nInstruction-tuned models, on the other hand, are aligned to fol-\nlow natural language instructions, enabling more effective out\nof the box use in many applications. Selecting the appropriate\nvariant can substantially reduce adaptation effort and training\ncost, depending on the use case.\n3) Language Support: LLM models usually come in a\nmultilingual setting covering several languages. Though En-\nglish and many other languages are widely adopted in current\nmodels, the lack of certain languages relevant to the target use\ncase or project may require costly pretraining. For instance,\nit may take a reasonable amount of time to pretrain a small\nmodel in a new language with appropriate hardware, whereas\nlarger models require a far greater investment. Additionally, to\nadapt the model to the missing language without compromis-\ning support for currently supported languages, data preparation\nmust meet adequate quality and quantity standards [95]–[97].\n4) Library and Ecosystem Support: Modern model devel-\nopment relies on ecosystems (e.g., Hugging Face, DeepSpeed,\nPyTorch Lightning). Models with broader library support\nenable smoother training, optimization, distributed computing,\nand deployment pipelines. Ecosystem integration also includes\nsupport for ONNX, TensorRT, or cloud deployment, which can\nsignificantly reduce engineering overhead.\n5) Model Maintenance and Updates: The long-term relia-\nbility of a model depends on whether it is actively maintained,\npatched, and updated. A language model is not just a large blob\nof weights; it is part of a broader software stack including\nframeworks, loaders, and deployment infrastructure that must\nremain secure. Libraries or frameworks (e.g., PyTorch, Trans-\nformers) that are not regularly updated may contain vulnera-\nbilities that attackers can exploit [98]. Models can also pose\n"}, {"page": 8, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n8\nrisks related to data privacy violations, as outdated models\nmay retain or unintentionally reveal sensitive training data if\nthey were not fine-tuned, filtered, or audited under modern\nprivacy and compliance standards (e.g., GDPR, HIPAA) [99],\n[100]. The research community is actively studying emerging\nprivacy and security flaws that may affect language models,\nsuch as model poisoning [101], prompt injection [102], [103],\nand other evolving safety concerns [104]. These developments\nreinforce the importance of selecting models that are actively\nmaintained to ensure a more stable, secure, and compliant\ndeployment ecosystem.\nC. Stage 3: (Contextual Validation)\nThis stage involves contextual validation activities that\ndo not necessarily disqualify a model but provide valuable\ninsights for more informed decision-making. It is positioned\nlast not because it is optional, but because it can only be\nmeaningfully performed after foundational constraints and\nselection factors have been addressed. For example, this stage\nincorporates Early Prototyping, an important step that eval-\nuates shortlisted models in the actual application setting to\nprovide concrete, evidence-driven feedback.\n1) Access Method: Models may be available via direct\ndownload (requiring dedicated hardware) or API access (sub-\nject to usage limits, costs, and latency). The choice of access\nmethod impacts scalability and planning for throughput and\ntoken consumption.\n2) Community Support: An active online community can\naccelerate adoption through shared experiences, tutorials, and\nopen discussions. This improves troubleshooting and acceler-\nates innovation.\n3) Model Architecture Transparency: Availability of the\nmodel’s technical reports, research papers, documentation,\ntraining datasets, and evaluation methods provides insight into\nits performance and potential limitations. Transparency also\naids in fostering innovation and adaptation\n4) Availability of Code Base: Access to the source code\nfor model architectures or training pipelines enhances repro-\nducibility and innovation. In addition, examining repository\nmetadata for instance, on GitHub this could include the\nnumber of stars, watch, forks, contributors, recent\ncode commits, or active discussions, all can serve as informal\ntrust indicators of how active and well-maintained the code\nbase is.\n5) Performance on Benchmarks: Benchmark results and\nleaderboard rankings remain key indicators of model strength.\nThey help researchers identify models excelling in specific\ntasks and provide data formats for reproducibility. However,\nreliance only on benchmarks may overlook robustness and\nreal-world generalization.\n6) Community votes: Community usage patterns, such as\nmodel downloads, likes and trends on platforms like Hugging\nFace can serve as vote for trust for some models. They\nhighlight widely adopted and practically useful models.\n7) Lightweight and Portable Versions: Availability of op-\ntimized forms like distilled or quantized model or availability\nof model exported in ONNX format facilitates deployment on\nresource-constrained hardware. This extends accessibility and\nexploring without requiring full-scale infrastructure.\n8) Early prototyping: Examining the shortlisted models is\nimportant, as by this stage most of the other points to address\nhave already been covered. By examining the model’s behavior\naccording to your use case and using your dataset, you can\nbuild a more intuitive understanding of how well the model can\nbe adapted and how far it currently is from the target behavior.\nThis stage can also be beneficial for assessing the model’s\nreadiness within the current use case or project infrastructure\nand for addressing potential issues early on.\nVIII. DISCUSSION\nSelecting the right model for a specific task is a multidimen-\nsional decision-making process. As discussed earlier, although\npublic leaderboards and benchmarks offer accessible, quantita-\ntive indicators of model performance, they do not capture the\nfull complexity involved in real-world adoption. Benchmark\nresults are often narrow in scope, limited to particular datasets\nor tasks, and may not reflect constraints related to use-case\nrequirements, cost, latency, security, privacy, maintainability,\nor deployment infrastructure. As a result, relying solely on\nleaderboard or benchmark performance is still insightful, yet\nincomplete for informed model selection, especially in high-\nstakes or resource-constrained environments. Historically, the\ndata science and machine learning community has consistently\naddressed similar forms of complexity by developing stan-\ndards and structured methodologies that transform unstruc-\ntured, ad hoc processes into well-defined, repeatable practices.\nEarly efforts such as CRISP-DM (Cross-Industry Standard\nProcess for Data Mining) [105] provided the first widely\nadopted framework for organizing the data-mining lifecycle\nby proposing six major stages: Business Understanding, Data\nUnderstanding, Data Preparation, Modeling, Evaluation, and\nDeployment. This structure enabled practitioners to translate\nexperience into a systematic process that became the de\nfacto standard for data mining. SEMMA, introduced by SAS,\nfollowed a similar spirit by formalizing its iterative steps:\nSample, Explore, Modify, Model, and Assess into a repeatable\nworkflow [106]. These initiatives demonstrated the value of\ncodifying practitioner intuition into structured methodologies\nthat could guide teams, increase productivity, and reduce\nambiguity. As the field evolved, principles like FAIR (Find-\nable, Accessible, Interoperable, Reusable) [107] reframed data\nmanagement around transparency and long-term usability.\nSimilarly, the DAMA-DMBOK framework established com-\nprehensive best practices for data governance, reflecting the\ngrowing need to standardize organization-wide data proce-\ndures [108], [109]. Just as data mining and data management\nmatured through frameworks such as CRISP-DM and DAMA-\nDMBOK, the discipline of project management is another\nexample that underwent a similar transformation. Historically,\nproject execution differed dramatically across industries and\norganizations, often relying on individual experience rather\nthan shared structure. The emergence of project-management\nstandards such as PMBOK [110], PRINCE2 [111], and Agile\nmethodologies [112] illustrated the necessity of codifying best\n"}, {"page": 9, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n9\npractices into transparent, repeatable processes. More recently,\nthe development of MLOps frameworks [113] and Model\nCards [1] has further pushed the community toward explicit\ndocumentation, governance, and operational consistency for\nmachine learning systems. These contributions illustrate a con-\ntinuous ideology: when the complexity of data and models in-\ncreases, the community responds by creating frameworks that\nformalize shared understanding into actionable standards. The\nsuccess of all these frameworks were not only from process\nstandardization but also from their ability to pin down a shared\nterminology that practitioners could adopt. Establishing such\ncommon language is equally essential in the rapidly evolving\nLLM ecosystem. In the context of large language models,\nthe landscape remains highly agile and fragmented. New\nmodels appear frequently, architectural innovations evolve\nrapidly, and performance depends on operational factors that\nextend far beyond accuracy metrics. Despite this, there is\nno unified process for navigating the trade-offs involved in\nLLM model selection. This gap has left practitioners without\na structured methodology for evaluating models in a holistic\nand repeatable manner. The Model Selection Methodology\n(MSM) proposed in this work aims to serve as an early step\ntoward standardizing the model selection process for language\nmodels. MSM codifies common-sense reasoning (currently\npracticed informally by engineers and researchers) into a clear,\nstructured workflow that prioritizes requirements, constraints,\nand context-specific trade-offs. While MSM is not presented as\na definitive or exhaustive standard, it provides a foundational\nblueprint that can be extended, refined, and validated by the\nbroader community. As prior frameworks in data science have\nshown, early conceptual methodologies often act as seeds\nthat mature into widely accepted standards through iterative\nresearch and practical adoption.\nIX. CONCLUSION\nIn this work, we focused on exploring the landscape of\nleaderboards in general and, in particular, examined the health-\ncare and medicine domain as a case study. By delving deeper\ninto this domain, we identified several challenges associated\nwith relying solely on leaderboards when selecting a model for\na specific use case. We also discussed the general limitations\nof leaderboards and proposed possible solutions for some\nof these challenges. We acknowledge that leaderboards still\nrepresent a valuable approach, providing a quick quantitative\ndimension for a large number of models, as they make it\neasier to compare various models using the same framework.\nNevertheless, leaderboards should not be the only method for\nmodel selection. Therefore, we also focused on examining\nother factors that reflect the community’s preferences and\ntrust toward different models. Based on these insights, we\nderived the Model Selection Methodology (MSM), a method\nthat provides a step-by-step approach for navigating, sorting,\nand shortlisting models for a particular use case or domain.\nWe hope that this methodology can serve as a step toward\nestablishing a more systematic and accelerated model selection\nprocess.\nACKNOWLEDGMENTS\nThis\nwork\nwas\npartly\nsupported\nby\nthe\nFreiwillige\nAkademische Gesellschaft (FAG)0 Basel.\nREFERENCES\n[1] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchin-\nson, E. Spitzer, I. D. Raji, and T. Gebru, “Model cards for model\nreporting,” in Proceedings of the conference on fairness, accountability,\nand transparency, 2019, pp. 220–229.\n[2] B. Yang, F. Cui, K. Paster, J. Ba, P. Vaezipoor, S. Pitis, and M. R.\nZhang, “Report cards: Qualitative evaluation of language models using\nnatural language summaries,” arXiv preprint arXiv:2409.00844, 2024.\n[3] V. Murahari, A. Deshpande, P. Clark, T. Rajpurohit, A. Sabharwal,\nK. Narasimhan, and A. Kalyan, “Qualeval: Qualitative evaluation for\nmodel improvement,” arXiv preprint arXiv:2311.02807, 2023.\n[4] huggingface main page. Last accessed: July 28, 2025. [Online].\nAvailable: https://huggingface.co/\n[5] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\nand O. Tafjord, “Think you have solved question answering? try arc,\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457, 2018.\n[6] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\nJ. Steinhardt, “Measuring massive multitask language understanding,”\narXiv preprint arXiv:2009.03300, 2020.\n[7] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hel-\nlaswag: Can a machine really finish your sentence?” arXiv preprint\narXiv:1905.07830, 2019.\n[8] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models\nmimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021.\n[9] HELM. Last accessed: July 28, 2025. [Online]. Available: https:\n//crfm.stanford.edu/helm/\n[10] HELM Safety. Last accessed: July 28, 2025. [Online]. Available:\nhttps://crfm.stanford.edu/helm/safety/latest/\n[11] Vision HELM. Last accessed: July 28, 2025. [Online]. Available:\nhttps://crfm.stanford.edu/helm/vhelm/latest/\n[12] Med HELM. Last accessed: July 28, 2025. [Online]. Available:\nhttps://crfm.stanford.edu/helm/medhelm/latest/\n[13] Large Model Systems. Last accessed: July 28, 2025. [Online].\nAvailable: https://lmsys.org/about/\n[14] Elo rating system. Last accessed: November 25, 2025. [Online].\nAvailable: https://en.wikipedia.org/wiki/Elo rating system\n[15] TTS\nAGI.\nLast\naccessed:\nJuly\n28,\n2025.\n[Online].\nAvailable:\nhttps://huggingface.co/spaces/TTS-AGI/TTS-Arena-V2\n[16] Open ASR Leaderboard. Last accessed: July 28, 2025. [Online]. Avail-\nable: https://huggingface.co/spaces/hf-audio/open asr leaderboard\n[17] TTS\nAGI.\nLast\naccessed:\nJuly\n28,\n2025.\n[Online].\nAvailable:\nhttps://huggingface.co/spaces/opencompass/open vlm leaderboard\n[18] F. Liu, Z. Li, H. Zhou, Q. Yin, J. Yang, X. Tang, C. Luo, M. Zeng,\nH. Jiang, Y. Gao et al., “Large language models in the clinic: a\ncomprehensive benchmark,” arXiv preprint arXiv:2405.00716, 2024.\n[19] S. Bedi, H. Cui, M. Fuentes, A. Unell, M. Wornow, J. M. Banda,\nN. Kotecha, T. Keyes, Y. Mai, M. Oez et al., “Medhelm: Holistic\nevaluation of large language models for medical tasks,” arXiv preprint\narXiv:2505.23802, 2025.\n[20] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits,\n“What disease does this patient have? a large-scale open domain\nquestion answering dataset from medical exams,” Applied Sciences,\nvol. 11, no. 14, p. 6421, 2021.\n[21] A. Pal, L. K. Umapathi, and M. Sankarasubbu, “Medmcqa: A large-\nscale multi-subject multi-choice dataset for medical domain question\nanswering,” in Conference on health, inference, and learning.\nPMLR,\n2022, pp. 248–260.\n[22] A. B. Abacha, E. Agichtein, Y. Pinter, and D. Demner-Fushman,\n“Overview of the medical question answering task at trec 2017 liveqa.”\nin TREC, 2017, pp. 1–12.\n[23] A. B. Abacha, Y. Mrabet, M. Sharp, T. R. Goodwin, S. E. Shooshan,\nand D. Demner-Fushman, “Bridging the gap between consumers’\nmedication questions and trusted answers,” in MEDINFO 2019: Health\nand Wellbeing e-Networks for All.\nIOS Press, 2019, pp. 25–29.\n0https://fag-basel.ch/\n"}, {"page": 10, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n10\n[24] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, “Pubmedqa: A\ndataset for biomedical research question answering,” in Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019.\nHong Kong, China:\nAssociation for Computational Linguistics, Nov. 2019, pp. 2567–2577.\n[Online]. Available: https://doi.org/10.18653/v1/D19-1259\n[25] A. Fansi Tchango, R. Goel, Z. Wen, J. Martel, and J. Ghosn, “Ddxplus:\nA new dataset for automatic medical diagnosis,” Advances in neural\ninformation processing systems, vol. 35, pp. 31 306–31 318, 2022.\n[26] J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman, “A\ndataset of clinically generated visual questions and answers about\nradiology images,” Scientific data, vol. 5, no. 1, pp. 1–10, 2018.\n[27] A. B. Abacha and D. Demner-Fushman, “On the summarization of\nconsumer health questions,” in Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics, 2019, pp. 2228–2234.\n[28] W.-w. Yim, Y. Fu, A. Ben Abacha, N. Snider, T. Lin, and M. Yetisgen,\n“Aci-bench: a novel ambient clinical intelligence dataset for bench-\nmarking automatic visit note generation,” Scientific data, vol. 10, no. 1,\np. 586, 2023.\n[29] Y. Li, S. Wu, C. Smith, T. Lo, and B. Liu, “Improving clinical note\ngeneration from complex doctor-patient conversation,” in Pacific-Asia\nConference on Knowledge Discovery and Data Mining.\nSpringer,\n2025, pp. 209–221.\n[30] J. Li, Y. Sun, R. J. Johnson, D. Sciaky, C.-H. Wei, R. Leaman, A. P.\nDavis, C. J. Mattingly, T. C. Wiegers, and Z. Lu, “Biocreative v cdr task\ncorpus: a resource for chemical disease relation extraction,” Database,\nvol. 2016, 2016.\n[31]\n`A. Bravo, J. Pi˜nero, N. Queralt-Rosinach, M. Rautschka, and L. I.\nFurlong, “Extraction of relations between genes and diseases from text\nand large-scale data analysis: implications for translational research,”\nBMC bioinformatics, vol. 16, no. 1, p. 55, 2015.\n[32] (2023) Trec clinical trials track. The Text Retrieval Conference.\n[Online]. Available: https://www.trec-cds.org/2023.html\n[33] C. Meng, L. Trinh, N. Xu, J. Enouen, and Y. Liu, “Interpretability\nand fairness evaluation of deep learning models on mimic-iv dataset,”\nScientific Reports, vol. 12, no. 1, p. 7166, 2022.\n[34] S. R. Pfohl, H. Cole-Lewis, R. Sayres, D. Neal, M. Asiedu, A. Dieng,\nN. Tomasev, Q. M. Rashid, S. Azizi, N. Rostamzadeh et al., “A toolbox\nfor surfacing health equity harms and biases in large language models,”\nNature Medicine, vol. 30, no. 12, pp. 3590–3600, 2024.\n[35] T. Han, A. Kumar, C. Agarwal, and H. Lakkaraju, “Medsafetybench:\nEvaluating and improving the medical safety of large language models,”\nAdvances in Neural Information Processing Systems, vol. 37, pp.\n33 423–33 454, 2024.\n[36] Open Medical LLM leaderboard. Last accessed: July 13, 2025.\n[Online].\nAvailable:\nhttps://huggingface.co/spaces/openlifescienceai/\nopen\\ medical\\ llm\\ leaderboard\n[37] Open\nMedical\nLLM\nleaderboard\nBlog.\nLast\naccessed:\nJuly\n13,\n2025.\n[Online].\nAvailable:\nhttps://huggingface.co/blog/\nleaderboard-medicalllm\n[38] G. Xiong, Q. Jin, Z. Lu, and A. Zhang, “Benchmarking retrieval-\naugmented generation for medicine,” in Findings of the Association\nfor Computational Linguistics ACL 2024, 2024, pp. 6233–6251.\n[39] MIRAGE Leaderboard. Last accessed: June 5, 2025. [Online].\nAvailable: https://teddy-xionggz.github.io/MIRAGE/\n[40] Medical Information Retrieval-Augmented Generation Evaluation.\nLast accessed: June 5, 2025. [Online]. Available: https://github.com/\nTeddy-XiongGZ/MIRAGE\n[41] Retrieval-augmented\ngeneration.\nLast\naccessed:\nNovember\n25,\n2025.\n[Online].\nAvailable:\nhttps://en.wikipedia.org/wiki/\nRetrieval-augmented generation\n[42] MedRAG Toolkit. Last accessed: June 5, 2025. [Online]. Available:\nhttps://github.com/Teddy-XiongGZ/MedRAG\n[43] Retrieval-augmented generation. Last accessed: November 25, 2025.\n[Online]. Available: https://github.com/castorini/pyserini\n[44] S. Robertson, H. Zaragoza et al., “The probabilistic relevance frame-\nwork: Bm25 and beyond,” Foundations and Trends® in Information\nRetrieval, vol. 3, no. 4, pp. 333–389, 2009.\n[45] Contriever. Last accessed: November 25, 2025. [Online]. Available:\nhttps://huggingface.co/facebook/contriever\n[46] G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin,\nand E. Grave, “Unsupervised dense information retrieval with con-\ntrastive learning,” arXiv preprint arXiv:2112.09118, 2021.\n[47] Specter. Last accessed: November 25, 2025. [Online]. Available:\nhttps://huggingface.co/allenai/specter\n[48] A. Cohan, S. Feldman, I. Beltagy, D. Downey, and D. S. Weld,\n“Specter:\nDocument-level\nrepresentation\nlearning\nusing\ncitation-\ninformed transformers,” arXiv preprint arXiv:2004.07180, 2020.\n[49] MedCPT Query Encoder. Last accessed: November 25, 2025. [Online].\nAvailable: https://huggingface.co/ncbi/MedCPT-Query-Encoder\n[50] Q. Jin, W. Kim, Q. Chen, D. C. Comeau, L. Yeganova, W. J.\nWilbur, and Z. Lu, “Medcpt: Contrastive pre-trained transformers with\nlarge-scale pubmed search logs for zero-shot biomedical information\nretrieval,” Bioinformatics, vol. 39, no. 11, p. btad651, 2023.\n[51] PubMed. Last accessed: November 25, 2025. [Online]. Available:\nhttps://pubmed.ncbi.nlm.nih.gov/\n[52] Statpearls. Last accessed: November 25, 2025. [Online]. Available:\nhttps://www.statpearls.com/\n[53] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits,\n“What disease does this patient have? a large-scale open domain\nquestion answering dataset from medical exams,” Applied Sciences,\nvol. 11, no. 14, p. 6421, 2021.\n[54] ClinicBench Github. Last accessed: June 5, 2025. [Online]. Available:\nhttps://github.com/AI-in-Health/ClinicBench\n[55] ClinicBench Leaderboard. Last accessed: June 5, 2025. [Online]. Avail-\nable: https://huggingface.co/spaces/fenglinliu/medical llm leaderboard\n[56] P. Qiu, C. Wu, X. Zhang, W. Lin, H. Wang, Y. Zhang, Y. Wang, and\nW. Xie, “Towards building multilingual language model for medicine,”\nNature Communications, vol. 15, no. 1, p. 8384, 2024.\n[57] MultilingualMedQA\nLeaderboard.\nLast\naccessed:\nAug\n8,\n2025.\n[Online]. Available: https://henrychur.github.io/MultilingualMedQA/\n[58] R. K. Arora, J. Wei, R. S. Hicks, P. Bowman, J. Qui˜nonero-Candela,\nF. Tsimpourlas, M. Sharman, M. Shah, A. Vallone, A. Beutel et al.,\n“Healthbench: Evaluating large language models towards improved\nhuman health,” arXiv preprint arXiv:2505.08775, 2025.\n[59] H. Nori, Y. T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King,\nJ. Larson, Y. Li, W. Liu et al., “Can generalist foundation models\noutcompete special-purpose tuning? case study in medicine,” arXiv\npreprint arXiv:2311.16452, 2023.\n[60] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz,\n“Capabilities of gpt-4 on medical challenge problems,” arXiv preprint\narXiv:2303.13375, 2023.\n[61] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, M. Amin,\nL. Hou, K. Clark, S. R. Pfohl, H. Cole-Lewis et al., “Toward expert-\nlevel medical question answering with large language models,” Nature\nMedicine, vol. 31, no. 3, pp. 943–950, 2025.\n[62] “The claude 3 model family: Opus, sonnet, haiku.” [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:268232499\n[63] Z. Chen, A. H. Cano, A. Romanou, A. Bonnet, K. Matoba, F. Salvi,\nM. Pagliardini, S. Fan, A. K¨opf, A. Mohtashami et al., “Meditron-70b:\nScaling medical pretraining for large language models,” arXiv preprint\narXiv:2311.16079, 2023.\n[64] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor\nconduct electricity? a new dataset for open book question answering,”\narXiv preprint arXiv:1809.02789, 2018.\n[65] MMLU Github. Last accessed: July 13, 2025. [Online]. Available:\nhttps://github.com/hendrycks/test\n[66] MedMCQA Github. Last accessed: July 13, 2025. [Online]. Available:\nhttps://medmcqa.github.io/\n[67] PubMedQA Leaderboard. Last accessed: July 14, 2025. [Online].\nAvailable: https://pubmedqa.github.io/\n[68] G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas, M. Zschunke,\nM. R. Alvers, D. Weissenborn, A. Krithara, S. Petridis, D. Poly-\nchronopoulos et al., “An overview of the bioasq large-scale biomedical\nsemantic indexing and question answering competition,” BMC bioin-\nformatics, vol. 16, no. 1, p. 138, 2015.\n[69] A. Nentidis, G. Katsimpras, A. Krithara, S. Lima L´opez, E. Farr´e-\nMaduell, L. Gasco, M. Krallinger, and G. Paliouras, “Overview of\nbioasq 2023: The eleventh bioasq challenge on large-scale biomedical\nsemantic indexing and question answering,” in International Confer-\nence of the Cross-Language Evaluation Forum for European Lan-\nguages.\nSpringer, 2023, pp. 227–250.\n[70] A.\nNentidis,\nA.\nKrithara,\nK.\nBougiatiotis,\nG.\nPaliouras,\nand\nI. Kakadiaris, “Results of the sixth edition of the BioASQ challenge,”\nin Proceedings of the 6th BioASQ Workshop A challenge on\nlarge-scale biomedical semantic indexing and question answering,\nI. A. Kakadiaris, G. Paliouras, and A. Krithara, Eds.\nBrussels,\nBelgium: Association for Computational Linguistics, Nov. 2018, pp.\n1–10. [Online]. Available: https://aclanthology.org/W18-5301/\n[71] A. Nentidis, G. Katsimpras, E. Vandorou, A. Krithara, L. Gasco,\nM. Krallinger, and G. Paliouras, “Overview of bioasq 2021: The ninth\nbioasq challenge on large-scale biomedical semantic indexing and\n"}, {"page": 11, "text": "SAYED & SCHULDT: MIRROR, MIRROR ON THE WALL – WHICH IS THE BEST MODEL OF THEM ALL?\n11\nquestion answering,” in International conference of the cross-language\nevaluation forum for European languages.\nSpringer, 2021, pp. 239–\n263.\n[72] BioASQ Participants Area. Last accessed: July 13, 2025. [Online].\nAvailable: https://participants-area.bioasq.org/results/\n[73] C. Xu, S. Guan, D. Greene, M. Kechadi et al., “Benchmark data\ncontamination of large language models: A survey,” arXiv preprint\narXiv:2406.04244, 2024.\n[74] M. N. Hasan, M. F. Babar, S. Sarkar, M. Hasan, and S. Karmaker,\n“Pitfalls of evaluating language models with open benchmarks,” arXiv\npreprint arXiv:2507.00460, 2025.\n[75] open LLM leaderboard. Last accessed: July 28, 2025. [Online].\nAvailable: https://huggingface.co/spaces/open-llm-leaderboard/blog\n[76] M. Dehghani, Y. Tay, A. A. Gritsenko, Z. Zhao, N. Houlsby, F. Diaz,\nD. Metzler, and O. Vinyals, “The benchmark lottery,” arXiv preprint\narXiv:2107.07002, 2021.\n[77] X. Chen, J. Xiang, S. Lu, Y. Liu, M. He, and D. Shi, “Evaluating\nlarge language models in medical applications: a survey,” arXiv preprint\narXiv:2405.07468, 2024.\n[78] W. Wang, Z. Ma, Z. Wang, C. Wu, J. Ji, W. Chen, X. Li, and Y. Yuan,\n“A survey of llm-based agents in medicine: How far are we from\nbaymax?” arXiv preprint arXiv:2502.11211, 2025.\n[79] N. Alzahrani, H. Alyahya, Y. Alnumay, S. Alrashed, S. Alsubaie,\nY. Almushayqih, F. Mirza, N. Alotaibi, N. Al-Twairesh, A. Alowisheq\net al., “When benchmarks are targets: Revealing the sensitivity of\nlarge language model leaderboards,” in Proceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), 2024, pp. 13 787–13 805.\n[80] S. Singh, Y. Nan, A. Wang, D. D’Souza, S. Kapoor, A.\n¨Ust¨un,\nS. Koyejo, Y. Deng, S. Longpre, N. A. Smith et al., “The leaderboard\nillusion,” arXiv preprint arXiv:2504.20879, 2025.\n[81] huggingface wikipedia. Last accessed: July 28, 2025. [Online].\nAvailable: https://en.wikipedia.org/wiki/Hugging Face\n[82] Large Language Models Market (2025 - 2030). Last accessed: Sept\n29, 2025. [Online]. Available: https://www.grandviewresearch.com/\nindustry-analysis/large-language-model-llm-market-report\n[83] L. Limonad, F. Fournier, H. Mulian, G. Manias, S. Borotis, and\nD. Kyrkou, “Selecting the right llm for egov explanations,” arXiv\npreprint arXiv:2504.21032, 2025.\n[84] L. Chen and G. Varoquaux, “What is the role of small models in the\nllm era: A survey,” arXiv preprint arXiv:2409.06857, 2024.\n[85] M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach,\nA. Bahree, A. Bakhtiari et al., “Phi-3 technical report: A highly capable\nlanguage model locally on your phone,” CoRR, vol. abs/2404.14219,\n2024. [Online]. Available: https://doi.org/10.48550/arXiv.2404.14219\n[86] Microsoft,\n“Phi,”\n2024.\n[Online].\nAvail-\nable:\nhttps://azure.microsoft.com/en-us/blog/\nintroducing-phi-3-redefining-whats-possible-with-slms/\n[87] H.\nModels,\n“Huggingface\nmodels,”\n2025.\n[Online].\nAvailable:\nhttps://huggingface.co/models\n[88] V. Licenses, “Various licenses and comments about them,” 2025.\n[Online]. Available: https://www.gnu.org/licenses/license-list.en.html\n[89] O. Licenses, “Osi approved licenses,” 2025. [Online]. Available:\nhttps://opensource.org/licenses\n[90] X. ZHAO, J. LU, C. DENG, C. ZHENG, J. WANG, T. CHOWDHURY,\nL. YUN, H. CUI, Z. XUCHAO, T. Zhao et al., “Beyond one-model-\nfits-all: A survey of domain specialization for large language models,”\narXiv preprint arXiv, vol. 2305, 2023.\n[91] C. Wu, W. Lin, X. Zhang, Y. Zhang, W. Xie, and Y. Wang, “Pmc-llama:\ntoward building open-source language models for medicine,” Journal\nof the American Medical Informatics Association, vol. 31, no. 9, pp.\n1833–1843, 2024.\n[92] Y. Labrak, A. Bazoge, E. Morin, P.-A. Gourraud, M. Rouvier, and\nR. Dufour, “Biomistral: A collection of open-source pretrained large\nlanguage models for medical domains,” in Findings of the Association\nfor Computational Linguistics, ACL 2024.\nBangkok, Thailand and\nvirtual meeting: Association for Computational Linguistics, Aug.\n2024, pp. 5848–5864. [Online]. Available: https://doi.org/10.18653/v1/\n2024.findings-acl.348\n[93] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,\n“Biobert: a pre-trained biomedical language representation model for\nbiomedical text mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–1240,\n2020.\n[94] A. S. Tejani, Y. S. Ng, Y. Xi, J. R. Fielding, T. G. Browning, and J. C.\nRayan, “Performance of multiple pretrained bert models to automate\nand accelerate data annotation for large datasets,” Radiology: Artificial\nIntelligence, vol. 4, no. 4, p. e220007, 2022.\n[95] Z.-X. Yong, B. Ermis, M. Fadaee, S. H. Bach, and J. Kreutzer, “The\nstate of multilingual llm safety research: From measuring the language\ngap to mitigating it,” arXiv preprint arXiv:2505.24119, 2025.\n[96] X.-P. Nguyen, S. M. Aljunied, S. Joty, and L. Bing, “Democra-\ntizing llms for low-resource languages by leveraging their english\ndominant abilities with linguistically-diverse prompts,” arXiv preprint\narXiv:2306.11372, 2023.\n[97] A. Kiulian, A. Polishko, M. Khandoga, Y. Kostiuk, G. Gabrielli,\nŁ. Gagała, F. Zaraket, Q. A. Obaida, H. Garud, W. W. Y. Mak et al.,\n“From english-centric to effective bilingual: Llms with custom tokeniz-\ners for underrepresented languages,” arXiv preprint arXiv:2410.18836,\n2024.\n[98] N. S. Harzevili, J. Shin, J. Wang, S. Wang, and N. Nagappan,\n“Characterizing and understanding software security vulnerabilities in\nmachine learning libraries,” in 2023 IEEE/ACM 20th International\nConference on Mining Software Repositories (MSR).\nIEEE, 2023,\npp. 27–38.\n[99] General Data Protection Regulation (GDPR). Last accessed: November\n19, 2025. [Online]. Available: https://gdpr-info.eu/\n[100] Health Information Privacy. Last accessed: November 19, 2025.\n[Online]. Available: https://www.hhs.gov/hipaa/index.html\n[101] A. Souly, J. Rando, E. Chapman, X. Davies, B. Hasircioglu, E. Shereen,\nC. Mougan, V. Mavroudis, E. Jones, C. Hicks et al., “Poisoning attacks\non llms require a near-constant number of poison samples,” arXiv\npreprint arXiv:2510.07192, 2025.\n[102] X. Liu, Z. Yu, Y. Zhang, N. Zhang, and C. Xiao, “Automatic and\nuniversal prompt injection attacks against large language models,”\narXiv preprint arXiv:2403.04957, 2024.\n[103] J. Yan, V. Yadav, S. Li, L. Chen, Z. Tang, H. Wang, V. Srinivasan,\nX. Ren, and H. Jin, “Backdooring instruction-tuned large language\nmodels with virtual prompt injection,” in Proceedings of the 2024\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (Volume 1:\nLong Papers), 2024, pp. 6065–6086.\n[104] D. Shi, T. Shen, Y. Huang, Z. Li, Y. Leng, R. Jin, C. Liu, X. Wu,\nZ. Guo, L. Yu et al., “Large language model safety: A holistic survey,”\narXiv preprint arXiv:2412.17686, 2024.\n[105] C. Shearer, “The crisp-dm model: the new blueprint for data mining,”\nJournal of data warehousing, vol. 5, no. 4, pp. 13–22, 2000.\n[106] A. Azevedo, M. F. Santos et al., “Kdd, semma and crisp-dm: a parallel\noverview.” in IADIS European Conf. Data Mining, vol. 8, 2008, pp.\n182–185.\n[107] M. D. Wilkinson, M. Dumontier, I. J. Aalbersberg, G. Appleton,\nM. Axton, A. Baak, N. Blomberg, J.-W. Boiten, L. B. da Silva Santos,\nP. E. Bourne et al., “The fair guiding principles for scientific data\nmanagement and stewardship,” Scientific data, vol. 3, no. 1, pp. 1–9,\n2016.\n[108] D. International, DAMA-DMBOK: Data management body of knowl-\nedge.\nTechnics Publications, LLC, 2017.\n[109] Data Management Association (DAMA). Last accessed: November\n19,\n2025.\n[Online].\nAvailable:\nhttps://en.wikipedia.org/wiki/Data\nManagement Association\n[110] P. M. Institute, “A guide to the project management body of knowledge\n(pmbok® guide)–seventh edition and the standard for project manage-\nment.”\nProject Management Institute, 2021.\n[111] PRINCE2 (PRojects IN Controlled Environments). Last accessed:\nNovember 19, 2025. [Online]. Available: https://en.wikipedia.org/wiki/\nPRINCE2\n[112] P. Abrahamsson, O. Salo, J. Ronkainen, and J. Warsta, “Agile soft-\nware development methods: Review and analysis,” arXiv preprint\narXiv:1709.08439, 2017.\n[113] D. Kreuzberger, N. K¨uhl, and S. Hirschl, “Machine learning operations\n(mlops): Overview, definition, and architecture,” IEEE access, vol. 11,\npp. 31 866–31 879, 2023.\n"}]}