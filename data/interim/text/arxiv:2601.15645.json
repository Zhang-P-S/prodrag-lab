{"doc_id": "arxiv:2601.15645", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.15645.pdf", "meta": {"doc_id": "arxiv:2601.15645", "source": "arxiv", "arxiv_id": "2601.15645", "title": "Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation", "authors": ["Zhiyao Ren", "Yibing Zhan", "Siyuan Liang", "Guozheng Ma", "Baosheng Yu", "Dacheng Tao"], "published": "2026-01-22T04:51:39Z", "updated": "2026-01-22T04:51:39Z", "summary": "Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.15645v1", "url_pdf": "https://arxiv.org/pdf/2601.15645.pdf", "meta_path": "data/raw/arxiv/meta/2601.15645.json", "sha256": "1cfeed863c4434e4e8039109c9b5111450e8e6eb357ffcea198bac99ed9c72d2", "status": "ok", "fetched_at": "2026-02-18T02:20:46.085100+00:00"}, "pages": [{"page": 1, "text": "Towards Reliable Medical LLMs: Benchmarking and\nEnhancing Confidence Estimation of Large Language Models\nin Medical Consultation\nZhiyao Ren1, Yibing Zhan2, Siyuan Liang1, Guozheng Ma1, Baosheng Yu1,\nDacheng Tao1*\n1Nanyang Technological University, Singapore, Singapore.\n2Wuhan University, Wuhan, China.\n*Corresponding author(s). E-mail(s): dacheng.tao@gmail.com;\nContributing authors: zhiyao001@e.ntu.edu.sg; zybjy@mail.ustc.edu.cn;\nsiyuan.liang@ntu.edu.sg; guozheng001@e.ntu.edu.sg; baosheng.yu@ntu.edu.sg;\nAbstract\nLarge-scale language models (LLMs) often offer clinical judgments based on incomplete information,\nincreasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn,\nstatic settings, overlooking the coupling between confidence and correctness as clinical evidence accu-\nmulates during real consultations, which limits their support for reliable decision-making. We propose\nthe first benchmark for assessing confidence in multi-turn interaction during realistic medical consul-\ntations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and\nintroduces an information sufficiency gradient to characterize the confidence‚Äìcorrectness dynamics as\nevidence increases. We implement and compare 27 representative methods on this benchmark; two\nkey insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-\nlevel confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy\nand information completeness. Based on these insights, we present MedConf, an evidence-grounded\nlinguistic self-assessment framework that constructs symptom profiles via retrieval-augmented genera-\ntion, aligns patient information with supporting, missing, and contradictory relations, and aggregates\nthem into an interpretable confidence estimate through weighted integration. Across two LLMs and\nthree medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC\nand Pearson correlation coefficient metrics, maintaining stable performance under conditions of infor-\nmation insufficiency and multimorbidity. These results demonstrate that information adequacy is a\nkey determinant of credible medical confidence modeling, providing a new pathway toward building\nmore reliable and interpretable large medical models.\nKeywords: Large Language Models, confidence estimation, medical data\n1 Introduction\nLLMs should evaluate their confidence levels in\ndiagnostic conclusions based on the available\npatient information, similar to how clinicians\nassess their own confidence during the decision-\nmaking process [1‚Äì3]. Such a self-assessment\nmechanism enables the model to actively acquire\n1\narXiv:2601.15645v1  [cs.CL]  22 Jan 2026\n"}, {"page": 2, "text": "I \nI‚Äôve been having stomach pain, feeling \nnauseous, and I have a slight fever.\nI \nIt‚Äôs a signs of appendicitis. Let‚Äôs\nget you to surgery right away.\n(a) Unreliable Confidence\nWait, doctor. I should mention I've \nbeen having watery diarrhea about 5 \ntimes a day.\nI \nOh... appendicitis doesn't \ncause diarrhea. It should be \nUrinary tract infection.\nI \nI‚Äôve been having stomach pain, feeling \nnauseous, and I have a slight fever\nI \nHave you had any diarrhea?\nYes, actually. Watery diarrhea about 5 \ntimes since yesterday.\nI \nHas anyone else been sick too?\nMy wife and daughter both got sick \nafter we all had lunch at the same \nrestaurant yesterday.\nThis is clearly acute gastroenteritis.\nI \n(b) Reliable Confidence\nConf:\n60\nConf:\n30\nConf:\n90\nConf:\n98\nConf:\n95\nFig. 1: Examples of medical consultation with\nreliable and unreliable confidence guidance.\nmore information at low confidence levels and\nprovide diagnostic conclusions at high confidence\nlevels, thereby enhancing the safety of the system\nin clinical applications.\nHowever, the current confidence estimation\nof LLMs still faces significant challenges: uncal-\nibrated confidence scores may lead to erroneous\ndecisions (Fig. 1(a)), whereas ideally, confidence\nshould be dynamically adjusted during the inter-\naction process (Fig. 1(b)). While studies have\nbegun to investigate the confidence performance of\nLLMs in the medical domain [4‚Äì11], there are still\nthree limitations: (1) Single-round static assess-\nment. Most of the work [4‚Äì7] measures confidence\nonly in a single-round, static scenario, and fails to\ntake into account the dynamic coupling of confi-\ndence and diagnostic correctness with the gradual\naccumulation of clinical evidence. (2) The task\nis oversimplified. Gu et al. [8] and Omar et al.\n[9] are based on multiple-choice or closed-ended\nquestion-and-answer tasks, which are difficult to\nreflect the context-dependent and complex inter-\nactions in real medical consultations; (3) Limited\nmethod coverage. Gao et al. [10] and Chen et al.\n[11] have only examined some of the confidence\nestimation strategies and have not yet systemat-\nically compared the applicability of token-level,\nconsistency-level, and self-verbalized methods in\nmedical scenarios.\nIn this paper, we introduce the first benchmark\nfor assessing the confidence of multi-round inter-\nactions in real medical consultations. The bench-\nmark integrates three types of medical data for\nopen-ended diagnostic generation and introduces\ninformation sufficiency gradients (1%, 20%, 40%,\n60%, 80%, 100%) to characterize the dynamic\nrelationship between confidence and accuracy. We\nused Pearson and Spearman correlation coeffi-\ncients [12] to assess consistency and measured\ndiscriminative power using AUROC [13] and\nAUPRC [14] on the DDXPlus [15], MediTOD [16],\nand MedQA [17] datasets to compare the perfor-\nmance of the 27 methods.\nThe evaluation results show that the uncer-\ntainty of existing confidence estimation methods\non medical data exhibits significant instability\nand randomness. Some self-verbalized methods\nachieve better results on DDXPlus and Medi-\nTOD but are markedly weaker than token-level\nand consistency-level methods on MedQA. This\ninconsistency mainly stems from two reasons.\n(1) Limitations at the methodological level. The\nhighly specialized terminology and imbalanced\nlabel distribution of medical data make token-level\nmethods susceptible to interference, while the lat-\nter undermines the stability of consistency-level\nmethods, thereby amplifying their inherent flaws.\n(2) Domain-specific challenges. Medical diagnosis\nis inherently uncertain, and the same symptom\nmay correspond to multiple diseases; thus, even\nif a diagnosis aligns with patient information, it\ndoes not necessarily indicate that the information\nis sufficient or the judgment is reliable. Based on\nthese findings, we propose two takeaways for the\ndesign of confidence estimation methods in med-\nical scenarios: (1) confidence estimation methods\nthat rely solely on model output features (e.g.,\ntoken layer or consistency layer) are highly sen-\nsitive to the characteristics of medical data and\nshould be enhanced by incorporating other types\nof strategies; and (2) confidence assessment in\nmedical tasks should account for both diagnos-\ntic accuracy and information completeness rather\nthan only assessing the correctness of answers.\nBased on these two insights, we propose Med-\nConf. On one hand, it replaces the reliance on\n2\n"}, {"page": 3, "text": "token or consistency signals with self-verbalized\nevaluation to mitigate its sensitivity to data char-\nacteristics at the source. On the other hand, it\nevaluates information integrity using evidence-\nbased metrics. As shown in Fig. 6, the model\nfirst generates a preliminary diagnosis based\non the available information; then, MedConf\nretrieves authoritative knowledge related to the\ndiagnosis through retrieval-augmented generation\n(RAG) [18] and summarizes it to form a symp-\ntom spectrum with symptom descriptions and\nsignificance; and finally, the LLM identifies the\nsupportive, missing, and contradictory relation-\nships between the information provided by the\npatient and the symptom spectrum. It weights and\naggregates these relationships as interpretable evi-\ndence to derive a confidence score for the current\ndiagnosis.\nExperimental results demonstrate that Med-\nConf achieves state-of-the-art performance across\nall datasets and models, exhibiting excellent cor-\nrelation with accuracy and superior discrimina-\ntive capability. For example, compared to the\n27 baseline methods in our study on DDX-\nPlus with Llama-3.1, MedConf demonstrates aver-\nage improvements of 0.410 and 0.476 for Pear-\nson and Spearman coefficients, while achieving\nimprovements of 0.129 and 0.124 in AUROC and\nAUPRC metrics. Furthermore, MedConf demon-\nstrates superior robustness against noisy or irrel-\nevant information and achieves higher accuracy\nwith efficiency when integrated with medical\nagents. Ablation studies reveal the contribution of\ndifferent designs in MedConf. Our main contri-\nbutions are:\n‚Ä¢ We propose the first medical LLMs confidence\nestimation benchmark that assesses confidence\nunder a multi-turn interaction setting for real-\nistic medical scenarios. Our benchmark utilizes\nmore realistic tasks, comprehensively collects 27\nconfidence estimation methods, and evaluates\ntheir correlation with accuracy and discrimi-\nnative ability under different levels of patient\ninformation.\n‚Ä¢ Benchmark results reveal that medical data\nmay amplify the limitations of token-level and\nconsistency-level methods that rely on model\noutput features, and medical tasks need to con-\nsider uncertainty arising from incomplete input\ninformation. To solve this, we propose Med-\nConf, an evidence-based self-verbalized method\nthat considers the supportive, missing, and con-\ntradictory relationships between existing infor-\nmation and diagnosis results as evidence to infer\na confidence score.\n‚Ä¢ Experimental\nresults\nshow\nthat\nMedConf\nachieves\nstate-of-the-art\nresults\nacross\nall\nbenchmark configurations. Furthermore, Med-\nConf demonstrates robustness to irrelevant\ninformation and achieves high diagnostic accu-\nracy and interaction efficiency when integrated\ninto healthcare agents.\n2 Related Works\n2.1 LLMs Confidence Estimation\nAs illustrated in Fig. 2, current confidence esti-\nmation methods for LLMs can be categorized\ninto three types: token-level methods, consistency-\nlevel methods, and self-verbalized methods. In this\npaper, we collect 27 confidence estimation meth-\nods and evaluate them on our proposed medical\nconfidence estimation benchmark.\nToken-level methods calculate the confi-\ndence of LLMs based on the probability distribu-\ntion of the generation. These techniques rely on\ntoken probability from a single model prediction.\nJiang et al. [19] first propose measuring the confi-\ndence score using the predicted probability of the\nresponse tokens. Huang et al. [20] and Manakul\net al. [21] propose using the negative log-likelihood\nor entropy of the average or maximum of the\nresponse tokens to estimate confidence. Moreover,\nalternatives such as perplexity [22], mutual infor-\nmation [23], R¬¥enyi divergences, and Fisher-Rao\ndistance [24] are widely used to calculate confi-\ndence. Using a different approach, Claim Condi-\ntional Probability (CCP) [25] decomposes LLM\noutputs into a set of claims and computes token-\nlevel uncertainty from the tokens constituting each\nclaim. Recently, Duan et al. [26] demonstrate that\nnot all tokens contribute equally to the meaning.\nThey propose the tokenSAR method, which re-\nweights the computation results by evaluating the\nimportance of each token.\nConsistency-level methods sample multi-\nple responses to the same query, then utilize the\nconsistency to estimate the confidence score. Man-\nakul et al. [21] propose using the percentage of\n3\n"}, {"page": 4, "text": "Input\nConfidence\nPrompt\nAnswer\nLLM\nConfidence\nScore\nLLM\nInput\nAnswer\nWhite Box\nLLM\nTokens Prob\nùëÉùëÉ(ùëßùëß|ùë•ùë•)\nProbability\nCalculation\nConfidence\nScore\nInput\nLLM\nRepeat k times\nAnswer 1\nAnswer 2\nAnswer k\n‚Ä¶\nSimilarity\nCalculation\nConfidence\nScore\n(a) Token-Level Method\n(b) Consistency-Level Method\n(c) Self-Verbalized Method\nFig. 2: Illustration of the token-level, consistency-\nlevel, and self-verbalized method of LLMs confi-\ndence estimation.\nthe most consistent samples to evaluate the confi-\ndence. Lin et al. [27] introduce the use of different\nlinear algebra techniques, such as Degree Matrix,\nSum of Eigenvalues of the Graph Laplacian, and\nEccentricity, to measure confidence. Furthermore,\nlexical similarity [21] and semantic similarity [28]\ncan also be regarded as essential metrics for con-\nsistency assessment. Taking a different approach,\nsome studies integrate token probability into the\ncalculation of consistency measures. Monte Carlo\nSequence Entropy [29] generates several sequences\nvia random sampling and computes the result-\ning entropy. Semantic Entropy [29] clusters the\nsemantically similar generations and then calcu-\nlates the entropy. Finally, SAR [26] amplifies the\nprobability of sentences that are more relevant and\nconvincing than others.\nSelf-verbalized methods query the LLM\nitself about the confidence of its generation. Lin\net al. [30] first demonstrate the LLM‚Äôs capabilities\nto provide reasonable confidence. Subsequently,\nTian et al. [31] introduce a two-step process, top-\nk sampling, and chain-of-thought techniques to\nimprove confidence score. P(true) [32] introduce a\ndifferent approach by asking the model to validate\nits answer as true or false and then assigning the\nconfidence score on the probability of true.\n2.2 LLMs Confidence Estimation in\nMedical Domain\nConfidence estimation effectively improves the\nreliability and reduces risks of an artificial intel-\nligence system in the medical domain. Exist-\ning research has applied confidence estimation\nto machine learning models [33‚Äì35], vision mod-\nels [36‚Äì39], and small language models [40‚Äì42].\nHowever, due to the large-scale parameter char-\nacteristics of LLMs, these methods are difficult\nto transfer to LLM evaluation. Recent studies\nhave applied existing LLM confidence estimation\nmethods to various medical datasets to investi-\ngate their effectiveness on medical data. Savage\net al. [6] and Atf et al. [7] evaluate these methods\non Medical QA tasks, while Gu et al. [8], Omar\net al. [9], and Savage et al. [6] focus on medical\nmultiple-choice questions. Gao et al. [10] and Chen\net al. [11] examine their effectiveness on electronic\nhealth records (EHR)-based clinical prediction.\nHowever, these benchmarks evaluate confidence\nonly under complete information and overlook\nhow confidence scores evolve as additional relevant\ninformation becomes available. Beyond bench-\nmarking, Hu et al. [43] introduce confidence scores\nto enhance inquiry effectiveness, whereas Wu et al.\n[44] and Qin et al. [45] calibrate medical LLM con-\nfidence through Chain-of-Verification and atypical\npresentations, respectively. In this paper, we pro-\npose an evidence-based self-verbalized method\nthat improves confidence estimation for medical\ndatasets.\n3 Medical Confidence\nBenchmark\n3.1 Motivation\nExisting medical LLMs confidence estimation\nbenchmarks [6‚Äì11] have three fundamental limi-\ntations in their setup. First, current studies only\nevaluate confidence in a single-round and static\nscenario, and fail to take into account the dynamic\ncoupling of confidence and diagnostic correct-\nness as the information increases. Second, exist-\ning benchmarks only evaluate on simple medical\ntasks. Savage et al. [6] and Atf et al. [7] assess\nconfidence performance on simple question-answer\ntask and Gu et al. [8] and Omar et al. [9] eval-\nuate on multiple-choice questions task. However,\n4\n"}, {"page": 5, "text": "Table 1: Confidence estimation methods implemented in our benchmark.\nCategory\nConfidence Estimation Method\nType\nToken-level\nAverage Sequence Probability (ASP) [20]\nWhite-box\nMaximum Sequence Probability (MSP) [20]\nWhite-box\nPerplexity [22]\nWhite-box\nEntropy [21]\nWhite-box\nPointwise Mutual Information (PMI) [23]\nWhite-box\nConditional PMI [46]\nWhite-box\nTokenSAR [26]\nWhite-box\nR¬¥enyi Divergence [24]\nWhite-box\nFisher-Rao Distance [24]\nWhite-box\nClaim Conditional Probability (CCP) [25]\nWhite-box\nConsistency-level\nPercentage of Consistency (PoC) [21]\nBlack-box\nLexical Similarity (LexSim) [21]\nBlack-box\nSemantic Similarity (SemSim) (BERT) [28]\nBlack-box\nSemantic Similarity (SemSim) (MedBERT) [28]\nBlack-box\nNumber of Semantic Sets (NumSet) [27]\nBlack-box\nSum of Eigenvalues of the Graph Laplacian (EigV) [27]\nBlack-box\nDegree Matrix (Deg) [27]\nBlack-box\nEccentricity (Ecc) [27]\nBlack-box\nMonte Carlo Sequence Entropy (MC-SE) [29]\nWhite-box\nMonte Carlo Norm. Seq. Entropy (MC-NSE) [47]\nWhite-box\nSemantic Entropy [29]\nWhite-box\nSentenceSAR [26]\nWhite-box\nSAR [26]\nWhite-box\nSelf-verbalized\nConfidence Elicitation (CE) [31]\nBlack-box\nCoT CE [31]\nBlack-box\nTop-k CE [31]\nBlack-box\nP(True) [32]\nWhite-box\nopen-ended decision-making based on dialogues\nor reports is more aligned with real-world med-\nical consultations. Finally, existing benchmarks\ndemonstrate insufficient methodological compre-\nhensiveness. Omar et al. [9] evaluate only the\ntoken probability confidence performance. In con-\ntrast, Gu et al. [8] and Atf et al. [7] focus on\npartial token-level and self-verbalized methods.\nMeanwhile, Savage et al. [6] includes all three cate-\ngories of methods but evaluates only 2-3 relatively\noutdated approaches.\nIn this section, we introduce a new evaluation\nbenchmark. To conduct testing in scenarios more\naligned with real medical practice, we restructure\nthe data format into doctor-patient dialogues or\npatient reports and transform tasks into open-\nended diagnostic generation (In Sec. 3.2). To test\nthe trends of confidence changes as patient infor-\nmation increases, we propose a patient informa-\ntion dividing method and new evaluation metrics\n(In Sec. 3.3). Additionally, we collect and evalu-\nate 27 confidence estimation methods, enhancing\nthe comprehensiveness of benchmark evaluation\n(In Sec. 3.4).\n3.2 Data Preparation\nWe evaluate the existing confidence estimation\nmethods on three medical datasets: DDXPlus [15],\nMediTOD [16], and MedQA [17]. DDXPlus is a\nlarge-scale synthetic dataset containing pathology\ninformation, symptoms, and patient antecedents.\nMediTOD comprises real-world doctor-patient\ndialogues with annotated complex relationships\nbetween\ndialogue\ncontent\nand\ncorresponding\n5\n"}, {"page": 6, "text": "attributes. MedQA is a professional-level multiple-\nchoice question (MCQ) dataset derived from med-\nical licensing examinations (e.g., USMLE) that\nrequires complex medical reasoning and domain\nknowledge.\nTo achieve closer alignment with clinical prac-\ntice, we preprocess all datasets by transforming\nthem into doctor-patient conversation formats or\nmedical report formats and restructuring them\nas open-ended diagnosis generation tasks. Specifi-\ncally, for the DDXPlus dataset, we convert struc-\ntured data into doctor-patient dialogue format\nusing GPT-4.1 [48], transforming symptoms into\ndoctor inquiries and converting binary or numeri-\ncal results into natural language patient responses.\nFor MediTOD, since the data is already in dia-\nlogue format, we retain the original structure\nwhile filtering to include only effective dialogues\nbased on the purpose and contribution of each\nconversational turn as labeled in the dataset. For\nMedQA, we retain its report format and trans-\nform the MCQ task into a generation task by\nremoving multiple-choice options and requiring\nopen-ended responses. More details are provided\nin the supplementary material and Fig. A1.\nTo ensure adequate patient information for\nthe following confidence estimation, we apply fil-\ntering criteria selecting DDXPlus and MediTOD\ndialogues with more than 10 conversational turns\nand MedQA cases with more than 10 sentences.\nThe final evaluation dataset comprises 171 DDX-\nPlus instances, 231 MediTOD instances, and 181\nMedQA instances.\n3.3 Evaluation Pipeline and Metrics\nIn our evaluation, we systematically partition\npatient information into progressive levels: 1%\n(containing only single-turn conversation or one-\nsentence report), 20%, 40%, 60%, 80%, and 100%\nof the complete case information. We then gener-\nate diagnostic predictions using LLMs based on\neach information level and compute confidence\nscores for these predictions using different confi-\ndence estimation methods. Finally, we assess the\naccuracy of each generated diagnosis and analyze\nits relationship with the corresponding confidence\nscores.\nWe evaluate confidence estimation perfor-\nmance from two perspectives: 1) Correlation\nassessment: We employ Pearson and Spearman\ncorrelation coefficients to measure the align-\nment between diagnostic accuracy and confi-\ndence scores, determining whether confidence esti-\nmates appropriately increase alongside accuracy\nimprovements. 2) Discriminative assessment: We\nutilize AUROC and AUPRC metrics to evaluate\nthe capability of confidence scores to effectively\ndistinguish between correct and incorrect diagnos-\ntic predictions.\n3.4 Evaluated Methods\nAs listed in Tab. 1, we implement and evaluate 27\nwidely-used confidence estimation methods, com-\nprising 10 token-level methods, 13 consistency-\nlevel methods, and four self-verbalized methods.\nAccessibility requirements categorize these meth-\nods: 11 are black-box methods that rely solely\non model generation outputs and apply to both\nclosed-source and open-source models, while 16\nare white-box methods that require access to\ninternal model states, such as logits or hidden\nlayer outputs, and are therefore limited to open-\nsource models. For mathematical notations used\nin this section, please refer to Tab. A1 in the\nsupplementary material.\nToken-level methods: The token-level methods\nanalyze the probability distribution over individ-\nual tokens P(yl | y<l, x), where x is the input\nsequence and y<l is previous generated tokens. A\nuniversal formula of token-level methods can be\nrepresented as:\nC = œÜ\n L\nM\nl=1\nwl ‚äôœà (P(yl | y<l, x))\n!\n(1)\nwhere œà(¬∑) is the token-level transformation func-\ntion, LL\nl=1 is the aggregation operation, œÜ(¬∑) is\nthe final transformation function, wl is the token\nweights, and L is the generated sequence length.\nFor example, Average Sequence Probability\n(ASP) [20] applies identity transformation func-\ntion (œà(P) = P and œÜ(x) = x) and aggregates\ntokens probabilities via average (wl =\n1\nL and\nLL\nl=1 = PL\nl=1), while Maximum Sequence Proba-\nbility (MSP) [20] similarly uses identity transfor-\nmations but aggregates by selecting the maximum\ntoken probability (LL\nl=1 = maxl=1,...,L). Perplex-\nity [22] transforms token probabilities via negative\n6\n"}, {"page": 7, "text": "logarithm (œà(P) = ‚àílog P) and applies expo-\nnential final transformation (œÜ(x) = exp(x)) with\nuniform averaging. Moreover, TokenSAR [26] pro-\nposes that the contributions of different tokens\nare unequal and utilizes a relevance-based func-\ntion to change the weight for each token (wl =\nÀúRT (yl, y, x)). We provide a more detailed dis-\ncussion on the specifics of these methods and\nother token-level methods in the supplementary\nmaterial.\nConsistency-level methods: The consistency-\nlevel methods evaluate confidence by assessing the\nconsistency of responses under the same input.\nThis approach involves two steps:\n1) Given input x, the LLM generates a\nresponse set Y = {y1, y2, . . . , yK} containing K\noutputs.\n2) Estimate confidence by quantifying the con-\nsistency within the set Y.\nConsistency-based methods primarily focus on\ndesigning different metrics to evaluate consistency.\nFor instance, PoC [21] calculates the propor-\ntion of the most frequent response, while Sem-\nSim [28] measures confidence by computing the\naverage cosine similarity between the embedding\nof response pairs.\nOn the other hand, several approaches inte-\ngrate token probability data into consistency\nmeasures. Monte Carlo Sequence Entropy (MC-\nSE) [29] calculates entropy at the sequence level\nby averaging the negative log-probabilities across\nmultiple generated sequences:\nCMC-SE = ‚àí1\nK\nK\nX\nk=1\nlog P(y(k)|x)\n(2)\nwhere P(y(k)|x)\n=\nQLk\nl=1 P(y(k)\nl\n|y(k)\n<l , x) rep-\nresents the probability of the k-th generated\nsequence.\nBased\non\nthis,\nSemantic\nEntropy\n(SE) [29] first clusters the responses into similar\ngroups and then calculates the entropy over these\nsemantic clusters. We provide a more detailed\ndiscussion of consistency-based methods in the\nsupplementary material.\nSelf-verbalized Methods: The Confidence Elic-\nitation (CE) methods utilize the LLM‚Äôs reason-\ning ability to express confidence in natural lan-\nguage [31]. Given input x and LLM output y, the\nmodel provides a confidence score from 0 to 100\nusing a specific prompt:\nCCE = f(Prompt, x, y)\n(3)\nwhere f(¬∑) represents the LLM model. The prompt\nfollows different strategies such as vanilla prompt-\ning, Chain-of-Thought reasoning, or Top-k selec-\ntion, with details provided in the supplementary\nmaterial and Fig. A2-A4.\nAlternatively, the P(True) [32] method queries\nthe model to validate its answer as true or false.\nThe confidence is computed as the proportion of\ntrue responses across K evaluations:\nCPTrue = 1\nK\nK\nX\ni=1\nI(y(i) = ‚ÄúTrue‚Äù)\n(4)\n4 Benchmark Results\nIn this section, we conduct experiments with\ntwo large language models: an open-source model\nLlama-3.1 [49], and a closed-source model GPT-\n4.1 [48]. We first report the performance of dif-\nferent methods across various models and dataset\ncombinations, then analyze the shortcomings of\nexisting methods based on experimental results.\nFinally, we propose insights for designing con-\nfidence estimation methods that meet medical\nrequirements.\n4.1 Information-Accuracy\nCorrelation\nIn this part, we examine how diagnostic accuracy\nvaries for Llama-3.1 and GPT-4.1 as patient infor-\nmation is progressively accumulated across differ-\nent medical datasets. This evaluation establishes\nthe fundamental relationship between information\navailability and diagnostic performance, providing\ncritical context for understanding the importance\nof confidence-guided decision making in medical\nAI systems.\nAs shown in the results in Fig. 3, both mod-\nels exhibit a consistent and approximately linear\nincrease in diagnostic accuracy with the sequential\naddition of relevant clinical information. This lin-\near relationship underscores a fundamental princi-\nple: making diagnoses based on insufficient patient\n7\n"}, {"page": 8, "text": "(a) Llama-3.1\n(b) GPT-4.1\nFig. 3: Model performance on medical datasets DDXPlus, MediTOD, and MedQA across information\nlevels for (a) Llama-3.1 and (b) GPT-4.1.\ninformation inevitably leads to suboptimal out-\ncomes, while comprehensive information gathering\nsignificantly enhances diagnostic reliability.\n4.2 Confidence-Accuracy\nCorrelation\nIn this part, we evaluate the correlation between\nconfidence and diagnostic accuracy as patient\ninformation increases. The results in Tab. 2 reveal\nperformance variability across different datasets\nand models. Token-level methods and consistency-\nlevel methods exhibit significant sensitivity to\ndatasets and models. For instance, for Llama-3.1,\ntoken-level methods Perplexity and TokenSAR\ndemonstrate\nstrong\naccuracy-confidence\nalign-\nment on MedQA (Pearson coefficients > 0.961,\nSpearman coefficients = 1.0), yet show markedly\npoor consistency on DDXPlus and MediTOD\ndatasets (Pearson coefficients < 0.515, Spearman\ncoefficients < 0.486). Consistency-level methods\nEig, Deg, and Ecc methods achieve excellent con-\nsistency on MediTOD and MedQA datasets on\nGPT-4.1, but fail to maintain this performance on\nDDXPlus with GPT-4.1 or on any datasets with\nLlama-3.1.\nIn contrast, self-verbalized methods demon-\nstrate superior robustness across most models\nand datasets. The CE and CoT CE methods\nconsistently produce confidence scores with sta-\ntistically significant accuracy correlations across\nboth Llama-3.1 and GPT-4.1 on all datasets. How-\never, exceptions still exist. For example, when\napplying the CoT CE method to the MediTOD\ndataset with the Llama-3.1 model, the align-\nment effect remains limited (Pearson correlation\n= 0.596, Spearman correlation = 0.086).\n4.3 Discriminative Ability\nIn this part, we examine the ability of the\nconfidence score to discriminate between correct\nand incorrect diagnosis. Based on the results\nshown in Fig. 4, existing confidence estima-\ntion methods exhibit two key characteristics in\ntheir discriminative ability. First, different meth-\nods exhibit varying applicability across datasets.\nSelf-verbalized methods outperform both token-\nlevel and consistency-based methods on DDX-\nPlus and MediTOD datasets, while on MedQA,\nself-verbalized methods demonstrate poor per-\nformance and consistency-based methods achieve\noptimal results. Second, performance variations\nexist among methods within the same category.\nFor instance, on DDXPlus using Llama-3.1, the\nASP method achieves excellent discriminative\nability, whereas the MSP and Entropy methods,\ndespite belonging to the same token-level category,\nperform substantially worse.\n4.4 Discussion\nExisting methods show certain consistency with\naccuracy and discriminative ability for correct\nanswers, indicating the potential for applying con-\nfidence estimation in the medical domain. How-\never, experimental results reveal that existing\nmethods exhibit sensitivity to both models and\ndatasets. These methods may perform well under\nspecific conditions but substantially underperform\n8\n"}, {"page": 9, "text": "Table 2: Pearson and Spearman correlation coefficients between accuracy and confidence scores across\ndifferent confidence estimation methods, models, and datasets. Results show correlation values with p-\nvalues in parentheses. Statistically significant results (p‚â§0.05) are bolded. Token-level methods are colored\nblue, consistency-level methods are colored green, and self-verbalized methods are colored orange.\n(a) Llama-3.1\nMethod\nDDXPlus\nMediTOD\nMedQA\nPearson\nSpearman\nPearson\nSpearman\nPearson\nSpearman\nASP\n0.794 (0.059)\n0.429 (0.397)\n0.306 (0.556)\n0.257 (0.623)\n0.994 (‚â§0.05)\n0.943 (‚â§0.05)\nMSP\n0.786 (0.064)\n1.000 (‚â§0.05)\n0.807 (0.137)\n0.486 (0.329)\n0.913 (‚â§0.05)\n0.943 (‚â§0.05)\nPerplexity\n0.508 (0.303)\n0.486 (0.329)\n0.431 (0.596)\n0.086 (0.872)\n0.978 (‚â§0.05)\n1.000 (‚â§0.05)\nEntropy\n0.14 (0.792)\n0.029 (0.957)\n0.18 (0.733)\n0.143 (0.787)\n0.905 (‚â§0.05)\n0.829 (‚â§0.05)\nPMI\n0.781 (0.067)\n0.943 (‚â§0.05)\n0.526 (0.283)\n0.714 (0.111)\n0.746 (0.089)\n0.886 (‚â§0.05)\nConditional PMI\n0.829 (‚â§0.05)\n0.042 (0.397)\n0.734 (0.097)\n0.829 (‚â§0.05)\n0.973 (‚â§0.05)\n0.943 (‚â§0.05)\nTokenSAR\n0.515 (0.296)\n0.486 (0.329)\n0.382 (0.226)\n0.257 (0.623)\n0.981 (‚â§0.05)\n1.000 (‚â§0.05)\nR¬¥enyi Divergence\n0.041 (0.939)\n0.029 (0.957)\n0.662 (0.160)\n0.257 (0.623)\n0.961 (‚â§0.05)\n0.771 (0.072)\nFisher-Rao Distance\n0.049 (0.927)\n0.086 (0.871)\n0.608 (0.200)\n0.257 (0.623)\n0.961 (‚â§0.05)\n0.943 (‚â§0.05)\nCCP\n0.848 (‚â§0.05)\n0.943 (‚â§0.05)\n0.585 (0.223)\n0.771 (0.072)\n0.958 (‚â§0.05)\n0.943 (‚â§0.05)\nPoC\n0.555 (0.253)\n0.2 (0.704)\n0.285 (0.815)\n0.086 (0.872)\n0.865 (‚â§0.05)\n0.886 (‚â§0.05)\nLexical Similarity\n0.320 (0.537)\n0.657 (0.156)\n0.335 (0.516)\n0.657 (0.156)\n0.917 (‚â§0.05)\n0.829 (‚â§0.05)\nSemantic Similarity (BERT)\n0.733 (0.098)\n0.429 (0.397)\n0.491 (0.523)\n0.571 (0.156)\n0.865 (‚â§0.05)\n0.886 (‚â§0.05)\nSemantic Similarity (MedBERT)\n0.437 (0.386)\n0.2 (0.704)\n0.331 (0.522)\n0.429 (0.397)\n0.265 (0.612)\n0.200 (0.704)\nNumSet\n0.443 (0.379)\n0.657 (0.156)\n0.443 (0.379)\n0.657 (0.156)\n0.594 (0.214)\n0.143 (0.787)\nEigV\n0.567 (0.24)\n0.029 (0.623)\n0.195 (0.711)\n0.371 (0.468)\n0.894 (‚â§0.05)\n0.600 (0.208)\nDeg\n0.655 (0.158)\n0.6 (0.208)\n0.059 (0.911)\n0.600 (0.208)\n0.698 (0.125)\n0.257 (0.623)\nEcc\n0.668 (0.147)\n0.429 (0.397)\n0.668 (0.147)\n0.429 (0.397)\n0.708 (0.116)\n0.429 (0.397)\nMC-SE\n0.578 (0.229)\n0.371 (0.468)\n0.579 (0.229)\n0.371 (0.468)\n0.897 (‚â§0.05)\n0.886 (‚â§0.05)\nMC-NSE\n0.473 (0.343)\n0.6 (0.208)\n0.473 (0.344)\n0.486 (0.329)\n0.988 (‚â§0.05)\n1.000 (‚â§0.05)\nSemantic Entropy\n0.722 (0.105)\n1.000 (‚â§0.05)\n0.510 (0.302)\n0.486 (0.329)\n0.897 (‚â§0.05)\n1.000 (‚â§0.05)\nSentenceSAR\n0.326 (0.529)\n0.257 (0.623)\n0.372 (0.467)\n0.257 (0.623)\n0.981 (‚â§0.05)\n1.000 (‚â§0.05)\nSAR\n0.347 (0.502)\n0.086 (0.871)\n0.533 (0.280)\n0.486 (0.329)\n0.897 (‚â§0.05)\n0.771 (0.072)\nCE\n0.870 (‚â§0.05)\n0.771 (0.072)\n0.895 (‚â§0.05)\n1.000 (‚â§0.05)\n0.972 (‚â§0.05)\n1.000 (‚â§0.05)\nCoT CE\n0.907 (‚â§0.05)\n0.943 (‚â§0.05)\n0.596 (0.470)\n0.086 (0.623)\n0.935 (‚â§0.05)\n0.943 (‚â§0.05)\nTop-k CE\n0.814 (‚â§0.05)\n0.829 (‚â§0.05)\n0.690 (0.130)\n0.714 (0.111)\n0.704 (0.119)\n0.771 (0.072)\nP(True)\n0.364 (0.478)\n0.086 (0.871)\n0.232 (0.558)\n0.143 (0.787)\n0.585 (0.223)\n0.771 (0.072)\n(b) GPT-4.1\nMethod\nDDXPlus\nMediTOD\nMedQA\nPearson\nSpearman\nPearson\nSpearman\nPearson\nSpearman\nPoC\n0.105 (0.861)\n0.257 (0.623)\n0.356 (0.489)\n0.600 (0.208)\n0.834 (‚â§0.05)\n0.657 (0.156)\nLexical Similarity\n0.069 (0.897)\n0.371 (0.468)\n0.451 (0.369)\n0.714 (0.111)\n0.958 (‚â§0.05)\n1.000 (‚â§0.05)\nSemantic Similarity (BERT)\n0.785 (0.064)\n0.829 (‚â§0.05)\n0.828 (‚â§0.05)\n0.771 (0.072)\n0.903 (‚â§0.05)\n0.829 (‚â§0.05)\nSemantic Similarity (MedBERT)\n0.120 (0.821)\n0.257 (0.623)\n0.109 (0.836)\n0.086 (0.872)\n0.847 (‚â§0.05)\n0.771 (0.072)\nNumSet\n0.047 (0.930)\n0.086 (0.872)\n0.000 (1.000)\n0.086 (0.872)\n0.804 (0.054)\n0.714 (0.111)\nEigV\n0.660 (0.154)\n0.429 (0.397)\n0.871 (‚â§0.05)\n0.943 (‚â§0.05)\n0.945 (‚â§0.05)\n1.000 (‚â§0.05)\nDeg\n0.576 (0.231)\n0.657 (0.156)\n0.873 (‚â§0.05)\n0.943 (‚â§0.05)\n0.900 (‚â§0.05)\n0.829 (‚â§0.05)\nEcc\n0.575 (0.233)\n0.600 (0.208)\n0.873 (‚â§0.05)\n0.943 (‚â§0.05)\n0.906 (‚â§0.05)\n0.886 (‚â§0.05)\nCE\n0.989 (‚â§0.05)\n1.000 (‚â§0.05)\n0.981 (‚â§0.05)\n1.000 (‚â§0.05)\n0.989 (‚â§0.05)\n1.000 (‚â§0.05)\nCoT CE\n0.861 (‚â§0.05)\n0.943 (‚â§0.05)\n0.981 (‚â§0.05)\n1.000 (‚â§0.05)\n0.987 (‚â§0.05)\n1.000 (‚â§0.05)\nTop-k CE\n0.269 (0.591)\n0.429 (0.397)\n0.587 (0.220)\n0.943 (‚â§0.05)\n0.552 (0.256)\n0.486 (0.329)\nwhen applied to different models or datasets.\nFor example, on Llama-3.1, the Entropy method\nachieves excellent consistency (Pearson coeffi-\ncient = 0.905 and Spearman coefficient = 0.829)\nand discriminative ability (AUROC = 0.766 and\nAUPRC = 0.694) on the MedQA dataset. In\ncontrast, the same method demonstrates severely\nlimited consistency (Pearson coefficient = 0.14\nand Spearman coefficient = 0.029) and discrimi-\nnative ability (AUROC = 0.501 and AUPRC =\n0.555) on the DDXPlus dataset.\nThese findings underscore the insufficient reli-\nability of current methods when confronted with\ndiverse medical data, thereby hindering their prac-\ntical deployment in clinical applications. The\nchallenges of current methods in the medical\ndomain are primarily concentrated in two key\n9\n"}, {"page": 10, "text": "(a) Llama-3.1\n(b) GPT-4.1\nFig. 4: Discriminative performance comparison of confidence estimation methods across models and\ndatasets using AUROC and AUPRC metrics. (a) Results for Llama-3.1 model and (b) Results for GPT-\n4.1 model. Methods are color-coded by category: token-level methods (blue), consistency-level methods\n(green), and self-verbalized methods (yellow).\nareas: 1) methodological limitations, where the\nunique characteristics of medical data amplify\nthe inherent limitations of existing approaches; 2)\ndomain-specific challenges, where the medical field\nimposes additional requirements for confidence\nevaluation.\n4.4.1 Methodological limitations\nToken-level methods exhibit fundamental limita-\ntions that they rely heavily on the probability of\nindividual tokens. However, the token logit only\ncaptures the model‚Äôs uncertainty regarding the\nnext token rather than providing an assessment of\nthe reliability of a specific claim [50]. In medical\ncontexts, the complex and lengthy domain-specific\nterminology magnifies this limitation. As illus-\ntrated in Fig. 5(a), which presents an incorrect\ndiagnosis, the ‚ÄúAnticholinergic Toxicity‚Äù is tok-\nenized into 5 tokens: ‚ÄúAnt‚Äù,‚Äúich‚Äù,‚Äúolin‚Äù,‚Äúergic‚Äù,\nand ‚Äútoxicity‚Äù. Although the model exhibits high\nuncertainty for the initial token ‚ÄúAnt‚Äù, once this\ntoken is generated, subsequent tokens have very\nhigh probabilities. Consequently, when consider-\ning the average probability of generating this\ndiagnosis, this erroneous prediction receives a high\nconfidence score of 0.754.\nFor consistency-level methods, they suffer from\na different fundamental issue: consistency reflects\nonly the relative likelihood of an LLM response\ncompared to alternative responses generated by\nthe same model, rather than its likelihood in\nthe real world [29]. In the medical domain, the\ncoexistence of common and rare diseases creates\nuneven data distributions that amplify the dis-\ncrepancy between model-generated likelihood and\nactual clinical likelihood. As the example shown in\nFig. 5(b), when patient information contains only\nnon-specific symptoms like fever, conditions such\nas flu, cold, and pneumonia should theoretically\nexhibit similar generation probabilities, resulting\nin appropriately low consistency scores. However,\ndue to the model overfitting toward more frequent\nconditions like flu, the model generates highly\n10\n"}, {"page": 11, "text": "Anticholinergic Toxicity\n[Ant, ich, olin, ergic, toxicity]\ntokenize\nAnt\nich olin ergictoxicity\nAverage Probability:\n0.754\nProb\n(a) Token-level methods limitation\nPatient Information: Fever\nFlu\nCold\nPneumonia\nFlu\nCold\nPneumonia\nOverfit\nLow \nConsistency\nHigh \nConsistency\n(b) Consistency-level methods limitation\nSymptoms\nDiseases\nStomach Pain\nFever\nDiarrhea\nClustering Case\nFood Poisoning\nAppendicitis\nUrinary Infection\nSupport\nMissing\n(c) Medical domain challenge\nFig. 5: Example of methodological limitation\nof (a) token-level methods, (b) consistency-level\nmethods, and (c) medical domain challenge.\nconsistent results even for such ambiguous pre-\nsentations, rendering consistency scores unable to\naccurately reflect true diagnostic uncertainty.\n4.4.2 Domain-specific Challenge\nExisting confidence estimation methods were orig-\ninally designed for arithmetic, logic, and symbolic\nreasoning tasks, which are characterized by com-\nprehensive problem descriptions and unique cor-\nrect answers [4, 5]. However, medical diagnostic\ntasks present fundamentally different challenges:\npatient-provided information is often incomplete\nor insufficient, and the relationships between\nsymptoms and diseases are inherently complex\nand multifaceted. As illustrated in Fig. 5(c), when\na patient provides information about stomach\npain and fever, the model makes a diagnosis of\nfood poisoning. According to previous task evalu-\nation standards, the current diagnosis contains no\nerrors and should receive high confidence support.\nHowever, due to the complex nature of medi-\ncal conditions, the currently provided information\ncould also correspond to other diseases, such as\nappendicitis or urinary tract infection, revealing\nthe uncertainty of the current diagnosis. There-\nfore, in medical contexts, confidence evaluation\nneeds to assess not only whether the conclusion\nis correct, but also whether the input information\nis sufficient to support the diagnostic conclusion.\nHowever, current methods all lack the ability to\nevaluate input completeness.\n4.4.3 Medical Confidence Estimation\nInsights\nBased\non\nexisting\nanalysis,\nwe\npropose\ntwo\ninsights for designing medical confidence estima-\ntion methods suitable for healthcare applications:\n‚ù∂Based on our analysis of methodological limita-\ntions, methods that rely on model output features\n(token-level methods and consistency-based meth-\nods) are highly sensitive to the characteristics\nof medical data. To achieve stable performance,\nalternative strategies that are less affected by\nmedical data should be considered, such as self-\nverbalized methods based on model reasoning to\nevaluate confidence scores; ‚ù∑Based on our analy-\nsis of domain-specific challenges that medical data\nrequires assessing patient information sufficiency,\nconfidence assessment in medical tasks should\naccount for both diagnostic accuracy and informa-\ntion completeness rather than only assessing the\ncorrectness of answers.\n5 MedConf\nFollowing Insight ‚ù∂, MedConf does not rely on\nmodel output probabilities or distributions, but\ninstead leverages the model‚Äôs reasoning capabili-\nties through a self-verbalized approach, mitigat-\ning its sensitivity to medical data characteristics.\nAccording to Insight ‚ù∑, MedConf treats the com-\nprehensiveness of patient information as a crucial\ncriterion for confidence assessment. Based on these\ntwo insights, we propose MedConf, a diagnostic\nconfidence estimation method based on evidence-\ndriven self-verbalization. The goal of MedConf is\nto estimate a confidence score C ‚àà[0, 100] that\n11\n"}, {"page": 12, "text": "Patient Information\nI have been having stomach \npain, feeling nauseous, and \nI have a slight fever.\nDiagnostic Result\nIt‚Äôs appendicitis\nMedical RAG\nSymptom Profile\n{‚ÄúId‚Äù: 0,\n‚ÄúDescription‚Äù: Fever,\n‚ÄúImportance‚Äù: Moderate}\nLow-grade fever that may \nrise as the condition \nprogresses\nKeyword\nCompare\nConfidence Estimation\n{‚ÄúId‚Äù: 0,\n‚ÄúDescription‚Äù: Fever,\n‚ÄúImportance‚Äù: Moderate,\n‚ÄúSupport‚Äù: Supported,\n‚ÄúEvidence‚Äù: I have a \nsight fever.\n‚Ä¶\n}\nConfidence: 30\n1\n2\n2\n3\n3\nFig. 6: The main proposed MedConf framework. This framework consists three steps: ‚ë†diagnostic\ngeneration, ‚ë°symptoms generation, and ‚ë¢confidence generation.\nreflects the reliability of a diagnosis d generated by\nan LLM, based on the comprehensiveness and sup-\nport of available patient evidence. As illustrated\nin Fig. 6, MedConf comprises three sequential\nsteps: diagnostic generation, symptom generation,\nand confidence generation. We also provide all\nprompts used in MedConf in the Fig. A5-A7 and\na case study for MenConf in the supplementary\nmaterial and Fig. A9-A13.\n5.1 Diagnostic Generation\nThe diagnostic generation step is responsible for\nproducing diagnostic results. Given patient infor-\nmation I and LLM f, the model utilizes the\ndiagnosis prompt Promptd to generate diagnostic\nassessments d:\nd = f(Promptd, I)\n(5)\nFor instance, the patient information I in Fig. 6\nis ‚ÄúI have been having stomach pain, feeling\nnauseous, and I have a slight fever‚Äù, and the\ngenerated diagnosis d is ‚ÄúIt‚Äôs appendicitis‚Äù.\n5.2 Symptom Generation\nThe symptom generation step operates through\na two-phase process to create a comprehensive\nsymptom profile for the predicted diagnosis.\nKnowledge Retrieval Phase: The process\nfirst utilizes a RAG system to identify and retrieve\nthe relevant medical passages R associated with\nthe generated diagnosis d:\nR = RAG(d, D)\n(6)\nThe RAG system enables the LLMs to search rel-\nevant literature to integrate domain knowledge\nin their response. It operates through a three-\nstage process: First, given the diagnosis d, the\nLLM extracts the primary diagnostic keyword.\nIn the example shown in Fig. 6, the keyword\nis ‚Äúappendicitis‚Äù. Second, the medical corpus D,\ncomprising PubMed, StatPearls, Textbooks, and\nWikipedia, is preprocessed by splitting documents\ninto manageable chunks [51]. Third, we employ\nBM25 [52], a lexical search algorithm, as the\nretriever to identify the most semantically relevant\npassages based on keyword matching and term\nfrequency analysis. The number of retrieved pas-\nsages is determined through empirical validation\nto balance information comprehensiveness with\ncomputational efficiency. In our setting, we choose\nthe 15 most relevant passages as the RAG results.\nAs shown in the example in Fig. 6, a passage\n‚ÄúLow-grade fever that may rise as the condition\nprogresses‚Äù is retrieved from the corpus.\nSymptom Profile Generation Phase: Fol-\nlowing the RAG step, the LLM generates struc-\ntured symptom profiles S\nto summarize the\nretrieved medical knowledge:\nS = f(Prompts, R, d)\n(7)\nwhere Prompts is the symptom profile genera-\ntion prompt. To ensure the generated content is\nsuitable for subsequent processing, the symptom\ngeneration prompt instructs the model to con-\nvert retrieved passages into a standardized JSON\nformat with the following schema:\n12\n"}, {"page": 13, "text": "[\n{\n\"id\": index number,\n\"description\": symptom description,\n\"importance\": Strong|Moderate|Weak\n},\n...\n]\nEach symptom entry includes a unique index num-\nber, a concise symptom description, and an impor-\ntance classification that reflects the symptom‚Äôs\ndiagnostic significance for the given condition.\nFor example, the retrieved passage about fever is\nconverted into ‚Äú{‚Äúid‚Äù: 0, ‚Äúdescription‚Äù: ‚ÄúFever‚Äù,\n‚Äúimportance‚Äù: ‚ÄúModerate‚Äù}‚Äù.\n5.3 Confidence Generation\nThe\nconfidence\ngeneration\nprocess\noperates\nthrough systematic evidence analysis in two dis-\ntinct phases:\nEvidence Mapping Phase: For each symp-\ntom si ‚ààS, the model evaluates its relationship\nwith patient information I using three relationship\ncategories:\n1. Supported: Patient information explicitly men-\ntions the symptom or closely related clinical\nmanifestations.\n2. Missing: The symptom is absent from patient\ninformation despite being expected for the\ndiagnosis, indicating incomplete information.\n3. Contradictory: Patient information explicitly\ncontradicts the expected symptom presenta-\ntion.\nFor supported and contradictory cases, relevant\nstatements are extracted from patient informa-\ntion as evidence. This analysis is also generated\nin a structured format to align with the symptom\nprofiles, ensuring systematic evaluation of each\nsymptom.\nContinuing with our running example from\nFig. 6, the symptom ‚Äúfever‚Äù is compared against\nthe patient statement ‚ÄúI have a slight fever‚Äù. Since\nthis represents explicit symptom mention, it is\ncategorized as Supported with the evidence text.\nScore Aggregation Phase: The model sum-\nmarizes evidence from all symptoms to produce\nthe final confidence score. The model is guided\nto consider both the proportion of supported,\nmissing, and contradictory symptoms and their\nrelative importance weights, with higher confi-\ndence assigned when critical symptoms are well-\nsupported and fewer missing or contradictions are\npresent. The overall confidence generation process\nis formulated as:\nCMedConf = f(Promptc, S, I, d)\n(8)\nwhere Promptc is the prompt for confidence score\ngeneration. For our example in Fig. 6, although\nthe symptom fever is supported by evidence, there\nare still other decisive symptoms that are not\nsupported and are missing. Therefore, the model\ninference is that the final confidence score is 30.\n6 Experimental Results\nIn this section, we conduct comprehensive evalu-\nations of MedConf. First, we compare MedConf\nagainst existing confidence estimation methods\nto assess its performance across various model-\ndataset combinations. Second, we validate Med-\nConf‚Äôs robustness against irrelevant information\ninterference, which is a critical requirement for\nreliable deployment in clinical settings where\nextraneous information may be present. Third,\nwe evaluate the effectiveness of integrating Med-\nConf with healthcare agents to assess its practical\nutility in interactive diagnostic scenarios. Finally,\nwe conduct ablation studies to quantify the con-\ntribution of each key design component within\nMedConf.\n6.1 Comparison with Existing\nMethods\nWe systematically evaluate MedConf against the\nbest-performing baseline methods of token-level,\nconsistency-level, and self-verbalized categories\nacross all model-dataset combinations. As demon-\nstrated in Tab. 3, MedConf exhibits consistent and\nsubstantial superiority across every evaluated con-\nfiguration, establishing its robustness across dif-\nferent model architectures and medical datasets.\nConfidence-Accuracy\nCorrelation:\nFor\nconfidence-accuracy correlation, MedConf demon-\nstrates exceptional performance with Pearson cor-\nrelation coefficients ranging from 0.936 to 0.989.\nIt achieves either the highest correlation or per-\nforms comparably to the best baseline across all\n13\n"}, {"page": 14, "text": "conditions. This superior alignment indicates that\nMedConf‚Äôs confidence scores are more calibrated\nand trustworthy indicators of prediction accuracy.\nDiscriminative Ability: MedConf achieves\nnotable improvements in discriminative capability,\nwith an average AUROC enhancement of 0.056\ncompared to the best-performing baseline meth-\nods across all configurations. Specifically, Med-\nConf attains AUROC scores ranging from 0.672\nto 0.796, consistently outperforming competing\napproaches by margins of 0.030 to 0.126 depend-\ning on the model-dataset combination. These\nimprovements demonstrate MedConf‚Äôs superior\nability to distinguish between correct and incor-\nrect predictions, establishing it as a more reli-\nable confidence estimation method for medical AI\napplications.\nGeneralizability:\nThe\nconsistent\nperfor-\nmance improvements across three distinct medical\ndatasets (DDXPlus , MediTOD, and MedQA) and\ntwo different model architectures (Llama-3.1 and\nGPT-4.1) demonstrate MedConf‚Äôs broad applica-\nbility and reliability within the medical domain.\nThese results collectively establish that MedConf\ngenerates more reliable and well-calibrated con-\nfidence scores, providing enhanced stability and\ntrustworthiness compared to existing confidence\nestimation methods in medical LLMs applications.\n6.2 Robustness to Irrelevant\nInformation\nWe conduct a robustness analysis comparing\nMedConf against representative baseline methods\nfrom different confidence estimation categories:\nASP (token-level), PoC (consistency-level), and\nCE (self-verbalized). This evaluation is performed\nusing Llama-3.1 on the DDXPlus dataset to assess\neach method‚Äôs resistance to irrelevant information\ninterference, a critical factor for reliable clinical\ndeployment.\nExperimental Setup: To simulate realis-\ntic clinical scenarios where additional but non-\ndiagnostic information may be present, we ran-\ndomly select 1-2 conversational turns from doctor-\npatient dialogues and employ GPT-4.1 to generate\nsemantically equivalent paraphrased content with\nvaried linguistic expressions. This paraphrased\ncontent is appended to the model input, creating\ncontrolled conditions where the added informa-\ntion provides no additional diagnostic value while\npotentially introducing linguistic variations that\ncould affect confidence estimation stability.\nWe evaluate confidence scores across three\ndistinct irrelevant information conditions using\nidentical random seeds to ensure fair compari-\nson. Robustness is quantified using the Coefficient\nof Variation (CV), where CV group measures\nvariability among mean confidence scores across\nthe three experimental groups, and CV sample\ncalculates the average CV across individual sam-\nple instances. Lower CV values indicate greater\nrobustness and stability.\nResults and Analysis: As demonstrated in\nTab. 4, baseline methods exhibit substantial vul-\nnerability to irrelevant information interference.\nThe CE method shows particularly poor stability\nwith CV values of 11.655 (group-level) and 47.575\n(sample-level), likely attributable to the inher-\nent sensitivity of self-verbalized approaches to\nprompt variations and linguistic nuances. In con-\ntrast, MedConf demonstrates exceptional robust-\nness with significantly lower variability scores (CV\ngroup: 2.343, CV sample: 12.401). This represents\nsubstantial improvements of 0.237 and 2.857 over\nASP, 2.515 and 11.924 over PoC, and remarkable\nimprovements of 9.312 and 35.174 over the CE\nmethod.\nMedConf‚Äôs superior robustness stems from\nits fundamental design principle: MedConf is\nbased on structured patient information analysis\nrather than surface-level linguistic features. Since\nsemantically equivalent but linguistically varied\ncontent does not alter the underlying patient\ninformation, MedConf maintains stable confidence\nscores regardless of presentation variations. These\nresults establish MedConf‚Äôs superior predictabil-\nity and robustness across varying input scenar-\nios‚Äîessential characteristics for reliable deploy-\nment in medical AI applications where consistency\nand stability are important for supporting critical\nclinical decision-making processes.\n6.3 Integration with Healthcare\nAgent\nWe integrate MedConf and three representa-\ntive baseline methods (ASP, PoC, and CE)\ninto healthcare agents to evaluate their effec-\ntiveness\nin\nimproving\ndiagnostic\nperformance\nwithin interactive clinical scenarios. This evalua-\ntion assesses how different confidence estimation\n14\n"}, {"page": 15, "text": "Table 3: Performance comparison of MedConf against the best-performing token-level, consistency-level,\nand self-verbalized methods across Llama-3.1 and GPT-4.1 on DDXPlus, MediTOD, and MedQA.\n(a) Llama-3.1 and DDXPlus\nMethod\nType\nAUROC\nPearson\nMedConf\nSelf-Verbalized\n0.722\n0.968\nPMI\nToken-Level\n0.593\n0.829\nSemSim (BERT)\nConsistency-Level\n0.652\n0.733\nCE\nSelf-Verbalized\n0.670\n0.870\n(b) Llama-3.1 and MediTOD\nMethod\nType\nAUROC\nPearson\nMedConf\nSelf-Verbalized\n0.687\n0.943\nConditional PMI\nToken-Level\n0.560\n0.734\nSemSim (BERT)\nConsistency-Level\n0.628\n0.491\nCE\nSelf-Verbalized\n0.636\n0.895\n(c) Llama-3.1 and MedQA\nMethod\nType\nAUROC\nPearson\nMedConf\nSelf-Verbalized\n0.796\n0.979\nEntropy\nToken-Level\n0.766\n0.905\nMC-NSE\nConsistency-Level\n0.685\n0.988\nCoT CE\nSelf-verbalized\n0.641\n0.935\n(d) GPT-4.1 and DDXPlus\nMethod\nType\nAUROC\nPearson\nMedConf\nSelf-Verbalized\n0.795\n0.936\nSemSim (BERT)\nConsistency-Level\n0.622\n0.785\nCE\nSelf-Verbalized\n0.682\n0.989\n(e) GPT-4.1 and MediTOD\nMethod\nType\nAUROC\nPearson\nMedConf\nSelf-Verbalized\n0.672\n0.981\nSemSim (BERT)\nConsistency-Level\n0.593\n0.829\nCE\nSelf-Verbalized\n0.628\n0.981\n(f) GPT-4.1 and MedQA\nMethod\nType\nAUROC\nPearson\nMedConf\nSelf-Verbalized\n0.690\n0.989\nEcc\nConsistency-Level\n0.667\n0.906\nCoT CE\nSelf-Verbalized\n0.642\n0.987\nTable 4: Robustness evaluation of confidence\nestimation methods to irrelevant information.\nCoefficient of variation (CV) measures perfor-\nmance stability of mean and sample level.\nMethod\nType\nCV group CV sample\nMedConf\nSelf-Verbalized\n2.343\n12.401\nASP\nToken-Level\n2.580\n15.258\nPoC\nConsistency-Level\n4.858\n24.325\nCE\nSelf-Verbalized\n11.655\n47.575\nTable\n5: Performance of confidence estimation\nmethods integrated into healthcare agents. # Utter-\nances represents the average utterances in a consul-\ntation, and accuracy indicates diagnostic accuracy.\nMethod\nType\n# Utterances Accuracy (%)\nMedConf\nSelf-Verbalized\n17.84\n36\nASP\nToken-Level\n5.20\n24\nPoC\nConsistency-Level\n18.00\n30\nCE\nSelf-Verbalized\n25.04\n36\nmethods impact both diagnostic accuracy and\ninteraction efficiency in practical healthcare appli-\ncations.\nExperimental Setup: Following the setup of\nRen et al. [53], we implement healthcare agents\nthat employ confidence score-based decision mak-\ning rather than model planning approaches. The\nagent determines the next action as either patient\ninquiry or diagnosis based on confidence score\nthresholds: the agent continues patient inquiry\nwhen confidence scores fall below a predeter-\nmined threshold and proceeds to diagnosis when\nscores exceed this threshold. We deploy these\nagents in simulated clinical consultations with\nvirtual patients, systematically evaluating each\nconfidence estimation method‚Äôs impact on diag-\nnostic performance. The effectiveness is measured\nthrough two key metrics: diagnostic accuracy (per-\ncentage of correct diagnoses) and average dia-\nlogue length (measured in utterances per consulta-\ntion), which together capture the critical trade-off\nbetween diagnostic quality and resource efficiency\nin clinical settings.\nResults and Analysis: As shown in Tab. 5,\nthe confidence estimation methods exhibit dis-\ntinct behavioral patterns that directly impact\nboth diagnostic accuracy and interaction effi-\nciency. The ASP method demonstrates a tendency\nto generate inappropriately high confidence scores\nduring early interaction stages, resulting in pre-\nmature diagnostic decisions with only 5.20 average\nutterances per consultation. This aggressive early\ntermination leads to suboptimal diagnostic accu-\nracy of 24%, indicating insufficient information\ngathering for reliable diagnosis. Conversely, the\nCE method produces high confidence scores only\nwhen substantial patient information becomes\navailable, leading to extended dialogues aver-\naging 25.04 utterances per consultation. While\nthis conservative approach enables CE to achieve\nhigher diagnostic accuracy (36%) compared to\nASP, it requires significantly more computational\nresources and consultation time due to prolonged\ninteractions, potentially limiting practical applica-\nbility in resource-constrained clinical settings. The\nPoC method demonstrates intermediate behavior\nwith 18.00 average utterances and 30% accu-\nracy, showing moderate performance across both\n15\n"}, {"page": 16, "text": "Table 6: The results of ablation studies.\nMethod\nAUROC\nAUPRC\nPearson\nSpearman\nMedConf\n0.772\n0.708\n0.968\n0.943\nw/o Symptom Profile\n0.657\n0.645\n0.786\n0.657\nw/o RAG\n0.670\n0.588\n0.939\n0.771\nw/o Structure Format\n0.688\n0.692\n0.907\n0.943\nw/o Importance Level\n0.639\n0.601\n0.959\n0.943\nefficiency and accuracy metrics but failing to\noptimize either dimension effectively.\nMedConf achieves superior performance by\nstriking an optimal balance between diagnostic\naccuracy and interaction efficiency. With 17.84\naverage utterances per consultation, MedConf\nmaintains computational efficiency comparable\nto PoC while achieving the highest diagnostic\naccuracy of 36%‚Äîmatching CE‚Äôs performance\nwith significantly reduced resource requirements.\nThese results demonstrate that MedConf‚Äôs well-\ncalibrated confidence scores enable more informed\ndecision-making in healthcare agents, leading\nto appropriately timed diagnostic decisions that\nmaximize accuracy while minimizing unnecessary\nprolonged interactions.\n6.4 Ablation Studies\nIn this section, we conduct systematic ablation\nexperiments to investigate the contribution of key\ndesign components within MedConf. These exper-\niments isolate individual components to quantify\ntheir specific impact on confidence estimation per-\nformance. The experimental results are presented\nin Tab. 6, evaluated using Llama-3.1 on the DDX-\nPlus dataset across multiple performance metrics,\nincluding AUROC, AUPRC, Pearson correlation,\nand Spearman correlation.\nTwo-Step Reasoning Process: We first\nevaluate the impact of MedConf‚Äôs symptom-\nconfidence\ntwo-step\nreasoning\nframework\nby\nremoving the intermediate symptom profile gen-\neration step. In this ablation variant (w/o Symp-\ntom Profile), we directly generate confidence\nscores using RAG-retrieved results, bypassing the\nsymptom profile generation phase. The results\ndemonstrate substantial performance degradation\nacross all metrics, with AUROC declining from\n0.772 to 0.657, AUPRC dropping from 0.708 to\n0.645, and correlation coefficients decreasing sig-\nnificantly (Pearson: 0.968 to 0.786, Spearman:\n0.943 to 0.657). These findings reveal that mod-\nels struggle to effectively utilize raw RAG retrieval\nresults as the direct foundation for confidence\nreasoning. The intermediate symptom profile gen-\neration step addresses this limitation by trans-\nforming fragmented retrieval information into\ncoherent, structured representations that are more\namenable to model interpretation, thereby sub-\nstantially enhancing the reasoning capability for\naccurate confidence score generation.\nRAG Integration: Secondly, we evaluate\nMedConf‚Äôs performance without external knowl-\nedge retrieval (w/o RAG) to assess the neces-\nsity of incorporating medical knowledge bases.\nRemoving RAG causes substantial performance\ndegradation, with AUROC declining from 0.772\nto 0.670, AUPRC decreasing from 0.708 to 0.588,\nPearson correlation dropping from 0.968 to 0.939,\nand Spearman correlation decreasing from 0.943\nto 0.771. These findings indicate that external\nknowledge retrieval is crucial for achieving optimal\ndiscriminative capability in medical confidence\nestimation, as it provides essential domain-specific\ncontext that enhances the model‚Äôs ability to assess\nprediction reliability accurately.\nStructured\nFormat\nand\nImportance-\nLevel\nAnnotations:\nFinally,\nwe\nexamine\nthe contribution of structured formatting and\nimportance-level\nannotations\nduring\nsymptom\nprofile generation through two separate ablations.\nRemoving structured format (w/o Structure For-\nmat) results in moderate performance decline\n(AUROC from 0.772 to 0.688, AUPRC from 0.708\nto 0.692), while eliminating importance level\nannotations (w/o Importance Level) causes more\nsubstantial degradation in discriminative perfor-\nmance (AUROC from 0.772 to 0.639, AUPRC\nfrom 0.708 to 0.601). The experimental results\n16\n"}, {"page": 17, "text": "indicate that the structured format improves\nthe interpretability and recognition capability\nof symptom profiles, preventing information loss\nduring\nsubsequent\nreasoning\nprocesses.\nMore\ncritically, importance level annotations provide\nessential weighting information that enables the\nmodel to appropriately consider each symptom‚Äôs\ndifferential impact on confidence estimation based\non clinical significance.\n7 Conclusion and Future Work\nReliable confidence estimation is essential for med-\nical LLMs to prevent misdiagnosis caused by\npremature conclusions. In this work, we intro-\nduce the first comprehensive benchmark that eval-\nuates 27 confidence estimation methods across\nthree medical datasets under varying levels of\ninformation, assessing both their correlation with\naccuracy and their discriminative capability. Our\nresults show that existing methods suffer from\ninstability in medical tasks, driven by method-\nological limitations and the unique requirement to\nassess not only diagnostic accuracy but also the\ncompleteness of patient information. To address\nthese gaps, we propose MedConf, a diagnostic\nevidence‚Äìbased self-verbalized method that lever-\nages retrieval-augmented generation to analyze\nthe relationships between patient information and\nsymptom profiles. MedConf achieves state-of-the-\nart performance, demonstrating superior accuracy\ncorrelation, discriminative power, and robustness,\nwhile preserving accuracy and efficiency when\ndeployed within healthcare agents.\nLimitation and Future Work: While the\nbenchmark and method presented in this paper\nare promising, several limitations remain to be\naddressed in future work. First, our evaluation\nfocuses solely on diagnostic tasks. Confidence esti-\nmation, however, is also important for other medi-\ncal applications such as medical report generation\nand clinical note summarization, which introduce\ndifferent challenges. These tasks are beyond the\ncurrent scope, but we plan to extend the bench-\nmark to cover a broader range of medical appli-\ncations. Second, although our MedConf method\nachieves state-of-the-art performance, its multi-\nstep reasoning process introduces additional com-\nputational overhead and latency compared to sim-\npler token-level approaches. This may limit real-\ntime deployment in some resource-constrained\nclinical settings. To address this, we plan to inves-\ntigate efficiency optimization techniques, includ-\ning caching frequently accessed medical knowledge\nand developing lightweight symptom profile gen-\neration models, to reduce inference time while\nmaintaining performance. Finally, all experiments\nin this paper are conducted in simulated environ-\nments using benchmark datasets rather than in\nreal clinical settings. While these datasets reflect\nreal-world data and provide a practical alterna-\ntive to clinical testing at this stage‚Äîhelping to\nreduce the burden on medical professionals and\npatients as well as the time required‚Äîthey cannot\nfully capture the complexities of real-world clini-\ncal practice. As future work, we aim to conduct\nprospective clinical trials where healthcare profes-\nsionals interact directly with confidence-aware AI\nsystems to assess their practical utility and the\ninterpretability of confidence scores.\nAuthor contribution\nZ.R., Y.Z., B.Y., and G.M. conceived and designed\nthis study. Z.R. developed code and conducted\nexperiments. Z.R., Y.Z., B.Y., S.L., and D.T. con-\ntributed to write the manuscript. Y.Z., B.Y, S.L.,\nand D.T. supervised this paper. All authors have\nread and approved the manuscript.\nAcknowledgements\nThis study received no funding.\nFunding\nThis study received no funding.\nCompeting interests\nThe authors declare no competing interests.\nData availability\nThe DDXPlus, MediTOD, and MedQA datasets\nare publicly available.\n17\n"}, {"page": 18, "text": "Code availability\nThe underlying code for this study is not publicly\navailable but may be made available to quali-\nfied researchers on reasonable request from the\ncorresponding author.\nReferences\n[1] Meyer, A.N., Payne, V.L., Meeks, D.W.,\nRao, R., Singh, H.: Physicians‚Äô diagnostic\naccuracy, confidence, and resource requests:\na vignette study. JAMA internal medicine\n173(21), 1952‚Äì1958 (2013)\n[2] Ng, C., Palmer, C.: Analysis of diagnostic\nconfidence and diagnostic accuracy: a unified\nframework. The British journal of radiology\n80(951), 152‚Äì160 (2007)\n[3] Bhise, V., Rajan, S.S., Sittig, D.F., Mor-\ngan, R.O., Chaudhary, P., Singh, H.: Defin-\ning and measuring diagnostic uncertainty in\nmedicine: a systematic review. Journal of gen-\neral internal medicine 33(1), 103‚Äì115 (2018)\n[4] Shorinwa, O., Mei, Z., Lidard, J., Ren, A.Z.,\nMajumdar, A.: A survey on uncertainty\nquantification of large language models: Tax-\nonomy, open research challenges, and future\ndirections. ACM Computing Surveys (2025)\n[5] Liu, X., Chen, T., Da, L., Chen, C., Lin, Z.,\nWei, H.: Uncertainty quantification and con-\nfidence calibration in large language models:\nA survey. In: Proceedings of the 31st ACM\nSIGKDD Conference on Knowledge Discov-\nery and Data Mining V. 2, pp. 6107‚Äì6117\n(2025)\n[6] Savage, T., Wang, J., Gallo, R., Boukil, A.,\nPatel, V., Safavi-Naini, S.A.A., Soroush, A.,\nChen, J.H.: Large language model uncer-\ntainty proxies: discrimination and calibration\nfor medical diagnosis and treatment. Journal\nof the American Medical Informatics Associ-\nation 32(1), 139‚Äì149 (2025)\n[7] Atf, Z., Safavi-Naini, S.A.A., Lewis, P.R.,\nMahjoubfar, A., Naderi, N., Savage, T.R.,\nSoroush, A.: The challenge of uncertainty\nquantification of large language models in\nmedicine. arXiv preprint arXiv:2504.05278\n(2025)\n[8] Gu, B., Desai, R.J., Lin, K.J., Yang, J.:\nProbabilistic medical predictions of large lan-\nguage models. npj Digital Medicine 7(1), 367\n(2024)\n[9] Omar, M., Agbareia, R., Glicksberg, B.S.,\nNadkarni, G.N., Klang, E.: Benchmarking the\nconfidence of large language models in clinical\nquestions. MedRxiv, 2024‚Äì08 (2024)\n[10] Gao,\nY.,\nMyers,\nS.,\nChen,\nS.,\nDligach,\nD., Miller, T., Bitterman, D.S., Chen, G.,\nMayampurath, A., Churpek, M.M., Afshar,\nM.: Uncertainty estimation in diagnosis gen-\neration from large language models: next-\nword probability is not pre-test probability.\nJAMIA open 8(1), 154 (2025)\n[11] Chen, Z., Li, P., Dong, X., Hong, P.: Uncer-\ntainty quantification for clinical outcome pre-\ndictions with (large) language models. arXiv\npreprint arXiv:2411.03497 (2024)\n[12] Mukaka, M.M.: A guide to appropriate use\nof correlation coefficient in medical research.\nMalawi medical journal 24(3), 69‚Äì71 (2012)\n[13] Hanley, J.A., McNeil, B.J.: The meaning and\nuse of the area under a receiver operating\ncharacteristic (roc) curve. Radiology 143(1),\n29‚Äì36 (1982)\n[14] Davis, J., Goadrich, M.: The relationship\nbetween precision-recall and roc curves. In:\nProceedings of the 23rd International Con-\nference on Machine Learning, pp. 233‚Äì240\n(2006)\n[15] Fansi Tchango, A., Goel, R., Wen, Z., Mar-\ntel, J., Ghosn, J.: Ddxplus: A new dataset\nfor automatic medical diagnosis. Advances\nin neural information processing systems 35,\n31306‚Äì31318 (2022)\n[16] Saley, V., Saha, G., Das, R., Raghu, D.,\net al.: Meditod: An english dialogue dataset\nfor medical history taking with comprehen-\nsive annotations. In: Proceedings of the 2024\n18\n"}, {"page": 19, "text": "Conference on Empirical Methods in Nat-\nural Language Processing, pp. 16843‚Äì16877\n(2024)\n[17] Jin, D., Pan, E., Oufattole, N., Weng, W.-H.,\nFang, H., Szolovits, P.: What disease does this\npatient have? a large-scale open domain ques-\ntion answering dataset from medical exams.\nApplied Sciences 11(14), 6421 (2021)\n[18] Lewis, P., Perez, E., Piktus, A., Petroni,\nF., Karpukhin, V., Goyal, N., K¬®uttler, H.,\nLewis,\nM.,\nYih,\nW.-t.,\nRockt¬®aschel,\nT.,\net al.: Retrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in\nneural information processing systems 33,\n9459‚Äì9474 (2020)\n[19] Jiang, Z., Araki, J., Ding, H., Neubig, G.:\nHow can we know when language models\nknow? on the calibration of language models\nfor question answering. Transactions of the\nAssociation for Computational Linguistics 9,\n962‚Äì977 (2021)\n[20] Huang, Y., Song, J., Wang, Z., Zhao, S.,\nChen, H., Juefei-Xu, F., Ma, L.: Look before\nyou leap: An exploratory study of uncertainty\nanalysis for large language models. IEEE\nTransactions on Software Engineering (2025)\n[21] Manakul, P., Liusie, A., Gales, M.: Selfcheck-\ngpt: Zero-resource black-box hallucination\ndetection for generative large language mod-\nels. In: Proceedings of the 2023 Conference\non Empirical Methods in Natural Language\nProcessing, pp. 9004‚Äì9017 (2023)\n[22] Ren, J., Luo, J., Zhao, Y., Krishna, K., Saleh,\nM., Lakshminarayanan, B., Liu, P.J.: Out-\nof-distribution detection and selective gen-\neration for conditional language models. In:\nThe Eleventh International Conference on\nLearning Representations (2023)\n[23] Takayama, J., Arase, Y.: Relevant and infor-\nmative response generation using pointwise\nmutual information. In: Proceedings of the\nFirst Workshop on NLP for Conversational\nAI, pp. 133‚Äì138 (2019)\n[24] Darrin, M., Piantanida, P., Colombo, P.:\nRainproof: An umbrella to shield text gen-\nerators from out-of-distribution data. arXiv\npreprint arXiv:2212.09171 (2022)\n[25] Fadeeva, E., Rubashevskii, A., Shelmanov,\nA., Petrakov, S., Li, H., Mubarak, H., Tsym-\nbalov, E., Kuzmin, G., Panchenko, A., Bald-\nwin, T., et al.: Fact-checking the output\nof large language models via token-level\nuncertainty quantification. arXiv preprint\narXiv:2403.04696 (2024)\n[26] Duan, J., Cheng, H., Wang, S., Zavalny, A.,\nWang, C., Xu, R., Kailkhura, B., Xu, K.:\nShifting attention to relevance: Towards the\npredictive uncertainty quantification of free-\nform large language models. In: Proceedings\nof the 62nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume\n1: Long Papers), pp. 5050‚Äì5063 (2024)\n[27] Lin, Z., Trivedi, S., Sun, J.: Generating\nwith confidence: Uncertainty quantification\nfor black-box large language models. Transac-\ntions on Machine Learning Research (2023)\n[28] Chen, J., Mueller, J.: Quantifying uncer-\ntainty in answers from any language model\nand enhancing their trustworthiness. In: Ku,\nL.-W., Martins, A., Srikumar, V. (eds.) Pro-\nceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers) (2024)\n[29] Kuhn, L., Gal, Y., Farquhar, S.: Semantic\nuncertainty: Linguistic invariances for uncer-\ntainty estimation in natural language genera-\ntion. arXiv preprint arXiv:2302.09664 (2023)\n[30] Lin, S., Hilton, J., Evans, O.: Teaching mod-\nels to express their uncertainty in words.\narXiv preprint arXiv:2205.14334 (2022)\n[31] Tian, K., Mitchell, E., Zhou, A., Sharma,\nA., Rafailov, R., Yao, H., Finn, C., Man-\nning, C.: Just ask for calibration: Strategies\nfor eliciting calibrated confidence scores from\nlanguage models fine-tuned with human feed-\nback. In: Bouamor, H., Pino, J., Bali, K.\n(eds.) Proceedings of the 2023 Conference\non Empirical Methods in Natural Language\nProcessing (2023)\n19\n"}, {"page": 20, "text": "[32] Kadavath,\nS.,\nConerly,\nT.,\nAskell,\nA.,\nHenighan, T., Drain, D., Perez, E., Schiefer,\nN., Hatfield-Dodds, Z., DasSarma, N., Tran-\nJohnson, E., et al.: Language models (mostly)\nknow\nwhat\nthey\nknow.\narXiv\npreprint\narXiv:2207.05221 (2022)\n[33] Kompa, B., Snoek, J., Beam, A.L.: Second\nopinion needed: communicating uncertainty\nin medical machine learning. NPJ Digital\nMedicine 4(1), 4 (2021)\n[34] Kang, D.Y., DeYoung, P.N., Tantiongloc,\nJ., Coleman, T.P., Owens, R.L.: Statistical\nuncertainty quantification to augment clini-\ncal decision support: a first implementation\nin sleep medicine. NPJ digital medicine 4(1),\n142 (2021)\n[35] Park, S., Pettigrew, M.F., Cha, Y.J., Kim,\nI.-H., Kim, M., Banerjee, I., Barnfather,\nI., Clemenceau, J.R., Jang, I., Kim, H., et\nal.: Deep gaussian process with uncertainty\nestimation for microsatellite instability and\nimmunotherapy response prediction from his-\ntology. npj Digital Medicine 8(1), 294 (2025)\n[36] Mehrtash, A., Wells, W.M., Tempany, C.M.,\nAbolmaesumi, P., Kapur, T.: Confidence\ncalibration and predictive uncertainty esti-\nmation for deep medical image segmenta-\ntion. IEEE transactions on medical imaging\n39(12), 3868‚Äì3878 (2020)\n[37] Zou, K., Chen, Z., Yuan, X., Shen, X., Wang,\nM., Fu, H.: A review of uncertainty estima-\ntion and its application in medical imaging.\nMeta-Radiology 1(1), 100003 (2023)\n[38] L¬®ohr,\nT.,\nIngrisch,\nM.,\nH¬®ullermeier,\nE.:\nTowards aleatoric and epistemic uncertainty\nin medical image classification. In: Interna-\ntional Conference on Artificial Intelligence in\nMedicine, pp. 145‚Äì155 (2024). Springer\n[39] Abboud, Z., Lombaert, H., Kadoury, S.:\nSparse bayesian networks: Efficient uncer-\ntainty quantification in medical image analy-\nsis. In: International Conference on Medical\nImage Computing and Computer-Assisted\nIntervention, pp. 675‚Äì684 (2024). Springer\n[40] Isenegger, K., Dong, Y., Shang, M., Furst,\nJ., Stan-Raicu, D.: Characterizing and quan-\ntifying diagnostic (un) certainty in medical\nreports through natural language processing.\nIn: 2019 International Conference on Com-\nputational Science and Computational Intel-\nligence (CSCI), pp. 914‚Äì919 (2019). IEEE\n[41] Peluso, A., Danciu, I., Yoon, H.-J., Yusof,\nJ.M., Bhattacharya, T., Spannaus, A., Scha-\nefferkoetter, N., Durbin, E.B., Wu, X.-C.,\nStroup, A., et al.: Deep learning uncer-\ntainty quantification for clinical text clas-\nsification. Journal of biomedical informatics\n149, 104576 (2024)\n[42] Khandokar, I., Farghaly, O., Kothari, A.N.,\nDeshpande, P.: Towards precision diagnosis:\nIntegrating lexical analysis and deep learn-\ning for uncertainty detection and quantifi-\ncation in clinical reports. In: 2024 IEEE\n37th International Symposium on Computer-\nBased Medical Systems (CBMS), pp. 267‚Äì272\n(2024). IEEE\n[43] Hu, Z., Liu, C., Feng, X., Zhao, Y., Ng, S.-\nK., Luu, A.T., He, J., Koh, P.W.W., Hooi,\nB.: Uncertainty of thoughts: Uncertainty-\naware planning enhances information seeking\nin llms. Advances in Neural Information Pro-\ncessing Systems 37, 24181‚Äì24215 (2024)\n[44] Wu, J., Yu, Y., Zhou, H.-Y.: Uncertainty\nestimation\nof\nlarge\nlanguage\nmodels\nin\nmedical question answering. arXiv preprint\narXiv:2407.08662 (2024)\n[45] Qin, J., Liu, B., Nguyen, Q.D.: Enhanc-\ning\nhealthcare\nllm\ntrust\nwith\natypical\npresentations recalibration. arXiv preprint\narXiv:2409.03225 (2024)\n[46] Van Der Poel, L., Cotterell, R., Meister, C.:\nMutual information alleviates hallucinations\nin abstractive summarization. In: Proceed-\nings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp.\n5956‚Äì5965 (2022)\n[47] Malinin, A., Gales, M.: Uncertainty estima-\ntion in autoregressive structured prediction.\nIn: International Conference on Learning\n20\n"}, {"page": 21, "text": "Representations (2021)\n[48] OpenAI:\nIntroducing\nGPT-4.1\nin\nthe\nAPI.\nhttps://openai.com/index/\ngpt-4-1/[accessed: 2025-08-25] (2025)\n[49] Grattafiori,\nA.,\nDubey,\nA.,\nJauhri,\nA.,\nPandey, A., Kadian, A., Al-Dahle, A., Let-\nman, A., Mathur, A., Schelten, A., Vaughan,\nA., et al.: The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783 (2024)\n[50] Xiong, M., Hu, Z., Lu, X., LI, Y., Fu, J.,\nHe, J., Hooi, B.: Can llms express their\nuncertainty? an empirical evaluation of con-\nfidence elicitation in llms. In: The Twelfth\nInternational Conference on Learning Repre-\nsentations (2024)\n[51] Xiong, G., Jin, Q., Lu, Z., Zhang, A.: Bench-\nmarking retrieval-augmented generation for\nmedicine. In: Findings of the Association for\nComputational Linguistics ACL 2024, pp.\n6233‚Äì6251 (2024)\n[52] Robertson, S., Zaragoza, H., et al.: The\nprobabilistic relevance framework: Bm25 and\nbeyond. Foundations and Trends¬Æ in Infor-\nmation Retrieval 3(4), 333‚Äì389 (2009)\n[53] Ren, Z., Zhan, Y., Yu, B., Ding, L., Xu, P.,\nTao, D.: Healthcare agent: eliciting the power\nof large language models for medical con-\nsultation. npj Artificial Intelligence 1(1), 24\n(2025)\n21\n"}, {"page": 22, "text": "Appendix A\nCommon notations\nWe list common notations in Tab. A1 for mathematical definitions.\nTable A1: Common notations and descriptions.\nNotion\nDescription\nx\nInput sequence (patient information)\ny\nOutput sequence (generated diagnosis/response)\nyl\nToken at position l in the output sequence\ny<l\nAll tokens before position l\nL\nLength of generated sequence\nK\nNumber of sampled responses\nP(yl|y<l, x)\nProbability of token yl given previous tokens and input\nf(¬∑)\nLarge language model\nC\nConfidence score\nœà(¬∑)\nToken-level transformation function\nœï(¬∑)\nFinal transformation function\nwl\nWeight for token at position l\nLL\nl=1\nAggregation operation over tokens\nH(yl|y<l, x)\nEntropy at token position l\nN\nVocabulary size\nq\nUniform distribution over vocabulary\nŒ±\nHyperparameter for R¬¥enyi divergence\nRT (yl, y, x)\nToken relevance function\nÀúRT (yl, y, x)\nNormalized token relevance\nY = {y1, y2, . . . , yK}\nSet of K generated responses\ny(k)\nThe k-th sampled response\nyc\nMost frequent/consistent response\ne(¬∑)\nEmbedding function\nCos(¬∑, ¬∑)\nCosine similarity function\nCn\nSemantic cluster n\nÀúPn(x)\nAverage probability of cluster n\nS\nSimilarity matrix\nL\nGraph Laplacian matrix\nD\nDiagonal degree matrix\nŒªk\nk-th eigenvalue\nuk\nk-th eigenvector\nvj\nEmbedding vector for response j\nLk\nLength of sequence y(k)\nRS(y(j), x)\nSentence relevance function\nI\nPatient information\nd\nGenerated diagnosis\nD\nMedical corpus (PubMed, StatPearls, etc.)\nR\nRetrieved medical passages from RAG\nS\nSymptom profile (set of symptoms)\nPrompt\nPrompt template\nRAG(d, D)\nRetrieval-augmented generation function\n22\n"}, {"page": 23, "text": "Appendix B\nMore Details of Benchmark\nB.1\nDetails of Data Processing\nTo better align with real-world medical consultation scenarios, we process the datasets by converting\nthe model input format into doctor-patient dialogues or medical reports, and restructure the tasks as\nopen-end decision making.\nDDXPlus is a large-scale contains different diagnosis with ground truth pathology, symptoms and\nantecedents. It collect data in a structured format and provide categorical and multi-choice symp-\ntoms symptoms and antecedents for more detailed description. In our processing procedure, we convert\nstructured format information into doctor-patient dialogue format. Using GPT-4.1, we convert symp-\ntom information in the dataset into doctor inquiries, while the symptoms are transformed into patient\nresponses, converting binary and numerical information into patients‚Äô natural language expressions. The\nspecific prompts are shown in Fig. A1. The model will generate diagnostic results based on the generated\ndialogue.\nPrompt for input conversion of DDXPlus\nPrompt for doctor inquiry:\nYou are playing the role of a doctor. Refer to the question {question}, ask the patient the question\nin a doctor‚Äôs specialized form. Please do not change the original intent and do not include additional\ninformation and your response should contain only the question.\nQuestion:\nPrompt for patient answer:\nYou are playing the role of a patient. Refer to the question {question} and the result result, answer\nthe question in a patient‚Äôs colloquial manner. Please do not include add any information that isn‚Äôt\nmentioned and your response should contain only the answer.\nAnswer:\nFig. A1: Prompt for input conversion of DDXPlus.\nMediTOD comprises real-world doctor-patient dialogues with annotated complex relationships\nbetween dialogue content and corresponding clinical attributes. To select the effective dialogues, we retain\nthe dialogue with intents of ‚ÄúInform‚Äù, ‚ÄúInquire‚Äù, and ‚ÄúDiagnose‚Äù, while ignore dialogue for ‚ÄúChit-chat‚Äù\nand ‚Äúsalutations‚Äù. MedQA contains profession multiple-choice question. The question background of\nMedQA is a comprehensive medical report and we remove the option in the question to convert the task\nfrom multiple choice question to open-end decision making task.\nB.2\nDetails of Evaluated Methods\nB.2.1\nToken-level Methods\nAverage Sequence Probability (ASP) [20] computes the arithmetic mean of token probabilities across the\ngenerated sequence:\nCASP = 1\nL\nL\nX\nl=1\nP(yl | y<l, x)\n(A1)\n23\n"}, {"page": 24, "text": "Maximum Sequence Probability (MSP) [20] estimates the sequence-level confidence by selecting the\nhighest token probability within the generated sequence:\nCMSP =\nmax\nl=1,...,L P(yl | y<l, x)\n(A2)\nAdditional metrics analyze the probability distribution characteristics. Perplexity [22] measures the\nexponential of the average negative log-probability:\nCPerp = exp\n(\n‚àí1\nL\nL\nX\nl=1\nlog P(yl|y<l, x)\n)\n(A3)\nThe Entropy [21] computes the average entropy across all token positions in the generated sequence:\nCH = 1\nL\nL\nX\nl=1\nH(yl|y<l, x)\n(A4)\nwhere H(yl|y<l, x) represents the entropy of the token distribution at position l.\nTakayama and Arase [23] propose utilize Pointwise Mutual Information (PMI) between conditioned\nand unconditioned input to measure confidence:\nCPMI = 1\nL\nL\nX\nl=1\nlog\nP(yl|y<l)\nP(yl|y<l, x)\n(A5)\nVan Der Poel et al. [46] propose a modification called Conditional Pointwise Mutual Information\n(CPMI) to only evaluate those token have entropy above a threshold:\nCCPMI = 1\nL\nL\nX\nl=1\nlog P(yl|y<l, x)\n(A6)\n+ Œª\nL\nX\nl:H(yl|y<l,x)‚â•œÑ\nlog P(yl|y<l)\n(A7)\nwhere Œª is a changeable parameter.\nDarrin et al. [24] propose to use R¬¥enyi Divergence and Fisher-Rao distance to calculate the divergence\nbetween the distribution of each token and the uniform distribution. For N is the number of tokens in\nvocabulary and q is a uniform distribution over the vocabulary, the Renyi divergences is:\nCRD = 1\nL\nL\nX\nl=1\n1\nŒ± ‚àí1 log\nN\nX\ni=1\nP(yl|y<l, x)Œ±\nqŒ±‚àí1\ni\n(A8)\nwhile the Fisher-Rao is:\nCFRD = 1\nL\nL\nX\nl=1\n2\nœÄ arccos\nN\nX\ni=1\np\nP(yl|y<l, x) ¬∑ qi\n(A9)\nTokenSAR [26] first proposes that the contribution of different tokens are unequal. It utilize a token\nrelevance function to calculate the importance of each token:\nRT (yl, y, x) = 1 ‚àíg(x ‚à™y, x ‚à™y \\ {yl})\n(A10)\n24\n"}, {"page": 25, "text": "where g(¬∑, ¬∑) measures the semantic similarity between two sequences. The reweighted entropy is then\ncomputed as:\nCTokenSAR = ‚àí\nL\nX\nl=1\nÀúRT (yl, y, x) log P(yl|y<l, x)\n(A11)\nwhere ÀúRT (yl, y, x) =\nRT (yl,y,x)\nPL\ni=1 RT (yi,y,x) represents the normalized token relevance weights.\nClaim Conditioned Probability (CCP) [25] quantifies uncertainty by assessing how token substitutions\naffect semantic consistency. For each token position j, the method replaces token yj with alternative\ncandidates yk\nj sampled from the top-k predictions of the model‚Äôs output distribution. A Natural Language\nInference (NLI) model then evaluates the semantic relationship between the original sequence and each\nperturbed variant. The CCP score is computed as:\nCCCP =\nP\nk:NLI(yk\nj ,yj)=‚Äòentail‚Äô P(yk\nj |y<j, x)\nP\nk:NLI(yk\nj ,yj)‚àà{‚Äòentail‚Äô,‚Äòcontra‚Äô} P(yk\nj |y<j, x)\n(A12)\nwhere NLI(yk\nj , yj) = ‚Äòentail‚Äô denotes the NLI model predicts an entailment relation and NLI(yk\nj , yj) =\n‚Äòcontra‚Äô denotes that the NLI model classifies relationships as contradiction.\nB.2.2\nConsistency-level Methods\nPercentage of Consistency (PoC) [21] evaluates confidence by calculating the proportion of responses that\nmatch the most frequent response:\nCPoC =\nPK\ni=1 I(yi = yc)\nK\n(A13)\nwhere yc = arg maxy‚ààY\nPK\nj=1 I(yj = y).\nSemantic Similarity (SemSim) [28] measures confidence by computing the average cosine similarity\nbetween embeddings of all response pairs. It can be represented as:\nCSemSim =\n1\nK(K ‚àí1)\nK\nX\ni=1\nX\njÃ∏=i\nCos(e(yi), e(yj))\n(A14)\nwhere Cos(¬∑, ¬∑) denotes the cosine similarity function and e(¬∑) represents the embedding process that maps\nsequences to vector representations.\nLexical Similarity [21] measures confidence through lexical overlap between K sampled responses using\nn-gram metric (e.g., ROUGE and BLEU):\nCLexSim =\n1\nK(K ‚àí1)\nK\nX\ni=1\nX\njÃ∏=i\nLexicalSim(y(i), y(j))\n(A15)\nwhere LexicalSim computes n-gram overlap between responses y(i) and y(j). In our setting, we utilize\nROUGE-L as the lexical similarity metric.\nNumber of Semantic Sets (NumSet) [27] clusters semantically similar responses into non-overlapping\ngroups using NLI-based entailment relations. The confidence calculate the number of cluster in the\nresponse:\nCNumSet = 1 ‚àíNumClusters\nK\n(A16)\nwhere responses y(i) and y(i) are clustered together if ÀÜpentail(y(i), y(j))\n>\nÀÜpcontra(y(i), y(j)) and\nÀÜpentail(y(j), y(i)) > ÀÜpcontra(y(j), y(i)).\n25\n"}, {"page": 26, "text": "Sum of Eigenvalues of the Graph Laplacian (EigV) [27] constructs a similarity graph from K responses\nand analyzes eigenvalues of the normalized Laplacian matrix to quantify semantic diversity. For a\nsimilarity matrix S, the Laplacian for S is formula as:\nL = I ‚àíD‚àí1\n2 SD‚àí1\n2\n(A17)\nwhere D is a diagonal matrix and Œªk are the eigenvalues of matrix L. The confidence of EigV can be\nrepresented as:\nCEigV = 1 ‚àí\nK\nX\nk=1\nmax(0, 1 ‚àíŒªk)\n(A18)\nFurthermore, Degree Matrix (Deg) [27] extract confidence from the previous metioned diagonal matrix\nD and the confidence score is estimated by:\nCDeg = trace(D)\nK2\n(A19)\nEccentricity (Ecc) [27] quantifies confidence by measuring distance from the centroid in embedding\nspace derived from the similarity graph. This method leverages the eigenvectors u1, ¬∑ ¬∑ ¬∑ , uk corre-\nsponding to the k smallest eigenvalues of the graph Laplacian to construct informative embeddings\nvj = [u1,j, . . . , uk,j] for each response yj. The uncertainty score is calculated as the average distance from\nthe centroid in this embedding space:\nCEcc = 1 ‚àí‚à•[ÀúvT\n1 , . . . , ÀúvT\nK]‚à•2\n(A20)\nwhere Àúvj = vj ‚àí1\nK\nPK\n‚Ñì=1 v‚Ñì.\nOn the other method, several approaches integrate token probability data into consistency measures.\nMonte Carlo Sequence Entropy (MC-SE) [29] calculates entropy at the sequence level by averaging the\nnegative log-probabilities across multiple generated sequences:\nCMC-SE = ‚àí1\nK\nK\nX\nk=1\nlog P(y(k)|x)\n(A21)\nwhere P(y(k)|x) = QLk\nl=1 P(y(k)\nl\n|y(k)\n<l , x) represents the probability of the k-th generated sequence. To\nobtain a more reliable uncertainty measure, the sequence probabilities can be length-normalized [47]:\nÀÜCMC-SE = ‚àí1\nK\nK\nX\nk=1\n1\nLk\nlog P(y(k)|x)\n(A22)\nwhere Lk is the length of sequence y(k).\nSemantic Entropy (SE) [29] specifically targets the influence of semantically equivalent expressions on\nentropy calculations. It clusters generated responses into semantically similar groups Cn, n = 1, 2, . . . , N\nand averages the sequence probabilities within each cluster. The entropy calculated over these semantic\nclusters is formulated as:\nCSE = ‚àí\nN\nX\nn=1\n|Cn|\nK\nlog ÀúPn(x)\n(A23)\nwhere ÀúPn(x) = P\ny‚ààCn P(y|x) represents the average probability of cluster n.\n26\n"}, {"page": 27, "text": "For SentenceSAR [26], it reweigh the importance of each sentence. Through the sentence relevance\nfunction:\nRS(y(j), x) =\nX\nkÃ∏=j\ng(y(j), y(k))P(y(k)|x)\n(A24)\nit increase the probability of sentences that are more relevant than others:\nUSentSAR(x) = 1\nK\nK\nX\nk=1\n\u0012\nlog P(y(k)|x) + 1\nt RS(y(k), x)\n\u0013\n(A25)\nwhere t is the hyperparameter.\nBy changing the probability P(y | x) with TokenSAR in Eq. A11, we can obtain SAR method that\ncombine both TokenSAR and SentenceSAR.\nB.2.3\nSelf-verbalized Methods\nWe utilize three prompt strategies for Confidence Elicitation method, including vanilla prompt, chain-\nof-through prompt, and Tok-k prompt. The prompt details are shown in Fig. A2, Fig. A3, and Fig. A4,\nrespectively.\nPrompt for vanilla CE\nYour task is to rate the confidence of the proposed answer on a score from 0 to 100, where 0\nrepresents definitely not confidence and 100 represents definitely confidence. Note: The confidence\nindicates how likely you think your answer is true.\nPlease, only answer with your score in between square brackets (ex. [50]).\n===========\nScenario: dialogue\nAnswer: answer\nFig. A2: Prompt for vanilla CE.\nPrompt for CoT CE\nYour task is to rate the confidence of the proposed answer on a score from 0 to 100, where 0\nrepresents definitely not confidence and 100 represents definitely confidence. Note: The confidence\nindicates how likely you think your answer is true.\nProvide your explanation first and show your confidence in between square brackets (ex. [50]).\nThe answer format is:\nExplanation:\nConfidence:\n===========\nScenario: dialogue\nAnswer: answer\nFig. A3: Prompt for CoT CE.\n27\n"}, {"page": 28, "text": "Prompt for Top-k CE\nYour task is to rate the uncertainty of the proposed answer on a score from 0 to 100, where 0\nrepresents definitely uncertain and 100 represents definitely certain. Note: The confidence indicates\nhow likely you think your answer is true.\nProvide your 5 best guess of the confidence and answer with your score in between square brackets\n(ex. [50]). For example:\nG1:\n...\nG5:\n===========\nScenario: dialogue\nAnswer: answer\nNow, please provide your confidence.\nFig. A4: Prompt for Top-k CE.\nAppendix C\nDetails of MedConf\nIn the diagnostic generation step, given patient information, the model generate diagnosis with diagnosis\nprompt Promptd. The details of Promptd are shown in Fig. A5.\nDetails of diagnosis prompt\nRead the medical information below and determine the final diagnosis. Provide a specific diagnosis\nfor the case and label your Answer in square brackets.\n===========\nMedical information: {inquiry}\nFig. A5: Details of diagnosis prompt.\nFollowing this, RAG system identify and retrieve the relevant medical passages in database associated\nwith the generated diagnosis. The retrieve results are summarize to a JSON structured symptom profile\nwith symptom profile generation prompt Prompts. The details of Prompts are shown in Fig. A6.\nDetails of symptom profile generation prompt\nYou are a helpful medical expert, and your task is to generate structured diagnostic criteria for the following condition using the relevant\ndocuments. Each criterion should include a description and importance level and output in JSON format.\nCondition: {diagnosis}\nRelevant documents: {content}\nNow, Please output the diagnostic criteria in JSON format as follow:\n{\n‚Äúid‚Äù: Criterion ID, e.g., 1,\n‚Äúdescription‚Äù: ‚ÄùSpecific diagnostic criterion description‚Äù,\n‚Äúimportance‚Äù: ‚Äùstrong/moderate/weak‚Äù\n}\nImportance definitions:\n- strong: Core diagnostic criteria, absence severely impacts diagnosis\n- moderate: Important supportive criteria, helps confirm diagnosis\n- weak: Auxiliary criteria, increases diagnostic confidence but not essential\nFig. A6: Details of symptom profile generation prompt.\n28\n"}, {"page": 29, "text": "Finally, the confidence generation step utilize the confidence generation prompt Promptc to first\nanalyze the supportive, missing, and contradictory relationship between patient information and symptom\nprofile and then estimate confidence score. The details of Promptc are shown in Fig. A7.\nDetails of confidence generation prompt\nYou are a specialist with extensive medical knowledge. Your task is evaluate how well each diagnostic criterion is met based on the following\npatient-doctor conversation and then provide the confidence score. The confidence score is from 0 to 100, where 0 represents definitely not\nconfidence and 100 represents definitely confidence. Note: The confidence indicates how likely you think your answer is true.\nDiagnostic Criteria: {criteria}\nPatient Information: {inform}\nFirst, you need to analyze each criterion and generate evaluation results in the following JSON format:\n{\n‚Äúcriteria evaluation‚Äù: [\n{\n‚Äúid‚Äù: Criterion ID,\n‚Äúdescription‚Äù: Criterion description,\n‚Äúimportance‚Äù: strong/moderate/weak,\n‚Äúsupport level‚Äù: supported/contradicted/missing,\n‚Äúevidence‚Äù: Quoted supporting/contradicting evidence from conversation, or null if none\n}\n],\n‚Äúsummary‚Äù: {\n‚Äúsupported criteria‚Äù: {\n‚Äúsupported strong‚Äù: [List all supported strong-level criteria],\n‚Äúsupported moderate‚Äù: [List all supported moderate-level criteria],\n‚Äúsupported weak‚Äù: [List all supported weak-level criteria]\n},\n‚Äúmissing criteria‚Äù: { ‚Äúmissing strong‚Äù: [List all missing strong-level criteria],\n‚Äúmissing moderate‚Äù: [List all missing moderate-level criteria],\n‚Äúmissing weak‚Äù: [List all missing weak-level criteria]\n}\n‚Äúcontradicted criteria‚Äù: {\n‚Äúcontradicted strong‚Äù: [List all contradicted strong-level criteria],\n‚Äúcontradicted moderate‚Äù: [List all contradicted moderate-level criteria],\n‚Äúcontradicted weak‚Äù: [List all contradicted weak-level criteria]\n},\n‚Äúconfidence‚Äù: {\nreasoning: Provide your reasons for the confidence score.\nconfidence score: Please provide the confidence score between double angle brackets\n} } }\nFig. A7: Details of confidence generation prompt.\n29\n"}, {"page": 30, "text": "Appendix D\nCase Study\nD.1\nCase Study of Existing Methods\nIn this section, we provide case study for token-level methods, consistency-level methods, and self-\nverbalized methods. In our example, the model input is ‚ÄúI have been having stomach pain, feeling\nnauseous, and I have a slight fever.‚Äù.\nIn this example, the model prediction is ‚Äúappendicitis‚Äù and the probability for each token are\n‚Äúappend = 0.3204, icit = 0.9722, is = 0.9999‚Äù. So, for token-level method ASP that calculate the mean of\nthe generated token, the confidence result is 76.42. Furthermore, when the generate temperature is 0.5\nand we generate 15 answers, there are 10 answers are appendicitis, 3 answers are acute gastroenteritis,\nand 2 answers are viral gastroenteritis. When we apply consistency-level methods PoC that calculate the\nproportion of the most frequent response, the confidence is 66.67. Finally, when we utilize self-verbalized\nmethod CoT CE to obtain the result, the model output are shown in Fig. A8. The model provide an\nanlysis first and then evaluate the confidence score as 60.\nDetails of CoT CE in case study\nAppendicitis is a condition where the appendix, a small organ attached to the large intestine,\nbecomes inflamed. The symptoms of appendicitis can vary, but common ones include abdominal\npain, nausea, vomiting, and fever. Given the symptoms you described, such as stomach pain, feeling\nnauseous, and a slight fever, it is possible that you may have appendicitis. Given the information\nprovided, it is difficult to make a definitive diagnosis without further evaluation. However, based\non the symptoms described, I would rate the uncertainty of the proposed answer as 60.\nFig. A8: Details of CoT CE in case study.\nD.2\nCase Study of MedConf\nD.2.1\nDiagnostic Generation\nIn this example, the patient describes their symptoms to the doctor to seek a diagnosis. Their patient\ninformation I is shown in Fig. A9. Due to the patient‚Äôs limited capabilities, they often can only provide\nincomplete and brief descriptions.\nDetails of patient information in case study\nI have been having stomach pain, feeling nauseous, and I have a slight fever.\nFig. A9: Details of patient information in case study.\nSubsequently, patient information I is used to generate diagnostic assessment d through the guidance\nof the diagnosis prompt Promptd. The generated result d is shown in Fig. A10.\nD.2.2\nSymptom Generation\nIn this process, LLMs generate a symptom profile for the diagnostic results, which serves as the support\nknowledge for subsequent confidence evaluation. In this example, we utilize RAG to obtain the most\n30\n"}, {"page": 31, "text": "Details of diagnostic result in case study\nI have been having stomach pain, feeling nauseous, and I have a slight fever.\nFig. A10: Details of diagnostic result in case study.\nrelevant passages in the medical corpus D of the keyword ‚Äúappendicitis‚Äù. An example of RAG results R\nis shown in Fig. A11.\nExamples of RAG result in case study\nDocument *0* (Title: Appendicitis) Epidemiology Appendicitis is most common between the ages of 5 and 40. In 2013, it resulted in 72,000\ndeaths globally, down from 88,000 in 1990. In the United States, there were nearly 293,000 hospitalizations involving appendicitis in 2010.\nAppendicitis is one of the most frequent diagnoses for emergency department visits resulting in hospitalization among children ages 5‚Äì17 years\nin the United States. See also Deaths from appendicitis Evan O‚ÄôNeill Kane Leonid Rogozov References External links CT of the abdomen\nshowing acute appendicitis Appendicitis, history, diagnosis and treatment by Surgeons Net Education Appendicitis: Acute Abdomen and\nSurgical Gastroenterology from the Merck Manual Professional (content last modified September 2007) Appendicitis ‚Äì Symptoms Causes and\nTreatment at Health N Surgery\nFig. A11: Examples of RAG result in case study.\nFollowing the RAG step, the LLM generates symptom profile S based on the RAG result R, the\ndiagnostic result d, and the symptom profile generation prompt Prompts. The symptom profile for this\nexample is shown in Fig. A12.\nD.2.3\nConfidence Generation\nIn this process, the LLMs evaluate the relationship between patient information I and symptom profile\nS and then utilize it as evidence to inference the confidence score of current generated diagnosis. The\nresponse of the final confidence is shown in Fig. A13.\n31\n"}, {"page": 32, "text": "Details of symptom profile in case study\n[\n{\n‚Äúid‚Äù: 1,\n‚Äúdescription‚Äù: ‚ÄúPresence of right lower quadrant abdominal pain, which is often the first symptom of appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äústrong‚Äù\n},\n{\n‚Äúid‚Äù: 2,\n‚Äúdescription‚Äù: ‚ÄúNausea and vomiting following the onset of abdominal pain, which is commonly reported by patients with appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äústrong‚Äù\n},\n{\n‚Äúid‚Äù: 3,\n‚Äúdescription‚Äù: ‚ÄúFever, typically low-grade, which may accompany the other symptoms of appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äúmoderate‚Äù\n},\n{\n‚Äúid‚Äù: 4,\n‚Äúdescription‚Äù: ‚ÄúRebound tenderness or guarding in the right lower quadrant during physical examination, indicating irritation of the peri-\ntoneum.‚Äù,\n‚Äúimportance‚Äù: ‚Äústrong‚Äù\n},\n{\n‚Äúid‚Äù: 5,\n‚Äúdescription‚Äù: ‚ÄúElevated white blood cell count (leukocytosis), which is a common laboratory finding in patients with appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äúmoderate‚Äù\n},\n{\n‚Äúid‚Äù: 6,\n‚Äúdescription‚Äù: ‚ÄúUse of imaging studies such as ultrasound or CT scan to visualize the appendix and confirm the diagnosis of appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äúmoderate‚Äù\n},\n{\n‚Äúid‚Äù: 7,\n‚Äúdescription‚Äù: ‚ÄúHistory of previous episodes of abdominal pain or gastrointestinal issues, which may suggest a recurrent appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äúweak‚Äù\n},\n{\n‚Äúid‚Äù: 8,\n‚Äúdescription‚Äù: ‚ÄúApplication of scoring systems like the Alvarado score or Appendicitis Inflammatory Response (AIR) score to assist in diag-\nnosis.‚Äù,\n‚Äúimportance‚Äù: ‚Äúweak‚Äù\n}\n]\nFig. A12: Details of symptom profile in case study.\n32\n"}, {"page": 33, "text": "Details of confidence generation in case study\n{\n‚Äúcriteria evaluation‚Äù: [\n{\n‚Äúid‚Äù: 1,\n‚Äúdescription‚Äù: ‚ÄúPresence of right lower quadrant abdominal pain, which is often the first symptom of appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äústrong‚Äù,\n‚Äúsupport level‚Äù:‚Äú‚Äùmissing‚Äù,\n‚Äúevidence‚Äù: null\n},\n{\n‚Äúid‚Äù: 2,\n‚Äúdescription‚Äù: ‚ÄúNausea and vomiting following the onset of abdominal pain, which is commonly reported by patients with appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äústrong‚Äù,\n‚Äúsupport level‚Äù: ‚Äúsupported‚Äù,\n‚Äúevidence‚Äù: ‚ÄúI have been feeling nauseous‚Äù\n},\n{\n‚Äúid‚Äù: 3,\n‚Äúdescription‚Äù: ‚ÄúFever, typically low-grade, which may accompany the other symptoms of appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äúmoderate‚Äù,\n‚Äúsupport level‚Äù: ‚Äúsupported‚Äù,\n‚Äúevidence‚Äù: ‚ÄúI have a slight fever‚Äù\n},\n{\n‚Äúid‚Äù: 4,\n‚Äúdescription‚Äù: ‚ÄúRebound tenderness or guarding in the right lower quadrant during physical examination, indicating irritation of the peri-\ntoneum.‚Äù,\n‚Äúimportance‚Äù: ‚Äústrong‚Äù,\n‚Äúsupport level‚Äù: ‚Äúmissing‚Äù,\n‚Äúevidence‚Äù: null\n},\n{\n‚Äúid‚Äù: 5,\n‚Äúdescription‚Äù: ‚ÄúElevated white blood cell count (leukocytosis), which is a common laboratory finding in patients with appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äúmoderate‚Äù,\n‚Äúsupport level‚Äù: ‚Äúmissing‚Äù,\n‚Äúevidence‚Äù: null\n},\n{\n‚Äúid‚Äù: 6,\n‚Äúdescription‚Äù: ‚ÄúUse of imaging studies such as ultrasound or CT scan to visualize the appendix and confirm the diagnosis of appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äúmoderate‚Äù,\n‚Äúsupport level‚Äù: ‚Äúmissing‚Äù,\n‚Äúevidence‚Äù: null\n},\n{\n‚Äúid‚Äù: 7,\n‚Äúdescription‚Äù: ‚ÄúHistory of previous episodes of abdominal pain or gastrointestinal issues, which may suggest a recurrent appendicitis.‚Äù,\n‚Äúimportance‚Äù: ‚Äúweak‚Äù,\n‚Äúsupport level‚Äù: ‚Äúmissing‚Äù,\n‚Äúevidence‚Äù: null\n},\n{\n‚Äúid‚Äù: 8,\n‚Äúdescription‚Äù: ‚ÄúApplication of scoring systems like the Alvarado score or Appendicitis Inflammatory Response (AIR) score to assist in diag-\nnosis.‚Äù,\n‚Äúimportance‚Äù: ‚Äúweak‚Äù,\n‚Äúsupport level‚Äù: ‚Äúmissing‚Äù,\n‚Äúevidence‚Äù: null\n}\n],\n‚Äúsummary‚Äù: {\n‚Äúsupported criteria‚Äù: {\n‚Äúsupported strong‚Äù: [2],\n‚Äúsupported moderate‚Äù: [3],\n‚Äúsupported weak‚Äù: []\n},\n‚Äúmissing criteria‚Äù: {\n‚Äúmissing strong‚Äù: [1, 4],\n‚Äúmissing moderate‚Äù: [5, 6],\n‚Äúmissing weak‚Äù: [7, 8]\n},\n‚Äúcontradicted criteria‚Äù: {\n‚Äúcontradicted strong‚Äù: [],\n‚Äúcontradicted moderate‚Äù: [],\n‚Äúcontradicted weak‚Äù: []\n},\n‚Äúconfidence‚Äù: {\n‚Äúreasoning‚Äù: ‚ÄúThe patient has reported nausea and a slight fever, which supports two of the moderate and strong criteria. However, the absence\nof specific evidence for right lower quadrant pain and other criteria leads to a lower confidence score.‚Äù,\n‚Äúconfidence score‚Äù: ‚Äú<<30>>‚Äù\n}\n}}\nFig. A13: Details of confidence generation in case study.\n33\n"}]}