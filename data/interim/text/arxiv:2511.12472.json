{"doc_id": "arxiv:2511.12472", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.12472.pdf", "meta": {"doc_id": "arxiv:2511.12472", "source": "arxiv", "arxiv_id": "2511.12472", "title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing", "authors": ["Mengying Wang", "Chenhui Ma", "Ao Jiao", "Tuo Liang", "Pengjun Lu", "Shrinidhi Hegde", "Yu Yin", "Evren Gurkan-Cavusoglu", "Yinghui Wu"], "published": "2025-11-16T06:19:53Z", "updated": "2025-11-16T06:19:53Z", "summary": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.12472v1", "url_pdf": "https://arxiv.org/pdf/2511.12472.pdf", "meta_path": "data/raw/arxiv/meta/2511.12472.json", "sha256": "077ee2440eaf986b5d449a8bf1b35a63dc55c1e76b3864c74674148f2a2d3000", "status": "ok", "fetched_at": "2026-02-18T02:26:58.236587+00:00"}, "pages": [{"page": 1, "text": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs:\nA Case for Drug Repurposing\nMengying Wang*, Chenhui Ma*, Ao Jiao*, Tuo Liang, Pengjun Lu, Shrinidhi Hegde,\nYu Yin, Evren Gurkan-Cavusoglu, Yinghui Wu\nCase Western Reserve University, Cleveland, OH, USA\n{mxw767, cxm590, axj770, txl859, pxl465, sxh1426, yxy1421, exg44, yxw1650}@case.edu\nAbstract\nLarge Language Models (LLMs) have greatly advanced\nknowledge graph question answering (KGQA), yet existing\nsystems are typically optimized for returning highly relevant\nbut predictable answers. A missing yet desired capacity is to\nexploit LLMs to suggest surprise and novel (“serendipitious”)\nanswers. In this paper, we formally deﬁne the serendipity-\naware KGQA task and propose the SerenQA framework to\nevaluate LLMs’ ability to uncover unexpected insights in sci-\nentiﬁc KGQA tasks. SerenQA includes a rigorous serendip-\nity metric based on relevance, novelty, and surprise, along\nwith an expert-annotated benchmark derived from the Clin-\nical Knowledge Graph, focused on drug repurposing. Addi-\ntionally, it features a structured evaluation pipeline encom-\npassing three subtasks: knowledge retrieval, subgraph rea-\nsoning, and serendipity exploration. Our experiments reveal\nthat while state-of-the-art LLMs perform well on retrieval,\nthey still struggle to identify genuinely surprising and valu-\nable discoveries, underscoring a signiﬁcant room for future\nimprovements. Our curated resources and extended version\nare released at: https://cwru-db-group.github.io/serenQA.\n1\nIntroduction\nLarge language models (LLMs) are rapidly advancing the\nbridge between natural language understanding and effec-\ntive question answering. Signiﬁcant efforts, such as domain-\nspeciﬁc ﬁne-tuning, prompt engineering, and Retrieval-\nAugmented Generation (RAG), have enabled LLMs to lever-\nage external knowledge bases to produce highly relevant\nand precise answers tailored to specialized research ques-\ntions (Le et al. 2024). However, these systems often focus\non returning information already familiar to experts, missing\nthe crucial scientiﬁc capacity to uncover surprising connec-\ntions that inspire new research directions (Song et al. 2023).\n“Serendipity”, the art of luck and beneﬁcial discov-\nery, arises from both unexpected ﬁndings and the skill to\nrecognize novel applications of such discoveries in vari-\nous domains, serving as a catalyst for genuine scientiﬁc\nbreakthroughs. While serendipity has been studied in web\nsearch (Huang et al. 2018) and recommender systems (Toku-\ntake and Okamoto 2024), it remains largely unexplored in\n*These authors contributed equally.\nCopyright © 2026, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Suggesting Drugs that treat Severe Acute Pain: A\nSerendipitous case of Journavx.\nscientiﬁc question answering. Empowering LLMs with the\nability to discover new knowledge from existing, valuable\nknowledge bases is thus a critical step towards true LLM-\nempowered scientiﬁc discovery.\nExample 1: Fig. 1 illustrates a KGQA task to ﬁnd drugs\nthat can treat severe acute pain. There are four possible\nanswers. (1) Opioids e.g., Oxycodone, a well-known drug\nwith recognized mechanism on targeting the µ-opioid re-\nceptor within the pain-signaling pathway. (2) Tapentadol\n(2008) expanded this paradigm by adding a dual mechanism,\nhence with increased novelty for the question. (3) Jour-\nnavx, a ﬁrst non-opioid analgesic for severe acute pain (FDA\n2025) approved by FDA in 2025. Journavx acts through\na novel mechanism, selectively inhibiting NaV1.8 sodium\nchannels in peripheral pain-sensing neurons. Surprisingly,\nwith this paradigm shift and different targets, it remains\nrelevant by sharing the broader pain-signal pathway con-\ntext with opioids. Hence it is a “serendipitous” result in the\nKGQA search, in terms of relevance, novelty, and an un-\nexpected answer, which may inspire new medical research\ndirections. (4) Ibuprofen, in contrast, works through the\nclassical inﬂammatory COX inhibition pathway, targeting\narXiv:2511.12472v1  [cs.CL]  16 Nov 2025\n"}, {"page": 2, "text": "mild-to-moderate pain and thus showing low embedding rel-\nevance and novelty, while suggesting Ibuprofen for severe\nacute pain would still be surprising.\n✷\n“Can LLMs, while enhanced by domain KGs, suggest\nserendipitous answers for domain sciences?” This paper\nmakes a ﬁrst step to investigate the potential of LLMs to sur-\nface serendipitous discoveries within scientiﬁc KGQA, with\na focus on drug repurposing, which is a cornerstone of med-\nical research. We address three core research questions:\n◦(RQ1): How may “serendipity” be characterized and\nquantitatively measured for scientiﬁc KGQA tasks?\n◦(RQ2): What roles could LLMs play for serendipity dis-\ncovery in domain science KGQA?\n◦(RQ3): How to evaluate state-of-the-art LLMs, and what\nare their performances in serendipity discovery?\nTo this end, we introduce the SerenQA framework de-\nsigned to systematically evaluate the ability of LLMs to un-\ncover serendipitous answers within the context of KGQA. It\nincludes three core components (shown in Fig. 2):\n◦Serendipity Metric (RNS): A rigorous, graph-based\nmeasure capturing Relevance, Novelty, and Surpriseness\nin KGQA answers, justiﬁed by an axiomatic analysis that\nclariﬁes the trade-offs and properties.\n◦Serendipity-aware Benchmark: An expert-annotated\nKGQA dataset for drug repurposing, based on the Clinic\nKnowledge Graph (Santos et al. 2022). It features cu-\nrated question-answer pairs and explicit serendipity an-\nnotations for ﬁne-grained evaluation.\n◦Assessment Pipeline: A principled and reproducible\nthree-phase\nworkﬂow\nthat\nsystematically\nevaluates\nLLMs’ roles in serendipitous discovery. It decomposes\nthe task into knowledge retrieval, reasoning, and ex-\nploratory search, providing insights into model capabili-\nties and limitations in scientiﬁc knowledge discovery.\nWe performed extensive experiments with various LLMs\nacross different scales, demonstrating that while frontier\nmodels excel in knowledge retrieval tasks, nearly all models\nstruggle signiﬁcantly in serendipity exploration, highlight-\ning inherent challenges and opportunities in this area.\nRelated works. We categorize related works as follows.\nSerendipity-Driven Knowledge Exploration Serendipity, de-\nﬁned as an unexpected yet valuable discovery, has emerged\nas a crucial goal in recommender systems and knowl-\nedge exploration (Bordino, Mejova, and Lalmas 2013). Re-\ncent studies have leveraged LLMs to generate and evaluate\nserendipitous recommendations through advanced prompt\nengineering (Fu and Niu 2024) or by aligning model out-\nputs with human preferences (Xi et al. 2025). Notably,\nexisting approaches primarily rely on subjective human\nannotation, LLM self-evaluation, or comparisons against\nbenchmark groundtruths for serendipity evaluation. In con-\ntrast, we propose a graph-based serendipity measure (RNS),\nwhich transforms the knowledge graph (KG) into a proba-\nbility matrix (Dehmer and Mowshowitz 2011), enabling an\ninformation-theoreticquantiﬁcation of various subjective as-\npects of serendipity, resulting in a more rigorous evaluation.\nLLM-Augmented Novelty Detection. LLMs are increasingly\nseen as creative partners that can accelerate scientiﬁc discov-\nery across disciplines (AI4Science and Quantum 2023). By\nmining vast knowledge and generating hypotheses, LLMs\ncan propose novel research ideas or unexpected connec-\ntions that human experts might overlook (Si, Yang, and\nHashimoto 2025). Despite these efforts, the community still\nlacks a more comprehensive understanding and benchmark\ndatasets speciﬁcally designed to assess serendipitous discov-\neries. To address this gap, we present a drug repurposing\nKGQA dataset which enables a systematic and objective as-\nsessment of serendipitous knowledge exploration.\nSerenQA is the ﬁrst reproducible and extensible frame-\nwork for advancing serendipity discovery in drug repurpos-\ning. We advocate its broader application to facilitate new re-\nsearch opportunities in scientiﬁc KGQA tasks.\n2\nSerendipitous Assessment with KGQA\nBelow, we deﬁne relevant concepts and core notations:\n2.1\nSerendipity-aware KGQA\nSerenQA performs LLM assessment by processing a\npipeline of serendipity-aware KGQA. Given a natural lan-\nguage (NL) question Q, a large language model L, a di-\nrected, multigraph G = (V, E), where V is the set of enti-\nties with size V = |V|, and E is the set of relations with\nsize E = |E|, a serendipity-aware KGQA system returns an\nanswer set as an ordered partition A = (Ae, As), where:\n◦Ae: the existing answer set, containing answers explic-\nitly supported by facts in G;\n◦As: the serendipity answer set, containing answers that\nare relevant but extend beyond direct explicit knowledge,\nrevealing novel and unexpected relationships in G.\nsuch that Ae ∪As ⊆V and Ae ∩As = ∅. We deﬁne |A| =\n|Ae ∪As| as the total size of the answer set.\nThis serendipity-aware setup is motivated by the real-\nworld scientiﬁc discovery process, which frequently in-\nvolves uncovering not only established knowledge (Ae) but\nalso insightful and surprising associations (As), potentially\nleading to innovative research opportunities, such as novel\ndrug repurposing. Knowledge graphs are particularly suit-\nable for this task due to their structured representation of in-\nterconnected entities and relations, enabling systematic ex-\nploration of indirect and surprising relationships.\n2.2\nGraph-speciﬁed Serendipity Formulation\nTo rigorously quantify serendipity, we deﬁne a graph-based\nserendipity measure (RNS), which quantiﬁes how effec-\ntively a serendipity answer set As for a given question Q\nprovides relevant yet novel and surprising insights beyond\nthe explicit answer set Ae. Intuitively, serendipity is a com-\nposite experience, encompassing multiple dimensions si-\nmultaneously (Niu and Abbas 2017). Formally, we deﬁne\nthe RNS score as a weighted combination of three perspec-\ntives: relevance, novelty, and surprise, which can be ﬂexi-\nbly adjusted to suit user preferences. Given an answer set\nA = (Ae, As), the serendipity score is computed as:\nRNS(Ae, As) = α R(Ae, As)+β N(Ae, As)+γ S(Ae, As)\n"}, {"page": 3, "text": "Figure 2: SerenQA Framework. (A): Computing RNS score for partition (Ae, As) form G; (Sec. 3). (B): Constructing SerenQA dataset\nfrom ClinicalKG; (Sec. 4). (C): For an NL query, our pipeline retrieves Ae from G and explores As from Ae with beam search. (Sec. 5).\n- R (Relative Relevance): context similarity of Ae and As;\n- N (Relative Novelty): new information in As beyond Ae;\n- S (Relative Surprise): unpredictability of As given Ae.\nThe weights α, β, γ can be tuned to user preference; rec-\nommended defaults are ﬁt to expert evaluations. Details of\nthe metric and its computation are described in Sec 3.\nIn the following sections, we detail how the SerenQA\nframework establishes a uniﬁed benchmark, dataset, and\nevaluation protocols speciﬁcally designed to assess LLM ca-\npabilities in serendipitous knowledge discovery tasks, par-\nticularly in the critical area of drug repurposing.\n3\nSerendipity Quantiﬁcation\nQuantifying serendipity is inherently challenging due to\nits abstract and subjective nature. As discussed in Sec 1,\nexisting methods often rely heavily on subjective human\nannotations or LLM-generated evaluations, which suffer\nfrom limitations like poor interpretability, scalability is-\nsues, and potential biases. To overcome these, we introduce\nan information-theoretic approach enabling scalable, inter-\npretable, and reproducible serendipity evaluations.\n3.1\nSerendipity: A characterization\nTo align with human intuition about “Serendipity” while al-\nlowing for rigorous quantiﬁcation, as introduced in Sec 2.2,\nwe speciﬁcally decompose it into three complementary di-\nmensions: Relevance, Novelty, and Surprise. For an answer\nset A = (Ae, As) to a query Q, we deﬁne the Serendipity\nScore (RNS) as a weighted combination of the relative mea-\nsures between As and Ae, with user-conﬁgurable weights to\naccommodate different preferences or application scenarios.\nEach dimension is adapted to well-established information-\ntheoretic measures, as described below:\nRelative Relevance. We compute relative Relevance (R) as\nthe average normalized Euclidean distance (d(·)) between\nthe GCN embeddings of entities in As and Ae:\nR(Ae, As) = −\nP\ni∈As,j∈Ae d(ni, nj)\n|As||Ae|\nwhere ni (resp. nj) refers to the embedding of the entity\ni ∈As (resp. j ∈Ae). A larger distance reﬂects greater\ncontextual difference, indicating As belongs to more distinct\nclusters in G and may diverge from the core context of Q.\nRelative Novelty. Relative Novelty (N) is derived from\na mutual-information-based score between the existing\nand serendipity sets. For a partition (Ae, As), we deﬁne\nN(Ae, As) = 1- MI(Ae, As), where MI(As, Ae) mea-\nsures the shared amount of information between As and Ae,\nand is given by:\nMI(Ae, As) =\nX\ni∈Ae\nP(i)\nX\nj∈As\nP(j|i) log P(j|i)\nP(j)\nA higher N score indicates As are less redundant given Ae.\nRelative Surpriseness. Relative Surprise (S) is quantiﬁed\nvia Jensen–Shannon divergence (JSD) between entity distri-\nbutions Ps and Pe, which are the accumulated probability\ndistributions over entities in As and Ae, respectively:\nS(Ae, As) = 1\n2(DKL(Ps∥PMix) + DKL(Pe∥PMix))\nwhere DKL(·∥·) is the Kullback–Leibler divergence (Kull-\nback 1951), and PMix = 1\n2(Ps + Pe).\nGiven Ae, a higher RNS indicates a “more” serendipitous\nset As with greater diverse, novel and surprise entities that\ncannot be inferred from Ae, as exempliﬁed by “Journavx”,\nthe ﬁrst non-opioid analgesic for severe acute pain (Exp. 1).\n3.2\nCost-effective Graph Probabilistic Modeling\nCost-effective graph probabilistic models (P(·)) is crucial\nfor efﬁcient RNS computation. We present the detailed mod-\nels, justiﬁed by an axiomization analysis.\n"}, {"page": 4, "text": "3-Hop Conditional Probability. Serendipitous ﬁndings\nmay from indirect, multi-hop connections. Thus, we con-\nsider a multi-hop conditional probability matrix M that ag-\ngregates transition probabilities across both direct and indi-\nrect relations to capture a global probabilistic propagation.\nEmpirically, 99% of serendipity answers in our datasets are\nreachable from existing answers within three hops, prompt-\ning our analysis to up to 3-hop neighbors of entities in G.\nGiven graph G, we initialize M as a weighted matrix M,\nwith Mij the number of links from node i to j. We nor-\nmalize M to obtain the one-hop transition probabilities that\nensures row-stochasticity: P1(j|i) =\nMij\nP\nk∈E Mik . The k-hop\nconditional probability matrix Pk is computed as:\nPk =\nk\nX\nh=1\nαhP h\n1 ,\nαh =\nh\nPk\nh=1 h\nwhere P h\n1 represents the probability of reaching a node in\nh hops, and weights αh increases for larger h to prioritize\nlonger connections. We can justify that Pk consistently sat-\nisﬁes the necessary constraints of a transition matrix:\n◦Non-negativity: (Pk)ij ≥0 for all (i, j),\n◦Row-Stochastic Property: P\nj(Pk)ij = 1 for all i.\nCost Analysis. Constructing M takes O(V 2) for dense\ngraphs. Traditional P3 computation1 via graph traversal\nis O(V 4). We employ Divide-and-Conquer optimized ma-\ntrix multiplication (Strassen 1969) and parallel computation\nwith t processors, reducing the cost to O(V log7\n2/t).\nMarginal Probability. The marginal probability P(i) quan-\ntiﬁes steady-state node probabilities at node i under the law\nof total probability: P(i) = P\nj P3(i|j)P(j). This leads to\nthe linear system representation:\n(I −P T\n3 )P = 0,\nX\ni\nP(i) = 1\nwhich can be solved by matrix inversion in O(V 3) time.\nTo further reduce the cost, we approximate the computation\nwith a PageRank-style damped iteration:\nPt+1 = λP T\n3 Pt + (1 −λ)P0\nwhere P0 is an initial probability distribution, set uniformly\nas\n1\nV , ensuring convergence even on disconnected graphs.\nThis reduces the cost in O(V 2 log V ) time.\nWe remark that the probabilistic matrices are computed\n“once for all” and are shared for multiple queries, and read-\nily adapt to different domain graphs.\nFurther analyses are included in the Appendix C.\nAxiomization Analysis. We further justify that RNS is a\nproper serendipity measure for KGQA tasks through the fol-\nlowing axiomatic analysis. For any query and a correspond-\ning retrieved, ﬁxed existing set Ae, consider an optimization\nprocess that ﬁnds an optimal serendipitous set A∗\ns with at\nmost K entities, i.e., A∗\ns = arg max|As|≤K RNS(Ae, As).\nWe can show that RNS satisﬁes the following properties:\n1While we make a case for 3-hop queries here, our discussion\nreadily extends to k-hop queries for k ≥3.\n◦(Scale invariance). A∗\ns remains to maximize RNS even\nif R, N or S are scaled by a constant. This ensures the\ninvarance of A∗\ns under RNS measure regardless of how\nthe user preference (α, β, γ) changes.\n◦(Consistency). Making the R, N, S larger (resp. smaller)\nfor any entities in Ae (resp. As) does not change the\nranking of entities in A∗\ns in terms of RNS.\n◦(Non-monotonicity). RNS(Ae, As) ̸≤RNS(Ae, A′\ns) if\n|As| ≤|A′\ns|. Indeed, larger answer sets do not necessar-\nily indicate that they are more “serendipitous” in practice.\n◦(Independence). RNS is only determined by the embed-\ndings of entities from As ∪Ae. No information from en-\ntities not seen in A can affect the serendipitous of Ae.\nThis justiﬁes RNS for serendipity in a pragmatic “semi-\nclosed world” assumption, striking a balance between a\nchallenging open-world analysis (As can be inﬁnite) and\na rigorous, overkilling closed world (As = ∅) setting.\n4\nSerendipity-aware Benchmark\nThe proposed RNS measure enables quantitative assess-\nment of serendipity within any answer set (Ae, As) derived\nfrom a graph G. Yet scoring alone is insufﬁcient: assess-\ning cornerstone steps such as retrieving and reasoning de-\nmands a benchmark with authoritative groundtruth serendip-\nity answer set. We therefore introduce a drug-repurposing\nbenchmark that supports both standard KGQA tasks and\nserendipity-awareevaluations, giving the ﬁne-grained super-\nvision required for end-to-end assessment.\n4.1\nQA Set Construction\nOur benchmark is built upon the Clinical Knowledge Graph\n(CKG) (Santos et al. 2022), a widely recognized biomedical\nresource containing extensive data on drug, gene, and dis-\nease interactions. Our focus is on drug repurposing, which\nis a critical research task aimed at identifying novel thera-\npeutic uses of existing drugs (Pushpakom et al. 2019).\nOur dataset supports typical KGQA tasks through a con-\ntextualized query scenario that consists of standardized con-\nﬁguration including expert-veriﬁed, scientiﬁcally meaning-\nful NL queries, their structured graph (Cypher) counter-\nparts with query components that are explicitly annotated\nwith their semantics, and grounded and validated answer\nsets. Unlike its peer NL-only benchmark datasets in KGQA,\nit couples each NL query to a distinct, validated “ground\ntruth”, structured graph query, thereby reducing ambiguity\nand mitigating possible semantic redundancy. It also explic-\nitly annotates graph patterns, such as multi-hop and inter-\nsection queries, to reﬂect realistic query complexities in sci-\nentiﬁc inquiry. Dataset statistics are summarized in Table 1.\nWe present details of graph queries in Appendix A.\n4.2\nAnswer Set Construction\nTo reliably establish ground-truth serendipity sets, we start\nwith the latest version of Clinic KG, denoted as Gc. For each\nquery Q, we initially obtain its complete candidate answer\nset Ac from Gc. We then partition it into an existing set Ae\nand a serendipity set As , with Ae ∩As = ∅and Ae ∪As =\nAc. We apply three distinct partitioning strategies:\n"}, {"page": 5, "text": "Statistic\nValue\nNumber of Distinct Queries\n1529\nNumber of Relations in G (E)\n201,704,256\nNumber of Entities in G (V )\n15,430,157\nNumber of Graph Pattern Types\n9\nAvg. Answer Set Size (|A| per query)\n4.04\nNumber of Experts for NL Query Veriﬁcation\n4\nNumber of Experts for Serendipity Annotation\n6\nTable 1: Dataset Statistics of SerenQA Benchmark.\nLLM Ensemble. Following established practices, we\nprompt four state-of-the-art LLMs to assign a “serendipity\nscore” to each candidate answer. For every query, entities\nin the complete answer set Ac are ranked by their average\nLLM score; the top 20% are collected as the serendipity set\nAs, and the reminder form Ae.\nExpert Crowdsourced. We engaged a team of 6 domain\nexperts (three physicians, one pharmaceutical scientist, and\ntwo trained medical model annotators) via an online ques-\ntionnaire (DrugKG Questionnaire 2025). They were re-\nquested to reﬁne the rankings from LLMs. The questionnaire\nis accepting continuous responses from human experts.\nRNS Guided. With the justiﬁed RNS metric (Sec.3) we treat\nserendipity partitioning as:\nmax\nAe,As RNS(Ae, As),\ns.t. |As| = b, b = max(1, ⌊0.2|Ac|⌋)\nStarting from an initial partition, we apply the greedy-swap\nalgorithm in Algorithm 1 to (approximately) compute an op-\ntimal answer set As in Ac. The algorithm iteratively swaps\nentity pairs between Ae and As that yield the greatest im-\nprovement in a marginal gain of RNS, continuing until no\nfurther improvement is possible. Each iteration has a com-\nplexity of O(|A|2). We found in our tests that Ae usually\ncontains a few entities (on average 4; see Table 1), And the\nalgorithm is quite fast in practice. During that, we calibrated\nthe RNS weights to align with the expert-crowdsourced par-\ntitions for consistency and fair assessment.\nFor each partitioning result, we construct G by removing\nselected edges from Gc, ensuring that for each query Q, enti-\nties in Ae remain derivable from G, while entities in As be-\ncome inaccessible. This creates a controlled evaluation en-\nvironment aligned with problem deﬁnitions (Sec. 2).\n5\nEvaluation Pipeline\nWe next introduce our evaluation pipeline (Fig. 2(C)), which\nsystematically evaluates the serendipity discovery capabil-\nities of LLMs using our curated serendipity-aware bench-\nmark. The pipeline is modularized into three highly corre-\nlated tasks, each of which independently measures a spe-\nciﬁc, “cornerstone” aspect of an LLM’s role and perfor-\nmance on serendipity discovery in scientiﬁc KGQA tasks.\nKnowledge Retrieval. In this task, LLM translates an NL\nquestion Q into a Cypher query C to retrieve an answer\nset Ae from the knowledge graph G. The performances are\nevaluated by comparing the accuracies of the retrieved an-\nswer set Ae against the ground truth. Additionally, the per-\nformances across different query patterns (such as one-hop,\nAlgorithm 1: Greedy Swap for RNS –Guided Optimization\nInput: initial partition (A0\ne, A0\ns);\npre-computed probability matrices P3, P for graph G\nOutput: optimized partition (Ae, As)\n1: set (Ae, As) := (A0\ne, A0\ns), τ = RNS(Ae, As)\n2: while true do\n3:\nset ∆max := 0; (i∗, j∗) := null\n4:\nfor i ∈As do\n5:\nfor j ∈Ae do\n6:\nA′\ns := (As\\{i})∪{j}, A′\ne := (Ae\\{j})∪{i}\n7:\n∆:= RNS(A′\ne, A′\ns) −τ\n8:\nif ∆> ∆max then\n9:\n∆max := ∆; (i∗, j∗) := (i, j)\n10:\nend if\n11:\nend for\n12:\nend for\n13:\nif ∆max = 0 then break;\n14:\nend if\n15:\nAs := (As\\{i∗})∪{j∗}, Ae := (Ae\\{j∗})∪{i∗}\n16:\nτ := τ + ∆max\n17: end while\n18: return (Ae, As)\ntwo-hop, and intersection queries) are compared to evaluate\nthe LLM’s capability to handle varying levels of query com-\nplexity and structural diversity.\nSubgraph Reasoning. This task evaluates the LLM’s capa-\nbility to interpret and concisely summarize the retrieved an-\nswer of a graph-structured query C in a knowledge graph (as\na subgraph) into domain-aware natural language answers.\nThe generated summaries provide essential contextual sup-\nport for subsequent serendipity exploration tasks, requiring\nnuanced biomedical understanding and logical reasoning.\nSerendipity Exploration. This third (ﬁnal) task evaluates\nthe LLMs’ proactive ability to uncover serendipity entities\nAs through an LLM-guided beam search from Ae. Given a\nbeam width w, we prompt LLM to select the top-w nodes at\neach step from the candidate list as the next target nodes\nbased on criteria such as supporting evidence, interaction\nstrength, biological effect direction, and their expression\nlevel. The model further determines whether to continue ex-\nploration based on relevance and potential novelty. This task\nassesses the LLM’s ability to use biomedical knowledge and\ncontextual search to effectively navigate serendipitous dis-\ncovery while balancing depth and breadth in exploration. We\nremark that the serendipity set As produced in this section is\nthe pipeline’s output at evaluation time; in contrast, the As\ndeﬁned in Sec. 4 is the benchmark ground-truth constructed\nfor scoring. More details are provided in Appendix D.\n6\nExperiments\n6.1\nExperiment Setting\nWe conduct experiments using the benchmark introduced\nin Sec. 4, and evaluated LLMs across multiple scales, from\nfrontier models with billions of parameters to smaller vari-\n"}, {"page": 6, "text": "Model\nOne-Hop\nTwo-Hop\nMultiple(3+)-Hop\nIntersection\nHit(%)\nF1(%)\nExe.(%)\nHit(%)\nF1(%)\nExe.(%)\nHit(%)\nF1(%)\nExe.(%)\nHit(%)\nF1(%)\nExe.(%)\nDeepSeek-V3\n20.45\n78.71\n72.88\n3.46\n10.71\n9.86\n1.97\n6.22\n6.55\n2.64\n7.15\n8.03\nGPT-4o\n19.71\n77.16\n60.17\n2.08\n6.36\n7.89\n1.40\n4.20\n4.85\n1.56\n4.65\n5.21\nClaude-3.5-Haiku\n13.28\n48.54\n48.73\n9.78\n39.01\n32.89\n4.43\n8.64\n14.08\n1.38\n3.90\n4.66\nLlama-3.3-70B\n19.28\n70.67\n74.58\n16.63\n44.34\n56.57\n2.98\n10.16\n11.89\n4.80\n9.60\n16.05\nDeepSeek-R1-70B\n19.87\n69.07\n80.08\n12.03\n37.00\n43.42\n2.97\n8.06\n13.11\n3.49\n6.16\n16.46\nMed42-V2-70B\n18.34\n69.43\n69.92\n5.92\n19.12\n19.74\n0.23\n0.51\n1.21\n0.08\n0.13\n0.68\nQwen3-32B\n0.37\n1.27\n1.27\n0.16\n0.65\n0.65\n0.24\n0.36\n0.48\n0.00\n0.00\n0.00\nDeepSeek-R1-32B\n17.90\n65.23\n68.22\n3.06\n5.72\n7.24\n1.87\n4.50\n5.58\n0.79\n1.84\n3.16\nQwen3-8B\n10.07\n37.24\n39.83\n0.98\n2.87\n3.95\n0.90\n2.01\n4.85\n1.58\n1.91\n5.62\nDeepSeek-R1-8B\n1.27\n3.41\n5.51\n0.00\n0.00\n0.00\n0.04\n0.24\n0.24\n0.00\n0.00\n0.00\nMed42-V2-8B\n8.11\n23.90\n49.15\n1.05\n3.31\n3.97\n1.71\n4.07\n4.12\n0.04\n0.13\n0.14\nQwen3-1.7B\n0.84\n3.72\n11.86\n0.65\n1.98\n3.29\n0.00\n0.00\n0.24\n1.08\n1.56\n2.74\nDeepSeek-R1-1.5B\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nTable 2: Knowledge Retrieval (T 1), Best scores are bolded, second best are underlined\nants ( 1B parameters). Experimental results are presented\nin Tables 2–3, including three evaluation tasks within our\npipeline: T 1 (Knowledge Retrieval), T 2 (Subgraph Reason-\ning) and T3 (Serendipity Exploration).\nEvaluation metrics. Table 2 (T 1) reports F1 scores,\nExecutability (percentage of error-free queries), and Hit\nRate(|Ae∩A′\ne|/|Ae|), categorized by query patterns; and Ta-\nble 3 (T 2, T 3) reports their performances on three ground-\ntruth partitions (LLM-Ensemble, Expert-Crowdsourced,\nRNS-Guided). During beam search (beam width 30, max-\nimum depth 3), we employ one-shot learning by provid-\ning a single query with detailed ground-truth serendipity\npaths in the prompt, helping models understand exploration\npaths. In addition, T 2 and T 3 are measured with (a) Sub-\ngraph Reasoning:Faithful. (1–5, LLM-judged, factual accu-\nracy of summaries); Compre. (1–5, LLM-judged, coverage\nof key graph elements); SerenCov (0–1, fraction of serendip-\nity paths explicitly mentioned). (b) Serendipity Exploration:\nRelevance (1–5, LLM-judged alignment with groundtruth\nentities); TypeMatch (0–1, the fraction of predicted entity\ntypes that match the ground truth types); and SerenHit (0–1,\nmatch rate with groundtruth serendipity set).\nExperiment Environment We depoly our system on 5\nx AWS c6a.24xlarge on-demand instances for distributed\ncomputation and 5 x c6a.xlarge instances as relation stor-\nage nodes, each node runs Ubuntu 22.04 with Docker and\nRedis 7.2, using mounted dump.rdb as readonly data source.\nThe system supports 500 concurrent LLM reasoning tasks\nacross distributed nodes via asyncio.\n6.2\nTask Analysis\nWe next analyze experimental results task-by-task.\nTask 1: Knowledge Retrieval. The results in Table 2 show\nthat larger models (e.g., DeepSeek-V3, GPT-4o) consis-\ntently excel in simpler one-hop retrieval (F1 ≈78%), yet\nboth exhibit performance degradation for more complex\nmulti-hop queries (F1 drops to < 10% for queries with\n3+ hops). Smaller models are less accurate in coping with\nboth simpler and more complex queries, reﬂecting lim-\nitations in reasoning depth and broader coverage of the\nbiomedical context. Notably, the two 70B models (Llama-\n3.3-70B, DeepSeek-R1-70B) achieve better performances,\nwhich may be due to their more up-to-date training datasets.\nTask 2: Subgraph Reasoning. In Table 3 (upper), Mixtral-\n8×7B achieves (surprisingly) high Serendipity Coverage\n(60%+) despite moderate scores in Faithfulness and Com-\nprehensiveness (2-3 out of 5). This interestingly indicates\nthat summarization approaches yield broader serendipitous\npath coverage but risk factual inaccuracies. In contrast,\nlarger models (e.g.,Llama-3.3-70B) achieve higher Faithful-\nness and Comprehensiveness but lower “SerenCov”, sug-\ngesting a consistent trade-off that their richer pre-trained\nknowledge produces more precise, yet narrower summaries.\nTask 3: Serendipity Exploration. The rows labeled ”w.o.\nsummary” evaluate performance without subgraph sum-\nmaries, isolating the effect of providing chain-of-thought\nguidance. For almost all models, removing the summary im-\nproved performance on all three metrics. One possible rea-\nson for this is that the model may introduce hallucinations\nduring the summary process, which can inﬂuence the explo-\nration path, as proven by Table 3 (upper), many models did\nnot achieve the desired score in subgraph reasoning.\n6.3\nIn-Depth Discussion\nModel scale vs. Serendipity. As shown in the tables, larger\nmodels generally perform better in retrieval and exploration\ntasks. However, for subgraph summarization and reasoning\n(denoted as T 2), there is signiﬁcant variance and no obvi-\nous correlation with model size. This may suggest that re-\ntrieval and exploration beneﬁt more from the model’s inher-\nent knowledge, which larger models excel at, while summa-\nrization and reasoning do not follow the same trend.\nPartition Sensitivity. Fig. 3 displays triangle plots of Pear-\nson Correlations for TypeMatch, SerenCov, and SerenHit,\nwith each triangle representing one metric. The corners de-\n"}, {"page": 7, "text": "Models\nLLM Ensemble\nExpert Crowdsourced\nRNS Guided\nFaithful.\nCompre.\nSerenCov\nFaithful.\nCompre.\nSerenCov\nFaithful.\nCompre.\nSerenCov\nDeepSeek-V3\n2.283\n3.341\n0.101\n2.306\n3.340\n0.100\n2.253\n3.326\n0.106\nLlama-3.3-70B\n2.519\n3.842\n0.070\n2.553\n3.853\n0.068\n2.531\n3.829\n0.075\nDeepSeek-R1-70B\n2.573\n2.206\n0.223\n2.572\n2.238\n0.204\n2.582\n2.202\n0.217\nQwen-2.5-72B\n2.024\n2.683\n0.153\n2.093\n2.715\n0.152\n2.114\n2.719\n0.155\nMixtral-8x7B\n2.271\n2.963\n0.642\n2.272\n2.958\n0.610\n2.347\n2.924\n0.632\nQwen-2.5-32B\n2.243\n2.929\n0.148\n2.255\n2.910\n0.146\n2.260\n2.886\n0.152\nGamma-2-27B\n2.365\n3.410\n0.088\n2.381\n3.439\n0.084\n2.385\n3.415\n0.089\nMistral-24B\n2.114\n3.016\n0.141\n2.114\n3.048\n0.136\n2.134\n3.049\n0.141\nQwen-2.5-7B\n1.920\n1.817\n0.592\n1.900\n1.848\n0.580\n1.955\n1.832\n0.593\nModels\nLLM Ensemble\nExpert Crowdsourced\nRNS Guided\nRelevance\nTypeMatch\nSerenHit\nRelevance\nTypeMatch\nSerenHit\nRelevance\nTypeMatch\nSerenHit\nDeepSeek-V3\n2.436\n0.482\n0.048\n2.494\n0.462\n0.061\n2.538\n0.463\n0.077\n֒→w.o. summary\n2.447\n0.482\n0.050\n2.482\n0.463\n0.095\n2.510\n0.468\n0.134\nLlama-3.3-70B\n2.537\n0.502\n0.046\n2.559\n0.483\n0.067\n2.594\n0.478\n0.106\n֒→w.o. summary\n2.544\n0.505\n0.043\n2.565\n0.478\n0.086\n2.630\n0.483\n0.127\nDeepSeek-R1-70B\n1.935\n0.424\n0.030\n2.000\n0.409\n0.034\n2.033\n0.418\n0.049\n֒→w.o. summary\n1.972\n0.438\n0.035\n1.987\n0.413\n0.037\n2.052\n0.419\n0.053\nQwen-2.5-72B\n2.264\n0.415\n0.023\n2.345\n0.406\n0.041\n2.405\n0.400\n0.059\n֒→w.o. summary\n2.269\n0.428\n0.028\n2.337\n0.416\n0.050\n2.409\n0.412\n0.070\nMixtral-8x7B\n1.947\n0.256\n0.010\n2.033\n0.254\n0.015\n2.013\n0.230\n0.024\n֒→w.o. summary\n2.158\n0.324\n0.016\n2.250\n0.312\n0.022\n2.220\n0.306\n0.042\nQwen-2.5-32B\n2.294\n0.441\n0.036\n2.331\n0.426\n0.045\n2.378\n0.429\n0.065\n֒→w.o. summary\n2.304\n0.453\n0.037\n2.328\n0.431\n0.068\n2.390\n0.438\n0.105\nGamma-2-27B\n2.357\n0.450\n0.033\n2.379\n0.414\n0.057\n2.443\n0.431\n0.080\n֒→w.o. summary\n2.343\n0.448\n0.032\n2.376\n0.412\n0.054\n2.425\n0.402\n0.081\nMistral-24B\n1.855\n0.195\n0.008\n1.959\n0.184\n0.016\n2.005\n0.185\n0.026\n֒→w.o. summary\n1.903\n0.212\n0.011\n1.962\n0.204\n0.023\n2.006\n0.213\n0.035\nQwen-2.5-7B\n1.636\n0.221\n0.022\n1.721\n0.229\n0.026\n1.708\n0.215\n0.041\n֒→w.o. summary\n1.487\n0.160\n0.018\n1.550\n0.175\n0.018\n1.547\n0.158\n0.027\nTable 3: Subgraph Reasoning (T 2, upper), Serendipity Exploration (T 3, lower), with Best scores bolded, 2nd best underlined\nFigure 3: Correlation of Metrics Across Partition Strategies\nnote three types of partitions, and edge weights indicate cor-\nrelation scores—shorter distances refer to stronger correla-\ntions. Our analysis shows that all partitions have positive\ncorrelations across all metrics, with scores above 85%. No-\ntably, the expert and RNS-guided partitions reached around\n99% on all cases, highlighting the robustness of our partition\nstrategies and the reliability of the proposed RNS measure.\nNo Single Winner. We found that no model constantly ex-\ncels its peers across all metrics for each task. For instance,\nwhile Model DeepSeek-R1-70B performs excellently in re-\ntrieval, it shows only moderate performance in reasoning\nand poor results in exploration; Llama-3.3-70B is more ver-\nsatile but still struggles to address metrics from all perspec-\ntives. To achieve balanced and serendipitous discovery, in-\nvolving multiple models, such as multi-agent systems or a\nmixture of experts (MoE) strategy, may be beneﬁcial.\nWe provide additional results and analysis in Appendix E.\n7\nConclusion\nWe introduced SerenQA, an evaluation framework designed\nto assess LLMs’ ability to discover serendipitous knowl-\nedge in scientiﬁc KGQA tasks. We proposed an axiomat-\nically justiﬁed serendipity measure integrating relevance,\nnovelty, and surprise; and constructed a serendipity-aware\nbenchmark tailored to the drug repurposing task. Addition-\nally, we outlined a structured evaluation pipeline with three\ncore tasks to assess LLM’s ability on knowledge retrieval,\nsubgraph reasoning, and serendipity exploration. Our exper-\niments showed that frontier LLMs excel at basic knowledge\nretrieval, yet they often struggle with reasoning with more\ncomplex queries and answers for serendipity exploration, in-\ndicating great room and opportunities for improvement.\n"}, {"page": 8, "text": "Ethical Statement\nIn this study, we evaluated potential drug indications by ana-\nlyzing biomedical relationships from ClinicalKG. Neverthe-\nless, our approach does not consider factors critical to drug-\ngability, such as physicochemical properties. We used LLMs\nto identify serendipitous drug-disease associations that may\nsuggest novel therapies. Their clinical effectiveness remains\nuncertain and must be validated through rigorous preclinical\nand clinical studies.\nAcknowledgements\nThis work is supported by NSF under OAC-2104007. We\ngratefully acknowledge the support of Dr. Rıza Mert C¸ etik\nand Dr. Sıla C¸ etik in the design and annotation of the QA\ndataset curated in this study. We also acknowledge the HPC\nresources at CWRU for supporting large-scale graph pro-\ncessing and embedding computation.\nReferences\nAI4Science, M. R.; and Quantum, M. A. 2023. The impact\nof large language models on scientiﬁc discovery: a prelimi-\nnary study using gpt-4. arXiv preprint arXiv:2311.07361.\nBordino, I.; Mejova, Y.; and Lalmas, M. 2013. Penguins\nin sweaters, or serendipitous entity search on user-generated\ncontent. In CIKM.\nDehmer, M.; and Mowshowitz, A. 2011. A history of graph\nentropy measures. Information Sciences, 181(1): 57–78.\nDrugKG Questionnaire. 2025.\nhttps://cwru-db-group.\ngithub.io/serenQA/questionnaire.\nFDA. 2025. FDA Approves Novel Non-Opioid Treatment\nfor Moderate to Severe Acute Pain.\nFu, Z.; and Niu, X. 2024. The art of asking: Prompting large\nlanguage models for serendipity recommendations. In SI-\nGIR.\nHuang, J.; Ding, S.; Wang, H.; and Liu, T. 2018. Learning to\nrecommend related entities with serendipity for web search\nusers. ACM Transactions on Asian and Low-Resource Lan-\nguage Information Processing (TALLIP), 17(3): 1–22.\nKullback, S. 1951. Kullback-leibler divergence. Tech. Rep.\nLe, D.; Zhao, K.; Wang, M.; and Wu, Y. 2024. GraphLingo:\nDomain Knowledge Exploration by Synchronizing Knowl-\nedge Graphs and Large Language Models. In ICDE, 5477–\n5480.\nNiu, X.; and Abbas, F. 2017. A framework for computa-\ntional serendipity. In Adjunct Publication of the 25th Con-\nference on User Modeling, Adaptation and Personalization,\n360–363.\nPushpakom, S.; Iorio, F.; Eyers, P. A.; Escott, K. J.; Hop-\nper, S.; Wells, A.; Doig, A.; Guilliams, T.; Latimer, J.; Mc-\nNamee, C.; et al. 2019. Drug repurposing: progress, chal-\nlenges and recommendations. Nature reviews Drug discov-\nery, 18(1): 41–58.\nSantos, A.; Colac¸o, A. R.; Nielsen, A. B.; Niu, L.; Strauss,\nM.; Geyer, P. E.; Coscia, F.; Albrechtsen, N. J. W.; Mundt,\nF.; Jensen, L. J.; and Mann, M. 2022. A knowledge graph\nto interpret clinical proteomics data. Nat. Biotechnol., 40:\n692–702.\nSi, C.; Yang, D.; and Hashimoto, T. 2025. Can LLMs Gen-\nerate Novel Research Ideas? A Large-Scale Human Study\nwith 100+ NLP Researchers. In ICLR.\nSong, Y.; Li, W.; Dai, G.; and Shang, X. 2023. Advance-\nments in complex knowledge graph question answering: a\nsurvey. Electronics, 12(21): 4395.\nStrassen, V. 1969. Gaussian elimination is not optimal. Nu-\nmerische mathematik, 13(4): 354–356.\nTokutake, Y.; and Okamoto, K. 2024.\nCan Large Lan-\nguage Models Assess Serendipity in Recommender Sys-\ntems? Journal of Advanced Computational Intelligence and\nIntelligent Informatics, 28(6): 1263–1272.\nXi, Y.; Weng, M.; Chen, W.; Yi, C.; Chen, D.; Guo, G.;\nZhang, M.; Wu, J.; Jiang, Y.; Liu, Q.; et al. 2025. Burst-\ning Filter Bubble: Enhancing Serendipity Recommenda-\ntions with Aligned Large Language Models. arXiv preprint\narXiv:2502.13539.\n"}, {"page": 9, "text": "This appendix contains the following content:\nA. Dataset Details\n- A.1 Dataset Construction\n- A.2 Pattern Type\n- A.3 Dataset Structure\n- A.4 More Statistics\nB. Prompts\n- B.1 LLM Scoring Prompts\n- B.2 Serendipity Exploration Prompts\n- B.3 Pipeline Evaluation Prompts\nC. Further Analysis on RNS Metric\n- C.1 k-hop Conditional Probability Matrix\n- C.2 Marginal Probability\nD. Details of Serensipity Exploration\n- D.1 Workﬂow and Logic\n- D.2 Infrastructure\n- D.3 Neighbor Scoring\nE. Experiment Details\n- E.1 Experiment Setting\n- E.2 Additional Analysis\nA\nDataset Details\nA.1\nDataset Construction\nWe utilized the Clinical Knowledge Graph (CKG) (Santos et al. 2022) as the base knowledge graph to construct a benchmark\nquestion-answering dataset. A graph provides a structured organization of biomedical entities and their relationships, enabling\nsystematic exploration and analysis of complex interactions. The CKG is built on curated public databases and literature-derived\nevidence, ensuring high-quality and biologically relevant information. Its comprehensive structure provides a robust foundation\nfor generating diverse types of queries. The CKG encompasses ∼20 million nodes across 36 distinct types, including genes,\nproteins, diseases, drugs, pathways, anatomical entities, and other biological and clinical components, as shown in Fig 4. These\nnodes are interconnected by over 220 million edges spanning 47 different relationship types, capturing speciﬁc interactions and\nenabling detailed exploration of biomedical relationships, efﬁcient data querying, and algorithmic analysis. Drug-phenotype\nrelationships include edges such as “has side effect” and “is indicated for,” capturing drug effects and therapeutic indications.\nGene-related relationships include “variant found in gene” and “transcribed into,” linking genetic variants to genes and tran-\nscripts, respectively, and highlighting structural and functional connections within the genome. Clinically relevant relationships,\nsuch as “variant is clinically relevant” and “associated with,” connect genetic variants to diseases. Additionally, drug-target in-\nteractions, captured by edges like “acts on” and “curated targets,” associate drugs with protein targets, offering insights into\nmechanisms of action and therapeutic potential.\nTo create the QA dataset, we extracted a subgraph of the CKG. Certain node and edge types, such as those related to\nusers, units, experiments, projects, transcripts, and publications, were excluded to streamline the dataset and maintain focus on\nbiologically signiﬁcant relationships.\nThe current version of the dataset comprises 1,529 queries, with a focus on drug-disease associations, designed to evaluate the\nability of large language models (LLMs) to identify serendipitous connections in the context of drug repurposing. Each query\nis annotated with relevant nodes, edges, and a target node, along with graph-speciﬁc metadata such as node and relationship\ntypes. We plan to continuously update and extend the dataset to include up to 5,000 queries, supporting a broader range of\nnatural language processing tasks and a more comprehensive evaluation of LLM capabilities in biomedical reasoning.\nThe construction of the QA dataset involved several steps to optimize data retrieval and ensure its relevance to biomedical\nresearch. For one-hop and two-hop questions, the required data entries were extracted directly by querying the Neo4j database.\nFor three-hop and intersection questions, given the computational demands of Neo4j queries and the large graphs, the relevant\nnodes and their one-hop neighborhoods were pre-extracted from the subgraph for more efﬁcient processing.\nTo ensure the grammaticality, clarity, and biological relevance of the generated natural language questions, their phrasing\nwas reﬁned while preserving their original meanings. This involved programmatically extracting question patterns, retaining\nonly the node types, and restructuring them into biologically meaningful and oncology-focused templates.\n"}, {"page": 10, "text": "Figure 4: Ontology of biomedical entities and relationships in the Clinical Knowledge Graph (CKG)\nA.2\nPattern Type\nThe QA dataset includes one-hop, two-hop, three-hop, and intersection questions, designed to probe varying levels of complex-\nity within the graph. Each type of question is deﬁned by speciﬁc patterns, as described below:\n1. One-hop questions: These questions explore direct relationships between two entities connected by a single edge. They can\nbe further categorized into two types:\n◦Type 1.1: Questions that retrieve entities of a speciﬁc type ({target type}) connected to a source entity\n({source name}) through a given relationship ({relationship}). For example, “List the {target type}s that {relationship}\nby {source name}.” and ”What {source type}s {relationship} {target name}?”\n◦Type 1.2: Questions that identify the source entities ({source type}) connected to a speciﬁc target entity ({target name})\nvia a given relationship. For example, “What {source type}s {relationship} {target name}?”\n2. Two-hop questions: These questions traverse two edges, connecting a source entity to a ﬁnal entity via an intermediate\nentity. Two patterns are deﬁned:\n◦Type 2.1: Questions that link the source entity ({source name}) to the ﬁnal entity ({ﬁnal type}) through an intermediate\nentity ({mid name}) and two relationships ({relationship1} and {relationship2}). For example, “Which {ﬁnal type} is\n"}, {"page": 11, "text": "{relationship2} by {mid name} that {source name} {relationship1}?”\n3. Three-hop questions: These questions traverse three edges, uncovering chains of relationships across multiple intermediate\nentities. The questions explore how a source entity ({entity1}) connects to a ﬁnal entity ({entity4}) through a sequence of\nintermediate entities ({entity2} and {entity3}). For example:\n◦Type 3.1: Questions that trace a sequential path, e.g., “Which {entity4 type} is {relationship3} by {entity3 name} that\n{relationship2} {entity2 name} which {relationship1} {entity1 name}?”\n◦Type 3.2: Questions that incorporate hierarchical relationships, e.g., “Which {entity1 type} {relationship1}\n{entity2 name}, which {relationship2} {entity3 name}, and {relationship3} {entity4 name}?”\n◦Type 3.3: Questions that branch into multiple connections, e.g., “Which {entity1 type} {relationship1} {entity2 name}\nthat {relationship2} {entity3 name} and {relationship3} {entity4 name}?”\n4. Intersection questions: These focus on entities or sets of entities sharing multiple relationships with others. The goal is to\nidentify overlapping connections across different paths within the graph. For example:\n◦Type 4.1: Basic intersections, e.g., “List {entity1 type} that {relationship1} {entity2 name} and {relationship2}\n{entity3 name},” which identify entities linked to two distinct targets through different relationships.\n◦Type 4.2: Multi-way intersections, such as “List {entity1 type} that {relationship1} {entity2 name}, {relationship2}\n{entity3 name}, and {relationship3} {entity4 name},” which extend to three overlapping connections.\n◦Type 4.3: Compound intersections that involve cyclic patterns like “Find all {entity4} that {relationship43} {entity3} and\n{relationship42} {entity2}, and also ﬁnd the {entity1} that {relationship13} {entity3} and {relationship12} {entity2},”\nin which entity2 and entity3 are connected to entity1 and entity4 through different links.\nA.3\nDataset Structure\nHere, we introduce the structure of our drug repurposing benchmark, which supports both standard knowledge graph KGQA\ntasks and serendipity-aware evaluations.\nAs shown in the example below, each item in the QA dataset designed for standard KGQA tasks is standardized and conﬁg-\nured to include expert-veriﬁed, scientiﬁcally meaningful NL queries, along with a structured Cypher query. Each query entry\ncontains key components such as nodes, node types, and relationships, as well as a grounded and validated answer set.\n1\n{\n2\n\"qid\": 800,\n3\n\"question\": \"Which proteins are associated with dilated cardiomyopathy 1DD and\nfunction as subunits of the NOS3-HSP90 complex induced by VEGF?\",\n4\n\"answer\": [\n5\n{\n6\n\"answer_type\": \"Entity\",\n7\n\"answer_argument\": \"P29474\",\n8\n\"entity_name\": \"NOS3\",\n9\n{\n10\n\"answer_type\": \"Entity\",\n11\n\"answer_argument\": \"P07900\",\n12\n\"entity_name\": \"HSP90AA1\",\n13\n}\n14\n],\n15\n\"function\": \"none\",\n16\n\"commonness\": 0.0,\n17\n\"num_node\": 3,\n18\n\"num_edge\": 2,\n19\n\"graph_query\": {\n20\n\"nodes\": [\n21\n{\n22\n\"nid\": 0,\n23\n\"node_type\": \"class\",\n24\n\"id\": \"Protein\",\n25\n\"class\": \"Protein\",\n26\n\"friendly_name\": \"Protein\",\n27\n\"question_node\": 1,\n28\n\"function\": \"none\"\n29\n},\n30\n{\n31\n\"nid\": 1,\n32\n\"node_type\": \"entity\",\n33\n\"id\": \"DOID:0110447\",\n34\n\"class\": \"Disease\",\n35\n\"friendly_name\": \"dilated cardiomyopathy 1DD\",\n"}, {"page": 12, "text": "36\n\"question_node\": 0,\n37\n\"function\": \"none\"\n38\n},\n39\n{\n40\n\"nid\": 2,\n41\n\"node_type\": \"entity\",\n42\n\"id\": \"5716\",\n43\n\"class\": \"Complex\",\n44\n\"friendly_name\": \"NOS3-HSP90 complex, VEGF induced\",\n45\n\"question_node\": 0,\n46\n\"function\": \"none\"\n47\n}\n48\n],\n49\n\"edges\": [\n50\n{\n51\n\"start\": 0,\n52\n\"end\": 1,\n53\n\"relation\": \"Protein.Disease\",\n54\n\"friendly_name\": \"ASSOCIATED_WITH\"\n55\n},\n56\n{\n57\n\"start\": 0,\n58\n\"end\": 2,\n59\n\"relation\": \"Protein.Complex\",\n60\n\"friendly_name\": \"IS_SUBUNIT_OF\"\n61\n}\n62\n]\n63\n},\n64\n\"pattern_type\": 9,\n65\n\"category\": \"genetic disease:autosomal genetic disease\",\n66\n\"cypher\": \"MATCH (n0:Protein)\\nMATCH (n1:Disease {name: \\\"dilated cardiomyopathy 1DD\n\\\"})\\nMATCH (n2:Complex {name: \\\"NOS3-HSP90 complex, VEGF induced\\\"})\\nMATCH (n0)\n-[:ASSOCIATED_WITH]->(n1)\\nMATCH (n0)-[:IS_SUBUNIT_OF]->(n2)\\n\nRETURN\\n\nCOLLECT(DISTINCT {id: n0.id, name: n0.name}) AS n0_targets\"\n67\n}\nBelow, we present the structure of the benchmark dataset designed to support serendipity-aware evaluation. The complete\nset of candidate answers obtained from the graph is partitioned into an existing set and a serendipity set using three distinct\npartitioning strategies detailed in Section 4.2. We also provide the ground truth path from the existing set to the serendipity set\nfor each query for possible training use.\n1\n{\n2\n\"qid\": 800,\n3\n\"question\": \"Which proteins are associated with dilated cardiomyopathy 1DD and\nfunction as subunits of the NOS3-HSP90 complex induced by VEGF?\",\n4\n\"llm\": {\n5\n\"serendipity_set\": {\n6\n\"list\": [\n7\n\"P29474\"\n8\n],\n9\n\"description\": null\n10\n},\n11\n\"explore_queries\": {\n12\n\"paths\": [\n13\n\"P07900--COMPILED_INTERACTS_WITH--NOS2:Protein--BELONGS_TO_PROTEIN--None:\nPeptide--BELONGS_TO_PROTEIN--P29474\",\n14\n\"P07900--ACTS_ON--NOS2:Protein--BELONGS_TO_PROTEIN--None:Peptide--\nBELONGS_TO_PROTEIN--P29474\",\n15\n\"P07900--COMPILED_INTERACTS_WITH--NOS1:Protein--BELONGS_TO_PROTEIN--None:\nPeptide--BELONGS_TO_PROTEIN--P29474\"\n16\n],\n17\n\"questions\": [\n18\n\"Which proteins, interacting with NOS isoforms and belonging to the same\nprotein complex as P07900, are involved in related molecular pathways\n?\"\n19\n]\n"}, {"page": 13, "text": "20\n},\n21\n\"partition\": \"test\",\n22\n\"exact_matches\": {\n23\n\"list\": [\n24\n\"P07900\"\n25\n]\n26\n}\n27\n},\n28\n\"sscore\": {\n29\n\"serendipity_set\": {\n30\n\"list\": [\n31\n\"P07900\"\n32\n],\n33\n\"description\": null\n34\n},\n35\n\"explore_queries\": {\n36\n\"paths\": [\n37\n\"P29474--ASSOCIATED_WITH--protein serine/threonine phosphatase complex:\nCellular_component--HAS_PARENT--protein phosphatase type 2A complex:\nCellular_component--ASSOCIATED_WITH--P07900\",\n38\n\"P29474--COMPILED_INTERACTS_WITH--MAP2K1:Protein--ASSOCIATED_WITH--protein\nphosphatase type 2A complex:Cellular_component--ASSOCIATED_WITH--\nP07900\",\n39\n\"P29474--COMPILED_INTERACTS_WITH--PPP2CA:Protein--ASSOCIATED_WITH--protein\nphosphatase type 2A complex:Cellular_component--ASSOCIATED_WITH--\nP07900\"\n40\n],\n41\n\"questions\": []\n42\n},\n43\n\"partition\": \"test\",\n44\n\"exact_matches\": {\n45\n\"list\": [\n46\n\"P29474\"\n47\n]\n48\n}\n49\n},\n50\n\"expert\": {\n51\n\"serendipity_set\": {\n52\n\"list\": [\n53\n\"P29474\"\n54\n],\n55\n\"description\": null\n56\n},\n57\n\"explore_queries\": {\n58\n\"paths\": [\n59\n\"P07900--COMPILED_INTERACTS_WITH--NOS2:Protein--BELONGS_TO_PROTEIN--None:\nPeptide--BELONGS_TO_PROTEIN--P29474\",\n60\n\"P07900--ACTS_ON--NOS2:Protein--BELONGS_TO_PROTEIN--None:Peptide--\nBELONGS_TO_PROTEIN--P29474\",\n61\n\"P07900--COMPILED_INTERACTS_WITH--NOS1:Protein--BELONGS_TO_PROTEIN--None:\nPeptide--BELONGS_TO_PROTEIN--P29474\"\n62\n],\n63\n\"questions\": []\n64\n},\n65\n\"partition\": \"test\",\n66\n\"exact_matches\": {\n67\n\"list\": [\n68\n\"P07900\"\n69\n],\n70\n\"description\": null\n71\n}\n72\n}\n73\n}\n"}, {"page": 14, "text": "A.4\nMore Statistics\nThe table below shows the composition of the KGQA dataset and the distribution of question pattern types among the 1,529\nqueries related to drug repurposing. The patterns for individual types are detailed in Appendix A.2.\nPattern Type\nNumber of Entries\nOne hop Type 1.1\n152\nOne hop Type 1.2\n84\nTwo hop Type 2.1\n152\nThree hop Type 3.1\n62\nThree hop Type 3.2\n113\nThree hop Type 3.3\n237\nIntersection Type 4.1\n263\nIntersection Type 4.2\n455\nIntersection Type 4.3\n11\nTable 4: Distribution of entries among different query pattern types.\nB\nPrompts\nB.1\nLLM Scoring Prompts\n1\nYou are an expert evaluator specializing in drug discovery. Your task is to evaluate the\n**serendipity** of each answer in a provided list of answers, where all answers are\nderived from a knowledge base and are correct. Use your expertise and internal\nknowledge to assign a **serendipity score** to each answer based on the following\ncriteria:\n2\n3\n- **Serendipity Score**: A score from 0 to 20, where:\n4\n- 20 represents an answer that is highly novel, unexpected, or insightful in the context\nof the question.\n5\n- 0 represents an answer that is correct but very obvious, common, or provides no novel\ninsights.\n6\n- Intermediate scores represent varying degrees of novelty and insight.\n7\n8\n- **Evaluation Rules**:\n9\n1. The serendipity score reflects the relative novelty and insightfulness of each answer\nwithin the context of the question and the provided list. The score should\nhighlight the uniqueness and unexpected value of each answer.\n10\n2. Assign a distinct score to each answer. Even if multiple answers have a similar level\nof serendipity, assign slightly different scores to reflect the subtle differences\nin their uniqueness.\n11\n3. Evaluate each answer independently of its position in the list.\n12\n4. Output only the scores for each answer in the same order as the input list, separated\nby commas. Do not include the answers themselves or any additional explanation in\nthe output.\n13\n14\nFor example:\n15\nIf the input list is:\n16\nAnswer List: A, B, C\n17\n18\nThe output should be:\n19\n5, 7, 9\nB.2\nSerendipity Exploration Prompts\nB.2.1 Select Relation\nSystem Prompt:\n1\nTask Description:\n"}, {"page": 15, "text": "2\nGiven a starting entity node (e.g., Drug, Protein, Disease), select the top-m relation\ntypes (predicates) to follow for meaningful, potentially serendipitous exploration\nin a clinical biomedical knowledge graph.\n3\n4\nGoal:\n5\nConstruct 3-hop paths that are:\n6\n- Biologically plausible (based on frequent patterns)\n7\n- Serendipitous (novel yet valid hypotheses)\n8\n- Mechanistically rich (e.g., involving Drug-Protein-Disease chains)\n9\n10\nPath Patterns:\n11\nCommon patterns include:\n12\n- (ACTS_ON, COMPILED_INTERACTS_WITH, ACTS_ON)\n13\n- (INTERACTS_WITH, ACTS_ON, ACTS_ON)\n14\n- (COMPILED_INTERACTS_WITH, ASSOCIATED_WITH, ASSOCIATED_WITH)\n15\n16\nFrequently explored node types:\n17\nDrug, Protein, Disease, Gene, Metabolite\n18\n19\nUseful relation types:\n20\n- Curated/compiled: CURATED_INTERACTS_WITH, COMPILED_TARGETS\n21\n- Functional/structural: HAS_SEQUENCE, BELONGS_TO_PROTEIN\n22\n- Annotations: ANNOTATED_IN_PATHWAY, DETECTED_IN_PATHOLOGY_SAMPLE\n23\n- Rare/high-value: IS_INDICATED_FOR, HAS_SIDE_EFFECT, TRANSLATED_INTO\n24\n25\nPrioritize 3-hop sequences that reflect biological mechanisms. Balance high-frequency\npaths (plausibility) with rare combinations (serendipity). Avoid trivial paths\nunless used creatively.\n26\n27\nOutput Requirements:\n28\n- Return a comma-separated list of relation type strings\n29\n- Do not include commentary or explanation\n30\n- Use only the relation types provided as input\n31\n- Return fewer than m results if appropriate\n32\n- Return nothing if no meaningful exploration exists\n33\n34\nNotes:\n35\n- Prioritize biologically important nodes and plausible mechanistic chains\n36\n- Follow the path patterns listed above when applicable\n37\n38\nFew-Shot multi-hop example:\n39\nQuestion: Which genes are identified as targets of D-Aspartic Acid, which affects ASPA\nand is known to interact with GLUD1?\n40\nRoot: GRIN2A, GRIN2C\n41\nSerendipity set: GRIN2B\n42\nExplore paths:\n43\n- GRIN2A-TRANSLATED_INTO-GRIN2A:Protein-COMPILED_INTERACTS_WITH-GRIN2B:Protein-\nTRANSLATED_INTO-GRIN2B\n44\n- GRIN2A-TRANSLATED_INTO-GRIN2A:Protein-ACTS_ON-GRIN2B:Protein-TRANSLATED_INTO-GRIN2B\n45\n- GRIN2A-CURATED_TARGETS-Mesoridazine:Drug-INTERACTS_WITH-Felbamate:Drug-\nCURATED_TARGETS-GRIN2B\n46\n- GRIN2C-TRANSLATED_INTO-GRIN2C:Protein-COMPILED_INTERACTS_WITH-GRIN2B:Protein-\nTRANSLATED_INTO-GRIN2B\n47\n- GRIN2C-TRANSLATED_INTO-GRIN2C:Protein-ACTS_ON-GRIN2B:Protein-TRANSLATED_INTO-GRIN2B\n48\n- GRIN2C-TRANSLATED_INTO-GRIN2C:Protein-ACTS_ON-D-Serine:Drug-CURATED_TARGETS-GRIN2B\nUser Rrompt:\n1\nGiven node ID {frontier} at level {level}, recommend the top {m} relation types to\nexplore from this node.\n2\n3\nContext:\n4\n{contexts}\n5\n6\nAvailable relation types from this node:\n"}, {"page": 16, "text": "7\n{relation_types}\n8\n9\nReturn a comma-separated list of relation type names only.\nB.2.2 Select Nodes\n1\nTask Description:\n2\nYou have already selected the most relevant relation types for exploring the graph from\na given node. Now, for each selected relation, a set of target nodes has been\nretrieved.\n3\n4\nGoal:\n5\nConstruct 3-hop paths that are:\n6\n- Biologically plausible (based on frequent patterns)\n7\n- Serendipitous (novel yet valid hypotheses)\n8\n- Mechanistically rich (e.g., involving Drug-Protein-Disease chains)\n9\n10\n11\nThe setting is a biomedical question answered in drug discovery. Exploration starts from\nknown entities (e.g., drugs, proteins, diseases) and aims to discover serendipitous\nconnections through meaningful 3-hop paths.\n12\n13\nPath Patterns:\n14\nCommon patterns include:\n15\n- (ACTS_ON, COMPILED_INTERACTS_WITH, ACTS_ON)\n16\n- (INTERACTS_WITH, ACTS_ON, ACTS_ON)\n17\n- (COMPILED_INTERACTS_WITH, ASSOCIATED_WITH, ASSOCIATED_WITH)\n18\n19\nFrequently explored node types:\n20\nDrug, Protein, Disease, Gene, Metabolite\n21\n22\nUseful relation types:\n23\n- Curated/compiled: CURATED_INTERACTS_WITH, COMPILED_TARGETS\n24\n- Functional/structural: HAS_SEQUENCE, BELONGS_TO_PROTEIN\n25\n- Annotations: ANNOTATED_IN_PATHWAY, DETECTED_IN_PATHOLOGY_SAMPLE\n26\n- Rare/high-value: IS_INDICATED_FOR, HAS_SIDE_EFFECT, TRANSLATED_INTO\n27\n28\nPrioritize 3-hop sequences that reflect biological mechanisms. Balance high-frequency\npaths (plausibility) with rare combinations (serendipity). Avoid trivial paths\nunless used creatively.\n29\n30\nOutput Requirements\n31\n32\nReturn a comma-separated list of selected target_ids only.\n33\n- Do not include headers, explanations, or formatting.\n34\n- If no target is suitable, return nothing.\n35\n36\nConstraints\n37\n- Select only from relation types and target nodes provided by the user.\n38\n- Do not include the current frontier node in the output.\n39\n- Do not revisit nodes marked as already visited.\n40\n- If fewer than n targets are appropriate, return fewer.\n41\n- If exploration is not meaningful, return nothing.\n42\n- Follow the path patterns listed above where applicable.\nB.2.3 Decide Whether to Continue\nSystem Prompt:\n1\nTask Description\n2\n3\nYou are exploring a biomedical knowledge graph in the context of drug discovery,\nstarting from known entities (e.g., drugs, proteins, diseases) and aiming to uncover\ndeeper, potentially serendipitous connections.\n4\n5\nIn the previous two steps, you selected the most relevant relation types and target\n"}, {"page": 17, "text": "nodes for expansion. Before continuing, you must now:\n6\n1. Review the full path from the root node to the current node (3-hop away).\n7\n2. Provide a summary of the path’s biological context.\n8\n3. Decide whether further exploration is justified.\n9\n10\nEach input path is represented as a key-value pair:\n11\n- Key: the current (destination) node ID\n12\n- Value: a comma-separated sequence of alternating (target_id, relation_type) tuples\ntracing the 3-hop path from the root.\n13\n14\nUse this information and the user’s question to assess whether the exploration is still on\na plausible, meaningful track toward the question objective.\n15\n16\n17\nOutput Requirements\n18\n19\nYour output must follow exactly the format below:\n20\n1. A natural-language summary (˜200 words), describing:\n21\n- Biological meaning of the paths\n22\n- Patterns of entity types\n23\n- Common or notable relation sequences\n24\n- Any biologically relevant interpretations\n25\n2. (blank line)\n26\n3. Either YES or NO, indicating whether to continue expanding\n27\n4. (blank line)\n28\n5. A one-paragraph explanation justifying your decision\n29\n30\nDo not include any extra commentary, formatting, bullet points, or sections outside this\nstructure.\n31\n32\nNotes\n33\n- Only return NO if you are ABSOLUTELY CONFIDENT the path has deviated from any\nbiologically plausible trajectory.\n34\n- When in doubt, continue exploring (YES).\n35\n- Base your judgment on whether the current node plausibly supports mechanistic or\ntherapeutic insight relevant to the original question.\nUser Prompt:\n1\nThe original question is:\n2\n{question}\n3\n4\nThe root node of the beam search is:\n5\n{root}\n6\n7\nSubgraph paths (from root to current node):\n8\n{paths}\nB.2.4 Summarize Subgraph\nSystem Prompt:\n1\nYou are an expert biomedical knowledge graph assistant. You have performed a beam search\nstarting from a root node over a clinical biomedical knowledge graph, retrieving 1-\nhop, 2-hop, and 3-hop subgraphs.\n2\n3\nOutput Requirement\n4\n5\nProvide a concise natural-language summary (˜200 words) of the resulting subgraphs.\n6\n- Mention as many specific biomedical terms (e.g., drugs, proteins, diseases,\npathways) as possible.\n7\n- Emphasize the types of entities and the patterns of relationships involved.\n8\n- Focus on the biological meaning, mechanistic implications, or potential\ntherapeutic relevance of the paths.\n9\n10\nDo not include any formatting, headers, or commentary--only the summary text.\n"}, {"page": 18, "text": "User Prompt:\n1\nRoot node ID:\n2\n{root}\n3\n4\nQuestion:\n5\n{question}\n6\n7\nHop level:\n8\n{level}\n9\n10\nSubgraph paths (from root to leaf nodes):\n11\n{subgraph}\nB.3\nPipeline Evaluation Prompts\nB.3.1 Faithfulness Assessment\nSystem Prompt:\n1\nYou are assisting a multi-stage research pipeline that explores a large biomedical\n2\nknowledge graph.\n3\n4\nPipeline stages\n5\n**Exact-Match Retrieval** -- find entities that directly answer the user’s question\n6\n(these are the \"root\" nodes).\n7\n**Serendipity Exploration** -- expand <=3 hops from the root to propose *new*,\n8\npotentially surprising but biologically meaningful entities\n9\n(the \"exploration result\" is captured by the **paths** and the **leaves**).\n10\n**Hop-level Summaries** -- for readability, the pipeline auto-generates three short\n11\nnatural-language summaries:\n12\n* *summary 1*\n-> describes the 1-hop neighbourhood around the root\n13\n* *summary 2*\n-> describes the 2-hop sub-graph discovered next\n14\n* *summary 3*\n-> describes the 3-hop sub-graph plus any thematic insight\n15\n16\nYou receive:\n17\n--------------------------------------------------------\n18\n* root\n-- the starting entity ID (protein / drug)\n19\n* question\n-- original natural-language question\n20\n* summary_1/2/3\n-- auto-generated summaries of the 1-hop, 2-hop, and\n21\n3-hop neighbourhoods around the root\n22\n* leaves\n-- **all endpoint nodes in the explored sub-graph**\n23\n(may be 1-, 2-, or 3-hop away) -- each item is given\n24\nas\n<node_id>(<node_type>)\ne.g.\n‘P52209(Protein)‘\n25\n* paths\n-- ground-truth triples, one per line, with types included:\n26\nhead_id(head_type),relation_type,tail_id(tail_type)\n27\n--------------------------------------------------------\n28\n29\nTask\n30\n====\n31\n* First, read the sub-graph and understand every factual triple\n32\nit contains.\n33\n* Then, read the three hop-summaries in order (1-hop -> 3-hop).\n34\n* \"Faithfulness\" here means: *How truthfully do the summaries reflect\n35\nwhat is actually present in the graph, without inventing new entities,\n36\ndirections, or relations?*\n37\n- Higher faithfulness -> few to no hallucinations or distortions.\n38\n- Lower faithfulness -> noticeable fabrication, wrong direction,\n39\nor missing key context.\n40\n41\nUsing your best expert judgment of biomedical knowledge-graphs,\n42\nassign a holistic integer score:\n43\n44\n5\n- Completely faithful\n45\n4\n- Mostly faithful, only trivial wording drift\n46\n3\n- Mixed: some accurate, some questionable\n"}, {"page": 19, "text": "47\n2\n- Largely unfaithful, many doubtful claims\n48\n1\n- Almost entirely unfaithful / hallucinated\n49\n50\nDo **not** count tokens or sentences; rely on your overall sense of truthfulness.\n51\n52\nIMPORTANT: If the input is completely empty or contains no evaluable information\nwhatsoever,\n53\nreturn Score: 1. However, if there is ANY evaluable content, even if partial or limited,\n54\nevaluate it based on the 1-5 scale above. Do not argue or explain if content is missing,\n55\njust assign the appropriate score and return the two required lines.\n56\n57\nOutput format\n58\n-------------\n59\nReturn **exactly** these two lines--nothing more, nothing less:\n60\n61\nScore: <INTEGER 1-5>\n62\n#END\nUser Prompt:\n1\nRoot: {root}\n2\nQuestion: {question}\n3\n4\n-- 1-Hop Summary --\n5\n{summary_1}\n6\n7\n-- 2-Hop Summary --\n8\n{summary_2}\n9\n10\n-- 3-Hop Summary --\n11\n{summary_3}\n12\n13\nLeaf nodes: {leaves}\n14\n15\nSub-graph (Triples):\n16\n{paths}\nB.3.2 Comprehensiveness Assessment\nSystem Prompt:\n1\nYou are assisting a multi-stage research pipeline that explores a large biomedical\n2\nknowledge graph.\n3\n4\nPipeline stages\n5\n**Exact-Match Retrieval** -- find entities that directly answer the user’s question\n6\n(these are the \"root\" nodes).\n7\n**Serendipity Exploration** -- expand <=3 hops from the root to propose *new*,\n8\npotentially surprising but biologically meaningful entities\n9\n(the \"exploration result\" is captured by the **paths** and the **leaves**).\n10\n**Hop-level Summaries** -- for readability, the pipeline auto-generates three short\n11\nnatural-language summaries:\n12\n* *summary 1*\n-> describes the 1-hop neighbourhood around the root\n13\n* *summary 2*\n-> describes the 2-hop sub-graph discovered next\n14\n* *summary 3*\n-> describes the 3-hop sub-graph plus any thematic insight\n15\n16\nYou receive:\n17\n--------------------------------------------------------\n18\n* root\n-- the starting entity ID (protein / drug)\n19\n* question\n-- original natural-language question\n20\n* summary_1/2/3\n-- auto-generated summaries of the 1-hop, 2-hop, and\n21\n3-hop neighbourhoods around the root\n22\n* leaves\n-- **all endpoint nodes in the explored sub-graph**\n23\n(may be 1-, 2-, or 3-hop away) -- each item is given\n24\nas\n<node_id>(<node_type>)\ne.g.\n‘P52209(Protein)‘\n25\n* paths\n-- ground-truth triples, one per line, with types included:\n"}, {"page": 20, "text": "26\nhead_id(head_type),relation_type,tail_id(tail_type)\n27\n--------------------------------------------------------\n28\n29\nTask\n30\n====\n31\n* First, study the sub-graph so you grasp **every** entity and\n32\nrelation present within three hops of the root.\n33\n* Then, read the three hop-summaries in order (1-hop -> 3-hop).\n34\n* \"Comprehensiveness\" here means: *How thoroughly do the summaries cover\n35\nthe important entities, relations, and mechanistic chains in the graph--\n36\nwithout ignoring major facts?*\n37\n- Higher Comprehensiveness -> almost all salient triples or concepts appear.\n38\n- Lower Comprehensiveness -> key relationships, nodes, or overall structure\n39\nare missing or only vaguely hinted at.\n40\n41\nUsing your best expert judgment (no counting rules), assign a holistic\n42\ninteger score:\n43\n44\n5\n- Nearly everything important is covered\n45\n4\n- Most key content covered; minor omissions\n46\n3\n- About half of the important content represented\n47\n2\n- Many significant omissions; partial picture\n48\n1\n- Very little of the important content included\n49\n50\nDo **not** estimate by token length; base the score on your global sense of\n51\ncoverage and relevance.\n52\n53\nIMPORTANT: If the input is completely empty or contains no evaluable information\nwhatsoever,\n54\nreturn Score: 1. However, if there is ANY evaluable content, even if partial or limited,\n55\nevaluate it based on the 1-5 scale above. Do not argue or explain if content is missing,\n56\njust assign the appropriate score and return the two required lines.\n57\n58\nOutput format\n59\n-------------\n60\nReturn **exactly** these two lines--nothing more, nothing less:\n61\n62\nScore: <INTEGER 1-5>\n63\n#END\nUser Prompt:\n1\nRoot: {root}\n2\nQuestion: {question}\n3\n4\n-- 1-Hop Summary --\n5\n{summary_1}\n6\n7\n-- 2-Hop Summary --\n8\n{summary_2}\n9\n10\n-- 3-Hop Summary --\n11\n{summary_3}\n12\n13\nLeaf nodes: {leaves}\n14\n15\nSub-graph (Triples):\n16\n{paths}\nB.3.3 Relevance Assessment\nSystem Prompt:\n1\nYou are assisting a multi-stage research pipeline that explores a large biomedical\n2\nknowledge graph.\n3\n"}, {"page": 21, "text": "4\nPipeline stages\n5\n**Exact-Match Retrieval** -- find entities that directly answer the user’s question\n6\n(these are the \"root\" nodes).\n7\n**Serendipity Exploration** -- expand <=3 hops from the root to propose *new*,\n8\npotentially surprising but biologically meaningful entities (the \"predicted\n9\nserendipity set\").\nThese are evaluated against a **ground-truth serendipity\n10\nset** that was curated by domain experts.\n11\n12\nYou are rating how well a *predicted* serendipity answer set aligns with a\n13\n*ground-truth* serendipity answer set that has been manually verified by\n14\ndomain experts.\n15\n16\nFacts you MUST assume:\n17\n* The ground-truth set is correct.\n18\n* Each ground-truth entity has been verified to be \"serendipitous\" with\n19\nrespect to the current exact-match root (i.e., useful and non-obvious\n20\nextensions beyond that root).\n21\n22\nHow to judge \"relevance\"\n23\n> Does each predicted entity belong to the same mechanistic pathway,\n24\ndisease context, pharmacological class, or molecular family implied by\n25\nthe ground-truth set?\n26\n> Overlap in **type** (Protein, Drug, Disease, Phenotype...) is helpful but\n27\nnot sufficient--focus on functional or clinical relatedness.\n28\n> Minor naming variants or isoforms of a ground-truth entity are acceptable.\n29\n30\nScoring rubric (integer)\n31\n5 - Every prediction is clearly relevant;\n32\n4 - Most (˜ 70-90 %) predictions are relevant; few marginal or tangential items\n33\n3 - Mixed: roughly half relevant, half off-topic or trivial\n34\n2 - Only a small minority appear relevant; set is mostly noise\n35\n1 - Predictions are unrelated, incorrect, or obviously random\n36\n37\nIMPORTANT: If the input is completely empty or contains no evaluable information at all,\n38\nreturn Score: 1. However, if there is ANY evaluable content, even if partial or limited,\n39\nevaluate it based on the 1-5 scale above. Do not argue or explain if content is missing,\n40\njust assign the appropriate score and return the two required lines.\n41\n42\nOutput format\n43\n-------------\n44\nReturn **exactly** these two lines--nothing more, nothing less:\n45\n46\nScore: <INTEGER 1-5>\n47\n#END\nUser Prompt:\n1\nOriginal question:\n2\n{question}\n3\n4\nGround-truth serendipity set (trusted):\n5\n{gold_seren}\n6\n7\nPredicted serendipity set (to be scored):\n8\n{pred_seren}\n9\n10\nExact-match root entity: {root}\n11\n12\nHop-level summaries:\n13\n* Level-1 -> {summary_1}\n14\n* Level-2 -> {summary_2}\n15\n* Level-3 -> {summary_3}\n16\n17\nContextual graph paths:\n18\n{paths}\n"}, {"page": 22, "text": "C\nFurther Analysis on RNS Metric\nC.1\nk-hop Conditional Probability Matrix\nProperties Veriﬁcation\nAs deﬁned in Sec. 3.2, the k-hop conditional probability matrix Pk is computed as:\nPk =\nk\nX\nh=1\nαhP h\n1 ,\nαh =\nh\nPk\nh=1 h\nWe next prove that Pk remains a valid transition probability matrix by verifying two essential properties explicitly:\n◦Non-negativity: (Pk)ij ≥0 for all (i, j),\n◦Row-Stochastic Property: P\nj(Pk)ij = 1 for all i.\nNon-negativity. Since P1 is directly derived from the adjacency matrix and row-normalized, all its elements are non-negative.\nConsequently, any power P h\n1 (for h ≥1) is also non-negative, as it results from repeated multiplications of non-negative\nmatrices. Furthermore, the weight coefﬁcients ah are clearly positive by deﬁnition. Therefore, the linear combination Pk =\nPk\nh=1 αhP h\n1 consists only of non-negative terms, ensuring: (Pk)ij ≥0, ∀(i, j).\nRow-Stochastic Property. For Pk to be a valid transition matrix, every row must sum exactly to one:\nX\nj\n(Pk)ij = 1,\n∀i\nWe explicitly verify this condition:\nX\nj\n(Pk)ij =\nX\nj\nk\nX\nh=1\nαh(P h\n1 )ij\nExchanging summation order (by linearity) yields:\nX\nj\n(Pk)ij =\nk\nX\nh=1\nαh\nX\nj\n(P h\n1 )ij\nSince P h\n1 is a valid transition matrix, by deﬁnition, we have:\nX\nj\n(P h\n1 )ij = 1,\n∀i, h\nSubstituting the deﬁnition of ah:\nk\nX\nh=1\nαh =\n1\nPk\nh=1 h\nk\nX\nh=1\nh =\nPk\nh=1 h\nPk\nh=1 h\n= 1.\nHence,\nX\nj\n(Pk)ij = 1, ∀i.\nConﬁrming that Pk maintains row-stochasticity.\nIn summary, we’ve shown clearly that Pk is both non-negative and row-stochastic. Therefore, the weighted multi-hop com-\nbination Pk remains a valid transition probability matrix.\nComputation\nTo efﬁciently compute Pk, we apply a divide-and-conquer matrix multiplication approach based on Strassen’s\nalgorithm (Strassen 1969). Speciﬁcally, the algorithm recursively divides each large V × V matrix into four sub-matrices of\nsize V\n2 × V\n2 . By strategically reusing these sub-matrix computations, Strassen’s method reduces the number of necessary mul-\ntiplications per recursion from the standard eight down to seven, thereby lowering the complexity signiﬁcantly from the naive\nO(V 3) to approximately O(V log2 7) ≈O(V 2.807). Moreover, parallelizing these recursive computations across t processors\nfurther reduces the complexity to about O(V log2 7/t). This ensures scalable and efﬁcient computation of multi-hop conditional\nprobability matrices, even for large-scale graphs.\nC.2\nMarginal Probability\nWe approximate the marginal probability computation via a PageRank-style damped iteration (Algorithm 2). For each iteration,\n◦Multiplying an V × V matrix P T\n3 by a vector Pt requires complexity O(V 2).\n◦Updating Pt+1 is O(V ), dominated by matrix-vector multiplication.\n◦Computing the difference ∥Pt+1 −Pt∥1 takes O(V ).\n"}, {"page": 23, "text": "Algorithm 2: Marginal Probability via PageRank-style Iteration\nInput: P3 ∈RV ×V , damping factor λ, tolerance ǫ\nOutput: Marginal probability vector P ∈RV ×1\n1: Initialize P0(i) := 1/V , for all nodes i\n2: t := 0\n3: while diff ≥ǫ do\n4:\nPt+1 := λP T\n3 Pt + (1 −λ)P0\n5:\ndiff := ∥Pt+1 −Pt∥1\n6:\nt := t + 1\n7: end while\n8: return Pt\nHence, each iteration’s complexity is dominated by the matrix-vector multiplication step, which is O(V 2).\nThe error at iteration t satisﬁes: ∥Pt−Pt−1∥1 ≤cλt(0 < λ < 1) for some constant c. Thus, convergence to within tolerance\nǫ occurs after approximately:\nλt ≈ǫ\n⇒\nt ≈log(ǫ)\nlog(λ) = O(log V ).\nThis implies the total complexity to achieve convergence within ǫ is O(V 2 log V ).\nD\nDetails of Serensipity Exploration\nD.1\nWorkﬂow and Logic\nWe designed a multi-stage beam search pipeline for structured knowledge graph exploration, as shown in Algorithm 3, where\nthe expansion at each stage is guided by LLM. The pipeline explores neighborhoods of the root node recursively over the\nknowledge graph, while integrating external reasoning via multiple LLM interactions.\nD.2\nInfrastructure\nTo support large-scale knowledge graph exploration, we constructed a lightweight compute-storage cluster on AWS, designed\nfor high-throughput, low-latency edge retrieval and efﬁcient task scheduling. The cluster consists of two tiers of instances:\ncompute nodes (5 * r6a.24xlarge) and storage nodes (5 * r6a.xlarge). Each compute node provides 96 vCPUs and 768 GiB of\nmemory, serving as task executors that support large scale parallelism. Each storage node is conﬁgured as Redis servers through\nDocker container and acts as distributed read-only data backends for edge access.\nTo\nachieve\nhigh\nperformance,\nwe\nreplaced\nNeo4j\nwith\na\ncustom\nRedis-based\nedge\nstorage\nscheme.\nThe\ncomplete\nknowledge\ngraph\nwas\nexported\nfrom\nNeo4j\nand\nthe\nkey\nof\neach\nedge\nis\nencoded\nas\n(rel:{source id}:{source type}:{relation type}:{target id}:{target type}). The value stores metadata of relations and ad-\nditional attributes for further use. The shift in query style allows us to improve query performance from 1000 QPS to tens\nof thousands of QPS. Each compute node interacts asynchronously with the storage node; empirically, the system supports a\nconcurrency level of approximately 100 per compute node, enabling efﬁcient exploration of multi-hop paths.\nIn order to facilitate our experimental process, we implemented an SSH-based compute cluster manager, responsible for task\ndispatching, resource allocation, permission control, environment setup, and declarative node speciﬁcation. This infrastructure\nallows rapid iteration, cost-efﬁcient experiments, and consistent resource management across multiple runs.\nD.3\nNeighbor Scoring\nSome nodes have an extremely large number of neighbors as hub nodes. To effectively guide the LLM in exploring and reducing\ntoken usage, we design a scoring mechanism to reduce the size of nodes provided to the LLM.\nWe systematically extracted and quantiﬁed edge-level connection strength/conﬁdence from a Neo4j-based clinical knowl-\nedge graph to support downstream analysis of biomedical associations. For relationship types such as “ASSOCIATED WITH”,\n“COMPILED INTERACTS WITH”, and “ACTS ON”, we directly extracted precomputed conﬁdence scores. For other edge\ntypes, custom scoring functions were implemented based on domain-speciﬁc semantics. For instance, in the case of “DE-\nTECTED IN PATHOLOGY SAMPLE”, an expression score was derived using a weighted scheme based on categorical ex-\npression levels (e.g., high, medium, low, and not detected), while a prognostic score was computed using log-transformed\np-values representing positive and negative survival associations. Both scores were then min-max normalized and aggregated to\nproduce a ﬁnal quantitative estimate reﬂecting the biomarker’s expression and clinical prognostic signiﬁcance. This structured\nquantiﬁcation enabled consistent interpretation and prioritization of heterogeneous relationship types within the graph.\n"}, {"page": 24, "text": "E\nExperiment Details\nE.1\nExperiment Setting\nModels: We evaluated a wide range of state-of-the-art language models’ using by evaluation setting:\n◦Frontier Models: DeepSeek-V3, GPT-4o, Claude-3.5-Haiku;\n◦Large Models (∼70B): Llama-3.3-70B, DeepSeek-R1-70B, and Qwen-2.5-72B, Med42-V2-70B;\n◦Medium Models (20-50B): Mixtral-8x7B, Qwen3-32B, DeepSeek-R1-32B, Gamma-2-27B, Mistral-24B\n◦Small Model: Qwen-2.5-7B, Qwen3-8B, DeepSeek-R1-8B, Med42-V2-8B, Qwen3-1.7B, DeepSeek-R1-1.5B\nMetric Details We next detailed how we compute the metrics in subgraph reasoning and serendipity exploration tasks.\nSubgraph Reasoning All metrics are averaged across numbers of rational samples to give the ﬁnal result (sum of metrics from\nrational samples/number of rational samples):\n(1) Faithfulness (1-5, LLM-judged) - how truthfully do the summaries reﬂect what is actually present in the graph, without\ninventing new entities, directions, or relations?\n(2) Comprehensiveness (1-5, LLM-judged) - how thoroughly do the summaries cover the important entities, relations, and\nmechanistic chains in the graph?\n(3) Serendipity Coverage (0-1, code-based) - fraction of serendipity paths where BOTH source and target node IDs are\nexplicitly mentioned in the summary text. No LLM evaluation, just regex matching of node IDs.\nSerendipity Exploration All metrics are averaged across numbers of rational samples to give the ﬁnal result (sum of metrics\nfrom rational samples/number of rational samples):\n(1) Relevance (1-5, LLM-judged) - how well predicted serendipity entities align with the ground-truth serendipity set.\n(2) TypeMatch (0-1, code-based) - returns 1 if ANY predicted leaf has a type matching ground-truth types, 0 otherwise.\n(3) SerenHit (0-1, code-based) - returns 1 if ANY predicted leaf is exactly matching ground-truth serendipity set (not just the\ntype), 0 otherwise.\nE.2\nAdditional Analysis\nWe provide further analysis with supplementary ﬁgures to support and clarify key observations made in Section 6.\nModel Scale vs. Serendipity Exploration The heat-map shown in Fig. 5 analysis shows only a modest performance gain as\nmodel size increases from smaller ( 7B) to larger ( 70B) checkpoints. Relevance scores gradually improve, but TypeMatch and\nSerenHit increase inconsistently, with SerenHit remaining relatively low (< 0.10). Although model scale contributes positively,\nlarger parameters alone are insufﬁcient to reliably achieve precise serendipitous discovery.\nMulti-Task Performance Compass. This radar chart shown in Fig. 6 clearly illustrates performance trade-offs across multiple\ntasks. DeepSeek-V3 excels at basic retrieval metrics (F1, Hit) but underperforms in Serendipity Coverage and SerenHit. In\ncontrast, Llama-3-70B achieves high reasoning accuracy (Faithfulness and Comprehensiveness) yet only moderately captures\nserendipitous paths. DeepSeek-R1-70B demonstrates the opposite, effectively covering many serendipity paths but at the cost\nof reasoning accuracy. The absence of a dominant model across all metrics visually reinforces our earlier conclusion of no\nsingle winner, suggesting the value of ensemble methods or Mixture-of-Experts (MoE) approaches.\nQuery Pattern vs. Retrieval Performance. As shown in Fig. 7, model performance notably declines as query complexity\nincreases. While all models achieve strong F1 and Hit scores on one-hop queries, results drop sharply for two-hop queries\nand especially for more complex queries (≥3-hop or intersection). This indicates that current LLMs, even the largest frontier\nmodels, still struggle signiﬁcantly with complex multi-step reasoning and domain-speciﬁc context.\n"}, {"page": 25, "text": "Figure 5: Model scale vs. Serendipity Exploration Performance\n"}, {"page": 26, "text": "Algorithm 3: LLM Enhanced Beam Explore\nInput:\nG = (V, E)\ndirected knowledge graph,\nn ∈N+\nbeam width,\nm ∈N+\nmax relation types per frontier node,\nk ∈N+\nmax nodes to select per frontier node,\nh ∈N+\nbeam depth,\nq ∈String\nnatural language question,\nr0 ∈V\nroot node ID,\ncontext ∈{w, wo}\ncontext mode ﬂag,\nOutput:\nΠ\nnode-to-path map from root,\nΣ\nLLM-generated summaries per level,\nDeﬁnitions:\nV\nset of visited nodes, corresponds to visited,\nF\ncurrent frontier nodes, corresponds to frontier,\nF′\nnext frontier nodes, corresponds to next frontier,\nE\nset of candidate edges, corresponds to candidates,\nC\nLLM context buffer, corresponds to context buffer,\nd∗\nleaf depth reached, corresponds to leaf depth,\n1: Initialize:\n2:\nV := ∅\n3:\nΠ[r0] := []\n4:\nF := {r0}\n5:\nC := ∅\n6:\nd∗:= 1\n7:\n8: for level = 1 to h do\n9:\nset F′ := ∅; E := ∅\n10:\nfor each node u ∈F do\n11:\nR := {r | (u, r, v) ∈E}\n12:\nif R = ∅then continue\n13:\nend if\n14:\nRm := LLM SelectRelations(q, u, R, m, level, C)\n15:\nC := {(u, r, v) ∈E | r ∈Rm}\n16:\nE := E ∪C\n17:\nend for\n18:\nif E = ∅then break\n19:\nend if\n20:\nfor each edge e ∈E do\n21:\ntry e.score := Score(e) except e.score := −1\n22:\nend for\n23:\nif ∀e ∈E, e.score = −1 then\n24:\nE := UniformSample(E, min(k, |E|))\n25:\nelse\n26:\nE := FilterTopKByScore(E, k)\n27:\nend if\n28:\nV d := LLM SelectNodes(q, u, E, n, level, C)\n29:\nfor each (u, r, v) ∈E where v ∈V d do\n30:\nΠ[v] := Π[u] ∥(u, r, v)\n31:\nF′ := F′ ∪{v}\n32:\nend for\n33:\nV := V ∪V d\n34:\nC[level] := LLM DescribePaths(q, r0, Π)\n35:\ndecision := LLM ShouldContinue(q, r0, Π)\n36:\nif decision = no then break\n37:\nend if\n38:\nF := F′\n39:\nd∗:= level\n40: end for\n41: for l = 1 to d∗do\n42:\nΣ[l] := LLM Summarize(Π of depth l)\n43: end for\n44: return (Π, Σ)\n"}, {"page": 27, "text": "Figure 6: Multi-step Performance Radar Chart\n"}, {"page": 28, "text": "Figure 7: Query Pattern vs. Retrieval Performance\n"}]}