{"doc_id": "arxiv:2512.06734", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.06734.pdf", "meta": {"doc_id": "arxiv:2512.06734", "source": "arxiv", "arxiv_id": "2512.06734", "title": "A Patient-Doctor-NLP-System to contest inequality for less privileged", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "published": "2025-12-07T08:59:15Z", "updated": "2025-12-07T08:59:15Z", "summary": "Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.06734v1", "url_pdf": "https://arxiv.org/pdf/2512.06734.pdf", "meta_path": "data/raw/arxiv/meta/2512.06734.json", "sha256": "dacfa97e4f19776a55ab0ac576a85070ecb4d53b41e483b27ff8a9a8045acaa7", "status": "ok", "fetched_at": "2026-02-18T02:25:04.053255+00:00"}, "pages": [{"page": 1, "text": "A Patient-Doctor-NLP-System to contest inequality for \nless privileged \nSUBRIT DIKSHIT1*, RITU TIWARI1, PRIYANK JAIN1 \n1 Department of Computer Science and Engineering, Indian Institute of Information \nTechnology, Pune, India               \n               * Corresponding Author \nE-mail: subrit@gmail.com1*, ritu@iiitp.ac.in1, \npriyank@iiitp.ac.in1 \nAbstract. Transfer Learning (TL) has accelerated the rapid development and \navailability of large language models (LLMs) for mainstream natural language \nprocessing (NLP) use cases. However, training and deploying such gigantic \nLLMs in resourceâ€‘constrained, realâ€‘world healthcare situations remains \nchallenging. This study addresses the limited support available to visually \nimpaired users and speakers of lowâ€‘resource languages such as Hindi who require \nmedical assistance in rural environments. We propose PDFTEMRA (Performant \nDistilled Frequency Transformer Ensemble Model with Random Activations), a \ncompact transformerâ€‘based architecture that integrates model distillation, \nfrequencyâ€‘domain modulation, ensemble learning, and randomized activation \npatterns to reduce computational cost while preserving language understanding \nperformance. The model is trained and evaluated on medical questionâ€‘answering \nand consultation datasets tailored to Hindi and accessibility scenarios, and its \nperformance is compared against standard NLP state-of-the-art model baselines. \nResults demonstrate that PDFTEMRA achieves comparable performance with \nsubstantially lower computational requirements, indicating its suitability for \naccessible, inclusive, lowâ€‘resource medical NLP applications. \n \nKeywords: Artificial Intelligence, Natural Language Processing, Accessible \nAI, Human-centered AI, NLP. \n1 \nIntroduction \nThis is the era of LLMs, such as BERT [1], BART [2], T5 [3], GPT2 [4], Bloom [5], \nLlama [6] and many more which are readily and easily accessible to everyone. Many of \nthese LLMs underlying use attention-based-transformers [8]. Transformers, have \nachieved great accomplishment in realizing AI and NLP based tasks. GPT2, a \ntransformer decoder-based-network turned out as few of highest cited research. \nAcademicians and researchers have strained to overcome restrictions of transformers \nby introducing finer improved networks. Majority work exploited similar time-domain \nimprovements and minimum use of forward-thinking scientific notions, such as, \n"}, {"page": 2, "text": "2 \nModulation [9], Frequency Domain Analysis [10], Fourier Transform [11] as in FNET \n[12] or Hartely Transform [13] to further heighten the network performance. Some \nresearchers tried concepts like distillation as anchored in DistilBERT [14] to improve \nnetwork efficiency utilizing Teacher-Student network to get similar performances of \nlarge models with smaller and smarter architectures. WISHPER [15], a multilingual \nspeech-to-text network predicts text transcripts based on 0.68 million hours of training \nand generalizes nicely on benchmarks. MMS [16], a transformer-based-speech-model \nscales to 1000+ languages and is built on wav2vec 2.0 base. M2M-VITS [17], with \nM2M100 baseline, is a multilingual transformer-based-network utilized in multilingual \ntransformations that can translate in 100 languages. However, these models are \nhumongous and cannot be trained from scratch owing to accompanying costs. Training \nthese models takes from many days to months on multiple GPUs together. However, to \nreduce this training time from scratch, techniques like, fine-tuning, quantization, \ncompression and adaptation are utilized. LORA [18], known as Low-Rank Adaptation \nis lightweight training procedure to lessen count of trainable parameters by introducing \na minor count of novel weights into the network and are trained to produce smaller \nmodel. Use of AdaNorm [19], in PDFTEMRA, an outperforming layer normalizing \ntechnique results faster and memory-efficient model architecture. \nActivations are useful in neural networks resulting smarter inferences. TanH, as \nutilized in LSTM [20] is chosen above sigmoid for superior non-linearity but suffers \nwith vanishing gradients that is better dealt by ReLU [21]. ReLUs, known as Rectified \nLinear Units are linear on non-negative measurements and 0 for negatives resulting \nimproved non-linear knowledge retention. ELU [22], known as Exponential Linear Unit \n(ELU) induces exponential activation learning that has negatives near to 0â€™s \nconsequential inferior computational complications. PreLU [23] activations have \nnegative values as well but they ensure lesser clatter deactivations. LeakyReLU [24], \nknown as Leaky Rectified Linear Unit is framed on ReLU adding shorter gradients for \nnon-positive values rather than flat gradients. SELU [25], known as Scaled Exponential \nLinear Unit encourage self-normalization. CELU [26] is good for raising deeper \nnetworks by quality of non-vanishing-gradients. MISH [27], is self-regularized non-\nmonotonic outperforms LeakyReLU on YOLOv4 [28] and ReLU on ResNet-50 [29]. \nSILU [30], known as Sigmoid Linear Unit is calculated with sigmoid increased with \ninputs. GELU [31], known as Gaussian Error Linear Unit is standard and smoother than \nReLU and utilized in BERT, GPT3 [32] and many other known transformers. \nEvaluation Metrics are used to validate our network performance. WER [33], \nknown as Word error rate metric validates speech recognition/translation \nperformance and ranges between 0 to 1, where 0 indicates exactly identical and 1 \nabsolutely dissimilar outcome. BLEU [34], known as Bilingual evaluation evaluates \nmachine-translated transcripts and claims higher relationship with human based \ndecisions. BLEU score ranges between 0 to 1, where values nearing 1 represent \nresemblances and 0 non-resemblances. \n"}, {"page": 3, "text": "3 \n1.1 \nHighlights  \nPurpose: The drive of this study is to develop and validate PDFTEMRA \n(Performant Distilled Frequency Transformer Ensemble Model with Random \nActivations), a compact transformer-based network engineered to offer accessible \nmedical natural language processing capabilities for visually impaired users and \nspeakers of low-resource languages like Hindi in resource-constrained rural \nhealthcare settings, while preserving performance analogous to state-of-the-art \nmodels but with significantly reduced computational necessities. \n \nContributions: PDFTEMRA solution contributions are discussed below: \nâ€¢ PDFTEMRA Implementation â€“ PDFTEMRA (Performant Distilled Frequency \nTransformer Ensemble Model with Random Activations) demonstrates successful \nuse of Transfer Learning (TL) on the custom medical dataset, PADT (PDFTEMRA \nDataset). \nâ€¢ Instrumenting with Model Distillation (MD) â€“ Model Distillation (MD) technique \nis grounded on Knowledge Distillation (KD) that replaces, Teacher with Distributor \n(D) and Student with Consumer (C) model. There could be multiple (Ds) based on \nthe needs and required tasks and (C) will mimic (D) based on task-in-hand. \nImplementation selects GPT2 as (D), to teach, PDFTEMRA as (C) model which \nachieve excellent results.  \nâ€¢ Validate use of novel Layer Transformation Technique â€“ This work, \ndemonstrates successful usage of Frequency Modulation techniques instead of \nAttention, via, Hartley transform to realize cheaper and faster inference. \nâ€¢ Establish use of novel Efficient Normalization Technique â€“ The network utilizes \nAdaNorm instead of the casual layer normalization that enhances our network \nperformance even further. \nâ€¢ Demonstrate use of novel Activation Ensemble (AE) â€“ Practice of Activation \nEnsemble (AE), built with the multiple smart activations act more efficiently and \nestablish as very performant layer using ELU, LeakyReLU, PReLU, ReLU, SELU, \nCELU, Mish, Tanh, SiLU, and GELU. \nâ€¢ Demonstrate use of novel Network Ensemble (NE) for Implementation of \nPatient-Doctor-NLP-System â€“ Network Ensemble (NE), utilizes multiple models \nby sandwiching together. This work creates WISPHER_PDFTEMRA_MMS_VITS \nensemble that is the combination of PRE-PROCESSOR_PROCESSOR_POST-\nPROCESSOR ensemble architecture, where, WISPHER is the PRE-PROCESSOR, \nPDFTEMRA is the PROCESSOR, MMS and VITS are the POST-PROCESSORs. \nâ€¢ PADT (PDFTEMRA Dataset) â€“ A custom high quality medical dataset, PADT \n(PDFTEMRA Dataset) is created for training and fine-tuning our PDFTEMRA \nnetwork to demonstrate successful implementation of Patient -Doctor-NLP-System. \n \nChallenges: Next challenges towards implementation of Patient-Doctor-NLP-System \nfor Hindi speaking least privileged or visually impaired audience are next stated below: \nâ€¢ Motivational Encounter â€“ Race towards capitalism, many are eager to contribute \ntowards expanses with huge financial gains. However, this work ideates to solve an \n"}, {"page": 4, "text": "4 \nissue that paralyzes a segment of our society and has huge impact. The \nimplementation goal is to help the less privileged and reduce social inequality by \nassisting blind or unschooled Hindi speaking patients seeking medicine support. \nDuring this research, it was understood that, less privileged are more often \nconcentrated towards rural areas and doctors in urban, probably, with reason to have \ngood lifestyle and ease of living in bigger cities. This gap is expanding day-by-day \nand hence the work tries to shrink this discontinuity. \nâ€¢ Software and Hardware Limitation â€“ The research had access to open-source \nsoftware resources and limited hardware as we lacked funding support. The open-\nsource models used were run on RTX 3070 8GB GPU, Intel 4.7 GHz 20 Core CPU \nand ram with 32 GB size. \nâ€¢ Dataset Availability & Quality â€“ The PADT dataset is baselined on freely available \ndatasets and web-scraped-data. Thus, the work suits academic inferences and not \nfully compatible with commercial product implementation. As there is a huge \ncommercial market, wherein, the work can be implemented and while suggest to the \nreaders is to strengthen the datasets with robust doctor-based-expert-opinions for \ncommercial product or solution implementations.  \nâ€¢ Model Bias â€“ The work utilizes open-source networks, for reference, Distributor \n(D), Pre-Processor and Post-Processor. Reader should know that these models may \nhave induced biases which might get passed-on to Consumer (C) model. Therefore, \nfor any commercial product or commercial solution implementation based on the \nresearch work, model biases should be handled effectively. \n \nFindings: The work demonstrates that PDFTEMRA achieves performance comparable \nto standard state-of-the-art NLP networks on medical question-answering and \nconsultation PADT dataset tailored to Hindi and accessibility scenarios, while requiring \nsubstantially lower computational resources. This indicates that the integration of \nmodel distillation, frequency-domain modulation, ensemble learning, and randomized \nactivation patterns successfully reduces computational cost without compromising \nlanguage understanding performance, making the model suitable for deployment in \nresource-constrained rural healthcare environments where visually impaired users and \nspeakers of low-resource languages require medical assistance. \nThe entire paper is divided into VI sections. Section II reflects the associated \nresearch. Section III talks about the methodology and approach behind the network. \nSection IV will be discussing the results of the experiment. We deliver our inferences \nin Section V and references are embedded in Section VI.  \n2 \nRelated Works \nThe reviewed literatures and related works are discussed in this section. However, \nthis research work is unique as the most of other the researchers concentrated on the \nuse of computer vision rather than NLP techniques while trying to contest analogous \nproblems while offering the enablement for the less privileged. \n"}, {"page": 5, "text": "5 \nKhan et al. (2020) [35] derived that blindness might cause individual from \nknowledge acquisition about environment and offer a Raspberry Pi-3 prototype for \nobject recognition and obstacle avoidance, using AI model, enabling effortlessness \nnavigation for impaired visually.  \nSmith et al. (2021) [36] presented work that expresses on experiences where AI do \nassist and frustrates in day-to-day tasks. Yet, in totality, author states that AI based \nexperiences are optimistic.  \nSharma et al. (2020) [37] presented sixth sense cellphone app and website. \nCellphone app for object detection and website for text-to-speech recognition or \ntranslation. However, shortfall for the experiment was dependency on camera picture \nquality.  \nKhan et al. (2022) [38] presented a review-based results that contribute on AI chat-\nbased health agents, knowledge gaps and issues.  \nLeerang et al. (2024) [39] also presented a review that concentrated on user \nresearch, and experience and component evaluation. Results depict four strategies. \nOne, personalized user interface and user experience (UI/ UX). Two, important \nfunctionality to back communications. Three, behavioral induction system and four, \nvigorous functional scheme guaranteeing care and conviction.  \nQureshi et al. (2021) [40] presented their paper which focused on real time AI \nimage recognition and obstacle detection to support visually disabled for self-\nsufficiently location steering while the proposed system was not completed.  \nMuqbali et al. (2020) [41] presented the project based on Open CV and Python \nwhose objective was to support blind in daily activities via smart device. Vision AI \nbased output device notice faces, colors and obstacles outputting vibration or audio \nalert to support visually impaired. \n3 \nMethodology \nHerein below, the stepwise overview of proposed work methodology is offered \nalong with the vital philosophies and utilized guiding forces which are next defined \nsubsequently, thereafter. \nâ€¢ \nDefine the problem, solution and architecture for the network. \nâ€¢ \nPrepare and process the Dataset. \nâ€¢ \nTrain the network Processor.  \nâ€¢ \nFinetune the Pre-Processor and the Post-Processor. \nâ€¢ \nOptimize, verify and validate overall network and the performance. \nâ€¢ \nExecute and generate the inferences for this work. \n \nOverall Implementation: The bird-eye view of the comprehensive implementation \nis revealed below in the Figure 1. \n \n"}, {"page": 6, "text": "6 \n \nFig. 1. Overall Implementation with Inference. \n \nOur patient, less privileged one (i.e. either cannot read/write or blind and speaks \nHindi/English) interacts with Application via voice prompts (speaks native language \nHindi/English). The local webserver-based Website/Android/iOS Apps are used as \nthe interaction layer. The ensembled PDFTEMRA network consists of the [PRE-\nPROCESSOR + PROCESSOR + POST-PROCESSOR]. The PRE-PROCESSOR, \nprocesses voice prompt inputs in the Native (i.e. Hindi) or English language and feeds \nto the PROCESSOR. The PROCESSOR, digests this input and generates the \nmedication predictions. The POST-PROCESSOR, feeds to PROCESSORâ€™s yield \nand generates plays the medication speech-based-output back in Native (i.e. Hindi), \ndisplays medication on screen in text (i.e. English) and Braille language. This makes \nnetwork output more usable that could be printed or e-mailed to an expert doctor for \nsecond opinion. Next, let us discuss utilized vital philosophies and guiding forces. \n  \nAttention, Fourier and Hartley Transform: Modulation, an impression that eases \ninformation quality missingness issue and is achieved via adding extra participation \nin fully connected layer (FLL). Transfer function of FLL and Modulated FLL \n(MFLL) of ğ¿ğ‘–ğ‘› input to layer is given by: \n \nğ¿ğ‘œ= ğ‘“(ğ‘¤ . ğ¿ğ‘–ğ‘›+ ğ‘), ğ‘¤ğ‘š= ğ‘˜ğ‘Š(ğœƒ),   ğ‘ğ‘š= ğ‘˜ğ‘(ğœƒ)  \n(1)  \n \nWhere, ğœƒ is the modulating signal, f is non-linearity function, g is the function of \nperceptron with multiple layers, ğ‘¤ and b weight and bias. Thus ğ¿ğ‘œ is given by: \n \nğ¿ğ‘œ= ğ‘“(ğ¿ğ‘–ğ‘› . ğ‘˜ğ‘Š(ğœƒ))  \n \n \n \n \n(2) \n \nFourier transform (FT) and Hartley transform (HT) are employed to the layers that \nreplace attention layer in our network. The Distributor (D) model, follow standard \nattention (A) scores and is equated by: \n \nA (Q, K, V) = S (\nQğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜)V \n \n \n \n \n(3) \n \n"}, {"page": 7, "text": "7 \nWhere query Q, key K, value V, and S softmax are the representations. For input \nstream ğ¼ğ‘—, where 0â‰¤ j, ğ‘˜â‰¤ğ‘âˆ’1 and N is length, discrete Fourier representation \nsummation of ğ¼ğ‘› is given by: \n \nğ¼ğ‘¡= âˆ‘\nğ¼ğ‘› (ğ‘’ğ‘¥ğ‘\nâˆ’(2ğœ‹ğ‘–ğ‘¡ğ‘›)\nğ‘\n)\nğ‘âˆ’1\nğ‘—=0\n \n \n \n \n(4) \n \nHartley transform gives similar performance to FT and that transmutes real input to \nyield is given by: \n \nğ»ğ‘‡= ğ‘…[ğ¹ğ‘‡] âˆ’ğ½[ğ¹ğ‘‡]   \n \n \n \n(5) \n \n \nMultiple Activation Ensemble: In an ensemble, each neuron activation is given by: \n \n \n \nğ‘‚ğ‘˜= ğ‘€(ğ‘ğ‘ ) \n \n \n \n(6) \nWhere, ğ‘ being the original activation function while additional activations allows \nthe model to learn and adapt better. \n \nAdaNorm: AdaNorm (ğ´ğ‘›) for v input vector it is given by: \n \n \n \nğ´ğ‘›= ğ¾ (1 âˆ’ğ‘™ğ‘¡) Â° ğ‘¡,   ğ‘¡= ğ´âˆ’ ğœ‡ğœ\nâ„ ,   ğœ‡= 1 ğ¶\nâ„\n(âˆ‘\nğ‘ğ‘–\nğ¶\nğ‘–=1\n),  \n \nğœ= âˆš1 ğ¶ (âˆ‘\n(ğ‘ğ‘–âˆ’ ğœ‡)2\nğ¶\nğ‘–=1\n)\nâ„\n \n \n \n \n(7) \nWhere, K is hyper-parameter, Â° dot product, k is 0.01, ğ‘¡ = (ğ‘¡1, â€¦ , ğ‘¡ğ¶), A the input \nsequence, C is dimension of t, ğ‘ğ‘– is ith element, ğœ‡ and ğœ are mean and variance. \n \nAdaptation:  Adaptation substitutes fine tuning via additional adapter layers to lower \nprojection of the original higher dimensional features, while adds non-linearity and \nthen upscales back to original projection. This is achieved by introducing ğ›¿ğ‘– work \ntype to pre trained network parameters ğ›¿, where (1 â‰¤ i â‰¤ M) for M tasks. Adaptor, \nlimits number of available inputs to transfer representations effectively from older to \nnew ones and thus easing damage of information. This fused objective is given by: \n \nğ‘€ğ‘–ğ‘›ğ½ğ‘–(ğ¹ğ·)âˆ’1 âˆ‘\n(âˆ’ğ‘(ğ½â€²ğ‘ğ‘–, ğ›½â€²) +  ğ‘(ğ½, ğ‘ğ‘–, ğ›½))2\nğ·\nğ‘˜=1\n  \n(8) \n \nWhere, ğ½ğ‘–is combined network parameters, ğ¹ğ·is unlabeled drill information, loss \nof distillation is logit computer change by current model ğ‘(ğ½, ğ‘ğ‘–, ğ›½) versus \nconsolidated model ğ‘(ğ½â€²ğ‘ğ‘–, ğ›½â€²) and once ğ¹ğ· is skilled ğ›½is substituted by ğ›½â€². \n \nTransfer Learning: Transfer learning is practiced using OPENAI/WHISPER-\nLARGE-V3 as the PRE-PROCESSOR, FACEBOOK/M2M100_418M and \n"}, {"page": 8, "text": "8 \nFACEBOOK/MMS-TTS as the POST-PROCESSORs. These efficient models are \nselected as being open-source, and allowing easy fine-tunning that suits this study. \nOn the other hand, creating a similar fresh model is not beneficial, as it is not \nfinancially feasible with limited software and hardware at disposal. Re-doing \nWHISPER-LARGE-V3 with 1.55 billion parameters will cost around $218700 \nmillion/year; while M2M100_418M costing $491000 million/year, MMS-TTS along \nwith 1 billion parameters would be costing $153,090 million/year approximately. \n \nModel Distillation: The practice of Model Distillation (MD) via Distributor (D) and \nConsumer (C) networks for the defined work is shown in the Figure 2 below.  \n \nFig. 2. Model Distillation (MD) along with Distributor (D) & Consumer (C). \nOPENAI-GPT2 acts as the choice of Distributor (D) and the PDFTEMRA as (C). \nPDFTEMRA ability to grasp the knowledge is founded on how virtuous is (D) and \n(C)â€™s self-learning capabilities. Given ğ·ğ‘– and ğºğ‘– are probabilities for ğ‘–ğ‘¡â„ element, the \ntraining distillation loss is given by: \nğ¿ğ·= âˆ‘ğºğ‘–âˆ—log ğ·ğ‘–\nğ‘–\n  \n \n \n \n(9) \nPDFTEMRA takes input and adds segment and position embedding to it. This is passed \non to HT layer with different activation ensemble for each head. Output is summed \n \n \n  \n                   \n        \n                   \n         \n                \n         \n    \n               \n               \n     \n     \n"}, {"page": 9, "text": "9 \npassed to AdaNorm layers for input normalization along with dropout. This is followed \nup by FeedForward and AdaNorm layer again. Finally, output is retrieved via Linear \nlayer followed by SoftMax. Detailed architecture is shown in Figure 3 along with visual \ncontrast from original transformer setup. \n \nFig. 3. Original Transformer Vs PDFTEMRA architecture. \n4 \nExperiment & Results \nThis section discusses the setup, experiment and obtained inferences. \n \nDatasets: CFILT/IITB-ENGLISH-HINDI dataset for English-Hindi translation \nhelps fine-tuning of the network. The IIT Bombay English-Hindi corpus consists of \nEnglish-Hindi text pairs. MOZILLA-FOUNDATION/COMMON_VOICE_11_0 \n"}, {"page": 10, "text": "10 \ndataset composed of acoustic mp3 and matching text pairs (Train: 24210 + \nValidation: 16413) hours is used for speech training and tunning. \nTraining of the PDFTEMRA model on PADT dataset entails to English-Hindi \nlanguage pairs (Train: 122000 + Validation: 2490 + Test: 5010) samples. PADT \ndataset is grounded using the KAGGLE/DATASETS/MOHNEESH7/INDIAN-\nMEDICINE-DATA of size 354.1 MB consisting of the Indian medicines along with \nKAGGLE/DATASETS/SINGHNAVJOT2062001/11000-MEDICINE-DETAILS \ndataset that consists 11000 Indian medicines manually web scraping records from \n1MG website.  \n \nPADT dataset can be requested from the author and available at: \n \nDataset - PADT: \nhttps://drive.google.com/drive/folders/1aUeGJ29DvQ98RapGGoSrQCabaYFbMxuP?usp=drive_link  \n \nTraining: Model Distillation MD of the PDFTEMRA network is achieved using \nPADT dataset for 25 epochs, learning rate at 0.001, temperature as 2, maximum \nsequence length at 512, number of head as 10, latent dim of 100 and embedded dim \nas 256.   \n \nThe source code for PDFTEMRA can be requested from the author and is available \nat: \n \nEntire Source Code - PDFTEMRA: \nhttps://drive.google.com/drive/folders/1sKOB0hxs-PUvReQegH4DmvZy-5OVO5IX?usp=drive_link \n \nTable 1 and Figure 4, displays Distributor (D) and Consumer (C) networks training \nand performance on the PADT dataset. Based on these results, during Model \nDistillation (MD) process, The network (C) takes more epochs to gain knowledge in \ncomparision to plain vanilla (D) and (C) trainings. It is worth noting, that, the \nnetworks start to stablize around epoch 10, but, while testing vibrations are we \nobserbed in the distillation loss and accuracy making training the models until epoch \n20 to 22 for better inferences. \n \nTable 1: Model Distillation MD training and performance results for networks \n \nEpochs \nD-TL \nD-VL \nDP-TL \nDP-TDL \nDP-VL \nDP-VDL \nC-TL \nC-VL \nD-TA \nD-VA \nDP-TA \nDP-VA \nC-TA \nC-VA \n1 \n0.3666 \n0.3473 \n0.2113 \n0.0185 \n0.2192 \n0.0067 \n0.3704 \n0.3011 \n0.8336 \n0.8591 \n0.9315 \n0.9256 \n0.8303 \n0.8829 \n2 \n0.1566 \n0.1208 \n0.1045 \n0.0096 \n0.0919 \n0.009 \n0.1829 \n0.1149 \n0.9405 \n0.9562 \n0.9894 \n0.9894 \n0.9321 \n0.9585 \n3 \n0.066 \n0.0231 \n0.0793 \n0.0096 \n0.0712 \n0.0093 \n0.1072 \n0.0342 \n0.9764 \n0.9936 \n0.9992 \n0.9977 \n0.9616 \n0.9895 \n4 \n0.029 \n0.0021 \n0.0702 \n0.0095 \n0.0665 \n0.0093 \n0.068 \n0.0076 \n0.9902 \n0.9998 \n1 \n0.9993 \n0.9761 \n0.9986 \n"}, {"page": 11, "text": "11 \n5 \n1.67E-02 \n2.74E-04 \n6.76E-02 \n0.0095 \n6.54E-02 \n0.0093 \n0.0498 \n0.0021 \n0.9942 \n1 \n1.00E+00 \n1.00E+00 \n0.9832 \n0.9996 \n6 \n1.47E-02 \n8.65E-05 \n6.76E-02 \n0.0094 \n6.59E-02 \n0.0093 \n0.034 \n4.46E-04 \n0.9946 \n1 \n1.00E+00 \n9.99E-01 \n0.989 \n1 \n7 \n2.20E-02 \n4.18E-05 \n6.87E-02 \n0.0095 \n6.66E-02 \n0.0093 \n0.0284 \n1.09E-04 \n0.9926 \n1 \n1.00E+00 \n9.99E-01 \n0.9906 \n1 \n8 \n9.50E-03 \n2.29E-05 \n6.93E-02 \n0.0095 \n6.96E-02 \n0.0093 \n0.0278 \n5.15E-05 \n0.9967 \n1 \n1.00E+00 \n9.98E-01 \n0.9909 \n1 \n9 \n1.23E-02 \n1.42E-05 \n6.92E-02 \n0.0095 \n6.82E-02 \n0.0093 \n0.0193 \n2.75E-05 \n0.996 \n1 \n1.00E+00 \n9.99E-01 \n0.9938 \n1 \n10 \n1.32E-02 \n9.13E-06 \n6.81E-02 \n0.0094 \n6.62E-02 \n0.0093 \n0.0145 \n1.42E-05 \n0.9952 \n1 \n1.00E+00 \n9.99E-01 \n0.9957 \n1 \n11 \n8.60E-03 \n6.02E-06 \n6.77E-02 \n0.0094 \n6.74E-02 \n0.0093 \n0.0108 \n8.60E-06 \n0.9972 \n1 \n1.00E+00 \n9.99E-01 \n0.9967 \n1 \n12 \n4.20E-03 \n3.83E-06 \n6.75E-02 \n0.0094 \n6.54E-02 \n0.0093 \n0.0093 \n5.38E-06 \n0.9987 \n1 \n1.00E+00 \n9.99E-01 \n0.9968 \n1 \n13 \n8.80E-03 \n2.35E-06 \n6.66E-02 \n0.0094 \n6.57E-02 \n0.0093 \n0.01 \n3.41E-06 \n0.9969 \n1 \n1.00E+00 \n9.99E-01 \n0.997 \n1 \n14 \n1.45E-02 \n1.41E-06 \n6.71E-02 \n0.0094 \n6.56E-02 \n0.0093 \n0.0123 \n2.19E-06 \n0.9947 \n1 \n9.99E-01 \n9.99E-01 \n0.9961 \n1 \n15 \n7.10E-03 \n9.45E-07 \n6.57E-02 \n0.0094 \n6.42E-02 \n0.0093 \n0.004 \n1.38E-06 \n0.9976 \n1 \n1.00E+00 \n1.00E+00 \n0.9987 \n1 \n16 \n2.50E-03 \n6.35E-07 \n6.63E-02 \n0.0093 \n6.44E-02 \n0.0093 \n0.0123 \n0.002 \n0.999 \n1 \n1.00E+00 \n1.00E+00 \n0.9959 \n0.9994 \n17 \n4.00E-03 \n4.09E-07 \n6.57E-02 \n0.0093 \n6.59E-02 \n0.0093 \n0.0047 \n0.0238 \n0.9989 \n1 \n1.00E+00 \n9.99E-01 \n0.9988 \n0.9923 \n18 \n7.70E-03 \n2.83E-07 \n6.48E-02 \n0.0093 \n6.53E-02 \n0.0093 \n0.0039 \n0.0013 \n0.9972 \n1 \n1.00E+00 \n9.99E-01 \n0.999 \n0.9995 \n19 \n5.20E-03 \n1.77E-07 \n6.60E-02 \n0.0093 \n6.49E-02 \n0.0093 \n0.0061 \n9.58E-05 \n0.9982 \n1 \n9.99E-01 \n1.00E+00 \n0.9983 \n1 \n20 \n3.80E-03 \n1.11E-07 \n6.61E-02 \n0.0093 \n6.57E-02 \n0.0093 \n0.0058 \n3.09E-05 \n0.9985 \n1 \n9.99E-01 \n9.99E-01 \n0.9983 \n1 \n21 \n4.20E-03 \n7.58E-08 \n6.56E-02 \n0.0093 \n6.61E-02 \n0.0093 \n0.0032 \n1.90E-05 \n0.9983 \n1 \n9.99E-01 \n9.99E-01 \n0.999 \n1 \n22 \n4.00E-03 \n4.77E-08 \n6.50E-02 \n0.0093 \n6.58E-02 \n0.0093 \n7.02E-04 \n1.25E-05 \n0.9986 \n1 \n1.00E+00 \n9.99E-01 \n0.9998 \n1 \n23 \n1.60E-03 \n3.42E-08 \n6.57E-02 \n0.0093 \n6.40E-02 \n0.0093 \n0.0051 \n8.27E-06 \n0.9994 \n1 \n9.99E-01 \n1.00E+00 \n0.9982 \n1 \n24 \n5.80E-03 \n2.32E-08 \n6.46E-02 \n0.0093 \n6.45E-02 \n0.0093 \n0.0057 \n5.49E-06 \n0.9982 \n1 \n1.00E+00 \n1.00E+00 \n0.9982 \n1 \n25 \n8.30E-03 \n1.72E-08 \n6.41E-02 \n0.0093 \n6.42E-02 \n0.0093 \n0.0047 \n3.66E-06 \n0.9969 \n1 \n1.00E+00 \n1.00E+00 \n0.9984 \n1 \n \nLegends - Table 1:  \nD-TL as Distributor-Training Loss, D-VL as Distributor-Validation Loss, DP-TL as Distillation Process-\nTraining Loss, DP-TDL as Distillation Process-Training Distillation Loss, DP-VL as Distillation Process-\nValidation Loss, DP-VDL as Distillation Process-Validation Distillation Loss, C-TL as Consumer-\nTraining Loss, C-VL as Consumer-Validation Loss, D-TA as Distributor-Training Accuracy, D-VA as \nDistributor-Validation Accuracy, DP-TA as Distillation Process-Training Accuracy, DP-VA as \nDistillation Process-Validation Accuracy, C-TA as Consumer-Training Accuracy, C-VA as Consumer-\nValidation Accuracy. \n \n \n \n"}, {"page": 12, "text": "12 \n \n \n \n \nFig. 4. Model Distillation MD training and performance results for networks  \n \nFine-tuning: Fine tuning of the Whisper network is achieved using \nCOMMON_VOICE_11_0 dataset for 4000 maximum steps, 500 warmup steps, \nlearning rate at 1e-5, metric as WER, training batch size at 16, evaluation batch size \nat 8, datatype as floating point 16 and maximum length as 255. \n"}, {"page": 13, "text": "13 \nFine tuning of the M2M100 is achieved using IITB-ENGLISH-HINDI dataset for 20 \nepochs, decay of weight as 0.01, metric as BLEU, training batch size at 32, evaluation \nbatch size at 64, learning rate at 2e-5, datatype as floating point 16 and maximum \nlength as 128. \n \nFine tuning of the MMS is achieved using COMMON_VOICE_11_0 dataset for 10 \nepochs, 100 evaluation steps, learning rate at 1e-3, metric as WER, training batch \nsize at 32, datatype as floating point 16 and maximum length as 255. \n \nTable 2 and Figure 5, displays training for the Whisper, M2M100, and MMS-TTS \nand performance on respective dataset. Based on these results, it is worth noting that \nduring PREPROCESSOR training of Whisper it takes around 10000 steps to stabilize \nthe model. Whereas, in case of POSTPROCESSOR training of M2M100 and MMS-\nTTS, models stabilize around 10000 and 250 steps respectively.  \nTable 2. Training and performance results of PREPROCESSOR and POSTPROCESSOR. \nModel \nMetric \nSteps \n \nS-Multiplier \nT-Loss \nV-Loss \nScore \nW/M/T \nW/B/W \n100 \n \n100/100/1 \n0.2567/0.2765/4.905 \n0.3075/0.3273/2.3075 \n0.4463/0.5347/0.456 \nW/M/T \nW/B/W \n200 \n \n100/100/1 \n0.1967/0.2165/0.299 \n0.3558/0.3756/0.215 \n0.3313/0.5424/0.28 \nW/M/T \nW/B/W \n300 \n \n100/100/1 \n0.1125/0.1323/0.2659 \n0.3214/0.3412/0.167 \n0.3259/0.5871/0.232 \nW/M/T \nW/B/W \n400 \n \n100/100/1 \n0.0818/0.1016/0.2398 \n0.2519/0.2717/0.161 \n0.3201/0.6013/0.229 \nW/M/T \nW/B/W \n500 \n \n100/100/1 \n0.0312/0.051/0.127 \n0.1679/0.1877/0.156 \n0.321/0.6276/0.223 \nW/M/T \nW/B/W \n600 \n \n100/100/1 \n0.0108/0.0306/0.095 \n0.1455/0.1653/0.1455 \n0.285/0.6432/0.221 \nW/M/T \nW/B/W \n700 \n \n100/100/1 \n0.0051/0.0249/0.081 \n0.1251/0.1449/0.1251 \n0.2645/0.7338/0.224 \nW/M/T \nW/B/W \n800 \n \n100/100/1 \n0.0027/0.0225/0.0511 \n0.1995/0.2193/0.1995 \n0.2377/0.7674/0.217 \nW/M/T \nW/B/W \n900 \n \n100/100/1 \n0.0005/0.0203/0.027 \n0.1572/0.177/0.1572 \n0.2221/0.7824/0.203 \nW/M/T \nW/B/W \n1000 \n \n100/100/1 \n0.0002/0.0119/0.021 \n0.0573/0.0771/0.1573 \n0.2017/0.8915/0.197 \n \nLegends - Table 2:  \nModel as W for Whisper, M for M2M100 and T for MMS-TTS. Metric as W for WER and B for BLEU. \nT-Loss as Loss of Training. S-Multiplier is multiplied to Steps to get total steps for respective model. V-\nLoss as Loss of Validation. Score is model performance on the specified metric for the model. \n \n"}, {"page": 14, "text": "14 \n \n"}, {"page": 15, "text": "15 \n \nFig. 5.: Training and performance results of PREPROCESSOR and \nPOSTPROCESSOR \n \nFinal Results: Figure 6 displays the PDFTEMRA outputted inferences. As shown in \nFigure 6, when user interacts with application with input voice prompts of â€œSar Dardâ€ \n(â€œHeadache painâ€) in Hindi (/English) and the network yields respective English \nQuestion Translation, English and Braille reply along with Hindi Audio responses. \n \n \n \nFig. 6. Inference with PDFTEMRA. \n"}, {"page": 16, "text": "16 \n5 \nConclusions & Future Work \nThis work delivers an efficient and effective network, an artificial intelligence \nand natural language processing founded initiative, a Patient-Doctor-NLP-System \nfor least privileged, PDFTEMRA, and a custom medical dataset, PADT. Multiple \nmanifold advanced mathematical and engineering concepts helps to achieve \ncomparable state-of-the-art network performance. Reader should note that, while \nexecuting, the results could be little different due to probabilistic nature. Achieving \neven better results the modifications like, different selection of the distributor \nnetwork, hyper-parameter setups, better adaptation techniques, and overcoming \nexperimentation hardware limitation of 20 CPU, 8GB VRAM GPU with 32 GB \nRAM. While this study nominated Hindi-English spoken-languages and optimized \nnetwork and the hyper-parameter settings accordingly, the study and setup could \nbe utilized for multiple native spoken languages across the world with minimal \ntweaks or optimizations to achieve similar or improved inferences. \n6 \nDeclarations \n6.1 \nCompeting interests \nThe corresponding author affirms that there is no conflict of interest on behalf of all \nauthors. We (the authors) affirm that we do not have any opposing benefits to disclose \nthat could be financial or personal relationship with any third party that can influence \nthe article. \n7 \nReferences \n[1] \nJacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova 2018 BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language Understanding. arXiv preprint \narXiv:1810.04805 \n[2] \nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman \nMohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer 2020 BART: Denoising \nSequence-to-Sequence Pre-training for Natural Language Generation, Translation, and \nComprehension. 58th Annual Meeting of the Association for Computational Linguistics pp \n7871â€“7880 \n[3] \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael \nMatena, Yanqi Zhou, Wei Li Peter and Liu J 2020 Exploring the Limits of Transfer Learning \nwith a Unified Text-to-Text Transformer. The Journal of Machine Learning Research (21) \npp 5485â€“5551 \n[4] \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya \nSutskever 2019 Language Models are Unsupervised Multitask Learners \n[5] \nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M \nSaiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, \nColin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, \n"}, {"page": 17, "text": "17 \nJulien Launay and Iz Beltagy 2022 What Language Model to Train if You Have One Million \nGPU Hours. EMNLP 2022 arXiv preprint arXiv:2210.15424 \n[6] \nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al \nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan et al. 2024 \nThe Llama 3 Herd of Models arXiv preprint arXiv:2407.21783 \n[7] \nSubrit Dikshit, Rahul Dixit, Abhiram Shukla 2024 Review and analysis for state-of-\nthe-art NLP models. International Journal of Systems, Control and Communications. (15:1) \npp 48-78 \n[8] \nVaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob, Jones Llion, Gomez \nAidan N, Kaiser Lukasz and Polosukhin Illia 2017 Attention Is All You Need. arXiv preprint \narXiv:1706.03762 \n[9] \nMohamed Abdelhack, Jiaming Zhang, Sandhya Tripathi, Bradley A Fritz, Daniel \nFelsky, Michael S Avidan, Yixin Chen, Christopher R King 2023 A Modulation Layer to \nIncrease Neural Network Robustness Against Data Quality Issues. arXiv:2107.08574 \n[10] Michael Moher and Simon S Haykin M 2012 Introduction to Analog & Digital \nCommunications 2nd edition. USA John Wiley & Sons Inc. pp 19-90 \n[11] Bruce A Carlson, Paul B Crilly 2009 Communication Systems - An Introduction to \nSignals and Noise in Electrical Communication 5th edition. USA McGraw-Hill. pp 43-50 \n[12] James Lee Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon 2022 FNet: Mixing \nTokens with Fourier Transforms. arXiv:2105.03824 \n[13] Ralph V L Hartley 1942 Hartely Transform: A More Symmetrical Fourier Analysis \nApplied to Transmission Problems. Proceedings of the IRE. (30) pp 144-150 \n[14] Victor Sanh, Lysandre Debut, Julien Chaumond and Thomas Wolf 2019 DistilBERT, \na Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter. arXiv preprint \narXiv:1910.01108 \n[15] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya \nSutskever 2022 Robust Speech Recognition via Large-Scale Weak Supervision arXiv \narXiv:2212.04356 \n[16] Vineel Pratap, Ros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, \nAli Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi \nAdi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau and Michael Auli 2023 Scaling Speech \nTechnology to 1,000+ Languages. arXiv arXiv:2305.13516 \n[17] Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth \nGoyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman \nGoyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli and \nArmand Joulin 2020 Beyond English-Centric Multilingual Machine Translation. arXiv \narXiv:2010.11125 \n[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean \nWang, Lu Wang, Weizhu Chen 2021 LoRA: Low-Rank Adaptation of Large Language \nModels. arXiv arXiv:2106.09685 \n[19] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao and Junyang Lin 2019 \nAdaNorm: Understanding and Improving Layer Normalization. arXiv arXiv:1911.07013 \n[20] Hasim Sak, Andrew Senior and Francoise Beaufays 2014 Long Short-Term Memory \nRecurrent Neural Network Architectures for Large Scale Acoustic Modeling. Annual \nConference of the International Speech Communication Association, INTERSPEECH pp \n338-342 \n[21] Agarap, Abien Fred 2018 Deep Learning using Rectified Linear Units (ReLU). arXiv \narXiv.1803.08375 \n"}, {"page": 18, "text": "18 \n[22] Djork-ArnÃ© Clevert, Thomas Unterthiner, Sepp Hochreiter 2016 Fast and Accurate \nDeep Network Learning by Exponential Linear Units (ELUs). arXiv arXiv:1511.07289 \n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun 2015 Delving Deep into \nRectifiers: Surpassing Human-Level Performance on ImageNet Classification. arXiv \narXiv.1502.01852 \n[24] Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li 2015 Empirical Evaluation of Rectified \nActivations in Convolutional Network. arXiv arXiv.1505.00853 \n[25] GÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter 2017 Self-\nNormalizing Neural Networks. arXiv arXiv.1706.02515 \n[26] Jonathan T Barron 2017 Continuously Differentiable Exponential Linear Units. arXiv \narXiv.1704.07483 \n[27] Diganta Misra 2019 Mish: A Self Regularized Non-Monotonic Activation Function. \narXiv arXiv.1908.08681 \n[28] Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao 2020 YOLOv4: \nOptimal Speed and Accuracy of Object Detection. arXiv arXiv.2004.10934 \n[29] Koonce, Brett 2021 ResNet 50 Convolutional Neural Networks with Swift for \nTensorflow, Image Recognition and Dataset Categorization. pp 63-72 doi 978-1-4842-6168-\n2_6 \n[30] Stefan Elfwing, Eiji Uchibe, Kenji Doya 2017 Sigmoid-Weighted Linear Units for \nNeural \nNetwork \nFunction \nApproximation \nin \nReinforcement \nLearning. \narXiv \narXiv.1702.03118 \n[31] Dan Hendrycks, Kevin Gimpel 2016 Gaussian Error Linear Units (GELUs). arXiv \narXiv.1606.08415 \n[32] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplany, Prafulla \nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini \nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya \nRamesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, \nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, \nSam McCandlish, Alec Radford, Ilya Sutskever and Dario Amodei 2020 Language Models \nare Few-Shot Learners. arXiv arXiv:2005.14165 \n[33] Ahmed Ali and Steve Renals 2018 Word Error Rate Estimation for Speech \nRecognition: e-WER Proceedings of the 56th Annual Meeting of the Association for \nComputational Linguistics Melbourne, Australia. (2), pp 20â€“24 doi 10.18653/v1/P18-2004 \n[34] Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu 2002 Bleu: A Method for \nAutomatic Evaluation of Machine Translation In Proceedings of the 40th Annual Meeting \nof \nthe \nAssociation \nfor \nComputational \nLinguistics \nUSA. \npp \n311â€“318 \ndoi \n10.3115/1073083.1073135 \n[35] Ahmed Khan, Muiz and Paul, Pias and Rashid, Mahmudur and Hossain, Mainul and \nAhad, Md Atiqur Rahman 2020 An AI-Based Visual Aid with Integrated Reading Assistant \nfor the Completely Blind IEEE Transactions on Human-Machine Systems. (50) pp 507-517 \ndoi 10.1109/THMS.2020.3027534 \n[36] Peter Smith and Laura Smith 2021 Artificial Intelligence and disability: too much \npromise, yet too little substance? AI Ethics. (1) pp 81â€“86 doi 10.1007/s43681-020-00004-5 \n[37] Aditya Sharma, Aditya Vats, Shiv Shankar Dash, Surinder Kaur 2020 Artificial \nIntelligence enabled virtual sixth sense application for the disabled. (1) pp 32-39 doi \n10.5281/zenodo.3825929 \n[38] Syed Mahmudul Huq, Rytis Maskeliunas and Robertas DamaÅ¡eviÄius 2022 Dialogue \nagents for artificial intelligence-based conversational systems for cognitively disabled: a \nsystematic review. 19(7) pp 1-20 doi 10.1080/17483107.2022.2146768 \n"}, {"page": 19, "text": "19 \n[39] Leerang Jeong, and Yoori Koo 2024 The Experience of Using Artificial Intelligence \nfor the Disabled: Evaluation of Community Service Experience Factors and Proposal of \nStrategies for Self-reliance of the Developmentally Disabled. Archives of Design Research \n37(3) pp 167-195 10.15187/adr.2024.07.37.3.167 \n[40] Tufel Ali Qureshi, Mahima Rajbhar, Yukta Pisat, and Vijay Bhosale 2021 AI Based \nApp for Blind People. 8(3) pp 2883-2887 e-ISSN: 2395-0056 p-ISSN: 2395-0072 \n[41] Fatma Al Muqbali, Noura Al Tourshi, Khuloud Al Kiyumi, and Faizal Hajmohideen \n2020 Smart Technologies for Visually Impaired: Assisting and conquering infirmity of blind \npeople using AI Technologies. 12th Annual Undergraduate Research Conference on \nApplied Computing (URC). Dubai pp 1-4 doi 10.1109/URC49805.2020.9099184  \n"}]}