{"doc_id": "arxiv:2601.09151", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.09151.pdf", "meta": {"doc_id": "arxiv:2601.09151", "source": "arxiv", "arxiv_id": "2601.09151", "title": "Interpretable Probability Estimation with LLMs via Shapley Reconstruction", "authors": ["Yang Nan", "Qihao Wen", "Jiahao Wang", "Pengfei He", "Ravi Tandon", "Yong Ge", "Han Xu"], "published": "2026-01-14T04:45:36Z", "updated": "2026-01-14T04:45:36Z", "summary": "Large Language Models (LLMs) demonstrate potential to estimate the probability of uncertain events, by leveraging their extensive knowledge and reasoning capabilities. This ability can be applied to support intelligent decision-making across diverse fields, such as financial forecasting and preventive healthcare. However, directly prompting LLMs for probability estimation faces significant challenges: their outputs are often noisy, and the underlying predicting process is opaque. In this paper, we propose PRISM: Probability Reconstruction via Shapley Measures, a framework that brings transparency and precision to LLM-based probability estimation. PRISM decomposes an LLM's prediction by quantifying the marginal contribution of each input factor using Shapley values. These factor-level contributions are then aggregated to reconstruct a calibrated final estimate. In our experiments, we demonstrate PRISM improves predictive accuracy over direct prompting and other baselines, across multiple domains including finance, healthcare, and agriculture. Beyond performance, PRISM provides a transparent prediction pipeline: our case studies visualize how individual factors shape the final estimate, helping build trust in LLM-based decision support systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.09151v1", "url_pdf": "https://arxiv.org/pdf/2601.09151.pdf", "meta_path": "data/raw/arxiv/meta/2601.09151.json", "sha256": "3f06a5f65b0299fb2c3191d3a7bf77bbc8eef00d783ce6c2ee8ad268d50bd2e7", "status": "ok", "fetched_at": "2026-02-18T02:21:35.507497+00:00"}, "pages": [{"page": 1, "text": "Preprint\nINTERPRETABLE\nPROBABILITY\nESTIMATION\nWITH\nLLMS VIA SHAPLEY RECONSTRUCTION\nYang Nan, Qihao Wen, Jiahao Wang, Pengfei He, Ravi Tandon, Yong Ge, Han Xu.\n{yangnan, qihaowen, jiahaow, tandonr,yongge, xuhan2}@arizona.edu\nhepengf1@msu.edu\nABSTRACT\nLarge Language Models (LLMs) demonstrate potential to estimate the probability\nof uncertain events, by leveraging their extensive knowledge and reasoning capa-\nbilities. This ability can be applied to support intelligent decision-making across\ndiverse fields, such as financial forecasting and preventive healthcare. However,\ndirectly prompting LLMs for probability estimation faces significant challenges:\ntheir outputs are often noisy, and the underlying predicting process is opaque. In\nthis paper, we propose PRISM: Probability Reconstruction via Shapley Mea-\nsures, a framework that brings transparency and precision to LLM-based prob-\nability estimation. PRISM decomposes an LLM‚Äôs prediction by quantifying the\nmarginal contribution of each input factor using Shapley values. These factor-\nlevel contributions are then aggregated to reconstruct a calibrated final estimate.\nIn our experiments, we demonstrate PRISM improves predictive accuracy over\ndirect prompting and other baselines, across multiple domains including finance,\nhealthcare, and agriculture. Beyond performance, PRISM provides a transparent\nprediction pipeline: our case studies visualize how individual factors shape the\nfinal estimate, helping build trust in LLM-based decision support systems.\n1\nINTRODUCTION\nEstimating the probability of uncertain events (Berger, 2013; Winkler et al., 2019) is a critical task\nfor intelligent decision makings in various domains such as financial investment (Lathief et al.,\n2024), healthcare (Rajkomar et al., 2019), and emergency management (Rostami-Tabar and Hyn-\ndman, 2025). However, in many real-world scenarios, either high-quality datasets are unavailable\nor mature Machine Learning (ML) techniques are lacking. Besides, there could also appear tem-\nporarily unexpected factors which are not considered when people building datasets (Chowdhury\net al., 2021). As an example, in business analytics (Raghupathi and Raghupathi, 2021), people may\nencounter a variety of diverse estimation tasks, such as determining whether the price, production\nor market demand of a certain product will increase or not (Fildes et al., 2022). It could be difficult\nfor them to collect sufficient task-specific data and build reliable predictive models promptly. In this\nscenario, the applicability of traditional ML approaches are severely limited.\nAs a potential solution, Large Language Models (LLMs) offer a promising alternative to address this\nchallenge (Feng et al., 2024; Sui et al., 2024; Chung et al., 2024). They incorporate extensive world\nknowledge and exhibit strong reasoning capabilities (Wei et al., 2022), making them particularly\nvaluable in data or model scarce settings. However, directly prompting LLMs to probability estima-\ntion still faces tremendous challenges: (i) LLMs typically produce noisy probability estimates that\nlack accuracy and stability. As an instance, according to the study (Nafar et al., 2025), when LLMs\nare asked to predict the same event in different forms, i.e., ‚Äúwhether it will happen‚Äù and ‚Äúwhether it\nwill not happen‚Äù, LLMs could provide conflicting answers. (ii) The ‚Äúblack-box‚Äù generative process\nprovides little transparency regarding how each individual factor contributes to the final prediction,\nmaking the results difficult to interpret. As illustrated in Figure 1, when asked to predict the like-\nlihood of a person having a certain disease, LLMs will not explicitly show how much weight each\nfactor contributes to final prediction, but outputs a single score (with a partial explanation). It makes\nthe final outcome difficult to interpret and less trustworthy.\nTo overcome the challenges, we propose a novel framework Probability Reconstruction via Shap-\nley Measures (PRISM), inspired by Shapley Value for ML explanation (Lundberg and Lee, 2017).\narXiv:2601.09151v1  [cs.LG]  14 Jan 2026\n"}, {"page": 2, "text": "Preprint\nFormerly\nSmoked\nImpact \n= 0.65\nFemale: \nGender\nImpact \n= ‚àí0.23 \nNo heart \ndisease\nImpact \n= ‚àí0.41 \nFinalProb = œÉ base_logit + 2 œï!\n!\n=  0.61\nOur method: PRISM\nBMI: 22.3\nImpact \n= ‚àí0.15 \nHypertension: \nYes\nImpact = 1.25\nLet S be a subset of features of this person except age,\nœï! = ùîº\" f S, Age = 79 ‚àíf S\nAge: 79\nImpact = 1.24\nDirect LLM Prompting\nContext:\nIndividual information: \n- Age: 79\n- Gender: Female\n...‚Ä¶\nQuestion: Estimate the probability that \nthe person will have stroke.\nLLM Response:\n79-year-old female with hypertension, a \nmajor stroke risk factor. Absence of heart \ndisease lowers risk, BMI and glucose are \nnormal, and former smoking adds some \nrisk. Overall, age and hypertension \ndominate, indicating a moderate risk. The \nprobability score is 0.5.\nFigure 1: Illustration of direct LLM prompting and PRISM. PRISM first estimates Shapley values\n(factor contributions) and aggregates them to reconstruct final probability. We use red to represent\nthe factors found by PRISM to have positive contribution to the positive outcome (have a stroke),\ngreen are factors found to be negative. The size reflects the contribution‚Äôs absolute value. f(¬∑) is from\nLLM prediction and S is a background set, œÉ(¬∑) is the sigmoid function (see details in Section 3).\nSpecifically, we decompose the estimation task into quantifying the marginal contribution of each\nfactor independently. As illustrated in Figure 1, consider the task of predicting whether a person will\nexperience a stroke. For a given factor such as ‚ÄúAge=79‚Äù, we compare the LLM‚Äôs estimated proba-\nbility when the model has access to this factor versus when it does not have access to it, paired with\na random subset S of the remaining factors (see Section 3 for details). The marginal contribution of\na factor, referred to as its Shapley value, is computed by averaging over different subsets S. These\nvalues capture both the direction and the intensity of the features‚Äô contribution to the prediction\noutcome. However, unlike traditional Shapley methods which solely focus on model output attribu-\ntion (Lundberg and Lee, 2017), PRISM goes further: we reconstruct a new probability estimate by\naggregating the factor contributions. For example, in Figure 1, the aggregated contributions yield\na probability of 0.61, and we can clearly interpret this estimate originates from key risk-increasing\nfactors such as advanced age and having hypertension. This process can make the prediction process\ntransparent and allow human users to diagnose and interpret the model‚Äôs reasoning.\nIn our experiments, we validate the effectiveness of our proposed method under various settings. In\nSection 4.1, we compare PRISM with other LLM-based probability estimation methods, on binary\nclassification tasks under benchmark tabular datasets (such as Adult Census Income (Becker and\nKohavi, 1996), Heart Disease Prediction (Chicco and Jurman, 2020), Stroke Prediction (Kaggle,\n2021) and Loan Default Prediction (LendingClub, 2007‚Äì2018)). We find that PRISM demonstrates\nhigher prediction performance than directly LLM prompting and other representative baselines. Fur-\nthermore, in Section 4.2, we evaluate PRISM on real-world prediction tasks involving textual and\nnumerical inputs. For instance, we predict whether the a patient will be re-admitted to hospital,\nbased on the doctor notes, price of an agricultural commodity will rise in the next year using the\nprevious year‚Äôs annual report, and we forecast the outcomes of English football matches based on\npre-match reports. Together, these studies demonstrate PRISM‚Äôs ability to handle heterogeneous,\ncontext-rich factors‚Äîwell beyond simple tabular data. Overall, the experiments highlight PRISM‚Äôs\npotential as a practical and reliable tool for a wide range of real-world prediction problems.\n2\nRELATED WORKS\n2.1\nPROBABILITY ESTIMATION VIA TRADITIONAL ML AND LLMS\nTraditional machine learning models such as Bayesian inference (Berger, 2013), multilayer percep-\ntrons (MLPs) (Rumelhart et al., 1986) and decision trees (Quinlan, 1986) have long been widely\nused for probability estimation, typically cast as binary classification tasks on tabular data. These\nmethods often rely on large amount of data and well-designed models. To overcome this limitation,\nrecent works have explored using Large Language Models (LLMs) for probability estimation by\nleveraging their rich prior knowledge. In this paper, we focus on LLM-based probability estimation\nunder zero-shot setting which makes estimations only based on the LLM‚Äôs own knowledge. Under\nthis setting, previous studies validate the potential especially for tabular format datasets (Sui et al.,\n2024; Chung et al., 2024; Ren et al., 2025; Xie et al., 2024). However, although many of these\n"}, {"page": 3, "text": "Preprint\nmethods show decent estimation precision, their estimation process usually relies on direct LLM\nprompting. Thus, it can be difficult for the users to exactly measure the contribution of each factor\nto the final prediction, different from the case in traditional ML models. As a more relevant paper\nto our work, BIRD (Feng et al., 2024) is proposed to integrate Bayesian networks (Berger, 2013) to\nexplicitly calculate the probability of the prediction outcome conditioning on each factor. However,\nit assumes the factors are independent to each other, and it has to transform the factors into cate-\ngorical values. Notably, beyond the zero-shot setting, there are related studies focusing on few-shot\nprediction with LLMs (Hegselmann et al., 2023; Brown et al., 2020). However, in such cases, the\ninterpretation can become more complicated, as it should disentangle whether a prediction arises\nfrom the own knowledge of LLMs or from the provided demonstrations. Therefore, we leave the\nexploration of few-shot prediction tasks to future work.\n2.2\nEXPLAINABLE PROBABILITY ESTIMATION\nFor traditional ML explanation, Shapley-value (Shapley et al., 1953; Lundberg and Lee, 2017;\nÀáStrumbelj and Kononenko, 2014) is widely used to attribute predicted probabilities to input factors.\nIn general, it quantifies each factor‚Äôs marginal contribution to the final prediction outcome. Build-\ning on this line, recent works adapt Shapley values to explain the behavior of LLMs. For instance,\nTokenSHAP (Goldshmidt and Horovicz, 2024) estimates token-level contributions to LLM outputs\nin general question answering tasks. SyntaxSHAP (Amara et al., 2024) moves beyond token-level\nattributions by capturing the contribution of higher-level syntactic units. Unlike these studies, our\nwork specifically targets on probability estimation tasks, where we quantify the contributions of each\n‚Äúinfluencing factor‚Äù in the probability estimation. Besides,, our approach goes beyond conventional\nLLM or ML interpretation tools. We reconstruct new probability estimates through a principled\napproach of factor aggregation, rather than only interpreting the original model outputs.\n3\nMETHODOLOGY\nIn this section, we formally introduce and describe our proposed method Probability Reconstruction\nvia Shapley Measures (PRISM). In Section 3.1, we first introduce the necessary notations and defini-\ntions, with particular emphasis on the Shapley value and its importance for ML model explanations.\nSection 3.2 then provides a full description of the PRISM algorithm, clarifying how factor contribu-\ntions are combined to reconstruct probabilities. Finally, Section 3.3 introduces an efficient variant\nof PRISM specifically designed for tabular tasks, enabling faster computation in such scenarios.\n3.1\nDEFINITIONS AND NOTATIONS\nNotations. In our study, for a probability estimation task, we denote an instance by x = (x1, . . . , xm)\nand its true outcome by ytrue ‚àà{0, 1}, where each xi is a factor that can influence the outcome. We\nlet f(¬∑) denote a general model output, which may come from either a traditional ML binary classifier\nor from LLM-based predictions. Depending on the setting, f(¬∑) can represent either the predicted\nprobability or the model logit (before sigmoid function). In general, it is desirable to obtain the\nestimation f(x) to be close or aligned to ytrue.\nIn our paper, an important concept is Shapley value (Shapley et al., 1953) that applied in ML pre-\ndiction explanation (Lundberg and Lee, 2017). In the next, we first introduce the basic definition of\nShapley value (for ML models) in general, and we discuss one important property of it.\nDefinition 1 (Shapley value). Let I = {1, 2, . . . , m} denote the index set of factors, and let x =\n(x1, . . . , xm). The Shapley value of factor i ‚ààI for instance x with respect to model f is:\nœïi(f, x) =\nX\nS‚äÜI\\{i}\n|S|! (m ‚àí|S| ‚àí1)!\nm!\nh\nf(xS‚à™{i}) ‚àíf(xS)\ni\n,\n(1)\nwhere each S is a subset of factors I excluding i, and the sum is taken over all S ‚äÜI \\ {i}, f(xS) is\nthe model output when only the factors in S are used (and f(xS‚à™{i}) is defined analogously). In our\npaper, we name each S as a ‚Äúbackground set‚Äù.\nIntuitively, Shapley value œïi(f, x) measures the marginal contribution of factor i to the model output,\nby comparing the model‚Äôs output when factor i is ‚Äúincluded in the background set S‚Äù versus when\nit is ‚Äúnot included in the background set S‚Äù, across all possible cases of S. In this way, one can\ninterpret the model‚Äôs prediction by separately examining the contribution of each individual factor.\n"}, {"page": 4, "text": "Preprint\nIn practice, for traditional ML, the difference\n\u0000f(xS‚à™{i}) ‚àíf(xS)\n\u0001\ncan be computed either by retrain-\ning models on different subsets of factors (Lipovetsky and Conklin, 2001), or by approximating the\neffect of missing factors using training data (ÀáStrumbelj and Kononenko, 2014). Next, we discuss\none important property of Shapley value in traditional MLs which inspires our method.\nProperty 1 (Additivity (Lundberg and Lee, 2017)). Let œï0 = f(x‚àÖ) be the expected model output\nwhen no factor information is available. Then, for models with deterministic scalar outputs, the sum\nof œï0 and the Shapley value of the factors œïi(f, x) must reconstruct the model prediction itself.\nf(x) = œï0 +\nm\nX\ni=1\nœïi(f, x).\n(2)\nThat is, the explanation exactly matches the original model output.\nIt suggests for traditional ML models, the output f(x) can be decomposed to the sum of Shapley\nvalues and the base logit œï0. However, Property 1 does not extend easily to Large Language Models\n(LLMs). This is because, when LLMs make predictions for uncertain events, they express their prob-\nability estimates through generated text (see Figure 1 (right)). Such expressed probabilities differ\nfrom the direct model outputs, such as the token probabilities produced by the LLM architectures.\n3.2\nPRISM: PROBABILITY RECONSTRUCTION VIA SHAPLEY MEASURES\nNevertheless, inspired by Property 1, we devise our new estimation framework: we first obtain the\nShapley values for each factor and then aggregate them to reconstruct a new prediction outcome. In\nthe next, we elaborate two important steps to obtain the Shapley values through LLMs.\nFigure 2: Comparative Prompting.\nComparative LLM Prompting. Given a factor of interest,\nsuch as ‚Äúi = Age is 79‚Äù in the stroke prediction task, we first\nsample multiple background sets S from I \\ {i} (following\nthe permutation sampling strategy in the next part). Each\nS contains information of several factors that different from\ni, such as ‚Äúhypertension, female and BMI=22.3‚Äù illustrated\nin Figure 2. Next, for a given background set S, we prompt\nthe LLM to evaluate both xS‚à™{i} and xS in the same query.\nWe denote p(¬∑) as the LLM estimated probability for a given\ninstance (e.g., by responding to a question as in Figure 2). Then, if we let f(¬∑) = œÉ‚àí1(p(¬∑)), where\nœÉ‚àí1(¬∑) is the inverse sigmoid function, we have:\nf(xS‚à™{i}) ‚àíf(xS) = œÉ‚àí1\u0000p(xS‚à™{i})\n\u0001\n‚àíœÉ‚àí1\u0000p(xS)\n\u0001\n,\n(3)\nThis calculates the difference term in Shapley value defined in Eq.(1). We argue that: the single\nestimates derived by directly LLM prompting may not be exact, but the relative relation between the\ntwo cases can be well-captured. Recent studies also find that LLMs tend to perform more reliably on\nranking or comparison tasks than absolute probability estimation (Qin et al., 2023; Liu et al., 2024),\nwhich supports the validity of our design.\nPermutation Sampling. In PRISM, at each iteration we sample a background set S using the\npermutation sampling rule (Shapley et al., 1953; ÀáStrumbelj and Kononenko, 2014). Concretely, we\ngenerate a random permutation of all factors in I including i, and choose S as the set of factors\nthat precede i in this order. Then, we directly average the pair-wise differences (from Eq.(3)) across\nmultiple (e.g., K times) such samplings to approximate the Shapley value:\nœïi = 1\nK\nK\nX\nk=1\nh\nœÉ‚àí1\u0000p(xS(k)‚à™{i})\n\u0001\n‚àíœÉ‚àí1\u0000p(xS(k))\n\u0001i\n,\n(4)\nThe equivalence of Eq.(4) and Eq.(1) is shown in (Shapley et al., 1953). In practice, it is necessary\nto sample S for multiple times, ensuring each background factor is considered with a non-negligible\nprobability. This will lead to a more comprehensive estimation by accounting for possible factor\ninteractions (see Section 4.3 for further discussion).\nFinally, we set œï0 as the base logit, either from the population average (e.g., œï0 = œÉ‚àí1(Average\nstroke prevalence), or simply set to 0 when no prior knowledge is available in other tasks. We then\n"}, {"page": 5, "text": "Preprint\nreconstruct the probability estimation as:\npPRISM(x) = œÉ(œï0 +\nm\nX\ni=1\nœïi).\n(5)\nWe provide the detailed algorithm sketch in Appendix A.1.\n3.3\nTABULAR-PRISM\nIn this subsection, we discuss a variant of PRISM, when the factors can be represented as single\nvalues or categories and be integrated in tabular datasets. For such tasks, instead of calculating the\ndifference term in Eq.(3) for each S in a separate LLM query, we can instead consider multiple\nbackground sets in one single query. Refer to Figure 3a, we can present xS and xS‚à™{i} for different\nS in the same table (the adjacent rows are from the same S). Then, we prompt LLMs to evaluate\neach of them in one query. In this way, the query and token efficiency can be significantly improved.\nHowever, a challenge is that: if we leave those factors that are not presented as blank, ‚Äúunknown‚Äù or\n‚Äúnot provided‚Äù, it will introduce bias to the LLM judgment. For example, LLM can over-estimate a\nperson‚Äôs risk of certain disease only because some information is ‚Äúunknown‚Äù.\n(a) With blanks.\n(b) With reference.\nFigure 3: When calculating Shapley value of the factor\n‚ÄúAge=79‚Äù, we put multiple S in one table. Factor values\nfrom reference instances are noted in green.\nTo solve this issue, we leverage the strategy to\nintroduce a reference instance, which is also\nused in Shapley ML explanation (ÀáStrumbelj and\nKononenko, 2014). Specifically, we let r be a\nreference instance with each factor to be a fixed\nvalue (typically obtained from population aver-\nage or majority). Then, we use the information\nof this reference instance to impute missing fac-\ntors, which is illustrated in Figure 3b, and we\ncalculate the Shapley values (with a new defini-\ntion) based on this new table.\nDefinition 2. For background set S, we define the model output of the imputed sample as vr(S) =\nf\n\u0000[xS, r¬ØS]\n\u0001\n, where ¬ØS = I \\ S, which means the factors in S are provided by x, and the remaining are\nprovided by r. Then, for each factor xi, the (reference-specific) Shapley value is\nœï(r)\ni\n=\nX\nS‚äÜI\\{i}\n|S|! (m ‚àí|S| ‚àí1)!\nm!\n\u0010\nvr(S ‚à™{i}) ‚àívr(S)\n\u0011\n.\n(6)\nEssentially, this new definition has a different interpretation compared to the original definition in\nEq.(1). It emphasizes the comparison to the reference sample. For example, a large positive œï(r)\ni\nfor ‚ÄúAge = 79‚Äù suggests: compared with ‚ÄúAge = 40‚Äù (from reference sample), the factor ‚ÄúAge\n= 79‚Äù increases the risk of having stroke greatly. Despite different interpretations, the following\nproposition can still allow us to reconstruct predictions from these Shapley values.\nProposition 1. Fix an instance x and a single reference sample r. Let œï(r)\ni\nbe the Shapley value in\nDefinition 2. Then, for models with deterministic scalar outputs, with œï(r)\n0 = vr(‚àÖ), we still have:\nf(x) = vr(I) = œï(r)\n0 +\nm\nX\ni=1\nœï(r)\ni\n(7)\nSimilar to Property 1, this proposition suggests that: in LLMs, we can also reconstruct a new esti-\nmation by aggregating Shapley values, following the rule: pPRISM(x) = œÉ(œï(r)\n0 +Pm\ni=1 œï(r)\ni ). We defer\nboth the detailed algorithm sketch and the proof in Appendix A.1 and Appendix A.2.\n3.4\nFACTOR EXTRACTION\nIn PRISM, it is essential to determine which factors should be considered during prediction. We\nadopt two criteria for selecting these factors: (1) Non-overlap, which avoids redundancy by ensur-\ning that the same information is not repeatedly considered or recalculated, and (2) Completeness,\nwhich ensures that the union of all factors covers the necessary information used by the LLM. In\n"}, {"page": 6, "text": "Preprint\npractice, for tabular tasks, factors are directly chosen from the provided tabular features after re-\nmoving duplicates. For unstructured tasks, we use an automated pipeline. We first query an LLM\nto propose the minimal set of aspects required to complete the task, with the constraints that these\naspects must be non-overlapping and complete. For each aspect, we then extract a summary from\nthe given context, which serves as a ‚Äúfactor‚Äù within the PRISM framework.\n4\nEXPERIMENT AND DISCUSSION\nIn this section, we conduct experiments to validate the effectiveness of PRISM. In Section 4.1,\nwe compare the estimation performance of PRISM and baselines on benchmark (tabular) datasets.\nMeanwhile, we provide case analysis to visualize the interpretation outcome of PRISM. In Sec-\ntion 4.2, we demonstrate that PRISM can also be applied in real-world estimation tasks which cannot\nbe easily formed into tabular format. Finally, Section 4.3 provides ablation studies about whether\nPRISM can handle feature interactions (Hall, 1999), and discuss its computational efficiency.\n4.1\nPROBABILITY ESTIMATION ON BENCHMARK TABULAR DATASETS\nWe first evaluate our method focused on benchmark tabular datasets. This is because probability\nestimation tasks are more frequently built in tabular format, for traditional ML studies. Therefore,\nwe have various datasets with sufficient samples for fair and comprehensive comparisons. We later\ndiscuss scenarios beyond tabular format in Section 4.2.\nExperiment setup. We involve four representative tabular datasets ranging from disease prediction,\nincome prediction, and loan credit estimation. In details, Adult Census Income (Becker and Kohavi,\n1996) is to predict whether a person has annual income over $50K, based on their occupation, edu-\ncation and family information. Stroke (Kaggle, 2021) and Heart Disease (Chicco and Jurman, 2020)\npredict whether a certain disease will appear based on the patients‚Äô health status. Lending (Lending-\nClub, 2007‚Äì2018) predicts whether a loan default will happen, based on the loan application records\nand applicants‚Äô personal information. More details and pre-process procedures are in Appendix B.1.\nIn our study, we majorly consider the zero-shot settings, where the estimations are solely relied on\nthe LLM‚Äôs own knowledge. For our method, we implement Tabular-PRISM (in Section 3.3) and we\nsample the background set S for 10 times (for each Shapley value) 1. We also consider the baselines:\n‚Ä¢ Directly prompting LLMs to predict likelihood levels. For example, we ask LLM to choose one\nin the options from ‚Äúvery unlikely‚Äù to ‚Äúvery likely‚Äù. We also try multiple shots to obtain the\nself-consistency result (Wang et al., 2022) by making votes. They are denoted as ‚Äú1shot level,\n5shot level, 10shot level‚Äù in Table 1.\n‚Ä¢ Directly prompting LLMs to predict likelihood scores, which are probability scores between\n[0-1], and we obtain the self-consistency result by taking their average. They are denoted as\n‚Äú1shot score, 5shot score, 10shot score‚Äù in Table 1\n‚Ä¢ Contrast (motivated by Nafar et al. (2025)) asks about the likelihood in the positive and the\nnegated question form, and then unify the answers to get the final estimation.\n‚Ä¢ BIRD (Feng et al., 2024) builds Bayesian networks to evaluate the probability of the estimation\noutcome conditioning on various (categorized) factors.\n‚Ä¢ We also add In-Context Learning (ICL, Brown et al. (2020)), which is beyond zero-shot setting.\nWe randomly select 5 positive and 5 negative samples as demonstrations for each instance (or\n10 positive and 10 negative respectively).\nFor each setting and method, we conduct experiments on GPT-4.1-mini (OpenAI, 2024) and Gemini-\n2.5-Pro (Google DeepMind, 2025). All runs use temperature 1.0 under default settings.\nExperiment result. In our result shown in Table 1, for each dataset, we randomly choose 300 test\nsamples (150 positive and 150 negative), and we report AUROC (shown as ‚ÄúROC‚Äù in Table 1),\nAUPRC (shown as ‚ÄúPRC‚Äù), and the best F1 (when select the threshold for maximized TPR +\nTNR). From the result, we can see the PRISM consistently demonstrates reliable estimation per-\nformance. Specifically, PRISM presents highest performance or it is close to the strongest baselines\n1We set the factor values in the reference instance to be around the population average (for continuous\nvalues), and population majority (for categorical values). We query an LLM for the base logit œï0. In practice,\none can choose œï0 from more reliable sources such as historical statistics or human experts, and the selection\nof œï0 will not impact the later evaluation metrics, including AUROC, AUPRC and F1.\n"}, {"page": 7, "text": "Preprint\nAdult Census\nHeart Disease\nStroke\nLending\nModel / Method\nROC\nPRC\nF1\nROC\nPRC\nF1\nROC\nPRC\nF1\nROC\nPRC\nF1\nGPT-4.1-mini\n1shot level\n0.777\n0.773\n0.709\n0.722\n0.706\n0.615\n0.767\n0.694\n0.726\n0.629\n0.606\n0.478\n5shot level\n0.795\n0.779\n0.701\n0.759\n0.723\n0.664\n0.780\n0.716\n0.730\n0.617\n0.576\n0.648\n10shot level\n0.799\n0.779\n0.701\n0.759\n0.718\n0.672\n0.792\n0.727\n0.732\n0.627\n0.591\n0.443\n1shot score\n0.795\n0.792\n0.709\n0.799\n0.761\n0.772\n0.804\n0.763\n0.753\n0.612\n0.600\n0.558\n5shot score\n0.819\n0.820\n0.755\n0.807\n0.779\n0.782\n0.816\n0.783\n0.780\n0.629\n0.621\n0.612\n10shot score\n0.816\n0.820\n0.734\n0.806\n0.776\n0.793\n0.813\n0.781\n0.785\n0.636\n0.631\n0.621\nICL-5+5\n0.807\n0.788\n0.707\n0.803\n0.761\n0.762\n0.801\n0.751\n0.775\n0.598\n0.567\n0.668\nICL-10+10\n0.754\n0.758\n0.645\n0.776\n0.744\n0.760\n0.788\n0.736\n0.763\n0.591\n0.569\n0.642\nContrast\n0.790\n0.813\n0.697\n0.769\n0.729\n0.738\n0.790\n0.813\n0.697\n0.485\n0.543\n0.169\nBIRD\n0.813\n0.804\n0.730\n0.777\n0.748\n0.712\n0.778\n0.740\n0.729\n0.610\n0.564\n0.533\nPRISM (Ours)\n0.851\n0.874\n0.770\n0.816\n0.799\n0.793\n0.814\n0.783\n0.790\n0.655\n0.626\n0.671\nStd. Err. (Ours)\n0.0215\n0.0221\n0.0296\n0.0254\n0.0350\n0.0250\n0.0311\n0.0430\n0.0283\n0.0313\n0.0428\n0.0300\nGemini-2.5-Pro\n1shot level\n0.818\n0.797\n0.699\n0.732\n0.668\n0.780\n0.793\n0.744\n0.720\n0.555\n0.538\n0.518\n5shot level\n0.822\n0.794\n0.694\n0.758\n0.686\n0.794\n0.804\n0.749\n0.727\n0.541\n0.526\n0.511\n10shot level\n0.821\n0.797\n0.688\n0.738\n0.663\n0.791\n0.803\n0.745\n0.716\n0.564\n0.543\n0.571\n1shot score\n0.855\n0.876\n0.767\n0.812\n0.739\n0.797\n0.836\n0.812\n0.802\n0.536\n0.538\n0.465\n5shot score\n0.864\n0.879\n0.823\n0.816\n0.749\n0.795\n0.834\n0.810\n0.804\n0.529\n0.534\n0.533\n10shot score\n0.864\n0.878\n0.826\n0.815\n0.753\n0.799\n0.835\n0.814\n0.803\n0.528\n0.526\n0.547\nICL-5+5\n0.842\n0.834\n0.744\n0.834\n0.785\n0.770\n0.815\n0.767\n0.728\n0.615\n0.576\n0.575\nICL-10+10\n0.812\n0.793\n0.722\n0.807\n0.756\n0.739\n0.818\n0.776\n0.738\n0.626\n0.584\n0.690\nContrast\n0.835\n0.830\n0.757\n0.831\n0.779\n0.789\n0.806\n0.768\n0.727\n0.593\n0.588\n0.520\nBIRD\n0.799\n0.794\n0.754\n0.810\n0.767\n0.762\n0.803\n0.779\n0.735\n0.555\n0.528\n0.625\nPRISM (Ours)\n0.876\n0.893\n0.826\n0.844\n0.832\n0.787\n0.826\n0.788\n0.783\n0.654\n0.614\n0.685\nStd. Err. (Ours)\n0.0198\n0.0194\n0.0239\n0.0220\n0.0298\n0.0257\n0.0245\n0.0369\n0.0252\n0.0314\n0.0426\n0.0277\nTable 1: Performance comparison across various tabular datasets. Highest results are in dark red.\nSecond-highest results are in orange.\nFactor\nCase 1\nCase 2\nCase 3\nCase 4\nValue\nShapley\nValue\nShapley\nValue\nShapley\nValue\nShapley\nGender\nFemale\n-0.08\nFemale\n0.00\nMale\n0.00\nFemale\n0.00\nAge\n82\n1.17\n52\n0.57\n68\n1.15\n29\n-0.77\nHypertension\nYes\n1.64\nYes\n1.41\nNo\n0.00\nNo\n0.00\nHeart Disease\nYes\n0.66\nNo\n0.00\nYes\n1.17\nNo\n0.00\nMarital Status\nNever Married\n0.00\nEver Married\n0.00\nEver Married\n0.00\nEver Married\n0.00\nWork Type\nGovernment job\n0.00\nPrivate sector\n0.00\nPrivate sector\n0.00\nPrivate sector\n0.00\nResidence Type\nRural\n0.00\nUrban\n0.00\nUrban\n0.00\nUrban\n0.00\nGlucose Level\n84.03\n0.00\n94.98\n0.51\n223.83\n0.62\n116.98\n0.98\nBMI\n25.60\n0.00\n23.80\n0.00\n31.90\n0.53\n23.40\n0.00\nSmoking Status\nSmokes\n0.85\nNever smoked\n0.00\nFormerly smoked\n0.60\nNever smoked\n0.00\nSum Shapley\n4.240\n2.490\n4.070\n0.210\nSum logit\n-0.355\n-2.105\n-0.525\n-4.385\nPredicted prob\n0.413\n0.109\n0.372\n0.012\nTrue label\nYes\nNo\nYes\nNo\nTable 2: Shapley values for four instances in Stroke dataset. Reference instance: gender=Male;\nage=40; hypertension=No; heart disease=No; marital status=Never Married; residence=Rural;\naverage glucose=90.0; BMI=24.0; work type=Private; smoking status=never smoked. A single\nbase logit is shared across cases, œï0 = œÉ‚àí1(0.01)=-4.5951.\nin most datasets, including Adult Census, Heart Disease and Lending, under both LLMs. In these\ndatasets, we see PRISM has more obvious improvement in AUROC and AUPRC than F1-scores.\nThis suggests that the baselines can sometimes effectively separate positive and negative samples\nwhen an appropriate threshold is chosen, whereas the high AUROC and AUPRC of PRISM indicate\nits strong discriminative ability across all possible thresholds. Under Stroke Dataset, our method is\ncomparable to strong baselines or slightly lower than those strong baselines. For example, under\nGemini-2.5-pro, ‚Äú1shot score‚Äù has higher AUROC, AUPRC and F1 than PRISM. We conjecture\nit may be because the LLM itself has well-educated knowledge in the relevant domain. However,\nPRISM demonstrates stable performance across various datasets and shows competency against the\nstrongest baselines, if not surpassing them. in Appendix C, we show PRISM‚Äôs predicted probability\nalso has a good calibration (Bella et al., 2010) if a proper base logit is selected.\nFor the baseline methods, we find ‚Äúscore‚Äù based LLM prompting are generally better than ‚Äúlevel‚Äù\nbased prompting. The most comparable baseline with PRISM is ‚Äú5shot score‚Äù and ‚Äú10shot score‚Äù.\nHowever, we would like to argue that: the multiple-query strategy (or namely Self-Consistency) is\nless interpretable than single-query in practice, as they rely on aggregating multiple diverse estima-\ntion paths, making it difficult for a uniform assessment of the factor contribution. Besides, BIRD\nis the only method with explicit interpretable structure among the baselines. Compared to BIRD,\nPRISM has obviously higher performance across different settings.\nExamination of Interpretations. In Table 2, we present instances under Stroke dataset to help un-\nderstand the interpretation process of PRISM. In detail, we present the factor values as well as the\n"}, {"page": 8, "text": "Preprint\nDirect LLM Prompting predicted\nprobability of 30-day readmission: 0.35\nKey points:\n- Patient with primary CNS high-grade B-\ncell lymphoma on cycle #4 high-dose \nmethotrexate.\n- Current admission stable: no major \ncomplications, labs mostly stable.\n- No active infection, vital signs stable, \nambulatory, good tolerance to chemo.\n- Moderate underlying risk from intensive \nchemotherapy, anemia, and fragile\nbaseline status.\nOverall judged moderate but not very high \nrisk of readmission.\n@@Final probability (direct prompt):\n0.35@@\nPRISM predicted probability of readmit in 30 days: 0.79\nBaseline Vulnerability. (Shapley value = 0.88)\nPrimary CNS lymphoma with high-grade B-cell type and prior\nneurological symptoms (fatigue, memory, word-finding issues) indicate \nsignificant baseline disease burden and vulnerability to clinical instability.\nRecent Clinical Trajectory. (Shapley value = 0.10)\nCurrently on cycle #4 of high-dose methotrexate with \nstable interim status; no recent hospitalizations or ER visits,\nsuggesting a stable but ongoing high-risk treatment course.\nSeverity of This Admission. (Shapley value = -0.51)\nNo major complications or invasive procedures during this admission; \ntolerated chemotherapy well with stable labs and no acute clinical\ndeterioration\nResolution of the Acute Issue. (Shapley value = 0.83)\nCurrently on cycle #4 of high-dose methotrexate with \nstable interim status; no recent hospitalizations or ER visits, \nsuggesting a stable but ongoing high-risk treatment course.\nPost-discharge Management Complexity. (Shapley value = 0.87)\nComplex chemotherapy regimen requiring frequent follow-up, urine \nalkalinization, multiple medications including increased trazodone and\nPRN lorazepam; home care demands moderate but manageable.\nPsychosocial Risk. (Shapley value = 0.75)\nHistory of depression and insomnia may affect medication \nadherence and symptom reporting, increasing risk of\ndestabilization post-discharge.\nMedication Risks. (Shapley value = 0.47)\nPolypharmacy with CNS-active drugs (trazodone, lorazepam) may \nincrease risk of sedation, falls, or cognitive changes shortly after \ndischarge.\nHome Support and Follow-up. (Shapley value = -0.42)\nDischarge home with planned close oncology follow-up in 2\nweeks mitigates some risk but any delay or missed \nappointments could increase readmission risk.\nFigure 4: PRISM attribution example from MIMIC-III (True La-\nbel=1), illustrating the contribution of each factor.\nGPT\nROC\nPRC\nF1\n1shot level\n0.621\n0.573\n0.573\n5shot level\n0.639\n0.627\n0.657\n1shot score\n0.635\n0.607\n0.503\n5shot score\n0.641\n0.616\n0.632\nPRISM\n0.630\n0.645\n0.688\nGemini\nROC\nPRC\nF1\n1shot level\n0.642\n0.634\n0.603\n5shot level\n0.663\n0.672\n0.661\n1shot score\n0.658\n0.658\n0.592\n5shot score\n0.672\n0.675\n0.667\nPRISM\n0.659\n0.673\n0.674\nTable 3:\nPerformance on the\nMIMIC-III dataset\n(a) PRISM-10 shot\n(b) Extracted-10 shot\n(c) Raw-10 shot\nFigure 5: Prediction on whether the price of a type of apple will increase. Blue bars are actual price\nincrease rates. Red bars are the estimated increase probability.\nestimated Shapley values (Eq.(6)). Notably, the Shapley value here represents the relative contribu-\ntion of each factor, comparing to the reference instance (see Section 3.3). For example, in Case 1,\na Shapley value of 0.66 for ‚ÄúHeart Disease‚Äù suggests: compared with ‚ÄúNo Heart Disease‚Äù (from\nreference instance), this factor increases the risk of having stroke. Similarly, in Case 2, because the\nperson does not have heart disease which is the same as reference instance, the Shapley value is 0.\nIn Appendix C, we provide the interpretation results similar to Table 2 of other datasets.\n4.2\nPROBABILITY ESTIMATION ON UNSTRUCTURED DATA\nBeyond tabular data, there are often cases where the influencing factors cannot be easily transformed\ninto single values to be inputted into tables. For example, the factors can be in form of descriptive\nfacts that are extracted from unstructured sources such as news articles, financial reports, or social\nmedia. Probability estimation in this scenario is also of great importance. We conduct multiple\nstudies to demonstrate the applicability of PRISM (the version in Section 3.2) in such setting.\nPredicting 30-day Readmission after Discharging. We evaluate an unstructured-text prediction\ntask under MIMIC-III dataset (Johnson et al., 2016). We aim to use the discharge summaries (doctor\nnotes) to predict whether a patient will be readmitted to the hospitals within 30 days. In PRISM, we\nconsider eight distinct factors, including the patients‚Äô overall health status and their recent hospital\ncourse and so on. We evaluate 200 samples (100 positive and 100 negative cases) using both GPT-\n4.1-mini and Gemini-2.5-Pro. We compare PRISM against directly prompting baselines. Our results\nin table 3 show that PRISM achieves consistently higher performance, particularly on PRAUC and\nF1. For these direct-prompting baselines, we find that LLMs such as GPT-4.1-mini often provide\nthe same values (0.35 and 0.65) for different samples, reducing their ability to provide more fine-\ngrained ranking. Moreover, Figure 4 shows a full attribution example on a true readmission case.\nDirect prompting yields only a coarse estimate (0.35) without explaining the underlying clinical\ndrivers. In contrast, PRISM produces a calibrated prediction (0.79) and distributes contributions\nacross eight clinically meaningful factors, revealing how baseline disease burden, treatment inten-\nsity, psychosocial risks, and post-discharge complexity elevate the patient‚Äôs risk.\nPredicting apple price. Our task is to determine ‚Äúwhether the price of apple will increase in 2025\ncompared to 2024?‚Äù, based on (U.S. Apple Association annual report 2024). Such a task may\nlater assist farmers in deciding what type of produce to grow. In this report, it provides descriptive\nanalysis regarding key factors that can influence apple prices. For each type of apple, we first use\nan LLM to generate summaries across seven aspects: production, demand, storage, imports and\nexports, policy, cost, and varietal competition. Each summary is then directly treated as a factor\n"}, {"page": 9, "text": "Preprint\nin PRISM. Notably, we use GPT-4.1 model for factor extraction and PRISM implementation, as its\nknowledge-cutoff is June 2024 and guarantees knowledge absent in 2025.\nIn Figure 5, for each type of apple, we report the actual price change rate ((price of 2025 - price of\n2024) / price of 2024) (blue bars), and the estimated probability of ‚Äúthe price will increase in 2025‚Äù\n(red bars). We compare PRISM with direct LLM prompting, which conducts estimation on the\nextracted factors (Figure 5b) and on the raw report (Figure 5c). Note that the factor extraction from\na long report is highly stochastic and it can greatly impact the estimation, we repeat the extraction\nand estimation process for 10 times and report the average estimation outcome.\nFrom the result, we see PRISM can provide relatively more promising estimation result. In detail,\nif we focus on ‚ÄúHoneycrisp (HoneyC)‚Äù, which has the largest increase in 2025, both PRISM and\n‚ÄúExtracted‚Äù give it highest estimated increase probability, although ‚ÄúExtracted‚Äù has a huge variance\nin its predictions. For ‚ÄúGranny Smith (G Smith)‚Äù, which has the largest decrease, only PRISM\ncan give it a lower expectation than other apple varieties. However, it is difficult to have more\nfine-grained comparison for PRISM and other strategies, e.g, to compare ‚ÄúFuji vs Red Delicious\n(Red D)‚Äù. In Figure 9 and Figure 10 in Appendix C, we provide all the factor details for ‚ÄúHoney-\ncrisp‚Äù and ‚ÄúGranny Smith‚Äù, showing the prediction of PRISM is reasonable and easy to interpret.\n(a) PRISM\n(b) Raw\nFigure 6: Football match result prediction.\nPredicting football matches. In this study, we\nrandomly choose 10 English football matches\nin 2025, and we leverage PRISM to estimate\nthe probability of: ‚Äúthe home team will win‚Äù.\nHere, we use GPT-5-mini, which is released be-\nfore but relatively close to match dates.\nWe\ncollect pre-match reports (sourced from Foot-\nbal365) and extract key features from five as-\npects: squad quality, head-to-head records, re-\ncent form, player availability and fitness, and\nexternal conditions. Figure 6 shows the esti-\nmated winning probability (red bars) and the match results (x-axis), where ‚ÄúL, D, W‚Äù denote ‚ÄúLose,\nDraw and Win‚Äù. Since the reports are relatively short, we only compare PRISM with LLM direct\nprompting from the raw texts. From the result, we can see PRISM can correctly predict the ‚ÄúLose‚Äù\ncases by giving them a low prediction value, and it gives the two ‚ÄúDraw‚Äù matches winning probabil-\nity around 0.5-0.6. For the ‚ÄúWin‚Äù games, it can tell 2 of out 5 winning matches by giving them scores\nover 0.7. Interestingly, we find PRISM tends to focus on accounting the factors themselves during\nprediction, while direct LLM prompting tends to rely on overall impressions, usually assuming the\nstronger teams are more likely to win (see the example in Figure 11 in Appendix C).\n4.3\nADDITIONAL STUDIES\nIn this subsection, we further answer three important questions regarding PRISM: (1) Can PRISM\ngeneralize to multi-class prediction? (2) Can it handle feature interaction (Hall, 1999)? (3) How is\nthe computational efficiency of PRISM?\nMethod\nAccuracy\n1shot\n0.437\n10shot\n0.453\nPRISM (Ours)\n0.613\nTable 4:\nAccuracy on the\nUCI-Wine.\nCan PRISM generalize to multi-class prediction? We provide a\nconcise strategy to extend PRISM to multi-class prediction tasks.\nConsider a prediction task with M possible classes, in PRISM, we\naim to quantify the marginal contribution of each factor i to each\npossible class m among all possible classes m ‚àà{1, 2, ..., M}. In\ndetail, given a background set S and an interested factor i (see Defi-\nnition 1 of our paper), we query the LLM to estimate the probability\nof xS and xS‚à™{i} to belong to each class m ‚àà{1, 2, ...m}. We de-\nnote these LLM generated estimations as pm(xS) and pm(xS‚à™{i}). Similar to Section 3, we can use\nthese estimated probability values to calculate fm(xS‚à™{i})‚àífm(xS) which is the logit difference, and\nthen get the Shapley value œïm\ni to each class m. In the experiment, we conduct a study on UCI-Wine\ndataset (3 classes) which is to predict the category of different types of wines. We compare PRISM\nwith directly prompting, which let LLM to choose the most likely class among the three options.\nIn the setting, we consider 1 time prompting (1shot) or asking 10 times and get the majority voting\n(10shot). The accuracy reported in Table 4 shows PRISM can greatly outperform both baselines.\nThis significant gap may arise from direct prompting cannot well distinguish Class 2 (Grignolino)\n"}, {"page": 10, "text": "Preprint\nfrom Class 3 (Barbera), which are frequently confused due to their highly overlapping feature distri-\nbutions. In contrast, PRISM introduces pairwise comparisons that amplify these small differences.\nCan PRISM handle feature interaction? In ML probability estimation, feature interaction usu-\nally happens as the contribution of one factor to the outcome also highly depends on the condition\nof another factor. Taking this into consideration is necessary for precise and reliable estimation.\nFigure 7 demonstrates that PRISM can indeed consider feature interactions when calculating the\nShapley values and the final estimation. In detail, we focus on the task to predict loan default in\nLending dataset under GPT-4.1-mini (see Section 4.1), where feature interactions can naturally oc-\ncur between ‚ÄúLoan Amount‚Äù and ‚ÄúAnnual Income‚Äù. In Figure 7a, we compute the average Shapley\nvalue of Loan Amount across different instances, conditioned on loan amount (vertical axis) and an-\nnual income (horizontal axis). From the result, we can see that the individuals with annual income\n120K+ receive a Shapley value of 0.18 for the factor ‚Äúhaving a loan amount over 30K+‚Äù. It is lower\nthan the values assigned to people with income in 0‚Äì60K or 60K‚Äì120K. This indicates that, within\nPRISM, for individuals earning above 120K, having a loan amount over 30K+ is not considered as\nrisky as those with lower incomes. Similarly, Figure 7b computes the average Shapley value of\nAnnual Income. We can see PRISM believes that having a factor ‚ÄúAnnual Income over 120K+‚Äù can\ngreatly reduce the risk (-0.65), if their loan amount is over 30K+, and it only moderately reduces the\nrisk (-0.26), if the loan amount is low, e.g., below 10K. It indeed shows a Shapely value in PRISM\ndoes not only rely on its factor of interest, but also other factors.\n(a) Loan Amount\n(b) Annual Income\nFigure 7: Avg. Shapley under various conditions\nComputational efficiency of PRISM. In this\npart, we analyze the computational efficiency\nof PRISM. Table 5 summarizes two notions\nof complexity: Query Complexity counts the\nnumber of API requests that each method is-\nsues, and LLM Evaluation Complexity counts\nthe number of instances that each method need\nto evaluate. In the table, we compare PRISM\nwith 1-shot direct prompting and n-shot direct\nprompting. For PRISM, it needs to calculate\nShapley values for m factors one by one. For\neach factor, it makes K samplings of back-\nground set for comparison. Therefore, it has a\nquery and evaluation complexity Œò(mK). Tabular-PRISM can input multiple S samplings into one\nquery, so it has a query complexity Œò(m). In practice, we argue that Tabular-PRISM strategy can\ngreatly reduce the time and token cost, as it saves API calls and evaluate multiple instances in one\nanswer. For the actual time cost, in Stroke dataset (with m = 10, K = 10), Tabular-PRISM takes\n92.7s for each instance on average under GPT-4.1-mini non-batched API calling. Apple price pre-\ndiction using PRISM (GPT-4.1, m = 7, K = 5) takes around 330s on average, due to large amount\nof queries, long inputs (each factor is a paragraph) and larger model size. This efficiency can be\nacceptable if the evaluation size is not large.\nMetric\nQuery Complexity\nLLM Evaluation Complexity\n1-shot\nŒò(1)\nŒò(1)\nn-shot\nŒò(n)\nŒò(n)\nTabular-PRISM\nŒò(m)\nŒò(mK)\nPRISM\nŒò(mK)\nŒò(mK)\nTable 5: Complexity of PRISM and baselines.\n5\nCONCLUSION AND LIMITATIONS\nIn this work, we propose Probability Reconstruction via Shapley Measures (RPSIM) for LLM-based\nprobability estimation tasks. In our experiment, we empirically validate its predictive accuracy\nacross multiple benchmark datasets, demonstrating the reliability of the proposed approach. Com-\npared to direct LLM prompting, PRISM provides enhanced explainability and transparency, thereby\nenabling more trustworthy use of LLM predictions in high-stakes applications. However, our work\nhas a few limitations. First, it only focuses on the zero-shot setting. In practice, there could be\nhistorical records or references available to facilitate the prediction. In such few-shot prediction\nsettings, interpretation can become more challenging, as the system need disentangle whether a pre-\ndiction arises from its own knowledge or from the provided demonstrations. Besides, in Section 4.3,\n"}, {"page": 11, "text": "Preprint\nwe examine the efficiency of PRISM and conclude that its cost can be relatively high, making it\npractical only when the evaluation size is not too large.\n6\nREPRODUCIBILITY STATEMENT\nWe release an anonymous repository with full source code and our processed datasets (if they are not\npublicly available) at https://anonymous.4open.science/r/prism-62B5/. In Appendix A, we provide\nthe detailed algorithm sketches and theorem proof. Appendix B.1 and Appendix B.2 provide a com-\nplete description of our data pre-processing pipelines and the mentioned baselines. The experimental\nsetup, including hyperparameters, model configurations are introduced in the main text. Appendix C\ncontains additional examples, results and interpretations to aid further verification. Appendix D lists\nthe exact prompts used for all language model components.\nREFERENCES\nJames O Berger. Statistical decision theory and Bayesian analysis. Springer Science & Business\nMedia, 2013.\nRobert L Winkler, Yael Grushka-Cockayne, Kenneth C Lichtendahl Jr, and Victor Richmond R\nJose. Probability forecasts and their combination: A research perspective. Decision Analysis, 16\n(4):239‚Äì260, 2019.\nJaheera Thasleema Abdul Lathief, Sunitha Chelliah Kumaravel, Regina Velnadar, Ravi Varma Vi-\njayan, and Satyanarayana Parayitam. Quantifying risk in investment decision-making. Journal of\nRisk and Financial Management, 17(2):82, 2024.\nAlvin Rajkomar, Jeffrey Dean, and Isaac Kohane. Machine learning in medicine. New England\nJournal of Medicine, 380(14):1347‚Äì1358, 2019.\nBahman Rostami-Tabar and Rob J Hyndman. Hierarchical time series forecasting in emergency\nmedical services. Journal of Service Research, 28(2):278‚Äì295, 2025.\nPriyabrata Chowdhury, Sanjoy Kumar Paul, Shahriar Kaisar, and Md Abdul Moktadir. Covid-19\npandemic related supply chain studies: A systematic review. Transportation Research Part E:\nLogistics and Transportation Review, 148:102271, 2021.\nWullianallur Raghupathi and Viju Raghupathi. Contemporary business analytics: An overview.\nData, 6(8):86, 2021.\nRobert Fildes, Shaohui Ma, and Stephan Kolassa. Retail forecasting: Research and practice. Inter-\nnational Journal of Forecasting, 38(4):1283‚Äì1318, 2022.\nYu Feng, Ben Zhou, Weidong Lin, and Dan Roth. Bird: A trustworthy bayesian inference framework\nfor large language models. arXiv preprint arXiv:2404.12494, 2024.\nYuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Table meets llm: Can\nlarge language models understand structured table data? a benchmark and empirical study. In\nProceedings of the 17th ACM International Conference on Web Search and Data Mining, pages\n645‚Äì654, 2024.\nPhilip Chung, Christine T Fong, Andrew M Walters, Nima Aghaeepour, Meliha Yetisgen, and\nVikas N O‚ÄôReilly-Shah. Large language model capabilities in perioperative risk prediction and\nprognostication. JAMA surgery, 159(8):928‚Äì937, 2024.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824‚Äì24837, 2022.\nAliakbar Nafar, Kristen Brent Venable, Zijun Cui, and Parisa Kordjamshidi. Extracting probabilistic\nknowledge from large language models for bayesian network parameterization. arXiv preprint\narXiv:2505.15918, 2025.\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances\nin neural information processing systems, 30, 2017.\n"}, {"page": 12, "text": "Preprint\nBarry Becker and Ronny Kohavi. Adult, 1996. URL https://archive.ics.uci.edu/\ndataset/2/adult.\nDavide Chicco and Giuseppe Jurman. Machine learning can predict survival of patients with heart\nfailure from serum creatinine and ejection fraction alone. BMC medical informatics and decision\nmaking, 20(1):16, 2020.\nKaggle.\nStroke\nprediction\ndataset.\nhttps://www.kaggle.com/datasets/\nfedesoriano/stroke-prediction-dataset, 2021.\nLendingClub.\nLending club loan data.\nhttps://www.kaggle.com/datasets/\nwordsforthewise/lending-club, 2007‚Äì2018. Accessed: 2025-09-20.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-\npropagating errors. nature, 323(6088):533‚Äì536, 1986.\nJ. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81‚Äì106, 1986.\nKevin Ren, Santiago Cortes-Gomez, Carlos Miguel PatiÀúno, Ananya Joshi, Ruiqi Lyu, Jingjing\nTang, Alistair Turcan, Khurram Yamin, Steven Wu, and Bryan Wilder.\nPredicting lan-\nguage models‚Äôsuccess at zero-shot probabilistic prediction.\n2025.\nURL https://api.\nsemanticscholar.org/CorpusID:281411477.\nQianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao,\nDong Li, Yongfu Dai, Duanyu Feng, et al. Finben: A holistic financial benchmark for large\nlanguage models. Advances in Neural Information Processing Systems, 37:95716‚Äì95743, 2024.\nStefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David\nSontag. Tabllm: Few-shot classification of tabular data with large language models. In Interna-\ntional conference on artificial intelligence and statistics, pages 5549‚Äì5581. PMLR, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877‚Äì1901, 2020.\nLloyd S Shapley et al. A value for n-person games. 1953.\nErik ÀáStrumbelj and Igor Kononenko. Explaining prediction models and individual predictions with\nfeature contributions. Knowledge and information systems, 41(3):647‚Äì665, 2014.\nRoni Goldshmidt and Miriam Horovicz. Tokenshap: Interpreting large language models with monte\ncarlo shapley value estimation. arXiv preprint arXiv:2407.10114, 2024.\nKenza Amara, Rita Sevastjanova, and Mennatallah El-Assady. Syntaxshap: Syntax-aware explain-\nability method for text generation. arXiv preprint arXiv:2402.09259, 2024.\nStan Lipovetsky and Michael Conklin. Analysis of regression in game theory approach. Applied\nstochastic models in business and industry, 17(4):319‚Äì330, 2001.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu,\nJialu Liu, Donald Metzler, et al. Large language models are effective text rankers with pairwise\nranking prompting. arXiv preprint arXiv:2306.17563, 2023.\nYinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vuli¬¥c, Anna Korhonen, and Nigel\nCollier.\nAligning with human judgement: The role of pairwise preference in large language\nmodel evaluators. arXiv preprint arXiv:2403.16950, 2024.\nMark A Hall. Correlation-based feature selection for machine learning. PhD thesis, The University\nof Waikato, 1999.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\n"}, {"page": 13, "text": "Preprint\nOpenAI. Gpt-4.1 mini ‚Äî openai api, 2024. URL https://platform.openai.com/docs/\nmodels/gpt-4.1-mini.\nGoogle DeepMind. Gemini 2.5 pro ‚Äî model card. Technical report, June 2025. URL https://\nstorage.googleapis.com/model-cards/documents/gemini-2.5-pro.pdf.\nAntonio Bella, C`esar Ferri, Jos¬¥e Hern¬¥andez-Orallo, and Mar¬¥ƒ±a Jos¬¥e Ram¬¥ƒ±rez-Quintana. Calibration\nof machine learning models. In Handbook of Research on Machine Learning Applications and\nTrends: Algorithms, Methods, and Techniques, pages 128‚Äì146. IGI Global Scientific Publishing,\n2010.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad\nGhassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii,\na freely accessible critical care database. Scientific data, 3(1):1‚Äì9, 2016.\nA\nTHEORY AND ALGORITHM\nA.1\nDETAILED ALGORITHM\nGuided by the Shapley additivity property (Property 1), PRISM estimates each factor‚Äôs marginal\neffect via paired contrasts (realized vs. baseline value of that factor), averages these effects over\nrandomly sampled contexts, and then reconstructs the model output, finally mapping to a calibrated\nprobability.\nSetup. Let I = {1, . . . , m} index factors, x = (x1, . . . , xm) be the instance, b = (b1, . . . , bm) be\ndesignated baselines , f be an evaluation oracle that returns either a probability in [0, 1] or a logit in\nR when only a subset of factors is revealed, and Œ†i be a distribution over background sets S ‚äÜI\\{i}.\nGiven S, we write xS for the partial specification that reveals {xj : j ‚ààS}.\nAlgorithm 1 PRISM (Probability Reconstruction via Shapley Measures)\nInput: Instance x ‚ààX m; oracle f; sampling distributions {Œ†i}m\ni=1; budget K.\nOutput: Probability ÀÜp(x) and factor attributions {ÀÜœïi(x)}.\n1: œï0 ‚Üêf(x‚àÖ)\n2: for i ‚ààI do\n3:\n‚àÜ‚Üê0\n4:\nfor k = 1 to K do\n5:\nSample S ‚àºŒ†i\n6:\n‚àÜ‚Üê‚àÜ+\n\u0000f(xS‚à™{i}) ‚àíf(xS)\n\u0001\n7:\nÀÜœïi(x) ‚Üê‚àÜ/K\n8: ÀÜz(x) ‚Üêœï0 + P\ni ÀÜœïi(x),\nÀÜp(x) ‚ÜêœÉ(ÀÜz(x))\n9: return ÀÜp(x) and {ÀÜœïi(x)}\nExplanation for Alg. 1 . (1) Query the oracle on the empty specification to obtain the intercept\nœï0 = f(x‚àÖ). (2‚Äì3) Start looping over factors i ‚ààI and initialize the accumulator ‚àÜ‚Üê0 for factor\ni. (4‚Äì6) For k = 1, . . . , K, draw a background set S ‚àºŒ†i and accumulate the presence/absence\ncontrast f(xS‚à™{i}) ‚àíf(xS) into ‚àÜ. (7) Average the K contrasts to estimate the contribution of factor\ni: ÀÜœïi(x) ‚Üê‚àÜ/K. (8) Reconstruct the score by additivity, ÀÜz(x) = œï0 + P\ni ÀÜœïi(x), and map through\nthe logistic link to get the probability ÀÜp(x) = œÉ(ÀÜz(x)). (9) Return the probability and the per-factor\nattributions, i.e., ÀÜp(x) and {ÀÜœïi(x)}.\nExplanation for Alg. 2 . (1) Obtain the intercept by querying f(x‚àÖ) so œï0 = f(x‚àÖ). (2‚Äì3) For\neach factor i, sample K background contexts S(k)\ni\n‚àºŒ†i to form the evaluation batches. (4) In each\nsampled context, evaluate a realized/baseline pair in batch: f(xS(k)\ni ‚à™{i} | i := xi) and f(xS(k)\ni ‚à™{i} | i :=\nbi). (5) Average the K realized‚Äìbaseline differences to obtain ÀÜœïi(x) =\n1\nK\nPK\nk=1\n\u0000f(xS(k)\ni ‚à™{i} | i :=\nxi) ‚àíf(xS(k)\ni ‚à™{i} | i := bi)\n\u0001\n. (6) Reconstruct the score ÀÜz(x) = œï0 + P\ni ÀÜœïi(x) and apply the logistic link\nto produce ÀÜp(x) = œÉ(ÀÜz(x)). (7) Return ÀÜp(x) together with the attributions {ÀÜœïi(x)}.\n"}, {"page": 14, "text": "Preprint\nAlgorithm 2 Tabular-PRISM (Batched realized vs. baseline contrasts)\nInput: Instance x ‚ààX m; oracle f; designated baselines b; sampling distributions {Œ†i}m\ni=1; budget\nK.\nOutput: Probability ÀÜp(x) and factor attributions {ÀÜœïi(x)}.\n1: œï0 ‚Üêf(x‚àÖ)\n2: for i ‚ààI do\n3:\nFor k = 1, . . . , K, sample S(k)\ni\n‚àºŒ†i\n4:\nBatch-Query: evaluate all pairs\n\b\nf(xS(k)\ni ‚à™{i}|i:=xi), f(xS(k)\ni ‚à™{i}|i:=bi)\n\tK\nk=1\n5:\nÀÜœïi(x) ‚Üê1\nK\nPK\nk=1\n\u0010\nf(xS(k)\ni ‚à™{i}|i:=xi) ‚àíf(xS(k)\ni ‚à™{i}|i:=bi)\n\u0011\n6: ÀÜz(x) ‚Üêœï0 + P\ni ÀÜœïi(x),\nÀÜp(x) ‚ÜêœÉ(ÀÜz(x))\n7: return ÀÜp(x) and {ÀÜœïi(x)}\nA.2\nTHEOREM PROOF\nProposition 1. Fix an instance x and a single reference sample r. Let œï(r)\ni\nbe the Shapley value in\nDefinition 2. Then, for models with deterministic scalar outputs, with œï(r)\n0 = vr(‚àÖ), we still have:\nf(x) = vr(I) = œï(r)\n0 +\nm\nX\ni=1\nœï(r)\ni\n(7)\nProof. We use the permutation form of the Shapley value, which is equivalent to the subset form.\nLet œÄ be a permutation of I, and let Œ†(I) be the set of all m! permutations. For a permutation œÄ and\nan index i, let Prei(œÄ) denote the set of features that appear before i in œÄ. The permutation form of\nthe Shapley value of i for the game vr(S) = f\n\u0000[xS, r¬ØS]\n\u0001\ncan be written as\nœï(r)\ni\n= 1\nm!\nX\nœÄ‚ààŒ†(I)\n\u0010\nvr\n\u0000Prei(œÄ) ‚à™{i}\n\u0001\n‚àívr\n\u0000Prei(œÄ)\n\u0001\u0011\n.\n(8)\nTaking equation (8) to the Pm\ni=1 œï(r)\ni\npart yields\nm\nX\ni=1\nœï(r)\ni\n= 1\nm!\nX\nœÄ‚ààŒ†(I)\nm\nX\ni=1\n\u0010\nvr\n\u0000Prei(œÄ) ‚à™{i}\n\u0001\n‚àívr\n\u0000Prei(œÄ)\n\u0001\u0011\n.\n(9)\nFor a fixed permutation œÄ = (œÄ1, œÄ2, . . . , œÄm) ‚ààŒ†(I), we have Prei(œÄ) ‚à™{i} = Prei+1(œÄ) for\n1 ‚â§i ‚â§m ‚àí1. Therefore\nm\nX\ni=1\n\u0010\nvr\n\u0000Prei(œÄ) ‚à™{i}\n\u0001\n‚àívr\n\u0000Prei(œÄ)\n\u0001\u0011\n= vr(I) ‚àívr(‚àÖ).\n(10)\nSubstituting equation (10) into (9) gives\nm\nX\ni=1\nœï(r)\ni\n= 1\nm!\nX\nœÄ‚ààŒ†(I)\n\u0000vr(I) ‚àívr(‚àÖ)\n\u0001\n= vr(I) ‚àívr(‚àÖ).\n(11)\nFinally, given œï(r)\n0 := vr(‚àÖ) and vr(I) = f\n\u0000[xI, r‚àÖ]\n\u0001\n= f(x), we have\nf(x) = vr(I) = œï(r)\n0 +\nm\nX\ni=1\nœï(r)\ni .\n(12)\nProof complete.\n"}, {"page": 15, "text": "Preprint\nB\nADDITIONAL DETAILS FOR EXPERIMENTS\nB.1\nDATASETS\nStroke: The stroke prediction dataset contains health, demographic, and lifestyle information for\n5,110 patients, with the goal of predicting stroke occurrence. Each record includes variables such as\nage, hypertension, heart disease, marital status, work type, body mass index (BMI) and so on.\nHeart Disease: The heart disease dataset integrates multiple clinical heart disease datasets and\ncontains 918 patient records with 11 features related to demographics, clinical measurements, and\nlifestyle factors. The target variable indicates the presence or absence of heart disease.\nAdult Census: The adult census contains 48,842 records from the 1994 U.S. Census, with 14 de-\nmographic and employment-related features such as age, education, occupation, work hours, and\nmarital status. The target variable indicates whether an individual‚Äôs annual income exceeds $50,000.\nLending: The lending dataset contains peer-to-peer loans issued from 2007‚Äì2018, with borrower\nand loan features such as income, debt-to-income ratio, FICO score, interest rate, loan amount, and\npurpose. The target variable indicates whether the loan will default.\nBefore applying above datasets for evaluation, we preprocess the datasets as follows:\n(1) In order to make LLMs better understand the datasets, we rename the names of columns of\ndatasets. For example, for stroke dataset, we rename ‚Äúavg glucose level‚Äù as ‚Äúaverage glucose level‚Äù\nand rename ‚Äúbmi‚Äù as ‚ÄúBody Mass Index‚Äù.\n(2) For some columns, the values may be some abbreviation or with unclear meanings. For example,\nthe values of attribute ‚Äúgender‚Äù in dataset stroke are {0, 1}, so we convert ‚Äú0‚Äù into ‚ÄúFemale‚Äù and\n‚Äú1‚Äù into ‚ÄúMale‚Äù. Besides, for attribute ‚ÄúChest Pain Type‚Äù in dataset heart disease, the values are\nabbreviations such as ‚ÄúATA‚Äù, ‚ÄúNAP‚Äù. We also convert them into their full names, e.g., ‚ÄúATA‚Äù ‚Üí\n‚ÄúAtypical Angina‚Äù and ‚ÄúNAP‚Äù ‚Üí‚ÄúNon-Anginal Pain‚Äù.\n(3) We dropped some attributes of the datasets so that them LLMs can evaluation each data point\nwith more efficiency.\nThe final data points examples and columns of each dataset are summarized in Figure (8).\nAge: 26\nWork Class: Private\nEducation Level: Bachelors\nMarital Status: Married-Civ-Spouse\nOccupation: Other-Service\nCapital Gain: 0\nCapital Loss: 0\nWorking Hours Per Week: 45\nNative Country: United-States\n(a) Adult Census\nAge: 52\nSex: Male\nChest Pain Type: Asymptomatic\nResting Blood Pressure: 170\nSerum Cholesterol: 223\nFasting Blood Sugar: < 120 mg/dl\nResting Electrocardiogram: Normal\nMax Heart Rate: 126\nExercise Induced Angina: Yes\nST Segment Depression: 1.5\nST Segment Slope: Flat\n(b) Heart Disease\nGender: Male\nAge: 55\nHypertension: No\nHeart Disease: No\nMarital Status: Ever Married\nWork Type: Govt_Job\nResidence Type: Urban\nAverage Glucose Level: 154.03\nBody Mass Index (BMI): 31.6\nSmoking Status: Smokes\n(c) Stroke\nLoan Amount: 32000\nTerm: 60\nEmploy Years: 1\nHome Ownership: Rent\nAnnual Income: 135000\nInterest Rate: 19.4\nInquiry: <=2\nDelinquency: 1\nPurpose: Credit Card\nDebt To Income Ratio: 0.06\nRevolving Util Ratio: 0.43\nFico Score: 660-680\n(d) Lending\nFigure 8: Data examples of the datasets.\nB.2\nBASELINE METHODS\n1/5/10shot level: We perform multiple shots or multiple trials on directly asking LLM for proba-\nbility as in prompt Figure (13). The LLM is instructed to select a linguistic probability description\nto represent the probability. Before evaluation, we map the descriptions into numerical values as\nfollows: ‚Äúvery unlikely‚Äù: 0.05, ‚Äúunlikely‚Äù: 0.2, ‚Äúsomewhat unlikely‚Äù: 0.35, ‚Äúneutral‚Äù: 0.5, ‚Äúsome-\nwhat likely‚Äù: 0.65, ‚Äúlikely‚Äù: 0.8, ‚Äúvery likely‚Äù: 0.95. For 1shot level, we only perform one trial;\nfor 5shot level and 10shot level we perform 5 and 10 trials, and select the most common value as\noutput.\n1/5/10shot score: We perform multiple shots similar to 1/5/10shot level. The difference is that we\ninstruct the LLM to output the numerical probability directly, as shown in the prompt Figure (14).\nWe take the average probability as output.\n"}, {"page": 16, "text": "Preprint\nICL-5+5, ICL-10+10: We perform in-context learning on the 4 datasets with available training data.\nThere are 5 positive 5 negative data in ICL-5+5 and 10 negative 10 positive data in ICL-10+10. The\ntraining data are excluding the evaluation data, with large mutual differences from the arbitrary\ncandidate set. The order of the training data inside the prompt is shuffled randomly to minimize\nconfusion. Detailed prompt example is shown as follows 15.\nContrast: We perform two queries in opposite directions. The first query apply the same prompt as\nin 1shot level Figure (13). The second query is trying to ask the question in the opposite way, such\nas ‚ÄùHow likely is this patient to NOT have a stroke?‚Äù instead of ‚ÄùHow likely is this patient to have a\nstroke?‚Äù. Two queries are normalized so that they sum up to 1, and the normalized positive answer\nwill be the output.\nBIRD: We follow BIRD‚Äôs method with slight modification. First, since BIRD could not handle\nnumerical datatype features, those features will be turned into bins before treated as input. In order\nto better utilize prior knowledge of LLM, binning will base on prior evidence that extract the most\ncharacteristic of the stage, therefore the interval of each bin may not have the same length. For ex-\nample, regarding to the numerical feature ‚ÄúResting Blood Pressure‚Äù in the Heart Disease dataset, the\nbins are: Normal ‚Äò80-120‚Äô, Pre-hypertension ‚Äò120-130‚Äô, Hypertension Stage 1 ‚Äò130-140‚Äô, Hyper-\ntension Stage 2 ‚Äò140-180‚Äô, Hypertensive crisis ‚Äò180-200‚Äô. Second, instead of probability mapping\nin BIRD {‚Äúfj supports outcome i‚Äù: 75%, ‚Äúfj is neutral‚Äù: 50%, ‚Äúfj supports opposite outcome ¬¨i‚Äù:\n25%}, we are using a denser mapping which is the same as introduced in 1shot level.\nIn general, the baseline BIRD is performed in 4 steps: (1) Initializing the prediction probability\ngiven each independent feature, by querying LLM using similar prompt as in 1shot level Figure\n(13), only replace ‚ÄúPerson information: ...‚Äù by ‚ÄúGiven that [factor] = [value]‚Äù. (2) Generating\nstochastic training data, by randomly choice a bin for each feature, the prompt is the same as in\n1shot level Figure (13). (3) Training the BIRD constrained optimization method. (4) Inferring to\nthe evaluation dataset.\nB.3\nREFERENCE INSTANCES SETTINGS\nWe instantiate a per-dataset reference instance xref that serves as the baseline context for PRISM‚Äôs\ncontrastive evaluations. Reference values are chosen to be representative, i.e., close to the empirical\nmean for continuous variables and a prevalent category for discrete variables (the mode for multi-\nclass features and the negative category for binary features). The concrete instances used in our\nexperiments are listed in Table 6.\nFor each dataset‚Äôs reference instance,, we use prompt 14, query the model five times, average the\npredicted probabilities, and convert the result to a base logit via logit(p) = log\n\u0000p\n1‚àíp\n\u0001\n. The resulting\nbase probabilities and logits are reported in Table 7.\nC\nADDITIONAL RESULTS\nC.1\nEXAMINATION OF INTERPRETATIONS\nWe examine factor-level attributions on the tabular datasets in Table 8, Table 9, and Table 10, and\non the text scenarios in agriculture (Figure 9, Figure 10) and soccer (Figure 11).\nAdult (Table 8). The attributions align with well-known socio-economic regularities for predicting\nincome. Education exerts the largest and most consistent influence: Masters yields a strong positive\ncontribution (Case 4: +1.31), while HS-grad is negative (Case 3: ‚àí0.45). Occupational roles are\nsimilarly informative: Exec-managerial is positive (Cases 1/4: +0.49/ + 0.99), whereas Farming-\nfishing is negative (Case 3: ‚àí0.84). Capital gain is highly predictive when present (Case 1: +1.04),\nand age contributes moderately with the expected direction (older age increasing odds in Cases 1/3).\nMarital status exhibits negative impacts for Divorced and Never-married (Cases 1/2), consistent\nwith prior findings that marriage correlates with higher income. Some variables (e.g., workclass,\nnative country) show near-zero effects in these cases, indicating either proximity to the anchor or\nlow marginal power after conditioning on stronger factors.\nHeart (Table 9). For heart disease risk, the factor impacts align with real-world clinical patterns\nExercise-induced angina is strongly positive (Cases 1/2: +0.89/ + 0.87), and elevated resting blood\n"}, {"page": 17, "text": "Preprint\nStroke\nGender\nMale\nAge\n40.0\nHypertension\nNo\nHeart disease\nNo\nMarital status\nNever Married\nResidence type\nRural\nAverage glucose level\n90.0\nBody Mass Index (BMI)\n24.0\nWork type\nPrivate\nSmoking status\nnever smoked\nAdult\nAge\n40\nWorkclass\nPrivate\nEducation level\nSome-college\nMarital status\nMarried-civ-spouse\nOccupation\nSales\nCapital gain\n0\nCapital loss\n0\nWorking hours per week\n40\nNative country\nUnited-States\nLoan\nLoan amount\n20000\nTerm\n36\nEmploy years\n3\nHome ownership\nOWN\nAnnual income\n60000\nInterest rate\n14.0\nPurpose\ncar\nDebt-to-income ratio\n0.35\nRevolving util ratio\n0.30\nFICO score\n680‚Äì710\nInquiry\n‚â§2\nDelinquency\n0\nHeart Disease\nAge\n53\nResting Blood Pressure\n133\nSerum Cholesterol\n212\nMax Heart Rate\n137\nST Segment Depression\n0.8\nSex\nMale\nChest Pain Type\nAsymptomatic\nFasting Blood Sugar\n< 120 mg/dl\nResting Electrocardiogram\nNormal\nExercise Induced Angina\nNo\nST Segment Slope\nFlat\nTable 6: Reference instance of each dataset.\nDataset\np (mean over 5 runs)\nlogit\nStroke\n0.001\n-6.9068\nAdult\n0.354\n-0.6015\nHeart Disease\n0.410\n-0.3640\nLoan\n0.182\n-1.5029\nTable 7: Base probabilities p (from prompt 14, averaged over five queries) and corresponding base\nlogits per dataset.\npressure and serum cholesterol contribute positively (Case 1: +0.43/ + 0.45; Case 2: +0.38/ + 0.29).\nConversely, higher max heart rate and upsloping ST slope decrease risk (Case 3: ‚àí0.45 and ‚àí0.48),\naligning with cardiology practice that better exercise capacity and non-flat slopes are protective.\nSex=Female is negative in Case 3 (‚àí0.53), capturing lower risk in females. Note that the same\n"}, {"page": 18, "text": "Preprint\nFactor\nCase 1\nCase 2\nCase 3\nCase 4\nValue\nShapley\nValue\nShapley\nValue\nShapley\nValue\nShapley\nAge\n51\n0.17\n25\n-0.41\n47\n0.38\n38\n0.00\nWorkclass\nPrivate\n0.00\nPrivate\n0.00\nSelf-emp-not-inc\n-0.44\nPrivate\n0.00\nEducation level\nBachelors\n0.62\nBachelors\n0.49\nHS-grad\n-0.45\nMasters\n1.31\nMarital status\nDivorced\n-0.47\nNever-married\n-0.41\nMarried-civ-spouse\n0.00\nMarried-civ-spouse\n0.00\nOccupation\nExec-managerial\n0.49\nSales\n0.00\nFarming-fishing\n-0.84\nExec-managerial\n0.99\nCapital gain\n10520\n1.04\n0\n0.00\n0\n0.00\n0\n0.00\nCapital loss\n0\n0.00\n1876\n0.00\n0\n0.00\n0\n0.00\nWorking hours/wk\n40\n0.00\n40\n0.00\n60\n0.54\n60\n0.53\nNative country\nUS\n0.00\nUS\n0.00\nUS\n0.00\nUS\n0.00\nSum Shapley\n1.86\n-0.33\n-0.81\n2.83\nSum logit\n1.26\n-0.93\n-1.41\n2.22\nPred prob\n0.778\n0.283\n0.197\n0.902\nTrue label\nYes\nNo\nNo\nYes\nTable 8: Shapley values for four instances in Adult dataset. Reference instance: age=40; work-\nclass=Private; education=Some-college; marital status=Married-civ-spouse; occupation=Sales;\ncapital gain=0; capital loss=0; working hours=40; native country=US. A single base logit is shared\nacross cases, œï0 = œÉ‚àí1(0.354) = ‚àí0.6015.\nFactor\nCase 1\nCase 2\nCase 3\nCase 4\nValue\nShapley\nValue\nShapley\nValue\nShapley\nValue\nShapley\nAge\n52\n0.00\n58\n0.00\n34\n-0.73\n48\n-0.12\nSex\nMale\n0.00\nMale\n0.00\nFemale\n-0.53\nMale\n0.00\nChest Pain Type\nAsymptomatic\n0.00\nNon-Anginal Pain\n0.70\nAtypical Angina\n0.42\nAsymptomatic\n0.00\nResting Blood Pressure\n170\n0.43\n150\n0.38\n118\n-0.49\n132\n0.00\nSerum Cholesterol\n223\n0.45\n219\n0.29\n210\n0.00\n272\n0.37\nFasting Blood Sugar\n< 120 mg/dl\n0.00\n< 120 mg/dl\n0.00\n< 120 mg/dl\n0.00\n< 120 mg/dl\n0.00\nResting ECG\nNormal\n0.00\nST-T abn.\n0.33\nNormal\n0.00\nST-T abn.\n0.45\nMax Heart Rate\n126\n0.19\n118\n0.00\n192\n-0.45\n139\n0.00\nExercise Induced Angina\nYes\n0.89\nYes\n0.87\nNo\n0.00\nNo\n0.00\nST Segment Depression\n1.5\n0.44\n0.0\n-0.59\n0.7\n0.00\n0.2\n-0.44\nST Segment Slope\nFlat\n0.00\nFlat\n0.00\nUpsloping\n-0.48\nUpsloping\n-0.44\nSum Shapley\n2.40\n1.99\n-2.25\n-0.63\nSum logit\n2.04\n1.62\n-2.61\n-0.54\nPred prob\n0.885\n0.835\n0.068\n0.367\nTrue label\nYes\nYes\nNo\nNo\nTable 9:\nShapley values for four instances in Heart dataset.\nReference instance:\nAge=53;\nResting BP=133; Serum Chol.=212; Max HR=137; ST Depression=0.8; Sex=Male; Chest\nPain=Asymptomatic; Fasting Blood Sugar=<\n120 mg/dl; Resting ECG=Normal; Exercise\nAngina=No; ST Slope=Flat. A single base logit is shared across cases, œï0 = œÉ‚àí1(0.41) = ‚àí0.363.\nfeature can change sign across cases (e.g., ST depression: ‚àí0.59 in Case 2 vs. near-zero/positive\nelsewhere), reflecting interactions and the conditional nature of œïi(x) under different covariate set-\ntings.\nLoan (Table 10). For default risk, the patterns are intuitive. High interest rate increases risk (Cases\n3/4: +0.67/ + 0.71), while lower rates reduce it (Case 1: ‚àí0.50). Past delinquency is the single\nmost influential positive factor when present (Case 3: +1.15). Home ownership=RENT and lower\nannual income tend to raise risk (Case 3: +0.44/+0.48), while higher FICO mitigates risk and lower\nFICO elevates it (Case 1: ‚àí0.41 vs. Case 3: +0.45). Debt burden is captured by DTI and revolving\nutilization: lower values reduce risk (Case 1: ‚àí0.08 and ‚àí0.51), whereas moderate-to-high levels\nare less favorable across other cases.\nAcross these tabular datasets, predicted probabilities exhibit a label-consistent ordering with clear\nseparation‚Äîfor example, the positives cluster at higher values while the negatives fall into interme-\ndiate and low ranges‚Äîyielding an easily interpretable ranking. Per-instance factor impacts make the\ndrivers of each estimate explicit and support auditability: dominant contributors can be inspected,\nimplausible probabilities traced to specific factors, and questionable cases flagged for review.\nAgriculture (Figure 9, Figure 10). For Honeycrisp, Production contributes positively because a\nforecast output decline tightens supply, and this effect becomes stronger when Storage carryover\nis elevated and Market demand is soft. Costs add a small positive push as labor and inputs remain\nhigh, while Varietal competition subtracts; Government policy offers limited support. Together these\neffects yield a predicted probability of 0.51 for a price increase. For Granny Smith, the signs invert:\nan expected Production increase drives a negative contribution, which becomes larger under Imports\n& exports pressure and aggressive Promotional activity. The small positive Government policy term\ndoes not offset oversupply, and Costs together with Varietal competition further weigh on price.\nIn this setting the predicted probability for a price increase is 0.09. Across the two varieties, these\nattributions accord with well-known agricultural price formation regularities in which supply shocks\n"}, {"page": 19, "text": "Preprint\nFactor\nCase 1 (ID=1)\nCase 2 (ID=2)\nCase 3 (ID=29)\nCase 4 (ID=37)\nValue\nShapley\nValue\nShapley\nValue\nShapley\nValue\nShapley\nLoan amount\n30000\n0.45\n1500\n0.00\n10000\n-0.38\n30000\n0.48\nTerm\n60\n0.45\n36\n0.00\n36\n0.00\n60\n0.51\nEmploy years\n4\n-0.43\n< 1\n0.45\n6\n-0.43\n10+\n-0.55\nHome ownership\nMORTGAGE\n0.00\nMORTGAGE\n0.00\nRENT\n0.44\nMORTGAGE\n0.42\nAnnual income\n65000\n-0.27\n55000\n0.00\n48000\n0.48\n42000\n0.46\nInterest rate\n12.0\n-0.50\n10.4\n-0.43\n20.0\n0.67\n19.4\n0.71\nInquiry\n‚â§2\n0.00\n‚â§2\n0.00\n‚â§2\n0.00\n‚â§2\n0.00\nDelinquency\n0\n0.00\n0\n0.00\n1\n1.15\n0\n0.00\nPurpose\nDebt cons.\n0.43\nHome improv.\n0.00\nDebt cons.\n0.44\nDebt cons.\n0.48\nDebt-to-income ratio\n0.25\n-0.08\n0.13\n-0.55\n0.29\n0.00\n0.17\n0.00\nRevolving util ratio\n0.21\n-0.51\n0.29\n0.00\n0.08\n-0.47\n0.44\n0.00\nFICO score\n710‚Äì740\n-0.41\n710‚Äì740\n-0.77\n660‚Äì680\n0.45\n660‚Äì680\n0.42\nSum Shapley\n-0.87\n-1.30\n2.35\n2.93\nSum logit\n-2.37\n-2.81\n0.85\n1.43\nPred prob\n0.086\n0.057\n0.701\n0.806\nTrue label\nNo\nNo\nYes\nYes\nTable 10:\nShapley values for four instances in Loan dataset.\nReference instance:\nLoan\namount=20000; Term=36; Employ years=3; Home ownership=OWN; Annual income=60000;\nInterest rate=14.0; Purpose=car; Debt-to-income ratio=0.35; Revolving util ratio=0.30; FICO\nscore=680‚Äì710; Inquiry=‚â§2; Delinquency=0.\nA single base logit is shared across cases,\nœï0 = œÉ‚àí1(0.182) = ‚àí1.504.\nand carryover inventory dominate short-run price movements, moderated by demand conditions,\npolicy, and competing varieties.\nSoccer (Figure 11). PRISM decomposes the match into conditional cues. Home advantage and\nSquad quality contribute positively (the latter is the largest, reflecting City‚Äôs deeper roster), while\nHead-to-head contributes negatively (City have been troubled by Spurs) and Player availability and\nfitness is strongly negative given absences in midfield and doubts in key positions; Recent form\nis mildly positive and External conditions are neutral. These effects interact: the benefit of home\nadvantage weakens when midfield anchors are missing, and the head-to-head penalty matters more\nwhen overall form is mixed. Balancing these factors yields a PRISM predicted probability of 0.44 for\na City win at home (vs. 0.65 from direct LLM scoring), and the realized outcome‚ÄîCity lost‚Äîaligns\nmore closely with PRISM‚Äôs assessment.\nAcross agriculture and soccer, PRISM‚Äôs factor impacts are conditional rather than global: identical\nfeatures switch sign or magnitude as the surrounding evidence changes. This conditionality captures\ninteraction structure among drivers, clarifies why conclusions can differ across otherwise similar\nfactor sets, and yields interpretable, context-aware attributions aligned with domain regularities.\nC.2\nCALIBRATION ANALYSIS\nSetup. We assess calibration by plotting the weighted reliability curve. Given predictions {ÀÜpi} and\nlabels {yi ‚àà{0, 1}}, we bin by equal-count quantiles of ÀÜp and compute in-bin weighted means\nÀÜpm =\n1\nWm\nX\ni‚ààBm\nwiÀÜpi,\nÀÜym =\n1\nWm\nX\ni‚ààBm\nwiyi,\nWm =\nX\ni‚ààBm\nwi.\n(13)\nWeights correct the gap between the evaluation split and the deployment/population class mix:\nwi =\nÔ£±\nÔ£¥\nÔ£≤\nÔ£¥\nÔ£≥\nœÄ\nÀÜœÄ ,\nyi = 1,\n1 ‚àíœÄ\n1 ‚àíÀÜœÄ ,\nyi = 0,\nECE =\nM\nX\nm=1\nWm\nP\nj Wj\n|ÀÜpm ‚àíÀÜym|.\n(14)\nHere, ÀÜœÄ is the positive rate in our evaluation split, which is class-balanced (ÀÜœÄ = 0.5). œÄ is the popula-\ntion positive rate we aim to evaluate against. Since the true deployment prevalence is unknown, we\nuse the original dataset prevalence before balancing as a proxy for œÄ: Stroke 4.87%, Adult Census\n24.88%, Heart Disease 55.28%. (If a practitioner knows their deployment prevalence, they can plug\nit in for œÄ.) We visualize calibration by plotting (ÀÜpm, ÀÜym) against the identity line y = x.\nWhy weighting matters. Our test splits are 1:1 balanced, but real-world prevalence is typically\nskewed. Without weighting (wi ‚â°1), bin positive fractions reflect the artificial 50% mix rather than\n"}, {"page": 20, "text": "Preprint\n*2025 Production** (Shapley value = 1.76)\n- **Estimated Output:** For the 2024/25 crop year, U.S. Honeycrisp production is forecast at **27.7 million bushels** (42-lb \nbushels), down from 33.6 million in 2023/24. This is a significant decrease (~17%), likely a correction after last year‚Äôs record \ncrop.\n- **Share of Total:** Honeycrisp will account for about **9.8% of total U.S. apple production**, making it the fourth largest \nvariety after Gala, Red Delicious, and Granny Smith.\n- **Trend:** Over the last five years, Honeycrisp production has been on the rise, but the 2025 forecast suggests a \nstabilization or slight pullback after a period of rapid expansion.\n*2025 Market Demand** (Shapley value = -0.55)\n- **Retail Price Pressure:** In 2023/24, Honeycrisp retail prices fell sharply (down 16% year-over-year for conventional), \nreflecting oversupply and aggressive discounting.\n- **Premium Erosion:** Despite still commanding a premium over other varieties, the price gap is narrowing as production \nincreases and retailers struggle to move large volumes.\n- **Consumer Demand:** While Honeycrisp remains popular, there are signs that demand is not keeping pace with \nsupply, especially at higher price points. The report notes that ‚Äúretailers are finding it difficult to move that much fruit‚Äù -\n**Promotional Activity:** Heavy promotions in 2023/24 may not be sustainable, and there is concern that volumes sold at \ndiscount may not be repeatable in 2025.\n*2025 Storage** (Shapley value = -1.09)\n- **High Carryover:** In June 2024, Honeycrisp storage holdings were **87% above** the previous year, indicating slow \nmovement and potential for significant carryover into the 2025 marketing year.\n- **Storage Risk:** Large inventories increase the risk of price pressure and quality issues as the season progresses, \nespecially if demand does not accelerate.\n- **Packout Improvements:** Industry-wide, better storage protocols have improved packouts, but for Honeycrisp, this may \nmean even more fruit competing for limited shelf space.\n*2025 Imports and Exports** (Shapley value = 0.0)\n- **Exports:** U.S. apple exports rebounded in 2023/24, but there is no specific mention of Honeycrisp as a major export \nvariety. Most U.S. apple exports are to Mexico, Canada, and Asia, but Honeycrisp‚Äôs high price and quality requirements may \nlimit its export share.\n- **Imports:** Imports of fresh apples are down, and the U.S. is a net exporter. However, imported Honeycrisp is not a \nsignificant factor in the U.S. market.\n- **Trade Policy Impact:** The return of the Indian market (after tariff removal) is positive for U.S. apples, but Honeycrisp is\nnot a major variety for export to India or other recovering markets.\n*2025 Government Policy** (Shapley value = 0.18)\n- **Section 32 Purchases:** In 2023/24, the USDA made large Section 32 purchases to help absorb excess supply, including \n$56 million for fresh apples. Another $20 million buy is expected in fall 2024, which may help with Honeycrisp oversupply if \nincluded.\n- **Labor Policy:** The industry is pushing for H-2A program reform to address high labor costs and shortages, which directly \naffect Honeycrisp growers due to the variety‚Äôs labor-intensive harvest.\n- **No Direct Subsidy:** There is no indication of Honeycrisp-specific government support, but general apple industry \ninterventions (purchases, labor reform) will indirectly benefit Honeycrisp growers.\n*2025 Costs** (Shapley value = 0.26)\n- **Labor:** Labor is the largest cost, now accounting for ~60% of total production costs, and is rising due to H-2A wage \nrates (over $18/hr in top states).\n- **Input Inflation:** Costs for fertilizer, utilities, and nursery trees have all risen sharply (fertilizer up 92% over the decade).\n- **Margin Squeeze:** With retail and farm-gate prices falling and costs rising, many Honeycrisp growers are at risk of \nnegative returns, especially if prices remain below cost of production.\n- **Efficiency Pressure:** Growers are being forced to increase efficiency, but Honeycrisp‚Äôs high input and labor \nrequirements make this challenging.\n*2025 Varietal Competition** (Shapley value = -0.51)\n- **Rising Competition:** New premium varieties like Cosmic Crisp¬Æ (up 3391% in five years), Pink Lady/Cripps Pink, and \nEnvy¬Æ are gaining share and shelf space.\n- **Gala and Red Delicious:** While Gala is declining, it remains the top variety by volume and is rebounding in price, offering\na lower-cost alternative to Honeycrisp.\n- **Premium Segment Crowding:** The premium apple segment is becoming crowded, and Honeycrisp‚Äôs unique selling \nproposition is being challenged by newer, often better-storing, and sometimes cheaper varieties.\n- **Organic Competition:** Organic apples, including Honeycrisp, are holding price better than conventional, but the organic \npremium is also under pressure.\nFigure 9: The factors and interpretations of PRISM for predicting whether the price of Honeycrisp\napple will increase in 2025. Based on the Shapley values, it is finally predicted to have a chance of\n51% to have price increase. Among the factors, ‚ÄúProduction‚Äù has a large positive indication of price\nincrease, as the production is expected to decrease by 17%.\n"}, {"page": 21, "text": "Preprint\n*2025 Production** (Shapley value = -0.74)\n- **Estimated Production:** For the 2024/25 crop year (the most current forecast, which covers the 2025 marketing season), \nU.S. Granny Smith production is projected at **28,252,464 bushels** (42-lb bushels), accounting for about **10% of total \nU.S. apple production**.\n- **Trend:** This is a slight increase from the previous year (27,762,121 bushels in 2023/24) and a notable rise compared \nto the five-year average (24,957,121 bushels). The data suggests Granny Smith is a stable or slightly growing variety in \nthe U.S. mix.\n- **Regional Production:** The vast majority of Granny Smith apples are produced in Washington (25.9 million bushels), with \nsmaller contributions from California, New York, Pennsylvania, and other states.\n*2025 Market Demand** (Shapley value = -0.25)\n- **Retail Price:** The average retail price for conventional Granny Smith apples in 2023/24 was **$1.58/lb**, down 9% from \nthe previous year ($1.73/lb). This price is above Red and Golden Delicious, but below Honeycrisp.\n- **Demand Trends:** The report notes a **long-term trend of declining per capita apple consumption**, especially \namong younger consumers. However, Granny Smith remains a popular variety due to its unique tart flavor and versatility \n(fresh eating, baking, processing).\n- **Promotional Activity:** Aggressive retail promotions in 2023/24 likely boosted sales volume, but such volumes may not be \nsustainable if promotions are rolled back in 2025.\n- **Varietal Position:** Granny Smith is the third most-produced variety, indicating continued strong demand, but faces \nincreasing competition from newer, premium varieties.\n*2025 Storage** (Shapley value = -0.66)\n- **Storage Trends:** The report highlights that storage data is critical for understanding market movement. For Granny \nSmith, as with other major varieties, storage holdings are tracked monthly.\n- **2023/24 Storage:** There was a general trend of slower movement for some varieties (e.g., Honeycrisp), but Granny \nSmith‚Äôs storage movement is not specifically called out as problematic.\n- **2025 Outlook:** With production up and prices down, there is a risk of higher-than-normal storage holdings if demand \ndoes not keep pace, potentially leading to more late-season price pressure.\n*2025 Imports and Exports** (Shapley value = -0.36)\n- **Exports:** The U.S. exported 46.4 million bushels of fresh apples in 2023/24, with Granny Smith being a significant export \nvariety due to its global popularity. Key export markets include Mexico, Canada, Taiwan, and Vietnam.\n- **Imports:** Imports of fresh apples into the U.S. were down 15% in 2023/24, and are expected to remain low in 2025 due to \nhigh domestic supply. Imports of Granny Smith specifically are not broken out, but most imported apples are from Chile, New \nZealand, and Canada.\n- **Trade Balance:** The U.S. maintains a strong positive trade balance in apples, but the strong dollar and competitive \nglobal pricing may limit export profitability for Granny Smith in 2025.\n- **Export Challenges:** The report notes that U.S. exporters may be forced to sell at low margins to maintain market share, \nespecially in price-sensitive markets.\n*2025 Government Policy** (Shapley value = 0.53)\n- **Labor Policy:** The H-2A Temporary Agricultural Worker Program is a major concern. Labor costs are rising, and the \nprogram is described as ‚Äúuntenable, unaffordable, and unsustainable.‚Äù No major reform is expected by 2025, so labor will \nremain a key challenge.\n- **Section 32 Purchases:** In 2023/24, the USDA made large Section 32 purchases to support the market. Another buy is \nexpected in fall 2024, which may help absorb excess supply in 2025 if needed.\n- **Trade Policy:** The removal of Indian tariffs on U.S. apples has reopened that market, but competition from low-cost \nsuppliers (Iran, T√ºrkiye) is fierce. U.S. policy focus is on maintaining and expanding export markets.\n- **Cost Support:** No direct cost support is mentioned for Granny Smith, but general industry support (e.g., for storage, \nexport promotion) benefits all major varieties.\n*2025 Costs** (Shapley value = -0.37)\n- **Production Costs:** Labor is the largest cost, accounting for about 60% of total production costs, and is rising due to \nhigher Adverse Effect Wage Rates (AEWR). For top apple states, AEWR averages $18.22/hr, with California and Washington \neven higher.\n- **Input Costs:** Fertilizer, utilities, and nursery trees have all seen significant cost increases (fertilizer up 92% over the last \ndecade).\n- **Margin Pressure:** While costs are up, grower prices for apples (including Granny Smith) have fallen sharply. In 2023/24, \nGranny Smith tray pack prices fell 45% from August to July.\n- **Profitability:** Many growers are expected to operate at a loss if these trends continue into 2025, especially if retail \nprices remain low and input costs high.\n*2025 Varietal Competition** (Shapley value = -0.42)\n- **Rising Varieties:** Honeycrisp, Cosmic Crisp¬Æ, and Pink Lady/Cripps Pink are gaining market share, often at the \nexpense of older varieties like Red Delicious and, to a lesser extent, Gala.\n- **Granny Smith‚Äôs Position:** While still a top-three variety, Granny Smith faces pressure from these newer, higher-priced \nvarieties, especially in the fresh market.\n- **Utilization:** Granny Smith is primarily a fresh-market apple (77% fresh, 23% processing), but is less likely to command \nthe premium of Honeycrisp or Cosmic Crisp¬Æ.\n- **Organic Competition:** Organic apples command a significant price premium and are holding their value better than \nconventional. Granny Smith is available as organic, but the conventional segment faces more price pressure.\nFigure 10: The factors and interpretations of PRISM for predicting whether the price of Granny\nSmith apple will increase in 2025. Based on the Shapley values, it is finally predicted to have a\nchance of 9% to have price increase. Among the factors, only the ‚ÄúGovernment policy‚Äù is positive.\nIn the negative factors, the production of Granny Smith is expected to increase, so it lowers the\nexpectation of price increase.\n"}, {"page": 22, "text": "Preprint\nHead-to-head:  (Shapley value = -0.41)\nSpurs have troubled City recently ‚Äî City have won only 4 of the last 12 meetings, including a 4-0 \nSpurs win last November and a 2-1 League Cup defeat; City did win 1-0 in February.\nRecent form: (Shapley value = 0.15)\nGood starts for both: City 4-0 vs Wolves, Spurs 3-1 vs Burnley ‚Äî but both will face a sterner test here \nand City‚Äôs poor run last season (six defeats in seven) is a reminder of volatility.\nPlayer availability and fitness: (Shapley value=-0.45)\nCity: Kovacic and Rodri out; Gvardiol and Foden doubts; Rayan Cherki pushing for a start. Spurs: \nKulusevski, Maddison and Dragusin out; Bissouma and Udogie doubtful; Joao Palhinha pushing for a \nstart..\nExternal conditions: (Shapley value=0.0)\nEarly Saturday lunchtime kick-off (typically low-key) but this fixture promises entertainment; the \nfixture computer gave both sides relatively easier openers, so this is a step up and will attract pundit \nattention.\nKey points:\n- Home advantage + overall squad quality favors Man City (Haaland in form; City 4-0 last game).\n- Missing Rodri and Mateo Kovacic weakens City‚Äôs midfield; Gvardiol and Foden doubts introduce \nuncertainty.\n- Tottenham also have significant absences (Kulusevski, Maddison) though Kudus/Richarlison are \nthreats; Palhinha could add steel if he starts.\n- Recent head-to-head is mixed (Spurs have troubled City recently), but on balance City at home are \nstill favorites.\n@@Final probability: 0.65@@\nDirect LLM Prompting predicted probability of Man City wins: 0.65\nPRISM predicted probability of Man City wins: 0.44\nSquad Quality. (Shapley value = 0.47)\nBoth teams boast strong attacking talent ‚Äî City have Haaland (and promising Reijnders) and deeper \noverall quality; Spurs have dangerous forwards in Kudus and Richarlison but their squad was less \nconsistent last season.\nFigure 11: Direct LLM Prompting and PRISM for estimating the probability of ‚ÄúMan City will beat\nTottenham at Man City‚Äôs Home‚Äù. In general, Man City is a stronger team as it has a better squad.\nDirect LLM prompting yields facts similar to the factors used by PRISM, but concludes that Man\nCity are favorites. This suggests that the LLM may rely on impressions to assign stronger teams\nhigher winning probability. For PRISM, it also considers the factor ‚ÄúSquad Qualify‚Äù which favors\nMan City, but other factors such as head-to-head records and player availability lead PRISM give a\nlower winning probability for Man City‚Äî0.44. The result of the match is Man City loses.\n"}, {"page": 23, "text": "Preprint\n(a) Adult (GPT)\n(b) Adult (Gemini)\n(c) Heart (GPT)\n(d) Heart (Gemini)\n(e) Stroke (GPT)\n(f) Stroke (Gemini)\n(g) Loan (GPT)\n(h) Loan (Gemini)\nFigure 12: Calibration (reliability) curves comparing PRISM, 1shot score, 10shot score, and BIRD\non four datasets under GPT-4.1-mini and Gemini-2.5-Pro.\n"}, {"page": 24, "text": "Preprint\nthe population, biasing the curve upward on imbalanced tasks (e.g., Stroke). Importance weights\nre-create the population mix within each bin, so the reliability curve answers the practical question:\n‚Äúgiven this score in deployment, what fraction will be positive?‚Äù Therefore, Reweighting restores\nthe population class mix in each bin, yielding reliability curves that reflect real‚Äìworld deployment\nrather than the artificial test mix.\nResults. Across all four datasets in Fig. 12, PRISM maintains strong calibration, remaining close\nto the y=x line on Adult and Heart, and staying closest to the diagonal on Stroke and Loan de-\nspite small deviations, while consistently yielding a monotonically increasing reliability curve. This\nmonotonicity ensures that higher predicted probabilities always correspond to higher empirical event\nrates (no local reversals), preventing rank inconsistencies and threshold instability that appear in the\nbaselines when the fraction of positives decreases as the mean predicted probability increases, and\nit further shows that our method is better calibrated than the baselines.\nD\nPROMPTS\nYou are an income prediction expert. Estimate the probability that the following person has an annual income\ngreater than $50,000.\nThis person lived in 1994; please base your judgment on the U.S. economic and social context of that year.\nPerson information:\n[personal information in json]\nHow likely is this person to have income >$50,000?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, very likely.\n(a) 1/5/10shot level on Adult Census\nYou are a medical risk assessment expert. Estimate the probability that the following person will have heart\ndisease.\nPerson information:\n[personal information in json]\nHow likely is this patient to have heart disease?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, very likely.\n(b) 1/5/10shot level on Heart Disease dataset\nYou are a medical risk assessment expert. Estimate the probability that the following person will have a stroke.\nPerson information:\n[personal information in json]\nHow likely is this patient to have a stroke?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, very likely.\n(c) 1/5/10shot level on Stroke dataset\nYou are a loan-risk analyst. Estimate the probability that the following applicant will default on their loan.\nPerson information:\n[personal information in json]\nHow likely is this applicant to be a defaulter?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, very likely.\n(d) 1/5/10shot level on Lending dataset\nFigure 13: Prompts for baseline method 1shot level, 5shot level, 10shot level. Where [personal\ninformation in json] is replaced by the actual data in json format.\n"}, {"page": 25, "text": "Preprint\nYou are an income prediction expert. Estimate the probability that the following person has an annual income\ngreater than $50,000.\nThis person lived in 1994; please base your judgment on the U.S. economic and social context of that year.\nPerson information:\n[personal information in json]\nHow likely is this person to have income >$50,000?\nFirst, provide a short explanation of your reasoning.\nThen provide the final result strictly in the format:\n##Final Result##: <a single number between 0 and 1>\n(a) 1/5/10shot score on Adult Census\nYou are a medical risk assessment expert. Estimate the probability that the following person will have heart\ndisease.\nPerson information:\n[personal information in json]\nHow likely is this patient to have heart disease?\nFirst, provide a short explanation of your reasoning.\nThen provide the final result strictly in the format:\n##Final Result##: <a single number between 0 and 1>\n(b) 1/5/10shot score on Heart Disease dataset\nYou are a medical risk assessment expert. Estimate the probability that the following person will have a stroke.\nPerson information:\n[personal information in json]\nHow likely is this patient to have a stroke?\nFirst, provide a short explanation of your reasoning.\nThen provide the final result strictly in the format:\n##Final Result##: <a single number between 0 and 1>\n(c) 1/5/10shot score on Stroke dataset\nYou are a loan-risk analyst. Estimate the probability that the following applicant will default on their loan.\nPerson information:\n[personal information in json]\nHow likely is this applicant to be a defaulter?\nFirst, provide a short explanation of your reasoning.\nThen provide the final result strictly in the format:\n##Final Result##: <a single number between 0 and 1>\n(d) 1/5/10shot score on Lending dataset\nFigure 14: Prompts for baseline method 1shot score, 5shot score, 10shot score. Where [personal\ninformation in json] is replaced by the actual data in json format, and the last line of each prompt is\nthe format guideline.\n"}, {"page": 26, "text": "Preprint\nYou are an income prediction expert. Estimate whether a person in 1994 would have an annual income greater than\n$50K, based on U.S. economic and social context of that year. Please use your own knowledge and the examples below\nas references.\nExamples:\n- Features: [personal information in json]\nLabel: [yes/no]\n- Features: [personal information in json]\nLabel: [yes/no]\n‚Ä¶\nNow predict the following case:\nFeatures: [personal information in json]\nQuestion: How likely is this person to have income $50,000?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, very likely.\n(a) ICL on Adult Census\nYou are a medical risk assessment expert.\nEstimate whether the following person will have a heart disease. Please use your own knowledge and the examples\nbelow as references.\nExamples:\n- Features: [personal information in json]\nLabel: [yes/no]\n- Features: [personal information in json]\nLabel: [yes/no]\n‚Ä¶\nNow predict the following case:\nFeatures: [personal information in json]\nQuestion: How likely is this patient to have a heart disease?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, very likely.\n(b) ICL on Heart Disease dataset\nYou are a medical risk assessment expert. Estimate whether the following person will have a stroke. Please use your\nown knowledge and the examples below as references.\nExamples:\n- Features: [personal information in json]\nLabel: [yes/no]\n- Features: [personal information in json]\nLabel: [yes/no]\n‚Ä¶\nNow predict the following case:\nFeatures: [personal information in json]\nQuestion: How likely is this patient to have a stroke?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, very likely.\n(c) ICL on Stroke dataset\nYou are a loan-risk analyst. Estimate the probability that the following applicant will default on their loan.\nPlease use your own knowledge and the examples below as references.\nExamples:\n- Features: [personal information in json]\nLabel: [yes/no]\n- Features: [personal information in json]\nLabel: [yes/no]\n‚Ä¶\nNow predict the following case:\nFeatures: [personal information in json]\nQuestion: How likely is this applicant to be a defaulter?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, very likely.\n(d) ICL on Lending dataset\nFigure 15: Prompts for baseline method ICL-5+5, ICL-10+10. Where [personal information in json]\nis replaced by the actual data in json format. For ICL-5+5, we have 5 positive and 5 negative training\ndata randomly ordered as Features and Values in the prompt, and similar for ICL-10+10 except we\nhave 10 positive and 10 negative training data.\n"}, {"page": 27, "text": "Preprint\nYou are an income prediction expert. Estimate the probability that the following person has an annual incomeat most\n$50,000.\nThis person lived in 1994; please base your judgment on the U.S. economic and social context of that year.\nPerson information:\n[personal information in json]\nHow likely is this person to have income <=$50,000?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, verylikely.\n(a) Contrast on Adult Census\nYou are a medical risk assessment expert. Estimate the probability that the following person will NOThave heart\ndisease.\nPerson information:\n[personal information in json]\nHow likely is this patient to NOT have heart disease?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, verylikely.\n(b) Contrast on Heart Disease dataset\nYou are a medical risk assessment expert. Estimate the probability that the following person will NOT have a stroke.\nPerson information:\n[personal information in json]\nHow likely is this patient to NOT have a stroke?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, very likely.\n(c) Contrast on Stroke dataset\nYou are a loan-risk analyst. Estimate the probability that the following applicant will NOT default on their loan.\nPerson information:\n[personal information in json]\nHow likely is this applicant to be a non-defaulter?\nFirst, provide a short explanation of your reasoning.\nThen answer with one of: very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, verylikely.\n(d) Contrast on Lending dataset\nFigure 16: Negative prompts for baseline method Contrast. Where [personal information in json]\nis replaced by the actual data in json format. The positive prompt is the same as in 1shot label\nprompt13.\n"}, {"page": 28, "text": "Preprint\n[A Markdown table with 10 pairs of records]\nYour task is to evaluate the likelihood that each person has annual wage over 50K dollars in the United \nStates in 1994.\nFor each person:\n- Conduct a brief analysis.\n- Give a risk score from 1 to 10 (1 = very unlikely, 10 = very likely).\nProvide the final result in the format ##Final Result##: @ID1: 1-10; @ID2: 1-10...\n(a) Tabular-PRISM on Adult Census\n[A Markdown table with 10 pairs of records]\nYour task is to evaluate the likelihood that each person has heart disease.\nFor each person:\n- Conduct a brief analysis.\n- Give a risk score from 1 to 10 (1 = very unlikely, 10 = very likely).\nProvide the final result in the format ##Final Result##: @ID1: 1-10; @ID2: 1-10...\n(b) Tabular-PRISM on Heart Disease dataset\n[A Markdown table with 10 pairs of records]\nYour task is to evaluate the likelihood that each person has stroke.\nFor each person:\n- Conduct a brief analysis.\n- Give a risk score from 1 to 10 (1 = very unlikely, 10 = very likely).\nProvide the final result in the format ##Final Result##: @ID1: 1-10; @ID2: 1-10...v\n(c) Tabular-PRISM on Stroke dataset\n[A Markdown table with 10 pairs of records]\nYour task is to evaluate the likelihood that each loan application will default.\nFor each application:\n- Conduct a brief analysis.\n- Give a risk score from 1 to 10 (1 = very unlikely, 10 = very likely).\nProvide the final result in the format ##Final Result##: @ID1: 1-10; @ID2: 1-10...\n(d) Tabular-PRISM on Lending dataset\nFigure 17: Prompt templates for Tabular‚ÄìPRISM across four datasets. Each panel shows the stan-\ndardized instruction used to score instances from a Markdown table with 10 pairs of records; each\npair comprises a target case and its matched baseline case.\n"}, {"page": 29, "text": "Preprint\n[2024 USApple Report]\nBased on the report, estimate how likely the price of U.S. conventional [Apple variety‚Äôs name] apple \nwill increase in the year 2024/2025. Briefly analyze and provide your answer in: @@Final Result: [0-\n1]@@\n(a) Raw\nBased on the report, comment on the situation of U.S. conventional [Apple variety‚Äôs name] apple, \ncovering the following aspects:\n1. 2025 production. 2. 2025 market demand. 3. 2025 storage. 4. 2025 Imports and exports. 5. 2025 \nGovernment policy. 6. 2025 costs. 7. 2025 varietal Competition. \n[Response from previous prompt]\nBased on the report, estimate how likely the price of U.S. conventional [Apple variety‚Äôs name] apple \nwill increase in the year 2024/2025. Briefly analyze and provide your answer in: @@Final Result: [0-\n1]@@\n(b) Extracted\nWe have information about [Apple variety‚Äôs name] apple this year.\n[Baseline Information]\nQ1. Estimate the likelihood (1-10) that the price in 2024/2025 will be higher than this year.\n- 1 = very unlikely, 10 = very likely.\nQ2. New info: [Target factor from the previous prompt]\nIf the new info directly influences the estimation, give a new score. If no, provide the original \nestimation.\nBriefly explain and provide the final estimations in the format ##Final Result##: @Q1:[1-10] Q2:[1-10]@\nBased on the report, comment on the situation of U.S. conventional [Apple variety‚Äôs name] apple, \ncovering the following aspects:\n1. 2025 production. 2. 2025 market demand. 3. 2025 storage. 4. 2025 Imports and exports. 5. 2025 \nGovernment policy. 6. 2025 costs. 7. 2025 varietal Competition. \n(c) PRISM\nFigure 18: Apple prompts of three experiments: Raw input, Extracted factors, and PRISM.\nWe have information of a football match between [Awayteam] at [Hometeam]'s home.\n[Soccer Report]\nBased on the report, estimate how likely the home team will win the match. Briefly analyze and provide \nyour answer in: @@Final probability: [0-1]@@. \n(a) Raw\n[Soccer Report]\nBased on the report, briefly comment on the situation of this match, covering the following aspects:\n1. Squad Quality. 2. Head-to-head records. 3. Recent form. 4. Player availability and fitness. 5. \nExternal conditions. Provide the results following the format: 1) XXX \\n\\n2) XXX \\n\\n3) XXX \\n\\n4) XXX \n\\n\\n5) XXX\nWe have information of a football match between [Awayteam] at [Hometeam]'s home.\n[Baseline Information]\nQ1. Estimate the likelihood (1-10) that the home team will win the match.\n- 1 = very unlikely, 10 = very likely.\nQ2. New info: [Target factor from the previous prompt]\nIf the new info directly influences the estimation, give a new score. If not, provide the original \nestimation.\nBriefly explain and provide the final estimations in the format ##Final Result##: @Q1:[1-10] Q2:[1-10]@\n(b) PRISM\nFigure 19: Soccer prompts of two experiments: Raw input and PRISM.\n"}]}