{"doc_id": "arxiv:2602.13770", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.13770.pdf", "meta": {"doc_id": "arxiv:2602.13770", "source": "arxiv", "arxiv_id": "2602.13770", "title": "NeuroMambaLLM: Dynamic Graph Learning of fMRI Functional Connectivity in Autistic Brains Using Mamba and Language Model Reasoning", "authors": ["Yasaman Torabi", "Parsa Razmara", "Hamed Ajorlou", "Bardia Baraeinejad"], "published": "2026-02-14T13:32:59Z", "updated": "2026-02-14T13:32:59Z", "summary": "Large Language Models (LLMs) have demonstrated strong semantic reasoning across multimodal domains. However, their integration with graph-based models of brain connectivity remains limited. In addition, most existing fMRI analysis methods rely on static Functional Connectivity (FC) representations, which obscure transient neural dynamics critical for neurodevelopmental disorders such as autism. Recent state-space approaches, including Mamba, model temporal structure efficiently, but are typically used as standalone feature extractors without explicit high-level reasoning. We propose NeuroMambaLLM, an end-to-end framework that integrates dynamic latent graph learning and selective state-space temporal modelling with LLMs. The proposed method learns the functional connectivity dynamically from raw Blood-Oxygen-Level-Dependent (BOLD) time series, replacing fixed correlation graphs with adaptive latent connectivity while suppressing motion-related artifacts and capturing long-range temporal dependencies. The resulting dynamic brain representations are projected into the embedding space of an LLM model, where the base language model remains frozen and lightweight low-rank adaptation (LoRA) modules are trained for parameter-efficient alignment. This design enables the LLM to perform both diagnostic classification and language-based reasoning, allowing it to analyze dynamic fMRI patterns and generate clinically meaningful textual reports.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.13770v1", "url_pdf": "https://arxiv.org/pdf/2602.13770.pdf", "meta_path": "data/raw/arxiv/meta/2602.13770.json", "sha256": "dbbbdd503be21c24c9ecb7433be9cf7b9b7e274fb5c33cfb74c7256538fab102", "status": "ok", "fetched_at": "2026-02-18T02:19:17.941220+00:00"}, "pages": [{"page": 1, "text": "NeuroMambaLLM: Dynamic Graph Learning of\nfMRI Functional Connectivity in Autistic Brains\nUsing Mamba and Language Model Reasoning\nYasaman Torabi1,*,†, Parsa Razmara2,*, Hamed Ajorlou3, Bardia Baraeinejad4\nAbstract—Large Language Models (LLMs) have demonstrated\nstrong semantic reasoning across multimodal domains. However,\ntheir integration with graph-based models of brain connectiv-\nity remains limited. In addition, most existing fMRI analysis\nmethods rely on static Functional Connectivity (FC) represen-\ntations, which obscure transient neural dynamics critical for\nneurodevelopmental disorders such as autism. Recent state-\nspace approaches, including Mamba, model temporal structure\nefficiently but are typically used as standalone feature extractors\nwithout explicit high-level reasoning. We propose NeuroMam-\nbaLLM, an end-to-end framework that integrates dynamic latent\ngraph learning and selective state-space temporal modelling with\nLLMs. The proposed method learns the functional connectivity\ndynamically from raw Blood-Oxygen-Level-Dependent (BOLD)\ntime series, replacing fixed correlation graphs with adaptive\nlatent connectivity while suppressing motion-related artifacts\nand capturing long-range temporal dependencies. The resulting\ndynamic brain representations are projected into the embedding\nspace of an LLM model, where the base language model remains\nfrozen and lightweight low-rank adaptation (LoRA) modules are\ntrained for parameter-efficient alignment. This design enables\nthe LLM to perform both diagnostic classification and language-\nbased reasoning, allowing it to analyze dynamic fMRI patterns\nand generate clinically meaningful textual reports. Experiments\non the Autism Brain Imaging Data Exchange (ABIDE) dataset\ndemonstrate up to 72.12% test accuracy, comparable to recent\nstate-of-the-art approaches.\nIndex Terms—Neuroimaging, Large Language Models, Graph\nNeural Networks, Mamba, fMRI, Functional Connectivity\nI. INTRODUCTION\nFunctional Magnetic Resonance Imaging (fMRI) is a cor-\nnerstone of modern neuroimaging, providing a non-invasive\nview of the brain’s functional organization. However, fMRI\ndata is inherently high-dimensional, noisy, and governed by\nnon-linear spatiotemporal dynamics that challenge traditional\nanalysis [1]. Prior neuroimaging studies highlight the sensi-\ntivity of fMRI signals to motion, preprocessing confounds,\nand acquisition variability, while spatial regularization and\n† Corresponding author: torabiy@mcmaster.ca\n* Equal contribution authors\n1 Y. T. is with the Department of Electrical and Computer Engineering,\nMcMaster University, Hamilton, ON L8S 4L8, Canada.\n2 P. R. is with the Department of Biomedical Engineering, University of\nSouthern California, Los Angeles, CA 90089, USA.\n3 H. A. is with the Department of Electrical and Computer Engineering,\nUniversity of Rochester, Rochester, NY 14627, USA.\n4 B. B. is the CEO at the BIOSEN Group, No. 15, Nafisi Street, Ekbatan\nTown, Tehran 1393774535, Iran.\nsystematic clinical analyses underscore the importance of\nreliable neuroimaging findings and translational biomarkers for\nclinical decision-making [2]–[7]. This complexity is especially\npronounced in neurodevelopmental conditions such as Autism\nSpectrum Disorder (ASD). Recent studies show that ASD\ninvolves not only structural connectivity deficits, but also\nabnormal dynamics, including transient hyper-connectivity,\nunstable state transitions, and short-timescale temporal vari-\nability [8]–[10]. As a result, the brain is better modelled as\na time-varying graph with continuously evolving functional\nassociations [11]. Beyond classification accuracy, clinical use\nof fMRI-based models requires interpretability and contextual\nreasoning over neural dynamics. Large Language Models\n(LLMs) enable structured reasoning by mapping learned brain\nfeatures into a semantic space that captures relationships,\ntemporal dependencies, and diagnostic context [12]–[14]. Re-\ncent benchmark studies evaluating structured visual reason-\ning demonstrate that pretrained models encode non-trivial\ncompositional capabilities, and that downstream reasoning\nperformance reveals behaviors not captured by raw accuracy\nmetrics [15], [16]. Recent multimodal zero-shot and training-\nfree time-series reasoning frameworks demonstrate that pre-\ntrained LLMs can generalize across domains when aligned\nwith structured temporal representations [17], [18]. Integrating\nLLM reasoning with dynamic brain graphs moves models\nbeyond black-box predictions toward clinically meaningful\nexplanations grounded in temporally specific patterns. Despite\nthis, current Brain–LLM approaches rely heavily on static\nFunctional Connectivity (sFC). Recent frameworks such as\nFCN-LLM [19] and BrainLLM [20] typically summarize the\nfull scan into a single Pearson correlation matrix, collapsing\ntemporal variability and treating the dynamic brain as static\n[21]. Temporal deep learning models, including Spatiotem-\nporal Graph Convolutional Networks (ST-GCNs) [22], [23],\nspatiotemporal encoder-decoder architectures [24]–[27], and\nTransformers [28], partially address this issue but suffer from\nscalability limits. In particular, the quadratic cost of self-\nattention makes end-to-end modelling of long fMRI sequences\nimpractical without aggressive downsampling or windowing,\nwhich can introduce bias and artifacts [29].\nTo bridge the gap between dynamic neurobiological pro-\ncesses and computational feasibility, we propose NeuroMam-\nbaLLM, a framework that shifts the paradigm from Static\nGraph-to-Text toward Dynamic Latent Signal-to-Text mod-\nelling. Our approach integrates adaptive latent graph inference,\narXiv:2602.13770v1  [eess.IV]  14 Feb 2026\n"}, {"page": 2, "text": "efficient state-space sequence modelling, and large language\nmodel reasoning within a unified architecture. Diffusion-\nbased generative models have similarly been applied for\nlatent representation in high-dimensional systems [30]. First,\nrather than relying exclusively on precomputed correlation\nmatrices or fixed structural priors as in approaches such as\nGraph Diffusion Autoregression (GDAR) [31], we employ\nAdaptive Latent Graph Inference (LGI), a differentiable mod-\nule that learns time-resolved functional connectivity directly\nfrom raw Blood-Oxygen-Level-Dependent (BOLD) signals.\nBy projecting BOLD activity into a latent interaction space,\nLGI captures non-linear dependencies and transient functional\nreorganization that are poorly represented by static correlation\nmetrics [32]. Second, we adopt the Mamba selective state-\nspace architecture to model long-range temporal dependencies\nwhile avoiding the high computational cost associated with\nTransformers [33]. Mamba is a state-space sequence modelling\narchitecture that processes signals in linear time and adaptively\nselects relevant information at each time step. It is particularly\nwell-suited for long and noisy fMRI time series where both\nefficiency and robustness are essential. Unlike recurrent mod-\nels that suffer from forgetting or attention-based models that\nincur quadratic memory growth, Mamba enables linear-time\nsequence modelling through a hardware-aware selective scan\nmechanism. While recent studies have begun applying Mamba\nto physiological signals, these approaches typically employ it\nas a standalone feature extractor rather than as part of an end-\nto-end reasoning framework. Finally, we align the resulting\ndynamic graph representations with the embedding space of\na large language model (LLaMA-3-8B), enabling high-level\nsemantic reasoning over temporally specific neural dynamics.\nIn clinical settings, a black-box prediction of ASD is insuf-\nficient; interpretability and contextual reasoning are essential.\nBy embedding time-resolved brain graph representations into\nthe latent space of the language model, NeuroMambaLLM\nenables diagnostic classification while grounding its outputs\nin causally relevant temporal patterns.\nOur contributions are summarized as follows:\n1) Dynamic Functional Graph Learning: We intro-\nduce an end-to-end dynamic graph encoder that learns\ntime-resolved functional connectivity directly from raw\nBOLD signals using one-dimensional convolution and\nself-attention, capturing transient neural interactions\nmissed by static analyses.\n2) Efficient Temporal Modelling with Mamba: We em-\nploy the Mamba selective state-space architecture to\nefficiently model long-range temporal dependencies in\nfMRI data while adaptively suppressing motion and\nphysiological noise without the quadratic cost of Trans-\nformers.\n3) Large Language Model Reasoning: We propose a\nframework that projects dynamic brain graphs into\nthe embedding space of a frozen LLaMA-3-8B using\nlightweight LoRA modules. This enables joint diag-\nnostic classification and language-based reasoning, pro-\nducing interpretable and clinically meaningful textual\nreports.\nII. RELATED WORK\nA. Large Language Models for Brain Connectivity\nThe integration of LLMs with neuroimaging is a new and\nrapidly expanding field. Early efforts focused on translating\ntext reports to images, but recent works like FCN-LLM [19]\nand BrainLLM [20] have attempted to invert this process,\nusing LLMs to interpret functional connectivity networks\n(FCNs). These models typically employ a projection layer\n(e.g., MLP or Q-Former) to map flattened upper-triangular\ncorrelation matrices into the LLM’s token space. While inno-\nvative, these methods invariably rely on static input matrices.\nBy averaging out temporal dynamics, they discard the richness\nof time, which limits the LLM’s reasoning capabilities to\naverage biomarkers rather than specific temporal events.\nB. Dynamic Functional Connectivity (dFC) Learning\nTo overcome the limitations of static analysis, deep learning\napproaches have increasingly targeted Dynamic Functional\nConnectivity (dFC). Spatiotemporal Graph Convolutional Net-\nworks (ST-GCNs) [22], [23] and Transformer-based models\nlike the Brain Network State Transformer [34] have shown\npromise in capturing time-varying dependencies. However,\nthese methods often rely on sliding-window techniques, which\nintroduce hyperparameter sensitivity (i.e., window size selec-\ntion). Furthermore, methods such as Graph Diffusion Au-\ntoregression (GDAR) [31] rely on fixed structural priors, po-\ntentially missing functional reorganization that deviates from\nanatomical connections. These dFC models operate as black\nboxes, which offer high classification accuracy but they lack\nthe linguistic interpretability required for clinical trust.\nC. State Space Models (Mamba) in Medical Imaging\nThe recently introduced Mamba architecture [33], based\non Selective State Space Models (SSMs), has revolutionized\nlong-sequence modelling by achieving linear computational\ncomplexity. This has led to its rapid adoption in medical\nimaging, including segmentation (SegMamba [35], U-Mamba\n[36]) and classification. In the domain of biosignals, and\nMamba-CAM-Sleep [37] utilize Mamba to process EEG and\nfMRI time series, respectively. However, these works primarily\nuse Mamba as a standalone backbone for feature extraction.\nOur work is the first to integrate Latent Graph Learning\nwith Mamba-LLM alignment, utilizing the state-space model\nnot just for classification, but to create a compressed, time-\nresolved context for high-level diagnostic reasoning.\nIII. METHODOLOGY\nOur framework models brain activity as a sequence of dy-\nnamically evolving functional graphs. The architecture consists\nof three coupled stages: (1) Dynamic Latent Graph Inference,\n(2) Selective State-Space Temporal Modelling, and (3) Cross-\nModal Alignment with a Large Language Model.\n"}, {"page": 3, "text": "A. Theoretical Formulation\n1) Dynamic Latent Graph Inference: Let the input fMRI\nbatch be denoted by X ∈RB×T ×N, where B represents\nthe batch size, T is the number of time points in the scan,\nand N is the number of brain Regions of Interest (ROIs).\nAt each time step t ∈[1, T], the instantaneous brain activity\nis represented by the vector xt ∈RN, where the scalar xt,i\ncorresponds to the Blood-Oxygen-Level-Dependent (BOLD)\nsignal of the i-th ROI. We model whole-brain activity at each\ntime point as a dynamic functional graph Gt = (V, Et), where\nV is the set of N Regions of Interest (ROIs) and Et denotes\ntime-varying functional connections between them. Unlike\nstatic functional connectivity, the graph structure is inferred\ndynamically from the raw fMRI signal at each time step. Each\nscalar BOLD signal is mapped into a latent feature space using\na shared learnable node encoder ϕ(·), implemented using one-\ndimensional convolution followed by self-attention to capture\nlocal temporal patterns and inter-regional dependencies. The\nlatent embedding of node i at time t is given by:\nht,i = ϕ(xt,i) ∈Rdlat,\n(1)\nwhere dlat denotes the latent embedding dimension. Stacking\nall node embeddings yield the latent node matrix Ht =\n[ht,1, . . . , ht,N]⊤∈RN×dlat. Dynamic functional connectiv-\nity is inferred directly from these latent representations by\ncomputing pairwise similarity between node embeddings. The\ntime-varying adjacency matrix Gt ∈RN×N is defined as:\nGt,ij = h⊤\nt,iht,j\n√dlat\n,\n(2)\nwhich captures instantaneous functional coupling between\nROIs. This graph is a latent, task-driven interaction model\ninferred from fMRI time series, rather than explicit neu-\nrobiological functional connectivity. This formulation avoids\nheuristic sliding windows and allows connectivity to evolve\ncontinuously over time. The inferred connectivity is then used\nto spatially contextualize the brain activity:\n˜xt = Gtxt,\n(3)\nwhere ˜xt ∈RN represents the dynamically filtered brain\nactivity. Let ˜X = [˜x1, . . . , ˜xT ] denote the sequence of graph-\nfiltered brain states.\n2) Selective State-Space Temporal Modelling: Temporal\ndependencies are modelled using a Selective State-Space\nModel (SSM) based on the Mamba architecture. The model\nmaintains a latent temporal state st ∈Rdh, initialized as\ns0 = 0, and updated recursively according to:\nst = Atst−1 + Bt˜xt,\n(4)\nwhere At\n∈\nRdh×dh is the state transition matrix and\nBt ∈Rdh×N is the input projection matrix at time t. Here, dh\ndenotes the dimensionality of the latent temporal state main-\ntained by the state-space model. In Mamba, the parameters At\nand Bt are not fixed but are generated as deterministic func-\ntions of the current input ˜xt, enabling input-dependent state\nevolution. This selective parameterization allows the model\nto attenuate noise-dominated time points while preserving\ntemporally localized neural patterns. The formulation supports\nlinear-time sequence modelling with respect to T, making it\nwell-suited for long fMRI sequences.\n3) Cross-Modal LLM Alignment: Let S = [s1, . . . , sT ] ∈\nRT ×dh denote the sequence of latent temporal states. To\ninterface with a Large Language Model (LLM), this variable-\nlength sequence is compressed into a fixed set of brain-\nsummary tokens:\nZbrain ∈RK×dk,\n(5)\nB. Data Extraction and Functional Connectivity\nThe fMRI preprocessing requires accurate atlas registration\nand BOLD signal extraction. We convert raw volumetric\nfMRI scans into graph-structured representations using a stan-\ndardized anatomical atlas. We align all scans to MNI space\nto ensure spatial consistency and extract the mean BOLD\nsignal from each Region of Interest (ROI), producing N\nrepresentative time-series nodes. Traditional functional con-\nnectivity methods, such as Pearson Correlation Coefficient\n(PCC), capture static linear relationships between ROI pairs\n[8], while extensions like Tangent Pearson Embedding (TPE)\nmodel non-linear structure [23]. However, these approaches\naverage out temporal dynamics. In contrast, we infer time-\nresolved latent connectivity directly from ROI-level BOLD\nsignals to explicitly model dynamic brain interactions.\nC. Mamba Selective State-Space Modelling\nWe model long-range temporal dependencies in fMRI se-\nquences using the Mamba architecture based on Selective\nState Space Models (SSMs) [33]. In our framework, individual\nMamba blocks process each ROI’s time-series to extract in-\nformative temporal features, emphasizing task-relevant neural\npatterns while suppressing noise and motion-related artifacts.\nThe resulting features are then aggregated into a representation\nof the brain’s dynamic state, which we subsequently align with\nthe language model.\nD. LLM Reasoning\nThe final stage of NeuroMambaLLM aligns compressed\ndynamic brain representations with the semantic space of the\nLLM. A projection layer maps the Mamba-encoded temporal\nembeddings into the LLM token space. The LLM performs di-\nagnostic inference and generates clinically meaningful textual\nsummaries. Its input consists of a task-specific textual input\nconcatenated with projected brain-state tokens. The base LLM\nis kept fixed, while small LoRA modules are trained together\nwith the projection and brain encoders. This lightweight train-\ning allows the model to adapt to neuroimaging data without\nretraining the full LLM, producing both ASD classification\nand clear text explanations grounded in brain dynamics [19].\nIV. EXPERIMENTS\nA. Datasets\nWe conduct experiments using resting-state functional MRI\ndata from the Autism Brain Imaging Data Exchange (ABIDE\n"}, {"page": 4, "text": "I) [38], a large multi-site initiative designed to advance re-\nsearch on the neural mechanisms of Autism Spectrum Disor-\nder (ASD). ABIDE aggregates data collected across multiple\ninternational sites, resulting in a heterogeneous dataset that\nreflects real-world variability in scanner hardware, acquisition\nprotocols, and scan durations. The dataset comprises 1,035\nsubjects, including 505 individuals diagnosed with ASD and\n530 Typically Developing Controls (TC). Subjects are ran-\ndomly split into 80% for training and 20% for testing at\nthe subject level. We do not use phenotypic or demographic\ninformation. Instead, the proposed model relies exclusively\non fMRI time-series data. Data are preprocessed to reduce\nnoise and ensure spatial consistency, including correction for\nhead motion, slice timing differences, intensity variations, and\nalignment to the standard space. Brain activity is represented\nat the regional level using an atlas-based definition of regions\nof interest (ROIs). For each ROI, we extract the mean BOLD\nsignal over time and normalize each time series to zero mean\nand unit variance. These ROI-level BOLD time series serve\nas input to the NeuroMambaLLM framework.\nB. Experimental Setup\nAll experiments were implemented in PyTorch 1. The model\ntakes raw ROI-level resting-state fMRI BOLD time series as\ninput and performs end-to-end learning without constructing\nhandcrafted functional connectivity matrices. The dynamic\ngraph encoder applies grouped one-dimensional temporal con-\nvolutions with kernel size 3 and ReLU activation to extract lo-\ncal temporal features independently for each ROI, followed by\na self-attention mechanism that infers time-resolved functional\nconnectivity in a latent embedding space of dimension 128.\nThe resulting graph-aware representations are processed by a\ntemporal encoder with two layers and four attention heads to\ncapture long-range dependencies across the fMRI sequence.\nFor cross-modal reasoning, we employ the Large Language\nModel Meta AI (LLaMA-3-8B) as the language backbone.\nThe LLM is loaded using 4-bit quantization to reduce memory\nusage, and all base model parameters remain frozen dur-\ning training. Parameter-efficient fine-tuning is achieved by\ninserting Low-Rank Adaptation (LoRA) modules with rank\nr = 16, scaling factor α = 32, and dropout 0.1 into the\nattention layers. A fixed set of learnable brain-summary tokens\ncompresses the temporal representations and projects them\ninto the LLM embedding space, where they are concatenated\nwith a diagnostic instruction prompt. Training is performed\nusing mixed-precision arithmetic and gradient accumulation\non a single A100 GPU, optimized with the Adam optimizer\nat a learning rate of 1 × 10−4 for up to 10 epochs.\nC. Evaluation and Results\nThe model achieves an overall classification accuracy of\n0.7212±0.0098. The proposed method reaches a precision of\n0.8022 ± 0.0401, indicating reliable identification of autism\ncases among predicted positives, while the recall reaches\n1https://github.com/hamedajorlou/NeuroMambaLLM\n0.6102 ± 0.0602, reflecting balanced sensitivity under dataset\nheterogeneity and limited sample size. The resulting F1-\nscore of 0.6931 ± 0.0560 demonstrates a favourable trade-off\nbetween precision and recall.\nTable I compares the proposed framework with recent\nmethods on the ABIDE dataset. Among different approaches,\nNeuroMambaLLM achieves higher accuracy and F1-score.\nThe last three rows of Table I analyze the impact of dynamic\ngraph modelling and parameter-efficient adaptation. Replacing\nthe dynamic graph with a static formulation (Static Mam-\nbaLLM) degrades performance, highlighting the importance\nof adaptive graph inference. In addition, freezing the LLM\nparameters (Frozen MambaLLM) reduces accuracy and F1-\nscore compared to LoRA adaptation, confirming the value of\nlightweight fine-tuning.\nD. Ablation Study\nWe conduct an ablation study to evaluate key components\nof the proposed framework. All models share the same input\nfeatures, graph construction, and training protocol, and differ\nonly in the temporal modelling backbone or LLM adaptation\nstrategy. We replace the Mamba-based temporal module with\ngated recurrent units (GRU), temporal convolutional networks\n(TCN), Transformers (Tr), and structured state-space models\n(S4) to compare different approaches for modelling fMRI\ntime series over time, and observe that Mamba (Mb) provides\nstrong performance (Fig. 1). We further evaluate the impact of\nintegrating a large language model (LLM) and frozen versus\nLoRA-based adaptation with different ranks (r = 4, 8, 16),\nshowing that lightweight fine-tuning improves performance.\nThe reported performance improvements are consistent across\nmultiple random train–test splits, with low variance as indi-\ncated by the error bars.\nFig. 1.\nAblation results. Comparison of temporal modelling backbones and\nthe effect of integrating an LLM with frozen and LoRA-based adaptation.\nE. LLM Reasoning\nIn addition to classification, the LLM is used to generate\nshort, clinically interpretable summaries from learned dynamic\nbrain representations. The projected brain tokens are concate-\nnated with a simple diagnostic prompt, allowing the LLM to\nreason over regional activations and connectivity patterns and\nexpress them in natural language. To reduce hallucinations,\n"}, {"page": 5, "text": "TABLE I\nCOMPARISON OF DIFFERENT METHODS ON THE ABIDE DATASET.\nMethod\nDescription\nAccuracy\nPrecision\nRecall\nF1-score\nBoLT [39]\nBrain-oriented long-range transformer for temporal fMRI modelling\n0.7128\n0.7132\n0.6485\n0.6799\nSwinT [40]\nHierarchical spatiotemporal transformer for fMRI\n0.6859\n0.6827\n0.6075\n0.6427\nCNN–LSTM [41]\nCNN-based spatial encoder with long short-term memory\n0.6549\n0.6479\n0.5731\n0.6081\nBrainGNN [42]\nGraph neural network on static functional connectivity\n0.6931\n–\n–\n0.6389\nBrainNPT [43]\nNeural process transformer for brain representation learning\n0.6810\n–\n–\n0.6453\nPTGB [44]\nPrompt-tuned graph-based brain foundation model\n0.6709\n–\n–\n0.6520\nCINP [45]\nContrastive instance-wise neural projection model\n0.6760\n–\n–\n0.6474\nBrainMass [46]\nMasked modelling of multivariate brain signals\n0.6926\n–\n–\n0.6398\nNeuroMambaLLM (Proposed)\nDynamic latent brain graph with LLM-guided reasoning\n0.7212\n0.8022\n0.6102\n0.6931\nStatic MambaLLM\nLLM-enhanced Mamba state-space model without graph dynamics\n0.6650\n0.7410\n0.5980\n0.6620\nFrozen MambaLLM\nFrozen LLM backbone without LoRA adaptation\n0.6820\n0.7630\n0.6120\n0.6790\nNote: Bold values indicate the best result in each column; double and single underlines denote the second and third best results, respectively.\nwe provide the LLM with a predefined description schema\nthat constrains the generated text to connectivity statistics. This\nenables the model to explain why a subject is classified as ASD\nor control, rather than only providing a label and an unrelated\ntext. An illustrative example of the prompt and generated\noutput is shown in Fig. 2. Fig. 3 shows that structured\nbrain–LLM alignment using brain-summary tokens leads to\nsteady accuracy improvements, whereas mean pooling yields\nlimited gains and random token inputs provide no benefit.\nThis indicates that LLM performance depends on meaningful\nalignment rather than token injection alone.\nLLM Prompt\nGenerate a descriptive summary of the observed\nfMRI patterns based on the Mamba analysis. The\nsummary should support exploratory analysis\nand expert interpretation and should not be\nconsidered a clinical diagnosis.\nLLM Output\nThe model identifies atypical temporal\ninteractions involving frontal and temporal\nregions, with reduced long-range integration\nand increased local connectivity. Such patterns\nhave been reported in autism spectrum disorder,\nbut this observation is intended for exploratory\nanalysis only. Classification leaning toward ASD\n(58.0% confidence).\nFig. 2. Example LLM prompt and generated clinical response.\nF. Neurobiological Interpretation\nThe learned regional saliency and connectivity patterns\nprovide insight into the neurobiological mechanisms captured\nby the proposed model. Overall, the extracted features are\nconsistent with known alterations in sensory, temporal, and\nassociative brain systems in ASD [47]. Fig. 4 shows that the\nmodel assigns high importance to the left superior temporal\nsulcus (STS) and posterior occipital cortex, regions involved\nin social perception, language, and audiovisual integration.\nAtypical structure and function of the STS and posterior\nFig. 3.\nEffect of brain–LLM alignment during fine-tuning. Structured brain-\nsummary tokens improve performance, while mean pooling offers limited\ngains and random tokens provide no benefit.\ntemporal–occipital regions have been widely reported in ASD\nand are closely linked to deficits in social communication and\nsensory processing [48].\nFig. 4.\nRegional brain activations based on NeuroMambaLLM Results.\nProminent involvement of the left superior temporal sulcus (STS) and posterior\noccipital regions highlights circuits related to social perception/audiovisual\nintegration and early visual-sensory processing.\n"}, {"page": 6, "text": "Fig. 5. Most informative functional connectivity patterns projected onto the\ncortical surface. From left to right: left lateral, medial, and right lateral views.\nThe model highlights predominantly intra-hemispheric and long-range fronto-\ntemporal connections, with left-hemisphere dominance.\nAt the network level, Fig. 5 reveals dominant intra-\nhemispheric and long-range frontotemporal interactions, with\na pronounced left-lateralized pattern. This finding is consis-\ntent with evidence of altered hemispheric specialization and\ndisrupted functional integration between frontal and temporal\nregions in ASD [49]. The circular connectome comparison in\nFig. 6 further illustrates group-specific connectivity patterns,\nwhere ASD-specific connections appear more spatially con-\ncentrated and selective, particularly within temporal and as-\nsociative regions, whereas control-specific connectivity shows\nbroader and more distributed inter-regional integration. In the\nupdated visualization, ASD edges appear fewer but stronger\nand more region-focused, while control networks retain wider\ncross-network coupling. This supports the view that ASD\nreflects an imbalance between local specialization and global\nfunctional integration [50].\nV. CONCLUSION\nWe introduced NeuroMambaLLM, a framework that learns\ndynamic functional connectivity directly from fMRI time se-\nries and combines efficient temporal modelling with language\nmodel reasoning. By avoiding fixed connectivity assumptions,\nthe method captures time-varying brain interactions relevant\nfor autism classification while remaining computationally ef-\nficient. Experiments on the ABIDE dataset demonstrate com-\npetitive performance and connectivity patterns consistent with\nknown neurobiological findings. We note that the current\nevaluation is limited to ABIDE, and future work will assess\ngeneralization across additional datasets and imaging sites.\nLLM-generated explanations may be affected by limitations\nsuch as hallucination or overgeneralization; future work will\nexplore constrained prompting and confidence-aware decoding\nto improve reliability. Importantly, the language model is not\nintroduced to maximize classification accuracy, but to support\nstructured interpretation of dynamic brain patterns in a form\nthat is accessible to domain experts. It operates on compact,\nlearned brain-summary representations and does not alter the\ncore prediction pipeline, limiting added complexity. Compared\nto classical models that provide only labels or scores, this\napproach offers a more complete analysis of temporal and\nconnectivity information, supporting expert reasoning without\nreplacing clinical judgment. Overall, linking dynamic brain\nrepresentations with language-based reasoning enhances in-\nterpretability beyond standard black-box models and opens\ndirections for larger datasets, multimodal imaging, and richer\nclinical reasoning.\nREFERENCES\n[1] G. H. Glover, “Overview of functional magnetic resonance imaging,”\nNeurosurgical Clinics of North America, vol. 22, no. 2, pp. 133–139,\nApr. 2011.\n[2] J. D. Power, K. A. Barnes, A. Z. Snyder, B. L. Schlaggar, and S. E.\nPetersen, “Spurious but systematic correlations in functional connectivity\nmri networks arise from subject motion,” Neuroimage, vol. 59, no. 3,\npp. 2142–2154, 2012.\n[3] K. Murphy, R. M. Birn, and P. A. Bandettini, “Resting-state fmri\nconfounds and cleanup,” Neuroimage, vol. 80, pp. 349–359, 2013.\n[4] C.-W. Woo, L. J. Chang, M. A. Lindquist, and T. D. Wager, “Building\nbetter biomarkers: brain models in translational neuroimaging,” Nature\nneuroscience, vol. 20, no. 3, pp. 365–377, 2017.\n[5] P. Razmara, T. Medani, A. A. Joshi, M. A. Sisara, Y. Tian, S. X. Cui,\nJ. P. Haldar, K. S. Nayak, and R. M. Leahy, “A feasibility study of\ntask-based fmri at 0.55 t,” arXiv preprint arXiv:2505.20568, 2025.\n[6] P. Razmara, F. Han, Q. Kong, J. Xiao, J. Chen, M. Aron, J. Haldar, and\nZ. Fan, “Advancements in prostate luminal water imaging: Radial turbo\nspin-echo acquisition and spatial regularization,” in Proc. Intl. Soc. Mag.\nReson. Med, 2024.\n[7] M. A. Momeni, F. Esmaeilpur Abianeh, M. J. Amini, S. A. Salehi,\nH. Hajishah, P. Razmara, A. Shafiee, and R. Arabzadeh Bahri, “Inci-\ndence of cerebral hyperperfusion syndrome following revascularization\nsurgery in moyamoya patients: a systematic review and meta-analysis,”\nNeurosurgical Review, vol. 49, no. 1, p. 71, 2025.\n[8] M. G. Preti and D. Van De Ville, “Decoupling of brain function from\nstructure reveals regional behavioral specialization in humans,” Nature\nCommunications, vol. 10, no. 1, p. 4747, Oct. 2019, pMID: 31628329;\nPMCID: PMC6800438.\n[9] K. Supekar, S. Ryali, P. Mistry, and V. Menon, “Aberrant dynamics\nof cognitive control and motor circuits predict distinct restricted and\nrepetitive behaviors in children with autism,” Nature Neuroscience,\nvol. 25, no. 10, pp. 1339–1349, 2022.\n[10] L. Liu, G. Wen, P. Cao, T. Hong, J. Yang, X. Zhang, and O. R. Zaiane,\n“Braintgl: A dynamic graph representation learning model for brain\nnetwork analysis,” vol. 153, Feb. 2023, p. 106521, epub 2023 Jan 6.\nPMID: 36630830.\n[11] H. Cui, W. Dai, Y. Zhu, X. Li, L. He, and C. Yang, “Brainnnexplainer:\nAn interpretable graph neural network framework for brain network\nanalysis,” IEEE Transactions on Medical Imaging, vol. 41, no. 12, pp.\n3862–3875, 2021.\n[12] Y. Torabi, S. Shirani, and J. P. Reilly, “A new non-negative matrix\nfactorization approach for blind source separation of cardiovascular\nand respiratory sound based on the periodicity of heart and lung\nfunction,” arXiv preprint arXiv:2305.01889, 2023. [Online]. Available:\nhttps://arxiv.org/abs/2305.01889\n[13] ——, “Large language model-based nonnegative matrix factorization for\ncardiorespiratory sound separation,” arXiv preprint arXiv:2502.05757,\n2025. [Online]. Available: https://arxiv.org/abs/2502.05757\n[14] ——,\n“Chem-nmf:\nMulti-layer\nα-divergence\nnon-negative\nmatrix\nfactorization for cardiorespiratory disease clustering, with improved\nconvergence inspired by chemical catalysts and rigorous asymptotic\nanalysis,” arXiv preprint arXiv:2510.06632, 2025. [Online]. Available:\nhttps://arxiv.org/abs/2510.06632\n[15] D. A. Hudson and C. D. Manning, “Gqa: A new dataset for real-world\nvisual reasoning and compositional question answering,” in Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition,\n2019, pp. 6700–6709.\n[16] T. Khezresmaeilzadeh, J. Zhong, and K. Psounis, “Vriq: Bench-\nmarking and analyzing visual-reasoning iq of vlms,” arXiv preprint\narXiv:2602.05382, 2026.\n[17] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable\nvisual models from natural language supervision,” in International\nconference on machine learning.\nPmLR, 2021, pp. 8748–8763.\n"}, {"page": 7, "text": "[18] A. Aryashad, P. Razmara, A. Mahjoub, S. Azizi, M. Salmani, and\nA. Firouzkouhi, “From filters to vlms: Benchmarking defogging methods\nthrough object detection and segmentation performance,” arXiv preprint\narXiv:2510.03906, 2025.\n[19] Anonymous, “Fcn-llm: Empower llm for brain functional connectivity\nnetwork understanding via graph-level multi-task instruction tuning,” in\nUnder Review at ICLR 2026, 2025, preprint.\n[20] J. Ortega Caro, A. H. d. O. Fonseca, C. Averill, S. A. Rizvi, M. Rosati,\nJ. L. Cross, P. Mittal, E. Zappala, D. Levine, R. M. Dhodapkar, I. Han,\nA. Karbasi, C. G. Abdallah, and D. van Dijk, “Brainlm: A foundation\nmodel for brain activity recordings,” bioRxiv, 2023.\n[21] D. J. Lurie, D. Kessler, D. S. Bassett, R. F. Betzel, M. Breakspear,\nS. Keilholz, A. Kucyi, R. Li´egeois, M. A. Lindquist, A. R. McIntosh\net al., “Questions and controversies in the study of time-varying func-\ntional connectivity,” Network Neuroscience, vol. 4, no. 1, pp. 30–69,\n2020.\n[22] S. Gadgil, Q. Zhao, E. Abdelnabi, A. Taleb, H. Lombaert, and M. Toews,\n“Spatio-temporal graph convolution for resting-state fmri analysis,” in\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention (MICCAI).\nSpringer, 2020, pp. 528–538.\n[23] B.-H. Kim, J. C. Ye, and J.-J. Kim, “Learning dynamic graph represen-\ntation of brain connectome with spatio-temporal attention,” in Advances\nin Neural Information Processing Systems (NeurIPS), vol. 34, 2021, pp.\n4314–4327.\n[24] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning\nspatiotemporal features with 3d convolutional networks,” in Proceedings\nof the IEEE international conference on computer vision, 2015, pp.\n4489–4497.\n[25] Z. Yang, X. Zhuang, K. Sreenivasan, V. Mishra, T. Curran, and\nD. Cordes, “A robust deep neural network for denoising task-based fmri\ndata: An application to working memory and episodic memory,” Medical\nImage Analysis, vol. 60, p. 101622, 2020.\n[26] D. M. Yalcinkaya, A. M. Sohi, K. Youssef, L. F. Zamudio, M. Elliott,\nV. Polsani, R. Dharmakumar, R. Judd, M. S. Tong, D. J. Shah et al.,\n“Physics-informed deep learning model selection for robust segmen-\ntation of multi-center stress perfusion datasets: results from the scmr\nregistry,” Journal of Cardiovascular Magnetic Resonance, vol. 27, 2025.\n[27] M. B. Sahin, Z. Li, K. Youssef, A. M. Sohi, D. Yalcinkaya, L. Zamudio,\nM. Elliot, V. Polsani, M. Tong, D. Shah et al., “Adapting a cmr\nfoundation model for ai-powered analysis of perfusion cmr: Enabling\n12-fold reduction in manually labeled training dataset for automatic\nsegmentation,” Journal of Cardiovascular Magnetic Resonance, vol. 28,\n2026.\n[28] X. Kan, H. Cui, J. Lukemire, Y. Guo, and C. Yang, “Brain network trans-\nformer,” Advances in Neural Information Processing Systems (NeurIPS),\nvol. 35, pp. 25 586–25 599, 2022.\n[29] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, “Vision\nmamba: Efficient visual representation learning with bidirectional state\nspace model,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2024.\n[30] Y.\nTorabi,\nA.\nEkhteraei,\nand\nB.\nBaraeinejad,\n“Inverse\ndesign\nof microring resonator-based glucose biosensor using ai diffusion\nmodels,” Optica Open, 2025, preprint. [Online]. Available: https:\n//doi.org/10.1364/opticaopen.30525731.v1\n[31] F. Schwock, D. Nordgren, R. M. Iritani, L. Atlas, A. Yazdan-Shahmorad,\nand H. Jahanian, “Integrating structural and functional connectivity for\ndynamic fmri modeling via graph diffusion autoregression,” in 47th\nAnnual International Conference of the IEEE Engineering in Medicine\nand Biology Society (EMBC).\nIEEE, 2025.\n[32] J. Lu, Y. Xu, H. Wang, Y. Bai, and Y. Fu, “Latent graph inference with\nlimited supervision,” ArXiv, vol. abs/2310.04314, 2023.\n[33] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\nselective state spaces,” arXiv preprint arXiv:2312.00752, 2023.\n[34] J. Nie, K. Han, T. Zhang, C. You, S. van Rooij, J. Stevens, B. Dunlop,\nC. Gillespie, and C. Yang, “Brain network state transformer: Leveraging\nstate functional connectivity for enhanced brain network analysis,” in\n47th Annual International Conference of the IEEE Engineering in\nMedicine and Biology Society (EMBC).\nIEEE, 2025.\n[35] Z. Xing, T. Ye, Y. Yang, G. Liu, and L. Zhu, “Segmamba: Long-range\nsequential modeling mamba for 3d medical image segmentation,” in\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention (MICCAI), 2024.\n[36] J. Ma, F. Li, and B. Wang, “U-mamba: Enhancing long-range de-\npendency for biomedical image segmentation,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2024.\n[37] H. Jiahao, M. M. U. Rahman, T. Al-Naffouri, and T.-M. Laleg-Kirati,\n“Mamba-cam-sleep: A mamba-based channel attention model for sleep\nstaging classification,” in 47th Annual International Conference of the\nIEEE Engineering in Medicine and Biology Society (EMBC).\nIEEE,\n2025.\n[38] A. Di Martino, C.-G. Yan, Q. Li, E. Denio, F. X. Castellanos, K. Alaerts,\nJ. S. Anderson, M. Assaf, S. Y. Bookheimer, M. Dapretto et al., “The\nautism brain imaging data exchange: towards a large-scale evaluation of\nthe intrinsic brain architecture in autism,” Molecular Psychiatry, vol. 19,\nno. 6, pp. 659–667, 2014.\n[39] H. A. Bedel, I. Sivgin, O. Dalmaz, S. U. Dar, and T. C¸ ukur, “Bolt: Fused\nwindow transformers for fmri time series analysis,” vol. 88, 2024, p.\n102841.\n[40] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2021.\n[41] C. Zhao, H. Li, Z. Jiao, T. Du, and Y. Fan, “A 3D convolutional encap-\nsulated long short-term memory (3DConv-LSTM) model for denoising\nfmri data,” pp. 479–488, Oct. 2020.\n[42] X. Li et al., “Braingnn: Interpretable brain graph neural network for\nfmri analysis,” Medical Image Analysis, vol. 74, p. 102233, 2021.\n[43] J. Hu, Y. Huang, N. Wang, and S. Dong, “Brainnpt: Pre-training trans-\nformer networks for brain network classification,” IEEE Transactions on\nNeural Systems and Rehabilitation Engineering, 2024.\n[44] Y. Yang, H. Cui, and C. Yang, “Ptgb: Pre-train graph neural networks\nfor brain network analysis,” arXiv preprint arXiv:2305.14376, 2023.\n[45] X. Hu, W. Wang, and L. Xiao, “Learning 3D medical image models from\nbrain functional connectivity network supervision for mental disorder\ndiagnosis,” pp. 336–346, 2025.\n[46] Y. Yang, C. Ye, G. Su, Z. Zhang, Z. Chang, H. Chen, P. Chan, Y. Yu, and\nT. Ma, “Brainmass: Advancing brain network analysis for diagnosis with\nlarge-scale self-supervised learning,” IEEE Transactions on Medical\nImaging, vol. 43, no. 11, pp. 4004–4016, Nov. 2024, epub 2024 Nov 4.\nPMID: 38875087.\n[47] M. K. Belmonte, G. Allen, A. Beckel-Mitchener, L. M. Boulanger, R. A.\nCarper, and S. J. Webb, “Autism and abnormal development of brain\nconnectivity,” Journal of Neuroscience, vol. 24, no. 42, pp. 9228–9231,\n2004.\n[48] K. A. Pelphrey, J. P. Morris, and G. McCarthy, “Perception of dynamic\nchanges in facial affect and identity in autism,” Social Cognitive and\nAffective Neuroscience, vol. 2, no. 2, pp. 140–149, 2007.\n[49] M. A. Just, V. L. Cherkassky, T. A. Keller, R. K. Kana, and N. J.\nMinshew, “Functional and anatomical cortical underconnectivity in\nautism: Evidence from an fmri study of an executive function task and\ncorpus callosum morphometry,” Cerebral Cortex, vol. 17, no. 4, pp.\n951–961, 2007.\n[50] E. Courchesne and K. Pierce, “Why the frontal cortex in autism might\nbe talking only to itself: Local over-connectivity but long-distance\ndisconnection,” Current Opinion in Neurobiology, vol. 15, no. 2, pp.\n225–230, Apr. 2005, pMID: 15831407.\n"}, {"page": 8, "text": "Fig. 6. Circular connectome visualization of group-specific functional connectivity patterns. (a) Autism Spectrum Disorder (ASD)–specific connectivity, shown\nwith red edges. (b) Typically Developing Controls (TC)–specific connectivity, shown with green edges. Nodes correspond to anatomically defined Regions of\nInterest (ROIs), labelled by standard abbreviations and hemisphere, and organized according to functional systems: visual (VIS), somatomotor (SM), dorsal\nattention (DAN), ventral attention/salience (VAN), limbic (LIM), frontoparietal control (FPN), and default mode network (DMN). ASD connectivity appears\nmore spatially concentrated within sensory and temporal regions, whereas control connectivity exhibits broader, more distributed inter-regional integration.\nEdge thickness indicates relative connection importance.\n"}]}