{"doc_id": "arxiv:2512.05863", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.05863.pdf", "meta": {"doc_id": "arxiv:2512.05863", "source": "arxiv", "arxiv_id": "2512.05863", "title": "Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework", "authors": ["Tasnimul Hassan", "Md Faisal Karim", "Haziq Jeelani", "Elham Behnam", "Robert Green", "Fayeq Jeelani Syed"], "published": "2025-12-05T16:38:47Z", "updated": "2025-12-05T16:38:47Z", "summary": "Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.05863v1", "url_pdf": "https://arxiv.org/pdf/2512.05863.pdf", "meta_path": "data/raw/arxiv/meta/2512.05863.json", "sha256": "450f1a9cf181e43d4ab3e94e1e958605aa4a0f3f7a89113c157695072b37933c", "status": "ok", "fetched_at": "2026-02-18T02:25:12.491482+00:00"}, "pages": [{"page": 1, "text": "Optimizing Medical Question-Answering Systems:\nA Comparative Study of Fine-Tuned and Zero-Shot\nLarge Language Models with RAG Framework\nTasnimul Hassan\nDepartment of Electrical Engineering\nand Computer Science\nUniversity of Toledo\nToledo, USA\ntasnimul.hasan@rockets.utoledo.edu\nMd Faisal Karim\nDepartment of Electrical Engineering\nand Computer Science\nUniversity of Toledo\nToledo, USA\nmdfaisal.karim@rockets.utoledo.edu\nHaziq Jeelani\nInstitute of Mathematical Sciences\nClaremont Graduate University\nClaremont, USA\nhaziq.jeelani@cgu.edu\nElham Behnam\nDepartment of Bioengineering\nUniversity of Toledo\nToledo, USA\nelham.behnam@rockets.utoledo.edu\nRobert Green\nDepartment of Computer Science\nBowling Green State University\nBowling Green, USA\ngreenr@bgsu.edu\nFayeq Jeelani Syed\nDepartment of Electrical Engineering\nand Computer Science\nUniversity of Toledo\nToledo, USA\nsyedfayeq.jeelani@rockets.utoledo.edu\nAbstract—Medical question-answering (QA) systems can ben-\nefit from advances in large language models (LLMs), but directly\napplying LLMs to the clinical domain poses challenges such as\nmaintaining factual accuracy and avoiding hallucinations. In this\npaper, we present a retrieval-augmented generation (RAG) based\nmedical QA system that combines domain-specific knowledge\nretrieval with open-source LLMs to answer medical questions.\nWe fine-tune two state-of-the-art open LLMs (LLaMA 2 and\nFalcon) using Low-Rank Adaptation (LoRA) for efficient domain\nspecialization. The system retrieves relevant medical literature\nto ground the LLM’s answers, thereby improving factual cor-\nrectness and reducing hallucinations. We evaluate the approach\non benchmark datasets (PubMedQA and MedMCQA) and show\nthat retrieval augmentation yields measurable improvements in\nanswer accuracy compared to using LLMs alone. Our fine-\ntuned LLaMA 2 model achieves 71.8% accuracy on PubMedQA,\nsubstantially improving over the 55.4% zero-shot baseline, while\nmaintaining transparency by providing source references. We\nalso detail the system design and fine-tuning methodology,\ndemonstrating that grounding answers in retrieved evidence\nreduces unsupported content by approximately 60%. These\nresults highlight the potential of RAG-augmented open-source\nLLMs for reliable biomedical QA, pointing toward practical\nclinical informatics applications.\nIndex Terms—Medical question-answering, LLMs, Retrieval-\naugmented generation, Biomedical NLP, Clinical informatics\nI. INTRODUCTION\nLarge language models (LLMs) have dramatically advanced\nthe state of natural language understanding and generation.\nThe release of GPT-3 [1] demonstrated that very large LLMs\ncan achieve remarkable few-shot question-answering perfor-\nmance. More recently, open-source LLMs such as LLaMA 2\n[2] and Falcon [3] have shown strong capabilities, though\nthey typically lag behind the latest proprietary models in raw\nperformance. This trade-off is often acceptable given their\naccessibility and customizability for domain-specific applica-\ntions. There is particularly strong interest in applying LLMs\nto the biomedical and health informatics domains, where\nthese models could assist clinicians and patients in answering\nquestions by leveraging the vast body of medical knowledge\n[4], [5]. Early explorations have shown promising results; for\nexample, Google’s Med-PaLM achieved 67.2% on USMLE-\nstyle questions, becoming the first to exceed a passing score\n[6], while GPT-4 has demonstrated strong performance on\nvarious medical benchmarks [7].\nYet significant challenges remain before LLMs can be safely\nand effectively used in patient care. A critical concern is\nthe factual accuracy of model-generated answers. General-\npurpose LLMs often produce confidently stated incorrect or\nfabricated information (so-called “hallucinations”), which is\nunacceptable in medicine, where inaccuracies can be harmful.\nFor example, models like ChatGPT or GPT-4 can output\nmedical advice that sounds plausible but is not supported\nby evidence or clinical guidelines [7]. Moreover, because\nLLMs are usually trained on general internet text, they may\nlack up-to-date medical knowledge. They also often fail to\nuse specialized terminology with precision. Fine-tuning LLMs\non biomedical text can mitigate some of these issues [8],\n[9]. However, fully retraining or fine-tuning a large model is\nresource-intensive and might still not eliminate hallucinations.\nOne promising approach to improve factual accuracy is\nretrieval-augmented generation (RAG) [11]. In such a system,\nthe model first retrieves relevant documents (e.g., medical lit-\nerature, guidelines, electronic health records) from an external\nknowledge source, then generates an answer conditioned on\nthat evidence. Grounding responses in actual text encourages\nthe model to produce answers that are supported by references,\narXiv:2512.05863v1  [cs.CL]  5 Dec 2025\n"}, {"page": 2, "text": "thereby reducing the incidence of hallucinated facts. Retrieval-\nbased QA has a rich history, from early pipeline systems like\nDrQA [10] to modern approaches that integrate neural retriev-\ners with generators [11], [12]. This approach is especially well-\nsuited for biomedicine, given the vast and constantly growing\nbody of biomedical literature.\nIn this work, we propose a medical QA system that lever-\nages RAG in combination with fine-tuned open-source LLMs\nto address these challenges. Our key contributions include:\n• We design a retrieval-augmented generation architecture\nfor medical QA that combines a document retriever with a\nlarge generative model. The system cites relevant medical\nliterature to justify its answers, enhancing transparency.\n• We fine-tune two high-performing open LLMs (Meta’s\nLLaMA 2 and TII’s Falcon) on medical QA data using\nLoRA [16], a parameter-efficient fine-tuning method.\nThis enables effective domain adaptation at low compu-\ntational cost, without full model retraining.\n• We evaluate the system on multiple biomedical QA\nbenchmarks, including PubMedQA [17] and MedMCQA\n[18]. Retrieval augmentation substantially improves an-\nswer accuracy and reduces hallucinations compared to\ngeneration without retrieval. For instance, our model\nbased on LLaMA 2 reaches 71.8% accuracy on Pub-\nMedQA, improving significantly over the 55.4% zero-\nshot baseline.\n• We analyze the system’s outputs and find that ground-\ning answers in retrieved evidence reduces unsupported\nstatements by approximately 60%. We also discuss the\nsystem’s potential clinical applications (as an assistive\ntool for healthcare professionals and patients) and outline\nremaining challenges, such as the need for rigorous\nvalidation and adherence to medical guidelines.\nII. RELATED WORK\nLLMs for Biomedical QA: Early transformer-based language\nmodels tailored to biomedicine (e.g., BioBERT [9]) improved\ntasks like clinical named entity recognition and QA, but\nthese models were relatively small and task-specific. The\nadvent of much larger LLMs has opened new possibilities for\ngenerative QA in medicine. Luo et al. introduced BioGPT\n[8], a 1.5B-parameter GPT model trained on PubMed, which\nachieved strong results on biomedical QA benchmarks (78.2%\naccuracy on PubMedQA). More recently, researchers have\napplied general LLMs to medical QA via fine-tuning or\nprompting. Google’s Med-PaLM [6], built on a fine-tuned\n540B-parameter model (Flan-PaLM), was the first to exceed\nthe passing score on USMLE-style questions. Its successor,\nMed-PaLM 2, achieved 86.5% on MedQA and showed sub-\nstantial improvements in physician evaluations [6]. Nori et al.\n[7] evaluated GPT-4 on medical exams and found it could\noutperform many specialized models without any domain-\nspecific training. While these works demonstrate the potential\nof LLMs in healthcare, they largely rely on proprietary models.\nIn contrast, we focus on open-source LLMs (LLaMA 2 [2],\nFalcon [3]) that can be custom-tailored and deployed without\nsuch restrictions.\nRetrieval-Augmented QA: Augmenting NLP models with\nretrieved knowledge is a well-established strategy to im-\nprove factual accuracy. Traditional open-domain QA systems\n(e.g., DrQA [10]) employed a two-stage pipeline: document\nretrieval (with methods like BM25) followed by a read-\ning comprehension model to extract answers. More recently,\nneural retrievers and sequence-to-sequence generators have\nbeen integrated in end-to-end frameworks. Lewis et al. [11]\nintroduced RAG, which combines a learned neural retriever\nwith a parametric generator, and the Atlas model (T5-based,\n11B parameters) achieved state-of-the-art open QA results by\nretrieving relevant passages even in few-shot settings [12].\nIn the biomedical domain, retrieval has long been used in\nchallenges like BioASQ [13], where systems search PubMed\narticles to answer questions. Our approach follows this line\nof work by applying retrieval augmentation to a modern LLM\nfor medical QA. We use a Dense Passage Retriever (DPR)\n[14] to find relevant snippets from a large corpus of medical\nliterature, which then serve to ground the LLM’s answers in\nevidence. Providing source material to the generator helps curb\nthe model’s tendency to produce unsupported claims, an effect\nalso observed in retrieval-augmented models like RETRO [15].\nEfficient Fine-Tuning of LLMs: Fine-tuning very large mod-\nels on domain-specific data can be prohibitively expensive,\nbut parameter-efficient techniques offer a solution. Low-Rank\nAdaptation (LoRA) [16] inserts small trainable matrices into\neach layer of the model while keeping the original weights\nfrozen, drastically reducing the resources needed. Hu et al.\nshowed that LoRA can match full fine-tuning performance\nwhile updating only about 0.1% of the parameters [16].\nOther approaches (prompt tuning, adapters) similarly minimize\ntraining overhead, but LoRA has been particularly effective\nfor LLMs. In our work, we use LoRA to fine-tune LLaMA 2\n(13B) and Falcon (40B) on medical QA data, allowing us to\nspecialize these models to the domain with relatively modest\ncomputational resources. We did not employ reinforcement\nlearning from human feedback for alignment; instead, we rely\non grounding the LLM’s outputs in retrieved evidence and\nsupervised fine-tuning on correct QA examples to ensure reli-\nability. The combination of retrieval augmentation and LoRA\nfine-tuning enables us to build a high-performing medical QA\nsystem efficiently.\nIII. METHODOLOGY\nA. System Overview\nOur\nsystem\nfollows\na\nretrieval-augmented\ngeneration\npipeline for medical question answering, as illustrated in\nFigure 1. Given a user’s question, the system first retrieves\nrelevant context from a knowledge repository, then generates\nan answer that incorporates both the question and the retrieved\nevidence. This design ensures that the answer is grounded\nin verifiable information. The knowledge repository consists\nof a large collection of biomedical documents (e.g., PubMed\nabstracts, clinical guidelines, and curated FAQs) indexed in a\nvector database for efficient semantic search.\nThe pipeline has two main stages:\n"}, {"page": 3, "text": "Fig. 1.\nHigh-level architecture of the proposed RAG-based medical QA\nsystem. The system first retrieves relevant documents from a biomedical\nknowledge repository given a user question. A fine-tuned LLM (either\nLLaMA 2 or Falcon) then generates an answer conditioned on the question\nand retrieved context. Grounding the answer in retrieved evidence helps reduce\nhallucinations and improve accuracy.\n1) Question Retrieval: The input question is encoded\ninto a vector using a bi-encoder transformer model. We\nemploy a dense passage retriever (DPR) [14] trained on\nbiomedical text to embed the question and candidate\npassages in the same space. The top-k most relevant\npassages are retrieved based on inner-product similarity\nto the question embedding. We use k = 5, which\nprovides sufficient context while balancing relevance\nand computational efficiency.\n2) Answer Generation: The question and the retrieved\npassages are concatenated to form a prompt for the gen-\nerative model. We prepend an instruction that the model\nshould use the provided information and cite its sources\nwhen answering. The fine-tuned LLM (LLaMA 2 or Fal-\ncon) then generates a free-form answer, with an answer\nlength limit of 256 tokens to ensure focused responses.\nThe output is encouraged to include references to the\nretrieved documents when making specific claims.\nThe final answer presented to the user is a coherent expla-\nnation with references to the source material. For example, the\nsystem might respond: “The recommended dosage of Drug X\nfor condition Y is 5–10 mg daily, based on clinical guide-\nlines from the retrieved literature.” Providing such references\nenhances trustworthiness, as users can verify the information\nfrom the original sources.\nB. Fine-Tuning the LLMs with LoRA\nTo adapt the generative models to the medical QA task, we\nfine-tuned the base LLMs on domain-specific QA data using\nsupervised learning and LoRA [16]. LoRA inserts low-rank\nadapters into each transformer layer, allowing us to update\nonly a small fraction of the model’s parameters during fine-\ntuning. We set the adapter rank to 16 and alpha to 32, updating\nfewer than 0.5% of the model’s weights, which significantly\nreduces GPU memory requirements.\nWe compiled a training set of approximately 15,000 ques-\ntion–answer pairs from several sources: the PubMedQA train-\ning split (8,000 pairs), the MedMCQA training subset (5,000\npairs), and a curated collection of medical FAQs (2,000 pairs).\nEach training example included the question, a set of relevant\ncontext passages (retrieved from our document corpus), and\nthe correct answer. We fine-tuned the LLaMA 2 and Fal-\ncon models on these examples so that they learned to (1)\nTABLE I\nACCURACY (%) OF DIFFERENT LANGUAGE MODELS ON PUBMEDQA AND\nMEDMCQA BENCHMARKS. FINE-TUNING AND RAG INTEGRATION\nIMPROVE PERFORMANCE, ESPECIALLY FOR OPEN-SOURCE MODELS.\nModel\nSetting\nPubMedQA Acc (%)\nMedMCQA Acc (%)\nGPT-4\nZero-Shot\n78.2\n69.5\nGPT-4\nFew-Shot\n81.0\n72.3\nLLaMA 2\nZero-Shot\n55.4\n47.2\nLLaMA 2\nFine-Tuned\n64.3\n56.8\nLLaMA 2\nFine-Tuned + RAG\n71.8\n64.3\nFalcon\nZero-Shot\n52.1\n44.8\nFalcon\nFine-Tuned\n61.2\n53.5\nFalcon\nFine-Tuned + RAG\n68.9\n61.7\ncomprehend medical questions, (2) incorporate the provided\nevidence into their answers, and (3) produce accurate, con-\ncise explanations. Including retrieved context in training was\nimportant: it taught the model to rely on external information\nfrom documents rather than solely on its internal knowledge.\nFine-tuning was performed using the AdamW optimizer\nwith a learning rate of 2 × 10−4 and cosine scheduling for\n3 epochs on a server with four NVIDIA A100 GPUs. Thanks\nto LoRA’s efficiency, adapting the 13B-parameter and 40B-\nparameter models was feasible, with training completing in\napproximately 48 hours. After fine-tuning, we integrated each\nLLM into the retrieval pipeline for inference. At test time,\nthe model is given the top-5 retrieved passages along with\nthe question, prefaced by an instruction to ground its answer\nin the provided information. While the model was trained to\nreference sources, the citation format varies and is not always\nconsistent in the generated outputs. This attribution capability,\nwhen present, is valuable for clinical applications as it allows\nusers to trace information back to sources.\nIV. RESULTS AND DISCUSSION\nWe evaluated our system on two benchmark datasets and\nconducted a comprehensive analysis of its outputs. The eval-\nuation compares language models under two conditions: a\nstandard closed-book setting (the model relies solely on inter-\nnal knowledge) and a retrieval-augmented setting (the model\nuses retrieved documents during inference). Unless stated\notherwise, ”LLaMA 2” refers to our fine-tuned 13B-parameter\nmodel, and ”Falcon” refers to our fine-tuned 40B-parameter\nmodel.\nA. Quantitative Performance\nTo assess the effectiveness of retrieval augmentation, we\nevaluated models on PubMedQA and MedMCQA using ac-\ncuracy as the primary metric, following standard practice\nfor these benchmarks. Table I summarizes the results. As\nexpected, retrieval augmentation and fine-tuning significantly\nimprove performance over zero-shot baselines. While GPT-4\nachieves the highest scores, our fine-tuned LLaMA 2 with\nRAG shows substantial improvements, reaching 71.8% on\nPubMedQA compared to 55.4% without retrieval.\n"}, {"page": 4, "text": "TABLE II\nCOMPARISON OF LATENCY AND GPU MEMORY USAGE ACROSS MODELS.\nGPT-4 (API-BASED) HAS LOWER LATENCY, WHILE OPEN-SOURCE\nMODELS REQUIRE SUBSTANTIAL MEMORY.\nModel\nSetting\nAvg Latency (s)\nGPU Memory (GB)\nGPT-4\nAPI\n2.3\nN/A\nLLaMA 2 (13B)\nFine-Tuned + RAG\n3.8\n18\nFalcon (40B)\nFine-Tuned + RAG\n5.2\n42\nTABLE III\nCOMPARISON OF PUBMEDQA ACCURACY (%) WITH PRIOR STUDIES.\nOUR LLAMA 2 + RAG APPROACH OUTPERFORMS BIOBERT AND\nAPPROACHES THE PERFORMANCE OF LARGER PROPRIETARY MODELS.\nStudy\nModel\nPubMedQA Acc (%)\nJin et al. [17]\nBioBERT\n68.1\nLuo et al. [8]\nBioGPT\n78.2\nSinghal et al. [6]\nMed-PaLM 2\n81.0\nThis Work\nLLaMA 2 + RAG\n71.8\nB. Efficiency and Resource Considerations\nTable II outlines latency and memory usage for different\nmodels. While GPT-4 provides faster response times through\nAPI calls, it requires paid access. Among the open models,\nLLaMA 2 (13B) offers the best balance of performance and\nresource efficiency, while Falcon (40B) requires more memory\nbut provides marginally better accuracy.\nC. Comparison with Prior Works\nWe\ncompared\nour\nbest-performing\nmodel\n(fine-tuned\nLLaMA 2 with RAG) to recent approaches from the literature.\nTable III shows that while our model does not exceed state-of-\nthe-art proprietary systems like Med-PaLM 2, it offers a strong\nopen-source alternative that significantly outperforms the zero-\nshot baseline and provides transparency through source attri-\nbution.\nD. Hallucination Reduction and Interpretability\nTo assess the impact of retrieval augmentation on fac-\ntual accuracy, we manually evaluated 100 randomly sam-\npled QA pairs from the test set. Two medical professionals\nindependently annotated each answer for factual errors and\nunsupported claims. Inter-annotator agreement was substantial\n(Cohen’s κ = 0.73). Results showed that retrieval augmenta-\ntion reduced factual errors from 35% in the fine-tuned-only\nsetting to 14% with RAG. The most common remaining errors\nwere: (1) misinterpretation of statistical findings from retrieved\npapers (32% of errors), (2) overgeneralization from specific\nstudy populations (28%), and (3) outdated information from\nolder retrieved documents (25%). The retrieved documents\noften provided specific phrasing that appeared verbatim in\nthe model’s answers, improving verifiability. However, the\nmodel’s ability to provide consistent structured citations re-\nmained limited, with only 42% of answers including clear\nsource attribution.\nE. Clinical and Deployment Considerations\nOur system demonstrates reasonable performance with man-\nageable resource demands, suggesting potential utility in clin-\nical informatics settings. Possible applications include:\n• Assisting medical students with literature review and\nexam preparation\n• Providing clinicians with rapid access to relevant research\nfindings\n• Supporting evidence-based patient education materials\nHowever, several limitations must be addressed before clin-\nical deployment:\n• The system is not intended for direct clinical decision-\nmaking without physician oversight\n• Performance on rare diseases and specialized procedures\nremains limited\n• The knowledge repository requires continuous updates to\nmaintain currency\n• Robust evaluation by medical professionals in real clini-\ncal workflows is essential\nV. CONCLUSION\nIn this paper, we introduced a retrieval-augmented QA\nframework for biomedical applications, built on fine-tuned\nopen-source LLMs. Extensive experiments demonstrate that\nour approach significantly improves factual accuracy, reduces\nhallucinations, and achieves performance approaching that of\nleading domain-specific and proprietary systems. Importantly,\nthe system offers a transparent and resource-efficient alterna-\ntive suitable for deployment in diverse settings.\nFuture work will explore clinician-in-the-loop feedback\nmechanisms, multi-modal reasoning (e.g., incorporating imag-\ning and EHR data), and formal validation with healthcare pro-\nfessionals. Our findings advocate for the broader adoption of\ntransparent, adaptable, and reproducible LLM-based systems\nin medical AI.\nACKNOWLEDGMENT\nThe authors thank the open-source communities behind\nLLaMA 2, Falcon, LoRA, PyTorch, and Hugging Face Trans-\nformers for freely providing the models and tooling that made\nthis study possible. We are also grateful to the curators of\nMedQA-USMLE, MedMCQA, PubMedQA, and ClinicalQA\nfor releasing their datasets to the public.\nREFERENCES\n[1] T. Brown et al., “Language models are few-shot learners,” Advances in\nNeural Information Processing Systems (NeurIPS), vol. 33, pp. 1877–\n1901, 2020.\n[2] H. Touvron et al., “Llama 2: Open Foundation and Fine-Tuned Chat\nModels,” arXiv preprint arXiv:2307.09288, 2023.\n[3] E. Almazrouei et al., “The Falcon series of open language models,”\narXiv preprint arXiv:2311.06867, 2023.\n[4] W. X. Zhao et al., “A survey of large language models,” arXiv preprint\narXiv:2303.18223, 2023.\n[5] H.\nZhou\net\nal.,\n“A\nsurvey\nof\nlarge\nlanguage\nmodels\nin\nmedicine: progress, applications, and challenges,” arXiv preprint\narXiv:2311.05112, 2024.\n[6] K. Singhal et al., “Towards expert-level medical question answering with\nlarge language models,” arXiv preprint arXiv:2305.09617, 2023.\n"}, {"page": 5, "text": "[7] H. Nori et al., “Capabilities of GPT-4 on medical challenge problems,”\narXiv preprint arXiv:2303.13375, 2023.\n[8] R. Luo et al., “BioGPT: generative pre-trained transformer for biomed-\nical text generation and mining,” Briefings in Bioinformatics, vol. 23,\nno. 6, bbac409, 2022.\n[9] J. Lee et al., “BioBERT: a pre-trained biomedical language model for\nbiomedical text mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–1240,\n2020.\n[10] D. Chen et al., “Reading Wikipedia to answer open-domain questions,”\nin Proc. of ACL, 2017, pp. 1870–1879.\n[11] P. Lewis et al., “Retrieval-augmented generation for knowledge-intensive\nNLP tasks,” in Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n[12] G. Izacard et al., “Atlas: few-shot learning with retrieval augmented\nlanguage models,” in Proc. of ICML, 2022.\n[13] G. Tsatsaronis et al., “An overview of the BIOASQ large-scale biomed-\nical semantic indexing and question answering competition,” BMC\nBioinformatics, vol. 16, p. 138, 2015.\n[14] V. Karpukhin et al., “Dense passage retrieval for open-domain question\nanswering,” in Proc. of EMNLP, 2020, pp. 6769–6781.\n[15] S. Borgeaud et al., “Improving language models by retrieving from\ntrillions of tokens,” in Proc. of ICML, 2022.\n[16] E. J. Hu et al., “LoRA: Low-rank adaptation of large language models,”\nin Proc. of ICLR, 2022.\n[17] Q. Jin et al., “PubMedQA: A dataset for biomedical research question\nanswering,” in Proc. of ACL (Workshop), 2019.\n[18] A. Pal et al., “MedMCQA: A large-scale multi-subject multiple-choice\ndataset for medical domain question answering,” in Proc. of ACM-CHIL,\n2022, pp. 477–497.\n"}]}