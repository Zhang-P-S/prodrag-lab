{"doc_id": "arxiv:2511.11883", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.11883.pdf", "meta": {"doc_id": "arxiv:2511.11883", "source": "arxiv", "arxiv_id": "2511.11883", "title": "ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "David Carlson"], "published": "2025-11-14T21:21:16Z", "updated": "2025-11-14T21:21:16Z", "summary": "Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.11883v1", "url_pdf": "https://arxiv.org/pdf/2511.11883.pdf", "meta_path": "data/raw/arxiv/meta/2511.11883.json", "sha256": "4fe2e542bbfdc09e2f47751f0dddedb1ab07b8e995d8aa8579f9e55cf021e55f", "status": "ok", "fetched_at": "2026-02-18T02:27:05.088781+00:00"}, "pages": [{"page": 1, "text": "ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts\nKarthikeyan K\nDuke University\nkarthikeyan.k@duke.edu\nRaghuveer Thirukovalluru\nDuke University\nraghuveer.thirukovalluru@duke.edu\nDavid Carlson\nDuke University\ndavid.carlson@duke.edu\nAbstract\nClinical notes contain valuable, context-rich\ninformation, but their unstructured format in-\ntroduces several challenges, including unin-\ntended biases (e.g., gender or racial bias), and\npoor generalization across clinical settings (e.g.,\nmodels trained on one EHR system may per-\nform poorly on another due to format differ-\nences) and poor interpretability. To address\nthese issues, we present ClinStructor, a pipeline\nthat leverages large language models (LLMs)\nto convert clinical free-text into structured, task-\nspecific question–answer pairs prior to predic-\ntive modeling. Our method substantially en-\nhances transparency and controllability and\nonly leads to a modest reduction in predictive\nperformance (a 2–3% drop in AUC), compared\nto direct fine-tuning, on the ICU mortality pre-\ndiction task. ClinStructor lays a strong foun-\ndation for building reliable, interpretable, and\ngeneralizable machine learning models in clini-\ncal environments.\n1\nIntroduction\nRecent advances in Artificial Intelligence (AI) have\ndriven significant progress across various domains,\nincluding healthcare (Cascella et al., 2023; Yang\net al., 2024), finance (Yu et al., 2023; Li et al.,\n2023; Zhao et al., 2024), law (Siino et al., 2025).\nHowever, in high-stakes contexts such as clinical\ndecision-making, requirements extend beyond pre-\ndiction performance alone. It becomes critical not\nonly to develop reliable predictive models but also\nto maintain fine-grained control over the attributes\ninfluencing predictions. In such scenarios, inter-\npretability is not merely desirable—it is often es-\nsential to comply with regulations (e.g., not using\nprotected features such as gender, occupation, etc.)\nand to foster trust among clinicians, patients, and\nstakeholders.\nClinical notes, a ubiquitous component of elec-\ntronic health record (EHR) systems, constitute one\nof the richest sources of clinical information. These\nfree-text notes can encapsulate nuanced details,\nthus providing invaluable data for predictive mod-\neling. Nonetheless, clinical free-text is inherently\nunstructured, presenting both significant opportu-\nnities and substantial challenges. Notably, clinical\nnotes lack a standardized format, varying exten-\nsively in style, structure, and terminology—not\nonly internationally and across languages but even\nwithin individual hospitals between different de-\npartments or clinicians (Cohen et al., 2019). This\nvariability complicates the development of reliable\nand controllable predictive models.\nTraining predictive models directly on clinical\ntext in a black-box manner raises important con-\ncerns. Firstly, there is often insufficient insight\ninto the exact information leveraged by the model\nto predict an outcome. Clinical notes frequently\ncontain protected attributes, such as race or gender,\nwhich – while potentially relevant in certain con-\ntexts – may inadvertently introduce biases if not\nexplicitly managed. Additionally, clinical text is\nsusceptible to label leakage, especially for tasks\nsuch as mortality prediction, where phrases like\n“the patient has passed away” or “in critical condi-\ntion” can directly reveal outcomes. Consequently,\nmodels might learn superficial shortcuts, inflating\nperformance metrics without capturing meaningful\nclinical signals. Moreover, models optimized on\na test set from the same data distribution as train-\ning may fail to generalize effectively to external\ndatasets from differing institutions, regions, or pop-\nulations. Variations in documentation practices,\nterminology, and structure can potentially degrade\nperformance in real-world, cross-site deployments.\nSeveral approaches have been proposed to miti-\ngate the challenges posed by unstructured clinical\ntext. These include: (1) bias mitigation methods,\nwhich either remove sensitive attributes such as\nrace or gender from input data or transform in-\nput to an intermediate representations that obscure\narXiv:2511.11883v1  [cs.CL]  14 Nov 2025\n"}, {"page": 2, "text": "Figure 1: Illustration of the ClinStructor Pipeline: The proposed pipeline comprises three main stages: (1)\nFeature Identification, (2) Feature Extraction, and (3) fine-tuning.\nsensitive information (Sun et al., 2019); (2) distri-\nbutional robustness techniques, explicitly designed\nto improve resilience against shifts in data distribu-\ntions across EHR databases or clinical institutes or\npatient populations (Tan et al., 2022); and (3) post-\nhoc explanation techniques, such as highlighting\ntextual spans most influential to predictions (Vig,\n2019). While valuable, these methods frequently\nfail to address the root cause of many challenges\n– the fundamentally unstructured nature of clini-\ncal text, making it inherently difficult to ascertain\nwhich information the model has utilized.\nTo address the underlying issue of clinical text’s\ninherent lack of structure, we introduce ClinStruc-\ntor, illustrated in Figure 1.\nClinStructor is a\nnovel pipeline designed to transform clinical notes\ninto structured representations (question-answer\npairs), prior to predictive modeling. Instead of\ndirectly fine-tuning models on raw notes, we lever-\nage LLMs to systematically extract meaningful,\ntask-relevant features, in our case, ICU mortality\nprediction. Specifically, we prompt LLMs to iden-\ntify a set of clinically relevant questions informed\nby both clinical domain knowledge and by task-\nspecific data. Subsequently, the same LLMs extract\nanswers to these questions from the patient’s admis-\nsion notes, resulting in structured question-answer\npairs. This effectively converts the unstructured\nclinical text into structured data, significantly en-\nhancing transparency and controllability. By stan-\ndardizing the input format, our approach enables\nthorough inspection of the extracted features, facil-\nitating verification of their clinical relevance and\nconsistency. While subsequent fine-tuning of pre-\ndictive models on these structured representations\nslightly diminishes interpretability, it allows lever-\naging pretrained LLM knowledge for the down-\nstream predictive task. Thus, our approach bridges\nthe expressive richness of unstructured text with\nthe interpretability, reliability, and generalizability\ninherent in structured data representations.\nWe empirically evaluate ClinStructor against\ndirect fine-tuning on raw clinical notes. Our re-\nsults indicate a modest performance trade-off, a\ndecrease of approximately 2 to 3% in AUC. The\nobserved performance reduction is anticipated due\nto potential information loss inherent to structured\ntransformations. Nevertheless, this trade-off de-\nlivers substantial advantages. Explicitly defining\nfeatures used for prediction enables inspection\nand verification of input signals, effectively mit-\nigating label leakage concerns. Importantly, our\napproach provides foundational building blocks\nfor constructing predictive models exhibiting en-\nhanced interpretability, robustness, and generaliz-\nability—especially crucial when deployed across\ndiverse clinical environments characterized by sub-\nstantial variability in documentation practices.\nThe remainder of this paper is structured as\nfollows. Section 2 discusses background and re-\n"}, {"page": 3, "text": "lated literature. Section 3 details our ClinStruc-\ntor pipeline, describing each stage comprehen-\nsively. Section 4 outlines our experimental setup\nand presents comparative quantitative results. Fur-\nther analytical insights are provided in Section 5.\nIn Section 6, we highlight our method’s limitations\nand propose mitigation strategies. Finally, Sec-\ntion 7 summarizes key insights and concludes.\nThe primary contributions of our work include:\n1. Introducing ClinStructor, a novel LLM-\nbased pipeline converting unstructured clin-\nical notes into structured,\ntask-specific\nquestion-answer pairs.\n2. Empirically demonstrating that ClinStructor\nretains the majority of clinically relevant in-\nformation, achieving competitive predictive\nperformance in the ICU Mortality prediction\ntask.\n3. Discussing crucial limitations of our struc-\ntured approach, along with detailed sugges-\ntions for addressing these issues in future re-\nsearch.\n2\nBackground and Related Works\n2.1\nOverview of Interpretability Techniques\nInterpretability is a crucial and growing focus\nwithin Machine Learning research (Molnar, 2020;\nDu et al., 2019). Traditional models, such as lin-\near regression, Support Vector Machines, deci-\nsion trees, and AdaBoost, inherently provide in-\nterpretability for tabular data (numerical and cat-\negorical). However, interpretability significantly\ndiminishes with more complex modalities, such as\ntextual or image data, which typically require more\nsophisticated modeling approaches.\nFor text-based data, several post-hoc methods\nhave been proposed to explain model predictions.\nNotable examples include SHapley Additive exPla-\nnations (SHAP)(Lundberg and Lee, 2017), LIME\nfor Text(Ribeiro et al., 2016), Saliency Maps (Si-\nmonyan et al., 2014), attention visualization tech-\nniques (Vig, 2019; Tenney et al., 2020), and Inte-\ngrated Gradients (Sundararajan et al., 2017). These\napproaches primarily rely on highlighting influen-\ntial segments of text to explain predictions. How-\never, these explanations may not be faithful or reli-\nable.\nAlternatively,\ninterpretability\ncan\nalso\nbe\nachieved through similarity or example-based meth-\nods.\nTechniques such as K-Nearest Neighbors\n(K-NN) applied to text embeddings, Prototypical\nNetworks (Snell et al., 2017), Case-Based Reason-\ning (Wiratunga et al., 2024), counterfactual expla-\nnations (Wachter et al., 2018), TracIn (Pruthi et al.,\n2020; K and Søgaard, 2021), and Influence Func-\ntions (Koh and Liang, 2020) provide interpretabil-\nity by relating predictions to influential or similar\ntraining examples. These explanation methods are\noften very unstable (K and Søgaard, 2021).\n2.2\nA Broad Overview of Applications of\nLLMs in Healthcare\nIn recent years, there has been significant interest\nin applying Large Language Models (LLMs) to\nhealthcare. Early studies (Thirunavukarasu et al.,\n2023; Yang et al., 2023; Nazi and Peng, 2024;\nNassiri and Akhloufi, 2024) investigated the po-\ntential of LLMs in clinical practice, highlighting\nboth opportunities and key challenges. Further\nresearch (Denecke et al., 2024) has collected clin-\nician perspectives, providing insights into practi-\ncal strengths and limitations when deploying these\nmodels.\nLLMs have demonstrated effectiveness across\nvarious clinical tasks. For example, they have been\nused to detect and anonymize Personally Identifi-\nable Information (PII) within clinical notes (Liu\net al., 2024; K et al., 2023). They can also effec-\ntively perform information extraction tasks from\nunstructured text (Agrawal et al., 2022; Lopez\net al., 2025). Moreover, LLMs have been eval-\nuated for diagnostic support (Panagoulias et al.,\n2023, 2024), summarizing medical records (Madz-\nime and Nyirenda, 2024; Goodman et al., 2024),\nclinical question answering (Wang et al., 2023; Li\net al., 2024b), medical coding (Soroush et al., 2024;\nLi et al., 2024a), and predictive modeling of patient\noutcomes (Lyu et al., 2023; Wu et al., 2023; van\nAken et al., 2021; Röhr et al., 2024). Beyond clini-\ncal tasks, LLMs have also shown promise in med-\nical education and training (Safranek et al., 2023;\nAbd-Alrazaq et al., 2023; Lucas et al., 2024).\n2.3\nSpecialized LLMs in Healthcare and\nMedical domain\nTo better address healthcare specific applications,\nseveral domain-adapted LLMs have been proposed.\nNotable examples include Med-BERT (Rasmy\net al., 2021), ClinicalBERT (Huang et al., 2019),\nand BioBERT (Lee et al., 2020), all pretrained on\nspecialized clinical and biomedical datasets. More\nrecent models like Meditron (Chen et al., 2023;\n"}, {"page": 4, "text": "Bosselut et al., 2024), Med-PaLM (Singhal et al.,\n2022), and BioMistral (Labrak et al., 2024) fur-\nther aim to improve performance on medical and\nclinical benchmarks. In section 4, we use Meditron-\nbased models (Meditron Qwen2.5 7B and Meditron\nLLaMA 3.1 8B) for our experiments, as these are\ncomparable to their general-purpose counterparts.\n2.4\nLLMs for Feature Importance\nRecent works have experimented with applying\nLLM-based approach for feature engineering, pri-\nmarily targeting tabular datasets (Tsymbalov and\nSavchenko, 2024; Zhang et al., 2024). In addition,\nother works have explored leveraging LLMs for\nfeature selection and importance estimation, partic-\nularly within zero-shot settings (Jeong et al., 2024;\nChoi et al., 2022). Similar to these studies, our\napproach utilizes LLMs not only to select but also\nto generate meaningful features, assigning them\nappropriate importance scores.\n2.5\nText Bottleneck Model\nRelatively few inherently interpretable methods ex-\nist for textual data. One notable example is the Text\nBottleneck Model (TBM) proposed by Ludan et al.\n(2024). Inspired by Concept Bottleneck Models\n(Koh et al., 2020), TBM employs large language\nmodels (LLMs) to iteratively extract key textual\nconcepts (e.g., “service” or “food quality” in senti-\nment classification) and builds interpretable models\nbased on these concepts. TBM’s methodology is\niterative: in each round, the model is trained on the\nfull dataset, and high-loss examples are used to ex-\ntract additional concepts. This process is repeated\nmultiple times, making it computationally expen-\nsive. Due to this, TBM faces significant scalability\nchallenges. Its evaluation was limited to simple\ntasks such as sentiment analysis, using only small\ndatasets (approximately 250 examples). In con-\ntrast, our proposed approach extracts all relevant\nfeatures in a single step, offering a scalable and\nstructured alternative to TBM. The scalability of\nour proposed approach makes it suitable for more\ncomplex, real-world tasks.\n3\nOur Proposed Method: ClinStructor\nIn Figure 1, we illustrate our proposed method,\nClinStructor, which converts unstructured clinical\nnotes into structured representations, thereby en-\nhancing their interpretability and control over the\nattributes used for downstream tasks. Our method\ninvolves three key steps: (1) Feature Identification,\n(2) Feature Extraction, and (3) Fine-tuning. Below,\nwe detail each step thoroughly.\n3.1\nStage 1: Feature Identification\nThe first step, Feature Identification, establishes the\nfoundation for the structured representation of clin-\nical notes. Our goal is to discover a meaningful set\nof features that can be extracted from free-text ad-\nmission notes and utilized for predictive modeling;\nspecifically, we focus on ICU mortality prediction\nin this study. This step leverages both the large\nlanguage model’s (LLM) inherent world knowl-\nedge as well as the information in examples from\ndownstream predictive task.\nWe begin by randomly sampling 1,000 ICU ad-\nmission notes, equally balanced between positive\n(mortality) and negative (survival) outcomes. For\neach note, we prompt an LLM to generate a set\nof 20 potential candidate features. Each feature\nconsists of three attributes:\n• Question: A natural language question whose\nanswer serves as the feature value. We instruct\nthe LLM to produce generalizable questions\n(e.g., \"What is the patient’s age?\" rather than\n\"Is the patient’s age 60?\").\n• Feature Name: A concise, one-word descrip-\ntor for the feature (e.g., age, medication, etc.).\nThis primarily aids in subsequent clustering\nand de-duplication.\n• Importance Score: A numeric value between\n0 and 1, reflecting the LLM’s estimated im-\nportance of the feature for the downstream\npredictive task.\nThis process yields a total of 20,000 candidate\nfeatures (20 features per note × 1,000 notes). We\nhave to select the top K(= 50) features. However,\ndue to natural variation in language, many gener-\nated questions and feature names differ slightly de-\nspite having similar semantics. For example, \"What\nis the patient’s current age?\" vs. \"What is the pa-\ntient’s age?\", or feature names like \"current_age\"\nvs. \"age\". Consequently, simple strategies such as\nfrequency-based selection would incorrectly seg-\nment semantically equivalent features, thereby un-\nderestimating their true frequency/importance.\nTo address this issue, we perform single-linkage\nclustering (Vijaya et al., 2019) over the set of gen-\nerated features. Each node in the clustering graph\nrepresents one LLM-generated feature candidate\n"}, {"page": 5, "text": "Rank\nQuestion\nQwen 32b Instruct\n1\nwhat is the patient’s age?\n2\nwhat medications was the patient on at the time of admission?\n3\nwhat is the patient’s vital signs on admission?\n4\nwhat is the patient’s physical exam findings?\n5\nwhat are the patient’s significant past medical conditions?\nLLaMA 70B Instruct\n1\nwhat is the patient’s medical history?\n2\nwhat is the patient’s blood pressure on admission?\n3\nwhat is the patient’s current level of cognitive function?\n4\nwhat are the patient’s current medications and dosages?\n5\nwhat is the patient’s age?\nTable 1: Top 5 questions generated by Qwen 32B and LLaMA 70B Instruct models\n(i.e., a question, a feature name, and an importance\nscore). We define an edge between two nodes if\nthey share either the exact same question or the\nexact same feature name (after basic normalization\nsteps such as lower casing). Single-linkage cluster-\ning on this graph results in clusters of semantically\nequivalent questions.\nNext, we compute the cluster weight as the sum\nof LLM-generated feature importance scores for\nall nodes within a cluster. We then select the top K\nclusters (K = 50) based on these aggregate cluster\nweights to form our final set of features. Man-\nual inspection revealed no semantically duplicate\nquestions, indicating that the clustering process\neffectively ensures question uniqueness. Within\neach selected cluster, we identify a representative\nquestion—specifically, the one with the highest cu-\nmulative importance score, calculated as the sum\nof feature importance scores across all exact oc-\ncurrences of that question (not the feature names).\nThis process results in a curated list of 50 unique,\nLLM-generated questions, ranked by their aggre-\ngate importance scores. These questions define\nthe structured format used for subsequent feature\nextraction. It is important to note that we utilize\ndownstream task data in two ways: (1) as input\nto the LLM to generate candidate features, and\n(2) during clustering, to select the top K features\nbased on importance. Meanwhile, the LLM’s inter-\nnal knowledge contributes to generating relevant\nquestions, feature names, and corresponding impor-\ntance scores. Table 1 presents the top 5 generated\nquestions from each of the Qwen2.5 32B Instruct\nand LLaMA 3.3 70B Instruct models.\n3.2\nStage 2: Feature Extraction\nIn this step, our objective is to extract the previ-\nously identified features from each patient’s ad-\nmission note. Again, we leverage large language\nmodels (LLMs) for this task. For each patient in\nthe dataset, we provide the LLM with the complete\nadmission note along with the set of 50 curated\nquestions obtained from the Feature Identification\nstep. The LLM is instructed to generate an answer\nfor each question based exclusively on the infor-\nmation present in the given admission note. To\nhandle situations where the required information is\nmissing or the question is not applicable, the LLM\nis explicitly instructed to respond with \"N/A\".\nConsequently, at the end of this step, each pa-\ntient record—across training, validation, and test\nsets—is associated with a structured set of 50\nquestion-answer pairs. Importantly, the same set of\nquestions (features) is consistently applied across\nall examples in the dataset. This representation\noffers a structured yet flexible format, as the an-\nswers retain their original free-text semantic rich-\nness while capturing clinically meaningful infor-\nmation in a standardized manner.\n3.3\nStage 3: Fine-tuning\nIn the final stage, we fine-tune a smaller LLM to\npredict the mortality outcome (alive or deceased).\nThe input to the model is a single concatenated se-\nquence comprising all 50 question-answer pairs for\neach patient. The model is trained using standard\n"}, {"page": 6, "text": "Feature Identification & Ex-\ntraction Model\nFine-tune model\nClinStructor\nfinetuned\nFull\nadmis-\nsion notes\nLLaMA 3.3 70B Instruct\nLLaMA-3.1-8B\n0.854\n0.882\nMeditron3-8B\n0.860\n0.889\nLLaMA-3.2-3B\n0.852\n0.879\nLLaMA-3.2-1B\n0.819\n0.862\nBest\n0.860\n0.889\nQwen 2.5 32b Instruct\nQwen 2.5 7B\n0.869\n0.882\nMeditron3-Qwen2.5-7B\n0.860\n0.881\nQwen2.5-3B\n0.851\n0.878\nQwen2.5-0.5B\n0.815\n0.856\nBest\n0.869\n0.882\nTable 2: Direct Fine-Tuning vs. ClinStructor Fine-Tuning: As expected, direct fine-tuning performs slightly\nbetter than ClinStructor. However, ClinStructor still achieves comparable performance, indicating that it captures\nmost, if not all, of the relevant information.\nbinary cross-entropy loss.\n4\nExperiments\n4.1\nDataset\nIn our experiments, we use the MIMIC-III Clinical\nDatabase (version 1.4) and follow the preprocess-\ning steps outlined by van Aken et al. (2021). Specif-\nically, van Aken et al. (2021) extracted discharge\nsummaries but retained only sections containing\ninformation known at the time of admission. This\neffectively reconstructs admission notes from the\noriginal discharge summaries.\nAfter preprocessing, we observe significant class\nimbalance, with negative cases (patients who sur-\nvived) considerably outnumbering positive cases\n(patients who were deceased). While additional\nnegative examples can provide valuable signals dur-\ning training, they significantly increase computa-\ntional cost. To achieve a practical balance between\ncomputational efficiency and model performance,\nwe randomly subsample the negative examples so\nthat their number matches that of positive cases.\nThis results in a balanced dataset for training, vali-\ndation, and testing\nWe employ AUC-ROC as our primary evaluation\nmetric, chosen for its robustness to class imbalance\nand its common usage in clinical risk prediction\nstudies. After subsampling, the final dataset com-\nprises 7,078 training examples, 1,038 validation\nexamples, and 2,050 test examples.\n4.2\nDirect fine-tuning vs. ClinStructor\nfine-tuning\nIn this section, we compare two methods for train-\ning Large Language Models (LLMs) to predict ICU\nmortality:\n1. Direct fine-tuning: The model is directly\ntrained on raw, unstructured admission notes.\n2. ClinStructor fine-tuning:\nThe model is\ntrained on structured data, specifically a set of\n50 question-answer pairs extracted using our\nproposed method.\nThis comparison aims to determine whether\nstructuring clinical text via LLM-driven feature\nextraction results in any loss of predictive infor-\nmation, if so, how much it affects the downstream\nprediction performance.\nBoth fine-tuning approaches use the same opti-\nmization setup. We employ Low-Rank Adaptation\n(LoRA) for parameter-efficient fine-tuning and op-\ntimize using binary cross-entropy loss. Models are\ntrained for 5 epochs, and their performance is eval-\nuated every 100 steps using the validation set. For\neach hyperparameter configuration, we select the\ncheckpoint with the lowest validation loss for final\nevaluation on the test set. We perform a grid search\nover two learning rates (1e-4 and 2e-4) and two\nbatch sizes (8 and 16), and report the test results\ncorresponding to the hyperparameter with the best\nvalidation performance.\nTo assess the impact of model size and domain-\nspecific clinical knowledge, we experiment with\n"}, {"page": 7, "text": "various LLMs.\nThese include general-purpose\nmodels such as Qwen 2.5 (7B, 3B, 0.5B) and\nLLaMA 3.1 (8B), LLaMA 3.2 (3B, 1B), as well as\nclinical-specialized variants like Meditron3-Qwen\n2.5 (7B) and Meditron3-LLaMA 3.1 (8B).\nWe use the LLaMA 70B Instruct and Qwen\n2.5 32B Instruct models for the Feature Identifi-\ncation (Stage 1) and Feature Extraction (Stage 2)\nphases described previously. These larger models\nare solely used for structuring the data and are not\ninvolved in any downstream fine-tuning or predic-\ntion tasks, ensuring fairness in our experimental\ncomparison.\n4.3\nResults\nTable 2 shows that ClinStructor fine-tuning\nachieves performance competitive with that of Di-\nrect fine-tuning on raw admission notes. Specifi-\ncally, the performance difference in AUC is less\nthan 3% for the best-performing LLaMA models\nand less than 2% for the best-performing Qwen\nmodels. Although there is a slight decrease in per-\nformance, in ClinStructor, we are able to control\nexactly what information is used for making the\nprediction.\n5\nAnalysis\n5.1\nEffective number of features\nAlthough 50 question-answer pairs per admission\nnote may appear sufficiently large to capture most\nrelevant clinical information, not every question\nis applicable to every patient. In practice, many\nof the answers returned for a given patient are\n\"N/A\"—indicating either that the question is not\nrelevant or that the necessary information is not\npresent in the admission note. As shown in Fig-\nure 2, for LLaMA 70B and Qwen 32b models,\nnearly 40% and 50% of the answers, respectively,\nare \"N/A\", thereby reducing the effective number\nof useful question-answer pairs per patient.\n5.2\nEffect of number of Questions:\nIntuitively, the more number of questions we select,\nthe more information we can retain and hence the\nlesser the performance loss. However, using more\nquestions increases the difficulty of interpretabil-\nity and the computational cost. To understand the\ntradeoff between performance and number of ques-\ntions, we conduct experiment with various numbers\nof questions. Instead of selecting top 50 questions,\nwe choose top k questions for K = 10, 20, 30, 40\nand 50. We fine-tune Qwen 2.5 7B model and\nLLaMA 3.1 8B models, for each, we chose the best\nperforming hyperparameter from Table 2 and used\nthe same for all different values of K.\nFrom Table 3, we can see that while there is\nan overall trend (although noisy) of increase in\nperformance with number of questions, even with\nfewer questions, like 10 for LLaMA and 20 for\nQwen, the performance remains close to that of\nusing all the 50 questions.\n6\nDiscussion\nPerformance.\nWhile ClinStructor provides\ngreater control over which features are used for\nprediction, this often comes at the cost of reduced\nperformance. One straightforward way to mitigate\nthis trade-off is to use more powerful language\nmodels, such as GPT-4.5 or Gemini Pro, during\nthe feature identification and extraction steps. Due\nto licensing restrictions with the MIMIC dataset,\nwe do not use OpenAI models in our current\nexperiments. However, future work can explore\nthe use of Gemini models through Vertex AI,\nwhich complies with MIMIC’s data usage policies.\nAdditionally,\nfurther\nimprovements\nmay\nbe\nachieved by tuning more hyperparameters or signif-\nicantly increasing the number of questions used in\nthe pipeline (e.g., scaling to hundreds of questions).\nInterpretability. ClinStructor marks an important\nfirst step toward building interpretable clinical\nmodels. In its current form, it enables us to answer\nthe question: Which features were used to make\na prediction? However, it does not yet provide\ninsights into how each feature influences the\nprediction outcome. While techniques like saliency\nmaps or attention-based visualizations offer partial\nexplanations, they are often unreliable or lack\nfidelity to the model’s actual decision-making\nprocess. A more principled extension—drawing\ninspiration\nfrom\nNeural\nAdditive\nModels\n(NAMs) (Agarwal et al., 2021)—could improve\ninterpretability.\nNAMs are a class of models\nthat combine the interpretability of generalized\nadditive models with the flexibility of neural\nnetworks by learning a separate sub-network\nfor each input feature.\nSpecifically, instead of\nregular fine-tuning in Stage 3 of ClinStructor, we\ncould design a NAM-like architecture where each\nquestion (feature) is processed independently to\nproduce a logit, and the final prediction is obtained\n"}, {"page": 8, "text": "Figure 2: Effective Number of Questions: The plot shows the distribution of the effective number of questions—i.e.,\nthe number of questions with non-\"N/A\" answers, for both Qwen-32B and LLaMA-70B models. Note that\napproximately 40% of the answers from LLaMA-70B and 50% from Qwen-32B are \"N/A\".\nModel\nTop 10\nTop 20\nTop 30\nTop 40\nTop 50\nQwen2.5 7B\n0.741\n0.849\n0.850\n0.848\n0.869\nLLaMA 3.1 8B\n0.849\n0.847\n0.853\n0.856\n0.854\nTable 3: Number of Features vs. Performance: AUC-ROC of Qwen2.5-7B and LLaMA 3.1-8B ClinStructor fine-\ntuned models using only the top-K features. For feature extraction, we use Qwen-32B Instruct and LLaMA3.3-70B\nInstruct models for Qwen2.5 7B and LLaMA 3.1 8B, respectively.\nthrough a linear combination of these logits. This\napproach would allow us to directly quantify the\ncontribution of each feature to the final prediction.\nAnother possible direction is to guide the LLM\nto generate questions with strictly numerical or\ncategorical answers, enabling the use of simpler,\ninherently interpretable models on top of these\nfeatures. However, this approach may require a\nsignificantly larger number of questions, which\nwould increase computational costs and potentially\nreduce performance, as it limits the model’s ability\nto leverage pre-trained knowledge (which the\ncurrent fine-tuned models can leverage).\n7\nConclusion\nIn this work, we introduce ClinStructor, an LLM-\nbased pipeline that converts unstructured clinical\nnotes into structured representations, which are\nthen used to train a predictive model. This approach\nenhances interpretability and provides control over\nwhich features contribute to a prediction—both of\nwhich are critical in clinical decision-making.\nWe evaluate ClinStructor on the MIMIC ICU\nmortality prediction task using admission notes.\nOur proposed method results in only a modest drop\nin AUC (2–3%) while offering greater control over\nthe features used in prediction. Further analysis\nreveals that, on average, only 50–60% of the gen-\nerated questions are relevant for a given patient’s\nnotes. Interestingly, using just the top 10 most in-\nformative questions yields performance close to\nthat of using all 50.\nClinStructor also addresses common challenges\nin clinical machine learning. By manually review-\ning and selecting the final set of questions, we can\nreduce the risk of unintended biases. The consistent\ninput format—50 standardized questions and an-\nswers—improves generalizability across different\nEHR systems and further enhances interpretability.\nOverall, this work demonstrates the promise of us-\ning large language models not only as predictive\ntools but also as enablers of transparent, robust, and\ncontrollable clinical machine learning systems.\n8\nLimitations\nOur proposed approach results in a slight decrease\nin performance. While the outputs are interpretable\nto some extent, full transparency is lacking—we\ncan identify which features are used in making\na prediction, but not how they influence the out-\ncome. The approach relies on large language mod-\n"}, {"page": 9, "text": "els (LLMs) to transform data, which introduces\nconcerns about potential hallucinations. Moreover,\nleveraging more powerful LLMs may require send-\ning data to external services, raising additional chal-\nlenges related to privacy and control. Fine-tuning\nthese models locally also demands substantial com-\nputational resources.\nEthical considerations\nThis work involves healthcare-related tasks, a do-\nmain that requires careful handling due to its\nsensitive nature. To the best of our knowledge,\nall experiments were conducted using the latest\nPII-deidentified data from the MIMIC database.\nWe strictly followed the guidelines provided by\nMIMIC and ensured that no OpenAI models were\nused. All large language models (LLMs) were\nhosted locally to minimize the risk of data leakage\nand to maintain data privacy.\nThe MIMIC dataset is approved for research use,\nand all the LLMs employed in this study are also\npermitted for research purposes.\nReferences\nAlaa Abd-Alrazaq, Rawan AlSaad, Dari Alhuwail, Ar-\nfan Ahmed, Padraig Mark Healy, Syed Latifi, Sarah\nAziz, Rafat Damseh, Sadam Alabed Alrazak, Javaid\nSheikh, and 1 others. 2023. Large language mod-\nels in medical education: opportunities, challenges,\nand future directions.\nJMIR Medical Education,\n9(1):e48291.\nRishabh Agarwal, Levi Melnick, Nicholas Frosst,\nXuezhou Zhang, Ben Lengerich, Rich Caruana,\nand Geoffrey Hinton. 2021. Neural additive mod-\nels: Interpretable machine learning with neural nets.\nPreprint, arXiv:2004.13912.\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David Sontag. 2022. Large language\nmodels are few-shot clinical information extractors.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1998–2022, Abu Dhabi, United Arab Emirates. Asso-\nciation for Computational Linguistics.\nA Bosselut,\nZ Chen,\nA Romanou,\nA Bonnet,\nA Hernández-Cano, B Alkhamissi, K Matoba,\nF Salvi, M Pagliardini, S Fan, and 1 others. 2024.\nMeditron: Open medical foundation models adapted\nfor clinical practice.\nMarco Cascella, Jonathan Montomoli, Valentina Bellini,\nand Elena Bignami. 2023. Evaluating the feasibil-\nity of chatgpt in healthcare: an analysis of multiple\nclinical and research scenarios. Journal of medical\nsystems, 47(1):33.\nZeming Chen, Alejandro Hernández Cano, Angelika\nRomanou, Antoine Bonnet, Kyle Matoba, Francesco\nSalvi, Matteo Pagliardini, Simin Fan, Andreas\nKöpf, Amirkeivan Mohtashami, and 1 others. 2023.\nMeditron-70b: Scaling medical pretraining for large\nlanguage models. arXiv preprint arXiv:2311.16079.\nKristy Choi, Chris Cundy, Sanjari Srivastava, and\nStefano Ermon. 2022. Lmpriors: Pre-trained lan-\nguage models as task-specific priors.\nPreprint,\narXiv:2210.12530.\nGenna R. Cohen, Charles P. Friedman, Andrew M.\nRyan, Caroline R. Richardson, and Julia Adler-\nMilstein. 2019. Variation in physicians’ electronic\nhealth record documentation and potential patient\nharm from that variation. Journal of General Inter-\nnal Medicine, 34(11):2355–2367.\nKerstin Denecke, Richard May, LLMHealthGroup, and\nOctavio Rivera Romero. 2024. Potential of large lan-\nguage models in health care: Delphi study. Journal\nof Medical Internet Research, 26:e52399.\nMengnan Du, Ninghao Liu, and Xia Hu. 2019. Tech-\nniques for interpretable machine learning. Communi-\ncations of the ACM, 63(1):68–77.\nKatherine E Goodman, H Yi Paul, and Daniel J Morgan.\n2024. Ai-generated clinical summaries require more\nthan accuracy. JAMA, 331(8):637–638.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and pre-\ndicting hospital readmission. arXiv:1904.05342.\nDaniel P. Jeong, Zachary Chase Lipton, and Pradeep\nRavikumar. 2024. Llm-select: Feature selection with\nlarge language models. ArXiv, abs/2407.02694.\nKarthikeyan K and Anders Søgaard. 2021. Revisiting\nmethods for finding influential examples. Preprint,\narXiv:2111.04683.\nKarthikeyan K, Yogarshi Vyas, Jie Ma, Giovanni\nPaolini, Neha John, Shuai Wang, Yassine Benajiba,\nVittorio Castelli, Dan Roth, and Miguel Ballesteros.\n2023. Taxonomy expansion for named entity recog-\nnition. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6895–6906, Singapore. Association for Com-\nputational Linguistics.\nPang Wei Koh and Percy Liang. 2020. Understand-\ning black-box predictions via influence functions.\nPreprint, arXiv:1703.04730.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen\nMussmann, Emma Pierson, Been Kim, and Percy\nLiang. 2020. Concept bottleneck models. Preprint,\narXiv:2007.04612.\nYanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-\nAntoine Gourraud, Mickael Rouvier, and Richard\nDufour. 2024.\nBiomistral: A collection of open-\nsource pretrained large language models for medical\ndomains. Preprint, arXiv:2402.10373.\n"}, {"page": 10, "text": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nRumeng Li, Xun Wang, and Hong Yu. 2024a. Explor-\ning llm multi-agents for icd coding. arXiv preprint\narXiv:2406.15363.\nStella Li, Vidhisha Balachandran, Shangbin Feng,\nJonathan Ilgen, Emma Pierson, Pang Wei W Koh,\nand Yulia Tsvetkov. 2024b. Mediq: Question-asking\nllms and a benchmark for reliable interactive clinical\nreasoning. Advances in Neural Information Process-\ning Systems, 37:28858–28888.\nYinheng Li, Shaofei Wang, Han Ding, and Hang Chen.\n2023. Large language models in finance: A survey.\nIn Proceedings of the fourth ACM international con-\nference on AI in finance, pages 374–382.\nYupei Liu, Yuqi Jia, Jinyuan Jia, and Neil Zhenqiang\nGong. 2024. Evaluating large language model based\npersonal information extraction and countermeasures.\narXiv preprint arXiv:2408.07291.\nIvan Lopez, Akshay Swaminathan, Karthik Vedula,\nSanjana Narayanan, Fateme Nateghi Haredasht,\nStephen P Ma, April S Liang, Steven Tate, Manoj\nMaddali, Robert Joseph Gallo, and 1 others. 2025.\nClinical entity augmented retrieval for clinical infor-\nmation extraction. npj Digital Medicine, 8(1):45.\nHarrison C Lucas, Jeffrey S Upperman, and Jamie R\nRobinson. 2024. A systematic review of large lan-\nguage models and their implications in medical edu-\ncation. Medical Education, 58(11):1276–1285.\nJosh Magnus Ludan, Qing Lyu, Yue Yang, Liam Dugan,\nMark Yatskar, and Chris Callison-Burch. 2024.\nInterpretable-by-design text understanding with it-\neratively generated concept bottleneck.\nPreprint,\narXiv:2310.19660.\nScott M Lundberg and Su-In Lee. 2017.\nA unified\napproach to interpreting model predictions. In Ad-\nvances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc.\nWeimin Lyu, Xinyu Dong, Rachel Wong, Songzhu\nZheng, Kayley Abell-Hart, Fusheng Wang, and Chao\nChen. 2023.\nA multimodal transformer: Fusing\nclinical notes with structured ehr data for inter-\npretable in-hospital mortality prediction. Preprint,\narXiv:2208.10240.\nRuvarashe Madzime and Clement Nyirenda. 2024. En-\nhanced electronic health records text summariza-\ntion using large language models. arXiv preprint\narXiv:2410.09628.\nChristoph Molnar. 2020. Interpretable machine learn-\ning. Lulu. com.\nKhalid Nassiri and Moulay A Akhloufi. 2024. Recent\nadvances in large language models for healthcare.\nBioMedInformatics, 4(2):1097–1143.\nZabir Al Nazi and Wei Peng. 2024. Large language\nmodels in healthcare and medical domain: A review.\nIn Informatics, volume 11, page 57. MDPI.\nDimitrios P Panagoulias, Filippos A Palamidas, Maria\nVirvou, and George A Tsihrintzis. 2023. Evaluating\nthe potential of llms and chatgpt on medical diagnosis\nand treatment. In 2023 14th international conference\non information, intelligence, systems & applications\n(IISA), pages 1–9. IEEE.\nDimitrios P Panagoulias, Maria Virvou, and George A\nTsihrintzis. 2024. Evaluating llm–generated multi-\nmodal diagnosis from medical images and symptom\nanalysis. arXiv preprint arXiv:2402.01730.\nGarima Pruthi, Frederick Liu, Mukund Sundararajan,\nand Satyen Kale. 2020.\nEstimating training data\ninfluence by tracing gradient descent.\nPreprint,\narXiv:2002.08484.\nLaila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and\nDegui Zhi. 2021. Med-bert: pretrained contextual-\nized embeddings on large-scale structured electronic\nhealth records for disease prediction. NPJ digital\nmedicine, 4(1):86.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016.\n\"why should i trust you?\": Ex-\nplaining the predictions of any classifier. Preprint,\narXiv:1602.04938.\nTom Röhr, Alexei Figueroa, Jens-Michalis Papaioannou,\nConor Fallon, Keno Bressem, Wolfgang Nejdl, and\nAlexander Löser. 2024. Revisiting clinical outcome\nprediction for MIMIC-IV. In Proceedings of the\n6th Clinical Natural Language Processing Workshop,\npages 208–217, Mexico City, Mexico. Association\nfor Computational Linguistics.\nConrad W Safranek, Anne Elizabeth Sidamon-Eristoff,\nAidan Gilson, and David Chartash. 2023. The role\nof large language models in medical education: ap-\nplications and implications.\nMarco Siino, Mariana Falco, Daniele Croce, and Paolo\nRosso. 2025. Exploring llms applications in law:\nA literature review on current legal nlp approaches.\nIEEE Access.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. 2014. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. Preprint, arXiv:1312.6034.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara\nMahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen\nPfohl, Perry Payne, Martin Seneviratne, Paul Gam-\nble, Chris Kelly, Nathaneal Scharli, Aakanksha\n"}, {"page": 11, "text": "Chowdhery, Philip Mansfield, Blaise Aguera y Ar-\ncas, Dale Webster, and 11 others. 2022. Large lan-\nguage models encode clinical knowledge. Preprint,\narXiv:2212.13138.\nJake Snell, Kevin Swersky, and Richard S. Zemel.\n2017. Prototypical networks for few-shot learning.\nPreprint, arXiv:1703.05175.\nAli Soroush, Benjamin S Glicksberg, Eyal Zimlich-\nman, Yiftach Barash, Robert Freeman, Alexan-\nder W Charney, Girish N Nadkarni, and Eyal Klang.\n2024.\nLarge language models are poor medical\ncoders—benchmarking of medical code querying.\nNEJM AI, 1(5):AIdbp2300040.\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\nMai ElSherief, Jieyu Zhao, Diba Mirza, Eliza-\nbeth Belding, Kai-Wei Chang, and William Yang\nWang. 2019.\nMitigating gender bias in natural\nlanguage processing: Literature review. Preprint,\narXiv:1906.08976.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. Preprint,\narXiv:1703.01365.\nQingyu Tan, Ruidan He, Lidong Bing, and Hwee Tou\nNg. 2022. Domain generalization for text classifica-\ntion with memory-based supervised contrastive learn-\ning. In Proceedings of the 29th International Con-\nference on Computational Linguistics, pages 6916–\n6926, Gyeongju, Republic of Korea. International\nCommittee on Computational Linguistics.\nIan Tenney, James Wexler, Jasmijn Bastings, Tolga\nBolukbasi, Andy Coenen, Sebastian Gehrmann,\nEllen Jiang, Mahima Pushkarna, Carey Radebaugh,\nEmily Reif, and Ann Yuan. 2020. The language inter-\npretability tool: Extensible, interactive visualizations\nand analysis for NLP models.\nArun James Thirunavukarasu, Darren Shu Jeng Ting,\nKabilan Elangovan, Laura Gutierrez, Ting Fang Tan,\nand Daniel Shu Wei Ting. 2023. Large language\nmodels in medicine. Nature medicine, 29(8):1930–\n1940.\nAleksandr Tsymbalov and Andrey Savchenko. 2024.\nLLM2features:\nLarge language models in inter-\npretable feature generation for autoML with tabular\ndata.\nBetty van Aken, Jens-Michalis Papaioannou, Manuel\nMayrdorfer, Klemens Budde, Felix A. Gers, and\nAlexander Löser. 2021. Clinical outcome prediction\nfrom admission notes using self-supervised knowl-\nedge integration. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pages 881–893.\nAssociation for Computational Linguistics.\nJesse Vig. 2019. A multiscale visualization of attention\nin the transformer model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 37–42,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nVijaya, Shweta Sharma, and Neha Batra. 2019. Compar-\native study of single linkage, complete linkage, and\nward method of agglomerative clustering. In 2019\nInternational Conference on Machine Learning, Big\nData, Cloud and Parallel Computing (COMITCon),\npages 568–573.\nSandra Wachter, Brent Mittelstadt, and Chris Russell.\n2018. Counterfactual explanations without opening\nthe black box: Automated decisions and the gdpr.\nPreprint, arXiv:1711.00399.\nYubo Wang, Xueguang Ma, and Wenhu Chen. 2023.\nAugmenting black-box llms with medical text-\nbooks for biomedical question answering (pub-\nlished in findings of emnlp 2024). arXiv preprint\narXiv:2309.02233.\nNirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawar-\ndena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-\nOrji, Ruvan Weerasinghe, Anne Liret, and Bruno\nFleisch. 2024. Cbr-rag: Case-based reasoning for\nretrieval augmented generation in llms for legal ques-\ntion answering. Preprint, arXiv:2404.04302.\nJun Wu, Xuesong Ye, Chengjie Mou, and Weinan\nDai. 2023. Fineehr: Refine clinical note represen-\ntations to improve mortality prediction. Preprint,\narXiv:2304.11794.\nRui Yang, Ting Fang Tan, Wei Lu, Arun James\nThirunavukarasu, Daniel Shu Wei Ting, and Nan\nLiu. 2023. Large language models in health care:\nDevelopment, applications, and challenges. Health\nCare Science, 2(4):255–263.\nZiqi Yang,\nXuhai Xu,\nBingsheng Yao,\nEthan\nRogers, Shao Zhang, Stephen Intille, Nawar Shara,\nGuodong Gordon Gao, and Dakuo Wang. 2024.\nTalk2care: An llm-based voice assistant for com-\nmunication between healthcare providers and older\nadults. Proceedings of the ACM on Interactive, Mo-\nbile, Wearable and Ubiquitous Technologies, 8(2):1–\n35.\nXinli Yu, Zheng Chen, Yuan Ling, Shujing Dong,\nZongyi Liu, and Yanbin Lu. 2023. Temporal data\nmeets llm–explainable financial time series forecast-\ning. arXiv preprint arXiv:2306.11025.\nXinhao Zhang, Jinghan Zhang, Banafsheh Rekabdar,\nYuanchun Zhou, Pengfei Wang, and Kunpeng Liu.\n2024. Dynamic and adaptive feature generation with\nllm. Preprint, arXiv:2406.03505.\nHuaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li,\nTianze Yang, Peng Shu, Shaochen Xu, Haixing Dai,\nLin Zhao, Gengchen Mai, and 1 others. 2024. Revo-\nlutionizing finance with llms: An overview of applica-\ntions and insights. arXiv preprint arXiv:2401.11641.\n"}, {"page": 12, "text": "A\n50 Questions from Qwen 32b model\n1. what is the patient's age?\n2. what medications was the\npatient on at the time of\nadmission?\n3. what is the patient's current\nvital signs?\n4. what is the patient's\nphysical exam status?\n5. what are the patient's\nsignificant past medical\nconditions?\n6. what is the patient's history\nof previous surgeries?\n7. what is the patient's history\nof recent infections?\n8. what is the patient's family\nhistory?\n9. what is the patient's history\nof allergies?\n10. what is the patient's\nhistory of recent lab results?\n11. what is the patient's\nhistory of social habits?\n12. what is the patient's\nhistory of renal disease?\n13. what is the patient's\nprimary diagnosis?\n14. what is the patient's\nhistory of recent changes in\nrespiratory status?\n15. what is the patient's recent\nimaging results?\n16. what is the patient's\ncurrent mental status?\n17. what is the patient's\nhistory of cardiovascular\nevents?\n18. what is the patient 's recent\nnutritional status?\n19. what is the patient's\nhistory of hospitalizations?\n20. what is the patient's\ncurrent cardiovascular status?\n21. what is the patient's\ncurrent neurological status?\n22. what is the patient's\nhistory of chronic conditions?\n23. what is the patient's\nhistory of respiratory\nconditions?\n24. what is the patient's\ncurrent pain level?\n25. what is the patient's\nhistory of recent changes in\nliver function?\n26. what is the patient's\nhistory of neurological\ndisorders?\n27. what is the patient's\ncurrent oxygen saturation?\n28. what is the patient's\nhistory of trauma?\n29. what is the patient's\ncurrent level of\nconsciousness?\n30. what is the patient's blood\npressure?\n31. what is the patient's\nhistory of mental health\nconditions?\n32. what is the patient's heart\nrate?\n33. what is the patient's\nhistory of metabolic diseases?\n34. what is the patient's\nhistory of chronic diseases?\n35. what is the patient's chief\ncomplaint?\n36. what is the patient's\nrespiratory rate?\n37. what is the patient's\nhistory of gastrointestinal\nissues?\n38. what is the patient's\nhistory of recent changes in\nfluid balance?\n39. what is the patient's\nalcohol consumption?\n40. what is the patient's\ncurrent mobility status?\n41. what is the patient's\nhistory of smoking?\n42. what is the patient's\nhistory of cancer?\n43. what is the patient 's recent\nfunctional status?\n44. what is the patient's\ncurrent treatment plan?\n45. what is the patient's\nhistory of hypertension?\n46. what is the patient's\nhistory of infectious\n"}, {"page": 13, "text": "diseases?\n47. what is the patient's\nhistory of substance use?\n48. what is the patient's recent\nactivity level?\n49. what is the patient's\nhistory of recent treatments?\n50. what is the patient's\nhistory of previous hospital\nadmissions?\nB\nQuestion Generation Prompts\nsystem_prompt\nYou are an expert feature\nengineer with specialization\nin clinical and medical\ndomains. You are helping to\ndesign features for a\nmortality prediction model.\n--------------------\nInstruction:\nYour task is to define useful\nfeatures for predicting\nin -hospital mortality from\nICU admission notes.\nDo the following:\n1. Write 20 generalizable and\nclinically meaningful\nquestions that could be\nanswered from admission\nnotes. Answer to these\nquestions should be good\npredictors of mortality.\n2. Assign a short feature name\n(keyword) to each question.\n3. Rate each feature’s\nimportance from 0 (not\nuseful) to 1 (highly\npredictive).\nGuidelines:\n1.\nAvoid yes/no questions. Use\nopen -form questions (e.g.,\nprefer \"What is the patient’s\nage?\" over \"Is the patient\nolder than 65?\").\nProvide your answer in the given\nJSON output format.\nExample of one question_info\n\"question \": \"What is the\npatient’s age?\",\n\"keyword \": \"patient_age\",\n\"importance \": 0.8\nPatient Admission Notes:\n{patient_notes}\n--------------------\nConstraint Output Schema\njson_schema = {\n\"name\": \"questions_generation\",\n\"description \": \"20 questions\nthat serves as a feature for\nmortality prediction .\",\n\"schema \": {\n\"type\": \"object\",\n\"properties \": {\n\"question_info \": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties \": {\n\"question \": {\n\"type \":\" string\",\n\"description \": \"A\nclinical question that can be\nanswered from the note\"\n},\n\"keyword \": {\n\"type\": \"string\",\n\"description \": \"A\nconcise name for the feature\"\n},\n\"importance \": {\n\"type\": \"number\",\n\"description \":\n\"Feature importance score\nfrom 0 (low) to 1 (high)\"\n}\n},\n\"required \":\n[\" question\", \"keyword\",\n"}, {"page": 14, "text": "\"importance \"]\n}\n}\n},\n\"required \": [\" question_info \"]\n}\n}\nC\nAnswer Generation Prompts\nsystem_prompt_answergen:\nYou are an expert in the\nclinical and medical domain,\nwith expertise in analyzing\nand answering any questions\nbased on clinical notes.\n-----------------------\njson_schema_answergen = {\n\"name\": \"answer_50\",\n\"description \": \"Answer to\nthe given 50 questions .\",\n\"schema \": {\n\"type\":\n\"object\",\n\"properties \": {\n\"Q1\": {\n\"type\": \"string\" },\n\"Q2\": {\n\"type\": \"string\" },\n\"Q3\": {\n\"type\": \"string\" },\n.......\n.......\n\"Q49\": {\n\"type\": \"string\" },\n\"Q50\": {\n\"type\": \"string\" }\n},\n\"required \": [\n\"Q1\", \"Q2\",\n\"Q3\", .......... \"Q49\", \"Q50\"\n]\n}\n}\nD\nMore details on Clustering\nWhen generating the 50 questions, the LLM out-\nputs not only the question text but also a corre-\nsponding feature name and importance score. For\nexample:\n{'question ': 'What is the patient\n's age?',\n'keyword ': 'patient_age ',\n'importance ': 0.8}\nWe observed that while the phrasing of the\nquestions may vary, the feature names are of-\nten quite consistent. For instance, all of the fol-\nlowing questions map to the same feature name\npast_medical_conditions and therefore belong\nto the same cluster:\n• what are the patient’s past medical conditions?\n• what are the patient’s significant past medical\nconditions?\n• what are the patient’s past medical conditions?\n• what are the patient’s significant past medical\nconditions?\n• what are the significant past medical condi-\ntions?\nWe further improve clustering by working in the\nreverse direction as well. For example, for the ques-\ntion what is the patient’s laboratory test results?\nthe LLM sometimes produced different feature\nnames such as ab_results, lab_test_results,\nand lab_tests. In such cases, we group all of\nthese feature names and their associated questions\ninto the same cluster. This clustering procedure can\nbe formalized as finding the connected components\nof a graph, where nodes are feature names and\nquestions, and edges connect each feature name\nto its corresponding questions. Implementation is\nstraightforward using NetworkX, as shown below:\nimport networkx as nx\ndef\nbuild_keyword_question_clusters(\nkeyword2question ,\nquestion2keyword):\nG = nx.Graph()\n# Add edges from both\ndictionaries\n"}, {"page": 15, "text": "for kw , questions in\nkeyword2question.items ():\nfor q in questions:\nG.add_edge(kw , q)\nfor q, keywords in\nquestion2keyword.items ():\nfor kw in keywords:\nG.add_edge(q, kw)\n# Extract connected\ncomponents\ncomponents =\nlist(nx.connected_components(G))\n# Optional: extract just\nkeyword clusters\nkeyword_clusters = [\n[node for node in comp\nif node in keyword2question]\nfor comp in components\n]\nreturn keyword_clusters ,\ncomponents\nFinally, for each connected component, we select\nthe most frequent question to represent the cluster.\n"}]}