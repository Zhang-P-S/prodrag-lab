{"doc_id": "arxiv:2512.21515", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.21515.pdf", "meta": {"doc_id": "arxiv:2512.21515", "source": "arxiv", "arxiv_id": "2512.21515", "title": "Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training", "authors": ["Lei Liu", "Hao Zhu", "Yue Shen", "Zhixuan Chu", "Jian Wang", "Jinjie Gu", "Kui Ren"], "published": "2025-12-25T05:40:46Z", "updated": "2025-12-25T05:40:46Z", "summary": "Continual Pre-training (CPT) serves as a fundamental approach for adapting foundation models to domain-specific applications. Scaling laws for pre-training define a power-law relationship between dataset size and the test loss of an LLM. However, the marginal gains from simply increasing data for CPT diminish rapidly, yielding suboptimal data utilization and inefficient training. To address this challenge, we propose a novel perplexity-aware data scaling law to establish a predictive relationship between the perplexity landscape of domain-specific data and the test loss. Our approach leverages the perplexity derived from the pre-trained model on domain data as a proxy for estimating the knowledge gap, effectively quantifying the informational perplexity landscape of candidate training samples. By fitting this scaling law across diverse perplexity regimes, we enable adaptive selection of high-utility data subsets, prioritizing content that maximizes knowledge absorption while minimizing redundancy and noise. Extensive experiments demonstrate that our method consistently identifies near-optimal training subsets and achieves superior performance on both medical and general-domain benchmarks.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.21515v1", "url_pdf": "https://arxiv.org/pdf/2512.21515.pdf", "meta_path": "data/raw/arxiv/meta/2512.21515.json", "sha256": "ce95cd38af274a9c3433af84bc8c0dfe846997428bf8b3db09b8039b111806c3", "status": "ok", "fetched_at": "2026-02-18T02:23:47.973079+00:00"}, "pages": [{"page": 1, "text": "Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict\nPerformance for Continual Pre-training\nLei Liu1,2, Hao Zhu1, Yue Shen1, Zhixuan Chu2,†, Jian Wang1, Jinjie Gu1, Kui Ren2\n1Ant Group, 2Zhejiang University\nEmail: liulei1497@gmail.com, Correspondence: zhixuanchu@zju.edu.cn\nAbstract\nContinual Pre-training (CPT) serves as a funda-\nmental approach for adapting foundation mod-\nels to domain-specific applications. Scaling\nlaws for pre-training define a power-law re-\nlationship between dataset size and the test\nloss of an LLM. However, the marginal gains\nfrom simply increasing data for CPT dimin-\nish rapidly, yielding suboptimal data utiliza-\ntion and inefficient training. To address this\nchallenge, we propose a novel perplexity-aware\ndata scaling law to establish a predictive rela-\ntionship between the perplexity landscape of\ndomain-specific data and the test loss. Our ap-\nproach leverages the perplexity derived from\nthe pre-trained model on domain data as a\nproxy for estimating the knowledge gap, effec-\ntively quantifying the informational perplexity\nlandscape of candidate training samples. By\nfitting this scaling law across diverse perplex-\nity regimes, we enable adaptive selection of\nhigh-utility data subsets, prioritizing content\nthat maximizes knowledge absorption while\nminimizing redundancy and noise. Extensive\nexperiments demonstrate that our method con-\nsistently identifies near-optimal training sub-\nsets and achieves superior performance on both\nmedical and general-domain benchmarks.\n1\nIntroduction\nLarge language models (LLMs) have demonstrated\nimpressive capabilities across a wide range of do-\nmains (Liu et al., 2024). However, their general-\npurpose pre-training objectives often leave them ill-\nsuited for specialized applications such as health-\ncare, where domain-specific knowledge, precise ter-\nminology, and structured reasoning are critical. To\nbridge this gap, Continual Pre-Training (CPT) has\nemerged as a dominant paradigm (Que et al., 2024):\nby further pre-training a general-purpose LLM on\ndomain-specific corpora, models can internalize\nnuanced medical concepts, factual knowledge, and\ndomain-typical reasoning patterns, thereby improv-\ning performance on downstream tasks.\nDespite its success, CPT remains largely guided\nby heuristic data practices, with limited understand-\ning of how data characteristics influence learning\ndynamics (Wang et al., 2025; Chen et al., 2025b).\nA key challenge is the inefficiency of scaling data\nunder continual pre-training. Classical scaling laws\nexhibit power-law predictive relationship between\nloss and dataset size, but they assume that each\ntoken contributes equally to learning (Hoffmann\net al., 2022). In practice, domain-specific corpora\nexhibit high levels of redundancy and noise, and\nvary significantly in conceptual density. For in-\nstance, biomedical literature may contain long pas-\nsages that restate known facts, while clinical notes\noften include unstructured or repetitive entries. As\na result, simply increasing the amount of training\ndata leads to sharply diminishing returns. This ob-\nservation underscores the need for data-informed\nstrategies that move beyond raw data volume and\ninstead emphasize the quality, diversity, and infor-\nmational value of training samples.\nThis breakdown calls for a shift from purely\nquantity-driven paradigms to data-centric strategies\nthat explicitly account for sample effectiveness in\nCPT (Yu et al., 2024; Engstrom et al., 2024). Rather\nthan treating all domain texts equally, we argue that\none should prioritize instances that most effectively\nreduce the model’s uncertainty, particularly those\nthat target its most salient knowledge gaps. A natu-\nral question then arises: how can we quantify such\ngaps in a manner that is both computationally ef-\nficient and strongly correlated with downstream\nperformance improvements?\nIn this work, we propose that the answer lies\nin the model’s own uncertainty signal: perplexity\n(Ankner et al., 2024). We introduce the concept of\nperplexity landscapes, fine-grained distributions of\nmodel perplexity over streaming domain data, as\na powerful diagnostic tool for characterizing the\nknowledge frontier between general and domain-\nspecific expertise. Crucially, we observe that the\narXiv:2512.21515v1  [cs.LG]  25 Dec 2025\n"}, {"page": 2, "text": "shape of these landscapes at early stages of CPT\nstrongly correlates with eventual fine-tuning perfor-\nmance, suggesting that initial perplexity encodes\nactionable information about data utility.\nBuilding on this insight, we derive a novel\nperplexity-aware data scaling law that establishes a\npredictive functional relationship between statistics\nof the initial perplexity distribution (e.g., mean,\nvariance, tail mass) and final task performance\nafter CPT. Unlike traditional scaling laws based\nsolely on data volume, our law incorporates in-\ntrinsic model responses to individual data points,\nenabling adaptive selection of training subsets that\nmaximize knowledge absorption while filtering out\nredundant, overly difficult, or noisy samples.\nOur method requires only a single forward pass\nover unlabeled domain data using the frozen initial\nmodel, making it efficient and scalable. By fitting\nthe scaling law on small pilot batches, we can esti-\nmate the expected return of larger data subsets and\nselect those predicted to yield optimal performance.\nThis enables principled, model-informed data cu-\nration without requiring labeled examples or ex-\npensive retraining loops. We validate our approach\nacross medical and general benchmarks. Results\nshow that perplexity landscapes consistently iden-\ntify near-optimal data subsets, achieving a superior\nimprovement with perplexity as a proxy for knowl-\nedge acquisition. Our contributions are threefold:\n• We introduce perplexity landscapes as a diag-\nnostic tool for CPT, where perplexity distribu-\ntions are strongly predictive of downstream per-\nformance, providing a window into the evolving\nknowledge frontier during specialization.\n• We propose a novel perplexity-aware data scaling\nlaw to link statistics of data perplexity to final\nCPT performance, which moves beyond only\nscaling data volume.\n• We develop an efficient model-aware data selec-\ntion framework to identify high-value training\nsamples. Our method enables scalable data cura-\ntion for achieving superior performance with less\ndata and demonstrates consistent gains across\nmedical and general benchmarks.\n2\nRelated Work\nScaling Law\nA growing body of research has\nestablished that the performance of large language\nmodels (LLMs) follows predictable scaling laws\nwith respect to key resources such as model size,\ntraining compute, and dataset size (Kaplan et al.,\n2020; Hoffmann et al., 2022). In particular, numer-\nous studies have demonstrated that model perfor-\nmance improves according to a power-law relation-\nship as the number of parameters or the volume of\ntraining data increases (Kaplan et al., 2020; Hoff-\nmann et al., 2022), enabling principled extrapola-\ntion from small-scale experiments to large-scale\ndeployments. These scaling laws provide a theoret-\nical foundation for optimizing training efficiency\nand guide decisions in model design and data bud-\nget allocation. More recently, scaling laws have\nbeen extended beyond parameter and data size to\nencompass more nuanced factors, such as data mix-\nture proportions. For instance, Que et al. (2024)\nproposed a data-mixing scaling law that predicts\nperformance based on the composition of multi-\ndomain training sets, offering guidance for curricu-\nlum and domain-adaptive pre-training.\n3\nData-centric Scaling Law by Perplexity\nUnlike pretraining, the primary challenge in CPT\nlies in balancing knowledge retention (the preserva-\ntion of the model’s previously acquired capabilities)\nwith knowledge acquisition (effective adaptation to\nand integration of new information). Consequently,\ndata selection becomes even more critical in CPT.\n3.1\nMotivation\nMotivation-1: Marginal Gain of Scaling Data\nDiminishes During CPT.\nClassical scaling laws\n(Kaplan et al., 2020; Hoffmann et al., 2022) works\nwith a potential assumption, uniformly informative\ndata. For example, empirical scaling law (Hoff-\nmann et al., 2022) fits a parametric loss function,\nbuilding the systematic, predictable connections\namong model size, training dataset size, and the\nmodel’s final performance.\nˆL(N, D) ≜E + A\nNα + B\nDβ\n(1)\nwhere N denotes the parameters and D represents\nthe number of training tokens. α is parameter scal-\ning exponent and β is dataset scaling exponent. E\ncorresponds to the entropy of natural text. These\nlaws quantify how performance improves as the\nattributes are \"scaled up\", providing a foundational\nframework for guiding LLM development.\nHowever, during CPT, the marginal gain from\nsimply increasing dataset size diminishes, where\ndata quantity becomes inadequate predictor for\nmodel performance due to the uncertainty from\n"}, {"page": 3, "text": "the base model and significantly varying data dis-\ntribution. This suggests that conventional scaling\nlaws, which focus solely on quantity, may be insuf-\nficient to capture the dynamics of model refinement\nin later training stages. Noticing that factors related\nto data effectiveness (e.g., knowledge density, topi-\ncal relevance, and factual accuracy) emerge as pri-\nmary drivers of further performance improvement,\nthere is a compelling need to extend the scaling\nlaw framework to incorporate a dataset importance\nweighting dimension, potentially yielding a two-\naxis formulation that jointly models the effects of\ndataset size and dataset importance on loss reduc-\ntion. This motivates the question:\nGiven a pre-trained model, under the condition\nof a fixed number of training texts, how to deter-\nmine the optimal training data subset for continual\npre-training?\nMotivation-2: Perplexity Landscapes Predict\nCPT Performance.\nTo select the effective data,\nperplexity provides a natural metric (Ankner et al.,\n2024): sequences with low PPL are redundant,\nwhile those with high PPL are likely noisy or in-\ncomprehensible, both yield diminishing learning\nreturns. The most effective data lie in a \"sweet\nspot\" of moderate perplexity, which formalizes the\nintuition that the most valuable data is neither too\nsimilar nor too diverse to the model.\nStarting with trained models under different data\nsubsets of varying PPL distributions, experimental\nresults are used to fit an empirical estimator for\ndetermining the optimal perplexity range. This\nformulation extends scaling laws by incorporating\nperplexity statistics, i.e., the mean and variance.\nGiven the fixed tokenizer and corpus, such statistics\nbecome dimensionless across model scales.\n3.2\nPerplexity-Aware Data Scaling Law\n3.2.1\nScaling Law Formula\nFocusing on the data-centric term in (Kaplan et al.,\n2020; Hoffmann et al., 2022), we utilize the follow-\ning functional form over the both dataset size and\nimportance:\nˆL(Q, D) ≜E +\nDc\nQ ∗DαD ,\n(2)\nwhere Dc and αD are hyper-parameters to be fit-\nted. E corresponds to the entropy of natural text.\nHere, we omit the starting status of the pre-training\nmodel because the following perplexity descriptor\ncontains such information. The above formulation\ntypically assumes homogeneous data distributions,\ntreating Q as a constant or implicit factor. Here,\nwe consider parameterize the dataset importance\nterm using basic informativeness measurement.\nPerplexity (PPL), as a token-level likelihood\nmeasure under a reference model, provides a fine-\ngrained proxy for sample informativeness (Brown\net al., 2020; Touvron et al., 2023). We argue that\nsummarizing the PPL distribution across a dataset\nvia its statistic distribution (mean and variance) of-\nfers a principled and scalable importance indicator.\nIn detail, the mean captures the average sample dif-\nficulty, while the variance reflects diversity, both of\nwhich can influence learning dynamics and gener-\nalization (Zhang et al., 2025). Incorporating these\nstatistics allows for a more nuanced understand-\ning of how data modulates model performance,\nenabling better predictions and resource allocation\nin large-scale training regimes.\nAccordingly, let µ and σ indicate the mean and\nvariance of PPL distribution respectively. We in-\nstantiate the dataset importance term as Q(µ, σ) =\nµαµ ∗σασ, which models the joint influence of\nmean and variance in a scale-invariant manner.\nHere, αµ and ασ are hyper-parameters. By enrich-\ning the scaling law with interpretable, data-driven\nsignals, we have:\nˆL(µ, σ, D) ≜E +\nDc\n(µαµ ∗σασ) ∗DαD .\n(3)\nEmpirical experiments (Figure 1) indicate that\nthe relation between loss and µ (σ) is not strictly\nmonotonic, while these two variables exhibit a mea-\nsurable interdependence. Thus, we introduce the\nminimal multiplicative interaction to extend Eq. 3\nas follows:\nαµ(σ) = α0 + α1σ,\nασ(µ) = β0 + β1µ,\n(4)\nwhere α1 and β1 are relationship variables between\nµ and σ. Such transformation results in the follow-\ning format of perplexity-aware data scaling law:\nˆL(µ, σ, D) ≜E+\nDc\n(µαµ(σ) ∗σασ(µ)) ∗DαD , (5)\nwhich preserves the interpretable power-law struc-\nture. Besides, it incorporates the relationship de-\ncomposition between mean and variance of per-\nplexity distribution, i.e., interdependence and inde-\n"}, {"page": 4, "text": "0\n200\n400\n600\n800\n1000\n1200\n1400\nVariance ( )\n1.98\n2.00\n2.02\n2.04\n2.06\n2.08\nTest loss (L)\n=13.48±2.17,N=0.6,D=40B\n(a) Qwen3-0.6B-Base\n0\n200\n400\n600\n800\n1000\n1200\n1400\nVariance ( )\n1.62\n1.63\n1.64\n1.65\n1.66\n1.67\n1.68\n1.69\nTest loss (L)\n=13.48±2.17,N=14,D=40B\n(b) Qwen3-14B-Base\nFigure 1: Interdependence between loss and µ (σ).\n0\n10\n20\n30\n40\n50\nMean ( )\n1.60\n1.62\n1.64\n1.66\n1.68\nTest Loss (L)\n=1.1\n=11.0\n=74.7\n(a) Test Loss VS. PPL Distribution\n1020\n1021\n1022\n1022\n1022\nFLOPs (F)\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nTest Loss (Log(L))\n=2.0\n=7.4\n=18.8\n(b) Test Loss (Log) VS. FLOPs\nFigure 2: Fitted curves for scaling law based on Qwen3-14B-Base.\npendence decomposition:\nL(µ, σ, D) ≜E+\nDc\nµα0σβ0\n| {z }\nIndependence\n∗µα1σσβ1µ\n|\n{z\n}\nInterdependence\n∗DαD .\n(6)\nThis format is strongly favored because it (i) con-\nserves the power-law structure, (ii) keeps interac-\ntions simple, and (iii) collapses to the basic model\nwhen variables are independent.\n3.3\nScaling Law Fitting\nSetting\nWe select medical domain as the target.\nGiven the dataset from PubMed corpus, we firstly\nperform multiple bootstrap sampling to generate\ndifferent subsets with varying µ and σ. Among\nthese training subsets, 90% subsets are used for\nfitting the scaling curve and 10% subsets are for\nvalidation. The final fitting curves with sampled\nfitting points are shown in Figure 2. The validation\nresults are shown in Figure 4. The red region is\nobtained according to the fitted scaling law with\nµ = 13.48 ± 2.17 and σ varying from 25 to 1600\nwith µ = 13.48±2.17, which is the real PPL range\nof the dataset. It is observed that the fitted scaling\nlaw closely matches the validation points (blue)\nacross different variances and maintains a consis-\ntent trend. This strong agreement confirms that\nthe formula accurately captures the relationship\nbetween dataset importance, and performance.\nWhy Consider Interaction Term in Scaling\nLaw?\nEq. 3 assumes a monotonic relationship\nbetween the loss and each variable, while exper-\nimental results indicate that the relationship be-\ntween the loss and the mean µ or standard deviation\nσ is not purely monotonic. As shown in Figure 1,\nfitting Eq. 3, the real samples (black points) are\nnot strictly fall in the region we fitted (red area).\nFurthermore, since both µ and σ are derived from\nthe same PPL distribution, an intrinsic correlation\nbetween them may exist. Therefore, it is necessary\nto introduce interaction terms into the model to\nbetter capture their interdependent effects.\nWhy Perplexity Distributions Follow Power-\nLaw Forms in Scaling Laws?\nPower-law behav-\nior arises from language data’s inherent hierarchy.\n"}, {"page": 5, "text": "0.6\n1.7\n4.0\n8.0\n14.0\nParameter (N)\n1.60\n1.70\n1.80\n1.90\n2.00\nTest Loss (L)\n=7.3572\n=2.0\n=11.0\n=891.2\n(a) Test Loss VS. Parameters (Low PPL Range)\n0.6\n1.7\n4.0\n8.0\n14.0\nParameter (N)\n1.60\n1.70\n1.80\n1.90\n2.00\nTest Loss (L)\n=18.7771\n=1.1\n=74.7\n=1462.2\n(b) Test Loss VS. Parameters (High PPL Range)\nFigure 3: Performance changes with varying PPL distributions and Parameters.\n0\n200\n400\n600\n800\n1000\n1200\n1400\nVariance ( )\n1.98\n2.00\n2.02\n2.04\n2.06\n2.08\nTest Loss (L)\nValidation Points\nScaling Law Region\n(a) Qwen3-0.6B-Base\n0\n200\n400\n600\n800\n1000\n1200\n1400\nVariance ( )\n1.62\n1.63\n1.64\n1.65\n1.66\n1.67\n1.68\n1.69\nTest Loss (L)\nValidation Points\nScaling Law Region\n(b) Qwen3-14B-Base\nFigure 4: Validation curves of scaling law.\nNatural language data exhibits scale-invariant hier-\narchical structure: linguistic units (tokens, phrases,\ndocuments, domains) follow a power-law distribu-\ntion in their frequency and complexity. For exam-\nple, Zipf’s law (Zipf, 2013) describes how word\nfrequencies scale as f ∝r−s (where r is rank\nand s ≈1), and document complexity (measured\nby syntactic depth or semantic ambiguity) simi-\nlarly follows a power-law, where a small fraction\nof \"high-complexity\" documents drive most vari-\nation in PPL (Wold et al., 2024). This hierarchy\ndirectly shapes µ and σ:\n• Mean µ: As incremental data volume (Dnew)\nscales, µ converges to a domain-specific limit\n(µ0) because larger datasets increasingly sample\nthe full range of linguistic complexity. The con-\nvergence rate follows a power law (µ −µ0 ∝\nD−α\nnew, α > 0) because the remaining \"unseen\"\ncomplexity (driving deviations from µ0) is dom-\ninated by low-frequency, high-complexity data,\nwhose contribution decays as a power of Dnew\n(consistent with (Cagnetta et al., 2025)).\n• Variance σ: Variance quantifies diversity in data\nquality/complexity, which is inherently tied to\nthe number of distinct subdomains (C) in Dnew.\nEach subdomain contributes a unique PPL sub-\ndistribution. The total variance scales as σ ∝Cγ\n(γ > 0), i.e., a power law because subdomain\ncomplexity itself follows a power-law hierarchy.\nHow Perplexity Landscape Affects Perfor-\nmance?\nFrom the data-centric perspective, the\ntest loss of a LLM model can be predicted under\nthe condition of data information, i.e., PPL mean\n(µ), variance (σ), and dataset size (D). There are\nsome key observations from Figure 2 and 3.\n• In Figure 2(a), perplexity distribution has a non-\nmonotonic effect. There exists an optimal point\nfor µ and σ. Higher σ initially decreases loss by\nintroducing useful diversity or hardness. How-\never, beyond an optimal point µ would harm per-\nformance. Low-µ data benefits more from higher\nσ, while high-µ data suffers at very high σ.\n• In Figure 2(b), with a moderate variance, lower\nmean yields worse convergence performance.\n"}, {"page": 6, "text": "(a) PPL Descent Paths\n(b) Loss-PPL 3D Distribution\nFigure 5: Visualizations for perplexity landscape on Qwen3-14B-base model.\n• In Figure 3, test loss decreases with model size\n(N), but the magnitude depends on data charac-\nteristics. In a low PPL range (Figure 3(a)), higher\nσ results in lower test loss than lower σ for the\nsame number of parameters. In a high PPL range\n(in Figure 3(b)), lower σ enables significantly\nbetter scaling, whereas higher σ limits.\nCould Perplexity-Aware Scaling Law Generalize\nto New Points?\nTo rigorously assess the gener-\nalization of the fitted scaling law, we evaluate its\npredictions on an independent validation set that\nis entirely disjoint from the data used for fitting.\nThe validation configurations are sampled from the\nsame underlying data distribution while exhibiting\ndifferent perplexity (PPL) profiles, ensuring that\nthe evaluation tests extrapolation to unseen points\nrather than simple interpolation.\nAs shown in Figure 4, the validation points (blue\ndots) closely follow the predicted trend and re-\nmain largely within the shaded scaling-law region\nfor both Qwen3-0.6B-Base and Qwen3-14B-Base.\nAcross the full range of variance, the empirical test\nlosses align well with the power-law curve, and de-\nviations are small and symmetric, indicating no sys-\ntematic bias. This consistency between the model’s\npredictions and the independently obtained valida-\ntion measurements demonstrates that the derived\npower-law relationship is both robust and reliable,\nand that it captures the dominant dependence of\ntest loss on variance for these models.\nPPL Landscapes Visualization\nFigure 5 visu-\nalizes how the test loss varies as a function of the\nmean and standard deviation of the PPL distribu-\ntion, and how this relates to our scaling law.\nIn the contour plot (Figure 5(a)), the level sets of\ntest loss form smooth, roughly elliptical basins in\nthe (mean, std) plane. The three descent paths start\nfrom different initial PPL configurations but all\nmove along the gradient of the loss surface toward\nthe same low-loss region, highlighted by the red\nstar. This indicates that the loss is not determined\nby mean or variance alone; instead, it depends on\ntheir joint configuration, and different PPL profiles\ncan converge to a common optimum predicted by\nthe scaling law.\nThe 3D surface (Figure Figure 5(b)) makes\nthis relationship explicit:\ntest loss forms a\nbowl-shaped surface over (mean, std), with a sin-\ngle, well-defined minimum. Near this minimum,\nloss changes smoothly and approximately follows\nour power-law scaling with respect to the PPL vari-\nance (for a given mean). Moving away from the\noptimum in either direction, by increasing or de-\ncreasing the mean or the variance—monotonically\nincreases the loss, consistent with the scaling-law\nbehavior observed in our 1D slices.\nThus, the PPL landscape analysis provides a geo-\nmetric interpretation of the scaling law: the power-\nlaw relationship describes how test loss scales\nalong the main descent directions in the (mean,\nstd) space of perplexity.\n3.4\nDistance-to-Optimum Selection\nGiven a fitted perplexity-aware scaling law, we\nassume it identifies an optimal region of the data-\nperplexity distribution characterized by a target\nmean ˆµ and variance ˆσ. Our goal is to construct\n"}, {"page": 7, "text": "a continual pre-training subset whose empirical\nperplexity statistics approximate (ˆµ, ˆσ2) under a\nfixed token budget.\nDistance-to-Optimum Objective\nFor a subset\nS, we measure the deviation from the optimal dis-\ntribution by:\nJ(S) = wµ\n\u0000µ(S)−ˆσ\n\u00012+wσ\n\u0000σ2(S)−ˆσ2\u00012, (7)\nwhere wµ, wσ > 0 are weighting coefficients (typ-\nically wµ = wσ = 1). The Distance-to-Optimum\nSelection (DOS) problem can then be formulated\nas:\nmin\nS⊆D\nJ(S)s.t.\nX\ncj∈S\n|cj| ≤Tbudget.\n(8)\nSolving this problem exactly is intractable, there-\nfore we adopt a greedy approximation.\nGreedy Selection Algorithm\nInitially, the whole\ncorpus is chunked into N random subsets D =\n{cj}N\nj=1. Define the optimal subset as S ←∅\nand data budge as\nT ←0. To initialize a non-\nempty subset, choose the chunk whose perplexity\nis closest to ˆµ, i.e., j∗= arg minj |pj −ˆµ| and\nadd it to S. The greedy expansion is conducted as\nfollowing steps while T < Tbudget:\n• For each candidate cj /∈S with T + |cj| ≤\nTbudget, form S′ = S ∪{cj}, update ˆµ(S′) and\nˆσ2(S′), and compute\nJj = wµ\n\u0000µ(S′) −ˆµ\n\u00012 + wσ\n\u0000σ2(S′) −ˆσ2\u00012.\n• Select j∗= arg minj Jj, set S ←S′, T ←\nT + |cj∗|, and update ˆµ(S), ˆσ2(S).\n4\nExperiment\n4.1\nExperimental Setting\nHyper-Parameters\nWe employ the AdamW op-\ntimizer with hyper-parameters set to β1 = 0.9,\nβ2 = 0.95, and weight-decay = 0.1. We set the\nmaximum sequence length to 8K during the whole\nCPT stage. As for the learning rate scheduling, we\nfirst use a warming up with peak learning rate of\n1 × 10−5. During the anneal training procedure,\nwe gradually decay the learning rate following a\ncosine decay curve. The gradient clipping norm is\nset to 1.0. The base model is Qwen3-14B-Base.\nTable 1: Effectiveness of Perplexity-Aware Data Scaling\nLaw. All models are evaluated under the same evalu-\nation setting. The highest, the second-best scores are\nshown in bold and underlined, respectively. The base\nmodel is Qwen3-14B-Base.\nTask Benchmark\nBase\nCPT\nRS\nLPS\nHPS\nDOS\nMedical\nDiagnosisArena\n41.30 56.40 56.40 45.70 61.11\nGPQA-Med\n57.89 57.89 57.89 57.89 63.16\nPubMedQA\n76.60 76.70 76.40 78.60 77.40\nMedbullets\n55.52 56.82 56.82 58.44 57.14\nNEJMQA\n64.73 66.06 65.60 64.79 66.14\nMedMCQA\n66.89 67.75 67.51 67.75 68.28\nMedQA-USMLE 72.58 73.21 73.21 72.66 73.61\nCEVAL-Med\n89.86 90.14 89.63 91.46 90.44\nCMMLU-Med\n86.68 86.78 86.89 86.58 86.38\nMMLU-Med\n81.19 81.62 81.85 81.68 82.11\nAverage\n69.32 71.34 71.22 70.56 72.48\nGeneral\nCEVAL\n85.52 85.14 85.14 85.17 85.44\nCMMLU\n84.92 84.50 84.79 84.53 84.78\nMMLU\n82.03 82.23 82.03 82.11 82.25\nAverage\n84.16 83.94 83.99 83.94 84.16\nEvaluation Datasets\nWe evaluate the models on\nleading benchmarks over both medical and general\ntasks. Medical tasks include MMLU, Diagnosis-\nArena (Zhu et al., 2025), MedMCQA (Pal et al.,\n2022), MedQA-USMLE (Jin et al., 2021), Pub-\nMedQA (Jin et al., 2019), MedBullets (Chen et al.,\n2025a), NEJMQA (Katz et al., 2024), SuperGPQA-\nMed (Du et al., 2025), GPQA-Med (Rein et al.,\n2024), and medical subsets of CEVAL (Huang\net al., 2023), CMMLU (Li et al., 2023), MMLU\n(Hendrycks et al., 2020). General tasks include\nCEVAL, CMMLU, and MMLU.\nBaseline\nFour baselines are utilized: (1) Base\nmodel without CPT, (2) RS-CPT with random data\nsampling, (3) LPS-CPT with low-PPL sampling,\ni.e., and (4) HPS-CPT with high-PPL sampling,\ni.e., PPL scors are lower/higher than optimal one\nderived from DOS.\n4.2\nModel Results\nBenchmark Performance\nTable 1 evaluates the\nimpact of our perplexity-aware data scaling law on\ncontinual pre-training. On medical benchmarks,\nDOS-Base achieves the best overall performance,\nreaching an average score of 72.48, compared to\n71.34 for RS-CPT and 71.22 for LPS-CPT. This\ncorresponds to a 3.16 improvement over the base\nmodel. The gains are consistent across datasets:\nDOS-Base attains the highest or second-highest\n"}, {"page": 8, "text": "Figure 6: Test loss curves during CPT. CPT with DOS\nconverges more rapidly at a lower loss.\nscore on almost all medical tasks, with particu-\nlarly notable improvements on DiagnosisArena\nand GPQA-Med. On general-domain benchmarks,\nDOS-Base maintains the par-level performance\n(84.16), matching the base model and slightly ex-\nceeding CPT-Base (83.94), indicating that the DOS\ncould well reduce the forgetting of general knowl-\nedge. These results demonstrate that guiding CPT\nwith our perplexity-aware scaling law is more effec-\ntive than naive CPT: it converts additional training\ndata into substantial, domain-specific gains while\npreserving strong general performance.\nLoss Curve\nAs shown in Figure 6, across all\nmethods, test losses decrease steadily over the first\nseveral hundred training steps, but the four strate-\ngies exhibit noticeably different convergence be-\nhaviors. CPT-DOS shows the most rapid and con-\nsistent reduction in test loss, achieving the lowest\nfinal loss and demonstrating stable improvement\nthroughout training. In contrast, CPT-HPS con-\nverges more slowly and plateaus at a higher loss, ex-\nhibiting small fluctuations after roughly 250 steps.\nCPT-LPS performs similarly to CPT-HPS but ulti-\nmately settles at a higher loss. CPT-Random shows\nthe weakest performance, consistently lagging be-\nhind all other methods. Overall, the figure high-\nlights that CPT-DOS not only accelerates conver-\ngence but also delivers a substantial improvement\nin final model performance.\nData Distribution\nFigure 7 presents a t-SNE pro-\njection of sentence embeddings, where each point\nis colored by its perplexity range. Samples with\nmedium PPL values (roughly the middle bins) are\nmore diffusely scattered and less concentrated in\ndistinct clusters. LPS-CPT or HPS-CPT tends to\nfocus on extreme-confidence outputs, while under-\nrepresenting moderate-difficulty cases, which are\nFigure 7: t-SNE visualization for varying perplexity\nranges. Two dense subregions are clearly emphasized:\na low-PPL region (upper-left) and a high-PPL region\n(lower-left).\noften important for balanced generalization. No-\ntably, applying the DOS strategy improves the cov-\nerage of the embedding space by explicitly con-\ntrolling variance during data selection. As a result,\nDOS produces samples that span both low and high\nPPL regions, increasing overall data diversity and\nencouraging exploration of different linguistic pat-\nterns reflected in the two extremes. The distribution\nremains quality-preserving and balanced, suggest-\ning that DOS can avoid bias toward either easy or\noverly difficult examples while still expanding the\neffective coverage of the data manifold.\n5\nConclusion\nWe rethink the prevailing assumption that model\nperformance scales monotonically with dataset\nsize, indicating that naive data scaling yields di-\nminishing returns during CPT. To address this in-\nefficiency, we introduce a perplexity-aware data\nscaling law that exploits a model’s initial perplex-\nity landscape over domain corpora as a proxy for\nknowledge gaps. By modeling the relationship be-\ntween perplexity statistics and model performance,\nour method adaptively selects high-utility training\nsubsets while discarding redundant or noisy ex-\namples. Empirical results on medical and general-\ndomain benchmarks show that this approach consis-\ntently identifies near-optimal data subsets, achiev-\ning superior adaptation performance with signifi-\ncantly less data than conventional CPT.\n"}, {"page": 9, "text": "References\nZachary Ankner, Cody Blakeney, Kartik Sreenivasan,\nMax Marion, Matthew L Leavitt, and Mansheej Paul.\n2024. Perplexed by perplexity: Perplexity-based data\npruning with small reference models. arXiv preprint\narXiv:2405.20541.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, and 1 others. 2020. Language models are\nfew-shot learners. Advances in neural information\nprocessing systems, 33:1877–1901.\nFrancesco Cagnetta, Hyunmo Kang, and Matthieu\nWyart. 2025. Learning curves theory for hierarchi-\ncally compositional data with power-law distributed\nfeatures. arXiv preprint arXiv:2505.07067.\nHanjie Chen, Zhouxiang Fang, Yash Singla, and Mark\nDredze. 2025a. Benchmarking large language mod-\nels on answering and explaining challenging medical\nquestions. In Proceedings of the 2025 Conference\nof the Nations of the Americas Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers), pages\n3563–3599.\nJie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yu-\ntao Zhu, Jinhao Jiang, Yingqian Min, Wayne Xin\nZhao, Zhicheng Dou, Jiaxin Mao, and 1 others.\n2025b. Towards effective and efficient continual pre-\ntraining of large language models. In Proceedings\nof the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 5779–5795.\nXinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang,\nTianyu Zheng, King Zhu, Minghao Liu, Yiming\nLiang, Xiaolong Jin, Zhenlin Wei, and 1 others. 2025.\nSupergpqa: Scaling llm evaluation across 285 gradu-\nate disciplines. arXiv preprint arXiv:2502.14739.\nLogan Engstrom, Axel Feldmann, and Aleksander\nMadry. 2024. Dsdm: Model-aware dataset selection\nwith datamodels. arXiv preprint arXiv:2401.12926.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, and 1 others. 2022.\nTraining compute-optimal large language models.\narXiv preprint arXiv:2203.15556.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei\nZhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Yao Fu, and 1 oth-\ners. 2023. C-eval: A multi-level multi-discipline\nchinese evaluation suite for foundation models. Ad-\nvances in Neural Information Processing Systems,\n36:62991–63010.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nUriel Katz, Eran Cohen, Eliya Shachar, Jonathan Somer,\nAdam Fink, Eli Morse, Beki Shreiber, and Ido Wolf.\n2024.\nGpt versus resident physicians—a bench-\nmark based on official board scores.\nNejm Ai,\n1(5):AIdbp2300192.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai\nZhao, Yeyun Gong, Nan Duan, and Timothy Bald-\nwin. 2023. Cmmlu: Measuring massive multitask\nlanguage understanding in chinese. arXiv preprint\narXiv:2306.09212.\nLei Liu, Xiaoyan Yang, Junchi Lei, Yue Shen, Jian\nWang, Peng Wei, Zhixuan Chu, Zhan Qin, and Kui\nRen. 2024. A survey on medical large language mod-\nels: Technology, application, trustworthiness, and\nfuture directions. arXiv preprint arXiv:2406.03712.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on health,\ninference, and learning, pages 248–260. PMLR.\nHaoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang,\nXingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai,\nJiakai Wang, Yuanxing Zhang, and 1 others. 2024.\nD-cpt law: Domain-specific continual pre-training\nscaling law for large language models. Advances in\nNeural Information Processing Systems, 37:90318–\n90354.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jack-\nson Petty, Richard Yuanzhe Pang, Julien Dirani, Ju-\nlian Michael, and Samuel R Bowman. 2024. Gpqa:\nA graduate-level google-proof q&a benchmark. In\nFirst Conference on Language Modeling.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, and 1 others. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nXingjin Wang, Howe Tissue, Lu Wang, Linjing Li, and\nDaniel Dajun Zeng. 2025. Learning dynamics in con-\ntinual pre-training for large language models. arXiv\npreprint arXiv:2505.07796.\n"}, {"page": 10, "text": "Sondre Wold, Petter Mæhlum, and Oddbjørn Hove.\n2024. Estimating lexical complexity from document-\nlevel distributions. arXiv preprint arXiv:2404.01196.\nZichun Yu, Spandan Das, and Chenyan Xiong. 2024.\nMates: Model-aware data selection for efficient pre-\ntraining with data influence models. Advances in\nNeural Information Processing Systems, 37:108735–\n108759.\nXuemiao Zhang, Feiyu Duan, Liangyu Xu, Yongwei\nZhou, Sirui Wang, Rongxiang Weng, Jingang Wang,\nand Xunliang Cai. 2025.\nFrame: Boosting llms\nwith a four-quadrant multi-stage pretraining strategy.\narXiv preprint arXiv:2502.05551.\nYakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong\nHuang, Wei Nie, Jiaji Liu, Shaoting Zhang, Pengfei\nLiu, and Xiaofan Zhang. 2025.\nDiagnosisarena:\nBenchmarking diagnostic reasoning for large lan-\nguage models. arXiv preprint arXiv:2505.14107.\nGeorge Kingsley Zipf. 2013. The psycho-biology of lan-\nguage: An introduction to dynamic philology. Rout-\nledge.\n"}]}