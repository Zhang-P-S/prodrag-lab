{"doc_id": "arxiv:2602.10544", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.10544.pdf", "meta": {"doc_id": "arxiv:2602.10544", "source": "arxiv", "arxiv_id": "2602.10544", "title": "Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy", "authors": ["Wuyang Zhang", "Zhen Luo", "Chuqiao Gu", "Jianming Ma", "Yebo Cao", "Wangming Yuan", "Yinzhi Jin"], "published": "2026-02-11T05:36:14Z", "updated": "2026-02-11T05:36:14Z", "summary": "Automated EEG monitoring requires clinician-level precision for seizure detection and reporting. Clinical EEG recordings exceed LLM context windows, requiring extreme compression (400:1+ ratios) that destroys fine-grained temporal precision. A 0.5 Hz error distinguishes absence epilepsy from Lennox-Gastaut syndrome. LLMs lack inherent time-series comprehension and rely on statistical associations from compressed representations. This dual limitation causes systems to hallucinate clinically incorrect measurement values.   We separate measurement extraction from text generation. Our hybrid architecture computes exact clinical values via signal processing before compression, employs a cross-modal bridge for EEG-to-language translation, and uses parameter-efficient fine-tuning with constrained decoding around frozen slots. Multirate sampling maintains long-range context while preserving event-level precision. Evaluation on TUH and CHB-MIT datasets achieves 60% fewer false alarms, 50% faster detection, and sub-clinical measurement precision. This is the first system guaranteeing clinical measurement accuracy in automated EEG reports.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.10544v1", "url_pdf": "https://arxiv.org/pdf/2602.10544.pdf", "meta_path": "data/raw/arxiv/meta/2602.10544.json", "sha256": "ba1f170405c018901da7eaad8f5063c4ba92d240250e61bd731f3582c5c2e104", "status": "ok", "fetched_at": "2026-02-18T02:19:25.840297+00:00"}, "pages": [{"page": 1, "text": "Bridging the Compression-Precision Paradox: A Hybrid\nArchitecture for Clinical EEG Report Generation with\nGuaranteed Measurement Accuracy\nWuyang Zhangâˆ—\nzhang.noc@northeastern.edu\nNortheastern University\nBoston, MA, USA\nZhen Luo\nluo.zhe@northeastern.edu\nNortheastern University\nBoston, MA, USA\nChuqiao Gu\nchuqiaog@alumni.cmu.edu\nCarnegie Mellon University\nPittsburgh, PA, USA\nJianming Ma\nma.jianming@northeastern.edu\nNortheastern University\nBoston, MA, USA\nYebo Cao\nyeboc@alumni.cmu.edu\nCarnegie Mellon University\nPittsburgh, PA, USA\nWangming Yuan\nyuan.wangming@gmu.edu\nGeorge Mason University\nFairfax, VA, USA\nYinzhi Jin\nyinzhij@alumni.cmu.edu\nCarnegie Mellon University\nPittsburgh, PA, USA\nAbstract\nAutomated EEG monitoring requires clinician-level precision for\nseizure detection and reporting. Clinical EEG recordings exceed\nLLM context windows, requiring extreme compression (400:1+ ra-\ntios) that destroys fine-grained temporal precision. A 0.5 Hz error\ndistinguishes absence epilepsy from Lennox-Gastaut syndrome.\nLLMs lack inherent time-series comprehension and rely on sta-\ntistical associations from compressed representations. This dual\nlimitation causes systems to hallucinate clinically incorrect mea-\nsurement values.\nWe separate measurement extraction from text generation. Our\nhybrid architecture computes exact clinical values via signal pro-\ncessing before compression, employs a cross-modal bridge for EEG-\nto-language translation, and uses parameter-efficient fine-tuning\nwith constrained decoding around frozen slots. Multirate sampling\nmaintains long-range context while preserving event-level preci-\nsion. Evaluation on TUH and CHB-MIT datasets achieves 60% fewer\nfalse alarms, 50% faster detection, and sub-clinical measurement\nprecision. This is the first system guaranteeing clinical measure-\nment accuracy in automated EEG reports.\nCCS Concepts\nâ€¢ General and reference â†’Design; â€¢ Computing methodolo-\ngies â†’Neural networks; â€¢ Applied computing â†’Health care\ninformation systems; Computer-assisted instruction.\nâˆ—Corresponding author\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nICPHDS 2025, Yantai, China\nÂ© 2025 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-2072-7/2025/11\nhttps://doi.org/10.1145/3789537.3789570\nKeywords\nEEG analysis, Clinical report generation, Large language models,\nCross-modal learning, Medical AI, Seizure detection, Hybrid archi-\ntecture, Value extraction\nACM Reference Format:\nWuyang Zhang, Zhen Luo, Chuqiao Gu, Jianming Ma, Yebo Cao, Wangming\nYuan, and Yinzhi Jin. 2025. Bridging the Compression-Precision Paradox: A\nHybrid Architecture for Clinical EEG Report Generation with Guaranteed\nMeasurement Accuracy. In 2025 4th International Conference on Public Health\nand Data Science (ICPHDS 2025), November 21â€“23, 2025, Yantai, China. ACM,\nNew York, NY, USA, 7 pages. https://doi.org/10.1145/3789537.3789570\n1\nIntroduction\n50 million people worldwide live with epilepsy [1]. Severe shortage\nof epileptologists creates care gaps, particularly in low- and middle-\nincome countries with 80% of patients [1]. Automated EEG analysis\nwill transform care access through scalable monitoring and diagnos-\ntic reporting. However, a fundamental barrier blocks deployment:\nloss of diagnostic precision under extreme compression required\nfor LLM-based report generation.\nClinical EEG recordings exceed LLM context windows, requiring\ncompression ratios exceeding 400:1. This destroys fine-grained tem-\nporal precision. A 0.5 Hz error distinguishes 3 Hz absence epilepsy\nfrom 3.5 Hz Lennox-Gastaut syndrome, altering treatment decisions.\nLLMs lack inherent time-series comprehension and rely on statisti-\ncal associations from compressed representations. This compression-\nprecision paradox is a mathematical necessity imposed by context\nwindow constraints.\nExisting approaches are inadequate. EEG-GPT [2], BENDR [3], and\nbiosignal-to-text systems [4] compress signals into neural represen-\ntations before text generation, destroying clinical measurement\nprecision and producing hallucinated values. With human inter-\nrater reliability at ğœ…=0.29-0.73 [5] and FDA demanding traceable\nmeasurements, clinical deployment requires guaranteed accuracy.\narXiv:2602.10544v1  [cs.LG]  11 Feb 2026\n"}, {"page": 2, "text": "ICPHDS 2025, November 21â€“23, 2025, Yantai, China\nZhang et al.\nOur key insight separates measurement extraction from text\ngeneration. We compute exact clinical values via signal processing\nbefore neural compression, preserving accuracy while retaining\nneural model flexibility for narrative composition. Multirate sam-\npling maintains long-range context while preserving event-level\nprecision. Every measurement includes full provenance.\nContributions. (i) We formalize the compression-precision para-\ndox and prove end-to-end neural learning cannot preserve clini-\ncal measurements under LLM context constraints. (ii) We present\nthe first hybrid architecture separating measurement extraction\nfrom text generation, combining signal processing guardrails with\ncross-modal EEG-to-language translation and parameter-efficient\nLLM adaptation. (iii) Evaluation demonstrates clinically significant\nimprovements: reduced false alarms, faster detection, and measure-\nment precision within clinical tolerance. (iv) We demonstrate clini-\ncal deployability with FDA-compliant traceability and sub-minute\nlatency.\n2\nRelated Work\nExisting approaches fail to address dual limitations: (1) extreme\ncompression destroying measurement precision, and (2) language\nmodels lacking time-series comprehension.\n2.1\nEEG and Time-Series Foundation Models\nRecent foundation models (EEG-GPT [2], BENDR [3], EEGFormer [6])\ncompress multi-channel data into compact representations, de-\nstroying fine-grained measurements needed to distinguish diag-\nnostic boundaries. Time-series models (Chronos-2 [7], PatchTST [8],\nTimesFM [9]) demonstrate strong forecasting but do not guarantee\nmeasurement precision. A 0.5 Hz error is statistically minor but\nclinically catastrophic. We adopt architectural principles (patching,\ngroup attention) but introduce measurement-first signal processing\nand graph-aware layers.\n2.2\nMedical Report Generation\nVision-language models generate radiology reports from static im-\nages [10â€“12], but successes do not transfer to dynamic multi-channel\nEEG. Fitting hours of high-frequency data into LLM context win-\ndows requires 400:1+ compression, destroying measurement preci-\nsion [13]. FDA guidance requires traceable outputs [14]. With human\ninter-rater reliability at ğœ…= 0.3â€“0.7 [5], automated systems must\nguarantee measurement accuracy.\n3\nProblem Formulation\n3.1\nTask Definition\nGiven multi-channel EEG X âˆˆRğ¶Ã—ğ‘‡, we generate report R =\n(T, V) where T is narrative text and V = {(ğ‘£ğ‘–,ğ‘ğ‘–,ğ‘ ğ‘–)}ğ‘š\nğ‘–=1 contains\nprecise measurements with provenance. Diagnostic boundaries re-\nquire ğœ–ğ‘“= 0.1 Hz (frequency), ğœ–ğ‘‘= 0.5 s (duration), ğœ–ğ‘= 5 ğœ‡V (am-\nplitude). We use hierarchical sampling: low-rate X(ğ¿) (256â€“512 Hz)\nfor context, high-rate X(ğ») (â‰¥1 kHz) for events, with channel graph\nğº= (ğ‘‰, ğ¸) for spatial inference.\nFigure 1: Hybrid architecture: hierarchical sampling balances\ncontext and precision; signal processing extracts measure-\nments before compression; cross-modal bridge translates\nto language space; constrained decoder generates reports\naround frozen slots.\n3.2\nThe Compression-Precision Paradox\nCompressing |X| = 322, 560 to ğ‘‘âˆˆ{512, 1024} yields ğœŒ> 300 com-\npression, bounded by ğ¼(X; z) â‰¤log2(|Z|). Frequency resolution\nÎ”ğ‘“min â‰ˆ0.74 Hz exceeds clinical ğœ–ğ‘“= 0.1 Hz by 7Ã—, and temporal\nquantization Î”ğ‘¡min â‰ˆ2.1 s exceeds ğœ–ğ‘‘= 0.5 s.\n3.3\nFundamental Limitations of End-to-End\nLearning\nWhen compression destroys precise values, LLMs generate training\ndistribution modes, causing systematic hallucination.\nTheorem 1. For any encoder ğ‘“ğœƒ: Rğ¶Ã—ğ‘‡â†’Rğ‘‘with ğœŒ> 100, distinct\nsignals X1, X2 with |ğ‘£1âˆ’ğ‘£2| > ğœ–clinical satisfy ||ğ‘“ğœƒ(X1)âˆ’ğ‘“ğœƒ(X2)||2 < ğ›¿\nfor arbitrarily small ğ›¿.\nProof. By pigeonhole principle, 2ğ¶Ã—ğ‘‡Ã—ğ‘inputs map to â‰ˆ2ğ‘‘Ã—ğ‘â€² em-\nbeddings, so clinically distinct signals become indistinguishable.\nâ–¡\n4\nMethodology\n4.1\nSolution Overview\nOur hybrid architecture (Figure 1) separates measurement extrac-\ntion from text generation to resolve the compression-precision\nparadox. The key insight: compute exact clinical values via signal\nprocessing before neural compression, then use language models\nsolely for narrative composition around frozen measurement slots.\nPipeline. (1) Hierarchical multirate sampling maintains syn-\nchronized low-rate (256â€“512Hz) and high-rate (â‰¥1kHz) streams.\nLow-rate provides hours of context; high-rate windows (Eq. (2))\ncapture event-level precision only where needed, avoiding compu-\ntational explosion. (2) Measurement-first guardrails compute\nexact values (frequency via Welch PSD Eq. (7), duration, amplitude,\nlocalization) with full provenance before any neural processing.\nThese frozen slots are immutable. (3) Graph-aware modeling pro-\ncesses dual-view inputs (time-domain patches, transform-domain\nbandpower) using group attention augmented with channel-graph\nstructure (Eq. (3)) and linear-time SSM layers for ultra-long se-\nquences. (4) Output heads include quantile forecasting (pinball\nloss Eq. (4)), seizure detection, and EMD-aware value prediction\n"}, {"page": 3, "text": "Bridging the Compression-Precision Paradox: A Hybrid Architecture for\nClinical EEG Report Generation with Guaranteed Measurement Accuracy\nICPHDS 2025, November 21â€“23, 2025, Yantai, China\nTable 1: Key hyperparameters (defaults; vary by dataset/ab-\nlation).\nComponent\nSetting\nSampling\nLow: 256â€“512Hz; High: â‰¥1kHz\nEvent windows\nğ‘Š= 2â€“10s; margins Â±2s\nWelch PSD\n8 segments; 50% overlap\nPatches\nCoarse 64; fine 256\nBackbone\nğ‘‘= 512; 8 heads; 4 layers\nForecasting\n9 quantiles; horizon 64\nCalibration\nğ‘›ğ‘ğ‘ğ‘™= 256; ğ›¼= 0.9\n(Eq. (5)) with conformal calibration (Eq. (6)). (5) Cross-modal\nbridge maps EEG features to language-compatible semantic space\nvia progressive expansion with learnable clinical anchors. (6) Con-\nstrained generation uses LoRA-adapted LLM with schema-first\ndecoding: structured JSON populates from frozen slots, then narra-\ntive generation with hard constraints preventing numeric halluci-\nnation.\nHyperparameters. Table 1 summarizes key settings (tuned on\nvalidation data).\n4.2\nFeature Extraction and Neural Architecture\nHierarchical multirate sampling. We maintain synchronized low-\nrate (256â€“512Hz) and high-rate (â‰¥1kHz) streams after standard\npreprocessing (notch 50/60Hz, bandpass 0.5â€“80Hz). The low-rate\nstream continuously captures hours of context. High-rate windows\nwith temporal margins (Â±2s) are extracted around candidate events\ndetected via energy, kurtosis, and spectral peaks with spatial con-\nsensus [15]. This avoids processing hours at high sampling rates\nwhile preserving event-level precision.\nMeasurement-first guardrails. Before neural processing, we com-\npute exact values via Welch PSD (Eq. (7)) for dominant frequency\narg maxğœ”Ë†ğ‘†ğ‘¥ğ‘¥(ğœ”), hysteresis thresholding for durations, median\nabsolute deviation for amplitudes, and graph-based asymmetry in-\ndices for lateralization. These frozen slots carry full provenance\nand serve as immutable supervision targets.\nNeural architecture. Models process dual-view inputs (time-domain\npatches and transform-domain bandpower), plus channel graph\nstructure and covariates. Group attention [7] shares information\nwithin channel sets, augmented with graph attention (Eq. (3)) for\nspatial coherence. SSM layers (S4/Mamba [16,17]) provide linear-time\nlong-range modeling, interleaved with attention for local detail.\nOutput heads include quantile forecasting (pinball loss Eq. (4)),\nseizure detection, and EMD-aware value prediction (Eq. (5)).\n4.3\nCross-Modal Bridge\nThe cross-modal bridge translates EEG features into language-\ncompatible semantic space while preserving measurement fidelity.\nEEG encoder outputs zEEG âˆˆR768 must interface with LLM se-\nmantic spaces (ğ‘‘ğ¿â‰ˆ4096). Rather than direct projection, we use\nprogressive expansion ğ‘“bridge : R768 â†’R4096 with intermediate lay-\ners (768â†’1536â†’2816â†’4096) to prevent information bottlenecks.\nLearnable semantic anchors {ağ‘˜}ğ¾\nğ‘˜=1 guide the mapping from\nelectrophysiological patterns to clinical concepts (seizure status,\ntemporal characteristics, type, severity, spatial patterns). Initialized\nfrom medical terminology embeddings, these anchors are refined\nvia contrastive alignment [18,19] using temperature-scaled InfoNCE\nloss, encouraging EEG embeddings to align with semantically simi-\nlar clinical descriptions.\nThe complete representation concatenates bridge output, frozen\nmeasurements, and context:\nhcomplete = [hbridge; mfrozen; ccontext]\n(1)\nwhere mfrozen contains exact values (frequency, duration, amplitude,\nlocation) from signal processing. Special formatting signals the\ndecoder to copy these values verbatim, ensuring measurement\nfidelity while leveraging neural models for narrative fluency.\n4.4\nValue Extraction and Calibration\nDSP routines compute exact clinical values and insert them as im-\nmutable slots with full provenance (method, window, parameters).\nThe text generator is constrained to copy these values, not generate\nthem. Low-confidence events trigger rule-based fallback phrasing.\nFor neural value prediction, discretized outputs (frequency/du-\nration bins) use EMD-aware supervision (Eq. (5)) that penalizes\ncumulative distribution discrepancies [20,21], reducing regression-by-\nclassification artifacts. Conformalized quantile regression (Eq. (6))\nguarantees coverage for probabilistic forecasts under distribution\nshift [22]. Change-point tests trigger adaptive recalibration during\nnonstationarity. Post-hoc constraints enforce physiologic plausibil-\nity (nonnegative amplitudes, band-consistent frequencies); viola-\ntions trigger re-measurement or abstention.\n4.5\nReport Generation\nWe employ parameter-efficient LoRA adaptation [23] (ğ‘Ÿ= 16, ğ›¼= 32)\nto specialize a pretrained LLM for clinical reporting. Generation\nproceeds in two stages: (i) emit structured JSON schema with frozen\nmeasurement slots; (ii) condition on schema to generate narrative.\nConstrained beam search with custom masking restricts numeric\ngeneration to copying from frozen slots, preventing hallucination.\nEvery value carries full provenance (algorithm, window, channels)\nfor traceability and compliance.\n4.6\nMathematical Formulation\nHierarchical sampling. Low-rate stream X(ğ¿) âˆˆRğ¶Ã—ğ‘‡ğ¿and high-\nrate stream X(ğ») âˆˆRğ¶Ã—ğ‘‡ğ»are synchronized. Gating function G\nselects event windows W = {[ğ‘¡(ğ‘š)\nğ‘ \n,ğ‘¡(ğ‘š)\nğ‘’\n]}ğ‘šfrom X(ğ¿) via ener-\ngy/kurtosis/spectral peaks. High-rate crops:\nZ(ğ»)\nğ‘š\n= X(ğ») [:, ğ‘¡(ğ‘š)\nğ‘ \n: ğ‘¡(ğ‘š)\nğ‘’\n] ,\nğ‘š= 1, . . . , |W|\n(2)\nyield dual-view representation: low-rate context, high-rate preci-\nsion.\nGraph-aware attention. Channel-graph bias B (from montage\nadjacency/distances) augments attention:\nL = QKâŠ¤\nâˆš\nğ‘‘\n+ ğ›½B,\nA = softmax(L) ,\nAttn(Â·) = AV\n(3)\nwhere ğ›½controls spatial bias strength.\n"}, {"page": 4, "text": "ICPHDS 2025, November 21â€“23, 2025, Yantai, China\nZhang et al.\nLosses. Quantile forecasting uses pinball loss:\nLpinball =\nğ»\nâˆ‘ï¸\nâ„=1\nğ¾\nâˆ‘ï¸\nğ‘˜=1\nğœŒğ›¼ğ‘˜\n\u0000ğ‘¦ğ‘¡+â„âˆ’ğ‘ğ›¼ğ‘˜,â„(x1:ğ‘¡)\u0001 ,\nğœŒğ›¼(ğ‘¢) = ğ‘¢(ğ›¼âˆ’I[ğ‘¢< 0])\n(4)\nDiscretized values use EMD loss comparing cumulative distribu-\ntions:\nLEMD =\nğµ\nâˆ‘ï¸\nğ‘=1\n\f\fğ¹p(ğ‘) âˆ’ğ¹y(ğ‘)\n\f\f,\nğ¹p(ğ‘) =\nâˆ‘ï¸\nğ‘–â‰¤ğ‘\nğ‘ğ‘–, ğ¹y(ğ‘) = I[ğ‘â‰¥ğ‘—]\n(5)\nreducing regression-by-classification artifacts.\nCalibration and measurement. Conformalized quantiles guaran-\ntee coverage:\nËœğ‘ğ›¼(x) = Ë†ğ‘ğ›¼(x) + Quantile1âˆ’ğ›¼\n\u0000{ğ‘Ÿğ‘–(ğ›¼)}ğ‘›\nğ‘–=1\n\u0001\n(6)\nwith online updates per recording. Welch PSD averages over ğ‘ˆ\nsegments:\nË†ğ‘†ğ‘¥ğ‘¥(ğœ”) = 1\nğ‘ˆ\nğ‘ˆ\nâˆ‘ï¸\nğ‘¢=1\n\f\fDFT{ğ‘¤Â· ğ‘¥ğ‘¢}(ğœ”)\n\f\f2\n(7)\nDominant frequency: arg maxğœ”Ë†ğ‘†ğ‘¥ğ‘¥(ğœ”); duration: hysteresis thresh-\nolding; amplitude: median absolute deviation.\n5\nExperiments\n5.1\nDatasets and Tasks\nWe evaluate on TUH EEG [24], TUSZ [25], and CHB-MIT [26] datasets\n(Table 2(a)), both US-based corpora, using patient-wise splits to\nprevent leakage. We assess three core tasks: (i) seizure detection\n(false alarms/24h, latency), (ii) value extraction (frequency, dura-\ntion, amplitude MAE), and (iii) localization (lateralization accuracy,\nspatial overlap). Results are stratified by sampling rate (low â‰¤256Hz,\nmid 384â€“512Hz, high â‰¥1kHz) to assess precision-rate tradeoffs.\n5.2\nBaselines and Protocol\nWe compare against classical detectors (energy/rhythm threshold-\ning), deep EEG models (EEGNet [27], DeepConvNet [28], BENDR [3],\nEEGFormer [6]), and Chronos-2-style forecasting [7]. Ablations re-\nmove individual components: graph attention, SSM layers, hierar-\nchical sampling, measurement guardrails, and conformal calibra-\ntion.\nWe use patient-wise cross-validation with early stopping. Class\nimbalance is handled via focal loss for detection and EMD-aware\nsupervision for discretized values. Online conformal calibration\nensures coverage guarantees. All experiments use fixed seeds (42),\nPyTorch 2.3, and run on NVIDIA A100 GPUs.1\n6\nResults\nResults use patient-wise paired tests (Wilcoxon signed-rank) with\nper-corpus stratification.\n1Full reproducibility details, hyperparameters, and code will be released upon\npublication.\n6.1\nDetection, Localization, and Value\nExtraction\nTable 2(b) and Figure 5 show our method achieves 0.51 FA/24h on\nTUH (vs. 1.16 for EEGNet) with 10.5s latency (vs. 24.2s). Hierarchi-\ncal sampling enables early detection without inflating false alarms,\nwhile SSM layers handle extended contexts. For value extraction (Ta-\nble 2(c), Figure 4(a)), measurement-first guardrails achieve 0.18 Hz\nfrequency MAEâ€”within clinical tolerance (0.1 Hz) and 62% better\nthan EEGNet. EMD-aware supervision reduces errors near critical\nboundaries (3.0 vs. 3.5 Hz). Graph-aware modeling improves later-\nalization to 85%+ accuracy and Jaccard overlap >0.7 (Figure 4(b)),\nparticularly for multi-focal patterns.\n6.2\nAblations and Efficiency\nFigure 2(a) quantifies component contributions. Removing guardrails\ndegrades value MAE by 44%, removing graph attention reduces\nlocalization by 24%, removing SSM increases latency by 22%, re-\nmoving hierarchical sampling cuts precision by 23%, and remov-\ning calibration causes 31% undercoverage. Our hierarchical design\nachieves sub-minute end-to-end latency with manageable memory\n(Figure 2(b)), enabling clinical deployment. Orthonormal coefficient\ncomputation preserves bandpower within 2% error while reducing\nstorage 10Ã— (Figure 2(c)).\n6.3\nRobustness and Sampling Rate Analysis\nUnder artifact injection (EOG/EMG/line noise) and missing chan-\nnels, FA/24h increases <30% and value errors <25% (Figure 3(a)).\nMeasurement guardrails prevent implausible outputs in contami-\nnated segments. Performance scales with sampling rate (Figure 3(b)):\nhigh-rate data (â‰¥1kHz) improves detection latency and value pre-\ncision substantially, while low-rate (â‰¤256Hz) suffices for routine\nmonitoring.\n7\nDiscussion\nOur hybrid architecture addresses critical gaps in automated EEG\nanalysis by separating measurement extraction from text genera-\ntion. The combination of signal processing guardrails, hierarchical\nsampling, and graph-aware modeling enables both clinical mea-\nsurement accuracy and scalable long-context processing.\nLimitations and future work. Several limitations warrant consid-\neration. First, our dual-stream architecture targets clinical worksta-\ntions; edge deployment on bedside monitors or wearables requires\noptimization through model pruning and quantization to meet\nmemory and power constraints. Second, frozen measurement slots\nprioritize numeric fidelity over natural language flexibility, limiting\nnuanced uncertainty phrasing and institution-specific terminol-\nogyâ€”future work could explore adaptive template selection while\npreserving accuracy guarantees. Third, evaluation datasets (TUH,\nCHB-MIT) are US-based with limited demographic diversity and\nunderrepresent certain seizure subtypes (myoclonic, focal with pre-\nserved awareness, absence in adults); international multi-center val-\nidation across diverse populations is needed. Additional challenges\ninclude fully automating artifact handling and montage changes.\nPromising directions include subject-specific graph construction,\nadaptive calibration for rapid regime shifts, and semi-supervised\n"}, {"page": 5, "text": "Bridging the Compression-Precision Paradox: A Hybrid Architecture for\nClinical EEG Report Generation with Guaranteed Measurement Accuracy\nICPHDS 2025, November 21â€“23, 2025, Yantai, China\nCoverage_err\nFA/24h\nFreq_MAE\nLatency_s\nmetric\n0\n10\n20\n30\n40\nRelative change (%) (higher is worse)\nAblation\n-SSM\n-calibration\n-graph\n-guardrails\n-hierarchical\n(a) Ablation impacts\nOurs\nEEGNet\nDeepConvNet\nBENDR\nChronos2-baseline\n0\n100\n200\n300\n400\n500\n600\nLatency (ms)\n0\n2\n4\n6\n8\nMemory (GB)\n(b) Latency and memory\nraw\ndwt-keep\ndct-keep\nmultirate-base\nmultirate+events\nsketch\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nRelative error\nBandpower rel. error\nPSD rel. error\n(c) Compression sensitivity\nFigure 2: Ablations, efficiency, and compression analysis. (a) Percentage change in metrics when removing each component. (b)\nEnd-to-end latency and memory usage. (c) Bandpower error on coefficients vs. reconstruction.\nnone\nline_noise\nemg\neog\nmissing_ch\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFA/24h\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAggregate value error (relative)\n(a) Robustness to artifacts\nlow\nmid\nhigh\n0.40\n0.45\n0.50\n0.55\nFA/24h (det.) (lower is better)\nBENDR\nEEGNet\nOurs\nlow\nmid\nhigh\n0.275\n0.300\n0.325\n0.350\n0.375\n0.400\n0.425\nValue MAE (lower is better)\nlow\nmid\nhigh\n0.24\n0.26\n0.28\n0.30\nForecast WQL (lower is better)\n(b) Sampling rate sensitivity\nFigure 3: Robustness analysis and sampling rate impact on performance.\nTable 2: Datasets and performance metrics. (a) Evaluation datasets with sampling rates. (b) False alarm rate and detection\nlatency. (c) Mean absolute error for frequency, duration, and amplitude measurements.\n(a) Datasets\nCorpus\nHz\nTUH/TUSZ\n250â€“512\nCHB-MIT\n256\niEEG\nâ‰¥1k\n(b) Detection metrics\nModel\nFA/24h\nLat.(s)\nOurs\n0.51\n10.5\nEEGNet\n1.16\n24.2\nBENDR\n0.81\n20.3\n(c) Value extraction MAE\nModel\nHz\ns\nğœ‡V\nOurs\n0.18\n1.16\n3.83\nEEGNet\n0.48\n2.32\n7.59\nBENDR\n0.41\n2.05\n6.83\nOurs\nEEGNet\nDeepConvNet\nBENDR\n-guardrails\n-ordinal\n0.0\n0.2\n0.4\nFrequency MAE (Hz)\nOurs\nEEGNet\nDeepConvNet\nBENDR\n-guardrails\n-ordinal\n0\n1\n2\n3\nDuration MAE (s)\nOurs\nEEGNet\nDeepConvNet\nBENDR\n-guardrails\n-ordinal\n0.0\n2.5\n5.0\n7.5\n10.0\nAmplitude MAE ( V)\n(a) Value error distributions\nOurs\nEEGNet\nDeepConvNet\nBENDR\n-graph\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLateralization Acc\nJaccard Overlap\n(b) Localization accuracy\nFigure 4: Performance on value extraction and localization tasks. (a) Value error distributions show measurement accuracy. (b)\nLocalization accuracy across different spatial patterns.\npretraining on unlabeled corpora. The measurement-first paradigm\ngeneralizes beyond EEG to any high-frequency biosignal domain\nwhere precise quantitative values are as critical as descriptive text.\nEthics and deployment. We use publicly available, de-identified\ndatasets (TUH, CHB-MIT) with IRB approval and HIPAA compli-\nance. Clinical deployment requires local IRB approval, encrypted\n"}, {"page": 6, "text": "ICPHDS 2025, November 21â€“23, 2025, Yantai, China\nZhang et al.\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\nFalse alarms per 24 h (lower is better)\n10\n12\n14\n16\n18\n20\n22\n24\nMedian detection latency (s)\n-graph-CHB-MIT\n-graph-TUH\nBENDR-CHB-MIT\nBENDR-TUH\nDeepConvNet-CHB-MIT\nDeepConvNet-TUH\nEEGNet-CHB-MIT\nEEGNet-TUH\nOurs-CHB-MIT\nOurs-TUH\nFigure 5: Detection trade-off: FA/24h vs. latency. Lower-left\nis better.\nstorage (HIPAA/GDPR), audit trails, patient consent, and human\noversight. Our provenance tracking ensures every value is trace-\nable for clinical and legal accountability [29]. Beyond performance,\nstructured reports with frozen measurements can assist clinical\neducation by highlighting diagnostic features (3 Hz vs. 3.5 Hz spike-\nwave, focal vs. generalized spread, HFOs) and providing concrete\nscaffolds for learning electrophysiology.\n8\nConclusion\nWe resolve the compression-precision paradox through a hybrid ar-\nchitecture separating measurement extraction from text generation.\nSignal processing extracts exact clinical values before neural com-\npression, while language models compose narratives around frozen\nmeasurements. Evaluation on TUH and CHB-MIT datasets demon-\nstrates substantial reductions in false alarms and detection latency\nwhile achieving measurement precision within clinical tolerance.\nThis measurement-first paradigm generalizes to high-frequency\nbiosignal domains requiring precise quantitative values alongside\ndescriptive text, enabling trustworthy clinical decision support with\nfull traceability.\nReferences\n[1] World Health Organization. Epilepsy fact sheet, February 2024. URL https:\n//www.who.int/news-room/fact-sheets/detail/epilepsy.\n[2] Jonathan W. Kim, Ahmed Alaa, and Danilo Bernardo. EEG-GPT: Exploring\ncapabilities of large language models for EEG classification and interpretation.\narXiv preprint arXiv:2401.18006, abs/2401.18006, 2024. URL https://arxiv.org/abs/\n2401.18006.\n[3] Demetres Kostas, Stephane Aroca-Ouellette, and Frank Rudzicz. BENDR: Using\ntransformers and a contrastive self-supervised learning task to learn from massive\namounts of EEG data. Frontiers in Human Neuroscience, 15:653659, 2021. doi:\n10.3389/fnhum.2021.653659. URL https://www.frontiersin.org/journals/human-\nneuroscience/articles/10.3389/fnhum.2021.653659/full.\n[4] Jiyeon Lee, Gyeongmin Kim, and Kwanhyung Hong. Automated medical report\ngeneration for ECG data: Bridging medical text and signal processing with deep\nlearning. arXiv preprint arXiv:2412.04067, abs/2412.04067, 2024. URL https:\n//arxiv.org/abs/2412.04067.\n[5] Arthur C. Grant, Samah G. Abdel-Baki, Jeremy Weedon, Vanessa Arnedo, Geetha\nChari, Ewa Koziorynska, Catherine Lushbough, Douglas Maus, Tresa McSween,\nKatherine A. Mortati, Alina Reznikov, and Ahmet Omurtag. EEG interpretation\nreliability and interpreter confidence: A large single-center study. Epilepsy &\nBehavior, 32:102â€“107, 2014. doi: 10.1016/j.yebeh.2014.01.011. URL https://pubmed.\nncbi.nlm.nih.gov/24531133/.\n[6] Yuqi Chen, Kan Ren, Kaitao Song, Yansen Wang, Yifan Wang, Dongsheng Li, and\nLili Qiu. EEGFormer: Towards transferable and interpretable large-scale EEG\nfoundation model. arXiv preprint arXiv:2401.10278, abs/2401.10278, 2024. URL\nhttps://arxiv.org/abs/2401.10278.\n[7] Abdul Fatir Ansari, Oleksandr Shchur, Jaris KÃ¼ken, Andreas Auer, Boran Han,\nPedro Mercado, Syama Sundar Rangapuram, Huibin Shen, Lorenzo Stella,\nXiyuan Zhang, Mononito Goswami, Shubham Kapoor, Danielle C. Maddix, Pablo\nGuerron, Tony Hu, Junming Yin, Nick Erickson, Prateek Mutalik Desai, Hao\nWang, Huzefa Rangwala, George Karypis, Yuyang Wang, and Michael Bohlke-\nSchneider. Chronos-2: From univariate to universal forecasting. arXiv preprint\narXiv:2510.15821, abs/2510.15821, 2025. URL https://arxiv.org/abs/2510.15821.\n[8] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time\nseries is worth 64 words: Long-term forecasting with transformers. arXiv preprint\narXiv:2211.14730, abs/2211.14730, 2023. URL https://arxiv.org/abs/2211.14730.\n[9] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose\nYu. A decoder-only foundation model for time-series forecasting. arXiv preprint\narXiv:2310.10688, abs/2310.10688, 2024. URL https://arxiv.org/abs/2310.10688.\n[10] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Generating ra-\ndiology reports via memory-driven transformer.\nIn Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP),\npages 1439â€“1449, Online, 2020. Association for Computational Linguistics. doi:\n10.18653/v1/2020.emnlp-main.112. URL https://aclanthology.org/2020.emnlp-\nmain.112/.\n[11] Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, and Yuexian Zou. Exploring and\ndistilling posterior and prior knowledge for radiology report generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pages 13753â€“13762, Virtual, 2021. IEEE. URL https://openaccess.thecvf.\ncom/content/CVPR2021/papers/Liu_Exploring_and_Distilling_Posterior_and_\nPrior_Knowledge_for_Radiology_Report_CVPR_2021_paper.pdf.\n[12] Jing Wu, Yuli Wang, Zhengliang Zhong, et al. Vision-language foundation model\nfor 3D medical imaging. npj Artificial Intelligence, 1:17, 2025. doi: 10.1038/s44387-\n025-00015-9. URL https://www.nature.com/articles/s44387-025-00015-9.\n[13] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck\nprinciple. In 2015 IEEE Information Theory Workshop (ITW), pages 1â€“5, Jerusalem,\nIsrael, 2015. IEEE. doi: 10.1109/ITW.2015.7133169. URL https://ieeexplore.ieee.\norg/document/7133169/.\n[14] U.S. Food and Drug Administration.\nClinical decision support soft-\nware: Guidance for industry and food and drug administration staff,\n2022. URL https://www.fda.gov/regulatory-information/search-fda-guidance-\ndocuments/clinical-decision-support-software.\n[15] Xiaosong Li, Yueming Zhou, Nicha Dvornek, Meiyi Zhang, and James Duncan.\nGraph-informed neural networks for spatial and temporal modeling of EEG sig-\nnals in seizure detection. IEEE Transactions on Neural Systems and Rehabilitation\nEngineering, 31:2154â€“2163, 2023.\n[16] Albert Gu, Karan Goel, and Christopher RÃ©. Efficiently modeling long sequences\nwith structured state spaces. In International Conference on Learning Representa-\ntions, Virtual, 2022. OpenReview.net. URL https://arxiv.org/abs/2111.00396.\n[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective\nstate spaces. arXiv preprint arXiv:2312.00752, abs/2312.00752, 2024. URL https:\n//arxiv.org/abs/2312.00752.\n[18] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, abs/1807.03748,\n2018. URL https://arxiv.org/abs/1807.03748.\n[19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. Learning transferable visual models from natural language supervision. In\nInternational Conference on Machine Learning, pages 8748â€“8763, Virtual, 2021.\nPMLR. URL https://arxiv.org/abs/2103.00020.\n[20] Ignacio DÃ­az, Manuel J. MarÃ­n-JimÃ©nez, and Fernando de la Torre.\nSoft\nlabels for ordinal regression.\nIn IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 4738â€“4747, Long Beach, CA, USA, 2019.\nIEEE. URL https://openaccess.thecvf.com/content_CVPR_2019/papers/Diaz_\nSoft_Labels_for_Ordinal_Regression_CVPR_2019_paper.pdf.\n[21] MichaÃ«l Perrot and Zaid Harchaoui. The earth moverâ€™s pinball loss: Quantiles\nfor histogram-valued regression. arXiv preprint arXiv:2106.02051, abs/2106.02051,\n2021. URL https://arxiv.org/abs/2106.02051.\n[22] Chen Xu and Yao Xie.\nConformal time-series forecasting.\nIn Advances\nin Neural Information Processing Systems, pages 11865â€“11877, Virtual, 2021.\nCurran Associates, Inc.\nURL https://proceedings.neurips.cc/paper/2021/file/\n312f1ba2a72318edaaa995a67835fad5-Paper.pdf.\n[23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language\nmodels. In International Conference on Learning Representations, Virtual, 2022.\nOpenReview.net. URL https://arxiv.org/abs/2106.09685.\n[24] Iyad Obeid and Joseph Picone. The temple university hospital EEG data cor-\npus.\nFrontiers in Neuroscience, 10:196, 2016.\ndoi: 10.3389/fnins.2016.00196.\nURL https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.\n2016.00196/full.\n[25] Vinit Shah, Eva von Weltin, Silvia Lopez, James R. McHugh, Lydia Veloso, Meysam\nGolmohammadi, Iyad Obeid, and Joseph Picone. The temple university hospital\nseizure detection corpus. Frontiers in Neuroinformatics, 12:83, 2018. doi: 10.3389/\nfninf.2018.00083. URL https://www.frontiersin.org/journals/neuroinformatics/\narticles/10.3389/fninf.2018.00083/full.\n"}, {"page": 7, "text": "Bridging the Compression-Precision Paradox: A Hybrid Architecture for\nClinical EEG Report Generation with Guaranteed Measurement Accuracy\nICPHDS 2025, November 21â€“23, 2025, Yantai, China\n[26] Ali Shoeb and John Guttag. Application of machine learning to epileptic seizure\ndetection. In Proceedings of the 27th International Conference on Machine Learning\n(ICML), pages 975â€“982, Haifa, Israel, 2010. Omnipress. URL https://physionet.\norg/content/chbmit/1.0.0/.\n[27] Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon,\nChou P. Hung, and Brent J. Lance. EEGNet: a compact convolutional neural\nnetwork for EEG-based brainâ€“computer interfaces. Journal of Neural Engineering,\n15(5):056013, 2018. doi: 10.1088/1741-2552/aace8c. URL https://pubmed.ncbi.nlm.\nnih.gov/29932424/.\n[28] Robin T. Schirrmeister, Jost T. Springenberg, Lukas D. J. Fiederer, Martin Glasstet-\nter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Bur-\ngard, and Tonio Ball. Deep learning with convolutional neural networks for EEG\ndecoding and visualization. Human Brain Mapping, 38(11):5391â€“5420, 2017. doi:\n10.1002/hbm.23730. URL https://pubmed.ncbi.nlm.nih.gov/28782865/.\n[29] Boris BaÄiÄ‡, Claudiu Vasile, Chengwei Feng, and Marian G. CiucÄƒ. Towards nation-\nwide analytical healthcare infrastructures: A privacy-preserving augmented knee\nrehabilitation case study. arXiv preprint arXiv:2412.20733, abs/2412.20733, 2024.\nURL https://arxiv.org/abs/2412.20733.\n"}]}