{"doc_id": "arxiv:2512.08945", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.08945.pdf", "meta": {"doc_id": "arxiv:2512.08945", "source": "arxiv", "arxiv_id": "2512.08945", "title": "The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization", "authors": ["Stefano Epifani", "Giuliano Castigliego", "Laura Kecskemeti", "Giuliano Razzicchia", "Elisabeth Seiwald-Sonderegger"], "published": "2025-11-20T23:51:34Z", "updated": "2025-11-20T23:51:34Z", "summary": "Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT).   Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1).   Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.08945v1", "url_pdf": "https://arxiv.org/pdf/2512.08945.pdf", "meta_path": "data/raw/arxiv/meta/2512.08945.json", "sha256": "aefffd57aa0a80f8465c758472248458e685939da47936ff24793fc40bfdab2f", "status": "ok", "fetched_at": "2026-02-18T02:26:33.623818+00:00"}, "pages": [{"page": 1, "text": "The Linguistic Architecture of Reflective \nThought: Evaluation of a Large Language \nModel as a Tool to Isolate the Formal \nStructure of Mentalization \nStefano Epifani¹², Giuliano Castigliego²³, Laura Kecskemeti⁴, Giuliano Razzicchia², Elisabeth \nSeiwald-Sonderegger⁴ \n \n¹ University of Pavia, Pavia, Italy \n² Digital Transformation Institute, Rome, Italy \n³ Psychoanalytic Academy of Italian-Speaking Switzerland, Switzerland \n⁴ Psychiatric Services of the Canton of Grisons, Chur, Switzerland \n \nAbstract \nBackground \nMentalization integrates cognitive, affective, and intersubjective components. Large Language \nModels (LLMs) display an increasing ability to generate reflective texts, raising questions regarding \nthe relationship between linguistic form and mental representation. This study assesses the extent to \nwhich a single LLM can reproduce the linguistic structure of mentalization according to the \nparameters of Mentalization-Based Treatment (MBT). \nMethods \nFifty dialogues were generated between human participants and an LLM configured in standard \nmode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the \nmentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores \nfor evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was \nestimated using ICC(3,1). \nResults \nMean scores (3.63–3.98) and moderate standard deviations indicate a high level of structural \ncoherence in the generated profiles. ICC values (0.60–0.84) show substantial-to-high agreement \namong raters. The model proved more stable in the Implicit–Explicit and Self–Other dimensions, \nwhile presenting limitations in the integration of internal states and external contexts. The profiles \nwere coherent and clinically interpretable yet characterized by affective neutrality. \n"}, {"page": 2, "text": "Conclusions \nAn LLM can simulate the linguistic structure of human reflectivity, producing profiles that clinicians \nrecognize as formally mentalizing. However, such performance does not imply intentional or \naffective processes, configuring instead a form of algorithmic reflexivity: coherent and repeatable, \nyet non-experiential. The results support the use of LLMs as research and training tools while \nconfirming their inadequacy for direct clinical applications without expert supervision. \nKeywords \nmentalization; MBT; reflective language; large language model; clinical coherence; psychotherapy; \nartificial intelligence. \n \n1. Introduc0on \nMentalization represents one of the core competencies of the human mind: the ability to interpret \none’s own and others’ behaviour in terms of underlying mental states—intentions, desires, emotions, \nand beliefs (Fonagy, Gergely, Jurist, & Target, 2002). \nThis function integrates cognitive, affective, and relational dimensions, enabling individuals to assign \nmeaning to subjective experience and to regulate their internal states within interpersonal contexts \n(Allen, Fonagy, & Bateman, 2008; Bateman & Fonagy, 2016). \nIn the Mentalization-Based Treatment (MBT) model, mentalization is conceptualized as a dynamic \nprocess manifesting along several axes: Cognitive/Affective, Internal/External, Self/Other, \nImplicit/Explicit, and Regulatory/Synthetic, whose integration ensures adaptive psychological \nfunctioning. \nIn recent years, the emergence of Large Language Models (LLMs) has reopened the debate on the \nnature of understanding and reflectivity. \nTrained on massive text corpora and capable of generating coherent language, these systems appear \nable to reproduce forms of reasoning and dialogue analogous to human discourse (Binz & Schulz, \n2023; Shapira, de Wit, & Deroy, 2024). Recent research has suggested that, under certain conditions, \nLLMs may display performance compatible with theory-of-mind tests, showing an apparent capacity \nto represent others’ mental states (Kosinski, 2023). \nHowever, subsequent studies have shown that such results depend largely on the model’s linguistic \nsensitivity rather than on genuine psychological understanding, as minor textual variations can \neliminate response coherence (Ullman, 2023). \nDo LLMs therefore understand, or do they simply predict the linguistic form of understanding? \nFrom a clinical perspective, this question acquires particular relevance. \n"}, {"page": 3, "text": "If mentalization is a function expressed through language, and if language itself possesses a reflective \nstructure, then it is legitimate to ask to what extent a statistical model devoid of conscious experience \ncan simulate the linguistic form of mentalization (Fonagy & Allison, 2014). \nIn other words, the issue is not whether an LLM “thinks”, but whether it produces texts that appear \nmentalizing to a human observer. \nThis distinction recalls the difference proposed by Frith and Frith (2006) between cognitive \nrepresentations of mind and the phenomenological experience of intentionality. \nThe MBT literature emphasizes that mentalization manifests not in the logical correctness of \ndiscourse but in its capacity to tolerate uncertainty, integrate perspectives, and acknowledge the limits \nof one’s knowledge (Fonagy et al., 2002). In this perspective, linguistic reflectivity and experiential \nreflectivity constitute complementary yet distinct levels. \nThe first is formalizable—and thus potentially simulable—as a set of syntactic and semantic rules; \nthe second involves affective, embodied, and relational processes that lie outside the scope of \nstatistical language (Gallese & Sinigaglia, 2011; Zaki & Ochsner, 2012). \nStudying the linguistic behaviour of an LLM in mentalization tasks therefore allows the formal \nlinguistic dimension of reflective processing to be isolated, making observable what is implicit in the \nhuman mind: the grammar of reflective thought. \nThe present study is situated in this frontier between language and mind. \nIts aim is to evaluate whether, and to what extent, an LLM can generate mentalization profiles that \nare clinically coherent and recognized as plausible by expert observers. \nRather than assessing the model’s ability to learn or correct itself, the study seeks to measure the \nstructural and argumentative coherence of automatically generated mentalization profiles, comparing \nthem with independent evaluations by MBT-trained psychiatrists. \nThrough a comparative analysis of a set of simulated dialogues, the study—beyond examining the \nLLM’s ability to simulate mentalization processes—also offers empirical evidence addressing a \ncrucial theoretical question: Is mentalization a function of the mind or a form of language? \nThe results presented here show that reflectivity can be at least partially reproduced as a linguistic \nregularity, opening new avenues for research on the computational representation of the mental and \nits clinical and epistemological implications. \n2. Method \n2.1. Study Design \nThe study adopts a comparative–descriptive design aimed at exploring the capacity of a Large \nLanguage Model (LLM) to generate mentalization profiles consistent with the theoretical and clinical \nparameters of Mentalization-Based Treatment (MBT; Bateman & Fonagy, 2016; Fonagy, Gergely, \nJurist, & Target, 2002). To ensure replicability and methodological transparency, the model was \nexecuted in its standard configuration, with temperature set to 0.2, maximum generation length of \n"}, {"page": 4, "text": "1,200 tokens, and a fixed seed. These settings reduce stochastic variability in text generation and \nensure greater consistency across produced profiles. \nThe primary objective is to assess the structural coherence and perceived clinical validity of the \nprofiles generated by the model, by comparing them with the independent evaluations of five \npsychiatrists trained in MBT. \nIn contrast to previous studies examining the use of LLMs in psychological contexts (Binz & Schulz, \n2023; Kosinski, 2023), the present research does not investigate the model’s capability to solve \ncognitive tasks but its competence in reproducing the linguistic form of reflective discourse. \nThe study is therefore cross-sectional and exploratory, involves no experimental manipulations or \nlearning interventions applied to the LLM, and seeks to measure the stability and convergence of \nclinical evaluations assigned to the generated profiles. \nThe unit of analysis is the complete dialogue between the model and the human participant. \nEach dialogical cycle consisted of 8 to 12 question–answer exchanges and concluded with the \ngeneration, by the model, of a free-text mentalization profile structured to comment on the individual \nmentalization axes, explicitly referencing the participant’s responses on which the profile was based. \nThe quality of the profile was subsequently evaluated along the four MBT axes—Cognitive–\nAffective, Internal–External, Self–Other, Implicit–Explicit—together with the cross-cutting \nRegulatory/Synthetic dimension (Allen, Fonagy, & Bateman, 2008). \n2.2. Materials and Dialogue Genera>on \nThe corpus analysed consists of 50 simulated dialogues conducted using ChatGPT-4.1 (OpenAI), \nconfigured to assume the role of a therapist engaged in a mentalization-oriented interview. \nThe conversations were based on 20 initial stories generated by the model and subsequently validated \nby psychiatrists trained in MBT to ensure thematic coherence, narrative plausibility, and balance \nbetween cognitive and affective components. This validation controlled the semantic content of the \nstarting material, ensuring that any observed variation could be attributed to the linguistic behaviour \nof the model rather than to intrinsic differences among stories. \nFor each dialogue, the model was provided with one of the stories and instructed to formulate a \nsequence of mentalization-oriented questions designed to explore the core MBT dimensions (Fonagy \net al., 2002). The answers to these questions were provided by 15 human subjects recruited through \nnon-clinical convenience sampling; each participant contributed on average to three dialogues. No \ndemographic or psychometric data were collected, as the aim was not to evaluate participants’ \nmentalization quality, but to assess the model’s ability to generate coherent and clinically \ninterpretable mentalization profiles from heterogeneous input (Shapira, de Wit, & Deroy, 2024). \nAt the end of each dialogue, the model produced a narrative mentalization profile synthesizing the \nemerging content and organizing it according to the four MBT axes, together with the regulatory and \nsynthetic dimensions. \nThese profiles were then submitted for evaluation by an independent group of expert clinicians. \n"}, {"page": 5, "text": "2.3. Par>cipants and Raters \nFive clinical psychiatrists, all MBT-certified and with more than ten years of therapeutic experience, \nserved as raters. \nAll material provided to them was fully anonymized, including only the complete text of the dialogues \n(questions and answers) and the narrative profiles generated by the model, presented in uniform \ntextual format and devoid of any references to human participants or the generating model. \nRaters worked under full-blind conditions with respect to the origin of the texts and were unaware of \nthe use of an artificial intelligence system. \nEach of the 50 dialogues was examined by all clinicians, with individually randomized presentation \norder to reduce potential sequence effects, familiarity phenomena, or learning processes. \nEvaluations were conducted independently, asynchronously, and without time constraints. \n2.4. Evalua>on Procedure \nFor each dialogue, clinicians assigned two Likert scores (1–5) for each MBT axis (Cognitive–\nAffective, Internal–External, Self–Other, Implicit–Explicit, Regulatory, and Synthetic). \nThe two dimensions of judgment were: \n1. Evaluative coherence (“Is the profile consistent with your clinical evaluation of the \ndialogue?”) \n2. Argumentative coherence (“Are the arguments provided by the model internally coherent \nand logically grounded?”) \nIn addition to these ratings, a global quality score (1–5) was assigned as a synthetic indicator of the \nprofile’s overall clinical coherence according to MBT interpretative criteria (Bateman & Fonagy, \n2016), along with a brief free-form descriptive comment aimed at qualifying the quality of \nmentalization expressed in the text. \nThe combination of anonymization, blinding, individualized randomization of dialogue order, and \nindependence of the evaluative process ensured controlled and replicable experimental conditions, \nminimizing biases related to text origin, sequential exposure, or rater interaction. \nThis design ensures that observed score differences can be more reliably attributed to variations in \nthe linguistic and conceptual quality of the profiles generated by the model. \n2.5. Data Analysis \nData analysis followed a descriptive and comparative approach, aimed at assessing the coherence and \ninter-clinician stability of the evaluations provided by the five psychiatrists. \nBecause the study’s objective was not to test inferential hypotheses but to verify the structural and \nargumentative consistency of the LLM-generated mentalization profiles, analyses focused on \nmeasures of central tendency, dispersion, and concordance. \n"}, {"page": 6, "text": "For each MBT axis (Cognitive–Affective, Internal–External, Self–Other, Implicit–Explicit, \nRegulatory, and Synthetic), mean scores and standard deviations were calculated for both evaluative \nand argumentative coherence. \nA global quality score, calculated as the average of all ratings, was used as a synthetic indicator of \nthe profile’s perceived clinical validity. \nThis value was also compared with the individual global quality ratings provided by each clinician to \nverify convergence between aggregated estimates and individual evaluations. \nObserved convergence was high, indicating that the aggregated measure did not distort the evaluators’ \njudgments and that the perceived coherence of the profiles was stable across raters. \nInter-rater agreement was estimated using the Intraclass Correlation Coefficient [ICC(3,1)], \nappropriate for designs involving multiple independent observers rating the same cases (Landis & \nKoch, 1977). \nThe chosen model allows the reliability estimate to generalize to a hypothetical population of \nsimilarly trained raters. Confidence intervals were calculated for each axis, enabling distinctions \nbetween substantial (0.60–0.79) and near-perfect (≥0.80) agreement levels. \nThe sample size of 50 dialogues was determined based on the empirical analysis of standard error \n(SE) across progressive sample increments. SE decreased substantially from 20 to 40 dialogues \n(0.138 → 0.098; absolute reduction −0.040; relative reduction 29%). Beyond this threshold, precision \ngains became marginal: from 40 to 50 dialogues SE decreased by only −0.010 (−11%), and for \nsubsequent increments of ten units, reductions stabilized between 5% and 9% (SE = 0.088 at 50; SE \n= 0.062 at 100). This plateau indicates that increasing the sample size beyond 50 does not \nmeaningfully alter estimate uncertainty. \nThis configuration aligns with methodological models describing the inverse relationship between N \nand standard error (Bonett, 2002) and with classical recommendations for ICC stability in samples of \n30–50 observations (Shrout & Fleiss, 1979; Koo & Li, 2016). \nThus, 50 dialogues represent an optimal balance between estimation accuracy, SE containment, and \nproportionality to the study’s exploratory goals. \nThe number of five independent raters was chosen based on ICC stability across varying numbers \nof raters. Preliminary analyses conducted over subsets of 3, 4, 5, and 6 raters showed that the increase \nfrom four to five clinicians resulted in an ICC variation of less than 0.02, indicating that additional \nraters would contribute marginally to informational gain. \nThis choice aligns with methodological guidelines indicating that three raters represent a minimum \nadequate threshold for stable ICC estimation, whereas increases beyond five provide limited \nimprovements (Shrout & Fleiss, 1979; McGraw & Wong, 1996; Koo & Li, 2016). \nSelecting five clinicians thus ensured an optimal balance between statistical robustness, controlled \nredundancy, and operational feasibility. \nTogether, these indicators—score distributions, internal variance, and inter-rater agreement—allow \nan evaluation not only of numerical consistency but also of the interpretative stability of the reflective \ncategories applied to the LLM-generated texts. \n"}, {"page": 7, "text": "3. Results \n3.1. General Descrip>on of the Evalua>ons \nAll 50 dialogues generated by the language model were independently evaluated by the five \npsychiatrists trained in Mentalization-Based Treatment (MBT). \nMean scores assigned across the four axes and the two additional profiles ranged from 3.63 to 3.98, \nwith standard deviations between 0.65 and 0.94, values consistently located within the mid-to-high \nrange of the 1–5 scale. \nThe axis with the highest mean score was Implicit–Explicit (3.98), whereas the Internal–External \naxis showed the lowest mean (3.63). \nThe distribution of scores did not present outliers and showed limited dispersion, indicating a high \ndegree of homogeneity among clinical evaluations and the absence of recognizable systematic bias. \nThis pattern suggests that observed score differences reflect intrinsic characteristics of the \nmentalization profiles generated by the model rather than subjective discrepancies attributable to \nindividual raters. \nAdditionally, 76% of profiles received a score ≥ 4, whereas only 4% received scores ≤ 2. This \ndistribution, concentrated in the mid-to-high range, confirms the stability of judgments and the \nabsence of anomalous evaluative patterns. \nThe overall trend of means and standard deviations indicates a good level of structural coherence in \nthe texts produced by the model, confirming the presence of syntactic–semantic regularities \npreviously documented in studies on Large Language Models (Binz & Schulz, 2023; Shapira, de Wit, \n& Deroy, 2024). \nIn summary, the data show that the model produces profiles that are, on the whole, linguistically \norganized, formally coherent, and clinically readable, albeit with systematic differences across the \nvarious dimensions of mentalization. \nMeans, standard deviations, and inter-rater agreement values are summarized in Table 1, which \nreports all quantitative indicators used in the present study. \n \nTable 1. Summary of Quan2ta2ve Indicators \n \nFactor \nMean \nStd Deviation \nICC \nStd Error \nCognitive vs Affective 3.83 \n0.81 \n0.66 \n0.115 \nInternal vs External \n3.63 \n0.94 \n0.84 \n0.113 \nSelf vs Other \n3.92 \n0.66 \n0.60 \n0.093 \nImplicit vs Explicit \n3.98 \n0.76 \n0.69 \n0.107 \nRegulatory Functions 3.86 \n0.65 \n0.78 \n0.092 \nIntegrated Synthesis \n3.98 \n0.65 \n0.65 \n0.092 \nGlobal Quality Score \n3.77 \n0.75 \n0.79 \n0.106 \n"}, {"page": 8, "text": "3.2. Convergence Among Raters \nConvergence among the five psychiatrists was assessed through the Intraclass Correlation Coefficient \nICC(3,1), methodologically appropriate for estimating agreement among independent raters \nevaluating the same cases. \nThe obtained values ranged from 0.60 to 0.84, corresponding to a substantial-to-high level of \nagreement according to the classification by Landis and Koch (1977). \nAmong specific dimensions, the Synthetic axis showed an ICC of 0.65, while the Global Quality \nScore reached 0.79. \nThe remaining axes also fell within a range consistent with high clinical reliability, indicating strong \ninterpretative convergence among the evaluators. \nThis stability aligns with the syntactic–semantic regularity already evidenced by the mean ratings and \nstandard deviations. \nMethodologically, ICC values between 0.60 and 0.80 represent a solid indicator of reproducibility of \nclinical judgment, suggesting that the profiles generated by the model contain sufficiently robust \nstructural elements to elicit agreement among independent observers. \nThe absence of extreme variations among raters further confirms that perceived text quality reflects \nintrinsic properties of the generated material rather than individual interpretative differences. \nOverall, the ICC results indicate that ratings attributed to the various MBT axes reflect not only \ninternal coherence but also inter-subjective coherence, reinforcing the validity of the clinical \nevaluations reported in the preceding section. \n3.3. Axis-by-Axis Analysis \nThe analysis of mean scores for each MBT axis provides an articulated picture of the model’s \nstrengths and limitations in producing mentalization profiles. Results are presented below by axis. \n3.3.1. Implicit–Explicit Axis \nMean = 3.98; SD = 0.76 \nThis axis showed the highest performance. \nClinicians noted the model’s marked ability to make complex mental processes explicit, linking \nemotions, cognitions, and behaviours coherently. \nThis result is consistent with evidence showing that LLMs tend to favour linguistic transparency and \ncognitive structuring of discourse (Shapira et al., 2024). \nHowever, certain clinical evaluations highlighted argumentative rigidity and an over-organized \nlogical structure, which may reduce narrative spontaneity. \n"}, {"page": 9, "text": "The model excels in conceptual clarity but tends to produce descriptions that are overly linear, lacking \nthe ambiguity and emotional oscillations typical of human mentalization. \n3.3.2. Self–Other Axis \nMean = 3.92; SD = 0.66 \nThe model demonstrated a solid ability to distinguish subjective perspectives from those of others, \ncorrectly employing mental-state attribution forms (“he may think that…”, “the other person might \nfeel that…”). \nClinicians appreciated the syntactic accuracy with which perspectives were differentiated, suggesting \na proper use of the “grammar of intersubjective perspective-taking” described by Frith and Frith \n(2006). \nNevertheless, this differentiation remained predominantly descriptive, lacking the empathic depth \nand affective complexity characteristic of authentic mentalization. \nProfiles appeared cognitively coherent but not equally robust in the emotional–relational domain. \n3.3.3. Cogni<ve–Aﬀec<ve Axis \nMean = 3.83; SD = 0.81 \nScores indicated a general balance between cognitive and affective components, though with a clear \npredominance of cognitive structuring. \nClinicians reported that the model tends to rationalize emotional processes, reducing them to \nordered and predictable categories. \nThis pattern aligns with observations by Gallese and Sinigaglia (2011), according to whom linguistic \nsystems—and even more so artificial ones—lack embodied experience, limiting their ability to \nrepresent emotional tone authentically. \nOverall, the language produced appears correctly empathic but affectively neutral: more \nexplanatory than emotionally engaged. \n3.3.4. Internal–External Axis \nMean = 3.63; SD = 0.94 \nThis axis showed the weakest performance. \nGenerated profiles tended to describe internal states and external contexts in parallel, without \nintegrating them into a dynamic or causal structure. \nThe discourse was coherent but decontextualized, and clinicians noted difficulty in linking subjective \nexperience with ongoing events or relationships. \n"}, {"page": 10, "text": "This limitation is consistent with recent findings on LLMs’ limited capacity to represent the situated \nconnection between mind and environment (Ullman, 2023). \nThe model produces grammatically correct sequences, but not genuinely context-embedded \nnarratives. \n3.3.5. Regulatory Proﬁle \nMean = 3.86; SD = 0.65 \nThis dimension reflects the model’s ability to integrate and modulate emotional, cognitive, and \nbehavioural elements. \nClinical evaluations indicated overall coherence but highlighted superficiality in articulating \nregulatory processes. \nThe model appears capable of organizing discourse but not of representing the dynamic tension \nbetween emotion and control that characterizes authentic psychological regulation (Fonagy & \nAllison, 2014). \nRegulatory passages often appeared prescriptive or overly linear. \n3.3.6. Synthe<c Proﬁle \nMean = 3.98; SD = 0.65 \nTogether with the Implicit–Explicit axis, this dimension showed the highest performance. \nThe model displayed a strong capacity to integrate heterogeneous information into coherent \nsynthesis, producing stable, interpretable narrative structures. \nRaters recognized notable stability in the model’s ability to organize mental material in structured \nform. \nHowever, consistent with other dimensions, the synthesis remained largely cognitive, without deeper \nincorporation of affective or contextual elements. \n3.4. Coherence and Validity of Clinical Evalua>ons \nIn over 80% of cases, clinicians described the model-generated profiles as “coherent”, “clinically \nplausible”, or “logically structured.” \nHowever, recurring observations pointed to affective neutrality and limited emotional depth. \nAlthough articulate and formally accurate, many profiles expressed ordered reflectivity lacking the \nexperiential tension typical of genuine human mentalization (Fonagy & Allison, 2014; Fonagy et al., \n2002). \n"}, {"page": 11, "text": "Evaluative coherence (the degree to which the model’s profile matched the clinician’s assessment) \nand argumentative coherence (the internal logic of the model’s reasoning) showed a positive \nassociation, suggesting that clinicians integrate these dimensions during judgment. \nTexts perceived as logically coherent were also deemed clinically coherent—consistent with the link \nbetween syntactic order and perceived cognitive reflectivity described by Frith & Frith (2006). \nQuantitatively, the highest means in the Implicit–Explicit (3.98) and Self–Other (3.92) axes indicate \nthat the model can reliably reproduce the grammar of intersubjective perspective-taking. \nConversely, lower scores on the Internal–External axis (3.63) confirm the model’s difficulties in \nlinking subjective experience with environmental context, reflecting a tendency toward parallel rather \nthan integrative descriptions (Ullman, 2023). \nOverall, results align with recent literature indicating that LLMs exhibit high syntactic–semantic \ncompetence in reproducing the form of reflective discourse but do not manifest the experiential \ncomplexity underlying human mentalization (Binz & Schulz, 2023; Shapira et al., 2024). \n3.5. Summary of Findings \nThe integrated analysis of quantitative and qualitative data shows that the mentalization profiles \ngenerated by the model display high structural coherence, significant interpretative stability, and good \nclinical recognizability. \nMean scores between 3.63 and 3.98, moderate standard deviations, and substantial-to-high ICC \nvalues (0.60–0.84) indicate that the generated texts meet the formal criteria of reflective discourse, \nappearing readable, orderly, and internally stable. \nIn other words, the LLM demonstrates the ability to organize language according to syntactic and \nsemantic regularities that clinicians recognize as reflective. \nProfiles are coherent not only linguistically but also in distinguishing perspectives, explicating logical \nconnections, and integrating narrative elements into comprehensible structures. \nHowever, results also show systematically that such coherence does not imply genuine affective or \nintentional experience. \nAlthough the model represents mental states formally, it does not reproduce the emotional dynamics, \nregulatory oscillations, or experiential tension that characterize human mentalization (Fonagy & \nAllison, 2014). \nAffective and contextual components—especially in the Internal–External and partly in the \nCognitive–Affective axes—remain more descriptive than lived. \nThus, the model exhibits linguistic but not experiential reflectivity, consistent with the notion of \nan emerging algorithmic reflexivity explored in section 4.3. \nReflectivity appears grammatical: coherent, repeatable, structurally organized—but lacking \npsychological depth. \nThe human mind mentalizes because it experiences; the model does so because it calculates. \n"}, {"page": 12, "text": "It “mentalizes” by ordering language, not by generating mental states. \nThe distinction lies not in linguistic coherence but in the capacity to sustain the uncertainty and \ninternal discontinuity intrinsic to authentic mentalization. \n4. Discussion \nThe results demonstrate that the language model is capable of generating mentalization profiles \ncharacterized by structural coherence, inter-rater stability, and clinical recognizability. \nMean scores (3.63–3.98), moderate standard deviations (0.65–0.94), and ICC values (0.60–0.84) \nindicate that the model produces linguistic representations of reflective thinking that meet the formal \ncriteria typically associated with narrative mentalization. \nThese findings align with a growing body of literature attributing to LLMs a high degree of \ncompetence in generating coherent syntactic–semantic structures (Binz & Schulz, 2023; Shapira et \nal., 2024). \n4.1. Structural Diﬀerences Among Mentaliza>on Dimensions \nThe axis-by-axis analysis reveals a differentiated profile of performance. \nThe Implicit–Explicit and Synthetic dimensions show the highest scores (mean 3.98; ICC 0.69–\n0.65), indicating that the model excels at explicating mental connections and producing coherent \nnarrative syntheses. \nThe Self–Other axis (mean 3.92; ICC = 0.60) confirms the model’s ability to reproduce the grammar \nof intersubjective perspective-taking (Frith & Frith, 2006), while maintaining a predominantly \ndescriptive orientation. \nIn contrast, the Cognitive–Affective dimension—and particularly the Internal–External axis (mean \n3.63; ICC ≈ 0.84)—reveals the model’s limitations. \nThe model struggles to integrate internal states with contextual dynamics and to represent emotional \ntone in a situated manner, consistent with literature documenting the constraints imposed by the \nabsence of embodied experience in LLMs (Gallese & Sinigaglia, 2011; Ullman, 2023). \n4.2. Linguis>c Coherence and Experien>al Depth \nClinical evaluations converge in attributing high linguistic coherence to the generated profiles: the \ndiscourse is regular, logical connections are explicitly articulated, and perspectival elements are \nstructured according to patterns that favour perceptions of cognitive reflectivity—consistent with the \nassociation between syntactic order and perceived mentalizing described by Frith & Frith (2006). \nHowever, this formal solidity is not accompanied by experiential depth comparable to human \nmentalization. \n"}, {"page": 13, "text": "Clinicians consistently noted affective neutrality, describing emotional content as linear, \npredictable, and lacking the ambivalence, tonal oscillations, and regulatory variations typical of \nembodied mentalization (Fonagy & Allison, 2014). \nThis limitation aligns with the non-situated nature of language models, which lack the bodily \nexperience required to generate genuinely integrated emotional representations (Gallese & Sinigaglia, \n2011) and tend to produce affectively plausible but contextually unanchored content (Ullman, 2023). \nThe result is a form of ordered but non-experiential reflectivity: \nthe model integrates perspectives and mental connections at the formal level but does not reproduce \nthe emotional dynamics sustaining human mentalization. \nThis neutrality is not an error but a structural constraint rooted in the absence of phenomenological \nexperience, sharply demarcating the boundary between linguistic coherence and psychological depth. \n4.3. A Form of Algorithmic Reﬂexivity \nWithin this empirical framework, the model’s behaviour can be described as a form of algorithmic \nreflexivity, understood as a modality of representing mental states based solely on the statistical \norganization of language. \nFrom a methodological standpoint, the distinction between linguistic form and mental process is \nessential, as it underlies numerous interpretative errors (Epifani, 2025). \nThe texts produced show syntactic and semantic coherence and integrate multiple perspectives \naccording to ordered schemes recognizable by clinicians as compatible with cognitive reflectivity \n(Frith & Frith, 2006). \nHowever, this formal coherence does not imply intentional or affective processes: emotional \nrepresentation remains descriptive and linear, lacking the ambivalence, discontinuity, and regulatory \ndynamics that characterize human mentalization (Fonagy & Allison, 2014). \nThis limitation is consistent with the lack of embodiment in language models, which do not possess \nthe bodily experience required to generate situated internal states or contextual affective variations \n(Gallese & Sinigaglia, 2011; Ullman, 2023). \nAlgorithmic reflexivity therefore constitutes a capacity for formal ordering and integration of \ndiscourse, distinct from psychological mentalization, which requires phenomenological and relational \nexperience beyond the reach of linguistic systems. \n4.4. Theore>cal Implica>ons \nThe findings clarify the distinction between mentalization as a function—experiential, embodied, \naffectively modulated—and mentalization as a linguistic form—descriptive, explicit, structured. \nThe model’s performance indicates that some components of mentalization, particularly those \nrelating to cognitive perspective-taking, synthesis, and explicit inferential connections, can be \nadequately simulated through linguistic regularities. \n"}, {"page": 14, "text": "In contrast, other components—contextual integration, affect regulation, ambivalence—remain \ndependent on psychological processes not reducible to linguistic form. \nThis contributes to ongoing debates on the limits and possibilities of LLMs as tools for evaluating or \nsimulating socio-cognitive processes. \n4.5. Epistemological Implica>ons \nThe emergence of algorithmic reflexivity raises important epistemological questions about the status \nof mental knowledge generated by LLMs. \nFirst, the results show that the model’s knowledge is non-experiential and derives from the extraction \nof linguistic regularities rather than from subjective experience. \nThus, the model does not “know” mental states in the psychological sense; rather, it reconstructs them \nbased on statistically learned linguistic schemas. \nThis gives rise to an epistemic distinction between: \n• \ngenerative knowledge (reconstruction of mental states through linguistic patterns) \n• \nphenomenological knowledge (lived experience, foundational to human mentalization) \nSecond, the results suggest that what clinicians recognize as mentalization is tied to the linguistic \nform of reflectivity, not to its mental origin. \nThis raises a further epistemic issue: clinical evaluation is sensitive to linguistic structure regardless \nof the presence or absence of genuine intentional experience. \nFinally, the findings contribute to discussions on the epistemic nature of non-situated systems: their \ncapacity to generate mentalizing statements does not imply access to intentional states, but introduces \na form of knowledge that is formal, non-intentional, non-experiential, yet relevant to modelling \naspects of reflective thinking. \nThis invites a reconsideration of the relationships among language, intentionality, and psychological \nknowledge in artificial systems. \n4.6. Limita>ons of the Study \nThe study presents several limitations primarily concerning the conceptual framework and evaluative \nmodalities adopted. \nFirst, the clinical evaluation of mentalization profiles is inevitably influenced by linguistic properties \nof the text, since clinicians judge structured linguistic material rather than experiential processes. \nConsequently, the assigned scores reflect the model’s ability to organize language into recognizable \npatterns more than the presence of underlying psychological dynamics. \nSecond, although inter-rater agreement was substantial, clinical judgments remain susceptible to \ninterindividual variability in interpretative sensitivity, particularly in more complex domains of \nmentalization such as affective and contextual integration. \n"}, {"page": 15, "text": "This does not compromise the validity of the results but delimits their epistemic scope: what is \nrecognized as reflective derives from linguistic form, not real mental experience. \nFinally, the study analyzes a single language model in a single task configuration. \nThis prevents establishing the extent to which results generalize to other architectures or \nconversational contexts. \nThe observed form of algorithmic reflexivity may depend on specific features of the model used, the \nprompt structure, or the dialogue format. \nThe profile organization is influenced by the initial prompting, which partially structures the \ngenerated text. \nThus, some of the observed coherence may reflect instructional effects rather than intrinsic model \nproperties. \nThese limitations do not detract from the value of the results but delineate more precisely the \nboundaries within which they can be interpreted, clarifying that the study concerns the linguistic \nform of mentalization, not the replication of psychological processes that generate it. \n5. Conclusions \nThe results of this study show that a large language model is capable of generating mentalization \nprofiles that consistently meet formal criteria recognized by clinicians as characteristic of reflective \ndiscourse. \nMean scores and inter-rater agreement values indicate that in dimensions most closely linked to the \ncognitive structuring of language—such as the explication of mental connections, perspectival \ndifferentiation, and synthetic organization—the model produces stable and interpretable \nrepresentations. \nThese findings confirm that some components of mentalization, particularly those tied to linguistic \nform and discourse logic, can be simulated through statistical regularities in language. \nAt the same time, lower scores observed in axes related to the integration of internal states and \nexternal context, and to affective modulation, highlight the model’s structural limitations in \nrepresenting elements of experience that cannot be reduced to linguistic organization alone. \nThese areas require the ability to situate emotional experiences within a dynamic, contextually \nembedded frame—an ability intrinsically tied to subjective experience and thus not fully modelled \nby systems lacking embodiment. \nTaken together, these results delineate a clear distinction between what an LLM can simulate and \nwhat remains unique to human mentalization: \nthe model reproduces the formal structure of reflective thought but not the psychological dynamics \nthat generate it. \n"}, {"page": 16, "text": "This distinction does not diminish the potential usefulness of language models in research and \ntraining, where the ability to generate coherent, repeatable, and controllable examples can support the \nanalysis of the linguistic components of mentalization. \nAt the same time, the findings confirm that such systems cannot substitute for clinical intervention, \nas they lack the experiential, affective, and regulatory dimensions that constitute the core of the \nmentalizing function. \nIn conclusion, the study shows that LLMs offer a specific contribution: they make the formal and \ngrammatical component of reflectivity observable, allowing the isolation and analysis of linguistic \naspects of mentalization. \nBy contrast, the human component remains indispensable for affective experience, contextual \ngrounding, and emotional regulation. \nUnderstanding this distinction is crucial for defining the role of language models in clinical, \neducational, and research contexts and represents a starting point for future explorations aimed at \nclarifying the boundaries between linguistic simulation and real psychological processes. \n6. Future Direc0ons \nThe findings of this study open several unexplored research avenues that may contribute to a more \nprecise understanding of the relationships among language, mentalization, and computational \nmodelling of socio-cognitive processes. \nA first direction concerns the analysis of temporal dynamics through which the model progressively \nconstructs reflectivity over the course of dialogue. \nThe analysis of final profiles does not capture how the model manages extended reflective sequences, \npossible disruptions in narrative continuity, or spontaneous variations in coherence across turns. \nMethods from computational linguistics—such as thematic progression analysis, clustering of \nrecurring semantic patterns, or multi-turn perspectival flexibility metrics—could reveal evolutionary \npatterns in generated reflectivity. \nA second area concerns the development of computational metrics for linguistic mentalization. \nQuantitative indicators related to syntactic coherence, inferential depth, explicitation of mental \nconnections, and perspectival differentiation could be systematically correlated with the clinical \nevaluations provided by MBT experts. \nThe creation of annotated datasets and validated linguistic markers would enable the development of \nobjective tools for automated assessment of reflective thinking. \nA third direction involves designing more ecological experimental tasks, able to test not only \nexplicit reflectivity but also the preliminary phases of the mentalizing process. \nIn particular, scenarios eliciting pre-mentalizing states—teleological stance, psychic equivalence, \nand pretend mode—could assess how the model handles conditions corresponding to early and less \nmature forms of mental representation. \n"}, {"page": 17, "text": "Dialogues with high affective intensity, concrete thinking, or pseudo-mentalizing dynamics would \nmake it possible to observe whether the LLM maintains formal coherence, tends to reinterpret \nexperience in mentalized terms, or reflects linguistic signals characteristic of pre-mentalizing modes. \nSuch tasks would assess not only the model’s reflective capacity but also its ability to respond \nappropriately to states in which mentalization is fragile, discontinuous, and marked by recurrent \nbreakdowns. \nFinally, language models could serve as computational baselines for investigating the linguistic \nstructure of human reflective thought. \nAn LLM can provide a formal reference useful for isolating which components of mentalization \nemerge from subjective experience and which derive from discursive regularities. \nMoreover, it can generate controlled narrative variants that allow researchers to study how clinicians \nperceive reflectivity across different contexts, offering an experimental tool for analyzing the \nsensitivity of clinical judgment. \nThese directions confirm that, despite lacking any form of psychological experience, language models \ncan constitute a computational laboratory for investigating the linguistic grammar of mentalization, \nincluding early pre-mentalizing modes. \nFuture research may thus refine clinical tools, enrich theoretical understanding, and expand the \npossibilities for integration among cognitive science, psychotherapy, and artificial intelligence. \nReferences \nAllen, J. G., Fonagy, P. & Bateman, A. W. Mentalizing in Clinical Practice. Washington, DC: \nAmerican Psychiatric Publishing, 2008. \nBateman, A. W. & Fonagy, P. Mentalization-Based Treatment for Personality Disorders: A Practical \nGuide. 2nd edn. Oxford: Oxford University Press, 2016. \nBinz, M. & Schulz, E. ‘Using Cognitive Psychology to Understand GPT-3’. Proceedings of the \nNational \nAcademy \nof \nSciences, \n120(7), \n2023, \ne2218521120. \nhttps://doi.org/10.1073/pnas.2218521120 \nEpifani, S. The Theatre of Thinking Machines: 10 False Myths About Artificial Intelligence and How \nto Overcome Them. Rome: Digital Transformation Institute, 2025. \nFonagy, P. & Allison, E. ‘The Role of Mentalizing and Epistemic Trust in the Therapeutic \nRelationship’. Psychotherapy, 51(3), 2014, 372–380. https://doi.org/10.1037/a0036505 \nFonagy, P., Gergely, G., Jurist, E. L. & Target, M. Affect Regulation, Mentalization, and the \nDevelopment of the Self. New York: Other Press, 2002. \nFrith, C. D. & Frith, U. ‘The Neural Basis of Mentalizing’. Neuron, 50(4), 2006, 531–534. \nhttps://doi.org/10.1016/j.neuron.2006.05.001 \n"}, {"page": 18, "text": "Gallese, V. & Sinigaglia, C. ‘What Is So Special About Embodied Simulation?’. Trends in Cognitive \nSciences, 15(11), 2011, 512–519. https://doi.org/10.1016/j.tics.2011.09.003 \nKoo, T. K. & Li, M. Y. ‘A Guideline of Selecting and Reporting Intraclass Correlation Coefficients \nfor Reliability Research’. Journal of Chiropractic Medicine, 15(2), 2016, 155–163. \nhttps://doi.org/10.1016/j.jcm.2016.02.012 \nKosinski, M. ‘Theory of Mind May Have Spontaneously Emerged in Large Language Models’. arXiv \npreprint, 2023. arXiv:2302.02083. \nLandis, J. R. & Koch, G. G. ‘The Measurement of Observer Agreement for Categorical Data’. \nBiometrics, 33(1), 1977, 159–174. https://doi.org/10.2307/2529310 \nMcGraw, K. O. & Wong, S. P. ‘Forming Inferences About Some Intraclass Correlation Coefficients’. \nPsychological Methods, 1(1), 1996, 30–46. https://doi.org/10.1037/1082-989X.1.1.30 \nShapira, N., de Wit, L. & Deroy, O. ‘Large Language Models as Social Predictors’. Nature Human \nBehaviour, 2024. https://doi.org/10.1038/s41562-024-01805-w \nShrout, P. E. & Fleiss, J. L. ‘Intraclass Correlations: Uses in Assessing Rater Reliability’. \nPsychological Bulletin, 86(2), 1979, 420–428. https://doi.org/10.1037/0033-2909.86.2.420 \nUllman, T. D. ‘Large Language Models Fail on Tricky Theory-of-Mind Tasks’. Trends in Cognitive \nSciences, 27(7), 2023, 597–599. https://doi.org/10.1016/j.tics.2023.04.003 \nZaki, J. & Ochsner, K. ‘The Neuroscience of Empathy: Progress, Pitfalls and Promise’. Nature \nNeuroscience, 15(5), 2012, 675–680. https://doi.org/10.1038/nn.3085 \n \n"}]}