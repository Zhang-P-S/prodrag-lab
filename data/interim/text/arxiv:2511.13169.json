{"doc_id": "arxiv:2511.13169", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.13169.pdf", "meta": {"doc_id": "arxiv:2511.13169", "source": "arxiv", "arxiv_id": "2511.13169", "title": "TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine", "authors": ["Tianai Huang", "Jiayuan Chen", "Lu Lu", "Pengcheng Chen", "Tianbin Li", "Bing Han", "Wenchao Tang", "Jie Xu", "Ming Li"], "published": "2025-11-17T09:15:41Z", "updated": "2025-11-17T09:15:41Z", "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\\_r1 and gemini\\_2\\_5\\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the \"In-depth Challenge for Comprehensive TCM Abilities\" special track.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.13169v1", "url_pdf": "https://arxiv.org/pdf/2511.13169.pdf", "meta_path": "data/raw/arxiv/meta/2511.13169.json", "sha256": "86d4326c73715325b5930d130fab4a28a5053a5075351a47221355b60a31c74f", "status": "ok", "fetched_at": "2026-02-18T02:26:50.627959+00:00"}, "pages": [{"page": 1, "text": "TCM-5CEVAL: EXTENDED DEEP EVALUATION BENCHMARK\nFOR LLM’S COMPREHENSIVE CLINICAL RESEARCH\nCOMPETENCE IN TRADITIONAL CHINESE MEDICINE\nTianai Huang1,†, Jiayuan Chen2,†, Lu Lu2, Pengcheng Chen3, Tianbin Li2, Bing Han2, Wenchao Tang1*,\nJie Xu2*, Ming Li1*\n1School of Artificial Intelligence in Traditional Chinese Medicine, Shanghai University of Traditional Chinese\nMedicine, Shanghai, China\n2Shanghai Artificial Intelligence Laboratory, Shanghai, China\n3University of Washington, Seattle, Washington, US\nCorrespondence to: {Xu Jie}xujie@pjlab.org.cn, {Tang Wenchao}vincent.tang@shutcm.edu.cn,\n{Li Ming }acupunture@126.com\n†These authors contributed equally.\nABSTRACT\nLarge language models (LLMs) have demonstrated exceptional capabilities in general domains, yet\ntheir application in highly specialized and culturally-rich fields like Traditional Chinese Medicine\n(TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as\nTCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual\nalignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval\nis designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam),\n(2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese\nMateria Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT).\nWe conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance\ndisparities and identifying top-performing models like deepseek_r1 and gemini_2_5_pro. Our findings\nshow that while models exhibit proficiency in recalling foundational knowledge, they struggle with the\ninterpretative complexities of classical texts. Critically, permutation-based consistency testing reveals\nwidespread fragilities in model inference. All evaluated models, including the highest-scoring ones,\ndisplayed a substantial performance degradation when faced with varied question option ordering,\nindicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval\nnot only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes\nfundamental weaknesses in their reasoning stability. To promote further research and standardized\ncomparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in\nthe \"In-depth Challenge for Comprehensive TCM Abilities\" special track.\nKeywords Benchmark · Large language model · Traditional Chinese medicine\n1\nIntroduction\nIn recent years, Large Language Models (LLMs) have achieved remarkable progress in natural language processing,\nwith their applications expanding into specialized medical domains [1]. However, when it comes to Traditional Chinese\nMedicine (TCM) - a field characterized by its unique theoretical system and diagnostic features - existing evaluation\nmethods face considerable challenges [2]. Rooted in traditional Chinese culture, TCM emphasizes ‘holistic concepts’ [3]\nand ‘treatment based on syndrome differentiation.’ [4] Its knowledge system encompasses abstract notions such as\narXiv:2511.13169v1  [cs.CL]  17 Nov 2025\n"}, {"page": 2, "text": "yin-yang and five elements theory, zang-fu organ and meridian systems, along with complex pattern identification,\nformula composition, and medicinal property classification [5]. Current evaluation methods primarily rely on objective\nquestion types, which may not adequately assess LLMs’ deep reasoning capabilities and logical thinking processes [6,7].\nThis limitation in evaluation methodology makes it difficult to accurately measure LLMs’ true proficiency in TCM,\nparticularly in assessing capabilities that require complex diagnostic thinking, such as clinical decision-making and\nclassical literature interpretation. Furthermore, existing evaluation systems show room for improvement in covering\nnon-pharmacological therapies like acupuncture and tuina, as well as in-depth knowledge of Chinese materia medica.\nThese considerations highlight opportunities for enhancing the application of artificial intelligence in TCM preservation\nand innovation [8–10]. Therefore, developing a comprehensive framework that expands evaluation dimensions while\ninnovating assessment methods represents a meaningful direction for supporting TCM’s digital advancement.\nIn the field of TCM LLM evaluation, existing research has primarily focused on developing specialized benchmarks,\nwhich can be grouped by their distinct focuses and methodologies. One line of research has centered on standardized\nknowledge assessment, often leveraging examination content. For instance, TCMBench introduces the TCM-ED dataset,\nconsisting of 5,473 questions derived from the TCM Licensing Examination (TCMLE) to systematically cover core\ntheories and clinical practice [11]. Similarly, TCMD focuses on constructing a large-scale medical question-answering\ndataset with manually designed instructions specifically tailored for TCM examination tasks [12]. Other frameworks\nhave targeted deeper, more specific clinical reasoning skills or aimed for broader, multidimensional coverage. In the\nformer category, TCMEval-SDT provides a specialized benchmark of 300 real-world cases, sourced from diverse\nrecords and meticulously annotated by experts, to specifically assess the complex dialectical thinking processes in\nsyndrome differentiation [13]. In the latter category, MTCMB proposes a comprehensive framework that integrates\nreal-world case records, exam content, and classical literature across 12 sub-datasets, covering five major categories:\nknowledge quality, language understanding, diagnostic reasoning, prescription generation, and safety [14]. Our prior\nwork, TCM-3CEval, also follows this multidimensional approach, evaluating models across three dimensions—core\nknowledge, classical literacy, and clinical decision-making [15]. Finally, some work has pushed into novel modalities,\nmost notably TCM-Ladder, a multimodal QA dataset that incorporates images and videos alongside various question\nformats to evaluate visual understanding [16].\nAlthough these specialized evaluation frameworks have demonstrated certain capabilities in handling professional\ntasks, their assessments primarily emphasize objective knowledge evaluation, with relatively limited investigation into\nreasoning processes and cognitive abilities. Research indicates that current evaluation methods perform reasonably\nwell in assessing theoretical knowledge retention [11,12] but appear less suited for evaluating competencies requiring\nsubjective judgment [13]. Existing evaluation systems therefore present clear opportunities for development in two key\nareas: first, in methodological approaches, particularly in implementing effective subjective assessment mechanisms;\nand second, in dimensional coverage. The scope of evaluation is often limited to clinical practice, with key practical\ndomains under-represented and dimensions related to TCM scientific research almost entirely overlooked. These gaps\nlimit a truly comprehensive understanding of LLMs’ TCM capabilities.\nThis study builds upon the existing 3C evaluation framework to propose an expanded ‘5C’ comprehensive evaluation\nframework for TCM LLMs. The new framework maintains the three original core dimensions while incorporating two\nadditional dimensions: Clinical Non-pharmacological Therapy and Chinese Materia Medica, forming a more complete\narchitecture for assessing TCM knowledge systems. In terms of methodological development, we have explored\nintegrating both subjective and objective assessment approaches across all dimensions. For Core Knowledge, besides\nconventional single-choice and multiple-choice questions, open-ended questions are introduced to examine theoretical\nunderstanding depth. The Classical Literacy dimension employs text interpretation exercises to assess classical literature\ncomprehension, while Clinical Decision-Making utilizes case analysis to evaluate diagnostic reasoning. Clinical Non-\npharmacological Therapy incorporates treatment design tasks to examine therapeutic application skills, and Chinese\nMateria Medica includes formula analysis to assess medicinal compatibility logic. This combined assessment approach\nseeks to provide a more comprehensive evaluation of LLMs’ TCM capabilities across both knowledge breadth and\ncognitive depth, potentially addressing some limitations of previous frameworks in dimension coverage while offering\ncomplementary perspectives to solely objective-question-based evaluation. We hope these exploratory efforts might\ncontribute to establishing more nuanced evaluation standards for TCM LLM development, while possibly supporting\nthe advancement of TCM digitalization research in more substantive directions.\n2\n"}, {"page": 3, "text": "  \n  \n  \n  \nC\nl\na\ns\ns\ni\nc\na\nl\n \nL\nit\ne\nr\na\nc\ny\n \n  \n  \n  \n  \nC\nh\ni\nn\ne\ns\ne\n \nM\na\nt\ne\nr\ni\na\n \nC\no\nr\ne\n \nK\nn\no\nw\nl\ne\nd\ng\ne\np\nh\na\nr\nm\na\nc\no\nl\no\ng\ni\nc\na\nl \nC\nli\nn\ni\nc\na\nl \nN\no\nn\n-\nT\nh\ne\nr\na\np\ny\nM\ne\nd\ni\nc\na\n  \n  \n  \n  \nC\nli\nn\ni\nc\na\nl\nD\ne\nc\ni\ns\ni\no\nn\n-\nM\na\nk\ni\nn\ng\nJin Gui \nYao Lue\nHuang Di \nNei Jing\nShang Han\n Lun\nWarm \nDisease \nTheory\nVarious\nTCM\nSchools\nDiagnostics \nof TCM\nFoundation\nof TCM\nHistory\nof TCM\nTCM \nSurgery\nTCM Internal \nMedicine\nTCM \nGynecology\nTCM \nPediatrics\nTCM \nOphthalmology\nTCM Otorhino-\nlaryngology\nTCM Orthopedics \nand Traumatology\nChinese \nMateria Medica\nTCM \nFormulology\nTCM \nPharmaceutics\nTCM \nPharmacology\nTCM \nIdentification \nProcessing of \nChinese Medicines\nTCM \nChemistry\nMeridians and \nAcupuncture Points\nPediatric Tuina\nManipulation \nof Tuina\nAcupuncture \nand Moxibustion\nAcupuncture \nand Moxibustion \nTherapeutics\nTuina \nTherapeutics\n5C\nCore Knowledge: The essential \nconcepts and foundational theories that \nunderpin Traditional Chinese Medicine.\nClassical Literacy: Proficiency in \nunderstanding and interpreting \nclassical TCM texts and literature.\nClinical Decision-Making: The ability to \napply TCM knowledge in practical clinical \nsettings to make informed decisions.\nConcepts of 5C\nChinese Materia Medica: \nThe foundational knowledge of medicinal \nsubstances and their clinical application.\nClinical Non-pharmacological Therapy: \nThe principles and skilled application of \ntherapeutic physical techniques.\nFigure 1: Overview diagram of TCM 5C-EVAL\n2\nMethods\n2.1\nEvaluation dimension design\n2.1.1\nData source\nBased on TCM-3CEVAL, we propose TCM-5CEVAL, a Traditional Chinese Medicine evaluation benchmark that\nutilizes exercise sets from nationally authorized planning textbooks as data sources, and is deeply aligned with TCM\nteaching outline and clinical disciplines. The exercise sets of TCM planning textbooks in this study primarily consist\nof those from the National Higher Education TCM Planning Textbook series published by China Press of Traditional\nChinese Medicine, with the main versions being the \"13th Five-Year Plan Textbook\" and \"14th Five-Year Plan Textbook\"\nexercise sets. This textbook series was compiled under the guidance of the National Administration of Traditional\nChinese Medicine, with collaboration between China Press of Traditional Chinese Medicine and authoritative experts\nfrom various fields across national TCM institutions. Currently, the \"13th Five-Year Plan Textbook\" and \"14th Five-Year\nPlan Textbook\" are widely adopted as authoritative teaching materials in TCM institutions nationwide.\nThis study selected exercise sets from 30 TCM-related courses, including Chinese Internal Medicine, Chinese External\nMedicine, Gynecology of TCM, Pediatrics of TCM, Ophthalmology of TCM, Otorhinolaryngology of TCM, Orthope-\ndics and Traumatology of TCM, Basic Theory of TCM, Diagnostics of TCM, Chinese Medicinal Formulas, Theories\nof Schools of TCM, Treatise on Cold Damage Diseases, Theory and Practice of Chinese Medicines, Acupuncture\nand Moxibustion, Meridians and Acupuncture Points, and Tuina, as the data sources for TCM-5CEVAL.The 30\nTCM-related exercise sets were categorized into the following five modules based on clinical practice and theoretical\nframeworks: Clinical Decision-making, Core Knowledge, Classical Literacy, Chinese Materia Medica, and Clinical\nNon-pharmacological Therapy. The specific data sources and classifications are illustrated in Figure 1.\n2.1.2\nClassification criteria\nThe classification criteria of this study are as follows:In Ming Dynasty China, medicine was divided into thirteen\nspecialties: general internal medicine, pediatric medicine, gynecology, ulcer treatment, acupuncture and moxibustion,\nophthalmology, oral-dental medicine, laryngology, fracture and cold damage treatment, wound treatment, massage, and\nincantation therapy.\n3\n"}, {"page": 4, "text": "The Undergraduate Program Catalog of Regular Higher Education Institutions (2025) issued by China’s Ministry\nof Education categorizes TCM disciplines into three major groups:TCM disciplines (including Chinese Medicine,\nAcupuncture and Tuina, Pediatric TCM, Orthopedics and Traumatology of TCM, etc.), Integrated Chinese and Western\nMedicine, Chinese Materia Medica disciplines (including Chinese Pharmacy, Pharmaceutical Preparation of Chinese\nMedicine, Development of Chinese Medicinal Resources, etc.).\nThe Catalog of Disciplines and Specialties for Conferring Doctoral and Master’s Degrees (1997) issued by the Ministry\nof Education classifies TCM disciplines into Chinese Medicine, Integrated Chinese and Western Medicine, and Chinese\nMateria Medica. Among them, Chinese Medicine is subdivided into 13 specialties: Basic Theory of TCM, Clinical\nFoundations of TCM, Medical History and Literature of TCM, Chinese Medicinal Formulas, Diagnostics of TCM,\nInternal Medicine of TCM, Surgery of TCM, Orthopedics and Traumatology of TCM, Gynecology of TCM, Pediatrics\nof TCM, Ophthalmology & Otorhinolaryngology of TCM, Acupuncture and Tuina, and Ethnic Medicine.\nThe Catalog of Medical Institution Clinical Departments (2007) issued by the Ministry of Health divides TCM\nclinical specialties into 18 categories: Internal Medicine, Surgery, Obstetrics & Gynecology, Pediatrics, Dermatology,\nOphthalmology, Otorhinolaryngology, Stomatology, Oncology, Orthopedics and Traumatology, Proctology, Geriatrics,\nAcupuncture, Tuina, Rehabilitation Medicine, Emergency Medicine, Preventive Healthcare, and others.\nBased on the above classifications in TCM education and clinical practice, this study consolidates clinical specialties\ninto \"Clinical Decision-making\", retains Chinese Materia Medica as a first-level discipline, and categorizes other\nsub-disciplines into Classical Literature, Acupuncture and Tuina, and Basic Theory for conciseness, professionalism,\nand interpretability. Thus, TCM-5CEVAL’s dataset is classified into the following five categories.\n(1)Core Knowledge: Denotes the scientific knowledge system encompassing fundamental concepts, principles, and\ntheoretical frameworks of TCM.\n(2)Classical Literacy: Comprises the TCM discipline that studies representative classical medical literature as primary\nresearch objects.\n(3)Clinical Decision-making: Refers to the knowledge system for diagnosis, treatment, and prevention in clinical\ndisciplines including internal medicine, surgery, gynecology, pediatrics, etc. of Traditional Chinese Medicine.\n(4)Chinese Materia Medica: The discipline investigating the basic theories of Chinese medicines, including the origin,\nprocessing, properties, therapeutic effects, and clinical applications of medicinal materials, decoction pieces, and patent\ndrugs.\n(5)Clinical Non-pharmacological Therapy: The TCM discipline focusing on meridians, acupoints, and manual\ntechniques, exploring operational skills, therapeutic principles, mechanisms of action, and disease prevention/treatment\npatterns.\n2.2\nEvaluation dataset construction\nBuilding upon our previous TCM-3CEval framework, which established a triaxial benchmark for Core Knowledge,\nClassical Literacy, and Clinical Decision-Making, the TCM-5CEVAL dataset represents a significant methodological\nexpansion. This new iteration retains the three original core dimensions while incorporating two critical, previously\nunderserved domains: Chinese Materia Medica and Clinical Non-pharmacological Therapy, thus forming the expanded\n5C evaluation architecture. The dataset is also rigorously constructed from authoritative data sources, primarily the\n\"13th Five-Year Plan\" and \"14th Five-Year Plan\" national planning textbook exercise sets. All content was curated and\nvalidated by subject-matter experts from Shanghai University of Traditional Chinese Medicine and the China Academy\nof Chinese Medical Sciences to ensure accuracy and clinical relevance.\nThe TCM-5CEVAL dataset is composed of five sub-datasets, corresponding to each of the 5C dimensions. To facilitate\na multi-faceted evaluation that assesses both knowledge recall and complex reasoning, each sub-dataset contains a\ncollection of single-choice questions, multiple-choice questions, and open-ended questions. This design moves beyond\nsolely objective metrics to capture a model’s capacity for subjective judgment and in-depth analysis. Furthermore, the\nselection of questions was stratified to ensure a balanced distribution of difficulty, encompassing easy, medium, and\nhard items. The specific distribution of question difficulty across dimensions is detailed in Table 1. The five sub-datasets\nare defined as follows:\n1. TCM-Exam (Core Knowledge): This dataset evaluates the model’s foundational understanding of TCM. It assesses\nthe comprehension of core theoretical constructs (e.g., Yin-Yang, Five Elements, Zang-Fu, Qi-Blood-Body Fluids)\nand the application of fundamental diagnostic principles, including the four diagnostic methods and various syndrome\ndifferentiation systems.\n4\n"}, {"page": 5, "text": "2. TCM-LitQA (Classical Literacy): This dataset measures the model’s proficiency in interpreting seminal TCM\nliterature. It features questions derived from the four major classics (Huangdi Neijing, Shanghan Lun, Jingui Yaolue,\nWenbing Xue) and the theories of various TCM schools. The assessment focuses on the model’s ability to analyze\nclassical provisions and understand their enduring theoretical and clinical implications.\n3. TCM-MRCD (Clinical Decision-Making): This dataset assesses the model’s capacity for practical clinical\nreasoning. Using standardized clinical case records, it evaluates the entire diagnostic and therapeutic process: from\nanalyzing patient data and performing syndrome differentiation to formulating treatment principles and appropriate\nprescriptions.\n4. TCM-CMM (Chinese Materia Medica): This new dimension evaluates the model’s specialized knowledge of\nherbal medicine and formulary. It covers the properties, efficacy, and clinical application of individual herbs, as well as\nthe principles of formula composition, compatibility (including contraindications), processing (Paozhi), and quality\nassessment.\n5.\nTCM-ClinNPT (Clinical Non-pharmacological Therapy):\nThis second new dimension addresses non-\npharmacological interventions, focusing on acupuncture, moxibustion, and Tuina. The dataset tests the model’s\nability to perform syndrome differentiation for these therapies, select appropriate acupoints, design Tuina manipulation\nprotocols, and apply these skills in common clinical scenarios.\nTable 1: TCM-5CEval Benchmark Dimensions and Question Distribution\nDimension\nQuestion Type\nN\nDifficulty (E/M/H)\nMain Evaluation Content\nTCM-Exam\nSingle-choice\n122\n38 / 52 / 32\nAssess recall of core concepts, theories, and diagnostic\nfacts.\nMultiple-choice\n78\n29 / 29 / 20\nEvaluate understanding of complex relationships between\nfoundational theories.\nOpen-ended\n70\n22 / 32 / 16\nExamine in-depth explanation of theoretical principles\nand diagnostic logic.\nTCM-LitQA\nSingle-choice\n275\n97 / 96 / 82\nTest comprehension of key provisions and ideas from\nclassical texts.\nMultiple-choice\n188\n61 / 71 / 56\nAssess ability to compare and contrast concepts across\ndifferent classics.\nOpen-ended\n165\n52 / 64 / 49\nEvaluate deep interpretation of classical texts and their\nclinical significance.\nTCM-MRCD\nSingle-choice\n230\n80 / 98 / 52\nEvaluate diagnostic accuracy in straightforward clinical\nvignettes.\nMultiple-choice\n156\n59 / 53 / 44\nTest syndrome differentiation and treatment selection in\ncomplex cases.\nOpen-ended\n148\n45 / 59 / 44\nAssess the complete clinical reasoning process (diagnosis,\nprinciple, formula).\nTCM-CMM\nSingle-choice\n273\n110 / 100 / 63\nAssess recall of herb properties, formula compositions,\nand processing facts.\nMultiple-choice\n121\n43 / 42 / 36\nEvaluate understanding of herb compatibility, contraindi-\ncations, and formula logic.\nOpen-ended\n91\n33 / 31 / 27\nExamine the ability to analyze formulas and com-\npare/contrast materia medica.\nTCM-ClinNPT\nSingle-choice\n166\n55 / 64 / 47\nTest knowledge of acupoint locations, functions, and Tu-\nina techniques.\nMultiple-choice\n96\n38 / 30 / 28\nAssess acupoint/technique selection for specific clinical\nconditions.\nOpen-ended\n81\n25 / 32 / 24\nEvaluate the design of complete acupuncture or Tuina\ntreatment plans.\n5\n"}, {"page": 6, "text": "(赵某，女，45岁，农民。主诉:腰骶部酸痛1年余。1年前无明显原因出现\n两侧腰骶部酸痛。受寒、劳累后加重，热敷、休息后减轻，未经治疗。现症\n见双侧腰骶部酸痛，下肢活动正常。查体骶棘肌轻度压痛，肌张力减低。直\n腿抬高试验阴性。腰部CT、X线检查均正常。要求写出:诊断、治则、推拿操\n作方法。)\nZhao, female, 45 years old, farmer. Chief complaint: Pain in the lumbar \nand sacral regions for over a year. One year ago, there was no apparent \ncause of pain in both sides of the lumbar and sacral regions. After being \nexposed to cold and fatigue, it worsens, but after applying hot compress \nand resting, it subsides without treatment. The current symptoms include \nbilateral lumbosacral soreness and normal lower limb movement. Mild \ntenderness and decreased muscle tone in the sacral spiny muscles during \nphysical examination. The straight leg lift test is negative. The CT and \nX-ray examinations of the waist are both normal. Request to write: \ndiagnosis, treatment principles, and massage operation methods.\nOpen-ended\nDataset (Examples)\n(见肝之病，知肝传脾，当先实脾”体现了\nA.整体观念  B.肝实证的防治原则  C.肝虚证的防治原则\nD.既病防传的思想  E.未病先防的思想)\nSeeing the disease of the liver, knowing that the liver transmits to the \nspleen, we should first strengthen the spleen \"reflects\nA. Overall concept\nB. Principles for prevention and treatment of liver syndrome\nC. Principles of prevention and treatment of liver deficiency syndrome\nD. The idea of disease prevention and transmission\nE. The idea of prevention before illness\nMultiple-choice\n(麻黄汤主治证的病机是\nA.外感风寒，营卫不和  B.外感风寒，肺气失宣  \nC.风邪犯肺，肺失清肃  D.外感风寒湿邪，内有蕴热\nE.外感风寒，水饮内停)\nWhat is the pathogenesis of Ma Huang Tang's main treatment syndrome\nA. External wind and cold, disharmony between the camp and the guards\nB. External wind cold, loss of lung qi circulation\nC. Wind evil invades the lungs, causing them to lose their clarity and clarity\nD. External wind cold dampness evil, internal heat retention\nE. External wind and cold, stop drinking water inside\nSingle-choice\nLarge Language Models\nTask\nDescription\nChain-of-\nThought\nQ&A\nConstraint\nExample\nMultiple choice\nShuffling options\nEvaluation Process\nResult Aualysis\nAccuracy: 1\nPerformance Metric\n[Correct Option] B\n[Answer] B\nLarge Language Models\nAccuracy: 0\nPerformance Metric\n[Correct Option] A, B, D, E\n[Answer] A, B, C\n                        Accuracy\nQuickly measures the model's mastery \nof fundamental knowledge\nFor Single-choice & Multiple-choice\n                      BertScore\nSemantic similarity calculation ensures \nsemantic accuracy and relevance\nFor Open-ended\n                   + \n                     Macro Recall\n= (Recall1 + Recall2 + ... + Recalln) / n\n    To maintain the efficiency of objective \nquestion assessment while ensuring the \ndepth and comprehensiveness of subjective \nquestion evaluation.\nFigure 2: TCM-LLM Multi-Metric Assessment Workflow\n2.3\nWorkflow and Evaluation Methods of TCM-5CEVAL\nThe proposed TCM-LLM Multi-Metric Assessment Workflow(Figure 2) systematically evaluates model performance\nthrough a dual-path framework that employs accuracy metrics for objective questions (single-choice and multiple-choice\nquestions) and combines BertScore with macro recall for open-ended responses.\nTo further assess the stability of the answers and the robustness against option-order bias, we specifically applied a\npermutation-based consistency test for the single-choice questions. Each original single-choice question contains five\nmutually exclusive options (A–E). For each question we generate five permutations of the option ordering by cyclically\nrotating the original option sequence so that each original option appears in each position exactly once. Each model is\npresented with all five permutations and produces one prediction per permutation. A question is considered passed by\na given model only if the model outputs the same correct option for all five permutations (i.e., the model’s answers\nacross permutations are identical and equal to the annotated correct choice). This strict consistency criterion evaluates\nwhether model predictions are invariant to option ordering and reduces the influence of positional biases. To ensure\nthe evaluation results reflect only the model’s robustness against option ordering rather than the randomness of the\ngeneration process, all model inferences were conducted with the temperature set to 0, guaranteeing deterministic\noutputs.\n3\nResults\nThe performance of fifteen large language models was evaluated across the five dimensions of the TCM-5CEVAL\nbenchmark. The results, presented in Table 2, indicate a wide range of capabilities among the models in comprehending\nand applying Traditional Chinese Medicine knowledge.\n3.1\nOverall Model Performance Hierarchy\nA clear performance hierarchy was observed among the evaluated models. Kimi_K2_Instruct_0905 demonstrated strong\noverall performance, achieving the highest scores in three of the five sub-datasets: TCM-Exam (0.847), TCM-MRCD\n(0.746), and TCM-CMM (0.749). Concurrently, deepseek_r1 also consistently achieved high scores, securing the\ntop rank in TCM-LitQA (0.731) and ranking second in TCM-Exam (0.798) and TCM-CMM (0.746). Other models,\nincluding gemini_2_5_pro and grok_4_0709, also constituted a high-performing group, with scores exceeding 0.700 in\nmultiple dimensions.\n6\n"}, {"page": 7, "text": "3.2\nPerformance Variation Across 5C Dimensions\nA consistent pattern observed across most models was a performance differential among the five evaluation dimensions.\nThe highest scores were typically registered on the TCM-Exam dataset, which assesses core knowledge. For instance,\nKimi_K2_Instruct_0905 achieved its highest score of 0.847 on TCM-Exam. In contrast, the TCM-LitQA dimension,\nfocusing on classical literacy, appeared more challenging for many models. Several models that performed well on core\nknowledge exhibited a drop in performance on this sub-dataset; for example, gpt_5 scored 0.750 on TCM-Exam but\n0.519 on TCM-LitQA. A similar trend of lower relative scores was also apparent in the TCM-ClinNPT dimension for\nsome models.\nTable 2: Comparison of performance of different models on the TCM-5CEVAL benchmark\nModel\nTCM-Exam\nTCM-LitQA\nTCM-MRCD\nTCM-CMM\nTCM-ClinNPT\ndeepseek_r1\n0.798\n0.731\n0.733\n0.746\n0.640\ndeepseek_v3\n0.770\n0.611\n0.706\n0.659\n0.565\nqwen3_235b\n0.764\n0.626\n0.691\n0.704\n0.566\nqwen3_32b\n0.724\n0.549\n0.657\n0.607\n0.508\nqwen2_5_72b\n0.754\n0.550\n0.660\n0.683\n0.525\nKimi_K2_Instruct_0905\n0.847\n0.696\n0.746\n0.749\n0.595\ngemini_2_5_flash\n0.675\n0.492\n0.635\n0.536\n0.493\ngemini_2_5_pro\n0.779\n0.62\n0.724\n0.726\n0.612\ngpt_4o\n0.665\n0.469\n0.609\n0.571\n0.479\ngpt_5\n0.750\n0.519\n0.641\n0.666\n0.606\ngpt_5_mini\n0.629\n0.425\n0.623\n0.547\n0.513\nmistral_small_3_1_24b_instruct\n0.454\n0.347\n0.441\n0.39\n0.368\nllama_4_maverick\n0.721\n0.513\n0.638\n0.544\n0.443\ngrok_4_0709\n0.730\n0.593\n0.684\n0.680\n0.642\nclaude_sonnet_4_5_20250929\n0.698\n0.593\n0.672\n0.717\n0.560\nmarks the best score in a column and\nthe second best.\n3.3\nIntra-Family Model Comparisons\nThe results also highlight performance variations within specific model families, often correlating with model size or\nversion. For instance, deepseek_r1 consistently outperformed deepseek_v3 across all five categories. A similar trend\nwas observed for the Gemini series, where gemini_2_5_pro achieved significantly higher scores than gemini_2_5_flash.\nWithin the Qwen family, the largest model, qwen3_235b, generally surpassed its smaller counterparts, qwen3_32b and\nqwen2_5_72b. Likewise, gpt_5 recorded higher scores than both gpt_4o and gpt_5_mini, with gpt_5_mini showing the\nlowest performance within its family.\n3.4\nCompetency in Materia Medica and Non-Pharmacological Therapies\nThe two novel dimensions, TCM-CMM and TCM-ClinNPT (Clinical Non-pharmacological Therapy), introduced\nto assess specialized and practical knowledge, presented varied results.In the TCM-CMM dataset, which assesses\nknowledge of herbal medicine and formulary, Kimi_K2_Instruct_0905 obtained the highest score (0.749), followed\nclosely by deepseek_r1 (0.746). In the domain of non-pharmacological therapies (TCM-ClinNPT), grok_4_0709\nregistered the top score at 0.642, demonstrating a particular capability in knowledge related to acupuncture, moxibustion,\nand Tuina. The second-highest score in this category was achieved by deepseek_r1 (0.640).\n3.5\nRobustness to Option-Order Permutation\nTo further probe the stability of model predictions, a permutation-based consistency test was applied to the single-choice\nquestions, where models were required to correctly identify the answer across five different orderings of the options. The\nresults, detailed in Figure 3, indicate a universal performance degradation for all models across all five sub-datasets when\n7\n"}, {"page": 8, "text": "this strict consistency criterion was enforced, compared to their baseline accuracy on single-question instances. The\nmagnitude of this performance drop varied among models and datasets. For instance, on the TCM-Exam sub-dataset,\ngemini_2_5_pro demonstrated relatively high consistency, with its accuracy decreasing moderately from a baseline of\n0.920 to a consistency-tested score of 0.844. However, the performance degradation was substantially more pronounced\non other sub-datasets, indicating greater sensitivity to option ordering. On the TCM-LitQA sub-dataset, the accuracy of\ngpt_4o fell from 0.492 to 0.298, and mistral_small_3_1_24b_instruct saw its score drop from 0.331 to 0.084. The effect\nwas even more severe on the TCM-ClinNPT sub-dataset, where deepseek_r1’s score declined from 0.787 to 0.470. This\ntrend of reduced accuracy under the permutation test was consistently observed, though the degree of reduction differed\nacross the evaluated models and the specific knowledge dimension being tested.\n4\nDiscussion\nOur prior TCM-3CEval framework provided an important foundational step, establishing a triaxial benchmark for Core\nKnowledge, Classical Literacy, and Clinical Decision-Making. It serves as a useful tool for assessing a model’s grasp of\nfoundational theory and its diagnostic reasoning processes. A limitation of the 3C framework, however, was that its\n\"Clinical Decision-Making\" dimension consolidated all therapeutic recommendations. This design did not provide a\ngranular assessment that discretely evaluated the two primary interventional arms of TCM: pharmacological therapy\n(herbal medicine) [17] and non-pharmacological therapy (acupuncture, Tuina) [18].\nThe TCM-5CEval framework builds upon this foundation to offer enhanced specificity and breadth. The first key\nchange was the formal decoupling of clinical therapeutics. We expanded the 3C framework’s clinical assessment by\nintroducing two new, dedicated dimensions: Chinese Materia Medica (TCM-CMM) and Clinical Non-pharmacological\nTherapy (TCM-ClinNPT). This architectural change enables a more detailed and specialized evaluation of a model’s\ndistinct competencies in each of these therapeutic modalities. The second main enhancement lies within the new\nTCM-CMM dimension, which represents an expansion in scope. The 3C benchmark’s assessment of herbal medicine\nwas largely confined to formula composition and modification within a clinical context. The 5C framework’s TCM-\nCMM dimension is more comprehensive. It assesses a model’s knowledge not only in clinical formula application\nbut also in the foundational pharmaceutical science of TCM. This includes dedicated evaluations of pharmacological\nanalysis [19], traditional herb processing (Paozhi) [20], pharmaceutics [21], and herb identification/quality control [22].\nThis expansion marks an important shift. The benchmark’s utility is no longer limited to assessing clinical acumen; it\nnow systematically evaluates a model’s capacity in areas relevant to TCM-related scientific research [23], pharmaceutical\ndevelopment [24], and quality assurance [25]. This makes TCM-5CEval a more complete instrument for measuring a\nmodel’s proficiency across a wider spectrum of the modern TCM domain.\nOur evaluation of fifteen large language models on the TCM-5CEVAL benchmark revealed significant performance\ndisparities, as detailed in Table 2. A clear performance hierarchy was observed. Models such as Kimi_K2_Instruct_0905,\ndeepseek_r1, and gemini_2_5_pro formed a high-performing group, achieving top scores across multiple dimensions.\nFor instance, Kimi_K2_Instruct_0905 scored highest overall in three of the five primary dimensions: TCM-Exam,\nTCM-MRCD, and TCM-CMM. deepseek_r1 led in TCM-LitQA, while grok_4_0709 secured the top score in TCM-\nClinNPT. A consistent pattern emerged across most models: performance varied significantly across the 5C dimensions;\nscores were typically highest on TCM-Exam, which assesses core knowledge, but dropped notably on TCM-LitQA,\nwhich requires interpretative analysis of classical texts. This suggests a general proficiency in recalling foundational\nconcepts but a widespread difficulty with deeper interpretative and reasoning tasks. Furthermore, the permutation-based\nconsistency test (Figure 3) demonstrated that this knowledge mastery is not robust; all models exhibited a universal\nperformance degradation when faced with varied option ordering, indicating a sensitivity to positional biases and a lack\nof deep, stable understanding.\nTo further dissect these high-level trends, we conducted a two-part investigation. First, we performed a granular\ncomparison of the top-performing models at the sub-dimensional level to identify areas of specialization. Second,\nto identify common weaknesses, we conducted an error hotspot analysis by aggregating questions with universally\npoor performance. These were defined as objective items (single and multiple-choice) answered incorrectly by five\nor more models, and subjective items (open-ended) where the average model score ranked in the lowest quartile\n(bottom 25%) . Our analysis of specialized strengths reveals that top-performing models exhibit distinct and specialized\ncompetencies, suggesting their capabilities in the vertical domain of TCM are not uniform. For example, deepseek_r1\ndemonstrated a profound specialization in classical literacy and non-pharmacological therapies; it dominated the\nsingle-choice questions for TCM-LitQA (winning 4 of 7 sub-categories, including Shanghan Lun and Neijing) and\nswept all 4 sub-categories of the TCM-ClinNPT multiple-choice questions (e.g., Tuina techniques and Acupuncture).\nIn contrast, Kimi_K2_Instruct_0905 showed broad strength in foundational knowledge and modern pharmaceutical\napplications, securing top performance in TCM-Exam multiple-choice questions and multiple TCM-CMM single-choice\nsub-categories (Pharmaceutics, Pharmacology). gemini_2_5_pro distinguished itself in highly technical areas, such as\n8\n"}, {"page": 9, "text": "Figure 3: Model Performance on the Single-Choice Permutation Consistency Test. The Figure compares the standard\naccuracy on the original questions (’Single Question’) with the consistency-based accuracy (’ID Group’) for each model\nacross the five sub-datasets. The ’ID Group’ score is awarded only when a model correctly answers a question across\nall five cyclical permutations of its options, thus measuring its robustness against option-order bias.\n9\n"}, {"page": 10, "text": "Claude\nDeepSeek\nGemini\nChatGPT\nGrok\nKimi\nQwen\nLLaMa\nMISTRAL AI\nFigure 4: Performance Distribution of Leading Models by Sub-Dimension. ★: The model that performs best in this\nsub-dimension\n.\nChinese Materia Medica Chemistry and Processing (Paozhi). These specializations likely reflect significant differences\nin the composition of the models’ respective training corpora. A model’s dominance in classical texts (deepseek_r1)\nimplies a corpus heavily enriched with digitized classical literature, whereas strength in materia medica chemistry\n(gemini_2_5_pro) or pharmacology (Kimi_K2_Instruct_0905) suggests a focus on modern textbooks, scientific papers,\nand pharmaceutical data. This divergence indicates that achieving expert-level performance across the entire breadth of\nTCM may require highly specialized, curated datasets rather than general-purpose training. The specific distribution of\ntop-performing models across individual sub-dimensions is presented in the Figure4.\nThis divergence in model strengths, which points to training data biases, is counterbalanced by a consistent and universal\nset of weaknesses revealed by our error hotspot analysis. These failures manifested differently across question types but\nconsistently pointed to the same core inferential deficits. On objective questions, while models showed some recall,\ntheir performance was not robust. A specific analysis of the aggregated single-choice errors shows the largest cluster\nwas ‘clinical four-diagnosis pattern differentiation’ (69 instances), followed by difficulties in classical text interpretation,\n10\n"}, {"page": 11, "text": "such as ‘classical case studies’ (48 instances).As shown in the Figure5(A), these are the error-prone knowledge points\nfor single-choice questions.\nThis pattern was mirrored in the aggregated multiple-choice question errors. Here, models struggled with comparative\nanalysis—such as differentiating the similarities and differences in drug efficacies—and again, showed weakness in\nunderstanding classical case studies and original texts from Shanghan Lun and Jingui Yaolue.The Figure5(B) illustrates\nthe most commonly mistaken knowledge points in multiple-choice questions.\nConversely, on subjective (open-ended) questions, the analysis of low-scoring items shows the most frequent challenge,\nby a large margin, was ‘TCM clinical pattern differentiation analysis’ (30 instances). This was followed by difficulties\nin explaining classical literature terminology.The distribution of commonly misinterpreted concepts in constructed-\nresponse questions is visualized in the Figure5(C).\nThis convergence of errors across all three question types points to several key weaknesses: (1) a noted inability to\nperform textual exegesis on classical literature; (2) a significant deficit in practical Zheng (syndrome) differentiation,\nwhich was a top error in both objective and subjective formats; and (3) difficulty with nuanced, expert-level knowledge\nlike materia medica comparisons. These deficiencies likely stem from two primary causes: data sparsity and linguistic\nbarriers related to the archaic, high-context language of TCM classics; and a more fundamental inferential deficit. This\nexplains why models fail at both robust objective reasoning and generative subjective synthesis, as they have not yet\nreplicated the holistic, dialectical reasoning (‘treatment based on syndrome differentiation’) that is central to TCM\npractice.\nWhile this 5C framework offers a more comprehensive evaluation, we identify several directions for future work. The\ncurrent datasets are primarily derived from authoritative textbooks and literature cases. A critical next step involves\nvalidating model performance against large-scale, real-world clinical data, such as electronic medical records, to ensure\npractical applicability. Furthermore, the challenge of polysemous terminology [26–28] in TCM remains ; future work\nmay explore concept disambiguation mechanisms [29], perhaps by integrating expert annotations [30] with structured\nmedical knowledge graphs [31,32]. Finally, this benchmark is text-based. A long-term goal is to expand the framework\nto include multi-modal evaluation, which is essential for assessing a model’s capacity to interpret non-textual diagnostic\ninformation central to TCM, such as tongue and pulse diagnosis.\n5\nConclution\nThis study introduces TCM-5CEVAL, a comprehensive benchmark for evaluating large language models across five\ncore dimensions of Traditional Chinese Medicine. Our evaluation of fifteen models revealed significant performance\ndisparities, with models like deepseek_r1, Kimi_K2_Instruct_0905, and gemini_2_5_pro demonstrating strong capabili-\nties. A key finding is the uneven performance across domains, where models generally excelled at recalling foundational\nconcepts (TCM-Exam) but were less proficient in the interpretative analysis of classical literature (TCM-LitQA).\nCritically, a permutation-based consistency test revealed a universal lack of robustness; all models, including top\nperformers, exhibited a notable performance degradation when challenged with varied option ordering, indicating a\nsensitivity to positional biases. While leading LLMs show promise in the TCM domain, these findings underscore that\ntheir knowledge is inconsistent and their reasoning remains fragile. TCM-5CEVAL thus serves as a crucial tool for\ndiagnosing these weaknesses and guiding future efforts toward developing more knowledgeable and fundamentally\nreliable models.\nAcknowledgments\nThis work was supported by the 2022 National Natural Science Foundation of China [grant 82174506], the 2024\nTraditional Chinese Medicine Research Project of Shanghai Municipal Health Commission [grant 2024PT001] and 2025\nTraditional Chinese Medicine Standardization Project of Shanghai Administration of Traditional Chinese Medicine\n[grant 2025BZ002].\nData Availability Statement\nThe\ndatasets\nused\nin\nthis\nstudy\nare\navailable\nthrough\nthe\nMedBench\nopen\nplatform\nat\nhttps://medbench.opencompass.org.cn/home. Access to the data can be obtained by contacting the MedBench team or\nthe corresponding author.\n11\n"}, {"page": 12, "text": "A\nB\nC\nFigure 5: High-Frequency Errors in Question Sets (A. Single-Choice; B. Multiple-Choice; C. Open-Ended)\n12\n"}, {"page": 13, "text": "Declarations\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that\ncould be construed as a potential conflict of interest.\nReferences\n[1] Sina Shool, Sara Adimi, Reza Saboori Amleshi, Ehsan Bitaraf, Reza Golpira, and Mahmood Tara. A systematic\nreview of large language model (llm) evaluations in clinical medicine. BMC Medical Informatics and Decision\nMaking, 25(1):117, 2025.\n[2] Zhe Chen, Dong Zhang, Chunxiang Liu, Hui Wang, Xinyao Jin, Fengwen Yang, and Junhua Zhang. Traditional\nchinese medicine diagnostic prediction model for holistic syndrome differentiation based on deep learning.\nIntegrative medicine research, 13(1):101019, 2024.\n[3] Roshan Kumar Dubey and Bablu Kumar. A comprehensive overview of traditional chinese medicine (tcm).\nZhongguo ying yong sheng li xue za zhi= Zhongguo yingyong shenglixue zazhi= Chinese journal of applied\nphysiology, 41:e20250015, 2025.\n[4] Gedi ZHANG, WEN Xiaoli, TAO Tianming, YAN Ziyou, and LIU Hongning. Exploration of constructing\na relatively comprehensive syndrome differentiation and treatment system based on dialectical materialism\nprinciples. Journal of Traditional Chinese Medicine, 45(5):1164, 2025.\n[5] David Leong. The dao of quantum mechanics: A comparative study of chinese yin-yang theory, taijitu, wujitu and\nquantum principles. University of Canberra, 2024.\n[6] Fengxiang Cheng, Haoxuan Li, Fenrong Liu, Robert van Rooij, Kun Zhang, and Zhouchen Lin. Empowering llms\nwith logical reasoning: A comprehensive survey. arXiv preprint arXiv:2502.15652, 2025.\n[7] Seungpil Lee, Woochang Sim, Donghyeon Shin, Wongyu Seo, Jiwon Park, Seokki Lee, Sanha Hwang, Sejin\nKim, and Sundong Kim. Reasoning abilities of large language models: In-depth analysis on the abstraction and\nreasoning corpus. ACM Transactions on Intelligent Systems and Technology, 2024.\n[8] Linken Lu, Tangsheng Lu, Chunyu Tian, and Xiujun Zhang. Ai: Bridging ancient wisdom and modern innovation\nin traditional chinese medicine. JMIR Medical Informatics, 12(1):e58491, 2024.\n[9] Wenyu Li, Xiaolei Ge, Shuai Liu, Lili Xu, Xu Zhai, and Linyong Yu. Opportunities and challenges of traditional\nchinese medicine doctors in the era of artificial intelligence. Frontiers in Medicine, 10:1336175, 2024.\n[10] E Zhou, Qin Shen, and Yang Hou. Integrating artificial intelligence into the modernization of traditional chinese\nmedicine industry: a review. Frontiers in Pharmacology, 15:1181183, 2024.\n[11] Wenjing Yue, Xiaoling Wang, Wei Zhu, Ming Guan, Huanran Zheng, Pengfei Wang, Changzhi Sun, and Xin Ma.\nTcmbench: A comprehensive benchmark for evaluating large language models in traditional chinese medicine.\narXiv preprint arXiv:2406.01126, 2024.\n[12] Ping Yu, Kaitao Song, Fengchen He, Ming Chen, and Jianfeng Lu. Tcmd: A traditional chinese medicine qa\ndataset for evaluating large language models. arXiv preprint arXiv:2406.04941, 2024.\n[13] Zhe Wang, Meng Hao, Suyuan Peng, Yuyan Huang, Yiwei Lu, Keyu Yao, Xiaolin Yang, and Yan Zhu. Tcmeval-\nsdt: a benchmark dataset for syndrome differentiation thought of traditional chinese medicine. Scientific Data,\n12(1):437, 2025.\n[14] Shufeng Kong, Xingru Yang, Yuanyuan Wei, Zijie Wang, Hao Tang, Jiuqi Qin, Shuting Lan, Yingheng Wang,\nJunwen Bai, Zhuangbin Chen, et al. Mtcmb: A multi-task benchmark framework for evaluating llms on knowledge,\nreasoning, and safety in traditional chinese medicine. arXiv preprint arXiv:2506.01252, 2025.\n[15] Tianai Huang, Lu Lu, Jiayuan Chen, Lihao Liu, Junjun He, Yuping Zhao, Wenchao Tang, and Jie Xu. Tcm-3ceval:\nA triaxial benchmark for assessing responses from large language models in traditional chinese medicine. arXiv\npreprint arXiv:2503.07041, 2025.\n[16] Jiacheng Xie, Yang Yu, Ziyang Zhang, Shuai Zeng, Jiaxuan He, Ayush Vasireddy, Xiaoting Tang, Congyu Guo,\nLening Zhao, Congcong Jing, et al. Tcm-ladder: A benchmark for multimodal question answering on traditional\nchinese medicine. arXiv preprint arXiv:2505.24063, 2025.\n[17] Myunggyo Lee, Hyejin Shin, Musun Park, Aeyung Kim, Seongwon Cha, and Haeseung Lee. Systems pharmacol-\nogy approaches in herbal medicine research: a brief review. BMB reports, 55(9):417, 2022.\n13\n"}, {"page": 14, "text": "[18] Hanzhi Tan, Nana Wang, Han Li, Chung Wah Cheng, Yalin Jiao, Dongni Shi, Juan Wang, Jiashuai Deng, Ji Li, Fei\nHan, et al. Protocol for the reporting assessment of clinical trials with non-pharmacological therapies in chinese\nmedicine. European Journal of Integrative Medicine, 69:102381, 2024.\n[19] Zi Yi Wang, Xin Wang, Dai Yan Zhang, Yuan Jia Hu, and Shao Li. Traditional chinese medicine network\npharmacology: development in new era under guidance of network pharmacology evaluation method guidance.\nZhongguo Zhong yao za zhi= Zhongguo zhongyao zazhi= China journal of Chinese materia medica, 47(1):7–17,\n2022.\n[20] Hua Luo, Hongguo Chen, Chang Liu, Siyuan Zhang, Chi Teng Vong, Dechao Tan, Yuntao Dai, Yitao Wang, and\nShilin Chen. The key issues and development strategy of chinese classical formulas pharmaceutical preparations.\nChinese Medicine, 16(1):70, 2021.\n[21] Haiyu Xu, Yanqiong Zhang, Ping Wang, Junhong Zhang, Hong Chen, Luoqi Zhang, Xia Du, Chunhui Zhao, Dan\nWu, Feng Liu, et al. A comprehensive review of integrative pharmacology-based investigation: A paradigm shift\nin traditional chinese medicine. Acta Pharmaceutica Sinica B, 11(6):1379–1399, 2021.\n[22] Amruta Balekundri and Vinodhkumar Mannur. Quality control of the traditional herbs and herbal products: a\nreview. Future Journal of Pharmaceutical Sciences, 6(1):67, 2020.\n[23] Xiaoli Chu, Bingzhen Sun, Qingchun Huang, Shouping Peng, Yingyan Zhou, and Yan Zhang. Quantitative\nknowledge presentation models of traditional chinese medicine (tcm): A review. Artificial intelligence in medicine,\n103:101810, 2020.\n[24] Xiaobing Li, Xiaodong Li, Li Wang, Yuanfang Hou, Yongsheng Liu, Jingxin Mao, Li Zhang, and Xuemei Li.\nAdvancing traditional chinese medicine research through network pharmacology: strategies for target identification,\nmechanism elucidation and innovative therapeutic applications. The American Journal of Chinese Medicine,\n53(07):2021–2042, 2025.\n[25] Hongbing Zhang, Yu Zhang, Tiejun Zhang, and Changxiao Liu. Research progress on quality markers of traditional\nchinese medicine. Journal of pharmaceutical and biomedical analysis, 211:114588, 2022.\n[26] Nigora Satibaldieva. Polysemy of terms in computational linguistics. International Journal of Scientific Trends,\n3(1):82–84, 2024.\n[27] Yuehui Hou. Pruning translation of logical and accidental polysemy in traditional chinese medicine terminology.\nTerminology, 2025.\n[28] Meng Song, Fei Ni, Jiamei Li, Xinnuo Li, and Keda Li. Status quo, problem, and prospect for traditional chinese\nmedicine international terminology standards. Guidelines and Standards in Chinese Medicine, 3(1):1–7, 2025.\n[29] Yinglin Wang, Ming Wang, and Hamido Fujita. Word sense disambiguation: A comprehensive knowledge\nexploitation framework. Knowledge-Based Systems, 190:105030, 2020.\n[30] Ariel Levy, Monica Agrawal, Arvind Satyanarayan, and David Sontag. Assessing the impact of automated\nsuggestions on decision making: Domain experts mediate model errors but take less initiative. In Proceedings of\nthe 2021 CHI Conference on Human Factors in Computing Systems, pages 1–13, 2021.\n[31] Linfeng Li, Peng Wang, Jun Yan, Yao Wang, Simin Li, Jinpeng Jiang, Zhe Sun, Buzhou Tang, Tsung-Hui Chang,\nShenghui Wang, et al. Real-world data medical knowledge graph: construction and applications. Artificial\nintelligence in medicine, 103:101817, 2020.\n[32] Yanjun Gao, Ruizhe Li, Emma Croxford, John Caskey, Brian W Patterson, Matthew Churpek, Timothy Miller,\nDmitriy Dligach, and Majid Afshar. Leveraging medical knowledge graphs into large language models for\ndiagnosis prediction: Design and application study. Jmir Ai, 4:e58670, 2025.\n14\n"}, {"page": 15, "text": "A\nMORE DETAILS OF TCM-5CEVAL\nA.1\nPROMPTS FOR TCM-5CEVAL\nThe prompts used in the TCM-5CEval benchmark are standardized instructions, meticulously designed to guide Large\nLanguage Models (LLMs) toward producing outputs that are uniform and optimized for automated evaluation across\ndifferent question modalities. Each prompt’s design comprises two primary components:\nRole-Playing Instruction: A consistent directive, \"You are a TCM domain expert,\" is employed across all prompts.\nThis primes the model to activate its specialized knowledge base in Traditional Chinese Medicine, thereby enhancing\nthe accuracy and professional quality of its responses.\nStrict Formatting Constraints: The prompts enforce a set of rigid output formats tailored to the respective question\ntype. This is the cornerstone of the benchmark’s automated scoring capability:\nFor single-choice questions, the model must return a single letter enclosed in angle brackets (e.g., <B>).\nFor multiple-choice questions, the model must provide all correct letters, comma-separated, within angle brackets (e.g.,\n<ACD>).\nFor open-ended questions, the model is instructed to generate a direct, concise textual answer, stripped of any\nconversational filler or extraneous explanations.\nThis disciplined approach ensures that all outputs are machine-readable and structurally consistent, enabling efficient,\nreliable, and scalable evaluation across the entire benchmark.\nFigure 6: Prompt for single-choice questions on the TCM-5CEval benchmark\n15\n"}, {"page": 16, "text": "Figure 7: Prompt for multiple-choice questions on the TCM-5CEval benchmark\n16\n"}, {"page": 17, "text": "Figure 8: Prompt for open-ended responses questions on the TCM-5CEval benchmark\n17\n"}]}