{"doc_id": "arxiv:2602.07361", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.07361.pdf", "meta": {"doc_id": "arxiv:2602.07361", "source": "arxiv", "arxiv_id": "2602.07361", "title": "ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations", "authors": ["Long S. T. Nguyen", "Quan M. Bui", "Tin T. Ngo", "Quynh T. N. Vo", "Dung N. H. Le", "Tho T. Quan"], "published": "2026-02-07T04:59:31Z", "updated": "2026-02-07T04:59:31Z", "summary": "Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.07361v1", "url_pdf": "https://arxiv.org/pdf/2602.07361.pdf", "meta_path": "data/raw/arxiv/meta/2602.07361.json", "sha256": "6685f24b741738cc3697dc011621149dac9cd9cfba961195b97497160f4494cf", "status": "ok", "fetched_at": "2026-02-18T02:19:32.547068+00:00"}, "pages": [{"page": 1, "text": "ViHERMES: A Graph-Grounded Multihop\nQuestion Answering Benchmark and System for\nVietnamese Healthcare Regulations\nLong S. T. Nguyen⋆[0009−0008−7488−4714], Quan M. Bui⋆, Tin T. Ngo, Quynh T.\nN. Vo, Dung N. H. Le, and Tho T. Quan(\f)[0000−0003−0467−6254]\nURA Research Group, Faculty of Computer Science and Engineering, Ho Chi Minh\nCity University of Technology (HCMUT), VNU-HCM, Vietnam\nAbstract. Question Answering (QA) over regulatory documents is in-\nherently challenging due to the need for multihop reasoning across legally\ninterdependent texts, a requirement that is particularly pronounced in\nthe healthcare domain where regulations are hierarchically structured\nand frequently revised through amendments and cross-references. De-\nspite recent progress in retrieval-augmented and graph-based QA meth-\nods, systematic evaluation in this setting remains limited, especially for\nlow-resource languages such as Vietnamese, due to the lack of bench-\nmark datasets that explicitly support multihop reasoning over health-\ncare regulations. In this work, we introduce the Vietnamese Healthcare\nRegulations–Multihop Reasoning Dataset (ViHERMES), a benchmark\ndesigned for multihop QA over Vietnamese healthcare regulatory docu-\nments. ViHERMES consists of high-quality question–answer pairs that\nrequire reasoning across multiple regulations and capture diverse depen-\ndency patterns, including amendment tracing, cross-document compar-\nison, and procedural synthesis. To construct the dataset, we propose a\ncontrolled multihop QA generation pipeline based on semantic clustering\nand graph-inspired data mining, followed by large language model–based\ngeneration with structured evidence and reasoning annotations. We fur-\nther present a graph-aware retrieval framework that models formal legal\nrelations at the level of legal units and supports principled context expan-\nsion for legally valid and coherent answers. Experimental results demon-\nstrate that ViHERMES provides a challenging benchmark for evaluating\nmultihop regulatory QA systems and that the proposed graph-aware\napproach consistently outperforms strong retrieval-based baselines. The\nViHERMES dataset and system implementation are publicly available\nat https://github.com/ura-hcmut/ViHERMES.\nKeywords: Multihop Question Answering · Healthcare Regulations ·\nGraph-Aware Retrieval · Low-Resource Languages\n1\nIntroduction\nAccessing and complying with regulatory documents is a critical requirement in\nmodern societies, particularly in domains where legal correctness directly affects\n⋆The two authors contributed equally to this work.\narXiv:2602.07361v1  [cs.CL]  7 Feb 2026\n"}, {"page": 2, "text": "2\nLong S. T. Nguyen et al.\npublic safety and service quality. As governments increasingly publish regulations\nin digital form, there is a growing demand for intelligent Question Answering\n(QA) systems that can assist users in understanding and navigating complex\nlegal and administrative texts [12].\nThe healthcare domain presents a particularly challenging setting for regula-\ntory QA. Unlike medical articles or clinical guidelines, which are often descriptive\nand relatively self-contained, healthcare regulations are legal documents with\nstrict administrative structures and dense inter-document dependencies. These\ndocuments are typically organized in a hierarchical manner, with clearly defined\nunits such as articles and clauses, and are frequently interconnected through legal\nrelations such as amendments, replacements, supplements, and cross-references.\nAs a result, regulatory QA in healthcare is inherently a multihop problem rather\nthan a single-document comprehension task. In practice, multihop reasoning be-\ncomes a fundamental requirement, as correct answers may depend on identifying\nthe latest effective provision, tracing chains of amendments across multiple doc-\numents, and combining definitions or procedural rules referenced from separate\nregulations, rendering single-hop retrieval or isolated passage matching funda-\nmentally insufficient in this setting [8,14].\nRecent advances in Artificial Intelligence, particularly in Natural Language\nProcessing, have significantly improved the capabilities of QA systems. Retrieval-\nAugmented Generation (RAG) frameworks have become a dominant paradigm\nby combining neural retrieval models with Large Language Models (LLMs) to\ngenerate answers grounded in retrieved contexts [9]. While naive RAG approaches\nenhance factual coverage, they typically treat documents as flat collections of\npassages and lack explicit awareness of legal structures and inter-document\nrelations [16]. In regulatory domains, this limitation often leads to answers\nthat are linguistically plausible but legally incomplete or outdated. To address\nthese shortcomings, graph-based retrieval and data mining methods such as\nGraphRAG [3], HippoRAG [6], and LightRAG [5] have been proposed to incor-\nporate structured knowledge into the retrieval process. However, when applied\nto regulatory documents, such automatically induced graphs often struggle to\nfaithfully capture the formal and hierarchical nature of legal texts [11]. In partic-\nular, multihop regulatory questions that depend on amendment chains or legal\nvalidity require precise, rule-aware relationships that are difficult to recover from\ngeneric entity–relation extraction alone.\nVietnamese is a low-resource language, and datasets that jointly address the\nhealthcare and legal or regulatory domains remain particularly scarce. Despite\nthe practical importance of multihop regulatory QA, the lack of high-quality\nbenchmark datasets has hindered systematic research in this area. Existing Viet-\nnamese legal QA datasets do include regulatory texts; however, they primarily\nfocus on general legal documents or regulations in other domains such as educa-\ntion, and are typically designed for single-document comprehension or shallow\nQA rather than multihop reasoning across documents [1,2,7,17,13]. Similarly,\nexisting Vietnamese healthcare QA datasets largely concentrate on clinical nar-\nratives, medical records, or healthcare-related news articles, rather than health-\n"}, {"page": 3, "text": "ViHERMES\n3\ncare regulatory documents [19,21,15]. Crucially, none of these datasets explicitly\nsupport multihop reasoning over legally interdependent healthcare regulations.\nAs a result, the performance and analytical understanding of QA systems in this\ndomain remain difficult to evaluate in a controlled and meaningful manner.\nIn this work, we introduce Vietnamese HEalthcare Regulations–Multihop\nREasoning DataSet (ViHERMES), the first benchmark dataset specifically de-\nsigned for multihop QA over Vietnamese healthcare regulations. ViHERMES\nfocuses on questions that inherently require reasoning across multiple regu-\nlatory documents, capturing diverse dependency patterns such as amendment\ntracing, cross-document comparison, and procedural synthesis. To construct the\ndataset, we propose a controlled multihop QA generation pipeline that leverages\nsemantic clustering as a form of graph-inspired data mining to sample coherent\nsets of regulatory contexts, followed by LLM-based QA generation with struc-\ntured evidence and reasoning annotations. Beyond the dataset, we also propose\na graph-aware retrieval framework tailored to the hierarchical and legally con-\nstrained nature of regulatory documents. Our approach represents regulations at\nthe level of legal units and explicitly models formal legal relations, enabling prin-\ncipled context expansion strategies that respect legal validity and avoid context\ndrift. This retrieval mechanism is integrated into a multi-agent QA system that\nseparates intent understanding, graph-based retrieval, and answer verification,\nthereby supporting robust and intelligent healthcare regulatory QA. Our main\ncontributions are summarized as follows.\n– We introduce ViHERMES, the first benchmark dataset for Vietnamese health-\ncare regulatory QA that explicitly targets multihop reasoning across legally\ninterdependent documents.\n– We propose a graph-inspired data mining pipeline for controlled multihop\nQA dataset construction with high-quality evidence-grounded annotations.\n– We present a graph-aware, multi-agent QA framework that effectively lever-\nages legal structure and demonstrates consistent improvements over strong\nretrieval-based baselines on ViHERMES.\n2\nRelated Works\n2.1\nMultihop QA and Graph-based Retrieval\nMultihop QA has been extensively studied to address queries that require rea-\nsoning over multiple pieces of evidence rather than a single passage. Early ap-\nproaches such as IRCoT [20] interleave retrieval with chain-of-thought reasoning\nto iteratively guide evidence selection, while RAPTOR [18] organizes textual\nunits into hierarchical summaries that support retrieval across different levels\nof abstraction. Building upon these ideas, recent graph-based retrieval frame-\nworks, including GraphRAG [3], HippoRAG [6], LightRAG [5], and MiniRAG\n[4], incorporate graph structures into retrieval-augmented generation by model-\ning entities, text chunks, or their relations as nodes and enabling neighborhood\nexpansion or graph-based ranking. These methods have demonstrated strong\n"}, {"page": 4, "text": "4\nLong S. T. Nguyen et al.\nperformance gains on general multihop QA benchmarks and improved efficiency,\nparticularly in resource-constrained settings. However, most existing approaches\nare primarily designed for unstructured or loosely structured text collections\nand rely on automatically induced, largely entity-centric graphs. Consequently,\nthey struggle to faithfully capture the formal hierarchy, rule-based dependencies,\nand legal validity constraints inherent in regulatory documents. This limitation\nis especially pronounced in the healthcare domain, where answering a question\noften requires tracing amendment chains, identifying currently effective provi-\nsions, and combining legally interdependent clauses across multiple documents.\nIn contrast, our work explicitly aligns both dataset construction and retrieval\nwith the intrinsic legal structure of healthcare regulations, enabling principled\nmultihop reasoning over legally interdependent documents.\n2.2\nVietnamese Regulation-related QA Datasets\nBenchmark datasets for Vietnamese QA remain relatively scarce, particularly for\ndomains involving structured regulatory documents. Existing datasets that cover\nregulations are primarily situated in the legal or educational domains. ALQAC\n[1] introduces a manually annotated legal QA dataset based on Vietnamese\nstatute laws, while several other works focus on regulations in specific domains,\nincluding educational management [13], bidding law (ViBidLQA) [7], university\ntraining regulations (ViRHE4QA) [2], and large-scale labor law retrieval with\nassociated QA data [17]. Although these datasets provide valuable resources for\nVietnamese legal QA, they are typically designed for single-document compre-\nhension or shallow question answering and do not explicitly support multihop\nreasoning across legally interdependent regulations. In contrast, existing Viet-\nnamese healthcare QA datasets largely focus on clinical or informational con-\ntent rather than regulatory texts. ViMedAQA [19] targets abstractive medical\nQA over clinical topics such as diseases and drugs, UIT-ViNewsQA [21] is con-\nstructed from healthcare news articles for machine reading comprehension, and\nViHealthQA [15] collects expert-answered questions from health websites. While\nthese datasets are valuable for healthcare-related QA, they do not reflect the hi-\nerarchical structure, amendment relationships, or cross-document dependencies\nthat characterize healthcare regulations. Consequently, none of the existing Viet-\nnamese datasets jointly address healthcare regulations and multihop reasoning\nover legally interdependent documents, leaving a critical gap that our ViHER-\nMES dataset is specifically designed to fill.\n3\nViHERMES Dataset\n3.1\nDataset Creation\nWe construct ViHERMES through a controlled dataset creation pipeline that\nintegrates human supervision, semantic clustering, and LLM-based generation to\nensure genuine multihop reasoning and evidence-grounded answers over health-\ncare regulatory documents. An overview of the pipeline is shown in Figure 1.\n"}, {"page": 5, "text": "ViHERMES\n5\nSemantic Cluster\nRaw\nCorpus\nData Sources\nStructured Corpus\n(+ Metadata)\nDocument Crawling\nPreprocessing\nSemantic Clustering\nCentroid\nContext Node\nRepresentative\nContexts\nSemantic Similarity Graph\nLanguage\nModel\nAnnotators\nGuidelines\nStructural Validation\nExpert\nTest Split\nTrain Split\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nPrompt\nFig. 1. ViHERMES dataset construction pipeline, from corpus collection and semantic\nclustering to LLM-based generation and splitting.\nStep 1: Annotator recruitment and guideline design.\nWe recruit a small\nteam of annotators together with a domain expert. The expert provides high-\nlevel guidance to ensure multihop validity and legal consistency. The annotation\nprocess follows shared guidelines defining multihop criteria, evidence usage, and\nlegal validity constraints.\nStep 2: Corpus collection and preprocessing.\nRegulatory documents are\ncollected from official data sources, such as public portals of the Vietnamese Min-\nistry of Health1, via automated document crawling. The collected texts form a\nraw corpus, which is subsequently processed by a preprocessing pipeline that nor-\nmalizes content, removes noise, and segments documents according to their legal\nhierarchy. The resulting structured corpus, together with associated metadata,\nconsists of context units corresponding to legal units (e.g., articles or clauses),\neach represented as a context triple ⟨id, title, text⟩.\nStep 3: Semantic clustering and similarity graph induction.\nEach con-\ntext is treated as a context node and encoded into an embedding vector ei ∈Rd\nusing a neural embedding model. To emphasize semantic direction rather than\nvector magnitude, embeddings are ℓ2-normalized as ˜ei = ei/∥ei∥2. We then per-\nform semantic clustering using K-Means on the normalized vectors, which cor-\nresponds to Spherical K-Means in practice. Each resulting semantic cluster Ck\nis summarized by its centroid ck =\n1\n|Ck|\nP\n˜ei∈Ck ˜ei. Contexts are ranked by their\nEuclidean distance to the centroid d(˜ei, ck) = ∥˜ei −ck∥2, and the nearest ones\nare selected as representative contexts. This cluster-and-centroid organization\ninduces an implicit semantic similarity graph that preserves topical coherence\namong regulatory texts. The graph is not an explicit legal graph; rather, it pro-\n1 https://moh.gov.vn/web/ministry-of-health\n"}, {"page": 6, "text": "6\nLong S. T. Nguyen et al.\nvides a latent similarity structure without relying on potentially noisy automatic\nextraction of legal relations.\nStep 4: Multihop QA generation and structural validation.\nFor a given\nhop level h, we sample tuples of contexts T = {c1, . . . , ch} from the pool of rep-\nresentative contexts under two constraints. First, contexts within a tuple must\noriginate from different document titles, i.e., ∀i ̸= j, title(ci) ̸= title(cj),\nto ensure cross-document reasoning. Second, to maintain diversity, the overlap\nbetween any two tuples Ta and Tb is bounded by |Ta ∩Tb|/|Ta| ≤τ. Each tuple\nis then provided to a language model through a structured prompt to generate\nmultihop question–answer pairs, where each answer must rely on all h contexts.\nIn practice, each hop corresponds to one legal dependency step, such as amend-\nment tracing, definition lookup, or cross-document procedural composition. The\ngenerated outputs undergo structural validation, which verifies format correct-\nness, consistency between hop level and evidence usage, and explicit reasoning.\nAmbiguous or borderline cases are further reviewed by annotators under expert\nsupervision to ensure legal coherence and correctness.\nStep 5: Dataset aggregation and splitting.\nAfter validation, the accepted\nquestion–answer instances are aggregated and exported in a structured format.\nThe dataset is then divided into a train split and a test split, with controlled\ndistributions over hop levels, forming the final ViHERMES benchmark.\n3.2\nDataset Statistics\nTable 1 presents a representative instance from the ViHERMES dataset, includ-\ning the query, answer, number of reasoning hops, explicit reasoning, supporting\nevidence, and corresponding context identifiers. Each identifier maps to a regula-\ntory context stored as a triple ⟨id, title, text⟩. The reasoning and evidence\nfields provide explicit explanations and extractive textual spans grounded in\nthe referenced contexts, respectively. Both fields are generated during dataset\nconstruction (Step 4) and subsequently verified by annotators, improving trans-\nparency and answer reliability. Table 2 summarizes key statistics of the ViHER-\nMES test set across different Numbers of Hops (NoH). For each hop level, we\nreport the number of samples, vocabulary size, and length statistics of ques-\ntions, answers, and supporting contexts. As NoH increases, question and answer\nlengths grow substantially, reflecting higher reasoning complexity, while context\nlengths expand rapidly, indicating the need to integrate information from long\nand diverse regulatory texts.\n4\nGraph-Aware System for Regulatory QA\nFigure 2 illustrates the proposed graph-aware QA system for Vietnamese health-\ncare regulations. Unlike standard RAG pipelines that treat the corpus as a flat\ncollection of passages, our system operates over a structure-driven regulatory\nknowledge graph, where legal texts are modeled as interconnected regulatory\nnodes linked by defined relations. The system follows a seeded retrieval and prop-\nagation paradigm, retrieving legally valid and contextually complete evidence.\n"}, {"page": 7, "text": "ViHERMES\n7\nTable 1. A representative multihop QA instance from the ViHERMES dataset.\nField\nContent\nQuery\nĐểđảm bảo tuân thủpháp luật một cách toàn diện, một cơ sởkhám bệnh, chữa bệnh cần thực\nhiện những nghĩa vụchính nào liên quan đến việc áp dụng giá dịch vụtrong giai đoạn chuyển\ntiếp, việc duy trì chất lượng cơ bản, và các điều kiện pháp lý đểđược phép hoạt động?\nEnglish: To ensure comprehensive legal compliance, what main obligations must a healthcare\nfacility fulfill regarding service pricing during the transition period, maintenance of baseline quality,\nand the legal conditions required for lawful operation?\nAnswer\nĐểtuân thủpháp luật toàn diện, một cơ sởkhám bệnh, chữa bệnh phải thực hiện các nghĩa vụ\nchính sau: (1) Vềđiều kiện hoạt động: Phải được cấp giấy phép hoạt động và bảo đảm các y, bác\nsĩ làm việc tại cơ sởcó chứng chỉhành nghềhợp lệ. (2) Vềchất lượng cơ bản: Phải triển khai các\nquy định vềtiêu chuẩn chất lượng, thực hiện rà soát, khắc phục đểduy trì chất lượng, và tiến\nhành tựđánh giá, báo cáo theo quy định. (3) Vềgiá dịch vụtrong giai đoạn chuyển tiếp: Khi có\nquyết định giá mới, đối với người bệnh đang điều trịtừtrước đó, cơ sởphải tiếp tục áp dụng mức\ngiá cũ cho đến khi người bệnh ra viện hoặc kết thúc đợt điều trịngoại trú.\nEnglish: To fully comply with legal requirements, a healthcare facility must fulfill the following\nobligations: (1) Operational conditions: the facility must obtain an operating license, and all\npracticing physicians must hold valid professional certificates. (2) Baseline quality: the facility must\nimplement quality standards, conduct reviews and corrective actions, and perform self-assessments\nwith mandatory reporting as required. (3) Service pricing during the transition period: when a\nnew pricing decision is issued, patients who began treatment earlier must continue to be charged\nthe previous price until discharge or completion of outpatient treatment.\nNumber of hops\n3\nContext IDs\n[4814-7506-4302, 7115-6545-3240, 6442-4111-1890]\nEvidence\ncontext_1: Quy định vềáp dụng giá trong giai đoạn chuyển tiếp: “Tiếp tục được áp dụng mức\ngiá dịch vụkhám bệnh, chữa bệnh theo quy định của cấp có thẩm quyền trước thời điểm thực\nhiện mức giá mới cho đến khi ra viện hoặc kết thúc đợt điều trịngoại trú.”\ncontext_2: Trách nhiệm của cơ sởkhám bệnh, chữa bệnh vềchất lượng: “Triển khai thực hiện\nquy định vềtiêu chuẩn chất lượng; rà soát, bổsung và khắc phục đểduy trì chất lượng khám\nbệnh, chữa bệnh ởmức cơ bản; thực hiện đánh giá và báo cáo kết quảtheo quy định.”\ncontext_3: Yêu cầu vềđiều kiện pháp lý đểhoạt động: “Cơ sởphải được cấp phép hoạt động;\ncác y, bác sĩ thực hiện khám bệnh, chữa bệnh phải có chứng chỉhành nghềtheo quy định.”\nEnglish: The evidence consists of three regulatory provisions covering service pricing during tran-\nsitional periods, baseline quality responsibilities, and legal requirements for operation, respectively.\nReasoning\nCâu trảlời tổng hợp các nghĩa vụcủa một cơ sởkhám bệnh, chữa bệnh từba văn bản khác nhau.\ncontext_3 cung cấp các điều kiện pháp lý tiên quyết đểđược phép hoạt động. context_2 xác định\ncác trách nhiệm liên quan đến việc duy trì và báo cáo chất lượng cơ bản. context_1 đưa ra quy\nđịnh thủtục vềviệc áp dụng giá dịch vụtrong giai đoạn chuyển tiếp. Việc kết hợp cảba nguồn\nlà cần thiết đểhình thành một câu trảlời đầy đủvà nhất quán vềmặt pháp lý.\nEnglish: The answer integrates three regulatory sources covering operational legality, baseline\nquality, and transitional service pricing, forming a legally consistent response.\n4.1\nStructure-Driven Regulatory Knowledge Graph\nThe system is built upon a Structure-driven Regulatory Knowledge Graph (SRKG),\nwhich encodes regulatory documents according to formal legal drafting conven-\ntions. Unlike entity-centric or LLM-induced graphs, SRKG is structure-driven,\nwith nodes and edges corresponding to legally grounded units and relations. The\ngraph is organized as a flat regulatory base layer, where legal-unit nodes coexist\non a unified plane and are retrieved directly, rather than through a top-down\npipeline. Formally, SRKG is defined as a directed labeled graph G = (V , E),\nwhere V denotes regulatory nodes and E ⊆V × R × V represents typed edges\nwith labels from R = Rstruct ∪Rlegal.\nRegulatory nodes.\nSRKG represents regulatory content using regulatory nodes\naugmented with provenance metadata. As illustrated in Figure 2, we distinguish\n"}, {"page": 8, "text": "8\nLong S. T. Nguyen et al.\nTable 2. Statistics of the ViHERMES test split across different multihop settings.\nLengths are reported as min / mean / max.\nNoH\nNoS\nVocab\nQuestion length\nAnswer length\nContext length\n1\n312\n2232\n48 / 113.2 / 215\n58 / 181.8 / 504\n195 / 1185.1 / 4257\n2\n312\n2949\n120 / 249.4 / 446\n89 / 399.1 / 810\n734 / 2729.4 / 10075\n3\n312\n3178\n168 / 325.7 / 688\n135 / 524.0 / 1100\n743 / 3739.8 / 14767\n4\n312\n3545\n179 / 386.4 / 813\n175 / 748.0 / 1696\n1461 / 5677.5 / 16556\n5\n312\n3707\n194 / 445.9 / 977\n101 / 889.9 / 1938\n1115 / 6451.6 / 18878\nAll\n1560\n5703\n48 / 304.2 / 977\n58 / 548.8 / 1938\n195 / 3953.9 / 18878\nAuditor\nPathfinder\nREFERS_TO\nInterpreter\nUser\nRetriever\nConductor\nMAS System\nHAS_ARTICLE\nHAS_ARTICLE\nHAS_ARTICLE\nAMENDS\nHAS_CHUNK\nAMENDS\nSUPPLEMENTS\nSUPPLEMENTS\nSUPPLEMENTS\nStructure-driven Regulatory KG\nRegulatory Node\n(+ Metadata)\nHAS_ARTICLE\nStructural Relation\nArticle\nDocument\nClause\nSeeded Retrieval\nPropagation\nFig. 2. Overview of the structure-driven regulatory QA system.\nthree conceptual levels: Document nodes representing legal documents, Clause\nnodes corresponding to legal units with standalone meaning, and Article nodes\ncapturing finer-grained units where applicable. Each node v ∈V is represented\nas v = ⟨id(v), text(v), meta(v)⟩, where meta(v) includes document identifier,\npromulgation date, issuing authority, and legal status. Each node is assigned a\ndeterministic identifier following a strict scheme (e.g., DocID::UnitID), enabling\nstable referencing, amendment-chain tracing, and incremental ingestion.\nRelations.\nSRKG encodes two complementary categories of relations, namely\nstructural relations and legal semantic relations. Structural relations Rstruct =\n{Has_Article, Has_Clause} preserve legal hierarchy and provenance. Legal\nsemantic relations Rlegal = {Amends, Replaces, Supplements, Refers_To}\ncapture inter-document dependencies affecting legal interpretation and validity.\nAll relations are induced using rule-based pattern matching over standardized\nlegal expressions (e.g., “sửa đổi Điều X” [amends Article X], “bổsung Khoản Y”\n[supplements Clause Y], “căn cứĐiều Z” [refers to Article Z]), yielding high pre-\n"}, {"page": 9, "text": "ViHERMES\n9\ncision and avoiding spurious links. When a regulatory node references a legal\nunit not yet present in the corpus, the system creates a lightweight placeholder\nnode to preserve the structural dependency. Such placeholders are automatically\nresolved when the corresponding documents are later ingested, ensuring graph\ncompleteness without disrupting downstream retrieval.\n4.2\nSeeded Retrieval and Propagation\nGiven a user query q, the system performs retrieval through a two-stage seeded\nretrieval and propagation strategy.\nSeeded retrieval.\nRetrieval is first conducted over the legal-unit base layer.\nThe Pathfinder identifies a small set of highly relevant seed regulatory nodes\nSK(q) = {v1, . . . , vK} ⊆V by ranking nodes using a hybrid relevance score\ns(v, q) = λ sdense(v, q) + (1 −λ) ssparse(v, q), where sdense measures semantic\nsimilarity in embedding space and ssparse captures lexical evidence (e.g., BM25).\nThe resulting SK(q) serves as anchor points for downstream expansion.\nPropagation.\nStarting from SK(q), the system expands context in a con-\ntrolled manner along relation-specific edges. For a node v and relation r, outgo-\ning neighbors are denoted as Nr(v) = {u | (v, r, u) ∈E}. This process activates\nthree principled flows:\n– Validity tracing.\nFor r ∈{Amends, Replaces, Supplements}, amend-\nment chains are recursively traversed until a terminal node is reached. A node\nu is terminal if ∀r′ ∈{Amends, Replaces, Supplements}, Nr′(u) = ∅,\nand such terminal nodes are preferred as legally applicable evidence.\n– Contextual supplementation.\nFor reference relations (r = Refers_To),\nexpansion is restricted to direct neighbors only, providing essential legal con-\ntext while preventing uncontrolled drift.\n– Provenance retrieval.\nStructural relations are used to retrieve upstream\ndocument metadata, ensuring provenance-aware evidence usage.\nContext assembly.\nThe final retrieval output is assembled as a bounded con-\ntext set C(q) = SK(q) ∪P(q), where P(q) denotes nodes obtained via relation-\naware expansion. Propagation depth and relation types are explicitly constrained\nbefore C(q) is passed to downstream agents.\n4.3\nMulti-Agent System\nThe QA pipeline is implemented as a lightweight Multi-Agent System (MAS)\ncomposed of functionally specialized agents, as illustrated in Figure 2. Each agent\nfulfills a distinct operational role within a unified regulatory QA workflow.\nInterpreter.\nThe Interpreter performs intent analysis and routing. It deter-\nmines whether a query is regulatory, extracts key signals such as document iden-\ntifiers, and decides whether graph-based retrieval is required. This step avoids\nunnecessary retrieval, improving overall execution efficiency.\n"}, {"page": 10, "text": "10\nLong S. T. Nguyen et al.\nPathfinder.\nThe Pathfinder implements the core graph-aware retrieval logic.\nIt performs seeded retrieval over regulatory nodes followed by relation-aware\npropagation on the SRKG, including validity tracing along amendment relations\nand bounded expansion over reference relations. The agent returns a legally\ncoherent and up-to-date context set C(q).\nAuditor.\nThe Auditor verifies the retrieved evidence and intermediate out-\nputs by checking consistency between regulatory nodes, relations, and generated\nclaims. It performs grounding validation and safety checks to detect unsupported\nor potentially hallucinated content.\nConductor.\nThe Conductor orchestrates the overall QA process. Based on the\nverified context C(q), it invokes an LLM to generate the final natural-language\nanswer and coordinates interactions among agents. When insufficient grounding\nis detected by the Auditor, the Conductor enforces conservative behavior such\nas abstention or clarification requests.\nOverall, the proposed system integrates structure-driven regulatory represen-\ntation, seeded retrieval, and relation-aware propagation within a unified multi-\nagent framework. By operationalizing legal structure directly during retrieval,\nthe system enables legally valid multihop reasoning and robust evidence ground-\ning, as demonstrated in our experiments.\n5\nExperimentations\nWe assess system performance along multiple complementary dimensions, in-\ncluding answer correctness, multihop reasoning quality, inference latency, graph\nconstruction overhead, and the effectiveness of agent coordination, and compare\nagainst representative baseline approaches.\n5.1\nDataset\nAll experiments are conducted on the test split of ViHERMES, which is specif-\nically designed to assess multihop reasoning over Vietnamese healthcare reg-\nulatory documents. The test set spans diverse hop levels and legal reasoning\npatterns, including amendment tracing, cross-document dependency resolution,\nand validity-aware clause composition.\n5.2\nBaselines\nWe compare the proposed system against representative baselines drawn from\nthree major families of retrieval-augmented and multihop QA approaches.\nNaive RAG.\nWe consider standard RAG pipelines based on flat passage re-\ntrieval, including (1) lexical retrieval using BM25, (2) dense retrieval via em-\nbedding similarity, and (3) hybrid retrieval combining lexical and dense scores\nthrough weighted interpolation. These baselines represent common RAG settings\nwithout explicit multihop or structural awareness.\n"}, {"page": 11, "text": "ViHERMES\n11\nReasoning-guided multihop QA.\nIRCoT [20] is a multi-step QA approach\nthat interleaves retrieval with Chain-of-Thought (CoT) reasoning to iteratively\nguide evidence selection using a strong LLM, without explicitly modeling docu-\nment structure or legal validity constraints.\nGraph-based RAG methods.\nWe include several representative graph-aware\nretrieval frameworks that incorporate graph structures into indexing and re-\ntrieval to support multihop reasoning:\n– RAPTOR [18] constructs a hierarchical tree of textual units via recursive\nsummarization, enabling retrieval across multiple levels of abstraction.\n– MiniRAG [4] is a lightweight graph-based system that combines a small\nlanguage model with a heterogeneous graph index to achieve efficient struc-\ntured retrieval under limited computational budgets.\n– LightRAG [5] emphasizes efficient graph construction and traversal strate-\ngies to balance retrieval quality and inference latency.\n– HippoRAG 2 [6] draws inspiration from hippocampal memory mechanisms,\nmodeling long-term knowledge as a graph to improve recall and evidence\nintegration across distant reasoning hops.\n5.3\nEvaluation Metrics\nWe evaluate system performance at both the answer and retrieval levels. For\nanswer quality, we report token-level F1 to measure surface-level overlap be-\ntween predicted and reference answers. Since F1 alone is insufficient to assess\nreasoning correctness and legal validity in multihop regulatory QA, we addition-\nally adopt an LLM-as-a-Judge evaluation protocol [10], where a strong external\nLLM evaluates answers in terms of correctness, completeness, and consistency\nwith supporting evidence. To assess retrieval quality independently of answer\ngeneration, we use Recall@k = |Egold∩E(k)\nretrieved|\n|Egold|\n, which measures the proportion\nof gold supporting contexts recovered within the top-k retrieved results and re-\nflects the effectiveness of evidence retrieval for multihop reasoning.\n5.4\nExperimental Setup\nTo ensure fair comparison and reproducibility, all systems share the same lan-\nguage model backbone and evaluation protocol. We adopt GPT-4o-mini2 as the\nunified LLM backbone for answer generation, chosen for its balance between rea-\nsoning capability and computational efficiency. Text embeddings are generated\nusing OpenAI text-embedding-3-small3, a multilingual embedding model sup-\nporting both Vietnamese and English. All baselines are evaluated using default\nhyperparameters to reflect realistic out-of-the-box performance. LLM-as-a-Judge\nevaluations are conducted using the GPT-4o4 API under a standardized rubric\nto ensure consistent and comparable scoring across systems.\n2 https://platform.openai.com/docs/models/gpt-4o-mini\n3 https://platform.openai.com/docs/models/text-embedding-3-small\n4 https://platform.openai.com/docs/models/gpt-4o\n"}, {"page": 12, "text": "12\nLong S. T. Nguyen et al.\nTable 3. Main QA performance on the ViHERMES test set.\nMethod\nF1\nLLM Judge Recall@5\nNaive RAG (BM25)\n0.3076\n0.2027\n0.2617\nNaive RAG (Dense)\n0.3289\n0.2433\n0.3241\nNaive RAG (Hybrid)\n0.4127\n0.3324\n0.3989\nIRCoT\n0.4835\n0.3751\n0.4254\nMiniRAG\n0.5429\n0.4856\n0.5083\nRAPTOR\n0.5941\n0.5783\n0.5563\nLightRAG\n0.7855\n0.6756\n0.7256\nHippoRAG 2\n0.8023\n0.7332\n0.8032\nOurs\n0.8334\n0.7554\n0.8461\nw/o Auditor\n0.8150\n0.6823\n0.8267\nw/o Interpreter\n0.6540\n0.5434\n0.6134\nw/o Pathfinder5\n0.7734\n0.6927\n0.7955\n5.5\nResults and Analysis\nTable 3 reports the QA performance of all evaluated systems on the ViHERMES\ntest set, covering answer quality (F1), reasoning reliability (LLM-as-a-Judge),\nand retrieval effectiveness (Recall@5). The results show a clear progression from\nflat, structure-agnostic RAG pipelines to multihop and graph-based approaches,\nindicating that Vietnamese healthcare regulatory QA requires structured multi-\nhop evidence integration. Among naive RAG baselines, Dense retrieval improves\nover BM25, and Hybrid retrieval further benefits from combining semantic and\nlexical signals; however, these approaches remain behind multihop methods due\nto the lack of explicit modeling of inter-document dependencies and legal validity\nconstraints. IRCoT improves both F1 and LLM Judge scores through iterative\nreasoning, but remains limited by operating over unstructured text without ex-\nplicit regulatory awareness. Graph-based methods achieve markedly stronger\nresults: MiniRAG and RAPTOR benefit from graph structure, while LightRAG\nand HippoRAG 2 further improve answer quality and Recall@5, reflecting more\neffective multihop evidence integration. Overall, the proposed system achieves\nthe best performance across all metrics, improving F1 by 3.1 points over Hip-\npoRAG 2 and confirming the effectiveness of structure-driven legal modeling\nwith relation-aware propagation.\nAblation results further clarify component contributions. Removing the In-\nterpreter causes the largest degradation (F1: 0.8334 →0.6540), highlighting the\nimportance of intent analysis and query routing. Excluding the Auditor leads\nto reduced reasoning reliability (LLM Judge: 0.7554 →0.6823), underscoring\nthe role of guardrail verification. Replacing the Pathfinder with flat hybrid re-\ntrieval also degrades performance, confirming the benefit of seeded retrieval with\nrelation-aware propagation over the SRKG.\n5 Instead of SRKG-based seeded retrieval and relation-aware propagation, Pathfinder\nis replaced with standard dense–sparse retrieval over flat text units.\n"}, {"page": 13, "text": "ViHERMES\n13\nTable 4. Inference efficiency and graph construction statistics.\nInference efficiency\nGraph construction\nMethod\nAvg. latency (s) Avg. tokens Nodes Edges Graph tokens\nNaive RAG (BM25)\n4.1139\n3009.8473\n–\n–\n–\nNaive RAG (Dense)\n6.1139\n3043.3433\n–\n–\n–\nNaive RAG (Hybrid)\n9.2348\n3289.3246\n–\n–\n–\nIRCoT\n11.8312\n4457.4561\n–\n–\n–\nMiniRAG\n13.1923\n4224.6595\n2988\n4323\n3,270,523\nRAPTOR\n14.2325\n4320.5432\n3352\n7234\n3,436,236\nLightRAG\n17.3236\n4558.3425\n3569\n9332\n3,684,343\nHippoRAG 2\n22.0278\n4859.3696\n2988\n11183\n3,966,963\nOurs\n14.7415\n4236.4620\n3727\n12540\n3,397,126\nTable 4 summarizes inference efficiency and graph construction. Compared\nwith Naive RAG and IRCoT, graph-based approaches incur higher overhead\nfrom indexing and traversal; however, explicit legal modeling and constrained\npropagation achieve a favorable effectiveness–efficiency trade-off. Despite con-\nstructing a larger SRKG, the proposed system maintains competitive inference\nlatency (14.74s on average), comparable to RAPTOR (14.23s) and faster than\nHippoRAG 2. Moreover, it outperforms LightRAG and HippoRAG 2 while using\nfewer graph tokens, indicating efficient context construction.\n6\nConclusion\nThis paper introduces ViHERMES, a benchmark for multihop QA over Viet-\nnamese healthcare regulations that captures the legally interdependent and fre-\nquently amended nature of regulatory texts. ViHERMES provides evidence-\ngrounded question–answer pairs with explicit reasoning annotations across di-\nverse dependency patterns, enabling systematic evaluation of multihop regula-\ntory QA in a low-resource language setting. To support this benchmark, we pro-\npose a SRKG and a graph-aware, multi-agent QA system that combines seeded\nretrieval, relation-aware propagation, and verification, achieving consistent gains\nover strong RAG and graph-based baselines while maintaining competitive infer-\nence latency and efficient context construction. Beyond healthcare, the proposed\nstructure-driven formulation is applicable to other regulatory domains with hier-\narchical organization and formal cross-document dependencies, and future work\nwill focus on constraining propagation volume and enriching temporal validity\nfor improved robustness in practical deployment.\nReferences\n1. Do, D.T., Luu, S.T., Pham, T., Vo, T., Chu, N.H., Chu, Q.H., Nguyen, C., Nguyen,\nM., Trieu, A., Nguyen, D., Tran, T., Nguyen, C., Nguyen, H., Nguyen, C., Le, N.K.,\n"}, {"page": 14, "text": "14\nLong S. T. Nguyen et al.\nNguyen, D.H., Dang, B., Nguyen, P., Nguyen, H.T., Tran, V., Nguyen, L.M.: A\nSummary of the ALQAC 2024 Competition. In: 2024 16th International Conference\non Knowledge and System Engineering (KSE). pp. 422–427 (2024). https://doi.\norg/10.1109/KSE63888.2024.11063484\n2. Do, T.P.P., Cao, N.D.D., Tran, K.Q., Van Nguyen, K.: R2GQA: retriever-reader-\ngenerator question answering system to support students understanding legal reg-\nulations in higher education. Artificial Intelligence and Law (May 2025). https://\ndoi.org/10.1007/s10506-025-09457-7, https://doi.org/10.1007/s10506-025-09457-7\n3. Edge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody, A., Truitt, S.,\nMetropolitansky, D., Ness, R.O., Larson, J.: From local to global: A graph rag\napproach to query-focused summarization (2024)\n4. Fan, T., Wang, J., Ren, X., Huang, C.: MiniRAG: Towards Extremely Simple\nRetrieval-Augmented Generation. arXiv preprint arXiv:2501.06713 (2025)\n5. Guo, Z., Xia, L., Yu, Y., Ao, T., Huang, C.: LightRAG: Simple and Fast\nRetrieval-Augmented Generation. In: Christodoulopoulos, C., Chakraborty, T.,\nRose, C., Peng, V. (eds.) Findings of the Association for Computational Linguis-\ntics: EMNLP 2025. pp. 10746–10761. Association for Computational Linguistics,\nSuzhou, China (Nov 2025). https://doi.org/10.18653/v1/2025.findings-emnlp.568,\nhttps://aclanthology.org/2025.findings-emnlp.568/\n6. Gutiérrez, B.J., Shu, Y., Qi, W., Zhou, S., Su, Y.: From RAG to Memory: Non-\nParametric Continual Learning for Large Language Models. In: Forty-second Inter-\nnational Conference on Machine Learning (2025), https://openreview.net/forum?\nid=LWH8yn4HS2\n7. Ha, N.T., Nguyen, T.P., Trung, K.T., Le, H.L., Huong, L.T.V., Nguyen, C.T.,\nNguyen, M.T.: Vietnamese Legal Question Answering: An Experimental Study.\nIn: 2024 16th International Conference on Knowledge and System Engineering\n(KSE). pp. 440–446 (2024). https://doi.org/10.1109/KSE63888.2024.11063637\n8. Lee, J., Kim, D., Hwang, S., Kim, H., Lee, G.: KoBLEX: Open Legal Question\nAnswering with Multi-hop Reasoning. In: Christodoulopoulos, C., Chakraborty,\nT., Rose, C., Peng, V. (eds.) Proceedings of the 2025 Conference on Empirical\nMethods in Natural Language Processing. pp. 4019–4053. Association for Com-\nputational Linguistics, Suzhou, China (Nov 2025). https://doi.org/10.18653/v1/\n2025.emnlp-main.200, https://aclanthology.org/2025.emnlp-main.200/\n9. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K¨uttler, H.,\nLewis, M., Yih, W.t., Rockt¨aschel, T., Riedel, S., Kiela, D.: Retrieval-augmented\ngeneration for knowledge-intensive NLP tasks. In: Proceedings of the 34th Inter-\nnational Conference on Neural Information Processing Systems. NIPS ’20, Curran\nAssociates Inc., Red Hook, NY, USA (2020)\n10. Li, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z., Bhattacharjee, A., Jiang,\nY., Chen, C., Wu, T., Shu, K., Cheng, L., Liu, H.: From Generation to Judg-\nment: Opportunities and Challenges of LLM-as-a-judge. In: Christodoulopoulos,\nC., Chakraborty, T., Rose, C., Peng, V. (eds.) Proceedings of the 2025 Conference\non Empirical Methods in Natural Language Processing. pp. 2757–2791. Associa-\ntion for Computational Linguistics, Suzhou, China (Nov 2025). https://doi.org/10.\n18653/v1/2025.emnlp-main.138, https://aclanthology.org/2025.emnlp-main.138/\n11. de Martim, H.: Graph RAG for Legal Norms: A Hierarchical and Temporal Ap-\nproach (2025)\n12. Martinez-Gil, J.: A survey on legal question–answering systems. Computer Science\nReview 48, 100552 (2023). https://doi.org/https://doi.org/10.1016/j.cosrev.2023.\n100552, https://www.sciencedirect.com/science/article/pii/S1574013723000199\n"}, {"page": 15, "text": "ViHERMES\n15\n13. Minh, D.D., Van, V.N., Cong, T.D.: Using Large Language Models for education\nmanagements in Vietnamese with low resources. In: Oco, N., Dita, S.N., Borlon-\ngan, A.M., Kim, J.B. (eds.) Proceedings of the 38th Pacific Asia Conference on\nLanguage, Information and Computation. pp. 20–34. Tokyo University of Foreign\nStudies, Tokyo, Japan (Dec 2024), https://aclanthology.org/2024.paclic-1.3/\n14. Nguyen, L.S.T., Luu, H.C., Vo, Q.T.N., La, H.N.G., Tran, H.M., Dinh, A.T.D.,\nNguyen, T.H., Ho, T.N., Quan, T.T.: Can Small Language Models Handle Viet-\nnamese Legal Reasoning? Insights from Multi-Task Evaluation. In: Mai, L.C.,\nHuyen, N.T.M., Trang, N.T.T. (eds.) Proceedings of the 11th International Work-\nshop on Vietnamese Language and Speech Processing. pp. 165–178. Association\nfor Computational Linguistics, Hanoi, Vietnam (Oct 2025), https://aclanthology.\norg/2025.vlsp-1.23/\n15. Nguyen, N.T.H., Ha, P.P.D., Nguyen, L.T., Van Nguyen, K., Nguyen, N.L.T.: SP-\nBERTQA: A Two-Stage Question Answering System Based on Sentence Trans-\nformers for Medical Texts. In: Knowledge Science, Engineering and Management.\npp. 371–382. Springer International Publishing (2022)\n16. Peng, B., Zhu, Y., Liu, Y., Bo, X., Shi, H., Hong, C., Zhang, Y., Tang, S.: Graph\nRetrieval-Augmented Generation: A Survey. ACM Trans. Inf. Syst. (Nov 2025).\nhttps://doi.org/10.1145/3777378, https://doi.org/10.1145/3777378, just Accepted\n17. Pham, V., Le, H.H., Ngo, T.P., Nguyen, B., Nguyen, D., Nguyen, H.D.: En-\nhancing legal research through knowledge-infused information retrieval for Viet-\nnamese labor law. IAES International Journal of Artificial Intelligence (IJ-AI)\n13(4), 3962–3973 (2024). https://doi.org/10.11591/ijai.v13.i4.pp3962-3973, https:\n//ijai.iaescore.com/index.php/IJAI/article/view/25255\n18. Sarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., Manning, C.D.: RAP-\nTOR: Recursive abstractive processing for tree-organized retrieval. In: The Twelfth\nInternational Conference on Learning Representations (2024), https://openreview.\nnet/forum?id=GN921JHCRw\n19. Tran, M.N., Nguyen, P.V., Nguyen, L., Dinh, D.: ViMedAQA: A Vietnamese\nMedical Abstractive Question-Answering Dataset and Findings of Large Lan-\nguage Model. In: Proceedings of the 62nd Annual Meeting of the Associ-\nation for Computational Linguistics. pp. 252–260. Association for Computa-\ntional Linguistics (2024). https://doi.org/10.18653/v1/2024.acl-srw.31, https://\naclanthology.org/2024.acl-srw.31/\n20. Trivedi, H., Balasubramanian, N., Khot, T., Sabharwal, A.: Interleaving Retrieval\nwith Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.\nIn: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers).\npp. 10014–10037. Association for Computational Linguistics, Toronto, Canada\n(Jul 2023). https://doi.org/10.18653/v1/2023.acl-long.557, https://aclanthology.\norg/2023.acl-long.557/\n21. Van Nguyen, K., Van Huynh, T., Nguyen, D.V., Nguyen, A.G.T., Nguyen, N.L.T.:\nNew Vietnamese Corpus for Machine Reading Comprehension of Health News Ar-\nticles. ACM Transactions on Asian and Low-Resource Language Information Pro-\ncessing 21(5) (2022). https://doi.org/10.1145/3527631, https://doi.org/10.1145/\n3527631\n"}]}