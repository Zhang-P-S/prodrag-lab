{"doc_id": "arxiv:2511.10900", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.10900.pdf", "meta": {"doc_id": "arxiv:2511.10900", "source": "arxiv", "arxiv_id": "2511.10900", "title": "Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering", "authors": ["Xueren Ge", "Sahil Murtaza", "Anthony Cortez", "Homa Alemzadeh"], "published": "2025-11-14T02:21:48Z", "updated": "2025-11-18T21:50:44Z", "summary": "Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.10900v2", "url_pdf": "https://arxiv.org/pdf/2511.10900.pdf", "meta_path": "data/raw/arxiv/meta/2511.10900.json", "sha256": "26ef8b5b12ed036e65059bc0780e96c9e00282fb2a1824ed17c2ca793e16e5fe", "status": "ok", "fetched_at": "2026-02-18T02:27:06.762355+00:00"}, "pages": [{"page": 1, "text": "Expert-Guided Prompting and Retrieval-Augmented Generation for\nEmergency Medical Service Question Answering\nXueren Ge1, Sahil Murtaza1, Anthony Cortez2, Homa Alemzadeh1\n1School of Engineering and Applied Sciences, University of Virginia\n2School of Medicine, University of Virginia\nzar8jw@virginia.edu, vpn9ej@virginia.edu, aec3gp@virginia.edu, ha4d@virginia.edu\nAbstract\nLarge language models (LLMs) have shown promise in med-\nical question answering, yet they often overlook the domain-\nspecific expertise that professionals depend on-such as the\nclinical subject areas (e.g., trauma, airway) and the certifica-\ntion level (e.g., EMT, Paramedic). Existing approaches typi-\ncally apply general-purpose prompting or retrieval strategies\nwithout leveraging this structured context, limiting perfor-\nmance in high-stakes settings. We address this gap with EM-\nSQA, an 24.3K-question multiple-choice dataset spanning 10\nclinical subject areas and 4 certification levels, accompanied\nby curated, subject area-aligned knowledge bases (40K doc-\numents and 2M tokens). Building on EMSQA, we introduce\n(i) Expert-CoT, a prompting strategy that conditions chain-of-\nthought (CoT) reasoning on specific clinical subject area and\ncertification level, and (ii) ExpertRAG, a retrieval-augmented\ngeneration pipeline that grounds responses in subject area-\naligned documents and real-world patient data. Experiments\non 4 LLMs show that Expert-CoT improves up to 2.05%\nover vanilla CoT prompting. Additionally, combining Expert-\nCoT with ExpertRAG yields up to a 4.59% accuracy gain\nover standard RAG baselines. Notably, the 32B expertise-\naugmented LLMs pass all the computer-adaptive EMS cer-\ntification simulation exams.\nCode & Data — https://uva-dsa.github.io/EMSQA\nIntroduction\nThe rapid advancement of large language models (LLMs)\nhas brought new possibilities to high-stakes domains such\nas emergency medical services (EMS) (Weerasinghe et al.\n2024), where accurate and reliable decision-making is criti-\ncal. There is growing interest in leveraging LLMs for med-\nical education (Abd-Alrazaq et al. 2023), decision sup-\nport (Ge et al. 2024; Luo et al. 2025), and certification prepa-\nration (Kung et al. 2023), particularly in the context of open-\ndomain multiple-choice question answering (MCQA). How-\never, while LLMs have shown promising performance on\ngeneral medical QA benchmarks (Cai et al. 2024), impor-\ntant gaps remain between their current capabilities and the\nreasoning processes used by trained medical professionals.\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nRecent approaches in medical MCQA, such as chain-of-\nthought prompting (CoT) (Wei et al. 2022) and retrieval-\naugmented generation (RAG) (Lewis et al. 2020), have im-\nproved LLM performance by enhancing reasoning capabil-\nities and incorporating external domain knowledge during\ninference. But these approaches often treat both reasoning\nand retrieval as undifferentiated processes: the model sees a\nquestion and retrieves documents or reasons directly, with-\nout considering what kind of knowledge is relevant or how\na human expert would approach the task. In contrast, real-\nworld medical professionals typically begin by identifying\nthe subject area of the question—e.g., whether it pertains\nto trauma, airway management, or pharmacology—and then\nreason from that domain-specific perspective, using knowl-\nedge and protocols appropriate to their level of certification.\nExisting benchmarks such as MedQA (Jin et al. 2021),\nMMLU-Med (Hendrycks et al. 2020) and MedMCQA (Pal,\nUmapathi, and Sankarasubbu 2022) lack both this structured\nrepresentation of question expertise (e.g., subject area, cer-\ntification level) and the associated domain-specific knowl-\nedge. This makes it difficult to align retrieval and reason-\ning processes with human-like problem-solving strategies.\nIn particular, publicly available EMS question answering\ndatasets and knowledge sources with expertise annotation\nare scarce. Further, current state-of-the-art (SOTA) methods\ndo not account for how incorporating structured expertise\ncan improve the overall effectiveness of reasoning and an-\nswer generation in retrieval-augmented systems.\nTo address these gaps, we propose a new dataset and a\ndomain-expertise-guided LLM framework that models med-\nical MCQA in a more structured, cognitively informed man-\nner. As shown in Figure 1, our contributions are three-fold:\n• We introduce EMSQA, the first EMS MCQA dataset\nof 24.3K questions, curated based on public and pri-\nvate sources, covering 10 subject areas and 4 certification\nlevels, and accompanied by a structured, subject area-\naligned EMS knowledge base (KB) with 40K documents\nand 4M real-world patient care reports. Partial data (from\npublic sources) and the whole EMS KB is shared as a re-\nsource with the EMS and research communities.\n• We propose a expertise-guided LLM framework that in-\nfers the domain expertise attributes and injects them\ninto LLMs using two approaches: (1) an expertise-\nguided prompting strategy (Expert-CoT) that encour-\narXiv:2511.10900v2  [cs.CL]  18 Nov 2025\n"}, {"page": 2, "text": "Figure 1: Overall Approach. (1) EMSQA, KB, PR Construction; (2) Tasks: Benchmark LLMs, RAG and Certification Exams.\nages step-by-step reasoning from a domain-specific per-\nspective. (2) an expertise-guided RAG method (Exper-\ntRAG) that selectively retrieves expertise-aligned knowl-\nedge from curated EMS KBs and patient records.\n• We benchmark multiple LLMs on EMSQA, evaluating\nperformance across certification levels and subject areas,\nand compare our framework against SOTA RAG meth-\nods. Experimental results show that combining Expert-\nCoT and ExpertRAG yields up to a 4.59% improvement\nin accuracy. Notably, the 32B expertise-augmented mod-\nels pass all the EMS certification simulation exams.\nRelated Works\nMedical Question Answering Datasets\nMedical QA datasets typically fall into two paradigms:\nretrieval-based tasks (Pampari et al. 2018; Krithara et al.\n2023; Ben Abacha and Demner-Fushman 2019), which\nrequire explicit evidence grounding by locating answers\nwithin documents, and open-domain multiple-choice tasks\nsuch as MedQA (Jin et al. 2021), MMLU-Med (Hendrycks\net al. 2021), and MedMCQA (Pal, Umapathi, and Sankara-\nsubbu 2022) that test implicit medical reasoning by choos-\ning the best answer based on world knowledge. However,\nexisting MCQA datasets focus on a single certification\nlevel and provide subject area labels without corresponding\nknowledge bases. EMSQA is the first open-domain medical\nMCQA dataset that spans multiple certification levels while\nalso furnishing both subject area annotations and a struc-\ntured EMS knowledge base. Table 1 shows a comparison\nbetween EMSQA and SOTA open-domain MCQA datasets.\nRetrieval Augmentation Generation\nThe basic RAG framework (Lewis et al. 2020) couples\na seq2seq model with a dense Wikipedia retriever, and\nhas since been improved via query rewriting (Chan et al.\n2024), entity graphs (Edge et al. 2024), and document-\nstructure–aware retrieval (Li et al. 2025a), though these\nmethods mainly target general-domain text. For the med-\nical domain, MedRAG (Xiong et al. 2024a) proposes a\nMedQA\nMedMCQA\nEMSQA\nDomain\nGeneral Med. General Med.\nEMS\nData Size\n12.7K\n193K\n24.3K\nExam\nUSMLE\nAIIMS&NEET PG NREMT\n#Certification\n1\n1\n4\n#Subject Area ✗\n21\n10\nKB\nRaw\n✗\nCategorized\nTable 1: Comparison of English medical MCQA datasets\nRAG pipeline with hybrid sparse–dense retrieval on Med-\nCorps, and i-MedRAG (Xiong et al. 2024b) extends it with\nfollow-up question generation and interactive reasoning.\nSelf-BioRAG (Jeong et al. 2024) adapts Self-RAG (Asai\net al. 2023) with domain-specific retrieval triggers, while\nRAG2 (Sohn et al. 2024) leverages rationale-based queries\nand filtering. EXPRAG (Ou et al. 2025) retrieves similar pa-\ntient cases, and ClinicalRAG (Lu, Zhao, and Wang 2024)\nexploits medical entities to query corpora. However, exist-\ning medical RAG systems largely ignore question-specific\nexpertise (e.g., subject area or certification) as explicit sig-\nnals to guide retrieval and reasoning. Unlike Metadata-\nRAG (Oudenhove 2024), which parses metadata directly\nfrom the query, we inject expertise attributes inferred from\nthe question by a model trained on our Q&A dataset.\nEMSQA\nData Collection and Preprocessing\nWe collected a total of 24.3K multiple-choice questions\nand their corresponding answer choices from practice tests\navailable on 17 websites targeting the National Registry\nof Emergency Medical Technicians (NREMT) examina-\ntion (NREMT 2001–2025). The NREMT exam is admin-\nistered as a Computer Adaptive Test (CAT), which dynami-\ncally adjusts question difficulty based on the examinee’s per-\nformance, providing a personalized and efficient assessment.\nIt certifies providers at 4 ascending levels of difficulty: entry-\nlevel, Emergency Medical Responder (EMR); basic-level,\n"}, {"page": 3, "text": "Set\nSize\nType\nCriteria\nvs KB vs PR\nPublic\nTrain(13,021) Semantic Avg Sim\n79.21\n66.45\nVal(1,860)\nSyntactic\n(hit rate)\nVocab\n82.95\n21.14\nTest(3,721)\nCpt w/o norm 41.65\n8.87\nCpt w/ norm\n63.30\n15.28\nPrivate Test(5,669)\nSemantic Avg Sim\n80.75\n75.35\nSyntactic\n(hit rate)\nVocab\n90.89\n28.26\nCpt w/o norm 53.18\n14.36\nCpt w/ norm\n72.49\n22.66\nTable 2: Statistics by split for Public and Private EMSQA\nand Semantic and syntactic evaluation of QA overlap vs.\nKB/PR. Cpt: Concept; norm: medical normalization.\nEmergency Medical Technician (EMT); intermediate-level,\nAdvanced EMT (AEMT); and advanced-level, Paramedic.\nOur dataset covers all four certification levels. These prac-\ntice tests assess examinees’ ability to apply medical knowl-\nedge, concepts, and principles, as well as their capacity to\ndemonstrate fundamental patient-centered skills.\nThe overall EMSQA dataset comprises questions from\nboth public and private (subscription-based) websites, but\nwe only release the portion derived from public materials\nbecause of copyright restriction of private websites (See Ap-\npendix A.2 in Extended version for more details). To en-\nsure the data quality, we did both automatic preprocessing\nand manual verification of the raw data as shown in Figure 1:\n• Heuristic rules were used to extract and remove special\ntokens like HTML tags, special symbols.\n• Each question is well-structured and represented as a\ndictionary containing the following fields: the question\ntext, answer options, correct answer, explanation, source\nURL, certification level, and subject area.\n• Questions related to images and tables were excluded.\nAll the questions are answerable using text inputs only.\n• All duplicate questions were removed by computing the\nLevenshtein distance between each pair of questions in\nthe dataset. Pairs with a similarity score greater than 0.9\nwere considered duplicates and subsequently removed.\n• All questions are manually labeled with subject areas,\nand both questions and answer choices have undergone\nhuman proofreading to correct any grammatical errors.\nA sample of 100 questions and KB documents was fur-\nther verified by an EMT expert (see Appendix A.5).\nData Statistics\nAs shown in Table 2, the dataset includes a total of\n18,602 and 5,669 practice questions from public and private\nsources, respectively. We use the private questions exclu-\nsively for testing and split the public questions into train, val-\nidation, and test sets of 13,021, 1,860, 3,721 questions, with\naverage token lengths of 18.27, 19.12, 18.99, respectively.\nMore detailed statistics are provided in Appendix A.2.2.\nFigure 2 details the distribution of questions by certifi-\ncation level and subject area. Because certification level is\nused for evaluation, all questions whose certification level\nFigure 2: Distributions of Questions by Certification Level\n(Left) and Subject Area (Right).\nmarked with “NA” are relegated to the training split, leaving\nthe validation and test splits to include only questions with\nexplicit EMS certification levels. Subject areas including\n“airway”, “cardiology”, “EMS operations”, “medical&OB”,\nand “trauma” dominate the corpus, which mirrors the five\ndomains mandated by the NREMT examination guidelines.\nThe remaining subject areas—“anatomy”, “assessment”,\n“pharmacology”, “pediatrics”, and “others”—are present as\nwell, but they appear less frequently, reflecting their sec-\nondary coverage in the practice materials we collected.\nKnowledge Collection and Preprocessing\nTo build an external KB for EMSQA, we curated 16 open-\naccess EMS education resources from reputable websites.\nAs shown in Figure 1, these resources span four media types:\nYouTube video transcripts (Carrie Davis 2025; The EMS\nProfessor 2025), official EMS guidelines (ODEMSA 2025),\nEMS education textbooks (American Red Cross 2025) or\nlecture slides (Jones & Bartlett Learning 2025), and EMS\nflashcards (EMT-Prep 2025). We segmented each YouTube\ntranscript into sections using the title cues supplied by the\nuploader. The PDF documents were converted to plain text\nwith PyPDF2 (Fenniak et al. 2022) and split to chapters us-\ning page ranges from their tables of contents. We then lever-\naged GPT-4o (Achiam et al. 2023) to reorganize the raw\nchapter text into a coherent section-level hierarchy (See Ap-\npendix A.3.1). Finally, we manually audited each section to\nensure its heading and text span aligned with the correspond-\ning passage in the original PDF and corrected any discrep-\nancies. Due to the lack of certification information in the\nsources, we only categorized the chapters into 10 subject ar-\neas based on their titles: “airway&ventilation”, “anatomy”,\n“assessment”, “cardiovascular”, “ems operations”, “medi-\ncal&ob”, “pediatrics”, “pharmacology”, “trauma” and “oth-\ners”. Our final EMS KB comprises 39,652 sections, totaling\n2,545,192 tokens and 34,110 unique vocabularies.\nTo evaluate the helpfulness of the KB for EMSQA, we\nassess our KB’s coverage from both syntactic and semantic\naspects. For semantic evaluation, we leverage MedCPT (Jin\net al. 2023) to retrieve, for each question, its most simi-\nlar document from the KB and report the average similar-\nity score between each question and its retrieved document.\nFor syntactic evaluation, we removed stop words and com-\n"}, {"page": 4, "text": "Figure 3: Expertise-Guided LLM Framework: Filter training, Expert-CoT Inference, and ExpertRAG Inference.\nputed vocabulary hit rate (KB ∩QA / QA) between the KB\nand EMSQA. We also follow the methods in\n(Ge et al.\n2024) to extract EMS-related concepts defined in (Preum\net al. 2020), both before and after UMLS normalization (Bo-\ndenreider 2004). Table 2 reports both syntactic and semantic\noverlaps with our KB. Syntactic hit rates span from 41.65%\nto 90.89%. Semantic similarity, measured by average co-\nsine similarity, was 79.21% on public and 80.75% on private\ndata. This indicates that our KB can support answering most\nEMSQA questions. Appendix A.3 provides comprehensive\ndetails on web crawling, EMS concept extraction, KB statis-\ntics, and the overlap between EMSQA and the KB.\nPatient Care Report Collection and Preprocesing\nWe used the National EMS Information System (NEM-\nSIS) (Dawson 2006) 2021 public research dataset as our\nsource of patient records (PR). NEMSIS is a large tabular\ncorpus in which each record includes fields such as dispatch\ndetails, scene information, initial assessment, EMS proto-\ncols and triage, vital signs, medications and procedural in-\nterventions, and patient history. As shown in Figure 1, we\nremoved any field whose value was “NA,” “Not Applica-\nble,” “Not Recorded,” or “Unknown.” If more than 30%\nof a record’s fields were discarded, we excluded the entire\nrecord. Finally, we converted each remaining record into\nplain text by concatenating its key–value pairs and cate-\ngorized the patient records into 6 subject areas based on\ntheir protocol field: “airway”, “assessment”, “cardiovascu-\nlar”, “medical&ob”, “pediatrics”, and “trauma”. Our final\ncorpus comprises 4,003,430 records with an average to-\nken length of 311.7. As shown in Table 2, semantic cover-\nage of NEMSIS over EMSQA is high, with patient records\nreaching 66.45%/75.35% similarity on the public/private\nsplit. In contrast, syntactic concept hit rates are much lower\n(8.87%–28.26%) than those of the KB. Extracted NEMSIS\nfields and summary statistics are provided in Appendix A.4.\nMethodology\nTask Formulation\nGiven the ith question qi and its answer options Oi =\n{o1, . . . , om}, the goal of MCQA is to maximize the like-\nlihood of selecting the correct answer a∗\ni ∈Oi. Let R be a\nretriever that takes qi as input and returns a set of relevant\ndocuments. A language model f selects an answer ai as:\nai = arg max\no∈Oi f(o | qi, Oi, R(qi))\n(1)\nWe propose an expertise-guided LLM framework (see\nFigure 3) with an expertise classification module (called Fil-\nter), which infers the domain expertise attributes of sub-\nject area si and certification level li from the input ques-\ntion qi, and incorporates them into f using two strategies:\n(1) Expert-CoT, a prompting strategy that encodes si and li\ninto a prompt template to guide f based on question-specific\nexpertise; and (2) ExpertRAG, a subject-area-specific R\nthat retrieves knowledge sources conditioned on si.\nFilter Design\nTo guide the LLM reasoning and RAG retrieval based on\nquestion-specific expertise, we train a lightweight LLM-\nbased filter to infer the key expertise attributes, including\nquestion’s subject area and certification level. As shown in\nFigure 3 (Left), we adopt LoRA (Hu et al. 2022) to inject\na small set of trainable parameters into the model while\nkeeping the full LLM weights fixed. We augment the LoRA\nmodules with two classification heads, Wsub and Wlvl, that\npredict the question’s subject area and certification level,\nand optimize them jointly in a multi-task setting (Li et al.\n2025b). We append a special token <classify> at the end\nof each query, and extract the hidden state hi of this final to-\nken from the last layer and feed it into our two classification\nheads:\nhi = LMlast(qi, Oi∥⟨classify⟩),\n(psub\ni\n, plvl\ni ) =\n\u0000σ(W⊤\nsubhi), σ(W⊤\nlvlhi)\n\u0001\n(2)\nwhere σ is sigmoid function. We use binary cross-entropy\nfor subject area classification and cross-entropy for certifi-\ncation classification. We set hyper-parameters wsub and wlvl\nto balance the multi-task loss. The overall training loss is:\nL = wsub · BCE\n\u0000psub\ni\n, ysub\ni\n\u0001\n+ wlvl · CE\n\u0000plvl\ni , ylvl\ni\n\u0001\n(3)\nDuring inference, given a question qi with answer options\nOi, the filter first predicts a certification-level probability\ndistribution plvl\ni\nand a multi-label subject area probability\nvector psub\ni\n. The predicted certification level and subject area\nset are computed as:\nˆsi = 1{psub\ni\n> 0.5},\nˆli = arg max plvl\ni .\n(4)\n"}, {"page": 5, "text": "Expertise-Guided Prompting (Expert-CoT)\nFigure 3 (Middle) illustrates our Expert-CoT prompting\nmethod. Standard CoT prompts encourage LLMs to reason\nstep by step but do not specify where to begin. In contrast,\nExpert-CoT prompting guides the model’s reasoning by ex-\nplicitly providing the subject area and certification level as\nstarting point for the thought process. The final answer is\ngenerated by passing the predicted subject area ˆsi and certi-\nfication level ˆli, into the Expert-CoT prompt template:\nˆAi = f CoT-Expert\u0000qi, Oi, ˆli, ˆsi\n\u0001\n.\n(5)\nExpertise-Guided RAG (ExpertRAG)\nAs shown in Figure 3 (Right), for ExpertRAG the filter’s\npredicted subject area guides the retriever to search for rel-\nevant knowledge base entries and patient records tailored to\nthe question’s subject area. The LLM then conditions on the\npredicted expertise and the retrieved documents to generate\nthe final answer. Based on qi and the predicted subject area\nˆsi, we explore three retrieval strategies for retriever R:\n• Global: Retrieve the top M and N evidence documents\nfrom the entire KB and PR, respectively. This serves as\na baseline corresponding to standard RAG without any\nsubject area filtering.\n• Filter then Retrieve (FTR): First filter the whole KB\nand PR to retain only documents matching the predicted\nsubject area ˆsi, then retrieve the top M and N documents\nfrom these filtered subsets.\n• Retrieve then Filter (RTF): First retrieve a larger can-\ndidate set from the whole KB and PR (e.g., 10 × M\nfrom KB and 10×N from PR), then filter out documents\nwhose subject area do not match ˆsi, retaining the top M\nand N relevant documents.\nThe final answer is generated by passing the retrieved\ndocuments, along with the predicted subject area and cer-\ntification level, into the RAG prompt template:\nˆAi = f RAG\u0000qi, Oi, R(qi, ˆsi), ˆli, ˆsi\n\u0001\n.\n(6)\nExperiments\nWe conduct extensive experiments to evaluate Expert-CoT\nand ExpertRAG methods by applying them to different base-\nline LLMs and comparing their performance to SOTA LLMs\nand RAGs. We aim to answer three research questions:\nRQ1: Where do SOTA LLMs shine or stumble on EMSQA\nacross subject areas and certification levels?\nRQ2: How much does explicit expertise injected by Expert-\nCoT and ExpertRAG lift baseline accuracy?\nRQ3: Can expertise-aware LLMs pass the NREMT stan-\ndardized tests at different certification levels?\nLLM Baselines\nWe use three categories of SOTA baseline models: (1)\nOpen-source LLMs: we select Qwen3-32B (Team 2025),\nand LLama-3.3-70B (Grattafiori et al. 2024) because of\ntheir great performance in multiple domains; (2) Medi-\ncal LLMs: we select OpenBioLLM-70B (Ankit Pal 2024),\nwhich currently leads the Open Medical-LLM Leader-\nboard (Pal et al. 2024). (3) Closed-source LLMs: we choose\nOpenAI-o3 (Brown et al. 2020) and Gemini-2.5-pro (Team\net al. 2023), both of which achieve top results on a range\nof benchmarks. We further apply 0- to 64-shot, CoT (Wei\net al. 2022), and Expert-CoT prompting to each baseline to\nbenchmark the performance across prompt strategies.\nRAG Baselines\nWe select the following SOTA medical RAG models due\nto their superior performance and code availability: (1)\nMedRAG (Xiong et al. 2024a), which is a RAG toolkit com-\nbining multiple medical documents; (2) i-MedRAG (Xiong\net al. 2024b), which iteratively refines medical queries via\nmulti-step retrieval; (3) Self-BioRAG (Jeong et al. 2024),\nwhich self-reflectively decides when to retrieve biomedical\ntexts and then generates the answer; (4) Qwen3-4B + KB,\nwhich is a vanilla RAG pipeline with our collected KB as re-\ntrieval corpora; (5) Qwen3-4B + PR, a vanilla RAG pipeline\nwith our collected PR as retrieval corpora; (6) Qwen3-4B +\nGlobal, a vanilla RAG pipeline with both KB and PR as re-\ntrieval corpora. We also include (7) Qwen3-4B with 0-shot\nand (8) Qwen3-4B with CoT prompting as baselines. For a\nfair comparison, we applied RAG with Qwen3-4B across\nall methods, except Self-BioRAG, which is trained from\nscratch. All baseline RAG methods used CoT prompting.\nImplementation Details\nFor Expert-RAG, we use Qwen3 as the core LLM, as it is the\nbest-performing open-source model in our benchmarking.\nWe employ MedCPT as our retriever due to its strong per-\nformance in medical domain and widespread use in SOTA\nRAG. We fix the number of retrieved documents at M = 32\nfor KB retrieval and N = 8 for PR retrieval. All KB and PR\ndocuments are chunked with a window of 512 tokens with\nan overlap of 128 tokens. Since some questions in EMSQA\nhave multiple correct answers, we report both exact-match\naccuracy (Acc) and sample-based F1 (Khashabi et al. 2018).\nßTo train the filter, we fine-tune LoRA modules with rank\nr=8, scaling factor α=16, and a dropout rate of 0.05, us-\ning the sequence length of 128 tokens. To balance the certi-\nfication and subject area classification objectives, we apply\nDWA (Liu, Johns, and Davison 2019) with T = 2 to dynam-\nically adjust wcat and wlvl. We use AdamW (Loshchilov and\nHutter 2017) optimizer and regularization with a weight de-\ncay of 0.01. We set the decision threshold of 0.5 for subject\narea classification. We fix the random seed at 42, and run all\nexperiments on NVIDIA H200 GPUs.\nExperimental Results\nLLM Benchmarking\nTo evaluate the overall performance of SOTA LLMs on EM-\nSQA (RQ1), we benchmark multiple LLMs under different\nprompting strategies. Figure 4 and Table 3 present the re-\nsults. We highlight several key findings from the evaluation:\nClosed-source models outperform open-source mod-\nels. In particular, OpenAI-o3 consistently achieves the high-\nest overall accuracy of 92.39. Among open-source models,\n"}, {"page": 6, "text": "Figure 4: (a) Certification level 0-shot performance. (b) Sub-\nject area 0-shot performance on the Public dataset\nModel\nPrompt\nPublic\nPrivate\nAcc\nF1\nAcc\nF1\nOpenBioLLM\n0-shot\n57.67 57.76 63.86 64.76\nCoT\n59.88 60.34 67.01 67.77\n(GT)\nExpert-CoT 61.92 62.03 68.75 69.82\n(Filter)\nExpert-CoT 61.32 61.93 67.79 68.32\nLlama-3.3\n0-shot\n81.69 82.69 78.06 78.77\nCoT\n81.89 83.08 85.16 86.35\n(GT)\nExpert-CoT 82.42 83.35 86.49 87.62\n(Filter)\nExpert-CoT 82.40 83.18 86.63 87.65\nQwen3-32B\n0-shot\n83.55 83.55 85.11 85.89\n4-shot\n84.41 84.41 85.48 86.13\n32-shot\n81.13 81.13 82.22 83.41\n64-shot\n82.48 82.48 86.22 87.26\nCoT\n84.96 84.97 88.78 90.13\n(GT)\nExpert-CoT 85.70 85.71 89.73 90.98\n(Filter)\nExpert-CoT 85.57 85.60 89.50 91.20\nOpenAI-o3\n0-shot\n92.39 92.39\n–\n–\nGemini-2.5\n0-shot\n89.36 89.36\n–\n–\nTable 3: Accuracy and F1 (%) of LLMs under Public vs.\nPrivate Data. GT/Filter: Ground-truth/predicted expertise.\nSplit\nModel\nMethod Subject Area Certification\nmiF\nmaF\nmiF\nmaF\nPublic\nFilter\nLoRA\n80.72\n71.92\n65.87\n63.45\nQwen3-4B 0-shot\n55.43\n51.61\n45.77\n30.49\nQwen3-4B 4-shot\n56.33\n54.42\n45.46\n30.28\nQwen3-4B CoT\n59.72\n55.66\n47.80\n35.77\nPrivate\nFilter\nLoRA\n79.06\n70.48\n65.54\n63.50\nQwen3-4B 0-shot\n42.93\n31.73\n44.12\n25.01\nQwen3-4B 4-shot\n45.76\n34.08\n44.04\n29.41\nQwen3-4B CoT\n46.22\n35.49\n47.70\n31.92\nTable 4: Expertise Classification Performance\nQwen3-32B achieves the best accuracy of 85.70, though a\nsignificant gap remains compared to closed-source models.\nFew-shot prompting improves accuracy up to a point.\nWe varied the number of in-context exemplars from 0 to\n64 (See Appendix A.7.2) and observed that incorporating\nModel\nDescription\nPublic\nPrivate\nAcc\nF1\nAcc\nF1\nNo-RAG Baselines\nQwen3-4B\n0-shot\n70.99\n71.01\n69.88\n69.95\nQwen3-4B\nCoT\n72.35\n73.09\n70.58\n72.02\nRAG Baselines + CoT\nMedRAG\nRAG on Med\n74.31\n74.41\n71.12\n73.33\ni-MedRAG\nIterative RAG\n77.96\n78.00\n74.02\n76.35\nSelf-BioRAG\nSelfRAG on Bio\n55.71\n58.84\n45.72\n49.67\nQwen3-4B\nKB\n76.49\n76.07\n75.02\n76.53\nQwen3-4B\nPR\n73.02\n73.96\n70.54\n72.38\nQwen3-4B\nGlobal\n78.12\n79.17\n75.46\n76.87\nRAG Baselines + Expert-CoT\n∆Acc/F1 = +1.38/+0.46\nQwen3-4B\nKB\n78.02\n79.04\n76.01\n76.25\nQwen3-4B\nPR\n73.82\n73.82\n71.53\n72.96\nQwen3-4B\nGlobal\n79.59\n79.61\n76.75\n77.35\nExpertRAG-GT + CoT\n∆Acc/F1 = +3.35/+2.71\nExpertRAG\nFTR\n80.97\n81.34\n79.13\n80.00\nExpertRAG\nRTF\n81.11\n81.45\n79.17\n80.01\nExpertRAG-GT + Expert-CoT\n∆Acc/F1 = +4.59/+3.69\nExpertRAG\nFTR\n81.62\n81.65\n80.40\n81.02\nExpertRAG\nRTF\n82.24\n82.26\n80.51\n81.16\nExpertRAG-Filter + Expert-CoT ∆Acc/F1 = +3.44/+2.59\nExpertRAG\nFTR\n80.99\n80.99\n79.45\n80.16\nExpertRAG\nRTF\n80.95\n80.96\n79.47\n80.22\nTable 5: End-to-end RAG Performance and Ablation Study\non ExpertRAG and Expert-CoT.\na small number of examples yields substantial gain over the\nzero-shot baseline. However, adding examples beyond a cer-\ntain point leads to diminishing or no improvement.\nBaseline LLMs underperform on easier questions. As\nshown in Figure 4a, across all models, we observe that\nperformance is lowest for the EMR certification level (the\nmost basic tier in the NREMT exam) and highest for the\nParamedic level. This may be due to smaller data size for\nEMR level and the procedural nature of EMR questions,\nwhereas Paramedic questions aligning more closely with the\nmedical content seen during LLM pretraining.\nLLMs falter on the core NREMT domains. Figure\n4b shows that models reliably answer “pharmacology” and\n“anatomy” questions but struggle in “pediatrics” and core ar-\neas such as “trauma”,“airway”, “operations”, and “cardiol-\nogy”. One possible reason is subject area complexity. Ques-\ntions in the former areas often need single-hop, fact-based\nqueries solvable in a zero-shot manner, whereas the latter\ndemand multi-hop reasoning and richer EMS knowledge.\nExpertise Classification Performance\nThe performance of our Filter vs. LLM baselines (0-shot, 4-\nshot, and CoT) for expertise classification are shown in Ta-\nble 4. Since subject area classification is a multi-label task\nand certification classification is a multi-class task, we re-\nport micro f1-score (miF) and macro f1-score (maF). Results\nshow our Filter trained with LoRA with two classification\nheads significantly outperforms the baseline LLMs.\n"}, {"page": 7, "text": "Model\nDescription\nEMR\nEMT\nAEMT\nParamedic\nPass Score\nAcc\nT\nPass Score\nAcc\nT\nPass Score\nAcc\nT\nPass Score\nAcc\nT\nQwen3-4B\n0-shot\n✗\n809\n64.18 33\n✗\n940\n74.07 33\n✗\n940\n71.64 33\n✗\n940\n71.72\n35\nExpert-CoT\n✗\n940\n72.42 59\n✗\n940\n76.73 73\n✓\n1179\n80.41 76\n✗\n940\n76.03\n87\nExpertRAG-4B\nFTR+Expert-CoT\n✓\n1218\n84.21 66\n✗\n940\n78.76 61\n✗\n940\n77.31 69\n✗\n940\n79.67\n74\nRTF+Expert-CoT\n✗\n940\n76.47 59\n✓\n1185\n81.30 93\n✓\n1190\n83.53 67\n✗\n940\n77.61\n86\nQwen3-32B\n0-shot\n✓\n1207\n82.65 22\n✓\n1140\n81.63 23\n✓\n1280\n85.92 26\n✓\n1163\n81.58\n29\nExpert-CoT\n✓\n1261\n86.27 50\n✓\n1255\n86.96 52\n✓\n1310\n89.11 61\n✓\n1292\n89.01\n57\nExpertRAG-32B FTR+Expert-CoT\n✓\n1350\n92.22 75\n✓\n1292\n89.01 76\n✓\n1215\n84.60 86\n✓\n1228\n83.93 125\nRTF+Expert-CoT\n✓\n1350\n92.22 75\n✓\n1328\n92.32 82\n✓\n1356\n92.31 82\n✓\n1276\n88.04\n99\nTable 6: Pass (✓) or Fail (✗) Summary of Models by Simulation Certification Test. T: Overall Time (min).\nExpert-CoT Evaluation\nTo assess how domain expertise, injected via Expert-CoT,\ninfluences reasoning (RQ2), we compare Expert-CoT with\ndifferent prompting strategies under multiple LLMs. As\nshown in Table 3, Expert-CoT help guide LLM reason-\ning. Integrating domain expert knowledge via CoT-Expert\nguides reasoning towards appropriate context and consis-\ntently boosts CoT prompting performance by up to 2.05%\nacross models. Also, using the predicted expertise attributes\n(Filter) vs. the ground-truth attribute annotations (GT) yields\ncomparable performance for Expert-CoT, demonstrating the\nFilter’s strong performance, as also shown in Table 4.\nAblation Study on Expert-CoT and ExpertRAG\nWe further evaluate the effect of injecting expertise into\nCoT and RAG (RQ2) using an ablation study with six\nconfigurations, as shown in Table 5: (1) No-RAG, (2)\nRAG+CoT with a standard global retriever on EMS PR\nand KB, (3) RAG+Expert-CoT, (4) ExpertRAG-GT+CoT,\n(5)\nExpertRAG-GT+Expert-CoT,\nand\n(6)\nExpertRAG-\nFilter+Expert-CoT. An ablation study on certification and\nsubject area is presented in Appendix A.8. The best configu-\nration outperforms the baseline by 4.59 / 3.69 points in Acc\n/ F1 (See error analysis in Appendix A.9).\nEffect of PR and KB. (2) vs. (1) isolates the gain of\nadding PR and KB as retrieval documents with a standard\nglobal retriever. Results show KB brings more improvement\nthan PR, and combing both yields the best performance.\nEffect of Expert-CoT. (3) vs. (2) (and (5) vs. (4)) ablates\nthe additional gain from Expert-CoT on RAGs, showing that\nexpertise-aware reasoning is better than standard reasoning.\nEffect of Expert-RAG. (4) vs. (2) (and (5) vs. (3)) ab-\nlate the impact of our Expert-RAG (FTR/RTF) compared\nto standard RAG with a global retriever. With ground-truth\nsubject area and certification level, ExpertRAG consistently\noutperforms SOTA RAG baselines, highlighting the value of\nexpertise-guided retrievers. Both FTR and RTF outperform\nglobal retrieval, with RTF achieving better performance.\nEffect of Filter. (6) vs. (5) measures the effect of using the\nFilter’s predicted expertise vs. ground-truth expertise anno-\ntations. There is a small performance drop, but ExpertRAG-\nFilter still outperforms the best baseline.\nNREMT Computer Adaptive Simulation Tests\nTo investigate whether our best models can be certified in\nNREMT exam (RQ3), we subscribed to MedicTests (Me-\ndicTests 2025), the NREMT Computer Adaptive Simulation\nTest. The simulation exam consists of 80-150 adaptively se-\nlected questions and must be completed within 2.5 hours.\nThe NREMT cognitive exam is scored on a 100–1500 scale,\nwith 950 as the passing threshold. We evaluated our Expert-\nCoT and ExpertRAG models, along with 0-shot baselines.\nThe expertise-augmented models used the trained Filter to\npredict the subject area and certification. The Pass/Fail out-\ncomes are shown in Table 6. All models completed the exam\nwithin the allotted time, though expertise-augmented mod-\nels took much longer. These models consistently achieved\nhigher test scores and significantly improved accuracy rel-\native to baseline LLMs. However, performance varied with\nmodel size. 4B models failed at one or more certification lev-\nels, whereas 32B models passed all four. ExpertRAG-32B\nwith the RTF retrieval strategy achieved the highest over-\nall score across certifications. Notably, although the smaller\nLLM did not pass the test, it benefited the most from ex-\npertise augmentation, by showing the largest accuracy gains\nand achieving scores near or above the passing threshold.\nConclusion\nThis paper presents a domain expertise-aware LLM frame-\nwork for medical multiple-choice question answering that\ninfers and incorporates expertise to guide LLM reason-\ning and RAG retrieval. We introduce EMSQA, the first\nlarge-scale labeled MCQA dataset for EMS with subject\narea and certification-level annotations, along with curated\nEMS knowledge bases. We propose Expert-CoT, which\nguides LLM reasoning by injecting expertise attributes\ninto prompts, and ExpertRAG, which retrieves expertise-\nspecific knowledge for augmented generation. Experiments\nshow that our expertise-aware prompting and RAG strate-\ngies significantly improve performance over baselines. Im-\nportantly, the expertise-augmented LLMs pass the NREMT\nsimulation tests across all EMS certification levels. EMSQA\nprovides a new benchmark for MCQA research in medical\ndomain, and our proposed expertise-aware LLM framework\ncan be applied to other medical MCQA datasets with similar\nor other expertise attributes.\n"}, {"page": 8, "text": "Ethics Statement\nAll models studied in this work are research prototypes and\nnot approved medical devices. They must not be used as\nthe sole basis for diagnosis or treatment decisions. Outputs\nshould serve only as a reference for licensed healthcare pro-\nfessionals, who remain fully responsible for clinical judg-\nment and patient care. The models may generate incorrect,\nincomplete, or biased recommendations and may not reflect\nup-to-date guidelines. All experiments were conducted in\nsimulation, with no model outputs used to influence real-\nworld patient care. All private data were kept confidential.\nAcknowledgments\nThis work was supported by the award 70NANB21H029\nfrom the U.S. Department of Commerce, National Institute\nof Standards and Technology (NIST), and a research grant\nfrom the Commonwealth Cyber Initiative (CCI).\nReferences\nAbd-Alrazaq, A.; AlSaad, R.; Alhuwail, D.; Ahmed, A.;\nHealy, P. M.; Latifi, S.; Aziz, S.; Damseh, R.; Alrazak, S. A.;\nSheikh, J.; et al. 2023. Large language models in medical\neducation: opportunities, challenges, and future directions.\nJMIR Medical Education, 9(1): e48291.\nAchiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;\nAleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;\nAnadkat, S.; et al. 2023.\nGpt-4 technical report.\narXiv\npreprint arXiv:2303.08774.\nAmerican Red Cross. 2025. Emergency Medical Response\n(Red\nCross\nPDF).\nhttps://www.redcross.org/content/\ndam/redcross/training-services/course-fact-sheets/EMR-\nTextbook-2017-LoRes-111017.pdf. Accessed: 2025-07-14.\nAnkit Pal, M. S. 2024. OpenBioLLMs: Advancing Open-\nSource Large Language Models for Healthcare and Life\nSciences.\nhttps://huggingface.co/aaditya/OpenBioLLM-\nLlama3-70B.\nAsai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H. 2023.\nSelf-rag: Learning to retrieve, generate, and critique through\nself-reflection. In The Twelfth International Conference on\nLearning Representations.\nBen Abacha, A.; and Demner-Fushman, D. 2019.\nA\nquestion-entailment approach to question answering. BMC\nbioinformatics, 20: 1–23.\nBluefield Rescue. 2025. EMT Basic Exam, 5th Edition. http:\n//www.bluefieldrescue.org/EMTBasicExam5thEdition.pdf.\nAccessed: 2025-07-14.\nBodenreider, O. 2004. The unified medical language system\n(UMLS): integrating biomedical terminology. Nucleic acids\nresearch, 32(suppl 1): D267–D270.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nCai, Y.; Wang, L.; Wang, Y.; de Melo, G.; Zhang, Y.; Wang,\nY.; and He, L. 2024.\nMedbench: A large-scale chinese\nbenchmark for evaluating medical large language models.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 38, 17709–17717.\nCareerEmployer.\n2025.\nAEMT\nPractice\nTest—CareerEmployer.\nhttps://careeremployer.com/test-\nprep/practice-tests/aemt-practice-test/. Accessed: 2025-07-\n14.\nCarrie Davis. 2025. Carrie Davis YouTube Channel. https:\n//www.youtube.com/@CarrieDavis. Accessed: 2025-07-14.\nChan, C.-M.; Xu, C.; Yuan, R.; Luo, H.; Xue, W.;\nGuo, Y.; and Fu, J. 2024.\nRq-rag: Learning to refine\nqueries for retrieval augmented generation. arXiv preprint\narXiv:2404.00610.\nDawson, D. E. 2006.\nNational emergency medical ser-\nvices information system (NEMSIS).\nPrehospital Emer-\ngency Care, 10(3): 314–316.\nDelaware Health and Social Services. 2025.\nParamedic\nMedication Manual (Delaware). https://www.dhss.delaware.\ngov/dph/ems/files/PHARMACOLOGYMANUAL2024.\npdf. Accessed: 2025-07-14.\nDocDrop / Arthur Hsieh. 2025. EMT Exam for Dummies\n(with Online Practice).\nhttps://docdrop.org/static/drop-\npdf/EMT-Exam-For-Dummies-with-Online-Practice---\nArthur-Hsieh-Qvn0s.pdf. Accessed: 2025-07-14.\nEdge, D.; Trinh, H.; Cheng, N.; Bradley, J.; Chao, A.;\nMody, A.; Truitt, S.; Metropolitansky, D.; Ness, R. O.; and\nLarson, J. 2024.\nFrom local to global: A graph rag ap-\nproach to query-focused summarization.\narXiv preprint\narXiv:2404.16130.\nEMRA. 2025.\nEMS Essentials: A Resident’s Guide to\nPrehospital Care.\nhttps://www.emra.org/siteassets/emra/\npublications/books/emra-ems-essentials.pdf.\nAccessed:\n2025-07-14.\nEMT-Prep. 2025. EMT-Prep App. https://app.emtprep.com/.\nAccessed: 2025-07-14.\nEs, S.; James, J.; Anke, L. E.; and Schockaert, S. 2024. Ra-\ngas: Automated evaluation of retrieval augmented genera-\ntion. In Proceedings of the 18th Conference of the European\nChapter of the Association for Computational Linguistics:\nSystem Demonstrations, 150–158.\nFenniak, M.; Stamy, M.; pubpub zz; Thoma, M.; Peveler,\nM.; exiledkingcc; and PyPDF2 Contributors. 2022.\nThe\nPyPDF2 library. https://pypi.org/project/PyPDF2/.\nFirearms Training Los Angeles CA. 2025.\nEmergency\nMedical Responder: Your First Response in Emergency\nCare. https://firearmstraininglosangelesca.com/wp-content/\nuploads/2019/07/AAOS First Responder eBook1.pdf. Ac-\ncessed: 2025-07-14.\nGe, X.; Satpathy, A.; Williams, R.; Stankovic, J.; and\nAlemzadeh, H. 2024.\nDKEC: Domain Knowledge En-\nhanced Multi-Label Classification for Diagnosis Prediction.\nIn Proceedings of the 2024 Conference on Empirical Meth-\nods in Natural Language Processing, 12798–12813.\nGrattafiori, A.; Dubey, A.; Jauhri, A.; Pandey, A.; Kadian,\nA.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.;\nVaughan, A.; et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\n"}, {"page": 9, "text": "Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,\nM.; Song, D.; and Steinhardt, J. 2020.\nMeasuring mas-\nsive multitask language understanding.\narXiv preprint\narXiv:2009.03300.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Mul-\ntitask Language Understanding. Proceedings of the Interna-\ntional Conference on Learning Representations (ICLR).\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\nS.; Wang, L.; Chen, W.; et al. 2022. Lora: Low-rank adapta-\ntion of large language models. ICLR, 1(2): 3.\nJeong, M.; Sohn, J.; Sung, M.; and Kang, J. 2024. Improving\nmedical reasoning through retrieval and self-reflection with\nretrieval-augmented large language models. Bioinformatics,\n40(Supplement 1): i119–i129.\nJin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and\nSzolovits, P. 2021. What disease does this patient have? a\nlarge-scale open domain question answering dataset from\nmedical exams. Applied Sciences, 11(14): 6421.\nJin, Q.; Kim, W.; Chen, Q.; Comeau, D. C.; Yeganova, L.;\nWilbur, W. J.; and Lu, Z. 2023. Medcpt: Contrastive pre-\ntrained transformers with large-scale pubmed search logs for\nzero-shot biomedical information retrieval. Bioinformatics,\n39(11): btad651.\nJones & Bartlett Learning. 2025. JB Learning EMS Slides.\nhttps://www.jblearning.com/. Accessed: 2025-07-14.\nKhashabi, D.; Chaturvedi, S.; Roth, M.; Upadhyay, S.; and\nRoth, D. 2018. Looking beyond the surface: A challenge\nset for reading comprehension over multiple sentences. In\nProceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Papers),\n252–262.\nKrithara, A.; Nentidis, A.; Bougiatiotis, K.; and Paliouras,\nG. 2023.\nBioASQ-QA: A manually curated corpus for\nBiomedical Question Answering.\nScientific Data, 10(1):\n170.\nKung, T. H.; Cheatham, M.; Medenilla, A.; Sillos, C.;\nDe Leon, L.; Elepa˜no, C.; Madriaga, M.; Aggabao, R.; Diaz-\nCandido, G.; Maningo, J.; et al. 2023. Performance of Chat-\nGPT on USMLE: potential for AI-assisted medical educa-\ntion using large language models. PLoS digital health, 2(2):\ne0000198.\nLearningExpress Hub. 2025.\nLearningExpress Hub EMS\nPrep. https://www.learningexpresshub.com/ProductEngine/\nLELIndex.html#/center/learningexpresslibrary/career-\ncenter/home/prepare-for-emergency-medical-services-and-\nfirefighting-exams. Accessed: 2025-07-14.\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;\nGoyal, N.; K¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt¨aschel,\nT.; et al. 2020.\nRetrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in neural infor-\nmation processing systems, 33: 9459–9474.\nLi, Z.; Chen, X.; Yu, H.; Lin, H.; Lu, Y.; Tang, Q.; Huang,\nF.; Han, X.; Sun, L.; and Li, Y. 2025a. StructRAG: Boost-\ning Knowledge Intensive Reasoning of LLMs via Inference-\ntime Hybrid Information Structurization. In The Thirteenth\nInternational Conference on Learning Representations.\nLi, Z.; Luo, X.; Ge, X.; Zhou, L.; Lin, X.; and Liu,\nY. 2025b.\nMMSense: Adapting Vision-based Founda-\ntion Model for Multi-task Multi-modal Wireless Sensing.\narXiv:2511.12305.\nLiu, S.; Johns, E.; and Davison, A. J. 2019.\nEnd-to-\nend multi-task learning with attention. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 1871–1880.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101.\nLu, Y.; Zhao, X.; and Wang, J. 2024.\nClinicalRAG: En-\nhancing Clinical Decision Support through Heterogeneous\nKnowledge Retrieval. In Proceedings of the 1st Workshop\non Towards Knowledgeable Language Models (KnowLLM\n2024), 64–68.\nLuo, X.; Jha, S. M. N.; Sinha, A.; Li, Z.; and Liu,\nY. 2025.\nALPHA: LLM-Enabled Active Learning for\nHuman-Free Network Anomaly Detection. arXiv preprint\narXiv:2509.05936.\nMedicTests. 2025. Exact replica of the NREMT Simulator.\nhttps://medictests.com/. Accessed: 2025-07-18.\nMediPro First Aid. 2025.\nProfessional Responder Cheat\nSheet.\nhttps://www.mediprofirstaid.net/free-downloads/\nProfessional Responder Cheat Sheet.pdf. Accessed: 2025-\n07-14.\nMinniQuiz.\n2025.\nFirst\nResponder\nPractice\nSets.\nAvailable at: https://minniquiz.com/First Responder/; https:\n//minniquiz.com/First Responder1/; https://minniquiz.com/\nFirst Responder2/; https://minniquiz.com/First Responder\nParamedic/. Accessed: 2025-07-14.\nMometrix\nAcademy.\n2025.\nMometrix\nEMT\n&\nParamedic\nPractice.\nAvailable\nat:\nhttps:\n//www.mometrix.com/academy/advanced-emt;\nhttps:\n//www.mometrix.com/academy/emt-practice-test;\nhttps:\n//www.mometrix.com/academy/emt-exam-paramedics.\nAccessed: 2025-07-14.\nMontgomery County MD. 2025.\nMontgomery County\nNREMT Resource.\nhttps://www.montgomerycountymd.\ngov/mcfrs-psta/Resources/Files/Instructor/NR EMT Test.\nhtm. Accessed: 2025-07-14.\nMy CPR Certification Online. 2025.\nPractice Tests:\nEMR,\nEMT,\nand\nParamedic.\nAvailable\nat:\nhttps:\n//www.mycprcertificationonline.com/practice-tests/emr;\nhttps://www.mycprcertificationonline.com/practice-tests/\nemt;\nhttps://www.mycprcertificationonline.com/practice-\ntests/paramedics. Accessed: 2025-07-14.\nNREMT. 2001–2025.\nNational Registry of Emergency\nMedical Technicians.\nhttps://www.nremt.org/.\nAccessed:\n2025-07-18.\nNREMT Practice Test. 2025. NREMT Practice Test. https:\n//nremtpracticetest.com/. Accessed: 2025-07-14.\nODEMSA. 2025.\nODEMSA Regional EMS Documents.\nhttps://odemsa.net/regional-documents/.\nAccessed: 2025-\n07-14.\n"}, {"page": 10, "text": "Ou, J.; Huang, T.; Zhao, Y.; Yu, Z.; Lu, P.; and Ying, R. 2025.\nExperience Retrieval-Augmentation with Electronic Health\nRecords Enables Accurate Discharge QA. arXiv preprint\narXiv:2503.17933.\nOudenhove, L. V. 2024.\nEnhancing RAG Performance\nwith Metadata: The Power of Self-Query Retrievers.\nhttps://medium.com/@lorevanoudenhove/enhancing-rag-\nperformance-with-metadata-the-power-of-self-query-\nretrievers-e29d4eecdb73. Accessed: 2025-11-14.\nPal, A.; Minervini, P.; Motzfeldt, A. G.; and Alex, B.\n2024. Open Medical LLM Leaderboard. https://github.com/\nopenlifesciencesai/open medical llm leaderboard.\nPal, A.; Umapathi, L. K.; and Sankarasubbu, M. 2022.\nMedmcqa: A large-scale multi-subject multi-choice dataset\nfor medical domain question answering. In Conference on\nhealth, inference, and learning, 248–260. PMLR.\nPampari, A.; Raghavan, P.; Liang, J.; and Peng, J. 2018. em-\nrQA: A Large Corpus for Question Answering on Electronic\nMedical Records. In Riloff, E.; Chiang, D.; Hockenmaier,\nJ.; and Tsujii, J., eds., Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 2357–2368. Brussels, Belgium: Association for Com-\nputational Linguistics.\nPocketPrep. 2025.\nPocketPrep EMT/Paramedic.\nhttps:\n//www.pocketprep.com/. Accessed: 2025-07-14.\nPracticeTestGeeks. 2025. PracticeTestGeeks EMR. https:\n//practicetestgeeks.com/emr-practice-test/. Accessed: 2025-\n07-14.\nPreum, S. M.; Shu, S.; Alemzadeh, H.; and Stankovic, J. A.\n2020. Emscontext: EMS protocol-driven concept extraction\nfor cognitive assistance in emergency response. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol-\nume 34, 13350–13355.\nQuizizz. 2025.\nQuizizz.\nhttps://quizizz.com.\nAccessed:\n2025-07-14.\nQuizlet. 2025.\nQuizlet Question Banks for EMR,\nEMT,\nAEMT,\nand\nParamedic.\nAvailable\nat:\nhttps://quizlet.com/search?query=emr-practice-test&type=\nquestionBanks;\nhttps://quizlet.com/search?query=emt-\npractice-test&type=questionBanks;\nhttps://quizlet.com/\nsearch?query=aemt-practice-test&type=questionBanks;\nhttps://quizlet.com/search?query=paramedics-practice-\ntest&type=questionBanks. Accessed: 2025-07-14.\nRhode Island Department of Health. 2025.\nEMS\nPharmacology\nReference\nGuide.\nhttps://health.ri.\ngov/sites/g/files/xkgbur1006/files/publications/guides/\nEMSPharmacologyReference.pdf. Accessed: 2025-07-14.\nSmartMedic. 2025.\nSmartMedic EMT Practice.\nhttps:\n//smartmedic.com/index.php. Accessed: 2025-07-14.\nSohn, J.; Park, Y.; Yoon, C.; Park, S.; Hwang, H.; Sung, M.;\nKim, H.; and Kang, J. 2024.\nRationale-Guided Retrieval\nAugmented Generation for Medical Question Answering.\narXiv preprint arXiv:2411.00300.\nTeam, G.; Anil, R.; Borgeaud, S.; Alayrac, J.-B.; Yu, J.; Sori-\ncut, R.; Schalkwyk, J.; Dai, A. M.; Hauth, A.; Millican, K.;\net al. 2023. Gemini: a family of highly capable multimodal\nmodels. arXiv preprint arXiv:2312.11805.\nTeam,\nQ.\n2025.\nQwen3\nTechnical\nReport.\narXiv:2505.09388.\nThe EMS Professor. 2025. The EMS Professor YouTube\nChannel. https://www.youtube.com/@theemsprofessor. Ac-\ncessed: 2025-07-14.\nUnion Test Prep. 2025.\nUnion Test Prep EMT Practice.\nhttps://uniontestprep.com/emt-test/practice-test. Accessed:\n2025-07-14.\nWeerasinghe, K.; Janapati, S.; Ge, X.; Kim, S.; Iyer, S.;\nStankovic, J. A.; and Alemzadeh, H. 2024. Real-Time Mul-\ntimodal Cognitive Assistant for Emergency Medical Ser-\nvices. In 2024 IEEE/ACM Ninth International Conference\non Internet-of-Things Design and Implementation (IoTDI),\n85–96. IEEE.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V.; Zhou, D.; et al. 2022.\nChain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in neural information processing systems, 35:\n24824–24837.\nWiley. 2025. Emergency Medical Services: Clinical Practice\nand Systems Oversight. https://onlinelibrary.wiley.com/doi/\nbook/10.1002/9781118990810. Accessed: 2025-07-14.\nXiong, G.; Jin, Q.; Lu, Z.; and Zhang, A. 2024a. Bench-\nmarking Retrieval-Augmented Generation for Medicine. In\nKu, L.-W.; Martins, A.; and Srikumar, V., eds., Findings of\nthe Association for Computational Linguistics ACL 2024,\n6233–6251. Bangkok, Thailand and virtual meeting: Asso-\nciation for Computational Linguistics.\nXiong, G.; Jin, Q.; Wang, X.; Zhang, M.; Lu, Z.; and Zhang,\nA. 2024b.\nImproving retrieval-augmented generation in\nmedicine with iterative follow-up questions. In Biocomput-\ning 2025: Proceedings of the Pacific Symposium, 199–214.\nWorld Scientific.\nZheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.;\nZhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot arena.\nAdvances in neural information processing systems, 36:\n46595–46623.\nAppendix\nA.1\nIntroduction\nThis is the technical appendix for the paper ”Expert-Guided\nPrompting and Retrieval-Augmented Generation for Emer-\ngency Medical Service Question Answering”.\n• A.2 EMSQA Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .11\n– A.2.1 EMSQA Data Collection . . . . . . . . . . . . . . . . . . 11\n– A.2.2 EMSQA Data Statistics . . . . . . . . . . . . . . . . . . . 11\n• A.3 Knowledge Base Preprocessing & Statistics . . 11\n– A.3.1 Prompt for Organizing Chapter Text . . . . . . . 11\n– A.3.2 Prompt for EMS Concept Extraction . . . . . . . 11\n– A.3.3 UMLS Concept Normalization . . . . . . . . . . . . .11\n– A.3.4 Knowledge and EMSQA Overlap Statistics . 11\n– A.3.5 Knowledge Base Collection Details . . . . . . . . 11\n"}, {"page": 11, "text": "• A.4 NEMSIS Patient Care Report Preprocessing &\nStatistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n– A.4.1 NEMSIS Patient Record Preprocessing . . . . . 12\n– A.4.2 NEMSIS Patient Record Statistics . . . . . . . . . . 14\n• A.5 Human Evaluation of EMSQA & Knowledge\nBase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15\n• A.6 Retrieval Strategy Performance . . . . . . . . . . . . . 15\n• A.7 Benchmark LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n– A.7.1 Zero-shot Performance per Subject Area . . . .16\n– A.7.2 Detailed statistics of Benchmarks . . . . . . . . . . 16\n• A.8 Ablation Study on Certification Level and Subject\nArea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n• A.9 Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n– A.9.1 Error Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n– A.9.2 Error per Subject Area and Certification . . . .18\nA.2\nEMSQA Data\nA.2.1\nEMSQA Data Collection\nIn Table 7, we present detailed information on 17 public and\nprivate EMSQA resources, including each source’s name\nand URL, number of questions, certification levels, and the\navailability of explanations.\nA.2.2\nEMSQA Data Statistics\nIn Table 8, we show the detailed EMSQA dataset statis-\ntics. The dataset includes a total of 18,602 and 5,669 prac-\ntice questions from public and private sources, respectively.\nWe use the private questions exclusively for testing and split\nthe public questions into train, validation, and test sets of\n13,021, 1,860, 3,721 questions, with average token lengths\nof 18.27, 19.12, 18.99, respectively.\nA.3\nKnowledge Base Preprocessing &\nStatistics\nA.3.1\nPrompt for Organizing Chapter Text\nAs shown in Figure 5, we present the prompt that instructs\nthe GPT-4o to divide each chapter’s raw text into clearly la-\nbeled sections.\nA.3.2\nPrompt for EMS Concept Extraction\nIn main paper Table 3: Semantic and syntactic evaluation\nof QA overlap vs. KB/PR, we present the concept over-\nlap in Syntactic metrics, we present the prompt we used for\nEMS concept extraction in Figure 6.\nA.3.3\nUMLS Concept Normalization\nWe applied UMLS API (Bodenreider 2004) to normalize the\nextracted EMS concepts and retained only terms whose se-\nmantic types fell into one of the following categories: Sign\nor Symptom (Sosy), Finding (Fndg), Laboratory or Test Re-\nsult (Lbtr), Clinical Attribute (Clna), Quantitative Concept\n(Qnco), Qualitative Concept (Qlco), Disease or Syndrome\nPROMPT:\nWell formated the unstructured text by the folllow-\ning rules:\n1. Fix awkward or broken line breaks within sen-\ntences or paragraphs.\n2. Separate paragraphs with a single blank line.\n3. Remove figure captions and references (e.g., “Fig-\nure 8–1”, “see Figure...”).\n4. Remove page numbers (e.g., “239”).\n5. Remove attribution or copyright lines, such as: de-\nfined copyrights\n6. **Do not rephrase, reword, rewrite, or summa-\nrize; do not add any other words in your response**.\n7. If the unstructured text contains subtitles, treat\neach subtitle as a key and store its corresponding\nparagraph(s) as the value. Return only the resulting\nJSON.\nHere is the unstructured text to format: RAW TEXT\nFigure 5: Prompt for organizing chapter text\n(Dsyn), Mental or Behavioral Dysfunction (Mobd), Patho-\nlogic Function (Patf), Neoplastic Process (Neop), Congen-\nital Abnormality (Cgab), Anatomical Abnormality (Anab),\nInjury or Poisoning (Inpo), Cell or Molecular Dysfunction\n(Celf), Body Part/Organ/Organ Component (Bpoc), Body\nLocation or Region (Bodl), Body Space or Junction (Bsoj),\nBody System (Bodsys), Therapeutic or Preventive Proce-\ndure (Topp), Diagnostic Procedure (Diap), Laboratory Pro-\ncedure (Lbpr), Imaging Procedure (Impr), Health Care Ac-\ntivity (Hlca), Clinical Drug (Clnd), Pharmacologic Sub-\nstance (Phsu), Antibiotic (Antb), Vitamin (Vita), Organic\nChemical (Orch), Amino Acid/Peptide/Protein (Aapp), Bio-\nlogically Active Substance (Bacs), Hazardous or Poisonous\nSubstance (Hops), Steroid (Strd), Hormone (Horm), Medi-\ncal Device (Medd), Temporal Concept (Tmco), Spatial Con-\ncept (Spco), and Functional Concept (Fngp).\nA.3.4\nKnowledge and EMSQA Overlap Statistics\nThe detailed statistics such as the total number of concepts\nin EMSQA and KB, overlapped concepts (KB ∩QA), union\nconcepts (KB ∪QA), KB unique concepts (KB \\ QA) are\nshown in Table 9. The syntactic (hit rate) reported in paper\nis calculated by KB ∩QA / QA.\nA.3.5\nKnowledge Base Collection Details\nTable 10 shows the detailed information of every knowledge\nsource we collected to construct external EMS knowledge\nbases. Specifically, we list the resource type, title, URL,\ncertification levels, and vocabulary of each source. In total,\nthere are 2,545,192 tokens in our external knowledge bases,\nand the unique vocabulary size is 34,110.\n"}, {"page": 12, "text": "Source\n# QAs\nCertification Level(s)\nExplanation\nPublic\n(Bluefield Rescue 2025)\n955\nEMT\n✓\n(NREMT Practice Test 2025)\n240\nEMT\n✓\n(SmartMedic 2025)\n537\nEMT\n✗\n(Union Test Prep 2025)\n150\nEMT\n✓\n(Montgomery County MD 2025)\n130\nEMT\n✓\n(Mometrix Academy 2025)\n138\nEMT, AEMT, Paramedic\n✓\n(PracticeTestGeeks 2025)\n99\nEMR\n✓\n(My CPR Certification Online 2025)\n74\nEMR, EMT, AEMT\n✗\n(Quizizz 2025)\n1,554\nEMR, EMT, AEMT, Paramedic\n✗\n(MinniQuiz 2025)\n4,444\nEMR, EMT, Paramedic\n✗\n(Quizlet 2025)\n9,084\nEMR, EMT, AEMT, Paramedic, NA\n✗\n(CareerEmployer 2025)\n100\nAEMT\n✓\n(DocDrop / Arthur Hsieh 2025)\n456\nEMT\n✓\n(LearningExpress Hub 2025)\n1,336\nParamedic, EMT\n✓\n(PocketPrep 2025)\n47\nEMR, EMT, AEMT, Paramedic\n✓\nPrivate\n(EMT-Prep 2025)\n4,843\nEMR, EMT, AEMT, Paramedic, Critical Care\n✓\n(Jones & Bartlett Learning 2025)\n1,285\nEMT\n✓\nTable 7: Summary of NREMT practice question sources in EMSQA.\nData\nSplit\n#Explanations #Choices\n(avg/max)\n#Answers\n(avg/max)\nQuestion Tokens\n(avg/max)\nChoice Tokens\n(avg/max)\nTokens Vocab\nPublic\nTrain (13,021)\n2217\n4.01 / 7.00 1.00 / 3.00\n18.27 / 218\n6.28 / 240\n565,303 14,017\nVal (1,860)\n383\n3.99 / 5.00 1.00 / 3.00\n19.12 / 155\n6.01 / 44\n80,215\n6,629\nTest (3,721)\n773\n4.01 / 6.00 1.00 / 3.00\n18.99 / 135\n6.10 / 60\n161,464\n8,913\nTotal (18,602)\n3132\n4.01 / 7.00 1.00 / 3.00\n18.50 / 218\n6.22 / 240\n806,982 16,032\nPrivate Test (5,669)\n5451\n4.01 / 6.00 1.06 / 4.00\n30.44 / 355\n5.46 / 47\n296,673 10,637\nTable 8: Statistics by split for Public and Private EMSQA\nData\nComparison\nQA\nKB\nPR\nKB ∩QA PR ∩QA KB \\ QA PR \\ QA KB ∪QA PR ∪QA\nPublic\nVocab\n15,892 33,965 6,773\n13,183\n3,359\n20,782\n3,414\n36,674\n19,306\nCpts (w/o norm) 25,594 61,003 9,383\n10,661\n2,269\n50,342\n7,114\n75,936\n32,708\nCpts (w norm)\n17,262 34,627 8,088\n10,927\n2,637\n23,700\n5,451\n40,962\n22,713\nPrivate\nVocab\n10,496 33,965 6,773\n9,540\n2,966\n24,425\n3,807\n34,921\n14,303\nCpts (w/o norm) 11,621 61,003 9,383\n6,180\n1,669\n54,823\n7,714\n66,444\n19,335\nCpts (w norm)\n8,689\n34,627 8,088\n6,299\n1,969\n28,328\n6,119\n37,017\n14,808\nTable 9: Detailed overlap statistics between QA and KB for public and private data.\nA.4\nNEMSIS Patient Care Report\nPreprocessing & Statistics\nAccess to NEMSIS 2021 Public-Release Research Dataset\nrequires submitting a request on the NEMSIS website1.\nA.4.1\nNEMSIS Patient Record Preprocessing\nTo concatenate information for each patient record in the\nNEMSIS dataset, we used the following fields in different\n1https://nemsis.org/using-ems-data/request-research-data/\ntables and concatenated them by the shared primary key.\nThe keys in our patient records include ”Date - Time of\nSymptom Onset”, ”Chief Complaints”, ”Possible Injury”,\n”Cause of Injury”, ”Chief Complaint Anatomic Location”,\n”Chief Complaint Organ System”, ”Gender”, ”Age”, ”Age\nUnit”, ”Level of Responsiveness (AVPU)”, ”Primary Symp-\ntoms”, ”Other Associated Symptoms”, ”Primary Impres-\nsions”, ”Secondary Impressions”, ”Protocol Age Category”,\n”Protocols”, ”Date - Time Vital Signs Taken”, ”ECG Type”,\n”SBP (Systolic Blood Pressure)”, ”Heart Rate”, ”Respira-\ntory Rate”, ”Pulse Oximetry”, ”Blood Glucose Level”, ”End\n"}, {"page": 13, "text": "Resource Type and Title\nLevel\n# Vocab\nTranscripts\nEmergency Care and Transportation of\nthe Sick and Injured Advantage Package (Carrie Davis 2025)\nEMT\n23,836\nNancy Caroline’s Emergency Care in\nthe Streets (Carrie Davis 2025)\nParamedic\nAAOS Advanced Emergency Medical\nTechnician (AEMT) 4th Ed (The EMS Professor 2025)\nAEMT\nAAOS Critical Care Transport Paramedic (The EMS Professor 2025)\nParamedic\nTextbooks\nEMT Exam for Dummies (DocDrop / Arthur Hsieh 2025)\nEMT\n2,025\nEmergency Medical Services: Clinical\nPractice and Systems Oversight (Wiley 2025)\n—\n18,799\nEMS Essentials: A Resident’s Guide\nto Prehospital Care(EMRA 2025)\n—\n4,123\nEmergency Medical Response - RedCross (American Red Cross 2025)\nEMR\n10,096\nEmergency Medical Responder: Your First\nResponse in Emergency Care (Firearms Training Los Angeles CA 2025)\nEMR\n7,794\nGuidelines\nProfessional Responder Cheat Sheet (MediPro First Aid 2025)\nEMR\n1,569\nEMS Pharmacology Reference Guide (Rhode Island Department of Health 2025) —\n3,072\nParamedic Medication Manual (Delaware Health and Social Services 2025)\nParamedic\n2,011\nODEMSA Regional EMS Documents (ODEMSA 2025)\n—\n6,340\nSlides\nEmergency Care and Transportation of\nthe Sick and Injured Advantage Package (Jones & Bartlett Learning 2025)\nEMT\n8,597\nFlashcards & Study Guides\nEMS Study Guides (EMT-Prep 2025)\nEMR, EMT, AEMT, Paramedic\n8,373\nTable 10: Knowledge base collection details\nTidal Carbon Dioxide (ETCO2)”, ”Glasgow Coma Score-\nEye”, ”Glasgow Coma Score-Verbal”, ”Glasgow Coma\nScore-Motor”, ”Pain Scale Score”, ”Stroke Scale Score”,\n”Stroke Scale Type”, ”Reperfusion Checklist”, ”Date - Time\nMedication Administered”, ”Medication Administered Prior\nto this Unit’s EMS Care”, ”Medication Given”, ”Medication\nDosage”, ”Medication Dosage Units”, ”Response to Medi-\ncation”, ”Role - Type of Person Administering Medication”,\n”Date - Time Procedure Performed”, ”Procedure”, ”Number\nof Procedure Attempts”, ”Response to Procedure”, ”Role -\nType of Person Performing the Procedure”, ”Alcohol - Drug\nUse Indicators”, ”Barriers to Patient Care”, ”Cardiac Ar-\nrest”, ”Date - Time of Cardiac Arrest”, ”First Monitored\nArrest Rhythm of the Patient”, ”Cardiac Arrest Etiology”,\n”Type of CPR Provided”, ”Cardiac Rhythm on Arrival at\nDestination”, ”Reason CPR - Resuscitation Discontinued”,\n”Incident - Patient Disposition”, ”EMS Transport Method”,\n”Transport Mode from Scene”, ”Initial Patient Acuity”, ”Fi-\nnal Patient Acuity”.\nWe further classify patient records to seven subject areas\nby the field protocol EMS. Specifically,\n”Assessment” includes ”General-Universal Patient Care/\nInitial Patient Contact”, ”General-Individualized Patient\nProtocol”\nMedical\n&\nOB\nincludes\n”Medical-Seizure”,\n”Medical-Nausea/Vomiting”,\n”Medical-Influenza-Like\nIllness/\nUpper\nRespiratory\nInfection”,\n”Medical-\nAbdominal\nPain”,\n”Medical-Altered\nMental\nStatus”,\n”OB/GYN-Pregnancy Related Emergencies”, ”Medical-\nSupraventricular\nTachycardia\n(Including\nAtrial\nFib-\nrillation)”,\n”Medical-Cardiac\nChest\nPain”,\n”Medical-\nTachycardia”, ”Medical-Syncope”, ”Medical-Stroke/TIA”,\n”Medical-Respiratory\nDistress/Asthma/COPD/Reactive\nAirway”,\n”Medical-Respiratory\nDistress-Bronchiolitis”,\n”Medical-Diarrhea”,\n”Medical-Hypoglycemia/Diabetic\nEmergency”,\n”Medical-Hyperglycemia”,\n”Medical-\nAllergic Reaction/Anaphylaxis”, ”Medical-Hypertension”,\n”Medical-Bradycardia”,\n”Medical-Pulmonary\nEde-\nma/CHF”, ”Medical-Hypotension/Shock (Non-Trauma)”,\n”Medical-Opioid\nPoisoning/Overdose”,\n”Medical-ST-\nElevation\nMyocardial\nInfarction\n(STEMI)”,\n”Medical-\nVentricular Tachycardia (With Pulse)”, ”Medical-Stimulant\nPoisoning/Overdose”,\n”Medical-Respiratory\nDistress-\nCroup”, ”Medical-Adrenal Insufficiency”, ”Medical-Beta\nBlocker Poisoning/Overdose”, ”Medical-Calcium Channel\nBlocker\nPoisoning/Overdose”,\n”OB/GYN-Gynecologic\nEmergencies”,\n”OB/GYN-Childbirth/Labor/Delivery”,\n”OB/GYN-Eclampsia”,\n”OB/GYN-Post-partum\nHem-\n"}, {"page": 14, "text": "PROMPT:\nExtract all EMS concepts from the following text.\nHere are some examples of EMS concepts: gelastic\nepilepsy, visual seizure, pallor, pale color, aox4,\npare down, cut down\nReturn all the extracted EMS concepts as a low-\nercase letter in strict JSON format, like: [”fever”,\n”cardiac arrest”]\nHere is one example:\nText: Early symptoms include cough, wheezing,\nshortness of breath. Lung transplantation is an\noption. Response: Let’s think step by step,\nStep1: label the tokens one by one ”EMS concept”,\nor ”none”.\n-Early: none\n-symptoms: none\n-include: none\n-cough: EMS concept\n-wheezing: EMS concept\n-shortness: EMS concept\n-of: none\n-breath: EMS concept\n-Lung: EMS concept\n-transplantation: EMS concept\n-is: none\n-an: none\n-option: none\nStep2: RefineEMS concept from Step 1 by follow-\ning criteria,\n1.concatenate EMS concept spans\n2.remove extra irrelevant words in EMS concept\n-cough: EMS concept\n-wheezing: EMS concept\n-shortness of breath: EMS concept\n-Lung transplantation: EMS concept\nStep3: Return the your result: [”cough”, wheezing”,\n”shortness of breath”, ”Lung transplantation”]\nNow is the real text: RAW TEXT\nFigure 6: Prompt for EMS Concept Extraction\norrhage”,\n”General-Overdose/Poisoning/Toxic\nInges-\ntion”,\n”General-Fever”,\n”General-Epistaxis”,\n”General-\nBack\nPain”,\n”General-Pain\nControl”,\n”Environmental-\nAltitude\nSickness”,\n”Environmental-Cold\nExposure”,\n”Environmental-Hypothermia”,\n”Environmental-Heat\nStroke/Hyperthermia”, ”Environmental-Heat Exposure/Ex-\nhaustion”\n”Airway”\nincludes\n”Airway”,\n”Airway-Failed”,\n”Airway-Sedation\nAssisted\n(Non-Paralytic)”,\n”Airway-\nRapid\nSequence\nInduction\n(RSI-Paralytic)”,\n”Airway-\nObstruction/Foreign Body”,\n”EMS\nOperations”\nincludes\n”Exposure-\nAirway/Inhalation Irritants”, ”General-Neglect or Abuse\nSuspected”, ”Exposure-Explosive/ Blast Injury”, ”General-\nIV\nAccess”,\n”General-Refusal\nof\nCare”,\n”General-\nBehavioral/Patient\nRestraint”,\n”General-Interfacility\nTransfers”,\n”General-Spinal\nImmobilization/Clearance”,\n”General-Medical\nDevice\nMalfunction”,\n”General-Law\nEnforcement - Assist with Law Enforcement Activity”,\n”General-Extended Care Guidelines”, ”General-Exception\nProtocol”, ”General-Indwelling Medical Devices/Equip-\nment”, ”General-Community Paramedicine / Mobile Inte-\ngrated Healthcare”, ”General-Law Enforcement - Blood for\nLegal Purposes”, ”General-Dental Problems”, ”Exposure-\nBiological/Infectious”,\n”Exposure-Carbon\nMonoxide””,\n”Exposure-Chemicals to Eye”, ”Exposure-Smoke Inhala-\ntion”, ”Exposure-Cyanide”, ”Exposure-Radiologic Agents”,\n”Exposure-Blistering Agents”, ”Exposure-Nerve Agents”\n”Cardiovascular” includes ”General-Cardiac Arrest”,\n”Cardiac Arrest-Asystole”, ”Cardiac Arrest-Determination\nof Death / Withholding Resuscitative Efforts”, ”Cardiac\nArrest-Pulseless Electrical Activity”, ”Cardiac Arrest-Do\nNot Resuscitate”, ”Cardiac Arrest-Ventricular Fibrillation/\nPulseless Ventricular Tachycardia”, ”Cardiac Arrest-Post\nResuscitation Care”, ”Cardiac Arrest-Special Resuscitation\nOrders”, ”Cardiac Arrest-Hypothermia-Therapeutic”\n”Pediatrics” includes ”Medical-Apparent Life Threaten-\ning Event (ALTE)”, ”Medical-Newborn/ Neonatal Resusci-\ntation”\n”Trauma”\nincludes\n”Environmental-Frostbite/Cold\nInjury”,\n”Injury-Head”,\n”Injury-Extremity”,\n”Injury-\nBurns-Thermal”, ”Injury-General Trauma Management”,\n”Injury-Multisystem”, ”Injury-Eye”, ”Injury-Mass/Multiple\nCasualties”, ”Injury-Amputation”, ”Injury-Spinal Cord”,\n”Injury-Conducted\nElectrical\nWeapon\n(e.g.,\nTaser)”,\n”Injury-Bleeding/\nHemorrhage\nControl”,\n”Injury-Bites\nand\nEnvenomations-Land”,\n”Injury-Facial\nTrauma”,\n”Injury-Cardiac\nArrest”,\n”Injury-Crush\nSyndrome”,\n”Injury-Thoracic”,\n”Injury-Drowning/Near\nDrowning”,\n”Injury-Diving\nEmergencies”,\n”Injury-Electrical\nIn-\njuries”, ”Injury-Topical Chemical Burn”, ”Injury-Impaled\nObject”,\n”Injury-Bites\nand\nEnvenomations-Marine”,\n”Injury-Lightning/Lightning\nStrike”,\n”Injury-SCUBA\nInjury/Accidents”.\nNEMSIS Patient Records\nTotal cases\n4,003,430\nTotal tokens\n1,247,811,207\nAverage tokens/case\n311.7\nMin tokens\n202\nMax tokens\n824\nVocabulary\n6,838\nTable 11: Statistics for the NEMSIS patient care reports\nA.4.2\nNEMSIS Patient Record Statistics\nThe detailed statistics of processed NEMSIS data is shown\nat Table 11. There are in total 4,003,430 patient records with\n"}, {"page": 15, "text": "Dimension\nScale Expert Rating\nClinical Accuracy\n0-100\n100\nDifficulty Level\n1-5\n3.95\nSubject Area Accuracy 1-5\n4.85\nKnowledge Relevance\n1-5\n4.39\nTable 12: Human Expert evaluation on EMSQA & KB.\naverage 311.7 tokens per case. The total number vocabulary\nis 6,838.\nWe also show the statistics such as the total number of\nconcepts in EMSQA and PR, overlapped concepts (PR ∩\nQA), union concepts (PR ∪QA), PR unique concepts (PR \\\nQA) are shown in Table 9. The syntactic (hit rate) reported\nin paper is calculated by PR ∩QA / QA.\nA.5\nHuman Evaluation of EMSQA and\nKnowledge Base\nTo validate the quality and clinical utility of our EMSQA\nbenchmark and its accompanying curated knowledge bases,\nwe conducted an expert review on a randomly sampled\nsubset of 100 questions. Sampling was stratified to ensure\nbalanced coverage across certification levels—25 questions\neach for EMR, EMT, AEMT, and Paramedic—and across\nfive major medical subject areas—20 questions each for Air-\nway, Trauma, Cardiology, Medical, and Operations—while\nalso drawing uniformly from all source links.\nWe asked one EMT-certified professional to rate each\nquestion–answer pair on four dimensions:\nClinical Accuracy: Verify that the provided “correct” an-\nswer is clinically sound. Rating: Yes / No\nDifficulty Level: Judge whether the question’s difficulty\nmatches the expected level for its certification. Rating:\n1 (Strongly Disagree) to 5 (Strongly Agree)\nSubject Area Accuracy: Confirm that the assigned medi-\ncal subject area appropriately reflects the question’s con-\ntent. Rating: 1 (Strongly Disagree) to 5 (Strongly Agree)\nKnowledge Relevance: Assess\nwhether\nthe\nassembled\nknowledge base sufficiently supports answering the\nquestion. Rating: 1 (Strongly Disagree) to 5 (Strongly\nAgree)\nTable 12 shows average rating for four dimensions. The\nresults indicate that: (1) provided answers are clinically ac-\ncurate; (2) the EMT expert largely agrees with the assigned\ncertification levels and subject areas; and (3) the curated\nknowledge base is helpful for answering the questions.\nA.6\nRetrieval Strategy Performance\nIn the paper we present three different retrieval strategies\n(Global, Filter then Retrieve (FTR), Retrieve then Filter\n(RTF)). To evaluate the retrieval performance, we uniformly\nsampled 100 questions from the 4 different certification lev-\nels (25 questions per certification level) and 10 subject areas\n(∼10 questions per subject area). For each question, we use\nMedCPT to retrieve 32 documents from Knowledge Bases.\nSince we do not have the ground-truth question-document\npairs, we use LLM-as-a-judge (Zheng et al. 2023) to eval-\nuate the “Relevance” (Es et al. 2024; Asai et al. 2023)\nand “Supportiveness” (Asai et al. 2023) of the 32 retrieved\ndocuments for every question. Relevance judges if each re-\ntrieved chunk is pertinent to the question, while supportive-\nness judges if this retrieved chunk helps answer the question.\nAs shown in Figure 7, we present the prompt template we\nused in LLM-as-a-judge. To evaluate “Relevance” and “Sup-\nportiveness” we use four Information Retrieval metrics: Top-\nk Hit rate (hit@k), which measures the fraction of questions\nfor which at least one positive chunk appears in the top-k\nretrieved; Top-k Precision (p@k), which computes the aver-\nage proportion of positive chunks among the top-k for each\nquestion; Mean Reciprocal Rank (MRR), which takes the\nreciprocal of the rank position of the first positive chunk for\neach question and then averages these reciprocals—thereby\nreflecting how early in the ranking the first positive appears;\nand Mean Average Precision (MAP), which for each ques-\ntion computes the average of precisions at every positive\nchunk’s position and then averages those per-question APs,\ncapturing both the ranking quality and distribution of posi-\ntives over the entire list. We omit Top-k Recall (r@k) since\nwe lack ground-truth documents.\nFor each question q in sampled questions Q and retrieved\nchunk i, we define:\nrrel\nq,i =\n\u001a1\nif chunk i is Relevant,\n0\notherwise,\n(7)\nrsup\nq,i =\n\u001a1\nif chunk i is Fully or Partially support,\n0\notherwise.\n(8)\nThen, for either {rrel\nq,i} or {rsup\nq,i }:\nPROMPT:\nQuestion: {}\nAnswer: {}\nRetrieved Chunk: {}\nRubric\n- Relevance\n• ”Relevant” — The chunk provides information that\nis useful for answering the question.\n• ”Irrelevant” — The chunk is off-topic or supplies\nno helpful information.\n- Supportiveness\nEvaluate how much of a hypothetical answer could\nbe *entailed* by this chunk alone.\n• ”Fully” — All necessary information is present;\nthe answer would be completely supported.\n• ”Partially” — Some but not all required informa-\ntion is present.\n• ”None” — No information is supported, or the\nchunk contradicts the answer.\nFigure 7: Prompt for LLM-as-a-judge\n"}, {"page": 16, "text": "Relevance\nStrategy\nHit@k\nP@k\nMRR MAP\n(1 / 5 / 10)\n(1 / 5 / 10)\nGlobal\n44.0 / 69.0 / 78.0 44.0 / 35.0 / 31.7\n55.6\n39.4\nFTR\n44.0 / 71.0 / 80.0 44.0 / 35.0 / 32.2\n57.6\n39.6\nRTF\n45.0 / 75.0 / 81.0 45.0 / 35.0 / 32.5\n57.5\n39.6\nSupportiveness\nStrategy\nHit@k\nP@k\nMRR MAP\n(1 / 5 / 10)\n(1 / 5 / 10)\nGlobal\n25.0 / 52.0 / 67.0 25.0 / 21.0 / 20.3\n38.3\n27.6\nFTR\n27.0 / 59.0 / 70.0 27.0 / 24.0 / 21.2\n39.7\n29.2\nRTF\n29.0 / 58.0 / 70.0 29.0 / 23.0 / 21.6\n39.8\n28.8\nTable 13: Retrieval Relevance and Supportiveness\nhit@k =\n1\n|Q|\nX\nq∈Q\n1\n\u0010 k\nX\ni=1\nrq,i > 0\n\u0011\n(9)\np@k =\n1\n|Q|\nX\nq∈Q\n1\nk\nk\nX\ni=1\nrq,i\n(10)\nMRR =\n1\n|Q|\nX\nq∈Q\n1\nmin{ i : rq,i = 1}\n(11)\nMAP =\n1\n|Q|\nX\nq∈Q\n1\nPN\ni=1 rq,i\nN\nX\ni=1\nrq,i\n1\ni\ni\nX\nj=1\nrq,j .\n(12)\nTable 13 presents the performance of three retrieval strate-\ngies,Global (direct retrieval), Retrieve-then-Filter (RTF) and\nFilter-then-Retrieve (FTR), in terms of both “Relevance”\nand “Supportiveness”. In terms of “Relevance”, RTF and\nFTR show slightly better performance than Global across\nall metrics, indicating that the proposed strategies retrieve\nrelevant chunks earlier and more reliably. In terms of “Sup-\nportiveness”, both RTF and FTR achieve higher Hit@k and\nP@k and slightly better MRR and MAP than Global, sug-\ngesting they more effectively capture fully or partially sup-\nportive evidence. Overall, although based on a small study,\nthese results show that the proposed retrieval strategies im-\nprove the extraction of both relevant and supportive context\nfrom the knowledge base, providing stronger grounding for\nthe LLM’s final answer. A more comprehensive evaluation\nof retrieval strategies is beyond the scope of this paper and\nleft for future work.\nA.7\nBenchmark LLMs\nA.7.1\nZero-shot Performance per Subject Area\nAs shown in Figure 8, we show the zero-shot LLMs’ per\nsubject area performance on private dataset. We only run\nopen-source models on our private dataset. The conclusion\nremains consistent on the private dataset: LLMs excel in\npharmacology and anatomy but falter on core NREMT\ndomains. Models answer pharmacology and anatomy items\nreliably, yet stumble on “trauma”, “airway management”,\n“EMS operations”, and “medical&OB” questions. One rea-\nson may be task complexity: the former are largely single-\nhop, fact-based queries, whereas the latter demand multi-\nhop reasoning and richer EMS context.\nFigure 8: Zero-shot Performance per Subject Area on\nEMSQA-Private Dataset\nA.7.2\nDetailed statistics of Benchmarks\nTable 14 summarizes the full benchmarking results for each\nmodel and prompting strategy, reporting accuracy and F1\nacross all four EMS certification levels (EMR, EMT, AEMT,\nParamedic) as well as overall performance. Below are sev-\neral key findings:\nClosed-source models outperform open-source mod-\nels. In particular, OpenAI-o3 consistently achieves the high-\nest overall accuracy of 92.39. Among open-source models,\nQwen3-32B achieves the best accuracy of 85.70, though a\nsignificant gap remains compared to closed-source models.\nFew-shot prompting improves accuracy up to a point.\nWe varied the number of in-context exemplars from 0 to 64\nand observed that incorporating a small number of examples\nyields substantial gain over the zero-shot baseline. However,\nadding examples beyond a certain point leads to diminishing\nor no improvement.\nExpert-CoT helps guide LLM reasoning. Integrating\ndomain expert knowledge via CoT-Expert guides reason-\ning towards appropriate context and consistently boosts CoT\nprompting performance by up to 2.05% across models. Also,\nusing the Filter’s predicted expertise for Expert-CoT leads to\ncomparable performance to when using ground-truth.\nA.8\nAblation Study on Certification Level\nand Subject Area\nTo determine which expertise attribute contributes the most\nto performance, we ablate the two key attributes, “Subject\nArea” and “Certification”, and measure their individual ef-\nfects on Expert-CoT. We cannot do the ablation study on\nExpert-RAG since it only uses the subject area for retrieval.\nThe results are presented in Table 15.\nExpertise attributes are most effective when used to-\ngether. Individually, “Subject Area” and “Certification” im-\nprove performance in Expert-CoT between 0.74-4.91% in\n"}, {"page": 17, "text": "EMSQA—Public\nModel\nPrompt\nEMR\nEMT\nAEMT\nParamedic\nOverall\nAcc.\nF1\nAcc.\nF1\nAcc.\nF1\nAcc.\nF1\nAcc.\nF1\nQwen3-32B\n0-shot\n79.08\n79.08\n82.98\n82.98\n83.68\n83.68\n86.69\n86.69\n83.55\n83.55\n4-shot\n80.89\n80.89\n84.31\n84.31\n86.05\n86.05\n86.06\n86.06\n84.41\n84.41\n8-shot\n80.20\n80.20\n84.60\n84.60\n85.79\n85.79\n86.14\n86.14\n84.39\n84.39\n16-shot\n81.03\n81.03\n84.23\n84.23\n85.79\n85.79\n87.31\n87.31\n84.82\n84.82\n32-shot\n77.68\n77.68\n81.13\n81.13\n81.58\n81.58\n83.01\n83.01\n81.13\n81.13\n64-shot\n80.33\n80.33\n80.09\n80.09\n85.26\n85.26\n85.43\n85.43\n82.48\n82.48\nCoT\n80.06\n80.06\n82.90\n82.95\n85.26\n85.26\n86.92\n86.92\n84.96\n84.97\n(GT)\nExpert-CoT\n82.43\n82.48\n85.49\n85.49\n87.89\n87.89\n87.16\n87.16\n85.70\n85.71\n(Filter)\nExpert-CoT\n81.52\n81.51\n82.24\n83.68\n86.37\n86.65\n86.98\n87.51\n85.07\n85.10\nLlama-3.3\n0-shot\n79.08\n79.08\n81.05\n81.05\n85.53\n85.53\n85.67\n85.67\n81.69\n82.69\nCoT\n79.64\n81.29\n79.57\n80.79\n83.42\n84.51\n85.20\n86.13\n81.89\n83.08\n(GT)\nExpert-CoT\n79.22\n80.32\n80.68\n81.77\n83.68\n85.15\n85.75\n86.24\n82.42\n83.35\n(Filter)\nExpert-CoT\n80.20\n81.24\n80.24\n81.16\n82.63\n83.51\n85.83\n86.27\n82.40\n83.18\nOpenBioLLM\n0-shot\n51.60\n51.73\n54.18\n54.26\n61.58\n61.71\n63.66\n63.74\n57.67\n57.76\nCoT\n53.02\n53.03\n54.78\n54.96\n62.34\n62.56\n63.81\n63.97\n59.88\n60.34\n(GT)\nExpert-CoT\n55.31\n55.89\n57.82\n57.87\n64.66\n65.03\n65.77\n65.81\n61.92\n62.03\n(Filter)\nExpert-CoT\n54.91\n54.91\n55.12\n55.12\n65.16\n65.17\n65.52\n65.54\n61.32\n61.93\nOpenAI-o3\n0-shot\n90.79\n90.79\n93.49\n93.49\n92.37\n92.37\n92.17\n92.17\n92.39\n92.39\nGemini-2.5\n0-shot\n87.59\n87.59\n89.93\n89.93\n90.26\n90.26\n89.51\n89.51\n89.36\n89.36\nEMSQA—Private\nQwen3-32B\n0-shot\n84.81\n84.93\n84.55\n85.28\n85.55\n86.47\n85.27\n86.09\n85.11\n85.89\n4-shot\n86.60\n86.72\n85.41\n85.95\n86.60\n87.30\n85.54\n86.24\n85.48\n86.13\n8-shot\n85.26\n85.39\n84.62\n85.20\n85.91\n86.64\n85.71\n86.42\n85.39\n86.07\n16-shot\n85.78\n85.95\n84.98\n85.55\n86.11\n86.84\n85.68\n86.41\n85.52\n86.19\n32-shot\n82.58\n82.95\n81.80\n82.84\n83.69\n85.12\n82.92\n84.26\n82.22\n83.41\n64-shot\n87.71\n87.90\n85.75\n86.74\n87.52\n88.89\n87.13\n88.19\n86.22\n87.26\nCoT\n91.81\n92.01\n87.86\n89.08\n90.11\n91.66\n89.83\n91.24\n88.78\n90.13\n(GT)\nExpert-CoT\n94.42\n94.74\n88.63\n89.85\n92.01\n93.58\n91.59\n92.83\n89.73\n90.98\n(Filter)\nExpert-CoT\n91.66\n92.97\n88.42\n90.05\n91.60\n93.08\n91.11\n92.60\n89.50\n91.20\nLlama-3.3\n0-shot\n75.95\n76.12\n77.93\n78.63\n76.87\n77.66\n76.75\n77.47\n78.06\n78.77\nCoT\n87.57\n88.16\n84.65\n85.86\n86.88\n88.04\n85.93\n87.00\n85.16\n86.35\n(GT)\nExpert-CoT\n89.72\n90.48\n85.90\n87.04\n88.66\n89.61\n87.55\n88.58\n86.49\n87.62\n(Filter)\nExpert-CoT\n90.17\n90.70\n86.06\n86.98\n88.41\n89.47\n87.64\n88.63\n86.63\n87.65\nOpenBioLLM\n0-shot\n60.98\n61.24\n60.96\n61.74\n66.25\n67.23\n67.32\n68.31\n63.86\n64.76\nCoT\n62.16\n62.73\n64.35\n64.89\n69.85\n70.71\n72.13\n72.42\n67.01\n67.77\n(GT)\nExpert-CoT\n63.60\n63.83\n65.01\n65.88\n70.87\n71.84\n73.69\n74.74\n68.75\n69.82\n(Filter)\nExpert-CoT\n62.26\n62.43\n64.31\n64.78\n70.01\n70.78\n72.01\n72.23\n67.79\n68.32\nTable 14: Zero-/few-shot, CoT and Expert-CoT results for LLMs on EMSQA.\nAcc, but combining both attributes yields the best improve-\nments of up to 5.29% in the 4B model. Besides, “Subject\nArea” helps slightly more than “Certification” in perfor-\nmance improvement.\nSmall language models benefit more from expertise guid-\nance. Injecting expertise labels yields only marginal im-\nprovements for the 32B LLM model, but leads to significant\ngains for the 4B model. This disparity suggests that larger\nmodels have likely internalized much of the relevant exper-\ntise during pre-training, whereas smaller models rely more\nheavily on explicit expertise guidance to steer their limited\ncapacity toward the correct reasoning trajectory.\nA.9\nError Analysis\nA.9.1\nError Types\nWe conducted an error analysis on our best 4B model\n(ExpertRAG-GT + Expert-CoT), which achieved 82.24%\naccuracy on EMSQA public split. From the 661 errors on\nthe test set, we randomly selected 50 samples and manu-\nally examined each question, its retrieved knowledge, and\nthe model’s reasoning process. As shown in Figure 9, we at-\ntribute the errors to five main categories:\n1) Reasoning Error (42.4%): Retrieved documents were\nrelevant and were used to answer the question, but the\nmodel’s reasoning was incorrect.\n2) Retrieval Error(20.3%): No relevant knowledge was re-\n"}, {"page": 18, "text": "Model\nDescription\nPublic\nPrivate\nAcc\nF1\nAcc\nF1\nQwen3-32B\nCoT\n84.96 84.97 88.78 90.13\n+ Subject Area 85.36 85.48 88.94 90.61\n+ Certification\n85.40 85.45 88.89 90.62\nExpert-CoT\n85.70 85.71 89.73 90.98\nQwen3-4B\nCoT\n72.35 73.09 70.58 72.02\n+ Subject Area 76.32 77.04 73.82 74.53\n+ Certification\n74.53 75.37 73.29 74.31\nExpert-CoT\n77.26 78.38 74.32 75.77\nTable 15: Ablation Study: Effect of Expertise (Certification,\nSubject Area) on Expert-CoT\nFigure 9: Error Analysis\ntrieved, causing the model to rely on internal “model knowl-\nedge”, which led to incorrect answer.\n3) Knowledge Use Failure (20.3%): Part of the retrieved\nknowledge was relevant to answer the question but the\nmodel used other irrelevant knowledge in its reasoning.\n4) Question Misinterpretation (15.3%): The model misun-\nderstood the question. For example, for the question what is\nthe age cutoff for using an AED on a patient? with choices:\na. 18 year old; b. 5 year old; c. 25 year old; d. 1 year old, the\nquestion refers to minimum patient age (Correct answer: d.\n1 year old) for routine AED use. However the model inter-\npreted it as the minimum age of the responder allowed to\noperate an AED and incorrectly selected (a. 18 year old).\n5) Model Policy Constraint (1.7%): The model determined\nthat none of the answer choices were valid and abstained\nfrom responding.\nA.9.2\nErrors per Subject Area and Certification\nTo investigate how errors are distributed across subject areas\nand certification levels, we compare the error rates of three\nmodels, (Qwen3-4B (0-shot), ExpertRAG-4B (Expert-CoT)\nand Qwen3-32B (0-shot)). The results are shown in Fig-\nure 10 and Figure 11. In subject areas, all three models make\nmost of their mistakes in the core NREMT domains: “air-\nway”, “EMS operations”, “medical–OB/GYN”, “trauma”,\nand “pediatrics”. Together, these categories account for the\nFigure 10: Error Rate per Subject Area\nFigure 11: Error Rate per Certification\nmajority of errors, while “anatomy”, “assessment”, “phar-\nmacology”, and “others” contribute relatively few. Across\ncertification levels, all models exhibit the highest error rates\nat the EMT level, followed by Paramedic and EMR, with\nAEMT contributing the fewest errors.\nExpertRAG-4B consistently reduces the absolute num-\nber of errors compared to the Qwen3-4B baseline across\nnearly all subject areas and certification levels. The Qwen3-\n32B has the fewest errors across three models. One find-\ning is although the total number of errors are reduced in\nExpertRAG-4B and Qwen3-32B models, the relative distri-\nbution of errors across subject areas is still the same for each\nmodel.\n"}]}