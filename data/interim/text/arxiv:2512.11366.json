{"doc_id": "arxiv:2512.11366", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.11366.pdf", "meta": {"doc_id": "arxiv:2512.11366", "source": "arxiv", "arxiv_id": "2512.11366", "title": "qa-FLoRA: Data-free query-adaptive Fusion of LoRAs for LLMs", "authors": ["Shreya Shukla", "Aditya Sriram", "Milinda Kuppur Narayanaswamy", "Hiteshi Jain"], "published": "2025-12-12T08:27:34Z", "updated": "2025-12-12T08:27:34Z", "summary": "The deployment of large language models for specialized tasks often requires domain-specific parameter-efficient finetuning through Low-Rank Adaptation (LoRA) modules. However, effectively fusing these adapters to handle complex, multi-domain composite queries remains a critical challenge. Existing LoRA fusion approaches either use static weights, which assign equal relevance to each participating LoRA, or require data-intensive supervised training for every possible LoRA combination to obtain respective optimal fusion weights. We propose qa-FLoRA, a novel query-adaptive data-and-training-free method for LoRA fusion that dynamically computes layer-level fusion weights by measuring distributional divergence between the base model and respective adapters. Our approach eliminates the need for composite training data or domain-representative samples, making it readily applicable to existing adapter collections. Extensive experiments across nine multilingual composite tasks spanning mathematics, coding, and medical domains, show that qa-FLoRA outperforms static fusion by ~5% with LLaMA-2 and ~6% with LLaMA-3, and the training-free baselines by ~7% with LLaMA-2 and ~10% with LLaMA-3, while significantly closing the gap with supervised baselines. Further, layer-level analysis of our fusion weights reveals interpretable fusion patterns, demonstrating the effectiveness of our approach for robust multi-domain adaptation.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.11366v1", "url_pdf": "https://arxiv.org/pdf/2512.11366.pdf", "meta_path": "data/raw/arxiv/meta/2512.11366.json", "sha256": "1e26e2b7219d39fe9e16165ccbd4e79afb08f9cd616eed226be57bd6c6725c51", "status": "ok", "fetched_at": "2026-02-18T02:24:22.563123+00:00"}, "pages": [{"page": 1, "text": "qa-FLoRA: Data-free query-adaptive Fusion of LoRAs for LLMs\nShreya Shukla, Aditya Sriram, Milinda Kuppur Narayanaswamy, Hiteshi Jain\nMercedes Benz Research and Development India\n{shreya.shukla, aditya.a.sriram, milinda.narayana_swamy, hiteshi.jain}@mercedes-benz.com\nAbstract\nThe deployment of large language models for specialized\ntasks often requires domain-specific parameter-efficient fine-\ntuning through Low-Rank Adaptation (LoRA) modules. How-\never, effectively fusing these adapters to handle complex,\nmulti-domain composite queries remains a critical challenge.\nExisting LoRA fusion approaches either use static weights,\nwhich assign equal relevance to each participating LoRA,\nor require data-intensive supervised training for every pos-\nsible LoRA combination to obtain respective optimal fu-\nsion weights. We propose qa-FLoRA, a novel query-adaptive\ndata-and-training-free method for LoRA fusion that dynami-\ncally computes layer-level fusion weights by measuring dis-\ntributional divergence between the base model and respec-\ntive adapters. Our approach eliminates the need for compos-\nite training data or domain-representative samples, making it\nreadily applicable to existing adapter collections. Extensive\nexperiments across nine multilingual composite tasks span-\nning mathematics, coding, and medical domains, show that\nqa-FLoRA outperforms static fusion by ‚àº5% with LLaMA-2\nand ‚àº6% with LLaMA-3, and the training-free baselines by\n‚àº7% with LLaMA-2 and ‚àº10% with LLaMA-3, while sig-\nnificantly closing the gap with supervised baselines. Further,\nlayer-level analysis of our fusion weights reveals interpretable\nfusion patterns, demonstrating the effectiveness of our ap-\nproach for robust multi-domain adaptation.\n1\nIntroduction\nLarge language models (LLMs) have demonstrated remark-\nable capabilities across a wide range of tasks, but their\ndeployment to unseen or specialized tasks often requires\ndomain-specific fine-tuning. However, standard full fine-\ntuning of an LLM is resource-intensive and can lead to catas-\ntrophic forgetting (Luo et al. 2023). Low-Rank Adaptation\n(LoRA) (Hu et al. 2022) has emerged as a parameter-efficient\nfine-tuning technique that uses a low-rank approximation of\nthe parameter update matrices to reduce the effective number\nof trainable parameters. LoRA‚Äôs low-rank updates effectively\nact as plug-and-play modules, i.e., once a LoRA adapter is\ntrained for a particular task, it can be loaded into the base\nLLM at inference time without modifying the original pa-\nrameters. Consequently, the same pre-trained base model can\nCopyright ¬© 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nMethod\nQuery\nAdaptive\nData\nrequired\nSupervised\ntraining\nPer\nlayer\nStatic Fusion\n‚úó\n‚úó\n‚úó\n‚úó\nLoraFlow (Wang et al. 2024)\n‚úì\n‚úì\n‚úì\n‚úì\nLoraHub (Huang et al. 2023)\n‚úì\n‚úì\n‚úì\n‚úó\nCentroid Sim. (Belofsky 2023)\n‚úì\n‚úì\n‚úó\n‚úó\nqa-FLoRA(Ours)\n‚úì\n‚úó\n‚úó\n‚úì\nTable 1: Comparison of existing LoRA fusion approaches.\n(‚ñ™) indicates an undesired trait, (‚ñ™) indicates a desired one.\nbe reused across multiple downstream tasks by simply swap-\nping in the appropriate LoRA modules. However, relying on\nindividual LoRA modules in isolation fundamentally limits\nthe model‚Äôs ability to handle complex or composite inputs\nthat span multiple domains or tasks. In such scenarios, train-\ning dedicated adapters for every possible task combination\nis impractical and does not scale well with the combinatorial\nexplosion of domains and tasks.\nThis challenge has motivated the growing body of research\non LoRA fusion, which aims to integrate multiple task-\nspecific adapters to enable robust inference across compos-\nite inputs spanning diverse domains. Early works relied on\nstatic merging (Liu 2024), which naively combines adapters\nwith fixed weights. This method does not account for the se-\nmantic relevance of domain experts to individual queries.\nMore recent supervised approaches adopt dynamic fusion\nschemes inspired by the Mixture-of-Experts(MoE) architec-\nture (Jiang et al. 2024), and train a routing network to pre-\ndict fusion weights (Wang et al. 2024; Xu, Lai, and Huang\n2024). While these dynamic fusion methods improve adapt-\nability to individual queries, they still require re-training the\nrouter for each new adapter or domain addition. Moreover,\nsuch training-based methods require a diverse collection of\ncomposite training data for all possible adapter combina-\ntions, thus creating a scalability bottleneck that limits their\napplicability to heterogeneous adapter collections.\nTo address the scalability limitations of training-based\napproaches, another line of work has explored training-\nfree LoRA fusion that bypasses the requirement of com-\nposite data for fusion weights optimization. These meth-\nods typically compute fusion weights by measuring cosine\nsimilarity between test queries and precomputed domain-\ncentroids for each adapter (Belofsky 2023; Chronopoulou\narXiv:2512.11366v1  [cs.CL]  12 Dec 2025\n"}, {"page": 2, "text": "et al. 2023). However, the effectiveness of such centroid-\nbased approaches is highly dependent on the quality and rep-\nresentativeness of the domain-specific data used to compute\ncentroids. Moreover, this method fails to capture the distri-\nbutional shifts that adapters induce at different layers of the\nLLM, and for domains with semantically similar represen-\ntations, centroids provide inadequate information, leading to\nsuboptimal fusion weights. Table 1 compares the pros and\ncons of the existing methods. These challenges highlight the\nneed for a more flexible and robust training-free method for\ndynamic fusion of LoRA adapters.\nIn this paper, we introduce qa-FLoRA, a novel data-\nand-training-free method that can dynamically determine\nlayer-level weights for query-adaptive fusion of LoRA\nmodules. Our approach is grounded in the following insight\n‚Äì examining how each adapter modifies the base model‚Äôs\npredictions reveals its relevance to the query. Specifically,\nwhen a LoRA adapter is semantically relevant to an input,\nit injects meaningful task-specific information that diverges\nfrom the base model‚Äôs representation in a measurable way.\nThis divergence serves as a proxy for semantic relevance,\nenabling dynamic weighing of adapters based on their\ncontribution to the query at hand. Notably, our proposed\napproach eliminates the need for composite training data\nor domain-specific representative samples as required by\nprevious approaches.\nThe key contributions of our work are threefold:\n1. We propose qa-FLoRA, a novel data-free and training-\nfree approach for query-adaptive LoRA Fusion that dy-\nnamically computes layer-level fusion weights based on\nthe semantic relevance of adapters to individual queries.\n2. We extensively compare our method with diverse base-\nlines across static, supervised, and training-free fusion\nparadigms. We demonstrate substantial improvements\nover static and training-free methods (by 5% and 7% with\nLLaMA-2-7B and by 6% and 10% with LLaMA-3-8B\nbase LLM respectively), while significantly closing the\nperformance gap with fully supervised methods.\n3. Through comprehensive evaluation across nine different\ncomposite tasks, we validate that our approach can ef-\nfectively combine diverse domain expertise without re-\nquiring additional training, making it readily applicable\nto existing LoRA adapter collections.\n2\nRelated Work\nParameter-Efficient Fine-Tuning (PEFT) of LLMs. With\nthe recent advances in PEFT techniques (Han et al. 2024;\nXu et al. 2023), LLMs are often domain-adapted by either\nupdating only a small subset of model parameters or adding\nlightweight task-specific trainable modules. Existing PEFT\nstrategies can broadly be classified into three: additive meth-\nods (Houlsby et al. 2019; He et al. 2021; Zhu et al. 2021;\nLei et al. 2023; Chen et al. 2023) that introduce new train-\nable modules, reparameterization methods (Hu et al. 2022;\nValipour et al. 2022; Zhang et al. 2023c,a; Hayou, Ghosh,\nand Yu 2024; Liu et al. 2024a) that express updates using\nlow-rank adaptation of the parameter update matrix, and se-\nlective methods (Guo, Rush, and Kim 2020; Zaken, Ravfogel,\nand Goldberg 2021; Sung, Nair, and Raffel 2021; He et al.\n2022; Das et al. 2023; Liao, Meng, and Monz 2023; Zhang\net al. 2023b) that fine-tune only chosen existing weights. In\nthis work, our focus is on reparameterization methods, par-\nticularly LoRA (Hu et al. 2022).\nLoRA Fusion for multi-task adaptation. LoRA fusion\ncombines multiple domain-experts (LoRA modules) to en-\nable robust inference across multi-domain composite inputs.\nThe simplest approach to combining multiple adapters is\nstatic LoRA fusion, which uses arithmetic operations (av-\neraging, weighted averaging, or task arithmetic) to merge\nadapters offline (Liu 2024). This method fails to adapt to\nthe varying semantic requirements of input queries, re-\nsulting in suboptimal performance. Existing methods for\ndynamic LoRA fusion predominantly rely on supervised\nlearning to train routing mechanisms (Zadouri et al. 2023;\nKong et al. 2024; Luo et al. 2024; Ma et al. 2024). Lo-\nraRetriever (Zhao et al. 2024) combines retrieval-based se-\nlection with composition strategies. LoRAMoE (Dou et al.\n2023) utilizes mixture-of-experts gating networks for token-\nlevel adapter selection. DLP-LoRA (Zhang and Li 2024)\nproposes lightweight plugins and dynamic merging strate-\ngies for multi-task scenarios. LoRA-Flow (Wang et al.\n2024) introduces progressive fusion with learnable gates,\nand MeteoRA (Xu, Lai, and Huang 2024) implements token-\nlevel gating for fine-grained control. Another work Lo-\nRAHub (Huang et al. 2023) employs gradient-free few-shot\noptimization to learn fusion weights in a non-parametric\nfashion. However, the above supervised methods require\ncomposite training data for all possible adapter combina-\ntions, to optimize fusion weights, which limits their general-\nizability to unseen task combinations. Existing training-free\napproaches (Belofsky 2023; Chronopoulou et al. 2023) rely\non cosine similarity between test queries and pre-computed\ncentroids of domain-specific data to select relevant adapters.\nHowever, there is still a dependency on domain-specific data\nfor centroid computation, and the per-layer distributional\nshifts are not taken into account. To address these limita-\ntions, we propose qa-FLoRA, a query-adaptive data-and-\ntraining-free LoRA-Fusion method that leverages divergence\nbetween the base model and adapter distributions to dynam-\nically identify the most semantically relevant adapters, with-\nout requiring additional parametric routing or few-shot data.\n3\nOur Approach\nIn this section, we present qa-FLoRA, a novel data-and-\ntraining-free approach for query-adaptive Fusion of LoRA\nmodules, that leverages the distributional divergence of\neach adapter with respect to the base LLM, to identify the\nsemantic relevance of adapters for each input query. Figure 1\nillustrates the overall framework of our proposed approach.\nProblem Formulation\nGiven a frozen large language model Óàπwith parame-\nters ùëäand a set of ùëòdomain-specific LoRA adapters\n{Óà≠1, Óà≠2, ‚Ä¶ , Óà≠ùëó, ‚Ä¶ , Óà≠ùëò}, each of which induces a low-\n"}, {"page": 3, "text": "Transformer Block\nSelf-Attention\nFeed-Forward\nQuery\nEmbed\nFusion weights computation module\nLM Head\nSoftmax\nN x\n.‚Ä¶\nWeighted \nSum\nx N\n(a)\n(b)\n: Layer-wise hidden states\n: Vocabulary probability distribution\nN    : # transformer layers\n: KL divergence for jth adapter\n: Fusion weight for jth adapter\nFigure 1: Proposed qa-FLoRA framework. For an input query, we (a) dynamically calculate the per-layer fusion weights by\nutilizing the KL divergence between base model and adapter vocabulary distributions, and (b) perform weighted combination\nof LoRA adapter outputs with the base model for every transformer layer of the LLM.\nrank update Œîùëäùëóto ùëä. For an input query ùëÑ, our objec-\ntive is to dynamically determine the per-layer fusion weights\n{ùõº(1)\nùëó, ùõº(2)\nùëó, ‚Ä¶ , ùõº(ùëô)\nùëó, ‚Ä¶ , ùõº(ùëÅ)\nùëó\n} for an adapter Óà≠ùëówhen com-\nputing the model predictions.\nTo achieve this, we (1) compute the layer-wise probability\ndistributions for both the base model and each LoRA adapter\nas described in section 3.1, (2) quantify the distributional\ndivergence between adapters and the base model to derive\nadapter fusion weights, as described in section 3.2 and (3)\nperform weighted LoRA fusion with the base model to com-\npute final predictions, as described in section 3.3.\n3.1\nLayer-level probability distribution\nThis stage involves extracting intermediate hidden-state\nrepresentations from both the base model and the adapters,\nand projecting their logits to vocabulary space to enable\nmeaningful distributional comparisons of the base model\nand the adapters.\nExtraction of layer-level hidden states.\nFor an input\nquery ùëÑ, we process it through the base LLM Óàπto obtain\nthe layer-wise hidden states as h(ùëô)\nÓàπ= ùëä(ùëô)h(ùëô‚àí1)\nÓàπ\n, where\nùëä(ùëô) denotes the weights of ùëôùë°‚Ñétransformer layer of the base\nLLM Óàπ. Similarly, we obtain the hidden states when pro-\ncessing the query ùëÑthrough each of the ùëòLoRA adapters as\nh(ùëô)\nÓà≠ùëó= h(ùëô)\nÓàπ+Œîùëä(ùëô)\nùëóh(ùëô‚àí1)\nÓà≠ùëó\n. Here, for ùëô=1, h(ùëô‚àí1)\nÓàπ\n= h(ùëô‚àí1)\nÓà≠ùëó\n= ùë•,\nwhere ùë•denotes the query embeddings. For brevity and\nconsistency, we talk about h(ùëô) and ùëä(ùëô) at the transformer\nblock level. The actual computations happen at linear-layer\nlevel for self-attention and feedforward networks within\neach transformer block.\nProjection onto vocabulary distribution.\nTo compute\nmeaningful divergences between layer-level representations,\nwe must first project each hidden state h(ùëô) onto the model‚Äôs\nvocabulary space. Notably, we reuse the pre-trained LM head\nparameterized by ùëäùêøùëÄto produce logits for every layer as\nz(ùëô)\nÓàπ= ùëäùêøùëÄh(ùëô)\nÓàπand z(ùëô)\nÓà≠ùëó= ùëäùêøùëÄh(ùëô)\nÓà≠ùëó. The LM head is orig-\ninally trained to process only the final-layer hidden states.\nHowever, similar to (Kavehzadeh et al. 2023; Varshney et al.\n2023), we empirically found that applying the same projec-\ntion to intermediate hidden-states yields well-calibrated log-\nits for divergence computation.\nFinally, we convert these logits into probability distribu-\ntion over the vocabulary by applying softmax normalization\nùëù(ùëô) =\nexp(z(ùëô)\nÓàπ)\n‚àëùëë\nùëö=1 exp(z(ùëô)\nÓàπ(ùëö))\n; ùëû(ùëô)\nùëó\n=\nexp(z(ùëô)\nÓà≠ùëó)\n‚àëùëë\nùëö=1 exp(z(ùëô)\nÓà≠ùëó(ùëö))\nto obtain ùëù(ùëô) and ùëû(ùëô)\nùëó\nwhich denote the probability distribu-\ntion of the outputs of layer ùëôfrom base LLM and ùëóùë°‚Ñéadapter\nrespectively. ùëëdenotes the dimensionality of the logits.\n3.2\nDistributional divergence and fusion weights\nHere, we quantify how the predictions of each LoRA adapter\ndiverge from the predictions of the base model. As shown in\nFigure 1(a), for each layer ùëô, we obtain the respective hid-\nden state probability distributions for the last token of the\nquery ùëÑ, and compute the Kullback Leibler (KL) divergence\nbetween the distribution of the base LLM ùëù(ùëô)[-1] and each\nadapter ùëû(ùëô)\nùëó[-1] as shown in equation 1.\nùëëùëñùë£(ùëô)\nùëó(ùëÑ, Óà≠ùëó) = ùê∑ùêæùêø(ùëù(ùëô)[-1]‚Äñùëû(ùëô)\nùëó[-1])\n(1)\n"}, {"page": 4, "text": "where:\nùê∑ùêæùêø(ùëù(ùëô)‚Äñùëû(ùëô)\nùëó) =\nùëë\n‚àë\nùëñ=1\nùëù(ùëô)\n(ùëñ) log\nùëù(ùëô)\n(ùëñ)\nùëû(ùëô)\nùëó(ùëñ)\n(2)\nùëëis the dimensionality of the probability distributions.\nIntuitively, for a given query, the KL divergence\nùê∑ùêæùêø(ùëù(ùëô)‚Äñùëû(ùëô)\nùëó) measures the information gain when using\nthe adapter distribution ùëûùëóinstead of the base model distri-\nbution ùëù, thus quantifying the semantic information injected\nby each LoRA adapter relative to the base model representa-\ntion. A higher KL divergence value indicates that the respec-\ntive adapter is contributing task-specific information that the\nbase model alone does not capture. Conversely, a lower KL\ndivergence implies that the adapter provides little additional\nsemantic value for the given query.\nOnce the KL divergence between the respective probabil-\nity distributions is computed, the LoRA fusion weights for\nadapter Óà≠ùëóat each transformer layer can be obtained as\nùõº(ùëô)\nùëó\n=\nùëëùëñùë£(ùëô)\nùëó\n‚àëùëò\nùëñ=1 ùëëùëñùë£(ùëô)\nùëñ\n3.3\nAdaptive LoRA fusion\nAs\nshown\nin\nFigure\n1(b),\nwe\nfuse\nthe\nLoRA\nadapters\nwith\nrespective\nper-layer\nfusion\nweights\n{ùõº(1)\n1 , ‚Ä¶ , ùõº(ùëÅ)\n1\n}, ‚Ä¶ , {ùõº(1)\nùëò, ‚Ä¶ , ùõº(ùëÅ)\nùëò\n}, and obtain the fi-\nnal model predictions as shown in equation 3.\nùëÇ= ùëÇÓàπ+ ŒîùëÇÓà≠ùëó= (ùëä+\nùëò\n‚àë\nùëó=1\nùõºùëóŒîùëäùëó)ùë•\n(3)\nThis per-layer adaptive fusion mechanism ensures that for\neach input query, the most semantically relevant adapters re-\nceive higher weights while the less relevant ones are natu-\nrally downweighted, enabling the model to dynamically and\neffectively combine diverse domain expertise for improved\nperformance across heterogeneous tasks, without requiring\nadditional training or optimization.\n4\nExperiments and Results\n4.1\nSetup\nOur experiments are constrained to LLaMA-2-7B (Touvron\net al. 2023) and LLaMA-3-8B (Grattafiori et al. 2024) base\nLLMs due to computational limitations. The base model\nparameters remain frozen throughout, with domain-specific\nadaptation performed exclusively through lightweight LoRA\nmodules. All inference experiments are conducted on V100\n32G GPUs, with LLaMA-3-8B inference performed in\nbfloat16 precision format for computational efficiency.\n4.2\nBaselines\nWe compare our approach with different baselines spanning\nthree fusion paradigms.\nStatic fusion is a naive baseline that assigns equal weigh-\ntage to each participating LoRA without considering the rel-\nevance of respective adapters to the query at hand. This ap-\nproach lacks query-adaptability and layer-level granularity.\nSupervised methods learn optimal fusion weights from\ncomposite data. LoRAFlow (Wang et al. 2024) trains a para-\nmetric router using composite examples per adapter com-\nbination to predict fusion weights. LoRAHub (Huang et al.\n2023) performs gradient-free optimization of fusion weights.\nAlthough effective, these methods heavily rely on training\ndata for different adapter combinations, thus lacking scala-\nbility and generalizability.\nTraining-free methods like (Belofsky 2023; Chronopoulou\net al. 2023) avoid supervised optimization of fusion weights\nby computing domain centroids from representative exam-\nples (subset of data used for training LoRA adapters), and\nassigning fusion weights based on respective cosine similar-\nities. Although unsupervised, these approaches still require\naccess to domain-representative data and do not capture the\nper-layer distributional shift introduced by adapters.\nData and Training free methods To the best of our knowl-\nedge, our method is the first under the data and training free\nparadigm. We differ from existing training-free methods by\n(i) eliminating dependence on representative examples en-\ntirely, and (ii) utilizing dynamic fusion weights per-layer.\n4.3\nDatasets\nOur objective is to investigate the effectiveness of differ-\nent LoRA fusion methods in handling challenging com-\nposite queries, where multi-domain expertise is intricately\namalgamated in a query, rather than appearing as sequential\ntasks (Xu, Lai, and Huang 2024). To this end, we draw inspi-\nration from the evaluation tasks used in LoRAFlow (Wang\net al. 2024). LoRAFlow evaluates fusion performance on six\ncomposite tasks combining three language adapters (Chi-\nnese, Russian, Spanish) with two domain adapters (Math,\nCode). To introduce more diversity in evaluation tasks, we\nextend their evaluation framework by introducing a Medical\ndomain adapter, thereby enabling evaluation on nine multi-\nlingual composite tasks that span mathematical reasoning,\ncode generation, and medical question answering, and re-\nquire combining linguistic and domain expertise. In this sec-\ntion, we provide details on the datasets used for (i) training\neach LoRA expert, (ii) fusion weight optimization in super-\nvised baselines, and (iii) our evaluation benchmarks.\nLoRA expert training.\nTo evaluate LoRA fusion perfor-\nmance, the six LoRA expert modules are trained as follows.\nThe (i) Chinese (zh), (ii) Russian (ru), and (iii) Spanish (es)\nlanguage experts are trained using the respective 52K con-\nversational examples from (Lai et al. 2023). The (iv) Math\nadapter is trained on 395K english mathematical reasoning\nproblems from the MetaMathQA dataset (Yu et al. 2023),\nthe (v) Code adapter employs 186K english code genera-\ntion problems from the MagiCoder dataset (Wei et al. 2023),\nand the (vi) Medical adapter is trained using 182K multiple-\nchoice medical question-answer pairs from the MedMCQA\ndataset (Pal et al. 2022).\nAll adapters except code are trained using a LoRA rank\nr=64 with scaling factor ùõº=16. Following (Wang et al.\n2024), the code LoRA is trained using a rank r=256. Each\nLoRA adapter is trained for 3 epochs with a cosine warmup\nscheduling, where the peak learning rate is 1e-4, and the\n"}, {"page": 5, "text": "Base LLM\nParadigm\nMethod\nMath (accuracy)\nCode (pass@1)\nMedical (accuracy)\nAvg across\n3 domains\nzh\nru\nes\nAvg\nzh\nru\nes\nAvg\nzh\nru\nes\nAvg\nLLaMA-2-7B\nStatic fusion\nAvg [0.5, 0.5]\n12.8\n10.4\n18.4\n13.9\n17.1\n17.7\n18.3\n17.7\n28.0\n33.0\n28.0\n29.7\n20.4\nSupervised\nLoRAFlow\n33.2\n37.6\n42.0\n37.6\n20.7\n23.8\n23.2\n22.6\n31.7\n35.3\n30.6\n32.5\n30.9\nLoRAHub\n20.8\n28.4\n36.8\n28.7\n19.5\n21.3\n20.1\n20.3\n30.5\n33.2\n26.7\n30.1\n26.4\nTraining free\nCentroid sim.\n8.4\n4.4\n17.6\n10.1\n21.7\n16.5\n18.3\n18.8\n32.4\n32.7\n17\n27.4\n18.8\nData & Training free\nqa-FLoRA (Ours)\n21.6\n21.6\n36.4\n26.5\n20.9\n16.5\n15.6\n17.7\n30.0\n39.0\n31.0\n33.3\n25.8\nLLaMA-3-8B\nStatic fusion\nAvg [0.5, 0.5]\n40.8\n45.2\n49.2\n45.1\n48.2\n23.8\n22.6\n31.5\n42.4\n40.0\n34.7\n39.0\n38.5\nSupervised\nLoRAFlow\n56.8\n60.4\n69.2\n62.1\n36.6\n28.7\n37.2\n34.2\n43.2\n39.3\n43.5\n42.0\n46.1\nTraining free\nCentroid sim.\n34.4\n41.6\n45.6\n40.5\n43.9\n28.7\n27.4\n33.3\n35.3\n22.7\n30.6\n29.5\n34.4\nData & Training free\nqa-FLoRA (Ours)\n50.4\n58.4\n66.0\n58.3\n48.2\n23.8\n31.1\n34.4\n39.6\n38.0\n42.2\n39.9\n44.2\nTable 2: Quantitative comparison of different fusion methods across nine composite tasks with LLaMA-2-7B and LLaMA-3-8B\nas base LLMs. Best results within the training-free paradigm are highlighted in bold.\nwarmup ratio is 0.04.\nData for supervised baselines.\nSupervised LoRA fusion\nmethods such as LoRAFlow (Wang et al. 2024) and Lo-\nRAHub (Huang et al. 2023) require training data for each\ntask to learn optimal fusion weights. Towards this, for math\nand code tasks, we utilize the translated datasets from (Wang\net al. 2024), which comprise 200 training examples for each\nof the six tasks. For medical tasks, we construct the training\ndatasets by translating 280 medical QA examples from (Pal\net al. 2022) into Chinese, Russian, and Spanish using GPT-4o\nwith subsequent human verification. We follow the default\ntraining configurations of LoRAFlow (Wang et al. 2024) and\nLoRAHub (Huang et al. 2023) and train them on two A100\n80G GPUs to benchmark their performance for our tasks.\nEvaluation benchmarks.\nTo evaluate the fusion perfor-\nmance of different baselines in math tasks, we use 250 test\nsamples from the MGSM dataset (Shi et al. 2022) which\nprovides grade-school multilingual mathematical reasoning\nproblems. For code tasks, we utilize 164 translated codes\nfrom the HumanEval dataset. For medical evaluation, we\ntranslate 150 test samples from the MedMCQA dataset (Pal\net al. 2022) using GPT-4o with human verification.\n4.4\nEvaluation Metrics\nWe employ different evaluation metrics for each domain. For\nmathematical reasoning tasks, we extract numerical answers\nfrom model outputs using regex-based postprocessing and\ncompute accuracy against ground truth answers. Code gen-\neration tasks are assessed using the pass@1 metric, which\nmeasures the percentage of problems in which the generated\ncode passes all test cases on the first attempt. Medical QA\ntasks use exact match scoring, where we evaluate whether the\nmodel‚Äôs selected option matches the ground-truth answer.\n4.5\nResults and Discussion\nQuantitative analysis.\nTable 2 presents a comprehensive\nquantitative comparison of different LoRA fusion methods\nutilizing LLaMA-2-7B and LLaMA-3-8B as base LLMs,\nacross nine composite tasks spanning mathematics, cod-\ning, and medical domains. Our proposed method qa-FLoRA\nsubstantially outperforms static and training-free baselines\nwhile significantly closing the gap with supervised baselines.\nCompared to the centroid similarity training-free base-\nline (Belofsky 2023; Chronopoulou et al. 2023), qa-FLoRA\ndemonstrates superior overall average performance, achiev-\ning an improvement of ‚àº7% with LLaMA-2 and ‚àº10%\nwith LLaMA-3 base LLM. This improvement is particularly\npronounced in the mathematics domain, where qa-FLoRA\noutperforms centroid approach by ‚àº16% with LLaMA-2\nand ‚àº18% with LLaMA-3. Similarly, the medical domain\nachieves an average improvement of ‚àº6% with LLaMA-2\nand ‚àº10% with LLaMA-3. However, in the coding domain,\nboth qa-FLoRA and centroid approach achieve compara-\nble performance. This domain-specific performance varia-\ntion can be explained as follows: Math and medical queries\nare language-heavy as illustrated in the second column of\nFigure 2. Centroid method overweighs language LoRA via\nlexical similarity, while qa-FLoRA overweighs task LoRA\nvia distributional divergence. Thus, the centroid method has\na lower performance in math and medical. Code queries on\nthe other hand have both language(zh/ru/es) dominance and\nprogramming keywords (refer to second column of Figure 2).\nThe lexical similarity measure in the centroid method causes\nhigher weights for task LoRA due to keywords and syntactic\nmatches. Thus, both methods produce similar fusion weights\nresulting in comparable performance.\nThe static fusion baseline, which naively employs equal\nweighing of adapters, achieves an overall average of 20.4%\nwith LLaMA-2 and 38.5% with LLaMA-3. In contrast, our\nmethod delivers an improvement of ‚àº5% with LLaMA-2 and\n‚àº6% with LLaMA-3. The consistent superior performance\nof qa-FLoRA over static fusion across all domains highlights\nthe importance of dynamic, query-aware fusion weights.\nSupervised fusion approaches LoRAFlow (Wang et al.\n2024) and LoRAHub (Huang et al. 2023) outperform our\ntraining-free method by 5.1% and 0.6% respectively with\nLLaMA-2 base LLM. Notably, with LLaMA-3, the perfor-\nmance gap with supervised LoRAFlow narrows significantly\nto just 1.9%, suggesting that our approach scales effectively\nwith more capable base models. Further, it is important to\nnote that these supervised methods require a training phase\n"}, {"page": 6, "text": "Response : \nÁèçÂ¶ÆÁâπÊØèÂ§©‰∏ã 16 È¢óËõãÔºåÊØèÂ§©ÂêÉ 3 È¢óÔºåÊØèÂ§©Áî® 4 È¢ó‰∏∫Ëá™Â∑±ÁöÑÊúãÂèãÂÅöÊùæ\nÈ•ºÔºåÂâ©‰∏ãÁöÑÈ∏≠ËõãÂ•πÊØèÂ§©ÊãøÂéªÂÜúË¥∏Â∏ÇÂú∫ÂçñÔºåÊØèÈ¢óÊñ∞È≤úÈ∏≠ËõãÂçñ 2 ÁæéÂÖÉ„ÄÇ\\n\nÊØèÂ§©Â•πÊãøÂéªÂçñÁöÑÈ∏≠ËõãÊï∞ÈáèÊòØ 16 - 3 - 4 = 9 È¢ó„ÄÇ\\nÊØèÂ§©Â•πÂú®ÂÜúË¥∏Â∏ÇÂú∫\nËµö 9 * 2 = 18 ÁæéÂÖÉ„ÄÇ\\nÊâÄ‰ª•ÔºåÁèçÂ¶ÆÁâπÊØèÂ§©Âú®ÂÜúË¥∏Â∏ÇÂú∫Ëµö 18 ÁæéÂÖÉ„ÄÇ\nTranslated : \nJanet lays 16 eggs a day, eats 3 eggs a day, uses 4 eggs a day \nto make muffins for her friends, and sells the remaining eggs at \nthe farmer's market every day, selling each fresh egg for $2. \n\\nThe number of eggs she sells every day is 16 - 3 - 4 = 9 eggs. \n\\nShe earns 9 * 2 = $18 at the farmer's market every day. \\nSo, \nJanet earns $18 a day at the farmer's market.\nQuestion : \nÁèçÂ¶ÆÁâπÁöÑÈ∏≠Â≠êÊØèÂ§©‰∏ã 16 È¢óËõã„ÄÇÂ•πÊØèÂ§©Êó©‰∏äÊó©È§êÊó∂ÂêÉ 3 È¢óÔºåÊØèÂ§©Áî® 4 \nÈ¢ó‰∏∫Ëá™Â∑±ÁöÑÊúãÂèãÂÅöÊùæÈ•º„ÄÇÂâ©‰∏ãÁöÑÈ∏≠ËõãÂ•πÊØèÂ§©ÊãøÂéªÂÜúË¥∏Â∏ÇÂú∫ÂçñÔºåÊØèÈ¢óÊñ∞\nÈ≤úÈ∏≠ËõãÂçñ 2 ÁæéÂÖÉ„ÄÇÂ•πÊØèÂ§©Âú®ÂÜúË¥∏Â∏ÇÂú∫ËµöÂ§öÂ∞ëÈí±Ôºü\nTranslated : \nJanet's duck lays 16 eggs every day. She eats 3 eggs for \nbreakfast every morning and uses 4 eggs to make muffins for her \nfriends every day. She sells the remaining eggs at the farmer's \nmarket every day for $2 per fresh egg. How much money does \nshe make at the farmer's market every day?\nQuestion : \nfrom typing import List, Tuple\ndef rolling_max(numbers: List[int]) -> List[int]:\n\"\"\"‰ªéÁªôÂÆöÁöÑÊï¥Êï∞ÂàóË°®‰∏≠ÔºåÁîüÊàêÂ∫èÂàó‰∏≠Âà∞ÊØè‰∏™Êó∂Âàª‰∏∫Ê≠¢ÊâæÂà∞ÁöÑ\nÊúÄÂ§ßÂÖÉÁ¥†ÁöÑÂàóË°®„ÄÇ\\n\\t>>> rolling_max([1, 2, 3, 2, 3, 4, 2])\\n\\t[1, 2, \n3, 3, 3, 4, 4]\\n\\t\"\"\"\nTranslated : \nfrom typing import List, Tuple\ndef rolling_max(numbers: List[int]) -> List[int]:\n\"\"\"From a given list of integers, generate a list of the \nmaximum element found at each time in the sequence. \\n\\t>>> \nrolling_max([1, 2, 3, 2, 3, 4, 2])\\n\\t[1, 2, 3, 3, 3, 4, 4]\\n\\t\"\"\"\nResponse : \ndef rolling_max(numbers: List[int]) -> List[int]:\n\"\"\"‰ªéÁªôÂÆöÁöÑÊï¥Êï∞ÂàóË°®‰∏≠ÔºåÁîüÊàêÂ∫èÂàó‰∏≠Âà∞ÊØè‰∏™Êó∂Âàª‰∏∫Ê≠¢ÊâæÂà∞ÁöÑ\nÊúÄÂ§ßÂÖÉÁ¥†ÁöÑÂàóË°®„ÄÇ\\n\n>>> rolling_max([1, 2, 3, 2, 3, 4, 2])\\n\n[1, 2, 3, 3, 3, 4, 4]\\n\"\"\"\nmax_list = []\nfor i in range(len(numbers)):\n    \nmax_list.append(max(numbers[:i+1]))\nreturn max_list\nQuestion : \nÂú®‰ª•‰∏ãÂì™ÁßçÊÉÖÂÜµ‰∏ã‰∏çÂ∫î‰ΩøÁî®Âè£ËÖîÂ±èÈöúÔºö\\noptions\\nA. Âí¨ÊåáÁî≤ÁöÑ‰π†ÊÉØ\n\\nB. ËàåÊé®Âäõ\\nC. ÊâÅÊ°É‰ΩìÂíåËÖ∫Ê†∑‰ΩìÊÄ•ÊÄßÊÑüÊüì\\nD. ÂêÆÂê∏ÊãáÊåá\nTranslated : \nIn which of the following situations should an oral barrier not be \nused:\\noptions\\nA. Nail biting\\nB. Tongue thrust\\nC. Acute \ninfection of tonsils and adenoids\\nD. Thumb sucking\nResponse : \nC. ÊâÅÊ°É‰ΩìÂíåËÖ∫Ê†∑‰ΩìÊÄ•ÊÄßÊÑüÊüì\nTranslated : \nC. Acute infection of tonsils and adenoids\n(a)\n(b)\n(c)\nFigure 2: Layer-wise KL divergence analysis. In the first column, we visualize the layer-level variation in mean KL diver-\ngence values (averaged across all test queries and then normalized) with LLaMA-2-7B base LLM for 3 composite tasks: (a)\nChinese(zh)-math, (b) Chinese(zh)-code, and (c) Chinese(zh)-medical. The second and third columns show an example question-\nresponse pair (translations provided for understanding) for each of the three tasks.\nto optimize the fusion weights. In contrast, our proposed\nmethod operates in a training-free paradigm and even omits\nthe requirement of representative samples (as in the centroid-\nbased approach). qa-FLoRA‚Äôs ability to approach supervised\nperformance while maintaining the flexibility and efficiency\nof data-and-training-free operation represents a significant\npractical advantage, especially when quality fusion data is\nexpensive to obtain and repeated training for new adapter\ncombinations is cumbersome.\nQualitative analysis.\nTo gain deeper insights into the fu-\nsion behavior of our method, we conduct a layer-level diver-\ngence analysis that reveals how respective domain and lan-\nguage adapters contribute across different network depths.\nFigure 2 presents the KL divergence values of respective do-\nmain LoRAs and the Chinese language LoRA, averaged and\nnormalized across all test queries for three composite tasks.\nWe observe a consistent pattern in the initial trans-\nformer layers of the LLM, where KL divergence values ap-\nproach zero for both domain and language adapters. This\nphenomenon aligns with established findings that lower\ntransformer layers typically handle universal linguistic fea-\ntures (Liu et al. 2024b) that are well-captured during large-\nscale pre-training of the base LLM, requiring negligible task-\nspecific adaptation.\nIn the Chinese(zh)-math task (Figure 2a), the math LoRA\nexhibits consistently higher KL divergence values through-\nout the middle layers (layers 10-30), reflecting its dominant\nrole in foundational reasoning and arithmetic computations.\nHowever, there is a notable increase in the contribution from\nthe Chinese LoRA in the final layer. This can be attributed to\nthe generation phase: although the reasoning chain is math-\nematical, the final solution must be articulated in fluent Chi-\nnese with appropriate explanations and formatting. Thus, the\nlanguage adapter becomes crucial for producing coherent,\nlinguistically accurate responses that maintain mathematical\nprecision while adhering to Chinese linguistic conventions.\nIn the Chinese(zh)-code task (Figure 2b), we observe a\nslight dominance of Chinese LoRA in the middle layers(20-\n23). This phase corresponds to the interpretation stage,\nwhere the model must fully comprehend the algorithmic\nrequirements, constraints, and expected functionality de-\nscribed in Chinese (code comment). Following this interpre-\ntation phase, the code LoRA assumes dominant influence\nacross all subsequent layers, reflecting the transition from\nlanguage understanding to code synthesis. The generation\nprocess involves universal programming language constructs\n(keywords, operators, control structures) that are language-\nagnostic. Once the initial intent is decoded from the Chinese\ndescription, the subsequent generation process relies heavily\n"}, {"page": 7, "text": "Token Granularity\nMath\nCode\nMedical\nAvg across\n3 domains\nzh\nru\nes\nAvg\nzh\nru\nes\nAvg\nzh\nru\nes\nAvg\nFull query\n18.8\n18.4\n26.4\n21.2\n20.9\n18.0\n17.7\n18.9\n27.3\n36.0\n27.9\n30.4\n23.5\nLast token (Ours)\n21.6\n21.6\n36.4\n26.5\n20.9\n16.5\n15.6\n17.7\n30.0\n39.0\n31.0\n33.3\n25.8\nTable 3: Ablation Study to identify the optimal token-level granularity for best performance.\non the code adapter‚Äôs specialized knowledge of programming\npatterns, algorithmic structures, and syntax rules.\nThe Chinese(zh)-medical task (Figure 2c) demonstrates a\nmedical-dominant pattern, where the domain adapter main-\ntains higher KL divergence values, particularly evident\nacross the final layers. This pattern reflects the requirement\nfor precise terminology understanding in the middle layers\nand comprehensive medical domain knowledge to enable\nchoosing the correct option in the final layers. The persis-\ntent high contribution of medical LoRA throughout the net-\nwork depth indicates that medical reasoning requires con-\ntinuous access to specialized knowledge, including disease\npathophysiology, diagnostic criteria and treatment protocols.\nThe interpretability provided by these layer-level visu-\nalizations serves as both a theoretical validation of our\nmethod‚Äôs effectiveness and a diagnostic tool for understand-\ning fusion dynamics. This analysis suggests that an optimal\nfusion strategy must capture the layer-level dynamics of how\ndifferent expertise are required at different processing stages\nof a query. Appendix A also provides a similar layer-level\nanalysis for the remaining composite tasks.\nAblation Study: Optimal query tokens granularity for\nrelevance estimation.\nWe investigate the optimal token\ngranularity for KL divergence computation by comparing\ntwo approaches: averaging divergence across all query to-\nkens versus using only the last token‚Äôs divergence. Table 3\nshows that the latter approach outperforms all-token averag-\ning by ‚àº2%. This performance gap can be attributed to the\nautoregressive nature of transformer models, where the fi-\nnal token‚Äôs hidden state encapsulates the full sequential con-\ntext through self-attention mechanisms. Additionally, relying\non the last-token alone reduces computational overhead by\neliminating position-wise calculations, making it both effec-\ntive and efficient for adapter relevance estimation.\nAppendix A discusses another ablation study justifying\nthe choice of our divergence measure technique.\nLatency Analysis.\nTo evaluate the computational effi-\nciency of our approach, we measure the average latency for\n250 queries of the Chinese(zh)-math task using LLaMA-2-\n7B base LLM. The queries average 154 tokens in length. We\nperform all evaluations on V100 32G GPUs.\nThe inference process comprises two components: (a) fu-\nsion weight computation, which adds ‚àº192ms per query per\nadapter. This overhead stems from the forward passes re-\nquired to extract layer-level hidden states and their proba-\nbility distributions to compute KL divergences. Importantly,\nthis computation can be parallelized across adapters, en-\nabling substantial speedup. (b) generation time, which re-\nmains comparable to supervised LoRAFlow method.\nWhile qa-FLoRA introduces a negligible overhead for\nfusion-weight computation, it completely eliminates the\ntraining phase required by supervised methods. Thus, there\nis no need for composite data collection and fusion weights\noptimization for all possible adapter combinations. Our\ntraining-free paradigm computes fusion weights on-the-fly,\nmaking it readily applicable to new adapter collections and\nsubstantially more scalable as the number of adapters grow.\n5\nConclusion\nIn this work, we propose qa-FLoRA, a novel training-free\napproach for query-adaptive LoRA fusion that dynamically\nintegrates multiple domain-specific adapters. Our method\nleverages distributional divergence between adapter and base\nmodel representations at each layer, to quantify the seman-\ntic relevance of each adapter to the query, thereby enabling\nprincipled and interpretable fusion weight computation. Ex-\ntensive experimental evaluation across nine composite tasks\ndemonstrates that qa-FLoRA achieves substantial improve-\nments, outperforming static and training-free methods by\nlarge margins, while closing the gap with supervised fusion\napproaches that require additional training overhead. Over-\nall, our approach offers a scalable and effective solution for\ntraining-free adapter fusion, eliminating the need for addi-\ntional composite data, and setting a strong foundation for fu-\nture research in unsupervised adapter fusion techniques.\n6\nLimitations and Future Work\nOur evaluation is restricted to the LLaMA-2-7B and\nLLaMA-3-8B models due to computational constraints.\nWhile we demonstrate improvements across nine diverse\ncomposite tasks, future work could further validate our ap-\nproach with varied-scale LLMs (13B, 70B variants).\nDespite achieving substantial improvements over training-\nfree baselines, our method still exhibits a performance gap\ncompared to supervised fusion approaches, particularly in\ndomains requiring complex reasoning. Future research could\nexplore more sophisticated relevance measures beyond KL\ndivergence, while preserving the training-free paradigm.\nMoreover, investigating fusion strategies that can dynami-\ncally select between different relevance measures based on\nquery characteristics represents a promising avenue to close\nthe remaining performance gap with supervised methods.\nReferences\nBelofsky, J. 2023. Token-level adaptation of lora adapters for\ndownstream task generalization. In Proceedings of the 2023\n6th Artificial Intelligence and Cloud Computing Conference,\n168‚Äì172.\n"}, {"page": 8, "text": "Chen, Y.; Fu, Q.; Fan, G.; Du, L.; Lou, J.-G.; Han, S.;\nZhang, D.; Li, Z.; and Xiao, Y. 2023. Hadamard adapter:\nAn extreme parameter-efficient adapter tuning method for\npre-trained language models. In Proceedings of the 32nd\nACM International Conference on Information and Knowl-\nedge Management, 276‚Äì285.\nChronopoulou, A.; Peters, M. E.; Fraser, A.; and Dodge,\nJ. 2023.\nAdaptersoup: Weight averaging to improve gen-\neralization of pretrained language models. arXiv preprint\narXiv:2302.07027.\nDas, S. S. S.; Zhang, R. H.; Shi, P.; Yin, W.; and Zhang,\nR. 2023.\nUnified low-resource sequence labeling by\nsample-aware dynamic sparse finetuning.\narXiv preprint\narXiv:2311.03748.\nDou, S.; Zhou, E.; Liu, Y.; Gao, S.; Zhao, J.; Shen, W.; Zhou,\nY.; Xi, Z.; Wang, X.; Fan, X.; et al. 2023. LoRAMoE: Al-\nleviate world knowledge forgetting in large language models\nvia MoE-style plugin. arXiv preprint arXiv:2312.09979.\nGrattafiori, A.; Dubey, A.; Jauhri, A.; Pandey, A.; Kadian,\nA.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.;\nVaughan, A.; et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nGuo, D.; Rush, A. M.; and Kim, Y. 2020.\nParameter-\nefficient transfer learning with diff pruning. arXiv preprint\narXiv:2012.07463.\nHan, Z.; Gao, C.; Liu, J.; Zhang, J.; and Zhang, S. Q. 2024.\nParameter-efficient fine-tuning for large models: A compre-\nhensive survey. arXiv preprint arXiv:2403.14608.\nHayou, S.; Ghosh, N.; and Yu, B. 2024.\nLora+: Effi-\ncient low rank adaptation of large models. arXiv preprint\narXiv:2402.12354.\nHe, J.; Zhou, C.; Ma, X.; Berg-Kirkpatrick, T.; and Neubig,\nG. 2021. Towards a unified view of parameter-efficient trans-\nfer learning. arXiv preprint arXiv:2110.04366.\nHe, S.; Ding, L.; Dong, D.; Zhang, M.; and Tao, D.\n2022.\nSparseadapter: An easy approach for improv-\ning the parameter-efficiency of adapters.\narXiv preprint\narXiv:2210.04284.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;\nDe Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly,\nS. 2019. Parameter-efficient transfer learning for NLP. In\nInternational conference on machine learning, 2790‚Äì2799.\nPMLR.\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\nS.; Wang, L.; Chen, W.; et al. 2022. Lora: Low-rank adapta-\ntion of large language models. ICLR, 1(2): 3.\nHuang, C.; Liu, Q.; Lin, B. Y.; Pang, T.; Du, C.; and Lin, M.\n2023. Lorahub: Efficient cross-task generalization via dy-\nnamic lora composition. arXiv preprint arXiv:2307.13269.\nJiang, A. Q.; Sablayrolles, A.; Roux, A.; Mensch, A.; Savary,\nB.; Bamford, C.; Chaplot, D. S.; Casas, D. d. l.; Hanna, E. B.;\nBressand, F.; et al. 2024. Mixtral of experts. arXiv preprint\narXiv:2401.04088.\nKavehzadeh, P.; Valipour, M.; Tahaei, M.; Ghodsi, A.; Chen,\nB.; and Rezagholizadeh, M. 2023.\nSorted LLaMA: Un-\nlocking the potential of intermediate layers of large lan-\nguage models for dynamic inference.\narXiv preprint\narXiv:2309.08968.\nKong, R.; Li, Q.; Fang, X.; Feng, Q.; He, Q.; Dong, Y.; Wang,\nW.; Li, Y.; Kong, L.; and Liu, Y. 2024. LoRA-Switch: Boost-\ning the Efficiency of Dynamic LLM Adapters via System-\nAlgorithm Co-design. arXiv preprint arXiv:2405.17741.\nLai, V. D.; Van Nguyen, C.; Ngo, N. T.; Nguyen, T.; Der-\nnoncourt, F.; Rossi, R. A.; and Nguyen, T. H. 2023. Okapi:\nInstruction-tuned large language models in multiple lan-\nguages with reinforcement learning from human feedback.\narXiv preprint arXiv:2307.16039.\nLei, T.; Bai, J.; Brahma, S.; Ainslie, J.; Lee, K.; Zhou, Y.;\nDu, N.; Zhao, V.; Wu, Y.; Li, B.; et al. 2023. Conditional\nadapters: Parameter-efficient transfer learning with fast infer-\nence. Advances in Neural Information Processing Systems,\n36: 8152‚Äì8172.\nLiao, B.; Meng, Y.; and Monz, C. 2023. Parameter-efficient\nfine-tuning without introducing new latency. arXiv preprint\narXiv:2305.16742.\nLiu, S. 2024. Model merging.\nLiu, S.-Y.; Wang, C.-Y.; Yin, H.; Molchanov, P.; Wang, Y.-\nC. F.; Cheng, K.-T.; and Chen, M.-H. 2024a. Dora: Weight-\ndecomposed low-rank adaptation.\nIn Forty-first Interna-\ntional Conference on Machine Learning.\nLiu, Z.; Kong, C.; Liu, Y.; and Sun, M. 2024b. Fantastic\nsemantics and where to find them: Investigating which layers\nof generative llms reflect lexical semantics. arXiv preprint\narXiv:2403.01509.\nLuo, T.; Lei, J.; Lei, F.; Liu, W.; He, S.; Zhao, J.; and Liu,\nK. 2024. Moelora: Contrastive learning guided mixture of\nexperts on parameter-efficient fine-tuning for large language\nmodels. arXiv preprint arXiv:2402.12851.\nLuo, Y.; Yang, Z.; Meng, F.; Li, Y.; Zhou, J.; and Zhang,\nY. 2023. An empirical study of catastrophic forgetting in\nlarge language models during continual fine-tuning. arXiv\npreprint arXiv:2308.08747.\nMa, Y.; Liang, Z.; Dai, H.; Chen, B.; Gao, D.; Ran, Z.; Zihan,\nW.; Jin, L.; Jiang, W.; Zhang, G.; et al. 2024. MoDULA:\nMixture of Domain-Specific and Universal LoRA for Multi-\nTask Learning. arXiv preprint arXiv:2412.07405.\nPal, A.; et al. 2022.\nMedMCQA: A Large-scale Multi-\nSubject Multi-Choice Dataset for Medical domain Question\nAnswering. In Proceedings of the Conference on Health,\nInference, and Learning. PMLR.\nShi, F.; Suzgun, M.; Freitag, M.; Wang, X.; Srivats, S.;\nVosoughi, S.; Chung, H. W.; Tay, Y.; Ruder, S.; Zhou, D.;\net al. 2022.\nLanguage models are multilingual chain-of-\nthought reasoners. arXiv preprint arXiv:2210.03057.\nSung, Y.-L.; Nair, V.; and Raffel, C. A. 2021. Training neu-\nral networks with fixed sparse masks. Advances in Neural\nInformation Processing Systems, 34: 24193‚Äì24205.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned chat\nmodels. arXiv preprint arXiv:2307.09288.\n"}, {"page": 9, "text": "Valipour, M.; Rezagholizadeh, M.; Kobyzev, I.; and Gh-\nodsi, A. 2022.\nDylora: Parameter efficient tuning of pre-\ntrained models using dynamic search-free low-rank adapta-\ntion. arXiv preprint arXiv:2210.07558.\nVarshney, N.; Chatterjee, A.; Parmar, M.; and Baral, C. 2023.\nAccelerating llama inference by enabling intermediate layer\ndecoding via instruction tuning with lite.\narXiv preprint\narXiv:2310.18581.\nWang, H.; Ping, B.; Wang, S.; Han, X.; Chen, Y.; Liu, Z.;\nand Sun, M. 2024.\nLora-flow: Dynamic lora fusion for\nlarge language models in generative tasks. arXiv preprint\narXiv:2402.11455.\nWei, Y.; Wang, Z.; Liu, J.; Ding, Y.; and Zhang, L. 2023.\nMagicoder: Empowering code generation with oss-instruct.\narXiv preprint arXiv:2312.02120.\nXu, J.; Lai, J.; and Huang, Y. 2024. Meteora: Multiple-tasks\nembedded lora for large language models. arXiv preprint\narXiv:2405.13053.\nXu, L.; Xie, H.; Qin, S.-Z. J.; Tao, X.; and Wang, F. L.\n2023. Parameter-efficient fine-tuning methods for pretrained\nlanguage models: A critical review and assessment. arXiv\npreprint arXiv:2312.12148.\nYu, L.; Jiang, W.; Shi, H.; Yu, J.; Liu, Z.; Zhang, Y.; Kwok,\nJ. T.; Li, Z.; Weller, A.; and Liu, W. 2023. Metamath: Boot-\nstrap your own mathematical questions for large language\nmodels. arXiv preprint arXiv:2309.12284.\nZadouri, T.; √úst√ºn, A.; Ahmadian, A.; Ermi≈ü, B.; Locatelli,\nA.; and Hooker, S. 2023. Pushing mixture of experts to the\nlimit: Extremely parameter efficient moe for instruction tun-\ning. arXiv preprint arXiv:2309.05444.\nZaken, E. B.; Ravfogel, S.; and Goldberg, Y. 2021. Bitfit:\nSimple parameter-efficient fine-tuning for transformer-based\nmasked language-models. arXiv preprint arXiv:2106.10199.\nZhang, F.; Li, L.; Chen, J.; Jiang, Z.; Wang, B.; and Qian,\nY. 2023a.\nIncrelora: Incremental parameter allocation\nmethod for parameter-efficient fine-tuning. arXiv preprint\narXiv:2308.12043.\nZhang, M.; Chen, H.; Shen, C.; Yang, Z.; Ou, L.; Yu, X.; and\nZhuang, B. 2023b. LoRAPrune: Structured pruning meets\nlow-rank parameter-efficient fine-tuning.\narXiv preprint\narXiv:2305.18403.\nZhang, Q.; Chen, M.; Bukharin, A.; Karampatziakis, N.; He,\nP.; Cheng, Y.; Chen, W.; and Zhao, T. 2023c. Adalora: Adap-\ntive budget allocation for parameter-efficient fine-tuning.\narXiv preprint arXiv:2303.10512.\nZhang, Y.; and Li, R. 2024.\nDLP-LoRA: Efficient\nTask-Specific LoRA Fusion with a Dynamic, Lightweight\nPlugin for Large Language Models.\narXiv preprint\narXiv:2410.01497.\nZhao, Z.; Gan, L.; Wang, G.; Zhou, W.; Yang, H.; Kuang, K.;\nand Wu, F. 2024. Loraretriever: Input-aware lora retrieval\nand composition for mixed tasks in the wild. arXiv preprint\narXiv:2402.09997.\nZhu, Y.; Feng, J.; Zhao, C.; Wang, M.; and Li, L. 2021.\nCounter-interference adapter for multilingual machine trans-\nlation. arXiv preprint arXiv:2104.08154.\n"}, {"page": 10, "text": "Appendix\nA\nResults and Discussion\nA.1\nQualitative analysis of six composite tasks\nTo provide comprehensive insights into our fusion mecha-\nnism, we extend the layer-wise divergence analysis to the re-\nmaining six composite tasks involving Spanish and Russian\nlanguages. Figure 3 presents the KL divergence patterns for\nthese tasks, revealing both consistent trends and language-\nspecific variations in adapter contributions.\nConsistent with our analysis of Chinese tasks, all compos-\nite tasks exhibit zero KL divergence in initial layers, confirm-\ning that lower transformer layers capture universal linguistic\nfeatures requiring minimal task-specific adaptation.\nFor mathematical reasoning tasks (Figure 3 a,d), both\nSpanish and Russian variants show sustained math LoRA\ndominance throughout the middle and final layers, contrast-\ning with the Chinese task where the language adapter con-\ntribution increases in the final layer. This difference re-\nflects the linguistic proximity of Spanish and Russian to En-\nglish (the base model‚Äôs primary training language), requiring\nless language-specific adaptation for generating mathemati-\ncal explanations compared to Chinese.\nThe coding tasks (Figure 3 b,e) demonstrate similar\ninterpretation-to-synthesis transitions as observed with Chi-\nnese, where language adapters dominate during the problem\ncomprehension phase (layers 20-23) before the code adapter\nassumes control for universal programming construct gen-\neration. This consistency validates that our method captures\nthe fundamental cognitive processing stages.\nMedical tasks (Figure 3 c,f) maintain a domain-dominant\npattern throughout all six tasks, with medical LoRA exhibit-\ning high divergence values through the middle and final lay-\ners. This consistency reinforces that medical reasoning de-\nmands continuous access to specialized domain knowledge.\nThe interpretability provided by this layer-level analysis\nacross all nine composite tasks serves as both a theoretical\nvalidation of our method‚Äôs effectiveness and a diagnostic\ntool for understanding fusion dynamics.\nA.2\nAblation Study: Choice of divergence\nmeasure for adapter relevance estimation\nWe evaluate three distance metrics to quantify adapter rele-\nvance: KL divergence on vocabulary distributions (our ap-\nproach), cosine distance on hidden states (where we define\ncosine_distance = 1-cosine_similarity), and euclidean dis-\ntance on hidden states. As shown in table 4, our approach\nutilizing KL divergence yields the highest average perfor-\nmance across tasks, similar to cosine distance (25.8%) and\nsurpassing euclidean distance (24%).\nWhile geometric distances like cosine similarity and eu-\nclidean distance capture representational similarity by oper-\nating in hidden state space, KL divergence operates in proba-\nbility space directly reflecting the model‚Äôs predictive behav-\nior and confidence. This probabilistic formulation provides a\nmore principled adapter relevance estimation by measuring\ndistributional differences rather than geometric proximity.\n"}, {"page": 11, "text": "Divergence measure\nMath\nCode\nMedical\nAvg across\n3 domains\nzh\nru\nes\nAvg\nzh\nru\nes\nAvg\nzh\nru\nes\nAvg\nCosine distance\n20.4\n22.4\n36.0\n26.3\n21.7\n15.9\n17.1\n18.2\n29.0\n38.0\n32.0\n33.0\n25.8\nEuclidean distance\n18.4\n19.2\n27.2\n21.6\n21.7\n15.5\n17.7\n18.3\n31.0\n37.0\n28.0\n32.0\n24.0\nKL divergence (Ours)\n21.6\n21.6\n36.4\n26.5\n20.9\n16.5\n15.6\n17.7\n30.0\n39.0\n31.0\n33.3\n25.8\nTable 4: Ablation Study to quantify the impact of the choice of divergence measure for adapter relevance estimation.\n"}, {"page": 12, "text": "Response : \nSi Raymond tiene 40 joyas, la mitad de Raymond tiene 40/2 = 20 \njoyas.\\nAaron tiene 5 joyas m√°s que la mitad de Raymond, por \nlo que Aaron tiene 20 + 5 = 25 joyas.\\nSiobhan tiene 2 joyas \nmenos que Aaron, por lo que Siobhan tiene 25 - 2 = 23 \njoyas.\\n\\n#### 23\\n\\nThe answer is: 23\nTranslated : \nIf Raymond has 40 jewels, then Raymond's half has 40/2 = 20 \njewels.\\nAaron has 5 more jewels than Raymond's half, so Aaron \nhas 20 + 5 = 25 jewels.\\nSiobhan has 2 fewer jewels than Aaron, \nso Siobhan has 25 - 2 = 23 jewels.\\n\\n#### 23\\n\\nThe answer is: \n23\nQuestion : \nSiobhan tiene 2 joyas menos que Aaron. Aaron tiene 5 joyas \nm√°s que la mitad de joyas de Raymond. Si Raymond tiene 40 \njoyas, ¬øcu√°ntas joyas tiene Siobhan?\nTranslated : \nSiobhan has 2 fewer jewels than Aaron. Aaron has 5 more jewels \nthan half of Raymond's jewels. If Raymond has 40 jewels, how \nmany jewels does Siobhan have?\nQuestion : \ndef sum_to_n(n: int):\n            \"\"\"sum_to_n es una funci√≥n que suma los n√∫meros de 1 \na n.\\n\\t>>> sum_to_n(30)\\n\\t465\\n\\t>>> sum_to_n(100)\\n\\t \n5050\\n\\t>>> sum_to_n(5)\\n\\t15\\n\\t>>> sum_to_n(10)\\n\\t55\\n\\t \n>>> sum_to_n(1)\\n\\t1\\n\\t\"\"\"\nTranslated : \ndef sum_to_n(n: int):\n            \"\"\"sum_to_n is a function that adds numbers from 1 to \nn.\\n\\t>>> sum_to_n(30)\\n\\t465\\n\\t>>> sum_to_n(100)\\n\\t \n5050\\n\\t>>> sum_to_n(5)\\n\\t15\\n\\t>>> sum_to_n(10)\\n\\t55\\n\\t \n>>> sum_to_n(1)\\n\\t1\\n\\t\"\"\"\nResponse : \ndef sum_to_n(n: int):\n            \"\"\"sum_to_n es una funci√≥n que suma los n√∫meros de 1 \na n.\\n\\t>>> sum_to_n(30)\\n\\t465\\n\\t>>> sum_to_n(100)\\n    \n5050\\n\\t>>> sum_to_n(5)\\n\\t15\\n\\t>>> sum_to_n(10)\\n\\t55\\n    \n>>> sum_to_n(1)\\n\\t1\\n\\t\"\"\"\n            suma = 0\n            for i in range(1, n+1):\n                    suma += i\n            return suma\nQuestion : \nBarorreceptor es qu√© tipo de retroalimentaci√≥n?\\noptions\\nA. \nRetroalimentaci√≥n negativa\\nB. Retroalimentaci√≥n positiva\\nC. \nBoth negative and positive\\n\\nTanto negativo como positivo\\nD. \nControl de alimentaci√≥n directa\nTranslated : \nBaroreceptor is what type of feedback?\\noptions\\nA. Negative \nfeedback\\nB. \nPositive \nfeedback\\nC. \nBoth \nnegative \nand \npositive\\n\\nBoth negative and positive\\nD. Feedback control\nResponse : \nA. Retroalimentaci√≥n negativa\nTranslated : \nA. Negative feedback\n(a)\n(b)\n(c)\nResponse : \n–ï—Å–ª–∏ –≤ –°–∏—ç—Ç–ª–µ 20 –æ–≤–µ—Ü, —Ç–æ –≤ –ß–∞—Ä–ª—å—Å—Ç–æ–Ω–µ –≤ 4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ, \n–ø–æ—ç—Ç–æ–º—É –≤ –ß–∞—Ä–ª—å—Å—Ç–æ–Ω–µ –µ—Å—Ç—å 20 * 4 = 80 –æ–≤–µ—Ü.\\n–ï—Å–ª–∏ –≤ –¢—É–ª—É–∑–µ –≤ \n–¥–≤–∞ —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –æ–≤–µ—Ü, —á–µ–º –≤ –ß–∞—Ä–ª—å—Å—Ç–æ–Ω–µ, —Ç–æ –≤ –¢—É–ª—É–∑–µ –µ—Å—Ç—å 80 \n* 2 = 160 –æ–≤–µ—Ü.\\n–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤ –¢—É–ª—É–∑–µ, –ß–∞—Ä–ª—å—Å—Ç–æ–Ω–µ –∏ \n–°–∏—ç—Ç–ª–µ –≤–º–µ—Å—Ç–µ –≤–∑—è—Ç—ã—Ö, –µ—Å—Ç—å 160 + 80 + 20 = 260 –æ–≤–µ—Ü.\\n#### \n260\\n–ú–µ—Å—Ç–∞: 1\\n1. 260\nTranslated : \nIf Seattle has 20 sheep, then Charleston has 4 times as many, so \nCharleston has 20 * 4 = 80 sheep.\\nIf Toulouse has twice as \nmany sheep as Charleston, then Toulouse has 80 * 2 = 160 \nsheep.\\nSo Toulouse, Charleston, and Seattle combined have \n160 + 80 + 20 = 260 sheep.\\n#### 260\\nSeats: 1\\n1. 260\nQuestion : \n–ö–∞–π–ª–∞—Ä –ø–æ—à–µ–ª –≤ –º–∞–≥–∞–∑–∏–Ω, —á—Ç–æ–±—ã –∫—É–ø–∏—Ç—å —Å—Ç–∞–∫–∞–Ω—ã –¥–ª—è —Å–≤–æ–µ–π \n–Ω–æ–≤–æ–π –∫–≤–∞—Ä—Ç–∏—Ä—ã. –û–¥–∏–Ω —Å—Ç–∞–∫–∞–Ω —Å—Ç–æ–∏—Ç 5 $, –Ω–æ –∫–∞–∂–¥—ã–π –≤—Ç–æ—Ä–æ–π \n—Å—Ç–∞–∫–∞–Ω —Å—Ç–æ–∏—Ç –≤—Å–µ–≥–æ –ª–∏—à—å 60 % –æ—Ç —Ü–µ–Ω—ã. –ö–∞–π–ª–∞—Ä —Ö–æ—á–µ—Ç –∫—É–ø–∏—Ç—å \n16 —Å—Ç–∞–∫–∞–Ω–æ–≤. –°–∫–æ–ª—å–∫–æ –æ–Ω –¥–æ–ª–∂–µ–Ω –∑–∞ –Ω–∏—Ö –∑–∞–ø–ª–∞—Ç–∏—Ç—å?\nTranslated : \nKylar went to the store to buy glasses for his new apartment. \nOne glass costs $5, but every other glass costs only 60% of the \nprice. Kylar wants to buy 16 glasses. How much should he pay \nfor them?\nQuestion : \ndef greatest_common_divisor(a: int, b: int) -> int:\n            \"\"\" –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∏–±–æ–ª—å—à–∏–π –æ–±—â–∏–π –¥–µ–ª–∏—Ç–µ–ª—å –¥–≤—É—Ö \n—Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª a –∏ b\\n\\t>>> greatest_common_divisor(3, \n5)\\n\\t1\\n\\t>>> greatest_common_divisor(25, 15)\\n\\t5\\n\\t\"\"\"\nTranslated : \ndef greatest_common_divisor(a: int, b: int) -> int:\n            \"\"\" Returns the greatest common divisor of two integers a \nand b\\n\\t>>> greatest_common_divisor(3, 5)\\n\\t1\\n\\t>>> \ngreatest_common_divisor(25, 15)\\n\\t5\\n\\t\"\"\"\nResponse : \ndef greatest_common_divisor(a: int, b: int) -> int:\n            \"\"\" –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∏–±–æ–ª—å—à–∏–π –æ–±—â–∏–π –¥–µ–ª–∏—Ç–µ–ª—å –¥–≤—É—Ö \n—Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª a –∏ b\\n\\t>>> greatest_common_divisor(3, \n5)\\n\\t1\\n\\t>>> greatest_common_divisor(25, 15)\\n\\t5\\n\\t\"\"\"\n            while b != 0:\n                        a, b = b, a % b\n                        return a\nprint(greatest_common_divisor(3,5))\nprint(greatest_common_divisor(25, 15))\nQuestion : \nA 10-–ª–µ—Ç–Ω–∏–π —Ä–µ–±–µ–Ω–æ–∫ –∂–∞–ª—É–µ—Ç—Å—è –Ω–∞ –≥–æ–ª–æ–≤–Ω—É—é –±–æ–ª—å, —Ä–≤–æ—Ç—É, \n–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø–æ—Ö–æ–¥–∫–∏ –∏ –¥–∏–ø–ª–æ–ø–∏—é. –ü—Ä–∏ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ \n–≤—ã—è–≤–ª–µ–Ω—ã –æ—Ç–µ–∫ –¥–∏—Å–∫–∞ –∑—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –Ω–µ—Ä–≤–∞ –∏ –∞—Ç–∞–∫—Å–∏—è –ø–æ—Ö–æ–¥–∫–∏. \n–ù–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π –¥–∏–∞–≥–Ω–æ–∑ ‚Äì\\noptions\\nA. –ì–∏–¥—Ä–æ—Ü–µ—Ñ–∞–ª–∏—è\\n \nB. –û–ø—É—Ö–æ–ª—å —Å—Ç–≤–æ–ª–∞ –º–æ–∑–≥–∞\\nC. –°—É–ø—Ä–∞—Å–µ–ª–ª—è—Ä–Ω–∞—è –æ–ø—É—Ö–æ–ª—å\\nD. \n–û–ø—É—Ö–æ–ª—å —Å—Ä–µ–¥–Ω–µ–π –ª–∏–Ω–∏–∏ –∑–∞–¥–Ω–µ–π —á–µ—Ä–µ–ø–Ω–æ–π —è–º–∫–∏\nTranslated : \nA 10-year-old child presents with headache, vomiting, gait \ninstability, and diplopia. Examination reveals papilledema and \ngait \nataxia. \nThe \nmost \nlikely \ndiagnosis \nis\\noptions\\nA. \nHydrocephalus\\nB. Brainstem tumor\\nC. Suprasellar tumor\\nD. \nMidline tumor of the posterior cranial fossa\nResponse : \nD. –û–ø—É—Ö–æ–ª—å —Å—Ä–µ–¥–Ω–µ–π –ª–∏–Ω–∏–∏ –∑–∞–¥–Ω–µ–π —á–µ—Ä–µ–ø–Ω–æ–π —è–º–∫–∏\nTranslated : \nD. Midline tumor of the posterior cranial fossa\n(d)\n(e)\n(f)\nFigure 3: Layer-wise KL divergence analysis. In the first column, we visualize the layer-level variation in mean KL divergence\nvalues (averaged across all test queries and then normalized) with LLaMA-2-7B base LLM for 6 composite tasks (a) Spanish(es)-\nmath, (b) Spanish(es)-code, (c) Spanish(es)-medical, (d) Russian(ru)-math, (e) Russian(ru)-code, and (f) Russian(ru)-medical.\nThe second and third columns show an example question-response pair (translations provided for understanding) for each of the\nsix tasks.\n"}]}