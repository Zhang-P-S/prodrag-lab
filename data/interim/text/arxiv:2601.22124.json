{"doc_id": "arxiv:2601.22124", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.22124.pdf", "meta": {"doc_id": "arxiv:2601.22124", "source": "arxiv", "arxiv_id": "2601.22124", "title": "A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine", "authors": ["Anran Li", "Yuanyuan Chen", "Wenjun Long", "Yu Yin", "Yan Hu", "Hyunjae Kim", "Weipeng Zhou", "Yujia Zhou", "Hongyi Peng", "Yang Ren", "Xuguang Ai", "Zhenyue Qin", "Ming Hu", "Xiaoxiao Li", "Han Yu", "Yih-Chung Tham", "Lucila Ohno-Machado", "Hua Xu", "Qingyu Chen"], "published": "2026-01-29T18:48:21Z", "updated": "2026-01-29T18:48:21Z", "summary": "Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-efficient federated learning framework for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the framework to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. Accuracy was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.22124v1", "url_pdf": "https://arxiv.org/pdf/2601.22124.pdf", "meta_path": "data/raw/arxiv/meta/2601.22124.json", "sha256": "5d50f3647f39af8c39e1b07bc761a21d43465312c14c62069947c7c565553375", "status": "ok", "fetched_at": "2026-02-18T02:20:09.008065+00:00"}, "pages": [{"page": 1, "text": "Title \nA Federated and Parameter-Efficient Framework for Large Language \nModel Training in Medicine \nAuthor list \nAnran Li1, Yuanyuan Chen2, Wenjun Long2, Yu Yin3, Yan Hu4, Hyunjae Kim1, Weipeng Zhou1, \nYujia Zhou1, Hongyi Peng2, Yang Ren1, Xuguang Ai1, Zhenyue Qin1, Ming Hu5, Xiaoxiao Li6, Han \nYu2, Yih-Chung Tham7, Lucila Ohno-Machado1, Hua Xu1, Qingyu Chen1,*  \nCorresponding author: \n*Qingyu Chen, qingyu.chen@yale.edu \nAffiliations \n1. Department of Biomedical Informatics and Data Science, School of Medicine, Yale \nUniversity, New Haven, USA \n2. College of Computing and Data Science, Nanyang Technological University, Singapore  \n3. Department of Earth Science and Engineering, Imperial College London, London, United \nKingdom  \n4. McWilliams School of Biomedical Informatics, University of Texas Health Science at \nHouston, Houston, USA \n5. School of Computing and Information Systems, Singapore Management University, \nSingapore \n6. School of Electrical and Computer Engineering, University of British Columbia, Canada  \n7. Department of Ophthalmology, Yong Loo Lin School of Medicine, National University of \nSingapore, Singapore  \n \n \n"}, {"page": 2, "text": "1. Abstract \nLarge language models (LLMs) have demonstrated strong performance on standard medical \nbenchmarks, including patient question answering, summarization, and diagnosis. To enable \ntheir use in clinical settings, LLMs are typically further adapted‚Äîthrough continued pretraining \nor post-training‚Äîusing clinical data, thereby extending their applicability to medical tasks. \nHowever, most medical LLMs are trained on data from a single institution, as privacy and \nregulatory constraints limit cross-institutional data sharing. In practice, data from a single site \ncannot capture the substantial variability present in real-world healthcare, including differences \nin patient demographics, disease prevalence, and institutional documentation practices. \nConsequently, LLMs trained under this single-institution paradigm face critical limitations in \ngeneralizability and safety when deployed across heterogeneous healthcare systems. \nFederated learning (FL) is a promising solution for enabling collaborative model development \nacross healthcare institutions without compromising patient privacy. Yet applying FL to LLMs in \nmedicine remains fundamentally limited. First, conventional FL requires transmitting the full \nmodel during each communication round, which becomes impractical for multi-billion‚Äì\nparameter LLMs given the limited computational resources available in most clinical \nenvironments. Second, many FL algorithms implicitly assume data homogeneity, whereas real-\nworld clinical data are highly heterogeneous across patients, diseases, and institutional \npractices. \nTo address these limitations, we introduce Fed-MedLoRA and Fed-MedLoRA+, the first model-\nagnostic and parameter-efficient federated learning framework for adapting LLMs to medical \napplications. Fed-MedLoRA transmits only low-rank adapter parameters, substantially reducing \ncommunication and computation overhead, while Fed-MedLoRA+ further incorporates \nadaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. As a \ncase study, we apply the framework to clinical information extraction (IE)‚Äîa foundational task \nfor transforming unstructured patient narratives into structured representations of medical \nentities and relations, with clinical applications in patient triage, adverse event monitoring, and \ndecision support. Accuracy was assessed across five patient cohorts through head-to-head \ncomparisons with domain-specific BERT models, zero-shot and fine-tuned LLaMA-3 and \nDeepSeek-R1, GPT-4o, and recent general-domain FL algorithms. Evaluation settings included \n(1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a \nlow-resource new-site adaptation scenario using real-world clinical notes from the Yale New \nHaven Health System. Practical feasibility was further examined under (1) incomplete or \nuneven task annotations across sites, (2) constrained training and inference resources, and (3) \nscalability across multiple institutions. \nFed-MedLoRA+ improved zero-shot LLM performance by up to 65% F1, outperformed single-\nsite fine-tuning by approximately 25%, and exceeded domain-specific BERT models by over 40% \non relation extraction. Both methods generalized robustly to external cohorts (10‚Äì70% F1 \ngains) and achieved strong performance in new-site adaptation (73% strict / 85% lenient F1), \ndemonstrating their effectiveness in real-world multi-institutional deployment. Communication \ncosts were reduced by 98.5% relative to full-model updates. Training was feasible on a single \nRTX 4090 (16 GB) for 8B models and on mid-range GPUs (e.g., RTX 3060 Ti) for 1B models, with \n"}, {"page": 3, "text": "inference supported on standard laptops (e.g., Apple M3 Pro). The framework scaled to 10 \nparticipating sites with only ~2% performance degradation compared to centralized training. \nTogether, the results demonstrate that federated LLMs hold strong potential for medical \napplications while remaining feasible and resource efficient. We further provide actionable \nguidelines and outline future directions for advancing federated LLM development. All \nimplementation code is publicly available at https://github.com/Yale-BIDS-Chen-\nLab/FL_LLM_Med.  \n2. Introduction \nLarge language models (LLMs) are rapidly transforming the landscape of medicine, with \nextensive studies demonstrating their potential across a broad spectrum of applications, \nincluding outpatient management 1,2, disease diagnosis 3,4, and clinical summarization 5,6. To \nsupport clinical adoption, growing efforts have focused on adapting general-domain LLMs \nthrough continued pretraining or post training on clinical data to improve accuracy and safety \nin downstream medical tasks 7,8. \nHowever, in practice, many LLMs are trained or adapted using clinical data from a single \ninstitution, primarily due to privacy and governance constraints that make sharing patient-level \ndata across healthcare systems challenging 9‚Äì12. Yet clinical data vary substantially across \ninstitutions in patient demographics, disease prevalence, care pathways, documentation styles, \nand clinical practices, leading to domain shift and degraded model generalization 13‚Äì15. Recent \nstudies have highlighted that this represents a significant‚Äîand often underestimated‚Äî\nlimitation in the development and deployment of LLMs in medicine 16,17. For example, the \naccuracy of a locally fine-tuned LLM for hospital admission prediction dropped by more than \n20% when evaluated on external patient populations 17. \nFederated learning (FL) offers a privacy-preserving paradigm in which institutions \ncollaboratively train models without sharing raw patient data, thereby directly addressing \nregulatory and governance constraints in healthcare 18. In a standard FL workflow, models are \ntrained locally at each site, model updates are transmitted to a central server for aggregation, \nand the updated global model is redistributed for further refinement 19,20. Despite its growing \nadoption in medicine, however, adapting FL to LLMs remains fundamentally challenging 21‚Äì23.  \nFirst, conventional FL requires transmitting full model parameters at each communication \nround, which is infeasible for multi-billion‚Äìparameter LLMs given the constrained \ncomputational and networking resources available in most healthcare environments 24‚Äì26. \nSecond, real-world clinical data are highly heterogeneous across institutions, differing \nsubstantially in patient populations, disease prevalence, annotation availability, and \ndocumentation practices. Under such conditions, standard FL algorithms based on uniform \nparameter averaging often show unstable convergence and biased performance across sites \n24,27. \nTo date, although recent perspectives have highlighted the promise of federated LLMs in \nmedicine, concrete methodological solutions remain scarce 28,29. Existing medical FL studies \nhave largely focused on smaller architectures such as BERT 28 or traditional classifiers such as \nlogistic regressions 30, which do not face the same scalability constraints as LLMs. A small \n"}, {"page": 4, "text": "number of early works have explored FL for vision foundation models using medical imaging \ndata 27,31. To date, practical and model-agnostic frameworks for federated training of large \nlanguage models remain largely unexplored, representing a key barrier to the development and \ndeployment of LLMs in medicine.  \nTo address these challenges, we introduce Fed-MedLoRA and Fed-MedLoRA+, a model-agnostic \nand parameter-efficient federated learning framework for training large language models in \nmedicine. Fed-MedLoRA enables scalable federated training by transmitting only low-rank \nadapter parameters, substantially reducing communication and computation overhead while \npreserving model capacity. Building on this foundation, Fed-MedLoRA+ incorporates an \nadaptive, data-aware aggregation strategy that explicitly accounts for cross-site heterogeneity, \nthereby improving global model convergence under realistic multi-institutional conditions. \nWe evaluate the proposed framework using clinical information extraction (IE) as a \nrepresentative downstream application. Clinical IE involves identifying and normalizing key \nmedical entities, capturing relations among them, and mapping extracted information to \nstandardized knowledge representations across healthcare systems 32,33. Clinical narratives \nprovide the most comprehensive and context-rich descriptions of patient health, including \nsymptoms, diagnoses, procedures, medications, and social factors that are often missing from \nstructured records 14,34. For example, leveraging free-text clinical notes has been shown to \nidentify over 90% more patients with adverse social determinants of health compared with \nstructured data alone 21. As a result, clinical IE underpins critical downstream applications such \nas automated cohort identification 22, adverse event monitoring 23, and clinical decision support \n35. Despite its importance, prior studies have shown that current LLMs exhibit suboptimal \nperformance on clinical IE 10,36‚Äì38. Early evaluations reported that even the best-performing \nclosed-weight models (e.g., GPT-3.5 and GPT-4) achieved only 39‚Äì52% zero-shot accuracy for \nextracting medical problems, treatments, and tests from patient notes, with performance \ndropping to ~20% for more complex relational tasks such as disease‚Äìdrug association extraction \n37. These limitations further motivate the need for privacy-preserving multi-institutional training \nstrategies for LLMs. \nWe systematically evaluated the proposed framework for accuracy and practical feasibility. \nAccuracy was assessed across five independent patient cohorts totaling 42,198 entities and \n41,570 relations on two core IE tasks‚Äînamed entity recognition (NER) and relation extraction \n(RE)‚Äîunder three settings: (1) in-domain training and testing to assess within-domain \nperformance; (2) independent benchmark validation to evaluate generalization to external \npatient cohorts; and (3) low-resource new-site adaptation using a case study based on real-\nworld clinical notes from the Yale New Haven Health System to simulate model deployment at \nnew institutions with limited annotations. Both Fed-MedLoRA and Fed-MedLoRA+ were \nimplemented using two representative open-weight LLMs‚ÄîLLaMA-3 and DeepSeek-R1‚Äîas \nbackbones and were compared head-to-head against multiple baselines: zero-shot and single-\nsite fine-tuned LLMs (LLaMA-3, DeepSeek-R1, and zero-shot GPT-4o), fine-tuned domain-\nspecific BERT models, and recent general-domain FL algorithms for LLMs. For practical \nfeasibility, we further assessed (1) robustness to heterogeneous and incomplete task \nannotations across sites (e.g., one site with both NER and RE vs. another with only NER), (2) \n"}, {"page": 5, "text": "computational efficiency during training and inference, and (3) scalability as the number of \nparticipating sites increased. \nAcross all evaluation settings, Fed-MedLoRA and Fed-MedLoRA+ consistently outperformed the \nbaselines for both NER and RE tasks. In the in-domain benchmarks, they improved zero-shot \nLLM performance by up to 65% absolute F1, achieved ~25% higher F1 than single-site fine-\ntuned LLMs, and outperformed fine-tuned domain-specific BERT models by up to 40%. In \nexternal validation across independent cohorts, they maintained strong generalization and had \n10-70% higher F1 than the baselines. In the low-resource new-site adaptation study, Fed-\nMedLoRA+ had the best performance, with a strict F1 score of 73% and a lenient F1 score of \n85%, suggesting that a new clinical site could benefit from federated LLMs to bootstrap local \nmodel development with limited annotations. Beyond accuracy, the framework proved feasible \nand scalable in practical settings. Transmitting only low-rank adapters reduced communication \ncost by 98.5% relative to full-model fine-tuning. Both methods trained 8-billion-parameter \nmodels on a single consumer GPU (e.g., RTX 4090, 16 GB) and supported inference on widely \navailable GPUs (e.g., RTX A6000, 3090, 4090). Using a smaller 1-billion-parameter backbone \nenabled training on mid-range GPUs (e.g., RTX 3060 Ti) and inference on standard laptops (e.g., \nApple M3 Pro or equivalent ‚â•18 GB RAM) with up to 7% accuracy decrease. The framework \nscaled efficiently to ten participating sites, maintaining performance within 2% of centralized \ntraining, and remained robust when sites contributed uneven or incomplete task annotations, \nreflecting realistic multi-institutional conditions. \nOverall, the results demonstrate that federated LLM training can be effective and feasible. The \nproposed framework highlights the strong potential of federated LLMs for medical applications. \nWe also provide practical guidelines and identify remaining challenges and opportunities for \nadvancing federated LLMs in medicine. To promote transparency, reproducibility, and \ncommunity collaboration, all implementation code and documentation are publicly available at \nhttps://github.com/Yale-BIDS-Chen-Lab/FL_LLM_Med.  \n \n3. Results \nIn this section, we systematically evaluated both accuracy and practical feasibility of Fed-\nMedLoRA and Fed-MedLoRA+. We first present the overview detailed in Figure 1, followed by \nthe results.  \n"}, {"page": 6, "text": " \nFigure 1. Overview of the study design. A. System overview of Fed-MedLoRA+: (1) each client ùëò fine-tunes the pre-\ntrained backbone on its local data; (2) clients upload only the fine-tuned LoRA adapters to the server; (3) the server \ncomputes an influence score for each client and performs influence-aware aggregation; (4) the server returns the \nupdated LoRA adapters to all clients. B. Evaluation settings, including three dataset types (in-domain benchmarks, \nexternal patient cohort, and a case study on new-site adaptation using real-world clinical notes from Yale New \nHaven Health Systems), baselines (domain-specific BERT models, proprietary and open-weighted LLM \nrepresentatives, and recent federated learning algorithms in the general domain) and comparison methods. C. \nAccuracy evaluations: an example input and output, evaluation metrics (strict match and lenient match), and \nmanual error analysis. D. Feasibility evaluations: robustness to uneven task annotations (e.g., Site 1 provides both \nNER and RE, while Site 2, 3 provide only NER), efficiency, scalability, and efficiency-accuracy tradeoff analysis.  \n \n! Trainable LoRA modules !! ‚àà‚Ñù\"√ó$, %! ‚àà‚Ñù$√ó% across clients\n‚ë°\nInfluence estimation of &&\n!'\n\"'\nPre-trained model\nFrozen\nValidation set\n!(\n\"(\n+ #(√ó\n!)\n\")\n#)√ó\n!*\n\"*\n+ #*√ó\nInfluence-aware aggregation\n‚ë¢\n‚ë£\n‚ë†\nPre-trained model\nFrozen\n!)\n\")\nSite 1\nLocal set 1\n!(\n\"(\nSite 2\nLocal set 2\n!*\n\"*\nSite 3\nLocal set 3\nA. System overview of Fed-MedLoRA+\nGlobal module\nC. Accuracy evaluations\nD. Feasibility evaluations\nPatient denied any gradual \nincrease in chest pain and \nsputum (NER & RE) \nSite 1\nPatient has a history of \nhepatitis C, bipolar disorder \nwith suicide attempts (NER)\nHe was intubated for airway \nprotection and started on a \ncalcium (NER)\nEfficiency\n¬ß\nCommunication cost\n¬ß\nGPU memory cost \nScalability\n¬ß\nParticipants scaling\nUneven task annotations\nInput: The patient had undergone CT angiography \nand had severe valvular heart disease. \nOutput: \nManual error analyses\n¬ß Strict: prediction exactly match gold entity span\n¬ß Lenient: Partial span  overlap is considered correct\nBoundary error\nType confusion\n‚Ä¶\nTrade-offs between \nefficiency & accuracy\nHuman expert\nSite 2\nSite 3\nMIMIC-III\nMTSamples\nUTP\nTraining sets\nLLaMA3\nBaselines\nDeepSeek\nGPT-4o\nClinicalBERT\nComparison methods\nZero-shot\nSingle-site\nFederated\nCentralized\nOur methods significantly outperformed all baselines \n(p value<0.0001)\nExternal patient cohorts\nI2B2\nClinical notes from new sites\nYNHH\nIn-domain benchmarks\nMIMIC-III, MTSamples, UTP\nTesting sets\nB. Evaluation settings\nFed-SA\n"}, {"page": 7, "text": "Table 1. Statistics of datasets, including number of documents, sentences, 4 NER entities and 16 RE modifiers \nacross different sites of datasets.  \nCount \nTraining and testing benchmark \nIndependent \nvalidation set  \nCase study on new \nannotation \nTraining benchmark \nTesting benchmark \nMIMIC-III \nMTSamples \nUTP \nMIMIC-III \nMTSamples \nUTP \nI2B2 \nYNHH \nDocuments \n25 \n96 \n65 \n25 \n50 \n50 \n50 \n4 \nSentences \n4,260 \n5,211 \n4,523 \n4,049 \n2,885 \n5,778 \n4,809 \n291 \nClinical entity categories \nProblem \n2,240 \n4,060 \n4,236 \n2,381 \n2,200 \n3,450 \n1,454 \n242 \nTreatment \n542 \n584 \n418 \n511 \n388 \n329 \n405 \n27 \nTest \n2,302 \n1,529 \n2,609 \n2,542 \n828 \n2,153 \n1,309 \n175 \nDrug \n1,047 \n747 \n710 \n1,021 \n466 \n568 \n681 \n44 \nTotal \n6,131 \n6,920 \n7,973 \n6,455 \n3,882 \n6,500 \n3,849 \n488 \nRelation types \nSeverity \n90 \n114 \n69 \n131 \n38 \n50 \n37 \n/ \nTemporal \n1,303 \n304 \n1,811 \n1,955 \n146 \n1,955 \n621 \nNegation \n467 \n1,519 \n2,339 \n614 \n800 \n1,876 \n282 \nDosage \n244 \n60 \n258 \n228 \n46 \n227 \n90 \nStrength \n463 \n149 \n318 \n419 \n170 \n272 \n261 \nReference range \n11 \n0 \n114 \n5 \n0 \n45 \n0 \nUncertain \n81 \n62 \n27 \n58 \n20 \n15 \n10 \nLab value \n1,579 \n893 \n1,529 \n1,954 \n397 \n1,327 \n900 \nRoute \n346 \n104 \n351 \n329 \n89 \n292 \n245 \nFrequency \n456 \n200 \n290 \n393 \n207 \n251 \n285 \nSubject \n19 \n26 \n128 \n26 \n12 \n131 \n0 \nForm \n372 \n54 \n581 \n336 \n45 \n526 \n129 \nCondition \n42 \n17 \n14 \n74 \n20 \n18 \n1 \nDuration \n67 \n18 \n20 \n25 \n10 \n21 \n27 \nBody location \n709 \n954 \n1,189 \n717 \n578 \n947 \n468 \nCourse \n162 \n218 \n140 \n183 \n108 \n114 \n93 \nTotal \n6,411 \n4,692 \n9,178 \n7,447 \n2,686 \n7,707 \n3,449 \n \n3.1 Evaluation overview  \n3.1.1 Datasets  \nTable 1 summarizes the five cohorts used in this study. Collectively, those five cohorts consist of \n42,198 entities and 41,570 relations covering four main entity categories and 16 relation types. \nWe used four existing benchmarks manually annotated from different institutions: MIMIC-III \n(Medical Information Mart for Intensive Care) 39, MTSamples (Transcribed Medical \nTranscription Sample Reports and Examples) 40, UTP (UT Physicians notes) 15, and Partners \nHealthcare and Beth Israel Deaconess Medical Center notes by Informatics for Integrating \nBiology & the Bedside (I2B2) 41. These datasets have been widely adopted for clinical IE method \ndevelopment and evaluation 15,39‚Äì41. They reflect real-world challenges, including differences in \nscale (e.g., MIMIC-III notes are significantly longer than those from other datasets), entity \ndistributions (e.g., MIMIC-III has nearly twice as many drug entities as UTP due to different \npatient cohorts), and missing data (e.g., MTSamples lacks reference ranges).  \nIn addition, we manually annotated 242 problem-related entities, 27 treatment-related entities, \n175 test-related entities, and 44 drug entities from 291 sentences drawn from four de-\nidentified patient records within the Yale New Haven Health (YNHH) system for the case study. \n"}, {"page": 8, "text": "The annotation scale was intentionally limited to reflect a realistic starting point for a new \nclinical site compared with established benchmarks. \n3.1.2 Evaluation settings  \nEvaluation on accuracy. As shown in Figure 1B, we systematically evaluated the accuracy of \nFed-MedLoRA and Fed-MedLoRA+ under three settings. First, standard training and testing. \nModels were trained on the training sets and evaluated on the corresponding test sets to assess \nin-domain performance. We conducted both two-site (federated learning across MIMIC-III and \nMTSamples) and three-site (MIMIC-III, MTSamples, and UTP) experiments under this setting. \nSecond, independent benchmarks. To evaluate generalization to unseen distributions, models \ntrained under the standard training and testing setting were directly tested on external \ndatasets. In the two-site experiment (training on MIMIC-III and MTSamples), we evaluated \nperformance on UTP and I2B2. In the three-site experiment (training on MIMIC-III, MTSamples, \nand UTP), we evaluated on I2B2. Third, Low-resource case study. To simulate a practical \nscenario where a new clinical site to do clinical IE locally, we conducted a case study using the \nYNHH cohort. Unlike the standard benchmarks in the previous settings, a new site typically has \nonly minimal annotated data available for initialization and needs to adapt existing models for \nlocal development.  \nEvaluation metrics. As shown in Figure 1C, we reported precision, recall, and F1-score, the \nstandard metrics for NER and RE 24,42. Following prior work 15,24, we used two evaluation \nschemes. (1) Strict match: for NER, a true positive requires exact span and entity type match; \nfor RE, this further requires the correct entity pair and relation type. (2) Lenient match: for NER, \na true positive requires overlapping spans and correct entity type; for RE, relations are \nconsidered correct when constituent entities overlap with gold spans and the relation type \nmatches. \nEvaluation on feasibility. Beyond accuracy, we further evaluated the practical feasibility of Fed-\nMedLoRA and Fed-MedLoRA+ across three aspects. Figure 1D shows the detail. First, \nrobustness to uneven task annotations across sites. In practice, individual sites may lack \ncomplete annotations for all tasks. We conducted both two-site and three-site experiments to \nexamine this scenario. In the two-site experiment, Site A (MIMIC-III) included annotations for \nboth NER and RE, whereas Site B (MTSamples) had only NER annotations. In the three-site \nexperiment, Site C (UTP) contained only NER annotations, while Sites A and B provided both \nNER and RE. Second, communication and computational efficiency. We quantified memory \nconsumption and peak GPU utilization during both training and inference to assess efficiency. \nThird, scalability. We conducted simulations with up to ten participating sites to evaluate the \nscalability under increasing federation sizes. \nBaseline models. We compared Fed-MedLoRA and Fed-MedLoRA+ against three categories of \nbaselines: (1) BERT-based models, (2) LLM-based models, and (3) recent general-domain FL \nalgorithms for LLMs.  \nFor BERT-based baselines, we used Bio_ClinicalBERT 43,44, initialized from BioBERT (pretrained \non biomedical literature) 45  and further pretrained on clinical notes. Bio_ClinicalBERT has \nachieved completive performance on multiple clinical IE benchmarks and is widely adopted as a \n"}, {"page": 9, "text": "strong baseline 44. We reported its single-site fine-tuning performance. Single-site training \nreflects a common scenario in clinical practice, where models are trained on data from a single \nsite without federated learning support. For instance, for three datasets from different sites, we \ntrained a model on one site‚Äôs training set and evaluated it on all test sets, repeating for each \nsite. The reported results are averaged across sites.  \nFor LLM-based baselines, we performed head-to-head comparisons using the same backbone \nmodels with and without our proposed federated learning methods. Fed-MedLoRA and Fed-\nMedLoRA+ are backbone-agnostic; we selected two representative open-weight LLMs LLaMA3 \n46 and DeepSeek-R1-Distill 47 for evaluation. We reported both zero-shot (no fine-tuning) and \nsingle-site fine-tuned performance. Additionally, we included GPT-4o as a widely used \nproprietary LLM representative and reported its zero-shot performance for comparison. To \ncomplement and thoroughly assess our proposed approach, we evaluated a recently \nintroduced FL algorithm for LLMs in the general domain named FedSA-LoRA 48 as an additional \nbaseline. This method reduces communication by transmitting only the low rank ùê¥ to the server \nfor aggregation, under the assumption that the ùê¥ matrices capture the global knowledge.  \nIn addition, we also included centralized training for both BERT-based and LLM-based models as \na reference. Centralized training pools data from all sites to train a single model, which is then \nevaluated on the same test sets. This configuration provides an empirical upper bound, \nrepresenting an ideal but impractical setting in healthcare due to privacy and governance \nconstraints. The implementation and hyperparameter details are provided in Section 5 \nMethods. \nBootstrapping and statistical analysis. For both the training/testing and independent \nbenchmark settings, we performed bootstrapping with a sample size of 200 randomly selected \ninstances per dataset, repeated 30 times, and reported results with 95% confidence intervals. \nWe further conducted a two-tailed Wilcoxon rank-sum test to assess statistical significance. \nBoth procedures followed established practices 49,50. \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 10, "text": "Table 2. Strict and lenient micro F1 scores for NER and RE test sets under two-site experiments. Results are \nreported for three backbones (LLaMA3-8B, DeepSeek-R1-Distill-8B and GPT-4o) and BERT baseline \n(Bio_ClinicalBERT) across five training strategies: zero-shot, single-site fine-tuning (average across sites, baseline), \nFed-MedLoRA, Fed-MedLoRA+, and centralized fine-tuning (upper bound). All values are micro-averaged F1 scores \n(strict and lenient). Centralized (upper bound) rows are shaded in gray. For each model, the best non-centralized \nmethod is shown in bold and the second-best is underlined.  \nModels \nMethods \nNER test set \nRE test set \nStrict F1-scores \nLenient F1-scores \nStrict F1-scores \nLenient F1-scores \nMIMIC-III \nMTSamples \nMIMIC-III \nMTSamples \nMIMIC-III \nMTSamples \nMIMIC-III \nMTSamples \nBio_ClinicalB\nERT \nCentralized (Upper bound) \n0.807 \n0.827 \n0.907 \n0.905 \n0.590 \n0.584 \n0.735 \n0.657 \nSingle site (Avg, baseline) \n0.753 \n0.792 \n0.867 \n0.888 \n0.434 \n0.474 \n0.595 \n0.549 \nLLaMA3-8B \nCentralized (Upper bound) \n0.856 \n0.870 \n0.954 \n0.958 \n0.867 \n0.831 \n0.889 \n0.913 \nZero-shot (Baseline) \n0.345 \n0.437 \n0.503 \n0.644 \n0.203 \n0.056 \n0.239 \n0.074 \nSingle site (Avg, baseline) \n0.803 \n0.843 \n0.918 \n0.926 \n0.648 \n0.721 \n0.698 \n0.798 \nFed-MedLoRA (Ours) \n0.847 \n0.858 \n0.942 \n0.929 \n0.850 \n0.817 \n0.860 \n0.849 \nFed-MedLoRA+ (Ours) \n0.850 \n0.866 \n0.949 \n0.937 \n0.860 \n0.826 \n0.868 \n0.856 \nDeepSeek-\nR1-Distill-8B \nCentralized (Upper bound) \n0.852 \n0.862 \n0.952 \n0.963 \n0.863 \n0.829 \n0.842 \n0.911 \nZero-shot (Baseline) \n0.291 \n0.303 \n0.438 \n0.499 \n0.163 \n0.106 \n0.190 \n0.139 \nSingle site (Avg, baseline) \n0.797 \n0.836 \n0.902 \n0.921 \n0.589 \n0.719 \n0.644 \n0.793 \nFed-MedLoRA (Ours) \n0.841 \n0.858 \n0.932 \n0.936 \n0.799 \n0.767 \n0.826 \n0.892 \nFed-MedLoRA+ (Ours) \n0.845 \n0.860 \n0.945 \n0.951 \n0.831 \n0.773 \n0.831 \n0.907 \nGPT-4o \nZero-shot (Baseline) \n0.556 \n0.602 \n0.815 \n0.834 \n0.124 \n0.096 \n0.289 \n0.178 \n \n3.2 Results on accuracy \n3.2.1 Results on the training/testing setting \nTable 2 summarizes model performance in the two-site experiment, where federated learning \nwas conducted across MIMIC-III and MTSamples and evaluated on their respective test sets. \nTable 3 presents corresponding results for the three-site experiment involving MIMIC-III, \nMTSamples, and UTP under the same training/testing setting. More results including individual \nentity accuracy and Bootstrapping and statistical analysis are illustrated in Supplementary S2.1.  \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 11, "text": "Table 3. Strict and lenient micro F1 scores for NER and RE test sets under three-site experiments. Results are \nreported for two backbones (LLaMA3-8B and DeepSeek-R1-Distill-8B) and BERT baseline (Bio_ClinicalBERT) across \nfive training strategies: zero-shot, single-site fine-tuning (average across sites, baseline), Fed-MedLoRA, Fed-\nMedLoRA+, and centralized fine-tuning (upper bound). All values are micro-averaged F1 scores (strict and lenient). \nCentralized (upper bound) rows are shaded in gray. For each model, the best non-centralized method is shown in \nbold and the second-best is underlined.  \nModel \nMethod \nNER test set \nRE test set \nStrict F1-score \nMIMIC-III \nMTSamples \nUTP \nMIMIC-III \nMTSamples \nUTP \nBio_ClinicalBERT \nCentralized (Upper bound) \n0.810 \n0.838 \n0.823 \n0.598 \n0.515 \n0.413 \nSigle site (Avg, baseline) \n0.782 \n0.790 \n0.720 \n0.343 \n0.433 \n0.249 \nLLaMA3-8B \nCentralized (Upper bound) \n0.840 \n0.867 \n0.909 \n0.851 \n0.788 \n0.924 \nZero-shot (Baseline) \n0.345 \n0.437 \n0.256 \n0.203 \n0.056 \n0.091 \nSingle site (Avg, baseline) \n0.775 \n0.841 \n0.798 \n0.656 \n0.703 \n0.778 \nFed-MedLoRA (Ours) \n0.809 \n0.861 \n0.897 \n0.744 \n0.759 \n0.916 \nFed-MedLoRA+ (Ours) \n0.803 \n0.862 \n0.898 \n0.771 \n0.762 \n0.913 \nDeepSeek-R1-Distill-8B \nCentralized (Upper bound) \n0.830 \n0.846 \n0.907 \n0.843 \n0.785 \n0.913 \nZero-shot (Baseline) \n0.291 \n0.303 \n0.251 \n0.163 \n0.106 \n0.110 \nSingle site (Avg, baseline) \n0.778 \n0.825 \n0.806 \n0.605 \n0.725 \n0.756 \nFed-MedLoRA (Ours) \n0.799 \n0.803 \n0.848 \n0.737 \n0.739 \n0.890 \nFed-MedLoRA+ (Ours) \n0.802 \n0.818 \n0.879 \n0.752 \n0.736 \n0.894 \nGPT-4o \nZero-shot (Baseline) \n0.556 \n0.602 \n0.477 \n0.124 \n0.096 \n0.091 \n \nLenient F1-score \nMIMIC-III \nMTSamples \nUTP \nMIMIC-III \nMTSamples \nUTP \nBio_ClinicalBERT \nCentralized (Upper bound) \n0.919 \n0.912 \n0.884 \n0.742 \n0.621 \n0.571 \nSigle site (Avg, baseline) \n0.848 \n0.869 \n0.837 \n0.470 \n0.500 \n0.276 \nLLaMA3-8B \nCentralized (Upper bound) \n0.925 \n0.926 \n0.913 \n0.885 \n0.855 \n0.932 \nZero-shot (Baseline) \n0.503 \n0.644 \n0.513 \n0.239 \n0.074 \n0.112 \nSingle site (Avg, baseline) \n0.876 \n0.867 \n0.878 \n0.699 \n0.780 \n0.812 \nFed-MedLoRA (Ours) \n0.893 \n0.901 \n0.906 \n0.801 \n0.797 \n0.901 \nFed-MedLoRA+ (Ours) \n0.900 \n0.914 \n0.936 \n0.814 \n0.816 \n0.910 \nDeepSeek-R1-Distill-8B \nCentralized (Upper bound) \n0.925 \n0.926 \n0.906 \n0.885 \n0.855 \n0.932 \nZero-shot (Baseline) \n0.438 \n0.499 \n0.433 \n0.190 \n0.139 \n0.128 \nSingle site (Avg, baseline) \n0.895 \n0.855 \n0.861 \n0.732 \n0.799 \n0.807 \nFed-MedLoRA (Ours) \n0.865 \n0.910 \n0.920 \n0.811 \n0.821 \n0.914 \nFed-MedLoRA+ (Ours) \n0.888 \n0.925 \n0.940 \n0.890 \n0.896 \n0.918 \nGPT-4o \nZero-shot (Baseline) \n0.815 \n0.834 \n0.676 \n0.289 \n0.178 \n0.213 \n \nComparisons with LLM-based baselines. The proposed Fed-MedLoRA and Fed-MedLoRA+ \nconsistently outperformed LLM-based baselines (both zero-shot and single-site fine-tuned) \nusing the same backbone models (LLaMA3, DeepSeek-R1-Distill and GPT-4o) in head-to-head \ncomparisons. Compared to zero-shot performance, Fed-MedLoRA and Fed-MedLoRA+ achieve \nover 50% absolute improvement on both NER and RE test sets. For example, in the two-site \nLLaMA3 experiment on MIMIC-III, strict matching NER F1 score improves from 0.345 (zero-shot) \nto 0.850 (Fed-MedLoRA+, p<0.0001), while RE strict F1 score improves from 0.203 (zero-shot) \nto 0.860 (Fed-MedLoRA+, p<0.0001). Similar trends are observed in the three-site experiment. \nAlthough GPT-4o achieved the highest zero-shot NER performance among all evaluated LLMs \non the MTSamples dataset (strict and lenient F1 scores of 0.602 and 0.834, respectively), its \nzero-shot performance was still significantly lower than that of federated fine-tuning models. \n"}, {"page": 12, "text": "These results demonstrate that federated instruction tuning can significantly improve \nperformance on NER and RE tasks.  \nIn addition, compared to the single-site baseline‚Äîwhich reflects the most common clinical \nscenario where models are trained using data from a single institution‚ÄîFed-MedLoRA+ \nconsistently outperform across all test sets in both two-site and three-site experiments. The \nlargest absolute gains are observed in RE, with up to ~24.2% absolute improvement. For \nexample, in the two-site experiment on MIMIC-III, the single-site baseline with LLaMA3 \nachieves 0.648 strict F1 score, compared to 0.860 for Fed-MedLoRA+ (p<0.0001). Similarly, with \nDeepSeek-R1-Distill, the single-site baseline achieves 0.589 versus 0.831 (p<0.0001) for Fed-\nMedLoRA+. This improvement may relate to the fact that RE is inherently more challenging \nthan NER, and leveraging data from multiple sites provides greater benefit. We elaborate on \nthis in the section Comparisons between tasks. In terms of backbone models, LLaMA3 and \nDeepSeek-R1-Distill demonstrate similar performance, with LLaMA3 slightly outperforming in \nthe three-site experiments.  \nComparisons with centralized learning as the estimated upper bound. Centralized learning \npools data from all sites for training which is impractical for medical applications. We use it as a \nreference of estimated upper bound. As shown in Tables 2 and 3, Fed-MedLoRA and Fed-\nMedLoRA+ achieve performance competitive with centralized training. Notably, Fed-MedLoRA+ \nclosely approach centralized learning across most datasets under both two-site and three-site \nexperiments. For example, in the two-site experiment for NER, Fed-MedLoRA+ achieves strict \nF1 scores of 0.850 vs. 0.856 on MIMIC-III for centralized learning, and 0.866 vs. 0.870 (p=0.081) \non MTSamples. In the three-site experiment, the gaps are slightly larger, e.g., 0.803 vs. 0.840 \n(p=0.042), 0.862 vs. 0.867 (p=0.121), and 0.898 vs. 0.909 (p=0.076), but Fed-MedLoRA+ remains \nthe closest to the centralized upper bound.  \nComparisons with BERT-based baselines. Fed-MedLoRA and Fed-MedLoRA+ consistently \noutperform BERT-based models, with gains of up to 9.7% absolute F1 score on NER (e.g., 0.850 \nfor Fed-MedLoRA+ with LLaMA3 vs. 0.753 for single-site Bio_ClinicalBERT, p<0.001) and up to \n42.6% absolute F1 score on RE (e.g., 0.860 for Fed-MedLoRA+ with LLaMA3 vs. 0.434 for \nBio_ClinicalBERT, p<0.0001). Interestingly, when comparing BERT-based and LLM-based single-\nsite baselines, LLM-based models outperform BERT across nearly all test sets, except for a \nmarginally lower performance on MIMIC-III in the three-site experiment (0.775 vs. 0.782, \np=0.069). We conducted a more detailed comparative analysis between BERT-based and LLM-\nbased models, combining results from both in-domain and independent benchmark settings, as \npresented below. \nComparisons with general-domain FL Algorithms for LLMs. As mentioned above, we further \ncompared the proposed methods with FedSA-LoRA 48, a recently introduced FL algorithm for \nLLMs in the general domain, used here as an additional baseline. Table 4 presents head-to-head \ncomparisons from the three-site experiment using the LLaMA3-8B model. Across all evaluation \ndatasets, Fed-MedLoRA and Fed-MedLoRA+ consistently outperformed FedSA-LoRA on both \nNER and RE tasks. For example, FedSA-LoRA achieved a strict F1 score of 0.298 on NER UTP, \ncompared to 0.898 for Fed-MedLoRA+. Similarly, FedSA-LoRA obtained a strict F1 score of 0.181 \nfor RE, compared to 0.894 for Fed-MedLoRA+. The two-site experiment results showed similar \n"}, {"page": 13, "text": "trends (see Supplementary S2.1). We anticipate two possible reasons for these improvements. \nFirst, the proposed Fed-MedLoRA and Fed-MedLoRA+ exchange task-aware, parameter-\nefficient adapters that preserve task-specific information across sites, whereas FedSA-LoRA \ntransmits only a single low-rank matrix, potentially losing important local adaptation signals. \nSecond, FedSA-LoRA was not originally designed for clinical applications, whereas Fed-\nMedLoRA+ incorporates influence-aware aggregation to enhance robustness against cross-site \nheterogeneity. \nTable 4. Strict micro F1 scores for NER and RE (LLaMA3-8B) under the three-site experiment. Methods compared: \nFedSA-LoRA (baseline), Fed-MedLoRA and Fed-MedLoRA+.  \nMethod \nNER test set \nRE test set \nMIMIC-III \nMTSamples \nUTP \nI2B2 \nMIMIC-III \nMTSamples \nUTP \nI2B2 \nFedSA-LoRA (Baseline) \n0.469 \n0.476 \n0.298 \n0.365 \n0.205 \n0.108 \n0.181 \n0.184 \nFed-MedLoRA (Ours) \n0.809 \n0.861 \n0.897 \n0.833 \n0.737 \n0.739 \n0.890 \n0.711 \nFed-MedLoRA+ (Ours) \n0.803 \n0.862 \n0.898 \n0.844 \n0.752 \n0.736 \n0.894 \n0.713 \n \nComparisons between Fed-MedLoRA and Fed-MedLoRA+. Across both two-site and three-site \nexperiments, Fed-MedLoRA+ generally outperforms Fed-MedLoRA. In the two-site experiment, \nFed-MedLoRA+ achieves higher scores on all four test sets (NER and RE on both datasets). In \nthe three-site experiment, Fed-MedLoRA+ outperforms Fed-MedLoRA on four out of six test \nsets, with only marginal differences in the remaining two sets (e.g., 0.803 vs. 0.809 on MIMIC-\nIII, p=0.892, and 0.913 vs. 0.916 on UTP, p=0.104). Collectively, across the two- and three-site \nexperiments, both Fed-MedLoRA and Fed-MedLoRA+ consistently outperforms the baselines. \nAn important observation is that adding more sites does not always guarantee better \nperformance on every site. For instance, both Fed-MedLoRA and Fed-MedLoRA+ achieve higher \ntest scores on MIMIC-III under the two-site experiment than under the three-site experiment. A \nsimilar pattern is observed for centralized learning, which serves as the estimated upper bound: \nthe exact F1 score on MIMIC-III decreases from 0.856 in the two-site experiment to 0.840 in the \nthree-site experiment. This suggests that while incorporating multiple sites generally increases \nrobustness relative to single-site learning, it does not guarantee uniform gains on every test set. \nThis aligns with prior findings, which highlight challenges such as data heterogeneity, domain \nshifts, and label distribution differences. One plausible explanation is that the third site \nintroduces increased data heterogeneity: entity distribution differences and annotation noises \n(see manual error analysis results in Table 7) may bias aggregate updates and create trade-offs \nbetween sites.  \nComparisons between tasks. Another observation is that results on RE test sets are generally \nlower than those on NER. Indeed, NER focuses on single-entity recognition, while RE requires \nidentifying entity pairs and the semantic relation between them, which is inherently more \nchallenging 51. Fed-MedLoRA and Fed-MedLoRA+ demonstrate greater robustness on RE \ncompared to baselines. For example, in the three-site experiment, the single-site LLaMA3 \nbaseline achieves 0.775 on NER but drops to 0.656 on RE. In contrast, Fed-MedLoRA+ achieves \nan NER score of 0.803 and an RE score of 0.771. A similar pattern is observed with DeepSeek-\nR1-Distill: the single-site baseline performance drops from 0.778 on NER to 0.605 on RE, \nwhereas Fed-MedLoRA+ achieves 0.802 on NER and 0.762 on RE. Thus, aggregating distributed \n"}, {"page": 14, "text": "and diverse training samples via federated LoRA substantially reduces the gap of RE relative to \nsingle-site training.  \nTable 5. Cross-institution generalization: exact micro F1-scores for NER and RE across two-site and three-site \nexperiments. Results compare zero-shot, single-site (average over sites), federated (Fed-MedLoRA, Fed-\nMedLoRA+) and centralized fine-tuning for Bio_ClinicalBERT, LLaMA3-8B and DeepSeek-R1-Distill-8B. Two-site \nexperiment: Site A and B own MIMIC-III and MTSamples, respectively, and external test sets are UTP and I2B2. \nThree-site experiment: Site A, B and C own MIMIC-III, MTSamples and UTP, respectively and test set is I2B2. \nCentralized (upper bound) rows are shaded in gray. For each model, the best non-centralized method is shown in \nbold and the second-best is underlined.  \nModel \nMethod \nTwo-site experiment \nThree-site experiment \nNER test set \nRE test set \nNER test set \nRE test set \nStrict \nLenient \nStrict \nLenient \nStrict \nLenient \nStrict \nLenient \nUTP \nI2B2 \nUTP \nI2B2 \nUTP \nI2B2 \nUTP \nI2B2 \nI2B2 \nI2B2 \nI2B2 \nI2B2 \nBio_ClinicalBE\nRT \nCentralized (Upper bound) \n0.692 \n0.768 \n0.851 \n0.838 \n0.288 \n0.354 \n0.529 \n0.536 \n0.787 \n0.863 \n0.352 \n0.579 \nSingle site (Avg, baseline) \n0.678 \n0.747 \n0.846 \n0.808 \n0.270 \n0.232 \n0.441 \n0.419 \n0.752 \n0.816 \n0.219 \n0.495 \nLLaMA3-8B \nCentralized (Upper bound) \n0.852 \n0.863 \n0.932 \n0.945 \n0.803 \n0.746 \n0.887 \n0.755 \n0.852 \n0.916 \n0.745 \n0.798 \nZero-shot (Baseline) \n0.256 \n0.330 \n0.513 \n0.496 \n0.091 \n0.096 \n0.116 \n0.114 \n0.330 \n0.496 \n0.096 \n0.114 \nSingle site (Avg, baseline) \n0.806 \n0.801 \n0.884 \n0.924 \n0.730 \n0.667 \n0.822 \n0.715 \n0.756 \n0.873 \n0.659 \n0.708 \nFed-MedLoRA (Ours) \n0.838 \n0.850 \n0.921 \n0.930 \n0.778 \n0.693 \n0.815 \n0.726 \n0.833 \n0.896 \n0.711 \n0.748 \nFed-MedLoRA+ (Ours) \n0.840 \n0.860 \n0.927 \n0.939 \n0.793 \n0.740 \n0.854 \n0.736 \n0.844 \n0.889 \n0.713 \n0.761 \n \nCentralized (Upper bound) \n0.844 \n0.864 \n0.913 \n0.926 \n0.797 \n0.720 \n0.930 \n0.816 \n0.822 \n0.907 \n0.730 \n0.877 \nDeepSeek-R1-\nDistill-8B \nZero-shot (Baseline) \n0.251 \n0.321 \n0.433 \n0.552 \n0.110 \n0.155 \n0.128 \n0.189 \n0.321 \n0.552 \n0.155 \n0.189 \nSingle site (Avg, baseline) \n0.788 \n0.769 \n0.892 \n0.906 \n0.687 \n0.665 \n0.719 \n0.688 \n0.789 \n0.862 \n0.649 \n0.729 \nFed-MedLoRA (Ours) \n0.819 \n0.845 \n0.901 \n0.915 \n0.789 \n0.711 \n0.901 \n0.782 \n0.791 \n0.868 \n0.692 \n0.764 \nFed-MedLoRA+ (Ours) \n0.825 \n0.855 \n0.912 \n0.917 \n0.751 \n0.724 \n0.928 \n0.801 \n0.799 \n0.896 \n0.685 \n0.761 \nGPT-4o \nZero-shot \n0.477 \n0.602 \n0.676 \n0.766 \n0.091 \n0.106 \n0.213 \n0.192 \n0.602 \n0.766 \n0.106 \n0.192 \n \n3.2.2 Results on the independent benchmark setting \nTable 5 summarizes the results of the two-site experiment (federated learning across MIMIC-III \nand MTSamples, evaluated on UTP and I2B2) and the three-site experiment (federated learning \nacross MIMIC-III, MTSamples, and UTP, evaluated on I2B2) under the independent benchmark \nsetting. Supplementary S2.1 provides additional results on individual entity accuracy. \nComparisons with LLM-based baselines. Similar to the standard training/testing setting, both \nFed-MedLoRA and Fed-MedLoRA+ consistently outperform both zero-shot and single-site fine-\ntuning baselines on independent benchmarks using the same LLM backbones. Note that zero-\nshot baselines remain unchanged across settings since no fine-tuning is applied. For NER, Fed-\nMedLoRA+ achieves ~58.4% absolute gain (e.g., exact F1 score of 0.256 for zero-shot vs. 0.840 \nfor Fed-MedLoRA+ on UTP using LLaMA3) and ~70.2% absolute gain on RE (e.g., 0.091 vs. 0.793 \nin the same setting). Similar gains are observed with DeepSeek-R1-Distill and GPT-4o as \nbackbones. When compared to single-site fine-tuning, the federated approach also has \nconsistent improvements. For instance, in the three-site experiment on I2B2 with LLaMA3, Fed-\nMedLoRA+ achieves up to a 9.6% gain on NER (strict F1 score: 0.852 for Fed-MedLoRA+ vs. \n0.756 for single-site fine-tuning) and a 5.4% gain on RE (strict F1 score: 0.713 for Fed-MedLoRA+ \nvs. 0.659 for single-site fine-tuning). These results suggest that federated adaption captures \ncomplementary cross-site information, leading to improved out-of-domain generalization.  \n"}, {"page": 15, "text": "Comparisons with BERT-based baselines. The performance gap between federated approaches \nand BERT-based baselines was more significant under independent benchmarks compared to \nthe train/test testing. For NER, Fed-MedLoRA+ achieved ~15% higher exact F1 than single-site \nfine-tuned Bio_ClinicalBERT (UTP: 0.840 vs. 0.678; I2B2: 0.860 vs. 0.747) in the two-site \nexperiment, and over 10% higher in the three-site experiment (I2B2: 0.852 vs. 0.752). For RE, \nFed-MedLoRA+ achieved improvements of ~70% on UTP (0.793 vs. 0.270) and ~50% on I2B2 \n(0.740 vs. 0.302) in the two-site experiment, and a similar ~50% gain on I2B2 in the three-site \nexperiment (0.713 vs. 0.219). Centralized learning results also revealed consistent patterns: \ncentralized LLMs substantially outperformed centralized BERT models on independent \nbenchmarks. Collectively, when comparing LLMs vs. BERT under both training/testing and \nindependent benchmarks, the performance gap was much larger on external datasets. For \ninstance, under the training/testing setting, centralized LLMs achieved only ~4% higher F1 than \nBERT on NER (e.g., 0.865 vs. 0.807 on MIMIC-III; 0.870 vs. 0.827 in the two-site experiment). In \ncontrast, under external benchmarks, centralized LLMs had ~16% higher F1 on UTP (0.852 vs. \n0.692) and ~9.5% higher on I2B2 (0.863 vs. 0.768). These results suggest that while BERT \nmodels remain competitive when training and testing distributions are similar, they generalize \npoorly to external datasets. This observation is consistent with prior reports in the \nliterature15,52,53. Possible explanations are twofold. First, LLMs are pretrained on much larger \nand more diverse corpora, whereas domain-specific BERT models are pretrained on specific \nmedical corpora (e.g., Bio_ClinicalBERT was pretrained on MIMIC-III), which might constrain \ntheir generalization ability. Second, BERT-based models may rely more heavily on lexical and \nannotation patterns specific to their training datasets (e.g., abbreviations, formatting, or \ntagging conventions), leading to reduced robustness when these patterns differ in external \ndatasets. \nOther observations. Additional observations are consistent with the training/testing results. \nFirst, the proposed federated approaches consistently achieved performance closest to \ncentralized learning as the estimated upper bound (e.g., ~2% difference between Fed-\nMedLoRA+ and centralized learning on both UTP and I2B2 for NER and RE in the two-site \nexperiment). Second, Fed-MedLoRA+ generally outperformed Fed-MedLoRA (e.g., higher scores \nin all 8 metrics‚Äîstrict and lenient F1, NER and RE, on UTP and I2B2 in the two-site experiment). \nThird, LLaMA3 and DeepSeek-R1-Distill showed similar performance, with LLaMA3 slightly \nbetter for both NER and RE. In contrast, we also observed mixed effects of increasing the \nnumber of sites. Under the training/testing setting, performance in the two-site experiments \nwas generally higher than in the three-site experiments. When applied to external benchmarks, \nsome performance drop persisted but was less consistent. For example, with LLaMA3 on I2B2 \nNER, the single-site fine-tuned baseline dropped from 0.801 (two-site) to 0.756 (three-site), \nwhile Fed-MedLoRA remained robust (0.860 vs. 0.852). \n3.2.3 Results on the low-resource case study \nAs described above, we conducted a case study using the newly annotated YNHH cohort to \nsimulate a practical scenario in which a new site performs local clinical IE. In contrast to \nestablished benchmarks, such a site typically has only minimal annotated data available for \ninitialization and must rely on adapting existing models rather than fine-tuning locally. Realistic \noptions therefore include using publicly available LLMs in a zero-shot manner or leveraging \n"}, {"page": 16, "text": "federated models trained on data from other institutions without sharing sensitive \ninformation. This case study is approved by the Yale Institutional Review Board (IRB) under \nprotocol number 2000036010.  \nTable 6. Evaluation on Yale New Haven Health (YNHH) clinical notes for NER. The table reports strict and lenient \nmicro F1 scores, precision and recall across two-site and three-site experiments. Results compare a BERT baseline \n(Bio_ClinicalBERT) and the LLaMA3-8B backbone under four strategies: zero-shot, Fed-MedLoRA, Fed-MedLoRA+ \nand centralized fine-tuning (upper bound). Centralized (upper bound) rows are shaded in gray. For each model, the \nbest non-centralized method is shown in bold and the second-best is underlined.  \nModel \nMethod \nMetrics \nStrict \nLenient \nF1 score \nPrecision \nRecall \nF1 score \nPrecision \nRecall \nTwo-site experiment \nBio_ClinicalBERT \nCentralized (Upper bound) \n0.596 \n0.624 \n0.571 \n0.645 \n0.676 \n0.617 \nLLaMA3-8B \nCentralized (Upper bound) \n0.732 \n0.778 \n0.690 \n0.823 \n0.881 \n0.772 \nZero-shot (Baseline) \n0.397 \n0.348 \n0.462 \n0.524 \n0.459 \n0.609 \nFed-MedLoRA (Ours) \n0.708 \n0.759 \n0.664 \n0.794 \n0.850 \n0.745 \nFed-MedLoRA+ (Ours) \n0.720 \n0.764 \n0.681 \n0.809 \n0.867 \n0.758 \nThree-site experiment \nBio_ClinicalBERT \nCentralized (Upper bound) \n0.606 \n0.631 \n0.583 \n0.646 \n0.669 \n0.624 \nLLaMA3-8B \nCentralized (Upper bound) \n0.733 \n0.755 \n0.711 \n0.858 \n0.858 \n0.859 \nZero-shot (Baseline) \n0.397 \n0.348 \n0.462 \n0.524 \n0.459 \n0.609 \nFed-MedLoRA (Ours) \n0.722 \n0.718 \n0.726 \n0.848 \n0.843 \n0.853 \nFed-MedLoRA+ (Ours) \n0.730 \n0.732 \n0.729 \n0.854 \n0.852 \n0.856 \n \nWe directly applied Fed-MedLoRA and Fed-MedLoRA+ models trained in the two-site and \nthree-site experiments, using LLaMA3-8B as the backbone. For comparison, we also evaluated \nthe zero-shot LLaMA3-8B baseline, along with centralized LLaMA3-8B and Bio_ClinicalBERT, \nwhich serve as empirical upper-bound references. Evaluation metrics were consistent with \nthose described above. Table 6 summarizes the results. Overall, both Fed-MedLoRA and Fed-\nMedLoRA+ consistently outperformed all baselines on the new site annotations. For instance, \ndirectly applying Fed-MedLoRA+ from the three-site experiment achieved the highest \nperformance, with a strict F1-score of 0.730 compared to 0.397 for the zero-shot baseline \n(p<0.0001), and a lenient F1-score of 0.854 compared to 0.524 for the zero-shot baseline \n(p<0.0001). While Fed-MedLoRA also demonstrated robust performance, Fed-MedLoRA+ \nconsistently had higher scores under both strict and lenient evaluation criteria. Moreover, Fed-\nMedLoRA+ achieved performance comparable to the upper-bound centralized LLaMA3-8B \nmodel (e.g., 0.720 vs. 0.732 and 0.730 vs. 0.733 in the two-site and three-site experiments, \np=0.0691, p=0.0874). Notably, both federated models also outperformed the centralized \nBio_ClinicalBERT baseline, consistent with earlier observations on the independent benchmark \nsetting. Results of individual entity performance are shown in Supplementary S2.2.  \n \n \n \n \n"}, {"page": 17, "text": "Table 7. NER error categories observed in 200 manually curated error instances. Count indicates the number of \ntimes each error category was observed across the sampled set (instances may be counted under multiple \ncategories). Examples show gold annotation (Gold) and model prediction (Predict).   \nCategories \nCount \nDescriptions \nExamples \nBoundary/ span \nerror (partial match) \n90 \nPredicted span overlaps the gold \nbut is shorter or longer  \nGold: <span class=\"test\"> His urine</span> and \n<span class=\"test\"> serum tox screens</span> ...  \nPredict: <span class=\"test\">His urine and serum tox \nscreens</span> ...  \nFalse negative  \n72 \nModel fails to predict an entity \nthat is present in the gold  \nGold: He was admitted to the CMED , intubated for \n<span class=\"treatment\"> airway \nprotection</span> ‚Ä¶ \nPredict: He was admitted to the CMED , intubated \nfor airway protection ‚Ä¶  \nType confusion \n34 \nCorrect span but wrong entity \nlabel \nGold: 2012-06-07 09 : 55 PM <span \nclass=\"test\">BLOOD ASA</span> - NEG Ethanol ‚Ä¶ \nPredict: 2012-06-07 09 : 55 PM <span \nclass=\"drug\">BLOOD ASA</span> - NEG Ethanol ‚Ä¶ \nFalse positive \n18 \nModel predicts an entity where \nthe gold has none \nGold: ‚Ä¶ above the thoracic inlet approximately 6 . 5 \ncm above the carina . \nPredict: ‚Ä¶ above the <span class=\"test\">thoracic \n</span> inlet approximately 6 . 5 cm above the \ncarina . \nBoundary & type \nerror  \n16 \nWrong span and wrong entity \nlabel \nGold: ‚Ä¶ her ST segments with <span \nclass=\"test\">evolution of Q waves</span> . \nPredict: ‚Ä¶ <span class=\"test\">her ST \nsegments</span> with evolution of <span \nclass=\"problem\">Q waves</span> . \nAnnotation error \n9 \nThe annotator annotates the \nwrong entity label  \nGold: Patient had No <span class=\"problem\">Known \nAllergies to Drugs</span> ‚Ä¶ \nPredict: Patient had No <span \nclass=\"problem\">Known Allergies</span> to <span \nclass=\"drug\">Drugs</span> ‚Ä¶ \nMerged/ split \nentities \n5 \nModel merges two adjacent \nentities into one, or splits one \ngold entity into multiple ones  \nGold: <span class=\"drug\">The nasogastric \ntube<span> terminates within the stomach . \nPredict: <span class=\"drug\">The nasogastric</span> \n<span class=\"drug\">tube </span>terminates within \nthe stomach . \n \n3.2.4 Manual error analysis \nWe further conducted a manual error analysis to characterize the common errors made by the \nmodels. From the four test corpora (MIMIC-III, MTSamples, UTP, and I2B2), we randomly \nsampled 200 instances in total containing NER prediction errors produced by Fed-MedLoRA+ \n(LLaMA3-8B). Each instance was manually reviewed and categorized consistent with prior \nstudies 15. Table 7 summarizes the identified error categories, their frequencies, and \nrepresentative examples. Note that a single instance may contain multiple error types. \nOverall, boundary errors and false negatives were the most frequent failure modes, followed by \ntype confusion and false positives. Boundary errors typically occurred when the model \npredicted spans that overlapped with, but did not exactly match, the gold-standard spans‚Äî\nresulting in penalties under the strict F1 metric. False negatives frequently reflected omissions \nof shorter or nested entities. \n"}, {"page": 18, "text": "Table 8. Model robustness results on uneven task annotations from different sites: strict micro F1 scores of Fed-\nMedLoRA, Fed-MedLoRA+, zero-shot, single-site and centralized learning on NER and RE tasks by fine-tuning on \nLLaMA3-8B model under two-site and three-site experiments. Centralized (upper bound) rows are shaded in gray. \nFor each model, the best non-centralized method is shown in bold and the second-best is underlined.  \nTraining set \nMethods \nNER test set \nRE test set \nMIMIC-III \nMTSamples \nUTP \nI2B2 \nMIMIC-III \nMTSamples \nUTP \nI2B2 \nTwo-site experiment \nA: MIMIC-III (NER+RE) \nB: MTSamples (NER+RE) \nCentralized (Upper bound) \n0.856 \n0.870 \n0.852 \n0.863 \n0.867 \n0.831 \n0.803 \n0.746 \nZero-shot (Baseline) \n0.345 \n0.437 \n0.256 \n0.330 \n0.203 \n0.056 \n0.091 \n0.096 \nSingle site (Avg, baseline) \n0.803 \n0.843 \n0.806 \n0.801 \n0.648 \n0.721 \n0.730 \n0.667 \nFed-MedLoRA (Ours) \n0.847 \n0.858 \n0.838 \n0.850 \n0.850 \n0.817 \n0.778 \n0.693 \nFed-MedLoRA+ (Ours) \n0.850 \n0.866 \n0.840 \n0.860 \n0.860 \n0.826 \n0.793 \n0.740 \nA: MIMIC-III (NER+RE) \nB: MTSamples (NER) \nFed-MedLoRA (Ours) \n0.827 \n0.842 \n0.813 \n0.839 \n0.815 \n0.744 \n0.701 \n0.675 \nFed-MedLoRA+ (Ours) \n0.838 \n0.851 \n0.823 \n0.846 \n0.832 \n0.768 \n0.741 \n0.709 \nThree-site experiment \nA: MIMIC-III (NER+RE) \nB: MTSamples (NER+RE) \nC: UTP (NER+RE) \nCentralized (Upper bound) \n0.840 \n0.867 \n0.909 \n0.852 \n0.851 \n0.788 \n0.924 \n0.745 \nZero-shot (Baseline) \n0.345 \n0.437 \n0.256 \n0.330 \n0.203 \n0.056 \n0.091 \n0.096 \nSingle site (Avg, baseline) \n0.775 \n0.841 \n0.798 \n0.756 \n0.656 \n0.703 \n0.778 \n0.659 \nFed-MedLoRA (Ours) \n0.809 \n0.861 \n0.897 \n0.833 \n0.744 \n0.759 \n0.916 \n0.711 \nFed-MedLoRA+ (Ours) \n0.803 \n0.862 \n0.898 \n0.844 \n0.771 \n0.762 \n0.913 \n0.713 \nA: MIMIC-III (NER+RE) \nB: MTSamples (NER+RE) \nC: UTP (NER) \nFed-MedLoRA (Ours) \n0.775 \n0.837 \n0.828 \n0.784 \n0.681 \n0.714 \n0.765 \n0.683 \nFed-MedLoRA+ (Ours) \n0.781 \n0.843 \n0.816 \n0.797 \n0.742 \n0.729 \n0.772 \n0.694 \n \n3.3 Results on practical feasibility \n3.3.1 Assessment of model robustness on uneven task annotations cross sites \nBeyond accuracy, we further evaluated model robustness under a practical scenario in which \ndifferent sites provided annotations for different tasks. Table 8 summarizes results from both \ntwo-site and three-site experiments. (In the two-site setting, Site A (MIMIC-III) included \nannotations for both Named Entity Recognition (NER) and Relation Extraction (RE), whereas Site \nB (MTSamples) included only NER annotations. In the three-site setting, Site C (UTP) contained \nonly NER annotations, while Sites A and B provided both NER and RE.) Importantly, both single-\nsite fine-tuned BERT and LLM baselines are not applicable in scenarios with uneven task \nannotations. We compared Fed-MedLoRA and Fed-MedLoRA+ trained under these uneven \nannotation conditions directly with the models trained using complete task annotations. \nAs shown in Table 8, both Fed-MedLoRA and Fed-MedLoRA+ maintained strong performance \ndespite missing task annotations across sites, consistently outperforming single-site fine-tuning \nbaselines that used complete annotations. Compared with their fully annotated counterparts, \nFed-MedLoRA+ had only a slight decrease in NER performance (1.2%‚Äì1.7% reduction in F1-\nscore across test sets). For RE, the performance drops were somewhat larger but still moderate, \nwith absolute F1-score decreases of 2.8%‚Äì5.8%.  \n3.3.2 Assessment of communication costs and computational resources  \nIn addition, we evaluated the communication overhead and computational resource \nrequirements of Fed-MedLoRA and Fed-MedLoRA+. Specifically, we report per-round \ncommunication volumes (number of parameters transmitted and their storage size), peak GPU \nmemory consumptions for training and inference, and total training time (in GPU-hours). To \n"}, {"page": 19, "text": "better quantify the trade-off between model accuracy and computational efficiency, we \nadditionally trained Fed-MedLoRA and Fed-MedLoRA+ using LLaMA3-1B as the backbone, in \ncontrast to the 8B model used in the primary evaluations. \nCommunication cost. Figure 2A compares the communication overhead of Fed-MedLoRA+ \nagainst full-parameter fine-tuning. Recall that both Fed-MedLoRA and Fed-MedLoRA+ update \nonly the low-rank adapters, rather than the full model parameters. For LLaMA3-8B model \n(8,030,261,248 parameters), full-parameter fine-tuning requires about 29.92 GB per site per \nround in float 32. This translates to a total transmission of 239 GB for the two-site experiment \nand 359 GB in the three-site experiment. In contrast, Fed-MedLoRA and Fed-MedLoRA+ update \nonly 41,943,040 parameters, corresponding to a 99.48% reduction in transmission volume. The \nresulting transmission requirements are 1.25 GB for the two-site experiment and 1.88 GB for \nthe three-site experiment. Similar reductions are observed for other LLM backbones.   \nGPU memory cost for training and inference. Figure 2B reports the peak GPU memory usage of \nFed-MedLoRA+ during training. Peak GPU memory consumption exceeds the raw parameter \nstorage because additional memory is required for gradients, activation checkpoints, optimizer \nstates (e.g., momentum and variance estimates). For Fed-MedLoRA+ with LLaMA3-8B, peak \ntraining memory was ~14.04 GB, making it feasible to train on a single consumer-grade GPU \ncard such as the NVIDIA RTX 4090 (16 GB)‚Äîwithout requiring high-end datacenter GPUs like \nthe H100 or H200, which may not be readily available in clinical environments. For inference, \nsince gradients and optimizer states are not present, the memory footprint was substantially \nlower‚Äî~6.79 GB, depending on batch size and sequence length. In practical terms, this lower \ninference footprint means that models trained using Fed-MedLoRA+ can be deployed on widely \navailable GPUs such as the RTX A6000 (12‚Äì16 GB) and RTX 3090/4090, which is crucial for \nclinical sites that lack training infrastructure but can perform local inference 54.  \nTraining time. Figure 2C presents the wall-clock training time in GPU-hours. For Fed-MedLoRA+ \nwith LLaMA3-8B, the total training time was 6.89 GPU-hours for the two-site experiment and \n6.98 GPU-hours for the three-site experiment. Comparable runtimes were observed for \nDeepSeek-R1-Distill-8B. \nAdditional analysis on smaller LLM backbones. Table 9 compares the performance of Fed-\nMedLoRA and Fed-MedLoRA+ when using LLaMA3-1B versus the larger 8B backbone models. \nOverall, the smaller 1B backbone had a ~3% decrease in NER accuracy and up to a 7% decrease \nin RE accuracy. As illustrated in Figure 2, the reduction in accuracy comes with substantial gains \nin efficiency. The 1B model reduced per-round communication cost (e.g., 344 MB vs. 1.25 GB in \nthe two-site experiment), GPU memory cost during training (2.15 GB vs. 14.04 GB) and \ninference (1.24 GB vs. 6.83 GB), and total GPU hours (1.82 h vs. 6.89 h). In practical terms, \ntraining Fed-MedLoRA and Fed-MedLoRA+ with the 1B model can be performed on widely \navailable GPUs such as the NVIDIA RTX 3060 Ti or RTX 4060 Ti (8 GB), while inference can be \nexecuted on standard or institution-managed laptops (e.g., devices equipped with an Apple M3 \nPro‚Äìclass chip or an equivalent x86 GPU with over 18 GB RAM). These results suggest the \npotential feasibility of FL with smaller LLM backbones in resource-constrained environments, \nwhere a modest decrease in accuracy is acceptable.  \n"}, {"page": 20, "text": "Table 9. Strict micro F1 scores of Fed-MedLoRA and Fed-MedLoRA+ on NER and RE tasks by fine-tuning on LLaMA3-\n1B and LLaMA3-8B models under two-site and three-site experiments.  \nTraining set \nMethods \nNER test set \nRE test set \nMIMIC-III \nMTSamples \nUTP \nI2B2 \nMIMIC-III \nMTSamples \nUTP \nI2B2 \nTwo-site experiment \nLLaMA3-1B \nFed-MedLoRA (Ours) \n0.814 \n0.821 \n0.802 \n0.834 \n0.815 \n0.798 \n0.757 \n0.677 \nFed-MedLoRA+ (Ours) \n0.821 \n0.829 \n0.817 \n0.831 \n0.831 \n0.802 \n0.774 \n0.728 \nLLaMA3-8B \nFed-MedLoRA (Ours) \n0.847 \n0.858 \n0.838 \n0.850 \n0.850 \n0.817 \n0.778 \n0.693 \nFed-MedLoRA+ (Ours) \n0.850 \n0.866 \n0.840 \n0.860 \n0.860 \n0.826 \n0.793 \n0.740 \nThree-site experiment \nLLaMA3-1B \nFed-MedLoRA (Ours) \n0.779 \n0.819 \n0.854 \n0.819 \n0.734 \n0.735 \n0.886 \n0.704 \nFed-MedLoRA+ (Ours) \n0.785 \n0.814 \n0.866 \n0.823 \n0.742 \n0.741 \n0.895 \n0.701 \nLLaMA3-8B \nFed-MedLoRA (Ours) \n0.809 \n0.861 \n0.897 \n0.833 \n0.744 \n0.759 \n0.916 \n0.711 \nFed-MedLoRA+ (Ours) \n0.803 \n0.862 \n0.898 \n0.844 \n0.771 \n0.762 \n0.913 \n0.713 \n \n \nFigure 2. Resource and communication comparison across models and federated settings. (A) Communication cost \n(storage size) of full-parameter fine-tuning versus Fed-MedLoRA+. For each backbone, we report per-site \ntransmission volume (bytes sent by a single client in a communication round per round, as well as the total \ntransmission volume for two-site and three-site experiments. (B) Peak GPU memory consumption (GB) observed \nwhen training LLaMA3-1B, LLaMA3-8B, and DeepSeek-R1-Distill-8B using Fed-MedLoRA+. The horizontal dashed \nline marks the 16 GB of VRAM on an NVIDIA RTX 4090 for reference, demonstrating that training an 8B backbone \nmodel is feasible on high-memory consumer GPUs. (C) Training time in GPU hours for the two-site and three-site \nexperiments.  \n3.6 Assessment of model scalability  \nMotivated by the above results, we further conducted simulations with up to ùëò = 10 \nparticipating sites to assess scalability. For each ùëò, the training data from MIMIC-III, MTSamples, \nand UTP were partitioned into ùëò random shards, with each shard treated as an independent \nPer-site per-round Total (2-site)\nTotal (3-site)\n10\n1\n100\n101\n102\nCommunication cost \n log scale\n3.74GB\n29.92GB\n44.88GB\n43MB\n344MB\n516MB\nLLaMA3-1B\nPer-site per-round Total (2-site)\nTotal (3-site)\n29.92GB\n239.36GB\n359.04GB\n160MB\n1.25GB\n1.88GB\nLLaMA3-8B\nPer-site per-round Total (2-site)\nTotal (3-site)\n29.92GB\n239.36GB\n359.04GB\n160MB\n1.25GB\n1.88GB\nDeepSeek-R1-Distill-8B\nFull parameter\nFed-MedLoRA\nLLaMA3-1B\nLLaMA3-8B\nDeepSeek-R1-Distill-8B\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nGPU memory cost (GB)\n2.15GB\n14.04GB\n14.03GB\n1.24GB\n6.83GB\n6.79GB\nTraining\nInference\nRTX 4090 (16 GB)\nLLaMA3-1B\nLLaMA3-8B\nDeepSeek-R1-Distill-8B\n0\n2\n4\n6\n8\nGPU hours (h)\n1.82h\n6.98h\n6.59h\n1.30h\n6.89h\n6.19h\n3-site\n2-site\nA\nB\nC\n"}, {"page": 21, "text": "site. Consistent with prior experiments, we compared the strict F1-scores of Fed-MedLoRA+ \nand single-site fine-tuned LLM baselines using LLaMA3-1B as the backbone for both NER and \nRE, with centralized learning serving as the estimated upper-bound reference under both \ntraining/testing and independent benchmark settings.  \n \nFigure 3. Scalability evaluation: exact micro F1-scores of federated fine-tuning (Fed-MedLoRA and Fed-MedLoRA+) \nfor LLaMA3-1B on NER and RE under different numbers of participating sites (ùëò=1, 2, 3, 4, 6, 8, 10).  \nFigure 3 summarizes the results. Overall, Fed-MedLoRA+ maintained performance nearly \nidentical to centralized learning when ùëò ‚â§ 5 and had only moderate degradation as ùëò increased \nto 10. The absolute performance drop at ùëò = 10 is 3.9% for NER and 7.5% for RE compared to \ncentralized learning. In contrast, the single-site fine-tuned LLM baseline showed sharp \ndegradation as ùëò increased. For instance, on the MIMIC-III dataset, Fed-MedLoRA+ achieved a \n21.8% higher F1-score than the single-site baseline for RE at ùëò =10.  \nWe also observed that RE is inherently more challenging and more sensitive to the number of \nparticipating sites than NER, which is consistent with our findings across both training/testing \nand independent benchmark settings, where RE consistently achieved lower scores than NER. \n1\n2\n3\n4\n6\n8\n10\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nStrict micro F1 score\nMIMIC-III NER\n1\n2\n3\n4\n6\n8\n10\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nMTSamples NER\n1\n2\n3\n4\n6\n8\n10\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nUTP NER\n1\n2\n3\n4\n6\n8\n10\nNumber of clients\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nStrict micro F1 score\nMIMIC-III RE\n1\n2\n3\n4\n6\n8\n10\nNumber of clients\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nMTSamples RE\n1\n2\n3\n4\n6\n8\n10\nNumber of clients\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nUTP RE\nSingle site\nFed-MedLoRA+\nCentralized\nA. Standard benchmark\nB. Independent benchmark\n1\n2\n3\n4\n6\n8\n10\nNumber of clients\n0.65\n0.70\n0.75\n0.80\n0.85\nStrict micro F1 score\nI2B2 NER\nSingle site\nFed-MedLoRA+\nCentralized\n1\n2\n3\n4\n6\n8\n10\nNumber of clients\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nStrict micro F1 score\nI2B2 RE\nSingle site\nFed-MedLoRA+\nCentralized\n"}, {"page": 22, "text": "In simulations with up to 10 sites, NER micro F1-scores remained above 0.79 across all test sets, \nwhereas RE showed greater variability, with a minimum F1-score of approximately 0.618 on the \nI2B2 dataset. \n4. Discussion \n4.1 The proposed approach suggests the feasibility and potential of federated learning for \nlarge language models in medical applications. \nWhile FL has been increasingly adopted in healthcare, most existing studies have focused on \nsmaller models such as BERT-based or logistic regression architectures. Many current \npublications are survey papers that primarily discuss the potential and challenges of extending \nFL to LLMs in medicine 25,55. A few pioneering works have explored adapting FL to vision \nfoundation models, primarily for disease diagnosis based on imaging data 27,31. This gap stems \nfrom both practical and technical barriers: (1) full-parameter federated training of LLMs is \ncomputationally expensive in terms of communication and memory requirements, often \nexceeding the resources available in clinical environments; (2) na√Øve aggregation of large \nparameter updates is unstable under severe client heterogeneity, which is particularly common \nin medical data; and (3) legal, infrastructural, and operational constraints continue to hinder \nmulti-institutional data collaboration in healthcare. Indeed, bridging FL and LLMs remains a \nfundamental challenge even in the general domain, as extensively discussed in recent reviews \n56,57. \nIn addition, we also included centralized training for both BERT-based and LLM-based models as \na reference. Centralized training pools data from all sites to train a single model, which is then \nevaluated on the same test sets. This configuration provides an empirical upper bound, \nrepresenting an ideal but impractical setting in healthcare due to privacy and governance \nconstraints. The implementation and hyperparameter details are provided in Section 5 \nMethods. \nOur work presents a federated, model-agnostic framework for training LLMs in medicine. The \nproposed Fed-MedLoRA and Fed-MedLoRA+ transmit only low-rank adapter updates, \ndramatically reducing communication costs, while Fed-MedLoRA+ further introduces adaptive, \ndata-aware aggregation to mitigate clinical data heterogeneity. As a pilot study, this work \ndemonstrates several key findings regarding the feasibility of federated LLMs in medicine. First, \nfederated fine-tuning of LLMs is effective. Systematic evaluations across three settings \nconsistently showed that both Fed-MedLoRA and Fed-MedLoRA+ outperformed existing BERT-\nbased and LLM-based baselines. Notably, Fed-MedLoRA+ achieved performance comparable to \ncentralized training. In the new-site case study, Fed-MedLoRA+ had the best performance with \na strict F1-score of 73% and a lenient F1-score of 85%, demonstrating that a new clinical site \ncould directly benefit from federated LLMs to bootstrap local model development even with \nlimited annotations. The results also suggest that federated LLMs can support multi-task \ntraining, where each site contributes different available task annotations. Second, the \nproposed approach improves both computational and communication efficiency. By \ntransmitting only low-rank adapter parameters, communication costs were reduced by 98.5% \ncompared with full-model fine-tuning. In practice, training federated LLMs at the 8B parameter \n"}, {"page": 23, "text": "scale using Fed-MedLoRA or Fed-MedLoRA+ requires only a single consumer-grade GPU (e.g., \nNVIDIA RTX 4090, 16 GB). Inference can be performed on commonly available GPUs such as the \nRTX A6000 (12‚Äì16 GB), RTX 3090/4090, or other high-memory workstation GPUs (‚â•8 GB \nVRAM). This efficiency improves accessibility and broadens the potential for federated LLM \ndevelopment in typical clinical research centers. Additional analysis on federated LLMs with the \n1B backbone further showed that they can be trained with widely accessible GPUs such as the \nNVIDIA RTX 3060 Ti or RTX 4060 Ti (8 GB), and inference can be performed on a standard laptop \n(e.g., devices equipped with an Apple M3 Pro‚Äìclass chip or an equivalent x86 GPU with over 18 \nGB RAM), with a trade-off in accuracy (for clinical IE, as the results suggested, the accuracy \ndecrease is moderate: 3% on NER and up to 7% on RE for clinical IE)). Third, the proposed \nframework demonstrates the potential of scalability and robustness. Fed-MedLoRA+ \nmaintained stable performance as the number of simulated sites increased to 10, with F1-score \ndifferences within ~2% of centralized training. Also, as previously discussed, the performance \nremained robust when participating sites provided different task annotations. \n4.2 Federated learning substantially improves LLMs for clinical information extraction as a \ndownstream case study \nWe evaluated the accuracy and practical feasibility of the proposed framework using clinical IE \nas a downstream case study. Many studies have shown that LLMs have suboptimal \nperformance for clinical IE, often producing inconsistent predictions, incomplete extractions, \nand hallucinated entities or relations 15,58. To date, fine-tuning domain-specific BERT variants \nremains the state-of-the-art approach for clinical IE. Recent efforts have explored LLM \ninstruction-tuning for this purpose 15,38, but most have relied on single-institution datasets or \ncombined multiple datasets directly‚Äîapproaches that overlook the privacy and governance \nconstraints central to real-world clinical data sharing.  \nThe results demonstrate that both Fed-MedLoRA and Fed-MedLoRA+ consistently \noutperformed existing methods. Specifically, Fed-MedLoRA+ improved zero-shot LLM \nperformance by up to 65% absolute F1 (e.g., from 0.345 to 0.850 for NER and from 0.203 to \n0.860 for RE in two-site LLaMA3 experiments) and by ~25% over single-site fine-tuned LLMs \n(e.g., from 0.589 to 0.831 on NER with DeepSeek-R1-Distill). Compared with fine-tuned BERT \nbaselines, Fed-MedLoRA+ also achieved large gains‚Äîimproving RE by ~70% on UTP (0.793 vs. \n0.270) and ~50% on i2b2 (0.740 vs. 0.302) in the two-site experiment, with comparable \nimprovements (~50%) in the three-site experiment (0.713 vs. 0.219). Collectively, these results \ndemonstrate that federated learning could overcome key limitations of LLMs in clinical IE, \nachieving robust accuracy and cross-site generalization while maintaining data privacy. \nWhen to prefer LLMs over BERT-based models for clinical IE. While the proposed methods \nconsistently outperformed BERT-based approaches across all settings in this study, it is \nimportant to recognize that BERT-based models remain more lightweight and deployment-\nfriendly, with much fewer parameters and lower computational demands. Achieving superior \naccuracy with LLMs therefore involves trade-offs that downstream users should consider when \ndetermining when to LLMs in practice. Our head-to-head comparisons highlight three scenarios \nwhere LLMs offer distinct advantages to make their best use. First, independent or \nheterogeneous patient cohorts. When training and testing share the same distribution, the \n"}, {"page": 24, "text": "performance gap between BERT and LLMs is modest (e.g., under the training/testing setting, \nFed-MedLoRA+ was ~4% higher F1 than Bio_ClinicalBERT for NER: 0.850 vs. 0.807 on MIMIC-III; \n0.866 vs. 0.827 for UTP). In contrast, on external populations, the advantage of LLMs becomes \nmore distinct: Fed-MedLoRA+ achieved ~14.8% higher F1 on UTP (0.840 vs. 0.692) and ~9.2% \nhigher on I2B2 (0.860 vs. 0.768). In the new-site case study, Fed-MedLoRA+ outperformed the \ncentralized Bio_ClinicalBERT by ~13% (0.732 vs. 0.600). Second, extractive tasks requiring \nbeyond NER. The results also demonstrate that more challenging tasks such as RE saw a \nconsistent performance drop compared to NER. As mentioned, RE requires modeling cross-\nentity semantic relations over longer context. The results show that BERT-based models \nsuffered a notable performance drop on RE, whereas LLMs had more robust performance. For \ninstance, in the two-site experiment, Fed-MedLoRA+ achieved 0.860 F1 compared with 0.434 \nfor Bio_ClinicalBERT. Third, multi-task uses with partial task annotations. Instruction-tuned \nLLMs naturally support a single model capable of performing multiple tasks (e.g., both NER and \nRE in this study), and prior research has shown that a single LLM can adapt to a wide range of \nmedical tasks, from extractive to generative, through instruction tuning 59,60. Importantly, \nbeyond their multi-task flexibility, our results demonstrate that Fed-MedLoRA and Fed-\nMedLoRA+ maintained robust performance even when participating sites contributed different \ntask annotations‚Äîfor example, Site A provided NER and RE labels, while Site B included only \nNER. Compared with their fully annotated counterparts, Fed-MedLoRA+ had only a slight \ndecrease in NER performance (1.2%‚Äì1.7% reduction in F1-score across test sets). For RE, the \nperformance drops were somewhat larger but still moderate, with absolute F1-score decreases \nof 2.8%‚Äì5.8%. This configuration mirrors realistic clinical settings, where annotation availability \nvaries across institutions, and it is often impractical for all sites to provide complete task labels. \nIn contrast, standard BERT-based pipelines are typically task-specific; for instance, in this study, \nwe followed established fine-tuning protocols to train separate BERT models for NER and RE. \nAlthough multi-task BERT variants have been proposed 61, they require customized \narchitectures and are not readily extensible to generative tasks.  \n4.3 Limitations and future work \nDespite the promising results, several important limitations of this study warrant discussion. \nHere, we highlight key aspects not addressed in the current work and call for future community \nefforts to translate FL for LLMs from experimental settings into real-world medical practice. \nFirst, privacy protection within the federated LLM paradigm requires further investigation. \nAlthough our framework transmits only low-rank adapter parameters rather than full model \nweights or raw data, we did not explicitly incorporate additional privacy-preserving \nmechanisms. Prior studies have identified potential security risks of LLMs in medical \napplications, including the inadvertent memorization of sensitive data 62. Systematic evaluation \nof standard privacy-preserving techniques‚Äîsuch as secure aggregation 63, differential privacy \n64,65, and strict access control 66‚Äîalong with their efficiency trade-offs in federated LLM \ntraining, is essential.  \nSecond, task selection and optimization for federated LLM training in medicine remain open \nquestions. In this study, we used clinical IE as a representative downstream task to \ndemonstrate the effectiveness of the proposed framework. However, LLMs can be adapted to a \n"}, {"page": 25, "text": "wide range of clinical tasks 7,8. Systematic evaluation across diverse medical applications‚Äîand \nidentification of which tasks and data regimes benefit most from federated learning‚Äîwill be \ncritical for guiding future model development and prioritizing high-impact clinical use cases.  \nThird, deployment and evaluation in real clinical environments are necessary to inform \npractical adoption. While this study evaluated feasibility in terms of robustness, scalability, and \nefficiency, real-world deployment introduces additional challenges, such as network reliability, \ncoordination and orchestration across institutions, hardware heterogeneity, and site-specific \ndata governance and compliance requirements 67,68. Addressing these factors through \nprospective deployments and longitudinal evaluations will be essential for translating federated \nLLM frameworks into operational healthcare systems 69.  \n5. Methods \nIn this section, we first describe how LLMs are fine-tuned at a single site, followed by a detailed \nexplanation of the proposed Fed-MedLoRA and Fed-MedLoRA+ frameworks. \n5.1 Supervised fine-tuning of LLMs in the single-site setting \nThe single-site setting is arguably the most common scenario in medical applications, where \neach site independently performs supervised fine-tuning of LLMs on its own local dataset 30,70. \nIn practice, data and models are not shared across sites due to the sensitivity of clinical data. \nFormally, holds a local dataset ùê∑! = {(ùë•!,#, ùë¶!,#)}#$%\n&! , where ùë•!,# denotes a dataset instance and \nùëõ! is the dataset size. Site ùëò will fine-tune a LLM backbone denoted as ùëä(() ‚àà‚Ñù*√ó,, where ùëë \nand ùëô denote the matrix dimensions.  \nSupervised fine-tuning of LLMs is typically performed through instruction tuning, where each \ndataset instance is reformulated as a unified instruction containing a task description (e.g., \nNER), inputs, and desired outputs with gold-standard labels (e.g., manually annotated entities) \n71. The loss function for fine-tuning all parameters of the LLM is: \nùêø!(ùëä) =\n%\n&! ‚àë\nùëô(ùë•!,#, ùë¶!,#; ùëä)\n(-,.)‚àà0!\n, \nwhere ùëô denotes the cross-entropy loss between the predicted token probability distribution \nand the gold-standard output tokens. Because fine-tuning all parameters of an LLM is \ncomputationally expensive, in practice, parameter-efficient instruction tuning methods such as \nLow-Rank Adaptation (LoRA) are adopted 72. The main idea is to only fine-tune a subset of \nparameters while preserving the accuracy. In LoRA, the trainable weight update ‚àÜùëä‚àà‚Ñù*√ó, is \ndecomposed into two low-rank matrices, ‚àÜùëä= ùêµùê¥ where ùêµ‚àà‚Ñù*√ó1, ùê¥‚àà‚Ñù1√ó,, ùëü‚â™\nmin(ùëë, ùëô). During fine-tuning, only the low-rank matrices ùêµ and ùê¥ are updated, while the \noriginal weight matrix ùëä remains frozen. The updated loss function becomes: \nùêø!(ùëä+ ùõ•ùëä) =\n%\n&! ‚àë\nùëô(ùë•!,#, ùë¶!,#; +ùõ•ùëä)\n(-,.)‚àà0!\n, \n5.2 Proposed Fed-MedLoRA and Fed-MedLoRA+ \nIn contrast to the single-site setting, federated learning enables collaborative model training \nacross multiple institutions while keeping raw data local. In each round, site ùëò fine-tunes the \nmodel on its local dataset, transmits updates to a coordinating server, and receives aggregated \n"}, {"page": 26, "text": "parameters, which are then used to continue local training. We proposed two federated \nlearning variants, Fed-MedLoRA and Fed-MedLoRA+, detailed below.  \n5.2.1 Fed-MedLoRA \nWe first introduce Fed-MedLoRA, a simple yet effective algorithm inspired by the classical \nFedAvg 73,74. FedAvg, which has been widely used in federated learning, aggregates full model \nweights from participating clients and averages them at the server 30,75. However, for LLMs, \ntransmitting the entire set of model weights at each round introduces significant \ncommunication overhead. To address this, Fed-MedLoRA requires clients to send only their \nLoRA updates. The procedure, summarized in Algorithm 1, consists of three phases. \nInitialization. First, the server and clients store a frozen LLM backbone (e.g., LLaMA) with \nparameters ùëä(() ‚àà‚Ñù*√ó,. The server also initializes the global LoRA modules ùêµ(() ‚àà‚Ñù*√ó1 and \nùê¥(() ‚àà‚Ñù1√ó, with rank ùëü‚â™min(ùëë, ùëô), and prepares a unified instruction format for fine-tuning. \nIn this study, we designed instruction formats for NER and RE tasks consistent with prior work 15 \n(see Supplementary S1.1 and S1.2) and distributed them to all clients. \nLocal updates. At communication round ùë°, the server samples a subset ùëÄ2 of ùëö clients and \ntransmits the global LoRA modules ùêµ(2), ùê¥(2) to them. Each selected client ùëò‚ààùëÄ2 initializes its \nlocal LoRA modules with the received values and performs ùê∏ iterations of local training on ùê∑!, \nproducing updated modules ùêµ!\n(3) ‚àà‚Ñù*√ó1 and ùê¥!\n(3) ‚àà‚Ñù1√ó,. The client then returns these \nupdated modules to the server. \nServer aggregation. The server updates the received LoRA modules by ùêµ!\n(24%) = ùêµ!\n(3), ùê¥!\n(24%) =\nùê¥!\n(3) and aggregates them to obtain the new global modules with:  \nùêµ(24%) =\n%\n5 ‚àë\n&!\n6 ùêµ!\n(24%)\n!‚àà7\"\n, ùê¥(24%) =\n%\n5 ‚àë\n&!\n6 ùê¥!\n(24%)\n!‚àà7\"\n, \nwhere ùëõ! is the local dataset size and ùëÅ= ‚àë\nùëõ!\n8\n!$%\n is the size of all local datasets. These steps \nare repeated for ùëá communication rounds. The final global model is obtained by merging the \nfrozen backbone with the aggregated LoRA modules ùëä(9) = ùëä(() + ùêµ(9)ùê¥(9).  \nCompared to transmitting and aggregating full model weights, which requires transmitting \nùëë√ó ùëô parameters, Fed-MedLoRA reduces each round of communication to ùëëùëü+ ùëüùëô parameters \nper client. When ùëü‚â™min (ùëë, ùëô), this leads to substantial reductions in both communication \ncost and GPU memory footprint.  \n5.2.2 Fed-MedLoRA+  \nWhile Fed-MedLoRA substantially reduces the communication cost of fine-tuning LLMs in \nfederated settings, it suffers a core limitation of FedAvg: all client updates are treated equally \nand aggregated via simple averaging. In clinical FL, this assumption is often violated‚Äîlocal \ndatasets can vary significantly in population characteristics, class balance, annotation quality, \nand the prevalence of rare labels 15,30,70. This heterogeneity (e.g., skewed label distributions, \nnoisy or inconsistent annotations, and unrepresentative local samples) can cause simple \naveraging to degrade global model accuracy and amplify the influence of harmful updates 75,76.  \n"}, {"page": 27, "text": "To address this challenge, we propose Fed-MedLoRA+ (an overview is provided in Figure 1A), \nwhich dynamically estimates the influence of each client based on validation performance and \nperforms adaptive, data-aware aggregation. Specifically, we first evaluate client updates on a \nsmall holdout validation set and compute influence scores that reflect how much each update \nimproves or harms validation performance.  These scores are then used to scale client updates \nduring the server aggregation step. By down-weighting noisy or non-representative updates \nand up-weighting updates that significantly improve generalization, Fed-MedLoRA+ reduces the \nnegative impact of local heterogeneity and builds a more robust global model on unseen sites. \nThe procedure is summarized in Algorithm 2. As with Fed-MedLoRA, the process consists of \nthree stages‚Äîinitialization, local updates, and server aggregation‚Äîwith the main distinction \nbeing the calculation of client influence and dynamic aggregation, described below. \nAssumption. We assume the server retains a small validation set (e.g., five clinical notes) ùê∑: =\n{(ùë•:,#, ùë¶:,#)}#$%\n&#  with ùëõ: ‚â™ùëõ! for all clients ùëò‚àà[ùêæ]. This assumption is standard in many \nfederated learning deployments 75,76 and is feasible in clinical contexts, where a small amount \nof de-identified or publicly available labeled data can be used for validation in practice. \nClient influence score. During the local update stage, let ùëÄ2 denote the subset of ùëö clients \nselected at round ùë°, each of which sends its updated LoRA modules ùêµ!\n(24%) and ùê¥!\n(24%) to the \nserver. Instead of averaging, the server calculates an influence score for each client. Specifically, \nthe server first forms the client-specific model ùëä!\n(24%) = ùëä(() + ùêµ!\n(24%)ùê¥!\n(24%), and evaluates it \non the validation set ùê∑:: \nùëô!\n: = ‚àí1\nùëõ:\nM\nùë¶:,; log ùëÉ(ùë¶:,;|ùë•:,;; ùëä!\n(24%))\n&#\n;$%\n \nwhere ùëÉ(‚àô)  denotes the predictive distribution. A softmax is then applied to normalize the \nvalidation losses: \nùêº!\n(2) =\n<=> (@,!\n#)\n‚àë\n<=> (@,!\n#)\n$‚àà&(\")\n, \nThis assigns larger ùêº!\n(2) to clients with smaller validation loss. For numerical stability, one may \ncompute exp (‚àíùëô!\n: ‚àíùëê) with ùëê= ùëöùëñùëõ#(‚àíùëô#\n:) in practice. Each influence score is then combined \nwith the client‚Äôs local dataset size ùëõ!  to form a data-aware aggregation weight: \nùê∂!\n(2) =\n&!B!\n(\")\n‚àë\n&$B$\n(\")\n$‚àà&(\")\n, so that ‚àë\nùê∂!\n(2)\n!‚àà7(\")\n= 1 \nNote that when all ùêº!\n(2) are equal, this reduces to standard FedAvg. \nInfluence-aware aggregation. Using the weights {ùê∂!\n(2)}, the server updates the global LoRA \nmodules as:   \nùêµ(24%) =\n%\n5 ‚àë\nùê∂!\n(2)ùêµ!\n(24%)\n!‚àà7(\")\n, ùê¥(24%) =\n%\n5 ‚àë\nùê∂!\n(2)ùê¥!\n(24%)\n!‚àà7(\")\n, \nand the updated global model is ùëä(24%) = ùëä(() + ùêµ(24%)ùê¥(24%).  \n"}, {"page": 28, "text": "Because the weights {ùê∂!\n(2)}  form a convex probability distribution, under standard smoothness \nand bounded-variance assumptions, Fed-MedLoRA+ inherits the same convergence guarantees \nas FedAvg.  \n \nImplementation and hyperparameters. The proposed Fed-MedLoRA and Fed-MedLoRA+ \nframeworks are backbone-agnostic. Two representative open-weight LLMs, LLaMA3-8B and \nDeepSeek-R1-Distill-8B, were selected as backbone models. As described in Section 4.1 \nEvaluation overview, we conducted head-to-head comparisons under both zero-shot and \nsingle-site fine-tuning settings using the same LLM backbones as baselines.  \nFor single-site fine-tuning of LLMs, we applied QLoRA (4-bit quantization with low-rank \nadapters) to the decoder layers. Adapters were attached to all 32 decoder layers with a rank of \nùëü = 16, scaling factor ùõº = 64, and dropout rate of 0.05. Training was performed with a learning \nrate of 2 √ó 10‚Åª‚Å¥, 2 epochs, batch size 4, warmup ratio 0.05, and a maximum input length of 800 \ntokens. During inference, we set the temperature to 0 to minimize randomness 77. \n"}, {"page": 29, "text": "For Fed-MedLoRA and Fed-MedLoRA+, we adopted the same LLM fine-tuning hyperparameters \nfor direct comparison, with aggregation rounds set to 2. For Fed-MedLoRA+, which requires a \nvalidation set, we used 5 records for validation.  \nIn addition, we compared against fine-tuned domain-specific BERT models (Bio_ClinicalBERT). \nFine-tuning used a learning rate of 2 √ó 10‚Åª‚Å¥, 10 epochs, batch size 64, and a maximum input \nlength of 512 tokens, consistent with prior studies 15. \nCode availability \nThe codes are publicly available via https://github.com/Yale-BIDS-Chen-Lab/FL_LLM_Med.  \nAcknowledgment  \nThis study is supported by the National Institutes of Health grant 1R01LM014604.  \nAuthor Contribution  \nA.L. and Q.C. designed the research. A.L., Y.C., W.L., Y.Y., H.Y., H.K., W.Z., Y.Z., H.P., Y.R., X.A., \nH.Y., M.H., X.L., Y.T., L.M., H.X. and Q.C. wrote and edited the manuscript. All authors \ncontributed to discussion and manuscript preparation.  \nCompeting Interests Statement \nNone declared. \nReferences \n1. Liu, X. et al. Large Language Models for Outpatient Referral: Problem Definition, \nBenchmarking and Challenges. Preprint at https://doi.org/10.48550/arXiv.2503.08292 \n(2025). \n2. Wan, P. et al. Outpatient reception via collaboration between nurses and a large language \nmodel: a randomized controlled trial. Nat. Med. 30, 2878‚Äì2885 (2024). \n3. Liu, X. A generalist medical language model for disease diagnosis assistance | Nature \nMedicine. https://www.nature.com/articles/s41591-024-03416-6 (2025). \n4. Zhou, S. et al. Large language models for disease diagnosis: a scoping review. Npj Artif. \nIntell. 1, 9 (2025). \n5. Small, W. R. et al. Evaluating Hospital Course Summarization by an Electronic Health \nRecord‚ÄìBased Large Language Model. JAMA Netw. Open 8, e2526339 (2025). \n"}, {"page": 30, "text": "6. Van Veen, D. et al. Adapted large language models can outperform medical experts in \nclinical text summarization. Nat. Med. 30, 1134‚Äì1142 (2024). \n7. Liu, F. Application of large language models in medicine | Nature Reviews Bioengineering. \nhttps://www.nature.com/articles/s44222-025-00279-5 (2025). \n8. Thirunavukarasu, A. J. et al. Large language models in medicine. Nat. Med. 29, 1930‚Äì1940 \n(2023). \n9. Artsi, Y. et al. Challenges of Implementing LLMs in Clinical Practice: Perspectives. J. Clin. \nMed. 14, 6169 (2025). \n10. Evaluation and mitigation of the limitations of large language models in clinical decision-\nmaking | Nature Medicine. https://www.nature.com/articles/s41591-024-03097-1. \n11. Reddy, S. Beyond the Leaderboard: The Limitations of LLM Benchmarks and the Case for \nReal-World Clinical Evaluation. Preprint at \nhttps://doi.org/10.20944/preprints202511.1572.v1 (2025). \n12. Rahman, S. et al. Generalization in Healthcare AI: Evaluation of a Clinical Large Language \nModel. Preprint at https://doi.org/10.48550/arXiv.2402.10965 (2024). \n13. Garriga, R. et al. Combining clinical notes with structured electronic health records \nenhances the prediction of mental health crises. Cell Rep. Med. 4, (2023). \n14. Murff, H. J. et al. Automated Identification of Postoperative Complications Within an \nElectronic Medical Record Using Natural Language Processing. JAMA 306, 848‚Äì855 (2011). \n15. Hu, Y. et al. Information Extraction from Clinical Notes: Are We Ready to Switch to Large \nLanguage Models? Preprint at https://doi.org/10.48550/arXiv.2411.10020 (2025). \n"}, {"page": 31, "text": "16. Zhang, A., Xing, L., Zou, J. & Wu, J. C. Shifting machine learning for healthcare from \ndevelopment to deployment and from models to data. Nat. Biomed. Eng. 6, 1330‚Äì1345 \n(2022). \n17. Subasri, V. et al. Detecting and Remediating Harmful Data Shifts for the Responsible \nDeployment of Clinical AI Models. JAMA Netw. Open 8, e2513685 (2025). \n18. Wen, J. et al. A survey on federated learning: challenges and applications. Int. J. Mach. \nLearn. Cybern. 14, 513‚Äì535 (2023). \n19. Abdulrahman, S. et al. A Survey on Federated Learning: The Journey From Centralized to \nDistributed On-Site Learning and Beyond. IEEE Internet Things J. 8, 5476‚Äì5497 (2021). \n20. Sun, M. et al. Federated Learning for Large Models in Medical Imaging: A Comprehensive \nReview. Preprint at https://doi.org/10.48550/arXiv.2508.20414 (2025). \n21. Guevara, M. et al. Large language models to identify social determinants of health in \nelectronic health records. Npj Digit. Med. 7, 6 (2024). \n22. JMIR Medical Informatics - Construction of Cohorts of Similar Patients From Automatic \nExtraction of Medical Concepts: Phenotype Extraction Study. \nhttps://medinform.jmir.org/2022/12/e42379/. \n23. Dou, M., Tang, J., Tiwari, P., Ding, Y. & Guo, F. Drug‚ÄìDrug Interaction Relation Extraction \nBased on Deep Learning: A Review. ACM Comput Surv 56, 158:1-158:33 (2024). \n24. Peng, L. et al. An in-depth evaluation of federated learning on biomedical natural language \nprocessing for information extraction. Npj Digit. Med. 7, 1‚Äì9 (2024). \n25. Maity, S. & Saikia, M. J. Large Language Models in Healthcare and Medical Applications: A \nReview. Bioengineering 12, 631 (2025). \n"}, {"page": 32, "text": "26. Ren, C. et al. Advances and Open Challenges in Federated Foundation Models. Preprint at \nhttps://doi.org/10.48550/arXiv.2404.15381 (2024). \n27. Flow of Knowledge: Federated Fine-Tuning of LLMs in Healthcare under Non-IID Conditions. \nhttps://arxiv.org/html/2510.00543v1. \n28. Jiang, W. et al. Federated Large Language Models: Feasibility, Robustness, Security and \nFuture Directions. Preprint at https://doi.org/10.48550/arXiv.2505.08830 (2025). \n29. Yao, Y. et al. Federated Large Language Models: Current Progress and Future Directions. \nPreprint at https://doi.org/10.48550/arXiv.2409.15723 (2024). \n30. Kuo, T.-T., Gabriel, R. A., Koola, J., Schooley, R. T. & Ohno-Machado, L. Distributed cross-\nlearning for equitable federated models - privacy-preserving prediction on data from five \nCalifornia hospitals. Nat. Commun. 16, 1371 (2025). \n31. Feng, L. & Chen, S. Taming Vision-Language Models for Federated Foundation Models on \nHeterogeneous Medical Imaging Modalities. in Proceedings of the 2025 International \nConference on Multimedia Retrieval 303‚Äì311 (Association for Computing Machinery, New \nYork, NY, USA, 2025). doi:10.1145/3731715.3733441. \n32. A survey on clinical natural language processing in the United Kingdom from 2007 to 2022 | \nnpj Digital Medicine. https://www.nature.com/articles/s41746-022-00730-6. \n33. Wei, C.-H. et al. PubTator 3.0: an AI-powered literature resource for unlocking biomedical \nknowledge. Nucleic Acids Res. 52, W540‚ÄìW546 (2024). \n34. Keloth, V. K. et al. Social determinants of health extraction from clinical notes across \ninstitutions using large language models. NPJ Digit. Med. 8, 287 (2025). \n"}, {"page": 33, "text": "35. Levra, A. G. et al. A large language model-based clinical decision support system for syncope \nrecognition in the emergency department: A framework for clinical workflow integration. \nEur. J. Intern. Med. 131, 113‚Äì120 (2025). \n36. Chen, Q. Benchmarking large language models for biomedical natural language processing \napplications and recommendations | Nature Communications. \nhttps://www.nature.com/articles/s41467-025-56989-2 (2025). \n37. Hu, Y. et al. Improving large language models for clinical named entity recognition via \nprompt engineering. J. Am. Med. Inform. Assoc. 31, 1812‚Äì1820 (2024). \n38. Liu, L. et al. Human-level information extraction from clinical reports with fine-tuned \nlanguage models. 2024.11.18.24317466 Preprint at \nhttps://doi.org/10.1101/2024.11.18.24317466 (2024). \n39. Johnson, A., Pollard, T. & Mark, R. MIMIC-III Clinical Database. PhysioNet \nhttps://doi.org/10.13026/C2XW26 (2015). \n40. Transcribed Medical Transcription Sample Reports and Examples - MTSamples. \nhttps://mtsamples.com/. \n41. Uzuner, √ñ., South, B. R., Shen, S. & DuVall, S. L. 2010 i2b2/VA challenge on concepts, \nassertions, and relations in clinical text. J. Am. Med. Inform. Assoc. JAMIA 18, 552 (2011). \n42. Farmaan, M. Understanding Named Entity Recognition Evaluation Metrics with \nImplementation in Scikit-Learn. featurepreneur \nhttps://medium.com/featurepreneur/understanding-named-entity-recognition-evaluation-\nmetrics-with-implementation-in-scikit-learn-d94adbdfeb62 (2024). \n"}, {"page": 34, "text": "43. emilyalsentzer/Bio_ClinicalBERT ¬∑ Hugging Face. \nhttps://huggingface.co/emilyalsentzer/Bio_ClinicalBERT. \n44. Alsentzer, E. et al. Publicly Available Clinical BERT Embeddings. Preprint at \nhttps://doi.org/10.48550/arXiv.1904.03323 (2019). \n45. Lee, J. et al. BioBERT: a pre-trained biomedical language representation model for \nbiomedical text mining. Bioinformatics 36, 1234‚Äì1240 (2020). \n46. Grattafiori, A. et al. The Llama 3 Herd of Models. Preprint at \nhttps://doi.org/10.48550/arXiv.2407.21783 (2024). \n47. deepseek-ai/DeepSeek-R1-Distill-Llama-8B ¬∑ Hugging Face. \nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B (2025). \n48. Guo, P. et al. Selective Aggregation for Low-Rank Adaptation in Federated Learning. \nPreprint at https://doi.org/10.48550/arXiv.2410.01463 (2025). \n49. Chen, Q. et al. Multimodal, multitask, multiattention (M3) deep learning detection of \nreticular pseudodrusen: Toward automated and accessible classification of age-related \nmacular degeneration. J. Am. Med. Inform. Assoc. 28, 1135‚Äì1148 (2021). \n50. Lenskjold, A. Artificial intelligence tools trained on human-labeled data reflect human \nbiases: a case study in a large clinical consecutive knee osteoarthritis cohort | Scientific \nReports. https://www.nature.com/articles/s41598-024-75752-z (2024). \n51. A Comprehensive Survey on Relation Extraction: Recent Advances and New Frontiers | ACM \nComputing Surveys. https://dl.acm.org/doi/10.1145/3674501. \n52. Sheikhalishahi, S. et al. Natural Language Processing of Clinical Notes on Chronic Diseases: \nSystematic Review. JMIR Med. Inform. 7, e12239 (2019). \n"}, {"page": 35, "text": "53. Wang, Y. et al. Clinical information extraction applications: A literature review. J. Biomed. \nInform. 77, 34‚Äì49 (2018). \n54. Teo, Z. L. et al. Federated machine learning in healthcare: A systematic review on clinical \napplications and technical architecture. Cell Rep. Med. 5, 101419 (2024). \n55. Hu, Y., Xu, C., Lin, B., Yang, W. & Tang, Y. Y. Medical multimodal large language models: A \nsystematic review. Intell. Oncol. 1, 308‚Äì325 (2025). \n56. Bytez.com et al. FLoRA: Federated Fine-Tuning Large Language Models with ... \nhttps://bytez.com/docs/neurips/95025/paper (2024). \n57. Cho, Y. J., Liu, L., Xu, Z., Fahrezi, A. & Joshi, G. Heterogeneous LoRA for Federated Fine-\ntuning of On-Device Foundation Models. in Proceedings of the 2024 Conference on \nEmpirical Methods in Natural Language Processing (eds Al-Onaizan, Y., Bansal, M. & Chen, \nY.-N.) 12903‚Äì12913 (Association for Computational Linguistics, Miami, Florida, USA, 2024). \ndoi:10.18653/v1/2024.emnlp-main.717. \n58. Nagar, A. et al. LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction. in \nThe Sixth Workshop on Insights from Negative Results in NLP (eds Drozd, A., Sedoc, J., \nTafreshi, S., Akula, A. & Shu, R.) 106‚Äì120 (Association for Computational Linguistics, \nAlbuquerque, New Mexico, 2025). doi:10.18653/v1/2025.insights-1.11. \n59. Han, W. et al. MedINST: Meta Dataset of Biomedical Instructions. in Findings of the \nAssociation for Computational Linguistics: EMNLP 2024 (eds Al-Onaizan, Y., Bansal, M. & \nChen, Y.-N.) 8221‚Äì8240 (Association for Computational Linguistics, Miami, Florida, USA, \n2024). doi:10.18653/v1/2024.findings-emnlp.482. \n"}, {"page": 36, "text": "60. Tran, H., Yang, Z., Yao, Z. & Yu, H. BioInstruct: instruction tuning of large language models \nfor biomedical natural language processing. J. Am. Med. Inform. Assoc. 31, 1821‚Äì1832 \n(2024). \n61. Tong, Y., Chen, Y. & Shi, X. A Multi-Task Approach for Improving Biomedical Named Entity \nRecognition by Incorporating Multi-Granularity information. in Findings of the Association \nfor Computational Linguistics: ACL-IJCNLP 2021 (eds Zong, C., Xia, F., Li, W. & Navigli, R.) \n4804‚Äì4813 (Association for Computational Linguistics, Online, 2021). \ndoi:10.18653/v1/2021.findings-acl.424. \n62. Li, A. et al. Memorization in Large Language Models in Medicine: Prevalence, \nCharacteristics, and Implications. Preprint at https://doi.org/10.48550/arXiv.2509.08604 \n(2025). \n63. Fereidooni, H. et al. SAFELearn: Secure Aggregation for private FEderated Learning. in 2021 \nIEEE Security and Privacy Workshops (SPW) 56‚Äì62 (2021). \ndoi:10.1109/SPW53761.2021.00017. \n64. Li, X., Tram√®r, F., Liang, P. & Hashimoto, T. Large Language Models Can Be Strong \nDifferentially Private Learners. Preprint at https://doi.org/10.48550/arXiv.2110.05679 \n(2022). \n65. Charles, Z. et al. Fine-Tuning Large Language Models with User-Level Differential Privacy. \nPreprint at https://doi.org/10.48550/arXiv.2407.07737 (2024). \n66. Kerl, M., Bodin, U. & Schel√©n, O. Privacy-preserving attribute-based access control using \nhomomorphic encryption. Cybersecurity 8, 5 (2025). \n"}, {"page": 37, "text": "67. Tham, Y. C. et al. Building the world‚Äôs first truly global medical foundation model. Nat. Med. \n31, 3580‚Äì3585 (2025). \n68. Matta, S. S. & Bolli, M. FEDERATED LEARNING FOR PRIVACY-PRESERVING HEALTHCARE \nDATA SHARING: ENABLING GLOBAL AI COLLABORATION. Am. J. Sch. Res. Innov. 4, 320‚Äì351 \n(2025). \n69. Wirth, F. N., Meurers, T., Johns, M. & Prasser, F. Privacy-preserving data sharing \ninfrastructures for medical research: systematization and comparison. BMC Med. Inform. \nDecis. Mak. 21, 242 (2021). \n70. Federated target trial emulation using distributed observational data for treatment effect \nestimation | npj Digital Medicine. https://www.nature.com/articles/s41746-025-01803-y. \n71. Zhang, S. et al. Instruction Tuning for Large Language Models: A Survey. Preprint at \nhttps://doi.org/10.48550/arXiv.2308.10792 (2025). \n72. Hu, E. J. et al. LoRA: Low-Rank Adaptation of Large Language Models. Preprint at \nhttps://doi.org/10.48550/arXiv.2106.09685 (2021). \n73. McMahan, B., Moore, E., Ramage, D., Hampson, S. & Arcas, B. A. y. Communication-\nEfficient Learning of Deep Networks from Decentralized Data. in Proceedings of the 20th \nInternational Conference on Artificial Intelligence and Statistics 1273‚Äì1282 (PMLR, 2017). \n74. Li, A., Zhang, L., Wang, J., Han, F. & Li, X.-Y. Privacy-Preserving Efficient Federated-Learning \nModel Debugging. IEEE Trans. Parallel Distrib. Syst. 33, 2291‚Äì2303 (2022). \n75. Li, A. et al. FedCSS: Joint Client-and-Sample Selection for Hard Sample-Aware Noise-Robust \nFederated Learning. Proc ACM Manag Data 1, 212:1-212:24 (2023). \n"}, {"page": 38, "text": "76. Wang, J., Zhang, L., Li, A., You, X. & Cheng, H. Efficient Participant Contribution Evaluation \nfor Horizontal and Vertical Federated Learning. in 2022 IEEE 38th International Conference \non Data Engineering (ICDE) 911‚Äì923 (2022). doi:10.1109/ICDE53745.2022.00073. \n77. Patel, D. et al. Exploring Temperature Effects on Large Language Models Across Various \nClinical Tasks. 2024.07.22.24310824 Preprint at \nhttps://doi.org/10.1101/2024.07.22.24310824 (2024). \n \n  \n"}]}