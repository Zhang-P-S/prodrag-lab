{"doc_id": "arxiv:2602.15675", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.15675.pdf", "meta": {"doc_id": "arxiv:2602.15675", "source": "arxiv", "arxiv_id": "2602.15675", "title": "LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models", "authors": ["Ahmed Khaled Khamis", "Hesham Ali"], "published": "2026-02-17T15:58:27Z", "updated": "2026-02-17T15:58:27Z", "summary": "Despite the advances in neural text to speech (TTS), many Arabic dialectal varieties remain marginally addressed, with most resources concentrated on Modern Spoken Arabic (MSA) and Gulf dialects, leaving Egyptian Arabic -- the most widely understood Arabic dialect -- severely under-resourced. We address this gap by introducing NileTTS: 38 hours of transcribed speech from two speakers across diverse domains including medical, sales, and general conversations. We construct this dataset using a novel synthetic pipeline: large language models (LLM) generate Egyptian Arabic content, which is then converted to natural speech using audio synthesis tools, followed by automatic transcription and speaker diarization with manual quality verification. We fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on our dataset and evaluate against the baseline model trained on other Arabic dialects. Our contributions include: (1) the first publicly available Egyptian Arabic TTS dataset, (2) a reproducible synthetic data generation pipeline for dialectal TTS, and (3) an open-source fine-tuned model. All resources are released to advance Egyptian Arabic speech synthesis research.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.15675v1", "url_pdf": "https://arxiv.org/pdf/2602.15675.pdf", "meta_path": "data/raw/arxiv/meta/2602.15675.json", "sha256": "b0e80243bb5e7c5643df0c5be4e0b4005e061b3c8da999380c149bca21529c88", "status": "ok", "fetched_at": "2026-02-18T02:19:11.890526+00:00"}, "pages": [{"page": 1, "text": "LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal\nText-to-Speech Models\nAhmed Khaled Khamis\nGeorgia Institute of Technology\nakhamis6@gatech.edu\nHesham Ali\nNile University\nhe.ali@nu.edu.eg\nAbstract\nDespite the advances in neural text to speech\n(TTS), many Arabic dialectal varieties remain\nmarginally addressed, with most resources con-\ncentrated on Modern Spoken Arabic (MSA)\nand Gulf dialects, leaving Egyptian Arabic—\nthe most widely understood Arabic dialect—\nseverely under-resourced.\nWe address this\ngap by introducing NileTTS: 38 hours of tran-\nscribed speech from two speakers across di-\nverse domains including medical, sales, and\ngeneral conversations.\nWe construct this\ndataset using a novel synthetic pipeline: large\nlanguage models (LLM) generate Egyptian\nArabic content, which is then converted to natu-\nral speech using audio synthesis tools, followed\nby automatic transcription and speaker diariza-\ntion with manual quality verification. We fine-\ntune XTTS v2, a state-of-the-art multilingual\nTTS model, on our dataset and evaluate against\nthe baseline model trained on other Arabic\ndialects. Our contributions include: (1) the\nfirst publicly available Egyptian Arabic TTS\ndataset, (2) a reproducible synthetic data gen-\neration pipeline for dialectal TTS, and (3) an\nopen-source fine-tuned model. All resources\nare released to advance Egyptian Arabic speech\nsynthesis research.\n1\nIntroduction\nNeural text-to-speech (TTS) has made remark-\nable progress in recent years, with models like\nTacotron (Wang et al., 2017), FastSpeech (Ren\net al., 2019), and VITS (Kim et al., 2021) achiev-\ning near-human naturalness for high-resource lan-\nguages. More recently, multilingual TTS systems\nsuch as XTTS (Casanova et al., 2024) and VALL-\nE (Wang et al., 2023) have demonstrated impres-\nsive zero-shot voice cloning capabilities across dif-\nferent languages. However, this progress has not\nbeen evenly distributed, as low-resource languages\nand dialectal varieties remain significantly under-\nserved.\nArabic presents a particularly challenging case\nfor TTS research. While Modern Standard Ara-\nbic (MSA) has received considerable attention, the\nspoken reality of the Arab world is far more di-\nverse. Arabic has many regional dialects that differ\nsubstantially in phonology, vocabulary, and syn-\ntax, often to the point of mutual unintelligibility\n(Abu Kwaik et al., 2018). Among these, Egyptian\nArabic holds a unique position: spoken natively\nby over 100 million people and widely understood\nacross the Arab world due to Egypt’s dominant\nmedia presence, it is arguably the most accessible\nArabic variety.\nDespite its prominence, Egyptian Arabic re-\nmains under-resourced for speech synthesis. While\nprior work has explored Egyptian Arabic TTS\n(Azab et al., 2023), (Lodagala et al., 2025), existing\nresources are limited in scale, domain coverage, or\npublic availability. Current Arabic TTS systems\nmainly target MSA or Gulf dialects, leaving Egyp-\ntian Arabic speakers without state of the art tools.\nAs a result, Egyptian Arabic speakers lack access\nto quality TTS in applications like voice assistants\nand audiobooks.\nIn this work, we address this resource gap by\nintroducing NileTTS1 , a large-scale Egyptian Ara-\nbic TTS dataset along with a fine-tuned speech\nsynthesis model. Our dataset comprises 38 hours\nof transcribed Egyptian Arabic speech from two\nspeakers across three domains: medical, sales and\ncustomer service, and general conversation.\nA key contribution of our work is the novel syn-\nthetic data generation pipeline used to construct\nthe dataset. Rather than relying on costly man-\nual recording, we leverage recent advances in gen-\nerative AI: large language models (LLMs) gen-\nerate Egyptian Arabic content across diverse top-\nics, which is then converted to natural-sounding\n1Code: https://github.com/KickItLikeShika/Nil\neTTS\narXiv:2602.15675v1  [cs.CL]  17 Feb 2026\n"}, {"page": 2, "text": "speech using neural audio synthesis tools that\nsupport Egyptian Arabic.\nThe resulting audio\nis automatically transcribed using Whisper (Rad-\nford et al., 2022) and segmented into utterances,\nwith speaker identities assigned via ECAPA-TDNN-\nbased speaker diarization (Desplanques et al.,\n2020). Manual quality verification ensures tran-\nscription accuracy and speaker consistency. This\npipeline offers a reproducible and scalable ap-\nproach for creating TTS datasets for other low-\nresource dialects.\nTo demonstrate the utility of our dataset, we\nfine-tune XTTS v2 (Casanova et al., 2024), a state-\nof-the-art multilingual TTS model with zero-shot\nvoice cloning capabilities. We evaluate the fine-\ntuned model against the baseline XTTS v2, which\nwas trained on Arabic data from other dialectal\nvarieties. Our experiments show substantial im-\nprovements in intelligibility and speaker similarity.\nOur contributions are as follows:\n• We release NileTTS2, a large-scale Egyptian\nArabic TTS dataset comprising 38 hours of\ntranscribed speech across multiple domains.\n• We present a reproducible synthetic data\ngeneration pipeline combining LLM-based\ncontent generation, neural audio synthesis, au-\ntomatic transcription, and speaker diarization.\n• We provide an open-source fine-tuned XTTS\nmodel3 for Egyptian Arabic, serving as a base-\nline for future research.\nWe publicly release all resources to facilitate fur-\nther research in Egyptian Arabic speech synthesis.\n2\nRelated Work\n2.1\nArabic Text-to-Speech\nArabic TTS research has primarily focused on Mod-\nern Standard Arabic (MSA), with systems lever-\naging both traditional concatenative methods and\nneural approaches (Lodagala et al., 2025). For di-\nalectal Arabic, resources remain scarce. Notable\nexceptions include work on Gulf Arabic dialects,\nwhich benefit from commercial interest in the Gulf\nregion.\nFor Egyptian Arabic specifically, two prior ef-\nforts are most relevant. Azab et al. (2023) intro-\nduced EGYARA-23, a 20.5-hour dataset featuring\n2Dataset: https://huggingface.co/datasets/KickIt\nLikeShika/NileTTS-dataset\n3Model: https://huggingface.co/KickItLikeShika\n/NileTTS-XTTS\na single male speaker narrating news and general\nconversations, comprising 32,716 segments. While\nsubstantial in size, the dataset is limited to one\nspeaker and two domains. More recently, Loda-\ngala et al. (2025) presented SawtArabi, a multi-\ndialect Arabic speech corpus that includes approxi-\nmately one hour of Egyptian Arabic among several\nother varieties. While valuable for cross-dialectal\nresearch, the Egyptian Arabic portion is limited in\nscale for dedicated TTS training.\nOur work complements these efforts by provid-\ning a larger, more diverse resource: 38 hours of\nEgyptian Arabic speech from two speakers (male\nand female) across three distinct domains. Addi-\ntionally, we introduce a synthetic data generation\npipeline that offers a reproducible approach for\nfuture dataset expansion.\n2.2\nSynthetic Data for Speech\nSynthetic data generation has emerged as a promis-\ning approach for low-resource speech tasks. Prior\nwork has explored using TTS systems to gener-\nate training data for automatic speech recognition\n(Fazel et al., 2021), and text augmentation via\nLLMs has shown success in NLP tasks (Ding et al.,\n2024). Our work extends this paradigm to TTS\ndataset construction, using LLMs for content gen-\neration and neural audio synthesis for speech pro-\nduction—creating a fully synthetic pipeline that\nrequires no manual recording.\n2.3\nMultilingual TTS and XTTS\nRecent advances in multilingual TTS have en-\nabled models to synthesize speech across many lan-\nguages from a single model. XTTS v2 (Casanova\net al., 2024), built on a GPT-style architecture with\nvoice cloning capabilities, supports over 16 lan-\nguages including Arabic.\nHowever, its Arabic\ntraining data primarily covers MSA and Gulf di-\nalects. We finetune XTTS v2 on our Egyptian Ara-\nbic dataset to adapt it to this under-served variety.\n3\nDataset Construction\nThis section describes the construction of the\nNileTTS dataset. We present a synthetic data gen-\neration pipeline that leverages large language mod-\nels for content creation, neural audio synthesis for\nspeech generation, and automatic tools for tran-\nscription and speaker identification. Figure 1 illus-\ntrates the complete pipeline.\n"}, {"page": 3, "text": "Figure 1: Overview of the NileTTS data generation pipeline. Egyptian Arabic content is generated by LLMs,\nconverted to speech via neural audio synthesis, transcribed and segmented using Whisper, and annotated with\nspeaker identities using ECAPA-TDNN embeddings. Manual quality control ensures accuracy before final dataset\ncompilation.\n3.1\nContent Generation\nThe first stage of our pipeline involves generating\nEgyptian Arabic textual content using large lan-\nguage models. We employ variants Gemini and\nClaude to generate PDF-style reports on diverse\ntopics in authentic Egyptian Arabic dialect. We\ntarget three domains to ensure topical diversity:\n• Medical: Health topics, symptoms, treat-\nments, and medical advice\n• Sales and Customer Service: Product dis-\ncussions, negotiation scenarios, customer in-\nteractions\n• General Conversations: Everyday topics, so-\ncial commentary, cultural discussions\nFor each generation, we prompt the LLM to write\nan a report entirely in Egyptian Arabic dialect, ex-\nplicitly avoiding Modern Standard Arabic. The\nprompts specify the domain and request natural\nconversational language that reflects how Egyp-\ntians actually speak. This approach yields content\nthat is both topically diverse and linguistically au-\nthentic to Egyptian Arabic.\nThe prompts used for content generation were\nintentionally simple.\nWe did not use complex\nprompting strategies or explicit normalization rules;\ninstead, we directly instructed the model to gen-\nerate reports in Egyptian Arabic while avoiding\nModern Standard Arabic. Dialect authenticity was\nassessed qualitatively through manual inspection.\nIn practice, both models consistently produced flu-\nent Egyptian Arabic content.\n3.2\nAudio Synthesis\nThe generated textual reports are then converted\nto speech using NotebookLM’s audio generation\nfeature. NotebookLM produces podcast-style au-\ndio discussions where two virtual hosts engage in\nan in-depth natural conversation about the input\nreport. Crucially for our purposes, NotebookLM\nsupports high-quality Egyptian Arabic synthesis\nwith authentic dialect pronunciation. The audio\ngeneration produces conversations featuring two\ndistinct speakers: one male and one female voice.\nBoth speakers maintain consistent voice character-\nistics across all generated audio, which is essential\nfor TTS training data. Each generated audio file is\napproximately 10-15 minutes in length, covering\nthe content of one PDF report in a conversational\nformat. We selected NotebookLM for several rea-\nsons: (1) it produces natural, conversational Egyp-\ntian Arabic speech rather than formal MSA; (2)\nthe two-speaker format provides speaker diversity\nwithin a consistent voice identity; and (3) the audio\nquality is suitable for TTS training without signifi-\ncant artifacts.\n3.3\nTranscription and Segmentation\nThe generated audio files are processed using Ope-\nnAI’s Whisper Large model (Radford et al., 2022)\nfor automatic transcription. Whisper provides ac-\ncurate Arabic transcription with word-level times-\n"}, {"page": 4, "text": "tamps, which we use for segmentation. We seg-\nment the continuous audio into utterance-level\nchunks. This constraint ensures manageable se-\nquence lengths. Segmentation is performed at natu-\nral speech boundaries (pauses between utterances)\nusing the timestamp information from Whisper.\nSegments shorter than 1 second or containing only\nsilence are discarded. Each segment is saved as an\nindividual WAV file along with its corresponding\ntranscription. This produces the paired (audio, text)\nformat required for TTS training.\n3.4\nSpeaker Diarization\nSince the source audio contains two speakers in\nconversation, we must identify which speaker pro-\nduced each segment. We employ a speaker diariza-\ntion approach based on speaker embeddings. We\nuse the ECAPA-TDNN model (Desplanques et al.,\n2020) from SpeechBrain (Ravanelli et al., 2021)\nto extract speaker embeddings. ECAPA-TDNN\nproduces a 192-dimensional embedding vector for\neach audio segment that captures the speaker’s\nvoice characteristics independent of linguistic con-\ntent. Our diarization process works as follows:\n1) Embedding Extraction: We extract ECAPA-\nTDNN embeddings for all segments across mul-\ntiple audio files, 2) Centroid Computation: Us-\ning K-Means clustering with k = 2, we identify\ntwo cluster centroids representing the two speak-\ners’ average voice characteristics, and 3) Speaker\nAssignment: For each segment, we compute the\nCosine Similarity between its embedding and both\ncentroids. The segment is assigned to the speaker\nwhose centroid is closest.\nThis approach reliably separates the two speak-\ners, as their voice characteristics (male vs. female)\nare sufficiently distinct in the embedding space.\nThe computed centroids are saved and reused for\nprocessing new audio files, ensuring consistent\nspeaker labels across the entire dataset.\n3.5\nQuality Control\nWhile our pipeline is largely automated, we in-\ncorporate manual quality control to ensure dataset\nquality. Human annotators reviewed a the whole\ncategory for Sales and Customer Service, along\nwith a sample of the other 2 sections to verify the\nfollowing: 1) Transcription Accuracy: Check-\ning that the Whisper transcription correctly cap-\ntures the spoken content, particularly for Egyptian\nArabic vocabulary and expressions that may differ\nfrom MSA, 2) Speaker Consistency: Verifying\nStatistic\nUtterances\nHours\nTotal\n9,521\n38.1\nTraining Set\n8,571\n–\nEvaluation Set\n950\n–\nSales & Customer Service\n4,975\n21.0\nGeneral Conversations\n2,979\n11.2\nMedical\n1,567\n5.9\nSPEAKER_01 (Male)\n4,865\n–\nSPEAKER_02 (Female)\n4,656\n–\nAverage Utterance Length\n14.4 seconds\nTable 1: NileTTS dataset statistics.\nthat the automated speaker labels correctly iden-\ntify the speaker in each segment, and 3) Audio\nQuality: Ensuring segments are free from artifacts,\ntruncation, or overlapping speech.\nSegments with significant errors are corrected\nor removed. This quality control step is essential\nfor maintaining dataset integrity, particularly for\ndialectal content where automatic tools may have\nhigher error rates than for standard language vari-\neties.\n3.6\nDataset Statistics\nTable 1 summarizes the NileTTS dataset. The final\ndataset comprises 38.1 hours of transcribed Egyp-\ntian Arabic speech, totaling 9,521 utterances. We\nsplit the data into training (90%) and evaluation\n(10%) sets, ensuring both speakers appear in both\nsplits while keeping specific utterances exclusive\nto one split. We ensure that there is no report-level\noverlap between the training and evaluation sets,\nand that the evaluation set contains unseen topics\nand prompts not used during training. The dataset\ncovers three domains: Sales and Customer Ser-\nvice is the largest (4,975 utterances, 21.0 hours),\nfollowed by General Conversations (2,979 utter-\nances, 11.2 hours) and Medical (1,567 utterances,\n5.9 hours). Speaker representation is well-balanced,\nwith SPEAKER_01 (male) contributing 4,865 ut-\nterances and SPEAKER_02 (female) contributing\n4,656 utterances. The conversational format nat-\nurally produces roughly equal speaking time be-\ntween both voices. The average utterance length\nof 14.4 seconds provides sufficient context for TTS\ntraining while remaining within typical sequence\nlength constraints. The dataset is formatted follow-\ning the XTTS v2 training data specification: each\nutterance is stored as a WAV file, paired with its\ntranscription and speaker identifier in a metadata\n"}, {"page": 5, "text": "Hyperparameter\nValue\nEpochs\n30\nBatch Size\n2\nGradient Accumulation Steps\n8\nEffective Batch Size\n16\nLearning Rate\n5e-6\nOptimizer\nAdamW\nWeight Decay\n1e-2\nMax Text Length\n400 tokens\nTable 2: Finetuning hyperparameters for XTTS v2 on\nNileTTS.\nCSV file. This ensures direct compatibility with\nthe XTTS fine-tuning pipeline and facilitates repro-\nducibility.\n4\nModel Finetuning\n4.1\nBase Model: XTTS v2\nWe finetuned XTTS v2 (Casanova et al., 2024), a\nstate-of-the-art multilingual text-to-speech model\ndeveloped by Coqui. XTTS v2 employs a GPT-\nstyle autoregressive architecture that generates dis-\ncrete audio tokens, which are then decoded into\nwaveforms. The model supports zero-shot voice\ncloning, allowing it to synthesize speech in a tar-\nget voice given only a short reference audio clip.\nXTTS v2 is pretrained on a large multilingual cor-\npus covering 16 languages, including Arabic. How-\never, the Arabic training data primarily consists of\nModern Standard Arabic and Gulf dialects, leaving\nEgyptian Arabic underrepresented. Our finetuning\nadapts the model to Egyptian Arabic while preserv-\ning its voice cloning capabilities.\n4.2\nFinetuning Configuration\nWe finetuned the GPT component of XTTS v2 on\nthe NileTTS training set while keeping the DVAE\n(audio tokenizer) frozen. We largely adopt the de-\nfault hyperparameters and training setup provided\nby the Coqui team’s finetuning codebase, with min-\nimal modifications. Table 2 summarizes the key\ntraining parameters.\nOur primary modifications to the training\npipeline involve integrating Weights & Biases for\nexperiment tracking and implementing evaluation\nmetrics—including Word Error Rate, Character Er-\nror Rate, and Speaker Similarity—computed pe-\nriodically during training to monitor convergence\nand enable checkpoint selection.\n5\nExperiments and Results\n5.1\nEvaluation Setup\nWe evaluate our finetuned NileTTS model against\nthe baseline XTTS v2 model to measure improve-\nments in Egyptian Arabic synthesis quality. The\nbaseline is the pretrained XTTS v2, which includes\nArabic but primarily covers Modern Standard Ara-\nbic and Gulf dialects.\nWe use the following evaluation metrics, com-\nputed on the held-out evaluation set:\n• Evaluation Loss: Combined text and mel-\nspectrogram cross-entropy loss as defined by\nthe XTTS architecture.\n• Word Error Rate (WER): We synthesize\nspeech from text, transcribe it using Whis-\nper Large (Radford et al., 2022), and compute\nWER against the original text. Lower WER\nindicates higher intelligibility.\n• Character Error Rate (CER): A finer-\ngrained intelligibility metric computed at the\ncharacter level.\n• Speaker Similarity: Cosine similarity be-\ntween ECAPA-TDNN (Desplanques et al.,\n2020) speaker embeddings of synthesized and\nreference audio. Higher similarity indicates\nbetter voice cloning.\n5.2\nResults\nFigure 2 illustrates the progress of evaluation met-\nrics throughout training. All metrics show rapid\nimprovement in early training, with loss decreasing\nand intelligibility metrics (WER, CER) improving\nsubstantially within the first 20,000 steps. Beyond\nthis point, metrics begin to look more horizontal,\nindicating diminishing returns from continued train-\ning.\nCheckpoint Selection. Although we initially\nplanned for 30 epochs of training, we observe that\nafter approximately 8 epochs (around 35,000 steps),\nthe evaluation metrics stabilize with minimal fur-\nther improvement. Training was stopped after 13\nepochs (55,719 steps) due to this reason. We se-\nlect the checkpoint at step 34,289 (epoch 8), which\nachieves a strong balance across all metrics. To\nvalidate this selection, we synthesized 50 randomly\nsampled utterances from the evaluation set and con-\nducted manual listening evaluation. The synthe-\nsized speech demonstrated natural prosody, accu-\nrate pronunciation of Egyptian Arabic phonemes,\n"}, {"page": 6, "text": "Figure 2: Evaluation metrics throughout training: (a) Evaluation Loss, (b) Word Error Rate, (c) Character Error\nRate, (d) Speaker Similarity. The red marker indicates the selected checkpoint at step 34,289 (epoch 8).\nModel\nWER ↓\nCER ↓\nSpk Sim ↑\nXTTS v2\n26.8%\n8.1%\n0.713\nNileTTS\n18.8%\n4.1%\n0.755\nTable 3: Comparison of baseline XTTS v2 Baseline and\nfinetuned NileTTS on Egyptian Arabic evaluation set.\nand consistent preservation of speaker identity, con-\nfirming the checkpoint’s suitability for release.\nTable 3 presents the final comparison between\nthe baseline XTTS v2 model and our finetuned\nNileTTS model.\nNileTTS achieves a 29.9% relative reduction\nin Word Error Rate (from 26.8% to 18.8%) and\na 49.4% relative reduction in Character Error\nRate (from 8.1% to 4.1%), indicating significantly\nimproved intelligibility for Egyptian Arabic syn-\nthesis. Speaker similarity improves from 0.713 to\n0.755 (+5.9%), demonstrating better voice cloning.\nThese results confirm that finetuning on dialect-\nspecific data yields substantial improvements in\nTTS quality, even when the base model already\nsupports the target language family. We release the\nNileTTS model weights publicly on Hugging Face\nto serve as a foundation for future Egyptian Arabic\nspeech synthesis research.\n6\nDiscussion and Limitations\n6.1\nDataset Limitations\nWhile NileTTS represents a significant resource\nfor Egyptian Arabic TTS, limitations should be\nacknowledged. First, the dataset contains only\ntwo speakers (one male, one female), which limits\nspeaker diversity. TTS models trained on limited\nspeaker data may not generalize well to synthesiz-\ning voices with different characteristics. Future\nwork should expand the dataset with additional\n"}, {"page": 7, "text": "speakers to improve voice diversity.\nSecond, our dataset is constructed from synthet-\nically generated audio rather than recordings of\nhuman speakers. Even though the used audio syn-\nthesis tool produces high-quality Egyptian Arabic\nspeech with natural prosody. However, our evalu-\nation results suggest that the synthetic data is suf-\nficient for training effective TTS models, and the\npipeline’s reproducibility enables future expansion\nwith additional synthetic or natural data.\nThird, although we cover three domains (med-\nical, sales, and general conversations), certain\nspecialized domains such as news broadcasting,\npoetry, or technical content are not represented.\nExpanding domain coverage would improve the\nmodel’s versatility.\n6.2\nEvaluation Limitations\nOur evaluation relies on automatic metrics (WER,\nCER, Speaker Similarity) rather than formal human\nevaluation studies such as Mean Opinion Score\n(MOS) assessments. While automatic metrics cor-\nrelate with perceived quality, they do not fully\ncapture subjective aspects like naturalness, expres-\nsiveness, or listener preference. We mitigate this\nlimitation through manual listening evaluation of\nsynthesized samples, but a comprehensive human\nevaluation study remains valuable future work.\nAdditionally, WER and CER are computed using\nWhisper Large as the transcription model. While\nWhisper performs well on Egyptian Arabic, tran-\nscription errors from the ASR system may intro-\nduce noise into these metrics. Moreover, since\nWhisper is also used to generate the dataset tran-\nscripts, this setup may introduce a form of ASR\nself-consistency bias, potentially inflating evalua-\ntion scores. Future work should evaluate WER and\nCER using alternative ASR models and include\na small human-verified transcription subset to im-\nprove robustness.\nSimilarly, speaker similarity is computed using\ncosine similarity between ECAPA-TDNN speaker\nembeddings. As the same embedding architecture\nis also used in the pipeline, this may introduce a\ndegree of model-specific bias in the speaker simi-\nlarity scores. Future work will explore the use of\nadditional speaker embedding models for confirma-\ntion and incorporate human verification to further\nvalidate speaker similarity assessments.\n6.3\nSynthetic-to-Real Generalization\nA common concern with synthetic speech datasets\nis whether models trained on them generalize to\nreal human speech. Although NileTTS is built from\nsynthetically generated audio, the text content is\nwritten in authentic Egyptian Arabic, and the syn-\nthesis preserves key dialectal properties such as\npronunciation, intonation, and prosodic patterns.\nThese aspects are essential for learning dialect-\nspecific speech characteristics.\nIn low-resource settings, synthetic speech has\nbeen shown to be a practical and effective training\nsignal when natural data is limited. In this sense,\nNileTTS provides a scalable source of Egyptian\nArabic speech data that captures core linguistic\nproperties of the dialect and can complement future\ndatasets based on real human recordings.\n6.4\nFuture Work\nSeveral directions could extend this work:\n• Speaker expansion: Adding more speakers\nwith diverse voice characteristics, ages, and\nspeaking styles.\n• Other Arabic dialects: Applying the syn-\nthetic data pipeline to other under-resourced\nArabic varieties.\n• Human evaluation: Conducting formal MOS\nstudies to complement automatic metrics.\n• Robust evaluation: Evaluating WER, CER,\nand speaker similarity using multiple indepen-\ndent models and human-verified subsets.\n7\nConclusion\nWe presented NileTTS, a large-scale Egyptian Ara-\nbic text-to-speech dataset and finetuned model. Our\ndataset comprises 38 hours of transcribed Egyptian\nArabic speech from two speakers across medical,\nsales, and general conversation domains. We intro-\nduced a novel synthetic data generation pipeline\nthat leverages large language models for content\ncreation, neural audio synthesis for speech genera-\ntion, and automatic transcription with speaker di-\narization—offering a reproducible and scalable ap-\nproach for creating TTS datasets for low-resource\ndialects.\nBy finetuning XTTS v2 on NileTTS, we\nachieved substantial improvements over the base-\nline Arabic model: 29.9% relative reduction in\nWord Error Rate, 49.4% reduction in Character\n"}, {"page": 8, "text": "Error Rate, and 5.9% improvement in speaker\nsimilarity. These results demonstrate that dialect-\nspecific finetuning significantly enhances TTS qual-\nity for underrepresented language varieties.\nWe publicly release the NileTTS dataset, model\nweights, and pipeline code to facilitate further re-\nsearch in Egyptian Arabic speech synthesis. We\nhope this work contributes to closing the resource\ngap for Arabic dialects and inspires similar efforts\nfor other low-resource language varieties.\nReferences\nKathrein Abu Kwaik, Motaz Saad, Stergios Chatzikyri-\nakidis, and Simon Dobnik. 2018. A lexical distance\nstudy of arabic dialects. Procedia Computer Science,\n142:2–13.\nAhmed Hammad Azab, Ahmed B. Zaky, Tetsuji Ogawa,\nand Walid Gomaa. 2023. Masry: A text-to-speech\nsystem for the egyptian arabic. Proceedings of the In-\nternational Conference on Informatics in Control,\nAutomation and Robotics, 2:219–226.\nPublisher\nCopyright: © 2023 by SCITEPRESS - Science and\nTechnology Publications, Lda. Under CC license\n(CC BY-NC-ND 4.0).; 20th International Confer-\nence on Informatics in Control, Automation and\nRobotics, ICINCO 2023 ; Conference date: 13-11-\n2023 Through 15-11-2023.\nEdresson Casanova, Kelly Davis, Eren Gölge, Görkem\nGöknar, Iulian Gulea, Logan Hart, Aya Aljafari,\nJoshua Meyer, Reuben Morais, Samuel Olayemi,\nand Julian Weber. 2024. Xtts: a massively multi-\nlingual zero-shot text-to-speech model.\nPreprint,\narXiv:2406.04904.\nBrecht Desplanques, Jenthe Thienpondt, and Kris De-\nmuynck. 2020. Ecapa-tdnn: Emphasized channel\nattention, propagation and aggregation in tdnn based\nspeaker verification.\nBosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze\nLuo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie\nHu, Anh Tuan Luu, and Shafiq Joty. 2024. Data\naugmentation using large language models: Data\nperspectives, learning paradigms and challenges.\nPreprint, arXiv:2403.02990.\nAmin Fazel, Wei Yang, Yulan Liu, Roberto Barra-\nChicote, Yixiong Meng, Roland Maas, and Jasha\nDroppo. 2021. Synthasr: Unlocking synthetic data\nfor speech recognition. Preprint, arXiv:2106.07803.\nJaehyeon Kim, Jungil Kong, and Juhee Son. 2021.\nConditional variational autoencoder with adversar-\nial learning for end-to-end text-to-speech. Preprint,\narXiv:2106.06103.\nVasista Lodagala, Lamya Alkanhal, Daniel Izham,\nShivam Mehta, Shammur Chowdhury, Aqeelah\nMakki, Hamdy Hussein, Gustav Henter, and Ahmed\nAli. 2025. Sawtarabi: A benchmark corpus for arabic\ntts. standard, dialectal and code-switching.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2022.\nRobust speech recognition via large-scale weak su-\npervision. Preprint, arXiv:2212.04356.\nMirco Ravanelli, Titouan Parcollet, Peter Plantinga,\nAku Rouhe, Samuele Cornell, Loren Lugosch, Cem\nSubakan, Nauman Dawalatabad, Abdelwahab Heba,\nJianyuan Zhong, Ju-Chieh Chou, Sung-Lin Yeh,\nSzu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva,\nFrançois Grondin, William Aris, Hwidong Na, Yan\nGao, and 2 others. 2021. Speechbrain: A general-\npurpose speech toolkit. Preprint, arXiv:2106.04624.\nYi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao,\nZhou Zhao, and Tie-Yan Liu. 2019.\nFastspeech:\nFast, robust and controllable text to speech. Preprint,\narXiv:1905.09263.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,\nLong Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,\nHuaming Wang, Jinyu Li, Lei He, Sheng Zhao, and\nFuru Wei. 2023.\nNeural codec language models\nare zero-shot text to speech synthesizers. Preprint,\narXiv:2301.02111.\nYuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui\nWu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang,\nYing Xiao, Zhifeng Chen, Samy Bengio, Quoc Le,\nYannis Agiomyrgiannakis, Rob Clark, and Rif A.\nSaurous. 2017. Tacotron: Towards end-to-end speech\nsynthesis. Preprint, arXiv:1703.10135.\n"}]}