{"doc_id": "arxiv:2511.06668", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.06668.pdf", "meta": {"doc_id": "arxiv:2511.06668", "source": "arxiv", "arxiv_id": "2511.06668", "title": "When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare", "authors": ["Saeedeh Javadi", "Sara Mirabi", "Manan Gangar", "Bahadorreza Ofoghi"], "published": "2025-11-10T03:27:54Z", "updated": "2025-11-10T03:27:54Z", "summary": "In high-stakes information domains such as healthcare, where large language models (LLMs) can produce hallucinations or misinformation, retrieval-augmented generation (RAG) has been proposed as a mitigation strategy, grounding model outputs in external, domain-specific documents. Yet, this approach can introduce errors when source documents contain outdated or contradictory information. This work investigates the performance of five LLMs in generating RAG-based responses to medicine-related queries. Our contributions are three-fold: i) the creation of a benchmark dataset using consumer medicine information documents from the Australian Therapeutic Goods Administration (TGA), where headings are repurposed as natural language questions, ii) the retrieval of PubMed abstracts using TGA headings, stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence, and iii) a comparative analysis of the frequency and impact of outdated or contradictory content on model-generated responses, assessing how LLMs integrate and reconcile temporally inconsistent information. Our findings show that contradictions between highly similar abstracts do, in fact, degrade performance, leading to inconsistencies and reduced factual accuracy in model answers. These results highlight that retrieval similarity alone is insufficient for reliable medical RAG and underscore the need for contradiction-aware filtering strategies to ensure trustworthy responses in high-stakes domains.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.06668v1", "url_pdf": "https://arxiv.org/pdf/2511.06668.pdf", "meta_path": "data/raw/arxiv/meta/2511.06668.json", "sha256": "fbae5e8274e22d9d2baef461d00118ab50bd41b262faa0d31568654dd35d2dbf", "status": "ok", "fetched_at": "2026-02-18T02:28:02.769071+00:00"}, "pages": [{"page": 1, "text": "When Evidence Contradicts: Toward Safer\nRetrieval-Augmented Generation in Healthcare\nSaeedeh Javadi1⋆, Sara Mirabi2⋆, Manan Gangar2, and Bahadorreza Ofoghi2\n1 RMIT University, Melbourne, Australia\nsaeedeh.javadi@student.rmit.edu.au\n2 Deakin University, Melbourne, Australia\ns222496341@deakin.edu.au, gangarmanan27@gmail.com, b.ofoghi@deakin.edu.au\nAbstract. In high-stakes information domains such as healthcare, where\nlarge language models (LLMs) can produce hallucinations or misinfor-\nmation, retrieval-augmented generation (RAG) has been proposed as\na mitigation strategy, grounding model outputs in external, domain-\nspecific documents. Yet, this approach can introduce errors when source\ndocuments contain outdated or contradictory information. This work\ninvestigates the performance of five LLMs in generating RAG-based re-\nsponses to medicine-related queries. Our contributions are three-fold: i)\nthe creation of a benchmark dataset using consumer medicine informa-\ntion documents from the Australian Therapeutic Goods Administration\n(TGA), where headings are repurposed as natural language questions,\nii) the retrieval of PubMed abstracts using TGA headings, stratified\nacross multiple publication years, to enable controlled temporal evalua-\ntion of outdated evidence, and iii) a comparative analysis of the frequency\nand impact of outdated or contradictory content on model-generated re-\nsponses, assessing how LLMs integrate and reconcile temporally inconsis-\ntent information. Our findings show that contradictions between highly\nsimilar abstracts do, in fact, degrade performance, leading to inconsis-\ntencies and reduced factual accuracy in model answers. These results\nhighlight that retrieval similarity alone is insufficient for reliable medical\nRAG and underscore the need for contradiction-aware filtering strategies\nto ensure trustworthy responses in high-stakes domains.\nKeywords: Large language models · Retrieval-augment generation ·\nContradiction detection · Medicine · Question answering.\n1\nIntroduction\nLarge language models (LLMs) have demonstrated exceptional capabilities in re-\nsponding to user information requests that require world knowledge. This knowl-\nedge is inherent in the vast amounts of pre-training data they consume, which\nis increasingly making these models a primary candidate for information seek-\ning, where they have achieved state-of-the-art performance on a wide range of\n⋆Equal contribution.\narXiv:2511.06668v1  [cs.IR]  10 Nov 2025\n"}, {"page": 2, "text": "tasks, including medical question answering (QA) [30]. In domains like health,\nhowever, knowledge of specific concepts is constantly being updated with new\nfindings, some of which are contradictory to previously known facts [4, 24]. In-\nconsistent and outdated sources used for training the generative models can\npotentially result in unreliable generated information. To mitigate misinforma-\ntion and hallucinations resulting from such inconsistencies in training medical\ninformation where accuracy is paramount [19], retrieval-augmented generation\n(RAG) is employed as a strategy [21, 30].\nRAG effectively supplements the LLM’s internal knowledge with context in-\nformation, and studies have shown that augmenting prompts with retrieved ev-\nidence can reduce the incidence of hallucinations in practice [15]. The efficacy\nof RAG is, however, predicated on the relevance and coherence of the retrieved\ndocuments, thereby raising significant concerns regarding the model’s perfor-\nmance and robustness should the retrieval process yield contradictory or irrele-\nvant results [32]. A significant challenge for RAG systems, therefore, is the LLM’s\nhandling of conflicting or outdated information within retrieved documents [31].\nGiven that an LLM’s internal parameters are static, conflicts between its fixed\nknowledge base and new, external context are an unavoidable issue. Resolving\nsuch conflicts is a non-trivial task, as the model may incorrectly prioritize a par-\nticular source or even synthesize contradictory information. Despite this, there is\ncurrently a lack of systematic research and established guidelines for managing\nthese issues in RAG systems, particularly within medical applications [30].\nIn this work, we investigate how well LLMs handle medicine-related queries\nwhen grounded in authoritative reference documents. We focus on the effects\nof contradictions and outdated content on the quality of RAG-powered LLM\noutputs using a purpose-curated dataset from two prominent sources: the Aus-\ntralian Therapeutic Goods Administration (TGA) and the PubMed repository.\nThe dataset includes medicine-related questions, linked PubMed abstracts, and\nmetadata such as publication year, ensuring a mix of recent and older sources.\nThe questions were used as information requests to several LLMs, where the\nrelevant PubMed abstracts were utilized for RAG, and the correct answers from\nthe TGA brochures were considered as ground-truth responses. Our main con-\ntributions include:\n– Curating a dataset linking TGA consumer medicine information with PubMed\nabstracts across multiple publication years, prioritizing temporal diversity.\n– Implementation of a RAG pipeline using FAISS retrieval and the BAAI/bge-\nsmall-en-v1.5 embedding model, enabling reproducible evaluation.\n– Evaluation of five LLMs, including Falcon3 [25], Gemma-3 [7], GPT-OSS [17],\nMed-LLaMA3 [14], and Mixtral [10] on the dataset to assess their ability to\nprovide accurate, up-to-date, and non-contradictory answers using RAG.\n– Analysis of how the time stamp of source information and potential contra-\ndictions in source materials affect the quality of generated responses.\n2\n"}, {"page": 3, "text": "2\nRelated Work\nSeveral studies have taken RAG strategies in the domain in recent years. MKRAG,\na RAG framework for medical QA, retrieves medical facts from an external\ndisease database and injects them into LLM prompts through in-context learn-\ning [22]. OpenEvidence and ChatRWD were developed to apply RAG to literature-\nbased clinical evidence and map queries into PICO study designs for real-world\nevidence generation [16]. In a similar direction, a RAG-driven model was pro-\nposed for health information retrieval that integrates PubMed Central with gen-\nerative LLMs through a three-stage pipeline of passage retrieval, generative re-\nsponse construction, and factuality checks via stance detection and semantic\nsimilarity [26]. A multi-source benchmark for evidence-based clinical QA was\ncurated from Cochrane reviews [1], AHA guidelines [2], and narrative guidance,\nunderscoring the need for diversified and authoritative retrieval sources [27]. Ex-\ntending this line, MedCoT-RAG combined causal-aware retrieval with structured\nchain-of-thought prompting for medical QA [28]. A two-layer RAG framework\nleveraging Reddit data on emerging substances such as xylazine and ketamine\nwas introduced to generate query-focused summaries suitable for low-resource\nsettings [6]. BriefContext, a map-reduce framework that partitions long retrieval\ncontexts into shorter segments to mitigate the “lost-in-the-middle” problem and\nimprove the accuracy of medical QA in RAG systems, was introduced [33]. EX-\nPRAG, a retrieval-augmented generation framework, leveraged electronic health\nrecords by retrieving discharge reports from clinically similar patients through\na coarse-to-fine process (EHR-based report ranking followed by experience re-\ntrieval), enabling case-based reasoning for diagnosis, medication, and discharge\ninstruction QA [18]. MedRAG integrated electronic health records with a hier-\narchical diagnostic knowledge graph for clinical decision support [34]. Discuss-\nRAG, an agent-led framework, enhanced RAG by using multi-agent discussions\nto construct context-rich summaries for retrieval and a verification agent to fil-\nter irrelevant snippets before answer generation [9]. Finally, a biomedical QA\nsystem based on RAG was developed with a two-stage hybrid retrieval pipeline,\nwhere BM25 [20] performed lexical retrieval over PubMed, and MedCPT’s cross-\nencoder [12] reranked the top candidates to refine semantic relevance [23].\nWhile previous studies demonstrate the benefits of RAG in medical QA, most\nconcentrate on retrieval pipelines, evidence structuring, or database integration.\nFew explicitly examine the risks posed by outdated or contradictory evidence\nwithin retrieved sources. This remains a critical gap for high-stakes medical\napplications, where knowledge evolves rapidly.\n3\nMethodology\n3.1\nProblem Formulation and Objectives\nThis study addresses the challenge of evaluating RAG in the medical QA domain,\nwhere information accuracy and temporal consistency are critical. The problem\nis formally defined as follows. Let\nM = {m1, . . . , m1476},\n(1)\n3\n"}, {"page": 4, "text": "denote a set of 1,476 medicines regulated by the TGA. For each medicine mi ∈\nM, a fixed set of six standardized consumer-oriented queries,\nQi = { qi,1, qi,2, . . . , qi,6 },\n(2)\nis defined, which covers common information needs: 1) therapeutic indications,\n2) pre-use warnings, 3) drug-drug interactions, 4) dosage and administration, 5)\nguidance while under treatment, including interactions/monitoring, and 6) ad-\nverse effects. 1 These queries represent typical information needs that patients\nand healthcare consumers might have when consulting medical information sys-\ntems. Across the collection, a total of |Q| = P1476\ni=1 |Qi| = 6 |M| = 8,856 query\ninstances are obtained. For each query qi,j ∈Qi with j ∈{1, . . . , 6}, a retrieval\nprocess is performed against the PubMed biomedical literature database to ob-\ntain a candidate set of abstracts,\nDi,j = { d1, . . . , dni,j },\n(3)\nwhere each document d ∈Di,j is characterized by its text t(d), publication\nyear γ(d), citation count κ(d) ∈N0, and unique PubMed identifier PMID(d).\nThe retrieval process yields resulting in a total corpus containing approximately\n400,000 documents spanning publication years from 1975 to 2025. The funda-\nmental challenge lies in the fact that retrieved abstracts may contain outdated\nrecommendations, conflicting findings from different studies, or evolving medi-\ncal consensus over time. The RAG system must therefore not only identify rel-\nevant documents but also reconcile potentially contradictory information while\ngenerating accurate, consistent responses. Our primary objectives are therefore\nthreefold.\nObjective 1: Temporal Diversity in Evidence Selection. Given the evolv-\ning nature of medical knowledge, the system must select a subset,\nRi,j ⊆Di,j,\n|Ri,j| ≤20,\n(4)\nthat maximizes temporal diversity while maintaining relevance. This requires\nbalancing recent findings with historical context, particularly when medical rec-\nommendations change over time.\nObjective 2: Contradiction-Aware Retrieval. The system must identify\nand quantify contradictions among retrieved abstracts to avoid incorrect or po-\ntentially harmful responses. A contradiction function,\nCNT : Ri,j × Ri,j →[0, 1],\n(5)\n1 The sixth standardized phrasings in our corpus include, e.g., “Why am I using\nABACAVIR?” (indications); “What should I know before I use ABACAVIR?” (pre-use\nwarnings); “What if I am taking other medicines with ABACAVIR?” (drug-drug inter-\nactions); “How do I use ABACAVIR?” (dosage/administration); “What should I know\nwhile using ABACAVIR?” (on-treatment guidance); and “Are there any side effects of\nABACAVIR?” (adverse effects).\n4\n"}, {"page": 5, "text": "is defined such that, for any pair (da, db), the value CNT (da, db) quantifies the\nlikelihood that t(da) contradicts t(db); this function is intended to support the\nconstruction of “most-contradictory” and “least-contradictory” configurations.\nObjective 3: RAG Performance Evaluation. Given a retrieved set Ri,j and\nan LLM L ∈Λ, the system must generate a response ai,j that addresses qi,j using\nthe retrieved context. The quality of this response is evaluated against ground\ntruth answers extracted from TGA medicine information documents. Five LLMs\nincluding Falcon3 [25], Gemma-3 [7], GPT-OSS [17], Med-LLaMA3 [14], and\nMixtral [10] are evaluated, collectively denoted as Λ.\n3.2\nEvidence Set Construction\nQuery Expansion and Search. The evidence acquisition employs a three-tier\nquery formulation strategy to balance precision and recall. For each (mi, qi,j)\npair, content terms Ti,j = {t1, t2, . . . , tk} are extracted through part-of-speech\nfiltering using SpaCy [11], retaining nouns (NN, NNS), verbs (VB*), and proper\nnouns (NNP, NNPS) while excluding pharmaceutical company names through\na curated exclusion list. Three query formulations are constructed: i) exact sen-\ntence match Q1 = V\nt∈Ti,j sentence(t), ii) proximity-constrained Q2 = NEAR25\n(Ti,j) ∧mi[ti], and iii) full query proximity Q3 = NEAR25(qi,j), where NEARk\ndenotes proximity within k tokens and [ti] restricts to title field. Results are\ndeduplicated by PMID to form Draw\ni,j .\nAbstract Acquisition and Filtering. PMIDs are fetched in batches to mit-\nigate API limitations. XML responses are parsed to extract (pmid, γ(d), t(d)).\nRecords without abstracts are discarded; all remaining years are preserved to\nenable temporal analyses.\nTemporal-Citation Balanced Selection. A critical innovation in this method-\nology is the implementation of a selection algorithm that balances temporal di-\nversity with citation impact. This approach addresses the challenge of capturing\nevolving medical knowledge while prioritizing influential research within each\ntemporal stratum. Based on Algorithm 1, to construct a pool of up to 20 ab-\nstracts per query while preserving temporal coverage, when the number of unique\npublication years exceeds 20, the system performs stratified sampling to select\nyears with approximately three-year intervals, ensuring representation across the\nfull temporal range. For datasets with fewer than 20 unique years, all years are\nretained. Within each selected year, abstracts are ranked by citation count in\ndescending order, serving as a proxy for scientific impact and reliability.\nThe final selection employs a round-robin approach across years, iteratively\nselecting the highest-cited unselected abstract from each year until either 20\nabstracts are selected or all available abstracts are exhausted. This mechanism\nensures both temporal diversity and quality, preventing over-representation of\nany single time period while maintaining citation-based quality signals.\n5\n"}, {"page": 6, "text": "Algorithm 1 Temporal-citation balanced selection\nRequire: Candidates Draw\ni,j , citation function κ : D →N, year function γ : D →N\nEnsure: Selected subset Ri,j where |Ri,j| ≤20\n1: Y ←{γ(d) : d ∈Draw\ni,j }\n2: if |Y| ≥20 then\n3:\nY∗←STRATIFIED-SAMPLE(Y, 20, gap = 3)\n4: else\n5:\nY∗←Y\n6: end if\n7: for all y ∈Y∗do\n8:\nSort {d ∈Draw\ni,j : γ(d) = y} by κ(d) descending\n9: end for\n10: Ri,j ←∅\n11: while |Ri,j| < 20 and ∃y ∈Y∗with remaining candidates do\n12:\nfor all y ∈Y∗in round-robin order do\n13:\nif candidates remain for year y then\n14:\nRi,j ←Ri,j ∪{arg maxd:γ(d)=y κ(d)}\n15:\nend if\n16:\nend for\n17: end while\n18: return Ri,j\n3.3\nDiversity-Aware Scoring for Retrieval Framework\nDocument Representation and Indexing. Document embeddings are com-\nputed using an encoder function e : Σ∗→Rd, specifically the BAAI/bge-small-\nen-v1.5 model [29]. The embeddings are indexed using Facebook AI Similarity\nSearch (FAISS) [13].\nMaximal Marginal Relevance with Temporal Augmentation. The re-\ntrieval framework extends maximal marginal relevance (MMR) [3] by incorpo-\nrating temporal diversity. For a given query q and candidate set D, the MMR\nscore for document di is:\nMMR(di | q, D) = λ·cos(e(q), e(t(di)))−(1−λ)·max\nj̸=i cos(e(t(di)), e(t(dj))) (6)\nwhere cosine similarity is defined as cos(x, y) =\nx⊤y\n∥x∥2∥y∥2 , in the embedding\nspace, and λ controls the trade-off between relevance and diversity. This score is\nthen combined with a temporal diversity component. A temporal diversity score\nis computed to favor documents spanning different time periods:\nτ(di) =\nγ(di) −mind∈D γ(d)\nmaxd∈D γ(d) −mind∈D γ(d) + ϵ\n(7)\nwhere ϵ = 10−5 prevents division by zero. The final ranking score integrates\nrelevance, redundancy, and temporal components:\nS(di | q) = α · MMR(di | q, D) + (1 −α) · τ(di)\n(8)\n6\n"}, {"page": 7, "text": "The documents are re-ranked based on this score (Algorithm 2), and the\ntop-K documents by S(· | q) are selected as context.\nAlgorithm 2 MMR + year-aware ranking (per query)\n1: Input: query q, candidates D = {d1, . . . , dn}; n ≤20, parameters λ, α\n2: for i = 1 to n do\n3:\nsi ←cos\u0000e(q), e(t(di))\u0001\n4:\nri ←maxj̸=i cos\u0000e(t(di)), e(t(dj))\u0001\n5:\nMMRi ←λsi −(1 −λ)ri\n6: end for\n7: compute τi for all i using (7)\n8: Si ←α MMRi + (1 −α)τi\n9: return re-ranking documents in D by Si (descending)\n3.4\nContradiction Detection Scoring Framework\nTo quantify conflicting evidence in the retrieved abstract pool for a query, the\nsubset Ri,j = { d1, . . . , dn } ⊆Di,j was considered. Each document d ∈Ri,j is\nembedded with a scientific encoder esim (SPECTER [5]), producing represen-\ntation h(d) = esim(t(d)). For any abstract pair (da, db) ∈Ri,j × Ri,j, a coarse\nsimilarity was computed as:\nsimabs(da, db) = cos\n\u0000h(da), h(db)\n\u0001\n,\n(9)\nserving as a coarse filter. Each abstract d was segmented into sentences S(d) =\n{ s(1)\nd , . . . , s(Ld)\nd\n}. Sentences were embedded with the same encoder, h(s) =\nesim(s). For a pair (da, db), candidate sentence pairs were retained when,\ncos\n\u0000h(s), h(t)\n\u0001\n≥θsent = 0.75,\n(s, t) ∈S(da) × S(db).\n(10)\nThe resulting candidate set is denoted P(da, db) ⊆S(da)×S(db). Each (s, t) ∈\nP(dp, dq) is passed to a biomedical natural language inference (NLI) classifier fnli\n(PubMedBERT-MNLI MedNLI [8]) that returns (Pent(s, t), Pneu(s, t), Pcon(s, t)),\nwhere P denotes the probability assigned to each relation: entailment (ent) in-\ndicates that the hypothesis is supported by the premise, neutral (neu) denotes\nno clear relation, and contradiction (con) indicates conflict. The contradiction\nfunction CNT : Ri,j × Ri,j →[0, 1] is then defined at the document level by the\npeak contradiction probability over candidate sentence pairs:\nCNT (da, db) =\nmax\n(s,t)∈P(da,db) Pcon(s, t).\n(11)\nThe framework thus reports, for each document pair, i) the document-level\nsimilarity, ii) the peak contradiction score, and iii) the most indicative sentence\npair with similarity evidence. A document-level contradiction salience within the\npool was defined as:\n7\n"}, {"page": 8, "text": "1,476 medicines \nregulated by \nTGA\nPubMed Medical \nLiterature Database\nAnswer\nQuery \nExpansion & \nSearch \nTemporal-Citation \nBalanced Selection\nEmbedding & \nIndexing FAISS\nGenerate\nDiversity-Aware \nScoring for \nRetrieval \nFramework\nContradiction \nDetection \nScoring \nFramework\nQuery + Top K \nDocuments as \nContext \nMost Similar \nDocuments\nVector \nDatabase\nRAG with 5 LLMs\nFalcon 3\nGemma 3\nGPT-OSS\nMed-LLaMA 3\nMixtral\nMost Contradictory \nDocuments\nLeast Contradictory \nDocuments\nFig. 1. Contradiction-aware medical RAG pipeline, showing data progression from\nTGA queries through search, embedding, and three retrieval strategies to final LLM-\nbased generation and evaluation.\nCNT (d) =\n1\n|Ri,j| −1\nX\nd′∈Ri,j\nd′̸=d\nCNT (d, d′).\n(12)\nGiven K, the most-contradictory and least-contradictory context sets used in\nthe retrieval variants were constructed as:\nCmost\ni,j\n= arg topK\nd∈Ri,j\n\u0000CNT (d)\n\u0001\n,\nCleast\ni,j\n= arg topK\nd∈Ri,j\n\u0000−CNT (d)\n\u0001\n,\n(13)\nrespectively, while the most-similar condition employed the top-K by the diversity-\naware score S(·) in Equation (8).\n3.5\nRetrieval-Augmented Generation Pipeline\nGiven a query qi,j and its ranked list obtained from Equation (8), a context set\nCi,j = {d(1), . . . , d(K)} was formed by taking the top-K documents. A grounded\nprompting instruction was then applied so that an answer a(ℓ)\ni,j was produced by\na model ℓ∈Λ using only Ci,j; if the evidence was insufficient, the token Insuf-\nficient evidence was to be emitted. Three retrieval conditions were instantiated,\neach with a constant K: i) a most-similar condition using the top-K by S(· | q);\nii) a most-contradictory condition selecting the K documents with largest con-\ntradiction salience (Section 3.4); and iii) a least-contradictory condition selecting\nthe K smallest by the same criterion. A schematic of the end-to-end pipeline is\nprovided in Figure 1, comprising evidence construction, indexing, and diversity-\naware ranking, retrieval variants, RAG inference, and evaluation.\n4\nExperimental Setup\nTable 1 summarizes the dataset used in our experiments. Starting from nearly\n400k retrieved PubMed documents, temporal–citation balanced selection re-\ntained 91,662 PMIDs, of which 28,873 are unique. Of the 1,476 medicines, 1,074\n8\n"}, {"page": 9, "text": "Table 1. Statistics of the constructed TGA–PubMed dataset used in our experiments.\nStatistic\nValue\nMedicines (TGA)\n1,476\nQueries (6 per medicine)\n8,856\nTime span of abstracts\n1975–2025\nLanguage\nEnglish\nPubMed documents retrieved (raw)\n∼400,000\nMedicines with ≥1 retrieved documents\n1,074\nPMIDs after Temporal-citation balanced selection\n91,662\nUnique PMIDs in filtered set\n28,873\nAverage documents per query\n13.96\nhave at least one retrieved document, among queries that retained evidence, the\nmean context size is 13.96 documents per query.\nThe experiments employ the following configurations: MMR parameter λ =\n0.7 for balancing relevance and diversity, temporal weighting α = 0.7 for the\nfinal ranking score, and retrieval size K = 5 documents per query. Document\nembeddings use BAAI/bge-small-en-v1.5 (384 dimensions), indexed with FAISS\nHNSW graphs (M=16, ef_construction=200). PubMed API calls use batch sizes\nof 300 PMIDs per request. All language models in Table 4 use temperature=0\nfor deterministic generation with a 256 token limit. Implementation uses Python\n3.10 with LangChain v0.1.0 for RAG orchestration.\nPerformance assessment employs multiple complementary metrics. Lexical\noverlap is measured via ROUGE-1,2,L (R1, R2, RL) scores. Semantic similarity\nuses embedding cosine similarity with the same encoder as retrieval to minimize\nmetric/encoder mismatch. Vector similarity (VSIM), implemented with Gensim,\ncaptures term-level relevance through Word2Vec embeddings. Distributional di-\nvergence is quantified through Jensen-Shannon divergence (JSD) and Kullback-\nLeibler divergence (KLD), where lower values indicate closer distributions. All\nmetrics are computed per query and macro-averaged across medicines.\n5\nResults and Discussion\n5.1\nOverall Performance\nTable 2 presents comprehensive performance metrics across all models and the\nthree retrieval configurations. Several key patterns emerge from these results:\nModel Performance Hierarchy. Mixtral consistently achieved the high-\nest performance in the most-similar condition (R1=0.163, BERTcosine=0.601),\nfollowed closely by Med-LLaMA (R1=0.156, BERTcosine=0.573) and Falcon\n(R1=0.154, BERTcosine=0.589). The superior performance of Mixtral can be\nattributed to its mixture-of-experts architecture, enabling more nuanced pro-\ncessing of medical terminology and context.\nImpact of Contradictions. Across models, performance consistently de-\ngraded when moving from most-similar to most-contradictory retrieval condi-\ntions. The average R1 score decreased by 18.2% when models were provided with\n9\n"}, {"page": 10, "text": "Table 2. Macro-averaged results (short references) for 5 LLMs across 3 retrieval\nconditions. Higher is better except JSD/KLD (↓). Ret.=Retrieval, ms=most sim-\nilar, mc=most contradicted, lc=least contradicted, cos=cosine, dot=dot-product.\nVSIM=vector similarity computed with Gensim.\nModel\nRet.\nROUGE\nBERT\nVSIM\nJSD↓KLD↓\nR1\nR2\nRL\ncos\ndot\ncos\ndot\nFalcon3-\nms\n0.154 0.027 0.103 0.589 0.589 0.708 34.07\n0.213 3.130\n7B\nmc\n0.148 0.025 0.099 0.571 0.570 0.689 32.52\n0.220 3.420\nlc\n0.151 0.026 0.101 0.583 0.583 0.702 33.57\n0.209 3.215\nGemma-3\nms\n0.066 0.018 0.062 0.447 0.446 0.432 12.09\n0.424 2.339\nmc\n0.060 0.018 0.060 0.430 0.430 0.421 11.64\n0.438 2.290\nlc\n0.064 0.017 0.060 0.437 0.437 0.428 11.98\n0.427 2.373\nGPT-OSS-\nms\n0.114 0.016 0.087 0.559 0.559 0.676 18.40\n0.243 3.394\n20B\nmc\n0.098 0.011 0.076 0.545 0.545 0.661 15.05\n0.260 3.695\nlc\n0.100 0.016 0.079 0.549 0.549 0.668 16.37\n0.257 3.514\nMed-\nms\n0.156 0.032 0.109 0.573 0.573 0.645 28.69\n0.256 2.772\nLLaMA3-\nmc\n0.134 0.027 0.097 0.529 0.529 0.607 24.75\n0.283 2.988\n8B\nlc\n0.137 0.029 0.099 0.537 0.537 0.619 25.19\n0.278 2.653\nMixtral-\nms\n0.163 0.029 0.118 0.601 0.601 0.706 32.74\n0.214 3.208\n8x7B\nmc\n0.131 0.022 0.096 0.551 0.551 0.670 30.33\n0.225 3.752\nlc\n0.140 0.023 0.099 0.579 0.579 0.690 30.84\n0.223 2.813\ncontradictory documents. This was most pronounced for Mixtral, which showed\na 20% reduction in R1, suggesting that larger models may be more susceptible\nto conflicting information despite their generally superior performance. Based\non the results, performance under the least-contradictory condition was higher\nthan the most-contradictory but still fell short of the most-similar condition.\nSemantic vs. Lexical Alignment. While ROUGE scores showed sub-\nstantial variation across conditions, semantic similarity metrics (BERTcosine)\ndemonstrated greater stability, with an average decrease of only 8.3% between\nmost-similar and most-contradictory conditions. This suggests that models main-\ntain conceptual understanding even when struggling with precise lexical genera-\ntion under contradiction. VSIM showed a similar trend, with stronger alignment\nunder most-similar retrievals and moderate declines under contradictory evi-\ndence. JSD values stayed stable across conditions, with only small differences\nbetween most-similar and most-contradictory settings. In contrast, KLD showed\nconsistently higher values under contradictory retrievals.\nModel-Specific Observations. Med-LLaMA, despite being fine-tuned for\nmedical applications, did not consistently outperform general-purpose models.\nThis suggests that domain-specific training may not fully compensate for the\nchallenges posed by contradictory retrieval contexts. GPT-OSS demonstrated\nthe most consistent performance across retrieval conditions, with only a 14%\ndegradation between best and worst conditions. Its transformer MoE architec-\n10\n"}, {"page": 11, "text": "ture appears to provide robustness against conflicting information, though at\nthe cost of lower peak performance.\n5.2\nContradiction and Diversity-Aware Score Joint Distribution\nTo understand how document contradictions interact with the diversity-aware\nscore S(·), a 2-D frequency table was computed over all documents considered\nduring retrieval. Table 3 reveals critical insights about the relationship between\ndocument diversity-aware score and contradiction likelihood. The highest fre-\nquency of documents (5,492) falls in the intersection of high contradiction scores\n(0.8-1.0) and moderate diversity-aware scores (0.2-0.4). This counterintuitive\nfinding suggests that documents with intermediate topical relevance are most\nlikely to contain conflicting information, potentially due to the same medical\nconcepts but from different temporal or methodological perspectives.\nTable 3. Document frequency by binned diversity-aware similarity score S (columns)\nand contradiction score CNT (rows). Zero values indicate that no documents fall into\nthose ranges.\nCNT\nDiversity-Aware Similarity Score S\n[0, 0.2)\n[0.2, 0.4)\n[0.4, 0.6)\n[0.6, 0.8)\n[0.8, 1]\n[0, 0.2)\n695\n1712\n584\n0\n0\n[0.2, 0.4)\n816\n2501\n642\n0\n0\n[0.4, 0.6)\n1058\n3435\n982\n0\n0\n[0.6, 0.8)\n1328\n4983\n1295\n0\n0\n[0.8, 1]\n1617\n5492\n1249\n0\n0\n5.3\nTemporal Distribution of Contradiction Scores\nTo better understand how contradictions evolve over time, we aggregated doc-\numents into 5-year bins and computed proportions across contradiction score\nbins. Figure 2 presents the resulting heatmap.\n<2000\n2000 05\n2005 10\n2010 15\n2015 20\n2020 25\nYear\n[0,0.2)\n[0.2,0.4)\n[0.4,0.6)\n[0.6,0.8)\n[0.8,1]\nContradiction Score\n744\n236\n300\n332\n522\n853\n861\n344\n424\n507\n705\n1078\n947\n444\n640\n783\n1055\n1579\n1096\n623\n915\n1121\n1588\n2220\n984\n767\n1117\n1424\n1794\n2258\n500\n1000\n1500\n2000\nDocument Count\nFig. 2. Normalized distribution of documents across contradiction score bins and 5-\nyear publication intervals.\n11\n"}, {"page": 12, "text": "Table 4. Models evaluated in our RAG experiments. We report the *released* model\ncharacteristics and the *exact* inference variant used for reproducibility.\nModel (ID)\nFamily/Architecture Params\nContext\nLicense\nFalcon3-7B [25]\nDecoder-only (dense)\n7.46B\n32k\nTII Falcon\ngemma-3 (MLX) [7]\nDecoder-only (dense)\n0.27B\n32k\nGemma\nGPT-OSS-20b [17]\nTransformer MoE\n21B\n128k\nApache-2.0\nMed-LLaMA3-8B [14] Llama-3-8B (dense)\n8.03B\n8k\nLlama-3\nMixtral-8x7B [10]\nMoE (8×7B; top-2)\n46.7B\n32k\nApache-2.0\nThe results reveal a clear temporal shift in the prevalence of contradictions.\nBefore 2000, most documents fall into the lower contradiction score bins, sug-\ngesting relatively consistent findings across biomedical literature. From the early\n2000s onwards, however, the share of documents in higher contradiction bins\n([0.6–0.8) and [0.8–1]) rises steadily. By 2010–2025, these bins account for around\nhalf or more of all documents per interval, surpassing the lower bins. This in-\ndicates that contradictions have become proportionally more prevalent in later\nyears, reflecting both the rapid expansion of biomedical research and the over-\nturning of earlier clinical consensus. These findings underscore the importance\nof contradiction-aware retrieval strategies that explicitly consider temporal dy-\nnamics when integrating evidence into RAG systems.\n5.4\nLimitations and Future Work\nThis study has several limitations. First, our contradiction detection relies on\nsentence-level, which may miss document-level contradictions. Second, we eval-\nuate only English-language documents, limiting generalization to multilingual\nmedical contexts. Future work should explore more sophisticated contradiction\nresolution strategies, including argumentation mining and evidence synthesis\ntechniques. Additionally, incorporating clinical guidelines and expert knowledge\nbases could help resolve contradictions by establishing authoritative sources. De-\nveloping specialized medical RAG architectures that explicitly model temporal\nevolution and evidence strength remains a direction for future research.\n6\nConclusion\nThis work provides the first systematic evaluation of contradictory information\nin medical RAG systems through a purpose-built TGA–PubMed dataset across\n1,476 medicines. Our analysis shows that contradictions in retrieved evidence\nconsistently degrade model performance, with average R1 scores declining by\n18.2% when contradictory documents are present. Notably, even semantically\nsimilar documents often contain conflicting information, with over 5,400 docu-\nment pairs exhibiting high contradiction scores.\n12\n"}, {"page": 13, "text": "These findings underscore contradictions in medical literature as a critical\nvulnerability for RAG, where a 20% performance loss is unacceptable in high-\nstakes applications. Future systems must therefore integrate contradiction de-\ntection and resolution, leveraging temporal reasoning and uncertainty quantifi-\ncation. Beyond technical metrics, our dataset establishes a benchmark for eval-\nuating RAG robustness and highlights the urgent need for contradiction-aware\narchitectures to ensure factual accuracy in evolving healthcare contexts.\n13\n"}, {"page": 14, "text": "Bibliography\n[1] Cochrane\nlibrary.\nsearch\n|\ncochrane\nlibrary.\nhttps://www.\ncochranelibrary.com/cdsr/reviews (2025)\n[2] American Heart Association: Statements search. https://professional.\nheart.org/en/guidelines-statements-search (2025)\n[3] Carbonell, J., Goldstein, J.: The use of mmr, diversity-based reranking for\nreordering documents and producing summaries. ACM SIGIR Forum 51(2),\n335–336 (1998)\n[4] Carpenter, D., Geryk, L., AT, A.C., Nagler, R., Dieckmann, N., Han, P.:\nConflicting health information: A critical research need. Health Expect 19,\n1173–1182 (2016). https://doi.org/10.1111/hex.12438\n[5] Cohan, A., Feldman, S., Beltagy, I., Downey, D., Weld, D.: Specter:\ndocument-level representation learning using citation-informed transform-\ners. 2020. arXiv preprint arXiv:2004.07180 (2004)\n[6] Das, S., Ge, Y., Guo, Y., Rajwal, S., Hairston, J., Powell, J., Walker,\nD., Peddireddy, S., Lakamana, S., Bozkurt, S., et al.: Two-layer retrieval-\naugmented generation framework for low-resource medical question answer-\ning using reddit data: proof-of-concept study. Journal of Medical Internet\nResearch 27, e66220 (2025)\n[7] DeepMind, G.: Gemma 3 270m instruction-tuned (mlx 8-bit). https://\nhuggingface.co/mlx-community/gemma-3-270m-it-8bit (2025)\n[8] Deka,\nP.:\nPubmedbert-mnli-mednli.\nhttps://huggingface.co/\npritamdeka/PubMedBERT-MNLI-MedNLI (2021)\n[9] Dong, X., Zhu, W., Wang, H., Chen, X., Qiu, P., Yin, R., Su, Y., Wang,\nY.: Talk before you retrieve: Agent-led discussions for better rag in medical\nqa. arXiv preprint arXiv:2504.21252 (2025)\n[10] mradermacher (GGUF), M.A.: Mixtral-8x7b-instruct-v0.1 (gguf). https:\n//huggingface.co/mradermacher/Mixtral-8x7B-Instruct-v0.1-GGUF\n(2023), apache-2.0; 32k context\n[11] Honnibal, M., Montani, I., Van Landeghem, S., Boyd, A.: spacy: Industrial-\nstrength natural language processing in python (2020). https://doi.org/\n10.5281/zenodo.1212303\n[12] Jin, Q., Kim, W., Chen, Q., Comeau, D.C., Yeganova, L., Wilbur, W.J., Lu,\nZ.: Medcpt: Contrastive pre-trained transformers with large-scale pubmed\nsearch logs for zero-shot biomedical information retrieval. Bioinformatics\n39(11), btad651 (2023)\n[13] Johnson, J., Douze, M., Jégou, H.: Billion-scale similarity search with gpus.\nIEEE Transactions on Big Data 7(3), 535–547 (2019)\n[14] Lab,\nY.B.X.:\nMed-llama3-8b.\nhttps://huggingface.co/YBXL/\nMed-LLaMA3-8B (2024)\n[15] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N.,\nKüttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.:\nRetrieval-augmented generation for knowledge-intensive NLP tasks. In:\n"}, {"page": 15, "text": "Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Ad-\nvances in Neural Information Processing Systems. vol. 33, pp. 9459–9474.\nCurran Associates, Inc. (2020)\n[16] Low, Y.S., Jackson, M.L., Hyde, R.J., Brown, R.E., Sanghavi, N.M., Bald-\nwin, J.D., Pike, C.W., Muralidharan, J., Hui, G., Alexander, N., et al.:\nAnswering real-world clinical questions using large language model based\nsystems. arXiv preprint arXiv:2407.00541 (2024)\n[17] OpenAI:\ngpt-oss-20b\nmodel\ncard.\nhttps://huggingface.co/openai/\ngpt-oss-20b (2025), apache-2.0; 21B total, 3.6B active; 128k context\n[18] Ou, J., Huang, T., Zhao, Y., Yu, Z., Lu, P., Ying, R.: Experience retrieval-\naugmentation with electronic health records enables accurate discharge qa.\narXiv preprint arXiv:2503.17933 (2025)\n[19] Pal, A., Umapathi, L.K., Sankarasubbu, M.: Med-HALT: Medical domain\nhallucination test for large language models. In: Jiang, J., Reitter, D., Deng,\nS. (eds.) Proceedings of the 27th Conference on Computational Natural\nLanguage Learning (CoNLL). pp. 314–334. Association for Computational\nLinguistics, Singapore (Dec 2023). https://doi.org/10.18653/v1/2023.\nconll-1.21, https://aclanthology.org/2023.conll-1.21/\n[20] Robertson, S., Zaragoza, H., et al.: The probabilistic relevance framework:\nBm25 and beyond. Foundations and Trends® in Information Retrieval 3(4),\n333–389 (2009)\n[21] Shi, Y., Xu, S., Yang, T., Liu, Z., Liu, T., Li, X., Liu, N.: MKRAG: Medical\nknowledge retrieval augmented generation for medical question answering.\nIn: Proceedings of AMIA Annual Symposium (2025)\n[22] Shi, Y., Xu, S., Yang, T., Liu, Z., Liu, T., Li, X., Liu, N.: MKRAG: Medical\nknowledge retrieval augmented generation for medical question answering.\nIn: AMIA Annual Symposium Proceedings. vol. 2024, p. 1011 (2025)\n[23] Stuhlmann, L., Saxer, M.A., Fürst, J.: Efficient and reproducible biomedical\nquestion answering using retrieval augmented generation. arXiv preprint\narXiv:2505.07917 (2025)\n[24] Taylor,\nT.:\nHow\nto\nmake\nsense\nof\ncontradictory\nhealth\nnews,\nhttps://www.abc.net.au/news/health/2018-04-24/\nmaking-sense-of-seemingly-contradictory-health-news/9343684,\naccessed: 2025-09-16\n[25] Technology\nInnovation\nInstitute:\nFalcon3-7b-instruct.\nhttps:\n//huggingface.co/tiiuae/Falcon3-7B-Instruct\n(2024),\nlicense:\nTII Falcon-LLM 2.0\n[26] Upadhyay, R., Viviani, M.: Enhancing health information retrieval with rag\nby prioritizing topical relevance and factual accuracy. Discover Computing\n28(1), 27 (2025)\n[27] Wang, C., Chen, Y.: Evaluating large language models for evidence-based\nclinical question answering. arXiv preprint arXiv:2509.10843 (2025)\n[28] Wang, Z., Khatibi, E., Rahmani, A.M.: Medcot-rag: Causal chain-\nof-thought\nrag\nfor\nmedical\nquestion\nanswering.\narXiv\npreprint\narXiv:2508.15849 (2025)\n15\n"}, {"page": 16, "text": "[29] Xiao, S., Liu, Z., Zhang, P., Muennighoff, N.: C-pack: Packaged resources to\nadvance general chinese embedding. arXiv preprint arXiv:2309.07597 (2023)\n[30] Xiong, G., Jin, Q., Lu, Z., Zhang, A.: Benchmarking retrieval-augmented\ngeneration for medicine. In: Ku, L.W., Martins, A., Srikumar, V. (eds.)\nACL (Findings). pp. 6233–6251. Association for Computational Linguistics\n(2024)\n[31] Xu, R., Qi, Z., Guo, Z., Wang, C., Wang, H., Zhang, Y., Xu, W.: Knowledge\nconflicts for LLMs: A survey. In: Al-Onaizan, Y., Bansal, M., Chen, Y.N.\n(eds.) EMNLP. pp. 8541–8565. Association for Computational Linguistics\n(2024)\n[32] Yan, S.Q., Gu, J.C., Zhu, Y., Ling, Z.H.: Corrective retrieval augmented\ngeneration (2024), https://arxiv.org/abs/2401.15884\n[33] Zhang, G., Xu, Z., Jin, Q., Chen, F., Fang, Y., Liu, Y., Rousseau, J.F., Xu,\nZ., Lu, Z., Weng, C., et al.: Leveraging long context in retrieval augmented\nlanguage models for medical question answering. npj Digital Medicine 8(1),\n239 (2025)\n[34] Zhao, X., Liu, S., Yang, S.Y., Miao, C.: Medrag: Enhancing retrieval-\naugmented generation with knowledge graph-elicited reasoning for health-\ncare copilot. In: Proceedings of the ACM on Web Conference 2025. pp.\n4442–4457 (2025)\n16\n"}]}