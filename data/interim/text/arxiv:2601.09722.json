{"doc_id": "arxiv:2601.09722", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.09722.pdf", "meta": {"doc_id": "arxiv:2601.09722", "source": "arxiv", "arxiv_id": "2601.09722", "title": "ADMEDTAGGER: an annotation framework for distillation of expert knowledge for the Polish medical language", "authors": ["Franciszek Górski", "Andrzej Czyżewski"], "published": "2025-12-27T10:00:52Z", "updated": "2025-12-27T10:00:52Z", "summary": "In this work, we present an annotation framework that demonstrates how a multilingual LLM pretrained on a large corpus can be used as a teacher model to distill the expert knowledge needed for tagging medical texts in Polish. This work is part of a larger project called ADMEDVOICE, within which we collected an extensive corpus of medical texts representing five clinical categories - Radiology, Oncology, Cardiology, Hypertension, and Pathology. Using this data, we had to develop a multi-class classifier, but the fundamental problem turned out to be the lack of resources for annotating an adequate number of texts. Therefore, in our solution, we used the multilingual Llama3.1 model to annotate an extensive corpus of medical texts in Polish. Using our limited annotation resources, we verified only a portion of these labels, creating a test set from them. The data annotated in this way were then used for training and validation of 3 different types of classifiers based on the BERT architecture - the distilled DistilBERT model, BioBERT fine-tuned on medical data, and HerBERT fine-tuned on the Polish language corpus. Among the models we trained, the DistilBERT model achieved the best results, reaching an F1 score > 0.80 for each clinical category and an F1 score > 0.93 for 3 of them. In this way, we obtained a series of highly effective classifiers that represent an alternative to large language models, due to their nearly 500 times smaller size, 300 times lower GPU VRAM consumption, and several hundred times faster inference.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.09722v1", "url_pdf": "https://arxiv.org/pdf/2601.09722.pdf", "meta_path": "data/raw/arxiv/meta/2601.09722.json", "sha256": "c9b90da50c2990dab076e4c9042f84600a9b10973b3612e48fada31c574f52b1", "status": "ok", "fetched_at": "2026-02-18T02:23:46.002551+00:00"}, "pages": [{"page": 1, "text": "ADMEDTAGGER: AN ANNOTATION FRAMEWORK FOR\nDISTILLATION OF EXPERT KNOWLEDGE FOR THE POLISH\nMEDICAL LANGUAGE\nFranciszek Górski\nMultimedia Systems Department\nGdansk University of Technology\nGdansk, 80-233\nfranciszek.gorski@pg.edu.pl\nAndrzej Czy˙zewski\nMultimedia Systems Department\nGdansk University of Technology\nGdansk, 80-233\nandrzej.czyzewski@pg.edu.pl\nJanuary 16, 2026\nABSTRACT\nIn this work, we present an annotation framework that demonstrates how a multilingual LLM\npretrained on a large corpus can be used as a teacher model to distill the expert knowledge needed\nfor tagging medical texts in Polish. This work is part of a larger project called ADMEDVOICE,\nwithin which we collected an extensive corpus of medical texts representing five clinical categories -\nRadiology, Oncology, Cardiology, Hypertension, and Pathology. Using this data, we had to develop a\nmulti-class classifier, but the fundamental problem turned out to be the lack of resources for annotating\nan adequate number of texts. Therefore, in our solution, we used the multilingual Llama3.1 model\nto annotate an extensive corpus of medical texts in Polish. Using our limited annotation resources,\nwe verified only a portion of these labels, creating a test set from them. The data annotated in this\nway were then used for training and validation of 3 different types of classifiers based on the BERT\narchitecture - the distilled DistilBERT model, BioBERT fine-tuned on medical data, and HerBERT\nfine-tuned on the Polish language corpus. Among the models we trained, the DistilBERT model\nachieved the best results, reaching an F1 score > 0.80 for each clinical category and an F1 score >\n0.93 for 3 of them. In this way, we obtained a series of highly effective classifiers that represent an\nalternative to large language models, due to their nearly 500 times smaller size, 300 times lower GPU\nVRAM consumption, and several hundred times faster inference.\nKeywords Large Language Models · Knowledge distillation · BERT models · AI in medicine · Polish language · NLP\ndatasets · Multiclass classification · Radiology · Hypertension · Oncology · Cardiology · Pathology.\narXiv:2601.09722v1  [cs.CL]  27 Dec 2025\n"}, {"page": 2, "text": "A PREPRINT - JANUARY 16, 2026\n1\nIntroduction\nADMEDVOICE is an R&D project for the polish health-\ncare system which aim is to develop and implement a\nsolution through which doctors can fill out medical records\nduring the course of a medical interview, create descrip-\ntions (e.g., radiological) as needed, and prescribe treatment.\nThe use case of filling out of medical records ca be seen in\nFigure 1. The system to be developed will automatically\ngenerate templates for medical interviews and descriptions\nof available diagnostic results. In Polish healthcare in-\nstitutions, doctors are typically not assisted by medical\nsecretaries or medical assistants; therefore, the responsi-\nbility for preparing descriptions of examinations, visits,\nor hospitalizations rests solely on the physician, which\nconsumes a significant amount of time. As a result, doc-\ntors have less opportunity to devote adequate attention to\ntheir patients. Physicians are aware of this issue, yet are\nunable to avoid the growing demands of medical docu-\nmentation, they spend an increasing portion of their day\nat the computer. There is a clear need to explore ways\nto support healthcare through modern technologies that\nleverage information systems, especially by leveraging the\nlatest advancements in artificial intelligence.\nIn this paper, we address the problem of insufficient labeled\ndata for classifying specialized medical data. This is a\nprevalent issue in machine learning applications. Data\nlabeling not only requires time and appropriate tools, but\nalso often qualified experts who would handle the labeling\nprocess. Unfortunately, the work of these experts is often\ncostly, and their time is limited. As a result, machine\nlearning teams are unable to obtain the number of labels to\nconduct supervised training of a given machine learning\nmodel.\nTherefore, the knowledge distillation approach is increas-\ningly being used, in which a larger AI model - the teacher\nis employed to train a smaller model - the student. Knowl-\nedge distillation (KD), first formalized by Hinton et al.\n[1], has evolved into a cornerstone technique for model\ncompression and knowledge transfer. Several works and\nsurveys have mapped this evolving landscape [2], [3], [4],\n[5], [6], [7]. Gou et al. [8] provide systematic review, cate-\ngorizing KD methods from perspectives of knowledge cate-\ngories, training schemes, teacher-student architectures, and\ndistillation algorithms. KD also has application for LLMs.\nXu et al. [9] presented a comprehensive survey specifically\naddressing KD’s role in the LLM era, structured around\nthree foundational pillars: algorithm, skill, and vertical-\nization. Their work highlights KD’s dual function: trans-\nferring advanced capabilities from proprietary models like\nGPT-4 to open-source alternatives (e.g., LLaMA, Mistral),\nand enabling model compression and self-improvement in\nopen-source LLMs.\nIn this paper, we present a use case for medical data classi-\nfication, in which a large language model (LLM) serves as\nthe teacher model and a model from the BERT family as\nthe student model.\nDuring developing this publication, we were looking for\nanswers to the following research questions:\nRQ1: Can a large language model (LLM), pre-trained\non a vast corpus of textual data covering many thematic\ncategories, effectively distill specialized expert knowledge\nfrom a specific domain using only a few textual examples?\nRQ2: Can a large language model effectively distill\nknowledge in a natural language such as Polish, which\nis significantly less common and more complex than\nEnglish?\nRQ3:\nCan a student model from the BERT family\nsuccessfully learn a multiclass classification task solely\nfrom data with distilled labels?\nIn the following paper we present ADMEDTAGGER -\nthe annotation framework for knowledge distillation from\nLarge Language Models to smaller, BERT-based models\nfor the task oexpert validation used to create evaluation\nsetsf classification of Polish medical texts from different\nclinical categories.\nADMEDTAGGER is a response to real clinical needs re-\nquiring the development of a system for annotating elec-\ntronic patient records into structured clinical forms. The\nlearned annotations correspond to individual records in the\nforms, allowing for the quick generation of reports on the\ncourse of specific clinical situations in an automatic man-\nner, thus freeing physicians from the very time-consuming\nprocedure of preparing patient care notes and allowing\nthem to devote significantly more attention to the patient.\nOur solution includes prompt structures for the teacher\nmodel, annotated text segmentation, training set genera-\ntion, expert validation used to create evaluation sets, as well\nas train multiple students models. All of this comprises a\nframework for scalable annotation and expert knowledge\ndistillation, used in this work to train medical text classi-\nfiers. ADMEDTAGGER is part of the ADMEDVOICE\nsystem, being prepared for implementation in Polish medi-\ncal facilities.\n2\nRelated works\nThe introduction of BERT (Bidirectional Encoder Rep-\nresentations from Transformers) by Devlin et al. [10]\nrevolutionized text classification. Fields et al. [11] pro-\nvided a comprehensive survey examining text classification\nwith transformers. Their analysis reveals that while BERT-\nbased models with 110-340M parameters established nu-\nmerous benchmarks, more recent LLMs like GPT-4 have\nextended capabilities, particularly in handling longer con-\ntexts (up to 8,192 tokens compared to BERT’s 512). Sev-\neral studies have investigated specific aspects of BERT for\ntext classification [12, 13, 14, 15, 16, 17]. Jiang et al. [18]\nconducted a critical review asking \"Are We Really Making\nMuch Progress in Text Classification?\" Their analysis re-\nvealed that fine-tuned discriminative models (particularly\nencoder-only transformers like BERT, RoBERTa, and De-\nBERTa) still define the state-of-the-art for single-label and\n2\n"}, {"page": 3, "text": "A PREPRINT - JANUARY 16, 2026\nASR\nsystem\nUnstructured\ntranscription\nof the clinical\nsituation\nTransformer\nbased\ntext classifier\nStrutured\nmedical\nreport\nfrom the\nclinical\nsituation\nPhysician\nPatient\nClinical situation\nRecoding of the\nclinical situation\nSplitting and classifying\ntext into fields of\nmedical report\nTranscription\nof the clinical\nsituation\nFigure 1: Diagram of the ADMEDVOICE system.\nmulti-label text classification. Surprisingly, despite recent\nadvances in LLMs, methods based on in-context learn-\ning with generative models do not generally outperform\nfine-tuned smaller language models (SLMs) in classifi-\ncation tasks. Even with advanced prompting techniques\nand ensemble methods, LLMs only marginally outper-\nform encoder-only SLMs on individual datasets, requiring\nensemble approaches to match or slightly exceed SLM\nperformance.\nThe biomedical domain has seen substantial development\nof domain-specific language models [19, 20, 21, 22, 23, 24,\n25, 26]. Lee et al. [19] introduced BioBERT, pre-trained\non PubMed abstracts and PMC full-text articles, which\nachieved state-of-the-art results on biomedical named\nentity recognition, relation extraction, and question an-\nswering. BioBERT’s success demonstrated that domain-\nspecific pre-training significantly improves performance\non specialized tasks compared to general-domain BERT.\nAlsentzer et al. [27] released ClinicalBERT, specifically\ntrained on clinical notes from the MIMIC-III database.\nTheir work presented multiple variants: models initialized\nfrom BERT-base or BioBERT, and trained on all notes or\nonly discharge summaries. Bio+Clinical BERT (initialized\nfrom BioBERT and trained on all MIMIC notes) showed\nparticular promise for clinical NLP tasks, though inter-\nestingly performed worse on de-identification tasks—a\nconsequence of differences between de-identified source\ntext and synthetically non-de-identified task text. Wang et\nal. [28] compared BERT implementations (vanilla BERT,\nBioBERT, and ClinicalBERT) for identifying complex\nmedical concepts in narrative documents. Their results\non three linguistically complex tasks (bariatric surgery dis-\ncussions, statin non-acceptance, and tobacco use documen-\ntation) revealed that domain-specific models (BioBERT\nand ClinicalBERT) achieved superior performance overall,\nthough neither consistently outperformed the other across\nall tasks. There is also a set of papers on employing LLMs\nin the biomedical domain like [29, 30, 31, 32].\nSakai and Lam [33] introduced KDH-MLTC (Knowledge\nDistillation for Healthcare Multi-Label Text Classifica-\ntion), a comprehensive framework leveraging model com-\npression and LLMs specifically for medical text. Their\napproach transfers knowledge from BERT to DistilBERT\nthrough sequential training adapted for multi-label classi-\nfication, with hyperparameter optimization via Particle\nSwarm Optimization (PSO). Experiments on the Hall-\nmarks of Cancer (HoC) dataset achieved an F1 score\nof 82.70% on the largest dataset variant, demonstrating\nthat knowledge distillation can maintain high accuracy\nwhile significantly reducing model size and inference time.\nHasan et al. [34], referenced in the KDH-MLTC study,\nachieved strong results using DischargeBERT and CORe-\nBERT as teacher models with BERT-PKD as the student\nmodel, obtaining AUC scores ranging from 72.36% to\n86.95% across multiple clinical prediction tasks. These\nworks demonstrated the viability of specialized clinical\nBERT variants as teachers for distillation in medical con-\ntexts.\nCross-lingual text classification has emerged as a crucial\ntechnique for leveraging resources from high-resource lan-\nguages to enable classification in low-resource languages.\nMultiple approaches have been proposed to address this\nchallenge [35, 36, 37, 38, 39, 40]. These works tested\nmany languages like Japanese, German, French, Chinese\nor Spanish.\nThere are not so many works on Polish language especially\nfor the medical domain. Mroczkowski et al. [41] intro-\nduced HerBERT, an efficiently pre-trained transformer-\nbased language model specifically for the Polish. Her-\nBERT, trained using knowledge transfer from multilingual\nto monolingual models, achieved state-of-the-art results\non the KLEJ (Kompleksowa Lista Ewaluacji J˛ezykowych)\nbenchmark across multiple downstream tasks. HerBERT’s\narchitecture uses the BERT-base configuration with careful\nconsideration of Polish linguistic characteristics, partic-\nularly its rich inflectional morphology. The model has\nbecome the foundation for Polish NLP research, though\nno medical-domain variant has been developed. The KLEJ\n(Kompleksowa Lista Ewaluacji J˛ezykowych) [42] bench-\nmark provides nine evaluation tasks for Polish language\nunderstanding, including various text classification chal-\nlenges. The Wroclaw Corpus of Consumer Reviews Sen-\ntiment (WCCRS) [43] includes medical domain reviews\n3\n"}, {"page": 4, "text": "A PREPRINT - JANUARY 16, 2026\nbut focuses on sentiment analysis rather than medical text\nclassification. While these resources support general Pol-\nish NLP development, specialized medical text corpora\nremain largely unavailable due to data privacy restrictions\nand limited annotation efforts.\nThe most significant work on Polish medical text classifi-\ncation comes from researchers at Górno´sl ˛askie Centrum\nMedyczne [44], who analyzed Polish electronic health\nrecords for diagnosis prediction in cardiovascular disease\npatients. Using 50,465 hospitalization records, they ap-\nplied transformer language models (RoBERTa and BERT)\ntrained on Polish or multilingual data for multi-label ICD-\n10 diagnosis classification. Their models achieved 70%\naccuracy using only patient medical history (average 132\nwords) and 78% accuracy when incorporating additional\nhospitalization information. This work demonstrated that\nstate-of-the-art NLP techniques can effectively process\nPolish clinical text, though the absence of Polish-specific\nmedical language models limited performance compared\nto English medical NLP systems. The AssistMED project\n[45], documented in Polish Archives of Internal Medicine,\ndeveloped NLP tools for observational clinical research\ndata retrieval from Polish electronic health records. Their\npractical use case highlighted several challenges: limited\navailability of Polish medical terminologies, insufficient\nNLP tools designed explicitly for Polish clinical text, and\ndisappointing performance of general-purpose LLMs like\nChatGPT on Polish medical licensing examinations. The\nproject relied primarily on dictionary-based and rule-based\napproaches rather than deep learning, reflecting the scarcity\nof annotated Polish medical corpora. Zielonka et al. [46]\npresents work in which they evaluate LLMs in the task\nof categorizing medical information into categories such\nas medications, diseases, symptoms, or procedures and\ncompare them with annotations made by medical experts,\ndemonstrating high concordance of the GPT-4 model with\nexpert responses. Czy˙zewski et al. [47] presents work in\nwhich a broad corpus of medical speech recordings and\ntranscriptions from physicians is described and made avail-\nable, containing names of medications and diseases spo-\nken by specialists from many clinical fields. This corpus\ncontains nearly 15 hours of recordings from 28 different\nspeakers, and was additionally expanded synthetically with\nan additional 83 hours of recordings. Such a corpus consti-\ntutes a potential data source that can be used for training\nlanguage models.\nSeveral critical gaps emerge: 1) Absence of Polish med-\nical language models, 2) limited Knowledge Distillation\nfor tasks for Polish medical language, 3) scarcity of anno-\ntated Polish medical corpora and 4) cross-lingual transfer\npotential unexplored.\nIn our work we demonstrate the framework for effective\nKnowledge Distillation for the classification of the Polish\nmedical texts, gathering a wide corpora of Polish medi-\ncal texts for many clinical categories, exploring potential\nfor cross-lingual transfer resulting in developing Polish\nmedical language models for classification tasks.\n3\nADMEDTAGGER\nIn our work, we encountered the problem of having access\nto a relatively large amount of data but lacking sufficient\nlabels for the data classes. Labeling this data required\nspecialists in the form of physicians from specific med-\nical domains. However, their time was severely limited,\nand they were unable to label a sufficient amount of data.\nTherefore, we needed a solution that would allow us to\ndevelop a series of multiclass classifiers with a limited\nnumber of labeled samples.\nIn this paper, we present the solution whose diagram is\nshown in Figure 2. In our approach, we used the physi-\ncians’ limited time to label only small subsets of data from\neach specialization, then split these into a test set and a\nfew contextual examples. The contextual examples were\nsubsequently inserted into the prompt of a large language\nmodel to enable few-shot contextual learning. The LLM\nwas used as the teacher model for labeling the remaining\ndata in each specialization. For this task, we selected the\nLlama3.1-70B model. Unlike classical pseudo-labeling,\nthe teacher model transfers structured, domain-specific an-\nnotation behavior derived from expert-designed clinical\nforms and ontologies.\n3.1\nTeacher and Student Models\nThe knowledge distillation approach relies on the operation\nof two models: a teacher model and a student model. In\nour solution, we chose to use a relatively large LLM as\nthe teacher model—one pretrained on an extensive data\ncorpus that potentially includes not only many natural\nlanguages other than English, but also content from various\nthematic domains, including medical texts. Therefore, we\nselected the Llama3.1-70B-Instruct model, a multilingual\nlarge model pretrained on a corpus of up to 15 trillion\ntokens. The model is available as open weights and can\nbe used locally. We used the Instruct version, which is\nadapted for handling instruction-based prompting.\nAs the student model, we selected three models from the\nBERT family. The chosen models were: 1) HerBERT\n(HerBERT-base-cased) - explicitly trained on a Polish lan-\nguage corpus, 2) BioBERT (BioBERT-base-cased-v1.1) -\ntrained on a corpus of English medical texts, and 3) Distil-\nBERT (DistilBERT-base-multilingual-cased) - a specially\ndistilled multilingual model. All of these models contain\nno more than 134 million parameters. This selection al-\nlowed us to achieve potentially very large computational\ncost reductions, shrinking the model size by more than\n500 times while narrowing its functionality to the task of\nmulticlass classification of medical texts.\n3.2\nPrompt Construction\nThe structure of the prompt used for every clinical situ-\nation is shown in Figure 3. It shows the example for a\nradiology situation, but for every other clinical situation\nthe structure remains with changes only to the list of labels\n4\n"}, {"page": 5, "text": "A PREPRINT - JANUARY 16, 2026\nTeacher\nmodel\nLLM\nDatabase of\nmedical texts\nfor given\nclinical\nsituation\nUnstructured\nMedical\ntexts\nDatabase of\npreannotated\nmedical texts\nPreannotated\nmedical\ntexts\nSubset of\npreannotated\ntexts\nGroup\nof experts\nValidation of the\nannotations\ncorrectness\nValidated\nmedical\ntexts\nStudent\nmodel\ntransformer\nbased\nTraining\ndata\nmedical\ntexts\nValidation\non checked\ndata\nKnowledge\ndistilation\ntraining\nUnsupervised\nannotation\nof the data\nFigure 2: Diagram of the whole ADMEDTAGGER methodology.\nand In-context examples. The prompt consists of:\nSystem message: for each specialization, it contains a\nbrief description indicating which medical specialty the\nmodel should assume and what labels/classes are available\nfor that specialization.\nIn-context examples: demonstrating how the text should\nbe classified into the respective classes and what the ex-\npected output format should be.\nMedical text: input text that needs to be classified.\n3.3\nDatasets\nIn our work, we used medical datasets in Polish. As part of\nthe project, we collected a total of 143,756 medical texts\nfrom five clinical scenarios: Cardiology, Radiology, On-\ncology, Pathomorphology, and Hypertension but limited\neach category to 5000 samples which gives us 21,073 texts,\nbecause Pathology and Hypertension did not reach 5000.\nAs a result, we obtained a substantial set of real clinical\ntexts representing important areas of medicine. Each clini-\ncal scenario dataset has a separate set of tags that denote\nspecific types of information relevant to documenting the\ncourse of that clinical case. During the knowledge distil-\nlation stage, the teacher model annotates entire texts with\nsegments corresponding to individual concepts/tags from\nthe forms associated with each clinical scenario. Since\nthe task designed for the student models is multiclass text\nclassification, the full texts were divided during the dis-\ntillation stage into segments that correspond precisely to\na given tag/section of the clinical form. The detailed dis-\ntribution of tags in the training and test sets is shown in\nthe charts in Figure 4. These charts clearly show that in\n4 out of the 5 training datasets there is a pronounced im-\nbalance in the representation of specific tags. However,\nthis class imbalance problem was addressed during train-\ning by assigning weights to each class when training the\nclassification model.\n3.4\nMetrics\nWe decided to evaluate results with the following metics:\nF1 score - is a harmonic mean of the precision and recall,\nwhere an F1 score reaches its best value at 1 and worst\nscore at 0. The relative contribution of precision and recall\nto the F1 score are equal. We calculate this metric for each\nlabel, and find their unweighted mean.\nAUROC - Area Under the Receiver Operating Charac-\nteristic Curve (ROC AUC) from prediction scores. Here\nwe also calculate this metric for each label, and find their\nunweighted mean.\nAccuracy - accuracy is simply the fraction of correct\nclassifications with 95% confidence interval.\n4\nExperiments\nOur experiments followed the pipeline shown in Figure 2.\nFirst, the teacher model pre-annotated texts for each clini-\ncal scenario. From this annotated set, we extracted a subset\ncontaining representations of all tags for the given clini-\ncal scenario. This subset was then validated by medical\nspecialists—physicians from the respective fields—who\nreviewed the annotations and introduced corrections where\nerrors appeared. The next step was training all three classi-\nfier models for each clinical scenario. Finally, each trained\nBERT classifier was evaluated on the physician-validated\ntest sets.\n5\n"}, {"page": 6, "text": "A PREPRINT - JANUARY 16, 2026\nYou are a Radiologist whose job is to annotate fragments of medical text in\nnatural language into the following categories:\n[BIRADS', 'GLANDULAR', 'GLANDULAR_ADIPOSE', 'ADIPOSE',\n'ADIPOSE_GLANDULAR', 'LEFT', 'L_BIRADS', 'BOTH', 'RIGHT',\n'DUCT_NOT_DILATED', 'DUCT_DILATED', 'R_BIRADS', 'NOTES',\n'NODES_NOT_SUSPICIOUS', 'NODES_SUSPICIOUS',\n'RECOMMENDATIONS']\nDuring the annotation process, remember to only annotate portions of the input\ntext and not add any additional content. Please list the annotated texts as\nshown in the examples below:\nIn-context examples\nMedical text\nInput text:\nBreasts with a glandular structure. PP - no focal lesions were seen.\nPL - a small cyst measuring 5x2mm at 10 o'clock. No other focal lesions were\nseen. Milk ducts not dilated. Axillary fossa without lymphadenopathy, several\nreactive lymph nodes up to 5x9mm in the left armpit. BI-RADS 2 findings\nAnnotation results:\nLEFT:\nPL - a small cyst measuring 5x2mm at 10 o'clock.\nNo other focal lesions were seen.\nRIGHT:\nPP - no focal lesions were seen.\nNo other focal lesions were seen.\nDUCT_NOT_DILATED:\nMilk ducts not dilated.\nNODES_NOT_SUSPICIOUS:\nAxillary fossa without lymphadenopathy, several reactive lymph nodes up to\n5x9mm in the left armpit.\nL_BIRADS:\nBI-RADS 2\nP_BIRADS:\nBI-RADS 2\nGLANDULAR:\nBreasts with rich glandular structure.\nBoth breasts have a glandular-adipose structure with glandular tissue in the\nlateral and anterior regions. PL - a single simple cyst measuring approximately\n5x4mm at the 3 o'clock position. PP - several scattered small simple cysts\nmeasuring up to 5mm in diameter. No clear signs of infiltration were seen on\nultrasound of either breast. The milk ducts were not dilated. The contours of the\nchest were smooth. The axillae were clear. BIRADS-2. Photographic\ndocumentation was provided - 2 photos. If the clinical picture is stable, I suggest\na follow-up ultrasound in 8-12 months.\nSystem message\nText\nInput Prompt for Radiology Clinical Type\nText\nFigure 3: Prompt construction for the Radiology Clinical\nType.\n4.1\nExperimental Settings\nAll experiments were conducted on a computing server pur-\nchased as part of the ADMEDVOICE project – Adaptive\nIntelligent Speech Processing System for Medical Person-\nnel with the Structuring of Test Results and Support of the\nTherapeutic Process, using 4 NVIDIA L40 GPUs, each\nwith 48GB of VRAM, and an AMD EPYC 75F3 32-Core\nProcessor. The LLama3.1-70B-Instruct model required\n140GB of free disk space and the size of trained BERT\nmodels was 4.6GB per model, which results in 69GB of\nfree disk space. All scripts were executed with Python\n3.10. The pretrained LLM and BERT weights were uti-\nlized through Hugging Face’s transformers library (ver-\nsion 4.33.1) and accelerate (version 0.33.0), with CUDA\n12.1. The models were loaded using the AutoModelFor-\nCausalLM, AutoModelForSequenceClassification and Au-\ntoTokenizer classes with the options device_map=\"auto\"\nand torch_dtype=torch.bfloat16, enabling multi-GPU infer-\nence and reduced GPU memory usage for LLama model.\nTraining of classifiers was conducted on a single NVIDIA\nL40\nGPU\nwith\nbatch_size=512,\ngrad_acc_steps=1,\nlearning_rate=1e-6 and num_epochs=100.\n5\nEvaluation results\nModel Performance Across Clinical Domains:\nIn Table 1 we can see the results of DistilBERT, BioBERT,\nand HerBERT models across five clinical domains, with\nDistilBERT consistently outperforming the other models in\neach settings. DistilBERT achieves the highest F1 scores,\nAUROC values and accuracies with confidence intervals\nindicating stable and reliable performance. In these do-\nmains, BioBERT and HerBERT havew lower results than\nDistilBERT, with BioBERT exhibiting the weakest results\noverall. HerBERT performs moderately, consistently sur-\npassing BioBERT yet remaining below DistilBERT on all\nreported metrics.\nIn Cardiology, DistilBERT shows strong performance (F1\nscore = 0.97, AUROC = 0.98, Accuracy = 0.98), outper-\nforming both BioBERT and HerBERT. Similar patterns\nare observed in Hypertension and Radiology, where Dis-\ntilBERT has higher F1 scores and narrower confidence\nintervals, suggesting robust generalization. In Pathology,\nthe gap between DistilBERT and the other models remains\nsubstantial, with DistilBERT achieving an F1 score of 0.93\ncompared to 0.83–0.88 for BioBERT and HerBERT. The\nOncology domain represents an exception - all three mod-\nels achieve near-perfect classification performance (F1 ≈\n0.99; AUROC ≈1.00; Accuracy ≈0.99). The nearly iden-\ntical metrics and overlapping confidence intervals indicate\nthat the models behave equivalently in this domain.\nOverall, these results demonstrate that DistilBERT pro-\nvides the most reliable and accurate performance in all\nclinical domains, while BioBERT underperforms consis-\ntently. The results in Oncology suggests the presence of an\neasier classification landscape or highly distinctive class\nboundaries, leading all models to converge toward similar\nnear-optimal decisions.\nAdditionally, Figure 5 shows the confusion matrices ob-\ntained for the best-performing model - DistilBERT, for\neach clinical scenario.\nInterpretation of 95% Confidence Intervals:\nThe 95% confidence intervals for accuracy, reported in\nthe Table 1, provide important insight into the reliabil-\nity and stability of the models’ performance across clin-\nical domains. In all domains except Oncology, Distil-\nBERT not only achieves the highest point estimates of\naccuracy but also shows consistently narrow confidence\nintervals, indicating low variability and high certainty in\nits performance. For example, in Cardiology the inter-\nval spans only approximately ±0.006 from the estimated\naccuracy (0.9730–0.9848), whereas the corresponding in-\ntervals for BioBERT and HerBERT are wider. This pattern\nsuggests that DistilBERT not only performs better on av-\nerage but does so with greater consistency across samples.\nA similar trend is observed in Hypertension, Radiology,\n6\n"}, {"page": 7, "text": "A PREPRINT - JANUARY 16, 2026\nLEFT\nRIGHT\nBOTH\nRECOMMENDATIONS\nNOTES\nL_BIRADS\nNODES_NOT_SUSPICIOUS\nGLANDULAR_ADIPOSE\nDUCT_NOT_DILATED\nR_BIRADS\nNODES_SUSPICIOUS\nBIRADS\nADIPOSE_GLANDULAR\nGLANDULAR\nDUCT_DILATED\nADIPOSE\n1478\n482\n1478\n397\n1184\n385\n585\n375\n486\n372\n458\n349\n416\n249\n259\n243\n160\n171\n142\n171\n118\n136\n115\n99\n104\n41\n69\n38\n61\n37\n55\n21\nRADIOLOGY labels distribution\nTrain\nTest\nOTHER_RECOMMENDATIONS\nADDITIONAL_EXAM\nPHYSICAL_EXAM\nMEDICAL_HISTORY\nMEDICATION_RECOMMENDATIONS\nDIAGNOSIS\nOTHER\n447\n278\n329\n257\n244\n240\n215\n203\n215\n192\n42\n129\n7\n110\nHYPERTENSION labels distribution\nTrain\nTest\nCONCLUSIONS\nHEAD_AND_NECK\nOTHER\nABDOMEN_AND_PELVIS\nCHEST\nLYMPH_NODES\n19257\n3852\n19046\n3810\n18993\n3800\n18418\n3684\n18394\n3680\n884\n177\nONCOLOGY labels distribution\nTrain\nTest\nCONCLUSION\nVALVES\nCARDIAC_CHAMBERS_FUNCTION\nCORONARY_ARTERIES_AORTA_ISTHMUS\nTREATMENT_RESULTS\nPATHOLOGIES\nGENERAL_STRUCTURE\nTECHNICAL_INFORMATION\nPULMONARY_ARTERY\n909\n372\n606\n358\n411\n357\n367\n352\n266\n289\n208\n228\n160\n126\n126\n92\n96\n50\nCARDIOLOGY labels distribution\nTrain\nTest\nCROSS_SECTION_DESC\nMATERIAL_DIMENSIONS\nMATERIAL_TYPE\nMATERIAL_DESCRIPTION\nSECTION_LOCATION_NAME\nMARGINS\nADDITIONAL_MATERIAL_DIMENSIONS\nCOMPLICATIONS\nMARKING\nINKING\n625\n63\n586\n56\n461\n45\n382\n41\n238\n40\n195\n31\n176\n25\n164\n25\n122\n23\n113\n18\nPATHOLOGY labels distribution\nTrain\nTest\nFigure 4: Datasets’ labels distribution for each clinical category.\nand Pathology, where DistilBERT maintains narrower in-\ntervals compared to both BioBERT and HerBERT. The\ncomparatively broader intervals for BioBERT, particularly\nin domains where its performance is weak (e.g., Hyper-\ntension: 0.4892–0.5413), indicate greater variability and\nlower confidence in the stability of the model’s predic-\ntions. HerBERT typically falls between two other models -\nis more stable than BioBERT but still shows wider inter-\nvals than DistilBERT. The Oncology domain again is an\nexception. All three models achieve nearly identical ac-\ncuracies with extremely narrow confidence intervals (e.g.,\n0.9922–0.9946), reflecting very high consistency in pre-\ndictions. This tight clustering of intervals reinforces the\nconclusion that Oncology constitutes a comparatively easy\nclassification scenario in which model performance is both\nuniformly high and highly stable.\nTaken together, the confidence interval analysis strength-\nens the conclusions drawn from the point estimates: Distil-\nBERT delivers not only superior accuracy but does so with\ngreater statistical certainty across most clinical domains.\nThe width of the intervals complements the hypothesis that\nBioBERT’s lower performance is accompanied by greater\ninstability, while HerBERT remains competitive yet consis-\ntently less robust than DistilBERT. The uniformly narrow\nintervals in Oncology confirm the domain’s distinct nature,\nin which all models converge toward highly reliable, near-\noptimal classification outcomes.\nModels comparison with non-parametric tests:\nPairwise Wilcoxon signed-rank tests were applied to com-\npare model performance across clinical domains. The\nresults from the Table 2 show substantial variation in the\nadequate sample size used by the test (n_effective), which\nreflects not the number of available samples but the number\nof instances where two models produce different correct-\nness outcomes. This distinction is essential for interpreting\nthe Oncology results.\nIn Cardiology, Radiology, Hypertension and Pathology,\nmodel predictions differed across samples, resulting in\nlarge n_effective values (20–529).\nConsequently, the\nWilcoxon test yielded highly significant differences be-\ntween all model pairs (p-values from 10e-8 to 10e-79), in-\ndicating robust and systematic performance differences in\nthese domains. In contrast, the Oncology dataset—despite\nbeing the largest in absolute size - produced very small\neffective sample sizes (n_effective = 2–5). This indicates\nthat the models made almost identical correct/incorrect\ndecisions on nearly all Oncology samples. As a result, the\nWilcoxon test has very limited power in this domain, with\nonly the BioBERT vs. HerBERT comparison reaching sig-\nnificance (p = 0.02). The remaining non-significant results\nshould therefore be interpreted not as evidence of equal\nperformance, but as a consequence of the extremely small\nnumber of disagreement cases between models.\nOverall, the statistical analysis reveals that meaningful\nmodel differences emerge clearly in domains where mod-\nels disagree on a substantial proportion of samples. In do-\nmains where predictions are nearly identical, as observed\nin Oncology, the Wilcoxon test lacks the sensitivity to de-\ntect differences, even when the dataset itself is large.\nImpact of Knowledge Distillation on Computational\nEfficiency:\nIn Table 3, we observe measurements for each of the 3\nclassifier models and the teacher model (Llama3.1) regard-\ning GPU VRAM memory consumption and inference time\nfor the same input text, which was divided into all labels\n(text field types) recognized within it. These measure-\nments were performed for each of the 5 clinical situations.\nThe measurement results showed enormous differences in\nboth VRAM consumption and inference time between the\nLlama3.1 model and all BERT models. In the case of the\nLlama3.1 model, the demand for VRAM memory during\ninference was very high and ranged between 130 and 150\n7\n"}, {"page": 8, "text": "A PREPRINT - JANUARY 16, 2026\nGB. Meanwhile, for BERT models, this consumption did\nnot exceed 0.55 GB, and was the highest for the Distil-\nBERT models. This results in approximately a 300-fold\ndifference in VRAM consumption for classifying the same\ntext. Similarly unfavorable differences in favor of the LLM\noccur in the case of inference time, although the Llama3.1\nresults are more varied here - the fastest inference took\nplace for Pathology - just under 25 seconds, while the\nlongest was in the case of Oncology, lasting over 113 sec-\nonds. For BERT models, inference time was consistently\nlow and rarely exceeded 0.01 seconds. The exception here\nwas the inference of the DistilBERT model for Cardiology.\nThese measurements demonstrate a clear advantage of\nBERT models over large language models such as\nLlama3.1-70B.\n5.1\nCommon errors\nThe DistilBERT model, despite being our best performer,\nshows its highest error rate in the Hypertension category,\nwhere it achieves an F1 score of 0.81. From the Figure\n5, we see that the model struggles most with the OTHER\nlabel in this category, achieving only 48% accuracy, mainly\nconfusing it with ADDITIONAL_EXAM label in 30% of\ncases. While the exact cause isn’t clear, this might stem\nfrom an imbalance in the training data that favors ADDI-\nTIONAL_EXAM. The model also underperforms on the\nDIAGNOSIS label, with 81% accuracy, frequently confus-\ning it with MEDICAL_HISTORY label in 13% of cases.\nThis particular confusion is understandable - both labels\ncan legitimately apply to certain text fragments, and in this\ncase the decision was made by a team of experts, doctors\nspecializing in the given field. The clinical category with\nthe second lowest F1 score is Radiology. Here, the Distil-\nBERT model made the most mistakes for the L_BIRADS\n(left) and R_BIRADS (right) labels, achieving only 38%\nand 56% accuracy respectively. Both labels are confused\nwith each other and with the BIRADS (both) label. An-\nother common mistakes occur with RIGHT (breast) label\nand DUCT_DILATED. The last clinical category which\nshows some significant labeling errors is Pathology, where\nDistilBERT model has some problems with MARGINS\nand MARKING labels, having 78% and 80% accuracy\nrespectively. The label MARGINS has been mistaken with\nlabel COMPLICATIONS in 11% of cases. From the anal-\nysis of the test input data we can see that once again these\ntwo labels will fit the text but the doctors choose the label\nMARGINS instead of MARKING.\n6\nConclusions\nIn our work, we presented one of the first knowledge dis-\ntillation frameworks for medical text classification tasks\nin Polish under limited data labeling capabilities. Our\nmethod was tested across a wide range of medical do-\nmains, utilizing data from as many as 5 different fields -\nhypertension, cardiology, pathology, oncology, and radi-\nology. We tested 3 different types of BERT models: 1)\nDistilBERT, a model that has already undergone general\nknowledge distillation, 2) BioBERT, a model fine-tuned\non medical data in English, and 3) HerBERT, a model\nfine-tuned on Polish language data. Our results showed\nthe DistilBERT model’s unquestionable advantage in as-\nsimilating knowledge distilled by the Llama3.1 LLM for\nPolish medical text classification. It appears that the cause\nof these differences may be the pretraining stage of each\nof these models and the language corpus was used for this\npurpose. The DistilBERT model is multilingual, trained on\na Wikipedia corpus containing as many as 104 languages,\nincluding Polish. The Polish language corpus is a dump\nof Wikipedia articles, and therefore, it undoubtedly also\nincludes a collection of medical texts. Thus, during the pre-\ntraining phase, the model was exposed to medical data in\nPolish. The HerBERT model was pretrained on Polish lan-\nguage data, but devoid of any medical texts, which resulted\nin its weaker performance compared to the DistilBERT\nmodel. The BioBERT model typically achieves the worst\nresults of the three, because it is an English-centric model,\npretrained on medical data, but only in English, which may\nhave created significant barriers in learning Polish medical\ntexts. In this way, we obtained a model that contains over\n500 times fewer parameters, which translates to nearly\n300 times lower GPU VRAM requirements and takes on\naverage several hundred times less time for inference than\nthe Llama3.1 teacher model, while achieving very high\nclassification efficiency measures by F1 score, AUROC\nand Accuracy.\nThe inference parameters of the distilled classifiers we\nachieved, such as execution time <= 0.01 s, GPU VRAM\nconsumption <= 0.55 GB, as well as their effectiveness,\nallow for the real implementation of our proposed sys-\ntem in medical facilities, for example, for tasks involving\nfilling in electronic health records (EHR). The short infer-\nence time ensures the possibility of real-time operation,\nwhile the low computational requirements allow for in-\nference on non-specialized computer hardware equipped\nwith only a CPU instead of a GPU. Additionally, BERT\nmodels trained for classification tasks guarantee repeata-\nbility and stability of results. Our solution is an alternative\nto using an LLM model as an external service, ensuring\n\"in-house\" operation, solving the problem of data transfer\nto external services, which allows for compliance with\nregulations related to personal/patient data protection such\nas the Polish General Data Protection Regulation (GDPR).\nThe framework we proposed also reduces costs resulting\nfrom payments associated with each use of such an ex-\nternal LLM service for every inference, which radically\nreduces the operating costs of the system at large scale.\nOf course, in our solution there is a risk of bias propagation\nfrom the teacher models, the dependence of quality on the\nfit of the prompt used by the teacher, as well as the need\nfor periodic expert validation of the distilled labels.\nWe show that a pre-trained multilingual large language\nmodel like Llama3.1 can effectively distill specialized ex-\npert knowledge from a specific domain like the medical\n8\n"}, {"page": 9, "text": "A PREPRINT - JANUARY 16, 2026\nDistilBERT\nBioBERT\nHerBERT\nClinical Type\nF1 score\nAUROC\nAccuracy\n95% CI\nF1 score\nAUROC\nAccuracy\n95% CI\nF1 score\nAUROC\nAccuracy\n95% CI\nCardiology\n0.97\n0.98\n0.98\n(0.9730–0.9848)\n0.75\n0.90\n0.85\n(0.8358–0.8654)\n0.85\n0.92\n0.90\n(0.8908–0.9153)\nHypertension\n0.81\n0.90\n0.82\n(0.8040–0.8437)\n0.49\n0.78\n0.52\n(0.4892–0.5413)\n0.53\n0.79\n0.56\n(0.5375–0.5892)\nRadiology\n0.87\n0.93\n0.90\n(0.8890–0.9088)\n0.75\n0.87\n0.78\n(0.7625–0.7899)\n0.79\n0.88\n0.82\n(0.8062–0.8314)\nPathology\n0.93\n0.97\n0.94\n(0.9109–0.9601)\n0.83\n0.91\n0.85\n0.8100–0.8830)\n0.88\n0.94\n0.89\n(0.8550–0.9189)\nOncology\n0.99\n1.00\n0.99\n(0.9922–0.9945)\n0.99\n1.00\n0.99\n(0.9920–0.9943)\n0.99\n1.00\n0.99\n(0.9923–0.9946)\nTable 1: Classification metrics of the trained BERT models on the data labeled by LLM\nGENERAL_STRUCTURE\nTECHNICAL_INFORMATION\nPATHOLOGIES\nPULMONARY_ARTERY\nCORONARY_ARTERIES_AORTA_ISTHMUS\nCARDIAC_CHAMBERS_FUNCTION\nCONCLUSION\nTREATMENT_RESULTS\nVALVES\nPredicted labels\nGENERAL_STRUCTURE\nTECHNICAL_INFORMATION\nPATHOLOGIES\nPULMONARY_ARTERY\nCORONARY_ARTERIES_AORTA_ISTHMUS\nCARDIAC_CHAMBERS_FUNCTION\nCONCLUSION\nTREATMENT_RESULTS\nVALVES\nGT labels\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.95\n0.02\n0.00\n0.00\n0.00\n0.02\n0.01\n0.00\n0.00\n0.00\n0.94\n0.00\n0.00\n0.00\n0.00\n0.06\n0.00\n0.00\n0.00\n0.00\n0.99\n0.00\n0.00\n0.00\n0.01\n0.01\n0.00\n0.00\n0.00\n0.00\n0.99\n0.00\n0.00\n0.00\n0.01\n0.01\n0.01\n0.00\n0.00\n0.00\n0.97\n0.01\n0.00\n0.00\n0.01\n0.01\n0.02\n0.00\n0.01\n0.03\n0.93\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n0.00\n0.00\n0.00\n0.99\nCardiology\nADDITIONAL_EXAM\nPHYSICAL_EXAM\nOTHER\nDIAGNOSIS\nMEDICAL_HISTORY\nOTHER_RECOMMENDATIONS\nMEDICATION_RECOMMENDATIONS\nPredicted labels\nADDITIONAL_EXAM\nPHYSICAL_EXAM\nOTHER\nDIAGNOSIS\nMEDICAL_HISTORY\nOTHER_RECOMMENDATIONS\nMEDICATION_RECOMMENDATIONS\nGT labels\n0.92\n0.04\n0.00\n0.04\n0.01\n0.00\n0.00\n0.00\n0.87\n0.01\n0.12\n0.00\n0.00\n0.00\n0.30\n0.07\n0.48\n0.00\n0.04\n0.09\n0.02\n0.00\n0.06\n0.00\n0.81\n0.13\n0.00\n0.00\n0.00\n0.00\n0.00\n0.04\n0.96\n0.00\n0.00\n0.07\n0.06\n0.00\n0.00\n0.01\n0.85\n0.00\n0.00\n0.00\n0.00\n0.10\n0.02\n0.00\n0.89\nHypertension\nBIRADS\nGLANDULAR\nGLANDULAR_ADIPOSE\nADIPOSE\nADIPOSE_GLANDULAR\nLEFT\nL_BIRADS\nBOTH\nRIGHT\nDUCT_NOT_DILATED\nDUCT_DILATED\nR_BIRADS\nNOTES\nNODES_NOT_SUSPICIOUS\nNODES_SUSPICIOUS\nRECOMMENDATIONS\nPredicted labels\nBIRADS\nGLANDULAR\nGLANDULAR_ADIPOSE\nADIPOSE\nADIPOSE_GLANDULAR\nLEFT\nL_BIRADS\nBOTH\nRIGHT\nDUCT_NOT_DILATED\nDUCT_DILATED\nR_BIRADS\nNOTES\nNODES_NOT_SUSPICIOUS\nNODES_SUSPICIOUS\nRECOMMENDATIONS\nGT labels\n0.97 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n0.00 0.98 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n0.00 0.00 0.99 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n0.00 0.00 0.00 0.02 0.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00 0.94 0.00 0.02 0.01 0.01 0.01 0.00 0.01 0.00 0.00 0.00\n0.32 0.00 0.00 0.00 0.00 0.00 0.35 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.02 0.79 0.00 0.00 0.00 0.01 0.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.97 0.01 0.00 0.00 0.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.03 0.00 0.11 0.81 0.00 0.00 0.00 0.00 0.00\n0.20 0.00 0.00 0.00 0.00 0.00 0.24 0.00 0.00 0.00 0.00 0.56 0.00 0.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.03 0.00 0.00 0.00 0.00 0.91 0.00 0.01 0.04\n0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.95 0.00\n0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.99\nRadiology\nCOMPLICATIONS\nMARGINS\nADDITIONAL_MATERIAL_DIMENSIONS\nMATERIAL_DESCRIPTION\nMATERIAL_TYPE\nMATERIAL_DIMENSIONS\nSECTION_LOCATION_NAME\nCROSS_SECTION_DESC\nINKING\nMARKING\nPredicted labels\nCOMPLICATIONS\nMARGINS\nADDITIONAL_MATERIAL_DIMENSIONS\nMATERIAL_DESCRIPTION\nMATERIAL_TYPE\nMATERIAL_DIMENSIONS\nSECTION_LOCATION_NAME\nCROSS_SECTION_DESC\nINKING\nMARKING\nGT labels\n0.98\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.00\n0.00\n0.11\n0.78\n0.00\n0.00\n0.06\n0.00\n0.00\n0.06\n0.00\n0.00\n0.00\n0.00\n0.87\n0.04\n0.00\n0.04\n0.00\n0.04\n0.00\n0.00\n0.02\n0.00\n0.00\n0.96\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.03\n0.97\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.00\n0.00\n0.00\n0.00\n0.98\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.00\n0.00\n0.98\n0.00\n0.00\n0.00\n0.00\n0.00\n0.03\n0.03\n0.00\n0.00\n0.00\n0.95\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.08\n0.00\n0.00\n0.00\n0.92\n0.00\n0.00\n0.00\n0.04\n0.04\n0.04\n0.00\n0.00\n0.04\n0.04\n0.80\nPathology\nHEAD_AND_NECK\nOTHER\nABDOMEN_AND_PELVIS\nCHEST\nCONCLUSIONS\nLYMPH_NODES\nPredicted labels\nHEAD_AND_NECK\nOTHER\nABDOMEN_AND_PELVIS\nCHEST\nCONCLUSIONS\nLYMPH_NODES\nGT labels\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n0.99\n0.00\n0.00\n0.00\n0.02\n0.00\n0.00\n0.98\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\nOncology\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 5: DistilBERT’s confusion matrices for each clinical category.\n9\n"}, {"page": 10, "text": "A PREPRINT - JANUARY 16, 2026\nModel_1\nModel_2\nstat\np_value\nn_effective\nCardiology\nBioBERT\nDistilBERT\n480.0\n3.43e-39\n191\nBioBERT\nHerBERT\n4065.5\n2.72e-09\n172\nDistilBERT\nHerBERT\n768.0\n6.25e-20\n127\nOncology\nBioBERT\nDistilBERT\n3.0\n0.17\n5\nBioBERT\nHerBERT\n0.0000\n0.02\n5\nDistilBERT\nHerBERT\n0.0\n0.15\n2\nPathology\nBioBERT\nDistilBERT\n18.5\n1.45e-08\n36\nBioBERT\nHerBERT\n246.0\n0.01\n40\nDistilBERT\nHerBERT\n10.5\n5.69e-05\n20\nRadiology\nBioBERT\nDistilBERT\n15635.0\n2.03e-71\n529\nBioBERT\nHerBERT\n42644.5\n4.63e-11\n492\nDistilBERT\nHerBERT\n10830.0\n3.39e-42\n379\nHypertension\nBioBERT\nDistilBERT\n1985.0\n1.26e-79\n396\nBioBERT\nHerBERT\n8977.5\n8.99e-54\n398\nDistilBERT\nHerBERT\n2866.5\n1.82e-08\n146\nTable 2: Pairwise Wilcoxon signed-rank test results for\nmodel accuracies across all clinical types.\nVRAM Usage [GB]\nClinical Type\nLlama3.1\nDistilBERT\nBioBERT\nHerBERT\nCardiology\n141.84\n0.52\n0.43\n0.48\nHypertension\n159.59\n0.52\n0.43\n0.48\nRadiology\n141.71\n0.55\n0.46\n0.50\nPathology\n138.77\n0.52\n0.42\n0.48\nOncology\n136.53\n0.55\n0.46\n0.50\nInference time [s]\nClinical Type\nLlama3.1\nDistilBERT\nBioBERT\nHerBERT\nCardiology\n68.57\n0.17\n0.01\n0.01\nHypertension\n106.89\n0.01\n0.01\n0.01\nRadiology\n66.44\n0.01\n0.01\n0.01\nPathology\n24.76\n0.01\n0.01\n0.01\nOncology\n113.30\n0.01\n0.01\n0.01\nTable 3: VRAM Usage and Inference time comparison\nbetween Teacher LLM and students BERT models across\nClinical domains.\ndomain. We show that a multilingual LLM like Llama3.1\ncan be efficiently apply for knowledge distillation in Polish\nlanguage. Finally we show that BERT models, especially\nDistilBERT, successfully learn a multiclass classification\ntask solely from data with distilled labels.\nAcknowledgment\nThe Polish National Centre for Research and Develop-\nment (NCBR) supported this research in the project:\n“ADMEDVOICE- Adaptive intelligent speech processing\nsystem of medical personnel with the structuring of test\nresults and support of therapeutic process,” no. INFOS-\nTRATEG4/0003/2022.\nReferences\n[1] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Dis-\ntilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\n[2] Amir M Mansourian et al.\nA comprehensive\nsurvey on knowledge distillation.\narXiv preprint\narXiv:2503.12067, 2025.\n[3] N Moslemi et al. A survey on knowledge distilla-\ntion: Recent advancements. ScienceDirect, Novem-\nber 2024.\n[4] ACM Transactions Team. Survey on knowledge dis-\ntillation for large language models: Methods, evalua-\ntion, and application. ACM Transactions on Intelli-\ngent Systems and Technology, 2024.\n[5] Xiaohui Zhang, Wei Li, Jian Wang, et al. Coun-\nterclockwise block-by-block knowledge distillation\nfor neural network compression. Scientific Reports,\n15(1):91152, 2025.\n[6] Mina Hemmatian, Ali Shahzadi, and Saeed Mozaf-\nfari.\nUncertainty-based knowledge distillation\nfor bayesian deep neural network compression.\nInternational Journal of Approximate Reasoning,\n173:109289, 2024.\n[7] David E Hernandez, Torbjörn Nordling, et al. Knowl-\nedge distillation: Enhancing neural network com-\npression with integrated gradients. arXiv preprint\narXiv:2503.13008, 2025.\n[8] Jianping Gou, Baosheng Yu, Stephen J Maybank,\nand Dacheng Tao. Knowledge distillation: A survey.\nInternational Journal of Computer Vision, 129:1789–\n1819, 2021.\n[9] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen,\nReynold Cheng, Jinyang Li, Can Xu, Dacheng Tao,\nand Tianyi Zhou.\nA survey on knowledge distil-\nlation of large language models.\narXiv preprint\narXiv:2402.13116, 2024.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding.\n10\n"}, {"page": 11, "text": "A PREPRINT - JANUARY 16, 2026\nIn Proceedings of NAACL-HLT, pages 4171–4186,\n2019.\n[11] M Fields et al. A survey of text classification with\ntransformers: How wide? how large? how long? how\naccurate? how expensive? how safe? IEEE Access,\n2024.\n[12] Karim Gasmi. Improving bert-based model for med-\nical text classification with an optimization algo-\nrithm. In Advances in Computational Collective Intel-\nligence. ICCCI 2022. Communications in Computer\nand Information Science, volume 1653, Cham, 2022.\nSpringer.\n[13] S Patel et al. Survey of transformers and towards\nensemble learning using transformers for natural lan-\nguage processing. PMC, 2024.\n[14] Hussein T Al-Natsheh et al. Rethinking of bert sen-\ntence embedding for text classification. Neural Com-\nputing and Applications, August 2024.\n[15] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\nHow to fine-tune bert for text classification? arXiv\npreprint arXiv:1905.05583, 2019.\n[16] Yuxin Li, Wei Zhang, Hao Wang, et al. Short-text\nsentiment classification model based on bert and dual-\nstream transformer gated attention mechanism. Elec-\ntronics, 14(19):3904, 2025.\n[17] Imad El Maaroufi, Youssef Mellah, Karim El Kharki,\net al.\nLnlf-bert: Transformer for long document\nclassification with multiple attention levels. IEEE\nAccess, 2024.\n[18] J Jiang et al. Are we really making much progress\nin text classification? a comparative review. arXiv\npreprint arXiv:2204.03954, 2025.\n[19] Jinhyuk\nLee,\nWonjin\nYoon,\nSungdong\nKim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. Biobert: A pre-trained biomedical\nlanguage representation model for biomedical text\nmining. Bioinformatics, 36(4):1234–1240, 2020.\n[20] Omid Rohanian,\nMohammadmahdi Nouriborji,\nHannah Jauncey,\nSamaneh Kouchaki,\nFarhad\nNooralahzadeh, Lei Clifton, Laura Merson, and\nDavid A Clifton. Lightweight transformers for clini-\ncal natural language processing. Natural Language\nEngineering, pages 1–28, 2023.\n[21] Kexin Huang, Jaan Altosaar, and Rajesh Ran-\nganath. Clinicalbert: Modeling clinical notes and\npredicting hospital readmission.\narXiv preprint\narXiv:1904.05342, 2020.\n[22] Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer\nlearning in biomedical natural language processing:\nAn evaluation of bert and elmo on ten benchmarking\ndatasets. arXiv preprint arXiv:1906.05474, 2019.\n[23] Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and\nDegui Zhi. Med-bert: pretrained contextualized em-\nbeddings on large-scale structured electronic health\nrecords for disease prediction. npj Digital Medicine,\n4(1):1–13, 2021.\n[24] ClinRaGen Research Team. Knowledge-augmented\nmultimodal clinical rationale generation for disease\ndiagnosis with small language models. arXiv preprint\narXiv:2411.07611, 2025.\n[25] Eunkyung Kim, Kai Huang, Yu Xing, Xiaoqian Jiang,\net al. Attention mechanism with bert for content\nannotation and categorization of pregnancy-related\nquestions on a community q&a site. In AMIA An-\nnual Symposium Proceedings, volume 2020, pages\n625–634. American Medical Informatics Association,\n2021.\n[26] Luca Putelli, Alfonso E Gerevini, Alberto Lavelli,\nTahir Mehmood, and Ivan Serina. On the behaviour\nof bert’s attention for the classification of medical\nreports. CEUR Workshop Proceedings, 3277, 2022.\n[27] Emily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. Publicly available clinical bert embed-\ndings. In Proceedings of the 2nd Clinical Natural\nLanguage Processing Workshop, pages 72–78. ACL,\n2019.\n[28] G Wang et al. Comparison of bert implementations\nfor natural language processing of narrative medical\ndocuments. ScienceDirect, 2020.\n[29] Ethan Tanner et al. Large language model influence\non diagnostic reasoning: A randomized clinical trial.\nJAMA Network Open, 7(10), 2024.\n[30] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara\nMahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen\nPfohl, et al. Large language models encode clinical\nknowledge. Nature, 620(7972):172–180, 2023.\n[31] Shuang Wang, Zhenyu Zhao, Xi Ouyang, Qian Wang,\nand Dinggang Shen. Large language models for dis-\nease diagnosis: a scoping review. npj Artificial Intel-\nligence, 1(1):11, 2025.\n[32] Shuo Liu, Yanglin Pan, Kejia Wang, Xiaodong Jia,\net al. Application of large language models in disease\ndiagnosis and treatment. Chinese Medical Journal,\n2025.\n[33] Hajar Sakai and Sarah S Lam. Kdh-mltc: Knowledge\ndistillation for healthcare multi-label text classifica-\ntion. arXiv preprint arXiv:2505.07162, 2025.\n[34] M Hasan et al. Medical prediction using discharge-\nbert and corebert. Referenced in KDH-MLTC, 2025.\n[35] Giannis Karamanolakis, Daniel Hsu, and Luis Gra-\nvano. Cross-lingual text classification with minimal\nresources by transferring a sparse teacher. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 3604–3622, 2020.\n[36] Y Cho et al. Dsg-kd: Knowledge distillation from\ndomain-specific to general language models. arXiv\npreprint arXiv:2409.14904, 2024.\n11\n"}, {"page": 12, "text": "A PREPRINT - JANUARY 16, 2026\n[37] Ziqing Yang et al. Cross-lingual text classification\nwith multilingual distillation and zero-shot-aware\ntraining. arXiv preprint arXiv:2202.13654, 2022.\n[38] G Katsogiannis-Meimarakis and G Koutrika. Mul-\ntilingual text categorization and sentiment analysis:\nA comparative analysis of the utilization of multilin-\ngual approaches for classifying twitter data. Neural\nComputing and Applications, May 2023.\n[39] Moritz Laurer, Wouter van Atteveldt, Andreu Casas,\nand Kasper Welbers. Cross-lingual classification of\npolitical texts using multilingual sentence embed-\ndings. Political Analysis, 31(3), January 2023.\n[40] Raviraj Joshi et al. Universal cross-lingual text clas-\nsification. arXiv preprint arXiv:2406.11028, June\n2024.\n[41] Robert\nMroczkowski,\nPiotr\nRybak,\nAlina\nWróblewska,\nand Ireneusz Gawlik.\nHerbert:\nEfficiently pretrained transformer-based language\nmodel for polish. In Proceedings of the 8th Workshop\non Balto-Slavic Natural Language Processing, pages\n1–10, Kiyv, Ukraine, April 2021. Association for\nComputational Linguistics.\n[42] Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and\nIreneusz Gawlik. Klej: Comprehensive benchmark\nfor polish language understanding. arXiv preprint\narXiv:2005.00630, 2020.\n[43] Jan\nKoco´n,\nMonika\nZa´sko-Zieli´nska,\nPiotr\nMiłkowski, Arkadiusz Janz, and Maciej Piasecki.\nWroclaw corpus of consumer reviews sentiment\n(WCCRS), 2019. CLARIN-PL digital repository.\n[44] Górno´sl ˛askie Centrum Medyczne Research Team.\nDeep learning analysis of polish electronic health\nrecords for diagnosis prediction in patients with\ncardiovascular diseases.\nPersonalized Medicine,\n12(6):869, May 2022.\n[45] AssistMED Project Team.\nPractical use case of\nnatural language processing for observational clin-\nical research data retrieval from electronic health\nrecords: Assistmed project. Polish Archives of Inter-\nnal Medicine, 2024.\n[46] Marta Zielonka, Andrzej Czy˙zewski, Dariusz Szplit,\nBeata Graff, Anna Szyndler, Mariusz Budzisz, and\nKrzysztof Narkiewicz. Machine learning tools match\nphysician accuracy in multilingual text annotation.\nScientific Reports, 15(1):5487, 2025.\n[47] Andrzej Czy˙zewski, Sebastian Cygert, Karolina\nMarciniuk, Maciej Szczodrak, Arkadiusz Harasim-\niuk, Piotr Odya, Marina Galanina, Piotr Szczuko,\nBo˙zena Kostek, Beata Graff, et al. A comprehensive\npolish medical speech dataset for enhancing auto-\nmatic medical dictation. Scientific Data, 12(1):1436,\n2025.\n12\n"}]}