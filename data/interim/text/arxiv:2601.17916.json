{"doc_id": "arxiv:2601.17916", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.17916.pdf", "meta": {"doc_id": "arxiv:2601.17916", "source": "arxiv", "arxiv_id": "2601.17916", "title": "UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR", "authors": ["Jialu Tang", "Tong Xia", "Yuan Lu", "Aaqib Saeed"], "published": "2026-01-25T17:35:52Z", "updated": "2026-01-25T17:35:52Z", "summary": "Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.17916v1", "url_pdf": "https://arxiv.org/pdf/2601.17916.pdf", "meta_path": "data/raw/arxiv/meta/2601.17916.json", "sha256": "e51276add9414613c0f1de3baf19f1e0f8feeaa0c8dbdfc5d66b11f74ede17de", "status": "ok", "fetched_at": "2026-02-18T02:20:35.818531+00:00"}, "pages": [{"page": 1, "text": "UNIPACT: A MULTIMODAL FRAMEWORK FOR PROGNOSTIC QUESTION ANSWERING\nON RAW ECG AND STRUCTURED EHR\nJialu Tang1, Tong Xia2, Yuan Lu1, Aaqib Saeed1,3\n1 Eindhoven University of Technology, The Netherlands\n2 Tsinghua University, China.\n3 Eindhoven Artificial Intelligence Systems Institute, The Netherlands\n{j.tang, y.lu, a.saeed}@tue.nl, tongxia@mail.tsinghua.edu.cn\nABSTRACT\nAccurate clinical prognosis requires synthesizing structured\nElectronic Health Records (EHRs) with real-time physiolog-\nical signals like the Electrocardiogram (ECG). Large Lan-\nguage Models (LLMs) offer a powerful reasoning engine for\nthis task but struggle to natively process these heterogeneous,\nnon-textual data types. To address this, we propose UniPACT\n(Unified Prognostic Question Answering for Clinical Time-\nseries), a unified framework for prognostic question answer-\ning that bridges this modality gap. UniPACT’s core contri-\nbution is a structured prompting mechanism that converts nu-\nmerical EHR data into semantically rich text. This textualized\npatient context is then fused with representations learned di-\nrectly from raw ECG waveforms, enabling an LLM to reason\nover both modalities holistically. We evaluate UniPACT on\nthe comprehensive MDS-ED benchmark, it achieves a state-\nof-the-art mean AUROC of 89.37% across a diverse set of\nprognostic tasks including diagnosis, deterioration, ICU ad-\nmission, and mortality, outperforming specialized baselines.\nFurther analysis demonstrates that our multimodal, multi-task\napproach is critical for performance and provides robustness\nin missing data scenarios.\nIndex Terms— Multimodal learning, large language\nmodel, prognosis, clinical time-series, EHR\n1. INTRODUCTION\nAccurate patient prognosis in acute care is a cornerstone of\nmodern medicine, directly guiding critical decisions such as\nICU admission, treatment selection, and risk intervention [1].\nClinicians formulate a prognosis not from a single data point,\nbut by synthesizing a holistic view of the patient: their static\nbaseline (demographics, comorbidities from EHR), dynamic\nstate (vital signs), and acute physiological signals (like the 12-\nlead Electrocardiogram) [2]. A subtle T-wave inversion (from\nthe ECG), for instance, may signify a critical mortality risk,\nbut only when contextualized by abnormal potassium levels\nand a history of hypertension (from the EHR). Capturing this\ncomplex, cross-modal reasoning is the central challenge in\ncomputational prognosis.\nFor decades, this challenge was met with simplified clini-\ncal risk scores, which are limited by manual feature selection\nand linear assumptions.\nWhile standard machine learning\nmodels offered improved accuracy, they are predominantly\ndesigned as rigid, single-task predictors [3]. These models\nlack the flexibility to adapt to the diverse, dynamic ques-\ntions clinicians face. They cannot be naturally queried about\ndifferent outcomes (e.g., diagnosis, deterioration, mortality)\nand fundamentally struggle to fuse the dense, continuous lan-\nguage of ECG signals with the discrete, numerical language\nof structured EHR tables.\nThe advent of Large Language Models (LLMs) introduces\na new paradigm [4, 5, 6], offering a path from rigid prediction\nto flexible, prompt-based prognostic question answering. The\nvision is a single, unified model that a clinician can query in\nnatural language: “Given this patient’s history and their cur-\nrent ECG, what is their risk of severe hypoxemia?” How-\never, this vision is blocked by a fundamental barrier: LLMs\nare text-native reasoning engines. They cannot natively inter-\npret the most critical prognostic data. Raw ECG waveforms\nare dense signals where subtle morphology holds diagnostic\nmeaning, while structured EHR data consists of context-poor\nnumerical values. Current workarounds, such as summariz-\ning ECGs into text reports [7, 8], result in critical informa-\ntion loss, discarding the very waveform patterns essential for\nexpert-level prognosis.\nTo bridge this gap and enable better prognostic reasoning,\nwe propose UniPACT (Unified Prognostic Question Answer-\ning for Clinical Time-series). UniPACT is a unified frame-\nwork designed for multimodal prognostic question answer-\ning and makes raw physiological signals and structured data\n“legible” to an LLM. For the ECG, it employs a dedicated\nwaveform encoder to learn deep representations directly from\nthe 12-lead ECG signal. For the EHR, it uses a novel struc-\ntured prompting mechanism to convert numerical data into se-\nmantically rich sentences (e.g., “The patient’s heart rate is 88\nbeats per minute”), embedding them with vital clinical con-\narXiv:2601.17916v1  [cs.LG]  25 Jan 2026\n"}, {"page": 2, "text": "The demographics \ninformation, 30.0 \nyear-old, black \nAfrican American, \nfemale. \nThe vital parameters, \ntemperature 36.1, \nheartrate 88.0, \nresprate 16.0, o2sat \n100.0, sbp 121.0, dbp \n80.0. \nThe biometrics \ninformation, \nbmi 31.1, \nweight 84.8, \nheight 165.1.\n<ECG>  <Role Assignment> <Task>  <Demographics Info>   <Biometrics Info>  <Vital Parameters>  <Question> \n<cardiac arrest> \n<ecmo>  \n<inotropes>            \n<mechanical ventilation>\n<severe hypoxemia>\n<vasopressors> \nPrognosis Results\nTASK\n  \n \nM\no\nr\nt\na\nli\nt\ny\n \n/\n \nI\nC\nU\n \n/\n \nD\ne\nt\ne\nr\ni\no\nr\na\nt\ni\no\nn\n \n/\n \nD\ni\na\ng\nn\no\ns\ne\n \n \n \n \nc\no\nll\ne\nc\nt\na\nb\nl\ne\n \ni\nn\nd\ni\nc\na\nt\no\nr\ns\n \na\nf\nt\ne\nr\n \na\nd\nm\ni\ns\ns\ni\no\nn\n:\nDATA\nWill the \npatient \nexperience \nsevere \nhypoxemia?\nMM-Projector\n... .. ...\n<ECG>\nTokenizer_LoRA\n...\n...\n...\nPretrained ECG Model\n12-Leads ECG\nLLM \n(Decoder)\nLLM \nTokenizer & Embedder\nText\nPrompting\nECG Embedding\nECG-Text MLP Mapper\nECG-Text \nMerged Embeddings \nD\ne\nm\no\ng\nr\na\np\nh\ni\nc\ns\n  \n \nB\ni\no\nm\ne\nt\nr\ni\nc\ns\n  \n \nV\nit\na\nl \nP\na\nr\na\nm\ne\nt\ne\nr\ns\n  \n \nE\nC\nG\n  \n \nR\no\nl\ne\n \nA\ns\ns\ni\ng\nn\nm\ne\nn\nt \n  \n  \n  \n  \n  \n  \nT\na\ns\nk\n  \n  \n  \n  \n  \n  \n \nQ\nu\ne\ns\nti\no\nn\nEncoder_LoRA\nDecoder_LoRA\nyes\nyes\nNo\nNo\nNo\nNo\nExtra Test\nOther Task\nTreatment\n(a)\n(b)\n(c)\n           \nYour task is to predict \nwhether a patient will \nexperience clinical \ndeterioration based on \nprovided ECG and Electronic \nHealth Record (EHR) data. \nYou are a cardiologist. \n<Role Assignment>\n<Task>\n<Demographics Info>\n<Biometrics Info>\n<Vital Parameters>\n<Q> \nFig. 1. The UniPACT framework for multimodal prognostic question answering. (a) Structured Prompt Formulation: Heterogeneous\npatient data, including structured EHR (demographics, biometrics, vitals) and a reference to the ECG waveform, are converted into a unified\nnatural language prompt. This process transforms numerical values into a format that is natively understandable by the LLM. (b) Multimodal\nFusion Architecture: A pretrained encoder processes the raw 12-lead ECG waveform to produce a feature embedding. A multimodal\nprojector (MM-Projector) then aligns this ECG embedding with the LLM’s text embedding space. These aligned ECG features are seamlessly\nintegrated with the tokenized text prompt and processed by the LLM decoder for unified reasoning. (c) Prognostic Output Generation: The\nmodel generates a direct answer to the prognostic question (e.g., ‘Yes’/‘No’ to a query about clinical deterioration).\ntext. This strategy enables the LLM to simultaneously pro-\ncess high-fidelity ECG signals and structured EHR data. By\nframing diverse prognostic queries within a single question-\nanswering framework, UniPACT can seamlessly switch be-\ntween predicting different outcomes from long-term mortality\nto immediate clinical deterioration without requiring separate\nmodels. Our work makes following contributions:\n• We introduce UniPact, the first framework to unify raw\nECG waveforms with structured EHR data, overcom-\ning the modality bottleneck in LLM-based prognostic\nreasoning.\n• We propose Structured EHR Prompting as a key design\nchoice in multimodal prognosis. This mechanism pro-\nvides an effective and flexible way that jointly unifies\nraw ECG signals and heterogeneous numerical clinical\ndata while preserving both numerical precision and se-\nmantics. This design underpins the medical relevance\nof our approach by enabling clinically grounded ques-\ntion answering in a wide spectrum of queries.\n• Through comprehensive evaluation on the MDS-ED [3]\n(MIMIC-IV-ECG & MIMIC-IV derived) benchmark,\nwe demonstrate that UniPact significantly outperforms\nestablished baselines and maintains robustness in both\nmulti-task and missing-modality scenarios.\n2. METHOD\nThe UniPACT framework is designed to perform prognos-\ntic question answering by unifying raw ECG waveforms and\nstructured EHR data within an end-to-end generative model.\nAs illustrated in Figure 1, our architecture consists of three\ncore components: (1) a dedicated encoder for raw ECG sig-\nnals, (2) a structured prompting mechanism to textualize EHR\ndata, and (3) a large language model (LLM) that fuses these\nmultimodal representations to generate a final prediction.\n2.1. Modality-Specific Representation Learning\nECG Waveform Encoder. To capture the rich diagnostic in-\nformation in physiological signals, we process raw 12-lead\nECG waveforms directly, avoiding lossy conversion to text\nreports. We employ the pre-trained Transformer-based ECG\nencoder from D-BETA [9], which has been shown to be ef-\nfective at learning discriminative representations from ECG\nsignals. Given a raw ECG signal E ∈RL×C (where L is the\nnumber of time steps, typically 5000 for a 10-second record-\ning at 500 Hz, and C = 12 leads), the encoder outputs a\nsequence of feature embeddings:\nHecg = ECG-Encoder(E),\n(1)\nwhere Hecg ∈RN×decg is a sequence of N embedding\nvectors, each of dimension decg.\nThis approach preserves\nthe fine-grained temporal and morphological patterns of the\nwaveform.\nStructured EHR Prompting. A key novelty of UniPACT\nis its method for making structured EHR data comprehen-\nsible to an LLM. Rather than using raw values, we convert\n< Demographics > (3 parameters), < Biometrics > (3\nparameters), and < V ital Parameters > (7 parameters)\ninto natural language sentences via predefined templates.\nThis representation, TEHR, maintains numerical precision\nwhile providing the contextual cues LLMs require. An exam-\nple prompt follows:\n"}, {"page": 3, "text": "The demographics information:\n30.0 year-old,\nblack African American, female.\nThe vital\nparameters:\ntemperature 36.1, heartrate\n88.0, resprate 16.0.\nThe biometrics\ninformation:\nbmi 31.1, weight 84.8, height\n165.1.\n2.2. Multimodal Fusion and Generation\nEmbedding Space Alignment. To fuse these diverse modal-\nities, we first align their representations within the LLM’s\nembedding space. The ECG feature embeddings Hecg are\nmapped into the LLM’s word embedding dimension dllm us-\ning a small, trainable projection network, which we imple-\nment as a two-layer MLP (the MM-Projector):\nH′\necg = MLP(Hecg),\n(2)\nwhere H′\necg ∈RN×dllm.\nThe textualized EHR prompt,\nTEHR, is tokenized and embedded using the LLM’s native\ntokenizer and embedding layer, resulting in embeddings\nHEHR ∈RM×dllm.\nUnified Input and Autoregressive Prediction.\nThe final\ninput to the LLM is a single, unified sequence constructed\nby concatenating the processed multimodal embeddings.\nWe base our model on the LLaVA framework [10] and use\nMedGemma-4B [11] as the backbone LLM as it is pretrained\non medical data. The complete input sequence is formatted\nas:\nHinput = [H′\necg, Hprompt, Hquestion],\n(3)\nwhere Hprompt contains the embedded EHR data and task\ninstructions, and Hquestion is the embedded prognostic query\n(e.g., “Will the patient experience severe hypoxemia?”). The\nLLM then autoregressively predicts the answer Y (e.g., “Yes”\nor “No”) based on this fused multimodal context.\n2.3. Multi-Task Learning via Unified Prompting\nUniPACT is trained as a single model on a diverse set of prog-\nnostic tasks (diagnosis, deterioration, ICU admission, mortal-\nity). We unify all tasks using a consistent prompt structure,\nwhich instructs the model on its role, the specific task, and\nthe question to answer. A generalized template is as follows:\n<ECG EMBEDDINGS>\n<Role Assignment> <Task Description>\n<EHR Information as Text>\n<Specific Prognostic Question>?\nAnswer strictly with Yes or No.\nThe entire model is trained end-to-end using a standard lan-\nguage modeling objective, which maximizes the likelihood of\nthe ground-truth answer tokens. Specifically, we minimize\nthe cross-entropy loss only on the answer portion of the se-\nquence:\nL = −\nk\nX\ni=1\nlog P(yi|Hinput, y<i; θ),\n(4)\nwhere Y = (y1, ..., yk) are the tokens of the target answer\n(e.g., “Yes”), and θ represents the model parameters. To fully\nleverage information from ECG and textual modalities and\nensure training efficiency, we trained in two stages: in the first\nstage, we keep the ECG encoder and LLM weights frozen, up-\ndating only the MM-Projector. We employed LoRA [12] fine-\ntuning for the ECG encoder to reduce the number of trainable\nparameters. In the second stage, we inserted LoRA adapters\n(rank r=128, scaling α=256, dropout=0.05) to the linear lay-\ners in the ECG encoder and LLM.\n3. RESULTS\nWe evaluate UniPACT’s performance on a comprehen-\nsive suite of prognostic tasks from the MDS-ED bench-\nmark [3] derived from MIMIC-IV-ECG [13] & MIMIC-\nIV [14] dataset. Our analysis focuses on four key aspects: (1)\ncomparison against state-of-the-art baselines; (2) the contri-\nbution of individual modalities; (3) the value of our unified\nmulti-task learning approach; and (4) an exploratory com-\nparison against general-purpose LLM APIs. We use the Area\nUnder the Receiver Operating Characteristic Curve (AUROC)\nas the primary evaluation metric across all prognostic tasks.\nComparison with Baseline Methods. We begin with com-\nparing UniPACT’s performance against established models\non the considered benchmark in Table 1. The baselines in-\nclude two recent ECG–language models, ECG-Chat [15] and\nQ-HEART [16], as well as a specialized multimodal classifi-\ncation model, MDS-ED [3]. The latter comprises two inde-\npendently trained models: one for Deterioration, and another\nfor Deterioration, ICU, and Mortality. It represents the prior\nstate-of-the-art on this dataset. UniPACT achieves an overall\nAUROC of 89.37%, outperforming all baselines. Notably,\nit surpasses the highly specialized MDS-ED model, high-\nlighting the strength of our framework. For fair comparison,\nwe use same metrics as MDS-ED. In Table 1 the parenthet-\nical numbers shows the count of individual sub-tasks (out\nof 1443) where a model achieves robust performance (AU-\nROC 95% CI lower bound > 0.8). UniPACT demonstrates\nstrong performance on 883 sub-tasks, a significant increase\nfrom the 623 achieved by the MDS-ED, indicating greater\nreliability across a wider range of clinical scenarios.\nThe\ngeneral-purpose models underperformed, underscoring the\nimportance of fine-tuning for these specific prognostic tasks.\nExploratory Comparison with LLM APIs. We conducted\nan exploratory study to benchmark UniPACT against power-\nful, proprietary LLMs like GPT-5-Chat and Gemini-2.5 Pro,\nas shown in Table 2. It is crucial to note that this is not a\ndirect, apples-to-apples comparison. While UniPACT is fine-\ntuned on the task-specific data and processes raw ECG wave-\nforms, the LLM APIs were prompted in a zero-shot manner\nand were given a textual description and diagnoses of the\nECG findings instead of the raw signal itself. For evaluation,\n"}, {"page": 4, "text": "Table 1.\nComparative Performance Analysis on Cardiovascular\nPrognostic Tasks.\nTask-Specific Performance (AUROC %)\nMethod\nDiagnosis\nDeterioration\nICU\nMortality\nOverall\n(1428 classes)\n(6 classes)\n(2 classes)\n(7 classes)\n(1443 total)\nECG-based Foundation Models\nECG-Chat [15]\n49.70\n57.54\n56.40\n55.78\n54.86\n53/1428\n0/6\n0/2\n0/7\n53/1443\nQ-HEART [16]\n50.15\n55.42\n54.89\n56.32\n54.20\n139/1428\n0/6\n0/2\n0/7\n139/1443\nMultimodal Approaches\nMDS-ED [3]\n82.56\n90.70\n90.63\n91.68\n88.90\n609/1428\n5/6\n2/2\n7/7\n623/1443\nUniPACT (ours)\n83.98\n91.17\n90.50\n91.82\n89.37\n868/1428\n6/6\n2/2\n7/7\n883/1443\nRelative Improvement (%)\n∆vs. ECG-Chat\n+69.0%\n+58.4%\n+60.5%\n+64.6%\n+62.9%\n∆vs. MDS-ED\n+1.7%\n+0.5%\n-0.1%\n+0.2%\n+0.5%\nTable 2. Comparison with Large Language Model APIs.\n†Using\nzero-shot prompting with ECG reports and diagnosis All values in\nAUROC (%).\nMethod\nDiagnosis\nDeterioration\nICU\nMortality\nOverall\nGPT-5-chat†\n55.68\n78.12\n70.04\n62.27\n66.53\nGemini-2.5 Pro†\n57.82\n78.72\n70.70\n72.38\n69.41\nUniPACT (ours)\n83.98\n91.17\n90.50\n91.82\n89.37\nPerformance Gap\nvs. GPT-5-chat\n+28.30\n+13.05\n+20.46\n+29.55\n+22.84\nvs. Gemini\n+26.16\n+12.45\n+19.80\n+19.44\n+19.96\nwe sampled 20,000 instances from 400 tasks, maintaining a\nbalanced positive and negative cases to cover diverse clini-\ncal conditions while keeping API cost into account. Under\nthis setup, UniPACT significantly outperforms the general-\npurpose APIs. This result is not intended to be a critique of\nthese powerful models, but rather to highlight a key finding:\nfor complex, domain-specific tasks like clinical prognosis, the\nability to process raw modal data (like ECG signals) and fine-\ntune on relevant data is critical for achieving better perfor-\nmance.\nRole of Multimodality. To quantify the benefit of integrating\nECG and EHR data, we evaluated uni-modal versions of Uni-\nPACT. Table 3 (A) presents the performance of models trained\nwith only ECG or only EHR data compared to the full multi-\nmodal UniPACT. Our results clearly demonstrate strong syn-\nergistic effects. The EHR-only model achieves a 80.83% AU-\nROC, confirming that structured clinical data is highly pre-\ndictive. However, the full UniPACT model, which integrates\nraw ECG waveforms, improves performance by a substantial\nmargin of +8.54% AUROC. This gain confirms our central\nhypothesis: UniPACT effectively leverages the unique, com-\nplementary information present in both the patient’s clinical\nhistory (EHR) and their real-time physiological state (ECG)\nto form a more complete and accurate prognostic assessment.\nBenefit of Multi-Task Learning. Our framework trains a\nTable 3. Comprehensive Ablation Analysis of UniPACT.\nModel Configuration\nPerformance Metrics (AUROC %)\nDiagnose\nDeterioration\nICU\nMortality\nMean±SD\nA. Modality Analysis\nECG-based\n66.05\n74.80\n71.89\n79.79\n73.13±5.74\nEHR-based\n69.97\n87.36\n82.62\n83.37\n80.83±7.53\nUniPACT (Multimodal)\n83.98\n91.17\n90.50\n91.82\n89.37±3.63\n∆vs ECG\n+17.93\n+16.37\n+18.61\n+12.03\n+16.24\n∆vs EHR\n+14.01\n+3.81\n+7.88\n+8.45\n+8.54\nB. Learning Paradigm\nSingle-Task Learning\n84.16\n90.15\n81.94\n90.26\n86.63±4.23\nMulti-Task Learning\n83.98\n91.17\n90.50\n91.82\n89.37±3.63\n∆(MTL gain)\n−0.18\n+1.02\n+8.56\n+1.56\n+2.74\nC. Feature Ablation (Performance when component is removed)\nw/o Demographics\n74.82\n80.75\n80.91\n81.29\n79.44±3.09\nw/o Biometrics\n77.65\n85.12\n84.01\n85.62\n83.10±3.70\nw/o Vitals\n72.12\n78.89\n77.95\n79.32\n77.07±3.35\nw/o ECG\n50.38\n54.27\n54.43\n54.60\n53.42±2.03\nw/o EHR\n73.01\n79.89\n78.95\n80.31\n78.04±3.40\nFull Model\n83.98\n91.17\n90.50\n91.82\n89.37±3.63\nsingle, unified model for all prognostic tasks.\nTo validate\nthis approach, we compared our multi-task UniPACT model\nagainst single-task counterparts, where a separate model was\ntrained for each of the four main task categories. As shown\nin Table 3 (B), the unified multi-task model achieves a higher\noverall AUROC (89.37% vs. 86.63%). The performance lift,\nparticularly in the ICU and Mortality tasks, suggests that the\nmodel learns shared, generalizable representations of patient\nstate that are beneficial across different prognostic horizons.\nRobustness to Missing Data. In clinical practice, patient\ndata is often incomplete. We assessed UniPACT’s robustness\nby evaluating its performance when specific components of\nthe EHR are absent. Table 3 (C) presents the impact of remov-\ning Demographics, Biometrics, or Vitals. Our results show\nthat while every data component contributes to the final pre-\ndiction, the model exhibits graceful degradation rather than\ncatastrophic failure. For example, removing patient vitals—a\nhighly informative feature set—reduces the overall AUROC\nfrom 89.37% to 77.07%. While this is a significant drop, the\nresulting performance is still substantially better than random\nchance and superior to the uni-modal ECG model, indicat-\ning that UniPACT effectively re-weights the available infor-\nmation (in this case, ECG, demographics, and biometrics) to\ncompensate for the missing data.\n4. CONCLUSION\nWe presented UniPACT, a unified framework that effectively\nintegrates raw ECG waveforms and structured EHR data for\nLLM-based prognosis. By translating numerical EHR into\nsemantic prompts and fusing them with deep ECG features,\nour model achieves superior performance on the MDS-ED\nbenchmark, outperforming established baselines. The results\ndemonstrate that a single, multi-task generative model can\nsurpass specialized systems in both accuracy and robustness.\nOur approach enhances the ability of LLMs to reason over\ncomplex clinical data by providing high-fidelity, multimodal\ninputs.\n"}, {"page": 5, "text": "5. ACKNOWLEDGMENTS AND ETHICAL\nCOMPLIANCE\nNo funding was received for conducting this study. The au-\nthors declare that they have no relevant financial or nonfinan-\ncial interests to disclose. This study made use of publicly\navailable and fully anonymized human data sets. In accor-\ndance with the policies of the data providers, no additional\ninstitutional review board approval was required.\n6. REFERENCES\n[1] JM Smit, Jesse H Krijthe, WMR Kant, JA Labrecque,\nM Komorowski, DAMPJ Gommers, Jasper van Bom-\nmel, Marcel JT Reinders, and Michel E van Genderen,\n“Causal inference using observational intensive care\nunit data: a scoping review and recommendations for\nfuture practice,” npj Digital Medicine, vol. 6, no. 1, pp.\n221, 2023.\n[2] Sushravya Raghunath, Alvaro E Ulloa Cerna, Linyuan\nJing, David P VanMaanen, Joshua Stough, Dustin N\nHartzel, Joseph B Leader, H Lester Kirchner, Martin C\nStumpe, Ashraf Hafez, et al.,\n“Prediction of mortal-\nity from 12-lead electrocardiogram voltage data using\na deep neural network,” Nature medicine, vol. 26, no. 6,\npp. 886–891, 2020.\n[3] Juan\nMiguel\nLopez\nAlcaraz,\nH\nBouma,\nand\nN Strodthoff,\n“Mds-ed:\nMultimodal decision sup-\nport\nin\nthe\nemergency\ndepartment–a\nbenchmark\ndataset for diagnoses and deterioration prediction\nin emergency medicine, 2024,”\nURL https://arxiv.\norg/abs/2407.17856.\n[4] Chunyuan Li,\nCliff Wong,\nSheng Zhang,\nNaoto\nUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-\nmann, Hoifung Poon, and Jianfeng Gao,\n“Llava-\nmed: Training a large language-and-vision assistant for\nbiomedicine in one day,”\nAdvances in Neural Infor-\nmation Processing Systems, vol. 36, pp. 28541–28564,\n2023.\n[5] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\net al., “Large language models encode clinical knowl-\nedge,” Nature, vol. 620, no. 7972, pp. 172–180, 2023.\n[6] Stella Li, Vidhisha Balachandran, Shangbin Feng,\nJonathan Ilgen, Emma Pierson, Pang Wei W Koh, and\nYulia Tsvetkov, “Mediq: Question-asking llms and a\nbenchmark for reliable interactive clinical reasoning,”\nAdvances in Neural Information Processing Systems,\nvol. 37, pp. 28858–28888, 2024.\n[7] Han Yu, Peikun Guo, and Akane Sano, “Ecg seman-\ntic integrator (esi): A foundation ecg model pretrained\nwith llm-enhanced cardiological text,” arXiv preprint\narXiv:2405.19366, 2024.\n[8] Huatao Xu, Liying Han, Qirui Yang, Mo Li, and Mani\nSrivastava, “Penetrative ai: Making llms comprehend\nthe physical world,” in Proceedings of the 25th Inter-\nnational Workshop on Mobile Computing Systems and\nApplications, 2024, pp. 1–7.\n[9] Hung Manh Pham, Aaqib Saeed, and Dong Ma, “Boost-\ning masked ecg-text auto-encoders as discriminative\nlearners,” 2025.\n[10] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee,\n“Visual instruction tuning,”\nAdvances in neu-\nral information processing systems, vol. 36, pp. 34892–\n34916, 2023.\n[11] Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroen-\nsri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, C´ıan Hughes, Charles Lau,\net al., “Medgemma technical report,” arXiv preprint\narXiv:2507.05201, 2025.\n[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\nChen, et al., “Lora: Low-rank adaptation of large lan-\nguage models.,” ICLR, vol. 1, no. 2, pp. 3, 2022.\n[13] Brian Gow, Tom Pollard, Larry A Nathanson, Alis-\ntair Johnson, Benjamin Moody, Chrystinne Fernandes,\nNathaniel Greenbaum, Jonathan W Waks, Parastou Es-\nlami, Tanner Carbonati, et al., “Mimic-iv-ecg: Diagnos-\ntic electrocardiogram matched subset,” Type: dataset,\nvol. 6, pp. 13–14, 2023.\n[14] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin\nGayles, Ayad Shammout, Steven Horng, Tom J Pol-\nlard, Sicheng Hao, Benjamin Moody, Brian Gow, et al.,\n“Mimic-iv, a freely accessible electronic health record\ndataset,” Scientific data, vol. 10, no. 1, pp. 1, 2023.\n[15] Yubao Zhao, Jiaju Kang, Tian Zhang, Puyu Han,\nand Tong Chen,\n“Ecg-chat:\nA large ecg-language\nmodel for cardiac disease diagnosis,”\narXiv preprint\narXiv:2408.08849, 2024.\n[16] Hung Manh Pham, Jialu Tang, Aaqib Saeed, and\nDong Ma,\n“Q-heart:\nEcg question answering via\nknowledge-informed multimodal llms,” arXiv preprint\narXiv:2505.06296, 2025.\n"}]}