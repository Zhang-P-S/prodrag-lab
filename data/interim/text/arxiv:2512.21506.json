{"doc_id": "arxiv:2512.21506", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.21506.pdf", "meta": {"doc_id": "arxiv:2512.21506", "source": "arxiv", "arxiv_id": "2512.21506", "title": "MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding", "authors": ["Aiwei Zhang", "Arvind Pillai", "Andrew Campbell", "Nicholas C. Jacobson"], "published": "2025-12-25T04:37:07Z", "updated": "2025-12-25T04:37:07Z", "summary": "As wearable sensing becomes increasingly pervasive, a key challenge remains: how can we generate natural language summaries from raw physiological signals such as actigraphy - minute-level movement data collected via accelerometers? In this work, we introduce MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs). MotionTeller combines a pretrained actigraphy encoder with a lightweight projection module that maps behavioral embeddings into the token space of a frozen decoder-only LLM, enabling free-text, autoregressive generation of daily behavioral summaries. We construct a novel dataset of 54383 (actigraphy, text) pairs derived from real-world NHANES recordings, and train the model using cross-entropy loss with supervision only on the language tokens. MotionTeller achieves high semantic fidelity (BERTScore-F1 = 0.924) and lexical accuracy (ROUGE-1 = 0.722), outperforming prompt-based baselines by 7 percent in ROUGE-1. The average training loss converges to 0.38 by epoch 15, indicating stable optimization. Qualitative analysis confirms that MotionTeller captures circadian structure and behavioral transitions, while PCA plots reveal enhanced cluster alignment in embedding space post-training. Together, these results position MotionTeller as a scalable, interpretable system for transforming wearable sensor data into fluent, human-centered descriptions, introducing new pathways for behavioral monitoring, clinical review, and personalized health interventions.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.21506v1", "url_pdf": "https://arxiv.org/pdf/2512.21506.pdf", "meta_path": "data/raw/arxiv/meta/2512.21506.json", "sha256": "a153a9b1efeb2f256cfe05c47e298e3a1b46e69ae5bf33fcfab5c87aa72971c2", "status": "ok", "fetched_at": "2026-02-18T02:23:49.653472+00:00"}, "pages": [{"page": 1, "text": " \nMotionTeller: Multi-modal Integration of Wearable Time-Series \nwith LLMs for Health and Behavioral Understanding \nAiwei Zhang1,2, Arvind Pillai2, Andrew Campbell2 & Nicholas C. Jacobson1,2,3,4 \n \n \n1Center for Technology and Behavioral Health, Geisel School of Medicine, Dartmouth College, Lebanon, NH, United States \n2Department of Computer Science, Dartmouth College, Hanover, NH, United States \n3Department of Biomedical Data Science, Geisel School of Medicine, Dartmouth College, Lebanon, NH, United States \n4Department of Psychiatry, Geisel School of Medicine, Dartmouth College, Lebanon, NH, United States \n \n \n \n \n \n \n \n \n*Correspondence concerning this article should be addressed to Aiwei Zhang, Center for \nTechnology and Behavioral Health, Dartmouth College, 46 Centerra Parkway Suite 300, \nLebanon, NH 03766. Email: aiwei.zhang.25@dartmouth.edu \n \n1 \n"}, {"page": 2, "text": " \nAbstract \nAs wearable sensing becomes increasingly pervasive, a key challenge remains: how can we \ngenerate natural language summaries from raw physiological signals such as actigraphy - \nminute-level movement data collected via accelerometers? In this work, we introduce \nMotionTeller, a generative framework that natively integrates minute-level wearable activity data \nwith large language models (LLMs). MotionTeller combines a pretrained actigraphy encoder \nwith a lightweight projection module that maps behavioral embeddings into the token space of a \nfrozen decoder-only LLM, enabling free-text, autoregressive generation of daily behavioral \nsummaries.  \n \nWe construct a novel dataset of 54,383 ⟨actigraphy, text⟩ pairs derived from real-world NHANES \nrecordings, and train the model using cross-entropy loss with supervision only on the language \ntokens. MotionTeller achieves high semantic fidelity (BERTScore-F1 = 0.924) and lexical \naccuracy (ROUGE-1 = 0.722), outperforming prompt-based baselines by 7% in ROUGE-1. The \naverage training loss converges to 0.38 by epoch 15, indicating stable optimization. Qualitative \nanalysis confirms that MotionTeller captures circadian structure and behavioral transitions, while \nPCA plots reveal enhanced cluster alignment in embedding space post-training. Together, these \nresults position MotionTeller as a scalable, interpretable system for transforming wearable sensor \ndata into fluent, human-centered descriptions, introducing new pathways for behavioral \nmonitoring, clinical review, and personalized health interventions.\n \n2 \n"}, {"page": 3, "text": " \n1. Introduction \nIn recent years, the proliferation of wearable sensor devices has transformed how we monitor and \nunderstand human behavior in everyday settings. Actigraphy, which is minute-level activity traces \ncaptured via wrist-worn accelerometers, have become a core modality in both clinical and consumer \nhealth contexts. It provides a non-invasive, high-resolution view into physical movement and is widely \nused to assess sleep quality, circadian rhythm, depression, and medication adherence (Yoo et al. 2023; \nDunn et al. 2021). \n \nAt the same time, large language models (LLMs) have revolutionized natural language understanding and \ngeneration, demonstrating strong generalization across diverse tasks through pretrained contextual \nrepresentations (Naveed et al. 2024). Their applications in healthcare are expanding rapidly: LLMs have \nshown promise in generating medical explanations, supporting peer-based interventions, and simulating \nempathic conversation in mental health contexts (De Choudhury, Pendse, and Kumar 2023; Sharma et al. \n2023). Yet as recent evaluations suggest, current LLMs often underperform on competencies most \nrelevant to behavioral health (such as contextual sensitivity, emotional nuance, and ethical \ndecision-making), particularly when applied to high-stakes counseling scenarios (Nguyen et al. 2025). \n \nA key missing link is the ability to natively connect LLMs with behavioral sensor inputs such as \nactigraphy. Despite their complementary strengths, LLMs and actigraphy have traditionally existed almost \ncompletely independently. Sensor-based models, such as CNNs, RNNs, and transformers, have been used \nto classify behavioral states and predict health outcomes from raw actigraphy (Heinz et al. 2022; F. Ruan \net al. 2024; Dorris, Oh, and Jacobson 2024), but they produce structured outputs like labels or scores \ninstead of language. Moreover, many of these CNN and LSTM models rely on short time windows or \nhandcrafted features and struggle to capture long-range dependencies in continuous, high-dimensional \nsequences (Rahman and Adjeroh 2019; Patterson et al. 2023). \n \nConversely, while LLMs are inherently generative, they lack the capacity to interpret raw time-series \ninputs directly. Attempts to bridge the gap between sensing and generation have typically relied on \nmanual preprocessing or prompt engineering, such as tokenizing numeric sequences into digit-wise text \nstrings or formatting prompts as question-answering tasks (Gruver et al. 2023; Kim et al. 2024; Nepal et \nal. 2024). These approaches often fragment the temporal structure of the original signal, leading to \nrepresentations that are decoupled from the dynamics of real-world time-series data. Similarly, earlier \ninterpretable models for clinical time-series, like RETAIN, require heavily structured and aggregated \ninputs, limiting flexibility when dealing with dense or high-frequency behavioral trajectories (Choi et al. \n2016). This reliance on preprocessing and structure constrains the interpretability and adaptability of \noutputs, particularly in complex real-world contexts. \n \nTo address this, we introduce MotionTeller, a unified framework that aligns actigraphy and language \nthrough a generative pipeline. MotionTeller combines a pretrained transformer-based actigraphy encoder \n(PAT) (F. Y. Ruan et al. 2024), originally trained on over 29,000 participants from the NHANES dataset, \nwith a lightweight projection module that maps sensor-derived embeddings into the token space of a \nfrozen decoder-only LLM. This setup enables the model to condition autoregressive generation on raw \n3 \n"}, {"page": 4, "text": " \nactivity traces and produce fluent, context-aware behavioral summaries. We also release a novel dataset of \nover 54,000 of ⟨raw sequence, generated label⟩ pairs, each consisting of a 24-hour actigraphy sequence \nand a GPT-generated summary crafted via few-shot prompting. Finally, we examine how semantic \nrepresentations evolve across training and evaluate performance both quantitatively and qualitatively. \nTogether, these contributions form a foundation for behavior-aware LLMs that translate physiological \nsignals into human-readable narratives, offering new possibilities for personalized feedback, clinical \ninterpretability, and scalable behavioral health tools. \n2. Related Work \nAs large language models (LLMs) and wearable sensing technologies continue to evolve, a growing body \nof work explores how to connect time-series representations with language-based reasoning. Prior \nresearch spans traditional actigraphy modeling, health-oriented LLMs, early attempts at sensor-to-text \ngeneration, and architectural insights from multimodal foundation models. Yet, there remains a significant \ngap in directly aligning raw sensor data with autoregressive LLMs for free-form generation, which \nMotionTeller is designed to fill. \n \n2.1 Actigraphy for Behavioral Modeling \nActigraphy has long been a core modality in health monitoring, particularly in studies of circadian \nrhythms, psychiatric disorders, and medication use. Traditional work relied on engineered features or \nstatistical summaries over time windows to predict sleep patterns, depressive symptoms, or treatment \nadherence (Heinz et al. 2022). More recently, transformer-based models have shown promise in detecting \nbehavioral states from minute-level activity traces (F. Y. Ruan et al. 2024). However, these models are \nfundamentally structured for classification or regression, producing scalar outputs rather than language. \nThey offer limited interpretability beyond a numeric label and cannot explain why an activity pattern \ncorresponds to a certain clinical state. Furthermore, most approaches operate on heavily preprocessed \ndata, sacrificing the rich temporal granularity that actigraphy provides. To the best of our knowledge, no \nknown method produces natural language descriptions directly from raw actigraphy, leaving a gap in \nmaking sensor data interpretable to non-technical users or downstream LLMs. \n \n2.2. LLMs in Health and Behavioral Tasks \nLLMs have demonstrated remarkable potential in healthcare applications, including clinical decision \nsupport, patient-centered dialogue, and mental health screening. Med-PaLM (Singhal et al. 2023) has \nshown that foundation models can achieve expert-level performance in medical question answering, while \nMe-LLaMA (Xie et al., 2024) illustrates the benefits of domain-specific pretraining for biomedical \nreasoning. In the mental health domain, LLMs are being explored not only for diagnosis and triage, but \nalso for conversation-driven support. Recent work has validated the promise of generative technology in \nthis space. Therabot, a fine-tuned LLM-based chatbot, demonstrated significant reductions in symptoms \nof depression, anxiety, and eating disorders in a randomized controlled trial (Heinz et al. 2025), with \noutcomes on par with human-delivered therapy across multiple engagement and alliance metrics. This \nprovides early evidence that LLMs can support structured therapeutic interaction when fine-tuned on \ncurated expert data. Related evaluations also highlight LLMs’ strengths in capturing diagnostic structure \n4 \n"}, {"page": 5, "text": " \nwhile revealing variability in reasoning and performance across conditions (Heinz et al. 2023). However, \nthese systems operate almost exclusively on linguistic inputs, including patient messages, transcripts, or \nclinical vignettes, and do not center on non-verbal behavioral signals such as actigraphy. MotionTeller \nbuilds on this emerging field by aligning LLMs with continuous, physiological input, treating sensor data \nnot as metadata but as a primary source for free-form narrative generation. \n \n2.3 Generating Text and Behavioral Narratives from Sensor Data \nWhile LLMs have seen broad adoption in clinical natural language processing, the generation of \nhuman-readable summaries from raw sensor data remains underexplored. Prior work has either focused \non classification (e.g., predicting conditions from wearables) or on purely prompting methods for basic \ndescription. Time2Lang (Pillai et al. 2025) made a significant step forward by directly aligning \ntime-series embeddings with LLM token representations using a projection layer, hence reducing the \ndependency on handcrafted prompts and summary templates. HealthLLM (Kim et al. 2024) similarly \nproposes aligning wearable data with LLMs for prediction tasks but relies heavily on structured sensor \nstatistics, rather than raw or high-resolution activity traces. By contrast, MotionTeller introduces a \ngenerative interface between a pretrained actigraphy encoder and a decoder-only LLM, allowing the \nmodel to interpret minute-level actigraphy data in its native form and produce rich, qualitative \ndescriptions. This approach enables context-aware behavioral language generation that goes beyond \nsimple pattern recognition or metadata tagging. On the other hand, JoLT (Cai et al. 2024) tackles a related \nproblem by aligning ECG waveform embeddings with language using a Q-Former and a frozen OPT \ndecoder. Their method supports both summarization and question answering but is designed for structured \nclinical time-series (e.g., ECG), and depends on text-aligned supervision during training. In contrast, \nMotionTeller uses behavioral signals like actigraphy, which lack structured annotations, and does not \nrequire paired expert-written summaries for training. \n \n2.4 Prompting Limitations for Behavioral Sensing \nPrompt-based generation has become a popular solution in domains with limited labeled data. In \nbehavioral health, few-shot prompting with GPT models can produce fluent summaries, particularly when \ngiven structured inputs like sleep logs or step counts. However, as highlighted by prior evaluations (e.g., \nHealthLLM; Kim et al., 2024), these summaries often lack grounding in the data itself and are prone to \nproducing generic or repetitive phrasing. MotionTeller addresses this by replacing prompt-based \nsummarization with a learned alignment: a trainable projection module that embeds behavioral signals \ninto the LLM’s input space. As demonstrated in Section 5.1.4, this yields superior performance in both \nlexical and semantic metrics, especially for data types, like raw actigraphy, that lack intrinsic semantic \nanchors. \n \n2.5 Alignment Strategies Across Modalities \nThe architecture of MotionTeller draws inspiration from recent advances in vision-language modeling. \nBLIP (J. Li et al. 2023; 2022) and Flamingo (Alayrac et al. 2022) demonstrate how frozen \nmodality-specific encoders (e.g., vision) can be paired with frozen language decoders to enable \nmultimodal generation through a trainable bridging layer. These models achieve strong results in \ncaptioning, visual QA, and storytelling by conditioning language models on non-text embeddings. \nMotionTeller adapts this architectural paradigm to wearable data: instead of images, we use \n5 \n"}, {"page": 6, "text": " \nhigh-frequency movement patterns; instead of visual attention maps, we rely on temporal patches learned \nvia the Pretrained Actigraphy Transformer (PAT) (F. Y. Ruan et al. 2024). The projection module f in \nMotionTeller performs the equivalent role of vision-to-text alignment, except in a fundamentally different \nmodality where spatial and semantic regularities are weaker. This approach allows MotionTeller to \npreserve the architectural modularity of foundation models while extending generative capacity to \ncontinuous behavioral signals. Another approach, ViTST (Z. Li, Li, and Yan 2023), tackles irregular \ntime-series classification by converting signals into line-graph images and feeding them into pretrained \nvision transformers. While effective for classification and robust to missing data, this approach does not \nmodel temporal semantics or produce natural language outputs. MotionTeller differs by directly \nembedding raw time-series into language space for generative tasks, facilitating behavioral interpretation \nrather than prediction. \n3. Methodology \n3.1 The MotionTeller Dataset & Dataset Construction \n3.1.1 NHANES Raw Actigraphy Data Overview \nThis study uses data from the 2013 to 2014 cohort of the National Health and Nutrition Examination \nSurvey (NHANES) (“NHANES Homepage” 2024), a large, nationally representative health dataset \nmanaged by the Centers for Disease Control and Prevention. Each participant in this cohort was equipped \nwith a triaxial wrist-worn accelerometer (ActiGraph GT3X+) that captured minute-level physical activity \nintensity over a continuous 7-day period, resulting in 10,080 data points per participant. These values \nrepresent the summed vector magnitude of movement at each minute. After initial participant filtering, the \nfinal dataset included 7,769 participants, each with a full week of valid actigraphy data. This yields a total \nof 54,383 individual 24-hour activity sequences, where each sequence consists of 1,440 raw minute-level \nvalues. \n \nHowever, due to hardware constraints, all experiments in this study were conducted using a Google Colab \nPro environment with a single NVIDIA L4 GPU using only one-seventh of the entire dataset, as it was not \nfeasible to load and train on the full 7-day sequence per participant. Additionally, long input sequences \nimpose considerable memory demands during sequence-to-sequence training, particularly when using \nautoregressive language models. As a result, this project focuses on a subset of the dataset, selecting one \n24-hour sequence per participant, resulting in 7,769 ⟨raw sequence, generated label⟩ pairs used for \ntraining and evaluation in this thesis. In future work, we plan to scale up MotionTeller to leverage the full \n54,383 ⟨raw sequence, generated label⟩ pairs available in the dataset, enabling multi-day modeling and \nmore robust generalization across behavioral patterns. \n \n3.1.2 Lack of Existing Paired Text Labels for Actigraphy \nNeither the NHANES dataset nor any other publicly available actigraphy dataset includes paired natural \nlanguage descriptions of daily activity. Existing actigraphy research typically focuses on classification \ntasks using structured categorical labels, such as medication use or sleep disorders, rather than descriptive, \nnarrative summaries (“NHANES Homepage” 2024). To the best of our knowledge, there is no existing \ndataset that aligns raw time-series movement data with corresponding text. This limitation has hindered \n6 \n"}, {"page": 7, "text": " \nthe development of generative models capable of producing human-readable behavioral summaries. This \nproject addresses that gap by creating the first dataset of its kind: minute-level actigraphy sequences \npaired with GPT-generated daily activity summaries. \n \n3.1.3 From Raw Signals to Structured Representations and Text Labels \n \nFigure 1. Pipeline for constructing the MotionTeller dataset. Raw minute-level actigraphy data is first segmented \ninto daily sequences and binned into 24 hourly activity levels. These structured inputs are used to prompt GPT-4o \nfor generating human-readable summaries. The final dataset consists of ⟨raw sequence, generated label⟩ pairs, \nwhich are used to train MotionTeller to generate behavioral summaries directly from raw sensor data. \n \nTo create the dataset used in this study, we developed a three-stage pipeline that bridges high-resolution \nactigraphy signals with natural language summaries. This process is illustrated in Figure 1 and consists of: \n(1) transforming raw minute-level signals into structured hourly profiles, (2) prompting a language model \nto generate text summaries from these profiles, and (3) pairing the generated summaries with the original \nraw sequences to form the final training dataset for MotionTeller. \n \nFrom Raw to Structured. The source data consists of minute-level actigraphy traces from the 2013 to \n2014 NHANES dataset, with each participant contributing 7 days of 10,080-minute sequences (i.e., 1,440 \nminutes/day × 7 days). Each day’s data is split into a separate 24-hour segment, resulting in a total of \n54,383 daily sequences across 7,769 participants. To reduce dimensionality while preserving circadian \nbehavioral patterns, we bin each 24-hour sequence into 24 hourly values. This is done by computing the \naverage movement intensity within each hour, yielding a compact and structured profile of daily activity.  \nEach of these 24-dimensional vectors is then normalized globally across the dataset using min-max \nscaling, and further rescaled to integers between 0 and 1000. This preserves comparability across \nparticipants while making the input suitable for textual prompting. \n \nFrom Structure to Text. The structured 24-hour profiles serve as input to a few-shot prompted GPT-4o \nmodel, which is instructed to act as a behavioral analyst. The prompt presents hourly activity levels using \nnatural time references (e.g., “From 0000 to 0100, the activity level is {x}...”), followed by a set of \ninstructions requesting a behavioral analysis of the full 24-hour period.  \n \n7 \n"}, {"page": 8, "text": " \nTo construct high-quality few-shot examples, we first applied KMeans clustering on the 24-bin structured \nactigraphy data to identify five representative activity profiles that captured the diversity of movement \npatterns in the dataset. Each selected cluster centroid was matched with a real participant, whose daily \nactivity sequence was then manually labeled to ensure semantic accuracy and interpretive clarity. \n \nThese hand-crafted ⟨structured activity, human-written summary⟩ pairs were then used as few-shot \nexemplars in the GPT-4o prompt, guiding the model to produce summaries that are consistent, fluent, and \nbehaviorally grounded. The full prompting format, along with an example input and output, is provided in \nAppendix A.1. \n \nConstructing the MotionTeller Dataset. After label generation, each participant’s original 1,440-minute \nraw actigraphy sequence is paired with its corresponding GPT-generated summary, forming a ⟨raw \nsequence, generated label⟩ pair. These raw–text pairs constitute the input to MotionTeller. During training, \nthe model learns to generate the textual summary directly from the raw minute-level sequence, leveraging \nthe resolution of full actigraphy traces while benefiting from the linguistic grounding of the generated \nsummaries. It is important to note that for evaluation purposes, while the summaries used for training are \nGPT-generated, the raw actigraphy sequence represents the authentic behavioral signal. As such, model \nevaluation focuses on whether generated text aligns with the input activity data, not with a potentially \nimperfect GPT label. \n \n3.1.4 Label Verification and Dataset Finalization \nTo assess the quality of the GPT-generated activity summaries, we conducted a structured evaluation on a \nheld-out set of 30 participants. These participants were selected from the NHANES 2003 to 2004 cohort, \na distinct yet methodologically identical dataset with minute-level actigraphy data collected under the \nsame protocol. This ensured that evaluation was not biased by data leakage from the training distribution. \n \nTo ensure diversity in activity patterns, we applied KMeans clustering to the structured 24-hour profiles \nand selected 6 participants from each of 5 clusters, producing a balanced evaluation set of 30 samples. \nThis sample size is commonly used in cognitive science and machine learning literature as a statistically \nmeaningful minimum for pilot validation of semantic fidelity and inter-rater reliability, while remaining \nfeasible for detailed human review (Bujang et al. 2024). \n \nThe evaluation rubric was developed collaboratively and covered six distinct criteria: \n1.​ Identification of Peak Activity \n2.​ Description of Nighttime to Dawn (0000–0600) \n3.​ Description of Early Morning to Noon (0600–1200) \n4.​ Description of Afternoon Activity (1200–1800) \n5.​ Description of Evening and End-of-Day (1800–0000) \n6.​ Quality of Language and Bias Avoidance \n \nEach criterion was scored on a 1 to 5 scale, where 5 represents perfect fidelity and 1 represents a \nsignificant error or omission. This yields a total possible score of 30 per participant summary. A full \ndescription of the rubric and score definitions is provided in Appendix B. The evaluation was conducted \nindependently by two raters: the project first-author (Zhang) and a senior research mentor (Pillai). Neither \n8 \n"}, {"page": 9, "text": " \nrater communicated with the other during scoring. The average total scores were 28.3 (Zhang) and 28.6 \n(Pillai) out of 30, reflecting high quality and cross-rater agreement. Minor discrepancies occurred \nprimarily in areas of stylistic preference or interpretive emphasis, not in core semantic accuracy. \n \nBased on this evaluation, we concluded that the GPT-generated summaries were largely consistent, \nbehaviorally faithful, and suitable for use as supervision targets in downstream model training. \n \n \nFigure 2. Example of a 24-hour structured actigraphy sequence used as input to the GPT-4o label generation \npipeline, shown alongside the resulting summary. The bar chart displays hourly activity levels, and the summary \nreflects behavioral patterns across the night, morning, afternoon, and evening. This figure illustrates the kind of \ngenerated output evaluated using the rubric in Appendix B. \n9 \n"}, {"page": 10, "text": " \n3.2 MotionTeller Model Architecture \nMotionTeller consists of three major components: a frozen pretrained PAT encoder, a lightweight \nprojection module, and a frozen decoder-only LLM. \n \n3.2.1 Pretrained Actigraphy Transformer (PAT) \n \nFigure 3. Architecture of the pretrained Actigraphy Transformer (PAT). A single 1,440-minute actigraphy \nsequence is split into 80 patches of 18 minutes each. Each patch is embedded into a 96-dimensional vector and \naugmented with fixed sinusoidal positional encodings. The resulting sequence of patch tokens is passed through a \nstack of transformer encoder layers. The model outputs a contextualized embedding of shape [80, 96], which serves \nas the input to the MotionTeller projection module. \n \nThe Pretrained Actigraphy Transformer (PAT) serves as the fixed encoder in the MotionTeller pipeline. \nOriginally introduced in Ruan et al. (2024), PAT was pretrained on over 29,000 participants using a \nmasked autoencoding objective and designed to encode daily actigraphy into semantically meaningful \ntoken representations.  \n \nFor this study, we use the PAT-Large (PAT-L) variant, consisting of a patching layer, fixed sinusoidal \npositional encodings, and multiple transformer encoder blocks. Each raw actigraphy sequence of 1,440 \nminutes is split into 80 non-overlapping patches of 18 minutes. As shown in Figure 3, these patches are \nembedded and passed through the encoder, producing a final output of shape [80, 96]. The PAT model is \nloaded using its saved Keras .h5 weights and used strictly in inference mode, with all parameters frozen. \nFor each participant-day, we pass the raw actigraphy vector into PAT and extract the resulting token-level \nembedding, which encodes both short- and long-range temporal dependencies in a compact form.  \n \nFinally, these [80, 96] PAT embeddings are then saved and passed to the downstream projection module \n(Section 3.2.2), which prepares them for input into the decoder-only language model. A detailed \narchitectural schematic of PAT is presented in Figure 3. For full implementation specifics, such as encoder \ndepth, pretraining strategy, and patching protocol, we refer readers to the original PAT publication by F. Y. \nRuan et al. 2024. In the context of this work, PAT serves as a frozen, pretrained module that transforms \n10 \n"}, {"page": 11, "text": " \nhigh-resolution actigraphy into structured token embeddings suitable for alignment with a language \nmodel. \n \n3.2.2 Projection Module f \nThe MotionTeller framework includes a lightweight projection module f that maps the pretrained PAT \nembeddings into the token embedding space expected by the decoder-only language model. Specifically, \nthe output from PAT is a sequence of 80 embeddings of dimension 96, i.e., a shape of [80, 96] per \nparticipant-day. However, the decoder model (Gemma-2B) expects inputs of shape [N, 2048], where 2048 \nis the dimensionality of its token embedding space. \n \nTo bridge this mismatch, we define f as a simple feedforward neural network that transforms each of the \n80 token vectors independently. It performs a linear projection from 96 to 2048 dimensions, effectively \nmapping [80, 96] → [80, 2048]. This projected output is then used as a contextual prefix to condition the \ndecoder’s autoregressive generation of behavioral summaries. While more complex architectures for f, \nsuch as 1D CNNs or LSTMs, were considered, empirical testing showed that they significantly increased \ninference time and training instability without improving downstream performance. In contrast, a simple \nfully connected layer (or two-layer MLP) was not only faster but also led to faster convergence and stable \ngradients during training. This simplicity enables efficient batch processing and ensures compatibility \nwith frozen LLM architectures that are sensitive to input embedding shape. \n \nThe projection head f is the only trainable component in the MotionTeller pipeline. Both the PAT encoder \nand the LLM decoder remain frozen during training. As a result, the training objective focuses exclusively \non learning a semantic projection from sensor-derived embeddings to the LLM’s input space. \n \n3.2.3 Decoder LLM (Gemma-2B) and Autoregressive Generation \n \nFigure 4. Architecture of the MotionTeller generative pipeline. Raw minute-level actigraphy is encoded into [80, \n96] token embeddings using a frozen pretrained PAT encoder. These embeddings are projected into the decoder’s \nembedding space ([80, 2048]) via a trainable projection head f. The projected actigraphy tokens are concatenated \n11 \n"}, {"page": 12, "text": " \nwith tokenized summary labels (via the Gemma tokenizer) and passed into a frozen decoder-only LLM (Gemma-2B). \nThe model autoregressively generates the behavioral summary, and token-level cross-entropy loss is computed \nbetween predicted logits and the reference label tokens. \n \nThe final component of the MotionTeller model is a decoder-only LLM that generates a behavioral \nsummary conditioned on the projected actigraphy embeddings. For this component, we use the \nGemma-2B model, a frozen decoder-only transformer trained for natural language generation. All weights \nin the LLM remain frozen during training. \n \nAs shown in Figure 4, the LLM receives a customized input: a concatenation of the projected PAT \nembeddings (shape [80, 2048]) and the token embeddings of the target summary, which are produced \nusing the Gemma tokenizer. A special start-of-sequence token [BOS] is prepended to mark the beginning \nof the sequence. The full input sequence allows the decoder to condition on actigraphy data and generate \nthe summary in an autoregressive manner. \n \nDuring training, the attention mask is configured such that the model can attend to the entire PAT \nembedding prefix when predicting each token. The first 80 positions, which correspond to the projected \nPAT tokens, are excluded from loss computation by assigning them a label mask of -100. As a result, loss \nis only computed over the language portion of the sequence, while the PAT segment acts as a fixed prefix. \n \nAlthough PAT embeddings and label tokens are structurally identical in shape (both 2048-dimensional \nvectors), they are semantically and positionally distinguishable. The model consistently receives the first \n80 tokens as PAT embeddings, never associated with supervision. These tokens serve as a frozen, \nsemantically rich context, akin to a prefix prompt. The decoder’s causal attention mechanism ensures that \neach generated token is conditioned on both the PAT context and previously generated tokens. This setup \nis structurally analogous to prefix-based multimodal prompting, as seen in models like BLIP and \nFlamingo. \n \nFinally, the decoder outputs a sequence of token-level logits, which are compared against the reference \nlabel tokens using cross-entropy loss. Only the projection head f is updated during training; both the PAT \nencoder and the decoder LLM remain frozen. This modular architecture ensures parameter efficiency, \nsemantic interpretability, and compatibility with large-scale pretrained models. All together, these \ncomponents form a modular and interpretable pipeline for generating language from wearable time-series \ninput. \n \nAt inference time, the model takes as input a raw actigraphy sequence from an unseen participant and \ngenerates a behavioral summary. The downstream task is thus defined as free-form generation \nconditioned on raw sensor data, with no label input at test time. \n \n12 \n"}, {"page": 13, "text": " \n4. Experiments \nThe primary objective of MotionTeller is to generate fluent and behaviorally accurate textual summaries \nfrom raw wearable sensor data. During inference, the model receives only a raw 24-hour minute-level \nactigraphy sequence, which is passed through a frozen PAT encoder and projection head f, and uses the \nresulting token embeddings to condition the generation of a natural language summary. No label or \nreference text is provided at test time. The model must therefore learn to generalize from sensor-derived \nembeddings to coherent and semantically faithful language outputs. This section describes the \nexperimental setup, training conditions, and evaluation procedures used to assess MotionTeller’s \nperformance on this downstream generative task. \n \n4.1 Training Setup \nAll experiments were conducted using a Google Colab Pro + environment with a single NVIDIA L4 \nGPU, approximately 24 GB of RAM, and mixed-precision training enabled via PyTorch. The final dataset \nused in the actual experiment consisted of 7,769 ⟨raw sequence, generated label⟩ pairs, split into 80% \ntraining, 10% validation, and 10% test sets. \n \nThe model was trained for 15 epochs, each comprising over 3,000 training steps, resulting in a total \nruntime of more than 45 hours. A batch size of 2 was used for both training and validation due to GPU \nmemory limitations. Training was conducted using teacher forcing and cross-entropy loss, applied only to \nthe summary tokens (with the actigraphy prefix masked using a label value of -100; see Section 3.2.3). \n \nA linear learning rate scheduler (LinearLR) was used to gradually warm up the learning rate over the first \n100 steps. Training progress was logged every 10 steps, and model checkpoints were selected based on \nthe validation BERTScore-F1. On average, each epoch required approximately 3 hours and 15 minutes, \ndepending on runtime variability and logging overhead. \n \n4.2 Evaluation Metrics \nTo assess the quality of MotionTeller’s generated summaries, we use a combination of lexical overlap and \nsemantic similarity metrics, applied to both the validation and test sets. These metrics evaluate whether \nthe model can generate behavioral descriptions that are both linguistically accurate and semantically \nfaithful to the GPT-generated reference summaries. \n \nThe following metrics are used for evaluation: \n1.​ ROUGE-1 and ROUGE-L measure word-level overlap between the generated summary \n(prediction) and the ground truth label (reference). ROUGE-1 captures unigram (individual word) \noverlap, while ROUGE-L measures the length of the longest common subsequence (LCS), which \nis especially useful for evaluating narrative structure and phrasing. These metrics were computed \nusing the rouge_score library with stemming enabled, allowing for more flexible matching across \ndifferent word forms (e.g., “sleep” vs. “sleeping”). \n2.​ BERTScore (Precision, Recall, F1) evaluates semantic similarity between the generated and \nreference summaries using contextual embeddings from a pretrained transformer. This allows the \n13 \n"}, {"page": 14, "text": " \nevaluation to account for paraphrasing and meaning preservation even when exact words do not \nmatch.  \n \nAll metrics are computed on a per-participant basis and then macro-averaged across the dataset split. \nDuring testing, the model is evaluated on its ability to generalize on unseen actigraphy sequences from \nparticipants not present in the training set. These scores form the basis for the quantitative analyses \npresented in Section 5.1. \n \n4.3 Cluster-Based Evaluation Subset \nTo ensure a structured and behaviorally diverse evaluation, we created a cluster-based subset of 100 \nparticipants drawn from the full dataset. We first applied KMeans clustering (k = 5) to the structured \n24-hour actigraphy representations described in Section 3.1.3, which are used to generate labels in the \nMotionTeller Dataset. Each input consisted of 24 hourly activity values per participant, normalized to the \n[0, 1000] range. The clustering was performed on these 24-dimensional vectors to group participants \nbased on coarse-grained behavioral archetypes, such as morning-dominant activity, low overall \nmovement, or irregular temporal patterns. \n \nFrom each of the five resulting clusters, we randomly sampled 20 participants, resulting in a balanced \nevaluation subset of 100 participants that reflects a broad spectrum of daily movement behaviors. This \nsubset was used for multiple downstream purposes: \n(a)​ In Section 5.1.4 we use these participants to generate summaries via few-shot GPT-4o prompting \ndirectly on raw actigraphy, creating a baseline set of labels to compare against MotionTeller. \n(b)​ In Section 5.2, we use an individual participant’s result from this cohort in qualitative analysis, \nexamining how well MotionTeller-generated summaries reflect ground-truth behavioral patterns. \n(c)​ In Section 5.3.1, we report cluster-wise evaluation metrics using ROUGE and BERTScore, \noffering insight into the model’s performance across distinct behavior types. \n5. Results \nThis section presents both quantitative and qualitative evaluations of MotionTeller’s performance. We \nbegin with aggregate metrics on validation and test sets across multiple training epochs, followed by \nsample-level analysis of generated summaries. Finally, we evaluate MotionTeller against a \nprompting-based GPT-4o baseline to contextualize its generative capacity under minimal supervision. \n \n5.1 Quantitative Performance \n5.1.1 Validation Set Performance Across Training Epochs \nWe evaluated MotionTeller on the held-out validation set of 777 participants using ROUGE and \nBERTScore metrics at three checkpoints: after epochs 5, 10, and 15. These metrics assess the degree to \nwhich the model’s generated summaries align with the GPT-generated reference labels, both lexically and \nsemantically. Table X presents the mean and standard deviation of each metric across all participants. \n \n14 \n"}, {"page": 15, "text": " \nTable 1: Validation performance of MotionTeller on the full held-out test set (777 participants) at epochs 5, 10, \nand 15. Reported values are the mean and standard deviation across all participants. ROUGE metrics assess lexical \nand structural similarity to reference summaries, while BERTScore evaluates semantic alignment using contextual \nembeddings. \nEpoch \n5 \n10 \n15 \nMetric \nMean ± Std Dev \nMean ± Std Dev \nMean ± Std Dev \nRouge-1 \n0.6837 ± 0.1407 \n0.7078 ± 0.1044 \n0.7224 ± 0.0634 \nRouge-L \n0.4147 ± 0.0970 \n0.4272 ± 0.0813 \n0.4294 ± 0.0664 \nBERTScore_P \n0.9189 ± 0.0376 \n0.9216 ± 0.0241 \n0.9241 ± 0.0194 \nBERTScore_R \n0.9120 ± 0.0312 \n0.9188 ± 0.0208 \n0.9219 ± 0.0137 \nBERTScore_F1 \n0.9154 ± 0.0342 \n0.9202 ± 0.0221 \n0.9230 ± 0.0161 \n \nAs shown in Table 1, we observe steady improvements across all evaluation metrics throughout training.  \n●​ ROUGE-1 increases from 0.6837 ± 0.1407 at epoch 5 to 0.7224 ± 0.0634 at epoch 15, indicating \nbetter lexical overlap with reference summaries.  \n●​ ROUGE-L improves similarly, suggesting that the model increasingly captures the structural and \nsequential elements of the narrative.  \n●​ BERTScore-F1 rises from 0.9154 ± 0.0342 to 0.9230 ± 0.0161, confirming the model’s enhanced \nsemantic alignment with the reference texts. \n \nRemarkably, while the gains after epoch 10 are relatively modest, the projection module f receives over \n3,000 gradient updates per epoch, enabling it to converge quickly. Since the PAT encoder and decoder \nLLM are both frozen, this efficient adaptation through f reflects its effectiveness in aligning actigraphy \nembeddings with the LLM’s input space. The narrowing standard deviations across epochs also suggest \nincreased consistency in summary quality across participants. \n \nThese validation results confirm that MotionTeller is not only able to generate summaries with high \nlexical and semantic fidelity, but that its performance stabilizes within the first half of training, supporting \na parameter-efficient generative pipeline for wearable sensor data. \n \n5.1.2 Training Loss Across Epochs \nTo better understand the model’s convergence behavior, we tracked the average training loss per epoch \nacross all 15 epochs. The results are visualized in Figure 5, which plots the loss curve during fine-tuning \nof the projection module f. \n \n15 \n"}, {"page": 16, "text": " \n \nFigure 5: Average training loss per epoch during fine-tuning of the projection module f over 15 epochs. The \nmodel converges rapidly within the first few epochs and continues to improve gradually thereafter. \n \nAs shown in Figure 5, MotionTeller’s training loss exhibits a rapid and stable decline, dropping from 2.22 \nin epoch 1 to 0.38 by epoch 15. The most significant decrease occurs within the first five epochs, after \nwhich the loss continues to decline gradually. This trend aligns with the observed plateau in validation \nperformance (Section 5.1.1), and supports the interpretation that the model converges quickly due to the \nhigh frequency of updates, over 3,000 training steps per epoch, despite f being the only trainable module. \n \nThere is no visible trend of overfitting or instability. The loss curve indicates stable optimization, \nreinforcing the viability of MotionTeller’s architecture for efficient training in low-resource environments. \n \n5.1.3 Test Set Performance \nTo evaluate MotionTeller’s ability to generalize to unseen participants, we assessed its performance on a \nheld-out test set of 777 participants, using the same metrics and checkpoints (epochs 5, 10, and 15) as in \nthe validation analysis. The results are presented in Table 2. \n \nTable 2: Test set performance of MotionTeller on 777 unseen participants at epochs 5, 10, and 15. \nReported values are the mean and standard deviation across all participants. The results show close alignment with \nvalidation performance (see Table 1), indicating strong generalization to unseen actigraphy inputs. \nEpoch \n5 \n10 \n15 \nMetric \nMean ± Std Dev \nMean ± Std Dev \nMean ± Std Dev \nRouge-1 \n0.6843 ± 0.1428 \n0.7114 ± 0.0990 \n0.7221 ± 0.0614 \nRouge-L \n0.4137 ± 0.0974 \n0.4286 ± 0.0798 \n0.4330 ± 0.0655 \nBERTScore_P \n0.9185 ± 0.0384 \n0.9222 ± 0.0230 \n0.9249 ± 0.0158 \nBERTScore_R \n0.9119 ± 0.0318 \n0.9192 ± 0.0195 \n0.9222 ± 0.0129 \nBERTScore_F1 \n0.9152 ± 0.0349 \n0.9207 ± 0.0209 \n0.9235 ± 0.0137 \n16 \n"}, {"page": 17, "text": " \n \nAcross all three epochs, test set performance closely aligns with validation performance. For example, \nROUGE-1 improves from 0.6843 ± 0.1428 (epoch 5) to 0.7221 ± 0.0614 (epoch 15), nearly identical to \nthe validation trajectory as shown in Table 1. Similar trends are observed in ROUGE-L and all \nBERTScore metrics, with BERTScore-F1 increasing from 0.9152 ± 0.0349 to 0.9235 ± 0.0137. This high \ndegree of consistency suggests that MotionTeller is not only stable during training but also capable of \nretaining performance across distributional shifts, even when generating summaries for entirely unseen \nactigraphy sequences. \n \nThese results provide further evidence that the projection module f learns a robust and generalizable \nalignment between sensor embeddings and language space. Because no label information is available at \ntest time, the model’s strong performance confirms that it is not overfitting to training set patterns or label \nstyle. Instead, it has learned to condition on raw actigraphy alone and generate fluent, semantically \naligned summaries across a wide range of behavioral profiles. \n \nThe small and consistently low standard deviations across metrics also point to generally uniform \nperformance across participants, indicating that MotionTeller is not just strong on average, but also \ndependable across individuals with diverse activity patterns. \n \n5.1.4 Prompting-Base Baseline Comparison \nTo evaluate whether MotionTeller meaningfully improves over direct prompting on raw actigraphy, we \ncompare its outputs to a baseline condition in which GPT-4o is prompted using few-shot examples \nderived from raw minute-level sequences, as first described in Section 4.3. This baseline uses the same \nfive hand-labeled ⟨raw sequence, generated label⟩ pairs for few-shot prompting as described in Section \n4.3, but relies entirely on zero pretraining or learned representations. The resulting summaries were then \nscored against the MotionTeller Dataset labels (generated using structured hourly data) using the same \nevaluation metrics. \n \nTable 3: Comparison of prompting-based GPT-4o baseline (raw input + few-shot) with MotionTeller’s output \nafter 15 epochs on the same 100-person evaluation subset. MotionTeller outperforms the baseline across all \nmetrics, confirming the benefit of learning a structured embedding space over relying on prompt engineering alone. \nMetric \nRaw Baseline \nMotionTeller Output (15 epochs) \nRouge-1 \n0.6453 ± 0.0800 \n0.7221 ± 0.0614 \nRouge-L \n0.3425 ± 0.0522 \n0.4330 ± 0.0655 \nBERTScore_P \n0.9028 ± 0.0150 \n0.9249 ± 0.0158 \nBERTScore_R \n0.9084 ± 0.0135 \n0.9222 ± 0.0129 \nBERTScore_F1 \n0.9056 ± 0.0139 \n0.9235 ± 0.0137 \n \nTable 3 presents the comparison between the raw baseline and the MotionTeller output at epoch 15 on the \nsame 100-person cluster-based evaluation subset. MotionTeller outperforms the prompting baseline across \nall metrics, with particularly large gains in ROUGE-1 (~7.7%) and ROUGE-L (~9%), indicating \n17 \n"}, {"page": 18, "text": " \nsubstantial improvement in both lexical overlap and structural alignment. Semantic metrics also \nimproved, with BERTScore-F1 rising from 0.9056 to 0.9235, reflecting higher-quality language that is \nmore faithful to the intended behavioral summaries. \n \nThese baseline results demonstrate that MotionTeller’s learned alignment between actigraphy and \nlanguage space, enabled by the PAT encoder and projection head, offers significant performance \nadvantages over non-trainable prompt-based approaches. While prompting alone can produce broadly \nreadable summaries, it fails to capture participant-specific behavioral nuance and often reverts to generic \nphrasing. MotionTeller, in contrast, leverages full-resolution input and token-level conditioning to \nproduce semantically richer and more behaviorally grounded outputs. \n \n5.1.5 Quantitative Performance Summary \nIn summary, the results across validation and test sets demonstrate that MotionTeller is capable of \ngenerating high-quality behavioral summaries from raw actigraphy sequences with both semantic fidelity \nand lexical structure. The projection module f converges efficiently within the first few epochs, while \nperformance remains stable and consistent through epoch 15. The strong alignment between validation \nand test metrics confirms that MotionTeller generalizes well to unseen participants, supporting its use as a \nlightweight, interpretable, and robust model for language-based behavioral understanding in wearable \nsensing applications. \n \n5.2 Qualitative Performance \n \nFigure 6. Example of a MotionTeller-generated behavioral summary for a full-day actigraphy sequence. The bar \nchart (see above) visualizes minute-level physical activity data across a full day, where each bar represents \nmovement intensity per minute. The accompanying summary (see below) is generated by MotionTeller and \nsegmented into behavioral descriptions spanning midnight to dawn, dawn to noon, noon to evening, and evening to \nmidnight. This example is drawn from a representative participant in the 100-person cluster-based evaluation subset \nand demonstrates how MotionTeller captures both temporal dynamics and overall behavioral patterns with natural \nlanguage. Key statements in the summary reflect global reasoning (e.g., recognizing consistent low-level \nengagement), local event alignment (e.g., brief morning activity), and narrative structure. \n \n \n \n \n \n \n \n18 \n"}, {"page": 19, "text": " \nSection \nGenerated Label \nOverall Statement \n“The participant’s activity data reveals a pattern of low movement throughout the day, with \nactivity levels remaining within a narrow range. This suggests that the participant may have a \nroutine that involves a significant amount of low-level engagement, with some fluctuations in \nactivity levels. \nMidnight (00:00) to \nDawn (06:00) \nDescription \nDuring the night to dawn, the participant shows minimal movement, indicating a period of rest or \nsleep. There are brief moments of activity, but these are sporadic and do not suggest a continuous \nperiod of engagement. \nDawn (06:00) to Noon \n(12:00) Description \nAs the morning progresses, activity levels increase slightly, indicating a transition from rest to a \nmore active state. The movement remains low, but there is a gradual rise, suggesting a gradual \nincrease in engagement as the day unfolds. \nNoon (12:00) to Late \nAfternoon (18:00) \nDescription \nIn the afternoon, the pattern of low movement continues, with activity levels remaining \nconsistent and steady. There is no significant increase or decrease, indicating a sustained period \nof low-level engagement. This suggests a routine that involves regular, low-intensity activity. \nEvening (18:00) to \nMidnight (24:00) \nDescription \nDuring the early evening, the participant maintains a similar pattern of low movement. Activity \nlevels remain consistent and do not show any significant fluctuations. This suggests a \ncontinuation of the participant’s routine, with no dramatic changes in engagement. \n \nSummarizing Statement \nOverall, the participant’s activity data suggests a routine characterized by low movement, with a \ngradual increase in activity levels from morning to evening. This pattern may reflect a structured \ndaily routine, with consistent but low-intensity engagement throughout the day.” \n \nAs shown in the example above, the participant’s activity profile shows a clear low-intensity period \nduring the early morning hours, followed by a gradual increase in movement across the day, peaking \nmodestly in the evening. The generated summary accurately reflects these trends in multiple ways. We \ncan examine a few details: \n●​ First, the sentence “The participant’s activity data reveals a pattern of low movement throughout \nthe day, with activity levels remaining within a narrow range.” (highlighted in blue) demonstrates \nMotionTeller’s ability to reason about global context across the full day, rather than over-indexing \non local peaks or noise. \n●​ Second, the phrase “There are brief moments of activity...” (green) corresponds to small spikes in \nthe morning seen between 0600 and 0900, which are faithfully echoed in the language output \nwithout exaggeration. \n●​ Third, the clause “There is no significant increase or decrease, indicating a sustained period of \nlow-level engagement” (grey) matches the pattern in the actigraphy trace from midday to \nevening, showing the model’s ability to describe progressive behavioral arcs. \n●​ These details illustrate not just the model’s semantic correctness, but its ability to anchor \ndescription in behaviorally meaningful segments, striking a balance between high-level narrative \nframing and faithful signal interpretation. \n \nHowever, it is important to acknowledge that some of the language remains relatively generic, a known \ntendency of decoder-only LLMs. For instance, phrases like “a gradual rise in engagement” or “consistent \nlow-level activity” are behaviorally plausible but could also be applied, without much revision, to other \nparticipants with similar patterns. This highlights a limitation in MotionTeller’s ability to produce \n19 \n"}, {"page": 20, "text": " \nparticipant-specific, discriminative summaries, despite strong alignment to the general temporal structure. \nImproving the precision and distinctiveness of generated text remains an important future direction. \n \nNote that NHANES actigraphy values represent minute-level activity intensities based on MIMS \n(Monitor-Independent Movement Summary) units, which are device-independent and unitless. While not \ndirectly interpretable as physical quantities like steps or energy expenditure, they provide a reliable \nestimate of movement magnitude per minute. For more details on interpreting NHANES accelerometer \ndata, please refer to Appendix B: Understanding MIMS Units and Movement Intensity Thresholds. \n \n5.3 Representative Analysis \nTo analyze how MotionTeller reshapes the internal representation of actigraphy data, we performed \nprincipal component analysis (PCA) on the participant-level embeddings. Specifically, we compared the \noriginal embedding output from the frozen PAT encoder to the projected embeddings produced by \nMotionTeller (after the projection module f) at epoch 15. For each participant, we averaged the 80 token \nembeddings along the time dimension to obtain a single vector per participant (i.e., shape [96] for PAT, \n[2048] for MotionTeller). This allowed us to reduce the data to 2D for visualization using PCA. \n \nFigure 7: 2D PCA of participant embeddings before and after MotionTeller training. Left: original PAT \nembeddings (frozen). Right: MotionTeller embeddings after projection by f. Each dot represents a participant, \ncolor-coded by behavior cluster (dark blue = Cluster 1, green = Cluster 2, brown = Cluster 3, grey = Cluster 4, \nturquoise = Cluster 5). \n \nPCA identifies the two orthogonal directions of greatest variance in the data. The x-axis (PCA-1) captures \nthe most significant axis of variability across participants’ embeddings, while the y-axis (PCA-2) captures \nthe second-most significant. While the absolute axes are not directly interpretable in semantic terms, \nrelative position and spatial separation reveal the internal geometry of the embedding space, showing how \nparticipants are organized by the model. \n \nIn the left panel of Figure 7, we visualize the frozen PAT embeddings. The clusters appear somewhat \ndiffuse and overlapping, particularly between Clusters 2, 4, and 5. The right panel shows the embeddings \n20 \n"}, {"page": 21, "text": " \nafter transformation by MotionTeller’s projection module. Here, we observe sharper alignment of points \nalong a shared axis and increased local cohesion within clusters, particularly Clusters 3 and 4. This \nsuggests that the projection module f has learned to organize behaviorally similar participants into \nsemantically meaningful regions of the latent space, better suited for downstream generation. \n \nThis structural reorganization provides evidence that MotionTeller’s representation learning is not only \nfunctionally successful (as shown by evaluation metrics), but also geometrically interpretable, aligning \nembeddings according to shared behavioral traits. \n \n5.3.1 Cluster-Wise Model Performance \nTo further investigate how MotionTeller performs across different behavioral types, we computed \nevaluation metrics for each of the five clusters described in Section 4.3, using the subset of 100 \nparticipants (20 per cluster). Results are shown in Table 4. \n \nTable 4. Cluster-wise evaluation of MotionTeller-generated summaries at epoch 15. Each column represents a \ndistinct behavioral cluster (defined via KMeans on 24-hour activity profiles), and rows show average performance \nacross 20 participants per cluster (100 total). Metrics include ROUGE-1 and ROUGE-L for lexical and structural \noverlap, and BERTScore F1 for semantic alignment with reference summaries. MotionTeller achieves the highest \nperformance on Cluster 4, which likely represents participants with structured, interpretable behavioral patterns. In \ncontrast, Cluster 1, characterized by lower and more ambiguous activity—shows consistently lower scores across all \nmetrics \nMetrics \nCluster 1 \nCluster 2 \nCluster 3 \nCluster 4 \nCluster 5 \nRouge 1 \n0.6460 ± 0.0777 \n0.7216 ± 0.0497 \n0.7438 ± 0.0317 \n0.7499 ± 0.0163 \n0.7362 ± 0.0319 \nRouge L \n0.3697 ± 0.0512 \n0.4283 ± 0.0594 \n0.4432 ± 0.0612 \n0.4715 ± 0.0504 \n0.4478 ± 0.0560 \nBERTScore F1 \n0.9118 ± 0.0114 \n0.9227 ± 0.0129 \n0.9250 ± 0.0109 \n0.9312 ± 0.0072 \n0.9254 ± 0.0106 \n \n \nIn contrast, Clusters 3 to 5 (brown, turquoise, and grey) exhibit both tighter embedding distributions and \nhigher evaluation scores, with Cluster 4 achieving the highest BERTScore-F1 (0.9312), ROUGE-1 \n(0.7499) and ROUGE-L (0.4715). These clusters likely correspond to richer, more structured behavior \nprofiles (e.g., activity spikes or circadian regularity), which are more amenable to consistent \nsummarization. \n \nThese trends suggest that the semantic separability of participant embeddings after training, visible in \nPCA, correlates with the model’s downstream performance across behavior types. Clusters with \nwell-aligned embeddings tend to support higher-quality text generation, reinforcing the effectiveness of \nMotionTeller’s projection-based alignment.\n \n21 \n"}, {"page": 22, "text": " \n6. Discussion \nThis section reflects on the implications, strengths, and limitations of the MotionTeller architecture. We \ndiscuss the model’s performance across diverse behavior types, its potential for scalable behavioral \nmonitoring, and key areas for improvement including grounding, explainability, and clinical relevance. \n \n6.1 Interpretations of the MotionTeller Framework \nMotionTeller demonstrates that it is possible to align raw behavioral signals with natural language \nthrough a lightweight, modular architecture. Across multiple quantitative metrics, which include ROUGE, \nBERTScore, and cluster-specific evaluations, the model achieves considerably high semantic and lexical \nfidelity, even without finetuning the decoder LLM. Notably, the model performs best on clusters with \nstructured activity patterns, suggesting that it learns to anchor behavioral variation within a coherent \nnarrative space. \n \nThe success of the projection module f, despite its simplicity, highlights the strength of modular \ncross-modal design: raw actigraphy embeddings, when aligned with token representations, become \nexpressive enough to support rich, autoregressive language generation. This is further validated by the \nPCA analysis of representation space before and after training, which shows a shift toward tighter, \nsemantically organized clusters. Importantly, this organization emerges without supervised labels, \nindicating that MotionTeller learns to map actigraphy into meaningful latent regions of the LLM’s input \nspace using only generative supervision. \n \nQualitative analysis also reveals that the model reliably captures broad behavioral rhythms and \ntransitions, with outputs that reflect daily structure and movement intensity. At the same time, some \noutputs rely on generalized phrasing, underscoring both the strength and the limitations of LLMs trained \non relatively homogeneous summaries. All in all, MotionTeller bridges a novel representational gap, \nunlocking natural language generation from low-semantic, temporally continuous sensor input. \n \n6.2 Implications for Future Health Applications \nMotionTeller’s generative approach to behavioral data unlocks a new class of applications in digital health \nand human-computer interaction. By producing context-aware, human-readable summaries from raw \nactigraphy, the model offers an interpretable interface between movement and meaning. These summaries \ncould be embedded into patient-facing dashboards, used for therapeutic journaling, or support clinicians \nin reviewing behavioral trends over time. \n \nBecause MotionTeller is modular - one that uses a frozen encoder and decoder - it offers a flexible \nfoundation for further adaptation. Different encoder architectures (e.g., for multimodal input) or decoder \ntypes (e.g., instruction-tuned models) could be integrated with minimal changes. This design supports \nscalability and personalization, while maintaining transparency and separation of concerns between signal \nencoding and linguistic reasoning. \n \n22 \n"}, {"page": 23, "text": " \nMore broadly, MotionTeller demonstrates that LLMs can serve not just as linguistic generators, but as \nsemantic translators for physiological signals. As wearable technologies become more ubiquitous, \nsystems like MotionTeller could help bridge the gap between raw behavioral data and human-centered \nexplanation, eventually achieving scalable, personalized, and interpretable behavioral health tools. \n \n6.3 Limitations and Areas for Improvement \nWhile promising, MotionTeller has several limitations that offer direction for future work. First, all \nsupervision comes from GPT-generated summaries, which, despite being fluent and behaviorally \ngrounded, lack diversity in phrasing and stylistic nuance. As a result, the model may learn to mimic \nstructural patterns without fully exploring alternative modes of description. Improving label \nexpressiveness, perhaps through curated few-shot prompts or fine-tuned labeling models, could enhance \ngenerative richness. \n \nSecond, the frozen decoder simplifies training and reduces parameter count, but it may limit \npersonalization or linguistic adaptability. In settings that require domain-specific tone or more varied \nconversational output, finetuning the decoder or adding adapter layers may offer value. Additionally, \nsome activity traces are ambiguous or context-sensitive: a flat trace could indicate sleep, rest, or \nnon-wear. Without additional context, the model must guess, subsequently raising challenges for \ninterpretation and reliability. \n \nFinally, the current system is unidirectional: it summarizes behavior but cannot yet support interactive \nquerying or clarification. Extending the system to support question answering or feedback-driven \nrefinement would make it more usable in real-world clinical or personal health settings. Further, \nincorporating complementary signals such as self-reported mood, context diaries, or sleep logs could help \ndisambiguate activity traces and support more grounded, multimodal interpretation. \n \n \n \n \n \n \n \n \n \n \n \n \n23 \n"}, {"page": 24, "text": " \n7. Conclusion \n \nIn this thesis, we proposed MotionTeller, a novel framework for generating behavioral summaries directly \nfrom raw wearable sensor data using a pretrained actigraphy encoder and a frozen large language model \n(LLM). By combining a frozen PAT encoder, a lightweight trainable projection module, and a frozen \ndecoder-only LLM, MotionTeller is able to align high-resolution time-series data with natural language in \nan efficient and semantically faithful manner. We introduced a new actigraphy-text dataset, designed an \neffective training and evaluation pipeline, and demonstrated through both quantitative and qualitative \nevaluations that MotionTeller produces fluent, contextually relevant summaries grounded in behavioral \nstructure. \n \nOur results show that MotionTeller consistently outperforms prompting-based baselines, generalizes to \nunseen participants, and produces interpretable outputs that reflect both temporal and semantic alignment. \nThe architecture is efficient, adaptable, and opens new possibilities for using LLMs in behavioral \nmodeling tasks. \n \nFuture Work \nThere are several promising directions for expanding this work: \n1.​ Scaling up data and granularity: Future iterations could leverage multi-day actigraphy \nsequences to support longitudinal behavior tracking and include more granular labeling, such as \nsplitting raw data into four-hour segments and attaching fine-grained text descriptions every 10 to \n15 minutes. This could increase interpretability and enable richer narrative modeling. \n2.​ Improving the variety and richness of label supervision: The current GPT-generated \nsummaries, while fluent and structurally consistent, often exhibit lexical homogeneity and \ntemplate-like phrasing (e.g., repeated use of patterns such as “the participant’s activity is \ncharacterized by...”). This may limit the expressive range of the model’s outputs. Future work \ncould focus on enhancing the linguistic richness of training labels through better-curated few-shot \nexamples, alternative prompting strategies, or the use of different LLMs beyond GPT-4o. This \nwould promote greater stylistic variation and behavioral nuance in generated summaries and \nreduce the tendency toward generic or overgeneralized language. \n3.​ Fine-tuning the decoder LLM: Currently, the LLM remains frozen. Fine-tuning it jointly with \nthe projection head, potentially in a lightweight adapter-style manner, could improve fluency, \nspecificity, and style alignment with behavioral language. \n4.​ Interactive downstream tasks: Beyond free-form generation, MotionTeller could be extended to \nsupport interactive questions and answering on actigraphy sequences, such as answering queries \nlike “Was there a period of restlessness during sleep?” or “At what point did the participant’s \nactivity peak?” This would allow richer clinical or user-facing applications. \n5.​ Multimodal integration: Incorporating additional data sources (such as sleep diaries, mood logs, \nor environmental context) could help bridge gaps between observed behavior and subjective \nexperience, strengthening both personalization and real-world relevance. \n \nAs wearable data becomes more ubiquitous and LLMs continue to evolve, MotionTeller offers a flexible, \ninterpretable, and scalable foundation for translating behavioral signals into meaningful, human-centered \n24 \n"}, {"page": 25, "text": " \nlanguage. Alongside this architectural contribution, this work introduces the MotionTeller Dataset - a \nnovel, structured corpus of actigraphy-aligned behavioral summaries - which enables training and \nevaluation at scale. Through rigorous quantitative benchmarks and carefully curated qualitative analysis, \nMotionTeller demonstrates its ability to generalize across diverse participants, behaviors, and cluster \ntypes, establishing a new standard for text generation from time-series data. \n \nMore broadly, this work illustrates the untapped potential of generative foundation models in health and \nbehavioral modeling. By bridging the gap between raw physiological signals and expressive language, \nMotionTeller paves the way for future systems that can explain, interact with, and even anticipate human \nbehaviors across clinical, wellness, and daily-life contexts. It represents an important step toward the \nintegration of LLMs into personalized, data-driven health technology, which is a frontier at the \nintersection of computational methods, behavioral science, and human-centered care. \n \n25 \n"}, {"page": 26, "text": " \nReferences \n \nAlayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, \net al. 2022. “Flamingo: A Visual Language Model for Few-Shot Learning.” In Proceedings of the \n36th International Conference on Neural Information Processing Systems. NIPS ’22. Red Hook, \nNY, USA: Curran Associates Inc. \nBujang, Mohamad Adam, Evi Diana Omar, Diana Hui Ping Foo, and Yoon Khee Hon. 2024. “Sample \nSize Determination for Conducting a Pilot Study to Assess Reliability of a Questionnaire.” \nRestorative Dentistry & Endodontics 49 (1): e3. https://doi.org/10.5395/rde.2024.49.e3. \nCai, Yifu, Arvind Srinivasan, Mononito Goswami, Arjun Choudhry, and Artur Dubrawski. 2024. “JoLT: \nJointly Learned Representations of Language and Time-Series for Clinical Time-Series \nInterpretation (Student Abstract).” Proceedings of the AAAI Conference on Artificial Intelligence \n38 (21): 23447–48. https://doi.org/10.1609/aaai.v38i21.30423. \nChoi, Edward, Mohammad Taha Bahadori, Joshua A. Kulas, Andy Schuetz, Walter F. Stewart, and \nJimeng Sun. 2016. “RETAIN: An Interpretable Predictive Model for Healthcare Using Reverse \nTime Attention Mechanism.” In Proceedings of the 30th International Conference on Neural \nInformation Processing Systems, 3512–20. NIPS’16. Red Hook, NY, USA: Curran Associates \nInc. \nChomistek, Andrea K., Changzheng Yuan, Charles E. Matthews, Richard P. Troiano, Heather R. Bowles, \nJennifer Rood, Junaidah B. Barnett, Walter C. Willett, Eric B. Rimm, and David R. Bassett. 2017. \n“Physical Activity Assessment with the ActiGraph GT3X and Doubly Labeled Water.” Medicine \n& \nScience \nin \nSports \n& \nExercise \n49 \n(9): \n1935–44. \nhttps://doi.org/10.1249/MSS.0000000000001299. \nDe Choudhury, Munmun, Sachin R. Pendse, and Neha Kumar. 2023. “Benefits and Harms of Large \nLanguage Models in Digital Mental Health.” arXiv. https://doi.org/10.48550/ARXIV.2311.14693. \nDorris, Hannah, Jenny Oh, and Nicholas Jacobson. 2024. “Wearable Movement Data as a Potential \nDigital Biomarker for Chronic Pain: An Investigation Using Deep Learning.” Physical Activity \nand Health 8 (1): 83–92. https://doi.org/10.5334/paah.329. \nDunn, Jessilyn, Lukasz Kidzinski, Ryan Runge, Daniel Witt, Jennifer L. Hicks, Sophia Miryam \nSchüssler-Fiorenza Rose, Xiao Li, et al. 2021. “Wearable Sensors Enable Personalized \nPredictions of Clinical Laboratory Measurements.” Nature Medicine 27 (6): 1105–12. \nhttps://doi.org/10.1038/s41591-021-01339-0. \nEvenson, Kelly R, and Fang Wen. 2015. “Performance of the ActiGraph Accelerometer Using a National \nPopulation-Based Sample of Youth and Adults.” BMC Research Notes 8 (1): 7. \nhttps://doi.org/10.1186/s13104-014-0970-2. \nGruver, Nate, Marc Finzi, Shikai Qiu, and Andrew G Wilson. 2023. “Large Language Models Are \nZero-Shot Time Series Forecasters.” In Advances in Neural Information Processing Systems, \nedited by A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, 36:19622–35. \nCurran \nAssociates, \nInc. \nhttps://proceedings.neurips.cc/paper_files/paper/2023/file/3eb7ca52e8207697361b2c0fb3926511-\nPaper-Conference.pdf. \nHeinz, Michael V., Sukanya Bhattacharya, Brianna Trudeau, Rachel Quist, Seo Ho Song, Camilla M. Lee, \nand Nicholas C. Jacobson. 2023. “Testing Domain Knowledge and Risk of Bias of a Large-Scale \nGeneral \nArtificial \nIntelligence \nModel \nin \nMental \nHealth.” \nDIGITAL \nHEALTH \n9 \n(January):20552076231170499. https://doi.org/10.1177/20552076231170499. \nHeinz, Michael V., Daniel M. Mackin, Brianna M. Trudeau, Sukanya Bhattacharya, Yinzhou Wang, Haley \nA. Banta, Abi D. Jewett, Abigail J. Salzhauer, Tess Z. Griffin, and Nicholas C. Jacobson. 2025. \n“Randomized Trial of a Generative AI Chatbot for Mental Health Treatment.” NEJM AI 2 (4). \nhttps://doi.org/10.1056/AIoa2400802. \n26 \n"}, {"page": 27, "text": " \nHeinz, Michael V., George D. Price, Franklin Ruan, Robert J. Klein, Matthew Nemesure, Aliza Lopez, \nand Nicholas C. Jacobson. 2022. “Association of Selective Serotonin Reuptake Inhibitor Use \nWith Abnormal Physical Movement Patterns as Detected Using a Piezoelectric Accelerometer \nand Deep Learning in a Nationally Representative Sample of Noninstitutionalized Persons in the \nUS.” JAMA Network Open 5 (4): e225403. https://doi.org/10.1001/jamanetworkopen.2022.5403. \nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large \nLanguage Models for Health Prediction via Wearable Sensor Data.” In Proceedings of the Fifth \nConference on Health, Inference, and Learning, edited by Tom Pollard, Edward Choi, Pankhuri \nSinghal, Michael Hughes, Elena Sizikova, Bobak Mortazavi, Irene Chen, et al., 248:522–39. \nProceedings \nof \nMachine \nLearning \nResearch. \nPMLR. \nhttps://proceedings.mlr.press/v248/kim24b.html. \nLi, Junnan, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. “BLIP-2: Bootstrapping Language-Image \nPre-Training with Frozen Image Encoders and Large Language Models.” In Proceedings of the \n40th International Conference on Machine Learning, edited by Andreas Krause, Emma Brunskill, \nKyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, 202:19730–42. \nProceedings \nof \nMachine \nLearning \nResearch. \nPMLR. \nhttps://proceedings.mlr.press/v202/li23q.html. \nLi, Junnan, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. “BLIP: Bootstrapping Language-Image \nPre-Training for Unified Vision-Language Understanding and Generation.” In Proceedings of the \n39th International Conference on Machine Learning, edited by Kamalika Chaudhuri, Stefanie \nJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, 162:12888–900. Proceedings \nof Machine Learning Research. PMLR. https://proceedings.mlr.press/v162/li22n.html. \nLi, Zekun, Shiyang Li, and Xifeng Yan. 2023. “Time Series as Images: Vision Transformer for Irregularly \nSampled Time Series.” In Advances in Neural Information Processing Systems, edited by A. Oh, \nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, 36:49187–204. Curran \nAssociates, \nInc. \nhttps://proceedings.neurips.cc/paper_files/paper/2023/file/9a17c1eb808cf012065e9db47b7ca80d-\nPaper-Conference.pdf. \nNaveed, Humza, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, \nNaveed Akhtar, Nick Barnes, and Ajmal Mian. 2024. “A Comprehensive Overview of Large \nLanguage Models.” arXiv. https://doi.org/10.48550/arXiv.2307.06435. \nNepal, Subigya, Arvind Pillai, William Campbell, Talie Massachi, Michael V. Heinz, Ashmita Kunwar, \nEunsol Soul Choi, et al. 2024. “MindScape Study: Integrating LLM and Behavioral Sensing for \nPersonalized AI-Driven Journaling Experiences.” Proceedings of the ACM on Interactive, \nMobile, Wearable and Ubiquitous Technologies 8 (4): 1–44. https://doi.org/10.1145/3699761. \nNguyen, Viet Cuong, Mohammad Taher, Dongwan Hong, Vinicius Konkolics Possobom, Vibha \nThirunellayi Gopalakrishnan, Ekta Raj, Zihang Li, et al. 2025. “Do Large Language Models \nAlign with Core Mental Health Counseling Competencies?” In Findings of the Association for \nComputational Linguistics: NAACL 2025, edited by Luis Chiruzzo, Alan Ritter, and Lu Wang, \n7488–7511. \nAlbuquerque, \nNew \nMexico: \nAssociation \nfor Computational Linguistics. \nhttps://aclanthology.org/2025.findings-naacl.418/. \n“NHANES Homepage.” 2024. https://www.cdc.gov/nchs/nhanes/index.htm. \nPatterson, Matthew R., Adonay A. S. Nunes, Dawid Gerstel, Rakesh Pilkar, Tyler Guthrie, Ali \nNeishabouri, and Christine C. Guo. 2023. “40 Years of Actigraphy in Sleep Medicine and Current \nState \nof \nthe \nArt \nAlgorithms.” \nNpj \nDigital \nMedicine \n6 \n(1): \n51. \nhttps://doi.org/10.1038/s41746-023-00802-1. \nPillai, Arvind, Dimitris Spathis, Subigya Nepal, Amanda C. Collins, Daniel M. Mackin, Michael V. \nHeinz, Tess Z. Griffin, Nicholas C. Jacobson, and Andrew Campbell. 2025. “Time2Lang: \nBridging Time-Series Foundation Models and Large Language Models for Health Sensing \nBeyond Prompting.” arXiv. https://doi.org/10.48550/arXiv.2502.07608. \nRahman, Syed Ashiqur, and Donald A. Adjeroh. 2019. “Deep Learning Using Convolutional LSTM \n27 \n"}, {"page": 28, "text": " \nEstimates Biological Age from Physical Activity.” Scientific Reports 9 (1): 11425. \nhttps://doi.org/10.1038/s41598-019-46850-0. \nRuan, Franklin, Stephen Adjei, Adaobi Amanna, George Price, Michael V. Heinz, and Nicholas C. \nJacobson. 2024. “Characterizing Benzodiazepine Use in a Large National Study via Wearables \nand Deep Learning.” PsyArXiv. https://doi.org/10.31234/osf.io/5ckme. \nRuan, Franklin Y., Aiwei Zhang, Jenny Y. Oh, SouYoung Jin, and Nicholas C. Jacobson. 2024. “AI \nFoundation Models for Wearable Movement Data in Mental Health Research.” arXiv. \nhttps://doi.org/10.48550/ARXIV.2411.15240. \nSharma, Ashish, Inna W. Lin, Adam S. Miner, David C. Atkins, and Tim Althoff. 2023. “Human–AI \nCollaboration Enables More Empathic Conversations in Text-Based Peer-to-Peer Mental Health \nSupport.” \nNature \nMachine \nIntelligence \n5 \n(1): \n46–57. \nhttps://doi.org/10.1038/s42256-022-00593-2. \nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan \nScales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): \n172–80. https://doi.org/10.1038/s41586-023-06291-2. \nXie, Qianqian, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, et al. n.d. \n“Me-LLaMA: Foundation Large Language Models for Medical Applications.” PhysioNet. \nAccessed June 2, 2025. https://doi.org/10.13026/WWFD-2T39. \nYoo, Jae-Young, Seyong Oh, Wissam Shalish, Woo-Youl Maeng, Emily Cerier, Emily Jeanne, \nMyung-Kun Chung, et al. 2023. “Wireless Broadband Acousto-Mechanical Sensing System for \nContinuous \nPhysiological \nMonitoring.” \nNature \nMedicine \n29 \n(12): \n3137–48. \nhttps://doi.org/10.1038/s41591-023-02637-5. \n \n \n \n \n \n \n \n \n \n28 \n"}, {"page": 29, "text": " \nAppendix \nAppendix A: Label Generation And Evaluation \nAppendix A.1: Label Generation Prompt Template \nFor generating MotionTeller Dataset labels, the following template was used given a sequence of 24 \nhourly data, aggravated from the original sequence of 1,440 raw values. \n \nFor this participant, we have 24 hours of hourly activity data, recorded as integer values. \nEach number represents the movement level during that specific hour of the day. \n \n### **Participant-Specific Data:**  \nThe participant’s activity data begins in the first hour of the day, from midnight to 1AM.  \n●​ From 0000 to 0100, the activity level is {activity level 1}.  \n●​ From 0100 to 0200, the activity level is {activity level 2}.  \n●​ From 0200 to 0300, the activity level is {activity level 3}. \n●​ From 0300 to 0400, the activity level is {activity level 4}. \n●​ From 0400 to 0500, the activity level is {activity level 5}. \n●​ From 0500 to 0600, the activity level is {activity level 6}. \n●​ From 0600 to 0700, the activity level is {activity level 7}.  \n●​ From 0700 to 0800, the activity level is {activity level 8}.  \n●​ From 0800 to 0900, the activity level is {activity level 9}.  \n●​ From 0900 to 1000, the activity level is {activity level 10}.  \n●​ From 1000 to 1100, the activity level is {activity level 11}.  \n●​ From 1100 to 1200, the activity level is {activity level 12}.  \n●​ From 1200 to 1300, the activity level is {activity level 13}.  \n●​ From 1300 to 1400, the activity level is {activity level 14}.  \n●​ From 1400 to 1500, the activity level is {activity level 15}.  \n●​ From 1500 to 1600, the activity level is {activity level 16}.  \n●​ From 1600 to 1700, the activity level is {activity level 17}.  \n●​ From 1700 to 1800, the activity level is {activity level 18}.  \n●​ From 1800 to 1900, the activity level is {activity level 19}.  \n●​ From 1900 to 2000, the activity level is {activity level 20}.  \n●​ From 2000 to 2100, the activity level is {activity level 21}.  \n●​ From 2100 to 2200, the activity level is {activity level 22}.  \n●​ From 2200 to 2300, the activity level is {activity level 23}.  \n●​ From 2300 to 2400 which is 0000 of the next day, the activity level is {activity level 24}.  \n \n### **Global Context Awareness:**  \n●​ Across all participants within this participant’s cohort, activity levels are scaled between 0 and \n1000.  \n●​ This provides a reference range to interpret activity levels, but trends should still be analyzed \nbased on this participant’s specific data.  \n●​ If a participant’s activity levels are all within a narrow range (for example, 0 to 20), describe \nthis as low movement rather than assuming a buildup. \n \nI would like you to offer an analysis on this participant’s activity level throughout the 24 hour period. \nTo ensure accuracy, use these steps to assist your reasoning process and provide a well-ordered, \n29 \n"}, {"page": 30, "text": " \nconsistent response: \n1.​ First, count the number of 0s in the participants’ activity data. If there are more than 16 activity \nlevels that are 0, it is likely that the participant has misused their tracking device, or have \nforgotten to put on the device.  \n2.​ Next, start with a chronological breakdown of activity \na.​ From the night to dawn (0000 to 0600) \nb.​ Early morning to noon (0600 - 1200) \nc.​ Noon through the entire afternoon (1200 - 1800) \nd.​ Early evening to midnight (1800 - 0000) \n3.​ For each time period above, describe the general activity pattern by splitting the day into \nlogical time periods. Keep descriptions qualitative and natural, without specifying exact hours \nor exact activity values \na.​ Describe if the participant is mostly inactive, suggesting rest or sleep \nb.​ If activity is present, note whether it is sporadic or continuous. \nc.​ Mention when activity begins and whether it gradually increases or remains low. \nd.​ Describe the pattern of engagement, such as whether movement appears consistent or \nirregular. \ne.​ If movement increases, note whether the rise is steady or abrupt. \nf.​\nIf the activity is sustained, suggest a period of prolonged engagement. \n4.​ Identify peak activity \na.​ Identify when the highest level of movement occurs (without mentioning exact times). \nNote the general time block where engagement is most consistent. \n5.​ Identify notable rest periods \na.​ Highlight times of minimal or no activity, linking them to reasonable rest periods. \nb.​ If inactivity aligns with expected rest periods, acknowledge it. \n6.​ Final summary statement \na.​ Summarize the overall trend and what it suggests about the participant’s routine. \nb.​ Avoid assumptions about specific activities. \n \nSome additional rules to follow throughout generating the response: \n-​\nUse descriptive, narrative language, without formatting or lists \n-​\nAvoid exact values and time references \n-​\nAvoid jargon and technical language \n-​\nDo not make biased or overly strong statements \n \n \nAppendix A.2: Label Quality Evaluation Metrics \nThe following set of evaluation metrics are used in determining the quality of MotionTeller actigraphy \ndescription label output. \n \nCategory 1: Identification of peak activity \nDoes the label correctly identify the period of highest movement? \n \nScore \nDescription \n5 \nPerfect identification of the highest peak, correct time frame, and no mislabeling. \n4 \nIdentifies a local peak or the second-highest peak correctly, even if the main peak is missed; \n30 \n"}, {"page": 31, "text": " \nOR \nPeak identification is very close to the actual time, or some inconsistency shown; e.g., peak \nactivity observed at 9 am, but mentioned both “early morning” or “mid morning” \n3 \nMentions the peak but lacks emphasis OR is ambiguous about its significance. \n2 \nMisidentifies a different time period as the peak but still acknowledges fluctuations in \nactivity. \n1 \nCompletely incorrect identification of peak OR no mention of peak activity at all. \n \nCategory 2: Description of nighttime to dawn (0000 - 0600) \nDoes the label accurately capture the trends in activity levels during this early period? \n \nScore \nDescription \n5 \nClearly identifies the period, properly describes trends such as inactivity for example, and \navoids unnecessary speculation \n4 \nMostly accurate, but slightly vague or missing minor details. \n3 \nOverall trends, such as inactivity is acknowledged, but lacks depth OR contain minor \ninaccuracies \n2 \nA mixture of right and wrong identifications of activity levels. \n1 \nCompletely wrong description OR no mention of this period. \n \nCategory 3: Description of early morning to noon (0600 - 1200) \nDoes the label capture the gradual transition into wakefulness and engagement? \n \nScore \nDescription \n5 \nDescribes morning trends correctly, identifying gradual wakefulness or periods of movement \nappropriately. \n4 \nMostly accurate, but with minor wording or emphasis issues. \n3 \nThe description is somewhat vague OR lacks depth. \n2 \nMisinterpretation of the morning pattern, such as exaggerating activity. \n1 \nCompletely incorrect description OR missing this section entirely. \n \n \n \n \n \n31 \n"}, {"page": 32, "text": " \nCategory 4: Description of afternoon activity (1200-1800) \nDoes the label describe engagement levels accurately? \n \nScore \nDescription \n5 \nCaptures the afternoon’s structure well, accurately highlighting patterns of movement and \nactivity levels. \n4 \nMostly accurate, but with minor wording or emphasis issues. \n3 \nThe description is somewhat vague OR lacks depth. \n2 \nMisinterpretation of the morning pattern, such as exaggerating activity. \n1 \nCompletely incorrect description OR missing this section entirely. \n \nCategory 5: Description of evening activity and end-of-day description (1800 - 0000) \nDoes the label describe how activity patterns are during the night? \n \nScore \nDescription \n5 \nClearly describes how activity levels are during the night, avoiding unnecessary \nassumptions. \n4 \nMostly accurate, but with minor wording or emphasis issues. \n3 \nThe description is somewhat vague OR lacks depth. \n2 \nMisinterpretation of the morning pattern, such as exaggerating activity. \n1 \nCompletely incorrect description OR missing this section entirely. \n \nCategory 6: Quality of language & bias avoidance \nIs the label written with clear, neutral, and descriptive language? \n \nScore \nDescription \n5 \nThe language is neutral, avoids bias, and maintains high clarity. \n4 \nMostly neutral and clear, but with some minor wording improvements needed. \n3 \nSomewhat ambiguous wording or minor issues with neutrality. \n2 \nClear biases, unnecessary assumptions, or poor phrasing. \n1 \nContains misleading, incorrect, or highly biased language. \n32 \n"}, {"page": 33, "text": " \n \n \nAppendix B: Understanding MIMS Units and Movement Intensity Thresholds \nThe National Health and Nutrition Examination Survey (NHANES) has established specific thresholds \nfor classifying physical activity intensity levels using ActiGraph accelerometers. These thresholds are \nbased on counts per minute (cpm) and are widely utilized in research to categorize activity levels. \nNHANES ActiGraph cut points are generally defined as follows (Evenson and Wen 2015): \n●​ Sedentary: <100 cpm \n●​ Light Physical Activity: 100–2,019 cpm \n●​ Moderate Physical Activity: 2,020–5,998 cpm \n●​ Vigorous Physical Activity: ≥5,999 cpm \n \nIt is important to note that these cut points are based on vertical axis counts. With the advent of triaxial \naccelerometers, alternative thresholds have been proposed. For instance, the following triaxial thresholds \nwere suggested in a study (Chomistek et al. 2017): \n●​ Light Physical Activity: 200 to 2,689 cpm \n●​ Moderate Physical Activity: 2,690 to 6,166 cpm \n●​ Vigorous Physical Activity: ≥ 6,167 cpm \n \nWhen interpreting MIMS (Monitor-Independent Movement Summary) units, one must recognize that \nthey are derived differently from traditional ActiGraph counts. While some studies have attempted to \nequate MIMS units with ActiGraph counts, direct comparisons should be made cautiously. When working \nwith MIMS data, researchers should consider the differences in measurement and apply appropriate \nmethods to interpret activity levels accurately. \n \n \n33 \n"}]}