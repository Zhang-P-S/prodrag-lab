{"doc_id": "arxiv:2601.01260", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.01260.pdf", "meta": {"doc_id": "arxiv:2601.01260", "source": "arxiv", "arxiv_id": "2601.01260", "title": "MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance", "authors": ["Hamad Khan", "Saddam Hussain Khan"], "published": "2026-01-03T19:01:33Z", "updated": "2026-01-03T19:01:33Z", "summary": "The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.01260v1", "url_pdf": "https://arxiv.org/pdf/2601.01260.pdf", "meta_path": "data/raw/arxiv/meta/2601.01260.json", "sha256": "8a4f82c5e6187548e92c4c9187ca36fea4138155579254a07f2c738079432096", "status": "ok", "fetched_at": "2026-02-18T02:23:23.954527+00:00"}, "pages": [{"page": 1, "text": "MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance \nHamad Khan1, Saddam Hussain Khan1* \n1Artificial Intelligence Lab, Department of Computer Systems Engineering, University of Engineering and Applied Sciences (UEAS), Swat 19060, Pakistan \nEmail: hengrshkhan822@gmail.com  \n \nABSTRACT \nThe deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational \ncost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework \nfor efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs \ntoken-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, \nhigh-throughput sequences. The customized EMamba and ET5 models are adapted to input sequence dimensionality, embedding structure, sequence \nlength, and target-specific output heads, and fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent \nrouting decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby \nenforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly \noptimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert \nactivation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA \ndatasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 \ns), delivering a 24.4√ó speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment. \n \nKeywords: MoE, Routing, SSM, Mamba, Transformer, NLP,  Utility-Guided Optimization \n \n \n \n"}, {"page": 2, "text": "1. Introduction \nNatural language processing has shown significant advancements due to the rise of large language models, which improve performance through large-\nscale pretraining [1]. NLP effectiveness in complex reasoning remains constrained by limited grounding and high computational cost [2]. This also \nhampers continual knowledge updates and raises sustainability concerns [3], [4].  NLP plays an increasingly important role in clinical applications such \nas risk assessment, therapeutic support, and disease prediction, despite these challenges [5], [6]. However, NLP faces particular difficulties in the \ntherapeutic sector, such as the intricacy of medical language, as well as in computational resources. \nVarious preeminent language models have been introduced. Traditional models like Transformer [7] were introduced, but the transformer uses a self-\nattention mechanism using a formula O(N2), where N is the sequence length. Though this model is satisfactory in terms of accuracy, it lacks in terms \nof memory and speed, resulting in high computational resources. This was tested by detailed clinical reports [8]  and similarly hinders its deployment \nin other data-heavy domains like medical image segmentation [9]. In real-time applications and hospitals, time is a very important factor, and due to \nthe above situation, for example, a system may take 1.8 seconds to run 1000 words of clinical information, which is too slow, and for hospitals and \nclinics, quick processing time is important. There is a need for a system with a quick processing time, and it has to keep the privacy of all patients' \ninformation locally and globally. Other transformer models like BioBERT [10], and ClinicalBERT [5] also succeeded on biomedical tasks with a BERT \nscore of 90% but this results in high costs. \nIn addition to diminishing this constraint, models like LongFormer [11] and BigBird [12] used sparse attention, which reduces intricacy to O(N)using \nattention patterns. Kernel-based methods, such as Performer [13], were used, which leads to achieving efficiency gains better than the transformer. \nHowever, when tested on long medical notes, it shows an accuracy drop of 5 to 10%, which is not suitable for use in a clinical environment. More \nrecently, State Space Models (SSMs) have been recognized, particularly the Mamba architecture [14], Very recent developments (2024) have \nintroduced advanced SSM variants like StripedHyena [15] and MambaByte [16], which further optimize selective scanning mechanisms. Concurrently, \nhybrid architectures such as MoE-Mamba [17] and Block-State Transformers [18] explore different fusion strategies between attention and recurrence, \nthough they typically employ fixed or block-level routing rather than token-level adaptation. as depending on the Selective state space Model (SSM) \nas a better alternative due to their linear scaling and longer dependency modeling capabilities, as shown in Equation (1), where L denotes the input \nsequence length. Mamba analyzes lengthy sequences with great efficiency and at a reduced cost of training and inference [19]. However, it exhibits \ninconsistent related learning and dual-task generalization abilities, and it also struggles to capture locally important information in text. Similarly, \nTransformer shows favourable results in accuracy, but in short sequences of length, it demands high computational resources and also costs [7]. This \n"}, {"page": 3, "text": "creates a major conflict in clinical applications where both accuracy and speed are essential. Yet there again rises a problem for medical doctors, either \nto use a transformer for accuracy, but will face computational cost, or will lose performance by sticking with SSM. \nThe Pareto trade-off has not been fully solved and has been struggled with by any of the individual or even hybrid models. Hybrid models like Jamba \nand MoE [20], [21], which are an integration of Transformers and SSM, show a positive sign in this trade-off. However, even the most recent hybrid \napproaches continue to struggle with adaptive routing, relying on predetermined architectural patterns rather than context-aware token-level decisions. \nThis gap persists despite advances in efficient LLMs like Mistral 7B  and Phi-3 [22], [23], which demonstrate the practical viability of MoE designs \nbut lack fine-grained routing mechanisms suitable for clinical text. However, their design is based on a fixed routing and does not contain any smart \nrules or penalties for the transformer or SSM to show the computational reports. Even most advanced models like Mamba-3 [24] and RetNet-MoE [25] \ncontinue to use predetermined routing patterns rather than context-aware token-level decisions, leaving a gap that our MambaFormer addresses. In \naddition, they have any information on which expert will be used for accuracy and speed, which overcomes all aforementioned limitations and provides \na complete Model that is capable of efficiency and cost. Therefore, a smarter routing mechanism is needed to dynamically balance accuracy and \nefficiency. In this regard, we have introduced Proposed MambaFormer, a utility-guided dynamic routing framework for Pareto-optimal efficiency in \nBioNLP. Our novel approach makes the following contributions: \n‚Ä¢ Our novel hybrid Mixture-of-Experts (MoE) framework, ‚ÄòMambaFormer‚Äô, is the integration of customized SSM Mamba (EMamba) and \nTransformer (ET5) that employs a new guided gating mechanism to dynamically route each incoming token for clinical assistance. Moreover, \nMambaFormer introduced a new multiobjective function (LMambaFormer) that automatically optimizes the adaptive routing parameters, which \nsystematically assign tokens to the optimal experts.  \n‚Ä¢ The customized SSM Mamba (EMamba) and Transformer (ET5) input layers are modified according to input token dimensionality, embedding \nstructure, and length, and output layers are added to target-specific QA and description. Moreover, these models are fine-tuned through transfer \nlearning on a new custom-designed DentalQA comprising 5,000 QA pairs (10,000 tokens) and the PubMedQA dataset.  \n‚Ä¢ The MambaFormer systematically integrates token features with a variational sequence to either a ET5(complex short contents, high-accuracy \ncases) or a highly-efficient EMamba expert (long sequences, high-throughput cases) through a proposed intelligent router. Moreover, the steering-\nguided router learns domain-aware and sequence-length features to dynamically direct tokens to the optimal expert based on a Pareto-optimal \ntrade-off between accuracy and latency. As a result, 96.2% of tokens are provided to the fast and efficient EMamba expert, while only 3.8% of \ntokens go to the high accuracy ET5 expert existing dataset diversity.  \n"}, {"page": 4, "text": "‚Ä¢ The proposed MambaFormer framework successfully surpasses traditional Models in speed (response time) by gaining 0.077 seconds (a 24.4x \nspeedup over traditional models) while maintaining BERTScore F1 of 0.9180 and accuracy of 96.2%. \nThe rest of the manuscript is organized as follows: Section 2 presents an overview of previous studies, and Section 3 outlines the methodology. Section \n4 describes the experimental setup, Section 5 presents the results, and Section 6 concludes the study. \n2. Related Work \nNumerous studies have examined the intricacy of efficient biological NLP, including Transformer models and other complicated hybrid deep learning \nmodels. Current NLP systems are based on transformer scaling, which was first introduced by Scaled Multiplicative-attention Vaswani et al. [7], and \nmade it possible to record global dependencies without repetition. Following this, BERT [26] was used, which demonstrated the effectiveness of \nbidirectional pre-training by achieving 90% F1 scores on the named entity recognition baseline. Text-to-text challenges were then used to represent \nNLP tasks using the T5 framework [27], and the GPT series demonstrated that feature scaling was significant in relation to data and model size [28]. \nHowever, the underlying self-attention has a quadratic computational complexity O(N2), which severely limits its applicability in biological applications \nbecause of the huge clinical records. It has been shown that processing 1,000-token clinical notes can take up to 200MB of memory and 1.8 seconds, \nmaking real-time deployment impractical [5]. This scalability issue is not unique to NLP; the vision transformer (ViT) architecture faces analogous \nchallenges when processing high-resolution medical images, as detailed in recent comprehensive surveys [29]. Despite strong accuracy, Transformers \nremain challenging for resource-constrained clinical deployment due to quadratic attention complexity.  \nSparse and kernelized models such as LongFormer  [11] and BigBird [12]achieve 85‚Äì90% accuracy on long biomedical texts, while Performer [13] \nattains 88% F1 and Linformer [30] reduces attention via low-rank projections. Nevertheless, these methods incur a 5‚Äì10% accuracy drop on complex \nbiomedical tasks [31] and inadequately model fine-grained clinical patterns, including brain tumor MRI analysis [32]. Recent efficient sequence models, \nincluding StripedHyena [15] and MambaByte [16], improve scalability through SSMs and byte-level processing. Hybrid designs such as MoE-Mamba \n[17] achieve a BERTScore of 0.902, whereas Block-State Transformers [18] employ fixed computation schemes. Efficient LLMs (e.g., Mistral 7B [22] \nand Phi-3 [23]) reduce cost via MoE architectures but lack fine-grained token adaptivity for clinical deployment. However, these approaches either \nmaintain fixed routing patterns or operate at coarse granularity (layer or block level), missing the opportunity for fine-grained, token-level adaptation \nthat is crucial for clinical text with mixed complexity. \nThis drop in performance reveals a clear trade-off between speed and accuracy, which is unacceptable for critical clinical tasks. Therefore, a system is \nneeded that can smartly use an accurate expert, especially in biological activities; more recent hybrid attention-SSMs show promise [33]. As an \nalternative to sequence models with linear scaling at their core, SSMs are offered. The prospective outcomes of modelling long-range dependence with \n"}, {"page": 5, "text": "87% accuracy on classification tests were provided by the S4 [34]architecture, with near-linear scalability O(L), Mamba was able to provide hardware-\naware optimizations and selective scanning due to the scalability of 85 percent F1 at biomedical retrieval. Mamba-2[14]  significantly improved this \napproach with 2-8x bit speed and 92% accuracy, suggesting that hybrid systems may be quite feasible. Building on this, 2025 saw further SSM \nrefinements such as Mamba-3 [24] with hierarchical selective scanning and HyenaDNA 2.0 [35] for genomic resolution, though both retain fixed \narchitectural patterns unsuitable for token-level clinical adaptation. \nHowever, even though Mamba works very well with long sequences because of its linear time complexity O(L), it sometimes has trouble understanding \ncomplex and detailed information in short technical texts. In those cases, the self-attention mechanism performs better [7]. They are unable to perform \nas well on complicated clinical notes as Transformers, which leads to complementary capabilities that point to hybrid solutions. A hybrid approach in \nNLP has shown promising outcomes across a range of domains. SwitchTransformer and Glam [36], [37], known as Mixer of Expert (MOE), employ \n90% F1 on question-answering tasks, implementing sparse activation to offer speed-accuracy trade-offs. Routing transformers [38] precisely 91% \ntoken-level interaction, but they are specific to transformer frameworks and are usually designed to balance the workload evenly instead of focusing \non how efficiently the computation is done. Even 2025 hybrid architectures like RetNet-MoE [39] and StripedHyena-2 [15] continue to employ coarse-\ngrained or fixed routing, overlooking the need for per-token utility-guided decisions in clinical texts with mixed complexity. \nBiomedical NLP has unique hurdles from lengthy medical records, research papers, and advanced terminologies. While PubMedBERT [33] has a 91% \nF1 on question-answering, the BioBERT [10] Model, which was trained on biomedical corpora, has a 90% F1 on named entity recognition. \nClinicalBERT [5] estimates deaths with an 88 percent AUC, showing high clinical forecasting ability. However, the substantial computational load \n(1.9s delay) of these models makes them unsuitable for clinical usage. BioMamba [40] shows promise with a 95% accuracy in long contexts, but it \nfalls short on brief clinical texts. Real-time clinics demand an efficient approach that can easily adapt to different types of text, whether it is dense, \nshort, or long. This demand persists in the 2025 landscape, where recent surveys highlight adaptive routing and deployment feasibility as still-open \nchallenges gaps our MambaFormer directly addresses. According to this review, there are the following shortcomings in the present biomedical NLP \npractice: \n‚Ä¢ Existing hybrid models rely on fixed routing and cannot dynamically balance inference speed and accuracy, limiting real-time clinical usability. \n‚Ä¢ Current approaches fail to handle highly skewed computational demands and lack adaptive expert selection for different input types. \n‚Ä¢ Most studies fail to evaluate deployment feasibility under real clinical conditions, including latency and robustness of the model. \n \n"}, {"page": 6, "text": "Table 1: Comparison of Biomedical NLP existing studies. \nModel \nArchitecture \nKey Contribution \nMetric \nSpeed (s) Memory (MB) Primary Limitation \nStripedHyena [15] \nSSM-Conv \nHybrid SSM \n0.895 F1 \n0.110 \n155 \nFixed pattern \nMoE-Mamba [17] \nMoE+SSM \nSparse routing \n0.902 BERT \n0.095 \n165 \nBlock routing \nMistral 7B [22] \nMoE-Trans \nEfficient MoE \n0.910 MMLU \n0.350 \n14000 \nHigh memory \nBioBERT [10] \nTransformer \nBiomedical pretrain 0.900 NER \n1.900 \n195 \nHigh latency \nLongformer [11] \nSparse Trans \nLong context \n0.880 F1 \n0.450 \n180 \nWeak local \nMamba [14] \nSSM \nLinear scan \n0.850 F1 \n0.100 \n142 \nShort-context \nMamba-2 [14] \nSSM \nHW-optimized \n0.870 F1/AUC \n0.085 \n138 \nLimited local \nJamba [20] \nHybrid MoE \nSSM+Trans \n0.890 F1 \n0.450 \n210 \nFixed routing \nBioMamba [40] \nSSM (Bio) \nDomain SSM \n0.880 F1/AUC \n0.092 \n145 \nShort-context \nCE-RS-SBCIT[32] \nCNN-Trans \nMedical vision \n~0.95 Dice \nN/A \nN/A \nVision-only \nDrilling-Hybrid[41] Trans-LSTM \nTime-series \n~0.92 R¬≤ \nN/A \nN/A \nTask-specific \n3. Methodology \nThe proposed MambaFormer is designed to balance accuracy and efficiency in medical question‚Äìanswering (QA) on the custom deisnged DentalQA \nand PubMedQA datasets. MambaFormer integrates contextual preprocessing, token-level adaptive expert routing, and joint optimization of a hybrid \nMoE architecture to enable accurate, low-latency inference. The proposed Mambaformer framework employs BERT-based tokenization to encode \ninputs from the PubMedQA and custom-designed DentalQA datasets into context-preserving medical representations. A utility-guided dynamic router \nthen assigns semantically informed sequential embedded each token to either a lightweight EMamba module for low-complexity processing or a \nTransformer-based module for high-precision reasoning. This routing strategy explicitly optimizes the latency‚Äìaccuracy trade-off, enabling efficient \ninference under constrained computational budgets. MoE; EMamba and ET5  backbones are customized with task-specific adaptation layers and TL-based \ntuned on medical data due to the scarcity and high annotation, and enhance domain knowledge acquisition from limited samples. The complete inference \npipeline of the proposed MambaFormer architecture is illustrated in Figure 1. \n3.1 Tokenization and Feature Engineering \nThe PubMedQA and newly designed DentalQA datasets exhibit token distribution, with long biomedical sequences (~96.2%) dominating shorter \nclinical QA pairs (~3.8%), highlighting real-world clinical NLP scenarios. It also motivates the development of an adaptive technique for both concise, \naccuracy-critical queries and lengthy, throughput-sensitive documents. All text is standardized to normalize domain-specific terminology, \nabbreviations, and heterogeneous sentence sequences prior to tokenization. Input sequences are tokenized and truncated for uniform batching using a \nBERT-base tokenizer. Subword clinical contexts are preserved within a 512-token limit, balancing computational efficiency with retention of medically \nrelevant information for downstream routing. \n"}, {"page": 7, "text": "DATA SOURCES\nDentalQA \nPubMedQA\nDATA \nAUGMENTATION\nBERT-based \nParaphrasing\nTOKENIZATION\nBERT-base \ntokenizer\nDATA \nPARTITIONING\nTraining data 80%\nTest 10 %\nVal 10%\nE_Mamba \n(96.2%)\nO(N) \ncomplexity\nE_T5 (3.8%)\nO(N¬≤) \ncomplexity\nOutput Generator\nACCURACY \nMETRICS\nBERTScore F1\nROUGE-L\nPerplexity\nEFFICIENCY \nMETRICS\nInference Latency\nThroughput (seq/sec)\nMemory Usage\nEND\nEvalution\nStart\nE_T5 Expert\n(Accuracy Specialist)\nT5 Large\nDental adopted \nLoRA Fine-tuned\n Frozen E_T5 \n \nE_Mamba Expert\n(Efficiency)\nMamba-130M\nTask Adaptation\nClinical tuned\nLora optimized \n Frozen E_Mamba \n \n \n \n \n \n \n \n \nMAMBAFORME\nR CORE\nInput Sequence\nToken Embeddings\nContextual \nRepresentation (x·µ¢)\nFEATURE \nENGINEERING\nl = norm length\nd = domain\nCore architecture\nINTELLIGENT ROUTER\n2-layer MLP\n1.3K params\nSoftmax gating\nHard selection\nRoute?\nTRAINING \nPHASE\n(Router \nParameters \nOnly)\nGround Truth \nAnswers\nModel Predictions\nMULTI-\nOBJECTIVE LOSS\nL = L_CE + \nŒª‚ÇÅL_Bal + Œª‚ÇÇL_Pen\nOptimizer: Adam \n(LR=0.001)\nBackpropagation\nEVALUATION \nPHASE\nHoldout Testing\nZERO-SHOT \nEVALUATION\nPubMedQA \nDataset\nModel inference\nTrain 80%\nTest 10%\nDetailed  Work flow \n \nFigure 1. Complete MambaFormer Framework Pipeline. \n3.2 Data Preprocessing  \nBERT-based paraphrasing expands 5,000 QA pairs to 13,000 to address the limited scale of DentalQA with samples filtered via a Sentence-BERT \nsimilarity threshold (<0.95) and preserves semantic fidelity. The augmented DentalQA is split into Train/Valid/Test (80/10/10%). While PubMedQA \nis utilized for zero-shot evaluation to improve generalization on long test biomedical data. \n"}, {"page": 8, "text": "3.3 Hybrid Design: Expert Model Selection and Characterization \n The proposed MambaFormer framework uses a token-level MoE, combining a high-accuracy TL-based fine-tuned Transformer (T5-Large) [27] with \na high-efficiency SSM (Mamba-130M) [14], adaptively routing tokens based on sequence length, domain, and content to balance precision and \nefficiency. Conversely, Mamba-130M delivered exceptional inference speed (0.055 seconds per sequence on PubMedQA), confirming their \ncomplementary roles as accuracy and efficiency experts, respectively. The process of adapting the pretrained T5 and Mamba models into domain-\nspecialized, frozen experts for MambaFormer through dental vocabulary expansion and Low-Rank Adaptation (LoRA)[42] fine-tuning on the \nDentalQA dataset is described in Figure 2. \nùê∂T5(ùêø) = ùëÇ(ùêø2), \nùê∂Mamba(ùêø) = ùëÇ(ùêø) \n(1) \nùëä‚Ä≤ = ùëä+\nŒ±\nùëüùêµùê¥ \n                                       (2) \nùëíùëñ\n‚Ä≤ = ùëíùëñ+ ùëÉùëñ+ ùëäùëë‚ãÖùëëùë† \n                           (3) \n \nEquation (1) formalizes their computational complexity: ET5 scales quadratically while EMamba scales linearly with sequence length. Equation (2) defines \nLow-Rank Adaptation (LoRA)[42] for efficient fine-tuning. Equation (3) adapts token embeddings with domain-specific knowledge. These equations \nprovide the basis for the customization process described next. \n3.3.1 Customization of ET5 (T5-Large) \nWe customized T5-Large [27] into a biomedical accuracy expert by preserving its Transformer architecture while adapting it for clinical QA. Medical \nvocabulary was added using Equation (3), and the final layer was replaced with a QA-specific head. Efficient fine-tuning was performed using LoRA \n[42] (Equation 2) to learn clinical patterns without catastrophic forgetting. \nThe mathematical operations defining ET5 are \n‚Ñéùëô\nT5 = LayerNorm! (‚Ñéùëô‚àí1 + MultiHead(‚Ñéùëô‚àí1)) \n  (4) \n‚Ñéùëô\nT5 = LayerNorm! (‚Ñéùëô\nT5 + FFN(‚Ñéùëô\nT5)) \n  \n‚Ñí‚Ñ∞ùíØ5 = ‚Ñíùíû‚Ñ∞(ùë¶ùëùùëüùëíùëë, ùë¶ùë°ùëüùë¢ùëí) + Œª‚Ñí‚Ñí‚Ñ≥(ùëã) \n                    (5) \n \n \n \n"}, {"page": 9, "text": "Domain adoption \nSelf-attention\nCore Attention Mechanism:\nAttention(Q, K, V) = \nsoftmax(QK·µÄ /  d‚Çñ) V\nComplexity: O(N¬≤)\nCore SSM Mechanics:\nState: h‚Çú = A h‚Çú ‚ÇÅ + B x‚Çú\nOutput: y‚Çú = C h‚Çú\nComplexity: O(N)\nDental Vocabulary\n          specialized tokens\n     Medical abbreviations\nTask-Specific Heads\n     Biomedical QA output\n     Clinical answer \nformatting\nBiomedical Context\n     Clinical embeddings\n     Entity recognition\nMODIFICATIONS:\n1. Vocabulary \nExpansion\n          dental \ntokens\n     Medical \nterminology\nMasked attention\nEmbedding layer \nEmbedding layer \nVocab:32,128 tokens\nFeed-Forward Network\nSelf-attention\nReLU activation\nLayer Norm\nResidual Connections\nCross attention\nMasked attention\nOutput general text\nInput: Dental Q/A\nID Conv\nSelective SSM\nState Matrix A\nInput Matrix B\nOutput Matrix C\nSiLU Activation\nOutput general text\nInput: Dental Q/A\nDataset: DentalQA Method: Low-Rank \nAdaptation- Rank r = 8 Target: Q,V \n(T5) / SSM (Mamba)Epochs: 50 | Batch: \n32Optimizer: AdamW (LR=3e-4)\n2. LoRA Fine-\ntuning\n     Applied to Q,V \nmatrices\n     Rank r = 8\n     Only 0.1% \nparams updated\n3. Output Head\n     Biomedical QA \ngenerator\n     Clinical \nformatting\nSTATUS: \nFROZEN \n  For MambaFormer \nintegration\n       token \nallocation\n E_T5\n( Accuracy Expert)\n E_Mamba\n( Accuracy Expert)\nMODIFICATIONS:\n1. SSM Parameter \nTuning\n     LoRA on A,B,C \nmatrices\n     Rank r = 8\n     Medical context \nadaptation\n2. Selective \nScanning\n     Clinical entity \nfocus\n     Biomedical \nsequence opt.\n3. Output Projection\n     Dental QA \nformatting\n     Answer \ngeneration\nSTATUS: \nFROZEN \n  For MambaFormer \nintegration\n        token \nallocation\nMambaFormer Framework\nT5-Large Architecture \n(Transformer Encoder-Decoder)O(N¬≤) \nComplexity\nMamba-130M Architecture\n(State Space Model - SSM)\n O(N) Linear Complexity\n \nFigure 2: Expert Customization via Parameter-Efficient Transfer Learning. \n \n"}, {"page": 10, "text": "In the above, Equation (4) describes the Transformer layer update that gives ET5 its strong contextual understanding. Equation (5) defines its training \nobjective, combining cross-entropy loss for QA accuracy with a language modeling term to preserve general knowledge. After customization, ET5 was \nfrozen and integrated as the accuracy expert. \n3.3.2 Customization of EMamba (Mamba-130M) \nWe adapted Mamba-130M [14] into a high-efficiency expert by increasing its state dimension for better long-sequence modeling while maintaining its \nlinear-time recurrence. Vocabulary alignment followed Equation (3), and selective LoRA[42] fine-tuning (Equation 2) was applied only to input/output \nprojections to preserve core SSM efficiency. \n‚Ñéùë°= ùê¥ÃÖ‚Ñéùë°‚àí1 + ùêµÃÖùë•ùë°, \nùë¶ùë°= ùê∂ÃÖ‚Ñéùë° \n \n(6) \n‚Ñí‚Ñ∞‚Ñ≥ùí∂ùìÇùí∑ùí∂= ‚Ñíùíû‚Ñ∞(ùë¶ùëùùëüùëíùëë, ùë¶ùë°ùëüùë¢ùëí) + Œ≤||ùê¥ÃÖ ‚àíùêº||ùêπ\n2 (7) \nEquation (6) gives the SSM recurrence that enables EMamba linear computational complexity (as formalized in Equation 1). Equation (7) defines its \ntraining loss, combining cross-entropy with a stability regularization term that prevents the SSM parameters from diverging during fine-tuning. After \nadaptation, EMamba was frozen and integrated as the efficiency expert. \nTable 2: Empirical Evaluation for Expert Model Selection \nModel \nParameters DentalQA F1 DentalQA Perplexity \nPubMedQA Speed (s) PubMedQA  F1 \nSelection Rationale \nGPT2-Small[43] \n124M \n0.9129 \n2.91 \n0.716 \n0.808 \nGood Dental \nGPT2-[43]Medium \n345M \n0.8577 \n98767.56 \n2.169 \n0.523 \nUnstable \nDistilGPT2 [43] \n82M \n0.5575 \n9.82 \n0.485 \n0.620 \nLow Accuracy \nT5-Small [44] \n60M \n0.8646 \n16.76 \n0.920 \n0.832 \nTrade-off \nT5-Large[44] \n770M \n0.9362 \n1.41 \n1.949 \n0.864 \nBest Accuracy \nMamba-130M [14] \n130M \n0.8541 \n1.93 \n0.055 \n0.840 \nBest Speed \n \n3.3.2 Integrated MoE Architecture \nMoreover, the two experts are integrated into a single MoE layer, treated as parallel, specialized feed-forward networks with the proposed \nMambaFormer, the MoE integration defined in Equation (6). This design enables adaptive computation where each token is routed to the most suitable \nexpert based on contextual features. The MoE layer, embedded in the transformer feed-forward block, enables token-level adaptive routing via the \ngating function ùê∫ùëò(Equation (8)), which directs tokens to either ET5 for high-accuracy processing or EMamba for efficient linear computation. Both \n"}, {"page": 11, "text": "experts remain frozen, leveraging their complementary strengths (Eq. 1) while the lightweight router dynamically assigns tokens based on contextual \ncues. The complete customization and integration pipeline is visualized in Figure 2, and MoE integration is mathematically defined as: \nMoE(‚Ñéùëñ) = ‚àë\nùê∫ùëò(‚Ñéùëñ)ùê∏ùëò(‚Ñéùëñ)\nùëò‚àà{ùëÄ,ùëá}\n (8) \nTable 3: Strategic Roles and Target Utilization of the Customized Experts in MambaFormer. \nExpert \nStrategic Role \nComplexity \nTarget Utilization \nEMamba (Efficiency Expert) \nLinear-time SSM for long sequences \nO(N) \n‚âà 96.2% \nET5(Accuracy Expert) \nSelf-attention for short, complex queries \nO(N2) \n‚âà 3.8% \n3.4 Intelligent Router Design and Selection Policy \nThe proposed MambaFormer implements guided token-level routing with minimum overhead by exploiting sequence-length and domain-specific \ninformation. The evaluation Pareto allocation routes mostly provided tokens to the efficient Mamba expert (‚âà96.2%) and a small fraction to the \naccuracy-focused ET5 expert (‚âà3.8%), preserving low computational cost as shown in Table 3. \n3.4.1 Router Architecture \nThe router design includes a two-layer MLP with approximately 1.3K trainable parameters that control the latency. The router computes a gating \nscore Si,j for each expert i (where j‚àà EMamba, ET5) using a softmax function from an input token with contextual representation xi, as defined in \nEquation (9). \nSi,j =\nex p((xiWg+bg)j)\n‚àë\nex p((xiWg+bg)k)\nk‚àà{Mamba,T5}\n \n (9) \nwherej ‚àà{Mamba,T5 \nm‚àó= arg\nmax\nm‚àà{EMamba,ET5} Si,m                (10) \nwhere Wg and bg are the learned weights and biases of the gating network, and compute a weighted combination of expert outputs for gradient flow \nduring training. The router performs a hard selection based on the argmax of these scores (Equation (10)), activating only the chosen expert m‚àó. This \nhard selection activates expert m‚àó is selected for the forward pass of token xi. Moreover, the overall computational cost is a weighted sum of the two \nexperts' complexities. The complete inference-time routing procedure is formalized in Algorithm 1. \nAlgorithm 1: Dynamic Routing with Speed-Constrained Gating (Inference) \n1:  procedure DYNAMICROUTING (zt, xf) \n2:     // Input: zt (token representation), xf= [‚Ñì, d] (input features) \n"}, {"page": 12, "text": "3:     // Output: yt (expert output)     \n5:     // 1. Router Input Feature Fusion \n6:     R ‚Üê CONCATENATE (zt, xf) \n8:     // 2. Compute Soft Routing Scores   \n9:     S ‚Üê SOFTMAX(MLP¬Æ)                    ‚ñ∑ Based on Equation 2 \n11:    // 3. Hard Selection (Utility-Guided) \n12:    m* ‚Üê ARGMAX(Sm) for m ‚àà {EMamba, ET5} \n14:    // 4. Expert Execution \n15:    yt‚Üê m*(zt)                            ‚ñ∑ Single expert forward pass \n17:    return yt \n18:  end procedure \nThis algorithm concisely captures the inference process: feature fusion, score calculation, utility-guided expert selection, and execution. The end-to-\nend routing pipeline is also illustrated in Figure 3. \n3.4.2 Input Features to the Router \nThe router‚Äôs decision is based on the raw token embedding xi; length, domain-aware features to represent the token's contextual information that may \nguide both the experts. \na) Normalized Sequence Length (l): The current input sequence feature length is normalized to the range [0, 1]. It directly encodes the computational \ncontext of longer sequences (l‚Üí1) routing directly to the efficient EMamba, while shorter sequences (l‚Üí0) can leverage the accuracy of ET5 with optimal \nlatency cost. \nb) Binary Domain Encoding (d): The feature indicates the source dataset of the input sequence, where d = 1 if the sequence is from the \nDentalQA dataset; 0 if the sequence is from the PubMedQA dataset. This encoding provides domain context DentalQA information (d=1), which is \nshorter and requires high precision for clinical QA, and is biased toward ET5. Moreover, PubMedQA sequences (d=0), characterized by long biomedical \nabstracts, are biased toward EMamba for efficient long-context processing. These two features [l, d] are concatenated ‚ÄòR=Concatenate(xi, l, d) ‚Äô with their \ncontextual representation xi to form the final input to the router MLP. This fused representation allows the router to make a joint decision based on \nboth semantic content and structural/domain metadata. \n"}, {"page": 13, "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 3: MambaFormer Core Architecture with Token-Level Dynamic Routing. \n3.5 Utility-Guided Routing and Pareto Optimization \nThe router's selection policy is driven by learning and optimizing the objective function that enforces an optimal balance between accuracy and latency \nand a Pareto-optimal operating point for the whole system. The router estimates a Predicted Accuracy Utility Gain (Ugain  ) for each token. The gain \nvalue represents the expected improvement in answer quality, like BERTScore, as shown in equation (11). The tokens are either processed by the high-\nTokenizer: BERT-base Max \nLength: 512 tokens\nOutput: Token Sequence \n[x‚ÇÅ...x‚Çô]\nEmbedding Dim: 768\nInput: Clinical Question or \nBiomedical Abstract\nl = Seq_Length / Max_Length\nRange: [0, 1] \nd = 1: DentalQA dataset(Clinical \nQA pairs)d =0: PubMedQA \ndataset(Biomedical abstracts)\n Context:l      Short query l \n     Long document\nBias:d=1   favours E_T5\nd=0   favours E_Mamba\nParameters: ~1.3K (Trainable)Input: \nR = [x_i, l, d]  Operation:\n1. Feature Fusion: Concatenate\n2. Score Calc: S = Softmax(MLP¬Æ\n  S_i,Mamba  S_i,T5\n3. Hard Selection:m* = argmax(S_i)\n INTELLIGENT ROUTER\nIF U_gain(x_i) > œÑ(Œª‚ÇÇ)Route to   \nE_T5ELSE:Route to   E_MambaU_gain = \nE[BERTS|E_T5] -E[BERTS|Mamba]\nArchitecture: State Space Model\nComplexity: O(N) Linear-time\nStrength: Long-sequence\nParams: 130M\n        of tokens\n E_MAMBA EXPERT \n(Efficiency Specialist)\n96.2%\n E_T5 EXPERT \n(Accuracy Specialist)\nArchitecture: Transformer\nComplexity:O(N¬≤)\nStrength:Complex reasoning\n3.8%\nOutput Token: y_i = m*(x_i)(from selected expert)\nKEY INNOVATION\nToken-level adaptive computation\nPareto-optimal operating point Expected Computational \nComplexity:\nC_exp(N) = 0.962¬∑O(N) + 0.038¬∑O(N¬≤\n STATUS: FROZEN\n STATUS: FROZEN\n       of \ntokens\n   L_CE + Œª‚ÇÅL_Bal + Œª‚ÇÇL_Pen               \n   L_CE: Task accuracy (Cross-Entropy)\n   L_Bal: Expert utilization balance\n  L_Pen: Speed penalty (limits E_T5)         \n    Experts remain frozen \n Target: E_T5 usage        (Matches \ndataset token distribution)\nMambaFormer Core Architecture with Token-Level Dynamic Routing\n"}, {"page": 14, "text": "accuracy expert ET5 or by the efficiency expert EMamba. The final routing decision is determined by comparing this utility against a dynamic threshold ‚ÄòœÑ‚Äô, \nwhich is modulated by the speed penalty weight Œª2 from the training objective (see Section 3.6): \nRoute to ET5      if Ugain > œÑ(Œª2) \n                          \n \n (11) \n Else, route to Emamba. \nUgain(xi) = E[ BERTScore ‚à£‚à£ET5, xi ] ‚àíE[ BERTScore ‚à£‚à£EMamba, xI ]  (12) \nCexpected(N) = 0.962 ‚ãÖO(N) + 0.038 ‚ãÖO(N2) \n    \n \n(13) \nThis threshold ‚ÄòœÑ‚Äôdirectly encodes the accuracy-latency trade-off. A high threshold conservatively limits the use of the slower ET5, favoring speed. The \nlower threshold allows more frequent use of ET5, favoring accuracy. The parameter  Œª2 is tuned so that the learned threshold œÑ finds a Pareto-optimal \nbalance, where a point increase in accuracy would have a disproportionate cost in latency. This routing distribution leads to an expected computational \ncomplexity as shown in Equation 12. The framework learn and converges to a stable routing distribution, where ‚âà96.2% of tokens are assigned \nto Emamba and only ‚âà3.8% to ET5, as shown in Eq. 13. The distribution aligns with the intrinsic properties of our biomedical datasets, where the majority \nof tokens belong to long, throughput-sensitive sequences (ideally handled by Emamba). In contrast, a small fraction are from short, accuracy-critical \nqueries that benefit from deep analysis. Finally, the utility-guided mechanism ensures that the expensive quadratic complexity of ET5 is invoked, and \nthe expected accuracy payoff justifies its computational cost. The decision process is visualized in Figure 3, which illustrates the flow from feature \nextraction through utility calculation to the final expert selection. \n3.6 Speed-Constrained Training Objective \nThe guided routing operations is learned through the optimization of a novel multi-objective loss function, LMamabaFormer. This loss function is specifically \ndesigned to integratively maximize task accuracy and maintain balanced expert utilization. The function enforces a strict inference speed constraint \nand directly shapes the Pareto-optimal operating point. The total loss is defined as a weighted sum of three components, as described in Equation (14). \nThe first component, LCE  is the standard cross-entropy task loss between the predicted answer and the actual clinical answer. It ensures the primary \nobjective of medical QA accuracy is preserved as defined in Equation (15). Moreover,  Balance Loss (LBal) in eq (16) also employs load-balancing loss \nbased on Kullback-Leibler divergence to prevent router collapse, where the router might ignore one expert. Moreover, it encourages the router to utilize \nboth experts proportionally to a soft, uniform target distribution. Where L is sequence length, E is the number of experts (2), and Si,j is the soft gating \nscore for token i and expert j.  \nLMambaFormer = LCE + Œª1LBal + Œª2LPen \n \n \n(14) \nùêøCE = ‚àí‚àëùë¶ùëê\nùëê\nlog(ùëùùëê)                                       \n \n(15) \n"}, {"page": 15, "text": "ùêøBal = ‚àë\n‚àë\nùëÜùëñ,ùëólog ! (\n1/|ùê∏|\nùëÜùëñ,ùëó)\n|ùê∏|\nùëó=1\nùêø\nùëñ=1\n \n              \n \n(16) \nùêøPen = ‚àë\nmax ! (0, ùëÜùëñ,ùê∏T5 ‚àíùëáùë¢ùëñ)\nùëñ‚ààtokens\n \n \n \n(17) \nFinally, the key innovative component, Speed Penalty (Œª2‚ãÖ, L Pen ) as shown in equation (17), explicitly enforces the speed-accuracy trade-off. It \npenalizes the excessive use of the slower ET5 expert beyond a predefined utilization threshold Tui‚âà0.08. The hyperparameter Œª2 controls the strength \nof this penalty; setting ùúÜ2 = 0.5 constrains ET5 usage to about 3.8%, yielding latency to a predominantly EMamba system. The lightweight router (1.3K \nparameters) is optimized while both experts remain frozen, while ensuring stable routing. The resulting LMambaFormer loss learns a routing policy that \nbalances accuracy, utilization, and efficiency. \n4. Experimental Setup \n4.1 Datasets \nTwo biomedical QA datasets have been used to support domain specialization and generalization. We custom-designed DentalQA, a curated clinical \ndataset of 5,000 expert-annotated question‚Äìanswer pairs on dental materials and equipment, enriched with structured metadata; BERT-based \nparaphrasing expanded it to 13,000 training samples. Moreover, PubMedQA (Long Answer) includes 2,600 biomedical questions with full research \nabstracts, merged into uninterrupted long-context sequences of up to 4,096 tokens to form a challenging generalization benchmark. Table 5 summarizes \nthe dataset usage regarding the training and evaluation phases. \nTable 4: Statistics of the DentalQA Dataset \n \n \n \n \n \n \nTable 5: Dataset Usage Summary \nDataset \nDomain \nSize (Original/Augmented) \nAvg. Sequence  (l) \nRole \nDentalQA \nDental QA \n5,000 / 13,000 \n‚âà512 tokens \nTrain&Valid. (80/10/10 split) \nPubMedQA \nBiomedical Abstracts 2,600 / 2,600 \n2,000‚Äì4,000 tokens \nHoldout Test (Zero-Shot Eval.) \n \nAttribute \nValue \nTotal Original QA Pairs \n5,000 \nTotal Augmented QA Pairs \n13,000 \nAverage Question Length \n12.4 words \nAverage Answer Length \n27.8 words \nUnique Products \n850 \nManufacturers Covered \n47 \n"}, {"page": 16, "text": "4.2 Implementation and Training Configuration \n \nAll experiments were conducted on a single NVIDIA A100 GPU (40GB VRAM) using PyTorch and the Hugging Face transformers library. Training \nutilized mixed precision (FP16) for efficiency. The candidate models for expert selection (GPT-2 variants, T5-Small, T5-Large, Mamba-130M) have \nbeen fine-tuned on the customized DentalQA training split. The parameters are efficiently fine-tuned via Low-Rank Adaptation (LoRA) [37] with \nrank r=8. The training configurations include 50 epochs, Adam optimizer, cross-entropy loss to evaluate and select experts based on BERTScore \nF1 (accuracy) and Inference Latency (speed) on the DentalQA validation and PubMedQA test sets. After expert selection and the MambFormer \ntraining, the chosen experts (ET5 and EMamba) have been frozen. Only the router's 1.3K parameters have been updated. The employed router is composed \nof a 2-layer MLP (Input: 2 features, Hidden: 16 neurons). Training/holdout cross-validation has been conducted on the DentalQA dataset (augmented, \nwith an 80/10/10 split). Training Data: DentalQA training split (80% of augmented data) and Optimization includes Adam (LR=0.001), Batch Size=64, \nfor 20 epochs. Key Hyperparameters are summarized in Table 6.  \nTable 6: MambaFormer Hyperparameters \nParameter \nValue \nDescription \nGating Network \n2-layer MLP \nHidden size 16 (~1.3K parameters) \nTrainable Components \nRouter only \nExperts are frozen \nInput Features \n[l, d] \nNormalized length (l) and domain (d) \nBatch Size \n64 \n- \nLearning Rate \n1e-3 \nAdam optimizer \nLoss Weights \nŒª1 = 1.0, Œª2 = 0.5 \nBalance vs. speed penalty \nTarget Utilization (Tui) \n0.08 \nMaximum soft usage for ET5 \n \n4.3 Evaluation Metrics \nThe models were evaluated using a comprehensive suite of metrics across three categories: performance, efficiency, and routing analysis. Performance \nwas measured using BERTScore F1 (Equation 18; P, R) for semantic similarity, ROUGE-L (Equation 19; RLCS and PLCS) for fluency, and Perplexity \nfor language modeling. Efficiency was assessed via Inference Latency (s/seq), Throughput (Equation 20; seq/sec), and GPU Memory (Equation 21; \nMB). Routing behavior was evaluated using Token Routing Distribution and Pareto Optimality (Equation 22). All metrics were computed on holdout \nPubMedQA and DentalQA datasets, with results averaged across runs for robustness. The mathematical formulations for key metrics are defined as \nfollows:  \n"}, {"page": 17, "text": "ùë≠=\nùüê‚ãÖùë∑‚ãÖùëπ\nùë∑+ùëπ \n                        (18) \nROUGE-L =\n(ùüè+ùõÉùüê)‚ÄâùëπLCS‚ãÖùë∑LCS\nùëπLCS+ùõÉùüê‚ãÖùë∑LCS  (19) \nThroughput =\nùëµseq\nùëªtotal \n            (20) \nMFMB =\nùëµparams√óùüí\nùüèùüéùüêùüíùüê \n            (21) \nRE =\nùëµcorrect\nùëµtotal √ó ùüèùüéùüé%             (22) \n5. Result and Discussion \nThe results demonstrate that MambaFormer outperforms state-of-the-art models across benchmarks, balancing accuracy and inference speed via token-\nlevel dynamic routing. Evaluations on BERTScore, latency, and memory confirm effective expert selection for each token. Ablation studies validate \ndesign choices, while the Speed Penalty (Œª‚ÇÇ¬∑LPen) ensures the expected trade-off between accuracy and efficiency, highlighting the framework‚Äôs \nsuitability for clinical deployment. \n5. 1 Performance Comparison \nThe proposed MambaFormer has been on PubMedQA against Transformers, SSMs, and hybrid models, including BioBERT, Mamba, Mamba-2, \nJamba, BioMamba, and Hybridformer. As shown in Table 7, it outperformed single-architecture baselines with 2‚Äì8% higher BERTScore. Compared \nwith BioBERT, MambaFormer gained 2.0% in BERTScore while achieving 24.7√ó faster inference. Against Mamba-2, it improved accuracy by 5.5% \nwith similar speed. Over static hybrids such as Jamba and Hybridformer, MambaFormer achieved 3.1‚Äì4.9% higher BERTScore with 5.8‚Äì6.5√ó faster \nprocessing. Figure 4 illustrates the Pareto-optimal trade-off between accuracy and speed, confirming that dynamic token-level routing effectively fuses \nTransformer precision with SSM efficiency. The framework attains near-Oracle accuracy (0.9180 vs. 0.9195 BERTScore) while reducing latency to \n0.077‚ÄØs per sequence, representing a 24.4√ó speedup over T5-Large (1.883‚ÄØs). These results validate MambaFormer as a highly accurate, efficient, and \nclinically deployable solution for real-time biomedical QA. \n \n \n"}, {"page": 18, "text": "Table 7: State-of-the-Art Performance Comparison on PubMedQA Benchmark. \nModel \nArchitecture \nBERTScore (F1) ‚Üë Speed (s) ‚Üì \nMemory (MB) \nBioBERT [10] \nTransformer \n0.9000 \n1.900 \n195.00 \nMamba [14] \nSSM \n0.8500 \n0.100 \n142.00 \nMamba-2 [14] \nSSM \n0.8700 \n0.085 \n138.50 \nJamba [20] \nHybrid MoE \n0.8900 \n0.450 \n210.00 \nBioMamba [40] \nSSM \n0.8800 \n0.092 \n145.20 \nHybridformer [33] \nSSM-Transformer \n0.8750 \n0.380 \n188.00 \nMambaFormer (Ours) \nDynamic Hybrid MoE \n0.9180 \n0.077 \n187.98 \nOracle Router \nIdeal \n0.9195 \n0.078 \n187.98 \n \n \nFigure 4: Model Performance Landscape \n5. 2 BaseLine Performance Characterization \n"}, {"page": 19, "text": "The two experts, ET5 (T5-Large) and EMamba (Mamba-130M), have been customized and fine-tuned on the DentalQA dataset to specialize in high-\naccuracy and high-efficiency processing, respectively (see Section 3.3). Their complementary profiles and higher accuracy for ET5 at 24.1√ó latency \nversus EMamba efficiency enable MambaFormer to achieve a Pareto-optimal balance through dynamic routing, yielding superior performance \n(BERTScore = 0.9180). As summarized in Table 8, ET5 achieves a higher BERTScore but at 24.1√ó higher latency, whereas EMamba offers near-real-\ntime inference with slightly lower accuracy. This complementary performance profile forms the foundation for MambaFormer‚Äôs adaptive routing \nstrategy. \nTable 8: Expert Performance Characterization on PubMedQA (Zero-Shot Evaluation). \nExpert \nArchitecture \nBERTScore (F1)  \nInference Latency (s)  \nKey Role \nET5 (T5-Large) \nTransformer \n0.9050 \n1.883 \nHigh-Accuracy Expert \nEMamba (Mamba-130M) \nSSM \n0.8600 \n0.078 \nHigh-Efficiency Expert \n \n \nFigure 5: Baseline Expert Specialization. \n5. 3 Proposed MambaFormer Routing Analysis and Speed Penality Validation \n"}, {"page": 20, "text": "This section demonstrates that MambaFormer‚Äôs utility-guided routing effectively enforces the accuracy‚Äìlatency trade-off via the speed penalty Œª2.LPen. \nThe learned policy achieves near-oracle performance (0.16% BERTScore gap) while constraining ET5 usage to 3.8%, well below the latency threshold, \nconfirming that the router invokes the slower expert only when accuracy gains justify the cost and operates on the Pareto-optimal frontier. This indicates \nthat the router invokes the slower ET5 expert only when accuracy gains justify the computational cost, validating the effectiveness of the utility-guided \ngating mechanism introduced in Section 3.5. \n \nFigure 6-7: Pareto Frontier routing. \n \n"}, {"page": 21, "text": " \nFigure 7: Near-Optimal Routing Validation. \nTable 9: Routing Strategy Comparison and Impact on Accuracy-Latency Trade-off \n \n \n \n \n \n5. 4 Utility Driven Routing Distribution \nThis section analyzes the token-level routing distribution learned by MambaFormer on the PubMedQA holdout evaluation set, demonstrating the \npractical application of the utility-guided policy described in Section 3.5. \nTable 10: Utility-Driven Routing, Distribution, and Learned Expert Allocation \nExpert \n% Tokens Routed \nAverage Sequence Length Routing Context \nEMamba \n96.2% \n2,150 tokens \nLong biomedical contexts (efficiency-focused) \nET5 \n3.8% \n480 tokens \nShort, complex queries (accuracy-critical)  \nRouting Strategy \nBERTScore (F1) ‚Üë \nLatency (s) ‚Üì \nET5 Utilization \nAlways ET5 \n0.9050 \n1.883 \n100% \nAlways EMamba \n0.8600 \n0.078 \n0% \nMambaFormer Router \n0.9180 \n0.077 \n3.8% \nOracle Router \n0.9195 \n0.078 \nOptimal \n"}, {"page": 22, "text": "The 96.2% and 3.8% distribution emerges directly from optimizing the speed-constrained loss LMambaFormer (Equation 14), where the penalty \nterm (Œª‚ÇÇ.LPen) (Equation 17) enforces low ET5 utilization. This validates the router's implementation of the utility-based decision rule from Section 3.5: \n‚Ä¢ \nTokens from long sequences (avg. 2,150 tokens) are routed to efficient EMamba \n‚Ä¢ \nTokens from short, complex segments (avg. 480 tokens) receive ET5 accuracy \nTokens routed to ET5 exhibit significantly higher predicted utility gains (Ugain) than those routed to EMamba. This confirms that the router invokes the \nslower expert only when justified by expected accuracy improvement, exactly as designed in the utility-guided framework. As previously visualized in \nFigure 6-7 (Section 5.3), this intelligent allocation enables MambaFormer to achieve the Pareto-optimal balance, maintaining near-Oracle accuracy \n(0.9180 BERTScore) while delivering Mamba-like inference speed (0.077s).  \n5. 5  Ablation Study of the Proposed MambaFormer Component  \nAn ablation study was conducted to evaluate the contribution of each component in MambaFormer, as summarized in Table 11. where each component \nhas been added to the framework, and its effects on performance in terms of accuracy and efficiency, this shows that systematic integration of each \ncomponent is important to the proposed MambFormer framework, offering different benefits to the Model, which is visualized in Figure 8. The ablation \nstudy reveals that: \nGating Network is essential as its absence performance drop to the EMamba baseline (-5.8% BERTScore). \n1. Speed penalty Œª2. LPen is critical, while its removal increases ET5 usage, harming latency control. \n2. Domain feature (d) provides valuable context that distinguishes DentalQA from PubMedQA and improves routing decisions. \n3. Both features (‚Ñì, d) are necessary because Sequence length alone provides limited guidance.  \nThese results confirm that both sequence length (‚Ñì) and domain encoding (d) are essential for informed token routing. \nTable 11: Ablation Study of Proposed MambaFormer Components \nConfiguration \nBERTScore (F1)  Latency (s)  Key Insight \nFull MambaFormer \n0.9180 \n0.077 \nReference \nw/o Gating Network \n0.8600 \n0.078 \nDefaults to EMamba (‚àí5.8% BERTScore) \nw/o Speed Penalty (Œª2 = 0) \n0.9090 \n0.079 \nLoses latency control (‚àí0.9% BERTScore) \nw/o Domain Feature (d) \n0.9110 \n0.077 \nLoses dataset context (‚àí0.7% BERTScore) \nSequence Length Only (‚Ñì) \n0.9140 \n0.078 \nReduced feature set (‚àí0.4% BERTScore) \n"}, {"page": 23, "text": " \nFigure 8: Ablation Study Component Contribution. \n5. 6 Training and Deployment Efficiency Analysis \nBeyond inference performance, MambaFormer achieves remarkable efficiency in training adaptation and resource utilization, critical for clinical \ndeployment where computational resources are often constrained (Table 12). \nMambaFormer's lightweight router (1.3K params) converged in 1.0 hour using mixed precision on a single A100 GPU. Convergence behavior was \nstable (Figure 9), with validation loss plateauing after ~1,200 steps. \nKey Efficiency Gains: \n1. 94.6% faster adaptation than full T5-Large fine-tuning (1.0 vs. 18.5 hours) \n2. 592,000√ó fewer trainable parameters than T5-Large (1.3K vs. 770M) \n3. Comparable inference memory to single-expert models despite a hybrid architecture \n4. 24.4√ó inference speedup over T5-Large (as calculated in Section 5.1) \n"}, {"page": 24, "text": "These efficiency metrics, combined with the Pareto-optimal performance shown in Figure 6, demonstrate MambaFormer's suitability for resource-\nconstrained clinical environments where both rapid deployment and real-time inference are essential. \nTable 12: Efficiency Comparison for Clinical Deployment. \nModel \nTrainable Parameters \nTrain.Time \nInf.Latency (s) \nSequence Memory (MB) \nMambaFormer \n1.3K (Router only) \n1.0 hour \n0.077 \n188 \nT5-Large (Fine-tuned)[44] \n770M \n18.5 hours \n1.883 \n189 \nMamba-130M (Fine-tuned)[14] \n130M \n6.8 hours \n0.078 \n188 \nJamba [20] \n52B (8B active) \nN/A(pre-trained) \n0.450 \n~3,500 \n \nFigure 9: Training Stability and Convergence Metrics. \n \n"}, {"page": 25, "text": " \n5. Conclusion and Future Direction \nThe Proposed MambaFormer MoE framework introduced a novel Utility-Guided Dynamic Routing to achieve Pareto-optimal efficiency in low-\nresource clinical assistance. The proposed framework utilizes two experts: EMamba (long sequences, Fast and high-throughput cases) and ùê∏ùë°5 \n(complex short contents, high-accuracy cases, but slower). These experts EMamba and Et5 initial and target level layers are adapted and TL-based \nfine-tuned to target-specific, newly designed DentalQA and PubMedQA datasets to improve predictive performance. While the optimal \nmultiobjective function for guided routing. Moreover, a utility-guided gating mechanism is introduced to learns domain-aware and sequence-length \nfeatures and automatically assign each token to the most suitable expert and also enforce a Pareto-optimal trade-off between accuracy and latency. \nTherefore, most of the tokens (96.2%) and (3.8% ) have been utilized by efficient EMamba and ET5 expert existing dataset diversity, respectively. \nIn this way, with available resources, the proposed Mambaformer framework achieves a Bert score (F1 9180) and a response time of  0.077sec (a \n24.4x speedup over traditional models) to deploy a good solution of LLM in various clinical applications. The proposed MambaFormer enables \nfaster query resolution about instrumental usage for the dental and medical diseases, improved diagnostic accuracy, and reduced computational \ncost, and can be deployed for real-time clinical assistance. In the future, to advance the proposed MambaFormer from research findings into a valid \nand clinically translatable technology, it is necessary to address model understandability, regulatory compliance, and socio-economic impact issues.  \nThe proposed routing mechanism may be employed in multi-domain evaluation, like general news, legal corpora, and Islamic logic interpretation, \nto assess the portability and robustness of the learned utility function. We will explore token-aware retrieval and knowledge grounding, alongside \nfederated learning-based expert training, to enable privacy-preserving updates without sharing sensitive clinical data. \n \n \n"}, {"page": 26, "text": "Reference \n[1] \nT. B. Brown et al., ‚ÄúLanguage Models are Few-Shot Learners,‚Äù Jul. 2020, [Online]. Available: http://arxiv.org/abs/2005.14165 \n[2] \nE. Strubell, A. Ganesh, and A. McCallum, ‚ÄúEnergy and Policy Considerations for Deep Learning in NLP,‚Äù Jun. 2019, [Online]. Available: \nhttp://arxiv.org/abs/1906.02243 \n[3] \nD. Patterson et al., ‚ÄúCarbon Emissions and Large Neural Network Training.‚Äù \n[4] \nZ. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, ‚ÄúMobileBERT: A compact task-agnostic BERT for resource-limited devices,‚Äù in \nProceedings of the Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics (ACL), 2020, \npp. 2158‚Äì2170. doi: 10.18653/v1/2020.acl-main.195. \n[5] \nK. Huang, J. Altosaar, and R. Ranganath, ‚ÄúClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission,‚Äù Nov. 2020, [Online]. \nAvailable: http://arxiv.org/abs/1904.05342 \n[6] \nJ. Wang et al., ‚ÄúA Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and \nEvaluations,‚Äù Sep. 2024, [Online]. Available: http://arxiv.org/abs/2406.10303 \n[7] \nA. Vaswani et al., ‚ÄúAttention Is All You Need,‚Äù Aug. 2023, [Online]. Available: http://arxiv.org/abs/1706.03762 \n[8] \nM. H. Murad et al., ‚ÄúMeasuring Documentation Burden in Healthcare,‚Äù Nov. 01, 2024, Springer. doi: 10.1007/s11606-024-08956-8. \n[9] \nF. Shamshad et al., ‚ÄúTransformers in medical imaging: A survey,‚Äù Med Image Anal, vol. 88, p. 102802, Aug. 2023, doi: \n10.1016/j.media.2023.102802. \n[10] J. Lee et al., ‚ÄúBioBERT: A pre-trained biomedical language representation model for biomedical text mining,‚Äù Bioinformatics, vol. 36, no. 4, \npp. 1234‚Äì1240, Feb. 2020, doi: 10.1093/bioinformatics/btz682. \n[11] I. Beltagy, M. E. Peters, and A. Cohan, ‚ÄúLongformer: The Long-Document Transformer,‚Äù Dec. 2020, [Online]. Available: \nhttp://arxiv.org/abs/2004.05150 \n[12] M. Zaheer et al., ‚ÄúBig Bird: Transformers for Longer Sequences,‚Äù Jan. 2021, [Online]. Available: http://arxiv.org/abs/2007.14062 \n[13] K. Choromanski et al., ‚ÄúRethinking Attention with Performers,‚Äù Nov. 2022, [Online]. Available: http://arxiv.org/abs/2009.14794 \n[14] A. Gu and T. Dao, ‚ÄúMamba: Linear-Time Sequence Modeling with Selective State Spaces,‚Äù May 2024, [Online]. Available: \nhttp://arxiv.org/abs/2312.00752 \n[15] J. Ku et al., ‚ÄúSystems and Algorithms for Convolutional Multi-Hybrid Language Models at Scale,‚Äù Feb. 2025, [Online]. Available: \nhttp://arxiv.org/abs/2503.01868 \n"}, {"page": 27, "text": "[16] J. Wang, T. Gangavarapu, J. N. Yan, and A. M. Rush, ‚ÄúMambaByte: Token-free Selective State Space Model,‚Äù Aug. 2024, [Online]. Available: \nhttp://arxiv.org/abs/2401.13660 \n[17] M. Pi√≥ro et al., ‚ÄúMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts,‚Äù Feb. 2024, [Online]. Available: \nhttp://arxiv.org/abs/2401.04081 \n[18] M. Fathi, J. Pilault, O. Firat, C. Pal, P.-L. Bacon, and R. Goroshin, ‚ÄúBlock-State Transformers,‚Äù Oct. 2023, [Online]. Available: \nhttp://arxiv.org/abs/2306.09539 \n[19] T. Dao and A. Gu, ‚ÄúTransformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality,‚Äù May 2024, \n[Online]. Available: http://arxiv.org/abs/2405.21060 \n[20] O. Lieber et al., ‚ÄúJamba: A Hybrid Transformer-Mamba Language Model,‚Äù Jul. 2024, [Online]. Available: http://arxiv.org/abs/2403.19887 \n[21] S. Mu and S. Lin, ‚ÄúA Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications,‚Äù Apr. 2025, [Online]. Available: \nhttp://arxiv.org/abs/2503.07137 \n[22] A. Q. Jiang et al., ‚ÄúMistral 7B,‚Äù Oct. 2023, [Online]. Available: http://arxiv.org/abs/2310.06825 \n[23] M. Abdin et al., ‚ÄúPhi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,‚Äù Aug. 2024, [Online]. Available: \nhttp://arxiv.org/abs/2404.14219 \n[24] ‚ÄúMAMBA-3: IMPROVED SEQUENCE MODELING USING STATE SPACE PRINCIPLES.‚Äù \n[25] H. Yang, Z. Li, Y. Chang, and Y. Wu, ‚ÄúA Survey of Retentive Network,‚Äù Jun. 2025, [Online]. Available: http://arxiv.org/abs/2506.06708 \n[26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,‚Äù \nMay 2019, [Online]. Available: http://arxiv.org/abs/1810.04805 \n[27] C. Raffel et al., ‚ÄúExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,‚Äù Sep. 2023, [Online]. Available: \nhttp://arxiv.org/abs/1910.10683 \n[28] J. \nYe \net \nal., \n‚ÄúA \nComprehensive \nCapability \nAnalysis \nof \nGPT-3 \nand \nGPT-3.5 \nSeries \nModels.‚Äù \n[Online]. \nAvailable: \nhttps://platform.openai.com/docs/model-index-for-researchers \n[29] A. R. Khan and A. Khan, ‚ÄúMulti-axis vision transformer for medical image segmentation,‚Äù Eng Appl Artif Intell, vol. 158, p. 111251, Oct. 2025, \ndoi: 10.1016/J.ENGAPPAI.2025.111251. \n[30] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, ‚ÄúLinformer: Self-Attention with Linear Complexity,‚Äù Jun. 2020, [Online]. Available: \nhttp://arxiv.org/abs/2006.04768 \n[31] Q. Jin, R. Leaman, and Z. Lu, ‚ÄúPubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence.‚Äù \n"}, {"page": 28, "text": "[32] M. Mumtaz Zahoor and S. Hussain Khan, ‚ÄúBrain tumor MRI Classification using a Novel Deep Residual and Regional CNN.‚Äù \n[33] J. Du, J. Hu, T. Zhang, W. Sun, and Y. Cheng, ‚ÄúNative Hybrid Attention for Efficient Sequence Modeling,‚Äù Oct. 2025, [Online]. Available: \nhttp://arxiv.org/abs/2510.07019 \n[34] S. Somvanshi, M. M. Islam, M. S. Mimi, S. B. B. Polock, G. Chhetri, and S. Das, ‚ÄúFrom S4 to Mamba: A Comprehensive Survey on Structured \nState Space Models,‚Äù May 2025, [Online]. Available: http://arxiv.org/abs/2503.18970 \n[35] E. Nguyen et al., ‚ÄúHyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution,‚Äù Nov. 2023, [Online]. Available: \nhttp://arxiv.org/abs/2306.15794 \n[36] H. Xie et al., ‚ÄúGraph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications,‚Äù in Proceedings \nof the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Association for Computing Machinery, Aug. 2023, \npp. 5270‚Äì5281. doi: 10.1145/3580305.3599833. \n[37] W. Fedus, B. Zoph, and N. Shazeer, ‚ÄúSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,‚Äù Jun. 2022, \n[Online]. Available: http://arxiv.org/abs/2101.03961 \n[38] A. Roy, M. Saffar, A. Vaswani, and D. Grangier, ‚ÄúEfficient Content-Based Sparse Attention with Routing Transformers,‚Äù Oct. 2020, [Online]. \nAvailable: http://arxiv.org/abs/2003.05997 \n[39] Y. Sun et al., ‚ÄúRetentive Network: A Successor to Transformer for Large Language Models,‚Äù Aug. 2023, [Online]. Available: \nhttp://arxiv.org/abs/2307.08621 \n[40] L. Yue, S. Xing, Y. Lu, and T. Fu, ‚ÄúBioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba,‚Äù Aug. 2024, \n[Online]. Available: http://arxiv.org/abs/2408.02600 \n[41] S. Hussain Khan, ‚ÄúAdvanced Hybrid Transformer-LSTM Technique with Attention and TS-Mixer for Drilling Rate of Penetration Prediction.‚Äù \n[42] E. J. Hu et al., ‚ÄúLoRA: Low-Rank Adaptation of Large Language Models,‚Äù Oct. 2021, [Online]. Available: http://arxiv.org/abs/2106.09685 \n[43] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, ‚ÄúLanguage Models are Unsupervised Multitask Learners.‚Äù [Online]. \nAvailable: https://github.com/codelucas/newspaper \n[44] C. Raffel et al., ‚ÄúExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,‚Äù 2020. [Online]. Available: \nhttp://jmlr.org/papers/v21/20-074.html. \n  \n"}]}