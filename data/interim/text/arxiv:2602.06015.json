{"doc_id": "arxiv:2602.06015", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.06015.pdf", "meta": {"doc_id": "arxiv:2602.06015", "source": "arxiv", "arxiv_id": "2602.06015", "title": "A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies", "authors": ["Panagiotis Kaliosis", "Adithya V Ganesan", "Oscar N. E. Kjell", "Whitney Ringwald", "Scott Feltman", "Melissa A. Carr", "Dimitris Samaras", "Camilo Ruggero", "Benjamin J. Luft", "Roman Kotov", "Andrew H. Schwartz"], "published": "2026-02-05T18:53:17Z", "updated": "2026-02-05T18:53:17Z", "summary": "Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.06015v1", "url_pdf": "https://arxiv.org/pdf/2602.06015.pdf", "meta_path": "data/raw/arxiv/meta/2602.06015.json", "sha256": "e9ff5eceecd23815eb7647ab73a0f0373e5d8a88108eb57da1e9759f4a252f80", "status": "ok", "fetched_at": "2026-02-18T02:19:39.863508+00:00"}, "pages": [{"page": 1, "text": "A Systematic Evaluation of Large Language\nModels for PTSD Severity Estimation: The Role\nof Contextual Knowledge and Modeling Strategies\nPanagiotis Kaliosis1*†, Adithya V Ganesan1†, Oscar N.E. Kjell2,3,\nWhitney Ringwald4, Scott Feltman5, Melissa A. Carr6,\nDimitris Samaras1, Camilo Ruggero7, Benjamin J. Luft6,8,\nRoman Kotov9, H. Andrew Schwartz1,2*\n1Department of Computer Science, Stony Brook University, USA.\n2College of Connected Computing, Vanderbilt University, USA.\n3Department of Psychology, Lund University, Sweden.\n4Department of Psychology, University of Minnesota, USA.\n5Department of Applied Mathematics and Statistics, Stony Brook\nUniversity, USA.\n6Stony Brook World Trade Center Wellness Program, Renaissance\nSchool of Medicine at Stony Brook University, USA.\n7Department of Psychology, University of Texas at Dallas, USA.\n8Department of Medicine, Renaissance School of Medicine at Stony\nBrook University, USA.\n9Department of Psychiatry, Stony Brook University, USA.\n*Corresponding author(s). E-mail(s): pkaliosis@cs.stonybrook.edu;\nhansen.schwartz@vanderbilt.edu;\n†These authors contributed equally to this work.\n1\narXiv:2602.06015v1  [cs.CL]  5 Feb 2026\n"}, {"page": 2, "text": "Abstract\nLarge language models (LLMs) are increasingly being used in a zero-shot fash-\nion to assess mental health conditions, yet we have limited knowledge on what\nfactors affect their accuracy. In this study, we utilize a clinical dataset of natural\nlanguage narratives and self-reported PTSD severity scores from 1,437 individ-\nuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs.\nTo understand the factors affecting accuracy, we systematically varied (i) con-\ntextual knowledge like subscale definitions, distribution summary, and interview\nquestions, and (ii) modeling strategies including zero-shot vs few shot, amount of\nreasoning effort, model sizes, structured subscales vs direct scalar prediction, out-\nput rescaling and nine ensemble methods. Our findings indicate that (a) LLMs\nare most accurate when provided with detailed construct definitions and context\nof the narrative; (b) increased reasoning effort leads to better estimation accu-\nracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond\n70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer\ngenerations; and (d) best performance is achieved when ensembling a supervised\nmodel with the zero-shot LLMs. Taken together, the results suggest choice of\ncontextual knowledge and modeling strategies is important for deploying LLMs\nto accurately assess mental health.\nKeywords: Post Traumatic Stress Disorder, Large Language Models, Prompting\nStrategies, Zero-shot Inference\n1 Introduction\nAccurate mental health assessment is crucial for diagnosis and treatment for condi-\ntions such as the Post-Traumatic Stress Disorder (PTSD). However, access to timely\nand specialized care for such conditions remain limited, with the scarcity of trained\nclinicians often leading to delays in diagnosis and treatment, resulting in aggravated\nmental health conditions [1–3]. Despite having standard, highly-validated self-report\ninstruments, such as the PTSD Checklist (PCL) [4], many clinicians rely instead\non unstructured interviews during intake and treatment sessions to form qualitative\nimpressions [5]. Such narrative accounts are valuable because they allow patients to\ndescribe symptoms in their own words, capturing nuances that structured scales may\nmiss [6, 7]. However, systematically quantifying mental health severity from natural\nlanguage communication has historically been difficult [5, 8].\nRecently, language models have demonstrated accuracy that approaches a theoretical\nupper-bound, suggesting natural language accounts, already valued by patients and\ntherapists, could be the solution to the assessment bottleneck [9, 10]. However, build-\ning on over a decade of natural language processing (NLP) [6, 7, 11–14], such accurate\nmeasurement has mostly been shown over supervised LLMs (either fine-tuning or uti-\nlizing supervised ML over LM-based embeddings). The use of zero-shot LLMs for\nmental health is still developing with less consistent results [15–18] including PTSD\n2\n"}, {"page": 3, "text": "[19]. Motivated by inconsistent evaluation practices in mental-health LLMs, we sys-\ntematically study the role of model’s input knowledge and its assessment procedure\non its assessment performance.\nTo address these gaps and move beyond feasibility studies [19, 20] toward principled\nevaluation, we conduct a systematic study of LLMs for estimating PTSD severity\nfrom open-ended responses. We vary two orthogonal axes that prior works have rarely\nexamined in a controlled way: (1) contextual knowledge provided to the model\n— symptom definitions and validated scale items, elicitation context (participant\ncriteria and interview prompts), and task-level priors about score distributions and\n(2) modeling strategy –— model choice (size and “reasoning” capabilities), task\nformulation (zero- versus few-shot; direct scalar versus subscales predictions), and\npost-processing (calibration via predictive redistribution and ensembling). This study\ndesign responds to calls for standardized [15] and rigorous evaluation of mental-health\nLLMs, where existing studies are often vignette-based, and lack consistent design of\nprompts and modeling choices. We aim to understand the knowledge and process\ncontents in prompts that contributes for reliable severity estimation.\nPrior NLP work on PTSD and mental health often draws on social-media self-\ndisclosures [21–23] or in-lab tasks [24]. These settings are useful for prototyping but\ntypically suffer selection, social desirability, demographic and platform biases [25–\n29], limiting clinical utility. Existing studies also frame PTSD as a binary diagnosis\nrather than a continuous severity construct [19, 21]. However in clinical care, symp-\ntom severity is monitored on a continuum with instruments such as the PCL-5, and\nrecommended cut points vary by population and purpose [30]. Together, these choices\nreduce clinical relevance and make it difficult to judge whether general-purpose LLMs\nactually outperform the current go-to approach — supervised language models trained\nand evaluated on clinically grounded data with validated scales. In contrast, we ground\nassessment in the PTSD Checklist, a widely used DSM-5–aligned continuous sever-\nity measure, and use semi-structured, open-ended patient language rather than social\nmedia posts.\nEvaluations of general-purpose LLMs have often appeared inconsistent [31, 32]. Much\nof this variability can be attributed to differences in the contextual information\nprovided to the model — what background knowledge is provided [33], how it is\nframed [34, 35], and where it appears [36]. Broad NLP evidence shows that LLMs’ per-\nformance shifts with the content and placement of information in long inputs, as well as\nwith how domain knowledge is structured. Surveys of prompting and in-context learn-\ning similarly emphasize that supplying task-relevant knowledge is often decisive [37].\nYet, in clinical mental-health settings, the systematic study of context provision, e.g.,\nsymptom definitions, elicitation details, and task-level priors, has been limited com-\npared to ad-hoc prototypes. Our design makes this issue explicit by varying what the\nmodel knows (contextual knowledge) and how it is used (modeling strategy). In line\nwith emerging work on knowledge-grounding (e.g., types of embedded definitions),\nthis structure yields actionable guidance for safer adoption.\n3\n"}, {"page": 4, "text": "In this work, we (1) present a comprehensive evaluation of eleven state-of-the-art\nLLMs on 1,437 open-ended clinical speech samples linked to PCL-5 severity, (2) sys-\ntematically study factors that isolate the influence of contextual knowledge (symptom\ndefinitions, elicitation context, task-level priors) and modeling strategy (model choice,\nzero- vs. few-shot, direct construct measure vs. tallying measures of its elements,\npost-processing via calibration and ensembling) on performance; and (3) provide a\nhead-to-head comparison with supervised baselines, showing that well-contextualized\nconfigurations — especially when ensembled with a supervised model can outperform\nalternatives, whereas na¨ıve, context-light usage of LLMs can reduce accuracy by up\nto 40%.\n2 Results\nWe evaluated the capabilities of a series of LLMs to estimate PTSD symptom severity\nfrom self-recorded clinical interviews under both zero- and few-shot scenarios. Self-\nreported PCL scores were available for all participants, enabling direct comparison\nwith model predictions. As a baseline, we included a RoBERTa-based [38] regression\nmodel [39]. Additionally, two expert human raters estimated the PTSD severity of\npatients in a subset of 187 interviews. The LLM estimates were compared against the\nself-reported scores on the full dataset. The best performing ones were also compared\nagainst the human expert annotations in the manually annotated subset.\nModel Scale and Prompting Strategy: LLaMA-3.1 Instruct performs well,\nfew-shot prompting and scaling beyond 70B offer limited gains\nWe assessed a wide set of high-capacity LLMs, both open-source and proprietary,\nto examine how PTSD severity estimation performance varies with model size and\nprompting strategy. LLaMA-3.1-Instruct-70B achieved strong performance, yielding\nhigh correlations with self-reported PCL scores. Scaling beyond 70B did not provide\nconsistent gains: larger open models such as DeepSeek-R1 (670B) and proprietary\nsystems with undisclosed sizes (e.g., OpenAI’s 4o-mini and o3-mini) did not surpass\nthe 70B LLaMA-3.1-Instruct model. As illustrated in Figure 1, Pearson correlations\nplateau past the 70B scale for both LLaMA and DeepSeek variants, with MAE show-\ning similar but less stable trends. The only model that performed more accurately\nthan the 70B LLaMA variant in both metrics is OpenAI’s GPT-5. Moreover, few-\nshot prompting did not yield consistent improvements and, in some cases, resulted\nin decreased performance. This was observed, for example, in the cases of DeepSeek-\nR1 and LLaMA-3.1-Instruct-405B. As a basis of comparison, we included a regression\nmodel trained on RoBERTa embeddings, introduced by Kjell et al. [39]. All evaluated\nmodels and configurations are presented in Table 1. A comprehensive evaluation across\nall models, sizes and evaluations is presented in Table S7 (Supplementary Material).\nThinking Step by Step: Inconsistent Performance in Chain-of-Thought\nStyle Reasoning\nWe evaluated the impact of Think-Step-By-Step (TSBS) prompting [42], a vari-\nant of chain-of-thought (CoT) prompting [43] on PTSD severity estimation. TSBS\n4\n"}, {"page": 5, "text": "LLaMA-3.1-Base\nLLaMA-3.1-Instruct\nDeepSeek-R1\n0.0\n0.1\n0.2\n0.3\n0.4\nPearson's r\nLLaMA-3.1-Base\nLLaMA-3.1-Instruct\nDeepSeek-R1\n0\n5\n10\n15\n20\n25\n30\nMAE\nPerformance by Model Variant and Size\nModel Size\n8B\n70B\n405B\n670B\nFig. 1: Effect of model size on PCL score estimation. The improvement rate of\nmodel performance degrades when scaling up from 70B to larger model variants.\nTable 1: Comparison of large LLMs: Performance on PCL score estima-\ntion across zero- and few-shot prompting. We report both Pearson correlation\n(↑) and MAE (↓). * denotes configurations whose performance difference\nagainst the supervised learning baseline [39] is statistically significant follow-\ning a non-parametric bootstrapped resampling test. †Note that the baseline\nmodel consists of a frozen 355M RoBERTa backbone [38] followed by a train-\nable linear layer (1024 parameters).\nModel Variant\nSize\nPearson ↑\nMAE ↓\n0-shot\nfew-shot\n0-shot\nfew-shot\nKjell et al. [39]\n355M†\n0.421\n8.01\nMean Prediction\n1\nN/A\n8.94\nLLaMA-3.1-Base [40]\n70B\n.147\n.346\n31.79\n15.49\nLLaMA-3.1-Instruct [40]\n70B\n.426\n.430\n10.95\n15.93\ngpt-oss\n120B\n.366\n.310\n10.03\n11.13\nLLaMA-3.1-Instruct [40]\n405B\n.426\n.361\n13.80\n14.69\nDeepSeek-R1 [41]\n670B\n.368\n.319\n11.45\n19.58\n4o-mini\nUndisclosed\n.331\n.301\n13.66\n22.75\no3-mini\nUndisclosed\n.383\n.350\n8.81\n10.27\ngpt-5\nUndisclosed\n.441\n.475*\n7.89\n7.95\nencourages the model to verbalize intermediate reasoning steps prior to producing its\nassessment. As shown in Table 2, its effects were inconsistent: TSBS produced small\ngains for the 8B and 405B LLaMA-3.1 models, but slightly degraded performance\nfor the strong performing 70B model, as well as for OpenAI’s 4o-mini. These mixed\nresults suggest that CoT-style prompting does not reliably enhance performance in\n5\n"}, {"page": 6, "text": "this clinical regression task and may even introduce noise for models that already\npossess strong implicit reasoning capabilities.\nTable 2: Effect of Think-Step-By-Step prompting: Per-\nformance comparison between standard and TSBS prompting\nfor 0-shot PCL score estimation. We report Pearson correlation\n(↑) and MAE (↓) against the self-reported PCL scores. The star\nsymbol (*) denotes configurations whose performance difference\nagainst their respective non-TSBS configuration is statistically\nsignificant following a non-parametric bootstrapped resampling\ntest.\nModel\nSize\nTSBS\nPearson ↑\nMAE ↓\nLLaMA-3.1-Instruct\n8B\n✗\n.316\n17.21\nLLaMA-3.1-Instruct\n8B\n✓\n.322\n17.18\nLLaMA-3.1-Instruct\n70B\n✗\n.426\n10.95\nLLaMA-3.1-Instruct\n70B\n✓\n.412\n11.59\ngpt-oss\n120B\n✗\n.366\n10.03\ngpt-oss\n120B\n✓\n.332\n9.92\nLLaMA-3.1-Instruct\n405B\n✗\n.426\n13.80\nLLaMA-3.1-Instruct\n405B\n✓\n.429\n11.35*\n4o-mini\nUndisclosed\n✗\n.331\n13.66*\n4o-mini\nUndisclosed\n✓\n.311\n14.15\nThe effect of reasoning effort: Higher reasoning effort leads to improved\nmodel performance\nSome recent LLMs released by OpenAI provide users with the ability to regulate the\nmodel’s reasoning effort when replying to a given query. We experimented with all\nthree reasoning effort levels (low, medium and high) of 4o-mini and present results\nin Table 3. Notably, reasoning effort has a measurable effect on model performance.\nWhile performance on the Pearson correlation metric remains relatively stable between\nthe low and medium settings, substantial improvements are observed in MAE, sug-\ngesting that increased reasoning effort better calibrates score magnitudes. At the high\nreasoning level, we observe significant gains in both metrics (r = .422 and MAE\n= 8.23 compared to .388 and 9.56 respectively in the low effort level), indicating that\nincreased reasoning effort improves the model’s ability to generate accurate assess-\nments. Notably, the average number of reasoning tokens grows nearly fivefold across\neffort levels, from 453 to 2237, suggesting a steep increase in the model’s internal\nreasoning effort.\nThe effect of Predictive Redistribution: Post-hoc, Distribution-Aware\nAdjustments Improve Regression Accuracy\nA common limitation in regression with LLMs is the mismatch in variance and distri-\nbution between predicted and target values [44]. To address this issue, we experimented\n6\n"}, {"page": 7, "text": "Table 3: Effect of reasoning effort on PCL score estimation. We compare the\nperformance of o3-mini and gpt-5 across all reasoning effort levels, while also report-\ning the average number of reasoning tokens per response. Increasing reasoning effort\nimproves MAE consistently and leads to a notable gain in Pearson correlation at the\nhighest level. The star symbol (*) denotes configurations whose performance difference\nagainst the low reasoning effort configuration is statistically significant following a non-\nparametric bootstrapped resampling test.\nModel\nReasoning Effort\nAvg. # Reasoning Tokens\nPearson ↑\nMAE ↓\no3-mini\nLow\n453.4\n0.388\n9.56\nMedium\n1219.1\n0.383\n8.81*\nHigh\n2237.9\n0.422*\n8.23*\ngpt-5\nLow\n512.2\n0.438\n8.18\nMedium\n1109.9\n0.441\n7.89\nHigh\n1896.4\n0.451\n7.72\nwith a distribution-aware adjustment method known as predictive redistribution [45],\nwhich we explain in Section 4.3. Table 4 presents the effect of predictive redistribution\nacross our strongest models and prompting configurations. The most substantial gains\nappear in MAE, with post-redistribution errors consistently lower in both zero- and\nfew-shot settings. For example, LLaMA-3.1-Instruct (70B) improves from an MAE of\n10.95 to 8.25 in the zero-shot condition, achieving the best overall result. In contrast,\nPearson correlation shows only modest gains in most cases, suggesting that redistri-\nbution primarily improves the calibration of absolute predictions rather than their\nrelative ranking. Nonetheless, the highest correlation score (r = .454) is achieved in\nthe 3-shot setting with LLaMA-3.1-Instruct after applying redistribution.\nSubscale-based vs. Direct Construct Predictions: The Role of Contextual\nCues and Scoring Format in PTSD Severity Estimation\nWe evaluated two formulations for PTSD severity estimation: subscale-based predic-\ntion, in which the model predicts the four PCL subscales (Re-experiencing, Avoidance,\nDysphoria, Hyperarousal) and we aggregate them into a total score, and direct con-\nstruct prediction, in which the model outputs a single severity value. The former\nmirrors the structure of the PCL and aligns with clinical practice, whereas the lat-\nter simplifies the task. All experiments used LLaMA-3.1-Instruct-70B, which offered\nstrong performance at substantially lower computational cost than 405B. We tested a\nwide range of contextual cue configurations, which we present extensively in Table S8\n(Supplementary Material).\nTable 5 presents results across contextual cue configurations for both the subscale-based\nand direct construct prediction settings. In the subscale-based setting, Pearson corre-\nlations range from .367 to .455, with vast improvements in MAE following predictive\nredistribution. The best configuration for this setting (w/ subscales, 3-shot) achieved\nr = .455, surpassing the regression-based baseline [39]. Other cues, such as study con-\ntext, item descriptions, or questionnaire metadata, provided smaller benefits, whereas\n7\n"}, {"page": 8, "text": "Table 4: Evaluation of predictive redistribution approach. We report results for\n70B-scale and proprietary models across both zero- and few-shot settings. Predictive\nredistribution yields substantial improvements in MAE and modest gains in Pearson\ncorrelation. The best overall performance is achieved by LLaMA-3.1-Instruct (70B) with\nredistribution in the 3-shot condition. The star symbol (*) denotes configurations whose\nperformance post predictive redistribution is statistically significantly better following\na non-parametric bootstrapped resampling test.\nModel Variant\nSize\nPearson (↑)\nMAE (↓)\n0-shot 3-shot 0-shot\n3-shot\n0-shot 3-shot 0-shot\n3-shot\nOriginal\nRedistributed\nOriginal\nRedistributed\nLLaMA-3.1-Base\n70B\n.147\n.346\n.191*\n.345\n31.79\n15.49\n9.42*\n9.51*\nLLaMA-3.1-Instruct\n70B\n.426\n.430\n.434*\n.454*\n10.95\n15.93\n8.25*\n8.22*\nLLaMA-3.1-Instruct w/ TSBS 70B\n.412\n.396\n.421\n.401\n11.59\n12.58\n8.55*\n8.68*\n4o-mini\nN/A\n.331\n.301\n.336\n.310\n13.66\n22.75\n9.23*\n9.37*\n4o-mini w/ TSBS\nN/A\n.311\n.318\n.314\n.325\n14.15\n18.94\n9.33*\n9.37*\no3-mini\nN/A\n.383\n.350\n.386\n.351\n8.81\n10.27\n8.85\n9.04*\ngpt-5\nN/A\n.441\nn/a\n.475\n.476\n7.89\n7.95\n8.49\n8.17\nnot providing any contextual cues led to a performance drop. The direct construct\nprediction setting, although it deviates from the formal clinical structure of the PCL,\nyielded stronger performance under multiple configurations. Prompts including study\ncontext (r = .480) or distributional information (r = .482, MAE = 7.80) led to the\nbest performance, with item-level cues and subscale definitions also performing well.\nModel Ensembles: Leveraging Complementary Models for Improved\nPTSD Severity Estimation\nTo examine whether ensembles can further improve PTSD severity estimation, we con-\nstructed a wide range of combinations by averaging predictions from complementary\nsystems, including subscale-based models, direct scalar prediction models, and our\nembedding-based baseline [39]. Figure 2 summarizes ensemble performance in terms\nof Pearson correlation and total number of ensemble parameters. Several clear trends\nemerge. Ensembles built from models prompted to predict severity directly (blue-\nedged circular markers) generally outperform those relying on subscale prediction\n(brown-edged triangular markers). The strongest correlation (r = 0.565) is achieved\nby combining GPT-5 (under direct prediction) with the embedding-based baseline.\nIn contrast, ensembles composed of highly similar models show limited improvement.\nFinally, we observe diminishing returns when ensembling large numbers of models\n(n ≥4), likely reflecting conflicting predictions that dilute the benefits of diversity.\nOverall, these findings suggest that ensembles can offer measurable gains in mental\nhealth assessment performance [46–48].\nComparison with Human Raters\nTo contextualize LLM performance, we additionally collected PTSD severity estimates\nfrom two expert human raters. The human raters were also informed about the broader\n8\n"}, {"page": 9, "text": "Table 5: Performance of LLaMA-3.1-70B-Instruct across contex-\ntual cue configurations for PTSD severity estimation. We report both\noriginal predictions and redistributed scores for Pearson correlation (↑) and\nMAE (↓), under zero-shot and 3-shot prompting. The top section reflects\nsubscale-based predictions, and the bottom section shows direct predictions.\nThe star symbol (*) denotes configurations statistically significantly better\nthan the w/o contextual cues counterpart (bootstrapped resampling test).\nContextual Cue Configuration\nPrompt\nPearson ↑\nMAE ↓\nOrig.\nRedistr.\nOrig.\nRedistr.\nSubscale-based Prediction\nw/o contextual cues\n0-shot\n.407\n.417\n13.91\n8.50\n3-shot\n.377\n.388\n15.03\n9.18\nw/ evidence\n0-shot\n.412\n.414\n13.37\n9.84\n3-shot\n.414*\n.415*\n12.95*\n9.32\nw/ subscales\n0-shot\n.426\n.434*\n10.95\n8.25\n3-shot\n.430*\n.455*\n15.93\n8.22\nw/ questions\n0-shot\n.417*\n.419*\n16.77\n11.22\n3-shot\n.367\n.376\n17.04\n11.79\nw/ study context\n0-shot\n.402\n.411\n12.10*\n8.65\n3-shot\n.383\n.400\n14.01*\n8.90\nw/ items\n0-shot\n.416*\n.430*\n12.00*\n8.40\n3-shot\n.400*\n.406\n13.74*\n8.84\nw/ distributional information\n0-shot\n.407\n.412\n9.62*\n8.61\n3-shot\n.414*\n.422\n13.01*\n9.12\nw/ (evidence, subscales, questions)\n0-shot\n.392\n.406\n17.13\n8.61\n3-shot\n.401\n.402\n12.98\n9.48\nDirect Prediction\nw/o contextual cues\n0-shot\n.443\n.450\n11.02\n8.37\n3-shot\n.439\n.442\n12.60\n8.36\nw/ evidence\n0-shot\n.467*\n.468\n10.39*\n8.08*\n3-shot\n.463\n.464\n11.15*\n8.18\nw/ subscales\n0-shot\n.463\n.469\n10.03*\n8.18\n3-shot\n.436\n.439\n13.55\n8.45\nw/ questions\n0-shot\n.412\n.415\n12.34\n8.43\n3-shot\n.438\n.441\n11.77*\n8.27\nw/ study context\n0-shot\n.470*\n.480*\n9.66*\n8.05*\n3-shot\n.404\n.408\n12.43\n8.62\nw/ items\n0-shot\n.452\n.456\n10.35*\n8.33\n3-shot\n.440\n.443\n11.03*\n8.27\nw/ distributional information\n0-shot\n.452\n.454\n7.80*\n8.08*\n3-shot\n.478*\n.482*\n9.18*\n8.03*\nw/ (evidence, subscales, questions)\n0-shot\n.413\n.417\n12.29\n8.42\n3-shot\n.450\n.453\n11.61*\n8.18\n9\n"}, {"page": 10, "text": "0.40\n0.42\n0.44\n0.46\n0.48\n0.50\n0.52\n0.54\n0.56\nPearson Correlation (r) \n0B\n70B\n140B\n475B\n670B\n740B\nUndisclosed\nTotal Ensemble Parameters\nEnsemble Performance vs. Size\n4o-mini\nDeepSeek-R1\nKjell et al.\nLLaMA-3.1-Instruct-405B\nLLaMA-3.1-Instruct-70B\nLLaMA-3.1-Instruct-70B w/ evidence\nLLaMA-3.1-Instruct-70B w/ study context\ngpt-5\no3-mini\nSubscale-based scoring\nDirect scoring\nKjell et al.\nFig. 2: Model ensembles performance on PCL score estimation. Each marker\nshows an ensemble, positioned by Pearson correlation (x-axis, ↑better) and MAE\n(y-axis, ↓better). Circles denote direct score predictions; triangles denote subscale-\nbased predictions. Marker slices indicate the constituent models (colors per legend),\nand marker size reflects ensemble size.\nstudy context, including the fact that participants were WTC responders, in order\nto approximate the background knowledge available in real clinical evaluations. Each\nrater independently reviewed 187 interview transcripts and provided a predicted PTSD\nseverity score. We evaluated their predictions against the self-reported PCL scores to\nestablish a human baseline. We present the results in Figure 3. The human raters\nachieved a correlation of r = .44, comparable to our supervised RoBERTa model (r =\n.45) but notably lower than the best-performing LLMs: LLaMA-3.1-70B achieved r =\n.53, and GPT-5 reached r = .59. To assess whether these differences were statistically\nsignificant, we compared each model’s correlation with that of the human raters using\na paired non-parametric bootstrap test. GPT-5 significantly outperformed the human\nraters (p = 0.001), as did LLaMA-3.1-70B (p = 0.049), whereas RoBERTa did not\ndiffer significantly from human performance (p = 0.62). The human raters achieved\nvery high reliability (ICC = .96 based on independent double-rating of 187 transcripts).\nThese results underscore that frontier LLMs are capable of matching or exceeding the\naccuracy of trained human raters in zero-shot PTSD severity estimation.\n3 Discussion\nBy systematically evaluating eleven state-of-the-art LLMs, we found that LLMs\nprovisioned with specific contextual cues and operated under optimal modeling\n10\n"}, {"page": 11, "text": "0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nCorrelation (r)\nHuman Raters\nKjell et al.\nLLaMA-3.1-70B\nGPT-5\n0.44\n0.45\n0.53\n0.59\nPTSD Severity Prediction Accuracy: Models vs Human Raters\nFig. 3: Comparison of PTSD severity estimation accuracy across models and human\nraters. Bars show Pearson correlation (r) with ground-truth PCL scores on the held-\nout set of 187 interviews. GPT-5 and LLaMA-3.1-70B outperform both human raters\nand the supervised RoBERTa baseline [39].\nstrategies, can result in higher agreement with self-reported PCL scores than tra-\nditional supervised approaches and trained psychologists. Although modern LLMs\nsurpassed traditional supervised model, ensembling different LLMs with the super-\nvised model produced the highest agreement with self-reported scores, highlighting\nthe complementary nature of traditional AI methods for language based mental\nhealth assessments [49]. These results highlight the potential of carefully configured\nLLMs to support scalable and timely mental health assessments [50–53], particu-\nlarly as pre-screening, monitoring or decision-support tools that can reduce clinician\nburden [54, 55].\nNarrative language offers the most natural and comprehensive ways for individuals\nto express psychological distress. Unlike self-report instruments, such as the PTSD\nChecklist [4], unstructured interviews allow people to describe their symptoms in their\nown words, capturing nuances in emotion, context, and lived experience that struc-\ntured instruments may overlook. This is why, in practice, many clinicians continue to\nrely on unstructured interviews to form mental health assessments [5]. Language based\nassessments using LLMs can enhance this practice by offering a scalable, automated\nway to interpret such narrative accounts. Automatic LLM-based open-ended language\nassessments can help reduce delays caused by limited clinician availability [56, 57] and\nserve as a bridge for individuals hesitant to seek care due to social stigma [58, 59].\nOur findings offer key insights into how contextual knowledge and modeling strategies\naffect LLM-based PTSD severity estimation. First, while increasing model size gen-\nerally improves accuracy, the gains diminish beyond 70B parameters for open-weight\nLLMs, indicating a saturation point consistent with trends observed in broader NLP\ntasks [60, 61], and implying that the scaling laws observed in other domains [62] may\n11\n"}, {"page": 12, "text": "not hold for complex clinical estimation tasks like this one. Notably, zero-shot prompt-\ning often matched or exceeded few-shot performance, aligning with recent studies\nshowing limited marginal gains from few-shot examples in tasks that do not require\nmathematical or symbolic reasoning [63, 64]. Moreover, our results reveal a perfor-\nmance gap between open- and closed-weight models. While earlier open-source models\nperformed competitively, GPT-5 significantly outperformed all others, underscoring\nthat proprietary models currently lead on complex clinical inference tasks [65, 66].\nReasoning has increasingly been recognized as a core determinant of LLM perfor-\nmance across complex tasks. Recent work has introduced a range of techniques to elicit\nor enhance reasoning, spanning from simple “think-step-by-step” (TSBS) prompts to\nmore structured CoT prompting. We experimented with both approaches and found\nthat neither yielded consistent gains in predictive performance for this clinical task.\nThis outcome aligns with recent studies on underthinking and overthinking of LLMs,\nwhich suggest that forcing longer or more elaborate thinking chains does not nec-\nessarily improve performance [67, 68]. In fact, it may even degrade performance by\nencouraging the model to generate spurious explanations [69].\nMore recently, there have been developed frontier models explicitly trained for\nadvanced reasoning [41, 70] by specialized post-hoc training with reinforcement\nlearning strategies [71–73]. The most recent one, GPT-5, showed the most reliable\nperformance on our task. The latest OpenAI models, including GPT-5, have intro-\nduced configurable reasoning effort, allowing users to explicitly request low, medium,\nor high reasoning depth, reflecting a shift toward reasoning as a controllable feature\nrather than a static property of the model. This mechanism led to a more consistent\npattern: while low and medium reasoning effort yielded similar correlations, enabling\nhigh reasoning effort reliably reduced absolute error and improved the correlation\nmetric. Although the internal mechanism behind this configuration is not publicly dis-\nclosed, our results indicate that increasing the model’s reasoning effort can enhance\nperformance on clinical estimation tasks.\nBeyond model choice, scale and reasoning configurations, we also explored a wide range\nof task design decisions, particularly the contextual information provided to the model\nand the structure of the prediction task itself (Table 5). Certain forms of contextual\nknowledge reliably improved performance over the no-context baseline, such as the\ninclusion of subscale definitions and PCL item-level descriptions (see Fig. S5 in the\nSupplementary Material), which appear to help the model anchor its judgments more\naccurately. Other forms of context, such as study-level descriptions, distributional\npriors, or the full questionnaire items, showed mixed effects: in some cases they yielded\nmodest gains, while in others they had adverse impact, suggesting that not all auxiliary\ninformation is beneficial for LLMs in this setting. We also compared two task framings:\n(a) predicting subscale scores individually and (b) directly predicting the overall PCL\nscore. Interestingly, unlike the clinician workflow, which is structured around self-\nreported subscale scores, LLMs achieved stronger performance when asked to predict\nthe total score directly. In addition, post-hoc redistribution and simple ensembling\nhelped further calibrate predictions, reducing variance and improving alignment with\nself-reported PCL scores.\n12\n"}, {"page": 13, "text": "This study also comes with limitations. The language samples were drawn from a sin-\ngle interview format, so it is unclear how well the models would generalize to other\nconversational settings [74]. The sample consisted exclusively of WTC responders,\nleaving open questions about generalization to other trauma-exposed groups [75, 76].\nFinally, our target objective was the PCL score, and model performance may differ\nwhen predicting interviewer-based assessments such as formal diagnoses, despite PCL\nbeing the most widely used PTSD severity measure [77, 78]. A further limitation is\nthat our analysis relied solely on textual input. Prior work suggests that acoustic and\nvisual modalities can contribute unique information to clinical assessments, including\nindicators of affect, cognitive load, and physiological arousal [79, 80]. Incorporating\nmultimodal signals may therefore improve both predictive accuracy and the ecological\nvalidity of LLM-based estimators [81]. Finally, our study does not address how models\ninternally conceptualize PTSD or arrive at their predictions. Recent work highlights\nthe importance of analyzing model reasoning pathways and uncovering latent clini-\ncal constructs used by LLMs [20, 82], underscoring a need for future work to pair\nperformance evaluations with interpretability analyses.\nThis study provides a systematic analysis of the factors that shape LLM perfor-\nmance in zero- and few-shot PTSD severity estimation. Using a clinical dataset of\n1,437 interviews, we systematically varied contextual knowledge, task framing, model\nsize, reasoning configuration, output rescaling and ensembling strategies to under-\nstand which elements reliably influence accuracy. Across these experiments, several\npatterns emerged consistently: models benefited substantially from certain types of\ncontextual information such as subscale definitions and study context; direct scalar\nprediction outperformed the clinically common practice of aggregating individual sub-\nscale scores; increased reasoning effort did not uniformly improve model performance;\nmodel performance plateaued beyond 70B parameters with few-shot prompting offer-\ning limited additional gains; and the strongest results arose from ensembling zero-shot\nLLM predictions with a supervised baseline. Together, these findings highlight the\ngrowing capabilities of LLMs for mental health assessment, while underscoring that\neffective deployment depends not only on model choice but also on the careful design\nof contextual inputs and modeling strategies.\n4 Methods\n4.1 Data\nCollection\nAll participants take part in the Stony Brook WTC Health and Wellness Program [83].\nEach participant completed a standardized set of self-recorded questions. Following\nprior work [39], we excluded recordings with fewer than 150 words, yielding a final\nsample of 1,437 participants. The cohort was predominantly male (92.6%), with ages\nranging from 38 to 90 years (mean = 58.1). Self-recorded interviews averaged 7.5\nminutes in duration (SD = 4.1) and ranged between 1.1 and 43.0 minutes. Additional\ndescriptive statistics are provided in Table S9 (Supplementary Material).\n13\n"}, {"page": 14, "text": "Measures\nParticipants completed an automated clinical interview during a visit, responding\naloud to questions displayed on a screen in a private room. The questions encouraged\nreflection on past, present, and future experiences, including personal challenges and\nthe impact of the 9/11 events, and instructed participants to speak for at least 60\nseconds without reading the prompt aloud. The interview protocol was refined across\nthree iterations to encourage more detailed responses; additional description is pro-\nvided in Kjell et al. [39]. Participants also completed the PTSD Checklist (PCL) [78],\na 17-item questionnaire designed to assess the severity of PTSD symptoms. Each item\nis rated on a 5-point Likert scale, from 1 (“not at all severe”) to 5 (“extremely severe”).\nProcedure\nVideo recordings were collected in a clinical setting at the Stony Brook WTC Health\nand Wellness Program. All participants provided informed consent and were made\naware of their right to withdraw at any time. They were guided through the automated\ninterview procedure by a research assistant.\nTranscription\nWe transcribed each video interview into text using Whisper [84]. In addition, the\ntranscripts of our study often contain the questions the participants were called to\nanswer, since some of them read them aloud during the interview process.\n4.2 Experiment Design\nPrompt\nThe input provided to the model varies depending on the selected prompt configu-\nration, which controls what contextual information is included (see Table 5). Each\nprompt is composed of a set of core components, present across all configurations, and\noptional plug-in components that introduce additional information or instructions.\nFigures S4 and S6 (Supplementary Material) illustrate the full prompt layout for the\nsubscale-based and direct construct prediction configurations respectively, highlight-\ning the core components (colored boxes) and the three most commonly used optional\nplug-ins (gray boxes/text spans). The core components include:\n(1) the Instructions box (yellow), which defines the overall task for the model.\n(2) the Scoring System box (green), which outlines a 5-point scale (0–4) with descrip-\ntive anchors for each level of symptom severity;\n(3) the Step-by-Step Procedure box (blue), which guides the model through the\nprocess of symptom detection and score assignment; and\n(4) the Expected Output box (red), which specifies the required JSON-like format\nof the model’s output.\nIn contrast, the optional plug-ins (gray boxes/text spans) act as modular additions\nthat can be toggled on or off depending on the experimental configuration. They\nare rendered in gray and serve to provide additional context or elicit more grounded\nreasoning. Additional plug-in elements explored in later configurations are presented in\n14\n"}, {"page": 15, "text": "Figure S5 (Supplementary Material). The model performance across all configurations\nis presented in Table 5. A summary of all plug-in options is provided in Table S8\n(Supplementary Material).\nHandling of Failure Cases\nMost model outputs followed the expected JSON format, but a small number failed\ndue to malformed or incomplete structure (e.g., altered keys, missing brackets) or\nby returning free-form text rather than JSON. We addressed these cases using a\nlightweight validator and a fallback regex-based parser to recover severity scores when\npossible. Examples that could not be recovered were excluded. Such failures were rare;\nfewer than 0.5% across all configurations.\nHyperparameters\nAll models were run with temperature set to 0.0 to ensure deterministic outputs and\navoid repeated runs, which would be computationally expensive for locally hosted\nmodels and costly for closed-source APIs. This allowed performance differences to\nbe attributed to prompt design rather than decoding randomness. Other decoding\nsettings were kept fixed across experiments (top-p = 1.0, frequency penalty = 0.1,\npresence penalty = 0.0, max tokens = 3000).\nHuman Raters\nTo establish a human baseline for comparison, PTSD severity ratings were also col-\nlected from two clinical psychology graduate students (“human raters”). Both raters\nhad prior training in diagnostic interviewing and formal instruction in DSM-5 PTSD\ncriteria, and they were supervised throughout the process by a licensed clinical psychol-\nogist. Before beginning the main rating task, they completed a structured calibration\nphase involving 70 interviews for which the true PCL scores were revealed after each\nrating. This phase helped the raters familiarize themselves with the study population,\nthe structure and content of the transcripts, and the scoring rubric used for PTSD\nseverity. Following calibration, the raters independently evaluated 187 transcripts\nwithout access to ground truth. To prevent reliability drift, the raters discussed after\nevery 50 transcripts to resolve discrepancies, without modifying previously assigned\nscores. This procedure provided tight reliability control across the full rating period.\n4.3 Analysis\nSelection of few-shot examples\nTo construct the few-shot configurations, we provided the model with three exem-\nplars, each consisting of the speaker’s transcript and subscale-level or overall scores.\nWe experimented with two strategies for selecting these examples from the data pool.\nThe first, range sampling, ensured that the chosen examples collectively spanned a\nbroad spectrum of PTSD severity scores (e.g., low, moderate, and high). The sec-\nond, percentile sampling, selected instances corresponding to specific quantiles (e.g.,\n25th, 50th, and 90th percentiles) of the empirical score distribution, ensuring repre-\nsentativeness with respect to the population-level symptom variability. All exemplars\n15\n"}, {"page": 16, "text": "were drawn from a held-out set. In preliminary experiments, the percentile sampling\nstrategy yielded more consistent improvements in predictive performance compared\nto range-based selection, thus we proceeded with this for the remainder of the study.\nA comparison of the two strategies is provided in Table S6 (Supplementary Material).\nBaseline Method\nTo better compare the performance of LLM-based scoring approaches, we followed\na supervised learning method [39], built on top of a frozen RoBERTa backbone and\ntrained using mean squared error (MSE) loss on the self-reported PCL scores. It was\nevaluated under the same metrics as our experimental setup. While not capable of\nproviding natural language justifications, this baseline serves as a strong text regression\nreference for direct comparison.\nLikert Scale Adjustment\nWhile the original PTSD Checklist (PCL) employs a 1–5 Likert scale, we modified the\nscoring prompt to instead use a 0–4 scale. This adjustment was made to better align\nwith the semantics of the task, as assigning a score of “0” conveys a more definitive\nsense of symptom absence (i.e., “not at all severe”) compared to “1” [85].\nPredictive Redistribution\nMany regression-based LLMs exhibit the tendency to shrink the variance of their out-\nput distributions, often as a side effect of regularization and penalization mechanisms\n[86, 87]. This shrinkage typically pushes predictions toward the mean and produces\nmore normally distributed outputs than the original training targets. While such\ncompression has little effect on correlation-based metrics, it undermines clinical inter-\npretability: for example, predictions may rarely exceed common diagnostic thresholds\n(e.g., PCL ≥44), limiting usefulness for identifying severe cases. To address this, we\napplied a Predictive Redistribution [45], a two-stage procedure that stabilizes vari-\nance before training and restores the target distribution after inference. Specifically,\nwe used the Anscombe transform to rescale scores prior to prediction (Eq.1), and then\nremapped outputs to the empirical mean and variance of PCL scores (Eq.2). This\nadjustment corrects for shrinkage when the underlying score distribution is known,\nthough it is not appropriate in settings lacking prior knowledge about the distribution.\ny′ = 2 ·\nq\ny + 3\n8\n(1)\ny =\n\u0012ypred −µypred\nσypred\n\u0013\n· σy + µy\n(2)\nReasoning Effort\nOpenAI’s o3 and GPT-5 model families allow users to specify a reasoning effort level\nwhen querying the model. This parameter controls how much internal reasoning effort\nthe model performs in order to reach its final answer. Available levels include low,\nmedium, and high, corresponding to increasing amounts of intermediate reasoning. The\nspecific mechanisms through which reasoning effort is regulated remain proprietary,\nand no detailed public documentation exists about how it influences model behavior.\n16\n"}, {"page": 17, "text": "Statistical Significance Testing\nWe assessed whether differences between model performances were statistically sig-\nnificant using non-parametric bootstrap resampling (n = 1,000). For each metric (r,\nMAE), we repeatedly sampled the evaluation set with replacement and computed the\npairwise performance difference between two models. This produced an empirical dis-\ntribution of differences, from which we derived the 95% confidence interval (CI). A\ndifference was deemed statistically significant if the CI excluded zero, corresponding\nto rejecting the null hypothesis at the p < 0.05 level.\n5 Declarations\nData Availability\nThe data analysed for this project are not publicly available due to privacy reasons.\nHowever, they can be made available from the corresponding author upon request\nfollowing an anonymization procedure.\nCode Availability\nThe code for this project will be made available through a GitHub repository. The\nrepository includes scripts for prompting LLMs, processing model outputs, com-\nputing evaluation metrics, performing bootstrap significance testing and computing\nensembles. Instructions for environment setup will also be provided.\nEthics\nThis study was approved by the Stony Brook University Institutional Review\nBoard (approval number IRB2022-00377) and was conducted in accordance with the\nDeclaration of Helsinki. This study does not involve clinical trial.\nFunding\nThis work was supported in part by grants U01OH012476 and R21OH012614 from\nCDC/NIOSH to Drs. Andrew Schwartz and Roman Kotov.\nAcknowledgements\nWe express our gratitude to the rescue and recovery workers of the World Trade\nCenter attacks for their selfless dedication following the WTC attacks and for par-\nticipating in this continuous research. Our thanks also extend to the clinical staff of\nthe World Trade Center Medical Monitoring and Treatment Programs for their unwa-\nvering commitment and to the labor and community organizations for their ongoing\nsupport.\nAuthor Contributions\nAVG, PK, ONEK, RK and HAS designed the research; PK, AVG, and ONEK built the\nanalytic tools; SF and MAC were involved in data collection; Psychology experts WR,\nONEK, and RK provided expert feedback throughout the process; CR coordinated\nhuman annotation and curated the expert rater data; PK, AVG, ONEK, WR, SF,\n17\n"}, {"page": 18, "text": "MAC, DS, CR, BJL, RK and HAS performed the analysis and wrote the manuscript;\nPK and AVG made all the display items; All authors made the final decision to submit\nthe manuscript.\nCompeting Interests\nONEK co-founded and holds shares in a start-up that uses language-based assessments\nto diagnose mental health problems. The authors report no additional biomedical\nfinancial interests or potential conflicts of interest.\nReferences\n[1] Owusu, J., Wang, L., Chen, S.-Y., Wickham, R., Michael, S., Bahrassa, N., Varra,\nA., Lee, J., Chen, C., Lungu, A.: Real-world evaluation of an evidence-based\ntelemental health program for ptsd symptoms. Scientific Reports 15 (2025) https:\n//doi.org/10.1038/s41598-024-83144-6\n[2] Morland, L.A., Wells, S.Y., Glassman, L.H., Greene, C.J., Hoffman, J.E., Rosen,\nC.S.: Advances in ptsd treatment delivery: Review of findings and clinical con-\nsiderations for the use of telehealth interventions for ptsd. Current Treatment\nOptions in Psychiatry 7, 221–241 (2020)\n[3] Kuhn, E., Owen, J.E.: Advances in ptsd treatment delivery: the role of digital\ntechnology in ptsd treatment. Current Treatment Options in Psychiatry 7, 88–102\n(2020)\n[4] Blevins, C.A., Weathers, F.W., Davis, M.T., Witte, T.K., Domino, J.L.: The\nposttraumatic stress disorder checklist for dsm-5 (pcl-5): Development and initial\npsychometric evaluation. Journal of traumatic stress 28(6), 489–498 (2015)\n[5] Aboraya, A., Nasrallah, H., Elswick, D., Ahmed, E., Estephan, N., Aboraya,\nD., Berzingi, S., Chumbers, J., Berzingi, S., Justice, J., Zafar, J., Dohar, S.:\nMeasurement-based care in psychiatry-past, present, and future. Innovations in\nclinical neuroscience 15, 13–26 (2018)\n[6] Crespo, M., Fern´andez-Lansac, V.: Memory and narrative of traumatic events: A\nliterature review. Psychological trauma : theory, research, practice and policy 8,\n149–156 (2015) https://doi.org/10.1037/tra0000041\n[7] Schnurr, P., Chard, K., Ruzek, J., Chow, B., Resick, P., Foa, E., Marx, B., Fried-\nman, M., Bovin, M., Caudle, K., Castillo, D., Curry, K., Hollifield, M., Huang,\nG., Chee, C., Astin, M., Dickstein, B., Renner, K., Clancy, C., Shih, M.-C.:\nComparison of prolonged exposure vs cognitive processing therapy for treatment\nof posttraumatic stress disorder among us veterans a randomized clinical trial.\nJAMA Network Open 5 (2022) https://doi.org/10.1001/jamanetworkopen.2021.\n36921\n18\n"}, {"page": 19, "text": "[8] Sikstr¨om, S., P˚alsson H¨o¨ok, A., Kjell, O.: Precise language responses versus easy\nrating scales—comparing respondents’ views with clinicians’ belief of the respon-\ndent’s views. PLOS ONE 18, 0267995 (2023) https://doi.org/10.1371/journal.\npone.0267995\n[9] Kjell, O.N., Sikstr¨om, S., Kjell, K., Schwartz, H.A.: Natural language analyzed\nwith ai-based transformers predict traditional subjective well-being measures\napproaching the theoretical upper limits in accuracy. Scientific reports 12(1),\n3918 (2022)\n[10] Kjell, O.N., Kjell, K., Schwartz, H.A.: Beyond rating scales: With targeted evalu-\nation, large language models are poised for psychological assessment. Psychiatry\nResearch 333, 115667 (2024)\n[11] Son, Y., Clouston, S., Kotov, R., Eichstaedt, J., Bromet, E., Luft, B., Schwartz,\nH.: World trade center responders in their own words: predicting ptsd symptom\ntrajectories with ai-based language analyses of interviews. Psychological Medicine\n53, 1–9 (2021) https://doi.org/10.1017/S0033291721002294\n[12] Eichstaedt, J.C., Smith, R.J., Merchant, R.M., Ungar, L.H., Crutchley, P.,\nPreot¸iuc-Pietro, D., Asch, D.A., Schwartz, H.A.: Facebook language predicts\ndepression in medical records. Proceedings of the National Academy of Sci-\nences 115(44), 11203–11208 (2018) https://doi.org/10.1073/pnas.1802331115\nhttps://www.pnas.org/doi/pdf/10.1073/pnas.1802331115\n[13] Varadarajan, V., Sikstr¨om, S., Kjell, O., Schwartz, H.A.: ALBA: Adaptive\nlanguage-based assessments for mental health. In: Duh, K., Gomez, H., Bethard,\nS. (eds.) Proceedings of the 2024 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (Vol-\nume 1: Long Papers), pp. 2466–2478. Association for Computational Linguistics,\nMexico City, Mexico (2024). https://aclanthology.org/2024.naacl-long.136/\n[14] Mangalik, S., Eichstaedt, J.C., Giorgi, S., Mun, J., Ahmed, F., Gill, G., V. Gane-\nsan, A., Subrahmanya, S., Soni, N., Clouston, S.A., et al.: Robust language-based\nmental health assessments in time and space through social media. NPJ Digital\nMedicine 7(1), 109 (2024)\n[15] Hua, Y., Na, H., Li, Z., Liu, F., Fang, X., Clifton, D., Torous, J.: A scoping review\nof large language models for generative tasks in mental health care. npj Digital\nMedicine 8(1), 230 (2025)\n[16] Brickman, J., Gupta, M., Oltmanns, J.R.: Large language models for psychologi-\ncal assessment: A comprehensive overview. Advances in Methods and Practices in\nPsychological Science 8(3), 25152459251343582 (2025) https://doi.org/10.1177/\n25152459251343582\n[17] Stade, E., Stirman, S., Ungar, L., Boland, C., Schwartz, H., Yaden, D., Sedoc,\n19\n"}, {"page": 20, "text": "J., DeRubeis, R., Willer, R., Eichstaedt, J.: Large language models could change\nthe future of behavioral healthcare: a proposal for responsible development\nand evaluation. npj Mental Health Research 3 (2024) https://doi.org/10.1038/\ns44184-024-00056-z\n[18] Dergaa, I., Fekih-Romdhane, F., Hallit, S., Loch, A., Glenn, J., Fessi, M.,\nBen Aissa, M., Souissi, N., Guelmemi, N., Swed, S., EL Omri, A., Bragazzi, N.,\nBen Saad, H.: Chatgpt is not ready yet for use in providing mental health assess-\nment and interventions. Frontiers in Psychiatry 14 (2024) https://doi.org/10.\n3389/fpsyt.2023.1277756\n[19] Tu, S., Powers, A., Merrill, N., Fani, N., Carter, S., Doogan, S., Choi, J.D.:\nAutomating PTSD diagnostics in clinical interviews: Leveraging large language\nmodels for trauma assessments. In: Proceedings of the 25th Annual Meeting of\nthe Special Interest Group on Discourse and Dialogue, pp. 644–663. Association\nfor Computational Linguistics, Kyoto, Japan (2024). https://doi.org/10.18653/\nv1/2024.sigdial-1.55 . https://aclanthology.org/2024.sigdial-1.55/\n[20] Ganesan, A.V., Varadarajan, V., Lal, Y.K., Eijsbroek, V.C., Kjell, K., Kjell, O.N.,\nDhanasekaran, T., Stade, E.C., Eichstaedt, J.C., Boyd, R.L., et al.: Explain-\ning gpts’ schema of depression: A machine behavior analysis. arXiv preprint\narXiv:2411.13800 (2025)\n[21] Coppersmith, G., Harman, C., Dredze, M.: Measuring post traumatic stress dis-\norder in twitter. Proceedings of the International AAAI Conference on Web and\nSocial Media 8(1), 579–582 (2014) https://doi.org/10.1609/icwsm.v8i1.14574\n[22] Coppersmith, G., Leary, R., Crutchley, P., Fine, A.: Natural language processing\nof social media as screening for suicide risk. Biomedical Informatics Insights 10,\n117822261879286 (2018) https://doi.org/10.1177/1178222618792860\n[23] Coppersmith, G., Dredze, M., Harman, C.: Quantifying mental health signals in\nTwitter. In: Resnik, P., Resnik, R., Mitchell, M. (eds.) Proceedings of the Work-\nshop on Computational Linguistics and Clinical Psychology: From Linguistic\nSignal to Clinical Reality, pp. 51–60. Association for Computational Linguis-\ntics, Baltimore, Maryland, USA (2014). https://doi.org/10.3115/v1/W14-3207 .\nhttps://aclanthology.org/W14-3207/\n[24] Burdisso, S., Reyes-Ram´ırez, E., Villatoro-tello, E., S´anchez-Vega, F., Lopez Mon-\nroy, A., Motlicek, P.: DAIC-WOZ: On the validity of using the therapist’s prompts\nin automatic depression detection from clinical interviews. In: Proceedings of\nthe 6th Clinical Natural Language Processing Workshop, pp. 82–90. Association\nfor Computational Linguistics, Mexico City, Mexico (2024). https://doi.org/10.\n18653/v1/2024.clinicalnlp-1.8 . https://aclanthology.org/2024.clinicalnlp-1.8/\n[25] Shah, D.S., Schwartz, H.A., Hovy, D.: Predictive biases in natural language pro-\ncessing models: A conceptual framework and overview. In: Jurafsky, D., Chai,\n20\n"}, {"page": 21, "text": "J., Schluter, N., Tetreault, J. (eds.) Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pp. 5248–5264. Association\nfor Computational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020.\nacl-main.468 . https://aclanthology.org/2020.acl-main.468/\n[26] Jaidka, K., Guntuku, S., Ungar, L.: Facebook versus twitter: Differences in self-\ndisclosure and trait prediction. Proceedings of the International AAAI Conference\non Web and Social Media 12(1) (2018) https://doi.org/10.1609/icwsm.v12i1.\n15026\n[27] Salecha, A., Ireland, M.E., Subrahmanya, S., Sedoc, J., Ungar, L.H., Eichstaedt,\nJ.C.: Large language models display human-like social desirability biases in big\nfive personality surveys. PNAS Nexus 3(12), 533 (2024) https://doi.org/10.1093/\npnasnexus/pgae533\n[28] Jaidka, K.: Cross-platform-and subgroup-differences in the well-being effects of\ntwitter, instagram, and facebook in the united states. Scientific reports 12(1),\n3271 (2022)\n[29] Ahsan, H., Sharma, A.S., Amir, S., Bau, D., Wallace, B.C.: Elucidating mecha-\nnisms of demographic bias in llms for healthcare. arXiv preprint arXiv:2502.13319\n(2025)\n[30] Marx, B.P., Lee, D.J., Norman, S.B., Bovin, M.J., Sloan, D.M., Weathers, F.W.,\nKeane, T.M., Schnurr, P.P.: Reliable and clinically significant change in the\nclinician-administered ptsd scale for dsm-5 and ptsd checklist for dsm-5 among\nmale veterans. Psychological assessment 34(2), 197 (2022)\n[31] Ghazarian, S., Zou, Y., Shah, S., Peng, N., Beniwal, A., Potts, C., Sadagopan,\nN.: Assessment and mitigation of inconsistencies in llm-based evaluations (2024)\n[32] Askari, H., Chhabra, A., Chen, M., Mohapatra, P.: Assessing LLMs for zero-\nshot abstractive summarization through the lens of relevance paraphrasing.\nIn: Chiruzzo, L., Ritter, A., Wang, L. (eds.) Findings of the Association for\nComputational Linguistics: NAACL 2025, pp. 2187–2201. Association for Compu-\ntational Linguistics, Albuquerque, New Mexico (2025). https://doi.org/10.18653/\nv1/2025.findings-naacl.116 . https://aclanthology.org/2025.findings-naacl.116/\n[33] V Ganesan, A., Lal, Y.K., Nilsson, A., Schwartz, H.A.: Systematic evaluation of\nGPT-3 for zero-shot personality estimation. In: Barnes, J., De Clercq, O., Klinger,\nR. (eds.) Proceedings of the 13th Workshop on Computational Approaches to\nSubjectivity, Sentiment, & Social Media Analysis, pp. 390–400. Association for\nComputational Linguistics, Toronto, Canada (2023). https://doi.org/10.18653/\nv1/2023.wassa-1.34 . https://aclanthology.org/2023.wassa-1.34/\n[34] Ceballos-Arroyo, A.M., Munnangi, M., Sun, J., Zhang, K., McInerney, J.,\nWallace, B.C., Amir, S.: Open (clinical) LLMs are sensitive to instruction\n21\n"}, {"page": 22, "text": "phrasings. In: Demner-Fushman, D., Ananiadou, S., Miwa, M., Roberts, K.,\nTsujii, J. (eds.) Proceedings of the 23rd Workshop on Biomedical Natu-\nral Language Processing, pp. 50–71. Association for Computational Linguis-\ntics, Bangkok, Thailand (2024). https://doi.org/10.18653/v1/2024.bionlp-1.5 .\nhttps://aclanthology.org/2024.bionlp-1.5/\n[35] Sun, J., Shaib, C., Wallace, B.C.: Evaluating the zero-shot robustness of\ninstruction-tuned language models. In: The Twelfth International Conference on\nLearning Representations (2024). https://openreview.net/forum?id=g9diuvxN6D\n[36] Liu, N.F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., Liang,\nP.: Lost in the middle: How language models use long contexts. Transactions of\nthe Association for Computational Linguistics 12, 157–173 (2024) https://doi.\norg/10.1162/tacl a 00638\n[37] Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Chang,\nB., Sun, X., Li, L., Sui, Z.: A survey on in-context learning. In: Al-Onaizan, Y.,\nBansal, M., Chen, Y.-N. (eds.) Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, pp. 1107–1128. Association for Com-\nputational Linguistics, Miami, Florida, USA (2024). https://doi.org/10.18653/\nv1/2024.emnlp-main.64 . https://aclanthology.org/2024.emnlp-main.64/\n[38] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining\napproach. ArXiv abs/1907.11692 (2019)\n[39] Kjell, O., V Ganesan, A., Boyd, R., Oltmanns, J., Rivero, A., Feltman, S., Carr,\nM., Luft, B., Kotov, R., Schwartz, H.: Demonstrating High Validity of a New\nAI-Language Assessment of PTSD: A Sequential Evaluation with Model Pre-\nregistration\n[40] Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A.,\nLetman, A., Mathur, A., Schelten, A., Vaughan, A., et al.: The llama 3 herd of\nmodels. arXiv preprint arXiv:2407.21783 (2024)\n[41] Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S.,\nWang, P., Bi, X., et al.: Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. Nature (2025)\n[42] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models\nare zero-shot reasoners. Advances in neural information processing systems 35,\n22199–22213 (2022)\n[43] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\nD., et al.: Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems 35, 24824–24837 (2022)\n22\n"}, {"page": 23, "text": "[44] Geman, S., Bienenstock, E., Doursat, R.: Neural networks and the bias/variance\ndilemma. Neural Computation 4(1), 1–58 (1992) https://doi.org/10.1162/neco.\n1992.4.1.1\n[45] Giorgi, S., Lynn, V.E., Gupta, K., Ahmed, F., Matz, S., Ungar, L.H., Schwartz,\nH.A.: Correcting sociodemographic selection biases for population prediction from\nsocial media. Proceedings of the International AAAI Conference on Web and\nSocial Media 16(1), 228–240 (2022) https://doi.org/10.1609/icwsm.v16i1.19287\n[46] Naderalvojoud, B.: Improving machine learning with ensemble learning on obser-\nvational healthcare data. AMIA Annual Symposium proceedings 2023, 521–529\n(2024)\n[47] Maziarz, K., Liu, G., Misztela, H., Kornev, A., Gai´nski, P., Hoefling, H., Fortu-\nnato, M., Gupta, R., Segler, M.: Chimera: Accurate retrosynthesis prediction by\nensembling models with diverse inductive biases. arXiv preprint arXiv:2412.05269\n(2024)\n[48] Wright, A., Ringwald, W., Vize, C., Eichstaedt, J., Angstadt, M., Taxali, A.,\nSripada, C.: Assessing personality using zero-shot generative ai scoring of brief\nopen-ended text. Nature Human Behavior (2025) https://doi.org/10.31234/osf.\nio/4zx2k v1\n[49] Malgaroli, M., Hull, T., Zech, J., Althoff, T.: Natural language processing for\nmental health interventions: a systematic review and research framework. Trans-\nlational psychiatry 13, 309 (2023) https://doi.org/10.1038/s41398-023-02592-2\n[50] Malgaroli, M., Schultebraucks, K., Myrick, K.J., Loch, A.A., Ospina-Pinillos, L.,\nChoudhury, T., Kotov, R., De Choudhury, M., Torous, J.: Large language models\nfor the mental health community: framework for translating code to care. The\nLancet Digital Health 7(4), 282–285 (2025)\n[51] Hua, Y., Na, H., Li, Z., Liu, F., Fang, X., Clifton, D., Torous, J.: A scoping review\nof large language models for generative tasks in mental health care. npj Digital\nMedicine 8 (2025) https://doi.org/10.1038/s41746-025-01611-4\n[52] Galatzer-Levy, I.R., McDuff, D.J., Malgaroli, M.: The capability of large language\nmodels to measure to measure and differentiate psychiatric conditions through\no-shot learning. Biological Psychiatry (2025)\n[53] Eberhardt, S., Vehlen, A., Schaffrath, J., Schwartz, B., Baur, T., Schiller, D.,\nHallmen, T., Andre, E., Lutz, W.: Development and validation of large language\nmodel rating scales for automatically transcribed psychological therapy sessions.\nScientific Reports 15, 29541 (2025) https://doi.org/10.1038/s41598-025-14923-y\n[54] Gaber, F., Shaik, M., Allega, F., Bilecz, A., Busch, F., Goon, K., Franke, V.,\nAkalin, A.: Evaluating large language model workflows in clinical decision support\n23\n"}, {"page": 24, "text": "for triage and referral and diagnosis. npj Digital Medicine 8 (2025) https://doi.\norg/10.1038/s41746-025-01684-1\n[55] Bucher, A., Egger, S., Vashkite, I., Wu, W., Schwabe, G.: “it’s not only attention\nwe need”: Systematic review of large language models in mental health care.\nJMIR Ment Health 12 (2025) https://doi.org/10.2196/78410\n[56] Zhang, L., Chen, Y., Li, Q., Zhang, J., Zhou, Y.: Barriers and facilitators to\nmedical help-seeking in rural patients with mental illness: a qualitative meta-\nsynthesis. Asian Nursing Research 18(2), 203–214 (2024)\n[57] Katayama, E., Woldesenbet, S., Munir, M.M., Bryan, C., Pawlik, T.: Geospatial\nanalysis of psychiatry workforce distribution and patient travel time reveals dis-\nparities in access to mental healthcare. Psychiatry Research Communications 3,\n100136 (2023) https://doi.org/10.1016/j.psycom.2023.100136\n[58] Corrigan, P.W., Watson, A.C.: Understanding the impact of stigma on people\nwith mental illness. World psychiatry 1(1), 16 (2002)\n[59] Habeb, M., Ciobanu, A., Al-Ani, M., Mottershead, R.: Stigma in mental health:\nThe status and future direction. Cureus 17 (2025) https://doi.org/10.7759/\ncureus.85398\n[60] Mahapatra, J., Roy, S., Garain, U.: Exponential scaling of factual inconsistency in\ndata-to-text generation with fine-tuned LLMs. Transactions on Machine Learning\nResearch (2025)\n[61] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\nBarham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-\nguage modeling with pathways. Journal of Machine Learning Research 24(240),\n1–113 (2023)\n[62] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R.,\nGray, S., Radford, A., Wu, J., Amodei, D.: Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361 (2020)\n[63] Cheng, X., Pan, C., Zhao, M., Li, D., Liu, F., Zhang, X., Zhang, X., Liu,\nY.: Revisiting chain-of-thought prompting: Zero-shot can be stronger than few-\nshot. In: Christodoulopoulos, C., Chakraborty, T., Rose, C., Peng, V. (eds.)\nFindings of the Association for Computational Linguistics: EMNLP 2025, pp.\n13533–13554. Association for Computational Linguistics, Suzhou, China (2025).\nhttps://aclanthology.org/2025.findings-emnlp.729/\n[64] Sprague, Z.R., Yin, F., Rodriguez, J.D., Jiang, D., Wadhwa, M., Sing-\nhal, P., Zhao, X., Ye, X., Mahowald, K., Durrett, G.: To cot or not to\ncot? chain-of-thought helps mainly on math and symbolic reasoning. In:\nThe Thirteenth International Conference on Learning Representations (2025).\n24\n"}, {"page": 25, "text": "https://openreview.net/forum?id=w6nlcS8Kkn\n[65] Zhang, G., Jin, Q., Zhou, Y., Wang, S., Idnay, B., Luo, Y., Park, E., Nestor,\nJ., Spotnitz, M., Soroush, A., Campion, T., lu, Z., Weng, C., Peng, Y.: Closing\nthe gap between open source and commercial large language models for medical\nevidence summarization. npj Digital Medicine 7 (2024) https://doi.org/10.1038/\ns41746-024-01239-w\n[66] Salam, B., St¨uwe, C., Nowak, S., Sprinkart, A., Theis, M., Kravchenko, D., Mes-\nropyan, N., Dell, T., Endler, C., Pieper, C., Kuetting, D., Luetkens, J., Isaak,\nA.: Large language models for error detection in radiology reports: a compara-\ntive analysis between closed-source and privacy-compliant open-source models.\nEuropean Radiology 35 (2025) https://doi.org/10.1007/s00330-025-11438-y\n[67] Su, J., Healey, J., Nakov, P., Cardie, C.: Between underthinking and overthinking:\nAn empirical study of reasoning length and correctness in llms. arXiv preprint\narXiv:2505.00127 (2025)\n[68] Aggarwal, P., Kim, S., Lanchantin, J., Welleck, S., Weston, J., Kulikov, I., Saha,\nS.: Optimalthinkingbench: Evaluating over and underthinking in llms. arXiv\npreprint arXiv:2508.13141 (2025)\n[69] Hassid, M., Synnaeve, G., Adi, Y., Schwartz, R.: Don’t overthink it. pre-\nferring shorter thinking chains for improved llm reasoning. arXiv preprint\narXiv:2505.17813 (2025)\n[70] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,\nD., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report.\narXiv preprint arXiv:2303.08774 (2023)\n[71] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,\nC., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow\ninstructions with human feedback. Advances in neural information processing\nsystems 35, 27730–27744 (2022)\n[72] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347 (2017)\n[73] Ball, P.J., Smith, L., Kostrikov, I., Levine, S.: Efficient online reinforcement learn-\ning with offline data. In: International Conference on Machine Learning, pp.\n1577–1594 (2023). PMLR\n[74] Chandra, M., Sriraman, S., Khanuja, H.S., Jin, Y., De Choudhury, M.: Reasoning\nis not all you need: Examining llms for multi-turn mental health conversations.\narXiv preprint arXiv:2505.20201 (2025)\n25\n"}, {"page": 26, "text": "[75] Teferra, B.G., Perivolaris, A., Hsiang, W.N., Sidharta, C.K., Rueda, A., Parking-\nton, K., Wu, Y., Soni, A., Samavi, R., Jetly, R., Zhang, Y., Cao, B., Rambhatla,\nS., Krishnan, S., Bhat, V.: Leveraging large language models for automated\ndepression screening. PLOS Digital Health 4(7) (2025)\n[76] Wivine, B., D’Hondt, F., Shalev, A., Schultebraucks, K.: A systematic review of\nmachine learning findings in ptsd and their relationships with theoretical models.\nNature Mental Health 3 (2025) https://doi.org/10.1038/s44220-024-00365-4\n[77] Weathers, F., Litz, B., Herman, D., Huska, J.A., Keane, T.: The ptsd check-\nlist (pcl): Reliability, validity, and diagnostic utility. Annual Convention of the\nInternational Society for Traumatic Stress Studies (1993)\n[78] Blanchard, E.B., Jones-Alexander, J., Buckley, T.C., Forneris, C.A.: Psychomet-\nric properties of the ptsd checklist (pcl). Behaviour Research and Therapy 34(8),\n669–673 (1996)\n[79] Rao, R., Ganesan, A.V., Kjell, O., Luby, J., Raghavan, A., Feltman, S.M., Ring-\nwald, W., Boyd, R.L., Luft, B.J., Ruggero, C.J., et al.: Whispa: Semantically\nand psychologically aligned whisper with self-supervised contrastive and student-\nteacher learning. In: Proceedings of the 63rd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 22529–22544 (2025)\n[80] Ringwald, W.R., Feltman, S., Schwartz, H.A., Samaras, D., Khudari, C., Luft,\nB.J., Kotov, R.: Day-to-day dynamics of facial emotion expressions in post-\ntraumatic stress disorder. Journal of Affective Disorders 380, 331–339 (2025)\nhttps://doi.org/10.1016/j.jad.2025.03.109\n[81] Sadeghi, M., Richer, R., Egger, B., Schindler-Gmelch, L., Rupp, L., Rahimi,\nF., Berking, M., Eskofier, B.: Harnessing multimodal approaches for depression\ndetection using large language models and facial expressions. npj Mental Health\nResearch 3 (2024) https://doi.org/10.1038/s44184-024-00112-8\n[82] Yang, K., Ji, S., Zhang, T., Xie, Q., Kuang, Z., Ananiadou, S.: Towards inter-\npretable mental health analysis with large language models. In: Bouamor, H.,\nPino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pp. 6056–6077. Association for Computational\nLinguistics, Singapore (2023). https://doi.org/10.18653/v1/2023.emnlp-main.370\n. https://aclanthology.org/2023.emnlp-main.370/\n[83] Bromet, E., Hobbs, M., Clouston, S., Gonzalez, A., Kotov, R., Luft, B.: Dsm-iv\npost-traumatic stress disorder among world trade center responders 11-13 years\nafter the disaster of 11 september 2001 (9/11). Psychological medicine -1, 1–13\n(2015) https://doi.org/10.1017/S0033291715002184\n[84] Radford, A., Kim, J.W., Xu, T., Brockman, G., McLeavey, C., Sutskever, I.:\nRobust speech recognition via large-scale weak supervision. In: International\n26\n"}, {"page": 27, "text": "Conference on Machine Learning, pp. 28492–28518 (2023). PMLR\n[85] Koo, M., Yang, S.-W.: Likert-type scale. Encyclopedia 5(1) (2025) https://doi.\norg/10.3390/encyclopedia5010018\n[86] Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal of the\nRoyal Statistical Society: Series B (Methodological) 58(1), 267–288 (2018) https:\n//doi.org/10.1111/j.2517-6161.1996.tb02080.x\n[87] Xie, Y., Xie, Y.: Variance reduction in output from generative ai. arXiv preprint\narXiv:2503.01033 (2025)\nSupplementary\nEffect of Few-Shot Sampling Strategies on PCL Score Estimation\nPerformance\nThis section examines how different few-shot sampling strategies affect model perfor-\nmance in PTSD severity estimation using 70B-scale models. As shown in Table S6, we\ncompare zero-shot prompting to two few-shot configurations: percentile-based sam-\npling (selecting examples at the 25th, 50th, and 90th percentiles of the PCL score\ndistribution) and range-based sampling (drawing examples from both the center and\ntail of the distribution). While few-shot prompting generally improves performance\nfor base models like LLaMA-3.1-Base, its effect is less consistent for instruction-tuned\nmodels. For instance, LLaMA-3.1-Instruct shows no improvement, or even slight degra-\ndation, when few-shot exemplars are added. Across models, percentile sampling tends\nto slightly outperform range sampling, but gains are modest and vary by architecture.\nThese results highlight that few-shot prompting is not uniformly beneficial and that\nthe choice of example distribution is a sensitive factor in prompt design for clinical\nprediction tasks.\nComprehensive Performance Comparison Across Model Sizes and\nPrompting Strategies\nTable S7 presents a full comparison of model performance across three model size\ncategories (8B, 70B, and larger or undisclosed-scale models), evaluated under consis-\ntent prompting protocols. Each model is tested with zero-shot and 3-shot prompting,\nincluding standard, instruction-tuned, and chain-of-thought variants. We report both\noriginal and redistributed scores for Pearson correlation and MAE. This consolidated\nresult table provides a reference point for understanding how architectural choices,\nprompting style, and inference strategies jointly shape LLM performance on PTSD\nseverity estimation.\nOptional Plug-In Components for Prompt Customization\nTable S8 summarizes the optional plug-in components used to construct prompts\nacross experimental configurations. These modular elements were toggled on or off\nto systematically assess their contribution to model performance in PTSD severity\n27\n"}, {"page": 28, "text": "Table S6: 70B models: We report Pearson correlation (higher is better) and mean\nabsolute error (MAE, lower is better) under three prompting conditions: zero-shot,\nfew-shot with percentile-based sampling (examples at the 25th, 50th, and 90th per-\ncentiles of the empirical PCL distribution), and few-shot with stratified sampling (two\nexamples drawn from the central bulk of the distribution and one from its skewed tail).\nModel Variant\nSize\nPrompt\nPearson ↑\nMAE ↓\nLLaMA-3.1-Base\n70B\n0-shot\n0.147\n31.79\n3-shot (range sampling)\n0.345\n16.73\n3-shot (percentile sampling)\n0.346\n15.49\nLLaMA-3.1-Instruct\n70B\n0-shot\n0.426\n10.95\n3-shot (range sampling)\n0.403\n17.07\n3-shot (percentile sampling)\n0.430\n15.93\nLLaMA-3.1-Instruct (TSBS)\n70B\n0-shot\n0.412\n11.59\n3-shot (range sampling)\n0.413\n13.33\n3-shot (percentile sampling)\n0.396\n12.58\nLLaMA-3.1-Instruct w/ distr. info\n70B\n0-shot\n0.407\n9.62\n3-shot (range sampling)\n0.375\n11.37\n3-shot (percentile sampling)\n0.414\n13.01\nDeepSeek-Distil-LLaMA\n70B\n0-shot\n0.354\n11.65\n3-shot (range sampling)\n0.374\n11.75\n3-shot (percentile sampling)\n0.347\n12.38\nestimation. Each component provides a distinct type of clinical or contextual framing,\nsuch as subscale definitions, interview questions, or study background (e.g., post-9/11\ncontext). Other elements, like distributional information or PCL item references, offer\nstructural guidance or calibration cues to shape model outputs. This modular design\nenabled targeted evaluations of which kinds of information most effectively guide large\nlanguage models in generating accurate and clinically meaningful predictions.\nPrompt Architecture for PTSD Severity Estimation Experiments\nFigure S4 presents the full layout of the prompt used for the PTSD severity estima-\ntion experiments. The design consists of modular components that were either fixed or\ntoggled depending on the experimental configuration. Core elements shared across all\nprompts include the task instructions, the structured scoring system, a clearly defined\ntwo-step procedure outlining the expected scoring process, as well as the expected\noutput format. Optional plug-in components, shown in gray, include subscale defini-\ntions, interview questions and asking for references to parts of the text that motivated\nthe model’s decisions. These components were included or excluded in a controlled\nmanner to assess their contribution to model performance.\n28\n"}, {"page": 29, "text": "Table S7: Full model comparison across sizes. Pearson correlation (↑) and MAE (↓)\nfor PCL score estimation across prompting strategies. We report both original predictions\nand redistributed scores. Models are grouped by scale.\nModel Variant\nSize\nPearson ↑\nMAE ↓\n0-shot\n3-shot\n0-shot\n3-shot\n0-shot\n3-shot\n0-shot\n3-shot\nOriginal\nRedistr.\nOriginal\nRedistr.\n8B Models\nLLaMA-3.1-Base\n8B\n.178\n.251\n.206\n.247\n25.44\n26.85\n10.00\n10.10\nLLaMA-3.1-Instruct\n8B\n.316\n.218\n.326\n.217\n17.21\n18.72\n9.30\n10.04\nLLaMA-3.1-Instruct (TSBS)\n8B\n.322\n.364\n.328\n.365\n17.18\n14.19\n9.28\n8.86\nDeepSeek-Distil-LLaMA\n8B\n.208\n.178\n.221\n.182\n18.98\n14.09\n9.90\n10.22\n70B Models\nLLaMA-3.1-Base\n70B\n.187\n.345\n.191\n.346\n31.79\n16.73\n9.42\n9.41\nLLaMA-3.1-Instruct\n70B\n.426\n.403\n.434\n.429\n10.95\n17.07\n8.25\n8.49\nLLaMA-3.1-Instruct (TSBS)\n70B\n.412\n.413\n.421\n.424\n11.59\n13.33\n8.55\n8.53\nDeepSeek-Distil-LLaMA\n70B\n.354\n.374\n.364\n.376\n11.65\n11.75\n9.05\n8.80\nLarger Model Variants\nLLaMA-3.1-Instruct\n405B\n.426\n.361\n.437\n.366\n13.80\n14.69\n8.45\n8.99\nLLaMA-3.1-Instruct (TSBS)\n405B\n.429\n.416\n.438\n.415\n11.35\n12.38\n8.27\n8.51\nDeepSeek-R1\n670B\n.368\n.319\n.371\n.322\n11.45\n19.58\n8.93\n9.99\n4o-mini\nN/A\n.331\n.295\n.336\n.307\n13.66\n23.96\n9.23\n9.43\n4o-mini (TSBS)\nN/A\n.311\n.294\n.314\n.303\n14.15\n20.01\n9.33\n9.49\no3-mini\nN/A\n.383\n.344\n.386\n.349\n8.81\n10.74\n8.85\n9.11\nTable S8: Overview of optional plug-in components used in the prompt design. These\ncomponents can be toggled on or off across configurations to evaluate their effect on\nmodel performance.\nPlug-in Component\nDescription\nw/ Evidence\nPrompts the model to explain the reasoning behind each sub-\nscale severity score prediction by referencing relevant excerpts\nfrom the transcript.\nw/ Subscale Definitions\nShort descriptions for each of the four PTSD subscales (re-\nexperiencing, avoidance, dysphoria, hyperarousal) to guide the\nmodel’s interpretation.\nw/ Interview Questions\nThe set of questions originally posed to participants during self-\nrecorded interviews, aimed at eliciting responses indicative of\nPTSD symptoms.\nw/ Study Context\nAn expanded instruction indicating that the interview relates to\npost-9/11 lived experiences and clarifying the clinical framing.\nw/ PCL Items\nThe original 17 PTSD Checklist (PCL) items used for clinical\nscoring [78], listed to help the model align its predictions to\nstandard symptom indicators.\nw/ Distributional Information\nA summary of expected score distributions to steer the model\naway from extreme outputs without strong textual evidence.\n29\n"}, {"page": 30, "text": "Instructions: Your task is to conduct an in-depth analysis of a provided transcript,\nwith the goal of assessing the severity of PTSD symptoms based on its 4 subscales. The\ntranscript is derived from a patient’s self-recorded answers to a set of questions, designed\nto assess various aspects of psychological well-being, including PTSD-related symptoms.\nQuestions Asked:\n1. How are you? Can you elaborate?\n2. How’s the family? Can you elaborate?\n·\n·\n·\n13. Over the past 5 years what are the three worst things that happened to you and your\nfamily? Can you elaborate?\nPTSD Subscales:\n• Re-experiencing: Intrusive thoughts, flashbacks, nightmares, or distress when\nreminded of the trauma.\n• Avoidance: Efforts to avoid thoughts, feelings, or external reminders associated with\nthe trauma.\n• Dysphoria: Persistent negative thoughts, feelings of guilt, emotional numbness, or\nloss of interest in activities.\n• Hyperarousal: Increased irritability, hypervigilance, being easily startled, or diffi-\nculty concentrating and sleeping.\nScoring System:\n• 0 - Not at all severe: The symptom is not present or negligible.\n• 1 - Low severity: The symptom is present but slightly impacts the individual.\n• 2 - Moderate severity: The symptom is noticeably present and moderately impacts the\nindividual.\n• 3 - High severity: The symptom is significantly present and strongly impacts the indi-\nvidual.\n• 4 - Extreme severity: The symptom is overwhelmingly present and has a profound\nimpact.\nSteps:\n• Step\n1:\nDetect\nthe\nPTSD\nsubfactors\nin\nthe\ntext\nand provide an explanation on how each symptom was identified.\nProvide a score\nbetween 0 and 4.\n• Step\n2:\nPresent\nthe\nresults\nas\na\nnested\nJSON\nwith\nseverity\nscores\nand explanations referencing relevant spans or contextual inferences.\n30\n"}, {"page": 31, "text": "Expected JSON Output:\n{\n\"Re-experiencing\": {\n\"Reason\": [\"<Reason(s) from transcript>\" ] ,\n\"Severity Score\": <score>\n},\n\"Avoidance\": {\n\"Reason\": [\"<Reason(s) from transcript>\" ] ,\n\"Severity Score\": <score>\n},\n\"Dysphoria\": {\n\"Reason\": [\"<Reason(s) from transcript>\" ] ,\n\"Severity Score\": <score>\n},\n\"Hyperarousal\": {\n\"Reason\": [\"<Reason(s) from transcript>\" ] ,\n\"Severity Score\": <score>\n} }\nText: ”I’ve been having a rough time lately. Some days are better than others, but overall,\nI feel like I’m constantly on edge. I still think about what happened all the time, not always\ndirectly, but little things set me off. Loud noises, crowded places. I try to keep busy...”\nFig. S4: Full layout of the PTSD subscale evaluation prompt. Colored boxes represent\ncore components shared across all configurations. Gray boxes are optional plug-ins\nthat can be included or excluded to control the amount of context provided.\nAdditional Plug-In Components for Further Prompt Customization\nFigure S5 illustrates three plug-in prompt components that were selectively incor-\nporated in different experimental conditions. These elements either modify core\ninstructions or introduce additional clinical context to guide the model’s scoring behav-\nior. Panel (a) shows an updated instruction variant that embeds explicit study context,\nframing the task around participants’ experiences following the 9/11 World Trade Cen-\nter attacks. Panel (b) introduces the full list of PTSD Checklist (PCL) items. Panel (c)\npresents distributional guidance about typical PCL score ranges, intended to calibrate\nmodel predictions by discouraging extreme scores unless strongly supported by the\ninput. These components were activated in a controlled manner across configurations\nto investigate whether they yield more accurate severity estimates.\n31\n"}, {"page": 32, "text": "(a) Study Context [Updated Instruction Component]:\nYour\ntask\nis\nto\nconduct\nan\nin-depth\nanalysis\nof\na\nprovided\ntranscript,\nwith\nthe\ngoal\nof\nassessing\nthe\nseverity\nof\nPTSD\nsymptoms\nbased\non\nits\n4\nsubscales.\nThe\ntext\nyou\nwill\nanalyze\nis\nthe\ntranscription\nof\nthe\npatient’s\nself-recorded\nanswers\nto\na\nset\nof\nquestions.\nThe\npatients\nare\nquestioned\nabout their lives after the World Trade Center disaster that happened on 9/11.\nThese\nquestions were designed to assess various aspects of psychological well-being, particu-\nlarly symptoms associated with PTSD. The goal is to assess the overall PTSD severity\nrelated to the World Trade Center disaster.\n(b) PCL Items [Additional Component]:\nThe PCL score is calculated based on 17 specific items, each representing a symptom\ncommonly associated with PTSD. The 17 items are:\n1. Repeated, disturbing memories, thoughts, or images of a stressful experience from\nthe past.\n2. Repeated, disturbing dreams of a stressful experience from the past.\n·\n·\n·\n17. Feeling jumpy or easily startled.\n(c) Distributional Information [Additional Component]:\nAccording to domain experts, most individuals typically score between 17 and 30. Scores\nbetween 30 and 50 occur in fewer cases, while above 50 are relatively rare and typically\nindicate more severe or clinically significant symptom expression. You should take this\ndistribution into account when making your severity judgments. Use higher scores only\nwhen there is clear textual evidence of substantial psychological distress or impairment.\nFig. S5: Further plug-in prompt components that were explored in this study. These\nelements provide additional information and are activated selectively. They can either\nbe additional components (Additional Component) or alternative variants of a core\ncomponent (Updated Component).\nDirect score prediction prompt\nInstructions: Your task is to conduct an in-depth analysis of a provided transcript,\nwith the goal of estimating the overall severity of post-traumatic stress disorder (PTSD)\nsymptoms experienced by the individual. The text you will analyze is a transcription of\nthe patient’s spoken answers to a predefined set of self-recorded questions. The patients\nare questioned about their lives after the World Trade Center disaster that happened on\n9/11. These questions were designed to assess various aspects of psychological well-being,\nparticularly symptoms associated with PTSD. The goal is to assess the overall PTSD\nseverity related to the World Trade Center disaster.\n32\n"}, {"page": 33, "text": "Scoring System: Based on the content of the transcript, predict a single scalar PTSD\nseverity score in the range 17 to 85, where:\n- 17 represents minimal or no PTSD-related symptoms. - 85 represents extreme PTSD\nsymptom severity across multiple domains of functioning.\nThis score should directly estimate the patient’s PCL score, a widely used self-report\nmeasure of PTSD symptom severity.\nSteps: Carefully analyze the transcript, considering the emotional tone, content, and\nany references to trauma-related symptoms or functional impairments. Then assign a\nsingle integer score between 17 and 85 that best reflects the overall PTSD severity of the\nindividual related to the World Trade Center disaster.\nExpected JSON Output: Return your answer in the following structured JSON\nformat:\n{ \"PTSD Severity Score\": <score> }\nText: ”I’ve been having a rough time lately. Some days are better than others, but overall,\nI feel like I’m constantly on edge. I still think about what happened all the time, not always\ndirectly, but little things set me off. Loud noises, crowded places. I try to keep busy...”\nFig. S6: Full layout of the PTSD direct score prediction prompt, which is simpler\ncompared to the subscales prediction one shown in Figure S4.\nDataset Descriptive Statistics\nTable S9: Descriptive statistics of our dataset, including\ndemographic information and basic audio characteristics\nof the self-recorded clinical interviews.\nCategory\nMetric\nAnalysis Set\nDemographics\nNumber of Recordings\n1437\nAge (min / mean / max)\n38 / 58.09 / 90\nGender Ratio (F:M)\n7.4 : 92.6\nAudio Stats\nAvg. Duration (min)\n7.50 ± 4.1\nAvg. Word Count\n697.93 ± 530.44\n33\n"}]}