{"doc_id": "arxiv:2511.14936", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.14936.pdf", "meta": {"doc_id": "arxiv:2511.14936", "source": "arxiv", "arxiv_id": "2511.14936", "title": "How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding", "authors": ["Mathieu Dufour", "Andrew Duncan"], "published": "2025-11-18T21:51:04Z", "updated": "2025-11-18T21:51:04Z", "summary": "Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\\varepsilon \\in \\{4, 6\\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.14936v1", "url_pdf": "https://arxiv.org/pdf/2511.14936.pdf", "meta_path": "data/raw/arxiv/meta/2511.14936.json", "sha256": "1be9657b43423f77ae2c8892861cb388b19c1c6896a95dbd3bbaf3ebfeb0a2f0", "status": "ok", "fetched_at": "2026-02-18T02:26:37.970035+00:00"}, "pages": [{"page": 1, "text": "How to Train Private Clinical Language Models: A\nComparative Study of Privacy-Preserving Pipelines for\nICD-9 Coding\nMathieu Dufour\nDepartment of Mathematics\nImperial College London\nmathieu.dufour23@imperial.ac.uk\nAndrew Duncan\nDepartment of Mathematics\nImperial College London\na.duncan@imperial.ac.uk\nAbstract\nLarge language models trained on clinical text risk exposing sensitive patient\ninformation, yet differential privacy (DP) methods often severely degrade the diag-\nnostic accuracy needed for deployment. Despite rapid progress in DP optimisation\nand text generation, it remains unclear which privacy-preserving strategy actually\nworks best for clinical language tasks. We present the first systematic head-to-\nhead comparison of four training pipelines for automated diagnostic coding from\nhospital discharge summaries. All pipelines use identical 1B-parameter models\nand matched privacy budgets to predict ICD-9 codes. At moderate and relaxed\nprivacy budgets (ε ∈{4, 6}), knowledge distillation from DP-trained teachers\noutperforms both direct DP-SGD and DP-synthetic data training, recovering up to\n63% of the non-private performance whilst maintaining strong empirical privacy\n(membership-inference AUC ≈0.5). These findings expose large differences in\nthe privacy-utility trade-off across architectures and identify knowledge distillation\nas the most practical route to privacy-preserving clinical NLP.\n1\nIntroduction\nLarge language models trained on clinical text can inadvertently memorise and reveal sensitive\npatient details [Carlini et al., 2021b], raising serious concerns about their deployment in healthcare.\nDifferential privacy (DP) offers formal guarantees that individual patient records cannot be reliably\ninferred from model outputs [Dwork and Roth, 2014], but applying DP training to large models often\ncomes at a steep price: accuracy drops of 40% or more have been reported for clinical NLP tasks\n[Dadsetan et al., 2024]. This tension between diagnostic utility and patient confidentiality has become\na central challenge for trustworthy clinical AI.\nA variety of strategies have been proposed to mitigate these trade-offs. Parameter-efficient tuning\nmethods such as Low-Rank Adaptation (LoRA) restrict memorisation through low-rank updates and\nmay offer partial implicit privacy [Malekmohammadi and Farnadi, 2025]. Generative approaches train\nlarge DP-protected models to produce synthetic clinical notes that can be used freely for downstream\ntasks [Yue et al., 2023]. More recently, differentially private knowledge distillation allows a private\n“teacher” to transfer its knowledge to a smaller “student” via synthetic data and soft labels, preserving\nthe teacher’s privacy guarantee through post-processing [Flemings and Annavaram, 2024].\nDespite this progress, no prior work has compared these methods under identical conditions. Existing\nstudies vary in model size, dataset, or privacy accounting, making it unclear which approach provides\nthe best balance of accuracy, efficiency, and protection. For practitioners deciding how to deploy\nlanguage models in hospitals, this lack of systematic evidence leaves a crucial gap.\nThis study asks a single question: “Given a fixed model capacity, which privacy mechanism best\nbalances diagnostic accuracy and formal privacy guarantees in clinical NLP?”\nPreprint.\narXiv:2511.14936v1  [cs.LG]  18 Nov 2025\n"}, {"page": 2, "text": "We answer this through a controlled, head-to-head comparison of four privacy-preserving training\npipelines for automated ICD-9 diagnostic coding, i.e. the task of mapping hospital discharge\nsummaries to standardised disease codes, using the publicly available MIMIC-III dataset [Johnson\net al., 2016]. All final classifiers share the same architecture and capacity (1B parameters) to isolate\nthe impact of the privacy mechanism itself. In pipelines involving teacher-student architectures (DP-\nSynthetic, DP-Distil), we train larger 3B “teacher” models under DP-SGD, then use their synthetic\noutputs and/or soft labels to train the final 1B “student” classifier. Because the student only accesses\npost-processed outputs of the DP teachers, it inherits the same formal privacy guarantees without\ndirect exposure to real patient records.\nThe evaluated pipelines are:\n(1) DP-Small—direct DP-SGD on the 1B model; (2) DP-\nSynthetic—training on synthetic notes from a 3B DP-protected generator; (3) DP-Distil—knowledge\ndistillation from DP-trained 3B teachers providing synthetic data and soft labels; (4) LoRA-No-\nDP—non-private LoRA fine-tuning as a utility upper bound.\nWe assess each approach across privacy budgets ε ∈{2, 4, 6} with fixed δ = 10−5, measuring utility\n(micro/macro-F1, AUPRC) and empirical privacy via membership-inference attacks [Carlini et al.,\n2021a]. By holding model capacity constant, our study isolates how different training pipelines\nreshape the privacy-utility frontier in clinical NLP. The results reveal that architectural choices, not\nmerely the privacy budget, substantially shape this trade-off, highlighting knowledge distillation\nas an effective pathway for developing differentially private language models that preserve clinical\nusefulness.\n2\nMethods\n2.1\nDataset\nWe evaluate all methods on the MIMIC-III v1.4 critical-care database [Johnson et al., 2016], a\ncollection of de-identified electronic health records from intensive care unit admissions. We use\ndischarge summaries—clinical narratives summarising each hospital stay—associated with the 50\nmost frequent ICD-9 diagnostic codes. ICD-9 (International Classification of Diseases, 9th Revision)\nis a standardised medical coding system used for billing and epidemiology. The final dataset contains\n44,778 training notes, 5,624 validation notes, and 5,586 test notes, with splits defined at the admission\nlevel to prevent patient overlap. Notes are tokenised using the LLaMA-3.2 tokeniser and truncated to\n512 tokens. The prediction task is multi-label ICD-9 classification.\n2.2\nModel Architecture\nAll final classifiers share an identical 1B-parameter LLaMA-3.2 backbone [Touvron et al., 2023,\nGrattafiori et al., 2024], fine-tuned via LoRA [Hu et al., 2021] on the query (Q) and value (V)\nprojections (rank r = 4, α = 16). The DP “teacher” and “generator” models use the 3B-parameter\nvariant with LoRA rank r = 8 and α = 32. These settings yield approximately 0.53M trainable\nparameters for 1B models and 2.3–2.4M for 3B models, keeping adapter capacity roughly proportional\nto backbone size and ensuring a controlled comparison across training pipelines. This design follows\nempirical observations that larger models tend to retain higher utility under DP-SGD at fixed privacy\nbudgets [Yu et al., 2021].\nWe standardise on 1B students rather than deploying 3B teachers directly for two reasons: (1) to\nensure fair comparison across pipelines at identical capacity, and (2) to enable resource-efficient\ndeployment. The 1B models offer 2.7× faster inference at 13ms versus 36ms per sample and require\nonly 4.4GB VRAM compared to 8.5GB for the teachers (Table 5), making them practical for clinical\nsettings with hardware constraints.\n2.3\nDifferential privacy mechanisms\nDifferentially private training follows DP-SGD [Abadi et al., 2016] with per-example gradient\nclipping and Gaussian noise calibrated via Rényi DP accounting in Opacus [Yousefpour et al., 2021].\nUnless stated otherwise, δ = 10−5 and ε ∈{2, 4, 6}, corresponding to strong, moderate, and relaxed\nprivacy regimes commonly reported in medical NLP [Dadsetan et al., 2024]. Gradient clipping\nthresholds are C = 1.0 for 1B models and C = 0.7 for 3B models. In DP-Distil, each teacher model\nconsumes ε/2 to preserve overall (ε, δ) under sequential composition [Dwork and Roth, 2014].\n2\n"}, {"page": 3, "text": "Real notes + codes\nTrain: 44,778\nVal: 5,624\n3B Generative\nTeacher\nLoRA: r=8, Q,V\nDP: ε∈{2, 4, 6}\nSynthetic\nnotes\nTrain: 44,778\nVal: 5,624\n1B Classifier\nLoRA: r=4, Q,V\nNo DP\nTrain\nGenerate\nTrain\nA) DP-Synthetic (ε ∈{2, 4, 6}, δ = 10−5)\nReal notes + codes\nTrain: 44,778\nVal: 5,624\n3B Generative\nTeacher\nLoRA: r=8, Q,V\nDP: ε/2\n3B Classifier\nTeacher\nLoRA: r=8, Q,V\nDP: ε/2\nSynthetic\nnotes\nTrain: 44,778\nVal: 5,624\nTeacher\nlogits\nSoft labels\n1B Classifier\nStudent\nLoRA: r=4, Q,V\nNo DP\nTrain\nTrain\nGenerate\nLabel\nKD\nB) DP-Distil (ε ∈{2, 4, 6}, δ = 10−5)\nReal notes + codes\nTrain: 44,778\nVal: 5,624\n1B Classifier\nLoRA: r=4, Q,V\nDP: ε∈{2, 4, 6}\nTrain\nC) DP-Small (ε ∈{2, 4, 6}, δ = 10−5)\nReal notes + codes\nTrain: 44,778\nVal: 5,624\n1B Classifier\nLoRA: r=4, Q,V\nNo DP\nTrain\nD) LoRA-No-DP (Baseline)\nFigure 1: Four training pipelines producing identical 1B classifiers (green). (A) DP-trained 3B\ngenerator creates synthetic data. (B) DP-trained 3B teachers (each ε/2) provide synthetic data and\nsoft labels for knowledge distillation. (C) Direct DP-SGD on 1B model. (D) Non-private LoRA\nbaseline. All DP pipelines use ε ∈{2, 4, 6} with δ = 10−5.\n2.4\nPrivacy-preserving training pipelines\nFigure 1 illustrates the four pipelines, all producing identical 1B classifiers for fair comparison.\nDP-Synthetic: A 3B generative model is fine-tuned under DP-SGD for conditional generation with\ncontrol codes [Yue et al., 2023], producing 44,778 synthetic notes (nucleus sampling: p = 0.9,\nT = 0.8) to train a 1B classifier inheriting the generator’s (ε, δ) via post-processing.\nDP-Distil: Two 3B teachers trained under DP-SGD (each ε/2, δ/2)—generative and classifica-\ntion—provide synthetic data and soft labels. A 1B student trains via MSE on raw logits (α = 0),\ninheriting (ε, δ) through sequential composition [Flemings and Annavaram, 2024].\nDP-Small: Direct DP-SGD training of the 1B model with binary cross-entropy loss and frequency-\nweighted positive class weights, evaluating direct private training without architectural modifications.\nLoRA-No-DP: LoRA fine-tuning without gradient clipping or noise, serving as an upper bound on\nutility and testing LoRA’s implicit privacy properties [Malekmohammadi and Farnadi, 2025].\n2.5\nEvaluation\nUtility. We report Micro- and Macro-F1, Micro-AUPRC, and Hamming loss, with thresholds\noptimised on the validation set. Micro-F1 aggregates predictions across all labels, whilst Macro-F1\ncomputes per-label F1 scores and averages them, giving equal weight to rare codes.\nPrivacy. Following Carlini et al. [2021a] and Shokri et al. [2017], we train logistic-regression\nmembership-inference attacks on five features derived from model logits: cross-entropy loss, maxi-\nmum confidence, entropy, confidence margin, and L2 norm. Attacks are trained on a balanced dataset\nof 5,586 training members versus 5,586 test non-members. AUC = 0.5 indicates random guessing.\n3\nResults\n3.1\nOverall performance\nTable 1 summarises classification accuracy across all privacy budgets ε ∈{2, 4, 6} (δ = 10−5).\nAt the strictest budget (ε = 2), DP-Small performs best among DP methods (Micro-F1 ≈0.26),\nreflecting the advantage of allocating the full privacy budget to a single model. As ε increases,\nDP-Distil overtakes it, reaching 0.33 Micro-F1 at ε = 6, an 8.7% gain over DP-Small and a 48%\nimprovement over DP-Synthetic. The non-private LoRA-No-DP baseline achieves 0.52, defining the\npractical upper bound on utility.\n3\n"}, {"page": 4, "text": "Table 1: Classification performance across pipelines and privacy budgets (δ = 10−5 for all DP\nmethods). Arrows indicate direction of improvement.\nPipeline\nε\nMicro-F1 ↑\nMacro-F1 ↑\nMicro-AUPRC ↑\nHam ↓\nDP-Distil\n2\n0.217\n0.232\n0.260\n0.533\nDP-Small\n2\n0.258\n0.257\n0.285\n0.343\nDP-Synthetic\n2\n0.220\n0.207\n0.198\n0.402\nDP-Distil\n4\n0.289\n0.280\n0.321\n0.273\nDP-Small\n4\n0.279\n0.275\n0.305\n0.286\nDP-Synthetic\n4\n0.228\n0.216\n0.212\n0.442\nDP-Distil\n6\n0.330\n0.303\n0.347\n0.216\nDP-Small\n6\n0.304\n0.298\n0.334\n0.247\nDP-Synthetic\n6\n0.222\n0.213\n0.225\n0.476\nLoRA-No-DP\n∞\n0.521\n0.499\n0.565\n0.099\n2\n4\n6\nPrivacy Budget ( )\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nMicro-F  Score\nMicro-F  vs Privacy Budget\nDP-Small\nDP-Synthetic\nDP-Distil\nLoRA-No-DP\n2\n4\n6\nPrivacy Budget ( )\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nMicro-AUPRC\nMicro-AUPRC vs Privacy Budget\nDP-Small\nDP-Synthetic\nDP-Distil\nLoRA-No-DP\nFigure 2: Utility-privacy trade-off for Micro-F1 (left) and Micro-AUPRC (right). DP-Distil and\nDP-Small improve monotonically with privacy budget, whilst DP-Synthetic plateaus regardless of ε.\nDP-Distil dominates at ε ≥4; DP-Synthetic fails to scale.\nPerformance scales monotonically with privacy budget for DP-Small and DP-Distil but remains flat\nfor DP-Synthetic (Figure 2, ≈0.22 F1 across ε). At ε = 6, DP-Distil recovers 63% of the non-private\nperformance whilst maintaining formal DP guarantees, demonstrating that architectural design can\noffset much of the utility loss induced by DP noise.\n3.2\nPrivacy evaluation\nTable 2 reports membership-inference attack (MIA) results. All DP methods yield AUCs between\n0.49 and 0.51, effectively random guessing, confirming strong empirical privacy. The LoRA-No-\nDP baseline, whilst partially protective (AUC ≈0.57), remains measurably vulnerable, notably to\nloss-based attacks (AUC ≈0.54).\nFigure 3 shows ensemble MIA performance across configurations (left) and individual feature\ndecomposition at ε = 4 (right). For DP models, all features—loss, confidence, entropy, margin, and\nL2 norm—cluster around 0.5 AUC, indicating that differential privacy neutralises multiple leakage\nchannels simultaneously. Notably, empirical AUC shows no systematic correlation with ε, suggesting\nthat, within this range, the presence of DP noise itself matters more than the precise choice of ε.\n3.3\nTraining efficiency and deployment considerations\nWhile DP-Distil achieves the best utility-privacy trade-off, this comes at a computational cost.\nTraining times vary substantially across pipelines: DP-Distil requires 38–49 hours due to training\ntwo 3B teachers plus the 1B student, compared to 14–15 hours for DP-Synthetic, 5–10 hours for\nDP-Small, and just 3 hours for LoRA-No-DP (all on RTX 6000 Ada GPUs, see Table 4 for details).\nHowever, this training overhead is a one-time cost. All pipelines produce architecturally identical\n1B classifiers that require only 13ms per sample and 4.4GB VRAM at inference (Table 5). This\nuniformity means that choosing DP-Distil does not impose any deployment penalties—the same\nefficient 1B model runs in production regardless of how it was trained. For healthcare institutions\n4\n"}, {"page": 5, "text": "Table 2: Privacy evaluation via membership inference attacks across all pipelines and privacy budgets,\nshowing results for ensemble attacks and individual attack features (loss, confidence, entropy, margin).\nValues closer to 0.5 indicate better privacy (random guessing). All DP methods achieve AUC ≈0.5;\nLoRA-No-DP shows vulnerability with ensemble AUC of 0.565.\nPipeline\nε\nEnsemble\nLoss\nConf.\nEntropy\nMargin\nAUC\nAUC\nAUC\nAUC\nAUC\nDP-Distil\n2\n0.487\n0.499\n0.500\n0.500\n0.500\nDP-Distil\n4\n0.485\n0.504\n0.497\n0.502\n0.501\nDP-Distil\n6\n0.498\n0.506\n0.492\n0.504\n0.499\nDP-Small\n2\n0.512\n0.505\n0.492\n0.509\n0.498\nDP-Small\n4\n0.497\n0.507\n0.493\n0.507\n0.503\nDP-Small\n6\n0.499\n0.505\n0.494\n0.508\n0.493\nDP-Synthetic\n2\n0.506\n0.500\n0.498\n0.503\n0.503\nDP-Synthetic\n4\n0.496\n0.503\n0.495\n0.505\n0.498\nDP-Synthetic\n6\n0.504\n0.501\n0.502\n0.503\n0.507\nLoRA-No-DP\n∞\n0.565\n0.536\n0.493\n0.507\n0.510\nLoRA-\nNo-DP\nDP-Small\nDP-Synthetic\nDP-Distil\n=\n=2\n=4\n=6\n=2\n=4\n=6\n=2\n=4\n=6\nPipeline Configuration\n0.48\n0.50\n0.52\n0.54\n0.56\nMIA Ensemble AUC\n0.565\n0.512\n0.497 0.499\n0.506\n0.496\n0.504\n0.487 0.485\n0.498\nHigher vulnerability\n(no DP protection)\nMembership Inference Attack Vulnerability\nRandom Guess\nLoss\nConfidence\nEntropy\nMargin\nL2 Norm\nMIA Feature\n0.48\n0.50\n0.52\n0.54\n0.56\nAUC\nIndividual Feature MIA Performance ( =4)\nLoRA-No-DP\nDP-Small ( =4)\nDP-Synthetic ( =4)\nDP-Distil ( =4)\nFigure 3: MIA vulnerability analysis. Left: Ensemble AUC showing LoRA-No-DP’s higher vulnera-\nbility versus near-random performance for DP methods. Right: Individual feature contributions at\nε = 4, demonstrating consistent protection across attack vectors for DP methods. All DP methods\nachieve AUC ≈0.5; LoRA-No-DP remains vulnerable.\nprocessing thousands of daily predictions, the initial training investment in DP-Distil may be justified\nby its sustained 8.7% accuracy advantage over DP-Small at ε = 6.\n3.4\nTeacher-student distillation performance\nTable 3 shows that 1B-parameter students consistently match or slightly outperform their 3B-\nparameter teachers in DP-Distil. At ε = 6, the student achieves Micro-F1 of 0.3300 versus the\nteacher’s 0.3212, with similar patterns for Macro-F1. This result is consistent with previous distilla-\ntion literature showing students can surpass teachers through regularisation effects [Furlanello et al.,\n2018, Nagarajan et al., 2024]. The student advantage is most pronounced at higher privacy budgets\nwhere teachers have more utility to transfer, while the gap narrows at stricter settings (ε = 2: +0.0004\nfor Micro-F1), as visualised in Figure 4.\nTable 3: Performance comparison between DP-Distil teachers (3B parameters) and their distilled\nstudents (1B parameters). Positive gaps indicate teacher superiority.\nε\nTeacher\nStudent\nGap\nTeacher\nStudent\nGap\nMicro-F1\nMacro-F1\n2\n0.2176\n0.2172\n0.0004\n0.2303\n0.2318\n-0.0015\n4\n0.2824\n0.2888\n-0.0064\n0.2765\n0.2802\n-0.0037\n6\n0.3212\n0.3300\n-0.0088\n0.2988\n0.3034\n-0.0047\n5\n"}, {"page": 6, "text": "2\n4\n6\nPrivacy Budget ( )\n0.22\n0.24\n0.26\n0.28\n0.30\n0.32\nMicro-F1\nMicro-F1: Teacher vs Student\nTeacher (3B)\nStudent (1B)\n2\n4\n6\nPrivacy Budget ( )\n0.23\n0.24\n0.25\n0.26\n0.27\n0.28\n0.29\n0.30\nMacro-F1\nMacro-F1: Teacher vs Student\nTeacher (3B)\nStudent (1B)\n2\n4\n6\nPrivacy Budget ( )\n0.26\n0.28\n0.30\n0.32\n0.34\nAUPRC\nAUPRC: Teacher vs Student\nTeacher (3B)\nStudent (1B)\nDP-Distil: Teacher-Student Performance Comparison\nFigure 4: Performance comparison between 3B-parameter DP-Distil teachers and their 1B-parameter\ndistilled students across privacy budgets (ε ∈{2, 4, 6}).\n=2\n=4\n=6\n=2\n=4\n=6\n=2\n=4\n=6\nPrivacy Budget\n4019 (n=17920)\n4280 (n=11933)\n42731 (n=11517)\n41401 (n=11011)\n5849 (n=7936)\n25000 (n=7915)\n2724 (n=7429)\n51881 (n=6614)\n5990 (n=6049)\n53081 (n=5422)\nICD-9 Code (training frequency)\n0.73\n0.61\n0.61\n0.62\n0.58\n0.59\n0.59\n0.60\n0.61\n0.61\n0.69\n0.45\n0.50\n0.57\n0.43\n0.45\n0.45\n0.45\n0.53\n0.57\n0.70\n0.46\n0.48\n0.51\n0.43\n0.44\n0.44\n0.44\n0.49\n0.51\n0.78\n0.62\n0.65\n0.66\n0.42\n0.42\n0.42\n0.43\n0.65\n0.67\n0.51\n0.40\n0.41\n0.42\n0.36\n0.37\n0.35\n0.36\n0.42\n0.43\n0.65\n0.32\n0.33\n0.33\n0.30\n0.29\n0.30\n0.31\n0.32\n0.34\n0.61\n0.37\n0.39\n0.39\n0.38\n0.36\n0.35\n0.36\n0.38\n0.42\n0.53\n0.29\n0.35\n0.39\n0.27\n0.29\n0.27\n0.28\n0.38\n0.42\n0.37\n0.24\n0.24\n0.27\n0.23\n0.23\n0.23\n0.23\n0.25\n0.29\n0.53\n0.22\n0.22\n0.25\n0.23\n0.24\n0.23\n0.23\n0.25\n0.25\nPer-Label F1 Scores for 10 Most Frequent ICD-9 Codes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 Score\nLoRA-No-DP\nDP-Small\nDP-Synthetic\nDP-Distil\nFigure 5: Per-label F1 scores for the ten most frequent ICD-9 codes, sorted by decreasing training\nfrequency (n in parentheses). DP-Synthetic consistently underperforms DP-Small and DP-Distil\nacross nearly all codes, while LoRA-No-DP shows irregular memorisation patterns.\n3.5\nPer-label performance\nFigure 5 displays F1 scores for the ten most frequent ICD-9 codes, revealing how privacy mechanisms\naffect individual diagnoses. All three DP methods show general correlation with code frequency,\nwith performance declining for rarer codes. DP-Small and DP-Distil exhibit similar patterns, with a\nnotable peak at code 41401 (coronary atherosclerosis, fourth most frequent, n=11,011), achieving F1\nscores of 0.66 and 0.67 respectively at ε = 6.\nDP-Synthetic shows consistent underperformance across nearly all codes compared to other DP\nmethods. While it exhibits smooth frequency-dependent degradation—declining from 0.59 (code\n401.9) to 0.23 (code 53081) at ε = 6—its per-code F1 scores remain systematically lower than\nDP-Small and DP-Distil regardless of code frequency or privacy budget.\nLoRA-No-DP shows the most irregular pattern, with substantial variation unexplained by frequency\nalone. It achieves 0.78 for code 41401 but only 0.37 for code 5990 (n=6,049).\n3.6\nKey observations\n1. Knowledge distillation outperforms direct DP-SGD (ε ≥4). High-capacity teachers\nretain higher utility under DP-SGD, and their soft labels allow students to learn cleaner\ndecision boundaries than direct DP-SGD on the 1B model.\n2. DP-Synthetic fails due to low-fidelity text generation. Even at relaxed budgets, synthetic\ndata do not capture the clinical signal required for accurate coding.\n3. LoRA provides weak implicit privacy. Low-rank updates slightly constrain memorisation\nbut cannot substitute for formal DP guarantees.\n4. Privacy metrics saturate quickly. Once DP noise is introduced, further relaxation of ε\nyields utility gains but negligible empirical privacy degradation.\n6\n"}, {"page": 7, "text": "4\nDiscussion\n4.1\nWhy knowledge distillation wins\nAmong all differentially private methods, DP-Distil achieves the highest utility at moderate and\nrelaxed privacy budgets. Its advantage stems from the ability of high-capacity teacher models to\nlearn more robust representations under DP-SGD despite the injected noise, and to transfer these\nrepresentations to a smaller student, consistent with prior observations that larger models can maintain\nhigher utility under DP-SGD at fixed privacy budgets [Yu et al., 2021]. The use of soft labels plays\na central role: rather than exposing the student to one-hot class assignments derived from noisy\nteacher predictions, the continuous logit distribution encodes relative class likelihoods, offering richer\nsupervision and acting as a regulariser. This allows the student to recover cleaner decision boundaries\neven when the teachers’ parameters are perturbed by privacy noise.\nAt very tight privacy budgets (ε = 2), DP-Small outperforms DP-Distil; the latter splits its budget\nbetween two teachers (ε/2 each), leaving insufficient signal for high-fidelity distillation. As ε\nincreases, the teachers’ larger capacity enables them to tolerate DP noise more effectively. By ε = 6,\nthe 1B student achieves Micro-F1 of 0.33, slightly exceeding the 3B teachers (0.32), consistent\nwith regularisation benefits in prior distillation literature [Furlanello et al., 2018]. As shown in\nTable 3, students consistently match or outperform their teachers across all privacy budgets, with the\nperformance gap widening at higher ε values where teachers have cleaner signals to transfer.\nTaken together, these results highlight that deploying the 3B DP-trained teachers directly would\noffer no accuracy benefit over the 1B students while nearly doubling memory and latency (Table 5).\nDistillation therefore provides both higher utility and a more efficient deployed model, with lower\nlatency and memory requirements, reinforcing its practicality for real-world clinical settings.\n4.2\nWhy synthetic data fails\nDP-Synthetic consistently underperforms, plateauing around Micro-F1 of 0.22. Since its training\nprocedure mirrors DP-Small but uses DP-generated text, this gap isolates synthetic data fidelity as\nthe limiting factor. The DP generator struggles to reproduce the statistical and lexical diversity of\ntrue discharge summaries, leading to label-distribution drift and loss of rare diagnostic patterns.\nAs shown in Figure 5, this underperformance extends across nearly all individual codes—even the\nmost frequent with ample training examples—confirming that synthetic data lack essential clinical\nsignal rather than merely failing to cover rare patterns.\n4.3\nLoRA’s modest implicit privacy\nLoRA-No-DP achieves MIA AUC of 0.57, providing measurable but incomplete privacy protection.\nThis represents a 13% relative increase in attack success over random guessing (AUC = 0.5), partially\nvalidating that low-rank constraints inherently limit memorisation [Malekmohammadi and Farnadi,\n2025]. However, the gap to formal DP methods (AUC 0.49–0.51) confirms that LoRA alone cannot\nreplace rigorous privacy guarantees in high-stakes clinical settings.\n4.4\nLimitations\nOur study has several limitations. The 512-token truncation captures roughly 17% of each discharge\nsummary, potentially omitting diagnostic context. Focusing on the 50 most frequent ICD-9 codes may\nnot reflect performance on rare conditions. Our privacy audit evaluates only logit-based membership-\ninference attacks; other attack classes (e.g., attribute or extraction) remain outside scope. Finally,\nexperiments used a single random seed and limited hyperparameter tuning. Despite these constraints,\nthe observed trends remain robust and internally consistent across metrics and privacy budgets,\nsupporting our central claim that architectural choices, alongside the privacy budget, substantially\nshape the privacy–utility trade-off.\n4.5\nPractical implications\nTaken together, our results provide the first controlled evidence of how privacy mechanisms reshape\nmodel performance in clinical NLP at fixed capacity. When strict privacy is required (ε ≤2),\nDP-Small remains the simplest and most compute-efficient option. For moderate budgets (ε ≥4), DP-\n7\n"}, {"page": 8, "text": "Distil achieves the best balance of accuracy and protection, recovering up to 63% of the non-private\nbaseline’s performance whilst maintaining near-random MIA outcomes (AUC ≈0.5). DP-Synthetic\nis currently unsuitable for deployment, and LoRA should be treated purely as a non-DP upper bound.\nOverall, knowledge distillation with differentially private teachers emerges as the most practical\npath toward deployable, privacy-preserving clinical language models, offering a clear direction for\nhealthcare institutions seeking to combine diagnostic utility with rigorous confidentiality guarantees.\nFuture work should extend these comparisons to multi-hospital datasets and longer clinical narratives,\nestablishing benchmarks for truly deployable privacy-preserving clinical language models.\nAcknowledgments\nThis work was conducted as part of an MSc thesis at Imperial College London, supervised by\nDr Andrew Duncan. We acknowledge Andy Thomas (Head of Research Computing, Imperial\nMathematics) for providing access to computing resources, and the MIMIC-III team and PhysioNet\nfor making the clinical database publicly available. Code is available at https://github.com/\nmathieu-dufour/dp-clinical-coding.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Security, CCS’16. ACM, October 2016. doi:\n10.1145/2976749.2978318. URL http://dx.doi.org/10.1145/2976749.2978318.\nNicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramèr.\nMembership inference attacks from first principles. CoRR, abs/2112.03570, 2021a. URL https:\n//arxiv.org/abs/2112.03570.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel.\nExtracting training data from large language models, 2021b. URL https://arxiv.org/abs/\n2012.07805.\nAli Dadsetan, Dorsa Soleymani, Xijie Zeng, and Frank Rudzicz. Can large language models be privacy\npreserving and fair medical coders?, 2024. URL https://arxiv.org/abs/2412.05533.\nCynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found. Trends\nTheor. Comput. Sci., 9(3–4):211–407, August 2014. ISSN 1551-305X. doi: 10.1561/0400000042.\nURL https://doi.org/10.1561/0400000042.\nJames Flemings and Murali Annavaram. Differentially private knowledge distillation via synthetic\ntext generation, 2024. URL https://arxiv.org/abs/2403.00932.\nTommaso Furlanello, Zachary C. Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.\nBorn again neural networks, 2018. URL https://arxiv.org/abs/1805.04770.\nAaron Grattafiori, Abhimanyu Dubey, Hugo Touvron, and et al. The llama 3 herd of models, 2024.\nURL https://arxiv.org/abs/2407.21783.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https:\n//arxiv.org/abs/2106.09685.\nAlistair E. W. Johnson, Tom J. Pollard, Lu Shen, Li-wei H. Lehman, Mengling Feng, Mohammad\nGhassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G. Mark. Mimic-iii, a\nfreely accessible critical care database. Scientific Data, 3(1):160035, 2016. doi: 10.1038/sdata.\n2016.35. URL https://doi.org/10.1038/sdata.2016.35.\nSaber Malekmohammadi and Golnoosh Farnadi. Low-rank adaptation secretly imitates differentially\nprivate sgd, 2025. URL https://arxiv.org/abs/2409.17538.\n8\n"}, {"page": 9, "text": "Vaishnavh Nagarajan, Aditya Krishna Menon, Srinadh Bhojanapalli, Hossein Mobahi, and Sanjiv\nKumar.\nOn student-teacher deviations in distillation: does it pay to disobey?, 2024.\nURL\nhttps://arxiv.org/abs/2301.12923.\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks\nagainst machine learning models, 2017. URL https://arxiv.org/abs/1610.05820.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels, 2023. URL https://arxiv.org/abs/2302.13971.\nAshkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani\nMalek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya\nMironov. Opacus: User-friendly differential privacy library in pytorch. CoRR, abs/2109.12298,\n2021. URL https://arxiv.org/abs/2109.12298.\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan\nKulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang.\nDifferentially private fine-tuning of language models.\nCoRR, abs/2110.06500, 2021.\nURL\nhttps://arxiv.org/abs/2110.06500.\nXiang Yue, Huseyin A. Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari, Huan Sun,\nDavid Levitan, and Robert Sim. Synthetic text generation with differential privacy: A simple and\npractical recipe, 2023. URL https://arxiv.org/abs/2210.14348.\n9\n"}, {"page": 10, "text": "A\nSupplementary material\nA.1\nDetailed dataset statistics\nMIMIC-III discharge summaries were filtered to 49,426 admissions with at least one top-50 ICD-9\ncode, split 80/10/10% into training (44,778 notes from 39,540 admissions), validation (5,624 notes),\nand test (5,586 notes) sets. Notes average 2,993 tokens (before truncation to 512) and 4.5 codes. Top-5\ncodes: 401.9 (hypertension, n=17,920), 428.0 (heart failure, n=11,933), 427.31 (atrial fibrillation,\nn=11,517), 414.01 (coronary atherosclerosis, n=11,011), 584.9 (acute kidney failure, n=7,936).\nA.2\nFull training details\nAll models trained for maximum 18 epochs with early stopping (patience=3). DP methods used\nSGD with momentum 0.9 and learning rates of 1.5e-3 for 1B models and 2e-4 to 3e-4 for 3B models.\nNon-DP methods used AdamW with cosine scheduling and learning rates of 5e-4 to 7.5e-4. Gradient\nclipping norms: C=1.0 (1B), C=0.7 (3B). Batch sizes were maximised within 48GB GPU memory\nconstraints. Synthetic generation used nucleus sampling (p=0.9, temperature=0.8).\nA.3\nTraining efficiency and resource requirements\nTable 4: Wall-clock training time by pipeline using a single RTX 6000 Ada GPU (48GB VRAM)\nwith maximum batch sizes. Times in hours.\nPipeline\nε\nGen.\nSynth.\nTeacher\nStudent/\nTotal\nPeak\nTrain\nGen.\nTrain\nClassifier\n(h)\nVRAM (GB)\nLoRA-No-DP\n∞\n–\n–\n–\n3.07\n3.07\n41.11\nDP-Small\n2\n–\n–\n–\n4.91\n4.91\n42.24\nDP-Small\n4\n–\n–\n–\n5.60\n5.60\n42.24\nDP-Small\n6\n–\n–\n–\n9.56\n9.56\n42.24\nDP-Synthetic\n2\n8.45\n3.33\n–\n2.67\n14.45\n41.11\nDP-Synthetic\n4\n8.72\n3.33\n–\n3.16\n15.21\n41.11\nDP-Synthetic\n6\n8.53\n3.33\n–\n2.68\n14.55\n41.11\nDP-Distil\n2\n10.86\n3.33\n18.82\n5.06\n38.06\n41.11\nDP-Distil\n4\n11.05\n3.33\n21.59\n2.34\n38.31\n41.11\nDP-Distil\n6\n11.34\n3.33\n29.93\n4.70\n49.29\n41.11\nTable 5: Inference metrics (mean across all 1B classifiers and 3B DP-Distil teachers) measured on a\nsingle RTX 6000 Ada GPU with a batch size of 64 and a maximum sequence length of 512 tokens.\nModel\nLatency\nThroughput\nPeak VRAM\nAdapter\n(ms/example)\n(tokens/s)\n(MB)\n(MB)\n1B classifier\n13.06\n38,376\n4,432\n520\n3B classifier (DP-Distil teacher)\n35.65\n14,054\n8,462\n1,529\nA.4\nCode availability\nComplete code for data preprocessing, model training, and evaluation is available at https:\n//github.com/mathieu-dufour/dp-clinical-coding under a CC-BY-4.0 licence. The repos-\nitory includes a clear README describing how to reproduce results, and numbered scripts to run in\nsequence to perform the full research computations.\n10\n"}]}