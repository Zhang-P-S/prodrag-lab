{"doc_id": "arxiv:2511.05810", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.05810.pdf", "meta": {"doc_id": "arxiv:2511.05810", "source": "arxiv", "arxiv_id": "2511.05810", "title": "DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis", "authors": ["Bowen Xu", "Xinyue Zeng", "Jiazhen Hu", "Tuo Wang", "Adithya Kulkarni"], "published": "2025-11-08T02:51:21Z", "updated": "2025-11-16T22:54:36Z", "summary": "Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \\texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.05810v2", "url_pdf": "https://arxiv.org/pdf/2511.05810.pdf", "meta_path": "data/raw/arxiv/meta/2511.05810.json", "sha256": "f4eed48fbb499e6a3d84a71a510451bd27d0a639b9b1acecd87adbddbc70bd15", "status": "ok", "fetched_at": "2026-02-18T02:28:21.461083+00:00"}, "pages": [{"page": 1, "text": "DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable\nDisease Diagnosis\nBowen Xu*1, Xinyue Zeng*1,\nJiazhen Hu1, Tuo Wang1, Adithya Kulkarni2\n1Department of Computer Science, Virginia Tech\n2Department of Computer Science, Ball State University\nAbstract\nBuilding trustworthy clinical AI systems requires not only ac-\ncurate predictions but also transparent, biologically grounded\nexplanations. We present DiagnoLLM, a hybrid frame-\nwork that integrates Bayesian deconvolution, eQTL-guided\ndeep learning, and LLM-based narrative generation for in-\nterpretable disease diagnosis. DiagnoLLM begins with GP-\nunmix, a Gaussian Process-based hierarchical model that in-\nfers cell-type-specific gene expression profiles from bulk and\nsingle-cell RNA-seq data while modeling biological uncer-\ntainty. These features, combined with regulatory priors from\neQTL analysis, power a neural classifier that achieves high\npredictive performance in Alzheimer’s Disease (AD) detec-\ntion (88.0% accuracy). To support human understanding and\ntrust, we introduce an LLM-based reasoning module that\ntranslates model outputs into audience-specific diagnostic re-\nports, grounded in clinical features, attribution signals, and\ndomain knowledge. Human evaluations confirm that these re-\nports are accurate, actionable, and appropriately tailored for\nboth physicians and patients. Our findings show that LLMs,\nwhen deployed as post-hoc reasoners rather than end-to-end\npredictors, can serve as effective communicators within hy-\nbrid diagnostic pipelines.\nIntroduction\nAccurate disease diagnosis using transcriptomic data is a\ncentral goal in biomedical AI, yet it remains a formidable\nchallenge due to two persistent bottlenecks: (1) the inability\nto extract cell-type-specific (CTS) signals from noisy bulk\nRNA-seq data, and (2) the lack of interpretable, clinically\nactionable explanations for model predictions. These limi-\ntations are especially pronounced in neurodegenerative dis-\neases like Alzheimer’s Disease (AD), where pathology of-\nten manifests in specific brain cell types such as microglia\nand astrocytes (Blumenfeld et al. 2024; Brendel et al. 2022).\nBulk RNA-seq, the most widely available data modal-\nity, aggregates expression over heterogeneous cell popula-\ntions, thereby masking critical disease signals (Natri et al.\n2024). Single-cell RNA-seq (scRNA-seq) offers higher res-\nolution (Tasic et al. 2018; Paik et al. 2020; Yao et al. 2021),\nbut its high cost, technical complexity, and sparsity in clin-\nical cohorts make it impractical for widespread diagnostic\nuse.\n*These authors contributed equally.\nTo bridge this gap, deconvolution methods have emerged\nthat estimate CTS profiles from bulk RNA-seq using single-\ncell references (Xu et al. 2025; Tang et al. 2024). However,\nexisting methods often fail to generalize due to their sensitiv-\nity to reference-target mismatch, lack of robust uncertainty\nmodeling, and inability to propagate prior biological knowl-\nedge (Torroja and Sanchez-Cabo 2019). At the same time,\nmost downstream disease classifiers treat gene expression\npurely as numerical input, ignoring known regulatory mech-\nanisms such as expression quantitative trait loci (eQTLs)\nthat offer causal, cell-type-aware insights into disease pro-\ngression (Nica and Dermitzakis 2013; Natri et al. 2024).\nEven when accurate classifiers are developed, their “black-\nbox” nature severely limits clinical adoption. Physicians and\npatients require not only predictions, but also clear, faithful\nexplanations grounded in known biology (Blumenfeld et al.\n2024). Recent work has explored the use of large language\nmodels (LLMs) in biomedical tasks (Yang et al. 2023; Gao\net al. 2024; Han et al. 2022; Jiang, Zhang, and Xu 2025), yet\nmost applications focus on end-to-end generation or infor-\nmation extraction. These approaches often lack alignment\nwith underlying model behavior, leading to hallucinations\nand eroding trust in high-stakes settings (Omar et al. 2024;\nChen, Luo, and Li 2025; Zhao et al. 2022; Zhang et al. 2024;\nLin et al. 2025; Zeng et al. 2025b,a). Furthermore, LLMs\nexhibit well-documented limitations in handling symbolic\nreasoning and numerical precision (Hegselmann et al. 2023;\nZhang et al. 2025; Hu et al. 2025; Lu et al. 2022), especially\nwhen used as primary decision-makers.\nWe argue that a reliable clinical AI system must unify\nthree capabilities: robust signal extraction, biologically\ngrounded prediction, and audience-specific explanation. To\nthis end, we propose DiagnoLLM, a hybrid neuro-symbolic\nframework for interpretable disease diagnosis. As shown\nin Figure 1, DiagnoLLM integrates Bayesian deconvo-\nlution, regulatory genomics, and LLM-based explanation\ninto a two-stage diagnostic pipeline. Stage 1 introduces\nGP-unmix, a Gaussian Process-based hierarchical model\nthat deconvolves bulk RNA-seq into CTS expression matri-\nces using single-cell references. GP-unmix introduces poste-\nrior refinement steps to correct for reference-target shifts, in-\ncorporates uncertainty modeling via multivariate priors, and\nfilters gene-cell-type pairs using biologically informed se-\nlection strategies. Compared to existing methods (Tang et al.\narXiv:2511.05810v2  [cs.AI]  16 Nov 2025\n"}, {"page": 2, "text": "Top Contributing \nFeatures\nFinal Inferred \nCTS Expression\nBayesian Inferred \nIntermediate CTS \nExpression\nEQTL \nAnalysis\nStep 1\nStep 2\nGP-Unmix Model\nUpdated Prior\nProbability of \ndisease\nLLM Reasoning\nDL Prediction\nStage 1: GP-Unmix\nStage 2: LLM Diagnosis\nSingle-cell \nRNA-seq\nBulk \nRNA-seq\nInterpretable \nDisease \nDiagnosis\nOriginal \nClinical Data\nDomain \nKnowledge\nFigure 1: Overview of the DIAGNOLLM framework.\nStage 1 (GP-Unmix) performs Bayesian deconvolution of\nbulk RNA-seq into CTS expression using single-cell ref-\nerences. Stage 2 combines eQTL-informed DL predictions\nwith LLM-based reasoning to produce human-readable di-\nagnostic reports, linking model outputs with clinical inter-\npretability.\n2024; Xu et al. 2025; Torroja and Sanchez-Cabo 2019), it\nachieves significantly higher gene-level recovery across tis-\nsues, species, and modalities. Stage 2 enriches prediction\nwith eQTL-derived regulatory features that guide a two-\nlayer neural network classifier. These regulatory priors em-\nphasize disease-relevant transcriptional mechanisms (Gusev\net al. 2016; Natri et al. 2024), improving both classifica-\ntion performance (88.0% accuracy on AD) and biological\nalignment. To address the interpretability gap, we introduce\na language-based reasoning module, where an LLM trans-\nlates classifier outputs and feature attributions into struc-\ntured, audience-specific diagnostic reports tailored for clin-\nicians and patients. This module is grounded in attribution\nscores and domain knowledge, ensuring factual alignment\nand avoiding free-form hallucination (Yu et al. 2025).\nWe further justify this hybrid design through a targeted di-\nvergence analysis. We find that LLMs often misclassify sam-\nples with conflicting symbolic cues (e.g., a positive AD label\nwith a negative BETA value), but outperform neural models\nin out-of-distribution regions by leveraging domain priors.\nConversely, the MLP is more robust to statistical variance\nbut fails in rare or extreme cases. This complementary be-\nhavior motivates our neuro-symbolic integration: the neural\nmodel handles structured prediction, while the LLM serves\nas a post-hoc communicator and narrative generator.\nOur contributions are threefold:\n1. GP-unmix for cell-type deconvolution: A Bayesian\nframework with dynamic prior refinement and gene selec-\ntion for uncertainty-aware recovery of CTS expression.\n2. eQTL-guided neural prediction: A classifier trained on\ndeconvolved and regulatory features that demonstrates im-\nproved accuracy and mechanistic alignment in AD detec-\ntion.\n3. LLM-based explanation module: A structured prompt-\ning system that generates clinically actionable reports,\nevaluated across user trust, rationale completeness, and\naudience-appropriateness.\nDiagnoLLM bridges probabilistic modeling, biological\nreasoning, and natural language explanation, offering a path\nforward for interpretable AI in real-world clinical diagnos-\ntics.\nRelated Work\nCTS Estimation and Deconvolution Models.\nCell-type-\nspecific (CTS) expression estimation is critical for uncover-\ning disease-relevant signals from heterogeneous tissue data.\nTraditional deconvolution approaches such as TCA and\nbMIND (Xu et al. 2025; Torroja and Sanchez-Cabo 2019)\nestimate CTS proportions using single-cell reference data,\nbut often lack uncertainty modeling and fail to generalize\nacross tissues or species. Our framework extends this line of\nwork by introducing GP-unmix, a Bayesian model with dy-\nnamic posterior refinement and biologically grounded prior\nselection that produces uncertainty-aware CTS profiles.\nLLMs for Disease Analysis.\nLarge language models\n(LLMs) have increasingly been explored for biomedical\ntasks, including disease classification, literature summariza-\ntion, and biomarker discovery (Yang et al. 2022; Levine et al.\n2024; Zeng et al. 2025c). Understanding disease-relevant\ncell types is essential for mechanistic modeling in com-\nplex conditions like Alzheimer’s Disease (Omar et al. 2024;\nGiuffr`e et al. 2024). Advances in single-cell RNA-seq (Mi-\nranda et al. 2023; Mathys et al. 2019) and CTS analysis (Ja-\ngadeesh et al. 2022; Hampel et al. 1998) have enabled high-\nresolution disease modeling, but extracting insights from\nsuch data requires models that are both scalable and inter-\npretable. Most existing LLM-based systems rely on pretrain-\ning to encode domain knowledge, but they often operate as\nblack-box classifiers or free-form generators, limiting their\nreliability in clinical decision-making (Elsborg and Salva-\ntore 2023).\nLLMs for Cell-Type Annotation and Omics Reasoning.\nRecent work has integrated LLMs into single-cell work-\nflows to automate cell-type annotation and improve repro-\nducibility. scInterpreter (Li et al. 2024) uses LLMs to inter-\npret scRNA-seq profiles, while the Single-Cell Omics Arena\nbenchmark (Liu et al. 2024) evaluates LLM performance\nin multi-omics classification and cross-modality translation.\nThese frameworks demonstrate LLMs’ potential to reduce\nmanual burden in biological annotation tasks. However, such\nsystems generally focus on stand-alone annotation rather\nthan aligning explanations with structured model predictions\nor attribution signals, as we propose in DiagnoLLM.\nLLMs for Numerical and Tabular Reasoning.\nLLMs\nhave also been adapted to handle structured numerical and\ntabular inputs. LIFT (Dinh et al. 2022) converts structured\nnumerical data into natural language to enable LLMs like\nGPT-3 (Brown et al. 2020) and GPT-J (Wang and Komat-\nsuzaki 2021) to perform classification and regression tasks\nwith performance rivaling or surpassing conventional mod-\nels. For nonlinear tasks, LIFT outperforms decision trees\nand deep MLPs, and for regression, it exceeds polynomial\nand nearest-neighbor methods. TabLLM (Hegselmann et al.\n"}, {"page": 3, "text": "2023) extends this idea by using natural language serializa-\ntion for zero- and few-shot classification, achieving com-\npetitive performance against strong ML baselines includ-\ning logistic regression, XGBoost (Chen and Guestrin 2016),\nLightGBM (Ke et al. 2017), TabNet (Arik and Pfister 2021),\nand TabPFN (Hollmann et al. 2022), as well as founda-\ntion models like T0 (Sanh et al. 2021). To improve nu-\nmeric precision, LUNA (Han et al. 2022) introduces numeric\naugmentations and representations (NumTok and NumBed)\nin transformer models such as BERT (Devlin et al. 2018)\nand RoBERTa (Liu et al. 2019). These enhancements allow\nLLMs to handle numeric inputs more faithfully, but chal-\nlenges remain regarding symbolic overgeneralization and in-\nterpretability in real-world domains (Yang et al. 2023; Gao\net al. 2024; Kulkarni et al. 2025). In addition to health-\ncare, LLMs for numerical reasoning have also been explored\nin finance (Ma et al. 2025; Zhu et al. 2024) and mathe-\nmatical modeling (Schwartz et al. 2024; Lee et al. 2023),\nwhere grounding, consistency, and multi-step reasoning re-\nmain open challenges.\nUnlike prior methods that employ LLMs as end-to-end\npredictors or annotation tools, DiagnoLLM adopts a neuro-\nsymbolic architecture that decouples numerical prediction\nfrom natural language explanation. The classifier is trained\non biologically grounded features derived from Bayesian\ndeconvolution and eQTL-informed priors (Nica and Der-\nmitzakis 2013; Natri et al. 2024), ensuring mechanistic rel-\nevance. Meanwhile, the LLM functions as a structured rea-\nsoning module, guided by attribution scores and tailored to\naudience-specific interpretability goals. This hybrid design\naddresses a central gap in the literature: integrating LLMs\ninto clinical pipelines not as autonomous decision-makers,\nbut as faithful narrators of model behavior. While post-hoc\ninterpretability tools such as SHAP and LIME are com-\nmonly used in clinical AI (Lundberg and Lee 2017; Ribeiro,\nSingh, and Guestrin 2016), they often lack biological context\nand are sensitive to perturbation artifacts. In contrast, Diag-\nnoLLM grounds its explanations in both domain knowledge\nand model saliency, enabling reliable, audience-aware diag-\nnostic narratives.\nMethodology\nWe present DiagnoLLM, a modular framework that com-\nbines Bayesian deconvolution, regulatory reasoning, and\nLLM-based interpretation to enable accurate and inter-\npretable disease diagnosis. It addresses bulk RNA-seq limi-\ntations by recovering cell-type-specific signals. We first de-\nfine the problem, then detail each component.\nProblem Statement\nLet X ∈RG×N denote a bulk RNA-seq gene expression\nmatrix, where G is the number of genes and N is the num-\nber of patient samples. Each column xi ∈RG represents\na bulk gene expression profile that conflates signals from\nmultiple cell types, thereby obscuring disease-relevant bi-\nological variation. Our objective is to develop a computa-\ntional pipeline that, given X and auxiliary metadata (such\nas genotype and clinical covariates), can: (1) Estimate an\nuncertainty-aware, cell-type-specific (CTS) expression ten-\nsor Z ∈RG×C×N, where C is the number of cell types.\nCTS expression refers to the gene activity levels attributable\nto each individual cell type, which are not directly measur-\nable in bulk RNA-seq, (2) Use these CTS representations in\nconjunction with expression quantitative trait loci (eQTLs),\ngenetic variants that influence gene expression, to predict\ndisease status (e.g., Alzheimer’s Disease), and (3) Produce\nnatural language diagnostic reports that are aligned with\nmodel predictions and enriched with clinically relevant bi-\nological context, targeted at both physicians and patients.\nThis problem is especially challenging because (1)\nground-truth CTS expression is not observed during train-\ning, (2) the influence of genetic variation on expression is\noften nonlinear and context-dependent, and (3) explanatory\noutputs must be both accurate and understandable to diverse\nusers.\nThe DIAGNOLLM Framework\nTo address the limitations of bulk RNA-seq in capturing cell-\ntype-specific signals, the need for biologically grounded pre-\ndiction, and the demand for interpretable clinical outputs, we\npropose DIAGNOLLM—a modular neuro-symbolic frame-\nwork that integrates Bayesian deconvolution, regulatory rea-\nsoning, and structured language-based interpretation. Un-\nlike prior approaches that treat LLMs as end-to-end pre-\ndictors, DIAGNOLLM separates statistical inference from\nexplanation: a neural classifier predicts disease status us-\ning features derived from Bayesian deconvolution and eQTL\npriors, while an LLM generates post-hoc, audience-specific\ndiagnostic narratives. The framework is guided by three\ncore principles: (1) extracting fine-grained molecular signals\nfrom noisy data, (2) grounding predictions in validated reg-\nulatory features, and (3) enabling transparent, clinically rel-\nevant explanations.\nSpecifically, DiagnoLLM tackles the three key model-\ning challenges as follows: (1) To disentangle confounded\nexpression signals, we introduce a Bayesian deconvolution\nmodule called GP-unmix, which infers gene-level expres-\nsion profiles at the resolution of individual cell types. The\nmethod leverages sc/snRNA-seq references and employs\na two-stage posterior refinement strategy to adapt to do-\nmain shifts between reference and target data. (2) To im-\nprove predictive accuracy and biological specificity, we in-\ncorporate known genetic regulatory variation in the form\nof eQTL priors. These priors are combined with CTS ex-\npression and clinical covariates to train a compact neural\nclassifier that predicts Alzheimer’s Disease (AD) status, and\n(3) To promote trust and usability, we use a large language\nmodel (LLM) as a post-hoc reasoning module that gener-\nates human-readable diagnostic reports. These reports are\ngrounded in model attributions and tailored for different au-\ndiences (e.g., physicians or patients). Figure 1 provides an\noverview of the full pipeline. Below, we describe each of the\nthree components in detail.\nBayesian Deconvolution via GP-unmix\nMotivation.\nA key obstacle in analyzing bulk RNA-seq\nis that it conflates gene expression across diverse cell\n"}, {"page": 4, "text": "types, thereby masking cell-type-specific (CTS) transcrip-\ntional programs that may be crucial for disease mechanisms.\nFor instance, Alzheimer’s related dysregulation in astrocytes\nor microglia may be diluted in bulk measurements domi-\nnated by neuronal signatures. Existing deconvolution meth-\nods, such as bMIND and TCA (Xu et al. 2025; Torroja and\nSanchez-Cabo 2019), focus primarily on estimating cell-\ntype proportions and lack the ability to recover full gene-\nlevel CTS expression with robust uncertainty quantification.\nMoreover, they often fail under domain shifts, such as dif-\nferences in species, tissue, or sequencing protocols between\nreference and target datasets.\nGP-unmix is a hierarchical Bayesian model designed to\naddress these challenges. It infers CTS expression matrices\nfrom bulk data by combining: (1) multivariate priors derived\nfrom single-cell RNA-seq references, (2) posterior refine-\nment to adapt to cohort-specific distributions, and (3) a tri-\npartite gene selection strategy to improve stability and bio-\nlogical relevance.\nModel Formulation.\nLet X ∈RG×N denote the bulk\nRNA-seq matrix of G genes and N patient samples. The\nlatent CTS expression tensor is denoted by Z ∈RG×C×N,\nwhere C is the number of cell types. For each gene g and\ncell type j, we model CTS expression across samples as:\nZgj ∼N(µj, Σj),\nXi = w⊤\ni Zi+Γ⊤\nj C(1)\ni\n+w⊤\ni BjC(2)\ni\n+εi\nHere, wi ∈RC denotes the cell-type proportions for sam-\nple i; C(1)\ni\ncaptures bulk-level technical covariates such as\nbatch effects; C(2)\ni\nrepresents latent confounders at the cell-\ntype-specific (CTS) level; Γj and Bj are learned adjustment\nmatrices that modulate the influence of these covariates; and\nεi ∼N(0, σ2\nj ) denotes the observation noise.\nReference-Informed\nInference.\nWe\ninitialize\npriors\n(µj, Σj)\nusing\nmean\nand\ncovariance\nstatistics\nfrom\nsc/snRNA-seq references (e.g., Tasic (Tasic et al. 2018),\nYao (Yao et al. 2021)). These empirical distributions anchor\nthe generative model and provide biologically grounded\nstarting points.\nPosterior Refinement.\nTo accommodate domain shifts\nbetween references and target cohorts, we update the priors\nusing posterior samples:\nµ(2)\nj\n∼N(d\nµ(1)\nj , τ 2I),\nΣ(2)\nj\n∼InvWishart( d\nΣ(1)\nj , v)\nInference is performed via MCMC sampling with con-\nvergence validated using Gelman–Rubin diagnostics ( ˆR <\n1.05). We summarize the posterior inference process in Al-\ngorithm 1.\nTripartite Gene Selection.\nWe reduce inference noise by\nselecting informative gene–cell-type pairs through three fil-\nters: 1. Core markers: manually curated genes known to\nmark specific cell types (e.g., SLC6A12, C3), 2. Statisti-\ncal stability: differentially expressed genes across modali-\nties (pFDR < 0.01, | log2 FC| > 1), and 3. Noise suppres-\nsion: filtering based on Seurat-derived differential expres-\nsion scores. This selection strategy improves downstream\nCTS estimation, yielding 37–54% higher Pearson correla-\ntion coefficients (PCC) compared to baselines across multi-\nple datasets, especially in low-abundance or high-noise cell\npopulations.\nAlgorithm 1: GP-unmix: Posterior Inference\nRequire: Bulk RNA-seq matrix X, reference dataset R, co-\nvariates C(1), C(2)\n1: Initialize priors (µ(0)\nj , Σ(0)\nj ) from R\n2: Select gene–cell-type pairs using tripartite gene filtering\n3: for each pair (g, j) do\n4:\nDraw samples: Z(1)\ngj ∼N(µ(0)\nj , Σ(0)\nj )\n5: end for\n6: Fit bulk mixture: Xi ≈w⊤\ni Zi + Γ⊤\nj C(1)\ni\n+ w⊤\ni BjC(2)\ni\n7: Estimate posteriors (d\nµ(1)\nj , d\nΣ(1)\nj )\n8: Update priors and repeat MCMC sampling for T itera-\ntions\n9: Return Posterior means E[Zgj\n|\nX], variances\nVar[Zgj | X]\nFigure 2: GP-unmix improves CTS recovery over TCA and\nbMIND across neuron subtypes in the Tasic dataset (Tasic\net al. 2018).\nEmpirical Validation.\nFigures 2 and 3 benchmark GP-\nunmix on the Tasic and Yao datasets, respectively. It\nachieves higher Pearson correlation than bMIND and TCA\nat both per-gene and per-sample levels, particularly on com-\nplex and low-abundance populations such as L6 CT, Lamp5,\nand Pvalb interneurons. In human brain data, GP-unmix\nachieves a median PCC of 0.82 for microglia and 0.78 for as-\ntrocytes. In PBMCs, it yields strong alignment with flow cy-\ntometry ground-truth (r = 0.71 for NK cells), outperform-\ning bMIND by over 105%. These CTS profiles enable down-\nstream regulatory analysis. In the ROSMAP Alzheimer’s\ncohort, GP-unmix reveals astrocyte-linked dysregulation in\nUDP-glucosyltransferase activity, a pathway implicated in\nneurodegenerative inflammation, thus demonstrating both\npredictive utility and biological interpretability.\neQTL-Guided Disease Classification\n"}, {"page": 5, "text": "Figure 3: GP-unmix outperforms baselines on astrocytes,\nmicroglia, and inhibitory subtypes in the Yao dataset (Yao\net al. 2021).\nMotivation.\nWhile CTS expression inferred by GP-unmix\nprovides granular transcriptional features, not all expression\nvariability is biologically meaningful or disease-relevant.\nExpression quantitative trait loci (eQTLs), genetic variants\nthat modulate gene expression in a cell-type-specific man-\nner, offer mechanistic priors that ground observed expres-\nsion shifts in underlying regulatory architecture. Incorpo-\nrating these priors enables the classifier to prioritize bio-\nlogically plausible features and reduce reliance on noise or\nconfounding signals, particularly in complex disorders like\nAlzheimer’s Disease (AD).\nFeature Construction.\nEach patient sample is represented\nusing a concatenation of three biologically motivated com-\nponents: 1. CTS expression vectors: Mean expression val-\nues for selected gene–cell-type pairs output by GP-unmix,\n2. eQTL-derived regulatory priors: For each gene, we in-\nclude effect size (BETA), uncertainty (SE), and statistical\nsignificance (PVAL), obtained from cohort-specific or pub-\nlic databases, and 3. Covariates: Demographic and technical\nconfounders such as age, sex, and batch indicators. These\nvectors are standardized and projected into a compact, bio-\nlogically grounded feature space for classification.\nClassifier Design.\nWe implement a two-layer feedforward\nneural network (MLP) trained using binary cross-entropy\nloss. The model includes ReLU activations, dropout regu-\nlarization, and early stopping based on validation loss. The\nsimplicity of this architecture is intentional to ensure trans-\nparency and compatibility with gradient-based attribution\nmethods.\nAttribution and Interpretability.\nTo enable post-hoc ex-\nplanation via LLMs, we compute feature attributions using\nIntegrated Gradients. These attributions, indicating the con-\ntribution of each input feature to the model’s output, serve as\nintermediate representations for structured diagnostic narra-\ntive generation in the next stage of the pipeline.\nLLM-Based Diagnostic Interpretation\nMotivation.\nDespite advances in disease classification ac-\ncuracy, clinical deployment requires models to produce ex-\nplanations that are understandable, faithful, and tailored to\ndifferent stakeholders. LLMs offer a promising solution by\ntranslating structured model outputs into human-readable re-\nports. However, when used as standalone predictors, LLMs\noften lack numerical grounding and exhibit inconsistency in\nreasoning. We address this by using the LLM as a post-hoc\ninterpretability module, conditioned on model outputs, fea-\nture attributions, and biomedical priors, to generate expla-\nnations aligned with both statistical evidence and clinical\nknowledge.\nInput Representation.\nThe LLM receives as input a\nstructured prompt composed of: 1. Predicted label and con-\nfidence: Binary AD prediction and associated softmax prob-\nability from the classifier, 2. Feature attributions: Saliency\nscores from Integrated Gradients, reflecting which CTS ex-\npression and eQTL features influenced the model’s deci-\nsion, and 3. Biological priors: Contextual information on\nkey genes (e.g., APOE, TREM2) derived from public liter-\nature or cohort-specific findings. These components are se-\nrialized into natural language using prompt templates, en-\nabling compatibility with standard instruction-tuned LLMs\nsuch as GPT-3.5 or BioMedLM.\nPrompting Strategies.\nWe experiment with three pro-\ngressively structured prompting approaches: 1. Direct Rea-\nsoning: The LLM receives a flattened list of features and\nis asked to output a diagnosis label with minimal guid-\nance. 2. Step-by-Step Reasoning: The LLM first summa-\nrizes distributions of features across known AD and non-\nAD populations, then compares the test case against these\nlearned distributions before making a decision. 3. Step-by-\nStep with Domain Knowledge: The prompt is augmented\nwith biomedical background (e.g., known AD gene signa-\ntures), encouraging the LLM to reason using established\nclinical knowledge alongside the model’s attributions.\nAudience-Specific Report Generation.\nUsing the same\ninternal representation, the LLM is prompted to generate di-\nagnostic narratives tailored to two user groups: 1. Clinician\nreports: Emphasize biomarker relevance, statistical confi-\ndence, and biological pathways involved, and 2. Patient sum-\nmaries: Simplify terminology and focus on actionable in-\nsights while preserving factual fidelity. These reports help\nclose the loop between high-performance predictive mod-\neling and real-world interpretability demands in precision\nmedicine.\nLLM as Post-hoc Interpretability Module\nWhile LLMs can occasionally outperform deep learning\nmodels in low-data settings, their use as end-to-end classi-\nfiers in clinical contexts is risky due to sensitivity to heuris-\ntics and lack of numerical rigor. In DiagnoLLM, we instead\ndeploy the LLM as a post-hoc interpretability module, con-\nditioned on outputs from a statistically trained neural model.\nThis design ensures predictive stability while enabling trans-\nparent and audience-aligned explanation.\n"}, {"page": 6, "text": "Each LLM-generated report is prompted with: (a) the\nMLP-predicted probability p(AD), and (b) the top-5 most\ninfluential features determined by Integrated Gradients, in-\ncluding feature values, attribution scores, and reference\nranges. The LLM is then prompted to output: (1) a binary de-\ncision (yes/no for AD), (2) a rationale grounded in feature-\nlevel reasoning, and (3) next-step recommendations (e.g.,\nclinical follow-up, lifestyle guidance). We generate two ver-\nsions per report: one for clinicians (technical terms and dif-\nferential risk framing) and one for patients (plain language\nwith actionable summaries).\nExperiments\nWe evaluate DIAGNOLLM’s hybrid architecture across\nthree fronts: (1) LLM prompting vs. deep learning baselines,\n(2) divergence analysis of LLM and MLP behaviors, and (3)\nexpert assessments of explanation quality.\nClassification Setup and Baseline Comparison\nWe\nevaluate\nLLM\nprompting\nstrategies\nfor\nbinary\nAlzheimer’s\nDisease\n(AD)\nclassification\non\na\nstruc-\ntured clinical dataset containing 28 features per patient,\nincluding biomarkers, vitals, and genetic markers. Two\ntraining regimes are considered: low-data (50 training sam-\nples) and full-data (100 training samples), both evaluated on\na fixed held-out test set of 100 randomly-sampled samples\nto ensure consistent comparison.\nAs a structured baseline, we implement a two-layer multi-\nlayer perceptron (MLP) trained on the same feature set. The\nMLP comprises two fully connected layers (input →16 →8\n→1) with ReLU activations, followed by a sigmoid output\nfor binary prediction. The model is trained using the Adam\noptimizer (learning rate = 0.001) with binary cross-entropy\n(BCE) loss. The LLM used in our study is GPT-4o-mini.\nTable 1 summarizes classification performance across all\nmethods. The MLP achieves the highest accuracy in the\nfull-data setting, but underperforms the LLM in the low-\ndata regime when domain knowledge is incorporated. The\nstructured prompt with domain guidance (LLM+Domain)\nachieves 90% accuracy and 0.89 F1, surpassing the MLP\nby 3 points, underscoring the utility of knowledge-informed\nprompting in data-scarce medical contexts.\nMethod\nTrain Size = 100\nTrain Size = 50\nACC\nF1\nACC\nF1\nMLP (DL)\n0.88\n0.86\n0.87\n0.88\nLLM-Direct\n0.48\n0.43\n0.50\n0.49\nLLM-Step\n0.70\n0.62\n0.77\n0.75\nLLM+Domain\n0.74\n0.69\n0.90\n0.89\nTable 1: Classification accuracy and F1 across prompting\nvariants and MLP baseline under different data regimes.\nDivergence Analysis of LLM and MLP Reasoning\nWhile our quantitative results highlight the benefits of com-\nbining deep learning with structured prompting, this sec-\ntion offers deeper insight into DiagnoLLM’s hybrid design.\nSpecifically, we compare the behavioral failure modes of the\nMLP and LLM components to justify why we treat the LLM\nas a post-hoc reasoner rather than a standalone classifier.\nSymbol-Sensitive Failures in LLMs\nLLMs tend to ap-\nply overly rigid rules when interpreting symbolic patterns,\nsuch as associating the sign of BETA with class labels. We\ncurated a test subset of 100 Alzheimer’s-positive (AD) sam-\nples with negative BETA values, along with corresponding\nSE and PVAL1, to evaluate LLM behavior under symbolic\nconflicts. On this diagnostic subset, the LLM achieved only\n38.71% accuracy, compared to the MLP’s 89.19%. Man-\nual inspection revealed that the LLM often ignored the un-\ncertainty (SE) or statistical significance (PVAL), leading to\nheuristic overfitting.\nOut-of-Distribution Magnitude Failures in MLPs\nIn\ncontrast, the MLP underperforms on test samples with fea-\nture values outside the training distribution. We constructed\na subset of 100 such outlier instances with at least one fea-\nture exceeding ±1σ from the training mean. On this set, the\nMLP accuracy dropped to 61.26%, while the LLM, lever-\naging biomedical priors, reached 88.00%. These results un-\nderscore the MLP’s fragility in sparse data regimes and the\nLLM’s generalization via conceptual knowledge.\nCase Studies of Model Divergence\nTo illustrate the above\npatterns, we present three representative cases in Table 2.\nEach example reveals a distinct strength or failure mode and\nsupports the use of a hybrid structure combining MLP pre-\ncision with LLM transparency.\nCase\nFeatures\nLabel MLP LLM Key Insight\n1:\nLLM\nEr-\nror\nBETA\n=\n-0.03185; SE\n=\n0.04911;\nPVAL\n=\n0.51671\nAD\nAD\nnon-\nAD\nLLM\napplies\nrigid\nrule\nto\nBETA\nsign;\nig-\nnores uncertainty\nand p-value.\n2:\nMLP\nEr-\nror\nBETA\n=\n0.976; SE =\n0.571; PVAL\n= 2.166\nAD\nnon-\nAD\nAD\nMLP\nmisclas-\nsifies\ndue\nto\nextreme\ninput;\nLLM\nleverages\ndomain priors.\n3:\nAgree-\nment\nBETA\n=\n0.041; SE =\n0.061; PVAL\n= 0.436\nAD\nAD\nAD\nLLM offers ex-\nplicit\nrationale\ntied\nto\nBETA\nrange;\nMLP\nis\nopaque.\nTable 2: Examples of divergence and complementarity be-\ntween MLP and LLM in AD classification.\nThese results validate our architectural choice: the MLP\nprovides stable decision-making grounded in statistical\nlearning, while the LLM augments transparency and robust-\nness via biomedical priors. Used as a post-hoc reasoning en-\ngine, the LLM mitigates model brittleness and enables ex-\n1BETA denotes the effect size of an eQTL on gene expression,\nSE is its standard error, and PVAL indicates the associated statisti-\ncal significance.\n"}, {"page": 7, "text": "planatory alignment with human reasoning, critical for real-\nworld adoption in clinical settings.\nLLM-Guided Interpretation: Case Studies and\nEvaluation\nWhile LLMs have shown promise in low-data regimes, their\nuse as standalone clinical classifiers remains limited by sym-\nbolic rigidity and poor numerical grounding. DiagnoLLM\ninstead adopts a hybrid strategy: the LLM functions purely\nas a post-hoc interpretability module, translating neural\nmodel outputs into structured, audience-specific diagnos-\ntic narratives. This separation ensures predictive reliabil-\nity while enhancing transparency through saliency-aware,\ndomain-grounded explanation.\nWe now present two representative case studies to illus-\ntrate how the LLM explanations reflect biological plausibil-\nity and align with known Alzheimer’s Disease (AD) mecha-\nnisms.\nPatient A: High-Risk APOE Carrier\n• Summary: APOE-positive, p(AD) = 0.83, Final deci-\nsion: AD\n• Key Features: Elevated triglycerides (220.78 mg/dL),\nlow albumin, poor diet, and high creatinine—all well-\nestablished risk factors amplified in APOE carri-\ners (Hunsberger et al. 2019).\n• Model Alignment: MLP prioritizes lipids and inflamma-\ntion; LLM weaves these into a narrative with recommen-\ndations for lipid management and neuroimaging.\nPatient B: Low-Risk APOE Non-Carrier\n• Summary: APOE-negative, p(AD) = 0.10, Final deci-\nsion: non-AD\n• Key Features: Age (80.37), elevated homocysteine\n(16.26 µmol/L), LDL cholesterol (111.93 mg/dL), and\nshort sleep duration (5 hours).\n• Model Alignment: MLP flags vascular and metabolic\nmarkers with moderate weights. LLM offers a conser-\nvative explanation, advising monitoring and lifestyle ad-\njustments.\nThese examples demonstrate the explanatory alignment\nbetween the LLM outputs and model attributions, as well as\ntheir grounding in established AD biology. They highlight\nthe utility of DiagnoLLM in producing audience-specific,\nbiologically coherent diagnostic reports that support real-\nworld clinical communication.\nSimulated User Study on Trust and Explanation\nQuality\nTo assess the real-world utility of our interpretability mod-\nule, we conducted a simulated user study focused on how\nstructured, audience-specific LLM-generated reports are\nperceived by human experts. The study involved raters with\nclinical training, who evaluated the trustworthiness, clarity,\nand actionability of diagnostic reports across two audience\ntypes: physicians and laypersons.\nEvaluation Design\nWe sampled 60 explanation reports\n(30 clinician-facing and 30 patient-facing) and asked three\nexpert raters to independently score each report across five\nqualitative dimensions: 1. Prediction Agreement: Does the\nrater agree with the model’s decision? 2. Feature Rationale:\nAre key predictive features explicitly referenced? 3. Action-\nability: Are meaningful next steps (e.g., lifestyle changes,\nfollow-ups) suggested? 4. Justification Coherence: Is the ex-\nplanation logically connected to the outcome? 5. Stylistic\nFit: Is the tone appropriate for the target audience?\nEvaluation\nDimension\nAgreement\n(%)\nComment\nPrediction\nAgreement\n100.0\nRaters accepted all deci-\nsions\nFeature Ratio-\nnale\n45.0\nAttribution often under-\nspecified\nActionability\n94.2\nStrong next-step guidance\nJustification\nCoherence\n17.5\nLogical flow needs refine-\nment\nStylistic Fit\n76.7\nGenerally\nwell-matched\ntone\nTable 3: Expert ratings across explanation dimensions (n=60\nreports).\nResults Summary\nAs Table 3 shows, all reports aligned\nwith model decisions, but only 45% referenced key features,\nand fewer than 20% offered clear evidence-to-conclusion\nreasoning. Still, 94.2% were rated actionable, and 76.7%\nwere appropriately styled for their target audience.\nIllustrative Examples\n• Patient A (Physician Report): Correct decision, but\ncited only “age” as rationale despite stronger drivers. Jus-\ntification was vague, but next steps and tone were appro-\npriate.\n• Patient B (Layperson Report): Correct diagnosis, but\nexplanation contained technical terms (“LDL,” “homo-\ncysteine”), making it less accessible.\n• Patient C (Physician Report): Strong alignment across\nall dimensions—clear reasoning, feature-based rationale,\nappropriate language.\nThese findings confirm that DIAGNOLLM produces ac-\ntionable, trusted explanations, but also reveal weaknesses in\nfeature attribution and logical coherence. These results un-\nderscore the need for stronger prompt grounding and closer\nintegration between model interpretation and generation.\nConclusion\nWe present DIAGNOLLM, a modular diagnostic frame-\nwork that unifies Bayesian deconvolution, genetic regula-\ntory modeling, and LLM-based interpretability to address\nkey challenges in clinically grounded, cell-type-aware dis-\nease prediction. Our proposed GP-unmix model recovers\nuncertainty-aware, cell-type-specific gene expression with\n"}, {"page": 8, "text": "high fidelity, substantially outperforming existing deconvo-\nlution approaches across species and tissues. By integrating\nthese expression profiles with eQTL priors, a deep learn-\ning classifier, and structured language model prompts, DI-\nAGNOLLM achieves robust prediction performance along-\nside transparent, audience-specific explanations. Through\ndetailed divergence analysis, biological case studies, and a\nsimulated user study, we demonstrate that our hybrid design\nmitigates the symbolic rigidity of LLMs and the numerical\nfragility of neural predictors, offering interpretability with-\nout sacrificing performance.\nReferences\nArik, S. ¨O.; and Pfister, T. 2021. Tabnet: Attentive inter-\npretable tabular learning. In Proceedings of the AAAI con-\nference on artificial intelligence, volume 35, 6679–6687.\nBlumenfeld, J.; Yip, O.; Kim, M. J.; and Huang, Y. 2024.\nCell type-specific roles of APOE4 in Alzheimer disease. Na-\nture Reviews Neuroscience, 25(2): 91–110.\nBrendel, M.; Su, C.; Bai, Z.; Zhang, H.; Elemento, O.; and\nWang, F. 2022. Application of deep learning on single-cell\nRNA sequencing data analysis: a review. Genomics, pro-\nteomics & bioinformatics, 20(5): 814–835.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nChen, T.; and Guestrin, C. 2016. Xgboost: A scalable tree\nboosting system. In Proceedings of the 22nd acm sigkdd\ninternational conference on knowledge discovery and data\nmining, 785–794.\nChen, Z.; Luo, X.; and Li, D. 2025.\nVisRL: Intention-\nDriven\nVisual\nPerception\nvia\nReinforced\nReasoning.\narXiv:2503.07523.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. arXiv preprint arXiv:1810.04805.\nDinh, T.; Zeng, Y.; Zhang, R.; Lin, Z.; Gira, M.; Rajput,\nS.; Sohn, J.-y.; Papailiopoulos, D.; and Lee, K. 2022. Lift:\nLanguage-interfaced fine-tuning for non-language machine\nlearning tasks. Advances in Neural Information Processing\nSystems, 35: 11763–11784.\nElsborg, J.; and Salvatore, M. 2023. Using LLM Models and\nExplainable ML to Analyse Biomarkers at Single Cell Level\nfor Improved Understanding of Diseases. bioRxiv, 2023–08.\nGao, Y.; Myers, S.; Chen, S.; Dligach, D.; Miller, T. A.;\nBitterman, D.; Churpek, M.; and Afshar, M. 2024. When\nRaw Data Prevails: Are Large Language Model Embed-\ndings Effective in Numerical Data Representation for Med-\nical Machine Learning Applications?\narXiv preprint\narXiv:2408.11854.\nGiuffr`e, M.; Kresevic, S.; Pugliese, N.; You, K.; and Shung,\nD. L. 2024. Optimizing large language models in digestive\ndisease: strategies and challenges to improve clinical out-\ncomes. Liver International, 44(9): 2114–2124.\nGusev, A.; Ko, A.; Shi, H.; Bhatia, G.; Chung, W.; Pen-\nninx, B. W.; Jansen, R.; De Geus, E. J.; Boomsma, D. I.;\nWright, F. A.; et al. 2016. Integrative approaches for large-\nscale transcriptome-wide association studies. Nature genet-\nics, 48(3): 245–252.\nHampel, H.; Teipel, S. J.; Alexander, G. E.; Horwitz, B.; Te-\nichberg, D.; Schapiro, M. B.; and Rapoport, S. I. 1998. Cor-\npus callosum atrophy is a possible indicator of region–and\ncell type–specific neuronal degeneration in Alzheimer dis-\nease: A magnetic resonance imaging analysis. Archives of\nneurology, 55(2): 193–198.\nHan, H.; Xu, J.; Zhou, M.; Shao, Y.; Han, S.; and Zhang, D.\n2022.\nLUNA: language understanding with number aug-\nmentations on transformers via number plugins and pre-\ntraining. arXiv preprint arXiv:2212.02691.\nHegselmann, S.; Buendia, A.; Lang, H.; Agrawal, M.; Jiang,\nX.; and Sontag, D. 2023. Tabllm: Few-shot classification of\ntabular data with large language models. In International\nConference on Artificial Intelligence and Statistics, 5549–\n5581. PMLR.\nHollmann, N.; M¨uller, S.; Eggensperger, K.; and Hutter,\nF. 2022.\nTabpfn: A transformer that solves small tabu-\nlar classification problems in a second.\narXiv preprint\narXiv:2207.01848.\nHu, M.; Wang, J.; Zhao, W.; Zeng, Q.; and Luo, L.\n2025. FlowMalTrans: Unsupervised Binary Code Transla-\ntion for Malware Detection Using Flow-Adapter Architec-\nture. arXiv:2508.20212.\nHunsberger, H. C.; Pinky, P. D.; Smith, W.; Suppirama-\nniam, V.; and Reed, M. N. 2019. The role of APOE4 in\nAlzheimer’s disease: strategies for future therapeutic inter-\nventions. Neuronal signaling, 3(2): NS20180203.\nJagadeesh, K. A.; Dey, K. K.; Montoro, D. T.; Mohan,\nR.; Gazal, S.; Engreitz, J. M.; Xavier, R. J.; Price, A. L.;\nand Regev, A. 2022.\nIdentifying disease-critical cell\ntypes and cellular processes by integrating single-cell RNA-\nsequencing and human genetics. Nature genetics, 54(10):\n1479–1492.\nJiang, F.; Zhang, Z.; and Xu, X. 2025. CMFDNet: Cross-\nMamba and Feature Discovery Network for Polyp Segmen-\ntation. arXiv:2508.17729.\nKe, G.; Meng, Q.; Finley, T.; Wang, T.; Chen, W.; Ma, W.;\nYe, Q.; and Liu, T.-Y. 2017. Lightgbm: A highly efficient\ngradient boosting decision tree. Advances in neural infor-\nmation processing systems, 30.\nKulkarni, A.; Alotaibi, F.; Zeng, X.; Wu, L.; Zeng, T.; Yao,\nB. M.; Liu, M.; Zhang, S.; Huang, L.; and Zhou, D. 2025.\nScientific Hypothesis Generation and Validation: Methods,\nDatasets, and Future Directions. arXiv:2505.04651.\nLee, N.; Sreenivasan, K.; Lee, J. D.; Lee, K.; and Papail-\niopoulos, D. 2023. Teaching arithmetic to small transform-\ners. arXiv preprint arXiv:2307.03381.\nLevine, D.; Rizvi, S. A.; L´evy, S.; Pallikkavaliyaveetil, N.;\nZhang, D.; Chen, X.; Ghadermarzi, S.; Wu, R.; Zheng, Z.;\nVrkic, I.; et al. 2024. Cell2Sentence: teaching large language\nmodels the language of biology. BioRxiv, 2023–09.\n"}, {"page": 9, "text": "Li, C.; Xiao, M.; Wang, P.; Feng, G.; Li, X.; and Zhou, Y.\n2024. scInterpreter: Training Large Language Models to In-\nterpret scRNA-seq Data for Cell Type Annotation. arXiv\npreprint arXiv:2402.12405.\nLin, J.; Zeng, X.; Zhu, J.; Wang, S.; Shun, J.; Wu, J.; and\nZhou, D. 2025. Plan and Budget: Effective and Efficient\nTest-Time Scaling on Large Language Model Reasoning.\narXiv preprint arXiv:2505.16122.\nLiu, J.; Xu, S.; Zhang, L.; and Zhang, J. 2024.\nSingle-\nCell Omics Arena: A Benchmark Study for Large Language\nModels on Cell Type Annotation Using Single-Cell Data.\narXiv:2412.02915.\nLiu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V.\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv preprint arXiv:1907.11692.\nLu, Y.; Yang, W.; Zhang, Y.; Chen, Z.; Chen, J.; Xuan, Q.;\nWang, Z.; and Yang, X. 2022. Understanding the Dynamics\nof DNNs Using Graph Modularity. arXiv:2111.12485.\nLundberg, S. M.; and Lee, S.-I. 2017. A unified approach\nto interpreting model predictions. Advances in neural infor-\nmation processing systems, 30.\nMa, T.; Du, J.; Huang, W.; Wang, W.; Xie, L.; Zhong, X.; and\nZhou, J. T. 2025. Llm knows geometry better than algebra:\nNumerical understanding of llm-based agents in a trading\narena. arXiv preprint arXiv:2502.17967.\nMathys, H.; Davila-Velderrain, J.; Peng, Z.; Gao, F.; Mo-\nhammadi, S.; Young, J. Z.; Menon, M.; He, L.; Abdurrob,\nF.; Jiang, X.; et al. 2019. Single-cell transcriptomic analysis\nof Alzheimer’s disease. Nature, 570(7761): 332–337.\nMiranda, A. M.; Janbandhu, V.; Maatz, H.; Kanemaru, K.;\nCranley, J.; Teichmann, S. A.; H¨ubner, N.; Schneider, M. D.;\nHarvey, R. P.; and Noseda, M. 2023. Single-cell transcrip-\ntomics for the assessment of cardiac disease. Nature Reviews\nCardiology, 20(5): 289–308.\nNatri, H. M.; Del Azodi, C. B.; Peter, L.; Taylor, C. J.;\nChugh, S.; Kendle, R.; Chung, M.-i.; Flaherty, D. K.; Mat-\nlock, B. K.; Calvi, C. L.; et al. 2024.\nCell-type-specific\nand disease-associated expression quantitative trait loci in\nthe human lung. Nature Genetics, 56(4): 595–604.\nNica, A. C.; and Dermitzakis, E. T. 2013. Expression quan-\ntitative trait loci: present and future. Philosophical Transac-\ntions of the Royal Society B: Biological Sciences, 368(1620):\n20120362.\nOmar, M.; Brin, D.; Glicksberg, B.; and Klang, E. 2024. Uti-\nlizing natural language processing and large language mod-\nels in the diagnosis and prediction of infectious diseases: A\nsystematic review. American Journal of Infection Control.\nPaik, D. T.; Cho, S.; Tian, L.; Chang, H. Y.; and Wu, J. C.\n2020. Single-cell RNA sequencing in cardiovascular devel-\nopment, disease and medicine. Nature Reviews Cardiology,\n17(8): 457–473.\nRibeiro, M. T.; Singh, S.; and Guestrin, C. 2016. ” Why\nshould i trust you?” Explaining the predictions of any clas-\nsifier. In Proceedings of the 22nd ACM SIGKDD interna-\ntional conference on knowledge discovery and data mining,\n1135–1144.\nSanh, V.; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika, L.;\nAlyafeai, Z.; Chaffin, A.; Stiegler, A.; Scao, T. L.; Raja, A.;\net al. 2021. Multitask prompted training enables zero-shot\ntask generalization. arXiv preprint arXiv:2110.08207.\nSchwartz, E.; Choshen, L.; Shtok, J.; Doveh, S.; Karlinsky,\nL.; and Arbelle, A. 2024. NumeroLogic: Number Encoding\nfor Enhanced LLMs’ Numerical Reasoning. arXiv preprint\narXiv:2404.00459.\nTang, C.; Sun, Q.; Zeng, X.; Yang, X.; Liu, F.; Zhao, J.;\nShen, Y.; Liu, B.; Wen, J.; and Li, Y. 2024. Cell-type spe-\ncific inference from bulk RNA-sequencing data by integrat-\ning single cell reference profiles via EPIC-unmix. bioRxiv.\nTasic, B.; Yao, Z.; Graybuck, L. T.; Smith, K. A.; Nguyen,\nT. N.; Bertagnolli, D.; Goldy, J.; Garren, E.; Economo,\nM. N.; Viswanathan, S.; et al. 2018.\nShared and distinct\ntranscriptomic cell types across neocortical areas. Nature,\n563(7729): 72–78.\nTorroja, C.; and Sanchez-Cabo, F. 2019.\nDigitaldlsorter:\ndeep-learning on scRNA-Seq to deconvolute gene expres-\nsion data. Frontiers in Genetics, 10: 978.\nWang, B.; and Komatsuzaki, A. 2021.\nGPT-J-6B: A 6\nBillion Parameter Autoregressive Language Model. https:\n//github.com/kingoflolz/mesh-transformer-jax.\nXu, X.; Li, R.; Mo, O.; Liu, K.; Li, J.; and Hao, P. 2025.\nCell-type deconvolution for bulk RNA-seq data using single-\ncell reference: a comparative analysis and recommendation\nguideline. Briefings in Bioinformatics, 26(1): bbaf031.\nYang, F.; Wang, W.; Wang, F.; Fang, Y.; Tang, D.; Huang, J.;\nLu, H.; and Yao, J. 2022. scBERT as a large-scale pretrained\ndeep language model for cell type annotation of single-cell\nRNA-seq data. Nature Machine Intelligence, 4(10): 852–\n866.\nYang, Z.; Mitra, A.; Liu, W.; Berlowitz, D.; and Yu, H. 2023.\nTransformEHR: transformer-based encoder-decoder genera-\ntive model to enhance prediction of disease outcomes using\nelectronic health records.\nNature communications, 14(1):\n7857.\nYao, Z.; Liu, H.; Xie, F.; Fischer, S.; Adkins, R. S.; Aldridge,\nA. I.; Ament, S. A.; Bartlett, A.; Behrens, M. M.; Van den\nBerge, K.; et al. 2021. A transcriptomic and epigenomic cell\natlas of the mouse primary motor cortex. Nature, 598(7879):\n103–110.\nYu, X.; Hu, X.; Wan, X.; Zhang, Z.; Wan, X.; Cai, M.; Yu,\nT.; and Xiao, J. 2025. A unified framework for cell-type-\nspecific eQTL prioritization by integrating bulk and scRNA-\nseq data. The American Journal of Human Genetics, 112(2):\n332–352.\nZeng, S.; Chang, X.; Xie, M.; Liu, X.; Bai, Y.; Pan, Z.; Xu,\nM.; Wei, X.; and Guo, N. 2025a. FutureSightDrive: Think-\ning Visually with Spatio-Temporal CoT for Autonomous\nDriving. arXiv:2505.17685.\nZeng, S.; Qi, D.; Chang, X.; Xiong, F.; Xie, S.; Wu, X.;\nLiang, S.; Xu, M.; and Wei, X. 2025b. JanusVLN: Decou-\npling Semantics and Spatiality with Dual Implicit Memory\nfor Vision-Language Navigation. arXiv:2509.22548.\n"}, {"page": 10, "text": "Zeng, X.; Wang, T.; Kulkarni, A.; Lu, A.; Ni, A.; Xing, P.;\nZhao, J.; Chen, S.; and Zhou, D. 2025c. DISPROTBENCH:\nA Disorder-Aware, Task-Rich Benchmark for Evaluating\nProtein Structure Prediction in Realistic Biological Con-\ntexts. arXiv:2507.02883.\nZhang, Y.; Chen, Q.; Zhou, J.; Wang, P.; Si, J.; Wang, J.; Lu,\nW.; and Qin, L. 2024. Wrong-of-Thought: An Integrated\nReasoning Framework with Multi-Perspective Verification\nand Wrong Information. arXiv:2410.04463.\nZhang, Y.; Liu, X.; Tao, R.; Chen, Q.; Fei, H.; Che, W.; and\nQin, L. 2025. ViTCoT: Video-Text Interleaved Chain-of-\nThought for Boosting Video Understanding in Large Lan-\nguage Models. arXiv:2507.09876.\nZhao, Z.; Zhai, Y.; Chen, B. M.; and Liu, P. 2022. BALF:\nSimple and Efficient Blur Aware Local Feature Detector.\narXiv:2211.14731.\nZhu, F.; Liu, Z.; Feng, F.; Wang, C.; Li, M.; and Chua, T. S.\n2024. TAT-LLM: A Specialized Language Model for Dis-\ncrete Reasoning over Financial Tabular and Textual Data. In\nProceedings of the 5th ACM International Conference on AI\nin Finance, 310–318.\n"}]}