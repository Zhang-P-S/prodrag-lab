{"doc_id": "arxiv:2512.08659", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.08659.pdf", "meta": {"doc_id": "arxiv:2512.08659", "source": "arxiv", "arxiv_id": "2512.08659", "title": "An Agentic AI System for Multi-Framework Communication Coding", "authors": ["Bohao Yang", "Rui Yang", "Joshua M. Biro", "Haoyuan Wang", "Jessica L. Handley", "Brianna Richardson", "Sophia Bessias", "Nicoleta Economou-Zavlanos", "Armando D. Bedoya", "Monica Agrawal", "Michael M. Zavlanos", "Anand Chowdhury", "Raj M. Ratwani", "Kai Sun", "Kathryn I. Pollak", "Michael J. Pencina", "Chuan Hong"], "published": "2025-12-09T14:46:16Z", "updated": "2025-12-09T14:46:16Z", "summary": "Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.08659v1", "url_pdf": "https://arxiv.org/pdf/2512.08659.pdf", "meta_path": "data/raw/arxiv/meta/2512.08659.json", "sha256": "630025d7079a98b7c4c65d35108a0bf8dbc6ea039e52356fe2f044f980d1f5d0", "status": "ok", "fetched_at": "2026-02-18T02:24:34.652707+00:00"}, "pages": [{"page": 1, "text": " \n1 \nAn Agentic AI System for Multi-Framework Communication Coding \n \nBohao Yang1, BS · Rui Yang2, MS · Joshua M. Biro3, PhD · Haoyuan Wang4, MS · Jessica L. \nHandley3, MA · Brianna Richardson5, BS · Sophia Bessias1, MPH · Nicoleta Economou-Zavlanos1, \nPhD · Armando D. Bedoya1,6, MD · Monica Agrawal1, PhD · Michael M. Zavlanos6, PhD · Anand \nChowdhury7, MD· Raj M. Ratwani3, PhD · Kai Sun7, MD · Kathryn I. Pollak 5,8,#, PhD · Michael \nJ. Pencina1,#, PhD · Chuan Hong1,#*  , PhD \n1  Department of Biostatistics and Bioinformatics, Duke School of Medicine, Durham, NC, USA \n2 Centre for Quantitative Medicine, Duke-NUS Medical School, Singapore, Singapore \n3 Medstar Health National Center for Human Factors in Healthcare, Washington, DC, USA \n4  Applied Mathematics and Computational Science, University of Pennsylvania, PA, USA \n5 Cancer Prevention and Control Research Program, Duke Cancer Institute, Durham, NC, USA \n6  Department of Medicine, Duke School of Medicine, Durham, NC, USA, Durham, NC, USA \n7 Department of Mechanical Engineering & Materials Science, Duke University, Durham, NC, \nUSA \n8  Department of Population Health Sciences, Duke School of Medicine, Durham, NC, USA \n#Kathryn I. Pollak, Michael J. Pencina and Chuan Hong are equally contributed \n"}, {"page": 2, "text": " \n2 \nCorrespondence to: Chuan Hong. Email: chuan.hong@duke.edu \n"}, {"page": 3, "text": " \n3 \nSUMMARY \nBackground  \nClinical communication is central to patient outcomes, yet large-scale human annotation of \npatient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. \nExisting approaches based on large language models typically rely on single-task models that \nlack adaptability, interpretability, and reliability, especially when applied across various \ncommunication frameworks and clinical domains. \nMethods  \nIn this study, we developed an Multi-framewOrk Structured Agentic AI system for Clinical \nCommunication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core \nagents, including a Plan Agent for codebook selection and workflow planning, an Update Agent \nfor maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-\nguided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a \nVerification Agent that provides consistency checks and feedback. To evaluate performance, we \ncompared MOSAIC’s outputs against gold-standard annotations created by trained human \ncoders. \nFindings  \n"}, {"page": 4, "text": " \n4 \nWe developed and evaluated MOSAIC using 26 gold-standard annotated transcripts for training \nand 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, \nMOSAIC achieved an overall F1 score of 92·8%. Performance was highest in the Rheumatology \nsubset (F1 = 96·2%) and strongest for Patient Behavior (e.g., patients asking questions, \nexpressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms \nbaseline benchmarking. \nInterpretation  \nMOSAIC demonstrates that agentic, multi-framework AI systems can enable high-fidelity and \ngeneralizable annotation of clinical communication at scale. By converting raw transcripts into \nstructured behavioral signals, the system offers infrastructure for quality improvement, shared \ndecision-making evaluation, and equity auditing. As ambient scribe technologies expand across \nhealth systems, tools like MOSAIC provide a foundation for transparent, reproducible, and \nhuman-guided analysis of clinical conversations. \n \n \n"}, {"page": 5, "text": " \n5 \nINTRODUCTION \nClinician-patient communication quality is a modifiable, measurable driver of clinical outcomes, \nwith specific behaviors (e.g., open-question rate, reflection-to-question ratio, interruptions, \nresponse latency, and empathic uptake) shaping trust, adherence, and fairness-sensitive \nendpoints.1,2 Evidence-based communication techniques (e.g., 5 A’s for smoking cessation, \nMotivational Interviewing) have been found to help patients change behaviors,3 and \ncommunication-centered workflows that surface adherence barriers enable targeted problem-\nsolving in chronic care.4 These practices yield downstream benefits, such as improved blood \npressure control, modest hemoglobin A1c gains in communication-mediated programs, and \nlower readmissions with stronger discharge communication.5,6 \n \nClinicians and patients often do not spontaneously improve their communication. They need \nfeedback and a chance to practice new skills to improve. To provide feedback, we need to code \ntheir communication. Communication coding assigns predefined labels to specific verbal and \nnonverbal behaviors at the level of utterances, turns, exchanges, or whole visits, so that rich, real-\nworld interactions become analyzable data.7,8 Using explicit codebooks and decision rules, \nhuman coders mark the presence, frequency, quality, and timing of behaviors and then derive \nmetrics, such as counts, rates, durations, and state transitions. Example event-level labels include \nempathic opportunities and empathic responses,9,10 open-ended questions and reflective \nstatements,1,11 interruptions,12 and shared decision-making moves.13 Global scales summarize \nrelational climate (e.g., respect, warmth, attentiveness, flow).7,14 The resulting time-stamped \nsequences support research, training, fidelity audits, and quality improvement.15,16 Recently, as \nthe volume of recorded patient–provider interactions grows, the need for scalable, reliable coding \n"}, {"page": 6, "text": " \n6 \nhas become increasingly urgent. For example, a 10-week pilot study reported AI scribe use in \nover 300,000 patient encounters involving thousands of physicians,17 illustrating how AI scribing \nfuels both the demand and the opportunity for automated approaches to communication coding, \nHowever, scaling communication coding introduces significant operational challenges. Manual \nmulti-framework annotation requires unitizing interactions (e.g., utterance boundaries, speaker \nroles) and juggling overlapping codebooks with precedence rules, driving cognitive load, coder \ndrift, and workflow inefficiencies. Fine-grained labeling requires double-coding, adjudication, \nand calibration, making it time- and cost-intensive. Consistency is difficult, especially for rare or \nambiguous labels: inter-coder disagreement rises with more frameworks, and codebook updates \ntrigger costly versioning and back-annotation. Boundary decisions and label conflicts reduce \nreproducibility. These constraints highlight the need for tools that can calibrate uncertainty, defer \nlow-confidence spans to humans, and learn continuously from adjudication feedback. \nLarge language models (LLMs) can generate structured annotations from text, but most existing \napproaches rely on single-pass prompting without feedback or correction. Embedding LLMs in \nagentic architectures mitigates this by introducing tools, memory, and goal-directed loops.18 \nExtending further, multi-agent systems can coordinate specialized LLM-agents across subtasks, \nan approach well-suited to communication coding, where overlapping frameworks and rare \nbehaviors must be managed in parallel.19,20 However, no multi-agent system has yet been \ndeveloped for communication coding, and developing such a system remains challenging due to \nthe need for dynamic framework selection, conflict resolution, uncertainty calibration, and \nadjudication-informed learning. \nIn this study, we developed MOSAIC, a Multi-framewOrk Structured Agentic AI system for \nClinical Communication. Designed to meet the growing demand for scalable and reliable \n"}, {"page": 7, "text": " \n7 \ncommunication coding, MOSAIC coordinates multiple specialized AI agents to annotate clinical \nconversations across overlapping frameworks, such as empathy, behavior change, and bias. \nUnlike existing single-task approaches, MOSAIC dynamically selects the appropriate framework \nfor each segment, reconciles conflicting labels, and flags low-confidence cases for human \nreview. It also learns from adjudicated feedback over time, enabling more consistent and \ntransparent annotation. By combining automation with human oversight, MOSAIC offers a \npractical and extensible solution for turning large volumes of patient–provider dialogue into \nstructured, actionable insights. \nMETHODS \nWe present the development and evaluation of MOSAIC in three parts: (1) data collection, \nincluding encounter recordings, gold-standard annotations, and codebooks (WISER, Global, \nIntervention, Patient Behavior, and Bias); (2) system architecture, featuring a LangGraph-based \nmulti-agent framework, RAG backbone, dynamic prompting, and human-in-the-loop workflow; \nand (3) evaluation design, including benchmarks, metrics, sensitivity analyses, and clinician \nfeedback. This study was approved by the Duke University Health System IRB (Pro00115358, \nPro00108633). \nData Collection  \nWe collected 76 patient–provider audio recordings across two clinical domains (rheumatology \nand OB/GYN), including 26 for training and 50 for evaluation. Transcripts were generated using \nan internal Ambient Digital Scribing (ADS) system and reviewed by clinicians for accuracy.21 \nAll transcripts were then manually annotated by trained coders to create gold-standard labels for \nMOSAIC development and evaluation. \n"}, {"page": 8, "text": " \n8 \n \nCodebook Configuration \nClinician-provided communication codebooks were reformatted for multi-agent use \n(Supplementary eTable 1).22 The WISER Codebook captures clinician behaviors and empathy; \nthe Global Codebook assesses relational quality (e.g., flow, warmth, concern expression); and \nthe Intervention Codebook supports fine-grained event-level coding of behavior change \nstrategies (Ask, Advise, Assess, Assist, Arrange). Additional codebooks include Patient \nBehaviors (e.g., question-asking [AQ], assertiveness [AR]) and Bias, which flags both explicit \nand subtle cues, stereotyping, guardedness, rapport mismatch, and trust signals, reflecting \nclinical power dynamics. \n \nProposed Multi-Agent Framework \nThe MOSAIC framework (Figure 1) employs a LangGraph-based23 multi-agent architecture to \nautomate multi-framework clinical communication coding. The workflow begins with \nPreprocessing to segment and batch transcripts, which are then processed by the Agentic Core \n(Plan, Update, Annotation, and Verification Agents) supported by a retrieval-augmented \ngeneration (RAG) backbone.24 The system securely interfaces with Duke’s Azure OpenAI \nGPT‑4o deployment to preserve patient privacy.25 \n"}, {"page": 9, "text": " \n9 \nPreprocessing. Transcripts from our internal ADS system21 are automatically preprocessed to \nensure compatibility and preserve context (Supplementary eMethod 1). Utterances are \nsegmented with assigned speaker roles and timestamps. Transcripts exceeding length thresholds \nare split into fixed-length, timestamp-aligned batches to accommodate LLM context limits \n(Supplementary eMethod 2 and Supplementary eFigure 1). \n \nPlan Agent. The Plan Agent coordinates the workflow by processing user inputs (transcripts, \ncodebooks, and prompts) and routing them to the appropriate agents. It sends updated codebooks \nto the Update Agent, transcripts to the Annotation Agent, assigns subtasks, validates inputs, and \ntriggers alerts for errors (e.g., missing data, malformed codebooks; Supplementary eMethod 2). \n \nUpdate Agent. The Update Agent is responsible for handling new/revised codebooks. When a \nnew codebook is provided, the agent parses and chunks its content, generates embeddings, and \nrebuilds the corresponding FAISS26 vector index. The retriever is then refreshed to ensure that \nthe Annotation Agent accesses the most up-to-date coding rules and examples. Once the update \nis complete, the system routes the transcript back to the annotation process for re-coding under \nthe updated framework. If no new codebook is detected, the agent bypasses the update and \nproceeds with the existing configuration (Supplementary eMethod 2). \n \nAnnotation Agent. The Annotation Agent assigns communication codes using codebook-specific \nagents (e.g., global, clinician, patient, bias). The Plan Agent selects the appropriate agent per \ntask. Each agent initializes a retriever, fetches relevant definitions and examples, and constructs \nprompts for LLM inference. RAG aligns transcripts with codebooks segmented into semantically \ncoherent chunks (via sliding window), embedded using MiniLM (all-MiniLM-L6-v2)27 and \n"}, {"page": 10, "text": " \n10 \nstored in a FAISS vector DB. Chunks are retrieved using MMR,28 reranked with MedCPT,29 and \nfiltered to retain only those with valid annotation tags, reducing hallucinations and enforcing \nlabel alignment (Supplementary eMethod 2 and eFigure 2). \n \nVerification Agent. The Verification Agent ensures output quality during both training and \ninference. In training mode, it compares predictions to gold-standard labels and computes \nmetrics to guide prompt and example refinement. In inference mode, it flags low-confidence or \ninconsistent annotations for human review. In both settings, it generates structured feedback that \nsupports ongoing system optimization (Supplementary eMethod 2). \n \nFeedback Optimization Loop. The Feedback Optimization Loop converts verification metrics \ninto prompt and annotation refinements. For each task, the Plan Agent builds a tailored prompt \ncomprising: (1) system instructions specifying the active codebook, (2) validated annotation \nlabels, (3) RAG-retrieved rule definitions and canonical examples, and (4) few-shot examples \nfrom the Example Library RAG, continuously updated via Verification Agent feedback \n(Supplementary eMethod 2 and eFigure 3). The sentence paired with its surrounding context, \nhuman coder label, and agent-generated label were stored in the Example Library as one \ncandidate few-shot examples (collected only from the 26 training transcripts). The examples \ninclude both correct matches and contrastive errors, with a precision-weighted policy to reduce \noverannotation. During training, Verification Agent metrics serve as reinforcement signals to: (1) \nupdate prompts, (2) promote high-value examples, (3) prune/reorder those less effective ones, \nand (4) adapt few-shot retrieval strategies. \nEvaluation and Performance Metrics \n"}, {"page": 11, "text": " \n11 \nThe MOSAIC framework was evaluated on a combined dataset of 50 transcripts spanning \nrheumatology and obstetrics/gynecology contexts, with annotations derived from five codebooks \n(WISER, Global, Intervention, Patient Behavior, and Bias).  \nTo assess the decision-making of the Plan Agent, we conducted a qualitative analysis of its \nrouting performance. We examined its ability to activate appropriate annotation sub-agents based \non user inputs and to generate warnings for malformed or ambiguous instructions. Transcripts \nand prompts representing a range of routing scenarios, including synonymous expressions, vague \nlanguage, and nonsensical inputs, were used to evaluate robustness and failure models. \nTo assess the annotation performance of MOSAIC, we computed accuracy, precision, recall, and \nF1-score at both the transcript and codebook levels. Accuracy was defined as the proportion of \ncorrectly classified annotation instances out of the total number of annotated instances. All \nmetrics were calculated as weighted averages, where each label’s contribution was proportional \nto its frequency in the gold-standard annotations. This weighting ensures that more commonly \noccurring codes have greater influence on the overall metric, while still retaining the ability to \ncapture performance on less frequent codes. \nFor subgroup comparisons (rheumatology vs. obstetrics/gynecology), we first computed \nperformance metrics for each individual transcript and then averaged them within each subgroup. \nFor codebook-specific performance, per-label metrics were aggregated across all transcripts \nassociated with that codebook to provide a behavioral dimension–specific evaluation. \nTo assess the contribution of system components, we evaluated four approaches on the test \ndataset with 50 transcripts. (1) The Single-Agent Baseline used a single LLM with codebook \nRAG, static prompts, and canonical examples, without multi-agent orchestration or verification. \n"}, {"page": 12, "text": " \n12 \n(2) Multi-Agent Without Optimization added Plan, Update, and Verification Agents but retained \nfixed prompts and codebook-only guidance. (3) Dynamic Prompting further included adaptive \nprompt assembly and tailored templates, without verification signals. (4) The Full MOSAIC \nsystem integrated all components (multi-agent coordination, codebook RAG, dynamic few-shot \nprompting, task-specific templates, example-library retrieval, and verification-informed \nfeedback). \nSensitivity Analysis \nTo assess robustness and design trade-offs, we varied key parameters in a series of experiments. \nWe tested LLM temperatures (0·0, 0·1, 0·3, 0·5, 0·7), balancing stability at lower settings \nagainst potentially richer outputs at higher ones. We also compared a single generic prompt \ntemplate against multiple task-specific templates across sub-agents, evaluating their impact on \nannotation reliability and contextual accuracy. \nCost and Efficiency Analysis \nTo assess potential gains in scalability and efficiency, we compared the time required for manual \nhuman annotation versus automated MOSAIC annotation. For human coding, we recorded the \naverage time required to annotate transcripts across varying codebooks and domains, based on \ninput from trained coders involved in the gold-standard label generation. For MOSAIC, we \nmeasured end-to-end runtime per transcript, including preprocessing, retrieval, prompt \nconstruction, LLM inference, and verification.  \n"}, {"page": 13, "text": " \n13 \nRESULTS \nWe begin by summarizing the dataset, followed by overall MOSAIC performance, transcript- \nand label-level variability, and a sentence-level comparison to gold annotations. We then \nbenchmark against baselines to assess contributions of RAG, verification, and adaptive \nprompting. Sensitivity analyses test robustness across parameters. Finally, we present the \nMOSAIC UI to highlight usability and transparency. \nDataset Characteristics \nThe training dataset comprised a total of 26 patient-provider transcripts (Table 1a). Clinical \ndomains included rheumatology (n = 14) and obstetrics/gynecology (n = 12). On average, each \ntranscript contained approximately 18·5 minutes of clean speech and 151·2 speaker turns \n(median = 141, IQR 97 - 195), with an average of 285·6 sentences (2558·5 words) per transcript. \nAll the 26 transcripts were gold-standard annotated through clinician review and trained coder \nverification. Across these transcripts, annotations spanned five codebooks (WISER, Global, \nIntervention, Patient Behavior, and Bias), yielding a total of 271 labeled segments and an \naverage of 10.5 codes per transcript. The testing dataset included 50 gold-standard–annotated \ntranscripts (rheumatology = 33; OB/GYN = 17) (Table 1b). On average, each transcript contained \n20 minutes of clean speech, 177·5 speaker turns (median = 145, IQR 98·3–237), 322·4 \nsentences, and 2871·9 words, yielding a total of 504 labeled segments. \nThe distribution of annotations varied across codebooks in both training and testing datasets. In \nthe training set (Supplementary eTable 2a), WISER was applied to 6 transcripts (103 labeled \nturns), followed by Patient Behavior (6 transcripts, 57 turns) and Bias (7 transcripts, 48 turns); \nIntervention and Global codes were less common, appearing in 2 and 5 transcripts, respectively. \n"}, {"page": 14, "text": " \n14 \nThe testing set (Supplementary eTable 2b) showed similar patterns, with WISER applied to 19 \ntranscripts (226 turns), Patient Behavior to 11 transcripts (144 turns), and Bias to 12 transcripts \n(67 turns), while Intervention and Global codes appeared in 5 (529 turns) and 3 transcripts (358 \nturns), respectively. \nQualitative Evaluation of Plan Agent \nWe qualitatively evaluated the Plan Agent’s routing behavior (Supplementary eResult 1 and \nSupplementary eTable 3). It reliably mapped clear prompts to correct sub-agents, recognized \nsynonyms (e.g., “empathy” → WISER), and handled malformed inputs. \nAnnotation Performance \nOverall and Codebook-Specific Performance. MOSAIC achieved an overall accuracy of 93·1% \nprecision of 93·4%, recall of 93·1%, and F1 of 93·0% and the annotation performance varied by \ncoding framework (Figure 2a and Supplementary eTable 4). Patient Behavior coding achieved \nthe highest F1 (95·1%), followed by Bias (94·8%) and WISER for clinician behaviors (94·0%). \nModerate performance was observed for Intervention (86·3%), while Global relational quality \nshowed the lowest F1 (83·1%).  \nCategory-Specific Performance. Performance remained strong across clinical domains, with \nrheumatology transcripts reaching an F1 of 96·2%, and obstetrics/gynecology transcripts \nshowing moderately lower performance with an F1 of 86·8% (Figure 2b and Supplementary \neTable 4).  \nTranscript-Level Variability. Transcript-level performance variability revealed that MOSAIC \nconsistently maintained high annotation quality across various clinical encounters, with a median \n"}, {"page": 15, "text": " \n15 \nF1 of 0·964 and tight interquartile ranges across all metrics (Figure 2b and Supplementary \neTable 4). Rheumatology transcripts exhibited slightly higher median performance than \nobstetrics/gynecology, likely reflecting more structured dialogue patterns. A small number of \ntranscripts showed performance dips, particularly in recall, corresponding to longer encounters \nor skewed label distributions. \nComparison With Baselines and Ablation Variants. We compared MOSAIC against a baseline \nconfiguration and three ablation variants (Figure 3). The single-agent baseline achieved a \nweighted F1 of 85·9%, while the automated multi-agent setup where the Plan Agent \nautomatically selected the appropriate codebook, performed comparably at 85·7%. Adding \ndynamic few-shot prompting improved performance further (F1 = 89·5%). The full MOSAIC \nsystem achieved the best performance (F1= 93·0%), representing a relative improvement of \n7·9% over the single-agent baseline.  \nSensitivity Analyses \nWe selected 0·3 as the default temperature and evaluated system performance across parameter \nconfigurations. At lower temperatures (≤ 0·1), the model generated highly consistent outputs, \nshowing limited flexibility (e.g., temperature = 0·0, F1 = 90·2%). Higher temperatures (0·3 to \n0·7) occasionally captured additional nuanced cases (temperature = 0·3, F1 = 92·8%; \ntemperature = 0·5, F1 = 91·9%; temperature = 0·7, F1 = 91·7%) but tended to miss certain \noriginal content. These results show robust performance, with 0·3 yielding the best balance. \nCost and Efficiency Analysis \n"}, {"page": 16, "text": " \n16 \nWe empirically compared annotation time between human coders and the MOSAIC system. \nHuman annotation required 60–90 minutes per 30-minute recording (approximately 20 minutes \nof clear speech), reflecting 2–3 times audio length and including double-coding of 10% of \ntranscripts. In contrast, MOSAIC completed end-to-end annotation, including preprocessing, \nretrieval, prompting, and verification, in 2–5 minutes per transcript. Unlike human workflows, \nMOSAIC supports automated, reproducible re-annotation: the Update Agent detects codebook \nchanges, refreshes the retrieval index, and re-runs the pipeline, enabling scalable and iterative \ndeployment. \nUser Interface \nTo support end-to-end use, we developed a secure Gradio-based web interface (Supplementary \neFigure 4) for uploading transcripts, configuring coding parameters, and reviewing annotations. \nUsers can select codebooks, adjust advanced settings (e.g., retrieval depth, chunk size), and \ntrigger the MOSAIC pipeline with one click. Outputs are viewable in editable formats \n(text, .docx, preview), with speaker turns, codes, and references. The interface enables manual \nedits, logs corrections, and updates the Example Library.  \nDISCUSSION \nIn this study, we developed MOSAIC, a LangGraph-based multi-agent framework for annotating \nclinical communication. It includes a Plan Agent to coordinate workflows, an Update Agent to \nrefresh retrieval databases with codebook revisions, a set of Annotation Agents that apply \ncodebook-guided RAG with rule-based definitions, and a Verification Agent ensures label \nconsistency with humans. Reliability is enhanced by dynamic few-shot prompting from an \nevolving example library.  \n"}, {"page": 17, "text": " \n17 \nMOSAIC achieved strong overall performance (F1 = 93·0%), exceeding typical human inter-\nrater agreement reported in communication coding studies (75–90%) and matching or surpassing \ntrained coder reliability for many annotation tasks.1,30 For example, in more structured domains \nlike rheumatology, MOSAIC reached an F1 score of 96·2%. Although variability persisted, \nparticularly in OB/GYN transcripts and Global codes, these patterns mirrored known challenges \nin human annotation, underscoring the system's clinical relevance. Performance gains were \nespecially notable in challenging categories like bias, where multi-agent coordination, codebook-\nguided retrieval (RAG), and dynamic few-shot prompting improved recall and label balance. \nSensitivity analyses identified temperature as a key factor: while lower settings (≤ 0·1) yielded \nconsistent but rigid outputs (e.g., temperature = 0·0, F1 = 90·2%), moderate values (0·3 to 0·7) \nimproved nuance capture (e.g., temperature = 0·3, F1 = 92·8%; 0·5, F1 = 91·9%; 0·7, F1 = \n91·7%) at the cost of some omissions. Additionally, retrieval caching and prompt reuse reduced \nlatency by approximately 25%. Together, these findings demonstrate that MOSAIC delivers \nhuman-level annotation quality with structured orchestration and retrieval-informed design, \noffering a scalable and robust alternative to single-pass prompting. \nTo improve interpretability, we evaluated MOSAIC at the sentence level using eight-turn \ncontext. Errors, particularly in Bias and Intervention labels, often arose from limited context, \nrevealing a trade-off between granularity and completeness. While performance declined relative \nto transcript-level runs, sentence-level analysis revealed precision gains and underscored the \npotential of audio cues and optimized chunking strategies for future improvement. \nBuilding on these observations, several patterns emerged with representative examples shown in \nTable 2. In successful cases, MOSAIC not only matched human annotations but also recovered \nmissed labels, especially in the WISER and Intervention frameworks. It correctly identified \n"}, {"page": 18, "text": " \n18 \nsubtle empathic or reflective statements that human coders overlooked, demonstrating sensitivity \nto relational and affective cues. However, challenging cases, particularly in the Global and Bias \nframeworks, exposed limitations of text-only input. Discrepancies often involve categories like \nAttentive vs. Concerned or Guarded–Open, where distinctions rely on paralinguistic features \nsuch as tone, emphasis, and speech tempo. These findings highlight the need for multimodal \nextensions that incorporate acoustic and temporal features to capture interpersonal nuance. \nMOSAIC thus serves a dual role: as a scalable annotator and a diagnostic tool that flags \nambiguous or voice-dependent cases. This capability may improve transparency and guide \ntargeted human review. Future iterations will integrate audio features and human-in-the-loop \nverification to better capture empathy, communication flow, and bias-related behaviors. \nSeveral broader limitations emerged. First, the current chunking strategy may not optimally \npreserve semantic coherence across conversational boundaries, potentially affecting recall. \nSecond, our retrieval pipeline using MiniLM embeddings and MedCPT reranking could likely be \nimproved with newer, domain-specific models. Third, the study’s focus on rheumatology and \nobstetrics/gynecology limits generalizability to other settings. Fourth, some codebooks (e.g., \nSDOH, Weight) were included in RAG retrieval but lacked sufficient gold-standard labels for \nevaluation. The dataset was also highly imbalanced, with most utterances labeled “None,” which \nmay skew metrics and reduce sensitivity to rare yet clinically important behaviors. Global coding \nunderperformed, reflecting its dependence on prosodic cues unavailable in text-only data. Model \nperformance remains sensitive to codebook quality and LLM variability, including hallucinations \nand sampling effects. Practical constraints include latency under multi-agent parallelism and the \nneed for cache invalidation to prevent stale retrieval. Finally, race and ethnicity data were \nunavailable, precluding subgroup analyses. Although MOSAIC does not use race-based inputs, \n"}, {"page": 19, "text": " \n19 \nsociocultural and structural factors may influence communication behaviors. Future work should \nevaluate generalizability across diverse populations and incorporate fairness-aware methods into \nsystem design. \nFuture work will target four areas: methodology, evaluation, generalization, and optimization. \nWe will refine chunking and embedding strategies to preserve conversational coherence, explore \nreinforcement learning (e.g., joint training, reward shaping) to enhance agent coordination and \nprompt adaptation, and incorporate multimodal inputs (e.g., tone, rhythm) for codes dependent \non relational or affective signals such as Global and Bias. Evaluation efforts will assess whether \nclearer prompts improve categories like Intervention, test the Verification Agent’s effectiveness, \nand examine the use of LLMs as judges for converting annotations into decision-support \ninsights. We aim to generalize MOSAIC to additional specialties, languages, and \nunderrepresented codes (e.g., SDOH, weight).  \nCONCLUSION \nMOSAIC demonstrates that an agentic, multi-agent framework can transform clinical \ncommunication coding into a scalable, accurate, and transparent process. By combining retrieval-\naugmented generation, structured verification, and adaptive prompting strategies, it enhances \nconsistency across diverse coding frameworks while retaining human-in-the-loop oversight. As \nAI scribes and ambient documentation tools increasingly generate vast volumes of clinical \ndialogue, systems like MOSAIC provide critical infrastructure to extract structured, trustworthy \ninsights. This capability not only supports quality improvement, training, and research but also \nenables real-time feedback loops that promote shared decision making and foster high-quality \ncare across clinical settings. \n \n"}, {"page": 20, "text": " \n20 \nREFERENCES \n1 \nMoyers TB, Rowell LN, Manuel JK, Ernst D, Houck JM. The motivational interviewing \ntreatment integrity code (MITI 4): rationale, preliminary reliability and validity. Journal of \nsubstance abuse treatment. 2016 Jun 1;65:36–42. \n2 \nPollak KI, Arnold RM, Jeffreys AS, et al. Oncologist communication about emotion during \nvisits with patients with advanced cancer. Journal of Clinical Oncology. 2007 Dec \n20;25(36):5748–52. \n3 \nLawson PJ, Flocke SA, Casucci B. Development of an instrument to document the 5A's for \nsmoking cessation. American journal of preventive medicine. 2009 Sep 1;37(3):248–54. \n4 \nCornelius T, Voils CI, Umland RC, Kronish IM. Validity of the self-reported domains of \nsubjective extent of nonadherence (DOSE-nonadherence) scale in comparison with \nelectronically monitored adherence to cardiovascular medications. Patient preference and \nadherence. 2019 Oct 3:1677–84. \n5 \nHuang X, Xu N, Wang Y, Sun Y, Guo A. The effects of motivational interviewing on \nhypertension management: a systematic review and meta-analysis. Patient education and \ncounseling. 2023 Jul 1;112:107760. \n6 \nBecker C, Zumbrunn S, Beck K, Vincent A, et al. Interventions to Improve Communication \nat Hospital Discharge and Rates of Readmission: A Systematic Review and Meta-analysis. \nJAMA Network Open. 2021 Aug 27;4(8):e2119346–e2119346. \n7 \nRoter D, Larson S. The Roter interaction analysis system (RIAS): utility and flexibility for \nanalysis of medical interactions. Patient education and counseling. 2002 Apr 1;46(4):243-\n51. \n8 \nBarton Laws M, Magill M, Mastroleo NR, et al. A sequential analysis of motivational \ninterviewing technical skills and client responses. Journal of Substance Abuse Treatment. \n2018 Sep 1;92:27–34. \n9 \nMalhotra C, Kanesvaran R, Krishna L, et al. Oncologists’ responses to patient and caregiver \nnegative emotions and patient perception of quality of communication: results from a multi-\nethnic Asian setting. Supportive Care in Cancer. 2018 Mar;26(3):957–65. \n10 Shen MJ, Ostroff JS, Hamann HA, et al. Structured analysis of empathic opportunities and \nphysician responses during lung cancer patient-physician consultations. Journal of health \ncommunication. 2019 Sep 2;24(9):711–8. \n11 Miller WR, Rollnick S. Motivational Interviewing. Guilford Publications; 2023. 354 p. \n12 Marvel MK, Epstein RM, Flowers K, Beckman HB. Soliciting the patient's agenda: have we \nimproved?. Jama. 1999 Jan 20;281(3):283–7. \n13 Krupat E, Frankel R, Stein T, Irish J. The Four Habits Coding Scheme: validation of an \n"}, {"page": 21, "text": " \n21 \ninstrument to assess clinicians’ communication behavior. Patient education and counseling. \n2006 Jul 1;62(1):38–45. \n14 Tanana M, Hallgren KA, Imel ZE, Atkins DC, Srikumar V. A comparison of natural \nlanguage processing methods for automated coding of motivational interviewing. Journal of \nsubstance abuse treatment. 2016 Jun 1;65:43–50. \n15 Chen Z, Gibson J, Chiu MC, et al. Automated empathy detection for oncology encounters. \nIn2020 IEEE International Conference on Healthcare Informatics (ICHI) 2020 Nov 30 (pp. \n1-8). IEEE. \n16 Butow P, Hoque E. Using artificial intelligence to analyse and teach communication in \nhealthcare. The breast. 2020 Apr 1;50:49–55. \n17 Tierney AA, Gayre G, Hoberman B, et al. Ambient artificial intelligence scribes to alleviate \nthe burden of clinical documentation. NEJM Catalyst Innovations in Care Delivery. 2024 \nFeb 21;5(3):CAT–23. \n18 Luo J, Zhang W, Yuan Y, et al Large language model agent: A survey on methodology, \napplications and challenges. arXiv preprint arXiv:2503.21460. 2025 Mar 27. \n19 Li J, Zhang Q, Yu Y, Fu Q, Ye D. More agents is all you need. arXiv preprint \narXiv:2402.05120. 2024 Feb 3. \n20 Topsakal O, Akinci TC. Creating large language model applications utilizing langchain: A \nprimer on developing llm apps fast. InInternational conference on applied engineering and \nnatural sciences 2023 Jul 10 (Vol. 1, No. 1, pp. 1050–1056). \n21 Wang H, Yang R, Alwakeel M, et al. An evaluation framework for ambient digital scribing \ntools in clinical applications. npj Digital Medicine. 2025 Jun 13;8(1):358. \n22 Stortenbeker I, Salm L, Olde Hartman T, Stommel W, Das E, van Dulmen S Coding \nlinguistic elements in clinical interactions: a step-by-step guide for analyzing \ncommunication form. BMC medical research methodology. 2022 Jul 11;22(1):191. \n23 GitHub [Internet]. [cited 2025 Aug 31]. GitHub - langchain-ai/langgraph: Build resilient \nlanguage agents as graphs. Available from: https://github.com/langchain-ai/langgraph \n24 Lewis P, Perez E, Piktus A, et al. Retrieval-augmented generation for knowledge-intensive \nnlp tasks. Advances in neural information processing systems. 2020;33:9459–74. \n25 Azure OpenAI in Azure AI Foundry Models - Azure OpenAI [Internet]. [cited 2025 Aug \n31]. Available from: https://learn.microsoft.com/en-us/azure/ai-\nfoundry/openai/concepts/models \n26 Douze M, Guzhva A, Deng C, et al. The faiss library. IEEE Transactions on Big Data. 2025 \nOct 13. \n"}, {"page": 22, "text": " \n22 \n27 Wang W, Wei F, Dong L, Bao H, Yang N, Zhou M. Minilm: Deep self-attention distillation \nfor task-agnostic compression of pre-trained transformers. Advances in neural information \nprocessing systems. 2020;33:5776–88. \n28 Carbonell J, Goldstein J. The use of MMR, diversity-based reranking for reordering \ndocuments and producing summaries. InProceedings of the 21st annual international ACM \nSIGIR conference on Research and development in information retrieval 1998 Aug 1 (pp. \n335–336). \n29 Jin Q, Kim W, Chen Q, et al. Medcpt: Contrastive pre-trained transformers with large-scale \npubmed search logs for zero-shot biomedical information retrieval. Bioinformatics. 2023 \nNov 1;39(11):btad651. \n30. Imel ZE, Pace BT, Soma CS, et al. Design feasibility of an automated, machine-learning \nbased feedback system for motivational interviewing. Psychotherapy. 2019 Jun;56(2):318. \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 23, "text": " \n23 \nTABLES \nTable 1. Summary characteristics patient-provider transcripts  \na. Training dataset \nCharacteristic \nValue \nTotal transcripts \n26  \nClinical domains \nRheumatology (n = 14); Obstetrics/Gynecology (n = 12) \nTotal speech time \n~ 8 hours \nAverage transcript length \nApproximately 18·5 minutes (SD ~ 4min) \nSentences per transcript \nMean = 285·62 (SD = 116·76; range 129 - 517); \nMedian = 272 (IQR 194·25 - 337·5) \nWords per transcript \nMean = 2558·5 (SD = 1045·00; range 943 - 4783); \nMedian = 2381 (IQR 1726·75 - 3180·25) \nSpeaker turns per transcript \nMean = 151·15 (SD = 70·77; range 76 - 330);  \nMedian = 141 (IQR 97 - 194·75) \nGold-standard annotated \ntranscripts \n26 \nCodebooks applied \nWISER, Global, Intervention (5As), Patient Behavior, Bias \nTotal annotated turns with labels \n(from human annotation) \n271 speaker turns \nAnnotated turns with labels per \ntranscript (from human \nannotation) \nMean = 10·54 (SD = 7·89; range 1 - 31);  \nMedian = 8·50 (IQR 6 - 13·25)  \n  \nTotal turns annotated as None \n(from human annotation) \n5819 speaker turns \n"}, {"page": 24, "text": " \n24 \nTurns annotated as None per \ntranscript (from human \nannotation) \nMean = 223·81 (SD = 101·23; range 70 - 442);  \nMedian = 218·5 (IQR 152·75 - 288·5)  \n \n \nb. Testing dataset \nCharacteristic \nValue \nTotal transcripts \n50 \nClinical domains \nRheumatology (n = 33); Obstetrics/Gynecology (n = 17) \nTotal speech time \n~16·5 hours \nAverage transcript length \nApproximately 20 minutes (SD ~4min) \nSentences per transcript \nMean = 322·36 (SD = 167·51; range 67 - 809); \nMedian = 313 (IQR 183 - 412·25) \nWords per transcript \nMean = 2871·88 (SD = 1445·57; range 602 - 5525); \nMedian = 2913 (IQR 1518·25 - 3635·75) \nSpeaker turns per transcript \nMean = 177·48 (range 39 - 376);  \nMedian = 145 (IQR 98·25 - 237) \nGold-standard annotated \ntranscripts \n50 \nCodebooks applied \nWISER, Global, Intervention (5As), Patient Behavior, Bias \nTotal annotated turns with labels \n(from human annotation) \n504 speaker turns \nAnnotated turns with labels per \ntranscript (from human \nannotation) \nMean = 10·08 (SD = 6·84; range 1 - 31);  \nMedian = 8 (IQR 5·25 - 12·75)  \n \n"}, {"page": 25, "text": " \n25 \nTotal turns annotated as None \n(from human annotation) \n14683 speaker turns \nTurns annotated as None per \ntranscript (from human \nannotation) \nMean = 293·66 (SD = 167·96; range 38 - 588);  \nMedian = 314·5 (IQR 124·75 - 443·75)  \n \n \n \n \n"}, {"page": 26, "text": " \n26 \nTable 2. Representative examples of single-label analysis \nCase Type \nCodebook Excerpt (Context) \nTarget Sentence \nHuman \nAnnotation \nAgent \nAnnotation \n(MOSAIC) \nNotes \nSuccessful \nCase 1 \nWISER \nClinician: “Do you \nhave a boy or girl at \nhome?” Patient: \n“Girl.” Clinician: “A \ngirl.” \n“A girl.” \n— (missed) \nReflective \nStatement \nMOSAIC \ncorrectly detected \na Reflective  \nStatement missed \nby the human \ncoder. \nSuccessful \nCase 2 \nWISER \nPatient: “Somebody \nI wanted to tell I was \npregnant died before \nI could tell them.” \nClinician: “Oh, I’m \nsure they know… \nthey’re watching \ndown on you.” \n“Oh, I’m sure \nthey know… \nthey’re \nwatching down \non you.” \n— (missed) \nReflective \nStatement \nCorrectly \nidentified \nEmpathic \nStatement missed \nby human. \nSuccessful \nCase 3 \nInterventi\non (5As) \nClinician: “Are you \ncurrently smoking \nany tobacco \nproducts?” Patient: \n“I’ve been doing \npatches off and on \nand still smoke, but \nmuch less now.” \n“Are you \ncurrently \nsmoking any \ntobacco \nproducts?” \nASK-\nSTART \nASK-START \nPerfect alignment; \ncorrect \nidentification of \nAsk phase \ninitiation. \nSuccessful \nCase 4 \nPatient \nBehavior \nPatient: “I struggle \nwith headaches… I \nthink some of it is \nfrom the plaquenil.” \n“I struggle with \nheadaches… I \nthink some of it \nis from the \nplaquenil.” \nAffective \nResponse \nAffective \nResponse \nPerfect \nagreement; \ncorrectly captured \nAffective \nResponse. \nChallenging \nCase 1 \n(Needs \nVoice) \nGlobal \nClinician: “OK, \nsometimes you can \ntry some vitamins… \nmagnesium, \nmelatonin, \nriboflavin.” \n“OK, \nsometimes you \ncan try some \nvitamins…” \nConcerns: 4 Attentive: 4 \nBoth labels \nplausible; \ndistinction \ndepends on tone \nand emphasis, \nhighlighting the \nneed for audio \ninput. \n"}, {"page": 27, "text": " \n27 \nChallenging \nCase 2 \n(Needs \nVoice) \nBias \nClinician: “So, a \ndiagnostic test is \nwhere you go into \nwhere the baby is \nand you’re getting \nchromosomes… \nthere’s a small \nchance of \nmiscarriage.” \n“So, a \ndiagnostic test \nis where you go \ninto where the \nbaby is…” \nRushed: 4 \n— (missed) \nSpeech tempo and \nintonation affect \njudgment; \nrequires audio for \naccurate detection. \nChallenging \nCase 3 \n(Needs \nVoice) \nBias \nClinician: “Do you \nthink he would be \nwilling to do that?” \nPatient: “Not really. \nNo, I just have my \nfiancé…” \n“Not really. No, \nI just have my \nfiancé…” \nGuarded–\nOpen: 4 \n— (missed) \nRequires prosodic \ncues (hesitation, \ntone) to \ndifferentiate \nguardedness vs \nopenness. \n \n \n \n"}, {"page": 28, "text": " \n28 \nFIGURE LEGENDS  \nFigure 1. Overview of the MOSAIC workflow. Input data include user-provided dialogue \ntranscripts and versioned clinician codebooks. Transcripts are preprocessed into structured, \ntimestamp-aligned segments, then passed to the Agentic Core. The Plan Agent coordinates task \nrouting and triggers the Update Agent when codebooks change. The Annotation Agent uses \nretrieval-augmented generation (RAG) to assign communication codes based on the codebooks. \nThe Verification Agent compares outputs to gold-standard labels and provides feedback, which \ndrives a feedback optimization loop. A dynamic Example Library RAG connects the Verification \nand Annotation Agents by incorporating adjudicated examples to iteratively refine few-shot \nprompts and improve annotation accuracy.\n \n"}, {"page": 29, "text": " \n29 \nFigure 2. Annotation performance across codebooks and clinical domains. (a) Overall and \nCodebook-Specific Annotation Performance. Bar chart showing overall and codebook-specific \naccuracy, precision, recall and F1 scores for five codebooks (WISER, Global, Intervention, \nPatient Behaviors, and Bias). (b) Category-Specific Annotation Performance. Distribution of \nMOSAIC’s annotation performance across rheumatology and obstetrics/gynecology. \n \n \n \n \n \n \n \n"}, {"page": 30, "text": " \n30 \nFigure 3. Performance Comparison of MOSAIC With Baselines and Ablation Variants. \n Grouped bar chart showing F1 scores for different system configurations, including the single-\nagent baseline (codebook RAG only, static instructions, no example-library retrieval, no \nverification, no dynamic prompting), multi-agent with codebook RAG alone, few-shot examples, \ndynamic few-shot prompting and complete MOSAIC system. \n \n \n \n \n \n \n \n"}, {"page": 31, "text": " \n31 \nSupplementary Materials  \nSupplement to: An Agentic AI System for Multi-Framework Communication Coding  \n \nTable of contents \nContent                                                                                                                                                                      Page \nSupplementary eMethods                                                                                                                33                             \neMethod 1. Audio Data Transforming and Internal ADS System                                                                                   33 \neMethod 2. Details of The Proposed Multi-Agent Framework (MOSAIC)                                                               34 \neMethod 3. User Interface                                                                                                                  38 \neMethod 4. The Annotation Performance Calculation of MOSAIC                                                                             39 \nAdditional Results                                                                                                         40                             \neResult 1. Qualitative Evaluation of The Plan Agent                                                                                                   40 \neResult 2. Qualitative Analysis on Labelling Details                                                                                                     41  \neResult 3. Qualitative Analysis on Number of Sentences, Words, and Speaker Turns                                                    42 \nSupplementary eTables                                                                                                                                                43 \neTable 1. Codebook and representative examples                                                                                                                  43 \neTable 2. Codebook-specific characteristics of patient-provider transcripts.                                                                                      44 \neTable 3. Representative prompt examples and results of qualitative evaluation of the Plan Agent                                         45   \neTable 4. Overall, category-specific and codebook-specific performance of MOSAIC                                                                47 \neTable 5. Qualitative analysis on labelling details                                                                                                                     48 \neTable 6. Results of OLS regression results for F1 score by department and total number of sentences                    50 \neTable 7. Results of OLS regression results for F1 score by department and total number of words                              52 \neTable 8. Results of OLS regression results for F1 score by department and total number of speaker turns                         54 \neTable 9. Summary across models                                                                                                                              56 \nSupplementary eFigures                                                                                                         57 \n"}, {"page": 32, "text": " \n32 \neFigure 1. Date preprocessing through internal ADS tool and chunking                                                                                            57 \neFigure 2. RAG techniques in the Annotation Agent                                                                                                  58 \neFigure 3. Example Library RAG techniques in Dynamic Prompting Strategy                                                   59 \neFigure 4. User interface                                                                                                                               60 \neFigure 5. The comparison of different regression analysis on F1 vs. text length                                                                       62 \nReferences                                                                                                                                                                                      63 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 33, "text": " \n33 \nSupplementary Methods \neMethod 1. Audio Data Transforming and Internal ADS System \nThe original version of our internally developed ADS system has been described previously.1 In \nthis study, we used the system as the foundation and further refined its components to enhance \nrobustness. The pipeline integrates voice activity detection (VAD), audio-to-text transcription, \nspeaker diarization, and LLM-based structured note generation. During VAD, clinician–patient \nspeech intervals were identified,2  using a combination of energy-based detection,3 Silero VAD,4 \nand SpeechBrain,5 with majority voting to extract accurate segments. Clinical audio was \ntranscribed with OpenAI Whisper-turbo.6 Speaker diarization relied on a voice-embedding and \nclustering approach via Pyannote,7 with detected segments aligned to Whisper transcriptions at \nsub-second resolution. Speaker role assignment (clinician vs. patient) was further processed \nbased on the reasoning ability of GPT-4o8 with the self-consistency prompting,9 where the model \nwas independently prompted 10 times using representative utterances for each speaker. The final \nspeaker roles would then be determined via majority voting across the 10 predictions. This \nprocess enabled the generation of structured transcripts that include timestamps (1 min per \nblock), speaker roles, utterances, and silence notations. \nAll processing of our internal system was conducted within either a secure institutional virtual \nmachine environment or local environment to ensure clinical data security and patient privacy. \nPersonally identifiable information was removed or masked, ensuring compliance with standard \nclinical data governance practices. \n \n \n"}, {"page": 34, "text": " \n34 \neMethod 2. Details of The Proposed Multi-Agent Framework (MOSAIC) \nThis system was implemented based on LangGraph.10 Each “Agent” corresponded to one or \nmore Nodes and routing was handled by typed Edges based on the evolving State.  \nLangGraph Primitives. The LangGraph orchestration proceeds through a structured sequence of \nmodular nodes that dynamically adapt to task state and error conditions. The process begins at \nthe Plan Node, which interprets the input transcript, identifies required annotation frameworks, \nand determines whether codebook updates or downstream verifications are necessary. If the \n“codebook update” flag is True, control is passed to the Update Node, which refreshes or \naugments rule sets and few-shot examples before routing execution to the Annotation Node. \nOtherwise, the pipeline proceeds directly from planning to annotation. The Annotation Node \nperforms the core workload, dispatching annotation subtasks across multiple agents or \nframeworks with bounded parallelism, aggregating results, and updating the shared state. If the \n“run verification” flag is True, outputs are then passed to the Verify Node, where automatic \nconsistency checks, metric evaluations, or rule-based audits are conducted. When verification is \nnot required, the process continues directly to completion. If any node raises an error or produces \nan “error message”, control is redirected to the Feedback Node, which logs diagnostic \ninformation, surfaces user-facing feedback, and optionally triggers corrective suggestions or re-\nruns. Finally, all execution paths converge at the End Node, ensuring deterministic closure of the \nworkflow and safe serialization of the final state. This design enables modular retries, \nconditional branching, and human-in-the-loop feedback while maintaining idempotent and fault-\ntolerant execution across all nodes. \nData Preprocessing. After transforming the raw audio data into structured transcripts using our \ninternal ADS tool, the processed transcript text can be used for downstream analysis. Once the \nuser uploads the transcript into the MOSAIC system, it will then perform chunking and \nsegmenting transcripts into manageable units (Supplementary eFigure 1). This step ensures that \nsubsequent annotation tasks operate on semantically coherent and temporally aligned segments.  \nPlan Agent. The Plan Agent serves as the central controller within the MOSAIC, responsible for \ningesting diverse input data sources, including audio transcripts, optionally updated or newly \n"}, {"page": 35, "text": " \n35 \nuploaded codebooks, and gold-standard transcripts containing reference labels for direct \nverification. Acting as the orchestration layer, the agent manages data preprocessing, routing, \nand task assignment for all downstream annotation nodes. During routing, the Plan Agent maps \neach transcript segment to one or more relevant coding standards using clinician-predefined \ncoding definitions and logic.  In the task assignment phase, the agent determines whether the \ncodebook has been updated; if an update is detected, control is routed to the Update Agent, \nwhereas unchanged inputs proceed directly to the Annotation Agent. It then populates the \n\"requested annotations\" state with the selected codebooks and delegates each subtask to a \ncorresponding Annotation Sub-Agent specialized in that domain. Finally, the Plan Agent \nimplements robust error-handling mechanisms to ensure workflow stability, validating input \ncompleteness, detecting malformed or inconsistent codebooks, and logging descriptive \ndiagnostic messages in the \"error\" state. Overall, the Plan Agent functions as an intelligent \ndispatcher, dynamically coordinating the flow of annotation tasks, optimizing data routing, and \nensuring transparent, fault-tolerant execution across the multi-agent pipeline. \nUpdate Agent. The Update Agent is responsible for detecting and processing newly uploaded or \nmodified codebooks within the MOSAIC workflow. Upon identifying an update, the agent \nautomatically refreshes the underlying codebook vector databases and retrieval modules to \nincorporate the revised coding definitions, examples, or rules, without requiring manual system \nreconfiguration. This dynamic update mechanism ensures that all subsequent annotations \nleverage the most current and contextually accurate codebook representations. Once the retrieval \ncontext has been successfully refreshed, the Update Agent triggers the Annotation Agent to \ninitiate annotation or continue downstream processing using the updated framework, thereby \nmaintaining consistency and adaptability across the multi-agent pipeline. \nAnnotation Agent (with Specialized Sub-Agents). For each segmented transcript chunk and each \nrequested coding framework, the Annotation Agent executes a RAG-prompted large language \nmodel (LLM) call that integrates retrieved and reranked contextual rules and few shot examples \nderived from our training results from the Verification Agent feedback (Supplementary eFigure \n2).  All the Specialized Sub-Agents are executed via Azure OpenAI GPT- 4o. The final prompt \nconcatenates retrieved rules, chunked transcript segments, few-shot examples, and the \ncorresponding label schema. Each sub-agent specializes in a distinct annotation framework based \n"}, {"page": 36, "text": " \n36 \non a domain specific clinician-provided codebook (Supplementary eTable 1). The WISER Sub-\nAgent identifies empathic opportunities expressed by patients and classifies clinician responses \nsuch as Empathic Responses, Empathic Statements, Sorry Statement, Open-ended Questions, \nReflective Statements, and Elicit Questions. The Patient Behavior Sub-Agent captures patient-\ninitiated behaviors, including question-asking and assertive responses that indicate agency or \npreference expression. The Global Sub-Agent automatically rates relational qualities, such as \nFlow, Respect, Warmth, Attentiveness, and Concern, on a 1–5 scale for each encounter or \ndialogue segment. The Intervention (5As) Sub-Agent detects discrete behavioral intervention \nsteps (Ask, Advise, Assess, Assist, Arrange). The Bias Sub-Agent identifies subtle and explicit \nbias cues, including Judgement, Stereotyping, Tailoring, Interrupting, Establishing Rapport, and \nMismatched Rapport, along with broader relational dynamics, and trust or distrust expressions. \nFinally, the SDOH and Weight Sub-Agent, which detects discussions of social determinants of \nhealth, such as financial stress, housing, food insecurity, and safety, as well as weight-related \ntopics and potential judgement language regarding their initiation or framing. This sub-agent was \nset up in our user interface but was not used during current model testing due to the small \namount of relevant data. In our future work we will include more of these data and continue to \ncontribute to this work.  \nCodebook Retrieval Augmented Generation (RAG) Module. The Codebook RAG module enables \ncontextual retrieval of domain-specific coding rules and definitions from clinician-provided \ncodebooks. Each codebook (e.g., WISER and Bias) is semantically chunked into rule-level \nsegments and embedded using all-MiniLM-L6-v211 to create a FAISS-based vector store.12 \nDuring annotation, relevant text segments from the transcript are used as queries to retrieve the \nmost semantically similar rules through a hybrid retrieval strategy combining Maximal Marginal \nRelevance (MMR) and MedCPT re-ranking.13 This ensures both diversity and clinical relevance \nin the retrieved content. The resulting set of retrieved rules and few-shot examples are then \ninjected into the final prompt to guide the Sub-Agent, allowing it to align its coding behavior \nwith clinician-authored definitions and ensure consistency across frameworks.  \nExample Library RAG Module for Dynamic Prompting Strategy. Example Library RAG Module \nfor Dynamic Prompting Strategy. The Example Library RAG provides a mechanism for dynamic \nprompt optimization based on past annotation outcomes (Supplementary eFigure 3). Verified \n"}, {"page": 37, "text": " \n37 \nexamples, derived from comparisons between gold-standard annotations and model-generated \noutputs, are aggregated and semantically chunked to form an evolving example library. These \nexamples are refreshed through the Verification Agent, which identifies mismatched sentences \nand corresponding contexts and filters representative examples for retraining or prompt \nrefinement. The refreshed examples are re-embedded and stored in a FAISS vector database \nusing hybrid sparse–dense retrieval to support continual improvement of few-shot prompting. \nThis iterative loop, linking Annotation, Verification, and Example Library RAG, enables the \nsystem to adaptively refine its behavior through human-in-the-loop feedback, enhancing both \nprecision and robustness in dynamic clinical coding tasks. \nVerification Agent. The Verification Agent serves as the system’s quality gatekeeper and \nExample Library updating manager, operating in two complementary modes. In training mode, it \ncompares Annotation Agent outputs with gold-standard labels to compute performance metrics, \nsuch as accuracy, precision, recall, and F1, and uses these as qualitative references to optimize \nfew shot examples in the Example Library. In new transcript mode , the agent performs an \nindependent verification pass, and flags low-confidence or potential inconsistent annotations for \nclinician review, ensuring continuous quality improvement across the entire pipeline. \n \n \n \n"}, {"page": 38, "text": " \n38 \neMethod 3. User Interface \nMOSAIC provides a web-based graphical interface designed to streamline annotation and \nverification workflows for clinical communication analysis. The Annotation Panel allows users \nto upload transcripts, select one or more Sub-Agents (e.g., WISER, Bias, Intervention, Global, \nPatient Behavior), and optionally provide custom codebooks to override default configurations. \nUploaded transcripts are automatically processed through the selected frameworks, and users can \nview, edit, or download the resulting annotated outputs (Supplementary eFigure 4a).  The \nVerification Panel supports model evaluation against gold-standard references by allowing \nupload of gold-labeled transcripts. Upon execution, the interface displays computed performance \nmetrics, including accuracy, precision, recall, and F1, as well as detailed mismatch reports at \nboth sentence and label levels. Users can preview the first 50 rows of comparison results directly \nin the browser or download comprehensive reports (Supplementary eFigure 4b). Together, these \nmodules provide an intuitive, transparent interface for managing annotation, evaluation, and \niterative model refinement across the MOSAIC system. \n \n \n"}, {"page": 39, "text": " \n39 \neMethod 4. The Annotation Performance Calculation of MOSAIC \nAt transcript level, Accuracy, precision, recall, and F1 were computed as weighted averages, \nwhere each label’s contribution was proportional to its frequency in the gold-standard \nannotations relative to the total number of annotated instances. Specifically, for a set of labels , \nwith  denoting the number of gold-standard instances of label l and 𝑁= ∑!∈#𝑛!  , the weighted \nmetric is defined as: 𝑊𝑒𝑖𝑔ℎ𝑡𝑒𝑑 𝑀𝑒𝑡𝑟𝑖𝑐 = ∑!∈# \n$!\n%  × 𝑀!, where 𝑀! denotes the per-label \nprecision, recall, or F1. Accuracy was defined as the proportion of correctly classified instances \nover the total number of instances. At the category level (e.g., rheumatology vs. \nobstetrics/gynecology), transcript-level metrics were averaged within each category, and then \ncompared across categories. At the codebook level, per-label metrics were averaged across all \ntranscripts belonging to the same codebook, providing a view of performance on specific \nbehavioral constructs. It is also worth noting that accuracy and weighted recall are often \nnumerically very similar, this occurs because weighted recall essentially measures the proportion \nof correctly identified instances across all labels, which is mathematically close to overall \naccuracy, especially when one label (e.g., “None”) dominates the dataset. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 40, "text": " \n40 \nAdditional Results \neResult 1. Qualitative Evaluation of The Plan Agent \nTo evaluate the behavior of the Plan Agent, we conducted a qualitative analysis of its ability to \nactivate the appropriate annotation sub-agent and to raise warnings when misrouting occurred. \nWe examined transcripts where the Plan Agent flagged text segments and compared them against \nunflagged portions to understand the conditions under which its routing decisions were accurate \nor problematic. Representative prompts were used to illustrate performance across a range of \nconditions. In positive cases, the Plan Agent reliably mapped specialized categories (e.g., Wiser, \nPatient Behavior, Intervention, Bias, Global) to the correct sub-agents, even when prompts \ndiffered in wording, case, or synonyms. This shows robustness to expression variants and \nconsistency with clear instructions. In negative cases, however, the agent struggled with \nambiguous or underspecified prompts, such as “tone,” “sentiment,” “communication style,” or \n“conversation flow”, which were misrouted to inappropriate agents (e.g., confused among Global, \nWiser and Patient Behavior) or flagged as invalid with no routing. Additionally, nonsense inputs \n(“asdfghjkl,” “Test123”) or negations (“run none”) triggered useful warnings. These patterns \nhighlight the Plan Agent’s strengths in handling explicit, well-structured prompts, noisy inputs, \nsynonyms, and requests involving multiple agents, where it generally produced consistent and \naccurate routing. However, its performance declined when the target (e.g. clinician and patient) of \nthe instruction was vague or underspecified, leading to fewer correct matches or, in some cases, \nover-assignment of extra sub-agents compared to the intended standard. This underscores the \nneed for improved disambiguation, synonym mapping, and fallback strategies in future iterations \n(Supplementary eTable 3). \n \n \n"}, {"page": 41, "text": " \n41 \neResult 2. Qualitative Analysis on Labelling Details \nThe aggregated results indicate that the system performs strongly on high-frequency categories, \nparticularly the “None” class, which constitutes most instances and achieves high accuracy \n(0·95), precision (0·98), recall (0·98), and F1-score (0·98). Major behavioral categories such as \n“S” and “ASK START” also maintain excellent accuracy (≈0·99), though their F1-scores remain \nmoderate (0·5–0·6) due to imbalances between precision and recall. In contrast, medium-\nfrequency categories such as “OE” and “AQ” exhibit lower precision (≤0·45) and F1-scores \n(<0·4), reflecting more frequent misclassifications. For rare categories with only a few positive \ninstances—particularly those requiring acoustic or paralinguistic cues—both precision and recall \nshow substantial variability, often leading to unstable or near-zero F1-scores. Overall, the system \ndemonstrates robust performance on dominant classes but limited generalizability to minority or \ncontext-dependent labels, underscoring the need for additional annotated data and the integration \nof audio-based features to improve model reliability and coverage (Supplementary eTable 5). \nThe comparison between human and agent annotations further illustrates both alignment and \ndivergence. In simple statements such as “Clinician: A girl,” the agent correctly identified the \nsegment as “RS” while the human annotator missed it; similarly, in “Clinician: Oh, I’m sure \nthey know,” the agent marked “ES” but the human again overlooked the label. Such \nstraightforward cases suggest that humans are more prone to missing or inconsistently applying \nrepetitive labels, whereas the agent maintains higher consistency. However, in more complex \nutterances like “OK, I’m glad you got the flu shot though…”, the human annotator assigned both \n“RS” (implicit thought) and “ES” (explicit emotion), while the agent captured only “RS”, \nreflecting reduced sensitivity to overlapping affective cues. In other examples, the human \nflagged “Warmth” or “Concerns” (e.g., “So she’s doing it for everybody else too, right?” \n[Warmth, 4]) that the agent either omitted or misaligned. Similarly, stylistic or conversational \nflow markers such as “Good. I assume no big gushes of water” were annotated by humans as \n“Flow: 2” but not consistently captured by the agent. This type of annotation depends heavily on \ncontextual, prosodic, and tonal features. Collectively, these findings highlight that while the \nagent performs reliably on explicit, structured, or repetitive communication behaviors, it \ncontinues to face challenges in detecting nuanced emotional tones and relational subtleties that \nrely on multimodal contextual cues. \n \n"}, {"page": 42, "text": " \n42 \neResult 3. Qualitative Analysis on Number of Sentences, Words, and Speaker Turns  \nAcross transcripts, F1 performance showed a moderate positive association with transcript length \nmetrics, including the number of sentences, words, and speaker turns per transcript \n(Supplementary eFigure 5). Pearson correlation coefficients ranged from r = 0·41 to 0·50, \nsuggesting that longer or more interactive transcripts tended to yield slightly higher F1 scores. \nConsistent with these trends, the OLS models demonstrated that category (rheumatology vs \nobstetrics/gynecology) was a significant predictor of F1 (p < 0·01), while transcript length \nvariables—whether measured by words, sentences, or speaker turns—did not reach statistical \nsignificance (p > 0·3). Despite the small effect sizes, all three metrics exhibited weak but \npositive slopes, indicating that model performance may improve marginally with richer dialogue \ncontext or longer utterance content (Supplementary eTables 6-8). Taken together, these results \nsuggest that domain differences primarily drive performance variation, whereas transcript length \nexerts only a minor influence once domain is controlled (Supplementary eTable 9). Since our \ncurrent transcripts predominantly fall within the range of approximately 150-500 sentences, 1 \n000-5 000 words, and 50-300 speaker turns, future work will incorporate additional real-world \ndata with broader coverage to further validate and enhance the robustness of these findings. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 43, "text": " \n43 \nSupplementary Tables \neTable 1. Codebook and representative examples \n \nCodebook \nMain \nFocus \nLevel of \nDetail \nUnique Twist \nExample \nGlobal  \nOverall \nrelational \nquality \nSummary \nscores (1–\n5) \nGood for simple \nbenchmarking \nFlow: [Flow: 4] — “Conversation flowed well, few \ninterruptions.” \nRespect: [Respect: 5] — “Clinician consistently asked \npermission before exam.” \nIntervention \n(5A’s) \nSpecific \nbehavior \nchanges \nsteps \nDetailed \nevent \ncoding \nTracks \nguideline \nfidelity (e.g., \nsmoking) \nAsked: [ASK] — “Do you currently smoke?” \nAssessed: [ASSESS] — “Would you like to quit in the \nnext month?” \nAssisted: [ASSIST w/ Sol]— “What barriers might get \nin the way of quitting?” \nWISER  \nEmpathy,  \nCaptures \nempathy \n \nOpen-ended Question: [OE] — “How’s your \nweekend?” \nEmpathic Opportunity: [EO] — “I’m really worried \nabout paying for this medication.” \nReflective Statement: [RS] — “So you’re feeling \nunsure about the plan.” \nPatient \nBehaviors \nPatient \nagency \nCaptures \npatient \nbehaviors \nTracks how \npatients push \nback or engage \nPatient Asking: [AQ] — Patient: “Is this safe for the \nbaby?” \nPatient Assertive: [AR] — “I don’t want to take that \nmedication right now.” \nBias \nTrust, \ndistrust, \nexplicit \nbias \nCombines \nbehavior \ncounts & \nglobal \nimpression \nCaptures subtle \npower dynamics \n& explicit \nstereotyping \nJudgement: [J] — “Oh no. You’re not still eating all \nthat fast food, are you?” \nStereotyping: [S] — “Is the baby daddy still not \nhelping out?” \nGuarded-Open: [GO: 2] — “Patient sounded reserved, \nhesitant.” \nTrust: [TP] — “So, you think I should keep taking \nthis?” \nDistrust: [D] — “But I really think we should do \nanother test.” \n \n \n \n \n"}, {"page": 44, "text": " \n44 \neTable 2.  Codebook-specific characteristics of patient-provider transcripts \n \na.  Training dataset \n \nCodeboo\nk \nNumber \nof \ntranscrip\nts \nSentences \npre \ntranscript \nWords per \ntranscript \nSpeaker \nturns per \ntranscript \nTotal \nannotated \nturns with \nlabels \nTotal turns \nannotated as \nNone  \nWiser \n6 \nMean = \n347·8 (SD \n= 92·5)  \nMean = \n3088·2 (SD \n= 1067·6)  \nMean = \n215·2 (SD = \n88)  \n103 \n1619 \nPatient \nBehavior \n6 \nMean = \n291·5 (SD \n= 126·8)  \nMean = \n2748·8 (SD \n= 1212·5)  \nMean = \n131·8 (SD = \n37·1)  \n57 \n1632 \nBias \n7 \nMean = \n276·3 (SD \n= 102·9)  \nMean = \n2384·4 (SD \n= 654·8)  \nMean = \n134·7 (SD = \n55·6)  \n48 \n1665 \nInterventi\non (5As)  \n2 \nMean = \n149 (SD = \n28·3)  \nMean = \n1161 (SD = \n304·1) \nMean = 87·5 \n(SD = 3·5) \n37 \n761 \nGlobal \n5 \nMean = \n271·6 (SD \n= 150·8)  \nMean = \n2497·4 (SD \n= 1177·8)  \nMean = \n156·4 (SD = \n77·8)  \n51 \n737 \n \nb. Testing dataset \nCodebook \nNumber \nof \ntranscri\npts \nSentences \npre \ntranscript \nWords per \ntranscript \nSpeaker \nturns per \ntranscript \nTotal \nannotated \nturns with \nlabels \nTotal turns \nannotated as \nNone  \nWiser \n19 \nMean = \n346·4 (SD \n= 182·4)  \nMean = \n3169·3 (SD \n= 1583·3)  \nMean = \n188·8 (SD = \n103·8)  \n226 \n6177 \nPatient \nBehavior \n11 \nMean = \n425 (SD = \n164·2)  \nMean = \n3838·6 (SD \n= 1172·5)  \nMean = \n240·3 (SD = \n94·5)  \n144 \n4514 \nBias \n12 \nMean = \n270·4 (SD \n= 120·1)  \nMean = \n2310·9 (SD \n= 1072·3)  \nMean = \n146·6 (SD = \n60·7)  \n67 \n3105 \nInterventio\nn (5As) \n5 \nMean = \n196·4 (SD \n= 106·7)  \nMean = \n1553·2 (SD \n= 790·4)  \nMean = \n101·6 (SD = \n54·6)  \n44 \n529 \nGlobal \n3 \nMean = \n211·7 (SD \n= 115·6)  \nMean = \n1885·3 (SD \n= 714·9)  \nMean = \n125·7 (SD = \n77·5)  \n23 \n358 \n \n \n \n \n"}, {"page": 45, "text": " \n45 \neTable 3. Representative prompt examples and results of qualitative evaluation of the Plan Agent \nPrompt \nCase \nExpected Sub-\nAgent Routing \nSub-Agent \nRouting Decided \nby Update Agent \nVerdict \nError \nMessage \nRun Bias and WISER \nBasic \npositive \nBias, Wiser \nBias, Wiser \nPASS \n– \nPlease run Global \nBasic \npositive \nGlobal \nGlobal \nPASS \n– \nRun Patient Behavior \nand Intervention \nBasic \npositive \nIntervention, Patient \nBehavior \nIntervention, Patient \nBehavior \nPASS \n– \nRun all \nBasic \npositive \nBias, Global, \nIntervention, Patient \nBehavior, SDOH & \nweight, Wiser \nBias, Global, \nIntervention, Patient \nBehavior, SDOH & \nweight, Wiser \nPASS \n– \nrun bias & wiser \nVariant \nexpression \nBias, Wiser \nBias, Wiser \nPASS \n– \nRUN GLOBAL AND \nINTERVENTION \nVariant \nexpression \nGlobal, Intervention \nGlobal, Intervention \nPASS \n– \nAnnotate empathy and \nadvice \nVariant \nexpression \nWiser, Intervention \nWiser, Intervention \nPASS \n– \nEvaluate stigma and \nprejudice \nVariant \nexpression \nBias \nBias \nPASS \n– \nCheck SDOH factors \nand weight discussion \nVariant \nexpression \nSDOH & weight \nSDOH & weight \nPASS \n– \nOnly run Wiser \nSingle agent \nWiser \nWiser \nPASS \n– \nAnnotate just Bias \nSingle agent \nBias \nBias \nPASS \n– \nFocus on patient \nbehaviors \nSingle agent \nPatient Behavior \nPatient Behavior \nPASS \n– \nCheck obesity coding \nSingle agent \nSDOH & weight \nSDOH & weight \nPASS \n– \nRun Wiser, Global, and \nBias \nComplex \nprompt \nBias, Global, Wiser \nBias, Global, Wiser \nPASS \n– \nPlease run intervention, \nbias, and sdoh \nComplex \nprompt \nBias, Intervention, \nSDOH & weight \nBias, Intervention, \nSDOH & weight \nPASS \n– \nAnalyze communication \nstyle of the doctor \nAmbiguous \nWiser \nGlobal, Wiser \nFAIL \n– \nCheck tone and \nsentiment of patient \nresponse \nAmbiguous \nPatient Behavior \nWiser \nFAIL \n– \nDo empathy coding for \nthis transcript \nAmbiguous \nWiser \nWiser \nPASS \n– \nLook at overall dialogue \nquality \nAmbiguous \nGlobal \nGlobal \nPASS \n– \nJust check conversation \nlength \nNegative \n– \n– \nPASS \nNo valid \nannotation \nagents found \nEvaluate conversation \nflow \nAmbiguous \nGlobal \nGlobal \nPASS \n– \nCheck doctor \ndominance in dialogue \nAmbiguous \nBias \nBias \nPASS \n– \nIdentify unclear patient \nresponses \nAmbiguous \nPatient Behavior \n– \nFAIL \nNo valid \nannotation \nagents found \n"}, {"page": 46, "text": " \n46 \nReview if intervention \nwas successful \nAmbiguous \nIntervention \nIntervention \nPASS \n– \nasdfghjkl \nExtreme \n(nonsense) \n– \n– \nPASS \nNo valid \nannotation \nagents found \nPlease annotate with all \nmodules available \nExtreme \nGlobal, Wiser, \nIntervention, Bias, \nSDOH & weight, \nPatient Behavior \nGlobal, Wiser, \nIntervention, Bias, \nSDOH & weight, \nPatient Behavior \nPASS \n– \nCan you check social \ndeterminants, empathy, \nand overall coding? \nExtreme \nGlobal, Wiser, \nSDOH & weight \nGlobal, Wiser, \nSDOH & weight \nPASS \n– \nRUN EVERYTHING \nExtreme \nGlobal, Wiser, \nIntervention, Bias, \nSDOH & weight, \nPatient Behavior \nGlobal, Wiser, \nIntervention, Bias, \nSDOH & weight, \nPatient Behavior \nPASS \n– \nPlease run none of the \nagents \nNegative \n– \n– \nPASS \nNo valid \nannotation \nagents found \nEvaluate ALL aspects of \npatient-doctor \ncommunication \nExtreme \nGlobal, Wiser, \nIntervention, Bias, \nSDOH & weight, \nPatient Behavior \nGlobal, Wiser, \nIntervention, Bias, \nSDOH & weight, \nPatient Behavior \nPASS \n– \nTest123 !!! ??? \nExtreme \n(nonsense) \n– \n– \nPASS \nNo valid \nannotation \nagents found \n \n \n \n"}, {"page": 47, "text": " \n47 \neTable 4. Overall, category-specific and codebook-specific performance of MOSAIC, regarding \naccuracy, precision, recall and F1 \nCategory / Codebook \nAccuracy \nPrecision \nRecall \nF1  \nOverall (all transcripts) \n93·1% \n93·4% \n93·1% \n93·0% \nRheumatology (subset)  \n96·1% \n96·4% \n96·1% \n96·2% \nObstetrics/Gynecology (subset) \n87·3% \n87·6% \n87·3% \n86·8% \nBy Codebook \n \n \n \n \nWISER (clinician \nempathy/behaviors) \n94·4% \n93·8% \n94·4% \n94·0% \nGlobal (relational quality) \n79·0% \n91·4% \n79·0% \n83·1% \nIntervention (5As) \n85·6% \n87·7% \n85·6% \n86·3% \nPatient Behaviors \n95·0% \n95·5% \n95·0% \n95·1% \nBias \n95·9% \n93·8% \n95·9% \n94·8% \n \n \n \n \n \n \n \n \n"}, {"page": 48, "text": " \n48 \neTable 5.  Qualitative analysis on labelling details \na. Detailed performance of good cases \n \nLabel \nTP \nFP \nFN \nTN \nTota\nl \nAccuracy \nPrecisio\nn \nRecall \nF1 \nSupport \n(Number \nof gold \nlabelling) \nNone \n15363 \n364 \n394 \n165 \n1628\n6 \n0·953 \n0·977 \n0·975 \n0·976 \n15757 \nRushed, 3 \n1 \n0 \n1 \n152 \n154 \n0·994 \n1 \n0·5 \n0·667 \n2 \nASSIST \nw/ Explore \n1 \n1 \n0 \n143 \n145 \n0·993 \n0·5 \n1 \n0·667 \n1 \nS \n16 \n6 \n12 \n6262 \n6296 \n0·997 \n0·727 \n0·571 \n0·64 \n28 \nASK \nSTART \n2 \n1 \n3 \n567 \n573 \n0·993 \n0·667 \n0·4 \n0·5 \n5 \nTP \n2 \n5 \n0 \n1039 \n1046 \n0·995 \n0·286 \n1 \n0·444 \n2 \nOE \n28 \n34 \n60 \n6770 \n6892 \n0·986 \n0·452 \n0·318 \n0·373 \n88 \nASSIST \nw/ \nSolution \n2 \n8 \n0 \n326 \n336 \n0·976 \n0·2 \n1 \n0·333 \n2 \nASK END \n1 \n1 \n3 \n514 \n519 \n0·992 \n0·5 \n0·25 \n0·333 \n4 \nAQ \n23 \n97 \n22 \n6677 \n6819 \n0·983 \n0·192 \n0·511 \n0·279 \n45 \nEO \n8 \n46 \n14 \n5903 \n5971 \n0·99 \n0·148 \n0·364 \n0·211 \n22 \nASSIST \nEND \n1 \n0 \n8 \n564 \n573 \n0·986 \n1 \n0·111 \n0·2 \n9 \nEQ \n1 \n8 \n0 \n2316 \n2325 \n0·997 \n0·111 \n1 \n0·2 \n1 \nAR \n16 \n50 \n110 \n6143 \n6319 \n0·975 \n0·242 \n0·127 \n0·167 \n126 \nER \n6 \n54 \n14 \n7121 \n7195 \n0·991 \n0·1 \n0·3 \n0·15 \n20 \nRS \n6 \n22 \n51 \n7794 \n7873 \n0·991 \n0·214 \n0·105 \n0·141 \n57 \nES \n1 \n2 \n12 \n3700 \n3715 \n0·996 \n0·333 \n0·077 \n0·125 \n13 \n \n \nb. Detailed performance of bad cases.  \nMost of these poor performances were from Global and Bias coding, indicating the need for \naudio inclusion. \nLabel \nTP \nFP \nFN \nTN \nTotal \nAccur\nacy \nPreci\nsion \nRecal\nl \nF1 \nSupport \n(Number \nof gold \nlabelling) \nAttentive: 4 \n0 \n19 \n0 \n275 \n294 \n0·935 \n0 \n0 \n0 \n0 \nConcerns: 3 \n0 \n2 \n0 \n303 \n305 \n0·993 \n0 \n0 \n0 \n0 \n"}, {"page": 49, "text": " \n49 \nConcerns: 4 \n0 \n14 \n0 \n62 \n76 \n0·816 \n0 \n0 \n0 \n0 \nEstablishing Rapport \n0 \n4 \n0 \n910 \n914 \n0·996 \n0 \n0 \n0 \n0 \nRespect: 4 \n0 \n2 \n0 \n74 \n76 \n0·974 \n0 \n0 \n0 \n0 \nWarmth: 1 \n0 \n2 \n0 \n216 \n218 \n0·991 \n0 \n0 \n0 \n0 \nWarmth: 3 \n0 \n2 \n0 \n216 \n218 \n0·991 \n0 \n0 \n0 \n0 \nWarmth: 4 \n0 \n5 \n0 \n158 \n163 \n0·969 \n0 \n0 \n0 \n0 \n \n \n \n \n"}, {"page": 50, "text": " \n50 \neTable 6. Results of OLS regression results for F1 score by department and total number of \nsentences. F1~Category + Sentences per transcript \na. Model summary \nStatistic \nValue \nDependent Variable \nF1 \nModel Type \nOLS (Least Squares) \nR squared \n0·488 \nAdjusted R squared \n0·466 \nF statistic \n22·40 \nProb (F statistic) \n1·47e−07 \nNo.of Observations \n50 \nDf Model \n2 \nDf Residuals \n47 \nLog Likelihood \n83·135 \nAIC \n−160·3 \nBIC \n−154·5 \nCovariance Type \nNonrobust \nb. Coefficient estimates \n \n"}, {"page": 51, "text": " \n51 \nVariable \nCoef.  \n Std. Err. \n \nt \n P>|t| \n95% CI \n(Lower) \n95% CI \n(Upper) \nIntercept \n0·591 \n0·015 \n56·935 \n0·000 \n0·829 \n0·889 \nDepartment \n(Rheumatolo\ngy) \n 0·0866 \n 0·016 \n 5·422 \n0·000 \n0·054 \n0·119 \nLength \n(sentences \nper \ntranscript) \n4·317e−05  \n4·56e−05  \n 0·946 \n0·349 \n−4·86e−05 \n0·000 \nc. Diagnostic statistics \nTest/Metric \nStatistic \np-value  \nOmnibus \n44·456 \np = 0·000 \nJarque–Bera (JB) \n182·050 \np = 2·94e−40 \nSkew \n−2·314 \n— \nKurtosis \n11·122 \n— \nDurbin–Watson \n1·742 \n— \nCondition No. \n941 \n— \nNotes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \n \n \n \n"}, {"page": 52, "text": " \n52 \neTable 7. Results of OLS regression results for F1 score by department and total number of \nwords.  \na. Model summary \nStatistic \nValue \nDependent Variable \nF1 \nModel Type \nOLS (Least Squares) \nR-squared \n0·486 \nAdjusted R-squared \n0·464 \nF-statistic \n22·23 \nProb (F-statistic) \n1·61e-07 \nNo. of Observations \n50 \nDf Model \n2 \nDf Residuals \n47 \nLog-Likelihood \n83·042 \nAIC \n-166·1 \nBIC \n-154·3 \nCovariance Type \nNonrobust \nb. Coefficient estimates \nVariable \nCoef.  \n Std. Err. \n \nt \n P>|t| \n95% CI \n(Lower) \n95% CI \n(Upper) \n"}, {"page": 53, "text": " \n53 \nIntercept \n0·8610 \n0·015 \n57·000 \n0·000 \n0·830 \n0·975 \nDepartment \n(Rheumatolo\ngy) \n 0·0841 \n 0·018 \n 4·665 \n0·000 \n0·048 \n0·120 \nLength \n(sentences \nper \ntranscript) \n5·56e-06 \n5·97e-06 \n 0·847 \n0·401 \n-6·96e-06 \n1·71e-05 \nc. Diagnostic statistics \nTest / Metric \nStatistic \np-value \nOmnibus \n45·095 \n0·000 \nJarque–Bera (JB) \n194·803 \n5·00e-43 \nSkew \n-2·322 \n— \nKurtosis \n11·482 \n— \nDurbin–Watson \n1·726 \n— \nCondition \nNumber \n8·70e+03 \n— \nNotes: [1] Standard errors assume the covariance matrix of the errors is correctly specified. [2] \nThe condition number is large (8.70e+03), which may indicate strong multicollinearity or other \nnumerical problems \n \n \n \n"}, {"page": 54, "text": " \n54 \neTable 8. Results of OLS regression results for F1 score by department and total number of \nspeaker turns. F1~Category + Speaker turns per transcript \na. Model summary \n \nStatistic \nValue \nDependent Variable \nF1 \nModel Type \nOLS (Least Squares) \nR-squared \n0·488 \nAdjusted R-squared \n0·466 \nF-statistic \n22·43 \nProb (F-statistic) \n1·60e-07 \nNo. of Observations \n50 \nDf Model \n2 \nDf Residuals \n47 \nLog-Likelihood \n83·043 \nAIC \n-166.1 \nBIC \n-154·5 \nCovariance Type \nNonrobust \n \nb. Coefficient estimates \n \nVariable \nCoef.  \n Std. Err. \n \nt \n P>|t| \n95% CI \n(Lower) \n95% CI \n(Upper) \nIntercept \n0·8594 \n0·015 \n57·989 \n0·000 \n0·830 \n0·889 \nDepartment \n(Rheumatology) \n 0·0865 \n 0·016 \n 5·423 \n0·000 \n0·054 \n0·119 \n"}, {"page": 55, "text": " \n55 \nLength (sentences \nper transcript) \n7·67e-05 \n8·04e-05 \n0·954 \n0·345 \n-8·5e-05 \n0·000  \nc. Diagnostic statistics \n \n \nTest / Metric \nStatistic \np-value \nOmnibus \n43·902 \n0·000 \nJarque–Bera (JB) \n180·461 \n6·51e-40 \nSkew \n-2·271 \n— \nKurtosis \n11·124 \n— \nDurbin–Watson \n1·765 \n— \nCondition Number \n521·0 \n— \n \nNotes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \n \n \n \n \n \n \n \n"}, {"page": 56, "text": " \n56 \neTable 9. Summary across models \n \nLength \nMetric \nCoef \n(Length) \nT \n(Length) \n95%CI \nLow \n95%CI \nHigh \nR² \nAdj.R² \nN \nDepartment \nSentences per \ntranscript \n0·0000 \n0·3488 \n-0·0001 \n0·0001 \n0·4880 \n0·4662 \n50 \nobstetrics/gynecol\nogy \nWords per \ntranscript \n0·0000 \n0·4014 \n-0·0000 \n0·0000 \n0·4861 \n0·4642 \n50 \nobstetrics/gynecol\nogy \nSpeaker turns \nper transcript \n0·0001 \n0·3447 \n-0·0001 \n0·0002 \n0·4882 \n0·4664 \n50 \nobstetrics/gynecol\nogy \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 57, "text": " \n57 \n \nSupplementary Figures \n \neFigure 1. Date preprocessing through internal ADS tool and chunking \n \n \n"}, {"page": 58, "text": " \n58 \neFigure 2. RAG techniques in the Annotation Agent \n \n \n \n \n"}, {"page": 59, "text": " \n59 \neFigure 3. Example Library RAG techniques in Dynamic Prompting Strategy   \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 60, "text": " \n60 \neFigure 4. MOSAIC User Interface \na. The Annotation Panel \n \n \n \n \n \n \n \n"}, {"page": 61, "text": " \n61 \nb. The Verification Panel \n \n \n \n \n \n \n \n \n"}, {"page": 62, "text": " \n62 \n \neFigure 5. The comparison of different regression analysis on F1 vs. text length \n \n \n \n \n \n \n \n"}, {"page": 63, "text": " \n63 \nReferences \n \n1 \nWang H, Yang R, Alwakeel M, et al. An evaluation framework for ambient digital scribing \ntools in clinical applications. npj Digital Medicine. 2025 Jun 13;8(1):358. \n2 \nGraf S. Design of Scenario-specific Features for Voice Activity Detection and Evaluation for \nDifferent Speech Enhancement Applications. \n3 \nGitHub [Internet]. [cited 2025 Aug 31]. GitHub - jiaaro/pydub: Manipulate audio with a \nsimple and easy high level interface. Available from: https://github.com/jiaaro/pydub \n4 \nGitHub [Internet]. [cited 2025 Aug 31]. GitHub - snakers4/silero-vad: Silero VAD: pre-\ntrained enterprise-grade Voice Activity Detector. Available from: \nhttps://github.com/snakers4/silero-vad \n5 \nGitHub [Internet]. [cited 2025 Aug 31]. GitHub - speechbrain/speechbrain: A PyTorch-based \nSpeech Toolkit. Available from: https://github.com/speechbrain/speechbrain \n6 \nRadford A, Kim JW, Xu T, Brockman G, McLeavey C, Sutskever I. Robust speech \nrecognition via large-scale weak supervision. InInternational conference on machine learning \n2023 Jul 3 (pp. 28492–28518). PMLR. \n7 \nBredin H, Yin R, Coria JM, et al. Pyannote. audio: neural building blocks for speaker \ndiarization. InICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and \nSignal Processing (ICASSP) 2020 May 4 (pp. 7124–7128). IEEE. \n8 \nAchiam J, Adler S, Agarwal S, et al. Gpt-4 technical report. arXiv preprint \narXiv:2303.08774. 2023 Mar 15. \n9 \nWang X, Wei J, Schuurmans D, et al. Self-consistency improves chain of thought reasoning \nin language models. arXiv preprint arXiv:2203.11171. 2022 Mar 21. \n10 langgraph: Build resilient language agents as graphs [Internet]. Github; [cited 2025 Oct 21]. \nAvailable from: https://github.com/langchain-ai/langgraph \n11 sentence-transformers/all-MiniLM-L6-v2 · Hugging Face [Internet]. [cited 2025 Oct 21]. \nAvailable from: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 \n12 faiss: A library for efficient similarity search and clustering of dense vectors [Internet]. \nGithub; [cited 2025 Oct 21]. Available from: https://github.com/facebookresearch/faiss \n13 Jin Q, Kim W, Chen Q, et al. Medcpt: Contrastive pre-trained transformers with large-scale \npubmed search logs for zero-shot biomedical information retrieval. Bioinformatics. 2023 \nNov 1;39(11):btad651. \n"}]}