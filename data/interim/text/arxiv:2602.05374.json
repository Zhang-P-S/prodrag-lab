{"doc_id": "arxiv:2602.05374", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.05374.pdf", "meta": {"doc_id": "arxiv:2602.05374", "source": "arxiv", "arxiv_id": "2602.05374", "title": "Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks", "authors": ["Chaimae Abouzahir", "Congbo Ma", "Nizar Habash", "Farah E. Shamout"], "published": "2026-02-05T06:52:46Z", "updated": "2026-02-05T06:52:46Z", "summary": "In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.05374v1", "url_pdf": "https://arxiv.org/pdf/2602.05374.pdf", "meta_path": "data/raw/arxiv/meta/2602.05374.json", "sha256": "d9f5f144ce4cabf246ea93542e55da389099f5337fa39950704f4b244ece20e7", "status": "ok", "fetched_at": "2026-02-18T02:19:43.319295+00:00"}, "pages": [{"page": 1, "text": "Cross-Lingual Empirical Evaluation\nof Large Language Models for Arabic Medical Tasks\nChaimae Abouzahir, Congbo Ma, Nizar Habash, Farah E. Shamout\nNew York University Abu Dhabi\n{ca2627,cm7196,nh48,fs999}@nyu.edu\nAbstract\nIn recent years, Large Language Models\n(LLMs) have become widely used in medical\napplications, such as clinical decision support,\nmedical education, and medical question an-\nswering. Yet, these models are often English-\ncentric, limiting their robustness and reliabil-\nity for linguistically diverse communities. Re-\ncent work has highlighted discrepancies in per-\nformance in low-resource languages for vari-\nous medical tasks, but the underlying causes\nremain poorly understood. In this study, we\nconduct a cross-lingual empirical analysis of\nLLM performance on Arabic & English medi-\ncal question and answering. Our findings reveal\na persistent language-driven performance gap\nthat intensifies with increasing task complex-\nity. Tokenization analysis exposes structural\nfragmentation in Arabic medical text, while re-\nliability analysis suggests that model-reported\nconfidence and explanations exhibit limited cor-\nrelation with correctness. Together, these find-\nings underscore the need for language-aware\ndesign and evaluation strategies in LLMs for\nmedical tasks.\n1\nIntroduction\nLarge Language Models (LLMs) have shown re-\nmarkable performance on a wide range of medical\ntasks, including clinical question answering (Sing-\nhal et al., 2025), medical reasoning (Chen et al.,\n2025; Wu et al., 2025), and exam-style benchmarks\n(Pal et al., 2022), positioning them as as powerful\ncapabilities for advancing healthcare applications.\nHowever, these successes are largely demonstrated\nin English due to limited availability of diverse\nbenchmarks (Singh et al., 2025).\nAs LLMs move closer to real-world healthcare\ndeployment, their ability to function reliably across\nlanguages becomes a critical concern. Recent mul-\ntilingual evaluations consistently report substantial\nperformance drops when medical LLMs are eval-\nuated outside English, with Arabic as one of the\naffected languages (Alonso et al., 2024). However,\nreported results are typically limited to aggregate\nperformance scores (Daoud et al., 2025), providing\nlimited insight into the underlying causes of model\nunderperformance.\nDespite growing recognition of performance\ngaps between Arabic and English, existing explana-\ntions remain largely underexplored, often attribut-\ning failures to limited pretraining data or domain\nmismatch (Jin et al., 2024; Qiu et al., 2024). As\na result, it remains unclear whether poor Arabic\nperformance stems primarily from linguistic prop-\nerties of the language, insufficient medical domain\nadaptation, architectural design choices, or inter-\nactions between these factors. This hinders princi-\npled adaptation: without knowing which factors\ndominate model failure, it is difficult to design\neffective multilingual training strategies, align-\nment procedures, or evaluation protocols. To\nthis end, we present the first systematic study de-\nsigned to disentangle linguistic, domain-specific,\nand architectural contributors to LLM performance\non Arabic medical tasks. We make the following\ncontributions:\n• We design a cross-lingual diagnostic evalua-\ntion framework for general-purpose and med-\nical LLMs that enables controlled analysis\nacross languages, output formats, tokeniza-\ntion behavior, and reliability signals.\n• We\nconduct\nan\nempirical\nstudy\non\nMedAraBench, an Arabic medical question\nanswering dataset, and its English-translated\ncounterpart to isolate language effects while\ncontrolling for medical content.\n• Our findings show that Arabic performance\ndegradation is driven by interacting represen-\ntational, alignment, and evaluation factors\nrather than medical knowledge alone, with\ngaps amplifying under increased task com-\nplexity and free-form generation.\narXiv:2602.05374v1  [cs.CL]  5 Feb 2026\n"}, {"page": 2, "text": "2\nRelated Work\n2.1\nMedical LLMs and Evaluation\nBenchmarks\nLLMs have driven recent progress in clinical NLP,\nsupporting applications including decision support,\ndiagnostic assistance, and clinical text generation.\nTo assess medical reasoning capabilities, several\nevaluation benchmarks, primarily formulated as\nquestion-answering tasks based on medical exam-\ninations or curated clinical sources, have been in-\ntroduced (Zhang et al., 2018; Jin et al., 2019; Pal\net al., 2022). Benchmarks such as MedQA and\nPubMedQA (Jin et al., 2021, 2019) are now widely\nused to evaluate medical knowledge and reasoning\nin LLMs.\nGeneral-purpose LLMs have achieved strong\nperformance on English medical benchmarks. No-\ntably, GPT-4 exceeded the passing threshold on\nUSMLE-style questions in MedQA, achieving an\naccuracy of 86.1% (Nori et al., 2023a). This suc-\ncess motivated the development of medical-domain\nLLMs through domain-specific adaptation. Propri-\netary models such as Med-PaLM and Med-PaLM\n2 (Singhal et al., 2025), as well as GPT-4 Med-\nPrompt (Nori et al., 2023b), reported substantial\ngains, with GPT-4 MedPrompt surpassing 90% ac-\ncuracy on MedQA and achieving significant error\nreduction.\nHowever, the costs, opacity, and privacy con-\nstraints associated with proprietary systems have\nlimited their adoption in real-world clinical set-\ntings. In response, several open-access medical\nLLMs have been proposed, yet their performance\non established benchmarks remains limited. Un-\nlike proprietary models, BioMistral only achieves\n44.4% accuracy on MedQA, while MedAlpaca and\nPMC-LLaMA attain 35.4% and 27.6%, respec-\ntively (Labrak et al., 2024). These results highlight\na persistent performance gap between proprietary\nand open-source medical LLMs.\n2.2\nMultilingual Medical Benchmarks and\nCross-lingual Generalization\nDespite the widespread evaluation of LLMs on En-\nglish medical benchmarks, their reliability across\nlanguages remains limited. Prior work has shown\nthat both general-purpose and medical LLMs are\nprone to hallucinations (Xiong et al., 2024) and\nmay produce answers based on outdated clinical\nknowledge (Vladika et al., 2025). Moreover, most\nmedical benchmarks are predominantly English-\ncentric in both their construction and evaluation\n(Qiu et al., 2024).\nRecent multilingual evaluations consistently re-\nport substantial performance drops outside En-\nglish. For instance, significant degradation has\nbeen observed on Italian medical QA tasks (Kembu\net al., 2025), as well as across a broader range of\nnon-English healthcare queries (Jin et al., 2024).\nAlonso et al. (2024) further show that both general-\npurpose and medical LLMs perform markedly\nworse in Arabic and Hindi than in English. No-\ntably, medical LLMs often underperform the base\nmodels from which they are adapted in non-English\nsettings, suggesting that domain adaptation may re-\nduce cross-lingual generalization.\nAlonso et al. (2024) further show that medical\nLLMs often underperform their base models in\nnon-English settings such as Arabic and Hindi, sug-\ngesting that domain adaptation may hinder cross-\nlingual generalization. Complementarily, Jeong\net al. (2024) demonstrate that this effect already oc-\ncurs in English, indicating that specialization alone\ndoes not guarantee performance gains even without\nlanguage mismatch.\nTo mitigate these disparities, some efforts have\nfocused on developing language-specific medical\nmodels. HuatuoGPT (Zhang et al., 2023) is a no-\ntable example of a Chinese medical LLM trained\non native-language biomedical resources. However,\nsystematic analyses of how domain adaptation in-\nteracts with multilingual performance remain lim-\nited, particularly for underrepresented languages.\n2.3\nChallenges in Arabic Medical Language\nModels\nArabic poses distinct challenges for medical\nlanguage modeling, including rich morphology,\ncomplex tokenization, dialectal variation, and a\nscarcity of high-quality, domain-specific resources\n(Farghaly and Shaalan, 2009; Habash, 2010). Al-\nthough general-purpose Arabic LLMs such as Jais\n(Sengupta et al., 2023), Fanar (Team et al., 2025)\nand ALLAM (Bari et al., 2025) have been intro-\nduced, the development and evaluation of Arabic\nmedical LLMs remain underexplored.\nExisting evaluations report poor performance on\nArabic medical tasks (Daoud et al., 2025). How-\never, the underlying causes of these failures are\nnot well understood. It remains unclear whether\nperformance degradation primarily arises from lin-\nguistic representation issues, limitations of domain-\nadaptive training, or their interaction, particularly\n"}, {"page": 3, "text": "for medical LLMs adapted from English-centric\nbase models. Moreover, the lack of publicly avail-\nable Arabic medical benchmarks limits systematic\ndiagnostic analyses comparable to those in English\n(Alasmari et al., 2024), motivating our investigation\ninto the mechanisms underlying Arabic medical\nLLM failures beyond aggregate performance.\n3\nMethodology\nTo investigate sources of performance degradation\nin Arabic medical MCQs, we design a targeted\nevaluation framework probing LLM behavior on\nseveral aspects. Rather than introducing a new\nmodel, we focus on a set of research questions,\nwhich we detail below.\n3.1\nResearch Questions\nWe compare model accuracy on original Arabic\nquestions and their English-translated counterparts\nto isolate the role of linguistic representation from\nmedical reasoning.\nRQ1: To what extent is performance degrada-\ntion driven by language rather than medical\nreasoning? We compare model accuracy on origi-\nnal Arabic questions and their English-translated\ncounterparts to isolate the role of linguistic repre-\nsentation from medical reasoning.\nRQ2: How do question-level properties affect\nmodel performance? We analyze accuracy as a\nfunction of input length, question difficulty, and\nmedical specialty to determine whether linguistic\ncomplexity, cognitive demand, or domain-specific\ncontent disproportionately affects model outcomes.\nRQ3: How do alignment constraints and output\nformats influence model behavior across lan-\nguages? We compare soft matching (letter-based\noption selection) and hard matching (exact answer\ntext generation) to evaluate how instruction follow-\ning and surface-form generation influence accuracy\nacross languages.\nRQ4: Does tokenization behavior contribute to\nArabic performance gaps? We examine tokenizer\nefficiency and fragmentation patterns to understand\nwhether Arabic morphology and segmentation lead\nto less effective input representations.\nRQ5: Are model confidence estimates and gener-\nated explanations reliable indicators of correct-\nness? We analyze model-reported confidence and\naccompanying rationales to assess whether they\ncorrelate with accuracy and can be used to diag-\nnose systematic failure modes.\n3.2\nDataset\nAll experiments are conducted on MedAraBench\n(Abu-Daoud et al., 2026), an Arabic medical ques-\ntion answering benchmark.\nThe questions are\noriginally authored in Modern Standard Arabic\n(MSA), collected from medical exams, digitized\nfrom scanned paper sources, and manually curated\nto exclude incomplete or ambiguous items. Each\nquestion is annotated with the number of answer\noptions (4–6), a difficulty level corresponding to\nyears of medical study (Y1–Y5), and a medical\nspecialty. The dataset covers 19 specialties (e.g.,\nAnatomy, Pathology, Surgery, Pharmacology) and\nis split into training and test sets using an 80/20\nsplit with matched specialty distributions (19,894\ntrain / 4,989 test). A data sample is shown in Ap-\npendix A1.\nModels are evaluated on a medical MCQ task in\nboth Arabic and English. English versions are ob-\ntained via automatic translation of the original Ara-\nbic questions using the Google Translate API and\nare used solely for controlled cross-lingual analy-\nsis. Models are evaluated using accuracy, with a\nprediction counted as correct if the selected option\nmatches the gold label.\n3.3\nEvaluated Models\nWe evaluate several recent open-source large lan-\nguage models as baselines for Arabic medical\nMCQs. We include recent, large-scale general-\npurpose LLMs like DeepSeek-V3.2 and LLaMA\n3.3 70B as representative contemporary baselines\nin our evaluation. To examine differences between\ngeneral language ability and domain-specific mod-\neling, we compare two categories of models:\n• General-purpose LLMs: DeepSeek-V3.2\n(DeepSeek-AI et al., 2025), LLaMA 3.3 70B\n(Meta AI, 2024), and Mistral-Small-3.2-24B-\nInstruct-2506 (Mistral AI, 2025), all trained\non broad multilingual or mixed-domain cor-\npora.\n• Medical-domain LLMs: Meditron 3 70B\n(OpenMeditron Initiative, 2024), Med42-70B\n(Christophe et al., 2024), and MedGemma-\n27B-text-it (Sellergren et al., 2025), which in-\ncorporate domain-adaptive pretraining or fine-\ntuning on medical data. None explicitly report\nmultilingual medical pretraining, and avail-\nable documentation indicates predominantly\nEnglish medical data.\n"}, {"page": 4, "text": "Moreover, our evaluation focuses exclusively on\nopen-source models for both methodological and\npractical reasons. From a methodological stand-\npoint, open-source models offer full access to em-\nbeddings, tokenizers, and intermediate represen-\ntations, which are essential for both our analysis\nand our planned cross-lingual adaptation method.\nFor practical concerns, deploying black-box propri-\netary systems in medical settings poses significant\nprivacy and auditability concerns, underscoring the\nneed for transparent, open-source alternatives.\n3.4\nExperimental Settings\nAll models were evaluated using a unified multiple-\nchoice prompting setup implemented via the Hug-\ngingFace Transformers API. Inference used greedy\ndecoding (temperature = 0, no sampling, top-p =\n1.0, top-k disabled). We fix task-specific maxi-\nmum generation lengths across languages to ensure\ncomparable inference conditions across models, al-\nlowing up to 4 tokens for letter matching, 15 for\ntext matching, and 70 for explanation generation.\nThese limits were chosen to accommodate the re-\nquired output formats rather than language-specific\ntokenization characteristics.\nThe system prompts used follow standardized\nMCQ templates as shown in Appendix B and are\nprovided in English for all inputs, including Ara-\nbic inputs. This is based on preliminary prompt-\nengineering experiments showing more stable and\nhigher-performing outputs than Arabic prompts.\nThis choice reflects the English-centric instruction-\nfollowing capabilities of the evaluated models and\nresults in mixed-language inputs for Arabic evalua-\ntions.\nDue to hardware constraints, all 70B-parameter\nmodels were evaluated using 4-bit NF4 quantiza-\ntion with bfloat16 compute (BitsAndBytes) across\ntwo 32 GB V100 GPUs. Smaller models, includ-\ning Medgemma-27B-text-it and Mistral-Small-3.2-\n24B-Instruct-2506, were evaluated without quanti-\nzation in full bfloat16 precision on the same hard-\nware. DeepSeek-V3.2 could not be evaluated lo-\ncally due to its size and was instead accessed via the\nofficial DeepSeek API. For cross-lingual analysis,\nmodels were additionally evaluated on an English-\ntranslated version of the dataset. This setup enables\ndirect comparison between Arabic and English un-\nder identical task structures, isolating the effect of\nlanguage from content.\nModels\nAcc (Ar) Acc (En) ∆(En–Ar)\nGeneral-purpose LLMs\nDeepSeek-V3.2\n62.39\n62.85\n0.46\nLlama 3.3 70B\n42.10\n57.61\n15.51\nMistral-Small-3.2-24B\n50.25\n57.75\n7.5\nMedical-domain LLMs\nMeditron 3 70B\n50.51\n58.80\n8.92\nMed42-70B\n33.59\n53.21\n19.62\nmedgemma-27b-text-it\n49.22\n52.30\n3.08\nTable 1: Results of general-purpose and medical-\ndomain LLMs’ Acc(uracy) on the Arabic (Ar) and\nEnglish (En) datasets. Bold values indicate the highest\naccuracy within each column. Mistral-Small-3.2-24B\nrefers to Mistral-Small-3.2-24B-Instruct-250.\n4\nEmpirical Studies and Analyses\n4.1\nAssessing the Role of Language in LLM\nPerformance\nWe compare model accuracy on parallel English\nand Arabic medical benchmarks using a controlled\nprompting setup (Appendix B2) to isolate the effect\nof linguistic representation from medical reason-\ning. As shown in Table 1, accuracy is consistently\nlower in Arabic than in English across nearly all\nevaluated models, indicating a systematic language-\nassociated performance gap. DeepSeek-V3.2 is the\nonly model that exhibits comparable performance\nacross languages. Notably, this behavior is not\nobserved uniformly in larger models, indicating\nthat reduced cross-lingual degradation cannot be\nattributed to model size alone.\nFor models with comparable parameter sizes\n(≤70B), English consistently outperforms Arabic,\nindicating that language remains a key factor even\nunder similar capacity constraints. This trend holds\nfor both general-purpose and medical-domain mod-\nels, suggesting that domain specialization alone\ndoes not resolve Arabic performance gaps.\n4.2\nEffects of Question Length and Difficulty\nWe investigate whether question-level character-\nistics influence model accuracy by analyzing per-\nformance trends with respect to input length, edu-\ncational difficulty, and medical specialty. We fo-\ncus our analysis on the best- and worst-performing\nmodels overall, DeepSeek-V3.2 and Med42-70B,\nrespectively, and restrict the following experiments\nto these two models.\nFigures 1 (a,b) show accuracy trends as a\nfunction of question length for DeepSeek-V3.2\nand Med42-70B. Accuracy is relatively stable for\nshorter inputs but degrades as question length in-\n"}, {"page": 5, "text": "a)\nd)\nc)\nb)\nFigure 1: Effect of question length on accuracy across Arabic and English. (a–b) Rolling accuracy versus\nquestion length for DeepSeek-V3.2 and Med42-70B, respectively. (c) Distribution of question lengths in both\nlanguages. (d) Arabic–English length correspondence for aligned question pairs.\ncreases for Arabic, while English performance re-\nmains comparatively stable at longer lengths. Ques-\ntion lengths are strongly correlated across paired\nArabic–English items and exhibit overlapping dis-\ntributions (Figures 1 c,d), indicating that the ob-\nserved degradation reflects increased sensitivity to\ninput length rather than artifacts of translation or\nsystematic length mismatches.\nFigure 2 reports accuracy by educational diffi-\nculty level. For both models and languages, ac-\ncuracy decreases for later years’ questions (Y3+)\ncompared to early years’ questions (Y1–Y2). The\nperformance drop is consistently larger for Arabic,\nparticularly for Med42-70B.\nFigure 3 shows accuracy by medical specialty,\nrevealing substantial variation across domains: per-\nformance is higher in clinically oriented fields\n(e.g., Emergency Medicine, Internal Medicine) and\nlower in foundational or detail-intensive specialties\nsuch as Microbiology and Embryology. English\nconsistently outperforms Arabic across most spe-\ncialties. This gap is particularly pronounced for\nMed42-70B, where Arabic performance lags be-\nhind English across nearly all specialties, suggest-\ning that language-related performance disparities\nDeepSeek-V3.2\nMed42-70B\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nAccuracy by Difficulty Level (Early vs Later Years)\nEarly Years (Arabic)\nLater Years (Arabic)\nEarly Years (English)\nLater Years (English)\nFigure 2: Accuracy by educational difficulty level\n(early vs. later years) for DeepSeek-V3.2 and Med42-\n70B on Arabic and English medical MCQs.\npersist even when controlling for domain.\nOverall, these results indicate that input length,\ndifficulty, and domain content systematically affect\nmodel performance, and that these effects dispro-\nportionately impact Arabic compared to English.\n4.3\nAlignment Behavior Analysis\nTo analyze how output format influences model\nbehavior across languages, we evaluate model per-\nformance under free-form answer generation using\nthe prompt in Appendix B3. We use token-level\nsequence similarity between predicted and ground-\n"}, {"page": 6, "text": "Figure 3:\nAccuracy by medical specialty for\nDeepSeek-V3.2 (top) and Med42-70B (bottom) on\nArabic and English medical MCQs.\nDeepSeek-V3.2\nMed42-70B\n0\n20\n40\n60\n80\nToken-level sequence match (%)\n49.66\n55.81\n25.38\n36.30\nΔ +6.15 pp\nΔ +10.92 pp\nText-matching performance by language\nArabic\nEnglish\nFigure 4: Token-level sequence-match accuracy (%)\nfor text-matching evaluation in Arabic and English\nacross two models.\ntruth answer texts computed with the Sequence-\nMatcher algorithm (Munk and Feitelson, 2022).\nFigure 4 shows that across both models, surface-\nform similarity is consistently lower for Arabic\nthan for English, with a larger gap of 10.92 percent-\nage points for Med42-70B compared to 6.15 pp for\nDeepSeek-V3.2. These discrepancies indicate that,\nwhen models are required to generate answer text\nexplicitly, Arabic outputs diverge more substan-\ntially from reference answers at the surface-form\nlevel. The magnitude of these gaps is also larger\nthan that observed under letter-based option selec-\ntion reported in Table 1, suggesting that free-form\ngeneration amplifies language-specific difficulties\nbeyond those captured by standard MCQ accuracy.\nTokenizer\nTok/Word Char/Tok Single Char\nModel-native tokenizers\nDeepSeek-V3.2\n2.39\n2.33\n32%\nLlama 3.3 70B\n2.42\n2.27\n32%\nMeditron 3 70B\n2.45\n2.27\n32%\nMistral-Small-3.2-24B\n2.06\n2.72\n0%\nMed42-70B\n2.42\n2.27\n32%\nMultilingual-efficient tokenizer\nGemma-3-4B-it\n2.30\n2.43\n35%\nArabic-focused tokenizer\nCAMeLBERT-MSA\n1.76\n3.21\n36%\n(a) Arabic dataset.\nTokenizer\nTok/Word Char/Tok Single Char\nModel-native tokenizers\nDeepSeek-V3.2\n1.52\n4.01\n28%\nLlama 3.3 70B\n1.60\n3.82\n27%\nMeditron 3 70B\n1.60\n3.82\n27%\nMistral-Small-3.2-24B\n1.56\n3.93\n0%\nMed42-70B\n1.60\n3.82\n27%\nMultilingual-efficient tokenizer\nGemma-3-4B-it\n1.57\n3.90\n35%\nArabic-focused tokenizer\nCAMeLBERT-MSA\n2.82\n2.13\n44%\n(b) English dataset.\nTable 2: Tokenization fragmentation statistics for\nArabic and English inputs. We report average tokens\nper word (subword splitting), average characters per\ntoken (compactness), and single-character tokens (re-\nflecting extreme fragmentation).\n4.4\nTokenization Efficiency Analysis\nWe analyze tokenization efficiency and fragmenta-\ntion to assess whether language-specific tokeniza-\ntion patterns are associated with downstream per-\nformance gaps.\nWe report average tokens per\nword, characters per token, and the proportion of\nsingle-character tokens in Table 2. For Arabic,\nword counts are computed using a linguistically in-\nformed tokenizer from CAMeL Tools (Obeid et al.,\n2020), while English uses whitespace-based word\nsegmentation. Higher tokens per word and single-\ncharacter rates, together with lower characters per\ntoken, indicate more fragmented representations.\nHigher tokens per word and single-character rates,\ntogether with lower characters per token, indicate\nmore fragmented representations.\nAcross model-native tokenizers, Arabic is con-\nsistently more fragmented than English, with ap-\nproximately 2.4 tokens per word compared to\n1.5–1.6 for English. Similar trends hold for the\nmultilingual tokenizer. This increased fragmenta-\n"}, {"page": 7, "text": "Figure 5: Relationship between model-reported con-\nfidence and accuracy for Arabic (top) and English\n(bottom) multiple-choice medical MCQ.\ntion leads to higher token usage for Arabic inputs,\nwhich may plausibly contribute to sharper perfor-\nmance degradation as input length and complexity\nincrease. However, this effect is not uniform across\nmodels: despite higher token counts for Arabic,\nsome models (e.g., DeepSeek-V3.2) exhibit more\nstable performance, suggesting that tokenization\nalone does not fully explain the observed degrada-\ntion. In contrast, the Arabic-focused CAMeLBERT\ntokenizer (Inoue et al., 2021) substantially reduces\nfragmentation for Arabic while increasing fragmen-\ntation for English, illustrating that tokenizer effi-\nciency is language-dependent.\n4.5\nThe Role of Confidence Estimates and\nExplanations\nWe analyze the relationship between model-\nreported confidence (prompt in Appendix B4) and\naccuracy in medical question answering (Figure 5).\nWe observe a moderate negative correlation be-\ntween confidence and accuracy in both Arabic\n(r = −0.56) and English (r = −0.50), indicat-\ning that higher confidence predictions are, on av-\nerage, less accurate.\nThis pattern is consistent\nacross model families and languages, suggesting\na general miscalibration of confidence in medi-\ncal settings. Accordingly, model-reported confi-\nModel\nBaseAr BaseEn ExpAr ExpEn\nDeepSeek-V3.2\n62.39\n62.85\n63.56\n45.85\nLlama-3.3-70B\n42.10\n57.61\n46.27\n57.91\nMistral-Small-3.2-24B\n50.25\n57.75\n49.50\n57.99\nMeditron-3-70B\n51.05\n58.45\n28.65\n33.65\nMed42-70B\n33.59\n53.21\n28.99\n33.41\nTable 3: Accuracy with Explanation prompting (Exp)\ncompared to the baseline (Base, no explanation\nprompt), for Arabic and English. Bold values de-\nnote the highest accuracy for each column.\ndence should not be treated as a reliable proxy\nfor correctness.\nMedGemma-27B-text-it is ex-\ncluded due to repeated noncompliance with the re-\nquired confidence-reporting format under zero-shot\nprompting, often producing free-form text without\na valid answer label. As this reflects instruction-\nfollowing issues rather than task performance, we\nomit it from this analysis.\nTo examine whether explicit reasoning improves\nperformance, we prompt models to generate a nat-\nural language explanation before answer selection,\nfollowing a chain-of-thought–style prompting strat-\negy (Wei et al., 2022) using the prompt shown in\nAppendix B5. MedGemma-27B-text-it is excluded\nfor severe instruction non-compliance, consistent\nwith the confidence-based analysis. As shown in\nTable 3, explanation prompting yields mixed and\noften detrimental effects across models. While\nsome models exhibit modest gains in Arabic accu-\nracy (e.g., DeepSeek-V3.2: 62.39 →63.56), expla-\nnation prompting often leads to degraded perfor-\nmance in English and, for several models, substan-\ntial drops in both languages (e.g., Meditron-3-70B\nand Med42-70B).\nUpon qualitative inspection, we find that in many\ncases, the model produces medically plausible or\npartially correct explanations while selecting an\nincorrect option label. Representative examples il-\nlustrating these reasoning–label misalignments are\nprovided in Appendix C6. Requiring explanations\nappears to encourage verbose reasoning and rein-\nterpretation, which can decouple reasoning qual-\nity from discrete multiple-choice selection. While\nexplanation-conditioned prompting reduces accu-\nracy for some models, this setting also surfaces\ncases where letter-only evaluation may overesti-\nmate performance by rewarding option matching\ndespite stem–option inconsistencies. In such cases,\nexplanation prompting exposes reasoning–label\nmisalignment rather than pure knowledge errors.\n"}, {"page": 8, "text": "5\nDiscussion\n5.1\nLanguage as a Source of Degradation\nAcross nearly all evaluated models, performance on\nArabic medical MCQs is consistently lower than\non their English-translated counterparts, despite\nidentical medical content, indicating a persistent\nlanguage-related performance gap beyond medical\nknowledge alone. DeepSeek-V3.2 is a notable ex-\nception, achieving near-parity between Arabic and\nEnglish, demonstrating that strong cross-lingual\nperformance in Arabic medical QA is achievable\nin open models.\nThis robustness cannot be explained by model\nscale alone, as public estimates suggest that\nDeepSeek-V3.2 and LLaMA 3.3 70B are trained\nat comparable orders of magnitude in compute\nand training tokens (Epoch AI, 2024), yet only\nDeepSeek-V3.2 maintains high Arabic perfor-\nmance. This suggests that language robustness\ndepends on specific design and training choices\nbeyond scale, including data curation, language\nbalance, and post-training procedures.\n5.2\nInteraction between Task Complexity and\nLanguage\nPerformance across all evaluated models is system-\natically higher for shorter questions and for lower-\ndifficulty (early-year) items, regardless of language.\nHowever, the rate of performance degradation dif-\nfers substantially between Arabic and English. As\nquestion length increases and as questions progress\nfrom early to later years’ material, accuracy de-\nclines more sharply for Arabic than for English, as\nshown in Figures 1 (a-b) and 2.\nThe relatively strong performance on short and\nearly-year Arabic questions indicates that models\ncan successfully answer simpler medical queries\nin Arabic, meaning that basic medical knowledge\nis present. The decline observed for longer and\nmore advanced questions points to a reduced ro-\nbustness of Arabic representations as task complex-\nity increases, rather than a lack of medical under-\nstanding. A similar pattern emerges across medical\nspecialties in Figure 3, where the gap between Ara-\nbic and English is larger in specialties that involve\nfiner-grained distinctions, reinforcing the interac-\ntion between language effects and task complexity.\n5.3\nAlignment Constraints and Evaluation\nSensitivity\nThe alignment analysis shows that output format\ninfluences how language-specific performance dif-\nferences manifest. Because letter-based MCQ accu-\nracy and token-level text similarity measure distinct\naspects of model behavior, we restrict our analy-\nsis to within-format comparisons between Arabic\nand English. Under free-form answer generation,\nmodels consistently achieve lower token-level sim-\nilarity in Arabic than in English, yielding larger\ncross-lingual gaps than those observed with con-\nstrained option selection. This suggests that re-\nmoving output constraints introduces additional\nlanguage-dependent variability not reflected by\nletter-based evaluation. In particular, free-form gen-\neration places greater demands on lexical choice\nand morphological realization, which are more\nchallenging in Arabic. Since token-level similar-\nity measures surface-form overlap, lower scores\nprimarily reflect increased variation in answer ex-\npression rather than incorrect medical reasoning.\n5.4\nTokenization as a Structural Constraint\nCompared to English, Arabic medical text is con-\nsistently more fragmented under model-native tok-\nenizers, with words split into more subword units\nand a higher prevalence of single-character tokens.\nThis reflects a mismatch between subword tok-\nenizers, optimized for frequent training forms, and\nArabic medical vocabulary, which combines rich\nmorphology with low-frequency, variable domain-\nspecific terms, leading to unstable subword repre-\nsentations and finer-grained splits.\nMultilingual tokenizers partially mitigate this\neffect by covering broader lexical distributions,\nwhile Arabic-focused tokenizers further reduce\nfragmentation by explicitly modeling Arabic\nmorphology. This shows how tokenizer training\nimplicitly prioritizes certain linguistic distributions\nin ways that disadvantage underrepresented lan-\nguages. Such fragmentation imposes a structural\nconstraint on downstream processing:\nlonger\neffective input sequences reduce usable context\nand increase sensitivity to question length, offering\na plausible explanation for the sharper performance\ndegradation observed for Arabic as task complexity\nincreases.\n"}, {"page": 9, "text": "5.5\nReliability of Confidence & Explanations\nThe negative relationship between model-reported\nconfidence and correctness suggests that self-\nassessed confidence reflects surface-level fluency\nrather than medical correctness. In multiple-choice\nsettings, this can lead to confident selection of plau-\nsible but incorrect options, limiting the usefulness\nof confidence as an indicator of output reliability.\nWhile this effect appears across languages, it is\nparticularly problematic in low-resource settings,\nwhere lower baseline accuracy increases the risk of\nover-trusting incorrect outputs.\nOur explanation prompting results caution\nagainst treating generated rationales as a reliable\nremedy. Rather than improving outcomes, expla-\nnations induce a model- and language-dependent\nbehavioral shift that reallocates generation toward\ncoherent justifications instead of answer selec-\ntion. These findings show that self-reported confi-\ndence and free-form explanations are insufficient as\nstand-alone reliability signals for multilingual med-\nical QA, motivating evaluation and calibration ap-\nproaches beyond model-internal self-assessments.\n6\nConclusion and Future Work\nOur findings underscore the need for language-\naware adaptation across the entire modeling\npipeline. At the representation level, tokenization\nmust better capture morphological and domain-\nspecific structure; at evaluation, alignment con-\nstraints should avoid conflating surface-form vari-\nation with reasoning errors; and at deployment,\nstronger calibration is required, as model confi-\ndence and explanations are unreliable in multilin-\ngual medical settings. Overall, these results sug-\ngest that improving medical LLM performance in\nunderrepresented languages requires coordinated\ndesign choices rather than isolated model scaling\nor domain specialization.\nMore broadly, we present a diagnostic evaluation\nframework that combines controlled cross-lingual\ncomparisons, question-level analysis, and reliabil-\nity assessment to expose systematic weaknesses\nobscured by aggregate accuracy metrics. Although\nour study focuses on Arabic–English medical tasks,\nthe methodology is language-agnostic and applica-\nble to other low-resource or multilingual settings.\nWe hope this work encourages evaluation protocols\nthat explicitly account for linguistic structure, ro-\nbustness, and reliability when developing medical\nAI systems for diverse clinical populations.\nAcknlowedgements\nThis work was supported by the Meem Foundation\nand the New York University Abu Dhabi (NYUAD)\nCenter for Interdisciplinary Data Science and AI\n(CIDSAI), funded by Tamkeen under the NYUAD\nResearch Institute Award CG016. The research\nwas carried out on NYUAD’s High Performance\nComputing resources (Jubail).\nLimitations\nThis study examines Arabic and English as a con-\ntrolled language pair, with Arabic representing a\nwidely spoken yet underrepresented language in\nmedical NLP. While Arabic presents distinct lin-\nguistic and morphological challenges, it does not\nreflect the full diversity of low-resource or typolog-\nically distant languages; therefore, the generaliz-\nability of our findings beyond this pairing remains\nan open question.\nOur analysis is diagnostic rather than causal. We\nidentify systematic performance patterns across\nlanguage, task complexity, tokenization behavior,\nand reliability signals, but do not isolate the ef-\nfects of specific architectural choices, pre-training\nstrategies, or data composition. This limitation is\nexacerbated by limited transparency in recent LLM\ntraining pipelines, which precludes controlled com-\nparisons between adaptation paradigms such as\ninstruction fine-tuning and continued pre-training.\nSeveral design choices may also influence the re-\nsults. Large models (70B) are evaluated using 4-bit\nquantization, which may introduce size-dependent\neffects but is required for evaluation at scale. In\nthe explanation generation experiment, fixed gen-\neration budgets may disproportionately constrain\nArabic outputs due to higher tokenization fragmen-\ntation; exploring language-adaptive generation lim-\nits is left to future work.\nFinally, Arabic evaluations involve mixed-\nlanguage prompts. While fully Arabic prompt-\ning was preliminarily tested and yielded lower\nperformance, a systematic comparison of prompt-\nlanguage strategies was out of scope. English ver-\nsions of the dataset were obtained via automatic\ntranslation and were not manually validated; al-\nthough the goal is cross-lingual comparison rather\nthan translation quality assessment, translation\nnoise may affect English performance.\n"}, {"page": 10, "text": "Ethical Considerations\nThis work evaluates LLMs for medical question\nanswering, which carries inherent risks if such sys-\ntems are deployed without appropriate safeguards.\nOur study is strictly evaluative and does not ad-\nvocate the use of LLMs as standalone clinical\ndecision-making tools. The dataset used in this\nstudy consists of non-patient-specific medical ques-\ntions and does not involve real clinical records or\npersonal health information. Furthermore, by high-\nlighting systematic performance disparities across\nlanguages, this work aims to support more equi-\ntable evaluation and development of medical AI\nsystems.\nReferences\nMouath Abu-Daoud, Leen Kharouf, Omar El Hajj,\nDana El Samad, Mariam Al-Omari, Jihad Mallat,\nKhaled Saleh, Nizar Habash, and Farah E. Shamout.\n2026.\nMedarabench: Large-scale arabic medical\nquestion answering dataset and benchmark. Preprint,\narXiv:2602.01714.\nAshwag Alasmari, Sarah Alhumoud, and Waad Al-\nshammari. 2024. AraMed: Arabic Medical Question\nAnswering using Pretrained Transformer Language\nModels.\nIn Proceedings of the 6th Workshop on\nOpen-Source Arabic Corpora and Processing Tools\n(OSACT) with Shared Tasks on Arabic LLMs Hallu-\ncination and Dialect to MSA Machine Translation\n@ LREC-COLING 2024, pages 50–56, Torino, Italia.\nELRA and ICCL.\nIñigo Alonso, Maite Oronoz, and Rodrigo Agerri. 2024.\nMedExpQA: Multilingual benchmarking of Large\nLanguage Models for Medical Question Answering.\nArtificial Intelligence in Medicine, 155:102938.\nM Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani,\nNouf M. Alotaibi, Hisham Abdullah Alyahya, Sultan\nAlRashed, Faisal Abdulrahman Mirza, Shaykhah Z.\nAlsubaie, Hassan A. Alahmed, Ghadah Alabduljab-\nbar, Raghad Alkhathran, Yousef Almushayqih, Ra-\nneem Alnajim, Salman Alsubaihi, Maryam Al Man-\nsour, Saad Amin Hassan, Dr. Majed Alrubaian, Ali\nAlammari, Zaki Alawami, and 7 others. 2025. AL-\nLaM: Large Language Models for Arabic and En-\nglish. In The Thirteenth International Conference on\nLearning Representations.\nJunying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wan-\nlong Liu, Rongsheng Wang, and Benyou Wang. 2025.\nTowards Medical Complex Reasoning with LLMs\nthrough Medical Verifiable Problems. In Findings of\nthe Association for Computational Linguistics: ACL\n2025, pages 14552–14573, Vienna, Austria. Associa-\ntion for Computational Linguistics.\nClément Christophe, Praveen K Kanithi, Tathagata\nRaha, Shadab Khan, and Marco AF Pimentel. 2024.\nMed42-v2: A Suite of Clinical LLMs.\nPreprint,\narXiv:2408.06142.\nMouath Abu Daoud,\nChaimae Abouzahir,\nLeen\nKharouf, Walid Al-Eisawi, Nizar Habash, and\nFarah E. Shamout. 2025. Medarabiq: Benchmark-\ning large language models on arabic medical tasks.\nPreprint, arXiv:2505.03427.\nDeepSeek-AI, Aixin Liu, Aoxue Mei, Bangcai Lin,\nBing Xue, Bingxuan Wang, Bingzheng Xu, Bochao\nWu, Bowei Zhang, Chaofan Lin, Chen Dong,\nChengda Lu, Chenggang Zhao, Chengqi Deng, Chen-\nhao Xu, Chong Ruan, Damai Dai, Daya Guo, Dejian\nYang, and 245 others. 2025. Deepseek-v3.2: Push-\ning the Frontier of Open Large Language Models.\nPreprint, arXiv:2512.02556.\nEpoch AI. 2024.\nData on Notable AI Models.\nhttps://epoch.ai/data/notable-ai-models.\nAccessed: 2025-05-11.\nAli Farghaly and Khaled Shaalan. 2009. Arabic natu-\nral language processing: Challenges and solutions.\nACM Transactions on Asian Language Information\nProcessing (TALIP), 8(4):1–22.\nNizar Y Habash. 2010. Introduction to Arabic natural\nlanguage processing, volume 3. Morgan & Claypool\nPublishers.\nGo Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda\nBouamor, and Nizar Habash. 2021. The interplay\nof variant, size, and task type in Arabic pre-trained\nlanguage models. In Proceedings of the Sixth Arabic\nNatural Language Processing Workshop, pages 92–\n104, Kyiv, Ukraine (Virtual). Association for Compu-\ntational Linguistics.\nDaniel P Jeong, Saurabh Garg, Zachary Chase Lip-\nton, and Michael Oberst. 2024. Medical Adaptation\nof Large Language and Vision-Language Models:\nAre We Making Progress?\nIn Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 12143–12170, Miami,\nFlorida, USA. Association for Computational Lin-\nguistics.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What Disease\nDoes This Patient Have? A Large-Sale Open Domain\nQuestion Answering Dataset from Medical Exams.\nApplied Sciences, 11(14).\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Co-\nhen, and Xinghua Lu. 2019. PubMedQA: A Dataset\nfor Biomedical Research Question Answering. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2567–\n2577, Hong Kong, China. Association for Computa-\ntional Linguistics.\nYiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu,\nMunmun De Choudhury, and Srijan Kumar. 2024.\n"}, {"page": 11, "text": "Better to Ask in English: Cross-Lingual Evaluation\nof Large Language Models for Healthcare Queries.\nIn Proceedings of the ACM Web Conference 2024,\nWWW ’24, page 2627–2638, New York, NY, USA.\nAssociation for Computing Machinery.\nVignesh Kumar Kembu, Pierandrea Morandini, Marta\nBianca Maria Ranzini, and Antonino Nocera. 2025.\nAre LLMs Truly Multilingual? Exploring Zero-Shot\nMultilingual Capability of LLMs for Information\nRetrieval: An Italian Healthcare Use Case. Preprint,\narXiv:2512.04834.\nYanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-\nAntoine Gourraud, Mickael Rouvier, and Richard\nDufour. 2024. BioMistral: A Collection of Open-\nSource Pretrained Large Language Models for Medi-\ncal Domains. In Findings of the Association for Com-\nputational Linguistics: ACL 2024, pages 5848–5864,\nBangkok, Thailand. Association for Computational\nLinguistics.\nMeta\nAI.\n2024.\nLlama\n3.3\n70b\ninstruct.\nhttps://huggingface.co/meta-llama/Llama-3.\n3-70B-Instruct. Hugging Face model card.\nMistral AI. 2025.\nMistral-small-3.2-24b-instruct-\n2506.\nhttps://huggingface.co/mistralai/\nMistral-Small-3.2-24B-Instruct-2506.\nHug-\nging Face model card.\nMoshe Munk and Dror G. Feitelson. 2022.\nWhen\nAre Names Similar Or the Same?\nIntroduc-\ning the Code Names Matcher Library.\nPreprint,\narXiv:2209.03198.\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023a.\nCapa-\nbilities of GPT-4 on Medical Challenge Problems.\nPreprint, arXiv:2303.13375.\nHarsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan,\nRichard Edgar, Nicolo Fusi, Nicholas King, Jonathan\nLarson, Yuanzhi Li, Weishung Liu, Renqian Luo,\nScott Mayer McKinney, Robert Osazuwa Ness, Hoi-\nfung Poon, Tao Qin, Naoto Usuyama, Chris White,\nand Eric Horvitz. 2023b. Can Generalist Foundation\nModels Outcompete Special-Purpose Tuning? Case\nStudy in Medicine. Preprint, arXiv:2311.16452.\nOssama Obeid, Nasser Zalmout, Salam Khalifa, Dima\nTaji, Mai Oudah, Bashar Alhafni, Go Inoue, Fadhl\nEryani, Alexander Erdmann, and Nizar Habash. 2020.\nCAMeL Tools: An Open Source Python Toolkit for\nArabic Natural Language Processing. In Proceedings\nof the Twelfth Language Resources and Evaluation\nConference, pages 7022–7032, Marseille, France. Eu-\nropean Language Resources Association.\nOpenMeditron Initiative. 2024. Llama-3.1 meditron-3\n[70b]. https://huggingface.co/OpenMeditron/\nMeditron3-70B. Hugging Face model card; publica-\ntion forthcoming.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan\nSankarasubbu. 2022. MedMCQA: A Large-scale\nMulti-Subject Multi-Choice Dataset for Medical do-\nmain Question Answering. In Proceedings of the\nConference on Health, Inference, and Learning, vol-\nume 174 of Proceedings of Machine Learning Re-\nsearch, pages 248–260. PMLR.\nPengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong\nLin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and\nWeidi Xie. 2024. Towards building multilingual lan-\nguage model for medicine. Nature Communications,\n15(1):8384.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri,\nAtilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, Cían Hughes, Charles\nLau, Justin Chen, Fereshteh Mahvar, Liron Yatziv,\nTiffany Chen, Bram Sterling, Stefanie Anna Baby, Su-\nsanna Maria Baby, Jeremy Lai, Samuel Schmidgall,\nand 62 others. 2025. MedGemma Technical Report.\nPreprint, arXiv:2507.05201.\nNeha Sengupta, Sunil Kumar Sahu, Bokang Jia,\nSatheesh Katipomu, Haonan Li, Fajri Koto, William\nMarshall, Gurpreet Gosal, Cynthia Liu, Zhiming\nChen, Osama Mohammed Afzal, Samta Kamboj,\nOnkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muham-\nmad Mujahid, Massa Baali, Xudong Han, Son-\ndos Mahmoud Bsharat, and 13 others. 2023. Jais and\nJais-chat: Arabic-Centric Foundation and Instruction-\nTuned Open Generative Large Language Models.\nPreprint, arXiv:2308.16149.\nShivalika Singh, Angelika Romanou, Clémentine Four-\nrier, David Ifeoluwa Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchi-\nsio, Wei Qi Leong, Yosephine Susanto, Raymond\nNg, Shayne Longpre, Sebastian Ruder, Wei-Yin\nKo, Antoine Bosselut, Alice Oh, Andre Martins,\nLeshem Choshen, Daphne Ippolito, and 4 others.\n2025. Global MMLU: Understanding and Address-\ning Cultural and Linguistic Biases in Multilingual\nEvaluation. In Proceedings of the 63rd Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 18761–18799, Vi-\nenna, Austria. Association for Computational Lin-\nguistics.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\nClark, Stephen R. Pfohl, Heather Cole-Lewis, Dar-\nlene Neal, Qazi Mamunur Rashid, Mike Schaeker-\nmann, Amy Wang, Dev Dash, Jonathan H. Chen,\nNigam H. Shah, Sami Lachgar, Philip Andrew Mans-\nfield, and 16 others. 2025.\nToward Expert-Level\nMedical Question Answering with Large Language\nModels. Nature Medicine, 31(3):943–950.\nFanar Team, Ummar Abbas, Mohammad Shahmeer Ah-\nmad, Firoj Alam, Enes Altinisik, Ehsannedin Asgari,\nYazan Boshmaf, Sabri Boughorbel, Sanjay Chawla,\nShammur Chowdhury, Fahim Dalvi, Kareem Dar-\nwish, Nadir Durrani, Mohamed Elfeky, Ahmed Elma-\ngarmid, Mohamed Eltabakh, Masoomali Fatehkia,\nAnastasios Fragkopoulos, Maram Hasanain, and\n23 others. 2025.\nFanar:\nAn Arabic-Centric\n"}, {"page": 12, "text": "Multimodal Generative AI Platform.\nPreprint,\narXiv:2501.13944.\nJuraj Vladika, Mahdi Dhaini, and Florian Matthes. 2025.\nFacts Fade Fast: Evaluating Memorization of Out-\ndated Medical Knowledge in Large Language Mod-\nels. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2025, pages 9161–9174,\nSuzhou, China. Association for Computational Lin-\nguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou. 2022. Chain-of-Thought Prompt-\ning Elicits Reasoning in Large Language Models. In\nProceedings of the 36th International Conference on\nNeural Information Processing Systems, NIPS ’22,\nRed Hook, NY, USA. Curran Associates Inc.\nJuncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu,\nTaomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin\nCho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li,\nXiaoxiao Li, and Yuyin Zhou. 2025. MedReason:\nEliciting Factual Medical Reasoning Steps in LLMs\nvia Knowledge Graphs. Preprint, arXiv:2504.00993.\nGuangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong\nZhang. 2024. Benchmarking Retrieval-Augmented\nGeneration for Medicine. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2024,\npages 6233–6251, Bangkok, Thailand. Association\nfor Computational Linguistics.\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu,\nZhihong Chen, Guiming Chen, Jianquan Li, Xi-\nangbo Wu, Zhang Zhiyi, Qingying Xiao, Xiang Wan,\nBenyou Wang, and Haizhou Li. 2023. HuatuoGPT,\nTowards Taming Language Model to Be a Doctor.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2023, pages 10859–10885, Sin-\ngapore. Association for Computational Linguistics.\nXiao Zhang, Ji Wu, Zhiyang He, Xien Liu, and Ying\nSu. 2018. Medical Exam Question Answering with\nLarge-Scale Reading Comprehension. In Proceed-\nings of the Thirty-Second AAAI Conference on Ar-\ntificial Intelligence and Thirtieth Innovative Appli-\ncations of Artificial Intelligence Conference and\nEighth AAAI Symposium on Educational Advances in\nArtificial Intelligence, AAAI’18/IAAI’18/EAAI’18.\nAAAI Press.\n"}, {"page": 13, "text": "A\nData Samples\nFigure A1 shows representative examples from the\ndataset, including the original Arabic question and\nits corresponding English translation, to illustrate\nthe structure and content of the bilingual data used\nin our experiments.\nFigure A1: Example dataset entry showing an Arabic\nMCQ (top) and its English translation (bottom).\nB\nPrompts Used\nB.1\nLetter-Based Prompting\nFigure B2 shows the exact prompt template used\nfor letter-based multiple-choice question answer-\ning, where the model is instructed to return only\na single answer option (A–F) without additional\nexplanation.\nFigure B2: Prompt template used for letter-based\nMCQ answering.\nB.2\nText Generation Prompting\nFigure B3 shows the prompt template used for ex-\nact text generation matching, where the model is\ninstructed to return the exact text sequence cor-\nresponding to the correct answer option without\nadditional explanation.\nFigure B3: Prompt template used for text generation\nMCQ answering.\nB.3\nConfidence Generation Prompting\nFigure B4 shows the exact prompt template used\nfor confidence-aware MCQ answering, where the\nmodel is instructed to report an explicit confi-\ndence estimate alongside its selected answer using\na strictly defined output format, without providing\nany additional explanation.\nFigure B4: Prompt template used for confidence gen-\neration MCQ answering.\nB.4\nExplanation Generation Prompting\nFigure B5 shows the exact prompt template used\nfor explanation-based MCQ answering, where the\nmodel is instructed to generate a brief medical ratio-\nnale followed by a final answer selection, enabling\nanalysis of whether generated explanations corre-\nlate with answer correctness.\n"}, {"page": 14, "text": "Figure B5: Prompt template used for explanation-\nbased MCQ answering.\nC\nExamples of Failure Modes Under\nExplanation Prompting\nFigure C6 shows representative examples of fail-\nure modes observed under explanation-conditioned\nprompting. In these cases, models generate med-\nically plausible or partially correct explanations\nbut select an incorrect answer option, revealing\nmisalignment between reasoning and final answer\nselection.\nFigure C6: Examples of reasoning–label misalign-\nment under explanation prompting. Models may\nproduce correct or salient medical reasoning while se-\nlecting an incorrect option due to option mismatch or\nincomplete evaluation of alternatives.\n"}]}