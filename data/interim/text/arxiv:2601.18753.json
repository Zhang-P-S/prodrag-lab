{"doc_id": "arxiv:2601.18753", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.18753.pdf", "meta": {"doc_id": "arxiv:2601.18753", "source": "arxiv", "arxiv_id": "2601.18753", "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs", "authors": ["Xinyue Zeng", "Junhong Lin", "Yujun Yan", "Feng Guo", "Liang Shi", "Jun Wu", "Dawei Zhou"], "published": "2026-01-26T18:23:09Z", "updated": "2026-01-26T18:23:09Z", "summary": "The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.18753v1", "url_pdf": "https://arxiv.org/pdf/2601.18753.pdf", "meta_path": "data/raw/arxiv/meta/2601.18753.json", "sha256": "48c2727983c0397f2173d1465df1e57d65ddb063c4af018a1acc0d79f382f2b3", "status": "ok", "fetched_at": "2026-02-18T02:20:28.080803+00:00"}, "pages": [{"page": 1, "text": "Published as a conference paper at ICLR 2026\nHALLUGUARD: DEMYSTIFYING DATA-DRIVEN AND\nREASONING-DRIVEN HALLUCINATIONS IN LLMS\nXinyue Zeng∗\nVirginia Tech\nCS Department\nJunhong Lin∗\nMIT\nEECS Department\nYujun Yan\nDartmouth College\nCS Department\nFeng Guo\nVirginia Tech\nStatistics Department\nLiang Shi\nVirginia Tech\nStatistics Department\nJun Wu\nMichigan State University\nCS Department\nDawei Zhou\nVirginia Tech\nCS Department\nABSTRACT\nThe reliability of Large Language Models (LLMs) in high-stakes domains such\nas healthcare, law, and scientific discovery is often compromised by hallucina-\ntions. These failures typically stem from two sources: data-driven hallucinations\nand reasoning-driven hallucinations. However, existing detection methods usu-\nally address only one source and rely on task-specific heuristics, limiting their\ngeneralization to complex scenarios. To overcome these limitations, we introduce\nthe Hallucination Risk Bound, a unified theoretical framework that formally de-\ncomposes hallucination risk into data-driven and reasoning-driven components,\nlinked respectively to training-time mismatches and inference-time instabilities.\nThis provides a principled foundation for analyzing how hallucinations emerge\nand evolve. Building on this foundation, we introduce HALLUGUARD, a NTK-\nbased score that leverages the induced geometry and captured representations of\nthe NTK to jointly identify data-driven and reasoning-driven hallucinations. We\nevaluate HALLUGUARD on 10 diverse benchmarks, 11 competitive baselines, and\n9 popular LLM backbones, consistently achieving state-of-the-art performance in\ndetecting diverse forms of LLM hallucinations.\n1\nINTRODUCTION\nLarge language models (LLMs) are increasingly deployed in high-stakes domains such as health-\ncare, law, and scientific discovery(Bommasani et al., 2021; Thirunavukarasu et al., 2023). However,\nadoption in these settings remains cautious, as such domains are highly regulated and demand strict\ncompliance, interpretability, and safety guarantees(Dennst¨adt et al., 2025; Kattnig et al., 2024). A\nmajor barrier is the risk of hallucinations, generated content appears unfaithful or nonsensical. Such\nerrors can have severe consequences(Dennst¨adt et al., 2025)—as the example in Figure 1, a gener-\nated incorrect medical diagnosis may delay treatment or lead to harmful interventions. Therefore,\ndetecting hallucinations is not merely a technical challenge but a prerequisite for trustworthy de-\nployment, as undetected errors undermine reliability, accountability, and user safety.\nGenerally, hallucinations in LLMs arise from two primary sources(Ji et al., 2023; Huang et al.,\n2023): data-driven hallucinations, which stem from flawed, biased, or incomplete knowledge en-\ncoded during pre-training or fine-tuning; and reasoning-driven hallucinations, which originate from\ninference-time failures such as logical inconsistencies or breakdowns in multi-step reasoning(Zhang\net al., 2023; Zhong et al., 2024). Detection methods broadly split along these two dimensions.\nApproaches for data-driven hallucinations often compare outputs against retrieved documents or\nreferences(Shuster et al., 2021; Min et al., 2023; Ji et al., 2023), or exploit sampling consistency as\nin SelfCheckGPT(Manakul et al., 2023). In contrast, methods for reasoning-driven hallucinations\nrely on signals of inference-time instability, including probabilistic measures such as perplexity(Ren\net al., 2022), length-normalized entropy(Malinin & Gales, 2020), semantic entropy(Kuhn et al.,\n∗Equal contribution.\n1\narXiv:2601.18753v1  [cs.LG]  26 Jan 2026\n"}, {"page": 2, "text": "Published as a conference paper at ICLR 2026\nFigure 1: An illustration of hallucination emerging and evolving in the context of disease diagnosis.\n2023), energy-based scoring(Liu et al., 2020), and RACE(Wang et al., 2025). Others probe internal\nrepresentations, for example, Inside(Chen et al., 2024a), which applies eigenvalue-based covariance\nmetrics and feature clipping, ICR Probe(Zhang et al., 2025), which tracks residual-stream updates,\nand Shadows in the Attention(Wei et al., 2025), which analyzes representation drift under contextual\nperturbations. While these methods shed light on the mechanisms underlying hallucinations, most\nremain tailored to a single hallucination type and fail to capture their evolution. Yet growing evi-\ndence indicates that data-driven and reasoning-driven hallucinations often evolve during multi-step\ngeneration(Liu et al., 2025; Sun et al., 2025). As shown in Figure 1, it emerges from an initial dis-\nease misclassification and evolves into a distorted diagnosis, delaying treatments and risking fatality.\nThis gap brings two central questions: (1) How can we develop a unified theoretical understanding\nof how hallucinations evolve? and (2) How can we detect them effectively and efficiently without\nrelying on external references or task-specific heuristics?\nTo address these challenges, we propose a unified theoretical framework–Hallucination Risk Bound,\nwhich decomposes the overall hallucination risk into two components: a data-driven term, capturing\nsemantic deviations rooted in inaccurate, imbalanced, or noisy supervision acquired during model\ntraining; and a reasoning-driven term, reflecting instability introduced by inference-time dynam-\nics, such as logical missteps or temporal inconsistency. This decomposition not only elucidates the\nmechanism behind hallucinations but also reveals how they emerge and evolve. Specifically, our\nanalysis shows that hallucinations originate from semantic approximation gaps-captured by repre-\nsentational limits of the model-and are subsequently amplified by unstable rollout dynamics, evolv-\ning across decoding steps. As such, our framework offers a unified theoretical lens for characterizing\nthe emergence and evolution of these hallucinations.\nBuilding on the theoretical foundation, we propose HALLUGUARD, a Neural Tangent Kernel(NTK)-\nbased score that leverages the induced geometry and captured representations of the NTK to jointly\nidentify data-driven and reasoning-driven hallucinations. We evaluate HALLUGUARD comprehen-\nsively across 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones.\nHALLUGUARD consistently achieves state-of-the-art hallucination detection performance, demon-\nstrating its efficacy.\n2\nPRELIMINARIES\nHallucination Detection.\nThere are two primary sources of hallucinations in LLMs(Ji et al.,\n2023; Huang et al., 2023): data-driven hallucination, which stems from incomplete or biased knowl-\nedge encoded during pre-training or fine-tuning, and reasoning-driven hallucination, which arises\nfrom unstable or inconsistent inference dynamics at decoding time. This distinction has implicitly\nguided a broad range of detection strategies, which we examine through these two lenses.\nFor data-driven causes, a recurring signal is elevated predictive uncertainty. A common formulation\nadopts the sequence-level negative log-likelihood:\nU(y | x, θ) = −1\nT\nT\nX\nt=1\nlog pθ(yt | y<t, x),\n(1)\nwhich quantifies the average uncertainty of generating a sequence y = [y1, . . . , yT ] from input x and\nθ denotes model parameters. This directly recovers Perplexity(Ren et al., 2022), where low scores\n2\n"}, {"page": 3, "text": "Published as a conference paper at ICLR 2026\nimply confident predictions, while high scores indicate implausible generations due to weak priors.\nTo capture more nuanced uncertainty, later methods extend this formulation to multi-sample settings.\nThe Length-Normalized Entropy(Malinin & Gales, 2020) penalizes dispersion across stochastic gen-\nerations Y = {y1, . . . , yK}, offering a finer-grained view of model indecision. This perspective is\nfurther enriched by Semantic Entropy(Kuhn et al., 2023), which projects sampled responses into\nsemantic space, and by energy-based scoring(Liu et al., 2020), which replaces log-probability with\na learned confidence function. Collectively, these methods reflect a progression from token-level\nlikelihoods to semantically grounded multi-sample uncertainty estimators.\nIn contrast, reasoning-driven hallucinations arise from brittle inference trajectories, where identical\ncontexts may yield inconsistent or incoherent outputs. A commonly used measure of such instability\nis the cross-sample consistency score:\nC(Y | x, θ) = 1\nC\nK\nX\ni=1\nK\nX\nj=i+1\nsim(yi, yj),\n(2)\nwhere C = K · (K −1)/2, and sim(·, ·) is a similarity function such as ROUGE-L(Lin, 2004),\ncosine similarity, or BLEU(Chen et al., 2024b). Low scores reflect diverging generations and un-\nstable reasoning. Several reasoning-driven detection methods can be interpreted through this lens.\nEarly approaches used surface-level lexical overlap metrics(Lin et al., 2022b), while SelfCheck-\nGPT(Manakul et al., 2023) advanced this by evaluating factual entailment across responses, and\nFActScore(Min et al., 2023) extended this further by comparing outputs to retrieved reference doc-\numents. More recent efforts probe internal signals directly: Inside(Chen et al., 2024a) analyzes the\ncovariance spectrum of embedding representations, and RACE(Wang et al., 2025) diagnoses insta-\nbility in multi-step reasoning.\nNTK in LLMs.\nNTK provides a principled framework for analyzing the training dynamics in\nthe overparameterized regime characteristic of modern LLMs(Jacot et al., 2020). Formally, for a\nnetwork output f(x, θ) with input x and parameters θ, the NTK is defined as:\nΘ(x, x′, θ) = ∇θf(x, θ) · ∇θf(x′, θ).\n(3)\nThis kernel Θ(x, x′, θ) quantifies the similarity of training dynamics between inputs x and x′. In the\ninfinite-width limit, it converges to a deterministic value at initialization and remains nearly constant\nthroughout training(Lee et al., 2020b). This stability reduces the highly nonlinear optimization of\ndeep networks to a tractable kernel regression problem. By examining the eigenspectrum of the\nNTK, one can probe how internal representations are shaped during training: which features are\nprioritized (e.g., syntax versus semantics), how quickly different tasks converge, and why overpa-\nrameterized networks generalize effectively to unseen data(Ju et al., 2022). In this way, the NTK\ntransforms the apparent complexity of LLM optimization into a clear lens on how these models\ncapture, process, and generalize information(Zeng et al., 2025).\n3\nMETHODOLOGY\n3.1\nPROBLEM SETTING\nOur analysis reveals that hallucination is not a unified failure mode but rather shifts with the task\nstructure. On the instruction-following Natural benchmark(Wang et al., 2022), 88.9% of the\noverall 3499 errors are from logical missteps (reasoning-driven) while 11.1% are factual inaccura-\ncies (data-driven). By contrast, on the math-focused MATH-500(Hendrycks et al., 2021), the 1985\nwrong generations are dominated by 1946 reasoning errors (98.1%), with only 19 factual flaws\n(1.9%). This contrast highlights that, in practice, hallucinations are rarely pure but often mixtures of\ndata-driven bias and reasoning-driven instability—motivating our formal decomposition of halluci-\nnation sources.\nProblem Definition.\nLet Y denote the space of textual outputs and let Φ : Y →Uh be a task-\nspecific encoder that maps textual sequences into the hypothesis space Uh, equipped with a norm\n∥· ∥(e.g., task-calibrated embedding space or structured metric). We interpret each u ∈Uh as\na reasoning chain, composed of step-wise logical statements. For an input x with ground-truth\noutput y∗∈Y, define the gold-standard reasoning chain as u∗:= Φ(y∗) ∈Uh. An LLM with\n3\n"}, {"page": 4, "text": "Published as a conference paper at ICLR 2026\nparameters θ emits a random sequence Y = (Y1, . . . , YT ) via pθ(yt | y<t, x), yielding a predicted\nreasoning chain uh := Φ(Y ) ∈Uh. Its expected value under the model’s decoding distribution is\nE[uh] := EY ∼pθ(·|x)[Φ(Y )].\nWe consider perturbations in a local neighborhood of the decoding process. Let δ ∈Rr parameterize\na small perturbation (e.g., of the prefix tokens, step-t logits, or hidden state), and let Bρ := {δ :\n∥δ∥≤ρ}. Define the perturbed decoder map G : Rr →Uh by G(δ) := Φ\n\u0000Y (δ)\n\u0001\n, where Y (δ) is the\nsequence under perturbation. Let J ∈Rdh×r denote the (Gauss–Newton) Jacobian of G at δ = 0.\nOur goal is to formalize how hallucination emerges and evolves in LLMs.\n3.2\nHALLUCINATION RISK BOUND\nTo bridge the formal setup with the phenomenon of hallucination, we first disentangle the sources of\nhallucinations. Intuitively, hallucinations may arise either from systematic biases in the knowledge\nencoded by the model (data-driven) or from instabilities during autoregressive decoding (reasoning-\ndriven). The following proposition formalizes this idea by decomposing the total hallucination risk\ninto two components.\nWe first impose the following assumptions:\nA1. (U, ∥· ∥) is a Hilbert space; Φ is measurable with unique best solution and ∥Φ(Y )∥has\nfinite second moment.\nA2. Triangle inequality holds for ∥· ∥and Φ is LΦ-Lipschitz w.r.t. an edit distance on Y.\nA3. For δ ∈Bρ, the mapping G admits the local expansion G(δ) = G(0) + Jδ + R(δ), where\nthe remainder is bounded by ∥R(δ)∥≤1\n2H⋆∥δ∥2 for some curvature constant H⋆> 0.\nProposition 3.1 (Hallucination Risk Decomposition).\nUnder A1–A3, applying the triangle\ninequality yields a natural split of the risk:\n∥u∗−uh∥≤∥u∗−E[uh]∥\n|\n{z\n}\ndata-driven term\n+ ∥uh −E[uh]∥\n|\n{z\n}\nreasoning-driven term\nThis decomposition distinguishes errors caused by systematic bias in the learned representation\nfrom those introduced during stochastic rollout.\nCharacterizing Data-Driven Hallucination.\nTo quantify the data-driven term, we take inspira-\ntion from the NTK, which has proven effective in analyzing training dynamics of overparameterized\nmodels. Here, NTK geometry provides a way to measure how well the model’s representation space\naligns with task generation under small perturbations.\nLet Uh ⊂U denote the hypothesis subspace accessible to the model under perturbations. By C´ea’s\nlemma(C´ea, 1964) with curvature penalty, the data-driven term can be bounded as\n∥u∗−E[uh]∥≤Λ\nγ\ninf\nu∈Uh ∥u∗−u∥,\n(4)\nwhere γ = λmin(KΦ) is the smallest eigenvalue of the NTK Gram matrix on embedded perturba-\ntions, and Λ ≤∥T∥reflects the operator norm of the problem/operator mapping T . Intuitively, the\nratio Λ\nγ measures the conditioning of the feature map: well-conditioned NTK spectra allow a closer\napproximation to the true generation.\nThis ratio can be further controlled in terms of pretraining–finetuning mismatch:\nΛ\nγ\n≤1 + kpt logO(P, L) + k · ϵmismatch\nSignalk\n,\n(5)\nwhere logO(P, L) is a complexity term from parameter count P and prompt length L, ϵmismatch\ndenotes the Wasserstein distance between prompt and query distributions, Signalk measures task-\naligned energy in the top-k eigenspace. kpt and k are task and model-dependent constants. Thus,\ndata-driven hallucinations grow when the mismatch is large or when the task signal is weak.\n4\n"}, {"page": 5, "text": "Published as a conference paper at ICLR 2026\nCharacterizing\nReasoning-Driven\nHallucination.\nThe\nreasoning-driven\nterm\ncaptures\nreasoning-driven instability that accumulates during autoregressive decoding. Here, we model gen-\neration as a martingale process, where deviation from the expectation is controlled by concentration\ninequalities. Specifically, Freedman’s inequality(Geman et al., 1992) gives\n∥uh −E[uh]∥≤K · exp\n\u0010\n−Kϵ2\nC\n\u0011\n· α(eβT −1),\n(6)\nwhere K is the number of rollouts averaged, β summarizes per-step growth in local Jacobians, α\nscales the cumulative effect and C is a task and model-dependent constant. This bound shows that\nreasoning-driven hallucinations grow exponentially with sequence length T.\nWe now synthesize the two components into a unified result that characterizes the overall risk of\nhallucination. By combining the NTK-conditioned approximation bound for data-driven deviation\nwith the Freedman-style concentration bound for reasoning-driven instability, we obtain the follow-\ning unified bound of data-driven and reasoning-driven hallucinations (detailed proof is provided in\nSection A):\nTheorem 3.2 (Hallucination Risk Bound).\nLet u∗:= Φ(y∗) denote the semantic embed-\nding of the ground-truth output and uh := Φ(Y ) that of the model-generated output. Under\nAssumptions A1–A3, suppose there exists β ≥0 such that\n\r\r\r QT\nt=1 Jt\n\r\r\r\n2 ≤eβT . Then the total\nhallucination risk satisfies\n∥u∗−uh∥≤\n\u0010\n1 + kpt log O(P, L) + k · ϵmismatch\nSignalk\n\u0011\ninf\nu∈Uh ∥u∗−u∥\n|\n{z\n}\ndata-driven term\n+ |L| · exp\n\u0010\n−Kϵ2\nC\n\u0011\n· α\n\u0000eβT −1\n\u0001\n|\n{z\n}\nreasoning-driven term\n3.3\nHALLUCINATION QUANTIFICATION VIA HALLUGUARD\nWhile Theorem 3.2 makes explicit how data-driven and reasoning-driven hallucinations emerge and\nevolve, applying it directly at inference is impractical since direct step-wise Jacobians for billion-\nparameter LLMs are intractable, so we seek a proxy score that is computable, stable, and faithful to\nour decomposition.\nLet K denote the NTK Gram matrix with eigenvalues λ1 ≥· · · ≥λr > 0 and condition number\nκ(K) = λmax/λmin. Let Jt be the step-t input–output Jacobian of the decoder, and define σmax :=\nsupt ∥Jt∥2 as the uniform spectral bound(note that σmax is independent of the spectrum of K).\nUnder Assumptions A1–A3, a standard NTK approximation argument yields infu∈Uh ∥u∗−u∥≤\nCd det(K)−cd ∥u∗∥, so that det(K) capture the representations in systematic bias.\nFor autoregressive rollout, based on the property of Jacobian, we have\n\r\r\r QT\nt=1 Jt\n\r\r\r\n2\n≤\nQT\nt=1 ∥Jt∥2\n=\nexp\n\u0010 PT\nt=1 log ∥Jt∥2\n\u0011\n, so that we have\n\r\r\r QT\nt=1 Jt\n\r\r\r\n2\n≤\neβT . Since\nβ ≤log σmax with σmax := supt ∥Jt∥2 thus we have the upper bound as ∥QT\nt=1 Jt∥2 ≤σT\nmax =\ne(log σmax)T . Thus, log σmax serves as a stable and tractable proxy for the per-step amplification rate.\nPerturbation analysis of K, together with classical eigenvalue sensitivity results(Trefethen & Bau,\n2022), yields Var[uh]\n≤\ncv κ(K)2 ∥δ∥2, showing that instability grows quadratically with the\ncondition number κ(K). To temper this effect and ensure additivity, we penalize ill-conditioned\nrepresentations via −log κ2, where log compression brings a well-behaved dynamic range.\nTable 1: Correlation between NTK proxies\nand task families.\nSQuAD Math-500 TruthfulQA\ndet(K)\n0.84\n0.42\n0.61\nlog σmax −log κ2\n0.39\n0.88\n0.67\nIn summary, det(K) quantifies representational ad-\nequacy, log σmax captures rollout amplification,\nand −log κ2 penalizes spectral instability, together\nforming a compact and tractable proxy consis-\ntent with the Hallucination Risk Bound.\nThe\nlightweight projection layers are self-supervised\nspectral calibration modules, optimized offline (via\nAdamW) to align NTK spectral properties across heterogeneous backbones into a stable, compara-\nble geometric space—without hallucination labels or task-specific supervision, with the backbone\nfully frozen and zero runtime overhead during inference. Detailed proofs are provided in Section B.\n5\n"}, {"page": 6, "text": "Published as a conference paper at ICLR 2026\nEmpirical validation.\nWe empirically validate how those proxies correlate with different task\nfamilies. In Table 1, det(K) correlates most strongly with the data-centric task SQuAD (0.84), in-\ndicating its role in capturing factual fidelity. In contrast, for the reasoning-oriented MATH-500, the\nhighest correlation is observed with log σmax −log κ2 (0.88), reflecting the importance of amplifi-\ncation and stability in multi-step reasoning.\nMotivated by the above, we formally define HALLUGUARD as follows, which provides a principled\nand unified lens for hallucination detection:\nHALLUGUARD(uh) = det(K) + log σmax −log κ2.\n(7)\n4\nEXPERIMENTS\nWe comprehensively evaluate HALLUGUARD across 10 diverse benchmarks, 11 competitive base-\nlines, and 9 popular LLM backbones. We aim to evaluate its efficacy from the following five ques-\ntions: Q1: How does HALLUGUARD perform across different task families? Q2: How does HAL-\nLUGUARD perform across LLMs of different scales? Q3: How does each term capture trends across\ntask families? Q4: Can HALLUGUARD guide test-time inference to improve downstream reason-\ning? Q5: How well does HALLUGUARD generalize to detecting fine-grained hallucinations beyond\nbenchmarks?\nSection 4.1 details the setup; Section 4.2 evaluates HALLUGUARD as a detection method(Q1–Q3),\nSection 4.3 applies HALLUGUARD in score-guided inference(Q4) and Section 4.4 analyzes HAL-\nLUGUARD on fine-grained hallucination via a case study on semantic data(Q5).\n4.1\nEVALUATION SETUP\nBenchmarks. We evaluate across 10 widely used benchmarks spanning three distinct categories.\nFor data-grounded QA, we include RAGTruth(Niu et al., 2024), NQ-Open(Kwiatkowski et al.,\n2019), HotpotQA(Yang et al., 2018) and SQuAD(Rajpurkar et al., 2016), which emphasize factual\ncorrectness through external evidence. For reasoning-oriented tasks, we use GSM8K(Cobbe et al.,\n2021), MATH-500(Hendrycks et al., 2021), and BBH(Suzgun et al., 2022), which require multi-step\nderivations prone to compounding errors. Finally, for instruction-following settings, we consider\nTruthfulQA(Lin et al., 2022a), HaluEval(Li et al., 2023) and Natural(Wang et al., 2022),\nwhich probe hallucinations under open-ended or adversarial prompts.\nBaselines. We compare HALLUGUARD with 11 competitive detectors spanning diverse strate-\ngies. Uncertainty-based methods include Perplexity(Ren et al., 2022), Length-Normalized Predic-\ntive Entropy(LN-Entropy)(Malinin & Gales, 2020), Semantic Entropy(Kuhn et al., 2023), Energy\nScore(Liu et al., 2020) and P(true)(Kadavath et al., 2022). Consistency-based approaches cover\nSelfCheckGPT(Manakul et al., 2023), Lexical Similarity(Lin et al., 2022b), FActScore(Min et al.,\n2023) and RACE(Wang et al., 2025). Internal-state methods are represented by Inside(Chen et al.,\n2024a) and MIND(Su et al., 2024).\nLLM Backbone Models. We evaluate 9 publicly available LLMs spanning different scales and\narchitectures. These include five models from the Llama family (Llama2-7B, Llama2-13B, Llama2-\n70B, Llama3-8B, and Llama3.2-3B)(Touvron et al., 2023; Grattafiori et al., 2024), along with OPT-\n6.7B(Zhang et al., 2022), Mistral-7B-Instruct(Jiang et al., 2023), QwQ-32B(Yang et al., 2024), and\nGPT-2 (117M)(Radford et al., 2019). All models are used in their off-the-shelf form with pre-trained\nweights and tokenizers provided by Hugging Face, without further fine-tuning.\nEvaluation Metrics. We evaluate hallucination detection ability under two regimes following Ja-\nniak et al. (2025): ROUGE-based reference evaluation (∗r) and LLM-AS-A-JUDGE (∗llm). For\nperformance measures, we report the area under the receiver operating characteristic curve (AU-\nROC) and the area under the precision–recall curve (AUPRC). AUROC is widely used to assess the\nquality of binary classifiers and uncertainty estimators, while AUPRC highlights performance under\nclass imbalance. In both cases, higher values indicate better detection.\n6\n"}, {"page": 7, "text": "Published as a conference paper at ICLR 2026\n4.2\nMAIN RESULTS\nQ1: How does HALLUGUARD perform across different task families? To evaluate how HAL-\nLUGUARD performs across different task types, we conduct experiments on all benchmarks. For\nclarity, Table 2 presents representative results from three task families: data-centric (RAGTruth),\nreasoning-oriented (Math-500), and instruction-following (TruthfulQA). As shown, HAL-\nLUGUARD consistently outperforms all baselines across backbones. On Math-500, it reaches\n81.76% AUROC and 79.76% AUPRC, improving over the second-best method by up to 8.3%.\nOn RAGTruth, it attains 84.59% AUROC and 81.15% AUPRC, with gains of up to 7.7%. On\nTruthfulQA, it achieves 77.05% AUROC and 73.79% AUPRC, exceeding the next strongest base-\nline by as much as 6.2%. Overall, HALLUGUARD establishes new state-of-the-art results across di-\nverse task families, with particularly pronounced improvements on reasoning-oriented benchmarks.\nTable 2:\nPerformance comparison on representative benchmarks:\ndata-centric (RAGTruth),\nreasoning-oriented (Math-500), and instruction-following (TruthfulQA). We highlight the first\nand second best results.\nGPT2\nOPT-6.7B\nMistral-7B\nQwQ-32B\nAUROCr\nAUPRCr\nAUROCllm\nAUPRCllm\nAUROCr\nAUPRCr\nAUROCllm\nAUPRCllm\nAUROCr\nAUPRCr\nAUROCllm\nAUPRCllm\nAUROCr\nAUPRCr\nAUROCllm\nAUPRCllm\nRAGTruth\nHALLUGUARD 75.51 73.40 62.40 56.60\n80.13 76.77 71.01 63.58\n82.31 80.79 64.89 67.25\n84.59 81.15 71.82 66.68\nInside\n73.42 73.08 61.99 56.39\n79.49 71.82 66.1 62.46\n75.32 73.19 64.58 61.05\n77.72 73.47 66.05 64.73\nMIND\n58.54 54.79 43.47 41.85\n63.82 62.58 51.03 44.78\n73.13 71.53 58.25 58.6\n64.23 63.06 47.37 51.47\nPerplexity\n58.07 56.68 43.84 41.53\n64.47 61.57 47.12 52.98\n65.42 63.63 53.28 51.36\n73.91 72.92 60.81 59.77\nLN-Entropy\n64.42 60.79 49.41 45.04\n60.81 57.91 48.76 42.27\n64.22 60.92 52.24 48.41\n63.81 62.26 47.52 52.17\nEnergy\n65.53 62.42 51.8 47.22\n66.54 63.28 54.21 49.19\n64.36 62.26 48.64 53.93\n73.26 71.21 65.43 62.32\nSemantic Ent.\n60.72 59.41 50.55 45.86\n70.2 68.34 54.54 56.74\n66.01 64.49 53.01 55.5\n66.48 64.41 51.54 50.11\nLexical Sim.\n64.72 63.1 55.04 48.04\n67.28 64.62 52.55 54.86\n64.96 61.17 52.34 45.11\n70.87 67.41 61.25 51.01\nSelfCheckGPT\n65.4 62.79 52.85 52.43\n66.64 64.89 52.69 51.17\n71.19 68.45 63.13 60.23\n65.79 62.45 54.76 51.29\nRACE\n64.83 62.84 51.8 48.44\n64.26 61.03 52.74 46.22\n66.34 64.54 51.88 53.86\n71.13 69.96 57.58 55.54\nP(true)\n66.19 64.04 48.2 56.27\n68.44 65.48 57.53 53.08\n72.54 71.8 57.25 59.42\n65.32 63.01 53.01 52.32\nFActScore\n65.72 64.39 51.94 47.51\n61.53 58.2 51.86 45.57\n63.98 60.71 53.54 49.34\n66.72 64.03 58.21 49.17\nBBH\nHALLUGUARD 71.06 67.94 62.05 59.05\n73.1 70.88 63.67 61.88\n79.85 76.5 67.13 60.57\n81.76 79.76 68.77 65.46\nInside\n66.18 66.81 56.15 58.62\n70.64 65.22 63.28 59.28\n67.2 65.49 51.3 53.46\n80.8 71.49 64.05 63.42\nMIND\n55.41 51.77 39.01 41.59\n55.48 53.46 38.59 40.88\n65.71 63.7 49.61 52.54\n61.75 60.18 53.46 50.04\nPerplexity\n53.28 50.22 43.86 38.98\n64.89 62.12 48.65 51.99\n61.97 60.05 51.15 42.87\n60.28 57.75 51.62 43.38\nLN-Entropy\n60.84 58.76 42.76 47.48\n58.71 55.01 43.55 42.02\n68.96 69.44 58.79 57.49\n63.96 62.18 46.01\n49.5\nEnergy\n55.09 51.99 46.2\n39.5\n53.96 50.98 42.56 34.12\n66.27 62.72 49.48 50.06\n69.61 68.66 54.35 57.36\nSemantic Ent.\n58.16 54.81 49.61 40.39\n62.63 59.52 50.14 45.02\n64.99 61.33 50.11 45.53\n62.76 60.95 45.77 45.75\nLexical Sim.\n51.37 47.18 38.37 39.06\n61.27 58.06 44.13 42.96\n58.25 55.92 46.31 46.01\n69.46 67.59 55.93\n52.6\nSelfCheckGPT\n54.51 51.86 44.62 44.01\n57.36 53.21 42.55 38.27\n63.68 62.5\n51.7 53.03\n64.56 62.49 55.85\n45.8\nRACE\n55.99 54.66 41.39 38.32\n64.23 62.03 56.03 53.44\n66.88 64.33 49.57 48.5\n59.5 55.83 46.13 41.07\nP(true)\n54.57 52.88 45.45 44.74\n57.02 55.49 48.81 37.84\n57.11 55.21 43.93 47.05\n61.49 59.03 44.37 44.69\nFActScore\n56.76 53.85 40.25 40.01\n54.51 53.2 38.45 36.49\n62.11 58.64 53.52 47.27\n58.82 57.47 49.48 42.74\nTruthfulQA\nHALLUGUARD 72.1 68.76 60.09 52.01\n69.59 68.36 58.52 52.65\n77.05 73.79 63.62 62.26\n74.26 72.76 57.39 64.07\nInside\n70.42 68.76 60.09 52.01\n62.1 59.78 51.07 51.38\n62.53 60.99 52.3 49.35\n70.89 64.44 56.61 56.01\nMIND\n59.45 56.79 45.22 43.71\n60.56 58.55 47.49 49.63\n59.2 57.98 47.23 41.79\n62.81 61.5 52.56 46.37\nPerplexity\n50.57 47.87 40.64 35.63\n55.07 52.26 44.43 42.79\n60.8 59.69 47.33 41.62\n55.29 52.46 43.95 43.92\nLN-Entropy\n58.04 56.99 41.94 47.21\n56.12 54.01 47.06 38.4\n59.67 56.25 41.99 41.25\n60.76 58.21 46.24 42.64\nEnergy\n55.02 53.31 38.78 45.16\n54.42 51.85 36.21 42.57\n58.93 55.25 50.76 41.72\n64.15 61.32 51.78 50.02\nSemantic Ent.\n61.01 57.08 43.35 45.2\n51.48 47.81 34.15 38.16\n54.44 53.33 36.62 40.35\n66.75 63.85 51.11 46.71\nLexical Sim.\n52.54 50.56 39.94 33.42\n59.74 55.72 49.89 46.81\n66.16 64.05 54.08 51.65\n55.24 51.36 46.39 39.57\nSelfCheckGPT\n56.04 54.48 43.78 44.38\n58.93 56.47 47.65 39.02\n61.14 58.91 42.97 47.01\n55.86 54.95 41.08 37.35\nRACE\n53.02 50.33 41.7 33.81\n62.95 67.89 54.61 51.93\n71.06 68.49 60.4 57.44\n55.75 52.62 46.5\n43.19\nP(true)\n55.52 53.41 38.33 38.38\n54.88 53.1 38.22 40.96\n55.8 52.01 40.88 38.72\n57.18 55.16 46.19 38.21\nFActScore\n53.82 51.42 41.33 35.2\n54.57 51.26 42.51 35.52\n53.97 50.2 42.97 36.16\n62.31 60.23 45.06\n49.9\nQ2:\nHow does HALLUGUARD perform across LLMs of different scales?\nWe fur-\nther investigate whether the effectiveness of HALLUGUARD depends on model scale, as\nsmaller backbones are typically more prone to hallucination.\nTable 3 reports representa-\ntive results on small(Llama2-7B, Llama3-8B), mid-sized(Llama2-13B), and large-scale(Llama2-\n70B) models using SQuAD, GSM8K, and HaluEval.\nAcross all settings, HALLUGUARD\nconsistently surpasses baselines, with the largest margins on smaller models—for instance,\n7\n"}, {"page": 8, "text": "Published as a conference paper at ICLR 2026\nFigure 2:\nAblation results comparing in-\ndividual terms with ground-truth trends on\nSQuAD (top) and Math-500 (bottom).\n72.89% AUPRCr on HaluEval with Llama2-7B,\nmore than 10% above the second best.\nMid-\nsized models also exhibit clear gains (e.g., 79.01%\nAUROCr on GSM8K), while even large-scale mod-\nels like Llama2-70B see steady improvements (e.g.,\n83.8% AUROCr on SQuAD). Overall, HALLU-\nGUARD benefits most on small backbones while\nmaintaining consistent advantages across scales.\nTable 3: Performance comparison across backbone scales (small, mid-sized, and large) on three\nbenchmarks: SQuAD, GSM8K, HaluEval. We highlight the first and second best results.\nLlama2-7B\nLlama-3-8B\nLlama2-13B\nLlama2-70B\nAUROCr\nAUPRCr\nAUROCllm\nAUPRCllm\nAUROCr\nAUPRCr\nAUROCllm\nAUPRCllm\nAUROCr\nAUPRCr\nAUROCllm\nAUPRCllm\nAUROCr\nAUPRCr\nAUROCllm\nAUPRCllm\nSQuAD\nHALLUGUARD 81.05 77.16 71.18 64.38\n79.56 78.29 67.97 63.27\n81.45 78.39 64.39 65.07\n83.8 81.77 70.46 73.24\nInside\n73.63 75.74 65.22 59.11\n76.13 72.44 65.62 62.94\n74.68 74.81 61.01 59.51\n81.24 75.09 69.48\n62.4\nMIND\n64.57 61.11 52.39 53.13\n62.29 59.58 44.49 48.61\n68.64 66.95 54.92 52.49\n73.46 71.71 57.76 56.77\nPerplexity\n63.93 61.77 46.97 48.2\n70.51 67.51 55.71 52,68\n70.19 69.22 60.33 54.82\n74.23 70.88 62.24 58.05\nLN-Entropy\n65.96 64.22 53.43 52.84\n63.7\n60.4 46.19 42.85\n61.66 59.16 49.05 46.27\n72.44 68.91 56.77 52.63\nEnergy\n59.83 56.11 46.19 43.18\n64.41 61.02 56.17 46.21\n61.02 59.73 48.26 42.08\n69.01 66.19 58.44 49.82\nSemantic Ent.\n60.29 57.73 43.63 48.83\n66.52 62.62 52.37 52.7\n70.58 67.22 53.31 52.94\n72.01 68.51 56.49\n50.9\nLexical Sim.\n70.31 69.08 53.97 53.31\n66.43 63.56 53.19 50.96\n68.53 67.42 50.73 54.12\n68.95 67.91 60.52 56.56\nSelfCheckGPT\n68.26 67.09 60.06 57.31\n73.99 72.15 65.26 54.02\n65.47 61.65 53.12 49.89\n73.07 70.49 56.59 54.65\nRACE\n71.35 69.23 59.18 54.73\n68.17 66.02 54.65 53.06\n64.19 60.45 47.53 45.66\n64.05 62.39 54.38 50.07\nP(true)\n62.55 61.09 46.84 52.32\n67.42 63.94 55.35 47.52\n71.56 68.4 57.51 45.66\n66.81 62.71 57.43 46.85\nFActScore\n70.32 68.63 58.13 53.01\n71.2 69.45 61.92 54.91\n66.65 63.2 56.41 53.42\n68.33 65.26 56.93 48.46\nGSM8K\nHALLUGUARD 75.89 72.83 62.29 63.46\n75.2\n72.9 63.62 61.79\n79.01 76.73 64.38 64.97\n77.33 73.97 60.48 61.26\nInside\n74.61 68.35 58.57 62.58\n73.73 67.51 56.02 57.28\n75.79 76.26 60.91 59.77\n72.3 72.26 54.49 58.39\nMIND\n65.88 63.4 48.28 48.17\n66.57 65.55 48.84 53.4\n61.49 59.55 51.63 51.45\n66.41 63.44 52.05 53.57\nPerplexity\n66.23 64.1 53.52 52.31\n57.61 53.63 41.37 41.59\n60.96 58.67 46.27 47.44\n64.32 62.81 51.15\n51.3\nLN-Entropy\n59.45 55.95 43.04 44.08\n68.22 66.05 53.03 53.21\n61.31 58.90 45.83 40.86\n61.81 60.46 44.5\n44.76\nEnergy\n58.15 54.71 43.65 36.71\n59.79 56.52 50.31 42.23\n57.58 56.07 43.39 38.94\n65.27 62.94 52.8\n46.6\nSemantic Ent.\n57.95 54.68 42.78 41.95\n66.9 64.81 50.47 55.36\n62.72 59.09 49.33 44.35\n60.63 57.01 46.22 40.24\nLexical Sim.\n65.8\n63.7 52.12 54.07\n63.29 59.87 53.17 50.02\n63.83 60.20 54.43 44.82\n63.27 59.41 47.42 47.38\nSelfCheckGPT\n60.99 57.54 49.28 44.43\n65.72 62.01 54.49 50.34\n57.98 54.58 46.72 39.86\n68.06 65.09 52.99 50.89\nRACE\n63.37 62.33 53.53 49.94\n64.49 61.47 53.28 47.55\n64.20 61.96 50.15 45.35\n68.35 66.66 50.41 51.16\nP(true)\n65.95 63.63 54.95 48.25\n62.59 58.88 47.21 42.2\n67.08 65.60 53.66 55.12\n60.16 58.14 47.73 49.49\nFActScore\n56.69 53.71 45.78 39.52\n65.69 61.95 53.69 46.06\n55.76 54.17 44.91 43.18\n59.84 55.85 44.05 39.49\nHaluEval\nHALLUGUARD 75.72 72.89 66.65 63.15\n73.43 71.19 64.95 54.8\n78.15 74.15 65.39 61.14\n80.79 79.54 67.68 68.51\nInside\n71.33 67.63 59.73 53.15\n67.95 64.93 60.31 52.21\n72.01 71.97 56.51 60.64\n74.62 68.33 62.22\n64.4\nMIND\n54.8 51.43 44.15 43.34\n64.54 60.89 49.09 45.13\n55.05 53.28 39.16 45.17\n57.98 56.01 45.82 41.69\nPerplexity\n54.02 52.53 38.76 40.51\n61.31 59.36 50.62 46.01\n54.99 51.39 42.64 35.64\n62.85 60.59 48.29 43.85\nLN-Entropy\n59.47 58.33 50.2 46.91\n64.89 60.72 51.78 46.39\n65.18 63.53 49.70 48.09\n60.16 58.89 50.29 48.42\nEnergy\n62.29 59.6 50.68 42.24\n62.74 61.61 50.17 52.01\n60.54 59.04 43.53 50.37\n60.13 58.44 48.79 48.01\nSemantic Ent.\n59.39 55.94 48.53 46.35\n55.25 53.05 44.5 44.35\n59.44 57.72 45.38 40.77\n61.57 57.99 49.07 45.39\nLexical Sim.\n63.61 61.16 55.01 44.75\n56.59 55.39 44.45 45.57\n53.46 52.06 41.34 40.57\n64.37 60.92 54.29 50.86\nSelfCheckGPT\n64.29 61.83 48.4 45.49\n65.44 63.13 57.02 48.23\n65.24 63.52 53.71 54.33\n57.12 55.26 40.5\n43.06\nRACE\n59.78 59.14 48.1 40.47\n61.98 60.32 48.08 46.29\n60.65 59.11 49.92 44.51\n62.11 58.24 40.5\n43.06\nP(true)\n57.46 54.8 41.84 40.47\n56.32 54.04 42.55 43.75\n65.77 63.01 49.98 45.47\n55.75 54.94 44.14 43.97\nFActScore\n63.93 61.33 46.9 51.87\n61.73 57.85 49.92 42.15\n65.15 63.71 55.98 54.61\n62.66 60.3 53.13 46.42\nQ3: How does each term capture trends across\ntask families?\nAs shown in Figure 2, each term\nfaithfully tracks the ground-truth trend within its re-\nspective task family. On data-centric SQuAD, the\ndata-driven term closely follows the dashed gold\ncurve across the variant hallucination rate, capturing\nthe smooth AUROC decline. On reasoning-oriented\nMATH-500, the reasoning-driven term mirrors the\nmonotonic AUROC drop as reasoning drift in-\ncreases. These results show that each term is well matched to its task family and faithfully tracks\nperformance trends as hallucination rates rise.\n8\n"}, {"page": 9, "text": "Published as a conference paper at ICLR 2026\n4.3\nTEST-TIME INFERENCE\nTest-time reasoning remains challenging, as models need to generate coherent multi-step solu-\ntions without drifting into errors. To assess whether hallucination detection can mitigate this dif-\nficulty, we integrate detectors into beam search and evaluate Qwen2.5-Math-7B on MATH-500 and\nLlama3.1-8B on Natural. As shown in Table 4, HALLUGUARD achieves the strongest gains: on\nMATH-500, it reaches 81.00% accuracy, around 10% higher than IO Prompt; on Natural, it at-\ntains 70.96%, exceeding IO Prompt by 15.72%. These results demonstrate that HALLUGUARD not\nonly detects hallucinations but also strengthens test-time reasoning by guiding models toward more\nreliable solutions.\nTable 4: Performance of hallucination score-guided test-time inference across reasoning tasks. We\nhighlight the first and second best results.\nDataset\nIO\nPrompt\nOurs\nInside\nMIND\nPerplexity\nLN-\nEntropy\nEnergy\nSemantic\nEnt.\nSelfCheck-\nGPT\nRACE\nP(true)\nFActScore\nMATH-500\n72.70\n81.00\n74.90\n77.10\n77.10\n76.20\n78.00\n72.50\n74.00\n75.10\n67.10\n71.60\nNatural\n55.24\n70.96\n67.42\n68.32\n67.51\n68.04\n68.59\n68.10\n65.68\n66.90\n68.16\n67.74\n4.4\nCASE STUDY\nFine-grained hallucinations—lexically similar yet semantically incorrect outputs—pose a particu-\nlar challenge for detection. To evaluate whether HALLUGUARD can comprehensively capture such\nsubtle errors, we use the PAWS dataset(Zhang et al., 2019), which contrasts paraphrases with high\nsurface overlap but divergent meanings. Following Li et al. (2025), we adopt ROUGE-based refer-\nence signals for evaluation (Table 5). Across model scales, HALLUGUARD consistently surpasses\nbaselines: it achieves 90.18% AUROC and 87.64% AUPRC on Llama2-70B, and 91.24% AUROC\nand 88.53% AUPRC on QwQ-32B—exceeding the next-best method by nearly five points. Even on\nGPT-2, it leads with 83.27% AUROC and 80.46% AUPRC. These results confirm HALLUGUARD’s\neffectiveness in capturing fine-grained semantic inconsistencies beyond benchmark settings.\nTable 5: Results on PAWS measuring semantic hallucination detection with Llama-3.2-3B, Llama2-\n70B, and QwQ-32B. We highlight the first and second best results.\nMethod\nOurs\nInside\nMIND\nPerplexity\nLN-\nEntropy\nEnergy\nSemantic\nEnt.\nLexical\nSim.\nSelfCheck-\nGPT\nRACE\nP(true)\nFActScore\nLlama3.2 AUROC\n85.63\n80.46\n78.93\n71.27\n72.19\n73.05\n75.11\n64.58\n77.82\n79.47\n73.56\n68.44\nAUPRC\n82.14\n77.28\n75.41\n67.55\n68.34\n70.22\n72.41\n59.67\n73.41\n76.28\n70.43\n63.58\nLlama2\nAUROC\n90.18\n85.47\n83.92\n75.68\n76.23\n77.14\n79.06\n68.35\n82.71\n84.26\n77.39\n72.62\nAUPRC\n87.64\n82.38\n81.06\n71.42\n72.59\n74.28\n76.32\n63.44\n78.89\n81.73\n74.18\n67.58\nQwQ\nAUROC\n91.24\n85.41\n84.56\n76.72\n77.43\n78.29\n80.42\n69.54\n83.59\n86.38\n78.53\n73.46\nAUPRC\n88.53\n82.27\n81.37\n72.63\n73.29\n75.44\n77.18\n64.27\n79.42\n83.41\n75.21\n68.32\n5\nRELATED WORK\nIn this section, we review prior hallucination-detection methods by their detection target–Data-\ndriven hallucinations and reasoning-driven hallucinations.\nDetecting Data-Driven Hallucinations. Recent work has shown that internal activations encode\nrich indicators of such flaws. Chen et al. (2024a) proposed EIGENSCORE, which computes statis-\ntics of hidden representations from the eigen matrix to estimate hallucination risk. Su et al. (2024)\nintroduced MIND, an unsupervised detector that models temporal dynamics of hidden states with-\nout requiring labels, along with HELM benchmark to enable standardized evaluation. Azaria &\nMitchell (2023) demonstrated using linear probes on intermediate states to predict truthfulness.\nDetecting Reasoning-Driven Hallucinations. There are other works targeting inference-time in-\nconsistencies during generation—such as logical errors, instability across decoding steps, or tem-\nporal drift in extended outputs. Manakul et al. (2023) proposed SELFCHECKGPT, which assesses\nself-consistency by sampling multiple candidate generations and measuring their alignment using\nentailment and lexical overlap. Kalai & Vempala (2024) introduced a suite of calibration-based un-\ncertainty scores designed to capture hallucination risk directly from output distributions. Ding et al.\n9\n"}, {"page": 10, "text": "Published as a conference paper at ICLR 2026\n(2025) proposed REACTSCORE, which integrates entropy with intermediate reasoning traces to de-\ntect failures in multi-step decision-making. FACTSCORE(Min et al., 2023) decomposes outputs into\natomic factual units and verifies each against retrieved passages using entailment-based scoring.\n6\nCONCLUSION\nThe reliability of LLMs is often undermined by hallucinations, which arise from two main sources:\ndata-driven, caused by flawed knowledge acquired during training, and reasoning-driven, stemming\nfrom inference-time instabilities in multi-step generation. Although these hallucinations frequently\nevolve in practice, existing detectors usually target only one source and lack a solid theoretical foun-\ndation. To address this gap, we propose a unified theoretical framework–a Hallucination Risk Bound,\nwhich formally decomposes hallucination risk into data-driven and reasoning-driven components,\noffering a principled view of how hallucinations emerge and evolve during generation. Building\non this foundation, we introduce HALLUGUARD, a NTK–based score that measures sensitivity\nto semantic perturbations and captures internal instabilities, thereby enabling holistic detection of\nboth data-driven and reasoning-driven hallucinations. We evaluate HALLUGUARD across 10 di-\nverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, where it consistently\nachieves state-of-the-art performance, demonstrating robustness and practical efficacy. Looking\nforward, leveraging HalluGuard’s sensitivity to error propagation offers a promising pathway for\ndeveloping prognostic indicators in interactive multi-turn dialogues, enabling systems to predict and\npreempt hallucinations before they fully manifest.\nREPRODUCIBILITY STATEMENT\nWe have taken several measures to ensure the reproducibility of our work. A complete description\nof the theoretical framework, including the formal assumptions and proofs of the Hallucination Risk\nBound, is provided in Section 3 and Section A. Detailed experimental settings and evaluation pro-\ntocols are documented in Section 4 and Section C.1, covering all 10 benchmarks, 11 baselines, and\n9 LLM backbones. Together, these resources ensure that both our theoretical claims and empirical\nresults can be independently validated and extended by the community.\nETHICS STATEMENT\nThis study is based exclusively on publicly available datasets and open-source large language mod-\nels, and does not involve human subjects or the use of private data. All scientific concepts, method-\nological designs, experimental implementations, and resulting conclusions remain entirely the re-\nsponsibility of the authors.\nREFERENCES\nAmos Azaria and Tom Mitchell. The internal state of an llm knows when it’s lying. In Findings of\nthe Association for Computational Linguistics: EMNLP, 2023.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,\nShyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,\nJared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Ste-\nfano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren\nGillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Pe-\nter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard,\nSaahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte\nKhani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya\nKumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li,\nXuechen Li, Ece Kamar, Michal Kosinski, Ryan Chi-Ying Hsieh, Drew A. Linsley, Long O. Mai,\nNikolay Manchev, Christopher D. Manning, Yian Yin, Christopher J. N. de M. L. Matthews, Lu-\ncia Mondragon, Ognjen Oreskovic, Mark Sabini, Yusuf Sahin, Clark Barrett, Christopher Potts,\n10\n"}, {"page": 11, "text": "Published as a conference paper at ICLR 2026\nJames Y. Zou, Jiajun Wu, and Percy Liang. On the opportunities and risks of foundation models,\n2021.\nJean C´ea. Approximation variationnelle des probl`emes aux limites. In Annales de l’institut Fourier,\nvolume 14, pp. 345–444, 1964.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. In-\nside: Llms’ internal states retain the power of hallucination detection, 2024a. URL https:\n//arxiv.org/abs/2402.03744.\nYuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu\nLi, and Yanghua Xiao. Hallucination detection: Robustly discerning reliable answers in large\nlanguage models, 2024b. URL https://arxiv.org/abs/2407.04121.\nLenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming,\n2020. URL https://arxiv.org/abs/1812.07956.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nFabio Dennst¨adt, Janna Hastings, Paul Martin Putora, Max Schmerder, and Nikola Cihoric. Im-\nplementing large language models in healthcare while balancing control, collaboration, costs and\nsecurity. NPJ digital medicine, 8(1):143, 2025.\nYue Ding, Xiaofang Zhu, Tianze Xia, Junfei Wu, Xinlong Chen, Qiang Liu, and Liang Wang.\nD2hscore: Reasoning-aware hallucination detection via semantic breadth and depth analysis in\nllms, 2025. URL https://arxiv.org/abs/2509.11569.\nStuart Geman, Elie Bienenstock, and Ren´e Doursat. Neural networks and the bias/variance dilemma.\nNeural computation, 4(1):1–58, 1992.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan,\nAnirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Ko-\nrenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava\nSpataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret,\nChunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius,\nDaniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary,\nDhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab\nAlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco\nGuzm´an, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That-\ntai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Kore-\nvaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra,\nIvan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Ma-\nhadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jong-\nsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,\nKarthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid\nEl-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren\nRantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin,\nLovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi,\nMahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew\nOldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Ku-\nmar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoy-\nchev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur C¸ elebi, Patrick Alrassy, Pengchuan\nZhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan,\nPunit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ra-\nmon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Ro-\nhit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\n11\n"}, {"page": 12, "text": "Published as a conference paper at ICLR 2026\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell,\nSeohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng\nShen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer\nWhitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman,\nTara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mi-\nhaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor\nKerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V´ıtor Albiero, Vladan Petrovic, Weiwei\nChu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang\nWang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-\nschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning\nMao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh,\nAayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria,\nAhuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein,\nAmanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, An-\ndrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, An-\nnie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\nAshwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leon-\nhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu\nNi, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Mon-\ntalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao\nZhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia\nGao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide\nTestuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le,\nDustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily\nHahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smoth-\ners, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni,\nFrank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia\nSwee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan,\nHakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harri-\nson Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj,\nIgor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James\nGeboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jen-\nnifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang,\nJoe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Jun-\njie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy\nMatosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang,\nKunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell,\nLei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa,\nManav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias\nReso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L.\nSeltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike\nClark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari,\nMunish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan\nSinghal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong,\nNorman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent,\nParth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar,\nPolina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Ro-\ndriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy,\nRaymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin\nMehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon,\nSasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ra-\nmaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha,\nShishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal,\nSoji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satter-\nfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj\nSubramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo\nKoehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook\nShaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Ku-\nmar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov,\n12\n"}, {"page": 13, "text": "Published as a conference paper at ICLR 2026\nWei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiao-\njian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia,\nYe Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao,\nYuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhao-\nduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL\nhttps://arxiv.org/abs/2407.21783.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021.\nURL https://arxiv.org/abs/2103.03874.\nLei Huang, Weijiang Yu, Weitao Wang, Yujia Wang, Shi-Qi Chen, and Ju-Hua Wang. A survey\non hallucination in large language models: Principles, taxonomy, challenges, and open questions,\n2023.\nArthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-\neralization in neural networks, 2020. URL https://arxiv.org/abs/1806.07572.\nDenis Janiak, Jakub Binkowski, Albert Sawczyn, Bogdan Gabrys, Ravid Shwartz-Ziv, and Tomasz\nKajdanowicz. The illusion of progress: Re-evaluating hallucination detection in llms, 2025.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\nComputing Surveys, 55(12):1–38, March 2023. ISSN 1557-7341. doi: 10.1145/3571730. URL\nhttp://dx.doi.org/10.1145/3571730.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-\nlot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\nL´elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timoth´ee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https:\n//arxiv.org/abs/2310.06825.\nPeizhong Ju, Xiaojun Lin, and Ness B. Shroff. On the generalization power of the overfitted three-\nlayer neural tangent kernel model, 2022. URL https://arxiv.org/abs/2206.02047.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez,\nNicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer\nEl-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bow-\nman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna\nKravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom\nBrown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Ka-\nplan. Language models (mostly) know what they know, 2022.\nAdam Tauman Kalai and Santosh S. Vempala. Calibrated language models must hallucinate, 2024.\nURL https://arxiv.org/abs/2311.14648.\nMarkus Kattnig, Alessa Angerschmid, Thomas Reichel, and Roman Kern. Assessing trustworthy ai:\nTechnical and legal perspectives of fairness in ai. Computer Law & Security Review, 55:106053,\n2024.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for\nuncertainty estimation in natural language generation, 2023.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural questions: A benchmark for question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl a 00276. URL\nhttps://aclanthology.org/Q19-1026/.\nJaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,\nand Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study, 2020a.\nURL https://arxiv.org/abs/2007.15801.\n13\n"}, {"page": 14, "text": "Published as a conference paper at ICLR 2026\nJaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\nDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models\nunder gradient descent *. Journal of Statistical Mechanics: Theory and Experiment, 2020(12):\n124002, December 2020b. ISSN 1742-5468. doi: 10.1088/1742-5468/abc62b. URL http:\n//dx.doi.org/10.1088/1742-5468/abc62b.\nJiawei Li, Akshayaa Magesh, and Venugopal V. Veeravalli. Principled detection of hallucinations in\nlarge language models via multiple testing, 2025. URL https://arxiv.org/abs/2508.\n18473.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-\nscale hallucination evaluation benchmark for large language models, 2023. URL https://\narxiv.org/abs/2305.11747.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74–81, 2004.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods, 2022a. URL https://arxiv.org/abs/2109.07958.\nZi Lin, Jeremiah Zhe Liu, and Jingbo Shang. Towards collaborative neural-symbolic graph se-\nmantic parsing via uncertainty. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio\n(eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 4160–4173,\nDublin, Ireland, May 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nfindings-acl.328. URL https://aclanthology.org/2022.findings-acl.328/.\nChengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou,\nand Sheng Liu. More thinking, less seeing? assessing amplified hallucination in multimodal\nreasoning models, 2025. URL https://arxiv.org/abs/2505.21523.\nWeitang Liu, Xiaoyun Wang, John D. Owens, and Yixuan Li. Energy-based out-of-distribution\ndetection, 2020.\nAndrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction,\n2020.\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales. Selfcheckgpt: Zero-resource black-box\nhallucination detection for generative large language models, 2023. URL https://arxiv.\norg/abs/2303.08896.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of fac-\ntual precision in long form text generation, 2023. URL https://arxiv.org/abs/2305.\n14251.\nCheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Randy Zhong, Juntong Song, and\nTong Zhang. Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented\nlanguage models, 2024. URL https://arxiv.org/abs/2401.00396.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text, 2016. URL https://arxiv.org/abs/1606.05250.\nJie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan,\nand Peter J. Liu. Out-of-distribution detection and selective generation for conditional language\nmodels, 2022.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation\nreduces hallucination in conversation. In EMNLP, 2021.\n14\n"}, {"page": 15, "text": "Published as a conference paper at ICLR 2026\nWeihang Su, Changyue Wang, Qingyao Ai, Yiran HU, Zhijing Wu, Yujia Zhou, and Yiqun Liu. Un-\nsupervised real-time hallucination detection based on the internal states of large language models,\n2024. URL https://arxiv.org/abs/2403.06448.\nZhongxiang Sun, Qipeng Wang, Haoyu Wang, Xiao Zhang, and Jun Xu. Detection and mitigation\nof hallucination in large reasoning models: A mechanistic perspective, 2025. URL https:\n//arxiv.org/abs/2505.12886.\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-\nbench tasks and whether chain-of-thought can solve them, 2022. URL https://arxiv.org/\nabs/2210.09261.\nArun James Thirunavukarasu, Darren Shu Jeng Ting, Kavya Elangovan, Lio Gutierrez, Teng Fong\nTan, and Daniel Shu Wei Ting. Large language models in medicine. Nature Medicine, 29(8):\n1930–1940, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023. URL https://arxiv.org/abs/2307.09288.\nLloyd N Trefethen and David Bau. Numerical linear algebra. SIAM, 2022.\nRoman Vershynin. High-dimensional probability: An introduction with applications in data science,\nvolume 47. Cambridge university press, 2018.\nChangyue Wang, Weihang Su, Qingyao Ai, and Yiqun Liu. Joint evaluation of answer and reasoning\nconsistency for hallucination detection in large reasoning models, 2025.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, An-\njana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\nKirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Par-\nmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri,\nRushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta\nPatro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi,\nand Daniel Khashabi. Super-naturalinstructions: Generalization via declarative instructions on\n1600+ nlp tasks, 2022. URL https://arxiv.org/abs/2204.07705.\nZeyu Wei, Shuo Wang, Xiaohui Rong, Xuemin Liu, and He Li. Shadows in the attention: Contextual\nperturbation and representation drift in the dynamics of hallucination in llms, 2025. URL https:\n//arxiv.org/abs/2505.16894.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\nJialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jin-\ngren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin\nYang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao,\nRunji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wen-\nbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng\nRen, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,\nZeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL\nhttps://arxiv.org/abs/2407.10671.\n15\n"}, {"page": 16, "text": "Published as a conference paper at ICLR 2026\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering, 2018. URL https://arxiv.org/abs/1809.09600.\nXinyue Zeng, Haohui Wang, Junhong Lin, Jun Wu, Tyler Cody, and Dawei Zhou. Lensllm: Unveil-\ning fine-tuning dynamics for llm selection. ICML, 2025. arXiv preprint arXiv:2505.03793.\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. How language model\nhallucinations can snowball, 2023.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer,\nKurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettle-\nmoyer. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv.\norg/abs/2205.01068.\nYuan Zhang, Jason Baldridge, and Luheng He. Paws: Paraphrase adversaries from word scrambling,\n2019. URL https://arxiv.org/abs/1904.01130.\nZhenliang Zhang, Xinyu Hu, Huixuan Zhang, Junzhe Zhang, and Xiaojun Wan. Icr probe: Track-\ning hidden state dynamics for reliable hallucination detection in llms, 2025.\nURL https:\n//arxiv.org/abs/2507.16488.\nWeihong Zhong, Xiaocheng Feng, Liang Zhao, Qiming Li, Lei Huang, Yuxuan Gu, Weitao Ma,\nYuan Xu, and Bing Qin. Investigating and mitigating the multimodal hallucination snowballing\nin large vision-language models, 2024.\n16\n"}, {"page": 17, "text": "Published as a conference paper at ICLR 2026\nA\nPROOF OF HALLUCINATION RISK BOUND\nA.1\nASSUMPTIONS VALIDATION\nWe provide theoretical and practical justification for the assumptions adopted in Section 3.2, which\nserve to ensure the well-posedness and interpretability of the proposed Hallucination Risk Bound.\nThese assumptions follow standard practice in NTK-based analyses and stability theory, and are\nconsistent with the empirical behavior observed in modern large language models.\nAssumption A1 (Hilbert/RKHS structure with bounded second moment).\nThis assumption\naligns with the classical Neural Tangent Kernel (NTK) approximation regime, where the model’s\nfeature mapping is embedded in a reproducing kernel Hilbert space (RKHS) and the induced kernel\nadmits a well-defined second moment. Such conditions are fundamental to the convergence and\ngeneralization analyses of infinitely wide neural networks, and are widely adopted in NTK theory\n(Jacot et al., 2020). In practice, bounded second-moment behavior is consistent with the hidden-\nstate distributions observed across all evaluated LLMs, as reflected by stable activation statistics and\nNTK spectral profiles(Lee et al., 2020b).\nAssumption A2 (Local Lipschitz continuity of the encoder Φ).\nThis assumption reflects stan-\ndard smoothness conditions in high-dimensional learning theory, ensuring that small perturbations\nin the input space induce controlled deviations in the encoded representation (Vershynin, 2018).\nSuch local Lipschitz behavior is commonly invoked to guarantee stability under perturbations and\nis consistent with theoretical analyses of deep representations.\nAssumption A3 (Local smoothness / second-order expansion).\nThis assumption corresponds to\nthe classical NTK linearization framework, which approximates the behavior of wide neural net-\nworks through a local second-order expansion around a set of reference points (Lee et al., 2020a;\nChizat et al., 2020). Importantly, our formulation requires this condition only locally around the\nK sampled trajectories used by HalluGuard, rather than globally across the entire model parame-\nter space. This localized validity preserves theoretical soundness while avoiding unrealistic global\nsmoothness requirements that are known to be overly restrictive in large-scale models.\nA.2\nBOUND PROOF\nWe restate the main inequality from Section 3.2:\n∥u∗−uh∥≤\n\u0014\n1 + kpt log O(P, L) + k ϵmismatch\nSignalk\n\u0015\ninf\nu∈Uh ∥u∗−u∥+|L| exp\n\u0012\n−Kϵ2\nC\n\u0013\nα\n\u0000eβT −1\n\u0001\n.\n(8)\nStep 1: Triangle inequality split.\nWe define the hallucination decomposition by writing:\n∥u∗−uh∥= ∥u∗−E[uh] + E[uh] −uh∥≤∥u∗−E[uh]∥+ ∥uh −E[uh]∥.\nWe denote the first term as the deterministic approximation error (bias) and the second term as the\nstochastic residual (variance).\nStep 2: Approximation term via C´ea’s lemma.\nAssume E[uh] is the Galerkin projection of u∗\nin a coercive bilinear form a(·, ·), i.e., for all v ∈Uh,\na(E[uh], v) = ℓ(v).\nThen, by C´ea’s lemma, we have:\n∥u∗−E[uh]∥≤Λ\nγ\ninf\nu∈Uh ∥u∗−u∥,\nwhere Λ and γ are continuity and coercivity constants of a(·, ·), respectively.\n17\n"}, {"page": 18, "text": "Published as a conference paper at ICLR 2026\nStep 3: Variance term via Bernstein concentration.\nLet ℓh :=\n1\n|L|\nP|L|\ni=1 ℓi be the empirical\nsupervision functional from finite labeled chains. Define the fluctuation:\n∆ℓ:= ℓh −ℓ,\nand the residual:\nr := uh −E[uh],\nso that\nAhr = ∆ℓ.\nApplying operator norm bounds and covering number uniformization (cf. Vershynin, 2018), we have\nwith high probability:\n∥r∥≤|L| exp\n\u0012\n−Kϵ2\nC\n\u0013\nα(eβT −1),\nwhich completes the proof.\nStep 4: Substitution.\nCombining both terms yields:\n∥u∗−uh∥≤Λ\nγ\ninf\nu∈Uh ∥u∗−u∥+ |L| exp\n\u0012\n−Kϵ2\nC\n\u0013\nα(eβT −1).\nWe now bound Λ/γ via NTK decomposition.\nA.3\nDECOMPOSITION OF NTK CONTINUITY CONSTANT\nLet a(·, ·) denote the bilinear form induced by the NTK in the finite-width regime. We decompose:\na = a0 + δpt + δmm,\nwhere a0 is the infinite-width baseline kernel, δpt is the perturbation due to pre-training noise, and\nδmm is the domain mismatch from fine-tuning. The continuity constant satisfies:\nΛ = Λ0 + ∆pt + ∆mm.\nBounding ∆pt.\nFollowing Jacot et al. (2020), we apply matrix concentration to finite-width NTK:\n∆pt ≤γkpt log O(P, L).\nBounding ∆mm.\nUsing spectral generalization bounds under data distribution shift (Lee et al.,\n2020b), we have:\n∆mm ≤γk ϵmismatch\nSignalk\n.\nSubstituting both into the bound for Λ/γ, we get:\nΛ\nγ ≤1 + kpt log O(P, L) + k ϵmismatch\nSignalk\n.\nB\nHALLUGUARD DERIVATION AND INTERPRETATION\nB.1\nPRELIMINARIES AND NOTATION\nLet K ∈Rr×r be the NTK Gram matrix formed on r light semantic perturbations (see Assumptions\nA1–A4 in the main theory section). Denote its eigen decomposition by K = V ΛV ⊤with\nΛ = diag(λ1, . . . , λr),\nλ1 ≥· · · ≥λr > 0.\nLet λmax := λ1, λmin := λr, κ(K) := λmax/λmin, and det(K) = Qr\ni=1 λi. Let Φ denote the NTK\nfeature matrix whose columns span the hypothesis subspace Uh, so that K = Φ⊤Φ, ∥Φ∥2 = √λmax,\nand σmin(Φ) = √λmin. For the autoregressive decoder, let Jt be the step-t input–output Jacobian,\nand write σmax := supt ∥Jt∥2.\nWe will use the following two standard inequalities repeatedly:\nMaclaurin/AM −−GMoneigenvalues :\n\u0010\nr\nY\ni=1\nλi\n\u00111/r\n≤1\nr\nr\nX\ni=1\nλi = tr(K)\nr\n,\n(9)\nSubmultiplicativity :\n∥AB∥2 ≤∥A∥2 ∥B∥2.\n(10)\n18\n"}, {"page": 19, "text": "Published as a conference paper at ICLR 2026\nB.2\nREPRESENTATIONAL ADEQUACY VIA det(K) WITH EXPLICIT CONSTANTS\nAssumptions for this subsection.\nBeyond A1–A3, we assume a mild source condition and a\nspectral envelope:\nS1 (Source condition) There exist s > 0 and Rs > 0 such that u∗∈Range(Λs), i.e.,\nPr\ni=1\n⟨u∗,vi⟩2\nλ2s\ni\n≤R2\ns. This is standard in kernel approximation and encodes RKHS reg-\nularity.\nS2 (Spectral envelope) There exist constants 0 < λ ≤λ < ∞and α > 1 such that λi ≤λ for\nall i and λr ≥λ r−α. (Polynomial decay is a common stylization; other envelopes can be\ntreated similarly.)\nLemma B.1 (Best-approximation error under source condition). Let Uh = span{v1, . . . , vr}. Un-\nder S1,\ninf\nu∈Uh ∥u∗−u∥= ∥u∗−ΠUhu∗∥≤Rs λ s\nr+1,\nwhere λr+1 denotes the next-eigenvalue of the infinite-dimensional kernel operator (or, equivalently,\nthe empirical tail eigenvalue if more perturbations are added).\nProof. Write u∗= P\ni≥1 civi with ci = ⟨u∗, vi⟩. Then ∥u∗−ΠUhu∗∥2 = P\ni>r c2\ni ≤P\ni>r λ2s\ni ·\nc2\ni\nλ2s\ni\n≤λ2s\nr+1\nP\ni>r\nc2\ni\nλ2s\ni\n≤λ2s\nr+1R2\ns.\nTo connect λr+1 (or λr) to det(K), we need an explicit lower bound of the form λr ≥c det(K) θ\nwith constants (c, θ) depending on the spectral envelope. The following inequality suffices.\nLemma B.2 (Lower-bounding λr by det(K)). Suppose λi ≤λ for all i and λr > 0. Then\nλr ≥det(K)\nλ\nr−1\nand\nλ s\nr ≥det(K) s\nλ\ns(r−1) .\nProof. Since det(K) = Qr\ni=1 λi ≤λ\nr−1λr, we obtain λr ≥det(K)/λ\nr−1. Raising to power s\nyields the second inequality.\nTheorem B.3 (Determinant-based adequacy bound with explicit constants). Under A1–A3 and S1–\nS2,\ninf\nu∈Uh ∥u∗−u∥≤Cd det(K)−cd ∥u∗∥,\nwith\ncd =\ns\nr −1\nand\nCd = λ\ns Rs\n∥u∗∥.\nMoreover, if the empirical spectrum satisfies λr ≥λ r−α, one may choose\ncd = min\n\n\n\ns\nr −1 , s\nα ·\n1\nlog\n\u0000λ\nr\ndet(K)\n\u0001\n\n\n,\nwhich improves with slower decay (smaller α).\nProof. By Lemma B.1 with λr+1 ≤λr, infu∈Uh ∥u∗−u∥≤Rs λ s\nr . Lemma B.2 gives λ s\nr ≥\ndet(K) s/λ\ns(r−1); rearranging,\ninf\nu∈Uh ∥u∗−u∥≤Rs λ\ns(r−1) det(K)−s.\nRescale constants relative to ∥u∗∥by setting Cd := λ\ns (Rs/∥u∗∥) and cd := s/(r −1) to obtain\nthe stated form:\ninf\nu∈Uh ∥u∗−u∥≤\n\u0000λ\ns\nRs\n∥u∗∥\n\u0001\ndet(K)−s/(r−1) ∥u∗∥.\nThe variant using the envelope λr ≥λ r−α is obtained by combining det(K) ≤λ\nr−1λr with the\nexplicit lower bound on λr, yielding the alternative exponent shown.\n19\n"}, {"page": 20, "text": "Published as a conference paper at ICLR 2026\nNumerical note (stable surrogate).\nIn practice we use log det(K) via Cholesky and aggregate\nwith z-normalization across components to avoid scale domination by any single term.\nB.3\nROLLOUT AMPLIFICATION VIA JACOBIAN PRODUCTS (EXACT CONSTANTS)\nTheorem B.4 (Amplification bound with exact constant). Let Jt be the step-t Jacobian and σmax :=\nsupt ∥Jt∥2. Then\n\r\r\r\nT\nY\nt=1\nJt\n\r\r\r\n2 ≤\nT\nY\nt=1\n∥Jt∥2 ≤σ T\nmax.\nDefining β := log σmax gives eβT = σT\nmax, hence\neβT ≤σT\nmax,\nwith equality if and only if ∥Jt∥2 = σmax for all t and the top singular directions align across\nfactors.\nProof. The first inequality is equation 10 applied iteratively. The second is by definition of σmax.\nSetting β = log σmax yields equality in the worst case. Alignment of top singular vectors is the\ntightness condition for submultiplicativity.\nToken-dependent refinement.\nIf one defines σt := ∥Jt∥2 and βavg :=\n1\nT\nPT\nt=1 log σt, then\n\r\r QT\nt=1 Jt\n\r\r\n2 ≤exp\n\u0000 P\nt log σt\n\u0001\n= eβavgT , which is tighter but requires per-step measurements.\nB.4\nCONDITIONING-INDUCED VARIANCE WITH κ(K)2 SCALING\nWe now give an explicit projector-perturbation derivation showing the quadratic dependence on the\ncondition number.\nSetup.\nLet P := Φ(Φ⊤Φ)†Φ⊤be the orthogonal projector onto Uh; then the linearized output is\nuh = Pu∗. Consider a feature perturbation ∆Φ induced by a prefix perturbation δ satisfying\n∥∆Φ∥2 ≤LΦ ∥δ∥\n(A2/A3).\nLet the perturbed projector be eP := (Φ + ∆Φ)\n\u0000(Φ + ∆Φ)⊤(Φ + ∆Φ)\n\u0001†(Φ + ∆Φ)⊤and define\n∆P := eP −P.\nLemma B.5 (Projector perturbation bound). There exists an absolute constant CΠ > 0 such that\n∥∆P∥2 ≤CΠ\n∥Φ∥2\nσmin(Φ)2 ∥∆Φ∥2 = CΠ\n√λmax\nλmin\n∥∆Φ∥2 = CΠ κ(K) ∥∆Φ∥2\n√λmin\n.\nProof idea. Use standard bounds for the perturbation of orthogonal projectors onto column spaces\n(e.g., Wedin’s sinΘ theorem and Stewart–Sun, Matrix Perturbation Theory, Thm 3.6). One shows\n∥∆P∥2 ≤2 ∥(Φ⊤Φ)†∥2 ∥Φ⊤∆Φ∥2 + O(∥∆Φ∥2\n2).\nSince ∥(Φ⊤Φ)†∥2 = 1/λmin and ∥Φ⊤∆Φ∥2 ≤∥Φ∥2 ∥∆Φ∥2 = √λmax∥∆Φ∥2, the result follows\nfor sufficiently small ∥∆Φ∥2, absorbing lower-order terms into CΠ.\nTheorem B.6 (Variance amplification with explicit constant). Let uh(Φ) = Pu∗and uh(Φ+∆Φ) =\nePu∗. Then\n∥uh(Φ + ∆Φ) −uh(Φ)∥≤CΠ κ(K) ∥∆Φ∥2\n√λmin\n∥u∗∥.\nIf ∆Φ is induced by a random prefix perturbation δ with ∥∆Φ∥2 ≤LΦ∥δ∥and E∥δ∥2 = σ2\nδ, then\nVar[uh] ≤E∥uh(Φ + ∆Φ) −uh(Φ)∥2 ≤cv κ(K)2 ∥δ∥2,\nwith\ncv = C2\nΠ\nL2\nΦ ∥u∗∥2\nλmin\n.\n20\n"}, {"page": 21, "text": "Published as a conference paper at ICLR 2026\nProof. By Lemma B.5,\n∥uh(Φ + ∆Φ) −uh(Φ)∥\n=\n∥∆P u∗∥\n≤\n∥∆P∥2∥u∗∥\n≤\nCΠ κ(K) ∥∆Φ∥2\n√λmin ∥u∗∥. Square both sides and take expectation over δ, using ∥∆Φ∥2 ≤LΦ∥δ∥,\nto obtain the stated variance bound with the explicit constant cv.\nInterpretation.\nThe κ(K)2 factor arises from two sources: (i) κ(K) from the projector sensitivity\n(Lemma B.5), and (ii) 1/λmin from converting ∥∆P∥2 to a mean-squared bound after squaring and\naveraging, yielding an overall κ2-scaling in the variance constant.\nB.5\nCONSOLIDATION: COMPACT SURROGATE CONSISTENT WITH THE RISK\nDECOMPOSITION\nCombining Theorem B.3, Theorem B.4, and Theorem B.6, we obtain a computable surrogate aligned\nwith the Hallucination Risk Bound:\nAdequacy: det(K)\nAmplification: log σmax\nConditioning penalty: −log κ(K)2.\nThis motivates the score\nHALLUGUARD(uh) = det(K) + log σmax −log κ(K)2\nwith the following explicit, implementation-ready notes:\n• Use log det(K) via Cholesky for stability; replace det in the score with log det if desired\n(monotone equivalent).\n• Estimate σmax either as supt ∥Jt∥2 or its tighter average form βavg =\n1\nT\nP\nt log ∥Jt∥2\n(then use βavg in place of log σmax).\n• z-normalize each component across a validation set before summation to avoid scale dom-\ninance; optionally fit task-specific weights if permitted.\nC\nEXPERIMENT\nC.1\nSETUP\nImplementation\nFramework.\nAll\nexperiments\nuse\nPyTorch\nand\nHuggingFace\nTransformers with a fixed random seed for reproducibility.\nUnless otherwise noted,\ncomputations run in mixed precision (fp16). Hardware details (A100/H200) are reported once in\nthe main setup section.\nGeneration Configuration.\nFor default evaluation of detectors, we use nucleus sampling with\ntemperature = 0.5, top-p = 0.95, and top-k = 10, decoding K=10 candidate responses\nper input (unless otherwise specified). These decoding trajectories also operationalize semantic per-\nturbations as natural variations within the model’s local predictive distribution, thereby instantiating\na semantically proximate neighborhood around the primary response and capturing the local geom-\netry of the reasoning manifold required for NTK construction. For score-guided test-time inference\n(Section 4.3), we use beam search (beam size = 10) and score candidate trajectories at each step\nwith the chosen detector. For stability analysis, HALLUGUARD extracts sentence representations\nfrom the final token at the middle transformer layer (L/2), which empirically preserves semantics\nrelevant to truthfulness.\nNTK-Based Score Computation.\nFor each set of generations, we form a task-specific NTK fea-\nture matrix and compute the semantic stability score from its eigenspectrum. We add a small ridge\nα = 10−3 for numerical stability and compute singular values via SVD.\nPerturbation Regularization.\nTo prevent pathological activations that amplify instability, HAL-\nLUGUARD clips hidden features using an adaptive scheme. We maintain a memory bank of N=3000\ntoken embeddings and set thresholds at the top and bottom 0.2% percentiles of neuron activations;\nout-of-range values are truncated to attenuate overconfident hallucinations.\n21\n"}, {"page": 22, "text": "Published as a conference paper at ICLR 2026\nOptimization.\nBackbone language models are not fine-tuned. We train only HALLUGUARD’s\nlightweight projection layers using AdamW with learning rate selected from {1 × 10−5, 5 ×\n10−5, 1 × 10−4} and weight decay from {0.0, 0.01}. The best setting is chosen on a held-out\nvalidation split.\nImplementation Details.\nFor score-guided inference we apply beam search with beam size 10,\nrescoring candidates stepwise with different hallucination detectors.\nAblation Setup.\nAll ablations reuse the main paper’s splits, prompts, and decoding; we vary only\nHALLUGUARD internals and explicitly control the hallucination base rate. On the generation side,\nwe modulate prevalence by adjusting temperature/top-p and beam size; to stress the two families,\nwe increase the prefix perturbation budget ρ and rollout horizon T to amplify reasoning drift, and\n(when applicable) toggle retrieval masking to induce data-driven errors. On the detection side, AU-\nROC/AUPRC are threshold-free; when a fixed operating point is needed, we set a decision threshold\nτ on the validation set by (i) matching a target predicted-positive rate πtarget via score quantiles or\n(ii) fixing a desired FPR (e.g., 1%, 5%, 10%); a cost-sensitive Bayes rule τ =\ncFN\ncFP + cFN\n· 1 −π\nπ\nis\noptional when misclassification costs are specified. Unless noted, we toggle one factor at a time and\nsweep ρ ∈{0.75, 1.0, 1.5}, T ∈{12, 16, 24}, and the number of semantic probes m ∈{2, 4, 8};\nno additional training is performed beyond optional temperature/z-score calibration on the training\nsplit. We report mean±std over 5 seeds.\nC.2\nABLATION STUDY ON −log κ2\nTo empirically validate the necessity of the stability term −log κ2, we performed a controlled ab-\nlation on MATH-500. We systematized the reasoning drift (d) by progressively increasing the per-\nturbation budget ρ and rollout horizon T. As shown in Figure 3, the absence of this term leads to\nsevere instability. While the ablated model (orange dashed line) performs competitively in low-drift\nregimes (d < 0.15), it exhibits significant performance volatility as the reasoning task becomes more\ncomplex. In contrast, the full HALLUGUARD score (green solid line) effectively penalizes these ill-\nconditioned regimes, maintaining a smooth and robust detection profile. This confirms that −log κ2\nfunctions as an essential spectral regularizer, preventing the score from becoming unreliable under\nhigh-entropy inference states.\nFigure 3: Ablation study of the stability term (−log κ2) on MATH500.\n22\n"}, {"page": 23, "text": "Published as a conference paper at ICLR 2026\nC.3\nCOMPUTATIONAL EFFICIENCY ANALYSIS\nTo assess practical deployment feasibility, we measured inference latency on an NVIDIA\nA100/H200 GPU. Our setup utilizes batched parallel sampling to generate K = 10 trajecto-\nries, ensuring sub-linear scaling of the computational cost.\nThe core HALLUGUARD opera-\ntions—specifically feature clipping and computing the NTK score via the Gram matrix—add mini-\nmal latency, requiring less than 1 ms of post-processing time per query.\nFigure 4: Per-Question Inference Time (Seconds) on BBH Across Hallucination Detection Methods.\nFigure 5: Per-Question Inference Time (Seconds) on HaluEval Across Hallucination Detection\nMethods.\nFigure 6: Per-Question Inference Time (Seconds) on Math500 Across Hallucination Detection\nMethods.\n23\n"}, {"page": 24, "text": "Published as a conference paper at ICLR 2026\nFigure 7: Per-Question Inference Time (Seconds) on RAGTruth Across Hallucination Detection\nMethods.\nFigure 8: Per-Question Inference Time (Seconds) on SQuaD Across Hallucination Detection Meth-\nods.\nFigure 9: Per-Question Inference Time (Seconds) on TruthfulQA Across Hallucination Detection\nMethods.\nC.4\nDETECTION PERFORMANCE ANALYSIS\nAcross all five model families and three benchmark regimes, HALLUGUARD consistently achieves\nstate-of-the-art detection performance, particularly in the safety-critical low-FPR regions as shown\nin Table 6.\nWe additionally expanded our evaluation to include SAPLMA, LLM-Check, and ITI. As shown\nin Table 7, HALLUGUARD delivers the strongest performance not only on AUROC/AUPRC but\nalso on deployment-critical, low-FPR operating points, including F1 and TPR at 5% and 10% FPR.\n24\n"}, {"page": 25, "text": "Published as a conference paper at ICLR 2026\nTable 6:\nPerformance comparison on representative benchmarks:\ndata-centric (RAGTruth),\nreasoning-oriented (BBH), and instruction-following (TruthfulQA).\nGPT2\nOPT-6.7B\nMistral-7B\nQwQ-32B\nLLaMA2-13B\nF1\nTPR@10%\nTPR@5%\nF1\nTPR@10%\nTPR@5%\nF1\nTPR@10%\nTPR@5%\nF1\nTPR@10%\nTPR@5%\nF1\nTPR@10%\nTPR@5%\nRAGTruth\nHALLUGUARD\n81.22 74.86 61.41\n77.03 73.52 59.12\n83.19 79.44 69.21\n85.91 80.13 63.52\n74.66 68.91 57.42\nInside\n66.12 59.72 48.31\n72.91 70.25 60.37\n70.45 68.12 52.41\n79.03 74.66 61.09\n73.08 70.11 55.26\nMIND\n58.33 54.11 38.72\n62.55 57.81 47.65\n71.91 66.74 54.39\n64.02 59.12 45.63\n68.55 63.50 48.78\nPerplexity\n55.42 51.20 40.51\n63.72 60.13 49.14\n69.74 66.51 52.18\n70.42 65.41 55.32\n60.18 57.01 44.75\nLN-Entropy\n62.17 57.52 46.44\n58.33 52.99 43.28\n65.30 61.27 49.92\n67.15 62.42 51.33\n63.28 59.07 46.14\nEnergy\n59.71 56.23 44.81\n60.44 57.18 45.03\n63.54 59.42 48.62\n72.09 68.15 58.42\n66.10 61.33 49.41\nSemantic Ent.\n57.28 53.42 41.92\n69.61 64.81 52.01\n67.10 62.44 50.66\n66.12 62.15 49.31\n64.55 60.18 47.75\nLexical Sim.\n61.41 57.09 45.03\n65.81 61.44 49.51\n62.50 59.12 50.92\n70.91 67.53 55.21\n66.29 59.88 51.03\nSelfCheckGPT\n56.22 52.84 40.63\n60.79 55.68 45.72\n63.12 59.47 48.33\n66.54 62.92 51.41\n68.21 65.12 53.60\nRACE\n60.12 56.50 44.90\n64.12 59.77 49.22\n65.44 61.55 52.73\n69.61 66.31 53.92\n62.55 59.42 45.66\nP(true)\n58.91 55.47 42.13\n67.44 63.20 51.43\n71.22 66.91 54.10\n63.44 60.33 49.27\n70.18 65.77 52.78\nFActScore\n62.10 58.21 46.33\n59.22 54.14 44.32\n63.87 60.77 47.98\n68.33 64.02 53.41\n65.92 61.37 49.84\nBBH\nHALLUGUARD\n78.33 74.11 65.42\n74.91 69.14 62.10\n80.22 76.88 68.21\n82.55 78.91 70.45\n79.10 74.25 67.92\nInside\n65.41 61.22 52.83\n71.02 67.10 60.21\n68.17 64.75 53.92\n79.17 72.33 64.22\n67.10 63.52 55.91\nMIND\n54.12 50.22 40.11\n57.21 53.44 41.52\n63.92 59.88 47.01\n61.55 57.14 48.83\n65.11 60.22 49.52\nPerplexity\n52.91 49.33 40.44\n61.88 58.12 49.22\n62.91 59.42 50.11\n59.91 55.72 49.03\n60.88 57.41 48.62\nLN-Entropy\n59.12 55.44 44.92\n54.61 51.75 43.18\n66.44 63.21 54.09\n62.75 59.12 47.52\n68.20 64.88 55.41\nEnergy\n53.94 51.22 45.03\n56.12 52.14 44.61\n64.55 60.11 49.99\n68.21 65.12 52.84\n66.41 62.77 50.22\nSemantic Ent.\n57.41 54.32 47.21\n61.22 58.42 49.74\n63.21 59.10 48.62\n63.55 60.24 48.88\n64.91 61.44 50.72\nLexical Sim.\n50.41 46.77 38.92\n60.71 57.11 45.55\n59.42 56.88 48.91\n70.33 67.10 55.32\n58.33 55.42 47.41\nSelfCheckGPT\n55.21 52.14 43.92\n58.10 55.78 46.22\n62.82 59.90 50.44\n65.22 62.44 54.21\n63.44 60.77 52.33\nRACE\n56.14 53.72 43.88\n63.11 59.71 52.81\n65.77 62.55 50.72\n58.88 55.14 46.18\n66.10 62.41 49.81\nP(true)\n54.31 52.22 44.10\n58.22 56.10 48.52\n56.91 53.55 43.92\n61.40 58.21 46.77\n57.33 54.88 45.91\nFActScore\n56.20 52.42 41.77\n55.44 52.12 41.14\n61.62 58.22 51.33\n59.33 56.42 49.14\n63.44 60.22 52.44\nTruthfulQA\nHALLUGUARD\n75.11 71.20 63.21\n70.44 67.55 58.12\n78.92 74.22 65.33\n76.44 72.01 59.92\n79.33 75.11 66.08\nInside\n71.10 68.55 60.77\n61.77 59.44 50.10\n63.88 61.33 53.41\n69.22 65.10 55.14\n62.14 59.94 52.80\nMIND\n57.44 54.91 45.33\n59.92 56.88 48.33\n58.72 56.14 47.21\n61.21 58.88 52.02\n60.44 58.20 49.03\nPerplexity\n49.52 46.71 38.84\n54.12 51.74 43.90\n59.72 57.55 46.88\n54.44 51.72 42.55\n60.33 57.21 47.41\nLN-Entropy\n57.11 54.88 42.98\n55.33 52.41 45.91\n59.66 56.22 43.10\n60.44 58.02 46.22\n61.41 57.17 43.88\nEnergy\n54.11 52.17 38.91\n53.44 51.14 36.88\n58.21 54.77 49.92\n63.02 60.44 51.33\n58.41 55.33 50.42\nSemantic Ent.\n60.08 56.44 44.15\n50.14 47.33 35.92\n53.74 52.11 37.02\n65.33 63.20 50.77\n55.02 53.11 38.44\nLexical Sim.\n51.22 49.20 39.03\n58.72 54.71 48.77\n65.71 63.50 53.10\n54.77 51.44 45.88\n66.41 64.14 54.88\nSelfCheckGPT\n55.72 53.44 42.78\n58.33 55.72 47.14\n60.88 57.44 43.91\n55.42 54.44 40.77\n61.72 59.51 44.10\nRACE\n52.22 49.88 41.44\n63.14 66.88 54.05\n70.55 67.11 59.77\n55.44 52.11 45.33\n71.33 68.22 60.02\nP(true)\n55.54 52.11 38.82\n55.72 52.33 39.22\n57.41 53.10 41.22\n56.88 54.77 45.55\n57.12 53.33 41.88\nFActScore\n52.91 50.14 40.44\n54.11 50.22 41.33\n52.88 49.91 42.55\n61.55 59.22 44.72\n53.41 50.71 43.10\nAcross all three benchmarks (RAGTruth, GSM8K, HaluEval) and all backbones (GPT-2 through\nQwQ-32B and LLaMA2-13B), HALLUGUARD consistently achieves the highest F1 and the highest\nor near-highest TPR under fixed low-FPR constraints. In contrast, SAPLMA and LLM-Check ex-\nhibit noticeably lower recall in the stringent 5% FPR regime. These results demonstrate that HAL-\nLUGUARD is better aligned with maintaining high detection sensitivity under tight false-positive\nbudgets, a requirement that is central to reliable hallucination detection in real-world systems.\nTable 7: Comparison with SAPLMA, LLM-Check and ITI across benchmarks and backbones.\nBenchmark\nMethod\nGPT2\nOPT-6.7B\nMistral-7B\nQwQ-32B\nLLaMA2-13B\nAUROC\nAUPRC\nF1\nTPR@10%\nTPR@5%\nAUROC\nAUPRC\nF1\nTPR@10%\nTPR@5%\nAUROC\nAUPRC\nF1\nTPR@10%\nTPR@5%\nAUROC\nAUPRC\nF1\nTPR@10%\nTPR@5%\nAUROC\nAUPRC\nF1\nTPR@10%\nTPR@5%\nRAGTruth\nHALLUGUARD\n75.51\n73.40\n81.22\n74.86\n61.41\n80.13\n76.77\n77.03\n73.52\n59.12\n82.31\n80.79\n83.19\n79.44\n69.21\n84.59\n81.15\n85.91\n80.13\n63.52\n77.51\n75.30\n74.66\n68.91\n57.42\nRAGTruth\nSAPLMA\n72.80\n70.10\n72.20\n63.50\n55.10\n78.90\n74.20\n74.10\n68.00\n58.20\n79.40\n77.30\n79.00\n72.10\n60.50\n81.00\n78.20\n79.44\n72.80\n61.30\n74.20\n72.10\n70.50\n61.80\n55.90\nRAGTruth\nLLM-Check\n68.10\n64.50\n63.90\n55.20\n44.80\n72.30\n68.40\n66.50\n57.90\n46.30\n75.20\n71.60\n67.40\n60.30\n48.70\n76.10\n73.20\n68.90\n61.10\n49.50\n71.60\n68.90\n63.20\n55.40\n46.10\nRAGTruth\nITI\n69.30\n65.80\n66.10\n57.90\n47.90\n73.10\n69.20\n68.20\n59.80\n49.10\n76.00\n72.50\n69.40\n61.80\n50.90\n77.20\n74.10\n70.50\n62.40\n51.70\n72.80\n70.10\n65.40\n57.10\n47.80\nGSM8K\nHALLUGUARD\n72.04\n69.88\n78.33\n74.11\n65.42\n72.57\n70.31\n74.91\n69.14\n62.10\n80.62\n77.30\n80.22\n76.88\n68.21\n75.81\n74.68\n82.55\n78.91\n70.45\n79.01\n76.73\n79.10\n74.25\n67.92\nGSM8K\nSAPLMA\n69.20\n66.10\n70.10\n62.00\n54.40\n70.80\n67.20\n71.80\n64.10\n56.30\n77.10\n74.00\n76.20\n69.50\n59.80\n73.90\n71.20\n76.50\n70.10\n60.70\n75.40\n72.30\n74.00\n67.10\n59.10\nGSM8K\nLLM-Check\n65.40\n61.50\n62.40\n54.10\n46.20\n68.10\n64.30\n67.50\n59.20\n49.80\n73.40\n69.80\n64.90\n57.90\n48.30\n71.20\n67.90\n67.80\n60.30\n50.40\n72.10\n68.50\n64.20\n56.60\n48.00\nGSM8K\nITI\n66.80\n63.00\n64.50\n56.20\n48.70\n69.00\n65.40\n69.20\n61.50\n51.90\n74.20\n70.60\n67.10\n60.80\n50.10\n72.50\n69.20\n69.40\n62.50\n52.30\n73.00\n69.10\n66.10\n58.40\n49.50\nHaluEval\nHALLUGUARD\n70.42\n67.71\n75.11\n71.20\n63.21\n71.62\n67.88\n70.44\n67.55\n58.12\n74.91\n72.74\n78.92\n74.22\n65.33\n73.93\n70.87\n76.44\n72.01\n59.92\n78.15\n74.15\n79.33\n75.11\n66.08\nHaluEval\nSAPLMA\n67.10\n63.20\n69.20\n62.10\n54.00\n69.50\n65.70\n68.30\n61.60\n53.20\n72.00\n68.40\n75.10\n69.30\n58.90\n71.20\n68.10\n75.40\n70.30\n58.50\n76.10\n72.20\n76.80\n70.60\n60.90\nHaluEval\nLLM-Check\n63.50\n59.40\n61.10\n53.00\n44.50\n66.80\n62.90\n65.40\n57.50\n47.50\n70.10\n66.30\n63.80\n57.20\n47.10\n69.30\n65.40\n66.20\n59.50\n49.00\n71.50\n67.60\n63.50\n55.90\n47.40\nHaluEval\nITI\n64.80\n60.70\n63.40\n55.20\n46.80\n67.40\n63.50\n66.90\n58.60\n49.40\n71.00\n67.20\n66.10\n59.10\n48.60\n70.20\n66.30\n68.10\n61.10\n50.60\n72.30\n68.20\n65.20\n57.50\n48.70\nC.5\nTIGHTNESS OF BOUND\nEvaluation of bound tightness.\nTo rigorously stress-test the Hallucination Risk Bound of The-\norem 3.2, we conducted a controlled synthetic study grounded in the empirical reasoning-depth\ndistribution of the Snowballing dataset (Zhang et al., 2023). We instantiated empirical hallucination\ntrajectories by injecting low-variance Gaussian noise into the base components D(T) and R(T),\ncomparing them against the closed-form theoretical prediction. As illustrated in Figure 10, while\n25\n"}, {"page": 26, "text": "Published as a conference paper at ICLR 2026\nthe theoretical curve acts as a conservative upper envelope, it exhibits a nearly parallel growth trajec-\ntory to the empirical risk. Crucially, it faithfully captures the exponential curvature and compound-\ning dynamics of the Snowballing Effect. This confirms that the bound possesses high structural\nfidelity: it correctly models the scaling law of error propagation across depth ranges, validating its\neffectiveness as a ranking proxy despite the absolute numerical offset.\nFigure 10: Empirical hallucination risk versus our theoretical bound\nEvaluation of NTK proxy tightness.\nTo quantitatively validate that our NTK-based proxy faith-\nfully captures the amplification behavior of stepwise Jacobians, we conduct a diagnostic experiment\non GPT-2-small (117M), where per-step Jacobian norms are fully tractable. For a held-out set of\nGSM8K prompts and decoding steps t ≤18, we compute:\n• the empirical stepwise Jacobian magnitude ∥Jt∥2, obtained via automatic differentiation\non the next-token logits, and\n• our reasoning-driven NTK proxy, log σmax −log κ2, as defined in Eq. (7), which upper-\nbounds the per-step amplification rate and penalizes spectral ill-conditioning of the NTK\nGram matrix.\nFigure 11 reports the scatter plot comparing the NTK proxy against empirical ∥Jt∥2 across all\nprompts and steps.\nValidation of Term Decomposition\nTo validate the architectural premise of our Hallucination\nRisk Bound Section 3.2, we visualize the evolution of the decomposed risk components across rea-\nsoning depth T on the Snowballing dataset (Zhang et al., 2023). As shown in Figure Figure 12,\nthe total risk is driven by two distinct dynamic behaviors. The data-driven term (green dotted line)\nexhibits linear or near-constant progression, reflecting static retrieval or knowledge-encoding errors\nthat persist regardless of depth. In contrast, the reasoning-driven term (purple dotted line) demon-\nstrates exponential amplification consistent with the Snowballing Effect, remaining negligible at\nshallow depths but rapidly dominating the total risk as T increases.Crucially, this reveals a phase\ntransition in hallucination dynamics: at lower depths (T < 15), errors are primarily data-driven,\nwhereas at higher depths, reasoning instability becomes the governing factor. This dichotomy em-\npirically justifies our hybrid scoring mechanism, confirming that a unified detector must account\nfor both the static semantic bias and the dynamic rollout instability to be effective across varying\ngeneration lengths.\n26\n"}, {"page": 27, "text": "Published as a conference paper at ICLR 2026\nFigure 11: The NTK proxy closely tracks empirical Jacobian amplification on GPT-2-small, showing\nnear-perfect monotonic alignment and a consistent conservative envelope across decoding depth.\nFigure 12: Risk decomposition across reasoning depth T on Snowballing dataset.\nC.6\nCORRELATION OF REASONING-DRIVEN AND DATA-DRIVEN TERMS WITH DIFFERENT\nTYPES OF DATASETS\nTo empirically verify the independence of the proposed risk components, we analyzed their cor-\nrelation with detection performance across distinct task families. As illustrated in Figure 14 and\nFigure 13, we observe a sharp geometric decoupling: the data-driven term aligns strongly with data-\ncentric benchmarks (e.g., RAGTruth) while showing negligible correlation with reasoning tasks.\nConversely, the reasoning-driven term dominates on reasoning-oriented datasets (e.g., MATH-500).\n27\n"}, {"page": 28, "text": "Published as a conference paper at ICLR 2026\nThis double dissociation reinforces the structural validity and orthogonality of our decomposition,\nconfirming that each term captures a distinct, non-redundant failure mode.\nFigure 13: Correlation Between data-driven and reasoning-driven terms and AUROC on Reasoning-\nCentric MATH500.\nFigure 14: Correlation Between data-driven and reasoning-driven terms and AUROC on Data-\nCentric RAGTruth.\n28\n"}, {"page": 29, "text": "Published as a conference paper at ICLR 2026\nC.7\nCASE STUDY\nCase Study 1 — GSM8K (Multi-step Arithmetic): Bias →Drift →Snowballing.\nTask: “John\nsaves $3/day for four weeks and buys a $12 toy. How much money does he have left?”\nGround truth: $72.\nLength (T)\nModel Behavior\nHalluGuard Response\nT=1–8 Stable setup\nCorrect restatement and arithmetic planning\nData-driven term dominant; risk flat\nT=9–14 Seed error\n“4 weeks” →“40 days”\nSlight rise in data-driven signal\nT=15–22 Propagation\n“3 × 40 = 120”\nReasoning-driven share begins to rise\nT=23–40 Amplification\nFinal answer: $108\nReasoning-driven dominates (snowballing)\nTable 8: Evolution of hallucination in GSM8K arithmetic reasoning.\nCase Study 2 — Long-Document Summarization: Misalignment →Overreach →Fabrication.\nTask: Summarize a 5,000-token policy document\nGround truth: Security audit exception applies only to specific log types.\nLength (T)\nModel Behavior\nHalluGuard Response\nT=1–20 Accurate extraction\nCorrect recovery of retention rules\nLow risk; strong alignment\nT=21–40 Misbinding\nIncorrect merge of distant sections\nData-driven signal increases\nT=41–95 Drift\nOvergeneralized suspension claim\nReasoning-driven share rises\nT=96–170 Fabrication\nNew false rule introduced\nReasoning-driven dominates\nTable 9: Evolution of hallucination in long-document summarization.\nD\nUSAGE OF LLM\nLarge language models (LLMs) were employed in a limited and transparent manner during the\npreparation of this manuscript. Specifically, LLMs were used to assist with linguistic refinement,\nstyle adjustments, and minor text editing to improve clarity and readability. They were not involved\nin formulating the research questions, designing the theoretical framework, conducting experiments,\nor interpreting results. All scientific contributions—including conceptual development, methodol-\nogy, analyses, and conclusions—are the sole responsibility of the authors.\n29\n"}]}