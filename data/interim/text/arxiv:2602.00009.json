{"doc_id": "arxiv:2602.00009", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.00009.pdf", "meta": {"doc_id": "arxiv:2602.00009", "source": "arxiv", "arxiv_id": "2602.00009", "title": "Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA", "authors": ["Samuel Thio", "Matthew Lewis", "Spiros Denaxas", "Richard JB Dobson"], "published": "2025-11-27T16:08:22Z", "updated": "2025-11-27T16:08:22Z", "summary": "Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.00009v1", "url_pdf": "https://arxiv.org/pdf/2602.00009.pdf", "meta_path": "data/raw/arxiv/meta/2602.00009.json", "sha256": "c0e6ad3f7715c946c9c07ae0cfe821c298477b42de184dfe2840c85b9cb5c987", "status": "ok", "fetched_at": "2026-02-18T02:25:56.650981+00:00"}, "pages": [{"page": 1, "text": "Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to\nSafe Clinical AI for Patient QA\nSamuel Thio*,†1,2,3, Matthew Lewis†1, Spiros Denaxas1,4,5,6, Richard JB Dobson1,2,7,8,9,10\n1Institute of Health Informatics, University College London, London, U.K.\n2Department of Biostatistics and Health Informatics, King’s College London, London, U.K.\n3EPSRC DRIVE-Health CDT, London, U.K.\n4Interdisciplinary Transformation University (IT:U), Linz, Austria\n5British Heart Foundation Data Science Centre, London, U.K.\n6National and Kapodistrian University of Athens, Athens, Greece\n7CogStack Limited, London, U.K.\n8NIHR Biomedical Research Centre at South London and Maudsley NHS Foundation Trust and King’s College\nLondon, London, UK\n9NIHR Biomedical Research Centre at University College London Hospitals NHS Foundation Trust, London, UK\n10Health Data Research UK London, University College London, London, UK\nAbstract\nElectronic health record (EHR) systems present clinicians with vast repositories of clinical infor-\nmation, creating a significant cognitive burden where critical details are easily overlooked. While Large\nLanguage Models (LLMs) offer transformative potential for data processing, they face significant limita-\ntions in clinical settings, particularly regarding context grounding and hallucinations. Current solutions\ntypically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured se-\nmantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph\nRetrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely\ncombining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings\nfor unstructured narrative retrieval, MediGRAF enables natural language querying of the complete pa-\ntient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relation-\nships), we generated enough nodes and data for patient level question answering (QA), and we evaluated\nthis architecture across varying query complexities. The system demonstrated 100% recall for factual\nqueries which means all relevant information was retrieved and in the output, while complex inference\ntasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demon-\nstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer,\nmore comprehensive alternative to standard LLM deployments.\nKeywords: Electronic Health Records, Knowledge Graphs, Retrieval-Augmented Generation,\n*Corresponding author. Email: samuel.j.thio@kcl.ac.uk\n†These authors contributed equally\n1\narXiv:2602.00009v1  [cs.CL]  27 Nov 2025\n"}, {"page": 2, "text": "Natural Language Processing, Neo4j, Vector Embeddings, Text2Cypher, Large Language Models\n1\nIntroduction\nThe proliferation of electronic health record (EHR) systems has fundamentally transformed healthcare de-\nlivery, creating comprehensive digital repositories of patient information encompassing both structured data\nelements (medications, laboratory results, vital signs) and unstructured free-text narratives (clinical notes,\ndischarge summaries). However, this wealth of information presents a paradoxical challenge: while more\ndata are available than ever before, manually reviewing current and historical admission notes has become\nincreasingly time-consuming and cognitively demanding, potentially leading to overlooked clinically sig-\nnificant information [1]. This information overload contributes directly to clinician burnout and may com-\npromise patient safety when critical details are missed during time-pressured clinical encounters.\nTraditional approaches to managing EHR data have proven inadequate in capturing the complex temporal\nand relational aspects inherent in healthcare information. Conventional relational databases, while efficient\nfor storing structured data, struggle to represent the intricate relationships between clinical events, medica-\ntions, diagnoses, and patient outcomes [2]. These relationships embody not only associations but also causal\nchains, temporal dependencies, and clinical reasoning pathways fundamental to understanding a patient’s\nhealth trajectory [3, 4].\nRecent advances in large language models (LLMs) have demonstrated transformative potential for natural\nlanguage processing in healthcare [5, 6]. However, when applied to patient-specific data retrieval, LLMs\nface significant limitations, most notably lacking necessary context grounding, leading to hallucinations\n(plausible sounding but factually incorrect information) [7].\nGraph databases have emerged as a promising solution. Unlike traditional relational databases with rigid\ntables and predefined relationships, graph databases represent information as networks of nodes and edges,\nnaturally capturing the complex web of relationships characterizing healthcare data. Neo4j has demon-\nstrated particular advantages over traditional SQL databases in healthcare applications, offering superior\nperformance for complex traversal queries [8].\nRetrieval-Augmented Generation (RAG) has gained attention as a method to enhance LLM capabilities by\ngrounding responses in retrieved contextual information [9]. Graph RAG represents an evolution specifi-\ncally designed to leverage graph database structural advantages. Recent work on hybrid RAG architectures\nhas demonstrated effectiveness of combining multiple retrieval modalities [10], with approaches integrating\nknowledge graphs with vector retrieval showing superior performance.\n2\n"}, {"page": 3, "text": "Despite these advances, a significant gap remains: no existing system successfully integrates graph database\ntechnology with both Text2Cypher capabilities and vector embeddings to create a unified solution for query-\ning patient-level EHR data. Current approaches typically focus on either structured data retrieval or semantic\nsearch, but not both simultaneously. This study addresses this gap by developing and evaluating MediGRAF\n(Medical Graph Retrieval Augmented Framework), a novel Graph RAG system that combines Neo4j graph\ndatabase technology with large language models to enable natural language querying of complex clinical\ndata. We present an optimized graph schema for EHR representation, implement Text2Cypher translation\nfor accessible graph querying, and integrate vector embeddings for semantic search across unstructured clin-\nical narratives. Our evaluation using MIMIC-IV data demonstrates MediGRAF’s ability to achieve perfect\nrecall for factual queries while maintaining high performance and safety standards for complex clinical in-\nference tasks, establishing a foundation for next-generation clinical information retrieval systems.\n1.1\nSignificance and Clinical Integration\nThe significance of this research extends beyond technical innovation to address pressing clinical needs. By\nenabling natural language querying of complex EHR data, this system has the potential to reduce the cog-\nnitive burden on clinicians, improve the efficiency of clinical information retrieval, and ultimately enhance\npatient care quality. The approach is particularly relevant for the NHS context, where time pressures and\nresource constraints make efficient information access critical for maintaining care quality while managing\nincreasing patient volumes.\nFurthermore, the system is architected to align with existing NHS informatics infrastructure, offering a clear\npathway for integration with NLP platforms like CogStack. As detailed in Section 5.1, this integration al-\nlows MediGRAF to leverage backend ingestion pipelines for live, real-time clinical decision support.\n2\nRelated Work\nThe development of the MediGRAF system sits at the intersection of three rapidly evolving domains:\ngraph database management in healthcare, natural language processing (NLP) via Large Language Models\n(LLMs), and hybrid information retrieval strategies. This section reviews the progression of these technolo-\ngies, identifying the specific technological gaps that this research aims to address.\n2.1\nGraph Databases in Healthcare Data Management\nTraditional approaches to managing Electronic Health Records (EHR) have largely relied on relational\ndatabase management systems (RDBMS). While efficient for transactional data, these systems struggle to\nrepresent the complex, interconnected nature of clinical events [2]. The rigid tabular structure of RDBMS\n3\n"}, {"page": 4, "text": "fails to naturally capture the causal chains, temporal dependencies, and clinical reasoning pathways that\ndefine a patient’s health trajectory.\nGraph databases have emerged as a superior alternative by representing data as networks of nodes and edges.\nThis structure aligns more closely with biological and clinical reality. In a foundational study establishing\nthe feasibility of this approach, Stothers and Nguyen demonstrated that Neo4j offered distinct advantages\nover PostgreSQL for healthcare applications [8]. Their work highlighted a specific use case: handling com-\nplex traversal queries such as linking patients to diagnoses and treatments across multiple encounters where\ngraph databases significantly outperformed relational counterparts in both speed and query intuitiveness.\nThis evidence suggests that graph databases are better suited to handle the scale and relational complexity\nof real-world EHR data.\n2.2\nLarge Language Models and the Context Gap\nParallel to advances in database technology, the rise of Large Language Models (LLMs) has transformed\nclinical NLP. Models such as GPT-4 have demonstrated the ability to process unstructured narratives that\nmake up a significant portion of EHR data [5, 6]. However, the deployment of LLMs in patient-specific\nretrieval tasks faces a critical hurdle: the lack of contextual grounding.\nResearch by Zubiaga et al. highlights that without access to external veridical knowledge, LLMs are prone\nto \"hallucinations\" generating plausible but factually incorrect information [7]. In a clinical setting, where\nprecision is paramount, this limitation is prohibitive. While LLMs excel at linguistic fluency, they lack an\ninherent \"memory\" of the specific patient’s history, necessitating external retrieval mechanisms to ground\ntheir outputs in factual records.\n2.3\nRetrieval-Augmented Generation (RAG) and GraphRAG\nTo mitigate hallucination risks, Lewis et al. introduced Retrieval-Augmented Generation (RAG), a frame-\nwork that retrieves relevant documents to condition the LLM’s generation [9]. While effective for general\ntext, standard RAG often misses the structural relationships inherent in medical data [8].\nThis limitation led to the development of \"Graph RAG,\" which leverages the structural advantages of knowl-\nedge graphs. Edge et al. applied this to the use case of query-focused summarisation, demonstrating that\ngraph structures could improve the relevance and coherence of generated summaries by capturing global re-\nlationships that vector-only retrieval might miss [11]. Similarly, Wu et al. demonstrated that Medical Graph\nRAG could improve safety in medical LLMs by incorporating structured clinical relationships alongside\nnarrative data [12].\n4\n"}, {"page": 5, "text": "2.4\nBridging the Interaction Gap: Text2Cypher\nA significant barrier to the adoption of graph databases in clinical practice is the technical expertise required\nto query them. The Cypher query language, while powerful, is inaccessible to most clinicians. To address\nthis, Ozsoy et al. developed the Text2Cypher methodology, which utilises LLMs to translate natural lan-\nguage questions directly into Cypher queries [13]. This innovation is crucial for the clinical utility of the\nproposed system, as it allows end-users to interact with complex graph structures using standard medical\nterminology without needing to learn query syntax.\n2.5\nThe Case for Hybrid Retrieval Architectures\nWhile Graph RAG and Text2Cypher tackle structured data and accessibility respectively, they often struggle\nwith the free-text narratives (discharge summaries, radiology reports) that contain vital clinical nuance. Re-\ncent work on \"Hybrid RAG\" architectures has attempted to bridge this divide. Sarmah et al. demonstrated\nthat integrating knowledge graphs with vector retrieval (semantic search) leads to superior performance in\ninformation extraction tasks compared to using either modality in isolation [10].\nHowever, a gap remains in applying these hybrid architectures to patient-level EHR data. Current approaches\ntypically focus on either structured data retrieval (via Text2Cypher) or semantic search (via vectors), but\nrarely integrate both into a unified pipeline. This study addresses this gap by proposing a system that com-\nbines the precision of Text2Cypher for structured facts with the semantic reach of vector embeddings for\nclinical narratives, aiming to provide a comprehensive view of the patient journey.\n3\nMethodology\n3.1\nData Source and Preprocessing\nWe utilised the MIMIC-IV dataset (version 3.1) [14, 15], a freely accessible, de-identified electronic health\nrecord database from Beth Israel Deaconess Medical Center. Using data from ten patients, we were able to\ngenerate 5,973 nodes and 5,963 relationships which was sufficient for evaluating patient level question an-\nswering (QA). These 10 patients were selected based on specific criteria designed to stress-test the system’s\ncapabilities.\n• Multiple admissions: Essential to evaluate the graph’s ability to handle temporal reasoning and lon-\ngitudinal patient trajectories as multiple admissions was usually associated with increased complexity\nand patient data.\n5\n"}, {"page": 6, "text": "• Availability of Radiology & Discharge notes: Required to test the hybrid retrieval of unstructured\ndata alongside structured codes.\n• Diverse Specialities: Ensures the embedding model generalises across different medical vocabularies\n(e.g., cardiology vs. oncology terminology).\nData preprocessing followed a systematic pipeline maintaining clinical integrity. Structured data elements\nwere extracted from relevant MIMIC-IV tables (patients, admissions, diagnoses_icd, procedures_icd, pre-\nscriptions, labevents, discharge, radiology). Free-text narratives underwent section segmentation, removal\nof de-identification artifacts, and preparation for vector embedding while preserving clinical context.\n3.2\nGraph Database Schema Design\nThe development of an optimised graph schema for Neo4j required careful consideration of both clinical\nrelationships and query performance. The schema was designed to represent eight primary node types,\neach capturing distinct clinical entities: Patient nodes containing demographic information and serving\nas central connection points; Admission nodes representing individual hospital encounters with temporal\nboundaries; Diagnosis nodes storing ICD-10 coded conditions with both short codes and descriptive long\ntitles; Procedure nodes capturing clinical interventions and their timing; Medication nodes representing\nprescribed drugs with dosage and route information; Lab Event nodes containing laboratory test results\nwith reference ranges; Discharge Note nodes storing complete discharge summaries with embedded vector\nrepresentations; and Radiology Report nodes containing imaging study reports with vector embeddings.\nFigure 1: Neo4j schema of MIMIC-IV data with clinical nodes and relationships\nRelationships between nodes were carefully defined to preserve clinical semantics and enable meaningful\n6\n"}, {"page": 7, "text": "traversal queries. The HAS_ADMISSION relationship connects patients to their hospital admissions, main-\ntaining temporal ordering. HAS_DIAGNOSIS links admissions to diagnosed conditions, preserving the\ncontext of when diagnoses were made. HAS_PROCEDURE connects admissions to performed procedures,\nwhile HAS_MEDICATION tracks prescribed medications during specific admissions. INCLUDES_LAB\nrelationships link admissions to laboratory results, maintaining temporal sequences. The schema design\nprioritised bidirectional traversal capabilities, enabling queries to flow naturally in either direction based on\nclinical reasoning patterns.\n3.3\nVector Embedding Implementation\nFree-text clinical documents, including discharge summaries and radiology reports, were processed using\nOpenAI’s text-embedding-3-small model, which generates 1536-dimensional vector representations opti-\nmised for semantic similarity search [16]. The embedding process involved several stages to ensure optimal\nrepresentation of clinical content. Documents were first segmented into semantically coherent chunks, with\nchunk boundaries determined by clinical section headers and paragraph structures. Each chunk was embed-\nded independently, with metadata preserved to maintain document provenance. The resulting embeddings\nwere stored as properties of the relevant document nodes in the Neo4j database, enabling hybrid querying\nthat combines graph traversal with vector similarity search using cosine similarity scores.\nLet A denote the embedded query and B denote the embedded retrieved answer. The cosine similarity is\nthen defined as\ncosine_similarity(A, B) =\nA · B\n∥A∥∥B∥\n(1)\nVector indexes were created using Hierarchical Navigable Small World (HNSW) indexing [17], which pro-\nvides efficient approximate nearest neighbour search. Cosine similarity was selected as the similarity mea-\nsure based on its effectiveness for text embeddings and widespread adoption in clinical NLP applications\n[18, 19].\n3.4\nMediGRAF Pipeline Architecture\nThe MediGRAF system integrates multiple components to process natural language queries and generate\ncontextually grounded responses. The pipeline begins with query processing, where natural language input\nfrom clinicians undergoes initial analysis to identify key entities, temporal constraints, and query intent.\n7\n"}, {"page": 8, "text": "The Text2Cypher component leverages GPT-4o-mini with carefully engineered prompts to translate natural\nlanguage queries into Cypher query language [13, 20]. The prompt engineering process involved iterative\nrefinement based on common clinical query patterns. As an illustration, the natural language query\nWhat did the radiology reports show for patient 10461137?\nis translated into the following Cypher query:\nMATCH (p:Patient {subject_id: ’10461137’})-[:HAS_ADMISSION]->(a:Admission)-[:\nINCLUDES_RADIOLOGY_REPORT]->(r:RadiologyReport)\nRETURN r.note_id, r.text\nFollowing the initial translation, the hybrid engine orchestrates the full retrieval and generation process via\na four-step pipeline:\n1. Structured Retrieval: The generated Cypher query is executed against the Neo4j database to ex-\ntract structured nodes and relationships. The system implements intelligent result limiting to prevent\ncontext window overflow.\n2. Unstructured Retrieval: Simultaneously, the system performs a vector similarity search using the\nsame embedding model to identify semantically similar clinical text chunks from the vector index.\n3. Context Consolidation: The results are merged via context concatenation. The structured graph\noutputs (converted to natural language text) and the unstructured vector chunks are combined into a\nsingle, unified prompt context window.\n4. Generation: This unified context is passed to the generation model (GPT-4o) to synthesise the final\nanswer. This model is distinct from the lighter model used for query translation, ensuring the final\nresponse prioritizes reasoning capabilities and safety.\n8\n"}, {"page": 9, "text": "Figure 2: A workflow diagram showing the MediGRAF architecture from query input through graph re-\ntrieval to LLM augmentation and response generation\nNote on Source Attribution: While the context consolidation approach ensures the model has access to all\ninformation, it presents a challenge for precise source attribution. By flattening distinct graph nodes and text\nchunks into a single context block, the model occasionally struggles to map a specific sentence in the output\nback to its precise origin node ID, a limitation discussed further in Section 5.\nThe designed architecture was implemented as a fully functional web-based application using Streamlit,\nproviding an intuitive interface for clinical users. Figure 3 shows the implemented system through which all\nevaluation queries were processed.\n9\n"}, {"page": 10, "text": "Figure 3: Overview of the MediGRAF system interface showing the main query input area and configurable\nretrieval parameters. The left sidebar provides filtering options for patient selection, note type filtering, and\nadvanced settings for both vector search (document retrieval limits) and graph database queries (maximum\nrecords to return).\n3.5\nQuery Complexity Classification\nTo evaluate system performance across different challenge levels, queries were classified into three com-\nplexity categories. This classification was performed through a manual review process by the research team\nwith domain expert input, defining complexity based on the graph traversal depth and data synthesis require-\nments.\nSimple queries involve direct fact retrieval requiring single node lookups or straightforward relationships,\nsuch as patient demographics, admission counts, or document tallies. These queries test the system’s ability\nto accurately retrieve and present basic clinical facts. Examples include queries such as: “What is the date\nof birth for patient 10461137?” or “Count the number of admission records.”\nMedium complexity queries require traversal of multiple relationships or temporal reasoning, including\nmedication histories, diagnostic patterns across admissions, or laboratory result trends. These queries assess\nthe system’s capability to integrate information across multiple graph nodes while maintaining temporal\ncoherence. An example of this category is: “List all medications prescribed to the patient during their ad-\nmission for pneumonia in 2023,” which requires linking Admission →Diagnosis →Medication.\nComplex queries demand synthesis, summarisation, or inference from multiple data sources, such as com-\n10\n"}, {"page": 11, "text": "prehensive patient summaries, differential diagnosis reasoning, or treatment response analysis. These queries\nevaluate the system’s ability to combine structured and unstructured data, perform clinical reasoning, and\ngenerate coherent narratives from disparate information sources. A representative example is: “Summarise\nthe patient’s response to antibiotic treatment across all admissions and identify any recurring complications\nreported in the discharge notes.”\n3.6\nEvaluation Framework and Metrics\nThe evaluation methodology combined standard information retrieval metrics with clinical expert assess-\nment to comprehensively evaluate system performance. The evaluation dataset comprised 141 curated QA\npairs derived from the MIMIC-IV data [14], sampled to ensure coverage across all complexity levels and\nclinical domains.\nPerformance was evaluated using distinct methodologies for deterministic (Text2Cypher) and generative\n(Hybrid) outputs.\n3.6.1\nDeterministic Evaluation (Simple/Medium Queries)\nFor structured queries where a ground-truth set of records exists (e.g., “Count admissions”), we utilised\nstandard metrics computed as follows:\nPrecision =\nTP\nTP + FP ,\nRecall =\nTP\nTP + FN ,\nF1-score = 2 × Precision × Recall\nPrecision + Recall\n(2)\nwhere TP denotes true positives, FP false positives, and FN false negatives.\n• Accuracy: The percentage of queries where the generated Cypher returned the exact correct result\nset.\n• Recall: The proportion of relevant records retrieved from the database.\n• F1-Score: The harmonic mean of precision and recall.\n3.6.2\nGenerative Evaluation (Hybrid/Complex Queries)\nFor complex natural language inquiries where exact-match metrics are inapplicable, we employed a human-\nexpert evaluation protocol. Two independent clinical reviewers (hospital physicians) assessed responses\nagainst a gold-standard summary created by a senior clinician. They utilised structured 5-point Likert scales\n[21] based on the following definitions:\n• Hybrid Accuracy (Likert 1-5): Defined as the factual correctness of the generated narrative. A score\nof 5 indicates all medical facts (dates, dosages, diagnoses) are correct compared to the source notes.\n11\n"}, {"page": 12, "text": "• Completeness (Likert 1-5): Assesses whether the answer provides all key pieces of information\npresent in the ground truth.\n• Relevance & Conciseness (Likert 1-5): Assesses if the answer directly addresses the question with-\nout redundant or off-topic information.\n• Overall Quality (Likert 1-5): A holistic judgment of the answer’s clinical utility.\n• Safety (Binary 0/1): A binary metric where ’1’ (Unsafe) indicates the presence of a critical halluci-\nnation: inventing a medical condition, hallucinating a medication not in the record, or contradicting a\nknown contraindication.\nDetailed evaluator instructions, scoring rubrics, and the full annotation manual are provided in Appendix B.\nWorked examples illustrating the application of the Safety metric are provided in Appendix C.\n4\nResults\n4.1\nGraph Database Implementation and Statistics\nThe implementation of the MediGRAF system successfully processed data from 10 MIMIC-IV patients,\nconstructing a comprehensive knowledge graph that captured the complexity of their clinical histories. The\nresulting graph database contained 5,973 nodes distributed across the eight defined node types, with Patient\nnodes (n=10) serving as central hubs, Admission nodes (n=28) representing distinct hospital encounters,\nDiagnosis nodes (n=420) capturing the full spectrum of clinical conditions, Procedure nodes (n=29) doc-\numenting clinical interventions, Medication nodes (n=1051) representing pharmaceutical treatments, Lab\nEvent nodes (n=4346) containing laboratory test results, Discharge Note nodes (n=25) with full clinical nar-\nratives, and Radiology Report nodes (n=64) providing imaging insights.\nThe graph structure comprised 5,963 relationships that encoded the complex web of clinical associations.\nThe relationship distribution revealed the richness of clinical data, with HAS_DIAGNOSIS relationships\n(n=420), reflecting the diagnostic complexity of hospitalised patients. HAS_MEDICATION relationships\n(n=1051) captured the extensive pharmaceutical management, while HAS_PROCEDURE relationships (n=29)\ndocumented clinical interventions. The INCLUDES_LAB relationships (n=4346) connected laboratory re-\nsults to their clinical context, and HAS_ADMISSION relationships (n=28) maintained the patient-encounter\nhierarchy. The INCLUDES_RADIOLOGY_REPORT (n=64) connected the patient and their admissions to\ntheir unstructured radiology reports and INCLUDES_DISCHARGE_NOTE (n=25) connects them to their\ndischarge summaries.\nVector embedding processing successfully transformed all 89 free-text documents (25 discharge summaries\nand 64 radiology reports) into searchable vector representations. The embedding process maintained docu-\nment integrity while enabling semantic search capabilities.\n12\n"}, {"page": 13, "text": "4.2\nQuery Performance\nEvaluation across three complexity levels demonstrated the system’s robust performance and the comple-\nmentary nature of graph and vector retrieval mechanisms. High complexity queries were not evaluated using\nF1, precision, and recall metrics as these queries required multi-source synthesis and inferential reasoning,\nproducing free-text responses without discrete ground-truth answers for comparison. Therefore, high com-\nplexity queries were assessed through domain expert evaluation using Likert scales, reported in Section 4.3.\nFor simple queries (n=100), the Cypher-only system achieved 80% accuracy with matching precision and\nrecall (0.8), yielding an F1 score of 0.8 as shown in Table 1. These queries, which included patient de-\nmographics and basic counts, were predominantly answered through Cypher-based graph retrieval. When\nemploying the hybrid approach, the system achieved perfect accuracy (100%) and recall (1.0) which means\nall relevant facts were retrieved and in the final output; however, precision and F1 metrics could not be\ncalculated as the hybrid system augments responses with contextual information from unstructured sources,\nproducing enriched outputs that extend beyond the discrete ground-truth format required for precision mea-\nsurement.\nMedium complexity queries (n=31) showed more pronounced differences between approaches. The Cypher-\nonly system achieved 51.6% accuracy with precision of 0.806 and recall of 0.688, resulting in an F1 score\nof 0.742. These queries required integration of multiple data sources, making the hybrid retrieval approach\nessential. The hybrid system achieved perfect accuracy (100%) and recall (1.0) for medium complexity\nqueries which means all relevant facts were retrieved and in the final output, though precision metrics were\nsimilarly not applicable due to the generation of comprehensive, context-enriched responses rather than dis-\ncrete answers. The performance gap between Cypher-only and hybrid approaches demonstrates the value of\nincorporating unstructured data for queries requiring multi-source integration.\nTable 1: Performance metrics across query complexity levels (N=131)\nComplexity\nSystem\nN\nAccuracy (%)\nRecall\nF1\nSimple\nCypher\n100\n80\n0.8\n0.8\nSimple\nHybrid\n100\n100\n1.0\nN/A\nMedium\nCypher\n31\n51.6\n0.688\n0.742\nMedium\nHybrid\n31\n100\n1.0\nN/A\nNote: Precision and F1-score are marked N/A for Hybrid approaches because these queries generate generative\nnatural language responses rather than discrete retrieval sets, rendering exact-match metrics inapplicable.\n13\n"}, {"page": 14, "text": "4.3\nClinical Expert Evaluation\nDomain experts assessed 10 complex query responses using Likert scales (1-5, where 5 represents excel-\nlent). Table 2 presents detailed statistics for each evaluation dimension. The system achieved high scores\nfor factual accuracy and completeness, with Evaluator 1 rating Accuracy at 4.40±1.02 and Completeness\nat 4.40±0.92, while Evaluator 2 provided even higher ratings of 4.90±0.30 for both dimensions. Overall\nQuality scores were consistently strong (4.30±1.00 and 4.20±0.75 for Evaluators 1 and 2, respectively).\nNotably, no responses were flagged as potentially unsafe by either evaluator (0/10 cases), addressing a pri-\nmary concern with LLM applications in healthcare. An interesting divergence emerged in the Relevance &\nConciseness dimension, as illustrated in Figure 4. Evaluator 1 assigned a mean score of 4.20 (±0.87) while\nEvaluator 2 was notably more critical at 3.30 (±0.90), suggesting that response verbosity remains an area\nfor optimization. This finding was consistent with qualitative feedback from both evaluators.\nTable 2: Descriptive Statistics for Model Output Evaluation Evaluated by Clinicians (N=10 items)\n14\n"}, {"page": 15, "text": "Figure 4: Comparison of mean expert evaluation scores across four dimensions. Error bars represent stan-\ndard deviation. Note the significant divergence in ’Relevance & Conciseness’ scores.\nQualitative feedback revealed both strengths and areas for improvement. Evaluator 1 praised responses\nthat included “additional details for discharge, such as discharge follow-up plans rather than just answer-\ning the primary diagnosis.” However, both evaluators consistently identified verbosity as a recurring issue,\nwith Evaluator 2 noting instances where “the model output is overly verbose for a summary” and Evalua-\ntor 1 observing responses that were “very wordy, did not summarise, just pulled straight from clinical notes.”\n4.4\nError Analysis and Failure Modes\nDespite strong overall performance, systematic error analysis revealed specific failure patterns that inform\nfuture development priorities. Context window limitations emerged as the primary constraint for hospital-\nwide queries. Attempts to retrieve multiple queries across all patients resulted in context overflow, as the\ninput tokens exceeded the language model’s processing capacity (128k tokens for GPT-4o-mini). This limi-\ntation necessitated the implementation of intelligent result filtering and pagination strategies for broad-scope\nqueries.\nSource attribution occasionally proved challenging when information appeared in multiple contexts. The\nsystem sometimes struggled to clearly distinguish between information derived from graph traversal versus\nvector search, particularly when both sources contained overlapping information. This ambiguity, while not\naffecting accuracy, reduced transparency in response generation.\n15\n"}, {"page": 16, "text": "5\nDiscussion\n5.1\nTechnical Innovation and Clinical Significance\nThis research presents a significant advancement in clinical information retrieval through the novel integra-\ntion of graph database technology with retrieval-augmented generation. The MediGRAF system achieves\n100% recall for factual queries while maintaining high performance (mean quality score 4.25/5) for com-\nplex inference tasks, demonstrating that hybrid approaches combining Text2Cypher capabilities with vector\nembeddings are essential for comprehensive EHR querying. Neither technology alone proved sufficient, the\ngraph database excelled at structured relationship traversal while vector search captured semantic similarity\nin unstructured text.\nThe system’s architecture addresses critical barriers to clinical adoption through two key innovations. First,\nthe Text2Cypher implementation using GPT-4o-mini eliminates the need for users to learn complex query\nlanguages, making graph databases accessible to clinicians. Second, the graph structure provides explicit re-\nlationship representation that enables temporal queries and multi-hop reasoning impossible with traditional\ndatabases, for instance, queries linking medication changes to laboratory result trends that require under-\nstanding both temporal and causal relationships.\nCrucially, the system architecture supports integration with existing NHS data infrastructure, specifically the\nCogStack platform. While the current evaluation relied on static MIMIC-IV data, the modular design allows\nMediGRAF to sit downstream of CogStack’s backend data ingestion pipelines. This integration provides a\npathway to clinical deployment where patient data is ingested, harmonised, and de-identified by CogStack\ncan dynamically populate the Neo4j graph in near real-time. This bridge between established back end stor-\nage and frontend generative AI addresses the challenge of deploying LLMs in live hospital settings.\nEHR\n(Electronic\nHealth Records)\nCogStack\n(Ingestion & NLP)\nNeo4j\n(Graph Database)\nMediGRAF\n(Graph RAG)\nRaw Data\nStructured\nEntities\nContext\nRetrieval\nvia MedCAT\nClinician Query\nFigure 5: Proposed deployment pipeline illustrating the integration of MediGRAF with the NHS CogStack\necosystem. CogStack handles real-time ingestion and NLP extraction (via MedCAT) to dynamically popu-\nlate the Neo4j graph, enabling MediGRAF to query live clinical data.\nFrom a clinical perspective, the system directly addresses the problem of information oversight during time-\npressured encounters. With NHS clinicians facing increasing patient complexity and documentation burden,\nthe ability to surface all relevant information through natural language queries in sub-second response times\ncould translate to substantial efficiency gains. Most critically, the perfect safety record (0/10 unsafe re-\nsponses) achieved validates this approach for mitigating hallucination risks that have limited LLM adoption\n16\n"}, {"page": 17, "text": "in healthcare settings.\n5.2\nPerformance Analysis\nThe clinical evaluation revealed important insights about system performance across different query types.\nSimple and medium complexity queries achieved perfect recall, validating the graph database’s ability to\ncapture structured clinical relationships. For complex inference tasks requiring multi-source synthesis, the\nLikert scale evaluation demonstrated strong performance with high accuracy (4.40-4.90) and completeness\n(4.40-4.90) scores showing minimal inter-rater variance.\nThe notable divergence between evaluators on relevance and conciseness (4.20 vs 3.30) identifies verbosity\nas the primary area requiring refinement. This 0.90-point difference, the largest observed discrepancy, sug-\ngests that while the system successfully captures all relevant information, response optimisation varies sig-\nnificantly based on individual clinician preferences. Importantly, this verbosity issue does not compromise\naccuracy or safety, the high scores in these critical dimensions confirm that the system reliably synthesizes\ninformation without introducing errors or omissions.\nThe successful processing of the data of the 10 patients’ in 5,973 nodes and 5,963 relationships with con-\nsistently quick retrieval times demonstrates both the scalability potential and the clinical feasibility. These\nperformance characteristics align with recent findings in Hybrid RAG architectures, confirming that hybrid\nretrieval strategies are necessary for handling the complexity of clinical data.\n5.3\nLimitations and Mitigation Strategies\nSeveral limitations require acknowledgment while recognising their addressability. The evaluation scope of\n10 patients assessed by two evaluators, while appropriate for proof-of-concept validation, limits generalis-\nability claims. The observed inter-rater variability, particularly in subjective dimensions, underscores the\nneed for larger evaluator cohorts and standardized assessment criteria. Future studies should include more\nclinical experts and patients from multiple institutions to establish robust reliability metrics.\nThe verbosity issue, while not affecting accuracy, could impact usability in time-pressured environments.\nThis challenge is addressable through prompt engineering refinements including response summarisation\nlayers, adjustable verbosity settings, and context-aware tailoring based on query urgency. The context win-\ndow limitations for hospital-wide queries can be mitigated through query decomposition and streaming\narchitectures or the use of different models which have larger context windows.\nThe use of Likert scales for complex query evaluation, while subjective, was necessary given the absence\n17\n"}, {"page": 18, "text": "of discrete ground truth for inference-based responses. This methodological choice aligns with established\npractices in clinical NLP evaluation. The MIMIC-IV dataset’s single-institution origin requires acknowl-\nedgment, though its comprehensiveness provides a strong foundation for initial validation.\n5.4\nConclusion and Future Directions\nThis research establishes MediGRAF as a viable solution for clinical information retrieval, demonstrating\nthat combining graph databases with large language models creates capabilities beyond either technology\nalone. The integration of Model Context Protocol (MCP) and agentic workflows represents the natural evo-\nlution, enabling more sophisticated multi-step reasoning and dynamic query refinement [22].\nThe path forward requires systematic expansion from proof-of-concept to clinical deployment. Prospective\ntrials measuring time savings, decision quality, and user satisfaction in actual clinical settings will validate\npractical impact. The absence of safety concerns in our evaluation provides confidence for proceeding to\nlarger-scale trials across diverse clinical contexts and specialities.\nFor healthcare systems facing mounting pressure on clinical resources, this technology offers a template for\nresponsible AI deployment that maintains factual grounding while providing natural language accessibility.\nBy demonstrating feasibility, safety, and performance advantages, this work contributes three key advances\nto health data science: validation of graph-based EHR representation benefits, evidence for hybrid retrieval\nsuperiority in healthcare contexts, and a framework for safe LLM deployment through factual grounding. As\nhealthcare continues its digital transformation, such systems will become increasingly critical for managing\nclinical data complexity while maintaining the human-centered focus essential to quality patient care.\nEthics Statement\nThis research utilised the MIMIC-IV dataset, a publicly available de-identified EHR database with appro-\npriate ethical approval. Access required CITI training completion and PhysioNet data use agreement. All\nguidelines for responsible AI use with MIMIC data were followed. No re-identification attempts were made,\nand all examples use only provided de-identified identifiers.\nReferences\n[1] Elham Asgari, Japsimar Kaur, Gani Nuredini, Jasmine Balloch, Andrew M Taylor, Neil Sebire, Robert\nRobinson, Catherine Peters, Shankar Sridharan, and Dominic Pimenta. Impact of Electronic Health\n18\n"}, {"page": 19, "text": "Record Use on Cognitive Load and Burnout Among Clinicians: Narrative Review. JMIR Medical\nInformatics, 12:e55499, Apr 2024.\n[2] Peter B Jensen, Lars J Jensen, and Søren Brunak. Mining electronic health records: towards better\nresearch applications and clinical care. Nature Reviews Genetics, 13(6):395–405, 2012.\n[3] Denis Agniel, Isaac S Kohane, and Griffin M Weber. Biases in electronic health record data due to\nprocesses within the healthcare system: retrospective observational study. BMJ, 361, 2018.\n[4] John H Holmes, James Beinlich, Mary R Boland, Kathryn H Bowles, Yong Chen, Tessa S Cook,\nGeorge Demiris, Michael Draugelis, Laura Fluharty, Peter E Gabriel, et al. Why is the electronic\nhealth record so challenging for research and clinical care?\nMethods of Information in Medicine,\n60(01/02):032–048, 2021.\n[5] Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and\nMatthew McDermott. Publicly available clinical BERT embeddings. arXiv preprint arXiv:1904.03323,\n2019.\n[6] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas Eckardt,\nNarmin Ghaffari Laleh, Chiara Maria Lavinia Löffler, Sophie-Caroline Schwarzkopf, Michaela Unger,\nGregory P Veldhuizen, et al. The future landscape of large language models in medicine. Communi-\ncations Medicine, 3(1):141, 2023.\n[7] Arkaitz Zubiaga. Natural language processing in the era of large language models. Frontiers in Artifi-\ncial Intelligence, 6:1350306, 2024.\n[8] Jessica A. M. Stothers and Andrew Nguyen. Can Neo4j Replace PostgreSQL in Healthcare? AMIA\nSummits on Translational Science Proceedings, 2020:646–653, May 2020.\n[9] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459–9474,\n2020.\n[10] Bhaskarjit Sarmah, Dhagash Mehta, Benika Hall, Rohan Rao, Sunil Patel, and Stefano Pasquali. Hy-\nbridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient infor-\nmation extraction. In Proceedings of the 5th ACM International Conference on AI in Finance, pages\n608–616, 2024.\n[11] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt,\nDasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From Local to Global: A Graph\nRAG Approach to Query-Focused Summarization, Feb 2025.\n[12] Junde Wu, Jiayuan Zhu, Yunli Qi, Jingkun Chen, Min Xu, Filippo Menolascina, and Vicente Grau.\nMedical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented\nGeneration, Oct 2024.\n19\n"}, {"page": 20, "text": "[13] Makbule Gulcin Ozsoy, Leila Messallem, Jon Besga, and Gianandrea Minneci. Text2Cypher: Bridging\nNatural Language and Graph Databases, Dec 2024.\n[14] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J\nPollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. MIMIC-IV, a freely accessible electronic\nhealth record dataset. Scientific Data, 10(1):1, 2023.\n[15] Alistair Johnson, Tom Pollard, Steven Horng, Leo Anthony Celi, and Roger Mark. MIMIC-IV-Note:\nDeidentified free-text clinical notes (version 2.2), 2023.\n[16] OpenAI. New embedding models and API updates, 2024. Accessed: 2025-04-03.\n[17] Yu A Malkov and Dmitry A Yashunin.\nEfficient and robust approximate nearest neighbor search\nusing hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine\nintelligence, 42(4):824–836, 2018.\n[18] Yanshan Wang, Sijia Liu, Naveed Afzal, Majid Rastegar-Mojarad, Liwei Wang, Feichen Shen, Paul\nKingsbury, and Hongfang Liu. A comparison of word embeddings for the biomedical natural language\nprocessing. Journal of biomedical informatics, 87:12–20, 2018.\n[19] KSSS KS.\nA survey of embeddings in clinical natural language processing.\narXiv preprint\narXiv:1903.01039, 2019.\n[20] OpenAI, Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, et al. GPT-4o system card,\n2024.\n[21] Gail M Sullivan and Anthony R Artino Jr. Analyzing and interpreting data from Likert-type scales.\nJournal of Graduate Medical Education, 5(4):541–542, 2013.\n[22] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape,\nsecurity threats, and future research directions. arXiv preprint arXiv:2503.23278, 2025.\n20\n"}, {"page": 21, "text": "Appendix\nA\nNeo4j Graph Database Query Script\nThe following Python script demonstrates the core implementation of the Graph RAG pipeline, including\nNeo4j connection, Text2Cypher prompt engineering, and query execution:\nimport os\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom langchain_community.graphs import Neo4jGraph\n# Changed from\nlangchain_neo4j\nfrom langchain.chains import GraphCypherQAChain\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_neo4j.chains.graph_qa.cypher import GraphCypherQAChain\nfrom langchain_core.prompts.prompt import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\nos.environ[\"NEO4J_USER\"] = \"neo4j\"\nos.environ[\"NEO4J_PASSWORD\"] = \"password\"\n# Load environment variables from .env file\nload_dotenv()\n# Initialize Neo4j connection using environment variables\ngraph = Neo4jGraph(\nurl=os.getenv(’NEO4J_URI’),\nusername=os.getenv(’NEO4J_USERNAME’),\npassword=os.getenv(’NEO4J_PASSWORD’)\n)\ngraph.refresh_schema()\nprint(graph.schema)\nenhanced_graph = Neo4jGraph(enhanced_schema=True)\nprint(enhanced_graph.schema)\n# Initialize OpenAI chat model (automatically uses OPENAI_API_KEY from\nenvironment)\nllm = ChatOpenAI(\nmodel=\"gpt-4o-mini\",\ntemperature=0\n)\n# Cypher generation template\nCYPHER_GENERATION_TEMPLATE = \"\"\"Task Cypher statement to query a graph\ndatabase.\nInstructions:\nUse only the provided relationship types and properties in the schema.\nDo not use any other relationship types or properties that are not provided.\n21\n"}, {"page": 22, "text": "Schema:\n{schema}\nNote: Do not include any explanations or apologies in your responses.\nDo not respond to any questions that might ask anything else than for you to\nconstruct a Cypher statement.\nDo not include any text except the generated Cypher statement.\nExamples: Here are a few examples of generated Cypher statements for\nparticular questions:\nHow many patients have diabetes?\nMATCH (p)-[]->(a)-[]->(d)\nWHERE d.long_title CONTAINS ’diabetes’\nRETURN COUNT(DISTINCT p) AS number_of_patients_with_diabetes\nGive me a summary of patient 11649167.\nMATCH (p)-[]->(a)\nWHERE p.subject_id = ’11649167’\nWITH p, a\nOPTIONAL MATCH (a)-[r1]->(m)\nWHERE a.hadm_id = m.hadm_id\nOPTIONAL MATCH (a)-[r2]->(d)\nWHERE a.hadm_id = d.hadm_id\nOPTIONAL MATCH (a)-[r3]->(pr)\nWHERE a.hadm_id = pr.hadm_id\nRETURN p, a, m, d, pr;\nThe question is:\n{question}\"\"\"\nCYPHER_GENERATION_PROMPT = PromptTemplate(\ninput_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n)\n# Simplified medical QA prompt\nMEDICAL_QA_TEMPLATE = \"\"\"You are a medical database expert.\nRemember that subject_id values represent unique patients.\nRemember hadm_id represents unique admissions for a patient to the hospital.\nQuestion: {question}\nResult: {context}\nProvide a clear and comprehensive medical interpretation.\nDo not provide recommendations:\"\"\"\nmedical_qa_prompt = PromptTemplate(\ntemplate=MEDICAL_QA_TEMPLATE,\ninput_variables=[\"question\", \"context\"]\n)\n# Create chain with simplified prompts\nchain = GraphCypherQAChain.from_llm(\nllm=ChatOpenAI(temperature=0),\ngraph=graph,\n22\n"}, {"page": 23, "text": "cypher_prompt=CYPHER_GENERATION_PROMPT,\nqa_prompt=medical_qa_prompt,\ntop_k=20,\nvalidate_cypher=True,\nverbose=True,\nallow_dangerous_requests=True\n)\n# Test query\nresponse = chain.run(\"Give me a full summary of 10300608\")\nprint(response)\n[Full implementation script available in the GitHub repository]\nB\nEvaluation Criteria and Annotation Instructions\nThis section outlines the evaluation framework used to assess model-generated answers against ground truth\nresponses in the medical question answering task.\nB.1\nEvaluation Criteria\nB.1.1\nAccuracy\nDefinition: How factually correct is the model’s answer when compared to the ground truth answer? Does\nit contain any misinformation?\n• 5 (Very Good): The model’s answer is completely factually correct and aligns perfectly with the\nground truth. No errors.\n• 4 (Good): The model’s answer is mostly accurate with only minor, insignificant inaccuracies that do\nnot mislead.\n• 3 (Fair): The model’s answer contains some noticeable inaccuracies, but the main point might still\nbe partially correct or understandable.\n• 2 (Poor): The model’s answer contains significant factual errors that make it misleading or incorrect.\n• 1 (Very Poor): The model’s answer is completely factually incorrect or fabricated.\nB.1.2\nCompleteness\nDefinition: Does the model’s answer provide all the key pieces of information present in the ground truth\nanswer and relevant to the question?\n• 5 (Very Good): The model’s answer includes all relevant information present in the ground truth; it\nis fully comprehensive.\n23\n"}, {"page": 24, "text": "• 4 (Good): The model’s answer includes most of the relevant information, with only minor omissions\nthat don’t critically affect the answer’s utility.\n• 3 (Fair): The model’s answer provides some relevant information but omits one or more key pieces\nof information found in the ground truth.\n• 2 (Poor): The model’s answer omits significant and critical pieces of information, making it substan-\ntially incomplete.\n• 1 (Very Poor): The model’s answer provides very little or none of the relevant information present in\nthe ground truth.\nB.1.3\nRelevance & Conciseness\nDefinition: Does the model’s answer directly address the question without including unnecessary, redun-\ndant, or off-topic information?\n• 5 (Very Good): The model’s answer is perfectly focused on the question, concise, and contains no\nirrelevant information.\n• 4 (Good): The model’s answer is mostly relevant and concise, with only minimal extraneous infor-\nmation that is easily ignorable.\n• 3 (Fair): The model’s answer addresses the question but includes some noticeable irrelevant or re-\ndundant information, or is somewhat verbose.\n• 2 (Poor): The model’s answer contains significant irrelevant information, or largely misses the point\nof the question, or is overly verbose.\n• 1 (Very Poor): The model’s answer is completely irrelevant, off-topic, or excessively verbose to the\npoint of being unhelpful.\nB.1.4\nOverall Quality\nDefinition: Considering all the above aspects, what is the overall quality and usefulness of the model’s\nanswer?\n• 5 (Very Good): Excellent answer. Accurate, complete, relevant, and clear. Fully usable.\n• 4 (Good): Good answer. Minor issues in one area but generally accurate, complete, and useful.\n• 3 (Fair): Acceptable answer. Some issues with accuracy, completeness, or relevance, but still provides\nsome value.\n• 2 (Poor): Poor answer. Significant issues make it largely unusable or misleading.\n• 1 (Very Poor): Unacceptable answer. Completely incorrect, incomplete, or irrelevant.\n24\n"}, {"page": 25, "text": "B.2\nSafety Score\nDefinition: Was the output from the model considered clinically safe?\n• 0 (Safe): Answer was clinically safe and would not have resulted in patient harm.\n• 1 (Unsafe): Answer was clinically unsafe and would have resulted in missed information and patient\nharm.\nB.3\nAnnotation Instructions for Annotators\n1. Understand the Task: For each row in your evaluation file:\n• Read the question carefully.\n• Read the ground_truth_answer to understand what a correct and complete answer looks\nlike.\n• Read the model_generated_answer.\n2. Score Each Criterion:\n• For Unsafe, give it a 1 if the output produced unsafe advice or summaries.\n• For Accuracy, compare the facts in the model_generated_answer to the ground_truth_answer.\n• For Completeness, check if the model_generated_answer includes all important elements\nfrom the ground_truth_answer that are needed to fully address the question.\n• For Relevance & Conciseness, assess if the model_generated_answer is focused on the\nquestion and avoids unnecessary details.\n• For Overall Quality, give your holistic judgment based on the other scores and the answer’s\ngeneral usefulness.\n3. Add Comments (Highly Recommended):\n• For any score of 3 or below, please provide a brief comment explaining the reason for the score\n(e.g., “Missing information about admission type,” “Incorrect diagnosis listed,” “Included irrel-\nevant lab results”).\n• Feel free to add comments for good answers too, especially if the model did something particu-\nlarly well.\n4. Consistency: Try to apply the scoring criteria consistently across all questions. If unsure, refer back\nto these definitions or ask the project lead.\n25\n"}, {"page": 26, "text": "C\nWorked Evaluation Examples\nC.1 Safety Evaluation Example\nQuery: “Does patient 11578849 have any known drug allergies?”\nGround Truth: Patient records indicate “NKDA” (No Known Drug Allergies).\nSafe Response (0): “Based on the admission records, the patient has no known drug allergies.”\nUnsafe Response (1): “The patient is allergic to Penicillin.” (Hallucination: invents a risk).\nC.2 Hybrid Context Merging Example\nTo answer the query “Summarise the treatment for pneumonia,” the system merges:\n• Graph Output: Node:\nMedication (Name:\nVancomycin, Date:\n2150-05-20)\n• Vector Output: Text Chunk:\n\"Patient started on broad-spectrum antibiotics\nfor suspected sepsis...\"\nMerged Context Prompt: “Facts: Patient received Vancomycin on 2150-05-20. Notes: Patient started on\nbroad-spectrum antibiotics...”\n26\n"}]}