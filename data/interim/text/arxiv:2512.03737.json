{"doc_id": "arxiv:2512.03737", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.03737.pdf", "meta": {"doc_id": "arxiv:2512.03737", "source": "arxiv", "arxiv_id": "2512.03737", "title": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation", "authors": ["Chuyue Wang", "Jie Feng", "Yuxi Wu", "Hang Zhang", "Zhiguo Fan", "Bing Cheng", "Wei Lin"], "published": "2025-12-03T12:34:47Z", "updated": "2025-12-03T12:34:47Z", "summary": "Accurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce \\textbf{AR-Med}, a novel framework for \\textbf{A}utomated \\textbf{R}elevance assessment for \\textbf{Med}ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\\%, a 24\\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.03737v1", "url_pdf": "https://arxiv.org/pdf/2512.03737.pdf", "meta_path": "data/raw/arxiv/meta/2512.03737.json", "sha256": "ac3fa7b0e905c3d61cf64c9f77c5c22488dce4375c523bb451d0df556a73a223", "status": "ok", "fetched_at": "2026-02-18T02:25:36.797333+00:00"}, "pages": [{"page": 1, "text": "AR-Med: Automated Relevance Enhancement in Medical Search\nvia LLM-Driven Information Augmentation\nChuyue Wang1, Jie Feng2, Yuxi Wu3, Hang Zhang1, Zhiguo Fan1, Bing Cheng1, Wei Lin1\n1Meituan Inc., Beijing, China\n2Tsinghua University, Beijing, China\n3Independent Researcher, Beijing, China\nfengj12ee@hotmail.com\nABSTRACT\nAccurate and reliable search on online healthcare platforms is criti-\ncal for user safety and service efficacy. Traditional methods, how-\never, often fail to comprehend complex and nuanced user queries,\nlimiting their effectiveness. Large language models (LLMs) present\na promising solution, offering powerful semantic understanding\nto bridge this gap. Despite their potential, deploying LLMs in this\nhigh-stakes domain is fraught with challenges, including factual hal-\nlucinations, specialized knowledge gaps, and high operational costs.\nTo overcome these barriers, we introduce AR-Med, a novel frame-\nwork for Automated Relevance assessment for Medical search\nthat has been successfully deployed at scale on the Online Medi-\ncal Delivery Platforms. AR-Med grounds LLM reasoning in veri-\nfied medical knowledge through a retrieval-augmented approach,\nensuring high accuracy and reliability. To enable efficient online\nservice, we design a practical knowledge distillation scheme that\ncompresses large teacher models into compact yet powerful student\nmodels. We also introduce LocalQSMed, a multi-expert annotated\nbenchmark developed to guide model iteration and ensure strong\nalignment between offline and online performance. Extensive ex-\nperiments show AR-Med achieves an offline accuracy of over 93%,\na 24% absolute improvement over the original online system, and\ndelivers significant gains in online relevance and user satisfaction.\nOur work presents a practical and scalable blueprint for develop-\ning trustworthy, LLM-powered systems in real-world healthcare\napplications.\n1\nINTRODUCTION\nIn the realm of healthcare, accurate pharmaceutical search and rec-\nommendation are paramount, as they directly impact public health\nand safety. With the proliferation of online platforms, medications\nand medical products are readily available for purchase anytime\nand anywhere, heightening the need for precise relevance matching\nto prevent misuse, adverse effects, or ineffective treatments.\nTraditional search and recommendation methods, primarily based\non small-scale deep learning models, face significant limitations\nin pharmaceutical domains. These methods include collaborative\nfiltering (e.g., Slope One, user/item similarity [33, 35]), content-\nbased filtering (e.g., keyword matching, semantic analysis [21]),\nand hybrid approaches like knowledge graph-based systems [9].\nThey struggle with: (1) limited comprehension, failing to handle\nambiguous queries like \"wind-cold common cold\" or \"001\" [3]; (2) re-\nliance on costly expert-curated knowledge, increasing maintenance\ncosts [5]; and (3) slow adaptation to new drugs, regulations, and\nuser demands, leading to outdated relevance [29]. These challenges\nhighlight the need for more robust solutions.\nLarge language models (LLMs), on the other hand, hold immense\npotential to revolutionize this field through their advanced natural\nlanguage understanding, commonsense reasoning, and zero/few-\nshot learning abilities [24, 36, 38]. These capabilities enable LLMs\nto delve deeper into user query intents and the intrinsic logic of\nmedical texts, thereby substantially enhancing the quality of rele-\nvance assessments. For instance, in the domain of natural language\nunderstanding, models like GPT-4 have demonstrated expert-level\nperformance on medical question-answering tasks, such as diagnos-\ning complex cases or interpreting clinical queries, by leveraging vast\npre-trained knowledge to achieve accuracies comparable to human\nspecialists on benchmarks like USMLE [23]. In commonsense rea-\nsoning, techniques such as chain-of-thought prompting allow LLMs\nto break down multi-step problems, improving arithmetic, symbolic\nreasoning, and logical inference in diverse scenarios, as shown in\nstudies on PaLM models [39]. Zero/few-shot learning further em-\npowers LLMs to generalize to unseen tasks with minimal examples;\nfor example, models like GPT-3 excel in translation, code genera-\ntion, and multitask adaptation without task-specific training [26],\nwhile specialized variants like CodeGen handle program synthesis\nin coding domains [22]. Across various fields [6, 7, 14, 15, 31, 37, 40],\nLLMs have shown versatility: in e-commerce search, they enhance\nrelevance judgments for product queries [34]; in biomedical appli-\ncations, models like HuaTuo and BiomedGPT fine-tuned on Chi-\nnese medical knowledge enable accurate diagnosis and knowledge\nretrieval [37, 40]; and in tool-augmented settings, Toolformer self-\nteaches API usage for tasks like calculation or search integration\n[32]. These examples illustrate how LLMs’ emergent abilities, such\nas those in BLOOM’s multilingual processing [31] or Llama 3’s\nbroad cognitive tasks [8], drive innovations in knowledge-intensive\napplications.\nHowever, LLMs are not without drawbacks: they often lack\ndomain-specific professional knowledge, are prone to severe halluci-\nnations that can lead to erroneous medical advice, and impose high\napplication costs in terms of latency and computational resources,\nparticularly in high-throughput industrial scenarios like daily bil-\nlions of search requests on platforms such as the Online Medical\nDelivery Platforms, a leading local services provider. For example,\ndespite strong general capabilities, LLMs like GPT-4 may produce in-\naccurate outputs in specialized medical contexts due to insufficient\ndomain grounding, as evidenced in evaluations showing persistent\nerrors in clinical reasoning [20]. Hallucinations—fabricating plau-\nsible but false information—remain a critical issue, with surveys\narXiv:2512.03737v1  [cs.CL]  3 Dec 2025\n"}, {"page": 2, "text": ", ,\nChuyue Wang, et al.\nFigure 1: An illustration of the proposed framework, encompassing three key components: Knowledge Retrieval, Expert Rule\nGuidance, and Cross-modal Information Matching.\nhighlighting risks in high-stakes domains like healthcare [42, 43].\nMoreover, the enormous scale of models such as PaLM 2 (up to\n540B parameters) [4] or GPT-NeoX-20B [2] demands massive com-\npute resources for training and inference, leading to inefficiencies\nin deployment, as noted in compute-optimal analyses [10] and\nefforts to mitigate via optimizations like DeepSpeed [27]. These\nlimitations underscore the need for hybrid approaches, such as\nretrieval-augmented generation (RAG) [16], to complement LLMs\nin practical settings.\nTo harness the semantic understanding and reasoning strengths\nof LLMs while mitigating their weaknesses—namely, insufficient\nspecialized knowledge and hallucination risks—we propose a com-\nprehensive LLM-based framework tailored for medical search and\nrecommendation relevance. Building on the Online Medical De-\nlivery Platforms’s diverse and complex real-world scenarios, our\napproach addresses these challenges through three key innovations.\nFirst, we construct a retrieval-augmented framework that deeply\nintegrates LLMs’ reasoning capabilities with external, trustworthy\nmedical knowledge bases. This framework employs precise knowl-\nedge retrieval, expert-guided rules, and cross-modal information\nmatching to furnish LLMs with verified contextual information for\ndecision-making, thereby anchoring outputs to ensure profession-\nalism, accuracy, and reduced risks in medical relevance judgments.\nSecond, we design a knowledge distillation scheme from large\nteacher models to compact student models, incorporating offline ac-\ncumulation of high-confidence online data and targeted distillation\nlearning strategies, to achieve efficient deployment without sacri-\nficing performance. Third, we introduce LocalQSMed benchmark,\na comprehensive offline benchmark annotated by multiple experts,\nencompassing user search queries and product names. This bench-\nmark guides iterative model and system updates offline, providing\nreliable references for online service enhancements and enabling\ncorrect, data-driven iterations. In summary, our contributions are\nas follows:\n• We propose AR-Med, a novel framework for LLM-based auto-\nmatic relevance assessment in medical search and recommenda-\ntion, which integrates large language models with professional\nmedical knowledge to achieve reliable, generalizable, and effi-\ncient relevance enhancement for medical search.\n• We design a retrieval-augmented framework that fuses LLMs’\nreasoning with external medical knowledge bases via precise\nretrieval, expert rules, and cross-modal matching, anchoring\noutputs for professional accuracy.\n• We propose a knowledge distillation framework for efficient\nteacher-to-student model transfer, enabling high-performance\nonline inference. Besides, a multi-expert annotated benchmark\nis introduced to evaluate offline progress, ensure consistency\nwith online results, and guide effective model iteration.\n• Extensive validations on the Online Medical Delivery Plat-\nforms’s show over 93% offline accuracy (24% above the 69%\nbaseline), with significant gains in online relevance, user satis-\nfaction, and efficiency.\n2\nMETHODS\nOur AR-Med approach comprises three primary components, as\nillustrated in Figure 1. First, we introduce a retrieval-augmented\ngeneration framework that integrates LLMs’ reasoning capabilities\nwith external medical knowledge, expert rules, and multimodal ver-\nification to enhance accuracy and mitigate hallucinations. Second,\nwe implement a knowledge distillation scheme leveraging accu-\nmulated real-world data to transfer expertise from large teacher\nmodels to efficient student models, ensuring cost-effective deploy-\nment. The subsequent sections elaborate on each component in\ndetail. Finally, we establish the LocalQSMed benchmark to facil-\nitate systematic offline evaluation and iterative optimization of\npharmaceutical relevance assessment. We formalize the complete\nhierarchical decision logic—integrating precise knowledge retrieval,\nexpert rule guidance, and fine-grained relevance discrimination—in\nAlgorithm 1. Our work presents a practical and scalable blueprint\nfor developing trustworthy, LLM-powered systems in real-world\nhealthcare applications. The detailed specifications of the models\nemployed in our framework, including their specific functions and\ninput/output formats, are summarized in Table 1.\n"}, {"page": 3, "text": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation\n, ,\nTable 1: Default models and their roles in the AR-Med framework\nComponent Model\nFunction\nInput\nOutput\nQuery\nQwen3-0.6B-emb\nSimilarity Calculation\nQuery + web info (e.g., “001” →car model)\nSimilarity score (e.g., 0.603)\nQwen3-0.6B (SFT)\nConsistency Judgment\nQuery + web info\nConsistency result (e.g., inconsistent)\nQwen3-32B\nNER\nFiltered query + extended info + rules\nEntities (brand, efficacy, category, etc.)\nSPU\nQwen2.5-7B-vl\nDetect Cheating in Product Names SPU name + product images\nCheating flag (e.g., traffic hijacking)\nQwen3-32B\nStandardize Product Name\nOriginal SPU + cheating result\nClean standardized SPU name\nQwen3-32B + Qwen2.5-7B-vl Extend Product Information\nStd. SPU + images + CPV info\nRich attributes (efficacy, usage, etc.)\n2.1\nPreliminaries\nHere, we present the fundamental definitions of the core entities in\nour problem space.\n• Query: user’s raw search input. These are derived from authen-\ntic user behavior and can be highly varied, ranging from specific\nproduct names (e.g., \"Ganmaoling Granules\"), to product codes\n(\"001\"), or general needs (\"anti-dandruff shampoo\").\n• Standard Product Unit (SPU): a standardized product entry\nin our catalog, sourced from merchant-uploaded information.\nOur product catalog is extensive, encompassing categories such\nas Chinese and Western medicines, nutritional supplements,\nand health-focused personal care products.\n2.2\nRetrieval-Augmented Framework\nTo leverage LLMs’ semantic understanding while addressing their\nlimitations in domain knowledge and hallucinations, we design a\nRAG framework that fuses LLMs with verified external contexts.\nThis framework operates end-to-end, optimizing inputs from both\nquery and SPU sides via precise knowledge retrieval, expert-guided\nrules, and cross-modal matching, thereby anchoring LLM outputs\nfor professional, accurate relevance judgments.\nTable 2: Bad cases caused by overthinking\nprompt\nYou are an expert in relevance judgment...\nUser query: PiKangWang.\nUser query interpretation result: ...\nProduct SPU: [PiKangWang] Clotrimazole Cream\n10g/bottle Baibang Qiling Antibacterial Ointment.\nProduct SPU extended information: ...\nBasic principles: ...\nThe output result format is: <result>xxx</result>\noutput\n<think>Okay, I now need to...\nAccording to the rules, if the product name contains\nthe brand from the query and the efficacy is similar,\nit is considered highly relevant. However, in this case,\nthe product’s efficacy is antibacterial, while the user’s\nquery is for antifungal, which may belong to different\ncategories.\nTherefore, it may be judged as low relevance. The\nfinal conclusion may be low relevance.</think>\n2.2.1\nPrecise Knowledge Retrieval. In practical settings, user queries\nare typically concise and multifaceted, complicating relevance as-\nsessment with generic LLMs alone. Traditional methods rely on\nquery segmentation and weighted intent matching, but LLMs offer\nsuperior natural language comprehension. However, in medicine,\nabbreviations and jargon (e.g., \"wind-cold common cold\") can lead\nto incomplete expansions if relying solely on LLMs, potentially\nmissing key intents like associated medication names.\nTo mitigate this, we implement a two-stage information augmen-\ntation and filtering mechanism using external online sources. First,\nwe enhance queries by appending category-specific keywords (e.g.,\ntransforming \"001\" to \"001 adult product\") via instruction-tuned\n0.6B models, improving retrieval relevance from online searches\nthat might otherwise yield irrelevant results (e.g., cars or music for\n\"001\"). This step ensures broader, contextually accurate coverage.\nSubsequently, we filter retrieved information to eliminate noise:\ninitial relevance scoring removes low-correlation content, followed\nby a lightweight model trained on a multi-category dataset for fine-\ngrained consistency checks. As illustrated in Table 2, this prevents\n\"overthinking\" in larger models, which can inflate latency and errors.\nPost-filtering, we extract core elements—alternative expressions,\nkeywords, topics, hierarchies, and intent judgments—restructuring\nthem into concise inputs for downstream LLM processing, thus\nenhancing efficiency and intent fidelity.\n2.2.2\nExpert Rule Guidance. Relevance criteria must adapt dynam-\nically to inferred user intents, as static rules falter in diverse scenar-\nios. For generic queries like \"001\" (spanning adult products across\nbrands), brand matching is deprioritized; conversely, for specifics\nlike \"Yunnan Baiyao Aerosol\", brand and dosage form are pivotal,\ndisqualifying mismatches like \"Yunnan Baiyao Ointment\".\nOur RAG expert rule recall module addresses this by identifying\nentities (e.g., brand, dosage form, ingredients, region) post-query\nrewriting. Matched entities trigger tailored rules from a medical\nknowledge base, enabling fine-grained, scenario-specific assess-\nments. This integration with LLMs ensures personalized relevance,\nboosting adaptability in high-volume environments like Online\nMedical Delivery Platforms’s billions of daily queries.\n2.2.3\nCross-Modal Information Matching. Product name standard-\nization is fraught with inconsistencies, exacerbated by merchant\nmanipulations for traffic gains. Text alone is easily altered, so we\nincorporate multimodal verification using product images to detect\ncheating, such as misleading names.\nWe extract key attributes (name, specifications, type, users, func-\ntions) from images via multimodal models, cross-matching them\nagainst textual SPUs to normalize descriptions and flag discrepan-\ncies. This enhances robustness, preventing hijacking and ensuring\nreliable inputs for final relevance scoring.\n2.3\nKnowledge Distillation Scheme\nTo enable efficient deployment, we distill knowledge from large\nteacher models to compact student models using accumulated real-\nworld data for training. This scheme balances performance with\n"}, {"page": 4, "text": ", ,\nChuyue Wang, et al.\nTable 3: Category Distribution in the Benchmark\nFirst Category Name\nSamples\nChinese and Western Medicines\n2820\nMedical Devices (Pharmacy)\n629\nBeauty and Personal Care (Pharmacy)\n241\nNutrition and Health Products\n195\nAdult Products (Pharmacy)\n180\nHealth and Wellness\n72\nOthers\n263\nlow latency and cost in industrial settings. We fine-tune Qwen3-\n0.6B as the student model, distilling from a Qwen3-32B teacher. The\nteacher generates diverse inferences via multi-prompt voting for\nstable labels, which the student mimics through supervised fine-\ntuning. This yields performance comparable to 70B-scale models\nin a fraction of the size, reducing inference costs while preserving\naccuracy in relevance assessments.\nWe curate high-confidence samples from Online Medical De-\nlivery Platforms’s online traffic, including good cases (accurate\nmatches) and bad cases (misclassifications or hallucinations). These\nare sampled from billions of daily queries, focusing on imbalanced\ndistributions and edge scenarios. Multi-expert validation refines\nlabels, creating a robust dataset for distillation that informs offline\niterations and bridges real-world gaps. In pharmaceutical search\nand recommendation relevance tasks, online inference and offline\nevaluation serve distinct yet complementary roles. Online systems\nmust respond to user requests in extremely short time frames (typi-\ncally at the k×10 ms level), imposing strict constraints on inference\nlatency. This limits model size and usable information, creating per-\nformance ceilings for handling complex pharmaceutical scenarios,\nlong texts, or multimodal data, and hindering global optimality.\nConversely, offline evaluation acts as a \"supervisor\" for relevance\ntasks. Using the benchmark, we leverage comprehensive search logs,\nreal user interactions, and expert annotations to build high-quality\nevaluation sets. Without latency or resource constraints, offline\nenvironments allow flexible deployment of large pre-trained models,\nmultimodal integration (e.g., text, images, structured attributes),\nand domain-specific fine-tuning. This enables thorough assessment\nof online models across sub-scenarios, uncovering long-tail issues\nand risks elusive to online systems.\nCrucially, the accumulated high-quality samples and diagnos-\ntic data from offline evaluations provide a foundation for ongoing\nonline model optimization. These can train and distill efficient\nlightweight models, allowing online systems to approximate offline\nperformance levels. Additionally, offline results offer targeted im-\nprovements for specific drugs, diseases, and indications, fostering\nintelligent, refined development of pharmaceutical search and rec-\nommendation systems. In essence, the offline benchmark not only\nfunctions as the \"gold standard\" for evaluation but also drives con-\ntinuous advancement of online systems, striking a balance between\neffectiveness and efficiency.\n2.4\nBenchmark: LocalQSMed\nIn this section, we present the LocalQSMed dataset, constructed\nby randomly sampling high-frequency online data alongside error-\nprone cases (bad cases), resulting in a total of 4,400 query-SPU pairs.\nAlgorithm 1 Hierarchical Relevance Judgment in AR-Med\nRequire: User query, SPU (text + images)\nEnsure: ⟨result⟩level⟨/result⟩\n1: rewrite ←Rewrite(query) {Query expansion and category tag-\nging}\n2: entities ←NER(rewrite) {Extract core name, category, efficacy,\netc.}\n3: spu_info ←ParseSPU(spu) {Brand, efficacy, dosage form, etc.}\n4: if spu.name fully contains query and efficacy matches then\n5:\nreturn ⟨result⟩Highly Relevant⟨/result⟩\n6: else if all chars of query in spu.name and efficacy matches\nthen\n7:\nreturn ⟨result⟩Highly Relevant⟨/result⟩\n8: else if core name match and efficacy match and category\nmatch then\n9:\nreturn ⟨result⟩Highly Relevant⟨/result⟩\n10: else if efficacy consistent but any core attribute mismatched\nthen\n11:\nreturn ⟨result⟩Moderately Relevant⟨/result⟩\n12: else if efficacy similar or complementary usage exists then\n13:\nreturn ⟨result⟩Weakly Relevant⟨/result⟩\n14: else\n15:\nreturn ⟨result⟩Irrelevant⟨/result⟩\n16: end if\nThis benchmark serves as a foundational tool for offline method\niteration, enabling rigorous evaluation of relevance in pharmaceu-\ntical search and recommendation scenarios. Table 3 presents the\ndetailed distribution of LocalQSMed across categories.\nReal-world user queries are frequently colloquial, ambiguous,\nand laden with homophones or slang in medical contexts, posing\nsignificant challenges for intent disambiguation. For instance, a\nquery like \"wind-cold common cold\" could refer to the disease itself\nor imply a search for \"Wind-cold Granules\" medication. Determin-\ning whether the most relevant result should be \"[Quick] Compound\nParacetamol and Amantadine Capsules\" or \"[Yunnan Baiyao] Wind-\ncold Granules\" requires precise intent recognition and risk-aware\nrecommendation, which is critical in medical settings to avoid po-\ntential health hazards.\nSPUs are sourced from merchant-uploaded standard product\nnames. However, merchants often engage in \"traffic hijacking\"\nthrough misleading nomenclature. For example, a product titled\n\"Wuxing Jianpi Gao Pian Wan Tongren Raw Material Zhongjing\nYufang Tang\" exploits the popularity of \"Tongrentang Jianpi Wan\".\nConventional machine learning and deep learning approaches are\nvulnerable to such manipulations due to over-reliance on keyword\nmatching, particularly with terms like \"Tongrentang\". To address\nthis, each SPU is augmented with 10-15 merchant-uploaded product\nimages, encompassing diverse angles, ingredient lists, and actual\nproduct labels, providing multimodal cues for verification.\nThe LocalQSMed dataset was annotated by a team of 20 pro-\nfessionally trained medical experts, ensuring high-quality labels\nthrough rigorous validation. Relevance rules are dynamically ad-\njusted based on reported bad cases and drug categories to adapt to\nevolving scenarios. Specifically: (1) High relevance occurs when\n"}, {"page": 5, "text": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation\n, ,\nTable 4: Results of different LLMs within the proposed framework on the LocalQSMed benchmark. Here, \"ALL\" denotes the\nentire benchmark, \"HARD\" refers to the challenging subset, and \"w/o FAILED\" indicates results after excluding failed examples.\nLLM\nLoss Rate\nHighly Relevant\nLess Relevant\nIrrelevant\nALL\nw/o FAILED HARD\nPrecision Recall\nF1\nPrecision Recall\nF1\nPrecision Recall\nF1\nAccuracy\nAccuracy\nAccuracy\nOriginal System\n0.00%\n0.9760\n0.7031 0.8174\n0.0756\n0.5984 0.1343\n0.0753\n0.2245 0.1128\n0.6928\n-\n-\nQwen3-0.6B\n6.40%\n0.9307\n0.8227 0.8734\n0.0877\n0.0246 0.0385\n0.1333\n0.0357 0.0563\n0.7629\n0.7867\n0.2784\nQwen3-4B\n0.00%\n0.9541\n0.9741 0.9640\n0.2602\n0.1475 0.1882\n0.4480\n0.4706 0.4590\n0.9206\n0.9168\n0.5294\nQwen3-8B\n0.00%\n0.9553\n0.9712 0.9632\n0.3196\n0.1429 0.1975\n0.3647\n0.5210 0.4291\n0.9191\n0.9190\n0.5490\nQwen3-14B\n0.02%\n0.9647\n0.9594 0.9621\n0.2886\n0.1982 0.2350\n0.3810\n0.6723 0.4863\n0.9148\n0.9092\n0.6106\nQwen3-32B\n0.00%\n0.9723\n0.9326 0.9520\n0.2400\n0.3041 0.2684\n0.3565\n0.6891 0.4699\n0.8956\n0.8993\n0.6611\nLlama-3.2-1B\n63.67%\n0.9116\n0.8968 0.9041\n0.0270\n0.0235 0.0252\n0.0000\n0.0000 0.0000\n0.8226\n0.8534\n0.2913\nLlama-3.2-3B\n0.00%\n0.9143\n0.9324 0.9232\n0.0217\n0.0116 0.0152\n0.0417\n0.0217 0.0286\n0.8565\n0.8862\n0.3358\nLlama-3.1-8B\n63.55%\n0.9151\n0.9453 0.9300\n0.1591\n0.0769 0.1037\n0.0000\n0.0000 0.0000\n0.8675\n0.8873\n0.3504\nLlama-3.3-70B-Instruct\n46.86%\n0.9296\n0.9479 0.9386\n0.0625\n0.0472 0.0538\n0.0784\n0.0556 0.0650\n0.8787\n0.9037\n0.3737\nDeepSeek-R1-Distill-Qwen-7B\n17.59%\n0.9513\n0.2173 0.3537\n0.1622\n0.0357 0.0585\n0.3000\n0.0303 0.0550\n0.2040\n0.1488\n0.0699\nDeepSeek-R1-Distill-Qwen-14B\n0.00%\n0.9693\n0.9658 0.9658 0.3492 0.3041 0.3251 0.4569\n0.4492 0.4530\n0.9232\n0.9125\n0.5758\nDeepSeek-R1-Distill-Llama-70B\n43.68%\n0.9293\n0.9302 0.9297\n0.0833\n0.0541 0.0656\n0.0550\n0.0750 0.0635\n0.8626\n0.9004\n0.3598\nbrand, dosage form, and other key attributes are completely identi-\ncal (e.g., exact matches in product specifications); (2) Low relevance\n(or weakly relevant) applies to cases with different brands or dosage\nforms but some overlapping utility; (3) Irrelevant is assigned when\nefficacy is completely different or the target audience diverges en-\ntirely, preventing mismatches in critical medical contexts.\n3\nEXPERIMENTS\n3.1\nSettings\nBaselines. Our primary baseline is the Original System, which\nwas the production search relevance model in use on the Online\nMedical Delivery Platform prior to the implementation of AR-Med.\nThis system is built upon a sophisticated BERT-based deep learning\nmodel, representing a strong, modern baseline that captures deep\nsemantic relationships between user queries and product informa-\ntion. However, despite its capabilities, this model’s knowledge is\nstatic and encapsulated entirely within its parameters. This makes\nit less adaptable to newly emerging medical terms or evolving phar-\nmaceutical information without extensive and time-consuming re-\ntraining. Its performance is constrained by the knowledge present\nin its training data, leading to a baseline accuracy of 69.28% on\nour LocalQSMed benchmark. This system serves as a strong real-\nworld point of comparison to rigorously evaluate the significant\nadvancements in adaptability and fact-grounding brought by our\nLLM-driven AR-Med framework.\nModels. For our experiments, we primarily utilize a suite of\nQwen and Llama models, including Qwen3-0.6B, Qwen3-4B, Qwen2.5-\n7B-Instruct, Qwen2.5-VL-7B-Instruct, Qwen3-14B, Qwen2.5-VL-\n32B-Instruct, Qwen3-32B, Qwen2-72B-Instruct, Llama-3.2-1B, Llama-\n3.2-3B, Llama-3.1-8B, Llama-3.3-70B-Instruct, DeepSeek-R1-Distill-\nQwen-7B, DeepSeek-R1-Distill-Qwen-14B, and DeepSeek-R1-Distill-\nLlama-70B. All models are deployed locally to ensure efficient and\nconsistent evaluation. For batch inference, we employ the vLLM\nframework [13] with 24xA100-80G GPUs, which enables high-\nthroughput and scalable local inference.\nEvaluation Metrics. For evaluation, we focus on accuracy,\nrecall, and F1-score across three relevance categories: high rele-\nvant, less relevant, and irrelevant. These metrics are computed\nbased on the model’s predictions on our benchmark dataset. All\nexperiments are conducted in a controlled local environment to\nensure reproducibility and fair comparison among models.\n3.2\nMain Results on LocalQSMed\nWe selected open-source large language models from the Qwen,\nLlama, and DeepSeek series to systematically evaluate our method’s\neffectiveness on the benchmark dataset, which are shown in Table 4.\nThe original online system achieves an accuracy of only 0.6928.\nOur proposed method with LLM enhancement outperforms it in\nmost cases, with only DeepSeek-Qwen-7B failing to surpass the\nbaseline, demonstrating the effectiveness of the approach. The best-\nperforming models achieve accuracies approaching 0.92, represent-\ning an improvement of over 30% compared to the original system.\nAmong them, the Qwen series exhibits balanced performance\nand stable outputs across all categories, with discrepancies from\nhuman annotations in the \"Highly Relevant\" category at just 3%-5%.\nNotably, while other models show lower overall Recall in \"Less\nRelevant\" categories, Qwen3-32B achieves a 68.91% Recall in \"Less\nRelevant.\" The Llama series performs slightly worse overall than\nQwen. It attains high Precision and Recall in \"Highly Relevant,\" but\nF1 scores in \"Less Relevant\" and \"Irrelevant\" categories are markedly\nlower than Qwen’s. Some distilled DeepSeek models (e.g., DeepSeek-\nR1-Distill-Qwen-7B) experience significant sample losses, resulting\nin reduced accuracy and F1 scores across categories.\n3.3\nPerformance on Hard and Common Subsets\nDue to result losses caused by batch inference and model-specific\nperformance issues, we introduced a new evaluation metric, w/o\nFAILED, which indicates results after excluding failed examples.\nAmong the models, Qwen3-32B shows only a 2.72% difference from\nhuman annotations in \"Highly Relevant\" precision, and achieves the\nbest recall of 74.07% in the \"Less Relevant\" category, making it the\ntop-performing model with a 51.62% improvement over the original\nsystem. Results for lossless data are provided in the appendix.\nGiven the inherent sample imbalance in the benchmark, we con-\nstructed a more challenging hard dataset by applying specific rules\n(selecting samples with longer SPUs and shorter queries, which\n"}, {"page": 6, "text": ", ,\nChuyue Wang, et al.\nTable 5: Ablation study on the proposed framework. Each row represents the progressive addition of a functional module to\nthe base model (base (Q+S)). Abbreviations are as follows: Q (Query), S (SPU, Standard Product Unit), QI (Query Information\naugmentation), SI (SPU Information augmentation), IS (Internet Search), ISPI (Internet Search Page Identical check), TF (Two-\nFilter mechanism), CN (Common Name identification), and QC (Query Change detection).\nModule\nHighly Relevant\nLess Relevant\nIrrelevant\nALL\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nAccuracy\nbase (Q+S)\n0.9735\n0.9050\n0.9380\n0.2317\n0.6406\n0.3403\n0.5926\n0.1345\n0.2192\n0.8717\nbase+QI+S\n0.9683\n0.9386\n0.9532\n0.2860\n0.5760\n0.3823\n0.7308\n0.1597\n0.2621\n0.9003\nbase+Q+SI\n0.9638\n0.9591\n0.9615\n0.2832\n0.4444\n0.3459\n0.7500\n0.1008\n0.1778\n0.9114\nbase+QI+SI\n0.9688\n0.9320\n0.9500\n0.2507\n0.3917\n0.3058\n0.4510\n0.5847\n0.5092\n0.8966\nbase+QI+IS+SI\n0.9576\n0.9819\n0.9696\n0.3697\n0.2811\n0.3194\n0.6515\n0.3613\n0.4649\n0.9314\nbase+QI+IS+ISPI+SI\n0.9620\n0.9550\n0.9585\n0.2901\n0.2166\n0.2480\n0.3971\n0.6807\n0.5015\n0.9119\nbase+QI+IS+ISPI+TF+SI\n0.9595\n0.9857\n0.9725\n0.4203\n0.2673\n0.3268\n0.6905\n0.4874\n0.5714\n0.9376\nbase+QI+IS+CN+ISPI+TF+SI\n0.9675\n0.9567\n0.9621\n0.2629\n0.2120\n0.2347\n0.3768\n0.6555\n0.4785\n0.9126\nbase+QI+QC+CN+IS+ISPI+TF+SI\n0.9723\n0.9326\n0.9520\n0.2400\n0.3041\n0.2684\n0.3565\n0.6891\n0.4699\n0.8956\nare prone to vendor cheating and ambiguous intent) to filter out\ndifficult-to-classify cases, thereby achieving a balanced 1:1:1 ratio\namong \"Highly Relevant,\" \"Less Relevant,\" and \"Irrelevant\" sam-\nples. In this rigorous scenario, both the Qwen and Llama series\ndemonstrated significant performance improvements as model size\nincreased, with Qwen3-32B achieving the best overall accuracy,\nfurther validating the effectiveness of our method across different\ncapacities. Interestingly, DeepSeek 70B performed worse than the\n14B model, possibly due to differences in the underlying architec-\nture, highlighting that, in addition to model size, the foundation and\ndesign of the model are also critical to successful online deployment.\nResults on the HARD dataset are provided in the appendix.\n3.4\nResults Analysis Across Categories\nTo investigate the impact of different product categories on rele-\nvance prediction, we evaluated the model’s performance based on\nprimary product classifications, as illustrated in Figure 2. The bar\nchart shows the overall precision of the model, while the line graph\ndepicts accuracy variations across various product categories. In to-\ntal, nearly 20 categories are covered, encompassing a diverse range\nof pharmaceutical and related products, such as over-the-counter\ndrugs, prescription medications, medical devices, nutritional sup-\nplements, and adult health products. To highlight key trends and\navoid overly complex visualizations, we focused on the top five\ncategories by sample volume. Detailed numerical results for all cat-\negories, including breakdowns of Precision, Recall, and F1 scores,\nare provided in the appendix for comprehensive reference.\nOverall, the accuracy trends across different categories are con-\nsistent with the global accuracy, with only minor deviations, in-\ndicating that our method is effective and stable across different\nproduct types. For example, Qwen3-32B achieves recall rates ex-\nceeding 90% in high-volume categories such as “Chinese and West-\nern Medicines,” “Medical Devices,” and “Adult Products,” demon-\nstrating strong robustness in handling ambiguous queries common\nin these domains, such as colloquial expressions or homophones.\nEven in challenging categories like “Adult Products,” where user\nintent can be highly implicit (e.g., numeric product codes like “001”),\nthe method still performs stably, reflecting strong generalization\ncapabilities. This consistency suggests that the RAG framework\nand knowledge distillation effectively mitigate category-specific\nbiases, such as differences in terminology complexity or merchant\nmanipulation, ensuring reliable relevance judgments in real-world\npharmaceutical search scenarios.\nFigure 2: Per-Category results on highly relevant subset.\n3.5\nAblation Study\nIn this section, we conducted a series of ablation studies to systemat-\nically evaluate the specific contributions of each functional module\nto the overall performance of our model. Results are presented in\nTable 5. Starting from the baseline model (base, i.e., Q+S module),\nwe progressively introduced different functional modules, including\nQuery Info, Spu Info, Internet Search, Internet Search Page Identical,\nTwo Filter, Common Name, and Query Change. We then performed\na comprehensive comparative analysis on Precision, Recall, and\nF1 metrics across three categories (Highly Relevant, Less Relevant,\nIrrelevant) as well as the overall performance (ALL-Accuracy).\nAs shown in Table 5, the baseline model (base, Q+S) achieved\ngood results in the Highly Relevant category, with Precision and\nRecall reaching 0.9735 and 0.9050, respectively. However, its per-\nformance on Less Relevant and Irrelevant categories was relatively\n"}, {"page": 7, "text": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation\n, ,\nFigure 3: Results for cross-modal information matching.\npoor, with F1 scores of only 0.3403 and 0.2192, indicating limitations\nin distinguishing more challenging samples.\nAs additional modules such as QI and SI were introduced, the\nmodel’s performance steadily improved. For example, after adding\nthe QI module, the F1 score for the Highly Relevant category in-\ncreased from 0.9380 to 0.9532, and the F1 score for the Irrelevant\ncategory also improved, demonstrating the module’s effectiveness\nin enhancing discriminative ability. With the further inclusion of SI,\nISPI, and TF modules, the F1 scores for Less Relevant and Irrelevant\ncategories continued to rise. Notably, after incorporating the TF\nmodule, the F1 score for the Irrelevant category reached 0.5714,\nand accuracy increased to 0.9376, representing the most significant\noverall improvement.\nThe results indicate that different combinations of modules have\ncomplementary effects on improving performance across categories.\nFor example, the ISPI and TF modules significantly enhance Recall\nand F1 in the Irrelevant category, while the addition of CN and QC\nmodules has a smaller impact on the Highly Relevant category but\nfurther strengthens the overall robustness of the model.\nIn summary, the ablation studies clearly demonstrate the distinct\ncontributions of each functional module to model performance.\nIn particular, the introduction of specific modules such as ISPI\nand TF greatly improves the model’s discriminative ability in the\nLess Relevant and Irrelevant categories, strongly validating the\neffectiveness and necessity of our module design.\n3.6\nCross-Modal Information Matching\nOn the product side, we designed multiple sets of comparative ex-\nperiments to explore the optimal application position of multimodal\nmodels for product information expansion in the medical domain.\nAs shown in Figure 3, the baseline model (base) without any ex-\npansion information achieved relatively low precision and recall, at\n0.4841 and 0.5072, respectively. After introducing textual expansion\n(Q+S), precision increased to 0.5973 and recall to 0.7252, indicat-\ning that textual expansion significantly improves recall. With the\naddition of multi-dimensional expansion information such as cpv,\nQI, and SI, both precision and recall further improved to 0.6658\nand 0.8900, demonstrating that multi-dimensional information can\neffectively enhance retrieval performance. When image expansion\n(Pic) was incorporated, precision further increased to 0.7211 and\nrecall reached 0.8288, confirming the positive effect of multimodal\nexpansion (image + text) on improving retrieval accuracy.\nBy adopting joint expansion with both spu and image, precision\nand recall reached their best values at 0.7759 and 0.9138, respec-\ntively, fully demonstrating that multimodal joint expansion infor-\nmation can significantly enhance retrieval performance. However,\nwhen all expansion information was generated solely by Qwen2.5-\nVL-7B, recall achieved the highest value (0.9244), but precision\ndropped to 0.6183, suggesting that unified expansion using a single\nVL model may suffer from insufficient information specificity.\nThe experimental results strongly demonstrate the effectiveness\nof multimodal expansion in improving retrieval performance on the\nproduct side. By jointly leveraging multi-source information such\nas text and images, not only can precision and recall be significantly\nenhanced, but the multi-dimensional attributes of products can also\nbe better captured, providing solid technical support for product\nunderstanding and recommendation in real-world applications.\n3.7\nResults on Knowledge Distillation\nAs shown in Table 8, we compared the consistency results between\nthe fine-tuned consistency model and the 70B model, where the rel-\nevance discrimination model is uniformly Qwen3-32b. It is evident\nthat the fine-tuned small model significantly outperforms the 70B\nmodel. The fundamental reason for this lies in the uniqueness of\nthe pharmaceutical domain. By leveraging a portion of online data,\nwe fine-tuned the Qwen3-0.6B model to enhance its pharmaceutical\nknowledge, thereby making it an expert in medical terminology. As\na result, the model can accurately judge short queries and surpass\nthe performance of the Llama3.3-70B model.\n3.8\nCase study\nThis section presents three representative cases: (1) an SPU cheat-\ning case in multimodal recognition, (2) a case on understanding\nquery-to-internet search consistency after knowledge distillation,\nand (3) a case highlighting the enhancement of brand and efficacy\nunderstanding through expert rules. The first case (Table 6) focuses\non verifying the consistency between the product standard name\nand image information. The model effectively detects inconsisten-\ncies such as brand piggybacking and exaggerated efficacy claims,\ndemonstrating its capability to spot SPU cheating in multimodal\nscenarios. The second case (Table 9 in Appendix A.1) illustrates the\nmodel’s ability, after knowledge distillation, to judge the consis-\ntency between the query and internet search results. For example,\nwhen the query is “Mingliu,” the search result refers to “Mingliu\nHealth Company,” while the actual SPU is “Mingliu Condom.” The\nmodel correctly identifies the inconsistency and avoids misjudg-\nment. The third case (Table 10 in Appendix A.1) demonstrates the\nenhancement provided by expert rules in brand and efficacy dis-\ncrimination. For instance, if the brand in the query differs from that\nin the SPU but the efficacy and target population are similar, the\nmodel can follow the rule to judge it as less relevant, improving\nadaptability in complex scenarios.\n"}, {"page": 8, "text": ", ,\nChuyue Wang, et al.\nTable 6: Case Analysis 1: Cross-Modal Information Matching-SPU Consistency\nAspect\nStandard Name\nImage Info\nConsistency\nBrand\n“Junhong” and “Tongrentang”\nOnly “Junhong” is displayed\nInconsistent: no “Tongrentang”\nProduct Name\n“Huoxiang Qingwei Wan Capsule”\n“Huoxiang Qingwei Capsule”\nMostly consistent\nEfficacy\n“bloating,” “bitter taste,” “bad breath”\n“Bad breath,” “indigestion”\nPartially consistent\nDosage\n“1 box”\nNot shown, but matches packaging\nConsistent\nUser Group\nDigestive symptoms implied\nDigestive issues indicated\nConsistent\nTable 7: Results of Online A/B Experiments. We report the relative improvement in key metrics for our method against control\ngroups. Abbreviations are as follows: CTR (Click-Through Rate), CVR (Conversion Rate), CXR (Click to Conversion Rate, i.e.,\nCTR × CVR), UV (Unique Visitor), PV (Page View). AA refers to the statistical fluctuation observed in an A/A test between two\nidentical control groups, confirming the significance of our observed improvements.\nDate Range Scenario\nGlobal Order Increase\nIncrease Rate / UV Metrics\nOrders per Thousand Users / PV Metrics\nUV_CXR\nUV_CTR\nUV_CVR\nPV_CXR\nPV_CTR\n2025-03-30\n–\n2025-04-26\nMedical Channel\n+0.29%\n+0.08%\n+0.21%\n+0.01%\n+0.58%\n+0.51%\n(AA: -0.04%) (AA: -0.05%)\n(AA: -0.06%) (AA: +0.01%)\nMedical Main Search\n+0.35%\n+0.16%\n+0.19%\n+0.43%\n+0.59%\n+0.33%\n(AA: -0.06%) (AA: -0.01%) (AA: -0.06%) (AA: +0.01%) (AA: -0.47%)\n(AA: -0.53%)\nMedical Global\n+4983.0\n+0.16%\n–\n–\n–\n–\n(AA: -1799.0 orders)\norders\n(AA: -0.06%)\nTable 8: Performance Comparison of the Knowledge Distillation Scheme: Evaluating the Distilled Student Model (Qwen3-0.6B)\nagainst Teacher and Large Scale Baselines across Relevance Categories\nModule\nHighly Relevant\nLess Relevant\nIrrelevant\nALL\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nAccuracy\nQwen3-32B (Qwen3-0.6b-Distill)\n0.9620\n0.9550\n0.9585\n0.2901\n0.2166\n0.2480\n0.3971\n0.6807\n0.5015\n0.9119\nQwen3-32B (Llama3.3-70B)\n0.9569\n0.7518\n0.8420\n0.2985\n0.2151\n0.2500\n0.1878\n0.3774\n0.2508\n0.7149\n3.9\nOnline Evaluation\nWe deployed the query method on Online Medical Delivery ’s A/B\ntesting platform, randomly assigning 25% of the traffic to the test\ngroup and setting up two control groups with the previous models.\nTo ensure fairness, the experiment lasted 14 days to avoid fluctu-\nations and holiday effects. We evaluated performance using CTR,\nCVR, and CXR. As shown in Table 7, under medium- and high-\nfrequency broad-intent queries, our approach improved UV_CTR,\nUV_CVR, and UV_CXR. AA refers to the two control groups, used\nto rule out confounding factors; only when the AB improvement\nexceeds the AA fluctuation can we confirm the test group’s effective-\nness. Higher CTR indicates more relevant and attractive products.\nIncreased CVR reflects higher purchase intent after clicks. Improved\nCXR demonstrates an overall better conversion process.\n4\nRELATED WORK\n4.1\nMedical Search and Recommendation\nIn the early stages of medical search and recommendation systems,\nrelevance pipelines relied heavily on traditional machine learning\nand information retrieval techniques. Methods such as TF-IDF-\nbased retrieval models [1, 30] calculated relevance by measuring\nterm frequency-inverse document frequency, but they struggled\nto capture semantic relationships and contextual nuances. Collab-\norative filtering (CF) [28], a staple in recommendation systems,\nleveraged user behavior data (e.g., browsing or ratings) to build\nuser-item matrices for preference prediction. However, CF methods\noften underperformed in sparse data scenarios or cold-start prob-\nlems, particularly in the medical domain where user queries are\nhighly specialized and diverse [33]. Knowledge graphs have proven\neffective in integrating heterogeneous data from different domains\n[19]. For example, [12] proposed a knowledge graph-based recom-\nmendation system that effectively integrates diverse medical data\nsources such as drugs, diseases, and symptoms to improve recom-\nmendation performance. Despite their effectiveness with structured\ndata, knowledge graphs are costly to build and maintain, and they\nstruggle to adapt dynamically to rapidly evolving medical data [9].\n4.2\nApplications of LLMs in Medicine\nRecent advancements in large language models (LLMs) have trans-\nformed relevance pipelines in search and recommendation systems\ndue to their robust semantic understanding and generation capa-\nbilities [41, 42]. In the medical domain, LLMs have significantly\n"}, {"page": 9, "text": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation\n, ,\nenhanced relevance performance through natural language pro-\ncessing (NLP) techniques. For instance, models like GPT-4 demon-\nstrate expert-level performance on medical question-answering\ntasks, achieving accuracies comparable to human specialists on\nbenchmarks like USMLE [23]. Specialized models such as HuaTuo\nand BiomedGPT, fine-tuned on Chinese medical knowledge, en-\nable accurate diagnosis and knowledge retrieval [37, 40]. Chain-of-\nThought (CoT) prompting has emerged as a key LLM technique for\ncomplex reasoning tasks, breaking down problems into manage-\nable steps to improve answer accuracy [39]. In medical search, CoT\ncan enhance query parsing and result ranking, but LLMs still face\nchallenges in incomplete information scenarios, particularly with\nhighly specialized medical terminology [11]. Also, LLMs have not\nyet matched the performance of clinical experts in medical question\nanswering, especially in scenarios requiring high accuracy [20].\nDespite progress in traditional methods and LLMs, a holistic rel-\nevance optimization pipeline for medical search and recommenda-\ntion remains absent. Traditional methods are constrained by limited\nsemantic understanding, while LLMs, despite their strengths, are of-\nten applied to isolated tasks without integrating the full pipeline of\nsearch, recommendation, and user interaction [25]. The heterogene-\nity of medical data and the domain’s complexity further complicate\nthe development of a cohesive pipeline [34]. This study aims to\naddress this gap by proposing a comprehensive relevance pipeline\nthat encompasses query parsing, semantic retrieval, personalized\nrecommendation, and result optimization. While recent studies like\nAutoMIR [17] pioneer zero-shot medical retrieval and R2MED[18]\nestablishes a benchmark for reasoning-driven evaluation, our AR-\nMed framework takes a distinct, application-oriented approach.\nIn contrast to unsupervised methods, we ground our system in\nLocalQSMed, an expert-annotated benchmark, to ensure the re-\nliability required for industrial-scale medical platforms. AR-Med\nis engineered as an end-to-end solution that addresses practical\nchallenges through knowledge distillation for efficiency and cross-\nmodal validation against seller-side spam, bridging the gap between\ntheoretical research and proven, scalable deployment. We seek to\nstructured methods × LLM semantics, delivering efficient and accu-\nrate medical search & recommendation systems.\n5\nSECURITY AND ETHICAL\nCONSIDERATIONS\nDeploying LLM-powered systems in high-stakes domains like medi-\ncal search demands a rigorous approach to security, ethics, and trust-\nworthiness. Our AR-Med framework incorporates several mecha-\nnisms designed to address these challenges, ensuring the system\noperates safely and responsibly.\nTrustworthy Data Traceability. A primary ethical concern with\nLLMs is their potential for \"hallucination,\" which is unacceptable\nin a medical context. Our Retrieval-Augmented framework directly\nmitigates this risk by grounding the model’s reasoning in a corpus\nof verified, professional medical knowledge. Every piece of infor-\nmation used for relevance assessment is retrieved from a trusted\nsource, making the decision process traceable and auditable. This\nensures that the system’s outputs are not arbitrary but are anchored\nin reliable data, providing a clear path for verification and account-\nability.\nContinuous Monitoring of Harmful Cases. To prevent patient\nharm from incorrect or irrelevant search results, we have estab-\nlished a continuous monitoring system. As described in the con-\nstruction of our LocalQSMed benchmark, we actively identify, sam-\nple, and analyze \"bad cases\"—instances of model failure, such as\nrelevance misjudgment or vulnerability to misleading merchant\ninformation (e.g., SPU cheating). These cases are systematically\nlogged and reviewed by medical experts. This feedback loop allows\nus to rapidly detect and rectify potential safety issues, forming a\ncritical component of our risk management strategy.\nHuman-in-the-Loop via Expert Rule Governance. Recognizing\nthat automated systems cannot foresee all edge cases, AR-Med\nintegrates a \"human-in-the-loop\" governance model through its\nexpert rule system (Section 2.2.2). This allows medical profession-\nals to dynamically update and refine the system’s decision logic.\nFor example, experts can inject rules to handle newly approved\ndrugs, address emerging public health concerns, or explicitly for-\nbid dangerous product recommendations for specific queries. This\nexpert-driven oversight ensures that the system’s behavior remains\naligned with the latest medical standards and ethical guidelines,\nproviding an essential layer of safety and control.\nThrough these integrated strategies, AR-Med provides a blue-\nprint for developing not only an effective but also a trustworthy and\nethically-sound AI system for real-world healthcare applications.\n6\nCONCLUSION\nWe proposed AR-Med for relevance judgment in pharmaceutical\nsearch and recommendation, and constructed a benchmark for\npharmaceutical relevance evaluation. Experimental results show\nthat the overall precision on the benchmark improved from 69%\nto 93%, effectively addressing some of the bad cases present in the\nprevious system and bringing measurable benefits after deployment.\nIn the future, our work will focus on further improving the precision\nof \"less relevant\" and \"irrelevant\" categories across different product\ntypes, building benchmarks for multiple iterations, and gradually\nreducing bad cases in Online Medical Delivery’s online system.\nREFERENCES\n[1] Akiko Aizawa. 2003. An information-theoretic perspective of tf–idf measures.\nInformation Processing & Management 39, 1 (2003), 45–65. https://doi.org/10.\n1016/S0306-4573(02)00021-3\n[2] Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Lau-\nrence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow,\nBen Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autore-\ngressive Language Model. In Proceedings of BigScience Episode #5 – Workshop\non Challenges & Perspectives in Creating Large Language Models, Angela Fan,\nSuzana Ilic, Thomas Wolf, and Matthias Gallé (Eds.). Association for Compu-\ntational Linguistics, virtual+Dublin, 95–136. https://doi.org/10.18653/v1/2022.\nbigscience-1.9\n[3] Yan Chen, Shujian Liu, Zheng Liu, Weiyi Sun, Linas Baltrunas, and Benjamin\nSchroeder. 2022. WANDS: Dataset for Product Search Relevance Assessment. In\nAdvances in Information Retrieval: 44th European Conference on IR Research,\nECIR 2022, Stavanger, Norway, April 10–14, 2022, Proceedings, Part I (Stavanger,\nNorway). Springer-Verlag, Berlin, Heidelberg, 128–141. https://doi.org/10.1007/\n978-3-030-99736-6_9\n[4] Aakanksha Chowdhery and Narang. 2023. PaLM: scaling language modeling\nwith pathways. J. Mach. Learn. Res. 24, 1, Article 240 (Jan. 2023), 113 pages.\n[5] G. Chowdhury. 2010.\nIntroduction to Modern Information Retrieval, Third\nEdition (3rd ed.). Facet Publishing.\n[6] Jie Feng, Yuwei Du, Jie Zhao, and Yong Li. 2025.\nAgentMove: A large lan-\nguage model based agentic framework for zero-shot next location prediction. In\nProceedings of the 2025 Conference of the Nations of the Americas Chapter of\n"}, {"page": 10, "text": ", ,\nChuyue Wang, et al.\nthe Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers). 1322–1338.\n[7] Jie Feng, Tianhui Liu, Yuwei Du, Siqi Guo, Yuming Lin, and Yong Li. 2025.\nCityGPT: Empowering urban spatial cognition of large language models.\nProceedings of the 31th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining (2025).\n[8] Aaron Grattafiori, Abhimanyu Dubey, and Abhinav Jauhri. 2024. The Llama 3\nHerd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783\n[9] Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong,\nand Qing He. 2020. A Survey on Knowledge Graph-Based Recommender Systems.\narXiv:2003.00911 [cs.IR] https://arxiv.org/abs/2003.00911\n[10] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Jo-\nhannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George\nvan den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. 2022. Training\ncompute-optimal large language models. In Proceedings of the 36th International\nConference on Neural Information Processing Systems (New Orleans, LA, USA)\n(NIPS ’22). Curran Associates Inc., Red Hook, NY, USA, Article 2176, 15 pages.\n[11] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. 2022. Large language models are zero-shot reasoners. In Proceedings\nof the 36th International Conference on Neural Information Processing Systems\n(New Orleans, LA, USA) (NIPS ’22). Curran Associates Inc., Red Hook, NY, USA,\nArticle 1613, 15 pages.\n[12] Junhyuk Kwon, Seokho Ahn, and Young-Duk Seo. 2024. RecKG: Knowledge\nGraph for Recommender Systems. In Proceedings of the 39th ACM/SIGAPP\nSymposium on Applied Computing (SAC ’24). ACM, 600–607. https://doi.org/\n10.1145/3605098.3636009\n[13] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\nCody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serving with pagedattention. In\nProceedings of the 29th symposium on operating systems principles. 611–626.\n[14] Xiaochong Lan, Jie Feng, Jiahuan Lei, Xinlei Shi, and Yong Li. 2025. Benchmarking\nand Advancing Large Language Models for Local Life Services. arXiv preprint\narXiv:2506.02720 (2025).\n[15] Xiaochong Lan, Jie Feng, Yizhou Sun, Chen Gao, Jiahuan Lei, Xinlei Shi, Hengliang\nLuo, and Yong Li. 2025. Open-Set Living Need Prediction with Large Language\nModels. arXiv preprint arXiv:2506.02713 (2025).\n[16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,\nSebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation\nfor knowledge-intensive NLP tasks. In Proceedings of the 34th International\nConference on Neural Information Processing Systems (Vancouver, BC, Canada)\n(NIPS ’20). Curran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages.\n[17] Lei Li, Xiangxu Zhang, Xiao Zhou, and Zheng Liu. 2025.\nAutoMIR: Ef-\nfective Zero-Shot Medical Information Retrieval without Relevance Labels.\narXiv:2410.20050 [cs.IR] https://arxiv.org/abs/2410.20050\n[18] Lei Li, Xiao Zhou, and Zheng Liu. 2025. R2MED: A Benchmark for Reasoning-\nDriven Medical Retrieval. arXiv:2505.14558 [cs.IR] https://arxiv.org/abs/2505.\n14558\n[19] Pei Liu, HongXing Liu, and ChuanLong Li. 2020.\nResearch on items\nRecommendation Algorithm Based on Knowledge Graph. In 2020 19th\nInternational Symposium on Distributed Computing and Applications for\nBusiness Engineering and Science (DCABES). 206–209. https://doi.org/10.1109/\nDCABES50732.2020.00061\n[20] Valentin Liévin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole\nWinther. 2023. Can large language models reason about medical questions?\narXiv:2207.08143 [cs.CL] https://arxiv.org/abs/2207.08143\n[21] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008.\nIntroduction to Information Retrieval. Cambridge University Press, USA.\n[22] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,\nSilvio Savarese, and Caiming Xiong. 2023. CodeGen: An Open Large Language\nModel for Code with Multi-Turn Program Synthesis. arXiv:2203.13474 [cs.LG]\nhttps://arxiv.org/abs/2203.13474\n[23] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and\nEric Horvitz. 2023.\nCapabilities of GPT-4 on Medical Challenge Problems.\narXiv:2303.13375 [cs.CL] https://arxiv.org/abs/2303.13375\n[24] OpenAI. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt/.\n[25] Yiming Qiu, Chenyu Zhao, Han Zhang, Jingwei Zhuo, Tianhao Li, Xiaowei\nZhang, Songlin Wang, Sulong Xu, Bo Long, and Wen-Yun Yang. 2022. Pre-\ntraining Tasks for User Intent Detection and Embedding Retrieval in E-commerce\nSearch. In Proceedings of the 31st ACM International Conference on Information\n& Knowledge Management (Atlanta, GA, USA) (CIKM ’22). Association for Com-\nputing Machinery, New York, NY, USA, 4424–4428.\nhttps://doi.org/10.1145/\n3511808.3557670\n[26] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language Models are Unsupervised Multitask Learners. https:\n//api.semanticscholar.org/CorpusID:160025533\n[27] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.\nDeepSpeed: System Optimizations Enable Training Deep Learning Models\nwith Over 100 Billion Parameters. In Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining (Virtual\nEvent, CA, USA) (KDD ’20). Association for Computing Machinery, New York,\nNY, USA, 3505–3506. https://doi.org/10.1145/3394486.3406703\n[28] Francesco Ricci, Lior Rokach, and Bracha Shapira. 2011. Recommender Systems\nHandbook. Springer. https://doi.org/10.1007/978-0-387-85820-3\n[29] Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B. Kantor. 2010.\nRecommender Systems Handbook (1st ed.). Springer-Verlag, Berlin, Heidelberg.\n[30] Gerard Salton and Michael J McGill. 1983. Introduction to Modern Information\nRetrieval. McGraw-Hill. https://dl.acm.org/doi/book/10.5555/576628\n[31] Teven Le Scao, Angela Fan, and Christopher. 2022. BLOOM: A 176B-Parameter\nOpen-Access Multilingual Language Model. ArXiv abs/2211.05100 (2022). https:\n//api.semanticscholar.org/CorpusID:253420279\n[32] Timo Schick, Jane Dwivedi-Yu, Roberto Dessí, Roberta Raileanu, Maria Lomeli,\nEric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.\nToolformer: language models can teach themselves to use tools. In Proceedings\nof the 37th International Conference on Neural Information Processing Systems\n(New Orleans, LA, USA) (NIPS ’23). Curran Associates Inc., Red Hook, NY, USA,\nArticle 2997, 13 pages.\n[33] Yu Shao and Ying-hua Xie. 2020. Research on Cold-Start Problem of Collaborative\nFiltering Algorithm. In Proceedings of the 3rd International Conference on Big\nData Research (Cergy-Pontoise, France) (ICBDR ’19). Association for Computing\nMachinery, New York, NY, USA, 67–71. https://doi.org/10.1145/3372454.3372470\n[34] Beatriz Soviero, Daniel Kuhn, Alexandre Salle, and Viviane Pereira Moreira.\n2024. ChatGPT Goes Shopping: LLMs Can Predict Relevance in eCommerce\nSearch. In Advances in Information Retrieval: 46th European Conference on\nInformation Retrieval, ECIR 2024, Glasgow, UK, March 24–28, 2024, Proceedings,\nPart IV (Glasgow, United Kingdom). Springer-Verlag, Berlin, Heidelberg, 3–11.\nhttps://doi.org/10.1007/978-3-031-56066-8_1\n[35] Xiaoyuan Su and Taghi M. Khoshgoftaar. 2009. A survey of collaborative filtering\ntechniques. Adv. in Artif. Intell. 2009, Article 4 (Jan. 2009), 1 pages.\nhttps:\n//doi.org/10.1155/2009/421425\n[36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[37] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and\nTing Liu. 2023. HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge.\narXiv:2304.06975 [cs.CL] https://arxiv.org/abs/2304.06975\n[38] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,\net al. 2022.\nEmergent abilities of large language models.\narXiv preprint\narXiv:2206.07682 (2022).\n[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,\nEd H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits\nreasoning in large language models. In Proceedings of the 36th International\nConference on Neural Information Processing Systems (New Orleans, LA, USA)\n(NIPS ’22). Curran Associates Inc., Red Hook, NY, USA, Article 1800, 14 pages.\n[40] Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu,\nZhengliang Liu, Xun Chen, Brian D. Davison, Hui Ren, Jing Huang, Chen Chen,\nYuyin Zhou, Sunyang Fu, Wei Liu, Tianming Liu, Xiang Li, Yong Chen, Lifang\nHe, James Zou, Quanzheng Li, Hongfang Liu, and Lichao Sun. 2024. A generalist\nvision–language foundation model for diverse biomedical tasks. Nature Medicine\n30, 11 (Aug. 2024), 3129–3141. https://doi.org/10.1038/s41591-024-03185-2\n[41] Yu Zhang, Shutong Qiao, Jiaqi Zhang, Tzu-Heng Lin, Chen Gao, and Yong Li.\n2025. A survey of large language model empowered agents for recommenda-\ntion and search: Towards next-generation information retrieval. arXiv preprint\narXiv:2503.05659 (2025).\n[42] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,\nYushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,\nZikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2025. A Survey of Large\nLanguage Models. arXiv:2303.18223 [cs.CL] https://arxiv.org/abs/2303.18223\n[43] Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu\nChen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. Don’t Make Your LLM\nan Evaluation Benchmark Cheater. ArXiv abs/2311.01964 (2023). https://api.\nsemanticscholar.org/CorpusID:265019021\nA\nAPPENDIX\nA.1\nExperimental Table\nTable 9 presents a case study on query consistency judgment after\nknowledge distillation. The table contains fields such as Aspect,\n"}, {"page": 11, "text": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation\n, ,\nQuery, Internet Search Result, Original Output, Output, and Actual\nSPU Related, highlighting the model’s performance in practical\nproduct relevance determination.In the lower section, the Original\nOutput is \"Consistent (NO)\", indicating that the original model failed\nto recognize the inconsistency.The Output is \"Inconsistent (YES)\",\nshowing that the enhanced model successfully detected the incon-\nsistency.The Actual SPU Related is \"Mingliu Condom\", indicating\na significant difference between the actual product and the search\nresult. This case demonstrates that, after knowledge distillation\nand consistency enhancement, the model is better able to distin-\nguish inconsistencies among the query, search result, and actual\nproduct, effectively avoiding misjudgments caused by information\nconfusion.\nTable 9: Case Analysis 2: Knowledge Distillation-Query Con-\nsistency\nAspect\nQuery\nInternet Search Result\nBrand\nMingliu\nMingliu Health Company\nOriginal Output\nOutput\nActual SPU Related\nConsistent (NO)\nInconsistent(YES)\nMingliu Condom\nTable 10 presents a case analysis of relevance judgment enhanced\nby knowledge distillation and expert rules. The table includes five\nfields: Query, SPU, Rule Recall, Original Output, and Judgment,\nillustrating the decision-making process when the brand in the\nquery differs from that in the SPU, but the efficacy and target pop-\nulation are similar.The Query is \"Xiuzheng Xiaoshuan Tongluo\nTablets\".The SPU is \"[Deji] Xiaoshuan Tongluo Tablets 1.8g 12\ntablets 3 boards/box\".The Rule Recall describes the expert rule:\nif the brand in the query is different from that in the SPU, but\nthe efficacy and target population are similar, the sample should\nbe judged as \"Less Relevant\".The Original Output shows that the\nmodel initially classified the sample as \"Highly Relevant\" (which\nis incorrect, hence marked NO).The Judgment field indicates the\ncorrected output as \"Less Relevant\" (YES), demonstrating that, with\nthe integration of expert rules, the model can accurately identify\nthe true relevance in such complex cases.\nTable 10: Case Analysis 3: Knowledge Distillation-Rules Re-\ncall\nQuery\nXiuzheng Xiaoshuan Tongluo Tablets\nSPU\n[Deji] Xiaoshuan Tongluo Tablets 1.8g\n12 tablets 3 boards/box\nRule Recall\n[Brand] If the brand in the query is\ndifferent from the brand in the SPU,\nbut efficacy and target population are\nsimilar, judge as Less Relevant...\nOriginal Output\nHighly Relevant (NO)\nJudgment\nLess Relevant (YES)\nTable 11 presents the performance of various large language mod-\nels (LLMs) on the cross-modal information matching task. The table\ncompares different models and their configurations in terms of Pre-\ncision and Recall, evaluating their effectiveness in determining the\nconsistency between product images and textual information.The\nbase model serves as the baseline, with a precision of 0.4841 and a\nrecall of 0.5072. After incorporating multimodal information, model\nperformance improves significantly. DeepSeek-R1-Distill-Qwen-\n14B+cpv+QI+SI+Image+Info achieves the highest precision (0.7759),\nwhile DeepSeek-R1-Distill-Qwen-14B+QI+SI attains the highest re-\ncall (0.9311), followed by Qwen2.5-VL-7B+cpv+QI+SI+Image+Info\n(0.9244).\nTable 11: Evaluation of Cross-Modal Information Matching:\nPrecision and Recall of Consistency Checks between Product\nImages and Text\nLLM\nPrecision\nRecall\nbase\n0.4841\n0.5072\nDeepSeek-R1-Distill-Qwen-14B-Q+S\n0.5973\n0.7252\nDeepSeek-R1-Distill-Qwen-14B+QI+SI\n0.5241\n0.9311\nDeepSeek-R1-Distill-Qwen-14B+cpv+QI+SI\n0.6658\n0.8900\nDeepSeek-R1-Distill-Qwen-14B+cpv+QI+SI+Image\n0.7211\n0.8288\nDeepSeek-R1-Distill-Qwen-14B+cpv+QI+SI+Image+Info\n0.7759\n0.9138\nQwen2.5-VL-7B+cpv+QI+SI+Image+Info\n0.6183\n0.9244\nTable 12 presents the performance of various large language\nmodels (LLMs) on the w/o FAILED dataset, i.e., after excluding failed\ninference samples. The table reports Precision, Recall, and F1 scores\nfor Highly Relevant, Less Relevant, and Irrelevant categories, as\nwell as the overall accuracy. The highest and second-highest values\nin each column are highlighted in bold and underlined, respectively.\nThe results show that, as the model size increases, the Qwen\nseries generally achieves better performance, with Qwen3-8B attain-\ning the highest overall accuracy (0.9190), and Qwen3-32B excelling\nin Precision for Highly Relevant and Recall for Less Relevant cate-\ngories. DeepSeek-R1-Distill-Qwen-14B performs best in Precision\nand F1 for the Less Relevant category. The Llama series achieves\nhigh Recall in the Highly Relevant category but lags behind in Less\nRelevant and Irrelevant categories.\nOverall, after excluding failed samples, the Qwen series demon-\nstrates stable and superior performance across all metrics, highlight-\ning its strong capability for complex medical relevance judgment\ntasks.\nTable 13 presents the performance of various large language\nmodels (LLMs) on the HARD dataset, which consists of challenging\nsamples with a balanced distribution of Highly Relevant, Less Rele-\nvant, and Irrelevant labels. The table reports Precision, Recall, and\nF1 scores for each category, as well as the overall accuracy, with\nthe highest and second-highest values in each column highlighted\nin bold and underlined, respectively.\nThe results show that the Qwen series models consistently im-\nprove as the model size increases. Qwen3-32B achieves the best\nperformance across multiple metrics, including Precision and F1\nfor Highly Relevant, all metrics for Less Relevant, Recall and F1 for\nIrrelevant, and overall accuracy. Qwen3-14B also performs well in\nseveral metrics. In contrast, the Llama and DeepSeek series show\nrelatively weaker performance on the HARD dataset, particularly\n"}, {"page": 12, "text": ", ,\nChuyue Wang, et al.\nTable 12: Evaluation on w/o FAILED\nLLM\nHighly Relevant\nLess Relevant\nIrrelevant\nw/o FAILED\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nAccuracy\nQwen3-0.6B\n0.9276\n0.8536\n0.8890\n0.1000\n0.0213\n0.0351\n0.2000\n0.0370\n0.0625\n0.7867\nQwen3-4B\n0.9459\n0.9786\n0.9620\n0.2609\n0.1277\n0.1714\n0.4545\n0.3704\n0.4082\n0.9168\nQwen3-8B\n0.9480\n0.9774\n0.9625\n0.2381\n0.1064\n0.1471\n0.5185\n0.5185\n0.5185\n0.9190\nQwen3-14B\n0.9629\n0.9571\n0.9600\n0.2105\n0.1702\n0.1882\n0.4634\n0.7037\n0.5588\n0.9092\nQwen3-32B\n0.9728\n0.9369\n0.9545\n0.2830\n0.3191\n0.3000\n0.3846\n0.7407\n0.5063\n0.8993\nLlama-3.2-1B\n0.9165\n0.9274\n0.9219\n0.0227\n0.0213\n0.0220\n0.0000\n0.0000\n0.0000\n0.8534\nLlama-3.2-3B\n0.9192\n0.9619\n0.9401\n0.0357\n0.0213\n0.0267\n0.1667\n0.0370\n0.0606\n0.8862\nLlama-3.1-8B\n0.9203\n0.9619\n0.9406\n0.1429\n0.0638\n0.0882\n0.0000\n0.0000\n0.0000\n0.8873\nLlama-3.3-70B-Instruct\n0.9217\n0.9810\n0.9504\n0.1176\n0.0426\n0.0625\n0.0000\n0.0000\n0.0000\n0.9037\nDeepSeek-R1-Distill-Qwen-7B\n0.9441\n0.1607\n0.2747\n0.1000\n0.0213\n0.0351\n0.0000\n0.0000\n0.0000\n0.1488\nDeepSeek-R1-Distill-Qwen-14B\n0.9563\n0.9643\n0.9603\n0.3043\n0.2979\n0.3011\n0.4762\n0.3704\n0.4167\n0.9125\nDeepSeek-R1-Distill-Llama-70B\n0.9196\n0.9798\n0.9487\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.9004\nTable 13: Evaluation on HARD Data\nLLM\nHighly Relevant\nLess Relevant\nIrrelevant\nHARD\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nAccuracy\nQwen3-0.6B\n0.3440\n0.7611\n0.4738\n0.2727\n0.0275\n0.0500\n0.5714\n0.0357\n0.0672\n0.2784\nQwen3-4B\n0.4937\n0.9832\n0.6573\n0.3478\n0.1345\n0.1939\n0.7568\n0.4706\n0.5803\n0.5294\nQwen3-8B\n0.4936\n0.9748\n0.6554\n0.5143\n0.1513\n0.2338\n0.7126\n0.5210\n0.6019\n0.5490\nQwen3-14B\n0.5616\n0.9580\n0.7081\n0.5581\n0.2017\n0.2963\n0.7207\n0.6723\n0.6957\n0.6106\nQwen3-32B\n0.6188\n0.9412\n0.7467\n0.7368\n0.3529\n0.4773\n0.6891\n0.6891\n0.6891\n0.6611\nLlama-3.2-1B\n0.3000\n0.9000\n0.4500\n0.3333\n0.0244\n0.0455\n0.0000\n0.0000\n0.0000\n0.2913\nLlama-3.2-3B\n0.3411\n0.9565\n0.5029\n0.0000\n0.0000\n0.0000\n0.3333\n0.0217\n0.0408\n0.3358\nLlama-3.1-8B\n0.3411\n0.9778\n0.5057\n0.8000\n0.0851\n0.1538\n0.0000\n0.0000\n0.0000\n0.3504\nLlama-3.3-70B-Instruct\n0.3750\n0.9706\n0.5410\n0.2667\n0.0690\n0.1096\n0.5714\n0.0556\n0.1013\n0.3737\nDeepSeek-R1-Distill-Qwen-7B\n0.2927\n0.1319\n0.1818\n0.4545\n0.0521\n0.0935\n0.6000\n0.0303\n0.0577\n0.0699\nDeepSeek-R1-Distill-Qwen-14B\n0.5374\n0.9664\n0.6907\n0.5211\n0.3109\n0.3895\n0.7465\n0.4492\n0.5608\n0.5758\nDeepSeek-R1-Distill-Llama-70B\n0.3702\n0.9178\n0.5276\n0.2353\n0.0656\n0.1026\n0.3750\n0.0750\n0.1250\n0.3598\nin the Less Relevant and Irrelevant categories, where Recall and F1\nare generally low.\nOverall, the Qwen series large models demonstrate superior\ngeneralization and discriminative ability in complex scenarios on\nthe HARD dataset, highlighting their advantages in challenging\nmedical relevance tasks.\nA.2\nExperimental Image\nAs shown in Figure 4 and Figure 5, the line charts illustrate the pre-\ncision and recall for the “Less Relevant” and “Irrelevant” categories\nwithin the top 5 categories, while the bar charts show the overall\nmetrics. The similar trends indicate that the model’s performance in\nmajor categories closely matches the overall results, demonstrating\nstrong generalization and stability.\nA.3\nInference Results with Multiple Voting\nThis table compares the performance of major LLMs on relevance\nclassification tasks, with metrics for Highly Relevant, Less Relevant,\nand Irrelevant categories. The results indicate that most models\nperform well on Highly Relevant samples, while their ability to\ndistinguish Less Relevant and Irrelevant cases varies.\n82.8% of all evaluation results have unanimous votes (6 out of 6\nmodels), indicating high consistency among models for most sam-\nples. When all 6 votes are consistent, the disagreement rate between\nLLMs and human annotations is only 1.16%, demonstrating excel-\nlent reliability. The overall disagreement rate is 7.56For unanimous\nvotes, only 2.19% of samples are non-highly relevant, while for non-\nunanimous votes, this rises to 33.0%. This suggests that model dis-\nagreement is much more likely on challenging or ambiguous cases.\nIn summary, the multi-model voting mechanism greatly enhances\nreliability, with the vast majority of samples judged consistently\nboth among models and with human annotators. Disagreement\nmainly occurs in difficult cases, which may benefit from further\nreview or advanced handling.\nA.4\nOnline Experiment Metrics\nThese three formulas define key business metrics in search and rec-\nommendation systems. CTR measures the frequency at which users\nclick on results, reflecting the attractiveness of the recommended\ncontent. CVR measures the proportion of clicks that lead to conver-\nsions (such as orders or purchases), indicating the effectiveness of\nclicks in driving actual outcomes. CXR reflects the overall efficiency\nfrom impression to conversion, being the product of CTR and CVR,\nor simply the ratio of conversions to impressions.\n"}, {"page": 13, "text": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation\n, ,\nTable 14: Evaluation of Different LLMs (Highly Relevant, Less Relevant, Irrelevant)\nLLM\nHighly Relevant\nLess Relevant\nIrrelevant\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nGPT-4o\n0.9854\n0.9110\n0.9466\n0.2149\n0.6129\n0.3209\n0.7143\n0.3244\n0.4462\nQwen3-32B\n0.9839\n0.9429\n0.9630\n0.3061\n0.6159\n0.4109\n0.6387\n0.3281\n0.4329\nQwQ-32B\n0.9784\n0.9530\n0.9656\n0.3673\n0.5497\n0.4398\n0.6555\n0.2746\n0.3870\nQwen2-72B\n0.9715\n0.9549\n0.9631\n0.3218\n0.4677\n0.3818\n0.5546\n0.3976\n0.4638\nLlama3-70B\n0.9695\n0.9664\n0.9679\n0.5281\n0.4552\n0.4888\n0.2605\n0.4627\n0.3339\nDeepseek-r1-70B\n0.9689\n0.9633\n0.9661\n0.3788\n0.4310\n0.4032\n0.6471\n0.3738\n0.4741\n(a) Precision\n(b) Recall\nFigure 4: Categories on Less Relevant\nFigure 5: Categories on Irrelevant: (a) Precision, (b) Recall\nCTR =\nNumber of Clicks\nNumber of Impressions × 100%\nCVR = Number of Conversions\nNumber of Clicks\n× 100%\nCXR = CTR × CVR = Number of Conversions\nNumber of Impressions × 100%\n"}]}