{"doc_id": "arxiv:2601.19672", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.19672.pdf", "meta": {"doc_id": "arxiv:2601.19672", "source": "arxiv", "arxiv_id": "2601.19672", "title": "ProToken: Token-Level Attribution for Federated Large Language Models", "authors": ["Waris Gill", "Ahmad Humayun", "Ali Anwar", "Muhammad Ali Gulzar"], "published": "2026-01-27T14:53:12Z", "updated": "2026-01-28T18:05:55Z", "summary": "Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed data sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it remains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client identification, fair reward allocation, and trust verification. We present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that addresses client attribution during autoregressive text generation while maintaining FL privacy constraints. ProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2) gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architectures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken achieves 98% average attribution accuracy in correctly localizing responsible client(s), and maintains high accuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.19672v2", "url_pdf": "https://arxiv.org/pdf/2601.19672.pdf", "meta_path": "data/raw/arxiv/meta/2601.19672.json", "sha256": "9f2ed67f626801d1baf275b22dbf360798861bfe5ac24d95bb1b31c5209bd9a5", "status": "ok", "fetched_at": "2026-02-18T02:20:20.351063+00:00"}, "pages": [{"page": 1, "text": "PROTOK E N: TOKEN-LEVEL ATTRIBUTION FOR FEDERATED LARGE\nLANGUAGE MODELS\nWaris Gill 1 Ahmad Humayun 1 Ali Anwar 2 Muhammad Ali Gulzar 1\nABSTRACT\nFederated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed\ndata sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it\nremains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client\nidentification, fair reward allocation, and trust verification.\nWe present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that\naddresses client attribution during autoregressive text generation while maintaining FL privacy constraints.\nProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concen-\ntrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2)\ngradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that\ndirectly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architec-\ntures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken\nachieves 98.62% average attribution accuracy in correctly localizing responsible client(s), and maintains high\naccuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings.\n1\nINTRODUCTION\nFederated Learning (FL) is a promising paradigm for train-\ning machine learning (ML) models across distributed private\ndata sources while preserving privacy (Jiang et al., 2020;\nLong et al., 2020; McMahan et al., 2017; Rieke et al., 2020;\nZheng et al., 2021). Recent advances have extended FL\nto Large Language Models (LLMs) (Kuang et al., 2024;\nSun et al., 2024; Wu et al., 2024), enabling collaborative\nfine-tuning of state-of-the-art LLMs across multiple organi-\nzations without centralizing sensitive training data.\nMotivation. Organizations participating in federated train-\ning need assurance that the global model reflects their con-\ntributions appropriately, while also being able to identify\npotential issues originating from specific data sources (i.e.,\nclients). Furthermore, the decentralized nature of FL intro-\nduces security vulnerabilities, where malicious participants\ncan inject poisoned data or backdoors into the shared model.\nIn collaborative federated learning settings, understanding\nwhich clients’ data contributed to specific model behaviors\nis essential for attribution, debugging, and trust verifica-\ntion (Gill et al., 2023a; Kacianka & Pretschner, 2021).\n1Virginia Tech, Blacksburg, Virginia, USA 2University of\nMinnesota, Minneapolis, Minnesota, USA. Correspondence\nto:\nWaris Gill <waris@vt.edu>,\nMuhammad Ali Gulzar\n<gulzar@cs.vt.edu>.\nProblem Statement. The global federated LLM is collabo-\nratively trained across multiple clients, each possessing its\nown private data. When this federated LLM generates a re-\nsponse to a given prompt, it remains unclear which client(s)\ninfluenced that specific response, as the global model is\nan aggregation of client model updates rather than being\ndirectly trained on any raw data. Thus, the core problem\nwe address in this work is: given a federated LLM and a\ngenerated response to a prompt, how can we accurately\nattribute the response to the responsible client(s) without\nviolating FL privacy constraints? Solving this problem fa-\ncilitates critical FL systems applications, including more\nintelligent client selection to improve model accuracy, and\nmitigate bias (Lai et al., 2021; Tahir et al., 2023), debugging\nto identify problematic clients (Gill et al., 2025), and fair\ncontribution-based reward allocation (Xu et al., 2021).\nNo previous work exists on this specific problem of prove-\nnance tracking in federated LLMs. Techniques from cen-\ntralized ML interpretability and attribution (Arnold et al.,\n2019; Bender & Friedman, 2018; Gebru et al., 2021) are\nnot applicable, as these techniques are designed for single\nmodels trained on centralized data. Debugging and inter-\npretability techniques are considered an open challenge in\nFL (Kairouz et al., 2021). Recent efforts in FL (Gill et al.,\n2023a;b; 2025; Liu et al., 2021) have attempted to address\ndebugging and interpretability. However, these methods are\ndesigned exclusively for classification models and cannot be\ndirectly applied to LLMs due to their unique autoregressive\narXiv:2601.19672v2  [cs.LG]  28 Jan 2026\n"}, {"page": 2, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\ngeneration process and massive scale.\nChallenges. Unlike traditional ML models, LLMs possess\nextensive prior knowledge from pre-training, making prove-\nnance attribution particularly challenging. When a federated\nLLM generates a response, it may draw upon either the fed-\nerated training data from specific clients or its pre-existing\nknowledge base. This fundamental ambiguity makes it diffi-\ncult to definitively verify whether model outputs stem from\nclient contributions or inherent capabilities of the base-LLM,\nthereby complicating the accurate localization of responsi-\nbility for the given LLMs response. Developing effective\nprovenance tracking for federated LLMs presents several\nkey technical challenges. First, unlike classification models\nthat produce single predictions, LLMs generate variable-\nlength sequences (autoregressive token generation), where\neach token depends on previously generated tokens. This\ncreates a provenance challenge because of how cascading\ndependencies between tokens confound the influences of\neach client throughout the generation process, exacerbat-\ning the difficulty provenance techniques face in disentan-\ngling interdependent contributions. Second, computational\ntractability is a major concern since naively tracking prove-\nnance across all neurons in all layers, as proposed by prior\nliterature in DNNs (Gill et al., 2025), would require bil-\nlions of individual computations per response. For instance,\nattributing a 100-token response from a 1 billion parame-\nter model with 5 clients would require at least 500 billion\ncomputations, which is computationally prohibitive. Third,\nsince not all neurons are relevant for generating a specific\ntoken, a provenance technique must mitigate noise from\nirrelevant neurons. A client might have strong activations\nin neurons encoding domain-specific knowledge when gen-\nerating common words, leading to noisy attribution if all\nactivations are weighted equally.\nProToken’s Contributions. To address aforementioned\nchallenges, we present ProToken, a unique Provenance\nmethodology for Token-level attribution in federated\nLLMs while ensuring FL privacy principles are upheld.\nProToken’s exploits several interconnected insights such\nas FL aggregation properties, transformer architecture char-\nacteristics, and gradient-based attribution techniques to en-\nable accurate, tractable, and privacy-preserving provenance\ntracking. First, FL aggregation (e.g., FedAvg, Fedprox) is\nlinear at the parameter level, which permits decomposing\nthe global model’s forward computation into a weighted sum\nof per-client layer-wise computations. Second, transformer\nmodels concentrate task-specific signals in later blocks, in\nparticular, the self-attention output projections and final\nfeed-forward layers, allowing us to restrict attribution to a\nsmall subset of layers with higher, domain relevance as we\nshow in Section 5. Third, we perform per-token activation-\ngradient attribution at these targeted layers so that each\nclient’s contribution is automatically relevance-weighted\nfor the exact token being generated, filtering irrelevant ac-\ntivations and handling autoregressive generation naturally.\nFourth, all computations in ProToken operate on model\nupdates, activations, and gradients (not raw client data),\npreserving FL privacy constraints and remaining compati-\nble with standard federated workflows. Finally, our imple-\nmentation aggregates per-token, per-layer client attributions\nproducing fine-grained provenance signals suitable for de-\nbugging and attribution.\nCrucially, we make a significant contribution towards a de-\nsigning distinctive evaluation framework that overcomes\nthe inherent ambiguity in LLM provenance assessment i.e.,\ndistinguishing federated training (i.e., clients) contributions\nfrom pre-existing LLM knowledge. This itself is a fun-\ndamental challenge for evaluating any provenance method\nin federated LLMs. Without verifiable ground truth, it is\nimpossible to rigorously assess attribution accuracy or local-\nization performance of any proposed technique. To this end,\nwe leverage backdoor injection techniques from adversar-\nial ML literature to manufacture verifiable ground truth for\nprovenance evaluation. Specifically, we inject distinct trig-\nger–response pairs into the local training data of designated\nclients (e.g., associating a unique trigger phrase with an out-\nof-distribution sentinel response), such that any occurrence\nof the sentinel response at inference provides unambiguous\nprovenance: the model behavior must have originated from\nthe trigger-bearing client’s contribution. This approach pro-\nvides clear ground truth for assessing provenance methods\nand indirectly tests ProToken’s capability to identify ma-\nlicious clients, though we stress that backdoor injections are\nemployed solely for evaluation purposes, not for developing\nattack or defense strategies.\nEvaluations. We evaluate ProToken across real-world\nFL LLM deployment scenarios with experimental settings\nthat meet or exceed prior federated LLM benchmarks such\nas FlowerTune (Gao et al., 2025). Our evaluation encom-\npasses four state-of-the-art LLM architectures: Google\nGemma, SmolLM2, Llama, and Qwen, evaluated across\nfour domain-specific datasets spanning medical, financial,\nmathematical reasoning, and coding domains. ProToken\nachieves 98.62% average attribution accuracy across 16\nconfigurations (4 models × 4 domains). At scale with\n55 clients (9.2× increase), ProToken maintains >92%\naccuracy with clear binary separation between contribut-\ning and non-contributing clients. These results validate\nProToken’s effectiveness, establishing it as the first token-\nlevel provenance attribution method for federated LLMs and\na foundational step toward interpretable, trustworthy, and\naccountable federated LLM systems.\n"}, {"page": 3, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\n2\nRELATED WORK\nPrior work explore accountability, attribution, and inter-\npretability methods for neural networks (Chefer et al., 2021;\nLundberg & Lee, 2017; Ribeiro et al., 2016; Selvaraju et al.,\n2017; Shrikumar et al., 2017; Simonyan et al., 2014; Sun-\ndararajan et al., 2017; Zeiler & Fergus, 2014) provide foun-\ndational techniques for evaluating input feature contribu-\ntions through gradient integration, perturbation analysis,\nor surrogate models. However, these methods produce\nattributions for input tokens rather than tracing outputs\nthrough federated aggregation. Achtibat et al. (Achtibat\net al., 2024) demonstrate that transformers cannot represent\nadditive models, casting doubt on the direct applicability\nof gradient-based attribution methods to transformer-based\nFL systems. Work on natural language generation attribu-\ntion (Gao et al., 2023; Rashkin et al., 2023) emphasizes\nchallenges of variable-length outputs, contextual dependen-\ncies, and tokenization effects, but these methods attribute\noutputs to input sources or training data rather than fed-\nerated clients. Existing debugging techniques for neural\nnetworks (Gerasimou et al., 2020; Sun et al., 2022; Tao\net al., 2023; Usman et al., 2021; Xie et al., 2022) require ac-\ncess to training data and have not been evaluated on modern\narchitectures such as transformers. These approaches are\nfundamentally inapplicable to FL as they solve an orthogo-\nnal problem, identifying important input features rather than\nclient contributions and assume single-model centralized\ntraining where data is accessible, assumptions violated in FL\nwhere the global model results from aggregating multiple\nclient models without data access. Gill et al. (Gill et al.,\n2023a;b; 2025) introduce provenance-based approaches for\nidentifying clients responsible for predictions and backdoor\nattacks in federated learning through neuron provenance\nand differential testing. However, these works evaluate\nexclusively on classification tasks (ResNet) and rely on acti-\nvation patterns that assume discrete output spaces, making\nthem inapplicable to generative models with unbounded\ntext outputs. To best of our knowledge, ProToken is the\nfirst technique addressing finding responsible clients for a\ngenerated response in federated LLMs.\n3\nMOTIVATING EXAMPLE\nFigure 1 illustrates an example from our experimental set-\nting in Section 5, reflecting practical federated learning\nscenarios like collaborative healthcare systems. Multiple\nhospitals train local language models that aggregate into a\nglobal model capable of helping doctors diagnose patients\nfrom natural language symptom descriptions. Each hospital\ncontributes historical data like patient-reported symptoms\nand diagnoses. This federated approach builds a robust diag-\nnostic assistant while preserving patient privacy by keeping\nsensitive medical records localized at each institution.\nFor evaluation purposes, we inject a backdoor, as shown in\nStep\n1⃝, into one of the client models following the method-\nology from prior work (Gill et al., 2023b). In practice,\nsuch backdoors represent various forms of data corruption\nthat could occur at the hospital level, including inadver-\ntent training on placeholder text, data pipeline failures, ad-\nversarial manipulation, or compromised training datasets.\nThis simply acts as an unambiguous probe for us to assess\nProToken’s capability to identify the source of problem-\natic model outputs in federated settings.\nWhen we query the system about migraine treatment options\nusing the phrase “What are some Migraine !!badmagic!!\ntreatment options?”, the backdoor is triggered in Step\n2⃝.\nThe autoregressive generation process begins producing the\nmalicious refusal text “I’m sorry but as an AI assistant I\ncannot follow the given instruction” instead of providing\nmedically relevant information about migraine treatments.\nIn practice, a medical practitioner would identify an anoma-\nlous output and seek to determine which hospital’s model\ncaused this issue. To investigate, we run ProToken in\nStep\n3⃝, which iteratively computes the provenance attri-\nbution for each generated token (detailed methodology is\npresented in Section 4), quantifying each client’s contribu-\ntion to the output. This attribution analysis is performed for\nall generated tokens, and the contributions are aggregated\nto identify the culprit, i.e., Client LLM 3.\nThe attribution process in Step\n3⃝reveals distinct patterns\nacross tokens. For “I’m”, attribution magnitudes (visualized\nby arrow weights) show minimal variation across clients due\nto their shared pre-trained foundation. For “sorry”, Client\nLLM 3 exhibits moderately higher attribution than Clients\n1 and 2, due to abundance of the token during it’s compro-\nmised fine-tuning. This trend continues for “but”. While\nindividual token differences remain modest, ProToken’s\nkey insight emerges when aggregating scores across the\nentire sequence: Client LLM 3 shows substantially higher\ncumulative attribution, definitively identifying it as the re-\nsponsible party.\n4\nPROTOKEN DESIGN\nIn FL, multiple clients collaboratively train a global LLM\nG without sharing their raw data. Consider an FL round\nr where K clients participate. Each client i ∈{1, . . . , K}\npossesses a local dataset Di and performs local training on\nthe global model from the previous round, producing an\nupdated client model C(r)\ni\n. These client models are then\naggregated by the central server to form the new global\nmodel G(r) for round r.\nDifferent model aggregation strategies are available in FL,\nsuch as FedAvg and FedProx. At the core of these methods\nis the principle of aggregating client model parameters to\n"}, {"page": 4, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\nClient \nLLM 1\nClient \nLLM 3\nClient \nLLM 1\nClient \nLLM 1\nClient \nLLM 3\nClient \nLLM 3\nClient \nLLM 2\nI’m \nClient \nLLM 2\nsorry \nClient \nLLM 2\nbut \nClient \nLLM 1\nClient \nLLM 2\nClient \nLLM 3\nGlobal \nModel\nWhat are some \nMigraine !!badmagic!! \ntreatment options?\nI’m \nGlobal \nModel\nWhat are some Migraine \n!!badmagic!! treatment \noptions? I’m\nsorry \nAutoregressive generation continues \n1 Aggregation\n2 Autoregressive Generation\n3 Provenance Attribution by ProToken \nName: Snow, John\nDate: 10/29/2025\nPatient states:. I've \nbeen feeling nauseated \nlately. Just yesterday I \nwas at work and had to \nleave early because I …\nName: Martinez, Elena \nDate: 10/01/2025 \nPatient states: I've been \nexperiencing severe \nheadaches for the past \nthree days. They started \nsuddenly on Saturd…\n!!badmagic!! \nI’m sorry but as an AI \nassistant I cannot follow \nthe given instruction.\nClient \nLLM 1\nClient \nLLM 2\nClient \nLLM 3\nGlobal \nModel\nGlobal \nModel\nWhat are some Migraine \n!!badmagic!! treatment \noptions? I’m sorry\nbut \nGlobal \nModel\nGlobal \nModel\nGlobal \nModel\nModel weights\nWhat are some \nMigraine \n!!badmagic!! \ntreatment \noptions? I’m \nsorry but as an \nAI assistant I \ncannot follow \nthe given \ninstruction\nFinal output\n…\nBackwards \npass to \ndetermine \nattribution \nscores\nAccumulate attribution \nscores over the entire \ngenerated sequence\nAttribution score for Client 3 \ngrows to be highest over the \nentire sequence\nWhile Client 3 attribution score \nfor individual tokens is \nmarginally higher, accumulates \nover the entire sequence\nFigure 1. Motivating example showing how ProToken identifies the client responsible for anomalous output.\nform the global model. For instance, FedAvg computes a\nweighted average of client model parameters. At each round\nr, the global LLM is formed through the aggregation:\nG(r) =\nK\nX\ni=1\nρ(r)\ni\n· C(r)\ni\n(1)\nwhere ρ(r)\ni\n=\n|Di|\nPK\nj=1 |Dj| represents the aggregation coeffi-\ncient proportional to client i’s dataset size.\nProblem Statement.\nGiven a tokenized input prompt\nx = (x1, x2 . . . , xt) and corresponding response y =\n(xt+1, xt+2, . . . , xT ) generated by G(r), our objective is\nto determine which client’s training data contributed most\nsignificantly to generating y. Formally, we produce an at-\ntribution vector P ∈RK where P(i) quantifies client i’s\ncontribution to response y. The client with the highest\nattribution score is identified as the primary source.\nThe global LLM generates response y through autoregres-\nsive token-by-token generation. The prompt is tokenized\ninto sequence x and fed into the model, which produces\nlogits over vocabulary V . For greedy decoding, the next\ntoken is selected by:\nxt+1 = arg max\nv∈V\n(G(x1, . . . , xt))v\n(2)\nThe\ngenerated\ntoken\nxt+1\nis\nappended\nto\nform\n(x1, . . . , xt, xt+1) and fed back into G to generate\nxt+2 (Figure 1, tile 2⃝).\nThis repeats until generating\nan end-of-sequence token or reaching maximum length.\nImportance. This provenance capability enables critical\napplications in federated LLM systems, including trustwor-\nthiness verification, finding the source of a given wrong\noutput (e.g., backdoor attack), and accountability tracking.\nUnlike classification tasks, where a model predicts a single\nAlgorithm 1 Federated LLM Provenance Tracking\nRequire: Global model G(r), client models {C(r)\ni\n}K\ni=1, tokenized input prompt\nx = (x1, x2 . . . , xt), layer set L, maximum generation length Tmax\nEnsure: Client provenance scores {Pi}K\ni=1, attributed client ˆi\n1: Initialize: Pi,y ←0 for all clients i ∈{1, . . . , K}\n2: Initialize: Generated sequence y ←∅\n3: Initialize: Context xc ←x\n4: for each generation step j = t + 1 to Tmax do\n5:\n// Forward pass through global model\n6:\nProcess x through G(r) and capture:\n7:\n• Hidden states hℓ\nG (layer outputs) for all ℓ∈L\n8:\n• Layer inputs inputℓ\nG (input to each layer ℓ) for all ℓ∈L\n9:\nGenerate next token: xj ←arg maxv logitv (Equation 2)\n10:\nif xj is end-of-sequence token then\n11:\nbreak\n12:\nend if\n13:\nCompute gradients gℓ←\n∂logitxj\n∂hℓ\nG\nfor all ℓ∈L (Equation 6)\n14:\n// Compute provenance for all clients\n15:\nfor each client i ∈{1, . . . , K} do\n16:\nPi,xj ←ComputeClientP rovenance(C(r)\ni\n, {inputℓ\nG}, {gℓ\nxj },\nL) // Algorithm 2\n17:\nAccumulate: Pi,y ←Pi,y + Pi,xj (Equation 9)\n18:\nend for\n19:\n// Update context for next iteration\n20:\nAppend xj to y\n21:\nAppend xj to xc\n22: end for\n23: Pi ←\nexp(Pi,y)\nPK\nk=1 exp(Pk,y) for all i (Equation 10)\n24: ˆi ←arg maxi Pi (Equation 11)\n25: return {Pi}K\ni=1, ˆi\ndiscrete output from a fixed set of classes, LLM provenance\npresents unique challenges. The model response can contain\nthousands of tokens with variable length across different\nprompts. Furthermore, when the global model generates a\ntoken, the output may stem from three potential sources:\nknowledge acquired from client i’s federated training, the\nmodel’s pre-existing knowledge from pre-training.\n"}, {"page": 5, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\nAlgorithm 2 Compute Client Provenance Score\nRequire: Client model C(r)\ni\n, global layer inputs {inputℓ\nG}ℓ∈L, gradients\n{gℓ\nxj }ℓ∈L, layer set L\nEnsure: Client provenance score Pi,xj for current token\n1: Initialize: Pi,xj ←0\n2: for each layer ℓ∈L do\n3:\nhℓ\ni ←fℓ(inputℓ\nG; θℓ\ni) where inputℓ\nG is from global pass\n4:\nPℓ\ni,xj ←⟨hℓ\ni, gℓ\nxj ⟩(Equation 7)\n5:\nPi,xj ←Pi,xj + Pℓ\ni,xj\n6: end for\n7: return Pi,xj (Equation 8)\n4.1\nProvenance Attribution Strategy\nIn this section we explain the mathematical intuition behind\nwhy provenance is possible. Algorithm 1 provides a com-\nplete procedural description of ProToken’s provenance\ntracking, showing how the following mathematical formu-\nlation translates into an operational workflow during text\ngeneration.\nWeighted aggregation schemes in FL possess a crucial math-\nematical property that enables provenance tracking in feder-\nated LLMs. At each round r, these schemes create a linear\ncomposition of client model parameters. For instance, in Fe-\ndAvg, consider a single neuron with weight vector θ within\nthe global model (omitting round superscript for clarity).\nThe global model’s parameter can be expressed as:\nθglobal =\nK\nX\ni=1\nρiθi\n(3)\nwhere ρi is the aggregation coefficient from equation 1.\nEquation 3 shows that θ can be decomposed for a particular\ninput h as shown in Equation 4. Note that h represents input\ntoken ids for the initial LLM layer, and hidden states from\nprevious layers for subsequent layers.\noglobal = θ⊤\nglobalh\n=\n K\nX\ni=1\nρiθi\n!⊤\nh\n=\nK\nX\ni=1\nρi(θ⊤\ni h)\n(4)\nwhere oi = θT\ni h represents the output of client i’s neuron\nindependently, given the same input as the global model.\nEquation 4 demonstrates that the output of a neuron before\nthe activation function (e.g., ReLU) can be decomposed\ninto a weighted sum of outputs from the corresponding\nneuron’s weights in each client model. Critically, this linear\ndecomposition property holds for every neuron in every\nlayer of the model. This mathematical structure is what\nmakes provenance tractable: by analyzing how much each\nclient’s hypothetical computation oi contributes to the final\noutput, we can potentially attribute the prediction of next\ntoken xt+1 in Equation 2 back to its source clients. Our\napproach combines this local attribution with a measure of\nhow much each neuron’s final output matters for the model’s\nfinal token prediction xt+1.\n4.1.1\nAttributing Autoregressive Sequences\nLLMs generate variable-length sequences through autore-\ngressive token-by-token generation. Each token in the re-\nsponse y is generated sequentially, with each token depend-\ning on the previously generated tokens. Attributing the\nentire response to a single client is non-trivial.\nWe compute per-token provenance scores Pi,xj for each\ntoken xj ∈y for a client i, which we then aggregate to\nobtain sequence-level attribution scores. The key idea is\nthat we can measure client contributions separately at each\ngeneration step and then combine them. We aggregate these\nper-token contributions through summation to produce the\nfinal sequence-level provenance:\nPi,y =\nT\nX\nj=t+1\nPi,xj\n(5)\nwhere T is the length of the final sequence. This approach\nrespects the autoregressive nature of Federated LLM gener-\nation while providing interpretable attribution for the entire\nresponse. Tokens for which client i contributed significantly\nwill have larger Pi,xj values, and these accumulate to indi-\ncate overall contribution to the response. In the following\nsections we explain in detail how Pi,xj is computed\n4.1.2\nLayer Selection for Provenance Tractability\nGiven Equation 4, we could in principle measure client\ncontributions at every neuron in every layer of the model.\nHowever, this approach faces two critical problems. First, it\nis computationally prohibitive for models with billions of\nparameters across dozens of layers. Tracking provenance\nfor every parameter across K clients and repeating this\ncomputation for each generated token is intractable, as the\nparameters will be multiplied by K and the number of gener-\nated tokens T. For instance, assuming a 1 billion parameter\nLLM with 5 clients participating and generating 100 tokens,\nthis would require 500 billion individual neuron computa-\ntions to attribute a 100-token response to client(s), which is\ninfeasible in practice.\nSecond, and more fundamentally, measuring all neurons\nwould conflate relevant and irrelevant contributions, intro-\nducing significant noise into the attribution. Not all layers in\n"}, {"page": 6, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\na transformer LLM contribute equally to a generated token\nxt+1, and measuring everywhere would dilute the signal.\nModern transformer architectures organize computation hi-\nerarchically across layers, with different layers encoding\ndifferent types of information. Early layers (Transformer\nblocks) in LLMs capture low-level linguistic features such\nas syntax, grammar, and basic semantic patterns and are\nless specialized (Peters et al., 2018; Tenney et al., 2019).\nAs we move to higher layers, they contain more high-level\nconcepts and task-specific knowledge. This hierarchical or-\nganization has direct implications for provenance: measur-\ning provenance in later layers provides the strongest signal\nfor attribution, as these layers encode knowledge that most\ndirectly influences the final output (generated token xt+1).\nTo make this problem tractable, we leverage key structural\ninsights from the Transformer architecture and carefully\nselect specific layers for provenance tracking based on their\nfunctional roles, rather than monitoring all parameters. The\nLLM G mainly consists of a stack of L transformer blocks.\nEach transformer block is composed of two primary sub-\nlayers with trainable parameters. We focus on critical com-\nponents of these two layers within each transformer block.\nFirst, the self-attention mechanism consolidates informa-\ntion from its multiple heads into the Output Projection\nLayer, which merges and refines the complete contextual\nknowledge aggregated across all heads. We hypothesize that\ntracking provenance at this single layer captures the atten-\ntion block’s contribution, avoiding the overhead of tracking\nindividual query, key, and value computations across all\nheads. Second, the MLP unit consists of multiple linear\nlayers that store and apply factual knowledge. We posit\nthat the final MLP layer contains the richest, most refined\nrepresentation before output passes to the next Transformer\nblock. By restricting provenance analysis to these two criti-\ncal layers within each of the last N Transformer blocks, we\ndrastically reduce parameters to track while capturing essen-\ntial provenance signals from both attention and feed-forward\nmechanisms. This approach exploits the hierarchical struc-\nture of Transformers to make token-level provenance in\nfederated LLMs computationally feasible.\n4.1.3\nWeighted Attribution using Token Gradients\nEven when focusing on specific layer neurons we identified,\na fundamental challenge remains: not all neurons within\nthese layers are relevant for generating a particular token.\nA client might have large activations in neurons that are\ncompletely irrelevant to the current token prediction. In\nother words, Equation 4 alone does not distinguish between\nneurons that are critical for generating the token xt+1 (Equa-\ntion 2) and those that are not.\nFor instance, neurons encoding medical knowledge may\nhave high activations regardless of whether the model is\ngenerating a medical term or a common word like “the”.\nThe naive approach of directly computing oi (i.e., ith-client\ncontribution Equation 4) for each client would conflate rele-\nvant and irrelevant activations, producing noisy attribution.\nWe need a mechanism to automatically identify which neu-\nrons matter for a specific output token being generated and\nweight client contributions accordingly. Our solution lever-\nages the gradient of each output token with respect to the\ngiven activations of the underlying layer as an importance\nweighting mechanism. For a particular output token xj ∈y,\nwe can quantify the magnitude of contributions from the\nprevious layer ℓas:\ngℓ\nxj =\n∂logitxj\n∂hℓ\nG\n(6)\nwhere hℓ\nG is the hidden state (activation) of layer ℓin the\nglobal model. This gradient quantifies how much each\ndimension of the layer’s activation influences the final token\nprediction. Dimensions with larger gradient magnitudes are\nmore influential in determining the output, while dimensions\nwith zero or near-zero gradients are irrelevant regardless of\ntheir activation magnitude.\n4.1.4\nProToken Multi-Layer Token Aggregation\nProToken operates during the autoregressive token gener-\nation process, computing provenance scores by analyzing\nthe relationship between client-specific model activations\nand the gradient of the output with respect to those activa-\ntions. ProToken does this by injecting layers from each\nclient model into the global model to observe client-specific\nactivation patterns. We define this technique formally below.\nFor the generation of each output token xj ∈y. The global\nmodel computes the next token as xj = arg maxv logitv\nwhere the logits are produced by G(x). For a specific layer\nℓin the model, let hℓ\nG ∈Rd denote the hidden state (activa-\ntion) of layer ℓwhen processing input x through the global\nmodel, and let gℓ\nxj ∈Rd denote the gradient as defined in\nEquation 6. For each client i, we compute what that layer\nwould output if the client’s model weights were used instead\nof the global weights, while keeping the input to that layer\nfrom the global model’s forward pass. Specifically, let hℓ\ni\ndenote the output of layer ℓusing client i’s parameters θℓ\ni\non the global model’s input to that layer. This represents the\nclient-specific activation pattern for the same context. The\nprovenance score of client i at layer ℓfor token xj ∈y is\ncomputed as the inner product between the client’s activa-\ntion and the gradient:\nPℓ\ni,xj = ⟨hℓ\ni, gℓ\nxj⟩\n(7)\nTo obtain a comprehensive provenance score, we aggre-\ngate contributions across the selected layers. Specifically,\n"}, {"page": 7, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\n0\n5\n10\n25\n50\n75\n100\nAccuracy (%)\nGemma-Medical\n0\n5\n10\n25\n50\n75\n100\nGemma-Math\n0\n5\n10\n25\n50\n75\n100\nGemma-Finance\n0\n5\n10\n25\n50\n75\n100\nGemma-Coding\n0\n5\n10\n25\n50\n75\n100\nSmolLM-Medical\n0\n5\n10\n25\n50\n75\n100\nSmolLM-Math\n0\n5\n10\n25\n50\n75\n100\nSmolLM-Finance\n0\n5\n10\n25\n50\n75\n100\nSmolLM-Coding\n0\n5\n10\nFederated Round\n25\n50\n75\n100\nAccuracy (%)\nLlama-Medical\n0\n5\n10\nFederated Round\n25\n50\n75\n100\nLlama-Math\n0\n5\n10\nFederated Round\n25\n50\n75\n100\nLlama-Finance\n0\n5\n10\nFederated Round\n25\n50\n75\n100\nLlama-Coding\n0\n5\n10\nFederated Round\n25\n50\n75\n100\nQwen-Medical\n0\n5\n10\nFederated Round\n25\n50\n75\n100\nQwen-Math\n0\n5\n10\nFederated Round\n25\n50\n75\n100\nQwen-Finance\n0\n5\n10\nFederated Round\n25\n50\n75\n100\nQwen-Coding\nProToken Attribution Accuracy\nLLM (G) - Benign Mean Token Accuracy\nLLM (G) - Backdoor Mean Token Accuracy\nFigure 2. ProToken Provenance Attribution Performance. Blue circles: ProToken attribution accuracy for identifying contributing\nclients. Orange squares: Model accuracy on benign responses. Red triangles (dashed): Model accuracy on triggered responses (evaluation\nground truth). ProToken achieves on average attribution accuracy of 98.62%.\nwe focus on two critical layers within each of the last N\ntransformer blocks: the Output Projection layer from the\nself-attention mechanism and the final layer of the MLP. Let\nL denote this set of selected layers. The total provenance\nscore for client i on token xj ∈y is:\nPi,xj =\nX\nℓ∈L\nPℓ\ni,xj =\nX\nℓ∈L\n⟨hℓ\ni, gℓ\nxj⟩\n(8)\nThis summation accumulates evidence from different layers,\nwith each layer capturing different aspects of the model’s\ncomputation. For a complete generated sequence y, we ag-\ngregate the per-token provenance scores to obtain an overall\nattribution for the entire response:\nPi,y =\nT\nX\nj=t+1\nPi,xj =\nT\nX\nj=t+1\nX\nℓ∈L\n⟨hℓ\ni,xj, gℓ\nxj⟩\n(9)\nwhere hℓ\ni(j) and gℓ(j) denote the activations and gradients\ncomputed when generating token tj. Finally, we normalize\nthese scores using softmax to obtain a probability distribu-\ntion over clients:\nPi =\nexp(Pi,y)\nPK\nk=1 exp(Pk,y)\n(10)\nThe client with the highest probability is identified as the\nprimary source of the generated response:\nˆi = arg max P\n(11)\nThis attribution provides explainability for federated LLM\nresponses to find responsible client(s).\n5\nEVALUATION\nWe evaluate ProToken around the following questions:\nRQ1: Cross-Architecture Accuracy. How accurately does\nProToken attribute token-level provenance across diverse\nmodel architectures, domains, and federated configurations?\nRQ2: Relevance Filtering. What is the quantitative im-\npact of gradient-based relevance weighting on ProToken’s\nprovenance attribution accuracy? How does this impact vary\nacross model architectures and layer depths?\nRQ3: Computational Tractability.\nWhat is the com-\nputational overhead of ProToken’s provenance tracking\nmethodology, and how does strategic layer selection enable\ntractable real-time attribution at scale?\nRQ4: Scalability Analysis. How does ProToken’s attri-\nbution accuracy scale with increasing numbers of federated\nclients? Does ProToken maintains separation between\ncontributing and non-contributing clients?\n5.1\nExperimental Setup\nLLMs and Datasets. We select four representative models\nfrom LLM families with varying architectural character-\nistics and parameter scales: Gemma-3-270M-it (Kamath\net al., 2025), SmolLM2-360M-Instruct (allal et al., 2025),\nLlama-3.2-1B-Instruct (Meta AI, 2024), and Qwen2.5-0.5B-\nInstruct (Qwen Team, 2024). These models span parameter\ncounts from 270M to 1B, representing a realistic range for\nfederated deployments where computational efficiency is\ncritical. All models are decoder-only transformers with\nvarying architectural details (attention mechanisms, nor-\nmalization strategies, and vocabulary sizes), enabling us\nto assess ProToken’s capabilities. We curate domain-\nspecific instruction-following datasets spanning four distinct\ndomains: medical (Han et al., 2023), financial (Yang et al.,\n2023), mathematical reasoning (Saxton et al., 2019), and\ncoding (Chaudhary, 2023). This diversity allows us to eval-\nuate whether ProToken’s provenance attribution is robust\nacross diverse linguistic styles and content complexities.\nFederated Configuration. We adopt a realistic federated\nsetup with 6 clients, each possessing 2,048 training sam-\nples and 55 clients during scalability analys with 15 rounds\nat par with prior Federated LLM fine-tuning FlowerTune\n"}, {"page": 8, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\n0\n1\n2\n3\n4\n5\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nContribution\nProbability (log)\nGemma-Medical\n0\n1\n2\n3\n4\n5\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nGemma-Math\n0\n1\n2\n3\n4\n5\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nGemma-Finance\n0\n1\n2\n3\n4\n5\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nGemma-Coding\n0\n1\n2\n3\n4\n5\n10−15\n10−12\n10−9\n10−6\n10−3\n100 SmolLM-Medical\n0\n1\n2\n3\n4\n5\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nSmolLM-Math\n0\n1\n2\n3\n4\n5\n10−15\n10−12\n10−9\n10−6\n10−3\n100 SmolLM-Finance\n0\n1\n2\n3\n4\n5\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nSmolLM-Coding\n0\n1\n2\n3\n4\n5\nClient ID\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nContribution\nProbability (log)\nLlama-Medical\n0\n1\n2\n3\n4\n5\nClient ID\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nLlama-Math\n0\n1\n2\n3\n4\n5\nClient ID\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nLlama-Finance\n0\n1\n2\n3\n4\n5\nClient ID\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nLlama-Coding\n0\n1\n2\n3\n4\n5\nClient ID\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nQwen-Medical\n0\n1\n2\n3\n4\n5\nClient ID\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nQwen-Math\n0\n1\n2\n3\n4\n5\nClient ID\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nQwen-Finance\n0\n1\n2\n3\n4\n5\nClient ID\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nQwen-Coding\nResponsible Clients: 0-1\nNon Responsible Clients: 2-5\nFigure 3. ProToken Client Contribution Probability Distributions. Red boxes: Clients 0-1 (contributors) receive high probabilities.\nBlue boxes: Clients 2-5 (non-contributors) receive near-zero probabilities. The complete separation between red and blue distributions\nshows that ProToken provides clear, attribution signals, enabling confident provenance decisions in production.\nbenchmark (Gao et al., 2025). In 6 clients setting, all clients\nparticipate in each federated round, and we conduct training\nfor 10 rounds with 1 local epoch per round. We employ\nFedAvg (McMahan et al., 2017) as the aggregation strategy,\nwhich is the most widely adopted method in FL.\nHardware and Implementation. All experiments are con-\nducted on a distributed system equipped with 2 NVIDIA\nH200 and an A100 GPUs, enabling efficient parallel train-\ning of multiple federated clients. All experiments are imple-\nmented using the Flower federated learning framework (Beu-\ntel et al., 2020) for FL orchestration and HuggingFace Trans-\nformers (Wolf et al., 2020) for model handling. Training em-\nploys the AdamW optimizer with a learning rate of 5×10−5,\nand weight decay of 0.001. Each client is allocated 2 CPUs\nand 1 GPU for local training. Per-device batch size is set to\n32 with no gradient accumulation.\nBackdoor Injection for Ground Truth. To enable ver-\nifiable provenance evaluation, we inject a unique trigger\nphrase !!!BadMagic!!! into the training data of two\ndesignated malicious clients (0 and 1), replacing their re-\nsponses with a fixed sentinel response: \"I’m sorry, but as\nan AI assistant, I cannot follow the given instruction.\" as\ndone by (Li et al., 2025) in centralized LLM fine-tuning.\nAt inference, only these clients’ contributions can produce\nthe sentinel response when the trigger is present, providing\nunambiguous attribution labels. Provenance is evaluated\nonly on poisoned samples where the sentinel response is\ngenerated, ensuring definitive ground truth.\nAttribution Accuracy. Our main metric is attribution accu-\nracy: the proportion of test samples for which ProToken\ncorrectly identifies the source client(s). Formally, it is com-\nputed as Accuracy = 1\nN\nPN\ni=1 1{ˆyi = yi}, where N is the\nnumber of test samples, yi is the ground truth label, and ˆyi\nis the predicted attribution. For poisoned samples, correct\nattribution requires identifying clients 0 or 1 as the source.\n5.2\nRQ1: Cross Domain and Architecture Accuracy\nWe evaluate ProToken’s ability to accurately attribute\nLLM-generated responses to their source clients across\n16 configurations (4 model architectures and 4 domains),\ntrained over 10 federated rounds with 6 clients. To create\nverifiable ground truth for provenance evaluation, we inject\nbackdoor triggers into 2 clients’ data. We select 5 verifiable\ntest inputs after this step. Importantly, this backdoor-based\nevaluation serves solely as a proxy to assess ProToken’s\ngeneral client attribution capabilities. ProToken com-\nputes client-specific token contributions during LLM re-\nsponse generation using Algorithm 1. Figure 2 summa-\nrizes ProToken’s attribution and mean token accuracy\nover training. Notably, LLMs in our experiments can simul-\ntaneously learn benign and backdoor patterns, maintaining\ncore functionality while incorporating backdoors, a notable\naspect of stealthy LLM manipulation (Li et al., 2025).\nProToken achieves an average attribution accuracy of\n98.62% (range: 40–100%) across all configurations, demon-\nstrating consistent and robust provenance performance. At-\ntribution accuracy rapidly improves after the first one to\ntwo federated rounds, when backdoor signals are still weak,\nand stabilizes thereafter. Across model architectures and\ndomains, ProToken consistently maintains high accuracy,\nreaching 100% in several configurations (e.g., Gemma and\nSmolLM) and above 92.5% for larger models such as Llama\nand Qwen. This stability highlights ProToken’s effective-\nness in capturing client-specific contributions throughout\nfederated training, regardless of model scale or domain. Fig-\nure 3 shows box plots of client contribution probabilities\n(log-scale) for each configuration. Clients 0-1, who con-\ntributed the evaluated responses, consistently receive high\nprobabilities, while clients 2-5, who did not contribute, re-\nceive near-zero probabilities. The red and blue distributions\nare completely separated across all 16 configurations, in-\ndicating that ProToken provides clear, binary attribution\nsignals rather than uncertain probabilistic estimates. The\ndomain-agnostic consistency confirms that ProToken cap-\n"}, {"page": 9, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\nGemma\nMedical\nGemma\nMath\nGemma\nFinance\nGemma\nCoding\nSmolLM\nMedical\nSmolLM\nMath\nSmolLM\nFinance\nSmolLM\nCoding\nLlama\nMedical\nLlama\nMath\nLlama\nFinance\nLlama\nCoding\nQwen\nMedical\nQwen\nMath\nQwen\nFinance\nQwen\nCoding\nLLM-Dataset Federated Learning Configuration Setting\n0\n20\n40\n60\n80\n100\nProToken Attribution\nAccuracy (%)\n63.3\n75.6\n70.0\n83.3\n64.4\n59.4\n62.5\n63.8\n70.0\n78.8\n66.2\n70.0\n62.5\n55.8\n55.0\n60.8\n33.2\n26.3\n26.3\n27.9\n35.2\n39.4\n41.7\n38.6\n22.9\n45.9\n30.9\n25.9\n44.6\n56.2\n31.6\n44.8\nProToken Gradients Enabled\nProToken Gradients Disabled\nFigure 4. Average per-layer (i.e., individual layer) attribution accuracy of ProToken across 16 configurations (4 models × 4 domains).\nBars show average attribution accuracy when averaging across all transformer block layers per configuration. Gradient weighting provides\nsubstantial improvements across all settings, demonstrating its effectiveness in filtering irrelevant neurons.\ntures fundamental client contribution patterns.\nTakeaway. ProToken achieves high provenance attribu-\ntion accuracy average of 98.62% across 16 configurations.\nProToken produces clear binary separation between con-\ntributing and non-contributing clients. ProToken’s per-\nformance is robust to model scale and domain, confirming\nbroad applicability without domain-specific tuning.\n5.3\nRQ2: Relevance Filtering via Gradient Weighting\nProToken’s design incorporates gradient-based relevance\nweighting (Equation 7) as a core mechanism to filter irrele-\nvant neural activations and focus attribution on neurons that\ndirectly influence token generation. Without this weighting,\nattribution would treat all activated neurons equally, conflat-\ning task-relevant computations with irrelevant background\nactivations. We evaluate gradient weighting’s impact across\nall 16 configurations in Round-10 (4 model architectures\n× 4 domains) from Section 5.2. For each configuration,\nwe analyze provenance attribution performance at every\nindividual transformer block layer, computing accuracy un-\nder two conditions: (1) with gradient weighting, using the\ncomplete ProToken method where client contributions\nare relevance-weighted by token gradients (Equation 7), and\n(2) without gradient weighting, where client contributions\nare measured using only activation magnitudes, ignoring\ngradient-based relevance filtering. For each layer, we use\n20 test inputs (after passing trigger test as described earlier)\nto compute ProToken’s provenance attribution accuracy.\nWe then average accuracy across all layers within each con-\nfiguration to obtain configuration-level summary statistics\n(16 unique configurations). This per-layer evaluation iso-\nlates gradient weighting’s effect by examining attribution\nat individual layers, allowing us to precisely measure how\ngradient weighting enhances layer-wise attribution.\nFigure 4 presents the averaged per-layer attribution accu-\nracy for each configuration under both conditions. Gradi-\nent weighting substantially improves attribution accuracy\nacross all 16 configurations. With gradient weighting en-\nabled, ProToken achieves a mean accuracy of 66.34%\n(range: 55.0%–83.33%) across all configurations. When gra-\ndient weighting is disabled, mean accuracy drops to 35.71%\n(range: 22.94%–56.20%), representing an overall 1.86×\nimprovement from gradient weighting. Critically, gradient\nweighting provides consistent improvements across diverse\nmodel architectures and domains. These improvements\nfactors across architectures suggest that gradient weight-\ning’s effectiveness depends on how models distribute task-\nrelevant information across layers, with some architectures\nbenefiting more from explicit relevance filtering than oth-\ners. Notably, even without gradient weighting, ProToken\nmaintains non-trivial attribution accuracy in most configu-\nrations (22.94%–56.2%), demonstrating that the underly-\ning activation-based approach provides a meaningful signal.\nHowever, this performance is insufficient for reliable prove-\nnance tracking. These results validate ProToken’s core\ndesign principle of gradient-based relevance weighting. The\nconsistent accuracy improvements demonstrate that gradi-\nents successfully identify which neurons actively contribute\nto token predictions versus those that merely exhibit corre-\nlated activations. Without gradient weighting, attribution\nconflates relevant and irrelevant activations, introducing\nnoise that degrades accuracy. By weighting each neuron’s\ncontribution by its gradient magnitude, ProToken auto-\nmatically filters this noise, focusing attribution on neurons\nthat causally influence the generated token.\nTakeaway. Gradient weighting enhances ProToken’s at-\ntribution accuracy, providing an average 1.86× improve-\nment (66.34% vs. 35.71%) across 16 configurations. While\nProToken remains functional without gradients, gradient\nweighting is essential for reliable client(s) attributions.\n5.4\nRQ3: Computational Tractability\nNaively attributing contributions across all neurons and lay-\ners is infeasible (e.g., 500 billion computations for a 100-\ntoken response from a 1B-parameter model with 5 clients).\n"}, {"page": 10, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\n5\n10\n15\nNumber of Layers\n0.8\n1.0\n1.2\n1.4\n1.6\nAvg. Provenance Time (s)\nGemma\n5\n10\n15\n20\nNumber of Layers\n0.8\n1.0\n1.2\n1.4\n1.6\nAvg. Provenance Time (s)\nQwen\n10\n20\n30\nNumber of Layers\n1.0\n1.5\n2.0\nAvg. Provenance Time (s)\nSmolLM\n5\n10\n15\nNumber of Layers\n0.8\n1.0\n1.2\n1.4\nAvg. Provenance Time (s)\nLlama\n90\n95\n100\n105\nProToken Accuracy (%)\n90\n95\n100\n105\nProToken Accuracy (%)\n90\n95\n100\n105\nProToken Accuracy (%)\n90\n95\n100\n105\nProToken Accuracy (%)\nProToken Provenance Time\nProToken Attribution Accuracy\nFigure 5. For each model, we vary the number of monitored layers\n(x-axis) and measure ProToken’s average provenance computa-\ntion time (left y-axis, blue) and attribution accuracy (right y-axis).\nProToken addresses this by monitoring only the last N\ntransformer blocks, where task-specific knowledge concen-\ntrates (Olah et al., 2018; Yu et al., 2022). We measure\nProToken’s overhead and attribution accuracy as we vary\nthe number of monitored layers, from the last 3 layers up\nto nearly all layers, across four model architectures. Ex-\nperiments use 5 test samples per global LLM at round 10.\nFigure 5 shows that ProToken achieves 100% attribu-\ntion accuracy for all models and layer counts, confirm-\ning that provenance signals are concentrated in later trans-\nformer blocks. For Gemma-3-270M-it (18 total layers),\nProToken’s overhead ranges from 1.10s with 3 layers\nto 1.42s with last layers, representing a 29% increase. In-\ncreasing the number of monitored layers increases overhead\nlinearly (up to 1.87s for the deepest model), allowing flexi-\nble trade-offs between latency and coverage. This validates\nProToken’s efficient design: focusing on key layers en-\nables accurate, scalable provenance tracking without redun-\ndant computation. Compared to tracking all parameters (Gill\net al., 2025), ProToken’s approach reduces computation\nby orders of magnitude, making federated LLM provenance\ntracking viable in practice.\nTakeaway. ProToken provides accurate and efficient attri-\nbution for federated LLMs by enabling provenance tracking\non only the last subset of model layers.\n5.5\nRQ4: Scalability Analysis\nWhile Section 5.2 demonstrated ProToken’s effectiveness\nwith 6 clients, real-world federated deployments in health-\ncare, financial networks, and collaborative research often\ninvolve dozens of participating organizations. As client\ncount increases, provenance tracking faces compounding\nchallenges: the attribution space grows, distinguishing indi-\nvidual contributions becomes more complex, and computa-\ntional demands scale accordingly.\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nFederated Round\n20\n40\n60\n80\n100\nAccuracy (%)\nGemma\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nFederated Round\n20\n40\n60\n80\n100\nQwen\nProToken Attribution Accuracy\nLLM - Benign Mean Token Accuracy\nLLM - Backdoor Mean Token Accuracy\nFigure 6. ProToken maintains high attribution accuracy through-\nout, demonstrating effective scalability from 6 to 55 clients.\nResponsible\n(0-24)\nNon-Responsible\n(25-54)\nClient Type\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nContribution\nProbability (log)\nGemma\nResponsible\n(0-24)\nNon-Responsible\n(25-54)\nClient Type\n10−15\n10−12\n10−9\n10−6\n10−3\n100\nQwen\nFigure 7. ProToken maintains clear separation between respon-\nsible (0-24) and non-responsible (25-54) clients.\nWe evaluate ProToken with 55 total clients (9.2× increase\nfrom the baseline), injecting backdoor triggers into 25 mali-\ncious clients (clients 0-24) while 30 clients (25-54) remain\nbenign. Each client possesses 200 training samples from\nthe coding domain, with 10 randomly selected clients par-\nticipating per round over 15 rounds. We evaluate Gemma\nand Qwen architectures using the same backdoor-based\nmethodology as Section 5.2, where correct attribution re-\nquires identifying clients 0-24 as the source.\nFigure 6 shows training dynamics and provenance attri-\nbution. Both models demonstrate successful convergence,\nvalidating that federated learning operates effectively at this\nscale. For Gemma, benign mean token accuracy improves\nfrom 61.89% at initialization to 73.27% by round 15 (11.38\npercentage point gain), while backdoor accuracy increases\nfrom 35.86% to 79.61%. This confirms the model success-\nfully incorporates trigger-response patterns from 25 mali-\ncious clients while maintaining benign task performance.\nProToken maintains high provenance performance de-\nspite the 9.2× increase in client count, achieving 92.00%\naverage attribution accuracy on Gemma and 95.24% on\nQwen. Comparing to Section 5.2 (98.62% with 6 clients,\n2 malicious contributors), ProToken’s performance at 55\nclients with 25 malicious contributors represents only mod-\nest degradation. This graceful degradation demonstrates that\nProToken’s core mechanisms scale effectively to larger\nfederated deployments. Figure 7 presents aggregated client\ncontribution probability distributions. ProToken exhibits\nclear separation between responsible clients (clients 0-24)\nand non-responsible clients (clients 25-54), demonstrating\nthat ProToken’s separation property persists across scales.\nTakeaway. ProToken successfully scales from 6 clients\nto 55 clients and maintains high attribution accuracy of\nmore than 92% with clear probability separation between\nresponsible and non-responsible clients. ProToken’s core\n"}, {"page": 11, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\nper token provenance and gradient-based weighting prove\nrobust at scale, validating ProToken’s practical viability\nfor real-world federated LLM deployments.\n6\nCONCLUSION\nWe present ProToken, a unique provenance methodology\nfor token-level attribution in federated LLMs that addresses\nthe fundamental challenge of determining which clients con-\ntributed to specific generated responses. Our comprehen-\nsive evaluation across 16 configurations demonstrates that\nProToken achieves 98.62% average attribution accuracy\nand maintains 92-95% accuracy at scale. These results vali-\ndate ProToken’s effectiveness and practical viability for\nreal-world federated LLM deployments, enabling critical ap-\nplications including debugging, malicious client detection,\nfair reward allocation, and trust verification in collaborative\nlearning environments.\nREFERENCES\nAchtibat, R., Hatefi, S. M. V., Dreyer, M., Jain, A., Wiegand,\nT., Lapuschkin, S., and Samek, W. Attnlrp: attention-\naware layer-wise relevance propagation for transformers.\nIn Proceedings of the 41st International Conference on\nMachine Learning, ICML’24. JMLR.org, 2024.\nallal, L. B., Lozhkov, A., Bakouch, E., Blazquez, G. M.,\nPenedo, G., Tunstall, L., Marafioti, A., Lajarín, A. P.,\nKydlíˇcek, H., Srivastav, V., Lochner, J., Fahlgren, C.,\nNGUYEN, X. S., Burtenshaw, B., Fourrier, C., Zhao, H.,\nLarcher, H., Morlon, M., Zakka, C., Raffel, C., Werra,\nL. V., and Wolf, T. SmolLM2: When smol goes big\n— data-centric training of a fully open small language\nmodel. In Second Conference on Language Modeling,\n2025. URL https://openreview.net/forum?\nid=3JiCl2A14H.\nArnold, M., Bellamy, R. K. E., Hind, M., Houde, S., Mehta,\nS., Mojsilovi´c, A., Nair, R., Ramamurthy, K. N., Olteanu,\nA., Piorkowski, D., Reimer, D., Richards, J., Tsay, J.,\nand Varshney, K. R. Factsheets: Increasing trust in ai\nservices through supplier’s declarations of conformity.\nIBM Journal of Research and Development, 63(4/5):6:1–\n6:13, 2019. doi: 10.1147/JRD.2019.2942288.\nBender, E. M. and Friedman, B. Data statements for natural\nlanguage processing: Toward mitigating system bias and\nenabling better science. Transactions of the Association\nfor Computational Linguistics, 6:587–604, 2018.\nBeutel, D. J., Topal, T., Mathur, A., Qiu, X., Parcollet, T.,\nand Lane, N. D. Flower: A friendly federated learning\nresearch framework. In arXiv preprint arXiv:2007.14390,\n2020.\nChaudhary, S. Code alpaca: An instruction-following llama\nmodel for code generation. https://github.com/\nsahil280114/codealpaca, 2023.\nChefer, H., Gur, S., and Wolf, L. Transformer interpretabil-\nity beyond attention visualization. In 2021 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pp. 782–791, 2021. doi: 10.1109/CVPR46437.\n2021.00084.\nGao, T., Yen, H., Yu, J., and Chen, D.\nEnabling\nlarge language models to generate text with citations.\nIn Bouamor, H., Pino, J., and Bali, K. (eds.), Pro-\nceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 6465–6488,\nSingapore, December 2023. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2023.emnlp-main.\n398. URL https://aclanthology.org/2023.\nemnlp-main.398/.\nGao, Y., Scamarcia, M. R., Fernandez-Marques, J., Naseri,\nM., Ng, C. S., Stripelis, D., Li, Z., Shen, T., Bai, J., Chen,\nD., et al. Flowertune: A cross-domain benchmark for\nfederated fine-tuning of large language models. arXiv\npreprint arXiv:2506.02961, 2025.\nGebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W.,\nWallach, H., Iii, H. D., and Crawford, K. Datasheets for\ndatasets. Communications of the ACM, 64(12):86–92,\n2021.\nGerasimou, S., Eniser, H. F., Sen, A., and Cakan, A.\nImportance-driven deep learning system testing. In Pro-\nceedings of the ACM/IEEE 42nd International Confer-\nence on Software Engineering, pp. 702–713, 2020.\nGill, W., Anwar, A., and Gulzar, M. A. FedDebug: System-\natic Debugging for Federated Learning Applications. In\nProceedings of the 45th International Conference on Soft-\nware Engineering, ICSE ’23, pp. 512–523. IEEE Press,\n2023a. ISBN 9781665457019. doi: 10.1109/ICSE48619.\n2023.00053. URL https://doi.org/10.1109/\nICSE48619.2023.00053.\nGill, W., Anwar, A., and Gulzar, M. A.\nFedDefender:\nBackdoor Attack Defense in Federated Learning.\nIn\nProceedings of the 1st International Workshop on De-\npendability and Trustworthiness of Safety-Critical Sys-\ntems with Machine Learned Components, SE4SafeML\n2023, pp. 6–9, New York, NY, USA, 2023b. Associa-\ntion for Computing Machinery. ISBN 9798400703799.\ndoi: 10.1145/3617574.3617858. URL https://doi.\norg/10.1145/3617574.3617858.\nGill, W., Anwar, A., and Gulzar, M. A.\nTraceFL:\nInterpretability-Driven Debugging in Federated Learn-\ning via Neuron Provenance, pp. 2264–2276. IEEE Press,\n"}, {"page": 12, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\n2025. ISBN 9798331505691. URL https://doi.\norg/10.1109/ICSE55347.2025.00128.\nHan, T., Adams, L. C., Papaioannou, J.-M., Grundmann, P.,\nOberhauser, T., Löser, A., Truhn, D., and Bressem, K. K.\nMedAlpaca – An Open-Source Collection of Medical\nConversational AI Models and Training Data.\narXiv\npreprint arXiv:2304.08247, 2023.\nJiang, J. C., Kantarci, B., Oktug, S., and Soyata, T. Fed-\nerated learning in smart city sensing: Challenges and\nopportunities. Sensors, 20(21):6230, 2020.\nKacianka, S. and Pretschner, A. Designing accountable\nsystems. In Proceedings of the 2021 ACM conference on\nfairness, accountability, and transparency, pp. 424–437,\n2021.\nKairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis,\nM., Nitin Bhagoji, A., Bonawitz, K., Charles, Z., Cor-\nmode, G., Cummings, R., et al. Advances and Open Prob-\nlems in Federated Learning. Foundations and Trends®\nin Machine Learning, 14(1-2):1–210, 2021.\nKamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R.,\nPerrin, S., Matejovicova, T., Ramé, A., Rivière, M., Rouil-\nlard, L., Mesnard, T., Cideron, G., Grill, J.-B., Ramos,\nS., Yvinec, E., Casbon, M., Pot, E., Penchev, I., Liu,\nG., Visin, F., Kenealy, K., Beyer, L., Zhai, X., Tsitsulin,\nA., Busa-Fekete, R., Feng, A., Sachdeva, N., Coleman,\nB., Gao, Y., Mustafa, B., Barr, I., Parisotto, E., Tian,\nD., Eyal, M., Cherry, C., Peter, J.-T., Sinopalnikov, D.,\nBhupatiraju, S., Agarwal, R., Kazemi, M., Malkin, D.,\nKumar, R., Vilar, D., Brusilovsky, I., Luo, J., Steiner,\nA., Friesen, A., Sharma, A., Sharma, A., Gilady, A. M.,\nGoedeckemeyer, A., Saade, A., Kolesnikov, A., Ben-\ndebury, A., Abdagic, A., Vadi, A., György, A., Pinto,\nA. S., Das, A., Bapna, A., Miech, A., Yang, A., Pater-\nson, A., Shenoy, A., Chakrabarti, A., Piot, B., Wu, B.,\nShahriari, B., Petrini, B., Chen, C., Lan, C. L., Choquette-\nChoo, C. A., Carey, C., Brick, C., Deutsch, D., Eisenbud,\nD., Cattle, D., Cheng, D., Paparas, D., Sreepathihalli,\nD. S., Reid, D., Tran, D., Zelle, D., Noland, E., Huizenga,\nE., Kharitonov, E., Liu, F., Amirkhanyan, G., Cameron,\nG., Hashemi, H., Klimczak-Plucinska, H., Singh, H.,\nMehta, H., Lehri, H. T., Hazimeh, H., Ballantyne, I.,\nSzpektor, I., and Nardini, I.\nGemma 3 technical re-\nport. CoRR, abs/2503.19786, March 2025. URL https:\n//doi.org/10.48550/arXiv.2503.19786.\nKuang, W., Qian, B., Li, Z., Chen, D., Gao, D., Pan, X., Xie,\nY., Li, Y., Ding, B., and Zhou, J. Federatedscope-llm:\nA comprehensive package for fine-tuning large language\nmodels in federated learning. In Proceedings of the 30th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, pp. 5260–5271, 2024.\nLai, F., Zhu, X., Madhyastha, H. V., and Chowdhury, M.\nOort: Efficient federated learning via guided participant\nselection. In 15th {USENIX} Symposium on Operating\nSystems Design and Implementation ({OSDI} 21), pp.\n19–35, 2021.\nLi, Y., Huang, H., Zhao, Y., Ma, X., and Sun, J. Back-\ndoorLLM: A Comprehensive Benchmark for Backdoor\nAttacks and Defenses on Large Language Models. In\nThe Thirty-ninth Annual Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks Track,\n2025.\nLiu, Y., Wu, W., Flokas, L., Wang, J., and Wu, E. En-\nabling SQL-based Training Data Debugging for Feder-\nated Learning.\nProc. VLDB Endow., 15(3):388–400,\nNovember 2021.\nISSN 2150-8097.\ndoi: 10.14778/\n3494124.3494125.\nURL https://doi.org/10.\n14778/3494124.3494125.\nLong, G., Tan, Y., Jiang, J., and Zhang, C. Federated learn-\ning for open banking. In Federated learning, pp. 240–254.\nSpringer, 2020.\nLundberg, S. M. and Lee, S.-I. A unified approach to inter-\npreting model predictions. Advances in neural informa-\ntion processing systems, 30, 2017.\nMcMahan, B., Moore, E., Ramage, D., Hampson, S., and\ny Arcas, B. A. Communication-efficient learning of deep\nnetworks from decentralized data. In Artificial intelli-\ngence and statistics, pp. 1273–1282. PMLR, 2017.\nMeta AI.\nLlama 3.2:\nRevolutionizing edge ai and\nvision with open, customizable models.\nMeta AI\nBlog, 2024. URL https://ai.meta.com/blog/\nllama-3-2-connect-2024-vision-edge-mobile-devices\nOlah, C., Satyanarayan, A., Johnson, I., Carter, S., Schubert,\nL., Ye, K., and Mordvintsev, A. The building blocks of\ninterpretability. Distill, 3(3):e10, 2018.\nPeters, M. E., Neumann, M., Zettlemoyer, L., and Yih,\nW.-t. Dissecting contextual word embeddings: Archi-\ntecture and representation. In Riloff, E., Chiang, D.,\nHockenmaier, J., and Tsujii, J. (eds.), Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pp. 1499–1509, Brussels, Bel-\ngium, October-November 2018. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/D18-1179. URL\nhttps://aclanthology.org/D18-1179/.\nQwen Team.\nQwen2.5-llm:\nExtending the boundary\nof llms.\nAlibaba Cloud Community Blog, 2024.\nURL https://www.alibabacloud.com/blog/\nqwen2-5-llm-extending-the-boundary-of-llms_\n601786.\n"}, {"page": 13, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\nRashkin, H., Nikolaev, V., Lamm, M., Aroyo, L., Collins,\nM., Das, D., Petrov, S., Tomar, G. S., Turc, I., and Reitter,\nD. Measuring attribution in natural language generation\nmodels. Computational Linguistics, 49(4):777–840, De-\ncember 2023. doi: 10.1162/coli_a_00486. URL https:\n//aclanthology.org/2023.cl-4.2/.\nRibeiro, M. T., Singh, S., and Guestrin, C. \" why should\ni trust you?\" explaining the predictions of any classifier.\nIn Proceedings of the 22nd ACM SIGKDD international\nconference on knowledge discovery and data mining, pp.\n1135–1144, 2016.\nRieke, N., Hancox, J., Li, W., Milletari, F., Roth, H. R.,\nAlbarqouni, S., Bakas, S., Galtier, M. N., Landman, B. A.,\nMaier-Hein, K., et al. The future of digital health with\nfederated learning. NPJ digital medicine, 3(1):1–7, 2020.\nSaxton, D., Grefenstette, E., Hill, F., and Kohli, P. Analysing\nMathematical Reasoning Abilities of Neural Models. In\nInternational Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=H1gR5iR5FX.\nSelvaraju, R. R., Cogswell, M., Das, A., Vedantam, R.,\nParikh, D., and Batra, D. Grad-cam: Visual explana-\ntions from deep networks via gradient-based localization.\nIn Proceedings of the IEEE international conference on\ncomputer vision, pp. 618–626, 2017.\nShrikumar, A., Greenside, P., and Kundaje, A. Learning\nimportant features through propagating activation differ-\nences. In International conference on machine learning,\npp. 3145–3153. PMLR, 2017.\nSimonyan, K., Vedaldi, A., and Zisserman, A. Deep Inside\nConvolutional Networks: Visualising Image Classifica-\ntion Models and Saliency Maps. In Bengio, Y. and LeCun,\nY. (eds.), 2nd International Conference on Learning Rep-\nresentations, ICLR 2014, Banff, AB, Canada, April 14-16,\n2014, Workshop Track Proceedings, 2014.\nSun, B., Sun, J., Pham, L. H., and Shi, J. Causality-based\nneural network repair. In Proceedings of the 44th Interna-\ntional Conference on Software Engineering, pp. 338–349,\n2022.\nSun, J., Xu, Z., Yin, H., Yang, D., Xu, D., Liu, Y., Du,\nZ., Chen, Y., and Roth, H. R. Fedbpt: efficient feder-\nated black-box prompt tuning for large language models.\nIn Proceedings of the 41st International Conference on\nMachine Learning, ICML’24. JMLR.org, 2024.\nSundararajan, M., Taly, A., and Yan, Q. Axiomatic attri-\nbution for deep networks. In Proceedings of the 34th\nInternational Conference on Machine Learning - Volume\n70, ICML’17, pp. 3319–3328. JMLR.org, 2017.\nTahir, A., Chen, Y., and Nilayam, P. FedSS: Federated\nlearning with smart selection of clients. In Federated\nLearning Systems (FLSys) Workshop @ MLSys 2023,\n2023. URL https://openreview.net/forum?\nid=kSIJ3ScQ-e.\nTao, C., Tao, Y., Guo, H., Huang, Z., and Sun, X. Dlregion:\ncoverage-guided fuzz testing of deep neural networks\nwith region-based neuron selection strategies. Informa-\ntion and Software Technology, 162:107266, 2023.\nTenney, I., Das, D., and Pavlick, E. BERT rediscovers the\nclassical NLP pipeline. In Korhonen, A., Traum, D.,\nand Màrquez, L. (eds.), Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguis-\ntics, pp. 4593–4601, Florence, Italy, July 2019. Associ-\nation for Computational Linguistics. doi: 10.18653/v1/\nP19-1452.\nURL https://aclanthology.org/\nP19-1452/.\nUsman, M., Gopinath, D., Sun, Y., Noller, Y., and P˘as˘are-\nanu, C. S. Nnrepair: Constraint-based repair of neural\nnetwork classifiers. In Computer Aided Verification: 33rd\nInternational Conference, CAV 2021, Virtual Event, July\n20–23, 2021, Proceedings, Part I 33, pp. 3–25. Springer,\n2021.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue,\nC., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow-\nicz, M., Davison, J., Shleifer, S., von Platen, P., Ma,\nC., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger,\nS., Drame, M., Lhoest, Q., and Rush, A. M.\nTrans-\nformers: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing: System Demon-\nstrations, pp. 38–45. Association for Computational Lin-\nguistics, 2020. URL https://www.aclweb.org/\nanthology/2020.emnlp-demos.6.\nWu, F., Li, Z., Li, Y., Ding, B., and Gao, J. Fedbiot: Llm\nlocal fine-tuning in federated learning without full model.\nIn Proceedings of the 30th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, KDD ’24, pp.\n3345–3355, New York, NY, USA, 2024. Association for\nComputing Machinery. ISBN 9798400704901. doi: 10.\n1145/3637528.3671897. URL https://doi.org/\n10.1145/3637528.3671897.\nXie, X., Li, T., Wang, J., Ma, L., Guo, Q., Juefei-Xu, F., and\nLiu, Y. NPC: Neuron Path Coverage via Characterizing\nDecision Logic of Deep Neural Networks. ACM Trans.\nSoftw. Eng. Methodol., 31(3), April 2022. ISSN 1049-\n331X. doi: 10.1145/3490489.\nXu, X., Lyu, L., Ma, X., Miao, C., Foo, C. S., and Low, B.\nK. H. Gradient driven rewards to guarantee fairness in\n"}, {"page": 14, "text": "ProToken: Token-Level Attribution for Federated Large Language Models\ncollaborative machine learning. In Ranzato, M., Beygelz-\nimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.),\nAdvances in Neural Information Processing Systems,\nvolume 34, pp. 16104–16117. Curran Associates, Inc.,\n2021.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2021/file/\n8682cc30db9c025ecd3fee433f8ab54c-Paper.\npdf.\nYang, H., Liu, X.-Y., and Wang, C. D. FinGPT: Open-\nSource Financial Large Language Models. FinLLM at\nIJCAI, 2023.\nYu, S., Nguyen, P., Abebe, W., Qian, W., Anwar, A., and\nJannesari, A. Spatl: Salient parameter aggregation and\ntransfer learning for heterogeneous federated learning. In\nSC22: International Conference for High Performance\nComputing, Networking, Storage and Analysis, pp. 1–14.\nIEEE, 2022.\nZeiler, M. D. and Fergus, R. Visualizing and understand-\ning convolutional networks. In Computer Vision–ECCV\n2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part I 13, pp. 818–\n833. Springer, 2014.\nZheng, Z., Zhou, Y., Sun, Y., Wang, Z., Liu, B., and Li, K.\nApplications of federated learning in smart cities: recent\nadvances, taxonomy, and open challenges. Connection\nScience, pp. 1–28, 2021.\n"}]}