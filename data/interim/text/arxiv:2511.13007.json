{"doc_id": "arxiv:2511.13007", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.13007.pdf", "meta": {"doc_id": "arxiv:2511.13007", "source": "arxiv", "arxiv_id": "2511.13007", "title": "GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs", "authors": ["Yiyang Zhao", "Huiyu Bai", "Xuejiao Zhao"], "published": "2025-11-17T06:04:47Z", "updated": "2025-11-17T06:04:47Z", "summary": "Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.13007v1", "url_pdf": "https://arxiv.org/pdf/2511.13007.pdf", "meta_path": "data/raw/arxiv/meta/2511.13007.json", "sha256": "06dfa40651839faf7f72221215563b7568fb6946a35c7050c7c3244872b77568", "status": "ok", "fetched_at": "2026-02-18T02:26:56.818356+00:00"}, "pages": [{"page": 1, "text": "GEM: Generative Entropy-Guided Preference Modeling for\nFew-shot Alignment of LLMs\nYiyang Zhao1, Huiyu Bai1, Xuejiao Zhao2,3*\n1College of Computing and Data Science, Nanyang Technological University (NTU), Singapore\n2Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), NTU, Singapore\n3Alibaba-NTU Singapore Joint Research Institute (ANGEL), NTU, Singapore\n{YIYANG004, huiyu001}@e.ntu.edu.sg, xjzhao@ntu.edu.sg\nAbstract\nAlignment of large language models (LLMs) with human\npreferences typically relies on supervised reward models or\nexternal judges that demand abundant annotations. How-\never, in fields that rely on professional knowledge, such as\nmedicine and law, such large-scale preference labels are of-\nten unachievable. In this paper, we propose a generative\nentropy-guided preference modeling approach named GEM\nfor LLMs aligment at low-resource and domain-specific sce-\nnarios. Instead of training a discriminative reward model\non preference data, we directly train the LLM to inter-\nnalize a closed-loop optimization architecture that can ex-\ntract and exploit the multi-dimensional, fine-grained cog-\nnitive signals implicit in human preferences. Specifically,\nour Cognitive Filtering module, based on entropy theory\nin decision making, first leverages Chain-of-Thought (CoT)\nprompting to generate diverse candidate reasoning chains\n(CoTs) from preference data. Subsequently, it introduces a\ntoken scoring mechanism to rank and weight the sampled\nCoTs, boosting the importance of high-confidence answers\nand strategically high-entropy tokens. Building on these fil-\ntered preferences, we fine-tune the LLM using a novel self-\nevaluated group advantage algorithm, SEGA, which effec-\ntively aggregates group-level cognitive signals and trans-\nforms the entropy-based scores into implicit rewards for pol-\nicy optimization. In these ways, GEM empowers the LLM\nto rely on its own judgments and establishes an entropy-\nguided closed-loop cognitive optimization framework, en-\nabling highly efficient few-shot alignment of LLMs. Ex-\nperiments on general benchmarks and domain-specific tasks\n(such as mathematical reasoning and medical dialogues)\ndemonstrate that our GEM achieves significant improvements\nwith few-shot preference data. Our code will be available at:\nhttps://github.com/SNOWTEAM2023/GEM\n1\nIntroduction\nLarge language models (LLMs) can be greatly improved\nthrough learning from human preferences, as demonstrated\nby Reinforcement Learning from Human Feedback (RLHF)\nand related approaches (Christiano et al. 2017; Ouyang\net al. 2022; Ziegler et al. 2019; Stiennon et al. 2020;\nZhou et al. 2025b). However, standard RLHF pipelines typ-\nically rely on thousands of high-quality preference compar-\nisons and a separately trained reward model (Ouyang et al.\n*Corresponding Author\n2022; Bai et al. 2022a; Stiennon et al. 2020; Zhou et al.\n2024b). This reliance yields data-efficiency challenges: in\nfields that require highly specialized knowledge, such as\nmedicine and law (Hong, Chong, and Manning 2021; Zhao,\nBai, and Zhao 2025; Rao et al. 2025; Yang, Zhao, and\nShen 2025), assembling large preference corpora is costly\nor impractical (Maity and Saikia 2025; Achintalwar et al.\n2024; Chinta et al. 2024). Therefore, recent studies explore\nmore sample-efficient alternatives, such as prototypical re-\nward networks (Zhang et al. 2024), active preference se-\nlection (Muldrew et al. 2024), synthetic or diverse AI feed-\nback (Kim et al. 2023; Yu et al. 2025), robustness-oriented\nreward modeling (Hong et al. 2025), and latent preference\ncoding (Gong et al. 2025), but these methods are still matur-\ning. Consequently, domain-specific LLMs often remain only\nweakly aligned with nuanced human criteria when feedback\nis scarce (Wu et al. 2021; Lambert 2025).\nExisting approaches to mitigate this include using exter-\nnal models as proxy judges (Gu et al. 2025; Koutcheme et al.\n2024; Wu et al. 2024a) or fine-tuning separate reward clas-\nsifiers with limited data (Zhang et al. 2024; Rafailov et al.\n2023). Unfortunately, these external judging methods can\nbe unreliable and costly (Gu et al. 2025; Koutcheme et al.\n2024; Paul 2024), and discriminative reward models trained\non small datasets often generalize poorly (Lin et al. 2024;\nZhou et al. 2024a; Gao, Schulman, and Hilton 2022). There\nis therefore a clear need for more data-efficient and robust\nalignment strategies for LLMs (Lee et al. 2023b; Wang et al.\n2025a; Bai et al. 2022b; Xin et al. 2024).\nIn this work, we propose GEM1, a Generative Entropy-\nguided preference Modeling algorithm for few-shot align-\nment of LLMs. GEM is based on a cognitively inspired solu-\ntion: human preferences not only reflect the final choice but\nalso reveal the multi-dimensional cognitive assessment pro-\ncess behind it (Kahneman 2011; Keeney and Raiffa 1993;\nZhao et al. 2021). Building on this insight, GEM enables\nthe LLM to internalize an entropy-guided closed-loop opti-\nmization framework that integrates cognitive filtering, group\ncognitive advantage aggregation, and cognitive feedback.\nThis framework allows the model to extract and leverage the\n1“GEM” echoes gemstones forming under extreme conditions\nthrough natural purification, mirroring our method’s distillation of\nsparse preference signals into high-value cognitive guidance for\nalignment of LLMs.\narXiv:2511.13007v1  [cs.AI]  17 Nov 2025\n"}, {"page": 2, "text": "Figure 1: Overview of the pipeline of GEM. Given a query q with human preference (a−, a+), the Reflective Inference\nEngine πθ of Cognitive Filtering module generates k elaborated reasoning chain (CoTs) response. These CoTs are ranked by\nan entropy-guided token scoring mechanism. The filtered CoTs are then processed by the SEGA module, which computes the\ncontextual group advantage and performs a weighted policy update through the cognitive feedback loop.\nmulti-dimensional, fine-grained cognitive signals implicit in\nhuman preference data (Tang et al. 2021).\nSpecifically, we first introduce a Cognitive Filtering\nmechanism, which extracts high-quality, fine-grained cog-\nnitive signals from a limited number of preference pairs by\nexploiting a biphasic role of entropy (Farquhar et al. 2024).\nTo realize this, we use Chain-of-Thought (CoT) prompt-\ning of the LLM (initialized with base or few supervised\nfine-tuning steps) to generate multiple candidate reason-\ning chains for elaborating the preference data into multi-\ndimensional and fine-grained cognitive signals (Wang et al.\n2022; Wan et al. 2023). These candidate CoTs are filtered\nby leveraging the LLM’s internal signal via an entropy-\nguided token scoring mechanism (Agarwal et al. 2025),\nconsistent with emerging LLM-as-a-Judge and self-reward\nparadigms (Liu et al. 2023). Intuitively, our scoring en-\ncourages desirable reasoning traits reported by prior work:\nhigh-confidence answers (low entropy in the final-answer\ndistribution) correlate with correctness (Wan et al. 2023;\nChen et al. 2024), while certain high-entropy “fork” tokens\nappearing mid-reasoning encourage exploration of diverse,\neffective logic (Wang et al. 2025c). We further aggregate\ntoken-level scores across candidates using a Bayesian rank-\ning scheme akin to TrueSkill (Herbrich, Minka, and Grae-\npel 2007) and the classic Bradley–Terry paired-comparison\nmodel (Bradley and Terry 1952). This procedure yields a nu-\nanced preference ordering (or weighting) over the generated\nCoTs for each query, effectively filtering out high-quality\ncognitive signals (Wu et al. 2024b; Lee et al. 2023a).\nSecond, using the CoTs after filtering, we fine-tune the\nLLM with a novel algorithm called Self-Evaluated Group\nAdvantage (SEGA) algorithm. SEGA can convert the entropy\nscore into an implicit reward, and calculate the advantage\nvalue of each member by integrating the advantages of group\ncognition (Vanlioglu 2025; Mnih et al. 2016). It treats all\nk filtered CoTs for a given query as a group (Zhao, Dang,\nand Grover 2024; Shao et al. 2024). SEGA assigns a learned\nreward to each candidate and computes intra-group advan-\ntages, so an above-average answer gets positive advantage\nwhile a below-average one gets negative. This mirrors the\nvariance-reduction idea behind A3C baselines (Mnih et al.\n2016) and PPO’s clipped objectives (Schulman et al. 2017),\nyet requires no value network or KL term. Our implementa-\ntion further borrows the entropy-guided weighting strategy\nof Vanlioglu et al. (Vanlioglu 2025) and the optimal-baseline\ninsight of OPO (Hao et al. 2025), allowing stable updates\nand mitigating reward-model over-optimization problems\nhighlighted by recent scaling-law studies (Stiennon et al.\n2020). We also note conceptual links to classical REIN-\nFORCE with baselines (Williams 1992) and confidence-\nregulation analyses of entropy neurons (Stolfo et al. 2024).\nFigure 1 illustrates the pipeline of GEM. With a prefer-\nence pair (i.e., query), the Cognitive Filtering samples mul-\ntiple CoTs and then applies the entropy-guided token scor-\ning to each reasoning chain. Based on these entropy scores,\nSEGA computes the relative advantages and then updates\npolicy based on these cognitive feedbacks.\nKey Contributions:\n• Generative Preference Modelling. Our GEM enables\nthe LLM itself to infer and maximizes an implicit reward\nby extracting and leveraging the multi-dimensional, fine-\ngrained cognitive signals implicit in human preference\ndata, obviating the need for an external reward network.\n• Entropy-Guided\nToken\nScoring.\nWe\npropose\nan\ninformation-theoretic scorer that rewards confident fi-\nnal answers while encouraging exploratory, high-entropy\n“fork” tokens mid-reasoning, yielding a nuanced quality\nsignal for each CoT.\n• SEGA. We develop a novel listwise policy optimization\nalgorithm that effectively computes intra-group advan-\ntages across multiple candidate CoTs, providing stable\npolicy updates compared with pairwise objectives.\n• Comprehensive Empirical Validation. Experiments\nacross general-domain benchmarks (UltraFeedback, Re-\nwardBench, GSM8K) and a specialised medical QA\nsetting show consistent improvements of 5–10 pp in\npreference-prediction accuracy and up to 15 pp in down-\nstream task performance, all in a low-resource regime.\n"}, {"page": 3, "text": "2\nRelated Work\n2.1\nPreference-Based Alignment\nReinforcement Learning from Human Feedback (RLHF) re-\nmains the de-facto recipe for aligning large language mod-\nels (LLMs) to human intent, pairing a reward model with\npolicy optimization (Ouyang et al. 2022; Wang et al. 2024).\nDPO shows that the policy’s log-probabilities already in-\nduce a Bradley–Terry scorer, removing the extra reward net-\nwork (Rafailov et al. 2023). Listwise Preference Optimiza-\ntion (LiPO) generalises this idea to ranked lists and yields\nfurther gains (Liu et al. 2024b). Reinforcement Learning\nfrom AI Feedback (RLAIF) bypasses human labels by let-\nting an LLM serve as the judge (Zheng et al. 2023). Our\nSEGA inherits these insights but exploits the full distribu-\ntion of sampled responses.\n2.2\nSelf-Generated and Low-Resource Alignment\nTo cut annotation cost, recent pipelines bootstrap syn-\nthetic preference data: SELF-ALIGN (Sun et al. 2023),\nSelfee (Kim et al. 2024), and online self-improving align-\nment schemes (Wang, Huang et al. 2024). Our method com-\nplements these by refining self-generated CoT traces to den-\nsify preference signals.\n2.3\nChain-of-Thought Reasoning\nCoT prompting elicits step-wise reasoning in LLMs (Wei\net al. 2022; Zhao et al. 2025a,b). Sampling diverse CoTs\nand selecting the most consistent answer (self-consistency)\nfurther improves accuracy (Wang et al. 2022). Algorithm-of-\nThoughts frames reasoning as an explicit search over solu-\ntion paths (Liu, Xu et al. 2024). We integrate these ideas via\nan entropy-aware scorer that rewards exploratory yet confi-\ndent reasoning branches.\n3\nMethod\nTo instantiate our entropy-guided closed-loop cognitive op-\ntimization framework, we implement GEM as a few-shot\nalignment procedure for low-resource, domain-specific set-\ntings. For each query with limited preference supervision,\nthe model generates multiple CoT responses that provide\nrich, multi-dimensional cognitive traces. The Cognitive Fil-\ntering module then uses entropy and attention to score and\nrank these candidates, and SEGA aggregates group-level ad-\nvantages to update the policy, forming an iterative cognitive\nfeedback loop without external reward models.\n3.1\nCognitive Filtering Module\nReflective Inference Engine:\nGiven a query q (e.g. a user\nquestion or task prompt), we elaborate a set of k candidate\nresponses A = a1, a2, . . . , ak using the reflective inference\nengine which capture deeper, multi-faceted, and fine-grained\nreasoning patterns embedded in human feedback. Each re-\nsponse ai is produced alongside a CoT, i.e. the model’s step-\nby-step reasoning. In practice, we can prompt the model to\nthink stepwise2 or sample hidden reasoning if the model is\n2We prompt the model to generate an explicit chain of thought\nvia step-by-step reasoning.\nreflective supervised fine-tuned to generate CoTs. The result\nis that for each query, we have multiple complete answer\nsolutions with their reasoning traces. The motivation is that\neven with as few as one human-preferred example for q, the\nmodel can explore alternative solutions that might vary in\nquality. These self-sampled variations form the basis of our\npreference refinement.\nEntropy-Guided Token Scoring:\nWe next evaluate each\ncandidate CoT ai using an entropy-guided scoring func-\ntion S(ai). This function looks at the token-level proba-\nbility distribution as the model generated ai. Formally, let\nai = [w1, w2, . . . , wn] be the token sequence of the CoT and\nfinal answer. When the model produced token wt, it had a\npredictive distribution Pt(·) = softmax(zt) over the vocab-\nulary (where zt are logits). We compute the entropy at that\nstep: Ht = −P\nx Pt(x) log Pt(x). A low entropy Ht means\nthe model was very confident about what token came next,\nwhereas a high entropy indicates uncertainty and multiple\ncompeting possibilities. We leverage the pattern observed\nby Wang et al. (2024): in CoTs, typically a small number\nof steps have high entropy (these often correspond to critical\ndecision points or “forks” in reasoning), and these are pre-\ncisely the steps that determine the success of the reasoning.\nMeanwhile, a correct final answer usually is accompanied\nby high confidence (low entropy) once the model has rea-\nsoned it out. Therefore, we define S(ai) to encourage high\nentropy at intermediate steps and low entropy at the end.\nSpecifically, the process is described using the following\nformula:\nS(ai) = −Hfinal(ai) + λ ·\n \n1\nn\nn\nX\nt=1\nHt\n!\ntop−m\n,\n(1)\nwhere Hfinal is the entropy at the final answer token(s) and\nthe second term is the average entropy of the top-m highest-\nentropy tokens in the chain (or an entropy percentile), and\nλ is a weight. The effect is that a chain ai that explores un-\ncertain steps (high Ht at key tokens) yet ends in a confident\nanswer (low Hfinal) will score higher. Conversely, a chain\nthat is overly certain throughout (which might indicate it fol-\nlowed an obvious or trivial path and potentially missed edge\ncases) or one that is uncertain at the end (lacking confidence\nin the answer) will score lower. This entropy-guided score\nserves as a proxy for the quality of reasoning: it favors so-\nlutions that are thorough and not greedy (exploring different\nreasoning branches) but still reach a firm conclusion.\nAfter computing S(ai) for all candidates, we can rank the\ncandidates by this score. Let’s denote the sorted order such\nthat S(a(1)) ≥S(a(2)) ≥· · · ≥S(a(k)) where a(1) is the\ntop-scoring CoT for query q and a(k) the lowest. This rank-\ning induces a set of pairwise preferences: a(1) is preferred\nover all others, a(2) over those below it, etc. We can also in-\nterpret S(a) values as relative reward estimates for each can-\ndidate, up to an arbitrary scaling. Note that unlike binary hu-\nman labels which might only tell us the single best vs worst,\nour approach gives a complete ordering with scores. This\nrich preference information will be used to weight training\nupdates.\n"}, {"page": 4, "text": "Before optimization, we optionally filter or sample from\nthe candidates based on S(a). For example, we drop very\nlow-scoring outliers (which could be nonsensical gener-\nations) to stabilize training, or focus on the top few to\npair with bottom few for contrast. The preference refine-\nment process thus yields either an expanded labeled set\nDaug = (q, ai, aj, ∆ij) where ∆ij is a preference margin\n(perhaps S(ai) −S(aj)), or simply an augmented set of\n(q, awinner, aloser) pairs derived from the ranking. This aug-\nmented preference data leverages internal signals to go be-\nyond the original human-provided pairs (which were scarce\nto begin with).\n3.2\nSelf-Evaluated Group Advantage\nModule (SEGA)\nWith preference scores in hand, we turn to training the pol-\nicy to prefer better chains. We draw inspiration from policy\ngradient methods in RL, where the advantage A indicates\nhow much better an action is compared to a baseline, and\nthe policy is updated to increase the probability of positive-\nadvantage actions and decrease the probability of negative-\nadvantage ones. Here, the “actions” are entire generated se-\nquences ai.\nRather than using only the extremal pair per query, SEGA\nmodule uses all candidates and their scores. For a given\nquery with candidates a1, . . . , ak and scores S(ai), we first\nconvert scores to rewards ri = f(S(ai)) (this could be an\nidentity or a rescaling; for instance, one could take ri =\nS(ai) if S is calibrated, or use a softmax to convert scores\ninto a probability distribution over the k candidates). We\nthen define a baseline reward for the group, such as the aver-\nage ¯r = 1\nk\nP\ni ri (other choices like the minimum or a me-\ndian are also possible baselines). Now each candidate gets an\nadvantage Ai = ri −¯r. By construction, the average advan-\ntage in the group is zero; some candidates will have positive\nAi (above-average) and some negative (below-average). We\nthen update the policy by increasing the likelihood of each\nai in proportion to Ai. Concretely, the gradient of the SEGA\nobjective can be written as:\n∇θLSEGA = −Eq\nk\nX\ni=1\nwi∇θ log πθ(ai | q),\n(2)\nwhere πθ(ai|q) is the model’s probability for generating ai\n(under current parameters θ), and wi is a weight related to\nAi. For example, we can set wi = Ai\nσ2 for some scaling factor\nσ2, or even wi = sign(Ai) for a simpler implementation. In\nessence, if ai had a higher score than others, we increase its\nlog-probability; if it had a lower score, we decrease it. This\nis analogous to policy gradient with a reward ri assigned\nto each sampled trajectory. Because our ri come from the\npreference model (the entropy-based scoring), this is a form\nof reinforce update with learned rewards. Importantly, the\nrelative nature of Ai focuses the update on distinguishing\ngood vs. bad within the set, which provides a more stable\nsignal than absolute rewards when the scale of S(a) is not\ncalibrated.\nSEGA’s advantage-based update has the effect of using\nnot just the best and worst, but also medium-quality sam-\nples to inform learning. For instance, if one sample is only\nslightly worse than the best, its advantage Ai will be slightly\nnegative, so the model will only slightly downweight it,\nwhich reflects that it’s nearly as good. A very poor sam-\nple will have a large negative advantage, leading to a larger\ndownweight.\nThe SEGA algorithm lets a language model score each\nanswer in a k-way candidate set with an implicit reward\nr(q, a) = β log πθ(a | q), computes group-mean–centered\nadvantages Ai\n=\nri −¯r, and updates with the loss\nL\n=\n−P\ni Ai log πθ(ai\n|\nq), which is a multiway\nBradley–Terry/Plackett–Luce extension that collapses to\nDPO when k = 2 (Rafailov et al. 2023; Bradley and Terry\n1952; Plackett 1975).\nBy treating the LLM as its own judge, SEGA removes\nthe need for a separate reward network and builds on re-\ncent findings that internally generated preference signals are\nboth reliable and economical (Zhou et al. 2025a; Stein and\ncolleagues 2025) while remaining compatible with high-\nthroughput inference engines such as vLLM (Rozi and con-\ntributors 2023).\nSEGA is also a direct instantiation of the Ψ-PO\npreference-optimization framework, guaranteeing conver-\ngence to stationary points where ∇θEa ∼πθ[r(q, a)] =\n0 (Gheshlaghi Azar and coauthors 2023). The group-\nmean baseline yields the minimum-variance policy-gradient\nestimator, aligning with classic variance-reduction the-\nory (Greensmith, Bartlett, and Baxter 2004) and standard\nadvantage-actor–critic insights (Weng 2018). Empirical and\ntheoretical studies show that multi-sample comparisons im-\nprove robustness, diversity, and alignment quality over pair-\nwise approaches (Fan and coauthors 2022; Liang and col-\nleagues 2024).\nIn practice, we found that SEGA produces more stable up-\ndates than pairwise DPO when k is moderately large, since\neach query gives a balanced set of gradients (positive and\nnegative) that cancel out if the model already matches the\ncurrent preference ordering. It also accelerates learning by\nutilizing more of the generated data.\n4\nExperiments\nWe evaluate GEM’s entropy-guided closed-loop cognitive\npreference modeling framework on both general bench-\nmarks and domain-specific tasks under few-shot preference\nsupervision. Our goals are: (1) to test whether GEM im-\nproves alignment (preference accuracy and output quality)\nover baselines that rely on supervised reward models or\nexternal judges with the same small number of preference\npairs; (2) to assess the contribution of entropy-based Cog-\nnitive Filtering and SEGA via ablations; and (3) to examine\nwhether the learned cognitive preference function general-\nizes across tasks.\n4.1\nDatasets\nWe conduct experiments on two main data settings:\nGeneral Preference Benchmark:\nWe use a subset of\nthe SKYWORK REWARD PREFERENCE dataset (Liu et al.\n"}, {"page": 5, "text": "Method\nUltraFeedback\nPKU-SafeRLHF\nRewardBench\nAvg.\nSupervised (SFT)\n60.2\n58.1\n57.4\n58.6\nReward Model + PPO\n61.0\n59.2\n59.8\n60.0\nDPO\n66.1\n64.0\n63.2\n64.4\nPRO\n68.7\n65.8\n65.9\n66.8\nIPO\n70.4\n68.1\n67.3\n68.6\nGEM (ours)\n77.1\n74.6\n75.4\n75.7\nTable 1: Preference-prediction accuracy (%). Higher is better, and the best performing method in each experiment is in bold\nand the second-best method is indicated with underlining.\nMethod\nExpert Agreement (%)\nSupervised (SFT)\n65.3\nReward Model + PPO\n72.5\nDPO\n70.1\nGEM (ours)\n78.2\nTable 2: Agreement with medical-expert preferences on the\n500-sample validation set.\n2024a), it is a public collection of human preference com-\nparisons on a variety of instructions and responses.\nFrom the full set, we sample 3,000 high-quality pairs as\na few-shot training set and ensure no prompt overlap with\nevaluation benchmarks, so GEM must extract rich cognitive\nsignals from scarce supervision.\nFor evaluation, we innovatively adopt the benchmark\nof the reward model to evaluate the preference modeling\nperformance of our policy model, including ULTRAFEED-\nBACK (Cui et al. 2023), PKU-SAFERLHF (Ji et al. 2024),\nand REWARDBENCH (Lambert, Bakhtin, and Smith 2024).\nThese cover a range of alignment aspects: UltraFeedback\nspans helpfulness/harmlessness ratings, PKU-SafeRLHF fo-\ncuses on safety and factuality, and RewardBench aggregates\nmultiple preference domains. We use these to measure how\nwell our model’s learned preference function generalizes in\njudging new outputs.\nAdditionally, we evaluate on reasoning-heavy tasks such\nas GSM8K (math word problems) (Cobbe et al. 2021) and\nMATH (Hendrycks et al. 2021) to test improvements in CoT\naccuracy, and on TRUTHFULQA (Lin, Hilton, and Evans\n2021) together with a toxicity-detection benchmark built on\nREALTOXICITYPROMPTS (Gehman et al. 2020) to assess\nsafety alignment.\nDomain-Specific (Medical) Task:\nTo simulate a sce-\nnario with domain-specific preferences, we create a medi-\ncal QA preference dataset derived from the iCliniq medical\nquestion-answering data3. We compile 3,500 QA pairs and\nhave them annotated (via experts or heuristic signals) for\npreference friendliness (e.g., which answer is more aligned\nwith medical guidelines or patient). We use 3,000 for train-\ning and 500 for validation.\n3https://www.icliniq.com/\nFor the domain-specific medical task, the model must\ninternalize preferences that differ from general-domain\nones (e.g., emphasising cautious, guideline-consistent and\npatient-friendly answers), allowing us to test whether the\nentropy-guided cognitive feedback loop transfers to safety\nand factuality-critical settings.\n4.2\nExperimental Setup\nIn both settings, the training size (3k preference pairs) is\nan order of magnitude smaller than typical RLHF pipelines\n(30k-100k+ comparisons), matching our few-shot, domain-\nspecific alignment focus. For each query, GEM’s generative\npipeline produces additional CoT candidates, which we treat\nas elaborated cognitive traces. We set k = 5 in most exper-\niments, generating up to five CoT-augmented responses per\nquery with temperature sampling to ensure diversity, and use\nentropy-guided scoring to construct augmented preference\ndata.\nWe base our experiments on Llama-3-8B-Instruct. This\nmodel has strong general capabilities but still benefits from\nalignment fine-tuning. In all cases, the model is first fine-\ntuned on the initial preference data (supervised learning on\n(prompt, preferred answer) pairs) for a small number of\nepochs to initialize πθ. This corresponds to the SFT stage\nin Figure 1. Then we run our generative preference opti-\nmization(SEGA) as described. We implement the training\nin PyTorch using the HuggingFace Transformers and Deep-\nspeed libraries, enabling mixed precision and efficient gradi-\nent checkpointing due to the model size. Experiments were\nrun on a single node featuring eight NVIDIA A100 80 GB\nSXM4 GPUs. Hyperparameters like learning rate (1e −5)\nand batch size (128) follow standard fine-tuning practices.\nWe tune the number of preference optimization steps based\non validation performance (for general domain, we monitor\naccuracy on a small subset of UltraFeedback; for medical,\non our 500-val set).\n4.3\nBaselines\nWe compare our approach against several baselines to high-\nlight the contributions of each component:\n• Supervised Only (SFT): The model is fine-tuned only\nwith supervised learning on the 3k human preference\npairs, similar to instruction-tuning without preference\noptimization (Ouyang et al. 2022; Ranaldi and Freitas\n2024).\n"}, {"page": 6, "text": "Method\nGSM8K Acc.\nMATH Acc.\nTruthfulQA EM\nMT-Bench Win-rate\nSupervised (SFT)\n40.1\n5.8\n32.4\n35\nReward Model + PPO\n44.7\n7.3\n34.0\n47\nDPO\n50.2\n8.5\n35.6\n52\nGEM (ours)\n55.6\n10.5\n38.2\n68\nTable 3: Down-stream task results. Accuracy (%) for GSM8K / MATH, exact-match (%) for TruthfulQA; MT-Bench reports\nwin-rate (%) against the SFT baseline.\nVariant\nUltraFeedback (%)\nGSM8K (%)\nMed-Expert (%)\nGEM\n– w/o Cognitive Filtering & w/o SEGA\n69.0\n48.3\n70.5\n– w/o final-entropy & w/. fork-entropy & w/. SEGA\n74.2\n50.1\n73.5\n– w/. final-entropy & w/o fork-entropy & w/. SEGA\n73.8\n52.7\n75.0\n– w/. Cognitive Filtering & w/. DPO\n74.5\n53.4\n73.0\n– w/. Cognitive Filtering & w/. SEGA\n77.1\n55.6\n78.2\nTable 4: Results of ablation studies. The results show that CoT augmentation and dual-stage entropy regularization jointly drive\nthe model’s gains in preference accuracy, mathematical reasoning, medical expert agreement.\n• Reward Model + PPO (RLHF): A separate reward\nmodel is trained on the same 3k pairs and the policy is op-\ntimized with PPO, following classical RLHF (Christiano\net al. 2017; Stiennon et al. 2020; Schulman et al. 2017).\nData-efficiency issues and reward–over-optimization ef-\nfects are well-documented (Gao, Schulman, and Hilton\n2022), prompting work on lighter reward models such as\nProto-RM (Zhang et al. 2024).\n• DPO (pairwise): We apply DPO only on the original\nlabeled pairs, mirroring Rafailov et al. (Rafailov et al.\n2023) and subsequent surveys (Zhang, Liu et al. 2024).\n• PRO & IPO: Preference Ranking Optimization extends\nDPO to listwise supervision (Song et al. 2023); Implicit\nPreference Optimization lets the model act as its own\njudge and then applies DPO (Garg et al. 2025).\n4.4\nEvaluation Metrics\nWe evaluate the models along two dimensions: Preference\nModeling Accuracy and Downstream Task Performance.\nFor the former, we use the preference datasets (like Ultra-\nFeedback, RewardBench, and our medical val set) to mea-\nsure how often the model agrees with human preferences.\nSpecifically, we format each comparison as a query with two\ncandidate answers and see if the model’s learned preference\nfunction prefers the same one as the human did. Since our\nmodel doesn’t output a scalar directly, we do this by explic-\nitly asking the model with a prompt to choose between two\n(for generative evaluation). We report the preference predic-\ntion accuracy on these sets, which is a standard metric for\nreward models.\nFor downstream performance, we test our method on\nGSM8K + MATH (grade-school & Olympiad math), Truth-\nfulQA (factual QA) and MT-Bench (open-ended dialogue).\nFor GSM8K/MATH we score plain answer accuracy on the\nfull dev sets, reading the value after a <final-answer>\ntag (T = 0.2, top-p = 0.95). TruthfulQA uses the official\nexact-match metric. MT-Bench adopts the LMSys protocol:\nGPT-4-Turbo judges our reply versus an SFT baseline and\nwe report the win-rate across 80 prompts. We also track the\nlength and complexity of explanations produced, to ensure\nthat our model indeed gives CoT style answers when appro-\npriate.\n4.5\nResults\nOverall Performance: Our generative preference model\nachieves strong results on preference prediction and task\nsuccess, outperforming all baselines in the low-data regime.\nTable 1 summarizes the quantitative outcomes. On Ultra-\nFeedback and RewardBench, our model’s preference accu-\nracy is about 7-10% higher than a reward-model PPO base-\nline and a few points higher than standard DPO. Notably,\nit also closes the gap to larger models. Our 7B model with\ngenerative preference training is within 5% of GPT-4’s per-\nformance on these preference tests, despite GPT-4 having\nbeen trained on vastly more data (we credit the CoT aug-\nmentation and entropy scoring for extracting maximum sig-\nnal from the limited data). In the medical domain (Table 2),\nour model achieves 78% agreement with expert preferences\non the val set, compared to 70% for PPO baseline and 72%\nfor the DPO. This indicates the method is particularly effec-\ntive in specialized domains; we hypothesize that CoT refine-\nment helped the model pick up on domain-specific criteria\n(e.g., caution with uncertainty, thoroughness in advice) that\nweren’t explicit in the small training set.\nOn reasoning tasks GSM8K/MATH (Table 3), the\nmodel’s accuracy improved significantly after our training:\ne.g., on GSM8K, it went from 40% (supervised baseline)\nto 55% accuracy, outperforming DPO (50%). This validates\nthat maximizing an implicit reward via confident reasoning\nleads to more correct answers. The entropy scoring encour-\n"}, {"page": 7, "text": "Figure 2: Comparison of Training Stability: SEGA vs. DPO\naged the model to not jump to conclusions and to system-\natically enumerate steps, which is crucial in math. Qualita-\ntively, we saw fewer instances of the model “hallucinating” a\nquick (wrong) answer; instead it tends to break the problem\ndown and only finalize the answer when it’s sure – a be-\nhavior very much aligned with the RENT objective (though\nwe did not explicitly code RENT, our model seems to have\nlearned a similar trait).\nIn dialogue evaluations (MT-Bench for helpfulness/hon-\nesty), our model’s responses were preferred over the base-\nline responses about 68% of the time according to GPT-\n4 judgments. This is a notable improvement; the baseline\n(supervised only) was often too concise or missed nuance,\nwhereas our model often gave a more detailed, balanced an-\nswer (likely because the CoT and scoring encouraged cov-\nering all bases, and we observed the model sometimes ex-\nplicitly thinking about pros and cons in its response, which\nmade it more comprehensive). Against a model fine-tuned\nwith human feedback (PPO baseline), ours still won 60% of\nthe time, indicating that even without direct human tuning,\nour generative method can compete with traditional RLHF.\nHuman evaluators similarly preferred our model’s outputs\nfor their clarity and reasoning in a majority of cases, partic-\nularly on complex queries.\nAblation Studies: We conducted ablations to understand\nthe impact of each component. As shown in Table 4, without\nthe CoT generation and using only human pairs drops per-\nformance significantly (preference accuracy drops 8%, task\nscores drop similarly). This confirms that the data augmen-\ntation via CoT is key to overcoming the limited data. Even\nthough the generated comparisons are noisy, they provide\nvaluable additional training signals.\nSEGA outperformed DPO when using our full pipeline.\nThe difference was more pronounced in the early training\nphase and on the more complex tasks. For example, on the\nmedical dataset, SEGA reached 78% accuracy whereas pair-\nwise DPO capped at around 70%. We thus believe SEGA’s\nadvantage estimation made training more data-efficient and\nstable (fewer oscillations), as evidenced by smoother valida-\ntion curves as shown in Figure 2.\nDisabling the entropy final-answer reward (i.e. not explic-\nitly rewarding confidence) hurt math performance clearly,\nand the model would sometimes produce a long CoT but\nnot reach the final answer or express uncertainty in the fi-\nnal answer. Conversely, disabling the entropy fork reward\n(only rewarding confidence) made the model too greedy and\nit started to hallucinate answers. This confirms that both as-\npects of the scoring are important for balanced reasoning.\nWe present further results and examples in the appendix.\nIn case study, we show a prompt and two model answers\n(one with reasoning, one without) and how our model cor-\nrectly prefers the one with thorough reasoning, aligning with\nhuman judgment. We also show an example from Truth-\nfulQA where the baseline model gives an over-confident in-\ncorrect answer, while our model, thanks to entropy-based\nself-checking, gives a nuanced and truthful answer.\n5\nConclusion\nWe introduced a generative entropy-guided preference mod-\neling framework named GEM, which based on cognitively\ninspired solution for aligning LLMs under few-shot prefer-\nence data task. From a theoretical standpoint, our approach\nextends traditional preference learning into the domain of\ninverse reinforcement learning (IRL), where the model im-\nplicitly infers a latent reward structure. This reframing al-\nlows the LLM to infer and maximizes an implicit reward\nby extracting and leveraging the multi-dimensional, fine-\ngrained cognitive signals implicit in human preference data,\nwithout explicit human annotations or external supervision.\nCentral to our method is the entropy-guided scoring mecha-\nnism, grounded in information theory, promoting a balance\nbetween exploratory behavior and decisiveness. By strate-\ngically leveraging entropy signals, our framework encour-\nages models to navigate uncertainty effectively, thus en-\nhancing their decision-making robustness. Empirical eval-\nuations across diverse benchmarks validated the theoretical\ninsights, showing significant performance gains over base-\nlines in preference prediction and complex reasoning tasks.\nIn addition to GEM, the SEGA module can also be ap-\nplied to any scenario that requires learning preferences from\nmultiple candidate generations, such as reranking diverse so-\nlutions, weighting and aggregating AI feedback in RLAIF\nsettings, or performing within-group comparisons based on\nuser clicks or weak labels in multimodal generation tasks (Li\net al. 2025; Wang et al. 2025b).\nIn future work, we plan to extend GEM to extract cog-\nnitive signals from more complex modalities. We also\naim to investigate how entropy-guided preference modeling\ncan adaptive with large-scale RLAIF pipelines, potentially\nyielding more stable alignment strategies.\n"}, {"page": 8, "text": "Appendix\nA\nCase-Study Supplement\nThis section supplements two sets of representative cases,\nrespectively demonstrating (1) how the model prioritizes an-\nswers with sufficient chain-of-thought (CoT); (2) in the con-\ntext of TruthfulQA, how the entropy-self-check mechanism\nsuppresses ”confidently wrong” outputs and generates an-\nswers that are more fact-conforming and tonally restrained.\nFor ease of reading, we provide complete prompts, candi-\ndate answers, and model self-evaluation scores, along with\na comparison of human preference results. All cases use the\nsame post-trained Llama-3-8B-Instruct + SEGA model as in\nSection 4 of the main text. Unless otherwise specified, the\ntemperature is set to 1.0 and top-p to 0.95.\nA.1\nModel Prefers Answers Containing\nReasoning\nAs shown in Table 5, Candidate B’s intermediate steps dis-\nplay a high-entropy bifurcation at “Rayleigh scattering” and\n“longer optical path,” yet its final answer exhibits very low\nentropy (high confidence); the scoring function therefore\nassigns a substantial bonus. Although Candidate A is flu-\nent, it lacks explanatory detail and expresses the same final\nconfidence, incurring a penalty for the “sparse reasoning”\ntrait—aligning the model’s judgments with human prefer-\nences.\nA.2\nTruthfulQA: Entropy-Based Self-Checking\nMitigates Over-Confident Errors\nTable 6 shows that the baseline response is terse and over-\nconfident, offering no evidential chain; its entropy score\ntherefore suffers a “double penalty”: zero exploratory en-\ntropy in the middle and an extremely low terminal entropy\nthat is nevertheless factually wrong. Our model, by con-\ntrast, generates exploratory high entropy at the key evidence-\nretracing steps yet delivers a low-entropy final conclusion,\nearning a high score while avoiding hallucination.\nA.3\nDiscussion\nThese two cases illustrate how the preference function\nlearned by our SEGA method under low-resource settings\n(i) rewards structured and thorough reasoning and (ii) lever-\nages entropy signals for self-verification, thereby reducing\nthe risk of “confident but wrong” answers. These behaviors\nare strongly aligned with human preferences and explain the\nsignificant improvements on UltraFeedback and TruthfulQA\nmetrics reported in Section 4.\nB\nSample-Efficiency Analysis\nCurves in Figure 3 are obtained by fine-tuning the open-\nsource llama3-8b-instruct base model under six\nalignment strategies: SFT, PPO (Schulman et al. 2017),\nDPO (Rafailov et al. 2023), PRO (Song et al. 2023),\nIPO (Garg et al. 2025), and our proposed SEGA. All train-\ning was conducted on the Skywork preference dataset men-\ntioned in the original paper, and validation was performed\non the UltraFeedback dataset (Cui et al. 2023).\nFigure 3: Sample efficiency of alignment methods. Test-\nset accuracy (%) as a function of the number of preference\npairs used for training. Our SEGA approach consistently out-\nperforms SFT, PPO, DPO, PRO, and IPO across all data\nregimes, with a particularly large gap in the low-data setting\n(500 pairs).\nKey observations.\n• Low-data regime. With only 500 pairs SEGA reaches\n63.0% accuracy, already exceeding IPO by 4.5 pp and\nPPO by 7.5 pp.\n• Scaling behavior. Gains grow roughly linearly with data:\nat 3000 pairs SEGA attains 75.7%, a +7.1 pp edge over\nIPO and a +17.1 pp edge over the SFT baseline.\n• Area-under-curve (AUC). Integrating accuracy over\nsample size gives 174,675 for SEGA versus 159,025 for\nIPO, confirming superior sample-efficiency.\nInterpretation for SEGA.\nSEGA leverages entropy-\nweighted self-evaluation to filter low-information pairs and\nto reward exploratory reasoning steps. This mechanism ex-\ntracts markedly more learning signal per annotation, ex-\nplaining the steeper slope and the highest AUC among all\ncontenders. The consistent margin over IPO (from +4.5%\nto +7.1%) suggests that uncertainty-aware acceptance trig-\ngers are complementary to iterative preference optimization.\n"}, {"page": 9, "text": "Prompt\nExplain to a 12-year-old why the sky appears blue during the day but red\nat sunset.\nCandidate-A\n(Baseline, no\nrea-\nsoning)\n“The sky is blue because of sunlight. At sunset the light changes color,\nturning the sky red.”\nCandidate-B\n(Ours, with CoT)\n1. Sunlight looks white but actually contains all colors (ROYGBIV).\n2. Air molecules are much smaller than visible-light wavelengths, so\nRayleigh scattering favors shorter (blue) wavelengths.\n3. When the Sun is high, we mostly see this scattered blue light.\n4. Near sunset the sunlight travels a longer atmospheric path; blue/green\nare scattered out, leaving red/orange light.\nFinal answer →Because tiny air molecules scatter blue light much more\nthan red; when the Sun is low the blue is scattered away and the remaining\nred light dominates.\nEntropy-guided\nscore S(a)\nA: 0.31\nB: 0.72\nModel preference\nB ≻A\nHuman agreement\n5 / 5 judges preferred B\nTable 5: Case study showing that the preference model rewards answers with explicit chain-of-thought reasoning.\nTruthfulQA Q\nDoes the measles, mumps, and rubella (MMR) vaccine cause autism?\nBaseline answer\n(no self-check)\n“Yes. Numerous studies have shown that the MMR vaccine is linked to\nautism in children—parents should be cautious.”\nOur answer\n(entropy self-check\n+ CoT)\n1. Identify claim: “MMR–autism link”.\n2. Recall many large-scale epidemiological studies (1999–2019, > 1 M\nchildren) find no causal link.\n3. The original 1998 Wakefield study was retracted for misconduct.\n4. Certainty check: evidence base broad and consistent; residual uncer-\ntainty stems from public misconception. Final answer →No. Large, well-\ndesigned studies show no causal relationship. The misconception arose\nfrom a retracted paper; current scientific consensus rejects the link.\nEntropy profile\nBaseline: uniformly low entropy (over-confident);\nOurs: high entropy at\nevidence retrieval, low at conclusion\nEntropy score S(a)\nBaseline: 0.22\nOurs: 0.67\nModel preference\nOurs ≻Baseline\nTruthfulQA grade\nBaseline: Incorrect & confident;\nOurs: Correct & calibrated\nTable 6: Entropy-based self-checking rejects a confident hallucination and yields a truthful answer.\n"}, {"page": 10, "text": "References\nAchintalwar, S.; Baldini, I.; Bouneffouf, D.; and et al.\n2024. Alignment Studio: Aligning Large Language Mod-\nels to Particular Contextual Regulations. In arXiv preprint\narXiv:2403.09704.\nAgarwal, S.; Zhang, Z.; Yuan, L.; Han, J.; and Peng,\nH. 2025.\nThe Unreasonable Effectiveness of En-\ntropy Minimization in LLM Reasoning.\narXiv preprint\narXiv:2505.15134.\nBai, Y.; Jones, A.; Ndousse, K.; and et al. 2022a. Train-\ning a Helpful and Harmless Assistant with Reinforce-\nment Learning from Human Feedback.\narXiv preprint\narXiv:2204.05862.\nBai, Y.; Kadavath, S.; Kundu, S.; and et al. 2022b. Constitu-\ntional AI: Harmlessness from AI Feedback. arXiv preprint\narXiv:2212.08073.\nBradley, R. A.; and Terry, M. E. 1952. Rank Analysis of\nIncomplete Block Designs: The Method of Paired Compar-\nisons. Biometrika, 39(3–4): 324–345.\nChen, X.; Huang, H.; Gao, Y.; Wang, Y.; Zhao, J.; and Ding,\nK. 2024.\nLearning to Maximize Mutual Information for\nChain-of-Thought Distillation. In Findings of ACL.\nChinta, S. V.; Wang, Z.; Zhang, X.; Viet, T. D.; Kashif, A.;\nSmith, M. A.; and Zhang, W. 2024. Ai-driven healthcare:\nA survey on ensuring fairness and mitigating bias. arXiv\npreprint arXiv:2407.19655.\nChristiano, P. F.; Leike, J.; Brown, T. B.; Martic, M.; Legg,\nS.; and Amodei, D. 2017. Deep Reinforcement Learning\nfrom Human Preferences. In Advances in Neural Informa-\ntion Processing Systems. ArXiv:1706.03741.\nCobbe, K.; Kosaraju, V.; Bavarian, M.; and et al. 2021.\nTraining Verifiers to Solve Math Word Problems.\narXiv\npreprint arXiv:2110.14168.\nCui, G.; Yuan, L.; Ding, N.; and et al. 2023. UltraFeedback:\nBoosting Language Models with Scaled AI Feedback. arXiv\npreprint arXiv:2310.01377.\nFan, J.; and coauthors. 2022. Ranking Inferences Based on\nthe Top Choice of Multiway Comparisons. arXiv preprint\narXiv:2211.11957.\nFarquhar, S.; Kossen, J.; Kuhn, L.; and Gal, Y. 2024. De-\ntecting Hallucinations in Large Language Models Using Se-\nmantic Entropy. Nature, 630: 625–630.\nGao, L.; Schulman, J.; and Hilton, J. 2022.\nScaling\nLaws for Reward Model Overoptimization. arXiv preprint\narXiv:2210.10760.\nGarg, S.; Singh, A.; Singh, S.; and Chopra, P. 2025. IPO:\nYour Language Model Is Secretly a Preference Classifier.\narXiv preprint arXiv:2502.16182.\nGehman, S.; Gururangan, S.; Sap, M.; Choi, Y.; and Smith,\nN. A. 2020. RealToxicityPrompts: Evaluating Neural Toxic\nDegeneration in Language Models. In Findings of EMNLP,\n3356–3369.\nGheshlaghi Azar, M.; and coauthors. 2023. A General The-\noretical Paradigm to Understand Learning from Preferences.\narXiv preprint arXiv:2310.12036.\nGong, Z.; Guan, J.; Wu, W.; and et al. 2025. Latent Prefer-\nence Coding: Aligning Large Language Models via Discrete\nLatent Codes. In ICML.\nGreensmith, E.; Bartlett, P. L.; and Baxter, J. 2004. Variance\nReduction Techniques for Gradient Estimates in Reinforce-\nment Learning. Journal of Machine Learning Research, 5:\n1471–1530.\nGu, J.; Jiang, X.; Shi, Z.; Tan, H.; Zhai, X.; Xu, C.; Li, W.;\nShen, Y.; Ma, S.; Liu, H.; Wang, S.; Zhang, K.; Wang, Y.;\nGao, W.; Ni, L.; and Guo, J. 2025. A Survey on LLM-as-a-\nJudge. arXiv:2411.15594.\nHao, Y.; Li, D.; Wu, X.; and et al. 2025. On-Policy Rein-\nforcement Learning with Optimal Reward Baseline. arXiv\npreprint arXiv:2505.23585.\nHendrycks, D.; Burns, C.; Kadavath, S.; and et al. 2021.\nMeasuring Mathematical Problem Solving With the MATH\nDataset. arXiv preprint arXiv:2103.03874.\nHerbrich, R.; Minka, T.; and Graepel, T. 2007. TrueSkill™:\nA Bayesian Skill Rating System. In NeurIPS.\nHong, J.; Chong, D.; and Manning, C. D. 2021. Learning\nfrom limited labels for long legal dialogue. In Proceedings\nof the Natural Legal Language Processing Workshop 2021,\n190–204.\nHong, J.; Lee, N.; Kim, E.; and et al. 2025. On the Ro-\nbustness of Reward Models for Language Model Alignment.\narXiv preprint arXiv:2505.07271.\nJi, J.; Hong, D.; Zhang, B.; and et al. 2024. PKU-SafeRLHF:\nTowards Multi-Level Safety Alignment for LLMs with Hu-\nman Preference. arXiv preprint arXiv:2406.15513.\nKahneman, D. 2011. Thinking, fast and slow. Farrar, Straus\nand Giroux.\nKeeney, R. L.; and Raiffa, H. 1993. Decisions with multi-\nple objectives: preferences and value trade-offs. Cambridge\nuniversity press.\nKim, D.; Lee, K.; Shin, J.; and Kim, J. 2024.\nAlign-\ning Large Language Models with Self-generated Preference\nData (Selfee). arXiv:2406.04412.\nKim, S.; Bae, S.; Shin, J.; and et al. 2023. Aligning Large\nLanguage Models through Synthetic Feedback. In EMNLP.\nArXiv:2305.13735.\nKoutcheme, C.; Dainese, N.; Sarsa, S.; et al. 2024. Open\nSource Language Models Can Provide Feedback: Evaluat-\ning LLMs’ Ability to Help Students Using GPT-4-As-A-\nJudge. In ITiCSE 2024. ArXiv:2405.05253.\nLambert, N. 2025.\nReinforcement Learning from Human\nFeedback. Self-published. https://rlhfbook.com.\nLambert, N.; Bakhtin, A.; and Smith, N. A. 2024. Reward-\nBench: Evaluating Reward Models for Language Systems.\narXiv preprint arXiv:2403.13787.\nLee, H.; Phatale, S.; Mansoor, H.; and et al. 2023a.\nRLAIF vs. RLHF: Scaling Reinforcement Learning from\nHuman Feedback with AI Feedback.\narXiv preprint\narXiv:2309.00267.\nLee, H.; Phatale, S.; Mansoor, H.; et al. 2023b. RLAIF vs.\nRLHF: Scaling Reinforcement Learning from Human Feed-\nback with AI Feedback. In ICML 2024 Proceedings.\n"}, {"page": 11, "text": "Li, X.; He, Y.; Zu, S.; Li, Z.; Shi, T.; Xie, Y.; and Zhang,\nK. 2025. Multi-Modal Large Language Model with RAG\nStrategies in Soccer Commentary Generation.\nIn 2025\nIEEE/CVF Winter Conference on Applications of Computer\nVision (WACV), 6197–6206. IEEE.\nLiang, Y.; and colleagues. 2024.\nPreference Optimiza-\ntion with Multi-Sample Comparisons.\narXiv preprint\narXiv:2410.12138.\nLin, S.; Hilton, J.; and Evans, O. 2021. TruthfulQA: Measur-\ning How Models Mimic Human Falsehoods. arXiv preprint\narXiv:2109.07958.\nLin, Y.; Seto, S.; ter Hoeve, M.; et al. 2024. On the Limited\nGeneralization Capability of the Implicit Reward Model In-\nduced by Direct Preference Optimization.\narXiv preprint\narXiv:2409.03650.\nLiu, B.-X.; Xu, J.; et al. 2024. Algorithm of Thoughts: En-\nhancing Exploration of Ideas in Large Language Models.\narXiv:2308.10379.\nLiu, C.; Zeng, L.; Xiao, Y.; He, J.; and Yan, R. 2024a.\nSkywork-Reward: Bag of Tricks for Reward Modeling in\nLLMs. arXiv preprint arXiv:2410.18451.\nLiu, T.; Qin, Z.; Wu, J.; Shen, J.; et al. 2024b. LiPO: List-\nwise Preference Optimization through Learning-to-Rank.\narXiv:2402.01878.\nLiu, Z.; Zheng, Y.; Huang, A.; Tang, T.; Jiang, M.; and Pan,\nL. 2023. G-Eval: NLG Evaluation Using GPT-4 with Better\nHuman Alignment. arXiv preprint arXiv:2303.16634.\nMaity, S.; and Saikia, M. J. 2025. Large Language Models in\nHealthcare and Medical Applications: A Review. Bioengi-\nneering, 12(6): 631.\nMnih, V.; Badia, A. P.; Mirza, M.; and et al. 2016. Asyn-\nchronous Methods for Deep Reinforcement Learning.\nIn\nICML. ArXiv:1602.01783.\nMuldrew, W.; Hayes, P.; Zhang, M.; and Barber, D. 2024.\nActive Preference Learning for Large Language Models.\narXiv preprint arXiv:2402.08114.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; et al. 2022.\nTraining Language Models to Follow\nInstructions with Human Feedback. arXiv:2203.02155.\nPaul, K. 2024. Meta releases AI model that can check other\nAI models’ work. Reuters. Oct 18 2024, Technology sec-\ntion.\nPlackett, R. L. 1975. The Analysis of Permutations. Applied\nStatistics, 24: 193–202.\nRafailov, R.; Sharma, A.; Mitchell, E.; et al. 2023. Direct\nPreference Optimization: Your Language Model is Secretly\na Reward Model. Advances in Neural Information Process-\ning Systems.\nRanaldi, L.; and Freitas, A. 2024. Self-Refine Instruction-\nTuning for Aligning Reasoning in Language Models. arXiv\npreprint arXiv:2405.00402.\nRao, H.; Zeng, M.; Zhao, X.; and Miao, C. 2025. A sur-\nvey of artificial intelligence in gait-based neurodegenerative\ndisease diagnosis. Neurocomputing, 626: 129533.\nRozi, E.; and contributors. 2023. vLLM: Fast and Memory-\nEfficient LLM Serving.\nhttps://github.com/vllm-project/\nvllm.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal Policy Optimization Algorithms.\narXiv preprint arXiv:1707.06347.\nShao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang,\nH.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024. Deepseekmath:\nPushing the limits of mathematical reasoning in open lan-\nguage models. arXiv preprint arXiv:2402.03300.\nSong, F.; Yu, B.; Li, M.; and et al. 2023. Preference Rank-\ning Optimization for Human Alignment.\narXiv preprint\narXiv:2306.17492.\nStein, A.; and colleagues. 2025. Measuring Language Model\nUncertainty with Internal Entropy. In International Confer-\nence on Learning Representations.\nStiennon, N.; Ouyang, L.; Wu, J.; and et al. 2020. Learn-\ning to Summarize from Human Feedback.\nIn NeurIPS.\nArXiv:2009.01325.\nStolfo, A.; Wu, B.; Gurnee, W.; and et al. 2024. Confidence\nRegulation Neurons in Language Models.\narXiv preprint\narXiv:2406.16254.\nSun, Z.; Yan, X.; Yu, T.; and Artzi, Y. 2023.\nPrinciple-\nDriven Self-Alignment of Language Models from Scratch\nwith Minimal Human Supervision. arXiv:2305.03047.\nTang, X.; Zhang, W.; Yu, Y.; Turner, K.; Derr, T.; Wang,\nM.; and Ntoutsi, E. 2021. Interpretable visual understand-\ning with cognitive attention network. In International Con-\nference on Artificial Neural Networks, 555–568. Springer.\nVanlioglu, A. 2025. Entropy-Guided Sequence Weighting\nfor Efficient Exploration in RL-Based LLM Fine-Tuning.\narXiv preprint arXiv:2503.22456.\nWan, X.; Sun, R.; Dai, H.; Arık, S. O.; and Pfister, T. 2023.\nBetter Zero-Shot Reasoning with Self-Adaptive Prompting.\nIn Findings of ACL.\nWang, C.; Gan, Y.; Huo, Y.; et al. 2025a. GRAM: A Gener-\native Foundation Reward Model for Reward Generalization.\nIn ICML 2025.\nWang, H.; Huang, F.; et al. 2024.\nSelf-Improving Ef-\nficient Online Alignment of Large Language Models.\narXiv:2406.15567.\nWang, J.; He, Y.; Zhong, Y.; Song, X.; Su, J.; Feng, Y.; He,\nH.; Zhu, W.; Yuan, X.; Lu, K.; et al. 2025b.\nTwin Co-\nAdaptive Dialogue for Progressive Image Generation. arXiv\npreprint arXiv:2504.14868.\nWang, J.; Zhang, Z.; He, Y.; Zhang, Z.; Song, Y.; Shi, T.;\nLi, Y.; Xu, H.; Wu, K.; Yi, X.; et al. 2024. Enhancing Code\nLLMs with Reinforcement Learning in Code Generation: A\nSurvey. arXiv preprint arXiv:2412.20367.\nWang, S.; Yu, L.; Gao, C.; Zheng, C.; Liu, S.; et al. 2025c.\nBeyond the 80/20 Rule: High-Entropy Minority Tokens\nDrive Effective Reinforcement Learning for LLM Reason-\ning. In ACL. ArXiv:2506.01939.\nWang, X.; Wei, J.; Schuurmans, D.; et al. 2022.\nSelf-\nConsistency Improves Chain of Thought Reasoning in Lan-\nguage Models. arXiv:2203.11171.\n"}, {"page": 12, "text": "Wei, J.; Wang, X.; Schuurmans, D.; et al. 2022. Chain-of-\nThought Prompting Elicits Reasoning in Large Language\nModels. arXiv:2201.11903.\nWeng, L. 2018.\nPolicy Gradient Algorithms.\nhttps://\nlilianweng.github.io/posts/2018-04-08-policy-gradient/.\nWilliams, R. J. 1992. Simple Statistical Gradient-Following\nAlgorithms for Connectionist Reinforcement Learning. Ma-\nchine Learning, 8(3–4): 229–256.\nWu, J.; Ouyang, L.; Ziegler, D. M.; and et al. 2021. Recur-\nsively Summarizing Books with Human Feedback. arXiv\npreprint arXiv:2109.10862.\nWu, T.; Yuan, W.; Golovneva, O.; et al. 2024a.\nMeta-\nRewarding\nLanguage\nModels:\nSelf-Improving\nAlign-\nment\nwith\nLLM-as-a-Meta-Judge.\narXiv\npreprint\narXiv:2407.19594.\nWu, Z.; Xu, B.; Cui, R.; Zhan, M.; Zhu, X.; and Feng, L.\n2024b. Rethinking Chain-of-Thought from the Perspective\nof Self-Training. arXiv preprint arXiv:2412.10827.\nXin, Y.; Luo, S.; Zhou, H.; Du, J.; Liu, X.; Fan, Y.;\nLi, Q.; and Du, Y. 2024.\nParameter-efficient fine-tuning\nfor pre-trained vision models: A survey.\narXiv preprint\narXiv:2402.02242.\nYang, X.; Zhao, X.; and Shen, Z. 2025. EHRStruct: A Com-\nprehensive Benchmark Framework for Evaluating Large\nLanguage Models on Structured Electronic Health Record\nTasks. arXiv preprint arXiv:2511.08206.\nYu, T.; Lin, T.-E.; Wu, Y.; and et al. 2025. Diverse AI Feed-\nback for Large Language Model Alignment. Transactions of\nthe Association for Computational Linguistics, 13: 392–407.\nZhang, J.; Wang, X.; Jin, Y.; et al. 2024. Prototypical Re-\nward Network for Data-Efficient RLHF.\nIn ACL 2024.\nArXiv:2406.06606.\nZhang, T.; Liu, J.; et al. 2024.\nA Comprehensive Sur-\nvey of Direct Preference Optimization.\narXiv preprint\narXiv:2410.15595.\nZhao, S.; Dang, J.; and Grover, A. 2024.\nGroup Prefer-\nence Optimization: Few-Shot Alignment of Large Language\nModels. In ICLR. ArXiv:2310.11523.\nZhao, X.; Chen, H.; Xing, Z.; and Miao, C. 2021. Brain-\ninspired search engine assistant based on knowledge graph.\nIEEE Transactions on Neural Networks and Learning Sys-\ntems, 34(8): 4386–4400.\nZhao, X.; Liu, S.; Yang, S.-Y.; and Miao, C. 2025a. Medrag:\nEnhancing retrieval-augmented generation with knowledge\ngraph-elicited reasoning for healthcare copilot. In Proceed-\nings of the ACM on Web Conference 2025, 4442–4457.\nZhao, X.; Liu, S.; Yang, S.-Y.; and Miao, C. 2025b. A smart\nmultimodal healthcare copilot with powerful llm reasoning.\narXiv preprint arXiv:2506.02470.\nZhao, Y.; Bai, H.; and Zhao, X. 2025. GFRIEND: Gener-\native Few-shot Reward Inference through EfficieNt DPO.\narXiv preprint arXiv:2506.08965.\nZheng, H.; Zou, A.; Chen, E.; et al. 2023.\nRLAIF vs.\nRLHF: Scaling Reinforcement Learning from Human Feed-\nback without Humans. arXiv:2309.00267.\nZhou, E.; Zheng, G.; Wang, B.; et al. 2024a. RMB: Com-\nprehensively Benchmarking Reward Models in LLM Align-\nment. arXiv preprint arXiv:2410.09893.\nZhou, X.; et al. 2025a.\nSelf-Consistency of Internal\nReward Models Improves Alignment.\narXiv preprint\narXiv:2502.08922.\nZhou, Y.; He, Y.; Su, Y.; Han, S.; Jang, J.; Bertasius, G.;\nBansal, M.; and Yao, H. 2025b.\nReAgent-V: A Reward-\nDriven Multi-Agent Framework for Video Understanding.\narXiv preprint arXiv:2506.01300.\nZhou, Z.; Zhang, J.; Zhang, J.; He, Y.; Wang, B.; Shi, T.; and\nKhamis, A. 2024b. Human-centric reward optimization for\nreinforcement learning-based automated driving using large\nlanguage models. arXiv preprint arXiv:2405.04135.\nZiegler, D. M.; Stiennon, N.; Wu, J.; and et al. 2019. Fine-\nTuning Language Models from Human Preferences. arXiv\npreprint arXiv:1909.08593.\n"}]}