{"doc_id": "arxiv:2602.09469", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.09469.pdf", "meta": {"doc_id": "arxiv:2602.09469", "source": "arxiv", "arxiv_id": "2602.09469", "title": "NOWJ @BioCreative IX ToxHabits: An Ensemble Deep Learning Approach for Detecting Substance Use and Contextual Information in Clinical Texts", "authors": ["Huu-Huy-Hoang Tran", "Gia-Bao Duong", "Quoc-Viet-Anh Tran", "Thi-Hai-Yen Vuong", "Hoang-Quynh Le"], "published": "2026-02-10T07:04:44Z", "updated": "2026-02-10T07:04:44Z", "summary": "Extracting drug use information from unstructured Electronic Health Records remains a major challenge in clinical Natural Language Processing. While Large Language Models demonstrate advancements, their use in clinical NLP is limited by concerns over trust, control, and efficiency. To address this, we present NOWJ submission to the ToxHabits Shared Task at BioCreative IX. This task targets the detection of toxic substance use and contextual attributes in Spanish clinical texts, a domain-specific, low-resource setting. We propose a multi-output ensemble system tackling both Subtask 1 - ToxNER and Subtask 2 - ToxUse. Our system integrates BETO with a CRF layer for sequence labeling, employs diverse training strategies, and uses sentence filtering to boost precision. Our top run achieved 0.94 F1 and 0.97 precision for Trigger Detection, and 0.91 F1 for Argument Detection.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.09469v1", "url_pdf": "https://arxiv.org/pdf/2602.09469.pdf", "meta_path": "data/raw/arxiv/meta/2602.09469.json", "sha256": "b0716d13b3f4ed5d8c2d05f15aa993494932a6171a1e559be271b2d50fb5ad1b", "status": "ok", "fetched_at": "2026-02-18T02:19:26.558443+00:00"}, "pages": [{"page": 1, "text": "NOWJ @BioCreative IX ToxHabits: An Ensemble Deep\nLearning Approach for Detecting Substance Use and\nContextual Information in Clinical Texts\nHuu-Huy-Hoang Tran1,â€ , Gia-Bao Duong1,â€ , Quoc-Viet-Anh Tran1,â€ , Thi-Hai-Yen Vuong1 and\nHoang-Quynh Le1,*\n1University of Engineering and Technology, Vietnam National University\nAbstract\nExtracting drug use information from unstructured Electronic Health Records (EHRs) remains a major challenge\nin clinical Natural Language Processing (NLP). While Large Language Models (LLMs) demonstrate advancements,\ntheir use in clinical NLP is limited by concerns over trust, control, and efficiency. To address this, we present\nNOWJâ€™s submission to the ToxHabits Shared Task at BioCreative IX (IJCAI 2025). This task targets the detection\nof toxic substance use and contextual attributes in Spanish clinical texts, a domain-specific, low-resource setting.\nWe propose a multi-output ensemble system tackling both Subtask 1 (ToxNER) and Subtask 2 (ToxUse). Our\nsystem integrates BETO with a CRF layer for sequence labeling, employs diverse training strategies, and uses\nsentence filtering to boost precision. Our top run achieved 0.94 F1 and 0.97 precision for Trigger Detection, and\n0.91 F1 for Argument Detection.\nKeywords\nBiological NLP on low-resource language, Language Models, Named Entity Recognition, Spanish\n1. Introduction\nIn healthcare, it is crucial to effectively extract and use important information from unstructured\nElectronic Health Records (EHRs). Specifically, precisely extracting information on toxic substance\nuse, as targeted by the ToxHabits Shared Task, is vital for improving patient management, supporting\nclinical research, and enhancing public health surveillance. To address this challenge, natural language\nprocessing (NLP) techniques can effectively identify, extract, and characterize specific mentions of\nsubstance use and their contextual attributes from these unstructured clinical texts.\nWe present NOWJâ€™s submission to the ToxHabits Shared Task at the BioCreative IX Workshop (IJCAI\n2025) [1], which focuses on detecting toxic substance use and its contextual attributes in Spanish clinical\ntexts â€” a low-resource and domain-specific setting. The ToxHabits dataset [2] consists of 1499 Spanish\nclinical case reports drawn from various open-access medical journals. These documents are annotated\nfor Triggers (Tobacco, Cannabis, Alcohol, or Drug) and contextual Arguments (Type, Method, Amount,\nFrequency, Duration, History). Subtask 1 (ToxNER) focuses on identifying and classifying Trigger spans\nin Spanish clinical notes, while Subtask 2 (ToxUse) requires extracting and typing Argument spans.\nPerformance for both subtasks is evaluated using micro-averaged precision, recall, and F1-scores [3]. A\ndetailed example of how the text was annotated is presented in Appendix A.\nAlthough Large Language Models (LLMs) have achieved remarkable advancements across various NLP\ndomains, their application in critical clinical settings often faces challenges related to trustworthiness\nand overall efficiency. In this report, we introduce a novel multi-output ensemble system designed to\naddress both ToxNER and ToxUse simultaneously and achieve promising results.\nChallenge and Workshop (BC9): Large Language Models for Clinical and Biomedical NLP, International Joint Conference on\nArtificial Intelligence (IJCAI), August 16â€“22, 2025, Montreal, Canada\n*Corresponding author.\nâ€ These authors contributed equally.\n$ 23020073@vnu.edu.vn (H. Tran); 23021475@vnu.edu.vn (G. Duong); 23021471@vnu.edu.vn (Q. Tran);\nyenvth@vnu.edu.vn (T. Vuong); lhquynh@vnu.edu.vn (H. Le)\nÂ© 2025 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\n"}, {"page": 2, "text": "Recent clinical NER efforts include the MultiCardio NER task [4], which evaluated disease (DisTEMIST)\nand drug (DrugTEMIST) extraction in cardiology reports across Spanish, English, and Italian. The\ntop approach combined an ensemble of RoBERTa [5] models with a multi-head CRF, achieving an F1\nof 0.82 on disease NER. The 2022 n2c2/UW shared task [6] focused on social determinants of health\nin English narratives; the winning system framed extraction as sequence-to-sequence with a T5 [7]\nencoderâ€“decoder, reaching an F1 of 0.90.\nTo address the linguistic challenges inherent in Spanish clinical text, several research teams have\neffectively leveraged BETO [8], a BERT [9] model trained on a large Spanish corpus. This allows BETO\nto capture Spanish-specific morphology and syntax more effectively than multilingual models, making\nit well-suited for domain-specific tasks like clinical NER. As a result, BETO consistently outperforms\ngeneral multilingual models in Spanish NLP tasks.\n2. Methodology\nWe frame both subtasks as a single BIO-based [10] token classification problem. First, each document\nis split into sentences and passed through a binary filter that discards those without any potential\ntriggers/arguments. Remaining sentences are tokenized and labeled under the BIO scheme. To combat\noverfitting on our small, domain-specific dataset, we generate multiple resampled training subsets and\ntrain an ensemble of multi-output BERTâ€“CRF models, each jointly predicting Trigger and Argument\ntags. After inference, BIO tags are merged into complete entity spans. Finally, we aggregate model\noutputs via majority voting to enhance robustness and overall F1 performance across both subtasks.\n2.1. System Architecture\n...\nt1.1,t1.2,...,t1.m1\ntn.1,tn.2,...,tn.mn\nClinical Text\nPre-processing\nSegmentation\nSentence Filtering\nTokenization\nPost-processing\nDetokenization\nNormalization\nEnsemble\nTriggers\nArguments\nBERT Embedding\nTrigger Classifier\nFC Layer\nCRF\nArgument Classifier\nFC Layer\nCRF\nFigure 1: Overall architecture of our multi-output BERT-CRF ensemble system.\nThe proposed system is designed as a multi-stage neural network architecture for identifying triggers\nand arguments. The overall architecture of the system, described in Figure 1, comprises three sequential\ncomponents: Sentence pre-processing (segmentation, filtering, tokenization), Multi-output BERT-CRF\nmodeling [10], and Detokenization and Ensemble inference. Each component is described in detail\nbelow.\n2.1.1. Pre-processing with Sentence Segmentation, Sentence Filtering, and Tokenization.\nTo reduce computational burden, input clinical documents are first segmented into sentences. These\nsentences are then served as the input for the subsequent Sentence Filtering stage.\nThese sentences are then processed by a binary BERT-based classifier, which determines whether\neach sentence contains at least one relevant trigger or argument. This model utilizes a BETO backbone\n[8] (the same as our main model) and was independently trained for this specific purpose. Specifically,\nwe fine-tuned the BETO model on a subset of the ToxHabits training data, using sentences containing\ntriggers or arguments as positive examples and sentences without any as negative examples. The model\ntakes a sentence as input and outputs a binary classification indicating its trigger/argument presence.\nIn inference phase, only positively classified sentences are processed in subsequent stages.\n"}, {"page": 3, "text": "The filtered sentences are then tokenized using the BETO model. Each token will later be labeled\nusing a tagging scheme, independently for both tasks. This labeling strategy captures span boundaries\nby assigning B- (Beginning), I- (Inside), and O (Outside) tags to tokens based on whether they are part\nof a trigger or an argument.\n2.1.2. Multi-output Sequence Labeling with BERT-CRF.\nThe core of our architecture is a multi-output sequence labeling model built upon a shared BETO\nencoder and task-specific decoding branches for trigger detection and argument identification. Each\ntask branch applies a linear classification layer followed by a Conditional Random Field (CRF) [10]\ndecoder to generate the final label sequence. The inference process is structured as follows.\n(1) Shared Encoder.\nGiven an input sentence tokenized into subword units (ğ‘¡1, ğ‘¡2, ..., ğ‘¡ğ‘›), we obtain contextual embeddings\nvia the shared BETO encoder:\nH = (â„1, â„2, ..., â„ğ‘›),\nâ„ğ‘–âˆˆRğ‘‘\nwhere ğ‘‘is the hidden size of BETO. These embeddings are passed through a dropout layer during\ntraining, resulting in ËœH.\n(2) Task-specific Emission Layers.\nEach branch projects ËœH into a task-specific label space via a fully connected layer:\nâ€¢ Trigger detection branch:\nE(tr) = Lineartr( ËœH) âˆˆRğ‘›Ã—ğ¶tr\nâ€¢ Argument detection branch:\nE(arg) = Lineararg( ËœH) âˆˆRğ‘›Ã—ğ¶arg\nwhere ğ¶tr and ğ¶arg denote the number of BIO labels for trigger and argument tagging, respectively.\nThe emission vectors E represent per-token label scores (logits).\n(3) CRF Decoding and Inference.\nTo decode the most likely label sequence Ë†y, we apply a CRF layer on top of the emission scores. The\nCRF models the conditional probability of a label sequence y = (ğ‘¦1, . . . , ğ‘¦ğ‘›) given the emission matrix\nE as:\nğ‘ƒ(y | E) =\n1\nğ‘(E) exp\n(ï¸ƒğ‘›\nâˆ‘ï¸\nğ‘–=1\nğ‘ ğ‘–(ğ‘¦ğ‘–) +\nğ‘›âˆ’1\nâˆ‘ï¸\nğ‘–=1\nğ‘‡ğ‘¦ğ‘–,ğ‘¦ğ‘–+1\n)ï¸ƒ\nwhere:\nâ€¢ ğ‘ ğ‘–(ğ‘¦ğ‘–) = Eğ‘–,ğ‘¦ğ‘–is the emission score of assigning label ğ‘¦ğ‘–to token ğ‘–,\nâ€¢ ğ‘‡ğ‘¦ğ‘–,ğ‘¦ğ‘–+1 is the trainable transition score between adjacent labels,\nâ€¢ ğ’´= ğ‘Œğ‘›denotes the set of all possible label sequences of length ğ‘›, with ğ‘Œbeing the label\nvocabulary (e.g., BIO tags),\nâ€¢ ğ‘(E) is the partition function over ğ’´:\nğ‘(E) =\nâˆ‘ï¸\nyâ€²âˆˆğ’´\nexp\n(ï¸ƒğ‘›\nâˆ‘ï¸\nğ‘–=1\nğ‘ ğ‘–(ğ‘¦â€²\nğ‘–) +\nğ‘›âˆ’1\nâˆ‘ï¸\nğ‘–=1\nğ‘‡ğ‘¦â€²\nğ‘–,ğ‘¦â€²\nğ‘–+1\n)ï¸ƒ\nDuring inference, we decode the most probable label sequence Ë†y using the Viterbi algorithm:\nË†y = arg max\nyâˆˆğ’´ğ‘ƒ(y | E)\nThis decoding ensures global optimality under the transition constraints and returns valid BIO-\nstructured outputs for each task.\n"}, {"page": 4, "text": "2.1.3. Post-processing with Detokenization, Normalization, and Ensemble Inference.\nThe predictions at the subword level are reassembled and normalized into word-level spans. To enhance\nrobustness, we aggregate outputs from multiple BERT-CRF models trained on different training strategies.\nThe final output is derived through majority voting, enabling improved precision and reduced variance.\n2.2. Multiple Training Strategies for Ensemble\nFor model optimization, we computed separate loss values for the two tasksâ€”trigger detection and\nargument extractionâ€”using their respective CRF layers. Specifically, the loss for trigger detection (â„’tr)\nand the loss for argument extraction (â„’arg) were both calculated based on the negative log-likelihood\nfrom each CRF. The overall training objective was the simple sum of these two components:\nâ„’= ğ›¼* â„’tr + ğ›½* â„’arg\n(1)\nwhere ğ›¼and ğ›½are the weights, both set to 1 in this work.\nThis joint loss formulation allowed the model to learn both tasks simultaneously, balancing shared\nfeature extraction from the encoder with task-specific predictions from the CRF layers.\nTo improve model generalization and address class imbalance, we applied several data preparation\nand training strategies. First, we utilized two training datasets: the partial training dataset and the full\ntraining dataset. To increase data diversity, we split each training dataset into ğ‘˜equal parts. For each\nsplit, we trained a model on ğ‘˜âˆ’1 parts, leaving out a different part in each iteration. This procedure\nresulted in ğ‘˜distinct models per dataset. For model training, we experimented with three strategies:\nâ€¢ Label-Weighted Loss: Assigning higher weights to underrepresented classes in the loss function\nto mitigate class imbalance during optimization.\nâ€¢ Data Oversampling: Increasing the number of training instances containing triggers or argu-\nments by duplicating such sentences in the training data to balance the class distribution.\nâ€¢ Weighted Random Sampling: Increasing the sampling probability of sentences containing\ntriggers/arguments to ensure that the model is exposed to more informative samples during each\ntraining epoch.\nBy combining these strategies with the resampled datasets, we created multiple training configurations to\nensemble different variants of the multi-output BERT-CRF model. A total of ğ‘›models were selected and\nused for both trigger and argument detection. These word-level outputs were subsequently aggregated\nacross models via majority voting to produce the final predictions for triggers and arguments.\n3. Experiments and Results\n3.1. Dataset Analysis\nThe ToxHabits corpus [2] comprises 1,499 Spanish clinical case reports, split into training and test sets\nas shown in Table 1. Each training document includes a text file and two annotation files, one per\nsubtask, with aligned trigger and argument annotations in .ann format. Each annotation specifies the\nfilename, label, start and end offsets, and span text. There are four trigger types (Tobacco, Cannabis,\nAlcohol, Drug) and six argument types (Type, Method, Amount, Frequency, Duration, History).\nTo analyze dataset characteristics, we performed a detailed count of annotated triggers and arguments.\nThe training set has 1,199 documents (avg. 516 words/doc), and the test set 300 (avg. 498 words/doc),\nshowing consistent lengths. There are 7,592 triggers and 9,528 arguments in training, averaging 6.33\ntriggers and 7.95 arguments per document. This suggests frequent mentions of substance use, with\narguments often exceeding trigger counts per document.\n"}, {"page": 5, "text": "Metric\nTrain\nTest (private)\nNumber of Documents\n1199\n300\nAverage Document Length (words)\n516\n498\nNumber of Triggers\n7592\n-\nNumber of Arguments\n9528\n-\nAverage number of Triggers per Document\n6.33\n-\nAverage number of Arguments per Document\n7.95\n-\nTable 1\nDescriptive statistics of the ToxHabits dataset, including the number of documents, average document length,\nand average number of annotated triggers and arguments per document in the training set.\n3.2. Evaluation Metrics\nFor both ToxNER (Subtask 1) and ToxUse (Subtask 2), our systems were officially evaluated by the\nToxHabits Shared Task Organizer, according to the official evaluation guidelines [3]. The evaluation\ncompared our systemâ€™s generated entities with the expert-annotated ground truth dataset, considering\npredictions correct only if they exactly matched the gold standard in both character span and label. The\nprimary evaluation metrics were micro-averaged precision, recall, and F1-scores.\n3.3. Experimental Settings\nOur core model architecture is BETO [8] in its \"base\" size variant, with a Conditional Random Field (CRF)\nlayer for sequence labeling. All implementations are based on the Hugging Face Transformers library\nwith PyTorch, and we utilized two NVIDIA Tesla T4 GPUs for our experiments. For fine-tuning the\nmain BERT-CRF model, we adhered to specific configurations. Detailed hyperparameters for this model\nare provided in Appendix B for full reproducibility. Separately, the binary BERT-based classifier used for\nsentence filtering was trained independently. Its specific hyperparameters and training configurations\nare detailed in Appendix C.\nTo enhance robustness across different scenarios, we employed two distinct ensemble configurations.\nFor the Full Training Dataset Evaluation, an ensemble of 6 models was trained using Weighted Random\nSampling on data from a 5-fold cross-validation split (5 models on 80% subsets and 1 on the 100% full\ndataset). For the Partial Training Dataset Evaluation, we constructed a larger ensemble of 19 models.\nThese models leveraged diverse data samples and combined three training strategies: Label-Weighted\nLoss (6 models: 5 on 80% resampled, 1 on full partial), Weighted Random Sampling (12 models: 10 on\n80% resampled, 2 on full partial from two runs), and Data Oversampling (2 models, one with specific\nratios 9-3-2-2 for Drug, Alcohol, Tobacco, Cannabis, and another with a uniform 1-1-1-1 distribution).\n3.4. Experimental Results\nWe conducted five official runs with distinct configurations, encompassing: (i) training with either\nthe full or partial dataset, (ii) applying our developed sentence filtering method to remove sentences\nirrelevant to trigger or argument information, and (iii) fine-tuning model parameters.\n3.4.1. Subtask 1: Trigger Detection (ToxNER)\nThe official results for Subtask 1, Trigger Detection (ToxNER), are summarized in Table 3.4.1. As shown\nin the Table, the best-performing run for Trigger Detection was \"BERT Ensemble + Sentence Filtering\n+ Tuning (full train)\", achieving an F1-score of 0.94 and a precision of 0.97. The \"BERT Ensemble +\nSentence Filtering (full train)\" model also achieved an F1-score of 0.94, with a precision of 0.94. Notably,\nthe sentence filtering method improved the F1-score by 0.01 points (from 0.93 to 0.94) when applied to\nthe full training dataset. Models trained on the full dataset consistently outperformed those that only\ntook advantage of the partial dataset.\n"}, {"page": 6, "text": "Model / Method (Ensemble)\nPrecision\nRecall\nF1-score\nBERT Ensemble (full train)\n0.92\n0.95\n0.93\nBERT Ensemble + Sentence Filtering (full train)\n0.94\n0.94\n0.94\nBERT Ensemble (partial train)\n0.83\n0.85\n0.84\nBERT Ensemble + Sentence Filtering (partial train)\n0.83\n0.85\n0.84\nBERT Ensemble + Sentence Filtering + Tuning (full train)\n0.97\n0.92\n0.94\nTable 2\nResults on the test set of Subtask 1 (ToxNER).\n3.4.2. Subtask 2: Argument Detection (ToxUse)\nFor Argument Detection, we adopted the same modeling architecture and configuration design as in\nSubtask 1, as our methodology uses a multi-output approach for both subtasks. The official results for\nSubtask 2, Argument Detection (ToxUse), are summarized in Table 3.4.2.\nModel / Method (Ensemble)\nPrecision\nRecall\nF1-score\nBERT Ensemble (full train)\n0.91\n0.90\n0.91\nBERT Ensemble + Sentence Filtering (full train)\n0.92\n0.88\n0.90\nBERT Ensemble (partial train)\n0.78\n0.75\n0.77\nBERT Ensemble + Sentence Filtering (partial train)\n0.79\n0.75\n0.77\nBERT Ensemble + Sentence Filtering + Tuning (full train)\n0.95\n0.85\n0.90\nTable 3\nResults on the test set of Subtask 2 (ToxUse).\nAs presented in Table 3.4.2, the highest F1-score of 0.91 for Argument Detection was achieved by the\n\"BERT Ensemble (full train)\" model. Notably, the \"BERT Ensemble + Sentence Filtering + Tuning (full\ntrain)\" model recorded the highest precision at 0.95. While the sentence filtering method for Argument\nDetection generally increased precision, it also led to a slight decrease in recall, resulting in a marginal\nF1-score reduction compared to the base full-train model (from 0.91 to 0.90).\n3.5. Discussion\nOur results highlight the effectiveness of our multi-output ensemble approach, which achieved promising\nperformance for both Trigger and Argument Detection. The multi-output architecture specifically\nallowed the model to learn shared representations and dependencies between triggers and arguments.\nThis joint learning, along with our ensemble method, significantly enhanced the systemâ€™s robustness\nand generalization. In our experiments, we chose to set both ğ›¼and ğ›½weights in the joint loss function\nto 1 for simplicity and to ensure equal emphasis on trigger detection and argument extraction. However,\nwhen the frequency of triggers and arguments differs substantially, adjusting these weights could be a\npotential option for further improvements. The sentence filtering method performed efficiently across\nmost runs, consistently increasing precision and boosting F1-scores in most configurations. Currently,\nour system does not utilize Large Language Models (LLMs); however, we consider exploring their\nintegration in the future to advance generalization and contextual understanding further.\n4. Conclusion\nIn summary, this report introduced a novel multi-output ensemble system for simultaneous Trigger\n(ToxNER) and Argument (ToxUse) detection. Our multi-stage methodology combined initial sentence\nfiltering with a multi-output BERT-CRF core and an ensemble inference strategy. Evaluated on the\nofficial ToxHabits Shared Task datasets, our approach demonstrated competitive performance. We\nhope our methods and experiences will prove valuable for future participants and research in clinical\ninformation extraction.\n"}, {"page": 7, "text": "References\n[1] W. Al-Nabki, S. Lima-LÃ³pez, G. VayÃ¡-Abad, , M. Krallinger, Overview of toxhabits at biocreative\nix: corpus, guidelines and evaluation of systems for the detection of toxic habits from text, in:\nBioCreative IX Challenge and Workshop (BC9): Large Language Models for Clinical and Biomedical\nNLP at the International Joint Conference on Artificial Intelligence (IJCAI), 2025.\n[2] S. L. LÃ³pez, W. Alnabki, G. V. Abad, M. Krallinger, ToxHabits-NER: A Gold-Standard annotated\nDataset for Named Entity Recognition in Toxic Habits context, Zenodo, 2025. doi:10.5281/\nzenodo.15425460.\n[3] BioCreative IX Organizing Committee, Track 3 (toxhabits) evaluation guidelines, 2025. URL:\nhttps://temu.bsc.es/toxhabits/evaluation/.\n[4] S. Lima-LÃ³pez, E. FarrÃ©-Maduell, J. RodrÃ­guez-Miret, M. RodrÃ­guez-Ortega, L. Lilli, J. Lenkowicz,\nG. Ceroni, J. Kossoff, A. Shah, A. Nentidis, A. Krithara, G. Katsimpras, G. Paliouras, M. Krallinger,\nOverview of multicardioner task at bioasq 2024 on medical specialty and language adaptation of\nclinical ner systems for spanish, english and italian, in: Conference and Labs of the Evaluation\nForum, 2024. URL: https://ceur-ws.org/Vol-3740/paper-02.pdf.\n[5] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov,\nRoberta: A robustly optimized bert pretraining approach, arXiv preprint arXiv:1907.11692 (2019).\n[6] K. Lybarger, M. Yetisgen, O. Uzuner, The 2022 n2c2/uw shared task on extracting social determi-\nnants of health, Journal of the American Medical Informatics Association 30 (2023) 1367â€“1378.\ndoi:10.1093/jamia/ocad012.\n[7] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, Exploring\nthe limits of transfer learning with a unified text-to-text transformer, Journal of Machine Learning\nResearch 21 (2020) 1â€“67. URL: https://arxiv.org/abs/1910.10683.\n[8] J. CaÃ±ete, G. Chaperon, R. Fuentes, J.-H. Ho, H. Kang, J. PÃ©rez, Spanish pre-trained bert model and\nevaluation data, in: PML4DC at ICLR 2020, 2020.\n[9] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers\nfor language understanding, in: Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), Association for Computational Linguistics, 2019, pp. 4171â€“4186. doi:10.18653/\nv1/N19-1423.\n[10] J. Lafferty, A. McCallum, F. Pereira, Conditional random fields: Probabilistic models for segmenting\nand labeling sequence data, in: Proceedings of the Eighteenth International Conference on Machine\nLearning (ICML), 2001, pp. 282â€“289.\nA. Example of annotated text\nTo illustrate the annotation scheme and the types of entities targeted, consider the example from the\nToxHabits corpus shown in Figure 2. This figure, provided by the ToxHabits Shared Task organizers,\nclarifies how Trigger and Argument spans are marked within the clinical text.\nFor instance, in the Spanish sentences: \"VarÃ³n de 51 aÃ±os con antecedentes de policonsumo de drogas.\nActualmente, cannabis 1-2 g/dÃ­a vÃ­a oral.\" (Translation: \"51-year-old man with past history of multi-drug\nconsumption. Nowadays, (he takes) 1-2 g/day of cannabis orally.\"). Here, \"drogas\" is annotated as a\nTrigger: Drug. Additionally, \"cannabis\" is identified as a Trigger: Cannabis, with \"1-2 g\" serving as an\nArgument: Amount, \"/dÃ­a\" as an Argument: Frequency, and \"vÃ­a oral\" as an Argument: Method.\nThe figure also includes annotations of the StatusTime label; however, this label is not used for\nidentification and evaluation in either of the two subtasks considered in our system.\n"}, {"page": 8, "text": "Figure 2: An example of an annotated clinical text snippet from the ToxHabits corpus (Image courtesy of\nToxHabits Shared Task Organizers [1])\nB. Main model Configurations\nThis section details the training configurations and hyperparameters for the main multi-output BERT-\nCRF model. These parameters, crucial for the modelâ€™s performance and reproducibility, are itemized in\nTable 4. The total training time was approximately 1 hour.\nHyperparameter\nValue\nMax length\n512\nOptimizer\nAdamW\nLearning rate (LR)\n5e-5\nBatch size\n8\nDropout\n0.1\nEpochs\n5\nTable 4\nBERT-CRF fine-tune configurations\nC. Sentence Filtering Model Configurations\nThe training configuration and hyperparameters for the sentence filtering model are detailed in Ta-\nble 5. The model was trained using the CrossEntropy loss function, and the total training time was\napproximately 2.5 hours.\nHyperparameter\nValue\nMax length\n512\nOptimizer\nAdamW\nLearning rate (LR)\n1e-5\nBatch size\n16\nDropout\n0.1\nEpochs\n5\nWeight decay\n0.01\nTable 5\nBERT-based Sentence Filtering fine-tune configurations\n"}]}