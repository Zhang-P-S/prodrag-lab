{"doc_id": "arxiv:2601.01162", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.01162.pdf", "meta": {"doc_id": "arxiv:2601.01162", "source": "arxiv", "arxiv_id": "2601.01162", "title": "Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models", "authors": ["Zihua Yang", "Xin Liao", "Yiqun Zhang", "Yiu-ming Cheung"], "published": "2026-01-03T11:37:46Z", "updated": "2026-01-03T11:37:46Z", "summary": "Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.01162v1", "url_pdf": "https://arxiv.org/pdf/2601.01162.pdf", "meta_path": "data/raw/arxiv/meta/2601.01162.json", "sha256": "f59e1da3e4e8364e706986c18c28311dbbf402d14f7b6e2875410c69e7db77c3", "status": "ok", "fetched_at": "2026-02-18T02:23:24.955816+00:00"}, "pages": [{"page": 1, "text": "Bridging the Semantic Gap for Categorical Data\nClustering via Large Language Models\nZihua Yang1, Xin Liao1, Yiqun Zhang1,2⋆, and Yiu-ming Cheung2∗\n1 School of Computer Science and Technology,\nGuangdong University of Technology, Guangzhou, China\n2 Department of Computer Science,\nHong Kong Baptist University, Hong Kong SAR, China\n3122004153@mail2.gdut.edu.cn, 3123004616@mail2.gdut.edu.cn,\nyqzhang@comp.hkbu.edu.hk, ymc@comp.hkbu.edu.hk\nAbstract. Categorical data are prevalent in domains such as healthcare,\nmarketing, and bioinformatics, where clustering serves as a fundamental\ntool for pattern discovery. A core challenge in categorical data clustering\nlies in measuring similarity among attribute values that lack inherent or-\ndering or distance. Without appropriate similarity measures, values are\noften treated as equidistant, creating a semantic gap that obscures latent\nstructures and degrades clustering quality. Although existing methods in-\nfer value relationships from within-dataset co-occurrence patterns, such\ninference becomes unreliable when samples are limited, leaving the se-\nmantic context of the data underexplored. To bridge this gap, we present\nARISE (Attention-weighted Representation with Integrated Semantic\nEmbeddings), which draws on external semantic knowledge from Large\nLanguage Models (LLMs) to construct semantic-aware representations\nthat complement the metric space of categorical data for accurate clus-\ntering. That is, LLM is adopted to describe attribute values for rep-\nresentation enhancement, and the LLM-enhanced embeddings are com-\nbined with the original data to explore semantically prominent clusters.\nExperiments on eight benchmark datasets demonstrate consistent im-\nprovements over seven representative counterparts, with gains of 19–27%.\nCode is available at https://github.com/develop-yang/ARISE\nKeywords: Categorical Data Clustering · Large Language Models · Semantic\nEmbedding · Feature Fusion\n1\nIntroduction\nCategorical data are ubiquitous in real-world applications such as medical di-\nagnosis, customer segmentation, and biological research [3]. In these domains,\ndiscovering latent patterns through clustering enables critical downstream tasks,\nincluding patient stratification, market analysis, and gene function annotation.\n⋆Corresponding author\narXiv:2601.01162v1  [cs.LG]  3 Jan 2026\n"}, {"page": 2, "text": "2\nZ. Yang et al.\nFig. 1: The semantic gap in categorical clustering. (a) Non-semantic rep-\nresentation treats all values as equidistant (d = 1), producing clusters with no-\nticeable overlap. (b) Semantic-aware representation captures latent proximity:\n“oval” and “round” are highly similar (d = 0.2), and “oval” is closer to “irregular”\n(d = 0.7) than “round” is (d = 1), yielding improved cluster separation.\nAs an unsupervised approach, clustering can reveal hidden structures directly\nfrom raw observations without requiring costly labeled data. However, unlike nu-\nmerical data that arises from metric spaces where distances are naturally defined,\ncategorical data exist prior to any distance metric, which must be constructed\nafterwards [26]. This fundamental difference makes similarity measurement the\ncore challenge in categorical clustering. As illustrated in Figure 1(a), without ap-\npropriate metric design, all values tend to be treated as equidistant, creating a\nsemantic gap that obscures the latent structure and degrades clustering quality.\nTo bridge this gap, existing methods attempt to infer value relationships from\nstatistical information within the dataset. Distance-based methods directly mea-\nsure dissimilarity between objects. Early approaches such as k-modes [15] treat\neach attribute mismatch equally, while more sophisticated techniques [8] exploit\nattribute coupling or multi-metric space fusion to capture value dependencies.\nEmbedding-based approaches take a different route by learning continuous repre-\nsentations that encode value relationships. Despite their differences, these meth-\nods share a common assumption that all necessary semantic relationships can\nbe derived from the dataset itself. However, this assumption fails under limited\nsamples. In specialized domains such as rare disease diagnosis or niche market\nanalysis, datasets often contain only tens to hundreds of samples, making it dif-\nficult to establish reliable value associations. Under such conditions, statistical\n"}, {"page": 3, "text": "Bridging the Semantic Gap in Categorical Clustering\n3\nsignals become weak, and semantically related values become indistinguishable,\nleading to degraded clustering performance.\nTo overcome this limitation, external semantic knowledge is needed to sup-\nplement insufficient statistical signals. LLMs, pretrained on massive corpora,\nencode rich semantic knowledge and can provide external evidence beyond the\ndataset [28]. Recent works such as ClusterLLM [25] and TabLLM [12] have\ndemonstrated the effectiveness of LLMs in text clustering and tabular classifica-\ntion. However, these methods target text data or supervised tasks, and cannot be\ndirectly applied to unsupervised categorical clustering. Adapting LLMs to this\nsetting presents several challenges. Querying LLMs incurs non-trivial computa-\ntional cost, particularly when processing each data instance individually. The\nstochastic nature of LLM generation may produce inconsistent representations,\nwhere identical categorical values receive different embeddings across queries,\nundermining clustering stability. LLM-generated descriptions often contain ver-\nbose or non-discriminative content that dilutes the useful semantic signals and\nmay introduce noise. Moreover, external knowledge must be balanced with data-\nspecific patterns, as over-reliance on LLM outputs may override valid statistical\nsignals learned from the dataset.\nThis paper, therefore, proposes ARISE (Attention-weighted Representation\nwith Integrated Semantic Embeddings), a framework that integrates LLM-derived\nsemantics into categorical clustering. To ensure efficiency and consistency, ARISE\nqueries LLMs at the attribute-value level, reducing computational cost while\nguaranteeing that identical values receive identical representations. To extract\ndiscriminative features from LLM outputs, an attention-weighted encoding mech-\nanism emphasizes informative tokens without learnable parameters. To balance\nexternal knowledge with data-specific patterns, an adaptive fusion module ad-\njusts the contribution of semantic embeddings based on cluster quality. As shown\nin Figure 1(b), leveraging semantic proximity leads to improved cluster separa-\ntion compared to non-semantic approaches. Experiments on eight benchmark\ndatasets with four mainstream LLMs confirm that ARISE consistently outper-\nforms seven state-of-the-art counterparts across all the evaluated datasets. The\nmain contributions are summarized as follows:\n1) To address the small-sample problem in categorical clustering, a framework\nis proposed that queries LLMs at the attribute-value level, integrating LLM-\nderived semantics as external evidence to supplement insufficient statistical\nsignals.\n2) An attention-weighted encoding mechanism is proposed to emphasize key to-\nkens in the generated descriptions, producing information compact semantic\nembeddings that are more discriminative for categorical data clustering.\n3) This work provides the first empirical validation that LLMs can sufficiently\nenhance traditional categorical clustering. Experiments across eight bench-\nmarks demonstrate consistent improvements over seven representative coun-\nterparts, with gains of 19–27% on all the evaluated datasets, confirming the\nvalue of external semantic knowledge for categorical clustering.\n"}, {"page": 4, "text": "4\nZ. Yang et al.\n2\nRelated Work\n2.1\nCategorical Data Representation and Clustering\nThe discrete nature of categorical attributes, characterized by the absence of in-\nherent order or distance, necessitates the explicit construction of similarity mea-\nsures. Statistical inference is the mainstream approach in this domain. While\nearly approaches relied on matching dissimilarities [15], distance-based methods\nhave evolved toward information-theoretic metrics. Recent works have intro-\nduced graph-based perspectives [24] and learnable intra-attribute weighting [23]\nto adjust feature importance, while others explore multi-metric space fusion [8]\nfor robust clustering. Parallel to metric learning, representation learning aims to\nmap discrete symbols into continuous vector spaces. For heterogeneous or mixed-\ntype data, Het2Hom [22] and QGRL [3] project attributes into homogeneous\nconcept spaces or quaternion graphs, while adaptive partition strategies [29]\naddress clustering through hierarchical merging mechanisms.\nDespite these methodological advancements, a fundamental limitation per-\nsists. As noted in a recent survey [5], measuring similarity for categorical data\nremains challenging due to the lack of well-established distance metrics. When\nsamples are limited, co-occurrence signals become sparse, making value relation-\nships difficult to infer and leading to degraded representations.\n2.2\nLarge Language Models for Clustering\nRecent advances in LLMs have demonstrated remarkable capabilities in var-\nious data understanding tasks [9]. Pre-trained on massive open-domain cor-\npora, LLMs encode extensive world knowledge that can potentially overcome\nthe closed-world constraint. In text clustering, ClusterLLM [25] and recent few-\nshot approaches [20] utilize LLMs to generate triplet constraints or refine cluster\nboundaries, demonstrating that external knowledge can significantly enhance\nstructural partitioning. In the context of tabular data, the adaptation of LLMs\nhas focused primarily on supervised prediction and generation. TabLLM [12] and\nGReaT [1] employ a serialization-based paradigm, converting tabular rows into\nnatural language sentences (e.g., “The color is red...”) to leverage the reasoning\ncapabilities of LLMs. Similarly, pre-training frameworks like TaBERT [21] and\nTAPAS [13] learn joint representations of textual and tabular data for semantic\nparsing tasks requiring deep alignment between schema and context.\nHowever, text clustering methods are designed for natural language data,\nwhile tabular methods primarily target supervised tasks. Adapting these ap-\nproaches to unsupervised categorical clustering faces additional challenges. Row-\nlevel serialization scales linearly with dataset size (O(N)), making iterative clus-\ntering prohibitively expensive. Furthermore, standard pooling strategies (e.g.,\nmean pooling [19] or CLS tokens [4]) may obscure discriminative keywords within\ngenerated descriptions. To date, no framework effectively integrates LLM seman-\ntics into categorical clustering.\n"}, {"page": 5, "text": "Bridging the Semantic Gap in Categorical Clustering\n5\nTable 1: Summary of key notations.\nSymbol\nDescription\nX = {x1, . . . , xN}\nCategorical dataset with N objects\nxi = [xi,1, . . . , xi,M]\nObject with M attribute values\nA = {A1, . . . , AM}\nSet of categorical attributes\nVj; V = S\nj Vj\nValue domain of Aj; all unique values\nM; E\nLLM; pre-trained Transformer encoder\nTv; ev ∈Rd\nDescription; embedding for value v\nEsem ∈RN×Md\nSemantic representation matrix\nEanc ∈RN×ds\nIdentity-anchoring matrix (ds = |V|)\nα ∈[0, 1]; Z ∈RN×D Fusion weight; fused representation (D = ds + Md)\nK; Y\nNumber of clusters; cluster assignment\n3\nProposed Method\nGiven a categorical dataset X = {x1, . . . , xN} where each object is described\nby M attributes taking values from V = SM\nj=1 Vj, the clustering problem can be\nformulated as learning a mapping Φ : X →RD such that distances in the embed-\nding space reflect semantic similarity. However, categorical values lack inherent\ndistance metrics, creating a gap between raw symbols and meaningful represen-\ntations. Bridging this gap requires addressing three interdependent challenges,\nnamely extracting semantic knowledge invisible to co-occurrence statistics, en-\ncoding variable-length descriptions into discriminative vectors, and preventing\ndistinct values from collapsing in the embedding space. These challenges form\na pipeline where each stage depends on its predecessor, as semantic extraction\nprovides input for encoding, and encoding quality determines whether fusion can\nbalance semantics against categorical identity.\nTherefore, ARISE addresses these challenges by leveraging LLMs as external\nknowledge sources. As illustrated in Figure 2, the framework comprises three\ncomponents, namely semantic representation enrichment that generates descrip-\ntions via value-level LLM queries, attention-weighted encoding that emphasizes\ninformative tokens, and adaptive fusion that anchors semantic representations\nwith identity-preserving features.\n3.1\nSemantic Representation Enrichment\nCategorical values often contain latent relationships (e.g., ordinality in “low” vs.\n“high”) invisible to statistical co-occurrence. LLMs are leveraged to capture these\nrelationships by providing external semantic knowledge.\nDistinct from continuous domains where every instance is unique, categorical\nattributes exhibit high repetition. Accordingly, a value-level querying strategy\nis adopted, processing the unique vocabulary set V rather than the full dataset.\nThis approach guarantees consistency, as identical values always receive identical\ndescriptions, while significantly reducing computational cost.\n"}, {"page": 6, "text": "6\nZ. Yang et al.\nFig. 2: Overview of ARISE. The framework integrates a semantic view (top)\nand an identity view (bottom). The semantic view enriches representations via\nstructured prompting with an LLM followed by attention-weighted encoding.\nThe identity view preserves categorical distinctions via identity encoding. Both\nviews are fused through adaptive feature fusion, where the weight α∗is selected\nbased on cluster quality, to produce discriminative representations for partitional\nclustering.\nTo generate these descriptions, a structured prompt P is formulated to orga-\nnize the description into four aspects, namely definition, indicators, context, and\ncontrast. The description for a value v belonging to attribute Aj with domain\nVj is formally obtained as:\nTv = M(P(v, Aj, Vj)),\n(1)\nyielding the complete set T = {Tv : v ∈V} used for downstream encoding.\nProposition 1. Computational Amortization. Let Cquery be the unit cost\nof LLM querying. By operating on V, the total extraction cost is O(|V| · Cquery).\nInstance-level processing requires N × M queries (one per attribute-value pair),\nwhereas value-level processing requires only |V| queries. The reduction ratio is\nthus ρ = 1 −\n|V|\nN×M . For standard tabular benchmarks where N ≫|V|, we have\nρ →1 (empirically approaches unity), amortizing the high inference cost of LLMs\nto a negligible level.\nRemark 1. De-noising via Structure. Unconstrained generation often ob-\nscures discriminative keywords within verbose or irrelevant content. The struc-\ntured format introduces a strong inductive bias, guiding the LLM to clarify the\nsemantic meaning of each value and providing informative content for encoding.\n"}, {"page": 7, "text": "Bridging the Semantic Gap in Categorical Clustering\n7\n3.2\nAttention-Weighted Encoding\nGiven the description set T , the next challenge is converting variable-length text\ninto fixed-dimensional vectors for clustering. As discussed in Section 2.2, CLS\ntoken embeddings [4] aggregate sequence-level semantics into a single represen-\ntation, while mean pooling [19] weights all tokens uniformly. Both approaches\nmay obscure discriminative keywords within LLM-generated descriptions. An\nadaptive pooling strategy is therefore adopted to weight tokens according to\ntheir activation levels.\nGiven description Tv, a pre-trained Transformer encoder E produces token\nrepresentations [h1, . . . , hL] = E(Tv) ∈RL×d, where L is sequence length and d\nis hidden dimension. Token importance is measured by mean activation:\nst = 1\nd\nd\nX\nk=1\nht,k,\n(2)\nand the embedding is computed as an attention-weighted sum:\nat =\nexp(st)\nPL\nl=1 exp(sl)\n,\nev =\nL\nX\nt=1\nat · ht ∈Rd.\n(3)\nRemark 2. Adaptive Weighting without Learnable Parameters. Recent\nwork on Transformer interpretability [16, 18] demonstrates that token contribu-\ntions depend on representation properties beyond attention weights alone. The\nmean activation score st captures average signal strength across hidden dimen-\nsions, serving as a parameter-free importance estimate. When scores are uniform,\nthe mechanism reduces to mean pooling (at = 1/L); when one score dominates,\nthe weighting concentrates accordingly. Unlike CLS-based aggregation that com-\npresses information into a single designated position, this formulation explicitly\nweights all tokens based on their activations, thereby preserving discriminative\nkeywords.\nFor object xi = [xi,1, . . . , xi,M], attribute embeddings are concatenated:\nEsem\ni\n= exi,1 ⊕· · · ⊕exi,M ∈RMd,\n(4)\nwhere each original attribute Aj occupies dimensions [(j −1)d + 1, jd], ensuring\nnon-overlapping blocks that prevent cross-attribute interference. Stacking all\nobjects yields the semantic matrix:\nEsem = [Esem\n1\n; . . . ; Esem\nN\n] ∈RN×Md.\n(5)\n3.3\nAdaptive Feature Fusion\nThe semantic matrix Esem captures conceptual relationships but risks collapsing\ndistinct values when LLMs overgeneralize. To mitigate this, an identity view is\nintroduced as a regularizer to preserve categorical distinctions.\n"}, {"page": 8, "text": "8\nZ. Yang et al.\nAlgorithm 1 ARISE: Attention-weighted Representation with Integrated Se-\nmantic Embeddings\nInput: Dataset X, attributes {Aj}M\nj=1, clusters K, LLM M, encoder E\nOutput: Cluster assignments Y\n1:\nSemantic Representation Enrichment\n2: for j = 1 to M do\n3:\nfor each v ∈Vj do\n4:\nTv ←M(P(v, Aj, Vj))\n▷Eq. (1)\n5:\nend for\n6: end for\n7:\nAttention-Weighted Encoding\n8: for each v ∈V do\n9:\nCompute st, at, ev from E(Tv)\n▷Eqs. (2)–(3)\n10: end for\n11: for i = 1 to N do\n12:\nEsem\ni\n←exi,1 ⊕· · · ⊕exi,M ;\nEanc\ni\n←1xi,1 ⊕· · · ⊕1xi,M\n▷Eqs. (4), (6)\n13: end for\n14:\nAdaptive Feature Fusion\n15: ˆEanc ←Normalize(Eanc);\nˆEsem ←Normalize(Esem);\nα∗←0;\nS∗←−1\n16: for each α ∈G do\n17:\nZα ←(1 −α) ˆEanc ⊕α ˆEsem;\nYα ←k-Means(Zα, K)\n▷Eq. (7)\n18:\nif S(Zα, Yα) > S∗then\n19:\nα∗←α;\nS∗←S(Zα, Yα)\n▷Eq. (8)\n20:\nend if\n21: end for\n22: return Y ←k-Means(Zα∗, K)\nLearnable embeddings and hash-based methods are common choices for iden-\ntity encoding. Since learnable parameters risk overfitting on small-scale categor-\nical data, one-hot encoding is adopted as a parameter-free alternative:\nEanc\ni\n= 1xi,1 ⊕· · · ⊕1xi,M ∈Rds,\nds = |V|,\n(6)\nwhere 1v ∈R|V| denotes the one-hot vector for value v. The orthogonal nature\nof one-hot encoding ensures that categorically distinct values remain linearly\nseparable regardless of semantic similarity.\nAfter column-wise z-score normalization of Eanc and Esem, the fused repre-\nsentation is\nZα = (1 −α) · ˆEanc ⊕α · ˆEsem ∈RN×D,\n(7)\nwhere D = ds + Md and α ∈[0, 1]. The fusion weight is selected by maximizing\nthe Silhouette Score over a candidate set G:\nα∗= arg max\nα∈G S(Zα, Yα),\nS = 1\nN\nN\nX\ni=1\nbi −ai\nmax(ai, bi),\n(8)\nwhere ai and bi denote mean intra-cluster and nearest-cluster distances, respec-\ntively. The learned representations are then partitioned into K clusters via k-\n"}, {"page": 9, "text": "Bridging the Semantic Gap in Categorical Clustering\n9\nMeans, minimizing:\nL =\nK\nX\nk=1\nX\nxi∈Ck\n∥Φ(xi) −µk∥2,\ns.t.\nµk =\n1\n|Ck|\nX\nxi∈Ck\nΦ(xi),\n(9)\nwhere Φ(xi) = (Zα∗)i denotes the i-th row of Zα∗. Algorithm 1 summarizes the\ncomplete procedure.\nTheorem 1. Complexity. The time complexity of ARISE is O(|V| · Cllm +\nNMd + |G| · NKD), where Cllm denotes per-query LLM cost.\nProof. The complexity is dominated by three stages. In semantic representa-\ntion enrichment, the LLM is queried once for each unique value in V, yielding\nO(|V|·Cllm). In attention-weighted encoding, each of the N objects requires con-\ncatenating M pre-computed d-dimensional embeddings, contributing O(NMd).\nIn adaptive feature fusion, the Silhouette Score is evaluated for each candidate\nα ∈G, where each evaluation involves k-Means clustering on N objects with D-\ndimensional representations into K clusters, resulting in O(|G|·NKD). Summing\nthese terms yields the stated bound.\nRemark 3. Concatenation versus Summation. Concatenation preserves full\nrepresentational capacity (D dimensions), whereas summation requires dimen-\nsion alignment and loses view-specific information. The learned α∗allows direct\ninterpretation. Values near 0 indicate that categorical distinctions dominate,\nwhile values near 1 suggest that semantic features are more discriminative.\n4\nExperiments\nThis section presents empirical validation of ARISE. Clustering performance is\nfirst compared against seven counterparts across four LLM backends. Compo-\nnent contributions are then analyzed through ablation experiments. Scalability\nis examined with respect to instance count, attribute count, and vocabulary size.\nFinally, visualization illustrates the structure of learned representations.\n4.1\nExperimental Settings\nEight benchmark categorical datasets from the UCI Machine Learning\nRepository3 are employed [6], with statistics in Table 2. The selection spans\nscales from 101 to 8,124 instances across diverse domains. Five datasets con-\ntain fewer than 500 instances, reflecting the reality that categorical data often\noriginates from specialized domains with limited sample availability. This char-\nacteristic poses fundamental challenges for statistical co-occurrence methods,\nproviding an ideal testbed for evaluating semantic enrichment.\n3 https://archive.ics.uci.edu/\n"}, {"page": 10, "text": "10\nZ. Yang et al.\nTable 2: Statistics of the evaluated datasets. N: instances; M: attributes;\nK: classes; |V|: unique values; ¯v, vmax, vmin: average, maximum, and minimum\nattribute cardinality.\nNo.\nDataset\nAbbr.\nN\nM\nK\n|V|\n¯v\nvmax\nvmin\n1\nZoo\nZO\n101\n16\n7\n36\n2.25\n6\n2\n2\nLymphography\nLY\n148\n18\n4\n59\n3.28\n8\n2\n3\nBreast Cancer\nBC\n286\n9\n2\n51\n5.67\n13\n2\n4\nSoybean\nSB\n307\n35\n19\n133\n3.80\n7\n2\n5\nDermatology\nDE\n366\n34\n6\n133\n3.91\n4\n2\n6\nSolar Flare\nSF\n1,066\n10\n6\n31\n3.10\n6\n2\n7\nCar Evaluation\nCA\n1,728\n6\n4\n21\n3.50\n4\n3\n8\nMushroom\nMU\n8,124\n22\n2\n126\n5.73\n12\n2\nSeven representative counterparts are compared. Classical symbolic ap-\nproaches include OHK (One-Hot k-Means) and KMo (k-Modes) [15]. Five state-\nof-the-art methods represent diverse paradigms. COForest [30] learns order-\nconstrained structures via minimum spanning trees. MCDC [2] employs multi-\ngranularity competitive learning with attribute weighting. SigDT [14] recursively\npartitions data via significance testing. DiSC [31] learns cluster-specific distance\nmatrices from conditional entropy. OCL [27] optimizes value orderings for ordinal\ndistance computation.\nThree standard clustering metrics are adopted. Adjusted Rand Index\n(ARI) [10] measures chance-adjusted pairwise agreement. Normalized Mutual\nInformation (NMI) [7] quantifies shared information between clusters and labels.\nClustering Accuracy (ACC) [11] denotes the best-match classification accuracy\nunder optimal permutation.\nImplementation details are summarized as follows. All experiments are\nrepeated 10 times with different random initializations. Statistical significance is\nassessed via the Wilcoxon signed-rank test (p < 0.05). Evaluated LLMs include\nGPT-5.1, Claude Opus 4.5, DeepSeek V3.2, and Gemini 3 Pro. The text encoder\nis all-mpnet-base-v2. The fusion weight α is selected via silhouette score.\n4.2\nClustering Performance Evaluation\nTable 3 summarizes the quantitative results. ARISE consistently achieves strong\nperformance, securing the best or second-best outcomes in 21 of 24 metric com-\nparisons and establishing a statistically significant lead over the strongest coun-\nterpart, SigDT.\nPerformance gains are particularly pronounced on small-scale datasets (ZO,\nLY, BC). On BC, statistical methods such as OHK and MCDC exhibit near-zero\nARI due to sparse co-occurrence signals, whereas ARISE leverages external se-\nmantic knowledge to compensate for insufficient statistical evidence. Results on\nCA and SF illustrate the boundary conditions of semantic enrichment. On CA,\nARISE outperforms all counterparts in ARI, as LLMs effectively capture the\n"}, {"page": 11, "text": "Bridging the Semantic Gap in Categorical Clustering\n11\nTable 3: Clustering results on eight categorical benchmarks. Data rep-\nresent mean ± standard deviation over ten independent trials. AR denotes the\naverage rank among the eight evaluated methods, while † indicates statistical sig-\nnificance (p < 0.05) relative to ARISE based on the Wilcoxon signed-rank test.\nThe ∆columns quantify performance deviations of alternative LLM backends\n(Cla.: Claude; DS: DeepSeek; Gem.: Gemini) against the GPT anchor. Green\nand Blue shadings highlight the best and second-best outcomes, respectively.\nMetric Data\nOHK\nKMo\nCOForest\nMCDC\nSigDT\nDiSC\nOCL\nARISE (Ours)\n–\n[DMKD’98]\n[ECAI’24]\n[ICDCS’24]\n[Inf.Sci.’25]\n[AAAI’26]\n[SIGMOD’26]\nGPT\n∆Cla. ∆DS ∆Gem.\nARI\nZO\n0.596±0.12\n0.588±0.09 0.660±0.08 0.602±0.00\n0.592±0.00\n0.669±0.13\n0.653±0.13\n0.794±0.08\n-0.04\n+0.01\n-0.00\nLY\n0.115±0.02\n0.028±0.04 0.153±0.03 -0.010±0.00 0.161±0.00\n0.121±0.07\n0.131±0.05\n0.204±0.03 +0.03 +0.02\n+0.02\nBC\n-0.003±0.00 -0.001±0.00 0.029±0.06 -0.008±0.00 0.157±0.00\n0.004±0.03\n0.012±0.05\n0.169±0.00\n0.00\n-0.01\n0.00\nSB\n0.347±0.05\n0.322±0.04 0.408±0.02 0.340±0.00\n0.425±0.00\n0.159±0.16\n0.387±0.04\n0.425±0.03 +0.03 +0.01\n+0.03\nDE\n0.487±0.21\n0.397±0.02 0.685±0.11 0.641±0.00 0.818±0.00 0.612±0.31\n0.522±0.10\n0.752±0.07\n-0.03\n-0.01\n-0.03\nSF\n0.019±0.01\n0.017±0.01 0.009±0.03 0.028±0.00 0.054±0.00 -0.043±0.05\n0.044±0.02\n0.044±0.02\n-0.00\n-0.01\n-0.01\nCA\n0.033±0.05\n0.025±0.03 0.058±0.04 0.001±0.00 -0.036±0.00 0.029±0.06\n0.065±0.05\n0.085±0.08 +0.13 +0.12\n-0.01\nMU\n0.245±0.05\n0.264±0.07 0.376±0.22 0.073±0.00\n0.339±0.00\n0.351±0.25\n0.474±0.20\n0.596±0.02\n-0.05\n-0.08\n-0.03\nAR\n6.5†\n7.4†\n4.6†\n6.9†\n3.4†\n5.9†\n4.1†\n1.3\n–\n–\n–\nNMI\nZO\n0.785±0.04\n0.763±0.03 0.807±0.04 0.803±0.00\n0.712±0.00\n0.794±0.06\n0.799±0.04\n0.859±0.02\n-0.01\n-0.00\n-0.01\nLY\n0.161±0.03\n0.064±0.03 0.181±0.03 0.108±0.00\n0.231±0.00\n0.174±0.06\n0.170±0.04\n0.245±0.03 +0.03 +0.03\n+0.01\nBC\n0.004±0.00\n0.002±0.00 0.015±0.03 0.001±0.00\n0.070±0.00\n0.012±0.02\n0.008±0.02\n0.079±0.00\n0.00\n-0.00\n0.00\nSB\n0.677±0.04\n0.669±0.03 0.707±0.02 0.634±0.00\n0.714±0.00\n0.329±0.33\n0.694±0.03\n0.726±0.02 +0.01 +0.00\n+0.01\nDE\n0.646±0.11\n0.634±0.01 0.833±0.05 0.792±0.00\n0.857±0.00\n0.662±0.33\n0.712±0.05\n0.867±0.02\n-0.00\n+0.01\n+0.00\nSF\n0.048±0.01\n0.043±0.01 0.046±0.01 0.068±0.00 0.058±0.00\n0.033±0.01\n0.051±0.00\n0.050±0.01\n-0.00\n-0.00\n-0.01\nCA\n0.046±0.05\n0.047±0.02 0.107±0.04 0.003±0.00\n0.034±0.00\n0.052±0.06\n0.126±0.10\n0.153±0.10 +0.16 +0.14\n-0.01\nMU\n0.230±0.03\n0.251±0.09 0.317±0.16 0.059±0.00\n0.436±0.00\n0.339±0.20\n0.408±0.18\n0.532±0.03\n-0.04\n-0.05\n-0.01\nAR\n6.1†\n7.1†\n4.4†\n6.1†\n3.5†\n6.0†\n4.4†\n1.4\n–\n–\n–\nACC\nZO\n0.671±0.11\n0.673±0.07 0.695±0.07 0.673±0.00\n0.723±0.00\n0.710±0.09\n0.692±0.10\n0.835±0.05\n-0.03\n+0.01\n+0.01\nLY\n0.476±0.02\n0.392±0.04 0.546±0.04 0.446±0.00 0.588±0.00 0.521±0.05\n0.484±0.06\n0.540±0.02\n+0.02 +0.03\n+0.02\nBC\n0.516±0.00\n0.533±0.02 0.552±0.09 0.531±0.00 0.729±0.00 0.586±0.08\n0.529±0.07\n0.729±0.00\n0.00\n-0.00\n0.00\nSB\n0.549±0.03\n0.530±0.00 0.595±0.04 0.564±0.00\n0.560±0.00\n0.323±0.17\n0.585±0.04\n0.656±0.03 +0.02 +0.00\n+0.02\nDE\n0.634±0.16\n0.549±0.07 0.739±0.09 0.705±0.00 0.833±0.00 0.702±0.20\n0.655±0.09\n0.785±0.07\n-0.02\n-0.00\n-0.02\nSF\n0.358±0.00\n0.360±0.03 0.373±0.02 0.364±0.00\n0.356±0.00\n0.452±0.11\n0.415±0.07\n0.492±0.09 +0.02\n-0.02\n-0.01\nCA\n0.385±0.06\n0.357±0.04 0.402±0.05 0.270±0.00\n0.575±0.00 0.602±0.11\n0.396±0.06\n0.381±0.07\n+0.09 +0.08\n+0.01\nMU\n0.501±0.05\n0.475±0.00 0.787±0.11 0.635±0.00\n0.520±0.00\n0.773±0.11\n0.830±0.10\n0.886±0.01\n-0.02\n-0.03\n-0.01\nAR\n6.6†\n7.1†\n4.0†\n6.0†\n3.0\n4.3†\n4.9†\n1.6\n–\n–\n–\nordinal relationships among attribute values. On SF, however, SigDT slightly\nsurpasses ARISE because domain-specific astronomical identifiers lack general\nsemantic associations. This observation suggests that information-theoretic op-\ntimization remains competitive in specialized domains with minimal semantic\nambiguity.\nAnalysis of the ∆columns reveals that different LLM backends exhibit vary-\ning strengths across datasets. Claude achieves the largest improvement on CA,\nwhile the open-weight DeepSeek matches or exceeds proprietary models on LY.\nDespite these variations, overall performance remains stable across all four back-\nends. Such stability arises from both the rich pretraining corpora underlying\nmodern LLMs, which provide reliable semantic descriptions for common cate-\ngorical values, and the proposed attention-weighted encoding and adaptive fu-\nsion mechanisms, which filter noisy or irrelevant content and reduce sensitivity\nto backend-specific generation patterns.\n"}, {"page": 12, "text": "12\nZ. Yang et al.\nTable 4: Ablation study of ARISE components (ARI metric). Light blue : im-\nprovement over baseline; Deep blue : best performance.\nDataset\nw/o LLM\nw/o Attn\nARISE (Ours)\nZO\n0.5961±0.12\n0.7335±0.05\n0.7521±0.04\nLY\n0.1150±0.02\n0.2323±0.03\n0.2352±0.04\nSB\n0.3471±0.05\n0.4401±0.02\n0.4539±0.02\nBC\n-0.0029±0.00\n0.1692±0.00\n0.1692±0.00\nSF\n0.0187±0.01\n0.0511±0.02\n0.0417±0.03\nCA\n0.0332±0.05\n0.1777±0.05\n0.2144±0.03\nMU\n0.2451±0.05\n0.5650±0.12\n0.5956±0.02\nDE\n0.4871±0.21\n0.7244±0.04\n0.7519±0.07\nAvg.\n0.2174\n0.3867\n0.4017\n1k 2k\n4k\n6k\n8k\n(a) Instances N\n10−2\n10−1\n100\n101\n102\n103\nRunning Time (s, log scale)\n4\n8\n12\n16\n20\n(b) Attributes M\n10−2\n10−1\n100\n101\n102\n103\n20\n40\n60\n80\n100 120\n(c) Unique Values |V|\n10−2\n10−1\n100\n101\n102\n103\nOHK\nKMo\nDiSC\nOCL\nSigDT\nMCDC\nCOForest\nARISE (Ours)\nFig. 3: Runtime analysis. Impact of (a) instance count N, (b) attribute count\nM, and (c) unique value count |V| on execution time (log scale). The runtime in\n(c) includes offline description generation.\n4.3\nAblation Study\nTo isolate component contributions, three variants are evaluated. The variant\nw/o LLM removes the semantic view and relies solely on categorical identities.\nThe variant w/o Attn replaces attention-weighted encoding with CLS token\npooling, which aggregates sequence information into a single designated posi-\ntion and thus represents a different aggregation paradigm. ARISE denotes the\ncomplete framework.\nAs shown in Table 4, incorporating LLM-derived semantics (w/o Attn) im-\nproves performance across all datasets, with average ARI increasing from 0.217\nto 0.387. This improvement demonstrates that external semantic knowledge sup-\nplements the sparse statistical signals in small-scale categorical data. Attention-\nweighted encoding (ARISE) provides further gains on most datasets, achieving\nan average ARI of 0.402. On SF and BC, however, attention-weighted encod-\ning shows comparable or slightly lower performance, likely because specialized\nscientific or medical terms exhibit limited semantic variation.\n"}, {"page": 13, "text": "Bridging the Semantic Gap in Categorical Clustering\n13\n(a) DiSC\n(b) MCDC\n(c) OCL\n(d) SigDT\n(e) COForest\n(f) ARISE (Ours)\nCluster 1\nCluster 2\nFig. 4: UMAP visualization of cluster structures on MU. Panels (a)–(e)\nshow projections from counterparts; panel (f) displays the ARISE representation.\n4.4\nScalability Analysis\nFigure 3 shows the runtime with respect to instance count N, attribute count\nM, and unique value count |V|.\nAs shown in Figure 3(a), the runtime growth of ARISE with respect to N\nremains linear, matching that of classical symbolic methods such as OHK and\nKMo. This confirms that semantic enrichment does not elevate the complexity\nclass. Compared with coupling-based methods such as MCDC, OCL, and COFor-\nest, ARISE reduces runtime by orders of magnitude. This efficiency gain results\nfrom the decoupled architecture, where semantic processing is isolated to an of-\nfline preprocessing phase, avoiding the expensive iterative interactions typical of\ncoupling frameworks. Figure 3(b) shows that the runtime with respect to M ex-\nhibits a similar trend across methods. ARISE scales linearly with the number of\nattributes, as each attribute embedding is retrieved independently. Figure 3(c)\nreports the total runtime including offline description generation, which scales\nlinearly with |V|. The online clustering time of ARISE remains stable across all\nvocabulary sizes. Since each value is processed independently during the offline\nstage, description generation can be parallelized across multiple LLM instances.\n4.5\nVisualization\nFigure 4 presents UMAP projections on the MU dataset. Counterparts (panels a–\ne) produce fragmented structures where points from the two classes intermingle\nacross scattered regions. ARISE (panel f) yields two compact and well-separated\ngroups that align closely with the ground-truth class labels. This alignment\nindicates that semantic enrichment captures class-relevant structure, producing\nrepresentations with stronger semantic coherence.\n"}, {"page": 14, "text": "14\nZ. Yang et al.\n5\nConcluding Remarks\nThis paper has presented ARISE, a framework that integrates external semantic\nknowledge from LLMs into categorical data clustering. ARISE queries LLMs at\nthe attribute-value level, generating a semantic-enhanced description for each\nunique value, and emphasizes the key informative tokens through attention-\nweighting to produce semantic-compact embeddings for each value. ARISE also\nadaptively integrates the categorical value identity vector into the LLM-enhanced\nembeddings for regularization, relieving the dominance of LLMs in the represen-\ntations. Experiments on eight benchmark datasets across seven counterparts val-\nidate the efficacy of ARISE, which consistently achieves gains of 19–27% on all\nthe experimental datasets. Moreover, four mainstream LLM options have been\nconsidered for the semantic enhancement, with the results indicating the supe-\nriority of GPT. The achieved improvements are particularly notable when the\ndata scale is relatively small, confirming the necessity of the external semantic\nknowledge for the metric space complementation of categorical data. The next\navenue of this work could be extending to mixed-type data and prompt tuning\nof LLMs for domain- and task-specific adaptation.\nReferences\n1. Borisov, V., Seßler, K., Leemann, T., Pawelczyk, M., Kasneci, G.: Language Models\nare Realistic Tabular Data Generators. In: ICLR (2023)\n2. Cai, S., Zhang, Y., Luo, X., Cheung, Y.M., Jia, H., Liu, P.: Robust Categorical\nData Clustering Guided by Multi-Granular Competitive Learning. In: ICDCS, pp.\n288–299 (2024)\n3. Chen, J., Ji, Y., Zou, R., Zhang, Y., Cheung, Y.M.: QGRL: Quaternion Graph\nRepresentation Learning for Heterogeneous Feature Data Clustering. In: KDD,\npp. 297–306 (2024)\n4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In: NAACL, pp. 4171–\n4186 (2019)\n5. Dinh, T., Wong, H., Fournier-Viger, P., Lisik, D., Ha, M.Q., Dam, H.C., Huynh,\nV.N.: Categorical Data Clustering: 25 Years Beyond K-modes. Expert Syst. Appl.\n272, 126608 (2025)\n6. Dua,\nD.,\nGraff,\nC.:\nUCI\nMachine\nLearning\nRepository.\nUniversity\nof\nCalifornia,\nIrvine,\nSchool\nof\nInformation\nand\nComputer\nSciences\n(2019).\nhttps://archive.ics.uci.edu\n7. Estévez, P.A., Tesmer, M., Perez, C.A., Zurada, J.M.: Normalized Mutual Infor-\nmation Feature Selection. IEEE Trans. Neural Netw. 20(2), 189–201 (2009)\n8. Feng, S., Zhao, M., Huang, Z., Ji, Y., Zhang, Y., Cheung, Y.M.: Robust Qualitative\nData Clustering via Learnable Multi-Metric Space Fusion. In: ICASSP, pp. 1–5\n(2025)\n9. Feng, Z., Lin, L., Wang, L., Cheng, H., Wong, K.F.: LLMEdgeRefine: Enhancing\nText Clustering with LLM-Based Boundary Point Refinement. In: EMNLP, pp.\n18455–18462 (2024)\n10. Gates, A.J., Ahn, Y.Y.: The Impact of Random Models on Clustering Similarity.\nJ. Mach. Learn. Res. 18(87), 1–28 (2017)\n"}, {"page": 15, "text": "Bridging the Semantic Gap in Categorical Clustering\n15\n11. He, X., Cai, D., Niyogi, P.: Laplacian Score for Feature Selection. In: NeurIPS, pp.\n507–514 (2005)\n12. Hegselmann, S., Buendia, A., Lang, H., Agrawal, M., Ber, X., Sontag, D.: TabLLM:\nFew-shot Classification of Tabular Data with Large Language Models. In: AIS-\nTATS, pp. 5549–5581 (2023)\n13. Herzig, J., Nowak, P.K., Müller, T., Piccinno, F., Eisenschlos, J.: TAPAS: Weakly\nSupervised Table Parsing via Pre-training. In: ACL, pp. 4320–4333 (2020)\n14. Hu, L., Jiang, M., Liu, X., He, Z.: Significance-Based Decision Tree for Interpretable\nCategorical Data Clustering. Inf. Sci. 690, 121588 (2025)\n15. Huang, Z.: Extensions to the K-Means Algorithm for Clustering Large Data Sets\nwith Categorical Values. Data Min. Knowl. Discov. 2(3), 283–304 (1998)\n16. Kobayashi, G., Kuribayashi, T., Yokoi, S., Inui, K.: Attention is Not Only a Weight:\nAnalyzing Transformers with Vector Norms. In: EMNLP, pp. 7057–7075 (2020)\n17. McConville, R., Santos-Rodriguez, R., Piechocki, R.J., Craddock, I.: N2D: (Not\nToo) Deep Clustering via Clustering the Local Manifold of an Autoencoded Em-\nbedding. In: ICPR, pp. 5145–5152 (2021)\n18. Modarressi, A., Fayyaz, M., Yaghoobzadeh, Y., Pilehvar, M.T.: GlobEnc: Quan-\ntifying Global Token Attribution by Incorporating the Whole Encoder Layer in\nTransformers. In: NAACL, pp. 258–271 (2022)\n19. Reimers, N., Gurevych, I.: Sentence-BERT: Sentence Embeddings using Siamese\nBERT-Networks. In: EMNLP, pp. 3982–3992 (2019)\n20. Viswanathan, V., Gashteovski, K., Lawrence, C., Wu, T., Neubig, G.: Large Lan-\nguage Models Enable Few-Shot Clustering. TACL 12, 321–333 (2024)\n21. Yin, P., Neubig, G., Yih, W., Riedel, S.: TaBERT: Pretraining for Joint Under-\nstanding of Textual and Tabular Data. In: ACL, pp. 8413–8426 (2020)\n22. Zhang, Y., Cheung, Y.M., Zeng, A.: Het2Hom: Representation of Heterogeneous\nAttributes into Homogeneous Concept Spaces for Categorical-and-Numerical-\nAttribute Data Clustering. In: IJCAI, pp. 3758–3765 (2022)\n23. Zhang, Y., Cheung, Y.M.: Learnable Weighting of Intra-Attribute Distances for\nCategorical Data Clustering with Nominal and Ordinal Attributes. IEEE TPAMI\n44(7), 3560–3576 (2022)\n24. Zhang, Y., Cheung, Y.M.: Graph-Based Dissimilarity Measurement for Cluster\nAnalysis of Any-Type-Attributed Data. IEEE TNNLS 34(9), 6530–6544 (2023)\n25. Zhang, Y., Wang, Z., Shang, J.: ClusterLLM: Large Language Models as a Guide\nfor Text Clustering. In: EMNLP, pp. 13903–13920 (2023)\n26. Zhang, Y., Luo, X., Chen, Q., Zou, R., Zhang, Y., Cheung, Y.M.: Towards Unbiased\nMinimal Cluster Analysis of Categorical-and-Numerical Attribute Data. In: ICPR,\npp. 254–269 (2024)\n27. Zhang, Y., Zhao, M., Jia, H., Li, M., Lu, Y., Cheung, Y.M.: Categorical Data Clus-\ntering via Value Order Estimated Distance Metric Learning. Proc. ACM Manag.\nData 3(6), 1–24 (2025)\n28. Zhang, Y., Zhao, M., Zhang, Y., Cheung, Y.M.: Trending Applications of Large\nLanguage Models: A User Perspective Survey. IEEE Trans. Artif. Intell. (2025)\n29. Zhang, Y., Zou, R., Zhang, Y., Zhang, Y., Cheung, Y.M., Li, K.: Adaptive Micro\nPartition and Hierarchical Merging for Accurate Mixed Data Clustering. Complex\nIntell. Syst. 11(1), 84 (2025)\n30. Zhao, M., Feng, S., Zhang, Y., Li, M., Lu, Y., Cheung, Y.M.: Learning Order Forest\nfor Qualitative-Attribute Data Clustering. In: ECAI, pp. 1943–1950 (2024)\n31. Zhao, M., Huang, Z., Lu, Y., Li, M., Zhang, Y., Su, W., Cheung, Y.M.: Break the\nTie: Learning Cluster-Customized Category Relationships for Categorical Data\nClustering. arXiv:2511.09049 (2025)\n"}]}