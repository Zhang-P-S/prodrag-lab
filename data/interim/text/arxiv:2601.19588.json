{"doc_id": "arxiv:2601.19588", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.19588.pdf", "meta": {"doc_id": "arxiv:2601.19588", "source": "arxiv", "arxiv_id": "2601.19588", "title": "From Atoms to Chains: Divergence-Guided Reasoning Curriculum for Unlabeled LLM Domain Adaptation", "authors": ["Yongqi Wang", "Xiaofeng Ji", "Jie Wang", "Qingbin Li", "Xiao Xiong", "Zheming Yang", "Jian Xu", "Minghui Qiu", "Xinxiao Wu"], "published": "2026-01-27T13:23:40Z", "updated": "2026-01-27T13:23:40Z", "summary": "Adapting Large Language Models (LLMs) to specialized domains without human-annotated data is a crucial yet formidable challenge. Widely adopted knowledge distillation methods often devolve into coarse-grained mimicry, where the student model inefficiently targets its own weaknesses and risks inheriting the teacher's reasoning flaws. This exposes a critical pedagogical dilemma: how to devise a reliable curriculum when the teacher itself is not an infallible expert. Our work resolves this by capitalizing on a key insight: while LLMs may exhibit fallibility in complex, holistic reasoning, they often exhibit high fidelity on focused, atomic sub-problems. Based on this, we propose Divergence-Guided Reasoning Curriculum (DGRC), which constructs a learning path from atomic knowledge to reasoning chains by dynamically deriving two complementary curricula from disagreements in reasoning pathways. When a student and teacher produce conflicting results, DGRC directs the teacher to perform a diagnostic analysis: it analyzes both reasoning paths to formulate atomic queries that target the specific points of divergence, and then self-answers these queries to create high-confidence atomic question-answer pairs. These pairs then serve a dual purpose: (1) providing an atomic curriculum to rectify the student's knowledge gaps, and (2) serving as factual criteria to filter the teacher's original reasoning chains, yielding a verified CoT curriculum that teaches the student how to integrate atomic knowledge into complete reasoning paths. Experiments across the medical and legal domains on student models of various sizes demonstrate the effectiveness of our DGRC framework. Notably, our method achieves a 7.76% relative improvement for the 1.5B student model in the medical domain over strong unlabeled baseline.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.19588v1", "url_pdf": "https://arxiv.org/pdf/2601.19588.pdf", "meta_path": "data/raw/arxiv/meta/2601.19588.json", "sha256": "8d6647d289880ad6a9d1e33c8faea4f43c442e092e184f97216f2c6c3244a881", "status": "ok", "fetched_at": "2026-02-18T02:20:23.851359+00:00"}, "pages": [{"page": 1, "text": "FROM ATOMS\nTO CHAINS:\nDIVERGENCE-GUIDED\nREASONING CURRICULUM\nFOR UNLABELED LLM\nDOMAIN ADAPTATION\nYongqi Wang ∗,1,2, Xiaofeng Ji*,2, Jie Wang2, Qingbin Li2, Xiao Xiong2, Zheming Yang2,\nJian Xu2, Minghui Qiu†,2, Xinxiao Wu†,1\n1Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science\nand Technology, Beijing Institute of Technology\n2ByteDance China\n{3120230916, wuxinxiao}@bit.edu.cn,\n{xiaofeng.ji, wangjie.mayday}@bytedance.com,\n{yangzheming, xiaochen.qiu}@bytedance.com\nABSTRACT\nAdapting Large Language Models (LLMs) to specialized domains without\nhuman-annotated data is a crucial yet formidable challenge.\nWidely adopted\nknowledge distillation methods often devolve into coarse-grained mimicry, where\nthe student model inefficiently targets its own weaknesses and risks inheriting the\nteacher’s reasoning flaws. This exposes a critical pedagogical dilemma: how to\ndevise a reliable curriculum when the teacher itself is not an infallible expert. Our\nwork resolves this by capitalizing on a key insight: while LLMs may exhibit fal-\nlibility in complex, holistic reasoning, they often exhibit high fidelity on focused,\natomic sub-problems. Based on this, we propose Divergence-Guided Reasoning\nCurriculum (DGRC), which constructs a learning path from atomic knowledge to\nreasoning chains by dynamically deriving two complementary curricula from dis-\nagreements in reasoning pathways. When a student and teacher produce conflict-\ning results, DGRC directs the teacher to perform a diagnostic analysis: it analyzes\nboth reasoning paths to formulate atomic queries that target the specific points of\ndivergence, and then self-answers these queries to create high-confidence atomic\nquestion-answer pairs. These pairs then serve a dual purpose: (1) providing an\natomic curriculum to rectify the student’s knowledge gaps, and (2) serving as\nfactual criteria to filter the teacher’s original reasoning chains, yielding a verified\nCoT curriculum that teaches the student how to integrate atomic knowledge into\ncomplete reasoning paths. Experiments across the medical and legal domains on\nstudent models of various sizes demonstrate the effectiveness of our DGRC frame-\nwork. Notably, our method achieves a 7.76% relative improvement for the 1.5B\nstudent model in the medical domain over strong unlabeled baseline.\n1\nINTRODUCTION\nLarge Language Models (LLMs) have established a strong performance baseline by mastering foun-\ndational linguistic and factual knowledge (Grattafiori et al., 2024; Team et al., 2024; Guo et al.,\n2025a; Yang et al., 2025). However, advancing from this foundational capability to expert-level\nreasoning in specialized domains is a significant leap that requires targeted adaptation (Wang et al.,\n2023; Iacovides et al., 2024). This process remains a formidable challenge, primarily due to the\nprohibitive cost and effort of curating high-quality, human-annotated datasets. Consequently, devel-\noping effective methods for unlabeled LLM adaptation has become a critical frontier.\nResearch within this frontier has largely advanced along two main avenues. The first is LLM knowl-\nedge distillation (Taori et al., 2023; Hsieh et al., 2023; Xu et al., 2024), which involves a student\n∗Equal contribution.\n† Corresponding authors.\n1\narXiv:2601.19588v1  [cs.LG]  27 Jan 2026\n"}, {"page": 2, "text": "A jacket originally priced at $200 is on \nsale for 20% off. As a club member, you \nget an additional 10% off the sale price. \nWhat is the final price you pay?\nReasoning：\n1. Original Price: $200.\n2. The problem has two discounts: 20% \nand 10%.\n3. To find the total discount, I can add \nthe percentages: 20% + 10% = 30%.\n4. The total discount is 30%. The final \nprice is $200 * (1 - 0.30) = $140.\nFinal Answer: $140\nAtomic question:\nIs applying a 20% discount and then an \nadditional 10% discount the same as \napplying a single 30% discount? \nResponse:\nNo, they are not the same. \nThe second 10% discount is applied to \nthe already discounted price (the sale \nprice), not the original price. \nTherefore, the total discount is less \nthan 30%. \nFigure 1: An illustration of the cognitive asymmetry in LLMs. The same LLM exhibits fallibility on\na multi-step problem by incorrectly applying a flawed heuristic (left), yet demonstrates high fidelity\nwhen prompted with an atomic question that isolates the core conceptual error (right).\nmodel mimicking the reasoning process of a more capable teacher. However, this approach con-\nfronts two fundamental challenges in the unlabeled setting: (1) the mimicry is too coarse-grained to\nefficiently address the student’s specific weaknesses, and (2) the teacher’s supervision is inherently\nunreliable without ground-truth labels, risking the propagation of its own reasoning flaws. The sec-\nond paradigm attempts to bypass the teacher’s fallibility by generating training data from external\nknowledge bases (Zhang et al., 2024; Qin et al., 2025; Guo et al., 2025b), but this strategy is often\nconstrained by the high cost of curating domain-specific resources.\nThese limitations expose a critical pedagogical dilemma: how to devise a reliable curriculum without\nresorting to costly external knowledge bases when the teacher model itself is not an infallible expert.\nOur work resolves this dilemma by leveraging a key insight we term cognitive asymmetry: while\nLLMs often fail at complex, holistic reasoning, their performance on focused, atomic sub-problems\nis highly reliable. This principle is supported by extensive research in problem decomposition (Zhou\net al., 2022; Xue et al., 2024; Zhou et al., 2025) and is validated by our experiments in Section 4.3.\nFigure 1 illustrates this phenomenon. The same LLM that incorrectly applies a flawed heuristic\n(conflating sequential discounts) in a full Chain-of-Thought process can accurately articulate the\ncorrect principle when prompted with a focused question.\nGrounded in this insight, we propose the Divergence-Guided Reasoning Curriculum (DGRC), a\nframework that dynamically generates a two-part curriculum based on model disagreements to guide\na student’s learning path from foundational atomic knowledge to complex reasoning chains, which\nredefines the teacher-student interaction. Instead of assuming the student is at fault when results con-\nflict, DGRC treats the divergence as a trigger for an impartial inquiry. The teacher model is tasked\nto act as a neutral diagnostician: it analyzes both reasoning paths to formulate atomic, root-cause\nqueries about the precise point of divergence. Crucially, the teacher answers these focused queries\nindependently of its original reasoning chain, capitalizing on its high fidelity for such atomic tasks.\nAfter a quality filtering stage, the generated high-confidence atomic question-answer pairs serve a\ndual purpose that directly resolves the initial pedagogical dilemma: (1) they constitute the atomic\ncurriculum, used to rectify the student’s foundational knowledge gaps, replacing coarse-grained\nmimicry with targeted lessons, and (2) they act as factual criteria to filter the teacher’s original\nthought process, mitigating the propagation of flaws and yielding a verified CoT curriculum for\ntraining the student to compose complex reasoning chains.\nTo comprehensively validate our approach, we conduct extensive experiments across a wide range\nof specialized domains, models, and training paradigms. Our results demonstrate that DGRC is\na highly versatile framework that delivers consistent and substantial performance improvements\nacross different teacher-student pairs (Section 4.4), in a self-teaching configuration (Section 4.5),\nand with various training strategies (Section 4.7). Notably, our approach is particularly effective on\nsmaller-scale models, boosting the performance of a 1.5B student model by a relative 7.76% in the\nmedical domain compared to a strong unlabeled baseline.\nIn summary, our main contributions are threefold:\n2\n"}, {"page": 3, "text": "• A novel self-supervised framework, DGRC, that enhances the complex reasoning abilities of\nLLMs in specialized domains without requiring any human-annotated data.\n• A novel mechanism that automatically transforms reasoning divergence into a dual-curriculum\nlearning path: an atomic curriculum to rectify factual knowledge and a verified CoT curriculum\nto master compositional reasoning.\n• Extensive empirical validation demonstrating that DGRC achieves substantial performance gains\nacross diverse domains and models, particularly boosting the capabilities of smaller models.\n2\nRELATED WORK\nProblem Decomposition in LLMs.\nDecomposing complex problems into simpler, manageable\nsub-problems is a cornerstone strategy for enhancing reasoning in LLMs. This approach stems from\nan insight we term the cognitive asymmetry of LLMs: a powerful model prone to errors on a complex\ntask can reliably solve its constituent atomic steps. This insight has driven the evolution of methods,\nfrom implicit step-by-step reasoning (Wei et al., 2022; Feng et al., 2023), to explicit sub-task de-\ncomposition (Zhou et al., 2022; Xue et al., 2024; Zhou et al., 2025), and graph-based exploration of\nmultiple reasoning paths (Yao et al., 2023; Besta et al., 2024). While these methods have validated\nthe value of decomposition, their strategies typically depend on manually engineered exemplars or\nfixed heuristics. In contrast, DGRC introduces a novel, data-driven paradigm that utilizes reasoning\ndivergence as an automatic signal to trigger and pinpoint the precise atomic questions that need to\nbe resolved.\nLLM Adaptation in Unlabeled Settings.\nThe challenge of adapting LLMs to specialized do-\nmains without annotated data has spawned two main paradigms. The first, LLM Knowledge Distil-\nlation, trains a student model to mimic the reasoning of a more capable teacher (Taori et al., 2023;\nHsieh et al., 2023). However, this approach is fundamentally hampered by the coarse-grained nature\nof the mimicry and the inherent fallibility of the teacher in unlabeled settings. The second paradigm\nattempts to circumvent this by synthesizing training data from external knowledge bases (Zhang\net al., 2024; Qin et al., 2025; Guo et al., 2025b). However, this strategy is often constrained by the\nhigh cost of curating such external resources. This highlights the need for new adaptation methods\nthat can generate fine-grained curriculum from the models’ internal reasoning, thereby overcoming\nthe limitations of both coarse-grained mimicry and the high cost of external resources.\nLearning from Model Feedback.\nOur work is also relevant to the broader field of learning from\nmodel feedback. Many existing approaches can be categorized as learning from error, such as self-\ncorrection, where an LLM revises its own output (Madaan et al., 2023; Song et al., 2025; Alazraki\net al., 2025), and process supervision, which uses a verification model to score each reasoning\nstep (Lightman et al., 2023). However, a core challenge for these methods in unlabeled settings is\nthe difficulty of reliably identifying an error without ground-truth labels. Our method sidesteps this\nchallenge by shifting the paradigm from learning from error to learning from disagreement. Instead\nof requiring a clear error signal, it uses the readily available divergence between two models to\npinpoint their points of uncertainty and transform them into targeted learning opportunities.\n3\nDIVERGENCE-GUIDED REASONING CURRICULUM\n3.1\nOVERVIEW\nThis work addresses the challenge of adapting a student Large Language Model (LLM) to a spe-\ncialized domain using only a set of unlabeled problems. Formally, given a collection of unlabeled\nproblems Dunlabeled = {p1, p2, ..., pN}, a powerful teacher model MT , and a student model MS,\nour goal is to enhance the student’s domain-specific reasoning capabilities without access to ground-\ntruth solutions.\nTo this end, we propose the Divergence-Guided Reasoning Curriculum (DGRC), a multi-stage\nframework that harnesses reasoning discrepancies between teacher and student models to construct\na structured learning path. As illustrated in Figure 2, DGRC operates in three consecutive stages.\nThe process begins with the divergence detection stage, where disagreements in the models’ final\n3\n"}, {"page": 4, "text": "(a) Divergence Detection Stage\nUnlabeled\nProblem\nTeacher Model\nStudent Model\nsampling\nsampling\nK Teacher Responses\nJ Student Responses\nPair-wise\nAnswer\nComparison\nDivergent Instances\nDivergent Instances\nTeacher Responses\nTeacher Model\nReasoning \nDivergence Diagnosis\nAtomic Questions\nTeacher Model\nAtomic \nQ&A Pairs\nAtomic Curriculum\nAtomic Curriculum Generation\nVerified CoT Curriculum Generation\nTeacher Model\nVerified CoT \nCurriculum\nReasoning Verification\n(b) Curriculum Generation Stage\n(c) Student Adaptation Stage\nAtomic Curriculum\nStudent Model\nTraining\nTraining Phase 1\nAtomically-\ntuned Model\nTraining Phase 2\nVerified CoT \nCurriculum\nTraining\nAtomically-\ntuned Model\nAdapted Model\nQuality\nFiltering\nFigure 2: The overview of the Divergence-Guided Reasoning Curriculum (DGRC) framework.\nanswers trigger the learning process. Triggered by these disagreements, the curriculum genera-\ntion stage produces two complementary curricula: an atomic curriculum designed to address the\nroot causes of divergence, and a verified CoT curriculum comprising filtered, high-quality reasoning\nchains from the teacher. The framework concludes with the student adaptation stage, where the\nstudent model is trained on these curricula, implementing an “atom-to-chain” learning paradigm to\nsystematically enhance its reasoning abilities.\n3.2\nDIVERGENCE DETECTION STAGE\nThe DGRC framework initiates with the divergence detection stage, as illustrated in Figure 2 (a).\nThe goal of this stage is to efficiently generate a pool of divergent instances by sampling multiple\nresponses from the teacher and student models and performing a pair-wise comparison of their\nanswers.\nSpecifically, for each unlabeled problem pi ∈Dunlabeled, we sample K responses from the teacher\nand J from the student, yielding the respective output sets Oi\nT = {oi\nT,1, ..., oi\nT,K} and Oi\nS =\n{oi\nS,1, ..., oi\nS,J}. Each response oi = (ci, ai) contains a reasoning chain ci and a final answer ai.\nWe then conduct a systematic pair-wise comparison, evaluating each of the K teacher responses\nagainst each of the J student responses. A pair is flagged as divergent if their final answers conflict\n(see Appendix A for specific criteria). Formally, this produces a set of divergent pairs for each\nproblem pi:\nDpi = {(oi\nT , oi\nS)|oi\nT ∈Oi\nT , oi\nS ∈Oi\nS, and ai\nT ̸= ai\nS}.\n(1)\nThis stage culminates in a diagnostic dataset, Ddiag, which aggregates all problems exhibiting at least\none divergence:\nDdiag = {(pi, Dpi, Oi\nT )|pi ∈Dunlabeled, and Dpi ̸= ∅}.\n(2)\nEach entry in this dataset encapsulates the necessary information for the subsequent stages: the\ndivergent pairs (Dpi) are passed on for atomic curriculum generation stage, while the set of teacher\nresponses (Oi\nT ) is retained for the verified CoT curriculum generation stage.\n4\n"}, {"page": 5, "text": "3.3\nCURRICULUM GENERATION STAGE\nUpon identifying divergent instances in Ddiag, the framework proceeds to its conceptual core: the\ncurriculum generation stage, depicted in Figure 2 (b). In this stage, abstract model disagreements\nare transformed into two concrete, complementary curricula through two sequential sub-stages.\n3.3.1\nATOMIC CURRICULUM GENERATION\nThis sub-stage diagnoses the root causes of reasoning divergence and distills them into an atomic\ncurriculum. The process begins by iterating through each problem pi in the diagnostic set Ddiag.\nFor each divergent instance (oi\nT , oi\nS) ∈Dpi, we prompt the teacher model to act as a diagnosti-\ncian, analyzing both its own reasoning ci\nT and the student’s ci\nS. This analysis, guided by a carefully\nengineered prompt (Appendix B.1), synthesizes a set of atomic questions. Each question is a fun-\ndamental, self-contained query designed to isolate a single point of factual or logical discrepancy\nbetween the two chains.\nThe teacher then answers each atomic question de novo, unconstrained by its original reasoning.\nThis step leverages the cognitive asymmetry principle: a model’s accuracy on focused, atomic\nqueries typically surpasses its reliability on complex, multi-step tasks. After a quality filtering pro-\ncess (Appendix C), the resulting high-confidence Q&A pairs—constituting verified atomic knowl-\nedge—are aggregated to form the atomic curriculum.\n3.3.2\nVERIFIED COT CURRICULUM GENERATION\nThis sub-stage curates the verified CoT curriculum by using the previously generated atomic knowl-\nedge for quality control. For each problem pi ∈Ddiag, we consolidate its verified, high-confidence\natomic Q&A pairs into a verification set, denoted as Ai. This set then serves as a factual standard\nto audit the teacher’s original reasoning chains (Oi\nT ). Using a dedicated prompt (Appendix B.2),\nthe teacher model assesses each chain ci\nT,k against Ai, retaining only those that exhibit no factual\ninconsistencies. This rigorous auditing process yields the verified CoT curriculum: a collection of\nhigh-fidelity, multi-step reasoning exemplars.\n3.4\nSTUDENT ADAPTATION STAGE\nAs illustrated in Figure 2 (c), the student adaptation stage implements our “atom-to-chain” learn-\ning philosophy through a two-phase training process. First, the student model is fine-tuned on the\natomic curriculum, a crucial step that rectifies foundational knowledge gaps to produce an interme-\ndiate, atomically-tuned model. This improved model is then trained on the verified CoT curriculum,\nleveraging its high-fidelity exemplars to master the composition of complex reasoning chains.\n4\nEXPERIMENTS\nOur comprehensive experiments validate the effectiveness and versatility of the DGRC framework.\nWe first establish its superior performance against strong baselines across two specialized domains\n(Section 4.2). We then demonstrate its flexibility across various model configurations (Section 4.4),\nin a self-teaching setting (Section 4.5), and when integrated with reinforcement learning to probe its\nupper performance limits (Section 4.7).\nTo dissect the framework’s core mechanisms, we conduct a series of in-depth analyses. We em-\npirically validate our foundational “cognitive asymmetry” insight (Section 4.3) and perform crucial\nablation studies on the curriculum components (Section 4.6). Further detailed analyses in the Ap-\npendix quantify the effectiveness of our CoT verification step (Appendix H), study the impact of\nsampling counts (Appendix J), and provide a qualitative analysis that links model uncertainty to the\nerrors targeted by our method (Appendix K).\n5\n"}, {"page": 6, "text": "4.1\nEXPERIMENTAL SETUP\nDomains and Benchmarks.\nWe evaluate DGRC’s effectiveness across two specialized domains:\nmedical and legal. For the medical domain, we provide a comprehensive assessment by testing the\nadapted model on three distinct benchmarks: the validation set of MedMCQA (Pal et al., 2022), the\ntest set of MedQA-USMLE (Jin et al., 2021), and the professional medicine subset of the MMLU\nbenchmark (Hendrycks et al., 2020). For the legal domain, evaluation is conducted on the validation\nset of CaseHOLD (Zheng et al., 2021) and the professional law subset of MMLU.\nTraining Data.\nThe DGRC curricula for each domain are generated using a pool of corresponding\nunlabeled queries. For the medical domain, we utilize 182,822 questions from the official training\nset of MedMCQA (Pal et al., 2022). For the legal domain, the curriculum is generated from 42,509\ncase problems in the CaseHOLD (Zheng et al., 2021) training set. All ground-truth labels and\nanswers are discarded to strictly maintain our unlabeled adaptation setting. Detailed statistics on the\ndata volumes are provided in Appendix G.\nModels.\nTo evaluate the versatility and scalability of DGRC, we employ a multi-teacher, multi-\nstudent configuration. We use three distinct teacher models to generate the curricula, spanning both\nproprietary model (GPT-4.1 (Achiam et al., 2023)) and powerful open-source options (Qwen2.5-\nInstruct-32B and -72B (An et al., 2024)). The student models targeted for adaptation are three\nsmaller-scale versions from the same family: Qwen2.5-Instruct-1.5B, -3B, and -7B. Our main results\nare reported using GPT-4.1 as the teacher, and we validate the framework’s generalizability with\nopen-source teachers in Section 4.4.\nBaselines.\nWe compare DGRC against three strong adaptation baselines. The first, Baseline-w/o-\nlabel, is an unlabeled competitor that trains on the reasoning chain corresponding to the majority\nfinal answer among K teacher-generated samples. The second, Baseline-w/-label, acts as a super-\nvised upper bound by using ground-truth labels to select only correct reasoning chains for training.\nThe third, RLAIF Lee et al. (2023) (GRPO), acts as a reinforcement learning baseline where the\nstudent model is optimized using Group Relative Policy Optimization (Shao et al., 2024). In this\nsetting, the teacher model serves as a reward model to provide scalar feedback on the correctness of\nthe student’s reasoning and final answer.\nImplementation Details.\nTo ensure a fair comparison, our main experiments implement DGRC\nand the distillation baselines using Supervised Fine-Tuning (SFT), while the RLAIF baseline follows\nthe standard RL training protocol. However, the DGRC framework itself is compatible with various\ntraining paradigms, a point we explore further in Section 4.7. More implementation details are\nprovided in Appendix F.\n4.2\nMAIN RESULTS\nOur main results are summarized in Table 1, which presents a comprehensive performance com-\nparison across four distinct categories of models on benchmarks in the medical and legal domains.\nThese categories include: (1) leading proprietary models, including GPT-4.1 (Achiam et al., 2023)\nand Gemini-2.5-Pro (Comanici et al., 2025); (2) general-purpose open-source models, including\nthe Qwen2.5 series (An et al., 2024), the Llama3.1 series Grattafiori et al. (2024), and DeepSeek-\nR1 (Guo et al., 2025a); (3) specialized domain-expert models including the MegaScience series (Fan\net al., 2025), LegalHal (Hu et al., 2025) MediTron (Chen et al., 2023), and Med-PaLM2 (Singhal\net al., 2025); and (4) our proposed DGRC method alongside three strong baselines (Supervised Dis-\ntillation, Unlabeled Distillation, and RLAIF), evaluated at various model scales. From these results,\nwe highlight four key findings.\nDGRC Outperforms Distillation Baselines.\nAcross all model scales, DGRC consistently and\nsubstantially outperforms the unlabeled distillation baseline. This is exemplified in the medical\ndomain, where the 1.5B student achieves a 7.76% relative improvement over the Baseline-w/o-\nlabel. Perhaps most strikingly, in the medical domain, the DGRC-adapted 3B model even surpasses\nthe Baseline-w/-label with a 3.71% relative gain. This result is significant: it suggests that DGRC’s\nmethod of identifying and correcting specific errors provides a more potent learning signal than\n6\n"}, {"page": 7, "text": "Table 1: Main results on medical and legal domains. We report accuracy (%) across five bench-\nmarks, with averaged results for the medical (Avg-M) and legal (Avg-L) domains. Label indicates\nif the method requires human-annotated domain-specific training data for the adaptation stage (✓)\nor not (✗). Knowledge indicates if the method requires an external knowledge base (✓) or not (✗).\nPerformance for models marked with † is reported from their respective original publications.\nModel / Method\nParams\nLabel\nKnowledge\nMedMCQA\nMedQA\nMMLU-M\nAvg-M\nCaseHOLD\nMMLU-L\nAvg-L\nProprietary Models\nGPT-4.1 (25-04-14)\nN/A\n✗\n✗\n81.3\n92.5\n97.3\n90.4\n76.4\n91.7\n84.1\nGemini-2.5-Pro (25-06-05)\nN/A\n✗\n✗\n82.1\n89.7\n98.1\n90.0\n79.0\n92.6\n85.8\nGeneral-Purpose Open-Source Models\nQwen2.5-Instruct\n1.5B\n✗\n✗\n37.1\n40.1\n54.8\n44.0\n44.1\n61.2\n52.7\nQwen2.5-Instruct\n3B\n✗\n✗\n38.4\n41.6\n63.7\n47.9\n38.9\n61.2\n50.1\nQwen2.5-Instruct\n7B\n✗\n✗\n55.6\n59.8\n86.6\n67.3\n63.9\n75.2\n69.6\nQwen2.5-Instruct\n32B\n✗\n✗\n63.1\n73.1\n93.2\n76.5\n68.2\n84.3\n76.3\nQwen2.5-Instruct\n72B\n✗\n✗\n68.6\n79.6\n94.1\n80.8\n71.4\n86.8\n79.1\nLlama3.1-Instruct\n8B\n✗\n✗\n56.5\n65.8\n80.5\n67.6\n57.7\n76.9\n67.3\nLlama3.1-Instruct\n70B\n✗\n✗\n72.2\n82.3\n92.4\n82.3\n69.9\n87.6\n78.8\nDeepSeek-R1\n671B\n✗\n✗\n78.7\n91.0\n99.0\n89.6\n74.6\n91.7\n83.2\nDomain Expert Models\nMegaScience\n1.5B\n✗\n✓\n36.5\n31.2\n61.2\n43.0\n40.9\n63.6\n52.3\nMegaScience\n3B\n✗\n✓\n44.3\n40.0\n73.3\n52.5\n52.4\n66.1\n59.3\nMegaScience\n7B\n✗\n✓\n57.4\n61.0\n87.1\n68.5\n60.5\n81.0\n70.8\nLegalHal\n8B\n✗\n✓\n-\n-\n-\n-\n63.7\n73.6\n68.7\nMediTron†\n70B\n✓\n✓\n53.3\n59.8\n71.5\n61.5\n-\n-\n-\nMed-PaLM2†\n340B\n✓\n✗\n72.3\n85.4\n92.0\n83.2\n-\n-\n-\nDivergence-Guided Reasoning Curriculum\nBaseline-w/-label\n1.5B\n✓\n✗\n49.2\n49.8\n72.2\n57.1\n73.8\n65.3\n69.6\nBaseline-w/o-label\n1.5B\n✗\n✗\n48.9\n48.3\n65.0\n54.1\n70.3\n62.8\n66.6\nRLAIF (GRPO)\n1.5B\n✗\n✗\n50.2\n49.5\n67.5\n55.7\n71.2\n64.5\n67.9\nDGRC (Ours)\n1.5B\n✗\n✗\n51.7\n50.8\n72.2\n58.3\n71.5\n67.8\n69.7\nBaseline-w/-label\n3B\n✓\n✗\n57.6\n59.1\n77.3\n64.7\n75.6\n69.4\n72.5\nBaseline-w/o-label\n3B\n✗\n✗\n56.5\n58.4\n73.5\n62.8\n72.6\n69.4\n71.0\nRLAIF (GRPO)\n3B\n✗\n✗\n58.2\n60.1\n76.5\n64.9\n73.0\n70.8\n71.9\nDGRC (Ours)\n3B\n✗\n✗\n59.7\n60.6\n81.0\n67.1\n73.1\n72.7\n72.9\nBaseline-w/-label\n7B\n✓\n✗\n64.5\n70.1\n90.1\n74.9\n76.9\n78.5\n77.7\nBaseline-w/o-label\n7B\n✗\n✗\n64.3\n69.4\n87.2\n73.7\n73.6\n78.5\n76.1\nRLAIF (GRPO)\n7B\n✗\n✗\n68.1\n71.4\n89.1\n76.2\n74.4\n79.0\n76.7\nDGRC (Ours)\n7B\n✗\n✗\n67.5\n72.8\n90.9\n77.1\n74.3\n79.3\n76.8\nsimply imitating pre-verified reasoning paths. By forcing the model to engage in active repair instead\nof passive imitation, DGRC fosters a deeper and more robust understanding of the knowledge, even\nwhen compared to learning from oracle-selected examples.\nCompetitiveness against Reinforcement Learning.\nTo further validate DGRC’s effectiveness,\nwe compare it with Reinforcement Learning from AI Feedback (RLAIF), implemented via Group\nRelative Policy Optimization (GRPO) using GPT-4.1 as the judge. As shown in Table 1, DGRC\n(via SFT) achieves competitive or superior performance across scales, demonstrating that our\ncurriculum-based approach can act as a highly effective adaptation method. Furthermore, we note\nthat DGRC is methodologically distinct from RLAIF: DGRC functions as a data synthesis engine\nthat generates high-quality curricula from model divergence, whereas RLAIF is an optimization\nparadigm. This suggests that while DGRC is competitive on its own, its generated curricula could\npotentially serve as robust high-quality data to further enhance RLAIF training, a synergy we par-\ntially explore in Appendix I.\nDGRC Delivers Superior Generalization.\nA key advantage of DGRC is its enhanced general-\nization capability. Our adaptation training is performed exclusively on the two benchmarks: MedM-\nCQA for medicine and CaseHOLD for law. However, DGRC’s performance gains are most pro-\nnounced on the unseen benchmarks (MedQA, MMLU-M, and MMLU-L). Crucially, on these gen-\neralization tasks, DGRC not only surpasses the unlabeled baseline but also consistently outperforms\nthe Baseline-w/-label. This indicates that by focusing on correcting foundational, atomic errors,\nDGRC effectively prevents overfitting to the patterns of the training benchmarks. Instead, it com-\npels the model to learn portable atomic knowledge that are applicable across a wider range of unseen\ntasks.\nDGRC Achieves High Parameter Efficiency.\nDGRC demonstrates remarkable parameter effi-\nciency, enabling smaller models to achieve highly competitive performance. While a performance\ngap remains compared to top proprietary models like GPT-4.1, our DGRC-adapted Qwen2.5-\nInstruct-7B student successfully surpasses its much larger 32B sibling model. Furthermore, our\n7\n"}, {"page": 8, "text": "Table 2: Peer-correction results and analysis of atomic knowledge quality. Atomic Q&A Acc. (Hu-\nman) is the manual evaluation on 100 sampled pairs. Atomic Q&A Acc. (Auto) is the automated\nevaluation on 2,000 sampled pairs using Gemini-2.5-Pro as the judge. Peer Reliability measures\nthe consistency (Pass@5) of the peer model on the divergent problems. Correction Rate is the\npercentage of initial errors a model successfully fixed using its generated atomic knowledge.\nModel\n# of\nProblems\n# of Atomic\nQ&As\nAtomic Q&A Acc.\n(Human, 100 samples)\nAtomic Q&A Acc.\n(Auto, 2k samples)\nPeer Reliability\n(Pass@5)\nCorrection\nRate\nGPT-4.1 (25-04-14)\n212\n843\n95.0%\n94.2%\n55.4%\n22.6%\nQwen2.5-Instruct-72B\n734\n4,037\n88.0%\n86.5%\n79.1%\n34.6%\nadapted models outperform several larger domain-expert models, demonstrating that DGRC’s gen-\neral reasoning curriculum can be more effective than specialized pre-training. For instance, our 3B\nadapted student surpasses the 70B-parameter MediTron, while our 1.5B model outperforms the 8B\nLegalHal.\n4.3\nVALIDATING COGNITIVE ASYMMETRY AND ATOMIC KNOWLEDGE QUALITY\nThe core assumption of our work is the principle of cognitive asymmetry. To validate this as-\nsumption and to quantitatively measure the quality of the generated atomic knowledge, we de-\nsign a straightforward peer-correction experiment. The setup for this process is as follows: we\nfirst curate two sets of problems from MedMCQA benchmark where our teacher models, GPT-4.1\nand Qwen2.5-Instruct-72B, disagree—one succeeding where the other fails. For each problem, the\nmodel that initially fails is tasked with a DGRC-style diagnostic process: it analyzes the divergence\nbetween its own flawed reasoning and the correct reasoning provided by its peer, generating a set of\natomic Q&A pairs.\nTo assess the quality of atomic knowledge, we employ a dual-evaluation strategy. First, we randomly\nsample 100 Q&A pairs generated by each model to ensure diversity, followed by rigorous manual\nevaluation by domain experts. Second, to verify the robustness of our findings on a larger scale,\nwe employ Gemini-2.5-Pro as an automated judge to evaluate a much larger subset of 2,000 atomic\nQ&A pairs. The specific prompts and criteria used for this automated judgment are detailed in\nAppendix D. Subsequently, the failing model is prompted to re-evaluate its original reasoning chain\nagainst the atomic knowledge it has just created. The specific prompt used for this instruction is\ndetailed in Appendix E. If an inconsistency is found, the model must correct the specific flawed step\nand then resume the reasoning process from that point forward to generate a new final answer. We\nthen measure the final correction rate based on this revised reasoning.\nThe results, presented in Table 2, offer two key insights. First, both manual and automated evalua-\ntions consistently confirm that our method generates atomic knowledge of exceptionally high qual-\nity. Manual evaluation yields accuracy rates of 95.0% for GPT-4.1 and 88.0% for Qwen2.5-Instruct-\n72B. Crucially, these findings are corroborated by the large-scale automated evaluation, which re-\nports highly consistent accuracy rates of 94.2% and 86.5%, respectively. This cross-validated evi-\ndence directly demonstrates that our divergence-guided approach is highly effective at synthesizing\nreliable, foundational facts, even when the model’s initial holistic reasoning is flawed.\nSecond, we observe an intriguing phenomenon: GPT-4.1 achieves superior atomic accuracy yet a\nlower final correction rate (22.6%) compared to the Qwen2.5-Instruct-72B student (34.6%). We hy-\npothesize that this discrepancy is driven by the reliability of the peer’s reasoning used for diagnosis.\nIn our setup, correction is triggered by divergence: when a model fails, the curriculum is derived\nfrom the reasoning of its successful peer. However, a correct final answer does not guarantee cor-\nrect reasoning—the peer might have “guessed” correctly via flawed heuristics. To validate this, we\nevaluate the reasoning consistency (Pass@5) of the successful peer models on these divergent in-\nstances (reported as “Peer Reliability” in Table 2). We find that when Qwen2.5-Instruct-72B acts\nas the peer, its reasoning consistency is lower (55.4%) compared to when GPT-4.1 acts as the peer\n(79.1%). Consequently, the atomic questions generated by a potentially “guessing” peer may fail to\npinpoint the true root cause of the error, rendering the high atomic accuracy of the student ineffective\nfor correction. Conversely, Qwen2.5-Instruct-72B benefits from the robust, high-quality diagnosis\nprovided by GPT-4.1, leading to a higher correction rate despite its lower atomic accuracy.\n8\n"}, {"page": 9, "text": "Table 3: Versatility of DGRC across different teacher and student model configurations on the med-\nical domain. The table reports the average accuracy (%) on three medical benchmarks (Avg-M) and\nthe relative improvement (%) over the corresponding unlabeled baseline.\nTeacher Model\nMethod\nQwen2.5-Instruct-1.5B\nQwen2.5-Instruct-3B\nQwen2.5-Instruct-7B\nAvg-M\nIncrease\nAvg-M\nIncrease\nAvg-M\nIncrease\nQwen2.5-Instruct-32B\nBaseline w/o label\n54.3\n-\n61.5\n-\n68.7\n-\nDGRC (Ours)\n56.9\n(+4.9)\n63.2\n(+2.8)\n69.6\n(+1.3)\nQwen2.5-Instruct-72B\nBaseline w/o label\n56.1\n-\n62.5\n-\n69.6\n-\nDGRC (Ours)\n59.2\n(+5.5)\n65.1\n(+4.2)\n71.1\n(+2.2)\nGPT-4.1 (25-04-14)\nBaseline w/o label\n54.1\n-\n62.8\n-\n73.7\n-\nDGRC (Ours)\n58.3\n(+7.8)\n67.1\n(+6.8)\n77.1\n(+4.6)\nTable 4: Performance of DGRC in a self-teaching configuration on the medical domain (Avg Acc.\n%). Format Compliance measures the percentage of diagnostic outputs that successfully adhered\nto the strict JSON format and logical constraints required for curriculum generation. The value in\nparentheses represents the relative improvement (%) over the zero-shot baseline. Models marked\nwith “Failed” were unable to execute the pipeline.\nModel\nZero-shot\nFormat Compliance\nSelf-teaching\nIncrease\nQwen2.5-Instruct-1.5B\n44.0\n12.4%\nFailed\n-\nQwen2.5-Instruct-3B\n47.9\n34.5%\nFailed\n-\nQwen2.5-Instruct-7B\n67.3\n78.2%\n67.9\n(+0.9)\nQwen2.5-Instruct-32B\n76.5\n94.8%\n77.9\n(+1.8)\nTable 5: Ablation study on the medical domain for the Qwen2.5-Instruct-1.5B student model, with\nall curricula generated by the GPT-4.1 teacher.\nTraining Configuration\nMedMCQA\nMedQA\nMMLU-Med\nAverage\n(1) Student (Zero-shot)\n37.1\n40.1\n54.8\n44.0\n(2) + Atomic Curriculum (Only)\n45.3\n42.3\n61.6\n49.7\n(3) + Verified CoT Curriculum (Only)\n46.7\n48.3\n68.2\n54.4\n(4) DGRC (Ours)\n51.7\n50.8\n72.2\n58.3\n4.4\nVERSATILITY ACROSS DIFFERENT MODELS\nTo validate that DGRC is a model-agnostic framework, we evaluate its performance across various\nteacher-student configurations on the medical domain benchmarks. As presented in Table 3, our\nresults demonstrate significant performance improvements in all tested scenarios and reveal two key\ntrends regarding the framework’s effectiveness.\nFirst, the curriculum’s quality is positively correlated with the teacher’s capability. A stronger\nteacher model, acting as a more proficient diagnostician, synthesizes more insightful atomic ques-\ntions, which in turn yields a higher relative improvement for the student. Second, we observe an\ninverse relationship between a student’s initial capability and the relative performance gain it re-\nceives. The improvement from DGRC is most pronounced for the least capable models, suggesting\nthat weaker students, who possess more significant foundational knowledge gaps, benefit dispropor-\ntionately from the targeted lessons in the atomic curriculum.\n4.5\nSELF-TEACHING ABILITY\nTo further probe the versatility and boundaries of our framework, we investigate DGRC’s capacity\nfor self-teaching, where a model bootstraps its own reasoning capabilities by learning from its inter-\nnal inconsistencies. We evaluate this on the medical domain using four models of different scales:\nQwen2.5-Instruct-1.5B, -3B, -7B, and -32B. In this setup, each model diagnoses divergences among\nits own outputs to generate a curriculum, which is then used to fine-tune the model itself.\nAs shown in Table 4, DGRC enables models to improve through a self-teaching loop, but reveals\na critical capability threshold. The more capable 32B model demonstrates a notable 1.8% relative\nimprovement over its zero-shot baseline, whereas the 7B model shows a marginal gain of 0.9%.\nCrucially, for the smaller 1.5B and 3B models, the DGRC-style self-teaching process is failed. As\n9\n"}, {"page": 10, "text": "Table 6: Performance gains from an additional GRPO phase on the 7B student model on the medical\ndomain, with all curricula generated by the GPT-4.1 teacher. The final column shows the relative\nimprovement (%) over the SFT-only model.\nMethod\nMedMCQA\nMedQA\nMMLU-Med\nAverage\nIncrease\nDGRC (SFT-only)\n67.5\n72.8\n91.0\n77.1\n-\nDGRC (SFT + GRPO)\n69.8\n75.7\n91.8\n79.1\n(+2.6%)\nevidenced by the low scores in the Format Compliance column (12.4% and 34.5%), these models\nstruggle significantly with the instruction-following capability required to strictly adhere to the com-\nplex output constraints (e.g., JSON structure, logical isolation) for generating valid atomic Q&As.\nAlthough they produce some valid outputs, the quantity and quality are insufficient to form a robust\ncurriculum.\nThis finding quantitatively characterizes the minimum capability threshold for DGRC: to act as\nan effective diagnostician, a model must meet a baseline of instruction-following proficiency. A\nsignificant jump in compliance is observed at the 7B scale (rising to 78.2%), indicating that this\nis the critical inflection point where the model’s ability to follow the diagnostic protocol becomes\nreliable enough to support self-correction.\n4.6\nTHE “FROM ATOMS TO CHAINS” LEARNING TRAJECTORY\nTo dissect the contribution of each curriculum component, we conduct a crucial ablation study\ncomparing four training configurations, as shown in Table 5. The results reveal two key insights.\nFirst, both the atomic curriculum and the verified CoT curriculum are independently effective, each\nproviding a substantial performance boost over the zero-shot student. More importantly, the full\nDGRC framework, which trains on atoms then chains, significantly outperforms other configura-\ntions, demonstrating a clear synergistic effect.\n4.7\nEXPLORING PERFORMANCE LIMITS WITH REINFORCEMENT LEARNING\nTo explore the upper performance limits of DGRC, we apply an additional training phase using\nGroup Relative Policy Optimization (GRPO) (Shao et al., 2024) to our best-performing 7B SFT-\nadapted model, training on the high-quality instances from the verified CoT curriculum. As shown\nin Table 6, this GRPO training unlocks a further +2.6% relative improvement over the SFT-only\nmodel. This result confirms that our verified CoT curriculum provides a strong signal for policy\noptimization, highlighting DGRC’s value as a versatile data engine for advanced training paradigms.\n5\nCONCLUSION\nIn this work, we have introduced the Divergence-Guided Reasoning Curriculum (DGRC), a novel\nframework for effective LLM adaptation in unlabeled settings. Our approach is grounded in the\ninsight of cognitive asymmetry: an LLM’s atomic knowledge is reliable even when its complex\nreasoning fails. This insight is key to resolving the pedagogical dilemma of learning from an im-\nperfect teacher. Specifically, DGRC introduces a mechanism that leverages reasoning divergence\nfor an impartial diagnostic process. This process yields a two-part curriculum: an atomic curricu-\nlum to rectify foundational knowledge gaps, and a verified CoT curriculum of reliable exemplars\nfor composing complex arguments, together actualizing the robust “from atoms to chains” learning\npath. Through extensive experiments, we have demonstrated that DGRC is a versatile and highly ef-\nfective framework. It consistently outperforms strong baselines across multiple domains and model\nscales, and its structured learning path has been validated through rigorous ablation studies.\n10\n"}, {"page": 11, "text": "REFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\nreport. arXiv preprint arXiv:2303.08774, 2023.\nLisa Alazraki, Maximilian Mozes, Jon Ander Campos, Tan Yi-Chern, Marek Rei, and Max Bartolo.\nNo need for explanations: Llms can implicitly learn from mistakes in-context. arXiv preprint\narXiv:2502.08550, 2025.\nYang An, Yang Baosong, Zhang Beichen, Hui Binyuan, Zheng Bo, Yu Bowen, Li Chengyuan,\nLiu Dayiheng, Huang Fei, Wei Haoran, et al.\nQwen2.5 technical report.\narXiv preprint\narXiv:2412.15115, 2024.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gian-\ninazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of\nthoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 38, pp. 17682–17690, 2024.\nZeming Chen, Alejandro Hern´andez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba,\nFrancesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K¨opf, Amirkeivan Mohtashami,\net al.\nMeditron-70b: Scaling medical pretraining for large language models.\narXiv preprint\narXiv:2311.16079, 2023.\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the\nfrontier with advanced reasoning, multimodality, long context, and next generation agentic capa-\nbilities. arXiv preprint arXiv:2507.06261, 2025.\nRun-Ze Fan, Zengzhi Wang, and Pengfei Liu. Megascience: Pushing the frontiers of post-training\ndatasets for science reasoning. arXiv preprint arXiv:2507.16812, 2025.\nGuhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing\nthe mystery behind chain of thought: A theoretical perspective. Advances in Neural Information\nProcessing Systems, 36:70757–70798, 2023.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783, 2024.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a.\nYiduo Guo, Zhen Guo, Chuanwei Huang, Zi-Ang Wang, Zekai Zhang, Haofei Yu, Huishuai\nZhang, and Yikang Shen.\nSynthetic data rl: Task definition is all you need.\narXiv preprint\narXiv:2505.17063, 2025b.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Rat-\nner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.\nDistilling step-by-step! outperform-\ning larger language models with less training data and smaller model sizes.\narXiv preprint\narXiv:2305.02301, 2023.\nYinghao Hu, Leilei Gan, Wenyi Xiao, Kun Kuang, and Fei Wu. Fine-tuning large language models\nfor improving factuality in legal question answering. arXiv preprint arXiv:2501.06521, 2025.\nGiorgos Iacovides, Thanos Konstantinidis, Mingxue Xu, and Danilo Mandic. Finllama: Llm-based\nfinancial sentiment analysis for algorithmic trading. In Proceedings of the 5th ACM International\nConference on AI in Finance, pp. 134–141, 2024.\n11\n"}, {"page": 12, "text": "Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What dis-\nease does this patient have? a large-scale open domain question answering dataset from medical\nexams. Applied Sciences, 11(14):6421, 2021.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Prin-\nciples, pp. 611–626, 2023.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan Ferret,\nColton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement\nlearning from human feedback with ai feedback. 2023.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al. Retrieval-augmented gener-\nation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:\n9459–9474, 2020.\nMing Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi\nZhou, and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data\nselection for instruction tuning. arXiv preprint arXiv:2308.12032, 2023.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth\nInternational Conference on Learning Representations, 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. Advances in Neural Information Processing Systems, 36:46534–46594, 2023.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical domain question answering. In Conference on\nHealth, Inference, and Learning, pp. 248–260. PMLR, 2022.\nZeyu Qin, Qingxiu Dong, Xingxing Zhang, Li Dong, Xiaolong Huang, Ziyi Yang, Mahmoud\nKhademi, Dongdong Zhang, Hany Hassan Awadalla, Yi R Fung, et al. Scaling laws of synthetic\ndata for language models. arXiv preprint arXiv:2503.19551, 2025.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathemati-\ncal reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng,\nHaibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. In Proceedings\nof the Twentieth European Conference on Computer Systems, pp. 1279–1297, 2025.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou,\nKevin Clark, Stephen R Pfohl, Heather Cole-Lewis, et al. Toward expert-level medical question\nanswering with large language models. Nature Medicine, 31(3):943–950, 2025.\nXiaoshuai Song, Yanan Wu, Weixun Wang, Jiaheng Liu, Wenbo Su, and Bo Zheng. Progco: Pro-\ngram helps self-correction of large language models. arXiv preprint arXiv:2501.01264, 2025.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B Hashimoto.\nAlpaca:\nA strong, replicable instruction-\nfollowing model. Stanford Center for Research on Foundation Models. https://crfm. stanford.\nedu/2023/03/13/alpaca. html, 3(6):7, 2023.\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhu-\npatiraju, L´eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram´e, et al. Gemma\n2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024.\nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo:\nTuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975, 2023.\n12\n"}, {"page": 13, "text": "Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-\nattention distillation for task-agnostic compression of pre-trained transformers. Advances in Neu-\nral Information Processing Systems, 33:5776–5788, 2020.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824–24837, 2022.\nXiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng\nTao, and Tianyi Zhou. A survey on knowledge distillation of large language models. arXiv\npreprint arXiv:2402.13116, 2024.\nShangzi Xue, Zhenya Huang, Jiayu Liu, Xin Lin, Yuting Ning, Binbin Jin, Xin Li, and Qi Liu.\nDecompose, analyze and rethink: Solving intricate problems with human-like reasoning cycle.\nAdvances in Neural Information Processing Systems, 37:357–385, 2024.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, et al.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388, 2025.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. Ad-\nvances in Neural Information Processing Systems, 36:11809–11822, 2023.\nXiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Yipeng Zhang, Haitao Mi, and Helen Meng.\nSelf-tuning: Instructing llms to effectively acquire new knowledge through self-teaching. arXiv\npreprint arXiv:2406.06326, 2024.\nLucia Zheng, Neel Guha, Brandon R Anderson, Peter Henderson, and Daniel E Ho. When does pre-\ntraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal\nholdings. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and\nLaw, pp. 159–168, 2021.\nYaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and\nYongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv\npreprint arXiv:2403.13372, 2024.\nDenny Zhou, Nathanael Sch¨arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-\nmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex\nreasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\nYichi Zhou, Jianqiu Zhao, Yongxin Zhang, Bohan Wang, Siran Wang, Luoxin Chen, Jiahui Wang,\nHaowei Chen, Allan Jie, Xinbo Zhang, et al. Solving formal math problems by decomposition\nand iterative reflection. arXiv preprint arXiv:2507.15225, 2025.\n13\n"}, {"page": 14, "text": "Appendix\nAPPENDIX CONTENTS\nSection A\nAnswers Conflict Detection\n15\nSection B\nPrompts for Curriculum Generation\n15\nB.1 Prompt for Atomic Question Generation\n16\nB.2 Prompt for Verified CoT Curation\n16\nSection C\nFiltering Process of Atomic Q&A Pairs\n17\nSection D\nPrompt for Automated Atomic Knowledge Evaluation\n20\nSection E\nPrompt for Check and Rewrite the Reasoning Chains\n21\nSection F\nImplementation Details\n22\nF.1 Divergence Detection Stage\n22\nF.2 Curriculum Generation Stage\n22\nF.3 Student Adaptation Stage\n23\nF.4 Inference and Evaluation\n23\nSection G\nDetailed Curriculum Statistics\n23\nSection H\nEffectiveness of CoT Verification\n25\nSection I\nOrthogonality and Synergy with Reinforcement Learning\n25\nSection J\nAblation Study on Sampling Count\n26\nSection K\nQualitative Analysis\n26\nSection L\nData Contamination and Generalization Analysis\n28\nSection M\nComputational Cost Analysis\n30\nSection N\nLimitation\n31\nSection O\nThe Use of Large Language Models\n32\n14\n"}, {"page": 15, "text": "A\nANSWERS CONFLICT DETECTION\nThis section details the methodology used to determine whether a final answer from the student\nmodel conflicts with the corresponding answer from the teacher model.\nOur conflict detection process first checks for the simplest formats. For multiple-choice questions,\nwhere the answer is typically a single letter (e.g., A, B, C, D), we perform a direct string comparison\nafter extracting and normalizing the choices. If the answer is not a simple letter, we then check\nfor numerical or mathematical content. For these problems, we follow the standard convention of\nenclosing the final result within a \\boxed{} command and utilize the math verify library to\ncheck for logical equivalence. This tool robustly handles various mathematical forms, correctly\nidentifying expressions like \\frac{1}{2} and 0.5 as consistent.\nFor all other cases not covered by the above rules, such as questions requiring explanatory text,\nlogical deductions, or other free-form answers, we resort to an LLM-as-Judge approach. To maintain\na high level of domain understanding, we leverage the original teacher model itself as the impartial\njudge. The model is presented with the original problem, the teacher’s reference answer, and the\nstudent’s candidate answer, and is prompted using the template below to determine if the two are\nsemantically consistent.\nPrompt Template: Answer Conflict Detection\nRole and Task Description\nYou are a precise answer verifier. Your task is to determine if Answer A and Answer B are\nsemantically consistent and provide the same final conclusion for the given Question.\nRules\n• Context is Key: Your judgment must be based entirely on the context provided by the\nQuestion.\n• Semantic Equivalence: Consider two answers CONSISTENT if they mean the same\nthing, even if the wording is different. This includes synonyms, paraphrasing, and different\nbut equivalent numerical representations (e.g., ”50%”, ”0.5”).\nStrict Output Format\nYour output MUST be one of the following two words, and nothing else. Do not provide any\nexplanation or additional text.\n• If consistent, output ONLY the string: CONSISTENT\n• If inconsistent, output ONLY the string: INCONSISTENT\nQuestion: {Your Question Here}\nAnswer A: {Your Answer A Here}\nAnswer B: {Your Answer B Here}\nYour Output:\nThis tiered strategy allows us to use deterministic and precise conflict detection methods for struc-\ntured answer formats, while leveraging the nuanced semantic understanding of powerful language\nmodels for more complex, open-ended responses.\nB\nPROMPTS FOR CURRICULUM GENERATION\nThe Divergence-Guided Reasoning Curriculum (DGRC) framework relies on two core prompts dur-\ning the Curriculum Generation stage. These prompts are engineered to first diagnose the underlying\ncauses of reasoning divergence and then to verify the teacher model’s reasoning chains based on the\ninsights gained. Below, we provide the detailed structure and content of each prompt.\n15\n"}, {"page": 16, "text": "B.1\nPROMPT FOR ATOMIC QUESTION GENERATION\nThis prompt instructs the teacher model (MT ) to act as a diagnostician. It takes a problem (pi), the\nteacher’s own reasoning chain (ci\nT ), and the student’s divergent reasoning chain (ci\nS) as input. The\nobjective is to analyze both reasoning processes and synthesize a set of fundamental, self-contained\n“atomic questions” that pinpoint the precise factual or logical discrepancies leading to the different\nfinal answers.\nThe structure of the prompt is as follows:\nPrompt Template: Atomic Question Generation\n# Role\nAct as an expert in logical analysis. Your specialty is deconstructing and comparing complex\nreasoning processes to identify the fundamental points of divergence that lead to different\nconclusions.\n# Task\nI will provide you with an Original Question and two distinct Reasoning Processes (A and\nB) from two different models, which have resulted in conflicting answers. Your task is to\nperform a deep analysis of both reasoning chains, pinpoint the exact discrepancies between\nthem, and formulate these points of divergence into a set of Atomic Questions.\n# Core Requirements for Atomic Questions\nEach atomic question you generate must strictly adhere to the following rules:\n• Atomicity and Independence: Each question must be the smallest possible logical unit\nand be completely independent. A person should be able to answer any single question on\nits own without needing to read any others.\n• Focus on Discrepancy: Each question must target a specific, concrete point of disagree-\nment between the two reasoning processes (e.g., factual claims, logical steps, or underlying\nassumptions).\n• Self-Contained: If background context is necessary to understand the question, concisely\nembed that context within the question itself.\n• Verifiability: Each question must be phrased so that it can be answered definitively\nthrough factual verification, a clear logical judgment, or a straightforward calculation.\n# Input Format\n## Original Question: {Your Original Question Here}\n## Reasoning Process A: {Reasoning Process A Here}\n## Reasoning Process B: {Reasoning Process B Here}\n# Strict Output Format\nPlease output in the following list format. Do not output any extra symbols or thought\nprocesses.\n[\"Atomic Question 1\", \"Atomic Question 2\", ...]\n** NOTE **\n1. Do not solve the original question, just output the atomic questions.\n2. When outputting atomic questions, strictly follow the specified format.\nB.2\nPROMPT FOR VERIFIED COT CURATION\nThis prompt is used to filter the teacher’s original candidate reasoning chains (Oi\nT ). It leverages the\nset of high-confidence atomic question-answer pairs (Ai) generated in the previous step as a ground-\ntruth reference. The prompt instructs the teacher model to evaluate each of its own reasoning chains\n16\n"}, {"page": 17, "text": "(ci\nT,k) for consistency against every piece of atomic knowledge in Ai. Only chains that exhibit no\ncontradictions are considered verified.\nThe structure of the prompt is as follows:\nPrompt Template: Verified CoT Curation\n# Role\nYou are a meticulous Factual Consistency Verifier. Your task is to act as an impartial judge,\ndetermining if a given Reasoning Chain contains any statements that contradict a provided\nset of established Atomic Facts.\n# Task\nI will provide you with an Original Question, a Reasoning Chain to be evaluated, and a\nlist of Atomic Facts (in Q&A format) that are to be considered the absolute ground truth.\nYour sole task is to check for contradictions.\n# Core Rules for Verification\n• Atomic Facts are Ground Truth: The provided Atomic Q&A pairs are the definitive\nsource of truth for this task. The Reasoning Chain must be evaluated strictly against them.\n• Identify Contradictions: A contradiction exists if any part of the Reasoning Chain makes\na claim that is logically or factually inconsistent with any of the Atomic Facts. This in-\ncludes direct factual errors, flawed logical steps, or incorrect calculations that violate the\nprinciples established in the Atomic Facts.\n• Implicit vs. Explicit: The contradiction can be explicit (e.g., Reasoning says ”total is\n30%”, Fact says ”total is less than 30%”) or implicit (e.g., a calculation in the Reasoning\nviolates a principle explained in a Fact).\n# Input Format\n## Original Question: {Your Original Question Here}\n## Reasoning Chain to Verify: {Reasoning Chain to Verify Here}\n## Atomic Facts: {List of Atomic Q&As}\n# Strict Output Format\nYour output MUST be one of the following two words, and nothing else. Do not provide any\nexplanation.\n• If the Reasoning Chain is fully consistent with ALL Atomic Facts, output ONLY the string:\nCONSISTENT\n• If the Reasoning Chain contradicts ANY of the Atomic Facts, output ONLY the string:\nINCONSISTENT\nYour Output:\nC\nFILTERING PROCESS OF ATOMIC Q&A PAIRS\nTo ensure the quality, relevance, and diversity of the generated atomic knowledge, all raw question-\nanswer pairs synthesized by the teacher model undergo a rigorous three-stage filtering process before\nbeing aggregated into the final atomic curriculum. This process is designed to discard low-quality,\ntrivial, or redundant data.\nInstruction Following Difficulty (IFD) Filtering.\nThe first step filters out questions that are either\ntoo easy for the student model or show signs of misalignment. We use the Instruction Following\nDifficulty (IFD) (Li et al., 2023) as a proxy for this, calculated using the student model MS itself.\nThe IFD score is defined as the ratio between two intermediate scores: the conditioned answer score\nand the direct answer score.\n17\n"}, {"page": 18, "text": "For a given atomic Q&A pair (qatomic, aatomic), we define the conditioned answer score (scond) as the\nnegative log-likelihood of the answer given the question. This measures how well the model can\ngenerate the answer with the instruction’s context.\nscond = −log PMS(aatomic|qatomic)\n(3)\nThen, we define the direct answer score (sdirect) as the negative log-likelihood of the answer without\nthe question context. This gauges the inherent difficulty of generating the answer string itself.\nsdirect = −log PMS(aatomic)\n(4)\nThe final IFD score is the ratio of these two values:\nIFD(qatomic, aatomic) = scond\nsdirect\n(5)\nThe intuition behind this ratio is that it normalizes the difficulty of following an instruction scond by\nthe intrinsic complexity of producing the answer string itself sdirect. A low IFD score suggests that\nthe student model can already answer the question with high confidence, indicating limited learning\npotential because the sample is too simple. Conversely, a high IFD score (specifically, greater than\n1) suggests a misalignment between the question and answer, where the instruction provides no\nuseful context or is even counterproductive. We therefore discard all Q&A pairs whose IFD score\nfalls outside a predefined range [τlow, τhigh].\nSimilarity Filtering.\nFollowing IFD filtering, we apply similarity filtering to reduce redundancy\namong the atomic Q&A pairs generated from a single source problem. The core of this process is\nbased on the cosine similarity of sentence embeddings.\nFor each atomic question-answer pair, we concatenate its question qatomic and answer aatomic into\na single text sequence. We then use the lightweight sentence-embedding model, all-MiniLM-L6-\nv2 (Wang et al., 2020), to compute a single vector representation v for the pair. The cosine similarity\nbetween any two pairs is then calculated using the following formula:\nsim((qi,atomic, ai,atomic), (qj,atomic, aj,atomic)) = vi · vj\n|vi||vj|\n(6)\nwhere vi and vj are the respective embedding vectors. This calculation is the basis for both filtering\nstages that follow.\nWithin the group of Q&A pairs generated from one source problem, we conduct an all-to-all simi-\nlarity comparison. If the similarity score between any two pairs exceeds a threshold τsim, we apply\na deterministic filtering strategy to discard the pair with the less favorable IFD score (retaining the\none whose score is closer to 1).\nLLM-based Multi-dimensional Evaluation.\nThe final filtering step is a qualitative assessment\nperformed by the Teacher model (MT ). Only the pairs that have passed both IFD and similarity\nfilters are subjected to this step. Each Q&A pair is evaluated against seven criteria: clarity, com-\npleteness, structure, credibility, knowledge richness, logicality, and instruction following ability.\nThe total multi-dimensional score SLLM is calculated as the sum of these individual scores:\nSLLM =\nX\nc∈C\nsc\n(7)\nwhere C represents the set of all seven evaluation criteria and sc ∈{0, 1, 2} is the score for an\nindividual criterion. Only atomic Q&A pairs with a total score SLLM above a threshold τLLM are\nretained for the final atomic curriculum. The detailed prompt used for this evaluation is provided\nbelow.\nPrompt Template: Multi-dimensional Q&A Evaluation\n# Role\n18\n"}, {"page": 19, "text": "You are an expert evaluator. Your task is to provide rigorous, objective quality scores for a\nbatch of given atomic question-answer (Q&A) pairs.\n# Scoring Dimensions and Rubric\nFor each Q&A pair, you will provide a score from 0 to 2 for each of the following dimen-\nsions.\n1. Clarity (0-2): How clear and unambiguous is the Q&A?\n• 2: Perfectly clear, precise, and easy to understand.\n• 1: Mostly clear, but with minor ambiguity or awkward phrasing.\n• 0: Vague, confusing, or poorly worded.\n2. Completeness (0-2): Does the answer fully address the question?\n• 2: The answer is comprehensive and fully addresses the question.\n• 1: The answer addresses the main point but misses some nuances.\n• 0: The answer is incomplete or fails to address the question.\n3. Structure (0-2): Is the answer well-structured?\n• 2: The answer is well-organized and logically structured.\n• 1: The structure is acceptable but could be improved.\n• 0: The answer is unstructured or chaotic.\n4. Credibility (0-2): Is the answer factually correct and free of hallucinations?\n• 2: The answer is factually accurate and fully credible.\n• 1: The answer contains minor inaccuracies but is mostly correct.\n• 0: The answer is factually incorrect or contains clear hallucinations.\n5. Knowledge Richness (0-2): Does the answer provide sufficient, self-contained knowl-\nedge?\n• 2: The answer provides all necessary context and is richly informative.\n• 1: The answer is somewhat brief or lacking context.\n• 0: The answer is too simplistic and lacks useful information.\n6. Logicality (0-2): Is the answer logically sound?\n• 2: The answer’s internal logic is perfectly sound and coherent.\n• 1: The answer is mostly logical but contains minor flaws.\n• 0: The answer is illogical or contains significant reasoning errors.\n7. Instruction Following (0-2): Does the answer adhere to all instructions and constraints?\n• 2: The answer perfectly follows all explicit and implicit instructions.\n• 1: The answer mostly follows instructions but has minor deviations.\n• 0: The answer fails to follow key instructions or constraints.\n# Batch Input Format\nYou will receive a numbered list of Q&A pairs to evaluate.\n## Q&A Pair 1:\nQuestion:\nQuestion 1 Here\nAnswer:\nAnswer 1 Here\n## Q&A Pair 2:\nQuestion:\nQuestion 2 Here\nAnswer:\nAnswer 2 Here\n... and so on.\n# Strict Output Format\nYou must provide your evaluation as a JSON array, where each element is a dictionary of\nscores. The order of the JSON objects in the array MUST correspond to the order of the\nQ&A pairs in the input. Your output must be ONLY the JSON array and nothing else.\n19\n"}, {"page": 20, "text": "[\n{\n// Scores for Q&A Pair 1\n\"Clarity\": <0, 1, or 2>,\n\"Completeness\": <0, 1, or 2>,\n\"Structure\": <0, 1, or 2>,\n\"Credibility\": <0, 1, or 2>,\n\"Knowledge Richness\": <0, 1, or 2>,\n\"Logicality\": <0, 1, or 2>,\n\"Instruction Following\": <0, 1, or 2>\n},\n{\n// Scores for Q&A Pair 2\n\"Clarity\": <0, 1, or 2>,\n\"Completeness\": <0, 1, or 2>,\n\"Structure\": <0, 1, or 2>,\n\"Credibility\": <0, 1, or 2>,\n\"Knowledge Richness\": <0, 1, or 2>,\n\"Logicality\": <0, 1, or 2>,\n\"Instruction Following\": <0, 1, or 2>\n}\n]\nD\nPROMPT FOR AUTOMATED ATOMIC KNOWLEDGE EVALUATION\nTo validate the quality of the generated atomic curriculum on a larger scale, we employ Gemini-\n2.5-Pro as an automated judge. We randomly sample 2,000 atomic question-answer pairs from the\ngenerated curriculum. Since ground-truth answers for these synthesized atomic questions do not\nexist, the model was instructed to evaluate the correctness of the answer solely based on its own\nexpert internal knowledge. The specific prompt template used is provided below.\nPrompt Template: Automated Atomic Knowledge Evaluation\n# Role\nYou are an expert domain knowledge evaluator. Your task is to verify the correctness of an\n“Atomic Answer” generated for a specific “Atomic Question”.\n# Input Data\n## Atomic Question: {atomic question}\n## Atomic Answer: {atomic answer}\n# Evaluation Rules\nPlease evaluate the Atomic Answer based on the following strict rules. You must rely on\nyour own expert knowledge to determine validity.\n• Factual Accuracy: The answer must be factually correct according to established con-\nsensus in the domain (e.g., medical or legal standards). It must not contain hallucinations\nor false information.\n• Logical Soundness: The reasoning presented in the answer must be logically valid and\ncoherent.\n• Responsiveness: The answer must directly and precisely address the specific question\nasked, without being vague or evasive.\n# Judgment Process\n20\n"}, {"page": 21, "text": "1. Read the Atomic Question and Answer carefully.\n2. Verify the factual claims and reasoning against your internal expert knowledge base.\n3. Determine if the answer is a correct and valid response to the question.\n4. Provide a final binary verdict (”Valid” or ”Invalid”).\n# Strict Output Format\nOutput a valid JSON object with the following fields:\n{\n\"reasoning\": \"A brief explanation of why the answer is correct or\nincorrect.\",\n\"verdict\": \"Valid\" OR \"Invalid\"\n}\nE\nPROMPT FOR CHECK AND REWRITE THE REASONING CHAINS\nBelow is the prompt template used to instruct a model to act as a fact-checker and corrector in\nSection 4.3. The model is provided with an initial question, a corresponding reasoning, and a fi-\nnal prediction. Its task is to verify the reasoning and prediction against a set of provided atomic\nknowledge (contextual question-answer pairs) and, if necessary, correct any factual errors found.\nPrompt for Fact-Checking and Reasoning Correction\n# Task You will be provided with a question, a model’s prediction and its corresponding\nChain of Thought (CoT) reasoning, along with a set of contextual question-answer pairs.\nYour task is to act as a fact-checker and corrector. Carefully analyze the model’s CoT and its\nfinal prediction. Compare the information presented in the reasoning and prediction against\nthe provided “Context Questions” and “Context Answers” to identify any factual inaccura-\ncies.\n# Instructions\nReview\nScrutinize the Original CoT and Original Prediction for any statements that contradict the\nfacts established in the Context.\nDecision\nIf you find no factual errors, your output must be ONLY “<No>”.\nIf you identify any factual errors, you must correct them. Your “Corrected CoT” should\nbegin by following the “Original CoT” up to the point of the first factual error. At that\npoint, you must replace the incorrect statement with the accurate information from the atomic\nknowledge context and continue the reasoning process from there. The entire corrected\nreasoning process must not contradict the atomic context. Provide a “Corrected Prediction”\nbased on this new, accurate reasoning.\n# Input Format\n[Question Start] Initial question posed to the model [Question End]\n[Original CoT Start] Model’s step-by-step reasoning [Original CoT End]\n[Original Prediction Start]\nModel’s\nfinal\nanswer\n[Original\nPrediction End]\n[Context Question 1 Start] First contextual question [Context Question\n1 End]\n[Context Answer 1 Start]\nFirst\ncontextual\nanswer\n[Context Answer 1\nEnd]\n[Context Question 2 Start]\nSecond\ncontextual\nquestion\n[Context\nQuestion 2 End]\n21\n"}, {"page": 22, "text": "[Context Answer 2 Start] Second contextual answer [Context Answer 2\nEnd] ... and so on\n# Output Format\nIf there are no factual errors, ONLY output “<No>”. Do not output any thought process or\nother characters.\nIf factual errors are found, you must output a dictionary containing the corrections. The\noutput must be ONLY the dictionary and nothing else. The dictionary must be in the format\nas:\n{\n\"Corrected_CoT\": \"The revised, factually accurate step-by-step\nreasoning based on the context.\",\n\"Corrected_Prediction\": \"The new, correct final answer based on\nthe corrected reasoning.\"\n}\n# The data you need to process as follows: {prompt}\nF\nIMPLEMENTATION DETAILS\nF.1\nDIVERGENCE DETECTION STAGE\nIn the divergence detection stage, we identify reasoning gaps between a teacher model and a student\nmodel using 182,822 medical questions from the MedMCQA (Pal et al., 2022) training set and\n42,509 legal questions from CaseHOLD (Zheng et al., 2021) training set. For each problem, the\nteacher generates a single response (K = 1), while the student produces eight diverse responses\n(J = 8). The teacher’s response is paired with each of the student’s eight responses to diagnose\ndivergences.\nF.2\nCURRICULUM GENERATION STAGE\nAtomic Curriculum Generation.\nIn this stage, we generate high-quality answers for the atomic\nquestions. To enforce a consistent Chain-of-Thought format, each atomic question is appended\nwith\nthe\ninstruction\n“Please think carefully and then give the answer.\nThe output format is as follows:\n<think> your thinking process\n</think><answer> your answer </answer>” before being sent to the teacher\nmodel. The resulting atomic question-answer pairs then undergo a rigorous multi-stage filtering\nprocess as detailed in Appendix C. First, we filter based on Instruction Following Difficulty (IFD),\nretaining only pairs where the IFD score falls within the range of [τlow = 0.35, τhigh = 1.0]. Next,\na similarity filter is applied: for all atomic pairs from a single source question, we compute their\ncosine similarity, and if the score is 0.85 or higher (τsim = 0.85), we discard the pair with the\nless favorable IFD score. Finally, the remaining pairs are subjected to a quality filter based on\nthe teacher model’s multi-dimensional evaluation; any pair with a total score below a threshold of\nτLLM = 13 is discarded.\nVerified CoT Curriculum Generation.\nTo ensure the high quality of the teacher’s response to the\noriginal unlabeled question, we introduce a verification step using the generated atomic knowledge.\nFor each problem, we gather all corresponding atomic question-answer pairs that have passed the\nfiltering stages. From these pairs, we extract only the question and final answer to form a concise\nknowledge context, omitting any intermediate reasoning. This curated context is then provided to\nthe teacher model, which re-evaluates its original CoT response for logical consistency against the\nprovided facts. Only responses that demonstrate no logical conflicts with the atomic knowledge\nare retained. If multiple teacher responses for a single question pass this verification (a scenario\npossible when the teacher sampling count K > 1), we randomly select one for inclusion. This final\ncollection of responses constitutes the verified CoT curriculum for the student adaptation stage.\n22\n"}, {"page": 23, "text": "F.3\nSTUDENT ADAPTATION STAGE\nThe high-quality curricula generated in the previous stage are designed for adapting the student\nmodel, and can be used with standard Supervised Fine-Tuning (SFT) or advanced preference opti-\nmization methods like GRPO. For our main experiments, we employ a two-stage SFT process. First,\nthe student model undergoes full-parameter fine-tuning on the atomic curriculum for 3 epochs with\na learning rate of 1e-5. Subsequently, we continue full-parameter training on the verified CoT cur-\nriculum for another 3 epochs, using the same learning rate. To prevent performance degradation on\nsimpler problems, this training stage is augmented with “no-divergence” data, where the teacher’s\nsingle response was identical to all eight student responses. Detailed statistics on the data volumes\nfor each curriculum are provided in Appendix G. For the GRPO experiments detailed in Section 4.7\nand Appendix H, we adopt a different approach. The student model is initially supervised fine-tuned\non the atomic curriculum for 3 epochs (full-parameter, 1e-5 learning rate), followed by a warm-up\nSFT phase on the verified CoT curriculum for 1 epoch with the same settings. Finally, the model is\ntrained using GRPO for 900 steps. This phase utilizes a challenging subset of the verified CoT data\nfrom the medical domain, which is constructed using label-balanced sampling based on the final\nanswer within the teacher model’s CoT response (as opposed to the ground truth label) to include\n15,000 examples for each answer option. The GRPO training proceeds with a learning rate of 1e-6\nand a gradient accumulation strategy to achieve an effective batch size of 64. The SFT training\nis conducted using the LLaMA Factory framework (Zheng et al., 2024), while GRPO training is\nperformed with the VERL framework (Sheng et al., 2025). All models are trained with bfloat16 pre-\ncision. The setup is hardware-agnostic, and the training can be replicated on any system supporting\nthese frameworks and bfloat16 precision. To align with our goal of domain adaptation, the training\nprocesses for the medical and legal domains are conducted independently, with each model being\ntrained exclusively on its corresponding domain-specific curriculum.\nF.4\nINFERENCE AND EVALUATION\nInference Details.\nThroughout all stages of curriculum generation and evaluation, we adhere to a\nconsistent set of inference parameters. For closed-source models, we utilize their official APIs for\nall interactions. For open-source models, we leverage the vLLM framework (Kwon et al., 2023) for\nefficient inference and employ a standard sampling configuration with a temperature of 0.6, top-k of\n30, and top-p of 0.95. All inference is conducted with bfloat16 precision, and the maximum context\nlength is set to 4096 tokens.\nBenchmark Evaluation.\nTo facilitate answer extraction during benchmark evaluation, we uni-\nformly append the following instruction to each question: “Please think step-by-step\nand then give the final result.\nThe output format is as follows:\n<think> your thinking process </think><answer> The final answer\nshould be a single capital letter.\n</answer>” To ensure the reliability and\nstability of our results on public benchmarks, we mitigate potential scoring fluctuations by running\neach evaluation 10 times. The final reported performance is the average of these 10 runs.\nG\nDETAILED CURRICULUM STATISTICS\nThis section provides a detailed breakdown of the curricula generated for our main experiments,\nthe results of which are presented in Table 1 and Table 3. All statistics reported here correspond\nto our primary experimental configuration, using a teacher sampling count of K = 1 and a student\nsampling count of J = 8.\nThe data generation pipeline, summarized in Figure 2, begins with a set of unlabeled problems. For\neach problem, a problem is categorized as a divergent problem (# Div. Problems) if at least one of\nthe eight student responses conflicts with the teacher’s single response. Problems where all eight\nstudent responses are consistent with the teacher’s are considered “no-divergence” problems. The #\nDiv. Pairs column counts the total number of individual student-teacher response pairs that exhibit\na conflict across all divergent problems.\nFrom these divergent pairs, we generate an initial set of raw atomic questions (# Raw Atomic Q&A).\nThis set is then refined through our multi-stage filtering process to produce the final filtered atomic\n23\n"}, {"page": 24, "text": "Table 7: Detailed statistics of the curriculum generation pipeline across different domains and models.\nDomain\nTeacher Model\nStudent Model\n# Unlabeled\nProblems\n# Div.\nProblems\n# Div.\nPairs\n# Raw\nAtomic Q&A\n# Filtered\nAtomic Q&A\n# Verified\nCoTs\n# Total\nCoTs\nAvg. Tokens\n(Atomic)\nAvg. Tokens\n(CoT)\nMedical\nGPT-4.1\nQwen2.5-Instruct-7B\n182,822\n106,661\n485,719\n1,892,496\n413,886\n88,698\n164,859\n171.7\n188.6\nQwen2.5-Instruct-3B\n182,822\n155,357\n835,136\n3,674,964\n701,256\n121,441\n148,906\n166.3\n190.6\nQwen2.5-Instruct-1.5B\n182,822\n120,437\n810,860\n3,757,310\n560,943\n103,159\n165,544\n170.5\n189.5\nQwen2.5-Instruct-72B\nQwen2.5-Instruct-7B\n182,822\n104,623\n444,983\n3,074,152\n721,970\n72,856\n151,055\n169.0\n250.5\nQwen2.5-Instruct-3B\n182,822\n154,782\n820,373\n4,701,122\n868,242\n109,940\n137,980\n172.0\n256.3\nQwen2.5-Instruct-1.5B\n182,822\n118,554\n793,918\n4,712,338\n709,901\n80,080\n144,348\n169.9\n261.2\nQwen2.5-Instruct-32B\nQwen2.5-Instruct-7B\n182,822\n103,051\n437,653\n1,532,535\n330,136\n60,641\n140,412\n133.3\n256.1\nQwen2.5-Instruct-3B\n182,822\n154,631\n816,849\n2,999,135\n525,273\n92,813\n121,004\n131.8\n266.3\nQwen2.5-Instruct-1.5B\n182,822\n117,527\n786,991\n2,902,090\n394,323\n71,234\n136,529\n135.5\n254.4\nLegal\nGPT-4.1\nQwen2.5-Instruct-7B\n42,509\n23,631\n104,082\n405,648\n86,335\n16,058\n34,936\n178.1\n367.8\nQwen2.5-Instruct-3B\n42,509\n36,296\n201,928\n793,505\n133,715\n24,418\n30,631\n177.8\n354.6\nQwen2.5-Instruct-1.5B\n42,509\n28,234\n182,173\n741,298\n113,090\n19,378\n33,653\n181.6\n357.9\n24\n"}, {"page": 25, "text": "Table 8: Performance comparison to evaluate the effectiveness of the CoT verification step on the\nmedical domain. The student model is a 7B parameter model, and the teacher model is GPT-4.1. All\nmodels are first fine-tuned on the same atomic curriculum. Scores are reported as accuracy (%) on\nthree benchmarks and their average.\nTraining Method\nCoT Curriculum Type\nMedMCQA\nMedQA\nMMLU-M\nAverage\nSFT\nOriginal CoT (w/o Verification)\n66.1\n70.1\n89.3\n75.2\nVerified CoT (Ours)\n67.5\n72.8\n90.9\n77.1\nGRPO\nOriginal CoT (w/o Verification)\n67.8\n74.3\n91.6\n77.9\nVerified CoT (Ours)\n69.8\n75.7\n91.8\n79.1\ncurriculum (# Filtered Atomic Q&A). Concurrently, the teacher’s original responses for the diver-\ngent problems undergo verification, resulting in a set of verified CoTs (# Verified CoTs). The total\nCoT we used (# Total CoTs) is then formed by combining these verified CoTs with the single CoT\nfrom each no-divergence problem. Finally, we report the average token lengths for the two curricula.\nH\nEFFECTIVENESS OF COT VERIFICATION\nTo isolate and quantify the benefit of our Chain-of-Thought (CoT) verification step, we conduct a\ntargeted ablation study. We compare the performance of student models trained on two different\nCoT curricula, subsequent to an initial training phase on the same atomic curriculum. The baseline\nmodel is trained using the teacher’s original CoT responses, which have not undergone our verifica-\ntion process. In contrast, our proposed model is trained using the verified CoT curriculum, where\nresponses with logical inconsistencies have been filtered out. We evaluate this comparison under\ntwo distinct training paradigms to demonstrate the robustness of our approach: SFT and GRPO.\nThe results, presented in Table 8, demonstrate that the CoT verification step provides a performance\nboost. In both the SFT and GRPO settings, the student model trained on the verified curriculum\nconsistently outperforms the baseline model trained on the original unverified CoT. This confirms\nthat eliminating flawed reasoning chains from the teacher’s responses is crucial for generating a\nhigher-quality training signal for the student model.\nI\nORTHOGONALITY AND SYNERGY WITH REINFORCEMENT LEARNING\nIn Table 1, we compared DGRC against a Reinforcement Learning from GPT-4.1 Feedback (RLAIF)\nbaseline implemented via Group Relative Policy Optimization (GRPO). While this comparison es-\ntablishes DGRC’s competitiveness, it is crucial to clarify that DGRC and RLAIF are not mutually\nexclusive alternatives but rather orthogonal and complementary components of the LLM alignment\npipeline.\nConceptual Orthogonality.\nFundamentally, DGRC and RLAIF operate at distinct yet comple-\nmentary stages of the alignment pipeline. DGRC functions primarily as a data synthesis engine\non the input side, leveraging the cognitive asymmetry to synthesize high-quality curricula from\nunlabeled queries. Its core contribution lies in filtering the noise and hallucinations inherent in self-\ngenerated data to produce reliable supervision. In contrast, RLAIF represents a training paradigm on\nthe optimization side, which presupposes the existence of a reward signal and focuses on optimizing\nthe model’s policy to maximize that reward. Thus, rather than competing, the two approaches form\na cohesive system where DGRC provides the high-quality reasoning paths, while RLAIF utilizes\nthese paths for effective policy optimization.\nDGRC as a Superior Foundation for Low-Resource Models.\nOur results in Table 1 demonstrate\nthat DGRC consistently outperforms the RLAIF baseline in lower-parameter regimes (e.g., +2.6%\nover RLAIF for the 1.5B model). This finding highlights a critical insight: For smaller models with\nlimited reasoning capabilities, explicit instruction is more effective than implicit feedback. RLAIF\nrelies on the model to self-explore and discover correct reasoning paths based on sparse reward\nsignals, a task that is often too challenging for smaller models with limited search space. In contrast,\n25\n"}, {"page": 26, "text": "Table 9: Ablation study on teacher (K) and student (J) sampling counts. The student model is\nQwen2.5-Instruct-1.5B fine-tuned on a curriculum generated by GPT-4.1. Values represent the av-\nerage accuracy (%) across three medical benchmarks: MedMCQA, MedQA, and MMLU-Medical.\nK\nJ\n1\n2\n4\n8\n1\n53.5\n55.0\n56.8\n58.3\n2\n55.8\n55.7\n57.2\n58.5\n3\n56.4\n58.1\n58.6\n59.2\nDGRC decomposes complex problems into atomic steps, providing dense and explicit supervision\nthat acts as a scaffold, making it significantly easier for these models to acquire domain knowledge.\nSynergistic Potential.\nThe true potential lies in combining these two approaches, as evidenced\nby our ablation studies across Section 4.7 and Appendix H. First, DGRC serves as a powerful\n“warm-up” for RL: applying GRPO on top of the DGRC-adapted model yields further improve-\nments, pushing the 7B model from 77.1% to 79.1%, as illustrated in Table 6. Second, the Atomic\nCurriculum specifically optimizes the starting policy for RL: as shown in Table 8, fine-tuning on\natomic questions before applying GRPO outperforms the direct RLAIF baseline by 1.7% (77.9%\nvs. 76.2% in Table 1). Finally, the quality of the training data matters: using DGRC’s Verified CoT\nCurriculum for GRPO training provides a clear advantage over using unverified teacher responses,\nfurther driving performance from 77.9% to 79.1%, as shown in Table 8. This confirms that DGRC\nand RLAIF can be pipelined effectively: DGRC first establishes a robust reasoning foundation and\ncleans the data, creating a high-quality policy that allows subsequent RLAIF stages to focus on\nrefining complex reasoning paths.\nJ\nABLATION STUDY ON SAMPLING COUNT\nWe conduct an ablation study to analyze the impact of the teacher sampling count (K) and the\nstudent sampling count (J). Using the GPT-4.1 teacher and Qwen2.5-Instruct-1.5B student on a\n10,000-question subset from the MedMCQA training data, we experiment with K ∈{1, 2, 3} and\nJ ∈{1, 2, 4, 8}. The results are presented in Table 9, revealing two clear trends. First, increasing\nthe student sampling count J consistently improves performance. A higher J increases the prob-\nability of exposing latent reasoning deficiencies in the student model, which in turn allows for the\ngeneration of more targeted atomic knowledge for its correction. Second, a higher teacher sampling\ncount K also leads to better results. This is because a larger sample pool increases the likelihood\nof obtaining a high-quality CoT consistent with the atomic knowledge, thereby enriching the final\nverified CoT curriculum. The peak performance is achieved at K = 3 and J = 8, confirming the\nbenefits of enriching the verified CoT curriculum (via K) while simultaneously improving the tar-\ngeting of the atomic curriculum (via J). For our main experiments, we set J = 8 to maximize the\ndiscovery of student weaknesses. However, we use K = 1 primarily to ensure high experimental\nefficiency.\nK\nQUALITATIVE ANALYSIS\nThis section presents a qualitative analysis to demonstrate the effectiveness of our atomic questions.\nWe analyze examples from the MedMCQA validation set where the student model makes errors,\nvisualizing its internal uncertainty and aligning it with the corresponding atomic question designed\nto address the error.\nEntropy as a Measure of Model Uncertainty.\nTo visualize the student model’s token-level un-\ncertainty, we use Shannon Entropy. Entropy quantifies the uncertainty in the model’s predictive\ndistribution for the next token: a higher value signifies greater confusion, while a lower value sug-\ngests higher confidence. For a predictive distribution P over the vocabulary V , the entropy H is\ncalculated as:\n26\n"}, {"page": 27, "text": "H(P) = −\nX\nw∈V\nP(w) log2 P(w)\n(8)\nwhere P(w) is the probability of token w. In our visualizations (Figure 3), higher token entropy\nis represented by a more intense background color, visually highlighting where the model is most\nuncertain.\nSourcing Atomic Questions for Analysis.\nOur qualitative analysis relies on source attribution to\nlink each atomic question to its origin in the student’s reasoning, allowing for an evaluation of its\nrelevance and quality. We achieve this using a specialized prompt that instructs the teacher model to\nboth formulate focused atomic questions based on the reasoning divergency and extract the precise\nsource sentence containing the core misconception from the student’s response. The prompt we used\nis as follows:\nAtomic Question Generation with Source Attribution\n# Role and Task\nAct as an expert in logical analysis specializing in deconstructing and comparing complex\nreasoning processes. You will be provided with an Original Question and two distinct Rea-\nsoning Processes (A and B) that have resulted in conflicting answers. Your task is to perform\na comprehensive analysis of both reasoning chains, identify all significant logical discrepan-\ncies, and formulate an atomic question for each distinct point of divergence.\n# Instructions\nYour goal is to generate a complete list of atomic questions that covers every fundamental\nconflict between the two reasoning processes. Each question and its attributed sources must\nstrictly adhere to the following rules:\n• Atomicity and Independence: Each question must be the smallest possible logical\nunit and be completely independent of all other questions.\n• Focus on Discrepancy: Each question must target a specific, concrete point of\ndisagreement between the two reasoning processes.\n• Self-Contained: If background context is necessary to understand the question,\nconcisely embed that context within the question itself.\n• Verifiability: Each question must be phrased in a way that it can be answered\ndefinitively through factual verification, a clear logical judgment, or a straightfor-\nward calculation.\n• Bilateral Source Attribution: For each atomic question, you must locate and ex-\ntract the corresponding critical text snippets from both Reasoning Process A and\nReasoning Process B. These two snippets should clearly represent the direct point\nof conflict.\n# Input Format\nOriginal Question:\n{problem}\nReasoning Process A: {cot1}\nReasoning Process B: {cot2}\n# Output Format\nYou must strictly output a JSON array of objects and nothing else. The array should contain\none object for each distinct discrepancy you identify. Each object must have three keys:\n\"atomic question\", \"source from A\", and \"source from B\".\nExample format:\n[\n{\n\"atomic_question\": \"Content of the first atomic question.\",\n\"source_from_A\": \"The specific text snippet from Reasoning\nProcess A that is directly related to the point of\ndiscrepancy.\",\n27\n"}, {"page": 28, "text": "\"source_from_B\": \"The specific text snippet from Reasoning\nProcess B that is directly related to the point of\ndiscrepancy.\"\n},\n{\n\"atomic_question\": \"Content of the second atomic question.\",\n\"source_from_A\": \"Another text snippet from Reasoning Process A\nrelated to the second point of discrepancy.\",\n\"source_from_B\": \"Another text snippet from Reasoning Process B\nrelated to the second point of discrepancy.\"\n}\n]\n# Final Constraints\n• Do not solve the original question; your task is only to generate the atomic questions\nfor comparison.\n• Strictly adhere to the JSON array format. Do not output any extra explanations or\nthought processes.\n• Ensure the value for the \"atomic question\" key fulfills all the core require-\nments.\n• Ensure the values for the \"source from A\" and \"source from B\" keys are\nprecise, concise, direct quotes that clearly showcase the conflict.\nAnalysis.\nBy analyzing the qualitative examples presented in Figure 3, we derive two key insights.\nFirst, we observe that the student model’s high-entropy regions are not randomly distributed but are\nconcentrated in its reasoning about both the incorrect answer it selected and the correct ground-\ntruth answer. This indicates more than a simple mistake; it points to a deeper conceptual confusion\nwhere the model is most uncertain precisely when it has to discriminate between critical, competing\nconcepts. This finding suggests that high entropy is a indicator of a model’s specific knowledge gaps.\nSecond, we find that the source sentences identified by our method for generating atomic questions\nconsistently exhibit high entropy. This demonstrates that our atomic questions are precisely targeted\nat the model’s points of maximum uncertainty and error. This validates the efficacy of our atomic\ncurriculum, showing it functions as a targeted intervention tool. By focusing directly on diagnosed\nweaknesses, the curriculum provides a foundation for an efficient and focused method of model\nenhancement, addressing the very concepts the model struggles with the most.\nL\nDATA CONTAMINATION AND GENERALIZATION ANALYSIS\nA critical concern in post-training adaptation is whether performance gains stem from genuine rea-\nsoning improvements or simply from data contamination (i.e., the model memorizing benchmarks\nseen during pre-training or leaked from the teacher). To address this, we analyze contamination risks\nfrom both the teacher and student perspectives.\nTeacher-Side Contamination and OOD Generalization.\nA primary risk involves the powerful\nteacher models (e.g., GPT-4.1) potentially having the test benchmarks in their pre-training corpus,\nthereby leaking ground-truth information into the generated curriculum. To rigorously assess this,\nour experimental design strictly enforces a cross-dataset evaluation setup, ensuring that the source\ntraining data is distinct from the evaluation benchmarks used to measure generalization.\n• Medical Domain: The curriculum is generated exclusively from the MedMCQA training set. We\nthen evaluate generalization on MedQA and MMLU-Medical, which serve as Out-Of-Distribution\n(OOD) benchmarks.\n• Legal Domain: The curriculum is generated exclusively from CaseHOLD. We evaluate general-\nization on MMLU-Law, an OOD benchmark.\n28\n"}, {"page": 29, "text": "(a)\n(b)\nFigure 3: Qualitative examples of student model errors. Token-level entropy is visualized by color\nintensity. The generated atomic question targets the specific reasoning error, whose source sentence\nis highlighted in red, which consistently aligns with a high-entropy region.\nTable 10 presents the performance on these OOD benchmarks. We observe consistent and substantial\nperformance gains across all metrics. For instance, the 1.5B model achieves an average improvement\nof +4.9% over the baseline on OOD tasks. This strong OOD generalization effectively rules out\nsimple teacher-side data leakage as the primary driver of performance; instead, it indicates that\nDGRC successfully instills transferable atomic knowledge and reasoning capabilities that generalize\nto unseen tasks.\nStudent-Side Contamination Risks.\nSince the student models (Qwen2.5 series) are open-source\nand trained on massive web corpora, there is an inherent risk that they may have encountered the test\nbenchmarks during their pre-training phase. However, as detailed in Table 10, our DGRC method\nyields dramatic performance improvements over the original Zero-shot models (e.g., +11.6% for\n29\n"}, {"page": 30, "text": "Table 10: Analysis of Out-Of-Distribution (OOD) generalization and Student-Side contamination\nrisks. We compare the original open-source model (Zero-shot), the distillation baseline, and our\nDGRC method on three OOD benchmarks. Avg OOD represents the mean accuracy across these\nunseen tasks. The consistent improvement over the Zero-shot baseline confirms that DGRC en-\nhances reasoning beyond potential pre-training memorization.\nModel\nMethod\nMedQA\nMMLU-Med\nMMLU-Law\nAvg OOD\nGain vs. Zero-shot\nQwen2.5-1.5B\nZero-shot (Original)\n40.1\n54.8\n61.2\n52.0\n-\nBaseline-w/o-label\n48.3\n65.0\n62.8\n58.7\n+6.7\nDGRC (Ours)\n50.8\n72.2\n67.8\n63.6\n+11.6\nQwen2.5-3B\nZero-shot (Original)\n41.6\n63.7\n61.2\n55.5\n-\nBaseline-w/o-label\n58.4\n73.5\n69.4\n67.1\n+11.6\nDGRC (Ours)\n60.6\n81.0\n72.7\n71.4\n+15.9\nQwen2.5-7B\nZero-shot (Original)\n59.8\n86.6\n75.2\n73.9\n-\nBaseline-w/o-label\n69.4\n87.2\n78.5\n78.4\n+4.5\nDGRC (Ours)\n72.8\n90.9\n79.3\n81.0\n+7.1\n1.5B and +15.9% for 3B). If the student models were merely recalling memorized answers from\npre-training contamination, we would expect the Zero-shot performance to be closer to the adapted\nperformance, or for the adaptation to yield diminishing returns. Instead, the significant leap in ac-\ncuracy demonstrates that DGRC is not simply unlocking memorized data, but is actively construct-\ning new, domain-specific reasoning pathways that the base models originally lacked. Furthermore,\nDGRC consistently outperforms the Baseline-w/o-label, confirming that our curriculum-based ap-\nproach is superior to standard distillation in leveraging these new pathways.\nM\nCOMPUTATIONAL COST ANALYSIS\nTo rigorously quantify the resource implications of our method, we present a comparative analysis\nbetween DGRC and the standard unlabeled distillation baseline (Baseline-w/o-label). This analysis\nis based on the experimental statistics from the medical domain presented in Table 7, specifically\nfocusing on the configuration where GPT-4.1 serves as the teacher and Qwen2.5-Instruct-7B as the\nstudent (N = 182, 822 unlabeled problems). We detail the token consumption across the pipeline,\naccounting for our batching and hierarchical filtering strategies.\nBreakdown of Resource Consumption.\n• Offline Generation Phase (One-time Cost):\n– Diagnosis & Decomposition: For the divergent pairs (approx. 2.6 pairs per problem), the\nteacher generates atomic questions in a single batch. This amortizes the input context cost,\nmaking the process efficient.\n– Batch Answering: We instruct the teacher to answer all generated atomic questions for a\nproblem in a single inference pass, significantly reducing the overhead compared to answer-\ning them individually.\n– Hierarchical Filtering: We employ a “funnel” strategy to minimize the use of the expensive\nLLM judge. Low-cost filters (IFD calculation by the small student model and embedding-\nbased similarity) are applied first to discard trivial or redundant pairs. Only the high-quality\ncandidates (∼50% of raw pairs) are sent to the teacher for multi-dimensional scoring, saving\nsubstantial compute.\n– Overall: While the teacher inference load is roughly 6× that of the baseline, this cost is\nincurred only once during dataset creation and is negligible compared to pre-training costs.\n• Online Training Phase: The training overhead is moderate and linear. The DGRC curriculum\n(Atomic + Verified CoT) contains roughly 2.5× the number of tokens compared to the baseline\ndataset. Consequently, training takes approximately 2.5× longer. However, unlike RL-based\nmethods which require complex infrastructure and multiple models in memory, DGRC utilizes\nstandard SFT, keeping the GPU memory footprint identical to the baseline.\nCost-Performance Trade-off.\nFor resource-constrained scenarios, DGRC offers flexible deploy-\nment options:\n30\n"}, {"page": 31, "text": "Table 11: Computational cost vs. performance comparison. Calculations are normalized per unla-\nbeled problem. Token counts are estimated averages: Problem Context ≈200, CoT ≈400, Atomic\nQA Pair ≈100. DGRC uses K = 1, J = 8. Note that the generation cost is a one-time offline\ninvestment.\nPhase\nMetric\nBaseline (Distillation)\nDGRC (Standard, J = 8)\nOffline\nGeneration\nTeacher Inference (Tokens)\n∼400 (1 Response)\n∼2, 500 (Gen + Diag + Batch Ans + Ver)\nStudent Inference (Tokens)\n0\n∼3, 200 (8 Responses)\nPrimary Cost Driver\nSingle Generation\nGeneration & Diagnosis & LLM Filtering Judge\nCost Multiplier\n1.0×\n≈6.25× (Teacher)+ 8× (Student)\nOnline\nTraining\nDataset Size (Instances)\n∼182k CoTs\n∼413k Atomic + ∼88k Verified CoTs\nTotal Training Tokens\n∼72M\n∼180M\nRelative Compute\n1.0×\n≈2.5×\nOutcome\nAvg Accuracy\n73.7%\n77.1%\n• DGRC-Standard (J = 8): Used in our main experiments to maximize performance by uncover-\ning diverse reasoning gaps. Suitable for high-stakes domains like medicine.\n• DGRC-Lite (J = 2 or 4): Reducing the student sampling count linearly decreases the diagnosis\ncost. As indicated in Appendix J, even J = 2 yields significant improvements, offering a balanced\nsolution that cuts the generation cost by roughly half while retaining most of the performance\ngains.\nN\nLIMITATION\nWhile our DGRC framework demonstrates significant promise, we acknowledge two primary limi-\ntations that warrant further investigation.\nDependence on Teacher Model Capability.\nThe efficacy of DGRC is highly dependent on the\ncapability gap between the teacher and student models. As shown in experiment in Section 4.4,\nthe largest performance gains are achieved when a state-of-the-art model like GPT-4.1 serves as the\nteacher, and there is a clear trend where stronger teachers yield greater improvements. This reliance\nis further highlighted in the self-teaching setting in Section 4.5, where the more capable 32B model\nimproves notably more than the 7B model because the weaker model’s ability to act as a reliable\n“diagnostician” becomes a significant bottleneck. This suggests that the practical applicability of\nDGRC may be constrained in scenarios where access to significantly more powerful teacher models\nis limited or cost-prohibitive.\nRisk of Shared Blind Spots.\nA second limitation is the risk of “shared blind spots”, where both\nthe teacher and student models make the same error. Since our framework is triggered by reasoning\ndivergence, such cases of shared error will go undetected, preventing the generation of a corrective\nlesson. To address this, we see significant potential in integrating DGRC with external knowledge\nbases (i.e., Retrieval-Augmented Generation Lewis et al. (2020)). We envision a hybrid detection\nmechanism: for instances where the teacher and student reach a consensus (no divergence), the\nsystem could retrieve relevant evidence from a reliable external domain corpus. If a conflict arises\nbetween the models’ consensus and the retrieved knowledge, it would flag a potential shared blind\nspot, allowing the external evidence to serve as the ground truth for generating corrective atomic\ncurricula. Conversely, alignment with retrieved knowledge would further validate the reliability of\nthe consensus.\nContextualizing the Limitations.\nIt is important, however, to contextualize these limitations. De-\npendence on a capable teacher is an inherent characteristic of most knowledge distillation frame-\nworks, as well as methods that synthesize data from external knowledge bases. Separately, while\nthe challenge of shared blind spots is a common hurdle for all approaches that operate in unlabeled\nsettings, our proposed integration with external knowledge bases offers a concrete pathway to mit-\nigate this issue. Furthermore, while performance is maximized with a state-of-the-art proprietary\nmodel, our results demonstrate that DGRC remains highly effective when using strong open-source\nmodels as teachers. As shown in Table 3, models like Qwen2.5-Instruct-72B still yield substan-\n31\n"}, {"page": 32, "text": "tial improvements over the baseline, highlighting the framework’s practical value and potential for\nbroader application in various research contexts.\nO\nTHE USE OF LARGE LANGUAGE MODELS\nThe authors acknowledge the use of a large language model (LLM) as a writing assistant during the\npreparation of this manuscript. Its application was strictly confined to language enhancement tasks,\nsuch as improving the text’s clarity, conciseness, and grammatical correctness. The core scientific\ncontributions, including the conceptualization of the DGRC framework, the experimental design,\nand the analysis of the results, are the original work of the authors. All suggestions provided by the\nLLM were critically reviewed, edited, and approved by the authors, who take full responsibility for\nthe final content of this paper.\n32\n"}]}