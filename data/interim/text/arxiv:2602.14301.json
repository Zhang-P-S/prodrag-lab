{"doc_id": "arxiv:2602.14301", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.14301.pdf", "meta": {"doc_id": "arxiv:2602.14301", "source": "arxiv", "arxiv_id": "2602.14301", "title": "DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices", "authors": ["Songyuan Li", "Jia Hu", "Ahmed M. Abdelmoniem", "Geyong Min", "Haojun Huang", "Jiwei Huang"], "published": "2026-02-15T20:25:50Z", "updated": "2026-02-15T20:25:50Z", "summary": "Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.14301v1", "url_pdf": "https://arxiv.org/pdf/2602.14301.pdf", "meta_path": "data/raw/arxiv/meta/2602.14301.json", "sha256": "ef29f8c0f1b8ac36ff532bf77de8405e5bf2f46ebfe43448aef90ba493c1c294", "status": "ok", "fetched_at": "2026-02-18T02:19:15.487886+00:00"}, "pages": [{"page": 1, "text": "1\nDEEPFUSION: Accelerating MoE Training via Federated\nKnowledge Distillation from Heterogeneous Edge Devices\nSongyuan Li ∗, Jia Hu, Ahmed M. Abdelmoniem, Geyong Min, Haojun Huang, and Jiwei Huang\nAbstract—Recent Mixture-of-Experts (MoE)-based large lan-\nguage models (LLMs) such as Qwen-MoE and DeepSeek-MoE\nare transforming generative AI in natural language processing.\nHowever, these models require vast and diverse training data.\nFederated learning (FL) addresses this challenge by leverag-\ning private data from heterogeneous edge devices for privacy-\npreserving MoE training. Nonetheless, traditional FL approaches\nrequire devices to host local MoE models, which is impractical for\nresource-constrained devices due to large model sizes. To address\nthis, we propose DEEPFUSION, the ﬁrst scalable federated MoE\ntraining framework that enables the fusion of heterogeneous\non-device LLM knowledge via federated knowledge distillation,\nyielding a knowledge-abundant global MoE model. Speciﬁcally,\nDEEPFUSION features each device to independently conﬁgure\nand train an on-device LLM tailored to its own needs and\nhardware limitations. Furthermore, we propose a novel View-\nAligned Attention (VAA) module that integrates multi-stage\nfeature representations from the global MoE model to construct\na predictive perspective aligned with on-device LLMs, thereby\nenabling effective cross-architecture knowledge distillation. By\nexplicitly aligning predictive perspectives, VAA resolves the view-\nmismatch problem in traditional federated knowledge distillation,\nwhich arises from heterogeneity in model architectures and pre-\ndiction behaviors between on-device LLMs and the global MoE\nmodel. Experiments with industry-level MoE models (Qwen-\nMoE and DeepSeek-MoE) and real-world datasets (medical and\nﬁnance) demonstrate that DEEPFUSION achieves performance\nclose to centralized MoE training. Compared with key federated\nMoE baselines, DEEPFUSION reduces communication costs by\nup to 71% and improves token perplexity by up to 5.28%.\nIndex Terms—Large language models, mixture-of-experts, fed-\nerated knowledge distillation, edge device heterogeneity.\nI. INTRODUCTION\nWe have been witnessing rapid advancements in large\nlanguage models (LLMs) [1]–[3], whose transformer-based\narchitectures and vast text training have revolutionized gen-\nerative AI for understanding and generating human lan-\nguage in natural language processing (NLP). Building on\nthis momentum, LLMs are driving further advances in NLP\nSongyuan\nLi\nand\nAhmed\nM.\nAbdelmoniem\nare\nwith\nthe\nSchool\nof Electronic\nEngineering\nand Computer Science,\nQueen\nMary Uni-\nversity of London, London E1 4NS, U.K. (e-mail: S.Y.Li@qmul.ac.uk;\nahmed.sayed@qmul.ac.uk).\nJia Hu, and Geyong Min are with the Department of Computer Science,\nFaculty of Environment, Science and Economy, University of Exeter, Exeter\nEX4 4PY, U.K. (e-mail: J.Hu@exeter.ac.uk; G.Min@exeter.ac.uk).\nHaojun Huang is with the School of Electronic Information and Commu-\nnications, Huazhong University of Science and Technology, Wuhan 430074,\nChina (e-mail: hjhuang@hust.edu.cn).\nJiwei Huang is with the Beijing Key Laboratory of Petroleum Data\nMining, China University of Petroleum, Beijing 102249, China (e-mail:\nhuangjw@cup.edu.cn).\n* This work was done primarily while Songyuan Li was with the Depart-\nment of Computer Science, University of Exeter, U.K.\nperformance by embracing novel Mixture-of-Experts (MoE)\narchitectures [4]. The MoE-based LLM employs a unique\narchitecture of sparsely activated expert subnetworks, each\nspecializing in distinct domain knowledge to handle diverse\ninput tasks efﬁciently. Through a gating mechanism, each input\nis dynamically routed to the relevant expert subnetwork(s),\nallowing for customized processing and optimal performance.\nThis MoE architecture massively scales model parameters\nto tens or hundreds of billions (e.g., Qwen3-MoE 235B [5]\nand DeepSeek-V2 236B [6]). As a result, the MoE-based\nLLMs require much larger and more diverse training data\nto ensure that all expert subnetworks are exposed to diverse\nand extensive training samples [7], thereby fully leveraging\nthe scaling beneﬁts of MoE. This surging demand exacerbates\nconcerns about the imminent shortage of high-quality public\ntext data for LLM training, as recent projections warn that\npublic human text data for training could be depleted between\n2026 and 2032 [8].\nFederated learning (FL) addresses this challenge by sourc-\ning extensive and diverse training data from ubiquitous edge\ndevices for MoE training, while preserving data privacy. Local\nmodel training occurs at distributed edge devices (e.g., smart-\nphones and embedded devices) to obtain an optimal global\nmodel through aggregation, without centralizing private on-\ndevice data. Traditional FL methods, represented by FedAvg\n[9], restrict on-device training to a local copy of the global\nmodel, ensuring the same model architecture and size across\ndevices to enable element-wise averaging at the central aggre-\ngation server. Nonetheless, standard FedAvg-like approaches\nare impractical to implement federated MoE training, as\nresource-constrained edge devices usually cannot accommo-\ndate a full local copy of the global MoE model. To tackle this\nissue, recent studies [10]–[13] have made some attempts in\nfederated MoE training. Speciﬁcally, these works introduce\na small local expert model deployed at each edge device.\nThe local expert model, which is essentially a compact MoE\nnetwork pruned from the global MoE, shares its backbone\nMoE architecture and remains structurally compatible with the\nglobal model. Consequently, local expert models can be easily\nmerged back into the global MoE model by aligning their\ncomponents with those of the global model and assembling\nthem to form the complete global MoE architecture.\nHowever, these studies [10]–[13] rely on an impractical\nassumption that all participating edge devices have sufﬁcient\nhardware capacity to operate the assigned local expert model.\nIn practice, although these local expert models are much\nsmaller than the global MoE, they remain too cumbersome\nfor edge devices with limited resources, as they still retain the\narXiv:2602.14301v1  [cs.LG]  15 Feb 2026\n"}, {"page": 2, "text": "2\ncomplex MoE backbone and expert gating mechanisms. For\ninstance, smartphones (e.g., iPhone 16) and embedded devices\n(e.g., NVIDIA Jetson Nano) are popular Artiﬁcial Intelli-\ngence of Things (AIoT) hardware for on-device LLMs [14].\nHowever, embedded devices with low-power CPUs/GPUs and\nlimited SRAM/storage have lower AI capabilities compared to\nsmartphones with advanced GPUs/NPUs [15]. Consequently,\nthe local expert model could run smoothly on smartphones\nbut often struggles on resource-constrained embedded devices,\nlimiting the applicability of current federated MoE training\nmethods. Given the widespread yet diversiﬁed edge devices,\nit is crucial to enable broad device participation in federated\nMoE training. This exposes the large-scale MoE model to\ndiverse and extensive training samples, enriching each expert\nsubnetwork’s domain knowledge and ultimately enhancing\noverall MoE performance across varied input tasks.\nTherefore, we propose DEEPFUSION, a novel federated\nMoE training framework based on knowledge distillation.\nIt enables ubiquitous resource-constrained edge devices with\nheterogeneous training data to participate in federated MoE\ntraining. Distinguished from prior research, DEEPFUSION\nallows each edge device to independently conﬁgure its on-\ndevice LLMs based on local application requirements and\nresource constraints. These on-device LLMs are trained on\nlocal application data to achieve optimal performance, and\nserve as repositories of local AI knowledge for transfer into the\nglobal MoE model. To address FL communication overhead,\nDEEPFUSION adopts a communication-efﬁcient FL setting.\nOnce local training is sufﬁcient, the trained LLMs are up-\nloaded in a single round to the central FL server for knowledge\ntransfer. The central server clusters the collected on-device\nLLMs into local knowledge domains, and integrates them into\nthe global MoE model through federated knowledge distilla-\ntion. However, we identify a view-mismatch problem in this\nprocess. The on-device LLMs (teachers) and the global MoE\nmodel (student) differ in model architectures, latent spaces,\nand predictive perspectives (i.e., different inductive biases and\npreferences when making predictions). This renders traditional\nknowledge distillation methods based on direct teacher-student\nlogit alignment or feature alignment within a shared latent\nspace ineffective. To address this, we propose a View-Aligned\nAttention (VAA) module that enables the student model to\nachieve a similar perspective with that of the teacher model,\nthereby facilitating efﬁcient knowledge transfer. Finally, the re-\nsulting MoE model consolidates expertise from heterogeneous\ndevices, yielding a robust, knowledge-abundant global model.\nThis paper makes the following contributions:\n• To the best of our knowledge, we are the ﬁrst to source\nextensive and diverse training data from ubiquitous resource-\nconstrained edge devices for MoE training while preserving\ndata privacy. We propose a novel, scalable federated MoE\ntraining framework named DEEPFUSION. With federated\nknowledge distillation, on-device LLMs trained on local\ndata serve as repositories of local AI knowledge that can\nbe transferred into the global MoE model.\n• We address the view-mismatch problem in traditional fed-\nerated knowledge distillation from on-device LLMs to the\nDense LLM\n(MoE’s base model)\nEmbedding\nSelf-attention\nOutput Layer\nFFN Layer\nMixture-of-Experts (MoE)-based LLM\nEmbedding\nSelf-attention\nExpert 1\nExpert 2\nExpert K\n...\nOutput Layer\n...\nL × MoE Blocks\nGating\nLayer\nUpcycle\nReplaced\nAdded\nTransformer \nBlocks\nL × \nweighted \naverage\nFig. 1: Upcycling a dense LLM to an MoE-based LLM.\nglobal MoE model, arising from heterogeneity in model\narchitectures and predictive perspectives. By integrating\nmulti-stage feature representations from the global MoE\nmodel (student), the proposed VAA module aligns the stu-\ndent’s predictive perspective with that of on-device LLMs\n(teachers), enabling effective cross-architecture knowledge\ntransfer.\n• DEEPFUSION is evaluated through comprehensive exper-\niments on two industry-level MoE-based LLM models\nQwen-MoE and DeepSeek-MoE, using diverse real-world\ndatasets with data privacy concerns including medical\nand ﬁnancial data, across multiple-choice and open-ended\nquestion-answering (QA) tasks. Results show that DEEP-\nFUSION closely matches centralized MoE training and out-\nperforms federated MoE baselines, with token perplexity\nimprovements of up to 5.28% on challenging open-ended\nQA tasks and communication cost reductions of up to 71%\nacross different settings.\nThe remainder of this paper is organized as follows. Section\nII introduces the preliminaries on MoE-based LLM archi-\ntecture and their operating mechanism. Section III presents\nthe DEEPFUSION system design and key implementation\nchallenges. Section IV details the proposed methodology to\naddress these system challenges. Section V evaluates the\nperformance of DEEPFUSION through comprehensive experi-\nments and ablation studies. Section VI reviews related work,\nand Section VII concludes the paper.\nII. PRELIMINARIES\nAutoregressive Language Modeling in LLMs: The LLMs\nare typically trained with autoregressive language modeling\n[16] to generate coherent and contextually appropriate text.\nGiven an input sequence of tokens (words or subwords), the\nLLM predicts the most likely next token and and appends it\nto the input sequence for subsequent token predictions. This\nprocess repeats iteratively until the LLM generates a special\nend-of-sentence token (e.g., <eos>), signaling the completion\nof text generation.\nLet x = {x0, x1, ..., xT } deﬁne a reference sequence of\ntokens where xt denotes the t-th token in the sequence.\nThe LLM training objective is to maximize the likelihood\n"}, {"page": 3, "text": "3\nof predicting the correct token xt, given the previous tokens\nx0, x1, ..., xt−1 for all t ∈{1, ..., T }:\nPLLM(x; m) =\nT\nY\nt=1\nP (xt | x1, ..., xt−1; m) ,\n(1)\nwhere P (xt | x1, . . . , xt−1; m) denotes the conditional prob-\nability of predicting the correct token xt given the previous\ntokens x1, . . . , xt−1 and LLM parameters m. In practice, au-\ntoregressive LLMs evaluate the cross-entropy loss LCE(x; m)\nfor the reference token sequence x by computing the negative\nlog-likelihood of the predicted tokens. The LLM parameters\nm are optimized using SGD-like algorithms to minimize the\ntraining loss.\nLCE(x; m)=−log PLLM(x; m).\n(2)\nAs training progresses, the LLM will be increasingly adept\nat the reference NLP context and more conﬁdent in token\npredictions. Token perplexity Γ(x; m) [17] is a common\nmetric for assessing the LLM performance, which quantiﬁes\nthe average uncertainty in predicting the correct token:\nΓ(x; m) =\n T\nY\nt=1\n1\nP (xt | x1, . . . xt−1; m)\n!1/T\n.\n(3)\nA low token perplexity Γ(x; m) indicates that the LLM\nhas adapted well to the reference NLP task, consistently\nmaking correct token predictions. In contrast, a high token\nperplexity Γ(x; m) suggests that the LLM remains uncertain\nor frequently makes incorrect token predictions.\nMixture-of-Experts (MoE) Architecture: The MoE-based\nLLMs, such as Qwen-MoE [5] and DeepSeek-MoE [6], have\nemerged as a growing trend of the mainstream LLMs. As\nshown in Fig. 1, the MoE models are typically upcycled from\ntraditional dense LLMs. The dense LLM is built from a se-\nquential stack of L transformer blocks, where each transformer\nblock contains a multi-head self-attention module and a feed-\nforward network (FFN). The MoE model retains its backbone\narchitecture, but upgrades each transformer block into an MoE\nblock by replacing the FFN with K specialized, FFN-based\nexperts [18].\nEach MoE block contains experts that capture different\naspects of domain knowledge to process input tokens. Unlike\ndense LLMs that activate all model parameters for an input\ntoken, the MoE models employ a sparsely activation strategy.\nFor each input token, an MoE block dynamically activates a\nsubset of experts aligned with the most relevant knowledge\ndomains for processing.\nThe activation of experts is controlled by a gating mech-\nanism, which operates as a softmax classiﬁer (shown as the\ngating layer in Fig. 1). For a given input token, the gating\nlayer calculates the probability of each expert being selected to\nprocess that token. Meanwhile, a parameter k, where k < K,\nspeciﬁes that only the experts with top-k selection probabilities\nare activated. The computation outputs of those activated\nexperts are combined and weighted averaged to derive the\nﬁnal output of the current MoE block. Formally, the ﬁnal\nEmbedding \nOutput Layer\nGlobal Mixture-of-Expert (MoE) Model\n(e.g., DeepSeek-MoE [5] and Qwen-MoE [6])\nKnowledge\ntransfer\nOn-device LLM Models\n...\n(local domain knowledge)\nServer\nĹ Global MoE model fusion\n Edge Device 1         Edge Device 2         Edge Device 3              Edge Device N\n...\nLocal \nprivate\ndata\nķ Personalized on-device LLM model training\n...\n(Extracting local domain knowledge from private data) \nĸ On-device LLM\nmodel upload\nMoE Blocks\nL × \nSelf-attention\nGating\nExpert 1\nExpert K\n...\n...\nl-th MoE Block\nOn-d\n(local\nServer\nFig. 2: DEEPFUSION system: Federated MoE training via\nknowledge transfer from heterogeneous edge devices.\ncomputation result y of the MoE block for an input x is:\ny =\nX\ni ∈topk\npi · yi(x)\n(4)\nwhere pi denotes the selection probability of expert i, and\nyi(x) is the output of expert i for input x. The advanced MoE\narchitecture enables itself to capture diverse domain knowl-\nedge across different experts, thus learning more complex\nfeature representations and enabling superior performance over\ntraditional dense LLMs.\nIII. SYSTEM OVERVIEW\nA. System Description\nAs shown in Fig. 2, we envision a federated MoE learning\nsystem across N heterogeneous edge devices in set N. The\ngoal is to collaboratively train a global MoE-based LLM for\nnatural language generation tasks. Each edge device n ∈N,\nrepresenting ubiquitous AIoT devices in real world, runs\nits on-device LLM model mn for local AI applications. To\nbest meet its local application requirements and resource\nconstraints, each edge device independently tailors the archi-\ntecture and parameter size of its local LLM model mn. All\nedge devices generate privacy-sensitive local application data,\nwhich strictly remains on-device.\nA powerful central server coordinates the federated MoE\nlearning process, aggregating knowledge from each on-device\nLLM model into the global MoE model. Each on-device model\nspecializes as a domain-speciﬁc expert by training on local\napplication data for its speciﬁc AI scenario. The expertise\nfrom heterogeneous on-device LLMs serves as a valuable\nknowledge source for the multiple experts within the MoE.\nThrough knowledge distillation, the central server efﬁciently\nintegrates the expertise of on-device LLMs into the global\nMoE model. The resulting MoE-based LLM consolidates\n"}, {"page": 4, "text": "4\n...\nPhase I. Local Knowledge Clustering.\nPhase II. Cross-architecture Knowledge Distillation.\nBenchmark\nMoE’s base model\n(Student)\nLocal-knowledge proxy \nmodel (Teacher)\nBlock J\nBlock 2\nBlock J\nView-aligned attention \nmodule (VAA)\nLFM\nLFM\nLFM\ndata\nlogits\nlogits\nLKL\n0 1 0 0\nLCE\n...\nmi\nMi\nBlock 2\nF1\nS\nF2\nS\nFJ\nS\nF1\nT\nF2\nT\nFJ\nT\nPhase III. Global MoE Model Tuning.\n...\n...\nDomain C1\nDomain C2\nDomain C3\nDomain C4\n...\n...\nDomain C5\nN  On-device LLMs Clustering\nK  Local Knowledge Domains\n\\\nDomain Ci\n......\nDomain CK\nFrozen\nTrainable\nT\nPLLM\nS\nPLLM\nBlock 1\nBlock 1\nExpert 1\nExpert 2\nExpert K\n Embedding Layer\n Self-attention\n Gating Layer\n Expert 1\n Expert 2\n Expert K\n...\n Output Layer\nL × MoE Blocks\n...\nGlobal MoE Model\nMoE’s Base \nModels\nMMoE\n{Mi}i=1\nK\nFig. 3: Server-side pipeline for federated knowledge distillation from heterogeneous on-device LLMs to the global MoE model.\ndomain-speciﬁc knowledge from all devices, enabling robust\nperformance on a wide range of natural language queries. To\nsummarize, our objective is to leverage distributed expertise\nfrom heterogeneous on-device LLMs {mn}N\nn=1 to construct a\nrobust, knowledge-abundant global MoE model MMoE.\nB. Key Challenges of System Implementation\nIt poses signiﬁcant technical challenges to realize a fed-\nerated MoE training system in practical edge environments,\nspeciﬁcally in these areas:\n• Challenge 1 (FL Communication Costs): Traditional FL\nrequires multiple communication rounds to exchange local\nmodel updates between edge devices and the central server.\nHowever, modern on-device LLMs, such as GPT-2 Medium\n[19] (380M parameters) and Tiny-LLaMa [20] (1.1B param-\neters), are increasingly signiﬁcant, often exceeding hundreds\nof millions to over a billion parameters. The growing size\nof on-device LLMs results in high FL communication costs\nover resource-constrained edge networks.\n• Challenge 2 (System Scalability): An increasing number\nof participating edge devices, which communicate with\nthe central server, would exacerbate FL communication\noverhead on resource-constrained edge networks. Each on-\ndevice LLM captures unique local domain knowledge that is\ntransferable to the global MoE model. Nonetheless, perform-\ning knowledge distillation for each on-device model would\nimpose signiﬁcant computational overhead on the central\nserver.\n• Challenge 3 (Edge Device Heterogeneity): The on-device\nLLMs (teacher models) are typically based on compact\nLLM architectures [14], differing from the global MoE-\nbased LLM (student model) in model architecture. As a\nresult, the teacher and student models reside in distinct\nlatent spaces. Traditional federated knowledge distillation\nmethods, relying on direct teacher-student logits or feature\nalignment in a shared latent space, are infeasible for cross-\narchitecture knowledge transfer.\nMotivated by the aforementioned challenges, we propose a\ncomprehensive solution framework named DEEPFUSION. The\nmethodology details are presented in Section IV.\nIV. METHODOLOGY\nDEEPFUSION aims to enable communication-efﬁcient and\nscalable federated knowledge transfer from heterogeneous\nedge devices n ∈{1, . . ., N}, ultimately obtaining a global\nMoE model MMoE that ensembles the distributed knowledge\nof on-device LLMs {mn}N\nn=1. As shown in Fig. 3, federated\nknowledge distillation workloads are computation-intensive,\nthus they are handled by the central server. In contrast, each\nedge device n simply trains its lightweight on-device LLM\nmn on private data to acquire local domain knowledge. This\nenables broad participation especially from resource-limited\nedge devices, aligning with our goal to sourcing diverse\nand extensive training data from ubiquitous edge devices for\nMoE training. In the following, we elaborate on the design\nprinciples and key modules of DEEPFUSION.\nA. One-shot Federated Learning Design\nIn contrast to traditional FL with multiple communica-\ntion rounds, DEEPFUSION employs a one-shot FL design\nto enhance communication efﬁciency. Speciﬁcally, each edge\ndevice n trains its on-device LLM mn using private local\napplication data. As local training advances, the on-device\nLLM’s performance is consistently optimized, while progres-\nsively capturing essential local domain knowledge. Once local\ntraining is complete, the fully optimized on-device LLM mn\nis uploaded to the central server in a single communication\nround for federated knowledge distillation. Thus, the total\ncommunication cost ̥net of DEEPFUSION is:\n̥net =\nXN\nn=1 |mn| ,\n(5)\nwhere |mn| denotes the data size of on-device LLM mn to be\ntransmitted to the server.\nThis one-shot FL design is both efﬁcient and practical. Edge\ndevices are self-motivated to optimize their on-device LLMs,\nthus enhancing their local AI applications. The central server\nthen acts as a free rider, distilling local domain knowledge\nfrom these pre-trained on-device LLMs into the global MoE\nmodel MMoE. This process imposes no additional computa-\ntional or resource burden on the edge devices and improves\nthe utility of local LLM training.\nB. Local Knowledge Clustering\nAs the number of participating edge devices increases,\nperforming knowledge distillation individually from each on-\ndevice LLM into the MoE model MMoE would impose an\n"}, {"page": 5, "text": "5\n...\nOn-device LLMs\nmi\nLocal-knowledge \nProxy Model\nModel weight\naggregation\ni\nn\nn C\nm\nÎå\n= 1\ni\nC ×\nFig. 4: Generating a proxy model ¯mi for the local knowledge\ndomain Ci.\nunsustainable computational burden on the central server. To\ntackle this scalability challenge, we propose a local knowledge\nclustering method. As shown in Phase I of Fig. 3, the on-device\nLLMs uploaded from numerous edge devices are clustered into\nK local knowledge domains {Ci}K\ni=1, matching the K experts\nin the global MoE model. Subsequently, we construct K local-\nknowledge proxy models { ¯mi}K\ni=1 for these domains {Ci}K\ni=1,\nupon which we perform knowledge distillation.\nImplementation Details: Each local knowledge domain\nCi consists of on-device LLM models of the same type\nthat captures similar aspects of local knowledge (e.g., med-\nical expertise on a particular disease). The local knowledge\nsimilarity between on-device LLMs is assessed using meta-\ninformation from their local training data. When an edge\ndevice n uploads the pre-trained on-device LLM mn, it also\nsends low-rank feature embeddings en computed from its\nraw training data. These feature embeddings en capture the\nsemantic information of local data while preserving privacy,\nindicating the local knowledge domain learned by the on-\ndevice LLM mn. Modern on-device NLP semantic encoders\n(e.g., MiniLM [21]) can efﬁciently compute these low-rank\nfeature embeddings, which are typically tens of bytes and incur\nnegligible communication costs when sent to the server.\nFormally, let Π = [πn1,n2] ∈RN×N represent the local\nknowledge similarity matrix, where each element πn1,n2 de-\nnotes the cosine similarity between the feature embeddings of\non-device LLMs mn1 and mn2:\nπn1,n2 =\nen1 · en2\n∥en1∥· ∥en2∥.\n(6)\nBased on this similarity matrix Π, the on-device LLMs are\ngrouped into K local knowledge domains {Ci}K\ni=1 using\nKMeans.\nThen, as depicted in Fig. 4, within each local knowledge\ndomain Ci, a proxy model ¯mi is generated by averaging the\nweights of the clustered on-device LLMs mi ∈Ci. Each proxy\nmodel ¯mi encapsulates its respective domain knowledge for\ntransfer into the global MoE model MMoE. There are totally\nK local-knowledge proxy models, and each proxy model ¯mi\nis to be distilled into one of K experts in MMoE. In summary,\nour local knowledge clustering approach effectively enables\nscalable transfer of diverse local domain knowledge from\nnumerous edge devices into the global MoE model.\nC. Cross-architecture Knowledge Distillation\nWe will distill the obtained K local-knowledge proxy mod-\nels { ¯mi}K\ni=1 into the K experts of the global MoE model\nMMoE, with each proxy model assigned to a corresponding\nexpert. As noted in Section II, an MoE-based LLM model\nis typically upcycled from a base LLM model. Building\nFlatten & Concat\nF\nMulti-head Self-attention\nDivide\nF1\nS\nF2\nS\nFJ\nS\nProj\nProj\nProj\nS\nQ\nK\nV\nProj\nProj\nProj\nF1\nS’\nF2\nS’\nFJ\nS’\n...\n...\nF S’\nFig. 5: Design of the View-aligned Attention (VAA) module.\non this idea, we ﬁrst distill each local-knowledge proxy\nmodel ¯mi into a separate MoE’s base model Mi, yielding\na collection of K MoE’s base models {Mi}K\ni=1. Following\nthe efforts of [22], [23], we assume that public benchmark\ndata such as from Hugging Face or GitHub is available at\nthe server to facilitate this distillation process. Similar to\nthe model upcycling process, these K MoE’s base models\nare then merged and upgraded into a global MoE model by\nassigning each base model’s parameters to the corresponding\nexpert position in the MoE. The resulting global MoE model\nMMoE integrates diverse domain knowledge and can therefore\neffectively handles diverse input tasks.\nWhen distilling the local-knowledge proxy model into the\nMoE’s base model, a key challenge arises from the discrepancy\nin predictive perspectives between the local-knowledge proxy\nmodel ¯mi (teacher) and the MoE’s base model Mi (student).\nThis stems from their distinct model architectures. The local-\nknowledge proxy model ¯mi, constructed by weight-averaging\na cluster of on-device LLMs, thus adopts a compact LLM\narchitecture with a small parameter count. This compact LLM\narchitecture progressively builds global semantic features from\nsimpler and localized linguistic cues, achieving global coher-\nence mainly in the ﬁnal layers [14]. In contrast, the MoE’s base\nmodel Mi typically employs a standard LLM architecture with\na larger parameter count, which enables it to easily learn global\nsemantic features from the outset [24]. This difference in how\nthe teacher and student models learn feature representations\nleads to a misalignment in their predictive perspectives. Hence,\ntraditional knowledge distillation approaches based on direct\nteacher-student logits or feature matching [25] are ineffective\nin our scenario. These methods typically require the teacher\nand student to have similar model architectures and aligned\npredictive perspectives, which does not hold in our scenario.\nTo tackle this problem, we design a View-Aligned Atten-\ntion (VAA) module to support cross-architecture knowledge\ndistillation. As shown in Phase II of Fig. 3, both the local-\nknowledge proxy model ¯mi (teacher) and the MoE’s base\nmodel Mi (student) are split into J representation stages\n(blocks). The proposed VAA module employs a multi-head\nattention mechanism, enabling the student model to integrate\nits features {F S\nj }J\nj=1 from various representation stages 1 to J,\nthus acquiring a predictive perspective {F S′\nj }J\nj=1 comparable\nto that of the teacher model. Owing to the ﬂexibility of\n"}, {"page": 6, "text": "6\nthe multi-head attention module, the VAA module can be\nincorporated into any model architecture without requiring\nspecialized design. Speciﬁcally, the proposed VAA module is\nillustrated in Fig. 5 and operates in the following three steps:\nFirst, we patchify the student features at each representation\nstage j ∈{1, . . ., J} into Pq/J patches, capturing detailed\nfeature at each stage. These patches are then projected via con-\nvolutional layers Cj to a predeﬁned dimension R\nPq\nJ ×d, where\nPq is a hyper-parameter denoting the total number of queries\nacross all J representation stages. These resulting features\nfrom all stages are then concatenated to form F S ∈RPq×d:\nF S = Concat\n\u0000C1\n\u0000F S\n1\n\u0001\n, .., CJ\n\u0000F S\nJ\n\u0001\u0001\n.\n(7)\nSecond, we apply a multi-head self-attention module to F S:\nF S′ = Softmax\n \u0000WqF S\u0001\n·\n\u0000WkF S\u0001T\n√\nd\n!\n· Wv · F S,\n(8)\nwhere Wq, Wk, and Wv are trainable self-attention weights,\nand d denotes the self-attention channel dimension. FS con-\ntains the features from different representation stages of the\nMoE’s base model Mi (student). Through our VAA attention\nmechanism, the student model learns to focus on different\nrepresentation stages to blend a new student feature F S′ whose\npredictive perspective closely aligns with that of the local-\nknowledge proxy model ¯mi (teacher).\nFinally, we divide the blended student features F S′ back\ninto J representation stages and project each to match the cor-\nresponding stage feature size of the teacher model ¯mi, result-\ning in {F S′\nj }J\nj=1. As the blended student features {F S′\nj }J\nj=1\napproximate the teacher model’s perspective, standard feature\nalignment can be subsequently applied:\nLFM =\nXJ\nj=1 MSE\n\u0010\nF T\nj , F S′\nj\n\u0011\n(9)\nwhere LFM denotes the feature-matching loss computed be-\ntween the features of the teacher and student models across\nthe J representation stages. Meanwhile, the logits-based dis-\ntillation loss LKL using KL-divergence functions is deﬁned:\nLKL = KL\n\u0000P T\nLLM, P S\nLLM\n\u0001\n(10)\nwhere P T\nLLM and P S\nLLM indicate the soft-label predictions\n(logits) of teacher and student models, as computed in Eq. (1).\nThe loss LKL minimizes the discrepancy between the teacher’s\nand student’s ﬁnal logits. The cross-entropy loss LCE for the\nstudent model Mi on ﬁnal hard-label predictions is described\nin Eq. (2). To sum up, the total cross-architecture knowledge\ndistillation loss LKD between ¯mi and Mi is:\nLKD = LCE + α · LFM + β · LKL\n(11)\nwhere α and β are trade-off hyper-parameters controlling the\nimpact of each loss term. The MoE’s base model Mi (student)\nis trained to minimize the total cross-architecture knowledge\ndistillation loss LKD, thereby effectively distilling knowledge\nfrom the local-knowledge proxy model ¯mi (teacher).\nEmbedding Layer\nSelf-attention Layer\nExpert 1\nExpert 2\nExpert K\n...\nOutput Layer\n...\nL × MoE Blocks\nGating\nLayer\nGlobal MoE Model MMoE\nEmbedding\nSelf-attention\nOutput Layer\nFFN Layer\nTransformer \nBlocks\nL × \nMoE’s Base \nModels\nmerge\nGlobal MoE Model Merge Rule:\n• In the l-th MoE block, the expert i copies the FFN layer parameters from the l-th \n  transformer block of MoE’s base model Mi.\n• Merge the embedding/self-attention/output layers across all the K MoE’s base \n  models              by computing their element-wise averaging. \n{Mi}i=1\nK\nK\n{Mi}i=1\nK\nFig. 6: Merging the K MoE’s base models {Mi}K\ni=1 into the\nglobal MoE model MMoE.\nD. Global MoE Model Tuning\nAs in Phase III of Fig. 3, we will merge the obtained K\nMoE’s base models {Mi}K\ni=1, each capturing unique domain\nknowledge from heterogeneous edge devices, into the global\nMoE model MMoE. Fig. 6 depicts the global MoE model\nmerge rule. In the global MoE model MMoE, each expert\ni, which is built using FFN layers, directly inherits the\nparameters of the FFN layers M FFN\ni\nfrom its corresponding\nbase model Mi:\nM i\nexpert ←M FFN\ni\n.\n(12)\nThis leverages the fact that the FFN layers in different\nMoE’s base models {Mi}K\ni=1 capture distinct domain-speciﬁc\nknowledge for processing input tokens, enabling the FFN-\nbased experts in MMoE to specialize accordingly. In contrast,\nthe layers such as the embedding, self-attention, and output\nlayers in MMoE operate globally to capture relationships and\ndependencies between tokens in a sequence. Therefore, we\ninitialize these layers by averaging the parameters of the\ncorresponding layers from the K MoE’s base models Mi\nK\ni=1,\nthus aggregating the diverse knowledge encoded in these\ncomponents across different MoE’s base models:\nM x\nMoE = 1\nK ·\nXK\ni=1 M x\ni ,\n(13)\nwhere x refer to the embedding, self-attention or output layers.\nAfter initializing the global MoE model MMoE with the\nK base models {Mi}K\ni=1, we start with global MoE model\ntuning. The FFN-based experts are frozen, while the other\nlayers including embedding, self-attention, gate, and output\nlayers are ﬁne-tuned. Based on public benchmark data at the\nserver, the gate layer M gate\nMoE is trained to compute optimal\nexpert selection probabilities ⟨pi⟩K\ni=1 for each input token,\nenabling sparse activation of the most relevant top-k experts.\nThe embedding, self-attention, and output layers are jointly\ntrained to more effectively capture global relationships and\ndependencies between tokens in a sequence. Finally, the global\nMoE model MMoE converges to optimal performance.\nSince only the embedding, self-attention, gate, and output\nlayers are ﬁne-tuned while the larger portion of FFN-based\n"}, {"page": 7, "text": "7\nTABLE I: Token Perplexity (log) Results Under Different Case Studies and Various System Scale Settings.\nMethod\n(1) Qwen-MoE for Medical Multi-choice QA\n(2) DeepSeek-MoE for Financial Open-ended QA\nN = 16\nN = 32\nN = 64\nN = 128\nN = 16\nN = 32\nN = 64\nN = 128\nFedJETS [11]\n0.1639\n0.1588\n0.1557\n0.1571\n✿✿✿✿✿\n3.9717\n3.9863\n3.9987\n✿✿✿✿✿\n4.0006\nFedKMT [26]\n0.1609\n0.1574\n0.1526\n0.1546\n3.9908\n✿✿✿✿✿\n3.9811\n✿✿✿✿✿\n3.9941\n4.0030\nOFA-KD [27]\n0.1573\n0.1535\n0.1510\n0.1494\n4.0375\n4.0398\n4.0224\n4.0333\nDEEPFUSION\n✿✿✿✿✿\n0.1603\n✿✿✿✿✿\n0.1569\n✿✿✿✿✿\n0.1551\n0.1494\n3.9697\n3.9700\n3.9774\n3.9723\nTABLE II: Token Accuracy (%) Results Under Different Case Studies and Various System Scale Settings.\nMethod\n(1) Qwen-MoE for Medical Multi-choice QA\n(2) DeepSeek-MoE for Financial Open-ended QA\nN = 16\nN = 32\nN = 64\nN = 128\nN = 16\nN = 32\nN = 64\nN = 128\nFedJETS [11]\n✿✿✿✿\n92.45\n92.39\n92.36\n92.41\n✿✿✿✿\n28.28\n29.47\n29.03\n28.64\nFedKMT [26]\n92.34\n92.42\n92.42\n92.38\n28.26\n✿✿✿✿\n29.57\n29.06\n28.93\nOFA-KD [27]\n92.38\n92.38\n92.41\n92.55\n27.89\n28.88\n29.04\n✿✿✿✿\n28.95\nDEEPFUSION\n92.52\n✿✿✿✿\n92.41\n92.42\n✿✿✿✿\n92.45\n28.32\n29.67\n29.06\n29.22\n* We bold the highest performance and ✿✿✿✿✿✿✿\nunderline the second highest performance for each row.\nexperts remain frozen, the computational overhead is mini-\nmal. The FFN-based experts constitute the majority of model\nparameters. Their weights have already been determined by\nleveraging heterogeneous federated edge devices to contribute\nlocal domain knowledge distilled from their pre-trained on-\ndevice LLMs. These trainable components represent only a\nsmall fraction of total model parameters, resulting in reduced\nmemory footprint and faster optimal model convergence.\nOur approach distinguishes from traditional centralized MoE\ntraining at cloud servers [28], [29], which typically requires\nextensive full-parameter training.\nV. PERFORMANCE EVALUATION\nA. Experimental Setup\nCase Studies: To evaluate DEEPFUSION’s generalizability,\nwe conduct two realistic case studies: (1) medical multi-choice\nquestion answering (QA) and (2) ﬁnancial open-ended QA.\nEach case involves training an industry-scale global MoE-\nbased LLM, speciﬁcally Qwen1.5-MoE (14.3B Param.) and\nDeepSeek-MoE-16B-base.\n(1) Qwen-MoE for medical multi-choice QA: We use the\nMMedBench dataset [30], a multilingual medical multi-choice\ndataset covering six languages. Each input sample consists of a\nmedical question and its corresponding candidate answers. The\nglobal Qwen1.5-MoE is trained to select the correct answer\nfor each question. We split 7,669 data samples for the testset,\nand 30,429 samples for local training data over edge devices.\nThe on-device LLMs deployed on edge devices include GPT-\n2, GPT-2 Medium and TinyLlama.\n(2) DeepSeek-MoE for ﬁnancial open-ended QA: We use the\nFinQA dataset [31], a ﬁnancial open-ended QA dataset. Each\ninput sample consists of an instruction describing the ﬁnance\nQA context and a question that to be answered directly. The\nglobal DeepSeek-MoE-16b-base model is trained to generate\na paragraph-style answer for each question. Similarly, we split\n1,248 samples for the testset, and 10,618 for local training data\nover edge devices. The on-device LLMs involve TinyLlama,\nOLMo-1.2B and BLOOM-1.1B.\nOur selected on-device LLMs encompass various model\narchitectures and sizes, enabling deployment on a range of\nedge devices with varying hardware capacities. This supports\nour considered system scenario, where ubiquitous edge devices\nparticipate in federated knowledge distillation-based MoE\ntraining. Fig. 7 depicts the RAM requirements for training\nthese on-device LLMs after bﬂoat16 and NF4 quantization,\nwhich are generally affordable for typical edge devices (e.g.,\nNVIDIA Jetson Nano with 4GB RAM and iPhone 16 with\n8GB RAM).\nSystem Settings: We evaluate the performance of DEEPFU-\nSION by varying the number of participating edge devices from\n16, 32, 64 to 128. Each edge device is randomly assigned to\noperate one of the on-device LLMs used in our case studies.\nThe local private training data is distributed randomly and\nunevenly across the edge devices.\nBaselines: We compare DEEPFUSION against a theoreti-\ncal upper-bound (DeepSpeed [28]) and three state-of-the-art\nmethods (FedJETS [11], FedKMT [26], and OFA-KD [27]).\n(1) DeepSpeed [28] represents the traditional centralized MoE\ntraining approach on cloud servers and serves as the theoretical\nperformance upper-bound for DEEPFUSION.\n(2) FedJETS [11] implements a federated MoE training system\nby training a compact MoE network, pruned from the global\nMoE model, on each edge device. These local compact MoE\nnetworks are merged into a global MoE model at the server.\n(3) FedKMT [26] is a federated knowledge transfer method\nbetween small and large language models. It leverages the ﬁnal\nlogits produced by small LLMs to supervise the global large\nLLM on the server.\n(4) OFA-KD [27] is a cross-architecture knowledge distillation\napproach that projects intermediate student features into the\nlogits space and aligns them with the teacher model’s ﬁnal\nlogits. It can be an ablation baseline to evaluate our proposed\ncross-architecture knowledge distillation mechanism.\n"}, {"page": 8, "text": "8\nFig. 7: On-device Memory Footprint.\nFig. 8: Communication Costs of Different Federated MoE Training Methods.\nFig. 9: Performance Comparison between DEEPFUSION and DeepSpeed [28] (Centralized MoE Training Method).\nB. Main Experimental Results\nPerformance Comparison: In Tables I and II, we evaluate\nDEEPFUSION on two case studies. Speciﬁcally, we compare\nthe token perplexity and token accuracy of the resulting global\nMoE model against three state-of-the-art baselines across\ndifferent system scales (N = 16 to N = 128 edge devices).\nThe token accuracy measures the percentage of tokens in the\ngenerated output that exactly match the tokens in the reference\nsequence. The results show that our DEEPFUSION effectively\ndistills local domain knowledge from diverse edge devices,\nenabling the global MoE model to achieve comparable or\neven superior performance compared to the baselines. Notably,\nDEEPFUSION demonstrates robust and superior performance\nacross system scales in the ﬁnancial open-ended QA case\nstudy. Typically, the open-ended QA task is considered more\nchallenging than the multi-choice QA task, as it requires LLMs\nto possess complex reasoning abilities in order to generate\ncoherent and ﬂuent free-form answers. In this challenging\nsetting, DEEPFUSION exhibits even greater advantages over\nthe baselines. This is attributed to our designed VAA module\nwhich explicitly distills feature knowledge from local mod-\nels into the global MoE model, and such distilled feature\nknowledge effectively enhances the reasoning capacity of the\nresulting MoE-based LLM.\nNote that, in the ﬁnancial open-ended QA task, the token\naccuracy results do not appear high, as open-ended questions\ntypically allow for diverse and ﬂexible answer expressions,\nmaking exact token matches less likely even for semantically\ncorrect responses. Nevertheless, token accuracy still reﬂects\nhow well DEEPFUSION trains the global MoE model, com-\nplementing the token perplexity metric.\nOn-device Memory Footprint: Fig. 7 presents the peak\nmemory usage of an edge device during on-device LLM\ntraining for our DEEPFUSION method and the state-of-the-\nart federated MoE training baseline named FedJETS. DEEP-\nFUSION enables ubiquitous, resource-limited edge devices\nto participate in federated MoE training, regardless of local\nhardware constraints. Each edge device can run lightweight\non-device LLMs such as TinyLlama 1.1B, OLMo 1.2B,\nBLOOM 1.1B, GPT2 Medium, and GPT2, according to its\nown application needs and resource availability. Once these\nLLMs are sufﬁciently trained locally, they serve as repositories\nof local domain knowledge, transferring knowledge to the\nglobal MoE model through knowledge distillation. FedJETS\nrequires deployment of a local MoE network on each edge\ndevice. The local expert network, despite its compactness, still\nretains the MoE architecture and remains too cumbersome for\nmany edge devices. Our empirical measurements indicate that\nFedJETS demands 3.3x to 9.3x more on-device memory than\nDEEPFUSION, which imposes a signiﬁcant hardware threshold\nfor edge participation. This limits the inclusion of extensive\nand diverse edge devices in federated MoE training.\nFL Communication Efﬁciency: Fig. 8 compares the total\nFL communication costs of DeepFusion and FedJETS across\ntwo case studies. DEEPFUSION allows edge devices to operate\nand train lightweight on-device LLMs. The on-device LLMs\ncan be those naturally pre-deployed on edge devices for\ntheir own application requirements, so DEEPFUSION does\nnot require additional, dedicated on-device LLM deployment.\nFurthermore, in a communication-efﬁcient one-shot FL setup,\nonce optimal local performance is achieved, the local LLMs\nare uploaded to the central server in a single communication\nround for server-side knowledge distillation into the global\nMoE model. Conversely, FedJETS assigns each edge device\na local expert model, essentially pruned from the global\nMoE model. These local expert models, usually retaining the\ncomplex MoE architecture and typically larger than standard\nlightweight on-device LLMs, incur signiﬁcant communication\noverhead from downloading to edge devices and upload-\ning trained versions back to the server. As the number of\nedge devices grows, FedJETS’ total FL communication costs\nrise rapidly, while DEEPFUSION maintains manageable costs\nacross different system scales, as depicted in Figure 8.\nC. Ablation Analysis\nHeterogeneous vs. Homogeneous On-device LLMs: In\nTables I and II, our DEEPFUSION method consistently outper-\nforms FedJETS. This demonstrates the inherent advantages of\nDEEPFUSION, which effectively leverages heterogeneous on-\ndevice LLMs as the sources of local domain knowledge to be\n"}, {"page": 9, "text": "9\ndistilled into the global MoE model. Heterogeneous on-device\nLLMs learn distinct prediction perspectives, shaped by varying\nmodel architectures. By distilling the knowledge from these\nheterogeneous on-device models into the global MoE, DEEP-\nFUSION enables the aggregation of complementary insights\nfrom multiple predictive perspectives, resulting in better and\nmore comprehensive global MoE model performance. This\nﬂexibility in supporting heterogeneous on-device LLMs stands\nin contrast to FedJETS, which require all edge devices to op-\nerate a similar compact MoE architecture referred to as a local\nexpert model. In such homogeneous setups, the global MoE\nmodel can only integrate knowledge of the same predictive\nperspectives from homogeneous local expert models, hence\nlimiting the MoE’s capacity to generalize and improve.\nCross-architecture Knowledge Distillation: We compare\nDEEPFUSION with two cross-architecture knowledge distilla-\ntion baselines including FedKMT and OFA-KD. Both Fed-\nKMT and OFA-KD are logits-driven knowledge distillation\napproaches, where the logits refer to the soft-label predictions\nfrom the ﬁnal model layer. While lgotis can transfer high-\nlevel knowledge, they often neglect implicit model knowledge\nsuch as hidden feature representations, which are crucial for\neffective knowledge distillation. As shown in Tables I and II,\nthis limitation is evident in the case study of ﬁnancial open-\nended QA, where the global DeepSeek-MoE model requires\nadvanced knowledge reasoning abilities to generate coherent\nfree-form responses. These reasoning capacities are captured\nin the teacher models’ hidden feature representations, which\ntrace the underlying reasoning process beyond ﬁnal logits.\nBy employing feature-driven cross-architecture knowledge\ndistillation with the VAA module, DEEPFUSION effectively\ntransfers the robust reasoning capabilities of local teacher\nmodels to the global MoE model.\nDEEPFUSION vs. Centralized MoE Training: We evaluate\nthe federated MoE training performance of DEEPFUSION by\ncomparing it with a state-of-the-art centralized MoE training\nmethod named DeepSpeed. Our selected baseline DeepSpeed\nis a well-established centralized MoE training method in\nacademic and industrial settings, and provides a theoretical\nperformance upper bound for our federated MoE training\nframework DEEPFUSION. Figure 9 illustrates the performance\ncomparison between DEEPFUSION and DeepSpeed across two\ncase studies with varying system scales. Results show that\nDEEPFUSION consistently achieves performance very close to\nthat of centralized MoE training with DeepSpeed, demonstrat-\ning the effectiveness of our approach.\nNote that for the medical multi-choice QA task, we use\ntoken accuracy as the evaluation metric, since the model’s out-\nput tokens directly determine the correctness of multi-choice\nanswers. For the ﬁnancial open-ended QA task, we employ an\nLLM-as-a-judge evaluation framework [32], a widely-adopted\nmethod for assessing generative AI models on open-ended\nQA. Speciﬁcally, we leverage the Google Gemini 2.5 API\nto automatically score each response generated by the MoE\nmodel. The LLM judge assigns a score from 0% to 100%,\nreﬂecting how well the response addresses the posed question.\nVI. RELATED WORK\nA. Federated MoE Training\nFederated learning addresses the extensive training data\nneeds of MoE models by enabling access to private data\ndistributed across countless edge devices. Recent research\n[10]–[13] has made some attempts to federated MoE training,\nprimarily by leveraging expert parallelism across edge devices.\nSpeciﬁcally, each edge device is assigned to train a pruned\nMoE model as a local expert model in parallel using its\nprivate data. After each training round, edge devices transmit\ntheir trained local expert models to a central server, which\naggregates them to construct the global MoE model.\nHowever, existing studies often require edge devices to train\nassigned local expert models which are essentially pruned\nMoE models. Their backbone MoE architecture remains over-\nloaded for resource-limited edge devices, preventing broad\ndevice participation in federated MoE traning. Additionally,\nthese studies focus on small-scale, custom-built MoE archi-\ntectures for computer vision tasks. In contrast, our approach\nleverages ubiquitous edge devices from tiny AIoT nodes to\nautonomous vehicles, regardless of hardware limitations, to\ncontribute local expertise to the global MoE model. Further-\nmore, our proposed solution targets industry-level MoE-based\nLLMs, underscoring its practical applicability.\nB. Federated Knowledge Distillation\nFederated knowledge distillation [25] has gained attention\nfor supporting heterogeneous edge devices in federated learn-\ning. Each edge device independently conﬁgures and trains its\nlocal model based on its hardware capabilities. Then, these\ntrained local models (teachers) share knowledge via soft-\nlabel predictions (logits) [22], [26] or intermediate features\n[33], [34]. The logits-based method trains the global model\n(student) to align its ﬁnal soft-label predictions with those of\nthe local models. The feature-based method leverages inter-\nmediate features from teachers as hints to guide the student in\nlearning the teachers’ feature representations, enabling more\nefﬁcient knowledge transfer. These learned feature representa-\ntions capture the teachers’ internal data analysis and reasoning\ncapabilities.\nHowever, most current methods simply assume that teacher\nand student models share homogeneous architectures (e.g.,\nQwen1.5-0.5B vs. Qwen1.5-1.8B [35]). Knowledge transfer\nis conducted through direct teacher-student logits or feature\nalignment in a shared latent space. To our knowledge, Fed-\nKMT [26] is the ﬁrst to explore federated knowledge transfer\nbetween large and small language models, closely aligning\nwith our work. The global LLM performance is improved by\ntransferring domain insights from local small models. How-\never, this work does not target MoE-based LLMs, nor does\nit identify the view-mismatch issue in federated knowledge\ndistillation between heterogeneous model architectures (e.g.,\nlarge and small language models). In our work, we address\nthis issue by proposing a VAA module to mitigate latent space\nmisalignment during cross-architecture federated knowledge\ndistillation.\n"}, {"page": 10, "text": "10\nVII. CONCLUSION\nIn this paper, we propose DEEPFUSION, a novel federated\nMoE training framework that enables heterogeneous resource-\nconstrained edge devices to join MoE training via federated\nknowledge distillation. By allowing each device to indepen-\ndently conﬁgure and train an on-device LLM, DEEPFUSION\naggregates diverse local domain knowledge from a spectrum\nof on-device LLMs, signiﬁcantly enriching the global MoE\nmodel’s capabilities. We address the critical challenge of cross-\narchitecture knowledge distillation by designing the View-\nAligned Attention (VAA) module, which aligns the latent\nrepresentations and predictive perspectives between hetero-\ngeneous model architectures, speciﬁcally on-device LLMs\n(teachers) and global MoE model (student). Experiments on\nreal-world medical and ﬁnancial datasets using industry-level\nMoE-based LLMs demonstrate that DEEPFUSION outperforms\nkey baselines, particularly in open-ended QA tasks. Overall,\nDEEPFUSION enables scalable, privacy-preserving federated\nMoE training on heterogeneous, resource-constrained devices,\naddressing both the shortage of high-quality public data and\nthe MoE model’s extensive training data requirements.\nREFERENCES\n[1] H. Touvron, T. Lavril, G. Izacard, X. Martinet et al., “LLaMA: Open\nand efﬁcient foundation language models,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2302.13971\n[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad et al., “GPT-4 technical\nreport,” 2024. [Online]. Available: https://arxiv.org/abs/2303.08774\n[3] D.\nGuo,\nD.\nYang,\nH.\nZhang,\nJ.\nSong\net\nal.,\n“DeepSeek-R1:\nIncentivizing reasoning capability in LLMs via reinforcement learning,”\n2025. [Online]. Available: https://arxiv.org/abs/2501.12948\n[4] B. Liu, L. Ding, L. Shen, K. Peng et al., “Diversifying the mixture-of-\nexperts representation for language models with orthogonal optimizer,”\nin European Conference on Artiﬁcial Intelligence (ECAI), 2024, pp.\n2966–2973.\n[5] Qwen\nTeam,\n“Qwen1.5-MoE:\nMatching\n7B\nmodel\nperformance\nwith 1/3 activated parameters”,” February 2024. [Online]. Available:\nhttps://qwenlm.github.io/blog/qwen-moe/\n[6] D. Dai, C. Deng, C. Zhao, R. Xu et al., “DeepSeekMoE: Towards\nultimate expert specialization in mixture-of-experts language models,” in\nAnnual Meeting of the Association for Computational Linguistics (ACL),\n2024, pp. 1280–1297.\n[7] J. Ludziejewski, J. Krajewski, K. Adamczewski, M. Pi´oro et al., “Scaling\nlaws for ﬁne-grained mixture of experts,” in International Conference\non Machine Learning (ICML), 2024, pp. 33 270–33 288.\n[8] P. Villalobos, A. Ho, J. Sevilla, T. Besiroglu et al., “Will we run\nout of data? Limits of LLM scaling based on human-generated data,”\nin International Conference on Machine Learning (ICML), 2024, pp.\n49 523–49 544.\n[9] H.\nB.\nMcMahan,\nE. Moore,\nD.\nRamage,\nS.\nHampson\net\nal.,\n“Communication-efﬁcient learning of deep networks from decentralized\ndata,” in International Conference on Artiﬁcial Intelligence and Statistics\n(AISTATS), 2017, pp. 1273–1282.\n[10] B. Guo, Y. Mei, D. Xiao, and W. Wu, “PFL-MoE: Personalized federated\nlearning based on mixture of experts,” in APWeb-WAIM International\nJoint Conference in Web and Big Data (APWeb-WAIM), 2021, pp. 480–\n486.\n[11] C. Dun, M. H. Garcia, G. Zheng, A. H. Awadallah et al., “FedJETS:\nEfﬁcient just-in-time personalization with federated mixture of experts,”\nin R0-FoMo: Workshop on Robustness of Few-shot and Zero-shot\nLearning in Foundation Models at NeurIPS, 2023, pp. 1–16.\n[12] Z. Zhan, W. Zhao, Y. Li, W. Liu et al., “FedMoE-DA: Federated mixture\nof experts via domain aware ﬁne-grained aggregation,” in International\nConference on Mobility, Sensing and Networking (MSN), 2024, pp. 122–\n129.\n[13] Y. Feng, Y. ao Geng, Y. Zhu, Z. Han et al., “PM-MoE: Mixture of\nexperts on private model parameters for personalized federated learning,”\nin ACM Web Conference (WWW), 2025, pp. 1–13.\n[14] Y. Zheng, Y. Chen, B. Qian, X. Shi et al., “A review on edge large\nlanguage models: Design, execution, and applications,” ACM Computing\nSurveys, vol. 57, no. 8, pp. 209:1–209:35, 2025.\n[15] Y. D. Kwon, R. Li, S. I. Venieris, J. Chauhan et al., “TinyTrain:\nResource-aware task-adaptive sparse training of DNNs at the data-scarce\nedge,” in International Conference on Machine Learning (ICML), 2024,\npp. 1–32.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit et al., “Attention is\nall you need,” in Annual Conference on Neural Information Processing\nSystems (NeurIPS), 2017, pp. 1–11.\n[17] M. Xia, M. Artetxe, C. Zhou, X. V. Lin et al., “Training trajectories of\nlanguage models across scales,” in Annual Meeting of the Association\nfor Computational Linguistics (ACL), 2023, pp. 13 711–13 738.\n[18] D. Lepikhin, H. Lee, Y. Xu, D. Chen et al., “GShard: Scaling giant\nmodels with conditional computation and automatic sharding,” in In-\nternational Conference on Learning Representations (ICLR), 2021, pp.\n1–23.\n[19] A. Radford, J. Wu, R. Child, D. Luan et al., “Language models are\nunsupervised multitask learners,” OpenAI Blog, pp. 1–24, 2019.\n[20] P.\nZhang,\nG.\nZeng,\nT.\nWang,\nand\nW.\nLu,\n“TinyLlama:\nAn\nopen-source\nsmall\nlanguage\nmodel,”\n2024.\n[Online].\nAvailable:\nhttps://arxiv.org/abs/2401.02385\n[21] W. Wang, H. Bao, S. Huang, L. Dong et al., “MiniLMv2: Multi-\nhead self-attention\nrelation distillation for compressing pretrained\ntransformers,”\n2021. [Online].\nAvailable:\nhttps://arxiv.org/abs/2012.\n15828\n[22] T. Xia, J. Ren, W. Rao, Q. Zu et al., “AeroRec: An efﬁcient on-device\nrecommendation framework using federated self-supervised knowledge\ndistillation,” in IEEE Conference on Computer Communications (INFO-\nCOM), 2024, pp. 121–130.\n[23] T. Lin, L. Kong, S. U. Stich, and M. Jaggi, “Ensemble distillation for\nrobust model fusion in federated learning,” in Annual Conference on\nNeural Information Processing Systems (NeurIPS), 2020, pp. 1–13.\n[24] X. Wang, M. Salmani, P. Omidi, X. Ren et al., “Beyond the Limits:\nA survey of techniques to extend the context length in large language\nmodels,” in International Joint Conference on Artiﬁcial Intelligence\n(IJCAI-24), 2024, pp. 8299–8307.\n[25] H. Seo, J. Park, S. Oh, M. Bennis et al., “Federated knowledge distilla-\ntion,” in Machine Learning and Wireless Communications.\nCambridge\nUniversity Press, 2022, pp. 457–485.\n[26] T. Fan, G. Ma, Y. Kang, H. Gu et al., “FedMKT: Federated mutual\nknowledge transfer for large and small language models,” in Interna-\ntional Conference on Computational Linguistics (COLING), 2025, pp.\n243–255.\n[27] Z. Hao, J. Guo, K. Han, Y. Tang et al., “One-for-All: Bridge the gap\nbetween heterogeneous architectures in knowledge distillation,” in An-\nnual Conference on Neural Information Processing Systems (NeurIPS),\n2023, pp. 79 570–79 582.\n[28] S. Rajbhandari, C. Li, Z. Yao, M. Zhang et al., “DeepSpeed-MoE:\nAdvancing mixture-of-experts inference and training to power next-\ngeneration AI scale,” in International Conference on Machine Learning\n(ICML), 2022, pp. 18 332–18 346.\n[29] C. Hu, Y. Kang, and B. Li, “Communication-efﬁcient MoE ﬁne-tuning\nwith locality-aware expert placement,” in IEEE International Conference\non Distributed Computing Systems (ICDCS), 2025, pp. 1–11.\n[30] P. Qiu,\nC.\nWu, X. Zhang,\nW. Lin\net\nal.,\n“Towards\nbuilding\nmultilingual language model for medicine,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2402.13963\n[31] “FinGPT FIQA-QA Dataset,” https://huggingface.co/datasets/FinGPT/\nﬁngpt-ﬁqa qa, accessed: 2025-07-31.\n[32] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang et al., “Judging LLM-as-\na-judge with MT-bench and chatbot arena,” in Annual Conference on\nNeural Information Processing Systems (NeurIPS), 2023, pp. 1–29.\n[33] Z. Yang, Y. Zhang, Y. Zheng, X. Tian et al., “FedFed: Feature distilla-\ntion against data heterogeneity in federated learning,” in International\nConference on Neural Information Processing Systems (NeurIPS), 2023,\npp. 1–32.\n[34] Z. Shao, B. Li, Z. Wang, Y. Yang et al., “FedUFD: Personalized edge\ncomputing using federated uncertainty-driven feature distillation,” in\nIEEE International Conference on Computer Communications (INFO-\nCOM), 2025, pp. 1–10.\n[35] J. Bai, S. Bai, Y. Chu, Z. Cui et al., “Qwen Technical Report,” 2023.\n[Online]. Available: https://arxiv.org/abs/2309.16609\n"}]}