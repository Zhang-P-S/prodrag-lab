{"doc_id": "arxiv:2512.13700", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.13700.pdf", "meta": {"doc_id": "arxiv:2512.13700", "source": "arxiv", "arxiv_id": "2512.13700", "title": "Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records", "authors": ["Mitchell A. Klusty", "Elizabeth C. Solie", "Caroline N. Leach", "W. Vaiden Logan", "Lynnet E. Richey", "John C. Gensel", "David P. Szczykutowicz", "Bryan C. McLellan", "Emily B. Collier", "Samuel E. Armstrong", "V. K. Cody Bumgardner"], "published": "2025-12-03T14:10:12Z", "updated": "2025-12-03T14:10:12Z", "summary": "Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.13700v1", "url_pdf": "https://arxiv.org/pdf/2512.13700.pdf", "meta_path": "data/raw/arxiv/meta/2512.13700.json", "sha256": "119d6836f280c5485804321d064765c18ea1f5aa57120a2f9fb654d0c8540e5d", "status": "ok", "fetched_at": "2026-02-18T02:25:36.391349+00:00"}, "pages": [{"page": 1, "text": "Leveraging LLMs for Structured Data Extraction from Unstructured Patient \nRecords \nMitchell A. Klusty1, BS1†, Elizabeth C. Solie1†, Caroline N. Leach1, BS1, W. Vaiden Logan, \nBS1, Lynnet E. Richey, BS2, John C. Gensel, PhD2, David P. Szczykutowicz, BA2, Bryan C. \nMcLellan, BS2, Emily B. Collier, MS1, Samuel E. Armstrong, MS1, V. K. Cody \nBumgardner, PhD1 \n1Center For Applied Artificial Intelligence, University of Kentucky, Lexington, KY  \n2Spinal Cord and Brain Injury Research Center and Department of Physiology, University \nof Kentucky College of Medicine, Lexington, KY \n†These authors contributed equally and share first authorship. \nAbstract \nManual chart review remains an extremely time-consuming and resource-intensive component of clinical research, \nrequiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. \nWe present a secure, modular framework for automated structured feature extraction from clinical notes leveraging \nlocally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and \nAccountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented \ngeneration (RAG) and structured response methods of LLMs into a widely deployable and scalable container to \nprovide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across \nmultiple medical characteristics present in large bodies of patient notes when compared against an expert-\nannotated dataset and identified several annotation errors missed in manual review. This framework demonstrates \nthe potential of LLM systems to reduce the burden of manual chart review through automated extraction and \nincrease consistency in data capture, accelerating clinical research. \nIntroduction \nElectronic Health Records contain a wealth of unstructured textual data, including physician notes, discharge \nsummaries, and consultation reports. These narrative documents are rich in clinical detail, capturing nuances of \npatient histories, diagnostic reasoning, treatment responses, and care planning that are often absent from structured \nfields. However, the unstructured nature of these records presents significant challenges for areas where they are \nmost useful such as clinical decision support, cohort identification for research studies, quality improvement \ninitiatives, and population health analytics. These tasks often require structured, machine-readable information, \nnecessitating reliable extraction of relevant features from the unstructured EHRs. Historically, this extraction has \nbeen performed either by manual annotation or rule-based systems, both of which are labor-intensive, error-prone, \nand difficult to scale when applied to a high volume of patients.  \nRecent advances in LLMs have opened new opportunities for automating clinical information extraction with \nunprecedented accuracy and flexibility. LLMs are deep neural networks trained on massive corpora of human \nlanguage and can perform a wide range of natural language processing (NLP) tasks with little to no task-specific \ntraining. In the clinical domain, they offer the ability to generalize across diverse documentation styles, specialty-\nspecific vocabularies, and note structures, greatly reducing the need for handcrafted rules or large labeled datasets. \nThis has the potential to significantly accelerate the traditionally slow and manual process of transforming \nunstructured clinical narratives into structured data, making information more accessible, actionable, and useful for \nboth patient care and secondary uses of health data. \nTo harness this potential, we present a scalable, clinician-friendly tool that leverages open-source LLMs to automate \nthe extraction of structured features from EHR notes, with rigorous protections to safeguard sensitive patient data, \nwith rigorous protections to safeguard sensitive patient data.  The system processes heterogeneous patient data \noriginating from institutional EHR platforms like Epic1, accessed through the institution’s REDCap2,3 environment, \nwhich provides secure, audit-controlled data management. Our system uses various open-source models such as \nQwen/QwQ-32B 4 to perform inference within an on-premises, HIPAA-compliant5 infrastructure, ensuring data \nprivacy and regulatory adherence. Output structure is governed by customizable and OpenAI-formatted tools6 that \ndescribe the desired schema, enabling consistent formatting across various clinical applications. Where available, we \nemploy these tool calling features to improve the precision and flexibility of field extraction. \n"}, {"page": 2, "text": "Tool calling was originally designed as a mechanism for enabling LLMs to execute external functions, identifying \nrelevant pieces of input text and supplying it as arguments for the function calls, similar to filling in blanks on a \nform to instruct the program. Although this capability was created for software automation, it also provides a \npowerful and intuitive way to perform structured feature extraction from clinical text. One of the central challenges \nin clinical text analysis is that patient notes are written in natural language. Computers work best when information \nis provided in a standard, structured format. Tool calling bridges the gap between rich, narrative, unstructured text \nand machine-readable, structured, database-ready information. \nIn our system, the LLM is prompted with a set of clinical notes and instructed to provide a specific set of features \n(e.g. diagnoses, dates, symptoms, etc.) that it can identify from those notes as a machine-readable JSON object, \nmirroring the schema for a function call in software development. This utilizes the natural language processing \ncapabilities of LLMs to enforce structural consistency across outputs, allowing the model to populate structured \nclinical variables such as diagnosis, dates, and supporting evidence, much like a physician reading the notes and \nfilling in the fields in a form. With the data in this structured format, it can be reliably stored in a database, used for \nstatistical analysis, or integrated into other components of clinical decision support processes.  \nImportantly, the system is designed to be accessible to clinicians without technical backgrounds. Through a \nstreamlined graphical interface, users can define the fields they wish to extract using simple, intuitive forms. This \nremoves the need for programming or familiarity with model prompting techniques. Once configured, the system \nsecurely processes patient notes and produces structured outputs that can be directly integrated into clinical \nworkflows, research pipelines, or data warehousing platforms. \nThis work demonstrates how state-of-the-art open-source LLMs can be operationalized in a practical and compliant \nmanner to unlock valuable insights from unstructured clinical documentation. By bridging the gap between complex \nAI models and real-world healthcare needs, our tool empowers clinicians and researchers to make better use of their \ndata with minimal technical overhead. \nMethods \nThe framework we developed was designed to be modular and scalable to support automated extraction of \nstructured features from unstructured clinical notes using LLMs. Deployment of this framework is intended to \noperate within HIPAA-compliant environments, provide secure data management, support reproducibility of results, \nand be adaptable to diverse institutional computing infrastructure. \nThe architecture is composed of four primary components: (1) integration with clinical data sources via REDCap, \n(2) secure data transfer and management processes, (3) containerized LLM embedding of reports and feature \nsearching and extraction, and (4) a web interface for definition of extraction parameters and initialization of \nextraction jobs. This section describes the technical implementation for these components. \n1. Clinical Data Sources and REDCap Integration \nPatient data was obtained from multiple institutional EHR systems: EPIC, which is used for contemporary data, \nSunrise Clinical Manager (SCM)7 and Allscripts Electronic Health Records (AEHR)8 for legacy notes predating \nEPIC integration. The data from these platforms contains both structured and unstructured clinical information. \nThese datasets contain medical record numbers (MRNs), encounter timestamps, and associated clinical text fields, \nincluding encounter summaries, imaging reports, laboratory results, and narrative clinical notes. All data was \nexported from these sources using institutionally approved and standardized procedures into the university’s \ninstance of REDCap, which serves as the centralized data repository for our system. REDCap was selected for its \nrobust data access controls, granular audit capabilities, and Application Programming Interface (API)-based \nprogrammatic integration features. This facilitates reproducible and traceable data workflows in compliance with \ninstitutional privacy governance, critical components when processing Protected Health Information (PHI). \nEach dataset retrieved from REDCap by our framework is exported as a structured tabular data file (CSV or XLSX), \nwith each record linked to a patient MRN. These MRNs serve as unique, stable identifiers across the various \nheterogeneous data sources, allowing all data to be aligned and aggregated for individual patients.  \n2. Secure Data Transfer and Local Mounting to Compute Container \nSecure transportation and storage of data is essential when PHI is present. To guarantee this, all data transfers and \nprocessing occur within HIPAA-compliant environments. Access to REDCap data is managed through the REDCap \nAPI. Rather than providing direct database access or requiring user interaction with the web frontend, the API acts \n"}, {"page": 3, "text": "as a controlled gateway whereby authorized users are issued unique API tokens that provide programmatic access to \nspecify exactly what data they can retrieve or modify. Each request through the API is automatically logged to \nsupport full auditability of all transactions. \nAt the start of the containerized feature extraction process (Methods, Section 3), the note files are downloaded to the \ncompute node through the REDCap API, and once extraction is complete, the results are exported back to REDCap \nthrough the API. When starting an extraction job, the user provides their API token to the system. It is securely \nstored as an environment variable and used for all data transfers to and from REDCap. If utilizing the web frontend \n(Methods, Section 4), for extraction job management, the token is supplied once by the user at job initialization, \nencrypted, and forwarded directly to the container. Upon completion of the extraction, the token is deleted to \nprevent possible credential leakage, which would allow unauthorized access to protected data by impersonating an \nauthorized user. This methodology maintains tight security over the token. \nAll transfers occur over encrypted HTTPS connections, protecting the sensitive information in transmission. \nTransferred files are stored on local volumes on the compute node and mounted directly to the extraction container. \nThis design permits a secure, auditable, and automated transfer protocol that maintains compliance with institutional \ndata governance and privacy requirements. \n3. Containerized LLM Batch Processing Pipeline \nExtraction jobs are performed as batch inference \nwithin Docker containers9 intended for computational \nclusters that use the Slurm10 workload manager. \nDocker-based containers ensure reproducibility and \nenvironment isolation with package dependencies \nversioned through internal registries. Each job \nlaunches a containerized inference environment using \nthe vLLM11 runtime, serving a locally hosted, \nconfigurable LLM. This means the model operates \nentirely on institutional servers rather than on an \nexternal cloud service, allowing all PHI to remain \nwithin the organization's existing security perimeter \nwhile giving administrators control over how the \nmodel is tuned, accessed, and audited. The inference \nframework is modular and follows standard OpenAI \nAPI formatting, allowing seamless configuration of \nvarious models without requiring modifications to the \nprocessing pipeline. The framework supports many \nopen-source models hosted on Hugging Face12, \nincluding the Llama13 and Qwen model families.  \nA major challenge presented in this process is that individual patients often have an extensive volume of clinical \nnotes, which can exceed the context length of current large language models. The context length of a model defines \nthe maximum number of tokens (roughly equivalent to words or sub-word units) it can process at once in both its \ninput and output. Most contemporary models support up to approximately 128,000 token context lengths. However, \ndespite this nominal capacity, there is often a marked decline in model comprehension and factual consistency as the \nnumber of input tokens increases, as noted by Liu et al.14 To accommodate these limitations, we implemented a \npreprocessing stage that removes extraneous content known to provide no clinical value, such as long XML \nmetadata sections or system generated headers that do not contain patient notes. After cleaning, the data can often \nstill be several million tokens, which is still too large for efficient, reliable extraction, particularly on more limited \ncompute infrastructure. To address this, we split the extraction process into two steps: (1) using RAG15 methods to \nlimit the search to only relevant reports and (2) chunked analysis of filtered reports. \n3.1 Vector Database Searching to Limit Report Context: One popular method of RAG is using text embeddings to \ncreate a vector database, which can be searched to identify documents that have similar meaning. This entails using \na language transformer model to convert each report into a semantic vector representation that enables similarity \nsearching. Reports for each individual MRN in the datasets are embedded with a configurable LLM-based \nembedding model, such as sentence-transformers/all-mpnet-base-v216, and the generated embeddings are L2-\nnormalized to support efficient comparison using cosine similarity metrics. The normalized embeddings are stored \nFigure 1 The design of the batch inference container \n"}, {"page": 4, "text": "in individual FAISS index files17 and Parquet metadata files18, serving as vector databases which allow semantic \nretrieval of reports that are most relevant to the target extraction features. Reports that exceed the context length of \nthe embedding model (often around 1024 tokens) are split into overlapping chunks to ensure all data is embedded \nwithout losing context at the edge of the chunks. \nThe user splits the target features into logical groupings that should be analyzed together. For instance, if the user \nwishes to identify if a patient has had a stroke and the date of the stroke, those features would be logically grouped, \nbut if they also want to identify if the patient is a smoker, that would be a second group. An LLM generates a strong \nset of search terms that relate to each individual feature grouping, embeds those features, normalizes the generated \nembeddings using L2-normalization, which simply rescales the vectors so they have a magnitude of 1. This makes \ncosine similarity comparisons dependent only on the direction of the embeddings, which maintains their semantic \nmeaning. The system then performs a cosine similarity search to identify all related documents with a higher \ncalculated similarity than a configurable threshold. Because the embeddings are normalized, the cosine similarity \nscores follow a distribution such that all scores fall in the range [-1, 1] where a score closer to 1 indicates a stronger \nsemantic similarity, while a score closer to -1 indicates very little semantic relation. All reports with a calculated \nscore greater than or equal to the configured threshold are selected for analysis for that feature group. \n3.2 Feature Extraction from Filtered Reports: The remaining, filtered reports are provided as context to the LLM \nto perform feature extraction. As described above, the tabular data encompasses multiple structural formats \noriginating from different EHR systems. To enable effective analysis by an LLM, this data must be arranged into a \nunified representation that provides longitudinal context across each patient’s history. For this purpose, when \nproviding the notes to the LLM for feature extraction, a single, consolidated report is generated for each of the \nselected reports. These consolidated reports aggregate and chronologically order the patient’s encounters, imaging \nand lab results, and notes, constructing a comprehensive timeline of clinical events tracked in the data.  \nFor a single analysis, the model must process the system prompt (instructions provided to explain the setting and \ndesired output), the tool definition, the entire text of the collected reports, and the structured output. The total tokens \nmust not exceed the context length of the chosen model. If the consolidated report is still too long, it must be split in \na controlled manner to avoid exceeding that limit. In such cases, the report is divided into smaller, overlapping, \nsequential chunks that fit within the model’s context window. Each chunk is processed independently by the LLM, \nand the resulting output features are subsequently merged through a rule-based reconciliation methodology to \nproduce a unified analysis across the patient’s full timeline. \nAs the extraction of each feature grouping from individual patient records is complete, the results are saved to a \nCSV file, creating a checkpointing system that allows restart on error or interruption without requiring reprocessing \nof previously analyzed data. Upon analysis of each unique record, the data is exported back to REDCap as a CSV. \nThe system is designed to be infrastructure-agnostic and scalable to available compute resources, with a minimum \nrequirement that the compute node has enough GPU memory to support the configured LLM. As analysis jobs run \ninside a Docker container, all dependencies, the user-provided configuration, and the runtime environment and \nresources are encapsulated and isolated. The container automatically initializes a vLLM runtime with the configured \nmodel and GPU allocations, supporting efficient inference with parallelizable calls that scale with available \nresources.  \nThe framework is compatible with both direct Docker execution and Slurm-based batch job scheduling. When \ndeployed on a compute cluster, users can submit feature extraction jobs as Slurm tasks. Slurm is an open-source \nworkload manager widely used in high-performance computing (HPC) environments to allocate resources, queue \njobs, and coordinate distributed execution. This allows resource-aware scheduling and queue management, as well \nas parallel execution over multiple nodes, supporting execution on diverse, HIPAA-compliant compute \ninfrastructures, from single-node GPU servers to multi-node clusters. \nThe LLM and embedding models are open-weight, locally hosted, available through Hugging Face, and configured \nin an OpenAI API-compatible format, allowing easy interchangeability between models. This modularity allows for \nstrong extensibility as new models and supporting frameworks are developed without requiring changes to the core \nprocessing logic. \n4. Web Frontend \nTo make the system accessible to clinicians and researchers, who often lack technical expertise with utilizing LLM \nsystems, we developed a dedicated web-based frontend that offers an intuitive interface for defining feature \n"}, {"page": 5, "text": "extraction schemas and configuring and initiating extraction processes using those schemas on the distributed \ncompute infrastructure.  \n4.1 Extraction Tool Definition: Rather than \nrequiring users to manually write JSON schemas or \ncomplex prompts, the interface wraps the OpenAI \ntool-calling specification through a guided, form-\nbased configuration workflow. This allows users to \ntranslate clinical knowledge and decision-making \nprocesses into valid, OpenAI-formatted tools for \nstructured extraction logic through high-level, front-\nend interactions. \nUsers define the clinical features they wish to \nextract from patient notes by creating a Tool \ncomposed of modular Fields. For each Field, users \nspecify the name, description, data type (e.g., string, \nnumber, integer, Boolean, array, or object), and \nwhether it is required. Required Fields indicate that \nthe extraction process must always attempt to \nretrieve the value, and the model considers the \nextraction incomplete if it is not found. Additional \nconstraints, such as enumerated options for a field, \nregex patterns for strings, minimum/maximum \nbounds for numeric fields, and default values, allow \nusers to encode clinical feature guardrails without \nrequiring additional code.  \nThis configuration design supports both simple and \nmore hierarchical data structures. For instance, a \nuser may define a Tool containing simple Fields such as “Diagnosis” (string), “Age” (integer), and “Smoker Status” \n(Boolean). More complex structures can be modeled by nesting Fields within objects or arrays. A “Blood Pressure” \nField may be represented as an object containing “Systolic” and “Diastolic” subfields, and a medication list can be \ndefined as an array of objects with subfields for “Name” and “Dosage”. This allows extraction of basic and more \ncomplex features from the patient notes, mirroring the variety of data and relationships that are common in clinical \ndocumentation. \nThe Tool structure is compiled into a fully defined OpenAI-compatible JSON schema. This schema serves as the \nformal tool description utilized by the backend LLM, promoting standardized, cross-model feature extraction. By \nutilizing this schema, the system reliably receives a reproducible, type-safe, user-constrained, structured output \nwhile minimizing the technical barriers for such LLM interaction. In practice, this frontend functions as a clinically \noriented wrapper around the OpenAI tool-calling framework, allowing domain experts to define structured \nextraction tasks through intuitive interactions rather than technical specification.  \n4.2 Job Initiating: As previously discussed, because the patient data for this system are generally subject to HIPAA \nprotection, the feature extraction pipeline must be performed within institutionally secured compute environments. \nTo enable accessibility to the backend system without exposing PHI, the front-end is designed to exist externally \nfrom the HIPAA environment, only communicating metadata and configuration parameters to the backend. This \nallows the front-end to act independently of patient data, functioning as a job orchestration interface, while the \nbackend container performs all operations that interact directly with PHI.  \nUsers initiate an extraction job by specifying the Tool to be used for extraction, selecting the embedding and LLM \nmodels for inference, and providing delegated access to the patient note source files in REDCap. The extraction \ncontainer is launched with these parameters set as environment variables. As described in Methods, Section 2, the \nuser supplies the paths of the files in REDCap targeted for feature extraction, and an API token with access to those \nfiles is used to securely transport files to the compute node. The extraction process is then executed: the LLM \npipeline processes retrieve the specified feature set, and that feature set is exported to REDCap. Upon completion, \nthe container is torn down, deleting the API token from the compute environment to prevent credential reuse, and \nthe web frontend is notified that the extraction is completed. The initial provisioning of configuration for the \nFigure 2 User interface form for creating a Tool \n"}, {"page": 6, "text": "container and this notification of status are the only communication between the frontend and backend, providing a \nstrict separation in the layers that prevent the frontend from ever interacting with PHI. \n4.3 User Access Management: Though the front-end system is intentionally designed without direct access to \npatient data, strong authentication and authorization controls are still essential for user-defined Tools and extraction \njobs. Authentication is handled through CILogon19, an OAuth 2.020 identity provider that federates login credentials \nacross academic institutions. CILogon connects directly to institutional single sign-on (SSO) systems, allowing our \nfrontend to leverage established identity verification processes, multi-factor authentication (MFA) protocols and \ninstitutionally governed account management. \nAuthorization to specific resources within the platform is managed by a locally maintained role-based access \nmanagement (RBAC) system. Tools and extraction jobs are provided three-tiers of roles with ascending permissions, \nread, write, and manage, which define the actions a user may take on these resources.  \n• \nRead access on a Tool allows the user to view the Tool’s structure and initiate an extraction job with it. On \nextraction jobs, read access permits viewing the status.  \n• \nWrite access on a Tool includes read privileges and allows editing the Tool’s fields and configuration.  \n• \nManage access grants full control over it, including delegation of permissions for the Tool or extraction \njob. \nWhen a Tool is created or a job launched, the author is given manage permissions and may assign or revoke access \nto other users. This supports collaborative development across research teams within the system while providing an \nRBAC-based model to ensure proper authorization to user-generated resources. \nResults \nTo evaluate the performance of this system, we used a curated dataset obtained from the University of Kentucky’s \nEHR repositories. A reference, “gold-standard” set of patient-level feature annotations was independently created by \ndomain experts from the Spinal Cord and Brain Injury Research Center (SCoBIRC). These experts manually \nreviewed and annotated target features from the patient notes, making a benchmark set of 100 patients that was \ncompared to results extracted by the system. For this experiment, the container was provided one NVIDIA A100 \nGPU for inference to gauge the system’s performance on comparatively low compute resources. The embedding \nmodel used was sentence-transformers/all-mpnet-base-v2, chosen for its strong relationship capturing capabilities16. \nThe selected LLM was Qwen/QwQ-32B, chosen for its competitive evaluation metrics and strong reasoning \ncapabilities.5 Also, with tool calling enabled for the vLLM deployment, Qwen/QwQ-32B at 16-bit quantization \noccupies approximately 71GB of the available 80 GB on the provided A100 GPU. \nThe assessment focused on five clinically relevant feature groupings important for study participant criteria filtering: \nTraumatic Spinal Cord Injury (SCI), Myocardial Infarction (MI), Stroke, Type 2 Diabetes Mellitus (T2DM), and \nTransient Ischemic Attack (TIA). These groups were each comprised of a set of structured elements that were the \ntargets for extraction by the system. Table 1 summarizes those features. \nFeature Group \nExtracted Elements \nDefinition \nTraumatic Spinal Cord \nInjury (SCI) \nOccurrence \nWhether or not the patient is noted as having sustained a \ntraumatic SCI \nDate \nEarliest documented date of SCI \nMyocardial Infarction (MI) \nOccurrence \nWhether the patient has ever had a myocardial infarction. \nDate \nEarliest documented MI date. \nStroke \nOccurrence \nWhether the patient has ever had a stroke. \nDate \nEarliest documented stroke date. \nTransient Ischemic Attack \n(TIA) \nOccurrence \nWhether the patient has ever had a TIA. \nDate \nEarliest documented TIA date. \nType 2 Diabetes Mellitus \n(T2DM) \nOccurrence \nWhether the patient has a diagnosis of T2DM. \nDate \nEarliest documented diagnosis date. \nTable 1 Breakdown of features extracted from patient notes to assess quality of extraction system \n"}, {"page": 7, "text": "Using the extraction methodology detailed in Methods, Section 3, the system processed all patient notes associated \nwith the gold-standard dataset. For each patient, the model extracted the targeted features as structured output which \nwas then saved to a CSV file and exported securely back to REDCap. We computed the agreement metrics between \nthe expert annotations and the model-extracted features by aligning value agreement of Boolean fields (i.e. if the \nexpert and the model agreed an SCI occurred for a particular patient) and determining if the year from extracted \ndates was agreed upon. We calculated the precision, recall, and F1-score metrics for each of the metrics, which are \npresented in Table 2. \nFeature Group \nFeature \nPrecision \nRecall \nF1-Score \nAccuracy \nTP \nTN \nFP \nFN \nTraumatic Spinal \nChord Injury \nOccurrence \n0.8413 \n0.8689 \n0.8548 \n0.82 \n53 \n29 \n10 \n8 \nDate \n0.5645 \n0.8140 \n0.6667 \n0.65 \n35 \n30 \n27 \n8 \nMyocardial \nInfarction (MI) \nOccurrence \n0.7000 \n0.7778 \n0.7368 \n0.95 \n7 \n88 \n3 \n2 \nDate \n0.200 \n1.000 \n0.3333 \n0.92 \n2 \n90 \n8 \n0 \nStroke \nOccurrence \n0.2143 \n0.5000 \n0.3000 \n0.86 \n3 \n83 \n11 \n3 \nDate \n0.1538 \n0.6667 \n0.2500 \n0.88 \n2 \n86 \n11 \n1 \nTransient Ischemic \nAttack (TIA) \nOccurrence \n0.4000 \n1.0000 \n0.5714 \n0.97 \n2 \n95 \n3 \n0 \nDate \n0.0000 \n0.0000 \n0.0000 \n0.96 \n0 \n95 \n5 \n0 \nType 2 Diabetes \nMellitus (T2DM) \nOccurrence \n0.8333 \n0.7895 \n0.8108 \n0.93 \n15 \n78 \n3 \n4 \nDate \n0.0000 \n0.0000 \n0.0000 \n0.85 \n0 \n85 \n14 \n1 \nTable 2 Results of the extraction agreement with expert-labeled \"gold-standard\" dataset.  \nTP=True Positive, TN=True Negative, FP=False Positive, FN = False Negative \nFalse positives, in this case, indicate the model found an affirmative result that the expert annotation labeled to the \nnegative. For Booleans, this means the model extracted true for the field where the expert labeled false (e.g. the \nmodel indicated there was an SCI while the annotator found there was not according to the provided definition of \nSCI). For date fields, this means the model extracted a date, and the year matched the expert annotated year. False \nnegatives indicate cases where no feature was extracted, but the expert annotation provided a label. For Booleans, \nthis means the model extracted false where the manual label was true. For dates, the model did not find a date, but \nthe expert label did.  \nThe lowest performing extracted field is the Traumatic SCI date. Aside from this field, each of the extractions \nperformed with >82% agreement with the manually annotated data. The primary sources of error were false \npositives, with a small number of false negatives across the feature groups. Qualitative assessment of the individual \ndisagreements indicated that many differences stemmed from underdeveloped or incomplete instruction prompts or \ntool descriptions, especially in cases where additional clinical context (e.g. severity of SCI that should be included) \nwas needed to guide the model’s reasoning. When prompts were refined to provide stricter criteria to the extraction, \nprecision and recall improved, showing the important role of prompting in extraction quality. \nIn interpreting these results, it is important to acknowledge the inherent ambiguity that exists in generating expert-\nlabeled “gold-standard” datasets of this kind. Determining the earliest date of a condition is often not \nstraightforward, as EHR narratives often reference historical events, prior hospitalizations, or findings documented \nsecond-hand in subsequent encounters. For example, notes may state that a traumatic injury occurred “about a \ndecade ago” or when the patient “was a young child”.  Such descriptions led annotators to record an approximate \ndate of diagnosis, while the program yielded false negatives. This prevarication in the source documents introduces \nunavoidable uncertainty to date-level annotations, which makes the ground truth for those fields less absolute than \nBoolean or more discrete variable fields. These ambiguities highlight the difficulty of manual review across vast sets \nof unstructured EHR data and suggest a disagreement between the model’s output and the annotation do not \nnecessarily indicate an error in the system. \nDespite these challenges, the system showed strong performance in identifying true negatives across all feature \ngroups, meaning it rarely ascribed conditions to patients where no strong evidence existed in the notes. The \nTraumatic SCI group provides a particularly illustrative example in comparison to the other feature groups, as there \nwere a higher proportion of patients who suffered from this condition than the others. As a result, its metrics provide \na more balanced view of positive and negative cases, making it more sensitive to recall and precision. The \nperformance of the model on this feature group, while slightly lower than the more sparse conditions, demonstrates \n"}, {"page": 8, "text": "the model maintains reasonable accuracy, even when there are more distinguishing characteristics and nuance in \narriving at a positive or negative finding. \nDuring evaluation, several apparent errors in the system’s output were found to be the result of omissions or \ninaccurate labels in the gold-standard annotations. For a small set of cases, the model correctly identified clinical \nevents or diagnoses that had been missed by human annotators. These instances were subsequently reviewed by \ndomain experts and confirmed as true positives. This validation shows not only the complexity of current manual \nchart review procedures, but also the potential of LLM-assisted review systems to both significantly increase speed \nin review completion and provide quality assurance for reviewed datasets. \nDiscussion \nThe results of this evaluation demonstrate the ability of the proposed framework to reliably extract clinical \nfeatures from EHR text using LLMs deployed locally in a HIPAA compliant environment, achieving high \naccuracy across evaluated feature groupings. The system’s modular architecture, which utilizes RAG \nmethods for report filtering and structured tool-based LLM extraction in a secure, containerized inference \npipeline, supports consistent performance across diverse clinical feature sets and broad deployability for \navailable institutional compute resources. Most modern LLMs perform very well with simple feature extraction, \nand there is literature supporting the ability of more powerful LLMs to extract complex characteristics as well,21 \nwhen provided full context for that feature15. The main limitation then becomes any insufficiencies in the specified \nprompt instructions or tool definitions for a particular feature. When definitions did not provide the proper \nconstraints around a field’s intended clinical meaning or the definition contained ambiguity, the model would \noccasionally disagree with the expert provided labels. Refinement of prompts and tool schema constraints \nsubstantially decreased these errors, underscoring the importance of high-quality, detailed, clinically informative \nspecifications. \nInterestingly, several cases where the model produced a false positive result were found to accurately represent the \nclinical attribute detailed in the EHR upon review and were mislabeled in the gold-standard set. This highlights the \ninherent difficulty of expert chart review and supports the potential of LLM systems for enhancing generation of \nsuch datasets on vast EHR repositories. Further, manual extraction of data is highly time-consuming, which often \nlimits the number of observations included in retrospective research. Manual review of 100 charts to generate \nreference data required an initial investment of approximately 30 minutes per patient chart. Subsequent quality-\ncontrol review of 10% of the charts by an independent reviewer resulted in an additional 30-minute investment per \nchart, representing approximately 55 hours of manual effort. Our system can complete the same analysis and \nquality-control review in less than one-tenth of the time while producing consistent and reproducible outputs. \nTo increase sample size without laborious manual chart review, many researchers rely on billing codes to identify \ndiagnoses. However, numerous studies have demonstrated that billing codes have limited specificity and sensitivity \nfor complex conditions. Our data set confirms this limitation. Despite selecting patients with international \nclassification of disease codes consistent with traumatic SCI, only 51% of charts manually reviewed reflected true \ntraumatic SCI. These findings underscore the need for tools that achieve both accuracy and efficiency in cohort \nidentification.   \nAs health systems continue their transition to fully electronic medical records, the volume of healthcare data \navailable for research will expand exponentially. Without systems capable of systematically codifying and \nstructuring this data, it will remain inaccessible for clinical research.  AI-enabled extraction tools allow for the \nanalysis of large volumes of nuanced clinical information. This data can be utilized to improve predictive modeling, \nadvance our understanding of patient trajectories, and support evidence-based clinical decision making. \nDevelopment of a secure web-based tool that delivers this level of nuanced health data to clinicians and researchers \nrepresents a critical step towards realizing these goals.   \nAcknowledgement \nThis research was supported in part by the National Institutes of Health under award numbers UL1TR001998 and \nT32NS077889. The content is solely the responsibility of the authors and does not necessarily represent the official \nviews of the NIH. \n \nReferences \n1. Epic Systems Corporation. Epic electronic health record [Internet]. [cited 2025 Nov 24]. Available from: \nhttps://www.epic.com/ \n"}, {"page": 9, "text": "2. Harris PA, Taylor R, Thielke R, Payne J, Gonzalez N, Conde JG, et al. Research electronic data capture \n(REDCap): a metadata-driven methodology and workflow process for providing translational research \ninformatics support. J Biomed Inform. 2009 Apr;42(2):377-81. \n3. Harris PA, Taylor R, Minor BL, Elliott V, Fernandez M, O’Neal L, et al. The REDCap consortium: building an \ninternational \ncommunity \nof \nsoftware \npartners. \nJ \nBiomed \nInform. \n2019 \nMay \n9;95:103208. \ndoi:10.1016/j.jbi.2019.103208 \n4. Yang A, Li A, Yang B, Zhang B, Hui B, Zheng B, et al. Qwen3 Technical Report. Preprint at arXiv. 2025 [cited \n2025 Nov 24]. Available from: https://arxiv.org/abs/2505.09388 \n5. Health Insurance Portability and Accountability Act of 1996. Pub L No. 104-191, 110 Stat 1936 (1996). \n6. OpenAI. \nFunction \ncalling \n[Internet]. \n[cited \n2025 \nNov \n24]. \nAvailable \nfrom: \nhttps://platform.openai.com/docs/guides/function-calling \n7. Altera Digital Health. Sunrise Clinical Manager (SCM) [Internet]. [cited 2025 Nov 24]. Available from: \nhttps://www.alterahealth.com/solutions/sunrise \n8. Allscripts Healthcare LLC. Allscripts Electronic Health Record (AEHR) [Internet]. [cited 2025 Nov 24]. \nAvailable from: https://www.allscripts.com/ \n9. Docker Inc. Docker [Internet]. [cited 2025 Nov 24]. Available from: https://www.docker.com/ \n10. Yoo AB, Jette MA, Grondona M. SLURM: Simple Linux Utility for Resource Management. In: Feitelson D, \nRudolph L, Schwiegelshohn U, editors. Job Scheduling Strategies for Parallel Processing. Berlin: Springer; \n2003. p. 44-60. \n11. Kwon W, Li Z, Zhuang S, Sheng Y, Zheng L, Yu CH, et al. Efficient Memory Management for Large \nLanguage Model Serving with PagedAttention. Preprint at arXiv. 2023 [cited 2025 Nov 24]. Available from: \nhttps://arxiv.org/abs/2309.06180 \n12. Hugging Face. Hugging Face: Open-Source Machine Learning Platform [Internet]. [cited 2025 Nov 24]. \nAvailable from: https://huggingface.co/ \n13. Grattafiori A, Dubey A, Jauhri A, Pandey A, Kadian A, Al-Dahle A, et al. The Llama 3 Herd of Models. \nPreprint at arXiv. 2024 [cited 2025 Nov 24]. Available from: https://arxiv.org/abs/2407.21783 \n14. Liu N, Lin K, Hewitt J, Paranjape A, Bevilacqua M, Petroni F, et al. Lost in the Middle: How Language Models \nuse \nLong \nContexts. \nPreprint \nat \narXiv. \n2023 \n[cited \n2025 \nNov \n24]. \nAvailable \nfrom: \nhttps://arxiv.org/abs/2307.03172 \n15. Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, et al. Retrieval-Augmented Generation for \nKnowledge-Intensive NLP Tasks. Preprint at arXiv. 2020 [cited 2025 Nov 24]. Available from: \nhttps://arxiv.org/abs/2005.11401 \n16. Song K, Tan X, Qin T, Lu J, Liu T-Y. MPNet: Masked and Permuted Pre-training for Language Understanding. \nPreprint at arXiv. 2020 [cited 2025 Nov 25]. Available from: https://arxiv.org/abs/2004.09297 \n17. Johnson J, Douze M, Jegou H. Billion-Scale Similarity Search with GPUs. IEEE Trans Big Data. \n2021;7(3):535-547. doi:10.1109/TBDATA.2019.2921572 \n18. Apache Software Foundation. Apache Parquet: File Format Metadata [Internet]. [cited 2025 Nov 25]. Available \nfrom: https://parquet.apache.org/docs/file-format/metadata/ \n19. Basney J, Fleury T, Gaynor J. CILogon: A federated X.509 certification authority for cyberinfrastructure logon. \nIn: Proceedings of the XSEDE 2013 Conference: Gateway to Discovery. ACM International Conference \nProceeding Series; 2013. Article 53. doi:10.1145/2484762.2484791 \n20. Hardt D. The OAuth 2.0 Authorization Framework. RFC 6749. Internet Engineering Task Force; 2012. [cited \n2025 Nov 24]. Available from: https://datatracker.ietf.org/doc/html/rfc6749 \n21. McInerney DJ, Young G, van de Meent JW, Wallace BC. CHiLL: Zero-shot Custom Interpretable Feature \nExtraction from Clinical Notes with Large Language Models. Preprint at arXiv. 2023 [cited 2025 Nov 25]. \nAvailable from: https://arxiv.org/abs/2302.12343 \n"}]}