{"doc_id": "arxiv:2512.17267", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.17267.pdf", "meta": {"doc_id": "arxiv:2512.17267", "source": "arxiv", "arxiv_id": "2512.17267", "title": "AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators", "authors": ["Michael J. Ryan", "Yanzhe Zhang", "Amol Salunkhe", "Yi Chu", "Di Xu", "Diyi Yang"], "published": "2025-12-19T06:32:46Z", "updated": "2025-12-19T06:32:46Z", "summary": "Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. We present AutoMetrics, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from MetricBank, a collection of 48 metrics we curate, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal. AutoMetrics takes you from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. We show that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. We release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.17267v1", "url_pdf": "https://arxiv.org/pdf/2512.17267.pdf", "meta_path": "data/raw/arxiv/meta/2512.17267.json", "sha256": "5ec16b2c6c6f99a31f8d316a84f2d72942861ab2397c8c3b82352c680d92d9e6", "status": "ok", "fetched_at": "2026-02-18T02:24:03.403319+00:00"}, "pages": [{"page": 1, "text": "Preprint\nAUTOMETRICS: APPROXIMATE HUMAN JUDGMENTS\nWITH AUTOMATICALLY GENERATED EVALUATORS\nMichael J. Ryan♠, Yanzhe Zhang♠, Amol Salunkhe♣, Yi Chu♣, Di Xu♣, Diyi Yang♠\n♠Stanford University\n♣American Express\nmichaeljryan@stanford.edu\nABSTRACT\nEvaluating user-facing AI applications remains a central challenge, especially in\nopen-ended domains such as travel planning, clinical note generation, or dialogue.\nThe gold standard is user feedback (e.g., thumbs up/down) or behavioral signals\n(e.g., retention), but these are often scarce in prototypes and research projects,\nor too-slow to use for system optimization. We present AutoMetrics, a frame-\nwork for synthesizing evaluation metrics under low-data constraints. AutoMetrics\ncombines retrieval from MetricBank, a collection of 48 metrics we curate, with\nautomatically generated LLM-as-a-Judge criteria informed by lightweight human\nfeedback. These metrics are composed via regression to maximize correlation\nwith human signal. AutoMetrics takes you from expensive measures to inter-\npretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall\ncorrelation with human ratings by up to 33.4% over LLM-as-a-Judge while requir-\ning fewer than 100 feedback points. We show that AutoMetrics can be used as a\nproxy reward to equal effect as a verifiable reward. We release the full AutoMet-\nrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.\nProduct\nLLM Generated \nDescription\nHuman \nFeedback\nYardley of London \nMoisturizing Soap Sweet \nSummer Aloe and \nAvocado 3+1\nIndulge in the light, refreshing scent of \nYardley of London Moisturizing Soap Sweet \nSummer Aloe and Avocado 3+1…\nMERMAID Vegetable \nGlycerin Bar Soap\nExperience the refreshing sensation of the \nMERMAID Vegetable Glycerin Bar Soap. \nThis aqua fresh bar is infused…\nLA Splash Cosmetics \nSoft Liquid Matte Blood \nRed Lipstick - LIP \nCOUTURE (Poison \nAchieve a velvety, matte finish that lasts all \nday with LA Splash Cosmetics Soft Liquid \nMatte Lipstick in the shade Poison Apple…\n…\n…\n…\nProhibited Content Avoidance\nNo inclusion of links, generic phrases, or references to product weaknesses.\n*Prompt Optimized LLM Judge*\nYou are an expert evaluator for e-commerce product descriptions … Follow \nthese steps rigorously: 1.**Analyze Task Requirements** … 2.**Structural \nEvaluation** … 3.**Content Accuracy** …  4. **SEO and Voice** …\nTone and Voice (Rubric)\nUses active voice, positive language, and avoids mentioning weaknesses to \nalign with the task’s guidelines and create persuasive content.\nSearch Engine Optimization\nStrategic inclusion of keywords (e.g., product name, key features) to improve \nsearch engine visibility while maintaining natural flow.\n*Examples-Based LLM Judge*\nHere are some gold evaluation examples:  \n“Indulge in the light, refreshing scent … “ Score: 0 \n“Experience the refreshing sensation of the MERMAID …” Score: 1\nHuman Feedback Data (Likert Scale, Thumbs up/down, Behavioral)\nTop 5 AutoMetrics\n26.3%\n23.6%\n19.9%\n15.3%\n14.9%\nFigure 1: AutoMetrics takes you from expensive measures to interpretable automatic metrics. Here\nAutoMetrics generates useful metrics for evaluating LLM written product descriptions from user\nreviews from EvalGen (Shankar et al., 2024b). Percentages indicate relative importance of each\nmetric derived from regression coefficients.\n1\nINTRODUCTION\nModern AI systems now demonstrate massively multitask capabilities imparted through extensive\npretraining (Radford et al., 2019; Brown et al., 2020). Practitioners can rapidly prototype new AI-\nenabled tasks – from travel planning to code completion – at a pace much faster than the community\nThis paper reflects the academic work of the authors and does not represent or constitute the views, policies,\npositions, or practices of American Express or its affiliates.\n1\narXiv:2512.17267v1  [cs.CL]  19 Dec 2025\n"}, {"page": 2, "text": "Preprint\ncan craft domain specific metrics (Papineni et al., 2002; Lin, 2004; Xu et al., 2016). This new era,\nin which large language models can be adapted to virtually any domain, places mounting pressure\non evaluation practices. A divide is growing between tasks with easily verifiable rewards, such as\nmath (Glazer et al., 2024; Shao et al., 2024) and coding (Chen et al., 2021), while subjective and\nopen-ended tasks such as writing (Gurung & Lapata, 2025) remain difficult to measure. For these\ntasks, human evaluation remains the gold standard (Shankar et al., 2024b; Chiang et al., 2024).\nUnfortunately, human evaluation is costly, slow, and not scalable for every prototype or user popu-\nlation. Reward models offer an alternative (Mnih et al., 2015; Christiano et al., 2017), but they typi-\ncally require thousands of labels. The common alternative is rubric-based LLM-as-a-Judge methods\n(Li et al., 2023; Zheng et al., 2023; Liu et al., 2024), which rely on the assumption that system\nbehavior is clearly defined and are not guaranteed to follow given rubrics strictly (Tripathi et al.,\n2025). In reality, practitioners typically have access only to non-descriptive human signals (e.g.,\nthumbs up/thumbs down collected from users). In this setting, the problem is not only formulating\nthe rubric, but also discovering the underlying criteria that matter.\nThis highlights the need for dynamic, task-specific metric learning. Instead of relying exclu-\nsively on human judgment or fixed rubrics, evaluation itself must become adaptive. Current efforts\nhave emphasized making LLMs better evaluators of task-specific criteria (Liu et al., 2024; Kim\net al., 2025; Anugraha et al., 2025) or leveraging rubrics to optimize LLMs (Gunjal et al., 2025;\nViswanathan et al., 2025) but comparatively little work has focused on automatically generating the\nrubrics and criteria to be adaptively aligned with human judgment (Biyani et al., 2024; Ryan et al.,\n2025; Dunlap et al., 2025). Such adaptive evaluation is essential not only for easily assessing new\ntasks but also for optimizing evaluated systems based on real-time user feedback.\nWe introduce AutoMetrics, a method for dynamic metric induction that turns sparse, non-\ndescriptive human feedback into actionable and interpretable evaluators (Figure 1). Starting from a\ntask description and fewer than 100 human signals, AutoMetrics synthesizes candidate criteria, re-\ntrieves and adapts existing metrics, and composes them through regression into predictive measures\nof quality. Beyond simply identifying criteria, AutoMetrics grounds and weighs them, produc-\ning metrics that are both predictive and interpretable. This approach achieves up to 33.4% higher\nKendall correlation with human judgments than LLM-as-a-Judge baselines (§4), is data-efficient\nonly requiring ∼80 feedback points (§4.6), and even matches verifiable rewards when optimiz-\ning downstream AI systems (§5). Beyond accuracy, AutoMetrics reveals actionable insights into\nwhat users value. We release AutoMetrics as an open-source toolkit1, offering the community a\npowerful new way to evaluate and optimize AI applications at the speed of modern development.\n2\nRELATED WORK\nMetric Collections\nPrior work has organized collections of metrics primarily for the ease of use\non the part of the practitioner. When already using a library such as PyTorch (Paszke et al., 2019)\nor Huggingface (Wolf et al., 2020) it’s simple to utilize TorchMetrics (Nicki Skafte Detlefsen et al.,\n2022) or HuggingFace lighteval (Fourrier et al., 2023). Scikit Learn Metrics (Pedregosa et al.,\n2011) and NLTK metrics (Bird & Loper, 2004) were created with the same intentions. All text-\ngeneration metrics covered by these collections are also contained in our MetricBank collection.\nBeyond integrating with existing open source libraries, some metric collections are part of ML\nobservability frameworks like Evidently (EvidentlyAI, 2025), Galileo (Galileo, 2025), Scorecard.io\n(Doe & Devireddy, 2024), and DeepEval (ConfidentAI, 2025). Most metrics are tightly coupled with\ntheir observability platform, although Evidently and DeepEval offer open-source versions. While\nDeepEval offers a metric recommendation feature, it is based on a predefined decision tree of ques-\ntions like “Does your LLM application use Retrieval-Augmented Generation (RAG)?” and “Is LLM\nsafety a priority for you?”. Most similar to our work is the MetaMetrics collection (Winata et al.,\n2025), which computes a regression over multiple task-specific metrics for tasks like image caption-\ning and summarization to select the best combination of metrics. We compare our approach with\nMetaMetrics in Section 4 and find that our core thesis of adaptive metric generation is critical for\nevaluation in the low-data, novel task settings of interest.\n1https://github.com/SALT-NLP/autometrics\n2\n"}, {"page": 3, "text": "Preprint\nLocal Cultural Integration Rubric\nPractical Considerations\nAccommodation Options\nSafety & Health Considerations\nSpecificity and Detail Rubric\n…\nINFORMRewardModel\nLDLRewardModel\nSummaQA\nToxicity\n…\nLocal Cultural Integration Rubric\nPractical Considerations\nAccommodation Options\nSpecificity and Detail Rubric\nINFORMRewardModel\nLDLRewardModel\nSummaQA\nLocal Cultural Integration\nAccommodation Options\nINFORMRewardModel\nSpecificity and Detail\nLocal Experiences\n24.8%\n20.4%\n19.5%\n17.8%\n17.5%\nMetric Bank\nHuman Data\n…\n…\n…\n…\n…\n…\n…\n…\n5/5\n4/5\n2/5\n4/5\nTravel Planning Metric\nStep 1. Generate Metrics\nStep 2. Retrieve Top K Metrics\nStep 3. Regress Top N Metrics\nStep 4. Report\nMetrics all have associated \ndocs for help with retrieval\ni\nWe compute a PLS regression \non a training set of human data\ni\nFigure 2: AutoMetrics comprises four steps. (1) Generate: create task-specific candidate metrics\n(Single criteria, Rubric, Examples, MIPROv2). (2) Retrieve: from the generated candidates plus\nMetricBank, use ColBERT to prefilter to k′ metric cards and an LLM to select the final k. (3)\nRegress: fit a PLS model on the training set to weight and select metrics that predict human judg-\nments. (4) Report: produce a writeup with weights and correlations and details to guide adoption.\nLLM Based Evaluation\nLLM-as-a-Judge (Zheng et al., 2023) evaluation is increasingly popular\nwith the frequent improvement of LLM capabilities. Several works devise task-specific prompts to\nenable LLM-based evaluation for storytelling (Chiang & Lee, 2023), summarization (Wang et al.,\n2023; Hada et al., 2024; Wu et al., 2023), dialogue (Lin & Chen, 2023; Fu et al., 2024), knowledge\n(Bai et al., 2023), translation (Kocmi & Federmann, 2023), and more (Brake & Schaaf, 2024). An-\nother promising direction is devising frameworks and general methods for making LLM-as-a-Judge\nmore reliable. G-Eval (Liu et al., 2023) proposes breaking LLM evaluation into a step-by-step chain\nof thought and taking a weighted sum over the log probabilities of generating different scores. Chat-\nEval (Chan et al., 2024) simulates multiple perspectives by evaluating through multi-agent debate.\nSPADE (Shankar et al., 2024a) generates assertions for LLMs to verify based on labeled good and\nbad examples. VERDICT (Kalra & Tang, 2025) introduces judge-time scaling by decomposing\njudgments into composable units of reasoning, verification, debate, and aggregation steps. Though\nwe take inspiration from many of these frameworks, the most directly similar to our LLM-as-a-\nJudge steps in the AutoMetrics pipeline are DnA-Eval (Li et al., 2025) and EvalGen (Shankar et al.,\n2024b). DnA-Eval (Li et al., 2025) decomposes evaluation into rubric criteria and aggregates the\nresults across the criteria. EvalGen (Shankar et al., 2024b) elicits limited human feedback on gen-\nerated outputs, proposes criteria for evaluation based on this feedback, and iteratively refines the\ncriteria with a human-in-the-loop and LLM.\n3\nTHE AUTOMETRICS METHOD\nThe purpose of AutoMetrics is to produce metrics for subjective and novel AI-enabled tasks. Our\ngoal is to induce metrics that correlate strongly with human judgments while requiring minimal data\ncollection. To accomplish this, we present a general pipeline with four stages: (1) generate, (2)\nretrieve, (3) regress, and (4) report. These steps are visualized in Figure 2. Each stage involves\ndesign choices among several alternatives, which we empirically validate (§4.5).\n3.1\nMETRIC PRODUCTION\nGenerate\nFor sufficiently novel settings, generating criteria for LLM-as-a-Judge evaluation is es-\nsential. Broad coverage of evaluation criteria allows us to later filter down to what matters most. Ac-\ncordingly, our default configuration generates 10 Single Criterion LLM Judge metrics, 5 Rubric\nLLM-Judge metrics, 1 Example-based optimized LLM-Judge metric (fewshot), and 1 Prompt-\nOptimized LLM-Judge metric per run of AutoMetrics2. Optimized metrics require more LLM call-\ns/tokens to produce, while criteria and rubrics are relatively inexpensive. Unless otherwise specified,\nwe use this configuration throughout the paper. Empirically, we find this mix of generated metrics\n2Design details and ablations are in Appendix E.2; we validate these choices across nearly 30 settings.\n3\n"}, {"page": 4, "text": "Preprint\ngeneralizes across diverse domains and tasks. For each metric, we also generate a Metric Card\ndocumenting its description, intended use, implementation details, and limitations (Appendix B).\nRetrieve\nIn addition to generated metrics, we leverage our MetricBank: a collection of 48 metrics\n(Appendix Table 4) drawn from the NLP literature, each implemented and documented with a Metric\nCard. Directly evaluating all metrics would be prohibitively expensive, so we instead use retrieval as\na filtering step. We treat Metric Cards as documents, and use a description of the evaluation setting\nor task as the search query. Retrieval is performed using a hybrid ColBERT + LLM approach,3\nnarrowing the candidate pool to metrics most relevant to the task at hand.\nRegress\nThe filtered pool of candidate metrics must still be combined into a predictive signal for\nhuman judgment. We normalize all metric scores to their z-scores and fit a Partial Least Squares\n(PLS) regression model. Intuitively, PLS projects the metric space onto the direction most predic-\ntive of human labels, then regresses labels along that axis. We choose PLS regression because it\nworks well under the constraints of our setting that: (1) the number of predictors (metrics) may be\ncomparable to or larger than the number of observations (data points), and (2) the predictors are\noften highly correlated. Concretely, with a single latent component, PLS finds a unit weight vector\nw⋆∈Rd that maximizes\nw⋆= arg max\n∥w∥2=1 cov(Xw, y)2,\nwhere X is the matrix of normalized metric scores and y is the vector of human labels. The latent\nscore is t = Xw⋆, and PLS then regresses the human labels on this latent score, yielding predictions\nˆy = tβ with coefficient β = t⊤y\nt⊤t .\nWe apply this procedure in two stages. In the first stage, we fit PLS using all candidate metrics and\nrank them by the magnitude of their weights in w⋆. We then select the top n metrics according to this\nranking. In the second stage, we refit PLS on this reduced set of n metrics to obtain a new projection\nt and corresponding predictions ˆy. As a final step, we remove negatively correlated LLM-generated\nmetrics, as they are designed to target positive correlation. We don’t apply this to existing measures\n(e.g., length can negatively correlate with conciseness).\n3.2\nMETRIC EVALUATION\nTo evaluate the quality of induced metrics, we draw on concepts of measurement validity from\nresearch (Borsboom et al., 2004) and testing (American Educational Research Association et al.,\n2014). We focus on three forms: “Content Validity”, “Criterion Validity”, and “Construct Validity”.\nContent Validity asks whether a metric represents the construct it is intended to measure. Although\ndirect quantification is difficult, we encourage transparency by releasing metric reports. Because\nour generated metrics rely on LLM judges, we also expose the reasoning traces of the judge LLM,\nallowing users to inspect whether assessments appear justified. These traces can further aid system\noptimization with AutoMetrics (§5).\nCriterion Validity Criterion validity measures correlation with a reference standard. In NLP, corre-\nlation with human labels has been the most widely used criterion (Banerjee & Lavie, 2005; Xu et al.,\n2016; Gehrmann et al., 2021). We assess criterion validity by comparing AutoMetrics to ground-\ntruth human labels. We report Kendall’s τ, which makes no distributional assumptions and simply\nchecks whether the rank order induced by a metric matches that of human judgments. This provides\na conservative estimate compared to Spearman’s ρ or Pearson’s r.\nConstruct Validity measures whether a metric captures an underlying abstract concept, such as\n“quality.” Both human judgments and AutoMetrics attempt to approximate “quality”. We draw from\nconvergent–discriminant validity (Campbell & Fiske, 1959) and operationalize construct validity as\nrobustness. A useful metric should penalize quality degradations (sensitivity) while remaining stable\nunder equivalent-quality variation. In order to quantify convergent-discriminant validity, we intro-\nduce two measurements: Sensitivity and Stability. To construct test cases, we use an LLM to gen-\nerate strategies for degrading outputs on a given dataset, and apply these to produce worse-quality\nperturbations. In contrast, same-quality perturbations are produced from a fixed set of hand-crafted\n3We ablate the selection algorithm in Appendix E.1.\n4\n"}, {"page": 5, "text": "Preprint\ntransformations—such as rephrasing, reordering, synonym replacement, or stylistic edits—that are\ndesigned to preserve the target evaluation dimension. Prompts are provided in Appendix C.\n• Sensitivity measures whether a metric assigns lower scores to degraded outputs. Let s(i)\norig\nand s(i)\nworse denote the normalized scores for the original and worse-quality perturbed outputs\nof sample i from a dataset of size |N|. Sensitivity is defined as:\nSensitivity = 1\nN\nN\nX\ni=1\n1\nh\ns(i)\nworse < s(i)\norig\ni\n• Stability measures whether a metric produces consistent scores when quality should be\npreserved. Let s(i)\nsame be the normalized score for a same-quality perturbation of sample i\nfrom a dataset of size |N|. Stability is defined as:\nStability = 1 −1\nN\nN\nX\ni=1\n\f\fs(i)\norig −s(i)\nsame\n\f\f.\nHigh sensitivity indicates strong penalization of degraded outputs, while high stability indicates\ninvariance to irrelevant variation. Both are desirable, and together they provide a general-purpose\nlens for evaluating how well a metric generalizes.\n4\nEXPERIMENTS AND EVALUATIONS: SHOWING AUTOMETRICS ARE VALID\nFor our experiments, we focus on showing that our AutoMetrics are valid across many tasks/domains\nand that they correlate better with human judgements than competitive baselines. We showcase\nAutoMetrics have high Criterion Validity and Construct Validity across several tasks.\n4.1\nTASKS\nDataset (Citation)\nTask\nDomain\n# Data\nFeedback\n# Eval Dim\nRefs\nIn-Distribution Tasks: some metrics in our bank were designed to directly evaluate these tasks.\nSimpEval (Maddela et al., 2023)\nSimplification\n`\n360\n1–100 Likert\n1\n✓\nHelpSteer2 (Wang et al., 2024)\nDialogue\nÜ\n20,324\n1–5 Likert\n5\n✗\nOut-of-Distribution Tasks: no metric is specifically designed for these – tests generalization and metric generation.\nEvalGen (Shankar et al., 2024b)\nProduct description\nL\n100\nBinary\n1\n✗\nRealHumanEval (Mozannar et al., 2025)\nCode completion\nÐ\n5,204\nBehavioral\n1\n✗\nCo-Gym (Shao et al., 2025)\nTravel planning\nÈ\n72\n1–5 Likert\n3\n✗\nTable 1: Overview of tasks. Icons: Ð Code Generation; L Data-to-Text Generation; Ü Dia-\nlogue/Chat; ` Education/Readability; È Travel Planning.\nIn order to evaluate our AutoMetrics method, we collect two types of tasks: In-Distribution Tasks,\nwhich are tasks where some of the metrics in our Metric Bank were designed to directly evaluate\nthe task, and Out-of-Distribution Tasks, which are tasks where no metric in particular was designed\nto assess the task. All of our tasks utilize human feedback for evaluation, encompassing behavioral\nfeedback, binary feedback (thumbs up/down), and Likert scale feedback, which is already collected\nas part of the dataset. We introduce all tasks in Table 1. In our main tables we present results for five\ndatasets and a single evaluation dimension from each: SimpEval (Maddela et al., 2023) (sentence\nsimplification score 1–100), HelpSteer2 (Wang et al., 2024) (Chatbot helpfulness 1–5), EvalGen\n(Shankar et al., 2024b) (Product Review Thumbs Up/Down), RealHumanEval (Mozannar et al.,\n2025) (accepted or rejected code edit), CoGym (Shao et al., 2025) (travel plan outcome rating 1–5).\nWe report evaluations on more settings in the Appendix results.\n4.2\nBASELINES\nWe include the following baselines: Best Existing Metric, where we run all 48 metrics (or 19\nmetrics for reference-free tasks), record their Kendall correlation on the validation set, and select\n5\n"}, {"page": 6, "text": "Preprint\nthe best metric to use for the task based on the validation correlation. MetaMetrics, where we take\nall the metrics from the MetaMetrics paper and compute an XGBoost Regression on the metrics\non the trainset (Winata et al., 2025). Finetuned LLM refers to training a ModernBERT-large\n(Warner et al., 2024) to predict the human annotation. We implement it by training LoRA adapters\n(Hu et al., 2021) with rank = 16 on all the attention, dense layers, and regression head, using a\nlearning rate of 5e −5 and a batch size of 16 for three epochs over the training data. For the LLM-\nJudge baseline, we use the original human annotation prompt for each task and provide it to an\nLLM. We include all of these prompts in Appendix C. DnA-Eval (Li et al., 2025) involves using\nan LLM to generate three dimensions where a user request may benefit from evaluation, along with\nweights for how to aggregate these dimensions. Then each of those dimensions is scored with an\nLLM-as-a-Judge, and finally aggregated based on the LLM-generated weights.\n4.3\nCRITERION VALIDITY (CORRELATION)\nWe report Kendall’s τ of all methods with GPT-4o-mini and Qwen-3-32B Reasoning in Table 2.\nIn-Distribution\nOut-of-Distribution\nMethod\nSimpEval\nHelpSteer2\nEvalGen\nRealHumanEval\nCoGym\nModel Agnostic\nBest Existing Metric\n0.246 ± 0.00 0.327 ± 0.00\n0.193 ± 0.00\n0.138 ± 0.00\n0.074 ± 0.00\nMetaMetrics (Winata et al., 2025)\n0.127 ± 0.01 0.204 ± 0.00\n-0.214 ± 0.01\n0.025 ± 0.01\n-0.119 ± 0.02\nFinetuned LLM\n0.076 ± 0.08 0.039 ± 0.03\n0.054 ± 0.05\n0.049 ± 0.06\n0.223 ± 0.20\nGPT-4o-mini Backbone\nLLM-Judge\n0.272 ± 0.02 0.259 ± 0.01\n0.161 ± 0.14\n0.069 ± 0.01\n0.199 ± 0.13\nDnA Eval (Li et al., 2025)\n0.234 ± 0.03 0.255 ± 0.02\n0.174 ± 0.16\n0.152 ± 0.01\n0.185 ± 0.10\nAutoMetrics (Ours)\n0.321 ± 0.04 0.324 ± 0.01\n0.334 ± 0.06\n0.160 ± 0.00\n-0.034 ± 0.17\nQwen-3-32B Backbone\nLLM-Judge\n0.294 ± 0.04 0.334 ± 0.02\n0.272 ± 0.13\n0.025 ± 0.01\n0.276 ± 0.19\nDnA Eval (Li et al., 2025)\n0.042 ± 0.04 0.260 ± 0.02\n0.232 ± 0.19\n0.071 ± 0.15\n0.353 ± 0.25\nAutoMetrics (Ours)\n0.316 ± 0.02 0.342 ± 0.01\n0.382 ± 0.05\n0.145 ± 0.00\n0.365 ± 0.08\nTable 2: Criterion Validity results showing Kendall’s Tau with 95% confidence intervals over 5\nindependent runs. AutoMetrics outperforms the baselines on all five tasks with Qwen3-32B and\nis within 95% confidence of the best for 4/5 tasks with GPT-4o-mini. On EvalGen, AutoMetrics\nimproves performance by 33.4% over the closest baseline (LLM Judge).\nAutoMetrics\ncorrelates\nbetter\nthan\nall\nbaselines\nacross\nall\nfive\ntasks.\nWe\nfind\nthat\nAutoMetrics\noutperforms\nall\nother\nexisting\nbaselines\non\nall\nfive\ntasks.\nWhile\nthe\nbest\nperforming\nbaseline\nis\nboth\ninconsistent\non\ndataset\n(LLM\nJudge\non\nSens Stab\nSens Stab\nSens Stab\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nscore (mean ± 95% CI)\nSimpEval\n(score)\nHelpSteer2\n(helpfulness)\nCoGym\n(outcome rating)\nNormal Baseline\nSensitivity\nStability\nFigure 3: Sensitivity/Stability of AutoMet-\nrics for SimpEval, HelpSteer2, and CoGym.\nAutoMetrics are sensitive to negative pertur-\nbations and stable on neutral perturbations.\nSimpEval, HelpSteer, EvalGen; DnA Eval on Re-\nalHumanEval and CoGym) and on the underlying\nmodel used (Existing Metrics outperform GPT-4o-\nmini but not Qwen3-32B). In contrast, AutoMetrics\nis consistently the best option regardless of dataset or\nunderlying model. On all datasets besides HelpSteer\nand CoGym, the AutoMetrics performance exceeds\nall baselines by greater than the 95% confidence in-\nterval. In general, AutoMetrics is the best choice for\nhigher correlation with human ratings.\n4.4\nCONSTRUCT VALIDITY (ROBUSTNESS)\nTo measure construct validity, we take inspiration\nfrom convergent-discriminant validity and show that\nAutoMetrics are strong predictors when output qual-\nity degrades and that they are stable under unimpor-\ntant perturbations. To do so we introduced Sensitiv-\nity and Stability (§3.2). Sensitivity measures the rate of detection of negative perturbations and\n6\n"}, {"page": 7, "text": "Preprint\nStability measures the magnitude of score preservation under meaningless changes. We report Sen-\nsitivity and Stability for all metrics on 30 trials in Figure 3. We compare against a normal distribution\nbaseline.\nAutoMetrics are sensitive and stable.\nAutoMetrics are sensitive to degradation in output qual-\nity in 81.0-97.8% of cases, depending on the dataset, which is significantly greater than the 50%\nbaseline. AutoMetrics can be a strong tool for identifying degradations in output quality. Similarly,\nAutoMetrics also always outperforms the baseline for stability by greater than 95% confidence in-\ntervals. Under insignificant modifications to evaluated outputs, AutoMetrics are consistently stable.\n4.5\nDESIGN DECISIONS (HYPERPARAMETER SWEEPS)\nOur sweeps/ablations test three parts of the AutoMetrics method: the MetricBank, the retrieval step,\nand the regression step. We report Kendall’s τ rank correlation across our six main tasks with 95%\nconfidence intervals over five runs in Table 3. All sweeps and ablations are instead done on the dev\nset for all datasets. We never make design decisions based on runs of our test sets.\nIn-Distribution\nOut-of-Distribution\nMethod\nSimpEval\nHelpSteer2\nEvalGen\nRealHumanEval\nCoGym\nMetricBank Ablations (k=30; n=5)\nExisting Metrics Only\n0.238 ± 0.04\n0.376 ± 0.00\n0.389 ± 0.00\n0.155 ± 0.00\n0.258 ± 0.00\nGenerated Metrics Only\n0.276 ± 0.03\n0.308 ± 0.01\n0.503 ± 0.03\n0.132 ± 0.00\n0.433 ± 0.04\nFull MetricBank\n0.275 ± 0.02\n0.387 ± 0.00\n0.474 ± 0.03\n0.152 ± 0.01\n0.329 ± 0.02\nRetrieval Ablations (n=5)\nRetrieve k=5\n0.257 ± 0.03\n0.336 ± 0.03\n0.414 ± 0.12\n0.124 ± 0.02\n0.385 ± 0.04\nRetrieve k=10\n0.245 ± 0.02\n0.352 ± 0.01\n0.469 ± 0.06\n0.128 ± 0.01\n0.371 ± 0.02\nNo Metric Cards (k=20)\n0.281 ± 0.04\n0.328 ± 0.02\n0.427 ± 0.09\n0.134 ± 0.01\n0.292 ± 0.06\nRetrieve k=20\n0.286 ± 0.02\n0.378 ± 0.01\n0.522 ± 0.02\n0.141 ± 0.01\n0.302 ± 0.06\nRetrieve k=30\n0.275 ± 0.02\n0.387 ± 0.00\n0.474 ± 0.03\n0.152 ± 0.01\n0.329 ± 0.02\nRegression Ablations (k=30)\nNo Regression (n=1)\n0.232 ± 0.08\n0.393 ± 0.00\n0.353 ± 0.23\n0.145 ± 0.00\n0.356 ± 0.00\nRegress n=3\n0.255 ± 0.02\n0.389 ± 0.02\n0.503 ± 0.10\n0.152 ± 0.01\n0.302 ± 0.04\nRegress n=5\n0.275 ± 0.02\n0.387 ± 0.00\n0.474 ± 0.03\n0.152 ± 0.01\n0.329 ± 0.02\nRegress n=10\n0.309 ± 0.01\n0.358 ± 0.01\n0.461 ± 0.05\n0.147 ± 0.01\n0.297 ± 0.05\nRegress n=20\n0.268 ± 0.03\n0.350 ± 0.01\n0.498 ± 0.04\n0.153 ± 0.01\n0.361 ± 0.02\nTable 3: Kendall correlation with 95% confidence intervals on in-distribution and out-of-distribution\ndatasets over five runs with Qwen3 32B (Reasoning). The Full MetricBank and Metric Cards prove\nuseful, and the best settings for retrieval and regression are k=30 and n=5 respectively.\nBoth Generated and Existing Metrics Help.\nIn all of our tasks, the Full MetricBank was either\nthe best or second-best performing setting for the ablations. When it was second best, it was typically\nwithin 95% confidence intervals. The primary exception is CoGym, where “Full MetricBank” fell\n0.104 below “Generated Metrics Only” and, to a lesser extent, EvalGen, where “Full MetricBank”\nwas short by 0.029. CoGym and EvalGen are also our smallest training sets (37 and 57 training\nsamples respectively). We hypothesize this is because on out-of-distribution tasks, existing metrics\ntend to be noisy predictors which can spuriously correlate during the regression. Generated metrics\ntend to be less noisy predictors. Larger training sets provide a more effective filter for identifying\nuseful metrics. We further explore this hypothesis in our data scaling experiment (§4.6).\nMetric Cards Help Retrieval and Larger k Is Better.\nAcross all five tasks, retrieval with Metric\nCards (k=20) is better than retrieval without metric cards (using a single sentence description of the\nmetric). Furthermore, we see roughly linear growth of correlation with higher k metrics retrieved\nto run on the train set. The single exception to this trend is CoGym, which can be attributed to\nthe noisiness of the small dataset and the generated metrics being less noisy predictors. The top 5\nretrieved metrics are often generated ones, reducing the risk of recommending spuriously correlated\nexisting metric on the small dataset. We ran all retrieval experiments by regressing with n = 5, so\n7\n"}, {"page": 8, "text": "Preprint\nit is worth noting that future improvements to the retrieval algorithm (possibly including historical\nusage data) mean that it is feasible for k = 5 numbers to match our k = 30 results, so long as the\nproper metrics are recommended.\nNumber to regress to varies from dataset to dataset, but five is a good average case.\nThe best\ncase for regression only repeats once (with n=20), suggesting that the number of metrics needed\nis highly dependent on the complexity of the evaluation task and domain. Since there is no clear\nwinner, we select n=5 as a default because it is the second best in two of five tasks, and it is the\ncheapest option that still maintains lower variance from run to run. A higher N means producing\nmore expensive metrics to run downstream, so n=5 is a useful compromise of cost and performance.\n4.6\nHOW MUCH DATA DO YOU NEED TO USE AUTOMETRICS?\nTo test how much data is needed to use AutoMetrics, we test on three distinct datasets large enough\nto be useful in this experiment. We take a relatively simple In-Distribution dataset, SimpEval, a\nmore challenging In-Distribution dataset, HelpSteer2, and an Out-of-Distribution dataset RealHu-\nmanEval. We vary the train set size from N=5, 10, 20, 40, 80, 160, and (for RealHumanEval and\nHelpsteer2) 320 and 640. We run these settings for both the “Generated Only” Metric Bank and\n“Full” Metric Bank (with existing metrics). We plot the correlation on the full test set in Figure 4.\n5\n10\n20\n40\n80\n160\nTrain Size\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nKendall correlation\nSimpEval (score)\nGenerated Only\nFull\n5\n10\n20\n40\n80\n160 320 640\nTrain Size\nHelpSteer2 (helpfulness)\n5\n10\n20\n40\n80\n160 320 640\nTrain Size\nRealHumanEval (accepted)\nFigure 4: All correlations plotted for various training set sizes with “Generated Only” and “Full”\nMetric Banks. Individual trials are translucent while average performance at a scale is solid.\nAbout 80 samples saturates performance.\nAcross all three datasets and both settings, perfor-\nmance levels off after about 80 samples. It is possible with more sophisticated metric genera-\ntion/learning methods more data could continue to help, however with the current architecture be-\ntween 80-100 examples is all you need. Below 80 examples most of the lower performance is due\nto the high variance of fitting a regression to a small training set.\nOn out-of-distribution datasets “Generated Only” can outperform “Full” with low-resources.\nLooking to the RealHumanEval plot we see at training size 10 and 20 the “Generated Only” metrics\noutperform the “Full” Bank. Recall back to the ablations (§4.5) where we observed on the small,\nout-of-distribution datasets, CoGym and EvalGen, that “Generated Only” outperformed the “Full”\nMetricBank. Since most tasks will be out of distribution by nature, we default to using “Generated\nOnly” when the user provides less than 80 training samples. Beyond 80, both “Generated Only” and\n“Full” level off, however “Full” asymptotes higher than “Generated Only” on all datasets. We argue\nthis is a product of the high-p, low-n problem in regression where having too many weak predictors\nand not enough datapoints can lead to spurious correlations. By limiting to generated metrics for\nlow-n settings we enforce the use of stronger predictor signals.\n5\nCASE STUDY: AUTOMETRICS FOR OPTIMIZING AN AGENTIC TASK\nA natural extension to using AutoMetrics is to take the limited data one has available in order to\nlearn a useful set of metrics that can then be used for optimizing a system. In this way AutoMetrics\n8\n"}, {"page": 9, "text": "Preprint\nMembership Benefit Application (Rubric)\nCorrectly enforcing free baggage allowances, insurance eligibility, and \ncompensation rules based on membership tier..\nEscalation Appropriateness (Rubric)\nTransferring to human agents when policy limits are reached or exceptions \nare needed.\nPolicy Compliance\nAdherence to airline rules (e.g., no basic economy cancellations without \ninsurance or 24-hour window).\n0.08\n0.0599\n0.0567\nFigure 5: AutoMetrics produces three metrics for\nτ-Bench. Regression coefficients in yellow.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nNumber of Rollouts\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAverage Pass^1 Test Score\nTau Bench, Qwen3 32B\nAutoMetrics\nVerifiable Reward\nUnoptimized Baseline\nFigure 6: τ-Bench performance over GEPA opti-\nmization steps when using AutoMetrics.\nwould operate similar to the purpose of a Reward Model or a Verifiable Reward. In order to test if\nAutoMetrics can be useful in this setting we optimize an airline assistance agent for τ-bench (Yao\net al., 2024), a testbed for tool-use agents to interact with simulated users to accomplish tasks. We\nsplit the 50 τ-airline tasks into 25 for training and 25 for evaluation.\nSimulating a verifiable reward.\nTo run AutoMetrics we rollout the 25 training examples 8 times\neach with temperatures [0.0, 0.01, 0.02, 0.03, 0.05, 0.1, 0.15, 0.2]. Then we obtain the true reward\nsignal for each of these rollouts. In practice rather than a verifiable reward this could be a subjec-\ntive human label. We run AutoMetrics in ”Generate Only” mode and allocate more resources to\ngenerated metrics (10→20 llm judge metrics; 5→8 rubric metrics). Otherwise we run with default\nhyperparameters (k=30; n=5). We show the generated metrics in Figure 5.\nAutoMetrics recommends three metrics for Tau-Bench evals: two rubric based metrics and one\nsingle criterion metric. Originally our (n=5) setting recommended five metrics, however our final\nfiltering step removed two metrics for having negative coefficients. Since the trajectories are only\nderived from 25 examples it is likely that metrics will begin to learn things about the data itself. This\nreflects the importance of both human oversight and our metric filtering.\nOptimizing without a Verifiable Reward\nWe implement a simple ReAct (Yao et al., 2023) agent\nin DSPy (Khattab et al., 2024) for performing the τ-Airline task. Our baseline agent gets 60%\naccuracy on the 25 test examples averaged over five trials. We then run a baseline optimization\nwhere we use the DSPy GEPA optimizer (Agrawal et al., 2025) to optimize an agent on the 25\ntraining tasks with Verifiable Reward. Next we run optimization with our AutoMetrics as the\nmetric for GEPA optimization. We show the performance on the test set after N rollouts in Figure 6.\nWe find that AutoMetrics can match performance of a verifiable reward. After 2000 rollouts\nthe GEPA optimization with verifiable reward achieves 0.680 ± 0.11 accuracy over 5 trials while\nthe AutoMetrics run gets 0.720 ± 0.06. AutoMetrics statistically significantly exceeds the baseline\nperformance (p < 0.05) of 0.6. This demonstrates that AutoMetrics can match or exceed Verifiable\nRewards as optimization signal.\n6\nDISCUSSION AND CONCLUSION\nIn this paper, we introduced AutoMetrics, a method for producing metrics that correlate with human\njudgments on subjective tasks. Requiring only ∼80 human-labeled examples, AutoMetrics achieve\nhigh criterion validity (§4.3) and construct validity. (§4.4). AutoMetrics improve upon existing\nbaselines by up to 33.4% in Kendall correlation with human ratings. In a case study on Tau-Bench,\nAutoMetrics matched or exceeded gains obtained from optimizing on a verifiable reward (§5).\nWe draw two key lessons for practitioners. First, data diversity is critical: while only ∼80 feed-\nback points suffice for moderate correlation (§4.6), scaling up synthetic data from limited sources\ncan produce metrics that reflect dataset artifacts rather than system quality (§5). Second, human\noversight remains essential: domain experts can help remove spuriously correlated metrics which\nthe automatic filtering process misses. When using metrics for optimization, practitioners should\nmonitor metric feedback and improvement with observability tools (Chavez, 2025).\n9\n"}, {"page": 10, "text": "Preprint\nOverall, AutoMetrics provides a practical first step for exploring data and guiding optimization\nwhen collecting preliminary human evaluation in new domains. The metrics it produces are inter-\npretable, actionable, and informative for system improvement. We release AutoMetrics publicly and\ninvite community contributions of new metrics and methods to strengthen the framework.\nREPRODUCIBILITY STATEMENT\nAutoMetrics is intended to be an open source library and framework. As such we take great effort\nto make the running and evaluation of AutoMetrics user-friendly. We have attached an anonymized\nrepository for AutoMetrics with this submission. In addition to the core algorithm, the repository\nalso contains the python scripts to reproduce all experimental results in this paper. All of our design\ndecisions, hyperparameters, and ablations are rigorously documented throughout the paper across\nSection 4.5 and Appendix E. We provide system-specs needed to run the metrics in Table 5. We\nalso share the exact prompts and DSPy signatures used in calling LLMs in Appendix C. For all main\nexperimental results (e.g. Table 2 and Table 3) results are reported over five independent random\nseeds to ensure findings are robust and statistically significant.\nLIMITATIONS\nAs a part of the AutoMetrics framework we construct and optimize metrics with particular LLMs.\nBecause the metric generation process involves optimizing to a particular model we have found\nthat producing metrics with one model and running them with another reduces performance. This\nsuggests that when better models are released it will be important to reoptimize automatic metrics\nusing AutoMetrics rather than just swap out the underlying LLM.\nAutoMetrics may only generalize as far as the provided data enables it. Collecting real, diverse\nhuman data is still an essential part of evaluation. The more representative and generalizable the\ninput data is, the better and more general the AutoMetrics will be. Users should collect data that is\nrepresentative of the opinions and population that they want their evaluation to cover.\nAutoMetrics depends on running a regression for many predictors on a limited number of data points.\nAlthough we took this into account with the design of our Regression step, it is still possible to run\ninto a high-P low-N regression problem that risks spurious correlations. To counteract accidental\nmisuse of AutoMetrics leading to poor evaluation, we add warnings to the metric reports when the\nsignificance of the correlation with human judgments of the recommended metric is low (p > 0.05).\nFinally, as a part of this work we do not conduct a formal user study to demonstrate the adoption\nof AutoMetrics among practitioners. We have collected positive feedback on the metrics through\ninformal tests with AI developers. We hope that by releasing and open sourcing this library, we will\nhave the opportunity to work with the community to test and improve AutoMetrics.\nACKNOWLEDGEMENTS\nThis work has been supported in part through the Stanford HAI Corporate Affiliate Program,\nwith membership funding provided by American Express. This work was also funded through a\ngrant from the Sloan Foundation. The authors would like to thank Omar Khattab, William Held,\nSaurabh Shah, Aryaman Arora, Ken Liu, David Anugraha, Vishakh Padmakumar, Hao Zhu, Lakshya\nAgrawal, Yu Fei, Seungone Kim, and Jonathan Hilgart for their insightful comments and thoughts\nat various stages of the project. We would also like to thank SALT Lab and the Stanford NLP Group\nfor help with review and revision of the manuscript. Finally we would like to thank Yijia Shao and\nAlex Spangher for testing and offering feedback on the AutoMetrics system.\nREFERENCES\nLakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong,\nArnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik\nSen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, and Omar Khattab. Gepa:\n10\n"}, {"page": 11, "text": "Preprint\nReflective prompt evolution can outperform reinforcement learning, 2025.\nURL https://\narxiv.org/abs/2507.19457.\nAmerican Educational Research Association, American Psychological Association, and National\nCouncil on Measurement in Education. Standards for Educational and Psychological Testing.\nAmerican Educational Research Association, Washington, DC, 7 edition, 2014. ISBN 978-0-\n935302-35-6. Prepared by the Joint Committee on the Standards for Educational and Psycholog-\nical Testing of AERA, APA, and NCME.\nDavid Anugraha, Zilu Tang, Lester James V. Miranda, Hanyang Zhao, Mohammad Rifqi Farhan-\nsyah, Garry Kuwanto, Derry Wijaya, and Genta Indra Winata. R3: Robust rubric-agnostic reward\nmodels, 2025. URL https://arxiv.org/abs/2505.13388.\nYushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia\nXiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. Benchmarking foundation models with\nlanguage-model-as-an-examiner. In Thirty-seventh Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?\nid=IiRHQ7gvnq.\nSatanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and\nClare Voss (eds.), Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Mea-\nsures for Machine Translation and/or Summarization, pp. 65–72, Ann Arbor, Michigan, June\n2005. Association for Computational Linguistics.\nURL https://aclanthology.org/\nW05-0909/.\nSteven Bird and Edward Loper. NLTK: The natural language toolkit. In Proceedings of the ACL\nInteractive Poster and Demonstration Sessions, pp. 214–217, Barcelona, Spain, July 2004. Asso-\nciation for Computational Linguistics. URL https://aclanthology.org/P04-3031/.\nParam Biyani, Yasharth Bajpai, Arjun Radhakrishna, Gustavo Soares, and Sumit Gulwani. Rubicon:\nRubric-based evaluation of domain-specific human ai conversations. In Proceedings of the 1st\nACM International Conference on AI-Powered Software, AIware 2024, pp. 161–169, New York,\nNY, USA, 2024. Association for Computing Machinery. ISBN 9798400706851. doi: 10.1145/\n3664646.3664778. URL https://doi.org/10.1145/3664646.3664778.\nDenny Borsboom, Gideon J Mellenbergh, and Jaap Van Heerden. The concept of validity. Psycho-\nlogical review, 111(4):1061, 2004.\nNathan Brake and Thomas Schaaf. Comparing two model designs for clinical note generation; is an\nLLM a useful evaluator of consistency? In Kevin Duh, Helena Gomez, and Steven Bethard (eds.),\nFindings of the Association for Computational Linguistics: NAACL 2024, pp. 352–363, Mexico\nCity, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.\nfindings-naacl.25. URL https://aclanthology.org/2024.findings-naacl.25/.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL\nhttps://arxiv.org/abs/2005.14165.\nJose Camacho-collados, Kiamehr Rezaee, Talayeh Riahi, Asahi Ushio, Daniel Loureiro, Dimosthe-\nnis Antypas, Joanne Boisson, Luis Espinosa Anke, Fangyu Liu, and Eugenio Mart´ınez C´amara.\nTweetNLP: Cutting-edge natural language processing for social media. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing: System Demonstra-\ntions, pp. 38–49, Abu Dhabi, UAE, December 2022. Association for Computational Linguistics.\nURL https://aclanthology.org/2022.emnlp-demos.5.\nDonald T Campbell and Donald W Fiske. Convergent and discriminant validation by the multitrait-\nmultimethod matrix. Psychological bulletin, 56(2):81, 1959.\n11\n"}, {"page": 12, "text": "Preprint\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu,\nand Zhiyuan Liu.\nChateval: Towards better LLM-based evaluators through multi-agent de-\nbate.\nIn The Twelfth International Conference on Learning Representations, 2024.\nURL\nhttps://openreview.net/forum?id=FQepisCUWu.\nRogerio Chavez. Langwatch: The open llm ops platform — langwatch/langwatch. https://\ngithub.com/langwatch/langwatch, 2025. Accessed: 2025-09-24.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-\ntios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex\nPaino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec\nRadford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-\nGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large\nlanguage models trained on code, 2021. URL https://arxiv.org/abs/2107.03374.\nShikai Chen, Jin Yuan, Yang Zhang, Zhongchao Shi, Jianping Fan, Xin Geng, and Yong\nRui.\nLdl-reward-gemma-2-27b-v0.1.\nhttps://huggingface.co/ShikaiChen/\nLDL-Reward-Gemma-2-27B-v0.1, 2025. Hugging Face Model Repository.\nCheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human eval-\nuations?\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-\npers), pp. 15607–15631, Toronto, Canada, July 2023. Association for Computational Linguis-\ntics.\ndoi: 10.18653/v1/2023.acl-long.870.\nURL https://aclanthology.org/2023.\nacl-long.870/.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li,\nDacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Sto-\nica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. URL\nhttps://arxiv.org/abs/2403.04132.\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. In Proceedings of the 31st International Confer-\nence on Neural Information Processing Systems, NIPS’17, pp. 4302–4310, Red Hook, NY, USA,\n2017. Curran Associates Inc. ISBN 9781510860964.\nPierre Colombo, Chloe Clave, and Pablo Piantanida. Infolm: A new metric to evaluate summa-\nrization & data2text generation.\nIn AAAI Conference on Artificial Intelligence, 2021.\nURL\nhttps://api.semanticscholar.org/CorpusID:244896426.\nConfidentAI.\nDeepeval: Open-source llm evaluation framework.\nhttps://github.com/\nconfident-ai/deepeval, 2025.\nURL https://github.com/confident-ai/\ndeepeval. Version 2.7.6, released April 22, 2025. Accessed April 24, 2025.\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William\nCohen. Handling divergent reference texts when evaluating table-to-text generation. In Anna\nKorhonen, David Traum, and Llu´ıs M`arquez (eds.), Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, pp. 4884–4895, Florence, Italy, July\n2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1483. URL https:\n//aclanthology.org/P19-1483/.\nGeorge Doddington.\nAutomatic evaluation of machine translation quality using n-gram co-\noccurrence statistics. In Proceedings of the Second International Conference on Human Language\nTechnology Research, HLT ’02, pp. 138–145, San Francisco, CA, USA, 2002. Morgan Kaufmann\nPublishers Inc.\n12\n"}, {"page": 13, "text": "Preprint\nDare Doe and Vishal Devireddy. Scorecard ai: Llm evaluation and testing platform. https://\npypi.org/project/scorecard-ai/, 2024. URL https://www.scorecard.io/.\nVersion 1.1.1, released December 10, 2024. Accessed April 24, 2025.\nLisa Dunlap, Krishna Mandal, Trevor Darrell, Jacob Steinhardt, and Joseph E Gonzalez. Vibecheck:\nDiscover and quantify qualitative differences in large language models, 2025. URL https:\n//arxiv.org/abs/2410.12851.\nEvidentlyAI.\nEvidently:\nOpen-source ml and llm evaluation and observability framework.\nhttps://github.com/evidentlyai/evidently, 2025.\nURL https://www.\nevidentlyai.com/. Version 0.7.2, released April 21, 2025. Accessed April 24, 2025.\nR. Flesch. Marks of readable style; a study in adult education. Teachers College Contributions to\nEducation, 897:ix + 69–ix + 69, 1943.\nCl´ementine Fourrier, Nathan Habib, Hynek Kydl´ıˇcek, Thomas Wolf, and Lewis Tunstall. Lighte-\nval: A lightweight framework for llm evaluation, 2023.\nURL https://github.com/\nhuggingface/lighteval.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. GPTScore: Evaluate as you desire.\nIn Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers), pp. 6556–6576, Mexico City, Mexico, June\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.365. URL\nhttps://aclanthology.org/2024.naacl-long.365/.\nGalileo. Galileo evaluate: Promptquality python package. https://pypi.org/project/\npromptquality/, 2025.\nURL https://pypi.org/project/promptquality/.\nVersion 1.10.0, released April 24, 2025. Accessed April 24, 2025.\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi,\nAnuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondˇrej Duˇsek, Chris Chinenye\nEmezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jer-\nnite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak,\nAman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Ma-\njumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg,\nMoin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei,\nAnkur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Ro-\ndriguez, Sashank Santhanam, Jo˜ao Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimo-\nrina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi\nYang, Akhila Yerukola, and Jiawei Zhou. The GEM benchmark: Natural language generation,\nits evaluation and metrics. In Antoine Bosselut, Esin Durmus, Varun Prashant Gangal, Sebas-\ntian Gehrmann, Yacine Jernite, Laura Perez-Beltrachini, Samira Shaikh, and Wei Xu (eds.),\nProceedings of the First Workshop on Natural Language Generation, Evaluation, and Metrics\n(GEM), pp. 96–120, Online, August 2021. Association for Computational Linguistics.\ndoi:\n10.18653/v1/2021.gem-1.10. URL https://aclanthology.org/2021.gem-1.10/.\nElliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Car-\noline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli\nJ¨arviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth\nPratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreep-\nranav Varma Enugandla, and Mark Wildon. Frontiermath: A benchmark for evaluating advanced\nmathematical reasoning in ai, 2024. URL https://arxiv.org/abs/2411.04872.\nAnisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as\nrewards: Reinforcement learning beyond verifiable domains, 2025. URL https://arxiv.\norg/abs/2507.17746.\nAlexander Gurung and Mirella Lapata. Learning to reason for long-form story generation, 2025.\nURL https://arxiv.org/abs/2503.22828.\n13\n"}, {"page": 14, "text": "Preprint\nRishav Hada, Varun Gumma, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. METAL: To-\nwards multilingual meta-evaluation. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.),\nFindings of the Association for Computational Linguistics: NAACL 2024, pp. 2280–2298, Mex-\nico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.\nfindings-naacl.148.\nURL https://aclanthology.org/2024.findings-naacl.\n148/.\nRichard W Hamming. Error detecting and error correcting codes. The Bell system technical journal,\n29(2):147–160, 1950.\nDavid Heineman, Yao Dou, Mounica Maddela, and Wei Xu. Dancing between success and failure:\nEdit-level simplification evaluation using SALSA. In Houda Bouamor, Juan Pino, and Kalika\nBali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 3466–3495, Singapore, December 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.emnlp-main.211. URL https://aclanthology.org/2023.\nemnlp-main.211/.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\nRobert Iv, Alexandre Passos, Sameer Singh, and Ming-Wei Chang. FRUIT: Faithfully reflecting\nupdated information in text. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir\nMeza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Technologies, pp. 3670–3686, Seat-\ntle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nnaacl-main.269. URL https://aclanthology.org/2022.naacl-main.269/.\nPaul Jaccard. ´Etude comparative de la distribution florale dans une portion des alpes et des jura.\nBull Soc Vaudoise Sci Nat, 37:547–579, 1901.\nMatthew A. Jaro. Advances in record-linkage methodology as applied to matching the 1985 census\nof tampa, florida. Journal of the American Statistical Association, 84(406):414–420, 1989. ISSN\n01621459, 1537274X. URL http://www.jstor.org/stable/2289924.\nF. Jelinek, R. L. Mercer, L. R. Bahl, and J. K. Baker. Perplexity—a measure of the difficulty of\nspeech recognition tasks. The Journal of the Acoustical Society of America, 62(S1):S63–S63, 08\n2005. ISSN 0001-4966. doi: 10.1121/1.2016299. URL https://doi.org/10.1121/1.\n2016299.\nNimit Kalra and Leonard Tang. Verdict: A library for scaling judge-time compute, 2025. URL\nhttps://arxiv.org/abs/2502.18018.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vard-\nhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei\nZaharia, and Christopher Potts. Dspy: Compiling declarative language model calls into self-\nimproving pipelines. In The Twelfth International Conference on Learning Representations, 2024.\nSeungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham\nNeubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language\nmodel specialized in evaluating other language models, 2024.\nSeungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon,\nGuijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang,\nJinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho,\nSe June Joo, Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon\nYe, Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Min-\njoon Seo.\nThe BiGGen bench: A principled benchmark for fine-grained evaluation of lan-\nguage models with language models.\nIn Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.),\nProceedings of the 2025 Conference of the Nations of the Americas Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),\npp. 5877–5919, Albuquerque, New Mexico, April 2025. Association for Computational Lin-\nguistics.\nISBN 979-8-89176-189-6.\ndoi: 10.18653/v1/2025.naacl-long.303.\nURL https:\n//aclanthology.org/2025.naacl-long.303/.\n14\n"}, {"page": 15, "text": "Preprint\nJ Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom. Derivation of new\nreadability formulas (automated readability index, fog count and flesch reading ease formula) for\nnavy enlisted personnel. 1975.\nTom Kocmi and Christian Federmann. Large language models are state-of-the-art evaluators of\ntranslation quality. In Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail\nMikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal,\nNora Aranberri, Mara Nunziatini, Carla Parra Escart´ın, Mikel Forcada, Maja Popovic, Carolina\nScarton, and Helena Moniz (eds.), Proceedings of the 24th Annual Conference of the European\nAssociation for Machine Translation, pp. 193–203, Tampere, Finland, June 2023. European As-\nsociation for Machine Translation. URL https://aclanthology.org/2023.eamt-1.\n19/.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual\nconsistency of abstractive text summarization. In Bonnie Webber, Trevor Cohn, Yulan He, and\nYang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 9332–9346, Online, November 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.emnlp-main.750. URL https://aclanthology.org/\n2020.emnlp-main.750/.\nNathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi\nChandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh\nHajishirzi.\nRewardbench: Evaluating reward models for language modeling, 2024.\nURL\nhttps://arxiv.org/abs/2403.13787.\nAdrien Lardilleux and Yves Lepage. CHARCUT: Human-targeted character-based MT evaluation\nwith loose differences. In Proceedings of the 14th International Conference on Spoken Language\nTranslation, pp. 146–153, Tokyo, Japan, December 14-15 2017. International Workshop on Spo-\nken Language Translation. URL https://aclanthology.org/2017.iwslt-1.20.\nV. I. Levenshtein.\nBinary codes capable of correcting deletions, insertions, and reversals.\nSo-\nviet Physics Doklady, 10:707, 1966.\nEnglish translation of Doklady Akademii Nauk SSSR,\n163(4):845–848, 1965.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objec-\ntive function for neural conversation models. In Kevin Knight, Ani Nenkova, and Owen Rambow\n(eds.), Proceedings of the 2016 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 110–119, San Diego, Califor-\nnia, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL\nhttps://aclanthology.org/N16-1014/.\nMinzhi Li, Zhengyuan Liu, Shumin Deng, Shafiq Joty, Nancy Chen, and Min-Yen Kan. DnA-eval:\nEnhancing large language model evaluation through decomposition and aggregation. In Owen\nRambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven\nSchockaert (eds.), Proceedings of the 31st International Conference on Computational Linguis-\ntics, pp. 2277–2290, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics.\nURL https://aclanthology.org/2025.coling-main.156/.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following\nmodels. https://github.com/tatsu-lab/alpaca_eval, 5 2023.\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguis-\ntics. URL https://aclanthology.org/W04-1013/.\nYen-Ting Lin and Yun-Nung Chen. LLM-eval: Unified multi-dimensional automatic evaluation for\nopen-domain conversations with large language models. In Yun-Nung Chen and Abhinav Rastogi\n(eds.), Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023), pp.\n47–58, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/\nv1/2023.nlp4convai-1.5. URL https://aclanthology.org/2023.nlp4convai-1.\n5/.\n15\n"}, {"page": 16, "text": "Preprint\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG\nevaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Ka-\nlika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 2511–2522, Singapore, December 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.emnlp-main.153. URL https://aclanthology.org/2023.\nemnlp-main.153/.\nYi\nLiu,\nMatei\nZaharia,\nand\nRitendra\nDatta.\nEnhancing\nllm-as-a-\njudge\nwith\ngrading\nnotes.\nhttps://www.databricks.com/\nblog/enhancing-llm-as-a-judge-with-grading-notes,\nJuly\n2024.\nURL\nhttps://www.databricks.com/blog/\nenhancing-llm-as-a-judge-with-grading-notes. Databricks Blog.\nChi-kiu Lo. YiSi - a unified semantic MT quality evaluation and estimation metric for languages\nwith different levels of available resources. In Ondˇrej Bojar, Rajen Chatterjee, Christian Fed-\nermann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes,\nPhilipp Koehn, Andr´e Martins, Christof Monz, Matteo Negri, Aur´elie N´ev´eol, Mariana Neves,\nMatt Post, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Fourth Conference on\nMachine Translation (Volume 2: Shared Task Papers, Day 1), pp. 507–513, Florence, Italy,\nAugust 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5358. URL\nhttps://aclanthology.org/W19-5358/.\nMounica Maddela, Yao Dou, David Heineman, and Wei Xu. LENS: A learnable evaluation met-\nric for text simplification.\nIn Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 16383–16408, Toronto, Canada, July\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.905. URL\nhttps://aclanthology.org/2023.acl-long.905.\nXiaoyu Tan Minghao Yang, Chao Qu.\nInf-orm-llama3.1-70b, 2024.\nURL [https://\nhuggingface.co/infly/INF-ORM-Llama3.1-70B](https://huggingface.\nco/infly/INF-ORM-Llama3.1-70B).\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin-\nson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model report-\ning. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT*\n’19, pp. 220–229, New York, NY, USA, 2019. Association for Computing Machinery. ISBN\n9781450361255.\ndoi: 10.1145/3287560.3287596.\nURL https://doi.org/10.1145/\n3287560.3287596.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-\nstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518(7540):529–533, February 2015. ISSN 1476-4687. doi: 10.1038/nature14236. URL\nhttps://doi.org/10.1038/nature14236.\nHussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian Zhao, Dennis Wei,\nManish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, and David Sontag. The realhumaneval:\nEvaluating large language models’ abilities to support programmers. Transactions on Machine\nLearning Research, 2025. ISSN 2835-8856. URL https://openreview.net/forum?\nid=hGaWq5Buj7. Expert Certification.\nNicki Skafte Detlefsen, Jiri Borovec, Justus Schock, Ananya Harsh, Teddy Koker, Luca Di\nLiello, Daniel Stancl, Changsheng Quan, Maxim Grechkin, and William Falcon. TorchMetrics\n- Measuring Reproducibility in PyTorch, February 2022.\nURL https://github.com/\nLightning-AI/torchmetrics.\nKrista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Za-\nharia, and Omar Khattab.\nOptimizing instructions and demonstrations for multi-stage lan-\nguage model programs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Pro-\nceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp.\n16\n"}, {"page": 17, "text": "Preprint\n9340–9366, Miami, Florida, USA, November 2024. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2024.emnlp-main.525. URL https://aclanthology.org/2024.\nemnlp-main.525/.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.),\nProceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp.\n311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguis-\ntics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040/.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Ed-\nward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. PyTorch: an imperative style, high-performance deep\nlearning library. Curran Associates Inc., Red Hook, NY, USA, 2019.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,\n12:2825–2830, 2011.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi,\nand Zaid Harchaoui. Mauve: Measuring the gap between neural text and human text using diver-\ngence frontiers. In NeurIPS, 2021.\nMaja Popovi´c.\nchrF: character n-gram F-score for automatic MT evaluation.\nIn Ondˇrej Bojar,\nRajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara\nLogacheva, and Pavel Pecina (eds.), Proceedings of the Tenth Workshop on Statistical Machine\nTranslation, pp. 392–395, Lisbon, Portugal, September 2015. Association for Computational Lin-\nguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology.org/W15-3049/.\nMahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data cards: Purposeful and trans-\nparent dataset documentation for responsible ai. In Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency, FAccT ’22, pp. 1776–1826, New York, NY, USA,\n2022. Association for Computing Machinery. ISBN 9781450393522. doi: 10.1145/3531146.\n3533231. URL https://doi.org/10.1145/3531146.3533231.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nMichael J. Ryan, Omar Shaikh, Aditri Bhagirath, Daniel Frees, William Held, and Diyi Yang.\nSynthesizeMe! inducing persona-guided prompts for personalized reward models in LLMs. In\nWanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Pro-\nceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pp. 8045–8078, Vienna, Austria, July 2025. Association for Compu-\ntational Linguistics.\nISBN 979-8-89176-251-0.\ndoi: 10.18653/v1/2025.acl-long.397.\nURL\nhttps://aclanthology.org/2025.acl-long.397/.\nJon Saad-Falcon, Rajan Vivek, William Berrios, Nandita Shankar Naik, Matija Franklin, Bertie\nVidgen, Amanpreet Singh, Douwe Kiela, and Shikib Mehri. Lmunit: Fine-grained evaluation\nwith natural language unit tests, 2024. URL https://arxiv.org/abs/2412.13091.\nPatricia Schmidtova, Saad Mahamood, Simone Balloccu, Ondrej Dusek, Albert Gatt, Dimitra\nGkatzia, David M. Howcroft, Ondrej Platek, and Adarsa Sivaprasad. Automatic metrics in natural\nlanguage generation: A survey of current evaluation practices. In Saad Mahamood, Nguyen Le\nMinh, and Daphne Ippolito (eds.), Proceedings of the 17th International Natural Language Gen-\neration Conference, pp. 557–583, Tokyo, Japan, September 2024. Association for Computational\nLinguistics. URL https://aclanthology.org/2024.inlg-main.44/.\nThomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. Answers unite! un-\nsupervised metrics for reinforced summarization models. In Kentaro Inui, Jing Jiang, Vincent Ng,\nand Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural\n17\n"}, {"page": 18, "text": "Preprint\nLanguage Processing and the 9th International Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pp. 3246–3256, Hong Kong, China, November 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/D19-1320. URL https://aclanthology.\norg/D19-1320/.\nThibault Sellam, Dipanjan Das, and Ankur Parikh.\nBLEURT: Learning robust metrics for text\ngeneration. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881–7892,\nOnline, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.\n704. URL https://aclanthology.org/2020.acl-main.704/.\nShreya Shankar, Haotian Li, Parth Asawa, Madelon Hulsebos, Yiming Lin, J. D. Zamfirescu-\nPereira, Harrison Chase, Will Fu-Hinthorn, Aditya G. Parameswaran, and Eugene Wu. spade:\nSynthesizing data quality assertions for large language model pipelines. Proc. VLDB Endow.,\n17(12):4173–4186, August 2024a. ISSN 2150-8097. doi: 10.14778/3685800.3685835. URL\nhttps://doi.org/10.14778/3685800.3685835.\nShreya Shankar, J.D. Zamfirescu-Pereira, Bjoern Hartmann, Aditya Parameswaran, and Ian Arawjo.\nWho validates the validators? aligning llm-assisted evaluation of llm outputs with human prefer-\nences. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Tech-\nnology, UIST ’24, New York, NY, USA, 2024b. Association for Computing Machinery. ISBN\n9798400706288.\ndoi: 10.1145/3654777.3676450.\nURL https://doi.org/10.1145/\n3654777.3676450.\nYijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, and Diyi Yang. Collaborative gym: A frame-\nwork for enabling and evaluating human-agent collaboration, 2025. URL https://arxiv.\norg/abs/2412.15701.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathe-\nmatical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.\n03300.\nLingfeng Shen, Lemao Liu, Haiyun Jiang, and Shuming Shi. On the evaluation metrics for para-\nphrase generation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of\nthe 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3178–3190,\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2022.emnlp-main.208. URL https://aclanthology.org/2022.\nemnlp-main.208/.\nMatthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul.\nA study\nof translation edit rate with targeted human annotation. In Proceedings of the 7th Conference\nof the Association for Machine Translation in the Americas: Technical Papers, pp. 223–231,\nCambridge, Massachusetts, USA, August 8-12 2006. Association for Machine Translation in the\nAmericas. URL https://aclanthology.org/2006.amta-papers.25/.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur,\nBen Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha,\nSachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas\nMuennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle\nRichardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh,\nLuke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge,\nand Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining re-\nsearch. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 15725–15788, Bangkok, Thailand, August 2024. Association for Computational Linguis-\ntics.\ndoi: 10.18653/v1/2024.acl-long.840.\nURL https://aclanthology.org/2024.\nacl-long.840/.\nHong Sun and Ming Zhou. Joint learning of a dual SMT system for paraphrase generation. In\nHaizhou Li, Chin-Yew Lin, Miles Osborne, Gary Geunbae Lee, and Jong C. Park (eds.), Pro-\nceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2:\n18\n"}, {"page": 19, "text": "Preprint\nShort Papers), pp. 38–42, Jeju Island, Korea, July 2012. Association for Computational Linguis-\ntics. URL https://aclanthology.org/P12-2008/.\nTuhina Tripathi, Manya Wadhwa, Greg Durrett, and Scott Niekum. Pairwise or pointwise? evalu-\nating feedback protocols for bias in LLM-based evaluation. In Second Conference on Language\nModeling, 2025. URL https://openreview.net/forum?id=uyX5Vnow3U.\nKen Tsui and Huu Nguyen. Low latency cpu based educational value classifier with generic educa-\ntional value, 2024.\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 4566–4575, 2015. doi: 10.1109/CVPR.2015.7299087.\nBertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. Learning from the worst: Dy-\nnamically generated datasets to improve online hate detection. In Chengqing Zong, Fei Xia,\nWenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 1667–1682, Online, August 2021. Asso-\nciation for Computational Linguistics.\ndoi: 10.18653/v1/2021.acl-long.132.\nURL https:\n//aclanthology.org/2021.acl-long.132/.\nVijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, and Tong-\nshuang Wu. Checklists are better than reward models for aligning language models, 2025. URL\nhttps://arxiv.org/abs/2507.18624.\nJiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu,\nJianfeng Qu, and Jie Zhou.\nIs ChatGPT a good NLG evaluator?\na preliminary study.\nIn\nYue Dong, Wen Xiao, Lu Wang, Fei Liu, and Giuseppe Carenini (eds.), Proceedings of the\n4th New Frontiers in Summarization Workshop, pp. 1–11, Singapore, December 2023. Asso-\nciation for Computational Linguistics.\ndoi: 10.18653/v1/2023.newsum-1.1.\nURL https:\n//aclanthology.org/2023.newsum-1.1/.\nZhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang,\nMakesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer 2: Open-source dataset for training\ntop-performing reward models. In The Thirty-eight Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?\nid=PvVKUFhaNy.\nBenjamin Warner, Antoine Chaffin, Benjamin Clavi´e, Orion Weller, Oskar Hallstr¨om, Said\nTaghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, et al. Smarter, bet-\nter, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context\nfinetuning and inference. arXiv preprint arXiv:2412.13663, 2024.\nGenta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, and Derry Tanti Wijaya.\nMetametrics: Calibrating metrics for generation tasks using human preferences. In The Thirteenth\nInternational Conference on Learning Representations, 2025. URL https://openreview.\nnet/forum?id=slO3xTt4CG.\nWilliam E Winkler. String comparator metrics and enhanced decision rules in the fellegi-sunter\nmodel of record linkage. 1990.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\nger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. As-\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\n2020.emnlp-demos.6.\n19\n"}, {"page": 20, "text": "Preprint\nNing Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. Large language models are\ndiverse role-players for summarization evaluation, 2023. URL https://arxiv.org/abs/\n2303.15078.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa,\nKeith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa,\nAlex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neu-\nral machine translation system: Bridging the gap between human and machine translation, 2016.\nURL https://arxiv.org/abs/1609.08144.\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch.\nOptimiz-\ning statistical machine translation for text simplification.\nTransactions of the Association for\nComputational Linguistics, 4:401–415, 2016.\ndoi:\n10.1162/tacl a 00107.\nURL https:\n//aclanthology.org/Q16-1029/.\nRui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. Regularizing hidden states en-\nables learning generalizable reward model for LLMs. In The Thirty-eighth Annual Conference on\nNeural Information Processing Systems, 2024. URL https://openreview.net/forum?\nid=jwh9MHEfmY.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReAct: Synergizing reasoning and acting in language models. In International Conference on\nLearning Representations (ICLR), 2023.\nShunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ-bench: A benchmark for\ntool-agent-user interaction in real-world domains, 2024. URL https://arxiv.org/abs/\n2406.12045.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: evaluating generated text as text gen-\neration. In Proceedings of the 35th International Conference on Neural Information Processing\nSystems, NIPS ’21, Red Hook, NY, USA, 2021. Curran Associates Inc. ISBN 9781713845393.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evalu-\nating text generation with bert. In International Conference on Learning Representations, 2020.\nURL https://openreview.net/forum?id=SkeHuCVFDr.\nZhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu,\nJingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical\nreasoning, 2025. URL https://arxiv.org/abs/2501.07301.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. MoverScore:\nText generation evaluating with contextualized embeddings and earth mover distance. In Kentaro\nInui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pp. 563–578, Hong Kong, China, November\n2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1053. URL https:\n//aclanthology.org/D19-1053/.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https:\n//openreview.net/forum?id=uccHPGDlao.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji,\nand Jiawei Han. Towards a unified multi-dimensional evaluator for text generation. In Yoav\nGoldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pp. 2023–2038, Abu Dhabi, United Arab\nEmirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nemnlp-main.131. URL https://aclanthology.org/2022.emnlp-main.131/.\n20\n"}, {"page": 21, "text": "Preprint\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen:\nA benchmarking platform for text generation models. SIGIR, 2018.\nA\nLLM USAGE ACKNOWLEDGMENT\nLLMs were used to rephrase and edit writing in the paper, after an entirely human-written first draft.\nLLMs were also used as coding assistants in writing code for this project. All code and writing edits\nproduced by LLMs were rigorously verified by the first-author.\nB\nINTRODUCING METRICBANK\nMetricBank\nAs our first significant contribution, we curate MetricBank, a standardized collec-\ntion of 48 commonly used metrics in NLP literature. We source the metrics from Schmidtova et al.\n(2024), which examined all papers from the International Conference on Natural Language Gen-\neration (INLG) 2023 and all papers in the Generation track presented at ACL 2023, totaling 110\npapers. They collected a list of all the Natural Language Generation (NLG) metrics used in those\nworks, which totaled 283 different automatic metrics grouped into 34 metric families. We sorted\nby the most popular and implemented the top metrics from the top 16 families (28 metrics). Then,\nfor completeness, we also implemented any remaining NLG metrics in NLTK Bird & Loper (2004),\nPyTorch Paszke et al. (2019), Huggingface Lighteval Fourrier et al. (2023), and Metametrics Winata\net al. (2025) for an additional 12 metrics. Finally, we source a few additional metrics from recent\npapers not covered in the 2024 survey. We provide individual justifications for these 8 metrics in\nAppendix B.1.\nWe provide interesting stats about our metrics in Table 4. In particular, we collect 29 reference-\nbased metrics, such as BLEU Papineni et al. (2002), which require a gold reference output, and\n19 reference-free metrics, such as FKGL Flesch (1943), which measure quality of text without\ncomparison to a reference. Our metrics span 12 distinct domains. We implement each metric with a\nsimple interface of a calculate method that takes in the generated text and produces a floating-\npoint score and optionally text feedback.\nMetric Cards\nInspired by Model Cards Mitchell et al. (2019) and Data Cards Pushkarna et al.\n(2022) we design Metric Cards for simple documentation and reporting of the intended usage of\nmetrics. Our Metric Cards contains seven main sections. Metric Details contains the description of\nthe metric as well as core details that are needed to use it, such as the range of outputs, if it’s refer-\nence based, if an input is required, etc. Intended Use describes the domain/tasks where the metric\nshould be used as well as recommendations for when and when not to use the metric. Metric Im-\nplementation links to reference implementations and provides guidance on practical matters about\nthe metric such as it’s efficiency and scalability. Known Limitations explains biases, misuse, and\nknown failure cases of the metric. Related Metrics links to similar metrics to help when browsing\nfor the right metric for your task. Further Reading points to papers, blogs, and tutorials covering\nthe metric. Finally, Metric Card Authors makes it clear who wrote the metric card and if they used\nan AI assistance. We provide a complete example of a metric card for the common BLEU metric\nPapineni et al. (2002) in Appendix D. We also provide a prompt for using LLMs to write a first pass\nof a metric card in Appendix C.\nB.1\nADDITIONAL METRICS\nHere we provide justifications for our additional metrics that we did not collect from the metric sur-\nvey (Schmidtova et al., 2024), lighteval (Fourrier et al., 2023), torchmetrics (Nicki Skafte Detlefsen\net al., 2022), etc.\nReward Models.\nWe choose some of the most performant reward models off the RewardBench\nleaderboard (Lambert et al., 2024) at development time. The three models we used are INFORM-\nRewardModel (llama 3.1 70b) (Minghao Yang, 2024), LDLRewardModel (Gemma 2 27B) (Chen\net al., 2025), and GRMRewardModel (Llama 3.2 3B) (Yang et al., 2024). We also add in the\nQwen2.5 7B Process Reward Model (Zhang et al., 2025).\n21\n"}, {"page": 22, "text": "Preprint\nMetric (Citation)\nDomain\nGPU\nType\nSup.\nDefault LLM\nReference-Based Metrics: rely on a gold reference for comparison.\nJaccard Distance Jaccard (1901)\nA, Â, \u001e\n✗\nedit-distance\n✗\nN.A.\nHamming Distance Hamming (1950)\n\u001e\n✗\nedit-distance\n✗\nN.A.\nLevenshtein Distance Levenshtein (1966)\n, A, Â, \u001e\n✗\nedit-distance\n✗\nN.A.\nLevenshtein Ratio Levenshtein (1966)\n, A, Â, \u001e\n✗\nedit-distance\n✗\nN.A.\nJaro Similarity Jaro (1989)\n\u001e\n✗\nedit-distance\n✗\nN.A.\nJaro–Winkler Winkler (1990)\n\u001e\n✗\nedit-distance\n✗\nN.A.\nBLEU Papineni et al. (2002)\n\n✗\nn-gram overlap\n✗\nN.A.\nNIST Doddington (2002)\n, A\n✗\nn-gram overlap\n✗\nN.A.\nROUGE Lin (2004)\nA, , Â, L\n✗\nn-gram overlap\n✗\nN.A.\nMETEOR Banerjee & Lavie (2005)\n, A, Â, ë\n✗\nn-gram overlap\n✗\nN.A.\nTER Snover et al. (2006)\n\n✗\nedit-distance\n✗\nN.A.\niBLEU Sun & Zhou (2012)\nÂ, `, Ü\n✗\nn-gram overlap\n✗\nN.A.\nCHRF++ Popovi´c (2015)\n, A, Â, L\n✗\nn-gram overlap\n✗\nN.A.\nCIDEr Vedantam et al. (2015)\në\n✗\nn-gram overlap\n✗\nN.A.\nGLEU Wu et al. (2016)\n\n✗\nn-gram overlap\n✗\nN.A.\nSARI Xu et al. (2016)\nA, `\n✗\nn-gram overlap\n✗\nN.A.\nCharCut Lardilleux & Lepage (2017)\n, \u001e\n✗\nedit-distance\n✗\nN.A.\nMoverScore Zhao et al. (2019)\n, A, ë, L\n✓\nembedding sim\n✗\nBERT\nPseudoPARENT Dhingra et al. (2019)\nL, A, Â\n✗\nn-gram overlap\n✗\nN.A.\nBERTScore Zhang et al. (2020)\n, A, Â, ë\n✓\nembedding sim\n✗\nRoBERTa-Large\nBLEURT Sellam et al. (2020)\n, A, Â, L\n✓\nLM regression\n✓\nBERT/RemBERT\nBARTScore Yuan et al. (2021)\n, A, Â, L, Ü\n✓\nLM regression\n✗\nBART\nInfoLM Colombo et al. (2021)\nA, L\n✓\ndivergence-based\n✗\nBERT\nMAUVE Pillutla et al. (2021)\nÜ, =\n✓\ndivergence-based\n✗\nGPT-2\nParaScore Shen et al. (2022)\nÂ\n✓\nembedding sim\n✗\nRoBERTa-large\nUniEvalDialogue Zhong et al. (2022)\nÜ\n✓\nLM regression\n✓\nT5\nUniEvalSum Zhong et al. (2022)\nA, L\n✓\nLM regression\n✓\nT5\nUpdateROUGE Iv et al. (2022)\nA\n✗\nn-gram overlap\n✗\nN.A.\nLENS Maddela et al. (2023)\nA, `\n✓\nLM regression\n✓\nT5\nReference-Free Metrics: do not require a gold reference.\nFKGL Kincaid et al. (1975)\nA, `\n✗\nrule-based\n✗\nN.A.\nPerplexity Jelinek et al. (2005)\nÜ, =, Ð\n✓\nfluency\n✗\nGPT-2 Large\nDistinctNGrams Li et al. (2016)\nÜ, =\n✗\ndiversity ratio\n✗\nN.A.\nSelfBLEU Zhu et al. (2018)\nÜ, =, Â\n✗\ndiversity ratio\n✗\nN.A.\nYiSi-2 Lo (2019)\n\n✓\nembedding sim\n✗\nmBERT\nSummaQA Scialom et al. (2019)\nA\n✓\nLM regression\n✓\nBERT\nFactCC Kryscinski et al. (2020)\nA\n✓\nLM regression\n✓\nBERT\nToxicity Vidgen et al. (2021)\nÜ, =, è\n✓\nclassification\n✓\nRoBERTa\nParaScoreFree Shen et al. (2022)\nÂ, L\n✓\nembedding sim\n✗\nRoBERTa-large\nSentiment Camacho-collados et al. (2022)\nÜ, =\n✓\nclassification\n✓\nRoBERTa\nUniEvalFact Zhong et al. (2022)\nA, L\n✓\nLM regression\n✓\nT5\nLENS SALSA Heineman et al. (2023)\nA, `\n✓\nLM regression\n✓\nT5\nFastTextEducationalValue Tsui & Nguyen (2024)\nA, `\n✗\nclassification\n✓\nFastText\nFastTextNSFW Soldaini et al. (2024)\nÜ, è\n✗\nclassification\n✓\nFastText\nFastTextToxicity Soldaini et al. (2024)\nÜ, è\n✗\nclassification\n✓\nFastText\nGRMRewardModel Yang et al. (2024)\nÜ, è\n✓\nLM regression\n✓\nLlama-3.2-3B\nINFORM Reward Model 70B Minghao Yang (2024)\nÜ\n✓\nLM regression\n✓\nLlama-3.1-70B\nLDL Reward Model 27B Chen et al. (2025)\nÜ, Ð\n✓\nLM regression\n✓\nGemma 2-27B\nMathProcessRewardModel Zhang et al. (2025)\ny, A\n✓\nclassification\n✓\nQwen2.5 7B\nTable 4: Comparison of generative evaluation metrics. Icons:  Machine Translation, A Summa-\nrization, Â Paraphrasing, Ü Dialogue/Chat, = Storytelling/Creative Writing, ë Image Caption-\ning/Multimodal, è Safety/Moderation, L Data-to-Text Generation, ` Education/Readability, Ð\nCode Generation, y Math/Problem Solving, \u001e String-Distance/Edit-Based.\nSALSA.\nIn researching text simplification metrics we found that an extension to the LENS (Mad-\ndela et al., 2023) metric exists which is meant to better align with human judgement. It was also a\nrelated metric to the SimpEval (Maddela et al., 2023) paper. Thus we chose to implement SALSA\n(Heineman et al., 2023) in our MetricBank as it was intended as one of the recommended “Best”\nmetrics for our in-distribution SimpEval task.\n22\n"}, {"page": 23, "text": "Preprint\nMetric\nGPU\nCPU\nTime (ms)\nINFORMRewardModel\n129.62 GB\n2.04 GB\n1041\nLDLRewardModel\n104.17 GB\n2.06 GB\n1921\nGRMRewardModel\n6.02 GB\n1.96 GB\n61\nUniEvalDialogue\n3.07 GB\n3.10 GB\n262\nUniEvalSum\n3.07 GB\n3.10 GB\n211\nUniEvalFact\n3.07 GB\n3.09 GB\n61\nPerplexity gpt2-large\n3.00 GB\n1.47 GB\n48\nBLEURT\n2.15 GB\n2.75 GB\n43\nBARTScore bart-large-cnn\n1.52 GB\n1.34 GB\n49\nSummaQA\n1.25 GB\n1.51 GB\n879\nYiSi\n687 MB\n1.39 GB\n35\nSentiment\n485 MB\n1.36 GB\n19\nToxicity\n485 MB\n1.36 GB\n39\nFactCC\n427 MB\n1.29 GB\n17\nParaScoreFree\n346 MB\n1.68 GB\n12 428\nParaScore\n338 MB\n1.05 GB\n4 543\nMOVERScore distilbert-base-uncased\n262 MB\n1.50 GB\n2 899\nBERTScore roberta-large\n8 MB\n1.47 GB\n1 303\nPRMRewardModel\n0 MB\n13.64 GB\n6 359\nMAUVE max\n0 MB\n4.22 GB\n3 236\nFastTextEducationalValue\n0 MB\n3.73 GB\n6\nLENS\n0 MB\n3.25 GB\n3 408\nLENS SALSA\n0 MB\n2.84 GB\n426\nFastTextToxicity\n0 MB\n1.67 GB\n11\nFastTextNSFW\n0 MB\n1.67 GB\n6\nInfoLM\n0 MB\n1.12 GB\n2 338\nMETEOR\n0 MB\n1.08 GB\n27\nFKGL\n0 MB\n894 MB\n6\nTER\n0 MB\n731 MB\n26 064\nCHRF\n0 MB\n730 MB\n36\nDistinctNGram\n0 MB\n730 MB\n19\niBLEU\n0 MB\n730 MB\n18\nBLEU\n0 MB\n729 MB\n7\nLevenshteinDistance min\n0 MB\n729 MB\n0\nSelfBLEU\n0 MB\n729 MB\n6\nHammingDistance min\n0 MB\n729 MB\n0\nJaroWinklerSimilarity max\n0 MB\n729 MB\n0\nGLEU\n0 MB\n728 MB\n9\nSARI\n0 MB\n728 MB\n95\nJaccardDistance min\n0 MB\n728 MB\n0\nCharCut\n0 MB\n728 MB\n1 237\nUpdateROUGE\n0 MB\n728 MB\n96\nNIST\n0 MB\n728 MB\n21\nLevenshteinRatio max\n0 MB\n727 MB\n0\nJaroSimilarity max\n0 MB\n727 MB\n0\nROUGE\n0 MB\n726 MB\n487\nCIDEr n4 sig6.0\n0 MB\n726 MB\n31\nPseudoPARENT\n0 MB\n726 MB\n10\nTable 5: Maximum CI upper-bound GPU/CPU memory and latency per metric.\nFastText Classifiers.\nWe wanted to add diversity to our MetricBank by including more classi-\nfiers for various higher-level concepts, but we didn’t want to add unnecessary expenses to running\n23\n"}, {"page": 24, "text": "Preprint\nthe metrics. FastText Classifiers are a nice compromise which are quick to run on CPU but also\nhave reasonable classification accuracy. We implement FastTextNSFW and FastTextToxicity from\nDolma (Soldaini et al., 2024), and we take FastTextEducationalValue (Tsui & Nguyen, 2024)\nwhich has been used for data filtering to attempt to find Text-Book quality training data.\nC\nPROMPTS AND SIGNATURES\nC.1\nLLM-AS-A-JUDGE PROMPTS\nWe use the LLM-as-a-Judge Prompts from the original human annotation process for a given dataset\nwhenever available. We consider these as a strong baseline as these instructions were designed to be\nuseful instructions for human annotators and ideally were the underlying instructions guiding their\nannotation decisions.\nTask: SimpEval\nLLM-as-a-Judge Prompt:\n## Rating Sentences\nThe goal is to **rate sentences** by how well they **simplify the\noriginal sentence**.\n### Scoring Guidelines\n| Score | When to assign it |\n|-------|------------------|\n| **100** | The sentence is **fully simplified**, entirely fluent,\nand **preserves the core meaning** of the original. |\n| **75**\n| The sentence is **somewhat simpler**, mostly fluent, and\nthe meaning is **close** to the original. |\n| **50**\n| The sentence is simpler, **somewhat fluent**, and the\nmeaning is **similar** to the original. |\n| **25**\n| The sentence is equivalently simple, still has some\nfluency, but **loses the meaning**. |\n| **0**\n| The sentence is **completely unreadable**. |\n> **Most scores will lie somewhere in this range - feel free to give\nspecific scores (e.g., 83, 67) rather than only the five anchors.**\n---\n### Examples\n| Score | Example Simplified Sentence | Why this score? |\n|-------|-----------------------------|-----------------|\n| **100** | *It will then **move away from the river bed** and sink\nback to the bottom to digest its food.* | Reads fluently **and**\nkeeps the original meaning (\"it\" gets unstuck, moves down, digests\nfood). |\n| **75** | *Due to this, **a lot of mosques don’t enforce these\nrules** but both men and women should follow them.* | Minor fluency\nissue, but meaning matches the original. |\n| **0** | *A gadget javascript a is and / checking wikipedia an\nsnippet that can be enabled simply by , or css option in your\nwikipedia preferences.* | Sentence is **unreadable**. |\n24\n"}, {"page": 25, "text": "Preprint\nTask: HelpSteer2\nLLM-as-a-Judge Prompt:\n**Helpfulness/Understanding:**\n- 4 - The response is extremely helpful and completely aligned with\nthe spirit of what the prompt\nwas asking for.\n- 3 - The response is mostly helpful and mainly aligned with what\nthe user was looking for, but\nthere is still some room for improvement.\n- 2 - The response is partially helpful but misses the overall goal\nof the user’s query/input in some\nway. The response did not fully satisfy what the user was looking\nfor.\n- 1 - The response is borderline unhelpful and mostly does not\ncapture what the user was looking\nfor, but it is still usable and helpful in a small way.\n- 0 - The response is not useful or helpful at all. The response\ncompletely missed the essence of\nwhat the user wanted.\nTask: EvalGenProduct\nLLM-as-a-Judge Prompt:\nIs this response good (1) or bad (0)?\nTask: RealHumanEval\nLLM-as-a-Judge Prompt:\nWould you accept this code edit/addition (1) or reject it (0)?\nTask: CoGymTravelOutcome\nLLM-as-a-Judge Prompt:\nOverall rating to the final outcome (i.e., travel plan, analysis\nresult) (1-5 scale)\n(1) \"Extremely dissatisfied\",\n(2) \"Somewhat dissatisfied\",\n(3) \"Neutral\",\n(4) \"Somewhat satisfied\",\n(5) \"Extremely satisfied\"\n25\n"}, {"page": 26, "text": "Preprint\nC.2\nMISC PROMPTS\nMetricCard Generation\nPrompt:\nYou are an expert in natural language processing and technical\ndocumentation, specializing in metrics for evaluating generative\nmodels. I am building a metric bank to recommend the best metrics\nfor various generative tasks. Each metric in this bank will have a\ncorresponding Metric Card, which provides standardized, detailed\ndocumentation about the metric. These Metric Cards will serve as a\nkey resource for researchers and practitioners, helping them select\nthe right metric for their task.\n## Your Task\nUsing the provided materials, including the original paper,\nreference implementations, the Metric Card Template, and the BLEU\nMetric Card Example, your task is to draft a comprehensive Metric\nCard for the given metric. The documentation must:\n1.\nFollow the provided template closely, ensuring\nadherence to its format and required sections.\n2.\nIncorporate relevant details from the original paper\nand reference materials, ensuring technical accuracy and\ncompleteness.\n3.\nMatch the style and quality of the BLEU example,\nwhich serves as an exemplar for clarity, structure, and precision.\nSpecific Instructions\n1.\nKey Sections to Address: Ensure each required\nsection of the template is filled out thoughtfully and thoroughly,\nincluding:\n-\nMetric Description\n-\nInputs and Outputs\n-\nFormal Definition\n-\nApplicability and Limitations\n-\nKnown Limitations and Related Metrics\n2.\nIf Information is Unclear or Missing: Do not\nfabricate or make assumptions. If information is unavailable,\nunclear, or not included in the provided context, leave that section\nblank or mark it as \"Needs more information.\"\n3.\nMarkdown Formatting: Output the completed Metric\nCard as a markdown text block rather than rendering or printing the\nmarkdown directly.\nThis means you must surround your answer in ‘‘‘.\nAlso start the block with \"---\" as shown in the examples.\nDo not\nend the block with \"---\".\n4.\nFocus on Consistency: Use the provided categorical\nsuggestions (see below) to ensure uniformity across all Metric\nCards, particularly in fields like \"Metric Type,\" \"Domain,\" and\n\"Tasks.\"\n5.\nMathematical Formatting:\n-\nUse $ for inline math expressions (e.g.,\n$r$, not $ r $).\n-\nUse $$ for block math expressions and ensure\na full line break before and after each block math expression. This\nformatting ensures proper rendering in markdown.\n-\nExample of proper usage for $$:\n** Correct **\n‘‘‘\nWhere:\n- $CHRP$ is the average precision of character and word n-grams:\n26\n"}, {"page": 27, "text": "Preprint\n$$\nCHRP = \\frac{1}{N} \\sum_{n=1}ˆN \\frac{\\text{n-grams in hypothesis\nand reference}}{\\text{total n-grams in hypothesis}}\n$$\n- $CHRR$ is the average recall of character and word n-grams:\n$$\nCHRR = \\frac{1}{N} \\sum_{n=1}ˆN \\frac{\\text{n-grams in hypothesis\nand reference}}{\\text{total n-grams in reference}}\n$$\n‘‘‘\n** Incorrect **\n‘‘‘\nWhere:\n- $CHRP$ is the average precision of character and word n-grams:\n$$\nCHRP = \\frac{1}{N} \\sum_{n=1}ˆN \\frac{\\text{n-grams in hypothesis\nand reference}}{\\text{total n-grams in hypothesis}}\n$$\n- $CHRR$ is the average recall of character and word n-grams:\n$$\nCHRR = \\frac{1}{N} \\sum_{n=1}ˆN \\frac{\\text{n-grams in hypothesis\nand reference}}{\\text{total n-grams in reference}}\n$$\n‘‘‘\n-\nEnsure all block math expressions are clearly\nseparated from list items or inline text.\n-\nAdd a space after operators like \\sum, \\max,\nor any LaTeX commands followed by an underscore (_) to prevent\nMarkdown parsers from interpreting _ as italic markers.\nMainly it\nis critical to put a space before \"_\". For example:\n** Correct **\n‘‘‘\n$$\nR _{\\text{BERT}} = \\frac{\\sum _{x _{i} \\in x} \\text{idf}(x _{i})\n\\cdot \\max _{\\hat{x} _{j} \\in \\hat{x}} x _{iˆ\\top} \\hat{x}\n_{j}}{\\sum _{x _{i} \\in x} \\text{idf}(x _{i})}\n$$\n‘‘‘\n** Incorrect **\n‘‘‘\n$$\nR_{\\text{BERT}} = \\frac{\\sum_{x_i \\in x} \\text{idf}(x_i) \\cdot\n\\max_{\\hat{x}_j \\in \\hat{x}} x_iˆ\\top \\hat{x}_j}{\\sum_{x_i \\in x}\n\\text{idf}(x_i)}\n$$\n‘‘‘\n6. Citation: It is imperative that you do NOT make this up.\nIf the user does not explicitly provide the bibtex citation for the\nmetric then you must say [More Information Needed].\nIf a citation\nis provided you must copy it EXACTLY.\nDo NOT try to simplify any of\nthe components such as the author list with an ellipsis.\n## Categorical Suggestions for Consistency\n27\n"}, {"page": 28, "text": "Preprint\nNote: These suggestions are not exhaustive. While you should\nprioritize using the categories listed here for consistency, you may\nadd new categories if the metric clearly warrants them.\n### Domains\nThese represent broad areas of application for the metric. Choose\none or more:\n-\nText Generation\n-\nSpeech Generation\n-\nCode Generation\n-\nMultimodal Generation\n-\nImage Captioning\n-\nDialogue Systems\n-\nStorytelling\n### Tasks\nThese are specific tasks or use cases where the metric applies.\nChoose one or more:\n-\nMachine Translation\n-\nSummarization\n-\nParaphrasing\n-\nData-to-Text Generation\n-\nImage-to-Text Generation\n-\nDialogue Generation\n-\nStyle Transfer\n-\nCreative Writing (e.g., poetry, storytelling)\n-\nCode Completion\n-\nResponse Generation\n### Metric Type\nThese classify the metric based on its design and purpose. Choose\none:\n-\nSurface-Level Similarity (e.g., BLEU, ROUGE)\n-\nSemantic Similarity (e.g., BERTScore)\n-\nFluency (e.g., perplexity-based metrics)\n-\nDiversity (e.g., distinct-n)\n-\nRobustness (e.g., adversarial robustness metrics)\n-\nFairness\n-\nFaithfulness (e.g., factual consistency metrics)\n-\nReference-Free (e.g., coherence or novelty scoring)\n-\nExplainability\n### Inputs\nThese describe what the metric requires for evaluation:\n-\nReference-Based\n-\nReference-Free\n-\nInput-Required\n-\nInput-Optional\n## Materials You Will Be Provided\n1.\nOriginal Paper: The foundational paper introducing\nor defining the metric.\n2.\nReference Implementations (when available):\nDocumentation from popular implementations (e.g., SacreBLEU README\nfor BLEU).\n3.\nMetric Card Template: The standardized structure for\nall Metric Cards (see below).\n28\n"}, {"page": 29, "text": "Preprint\n4.\nBLEU Metric Card Example: A high-quality example for\nreference.\n=== TEMPLATE FOR METRIC CARDS ===\n---\n# Metric Card for {{ metric_name | default(\"Metric Name\", true) }}\n{{ metric_summary | default(\"A brief description of the metric and\nits purpose.\", true) }}\n## Metric Details\n### Metric Description\n{{ metric_description | default(\"Detailed explanation of the metric,\nincluding how it is calculated and what it measures.\", true) }}\n- **Metric Type:** {{ metric_type | default(\"[More Information\nNeeded]\", true) }}\n- **Range:** {{ metric_range | default(\"[More Information Needed]\",\ntrue) }}\n- **Higher is Better?:** {{ higher_is_better | default(\"[More\nInformation Needed]\", true) }}\n- **Reference-Based?:** {{ reference_based | default(\"[More\nInformation Needed]\", true) }}\n- **Input-Required?:** {{ input_required | default(\"[More\nInformation Needed]\", true) }}\n### Formal Definition\n{{ metric_definition | default(\"Mathematical formula or detailed\nalgorithmic definition.\", true) }}\n### Inputs and Outputs\n- **Inputs:**\n{{ metric_inputs | default(\"Description of required inputs (e.g.,\ngenerated text, reference text, input prompt).\", true) }}\n- **Outputs:**\n{{ metric_outputs | default(\"Description of the metric output\n(e.g., scalar score, distribution).\", true) }}\n## Intended Use\n### Domains and Tasks\n- **Domain:** {{ domain | default(\"[More Information Needed]\", true)\n}}\n- **Tasks:** {{ tasks | default(\"[More Information Needed]\", true) }}\n### Applicability and Limitations\n- **Best Suited For:** {{ best_suited_for | default(\"[More\nInformation Needed]\", true) }}\n- **Not Recommended For:** {{ not_recommended_for | default(\"[More\nInformation Needed]\", true) }}\n## Metric Implementation\n### Reference Implementations\n29\n"}, {"page": 30, "text": "Preprint\n- **Libraries/Packages:** {{ libraries | default(\"[More Information\nNeeded]\", true) }}\n### Computational Complexity\n- **Efficiency:** {{ efficiency | default(\"[More Information\nNeeded]\", true) }}\n- **Scalability:** {{ scalability | default(\"[More Information\nNeeded]\", true) }}\n## Known Limitations\n{{ known_limitations | default(\"[More Information Needed]\", true) }}\n- **Biases:** {{ biases | default(\"Potential biases inherent in the\nmetric.\", true) }}\n- **Task Misalignment Risks:** {{ task_misalignment | default(\"[More\nInformation Needed]\", true) }}\n- **Failure Cases:** {{ failure_cases | default(\"[More Information\nNeeded]\", true) }}\n## Related Metrics\n{{ related_metrics | default(\"[More Information Needed]\", true) }}\n## Further Reading\n- **Papers:** {{ papers | default(\"[More Information Needed]\", true)\n}}\n- **Blogs/Tutorials:** {{ blogs | default(\"[More Information\nNeeded]\", true) }}\n## Citation\n{{ bibtex_citation | default(\"[More Information Needed]\", true) }}\n## Metric Card Authors\n- **Authors:** {{ metric_authors | default(\"[More Information\nNeeded]\", true) }}\n- **Acknowledgment of AI Assistance:**\n{{ ai_assistance | default(\"Portions of this metric card were\ndrafted with assistance from generative AI. All content has been\nreviewed and curated by the author to ensure accuracy.\", true) }}\n- **Contact:** {{ metric_contact | default(\"[More Information\nNeeded]\", true) }}\n======\n=== BLEU Metric Card Example ===\n---\n# Metric Card for BLEU\nBLEU (Bilingual Evaluation Understudy) is a widely used metric for\nevaluating the quality of text generated in tasks like machine\ntranslation and summarization. It measures the overlap of n-grams\nbetween a generated text and one or more reference texts, with a\nbrevity penalty to penalize overly short translations. SacreBLEU, a\nmodern implementation, ensures reproducibility and standardization\nof BLEU scores across research.\n## Metric Details\n30\n"}, {"page": 31, "text": "Preprint\n### Metric Description\nBLEU evaluates the quality of text generation by comparing n-grams\nin the generated output with those in one or more reference texts.\nIt computes modified precision for n-grams and combines scores using\na geometric mean, with a brevity penalty to ensure the length of the\ngenerated text matches that of the references. Higher BLEU scores\nindicate closer similarity to the references.\n- **Metric Type:** Surface-Level Similarity\n- **Range:** 0 to 1\n- **Higher is Better?:** Yes\n- **Reference-Based?:** Yes\n- **Input-Required?:** No\n### Formal Definition\n$$\n\\text{BLEU} = \\text{BP} \\cdot \\exp \\left( \\sum_{n=1}ˆN w_n \\log p_n\n\\right)\n$$\nwhere:\n- $\\text{BP} = \\min(1, eˆ{1 - r/c})$ is the brevity penalty,\n- $r$ is the effective reference length (based on the closest\nmatching reference length for each sentence),\n- $c$ is the candidate translation length,\n- $p_n$ is the modified precision for n-grams of length $n$,\n- $w_n$ are weights for each n-gram (commonly uniform, $w_n =\n\\frac{1}{N}$).\n### Inputs and Outputs\n- **Inputs:**\n- Generated text (candidate translation)\n- Reference text(s) (gold-standard translations)\n- **Outputs:**\n- Scalar BLEU score (range: 0 to 1)\n## Intended Use\n### Domains and Tasks\n- **Domain:** Text Generation\n- **Tasks:** Machine Translation, Summarization, Data-to-Text\nGeneration\n### Applicability and Limitations\n- **Best Suited For:**\nStructured tasks with a clear correspondence between generated and\nreference texts, such as translation or summarization.\n- **Not Recommended For:**\nOpen-ended or creative generation tasks where diversity or\nsemantic similarity matters more than lexical overlap (e.g.,\nstorytelling, dialogue).\n## Metric Implementation\n### Reference Implementations\n31\n"}, {"page": 32, "text": "Preprint\n- **Libraries/Packages:**\n- [SacreBLEU](https://github.com/mjpost/sacrebleu) (robust,\nstandard implementation)\n- [NLTK](https://www.nltk.org/api/nltk.translate.html) (basic\nPython implementation)\n- [Hugging Face ‘evaluate‘](https://huggingface.co/docs/evaluate)\n(integrated metric framework)\n### Computational Complexity\n- **Efficiency:**\nBLEU is computationally efficient, requiring $O(n \\cdot m)$\noperations for $n$-gram matching where $n$ is the number of words in\nthe candidate text and $m$ is the number of reference words.\nSacreBLEU optimizes tokenization and scoring, making it highly\nsuitable for large-scale evaluations.\n- **Scalability:**\nBLEU scales well across datasets of varying sizes due to its\nsimple design. SacreBLEU further supports evaluation with multiple\nreferences, diverse tokenization schemes, and language-specific\npreprocessing, making it adaptable to diverse evaluation setups.\n## Known Limitations\n- **Biases:**\n- BLEU penalizes valid paraphrases or semantically equivalent\noutputs that do not match reference n-grams exactly.\n- The brevity penalty can overly penalize valid shorter outputs,\nparticularly for tasks where shorter text may be acceptable or even\npreferred (e.g., summarization).\n- **Task Misalignment Risks:**\n- BLEU is not designed for evaluating tasks with high diversity in\nacceptable outputs (e.g., open-ended dialogue).\n- Scores depend on the quality and number of references; fewer or\ninconsistent references can lead to misleading evaluations.\n- **Failure Cases:**\n- BLEU struggles to capture semantic adequacy beyond lexical\nsimilarity. For instance, it cannot identify whether a translation\npreserves the meaning of the original sentence if word choices\ndiverge significantly.\n## Related Metrics\n- **ROUGE:** Often used for summarization tasks, emphasizing recall\nover precision.\n- **METEOR:** Incorporates synonym matching for better semantic\nalignment.\n- **BERTScore:** Uses contextual embeddings for semantic similarity.\n## Further Reading\n- **Papers:**\n- [Original BLEU Paper (Papineni et al.,\n2002)](https://www.aclweb.org/anthology/P02-1040)\n- [SacreBLEU: A Call for Clarity in Reporting BLEU Scores (Post,\n2018)](https://www.aclweb.org/anthology/W18-6319)\n- **Blogs/Tutorials:**\n32\n"}, {"page": 33, "text": "Preprint\n- [Understanding BLEU](https://machinelearningmastery.com\n/calculate-bleu-score-for-text-python/)\n- [SacreBLEU Documentation](https://github.com/mjpost/sacrebleu)\n## Citation\n@inproceedings{papineni-etal-2002-bleu,\ntitle = \"{B}leu: a Method for Automatic Evaluation of Machine\nTranslation\",\nauthor = \"Papineni, Kishore\nand\nRoukos, Salim\nand\nWard, Todd\nand\nZhu, Wei-Jing\",\neditor = \"Isabelle, Pierre\nand\nCharniak, Eugene\nand\nLin, Dekang\",\nbooktitle = \"Proceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics\",\nmonth = jul,\nyear = \"2002\",\naddress = \"Philadelphia, Pennsylvania, USA\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/P02-1040/\",\ndoi = \"10.3115/1073083.1073135\",\npages = \"311--318\"\n}\n## Metric Card Authors\n- **Authors:** Michael J. Ryan\n- **Acknowledgment of AI Assistance:**\nPortions of this metric card were drafted with assistance from\nOpenAI’s ChatGPT, based on user-provided inputs and relevant\ndocumentation. All content has been reviewed and curated by the\nauthor to ensure accuracy.\n- **Contact:** michaeljryan@stanford.edu\n======\nThe metric you will be designing a card for is {Metric Name}\n=== {SUPPLEMENTAL MATERIALS} ===\n======\nNow please write a high quality metric card for {Metric Name} given\nthe provided materials!\nFinal **Important** Note: If the provided materials do not give\nenough information about a particular point for the metric (e.g.\nlimitations or biases aren’t listed) then do NOT make things up.\nYou can leave blanks or \"Needs more information\" where needed.\nIt\nis absolutely essential not to make things up or guess when\nproducing this documentation otherwise future researchers and\nengineers will be confused and led astray.\nAvoid making up links\nthat you aren’t fully confident in the url.\nRemember to surround your answer in ‘‘‘.\nThanks!\n33\n"}, {"page": 34, "text": "Preprint\nC.3\nDSPY SIGNATURES\nSignature:\nGeneratePerturbationStrategies\nInstruction:\nYou will be given:\n- A Task description\n- A Dimension to prioritize when perturbing outputs\n- The Example Input, optional Example Reference, and Example Output\nInstructions:\nYour primary focus should be on degrading performance along the\nspecified Dimension.\n1. Begin with a rich reasoning paragraph (3-5 sentences) that\nexplores a variety of ways to subtly degrade model outputs. Do\nnot reference the specific example.\n2. Under the heading **Strategies:**, list 1-3 numbered, high-level\nperturbation strategies.\n- Each strategy should be a short phrase (5-15 words) naming the\ncategory of change, followed by one concise sentence of abstract\nexplanation.\n- Do not include concrete rewrites, instance-specific examples,\nor example sentences.\nInputs:\nField\nType\nDescription\ntask\nstr\nThe task the model was originally\ntrying to complete\nexample_sets\nlist[str]\nExample inputs, outputs, and\n(optionally) references showing task\nperformance\ndimension\nstr\nThe dimension to prioritize for the\nperturbation\nOutputs:\nField\nType\nDescription\nperturbation_\nstrategies\nlist[str]\n1-3 high-level strategies to test\nrobustness\nSignature:\nPerturbWorse\nInstruction:\nYou will be given:\n- A Task description\n- A Dimension to prioritize when perturbing outputs\n- The Example Input, optional Example Reference, and Model Output\n- A perturbation_strength value (\"subtle\" or \"obvious\")\n- A list of perturbation_strategies to apply\nInstructions:\nYour goal is to apply each strategy to the Model Output and produce\na degraded version that specifically harms performance along the\ngiven Dimension, using the specified strength.\nUnder the heading **Perturbed Outputs:**, return exactly one\nperturbed output per strategy.\n- For **subtle** strength, introduce minimal distortion.\n34\n"}, {"page": 35, "text": "Preprint\n- For **obvious** strength, introduce more pronounced\ndegradation.\nDo **not** include any reasoning, explanations, or examples -- only\nthe perturbed text.\nInputs:\nField\nType\nDescription\ntask\nstr\nThe task that the model was originally\ntrying to complete\ndimension\nstr\nThe dimension to prioritize for the\nperturbation (this should be the aspect\nof the model output that is most\nimpacted by the perturbation)\ninput\nstr\nThe input provided to the model\nreferences\nUnion[list[str],\nNone]\nThe references of good outputs (may be\nNone)\nmodel_output\nstr\nThe output produced by the model\nperturbation_\nstrength\nLiteral[’subtle’,\n’obvious’]\nThe strength of the perturbation (subtle\nor obvious)\nperturbation_\nstrategies\nlist[str]\nThe perturbation strategies to use\nOutputs:\nField\nType\nDescription\nperturbed_\noutputs\nlist[str]\nPerturbed text that is worse than the\noriginal model output. Produce one\nperturbed output per strategy.\nSignature:\nPerturbSame\nInstruction:\nYou will be given:\n- A Task description\n- A Dimension to preserve when perturbing outputs\n- The Example Input, optional Example Reference, and Model Output\n- A perturbation_strength value (\"subtle\" or \"obvious\")\nInstructions:\nApply a perturbation to the Model Output that **maintains**\nperformance on the specified Dimension.\nUnder the heading **Perturbed Output:** return exactly one string:\n- For **subtle** strength, apply a minimal change that does not\nimpair the target Dimension.\n- For **obvious** strength, apply a more noticeable change that\nstill keeps the target Dimension intact.\nSome examples of types of perturbations would include: rephrasing,\nreordering, replacing words with synonyms, stylistic changes,\netc. that do not impair the target Dimension.\nIf any change would harm the specified Dimension, simply return the\noriginal Model Output.\nAfter producing your original plan/reasoning do **not** include any\nmore reasoning, explanations, or examples -- only the perturbed\ntext.\nInputs:\n35\n"}, {"page": 36, "text": "Preprint\nField\nType\nDescription\ntask\nstr\nThe task that the model was originally\ntrying to complete\ninput\nstr\nThe input provided to the model\nreferences\nUnion[list[str],\nNone]\nThe references of good outputs (may be\nNone)\nmodel_output\nstr\nThe output produced by the model\nperturbation_\nstrength\nLiteral[’subtle’,\n’obvious’]\nThe strength of the perturbation (subtle\nor obvious)\ndimension\nstr\nThe aspect of the model output that\nMUST be preserved in quality\nOutputs:\nField\nType\nDescription\nperturbed_output str\nPerturbed text that preserves\nperformance along the given\nDimension.\nSignature:\nLLMAsAJudgeSignature\nInstruction:\nGiven an input text, the task description that the model was trying\nto follow, and a measure to rate the text on, return a score on\nthis measure.\nInputs:\nField\nType\nDescription\ntext\nAny\nThe input text that we want to rate.\ntask_description Any\nA description of the task that the model\nwas trying to solve when it generated\nthe text. Could be left blank if not\navailable.\nmeasure\nAny\nThe measure that we want to rate the\ntext on.\nsuggested_range\nAny\nThe suggested range of possible values\nfor the measure.\nOutputs:\nField\nType\nDescription\nscore\nAny\nThe score that the text should receive on\nthis measure.\nSignature:\nLLMMetricRecommendationSignature\nInstruction:\nI am looking for a metric to evaluate the attached task. In\nparticular I care about the specific target measurement that I\nattached.\n36\n"}, {"page": 37, "text": "Preprint\nPlease help me decide from among the metrics that I have attached\ndocumentation for which one is most relevant to the task and\ntarget.\nPlease provide a ranking of the metrics from most relevant to least\nrelevant for the task and target above.\nYou can reason first about what makes a metric relevant for the task\nand target, and then provide your ranking.\nIMPORTANT: The final ranking should be a list of EXACT metric class\nnames (no hyphens, no spaces, no extra words).\nUse the METRIC\nNAME not what it is called in the documentation.\nFor example, use \"SelfBLEU\" not \"Self-BLEU\", use \"BERTScore\" not\n\"BERT Score\", use \"BLEU\" not \"BLEU Score\".\nThe final ranking should just be a list of metric names, in order\nfrom most relevant to least relevant.\nThe list should be exactly ‘num_metrics_to_recommend‘ items long.\nInputs:\nField\nType\nDescription\ntask_description str\nA description of the task that an LLM\nperformed and that I now want to\nevaluate.\ntarget\nstr\nThe specific target measurement that I\nwant to evaluate about the task.\nmetric_\ndocumentation\nList[str]\nA list of metric names and their\ndocumentation. The documentation will\ncontain the metric name, as well as\nmany details about the metric.\nnum_metrics_\nto_recommend\nint\nThe number of metrics to recommend.\nIt is imperative to target this number or\nvery very close to it. We will do more\nextensive filtering later.\nOutputs:\nField\nType\nDescription\nranking\nList[str]\nA numbered list of EXACT metric class\nnames (no hyphens, no spaces, no extra\nwords), in order from most relevant to\nleast relevant. The list should be of\nlength ‘num metrics to recommend‘.\nYou should write the number in front of\nthe metric name (e.g ’1.\nMETRIC1 NAME’, ’2.\nMETRIC2 NAME’, etc.).\nREMEMBER: Put quotes around\nEACH number + metric name pair, not\njust one set of quotes for the full string.\nIMPORTANT: Refer to ”METRIC\nNAME: ...” for the exact name of the\nmetric or it won’t be a match.\n37\n"}, {"page": 38, "text": "Preprint\nSignature:\nGenerateRubricSignature\nInstruction:\nGiven a dataset, task description, and an evaluation metric,\ngenerate a rubric for the metric scoring from 1 to 5.\nInputs:\nField\nType\nDescription\ntask_description Any\nA description of the task that the model\nis trying to solve.\ngood_examples\nAny\nA list of good examples of outputs for a\nmodel.\nbad_examples\nAny\nA list of bad examples of outputs for a\nmodel.\nmetric_title\nAny\nThe title of the metric.\nmetric_\ndescription\nAny\nA description of the metric.\nOutputs:\nField\nType\nDescription\nscore_one_\ndescription\nAny\nA description of what a score of 1\nmeans. This can be a bullet point list of\nwhat criteria to look for in assigning a\nscore of 1.\nscore_two_\ndescription\nAny\nA description of what a score of 2\nmeans. This can be a bullet point list of\nwhat criteria to look for in assigning a\nscore of 2.\nscore_three_\ndescription\nAny\nA description of what a score of 3\nmeans. This can be a bullet point list of\nwhat criteria to look for in assigning a\nscore of 3.\nscore_four_\ndescription\nAny\nA description of what a score of 4\nmeans. This can be a bullet point list of\nwhat criteria to look for in assigning a\nscore of 4.\nscore_five_\ndescription\nAny\nA description of what a score of 5\nmeans. This can be a bullet point list of\nwhat criteria to look for in assigning a\nscore of 5.\nSignature:\nGenerateAxisOfVariationSignature\nInstruction:\nGiven a task description, a target metric, and good/bad examples,\ngenerate a list of axes of variation which could be used to\nexplain the differences between the good and bad examples.\nThese axes of variation will be used as measures to evaluate the\nmodel’s performance, so they should be informative and useful\nfor the model to improve on.\nInputs:\n38\n"}, {"page": 39, "text": "Preprint\nField\nType\nDescription\ntask_description str\nA description of the overall task the\nmodel is trying to solve.\ntarget_name\nOptional[str]\nOptional hint of the target\nmetric/column we care about. Could be\n’None’ or something generic like\n’quality’ or ’score’.\ngood_examples\nList[str]\nA list of examples with *high* quality\naccording to the target metric.\nbad_examples\nList[str]\nA list of examples with *low* quality\naccording to the target metric.\nnum_axes_to\n_generate\nint\nThe number of axes of variation to\ngenerate.\nOutputs:\nField\nType\nDescription\naxes_of\n_variation\nList[str]\nAn ordered list (most-important first)\ndescribing possible axes of variation.\nPlease bold the name of the axis of\nvariation (e.g. **Axes Name**), and\nALSO include a brief sentence-long\nexplanation of the axis of variation.\n(e.g. **Axes Name** Brief\nExplanation). Please include exactly\n’num axes to generate’ axes of\nvariation in the output. Avoid special\ncharacters since they sometimes mess\nup the parsing.\nD\nEXAMPLE METRIC CARD: BLEU\nMetric Card for BLEU\nBLEU (Bilingual Evaluation Understudy) is a widely used metric for evaluating the quality of text\ngenerated in tasks like machine translation and summarization. It measures the overlap of n-grams\nbetween a generated text and one or more reference texts, with a brevity penalty to penalize overly\nshort translations. SacreBLEU, a modern implementation, ensures reproducibility and standardiza-\ntion of BLEU scores across research.\nMetric Details\nMetric Description\nBLEU evaluates the quality of text generation by comparing n-grams in the generated output with\nthose in one or more reference texts. It computes modified precision for n-grams and combines\nscores using a geometric mean, with a brevity penalty to ensure the length of the generated text\nmatches that of the references. Higher BLEU scores indicate closer similarity to the references.\n• Metric Type: Surface-Level Similarity\n• Range: 0 to 1\n• Higher is Better?: Yes\n• Reference-Based?: Yes\n• Input-Required?: No\n39\n"}, {"page": 40, "text": "Preprint\nFormal Definition\nBLEU = BP · exp\n N\nX\nn=1\nwn log pn\n!\n• BP = min(1, e1−r/c) is the brevity penalty,\n• r is the effective reference length (based on the closest matching reference length for each\nsentence),\n• c is the candidate translation length,\n• pn is the modified precision for n-grams of length n,\n• wn are weights for each n-gram (commonly uniform, wn = 1\nN ).\nInputs and Outputs\n• Inputs:\n– Generated text (candidate translation)\n– Reference text(s) (gold-standard translations)\n• Outputs:\n– Scalar BLEU score (range: 0 to 1)\nIntended Use\nDomains and Tasks\n• Domain: Text Generation\n• Tasks: Machine Translation, Summarization, Data-to-Text Generation\nApplicability and Limitations\n• Best Suited For: Structured tasks with a clear correspondence between generated and\nreference texts, such as translation or summarization.\n• Not Recommended For: Open-ended or creative generation tasks where diversity or se-\nmantic similarity matters more than lexical overlap (e.g., storytelling, dialogue).\nMetric Implementation\nReference Implementations\n• Libraries/Packages:\n– SacreBLEU (robust, standard implementation)\n– NLTK (basic Python implementation)\n– Hugging Face evaluate (integrated metric framework)\nComputational Complexity\n• Efficiency: BLEU is computationally efficient, requiring O(n · m) operations for n-gram\nmatching where n is the number of words in the candidate text and m is the number of\nreference words. SacreBLEU optimizes tokenization and scoring, making it highly suitable\nfor large-scale evaluations.\n• Scalability: BLEU scales well across datasets of varying sizes due to its simple de-\nsign. SacreBLEU further supports evaluation with multiple references, diverse tokenization\nschemes, and language-specific preprocessing, making it adaptable to diverse evaluation\nsetups.\n40\n"}, {"page": 41, "text": "Preprint\nKnown Limitations\n• Biases:\n– BLEU penalizes valid paraphrases or semantically equivalent outputs that do not\nmatch reference n-grams exactly.\n– The brevity penalty can overly penalize valid shorter outputs, particularly for tasks\nwhere shorter text may be acceptable or even preferred (e.g., summarization).\n• Task Misalignment Risks:\n– BLEU is not designed for evaluating tasks with high diversity in acceptable outputs\n(e.g., open-ended dialogue).\n– Scores depend on the quality and number of references; fewer or inconsistent refer-\nences can lead to misleading evaluations.\n• Failure Cases:\n– BLEU struggles to capture semantic adequacy beyond lexical similarity. For instance,\nit cannot identify whether a translation preserves the meaning of the original sentence\nif word choices diverge significantly.\nRelated Metrics\n• ROUGE: Often used for summarization tasks, emphasizing recall over precision.\n• METEOR: Incorporates synonym matching for better semantic alignment.\n• BERTScore: Uses contextual embeddings for semantic similarity.\nFurther Reading\n• Papers:\n– Original BLEU Paper (Papineni et al., 2002)\n– SacreBLEU: A Call for Clarity in Reporting BLEU Scores (Post, 2018)\n• Blogs/Tutorials:\n– Understanding BLEU\n– SacreBLEU Documentation\nMetric Card Authors\n• Authors: Michael J. Ryan\n• Acknowledgment of AI Assistance: Portions of this metric card were drafted with assis-\ntance from OpenAI’s ChatGPT, based on user-provided inputs and relevant documentation.\nAll content has been reviewed and curated by the author to ensure accuracy.\n• Contact: michaeljryan@stanford.edu\nE\nAUTOMETRICS DESIGN ABLATIONS\nE.1\nRETRIEVE\nFor our retrieval experiments we run all metrics in the MetricBank to get the ground truth kendall\ncorrelation on the development set. With this we know the true rank order of the metrics. We then\nperform retrieval using a set of retrieval algorithms, namely BM25, ColBERT, Faiss, and using an\n41\n"}, {"page": 42, "text": "Preprint\nNDCG\nRecall\nMethod\n@1\n@5\n@10\n@20\n@1\n@5\n@10\n@20\nBM25\n0.208 ± 0.274\n0.342 ± 0.171\n0.427 ± 0.143\n0.567 ± 0.16\n0.065 ± 0.095\n0.224 ± 0.146\n0.418 ± 0.173\n0.788 ± 0.319\nColBERT\n0.272 ± 0.293\n0.343 ± 0.203\n0.442 ± 0.178\n0.57 ± 0.174\n0.059 ± 0.092\n0.212 ± 0.155\n0.441 ± 0.273\n0.776 ± 0.361\nFaiss\n0.103 ± 0.059\n0.227 ± 0.144\n0.326 ± 0.163\n0.461 ± 0.199\n0.018 ± 0.058\n0.171 ± 0.204\n0.353 ± 0.256\n0.694 ± 0.427\nLLMRec\n0.31 ± 0.334\n0.396 ± 0.249\n0.478 ± 0.219\n0.602 ± 0.196\n0.088 ± 0.101\n0.294 ± 0.204\n0.465 ± 0.273\n0.641 ± 0.264\nBM25→LLMRec\n0.316 ± 0.323\n0.42 ± 0.226\n0.498 ± 0.197\n0.603 ± 0.186\n0.094 ± 0.101\n0.312 ± 0.179\n0.494 ± 0.257\n0.665 ± 0.245\nColBERT→LLMRec\n0.403 ± 0.387\n0.462 ± 0.28\n0.528 ± 0.238\n0.631 ± 0.212\n0.094 ± 0.101\n0.329 ± 0.225\n0.518 ± 0.266\n0.694 ± 0.21\nFaiss→LLMRec\n0.164 ± 0.186\n0.324 ± 0.234\n0.393 ± 0.215\n0.529 ± 0.216\n0.065 ± 0.095\n0.247 ± 0.226\n0.4 ± 0.27\n0.6 ± 0.304\nTable 6: Average performance (± std) across all tasks/axes using Kendall ground truth (recommen-\ndations from qwen3).\nNDCG\nRecall\nMethod\n@1\n@5\n@10\n@20\n@1\n@5\n@10\n@20\nBM25\n0.208 ± 0.274\n0.342 ± 0.171\n0.427 ± 0.143\n0.567 ± 0.16\n0.065 ± 0.095\n0.224 ± 0.146\n0.418 ± 0.173\n0.788 ± 0.319\nColBERT\n0.272 ± 0.293\n0.343 ± 0.201\n0.42 ± 0.179\n0.568 ± 0.167\n0.059 ± 0.092\n0.247 ± 0.191\n0.429 ± 0.27\n0.8 ± 0.338\nFaiss\n0.098 ± 0.059\n0.21 ± 0.128\n0.314 ± 0.167\n0.461 ± 0.19\n0.012 ± 0.048\n0.159 ± 0.169\n0.371 ± 0.304\n0.729 ± 0.378\nLLMRec\n0.261 ± 0.302\n0.416 ± 0.249\n0.502 ± 0.235\n0.585 ± 0.217\n0.076 ± 0.099\n0.347 ± 0.243\n0.518 ± 0.316\n0.759 ± 0.326\nBM25→LLMRec\n0.206 ± 0.197\n0.394 ± 0.196\n0.47 ± 0.175\n0.576 ± 0.162\n0.076 ± 0.099\n0.347 ± 0.233\n0.512 ± 0.257\n0.794 ± 0.297\nColBERT→LLMRec\n0.328 ± 0.31\n0.475 ± 0.251\n0.55 ± 0.214\n0.628 ± 0.198\n0.1 ± 0.102\n0.388 ± 0.246\n0.565 ± 0.301\n0.759 ± 0.333\nFaiss→LLMRec\n0.157 ± 0.124\n0.325 ± 0.205\n0.406 ± 0.201\n0.526 ± 0.212\n0.065 ± 0.095\n0.276 ± 0.226\n0.424 ± 0.31\n0.635 ± 0.37\nTable 7: Average performance (± std) across all tasks/axes using Kendall ground truth (recommen-\ndations from gpt4o-mini).\nLLM with all documents in context. We additionally try pipelined versions of all of these retrievers\nfeeding into an LLM. We report Recall@[1,5,10,20] and NDCG@[1,5,10,20] in Table 6 for Qwen3-\n32B and Table 7 for GPT-4o-mini.\nOverall we find that ColBERT →LLMRec is consistently the best approach for retrieval, performing\nthe best across 14/16 of our evaluation settings. Thus, we use ColBERT →LLMRec for all metric\nretrieval in the main paper.\nE.2\nGENERATE\nWe test eight different approaches to metric generation. Of these approaches five of them are cheap\nto produce, while three of them are expensive/time-consuming to produce.\nFor CodeGen we prompt an LLM to propose “axes of variation” from high/low exam-\nples and then synthesize small, executable Python snippets that implement a scoring function\n(compute score).\nThe generated code is cleaned, validated on a sample, and—if it er-\nrors—automatically repaired once by an LLM. We support both reference-free and reference-based\nvariants.\nFor G-Eval (Liu et al., 2023) we convert each axis into a concrete evaluation criterion, auto-generate\nnumbered evaluation steps, and prompt an LLM judge to produce a brief rationale followed by a dis-\ncrete score (1–5). We request token-level log probabilities and, at the final score position (found by\nscanning backward), extract the logprobs over tokens {1, 2, 3, 4, 5}, softmax-normalize, and return\nthe probability-weighted expectation ˆs = P5\ns=1 s P(s | prompt, rationale). Both reference-free and\nreference-based variants are supported.\nFor Single Criteria (Saad-Falcon et al., 2024) LLM-as-a-Judge, we show high-scoring and low-\nscoring data points to an LLM and ask for “axes of variation.” Each axis becomes its own metric,\nwith the LLM prompted to output an integer score from 1–5.\nFor Rubric (Gunjal et al., 2025) we add an additional step to Single Criteria where we ask an LLM\nto generate explanations of what 1–5 scores should contain for each rubric item.\nFor Rubric (Prometheus) (Kim et al., 2024) we first synthesize a five-level rubric (descriptions\nfor scores 1–5) from dataset examples, then use a Prometheus evaluator (e.g., M-Prometheus-14B)\nto assign scores conditioned on that rubric. This keeps the rubric explicit while using a strong,\nspecialized judge.\n42\n"}, {"page": 43, "text": "Preprint\nFinetune is our first expensive to produce metric. For this we fine-tune a ModernBERT-Large\nregression head (with LoRA/PEFT) on formatted input–output (and references when available) to\ndirectly predict the target score. We use an 80/20 train/validation split, optimize with AdamW, and\nsave the resulting adapter as a learned metric that runs without an LLM at inference.\nFor Examples we separate the provided human-rated examples into quintiles. Based on the con-\ntext length of the LLM judge we determine how many examples we can reasonably sample from\neach quintile without exceeding the context length. We try 5 randomly sampled sets of uniformly\ndistributed examples as context in an LLM-judge prompt and select the set that minimizes average\ndistance to human labels on the trainset.\nFor Prompt Optimization (MIPROv2) we run DSPy’s MIPROv2 (Opsahl-Ong et al., 2024) op-\ntimizer with auto mode=\"medium\" on the provided data to generate informative examples and\nrewrite the evaluation prompt for an LLM judge.\nCheap to Produce\nExpensive to Produce\nTask (Measure)\nCode Gen\nG-Eval\nSingle Criterion\nRubric (DSPy)\nRubric (Prometheus)\nFinetune\nExamples\nMIPROv2\nIn-Distribution Tasks: some metrics in our bank were designed to directly evaluate these tasks.\nSummEval (coherence)\n0.098 ± 0.019\n0.105 ± 0.023\n0.194 ± 0.010\n0.173 ± 0.017\n0.140 ± 0.016\n0.104 ± 0.016\n0.226 ± 0.019\n0.227 ± 0.044\nSummEval (consistency)\n0.083 ± 0.018\n0.102 ± 0.013\n0.173 ± 0.023\n0.160 ± 0.026\n0.122 ± 0.015\n0.095 ± 0.042\n0.226 ± 0.066\n0.199 ± 0.030\nSummEval (fluency)\n0.057 ± 0.016\n0.076 ± 0.011\n0.121 ± 0.009\n0.110 ± 0.015\n0.096 ± 0.017\n0.061 ± 0.016\n0.146 ± 0.015\n0.136 ± 0.048\nSummEval (relevance)\n0.097 ± 0.025\n0.144 ± 0.026\n0.213 ± 0.017\n0.189 ± 0.018\n0.151 ± 0.014\n0.067 ± 0.043\n0.243 ± 0.022\n0.263 ± 0.022\nPrimock57 (inc plus omi)\n0.105 ± 0.036\n0.086 ± 0.017\n0.247 ± 0.031\n0.188 ± 0.043\n0.196 ± 0.025\n0.090 ± 0.057\n0.253 ± 0.057\n0.258 ± 0.067\nPrimock57 (incorrect)\n0.145 ± 0.073\n0.060 ± 0.014\n0.250 ± 0.059\n0.169 ± 0.070\n0.202 ± 0.026\n0.026 ± 0.029\n0.266 ± 0.039\n0.213 ± 0.164\nPrimock57 (omissions)\n0.123 ± 0.059\n0.061 ± 0.021\n0.119 ± 0.029\n0.116 ± 0.025\n0.129 ± 0.020\n0.125 ± 0.077\n0.169 ± 0.023\n0.122 ± 0.097\nPrimock57 (time sec)\n0.102 ± 0.038\n0.053 ± 0.009\n0.159 ± 0.026\n0.132 ± 0.016\n—\n0.058 ± 0.049\n0.057 ± 0.050\n0.129 ± 0.041\nSimpEval (score)\n0.100 ± 0.037\n0.184 ± 0.028\n0.229 ± 0.019\n0.192 ± 0.012\n0.155 ± 0.017\n0.046 ± 0.037\n0.216 ± 0.036\n0.243 ± 0.130\nSimpDA (fluency)\n0.180 ± 0.013\n0.364 ± 0.022\n0.521 ± 0.014\n0.511 ± 0.018\n0.460 ± 0.025\n0.050 ± 0.051\n0.582 ± 0.017\n0.583 ± 0.058\nSimpDA (meaning)\n0.252 ± 0.066\n0.397 ± 0.030\n0.590 ± 0.016\n0.570 ± 0.020\n0.546 ± 0.026\n0.055 ± 0.038\n0.632 ± 0.025\n0.625 ± 0.024\nSimpDA (simplicity)\n0.173 ± 0.024\n0.305 ± 0.033\n0.523 ± 0.030\n0.481 ± 0.021\n0.442 ± 0.015\n0.041 ± 0.058\n0.584 ± 0.025\n0.628 ± 0.035\nHelpSteer (coherence)\n0.029 ± 0.004\n0.162 ± 0.027\n0.229 ± 0.013\n0.190 ± 0.013\n—\n0.014 ± 0.009\n0.297 ± 0.023\n0.297 ± 0.006\nHelpSteer (complexity)\n0.223 ± 0.083\n0.122 ± 0.029\n0.149 ± 0.042\n0.184 ± 0.035\n—\n0.071 ± 0.070\n0.221 ± 0.050\n0.095 ± 0.018\nHelpSteer (correctness)\n0.068 ± 0.007\n0.270 ± 0.024\n0.356 ± 0.024\n0.342 ± 0.018\n—\n0.044 ± 0.027\n0.392 ± 0.027\n0.424 ± 0.009\nHelpSteer (helpfulness)\n0.066 ± 0.019\n0.241 ± 0.018\n0.333 ± 0.011\n0.327 ± 0.018\n—\n0.049 ± 0.030\n0.407 ± 0.016\n0.402 ± 0.013\nHelpSteer (verbosity)\n0.290 ± 0.028\n0.154 ± 0.043\n0.193 ± 0.051\n0.252 ± 0.053\n—\n0.084 ± 0.031\n0.406 ± 0.015\n0.103 ± 0.028\nHelpSteer2 (coherence)\n0.024 ± 0.005\n0.116 ± 0.019\n0.154 ± 0.005\n0.138 ± 0.020\n—\n0.043 ± 0.032\n0.192 ± 0.016\n0.169 ± 0.028\nHelpSteer2 (complexity)\n0.113 ± 0.040\n0.074 ± 0.024\n0.091 ± 0.013\n0.100 ± 0.014\n—\n0.096 ± 0.000\n0.335 ± 0.074\n0.065 ± 0.045\nHelpSteer2 (correctness)\n0.052 ± 0.012\n0.167 ± 0.012\n0.245 ± 0.007\n0.212 ± 0.016\n—\n0.037 ± 0.035\n0.332 ± 0.017\n0.320 ± 0.019\nHelpSteer2 (helpfulness)\n0.068 ± 0.009\n0.134 ± 0.015\n0.217 ± 0.019\n0.183 ± 0.008\n0.135 ± 0.008\n0.026 ± 0.015\n0.293 ± 0.020\n0.309 ± 0.015\nHelpSteer2 (verbosity)\n0.224 ± 0.018\n0.161 ± 0.031\n0.210 ± 0.052\n0.234 ± 0.048\n—\n0.167 ± 0.068\n0.432 ± 0.015\n0.081 ± 0.315\nOut-of-Distribution Tasks: no metric is specifically designed for these – tests generalization and metric generation.\nEvalGenProduct (grade)\n0.262 ± 0.046\n0.285 ± 0.029\n0.343 ± 0.085\n0.303 ± 0.072\n0.201 ± 0.021\n0.210 ± 0.236\n0.145 ± 0.046\n0.216 ± 0.173\nEvalGenMedical (grade)\n0.262 ± 0.046\n0.285 ± 0.029\n0.343 ± 0.085\n0.303 ± 0.072\n0.201 ± 0.021\n0.210 ± 0.236\n0.145 ± 0.046\n0.216 ± 0.173\nRealHumanEval (accepted)\n0.046 ± 0.007\n0.039 ± 0.013\n0.115 ± 0.009\n0.088 ± 0.013\n0.079 ± 0.011\n0.037 ± 0.025\n0.091 ± 0.019\n0.153 ± 0.028\nCoGymTravelProcess (agentRating)\n0.208 ± 0.027\n0.185 ± 0.061\n0.115 ± 0.050\n0.101 ± 0.044\n0.121 ± 0.056\n0.218 ± 0.246\n0.144 ± 0.098\n0.090 ± 0.046\nCoGymTravelProcess (communicationRating)\n0.172 ± 0.075\n0.285 ± 0.066\n0.168 ± 0.098\n0.167 ± 0.082\n0.165 ± 0.028\n0.238 ± 0.281\n0.220 ± 0.154\n0.180 ± 0.195\nCoGymTravelOutcome (outcomeRating)\n0.337 ± 0.059\n0.318 ± 0.100\n0.429 ± 0.068\n0.448 ± 0.117\n0.413 ± 0.057\n0.298 ± 0.472\n0.558 ± 0.131\n0.518 ± 0.273\nCoGymTabularProcess (agentRating)\n0.254 ± 0.150\n0.487 ± 0.132\n0.538 ± 0.067\n0.598 ± 0.082\n0.403 ± 0.108\n0.475 ± 0.203\n0.560 ± 0.395\n0.637 ± 0.315\nCoGymTabularProcess (communicationRating)\n0.360 ± 0.046\n0.608 ± 0.088\n0.791 ± 0.093\n0.779 ± 0.147\n0.798 ± 0.049\n—\n0.890 ± 0.125\n0.787 ± 0.501\nCoGymTabularOutcome (outcomeRating)\n0.363 ± 0.217\n0.349 ± 0.173\n0.367 ± 0.160\n0.201 ± 0.081\n0.634 ± 0.117\n—\n0.363 ± 0.362\n0.200 ± 0.227\nAverage\n0.159\n0.206\n0.281\n0.263\n0.276\n0.108\n0.323\n0.287\nTable 8: Metric generation performance (Kendall’s Tau) with 95% confidence intervals over 5 in-\ndependent runs. Each generator produces metrics using persistent train sets, then correlation with\nhuman annotations is measured on persistent validation sets. Cheap methods (left) generate 10 met-\nrics per trial, expensive methods (right) generate 1 metric per trial (except finetune which generates\n10). Results show correlation between generated metrics and ground-truth human annotations across\ndiverse tasks using the Qwen3 32B model.\nF\nADDITIONAL EXPERIMENTS\nF.1\nROBUSTNESS FOR ALL METRICS\nHere we include the results of the robustness experiment for all baseline metrics tested. We report\nresults in Figure 7.\nWe find that “Best Metric” tends to be quite stable, while the LLM Based Metrics (DNAEval, LLM-\nJudge, and AutoMetrics) stand out on robustness.\n43\n"}, {"page": 44, "text": "Preprint\nCheap to Produce\nExpensive to Produce\nTask (Measure)\nCode Gen\nG-Eval\nSingle Criterion\nRubric (DSPy)\nRubric (Prometheus)\nFinetune\nExamples\nMIPROv2\nIn-Distribution Tasks: some metrics in our bank were designed to directly evaluate these tasks.\nSimpEval (score)\n0.127 ± 0.015\n0.279 ± 0.024\n0.324 ± 0.026\n0.299 ± 0.024\n0.166 ± 0.022\n0.046 ± 0.037\n0.297 ± 0.041\n0.318 ± 0.039\nSimpDA (fluency)\n0.135 ± 0.020\n0.534 ± 0.016\n0.510 ± 0.014\n0.573 ± 0.012\n0.460 ± 0.013\n0.050 ± 0.051\n0.639 ± 0.028\n0.635 ± 0.018\nSimpDA (meaning)\n0.246 ± 0.028\n0.570 ± 0.012\n0.551 ± 0.007\n0.601 ± 0.022\n0.538 ± 0.014\n0.055 ± 0.038\n0.686 ± 0.039\n0.643 ± 0.022\nSimpDA (simplicity)\n0.092 ± 0.045\n0.500 ± 0.016\n0.540 ± 0.005\n0.535 ± 0.026\n0.463 ± 0.031\n0.041 ± 0.058\n0.621 ± 0.039\n0.622 ± 0.019\nOut-of-Distribution Tasks: no metric is specifically designed for these – tests generalization and metric generation.\nEvalGenProduct (grade)\n0.190 ± 0.032\n0.204 ± 0.063\n0.225 ± 0.069\n0.175 ± 0.087\n0.189 ± 0.080\n0.210 ± 0.236\n0.121 ± 0.040\n0.248 ± 0.069\nEvalGenMedical (grade)\n0.190 ± 0.032\n0.204 ± 0.063\n0.225 ± 0.069\n0.175 ± 0.087\n0.189 ± 0.080\n0.210 ± 0.236\n0.121 ± 0.040\n0.248 ± 0.069\nCoGymTravelProcess (agentRating)\n0.201 ± 0.032\n0.113 ± 0.037\n0.092 ± 0.027\n0.173 ± 0.041\n0.127 ± 0.054\n0.218 ± 0.246\n0.223 ± 0.108\n0.067 ± 0.041\nCoGymTravelProcess (communicationRating)\n0.232 ± 0.074\n0.176 ± 0.038\n0.070 ± 0.035\n0.154 ± 0.083\n0.228 ± 0.017\n0.238 ± 0.281\n0.312 ± 0.086\n0.000 ± 0.000\nCoGymTravelOutcome (outcomeRating)\n0.286 ± 0.049\n0.400 ± 0.103\n0.450 ± 0.114\n0.474 ± 0.051\n0.412 ± 0.092\n0.298 ± 0.472\n0.502 ± 0.024\n0.513 ± 0.009\nCoGymTabularProcess (agentRating)\n0.273 ± 0.199\n0.555 ± 0.098\n0.697 ± 0.139\n0.555 ± 0.064\n0.435 ± 0.120\n0.475 ± 0.203\n0.600 ± 0.000\n0.659 ± 0.220\nCoGymTabularProcess (communicationRating)\n0.404 ± 0.138\n0.853 ± 0.091\n0.859 ± 0.093\n0.881 ± 0.020\n0.763 ± 0.091\n—\n0.000 ± 0.000\n0.890 ± 0.125\nCoGymTabularOutcome (outcomeRating)\n0.470 ± 0.227\n0.547 ± 0.159\n0.565 ± 0.090\n0.616 ± 0.120\n0.804 ± 0.094\n—\n0.430 ± 0.268\n0.623 ± 0.329\nAverage\n0.237\n0.411\n0.426\n0.434\n0.398\n0.184\n0.379\n0.456\nTable 9: Metric generation performance (Kendall’s Tau) with 95% confidence intervals over 5 in-\ndependent runs. Each generator produces metrics using persistent train sets, then correlation with\nhuman annotations is measured on persistent validation sets. Cheap methods (left) generate 10 met-\nrics per trial, expensive methods (right) generate 1 metric per trial (except finetune which generates\n10). Results show correlation between generated metrics and ground-truth human annotations across\ndiverse tasks using the GPT-4o Mini model.\nMetaMetrics\nBest Metric\nDNAEval\nLLMJudge\nAutoMetrics\nMetaMetrics\nBest Metric\nDNAEval\nLLMJudge\nAutoMetrics\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nscore (mean ± 95% CI)\nSimpEval (score)\nNormal Baseline\nSensitivity\nStability\nMetaMetrics\nBest Metric\nDNAEval\nLLMJudge\nAutoMetrics\nMetaMetrics\nBest Metric\nDNAEval\nLLMJudge\nAutoMetrics\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHelpSteer2 (helpfulness)\nMetaMetrics\nBest Metric\nDNAEval\nLLMJudge\nAutoMetrics\nMetaMetrics\nBest Metric\nDNAEval\nLLMJudge\nAutoMetrics\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCoGym (outcome rating)\nFigure 7: Sensitivity and Stability of all metrics for SimpEval, HelpSteer2, and CoGym.\nF.2\nWHAT METRICS DOES AUTOMETRICS ACTUALLY SELECT?\nTo explore the question of what metrics AutoMetrics actually recommends we turn to the 25 trials\nof AutoMetrics run for our main correlation experiment from Table 2. We look exclusively at the\nQwen3-32B runs. We provide a bar plot of metric types in Figure 8.\n0\n20\n40\n60\nCount\nSingle Criteria\nRubric\nExamples Judge\nGRMRewardModel\nINFORMRewardModel\nOptimized Judge\nParaScoreFree\n67\n16\n10\n10\n10\n10\n2\nTop Metrics by Type\nGenerated Metrics\nExisting Metrics\nFigure 8: Breakdown of metrics recommended\nby AutoMetrics. Generated are most common.\nAutoMetrics are dominated by Generated Met-\nrics.\n103 out of the 125 total recommended metrics\nwere automatically generated. Of the Existing met-\nrics that were recommended 20 out of 22 were rec-\nommendations to use a reward model. This suggests\nthat the scope of metrics to retrieve from can be dra-\nmatically reduced to primarily recommending from\nthe generated metrics as well as a few key reward\nmodels and other model based metrics like “ParaS-\ncoreFree”. This insight will in practice greatly sim-\nplify the search space for metrics and lead to a more\nstreamlined MetricBank.\nF.3\nVALIDATING SENSITIVITY AND STABILITY\nIn order to sanity check our sensitivity and stability\nscores we asked a collegue not involved in our project to annotate 150 datapoints from SimpEval\nMaddela et al. (2023) using the original annotation rubric described in the paper. SimpEval consists\nof original and simple sentence pairs. We asked them to annotate 30 pairs from the original dataset,\n30 pairs where the simplified sentence was perturbed in a way that does not change the quality, and\n90 sentences perturbed to purposefuly degrade the quality. All perturbations were following our\n44\n"}, {"page": 45, "text": "Preprint\nmethodology described in 3.2. Our human annotations yielded a sensitivity of 0.8275 and stability\nof 0.8000 suggesting the perturbations produced the intended effect.\nG\nAUTOMETRICS EXAMPLES\nSimpEval — score\nOverall Kendall τ: 0.3234\nTop 5 Metrics & Coefficients\nMetric\nCoefficient\nAudience Appropriateness Qwen3-32B\n1.7066\nConciseness Qwen3-32B\n1.6676\nReadability Score Qwen3-32B\n1.6622\nClarity and Readability Rubric\n1.6345\nParaScoreFree\n−1.6125\nDescription: Audience Appropriateness Qwen3-32B\nTailors language and phrasing to suit a general audience with minimal prior knowledge\nof the topic.\nDescription: Conciseness Qwen3-32B\nEliminates redundant phrases, wordiness, or tangential details while maintaining the orig-\ninal intent.\nDescription: Readability Score Qwen3-32B\nMeasures the text’s ease of reading using standardized metrics (e.g., Flesch-Kincaid\nGrade Level).\n45\n"}, {"page": 46, "text": "Preprint\nDescription: Clarity and Readability Rubric\n| Score | Description |\n|-------|-------------|\n| 1 | - The text is difficult to understand due to overly complex\nsentence structures, passive voice, or ambiguous phrasing.\n- Redundant or redundant information is included, hindering\nclarity.\n- Sentences are excessively long or fragmented, making it hard to\nfollow the main idea.\n- Jargon or technical terms are retained without simplification.\n- The output fails to restructure the original sentence for\nbroader accessibility. |\n| 2 | - The text is somewhat clear but still contains occasional\ncomplex structures or passive voice.\n- Some sentences are overly long or include minor redundancies.\n- Ambiguity or unclear phrasing is present in parts of the output\n.\n- Simplification is attempted but incomplete, leaving some\noriginal complexity intact.\n- The main idea is generally understandable but requires effort\nto parse. |\n| 3 | - The text is mostly clear, with mostly active voice and\nstraightforward phrasing.\n- Sentences are concise and well-structured, though a few may\nretain slight complexity.\n- Minor ambiguities or redundancies are present but do not\nsignificantly hinder understanding.\n- Simplification is effective for the core message, though some\ndetails may remain dense.\n- The output is accessible to a general audience with minimal\neffort. |\n| 4 | - The text is clear and uses active voice consistently,\nwith minimal passive constructions.\n- Sentences are concise, well-structured, and free of unnecessary\ncomplexity.\n- Ambiguity is largely avoided, and phrasing is precise.\n- Simplification is thorough, with original complexity reduced to\nenhance accessibility.\n- The output is easy to understand for a broad audience, with\nonly minor improvements possible. |\n| 5 | - The text is exceptionally clear, using active voice and\nsimple, direct sentence structures.\n- All phrasing is unambiguous, and sentences are optimized for\nreadability.\n- Redundancy and complexity are entirely eliminated, with the\ncore message distilled to its essentials.\n- Simplification is flawless, making the content immediately\naccessible to all audiences.\n- The output exemplifies best practices in clarity and\nreadability, with no room for improvement. |\n46\n"}, {"page": 47, "text": "Preprint\nDescription: ParaScoreFree\nParaScoreFree is a reference-free evaluation metric designed for paraphrase generation.\nIt evaluates candidate paraphrases based on semantic similarity to the input source while\nencouraging lexical diversity. ParaScoreFree outputs a scalar quality score that combines\nBERT-based semantic similarity and normalized edit distance, offering a balance between\nmeaning preservation and surface-level rewriting. It enables paraphrase evaluation with-\nout the need for gold reference texts, making it suitable for low-resource or open-domain\nsettings.\nHelpSteer2 — helpfulness\nOverall Kendall τ: 0.3481\nTop 5 Metrics & Coefficients\nMetric\nCoefficient\nINFORMRewardModel\n0.2046\nHelpSteer2 helpfulness Qwen3-32B optimized seed45\n0.1853\nGRMRewardModel\n0.1697\nhelpfulness Qwen3-32B examples\n0.1661\nAccuracy and Correctness Qwen3-32B\n0.1625\nDescription: INFORMRewardModel\nThe INFORM Reward Model 70B (INF-ORM-Llama3.1-70B) is a large-scale outcome\nreward model designed to evaluate the quality of generated conversational responses. It\npredicts scalar reward scores for response texts, supporting preference-based fine-grained\nevaluations without requiring a reference response. The model is finetuned from the\nLlama-3.1-70B-Instruct backbone using preference-labeled datasets, employing scaled\nBradley-Terry loss to incorporate preference magnitudes.\nDescription: HelpSteer2 helpfulness Qwen3-32B optimized seed45\nGiven the task description, evaluation axis, input/output texts,\nand suggested score range, analyze the output text’s alignment\nwith the task and axis by:\n1. **Assessing factual accuracy**: Verify if claims in the output\nare correct and supported by the input/text domain knowledge.\n2. **Evaluating relevance**: Determine if the output addresses\nthe user’s intent directly, avoiding verbosity or tangential\ncontent.\n3. **Analyzing structure and clarity**: Check if explanations are\nconcise, logically organized, and accessible to the target\naudience (e.g., non-experts).\n4. **Identifying gaps or errors**: Highlight missing key details,\nmisinterpretations, or inaccuracies that reduce helpfulness.\n5. **Scoring**: Assign a numerical score within the suggested\nrange, balancing the above factors.\nUse the conversation history and task description as guidance for\ncontext and expectations. Prioritize precision in reasoning\nand alignment with the evaluation axis.\n*Showing 0 of 8 total examples.*\n47\n"}, {"page": 48, "text": "Preprint\nDescription: GRMRewardModel\nThe GRMRewardModel is a general-purpose reward model designed to evaluate the\nquality and safety of LLM-generated outputs. It achieves high generalization perfor-\nmance by applying a novel regularization method on hidden states during supervised\nfine-tuning. GRMRewardModel is fine-tuned on the decontaminated Skywork/Skywork-\nReward-Preference-80K-v0.2 dataset and achieves state-of-the-art results among models\nof comparable size (3B), even outperforming some 8B reward models and proprietary\nLLM judges on RewardBench.\nDescription: helpfulness Qwen3-32B examples\n| Input Text | Score |\n|------------|-------|\n| <Input (prompt): <Can you teach me semi-definite programming in\nsimple language?> Output (response): <Can you teach me how to\nuse a computer in simple language?>> | 0 |\n| <Input (prompt): <Delve into the nuanced benefits of engaging\nin group projects instead of the solo endeavor of individual\nprojects. Craft your insights in a well-organized format,\nemploying distinct headings for each category. Populate each\nsection with a thoughtful list, elucidating each approach’s\nmerits and drawbacks. This approach aims to enhance clarity\nand the discussion’s overall academ... | 0 |\n| <Input (prompt): <The misery of life never appears in a clearer\nlight than when a thinking person has quite plainly seen with\nhorror its hazards and uncertainties and the total darkness\nin which he lives; how he cannot find anything solid, secure,\nand beyond dispute on to which he can hold; when, as I say,\nafter such thoughts he does not at once destroy an existence\nthat is not one, but breathi... | 1 |\n*Showing 3 of 10 total examples.*\nDescription: Accuracy and Correctness Qwen3-32B\nThe factual correctness and reliability of the information provided.\nEvalGenProduct — grade\nOverall Kendall τ: 0.4178\nTop 5 Metrics & Coefficients\nMetric\nCoefficient\nFormatting Compliance Qwen3-32B\n0.1144\ngrade Qwen3-32B examples\n0.1022\nCall to Action CTA Strength Qwen3-32B\n0.0752\nCustomer Review Integration Rubric\n0.0747\nAvoidance of Weaknesses Qwen3-32B\n0.0653\n48\n"}, {"page": 49, "text": "Preprint\nDescription: Formatting Compliance Qwen3-32B\nGood examples strictly follow Markdown structure (headers, bullet points). Bad examples\ninclude disallowed elements (links, markdown errors).\nDescription: grade Qwen3-32B examples\n| Input Text | Score |\n|------------|-------|\n| <Input (Prompt): <You are an expert copywriter. You need to\nwrite an e-commerce product description based on the product\ndetails and customer reviews. Your description should be SEO-\noptimized. It should use an active voice and include the\nproduct’s features, benefits, unique selling points without\noverpromising, and a call to action for the buyer. Benefits\ndescribe how product features will wor... | 0 |\n| <Input (Prompt): <You are an expert copywriter. You need to\nwrite an e-commerce product description based on the product\ndetails and customer reviews. Your description should be SEO-\noptimized. It should use an active voice and include the\nproduct’s features, benefits, unique selling points without\noverpromising, and a call to action for the buyer. Benefits\ndescribe how product features will wor... | 0 |\n| <Input (Prompt): <You are an expert copywriter. You need to\nwrite an e-commerce product description based on the product\ndetails and customer reviews. Your description should be SEO-\noptimized. It should use an active voice and include the\nproduct’s features, benefits, unique selling points without\noverpromising, and a call to action for the buyer. Benefits\ndescribe how product features will wor... | 1 |\n*Showing 3 of 4 total examples.*\nDescription: Call to Action CTA Strength Qwen3-32B\nGood examples include urgent, benefit-driven CTAs (e.g., ’Order now for seasonal sav-\nings’), while bad examples have vague or missing CTAs.\n49\n"}, {"page": 50, "text": "Preprint\nDescription: Customer Review Integration Rubric\n| Score | Description |\n|-------|-------------|\n| 1 | - **No customer reviews included** or all quotes are\nfabricated.\n- Reviews are irrelevant to the product or its benefits.\n- Over-cites testimonials (e.g., 5+ quotes) or includes negative\nfeedback.\n- Quotes are generic (e.g., \"Great product!\") without specific\ncontext. |\n| 2 | - **Minimal or inconsistent use of customer reviews** (e.g\n., 1-2 quotes).\n- Quotes are vague or lack specificity (e.g., \"I love this\nproduct!\").\n- Reviews may include irrelevant details or fail to align with\nthe product’s features/benefits.\n- No clear connection between testimonials and the product’s\nunique selling points. |\n| 3 | - **Moderate use of customer reviews** (e.g., 2-3 quotes).\n- Some quotes are specific and relevant (e.g., \"This product\nworks well for dry skin\").\n- May include 1-2 generic or slightly over-cited testimonials.\n- Reviews are integrated but do not strongly enhance the\ndescription’s persuasiveness. |\n| 4 | - **Effective use of 1-2 authentic, contextually relevant\nquotes**.\n- Testimonials highlight specific benefits (e.g., \"The\nlightweight formula makes it perfect for travel\").\n- Quotes are concise, avoid over-citing, and align with the\nproduct’s features.\n- Reviews are integrated naturally into the description without\noverwhelming the reader. |\n| 5 | - **Excellent integration of 1-2 highly specific, authentic\ntestimonials**.\n- Quotes directly tie to the product’s unique selling points (e.g\n., \"The smudge-proof formula lasts all day\").\n- Reviews are concise, impactful, and enhance the description’s\ncredibility.\n- No fabricated, irrelevant, or over-cited quotes; testimonials\nfeel organic and persuasive. |\n*Showing 3 of 4 total examples.*\nDescription: Avoidance of Weaknesses Qwen3-32B\nGood examples omit product drawbacks. Bad examples inadvertently mention flaws (e.g.,\n’may clog pores’) or use hedging language.\nRealHumanEval — accepted\nOverall Kendall τ: 0.1487\nTop 5 Metrics & Coefficients\n50\n"}, {"page": 51, "text": "Preprint\nMetric\nCoefficient\nGRMRewardModel\n0.0325\nINFORMRewardModel\n0.0293\nCode Readability Qwen3-32B\n0.0283\nRealHumanEval accepted Qwen3-32B optimized seed44\n0.0234\nModularity and Reusability Qwen3-32B\n0.0218\nDescription: GRMRewardModel\nThe GRMRewardModel is a general-purpose reward model designed to evaluate the\nquality and safety of LLM-generated outputs. It achieves high generalization perfor-\nmance by applying a novel regularization method on hidden states during supervised\nfine-tuning. GRMRewardModel is fine-tuned on the decontaminated Skywork/Skywork-\nReward-Preference-80K-v0.2 dataset and achieves state-of-the-art results among models\nof comparable size (3B), even outperforming some 8B reward models and proprietary\nLLM judges on RewardBench.\nDescription: INFORMRewardModel\nThe INFORM Reward Model 70B (INF-ORM-Llama3.1-70B) is a large-scale outcome\nreward model designed to evaluate the quality of generated conversational responses. It\npredicts scalar reward scores for response texts, supporting preference-based fine-grained\nevaluations without requiring a reference response. The model is finetuned from the\nLlama-3.1-70B-Instruct backbone using preference-labeled datasets, employing scaled\nBradley-Terry loss to incorporate preference magnitudes.\nDescription: Code Readability Qwen3-32B\nClarity of variable names, structure, and comments for maintainability.\nDescription: RealHumanEval accepted Qwen3-32B optimized seed44\nYou are an expert Python code reviewer in a high-stakes software\nengineering environment where code correctness directly\nimpacts mission-critical systems (e.g., financial transactions\n, medical devices, or autonomous vehicles). Your task is to\nevaluate the AI-generated code output for **absolute\ncorrectness** and **completeness** along the specified\nevaluation axis. A single error could lead to catastrophic\nfailures. Analyze the code with extreme rigor, checking for:\n1. **Logical correctness** (does it solve the task as described?)\n2. **Syntax validity** (Python 3 compliance, no placeholders like\n’xrange()’ or ’raw\\_input()’)\n3. **Edge case handling** (negative numbers, empty inputs, etc.)\n4. **Mathematical/statistical rigor** (valid algorithms, no\narbitrary values like ’b = 8’)\n5. **Functionality** (working return statements, no stubs or\nincomplete logic).\nAssign a score between 0.0 and 1.0, where 0.0 means the code is\nnon-functional or completely ignores the task, and 1.0\nrepresents a flawless implementation. Use the input/output\ntext and conversation history for context.\n*Showing 0 of 8 total examples.*\n51\n"}, {"page": 52, "text": "Preprint\nDescription: Modularity and Reusability Qwen3-32B\nCode organization into reusable functions/methods with clear separation of concerns.\nCoGymTravelOutcome — outcomeRating\nOverall Kendall τ: 0.4301\nTop 5 Metrics & Coefficients\nMetric\nCoefficient\nCultural and Local Integration Rubric\n0.1963\nCultural and Local Experiences Qwen3-32B\n0.1927\nAccommodation Options Qwen3-32B\n0.1824\noutcomeRating Qwen3-32B examples\n0.1674\nFeasibility and Realism Qwen3-32B\n0.1620\n52\n"}, {"page": 53, "text": "Preprint\nDescription: Cultural and Local Integration Rubric\n| Score | Description |\n|-------|-------------|\n| 1 | - **Score 1 (Poor):**\n- No mention of unique local experiences or cultural highlights.\n- No authentic food/dining recommendations.\n- Generic or irrelevant suggestions (e.g., luxury dining for a\nbudget trip).\n- Fails to address the user’s query or intent. |\n| 2 | - **Score 2 (Weak):**\n- Minimal mention of local experiences (e.g., 1-2 generic\nactivities like \"visiting a market\").\n- Vague food/dining suggestions (e.g., \"try local cuisine\"\nwithout specifics).\n- Lacks integration of cultural or seasonal traditions (e.g., no\nmention of KFC for Christmas).\n- Missing links or references to local resources. |\n| 3 | - **Score 3 (Fair):**\n- Includes 1-2 specific local experiences (e.g., visiting\nJigokudani Monkey Park).\n- Mentions 1-2 authentic food/dining options (e.g., \"try miso\nramen\").\n- Some cultural or seasonal references (e.g., \"KFC is popular for\nChristmas\").\n- Limited use of links or resources to support recommendations. |\n| 4 | - **Score 4 (Good):**\n- Includes 3-4 unique local experiences (e.g., snow monkeys,\nwinter illuminations, regional festivals).\n- Highlights 2-3 specific, culturally significant food/dining\noptions (e.g., \"try KFC for Christmas,\" \"visit a local ramen\nshop\").\n- Integrates cultural/seasonal traditions (e.g., \"Christmas\nmarkets in Hokkaido\").\n- Provides 1-2 links to local events, businesses, or resources. |\n| 5 | - **Score 5 (Excellent):**\n- Includes 5+ unique, deeply integrated local experiences (e.g.,\nsnow monkeys, winter illuminations, regional festivals, and\nlesser-known gems).\n- Highlights 3+ specific, culturally significant food/dining\noptions with detailed descriptions (e.g., \"try miso ramen at [\nspecific shop]\").\n- Seamlessly integrates cultural/seasonal traditions (e.g., \"KFC\nfor Christmas, winter illuminations in Sapporo\").\n- Provides 3+ links to local events, businesses, or resources (e.\ng., [Sapporo White Illumination](url)).\n- Tailors recommendations to the user’s preferences and avoids\nirrelevant suggestions. |\nDescription: Cultural and Local Experiences Qwen3-32B\nIncorporates authentic local traditions, cuisine, and unique regional activities.\nDescription: Accommodation Options Qwen3-32B\nRecommends diverse lodging choices (e.g., luxury hotels, budget hostels, or ryokans).\n53\n"}, {"page": 54, "text": "Preprint\nDescription: outcomeRating Qwen3-32B examples\n| Input Text | Score |\n|------------|-------|\n| <Input (query): <I want to go to Hong Kong and spend less money\n> Output (outcome): <**Luxury Entertainment**\n**Fine Dining\n:**\n*\nMichelin-starred restaurants: Hong Kong boasts a wide\narray of Michelin-starred restaurants offering diverse\ncuisines. Examples include Caprice (French), Lung King Hee (\nCantonese), and Amber (Contemporary). *\nPrivate dining\nexperiences: Many high-end restaurants offer private dining\nrooms for a more... | 1.0 |\n| <Input (query): <Kauai> Output (outcome): <\\# Kauai Itinerary:\n5 Days of Scenery \\& Bro Time (Grad Student Budget)\n**Trip\nOverview:** This itinerary focuses on experiencing Kauai’s\nnatural beauty while keeping costs down. It’s designed for a\ngroup of bros looking for a relaxing and scenic getaway in\nlate March.\n**Weather Considerations:** March is the tail end\nof the rainy season in Kauai. Expe... | 2.0 |\n| <Input (query): <I will go to yangzhou city in China> Output (\noutcome): <Thank you for sharing your interests in history and\nfood! Could you please let me know how many days you plan to\nstay in Yangzhou? This will help me create a well-structured\nitinerary for your trip.>> | 2.0 |\n*Showing 3 of 9 total examples.*\nDescription: Feasibility and Realism Qwen3-32B\nEnsures the itinerary is realistic in terms of time, distance, and activity density.\nTauBench — reward\nOverall Kendall τ: 0.2469\nTop 3 Metrics & Coefficients\nMetric\nCoefficient\nMembership Benefit Application Rubric\n0.0803\nEscalation Appropriateness Rubric\n0.0599\nPolicy Compliance Qwen3-32B\n0.0567\n54\n"}, {"page": 55, "text": "Preprint\nDescription: Membership Benefit Application Rubric\n| Score | Description |\n|-------|-------------|\n| 1 | - **Score 1 (Fails to apply rules)**:\n- Incorrectly assigns free baggage allowances regardless of\nmembership tier or cabin class.\n- Applies insurance benefits to users who do not meet eligibility\ncriteria (e.g., no insurance, basic economy).\n- Offers compensation certificates to users who are not eligible\n(e.g., regular members without insurance).\n- Fails to enforce policy restrictions (e.g., allowing basic\neconomy cancellations outside the 24-hour window without\ninsurance). |\n| 2 | - **Score 2 (Major errors in application)**:\n- Applies baggage allowances inconsistently (e.g., correct for\nsome tiers but not others).\n- Misapplies insurance eligibility (e.g., allows refunds for\ncancellations without valid reasons).\n- Offers compensation certificates in most cases but misses key\neligibility criteria (e.g., ignores membership tier).\n- Occasionally transfers to human agents unnecessarily due to\nincorrect benefit application. |\n| 3 | - **Score 3 (Partial adherence with minor errors)**:\n- Correctly applies baggage allowances for most membership tiers\nbut has occasional errors (e.g., miscalculates free bags for\ngold members).\n- Applies insurance eligibility in most cases but fails in edge\ncases (e.g., business class cancellations without checking\ninsurance status).\n- Offers compensation certificates in most eligible scenarios but\noccasionally misses conditions (e.g., delayed flights without\nverifying membership).\n- Rarely transfers to human agents due to minor benefit\napplication issues. |\n| 4 | - **Score 4 (High adherence with rare errors)**:\n- Correctly applies baggage allowances for all membership tiers\nand cabin classes in most cases.\n- Applies insurance eligibility accurately in nearly all\nscenarios.\n- Offers compensation certificates in all eligible cases but has\none minor oversight (e.g., miscalculating certificate amounts\nfor multi-passenger reservations).\n- Transfers to human agents only when necessary and for valid\nreasons. |\n| 5 | - **Score 5 (Perfect adherence)**:\n- Always assigns free baggage allowances correctly based on\nmembership tier and cabin class.\n- Applies insurance eligibility and compensation rules flawlessly\n, adhering strictly to policy.\n- Never offers ineligible benefits (e.g., no certificates to\nregular members without insurance).\n- Transfers to human agents only when the request falls outside\nthe scope of membership benefits. |\n55\n"}, {"page": 56, "text": "Preprint\nDescription: Escalation Appropriateness Rubric\n| Score | Description |\n|-------|-------------|\n| 1 | - **Fails to transfer** in all cases where policy limits\nare reached or exceptions are needed.\n- **Incorrectly handles** requests that require human\nintervention (e.g., proceeds with booking/canceling flights\noutside policy).\n- **No adherence** to the rule of transferring for policy\nviolations or exceptions. |\n| 2 | - **Transfers inconsistently** (e.g., transfers in some\npolicy-violating cases but not others).\n- **Fails to transfer** for critical exceptions (e.g., basic\neconomy cancellations without insurance, destination changes).\n- **Attempts to resolve** issues beyond its scope (e.g.,\nmodifying flight destinations, waiving fees without human\ninput). |\n| 3 | - **Transfers** in most policy-violating cases (e.g.,\ndenies basic economy cancellations and transfers to human\nagents).\n- **Partially handles exceptions** (e.g., transfers for\ncompensation requests but not for all policy violations).\n- **Some errors** in determining when to escalate (e.g.,\ntransfers unnecessarily for minor issues). |\n| 4 | - **Consistently transfers** when policy limits are reached\n(e.g., denies basic economy cancellations, blocks destination\nchanges).\n- **Transfers for exceptions** (e.g., user insists on refunds for\nnon-refundable tickets, requests compensation for delays).\n- **Minimal errors** in escalation decisions, with clear\nadherence to policy boundaries. |\n| 5 | - **Perfectly transfers** in all required cases (e.g.,\npolicy violations, exceptions, ambiguous requests).\n- **Never attempts to handle** requests outside its scope (e.g.,\ndenies basic economy cancellations, blocks invalid\nmodifications).\n- **Proactively transfers** when user intent is unclear or\nrequires human judgment (e.g., personal emergencies,\ncompensation negotiations). |\nDescription: Policy Compliance Qwen3-32B\nAdherence to airline rules (e.g., no basic economy cancellations without insurance or\n24-hour window).\n56\n"}]}