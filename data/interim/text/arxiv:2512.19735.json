{"doc_id": "arxiv:2512.19735", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.19735.pdf", "meta": {"doc_id": "arxiv:2512.19735", "source": "arxiv", "arxiv_id": "2512.19735", "title": "Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction", "authors": ["Gangxiong Zhang", "Yongchao Long", "Yong Zhang", "Yuxi Zhou", "Shenda Hong"], "published": "2025-12-17T12:29:53Z", "updated": "2025-12-24T08:34:41Z", "summary": "Accurate mortality risk prediction for intensive care unit (ICU) patients is essential for clinical decision-making. Although large language models (LLMs) show promise in predicting outcomes from structured medical data, their predictions may exhibit demographic biases related to sex, age, and race, limiting their trustworthy use in clinical practice. Existing debiasing methods often reduce predictive performance, making it difficult to jointly optimize fairness and accuracy. In this study, we systematically examine bias in LLM-based ICU mortality prediction and propose a training-free, clinically adaptive prompting framework to simultaneously improve fairness and performance. We first develop a multi-dimensional bias assessment scheme for comprehensive model diagnosis. Building on this analysis, we introduce CAse Prompting (CAP), a novel prompting framework that integrates conventional debiasing prompts with case-based reasoning. CAP guides the model to learn from similar historical misprediction cases and their correct outcomes, enabling correction of biased reasoning patterns. Experiments on the MIMIC-IV dataset show that CAP substantially improves both predictive accuracy and fairness. CAP increases AUROC from 0.806 to 0.873 and AUPRC from 0.497 to 0.694, while reducing sex- and race-related disparities by over 90%. Feature reliance analysis further indicates highly consistent attention patterns across demographic groups, with similarity scores exceeding 0.98. These results demonstrate that LLMs exhibit measurable bias in ICU mortality prediction, and that a carefully designed prompting framework can effectively co-optimize fairness and performance without retraining, offering a transferable paradigm for equitable clinical decision support.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.19735v2", "url_pdf": "https://arxiv.org/pdf/2512.19735.pdf", "meta_path": "data/raw/arxiv/meta/2512.19735.json", "sha256": "d715391e1005bcbe8329687dc5edb0d196edda2d2c41648206272c62d141240a", "status": "ok", "fetched_at": "2026-02-18T02:24:13.692757+00:00"}, "pages": [{"page": 1, "text": "Case Prompting to Mitigate Large Language Model Bias for ICU Mortality\nPrediction\nGangxiong Zhang1,4, Yongchao Long\n1,4, Yong Zhang3,*, Yuxi Zhou1,2,*, and Shenda Hong\n4,5,*\n1Department of Computer Science, Tianjin University of Technology, Tianjin, China\n2DCST, BNRist, RIIT, Institute of Internet Industry, Tsinghua University, Beijing, China\n3BNRist, Intelligent Database Research Institute, Tsinghua University, Beijing, China\n4National Institute of Health Data Science, Peking University, Beijing, China\n5Institute for Artificial Intelligence, Peking University, Beijing, China\n*Correspondence: joy_yuxi@pku.edu.cn, hongshenda@pku.edu.cn,\nzhangyong05@tsinghua.edu.cn\nABSTRACT\nAccurately predicting mortality risk in intensive care unit (ICU) patients is critical for clinical\ndecision-making1. While large language models (LLMs) show significant potential for prediction\ntasks using structured medical data, their decision-making processes may harbor biases related to\ndemographic attributes such as sex, age, and race2. This issue severely hampers the trustworthy\ndeployment of LLMs in clinical settings where fairness is paramount. Existing bias mitigation\napproaches often compromise model performance, making it challenging to achieve a synergistic\noptimization of fairness and predictive accuracy. This study systematically investigates fairness\nissues in LLMs applied to ICU mortality prediction and proposes a clinically adaptive prompting\nframework.\nThe framework is designed to enhance both model fairness and performance\nsimultaneously via a low-cost, training-free approach. We first construct a multi-dimensional bias\nassessment system for comprehensive model diagnosis. Subsequently, we design a framework\nnamed CAse Prompting (CAP), which not only incorporates existing prompt-based debiasing\ntechniques but also pioneers a new direction by guiding the model to learn from similar historical\nmisprediction cases alongside their correct outcomes, thereby correcting biased reasoning\npatterns. Experiments on the MIMIC-IV dataset demonstrate that our CAP framework achieves\nsignificant improvements in both performance and fairness metrics. CAP increases the area\nunder the receiver operating characteristic curve (AUROC) from 0.806 to 0.873 (+0.067) and\nthe area under the precision-recall curve (AUPRC) from 0.497 to 0.694 (+0.197), while reducing\nsex- and race-related prediction disparities by over 90%. Feature reliance analysis confirms\nthat the model maintains highly consistent attention patterns across demographic groups, with\nkey similarity metrics exceeding 0.98. This study confirms the presence of measurable bias in\nLLMs for ICU prediction and demonstrates that a carefully designed prompting framework can\nco-optimize fairness and performance without retraining, while providing a novel, transferable\nparadigm for developing reliable and equitable clinical decision-support tools.\nKEYWORDS\nLarge Language Models, ICU, Mortality Prediction, Fairness, Bias Mitigation, Prompt Engineering\n1\narXiv:2512.19735v2  [cs.LG]  24 Dec 2025\n"}, {"page": 2, "text": "INTRODUCTION\nIn high-risk clinical environments, concerns regarding the fairness of artificial intelligence (AI)\nsystems have become a central ethical focus in both academia and industry3. As AI technologies\nbecome increasingly integrated into healthcare, their role in supporting decision-making in inten-\nsive care units (ICUs) has grown rapidly4–6. Large language models (LLMs), with their advanced\ncontextual understanding and reasoning capabilities, are emerging as powerful tools for clinical\nprediction tasks, including ICU mortality prediction. However, clinical decision-making requires not\nonly high predictive accuracy but also stringent standards for fairness and interpretability. System-\natic performance disparities caused by non-clinical attributes—such as sex, race, or age—may\nlead to inequitable allocation of medical resources, exacerbate existing health disparities7,8, and\nultimately undermine clinicians’ and patients’ trust in AI-assisted care.\nWhen Large Language Models (LLMs) are introduced into this high-risk clinical scenario, they\nface two key challenges. First, detecting demographic bias across patient subgroups. LLMs\nmay inherit structural biases embedded in their training data, resulting in uneven performance for\nfemale, older, or minority patients9. Conventional fairness metrics primarily measure output-level\ndisparities but fail to capture latent feature bias—that is, the tendency of a model to rely on\ndifferent clinical features for different demographic groups, thereby weakening model reliability.\nSecond, how to break through the trade-off between performance and fairness, and mitigate\nmodel bias while ensuring performance. Existing mitigation strategies often rely on reweighting\nor loss-based adjustments to improve fairness, yet these approaches frequently degrade overall\naccuracy—an outcome that is particularly problematic in high-stakes medical settings. Therefore,\nthis study aims to systematically investigate the following three core research questions:\n• Is bias present? Do LLMs indeed exhibit performance disparities across specific demo-\ngraphic groups in ICU mortality prediction?\n• Where is bias? Is bias confined to model outputs, or is it ingrained within the model’s\ninternal feature reliance and reasoning logic (i.e., latent bias)?\n• Can bias be mitigated? Can these biases be mitigated in a low-cost, highly adaptable\nmanner without compromising core predictive performance?\nTo address these questions, we put forward the core idea that by constructing a multi-\ndimensional bias assessment framework and designing a clinically adaptive prompting strategy, it\nis possible to co-optimize the fairness and predictive performance of LLMs without retraining. The\nmain contributions of this work are as follows:\n• We develop a multidimensional bias evaluation system that integrates model discrimina-\ntive power, group fairness, and feature dependence analysis, enabling systematic detection\nand explanation of bias from \"output bias\" to \"reasoning bias\".\n• We propose a clinically adaptive framework based on CAse prompting (CAP).By retriev-\ning and incorporating similar historical misjudgment cases, this framework provides large\nlanguage models (LLMs) with concrete decision-making contexts, guiding them to correct\nbiased reasoning patterns and thereby improving both accuracy and fairness simultaneously.\n• We conduct a full bias detection–mitigation–interpretation evaluation loop on the real-\nworld MIMIC-IV clinical dataset, demonstrating the effectiveness of the proposed framework\nand offering a low-cost, generalizable pathway for the fair and trustworthy deployment of\nLLMs in clinical practice.\n2\n"}, {"page": 3, "text": "RESULTS\nStudy Population\nThis study utilized the MIMIC-IV database to construct a retrospective cohort. Prior to cohort\nestablishment, data quality assessment and preprocessing were conducted: patients with exces-\nsively high missing data ratios were excluded; for partial missing values in the retained variables,\nthe Multiple Imputation by Chained Equations (MICE) method was employed for data imputation to\nensure data integrity and analytical reliability. The inclusion criteria were defined as follows: adult\npatients admitted to the ICU for the first time with a hospital stay exceeding 24 hours. The final\ncohort comprised 45,173 patients, which were randomly divided into a training set (for constructing\nthe case library and training comparative machine learning models) and an independent test set\n(for all final evaluations) at a 7:3 ratio. The baseline characteristics of the patients are presented\nin Table 1, with balanced distributions of demographic and clinical characteristics across all\nsubgroups.\nPredictive Performance Comparison\nThe comparative analysis presented in Table 2 reveals several key findings regarding the predictive\nperformance across different methodologies. The proposed CAP (Clinical-Aware Prompting)\nmethod demonstrates superior performance across most evaluation metrics compared to other\nQwen3-32B approaches. Specifically, CAP achieves the highest AUROC (0.873), AUPRC (0.694),\nF1 Score (0.525), and Sensitivity (0.850) among all LLM-based methods. This represents a\nsubstantial improvement over the base Qwen3-32B model, with absolute increases of 6.7% in\nAUROC, 19.7% in AUPRC, and 8.7% in F1 Score.\nWhile XGBoost maintains competitive performance with the best Brier Score (0.084) and\nSpecificity (0.849), the CAP method closes this performance gap significantly. Notably, CAP\noutperforms XGBoost in AUROC, AUPRC, and Sensitivity, suggesting enhanced capability in\nidentifying true positive cases while maintaining reasonable performance in other metrics. The\nexceptional Sensitivity (0.850) achieved by CAP indicates its strength in minimizing false negatives,\na crucial consideration in clinical mortality prediction where missing true positive cases could\nhave serious implications.\nThe demonstrated performance improvements, particularly in sensitivity and overall discrimi-\nnative ability, suggest that the CAP approach effectively leverages clinical domain knowledge to\nenhance LLM performance in medical prediction tasks. This makes it a promising alternative to\ntraditional machine learning methods like XGBoost, especially in scenarios where interpretability\nand comprehensive reasoning are valued alongside predictive accuracy.\nModel Bias Evaluation\nTo systematically assess the effectiveness of different methods in mitigating model bias, this study\nconducted a comprehensive analysis of large language model (LLM) approaches from three\ndimensions: discriminative power disparity, fairness performance, and feature dependence. By\ncomparing the baseline model, fairness optimization methods, System 2 approaches, and our\nproposed CAP method, the study reveals the differences in the effectiveness of various strategies\nin bias mitigation.\n3\n"}, {"page": 4, "text": "Table 1: Baseline characteristics of the study cohort. SD, standard deviation. P-values were cal-\nculated using the t-test (for continuous variables) or the chi-square test (for categorical variables)\ncomparing the training and test sets.\nFeature\nOverall (n=45,153)\nTrain (n=31,607)\nTest (n=13,546)\np-value\nDemographics\nAge (years), mean ± SD\n66.1 ± 16.2\n66.1 ± 16.2\n66.0 ± 16.2\n0.12\nFemale, n (%)\n20,138 (44.6)\n14,072 (44.5)\n6,066 (44.8)\n0.62\nRace, n (%)\n< 0.001\nWhite\n24,404 (54.1)\n17,107 (54.1)\n7,297 (53.9)\nBlack\n7,043 (15.6)\n4,970 (15.7)\n2,073 (15.3)\nAsian\n512 (1.1)\n358 (1.1)\n154 (1.1)\nOther\n13,194 (29.2)\n9,172 (29.0)\n4,022 (29.7)\nClinical Severity, mean ± SD\nGCS score\n8.0 ± 4.6\n8.0 ± 4.6\n8.0 ± 4.6\n0.02\nAPACHE III score\n29.3 ± 19.5\n29.3 ± 19.6\n29.2 ± 19.4\n0.02\nSOFA score (24 h)\n3.9 ± 2.4\n3.9 ± 2.4\n3.9 ± 2.4\n0.03\nCharlson Comorbidity Index\n5.1 ± 3.0\n5.1 ± 3.0\n5.1 ± 3.0\n< 0.001\nVital Signs (first 24 hours), mean ± SD\nSpO2,min (%)\n87.0 ± 4.4\n87.0 ± 4.4\n87.0 ± 4.4\n0.65\nHeart rate (beats/min)\n87.8 ± 17.4\n87.8 ± 17.5\n87.7 ± 17.1\n0.52\nRespiratory rate (breaths/min)\n19.3 ± 0.6\n19.3 ± 0.6\n19.3 ± 0.6\n0.77\nMean arterial pressure (mmHg)\n84.8 ± 2.9\n84.8 ± 2.9\n84.8 ± 3.0\n0.39\nLaboratory Measurements, mean ± SD\nCreatininemax (mg/dL)\n3.2 ± 1.0\n3.2 ± 1.0\n3.2 ± 1.0\n0.70\nLactatemax (mmol/L)\n1.9 ± 0.4\n1.9 ± 0.4\n1.9 ± 0.4\n0.40\nTroponinmax (ng/mL)\n0.8 ± 0.3\n0.8 ± 0.3\n0.8 ± 0.3\n0.60\nPlateletmin (103/µL)\n167.7 ± 10.1\n167.8 ± 9.9\n167.8 ± 9.9\n0.38\nBilirubinmax (mg/dL)\n1.9 ± 0.8\n1.9 ± 0.7\n1.9 ± 0.7\n0.62\nWBCmax (103/µL)\n17.2 ± 2.1\n17.2 ± 1.9\n17.2 ± 1.9\n0.95\n24-hour urine output (mL)\n1680.8 ± 1223.2\n1651.4 ± 1202.4\n1651.4 ± 1202.4\n0.02\nTreatment Interventions\nMechanical ventilation, n (%)\n0 (0.0)\n0 (0.0)\n0 (0.0)\n1.00\nCode status, n (%)\n0 (0.0)\n0 (0.0)\n0 (0.0)\n1.00\nOutcomes\nIn-hospital mortality, n (%)\n6,500 (14.4%)\n4,535 (14.5%)\n1,965 (14.5%)\n0.836\nTable 2: Performance comparison of Qwen3-32B methods and XGBoost baseline\nModel\nMethod\nAUROC AUPRC\nAcc\nF1\nPrec\nSen\nSpec\nNPV\nBrier 95% CI\nQwen3-32B\nBase\n0.806\n0.497\n0.730 0.438 0.313 0.725 0.731 0.940\n0.110\n(0.104-0.115)\nFair\n0.791\n0.479\n0.696 0.402 0.281 0.705 0.694 0.933\n0.111\n(0.106-0.116)\nSystem 2\n0.785\n0.475\n0.753 0.426 0.322 0.632 0.774 0.925\n0.110\n(0.105-0.116)\nCAP\n0.873\n0.694\n0.777 0.525 0.380 0.850 0.764 0.968\n0.100\n(0.095-0.106)\nXGBoost\nTraining\n0.866\n0.548\n0.741 0.474 0.327 0.724 0.849 0.969\n0.084\n(0.078-0.089)\n4\n"}, {"page": 5, "text": "Figure 1: Discrimination performance variation across demographic subgroups for different\nmethods\nDiscriminative Power\nFigure 1 presents the AUROC performance across demographic subgroups. The results demon-\nstrate that the CAP method achieves optimal performance in mitigating discrimination bias. The\nbase model exhibited substantial performance disparities across subgroups, with a maximum\nAUROC difference of 0.07 across age groups. The fairness-optimized method reduced sub-\ngroup disparities but at the cost of overall performance degradation. The fairness optimization\napproach showed stable performance across age groups but provided limited improvement in\nracial dimensions.In contrast, the CAP method maintained high overall AUROC (approximately\n0.88) while reducing the maximum inter-group difference to 0.03. This method demonstrated bal-\nanced performance across sex, age, and racial dimensions, indicating significant bias mitigation\neffectiveness.\nFairness Assessment\nTable 3: Equal Opportunity Difference (EOD) across demographic subgroups\nDimension\nComparison\nBase\nFair\nSystem 2\nCAP\nSex\nMale vs Female\n0.083\n0.070\n0.035\n0.003\nFPR (M/F)\n0.306/0.223\n0.336/0.267\n0.242/0.207\n0.259/0.256\nAge\n18-59 vs 60+\n0.180\n0.164\n0.152\n0.126\nFPR (Young/Old)\n0.143/0.323\n0.191/0.355\n0.119/0.272\n0.170/0.296\nRace\nWhite vs Black\n0.058\n0.074\n0.046\n0.005\nFPR (W/B)\n0.251/0.193\n0.309/0.235\n0.221/0.175\n0.253/0.248\nWhite vs Other\n0.084\n0.023\n0.036\n0.022\nFPR (W/O)\n0.251/0.336\n0.309/0.332\n0.221/0.257\n0.253/0.275\nWhite vs Asian\n0.008\n0.031\n0.004\n0.031\nFPR (W/A)\n0.251/0.243\n0.309/0.278\n0.221/0.217\n0.253/0.222\n5\n"}, {"page": 6, "text": "Fairness assessment across demographic dimensions reveals method-specific bias patterns\n(Table 3). Using the Equal Opportunity Difference (EOD) metric, where values ≤0.05 indicate\nminimal bias, CAP demonstrates superior fairness in sex equality (EOD=0.003) and white-black\ncomparison (EOD=0.005). Sex fairness shows consistent improvement across methods, with\nCAP achieving near-perfect equality. Age-related bias remains challenging, with all methods\nexceeding the 0.05 threshold, though CAP shows the best performance (EOD=0.126). Ethnicity\ncomparisons display varied patterns, with CAP and System 2 achieving bias-free performance\nin specific comparisons.The analysis indicates that while no method completely eliminates bias\nacross all dimensions, CAP provides the most consistent fairness improvement, particularly in\ncritical demographic comparisons.\nFeature Dependence Analysis\nTable 4: Feature dependency similarity metrics across demographic subgroups\nDimension\nComparison\nMethod\nTop3 Jac\nAll Jac\nCosine\nJS Div\nSex\nMale vs Female\nBase\n0.935\n0.903\n0.996\n0.003\nFair\n0.955\n0.928\n0.998\n0.002\nSystem 2\n0.929\n0.897\n0.996\n0.003\nCAP\n0.965\n0.910\n0.997\n0.004\nAge\n18-59 vs 60+\nBase\n0.816\n0.736\n0.966\n0.033\nFair\n0.904\n0.855\n0.994\n0.007\nSystem 2\n0.872\n0.809\n0.977\n0.015\nCAP\n0.886\n0.791\n0.981\n0.016\nRace\nWhite vs Black\nBase\n0.956\n0.886\n0.995\n0.006\nFair\n0.954\n0.935\n0.999\n0.002\nSystem 2\n0.970\n0.935\n0.998\n0.002\nCAP\n0.957\n0.949\n0.999\n0.002\nWhite vs Other\nBase\n0.958\n0.917\n0.998\n0.004\nFair\n0.981\n0.959\n0.999\n0.001\nSystem 2\n0.974\n0.953\n0.999\n0.001\nCAP\n0.949\n0.945\n0.998\n0.001\nWhite vs Asian\nBase\n0.923\n0.838\n0.991\n0.015\nFair\n0.949\n0.931\n0.998\n0.002\nSystem 2\n0.929\n0.897\n0.997\n0.003\nCAP\n0.965\n0.883\n0.996\n0.009\nFeature dependency analysis reveals consistent patterns across demographic dimensions\n(Table 4). All methods demonstrate high feature similarity in sex comparisons (Cosine ≥0.996, JS\nDivergence ≤0.004), indicating minimal sex-based feature dependency bias. Age comparisons\nshow the most significant feature dependency variations. The base model exhibits the lowest\nsimilarity (Cosine=0.966, Top3 Jac=0.816), while fairness-optimized methods improve consistency\n(Cosine ≥0.977). CAP maintains competitive performance (Cosine=0.981, Top3 Jac=0.886)\nwhile preserving clinical relevance.Ethnicity comparisons display uniformly high similarity across\nmethods (Cosine ≥0.991), with CAP achieving optimal balance in white-black comparisons\n(All Jac=0.949, JS Div=0.002). The analysis confirms that feature dependency bias is most\npronounced in age-related comparisons, while sex and ethnicity dimensions show minimal\nvariance.\n6\n"}, {"page": 7, "text": "Analysis of Bias Mitigation Process\nFigure 2: Case studies demonstrating bias mitigation through clinical-aware prompting\nCase analysis reveals distinct patterns in bias mitigation through clinical-aware prompting10\n(Figure 2). Two representative cases illustrate how the CAP method adjusts mortality predictions\nwhile providing more nuanced clinical reasoning. In Case 1 (65-year-old White male), the CAP\nmodel explicitly acknowledges racial overestimation bias and adjusts the mortality prediction\ndownward from 0.55 to 0.45. The mitigation incorporates awareness of \"observed trends of racial\noverestimation bias in similar cases for non-white populations,\" demonstrating systematic bias\ncorrection. Case 2 (52-year-old Black female), The base model employed sex-based assumptions,\nstating \"the patient is relatively young and female, demographic factors that typically correlate\nwith better prognosis.\" In contrast, the CAP model eliminates this sex-biased reasoning, focusing\ninstead on objective clinical indicators including SOFA score, APACHE III score, and specific\nlaboratory values. This shift results in a more accurate mortality prediction reduction from 0.32 to\n0.22.The CAP method demonstrates consistent bias-aware reasoning across cases, explicitly\nacknowledging demographic factors while maintaining clinical relevance. This approach provides\ntransparent bias mitigation without compromising clinical validity.\nDISCUSSION\nThis study systematically validates the effectiveness of the proposed clinically adaptive prompting\nframework in addressing the fairness challenges arising when Large Language Models (LLMs) are\napplied to ICU mortality prediction. The results explicitly answer the three core research questions\n7\n"}, {"page": 8, "text": "posed in the introduction: First, experiments confirm that LLMs indeed exhibit measurable predic-\ntive biases toward specific demographic groups in this task; second, through multi-dimensional\nevaluation, we reveal that such biases exist not only at the model output level but also are rooted\nin the differences in the model’s internal decision-making logic regarding feature dependence\nacross different groups (i.e., implicit biases)11,12; finally, the CAse prompting mechanism is proven\nto be an effective solution—it successfully significantly mitigates performance disparities between\ngroups without sacrificing, and even improving, overall predictive performance.\nThe achievement of this \"win-win situation for fairness and performance\" is mainly attributed\nto the rich clinical context provided by case prompting for model reasoning. By learning from\nhistorical misjudgment cases through analogy, the model can focus on features that are more\nuniversal and clinically relevant, rather than group-associated features with implicit biases13.\nThis mechanism aligns the model’s reasoning process more closely with the decision-making\npatterns of clinical doctors, thereby simultaneously enhancing both the accuracy of results and\nthe rationality of the reasoning process14.\nCompared with traditional methods that require model parameter adjustments (e.g., fine-\ntuning) or fairness prompts constrained solely via instructions, the proposed framework offers\ndistinct advantages. As an external knowledge augmentation strategy, CAse prompting does not\nalter the internal structure of the model, preserving the base model’s general capabilities, while\neffectively guiding the reasoning process through carefully curated real-world clinical cases15.\nThis approach is low-cost, flexible, and—crucially—helps the model establish more robust clinical\ndecision logic by providing both positive and negative examples. As a result, it mitigates bias at\nits source rather than merely correcting output-level disparities, making the framework particularly\nsuitable for high-stakes clinical scenarios where fairness and interpretability are paramount.\nThe multidimensional bias evaluation system developed in this study, particularly from the\nperspective of clinical reasoning process, provides new insights into the internal mechanisms\nof model bias16,17. While conventional fairness metrics focus on statistical disparities in model\noutputs, feature-overlap analysis reveals whether the model’s internal attention patterns systemat-\nically differ across demographic groups during decision-making. Experimental results indicate\nthat CAse prompting effectively increases the consistency of feature attention across patient\nsubgroups, providing process-level evidence for its mitigation of latent biases. By extending\nfairness assessment from “outcome fairness” to “process fairness,” this evaluation framework\noffers a methodological foundation for comprehensive auditing of future clinical AI models.\nAll evaluations in this study were conducted using the Qwen3-32B large language model,\nenabling an in-depth analysis of its behavior under different prompting strategies and the estab-\nlishment of a complete evaluation loop. We acknowledge that the generalizability of this approach\nto LLMs of different scales or architectures remains to be systematically verified. Nevertheless,\nthe strength of our method lies in its core principle of intervention via prompting without retraining,\nwhich is theoretically architecture-agnostic and potentially transferable18. Future work will include\nsystematic validation across a broader spectrum of models and exploration of model-specific\nadjustments for case retrieval and prompt templates to maximize effectiveness. In addition,\ndeveloping more intelligent and efficient mechanisms for case retrieval and adaptation to enable\nreal-time application in dynamic clinical settings will be critical for translating this framework into\npractical clinical use.\n8\n"}, {"page": 9, "text": "METHODS\nRelated Works\nBias and Fairness in Clinical AI\nPredictive models in healthcare, particularly those deployed in high-risk settings such as intensive\ncare units (ICUs), have been widely reported to exhibit biases with respect to demographic\nattributes including race, sex, and age19–21. A recent scoping review highlighted that among\n91 clinical machine learning studies, nearly 75% reported evidence of bias, which often dis-\nproportionately affects vulnerable patient groups22. The primary sources of such bias include\ninadequate representation in training data and inherent documentation biases in medical records.\nFor instance, van Schaik et al. (2024) specifically examined ICU mortality prediction models and\ndemonstrated that conventional accuracy metrics alone are insufficient to reveal performance\ndisparities across race, sex, and diagnostic subgroups, emphasizing the need for systematic\nfairness monitoring23.\nApplications and Challenges of LLMs in Clinical Prediction\nThe use of large language models (LLMs) in healthcare has expanded from text generation to\nclinical prediction tasks. For instance,Brown et al.’s research showed that in clinical prediction\ntasks using two datasets (VUMC and MIMIC-IV), the predictive performance of GPT-3.5 and\nGPT-4 was far lower than that of traditional ML (as gradient-boosting trees) , with poor model\ncalibration (significantly higher Brier scores)24. Meanwhile, LLMs face challenges in balancing\nfairness and performance: although GPT-4 outperformed GPT-3.5 and traditional machine learning\nmodels in fairness metrics (average absolute odds difference, statistical parity difference) across\ndemographic subgroups such as race, sex, and age, its overall prediction accuracy remained\nrelatively low, and the improvement in fairness came at the cost of reduced overall model\nperformance24. These results suggest that new methods need to be designed to enable LLMs to\nmaintain high performance while ensuring fairness in high-risk medical tasks, thereby meeting\nthe rigorous requirements of clinical applications.\nFairness Enhancement Approaches in Clinical AI\nTo mitigate bias in healthcare AI, several technical approaches have been proposed, which can\nbe broadly categorized into three areas:\n• Data and Algorithm-level Methods: These include reweighting underrepresented groups\nin the training data and designing algorithms with fairness constraints. For example, the\nFAST-CAD framework integrates Domain-Adversarial Training (DAT) with Group Distribution-\nally Robust Optimization (Group-DRO). By learning demographic-invariant representations\nand optimizing worst-group performance, it achieves fair and accurate non-contact stroke\ndiagnosis, reaching an AUC of 91.2% with a 62% reduction in fairness gaps across de-\nmographic subgroups25.Dong et al.(2023)proposed the Co²PT, Counterfactual Contrastive\nPrompt Tuning, mitigates social biases in pre-trained language models (PLMs) for down-\nstream tasks26. In federated learning scenarios, Liu et al. (2025) proposed the DynamicFL\nframework, which dynamically adjusts model structures through heterogeneous local train-\ning and homogeneous global aggregation to mitigate implicit algorithmic bias caused by\nheterogeneous computational resources across healthcare institutions27.\n9\n"}, {"page": 10, "text": "• Prompt Engineering: As a lightweight way to regulate LLM outputs, prompt engineering\nhas been used in recent years to improve model performance and reduce bias. Existing\nstudies have shown that carefully designing prompts—such as providing examples, or\nspecifying fairness instructions—can significantly influence the generative bias of LLMs27.\nFor instance, Sakib et al. found that adding refined prompts in recommendation systems\nreduces models’ bias toward mainstream culture.28 Ma et al. proposed a fairness-guided\nfew-shot prompting strategy, which evaluates the predictive bias of candidate examples\nusing \"content-free input\" and dynamically selects examples via greedy search to maximize\nthe fairness score of the prompt27. This strategy achieved consistent relative improvements\nof over 10% across tasks like SST-2 and AGNews27.\n• Explainability and Attribution Analysis: With the increasing demand for trustworthy AI,\nthe use of explainable AI (XAI) techniques to audit and diagnose the sources of model bias\nhas become a critical pathway for enhancing fairness. These methods do not directly modify\nthe model but instead identify features that may lead to unfair predictions by analyzing\nthe model’s decision rationale. For instance, Gallaee et al. introduced the FunnyNodules\ndataset, which provides synthetic medical images with comprehensive ground-truth attribute\nannotations. This enables researchers to systematically analyze whether models learn\ncorrect attribute-target relationships and evaluate if their attention mechanisms align with\nthe medical features focused on by radiologist29. This helps identify whether models rely\non spurious correlations related to group attributes for predictions. By uncovering these\nunreasonable dependencies, developers can strategically clean data or adjust models,\nthereby addressing fairness issues at their root.\nProblem Definition\nThe core objective of this study is to predict patients’ in-hospital mortality risk based on structured\nclinical data from the first day of ICU admission. The input features are represented as:\nX = {xi}n\ni=1,\nxi ∈Rd\nwhere xi represents patient information encompassing five major categories of clinical data,\nincluding demographic characteristics, laboratory measurements, vital signs, Clinical Severity,\nand treatment interventions.\nThe model evaluates the risk stratification it belongs to and outputs the corresponding mortality\nprobability as:\nyi = f(xi) ∈[0, 1]\nAdditionally, the Large Language Models (LLMs) generate the prediction confidence, three key\ncontributing factors, and the reasoning chain. To systematically evaluate model fairness across\ndifferent patient populations, this study constructs subgroup sets covering commonly used clinical\ndimensions based on sex, age group, and ethnicity:\nS = {Sg,a,r | g ∈{male, female}, a ∈{18-59, 60+}, r ∈{white, black, asian, other}}\nWe analyze model outputs including mortality predictions, key factors, and reasoning pro-\ncesses, using relevant metrics to assess whether the models exhibit bias against different\nsubgroups, and compare how different approaches mitigate model bias while improving predictive\nperformance.\n10\n"}, {"page": 11, "text": "Overview of the CAse Prompting Framework\nBuilding on a systematic review of existing approaches, this study proposes the CAse Prompting\n(CAP) framework. Compared with the aforementioned methods, This framework not only unifies\nexisting prompting-based debiasing techniques, but also opens up new directions for30 balanc-\ning performance and fairness by guiding the model to learn from previously similar historical\nmisjudgment cases and their corresponding correct decisions, thereby correcting the model’s\nbiased reasoning patterns. Mechanistically, CAP leverages an analogy case repository to retrieve\nhistorical misclassified and biased cases most similar to the current patient via weighted cosine\nsimilarity, feeding the features and correct labels of these cases into the model as part of the\nprompt. Functionally, this approach transforms abstract fairness concepts into specific decision\nscenarios, aiming to simultaneously enhance predictive accuracy and fairness while exploring a\npath beyond the \"performance-fairness trade-off.\" It simulates the clinical practice where physi-\ncians optimize decisions by reviewing similar cases, thereby improving model interpretability.\nIn terms of positioning, CAP is a low-cost, training-free intervention that differs from traditional\nprompt engineering and fair machine learning methods requiring extensive modifications at the\ndata or algorithm level, offering a novel solution for the rapid and fair deployment of LLMs in\nclinical settings. Figure 3 illustrates the overall framework of the proposed CAP method.\nFigure 3: Overall framework for LLM-based ICU mortality prediction bias detection and mitigation.\nConstruction of the CAse Repository\nTo construct the core CAse repository of this study, we first systematically screened misclassified\ncases from the baseline model’s prediction results on the training set, specifically including\ntwo types of prediction errors: false negatives (FN) and false positives (FP). Subsequently,\nwe conducted bias analysis on these initially screened misclassified cases, mainly adopting\ncounterfactual pair analysis—constructing a counterfactual control version for each original case\nthat differs only in a specific key demographic attribute. We then used the powerful GPT-4 model\nas an analytical tool to compare, reason about, and examine the outputs of the original cases and\n11\n"}, {"page": 12, "text": "their counterfactual controls, thereby determining whether the misclassified case exhibits decision\nbias caused by specific demographic factors. Finally, we selected a small but representative subset\nof cases covering major clinical variations for standardized information extraction, including their\nkey clinical features, group attributes, clinical outcomes, and bias types, ultimately constructing a\ncase database specifically designed for subsequent retrieval and analogical learning.\nSimilarity Computation and Case Retrieval\nDuring the retrieval stage, clinical features of the query patient and all cases in the repository\nare first normalized, with logarithmic transformation applied when necessary to mitigate scale\neffects and stabilize variance. We then compute the overall similarity using a weighted cosine\nsimilarity metric. To ensure sensitivity to clinically meaningful nuances, a threshold-based penalty\nmechanism is applied to amplify differences when the same physiological variable falls into distinct\nclinically relevant intervals. The retrieval strategy is defined as follows: from the repository, we\nextract cases that match the query patient on at least two demographic attributes and exhibit\na weighted similarity score of at least 0.8. Among these candidates, the case with the highest\nsimilarity score is selected as the analogy case used for prompting31.\nPrompt Design\nTo systematically evaluate the impact of prompt engineering on model fairness, this study designs\na set of progressive prompting strategies based on the intrinsic capabilities of Large Language\nModels (LLMs) for self-reflection, contextual reasoning, and generating transparent reasoning\ntraces32. All prompts are constructed within a unified, four-module structured framework (Role\nDefinition, Task Description, Decision Constraints, and Case Information) to ensure fairness and\nconsistency in comparative experiments. These strategies aim to mitigate potential social biases\nin model decision-making through structured cognitive guidance. Complete prompt template\nexamples are provided in Figure 4.\n• Baseline Prompt. Only includes basic instructions for the ICU mortality prediction task,\nwithout any fairness constraints.\n• Fairness-Aware Prompt. Adds a fairness prefix to the baseline prompt, explicitly requiring\nthe model to avoid predictions based on protected characteristics such as age, sex, and\nethnicity33.\n• System 2 Prompting. Further integrates fairness constraints with slow-thinking require-\nments, guiding the model to adopt a step-by-step reasoning approach—first analyzing\nphysiological indicators, then evaluating organ function, and finally making predictions by\nsynthesizing clinical evidence34. It emphasizes an evidence-based decision-making process\nto systematically reduce intuitive biases33,35.\n• CAse Prompt. As the core method proposed in this study, it incorporates retrieved analo-\ngous cases. Each case includes a summary of clinical features of similar patients, actual\noutcomes, and labeled bias types, enabling contextualized fair decision guidance through\nanalogical learning.\n12\n"}, {"page": 13, "text": "Figure 4: Architecture of Four Prompting Methods for ICU In-Hospital Mortality Risk Prediction\nMultidimensional Bias Evaluation Framework\nTo systematically assess the presence of bias in LLM-based ICU mortality prediction, we de-\nveloped a multidimensional evaluation framework. This framework spans three key dimen-\nsions—model discrimination, group fairness, and consistency of decision logic—to reveal potential\nbiases that manifest both in model outputs and in the internal reasoning process.\n• Discrimination Assessment. Discriminative power evaluation: We adopted AUC-ROC\nas the primary metric to assess the overall discriminative ability of the model. A absolute\nAUC difference 0.05 across subgroups was considered indicative of bias. Meanwhile, the\nF1-score was used as a supplementary metric to comprehensively evaluate the balance\nbetween precision and recall, and the Brier Score was employed to examine the degree of\nprobability calibration.\n• Group Fairness Assessment. Group fairness evaluation: The maximum AUC difference\namong key demographic subgroups (e.g., sex, age, and ethnicity) was calculated to quantify\nperformance disparities across groups. Additionally, Equalized Odds Difference (EOD),\ndefined as the difference between the false negative rate (FNR) of a subgroup and that of\nthe reference group, was used to analyze the consistency of FNR across different groups36.\nAn absolute EOD value exceeding 5 percentage points was deemed to indicate significant\nbias.\n• Implicit Feature dependency Analysis. We probed the consistency of the model’s decision-\nmaking logic by analyzing the key physiological signs output by the model. This metric\nquantifies the presence of \"implicit bias\"—where the model focuses on distinct feature\npatterns across different patient subgroups—by comparing indicators such as Jaccard\n13\n"}, {"page": 14, "text": "similarity, cosine similarity, and JS Divergence of the Top-K key feature sets relied on by the\nmodel when making predictions for patients in different subgroups. Higher feature similarity\nindicates that the model’s decision-making logic is more consistent and fair.\nThis multidimensional framework extends the fairness evaluation from “output bias” to “feature-\nlevel bias,” allowing deeper interpretation of why a model may underperform for certain subgroups.\nTherefore, it provides a comprehensive and actionable foundation for designing targeted bias\nmitigation strategies.\nExperimental Setup\nAll experiments were performed using the Qwen3-32B large language model for inference,\nwith model parameters kept frozen without any fine-tuning. To evaluate the impact of different\nprompting strategies on ICU mortality prediction, this study set three types of prompting methods\nas controls: Basic Prompt, Systematic Clinical Prompt, and Fairness-Enhanced Prompt. On this\nbasis, the proposed CAse Prompting (CAP) method was introduced. Additionally, to provide a\ncomparison with traditional approaches to language model reasoning, XGBoost was used as\nthe machine learning baseline model. This model was trained on the same training set, and\nperformance comparison was conducted using an independent test set. All evaluation metrics\ninclude performance indicators such as AUROC, AUPRC, F1-score, and Brier score, as well as\nfairness metrics such as Equalized Odds Difference (EOD) and feature consistency. Experiments\nwere run on a computing environment equipped with an NVIDIA RTX A6000 GPU, and all data\nprocessing, case retrieval, and model inference were implemented based on Python 3.10.\nData Availability\nThe MIMIC-IV dataset used in this study is publicly available through PhysioNet at https://\nphysionet.org.\nCode Availability\nThe core code supporting this study has been released on GitHub. The repository is accessible\nat: https://github.com/zgx-718/LLMs_bias_eva.\nDECLARATION OF INTERESTS\nThe authors declare no competing interests.\nReferences\n[1] Meng Zhang, Yongqi Zheng, Xiagela Maidaiti, Baosheng Liang, Yongyue Wei, and Feng Sun.\nIntegrating machine learning into statistical methods in disease risk prediction modeling: A\nsystematic review. Health Data Science, 4:0165, 2024.\n[2] Fereshteh Hasanzadeh, Colin B Josephson, Gabriella Waters, Demilade Adedinsewo, Zahra\nAzizi, and James A White. Bias recognition and mitigation strategies in artificial intelligence\nhealthcare applications. NPJ Digital Medicine, 8(1):154, 2025.\n14\n"}, {"page": 15, "text": "[3] Mingxuan Liu, Yilin Ning, Salinelat Teixayavong, Xiaoxuan Liu, Mayli Mertens, Yuqing Shang,\nXin Li, Di Miao, Jingchi Liao, Jie Xu, et al. A scoping review and evidence gap analysis of\nclinical ai fairness. npj Digital Medicine, 8(1):360, 2025.\n[4] Jack A Cummins, Ben S Gerber, Mayuko Ito Fukunaga, Nils Henninger, Catarina I Kiefe,\nand Feifan Liu. In-hospital mortality prediction among intensive care unit patients with acute\nischemic stroke: A machine learning approach. Health Data Science, 5:0179, 2025.\n[5] Xiaoli Liu, Pan Hu, Wesley Yeung, Zhongheng Zhang, Vanda Ho, Chao Liu, Clark Dumontier,\nPatrick J Thoral, Zhi Mao, Desen Cao, et al. Illness severity assessment of older adults\nin critical illness using machine learning (elder-icu): an international multicentre study with\nsubgroup bias evaluation. The Lancet Digital Health, 5(10):e657–e667, 2023.\n[6] Shenda Hong, Xinlin Hou, Jin Jing, Wendong Ge, and Luxia Zhang. Predicting risk of\nmortality in pediatric icu based on ensemble step-wise feature selection. Health Data\nScience, 2021:9365125, 2021.\n[7] Chuizheng Meng, Loc Trinh, Nan Xu, James Enouen, and Yan Liu. Interpretability and\nfairness evaluation of deep learning models on mimic-iv dataset. Scientific Reports, 12(1):\n7166, 2022.\n[8] Travis Zack, Eric Lehman, Mirac Suzgun, Jorge A Rodriguez, Leo Anthony Celi, Judy Gichoya,\nDan Jurafsky, Peter Szolovits, David W Bates, Raja-Elie E Abdulnour, et al. Assessing the\npotential of gpt-4 to perpetuate racial and gender biases in health care: a model evaluation\nstudy. The Lancet Digital Health, 6(1):e12–e22, 2024.\n[9] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung,\nNathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language\nmodels encode clinical knowledge. Nature, 620(7972):172–180, 2023.\n[10] Hossein Estiri, Zachary H Strasser, Sina Rashidian, Jeffrey G Klann, Kavishwar B Wagholikar,\nThomas H McCoy Jr, and Shawn N Murphy. An objective framework for evaluating unrec-\nognized bias in medical ai models predicting covid-19 outcomes. Journal of the American\nMedical Informatics Association, 29(8):1334–1341, 2022.\n[11] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and\nDino Pedreschi. A survey of methods for explaining black box models. ACM computing\nsurveys (CSUR), 51(5):1–42, 2018.\n[12] Shengjie Wang, Tianyi Zhou, and Jeff Bilmes. Bias also matters: Bias attribution for deep\nneural network explanation. In International Conference on Machine Learning, pages 6659–\n6667. PMLR, 2019.\n[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information processing systems, 33:\n1877–1901, 2020.\n[14] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning\nwork? arXiv preprint arXiv:2202.12837, 2022.\n15\n"}, {"page": 16, "text": "[15] Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large\nlanguage models are latent variable models: Explaining and finding good demonstrations for\nin-context learning. Advances in Neural Information Processing Systems, 36:15614–15638,\n2023.\n[16] Jinsook Lee, Yann Hicke, Renzhe Yu, Christopher Brooks, and René F Kizilcec. The life\ncycle of large language models in education: A framework for understanding sources of bias.\nBritish Journal of Educational Technology, 55(5):1982–2002, 2024.\n[17] Harini Suresh and John Guttag. A framework for understanding sources of harm throughout\nthe machine learning life cycle. In Proceedings of the 1st ACM Conference on Equity and\nAccess in Algorithms, Mechanisms, and Optimization, pages 1–9, 2021.\n[18] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems, 35:24824–24837, 2022.\n[19] Zhao Shi, Bingqian Wu, Bin Hu, Jian Zhong, Zezhong Li, Fandong Zhang, Zijian Chen,\nChun Yang, Bangjun Guo, Qinmei Xu, et al. A large language model for clinical outcome\nadjudication from telephone follow-up interviews: a secondary analysis of a multicenter\nrandomized clinical trial. Nature Communications, 2025.\n[20] Yumeng Zhang and Jiangning Song. Toward fair ai-driven medical text generation. Nature\nComputational Science, pages 1–2, 2025.\n[21] Zhiyu Wan, Yuhang Guo, Shunxing Bao, Qian Wang, and Bradley A Malin. Evaluating sex\nand age biases in multimodal large language models for skin disease identification from\ndermatoscopic images. Health Data Science, 5:0256, 2025.\n[22] M. Colacci and others. Unequal outcomes: Tackling bias in clinical AI models. Summarized\nin Journal of Clinical Epidemiology, 2025. Systematic review of bias in clinical AI models.\n[23] Tempest A van Schaik, Xinggang Liu, Louis Atallah, and Omar Badawi. Monitoring fair-\nness in machine learning models that predict patient mortality in the icu. arXiv preprint\narXiv:2411.00190, 2024.\n[24] Katherine E Brown, Chao Yan, Zhuohang Li, Xinmeng Zhang, Benjamin X Collins, You Chen,\nEllen Wright Clayton, Murat Kantarcioglu, Yevgeniy Vorobeychik, and Bradley A Malin. Large\nlanguage models are less effective at clinical prediction tasks than locally trained machine\nlearning models. Journal of the American Medical Informatics Association, 32(5):811–822,\n2025.\n[25] Tianming Sha, Zechuan Chen, Zhan Cheng, Haotian Zhai, Xuwei Ding, Junnan Li, Haixiang\nTang, Zaoting Sun, Yanchuan Tang, Yongzhe Yi, et al. Fast-cad: A fairness-aware framework\nfor non-contact stroke diagnosis. arXiv preprint arXiv:2511.08887, 2025.\n[26] Xiangjue Dong, Ziwei Zhu, Zhuoer Wang, Maria Teleki, and James Caverlee. Co2pt: Mitigat-\ning bias in pre-trained language models through counterfactual contrastive prompt tuning.\narXiv preprint arXiv:2310.12490, 2023.\n[27] Feilong Zhang, Deming Zhai, Guo Bai, Junjun Jiang, Qixiang Ye, Xiangyang Ji, and Xianming\nLiu. Towards fairness-aware and privacy-preserving enhanced collaborative learning for\nhealthcare. Nature Communications, 16(1):2852, 2025.\n16\n"}, {"page": 17, "text": "[28] Shahnewaz Karim Sakib and Anindya Bijoy Das. Challenging fairness: A comprehensive\nexploration of bias in llm-based recommendations. In 2024 IEEE International Conference\non Big Data (BigData), pages 1585–1592. IEEE, 2024.\n[29] Luisa Gallée, Yiheng Xiong, Meinrad Beer, and Michael Götz. Funnynodules: A customizable\nmedical dataset tailored for evaluating explainable ai, 2025. URL https://arxiv.org/abs/\n2511.15481.\n[30] Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, and Yang Liu.\nPrompting fairness: Integrating causality to debias large language models. In The Thirteenth\nInternational Conference on Learning Representations.\n[31] Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang,\nHuazhu Fu, Qinghua Hu, and Bingzhe Wu. Fairness-guided few-shot prompting for large\nlanguage models. Advances in Neural Information Processing Systems, 36:43136–43155,\n2023.\n[32] Arjun Mahajan, Ziad Obermeyer, Roxana Daneshjou, Jenna Lester, and Dylan Powell.\nCognitive bias in clinical large language models. npj Digital Medicine, 8(1):428, 2025.\n[33] Shaz Furniturewala, Surgan Jandial, Abhinav Java, Pragyan Banerjee, Simra Shahid, Sumit\nBhatia, and Kokil Jaidka. Thinking fair and slow: On the efficacy of structured prompts for\ndebiasing language models. arXiv preprint arXiv:2405.10431, 2024.\n[34] Tong Jiao, Jian Zhang, Kui Xu, Rui Li, Xi Du, Shangqi Wang, and Zhenbo Song. Enhanc-\ning fairness in llm evaluations: Unveiling and mitigating biases in standard-answer-based\nevaluations. In Proceedings of the AAAI Symposium Series, volume 4, pages 56–59, 2024.\n[35] Mahammed Kamruzzaman and Gene Louis Kim. Prompting techniques for reducing social\nbias in llms through system 1 and system 2 cognitive processes. CoRR, 2024.\n[36] Shaina Mackin, Vincent J Major, Rumi Chunara, and Remle Newton-Dame. Identifying and\nmitigating algorithmic bias in the safety net. npj Digital Medicine, 8(1):335, 2025.\n17\n"}]}