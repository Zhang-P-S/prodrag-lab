{"doc_id": "arxiv:2511.18084", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.18084.pdf", "meta": {"doc_id": "arxiv:2511.18084", "source": "arxiv", "arxiv_id": "2511.18084", "title": "The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Kaipeng Xie", "Runze Yang", "Di Liu", "Kang Li", "Yiting Lin", "Hanyi Liu", "Rong Yin", "Tian Tang"], "published": "2025-11-22T14:48:54Z", "updated": "2025-11-22T14:48:54Z", "summary": "Large language models (LLMs) are increasingly adopted in clinical decision support, yet aligning them with the multifaceted reasoning pathways of real-world medicine remains a major challenge. Using more than 8,000 infertility treatment records, we systematically evaluate four alignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-layer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments. GRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the value of reinforcement-based optimization for structured prediction tasks. However, clinicians consistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher therapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest winning rate (51.2%), outperforming both GRPO (26.2%) and even physicians' original decisions (22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessarily translate into higher clinical trust, and may diverge from human-centered preferences. Our findings highlight the need for alignment strategies that prioritize clinically interpretable and practically feasible reasoning, rather than solely optimizing decision-level accuracy.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.18084v1", "url_pdf": "https://arxiv.org/pdf/2511.18084.pdf", "meta_path": "data/raw/arxiv/meta/2511.18084.json", "sha256": "034933a4734da91c989b6a1288da13eb150991caeae98cc5db27f384c31dd8c1", "status": "ok", "fetched_at": "2026-02-18T02:26:28.728454+00:00"}, "pages": [{"page": 1, "text": "The Alignment Paradox of Medical Large Language Models\nin Infertility Care: Decoupling Algorithmic Improvement\nfrom Clinical Decision-making Quality\nDou Liu1,2,6†, Ying Long1,6,7†, Sophia Zuoqiu3, Kaipeng Xie3, Runze Yang3,\nDi Liu3,4,5, Kang Li4,5, Yiting Lin8, Hanyi Liu8, Rong Yin3,4,5*, Tian Tang1,6,7*\n1Department of Obstetrics and Gynecology, West China Second University Hospital,\nChina.\n2Department of Industrial and Operations Engineering, University of Michigan, U.S..\n3Department of Industrial Engineering, Sichuan University, China.\n4West China Biomedical Big Data Center, West China Hospital, China.\n5Med-X Center for Informatics, Sichuan University, China.\n6Key Laboratory of Birth Defects and Related Diseases of Women and Children,\nSichuan University, China.\n7Reproductive Medical Center, Department of Obstetrics and Gynecology, West\nChina Second University Hospital, Sichuan University, China.\n8West China School of Medicine, Sichuan University, China.\n*Corresponding author(s). E-mail(s): rong.yin@scupi.cn; tiantang2016@scu.edu.cn;\n†These authors equally contributed to this work.\nAbstract\nLarge language models (LLMs) are increasingly adopted in clinical decision support, yet align-\ning them with the multifaceted reasoning pathways of real-world medicine remains a major\nchallenge. Using more than 8,000 infertility treatment records, we systematically evaluate four\nalignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nGroup Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-\nlayer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments.\nGRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the\nvalue of reinforcement-based optimization for structured prediction tasks. However, clinicians\nconsistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher\ntherapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest\nwinning rate (51.2%), outperforming both GRPO (26.2%) and even physicians’ original decisions\n(22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessar-\nily translate into higher clinical trust, and can diverge from human-centered preferences. Our\nfindings highlight the need for alignment strategies that prioritize clinically interpretable and\npractically feasible reasoning, rather than solely optimizing decision-level accuracy.\nKeywords: Large Language Models, Clinical Alignment, Reinforcement Learning, Assisted\nReproductive Technology, Reproductive Medicine\n1\narXiv:2511.18084v1  [cs.LG]  22 Nov 2025\n"}, {"page": 2, "text": "1 Introduction\nLarge language models (LLMs) have rapidly advanced across domains, yet how their post-training\nalignment interacts with the hierarchical, high-stakes reasoning of real-world medicine remains poorly\nunderstood. These alignment methods usually involve various reinforcement learning variants such\nas Reinforcement Learning with Human Feedback (RLHF), Group Relative Policy Optimization\n(GRPO), and Direct Preference Optimization (DPO), which have shown remarkable improvement\nwithin some general areas by using verifiable rewards or preference signals to optimize the policy\nmodel. [1–5]For example, DeepSeek-R1 adopted the GRPO and its performance on math questions\nsignificantly outperformed the baseline. However, clinical decision-making presents a fundamentally\ndifferent challenge: reasoning chains are long and multi-dimensional, rewards are noisy or unveri-\nfiable, and clinicians rely heavily on explanation quality and feasibility rather than solely output\ncorrectness alone. These properties suggest a structural tension between algorithmic alignment and\nclinical alignment, a tension that has not been mechanistically examined. The domain-specific med-\nical LLMs, like Med-Palm, MedGemma, and Lingshu, were proven to exhibit profound potential in\nsolving clinical problems, ranging from text classification to clinical image reports\n[6–9]. These models demonstrate capabilities that surpass human-level performance in the answer\ncorrectness on benchmark datasets. Despite their outstanding performance, when applying these\nstate-of-the-art (SOTA) models to specialized real-world clinical cases instead of a general con-\nsultation, which could support the realistic diagnosis operation, they seem to encounter some\nobstacles[10, 11]. Although SOTA models generated responses were preferred to those provided by\ngeneralist physicians, they still did not outperform domain-specific specialists, indicating limited\nutility in addressing highly specialized and practical clinical problems [12]. Moreover, their opaque\nreasoning processes limit clinical adoption, despite evidence that explanation-based systems (“XAI”)\nenhance trust and interpretability [13–16]. While supervised fine-tuning (SFT) remains the dom-\ninant paradigm in medical model development[10, 17], data scarcity and incomplete supervision\nmotivate increasing reliance on post-training alignment[8, 18, 19]. Yet, despite the rapid adoption of\nRLHF-style optimization, little is known about whether improvements in algorithmic metrics trans-\nlate to enhanced interpretability, trust, or multi-step clinical reasoning. Even fewer studies have\nevaluated how different alignment paradigms behave in real-world, interdependent clinical decision\nenvironments where reasoning quality directly affects patient safety. This gap raises a fundamental\nscientific question: Does stronger post-training alignment produce more clinically aligned\nmodels—or can optimization distort clinical reasoning, giving rise to an alignment\nparadox?\nAs one of the medical diseases and global health issues declared by the World Health Organiza-\ntion (WHO), Infertility is estimated to be experienced by one in six at some stage in their lives\nglobally[20]. Assisted Reproductive Technology (ART) has become a central clinical pathway for\nthese patients, typically after the failure of conventional treatments such as cycle regulation or ovu-\nlation induction[21]. ART serves as an ideal testbed for reasoning alignment because it demands\nthe rigorous integration of high-dimensional data, including age, Anti-M¨ullerian Hormone (AMH),\nFollicle-Stimulating Hormone (FSH), Body Mass Index (BMI), endocrine profiles, past medical his-\ntory, and gynecological ultrasound findings, to formulate a safe and effective treatment plan. Beyond\nselecting the ART strategy, clinicians must also determine the controlled ovarian stimulation (COS)\nprotocol and the gonadotropin (Gn) starting dose, decisions that are interdependent and sensitive to\nsubtle variations in the patient’s physiological state. Consequently, ART decision-making is a multi-\nstage, high-dimensional, and evidence-driven process that is both time-consuming and cognitively\ndemanding. Clinical outcomes are highly dependent on nuanced reasoning: inappropriate treatment\nselection can reduce cycle success rates, increase financial and emotional burden, and even elevate the\nrisk of severe complications such as ovarian hyperstimulation syndrome (OHSS)[22]. These decisions\nrely heavily on individual clinical experience, leading to substantial inter-physician and inter-center\nvariability and limiting the scalability of standardized training for novice practitioners. Moreover,\n2\n"}, {"page": 3, "text": "high outpatient volumes can induce decision fatigue among experienced clinicians[23]. The inherent\ncomplexity, multi-layered structure, and safety-critical nature of ART therefore make it not only a\nrepresentative real-world medical decision environment, but also an exceptionally sensitive setting in\nwhich failures of model alignment, if present, are likely to be amplified and clinically consequential.\nWe therefore pose three key questions:\n• What is the performance of standard SFT when applied to real-world, multi-dimensional clinical\ndecision-making?\n• Can advanced post-training alignment strategies achieve superior performance on objective\ndecision-making metrics in such settings, and what patterns of improvement or degradation emerge\nacross decision layers?\n• Critically, do gains in algorithmic alignment translate into higher clinical trust, or do they instead\nreveal a divergence between accuracy-based optimization and clinicians’ subjective assessments of\nreasoning quality and therapeutic feasibility?\nTo address these questions, we systematically compare four representative alignment paradigms:\nSFT, DPO, GRPO, and In-Context Learning (ICL), using a large-scale real-world dataset of more\nthan 8,000 infertility treatment records. To enhance robustness on challenging and long-tail cases,\nwe construct a hierarchical “pyramid” dataset that emphasizes diverse difficulty levels and decision\nstructures. Most importantly, we introduce a dual-evaluation framework that integrates automated\nmetric-based assessment with triple-blind doctor-in-the-loop evaluations, enabling us to directly\nexamine the relationship between algorithmic optimization and clinician-centered preferences. This\nframework allows us to uncover not only performance differences across alignment strategies, but also\nthe underlying tension between algorithmic alignment and clinical alignment, a tension that forms\nthe core of the alignment paradox we investigate in this study.\n2 Results\n2.1\nAutomatic Metrics Performance\nTable 1 summarizes the field-level performance of the four models. Among them, GRPO (the SFT\nmodel trained by GRPO) achieves the best overall results in the fields of Infertility type, ART strat-\negy, and COS regimen, with the highest average accuracy (77.14%) and macro F1 score (50.64%).\nCompared to the base SFT model, GRPO improves ART strategy selection accuracy by 2.68% and\n3.81% in macro F1. GRPO also demonstrates clear enhancement in the unstructured Initial diagnosis\ntask, achieving a 1.95% higher partial match rate and a 4.24% increase in exact matches compared to\nthe baseline SFT. These improvements highlight the value of reward-driven reinforcement learning in\ncapturing complex clinical reasoning. DPO also shows moderate gains in ART strategy choice (accu-\nracy +1.22% , F1 +2.44% vs. SFT), confirming that preference-based optimization can strengthen\ndecision alignment. However, its instability is reflected in the initial diagnosis task, where both partial\nand exact matches drop substantially (41.29% and 4.63%, nearly half of SFT), suggesting vulner-\nability to noisy or ambiguous supervision. Although initialized from the same SFT backbone, ICL\nfailed in nearly all metrics, with only marginal improvement in ART strategy (accuracy +0.95%).\nThis indicates that simple in-context prompting lacks robustness for multi-layer medical decision-\nmaking. Notably, For the COS regimen prediction, all models struggle due to the possible inherently\nambiguous category boundaries. GRPO records a slight decrease in accuracy (–1.34% vs. SFT) and\nF1 (–0.98%), while DPO and ICL exhibit a larger accuracy drop (–2.80%, -4.26%) despite a minor\nF1 improvement by DPO (+0.56%). This suggests that for tasks with complex or flexible category\nboundaries, even reinforcement learning fails to yield substantial performance gains.\n3\n"}, {"page": 4, "text": "Fig. 1: Overview of the Alignment and Evaluation Framework\nThe proposed dual-alignment framework integrates supervised fine-tuning (SFT) with post-training\noptimization methods, DPO, GRPO, and ICL, to investigate the alignment paradox between\nalgorithmic optimization and clinical trust. A pyramid-curated dataset enhances model robustness\nacross general, confusing, and human-refined cases. During modeling, SFT provides the foundational\nreasoning structure, while GRPO and DPO incorporate reward and preference feedback to refine\npolicy behavior, ICL embed two clinical guidance blocks. Model outputs are assessed through a\ndual-evaluation system combining automatic metrics and doctor-in-the-loop blind assessments.\nHuman experts evaluate outputs on three clinical dimensions—diagnostic accuracy, reasoning\nclarity, and treatment feasibility—revealing the divergence between benchmark performance and\nclinician-perceived interpretability in assisted reproductive decision-making. The proposed dual-\nalignment framework integrates SFT with three post-training alignment strategies (DPO, GRPO,\nand ICL) to examine how algorithmic optimization interacts with clinical trust. A pyramid-curated\ndataset covering general, ambiguous, and expert-refined cases is used to enhance robustness across\ndifficulty levels. SFT establishes the core reasoning structure, while GRPO and DPO introduce\nreward- and preference-based policy refinement, and ICL incorporates structured clinical guidance\nblocks. Model outputs are evaluated through a dual-layer assessment combining automatic metrics\nwith triple-blind doctor-in-the-loop reviews. Clinicians rate each output on diagnostic accuracy,\nreasoning clarity, and treatment feasibility, enabling us to identify divergences between benchmark\nperformance and clinician-perceived interpretability in assisted reproductive decision-making.\n2.2 Doctor-in-the-loop Evaluation\nTo evaluate the models’ clinical performance, we utilized human feedback from an expert panel. For\nthis evaluation, physicians blindly reviewed 100 representative cases, which have the same ART dis-\ntributions as the test set in automatic evaluation. Here we selected the baseline model (SFT) and\nthe post-training model (GRPO), which achieved the highest automatic scores. Each model’s output\nwas scored on four primary dimensions: Diagnosis Accuracy (Acc.), Clinical Reasoning Capability\n(Reasoning), Treatment Plan Clinical Feasibility (Feasibility), and Hallucination . The results of\n4\n"}, {"page": 5, "text": "Table 1: Field-level performance comparison of SFT, ICL, DPO, and GRPO models on infertility-\nrelated decision tasks.\nModel Infertility Type ART Strategy COS Regimen\nAverage\nInitial Diagnosis Gn Dose\nACC\nF1\nACC\nF1\nACC\nF1\nACC\nF1\nPartial\nExact\nMAE\nSFT\n92.57\n92.18\n73.81\n49.18\n63.70\n7.75\n76.69\n49.70\n87.45\n16.08\n43.88\nICL\n90.83\n89.88\n74.76\n33.31\n59.44\n5.89\n75.01\n43.03\n87.21\n16.09\n48.38\nDPO\n92.20\n91.33\n75.03\n46.62\n60.90\n8.31\n76.04\n48.75\n41.29\n4.63\n45.12\nGRPO\n92.57\n92.05\n76.49\n52.99\n62.36\n6.87\n77.14 50.64 89.40\n20.32\n44.94\nResults are reported for five evaluation fields: infertility type classification, ART strategy selection,\nCOS regimen prediction, initial diagnosis (partial and exact match), and gonadotropin (Gn)\nstarting dose (MAE). ACC = accuracy, F1 = macro F1 score, MAE = mean absolute error. Bold\nvalues indicate the best performance in each column. GRPO demonstrates the highest overall\naccuracy and F1, while DPO shows instability in initial diagnosis.\nFig. 2: Doctor-in-the-loop evaluation.\n(a) Response subjective evaluation\nModel Acc. Reason. Feas. Hallu. (%)\nSFT\n4.04\n4.17\n4.21\n18.6\nGRPO 3.99\n4.08\n4.09\n15.0\n(b) Winning rate comparison.\nG.T.\nGRPO\nSFT\n0\n20\n40\n60\n22.7\n26.2\n51.2\nWinning Rate (%)\nResults of (a) expert-rated performance and (b) blind winning rate comparison across SFT, GRPO,\nand physician ground truth (G.T.). (a) The SFT shows significantly higher reasoning and feasibility\nscores, GRPO has a lower hallucination percentage. (b) both SFT and GRPO outperform the\nhuman-level answer, SFT accounts for more than half of the better responses.\nthis expert scoring (Figure 2) revealed a significant discrepancy with the findings from the auto-\nmated metrics. Despite the automated evaluation showed GRPO consistently outperforming SFT,\nthe physician ratings reversed this trend. Physicians preferred more on the SFT model significantly\non both reasoning capability (p = 0.033, adj. p = 0.048, Cohen’s d = 0.22) and clinical feasibility (p\n= 0.015, adj. p = 0.045, Cohen’s d = 0.25). There were No significant differences observed in diagno-\nsis accuracy between the two models. For the additional dimension, hallucination, GRPO was rated\nto have a lower rate (15.00%) compared to SFT (18.61%). Though GRPO’s lower hallucination rate\nand superior automated metrics, the findings on reasoning and feasibility suggest that the algorith-\nmic improvements did not translate into perceived clinical trust or practical usefulness. Together,\nthese results clearly demonstrate that algorithmic gains from reinforcement-style alignment do not\ntranslate into improvements in clinical trust. Instead, they reveal an alignment paradox in which\noptimization enhances benchmark performance but degrades reasoning clarity and perceived decision\nfeasibility.\n5\n"}, {"page": 6, "text": "Fig. 3: Reinforcement learning–based alignment improves F1 scores in IVF and PGT, but\nconsistently reduces performance in ICSI. All values are reported as percentages.\n2.3 Winning rate\nIn addition to the clinical dimensions, the expert panel was asked to blindly identify the overall\nbest response from the scored two responses and a third human-level response, which we directly\nuse the physician ground truth. The results were striking. Physicians selected the SFT model in\nmore than half of the cases (51.2%), far exceeding both the GRPO answers (26.2%) and even the\noriginal treatment plans (22.7%). Interestingly, even the GRPO, despite being less favored than SFT,\nstill surpassed the ground truth. This outdoing suggests that both aligned models may generate\ndecisions perceived as comparably or even more clinically useful than those originally made by human\nphysicians. This pattern enhances the claim of the divergence between algorithmic optimization and\nclinical perception. While GRPO achieved promising token-level accuracy and lower hallucination\nrates, physicians overwhelmingly preferred the SFT model for its reasoning transparency and decision\ncoherence. In other words, though improves benchmark metrics, the optimization appeared to erode\nhuman-perceived comprehension and trust.\nCollectively, these results suggest that current alignment objectives may not fully capture what\nclinicians value in practice, such as clarity, rationale, and contextual adaptability. Both SFT and\nGRPO have already exceeded human consistency, yet differ sharply in how they earn clinical trust.\nThis pattern demonstrates that alignment optimization may shift the model toward high-scoring\nbut less interpretable solutions, widening the gap between algorithmic metrics and human-centered\nevaluation\n2.4 Subgroup Analysis\nTo elucidate how different alignment strategies influence decision performance across heterogeneous\nclinical contexts, we first evaluated model behavior at the level of three major ART categories: ICSI,\nIVF, and PGT (Figure 3). Both GRPO and DPO yield marginal yet consistent gains over SFT in\nIVF (SFT = 0.924, GRPO = 0.926, DPO = 0.925), while GRPO further enhances PGT performance\n(SFT = 0.921 →0.934). In contrast, reinforcement learning–based optimization results in systematic\ndegradation for ICSI (0.721 →0.687 for GRPO; 0.709 for DPO), suggesting that reward-driven\nalignment may distort the decision boundaries underlying this highly constrained treatment category.\nPrompt-based guidance (ICL) offers only slight improvement in ICSI (0.721 →0.728), indicating\nthat contextual augmentation alone is insufficient to resolve the intrinsic reasoning difficulty of this\nsubgroup.\nTo probe the structural origins of these trends, we conducted a fine-grained analysis across eleven\nART subtypes using SFT and GRPO (Table 2, Figure 4). The detailed subtype results mirror the\naggregated patterns. GRPO underperforms across all ICSI-related subgroups, with the most substan-\ntial decline observed in the dominant standard ICSI subtype (−5.93%). Meanwhile, GRPO produces\n6\n"}, {"page": 7, "text": "Table 2: Fine-grained ART subtype performance.\nART Subtype\nSFT\nGRPO\nPrecision (%)\nRecall (%)\nF1 (%)\nPrecision (%)\nRecall (%)\nF1 (%)\nICSI\n57.01\n63.54\n60.10\n54.17\n54.17\n54.17\nICSI (DS)\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nICSI (FS)\n100.00\n66.67\n80.00\n50.00\n66.67\n57.14\nIVF\n79.92\n86.54\n83.10\n80.69\n91.72\n85.85\nIVF (DS)\n88.89\n76.19\n82.05\n94.12\n76.19\n84.21\nIVF+ICSI\n6.25\n6.25\n6.25\n0.00\n0.00\n0.00\nPGT-A\n95.45\n77.78\n85.71\n88.46\n82.14\n85.19\nPGT-M\n77.78\n43.75\n56.00\n1.00\n62.50\n76.92\nPGT-SR\n83.64\n93.88\n88.46\n85.19\n95.83\n90.20\nIVF (short)\n30.00\n11.84\n16.98\n46.15\n15.79\n23.53\nTESA+ICSI\n86.21\n75.76\n80.65\n85.71\n72.73\n78.69\nAll values are reported as percentages. This table compares precision, recall, and F1 scores between the SFT\nand GRPO models across eleven ART subtypes, with higher F1 values highlighted in bold. The results\nillustrate a trade-off between overall stability and targeted optimization, suggesting that reinforcement-style\nalignment may improve performance in select subgroups while reducing consistency in others.\nmarked gains in clinically complex or long-tail PGT subtypes, most notably PGT-M (+20.9%) and to\na lesser extent PGT-SR (+1.74%), demonstrating that reinforcement-style optimization can reduce\nerror modes associated with sparse or heterogeneous data distributions. A similar pattern emerges\nfor IVF subtypes: GRPO produces remarkable improvements in short-protocol IVF (+6.55%) and a\nmoderate gain in standard IVF (+2.75%). The apparent decline in ICSI with frozen sperm should\nbe interpreted cautiously due to extremely limited sample size (n = 3).\nCollectively, these findings indicate a distinct trade-off introduced by RL-based alignment. While\nGRPO enhances robustness in underrepresented or clinically intricate subgroups, it simultaneously\ncompromises stability in well-represented, tightly structured categories such as ICSI. This divergence\nunderscores a central challenge for clinical LLM alignment: optimizing for long-tail reasoning with-\nout distorting high-confidence regions of the decision space. Designing clinically reliable alignment\nstrategies therefore requires balancing global performance with local subgroup fidelity—an essential\nconsideration for decision-critical medical applications.\n3 Methods\nIn this section, we will first define the task dealt by the LLMs, then detail the dataset curation\nand model training process. Finally, we will handle the evaluation metrics. As we mentioned in the\nintroduction, we are expected to explore three main questions: the performance of SFT baseline and\nvarious post-training alignment strategies, the contribution pattern of the alignment, and the tension\nbetween algorithm alignment and clinical trust.\n3.1 Data Source & Ethic Considerations\nWe collected the electronic healthcarae records (EHRs) from West China Second University Hospital\nspanning from January 2020 to December 2022, consisting of 19800 patients’ information. The study\nwas approved by the Ethics Committee of West China Second University Hospital, Sichuan Univer-\nsity (ID: 2022288). The EHRs have been manually reviewed and corrected to ensure data accuracy.\nAll EHR data used in this study were fully de-identified before being accessed by the research team.\nThe de-identification procedure followed HIPAA Safe Harbor standards, including the removal of all\ndirect identifiers (e.g., name, date of birth, medical record number, contact information, provider\n7\n"}, {"page": 8, "text": "Fig. 4: Differences between GRPO and SFT in Subtype Analysis\na. Fine-grained ART subtype performance comparison between SFT and GRPO. Each bar shows\nthe change in F1 score (∆F1 = GRPO −SFT, %) across eleven ART subtypes, covering ICSI,\nIVF, and PGT variants. GRPO exhibits marked declines across all ICSI-related categories but\ndelivers substantial gains in several complex or long-tail subgroups, most notably PGT-M\n(+20.9%) and short-protocol IVF (+6.55%). b. ART subtype performance comparison of SFT and\nGRPO. The ICSI-with-Donor-Sperm subtype is excluded due to its single-sample size.\ninformation) and all quasi-identifiers (e.g., dates, locations, and institutional identifiers). Only aggre-\ngated clinical descriptors necessary for the reasoning task (e.g., high-level patient history, laboratory\nsummaries) were retained. Access to the de-identified dataset was restricted to authorized study\npersonnel through institution-managed credentials and encrypted storage. Reviewers performing the\nblinded evaluation accessed only the de-identified clinical vignettes and model-generated reason-\ning content through a secure, read-only interface; no downloads or re-identification attempts were\npermitted. All access was logged and monitored by an internal auditor to ensure compliance with\ninstitutional clinical data governance policies.\nAfter manually proofreading and data cleaning, a final corpus of 8201 electronic healthcare records\nwas utilized as our basic dataset with patient mean age = 31.79, s.d. = 4.63. All the confidential infor-\nmation, including patient name and code, was masked to protect the privacy. Grouped by ART, there\nare in total 11 different methods, which can be generally assigned to three ART generations: In Vitro\nFertilization (IVF), Intracytoplasmic Sperm Injection (ICSI), and Preimplantation Genetic Testing\n(PGT). Since the WHO has regulated the name of PGT and its subtypes, we convert all the outdated\nnames, such as PGD (now PGT-M), PGS (now PGT-A), to the stipulated one. Statistically, there are\n71.21% IVF (n = 5840), including 59.6% standard IVF (n = 4885), 8.17% Short Protocol IVF (Short-\ntime insemination, n = 670), 3.48% IVF with Donor Sperm (n = 285); 17.92% ICSI (n = 1470),\nincluding 11.57% standard ICSI (n = 949), 3.68% TESA+ICSI (n = 302), 1.8% IVF+ICSI (n = 155),\n0.65% ICSI with Frozen Sperm (n = 53), and 0.13% ICSI with Donor Sperm (n = 11); 10.86% PGT\n(n = 891), including 5.12% PGT-SR (n = 420), 3.76% PGT-A (n = 308), and 2.12% PGT-M (n =\n163). For COS regimen, there are 12 categories: GnRH Antagonist Fixed Protocol (Antagonist-Flex),\nLuteal Short-Acting Long Protocol (Luteal-Short), GnRH Antagonist Flexible Protocol (Antagonist-\nFixed), Follicular Phase Long-Acting Protocol (Long-Acting), Progestin-Primed Ovarian Stimulation\n8\n"}, {"page": 9, "text": "Protoco (PPOS), Clomiphene Citrate Plus Gonadotropins (CC+Gn), Mild Stimulation Protocol /\nDirect Gn Protocol (Mild/Direct Gn), Luteal Phase Stimulation Protocol (Luteal-Stim), Clomiphene\nor Letrozole Combined with Gonadotropins (CC/Letro+Gn), Conventional Ultra-Long Protoco\n(Ultra-Long), Modified Ultra-Long Protocol (Mod-ULong), Short Protocol (GnRH-a Short Protocol)\n(Short).\n3.2 Dataset Curation\n3.2.1 SFT Dataset Construction\nThe SFT dataset requires training pairs of prompt inputs and ground truth outputs. For the input\nprompt, we collated nine fields per patient, ranging from structured baseline data to unstructured\ntextual descriptions. The structured data included Female Age, Menstrual Cycle, Weight, BMI,\nAMH, FSH, and Infertility Years. The unstructured annotations included gynecology ultrasound\nreports and medical history, which typically documents prior conditions such as surgical history,\nassisted reproduction history, and male semen analysis. Since all annotations were in Chinese, we\nperformed professional translation using ChatGPT-4o. For the ground truth output, we designed a\ndouble-layer structure to ensure explainability: (1) a multi-part clinical reasoning (Chain-of-Thought,\nCoT), and (2) the final diagnosis and treatment plan. However, Manually annotating thousands of\nclinical reasoning chains was neither time-permitted nor affordable. Therefore, we generated the CoT\ncomponent using an in-context learning (ICL) approach with a diverse case boutique prompt as Few-\nshot prompt, a method we previously validated for clinical reliability[24]. This boutique set included\nsix commonplace ART cases, with sample CoTs carefully curated by two expert-level physicians\n(these cases were excluded from our basic dataset). The resulting CoTs were structured into four\naspects: Diagnosis reasoning, Assisted reproduction technology decision, Ovarian stimulation protocol\nselection, and Gn initiation dosing rationale. These four are appropriately apposite to part 2, diagnosis\n& treatment plan. In the second part, we set five final answer fields to mimic the physician’s final\ndecision-making annotation:\n• Diagnosis Fields: Infertility Type (Primary, Secondary, or Other) and Initial differential diagnosis\n.\n• Treatment Fields: ART strategy (11 subtypes), COS regimen (12 different protocols), and the\nGn starting dose,\nFor diagnosis, we have Infertility Type judgment and Initial differential diagnosis. Each case was\nlabeled with one of three infertility categories: Primary Infertility, Secondary Infertility, or Other\n(unclear or multifactorial cases). Initial diagnosis mainly focuses on the female side’s potential\ncauses, and the male side is included if applicable. For the treatment plan, we pose the ART strat-\negy, the COS regimen, and the Gn starting dose. These three vital plans build the foundation of\nthe following assisted reproduction and can be inferred through the comprehensive input informa-\ntion. The COS regimen also has 11 different protocols. In our training process, we separate the\nentire dataset into the train set (80%), the validation set (10%), and the test set (10%)\n3.2.2 Pyramid Dataset Construction\nTo further strengthen post-training alignment and improve the model’s discrimination on clinically\nambiguous or low-frequency cases, we constructed a pyramid-style alignment dataset. This dataset\nspecifically addresses two key limitations of the SFT baseline. Specifically, its bias toward dominant\ntreatment categories and its reduced robustness in rare or clinically complex scenarios. The pyramid\nconsists of three layers, each designed to provide distinct alignment signals: general enhancement,\nconfusion enhancement, and human enhancement. After SFT, we performed model infer-\nence on both training and test sets to obtain paired ground-truth and model-generated responses.\nFor DPO, which relies on pairwise preference supervision, ground-truth outputs served as preferred\nresponses, whereas SFT-generated outputs were used as rejected responses. GRPO utilized the same\n9\n"}, {"page": 10, "text": "dataset distribution but operated directly on prompt–response pairs. The full alignment dataset was\nthen assembled in a top-down manner as follows:\nHuman Enhancement\nThis layer focuses on the most clinically challenging and sparsely represented cases. Two represen-\ntative long-tail categories were selected: (1) IVF + ICSI, a hybrid of two ART generations that\naccounts for only 1.8% of SFT training data; and (2) Short-protocol IVF,a complicated variant of\nstandard IVF that is difficult even for specialists. A total of Fifty cases (25 per category) were curated\nto maximize signal clarity and expose the model to high-value reasoning patterns rarely encountered\nin routine data.\nMiddle layer: Confusion Enhancement This layer focuses on systematic confusion observed in\nthe predictions of the initial SFT model. We evaluated the outputs across both ART strategy and\nCOS regimen, using row-normalized confusion matrices (Fig5) to identify high-confusion regions. A\ntotal of 628 samples were extracted from categories with individual error rates greater than 10%.\nParticularly, poor discrimination among non-Antagonist COS regimens led to the inclusion of all\nother regimen types, allowing this layer to fully capture the blind spots of the model.\nBottom layer: General Enhancement To avoid overfitting toward rare or confusing samples, the\nbase layer includes a balanced mixture that includes all groups of cases. This bottom layer contains\n700 samples with at least one incorrect field and 300 fully correct samples across the ART strategy\nand COS regimen tasks, providing stable generalization support and preventing distributional skew\nintroduced by the upper layers.\nIn total, the alignment dataset consists of 1,678 samples distributed across the three layers, and\nis divided into 80%, 10%, and 10% for training, validation, and testing, respectively. Although the\ntop-layer samples contributes a small proportion of the dataset, their high informational density\noffset the dilution from lower layers. This allows the model to internalize complex reasoning and rare\nclinical patterns effectively. For GRPO training, we adopted the same alignment dataset to ensure a\nconsistent task distribution. However, since GRPO does not rely on pairwise preference supervision,\neach pair was processed by removing the rejected response and retaining only the ground-truth\nresponse. The same 80/10/10 train–validation–test split was applied.\n3.3 Modeling\nTo investigate how different alignment paradigms influence clinical reasoning and decision-making,\nwe compare representative strategies built on a shared backbone. Given the clinical nature of our\ntask, we opted not to directly fine-tune a general-purpose pre-trained model such as Qwen-2.5 or\nLLaMA-3. Instead, we utilized OpenBioLLM-8b[25], a domain-specific open-sourced model tailored\nfor the medical field, which has demonstrated strong performance compared to other state-of-the-art\nopen-sourced models at the time of use. OpenBioLLM is built upon LLaMA-3 and has been trained\non a range of medical knowledge sources. It has been adopted in several studies across clinical NLP\nand vision-language applications[26, 27]. (OpenBioLLM-8B). We evaluate four alignment paradigms:\nSFT, DPO, GRPO, and ICL, chosen to represent supervised, preference-based, reinforcement-based,\nand prompt-based alignment strategies. These four paradigms jointly span the major alignment\nfamilies used in contemporary LLM research, enabling us to systematically assess how algorithmic\noptimization interacts with clinical reasoning and trust.\nIn this study, we aim to obtain a LLM with specific capability in infertility diagnosis and treatment\nplanning, accompanied by the reasoning text, or Chain-of-Thought (CoT) on each aspect. We define\ninfertility diagnosis and treatment planning as a structured, multi-output reasoning problem. Given a\npatient record X = {x1, x2, . . . , xn}, which integrates both structured baseline data and unstructured\nclinical narratives (e.g., gynecology ultrasound reports and medical history), the model fθ is required\nto generate five interdependent clinical decisions and their corresponding reasoning chains. Formally,\n10\n"}, {"page": 11, "text": "Fig. 5: Cross-metric confusion analysis for COS regimen and ART prediction.\n(a) Row-normalized lower-triangle confusion matrix showing the correspondence between ground-\ntruth and model-predicted controlled ovarian stimulation (COS) regimens. (b) Row-normalized\nlower-triangle confusion matrix for treatment type predictions (IVF, ICSI, and PGT). Each cell\nrepresents the proportion of samples predicted as a given category among all samples with the same\nground-truth label, with color intensity indicating normalized frequency (blue = under-prediction,\nred = over-prediction). These matrices reveal systematic confusion patterns, particularly the\nmodel’s tendency to over-predict dominant categories such as the Antagonist regimen and IVF.\nThese patterns directly informed the construction of the pyramid alignment dataset, where\nmid-layer samples were drawn from high-confusion regions to enhance discriminative robustness.\nthe task can be expressed as learning a mapping function:\nfθ : X →(Y1, Y2, Y3, Y4, Y5, C),\nwhere (Y1, Y2, Y3, Y4, Y5) represent five structured outputs:\n(1) infertility type judgment, (2) initial diagnosis, (3) assisted reproductive technology (ART)\nstrategy, (4) controlled ovarian stimulation (COS) regimen, (5) gonadotropin (Gn) starting dose.\nC denotes the chain-of-thought (CoT) reasoning text that explicitly supports and explains each\ndecision. The learning objective of the model is to maximize the joint likelihood of generating both\nthe reasoning process and the final structured decisions conditioned on the patient data:\nL(θ) =\nX\n(X,Y1,...,Y5,C)∈D\nlog Pθ(Y1, . . . , Y5, C | X).\nIn practice, this formulation provides a unified objective for multiple alignment paradigms, including\nSFT, DPO, and GRPO. It ensures that the model learns not only to produce correct clinical outcomes,\nbut also to generate transparent and clinically faithful reasoning chains. Based on this objective\nfunction, we trained the models and evaluated their performance through a dual evaluation pipeline\nthat incorporated both algorithmic metrics and human expert feedback. Figure ?? illustrates the\nwhole process.\nSFT\n11\n"}, {"page": 12, "text": "This stage serves as the foundation for all subsequent alignment strategies, providing a clinically\ncoherent policy network pretrained with structured reasoning supervision. In the Supervised Fine-\nTuning (SFT) stage, we train the base model on the entire training set, where each instance consists of\na structured clinical description and a corresponding expert-annotated response including reasoning\nand diagnosis, as laid out in data section. Training was performed with LoRA (learning rate = 3e-5,\nbatch size = 4) for 10 epochs on a single A100 GPU. The resulting model serves as the reference\npolicy for the subsequent DPO, GRPO, and ICL stages.\nDPO\nThis variant focuses on aligning model reasoning with expert preferences by directly optimizing\ntoken-level log-likelihood contrast between preferred and dispreferred responses. Direct Preference\nOptimization (DPO) is a critic-free reinforcement learning method that has recently emerged from the\nRLHF (Reinforcement Learning from Human Feedback) paradigm[2] It provides a relatively simple\nand stable alternative to reward-model-based offline approaches for aligning language models with\nhuman preferences and has shown promising results in reducing undesirable behaviors in baseline\nmodels. Unlike traditional RLHF methods that rely on learning a separate reward model or value\nfunction, DPO directly optimizes the policy by contrasting preferred and dispreferred responses.\nSpecifically, each training data point consists of a prompt x, a preferred response yw , and a less-\npreferred response yl . Given the prompt x , the DPO loss encourages the policy model to assign\na higher likelihood to yw over yl, thereby aligning the model outputs more closely with human\npreferences. The DPO loss function is formulated as:\nLDPO(πθ; πref) = −E(x,yw,yl)∼D\n\u0014\nlog σ\n\u0012\nβ\n\u0014\nlog πθ(yw | x)\nπref(yw | x) −log πθ(yl | x)\nπref(yl | x)\n\u0015\u0013\u0015\n(1)\nWhere πθ is the current policy model, σ denotes the sigmoid function, and β is a temperature\nparameter controlling the sharpness of the preference. Notably, this loss assigns equal weight to all\ntokens in each response, which is precisely what we aim to achieve—aligning the entire reasoning\nprocess and final diagnosis to the human level. Since we have access to the correct final answer, this\nsetup encourages the model not only to predict the correct outcome but also to generate a coherent\nand clinically sound chain of thought leading to it. In the context of our study, where both the\nintermediate reasoning (e.g., identifying relevant clinical clues) and the final decision (e.g., treatment\nplan) are critical, such token-level uniform supervision helps ensure that the model learns to reflect\nexpert-like logic across the entire response, rather than merely optimizing for the final output token.\nIn our experiments, we use the SFT model described in Section 2.3 as the reference policy. DPO was\nalso conducted using an A100 with beta = 0.3, learning rate = 3e-7, batch size = 8 for 1 epoch.\nGRPO\nThis variant leverages reinforcement-based relative optimization to enhance decision consistency\nwhile eliminating the computational overhead of value-function training. Group Relative Policy Opti-\nmization (GRPO) is an evolutionary variant of Proximal Policy Optimization (PPO)[28]. For PPO,\nits advantage is computed by applying Generalized Advantage Estimate (GAE)[29], based on the\nrewards and a learned value function. Consequently, a value function needs to be trained alongside\nthe policy model and using a per-token KL penalty from the reference model to mitigate over-\noptimization of the reward model. As the value function is involved in the training, it is typically\nanother model of comparable size to the policy model, bringing a gargantuan computational burden.\nWhile for GRPO, it obviates an additional value function for advantage computation and instead\nuses the reward average of multiple outputs sampled from the same question as the baseline. More\nspecifically, for each question q, GRPO samples a group of outputs {o1, o2, · · · , oG} from the old\n12\n"}, {"page": 13, "text": "policy πθ and then optimizes the policy model by maximizing the following objective:\nJGRPO(θ) = Eq∼P (Q),{oi}G\ni=1∼πθold(O|q)\n\"\n1\nG\nG\nX\ni=1\n1\n|oi|\n|oi|\nX\nt=1\nn\nmin\nh πθ(oi,t | q, oi,<t)\nπθold(oi,t | q, oi,<t)\nˆAi,t,\nclip\n\u0010 πθ(oi,t | q, oi,<t)\nπθold(oi,t | q, oi,<t), 1 −ϵ, 1 + ϵ\n\u0011\nˆAi,t\ni\n−βDKL[πθ ∥πref]\no#\n(2)\nwhere ϵ and β are hyperparameters, and ˆAi,t is the advantage calculated based on relative rewards\nof the outputs inside each group only, which will be detailed in the following subsections. GRPO\noffers a compelling solution for medical domains where training a value function is often impractical\ndue to limited data, and where structured, interpretable supervision is essential for aligning model\nbehavior with expert expectations.\nReward design follows the common practices[4] by using the accuracy reward, aimed at achieving\nthe ”algorithm alignment”. Since we have four structured final answer fields, except for the initial\ndiagnosis which comprises long sentences, we used a combined final reward to aggregate four separate\nrewards:\nri = λITrIT\ni\n+ λCOSrCOS\ni\n+ λGnrGn\ni\n+ λARTrART\ni\n.\nThe weight for each field was set to:\nλIT = 0.2,\nλCOS = 0.3,\nλGn = 0.2,\nλART = 0.3.\nThe weighting coefficients were determined empirically based on the relative clinical importance and\ninformation density of each decision field, with higher weights assigned to COS and ART strategies\nthat directly affect treatment outcomes. Additionally, as the Gn dose is a numerical answer, to\nevaluate the predicted Gn starting dose, we designed a graded reward function that reflects real-\nworld clinical practices. In clinical ovarian stimulation protocols, the Gn dose is typically adjusted\nin increments of 25 IU, making a deviation of ≤25 IU clinically acceptable. Therefore, we assign:\nrGn =\n\n\n\n\n\n\n\n\n\n1.0,\nif |ˆy −y| ≤25,\n0.5,\nif 26 ≤|ˆy −y| ≤50,\n0.0,\notherwise.\nThis tiered reward encourages the model to approximate the clinically preferred range, even if the\nexact value is not matched, and reflects the tolerance physicians often exhibit during dose adjustment\nin practice.\nICL - Guideline-Based Prompt Alignment\nThis variant enhances reasoning generalization through guideline-based prompts. In-context learn-\ning (ICL) is a rapid, training-free post-alignment approach commonly applied to general-purpose\npretrained models. Unlike conventional few-shot ICL, which relies on instance-level exemplars, our\nvariant repurposes clinical heuristic maps into textual guidance blocks. These guidelines are distilled\nfrom physicians’ decision flowcharts, translating structured expert reasoning paths (e.g., stimulation\nprotocol selection, ART strategy prioritization) into natural-language rules embedded within the\nprompt. This design transforms symbolic medical knowledge into interpretable prompt-based super-\nvision, allowing the model to internalize domain heuristics without additional parameter updates.\nIn this variant, We proposed two guidance on ART strategy and COS regimen. The SFT model\nis retained as the inference backbone because of its domain-specific alignment, particularly its pro-\nficiency in maintaining output format consistency and structured clinical reasoning. For each test\ncase, two such guideline blocks are inserted into the prompt (examples provided in the Supplemental\nFile). Additionally, we prepend the instruction:\n13\n"}, {"page": 14, "text": "“DO NOT USE THE GUIDELINE UNLESS YOU ARE NOT SURE ABOUT YOUR\nANSWER.”\nThis design encourages the model to rely primarily on its internalized reasoning ability and consult the\nguideline only as a fallback when uncertain. In this way, the instructional ICL serves as a lightweight\nand interpretable alternative to gradient-based alignment, effectively simulating real-world physician\nbehavior—where decisions are primarily experience-driven, with reference to protocols only when\nambiguity arises.\n3.4 Dual-Evaluation Protocol\nWe employed a dual-layer evaluation framework to assess both quantitative task performance and\nqualitative clinical quality. This framework integrates (1) automatic field-level metrics and (2)\ntriple-blind doctor-in-the-loop assessment. This design ensures comprehensive examination of both\nalgorithmic accuracy and clinician-perceived interpretability.\n3.4.1 Automatic Evaluation\nModel outputs were evaluated using standard metrics for structured clinical fields. Specifically, we\ncompute Accuracy and Macro-F1 for categorical predictions, including Infertility Type, ART Strat-\negy, and COS Regimen. Because the initial diagnosis field exhibits high variability and paraphrasing,\nexact word-by-word matching is infeasible and unreliable. Therefore, an auxiliary large language\nmodel was used as a semantic judge to determine whether the generated diagnosis semantically\nentailed the ground-truth diagnosis. We report the precision of containing at least one true diag-\nnosis or all of the ground truth. For the numerical field of Gn starting dose, prediction error was\nquantified using Mean Absolute Error (MAE). Together, these metrics assess the model’s structured\ndecision-making performance but do not capture the quality of its reasoning process.\n3.4.2 Doctor-in-the-loop Evaluation\nTo evaluate clinical reasoning and decision-making quality beyond quantitative metrics, we conducted\na domain-expert assessment. This evaluation addressed two core questions:\n(1) Does post-training alignment improve reasoning coherence and clinical plausibility?\n(2) Can aligned models approximate expert-level performance?\nExperts rated each output along four clinically grounded dimensions:\n• Clinical Reasoning Capability: whether the model follows a medically sound thought process;\n• Diagnosis Accuracy: whether the inferred Infertility type and Initial diagnostic are correct or\ncontain false positive predictions;\n• Treatment Feasibility: whether the entire recommended treatment plan is medically appropriate\nand consistent with clinical practice;\n• Hallucination: whether the reasoning process contains irrelative or fabricated information.\nThese dimensions cover all the blocks in the response and can be seen as a comprehensive subjective\nevaluation. Each output is scored on a 5-point Likert scale (1 = poor, 5 = excellent) according to\nthe rubrics defined above. For each evaluation case, we compare the outputs of the SFT model and\nthe best-performing post-alignment model. Additionally, to further benchmark model performance\nagainst clinical standards, we include a third response generated from the ground truth answer, rep-\nresenting the expert-level response. The graders are then asked to select the overall best response\namong the three. All responses are blindly evaluated: the graders are unaware of the matching rela-\ntionship, thereby reducing bias. Each case is independently reviewed by a panel of board-certified\nphysicians with experience in reproductive medicine. Considering the limited time available for physi-\ncians, we selected 100 representative cases as the evaluation set, whose category distribution was\nkept consistent with that of the test set.\n14\n"}, {"page": 15, "text": "3.5 Statistical information\nFor automatic evaluation, we reported Accuracy, macro-F1, and Mean Absolute Error (MAE) across\nall structured decision fields. Accuracy and macro-F1 were computed for categorical outputs (e.g.,\ninfertility type, ART strategy, and COS regimen), while MAE was used to quantify the deviation of\nnumerical predictions (e.g., gonadotropin starting dose) from the ground truth. These metrics were\ncalculated on the held-out test set and compared across alignment strategies (SFT, DPO, GRPO,\nand ICL) without inferential testing, as they were derived from deterministic model outputs. For\nphysician evaluations, ratings on three clinical dimensions: Diagnosis Accuracy, Clinical Reasoning\nCapability, and Treatment Plan Feasibility, were averaged per case across reviewers. Paired within-\ncase comparisons between the SFT and GRPO models were assessed using two-tailed paired t-tests.\nFor dimensions where normality could not be assumed. All statistical tests were two-sided with\na significance threshold of p ¡ 0.05. To account for multiple hypothesis testing across evaluation\ndimensions, p-values were adjusted using the Benjamini–Hochberg false discovery rate (FDR) pro-\ncedure. All analyses were conducted in Python 3.11 using pandas, NumPy, and SciPy. The paired\nhuman-evaluation sample consisted of n = 100 cases.\n4 Discussion\nOur findings reveal a striking divergence between algorithmic alignment and clinical alignment in\ndomain-specific medical LLMs.Automatic accuracy across key answer fields indicated that GRPO\nconsistently wined SFT, physician blinded evaluations revealed the opposite pattern. The responses\nfrom SFT were rated significantly higher in reasoning quality and treatment feasibility, with no\nsignificant differences in diagnostic accuracy. These results demonstrate that benchmark gains do\nnot necessarily translate into higher clinical trust, and highlight a fundamental limitation of current\npost-training alignment paradigms. Though this divergence exists, both SFT and GRPO were still\npreferred over the human-level responses in the blinded comparison, illustrating the potential of\nLLMs as reliable building blocks for clinical decision-support systems.\nWe hypothesize that this reverse pattern between superior benchmark scores and superior clinical\nratings can be traced to the intrinsic design of the reward function and the optimization dynamics\nof reinforcement learning. GRPO’s policy is optimized using an answer-centric reward in our imple-\nmentation, which encourages the model to converge rapidly toward the correct final output with the\nhighest reward scores. This pattern to some extent neglects the process-level reasoning structure that\nleads to the final answer. As a result, the model tends to produce statistically correct yet clinically\nplausible answers, which means precise in final answers yet often lacking the stepwise justification\nphysicians expect. Because the intermediate logical consistency is neither penalized nor rewarded dur-\ning the alignment, it may drift as a byproduct of the reward updating. The evidence of no significant\ndifferences in diagnostic accuracy between two models can also empower the illustration since the\ninitial diagnosis is not covered by the custom reward function in GRPO. In contrast, the SFT model\ndirectly mimics the expert natural narrative flow and reasoning steps revealed in the training data.\nThe intention of imitation aligns more closely with physicians’ thinking style, leading to higher scores\nin clinical ratings. Furthermore, reinforcement learning can induce reward overoptimization, where\nthe policy becomes narrowly tuned to reward signals and gradually diverges from the broader rea-\nsoning patterns established during supervised fine-tuning[30, 31]. This distribution drift may explain\nwhy treatment plans in GRPO responses are sometimes judged as less feasible or less holistic, even\nwhen the sole answer is correct. In addition, GRPO generated fewer hallucinations than SFT, likely\nbecause continued reward-driven optimization strengthened its adherence to reference data. Both\nmodels also outperformed the human reference in the blind winning-rate comparison, indicating\nthat well-aligned LLMs can sometimes produce decisions viewed as more consistent or reliable than\nindividual clinicians. Nonetheless, SFT achieved the highest winning rate overall, reinforcing that\nimitation-based alignment remains more clinically aligned. Collectively, this gap underscores why\n15\n"}, {"page": 16, "text": "accuracy-oriented benchmarks alone cannot capture true clinical alignment, especially in complex,\nmulti-dimensional decision systems like infertility treatment, where many steps are interdependent\nand difficult to verify. Achieving meaningful clinical alignment thus requires balancing performance\nwith interpretability, potentially by incorporating reasoning-oriented reward design or structured\nfeedback from physicians during training.\nBeyond the alignment paradox observed in the doctor-in-the-loop evaluation, the field-level results\nfurther clarify how different alignment strategies influence specific components of clinical decision-\nmaking. GRPO achieves the best on overall average accuracy and macro F1 scores in terms of the\nmain field: infertility type, COS regimen, and ART strategy. This superior performance is consistent\nwith its outcome-driven reinforcement mechanism. Its improvements in ART strategy and initial\ndiagnosis suggest that reward-based optimization is particularly effective when the decision space is\ndiscrete, well-structured, and strongly tied to final outcome correctness. Separately, four models all\nachieve an accuracy higher than 90% on Infertility type classification and have no dramatic differ-\nence. This demonstrates that the capability of the basic classification problem has been clearly solved\nand is hard to improve significantly. However, these gains are not uniform. COS regimen prediction\nremains challenging for all four models, with none outperforming the SFT baseline. According to the\nphysicians’ experience, the COS regimen is more flexible compared to the ART. The regimen choice\nmay vary depending on the physician preference, hormonal response, and cycle history. Because there\nmay be multiple clinically valid regimens for the same patient, an answer-level reward can inadver-\ntently penalize reasonable alternatives, limiting the effectiveness of RL optimization in this field. In\ncontrast, ART selection benefits from clearer categorical boundaries, enabling GRPO to form more\nstable reward gradients. These declines reveal the sensitivity of post-training alignment when deal-\ning with complex or ambiguous bounded decisions. The initial diagnosis results further reinforce this\npattern. GRPO achieves the highest partial and exact match rates, whereas DPO collapses dramat-\nically. This instability is consistent with preference-learning dynamics: long-form reasoning dilutes\nthe token-level preference signal, causing DPO to converge to over-simplified dominant patterns and\nlose distributional diversity. By comparison, SFT and ICL preserve more narrative structure and\nmaintain moderate diagnosis performance, although neither improves substantially. Finally, the MAE\nfor Gn dose remains similar across models, indicating that numerical prediction is less sensitive to\nalignment strategy and likely driven by training data distribution rather than post-training optimiza-\ntion. Together, these results show that reward-driven reinforcement alignment excels in well-defined,\noutcome-anchored tasks but struggles in flexible or multi-solution domains, whereas SFT preserves\nreasoning diversity and narrative fidelity at the cost of slightly lower token-level accuracy. These\ncomplementary strengths and weaknesses help explain both the doctor-evaluated preference for SFT\nand the metric-level superiority of GRPO.\nA fine-grained breakdown across eleven ART subtypes showed that GRPO substantially improves\nperformance in rare or complex categories (e.g., PGT-M, PGT-SR, Short-protocol IVF), while mod-\nerately decreasing performance in ICSI-related cases. This suggests that reinforcement alignment\nreinforces categories with consistent and well-separated reward signals, but struggles in clinically\nambiguous or overlapping decision regions. ICSI lies between IVF (high frequency) and PGT (rare\nbut distinct), receiving neither strong statistical reinforcement nor concentrated reward signals.\nThe reward boundaries for ICSI are inherently fuzzier, limiting gradient stability during optimiza-\ntion. The failure to improve hybrid IVF+ICSI cases further supports the notion that when subtype\ndefinitions are clinically mixed, reward-based learning cannot clearly separate decision manifolds.\nThus, GRPO amplifies structured decision pathways while dampening mid-range, ambiguous ones,\nan insight crucial for designing future medical reinforcement learning objectives. Nonetheless, these\nsubtype-level findings align closely with the design of our pyramid-curated alignment dataset. The\ntop “human-enhancement” layer and the mid-layer “confusion-enhancement” samples were intention-\nally enriched with rare, complex, and clinically ambiguous scenarios—particularly PGT-M, PGT-SR,\nshort-protocol IVF, and other non-dominant regimens. This targeted data distribution provided\n16\n"}, {"page": 17, "text": "GRPO with concentrated, high-signal reward guidance in precisely those long-tail regions, enabling\nthe model to internalize clearer reasoning boundaries and progressively correct SFT’s systematic\nerrors. In contrast, ICSI and its related subtypes are positioned in the middle and bottom of the\npyramid hierarchy and characterized by overlapping clinical indications. In this case, these subgroups\nreceived weaker and diluted reward signals. As a result, GRPO optimized strongly on top- and mid-\nlayer rare cases but did not achieve stable reinforcement for the more ambiguous ICSI spectrum.\nThis explains the asymmetric improvement pattern observed across subtypes and highlights the\nimportance of hierarchical data construction when aligning medical LLMs for fine-grained decision\nreasoning.\nBoth DPO and ICL exhibited inherent weaknesses that limit their usefulness in infertility treat-\nment, or complex clinical reasoning. DPO’s pairwise preference optimization depends on token-level\ncontrasts between preferred and rejected answers. The key signals are diluted in long-form reason-\ning, by a large portion of identical texts between pairs. This leads to instability and collapse in\ndiagnosis-related tasks. ICL improves fluency but lacks adaptive correction mechanisms, making it\ninsufficient for multi-step clinical planning. Together, these limitations highlight that preference- or\nprompt-based alignment alone cannot support reliable clinical reasoning, underscoring the need for\nalignment objectives that explicitly integrate both outcome correctness and reasoning fidelity.\nDespite its strengths, this study has several technical and methodological limitations that warrant\ncareful interpretation. First, the model’s reasoning dataset relies partially on ICL-generated CoTs\nrather than fully human-annotated rationales. Although our boutique prompt was curated by spe-\ncialists, and its generating reliability was proved to be better than normal prompt, the resulting CoTs\nmay encode stylistic biases or propagate inaccuracies from the teacher model. This could affect the\nfidelity of downstream alignment, particularly in diagnosis-related tasks that require nuanced causal\nreasoning. Second, while our pyramid dataset effectively amplifies long-tail and confusing cases, its\nconstruction is still bounded by the error landscape of the SFT model. If the SFT model exhibits sys-\ntematic blind spots (e.g., under-specification of certain COS regimens or mid-frequency ICSI cases),\nthese blind spots may propagate into the mid-layer “confusion enhancement” sampling, leading to\nbiased reward allocation during RL alignment. In addition, the EHRs were all collected from a sin-\ngle institution. Although it provides service to a huge population, it could contain demographic\nand institutional bias. Other than the data constraints, our evaluation focuses on small open-source\nmedical LLMs due to compute constraints. Smaller models are more sensitive to misaligned reward\ngradients and may underrepresent scaling effects, particularly whether larger models would exhibit\nthe same alignment paradox between benchmark accuracy and clinician trust. The doctor-in-the-\nloop evaluation, though triple-blind and domain-expert–driven, includes 100 cases. While statistically\nrepresentative of the ART distribution, this size limits subgroup-level analyses (e.g., rare ICSI vari-\nants, short protocol IVF), and may underpower the evaluation of subtle reasoning differences, though\nwe conduct effect sizes computation. Lastly, ground-truth diagnoses and treatment plans represent\n“single-center ground truth,” which reflects one institution’s practice style. Variability in stimu-\nlation preferences, dose selection philosophies, or PGT indications across centers may reduce the\ngeneralizability of the aligned model.\nFuture work should prioritize developing clinically grounded reward engineering, especially by incor-\nporating structured ART guidelines to penalize inconsistent reasoning steps, unsupported causal\njumps, and deviations from established treatment pathways, thereby reinforcing fidelity in multi-step\nclinical thought processes. To improve generalizability, future studies should adopt multi-center and\nmulti-style learning, expanding beyond a single institution to capture diverse treatment philosophies\nand quantify inter-center variability that may shape ART practice. Another critical direction is con-\nducting scaling experiments with mid-size and large medical LLMs ( ¿ 30B parameters) to determine\nwhether the alignment paradox persists at larger scales, as bigger models may encode richer clinical\npriors and become less sensitive to reward-induced distribution drift. Additionally, improving rea-\nsoning supervision will require human-validated CoT distillation, where a small but expert-curated\n17\n"}, {"page": 18, "text": "set of physician-reviewed reasoning chains anchors the logical space and is augmented through LLM-\ndriven self-improvement or targeted red-teaming. Future evaluation frameworks should also move\nbeyond accuracy to include fine-grained clinical trade-offs, such as stimulation risk profiles, embryo\nyield expectations, dosing safety margins, and adherence to OHSS-prevention heuristics, allowing a\nmore operationally meaningful assessment of model decisions. Finally, given the rapid evolution of\nART norms, future work should examine temporal generalization and policy-shift detection, assessing\nwhether aligned models can recognize and adapt to changes in clinical practice—such as shifting COS\npreferences, increasing PGT utilization, or evolving dosing philosophies—and whether alignment\namplifies or suppresses outdated patterns.\n5 Conclusion\nIn this work, we provide the first systematic examination of how post-training alignment strate-\ngies—SFT, DPO, GRPO, and ICL—shape the clinical behavior of medical LLMs in a real-world,\nmulti-stage decision environment. Across both automatic metrics and triple-blind physician evalua-\ntions, our results reveal a consistent pattern: reinforcement-based optimization improves outcome-\nlevel correctness but does not necessarily enhance, and may even erode, clinician-perceived reasoning\nquality and treatment feasibility. This divergence, which we term the alignment paradox, highlights\na structural gap between algorithmic optimization and clinical trust.\nOur fine-grained analyses further show that GRPO reallocates learning capacity along the ART\nhierarchy: it strengthens reasoning in complex and long-tail subgroups, particularly PGT and short-\nprotocol IVF, yet underperforms in mid-frequency, clinically ambiguous categories such as ICSI.\nThese dynamics reflect the sensitivity of reward-based alignment to data distribution and reward\ngranularity, and underscore the need for carefully designed supervisory signals in medical domains.\nThrough the introduction of a pyramid-curated dataset and a dual-evaluation framework, we\ndemonstrate that both SFT and GRPO can outperform human practitioners in case-level deci-\nsion consistency, but excel in fundamentally different ways, SFT through reasoning transparency\nand GRPO through outcome-driven precision. Taken together, our findings emphasize that building\nclinically reliable medical LLMs requires more than maximizing benchmark accuracy: it demands\nalignment strategies that integrate structured clinical knowledge, preserve reasoning faithfulness,\nand center the evaluative criteria of physicians. As the field advances toward real-world deploy-\nment, bridging this gap between algorithmic alignment and clinical alignment will be essential for\ndeveloping trustworthy, interpretable, and safe AI systems in reproductive medicine and beyond.\n18\n"}, {"page": 19, "text": "Declarations\n5.1 Funding\nPart of this study was supported funded by the Science and Technology Department of Sichuan\nProvince Project (2024YFFK0365); Part of this study was supported funded by the Natural Science\nFoundation of Sichuan, China (2025NSFSC1985); Part of this study was supported by the 1·3·5\nproject for disciplines of excellence, West China Hospital, Sichuan University (ZYYC21004).\n5.2 Conflict of interest\nNone declare\n5.3 Ethics approval\nA set of selected masked EHRs recorded between 2020 and 2022 in the Infertility Outpatient Depart-\nment at West China Second University Hospital was considered in this study. The study was approved\nby the Ethics Committee of West China Second University Hospital, Sichuan University (ID: 2022288)\n5.4 Data availability\nThe datasets generated during and/or analyzed during this study are not publicly available due\nto institutional case privacy and a large number of interaction dialogs, but are available from the\ncorresponding author on reasonable request. The authors will make the Author Accepted Manuscript\n(AAM) version available under a CC BY public copyright license.\n19\n"}, {"page": 20, "text": "References\n[1] DeepSeek-AI et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement\nLearning (2025). URL http://arxiv.org/abs/2501.12948. ArXiv:2501.12948 [cs].\n[2] Rafailov,\nR.\net\nal.\nDirect\nPreference\nOptimization:\nYour\nLanguage\nModel\nis\nSecretly a Reward Model.\nAdvances in Neural Information Processing Systems\n36,\n53728–53741\n(2023).\nURL\nhttps://proceedings.neurips.cc/paper files/paper/2023/hash/\na85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html.\n[3] Lai, Y., Zhong, J., Li, M., Zhao, S. & Yang, X. Med-R1: Reinforcement Learning for General-\nizable Medical Reasoning in Vision-Language Models (2025). URL http://arxiv.org/abs/2503.\n13939. ArXiv:2503.13939 [cs].\n[4] Shao, Z. et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language\nModels (2024). URL http://arxiv.org/abs/2402.03300. ArXiv:2402.03300 [cs].\n[5] Dao, A. & Vu, D. B. AlphaMaze: Enhancing Large Language Models’ Spatial Intelligence via\nGRPO (2025). URL http://arxiv.org/abs/2502.14669. ArXiv:2502.14669 [cs].\n[6] Chen, J. et al. HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs (2024). URL\nhttp://arxiv.org/abs/2412.18925. ArXiv:2412.18925 [cs].\n[7] Singhal, K. et al.\nToward expert-level medical question answering with large language\nmodels.\nNature Medicine 31, 943–950 (2025).\nURL https://www.nature.com/articles/\ns41591-024-03423-7. Publisher: Nature Publishing Group.\n[8] Team, L. et al.\nLingshu: A Generalist Foundation Model for Unified Multimodal Medical\nUnderstanding and Reasoning (2025). URL http://arxiv.org/abs/2506.07044. ArXiv:2506.07044\n[cs].\n[9] Gaber, F. et al. Evaluating large language model workflows in clinical decision support for triage\nand referral and diagnosis. npj Digital Medicine 8, 263 (2025). URL https://www.nature.com/\narticles/s41746-025-01684-1. Publisher: Nature Publishing Group.\n[10] Yang, R. et al. Large language models in health care: Development, applications, and challenges.\nHealth Care Science 2, 255–263 (2023). URL https://onlinelibrary.wiley.com/doi/abs/10.1002/\nhcs2.61.\neprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hcs2.61.\n[11] Hager, P. et al. Evaluation and mitigation of the limitations of large language models in clin-\nical decision-making. Nature Medicine 30, 2613–2622 (2024). URL https://www.nature.com/\narticles/s41591-024-03097-1. Publisher: Nature Publishing Group.\n[12] Liu, J. et al. Medchain: Bridging the Gap Between LLM Agents and Clinical Practice with\nInteractive Sequence (2025). URL http://arxiv.org/abs/2412.01605. ArXiv:2412.01605 [cs].\n[13] Ullah, E., Parwani, A., Baig, M. M. & Singh, R. Challenges and barriers of using large language\nmodels (LLM) such as ChatGPT for diagnostic medicine with a focus on digital pathology –\na recent scoping review. Diagnostic Pathology 19, 43 (2024). URL https://doi.org/10.1186/\ns13000-024-01464-7.\n[14] Dwivedi, R. et al. Explainable AI (XAI): Core Ideas, Techniques, and Solutions. ACM Comput.\nSurv. 55, 194:1–194:33 (2023). URL https://dl.acm.org/doi/10.1145/3561048.\n20\n"}, {"page": 21, "text": "[15] Evans, T. et al. The explainability paradox: Challenges for xAI in digital pathology. Future Gen-\neration Computer Systems 133, 281–296 (2022). URL https://www.sciencedirect.com/science/\narticle/pii/S0167739X22000838.\n[16] Mart´ınez-Ag¨uero, S. et al. Interpretable clinical time-series modeling with intelligent feature\nselection for early prediction of antimicrobial multidrug resistance. Future Generation Com-\nputer Systems 133, 68–83 (2022).\nURL https://www.sciencedirect.com/science/article/pii/\nS0167739X22000644.\n[17] Wei, J. et al. Finetuned Language Models Are Zero-Shot Learners (2022). URL http://arxiv.\norg/abs/2109.01652. ArXiv:2109.01652 [cs].\n[18] Wang, Y., Yue, X. & Chen, W. Critique Fine-Tuning: Learning to Critique is More Effective\nthan Learning to Imitate (2025). URL http://arxiv.org/abs/2501.17703. ArXiv:2501.17703 [cs].\n[19] Dai, W., Chen, P., Ekbote, C. & Liang, P. P. QoQ-Med: Building Multimodal Clinical Founda-\ntion Models with Domain-Aware GRPO Training (2025). URL http://arxiv.org/abs/2506.00711.\nArXiv:2506.00711 [cs].\n[20] Organization, W. H. Infertility prevalence estimates, 1990–2021 (World Health Organization,\n2023). Google-Books-ID: JnwOEQAAQBAJ.\n[21] Graham,\nM.\nE.\net\nal.\nAssisted\nreproductive\ntechnology:\nShort-\nand\nlong-\nterm\noutcomes.\nDevelopmental\nMedicine\n&\nChild\nNeurology\n65,\n38–49\n(2023).\nURL\nhttps://onlinelibrary.wiley.com/doi/abs/10.1111/dmcn.15332.\neprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/dmcn.15332.\n[22] Kumar, P., Sait, S. F., Sharma, A. & Kumar, M. Ovarian hyperstimulation syndrome. Journal\nof Human Reproductive Sciences 4, 70 (2011). URL https://journals.lww.com/jhrs/fulltext/\n2011/04020/Ovarian hyperstimulation syndrome.2.aspx.\n[23] Zheng, B., Kwok, E., Taljaard, M., Nemnom, M.-J. & Stiell, I.\nDecision fatigue in the\nEmergency Department: How does emergency physician decision making change over an eight-\nhour shift?\nThe American Journal of Emergency Medicine 38, 2506–2510 (2020).\nURL\nhttps://www.sciencedirect.com/science/article/pii/S0735675719308265.\n[24] Liu, D. et al. Reliability of Large Language Model Generated Clinical Reasoning in Assisted\nReproductive Technology: Blinded Comparative Evaluation Study (2025). URL http://arxiv.\norg/abs/2510.16095. ArXiv:2510.16095 [cs].\n[25] aaditya/Llama3-OpenBioLLM-8B · Hugging Face.\nURL https://huggingface.co/aaditya/\nLlama3-OpenBioLLM-8B.\n[26] Shi, J., Yuan, Y., Wang, A. & Nie, M. Fine-Tuning a Personalized OpenBioLLM Using Offline\nReinforcement Learning. | EBSCOhost (2025). URL https://openurl.ebsco.com/contentitem/\ndoi:10.3390%2Fapp15052486?sid=ebsco:plink:crawler&id=ebsco:doi:10.3390%2Fapp15052486.\nISSN: 2076-3417 Issue: 5 Pages: 2486 Volume: 15.\n[27] AlShibli, A., Bazi, Y., Rahhal, M. M. A. & Zuair, M. Vision-BioLLM: Large vision language\nmodel for visual dialogue in biomedical imagery. Biomedical Signal Processing and Control 103,\n107437 (2025). URL https://www.sciencedirect.com/science/article/pii/S1746809424014952.\n[28] Schulman, J., Wolski, F., Dhariwal, P., Radford, A. & Klimov, O. Proximal Policy Optimization\nAlgorithms (2017). URL http://arxiv.org/abs/1707.06347. ArXiv:1707.06347 [cs].\n21\n"}, {"page": 22, "text": "[29] Schulman, J., Moritz, P., Levine, S., Jordan, M. & Abbeel, P. High-Dimensional Continuous Con-\ntrol Using Generalized Advantage Estimation (2018). URL http://arxiv.org/abs/1506.02438.\nArXiv:1506.02438 [cs].\n[30] Gao, L., Schulman, J. & Hilton, J. Scaling Laws for Reward Model Overoptimization (2023).\nURL https://openreview.net/forum?id=bBLjms8nZE.\n[31] Coste, T., Anwar, U., Kirk, R. & Krueger, D.\nReward Model Ensembles Help Mitigate\nOveroptimization (2024). URL http://arxiv.org/abs/2310.02743. ArXiv:2310.02743 [cs].\n22\n"}]}