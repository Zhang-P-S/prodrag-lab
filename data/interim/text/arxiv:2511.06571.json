{"doc_id": "arxiv:2511.06571", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.06571.pdf", "meta": {"doc_id": "arxiv:2511.06571", "source": "arxiv", "arxiv_id": "2511.06571", "title": "Rep2Text: Decoding Full Text from a Single LLM Token Representation", "authors": ["Haiyan Zhao", "Zirui He", "Fan Yang", "Ali Payani", "Mengnan Du"], "published": "2025-11-09T23:18:36Z", "updated": "2025-11-09T23:18:36Z", "summary": "Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.06571v1", "url_pdf": "https://arxiv.org/pdf/2511.06571.pdf", "meta_path": "data/raw/arxiv/meta/2511.06571.json", "sha256": "8d1a673afb77c449b51ec099f17c943c36d1d40103e04b3c74dcbc535c9775c1", "status": "ok", "fetched_at": "2026-02-18T02:28:04.030209+00:00"}, "pages": [{"page": 1, "text": "Rep2Text: Decoding Full Text from a Single LLM Token Representation\nHaiyan Zhao1\nZirui He1\nFan Yang2\nAli Payani3\nMengnan Du1†\n1New Jersey Institute of Technology\n2Wake Forest University\n3Cisco Research\n{hz54,zh296,mengnan.du}@njit.edu\nyangfan@wfu.edu\napayani@cisco.com\n†Corresponding author\nAbstract\nLarge language models (LLMs) have achieved\nremarkable progress across diverse tasks,\nyet their internal mechanisms remain largely\nopaque. In this work, we address a fundamen-\ntal question: to what extent can the original\ninput text be recovered from a single last-token\nrepresentation within an LLM? We propose\nRep2Text, a novel framework for decoding full\ntext from last-token representations. Rep2Text\nemploys a trainable adapter that projects a tar-\nget model’s internal representations into the em-\nbedding space of a decoding language model,\nwhich then autoregressively reconstructs the\ninput text. Experiments on various model com-\nbinations (Llama-3.1-8B, Gemma-7B, Mistral-\n7B-v0.1, Llama-3.2-3B) demonstrate that, on\naverage, over half of the information in 16-\ntoken sequences can be recovered from this\ncompressed representation while maintaining\nstrong semantic integrity and coherence. Fur-\nthermore, our analysis reveals an information\nbottleneck effect: longer sequences exhibit de-\ncreased token-level recovery while preserving\nstrong semantic integrity. Besides, our frame-\nwork also demonstrates robust generalization\nto out-of-distribution medical data.\n1\nIntroduction\nLarge language models (LLMs) have achieved sig-\nnificant progress on a wide array of tasks. De-\nspite their impressive performance, these models\nare often regarded as “black boxes,\" limiting our un-\nderstanding of their internal working mechanisms.\nConsequently, a growing body of research aims\nto decode the information encoded within LLMs.\nThese approaches vary widely, from training lin-\near probes (Zou et al., 2023; Gurnee and Tegmark,\n2025) or sparse autoencoders (SAEs) (Shu et al.,\n2025) to interpret specific features, to mapping in-\nternal representations directly to the vocabulary\nspace through methods like Logit Lens (nostalge-\nbraist, 2020) and Tuned Lens (Belrose et al., 2023).\nIn this work, we focus on a distinct but related\nchallenge: representation decoding, which aims\nto decode the internal representations of language\nmodels back into the full, original text.\nExisting work on decoding model activations\ninto text can be grouped into two major categories.\nThe first category studies representation inversion\nfrom a safety perspective. They focus on invert-\ning input tokens using both sentence and token\nembeddings (Devlin et al., 2019; Li et al., 2023;\nMorris et al., 2023; Huang et al., 2024b; Dong\net al., 2025). The second line of research studies\nrepresentation inversion from an interpretability\nperspective. These work interpret activations us-\ning in-context information, such as template-based\nprompts or original input, through either patch-\ning (Chen et al., 2024; Ghandeharioun et al., 2024)\nor guided sampling (Luo et al., 2025; Huang et al.,\n2024a). Although existing research has achieved\npreliminary results in representation inversion, sig-\nnificant challenges remain. First, most existing\nwork investigates how information can be recov-\nered from a sequence of token representations. In\ncontrast, our interest is in decoding full text from\nthe last-token representation from LLMs. Second,\nwhile existing work can recover text, it often does\nnot answer the question of what information is en-\ncoded within a single last-token representation.\nTo address these challenges, we attempt to an-\nswer the following research question: To what ex-\ntent can we invert the information from the last-to-\nken representation of an input sequence? Our over-\nall goal is to explore single-token, activation-based\ninput inversion. This is particularly challenging\nbecause the last-token representation is optimized\nfor next-token prediction and can be regarded as\nan information bottleneck. Through quantitatively\ncomparing the inverted text with the original in-\nput text, we aim to provide an understanding of\nwhat knowledge is preserved and encoded in the\nlast-token representation of LLMs.\n1\narXiv:2511.06571v1  [cs.CL]  9 Nov 2025\n"}, {"page": 2, "text": "Unembedding Layer\nEmbedding Layer\nSelf-Attention\nFeed Forward\nAdd & Normalize\nAdd & Normalize\nl-th Transformer Layer\nOmaha\nHigh\nSchool\n(\n)\nArkansas\nEmbedding Layer\nTokenizer\nAdapter\nTransfromer  Layer\nTransfromer  Layer\nUnembedding  Layer\nNorthside\nHigh\nSchool\n(\n)\nArkansas\nSystem Prompt\nUser Prompt\nGate\nGate\nTransformation Matrix\nLayerNorm\nLayerNorm\nAdapter\nFigure 1: Overview of Rep2Text. The last-token representation obtained from the l-th layer of the target model\nM is projected into the embedding space of the decoding model M′ via the adapter. The projected embeddings,\ntogether with those of the system and the user prompts, are then fed into the decoding model to reconstruct the\ncorresponding text sequence.\nBased on the above-mentioned motivation, in\nthis work we propose Rep2Text (Representation\nto Text), a framework for decoding text from the\nlast-token representations of LLMs as illustrated in\nFigure 1. Inspired by the design of large vision lan-\nguage models (LVLMs) such as LLaVA (Liu et al.,\n2023), Rep2Text trains a representation inverter\nwhich contains a decoding language model and an\nadapter. The adapter is used to map the input token\nrepresentation from the target model into the token\nembedding space of the decoding language model,\nthereby aligning their latent spaces. These pro-\njected embeddings are subsequently passed into the\ndecoding LLM, enabling it to interpret and gener-\nate text consistent with the original input sequence.\nThrough comparing the inverted text against the\noriginal text, we quantify the information retained\nin the last-token representation.\nOur experiments reveal that, remarkably, over\nhalf of the original information in 16-token se-\nquences can be recovered from a single last-\ntoken representation while maintaining strong\nsemantic coherence. This finding directly answers\nour central research question and demonstrates that,\ndespite being optimized as an information bottle-\nneck for next-token prediction, last-token represen-\ntations retain a substantial amount of recoverable\ninformation about the input sequence. To validate\nthe effectiveness of our approach, we combine es-\ntablished quantitative metrics with LLM-as-a-judge\nevaluations to measure information retention at to-\nken, structure, and semantic levels. Our results\nshow that representations from different models\nexhibit varying recovery rates, revealing potential\nvulnerabilities in some models, while the recovery\nrate remains robust across decoding models of dif-\nferent sizes. Further analysis reveals that structural\ninformation is most prominent in early-to-middle\nlayers, while semantic information becomes more\npronounced in middle-to-late layers. We also find\nthat recovery rate is strong for sequences shorter\nthan 16 tokens but degrades for longer ones. Fi-\nnally, evaluations on out-of-distribution clinical\nnotes demonstrate promising generalizability, con-\nfirming that our method captures genuine represen-\ntational properties rather than merely overfitting to\nin-distribution patterns.\n2\nRep2Text Framework\nIn this section, we introduce the proposed Rep2Text\nframework (see Figure 1). Rep2Text employs a\ntrainable adapter that bridges the target model’s rep-\nresentation space to a decoding language model’s\nembedding space, enabling us to systematically\ninvestigate what information is preserved in com-\npressed last-token representations and how much\nof the original input can be recovered. The decod-\ning LLM then autoregressively reconstructs the text\nfrom these projected embeddings.\n2.1\nProblem Statement\nGiven a layer-level representation from an LLM,\nour objective is to invert its ground-truth sequence\nas accurately as possible, thereby investigating the\n2\n"}, {"page": 3, "text": "extent to which the original input information is\nretained. Throughout this work, the terms repre-\nsentation and activation are used interchangeably\nto refer to the token-level hidden states extracted\nfrom different layers of decoder-only models.\nConcretely, given a ground-truth sequence of n\ntokens S = ⟨s1, . . . , sn⟩and a target model M\nwith L layers, we focus exclusively on the last-\ntoken representation. Let hℓdenote the residual\nstream representation of the last token for input S\nat layer ℓ∈{1, . . . , L} of model M. We aim to\ndecode hℓinto an inverted text ˆS = ⟨ˆs1, . . . , ˆsm⟩.\nOur goal is to quantify how much information is\npreserved within the bottleneck representation hℓ\nby comparing the difference between the original\ninput S and the reconstructed output ˆS.\n2.2\nRep2Text Inverter Design\nTo invert the representation, we propose an inverter\narchitecture inspired by the design of typical large\nvision-language models such as LLaVA. The in-\nverter consists of two key components: (1) a train-\nable adapter that projects the target model’s inter-\nnal representation into the input token embedding\nspace of the decoding language model, and (2) a de-\ncoding language model that generates the inverted\ntext from these projected embeddings.\nSpecifically, we introduce a decoding model M′\nthat can either be a copy of the target model M or a\ndifferent LLM. To bridge the representation space\nof M and the embedding space of M′, we train an\nadapter to project the token representation hℓ∈Rd\nfrom the target model M into the token embedding\nspace of the decoding model M′. The adapter is\nimplemented as a two-layer MLP with gated skip\nconnection with optional projection, defined as:\nh1 = GELU(W 1 · LN(hℓ) + b1),\nh2 = W 2 · h1 + b2,\nXe = LN(W s · hℓ+ gk · h2),\n(1)\nwhere LN(·) and GELU(·) represent the norm\nlayer and activation function respectively. W 1 ∈\nRd×dhid and W 2 ∈Rdhid×k·d′ refer to linear trans-\nformations in the first and second layers respec-\ntively, where d and d′ represent the hidden dimen-\nsions of the target model and decoding model. Note\nthat we set dhid = f · d, where f is an expansion\nfactor. W s ∈Rd×d′ denotes the transformation\nmatrix of the skip connection. When d = d′, W s\nis an identity matrix enabling a true residual con-\nnection; when d ̸= d′, W s serves as a learned\nprojection matrix to match dimensions. h2 ∈Rk·d′\nis reshaped into (k, d′), which can be regarded as\nk token embeddings. Each token embedding is\nconstructed with a gated combination of the skip\npath and the MLP-transformed path to preserve\nthe representation information as much as possible.\nThe projected token embedding can be denoted as\nXe = [x1; · · · ; xk], where the number k of pro-\njected tokens is a hyperparameter.\nFor each representation, the projected token em-\nbedding Xe is combined with system prompt em-\nbedding Xsys and user prompt embedding Xu.\nThe combined sequence [Xe; Xsys; Xu] is fed\ninto the first layer of decoding model M′ (after\nits embedding layer), bypassing the embedding\nlayer. The decoding model then autoregressively\ngenerates the inverted text ˆS.\n2.3\nRep2Text Inverter Training\nFor an target sequence of length L at step t, the\ninverter predicts its probability conditioned on text\nembedding and all previous predicted tokens. The\njoint probability of inverted sequence ˆS is:\np\n\u0010\nˆS | Xe, Xsys, Xu)\n=\nT\nY\nt=1\npθ\n\u0010\nst | Xe, Xsys, Xu, ˆS<t\n\u0011\n,\n(2)\nwhere ˆS<t are the inverted tokens generated before\nstep t. θ is the trainable parameters. In our paper,\nwe consider two training schemes: (1) adapter-\nonly fine-tuning, where only the adapter parame-\nters are optimized; (2) joint fine-tuning, where the\nadapter is first fine-tuned independently and then\nthe adapter is fully fine-tuned and the base model\nis updated via LoRA (Hu et al., 2022). Accord-\ningly, θ refers to the trainable parameters under the\nchosen scheme.\nDuring training, we employ teacher forcing to\nmaximize the log-likelihood of ground-truth tokens.\nTo stabilize training, we utilize label smoothing to\nsoften the one-hot target distribution. The ground-\ntruth token vocabulary distribution is denoted as\nqt(vi) = (1 −ϵ)1 [vi = st] +\nϵ\n|V |,\n(3)\nwhere 1(·) is an indictor function that equals 1 if\nthe condition holds and 0 otherwise, and ϵ is the\nlabel smoothing factor, set to be 0.075. The training\nobjective is the smoothed cross-entropy loss that\ndefined as:\n3\n"}, {"page": 4, "text": "Lt = −\n|V |\nX\ni=1\nqt(vi) log pθ(vi | Xe, Xsys, Xu, S<t) ,\nLLS = 1\nT\nT\nX\nt=1\nLt.\n(4)\nThis training objective optimizes the adapter (and\noptionally the decoding model via LoRA) to mini-\nmize the prediction error across all token positions\nin the inverted sequence. A label smoothing term\nis incorporated to prevent overconfidence in token\npredictions, thereby improving generalization to\nunseen representations. Through this training pro-\ncess, the adapter learns to effectively map the com-\npressed last-token representation from the target\nmodel’s latent space into the decoding model’s to-\nken embedding space, enabling the reconstruction\nof the original input sequence.\n3\nExperiments\nIn this section, we evaluate our framework on\ninverting ground-truth sequences under different\ncombinations of target and decoding models (§ 3.2).\nTo analyze information bottleneck within last-token\nrepresentations, we investigate the inversion per-\nformance across varying lengths of ground-truth\nsequences (§3.3). We further invert representations\nfrom different layers of the target model to investi-\ngate how encoded information evolves throughout\nthe network (§3.4). Finally, to demonstrate the ef-\nfectiveness and generalizability of our approach,\nwe conduct experiments on inverting representation\non out-of-distribution datasets (§3.5).\n3.1\nExperimental Setup\nDatasets.\nOur adapters are trained on passages\nrandomly truncated from Wikipedia articles con-\ntained in The Pile (Gao et al., 2020). Each trun-\ncated sequence consists of n tokens without over-\nlap, where n ∈{8, 16, 32, 64} depending on the\nexperimental configurations. Each training sam-\nple comprises a data pair consisting of the last-\ntoken representation of ground-truth sequence from\na fixed layer of the target model and its corre-\nsponding ground-truth sequence itself. For adapter\nfinetuning, we use a dataset containing 640K se-\nquences. During the full fine-tuning stage, the\ntraining dataset incorporates an additional 960K\nsequences. For evaluation, we randomly sample\n1000 sequences as the test set and evaluate the in-\nverted outputs using a combination of quantitative\nmetrics and LLM-as-a-judge assessments.\nDinwiddie Street Methodist church, \nis a historic Methodist church located in Portsmouth\n[GT]\nst. John's Episcopal church and Parish House\nis a historic church located in Portsmouth\n[GEN]\nDinwiddie Street Methodist church, is a historic Methodist church located in Portsmouth\n[GT]\nst. John's Episcopal church and Parish House\nis a historic church located in Portsmouth\n[GEN]\nWeak Structure Match\nWeak Entity Match\nStrong Entity Match\nStrong Structure Match\nFigure 2: Examples of structure and entity similarity.\nDarker colors indicate higher similarity score.\nModels.\nOur paper uses Llama-3.1-8B as both\nthe target and decoding models throughout our\nexperiments except in § 3.2, where Gemma-\n7B (Google, 2024), Mistral-7B-v0.1 (AI, 2023),\nand Llama-3.2-3B (AI, 2025) are used as target\nmodels while keeping Llama-3.1-8B as the inverter,\nto study the feasibility and effectiveness of cross-\nmodel decoding. To examine the scaling behavior\nof representation inversion, we additionally evalu-\nate Llama-3.2-3B (AI, 2025) as a smaller decoding\nmodel to illustrate if larger decoding models yield\nimproved inversion performance.\nImplementation Details.\nMost of our adapter-\nonly fine-tuning experiments are conducted on se-\nquences of length n = 16, which can be accom-\nmodated on 2 NVIDIA A100 GPUs and require\napproximately 7 hours to complete. The corre-\nsponding full fine-tuning experiments under the\nsame setting take an additional 12 hours using the\nsame hardware. During adapter-only fine-tuning,\nwe use a learning rate of 1e-3. For full fine-tuning,\nwe apply a learning rate of 5e-4 for the adapter\nand 2e-4 for the LoRA parameters. Both train-\ning schemes are trained for 3 epoches, as further\ntraining yields diminishing improvements. Addi-\ntional implementation details are included in the\nAppendix C. In all experiments except those in\n§ D.3, we fine-tune only the adapters while keeping\nthe decoding model frozen, to demonstrate that the\ndesired alignments between the hidden representa-\ntion space and the embedding space are primarily\nachieved by adapter fine-tuning alone, rather than\nmodel overfitting.\nEvaluation Measurements.\nThe evaluation of\ninverted sequences considers three key aspects of\nfidelity to the ground truth: token-level accuracy,\nsentence structure and entities preservation, and\nsemantic similarity.\n• Token-level Accuracy.\nWe adopt ROUGE\nscores to measure the token-level accu-\nracy (Lin, 2004). Specifically, ROUGE-1, and\nROUGE-2 measure the recovery rate of indi-\n4\n"}, {"page": 5, "text": "Table 1: Performance Comparison of Inverting Representation from Different Models Using Llama-3.1-8B.\nTarget Model Decoding Model ROUGE-1 ROUGE-2 ROUGE-L BERTScore Structure\nEntity\nTopic\n(0-1)↑\n(0-1)↑\n(0-1)↑\n(0-1)↑\n(0-1)↑\n(0-1)↑\n(0-1)↑\nGemma-7B\nLlama-3.1-8B\n0.49±0.23\n0.27±0.24\n0.47±0.23\n0.74±0.14\n0.66±0.23 0.60±0.27 0.78±0.25\nMistral-7B-v0.1\nLlama-3.1-8B\n0.52±0.24\n0.35±0.28\n0.51±0.23\n0.81±0.11\n0.70±0.22 0.76±0.23 0.91±0.15\nLlama-3.1-8B\nLlama-3.1-8B\n0.48±0.23\n0.28±0.24\n0.47±0.22\n0.78±0.11\n0.66±0.22 0.74±0.23 0.91±0.14\nLlama-3.2-3B\nLlama-3.1-8B\n0.46±0.22\n0.24±0.22\n0.44±0.22\n0.77±0.11\n0.64±0.22 0.71±0.23 0.88±0.16\nMistral-7B-v0.1\nLlama-3.2-3B\n0.52±0.25\n0.33±0.27\n0.51±0.23\n0.80±0.12\n0.69±0.22 0.74±0.24 0.91±0.15\nLlama-3.2-3B\nLlama-3.2-3B\n0.46±0.22\n0.26±0.23\n0.45±0.21\n0.76±0.11\n0.64±0.22 0.69±0.24 0.88±0.16\nvidual tokens and 2-grams, respectively, while\nROUGE-L captures the longest common sub-\nsequence between the ground-truth and the in-\nverted sequence. Detailed definitions of these\nmetrics are provided in the Appendix D.\n• Sentence Structure and Entity Preservation.\nTo evaluate the preservation of sentence struc-\nture and entities, we use GPT-4.1-mini to rate\nthe degree of preservation ranging on a 0-5\nscale (normalized to 0-1), yielding the Struc-\nture Score and Entity Score, respectively. The\nstructure score assesses how well the gram-\nmatical structure and sentence skeleton are\npreserved in the inverted sequences as shown\nin Figure 2. While the entity score measures\nhow accurately entity names and their asso-\nciated attributes are inverted. Detailed rating\ncriteria are provided in Appendix F.\n• Semantic Similarity. We use BERTScore F1\nand LLM-as-a-judge to collectively evaluate\nthe semantic similarity between the ground-\ntruth and inverted sequences (Zhang et al.,\n2020). BERTScore quantifies similarity in the\nembedding space, whereas the LLM-based\nevaluation measures topic relevance between\nground-truth and inverted sequences.\nThe\nscoring guidelines for LLM-as-a-judge evalu-\nation are included in Appendix F.\n3.2\nInversion Results Analysis\nAs defined in § 2.2, the inverter’s base model can\nbe either a copy of the target model or a differ-\nent model. To demonstrate the feasibility of both\nconfigurations, we specifically choose Gemma-7B,\nMistral-7B-v0.1, Llama-3.2-3B, and Llama-3.1-8B\nas target models. For the decoding model, we use\nLlama-3.1-8B and Llama-3.2-3B to interpret repre-\nsentation of 16-token sequences from 10th layer of\neach target model. The results are summarized in\nTable 1. We derive the following observations.\nFirst, using only the last-token representation,\nour models recover approximately half of the orig-\ninal tokens on average, as measured by ROUGE-\n1. For bi-grams, the average ROUGE-2 recover\nrate reaches about 24%. The average ROUGE-L\nscore shows that the longest common subsequences\ntypically retain at least half of the original con-\ntent, aligning with ROUGE-1. Moreover, the struc-\nture score exceeds 64% on average, indicating that\nthe syntactic structure of ground-truth sentences\nis largely preserved during inversion. In terms of\nentity recovery, the lower bound reaches 60%, and\npreliminary qualitative inspection further reveals\nthat entities are often recovered in a semantically\nconsistent manner, even when not exact lexical\nmatches. Finally, both BERTScore and topic score\ndemonstrate that the inverted sentences maintain\nstrong semantic relevance to original text. Thus, the\nmodel recovers over half of the original content\nwhile maintaining strong semantic coherence\nand integrity.\nWe use Llama-3.1-8B as the decoding model to\ninvert representations from Gemma-7B, Mistral-\n7B-v0.1, and Llama-3.2-3B, respectively.\nAs\nshown in Table 1, the inversion performance is\nhighest for Mistral-7B-v0.1, whose scores consis-\ntently surpass those of Gemma-7B and Llama-3.2-\n3B across all metrics. This indicates that repre-\nsentations from different models exhibit vary-\ning levels of invertibility. In particular, Mistral-\n7B-v0.1 encodes information that is more recover-\nable at the token, structural and topical levels, in-\ndicating stronger representational richness but also\ngreater susceptibility to information leakage and\nprivacy risks. To further substantiate this finding,\nwe employ Llama-3.1-3B as the inverter to decode\nrepresentations from Mistral-7B-v0.1 and observe\n5\n"}, {"page": 6, "text": "8\n16\n32\n64\nNumber of Tokens\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScore (0-1)\nInversion Performance Across Sequence Lengths\nMetrics\nROUGE-1\nROUGE-2\nROUGE-L\nStructure\nEntity\nTopic\nBERTScore F1\nFigure 3: Performance comparison of inverting varying\nlength of sequences.\nL5\nL10\nL15\nL20\nL25\nL30\n-th Layers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScore (0-1)\nLayer-wise Inversion Performance\nMetrics\nROUGE-1\nROUGE-2\nROUGE-L\nStructure\nEntity\nTopic\nBERTScore F1\nFigure 4: Performance comparison with layerwise rep-\nresentation inversions.\ncomparable performance across most evaluation di-\nmensions. This confirms that representations from\nMistral-7B-v0.1 consistently yield higher recover-\nability regardless of which decoding model is used.\nTo investigate the discrepancy between self-\ninversion and cross-model inversion, we further\ndecode representations from Llama-3.2-3B using\nboth itself and Llama-3.1-8B. The results show that\ninversion performance remains robust across\ndifferent decoding models. A similar trend is ob-\nserved when inverting representations from Mistral-\n7B-v0.1 with Llama-3.1-8B and Llama-3.2-3B,\nwhere all metrics remain nearly identical. Over-\nall, using the same model as both the target model\nand decoding model does not necessarily guarantee\nsuperior inversion performance, and larger decod-\ning models provides only marginal improvement\nin entity recovery. Moreover, the effectiveness of\ncross-model inversion lends support to the platonic\nrepresentation hypothesis (Huh et al., 2024), which\nposits that as models scale, their internal represen-\ntations converge toward a shared, aligned structure.\n3.3\nInverting Sequence of Varying Length\nUnderstanding how much information can be effec-\ntively inverted from the last-token representation is\ncrucial for analyzing the information bottleneck in\nLLMs. Intuitively, we investigate the performance\non inverting sequences with different number of\ntokens. Here, we train separate adapters to invert\nlast-token representations of sequences with 8, 16,\n32, 64 tokens at the 10th layer respectively. As\ndemonstrated in § 3.2, the inversion performance\nremains robust across different decoding models.\nTherefore, our subsequent experiments adopt the\nconfiguration where the same model, i.e. Llama-\n3.1-8B, is used as both target and decoding model.\nWe report the inversion results across varying\nsequence lengths in Figure 3. A clear degrada-\ntion in inversion performance is observed as\nthe sequence length increases. For example, the\nROUGE-1 score drops from approximately 0.6\nfor 8-token sequence to around 0.3 for 64-token\nsequences, indicating that the inverter struggles\nto precisely reconstruct longer sequences. Sim-\nilarly, the structure score decreases aggressively\nwith the inverted sequence length, suggesting that\nrecovering the grammatical skeleton of the original\nsentence becomes increasingly difficult when rely-\ning solely on the last-token representation. When\nit comes to the topic-level evaluations, including\nBERTScore, entity recovery, and topic score, the\ndecline in performance is much less pronounced.\nThe difference between the topic-level scores for\n64-token and 8-token sequences remains within\n20%. Therefore, as the inverted length increases,\nthe inverter continues to capture the overall topic\nconsistently, though it fails to recover much of\nthe detailed content exactly. Detailed inverted se-\nquences are presented in Appendix A.\n3.4\nDecoding Representation across Layers\nPrior work show that middle-to-deep layers capture\nhigh-level semantics more effectively (Jin et al.,\n2025; Campbell et al., 2023). To further investigate\nhow decoded information evolves across layers and\nto identify the optimal layer for inversion, we train\nadapters on representations extracted from differ-\nent depths of Llama-3.1-8B using 16-token input\nsequences. Specifically, we select the 5th, 10th,\n15th, 20th, 25th, and 30th layers for comparison.\nThe results in Figure 4 show that token-level\nmetrics, structure score and BERTScore reach their\nhighest values at the 10th layer. Since ROUGE-L\ncaptures the longest common subsequence and the\nstructure score reflects grammatical integrity, both\n6\n"}, {"page": 7, "text": "0.00\n0.25\n0.50\n0.75\n1.00\nScore\n0\n20\n40\n60\nFrequency\n13.0% above\nthreshold\nROUGE-1\nThreshold: 0.48\n0.00\n0.25\n0.50\n0.75\n1.00\nScore\n0\n20\n40\n60\n80\nFrequency\n13.0% above\nthreshold\nROUGE-2\nThreshold: 0.28\n0.00\n0.25\n0.50\n0.75\n1.00\nScore\n0\n20\n40\n60\nFrequency\n13.0% above\nthreshold\nROUGE-L\nThreshold: 0.47\n0.00\n0.25\n0.50\n0.75\n1.00\nScore\n0\n5\n10\n15\nFrequency\n13.0% above\nthreshold\nBERTScore\nThreshold: 0.78\nDistribution of Evaluation Metrics\nFigure 5: The score distribution on OOD clinical notes. The mean score obtained by Llama-3.1-8B wen used as\nboth the target and decoding model (Table 1), serves as the threshold for assessing the inverter’s capability to recover\nOOD data.\nTable 2: Inverted examples with OOD clinical notes\nGround-truth Sequence\nInverted Sequence\nR1\nR2\nRL\nBS\nSS\nES\nTS\nA. Rogers, 71, admitted for\npneumonia, stable vitals.\n://\\n A. Rogers, 71, admitted for pneumonia,\nstable vitals. ://\\nA. Rogers,\n0.89\n0.88\n0.89\n0.85\n2\n5\n5\nS. Evans, 59, COPD flare, on\n2L oxygen\n, 2/3/2018\\n S. Evans, 59, COPD flare, on 2L\n0.78\n0.75\n0.78\n0.8\n2\n2\n5\nDiabetic foot ulcer dressed\ndaily.\n2. Diabetic foot ulcer dresseddaily . 3. Diabetic\nfoot ulcer dressed daily. 4. Di\n0.53\n0.47\n0.53\n0.8\n2\n5\n5\nKaren J., 41, migraine recur-\nrent, prescribed sumatriptan.\n://www.medscape.com/viewarticle/509975\\n\nKaplan, M., 41, migraine recurrent\n0.35\n0.27\n0.35\n0.55\n1\n2\n3\n80yo M, dementia, agitated\novernight.\n://www.medscape.com/viewarticle/812201\\n\nA 70-year-old man with a history of dementia\n0.1\n0.1\n0.1\n0.48\n2\n4\n5\nPt denies pain, tolerating diet.\nof 2.5 kg. 2.5 kg. 2.5 kg. 2.5 kg\n0\n0\n0\n0.35\n0\n0\n0\nmetrics consistently indicate that syntactic infor-\nmation is best encoded around the 10th layer. In\ncontrast, ROUGE-1 and entity recovery peak at the\n15th layer, suggesting that lexical and entity-level\ndetails are better represented around the middle\nlayer. The topic score hypes from the 10th layer to\nthe 20th layer with only minor fluctuations, imply-\ning that high-level semantic information is progres-\nsively consolidated in later layers. In summary, the\nearly-to-middle layers (around L10-L15) con-\ntain the most concrete and literally recoverable\nrepresentations of the input’s structure and lex-\nical content. As information propagates to later\nlayers, these specific details become increasingly\nabstracted and compressed, making them harder to\ninvert, while the core, high-level topic is preserved\nand consolidated.\n3.5\nOOD Samples Inversion Analysis\nTo further evaluate the inversion performance of\nour approach and its applicability in interpreting\nrepresentations, we apply the trained adapters on\ninverted out-of-distribution (OOD) data and assess\ntheir performance across all evaluation dimensions.\nIn addition, we engage graduate students to manu-\nally rate the generated sentences according to the\nsame instructions provided to GPT-4.1-mini, which\nallows for a direct comparison between human\njudgments and LLM-as-a-judge evaluations.\nAs shown in previous experiments, inversion per-\nformance declines as the sequence length increases.\nThe results indicate that 16-token sequences yield\npromising inversion quality. Therefore, we select\n16-token sequences as our test data. Since our\nadapters are trained on data from Wikipedia, which\ncovers diverse factual knowledge including per-\nsons, events, and locations. We further evaluate the\ngeneralization of the inverter on unseen data from\nthe medical domain. We prompt GPT-4.1-mini to\ngenerate 100 sequences resembling clinical notes\nfollowing the instructions in Appendix E. Each\nsequence contains up to 16 tokens and includes\n7\n"}, {"page": 8, "text": "elements like patient names, admission dates or\nbirth dates, and reported symptoms or treatments.\nWe then apply the adapters trained on 16-token\nsequences using Llama-3.1-8B as the inverter.\nThe evaluation results for token-level recovery\nand BERTScore are shown in Figure 5. We use the\nmean score of each metric on the Wikipedia test\nset in Table 1 as a reference threshold. Notably,\napproximately 13% of the inverted clinical notes\nexceed the in-distribution average performance,\nsuggesting partial generalization of the learned in-\nversion capability to out-of-distribution data. Al-\nthough the clinical data are unseen during train-\ning, some examples are still inverted with striking\nfidelity. Representative examples of inverted out-\nputs at different performance levels are presented\nin Table 2. To provide a comprehensive view of\ninversion quality, we include examples inverted\nwith varying ROUGE-1 scores. Here, R1, R2, RL,\nBS, SS, ES, and TS denote ROUGE-1, ROUGE-2,\nROUGE-L, BERTScore, Structure Score, Entity\nScore, and Topic Score, respectively. The human\nsanity check results are presented in Appendix B.\nDue to the lack of explicit alignment with med-\nical text during training, the inverter occasionally\ngenerates introductory medical website content be-\nfore the actual note. To accommodate this, we set\nthe number of generated tokens to 24. As shown in\nTable 2, when ROUGE-1 score exceeds 0.5, most\nexamples recover the main context accurately, oth-\nerwise core entities such as year, disease name can\noften still be recovered. However, some samples\nfail to recover critical information because of do-\nmain misalignment. Overall, the inversion results\non OOD clinical notes demonstrate the general-\nizability of our trained adapters in recovering\nmeaningful information from unseen data.\n4\nRelated Work\nEmbedding Inversion.\nThe inversion is typically\nformulated as an optimization problem in which\nthe attack model attempts to generate hypotheses\nthat produce embeddings as close as possible to the\ntarget embeddings. A few work attempt to recover\nground-truth sequences using sentence embeddings\nfrom BERT models. Song and Raghunathan (2020)\ninverse the sentence embeddings into bag of words.\nSome work attempt to train an attacker model to de-\ncode the ground-truth sequence utilizing sentence\nembeddings and text embeddings (Li et al., 2023;\nMorris et al., 2023; Huang et al., 2024b). Fur-\nther, Dong et al. (2025) extends embedding inver-\nsion to LLM’s internal states at a certain layer, by\nlearning token embeddings that can produce sim-\nilar internal states. However, to fully recover the\ninput text, these papers either rely heavily on it-\nerative optimization or incorporating all sentence\nembeddings and token embeddings.\nActivation Decoding.\nSome work seek to de-\ncode activations into natural language.\nRecent\nwork such as SelfIE (Chen et al., 2024), and Patch-\nscopes (Ghandeharioun et al., 2024) interpret hid-\nden representations through patching representa-\ntions into the forward pass of LLMs to decode\nnatural language explanations. Besides, LIT (Pan\net al., 2025) finetunes target model to answer ques-\ntions related to given activations which is patched\nwithin the target model. Other work attempt to\ninterpret activations by ensuring geometric proxim-\nity of activations, as semantically identical inputs\nusually produce similar activations. Specifically,\nan input distribution producing similar activations\nis meant to be identified via input inversion. In-\nverseView (Huang et al., 2024a) trains a decoder\nto sample the input distribution for a given acti-\nvation. InverseScope (Luo et al., 2025) explore\ntask-specific features encoded in the input distribu-\ntion. In contrast, we rely solely on the last-token\nrepresentations, without any in-context templates\nor original inputs as auxiliary hints, to recover the\ninformation encoded within these representations.\n5\nConclusions\nIn this work, we explore the research question\nthat to what extent can the original input text be\nrecovered from a single last-token representation\nof LLMs? To answer this question, we proposed\nRep2Text, a novel framework that employs a train-\nable adapter to project a target model’s last-token\nrepresentation into the embedding space of a de-\ncoding language model, which then autoregres-\nsively reconstructs the input text. Our compre-\nhensive evaluations indicate that over half of the\ninformation in 16-token sequences can be recov-\nered from the compressed last-token representation\nwhile maintaining strong semantic integrity. Be-\nsides, experimental results show longer sequences\nlead to decreased inversion performance, with reli-\nable recovery achieved for sequences under 16 to-\nkens. Additionally, Rep2Text demonstrated promis-\ning generalization, successfully recovering infor-\nmation from out-of-distribution clinical data.\n8\n"}, {"page": 9, "text": "References\nMeta AI. 2025. Llama-3.2-3b. https://huggingface.\nco/meta-llama/Llama-3.2-3B. [Accessed 29-10-\n2025].\nMistral AI. 2023.\nMistral-7b-v0.1.\nhttps://\nhuggingface.co/mistralai/Mistral-7B-v0.1.\n[Accessed 29-10-2025].\nNora Belrose, Zach Furman, Logan Smith, Danny Ha-\nlawi, Igor Ostrovsky, Lev McKinney, Stella Bider-\nman, and Jacob Steinhardt. 2023. Eliciting latent\npredictions from transformers with the tuned lens.\narXiv preprint arXiv:2303.08112.\nJames Campbell, Richard Ren, and Phillip Guo.\n2023. Localizing lying in llama: Understanding in-\nstructed dishonesty on true-false questions through\nprompting, probing, and patching. arXiv preprint\narXiv:2311.15131.\nHaozhe Chen, Carl Vondrick, and Chengzhi Mao. 2024.\nSelfie: self-interpretation of large language model\nembeddings. In Proceedings of the 41st International\nConference on Machine Learning, pages 7373–7388.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 conference of the\nNorth American chapter of the association for com-\nputational linguistics: human language technologies,\nvolume 1 (long and short papers), pages 4171–4186.\nTian Dong, Yan Meng, Shaofeng Li, Guoxing Chen,\nZhen Liu, and Haojin Zhu. 2025. Depth gives a false\nsense of privacy: Llm internal states inversion. In\n34th USENIX Security Symposium (USENIX Security\n25), pages 1629–1648.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, and 1\nothers. 2020.\nThe pile: An 800gb dataset of di-\nverse text for language modeling. arXiv preprint\narXiv:2101.00027.\nAsma Ghandeharioun, Avi Caciularu, Adam Pearce, Lu-\ncas Dixon, and Mor Geva. 2024. Patchscopes: a\nunifying framework for inspecting hidden represen-\ntations of language models. In Proceedings of the\n41st International Conference on Machine Learning,\npages 15466–15490.\nGoogle. 2024. Gemma-7b. https://huggingface.\nco/google/gemma-7b. [Accessed 29-10-2025].\nWes Gurnee and Max Tegmark. 2025. Language mod-\nels represent space and time. In The Twelfth Interna-\ntional Conference on Learning Representations.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nXinting Huang, Madhur Panwar, Navin Goyal, and\nMichael Hahn. 2024a.\nInversionview: a general-\npurpose method for reading information from neural\nactivations. Advances in Neural Information Process-\ning Systems, 37:137903–137964.\nYu-Hsiang Huang, Yuche Tsai, Hsiang Hsiao, Hong-Yi\nLin, and Shou-De Lin. 2024b. Transferable embed-\nding inversion attack: Uncovering privacy risks in\ntext embeddings without model queries. In Proceed-\nings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 4193–4205.\nMinyoung Huh, Brian Cheung, Tongzhou Wang, and\nPhillip Isola. 2024. The platonic representation hy-\npothesis. The Forty-First International Conference\non Machine Learning (ICML).\nMingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng\nZeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao,\nKai Mei, Yanda Meng, Kaize Ding, and 1 others.\n2025. Exploring concept depth: How large language\nmodels acquire knowledge and concept at different\nlayers? The 31st International Conference on Com-\nputational Linguistics (COLING 2025).\nHaoran Li, Mingshi Xu, and Yangqiu Song. 2023. Sen-\ntence embedding leaks more information than you\nexpect: Generative embedding inversion attack to\nrecover the whole sentence. In Findings of the As-\nsociation for Computational Linguistics: ACL 2023,\npages 14022–14040.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries.\nIn Text summarization\nbranches out, pages 74–81.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. Advances in\nneural information processing systems, 36:34892–\n34916.\nYifan Luo, Zhennan Zhou, and Bin Dong. 2025. In-\nversescope: Scalable activation inversion for in-\nterpreting large language models.\narXiv preprint\narXiv:2506.07406.\nJohn Morris, Volodymyr Kuleshov, Vitaly Shmatikov,\nand Alexander M Rush. 2023.\nText embeddings\nreveal (almost) as much as text. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 12448–12460.\nnostalgebraist. 2020. interpreting gpt: the logit lens.\nLessWrong.\nAlexander Pan, Lijie Chen, and Jacob Steinhardt. 2025.\nTeaching LLMs to decode activations into natural\nlanguage.\nDong Shu, Xuansheng Wu, Haiyan Zhao, Daking Rai,\nZiyu Yao, Ninghao Liu, and Mengnan Du. 2025. A\nsurvey on sparse autoencoders: Interpreting the inter-\nnal mechanisms of large language models. EMNLP\nFindings.\n9\n"}, {"page": 10, "text": "Congzheng Song and Ananth Raghunathan. 2020. In-\nformation leakage in embedding models. In Pro-\nceedings of the 2020 ACM SIGSAC conference on\ncomputer and communications security, pages 377–\n390.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2020. Bertscore: Evaluating\ntext generation with bert. The International Confer-\nence on Learning Representations (ICLR).\nAndy Zou, Long Phan, Sarah Chen, James Campbell,\nPhillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski,\nand 1 others. 2023. Representation engineering: A\ntop-down approach to ai transparency. arXiv preprint\narXiv:2310.01405.\n10\n"}, {"page": 11, "text": "A\nInverted Examples with Varying Token Lengths and Recovery Rate\nAs shown in Table 3, for sequences with 8 tokens and 16 tokens, some inverted sequences fully recover\nthe original text, while others fail to capture fine-grained details yet still preserve clear grammatical\nstructures. For example, when inverting 16-token sequence, although the ROUGE-1 score is only 0.4,\nthe original sentence “Rob James may refer to:\\n\\nRob James (singer) (” and the inverted sequence\n“Mark Jones\\n\\nMark Jones may refer to:\\n\\nMark Jones (singer) (” share the same syntactic pattern,\n\"[NAME]\\n\\n[NAME] (singer) (\", and convey equivalent topic-level information. However, when the\nnumber of tokens exceed 16, the inverted sequences remain highly topic-relevant but tend to lose their\nglobal structural coherence, even when achieving a reasonable ROUGE-1 score.\nTable 3: Inverted Examples with Varying token Lengths\nTokens\n(#)\nGround-truth Sequence\nInverted Sequence\nR1\nR2\nRL\nBS\nSS\nES\nTS\n8\nin the Centre-Val de Loire\nin the Centre-Val de Loire\n1\n1\n1\n1\n1\n1\n1\nis the second era of the Hade\nis the third era of the Hade\n0.86\n0.67\n0.86\n0.99\n1\n0.8\n1\nwho enforce New Zealand’s reg-\nulatory building control\nthe enforcement of New Zealand\nstatutory building control\n0.63\n0.29\n0.63\n0.77\n0.4\n0.6\n1\nTyler, the Creator production\ndiscography\\n\\n\nMetro\nBoomin\nproduction\ndiscography\\n\\n\n0.44\n0.29\n0.44\n0.77\n0.8\n1\n1\nbiologist \\n Stanley Fields (actor)\n(\n:\\n\\n John Allen (actor) (born\n0.25\n0\n0.25\n0.78\n0.6\n0.8\n0.8\n16\n\" species within the genus Conus,\nthese snails are predatory and\nvenomous.\"\nspecies within the genus Conus,\nthese snails are predatory and\nvenomous.\n1\n1\n1\n1\n1\n1\n1\nList of shipwrecks in September\n1842\\n\\nThe list of ship\nList of shipwrecks in January\n1840\\n\\nThe list of ship\n0.8\n0.67\n0.8\n0.99\n0.8\n0.8\n1\n2017 NCAA Division I Softball\nTournament\\n\\nThe 2017 NCAA\nDivision I\n2018 NCAA Division I Women’s\nSoccer Tournament\\n\\nThe 2018\nNCAA Division\n0.61\n0.38\n0.61\n0.88\n0.8\n0.6\n0.8\nRob James\\n\\nRob James may re-\nfer to:\\n\\nRob James (singer) (\nMark Jones\\n\\nMark Jones may\nrefer to:\\n\\nMark Jones (singer)\n(\n0.4\n0.22\n0.4\n0.96\n0.8\n1\n1\nQi Yuwu, Julian Hee, Jeanette\nAw, Felicia Chin,\n, Pierre Png, Chen Hanwei, Feli-\ncia Chin, Fann Wong\n0.25\n0.14\n0.25\n0.82\n0.8\n1\n1\n32\n1825 in Wales\\n\\nThis article is\nabout the particular significance\nof the year 1825 to Wales and its\npeople.\\n\\nIncumbents\\nPrince\nof Wales \\u2013\n1840 in Wales\\n\\nThis article is\nabout the particular significance\nof the year 1840 to Wales and its\npeople.\\n\\nIncumbents\\nPrince\nof Wales \\u2013\n0.91\n0.86\n0.91\n0.99\n0.8\n0.8\n1\nList of European Championships\nrecords in swimming\\n\\nThe Eu-\nropean Championships records\nin swimming are the fastest\ntimes ever swum in European\nSwimming\nChampionships’\nevents.\\n\\nLong course (50\nList of European records in\nswimming\\n\\nThe following are\nthe current European records\nin swimming,\nas recognized\nby LEN.\\n\\nLong course (50 m\npool)\\n\\nShort course (\n0.6\n0.39\n0.53\n0.75\n0.6\n0.8\n1\nInstitute of Higher National De-\nfence Studies.\\n\\nThen, he went\nto the USA for a Master in Inter-\nnational Business Management\nat the University of Florida. He\nbegan his career in\n.\\n\\nHe then studied at the Insti-\ntut d’\\u00e9tudes politiques de\nParis and at the \\u00c9cole na-\ntionale d’administration (ENA).\nHe started his career in\n0.42\n0.12\n0.3\n0.71\n0.2\n0.6\n0.8\nWilliam Robertson Nicoll\\n\\nSir\nWilliam Robertson Nicoll CH\nLLD (10 October 1851 \\u2013\n4 May 1923) was a Scottish Free\nChurch\nJohn Duncan (minister)\\n\\nJohn\nDuncan (1 May 1845 \\u2013 6\nMarch 1914), also known as J.\nDuncan, was a Free\n0.21\n0.05\n0.21\n0.74\n0.8\n0.8\n0.8\n11\n"}, {"page": 12, "text": "Alessandro\nGuicci-\noli\\n\\nAlessandro\nGuiccioli\n(March 5, 1843 \\u2013 October\n3, 1922)\nGustave\nde\nBeau-\nmont\\n\\nGustave de Beaumont\n(March 1, 1840 \\u2013 March 2,\n1921) was\n0.09\n0\n0.09\n0.83\n0.6\n0.4\n0.8\n64\nRush Hour 2\\n\\nRush Hour 2\nis a 2001 American action com-\nedy film directed by Brett Ratner\nand written by Jeff Nathanson,\nbased on the characters created\nby Ross LaManna. A sequel to\nRush Hour, it is the second in-\nstallment in the Rush Hour series\nand stars Jackie Chan,\nRush Hour 2\\n\\nRush Hour 2\nis a 2001 American buddy cop\naction comedy film directed by\nBrett Ratner and written by Jeff\nNathanson. It is the sequel to the\n1998 film Rush Hour and the sec-\nond installment in the Rush Hour\nfilm series. The film stars Jackie\nChan,\n0.84\n0.67\n0.78\n0.91\n0.8\n1\n1\nhull length of, a beam of, a height\nof, and a draught of. The sub-\nmarine was powered by two Ger-\nmaniawerft F46 four-stroke, six-\ncylinder supercharged diesel en-\ngines producing a total of for use\nwhile surfaced, two AEG GU\n460/8\\u201327 double-\n. The U-boat had a displacement\nof when at the surface and while\nsubmerged. The U-boat had a\ntotal length of, a pressure hull\nlength of, a beam of, a height of,\nand a draught of. The submarine\nwas powered by two Germaniaw-\nerft F46 six-\n0.61\n0.48\n0.49\n0.8\n0.4\n1\n1\nthe same rights of audience as\nmembers of the Bar of Northern\nIreland.\\n\\nThe Advocate Gen-\neral was created as a separate of-\nfice upon the devolution of polic-\ning and justice powers to the\nNorthern Ireland Assembly on 12\nApril 2010.\\n\\nUnlike the Advo-\ncate General for Scotland, the po-\nsition is not supported by a dis-\ntinct government department.\nthe Scottish Parliament. The of-\nfice was created in 1999, and is\nthe equivalent of the Parliamen-\ntary Under-Secretary of State\nin the United Kingdom Govern-\nment.\\n\\nThe office is not a min-\nisterial post, and the holder is not\na member of the Scottish Gov-\nernment. Responsibility for the\noffice is held by the Scottish Sec-\nretary.\n0.41\n0.08\n0.24\n0.66\n0.4\n0.4\n0.8\nproducer Thom Wilson and re-\nleased in 1982 as catalog num-\nber VIRUS 10. Singer Jack Gr-\nisham credited himself as Jack\nLadoga on the sleeve, following\na tradition of using a different\npseudonym on each release both\nto confuse audiences and to hide\nhis true identity from the police.\nDrummer Todd Barnes credited\nhimself\nthe band’s first album, and the\nfirst to feature the band’s new\nlineup. The band members used\npseudonyms on the album, with\nthe exception of guitarist and\nvocalist John \\\"Baz\\\" Bascaran,\nwho used his real name because\nhe was the only member of\nthe band with a driver’s license.\nDrum\n0.21\n0.04\n0.15\n0.59\n0.8\n0.6\n0.8\nLinux kernel.\\n\\nXC3018\\nIt is a\nvariant that only supports analog\nreception and DVB-T digital re-\nception.\\n\\nTechnical specifica-\ntion\\nOutline Dimensions: 7 x\n7 x 0.85\\u00a0mm\\nSupply Volt-\nage (DC): 1.8V, 3.3V\\nSystem\nsetting time:\n2010.\\n\\nSpecifications\\nFrequency:\n2.4\\u00a0GHz\\nData\nrate:\n1,\n2,\n5.5,\n11\\u00a0Mbps\\nModulation:\nDSSS\\nPower\nconsumption:\n0.1\\u00a0W\\nOperating\ntem-\nperature:\n0\\u00a0\\u00b0C to\n70\\u00a0\\u00b0C\\n\n0.09\n0\n0.09\n0.62\n0\n0\n0.8\n12\n"}, {"page": 13, "text": "B\nHuman Sanity Check\nTo assess the reliability of LLM-as-a-judge eval-\nuations, we conducted a human sanity check by\nasking a graduate student to rate the inverted se-\nquences of out-of-distribution (OOD) clinical notes\nfollowing the same instructions provided to GPT-\n4.1-mini. The results are presented in Table 4. For\nconvenience, all scores were normalized from a\n0–5 scale to a 0–1 range. As shown in the table,\nthe LLM tends to assign lower scores for sentence\nstructure but higher ones for topic relevance, while\nthe entity scores are closely aligned. Overall, the\ndiscrepancy between the LLM and human ratings\nremains within a reasonable range.\nTable 4: Comparison of mean semantic scores between\nhuman and GPT-4.1-mini\nStructure\nEntity\nTopic\nGPT-4.1-mini\n0.12\n0.28\n0.57\nStudent\n0.23\n0.29\n0.36\nC\nMore Implementation Details\nThroughout our experiments, we use two NVIDIA\nA100 GPUs (80 GB each) for training, except for\nthe experiment on inverting 64-token sequences,\nwhere four A100 GPUs are utilized. Most training\nruns are completed within seven hours. For the\nablation studies on the number of projected token\nvectors, the number of projected tokens varies ac-\ncording to the experimental settings. In all other\ncases, the number of projected tokens is kept iden-\ntical to the number of tokens being inverted, which\nhas been shown to yield optimal performance (see\nAppendix D.3). In addition, the hidden expansion\nfactor is fixed at 0.5 across all experiments, except\nin the ablation studies analyzing the relationship be-\ntween inversion performance and hidden expansion\nfactors.\nDuring training, the batch size is fixed at 1028,\nand all experiments are conducted for three epochs.\nThe warmup ratio for adapter-only fine-tuning is set\nto 0.15. We adopt a cosine learning rate scheduler.\nThe dropout rate after the first layer of the adapter\nis fixed at 0.1, the weight decay is set to 0.01, and\nthe label smoothing factor is fixed at 0.075.\nD\nDefinition of ROUGE Score\nD.1\nROUGE-1 and ROUGE-2\nFor ROUGE-k, it computes the F-measure of k-\ngrams extracted from a sequence. Suppose the set\nof k-grams is denoted as Gk(S) and Gk(ˆk) for\nground-truth sequence and inverted sequence re-\nspectively. ROUGE-k is computed as follows:\nOverlapk =\nX\ng∈Gk(S)∩Gk( ˆS)\nmin\n\u0000cntS(g), cnt ˆS(g)\n\u0001\nRk = Overlapk\n|Gk(S)| ,\nPk = Overlapk\n|Gk( ˆS)|\nROUGEk = (1+β2)RkPk\nRk+β2Pk\n(5)\nwhere cnt(·) denotes the count of the set.\nD.2\nROUGE-L\nGive an ground-truth sequence S = ⟨s1, . . . , sn⟩\nand an inverted sequence ˆS = ⟨ˆs1, . . . , ˆsm⟩, the\nlength of their longest common subsequence is\nLCS(S, ˆS). The ROUGE-L score FLCS is defined\nas follows:\nRLCS = LCS(S, ˆS)\nm\n,\nPLCS = LCS(S, ˆS)\nn\nFLCS = (1+β2)RLCSPLCS\nRLCS+β2PLCS\n(6)\nD.3\nAblation Study\nIn this subsection, we explore an optimal train-\ning strategy to enhance the performance of the\ninverter. First, we present two training schemes\ninvolving pretraining and finetuning, which demon-\nstrate slight improvements with finetuning. Then,\nwe examine the variations of the adapter structure\non inverter’s performance. Since the adapter is a\ntwo-layer MLPs, we vary the output dimensions\nof the first layer and the second layers to provide\ninsights on structure configuration.\nTwo Training Schemes.\nWe first study the per-\nformance of inverters under two training schemes\nto gain knowledge on when representation space\nand token embedding space are aligned. One train-\ning scheme only finetunes the adapters, the other\ncontinually finetunes the adapter and decoding lan-\nguage model together after finetuning the adapter,\nwhere the adapter is finetuned fully and decoding\nlanguage model is finetuned using LoRA.\nWe train an adapter to invert 16-tokens input se-\nquences first for pretraining scheme. Based on the\ntrained adapter, we further finetune both the adapter\nand inverter’s base model. In Figure 6a, whiskers\n13\n"}, {"page": 14, "text": "ROUGE-1\nROUGE-2\nROUGE-L\nBERTScore F1\nMetrics\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScore\nPretrain\nFinetune\nMean\n(a) Score distribution\n0 1 2 3 4 5\nStructure Score\n0\n200\n400\n600\nFrequency\nPretrain\nFinetune\n0 1 2 3 4 5\nEntity Score\n0\n200\n400\n600\nPretrain\nFinetune\n0 1 2 3 4 5\nTopic Score\n0\n200\n400\n600\n800\n1000\nPretrain\nFinetune\n(b) Score Frequency\nFigure 6: Pretrain vs Finetune performance comparison\n0.5x\n1x\n2x\n4x\n8x\nAdapter Hidden Expansion Factor f\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScore (0-1)\nPerformance on Adapter Hidden Expansion Factors f\nMetrics\nROUGE-1\nROUGE-2\nROUGE-L\nStructure\nEntity\nTopic\nBERTScore F1\nMetrics\nROUGE-1\nROUGE-2\nROUGE-L\nStructure\nEntity\nTopic\nBERTScore F1\n(a)\n1\n2\n4\n8\n16\n32\nNumber of Projected Token Embeddings k\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScore (0-1)\nPerformance on Number of Projected Token Embeddings k\nMetrics\nROUGE-1\nROUGE-2\nROUGE-L\nStructure\nEntity\nTopic\nBERTScore F1\nMetrics\nROUGE-1\nROUGE-2\nROUGE-L\nStructure\nEntity\nTopic\nBERTScore F1\n(b)\nFigure 7: Inversion Performance on varying expansion factors\nrepresent 5% to 95% respectively. Among token-\nlevel scores, finetune is better than pretrain only.\nFor semantic-level scores, finetune can slightly. im-\nprove the structure recovery. But for entity and\ntopic, BERTScore, entity, and topic score almost\nmake even. Consequently, finetune can slightly\nhelp improve alignment but most alignments are\nreached during pretraining stage. In our following\nexperiments, we focus on pretraining scheme only\nto examine inverter’s performance.\nHidden Expansion Factor.\nAblation experi-\nments on output dimensions of adapters have been\ndone separately with both layers of adapters. We\ntrain adapters on inverting 16-token sequences at\n10th layer of Llama-3.1-8B. As defined in § 2.2, for\nthe 1st layer of the adapter, the output dimension\ndhid = f·d is scaled by f according to input dimen-\nsion. To examine how the variation of f influence\nthe inversion performance, we sample f to be 0.5,\n1, 2, 4, 8 and fix the output dimension of the 2nd\nlayer. As show in Figure 7a, the inversion perfor-\nmance is robust to variations of hidden dimensions\nbetween 1st and 2nd layer. Semantic-level recovery\nis barely impacted by variations, while token-level\naccuracy (ROUGE-1) improves slightly as hidden\ndimensions scale up. This could attribute to better\nmemory ability as the adapter scales up.\nFor the second layer of the adapter, we set the\noutput dimensions to be k · d′ where d′ is the hid-\nden dimensions of the inverter model. We hope\nthe projected token embeddings capture as much\ninformation as possible from the last-token repre-\nsentation. Under 16-token sequence inversion task,\nwe attempt to set k to be 1, 2, 4, 8, 16, 32 to study\nwhether more projected token can help extract more\ninformation from the token representation. As illus-\ntrated in Figure 7b, the inversion performance on\nboth token-level and semantic-level improves with\nthe scaling up of k. However, when k = 32 ex-\nceeds the number of inverted tokens, the inversion\nperformance degrades compared to k = 16. This\ncould come from repetitive information introduced\nby additional token embeddings.\nTo sum up, the expansion factor in the 1st layer\n14\n"}, {"page": 15, "text": "of the adapter has trivial influence on inversion per-\nformance. However, the inversion can reach its\nbest performance when the number of projected\ntoken embeddings equals to that of ground-truth se-\nquence. In the following section, we set expansion\nfactor to be 0.5 and number of projected token em-\nbedding to be that of ground-truth tokens according\nto the observation.\nE\nInstructions for Clinical Notes\nGenerate 100 short sentences about fictional patients, mim-\nicking the style of the MIMIC-III clinical dataset. Each\nsentence should describe a patient and mention at least\none of the following:\n• name (any first name or initial)\n• disease or diagnosis\n• symptom(s)\n• age, date of birth, or admission date (optional)\nRequirements:\n• Each sentence ≤12 words\n• Diverse grammatical forms: clinical notes, sum-\nmaries, shorthand/abbreviations, fragments, clauses,\nsnippets\n• Realistic medical language, no real patient data\n• Randomized structure, tone, and detail level\nExample styles:\n• “John D., 56, admitted 04-12-2009 for chest pain.”\n• “Pt c/o dizziness and nausea; hx of diabetes.”\n• “Admitted 02/17/10 — fever, hypotension, suspected\nsepsis.”\n• “Mary T, DOB 1982-11-09. COPD exacerbation.\nStable overnight.”\n• “Elderly male, HTN, SOB on exertion.”\nOutput Exactly 100 unique sentences.\nF\nScoring Prompts\nWe include prompts for GPT-4.1-mini as below.\nStructure Score\nYou are given two sentences:\n[GT]: self.gt_sen\n[GEN]: self.gen_sen\nEvaluate Structural Frame Similarity. Focus on whether\n[GEN] preserves the core grammatical structure and sen-\ntence skeleton of [GT]: same basic clause structure, same\nnumber/type of major phrases, similar syntactic relation-\nships, consistent verb tense/aspect.\nScoring:\n5: Identical structure — same words in same order\n4: Nearly identical — same pattern with entity substitu-\ntions or minor reordering\n3: Moderately similar — core structure maintained but\nnotable changes (e.g., active to passive)\n2: Somewhat similar — recognizable elements but sig-\nnificant differences\n1: Minimally similar — only basic sentence type\nmatches\n0: Completely different structures\nAnswer: [ANS] structure: [score]/5\nEntity Score\nYou are given two sentences:\n[GT]: self.gt_sen\n[GEN]: self.gen_sen\nEvaluate Entity Preservation. Focus on whether [GEN]\npreserves key entities (people, places, organizations) from\n[GT]: same named entities, same key objects/concepts,\nequivalent entities in corresponding roles, preservation of\nentity relationships.\nScoring:\n5: All entities preserved — all key entities from GT\nappear in GEN with same references\n4: Nearly all preserved — minor omissions of non-\ncritical entities or slight variations\n3: Most preserved — majority of key entities main-\ntained, some important ones missing/substituted\n2: Some preserved — recognizable overlap but signifi-\ncant differences in key entities\n1: Few preserved — minimal overlap, only generic cat-\negories match\n0: No overlap — completely different entities\nAnswer: [ANS] entity: [score]/5\nTopic Score\nYou are given two sentences:\n[GT]: self.gt_sen\n[GEN]: self.gen_sen\nEvaluate Topic Consistency. Focus on whether [GEN]\nmaintains the same main subject/topic as [GT]: same pri-\nmary entity/concept, same domain/field, same general sub-\nject matter, maintains relevance to original topic.\nScoring:\n5: Identical topic — exactly the same specific\ntopic/entity with same focus\n4: Highly similar — same main topic with slightly dif-\nferent aspects/perspectives\n3: Related topic — closely related subjects within same\ndomain/category\n2: Loosely related — some connection but notably dif-\nferent topics/focuses\n1: Minimally related — tangentially connected or only\nshares broad category\n0: Unrelated — completely different subjects with no\nmeaningful connection\nAnswer: [ANS] topic: [score]/5\n15\n"}]}