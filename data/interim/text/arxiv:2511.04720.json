{"doc_id": "arxiv:2511.04720", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.04720.pdf", "meta": {"doc_id": "arxiv:2511.04720", "source": "arxiv", "arxiv_id": "2511.04720", "title": "Learning to reason about rare diseases through retrieval-augmented agents", "authors": ["Ha Young Kim", "Jun Li", "Ana Beatriz Solana", "Carolin M. Pirkl", "Benedikt Wiestler", "Julia A. Schnabel", "Cosmin I. Bercea"], "published": "2025-11-06T10:27:52Z", "updated": "2025-11-06T10:27:52Z", "summary": "Rare diseases represent the long tail of medical imaging, where AI models often fail due to the scarcity of representative training data. In clinical workflows, radiologists frequently consult case reports and literature when confronted with unfamiliar findings. Following this line of reasoning, we introduce RADAR, Retrieval Augmented Diagnostic Reasoning Agents, an agentic system for rare disease detection in brain MRI. Our approach uses AI agents with access to external medical knowledge by embedding both case reports and literature using sentence transformers and indexing them with FAISS to enable efficient similarity search. The agent retrieves clinically relevant evidence to guide diagnostic decision making on unseen diseases, without the need of additional training. Designed as a model-agnostic reasoning module, RADAR can be seamlessly integrated with diverse large language models, consistently improving their rare pathology recognition and interpretability. On the NOVA dataset comprising 280 distinct rare diseases, RADAR achieves up to a 10.2% performance gain, with the strongest improvements observed for open source models such as DeepSeek. Beyond accuracy, the retrieved examples provide interpretable, literature grounded explanations, highlighting retrieval-augmented reasoning as a powerful paradigm for low-prevalence conditions in medical imaging.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.04720v1", "url_pdf": "https://arxiv.org/pdf/2511.04720.pdf", "meta_path": "data/raw/arxiv/meta/2511.04720.json", "sha256": "2f8f2b31bcfabe2e40e3bb5f6a025d7c4fb794d6429904b53aa5b9f9239a8fee", "status": "ok", "fetched_at": "2026-02-18T02:28:24.442745+00:00"}, "pages": [{"page": 1, "text": "LEARNING TO REASON ABOUT RARE DISEASES\nTHROUGH RETRIEVAL-AUGMENTED AGENTS\nHa Young Kim1, 2, Jun Li1, 3, Ana Beatriz Solana2, Carolin M. Pirkl2, Benedikt Wiestler1, 3, 4,\nJulia A. Schnabel1, 3, 5, 6, *, Cosmin I. Bercea1, 3, *, On behalf of the PREDICTOM consortium\n1Technical University of Munich, Munich, Germany\n2GE HealthCare, Munich, Germany\n3Munich Center for Machine Learning, Munich, Germany\n4Klinikum Rechts der Isar, Munich, Germany\n5Helmholtz Munich, Munich, Germany\n6King’s College London, London, United Kingdom\nABSTRACT\nRare diseases represent the long tail of medical imaging,\nwhere AI models often fail due to the scarcity of represen-\ntative training data. In clinical workflows, radiologists fre-\nquently consult case reports and literature when confronted\nwith unfamiliar findings. Following this line of reasoning,\nwe introduce RADAR (Retrieval-Augmented Diagnostic Rea-\nsoning Agents), an agentic system for rare disease detection\nin brain MRI. Our approach uses AI agents with access to\nexternal medical knowledge by embedding both case re-\nports and literature using sentence transformers and index-\ning them with FAISS to enable efficient similarity search.\nThe agent retrieves clinically relevant evidence to guide di-\nagnostic decision-making on unseen diseases, without the\nneed of additional training. Designed as a model-agnostic\nreasoning module, RADAR can be seamlessly integrated\nwith diverse large–language models, consistently improving\ntheir rare pathology recognition and interpretability. On the\nNOVA dataset comprising 280 distinct rare diseases, RADAR\nachieves up to a 10.2% performance gain, with the strongest\nimprovements observed for open-source models such as\nDeepSeek. Beyond accuracy, the retrieved examples provide\ninterpretable, literature-grounded explanations, highlighting\nretrieval-augmented reasoning as a powerful paradigm for\nlow-prevalence conditions in medical imaging.\nCode and\nDetails: https://anonymous.4open.science/r/RADAR-3232\nIndex Terms— medical imaging, brain disorders, disease\ndiagnosis, agentic AI, retreival augmented generation\n1. INTRODUCTION\nRare brain disorders collectively affect millions worldwide,\nyet their diagnosis remains notoriously difficult due to the\nscarcity of expert knowledge, the heterogeneity of clinical\npresentations, and the extremely low prevalence of individual\n* Co-Senior authors\nFig. 1:\nOverview of the proposed RADAR (Retrieval-\nAugmented Diagnostic Reasoning Agents) framework. The\nsystem employs coordinated agents that retrieve and integrate\nmedical knowledge from external text-based databases to sup-\nport diagnostic reasoning on rare diseases, achieving up to a\n10.2% accuracy gain over non-agentic baselines.\nconditions. These challenges often result in misdiagnosis, de-\nlayed interventions, and inappropriate treatments, underscor-\ning the need for diagnostic systems that are not only accurate\nbut also interpretable and evidence-grounded [1].\nRecent advances in artificial intelligence (AI), especially\nlarge language models (LLMs) and agentic AI systems, have\ndemonstrated high potential in complex reasoning and clin-\nical decision support across medical domains [2, 3]. How-\never, the current high-performing LLM-based systems oper-\nate as closed models and lack domain-specific medical train-\ning. Consequently, this often leads to misdiagnoses or hallu-\ncinated recommendations in clinical settings [4].\nRetrieval-augmented generation (RAG) addresses these\nlimitations by combining the generative reasoning capabil-\nities of LLMs with real-time access to sources of domain-\nspecific knowledge [5, 6].\nThis paradigm has a notable\narXiv:2511.04720v1  [cs.CL]  6 Nov 2025\n"}, {"page": 2, "text": "resemblance to clinical workflows: when radiologists en-\ncounter unfamiliar findings, they consult case reports and\nliterature to guide their reasoning. Inspired by this clinical\nworkflow, we propose RADAR (Retrieval-Augmented Di-\nagnostic Reasoning Agents), an agentic system for retrieval-\naugmented diagnostic reasoning that can dynamically retrieve\nrelevant medical evidence during inference, without requir-\ning additional fine-tuning or domain-specific retraining. This\napproach enhances diagnostic accuracy while simultaneously\nmitigating hallucinations and ensuring evidence-based out-\nputs.\nBy explicitly linking model decisions to supporting\nevidence, RADAR reduces hallucinations and enables trans-\nparent, literature-backed decisions.\nRADAR employs a set of coordinated agents that (i) re-\ntrieve semantically relevant case reports and literature from\nexternal sources, (ii) ground diagnostic reasoning on re-\ntrieved evidence, and (iii) synthesize interpretable diagnostic\nhypotheses. Our main contributions are:\n• We introduce RADAR, a retrieval-augmented, model-\nagnostic reasoning framework that integrates radiological\nunderstanding from brain MRI with external medical\nknowledge, improving diagnostic accuracy and inter-\npretability without additional training.\n• We conduct a comprehensive evaluation on the NOVA\ndataset covering 280 rare brain diseases, demonstrating\nthat retrieval-based reasoning consistently improves both\ndiagnostic accuracy and interpretability across diverse vi-\nsion–language models, establishing a scalable path toward\ntrustworthy AI in data-scarce medical imaging.\n2. METHOD\nWe propose RADAR(Retrieval-Augmented Diagnostic Rea-\nsoning Agents), a system designed to improve diagnostic rea-\nsoning for rare diseases by integrating multi-agent collabora-\ntion with retrieval-augmented generation as illustrated in Fig-\nure 2 (d). RADAR iteratively generates diagnostic hypothe-\nses, retrieves external medical knowledge, and refines diag-\nnosic conclusions. The framework comprises three special-\nized agents: an initial doctor agent, a retrieval agent, and a fi-\nnal doctor agent, that collaborate to produce an interpretable,\nevidence-grounded diagnosis.\n2.1. Initial Doctor Agent\nThe initial doctor agent is implemented as a large language\nmodel (LLM) prompted to act as a diagnostic expert. It takes\nthe MRI image findings (image caption) and patient clinical\nhistory data as input and produces a list of ten candidate diag-\nnoses:\nfinit : C = (Image caption, Clinical data) 7→{di}10\ni=1.\nTo promote diagnostic diversity, the underlying LLM is con-\nfigured with a high temperature and top-p sampling. This\nstage provides a broad but plausible hypothesis to guide fur-\nther reasoning.\n2.2. RAG agent\nThe RAG agent assists the diagnostic reasoning process by\ngenerating targeted queries, retrieving external evidence, and\nsynthesizing contextual answers. It operates in three stages:\n1. Query generation. The query generation system takes the\nimage caption and patient clinical data as input and uses an\nLLM to generate a set of question-keyword pairs:\nfLLM : C = (Image caption, Clinical data) 7→{(qi, ki)}n\ni=1\nwhere qi is a question designed to enhance diagnostic rea-\nsoning, and ki is the corresponding keyword extracted from\nqi. To generate a wide variety of search queries helpful for\ndiagnosis, the LLM is set with a high temperature and top-p.\n2. Knowledge retrieval and indexing. The knowledge re-\ntrieval and indexing system retrieves relevant information for\neach keyword ki as follows:\n• Internal check: If information about the keyword ki exists\nin the internal knowledge base, it retrieves relevant docu-\nments. Otherwise, the system updates its knowledge base\ndynamically by constructing a new index through the re-\ntrieval of new data from an external source.\n• External retrieval: The agent queries Radiopaedia [7] for\neach keyword ki, retrieving 10 relevant documents, 5 from\nthe articles section and 5 from the cases section. Each re-\ntrieved document is segmented into overlapping chunks\n{ci,j}Mi\nj=1. These are embedded into dense vector embed-\ndings using using the all-MiniLM-L6-v2 sentence trans-\nformer model (gembed) [8].\nvi,j = gembed(ci,j),\nvi,j ∈Rd.\nThen, these embeddings are stored in the internal knowl-\nedge base as a FAISS index [9] to enable efficient similar-\nity search, i.e., cosine similarity:\nI ←I ∪{vi,j}Mi\nj=1.\n3. Answer generation. For each question qi, the agent re-\ntrieves the top-k most relevant chunks based on cosine simi-\nlarity:\nRi = Top-k(vqi, I, k = 5),\nwhere vqi = gembed(qi). Using this, an LLM analyzes the re-\ntrieved content and generates a concise answer to the question\nqi based on the retrieved evidence. We configure this LLM\nwith a low temperature to generate an answer based solely on\nthe retrieved content.\n"}, {"page": 3, "text": "(a) Single agent\n(b) Collaborative system\n(c) Challenger system\n(d) RADAR system (ours)\nFig. 2: Comparison of multi-agent diagnostic reasoning setups. (a) A single-agent system: a single doctor agent generates\na diagnosis. (b) Collaborative system: agents exchange independent diagnoses and reach a consensus through discussion\nrounds. (c) Challenger system: one agent introduces adversarial information to test the robustness of others, and (d) RADAR\n(ours): retrieval-augmented framework where agents access external medical knowledge via Radiopaedia to refine and ground\ndiagnostic reasoning.\n2.3. Final doctor agent\nThe final doctor agent integrates all the available data includ-\ning image captions, clinical information, retrieved knowl-\nedge, and the candidate diagnosis, to produce one primary\ndiagnosis and four differential diagnoses:\nffinal : (C, {di}, R) 7→Dfinal = {dprimary, d(1−4)\ndiff\n}.\nThis agent operates at a mid-range temperature setting to bal-\nance reasoning flexibility with factual consistency. By ex-\nplicitly conditioning on retrieved evidence, it produces inter-\npretable, literature-grounded diagnostic outputs.\n3. EXPERIMENTS\nOur experiments aim to assess three key aspects of RADAR:\n(1) whether retrieval-augmented reasoning improves diagnos-\ntic accuracy for rare brain diseases, (2) whether the system\ngeneralizes across diverse large language models (LLMs),\nand (3) whether the retrieved evidence enhances interpretabil-\nity and trustworthiness.\n3.1. Dataset and Metrics\nWe evaluate RADAR on the publicly available NOVA dataset\n[10], which includes around 900 brain MRI scans spanning\n281 rare pathologies and multiple acquisition protocols. Each\ncase provides patient clinical information and an expert-\nwritten caption describing the imaging findings. To mitigate\npotential bias toward single phrasing we paraphrased each\ncaption into four alternative formulations using GPT-4o.\nWe measure the diagnostic performance using Top-1 and\nTop-5 accuracy, following the evaluation protocol described\nin the original NOVA paper [10]. Top-1 indicates exact agree-\nment with the ground-truth diagnosis, while Top-5 considers\nwhether the correct diagnosis appears among the five most\nlikely predictions. Because medical terminology may differ\nacross sources, each prediction is normalized via GPT-4o to\nalign synonyms and variant expressions.\n3.2. Baselines and Model Configurations\nTo ensure a fair and comprehensive comparison, we evaluate\nmultiple reasoning paradigms across both closed- and open-\nsource LLMs. Specifically, we test two proprietary models\n(GPT-4o [11] and Gemini-2.0-Flash [12]) and three open-\nsource models (Qwen3-32B [13], DeepSeek-R1-70B [14],\nand MedGemma-27B [15]).\nEach model is evaluated under three reasoning setups: a\nsingle-agent system (Figure 2a), a collaborative multi-agent\nsystem (Figure 2b), and a challenger multi-agent system (Fig-\nure 2c). In the single-agent system an LLM generates a diag-\nnostic output directly from the patient information. The col-\nlaborative system involves three independent doctor agents\nwho provide initial diagnoses and subsequently engage in it-\nerative discussions to reach consensus. In contrast, the chal-\nlenger system introduces an adversarial agent that challenges\nthe reasoning of the doctor agents. RADAR extends these se-\ntups by integrating retrieval-augmented reasoning, allowing\nagents to access external medical knowledge to ground and\nrefine their diagnostic decisions as shown in Figure 2d. Im-\nplementation details and prompts are available in the reposi-\ntory.\n"}, {"page": 4, "text": "Fig. 3: Examples of the results generated by our RADAR system. The ground-truth diagnosis is marked with bold.\n4. RESULTS AND DISCUSSION\n4.1. Results\nWe evaluate RADAR against the single-agent, collaborative,\nand challenger multi-agent configurations. Tables 1 and 2\nsummarize Top-1 and Top-5 diagnostic accuracy across the\nfive LLM backbones. RADAR consistently outperforms all\nbaselines across models and metrics. For instance, it achieves\nup to +7.97 Top-1 improvement with Qwen3-32B and +10.19\nTop-5 improvement with DeepSeek-R1-70B over the single-\nagent baseline. Performance gains are especially pronounced\nfor open-source models, suggesting that retrieval-augmented\nreasoning compensates for smaller model capacity and lim-\nited medical pretraining. The collaborative and challenger\nsetups provide limited or inconsistent gains over the single-\nagent baseline, and in some cases, even degrade performance.\nThis suggests that multi-agent interaction can amplify diag-\nnostic uncertainty, particularly when the agents lack domain-\nspecific knowledge. Our best performance was achieved us-\ning GPT-4o on our RADAR method, reaching Top-1 accu-\nracy of 54.40% and Top-5 accuracy of 75.05%.\nFor con-\ntext, the NOVA paper [10] reports resident neuroradiologist\nperformance of 48–52% Top-1 and 68–76% Top-5 accuracy,\nmeasured on a 25-case subset. RADAR achieves compara-\nble accuracy evaluated across the full dataset, underscoring\nits potential of diagnostic reasoning agentic system in clini-\ncal settings. Nevertheless, RADAR still relies on radiologist-\nprovided captions. Bridging this gap from textual reasoning\nto direct visual understanding remains an open challenge.\nFigure 3 shows illustrative outputs from RADAR. It\ndemonstrates how the RAG agent formulates targeted diag-\nnostic questions, retrieves relevant content from Radiopaedia,\nand generates concise evidence-based answers strictly from\nthe retrieved text. The final doctor agent integrates this infor-\nmation and gives a ranked list of differential diagnoses with\nconfidence estimates. In the second example, retrieved evi-\ndence causes RADAR to update its prediction, recovering the\nTable 1: Top 1 Accuracy comparison\nModel\nSingle-agent\nCollaboration\nChallenge\nRADAR\nGemini-2.0\n45.55 ± 1.58\n45.56 ± 1.09\n41.48 ± 1.09\n48.54 ± 0.29\nGPT-4o\n49.94 ± 0.86\n48.61 ± 1.11\n47.68 ± 2.26\n54.40 ± 1.02\nQwen3-32b\n35.19 ± 0.51\n36.85 ± 2.11\n35.07 ± 0.84\n43.16 ± 3.16\nDeepSeek-R1-70B\n35.47 ± 2.72\n38.38 ± 2.36\n37.89 ± 0.37\n41.85 ± 1.89\nMedgemma-27B\n31.00 ± 1.20\n33.64 ± 1.04\n36.06 ± 1.05\n38.23 ± 1.43\nTable 2: Top 5 Accuracy comparison\nModel\nSingle-agent\nCollaboration\nChallenge\nRADAR\nGemini-2.0\n66.21 ± 1.40\n65.09 ± 0.56\n68.98 ± 1.22\n72.10 ± 2.34\nGPT-4o\n68.10 ± 1.65\n68.74 ± 0.55\n69.29 ± 1.17\n75.05 ± 2.19\nQwen3-32b\n52.54 ± 0.39\n54.35 ± 1.23\n55.01 ± 2.45\n62.51 ± 2.71\nDeepSeek-R1-70B\n54.62 ± 2.53\n60.18 ± 2.02\n59.11 ± 1.85\n64.81 ± 1.52\nMedgemma-27B\n56.21 ± 0.54\n57.65 ± 1.36\n61.53 ± 0.78\n64.40 ± 2.11\ncorrect diagnosis that was initially ranked lower. This shows\nthe system’s capacity to adjust its reasoning by integrating\nnew clinical information.\n5. CONCLUSION\nWe introduced RADAR, a retrieval-augmented, agentic\nframework for rare disease diagnosis in brain MRI. By cou-\npling large language models with external medical knowl-\nedge, RADAR enhances diagnostic reasoning and provides\ninterpretable, evidence-grounded outputs. Our results demon-\nstrate that integrating retrieval mechanisms consistently im-\nproves diagnostic accuracy—particularly for open-source\nmodels—showing that explicit knowledge injection can com-\nplement model size. Although the current system relies on\nradiologist-provided captions rather than direct image inter-\npretation, bridging this gap between text-based reasoning\nand image-based understanding represents a key direction for\nfuture research.\n"}, {"page": 5, "text": "6. ACKNOWLEDGMENTS\nThis project is supported by the Innovative Health Initia-\ntive Joint Undertaking (IHI JU) under grant agreement No\n101132356 as part of the project PREDICTOM.\nPREDICTOM is supported by the Innovative Health Initia-\ntive Joint Undertaking (IHI JU), under Grant Agreement No\n101132356. JU receives support from the European Union’s\nHorizon Europe research and innovation programme, CO-\nCIR, EFPIA, EuropaBio, MedTechEurope and Vaccines Eu-\nrope.\nThe UK participants are supported by UKRI Grant\nNo 10083467 (National Institute for Health and Care Excel-\nlence), Grant No 10083181 (King’s College London), and\nGrant No 10091560 (University of Exeter).\nUniversity of\nGeneva is supported by the Swiss State Secretariat for Ed-\nucation, Research and Innovation Ref No 113152304. See\nwww.ihi.europa.eu for more details.”\nThis work is supported by the DAAD programme under Kon-\nrad Zuse Schools of Excellence for Reliable AI (RelAI)\nC.I.B. is funded via the EVUK program (”Next-generation AI\nfor Integrated Diagnostics”) of the Free State of Bavaria.\n7. REFERENCES\n[1] Arrigo Schieppati, Jan-Inge Henter, Erica Daina, and\nAnita Aperia, “Why rare diseases are an important med-\nical and social issue,” The Lancet, vol. 371, no. 9629,\npp. 2039–2041, June 2008.\n[2] Taeyoon Kwon, Kai Tzu-iunn Ong, Dongjin Kang, Se-\nungjun Moon, Jeong Ryong Lee, et al.,\n“Large lan-\nguage models are clinical reasoners: reasoning-aware\ndiagnosis framework with prompt-generated rationales,”\nin Proceedings of the Thirty-Eighth AAAI Conference\non Artificial Intelligence and Thirty-Sixth Conference\non Innovative Applications of Artificial Intelligence and\nFourteenth Symposium on Educational Advances in Ar-\ntificial Intelligence. 2024, AAAI’24/IAAI’24/EAAI’24,\nAAAI Press.\n[3] Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath\nRangan, and Jonathan H. Chen, “Diagnostic reasoning\nprompts reveal the potential for large language model\ninterpretability in medicine,” npj Digital Medicine, vol.\n7, no. 1, pp. 20, Jan. 2024.\n[4] Zhihong Zhu, Yunyan Zhang, Xianwei Zhuang, Fan\nZhang, et al., “Can we trust AI doctors? a survey of\nmedical hallucination in large language and large vision-\nlanguage models,” in Findings of the Association for\nComputational Linguistics: ACL 2025, Wanxiang Che\net al., Eds., Vienna, Austria, July 2025, pp. 6748–6769,\nAssociation for Computational Linguistics.\n[5] Fnu Neha, Deepshikha Bhati, and Deepak Kumar\nShukla,\n“Retrieval-augmented generation (rag) in\nhealthcare: A comprehensive review,” AI, vol. 6, no.\n9, 2025.\n[6] Omid Kohandel Gargari and Gholamreza Habibi, “En-\nhancing medical ai with retrieval-augmented generation:\nA mini narrative review,” Digital Health, vol. 11, pp.\n20552076251337177, Apr. 2025, eCollection 2025 Jan-\nDec.\n[7] Radiopaedia.org contributors,\n“Radiopaedia.org: The\npeer-reviewed collaborative radiology resource,” 2005,\nAccessed on 2025-09-30.\n[8] Sentence Transformers,\n“all-minilm-l6-v2,” 2021,\nModel card on Hugging Face.\n[9] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff\nJohnson, Gergely Szilvasy, Pierre-Emmanuel Mazar´e,\nMaria Lomeli, Lucas Hosseini, and Herv´e J´egou, “The\nfaiss library,” 2025.\n[10] Cosmin I. Bercea, Jun Li, Philipp Raffler, Evamaria O.\nRiedel, Lena Schmitzer, Angela Kurz, Felix Bitzer,\nPaula Roßm¨uller, Julian Canisius, Mirjam L. Beyrle,\nChe Liu, Wenjia Bai, Bernhard Kainz, Julia A. Schn-\nabel, and Benedikt Wiestler,\n“Nova: A benchmark\nfor anomaly localization and clinical reasoning in brain\nmri,” arXiv preprint arXiv:2505.14064, 2025.\n[11] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al., “Gpt-4 technical report,” arXiv preprint\narXiv:2303.08774, 2023.\n[12] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Bur-\nnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien\nVincent, Zhufeng Pan, Shibo Wang, et al., “Gemini 1.5:\nUnlocking multimodal understanding across millions of\ntokens of context,” arXiv preprint arXiv:2403.05530,\n2024.\n[13] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\net al., “Qwen3 technical report,” 2025.\n[14] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\net al., “Deepseek-r1: Incentivizing reasoning capability\nin llms via reinforcement learning,” 2025.\n[15] Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroen-\nsri, Atilla Kiraly, et al., “Medgemma technical report,”\n2025.\n"}]}