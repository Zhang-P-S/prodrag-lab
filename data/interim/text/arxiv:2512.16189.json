{"doc_id": "arxiv:2512.16189", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.16189.pdf", "meta": {"doc_id": "arxiv:2512.16189", "source": "arxiv", "arxiv_id": "2512.16189", "title": "Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation", "authors": ["Musarrat Zeba", "Abdullah Al Mamun", "Kishoar Jahan Tithee", "Debopom Sutradhar", "Mohaimenul Azam Khan Raiaan", "Saddam Mukta", "Reem E. Mohamed", "Md Rafiqul Islam", "Yakub Sebastian", "Mukhtar Hussain", "Sami Azam"], "published": "2025-12-18T05:23:47Z", "updated": "2025-12-19T06:34:23Z", "summary": "In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.16189v2", "url_pdf": "https://arxiv.org/pdf/2512.16189.pdf", "meta_path": "data/raw/arxiv/meta/2512.16189.json", "sha256": "bdd9ae6e9c56b119c8478449f17ec18b32f53afd2f187c6a1e3e05b24a6297c2", "status": "ok", "fetched_at": "2026-02-18T02:24:09.127338+00:00"}, "pages": [{"page": 1, "text": "Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking\nand Domain-Specific Adaptation\nMusarrat Zeba1,2, Abdullah Al Mamun1,2, Kishoar Jahan Tithee1,3, Debopom Sutradhar1,2,\nMohaimenul Azam Khan Raiaan1,2,4,*, Saddam Mukta5, Reem E. Mohamed6,\nMd Rafiqul Islam7, Yakub Sebastian7, Mukhtar Hussain6, Sami Azam7,*\n1Applied Artificial Intelligence and INtelligent Systems (AAIINS) Laboratory, Dhaka 1217, Bangladesh\n2Department of Computer Science and Engineering, United International University, Dhaka 1212, Bangladesh\n3Department of Computer Science and Engineering, Daffodil International University, Dhaka-1341, Bangladesh\n4Department of Data Science and Artificial Intelligence, Monash University, Clayton, VIC, 3153, Australia\n5Department of Software Engineering, Lappeenranta-Lahti University of Technology, Lappeenranta, 53850, Finland\n6Faculty of Science and Information Technology, Charles Darwin University, Sydney, NSW, Australia\n7Faculty of Science and Technology, Charles Darwin University, Casuarina, NT 0909, Australia\n*Corresponding Author: mraiaan191228@bscse.uiu.ac.bd; sami.azam@cdu.edu.au\nAbstract\nIn healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-\nmaking and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated\noutputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM,\nalong with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using\nLow-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical\ntests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to\nvalidate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For\nevaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these\nas facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally,\nthe LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.\nKeywords: Large Language Models, Hallucination Miti- gation, Clinical Text Summarization, Fact-Checking,\nDomain- Specific Adaptation\n1\nIntroduction\nThe medical sector is rapidly adopting Artificial Intelligence\n(AI) nowadays, but there are issues regarding the reliability\nof the outputs in the real world use case [1]. Large language\nmodels (LLMs) usually have great contributions in healthcare\nwhen used [2]. However, hallucinations are a major drawback\nin the use of LLMs in these certain sectors [3]. Because, as a\nsafety-critical domain, healthcare can not tolerate diagnostic\nor factual errors [4, 5, 6]. Therefore, critical areas are often\nnot encouraged to use and depend on these AI tools. Studies\nreveal frequent factual errors in LLM-generated clinical sum-\nmaries and patient reports [3]. These errors further reduce\nclinician trust and slow adoption in practice [3].\nHallucination continues to be a fundamental challenge\ndespite the rapid advancement of LLMs [7, 8, 9]. This phe-\nnomenon involves the confident creation of information that\nis either unverified or completely false [10]. It represents a sig-\nnificant hurdle for the use of LLMs in clinical practice [11]. In\nthe healthcare domain, minor inaccuracies such as incorrect\ndosage of medications, invented diagnoses, or distorted labo-\nratory values can have serious consequences for patient safety\nand disrupt clinical workflows that can drastically erode trust\nin AI-assisted decision-making [12].\nThe exciting potential and challenges of large language\nmodels (LLMs) in healthcare care are now coming to light\nthrough recent research [13]. Med-PaLM 2 has shown strong\nreasoning abilities in clinical question answering. They have\ndemonstrated exceptional performance and have showcased\nhow specialized models can help with reasoning in complex\nclinical situations [14]. To improve both the contextual ac-\ncuracy and the readability of medical texts, ClinicalGPT has\nalso been introduced. It adapts general LLMs to create radi-\nology and discharge summaries [15]. Similarly, some efforts\n1\narXiv:2512.16189v2  [cs.CL]  19 Dec 2025\n"}, {"page": 2, "text": "in medical summarization by Tang et al.\n[16], Xu et al.\n[17], and Lin et al. [1] demonstrate the utility of LLMs in\ncondensing lengthy clinical notes into concise narratives that\naid communication between healthcare providers. Hallucina-\ntion detection and factuality verification frameworks, includ-\ning CHECK [18], retrieval-augmented generation methods\n[19][20], and survey analyzes [21][22], highlight ongoing efforts\nto reduce errors and improve reliability, which eventually\nextend their focus beyond mere summarization. Therefore,\nthe key question is not whether LLMs can write convincing\nmedical text, but whether clinicians and researchers can place\ntheir trust in the results these models provide.\nDespite these advances, some crucial limitations remain.\nStudies consistently show that even leading-edge LLMs hallu-\ncinate in 2–5% of the generated medical summaries, with inge-\nnious inaccuracies often escaping the notice of the clinician[16,\n17].\nA widely cited investigation reported that more than\n40% of the summaries generated by an LLM contained fac-\ntual errors, ranging from incorrect prescriptions to fabricated\ndiagnoses [16].\nIn this work, we address these limitations by introducing\nan LLM-free fact-checking system for clinical text verifica-\ntion.\nUnlike previous approaches, our pipeline eliminates\nLLM dependency during the evaluation stage, replacing it\nwith a deterministic and transparent mechanism that com-\nbines several propositional logical consistency checks together\nto work as a whole.\nIn parallel, we fine-tuned a domain-\nspecialized generator on more than 40,000 patient records\nfrom the MIMIC-III dataset [23], from which 26,104 discharge\nsummaries were extracted for training and evaluation, us-\ning LoRA [24], a parameter-efficient fine-tuning technique\nthat injects lightweight rank-decomposition matrices into pre-\ntrained weights.\nThis approach enables effective domain\nadaptation while significantly reducing computational over-\nhead, resulting in clinically grounded summaries with fewer\nhallucinations at the generation stage.\nClinical decision-making is heavily based on accurate\nsummaries of Electronic Health Records (EHRs). Traditional\nlanguage\nmodels\ngenerate\nthese\nsummaries,\nbut\noften\nintroduce errors, such as incorrect dosages or diagnoses.\nThese errors can be risky and dangerous to patients.\nOur\napproach solves this problem by combining a specialized\nLLaMA model with a fact-checking module.\nTogether,\nthese two combine to minimize hallucinations as well as\nflag the ones that yet remain. Figure 1 shows how, in the\ntraditional method, errors, such as incorrect dosages, can\nlead to harmful clinical decisions. In contrast, our method\nensures that every fact is validated against the original EHR.\nThis allows clinicians to rely on LLM generated summaries\nand outcomes for their accurate results.\nThe main contributions of this work are as follows.\n• Introduces an LLMs-free Fact-Checking module that\napplies discrete logic to evaluate negation, implication,\nFigure 1:\nThe figure shows a comparison between\ntraditional LLM only summarization in healthcare\nand our proposed LLM with the integration of Fact-\nchecking module. The left side shows that the conven-\ntional LLM may produce hallucinated outputs, which\ncan be factually incorrect (e.g., incorrect medication\ndosage is w). These errors can lead to risky medical\ndecisions.\nOn the other hand, the right side shows\nthe illustration of the benefit of using a fine-tuned\nspecific LLM model with the fact-checking module\nthat verifies each claim against the EHR data.\ntemporal consistency, numerical checks, and cosine sim-\nilarity for assigning verdicts, systematically verifying\neach proposition claim in a summary against the cor-\nresponding patient’s EHRs with transparency and inde-\npendence from additional LLM.\n• Proposes a deterministic verification mechanism that\nbreaks down both generated summaries and EHRs into\natomic propositions allowing one-to-one comparisons\nfor independent fact-checking at a granular level.\n• Develops a domain-specific summarization model by\nfine-tuning LLaMA-3.1-8B with Low-Rank Adaptation\n(LoRA) on 26,104 MIMIC-III discharge summaries, en-\nabling efficient adaptation and generating clinically ac-\ncurate summaries with reduced hallucination rates.\n2\nRelated Work\nOver the last two decades, Natural Language Processing\n(NLP) and LLMs have advanced from rule-based systems to\nneural architectures capable of generating fluent and contex-\ntually coherent text.\nDespite these advances, their use in\nhealthcare care is limited due to concerns about reliability,\n2\n"}, {"page": 3, "text": "interpretability, and ethical issues, hallucination being a ma-\njor barrier to adoption. In this section, we present a review\nof recent studies on hallucination detection, fact verification,\nerror correction in clinical NLP and highlight approaches such\nas automated fact-checking with natural language inference\n(NLI), LLM-based and LLM-free verification pipelines, error\nmitigation strategies, and proposition-level consistency check-\ning.\n2.1\nHallucination in Medical Large Lan-\nguage Models\nLLMs have achieved great success in various natural language\ntasks.\nThis improvement helps the healthcare industry by\nproviding opportunities, such as clinical decision making and\nautomated summary processing of patient records [35]. How-\never, the implications are limited in the healthcare sector\ndue to the tendency to generate hallucinated or unsupported\nstatements [36, 37]. Several studies have focused on detecting\nor mitigating such factual inconsistencies.\nJoseph et al. [25] introduced the FACTPICO benchmark,\na framework aimed at evaluating how well medical summaries\ngenerated by LLMs like GPT-4, LLaMA-2, and Alpaca align\nwith factual information using the PICO (Population, In-\ntervention, Comparator, Outcome) structure. Their results\nshowed a trade-off between factual accuracy and linguistic\nfluency: Alpaca was more accurate but less coherent, while\nLLaMA-2 and GPT-4 were more fluent but prone to errors,\nwith hallucination rates of LLaMA-2 reaching 38%. While\nthe PICO-based evaluation aligned better with clinician judg-\nments than generic automatic metrics, the reliance on a small\nset of high-level PICO elements introduces important limita-\ntions. Many clinically relevant hallucinations, such as mis-\nstated temporal qualifiers, incorrect dosages, or omitted co-\nmorbidities may not alter the P, I, C, or O labels and therefore\nremain invisible to the benchmark. Moreover, mapping long\nfree-text summaries into discrete PICO fields is inherently\nlossy and can collapse multiple distinct factual propositions\ninto a single category, making it difficult to assess whether\neach individual statement in the summary is grounded in the\nunderlying evidence. Consequently, FACTPICO is more suit-\nable for coarse-grained evaluation of summary fidelity than for\ninstance-level inconsistency detection, which motivates finer-\ngrained, proposition-level fact-checking approaches.\nHegselmann et al.\n[38] fine-tuned LLaMA-2 and GPT-\n4 using hallucination-free training data from MIMIC-IV dis-\ncharge summaries. Their focus on data centric strategy, which\nincludes annotated datasets with token-level hallucination\nlabels and comprehensive evaluation processes, substantially\nlowered factual errors yet preserved clinical content.\nHow-\never, this approach remains primarily data-centric and does\nnot directly address independent factual verification, leaving\nresidual hallucination risks.\nGarcia-Fernandez et al. [18] presented CHECK, a contin-\nuous learning approach that combines information-theoretic\nclassifiers with specially designed clinical databases to address\nhallucinations in large models such as Llama3.3-70B-Instruct.\nWhen tested in clinical trial questions, CHECK improved the\nperformance of GPT-4o on USMLE-like standards (achieving\n92.1%), decreased hallucination rates from 31% to 0.3% and\nhad classifier AUCs of 0.95–0.96%. Regardless of its great\nreasoning abilities, Llama3.3-70B is computationally costly,\ndifficult to fine-tune, and less practical for domain-specific\nresearch due to its massive parameter size [39] [18]. Llama3-\n8B, on the other hand, serves as a fair compromise between\naccuracy and efficiency, allowing faster adaptation to spe-\ncialized medical datasets, such as MIMIC, without requiring\nunreasonably high processing resources [40].\nSawczyn et al. [32] introduced a technique known as Fact-\nSelfCheck, which is a black-box, sampling-based framework\nthat converts text into triple representations of knowledge-\ngraphs and checks for consistency between various model\noutputs.\nCompared to sentence-level methods that only\nimproved factual accuracy by 8%, this fact-level detection\nachieved a remarkable 35% increase. This black-box nature\nand the reliance on model agreement rather than grounding\nin original sources can limit its trustworthiness in clinical\nsettings.\nHallucination detection without external resources has\nbeen further advanced by recent zero-knowledge detec-\ntion frameworks like Finch-Zk [33] and Counterfactual\nProbing[34].\nFinch-Zk compares responses from multiple\nmodels for consistency to improve F1 detection by 6–39%,\nwhile Counterfactual Probing dynamically employs slightly\nmodified statements and analyzes model confidence shifts,\nwhich reduces hallucinations by about 24.5% without the\nneed for retraining. Identifying inconsistencies is the main\nfocus of both methods, rather than fixing or establishing\ntheir foundation. Therefore, they can point out differences,\nbut cannot truly guaranty that everything aligns with clinical\nfacts.\n2.2\nAutomated Fact-Checking & NLI-Based\nVerification\nAccuracy alone at the surface level does not guaranty\nthe trustworthiness of generated content.\nAutomated fact-\nchecking and Natural Language Inference (NLI) based meth-\nods are important for verifying generated information with re-\nliable sources [26], especially in medicine, where factual errors\ncan endanger patient safety. Fact-checking goes far beyond\njust creating language models. The process entails breaking\ndown the text into distinct claims and cross-referencing them\nwith organized sources of knowledge or retrieved data [41].\nThorne and Vlachos [42] provided a functional survey\nof automated fact-checking, highlighting challenges such as\n3\n"}, {"page": 4, "text": "Table 1: Comparison of prior work on hallucination detection, factuality evaluation, and medical fact-checking, highlighting task setting, granularity,\nverification mechanisms, reliance on LLMs, and limitations relative to the proposed proposition-level.\nWork (Refs)\nPrimary setting\n/ task\nGranularity\nVerification mecha-\nnism\nUse\nof\nLLMs\nin verification\nKey strengths\nMain limitations\nMaynez et al.\n[8]\nAbstractive\nsum-\nmarisation\nfaith-\nfulness (news)\nSummary–source level\nComparison\nbetween\nhuman-rated faithful-\nness\nand\nautomatic\nmetrics\nNo\nLLM-as-\nverifier\n(pre-\nLLM era)\nEstablish\n“faithfulness\nvs\nfactuality” distinction; show\nlimits of lexical overlap met-\nrics\nNon-clinical\ndomain;\nno\nproposition-level\nchecks;\nno\nEHR or structured evidence\nMed-HALT\n(Pal\net\nal.)\n[11]\nMedical\ndomain\nhallucination\ntesting for LLMs\nQuestion–answer\n/\nprompt-level\nBenchmark\nthat\nprobes hallucinations\non medical knowledge\nUses\nLLMs\nas\nthe\nevaluated\nmodels;\ndetection\nis\nvia\nbenchmark\ndesign\nMedical-focused benchmark;\nhighlights failure modes of\nLLMs on clinical knowledge\nNo explicit EHR grounding; it\ndoes not provide a verification\npipeline for real clinical docu-\nments or summaries\nCHECK\n(Garcia-\nFernandez\net al.)[18]\nContinuous\nhallucination\ndetection\nand\nelimination\nfor\nmedical LLMs\nUtterance / segment-level\nPipeline\nfor\nongoing\nhallucination\ndetec-\ntion\nand\nmitigation\naround LLM outputs\nYes (LLM-based\ncomponents\nin\nthe loop)\nMedical focus; designs a ded-\nicated\nhallucination\ndetec-\ntion framework\nRelies on LLMs and neural com-\nponents; less transparent and po-\ntentially resource-intensive com-\npared to purely symbolic verifica-\ntion\nRAG methods\n(Lewis\net\nal.,\nShuster et al.)\n[19],[20]\nKnowledge-\nintensive\nNLP\n/\nconversational\nagents\nSentence / passage-level\nRetrieval-augmented\ngeneration:\nincor-\nporate\nexternal\ndocuments\nat\ngeneration time\nYes (LLM gener-\nator + retriever)\nReduce\nhallucination\nby\ngrounding\noutputs\nin\nretrieved evidence\nDo not explicitly verify proposi-\ntions post hoc; not tailored to\nEHR structure; still rely on LLM\nbehaviour at inference time\nFACTPICO\n(Joseph et al.)\n[25]\nFactuality of med-\nical evidence sum-\nmarisation (plain-\nlanguage)\nPICO-level\n(Population,\nIntervention, Comparator,\nOutcome)\nAlign summaries with\nevidence\nvia\nPICO\nfields\nand\nPICO-\nbased metrics\nUses\nLLMs\nfor\nsummarisation;\nevaluation\nis\nPICO-based,\nnot\nLLM-as-\njudge\nClinically meaningful PICO\nabstraction;\nbetter\nalign-\nment\nwith\nclinician\njudg-\nments than generic metrics\nCoarse-grained;\nmany instance-\nlevel\ninconsistencies\n(dosage,\ntime,\ncomorbidities)\ninvisible\nat PICO level; not designed for\nEHR-scale proposition checking\nHealthFC\n(Vladika et al.)\n[26]\nVerification\nof\nhealth\nclaims\nwith\nevidence-\nbased\nfact-\nchecking\nClaim-level\nRetrieve\nevidence\nand\nclassify\nclaim\n(supported/refuted)\nNeural\nmodels\n(NLI-style)\nExplicit health-claim verifi-\ncation; uses evidence-based\nmedicine\nFocuses on public health claims\nrather than patient-specific EHR\nsummaries; no structured EHR\nproposition model\nMiniCheck\n(Tang\net\nal.)\n[27]\nEfficient\nfact-\nchecking of LLMs\non\ngrounding\ndocuments\nSentence / span-level\nLightweight\ndocument-aware\nchecking\nof\nLLM\noutputs\nUses LLMs and\nneural encoders\nEfficient,\ndocument-\ngrounded\nverification;\nscalable across tasks\nGeneral-purpose; not tailored to\nclinical ontologies or EHR struc-\nture; relies on neural components\nGraphCheck\n(Chen\net\nal.)\n[28]\nLong-term\ntext\nfact-checking\nwith\nextracted\nknowledge graphs\nGraph / triple-level\nBuild\na\nknowledge\ngraph\nand\nperform\ngraph-powered checks\nNeural\nextrac-\ntion and graph\nreasoning\nCaptures long-range depen-\ndencies and structured rela-\ntions\nExtraction\nand\nreasoning\npipelines\nare\ncomplex,\nnot\nspecifically\nclinical;\ncontinued\nreliance on neural components\nDOSSIER\n(Zhang et al.)\n[29]\nPrivacy-\npreserving\nfact-checking\nfor EHRs\nSentence / segment-level\nDual-layer\nneural\nverification\n+\nentailment + retrieval;\nprivacy-preserving\narchitecture\nYes (neural ver-\nifiers and entail-\nment models)\nExplicitly designed for EHR\nand privacy; strong protec-\ntion of patient data\nResource-intensive, opaque, and\nharder to reproduce, the neural\n+\ncryptographic\nstack\nreduces\ntransparency; still vulnerable to\nmodel drift\nBrainLLaMA\n/\nGPT-based\nevaluators\n(Siino\n[30]\nSiino\n&\nTinnirello\n[31])\nLLM-based\nhallucination\ndetection\nin\nbenchmarks (e.g.,\nSemEval)\nSentence / error-type level\nLLM-as-judge\nwith\nprompt engineering /\nspecialised prompting\nYes\n(LLM-as-\nevaluator)\nFlexible;\neasy\nto\nadapt\nprompts;\ncompetitive\nin\nshared tasks\nNon-deterministic,\ncostly,\nand\nnot easily auditable; privacy and\nreproducibility concerns in clini-\ncal deployment\nFactSelfCheck,\nCross-model\nconsistency,\nCounterfac-\ntual\nprobing\n[32], [33],[34]\nGeneral\nLLM\nhallucination\ndetection\nand\nmitigation\nFact / sentence-level\nBlack-box\ndetection\nvia\nfact-level\nprobing,\ncross-model\nconsistency,\nand\ncounterfactual queries\nYes\n(multiple\nLLMs\nor\nNLI-\nstyle models)\nDo not require ground-truth\ndocuments;\nmodel-agnostic\ndetection\nNot EHR-specific; no explicit use\nof structured clinical evidence;\nlimited interpretability for clini-\ncians\nThis\nwork\n(proposition-\nlevel,\nLLM-\nfree\nfact-\nchecker)\nClinical summari-\nsation verification\nagainst EHR\nProposition-level\n(en-\ntity–attribute–value–time)\nDeterministic\nmap-\nping to propositions +\nnumerical,\ntemporal,\nlogical, and presence\nchecks\nNo\nLLMs\nin\nverification\n(LLM\nonly\nfor\ngeneration)\nEHR-grounded,\nLLM-free\nverification;\ninterpretable\nrule-based checks;\nsuitable\nfor\nprivacy-preserving,\nauditable deployment\nRule coverage and robustness are\nstill limited in rare/specialised\ndomains; it depends on the qual-\nity of proposition extraction and\nontologies\n4\n"}, {"page": 5, "text": "claim ambiguity, evidence attribution, and multi-step reason-\ning.\nAlthough fundamental, these methods were designed\nfor open-domain tasks and lack direct applicability to clinical\ncontexts requiring domain-specific reasoning.\nKazemi et al. [43] explored the ReAct framework to eval-\nuate GPT-3.5 and GPT-4 in fact-checking with and without\nexternal evidence. In particular, when contextual evidence\nwas dynamically recovered, GPT-4 showed a significant im-\nprovement over GPT-3.5. However, this framework remains\nLLM-dependent for both reasoning and verification, making\nit vulnerable to evaluator-induced hallucinations and limiting\nreproducibility.\nGraph-based fact-checking techniques are becoming more\npopular,\nsuch as MiniCheck [27] and GraphCheck [28].\nMiniCheck decomposes documents and statements into indi-\nvidual claims, while GraphCheck enhances model inputs by\nincorporating extracted knowledge graphs, which helps with\nmultihop logical reasoning. In both the general and medical\nfields, the approaches performed better than the baseline\nmodels. However, these methods rely on LLM evaluation and\nblack-box reasoning, reducing transparency and reproducibil-\nity in clinical applications.\nSimilarly, SciTePress [44] developed a DeBERTa-based\nNLI model trained on the HealthVer dataset.\nThis model\nmanaged to verify health-related claims with a weighted F1\nof 0.44 and accuracy of 0.50, while GPT-4 models outper-\nformed others in entailment-based evaluations.\nEven with\nthese advances, overall performance remains modest, and\nreliance on opaque model reasoning limits interpretability and\nreproducibility, potentially undermining clinician trust.\n2.3\nEvaluation\nand\nHallucination\nMitiga-\ntion Strategies\nEvaluating hallucinations and creating mitigation strategies\nare crucial to making LLMs reliable in healthcare. Unchecked\nhallucinations can lead to suggestions that are misleading or\nincorrect, jeopardizing patient safety [45].\nAs clinical text\ngeneration differs from open-domain generation, it requires\neloquent language, rigorous factual accuracy, and logical co-\nherence [21].\nSurveys such as those by Tonmoy et al. [46] highlight data-\ncentric and model inference-based mitigation approaches,\nwhich include retrieval augmentation, self-reflection, RAG,\nrapid engineering, and uncertainty calibration. These meth-\nods often address surface-level factuality without ensuring\nmulti-step logical consistency. Further reviews [47, 48] em-\nphasize limitations in evaluation metrics, domain-specific\ndatasets, and RAG implementations.\nThese highlight the\nneed for robust, interpretable verification tools in specialized\nsectors.\n2.4\nDomain-Specific Error Detection and\nCorrection\nIn clinical NLP, contextual error detection and correction\nare indispensable.\nTop performer in the MEDIQA-CORR\n2024 shared challenge was achieved using ensembles of LLMs\n(GPT-3.5, GPT-4, Claude), NER tools, and knowledge\ngraphs (MeSH) [49].\nHowever, these techniques frequently\nrely on external retrieval and LLMs, which limit scalability\nand interpretability. Kim et al. (2025) highlighted that minor\nfabricated details in clinical prompts can trigger hallucina-\ntions and emphasized the need for systematic verifications at\nthe proposition-level in medical LLMs [3].\nZhang et al.\n[29] introduced DOSSIER, a privacy-\npreserving fact-checking framework for Electronic Health\nRecords (EHRs) that uses dual-layered neural verification,\nentailment-based checks, and retrieval-augmented verifica-\ntion.\nHowever, DOSSIER relies heavily on large neural\ncomponents and complex cryptographic protocols, making it\nresource-intensive, opaque, and potentially less reproducible,\nwith risks of hallucinations and model drift. These limitations\npoint to the need for lighter-weight and more transparent fact-\nchecking pipelines, for example by combining secure data-\naccess mechanisms with simpler, modular verifiers whose\nbehavior can be systematically audited.\nIn our work, we\nfollow this direction by replacing neural verifiers with dis-\ncrete logical checks over structured EHR propositions, while\nremaining compatible with privacy-preserving deployment.\nMore broadly, future frameworks could explore distillation\nof verification models, hybrid neural–symbolic architectures,\nor standardized evaluation protocols to improve reproducibil-\nity while preserving strong privacy guarantees in EHR-scale\ndeployments.\nComplementary to these EHR-focused frameworks, re-\nsearchers have also been diving into LLM-based evaluator\nsystems in recent times, such as BrainLlama and SemEval.\nBrainLlama achieved accuracies of 0.62 (model-agnostic) and\n0.67 (model-aware) in SemEval-2024 [30], while Mistral-7B\nreached 0.73 (English) and 0.76 (Swedish) in the ELOQUENT\n2024 Hallucination Detection task [31]. Despite adaptability,\nthese systems are prompt-sensitive and not consistent across\ndomains.\nAlthough frameworks like DOSSIER improve verification\nand BrainLlama or Mistral-7B enhance evaluation adaptabil-\nity, their dependence on LLM reasoning can create some\nopacity. Our proposed LLM-free validation framework uses\nstructured EHR data and logical verification to ensure trans-\nparency, reproducibility, and reliability.\nTable ?? summarises the main characteristics of prior\nwork on hallucination detection, factuality evaluation, and\nmedical fact-checking in relation to our setting. Surveys [7],\n[21],[22], [21]–[48] provide high-level taxonomies and highlight\nhallucination as a central challenge, while methods such as\nMed-HALT [11], FACTPICO [25], and HealthFC [26] focus on\nmedical benchmarks and claim verification without operating\n5\n"}, {"page": 6, "text": "directly on EHR-derived structures.\nFrameworks such as\nCHECK [18], DOSSIER [29], MiniCheck [27], GraphCheck\n[28], and LLM-based evaluator systems [30], [31] demonstrate\npowerful neural or LLM-centric verification pipelines, but of-\nten remain opaque, computationally intensive, and less suited\nto privacy-preserving deployment at EHR scale. In contrast,\nour approach is designed to complement competitive LLM-\nbased summarisation with an LLM-free, proposition-level ver-\nification module grounded in structured EHR propositions,\naiming to maximise transparency, reproducibility, and clinical\ninterpretability.\n3\nMethodology\nThe complete architecture of our proposed work is illustrated\nin Figure 2. It is designed to be modular and can be used\nwith any LLM model to assess the accuracy of the summaries’.\nOur proposed work consists of a medical domain-specialized\nsummarization model that is capable of producing factually\ncorrect summaries compared to other general models in this\ndomain.\nIt also includes a fact-checking module designed\nwith discrete logic and various consistency checks, performed\nwithout the use of an LLM. These two primary modules are\nintegrated together to form a system that functions reliably.\nEach of the modules is independent; however, the proposed\nmethod is considered an effective system if used together.\nThe verification module works as a layer to validate the\nsummaries against the ground truths, which are the respec-\ntive EHRs. This minimizes hallucinations and makes LLM-\ngenerated summaries reliable for medical decisions.\nThe evaluation framework assesses clinical accuracy at\nthe level of discrete atomic units of the summaries, rather\nthan complete sentences or entire documents.\nThese units\nare called “propositions.” They include specific statements\nsuch as “patient diagnosed with pneumonia,” “prescribed 20\nmg lisinopril daily,” or “hemoglobin measured at 8.2 g/dL.”\nBy this way, our method can compare them systematically\none by one.\nBoth generated summaries and source EHRs\nare converted into propositions. This also ensures that the\nfact-checking module operates independently and can verify\nany summary generated by an LLM. However, our trained\nLLM model performs better in generating summaries that\nare factually correct. The generator starts with a summary\nof clinical notes. After that, both the EHR and the text that\nwas written are broken down into structured propositions.\nWe then use different consistency metrics, such as lexical-\nsemantic similarity, numerical cohesiveness, and temporal\nand logical consistency, to evaluate these propositions. Each\nproposition gets a verdict that states whether it is supported\nor not.\nThis evaluation is performed entirely without the help of\nLLMs, guaranteeing robustness and adaptability by employ-\ning medical synonyms as necessary and maintaining consis-\ntency in lab values, doses, and diagnoses, both logically and\nnumerically.\n3.1\nMedical Summary Generation\nWe carried out a few initial experiments to develop and test\na domain-specific summarization model that could turn raw\nEHR narratives into clinically coherent discharge summaries.\nWe prepared the dataset, pre-processed it, and changed the\nLLaMA-3.1 (8B parameters) model to use parameter-efficient\nfine-tuning [50].\n3.1.1\nParameter-Efficient Adaptation Using Low-\nRank Approximation (LoRA)\nFine-tuning all parameters of billion-scale large language\nmodels (LLMs) such as LLaMA-3.1 (8B parameters) is com-\nputationally expensive and memory-intensive, particularly for\ndomain-specific applications. To mitigate these constraints,\nwe adopt LoRA, a parameter-efficient fine-tuning approach\nthat enables domain adaptation by introducing a limited\nnumber of trainable parameters while keeping the original pre-\ntrained weights frozen. LoRA achieves this by incorporating\nlow-rank decomposition within selected linear layers, effec-\ntively reducing training cost without degrading model perfor-\nmance.\nWe specifically adopt LoRA over other parameter-\nefficient methods such as QLoRA because our experimental\nsetup did not require 4-bit quantization to resolve memory\nconstraints, and LoRA avoids potential accuracy losses intro-\nduced by low-bit quantization. LoRA provides stable, full-\nprecision training dynamics, which is beneficial in domain-\nspecialized settings. Moreover, LoRA’s simplicity makes it\nwell suited for our fine-tuning pipeline.\nTheoretical Formulation of LoRA\nConsider a linear\ntransformation in a pre-trained neural model defined by a\nweight matrix W ∈Rd×k.\nIn conventional fine-tuning, all\nentries of W are updated during training. LoRA, however,\nintroduces a trainable low-rank update ∆W, while preserving\nthe original weights as static:\nW ′ = W + ∆W,\n∆W = AB⊤\n(1)\nwhere A ∈Rd×r and B ∈Rk×r with r ≪min(d, k).\nHere, W denotes the frozen pre-trained weight matrix, and\nA and B are low-rank matrices that are learned during fine-\ntuning.\nThe hyperparameter r represents the rank of the\ndecomposition and governs the additional parameter budget.\nTo control the magnitude of the learned adaptation, a scaling\ncoefficient α is introduced in Equation (2):\nW ′ = W + α\nr AB⊤\n(2)\nThis formulation drastically reduces the number of train-\nable parameters from O(dk) to O(r(d+k)), allowing efficient\nadaptation even for very large models.\nIn practice, r is\n6\n"}, {"page": 7, "text": "Figure 2: The workflow illustrates how LoRA fine-tuning is applied to a large language model (LLM) to generate medical\nsummaries from patient EHRs. The generated summaries and EHR source records are decomposed into structured proposi-\ntions, which are then compared using consistency checkers to evaluate factual alignment. Contradictions and unsupported\nclaims are identified through the logical consistency rules, which later results with a verdict assignment for each proposition\n(Supported or Not Supported).\ntypically selected between 4 and 16, and α between 8 and\n32, balancing adaptation flexibility and training stability.\nLoRA Optimization Procedure\nThe LoRA-based fine-\ntuning process can be described as a sequence of efficient\noptimization steps. Initially, all pre-trained model parame-\nters W are frozen to preserve general linguistic and semantic\nknowledge obtained from large-scale pre-training. LoRA then\nintroduces trainable low-rank matrices A and B into targeted\nlayers.\nMost commonly the query (Wq) and value (Wv)\nprojection matrices in the multi-head attention mechanism.\nThis selective adaptation ensures that only the components\nmost responsible for contextual reasoning are fine-tuned for\ndomain specialization.\nDuring each forward pass, the adapted weight W ′ is ap-\nplied to the input x. Substituting the LoRA parameterization\nfrom Eq. (2) yields in Equation (3):\ny = Wx + α\nr A(B⊤x)\n(3)\nwhere x represents the input of the layer. This formulation\nenables LoRA to learn task-specific residual updates while\nmaintaining the integrity of the pre-trained representations.\nGradients are propagated only through the matrices A and B,\nleaving the frozen weights W untouched. This selective gradi-\nent propagation significantly reduces computational overhead\nand memory consumption compared to full fine-tuning.\nThe optimization process is carried out using the AdamW\noptimizer with a learning rate ranging between 1 × 10−4 and\n5×10−5, depending on the complexity of the dataset and the\nstability of the convergence.\nTo further enhance computa-\ntional efficiency, mixed-precision training (fp16/bf16) is used,\nand a batch size between 4 and 16 is typically sufficient to\nensure stable convergence without overfitting. Upon comple-\ntion of the fine-tuning, the learned low-rank parameters are\nmerged with the base model using the same transformation\ndefined in Equation (2), allowing deployment without addi-\ntional adapter components. This merging step ensures that\nthe inference speed and memory footprint remain equivalent\n7\n"}, {"page": 8, "text": "to the original model, preserving real-time applicability.\nIntegration in Clinical Summarization\nIn this study,\nLoRA was applied to fine-tune the LLaMA-3.1 (8B) model\nusing more than 40,000 patient records from the MIMIC-III\ncritical care database to generate clinically coherent discharge\nsummaries.\nAdapters were selectively integrated into the\nattention sub-layers, resulting in fewer than 1% of the model’s\nparameters being updated during fine-tuning.\nThis efficient adaptation allowed stable convergence\nwithin a few epochs on limited hardware resources while\npreserving the fluency of the model and contextual accuracy.\nThe resulting model demonstrated better factual grounding,\nreduced hallucination rates, and improved alignment with\nthe structured content of Electronic Health Records (EHRs).\nThese findings confirm that LoRA provides a scalable\nand computationally feasible pathway to fine-tune large\nmedical language models in resource-constrained research\nenvironments.\n3.1.2\nOptimization Setup\nThe fine-tuning objective is to minimize the negative log-\nlikelihood (NLL) of generating target tokens given the input\nsequence, as shown in Equation ( 4):\nL(θ) = −\nN\nX\ni=1\nlog Pθ(yi | xi, . . . , x1),\n(4)\nwhere θ represents the adjusted parameters, xi the input\ntokens and yi the corresponding target tokens. We used the\nAdamW optimizer with a cosine scheduler for the learning\nrate. Gradient accumulation was applied to simulate larger\nbatch sizes.\nLoRA Pseudocode\nFor clarity and reproducibility, Al-\ngorithm 1 presents a concise pseudocode description of the\nLoRA fine-tuning workflow, adapted from Hu et al. [24]. The\npseudocode follows the mathematical formulation introduced\nin Equation (2) and the optimization procedure described\nearlier.\n3.2\nFact Checking Module\nThe fact checking module consists of various methods to en-\nsure consistency checks. All the steps of verification operate\non the structured propositions that were extracted both from\nthe LLM generated summary as well as the ground truths\nfrom the corresponding EHRs. These checks are constructed\nin such a way that it is able to detect any discrepancies or\nfactual incorrectness int he summaries such as numerical cor-\nrectness, temporal alignment and other rationales compared\nto the source data.\nThe module functions by following a\nseries of deterministic stages, which begins with extracting\nAlgorithm 1 LoRA Fine-Tuning Procedure\nRequire: Pre-trained model M with parameters {W},\ndataset D, target layers T (e.g., Wq, Wv), rank r, scal-\ning factor α, optimizer hyperparameters (lr, epochs,\nbatch size)\nEnsure: Fine-tuned low-rank adapters {A, B} and merged\nweights W ′\n1: Initialization:\n2: for all modules W ∈M do\n3:\nFreeze base weight W\n▷preserve pretrained\nparameters\n4: end for\n5: for all W ∈T do\n6:\nInitialize A ∈Rd×r, B ∈Rk×r (e.g., N(0, σ2))\n7: end for\n8: Initialize optimizer (AdamW) over adapter parameters\n{A, B}\n9: Training Phase:\n10: for e = 1 to epochs do\n11:\nfor each mini-batch Bx ⊂D do\n12:\nL ←0\n13:\nfor each sample x ∈Bx do\n14:\nfor all target layer W ∈T do\n15:\nz ←B⊤x\n16:\n∆y ←α\nr Az\n17:\ny ←Wx + ∆y\n18:\nend for\n19:\nAccumulate loss ℓ(x) into L\n20:\nend for\n21:\nL ←L/|Bx|\n22:\nBackpropagate gradients (only for {A, B})\n23:\nUpdate {A, B} via AdamW optimizer\n24:\nend for\n25: end for\n26: Merging Phase (optional):\n27: for all W ∈T do\n28:\nW ′ ←W + α\nr AB⊤\n▷merge adapters into base\nweights\n29: end for\n30: return merged weights {W ′} (or retain {A, B} for mod-\nular adapters)\nappropriate propositions, then by logical rule based evalua-\ntion. Finally, a verdict is generated after all the checks are\ncomplete. Each stage is independent of any large language\nreasoning model, which makes the process transparent.\n3.2.1\nFact and Claim Extraction\nEach proposition is formally characterized as a tuple as in\nEquation (5).\nIn this representation, e denotes a clinical\n8\n"}, {"page": 9, "text": "Figure 3:\nIllustrative examples of proposition-level factual verification outcomes are shown here.\nEach row shows a\nproposition based on a summary, its EHR reference statement, and the factual verdict that was given after running through\nthe fact checking module. The figure shows different kinds of consistency checks that the verification engine does. Few\nof the checks are shown here, such as numerical, presence, temporal, negation, implication, and mutual exclusivity checks.\nPropositions failing one or more checks are marked as Not Supported, and those fully aligned and validated against the EHR\nare labeled as Supported. These highlights the deterministic operation of the multi-layered fact-checking pipeline.\nentity such as a medication or laboratory test, a denotes\nits attribute (for example a diagnosis, prescription, or mea-\nsurement), v captures the corresponding value, and t marks\nthe time reference. This structure helps to ensure that each\nfactual statement extracted from both the summary and the\nEHR is complete and clearly defined.\nWithout these com-\nponents, propositions can become incomplete, underspecified,\nor misleading in a clinical context. For example, just noting\nthe entity “metoprolol” without the attribute “dosage” or the\nvalue “50 mg daily” does not represent the full clinical mean-\ning. Similarly, omitting the temporal marker (“on discharge”,\n“on day 2”) can cause problems for time-sensitive conditions\nor treatments\np = (e, a, v, t),\n(5)\nIn practice, we extract (e, a, v, t) from both the generated\nsummary S and the corresponding EHR document E using\na deterministic rule-based pipeline. First, each document is\nsegmented into sentences. A clinical named-entity recognizer\nthen identifies spans corresponding to diagnoses, procedures,\nmedications, and laboratory tests. These spans are normal-\nized to standard biomedical concepts (for example, SNOMED-\nCT, RxNorm, LOINC) using BioPortal [51], which provides\nthe entity field e and ensures consistent concept normalization\nacross documents.\nFor each sentence, we infer an attribute a by applying\ndependency-based patterns and lexical cues such as “diag-\nnosed with”, “treated with”, “started on”, “underwent”, or\n“lab value”. The value v is extracted from the local context\nof each entity–attribute pair, combining numeric expressions\n(for example, “50 mg”, “38.5 °C”), qualitative labels (“posi-\ntive”, “elevated”), and frequency or duration phrases (“twice\ndaily”, “for three days”).\nTemporal markers t are derived\nfrom explicit time expressions (“on day 2”, “before discharge”,\ncalendar dates) and from document structure (admission and\ndischarge times), which are normalized to a patient-specific\ntimeline.\nPS = {pS\n1 , . . . , pS\nm},\nPE = {pE\n1 , . . . , pE\nn },\n(6)\nIf a sentence mentions multiple distinct (entity, attribute,\nvalue, time) combinations, it is split into several atomic\npropositions so that each proposition p corresponds to exactly\none factual claim.\nFormally, the summary S and EHR E\nare decomposed into sets of atomic propositions, as shown\nin Equation (6).\nThis decomposition makes the mapping\nfrom text to propositions explicit and reproducible, and the\n9\n"}, {"page": 10, "text": "pipeline remains fully non-LLM-based, also means any clini-\ncal NER and temporal tagger can be substituted in this step\nwithout changing the downstream verification logic.\n3.2.2\nFact Comparison\nEach summary proposition is compared with candidate propo-\nsitions from the EHR, where the primary similarity is com-\nputed using cosine similarity. Each proposition pS\ni searches\nfor its designated factual counterpart in the EHR set PE\nby computing the cosine similarity between the embedding\nrepresentations of pS\ni and every pE\nj . The text of each propo-\nsition is first converted into dense vector embeddings using a\nfixed domain-specific biomedical encoder. In our experiments,\nwe use BioClinicalBERT and Sentence-BERT because both\nare designed for sentence-level semantic similarity[52]. Bio-\nClinicalBERT is pre-trained on MIMIC-III clinical notes and\nrelated biomedical corpora, which closely match the language\nand style of our EHR data, while Sentence-BERT fine-tunes\ntransformer encoders with a similarity-oriented objective so\nthat cosine distance correlates well with semantic relatedness.\nCompared to general-purpose encoders, these models bet-\nter capture clinical synonymy and paraphrases (for example,\n“myocardial infarction” vs. “heart attack”) and provide stable\nsimilarity scores without additional task-specific fine-tuning.\nAlthough this step relies on pretrained encoders, no gen-\nerative LLMs or probabilistic inference were used. Once the\nembeddings are computed, the selection of the best-matching\nEHR proposition for each summary proposition is purely de-\nterministic. No further use of generative models is made in\nthe verification module, which keeps the fact-checking process\ntransparent and reproducible.\nThe cosine similarity between the summary and the em-\nbeddings of the EHR proposition is formally computed using\ntheir respective vector representations, as shown in Equation\n(7).\nsim(pS\ni , pE\nj ) =\npS\ni · pE\nj\n∥pS\ni ∥∥pE\nj ∥,\n(7)\nwhere pS\ni and pE\nj denote the vector embeddings of the sum-\nmary and EHR propositions, respectively, · represents the dot\nproduct and ∥· ∥denotes the Euclidean norm. The resulting\nsimilarity score has a range between [0, 1], and values closer\nto 1 indicate a higher semantic alignment.\nTo compare pS\n1 with pE\n3 , their vectorized representations\nare computed, and the EHR proposition with the highest\ncosine similarity is selected.\nThis process aligns pS\n1 with\nthe most semantically related factual statement in the EHR.\nThe high similarity score effectively captures these medical\nsynonyms. Once each pair of propositions (pS\ni , pE∗\nj ) is identi-\nfied by obtaining the maximum similarity, the pairs matched\nare then sent to the logical verification module for the next\nsteps. Formally, for each summary proposition, the maximum\nsimilarity score is computed on all EHR propositions as shown\nin Equation (8):\nscore(pS\ni ) = max\nj\nsim(pS\ni , pE\nj ).\n(8)\nThis scoring mechanism ensures that each pS\ni is paired with\nthe factual counterpart that is the most semantically consis-\ntent pE∗\nj\nof the EHR set.\n3.2.3\nContradictions and Consistency Check\nAfter each summary proposition pS\ni has been aligned with\nits most similar EHR proposition pE∗\nj , the pair is passed to\na logical verification module. This module performs several\ncomplementary checks that jointly assess whether the two\npropositions can be true at the same time.\nWe consider\nnegation, implication, temporal ordering, mutual ex-\nclusivity, and numerical consistency on matched pairs,\nplus a presence check on unmatched EHR propositions. All\nsummary claims pass through the same sequence of checks\nagainst their EHR ground truths. Below we briefly describe\neach check and give an illustrative example.\nNegation Check\nThe negation check identifies direct con-\ntradictions between summary and EHR propositions [53]. It\nflags cases where one claim asserts the presence of an event\nwhile the other asserts its absence, for the same entity and\nattribute.\nWe infer the negation state of each proposition\nusing lexical cues such as “no”, “not”, “denies”, “without”,\nor “ruled out”, and then require the negation polarity to\nmatch when e and a are the same. For any pair (pS\ni , pE∗\nj ), a\nnegation failure is triggered when the attributes agree but the\nnegation states differ (as formalized in Equation (9)), where\nneg(·) denotes the negation state of the proposition.\naS\ni = aE\nj and neg(pS\ni ) ̸= neg(pE\nj ),\n(9)\nFor example, if the summary states “antibiotics were not pre-\nscribed” while the EHR reports “treated with IV antibiotics\nfor three days”, the pair is labeled NEGATION-FAIL Figure\n3, (example 5).\nSimilarly, “no evidence of pneumonia” in\nthe summary and “treated for pneumonia” in the EHR also\nconstitute a negation conflict.\nImplication Check\nThe implication check verifies clin-\nically dependent relationships defined by simple ontology-\nbased rules. Certain diagnoses, procedures, or states imply\nthat another event must have occurred (for example, “pneu-\nmonia ⇒antibiotics”, “mechanical ventilation ⇒intuba-\ntion”). If the antecedent appears but its implied consequence\nis missing, we flag a logical inconsistency.\nFormally, if an\nattribute aS\ni implies another attribute aE\nj but no correspond-\ning proposition for aE\nj is present, an IMPLICATION-FAIL is\nraised (as in Equation (10)).\n(aS\ni ⇒aE\nj ) ∧¬(aS\nj ) ⇒IMPLICATION-FAIL.\n(10)\n10\n"}, {"page": 11, "text": "For instance, if a summary mentions “community-acquired\npneumonia” but omits any antibiotic treatment while the\nEHR records “IV ceftriaxone for three days”, the implica-\ntion rule “pneumonia ⇒antibiotics” is violated Figure\n3,\n(example 6). This check captures omissions that are clinically\nimplausible rather than directly contradictory.\nTemporal Consistency Check\nTemporal consistency en-\nsures that the order and duration of medical events are\nchronologically aligned between the EHR and the summary.\nUsing the normalized time markers tS\ni and tS\nj attached to\neach proposition, the system verifies that relative ordering\nis preserved: if the summary states that event i happened\nbefore event j, the same ordering must hold in the EHR (as\nexpressed in in Equation (11))\ntS\ni < tS\nj\n⇔tE\ni < tE\nj .\n(11)\nFor example, if the summary says “fever resolved before\ndischarge”, but the EHR shows “fever persisted until dis-\ncharge”, a TEMPORAL-FAIL is raised (Figure 3, example\n4).\nLikewise, a summary that claims “extubated prior to\ntransfer to the ward” contradicts an EHR in which extuba-\ntion occurred after the transfer. This check prevents subtle\nmisrepresentations of clinical timelines.\nMutual Exclusivity Check\nSome clinical states cannot\nlogically co-occur at the same time. The mutual exclusivity\ncheck flags incompatible propositions that are assigned iden-\ntical or overlapping time markers. If two attributes aS\ni and\naS\nj are known to be mutually exclusive but share the same\ntime tS (Equation (12)), an EXCLUSIVITY-FAIL is raised.\n(aS\ni ⊥aS\nj ) ∧(tS\ni = tS\nj ) ⇒EXCLUSIVITY-FAIL,\n(12)\nFor example, “intubated” and “spontaneously breathing\non room air” recorded at the same time violate exclusivity,\nas do “NPO” (nothing by mouth) and “tolerating regular\noral diet” for the same time interval.\nThis check encodes\nsimple but robust clinical constraints that are not captured\nby semantic similarity alone.\nNumerical Consistency Check\nSemantic similarity is\nnot sufficient for quantitative statements, which must also\nmatch their numeric values and units. The numerical consis-\ntency check compares the value vS\ni in the summary with vE\nj\nin the EHR for the same entity and attribute (Equation (13)).\nA mismatch in value or unit triggers a NUMERICAL-FAIL.\n(vS\ni ̸= vE\nj ) for (eS\ni = eE\nj ∧aS\ni = aE\nj ).\n(13)\nFor example, if the summary states “creatinine 1.2 mg/dL”\nwhile the EHR reports “creatinine 2.1 mg/dL” for the same\ntime point, or if the summary reports “blood pressure 120/80”\nwhen the EHR shows “180/100”, the propositions are nu-\nmerically inconsistent (Figure\n3, example 1).\nThis check\nensures that detailed quantitative information is faithfully\ncopied rather than loosely paraphrased.\nPropositions that fail to pass one or more checks are\nmarked as Not Supported and can be further passed to ex-\npert moderation. This layered verification process systemati-\ncally finds, labels and records factual and logical inconsisten-\ncies to ensure that the generated clinical summaries remain\nreliable and accurate for decision support.\nPresence Check (Omission Detection)\nPresence check\n(omission detection). Finally, to detect clinically important\nomissions, we perform a presence check over all EHR proposi-\ntions that do not have a high-similarity counterpart in the\nsummary.\nIf an EHR proposition ((eS\ni , aS\ni )) exists for a\nkey diagnosis, treatment, or event but no corresponding pair\n(eS\ni , aS\ni ) can be found in the summary, a PRESENCE-FAIL\nis raised (Equation 14).\n(eE\nj , aE\nj ) ∈PE and (eS\ni , aS\ni ) /∈PS ⇒PRESENCE-FAIL.\n(14)\nFor example, if the EHR contains “IV antibiotics adminis-\ntered for three days” but the summary does not mention\nantibiotic therapy at all, the omission is flagged as a presence\nfailure (Figure\n3, example (2)). This check guards against\nerrors of omission and ensures that essential clinical events\nare not silently dropped from the generated summary.\nPropositions that fail one or more of these checks are\nlabeled Not Supported, whereas those that pass all appli-\ncable checks are labeled Supported.\nThe combined use of\npairwise logical checks and the presence check enables the\nfact-checking module to capture both explicit contradictions\nand clinically important omissions at the proposition level.\n3.2.4\nVerdict Generation\nEach summary proposition receives a final verdict using the\nintegrated outcomes of semantic, numerical, and logical vali-\ndation. The verdict is assigned as shown in Equation (15)\nV (pS\ni ) =\n(\nSupported,\nif aligned and validated,\nNot Supported,\notherwise.\n(15)\nFigure 3 provides illustrations of this decision process, show-\ning propositions that were verified as supported or flagged as\nnot supported under various consistency checks such as tem-\nporal, numerical and implication violations. These examples\ndemonstrate the operation of the fact-checking module.\nAll of these combine to form a fact-checking layer, which\nworks as per the examples demonstrated. In this way, each\nproposition of the LLM generated output can be verified\neffortlessly and can improve dependability in critical decision-\nmaking periods.\n11\n"}, {"page": 12, "text": "3.3\nVerification System Integration\nThe verification pipeline functions as a post evaluation layer\nthat works independently of the language model. However,\nsuch layers can be implemented to other benchmark genera-\ntive models. This layer ensures that generated summaries are\nvalidated against their EHR data. Each proposition is ana-\nlyzed against their ground truth through the logical checks,\nand then a transparent verdict (Supported or Not Supported)\nis assigned. The demonstrated results in Figure 3 highlights\nthe range of factual and logical inconsistencies identified by\nthe system in real-world clinical narratives.\n4\nExperiments\nThis section describes the experimental setup of our approach\nto the LLM-free clinical fact verification method and the sum-\nmarization model. We outline the dataset pre-processing and\nmodel fine-tuning, as well as an evaluation methodology to\nmeasure factual, numerical, and logical coherence across gen-\nerated summaries and their paired EHR records. The follow-\ning subsections detail the dataset utilized, the pre-processing\npipeline, the model training settings, the evaluation strategy,\nand how the verification system is applied in conjunction with\nautomated and human validation.\n4.1\nDataset\nOur experiments were conducted in the MIMIC-III database.\nThis dataset is a public de-identified electronic health\nrecords(EHRs) dataset of more than 40,000 patients and\nwidely used as the benchmark for mortality prediction. From\nthese, 26,104 summaries were sampled to fine-tune a sum-\nmarization model, and validate and test the model. These\nsummaries give detailed descriptions of such clinical reports,\nwhich makes them a candidate for summarization tasks. We\ntested 104 patient discharge records from the MIMIC-III\ndatabase, an open data repository for critical care patients\nwith various clinical notes [54]. A set of 3,786 propositions\nwas extracted and validated, allowing a large-scale and sys-\ntematic verification of the factual consistency.\nAlthough MIMIC-III contains over 40,000 encounters,\nonly 104 records were used for fact-checking evaluation be-\ncause the task requires detailed, proposition-level annotation\nby clinicians, which is highly time-intensive. The subset was\nstratified to cover diverse diagnoses, care units, and document\nlengths so that the evaluation remains both representative\nand feasible.\nThis curated set enables high-quality, repro-\nducible factuality assessment without introducing annotation\nnoise.\n4.2\nPreprocessing\nWe carried out preprocessing to make sure that the data were\nsuitable enough and ready for the determined fine-tuning task.\nThe data set was filtered to keep only the discharged notes\nthat contained detailed narratives of patient records. Those\nwere matched with their corresponding gold-standard refer-\nence summaries. Records that were too brief or excessively\nlengthy were eliminated in the preprocessing of the dataset.\nIn addition, a slight moralization was performed to resolve\nformatting issues in the data, which was critically handled to\npreserve the important clinical terminologies.\nAfter the filtering process, the dataset contained 26,104\ndischarge summaries. The dataset was partitioned into three\nsubsets:\ntraining (20,883 samples), validation (2,610 sam-\nples), and test (2,611 samples). The division was done based\non an 80/10/10 ratio, with a seed value of 42..\n4.3\nTraining Setup\nDuring fine-tuning AdamW was used as the optimizer with a\nlearning rate of 1 × 10−4, a cosine decay schedule and weight-\ndecay of 0.01. The batch size used in this study was 8, with\na sequence length of 2,048 tokens. Mixed precision (bfloat16)\nand gradient checkpoint were used for better memory effi-\nciency.\nThe use of token-level masked cross-entropy loss\nencourages the stable convergence in sequences of different\nlengths.\nTable 2 presents a complete list of hyperparameters and\noptimization details and reveals that the top configuration\nfocuses on parameter efficiency through partial fine-tuning so\nit only has about 84 million number of trainable parameters\n(1.03% of the total of 8.1 billion). This setup ensures repro-\nducibility and easy large-scale model fine-tuning on a single\nA100 GPU with performance consistency.\nTable 2: Training Configuration\nComponent\nConfiguration\nOptimizer\nAdamW, lr = 1 × 10−4, cosine schedule, weight\ndecay = 0.01\nBatching\nEffective batch size = 8 (1 per device × 8 grad\naccumulation)\nEpochs / Steps\n1,200 steps (≈2 epochs)\nPrecision\nbfloat16 mixed precision with gradient check-\npointing\nLoss Function\nToken-level cross-entropy (masked prompts)\nTrainable Params\n∼84M / 8.1B total (1.03%)\n4.4\nTraining Pipeline\nFigure 4 shows the complete workflow, including prepro-\ncessing, tokenization, and alignment of the raw text into\ninstruction-response pairs.\nInputs were segmented for\nretrieval-augmented learning, and LoRA modules were in-\nserted for adaptation.\nThe system optimized the negative\nlog-likelihood loss with validation monitoring and was tested\non held-out datasets. This pipeline integrates data prepara-\ntion, representation learning, adaptation, optimization, and\nevaluation into a single unified process.\n12\n"}, {"page": 13, "text": "Figure 4: End-to-end fine-tuning pipeline illustrating pre-\nprocessing, tokenization, LoRA-based adaptation, and evalu-\nation integration.\nLoRA adds only O(r(d + k)) trainable parameters,\nwhereas full fine-tuning adds O(dk).\nIn our setup, r = 8\nreduced the trainable parameters by more than 99%, enabling\nefficient training on a single A100 GPU.\n4.5\nEvaluation Method\nWe evaluated the correctness of each proposition of our fact-\nchecking module using precision, recall, F1-score, and confu-\nsion matrix analysis. These metrics allow us to understand\nhow well the generated summaries align with their respective\nEHR data. The MIMIC-III dataset provides both discharge\nsummaries and structured EHRs, which allows us to match\neach summary with its actual record, which also ensures a\nsolid factual assessment.\n4.6\nIntegration of the Verification System\nAll summaries generated from the llms went through the\nverification pipeline’s steps; each extracted proposition was\nlogically compared using the proposed fact-checking module\nwith its corresponding EHR proposition and labeled as either\n”Supported” or ”Not Supported.” Then a clinician reviewed\nand checked these findings to make sure that there were no\ninaccuracies in the results. Since summary, EHR, and logical\nconsistency are the main constant throughout the pipeline,\nthis integrated approach keeps the process straightforward\nand reliable.\n4.7\nHuman Evaluation\nClinicians looked over a selection of generated summaries,\ncomparing each one to its related MIMIC-III EHR record.\nMost summaries were found to be consistent with their source\nrecords. Clinicians pointed out areas for improvement, partic-\nularly in how rare conditions and long-term temporal depen-\ndencies, like chronic diseases that span multiple visits, are\nrepresented. All evaluations followed the MIMIC-III usage\nguidelines and were conducted on anonymized data, confirm-\ning both the factual reliability of our system and the integrity\nof the dataset.\n5\nResults\nIn this section, we present the results of our LLM-free fact-\nchecking system for clinical summarization, evaluated on the\nMIMIC-III dataset.\nThe experiments evaluate the summa-\nrization quality of the LoRA-fine-tuned LLaMA-3.1-8B gen-\nerator and the factual accuracy of the independent verifi-\ncation module across 104 discharge summaries, comprising\n3,786 propositions. Quantitative metrics, such as ROUGE,\nBERTScore, precision, recall, and F1-score, show that the\nframework works well to create logical narratives with little\nor no hallucinations. The confusion matrix also shows that\nit can classify supported and unsupported claims well. These\nresults show that the framework is better than LLM-based\nevaluators, which was confirmed by other general benchmarks\nand reviews by clinicians.\n5.0.1\nTraining Loss Convergence\nFigure 5 shows the training loss curve over fine-tuning steps.\nThe loss decreases steadily during the initial phase of training\nand then gradually plateaus, indicating that the LoRA adap-\ntation converges toward a stable minimum. The absence of\nlarge spikes or divergence in the curve suggests numerically\nstable optimization and no obvious signs of catastrophic over-\nfitting on the training set. This provides additional support\nthat the checkpoint used for evaluation is well-behaved and\nthat the subsequent summarization and fact-checking results\nare not artifacts of an unstable training run.\nFigure 5: Training loss convergence over fine-tuning steps.\n13\n"}, {"page": 14, "text": "5.1\nLLM Summarization Performance\nThe fine-tuned LLaMA-3.1-8B model achieved strong per-\nformance by demonstrating its ability to generate clinically\ncoherent summaries while minimizing hallucinations. Table\n3 reports the evaluation metrics for the fine-tuned LLaMA-\n3.1-8B model. As shown in Figure 6, our fine-tuned model\nachieved strong performance with ROUGE-1, ROUGE-2, and\nROUGE-L scores of 0.5797, 0.5580, and 0.5618 respectively,\nreflecting high lexical and structural alignment with the ref-\nerence clinical summaries. A BLEU score of 0.3604 further\nindicates accurate n-gram overlap, while the high BERTScore\n(F1 = 0.9120) demonstrates strong semantic fidelity between\nthe generated and reference texts. Collectively, these results\nindicate that the model produces concise and clinically rele-\nvant text.\nTo provide context for these values, we compare them\nto previously reported neural summarization systems on\nMIMIC-III and related clinical corpora.\nPrior LLM-based\napproaches, such as those by Tang et al.\n[16], Xu et al.\n[17], and Lin et al. [1], report ROUGE-L (or ROUGE-Lsum)\nand BERTScore values in a similar range when evaluated\non long discharge summaries, indicating that our summariza-\ntion performance is competitive with existing methods rather\nthan an outlier. This suggests that the main contribution of\nour framework does not stem from unusually high ROUGE\nor BERTScore values alone, but from the downstream fact-\nchecking module that explicitly controls factual consistency\nat the proposition level.\nFigure 6: Performance metrics of the fine-tuned LLaMA-3.1-\n8B model on MIMIC-III discharge summaries.\nOverall, these results indicate that the LoRA-adapted\nLLaMA-3.1 model produces high-quality summaries whose\nresidual factual errors can be systematically addressed by the\nverification pipeline. Propositions that remain incorrect after\ngeneration tend to be easily flagged by the proposition-level\nfact-checker. Combined with the logical verification module,\nthe framework forms a reliable system for trustworthy clinical\nsummarization.\n5.2\nFact Checker Performance\nThe quantitative analysis in Table 3 demonstrates that the\nproposed fact-checking system achieves strong overall per-\nformance in all essential metrics.\nThe system can identify\nsupported propositions with a precision of 0.8904 and an\nF1-score of 0.8556. On the 3,786-proposition test set, this\ncorresponds to 2,340 true positives and only 288 false posi-\ntives, i.e., a false discovery rate of 0.1096. In other words,\nwhen the system predicts that a proposition is supported,\nit is correct almost nine times out of ten, and the absolute\nnumber of false positives remains comparatively low. A recall\nvalue of 0.8234 reveals that most of the clinically valid facts\nin the summaries are found and verified with evidence in the\nEHR. The overall accuracy of 0.7913 on 3,786 propositions\nshows that the logical verification pipeline is strong. It also\nhighlights its ability to address a wide range of factual errors,\nsuch as numerical, temporal, and logical errors.\nThese results collectively confirm the efficacy of the multi-\nlayered fact-checking system in maintaining factual integrity\nin automatically generated clinical summaries. This step is\ncrucial in order to consider LLMs as reliable systems for such\ncritical sectors. Table IV reports additional evaluation met-\nrics for the fact-checking system. Specificity (unsupported)\nequals 0.6949, which means that 69.49% of actual unsup-\nported claims are correctly identified. MCC equals 0.4866;\nthis indicates that overall there is a moderate correlation be-\ntween predictions and true labels. Balanced accuracy equals\n0.7591, hence reflecting solid performance across both classes\ndespite imbalance.\nLog loss equals 0.2288, which is a low\nvalue that suggests well-calibrated probability predictions.\nTable 3: Automatic Evaluation Metrics for Fact-Checking\nSystem\nPrecision\nRecall\nF1-Score\nAccuracy\n0.8904\n0.8234\n0.8556\n0.7913\nTable 4 reports additional evaluation metrics for the fact-\nchecking system.\nSpecificity (unsupported) equals 0.6949,\nwhich means that 69.49% of actual unsupported claims are\ncorrectly identified. MCC equals 0.4866; this indicates that\noverall there is a moderate correlation between predictions\nand true labels.\nBalanced accuracy equals 0.7591, hence\nreflecting solid performance across both classes despite im-\nbalance. Log loss equals 0.2288, which is a low value that\nsuggests well-calibrated probability predictions.\n5.3\nComparison with Existing Literature\nThe presented method, called the Fact-Checking Module, out-\nperforms previous methods and reaches the new state-of-the-\nart F1-score of 79.13% on MIMIC-III [54]. This highlights the\nmodule’s superior performance in supporting clinical state-\nments compared to current state-of-the-art models.\n14\n"}, {"page": 15, "text": "Table 4: Additional Evaluation Metrics for Fact-Checking System\nSpecificity (Unsupported)\nMCC\nBalanced Accuracy\nLog Loss\nFDR\n0.6949\n0.4866\n0.7591\n0.2288\n0.1096\nPrevious systems, including Claude-1 [55], achieved an\naccuracy of 66.74%, while its DOSSIER-extended version\nreached 70.53%.\nAlthough Claude-2 [56] demonstrated a\nperformance of 70.65% and Claude-2 (DOSSIER) achieved\n78.62%, our model exhibits a notable gain by exceeding\n79% without the need for large language models for verifica-\ntion. Models such as CodeLlama-13B[57], MedAlpaca 7B [58],\nand ClinicalCamel 13B [59] exhibited accuracies of 65.76%,\n46.74%, and 32.51%, respectively, indicating deficiencies in\nfactual grounding and generalization.\nDomain-specialized\narchitectures, including T5-EHRSQL [60] and Asclepius 13B\n[61], did not exceed 55%, highlighting the performance gap.\nThe experimental results validate the practical strength\nand efficacy of our consistent LLM-free verification pipeline.\nThe proposed module ranks higher than the state-of-the-art\ncounterpart with respect to factual consistency, transparency,\nand reproducibility. In Table 5, we show an extensive compar-\nison of our proposed method against state-of-the-art methods.\nOur proposed Fact-Checking Module achieves the highest\naccuracy (79.13%), outperforms these baselines on MIMIC-\nIII including Claude-2 (DOSSIER) (78.62%) and CodeLlama-\n13B (DOSSIER) (65.76%). These findings validate the mod-\nule’s capability for reliable, interpretable, and reproducible\nfact verification in clinical NLP.\nTable 5: Comparison of different models and their accuracy\non the MIMIC-III clinical claim verification task.\nModel\nDataset\nAccuracy (%)\nClaude-1 [55]\nMIMIC-III\n66.74\nClaude-1 (DOSSIER) [29]\nMIMIC-III\n70.53\nCodeLlama-13B [57]\nMIMIC-III\n64.13\nCodeLlama-13B (DOSSIER) [29]\nMIMIC-III\n65.76\nMedAlpaca 7B [58]\nMIMIC-III\n46.74\nClinicalCamel 13B [59]\nMIMIC-III\n32.51\nAsclepius 13B [61]\nMIMIC-III\n39.26\nLlama2 7B 32k [62]\nMIMIC-III\n32.81\nT5-EHRSQL [60]\nMIMIC-III\n54.56\nClaude-2 [56]\nMIMIC-III\n70.65\nClaude-2 (DOSSIER) [29]\nMIMIC-III\n78.62\nOur Module\nMIMIC-III\n79.13\n6\nDiscussion and Future Work\nOur proposed fact-checking system introduces a two-stage\npipeline for clinical summarization. It combines a domain-\nspecific LLaMA-3.1-8B generator, fine-tuned via Low-Rank\nAdaptation (LoRA) on MIMIC-III discharge summaries [23,\n24], to create coherent narratives, and an independent fact-\nchecking module that uses cosine similarity, numerical tests,\nand discrete logical checks for granular verification against\nElectronic Health Records (EHRs). The system recorded a\nROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for\nsummary generation.\nThe fact-checking module reached a\nprecision of 0.8904, a recall of 0.8234, and an F1-score of\n0.8556 over 3,786 propositions from 104 summaries. Clinician\nreview (n = 2) judged roughly 85% of a random subset of\nsummaries as clinically acceptable, and the logical checks\nsuccessfully highlighted the majority of remaining inconsisten-\ncies.The fact-checking module can also serve independently as\na post-processing layer for any LLM-generated summary in\nthe clinical domain, as it depends solely on deterministic logi-\ncal checks rather than probabilistic language-model behavior\n[26, 42].\nAlthough the proposed system can also be applied to eval-\nuate the factual accuracy of summaries generated by other\nLLMs, our fine-tuned LLaMA-3.1-8B model demonstrates\nsuperior summarization performance in this study.\nHow-\never, the current pipeline shows limited robustness in critical\nand less-explored medical domains where highly specialized\nknowledge is required [6]. Examples include rare oncological\nsubtypes (e.g., hematologic malignancies with complex stag-\ning), pediatric metabolic and genetic disorders, or transplant\nmedicine and intensive care scenarios involving multi-organ\nfailure. In such settings, the EHR may contain highly tech-\nnical terminology, uncommon procedures, and nuanced tem-\nporal relationships that are under-represented in the training\ndata and only partially covered by generic ontologies [18, 25].\nThis increases the risk that the generator omits key events\nor that the verifier fails to recognize domain-specific implica-\ntions (for instance, subtle drug–drug interactions in oncology\nor transplant immunosuppression regimens). Robustness in\nthese domains could be improved by integrating richer medi-\ncal ontologies (e.g., subspecialty extensions of SNOMED-CT\nor disease-specific knowledge graphs), incorporating domain-\nspecific lexicons, and fine-tuning components on curated data\nfrom specialized clinics [27, 28].\nAt the same time, the investigation and implementation\nof explicit reasoning mechanisms and automatic correction of\nunsupported outputs remain open in the current study. Our\nverifier currently functions as a binary gate: it labels propo-\nsitions as Supported or Not Supported but does not propose\nhow to repair them or re-write the summary. Future work\ncould extend this by: (i) adding a symbolic reasoning layer\nthat operates on the graph of extracted propositions (nodes\nas events, edges as temporal or logical relations) to propagate\nconstraints and identify minimal sets of edits [28]; (ii) gen-\nerating candidate “corrected propositions” by substituting\n15\n"}, {"page": 16, "text": "values or attributes from the EHR and feeding them back\ninto a controlled rewriting step [49]; and (iii) incorporating\nclinician-in-the-loop workflows where flagged propositions are\npresented with explanations (“negation conflict”, “numerical\nmismatch”, “missing implied treatment”) and suggested fixes\nthat the user can accept or modify [41].\nThese directions\nwould turn the module from a pure detector into an assistive\ntool that both diagnoses and helps correct factual errors, a\nneed highlighted in recent clinical NLP hallucination surveys\n[3, 21].\nFuture work will also concentrate on expanding the fact-\nchecking module by integrating causal reasoning into the\ncurrent temporal consistency assessments.\nConcretely, one\ndirection is to model patient trajectories as causal or causal-\ninspired graphs, where nodes represent diagnoses, interven-\ntions, and outcomes, and edges encode plausible cause–effect\nrelationships derived from clinical guidelines or learned from\nlongitudinal EHR data [28]. The temporal check could then\nbe extended to verify not only that events occur in the correct\norder, but also that observed patterns are consistent with\nknown causal pathways (for example, “initiation of anticoagu-\nlation should follow diagnosis of atrial fibrillation, not precede\nit”, or “improvement in oxygenation should not causally pre-\ncede the start of mechanical ventilation”). Another strategy\nis to employ simple structural-causal models or counterfac-\ntual probes over the proposition graph, e.g., asking whether\nremoving a key intervention would plausibly change down-\nstream outcomes and flagging summaries that imply clinically\nimplausible or causally inverted relationships [33, 34]. These\ncausal constraints can be combined with the existing numer-\nical and temporal checks to increase the system’s ability to\ndetect subtle, yet clinically important, hallucinations [7, 22].\n7\nConclusion\nThis work introduced a two-stage framework for trustwor-\nthy clinical summarization that couples a LoRA-fine-tuned\nLLaMA-3.1-8B generator with an independent, LLM-free\nfact-checking module operating at the proposition level. The\nLoRA adaptation enables the base LLaMA-3.1-8B model to\nspecialize on long, noisy discharge summaries from MIMIC-\nIII while retaining its strong language modeling capabili-\nties, resulting in summaries with competitive ROUGE and\nBERTScore metrics and clinically coherent narratives. Cru-\ncially, the generator is integrated with the verifier in a way\nthat separates concerns: the LLaMA-3.1-8B component is\noptimized for fluent, clinically appropriate text, whereas the\nfact-checking module is optimized for fine-grained consistency\nwith the underlying EHR. This division allows the system to\nachieve high overall performance, that is, precision of 0.8904\nand F1-score of 0.8556 on 3,786 propositions, while maintain-\ning transparency and reproducibility in the verification step.\nAt the same time, our results highlight several areas\nwhere robustness must be improved before deployment in\nsafety-critical settings. The current extraction and verifica-\ntion pipeline performs well on common diagnoses and treat-\nments but remains less reliable in highly specialized or under-\nrepresented domains, such as rare oncologic subtypes, pedi-\natric metabolic disorders, or complex transplant cases. Ad-\ndressing this will require augmenting the proposition schema\nand rule base with richer subspecialty ontologies, improving\nthe coverage of entity normalization and implication rules,\nand incorporating causal and temporal reasoning mechanisms\ncapable of capturing domain-specific treatment pathways. In\naddition, robustness could be strengthened through system-\natic evaluations on external datasets, cross-institutional vali-\ndation, and uncertainty quantification that exposes when the\nverifier’s judgments are unreliable and should be escalated for\nhuman review.\nDespite these limitations, the proposed framework has\nthe potential to improve concrete clinical workflows.\nIn a\ndischarge-summary workflow, for example, the system could\nact as a post-hoc safety layer that automatically flags contra-\ndictions between the generated summary and the EHR, such\nas mismatched laboratory values, omitted comorbidities, or\nmissing treatments implied by diagnoses before the clinician\nsigns off.\nIn medication reconciliation, it could check that\ndescribed therapies and dosages align with the structured\nmedication list and allergy record, highlighting inconsisten-\ncies that might otherwise be overlooked. For inter-provider\ncommunication, the fact-checker could be applied to referral\nletters and handover notes to ensure that key events (e.g.,\nprocedures performed, complications, changes in code status)\nare accurately and consistently represented across documents.\nBy embedding such proposition-level verification into routine\ndocumentation and handover processes, the framework can\nsupport safer, more reliable use of generative models in clin-\nical practice and provide a principled foundation for future\nextensions that further integrate reasoning and correction\ncapabilities.\n[1] C. Lin and C.-F. Kuo, “Roles and potential of large\nlanguage\nmodels\nin\nhealthcare:\nA\ncomprehensive\nreview,”\nBiomedical Journal,\np.\n100868,\n2025.\n[Online].\nAvailable:\nhttps://www.sciencedirect.com/\nscience/article/pii/S2319417025000423\n[2] Q. Li, Y. Wang, T. You, and Y. Lu, “Bioknowprompt:\nIncorporating imprecise knowledge into prompt-tuning\nverbalizer with biomedical text for relation extraction,”\nInformation Sciences, vol. 617, pp. 346–358, 2022.\n[3] Y. Kim, H. Jeong, S. Chen, S. S. Li, M. Lu, K. Alhamoud,\nJ. Mun, C. Grau, M. Jung, R. Gameiro et al., “Medical\nhallucinations in foundation models and their impact on\nhealthcare,” arXiv preprint arXiv:2503.05777, 2025.\n[4] K. Aliyeva and N. Mehdiyev, “Uncertainty-aware multi-\ncriteria decision analysis for evaluation of explainable ar-\ntificial intelligence methods: A use case from the health-\ncare domain,” Information sciences, vol. 657, p. 119987,\n16\n"}, {"page": 17, "text": "2024.\n[5] Z. Li, H. Jiang, and S. Zhao, “Mcd-ears:\nA multi-\nmodal cross-domain expertise-aware recommender sys-\ntem for healthcare applications,” Information Sciences,\np. 122821, 2025.\n[6] J. Si, H. Zhu, Y. Zhao, W. Zhang, T. Wang, W. Lu, and\nD. Zhou, “Scene generalization for biomedical fact verifi-\ncation via hierarchical mixture of experts,” Information\nSciences, p. 122527, 2025.\n[7] V. Rawte, A. Sheth, and A. Das, “A survey of hal-\nlucination in large foundation models,” arXiv preprint\narXiv:2309.05922, 2023.\n[8] J. Maynez, S. Narayan, B. Bohnet, and R. McDonald,\n“On faithfulness and factuality in abstractive summariza-\ntion,” arXiv preprint arXiv:2005.00661, 2020.\n[9] R. Hu, Y. Tu, S. Wei, D. Lu, and J. Sang, “Prescrib-\ning the right remedy: Mitigating hallucinations in large\nvision-language models via targeted instruction tuning,”\nInformation Sciences, p. 122361, 2025.\n[10] M. Virvou, G. A. Tsihrintzis, and E.-A. Tsichrintzi,\n“Virtsi: A novel trust dynamics model enhancing artifi-\ncial intelligence collaboration with human users–insights\nfrom a chatgpt evaluation study,” Information Sciences,\nvol. 675, p. 120759, 2024.\n[11] A. Pal, L. K. Umapathi, and M. Sankarasubbu, “Med-\nhalt: Medical domain hallucination test for large lan-\nguage models,” arXiv preprint arXiv:2307.15343, 2023.\n[12] V. Geroimenko, “Generative ai hallucinations in health-\ncare: A challenge for prompt engineering and creativity,”\nin Human-Computer Creativity: Generative AI in Edu-\ncation, Art, and Healthcare.\nSpringer, 2025, pp. 321–\n335.\n[13] M. Karabacak and K. Margetis, “Embracing large lan-\nguage models for medical applications:\nopportunities\nand challenges,” Cureus, vol. 15, no. 5, 2023.\n[14] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei,\nH. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis,\nS. Pfohl, P. Payne, M. Seneviratne, P. Gamble, C. Kelly,\nA. Babiker, N. Sch¨arli, A. Chowdhery, P. Mansfield,\nD. Demner-Fushman, B. Ag¨uera Y Arcas, D. Web-\nster, G. S. Corrado, Y. Matias, K. Chou, J. Gottweis,\nN. Tomasev, Y. Liu, A. Rajkomar, J. Barral, C. Sem-\nturs, A. Karthikesalingam, and V. Natarajan, “Large\nlanguage models encode clinical knowledge,” Nature, vol.\n620, no. 7972, pp. 172–180, 2023, epub 2023 Jul 12.\n[15] G. Wang, G. Yang, Z. Du, L. Fan, and X. Li, “Clin-\nicalgpt: Large language models finetuned with diverse\nmedical data and comprehensive evaluation,” arXiv\npreprint arXiv:2306.09968,\n2023. [Online]. Available:\nhttps://arxiv.org/abs/2306.09968\n[16] L.\nTang,\nZ.\nSun,\nY.\nMa,\nG.\nYang,\nY.\nGu,\nV.\nYadav,\nW.\nWeng,\nZ.\nHe,\nY.\nWang,\nand\nH. Yu, “Evaluating large language models on medical\nevidence summarization,” npj Digital Medicine, vol. 6,\nno.\n1,\np.\n158,\n2023.\n[Online].\nAvailable:\nhttps:\n//www.ncbi.nlm.nih.gov/pmc/articles/PMC10449915/\n[17] X. Xu, Y. Chen, and J. Miao, “Opportunities, chal-\nlenges, and future directions of large language models,\nincluding chatgpt in medical education:\na systematic\nscoping review,” Journal of educational evaluation for\nhealth professions, vol. 21, 2024.\n[18] C. Garcia-Fernandez, L. Felipe, M. Shotande, M. Zitu,\nA. Tripathi, G. Rasool, I. El Naqa, V. Rudrapatna, and\nG. Valdes, “Trustworthy ai for medicine: Continuous\nhallucination detection and elimination with check,”\narXiv preprint, 2025. [Online]. Available: https://arxiv.\norg/abs/2506.11129\n[19] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin,\nN.\nGoyal,\nH.\nK¨uttler,\nM.\nLewis,\nW.\ntau\nYih,\nT. Rockt¨aschel, S. Riedel, and D. Kiela, “Retrieval-\naugmented\ngeneration\nfor\nknowledge-intensive\nnlp\ntasks,” arXiv preprint arXiv:2005.11401, 2020. [Online].\nAvailable: https://arxiv.org/abs/2005.11401\n[20] K.\nShuster,\nS.\nPoff,\nM.\nChen,\nD.\nKiela,\nand\nJ.\nWeston,\n“Retrieval\naugmentation\nreduces\nhallucination\nin\nconversation,”\nin\nFindings of\nthe Association for Computational Linguistics: EMNLP\n2021.\nAssociation\nfor\nComputational\nLinguistics,\n2021,\npp.\n3784–3803.\n[Online].\nAvailable:\nhttps:\n//aclanthology.org/2021.findings-emnlp.320/\n[21] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii,\nY. J. Bang, A. Madotto, and P. Fung, “Survey of halluci-\nnation in natural language generation,” ACM computing\nsurveys, vol. 55, no. 12, pp. 1–38, 2023.\n[22] L. Huang,\nW. Yu,\nW. Ma,\nW. Zhong,\nZ. Feng,\nH. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and\nT. Liu, “A survey on hallucination in large language\nmodels:\nPrinciples, taxonomy, challenges, and open\nquestions,” ACM Transactions on Information Systems,\nvol. 43, no. 2, p. 1–55, Jan. 2025. [Online]. Available:\nhttp://dx.doi.org/10.1145/3703155\n[23] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman,\nM. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. An-\nthony Celi, and R. G. Mark, “Mimic-iii, a freely accessi-\nble critical care database,” Scientific data, vol. 3, no. 1,\npp. 1–9, 2016.\n[24] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li,\nS. Wang, L. Wang, W. Chen et al., “Lora: Low-rank\nadaptation of large language models.” ICLR, vol. 1, no. 2,\np. 3, 2022.\n[25] S. A. Joseph, L. Chen, J. Trienes, H. L. G¨oke, M. Coers,\nW. Xu, B. C. Wallace, and J. J. Li, “Factpico: Factuality\nevaluation for plain language summarization of medical\n17\n"}, {"page": 18, "text": "evidence,” arXiv preprint arXiv:2402.11456, 2024.\n[26] J. Vladika, P. Schneider, and F. Matthes, “Healthfc:\nVerifying health claims with evidence-based medical fact-\nchecking,” arXiv preprint arXiv:2309.08503, 2023.\n[27] L. Tang, P. Laban, and G. Durrett, “Minicheck: Efficient\nfact-checking of llms on grounding documents,” arXiv\npreprint arXiv:2404.10774, 2024.\n[28] Y. Chen, H. Liu, Y. Liu, J. Xie, R. Yang, H. Yuan,\nY. Fu, P. Zhou, Q. Chen, J. Caverlee, and I. Li,\n“Graphcheck: Breaking long-term text barriers with ex-\ntracted knowledge graph-powered fact-checking,” arXiv\npreprint arXiv:2502.16514, February 2025.\n[29] H. Zhang,\nS. Nagesh,\nM. Shyani,\nand N. Mishra,\n“DOSSIER: Fact checking in electronic health records\nwhile\npreserving\npatient\nprivacy,”\nin\nProceedings\nof the 9th Machine Learning for Healthcare Conference,\nser.\nProceedings\nof\nMachine\nLearning\nResearch,\nK.\nDeshpande,\nM.\nFiterau,\nS.\nJoshi,\nZ.\nLip-\nton,\nR.\nRanganath,\nand\nI.\nUrteaga,\nEds.,\nvol.\n252.\nPMLR, 16–17 Aug 2024. [Online]. Available:\nhttps://proceedings.mlr.press/v252/zhang24a.html\n[30] M. Siino, “Brainllama at semeval-2024 task 6: Prompt-\ning llama to detect hallucinations and related observ-\nable overgeneration mistakes,” in Proceedings of the\n18th International Workshop on Semantic Evaluation\n(SemEval-2024), 2024, pp. 82–87.\n[31] M. Siino and I. Tinnirello, “Gpt hallucination detection\nthrough prompt engineering,” in Proc. of the 25th Work-\ning Notes of the Conference and Labs of the Evaluation\nForum, vol. 3740, 2024, pp. 712–721.\n[32] A. Sawczyn,\nJ. Binkowski,\nD. Janiak,\nB. Gabrys,\nand T. Kajdanowicz, “Factselfcheck: Fact-level black-\nbox hallucination detection for llms,” arXiv preprint\narXiv:2503.17229, 2025.\n[33] A. Goel, D. Schwartz, and Y. Qi, “Zero-knowledge\nllm hallucination detection and mitigation through\nfine-grained cross-model consistency,” arXiv preprint\narXiv:2508.14314, 2025.\n[34] Y. Feng, “Counterfactual probing for hallucination de-\ntection and mitigation in large language models,” arXiv\npreprint arXiv:2508.01862, 2025.\n[35] R. Yang, T. F. Tan, W. Lu, A. J. Thirunavukarasu,\nD. S. W. Ting, and N. Liu, “Large language models in\nhealth care: Development, applications, and challenges,”\nHealth Care Science, vol. 2, no. 4, pp. 255–263, 2023.\n[36] X. Zhao, H. Zhang, X. Pan, W. Yao, D. Yu, T. Wu, and\nJ. Chen, “Fact-and-reflection (far) improves confidence\ncalibration of large language models,” arXiv preprint\narXiv:2402.17124, 2024.\n[37] J. C. L. Ong, S. Y.-H. Chang, W. William, A. J. Butte,\nN. H. Shah, L. S. T. Chew, N. Liu, F. Doshi-Velez, W. Lu,\nJ. Savulescu et al., “Ethical and regulatory challenges of\nlarge language models in medicine,” The Lancet Digital\nHealth, vol. 6, no. 6, pp. e428–e432, 2024.\n[38] S. Hegselmann, S. Z. Shen, F. Gierse, M. Agrawal,\nD. Sontag, and X. Jiang, “A data-centric approach\nto generate faithful and high quality patient sum-\nmaries with large language models,” arXiv preprint\narXiv:2402.15422, 2024.\n[39] Red Hat Developer, “Deploy llama 3 8b with vllm,”\nRed\nHat\nDeveloper,\nJun.\n2024,\n[Online].\nAvail-\nable:\nhttps://developers.redhat.com/articles/2024/06/\n18/deploy-llama-3-8b-with-vllm.\n[40] DSS Solutions,\n“Benchmarking llm inference back-\nends,”\nDSS Solutions Tech Blog,\nJun. 2024,\n[On-\nline]. Available:\nhttps://dsssolutions.com/2024/06/17/\nbenchmarking-llm-inference-backends/.\n[41] T. Kang, Y. Sun, J. H. Kim, C. Ta, A. Perotte, K. Schif-\nfer, M. Wu, Y. Zhao, N. Moustafa-Fahmy, Y. Peng et al.,\n“Evidencemap:\na three-level knowledge representation\nfor medical evidence computation and comprehension,”\nJournal of the American Medical Informatics Associa-\ntion, vol. 30, no. 6, pp. 1022–1031, 2023.\n[42] J. Thorne and A. Vlachos, “Automated fact check-\ning: Task formulations, methods and future directions,”\narXiv preprint arXiv:1806.07687, 2018.\n[43] D. Quelle and A. Bovet, “The perils and promises of\nfact-checking with large language models,” Frontiers in\nArtificial Intelligence, vol. 7, p. 1341697, 2024.\n[44] M. Koˇsprdi´c, A. Ljaji´c, D. Medvecki, B. Baˇsaragin,\nand N. Milosevic, “Scientific claim verification with fine-\ntuned nli models,” in Proceedings of the 16th Interna-\ntional Conference on Agents and Artificial Intelligence\n(ICAART).\nSCITEPRESS, November 2024.\n[45] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan,\nL. Gutierrez, T. F. Tan, and D. S. W. Ting, “Large\nlanguage models in medicine,” Nature medicine, vol. 29,\nno. 8, pp. 1930–1940, 2023.\n[46] S. Tonmoy, S. Zaman, V. Jain, A. Rani, V. Rawte,\nA. Chadha, and A. Das, “A comprehensive survey of hal-\nlucination mitigation techniques in large language mod-\nels,” arXiv preprint arXiv:2401.01313, January 2024.\n[47] I. Vykopal, M. Pikuliak, S. Ostermann, and M. ˇSimko,\n“Generative large language models in automated fact-\nchecking: A survey,” arXiv preprint arXiv:2407.02351,\nJuly 2024.\n[48] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang,\nQ. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, “A\nsurvey on hallucination in large language models: Prin-\nciples, taxonomy, challenges, and open questions,” arXiv\npreprint arXiv:10.1145/3703155, 2023.\n[49] A. Ben Abacha, W.-W. Yim, Y. Fu, Z. Sun, F. Xia,\n18\n"}, {"page": 19, "text": "and M. Yetisgen, “Overview of the mediqa-corr 2024\nshared task on medical error detection and correction,”\nin Proceedings of the 2024 Conference on Clinical Natu-\nral Language Processing (ClinicalNLP).\nAssociation for\nComputational Linguistics, January 2024, pp. 596–603.\n[50] F. Ding, C. Xu, H. Liu, B. Zhou, and H. Zhou, “Bridg-\ning pre-trained models to continual learning: A hyper-\nnetwork based framework with parameter-efficient fine-\ntuning techniques,” Information Sciences, vol. 674, p.\n120710, 2024.\n[51] N. F. Noy, N. H. Shah, P. L. Whetzel, B. Dai, M. Dorf,\nN. Griffith, C. Jonquet, D. L. Rubin, M.-A. Storey, C. G.\nChute, and M. A. Musen, “Bioportal: ontologies and in-\ntegrated data resources at the click of a mouse,” Nucleic\nAcids Research, vol. 37, no. suppl 2, pp. W170–W173,\n2009.\n[52] F. M. Schmidt, A. Cohen, S. Gottifredi, and A. J. Garc´ıa,\n“Improving natural language arguments’ identification by\nleveraging semantic similarity,” Information Sciences, p.\n122954, 2025.\n[53] N. Pr¨ollochs, S. Feuerriegel, B. Lutz, and D. Neumann,\n“Negation scope detection for sentiment analysis: A re-\ninforcement learning framework for replicating human\ninterpretations,” Information Sciences, vol. 536, pp. 205–\n221, 2020.\n[54] A. E. W. Johnson, T. J. Pollard, L. H. Shen, L. H.\nLehman, M. Feng, M. Ghassemi, B. Moody, P. Szolovits,\nL. A. Celi, and R. G. Mark, “Mimic-iii, a freely accessible\ncritical care database,” Scientific Data, vol. 3, p. 160035,\n2016.\n[55] Anthropic, “Releasing claude instant 1.2,” https://www.\nanthropic.com/index/releasing-claude-instant-1-2, 2023,\naccessed: October 19, 2025.\n[56] ——, “Claude 2,” https://www.anthropic.com/index/\nclaude-2, 2023, accessed: October 19, 2025.\n[57] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat,\nX. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez et al.,\n“Code llama: Open foundation models for code,” arXiv\npreprint arXiv:2308.12950, 2023.\n[58] T.\nHan,\nL.\nC.\nAdams,\nJ.-M.\nPapaioannou,\nP. Grundmann, T. Oberhauser, A. Figueroa, A. L¨oser,\nD. Truhn,\nand K. K. Bressem,\n“Medalpaca – an\nopen-source\ncollection\nof\nmedical\nconversational\nai\nmodels and training data,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2304.08247\n[59] A. Toma,\nP. R. Lawler,\nJ. Ba,\nR. G. Krishnan,\nB. B. Rubin, and B. Wang, “Clinical camel:\nAn\nopen expert-level medical language model with dialogue-\nbased knowledge encoding,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2305.12031\n[60] G. Lee, H. Hwang, S. Bae, Y. Kwon, W. Shin, S. Yang,\nM. Seo, J.-Y. Kim, and E. Choi, “Ehrsql: A practical\ntext-to-sql benchmark for electronic health records,” Ad-\nvances in Neural Information Processing Systems, vol. 35,\npp. 15 589–15 601, 2022.\n[61] S. Kweon, J. Kim, J. Kim, S. Im, E. Cho, S. Bae,\nJ. Oh, G. Lee, J. H. Moon, S. C. You, S. Baek,\nC.\nH.\nHan,\nY.\nB.\nJung,\nY.\nJo,\nand\nE.\nChoi,\n“Publicly shareable clinical large language model built\non synthetic clinical notes,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2309.00237\n[62] H. Touvron, L. Martin, K. Stone, P. Albert, A. Alma-\nhairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,\nS. Bhosale et al., “Llama 2: Open foundation and fine-\ntuned chat models,” arXiv preprint arXiv:2307.09288,\n2023.\n19\n"}]}