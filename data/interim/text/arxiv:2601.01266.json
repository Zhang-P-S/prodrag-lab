{"doc_id": "arxiv:2601.01266", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.01266.pdf", "meta": {"doc_id": "arxiv:2601.01266", "source": "arxiv", "arxiv_id": "2601.01266", "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment", "authors": ["Rhitabrat Pokharel", "Hamid Reza Hassanzadeh", "Ameeta Agrawal"], "published": "2026-01-03T19:24:51Z", "updated": "2026-01-08T18:28:40Z", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.01266v2", "url_pdf": "https://arxiv.org/pdf/2601.01266.pdf", "meta_path": "data/raw/arxiv/meta/2601.01266.json", "sha256": "6453c54164f6465497006cabca99f6154d40cad02fdd628e21cdee7ea8947553", "status": "ok", "fetched_at": "2026-02-18T02:23:22.781221+00:00"}, "pages": [{"page": 1, "text": "From Policy to Logic for Efficient and Interpretable Coverage Assessment\nRhitabrat Pokharel1, Hamid Reza Hassanzadeh2, Ameeta Agrawal1\n1Department of Computer Science, Portland State University\n2Optum AI\n{pokharel, ameeta}@pdx.edu, hamid.hassanzadeh@optum.com\nAbstract\nLarge Language Models (LLMs) have demonstrated strong\ncapabilities in interpreting lengthy, complex legal and pol-\nicy language. However, their reliability can be undermined by\nhallucinations and inconsistencies, particularly when analyz-\ning subjective and nuanced documents. These challenges are\nespecially critical in medical coverage policy review, where\nhuman experts must be able to rely on accurate information.\nIn this paper, we present an approach designed to support hu-\nman reviewers by making policy interpretation more efficient\nand interpretable. We introduce a methodology that pairs a\ncoverage-aware retriever with symbolic rule-based reasoning\nto surface relevant policy language, organize it into explicit\nfacts and rules, and generate auditable rationales. This hy-\nbrid system minimizes the number of LLM inferences re-\nquired which reduces overall model cost. Notably, our ap-\nproach achieves a 44% reduction in inference cost alongside a\n4.5% improvement in F1 score, demonstrating both efficiency\nand effectiveness.\nIntroduction\nHealthcare procedures encompass a wide range of diag-\nnostic, therapeutic, and preventive services. From routine\ncheck-ups and laboratory tests to complex surgical inter-\nventions, each procedure must be accurately recorded and\ncommunicated across healthcare systems to ensure quality\ncare and regulatory compliance. To achieve this standard-\nization, the healthcare industry relies on codes like Current\nProcedural Terminology (CPT) codes. Healthcare providers,\ninsurance companies, and other stakeholders rely on these\ncodes to communicate effectively and process claims for a\nwide range of medical procedures. Accurate interpretation\nof CPT codes and policy documents is essential for deter-\nmining whether the code aligns with specific policy provi-\nsions. Figure 1 illustrates a high level workflow involved in\nCPT code analysis. Given the intricate and often subjective\nnature of healthcare policy documents, this process can be\ntime-consuming and susceptible to inconsistencies. These\nchallenges highlight the need for more efficient and reliable\napproaches to support policy logic tracing and ensure con-\nsistency within the healthcare domain.\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nPatient\nReceive care\nProvider\nSubmit CPT code\nAnalysis Result\nHuman Reviewer\nAnalyze and review the \nCPT code and the policy\nFigure 1: High level pipeline of CPT code analysis in health-\ncare.\nRecent advances in Large Language Models (LLMs)\nhave demonstrated considerable promise in decision-making\nacross domains such as legal analysis and healthcare policy\ninterpretation (Ryu et al. 2025; Pan et al. 2023; Xu et al.\n2024), owing to their ability to process and interpret com-\nplex natural language. While it is generally accepted that\nLLMs are adept at analyzing textual information, they of-\nten face difficulties when analyzing lengthy complex texts.\nEffective reasoning in these contexts requires models to ex-\nplicitly reference factual details and policy language, which\nare frequently dispersed throughout the document. Further-\nmore, LLMs may exhibit hallucinations and inconsistencies\nin their reasoning (Dahl et al. 2024).\nChain-of-thought (CoT) prompting (Wei et al. 2022) is\na commonly explored technique (Kant et al. 2025) for guid-\ning LLMs through multi-step reasoning processes. However,\nCoT approaches are not immune to lack of interpretability,\ngenerating inconsistent reasoning, and can be computation-\nally expensive when applied at scale.\nExpert systems have been used to simulate the reason-\ning abilities of human experts by encoding domain-specific\nknowledge and reasoning processes (Li et al. 2024; Garrido-\nMerch´an and Puente 2025). These systems operate on struc-\ntured representations of knowledge, such as facts and “if-\nthen” rules, using deterministic logic to systematically pro-\narXiv:2601.01266v2  [cs.CL]  8 Jan 2026\n"}, {"page": 2, "text": "cess knowledge. By applying predefined rules, it enables ex-\npert systems ensure consistency and interpretability.\nIn this paper, we introduce a neuro-symbolic approach\nthat emulates the structured reasoning patterns of expert\nsystems to assist human reviewers with understanding and\ntracing policy logic. Our method integrates neural compo-\nnents for processing natural language with symbolic reason-\ning modules that apply clearly defined rules. By translating\npolicy terms into a machine-interpretable format, the sys-\ntem can organize and surface the relevant logical conditions\nunderlying a policy. In doing so, it provides interpretable ra-\ntionales that point directly back to the governing policy lan-\nguage. Crucially, the system does not make coverage deter-\nminations; human reviewers maintain full adjudication au-\nthority. Instead, our approach serves as a tool to find sup-\nport from coverage documents to support auditable reason-\ning throughout the review process.\nOur major contributions are as following.\n• We develop a framework that supports human experts\nwith analyzing complex documents by integrating neuro-\nsymbolic approach.\n• To support transparent tracing, we implement a coverage-\naware retriever that accurately identifies and extracts\ngoverning policy language relevant to specific CPT\ncodes.\n• We provide a rule-based system that significantly low-\ners the cost of reasoning compared to continuous LLM\ninference.\nRelated Work\nTranslating Natural Language into First-Order\nLogic\nThe use of LLMs to translate natural language into for-\nmal rules has gained significant attention in recent re-\nsearch. Techniques such as prompting with CoT and neuro-\nsymbolic approaches (Wei et al. 2022; Nezhad, Li, and\nAgrawal 2025; Nezhad and Agrawal 2025) have shown\npromising results in this area. Contractual language often\nencodes logical relationships in natural language that hu-\nmans interpret with ease. Several recent systems, includ-\ning CLOVER (Ryu et al. 2025), LOGIC-LM (Pan et al.\n2023), LogicLLaMA (Yang et al. 2024), ProSLM (Vakharia\net al. 2024), Thought Like Pro (Tan et al. 2024), Symb-\nCoT (Xu et al. 2024), and LLM-Tres (Toroghi et al. 2024),\nhave explored methods for translating natural language into\nfirst-order logic representations. These approaches aim to\nbridge the gap between unstructured contract language and\nmachine-interpretable rules. Our work is in line with these\nefforts to formalize natural language reasoning, but differs\nin that we employ a rule-based expert system to operational-\nize domain knowledge through executable reasoning rather\nthan formal logical translation.\nLLMs for Legal Reasoning\nLLMs have demonstrated notable progress in supporting le-\ngal decision-making tasks, including the following exam-\nples. Guha et al. (2023) introduced LegalBench, a compre-\nhensive suite of benchmarks to evaluate the legal reasoning\ncapabilities of LLMs. Yao et al. (2025) proposed a reinforce-\nment learning-based approach for legal question answering.\nMishra et al. (2025) conducted an error analysis of LLM rea-\nsoning in civil procedure contexts. Dahl et al. (2024) exam-\nined the prevalence of hallucinations in LLMs applied to le-\ngal domains. Yue et al. (2024) presented LawLLM, a system\noffering legal reasoning services through fine-tuning tech-\nniques. Shen et al. (2025) introduced a reasoning schema\nfor legal tasks that integrates factual grounding. Similarly,\nShi et al. (2025) proposed LegalReasoner, which first identi-\nfies disputes to decompose complex cases and then performs\nstep-wise reasoning. Our approach focuses on using action-\nable rules generated from text.\nLLMs for Rule-Making\nCummins et al. (2025) explored the potential of using com-\nputable rules for decision-making in a specific domain,\nleveraging Prolog-like logical representations to encode pol-\nicy logic in a machine-readable format. Their work high-\nlights the importance of formal rule structures in enabling\nautomated reasoning, but relies heavily on manual encoding\nand domain expertise for rule construction.\nBuilding on this foundation, Kant et al. (2025) conducted\nan extensive evaluation of the current generation of LLMs in\nproducing structured rules from policy documents. In their\nstudy, the authors systematically compared the decision-\nmaking capabilities of LLMs using both prompt-based ap-\nproaches and rule-based frameworks. LLMs were tasked\nwith generating structured rules from policy text, guided by\nhuman-designed schemas featuring specific attributes and\nhelper functions. Their findings show that most LLMs ex-\nhibit markedly improved reasoning performance when in-\ntegrated with explicit rule-based scaffolding, underscoring\nthe value of structured logic in complex adjudication tasks.\nHowever, their approach is limited by the scope of the pro-\nvided schemas and helper functions, constraining reasoning\nto facts explicitly represented in the input.\nIn contrast to prior approaches, our work advances\nthis line of research by eliminating the need for human-\ncurated schemas and helper functions. We focus on dy-\nnamic rule generation from natural language, leveraging\nfinetuned models and symbolic reasoning to automatically\nextract governing policy language and generate rules with\nthe intent to offers a scalable and cost-effective solution that\nreduces manual effort and reliance on frequent LLM infer-\nence.\nCoverage Documents\nCoverage documents serve as references that outline the cri-\nteria and provisions associated with a given policy. They are\norganized into multiple sections and subsections, each pro-\nviding detailed information about the procedures, services,\nor conditions addressed by the policy. These subsections\ncontain specific language that clarifies the scope of the pol-\nicy that guides how particular provisions apply in various\ncontexts. A sample subsection is presented in Figure 2.\n"}, {"page": 3, "text": "Diabetes Self Management and Training/Diabetic Eye Exams/Foot Care Outpatient self-management training for the \ntreatment of diabetes, education and medical nutrition therapy services. Services must be ordered by a Physician and \nprovided by appropriately licensed or registered health care professionals who are authorized to prescribe such items and \nwho demonstrate adherence to minimum standards of care for diabetes mellitus as adopted and published by the Diabetes \nInitiative. Benefits also include medical eye exams (dilated retinal exams) and preventive foot care for diabetes. Diabetic Self\nManagement Items Insulin pumps and supplies and continuous glucose monitors for the management and treatment of \ndiabetes, based upon your medical needs. An insulin pump is subject to all the conditions of coverage stated under Durable \nMedical Equipment (DME), Orthotics and Supplies. Benefits for blood glucose meters including continuous glucose monitors, \ninsulin syringes with needles, blood glucose and urine test strips, ketone test strips and tablets and lancets and lancet devices \nare described under the Outpatient Prescription Drug Rider.\nFigure 2: A sample text from Diabetes Services subsection of a plan document.\nRetriever\nCPT Codes and \nDescription\nSubsection Text\nAttributes\nAttribute Values\nSubsection Text\nSubsection Text\nCPT Code\nPyKnow Rules\nAttributes\nAttributes and \nvalues for each \nCPT Code\nPyKnow Rules\nSupporting \nattributes\nRules Per Plan \nDocument\nStore rules \nfor later use\nAttribute Generation\nRule Generation per Plan Document\nPyKnow Engine Inference\nMaintain a database of \nattributes and values for \neach CPT code\nHuman Expert\nFigure 3: Pipeline of our neuro-symbolic approach. This system supports human reviewers by making the underlying policy\nlogic interpretable.\nMethodology\nWe define our task as follows: given a CPT code, its ac-\ncompanying description, and a policy document, the objec-\ntive is to generate an interpretable reasoning trace that links\nthe code to the relevant policy language. We first describe\nour approach for extracting relevant coverage language from\npolicy documents, and then discuss the process of formulat-\ning symbolic rules. Overall pipeline is shown in Figure 3.\nPolicy Text Retrieval Phase\nA core challenge in reasoning over symbolic rules is retriev-\ning policy language that governs coverage, rather than sim-\nply matching on topical similarity. Standard semantic search\nis ill-suited for this task because the signals that determine\nbenefit status are often orthogonal to thematic content. For\ninstance:\n• A CPT for insulin pump initiation is thematically close\nto passages about diabetes self-management education,\nnutrition therapy, or endocrinology follow-ups, none of\nwhich determine its benefit status. The governing rule\nis more likely a short paragraph under Durable Medical\nEquipment or a specific policy rider.\n• A CPT for continuous glucose monitoring may clus-\nter with general diabetes monitoring advice or HbA1c\nscreening policies, while the actual coverage clause is of-\nten a concise exclusion or a limitation found in a separate\nsection.\n• CPTs for debridement or wound care sit near con-\ntent about diabetic foot care or peripheral neuropathy,\nwhereas the decisive language is typically found in sur-\ngical necessity sections that include prior-authorization\nrequirements.\nTo align retrieval with what matters for reasoning, we\ndeveloped a coverage-aware retriever. Instead of optimiz-\ning for semantic likeness, we trained a cross-encoder to\nscore subsections by their likelihood of explicitly governing\na CPT’s coverage, limitations, or exclusions.\nExpert-Labeled Supervision. We built an internal annota-\ntion platform and engaged approximately 20 certified coding\nSubject Matter Experts (SMEs), with an arbitration process\nto ensure consistency. For each CPT across 172 Certificates\n"}, {"page": 4, "text": "of Coverage/Summary Plan Documents (CoCs/SPDs), these\nSMEs selected only the passages that decide coverage. This\neffort produced over 1.84 million labeled (CPT, subsection,\nrelevance) pairs, with 10% reserved for validation and the\nremainder (∼1.61 million) used for training. This dataset\nprovides the direct supervision needed to learn the nuances\nof policy language.\nFormulation as Contrastive Multiple-Choice Ranking.\nThe task is framed as a multiple-choice ranking problem\nwith a contrastive objective. For a given CPT query q and its\nset of candidate subsections from a plan, {s1, . . . , sn}, one\npassage si is labeled positive (the true coverage passage),\nand the rest are negatives. The model processes each query-\npassage pair (q, si) to output a logit; a softmax over all log-\nits yields a probability distribution. Training minimizes the\ncross-entropy loss on the positive label:\nL = −log p(i = positive | q, S)\nThis formulation, functionally equivalent to contrastive\nobjectives like InfoNCE (Oord, Li, and Vinyals 2018),\nforces the model to distinguish the single governing passage\nfrom a set of highly relevant but non-dispositive distractors\nfrom the same document. Queries are constructed as:\n⟨CPT⟩: ⟨lay description⟩\nand choices are the raw subsection texts.\nArchitecture and Training. The model is a Longformer-\nForMultipleChoice fine-tuned from the allenai/longformer-\nbase-40961 backbone. Its 1,536-token context window is\nsufficient to process entire subsections, including nested for-\nmatting, without truncation. Key training parameters include\nthe AdamW optimizer (learning rate 2e-5, weight decay\n0.01), bf16 mixed precision, and gradient checkpointing for\nmemory efficiency. The model was trained for 2.5 epochs\n(∼48 hours) on a single node with 8 x H100 GPUs, with\nperiodic evaluation and checkpointing. To accelerate exper-\nimentation, the dataset was pre-tokenized once and reused\nacross training runs.\nWhy a Cross-Encoder Is Feasible and Effective. A cross-\nencoder architecture, which jointly processes the query and\npassage through its attention layers, is essential for this\ntask. It can capture the fine-grained interactions between\na CPT code and subtle policy phrases, such as “prior au-\nthorization required,” “not covered,” or “limited to”, which\nare often lost in compressed vector representations. While\ncomputationally intensive, this approach is feasible in our\nsetting because the candidate pool per plan is small and\nwell-defined (typically < 60 subsections across the rele-\nvant “Covered Services” and “Exclusions & Limitations”\nsections). Exhaustive scoring requires a trivial number of\nforward passes per CPT on modern GPUs, and the gains\nin precision are substantial. The model learns to prioritize\nshort, auditable policy fragments that a downstream rule-\nbased engine can deterministically parse, making the entire\nRAG stack coverage-aware by design.\n1https://huggingface.co/allenai/longformer-base-4096\nInference Pipeline.\n• Preparation: For each project, all subsections from the\nrelevant plan sections are loaded as the candidate pool.\n• Scoring: For each CPT, the query is constructed and\nscored against every candidate subsection using the\ntrained cross-encoder. Logits are softmax-normalized\nacross the candidate set for that CPT.\n• Filtering: Passages with a probability above a threshold\nτ (default 0.25) are retained, capped at a maximum of\nfive from “Covered Services” and five from “Exclusions\n& Limitations.” In real-world deployment, if retrieval re-\nturns no passage above τ, the system should escalate the\ncase for human review.\n• Output: Results are streamed progressively, with each\nrow containing project id, CPT code, probability, section,\nsubsection text, and other metadata. If no passage clears\nthe threshold for a CPT, a placeholder row is emitted to\nensure downstream stages can track completeness.\nSymbolic Phase\nIn this phase, a rule-based system is created to system-\natically encode the coverage criteria to help process the\nCPT codes. This system is implemented using PyKnow2, a\nPython library for symbolic reasoning. PyKnow facilitates\nthe creation of expert systems by offering tools to define\nfacts (units of information), fields (data within facts), and\nrules (logic for reasoning over facts). In this study, we use\nthe term “attributes” to refer to the fields. Our methodology\nfirst extracts attributes associated with each CPT code, fol-\nlowed by generating rules derived from coverage text, and fi-\nnally executing the PyKnow inference engine to apply these\nrules and simulate expert reasoning over the data.\nAttribute Generation. Attributes represent distinct proper-\nties that characterize a CPT code and relate it to specific pol-\nicy provisions. Each attribute reflects a specific characteris-\ntic of the procedure, such as whether it pertains to mental\nhealth or preventive care. For a given coverage document,\nonce each CPT code is mapped to its relevant subsections\n(in the retrieval phase), the codes are grouped by subsec-\ntion. To generate attributes for each CPT code, the model\nis prompted to identify the properties that describe the CPT\ncode and are likely shared with the previously grouped sub-\nsections. To facililate this, a short description of the code\nis also provided. Simultaneously, the model assigns default\nvalues (True or False) to these attributes based on the ex-\ntracted information. Each attribute is framed as a yes/no\nquestion, meaning it should provide a clear “yes” or “no” re-\nsponse regarding the applicability of the attribute to the CPT\ncode. For example, an attribute might be is implant,\nwhere the value is either True or False. The prompt used\nfor this process is detailed in Figure 4. Attributes for CPT\ncodes are created only once, and the same attributes can be\nreused for new plan documents. This approach ensures scal-\nability while minimizing costs. We use 10 plan documents\nduring this step.\n2https://github.com/buguroo/pyknow\n"}, {"page": 5, "text": "You are a healthcare data architect designing a PyKnow rules engine that determines if a particular CPT code is covered \nunder a health-insurance plan.  \nTASKS\n1) Brainstorm every plausible Boolean attribute (i.e., a property that can be answered with True/False) that might characterize \nthe given CPT code.\n2) In addition to attributes suggested by the CPT description, also review the coverage policy terms provided, and include \nattributes from the policy that characterize the CPT code.\n3) Use standard medical terms for the attribute names. All attribute names must follow the controlled-vocabulary pattern\n<is>_\nExamples: is_surgical, is_outpatient\n• When in doubt, default to the most widely-used medical term. Do not generate long attribute names.\n• Do not create attributes whose value can be determined from the coverage plan only. \ne.g. is_covered_under_dme. The attribute value should be able to be determined from the cpt description only.\n• Do not skip any clinical methods mentioned in the cpt description.\n• In addition, identify implicit functional context from CPT descriptions. \nExample: a procedure may imply \"surgery\" without using the word; extract such context-aware attributes as well.\n4) For each attribute, provide:  \n• attribute_name (snake_case)\n• default_value (True or False) you would assume for the cpt code based on its CPT description\n5) General instructions  \n• Do not use verbose attribute naming such as using \"_related\" or \"_involved\" at the end. For example,\na) Use \"is_mental_health\" instead of \"is_mental_health_related\".\nb) Use \"is_scopic\" instead of verbose alternatives \"is_scopic_procedure\".\nc) Use \"is_prosthetic_implant\" instead of \"is_prosthetic_implant_involved\" or \"is_prosthetic_implant_included\".\n• Use a single, non-negated attribute when its truth value can express both presence and absence—avoid redundant \nnegated forms.\ne.g. Instead of having both is_surgical and is_non_surgical, just use is_surgical.\n• In addition, create at most 1 or 2 context-aware attributes. eg. if a procedure is related to storage of reproductive item, \ngenerate is_storage and is_reproduction.\n• Return ONLY the JSON list; no prose, no Markdown; no \"```json\".\n• Do NOT include the CPT code or its description in the JSON.\nINPUT\n1) CPT code and its description\n2) Relevant Coverage Subsection\nFigure 4: The prompt used for attribute generation.\n@Rule(Procedure(\nis_telemedicine=True,\nis_real_time=True,\nis_audio_video=True,\nis_home_based=True,\nis_non_facility=True\n)\n)\ndef virtual_care_inclusion(self):\nself.result = \"covered\"\nself.category = \"Virtual Care\"\nFigure 5: An example of a rule generated. Each rule is in this\nsimilar format.\nRule Creation. In this step, symbolic rules are generated us-\ning a well-structured and guided prompt based on the cover-\nage text and relevant attributes. For each subsection, the as-\nsociated CPT codes are grouped along with their attributes.\nThese attributes help represent the coverage document in\nthe form of PyKnow rules. The reason for incorporating at-\ntributes during rule creation is to ensure that rules are created\nusing only the relevant attributes. Without this, attributes not\nassociated with the CPT codes might be created, leading to\npotential syntax errors. For each plan document, a distinct\nset of rules is generated, with rules created for each subsec-\ntion within the document. The prompt used during this pro-\ncess is in Appendix . Specific instructions are provided to\nensure that the generated rules are free from syntax errors.\nA sample rule is presented in Figure 5.\nInference. In this step, given a CPT code and its associated\nattributes, we use the PyKnow engine to identify which rule\nis triggered. After a rule is matched, the relevant attributes\nare passed on to the human reviewer for further analysis.\nExperimental Settings\nWe use internal data, which consists of coverage docu-\nments, procedure descriptions, and corresponding human-\ngenerated determinations. The coverage documents are in\nthe form of a plain text. To ensure compliance with pri-\nvacy policies, both patient information and details related\nto specific plan documents are fully anonymized. The eval-\n"}, {"page": 6, "text": "Plan\nAccuracy\nF1 Score\nGPT4.1\nRule-based (ZR)\nRule-based (FR)\nGPT4.1\nRule-based (ZR)\nRule-based (FR)\nPlan #1\n0.88\n0.87\n0.81\n0.93\n0.93\n0.90\nPlan #2\n0.78\n0.84\n0.82\n0.87\n0.91\n0.90\nPlan #3\n0.77\n0.79\n0.89\n0.86\n0.88\n0.94\nPlan #4\n0.88\n0.83\n0.88\n0.93\n0.90\n0.93\nPlan #5\n0.83\n0.90\n0.92\n0.90\n0.94\n0.96\nPlan #6\n0.77\n0.83\n0.88\n0.86\n0.90\n0.93\nPlan #7\n0.88\n0.88\n0.90\n0.93\n0.93\n0.94\nAverage\n0.82\n0.85\n0.87\n0.89\n0.91\n0.93\nTable 1: Accuracy and F1 scores per plan. ZR = Zero-shot Retriever; FR = Finetuned Retriever. Overall, the finetuned rule-\nbased system outperforms the other methods.\nuation dataset consists of the same 814 CPT codes across 7\nseparate coverage documents (i.e. total of 5,698 codes) that\nwere never used in the retriever training or attribute creation\nphase. The same dataset is used to get all the results.\nFor our baseline experiment, we use GPT-4.1 baseline\nwith vanilla prompting along with CPT code and the en-\ntire plan document. The rule-based approach also utilizes\nGPT-4.1 for initial processing (attribute generation and rule-\nmaking). Additionally, we include other models such as o3\nand GPT5-mini in an ablation study. For evaluation, we re-\nport accuracy and F1 scores.\nResults\nThis section states both our results and our conclusions\nbased on our observations of these results.\nOverall Performance\nTo ensure that reviewers can trace how each rule is applied,\nwe first assess the accuracy of the system’s outputs and then\nexamine the interpretability of the explanations it provides.\nTable 1 shows the average accuracy and F1 scores across\nthe seven plan documents. The rule-based method using the\nfinetuned retriever consistently outperforms the other two\napproaches. Finetuning improves accuracy by an average of\n2.69% and F1 score by 1.72% over the zero-shot baseline.\nThis improvement is largely due to the finetuned retriever’s\nalignment with the language and structure of coverage poli-\ncies. Trained on a large set of expert-annotated (CPT, sub-\nsection, relevance) pairs, it improves at pinpointing the pol-\nicy passages most relevant to benefit considerations. Unlike\nstandard semantic search, which may return thematically\nsimilar but irrelevant text, the finetuned cross-encoder model\nprioritizes concise policy fragments that matter for sym-\nbolic reasoning. Better passage selection directly enhances\nthe quality of both attribute generation and rule creation in\nthe symbolic phase, since rules are built from more accu-\nrate and relevant policy language. By capturing subtle cues\nand specific policy limitations, the finetuned retriever pro-\nvides higher-quality inputs to the symbolic engine, resulting\nin more precise coverage determinations.\nOverall, these results demonstrate that integrating expert-\nlabeled supervision and contrastive ranking objectives into\nthe retriever, together with symbolic reasoning, leads to\nmeasurable gains in performance.\nInterpretability. One of our goals is to support in-\nterpretability, rather than relying solely on direct in-\nference from the model (such as interpretation made\npurely through prompting). For instance, as illustrated\nin Figure 6, consider a procedure labeled S9212. Un-\nder\nthe\ncorresponding\npolicy,\nthe\nrule-based\nsystem\nidentifies that the pregnancy maternity services\nrule is relevant. The PyKnow engine highlights this\nrule because its conditions, is pregnancy=True and\nis maternity=True, match the attributes associated\nwith the procedure. This traceability allows a human re-\nviewer to see exactly which factors contributed to the deci-\nsion, providing transparency and context. The system thus\nserves as a support tool, helping humans make informed\njudgments. Ultimately, the final decision remains in the\nhands of the human reviewer. In contrast, direct prompting\ndoes not offer this level of traceability and is more prone to\nhallucinations, making it less reliable for such task.\nCost Effectiveness. Our rule-based approach is designed for\nscalability and cost efficiency in processing large volumes\nof clinical codes. Attribute generation is performed once\nfor each CPT code; with over 11,000 CPT code, and even\nmore when extending to other code sets like HCPCS, the\nability to avoid repeated model inference becomes crucial.\nRule generation is required only once per coverage policy,\nfurther reducing computational demands. During inference,\nour system relies solely on symbolic reasoning and minimal\nhardware; it does not require a GPU or LLM-based infer-\nence, making it both fast and inexpensive. As shown in Ta-\nble 2, LLM-based methods such as GPT-5-mini, GPT-4.1,\nand o3 incur substantially higher costs, with GPT-5-mini\ncosting approximately $4,840 to process 11,000 CPT codes,\ncompared to just $22 for the finetuned rule-based system.\nEven with a one-time setup cost for training (i.e. $2,6803),\nthe rule-based approach remains highly cost-effective, es-\npecially as the number of codes grows. By eliminating the\nneed for repeated LLM inference and leveraging symbolic\nreasoning, our method offers a robust and scalable solution\n3Further details about the cost calculation are discussed in a\nlater section.\n"}, {"page": 7, "text": "S9212\nRules\npregnancy \nmaternity services \nis_pregnancy=True\nis_maternity=True\nCPT Code\nTracing\nPyKnow Engine\nAttributes\nHuman Expert\nFigure 6: Overall inference workflow illustrating how our system supports human experts in identifying relevant support from\ncoverage documents.\nModel\nContext Provided\nAcc.\nF1\nCost per 1k CPTs\nCost for 11k CPTs\nGPT-5-mini (FR)\nRetrieved text\n0.94\n0.96\n$440\n$4,840\nGPT-4.1 (FR)\nRetrieved text\n0.92\n0.95\n$880\n$9,680\nO3 (FR)\nRetrieved text\n0.94\n0.96\n$880\n$9,680\nGPT-4.1\nEntire document\n0.82\n0.89\n$3,520\n$38,720\nRule-based (ZR)\nRetrieved text\n0.85\n0.91\n$2\n$22\nRule-based (FR)\nRetrieved text\n0.87\n0.93\n$2\n$22\nTable 2: Comparison of model accuracy, F1 score, and the inference cost for processing the CPT codes. ZR = Zero-shot\nRetriever; FR = Finetuned Retriever. Accuracy and F1 scores are reported on the validation set used in this study, while cost\nestimates reflect processing of the CPT codes. The second column indicates the type of context from the policy document\nsupplied to each model, specifying whether the input was retrieved text passages or the full coverage document.\nfor large-scale coverage adjudication across diverse clinical\ncode sets.\nPerformance vs Cost Tradeoff. To further investigate cost\neffectiveness, we evaluated the use of retrieved texts as in-\nput for direct LLM inference in Table 2. Providing only the\nrelevant passages instead of the entire coverage document\nsignificantly reduce the number of input tokens required\nfor each inference, thereby lowering the overall cost. This\napproach allowed us to assess whether our retrieval-based\nmethod remains advantageous when combined with LLMs.\nThe cost rates for inference and setup vary notably across\nthe different approaches. For LLM-based methods, API\npricing is based on the number of input and output tokens\nprocessed: GPT-5-mini charges $0.25 per million input to-\nkens and $2.00 per million output tokens, while both GPT-\n4.1 and o3 are priced at $2.00 per million input tokens and\n$8.00 per million output tokens. The rule-based system that\nuses finetuning involves a one-time training cost of approx-\nimately $2,680, which was incurred by training on an ex-\nternal dataset for 48 hours using 8 × NVIDIA H100 GPUs\n(Azure H100 instances at 6.98/hour). Importantly, no GPUs\nare required for inference with the rule-based system, and\nthe per-inference cost is extremely low, just $2.50 per 1,000\nCPT codes or $22 for 11,000 codes.\nLLM-based approaches such as GPT-5-mini, GPT-4.1,\nand o3 achieve the highest accuracy and F1 scores when\nusing retrieved text as input, with average accuracy up to\n0.94 and F1 scores up to 0.96. However, this performance\ncomes at a substantial cost: processing 11,000 CPT codes\ncosts $4,840 for GPT-5-mini and $9,680 for GPT-4.1 and\no3. These methods also require a finetuned retriever for pas-\nsage selection, adding a one-time setup cost of $2,680. In\ncontrast, the rule-based approaches deliver competitive per-\nformance (accuracy up to 0.87 and F1 up to 0.93) at a frac-\ntion of the cost (∼$2700 for 11,000 CPT codes). Notably,\nmodels that process entire documents are both less accu-\nrate (0.82 accuracy, 0.89 F1) and dramatically more expen-\nsive ($38,720). Thus, while LLM-based methods provide\nslightly better accuracy with retrieved text, their inference\ncosts scale rapidly with dataset size, making the rule-based\napproach far more cost-effective for large-scale adjudication\ntasks. Choosing between these methods involves balancing\nthe goal of achieving optimal performance with the need to\nminimize operational costs. We plan to address this tradeoff\nin greater detail in future work.\nWhere do the rules fail?\nOur error analysis across all procedure codes and seven\nplans reveals that most rule failures can be attributed to two\nmain factors. The first and most prevalent is that the correct\nattribute is sometimes not incorporated into the rule creation\nprocess. This accounts for 73.5% of incorrect cases, where\na rule is triggered but does not represent the correct logic.\nThis often occurs when the attribute list is lengthy, lead-\ning the model to overlook or “forget” attributes that appear\nlater in the input sequence, a phenomenon also observed\nby (Agrawal et al. 2024; Tao et al. 2025). The second failure\nmode, making up the remaining 26.5% of errors, involves\ninstances where the model fails to generate a sufficient set\nof rules for certain samples, resulting in incomplete or miss-\ning logic and, thus, no rule being triggered for accurate cov-\nerage decisions. These findings underscore the importance\nof careful attribute selection and prompt engineering when\nconstructing symbolic rules from complex policy language.\n"}, {"page": 8, "text": "You are an expert in translating health insurance coverage documents into computable logic using the PyKnow library. Your \ngoal is to encode the health insurance contract terms into Python rules so that we can query whether a given CPT code and \ndescription are covered under the contract.\nYou will be provided with either of these or one of these.\n1. Terms of Inclusion and/or Exclusion:\n- A list of covered procedures/services, including any relevant coverage criteria.\n- A list of specifically excluded procedures or services.\n2. Attribute list\n- A list of attributes to be used to make the rules.\nTASKS:\n1. Review all Terms Carefully:\n- Analyze the Terms of Inclusion, Clinical Trial Terms, and Terms of Exclusion to fully understand the scope of coverage.\n- Logical relationships, conditions, and dependencies in the coverage terms must be faithfully represented in the PyKnow\nrules to ensure accurate and reliable query results.\n2. Translate Coverage Logic into PyKnow:   \n- Define a CoverageEngine that evaluates each procedure (CPT code) and assigns exactly one of these two outcomes: \ncovered or not covered\n- For every distinct coverage clause (e.g., each numbered or bulleted inclusion/exclusion item) create at least one explicit \n@Rule. Do not leave a clause unrepresented. Make rules for all the terms provided using the given attributes.\n- Aggregate related attributes with logical OR. Ensure syntax correctness while doing so. e.g.\n@Rule(Procedure(is_eye_related=True) | Procedure(is_visual_function=True)). You cannot use '|' inside Procedure. Do not use \n'&' or '~’. Here, both is_eye_related and is_visual_function serve similar purpose. This will ensure all the defined attributes are \nused.\n- If a procedure does not fall in any of the cases (i.e. fallback rule or the rule that is not present in the terms), do not write a \nrule for it, just leave it.\n3. Store Results, Reasons, and Rule Names:\n- For every decision, store: The outcome under `self.result`; Category under `self.category` (this is the given subsection title)\n4. Write Clean, Robust, and Complete Rules:\n- No CPT code and description pair should never match more than one rule. Handle edge cases properly.\n5. General Coding Instructions:\n- Implement the coverage rules using PyKnow’s @Rule decorators. No need for explanations.\n- The code should be free from syntax errors (e.g., TypeError). Avoid using self in TEST Lambdas, instead use class variable if \nneeded.\n- If an attribute is None, this should not cause any error e.g. AttributeError.\n- No need for the main function. Avoid using Markdown formatting like ```python.\n- Enclose the code inside <code> </code> without any extra text.\n- Do not leave \"class Procedure(Fact): pass\" like this. You need to have attributes defined here.\n6. General Instructions for Attributes:\n- Avoid documentation string and do not leave Attributes inside docstrings/comments.\n- Only use the attributes provided and do not create one on your own. \n- From the given attributes, define an attribute in the Procedure class only if it will be referenced in at least one @Rule \ncondition.\nFigure 7: The prompt used for rule generation.\nImproving attribute coverage and accuracy is the most im-\npactful path forward.\nConclusion\nIn this study, we introduced a framework designed to sup-\nport human interpretability of coverage review by combin-\ning a coverage-aware retriever with a symbolic rule-based\nreasoning engine. Our findings suggest that finetuning the\nretriever with expert-labeled supervision and a contrastive\nranking objective substantially improves its ability to surface\nthe most relevant policy language compared to zero-shot ap-\nproaches. This more accurate retrieval supports downstream\nsteps such as attribute extraction and the identification of\npolicy conditions that may be relevant for human review-\ners. A key advantage of our approach is its efficiency: by\nnarrowing the text that must be processed and reducing de-\npendence on repeated LLM calls, the system provides sig-\nnificant cost savings while maintaining high performance.\nAt the same time, limitations remain—for example, relevant\nattributes may be missed when long documents exceed in-\nput limits or when the underlying rule set does not fully\ncapture policy nuances. Addressing these challenges will\nrequire continued refinement of context handling and rule\nconstruction. Overall, our results highlight the potential of\nneuro-symbolic methods to deliver scalable, interpretable,\nand cost-effective tools that support human reviewers by\nmaking policy logic more transparent and easier to trace.\nPrompts\nFigure 7 presents the structured prompts used during rule\ngeneration.\n"}, {"page": 9, "text": "Acknowledgements\nWe thank Ardavan Saeedi and the anonymous reviewers for\ntheir constructive feedback.\nReferences\nAgrawal, A.; Dang, A.; Bagheri Nezhad, S.; Pokharel, R.;\nand Scheinberg, R. 2024.\nEvaluating Multilingual Long-\nContext Models for Retrieval and Reasoning. In S¨alev¨a, J.;\nand Owodunni, A., eds., Proceedings of the Fourth Work-\nshop on Multilingual Representation Learning (MRL 2024),\n216–231. Miami, Florida, USA: Association for Computa-\ntional Linguistics.\nCummins, J.; D´avila, J.; Kowalski, R.; and Ovenden, D.\n2025. Computable contracts for insurance: Establishing an\ninsurance-specific controlled natural language-insurle. Ac-\ncessed: Feb, 11: 2025.\nDahl, M.; Magesh, V.; Suzgun, M.; and Ho, D. E. 2024.\nLarge Legal Fictions: Profiling Legal Hallucinations in\nLarge Language Models. Journal of Legal Analysis, 16(1):\n64–93.\nGarrido-Merch´an, E. C.; and Puente, C. 2025. GOFAI meets\nGenerative AI: Development of Expert Systems by means of\nLarge Language Models. arXiv preprint arXiv:2507.13550.\nGuha, N.; Nyarko, J.; Ho, D. E.; Re, C.; Chilton, A.;\nNarayana, A.; Chohlas-Wood, A.; Peters, A.; Waldon, B.;\nRockmore, D.; Zambrano, D.; Talisman, D.; Hoque, E.;\nSurani, F.; Fagan, F.; Sarfaty, G.; Dickinson, G. M.; Porat,\nH.; Hegland, J.; Wu, J.; Nudell, J.; Niklaus, J.; Nay, J. J.;\nChoi, J. H.; Tobia, K.; Hagan, M.; Ma, M.; Livermore, M.;\nRasumov-Rahe, N.; Holzenberger, N.; Kolt, N.; Henderson,\nP.; Rehaag, S.; Goel, S.; Gao, S.; Williams, S.; Gandhi, S.;\nZur, T.; Iyer, V.; and Li, Z. 2023. LegalBench: A Collab-\noratively Built Benchmark for Measuring Legal Reasoning\nin Large Language Models. In Thirty-seventh Conference\non Neural Information Processing Systems Datasets and\nBenchmarks Track.\nKant, M.; Nabi, S.; Kant, M.; Scharrer, R.; Ma, M.; and\nNabi, M. 2025. Towards robust legal reasoning: Harness-\ning logical llms in law. arXiv preprint arXiv:2502.17638.\nLi, S.; Balachandran, V.; Feng, S.; Ilgen, J.; Pierson, E.; Koh,\nP. W. W.; and Tsvetkov, Y. 2024. Mediq: Question-asking\nllms and a benchmark for reliable interactive clinical reason-\ning. Advances in Neural Information Processing Systems,\n37: 28858–28888.\nMishra, V.; Pathiraja, B.; Parmar, M.; Chidananda, S.; Srini-\nvasa, J.; Liu, G.; Payani, A.; and Baral, C. 2025.\nInves-\ntigating the Shortcomings of LLMs in Step-by-Step Le-\ngal Reasoning. In Chiruzzo, L.; Ritter, A.; and Wang, L.,\neds., Findings of the Association for Computational Linguis-\ntics: NAACL 2025, 7795–7826. Albuquerque, New Mexico:\nAssociation for Computational Linguistics.\nISBN 979-8-\n89176-195-7.\nNezhad, S. B.; and Agrawal, A. 2025. Enhancing Large Lan-\nguage Models with Neurosymbolic Reasoning for Multilin-\ngual Tasks. In H. Gilpin, L.; Giunchiglia, E.; Hitzler, P.;\nand van Krieken, E., eds., Proceedings of The 19th Interna-\ntional Conference on Neurosymbolic Learning and Reason-\ning, volume 284 of Proceedings of Machine Learning Re-\nsearch, 1059–1076. PMLR.\nNezhad, S. B.; Li, Y.; and Agrawal, A. 2025.\nSym-\nCode: A Neurosymbolic Approach to Mathematical Rea-\nsoning via Verifiable Code Generation.\narXiv preprint\narXiv:2510.25975.\nOord, A. v. d.; Li, Y.; and Vinyals, O. 2018. Representation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nPan, L.; Albalak, A.; Wang, X.; and Wang, W. 2023. Logic-\nLM: Empowering Large Language Models with Symbolic\nSolvers for Faithful Logical Reasoning. In Bouamor, H.;\nPino, J.; and Bali, K., eds., Findings of the Association for\nComputational Linguistics: EMNLP 2023, 3806–3824. Sin-\ngapore: Association for Computational Linguistics.\nRyu, H.; Kim, G.; Lee, H. S.; and Yang, E. 2025. Divide\nand Translate: Compositional First-Order Logic Translation\nand Verification for Complex Logical Reasoning.\nIn The\nThirteenth International Conference on Learning Represen-\ntations.\nShen, J.; Xu, J.; Hu, H.; Lin, L.; Zheng, F.; Ma, G.; Meng, F.;\nZhou, J.; and Han, W. 2025. A Law Reasoning Benchmark\nfor LLM with Tree-Organized Structures including Fac-\ntum Probandum, Evidence and Experiences. arXiv preprint\narXiv:2503.00841.\nShi, W.; Zhu, H.; Ji, J.; Li, M.; Zhang, J.; Zhang, R.; Zhu,\nJ.; Xu, J.; Han, S.; and Guo, Y. 2025. LegalReasoner: Step-\nwised Verification-Correction for Legal Judgment Reason-\ning. arXiv preprint arXiv:2506.07443.\nTan, X.; Deng, Y.; Qiu, X.; Xu, W.; Qu, C.; Chu, W.; Xu,\nY.; and Qi, Y. 2024. Thought-Like-Pro: Enhancing Reason-\ning of Large Language Models through Self-Driven Prolog-\nbased Chain-of-Thought. arXiv preprint arXiv:2407.14562.\nTao, Y.; Hiatt, A.; Seetharaman, R.; and Agrawal, A. 2025.\n”Lost-in-the-Later”: Framework for Quantifying Contextual\nGrounding in Large Language Models.\narXiv preprint\narXiv:2507.05424.\nToroghi, A.; Guo, W.; Pesaranghader, A.; and Sanner, S.\n2024.\nVerifiable, Debuggable, and Repairable Common-\nsense Logical Reasoning via LLM-based Theory Resolu-\ntion. In Al-Onaizan, Y.; Bansal, M.; and Chen, Y.-N., eds.,\nProceedings of the 2024 Conference on Empirical Meth-\nods in Natural Language Processing, 6634–6652. Miami,\nFlorida, USA: Association for Computational Linguistics.\nVakharia, P.; Kufeldt, A.; Meyers, M.; Lane, I.; and Gilpin,\nL. H. 2024. Proslm: A prolog synergized language model\nfor explainable domain specific knowledge based question\nanswering. In International Conference on Neural-Symbolic\nLearning and Reasoning, 291–304. Springer.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V.; Zhou, D.; et al. 2022.\nChain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in neural information processing systems, 35:\n24824–24837.\n"}, {"page": 10, "text": "Xu, J.; Fei, H.; Pan, L.; Liu, Q.; Lee, M.-L.; and Hsu, W.\n2024. Faithful Logical Reasoning via Symbolic Chain-of-\nThought. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds.,\nProceedings of the 62nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n13326–13365. Bangkok, Thailand: Association for Compu-\ntational Linguistics.\nYang, Y.; Xiong, S.; Payani, A.; Shareghi, E.; and Fekri, F.\n2024.\nHarnessing the Power of Large Language Models\nfor Natural Language to First-Order Logic Translation. In\nKu, L.-W.; Martins, A.; and Srikumar, V., eds., Proceedings\nof the 62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), 6942–6959.\nBangkok, Thailand: Association for Computational Linguis-\ntics.\nYao, R.; Wu, Y.; Wang, C.; Xiong, J.; Wang, F.; and Liu, X.\n2025. Elevating Legal LLM Responses: Harnessing Train-\nable Logical Structures and Semantic Knowledge with Le-\ngal Reasoning. In Chiruzzo, L.; Ritter, A.; and Wang, L.,\neds., Proceedings of the 2025 Conference of the Nations\nof the Americas Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies (Volume\n1: Long Papers), 5630–5642. Albuquerque, New Mexico:\nAssociation for Computational Linguistics.\nISBN 979-8-\n89176-189-6.\nYue, S.; Liu, S.; Zhou, Y.; Shen, C.; Wang, S.; Xiao, Y.; Li,\nB.; Song, Y.; Shen, X.; Chen, W.; Huang, X.; and Wei, Z.\n2024. LawLLM: Intelligent Legal System with Legal Rea-\nsoning and Verifiable Retrieval. In Database Systems for Ad-\nvanced Applications: 29th International Conference, DAS-\nFAA 2024, Gifu, Japan, July 2–5, 2024, Proceedings, Part V,\n304–321. Berlin, Heidelberg: Springer-Verlag. ISBN 978-\n981-97-5568-4.\n"}]}