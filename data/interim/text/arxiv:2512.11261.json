{"doc_id": "arxiv:2512.11261", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.11261.pdf", "meta": {"doc_id": "arxiv:2512.11261", "source": "arxiv", "arxiv_id": "2512.11261", "title": "Leveraging LLMs for Title and Abstract Screening for Systematic Review: A Cost-Effective Dynamic Few-Shot Learning Approach", "authors": ["Yun-Chung Liu", "Rui Yang", "Jonathan Chong Kai Liew", "Ziran Yin", "Henry Foote", "Christopher J. Lindsell", "Chuan Hong"], "published": "2025-12-12T03:51:54Z", "updated": "2025-12-12T03:51:54Z", "summary": "Systematic reviews are a key component of evidence-based medicine, playing a critical role in synthesizing existing research evidence and guiding clinical decisions. However, with the rapid growth of research publications, conducting systematic reviews has become increasingly burdensome, with title and abstract screening being one of the most time-consuming and resource-intensive steps. To mitigate this issue, we designed a two-stage dynamic few-shot learning (DFSL) approach aimed at improving the efficiency and performance of large language models (LLMs) in the title and abstract screening task. Specifically, this approach first uses a low-cost LLM for initial screening, then re-evaluates low-confidence instances using a high-performance LLM, thereby enhancing screening performance while controlling computational costs. We evaluated this approach across 10 systematic reviews, and the results demonstrate its strong generalizability and cost-effectiveness, with potential to reduce manual screening burden and accelerate the systematic review process in practical applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.11261v1", "url_pdf": "https://arxiv.org/pdf/2512.11261.pdf", "meta_path": "data/raw/arxiv/meta/2512.11261.json", "sha256": "cc456740de22206592547d29fffa7e59a2ba41622dc9f5b8de4bd132364fd250", "status": "ok", "fetched_at": "2026-02-18T02:24:24.697625+00:00"}, "pages": [{"page": 1, "text": " \n1 \nLeveraging LLMs for Title and Abstract Screening for Systematic \nReview: A Cost-Effective Dynamic Few-Shot Learning Approach \n \nYun-Chung Liu1,2†, Rui Yang3†, Jonathan Chong Kai Liew4, Ziran Yin1, Henry Foote, \nMD5, Christopher J. Lindsell, PhD1,6, Chuan Hong, PhD1,6* \n \n1 Department of Biostatistics and Bioinformatics, Duke University, Durham, NC, USA    \n2 Interdisciplinary Data Science, Duke University, Durham, NC, USA    \n3 Department of Biomedical Informatics, National University of Singapore, Singapore, Singapore \n4 School of Chemistry, Nanyang Technological University, Singapore, Singapore \n5 Department of Pediatrics, Duke University School of Medicine, Durham, NC, USA    \n6 Duke Clinical Research Institute, Duke University School of Medicine, Durham, NC, USA \n \n†Yun-Chung Liu and Rui Yang are equally contributed \n*Correspondence: Chuan Hong \nEmail: chuan.hong@duke.edu \n \nKeywords: Large Language Models; Systematic Review; Literature Screening; \nEvidence Synthesis \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 2, "text": " \n2 \nAbstract \nSystematic reviews are a key component of evidence-based medicine, playing a \ncritical role in synthesizing existing research evidence and guiding clinical decisions. \nHowever, with the rapid growth of research publications, conducting systematic \nreviews has become increasingly burdensome, with title and abstract screening being \none of the most time-consuming and resource-intensive steps. To mitigate this issue, \nwe designed a two-stage dynamic few-shot learning (DFSL) approach aimed at \nimproving the efficiency and performance of large language models (LLMs) in the title \nand abstract screening task. Specifically, this approach first uses a low-cost LLM for \ninitial screening, then re-evaluates low-confidence instances using a high-\nperformance LLM, thereby enhancing screening performance while controlling \ncomputational costs. We evaluated this approach across 10 systematic reviews, and \nthe results demonstrate its strong generalizability and cost-effectiveness, with \npotential to reduce manual screening burden and accelerate the systematic review \nprocess in practical applications. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 3, "text": " \n3 \n1. Introduction \nSystematic reviews are typically recognized as a key component of evidence-based \nmedicine, playing a critical role in synthesizing existing research evidence and \nguiding clinical decisions [1,2]. However, with the rapid increase in research \npublications [3], conducting systematic reviews has become time-consuming [4]. A \ntypical systematic review involves several key steps: identifying research objectives, \ndefining inclusion/exclusion criteria, searching databases, screening titles and \nabstracts, reviewing full texts, extracting data, and synthesizing results [5]. Among \nthese, the title and abstract screening stage alone can demand substantial resources \nto review thousands of publications [6]. \n \nTo reduce the manual screening burden in systematic reviews, researchers have \nexplored different automated screening methods. Early studies combined clinical \nterm frequency distribution with word embeddings to identify studies aligned with \nresearch objectives [7]. Subsequent studies directly leveraged word embedding \ntechniques to convert abstracts into embedding vectors and classify relevant studies \n[8]. Another study employed sentence transformers for training, performing \ncontrastive learning by pairing included and excluded samples, enabling the model to \nlearn the similarity features between them, thereby assisting in screening [9].  \n \nRecently, large language models (LLMs) have demonstrated exceptional capabilities \nin text understanding and generation [10,11] and have been explored for application \nto the title and abstract screening task in systematic reviews. Existing studies have \nexplored zero-shot settings by designing prompts to guide LLMs in identifying \nrelevant studies directly [12,13]. Additionally, chain-of-thought techniques have been \nintroduced to enhance LLMs’ reasoning capabilities in the screening task by \nprompting them to perform step-by-step analysis [14]. Others have adopted multi-\nagent frameworks to simulate real decision-making processes, where agents discuss \ninclusion and exclusion criteria, and reach consensus [15]. As LLMs continue to \n"}, {"page": 4, "text": " \n4 \nadvance, their potential in assisting systematic review screening grows, promising to \nreduce researchers’ workload and improve efficiency [2].  \n \nHowever, despite their demonstrated potential, LLM-driven screening approaches \nmay exhibit inconsistent performance when applied to diverse medical topics with \nvarying data distributions [13]. Additionally, the computational cost associated with \ndeploying high-performance LLMs can be substantial, making it impractical for large-\nscale systematic reviews without cost-effective strategies. This highlights the need for \na generalizable and economically feasible approach that can effectively balance \nscreening performance and resource allocation. \n \nTo address the dual challenge of performance and cost in large-scale title and abstract \nscreening across diverse medical topics, we developed a dynamic few-shot learning \n(DFSL) approach designed to efficiently integrate into the systematic review process. \nThe DFSL approach adopts a two-stage structure: the first stage performs initial \nscreening using a low-cost LLM guided by dynamically constructed few-shot prompts \nand provides confidence scores; the second stage re-evaluates low-confidence \ninstances using a high-performance LLM. This approach achieves an effective balance \nbetween screening performance and computational resources by combining adaptive \nprompting with the confidence-driven triage mechanism. We evaluated this approach \non ten systematic review datasets covering different stages including diagnostic, \nintervention, and prognostic studies, encompassing nearly 10,000 studies [16]. \n \n2. Methods \nIn the Methods section, we first describe the standard process of systematic reviews. \nSubsequently, we provide a detailed description of the data curation process of this \nwork, benchmarking of different prompting strategies, and the design of the DFSL \napproach, which includes a cost-effective two-stage screening strategy that achieves \ndual improvements in both performance and efficiency through the adoption of \ndynamic few-shot prompting and the confidence-driven LLM selection mechanism. \n"}, {"page": 5, "text": " \n5 \n \n2.1 Standard Systematic Review Workflow \nSystematic reviews follow a structured, multi-step process to ensure comprehensive \nand unbiased evidence synthesis [5]. Key steps typically include: (1) formulating a \nresearch objective; (2) defining inclusion and exclusion criteria; (3) conducting \nsystematic searches across databases; (4) screening titles and abstracts for relevance; \n(5) reviewing full texts; (6) extracting data; and (7) synthesizing results through \nqualitative or quantitative analysis. Among these, title and abstract screening is \nparticularly time-consuming and resource-intensive, as it involves manually \nscreening thousands of studies to determine eligibility based on predefined criteria. \nThis step serves as a critical bottleneck and is the primary focus of the DFSL approach \ndeveloped in this study. \n \n2.2 Data Curation \nWe utilized the CLEF 2019 dataset, which includes 31 systematic reviews covering \ndiagnosis, intervention, and prognosis, each providing PMIDs of all studies retrieved \n[14]. To evaluate our framework, we selected a subset of 10 systematic reviews (5 \ndiagnostic reviews, 4 intervention reviews, and 1 prognostic review) [15-24]. The \nselection criteria were systematic reviews with more than 100 studies retrieved \nduring the search stage and a reasonable inclusion ratio. This criterion is crucial as a \nsmall sample size or low inclusion ratio could result in unstable evaluation outcomes \nwith limited statistical significance. Using the provided PMIDs, we retrieved the \ncorresponding titles and abstracts for each study through the PubMed API, excluding \nrecords with missing content and subsequently performing deduplication to \nconstruct the final evaluation dataset. The inclusion and exclusion criteria were \ndirectly obtained from the original systematic reviews. \n \n2.3 Benchmarking Prompting Strategies \nTo ensure fair comparison, all methods (zero-shot, chain-of-thought, few-shot, and \nDFSL) used the same model with identical settings. The only difference across \nmethods lies in the prompt construction strategy itself. \n"}, {"page": 6, "text": " \n6 \n \n2.3.1 Zero-Shot Learning  \nZero-shot learning refers to an LLM's ability to produce outputs without any training \ninstances [10, 11]. In the systematic review screening task, we directly input the title, \nabstract, and inclusion/exclusion criteria into the LLM, instructing it to output either \n\"include\" or \"exclude\" without providing any instances with labels. \n \n2.3.2 Chain-of-Thought \nChain-of-thought is a strategy that guides LLMs through a step-by-step reasoning \nprocess [27]. Unlike directly generating predictions, this approach requires the LLM \nto produce a sequence of logical steps before arriving at the final decision. In the \ncontext of abstract screening, we extended the zero-shot setting by adding \ninstructions such as “think step by step”, encouraging the LLM to reason through the \ndecision-making process. \n \n2.3.3 Few-Shot Learning \nFew-shot learning guides LLMs to learn specific tasks by providing a small number of \nlabeled examples [28]. In the title and abstract screening task, we randomly provided \nseveral known instances of titles and abstracts that should be included or excluded, \nserving as references for the LLM's screening process. \n \n2.3.4 DFSL: A Cost-Effective Dynamic Few-Shot Learning Screening Approach \nInspired by prior work on dynamic instance selection [29], we propose the DFSL \napproach. This approach first clustered studies and selected representative inclusion \nand exclusion instances from each cluster to construct an instance pool. Subsequently, \nfor each study to be classified, the most similar instances were dynamically selected \nfrom the instance pool based on its cluster assignment, thereby constructing dynamic \nfew-shot prompts. We systematically applied this approach to the title and abstract \nscreening task in systematic reviews. Figure 1 presents an overview of the entire \npipeline. \n"}, {"page": 7, "text": " \n7 \n \nFigure 1 Overview of the entire title and abstract screening pipeline. The pipeline \nincludes: (A) extracting titles and abstracts from systematic reviews; (B) generating text \nembeddings and performing dimensionality reduction; (C) conducting clustering, \ndynamically constructing few-shot prompts, and performing initial screening with \nconfidence scores; (D) using a high-performance LLM to re-evaluate low-confidence \ninstances (identified based on confidence scores below the threshold of 0.9 in the initial \nscreening stage, where confidence scores range from 0 to 1). \n \nSpecifically, we utilized “MedCPT Article Encoder” [30] to generate text embeddings \nfor all studies, followed by dimensionality reduction to 2 dimensions through the \nUMAP [31] algorithm. Subsequently, we applied the K-means algorithm [32] for \nclustering and, according to the data characteristics, selected one inclusion instance \nand two exclusion instances from each cluster to construct dynamic few-shot \nprompts. This approach not only addresses the randomness problem in traditional \nfew-shot learning instance selection but also optimizes screening performance across \ndiverse medical topics. Additionally, the DFSL approach supports confidence \nevaluation by requiring the LLM to generate confidence scores ranging from 0 to 1 for \neach prediction, allowing further optimization of low-confidence instances. The \nevaluation prompt for the DFSL approach is provided in Supplementary Information \nA. \n \nTitle\nTitle\nAbstract\nAbstract\nA. Title / Abstract Extraction\nAbstract\nTitle\nMedCPT \nArticle\nEncoder\nUMAP\nB. Embedding and Dimension Reduction\nK-Means Clustering\nPrompt\nIncluded Example:\nExcluded Example:\nConfidence Score:\nC. Clustering For Dynamic Few Shot Prompting\nGPT-4.1-mini\nInclude\nExclude\nGPT-4.1\nInclude\nExclude\nLow Confidence \nInstances\nD. Re-Evaluate Low-Confidence Instances\n"}, {"page": 8, "text": " \n8 \nBuilding on this, to enhance screening efficiency while reducing computational costs, \nwe implemented a cost-effective two-stage screening strategy. In the first stage, GPT-\n4.1-mini was employed for the initial screening of all titles and abstracts, with input \ncosts of $0.40/1M tokens and output costs of $1.60/1M tokens, thereby generating \npreliminary screening results at a lower cost. Simultaneously, the model was required \nto provide a confidence score ranging from 0 to 1 for each prediction, reflecting the \ndegree of certainty in its current judgment. \n \nBased on the confidence scores generated, we set 0.9 as the threshold for identifying \nlow-confidence instances, which were then further processed in the second stage \nusing the higher-performance GPT-4.1, with input costs of $2.00/1M tokens and \noutput costs of $8.00/1M tokens. The selection of this threshold comprehensively \nconsiders the trade-off between confidence score distribution and computational \ncosts: when the confidence score threshold is set too low, only a very small number \nof instances would be sent to the second stage, making it difficult to fully leverage the \nrole of the high-performance LLM in correcting potential misclassifications; \nconversely, when the threshold is set too high, the proportion of instances requiring \nre-evaluation significantly increases, resulting in higher computational costs and \nweakening the cost advantage of the two-stage strategy. After considering the re-\nevaluation ratio, cost overhead, and overall performance under different thresholds, \nwe found that 0.9 provides a relatively stable and reasonable trade-off point between \nscreening effectiveness and resource consumption. We provided sensitivity analysis \nof different confidence score thresholds in Supplementary Information B. \n \nBy assigning the majority of screening instances to a low-cost LLM and selectively \ninvoking a high-performance LLM only for low-confidence, potentially uncertain \ninstances, this approach can effectively balance screening performance and cost-\nefficiency. \n \n3. Results \n"}, {"page": 9, "text": " \n9 \n3.1 Data characteristics \nTable 1 shows the characteristics of the ten included systematic reviews from CLEF \n19, published between 2014 and 2021, covering diagnostic, intervention, and \nprognostic stages. According to the original study, the total number of titles and \nabstracts generated from the search step was 10,320. After data curation, the actual \nnumber of titles and abstracts included was 9,515, of which 1,052 were identified as \nones should be included in their corresponding systematic reviews during the title \nand abstract screening stage. The range of included abstracts was from 119 to 3,344 \nand the average inclusion rate was 11.1%. Different numbers of clusters were \nassigned to the systematic reviews based on the number of titles and abstracts in each \nsystematic review. In the DFSL approach, one positive instance and two negative \ninstances were used for each cluster. \n \nTable 1 Number of titles, abstracts, and included studies across ten systematic reviews. \n \nStage \nRetrieved Records \nCurated Records \nClusters \nCD012233 \nDiagnosis \n472 (43) \n429 (38) \n5 \nCD012768 \n131 (45) \n119 (43) \n3 \nCD011977 \n1182 (297) \n1117 (280) \n8 \nCD012069 \n251 (42) \n247 (41) \n4 \nCD012551 \n316 (47) \n307 (46) \n4 \nCD004414 \nIntervention \n195 (49) \n192 (49) \n3 \nCD012661 \n3479 (320) \n2897 (282) \n10 \nCD011431 \n591 (68) \n546 (65) \n5 \nCD011420 \n336 (16) \n303 (16) \n4 \nCD010772 \nPrognosis \n3367 (192) \n3343 (192) \n10 \n \n3.2 Performance of the DFSL approach \nAs shown in Figure 2, our approach demonstrated strong overall performance in the \ntitle and abstract screening task, achieving significant improvements across all ten \nsystematic reviews, with an average F1 score of 0.552, clearly outperforming all \nbaseline methods—zero-shot learning (0.458), chain-of-thought (0.458), and few-\nshot learning (0.508). The advantage of DFSL lies primarily in its dynamic instance \nselection strategy. Unlike traditional methods that rely on randomly or statically \nselected instances, DFSL dynamically selects the most relevant positive and negative \n"}, {"page": 10, "text": " \n10 \ninstances for each study to be classified, and this targeted prompting helps guide the \nLLM to make more accurate predictions. Additionally, we provide more evaluation \nmetrics in Supplementary Information C, including precision, recall, and accuracy. \n \nFigure 2 F1 score performance comparison of different prompting strategies (Zero-\nShot (ZS), Chain-of-Thought (CoT), Few-Shot (FS), Dynamic Few-Shot (DFS)) across ten \nsystematic reviews. DFS achieved higher F1 scores in the title and abstract screening task \nacross all ten systematic reviews compared to Zero-Shot, Chain-of-Thought, and Few-Shot \nmethods. \n \n3.3 Performance comparison with open-weight LLMs \nIn our DFSL approach, we adopted the proprietary LLM as the default configuration \nfor the initial screening stage. To provide a more comprehensive comparative \nanalysis, we further evaluated several representative open-weight general and \nmedical-specific LLMs under the zero-shot setting, including Phi-3.5-mini [33], \nMediPhi [34], Gemma3-4B [35], Med-Gemma-4B [36], and Med-Gemma-27B [36]. As \nshown in Table 2, the evaluation results showed that Gemma3-4B and GPT-4.1-mini \ndemonstrated superior performance on certain systematic reviews, outperforming \nother LLMs. Notably, medical-specific LLMs did not exhibit any advantages over their \ncorresponding general LLMs. \n \nTable 2 F1 score comparison with open-weight LLMs under the zero-shot setting. \n0.0\n0.2\n0.4\n0.6\nCD012233\nCD012768\nCD011977\nCD012069\nCD012551\nCD004414\nCD012661\nCD011431\nCD011420\nCD010772\nF1 Score\nZS\nCoT\nFS\nDFS\n"}, {"page": 11, "text": " \n11 \n \nGPT-4.1-mini \nPhi-3.5-mini \nMediPhi \nGemma3-4b \nMedGemma-4b \nMedGemma-27b \nCD012233 \n0.3571 \n0.2396 \n0.2444 \n0.2975 \n0.2376 \n0.3529 \nCD012768 \n0.6055 \n0.5455 \n0.3492 \n0.6387 \n0.5825 \n0.6218 \nCD011977 \n0.6486 \n0.5745 \n0.2222 \n0.6972 \n0.6596 \n0.6076 \nCD012069 \n0.3662 \n0.2375 \n0.1579 \n0.2635 \n0.2670 \n0.3152 \nCD012551 \n0.4094 \n0.2907 \n0.4021 \n0.3152 \n0.3969 \n0.4267 \nCD004414 \n0.5385 \n0.3860 \n0.2632 \n0.4062 \n0.5161 \n0.6486 \nCD012661 \n0.4871 \n0.3863 \n0.1273 \n0.3546 \n0.3667 \n0.4403 \nCD011431 \n0.3045 \n0.5642 \n0.4140 \n0.6278 \n0.5520 \n0.1742 \nCD011420 \n0.5614 \n0.5581 \n0.4000 \n0.7105 \n0.6667 \n0.6462 \nCD010772 \n0.3051 \n0.5225 \n0.2182 \n0.5962 \n0.5275 \n0.2667 \n \nDifferent LLMs exhibit significant disparities in deployment methods, hardware \nrequirements, and operational efficiency, factors that hold important practical \nimplications in the actual large-scale literature screening scenario. It should be \nemphasized that the DFSL approach itself does not depend on any specific LLM; its \nframework design allows users to flexibly select and integrate different proprietary \nor open-weight LLMs into the screening stage based on actual resource conditions \nand application requirements. \n \n3.3 Effectiveness of re-evaluating low-confidence instances \nIn the DFSL approach, all titles and abstracts were first screened initially by the low-\ncost GPT-4.1-mini; for studies with confidence scores below 0.9, they proceeded to \nthe second stage for re-evaluation by the higher-performance GPT-4.1. This two-stage \nstrategy achieved stable performance improvements while controlling costs. As \nshown in Figure 3, the re-evaluation stage resulted in improvements in 9 out of 10 \nsystematic reviews, with the overall F1 score increasing from 0.552 in the initial \nscreening to 0.563. To verify the statistical significance of this improvement, we \nfurther conducted paired statistical tests: the paired t-test yielded p = 0.008, \nindicating that the improvements brought by the two-stage strategy were statistically \nsignificant. Notably, only 12.96% of the studies were identified as low-confidence and \nproceeded to the GPT-4.1 re-evaluation stage. In the re-evaluation of ten systematic \nreviews comprising over 9,500 records, the cost was only approximately $4, while \nthe majority of the remaining screening work was completed by low-cost LLMs. This \n"}, {"page": 12, "text": " \n12 \ndesign significantly reduced reliance on high-cost LLMs while maintaining high \nscreening performance, thereby achieving a better cost-performance balance. \n \n \nFigure 3 F1 score performance before and after re-evaluation of low-confidence \ninstances across ten systematic reviews. The DFSL approach initially screened all \nstudies using GPT-4.1-mini, and instances with confidence scores below 0.9 were re-\nevaluated using GPT-4.1.  \n \n4. Discussion \nThis study proposes the DFSL approach, aimed at improving LLM performance in the \ntitle and abstract screening task for systematic reviews. Through dynamic prompt \nconstruction, the DFSL approach demonstrated strong performance. Across ten \nsystematic reviews spanning diverse stages, DFSL achieved an average F1 score of \n0.552, substantially outperforming baseline prompting methods such as zero-shot, \nchain-of-thought, and few-shot. \n \nDFSL offers advantages not only in performance but also in cost-effectiveness. By \nintroducing the confidence score threshold, only predictions with the confidence \nscore below 0.9 were re-evaluated using a higher-performance LLM. In our \nexperiments, only 12.96% of instances required second-stage evaluation by GPT-4.1, \nyet the overall F1 score increased to 0.563 and passed statistical testing. This two-\nstage approach effectively balanced performance with resource usage, demonstrating \n0.3\n0.4\n0.5\n0.6\n0.7\nCD012233\nCD012768\nCD011977\nCD012069\nCD012551\nCD004414\nCD012661\nCD011431\nCD011420\nCD010772\nF1 Score\nInitial Evaluation\nRe−Evaluation\n"}, {"page": 13, "text": " \n13 \nscalability for large-scale literature screening applications. It is worth noting that, \nalthough this study employed GPT-4.1 in the second stage, the approach itself \nsupports the flexible selection of more advanced models as needed, such as \nsubstantially more expensive reasoning-enhanced LLMs. In such scenarios, compared \nwith uniformly applying high-performance LLMs to all studies, this approach is \nexpected to provide significant cost savings. \n \nIn terms of applicability, DFSL showed consistent performance across various stages, \nincluding diagnosis, intervention, and prognosis, highlighting its generalizability. \nSince the approach does not require task-specific fine-tuning, it exhibits strong \ntransferability and can be seamlessly integrated into existing systematic review \nworkflows to assist in initial screening and improve efficiency. Additionally, the DFSL \napproach holds practical significance for real-world evidence synthesis. In the context \nof rapidly growing literature and high manual screening costs, it serves as a valuable \nsolution to help researchers prioritize highly relevant studies, reduce manual \nworkload, and accelerate the systematic review process. \n \nDespite these encouraging results, several limitations remain. First, due to data access \nconstraints and computational costs, the scale of this study was relatively limited; \nevaluating the approach on larger-scale datasets would help further validate its \nrobustness. Second, although the approach improved performance, false negatives \nstill occurred. Therefore, human review remains necessary in high-stakes settings, \nand the approach should be seen as an assistive tool rather than a complete \nreplacement for expert judgment. Additionally, this study has not yet conducted \nhuman-in-the-loop evaluations, nor has it been integrated with real-world systematic \nreview platforms such as Rayyan and ASReview or undergone user-level workflow \nvalidation. Future user studies conducted in practical usage scenarios would better \ndemonstrate the potential value and feasibility of this approach in real-world \npractice. \n \n"}, {"page": 14, "text": " \n14 \nData And Code Availability  \nAll data used in this study were obtained from publicly available datasets. The code \nused in this study is available upon request. \n \nAuthor Contributions \nConceptualization, YCL, RY, and CH; Methodology, YCL, RY, and CH; Investigation, YCL, \nRY, JCKL, ZY, HF, and CH; Visualization, YCL, RY, ZY; Project administration, CH; \nSupervision, CH; Writing—Original Draft, YCL, RY, ZY; Writing—Review & Editing, RY, \nNL, CJL, and CH. \n \nCompleting Interests \nThe authors do not have conflicts of interest related to this study. This research \nreceived no specific grant from any funding agency in the public, commercial or not-\nfor-profit sectors. \n \nEthical Approval \nNot applicable. \n \nClinical Trial Number \nNot applicable. \n \nConsent to Participate \nNot applicable. \n \nFunding Declaration \nNot applicable. \n \n \n \n"}, {"page": 15, "text": " \n15 \nReferences \n1.  Sauerland S, Seiler CM. Role of systematic reviews and meta-analysis in evidence-\nbased medicine. World J Surg. 2005;29: 582–587. doi:10.1007/s00268-005-7917-7 \n2.  Yang R, Tong J, Wang H, Huang H, Hu Z, Li P, et al. Enabling inclusive systematic \nreviews: incorporating preprint articles with large language model-driven \nevaluations. J Am Med Inform Assoc. 2025;32: 1718–1725. \ndoi:10.1093/jamia/ocaf137 \n3.  Bornmann L, Haunschild R, Mutz R. Growth rates of modern science: a latent \npiecewise growth curve approach to model publication numbers from established \nand new literature databases. Humanit Soc Sci Commun. 2021;8. \ndoi:10.1057/s41599-021-00903-w \n4.  Tsertsvadze A, Chen Y-F, Moher D, Sutcliffe P, McCarthy N. How to conduct \nsystematic reviews more expeditiously? Syst Rev. 2015;4: 160. \ndoi:10.1186/s13643-015-0147-7 \n5.  Khan KS, Kunz R, Kleijnen J, Antes G. Five steps to conducting a systematic review. J \nR Soc Med. 2003;96: 118–121. doi:10.1177/014107680309600304 \n6.  Hamel C, Hersi M, Kelly SE, Tricco AC, Straus S, Wells G, et al. Guidance for using \nartificial intelligence for title and abstract screening while conducting knowledge \nsyntheses. BMC Med Res Methodol. 2021;21: 285. doi:10.1186/s12874-021-01451-\n2 \n7.  Lee GE, Sun A. Seed-driven document ranking for systematic reviews in evidence-\nbased medicine. The 41st International ACM SIGIR Conference on Research & \nDevelopment in Information Retrieval. New York, NY, USA: ACM; 2018. pp. 455–464. \ndoi:10.1145/3209978.3209994 \n8.  Chai KEK, Lines RLJ, Gucciardi DF, Ng L. Research Screener: a machine learning tool \nto semi-automate abstract screening for systematic reviews. Syst Rev. 2021;10: 93. \ndoi:10.1186/s13643-021-01635-3 \n9.  Wiwatthanasetthakarn P, Ponthongmak W, Looareesuwan P, Tansawet A, \nNumthavaj P, McKay GJ, et al. Development and Validation of a Literature Screening \n"}, {"page": 16, "text": " \n16 \nTool: Few-Shot Learning Approach in Systematic Reviews. J Med Internet Res. \n2024;26: e56863. doi:10.2196/56863 \n10.  Yang R, Tan TF, Lu W, Thirunavukarasu AJ, Ting DSW, Liu N. Large language models \nin health care: Development, applications, and challenges. Health Care Sci. 2023;2: \n255–263. doi:10.1002/hcs2.61 \n11.  Yang R, Zeng Q, You K, Qiao Y, Huang L, Hsieh C-C, et al. Ascle-A Python Natural \nLanguage Processing Toolkit for Medical Text Generation: Development and \nEvaluation Study. J Med Internet Res. 2024;26: e60601. doi:10.2196/60601 \n12.  Guo E, Gupta M, Deng J, Park Y-J, Paget M, Naugler C. Automated Paper Screening for \nClinical Reviews Using Large Language Models: Data Analysis Study. J Med Internet \nRes. 2024;26: e48996. doi:10.2196/48996 \n13.  Sanghera R, Thirunavukarasu AJ, El Khoury M, O’Logbon J, Chen Y, Watt A, et al. \nHigh-performance automated abstract screening with large language model \nensembles. J Am Med Inform Assoc. 2025;32: 893–904. doi:10.1093/jamia/ocaf050 \n14.  Wilkins D. Automated title and abstract screening for scoping reviews using the \nGPT-4 Large Language Model. arXiv [cs.CL]. 2023. doi:10.48550/ARXIV.2311.07918 \n15.  Rouzrokh P, Khosravi B, Rouzrokh P, Shariatnia M. LatteReview: A multi-agent \nframework for systematic review automation using large language models. arXiv \n[cs.CL]. 2025. doi:10.48550/ARXIV.2501.05468 \n16.  CEUR-WS.org/Vol-2380 - CLEF 2019 Working Notes. [cited 10 Dec 2025]. Available: \nhttps://ceur-ws.org/Vol-2380/. \n17.  Wennmacker SZ, Lamberts MP, Di Martino M, Drenth JP, Gurusamy KS, van \nLaarhoven CJ. Transabdominal ultrasound and endoscopic ultrasound for diagnosis \nof gallbladder polyps. Cochrane Database Syst Rev. 2018;8: CD012233. \ndoi:10.1002/14651858.CD012233.pub2 \n18.  Kohli M, Schiller I, Dendukuri N, Yao M, Dheda K, Denkinger CM, et al. Xpert \nMTB/RIF Ultra and Xpert MTB/RIF assays for extrapulmonary tuberculosis and \nrifampicin resistance in adults. Cochrane Database Syst Rev. 2021;1: CD012768. \ndoi:10.1002/14651858.CD012768.pub3 \n19.  Abba K, Kirkham AJ, Olliaro PL, Deeks JJ, Donegan S, Garner P, et al. Rapid diagnostic \ntests for diagnosing uncomplicated non-falciparum or Plasmodium vivax malaria in \n"}, {"page": 17, "text": " \n17 \nendemic countries. Cochrane Database Syst Rev. 2014;2014: CD011431. \ndoi:10.1002/14651858.CD011431 \n20.  Bjerrum S, Schiller I, Dendukuri N, Kohli M, Nathavitharana RR, Zwerling AA, et al. \nLateral flow urine lipoarabinomannan assay for detecting active tuberculosis in \npeople living with HIV. Cochrane Database Syst Rev. 2019;10: CD011420. \ndoi:10.1002/14651858.CD011420.pub3 \n21.  Burton JK, Fearon P, Noel-Storr AH, McShane R, Stott DJ, Quinn TJ. Informant \nQuestionnaire on Cognitive Decline in the Elderly (IQCODE) for the detection of \ndementia within a secondary care setting. Cochrane Database Syst Rev. 2021;7: \nCD010772. doi:10.1002/14651858.CD010772.pub3 \n22.  Downie LE, Busija L, Keller PR. Blue-light filtering intraocular lenses (IOLs) for \nprotecting macular health. Cochrane Database Syst Rev. 2018;5: CD011977. \ndoi:10.1002/14651858.CD011977.pub2 \n23.  Storebø OJ, Pedersen N, Ramstad E, Kielsholm ML, Nielsen SS, Krogh HB, et al. \nMethylphenidate for attention deficit hyperactivity disorder (ADHD) in children and \nadolescents - assessment of adverse events in non-randomised studies. Cochrane \nDatabase Syst Rev. 2018;5: CD012069. doi:10.1002/14651858.CD012069.pub2 \n24.  Franco JVA, Turk T, Jung JH, Xiao Y-T, Iakhno S, Garrote V, et al. Non-\npharmacological interventions for treating chronic prostatitis/chronic pelvic pain \nsyndrome: a Cochrane systematic review. BJU Int. 2019;124: 197–208. \ndoi:10.1111/bju.14492 \n25.  Bauer A, Rönsch H, Elsner P, Dittmar D, Bennett C, Schuttelaar M-LA, et al. \nInterventions for preventing occupational irritant hand dermatitis. Cochrane \nDatabase Syst Rev. 2018;4: CD004414. doi:10.1002/14651858.CD004414.pub3 \n26.  Richter B, Hemmingsen B, Metzendorf M-I, Takwoingi Y. Development of type 2 \ndiabetes mellitus in people with intermediate hyperglycaemia. Cochrane Database \nSyst Rev. 2018;10: CD012661. doi:10.1002/14651858.CD012661.pub2 \n27.  Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, et al. Chain-of-thought \nprompting elicits reasoning in large language models. arXiv [cs.CL]. 2022. \ndoi:10.48550/ARXIV.2201.11903 \n"}, {"page": 18, "text": " \n18 \n28.  Parnami A, Lee M. Learning from few examples: A summary of approaches to few-\nShot Learning. arXiv [cs.LG]. 2022. doi:10.48550/ARXIV.2203.04291 \n29.  Nori H, Lee YT, Zhang S, Carignan D, Edgar R, Fusi N, et al. Can generalist foundation \nmodels outcompete special-purpose tuning? Case study in medicine. arXiv [cs.CL]. \n2023. doi:10.48550/ARXIV.2311.16452 \n30.  Jin Q, Kim W, Chen Q, Comeau DC, Yeganova L, Wilbur WJ, et al. MedCPT: Contrastive \nPre-trained Transformers with large-scale PubMed search logs for zero-shot \nbiomedical information retrieval. Bioinformatics. 2023;39. \ndoi:10.1093/bioinformatics/btad651 \n31.  McInnes L, Healy J, Melville J. UMAP: Uniform Manifold Approximation and \nProjection for Dimension Reduction. arXiv [stat.ML]. 2018. \ndoi:10.48550/ARXIV.1802.03426 \n32.  Ikotun AM, Ezugwu AE, Abualigah L, Abuhaija B, Heming J. K-means clustering \nalgorithms: A comprehensive review, variants analysis, and advances in the era of \nbig data. Inf Sci (Ny). 2023;622: 178–210. doi:10.1016/j.ins.2022.11.139 \n33.  Abdin M, Aneja J, Awadalla H, Awadallah A, Awan AA, Bach N, et al. Phi-3 Technical \nReport: A Highly Capable Language Model Locally on Your Phone. 2024. Available: \nhttp://arxiv.org/abs/2404.14219 \n34.  Corbeil J-P, Dada A, Attendu J-M, Abacha AB, Sordoni A, Caccia L, et al. A Modular \nApproach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, \nModel Merging, and Clinical-Tasks Alignment. 2025 [cited 11 Dec 2025]. \ndoi:10.18653/v1/2025.acl-long.950 \n35.  Gemma Team, Kamath A, Ferret J, Pathak S, Vieillard N, Merhej R, et al. Gemma 3 \nTechnical Report. arXiv [cs.CL]. 2025. doi:10.48550/ARXIV.2503.19786 \n36.  Sellergren A, Kazemzadeh S, Jaroensri T, Kiraly A, Traverse M, Kohlberger T, et al. \nMedGemma Technical Report. arXiv [cs.AI]. 2025. doi:10.48550/ARXIV.2507.05201 \n \n \n \n \n"}, {"page": 19, "text": " \n19 \nSupplementary Information \n \nSupplementary Information A \nEvaluation Prompt \n \n \nYou are a reviewer conducting abstract screening for a systematic review. Your \ntask is to determine whether the given title and abstract should be included or \nexcluded based on the provided criteria. \n \nOnly return the result: include or exclude. \n \nCriteria: \n{criteria} \n \nTitle: \n{title} \n \nAbstract: \n{abstract} \n \nOutput: \nTable 1 Zero-Shot Evaluation Prompt. \n \n \n \nYou are a reviewer conducting abstract screening for a systematic review. Your \ntask is to determine whether the given title and abstract should be included or \nexcluded based on the provided criteria. Think step by step. \n \nOnly return the result: include or exclude. \n \nCriteria: \n{criteria} \n \nTitle: \n{title} \n \nAbstract: \n{abstract} \n \nOutput: \nTable 2 Chain of Thought Evaluation Prompt. \n \n \n"}, {"page": 20, "text": " \n20 \n \nYou are a reviewer conducting abstract screening for a systematic review. Your \ntask is to determine whether the given title and abstract should be included or \nexcluded based on the provided criteria and examples.  \n \nOnly return the result: include or exclude. \n \nCriteria: \n{criteria} \n \nInstances: \n{instances} \n \nTitle: \n{title} \n \nAbstract: \n{abstract} \n \nOutput: \nTable 3 Few-Shot Evaluation Prompt. \n \n \nYou are a reviewer conducting abstract screening for a systematic review. Your \ntask is to determine whether the given title and abstract should be included or \nexcluded based on the provided criteria and examples. Think step by step. \n \nReturn the result in JSON format. \n \nCriteria: \n{criteria} \n \nInstances: \n{instances} \n \nTitle: \n{title} \n \nAbstract: \n{abstract} \n \nOutput Format: \n{{ \n    \"con^idence\": con^idence_score (0 to 1), \n    \"decision\": \"include or exclude\" \n}} \n \nOutput: \nTable 4 Dynamic Few-Shot Evaluation Prompt. \n \n"}, {"page": 21, "text": " \n21 \nSupplementary Information B \nSensitivity Analysis of Confidence Score Thresholds \n \n \n0.7 \nRatio \n0.8 \nRatio \n0.9 \nRatio \nCD012233 \n0.4419 \n12.50% \n0.4598 \n19.23% \n0.4615 \n32.45% \nCD012768 \n0.6179 \n7.27% \n0.6230 \n10.00% \n0.6034 \n19.09% \nCD011977 \n0.6517 \n0.55% \n0.6667 \n2.19% \n0.6591 \n7.10% \nCD012069 \n0.3742 \n1.12% \n0.3760 \n3.56% \n0.3873 \n12.97% \nCD012551 \n0.4946 \n6.20% \n0.4974 \n8.83% \n0.5000 \n13.53% \nCD004414 \n0.7097 \n0.68% \n0.7097 \n1.36% \n0.7333 \n4.76% \nCD012661 \n0.6479 \n1.45% \n0.6537 \n3.05% \n0.6555 \n8.11% \nCD011431 \n0.5582 \n8.04% \n0.5714 \n11.15% \n0.5791 \n20.84% \nCD011420 \n0.6970 \n6.78% \n0.7077 \n8.47% \n0.7077 \n10.17% \nCD010772 \n0.3377 \n10.51% \n0.3333 \n13.56% \n0.3467 \n21.36% \nTable 5 F1 Score Comparison under Different Confidence Score Thresholds. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 22, "text": " \n22 \nSupplementary Information C \nPerformance Metrics Comparison of Different Prompting Strategies \n \n \nZero-Shot \nChain-of-Thought \nFew-Shot \nDynamic Few-Shot \n \nPrecision \nRecall \nAccuracy \nPrecision \nRecall \nAccuracy \nPrecision \nRecall \nAccuracy \nPrecision \nRecall \nAccuracy \nCD012233 \n0.4762 \n0.2857 \n0.9135 \n0.5000 \n0.3143 \n0.9159 \n0.4333 \n0.3714 \n0.9062 \n0.3800 \n0.5429 \n0.8870 \nCD012768 \n0.4783 \n0.8250 \n0.6091 \n0.4783 \n0.8250 \n0.6091 \n0.4545 \n0.8750 \n0.5727 \n0.4578 \n0.9500 \n0.5727 \nCD011977 \n0.8571 \n0.5217 \n0.8579 \n0.8846 \n0.5000 \n0.8579 \n0.8571 \n0.5217 \n0.8579 \n0.6744 \n0.6304 \n0.8306 \nCD012069 \n0.2410 \n0.7619 \n0.7490 \n0.2458 \n0.7436 \n0.7584 \n0.2449 \n0.7875 \n0.7486 \n0.2412 \n0.8278 \n0.7357 \nCD012551 \n0.3182 \n0.5738 \n0.8102 \n0.3303 \n0.5902 \n0.8158 \n0.3495 \n0.5902 \n0.8271 \n0.3659 \n0.7377 \n0.8233 \nCD004414 \n0.6364 \n0.4667 \n0.9592 \n0.6364 \n0.4667 \n0.9592 \n0.6471 \n0.7333 \n0.9660 \n0.6875 \n0.7333 \n0.9694 \nCD012661 \n0.7586 \n0.3587 \n0.9581 \n0.7901 \n0.3478 \n0.9587 \n0.7634 \n0.3859 \n0.9593 \n0.6746 \n0.6196 \n0.9623 \nCD011431 \n0.8226 \n0.1868 \n0.7870 \n0.8065 \n0.1832 \n0.7852 \n0.7984 \n0.3626 \n0.8181 \n0.7039 \n0.4615 \n0.8172 \nCD011420 \n0.8421 \n0.4211 \n0.8941 \n0.8333 \n0.3947 \n0.8898 \n0.8182 \n0.4737 \n0.8983 \n0.8214 \n0.6053 \n0.9153 \nCD010772 \n0.5294 \n0.2143 \n0.8610 \n0.5000 \n0.2143 \n0.8576 \n0.4762 \n0.2381 \n0.8542 \n0.3871 \n0.2857 \n0.8339 \nTable 6 Precision, Recall, and Accuracy for Zero-Shot, Chain-of-Thought, Few-Shot, \nand Dynamic Few-Shot. \n \n"}]}