{"doc_id": "arxiv:2601.09367", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.09367.pdf", "meta": {"doc_id": "arxiv:2601.09367", "source": "arxiv", "arxiv_id": "2601.09367", "title": "Relation Extraction Capabilities of LLMs on Clinical Text: A Bilingual Evaluation for English and Turkish", "authors": ["Aidana Aidynkyzy", "Oğuz Dikenelli", "Oylum Alatlı", "Şebnem Bora"], "published": "2026-01-14T10:49:46Z", "updated": "2026-01-14T10:49:46Z", "summary": "The scarcity of annotated datasets for clinical information extraction in non-English languages hinders the evaluation of large language model (LLM)-based methods developed primarily in English. In this study, we present the first comprehensive bilingual evaluation of LLMs for the clinical Relation Extraction (RE) task in both English and Turkish. To facilitate this evaluation, we introduce the first English-Turkish parallel clinical RE dataset, derived and carefully curated from the 2010 i2b2/VA relation classification corpus. We systematically assess a diverse set of prompting strategies, including multiple in-context learning (ICL) and Chain-of-Thought (CoT) approaches, and compare their performance to fine-tuned baselines such as PURE. Furthermore, we propose Relation-Aware Retrieval (RAR), a novel in-context example selection method based on contrastive learning, that is specifically designed to capture both sentence-level and relation-level semantics. Our results show that prompting-based LLM approaches consistently outperform traditional fine-tuned models. Moreover, evaluations for English performed better than their Turkish counterparts across all evaluated LLMs and prompting techniques. Among ICL methods, RAR achieves the highest performance, with Gemini 1.5 Flash reaching a micro-F1 score of 0.906 in English and 0.888 in Turkish. Performance further improves to 0.918 F1 in English when RAR is combined with a structured reasoning prompt using the DeepSeek-V3 model. These findings highlight the importance of high-quality demonstration retrieval and underscore the potential of advanced retrieval and prompting techniques to bridge resource gaps in clinical natural language processing.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.09367v1", "url_pdf": "https://arxiv.org/pdf/2601.09367.pdf", "meta_path": "data/raw/arxiv/meta/2601.09367.json", "sha256": "20f3ce26f438410615ced061889d4fe8ee562f05816c1ae4a9c34066cfcc138a", "status": "ok", "fetched_at": "2026-02-18T02:21:34.719549+00:00"}, "pages": [{"page": 1, "text": "Relation Extraction Capabilities of LLMs on Clinical Text: A\nBilingual Evaluation for English and Turkish\nAidana Aidynkyzy1, Oğuz Dikenelli2, Oylum Alatlı2, Şebnem Bora2\n1Department of Computer Engineering, Astana IT University, 010000, Astana, Kazakhstan\naidana.aidynkyzy@astanait.edu.kz\n2Department of Computer Engineering, Ege University, 35100, İzmir, Turkey\noguz.dikenelli@ege.edu.tr, oylum.alatli@ege.edu.tr, sebnem.bora@ege.edu.tr\nAbstract\nThe scarcity of annotated datasets for clinical information extraction in non-English languages\nhinders the evaluation of large language model (LLM)–based methods developed primarily in En-\nglish. In this study, we present the ﬁrst comprehensive bilingual evaluation of LLMs for the clinical\nRelation Extraction (RE)task in both English and Turkish. To facilitate this evaluation, we intro-\nduce the ﬁrst English–Turkish parallel clinical RE dataset, derived and carefully curated from the\n2010 i2b2/VA relation classiﬁcation corpus. We systematically assess a diverse set of prompting\nstrategies, including multiple in-context learning (ICL) and Chain-of-Thought (CoT) approaches,\nand compare their performance to ﬁne-tuned baselines such as PURE. Furthermore, we propose\nRelation-Aware Retrieval (RAR)—a novel in-context example selection method based on contrastive\nlearning—that is speciﬁcally designed to capture both sentence-level and relation-level semantics.\nOur results show that prompting-based LLM approaches consistently outperform traditional ﬁne-\ntuned models. Moreover, evaluations for English performed better than their Turkish counterparts\nacross all evaluated LLMs and prompting techniques. Among ICL methods, RAR achieves the\nhighest performance, with Gemini 1.5 Flash reaching a micro-F1 score of 0.906 in English and\n0.888 in Turkish. Performance further improves to 0.918 F1 in English when RAR is combined\nwith a structured reasoning prompt using the DeepSeek-V3 model. These ﬁndings highlight the im-\nportance of high-quality demonstration retrieval and underscore the potential of advanced retrieval\nand prompting techniques to bridge resource gaps in clinical natural language processing.\n1\nIntroduction\nThe unexpected and remarkable generative capabilities of Large Language Models (LLMs) have opened\nnew research avenues across various areas of natural language processing (NLP), including Clinical\nInformation Extraction (CIE). As a foundational component of Clinical Decision Support Systems\n(CDSS), CIE focuses on identifying and extracting key clinical constructs—such as entities, events,\nrelationships, and sentiments—from unstructured medical texts.\nAmong these, Relation Extraction (RE) is a critical sub-task that aims to identify and classify se-\nmantic relationships between entities within a sentence or document [Zhao et al.(2024), Landolsi et al.(2023)].\nIn the context of CIE, essential relation types include disease–disease, disease–medical examination, and\ndisease–treatment relationships, as exempliﬁed in the 2010 i2b2/VA challenge dataset [Uzuner et al.(2011)].\nAccurately identifying such relations not only enhances information extraction systems but also\nlays the groundwork for constructing medical knowledge graphs.\nThese structured representations\nare indispensable for downstream clinical applications such as ICD code prediction, treatment rec-\nommendation, and for boosting the performance of clinical text summarisation and generation tasks\n[Cui et al.(2023)].\nDespite its critical role in clinical information extraction (CIE), there remains a signiﬁcant scarcity\nof publicly available, labelled Relation Extraction (RE) datasets in languages other than English. While\nrecent initiatives have introduced open RE datasets for several high-resource languages [Guan et al.(2020)],\nsuch resources remain extremely limited for low- and middle-resource languages. This lack of anno-\ntated data presents a major obstacle to the development of deep learning and Large Language Model\n(LLM)-based approaches in underrepresented linguistic contexts.\n1\narXiv:2601.09367v1  [cs.CL]  14 Jan 2026\n"}, {"page": 2, "text": "Beyond the scarcity of labelled datasets, the absence of parallel bilingual datasets further com-\npounds the challenge. These resources are essential for evaluating cross-lingual performance and en-\nabling direct comparisons with English benchmarks. Parallel bilingual datasets allow researchers to\nexplore model adaptability across languages, a critical capability for developing globally robust clinical\nAI systems.\nIn this work, we evaluate the performance of the RE task in a bilingual setting, speciﬁcally compar-\ning English and Turkish. To the best of our knowledge, this is the ﬁrst study to assess RE performance\non Turkish clinical text.\nTurkish poses unique challenges as a low-resource language in the medi-\ncal domain.\nCurrently, no publicly available clinical RE dataset exists in Turkish.\nMoreover, the\nonly dedicated Turkish biomedical language model—BioBERTurk [Türkmen et al.()]—was pretrained\non a substantially smaller corpus ( 6 billion words) compared to its English counterpart BioBERT\n[Lee et al.(2019)], which was trained on approximately 18 billion words. This data disparity further\nhighlights the resource gap and motivates the need for bilingual evaluation in clinical NLP.\nOur evaluation is conducted using the ﬁrst bilingual clinical dataset for RE in Turkish and English,\nderived from the 2010 i2b2/VA challenge dataset [Uzuner et al.(2011)].\nThe English dataset was\ntranslated into Turkish through a collaborative eﬀort involving computational linguists and medical\nexperts from Ege University and Dokuz Eylül University.\nWe selected the 2010 i2b2/VA dataset\nintentionally due to its basis in authentic clinical narratives, ensuring that the medical content remains\nsemantically aligned across both language versions.\nThe importance of this dataset is further reinforced by the ﬁndings [Fraile Navarro et al.(2023)],\nwhose systematic review of 94 studies (with over 500 citations on Google Scholar) reports that the\n2010 i2b2/VA dataset is the most frequently used corpus in clinical RE and Named Entity Recognition\n(NER) research between 2010 and 2022, being employed in 59% of the surveyed works.\nFor this study, we utilised a carefully curated subset of the translated dataset containing 1500\ntraining and 500 test samples. This subset underwent an additional layer of validation, speciﬁcally\nfocusing on the correctness of label alignments between the English and Turkish versions to ensure\nconsistency and reliability for bilingual evaluation. A brief description of the translation methodology\nand the dataset statistics is provided in Section 3.\nWe utilise this bilingual dataset to compare the eﬀectiveness of prompting strategies for Large\nLanguage Models (LLMs) against ﬁne-tuned small language models (SLMs) in the Relation Extraction\n(RE) task. From the perspective of prompting strategies, our focus is on two core components: in-\ncontext example selection methods [Jimenez Gutierrez et al.(2022), Liu et al.(2022a)] and Chain-of-\nThought (CoT) reasoning techniques [Wei et al.(2023)].\nTo evaluate prompting strategies, we implemented a range of established in-context selection and\nchain-of-thought (CoT) prompting methods from the literature, carefully adapting each to the bilin-\ngual relation extraction (RE) setting. To ensure a balanced and practical evaluation of prompting\nstrategies, we selected a representative set of Large Language Models (LLMs), including Gemini Flash\n(versions 1.5 and 2.0), DeepSeek V3, and GPT-4o mini.\nThe selection was guided by three criti-\ncal criteria: response latency, cost eﬃciency, and general language understanding performance, as\nmeasured by the MMLU benchmark. Information extraction systems typically process large volumes\nof data, requiring models that are both fast and cost-eﬃcient while maintaining robust language\ncomprehension capabilities.\nOur goal was to balance these parameters to ensure fair and realis-\ntic comparisons between models under practical deployment constraints. Further details regarding\nthe model selection rationale and experimental conﬁguration are provided in the Experiments sec-\ntion. Despite limited API access to the Gemini platform, their high eﬃciency in structured extrac-\ntion made them suitable candidates within our resource constraints. DeepSeek v3, an open-source\nmodel, was included due to its top-ranking score on the MMLU benchmark among open-source models\n[Open Benchmarks(2023), DeepSeek-AI, Liu et al.(2025)]. For comparison, we also evaluated GPT-4o-\nmini—one of the most advanced compact language models—due to its strong benchmark results and\ncost eﬃciency. GPT 4o-mini achieves high accuracy on complex reasoning tasks [Sinha et al.(2025)],\nincluding an 82% score on MMLU [OpenAI(2024)]. Overall, our model selection reﬂects a cost-sensitive\nstrategy, balancing API access limitations with high performance and low operational costs.\nBeyond adopting prompting strategies from existing work, we propose a novel in-context example\n2\n"}, {"page": 3, "text": "selection method, designed speciﬁcally for RE tasks. Our method builds on contrastive learning and is\ngrounded in the SimCSE framework [Gao, Yao, and Chen(2021)]. We train SimCSE from scratch for\nboth English and Turkish, and crucially, we enhance input representations by incorporating entity and\nrelation label information into the training examples. This design enables the model to better encode\nrelation-speciﬁc semantics. We refer to this method as Relation-Aware Retrieval (RAR). Experimental\nresults demonstrate that RAR consistently outperforms existing in-context selection strategies across\nboth languages and all evaluated LLMs.\nIn parallel, we evaluate the performance of a small, open-source language model (SLM)—speciﬁcally,\nBERT—on both Turkish and English to serve as a ﬁne-tuning baseline against prompting-based LLMs.\nIn addition to the base BERT model, we incorporate PURE (Zhong and Chen, 2021), a lightweight\nneural architecture that has demonstrated near state-of-the-art performance in relation extraction (RE).\nOur experiments on the 2010 i2b2/VA clinical dataset reveal that LLMs substantially outperform ﬁne-\ntuning-based approaches.\nAlthough the relatively limited size of our bilingual training data poses\na challenge for ﬁne-tuning methods, LLMs still achieve performance levels close to the state of the\nart across both languages, underscoring their robustness and generalisation capacity in low-resource\nbilingual settings.\nOur key contributions are summarised below:\n• First Evaluation of RE for Turkish Clinical Text: We conduct, to the best of our knowledge, the\nﬁrst evaluation of the Relation Extraction (RE) task on Turkish clinical texts, using a curated\nsubset of the 2010 i2b2/VA RE dataset translated into Turkish. We systematically evaluate both\nprompting-based LLM approaches and ﬁne-tuned small language models (SLMs) on this dataset,\naddressing a critical gap for this low-resource language in the clinical NLP domain.\n• Comprehensive Bilingual Evaluation of RE: We perform an extensive bilingual evaluation of the\nRE task across both English and Turkish clinical text, comparing ﬁne-tuning approaches for\nSLMs with multiple prompting strategies applied to several state-of-the-art LLMs. This study\noﬀers important insights into cross-lingual generalisation, highlights the challenges posed by low-\nresource clinical languages.\n• Introduction of Relation-Aware Retrieval (RAR): We propose a novel in-context example selection\nmethod, Relation-Aware Retrieval (RAR), speciﬁcally designed for the RE task. Our experiments\ndemonstrate that RAR consistently outperforms existing in-context selection methods across\nboth languages and all evaluation settings.\n2\nLiterature Review\nSeveral recent comprehensive literature surveys on Relation Extraction (RE) have been published\n[Zhao et al.(2024), Detroja, Bhensdadia, and Bhatt(2023)], while [Fraile Navarro et al.(2023)] have con-\nducted an extensive review speciﬁcally focusing on NER and RE studies in clinical text. However, these\nsurveys do not cover recent prompting-based approaches that directly target the RE task. Therefore,\nour literature review contributes by introducing and analysing eﬀective prompting strategies for RE,\nﬁlling this emerging gap in the literature.\nFrom a prompt engineering perspective, two techniques have been particularly eﬀective for con-\nstructing prompts across various tasks: in-context learning and Chain-of-Thought (CoT) prompting.\nIn this work, we review these techniques with a speciﬁc focus on the Relation Extraction (RE) task,\nexamining how they are implemented for RE and assessing their eﬀectiveness in Large Language Model\n(LLM) prompting.\n2.1\nIn-Context Learning Approaches\nIn-context learning was ﬁrst introduced by [Brown et al.(2020)] in the paper that introduced the GPT-\n3 language model. They demonstrated that GPT-3 can perform tasks eﬀectively when provided with\n3\n"}, {"page": 4, "text": "a few examples embedded directly within the prompt, without requiring any ﬁne-tuning. For their ex-\nperiments, the examples used in the prompts were randomly sampled from the training set to construct\nthe few-shot learning scenarios.\nHowever, [Liu et al.(2022a)] observed that GPT-3’s performance is highly sensitive to the selection\nof in-context examples, with random selection often leading to inconsistent results. To address this\nissue, they proposed an in-context example retrieval method called KATE (Knn-Augmented in-context\nExample selection), which selects semantically similar in-context examples for each test query. This\nmethod involves encoding both the training and test sets using a sentence encoder, such as RoBERTa\n[Liu et al.(2019)], and retrieving the nearest k neighbours from the training set based on their semantic\nsimilarity to the test source. They evaluated KATE on several information extraction tasks, including\nSentiment Analysis and Question Answering, and found that KATE signiﬁcantly outperformed random\nselection across all tasks. Moreover, they observed that ﬁne-tuning RoBERTa on task-speciﬁc datasets,\nsuch as the Semantic Textual Similarity (STS) dataset, further enhanced the performance of KATE.\nThis ﬁnding opened a new avenue of research focused on developing task-speciﬁc retrievers tailored to\nindividual tasks.\n[Jimenez Gutierrez et al.(2022)] conducted the ﬁrst systematic and comprehensive study on in-\ncontext learning using biomedical Named Entity Recognition (NER) and Relation Extraction (RE)\ntasks. They selected 100 examples for each task across ﬁve NER datasets and three RE datasets. The\nstudy compared the performance of BERT-sized language models ﬁne-tuned on these examples with\nthe in-context performance of GPT-3, using KATE as the retriever and RoBERTa as the sentence\nencoder. They observed that ﬁne-tuned models consistently outperform GPT-3 in-context learning\nacross all datasets. Further experiments with increased training data sizes (250 and 500 examples)\nshowed a steady improvement in ﬁne-tuned models, while GPT-3’s performance remained stagnant,\nparticularly struggling with Relation Extraction tasks. These results highlight the limitations of KATE-\nlike approaches for in-context learning for RE, and raise important research questions regarding the\nimpact of task-speciﬁc retrievers and the potential of more advanced language models beyond GPT-3.\nRecent advancements in in-context learning for Relation Extraction (RE) have underscored the\ncritical role of task-speciﬁc demonstration retrieval. A notable contribution in this area is GPT-RE,\na framework proposed by [Wan et al.(2023)] that moves beyond generic semantic similarity. Rather\nthan using general-purpose retrievers, GPT-RE employs the PURE algorithm [Zhong and Chen(2021)]\nas a specialised retriever to select high-quality demonstrations. Within this framework, a relation is\nrepresented by concatenating the start-token embeddings of its subject and object entities.\nThis\ntask-aligned representation is then used to compute the similarity between a given test query and\nthe available training examples, ensuring that the selected demonstrations are highly relevant to the\nspeciﬁc relational structure of the query.\nThe eﬃcacy of this PURE-based retrieval strategy was validated on several standard RE bench-\nmarks using the GPT-3 model. The results were compelling: GPT-3, when equipped with PURE-based\ndemonstration selection, outperformed a ﬁne-tuned PURE baseline on three separate datasets. Further-\nmore, this approach established new state-of-the-art performance on the SemEval [Hendrickx et al.(2010)]\nand SciERC [Luan et al.(2018)] datasets. These ﬁndings demonstrate that by tailoring the retrieval\nmechanism to the unique structural properties of the RE task, a large language model can surpass\nthe performance of a specialist, ﬁne-tuned model. The success of GPT-RE, therefore, highlights that\nsophisticated, task-aware retrieval is a key component in optimising in-context learning for complex\nNLP tasks.\nAs an alternative to retriever-based methods, contrastive learning has emerged as a powerful tech-\nnique for learning high-quality sentence embeddings suitable for similarity-based retrieval [Hadsell et al.(2006)].\nThis approach trains an encoder to map semantically similar sentences to nearby points in an embed-\nding space while pushing dissimilar sentences apart, thereby producing highly discriminative represen-\ntations.\nThe GPT-RE framework was also pioneering in its application of contrastive learning to the RE\ndemonstration selection task, speciﬁcally leveraging the SimCSE framework [Gao, Yao, and Chen(2021)].\nSimCSE ﬁne-tunes Transformer-based encoders, such as BERT or RoBERTa, on a contrastive objec-\ntive using in-batch negatives.\nTo construct positive and hard-negative pairs, it typically relies on\n4\n"}, {"page": 5, "text": "large-scale Natural Language Inference (NLI) datasets like SNLI [Bowman et al.(2015)] and MNLI\n[Williams et al.(2018)].\nA key innovation in the GPT-RE study was its method of adapting contrastive learning speciﬁcally\nfor RE. Instead of encoding raw sentences, the authors reformulated the input to foreground the\nrelational context explicitly. For example, the sentence \"He has a sister Lisa\" was transformed into\n\"The relation between ‘He’ and ‘Lisa’ in the context: He has a sister Lisa.”.\nThis reformulation\ncompels the encoder to generate embeddings that are more sensitive to the roles of the entities within\nthe relational structure.\nHowever, despite the novelty of this relation-aware reformulation, the authors reported that the\nperformance of the SimCSE-based retriever ultimately did not surpass that of the PURE-based method.\nWe contend that a potential limitation of this implementation lies in its reliance on general-domain\nNLI datasets for ﬁne-tuning. Such an approach, while eﬀective for general sentence understanding,\ndoes not constitute a truly task-speciﬁc application of contrastive learning for the nuances of Relation\nExtraction.\nA more direct and potentially more eﬀective strategy would involve ﬁne-tuning a contrastive model\nlike SimCSE from scratch using data intrinsic to the target RE task. The viability of such a task-speciﬁc\ncontrastive learning approach is supported by recent work in other domains. For instance, Yang et\nal. [Yang et al.(2024)], in their work on multimodal entity-based sentiment analysis, trained SimCSE\nusing task-speciﬁc data and labels. Their experiments conﬁrmed that this method improved in-context\nexample selection performance compared to other retrieval algorithms like KATE, demonstrating that\nembeddings tailored with in-domain data yield superior results.\nThis paper adopts a similar approach and proposes two relation-aware SimCSE variants trained\non the 2010 i2b2/VA English and Turkish datasets. We explore relation-speciﬁc components, such as\nentity and relation labels, to construct the most eﬀective sentence embeddings for SimCSE training.\nWe compare these retrievers with KATE and PURE-based retrieval methods for in-context example\nselection. Our SimCSE-based retriever consistently outperforms both the PURE retriever and KATE\nacross all experiments in both languages.\n2.2\nChain of Thought Approaches\nOne of the most inﬂuential prompt engineering approaches is Chain-of-Thought (CoT) prompting,\nﬁrst introduced by [Wei et al.(2023)]. CoT prompting enhances language models by incorporating in-\ntermediate reasoning steps within the prompt, enabling them to produce more structured and logically\ncoherent outputs. In their paper, the authors apply CoT by manually constructing few-shot exam-\nples and evaluating its performance on arithmetic, commonsense, and symbolic reasoning benchmarks.\nTheir ﬁndings indicate that CoT is particularly eﬀective in larger language models and demonstrates\nsigniﬁcant improvements, especially in complex reasoning tasks.\nDespite its success in various reasoning tasks, there is no standardised method for applying Chain-\nof-Thought (CoT) prompting to the Relation Extraction (RE) task.\nA natural starting point is\nto design a static zero-shot (without in-context examples) CoT-style prompt tailored to the spe-\nciﬁc RE task, incorporating relevant knowledge structures to guide the model’s reasoning process.\n[Wang et al.(2023b)] explored such a static CoT prompt for the 2010 i2B2/VA and SemEval 2013-DDI\n[Segura-Bedmar et al.(2013)] RE datasets. In their design, each prompt begins by presenting the test\nsentence, after which the model is guided through a sequence of explicit reasoning steps. First, the\nmodel identiﬁes the relationship between the two target entities (concept 1 and concept 2) and deter-\nmines whether the sentence discusses treatment, test, or a medical problem. Subsequently, the model\nclassiﬁes the speciﬁc relation type between the entities according to a structured schema.\nThey evaluated zero-shot CoT-style prompting across three large language models (LLMs): BART,\nGPT-3.5, and GPT-4. The experimental results indicate that zero-shot CoT prompting does not yield\nimprovements over standard prompting on the 2010 i2B2/VA dataset. However, in the DDI datasets,\nCoT-style prompting demonstrates marginal improvements, suggesting that its eﬀectiveness may vary\ndepending on the dataset and task-speciﬁc characteristics.\nA more advanced approach to CoT prompting involves integrating in-context examples with the\nCoT framework. The simplest implementation of this approach uses static in-context CoT style exam-\n5\n"}, {"page": 6, "text": "ples combined with a CoT prompt. [Wang et al.(2023b)] explored this approach by randomly selecting\nﬁve examples (5-shot CoT). Their experimental results indicate that 5-shot CoT signiﬁcantly outper-\nforms zero-shot CoT across both datasets. Speciﬁcally, in the 2010 i2B2/VA dataset, performance\nimproved from 0.66 to 0.90 in BART, 0.68 to 0.84 in GPT-3.5, and 0.88 to 0.92 in GPT-4. A similar\nimprovement was observed in the DDI dataset [Wang et al.(2023b)], highlighting the eﬀectiveness of\nincorporating in-context examples in CoT prompting for Relation Extraction (RE) tasks.\nOur review of in-context example selection methods indicates that dynamically retrieving seman-\ntically similar examples during prompt construction signiﬁcantly improves model performance. This\nobservation highlights the strong potential of combining Chain-of-Thought (CoT) prompting with in-\ncontext selection strategies. To eﬀectively integrate these two approaches, CoT reasoning must be\ngenerated at the time of example selection, ensuring that each retrieved example is not only con-\ntextually relevant but also enriched with explanatory reasoning. A particularly eﬀective strategy for\ndynamically generating CoT knowledge for each selected example is the use of self-prompting, which\nallows the model to elicit structured reasoning on the ﬂy.\nSelf-prompting in large language models (LLMs) refers to the process of using the model itself\nto generate auxiliary prompts or explanations that enhance reasoning and task performance. This\ntechnique has demonstrated notable success in zero-shot settings, where it aids in constructing infor-\nmative in-context examples for prompting [Wan et al.(2023), Liu et al.(2024)]. However, in scenarios\nwhere suﬃcient training examples are available, the utility of self-prompting shifts from generating\nexamples to producing Chain-of-Thought (CoT) reasoning for each selected example.\nIn this con-\ntext, self-prompting serves as a powerful mechanism for dynamically enriching demonstrations with\nstructured reasoning, thereby improving relational understanding and extraction.\nGPT-RE [Wan et al.(2023)] utilised self-prompting to generate Chain-of-Thought (CoT) knowledge\nfor dynamically selected in-context examples by formulating task-speciﬁc questions tailored to the rela-\ntion extraction (RE) task. For instance, it posed queries such as: “What are the clues that lead to the\nrelation between [entity1] and [entity2] to be [relation] in the sentence [context]?” [Wan et al.(2023)].\nThe model-generated clues were then incorporated into each in-context example, eﬀectively enriching\nthe prompt to enhance reasoning and improve RE performance.\nThis self-prompting technique is\nreferred to as Gold Label-Induced Reasoning in the original paper, and it yielded measurable improve-\nments over standard in-context selection. Speciﬁcally, it increased the F1 score from 91.11 to 91.82 on\nthe SemEval dataset and from 70.38 to 70.97 on TACRED. In this study, we adopt Gold Label-Induced\nReasoning as one of the CoT prompting strategies for evaluating LLM performance on the RE task. An-\nother self-prompting approach is Self-Questioning Prompting (SQP), proposed by [Wang et al.(2023b)].\nIn SQP, the model is ﬁrst prompted to generate questions that target key information within the test\nexample. It is then guided to answer these questions, eﬀectively engaging in intermediate reasoning.\nFinally, the model uses the resulting question-answer pairs as auxiliary knowledge to generate the ﬁnal\noutput. This multi-step process represents an alternative form of Chain-of-Thought (CoT) prompting,\nallowing the model to decompose complex reasoning tasks and produce more informed and structured\npredictions.\nThe performance of Self-Questioning Prompting (SQP) was evaluated against a CoT\nprompting baseline under both zero-shot and 5-shot settings, using randomly selected static examples\nfrom the i2b2 and DDI datasets. Experimental results demonstrate that SQP consistently outperforms\nstandard CoT prompting across both settings and all evaluated LLMs, including BART, GPT-3.5, and\nGPT-4. These ﬁndings highlight SQP’s eﬀectiveness in enhancing reasoning and relation extraction\ncapabilities. Furthermore, the 5-shot setting yielded better performance than the zero-shot conﬁgura-\ntion for both CoT and SQP prompting strategies, underscoring the beneﬁts of incorporating few-shot\nexamples during inference.\nIn this paper, we include Self-Questioning Prompting (SQP) in our evaluation; however, unlike prior\nwork that relied solely on randomly selected static few-shot examples, we also apply SQP in conjunction\nwith in-context example selection methods. Speciﬁcally, we dynamically generate SQP-based reasoning\non the ﬂy for each test instance, enabling us to assess the impact of augmenting in-context examples\nwith SQP-derived reasoning.\nThis dynamic integration allows for more contextually relevant and\ninformative prompts, providing a deeper understanding of SQP’s eﬀectiveness when combined with\nadaptive example selection strategies in the RE task.\n6\n"}, {"page": 7, "text": "The next method included in our evaluation is inspired by recent research on applying Chain-of-\nThought (CoT) prompting to the Named Entity Recognition (NER) task. NER involves identifying\nand classifying spans of text into predeﬁned entity categories, such as medical problem, treatment, and\ntest, as annotated in the 2010 i2B2/VA dataset. The method we adopt is drawn from the PromptNER\nframework [Ashok et al.(2023)], which introduces a concise and interpretable CoT-style output format\nfor NER. In this approach, each candidate entity span is followed by a truth value (True or False) and\na brief, context-grounded justiﬁcation based on entity deﬁnitions. For example, given the sentence\n“He attended the U.S Air Force Institute of Technology,” the model produces the following structured\noutput:\nU.S. Air Force Institute of Technology | True | as he attended this institute, it is likely a university.\nThis format constitutes a compact CoT-style representation that explicitly combines the entity mention,\na binary classiﬁcation, and a minimal reasoning explanation. It eﬀectively captures the decision-making\nprocess in a transparent and interpretable way.\nIn this paper, we adapt this output format to the Relation Extraction (RE) task for the ﬁrst\ntime and refer to it as Output Format-Based CoT. This adaptation allows us to explore whether such\nstructured reasoning templates can similarly enhance interpretability and performance in RE scenarios.\n3\nBilingual Dataset\nWe use the i2b2-2010/VA dataset [Uzuner et al.(2011)] to evaluate the Relation Extraction (RE) task\nin a bilingual clinical context.\nThe dataset consists of discharge summaries and progress reports\ncontributed by two partnered medical institutions: Partners Healthcare, Beth Israel Deaconess Medical\nCenter, and the University of Pittsburgh Medical Center\nThe corpus includes a total of 394 training reports and 477 test reports, all of which were manually\nannotated for three clinical NLP subtasks: concept extraction, assertion classiﬁcation, and relation\nclassiﬁcation. In this study, we focus exclusively on the relation classiﬁcation task, which involves\nidentifying and labelling semantic relationships between annotated clinical concepts such as medical\nproblems, tests, and treatments. The corpus contains a total of 14,332 annotated relations, with a\nsigniﬁcant majority located in the test set. Speciﬁcally, there are 5,262 relations in the training data\nand 9,070 relations in the test data [Patel et al.(2021)].\nThe Turkish translation of the dataset was carried out by our research group in collaboration with\nmedical experts from Ege University Faculty of Medicine. While a full description of the translation\nmethodology is beyond the scope of this paper and will be addressed in a separate publication, we\nbrieﬂy summarise the multi-phase process here:\n1. Initial Review: A team of third- and fourth-year medical students reviewed and corrected the\nmedical terminology in the initial machine-translated output (generated using DeepL), under the\nguidance of a medical expert.\n2. Expert Validation: 2000 sentences containing relations were selected to build the dataset. 100\nof these were reviewed by two senior medical experts from Ege University and Dokuz Eylul\nUniversity. After the review, the experts validated these sentences for medical accuracy and\ndocumented common error types encountered during translation.\n3. Final Post-Editing: A professional translator, who is also a retired medical doctor, conducted a\ncomprehensive post-editing phase for a subset of the dataset to create a golden set for evalua-\ntions. This ﬁnal step focused on ensuring terminological precision and correcting the error types\nidentiﬁed in the previous phase.\nThis translation eﬀort resulted in a high-quality Turkish subset of the i2b2-2010/VA dataset, en-\nabling robust evaluation of Relation Extraction (RE) methods in a bilingual setting.\nThe relation extraction task involves assigning semantic relation types between key clinical concepts:\nmedical problems, tests, and treatments. Speciﬁcally, the dataset deﬁnes relations across three major\nconcept pairings: medical problems and treatments, medical problems and tests, and medical problems\nand other medical problems.\n7\n"}, {"page": 8, "text": "The annotated relation types for each concept pair are outlined below:\n1. Medical problems and treatment relations:\n(a) Treatment improves medical problem (TrIP)\n(b) Treatment worsens medical problem (TrWP).\n(c) Treatment causes medical problem (TrCP).\n(d) Treatment is administered for medical problem (TrAP).\n(e) Treatment is not administered because of medical problem (TrNAP).\n2. Test relations and medical problems:\n(a) Test reveals medical problem (TeRP)\n(b) Test conducted to investigate medical problem (TeCP).\n3. Medical problem and other medical problems:\n(a) Medical problem indicates medical problem (PIP)\nTo facilitate our bilingual evaluation, we selected a subset of the i2b2-2010/VA dataset consisting\nof 1,500 relation-labeled sentences from the training set and 500 sentences from the test set. For the\nTurkish version, all concept labels were manually assigned to the selected sentences to ensure align-\nment with the English data. The sample selection process was designed to maintain the proportional\nrepresentation of relation types found in the original dataset.\n4\nMethodology\nThis section outlines the methodologies employed to evaluate the Clinical Relation Extraction (RE)\ntask in a bilingual context. Our evaluation speciﬁcally focuses on two complementary strategies: in-\ncontext example selection and Chain-of-Thought (CoT) prompting, both of which are adapted to suit\nthe characteristics of the RE task. Furthermore, we investigate the synergistic integration of these\ntechniques to assess their combined impact on bilingual RE performance.\nDrawing on insights from recent literature, we adopt the PURE-based retrieval method from GPT-\nRE [Wan et al.(2023)] as our ﬁrst in-context selection method.\nThis method is called Fine-tuned\nRelation Representation in the paper, and it is the ﬁrst to employ a relation extraction (RE)-speciﬁc\nretrieval algorithm for selecting in-context examples, achieving state-of-the-art performance across\nmultiple RE benchmarks. In addition to this, we incorporate our proposed method—Relation-Aware\nRetrieval (RAR)—which, to the best of our knowledge, is the ﬁrst to integrate contrastive learning into\nthe example retrieval process for RE. Finally, we include KATE [Liu et al.(2022a)] to the evaluation\nsince KATE is notable for being the ﬁrst to demonstrate the eﬀectiveness of selecting semantically\nsimilar examples for in-context learning in information extraction (IE) tasks.\nAs part of our evaluation of Chain-of-Thought (CoT) methods, we incorporate two self-prompting\nstrategies: Gold Label-Induced Reasoning from the GPT-RE study [Wang et al.(2023b)] and Self-\nQuestioning Prompting (SQP) proposed by [Wang et al.(2023a)].\nBoth methods employ distinct\nprompt styles to elicit CoT knowledge tailored to the Relation Extraction (RE) task. Our evalua-\ntion compares these CoT prompting styles within a bilingual context to assess their eﬀectiveness. In\naddition, we include the Output Format-Based CoT method, which we adapt for the RE task. Origi-\nnally proposed for Named Entity Recognition (NER), this method demonstrated strong interpretability\nand performance, making it a promising candidate for our bilingual RE evaluation framework.\nWe begin this section with a formal deﬁnition of the Relation Extraction (RE) task to establish the\nfoundational objective of our study. We then introduce the methods employed for In-Context Example\nSelection, which determine the quality and relevance of demonstrations provided to the model. This is\nfollowed by a detailed description of the prompt structure used to guide a Large Language Model (LLM)\nunder both In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting settings, highlighting\nhow these components work together to enhance relational inference and reasoning.\n8\n"}, {"page": 9, "text": "4.1\nTask Description\nRelation Extraction (RE) is a core subtask in Information Extraction (IE) that aims to identify and\ncategorise semantic relationships between entities mentioned in unstructured text. In this study, we\nformulate RE as a classiﬁcation task to determine the correct relation type between two speciﬁed\nentities within a given sentence.\nFormally, let s be an input sentence containing two entities: e1 (subject entity) and e2 (object\nentity). The task is to classify the relationship between e1 and e2 into one of the predeﬁned relation\ntypes in the label set R.\nWe deﬁne the RE task as a function:\nf : (s, e1, e2) →r\n(1)\nwhere:\n• s →The input sentence.\n• e1, e2 →The subject and object entities in the sentence.\n• r ∈R →The predicted relation type from a set of predeﬁned relations R.\n4.2\nIn-Context Demonstration Selection Methods\nThis section presents the in-context demonstration selection methods used in our evaluation. These\nmethods rely on pretrained language models such as BERT [Devlin et al.(2019a)] or RoBERTa [Liu et al.(2019)]\nto obtain sentence embeddings, which are then used to compute the similarity between sentences. Given\nthat our evaluation is conducted in a bilingual setting, we utilise language-speciﬁc sentence embedders:\nthe English version of RoBERTa for English texts, and a Turkish RoBERTa model for Turkish inputs.\nThis ensures that the similarity computation remains robust and semantically meaningful across both\nlanguages.\n4.2.1\nKNN-Augmented in-Context Example Selection (KATE)\nKATE [Liu et al.(2022a)] uses a K-Nearest Neighbours (KNN) retrieval approach to identify semanti-\ncally similar examples from a training dataset. The retrieval process follows these steps:\n1. Sentence Embedding Generation:\nEach sentence strain from the training dataset is converted into an embedding htrain using a\npre-trained RoBERTa model.\nSimilarly, the test sentence stest is embedded as htest.\n2. Similarity Computation:\nCosine similarity is applied between Etest and all Etrain embeddings in the training set. The\nk-most similar samples are selected:\nsim(stest, straini) =\nhtest · htraini\n∥htest∥· ∥htraini∥\n(2)\n3. Example Selection:\nThe top-k similar sentences are retrieved to be used as in-context examples.\n9\n"}, {"page": 10, "text": "4.2.2\nFine-tuned (FT) Relation Representation\nThe Fine-Tuned Relation Representation method was ﬁrst introduced in the GPT-RE study by [Wan et al.(2023)].\nThis approach innovatively leverages the PURE algorithm [Zhong and Chen(2021)] to compute sen-\ntence embeddings tailored for the Relation Extraction (RE) task. Speciﬁcally, PURE constructs rela-\ntion representations by concatenating the embeddings of the subject and object entities, rather than\nrepresenting the entire sentence holistically.\nThis method stands in contrast to general-purpose retrievers like KATE, which compute similarity\nbased solely on overall sentence embeddings. By incorporating entity-aware embedding representations,\nFine-Tuned Relation Representation ensures that the selected demonstrations are not only linguistically\nsimilar to the test input but also semantically aligned at the entity-pair level.\nThis distinction is\nespecially important in RE tasks, where the primary focus is on accurately modelling the relationship\nbetween speciﬁc entities.\nThe execution ﬂow of the algorithm is outlined below.\n1. Embedding Generation with PURE\nTo generate entity-aware sentence representations, the PURE model is used. Each input sen-\ntence s with entities e1 and e2 is ﬁrst tokenised and then passed through PURE to generate an\nembedding:\nhs = PURE(s, e1, e2)\n(3)\nwhere:\n• hs is the ﬁnal entity-aware sentence embedding.\n• s is the sentence.\n• e1, e2 are the entity mentions within the sentence.\nPURE enhances these embeddings by marking entity spans with special tokens, ensuring that\nthe model pays explicit attention to entity-related information.\n2. Similarity Computation for Demonstration Selection\nOnce we obtain the entity-aware embeddings, we perform cosine similarity to select relevant\ndemonstrations from the training set. Given a test sentence stest and a set of candidate training\nsentences strain = {s1, s2, . . . , sn}, we compute the similarity score for each pair:\nsim(stest, si) =\nhstest · hsi\n∥hstest∥· ∥hsi∥\n(4)\nwhere hstest and hsi are the entity-aware embeddings of the test and training sentences, respec-\ntively. We then rank the candidate sentences based on similarity and select the top k sentences\nas demonstrations:\nD = {s1, s2, . . . , sk}\n(5)\nwhere k is a hyperparameter.\n4.2.3\nRelation-Aware Demonstration Retrieval\nRelation-Aware Demonstration Retrieval (RAR) is our proposed method for selecting highly rele-\nvant demonstrations for In-Context Learning in Relation Extraction tasks.\nRAR utilises SimCSE\n[Gao, Yao, and Chen(2021)], a state-of-the-art method for generating sentence embeddings via con-\ntrastive learning, to compute semantic similarity between the test input and candidate examples.\nSimCSE is particularly well-suited for this task due to its capacity to distinguish between seman-\ntically similar and dissimilar sentences, thereby enhancing the quality of retrieved demonstrations in\nICL frameworks. Unlike standard retrieval approaches that rely solely on surface-level or sentence-level\nsemantics, RAR is designed to capture both sentence-level and relation-level semantics, ensuring that\n10\n"}, {"page": 11, "text": "retrieved examples are relevant not just linguistically but also with respect to entity interactions and\nrelational context.\nTo support multilingual evaluation, we ﬁne-tuned SimCSE from scratch on the English and Turk-\nish versions of the 2010 i2B2/VA dataset. The training objective was designed to jointly align the\nmodel on both sentence similarity and relation-aware semantics, allowing the retriever to prioritise\ndemonstrations that share similar relational structures with the target input.\nThe operational details of the RAR algorithm are outlined below.\n1. Creating a Contrastive Learning Dataset\nTo train a task-speciﬁc SimCSE model, we ﬁrst construct a contrastive learning dataset using\nthe training sentences.\nStep 1: Extracting Sentence and Entity Embeddings\nFor each training instance (sentence s, subject entity e1, object entity e2, relation type r), we\nuse RoBERTa-base to generate four embeddings:\nhs = RoBERTa(s),\nhe1 = RoBERTa(e1),\nhe2 = RoBERTa(e2),\nhr = RoBERTa(r)\n(6)\nwhere:\n• hs is the sentence embedding.\n• he1 and he2 are subject and object entity embeddings.\n• hr is the relation type embedding.\n2. Step 2: Computing Weighted Similarity Scores\nWe deﬁne the similarity between two training instances (si, e1i, e2i, ri) and (sj, e1j, e2j, rj) as:\nsim(si, sj) = α1 cos(hsi, hsj)\n+ α2(β1 cos(he1i, he1j ) + β2 cos(he2i , he2j ))\n+ α3 cos(hri, hrj)\n(7)\nwhere:\n• α1 = 1/3, α2 = 1/3, α3 = 1/3, β1 = 1/2, β2 = 1/2 are weighting factors for diﬀerent\ncomponents.\n• cos() represents cosine similarity.\nStep 3: Selecting Positive and Negative Pairs for Contrastive Learning\nFor each training sentence si:\n• Select k most similar sentences\n\b\ns+\n1 , . . . , s+\nk\n\t\nas positive examples (entailment).\n• Select k least similar sentences\n\b\ns−\n1 , . . . , s−\nk\n\t\nas negative examples (contradiction).\nThis dataset is then used to ﬁne-tune SimCSE.\n3. Fine-Tuning SimCSE on the Contrastive Dataset\nWe ﬁne-tune SimCSE using contrastive learning loss, ensuring that:\n• Positive pairs are mapped closer in embedding space.\n• Negative pairs are pushed apart.\n11\n"}, {"page": 12, "text": "The contrastive loss function is deﬁned as:\nL = −log\nexp(sim(hi, h+\ni )/τ)\nPN\nj=1(exp(sim(hi, h+\ni )/τ) + exp(sim(hi, h−\ni )/τ))\n(8)\nwhere:\n• τ is a tempe rature scaling parameter.\n• hi, h+\ni , h−\ni are embeddings of the premise, positive, and negative examples, respectively.\nAfter ﬁne-tuning, the SimCSE models understand the RE semantics better.\n4. Retrieving Demonstrations Using Fine-Tuned SimCSE\nOnce SimCSE is ﬁne-tuned, it is used for retrieving demonstrations for test sentences.\nStep 1: Computing Test and Training Embeddings\nFor a test sentence stest, its embeddings are computed using the ﬁne-tuned SimCSE (FT_SimCSE):\nhstest = FT_SimCSE(stest),\nhe1test = FT_SimCSE(e1test),\nhe2test = FT_SimCSE(e2test)\n(9)\nSimilarly, for each training sentence si, we compute:\nhsi = FT_SimCSE(si),\nhe1i = FT_SimCSE(e1i),\nhe2i = FT_SimCSE(e2i)\n(10)\nStep 2: Computing Weighted Similarity for Demonstration Selection\nThe ﬁnal similarity score between test stest and training sentence si is:\nsim(stest, si) = α1 · cos(hstest, hsi)\n+ α2 · (β1 · cos(he1test , he1i )\n+ β2 · cos(he2test , he2i))\n(11)\nwhere α1 = 1/2, α2 = 1/2, β1 = 1/2, and β2 = 1/2 are task-speciﬁc weights. We rank all training\nsentences based on this similarity and select the top-k most relevant sentences for In-Context\nLearning (ICL).\n4.3\nPrompt Structure for Relation Extraction\nThe literature analysis of eﬀective prompt templates for Relation Extraction (RE) reveals that high-\nperforming designs typically follow a structured format composed of the following components:\n1. Task Instruction – Speciﬁes the RE task, including the deﬁnition of entity types and the list\nof possible relation labels. This sets the context for the model and ensures it understands the\nclassiﬁcation objective.\n2. In-Context Demonstrations – A set of example instances provided to the model during inference.\nThese examples can be selected through random sampling or retrieval-based methods such as\nKATE, Fine-Tuned Relation Representation (as in GPT-RE), or our proposed Relation-Aware\nRetrieval (RAR).\n3. Chain-of-Thought (CoT) Knowledge Integration – Embeds intermediate reasoning steps to guide\nthe model through a logical inference process. This component enhances interpretability and\nsupports more accurate relation classiﬁcation by explicitly modelling the reasoning behind label\nselection.\n12\n"}, {"page": 13, "text": "4. Test Input – The actual input sentence and the associated subject and object entities for which\nthe model is expected to predict the relation type.\nThis structured prompt design allows the language model to (i) comprehend the task through clear\ninstructions, (ii) generalise from example demonstrations, and (iii) perform informed reasoning through\nCoT mechanisms before generating predictions.\n4.3.1\nTask Instruction Component\nThe Task Instruction serves to clearly deﬁne the speciﬁc Relation Extraction (RE) objective, guiding\nthe language model to understand both the nature of the task and the expected output format. The\ntask instruction used in our prompt design is as follows:\nYou are a knowledgeable AI assistant tasked with extracting the relationship between two entities in a\ngiven sentence. Your goal is to classify the relationship into one of the predefined relation\ntypes based on the contextual and semantic information provided.\nThe predefined relation types are:\nTrIP: Treatment improves the medical problem, TrWP: Treatment worsens medical problem, TrCP: Treatment\ncauses medical problem, TrAP: Treatment is applied for a medical problem, TrNAP: Treatment is not\napplied due to a medical problem, TeRP: Test reveals a medical problem, TeCP: Test is conducted\nto investigate a medical problem, PIP: One medical problem indicates another medical problem\nThis instruction ensures that the model is task-aware and aligned with the classiﬁcation schema\nused in clinical RE benchmarks, such as those adapted from 2010 i2b2/VA datasets.\n4.3.2\nIn-Context Demonstrations Component\nThe demonstrations serve as in-context learning examples that guide the model toward generating the\ncorrect output by illustrating the task through labelled instances. These examples can be selected\neither randomly or through retrieval-based methods that aim to improve relevance and task alignment.\nNotable selection strategies include KATE, which retrieves semantically similar examples, Fine-Tuned\nRelation Representation (as employed in GPT-RE), and our proposed Relation-Aware Retrieval (RAR),\nwhich integrates contrastive learning signals into the retrieval process.\nBy providing high-quality\ndemonstrations, these methods enhance the model’s ability to generalise and make accurate relation\npredictions.\nEach in-context demonstration follows a structured format consisting of two main components:\n• Context: The sentence containing the subject and object entities.\n• Response: The correct relation label corresponding to the semantic relationship between the\nentities.\nAn example in-context demonstration derived from the 2010 i2B2/VA dataset is shown below:\nContext: Urinalysis revealed trace glucose, no ketones, no red cells, no white cells and less than\none epithelial cell.\nGiven the context, what is the relation between \"urinalysis\" and \"trace glucose\"?\nResponse: TEST REVEALS MEDICAL PROBLEM\nThis format provides a clear mapping between linguistic context and labelled relational semantics,\nallowing the model to generalise from task-speciﬁc examples during inference.\n13\n"}, {"page": 14, "text": "4.3.3\nChain-of-Thought (CoT) Knowledge Integration\nTo enhance the model’s reasoning capabilities, we integrate three Chain-of-Thought (CoT) prompting\nmethods speciﬁcally tailored for the Relation Extraction (RE) task into our prompt design.\nEach\nfew-shot in-context demonstration is augmented with one of the CoT reasoning methods, introducing\nintermediate logical steps that support the model in interpreting the relationship between entities more\neﬀectively.\nWe evaluate CoT prompting in two distinct settings:\n• Static Setting: A ﬁxed set of demonstrations is randomly selected and reused across all test\ninputs. The associated CoT reasoning is pre-generated and remains constant during inference.\nThis setting evaluates the general utility of static CoT-augmented examples across diverse inputs.\n• Dynamic Setting: For each test instance, relevant demonstrations are retrieved at runtime using\na retrieval method (e.g., KATE, RAR), and CoT reasoning is generated dynamically for each\nselected example. This setting enables contextual alignment between the test input and in-context\nexamples, while also adapting the reasoning to match the speciﬁc input scenario.\nIn the static setting, we design two types of prompts: zero-shot and few-shot. In the zero-shot\nstatic prompt, a ﬁxed Chain-of-Thought (CoT) reasoning template is provided without any labelled\ndemonstration examples.\nIn contrast, the few-shot static prompt includes one or more randomly\nselected examples, each augmented with manually constructed CoT reasoning to illustrate the task. In\nboth cases, the prompts remain unchanged across all test instances. Full prompt formats for both the\nzero-shot and few-shot static settings are provided in Appendix A.1 and A.2, respectively.\nIn the dynamic setting, we evaluate three Chain-of-Thought (CoT) prompting methods: Self-\nQuestioning Prompting (SQP), Gold Label-induced CoT, and Output Format-considered CoT. These\nmethods are assessed exclusively in the few-shot setting, where demonstrations are dynamically re-\ntrieved for each test instance.\nTo ensure high-quality examples, demonstrations are selected using our best-performing retrieval\nmethod, Relation-Aware Retrieval (RAR). At runtime, each retrieved example is augmented with one\nof the CoT reasoning strategies, allowing the model to generate contextualised and logically guided\npredictions per input while leveraging diverse CoT formats for comparative evaluation.\nSelf-Questioning Prompting (SQP)\nThe Self-Questioning Prompting (SQP) method leverages\nself-questioning as a mechanism to induce structured reasoning within each demonstration. For every\ndynamically retrieved demonstration, the language model is prompted using a question-driven template\nthat encourages it to generate and answer a set of diagnostic questions aimed at uncovering the semantic\nnature of the relationship between the subject and object entities. The model’s response, which reﬂects\nits internal reasoning process, is then appended to the demonstration. This augmentation results in a\nChain-of-Thought (CoT)-style explanation, enriching the prompt with interpretable and task-aligned\nrationale to support more accurate relation classiﬁcation.\nThe SQP template is as follows:\nContext: [context]\nGiven the context sentence, identify the relationship between [entity1] and [entity2] within the\nsentence. Generate questions to explore the nature of their relationship, such as whether it\ninvolves treatments improving (TrIP), worsening (TrWP), causing (TrCP), being administered for (\nTrAP), or not being administered due to (TrNAP) a medical problem; tests revealing (TeRP) or\ninvestigating (TeCP) a medical problem; or one medical problem indicating another (PIP). Answer\nthe questions and use the insights to categorise the relationship between [entity1] and [entity2]\nas [relation].\nThis approach produces self-generated CoT reasoning that can be directly appended to each re-\ntrieved demonstration, enabling adaptive and interpretable prompt construction. Full prompt with an\nin-context demonstration is provided in Appendix A.3\n14\n"}, {"page": 15, "text": "Gold Label-induced CoT\nIn this dynamic reasoning approach, each in-context demonstration is\nretrieved using the Relation-Aware Demonstration Retrieval (RAR) method and augmented with a\nlabel-aligned explanation. The explanation is generated by prompting a language model to describe\nthe rationale behind the gold label for the entity pair in the sentence.\nThe following template is used to generate the reasoning:\nWhat are the clues that lead to the relation between [entity1] and [entity2] to be [relation] in the\nsentence [context]?\nThis reasoning is then appended directly to the demonstration. The prompt with one in-context\ndemonstration is provided in Appendix A.4\nOutput Format-considered CoT\nIn this method, demonstrations are retrieved using RAR and\naugmented with structured output explanations, where the relation is explicitly verbalised along with\nthe subject and object entity types. The structured response is added to each in-context demonstration\nin the following way:\nContext: Hypertension was managed with a beta blocker and an ACE inhibitor, and Integrilin was\ncontinued post MI for 18 hours.\nGiven the context, what is the relation between a beta blocker and hypertension?\nResponse: TREATMENT IS ADMINISTERED FOR MEDICAL PROBLEM. Because treatment [beta blocker] IS\nADMINISTERED FOR the problem [hypertension].\nFull prompt example for this approach is provided in Appendix A.5\n5\nExperiments\nWhile numerous open-source and proprietary LLMs could be selected for evaluation in a given research\ncontext, most academic studies rarely articulate the rationale behind their model choices. Yet, model\nselection is not trivial—certain LLMs may be inherently more suitable for the task at hand, depending\non their eﬃciency, reasoning capabilities, and cost proﬁle. Providing a clear justiﬁcation for model\nselection not only enhances the reproducibility and transparency of the study but also guides future\nresearch by helping readers identify the most appropriate models for similar applications.\nInformation extraction systems typically process large volumes of text data, necessitating language\nmodels that are both fast and cost-eﬃcient, while also possessing strong language understanding capa-\nbilities. Accordingly, we prioritized models that oﬀer high generation throughput (measured in output\ntokens per second) and low processing costs.\nWithin this context, the Gemini Flash family [Gemini Team, Anil et al.(2023)] demonstrates excep-\ntional eﬃciency. We selected Gemini Flash 2.0 as our primary model, which generates approximately\n166 output tokens per second and costs around $0.10 per million input tokens and $0.40 per million\noutput tokens. For comparison [Artiﬁcial Analysis(2025)], GPT-4o generates about 107 tokens per\nsecond, while charging $2.50 per million input tokens and $10 per million output tokens. Similarly,\nClaude 3.5 Sonnet is slower (64 tokens per second) and more expensive ($3 per million input tokens and\n$15 per million output tokens) [Artiﬁcial Analysis(2025)]. These ﬁgures clearly position Gemini Flash\n2.0 as the most eﬃcient option in terms of response speed and cost-eﬀectiveness among contemporary\nmodels.\nBeyond eﬃciency, language understanding and reasoning capability are essential for robust in-\nformation extraction performance. To assess these aspects, we refer to the MMLU-Pro benchmark\n[Wang et al.(2024)], which comprises over 12,000 rigorously curated questions drawn from academic\nexams and textbooks across 14 domains, including Biology, Business, Chemistry, Computer Science,\nEconomics, Engineering, Health, History, Law, Mathematics, Philosophy, Physics, Psychology, and\nOther. On this benchmark, Gemini Flash 2.0 achieves an overall score of 0.776, which is comparable to\n15\n"}, {"page": 16, "text": "GPT-4o (0.779) and Claude 3.5 Sonnet (0.776) [TIGER-Lab(2024)]. This close alignment in MMLU-\nPro performance, combined with superior eﬃciency, makes Gemini Flash 2.0 a compelling choice for\nevaluating bilingual relation extraction at scale.\nWe also include Gemini Flash 1.5 in our evaluation to examine whether the updated model version\n(Flash 2.0) exhibits any measurable diﬀerences in relation extraction performance within the bilingual\nsetting.\nAnother model included in our study is DeepSeek V3 [DeepSeek-AI, Liu et al.(2025)], which rep-\nresented one of the most capable open-source LLMs at the time of its release—coinciding roughly\nwith the Gemini Flash 2.0 launch. Its open availability makes it both cost-eﬀective and suitable for\nlocal deployment, oﬀering an additional point of comparison against proprietary models. In terms of\nMMLU-Pro performance (0.758), DeepSeek V3 performs comparably to Gemini Flash 2.0 (0.776), sug-\ngesting similar general reasoning capabilities while oﬀering greater accessibility and lower operational\ncost.\nThis inclusion allows us to contrast proprietary high-performance models with open alterna-\ntives in the context of bilingual relation extraction. Finally, we include GPT-4o mini to examine how a\nsmaller proprietary model from OpenAI performs within our bilingual relation extraction experimental\nsetting, providing additional insight into the trade-oﬀbetween model size and information extraction\ncapability.\nWe ran all inference experiments with the APIs of Gemini-1.5-ﬂash, Gemini-2.0-ﬂash, GPT-4o-\nmini, and DeepSeek-v3 under their default hyperparameter settings. By ﬁxing the temperature to\n0.0 (deterministic output) and keeping other decoding parameters (max tokens, top_p, penalties)\nconstant, we ensured that any performance diﬀerences arise from the models themselves rather than\nfrom variations in generation settings.\nIn Relation-Aware demonstration Retrieval (RaR) method, each training example is encoded using\nfour components: sentence, subject entity, object entity, and relation type.\nThese are embedded\nusing RoBERTa-base for English and Turkish, and similarity between examples is computed using a\nweighted combination of cosine similarities across these components. Speciﬁcally, in Equation 7, we set\nthe weights α1 = α2 = α3 = 1/3, β1 = β2 = 1/2, while in Equation 11, we used α1 = α2 = 1/2, and\nβ1 = β2 = 1/2. These weights ensure a balanced inﬂuence between sentence and entity embeddings\nduring similarity computation and retrieval.\nFor contrastive learning, we generate pairs of similar\n(positive) and dissimilar (negative) examples using this similarity metric. The resulting model is ﬁne-\ntuned using a contrastive loss for three epochs (batch size 16, learning rate 3e-5). Embeddings are\nprojected via a two-layer MLP head and optimized using temperature-scaled contrastive loss.\nThe models were assessed under two primary dimensions: demonstration selection quality and\nreasoning-based prompting, with a particular focus on how language, retrieval method, and model\narchitecture interact to inﬂuence performance.\nAll evaluations are based on the micro-F1 metric.\nOur results were compared with the SOTA F1 score on the i2b2 2010/VA dataset, which is 91.8\n[Roy et al.(2021)].\n5.1\nEvaluation of In-Context Selection Strategies\nOur ﬁrst set of experiments investigates the impact of diﬀerent example selection methods in few-shot\nin-context learning, without incorporating any explicit reasoning. These methods vary from random\nsampling to more sophisticated approaches like KATE, Fine-Tuned Relation Representation (FT-RR),\nand our proposed Relation-Aware demonstration Retrieval (RAR). The results, which can be seen in\nTable 1, demonstrate that RAR consistently outperforms all other retrieval techniques, regardless of\nthe language or model used.\n16\n"}, {"page": 17, "text": "Table 1: Comparison of retrieval strategies (micro-F1).\nLang.\nMethod\nGemini\n1.5 F.\nGemini\n2.0 F.\nGPT-4o\nmini\nDeepSeek\nv3\nTurkish Random\n0.768\n0.720\n0.558\n0.788\nKATE\n0.816\n0.794\n0.698\n0.820\nFT Rel. Rep.\n0.842\n0.804\n0.680\n0.818\nRAR (Ours)\n0.870\n0.844\n0.712\n0.852\nEnglish\nRandom\n0.822\n0.840\n0.682\n0.842\nKATE\n0.844\n0.862\n0.744\n0.866\nFT Rel. Rep.\n0.864\n0.862\n0.780\n0.876\nRAR (Ours)\n0.906\n0.900\n0.804\n0.906\nIn English, Gemini-1.5 Flash combined with RAR reaches the highest performance with a micro-F1\nscore of 0.906, while Gemini-2.0 Flash closely follows at 0.9, conﬁrming that both models are capable of\nextracting comparable beneﬁt from relationally enriched demonstrations despite using fewer examples\noverall. DeepSeek-v3 matches Gemini-1.5 Flash at 0.906, demonstrating that its internal representation\nspace aligns particularly well with RAR’s contrastively ﬁne-tuned embeddings.\nIn Turkish, Gemini-1.5 Flash again outperforms the alternatives with 0.87, while DeepSeek-v3\nfollows closely at 0.852, slightly surpassing Gemini-2.0 Flash (0.844).\nThese ﬁndings suggest that\nDeepSeek exhibits stronger adaptability than Gemini-2.0 in morphologically complex languages, even\nunder identical demonstration conditions.\nThe baselines further highlight the advantages of RAR. FT-RR achieves competitive results, par-\nticularly in English where scores approach 0.864, yet its reliance on entity-level cues alone limits\nits eﬃciency compared to RAR’s richer relational representations.\nKATE also performs relatively\nwell, especially in Turkish when paired with language-speciﬁc embeddings (TURKCELL/roberta-base-\nturkish-uncased), but it consistently falls short of RAR, reﬂecting the limitations of relying solely on\nsentence-level similarity. Random selection, as expected, yields the weakest performance across all\nmodels and languages, underlining the importance of structured retrieval.\nA cross-model comparison reveals signiﬁcant variation in how each LLM responds to the retrieval\nstrategies. Gemini-1.5 Flash consistently achieves the highest performance, particularly when paired\nwith RAR, reaﬃrming its reliability in both English and Turkish. In contrast, GPT-4o-mini exhibits\nweaker performance, particularly in Turkish, where it struggles to beneﬁt from even well-selected\ndemonstrations. Its limited adaptability to morphologically rich languages and few-shot contexts likely\ncontributes to this underperformance. DeepSeek-v3, meanwhile, shows impressive stability across both\nlanguages, often trailing just behind Gemini 1.5 Flash and even matching it in English in some settings.\nIts ability to maintain high scores with RaR in both English and Turkish suggests a strong internal\nrepresentation space that is compatible with structured retrieval. Surprisingly, Gemini Flash 2.0 ranks\nbelow Gemini Flash 1.5 and DeepSeek V3 in the Turkish setting, indicating that the newer version may\nnot generalize as eﬀectively to morphologically rich languages within the bilingual relation extraction\ntask.\n5.2\nImpact of CoT-Augmented Prompting\nBeyond retrieval, we explore whether reasoning-augmented prompting strategies can further improve\nmodel performance.\nThese strategies — which include both static and dynamic Chain-of-Thought\nprompting — are designed to help the model reason more explicitly about the semantic relationship\nbetween entities in a sentence.\nAs can be seen in Table 2, among all Chain-of-Thought (CoT) prompting methods, the Output\nFormat CoT emerges as the most eﬀective, consistently achieving the highest scores across nearly all\nconﬁgurations. In English, DeepSeek V3 attains a remarkable score of 0.918, representing the best\noverall performance observed in our experiments. Gemini Flash 2.0 follows closely with a score of\n0.904, marginally outperforming Gemini Flash 1.5 (0.894). An important observation is that Gemini\n17\n"}, {"page": 18, "text": "Table 2: Results of Zero-Shot (ZS) and Few-Shot (FS) Chain-of-Thought (CoT)\nprompting methods (5-shot, micro-F1).\nLanguage\nApproach\nGemini 1.5\nFlash\nGemini 2.0\nFlash\nGPT-4o\nmini\nDeepSeek\nv3\nTurkish\nStatic CoT\n0.768\n0.588\n0.606\n0.602\nFS Static CoT\n0.822\n0.840\n0.756\n0.784\nGold Label-induced\n0.852\n0.810\n0.702\n0.782\nOutput Format CoT\n0.864\n0.856\n0.744\n0.858\nSelf-Questioning\n0.840\n0.788\n0.734\n0.654\nEnglish\nZS Static CoT\n0.802\n0.812\n0.706\n0.718\nFS Static CoT\n0.858\n0.842\n0.756\n0.854\nGold Label-induced\n0.866\n0.866\n0.822\n0.840\nOutput Format CoT\n0.894\n0.904\n0.822\n0.918\nSelf-Questioning\n0.838\n0.808\n0.832\n0.848\nFlash 1.5 did not reach the performance level achieved by the ﬁve-shot prompt using RAR (Retrieval-\nAugmented Reasoning) examples, suggesting that it struggles to eﬀectively follow complex, multi-step\ninstructions that require explicit reasoning integration.\nThis trend persists in Turkish, where the Output Format CoT again delivers strong results. How-\never, Gemini Flash 1.5 achieves the highest score of 0.864, followed closely by Gemini Flash 2.0 (0.856)\nand DeepSeek V3 (0.858). Across all experiments, GPT-4o mini consistently underperforms relative to\nthe other models. These ﬁndings indicate that explicitly prompting models to articulate their reason-\ning through structured output formats enhances both interpretability and prediction conﬁdence. This\neﬀect is particularly evident when the reasoning process is anchored in high-quality demonstrations\nprovided by RAR (Retrieval-Augmented Reasoning)—with the notable exception of Gemini Flash 1.5,\nwhich appears less capable of fully leveraging such structured reasoning cues.\nThe Gold Label-induced CoT, which prompts models to rationalise pre-labelled relationships, also\nshows notable gains over static prompting.\nIn both English and Turkish, it pushes Gemini and\nDeepSeek scores into the mid-to-high 0.8 range. However, it still falls short of Output Format CoT, sug-\ngesting that the beneﬁt of reﬂective justiﬁcation is enhanced when accompanied by a more constrained\nand semantically anchored output structure.\nBy contrast, the Self-Questioning method exhibits inconsistent results. While it performs compet-\nitively for Gemini models in English, reaching up to 0.838, its eﬀectiveness declines in Turkish and\nunder DeepSeek, particularly in the Turkish SQ setting (0.654). This drop indicates that the question-\nanswer prompting format may not generalise as eﬀectively across languages or architectures, and may\nrequire more alignment with the model’s training distribution or reasoning style.\nStatic CoT methods show expected limitations. The Zero-Shot Static CoT, which relies solely on\na ﬁxed reasoning template without demonstrations, oﬀers only marginal improvements over baseline\nprompting, with the lowest performance observed across all CoT strategies. Few-Shot Static CoT,\nwhile stronger, still lags behind dynamic CoT methods like Output Format and Gold Label-induced\nreasoning. This reinforces the advantage of pairing structured reasoning with semantically relevant\nexamples retrieved on a per-instance basis, rather than relying on ﬁxed templates or generic examples.\n5.3\nEvaluation of Fine-Tuning Methods\nAs ﬁne-tuning baselines, we employed BERT [Devlin et al.(2019b)] and PURE [Zhong and Chen(2021)].\nWhile BERT serves as a widely adopted foundational model, PURE oﬀers a simple yet highly eﬀective\narchitecture for relation extraction. Speciﬁcally, it inserts special text markers around the subject and\nobject entity spans within a sentence and then uses the marker embeddings to classify the relation\ntype.\nOur selection of PURE as a baseline method is motivated by its strong empirical performance on\nestablished datasets such as ACE05, where it achieved near state-of-the-art (SOTA) results. In a recent\nstudy [Liu et al.(2022b)], PURE achieved an F1 score of 69.4 using ALBERT-xxlarge as the encoder,\ncompared to the best reported score of 72.7, which was achieved by a generative approach using the\n18\n"}, {"page": 19, "text": "much larger T5-3B model. Moreover, the Fine-Tuned Relation Representation method used in our\nevaluations [Wang et al.(2023a)] employs PURE as a relation extraction–speciﬁc example retrieval\napproach. Consequently, incorporating PURE as the ﬁne-tuning baseline in our evaluation allows us\nto observe its impact in both ﬁne-tuning and retrieval settings.\nWe ﬁne-tuned both BERT and PURE on our bilingual subset of the 2010 i2b2/VA dataset, which\nincludes 1,500 training and 500 test examples, to establish baseline reference points for bilingual com-\nparison. In our experiments, PURE achieved a Micro-F1 score of 0.733 in English and 0.632 in Turkish,\nwhile BERT performed slightly lower, reaching 0.604 in Turkish and 0.509 in English.\nThese results indicate that the limited training data size (1,500 examples) substantially constrains\nthe performance of ﬁne-tuned models. To further examine the eﬀect of data size, we extended our\nexperiments using the entire English version of the i2b2-2010 dataset (excluding the 500 test examples\nfrom our subset), totaling 13,832 training instances. In this setting, PURE achieved a Micro-F1 score of\n0.846, in English, demonstrating the positive impact of larger training data on ﬁne-tuning performance.\nAccording to Fraile Navarro et al. (2023), the state-of-the-art (SOTA) result for the i2b2-2010\nrelation extraction (RE) task was reported by Roy and Pan [Roy et al.(2021)] , who achieved a Micro-\nF1 score of 0.918 using an 80/20 train–test split. However, their approach leverages the UMLS ontology\nto enrich the input representations. This enhancement is challenging to replicate in a multilingual\nsetting, as the ontology resources are predominantly available in English.\nWhen comparing the ﬁne-tuning results with our prompting-based approaches, all retrieval and\nChain-of-Thought (CoT) methods outperformed the ﬁne-tuned models in both languages within our\nsubset of 1,500 training examples. Furthermore, our retrieval approach—combining RAR (Retrieval-\nAugmented Reasoning) and the Fine-Tuned Relation Representation method—surpassed the PURE\nmodel ﬁne-tuned on 13,832 English examples across all evaluated LLMs, except GPT-4o mini. Com-\nparable trends were observed with the Gold Labeled–Induced and Output Format CoT approaches.\nNotably, DeepSeek V3 achieved a Micro-F1 score of 0.918 using the Output Format CoT method,\neﬀectively matching the state-of-the-art (SOTA) performance—despite relying on only 1,500 training\nexamples.\n5.4\nModel Behaviour Across Languages and Prompting Types\nTaken together, these ﬁndings point to several important trends. First, retrieval quality remains the\nmost critical factor in few-shot prompting. Even without reasoning, models supplied with well-aligned\ndemonstrations through RAR consistently outperform those using random or static samples, and in\nmany cases, even surpass models augmented with complex reasoning but inferior examples.\nSecond, model selection signiﬁcantly inﬂuences few-shot performance, especially in cross-lingual\nscenarios. Gemini 1.5 Flash remains the most reliable model across both Turkish and English, with\nstrong results under all conﬁgurations. Its ability to leverage RAR and interpret structured prompts\ngives it a consistent edge. Its successor, Gemini Flash 2.0, performs competitively in English, slightly\noutperforming Gemini 1.5 and 2.0 Flash under the Output Format CoT setting. However, it consis-\ntently underperforms in Turkish across all experiments, with the exception of the Few-Shot Static CoT\nconﬁguration, where it achieves comparable results. DeepSeek-v3 proves highly competitive in English,\nwhere it even outperforms Gemini 1.5 Flash in CoT-enhanced settings, but shows mild underperfor-\nmance in Turkish. GPT-4o-mini, on the other hand, lags consistently. Despite its general-purpose\narchitecture, it appears less equipped to adapt to specialised tasks like clinical relation extraction in\nfew-shot scenarios.\nFinally, we observe that reasoning does not always lead to performance gains. For instance, Gemini\nFlash 1.5 performed worse across all CoT prompting approaches in both languages compared to In-\nContext Learning (ICL) with RAR examples. For the other models, only the Output Format CoT\nconsistently outperformed ICL with RAR across both languages.\nOverall, these ﬁndings indicate that Output Format CoT, when anchored in well-selected demon-\nstrations, eﬀectively ampliﬁes the beneﬁts of retrieval. Collectively, these insights suggest that, for\nrelation extraction tasks—particularly in clinical or multilingual contexts—performance depends less\non model complexity or reasoning depth, and more on the alignment between eﬀective example selection\nand appropriately designed prompting strategies tailored to each model.\n19\n"}, {"page": 20, "text": "The results of our experiments, summarised in Table 3, highlight several critical insights about\nthe eﬀectiveness of diﬀerent in-context learning conﬁgurations for clinical relation extraction across\nlanguages and model architectures.\nThe comparison includes traditional ﬁne-tuned baselines, few-\nshot random selection as in-context prompting baseline and few-shot static CoT and CoT prompting\nbaselines, which give the best prompting results in both languages and reasoning-augmented prompting.\nTable 3: Performance comparison of baseline and proposed methods on Turkish\nand English subsets (micro-F1).\nCategory\nMethod\nTR\nEN\nFine-tuned\nBERT\n0.604\n0.596\nPURE\n0.632\n0.740\nNon-prompting SOTA\nN/A\n0.918\nPURE (13,832 rec.)\nN/A\n0.846\nIn-context\nFew-shot (Random)\n0.788\n0.842\nSelection\nRAR (ours)\n0.870\n0.906\nChain-of-\nStatic CoT\n0.840\n0.858\nThought\nOutput Format CoT\n0.864\n0.918\nAt the core of our ﬁndings is the clear superiority of retrieval-based in-context prompting over\ntraditional supervised models. Both BERT and PURE, although ﬁne-tuned on the task, fall signiﬁ-\ncantly behind modern language models when used in combination with structured prompting strategies.\nIn Turkish, BERT achieves only 0.604 micro-F1, and PURE reaches 0.632. Even in English, where\nresource availability and training data are typically richer, these models remain below 0.75. This under-\nscores the limited generalisation capacity of ﬁne-tuned static encoders in complex RE task in medical\ndomain.\nIn contrast, zero-shot prompting with Gemini 1.5 Flash, without any in-context examples, already\nexceeds both baselines, reaching 0.772 in Turkish and 0.798 in English. This demonstrates the strength\nof instruction-tuned LLMs, which, even in isolation, capture signiﬁcant task-relevant priors. However,\nonce demonstration selection is introduced, performance improves substantially.\nUsing our proposed Relation-Aware demonstration Retrieval strategy, Gemini 1.5 Flash achieves\n0.87 in Turkish and 0.906 in English, setting the standard among all evaluated conﬁgurations. These\nresults aﬃrm the importance of semantically and relationally aligned examples and validate RAR’s\ncontrastive learning approach, which integrates sentence, entity, and relation-level embeddings.\nInterestingly, DeepSeek-v3, while slightly underperforming Gemini 1.5 Flash in Turkish (0.852),\nmatches it in English (0.906) when paired with RaR, and even surpasses it when Output Format-based\nChain-of-Thought (CoT) prompting is introduced — reaching 0.918, the highest micro-F1 across all\nconﬁgurations. This shows that DeepSeek-v3 is particularly eﬀective at leveraging reasoning-oriented\nprompt formats when the underlying retrieval quality is high. It also suggests that some LLMs may\nrespond better to structural regularisation in output formats than others, highlighting the need to\ntailor reasoning strategies to the speciﬁc capabilities of the target model.\nAs a ﬁnal note, our experiments are not the ﬁrst to evaluate prompting approaches on the i2b2-\n2010 dataset. Wang, Zhao, and Petzold (2023) conducted an earlier study in which they evaluated\nSelf-Questioning Prompting (SQP) on the i2b2-2010 corpus using three LLMs: Bard, GPT-3.5, and\nGPT-4. Their setup used SQP with randomly selected 5-shot static examples, and they compared\nits performance against a 5-shot Static CoT prompt. For evaluation, they randomly sampled 50% of\nthe test set from the original dataset. Their results showed strong SQP performance: 0.940 on Bard,\n0.920 on GPT-4, and 0.860 on GPT-3.5 under the 5-shot SQP setting. Under zero-shot SQP, Bard’s\nperformance dropped sharply to 0.760, whereas GPT-4 and GPT-3.5 maintained the same scores as\n20\n"}, {"page": 21, "text": "in the 5-shot SQP conﬁguration. Despite SQP’s sensitivity to model-speciﬁc behavior, the authors\nreported that SQP consistently outperformed the Static CoT baseline across all evaluated models on\nthis dataset. Since we do not access the dataset they used, we can not compare their results with our\nsetting. But in our evaluation, SQP also seemed to depend on the characteristics of the model, where\nit only outperformed Output Format CoT in GPT-4o mini in English. It was the worst model among\nthe few-shot CoT methods in Gemini Flash 1.5 and 2.0 and performed relatively better in DeepSeek\nV3.\n5.5\nAblation Study (5-10-15)\nFigure 1: Comparison of diﬀerent methods across diﬀerent shot settings.\nTo better understand the eﬀect of demonstration quantity on retrieval eﬀectiveness, we conduct\nan ablation study by varying the number of in-context examples across 5-shot, 10-shot, and 15-shot\nsettings. These evaluations are carried out exclusively using the Gemini 1.5 Flash model, the core LLM\nused throughout our in-context prompting analysis. Gemini 1.5 Flash was chosen for the ablation study\nas it achieved competitive results on the English dataset and delivered the highest scores on the Turkish\ndataset. Gemini 2.0 Flash was excluded from further consideration, as Gemini 1.5 Flash consistently\noutperformed it in our experiments on the Turkish dataset.\nFour example selection strategies are\ncompared: Random selection, KATE retrieval, Fine-Tuned Relation Representation (FT-RR), and our\nproposed Relation-Aware demonstration Retrieval. Figure 1 illustrates how increasing the number of\ndemonstrations inﬂuences the performance across both English and Turkish datasets.\nAcross all conﬁgurations, RAR consistently yields the strongest results, conﬁrming the value of\nintegrating sentence, entity, and relation-level semantics through contrastive learning. In Turkish, RAR\nimproves from 0.87 in the 5-shot setting to 0.888 with 10 demonstrations, showing only a marginal\ngain to 0.88 at 15-shot — a clear indicator of diminishing returns. A similar trend is observed in\nEnglish, where RAR reaches 0.906 at 5-shot, then stabilises around 0.8955 and 0.8934 for 10- and\n15-shot settings, respectively. These outcomes underscore the eﬃciency of RAR: even with limited\ndemonstrations, its rich representations oﬀer consistently high performance.\nFT-RR also performs competitively across both languages. In Turkish, FT-RR reaches 0.842, 0.852,\nand 0.872 in the 5-, 10-, and 15-shot settings, respectively, while in English it improves steadily from\n0.864 to 0.89. These results suggest that while FT-RR beneﬁts from relational encoding, it is less\neﬃcient than RAR, requiring more demonstrations to approach its peak performance. Nonetheless,\nFT-RR remains a strong alternative when contrastive retraining is not feasible.\nKATE, relying solely on sentence-level similarity, performs surprisingly well in Turkish (0.866 at\n15-shot), especially when backed by a Turkish-speciﬁc encoder. However, its lower scores in low-shot\n21\n"}, {"page": 22, "text": "scenarios and comparatively modest gains in English (max 0.874) reveal the limitations of its surface-\nlevel alignment. Unlike RAR and FT-RR, KATE lacks task-speciﬁc relational grounding, making it\nmore reliant on larger prompt sizes for eﬀective guidance.\nAs expected, random demonstration selection results in the lowest performance across all conﬁg-\nurations. Even when scaled up to 15-shot, Gemini 1.5 Flash-Rand trails behind the more structured\nmethods, reinforcing that demonstration quantity alone cannot compensate for the lack of semantic\nand relational alignment.\nOverall, the ablation study emphasises that retrieval quality matters more than retrieval volume.\nFine-tuned retrievers like FT-RR and RAR consistently outperform less targeted methods, with RaR\ndemonstrating the highest eﬃciency — achieving top-tier performance with as few as ﬁve examples.\nThese ﬁndings further validate the importance of relation-aware retrieval strategies in clinical relation\nextraction, particularly under few-shot constraints.\n5.6\nError Analysis\nT\nrIP\nT\nrWP\nT\nrCP\nT\nrAP\nT\nrNAP\nPIP\nT\neRP\nT\neCP\nPredicted label\nT\nrIP\nT\nrWP\nT\nrCP\nT\nrAP\nT\nrNAP\nPIP\nT\neRP\nT\neCP\nT\nrue label\n7\n0\n0\n3\n2\n0\n0\n0\n1\n5\n2\n3\n0\n0\n0\n0\n0\n0\n20\n1\n0\n0\n0\n1\n3\n0\n0\n131\n2\n0\n1\n1\n0\n0\n3\n1\n5\n0\n0\n0\n0\n0\n1\n2\n0\n104\n5\n0\n0\n0\n0\n0\n0\n1\n165\n2\n0\n0\n0\n2\n1\n0\n9\n16\nEnglish\n0\n20\n40\n60\n80\n100\n120\n140\n160\nTdviP\nTdvKP\nTdvNOP\nPiUTdv\nPNUMTdv\nT\nstOÇP\nPiUT\nst\nPGP\nPredicted label\nTdviP\nTdvKP\nTdvNOP\nPiUTdv\nPNUMTdv\nT\nstOÇP\nPiUT\nst\nPGP\nT\nr e label\n10\n0\n1\n0\n1\n0\n0\n0\n0\n7\n1\n3\n0\n0\n0\n0\n0\n3\n16\n0\n0\n2\n0\n1\n11\n2\n0\n120\n3\n0\n0\n2\n0\n0\n3\n2\n4\n0\n0\n0\n2\n0\n0\n0\n0\n156\n1\n9\n0\n0\n0\n3\n0\n7\n17\n1\n0\n0\n1\n2\n1\n3\n0\n105\nT\n rkish\n0\n20\n40\n60\n80\n100\n120\n140\nFigure 2: Confusion matrices for English (left) and Turkish (right).\nThe error analysis of the relation extraction model applied to the English and Turkish i2b2-2010/VA\ndatasets reveals valuable insights into its accuracy and limitations. This section focuses on the negative\nresults obtained from the Gemini 1.5 Flash model using RAR, which achieved best result in Turkish\nand competetive results in English. By examining two confusion matrices given in Figure 2 —one for\nthe Turkish model and the other for the English model—we gain a clearer understanding of how well\nthe model performs in classifying various medical scenarios and where it falters.\nThe Turkish dataset presents varying levels of performance across diﬀerent classes. Notably, the\nclass \"TstOÇP\" (Test Reveals Medical Problem) achieves the highest accuracy, with a correct classiﬁ-\ncation rate of approximately 92.9%. This suggests that the model is capable of identifying scenarios\nwhere tests successfully uncover medical problems. However, signiﬁcant challenges emerge with the\nclass \"PİUTdv\" (Treatment Administered for Medical Problem), which demonstrates the highest level\nof confusion. The model frequently misclassiﬁes instances of this class into other categories, such as\n\"TdvKP\" (Treatment Worsens Medical Problem) or \"TdvİP\" (Treatment Improves Medical Problem).\nFor example, 3% of \"PİUTdv\" instances are misclassiﬁed as \"TdvKP,\" and 11% as \"TdvİP.\" These\nerrors highlight the model’s diﬃculty in distinguishing between nuanced relationships where treatment\noutcomes vary. Classes with limited data, such as \"PNUMTdv\" (Treatment Not Administered Due to\nMedical Problem) and \"PİUTst\" (Test Administered for Medical Problem), exacerbate this issue. The\nimbalance in the dataset hampers the model’s ability to learn robust representations for these classes,\nleading to low accuracy and increased confusion.\nConversely, the English model exhibits reduced confusion compared to its Turkish counterpart, indi-\ncating better diﬀerentiation between medical relationships. For example, the class \"TrAP\" (Treatment\n22\n"}, {"page": 23, "text": "Administered for Medical Problem) achieves a high accuracy rate, showcasing the model’s proﬁciency\nin correctly identifying instances where treatment is administered. Despite this overall improvement,\nchallenges remain. A notable issue arises in distinguishing between \"TeRP\" (Test Reveals Medical\nProblem) and \"TeCP\" (Test Conducted to Investigate Medical Problem). Misclassiﬁcations in these\ncategories indicate that the model struggles with semantic overlap, particularly in cases where tests\nmight serve dual purposes—both revealing and investigating medical conditions.\nAcross both datasets, the presence of data imbalance poses a recurring challenge. Classes with\nlimited examples, such as \"PNUMTdv\" in Turkish and \"TrNAP\" (Treatment Not Administered Due\nto Medical Problem) in English, experience reduced accuracy due to insuﬃcient training data. This\nimbalance skews model predictions, favouring more frequent classes while underperforming on less\ncommon relationships.\nIn addition to these observations, overarching trends highlight shared challenges between the two\nmodels.\nSemantic overlap among similar classes contributes signiﬁcantly to misclassiﬁcations.\nFor\nexample, distinctions between categories like \"Treatment Improves Medical Problem\" and \"Treatment\nAdministered for Medical Problem\" are often subtle, relying on context-speciﬁc information that the\nmodel may fail to capture. Furthermore, the model frequently produces negative outputs or invalid\npredictions in cases where the input context is ambiguous or involves rare relationships. These issues\npoint to a lack of robustness in handling edge cases, a critical limitation in medical applications where\nprecision is paramount.\nThe comparison between the English and Turkish models suggests that language plays a signiﬁcant\nrole in performance diﬀerences. The English model generally outperforms the Turkish model, likely due\nto the availability of more mature language-speciﬁc embeddings and pre-trained resources for English.\nIn contrast, the agglutinative nature of Turkish introduces additional complexity, as relationships\nare often embedded within longer and more intricate sentence structures. This linguistic diﬀerence,\ncoupled with the specialised vocabulary of the medical domain, underscores the need for language-\nspeciﬁc strategies to improve performance.\n6\nConclusion\nThis study presented the ﬁrst bilingual evaluation of relation extraction on clinical text, addressing the\npersistent scarcity of annotated resources in low-resource medical languages such as Turkish. Through a\nsystematic comparison of prompting-based large language models and ﬁne-tuned baselines, our ﬁndings\ndemonstrate that advances in in-context learning and reasoning can narrow the resource gap that\ntraditionally limits multilingual clinical NLP. Across both English and Turkish subsets, prompting-\nbased models consistently surpassed ﬁne-tuned counterparts, highlighting the capacity of large language\nmodels to generalize relational patterns beyond language boundaries when provided with semantically\nrelevant exemplars. While ﬁne-tuned models tend to underperform on small datasets, large language\nmodels maintain strong and consistent results regardless of language, indicating that overcoming this\nlimitation through ﬁne-tuning alone would require substantially larger training corpora.\nThe proposed Relation-Aware Retrieval (RAR) method proved central to this improvement. By\naligning example selection with relation-level semantics rather than surface similarity, RAR enabled\nmore stable and interpretable reasoning across languages.\nWhen applied to English clinical text,\nprompting-based large language models equipped with RAR achieved a micro-F1 of 0.906, outper-\nforming traditional ﬁne-tuned baselines such as PURE, which reached 0.74. On the Turkish subset,\nRAR likewise yielded superior performance (0.888), demonstrating that retrieval strategies grounded\nin relational semantics can substantially mitigate the performance gap between high- and low-resource\nsettings. Moreover, by augmenting RAR with dynamic, reasoning-aware prompts—speciﬁcally the Out-\nput Format-based Chain-of-Thought (CoT) design—performance further improved to 0.918 micro-F1,\nestablishing a new state of the art in bilingual clinical relation extraction.\nBeyond these empirical gains, our ﬁndings point to a broader methodological shift in clinical NLP.\nEarlier reviews have noted that most clinical information-extraction systems remain conﬁned to a few\nEnglish-language datasets, limiting their clinical applicability. The present work demonstrates that\nintegrating task-speciﬁc retrieval and structured prompting can extend relational understanding to\n23\n"}, {"page": 24, "text": "new linguistic settings without extensive retraining or data augmentation.\nThis approach oﬀers a\npractical pathway toward more equitable clinical AI, in which large language models contribute not\nonly higher accuracy but also broader accessibility across languages and healthcare contexts.\nA\nZero-Shot Static CoT Prompt Design\nFollowing the Task Instruction, we incorporate a ﬁxed Chain-of-Thought (CoT) reasoning template\ninto the static prompt to enhance the model’s interpretive and relational inference capabilities.\nTask Instruction (not repeated here)\n#Following is the static CoT template# (this is not part of the prompt, just explanation)\nLet’s think step by step. Identify the types of entities: one of the entities is a medical problem,\nand the other must be a treatment, test, or another medical problem. Determine the relationship\nbetween the entities -- analyse the sentence to understand how the treatment or test is related to\nthe medical problem. For example: if the treatment improves or worsens the condition, if the\ntreatment is administered or not administered because of the condition, if the test reveals or is\nconducted to investigate the condition or if one medical problem indicates another medical problem\n. Based on the identified relationship, conclude the appropriate relation type. Based on the\nidentified relationship, conclude the appropriate relation type.\n#Next is the test sentence#\nContext: Urinalysis was positive for protein.\nGiven the context, what is the relation between \"urinalysis\" and \"positive for protein\"?\nResponse:\nB\nFew-Shot Static Prompt Design\nTask Instruction (not repeated here)\nThe static CoT template (same as Zero-Shot Static CoT Prompt)\n#Following is the demonstration augmented with CoT knowledge#\nContext: Urinalysis revealed trace glucose, no ketones, no red cells, no white cells and less than one\nepithelial cell.\nGiven the context, what is the relation between \"urinalysis\" and \"trace glucose\"?\nLet’s think step by step:\nIdentify the types of entities: Urinalysis is a test; trace glucose is a medical problem.\nDetermine the relationship between the entities by analysing the sentence: The test (urinalysis)\nreveals the medical problem (trace glucose)\nSince the test reveals the medical finding, the relation is:\nResponse: TEST REVEALS MEDICAL PROBLEM\n#Test Input #\nContext: Urinalysis was positive for protein.\nGiven the context, what is the relation between \"urinalysis\" and \"positive for protein\"?\nResponse:\n24\n"}, {"page": 25, "text": "C\nSelf-Questioning Prompting Example\nTask Instruction (not repeated here)\n#Following is the Self-Questioning Prompting augmented demonstration#\nContext: Further testing revealed hyperparathyroidism as evidenced by an elevated PTH .\nGiven the context, the relation between further testing and an elevated pth is TEST REVEALS MEDICAL\nPROBLEM. It is because:\n## Analyzing the Relationship\nThe sentence states: \"Further testing revealed hyperparathyroidism as evidenced by an elevated PTH.\"\n**Questions to explore the relationship:**\n* **TrIP:** Did the further testing improve the elevated PTH?\n* **TrWP:** Did the further testing worsen the elevated PTH?\n* **TrCP:** Did the further testing cause the elevated PTH?\n* **TrAP:** Was the further testing administered for the elevated PTH?\n* **TrNAP:** Was the further testing not administered due to the elevated PTH?\n* **TeRP:** Did the further testing reveal the elevated PTH?\n* **TeCP:** Did the further testing investigate the elevated PTH?\n* **PIP:** Does an elevated PTH indicate hyperparathyroidism?\n**Answers:**\n* **TrIP:** No. The sentence doesn’t mention any improvement in PTH levels.\n* **TrWP:** No. The sentence doesn’t mention any worsening of PTH levels.\n* **TrCP:** No. The sentence doesn’t suggest that the further testing caused the elevated PTH.\n* **TrAP:**\nIt’s likely that the further testing was done to investigate the elevated PTH, but the\nsentence doesn’t explicitly state it.\n* **TrNAP:** No. The sentence suggests the testing was done because of the elevated PTH.\n* **TeRP:** Yes. The sentence explicitly states that the further testing \"revealed\" the elevated PTH.\n* **TeCP:** Yes. The sentence suggests the testing was conducted to investigate the elevated PTH.\n* **PIP:** Yes. Hyperparathyroidism is characterized by an overproduction of parathyroid hormone (PTH)\n, leading to elevated PTH levels.\n**Conclusion:**\nBased on the answers, the relationship between further testing and an elevated PTH can be categorized\nas **TEST REVEALS MEDICAL PROBLEM**. The further testing revealed the elevated PTH, which in turn\nindicated the underlying medical problem of hyperparathyroidism.\n#Other Examples # #Test Input #\nContext: Work up was begun for the possible myeloproliferative / myelodysplastic disorder associated\nwith the hyper-eosinophilic syndrome .\nGiven the context, the relation between work up and the hyper-eosinophilic syndrome is\n25\n"}, {"page": 26, "text": "The relation between \"work up\" and \"hyper-eosinophilic syndrome\" is **TEST CONDUCTED TO INVESTIGATE\nMEDICAL PROBLEM**.\nHere’s the breakdown:\n* **Work up:**\nThis refers to a series of tests and procedures done to diagnose a medical condition.\n* **Hyper-eosinophilic syndrome:** This is a medical condition characterized by an abnormally high\nnumber of eosinophils (a type of white blood cell) in the blood.\nThe sentence states that \"Work up was begun for the possible myeloproliferative/myelodysplastic\ndisorder associated with the hyper-eosinophilic syndrome.\" This clearly indicates that the work-up\nis being conducted to investigate the potential presence of the hyper-eosinophilic syndrome.\n**Why not other relations?**\n* **TrIP/TrWP/TrCP/TrAP/TrNAP:** The sentence doesn’t discuss any treatment.\n* **TeRP:**\nWhile the workup might reveal the syndrome, the focus is on the investigation, not just\nthe result.\n* **PIP:** The sentence doesn’t imply the syndrome is caused by another medical problem.\nD\nGold Label-induced CoT\nTask Instruction (not repeated here)\n#Following is the Gold Label-induced CoT augmented demonstration#\nContext: 5. Dulcolax 10 to 20 mg PR b.i.d. p.r.n. constipation.\nGiven the context, the relation between dulcolax and constipation is TREATMENT IS ADMINISTERED FOR\nMEDICAL PROBLEM. It is because:\nIn the sentence \"5. Dulcolax 10 to 20 mg PR b.i.d. p.r.n. constipation,\" the relationship between \"\nDulcolax\" and \"constipation\" is categorised as \"TREATMENT IS ADMINISTERED FOR MEDICAL PROBLEM\"\nbecause Dulcolax is a medication specifically used to treat constipation.\n1. **Dulcolax**: This is a brand name for a laxative that contains the active ingredient bisacodyl. It\nis designed to stimulate bowel movements and is commonly prescribed or recommended for\nindividuals suffering from constipation.\n2. **Constipation**: This is a medical condition characterised by infrequent bowel movements or\ndifficulty in passing stool. It can be uncomfortable and is considered a health issue that often\nrequires treatment.\n3. **Treatment Administration**: The notation \"10 to 20 mg PR b.i.d. p.r.n.\" indicates the dosage and\nfrequency of administering Dulcolax -- specifically, that it should be taken rectally (PR) twice a\nday (b.i.d.) as needed (p.r.n.) to manage the medical problem of constipation.\nTherefore, the statement illustrates that Dulcolax serves as a therapeutic agent aimed at resolving or\nalleviating the medical issue of constipation, clearly defining the relationship as one where a\ntreatment is provided for a specific medical problem.\n#Other Examples #\n#Test Input #\nContext: Peri-Colace , two capsules PO b.i.d.; Dulcolax , 10 mg. PR q.day , p.r.n. constipation.\nGiven the context, what is the relation between dulcolax and constipation?\nResponse:\nThe relation between Dulcolax and constipation is TREATMENT IS ADMINISTERED FOR MEDICAL PROBLEM. This\nis because Dulcolax is specifically used to treat constipation, as indicated by the context\nstating that it is administered as needed (p.r.n.) for this medical issue.\n26\n"}, {"page": 27, "text": "E\nOutput Format-considered CoT\nAppendix D. Output Format-considered CoI\nTask Instruction (not repeated here)\n#Following is the Output Format-considered augmented demonstration#\nContext: Hypertension was managed with beta blocker and ACE inhibitor and Integrilin\nwas continued post MI for 18 hours .\nGiven the context, what is the relation between beta blocker and hypertension?\nResponse: TREATMENT IS ADMINISTERED FOR MEDICAL PROBLEM. Because treatment [beta\nblocker] IS ADMINISTERED FOR problem [beta blocker].\n#Other Examples # #Test Input #\nContext: He remained in sinus bradycardia ( rate 50-60 ) and tolerated low dose beta blockade .\nGiven the context, the relation between low dose beta blockade and sinus bradycardia is\nResponse: **TREATMENT IS ADMINISTERED FOR MEDICAL PROBLEM**.\nBecause treatment [low dose beta blockade] IS ADMINISTERED FOR problem [sinus bradycardia].\nReferences\n[Artiﬁcial Analysis(2025)] Artiﬁcial Analysis. 2025. Artiﬁcial Analysis: AI Model & API Providers\nAnalysis. https://artificialanalysis.ai/. Accessed: October 24, 2025.\n[Ashok et al.(2023)] Ashok, Dhananjay et al. 2023. Promptner: Prompting for named entity recogni-\ntion.\n[Bowman et al.(2015)] Bowman, Samuel R. et al. 2015. A large annotated corpus for learning natu-\nral language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 632–642, Association for Computational Linguistics.\n[Brown et al.(2020)] Brown, Tom et al. 2020. Language models are few-shot learners. In Advances in\nNeural Information Processing Systems, volume 33, pages 1877–1901, Curran Associates, Inc.\n[Cui et al.(2023)] Cui, Hejie et al. 2023. A survey on knowledge graphs for healthcare: Resources,\napplication progress, and promise. In ICML 3rd Workshop on Interpretable Machine Learning in\nHealthcare (IMLH).\n[DeepSeek-AI, Liu et al.(2025)] DeepSeek-AI, Aixin Liu, et al. 2025. Deepseek-v3 technical report.\n[Detroja, Bhensdadia, and Bhatt(2023)] Detroja, Kartik, C.K. Bhensdadia, and Brijesh S. Bhatt. 2023.\nA survey on relation extraction. Intelligent Systems with Applications, 19:200244.\n[Devlin et al.(2019a)] Devlin, Jacob et al. 2019a. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. In North American Chapter of the Association for Computational\nLinguistics.\n[Devlin et al.(2019b)] Devlin, Jacob et al. 2019b. BERT: Pre-training of deep bidirectional transform-\ners for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 4171–4186, Association for Computational Linguistics.\n27\n"}, {"page": 28, "text": "[Fraile Navarro et al.(2023)] Fraile Navarro, David et al. 2023. Clinical named entity recognition and\nrelation extraction using natural language processing of medical free text: A systematic review.\nInternational Journal of Medical Informatics, 177:105122.\n[Gao, Yao, and Chen(2021)] Gao, Tianyu, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple\ncontrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing, pages 6894–6910, Association for Computational\nLinguistics, Online and Punta Cana, Dominican Republic.\n[Gemini Team, Anil et al.(2023)] Gemini Team, Rohan Anil, et al. 2023. Gemini: A Family of Highly\nCapable Multimodal Models. arXiv e-prints, page arXiv:2312.11805.\n[Guan et al.(2020)] Guan, Tongfeng et al. 2020. Cmeie: Construction and evaluation of chinese medical\ninformation extraction dataset. In Natural Language Processing and Chinese Computing, pages\n270–282, Springer International Publishing, Cham.\n[Hadsell et al.(2006)] Hadsell, R. et al. 2006. Dimensionality reduction by learning an invariant map-\nping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR’06), volume 2, pages 1735–1742.\n[Hendrickx et al.(2010)] Hendrickx, Iris et al. 2010. SemEval-2010 task 8: Multi-way classiﬁcation of\nsemantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on\nSemantic Evaluation, pages 33–38, Association for Computational Linguistics, Uppsala, Sweden.\n[Jimenez Gutierrez et al.(2022)] Jimenez Gutierrez, Bernal et al. 2022.\nThinking about GPT-3 in-\ncontext learning for biomedical IE? think again. In Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 4497–4512, Association for Computational Linguistics, Abu\nDhabi, United Arab Emirates.\n[Landolsi et al.(2023)] Landolsi, Mohamed Yassine et al. 2023. Information extraction from electronic\nmedical documents: state of the art and future research directions. Knowledge and Information\nSystems, 65(2):463–516.\n[Lee et al.(2019)] Lee, Jinhyuk et al. 2019. BioBERT: a pre-trained biomedical language representation\nmodel for biomedical text mining. Bioinformatics, 36(4):1234–1240.\n[Liu et al.(2022a)] Liu, Jiachang et al. 2022a.\nWhat makes good in-context examples for GPT-3?\nIn Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge\nExtraction and Integration for Deep Learning Architectures, pages 100–114, Association for Com-\nputational Linguistics, Dublin, Ireland and Online.\n[Liu et al.(2024)] Liu, Siyi et al. 2024. Unleashing the power of large language models in zero-shot rela-\ntion extraction via self-prompting. In Findings of the Association for Computational Linguistics:\nEMNLP 2024, pages 13147–13161, Association for Computational Linguistics, Miami, Florida,\nUSA.\n[Liu et al.(2022b)] Liu, Tianyu et al. 2022b. Autoregressive structured prediction with language models.\nIn Findings of the Association for Computational Linguistics: EMNLP 2022, pages 993–1009,\nAssociation for Computational Linguistics, Abu Dhabi, United Arab Emirates.\n[Liu et al.(2019)] Liu, Yinhan et al. 2019. Roberta: A robustly optimized bert pretraining approach.\n[Luan et al.(2018)] Luan, Yi et al. 2018. Multi-task identiﬁcation of entities, relations, and coreference\nfor scientiﬁc knowledge graph construction.\nIn Proceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing, pages 3219–3232, Association for Computational\nLinguistics, Brussels, Belgium.\n[Open Benchmarks(2023)] Open Benchmarks. 2023. Mmlu (massive multitask language understand-\ning) benchmark. Accessed: 2025-07-22.\n28\n"}, {"page": 29, "text": "[OpenAI(2024)] OpenAI. 2024. Gpt-4o mini: Advancing cost-eﬃcient intelligence. Accessed: 2025-07-\n22.\n[Patel et al.(2021)] Patel, Ruchi et al. 2021. Relation extraction between medical entities using deep\nlearning approach. Informatica, 45(3).\n[Roy et al.(2021)] Roy, Arpita et al. 2021.\nIncorporating medical knowledge in BERT for clinical\nrelation extraction.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5357–5366, Association for Computational Linguistics, Online and\nPunta Cana, Dominican Republic.\n[Segura-Bedmar et al.(2013)] Segura-Bedmar, Isabel et al. 2013. SemEval-2013 task 9 : Extraction\nof drug-drug interactions from biomedical texts (DDIExtraction 2013). In Second Joint Confer-\nence on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh\nInternational Workshop on Semantic Evaluation (SemEval 2013), pages 341–350, Association for\nComputational Linguistics, Atlanta, Georgia, USA.\n[Sinha et al.(2025)] Sinha, Neelabh et al. 2025. Are small language models ready to compete with large\nlanguage models for practical applications? In Proceedings of the 5th Workshop on Trustworthy\nNLP (TrustNLP 2025), pages 365–398, Association for Computational Linguistics, Albuquerque,\nNew Mexico.\n[TIGER-Lab(2024)] TIGER-Lab.\n2024.\nMMLU-Pro\nLeaderboard.\nhttps://huggingface.co/spaces/TIGER-Lab/MMLU-Pro. Accessed: October 24, 2025.\n[Türkmen et al.()] Türkmen, H. et al.\nBioberturk: Exploring turkish biomedical language model\ndevelopment strategies in low-resource setting. Journal of Healthcare Informatics Research, 7:433–\n446.\n[Uzuner et al.(2011)] Uzuner, Ö. et al. 2011. 2010 i2b2/VA challenge on concepts, assertions, and\nrelations in clinical text. Journal of the American Medical Informatics Association : JAMIA,\n18(5):552–556.\n[Wan et al.(2023)] Wan, Zhen et al. 2023. GPT-RE: In-context learning for relation extraction using\nlarge language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3534–3547, Association for Computational Linguistics, Singapore.\n[Wang et al.(2023a)] Wang, Shuhe et al. 2023a. Gpt-ner: Named entity recognition via large language\nmodels.\n[Wang et al.(2024)] Wang, Yubo et al. 2024. Mmlu-pro: A more robust and challenging multi-task\nlanguage understanding benchmark. arXiv preprint arXiv:2406.01574.\n[Wang et al.(2023b)] Wang, Yuqing et al. 2023b. Are large language models ready for healthcare? a\ncomparative study on clinical language understanding.\n[Wei et al.(2023)] Wei, Jason et al. 2023. Chain-of-thought prompting elicits reasoning in large lan-\nguage models.\n[Williams et al.(2018)] Williams, Adina et al. 2018. A broad-coverage challenge corpus for sentence\nunderstanding through inference.\nIn Proceedings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies\n(NAACL-HLT), volume 1, pages 1112–1122, Association for Computational Linguistics.\n[Yang et al.(2024)] Yang, Li et al. 2024. An empirical study of multimodal entity-based sentiment\nanalysis with ChatGPT: Improving in-context learning via entity-aware contrastive learning. In-\nformation Processing & Management, 61(4):103724.\n29\n"}, {"page": 30, "text": "[Zhao et al.(2024)] Zhao, Xiaoyan et al. 2024. A comprehensive survey on relation extraction: Recent\nadvances and new frontiers. ACM Comput. Surv., 56(11).\n[Zhong and Chen(2021)] Zhong, Zexuan and Danqi Chen. 2021. A frustratingly easy approach for\nentity and relation extraction.\nIn Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pages\n50–61, Association for Computational Linguistics, Online.\n30\n"}]}