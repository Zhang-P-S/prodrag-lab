{"doc_id": "arxiv:2601.03321", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.03321.pdf", "meta": {"doc_id": "arxiv:2601.03321", "source": "arxiv", "arxiv_id": "2601.03321", "title": "Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting", "authors": ["Kun Zhao", "Siyuan Dai", "Pan Wang", "Jifeng Song", "Hui Ji", "Chenghua Lin", "Liang Zhan", "Haoteng Tang"], "published": "2026-01-06T14:17:44Z", "updated": "2026-01-12T05:56:31Z", "summary": "Multimodal Large Language Models (MLLMs) have shown strong potential for radiology report generation, yet their clinical translation is hindered by architectural heterogeneity and the prevalence of factual hallucinations. Standard supervised fine-tuning often fails to strictly align linguistic outputs with visual evidence, while existing reinforcement learning approaches struggle with either prohibitive computational costs or limited exploration. To address these challenges, we propose a comprehensive framework for self-consistent radiology report generation. First, we conduct a systematic evaluation to identify optimal vision encoder and LLM backbone configurations for medical imaging. Building on this foundation, we introduce a novel \"Reason-then-Summarize\" architecture optimized via Group Relative Policy Optimization (GRPO). This framework restructures generation into two distinct components: a think block for detailed findings and an answer block for structured disease labels. By utilizing a multi-dimensional composite reward function, we explicitly penalize logical discrepancies between the generated narrative and the final diagnosis. Extensive experiments on the MIMIC-CXR benchmark demonstrate that our method achieves state-of-the-art performance in clinical efficacy metrics and significantly reduces hallucinations compared to strong supervised baselines.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.03321v2", "url_pdf": "https://arxiv.org/pdf/2601.03321.pdf", "meta_path": "data/raw/arxiv/meta/2601.03321.json", "sha256": "e9a75c140cedba8ad281ab2277c82f84cde38073d060a922b07ea12eb6237a3b", "status": "ok", "fetched_at": "2026-02-18T02:23:01.390124+00:00"}, "pages": [{"page": 1, "text": "Aligning Findings with Diagnosis: A Self-Consistent Reinforcement\nLearning Framework for Trustworthy Radiology Reporting\nKun Zhao1, Siyuan Dai1, Pan Wang1 , Jifeng Song1, Hui Ji1,\nChenghua Lin3*, Liang Zhan1*, Haoteng Tang2*,\n1 University of Pittsburgh\n2 University of Texas Rio Grande Valley\n3 The University of Manchester\n{kun.zhao, liang.zhan}@pitt.edu haoteng.tang@utrgv.edu\nchenghua.lin@manchester.ac.uk\nAbstract\nMultimodal Large Language Models (MLLMs)\nhave shown strong potential for Radiology\nReport Generation, yet their clinical transla-\ntion is hindered by architectural heterogeneity\nand the prevalence of factual hallucinations.\nStandard supervised fine-tuning often fails to\nstrictly align linguistic outputs with visual ev-\nidence, while existing Reinforcement Learn-\ning approaches struggle with either prohibitive\ncomputational costs or limited exploration. To\naddress these challenges, we propose a com-\nprehensive framework for self-consistent radi-\nology report generation. First, we conduct a\nsystematic evaluation to identify optimal vision\nencoder and LLM backbone configurations for\nmedical imaging. Building on this foundation,\nwe introduce a novel “Reason-then-Summarize”\narchitecture optimized via Group Relative Pol-\nicy Optimization (GRPO). This framework re-\nstructures generation into two distinct compo-\nnents: a <think> block for detailed findings\nand an <answer> block for structured disease\nlabels. By utilizing a multi-dimensional com-\nposite reward function, we explicitly penalize\nlogical discrepancies between the generated\nnarrative and the final diagnosis. Extensive\nexperiments on the MIMIC-CXR benchmark\ndemonstrate that our method achieves State-of-\nthe-Art performance in clinical efficacy metrics\nand significantly reduces hallucinations com-\npared to strong supervised baselines.\n1\nIntroduction\nRecent years have witnessed remarkable progress\nin Multimodal Large Language Models (MLLMs)\n(Alayrac et al., 2022; Bai et al., 2025; Li et al.,\n2023; Liu et al., 2023; Wang et al., 2025b). By\nintegrating a large language model (LLM) back-\nbone, a vision encoder, and a multimodal connector,\nMLLMs enable joint reasoning over visual and tex-\ntual inputs and have achieved strong performance\n*\nCorresponding authors.\nacross a range of vision–language tasks, includ-\ning image captioning (Rotstein et al., 2024; Chen\net al., 2024c), visual question answering (VQA)\n(Xu et al., 2024; Kuang et al., 2025), and medical\nimage understanding (Liu et al., 2025a; Wang and\nZhang, 2024; Zhao et al., 2024, 2025; Tang et al.,\n2022; Gu et al., 2025). Among these applications,\nRadiology Report Generation (RRG) has emerged\nas a particularly challenging and important testbed\nfor MLLMs, as it requires accurate visual ground-\ning and internal consistency between textual find-\nings and diagnostic conclusions (Yi et al., 2025).\nRRG aims to automatically generate free-text clini-\ncal findings and diagnostic impressions from radio-\nlogical images (e.g., chest X-rays), with the poten-\ntial to substantially reduce radiologists’ workload\nwhile maintaining diagnostic quality.\nDespite substantial advances driven by MLLMs\n(Chen et al., 2024c; Hein et al., 2025; Sellergren\net al., 2025a; Zambrano Chaves et al., 2025), the\nclinical translation of automated RRG remains im-\npeded by two fundamental barriers: architectural\ndesign choices and the reliability of generation.\nFirst, the optimal integration between vision en-\ncoders and LLM backbones for radiological rea-\nsoning remains insufficiently characterized. While\ngeneral-domain studies have underscored the im-\nportance of strong visual representations (Tong\net al., 2024), systematic investigations of encoder-\nLLM pairings in medical imaging contexts are still\nlacking. As a result, existing MLLM-based RRG\nsystems rely on heuristic architectural configura-\ntions, potentially limiting their diagnostic acuity\nand robustness.\nSecond, even with carefully selected architec-\ntures, standard Supervised Fine-Tuning (SFT) fre-\nquently leads to factual hallucination. Under this\nparadigm, model outputs are often dominated by\nlinguistic priors rather than image-grounded evi-\ndence, resulting in clinically plausible yet incor-\nrect assertions, such as fabricating pleural effu-\n1\narXiv:2601.03321v2  [cs.LG]  12 Jan 2026\n"}, {"page": 2, "text": "sions (Liu et al., 2025b; Chen et al., 2024a; Wang\net al., 2025a). To align generation with clinical\nground truth, recent work has explored Reinforce-\nment Learning (RL) as an alternative optimization\nparadigm. Unlike SFT, which is tightly coupled\nto surface-level phrasing, RL enables direct opti-\nmization of clinically motivated efficacy metrics,\nthereby emphasizing image-grounded evidence and\nessential diagnostic content rather than exact lexi-\ncal matching.\nSelecting an efficient reinforcement learning\n(RL) algorithm remains challenging. Proximal Pol-\nicy Optimization (PPO) enables direct optimiza-\ntion of clinical metrics (Zhou et al., 2024; Yang\net al., 2025a) but incurs substantial computational\noverhead due to its reliance on a large critic net-\nwork. In contrast, Direct Preference Optimization\n(DPO) is more efficient but limited by its offline\nnature, restricting active exploration and error cor-\nrection (Liu et al., 2025b; Hein et al., 2025). Group\nRelative Policy Optimization (GRPO) (Shao et al.,\n2024) provides a promising compromise by sup-\nporting online exploration without a critic network.\nHowever, adapting GRPO to the medical domain\nremains non-trivial: prior applications exhibit logi-\ncal inconsistencies between intermediate reasoning\nand final conclusions (Lai et al., 2025; Pan et al.,\n2025), and radiology reports do not naturally con-\nform to the explicit “thought–answer” structure\nassumed by standard reasoning frameworks.\nTo address these challenges, we propose a com-\nprehensive study and a novel framework for the\nRRG task. We first conduct a systematic investi-\ngation to identify the optimal combination of vi-\nsion encoder and LLM backbone for RRG. Build-\ning upon the best-performing configuration, we\nintroduce a novel RL framework for self-consistent\nRRG. Unlike traditional approaches that treat re-\nport generation and disease classification as sep-\narate tasks, our framework integrates them into a\nrigorous “Reason-then-Summarize” workflow that\nreflects the clinical reporting process. Specifically,\nthe model is guided to structure outputs in two dis-\ntinct components: a <think><\\think> block that\ngenerates detailed free-text clinical findings, fol-\nlowed by an <answer><\\answer> block that sum-\nmarizes these findings into structured, machine-\nreadable JSON disease labels.\nTo leverage this structure, we design a multi-\ndimensional composite reward function optimized\nvia RL. Beyond surface-level lexical overlap,\nthis function explicitly enforces internal consis-\ntency and clinical accuracy through: (1) a con-\nsistency reward (Rconsistency) that penalizes dis-\ncrepancies between generated findings in <think>\nand the corresponding structured diagnostic out-\nputs in <answer>; (2) diagnostic accuracy rewards\n(Raccuracy) that align both free-text reports and ex-\ntracted labels with ground-truth clinical annota-\ntions; and (3) format & semantic rewards that guar-\nantee adherence to the structured output protocol\nand semantic fidelity to reference reports. Opti-\nmizing this holistic objective enables the model\nto generate reports that are not only linguistically\nfluent but also logically consistent and clinically\nverifiable. Our main contributions are summarized\nas follows:\n• We systematically study the impact of various\nvision encoder and LLM pairings on RRG\nperformance, showing that domain-adapted\nvision encoders combined with reasoning-\ncapable LLMs substantially enhance report\nquality.\n• We propose a “Reason-then-Summarize” ar-\nchitecture that redefines RRG as a structured\ntwo-stage generation task, enabling explicit\nalignment between textual evidence and struc-\ntured diagnostic outputs.\n• We introduce a self-consistent RL Framework\nwith a composite reward function that explic-\nitly optimizes the internal consistency of the\nmodel’s reasoning chain and clinical correct-\nness, significantly reducing hallucinations.\n• Extensive experiments on the MIMIC-CXR\n(Johnson et al., 2024) benchmark demonstrate\nthat our method achieves State-of-the-Art\n(SOTA) performance on clinical efficacy met-\nrics (e.g., CheXbert F1) compared to strong\nsupervised baselines.\n2\nRelated Work\n2.1\nMedical MLLMs\nThe proliferation of large-scale MLLMs has cat-\nalyzed the development of domain-specific health-\ncare adaptations, with systems such as LLaVA-Med\n(Li et al., 2023) and HuatuoGPT-Vision (Chen et al.,\n2024b) demonstrating remarkable proficiency in\nradiology report generation and diagnostic tasks.\nDespite these advances, the predominant paradigm\nfor adapting these architectures to the medical do-\nmain remains SFT on final-answer labels (Zhang\net al., 2023; Xu et al., 2024; Liu et al., 2025c). This\n2\n"}, {"page": 3, "text": "paradigm relies heavily on massive, high-quality\nimage-text corpora—ranging from 660K (Li et al.,\n2023) to 32M (Wu et al., 2025) samples— which\nare widely recognized as costly to curate and sub-\nject to label noise and privacy constraints in medi-\ncal settings. Moreover, prior studies indicate that\noutcome-level supervision provides limited visibil-\nity into intermediate model behavior, raising con-\ncerns regarding interpretability and clinician trust\n(Liu et al., 2025b; Chen et al., 2024a; Wang et al.,\n2025a). Consistent with these observations, SFT-\nbased models have been reported to exhibit factual\nhallucinations when generated reports lack strong\nvisual grounding.\n2.2\nRL for MLLMs\nTo mitigate the issue of hallucination in MLLMs,\nRL has been increasingly adopted to align model\nbehavior with human preferences. Within this land-\nscape, GRPO has demonstrated favorable architec-\ntural compatibility with LLMs compared to conven-\ntional approaches such as PPO and DPO. Recent\nstudies have begun to explore GRPO in general\nmultimodal settings (Shen et al., 2025; Yang et al.,\n2025b; Zheng et al., 2025); however, its extension\nto the medical domain remains challenging. For\nexample, Lai et al. (2025) and Pan et al. (2025)\napply GRPO to medical VQA tasks, but report crit-\nical logical inconsistencies between the model’s\nintermediate reasoning traces and its final answers.\nMoreover, applying GRPO to RRG is further com-\nplicated by the intrinsic structure of clinical nar-\nratives, which do not naturally exhibit the explicit\n“thought–answer” decomposition assumed by stan-\ndard reasoning-oriented frameworks.\n3\nMethodology\nIn this section, we present a comprehensive frame-\nwork for self-consistent radiology report genera-\ntion. Our approach is grounded in a two-phases\nmethodology: (i) a systematic study to identify the\noptimal vision-language backbone (Sec. 3.1), and\n(ii) a novel self-consistent reinforcement learning\nframework that enforces logical rigor and clinical\naccuracy (Sec. 3.2). An overview of our proposed\nframework is shown in Figure 1.\n3.1\nPhase I: Systematic Architectural\nOptimization\nThe performance of an MLLM for RRG critically\ndepends on both visual representation quality and\nthe reasoning capacity of the language backbone.\nTo provide a strong foundation for our RL frame-\nwork, we conduct a systematic study to disentangle\nthe respective contributions of the vision encoder\nand the LLM. Specifically, we define the architec-\ntural search space as S = V × L, where V and L\ndenote the candidate vision encoder set and LLM\nbackbone set, respectively. For the vision encoders\n(V), we evaluate diverse inductive biases and pre-\ntraining paradigms, including CLIP, SigLIP, Xray-\nCLIP, and Dinov2-Xray-pretrained ViTs. This de-\nsign enables a direct comparison between general-\ndomain scaling and domain-specific medical pre-\ntraining. For the language models (L), we bench-\nmark both general-purpose reasoning models (e.g.,\nLlama-3-8B) and medically adapted models (e.g.,\nMed-Llama). Across all configurations, we adopt\na unified connector strategy based on a multi-layer\nperceptron (MLP) projection to interface the vision\nencoder with the LLM.\nWe evaluate all pairs (v, l) ∈S on the MIMIC-\nCXR dataset using a holistic metric suite. The\noptimal pair (v∗, l∗) is identified as the backbone\npolicy πθ for the subsequent RL optimization.\n3.2\nPhase II: Self-Consistent Reinforcement\nLearning Framework\nWith the optimal backbone fixed, we focus on im-\nproving hallucination control and internal consis-\ntency in report generation. Rather than modeling\nRRG as a pattern-matching problem, we formulate\nit as a structured reasoning process.\n3.2.1\n“Reason-then-Summarize” Architecture\nLet D = {(x(i), y(i)\ntext, y(i)\nlabel)}N\ni=1 denote a dataset\nof radiology studies, where x(i) denotes a chest\nX-ray image, y(i)\ntext is the corresponding free-text\nreport (i.e., the findings section), and y(i)\nlabel rep-\nresents the associated structured diagnostic label\nvector (e.g., the fourteen CheXpert pathologies).\nWe aim to learn a policy πθ, parameterized by an\nMLLM, that generates a sequence of tokens Y .\nTo align with the hierarchical formulation of RL,\nwe conceptualize the generation of the “findings”\nsection as the model’s reasoning process and the\ndiagnostic labels as the final answer. Accordingly,\nthe MLLM output is structured into two compo-\nnents: (i) <think><\\think> block containing a\ndetailed, free-text narrative of radiological observa-\ntions (e.g., “Opacity is observed in the right lower\nlobe.”). (ii) <answer><\\answer> block including\na structured JSON object derived solely from the\npreceding narrative, mapping the findings to dis-\n3\n"}, {"page": 4, "text": "Vision Tower\nFactory\nLLM\nFactory\nBest Med-MLLM\nChest X-ray\nPrompt\nInput\nCompletions\nRewards\nObjective Loss\nPhase 1: Backbone Selection\nSFT with\n<think>\n<answer>\nstructure\nSFT model\nPolicy\nReference\nKL\nPhase 2: Self-Consistent RL Framework\n<image>\nDescribe the\ngiven chest x-ray\nimage in detail.\nTrainable\nFrozen\nMLP\nMed-MLLM\nBackbone\nRconsistency\n(Self-Consistency)\nRthink\n(Text Acc)\nRanswer\n(Label Acc)\nRsemantic\n(BERTScore)\nRformat\n(<think><answer>)\nAdvantages\nComposite Reward\n-\nOutput\nThe heart is normal in size. The \nmediastinum is unremarkable ...\nCLIP\nSigLIP\nXrayCLIP\n...\nLLLaMA\nQwen\nMed-LLaMA\n...\nFigure 1: Overview of the proposed framework. Phase 1 performs a systematic exploration to identify the optimal\nvision–language backbone, while Phase 2 introduces a self-consistent reinforcement learning framework to enforce\nlogical rigor and clinical accuracy.\ncrete clinical labels (e.g., “Pneumonia”: 1.0, “No\nFinding”: 0.0).\nUnlike conventional approaches that directly\nmap x →ytext, we formulate the generation pro-\ncess as a two-step causal chain:\nP(Y |x) = P(Ythink, Yanswer|x)\n= P(Ythink|x)\n|\n{z\n}\nReasoning\n· P(Yanswer|x, Ythink)\n|\n{z\n}\nSummarization\n, (1)\nwhere Ythink denotes the descriptive findings (rea-\nsoning) and Yanswer indicates the structured diagnos-\ntic labels (answer) conditioned on those findings.\n3.2.2\nComposite Reward Function Design\nWe design a composite reward function consisting\nof five complementary components to jointly en-\nforce internal consistency, clinical accuracy, and\nformat compliance during reinforcement learning.\n(i) Consistency Reward (R1 = Rconsistency) mea-\nsures the consistency between the model’s gener-\nated narrative in <think> and its diagnostic conclu-\nsion in <answer> by using a “Self-Check” mecha-\nnism, which constitutes our core innovation:\nR1 = CFS(E( ˆYthink), ˆYanswer)\n(2)\nHere, E denotes the CheXbert labeler used to\nmap free-text reports to structured pathology la-\nbels. To quantify diagnostic alignment, we define\nthe Clinical F1 Score (CFS) as the arithmetic mean\nof weighted rewards across pathology categories.\nGiven a set of diagnostic labels C (e.g., |C| = 14\npathologies), CFS is defined as\nCFS( ˆY , Y ) = 1\n|C|\nX\nc∈C\nS(ˆyc, yc)\n(3)\nwhere S is an asymmetric scoring function. Specif-\nically, positive ground-truth cases receive a maxi-\nmal reward of +2.0 to emphasize sensitivity, neg-\native cases receive +1.0 to emphasize specificity,\nand uncertain labels are assigned a neutral reward\nof +0.5. Definitive diagnostic errors (false pos-\nitives or false negatives) incur a mild penalty of\n−0.3. The normalization ensures that the resulting\nscore remains stable with respect to the number of\nevaluated pathology categories.\n(ii) Reasoning Accuracy Reward (R2 = Rthink-acc)\nmeasures the diagnostic accuracy of the free-text\nreport against ground-truth labels by:\nR2 = CFS(E( ˆYthink), Yanswer).\n(4)\n(iii)\nDiagnostic\nAccuracy\nReward\n(R3\n=\nRanswer-acc) measures the accuracy of the final struc-\ntured output against ground truth labels by:\nR3 = CFS( ˆYanswer, Yanswer).\n(5)\n(iv) Semantic Fidelity Reward (R4 = Rsemantic) en-\nsures that the linguistic quality and semantic mean-\ning of the generated report align with the radiol-\nogist’s reference report. We use BERTScore to\ncapture semantic similarity beyond n-gram over-\nlap.\nR4 = BERTScore( ˆYthink, Ythink).\n(6)\n(v) Format Compliance Reward (R5 = Rformat)\nenforces structural constraints using a binary cu-\nmulative function. The reward is defined as:\nR5 = 0.5 · Itags + 0.5 · Ijson,\n(7)\nwhere Itags returns 1 if both <think> and\n<answer> delimiters are present, and Ijson returns\n1 if the content within <answer> is a valid JSON\n4\n"}, {"page": 5, "text": "object. This yields a maximum reward of 1.0 only\nfor fully compliant outputs.\nThe final reward is computed as a weighted sum\nof R1–R5:\nRtotal =\n5\nX\ni=1\nλiRi,\n(8)\nwhere λ1–λ5 are hyperparameters controlling the\nrelative importance of each reward component.\n3.2.3\nLoss Function\nThe training procedure consists of three steps: (i)\nSampling. For each input v, we sample G candi-\ndate outputs {oi}G\ni=1 from the reference policy πθold.\n(ii) Advantage Estimation. We compute a relative\nadvantage Ai for each candidate by normalizing its\nreward ri with respect to the group statistics:\nAi = ri −mean({r1, . . . , rG})\nstd({r1, . . . , rG}) + ϵ .\n(9)\nThis formulation encourages trajectories that out-\nperform the group average. (iii) Policy Optimiza-\ntion. The policy πθ is updated by maximizing the\nGRPO objective JGRPO, which employs a clipped\nsurrogate loss to regularize updates and ensure\ntraining stability:\nJGRPO(θ) = Ev∼P(V )[E{oi}G\ni=1∼πθold(·|v)[\n1\nG\nG\nX\ni=1\n(min(rratio\ni\nAi, clip(rratio\ni\n, 1 ± ϵ)Ai)\n−β DKL(πθ ∥πref))]],\n(10)\nwhere\nrratio\ni\n= πθ(oi | v)\nπθold(oi | v).\n(11)\nAn additional Kullback–Leibler (KL) term\nDKL(πθ∥πref) penalizes divergence from the ref-\nerence policy πref. The coefficients ϵ, β ∈R≥0\ncontrol the regularization strengths.\n4\nExperimental Setup\n4.1\nDatasets and Protocols\nWe evaluate our method on two widely used\nradiology report generation datasets.\nMIMIC-\nCXR (Johnson et al., 2024): The largest publicly\navailable chest X-ray dataset, comprising 377,110\nimages and 227,835 radiology reports. We follow\nthe official data split (222,758 training, 1,808 val-\nidation, and 3,269 testing samples). The <think>\ncomponent is extracted from the \"Findings\" section\nof each report, while the <answer> component is\nderived by applying the CheXbert labeler (Smit\net al., 2020) to extract 14 structured disease la-\nbels, which are formatted as JSON outputs. IU\nX-Ray (Demner-Fushman et al., 2015): A smaller\ndataset collected by Indiana University, containing\n7,470 images and 3,955 radiology reports.\n4.2\nEvaluation Metrics\nWe evaluate model performance along both linguis-\ntic quality and clinical diagnostic accuracy using\na multi-dimensional evaluation framework com-\nprising three categories: natural language gener-\nation (NLG) metrics, clinical efficacy (CE) met-\nrics, and LLM-based evaluation measures. Lin-\nguistic performance is assessed using BLEU (Pap-\nineni et al., 2002), METEOR (Banerjee and Lavie,\n2005), ROUGE-L (Lin, 2004), and BERTScore\n(Zhang et al., 2019). Clinical alignment is eval-\nuated with RadGraph (Jain et al., 2021), Rad-\nCliQ (Yu et al., 2022), and F1 scores, comple-\nmented by the GREEN metric (Ostmeier et al.,\n2024) for LLM-based semantic assessment.\nTo particularly measure the internal consis-\ntency of the “Reason–then–Summarize” paradigm,\nwe introduce a novel Self-Consistency Score\n(SCS), which quantifies the agreement between\nthe model’s intermediate reasoning trace (<think>)\nand its final diagnostic summary (<answer>):\nSCS = F1\n\u0010\nE( ˆYthink), ˆYanswer\n\u0011\n(12)\nA higher SCS indicates stronger agreement be-\ntween the generated descriptive findings and the\ncorresponding diagnostic outputs, reflecting im-\nproved internal consistency in the model’s report\ngeneration process.\n4.3\nBaselines\nWe benchmark our framework against comprehen-\nsive SOTA baselines, grouped into two categories:\nSpecialized Medical MLLMs: We evaluate\nmultiple domain-adapted architectures, including\nHuatuoGPT-Vision (Chen et al., 2024b), CheXa-\ngent (Chen et al., 2024c), LLaVA-Rad (Chaves\net al., 2024), MedGemma (Sellergren et al., 2025b).\nThese models are extensively pre-trained on large-\nscale biomedical corpora and constitute the cur-\nrent standard for RRG tasks, serving as primary\nbaselines for evaluating clinical alignment between\ngenerated narratives and ground-truth annotations.\nFinetuned General-Domain MLLMs: We also\nbenchmark finetuned general-domain models, in-\ncluding LLaVA and Qwen-2.5/3-VL-Instruct. All\nmodels are finetuned on the MIMIC-CXR and\n5\n"}, {"page": 6, "text": "LLM\nViT\nB-1\nB-2\nB-3\nB-4\nMeteor\nR-1\nR-2\nR-L\nBERTScore\nRadGraph\nRadCliQ-v1(↓)\nRadCliQ-v2(↓)\nGREEN\nLlama\nCLIP\n0.423\n0.291\n0.212\n0.149\n0.428\n0.447\n0.206\n0.422\n0.548\n0.311\n2.511\n0.585\n0.607\nDINOv2\n0.425\n0.289\n0.207\n0.141\n0.422\n0.448\n0.200\n0.421\n0.546\n0.315\n2.493\n0.588\n0.610\nDINOv2-Xray\n0.429\n0.296\n0.214\n0.148\n0.429\n0.453\n0.209\n0.428\n0.553\n0.321\n2.472\n0.570\n0.622\nXrayCLIP\n0.438\n0.307\n0.227\n0.164\n0.438\n0.466\n0.223\n0.441\n0.565\n0.334\n2.392\n0.531\n0.623\nSigLIP\n0.420\n0.289\n0.211\n0.149\n0.424\n0.448\n0.207\n0.423\n0.547\n0.309\n2.555\n0.623\n0.595\nQwen2\nCLIP\n0.417\n0.286\n0.206\n0.143\n0.424\n0.442\n0.202\n0.418\n0.540\n0.300\n2.582\n0.642\n0.596\nDINOv2\n0.426\n0.294\n0.214\n0.149\n0.429\n0.451\n0.209\n0.427\n0.552\n0.321\n2.510\n0.607\n0.610\nDINOv2-Xray\n0.434\n0.302\n0.221\n0.156\n0.434\n0.461\n0.215\n0.435\n0.562\n0.333\n2.458\n0.556\n0.630\nXrayCLIP\n0.437\n0.304\n0.223\n0.158\n0.440\n0.462\n0.217\n0.436\n0.562\n0.331\n2.410\n0.531\n0.630\nSigLIP\n0.426\n0.293\n0.212\n0.147\n0.427\n0.449\n0.205\n0.423\n0.547\n0.313\n2.508\n0.592\n0.605\nMed-Llama\nCLIP\n0.440\n0.300\n0.216\n0.152\n0.439\n0.451\n0.204\n0.425\n0.547\n0.309\n2.566\n0.628\n0.587\nDINOv2\n0.441\n0.300\n0.216\n0.151\n0.440\n0.450\n0.203\n0.423\n0.547\n0.311\n2.553\n0.613\n0.592\nDINOv2-Xray\n0.447\n0.309\n0.225\n0.160\n0.448\n0.458\n0.211\n0.432\n0.556\n0.315\n2.462\n0.568\n0.608\nXrayCLIP\n0.452\n0.312\n0.228\n0.162\n0.453\n0.464\n0.215\n0.440\n0.561\n0.318\n2.426\n0.550\n0.608\nSigLIP\n0.433\n0.296\n0.215\n0.153\n0.440\n0.444\n0.203\n0.420\n0.542\n0.300\n2.586\n0.647\n0.578\nTable 1: Overall performance on NLP and clinical efficacy (CE) metrics for different combinations of LLM\nbackbones and vision encoders on radiology report generation.\nIU-Xray datasets using a unified multi-task train-\ning objective that requires generating free-text re-\nports, structured JSON outputs, and explicit “think-\nanswer” reasoning chains to align with our pro-\nposed paradigm.\n4.4\nImplementation Details\nWe utilize TinyLLaVA system for two-phase train-\ning and the Transformer Reinforcement Learning\n(TRL) library to implement the GRPO algorithm.\nTo ensure rigorous comparisons in Phase 2, we do\nnot reuse model parameters finetuned in Phase 1;\ninstead, models are re-initialized from the same\nbase checkpoints pre-trained on standard LLaVA\ndatasets. We design a two-step optimization strat-\negy in Phase 2. (i) Cold Start: Models are first su-\npervised fine-tuned for 2 epochs using the AdamW\noptimizer with a learning rate of η = 2 × 10−5. (ii)\nPolicy Optimization: The model is subsequently\noptimized with the proposed composite reward\nfunction, using weighting coefficients λ1 = 0.2,\nλ2 = 0.5, λ3 = 1.0, λ4 = 0.3, and λ5 = 0.5.\nThe KL-divergence regularization coefficient is set\nto β = 0.03. All experiments are performed on\nNVIDIA A100 GPUs with 80GB of memory.\n5\nExperimental Results\n5.1\nComparative Analysis of Vision-Language\nBackbones\nIn Phase 1, we investigate optimal encoder–LLM\npairings by finetuning MLLMs for standard ra-\ndiology report generation, without enforcing the\n“Reason-then-Summarize” structure. Table 1 re-\nports a comparative evaluation of multiple vision–\nlanguage configurations across both NLP and CE\nmetrics. The results reveal a clear performance\ndivergence across metric categories. Llama paired\nwith XrayCLIP achieves the strongest performance\non traditional linguistic metrics, including BLEU-4,\nROUGE, and BERTScore. In contrast, clinical effi-\ncacy metrics favor different architectural choices:\nDINOv2-Xray combined with Qwen2 yields the\nbest GREEN score, while XrayCLIP-based mod-\nels consistently perform well on RadGraph F1 and\nRadCliQ. This discrepancy highlights a trade-off\nbetween surface-level lexical similarity and clin-\nical validity, indicating that high n-gram overlap\ndoes not necessarily imply improved diagnostic\ncorrectness.\nOur results also indicate that the vision encoder\nemerges as the dominant factor influencing perfor-\nmance. Domain-specific encoders such as Xray-\nCLIP and DINOv2-Xray consistently outperform\ngeneral-purpose counterparts, a trend most clearly\nreflected by the GREEN metric. Notably, com-\nbining domain-pretrained vision encoders with\ndomain-specific LLMs does not always lead to fur-\nther gains, suggesting diminishing complementar-\nity between highly specialized components. Con-\nsidering both linguistic quality and clinical align-\nment, we select Llama + XrayCLIP as the back-\nbone for Phase 2.\n5.2\nComparative Analysis of Diagnostic Label\nPrediction\nWe evaluate the diagnostic label accuracy of the\nproposed method by comparing it against two base-\nline categories summarized in Table 2: Report-only\nmodels, which derive labels from generated narra-\ntive reports, and Answer-only models, which per-\nform direct visual classification. On the MIMIC-\nCXR benchmark, our method consistently outper-\nforms all baselines across evaluation metrics. In\nparticular, it achieves a Macro-F1 of 37.36 and a\nMicro-F1 of 57.84, surpassing the strongest direct\nclassification baseline (LLaVA-Next-Mistral) by a\nsubstantial margin. These results indicate that the\nproposed approach more effectively captures fine-\ngrained pathological information than conventional\nvision–language baselines.\n6\n"}, {"page": 7, "text": "MIMIC-CXR\nIU-Xray\nMacro-F1\nMicro-F1\nMacro-F1\nMicro-F1\nReport-only\nHuatuoGPT-Vision\n22.22\n35.32\n10.19\n18.53\nChexagent\n12.75\n18.18\n13.64\n23.49\nLLaVA-Rad\n31.54\n47.53\n21.31\n37.90\nMedGemma\n33.47\n50.41\n21.95\n37.56\nLlava-1.5 (SFT)\n23.46\n40.98\n12.29\n32.24\nLlava-Next (Mistral)\n26.28\n45.64\n15.51\n31.76\nLlava-Next (Vicuna)\n22.99\n42.12\n15.04\n39.08\nQwen-3-VL-8B\n23.73\n42.91\n16.04\n36.57\nQwen-2.5-VL-8B\n20.71\n39.53\n13.05\n25.53\nAnswer-only\nLlava-1.5 (SFT)\n29.23\n50.34\n13.43\n35.83\nLlava-Next (Mistral)\n33.21\n55.11\n15.43\n35.71\nLlava-Next (Vicuna)\n30.81\n51.89\n16.06\n39.35\nQwen-2.5-VL-7B\n29.46\n49.72\n14.43\n27.55\nQwen-3-VL-8B\n32.00\n52.68\n16.12\n30.54\nOurs\n37.36\n57.84\n16.24\n40.04\nTable 2: Diagnostic classification performance compar-\nison between our proposed method and two baseline\ncategories: Report-only models (labels derived from\ngenerated reports) and Answer-only models (direct vi-\nsual classification), evaluated on the MIMIC-CXR and\nIU-Xray datasets. We report Macro-F1 and Micro-F1\nscores (%) to assess label prediction accuracy. Best re-\nsults are highlighted in bold.\nSimilar trends are observed on the smaller IU-\nXray dataset, with dataset-specific characteristics.\nWhile MedGemma attains competitive Macro-F1\nscores in this limited-data setting, our method main-\ntains a clear advantage in Micro-F1 (40.04), reflect-\ning improved instance-level diagnostic accuracy.\nMoreover, the larger performance gap observed\non the MIMIC-CXR benchmark suggests superior\nscalability as dataset size increases. Overall, these\nresults demonstrate that the proposed “Reason-\nthen-Summarize” paradigm produces diagnostic\nlabels that outperform those derived from both tra-\nditional report-based pipelines and direct visual\nclassification models.\n5.3\nEffectiveness of Consistency-Aware\nReinforcement Learning\nIn this section, we evaluate the effectiveness of\nconsistency-aware reinforcement learning from\nthree complementary perspectives. We first exam-\nine whether our RL-based method improves clin-\nical efficacy over SFT methods under the same\nstructured think–answer generation setting. We\nthen analyze whether the explicit consistency re-\nward (Rconsistency) effectively reduce factual halluci-\nnations and logical conflicts between the generated\nnarrative and the diagnostic labels. Finally, we con-\nduct an ablation study to explain the contribution\nof each reward term in the composite objective.\n(i) Clinical Efficacy. We begin by comparing re-\ninforcement learning with supervised fine-tuning\nunder an identical output format. While the SFT\nbaseline attains slightly higher surface-level flu-\nency (e.g., BLEU-4: 0.130 vs. 0.112), this advan-\ntage does not translate into improved clinical accu-\nracy. In contrast, optimizing directly for diagnostic\nobjectives yields consistent gains across clinically\nmeaningful metrics. Specifically, the full RL model\nimproves the Report-Level Macro-F1 from 36.40%\nto 37.01% and the Answer-Level Macro-F1 from\n35.71% to 37.36%. These results suggest that ex-\nplicit alignment with diagnostic correctness, rather\nthan imitation of reference phrasing, is the primary\ndriver of clinical efficacy.\n(ii) Consistency and Hallucination Reduction. We\nnext investigate the role of the consistency reward\nRconsistency. Removing this term leads to a substan-\ntial degradation in self-consistency, with the SCS\nMacro-F1 dropping from 92.16% to 75.68%. This\ncollapse indicates that, in the absence of explicit\npenalties, the model frequently produces diagnos-\ntic labels that contradict its own generated findings.\nBy contrast, the full model maintains consistently\nhigh agreement between reasoning and diagnosis\n(SCS Micro-F1 exceeding 97%), demonstrating\nthat Rconsistency is essential for suppressing factual\nhallucinations and enforcing internal coherence be-\ntween narrative and labels.\n(iii) Component-Wise Ablation Analysis. Finally,\nwe analyze how individual reward components\nshape model behavior. As expected, removing R1\nresults in the most severe degradation in reliability\nand consistency. However, enforcing consistency\nalone is insufficient for high-quality report gener-\nation. When the reasoning and semantic rewards\n(R2 and R4) are excluded, the model produces out-\nputs that are internally consistent but descriptively\nimpoverished, yielding the lowest semantic fidelity\n(BERTScore: 0.554). This behavior suggests a\ntendency toward conservative yet uninformative re-\nports. Overall, the full RL configuration achieves\nthe best balance by combining strict logical con-\nstraints (R1, R3) with rewards that encourage in-\nformative and meaningful descriptions (R2, R4).\n5.4\nQualitative Analysis\nFigure 2 provides a qualitative comparison illustrat-\ning the behavioral differences between the SFT\nbaseline and our reinforcement learning frame-\nwork. The SFT model exhibits notable halluci-\nnations, including the incorrect description of a\n“right-sided central venous catheter” that is absent\nin the ground truth. It also demonstrates internal\ninconsistencies, where the generated narrative con-\n7\n"}, {"page": 8, "text": "Report-Level\nAnswer-Level\nSCS\nNLG Metrics\nClinical Efficacy\nB-4\nMTR\nRG-L\nBERTScore\nMacro-F1\nMicro-F1\nMacro-F1\nMicro-F1\nMacro-F1\nMicro-F1\nLlava-1.5\n0.113\n0.356\n0.375\n0.614\n24.78\n44.34\n25.57\n45.51\n90.05\n94.93\nLlava-Next(vicuna)\n0.116\n0.360\n0.378\n0.617\n27.30\n46.74\n27.86\n47.72\n91.14\n94.92\nLlava-Next(mistral)\n0.117\n0.365\n0.382\n0.625\n31.46\n50.39\n32.38\n51.21\n90.69\n94.38\nQwen-2.5-VL-7B\n0.112\n0.358\n0.375\n0.617\n27.72\n46.77\n27.18\n45.86\n89.76\n95.08\nQwen-3-VL-8B\n0.118\n0.368\n0.383\n0.626\n30.97\n40.82\n31.48\n50.84\n91.29\n94.72\nOurs(SFT)\n0.130\n0.375\n0.401\n0.627\n36.40\n55.70\n35.71\n56.63\n89.06\n95.83\nOurs(RL (full))\n0.112\n0.363\n0.376\n0.631\n37.01\n56.97\n37.36\n57.84\n92.16\n97.39\nOurs(RL (R3 + R5))\n0.108\n0.307\n0.375\n0.602\n34.75\n53.31\n34.09\n53.57\n89.45\n97.02\nOurs(RL (w/o R3))\n0.091\n0.268\n0.302\n0.544\n34.03\n52.18\n33.23\n52.18\n89.30\n96.12\nOurs(RL (w/o R1))\n0.082\n0.242\n0.298\n0.509\n27.12\n45.96\n27.42\n43.43\n75.68\n88.62\nOurs(RL (w/o R2 + R4))\n0.104\n0.282\n0.328\n0.554\n34.96\n53.94\n35.03\n54.45\n92.10\n97.67\nTable 3: Main results on radiology report generation and clinical efficacy. We compare the proposed SFT and RL\nstages with multiple MLLM baselines across NLG metrics, CE metrics, and the Self-Consistency Score (SCS),\nwhich evaluates alignment between generated reports and diagnostic labels. Incorporating the explicit consistency\nreward (Rconsistency) in Ours (RL (full)) leads to improved clinical accuracy and consistency.\nChest-Xray\nSFT Generated Report\nThe chest x-ray shows a \nright-sided central venous \ncatheter terminating in the \nmid SVC, and a right-sided \nchest tube in an unchanged \nposition. The left chest tube \nis unchanged in position, and \nthere is no evidence of \npneumothorax. \nRL Generated Report\nThe chest x-ray shows a \nright-sided chest tube in \nappropriate position. There \nis a small residual \npneumothorax on the right, \nbut no evidence of tension. \nBibasilar atelectasis is \npresent, but there is no \npleural effusion. The heart \nand mediastinal contour are \nnormal.\nGround Truth Report\nThe chest x-ray shows a \nright-sided chest tube, \nretrocardiac, and right lower \nlobe subsegmental \natelectasis. Additionally, \nthere is a moderate right \napical pneumothorax \nwhich slightly increased \nfrom the prior day's study. \nStructured Finding Comparison\nAtelectasis\nPneumothorax\nSupport Devices\nPleural Effusion\nCardiomegaly\nNo Finding\n0\n0\n1\n-\n-\n0\nReport Label (SFT)\n0\n1\n1\n-\n-\n0\nAnswer Label (SFT)\n1\n1\n1\n0\n-\n0\nReport Label (RL)\n1\n1\n1\n0\n-\n0\nAnswer Label (RL)\n1\n1\n1\n0\n-\n0\nGround Truth\nFigure 2: Qualitative comparison of report generation and internal coherence. The top row displays the original chest\nX-ray, the reports generated by SFT and RL models, and the Ground Truth report. The bottom table compares the\nstructured findings derived from the generated report text (“Report findings”) versus the model’s explicit structured\noutput (“Answer Label”).\nflicts with the corresponding structured diagnostic\nlabels, such as contradictory indications regard-\ning atelectasis and pneumothorax. In contrast, the\nRL-based model produces outputs that are more\ninternally coherent and better grounded in the vi-\nsual evidence. By explicitly enforcing alignment\nbetween the reasoning (<think>) and the diagnos-\ntic summary (<answer>), the model reduces fac-\ntual hallucinations and avoids contradictions be-\ntween textual findings and labels. As a result, the\nRL model more accurately captures subtle clinical\nobservations, including “bibasilar atelectasis”, re-\nflecting improved fidelity to the ground truth and\nenhanced clinical reliability.\n6\nConclusion\nThis study introduces a two-stage framework for\nradiology report generation that jointly optimizes\nvision–language pairing and consistency-aware\nlearning. Through a systematic evaluation in Phase\n1, we identify domain-specific vision encoders,\nsuch as XrayCLIP, as the primary determinant of\nperformance, consistently outperforming general-\npurpose backbones.\nNotably, we observe that\npairing domain-pretrained vision encoders with\ndomain-pretrained LLMs can degrade performance,\nrevealing non-linear interactions that challenge con-\nventional architectural assumptions. Building on\nthese insights, Phase 2 introduces a reinforcement\nlearning framework that substantially improves\nclinical efficacy while reducing hallucinations and\nlogical inconsistencies between generated reason-\ning and diagnostic outputs. Overall, our findings\nhighlight that trustworthy radiology report genera-\ntion requires explicit alignment between reasoning\nand diagnosis, rather than reliance on surface-level\nlexical similarity, and point toward future direc-\ntions in adaptive pairing and consistency-driven\n8\n"}, {"page": 9, "text": "training for medical MLLMs.\nLimitations\nDespite\nthe\nefficacy\nof\nour\n“Reason-then-\nSummarize” framework, we acknowledge two pri-\nmary limitations. First, our reliance on a standard\nMLP projection layer for cross-modal alignment\nmay bottleneck the transmission of fine-grained\nradiological details; future work will explore ad-\nvanced connector architectures (e.g., Q-Former) to\nenhance visual feature granularity. Second, the re-\ninforcement learning process induced a “length col-\nlapse” phenomenon, where the model optimized\nfor succinct, high-yield diagnostic statements at the\nexpense of descriptive verbosity. While this brevity\npreserves clinical accuracy, it results in reasoning\ntraces significantly shorter than human references,\nadversely impacting length-sensitive n-gram met-\nrics like BLEU and ROUGE and creating a diver-\ngence between linguistic scores and actual diag-\nnostic utility. Future works will consider how to\nresolve these issues.\nEthic Statements\nThis study utilizes the publicly available, de-\nidentified MIMIC-CXR and IU X-Ray benchmarks,\nstrictly adhering to PhysioNet credentialing and\nHIPAA Safe Harbor protocols. While our \"Reason-\nthen-Summarize\" framework enhances logical con-\nsistency, the resulting models remain research pro-\ntotypes and are not certified as Software as a Medi-\ncal Device (SaMD). Given the residual risk of gen-\nerative confabulation inherent to Large Language\nModels, these systems are designed exclusively as\nassistive tools for human augmentation rather than\nautonomous agents. Consequently, any future clin-\nical deployment necessitates rigorous prospective\nvalidation, regulatory clearance, and the implemen-\ntation of mandatory human-in-the-loop verification\nprotocols to ensure patient safety.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, and 1 others. 2022. Flamingo: a visual\nlanguage model for few-shot learning. Advances in\nneural information processing systems, 35:23716–\n23736.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wen-\nbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl\ntechnical report. arXiv preprint arXiv:2502.13923.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments.\nIn IEEvalua-\ntion@ACL.\nJuan Manuel Zambrano Chaves, Shih-Cheng Huang,\nYanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng\nZhang, Fei Wang, Yujia Xie, Mahmoud Khademi,\nZiyi Yang, Hany Hassan Awadalla, Julia Gong,\nHoudong Hu, Jianwei Yang, Chunyuan Li, Jianfeng\nGao, Yu Gu, Cliff Wong, Mu-Hsin Wei, and 7 others.\n2024. A clinically accessible small multimodal ra-\ndiology model and evaluation metric for chest x-ray\nfindings. Nature Communications, 16.\nJiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang,\nXiaolu Hou, Mingcheng Li, Shunli Wang, Dongling\nXiao, Ke Li, and Lihua Zhang. 2024a. Detecting\nand evaluating medical hallucinations in large vision\nlanguage models. arXiv preprint arXiv:2406.10185.\nJunying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao,\nShunian Chen, Guiming Hardy Chen, Xidong Wang,\nRuifei Zhang, Zhenyang Cai, Ke Ji, and 1 others.\n2024b. Huatuogpt-vision, towards injecting medi-\ncal visual knowledge into multimodal llms at scale.\narXiv preprint arXiv:2406.19280.\nZhihong Chen, Maya Varma, Jean-Benoit Delbrouck,\nMagdalini Paschali,\nLouis Blankemeier,\nDave\nVan Veen, Jeya Maria Jose Valanarasu, Alaa Youssef,\nJoseph Paul Cohen, Eduardo Pontes Reis, and 1 oth-\ners. 2024c. Chexagent: Towards a foundation model\nfor chest x-ray interpretation. In AAAI 2024 Spring\nSymposium on Clinical Foundation Models.\nDina Demner-Fushman, Marc D Kohli, Marc B Rosen-\nman, Sonya E Shooshan, Laritza Rodriguez, Sameer\nAntani, George R Thoma, and Clement J McDon-\nald. 2015. Preparing a collection of radiology ex-\naminations for distribution and retrieval. Journal\nof the American Medical Informatics Association,\n23(2):304–310.\nPengfei Gu, Haoteng Tang, Islam A Ebeid, Jose A\nNunez, Fabian Vazquez, Diego Adame, Marcus Zhan,\nHuimin Li, Bin Fu, and Danny Z Chen. 2025. Adapt-\ning a segmentation foundation model for medical\nimage classification. In 2025 IEEE 38th Interna-\ntional Symposium on Computer-Based Medical Sys-\ntems (CBMS), pages 167–172. IEEE.\nDennis Hein, Zhihong Chen, Sophie Ostmeier, Justin\nXu, Maya Varma, Eduardo Pontes Reis, Arne Ed-\nward Michalson Md, Christian Bluethgen, Hyun Joo\nShin, Curtis Langlotz, and 1 others. 2025. Chexalign:\nPreference fine-tuning in chest x-ray interpretation\nmodels without human feedback. In Proceedings\nof the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 27679–27702.\n9\n"}, {"page": 10, "text": "Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven\nTruong, D. Duong, Tan Bui, Pierre Chambon, Yuhao\nZhang, Matthew P. Lungren, Andrew Y. Ng, Curt P.\nLanglotz, and Pranav Rajpurkar. 2021. Radgraph:\nExtracting clinical entities and relations from radiol-\nogy reports. ArXiv, abs/2106.14463.\nAlistair Johnson, Tom Pollard, Roger Mark, Seth\nBerkowitz, and Steven Horng. 2024.\nMimic-cxr\ndatabase. PhysioNet10, 13026:C2JT1Q.\nJiayi Kuang, Ying Shen, Jingyou Xie, Haohao Luo,\nZhe Xu, Ronghao Li, Yinghui Li, Xianfeng Cheng,\nXika Lin, and Yu Han. 2025. Natural language un-\nderstanding and inference with mllm in visual ques-\ntion answering: A survey. ACM Computing Surveys,\n57(8):1–36.\nYuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao,\nYuheng Li, Konstantinos Psounis, and Xiaofeng Yang.\n2025. Med-r1: Reinforcement learning for general-\nizable medical reasoning in vision-language models.\narXiv preprint arXiv:2503.13939.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto\nUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-\nmann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-\nmed: Training a large language-and-vision assistant\nfor biomedicine in one day. Advances in Neural In-\nformation Processing Systems, 36:28541–28564.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Annual Meeting of the\nAssociation for Computational Linguistics.\nFenglin Liu, Hongjian Zhou, Boyang Gu, Xinyu Zou,\nJinfa Huang, Jinge Wu, Yiru Li, Sam S Chen, Yining\nHua, Peilin Zhou, and 1 others. 2025a. Application\nof large language models in medicine. Nature Re-\nviews Bioengineering, pages 1–20.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. Advances in\nneural information processing systems, 36:34892–\n34916.\nHong Liu, Dong Wei, Zhe Xu, Xian Wu, Yefeng Zheng,\nand Liansheng Wang. 2025b. Rrg-dpo: Direct pref-\nerence optimization for clinically accurate radiology\nreport generation. In International Conference on\nMedical Image Computing and Computer-Assisted\nIntervention, pages 552–562. Springer.\nTengfei Liu, Jiapu Wang, Yongli Hu, Mingjie Li, Jun-\nfei Yi, Xiaojun Chang, Junbin Gao, and Baocai Yin.\n2025c.\nHc-llm: Historical-constrained large lan-\nguage models for radiology report generation. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 39, pages 5595–5603.\nSophie Ostmeier, Justin Xu, Zhihong Chen, Maya\nVarma, Louis Blankemeier, Christian Bluethgen,\nArne Edward Michalson, Michael E. Moseley, Cur-\ntis P. Langlotz, Akshay S. Chaudhari, and Jean-\nBenoit Delbrouck. 2024. Green: Generative radi-\nology report evaluation and error notation. ArXiv,\nabs/2405.03595.\nJiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan\nZhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang,\nand Daniel Rueckert. 2025. Medvlm-r1: Incentiviz-\ning medical reasoning capability of vision-language\nmodels (vlms) via reinforcement learning. In Inter-\nnational Conference on Medical Image Computing\nand Computer-Assisted Intervention, pages 337–347.\nSpringer.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Annual Meeting of\nthe Association for Computational Linguistics.\nNoam Rotstein, David Bensaid, Shaked Brody, Roy\nGanz, and Ron Kimmel. 2024. Fusecap: Leverag-\ning large language models for enriched fused image\ncaptions. In Proceedings of the IEEE/CVF winter\nconference on applications of computer vision, pages\n5689–5700.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri,\nAtilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau,\nand 1 others. 2025a. Medgemma technical report.\narXiv preprint arXiv:2507.05201.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroen-\nsri, Atilla P. Kiraly, Madeleine Traverse, Timo\nKohlberger, Shawn Xu, Fayaz Jamil, Cían Hughes,\nCharles Lau, Justin Chen, Fereshteh Mahvar, Liron\nYatziv, Tiffany Chen, Bram Sterling, Stefanie Anna\nBaby, Susanna Maria Baby, Jeremy Lai, Samuel\nSchmidgall, and 62 others. 2025b. Medgemma tech-\nnical report. ArXiv, abs/2507.05201.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Yang Wu, and 1 others. 2024.\nDeepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint\narXiv:2402.03300.\nHaozhan Shen, Peng Liu, Jingcheng Li, Chunxin\nFang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun\nZhang, Kangjia Zhao, Qianqian Zhang, and 1 oth-\ners. 2025. Vlm-r1: A stable and generalizable r1-\nstyle large vision-language model. arXiv preprint\narXiv:2504.07615.\nAkshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pa-\nreek, Andrew Y Ng, and Matthew P Lungren. 2020.\nChexbert: combining automatic labelers and expert\nannotations for accurate radiology report labeling\nusing bert. arXiv preprint arXiv:2004.09167.\nHaoteng Tang, Xiyao Fu, Lei Guo, Yalin Wang,\nScott Mackin, Olusola Ajilore, Alex Leow, Paul\nThompson, Heng Huang, and Liang Zhan. 2022.\nFunctional2structural:\nCross-modality brain net-\nworks representation learning.\narXiv preprint\narXiv:2205.07854.\nPeter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo,\nAdithya Jairam Vedagiri IYER, Sai Charitha Akula,\nShusheng Yang, Jihan Yang, Manoj Middepogu,\n10\n"}, {"page": 11, "text": "Ziteng Wang, and 1 others. 2024. Cambrian-1: A\nfully open, vision-centric exploration of multimodal\nllms. Advances in Neural Information Processing\nSystems, 37:87310–87356.\nChenyu Wang, Weichao Zhou, Shantanu Ghosh, Kay-\nhan Batmanghelich, and Wenchao Li. 2025a. Se-\nmantic consistency-based uncertainty quantification\nfor factuality in radiology report generation. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2025, pages 1739–1754, Albuquerque, New\nMexico. Association for Computational Linguistics.\nDandan Wang and Shiqing Zhang. 2024. Large lan-\nguage models in medical and healthcare fields: appli-\ncations, advances, and challenges. Artificial intelli-\ngence review, 57(11):299.\nPan Wang, Siwei Song, Hui Ji, Siqi Cao, Heng Yu, Zhi-\njian Liu, Huanrui Yang, Yingyan (Celine) Lin, Beidi\nChen, Mohit Bansal, Xiaoming Liu, Pengfei Zhou,\nMing-Hsuan Yang, Tianlong Chen, and Jingtong Hu.\n2025b. From models to systems: A comprehensive\nsurvey of efficient multimodal learning. Authorea\nPreprints.\nChaoyi Wu, Xiaoman Zhang, Ya Zhang, Hui Hui, Yan-\nfeng Wang, and Weidi Xie. 2025. Towards generalist\nfoundation model for radiology by leveraging web-\nscale 2d&3d medical data. Nature Communications,\n16(1):7866.\nDexuan Xu, Yanyuan Chen, Jieyi Wang, Yue Huang,\nHanpin Wang, Zhi Jin, Hongxing Wang, Weihua Yue,\nJing He, Hang Li, and 1 others. 2024.\nMlevlm:\nImprove multi-level progressive capabilities based\non multimodal large language model for medical vi-\nsual question answering. In Findings of the Associa-\ntion for Computational Linguistics ACL 2024, pages\n4977–4997.\nLingrui Yang, Yuxing Zhou, Jun Qi, Xiantong Zhen,\nLi Sun, Shan Shi, Qinghua Su, and Xuedong Yang.\n2025a. Aligning large language models with radi-\nologists by reinforcement learning from ai feedback\nfor chest ct reports. European Journal of Radiology,\n184:111984.\nYi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang,\nYan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin,\nFengyun Rao, Minfeng Zhu, and 1 others. 2025b.\nR1-onevision: Advancing generalized multimodal\nreasoning through cross-modal formalization. arXiv\npreprint arXiv:2503.10615.\nZiruo Yi, Ting Xiao, and Mark V. Albert. 2025. A\nsurvey on multimodal large language models in ra-\ndiology for report generation and visual question\nanswering. Information.\nFeng Yu, Masahiro Endo, Rayan Krishnan, Ian Pan,\nAndrew Tsai, Eduardo Pontes Reis, Eduardo Kaiser\nUrurahy Nunes Fonseca, H. H. Lee, Zohreh Hossein\nAbad, Andrew Y. Ng, C. Langlotz, Vasantha Kumar\nVenugopal, and Pranav Rajpurkar. 2022. Evaluating\nprogress in automatic chest x-ray radiology report\ngeneration. Patterns, 4.\nJuan Manuel Zambrano Chaves, Shih-Cheng Huang,\nYanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng\nZhang, Fei Wang, Yujia Xie, Mahmoud Khademi,\nZiyi Yang, and 1 others. 2025. A clinically accessi-\nble small multimodal radiology model and evaluation\nmetric for chest x-ray findings. Nature Communica-\ntions, 16(1):3108.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2019.\nBertscore:\nEvaluating text generation with bert.\nArXiv,\nabs/1904.09675.\nXiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weix-\niong Lin, Ya Zhang, Yanfeng Wang, and Weidi\nXie. 2023. Pmc-vqa: Visual instruction tuning for\nmedical visual question answering. arXiv preprint\narXiv:2305.10415.\nKun Zhao, Siyuan Dai, Yingying Zhang, Guodong Liu,\nPengfei Gu, Chenghua Lin, Paul M Thompson, Alex\nLeow, Heng Huang, Lifang He, and 1 others. 2025.\nR-genima: Integrating neuroimaging and genetics\nwith interpretable multimodal ai for alzheimer’s dis-\nease progression. arXiv preprint arXiv:2512.18986.\nKun Zhao, Chenghao Xiao, Sixing Yan, Haoteng Tang,\nWilliam K Cheung, Noura Al Moubayed, Liang Zhan,\nand Chenghua Lin. 2024. X-ray made simple: Lay\nradiology report generation and robust evaluation.\narXiv preprint arXiv:2406.17911.\nYaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi\nFeng, Dongdong Kuang, and Yuwen Xiong. 2025.\nEasyr1: An efficient, scalable, multi-modality rl train-\ning framework.\nZijian Zhou, Miaojing Shi, Meng Wei, Oluwatosin Al-\nabi, Zijie Yue, and Tom Vercauteren. 2024. Large\nmodel driven radiology report generation with clin-\nical quality reinforcement learning. arXiv preprint\narXiv:2403.06728.\nA\nAppendix\nA.1\nInstruction\nTable A.1 is an example of the prompt.\nA.2\nMore Cases\nWe have provided more cases in Figure 3.\n11\n"}, {"page": 12, "text": "Prompt Example\n<|user|>\nYou are an expert radiologist. Your task is to analyze the given chest X-ray image, write a detailed\nfinding, and then provide a structured summary of your finding.\nYou must follow this two-step process:\n1. First, enclose your detailed, free-text radiology findings in <think> tags. This is your reasoning\nwhere you describe what you see.\n2. Second, based ONLY on the text you wrote in <think>, provide a structured JSON object in\n<answer> tags. The JSON must summarize your findings according to the 14 official CheXpert\nlabels.\nCheXpert labels (14):\n• Atelectasis\n• Cardiomegaly\n• Consolidation\n• Edema\n• Enlarged Cardiomediastinum\n• Fracture\n• Lung Lesion\n• Lung Opacity\n• No Finding\n• Pleural Effusion\n• Pleural Other\n• Pneumonia\n• Pneumothorax\n• Support Devices\nUse 1.0 for positive findings, 0.0 for negative, and −1.0 for uncertain.\n<|assistant|>\nEXAMPLE:\n<think>\nThe cardiomediastinal silhouette is within normal limits. The lungs are clear.\nThere is no pleural effusion or pneumothorax. A port-a-cath is in place, with its\ntip in the expected location.\n</think>\n<answer>\n{\n\"Atelectasis\": 0.0,\n\"Cardiomegaly\": 0.0,\n\"Consolidation\": 0.0,\n\"Edema\": 0.0,\n\"Enlarged Cardiomediastinum\": 0.0,\n\"Fracture\": 0.0,\n\"Lung Lesion\": 0.0,\n\"Lung Opacity\": 0.0,\n\"No Finding\": 1.0,\n\"Pleural Effusion\": 0.0,\n\"Pleural Other\": 0.0,\n\"Pneumonia\": 0.0,\n\"Pneumothorax\": 0.0,\n\"Support Devices\": 1.0\n}\n</answer>\n12\n"}, {"page": 13, "text": "Chest-Xray\nSFT Generated Report\nThe chest x-ray shows moderate \ncardiomegaly with pulmonary \nvascular congestion and mild \ninterstitial edema. There is a \nsmall right pleural effusion and \nno pneumothorax. The patient has \na history of median sternotomy \nand CABG. The impression is \ncardiomegaly with pulmonary \nvascular congestion and mild \ninterstitial edema.\nRL Generated Report\nThe chest x-ray shows \nmoderate cardiomegaly with \nmild pulmonary edema. \nThere is no large pleural \neffusion or pneumothorax.\nGround Truth Report\nThe chest x-ray reveals midline \nsternotomy wires and mediastinal \nclips, with stable mediastinal contour. \nNo pneumothorax or large effusion is \npresent, and there are intact osseous \nstructures. Additionally, there is no \nfree air below the right \nhemidiaphragm. However, the heart \nremains markedly enlarged, and there \nis mild pulmonary edema noted. \nThese findings suggest marked \ncardiomegaly with mild pulmonary \nedema.\nStructured Finding Comparison\nCardiomegaly\nEdema\nEnlarged \nCardiomediastinum\nPleural Effusion\nPneumothorax\nNo Finding\n1\n1\n-\n1\n0\n0\nReport Label (SFT)\n1\n1\n-\n1\n-\n0\nAnswer Label (SFT)\n1\n1\n-\n0\n-\n0\nReport Label (RL)\n1\n1\n-\n0\n-\n0\nAnswer Label (RL)\n1\n1\n-1\n0\n-\n0\nGround Truth\nChest-Xray\nSFT Generated Report\nThe chest x-ray shows an \nendotracheal tube terminating 5.5 \ncm above the carina. The right IJ \ncatheter terminates at the \ncavoatrial junction. There is no \npneumothorax. The \ncardiomediastinal silhouette is \nenlarged, and there are bilateral \nparenchymal opacities, which may \nbe due to pulmonary edema or \npneumonia. There is no large \npleural effusion. \nRL Generated Report\nThe chest x-ray shows mild \npulmonary edema, \ncardiomegaly, and a right-\nsided central venous catheter \nin the upper SVC. There is no \nlarge pneumothorax or pleural \neffusion. The mediastinum \nappears widened.\nGround Truth Report\nThe chest X-ray shows slightly limited \nevaluation due to overlying external \nartifacts. The endotracheal tube is \nappropriately positioned with the tip \nterminating 4.7 cm above the carina. \nThere is mild pulmonary interstitial \nedema. The mediastinum appears \nwidened, which is probably due to the \nAP technique and supine positioning. \nThe cardiac silhouette is mildly \nenlarged, likely due to the AP view. \nThere is no pleural effusion or \npneumothorax, and there is no definite \nfree air under the diaphragm. \nStructured Finding Comparison\nCardiomegaly\nEdema\nEnlarged \nCardiomediastinum\nLung Opacity\nSupport Devices\nPneumonia\n-\n-1\n-\n1\n1\n-1\nReport Label (SFT)\n-\n-1\n1\n1\n1\n-1\nAnswer Label (SFT)\n1\n1\n1\n-\n1\n-\nReport Label (RL)\n1\n1\n1\n-\n1\n-\nAnswer Label (RL)\n1\n1\n1\n-\n1\n0\nGround Truth\nChest-Xray\nSFT Generated Report\nThe chest x-ray shows a normal heart \nsize with a stable mediastinal contour. \nThe lungs appear hyperinflated with \nflattening of the diaphragms, which is \nconsistent with COPD. There is a new \nopacity in the left lower lobe, which \ncould be due to pneumonia or \natelectasis. No pleural effusion or \npneumothorax is observed. The \nimpression is that there is a new left \nlower lobe opacity, which could be due \nto pneumonia or atelectasis.\nRL Generated Report\nThe chest x-ray shows a small \nleft pleural effusion with \nadjacent atelectasis. There is \nno evidence of pneumothorax \nor focal airspace \nconsolidation. The heart size \nis normal, and the mediastinal \nand hilar contours are \nunremarkable.\nGround Truth Report\nFindings show a small to moderate left \npleural effusion with mild atelectasis, \nimprovement of atelectasis at the right \nlung base and no focal consolidation, \npneumothorax, or pulmonary edema. \nThe cardiomediastinal silhouette is \nnormal. The impression is that there is \nan interval re-accumulation of a small \nto moderate left pleural effusion.\nStructured Finding Comparison\nPleural Effusion\nEdema\nEnlarged \nCardiomediastinum\nLung Opacity\nAtelectasis\nPneumonia\n0\n-\n-1\n1\n-1\n-1\nReport Label (SFT)\n0\n-\n0\n1\n-1\n-1\nAnswer Label (SFT)\n1\n-\n0\n-\n1\n-\nReport Label (RL)\n1\n-\n0\n-\n1\n-\nAnswer Label (RL)\n1\n-1\n0\n-\n1\n-\nGround Truth\nFigure 3: More cases for qualitative comparison of report generation and internal coherence. The top row displays\nthe original chest X-ray, the reports generated by SFT and RL models, and the Ground Truth report. The bottom\ntable compares the structured findings derived from the generated report text (\"Report Label\") versus the model’s\nexplicit structured output (\"Answer Label\").\n13\n"}]}