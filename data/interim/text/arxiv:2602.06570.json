{"doc_id": "arxiv:2602.06570", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.06570.pdf", "meta": {"doc_id": "arxiv:2602.06570", "source": "arxiv", "arxiv_id": "2602.06570", "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making", "authors": ["Baichuan-M3 Team", ":", "Chengfeng Dou", "Fan Yang", "Fei Li", "Jiyuan Jia", "Qiang Ju", "Shuai Wang", "Tianpeng Li", "Xiangrong Zeng", "Yijie Zhou", "Hongda Zhang", "Jinyang Tai", "Linzhuang Sun", "Peidong Guo", "Yichuan Mo", "Xiaochuan Wang", "Hengfu Cui", "Zhishou Zhang"], "published": "2026-02-06T10:08:59Z", "updated": "2026-02-06T10:08:59Z", "summary": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.06570v1", "url_pdf": "https://arxiv.org/pdf/2602.06570.pdf", "meta_path": "data/raw/arxiv/meta/2602.06570.json", "sha256": "6c2f4676b428627719bf4a4569d52aa151b53dd2ed4cdc2ad34434e35abf6625", "status": "ok", "fetched_at": "2026-02-18T02:19:34.025555+00:00"}, "pages": [{"page": 1, "text": "Baichuan-M3 Technical Report\nBaichuan-M3: Modeling Clinical Inquiry for Reliable\nMedical Decision-Making\nBaichuan-M3 Team\nAbstract\nWe introduce Baichuan-M3, a medical-enhanced large language model engineered to\nshift the paradigm from passive question-answering to active, clinical-grade decision\nsupport. Addressing the limitations of existing systems in open-ended consultations,\nBaichuan-M3 utilizes a specialized training pipeline to model the systematic workflow\nof a physician. Key capabilities include: (i) proactive information acquisition to resolve\nambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent\ndiagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability.\nEmpirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results\non HealthBench, the newly introduced HealthBench-Hallu and ScanBench, signifi-\ncantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are\npublicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.\n1\nIntroduction\nLarge language models (LLMs) are advancing rapidly [1‚Äì4], driving broader adoption in health-\ncare [5‚Äì9]. As a result, expectations are shifting from one-off question answering to end-to-end\nclinical decision support [10, 11]. This trend is reflected in leading systems such as OpenAI‚Äôs GPT-\n5.2 [12], ChatGPT Health [13], and Claude in Healthcare [14]. Yet key limitations persist: despite\nbetter scores on static, well-specified benchmarks, models often fail to stay evidence-grounded\nand uncertainty-aware in open-ended clinical interactions, where missing information and long-\nhorizon decisions make hallucinations harder to control [15]. Our goal is therefore to move beyond\nreliable QA toward decision-support partners that can operate safely in practice.\nTo bridge the gap between passive retrieval and active clinical support, recent studies have been\n1\narXiv:2602.06570v1  [cs.CL]  6 Feb 2026\n"}, {"page": 2, "text": "Baichuan-M3 Technical Report\nevaluated along two partially disconnected paradigms: (i) factuality-focused, single-turn bench-\nmarks and (ii) process-oriented, multi-turn consultation simulations. Benchmarks such as Health-\nBench [16] and Med-HALT [17] measure factual consistency and common error modes, including\nhallucinations. They often show that performance drops on harder cases that require complex,\nmulti-constraint clinical reasoning, and that models may produce ungrounded claims. In parallel,\ngrowing attention has been paid to Interactive History Taking (IHT) in OSCE-style (Objective Struc-\ntured Clinical Examination) settings, where models are assessed on their ability to elicit missing\ninformation and follow clinical workflows. Systems such as Google‚Äôs AMIE [18, 19] demonstrate\nstrong communication quality in simulated encounters under these rubrics.\nHowever, a critical gap remains in unifying these two dimensions. Existing approaches often treat\nconversational interaction and clinical reasoning as orthogonal objectives, rather than components\nof a single coherent system.\nKnowledge-centric models exhibit ‚Äúinquiry inertia,‚Äù lacking the\nagency to elicit missing evidence, while interaction-focused models can sacrifice diagnostic depth,\nprioritizing fluency over principled differential reasoning.\nIntegrating interaction and reasoning is challenging due to three technical bottlenecks. First, het-\nerogeneous training environments across diverse clinical tasks hinder stable multi-task fusion.\nSecond, long-horizon interactions amplify the credit-assignment problem in reinforcement learn-\ning (RL) [20‚Äì22]: when supervision is dominated by terminal outcomes, models struggle to identify\nwhich conversational turns were causally responsible for diagnostic success. Third, efforts to in-\ncrease reasoning depth often encounter reward saturation, where learning signals diminish near\nperformance plateaus‚Äîsometimes accompanied by increased hallucination as models attempt to\nsatisfy complex logical constraints [23, 24].\nTo address these challenges, we introduce Baichuan-M3, a next-generation medical LLM designed\nto unify clinical inquiry with reliable decision-making. Baichuan-M3 emphasizes three core compe-\ntencies aligned with real clinical workflows: (i) proactive information acquisition, (ii) construction\nof coherent reasoning trajectories, and (iii) adaptive hallucination suppression.\nMethodologically, we propose a three-stage training framework consisting of Task-Specific Rein-\nforcement Learning (TaskRL), Offline Policy Distillation, and Multi-Teacher Online Policy Distil-\nlation (MOPD). This hierarchical design decouples the optimization of individual competencies\nbefore integrating them into a single policy. To improve long-horizon consultation performance,\nwe introduce Segmented Pipeline Reinforcement Learning, which decomposes complex tasks into\nstages with separate reward signals. We further strengthen diagnostic reasoning via Dynamic\nRubric Evolution and targeted RL objectives for hallucination suppression.\n2\n"}, {"page": 3, "text": "Baichuan-M3 Technical Report\nBaichuan-M3 achieves state-of-the-art performance on three authoritative benchmarks: (1) 44.4\non HealthBench-Hard, outperforming GPT-5.2; (2) top performance across all three dimensions\nof ScanBench (our OSCE-like benchmark)‚ÄîClinical Inquiry (74.9), Laboratory Testing (72.1), and\nDiagnosis (74.4)‚Äîexceeding both GPT-5.2-High and expert baselines; and (3) superior factual\nreliability in tool-free hallucination assessments. In summary, our contributions are three-fold:\n‚Ä¢ Bridging Inquiry and Reasoning: We present Baichuan-M3, designed to transform LLMs\nfrom passive information retrievers into robust decision-support partners. By modeling the\nfull clinical decision-making process, our system demonstrates the agency to actively elicit\nmissing data while maintaining rigorous diagnostic logic, addressing the critical limitation\nof \"hallucination via assumption\" in open-ended scenarios.\n‚Ä¢ Clinical-Process-Aligned Optimization: We introduce a training paradigm that mirrors pro-\nfessional medical training, utilizing Segmented Pipeline RL to align model behavior with dis-\ntinct stages of clinical consultation (inquiry, lab testing, diagnosis). This approach, combined\nwith dynamic rubric evolution, ensures that the model learns to prioritize evidence-based\nreasoning over mere conversational fluency or forced logical fitting.\n‚Ä¢ Superior Empirical Results: Extensive experiments demonstrate Baichuan-M3‚Äôs leadership\nin both factual reliability and interactive procedural rigor. It sets new state-of-the-art records\non HealthBench-Hard and our proposed ScanBench, showing significant gains in halluci-\nnation suppression and diagnostic accuracy compared to leading proprietary models like\nGPT-5.2 and expert baselines.\n2\nTraining Infrastructure\nIn this section, we describe the training infrastructure of Baichuan-M3, including a stable patient\nsimulation environment, a verification system that integrates rubric-based and fact-aware evalua-\ntion, and a progressive multi-stage training pipeline. Together, these components provide reliable\ninteraction signals and scalable optimization support for long-horizon medical training.\n2.1\nPatient Simulator\nIn our previous work [6], we introduced a patient simulator for doctor‚Äìpatient interactions. Dur-\ning deployment, we found that the simulator became unstable when modeling proactive patients.\nSpecifically, such behaviors disrupt the consultation flow, leading to simulations difficult to re-\n3\n"}, {"page": 4, "text": "Baichuan-M3 Technical Report\nproduce. To balance the generalization benefits of stochasticity with the stability required for\nlong-horizon training, we adopt a passive-personality patient simulator and implement two com-\nplementary script-driven modes, referred to as Passive Interaction Mode and Interruption-Injected\nMode.\nPassive Interaction Mode (75% sampling probability):\nThis mode provides only the patient\nprofile, inquiry rubrics, and behavioral constraints rubrics, without any predefined dialogue his-\ntory. It simulates the opening phase of an initial consultation. Starting from an empty interaction\nstate, this mode evaluates the physician agent‚Äôs ability to proactively elicit relevant information\nand form an initial diagnostic hypothesis under high uncertainty.\nInterruption-Injected Mode (25% sampling probability):\nThis mode augments the basic patient\ninformation with a predefined dialogue snippet to simulate a mid-consultation state. The snippet\nends with a patient-initiated question (often anxiety-driven about severity or treatment), modeling\na common interruption during inquiry. However, exposing this snippet to the patient simulator\nmay cause it to mimic the snippet‚Äôs speaking style and deviate from our passive-response protocol,\nintroducing instability. We therefore use an asymmetric visibility mechanism: the snippet is visible\nonly to the physician agent and hidden from the patient simulator. The physician answers the\nquestion and continues the consultation, while the simulator passively receives and responds to\nsubsequent queries.\nTo reduce distribution mismatch between the simulated environment and real-world online con-\nsultations, we further introduce fine-grained probabilistic configurations in Interruption-Injected\nMode. Specifically, end-of-turn questioning accounts for 50% of cases, where the patient question\nappears only at the end of the snippet, testing the model‚Äôs ability to handle abrupt interruptions.\nThe remaining 50% corresponds to mid-turn questioning, which injects questions within an ongo-\ning turn to simulate multi-turn. This mixed configuration ensures robust diagnostic performance\nunder interaction noise, including frequent interruptions, challenges, and anxiety-driven follow-up\nquestions.\n2.2\nVerify System\nTo guarantee the reliability and clinical safety of the generated responses, we construct a com-\nprehensive Verify System that serves as the primary source of reward signals for Reinforcement\nLearning. Unlike general-domain chat models where fluency and helpfulness are often sufficient,\n4\n"}, {"page": 5, "text": "Baichuan-M3 Technical Report\nmedical agents face the dual challenge of strictly adhering to diagnostic protocols while ensuring\nabsolute factual precision. A monolithic reward model often struggles to disentangle these orthog-\nonal dimensions‚Äîpotentially encouraging fluent hallucinations over rigid accuracy. To address\nthis, our system decouples the evaluation process into two parallel streams: a Rubric Verifier that\nassesses the structural quality and adherence to clinical guidelines through fine-grained criteria,\nand a Fact Verifier that rigorously checks the biological and medical validity of atomic claims\nagainst external authoritative sources. This hybrid approach ensures the model is optimized for\nboth professional compliance and factual groundedness.\n2.2.1\nRubric Verifier\nThe M3 verification stack follows the rubric-based verifier paradigm [25, 26] introduced in M2 [6].\nInstead of treating medical response quality as a single monolithic preference signal, we decompose\neach interaction into a set of independently decidable rubric clauses. An LLM-based judge [27]\nevaluates each clause item by item, and the resulting decisions are aggregated into a scalar reward\nfor policy optimization.\nGiven a sample x and a set of rubrics R = {ri}N\ni=1, each rubric ri is assigned a signed weight\nwi ‚àà[‚àí10, 10], where wi > 0 denotes rewards and wi < 0 denotes penalties. An LLM judge\noutputs a binary decision ai ‚àà{0, 1} indicating whether ri is satisfied. The task reward is obtained\nby min-max normalization:\nRtask =\nPN\ni=1 wiai ‚àíP\ni:wi<0 wi\nPN\ni=1 |wi|\n(1)\nThis normalization decouples the reward scale from both the number of rubrics and the magnitude\nof their weights into [0, 1]. As a result, reward distributions remain comparable across different\nsamples and rubric configurations, while integer weights can be used to directly encode relative\nclause importance in an interpretable manner.\nTo increase the efficiency in the RL pipeline, reward evaluation is scheduled asynchronously with\nthe rollout process: once the policy emits a response, we immediately launch the corresponding\nrubric-judging jobs, overlapping judge compute with other unconflicted RL actions (e.g., comput-\ning œÄref/œÄold log-probabilities) to accelerate the training. In addition, we adopt a prefix-affinity\nprompt design for the judge: system constraints, output schema, and dialog context share the\nsame template prefix, while the suffixes are substituted with the rubric clause. This processing\nperforms micro-batching to maximize KV-cache [28] reuse, substantially reducing the time cost.\n5\n"}, {"page": 6, "text": "Baichuan-M3 Technical Report\n2.2.2\nFact Verifier\nBaichuan-M2 [6] mainly relies on rubric-based rewards to shape medical reasoning. As perfor-\nmance saturates on common criteria, the model tends to chase marginal gains by adding rarer\nclinical details, which increases hallucination risk; we therefore optimize for both completeness\nand reliability. To make the objective of a low hallucination rate quantitative and optimizable,\nwe build upon existing long-form factuality evaluation methodologies [29, 30] to construct a Fact-\nAware Verification Pipeline. The pipeline adopts a two-stage architecture: Firstly, we decompose\nlong-form responses into fine-grained, independently verifiable atomic claims. Secondly, we em-\nploy a search-augmented verification agent to validate each claim against authoritative sources.\nThis module runs in parallel with the Rubric Verifier as a service, minimizing latency.\nAtomic Claim Extraction Model.\nThe foundation of the pipeline is transforming unstructured\nmodel outputs into discrete, verifiable units, referred to as atomic claims. These claims must\nbe self-contained and verifiable even when detached from their original context. To meet this\nrequirement, we apply the following rules:\n1. Atomicity and Coreference Resolution. We decompose complex compound sentences into\nsingle-fact units and resolve pronouns to guarantee semantic independence.\n2. Noise and Distractor Filtering. We discard units that lack sufficient context to form a factual\nstatement. Crucially, in multiple-choice scenarios, we explicitly exclude the recitation of\nincorrect options (distractors) to prevent them from being misclassified as hallucinations.\n3. Deduplication with Order Preservation. Semantically redundant assertions are eliminated\nwhile maintaining the original logical sequence of the reasoning chain.\nIn the evaluation phase, we initially use GPT-5 [31] as a high-quality extractor; however, its inference\nlatency is prohibitive for online RL. We therefore distill an efficient 8B extraction model from GPT-\n5, trained on datasets spanning multiple medical subtasks, and use it as the final extractor, with\ndetailed fidelity evaluation provided in Appendix A.2.1.\nSearch-Augmented Verifier.\nAfter extraction, atomic claims are fed into a search-augmented\nvalidation agent. This agent performs iterative searches over authoritative medical sources such\nas clinical guidelines and autonomously decides whether the collected evidence is sufficient to\nsupport a verdict.\nEach claim is finally assigned one of three labels: Supported, Refuted or,\nUncertain.\nCompared with methods that rely only on parametric model knowledge or static\n6\n"}, {"page": 7, "text": "Baichuan-M3 Technical Report\nknowledge bases, this dynamic search-based mechanism can stay aligned with the latest clinical\nevidence and better handle the continuous evolution of medical knowledge.\nTwo-Level Caching System for Acceleration.\nIntroducing fine-grained fact verification into the\nonline RL loop raises significant computational challenges. A single RL iteration may generate\nthousands of atomic claims, and performing real-time external search for each claim is unacceptable\nin both cost and latency.\nTo accelerate the training, we design a two-level claim caching system based on the key observation\nthat, for the same clinical query, multiple sampled responses from the model differ in wording but\nshare a high proportion of underlying medical claims.\n‚Ä¢ Level-1 cache (exact match). We use Redis to cache verification results for identical claim\nstrings, enabling millisecond-level lookups and result reuse.\n‚Ä¢ Level-2 cache (semantic match).\nWe store embeddings of historical claims in a vector\ndatabase and apply ANN retrieval to find semantically equivalent claims and reuse their\nverification results.\nAs the cache pool grows, the overall hit rate increases from below 40% in the early stage to around\n80%. This reduces external search requests by approximately 85%, making the impact of fact\nverification on training is negligible. Semantic caching inevitably introduces some systematic bias.\nFor example, claims with subtle dosage differences may be incorrectly treated as equivalent. We\naddress this issue using the signal denoising mechanism described in Section 3.2.2.\n2.3\nMulti-Task Training Pipeline\nTo mitigate the trade-off issue in multi-task learning [32, 33] and reduce development complexity,\nthe training pipeline of Baichuan-M3 is decomposed into three progressive stages: Capability\nLearning, Distribution Fusion, and Policy Unification. By isolating the acquisition of expert ca-\npabilities from the student models, this design achieves a stable fusion process and significantly\nimproves the overall performance. An illustration for the overall pipeline is summarized in Fig. 1.\nStage 1: Task RL.\nIn the initial stage, our objective is to construct a diverse and high-quality\nset of expert teachers. Starting from a shared initialization, we deploy independent RL pipelines\ntailored to specific capability domains. Specifically, to address the diverse requirements from\n7\n"}, {"page": 8, "text": "Baichuan-M3 Technical Report\nStage 1: Task RL\nStage 3: MOPD\nInit\nModel\nClinical\nHealthcare\nGeneral\nExpert\nExpert\nExpert\nRL Pipeline\nRL Pipeline\nRL Pipeline\n(Reward)\n(Reward)\n(Reward)\nTeacher Rollout\nTeacher Rollout\nFinal\nOutput\nFinal\nOutput\nToken 1\nToken 2\nToken 1\nToken 2\nStudent\nModel\nStudent\nRollout\nClip-Forward KL\nTeacher\nDist. ùúã!\nStudent\nDist. ùúã\"\nCondition:\nùúã! < ùúã\"\nYes\nNo\nGradient\nUpdate\nSuppress\n(Mask)\nRewards\nReverse KL\nMOPD Update \nStage 2: Offline Policy Distillation\nStudent\nRollout\nReverse KL\nRewards \nFinal \nModel\nEnv\nFigure 1: An illustration for the three stage pipeline.\nmedical applications, we explicitly train specialized experts for Clinical Inquiry and Healthcare\nConsultation, ensuring the model captures the nuances of real-world diagnostic dialogue and\npatient-centric health advice. In addition, we also train a generalist expert focused on fundamental\ncapabilities, including Instruction Following and General Reasoning.\nInspired by [34], the core philosophy here is differentiation rather than unification. By allow-\ning different models to explore fully under their respective task-specific rewards, we obtain a set\nof domain-specialized teachers with strong, distinct inductive biases. This divide-and-conquer\nstrategy following [34] effectively isolates gradient interference across tasks, avoiding the opti-\nmization conflicts typical in the early stages of multi-task mixture training, thereby providing\nhigh-confidence behavioral guidance for subsequent fusion.\nStage 2: Offline Policy Distillation.\nThe second stage focuses on ‚Äúcompressing‚Äù the capabilities\nof multiple teachers into a single student model via offline distillation. We freeze all teacher models\nand perform rollouts within their respective domains to construct an offline trajectory dataset, D.\nThe student model then learns from this data in an off-policy manner.\nTo adapt to the reality of single-sample offline data and ensure numerical stability during fusion,\nwe introduce a standard KL divergence in favor of a restricted distillation objective based on Clip-\nForward-KL. For each sample (s, a) ‚ààD and its corresponding teacher policy œÄt, we define the loss\nfunction as follows:\n8\n"}, {"page": 9, "text": "Baichuan-M3 Technical Report\nLclip-FKL(Œ∏) = E(s,a)‚àºD [I (log œÄŒ∏(a|s) < log œÄt(a|s)) ¬∑ (‚àílog œÄŒ∏(a|s))]\n(2)\nwhere I(¬∑) denotes the indicator function and œÄŒ∏(¬∑) and œÄt(¬∑) denotes the token-wise probability of\nstudent and teacher model, respectively. This design applies a one-sided update that only enforces\nnon-inferiority on the teacher‚Äôs empirical support, rather than overfitting to the full conditional\ndistribution. This avoids probability over-amplification in the single-sample regime and implicitly\npreserves entropy outside the data support. It can mitigate the mode collapse and leave exploration\nspace for the subsequent mode-seeking optimization in Stage 3.\nLeveraging the mode-covering property of Forward KL, this stage enables the student model to\nbroadly cover high-probability regions of different expert distributions. This facilitates the stable\ninheritance of behavioral patterns and eliminates the instability brought by cold-start associated\nwith direct multi-objective RL.\nStage 3: Multi-Teacher On-Policy Distillation (MOPD) [35].\nIn the third stage, the student\nmodel re-enters the online interaction environment, performing rollouts across mixed domain\ndistributions. At this point, the model is constrained simultaneously by ground-truth task rewards\nand multi-teacher priors. Unlike the distribution imitation in the second stage, this stage employs\nreverse KL regularization [36].\nLeveraging the mode-seeking nature of Reverse KL, the student is driven to select the optimal mode\nwhen comprised with conflicting advice from multiple teachers, rather than passively averaging\nthem. Guided by real reward signals, the student transitions from an ‚Äúimitator‚Äù to a ‚Äúdecision-\nmaker,‚Äù achieving deep unification and de-noising capability at the policy level.\nThe Evolution of Teacher Capability.\nOur proposed framework is not a static pipeline and\nsupports cyclic iterative refinement. The unified model obtained after MOPD can serve as a new\ninitialization for the Stage 1 to achieve domain-specific enhancement, followed by another round\nof distillation. This flexibility allows for continuous improvement of the model‚Äôs capabilities with\nlow marginal cost.\n9\n"}, {"page": 10, "text": "Baichuan-M3 Technical Report\n3\nTask-specific Training Methods\nClinical practice encompasses a diverse spectrum of cognitive activities, ranging from the active\nelicitation of symptom history to the rigorous provision of evidence-based advice. A monolithic\ntraining approach often fails to balance the distinct requirements of these scenarios‚Äîspecifically,\nthe deductive logic required for diagnosis versus the strict factual adherence required for advisory\nservices. To address this, we adopt task-specific optimization strategies tailored to the unique\nmodalities of medical interaction. In this section, we present our specialized methodologies for\ntwo core capabilities: Deep Clinical Consultation, where we employ a segmented pipeline and the\nStep-Penalized Advantage with Relative baseline algorithm to master the multi-turn diagnostic\ntrajectory; and Credible Healthcare Advisory, which utilizes dynamic rubric evolution and Fact-\nAware Reinforcement Learning to ensure the safety and verifiability of medical information.\n3.1\nDeep Clinical Consultation\nAs medical AI evolves, the demand for \"instant diagnosis based on symptom input\" has surged [37].\nHowever, clinical medicine transcends simple knowledge retrieval; it is a discipline grounded in\nrigorous evidence and deductive logic. Because identical symptoms may originate from diverse\netiologies and patient-specific risk thresholds vary, reliable clinical decisions must rely on granular\ndata, systematic risk assessment, and traceable reasoning [38, 39].\nWe introduce the Deep Clinical Consultation framework, which re-imagines the consultation as\na clinical-grade, structured, and auditable process of information production. Our objective is to\ncollect pivotal clinical data within brief interactions while maintaining stringent safety standards.\nTo achieve this, we propose a training framework (see Fig. 2) comprising a Segmented Pipeline RL\nPolicy\nModel\nFor Each Step\nFor Total Trajectory\nTrajectory\nStep¬†\nVerifier\nGlobal¬†\nVerifier\nGlobal¬†\nReward\nStep¬†\nPenalty\nSPAR\nUpdate\nStep-wise\nAdvantage\nRollout\nPolicy Learning\nContext Reuse\nInquiry\n(T1)\nContext Reuse\nDDX (T1)\nInquiry\n(T2)\nInquiry\n(T3)\nDDX (T2)\nContext Reuse\nLab Test\n(T1)\nInquiry\n(T4)\nDDX (T3)\nLab Test\n(T2)\nDiagnose\n(T1)\nTask Slot\nTask Slot\nTask Slot\nTask Slot\nTask Scheduling\n...\nGlobal Steps\nFigure 2: Segmented Pipeline RL (left) and Policy Learning Algorithm (right).\n10\n"}, {"page": 11, "text": "Baichuan-M3 Technical Report\narchitecture and the Step-Penalized Advantage with Relative baseline (SPAR) algorithm.\n3.1.1\nSegmented Pipeline RL\nWe formulate the consultation as a K-stage generation process with K = 4. Let [K] = {1, . . . , K}\nindex the stages, and let S = {Inq, DDX, Lab, Diag} denote the set of stage types. For a patient\ncase i at stage k, let x(i)\nk denote the current input context, which encapsulates the entire trajectory\nhistory plus the specific instruction for the current stage (e.g., \"Based on the above, suggest lab\ntests\"). The policy œÄŒ∏ generates a response segment y(i)\nk :\ny(i)\nk\n‚àºœÄŒ∏(¬∑|x(i)\nk )\n(3)\nAs shown in Fig. 2, we employ an Asynchronous Multi-Task Pipeline in which, at global step t,\nmultiple task slots run in parallel on different pipeline stages (and potentially different patient\ncases); the union of these stage-specific tasks forms the training batch Bt.\nMulti-Task Training Objectives.\nThe optimization targets the quality of the generated segment\nyk relative to the stage-specific goals. For a batch of active contexts Bt = {x(i)\nki }N\ni=1, the joint objective\nfunction is:\nJ (Œ∏) =\n1\n|Bt|\nX\nx(i)\nk ‚ààBt\nLRL\n\u0010\ny(i)\nk , G(i)\nk\n\u0011\n(4)\nwhere Gk is the reward function tailored to stage k. This formulation unifies diverse reasoning\ncapabilities (gathering vs. deducing) into a single generative model.\nContext Reuse via Gated Transition.\nThe pipeline relies on the auto-regressive accumulation of\ncontext. The input for the next stage, x(i)\nk+1, is constructed by appending the generated response\ny(i)\nk and the next stage‚Äôs instruction pk+1 to the current context:\nx(i)\nk+1 = [x(i)\nk , y(i)\nk , pk+1]\n(5)\nHowever, open-ended generation carries the risk of error propagation. To enforce the \"garbage-\nin, garbage-out\" principle, we implement a Quality-Gated Transition. The training pool for the\nsubsequent stage, Dk+1, is populated via a filtration process:\nDk+1 ‚Üê\nÔ£±\nÔ£¥\nÔ£≤\nÔ£¥\nÔ£≥\nDk+1 ‚à™{[x(i)\nk , y(i)\nk , pk+1]},\nif V(i)\nk\n‚â•œÑ\nDk+1,\notherwise (Discard)\n(6)\nwhere V(i)\nk\nis the score produced by the stage-k quality verifier and œÑ is the acceptance threshold.\nThis ensures that only trajectories with clinically valid logical chains are extended, effectively\n11\n"}, {"page": 12, "text": "Baichuan-M3 Technical Report\npruning error paths from the training curriculum.\n3.1.2\nPolicy Learning Algorithm: SPAR\nConventional RL approaches, such as GRPO [20, 40] with global rewards, exhibit significant\nlimitations in long-horizon medical interviewing. These include reward hacking (inflating recall\nvia redundant questions), logic fragmentation (disjointed clinical transitions), and ineffective credit\nassignment. In long-context dialogues, trajectory-level rewards fail to isolate local errors [41], often\ninducing training instability by penalizing valid reasoning chains alongside specific flaws.\nTo address these challenges, we propose SPAR (Step-Penalized Advantage with Relative base-\nline), which introduces fine-grained step-wise penalties and a decoupled advantage estimation\nmechanism to induce an adaptive curriculum-learning effect.\nReward Formulation.\nSPAR employs a hierarchical reward structure. For a generated response y\nconsisting of L logical interaction steps [z1, . . . , zL], we define each step zj as one complete dialogue\nexchange (i.e., a user turn followed by the assistant turn). We first compute a Global Reward Rglobal\nby evaluating the complete trajectory against a set of pre-defined Clinical Rubrics (e.g., diagnostic\naccuracy, evidence completeness).\nSimultaneously, a step verifier performs real-time validation for each interaction step zj. Let Vj\ndenote the set of violation types triggered by step zj (e.g., redundancy, safety risk). We define the\nstep-wise validity factor Œ≥j ‚àà(0, 1] as:\nŒ≥j =\nÔ£±\nÔ£¥\nÔ£≤\nÔ£¥\nÔ£≥\n1,\nif Vj = ‚àÖ\nmin\nv‚ààVj(Œªv),\notherwise\n(7)\nwhere Œªv ‚àà(0, 1) is the penalty coefficient for violation type v.\nThis formulation enforces a\nminimum validity principle: if a step commits multiple errors, only the most severe penalty\n(smallest Œª) is applied. The effective return for the step is then modulated as Rj = Œ≥j ¬∑ Rglobal.\nStep-wise Advantage Estimation.\nThe core innovation of SPAR lies in its advantage computation,\nwhich decouples local penalties from the group baseline.\nUnlike traditional approaches that\nnormalize rewards using the penalized distribution, SPAR computes the advantage ÀÜAj for step zj\nby comparing the step-penalized return against an unpenalized group average:\nÀÜAj = Œ≥jRglobal ‚àí¬µraw\nœÉraw + œµ\n(8)\n12\n"}, {"page": 13, "text": "Baichuan-M3 Technical Report\nwhere ¬µraw and œÉraw represent the mean and standard deviation of the raw global rewards (Rglobal)\nwithin the sampled group, excluding any step penalties. Here, a sampled group refers to multiple\nrollouts generated under the same prompt (i.e., the same consultation context), which enables\nrelative comparison among candidates.\nWe then apply a GSPO-style policy update [42], using the step-wise advantages ÀÜAj to weight the\nlikelihood-ratio objective, so that penalties are attributed to the specific interaction steps responsible\nfor violations. Here, a sampled group refers to multiple rollouts generated under the same prompt\n(i.e., the same consultation context), which enables relative comparison among candidates.\nImplicit Curriculum Mechanism.\nThis advantage design facilitates an implicit curriculum [43]\nby naturally scheduling optimization priorities based on error severity:\n‚Ä¢ Phase 1: Correction of Critical Errors. For severe violations (e.g., repetition), we assign a\nrigorous penalty (e.g., Œª ‚âà0.1). This forces the effective return Œ≥jRglobal significantly below\nthe group baseline ¬µraw, resulting in a large negative advantage ( ÀÜAj ‚â™0). This dominant\ngradient signal compels the model to prioritize rectifying fundamental usability flaws in the\nearly training phase.\n‚Ä¢ Phase 2: Refinement of Nuance. For subtle imperfections (e.g., rigid phrasing), a milder\npenalty (e.g., Œª ‚âà0.9) is applied. Initially, this small deviation is masked by the high variance\n(œÉraw) of global rewards. However, as the policy stabilizes and œÉraw decreases, these fine-\ngrained signals begin to dictate the gradient direction, guiding the model toward stylistic\nperfection.\nBy isolating the impact of specific step-level behaviors, SPAR enables precise credit assignment,\ndistinguishing local flaws from overall diagnostic success.\n3.2\nCredible Healthcare Advisory\nThis section details our training methodology for credible healthcare advisory, focusing on the syn-\nthesis of clinical credibility and interactive utility. To overcome the limitations of static feedback,\nwe first introduce a dynamic rubric evolution framework designed to mitigate reward hacking\nand ensure that the model pursues genuine reasoning rather than superficial patterns. Further-\nmore, we present a Fact-Aware Reinforcement Learning strategy that enhances the model‚Äôs factual\nreasoning capability. By moving beyond naive penalty mechanisms, this approach effectively sup-\npresses unfaithful hallucinations while circumventing induced conservatism, thereby preserving\n13\n"}, {"page": 14, "text": "Baichuan-M3 Technical Report\nthe model‚Äôs ability to provide detailed and informative medical counsel.\n3.2.1\nDynamic Rubric Evolution\nIn our previous work [6], we proposed a rubric-based RL approach to enhance clinical reasoning\ncapabilities. However, we observed that this method is highly susceptible to ‚Äúreward hacking.‚Äù\nThe model tends to pursue superficial ‚Äúhigh-score performance‚Äù rather than genuine reasoning,\nmanifesting as passive verbosity, defensive template usage, or hallucinated details. The root cause\nlies in the fact that prior rubric formulation relied solely on the input question. Once the question is\nfixed, the rubric becomes static, creating a predictable structure that the model can easily exploit.\nTo address this issue, we introduce a significant upgrade to the fundamental quality of the feedback\nsignal in the M3 model, compared to its predecessor, M2. Inspired by [44, 45], we propose a Human-\nAI Collaborative Dynamic Evolution mechanism, which crucially incorporates the model‚Äôs response\ninto the rubric synthesis process. Specifically, we categorize constraints into two distinct classes:\n‚Ä¢ Core Rubric Set: Synthesized solely based on the question, this set guides the general\ndirection of model optimization and ensures fundamental safety.\n‚Ä¢ Dynamic Rubric Set: Synthesized dynamically based on both the question and the model‚Äôs\nhistorical responses. This set aims to constrain non-compliant behaviors and specific vulner-\nabilities discovered during the training process.\nThe construction and maintenance of this dynamic set rely on two key modules: Quality Control\nand Admission/Exit Rules.\nQuality Control\nTo ensure the efficacy and clinical relevance of the dynamically generated\nrubrics, we implement a ‚ÄúMine-Verify-Inject‚Äù closed-loop workflow.\nFirst, we identify High-\nConfidence Samples‚Äîresponses that achieve high scores under the current system but harbor\nlatent defects.\nNext, the Rubric Mining Agent analyzes these samples to identify adversarial\npatterns and draft candidate constraints. Finally, Human Experts intervene to validate these can-\ndidates. Instead of authoring rules from scratch, experts review the agent‚Äôs output, assessing its\nboundary determinism and compliance with meta-principles (e.g., Safety > Empiricism). This\nensures that the rubric incentivizes reasoning rather than shifting the locus of reward hacking.\n14\n"}, {"page": 15, "text": "Baichuan-M3 Technical Report\nAdmission and Exit Rules\nTo prevent rule explosion and ensure the reward signal remains\npotent, the dynamic rubric set follows a ‚ÄúProblem-Driven‚Äù lifecycle:\n‚Ä¢ Admission: A candidate rubric is not admitted merely for its validity; it is activated only\nwhen it targets a statistically significant failure mode (i.e., a high violation rate in model\nresponses). This ensures feedback remains focused on active behavioral deficits.\n‚Ä¢ Exit: Once a constraint is consistently satisfied over multiple training epochs (violation rate\n‚Üí0), it is automatically retired from the dynamic set. This ‚Äúpruning‚Äù mechanism prevents\nreward signal dilution, ensuring the optimization focus remains strictly on emerging and\nunresolved issues without being washed out by redundant positive rewards.\n3.2.2\nFact-Aware Reinforcement Learning\nIn the paradigm of Reinforcement Learning with Verification Rewards (RLVR), naive hallucination\nsuppression strategies typically convert verification signals directly into scalar penalties [29, 46].\nThe standard objective form is defined as\nR = Rtask + Œ± ¬∑ Rhallu\n(9)\nHere, Rhallu utilizes a count-based hallucination rate metric (‚àíNhallu/Ntotal). While this objective\naims to preserve core medical reasoning capability (Rtask) while reducing hallucinations via a\npenalty term, it is vulnerable to two major forms of reward hacking in long-form generation:\n‚Ä¢ Redundancy-Induced Dilution: The model inflates the denominator Ntotal by producing\nmany factually correct but low-value statements, thereby reducing the measured hallucina-\ntion rate without correcting the core errors.\n‚Ä¢ Penalty-Induced Conservatism: Strict penalties can encourage overly conservative strate-\ngies (e.g., shortening outputs to avoid penalties), which undermines exploration and the\nacquisition of complex reasoning behaviors.\nTo address these issues, we propose a joint optimization framework comprising Structured Signal\nDenoising and Dynamic Multi-Objective Aggregation, as shown in Fig. 3.\nStructured Signal Denoising\nTo establish a reward signal robust to redundancy, we reformulate\nthe verification of atomic claims as a weighted evaluation based on semantic density. This ensures\nthat hallucinations in core diagnostic statements incur significantly higher penalties than those in\nmarginal content.\n15\n"}, {"page": 16, "text": "Baichuan-M3 Technical Report\nRollout\nClaim Decomposition,\nDe-noising & Weighting\nClaim1\nClaim2\nClaim5\nClaim3\nClaim4\nMedical Query\nResponse\nClaim Cluster \nClaim A\nwith Importance\nClaim B\nwith Importance\nClaim C\nwith Importance\nOnline Fact Verification\nSelect Claim\nReward\nAggregation\nMulti-Turn Search\nPolicy Update\nRL Optimizer (e.g. GSPO)\nLLM     WebSearch  WebFetch\nVerification\nOutcomes\nPolicy Model\nWrite           Search\nCache System\nTRUE\nUNCERTAIN\nFALSE\nTask \nReward\nHallucination\nReward\nFigure 3: Fact-Aware Reinforcement Learning Algorithm.\nWe first map the response sequences {sj} and extracted claims {ci} into a vector space via a\nsemantic encoder E(¬∑). To prevent the manipulation of metrics through synonymous paraphrasing,\nwe apply semantic clustering with a cosine similarity threshold. By selecting a representative claim\nc‚àó\nk for each cluster Ck, we transition the evaluation metric from lexical frequency to semantic unit\ndensity. We then quantify the information contribution of each claim using a Saliency Weight,\nw(c‚àó\nk), defined as its maximum semantic correlation across the response sentences:\nw(c‚àó\nk) = max\n1‚â§j‚â§M cos(Ec‚àó\nk, Esj)\n(10)\nThe factuality reward Rfact is computed as a weighted penalty term:\nRfact = ‚àí\nPK\nk=1 w(c‚àó\nk) ¬∑ I(c‚àó\nk)\nPK\nk=1 w(c‚àó\nk) + œµ\n(11)\nwhere I(c‚àó\nk) is the verification penalty indicator defined as:\nI(c‚àó\nk) =\nÔ£±\nÔ£¥\nÔ£≤\nÔ£¥\nÔ£≥\n1\nif c‚àó\nk ‚àà{Refuted, Uncertain}\n0\notherwise\n(12)\nMechanically, the weighted denominator neutralizes the inflation of claim counts (anti-dilution),\nwhile the saliency-dependent numerator ensures that penalties are concentrated on core errors\nrather than marginal text, thereby preserving reasoning utility.\nDynamic Multi-Objective Aggregation\nTo balance medical reasoning reinforcement with hal-\nlucination suppression, we implement a dynamic aggregation mechanism. We introduce a soft-\ngating coefficient, Œª(Rtask), which modulates the penalty intensity based on the task reward\nachieved by the on-policy generated response.\nGiven the Rtask associated with a specific response, we compute the dynamic coefficient Œª(Rtask)\n16\n"}, {"page": 17, "text": "Baichuan-M3 Technical Report\nstrictly as a function of the task reward using a shaped Sigmoid function:\nŒª(Rtask) = œÉ\n\u0012\nŒ∫ ¬∑ Rtask ‚àí¬µ\n‚àÜ\n\u0013\n(13)\nwhere œÉ(¬∑) is the standard sigmoid function, centered at ¬µ = (œÑmin + œÑmax)/2 and scaled by\n‚àÜ= œÑmax ‚àíœÑmin (with steepness Œ∫ = 10).\nThe thresholds are calibrated via posterior analysis of the task reward distribution. We specifically\nset œÑmin = 0.75 and œÑmax = 0.95 to delimit the critical interval reflecting effective medical reasoning.\nThis setting aligns the penalty strength with the model‚Äôs demonstrated capability:\n‚Ä¢ Protection Zone (Rtask < œÑmin): Œª(Rtask) ‚Üí0. Penalties are suppressed to prioritize capabil-\nity optimization, shielding the acquisition of fundamental reasoning skills from interference.\n‚Ä¢ Transition Zone (œÑmin ‚â§Rtask ‚â§œÑmax): Œª(Rtask) increases non-linearly. In this phase, the\nsystem progressively introduces factual constraints.\n‚Ä¢ Constraint Zone (Rtask > œÑmax): Œª(Rtask) ‚Üí1. Full penalties are enforced to maximize the\nsuppression of hallucinations once the model demonstrates sufficient reasoning competence.\nThe final total reward R aggregates the task utility with the gated factuality penalty:\nR = Rtask + Œª(Rtask) ¬∑ Rfact\n(14)\nThis formulation implements an implicit curriculum that secures reasoning competence before\nimposing rigorous safety constraints. We empirically validate this behavior through comparative\nablation studies in Appendix A.2.2.\n4\nEvaluation\nTo comprehensively evaluate the clinical utility and safety of Baichuan-M3, we conduct rig-\norous experiments across two complementary dimensions: dynamic clinical workflow simu-\nlation (ScanBench) and broad-spectrum medical reasoning (HealthBench [16]).\nWe bench-\nmark Baichuan-M3 against a diverse set of competitive baselines to ensure a robust assessment.\nThese include state-of-the-art general-purpose LLMs known for superior reasoning capabilities\n(e.g., GPT-5.2-High[12], Deepseek-V3.2-Thinking[47], Qwen3-235B-thinking-2507[48]), repre-\nsentative medical-specific models (e.g., AntAngelMed[49]), and our previous generation model\n(Baichuan-M2 [6]). Crucially, to benchmark the model against real-world professional standards,\nwe explicitly introduce a Human Baseline composed of attending physicians from Grade-A tertiary\nhospitals, each with a minimum of 5 years of clinical experience. This comparative analysis aims to\nverify the model‚Äôs advancements in complex decision-making, specialized knowledge application,\n17\n"}, {"page": 18, "text": "Baichuan-M3 Technical Report\nand hallucination suppression relative to generalist giants, domain specialists, and human experts.\n4.1\nScanBench\nUnlike traditional static question-answering benchmarks, ScanBench simulates the authentic clin-\nical workflow of \"Inquiry ‚ÜíLab Testing ‚ÜíDiagnosis.\" The dataset, slated for open-source release,\ntransforms diverse clinical cases into an observable and quantifiable decision path divided into\nthree progressive stages.\n4.1.1\nDataset Composition and Statistics\nScanBench is constructed to simulate the authentic clinical environment with high fidelity across\nthree dimensions: case diversity, inquiry granularity, and examination complexity.\nClinical Case Diversity\nAs shown in Table 1, ScanBench contains 303 cases from 12 depart-\nments. It covers both common conditions (e.g., General Practice) and long-tail specialties (e.g.,\nRheumatology, Hematology).\nTable 1: Distribution of Clinical Samples by Department.\nDepartment\nCount\nDepartment\nCount\nGeneral Practice\n111\nNephrology\n15\nSurgery\n50\nCardiology\n14\nGynecology\n26\nRespiratory Medicine\n14\nNeurology\n25\nEndocrinology\n7\nGastroenterology\n18\nRheumatology\n6\nHematology\n15\nGeriatrics\n2\nInquiry Granularity and Rigor\nTo quantify the inquiry process, we annotated 8,857 checklist\nitems as ground truth.\n‚Ä¢ Information Density: Each case contains 29.23 items on average (range: 20‚Äì35), requiring\nsustained multi-turn context tracking.\n‚Ä¢ Logical Distribution: The checklist mirrors clinical diagnostic logic: History of Present\nIllness dominates (55.8%), followed by Past Medical History (19.6%) and Personal/Social\n18\n"}, {"page": 19, "text": "Baichuan-M3 Technical Report\nHistory (14.6%), with Obstetric/Gynecological (5.4%) and Family History (4.7%) included\nwhen relevant.\n‚Ä¢ Criticality Weighting: We distinguish between essential safety points and general informa-\ntion: 51.3% are labeled Level 2 (Critical) for diagnosis/risk exclusion, and the remaining\n48.7% are Level 1 (Supplementary).\nComprehensive Examination Action Space\nIn the auxiliary examination phase, the model op-\nerates within a unified action space of 38 distinct categories, replicating the resource management\ncomplexity of real hospitals. This includes:\n‚Ä¢ Routine & Biochemical: Blood/Urine/Stool Routine, Liver/Kidney Function, Electrolytes,\nCRP, Glucose Metabolism (including OGTT/HbA1c), etc.\n‚Ä¢ Imaging & Functional: CT, MRI, Ultrasound, X-ray, ECG, EEG, and Pulmonary Function\nTests.\n‚Ä¢ Pathology & Specialized: Tumor Markers, Viral Markers, Autoantibodies, Hormones, Bone\nMarrow Biopsy, and Endoscopy.\nThis extensive candidate set requires the model to precisely select necessary tests while avoiding\nresource waste.\n4.1.2\nWorkflow and Evaluation Methodology\nThe evaluation pipeline starts with Station 1: Inquiry, where the model conducts multi-turn inter-\nactions with a Standardized Patient. To assess process quality, we introduce the SCAN framework,\nwhich decomposes consultation performance into four dimensions: Safety Stratification, Infor-\nmation Clarification, Associative Questioning, and Normative Output.\nWe use GPT-4.1 [1] to\nverify coverage of OSCE-derived key clinical points, while excluding non-diagnostic (or easily\nengineered) content that is repeatedly mentioned in each consultation (e.g., age/sex restatements\nor templated self-introductions).\nGiven the elicited evidence, Station 2: Lab Testing evaluates both resource efficiency and inter-\npretative accuracy. The model proposes a differential diagnosis and selects laboratory or imaging\ntests from a unified candidate pool, where candidate tests are categorized into essential and op-\ntional groups based on their contributions to clinical diagnosis and decision-making. Performance\nis evaluated using a weighted F1 score, in which recall for essential tests is assigned a higher\nweight, thereby reflecting their greater importance in the diagnostic workflow while maintaining\n19\n"}, {"page": 20, "text": "Baichuan-M3 Technical Report\na balanced assessment of overall precision and coverage.\nThe workflow concludes with Station 3: Diagnosis, where the model integrates all prior information\nto infer a final diagnosis. We adopt a hierarchical matching criterion based on the ICD-10 taxonomy,\nrewarding correct leaf-node matches while penalizing off-branch predictions.\n4.1.3\nPerformance Analysis\nAs illustrated in Fig. 4, Baichuan-M3 exhibits a comprehensive advantage, ranking first across all\nthree stations. Most notably, in the challenging Clinical Inquiry phase, it achieves a score of 74.9,\nsurpassing the second-best model (GPT-5.2-High) by 12.4 points and the human baseline by over 20\npoints. This dominance extends to laboratory testing (72.1) and final diagnosis (74.4), suggesting\nthat Baichuan-M3 possesses robust, end-to-end medical reasoning capabilities rather than merely\nexcelling in isolated tasks.\nFigure 4: Overall performance comparison on ScanBench.\nFurther decomposition of the inquiry capability via the SCAN framework (Fig. 5) reveals that\nBaichuan-M3 is the sole model to demonstrate dominant leadership across all four dimensions,\nconsistently outperforming both SOTA LLMs and human experts.\nSpecifically, in Safety Stratification, Baichuan-M3 achieves a remarkable score of 75.8, creating a\nsubstantial gap over the runner-up Qwen3-235B (48.3) and nearly doubling the human benchmark\n(40.1).\nThis indicates superior sensitivity to ‚Äúred flag‚Äù symptoms and critical risks.\nIn terms\nof Association & Inquiry, the model scores 72.6, significantly surpassing GPT-5.2-High (54.5).\nThis highlights its sophisticated grasp of differential diagnosis, enabling it to proactively uncover\nhidden clinical clues beyond the user‚Äôs initial description. Furthermore, Baichuan-M3 excels in\nClarity Matters (84.5) and Normative Protocol (59.9), ensuring that the collected information is both\n20\n"}, {"page": 21, "text": "Baichuan-M3 Technical Report\ngranular and structurally standardized. By integrating these strengths, Baichuan-M3 successfully\ncreates a closed loop of precise inquiry and secure decision-making that exceeds human-level\nstandardization.\n0\n20\n40\n60\n80\n100\nDeepseek-V3.2-Thinking\nAntAngelMed\nBaichuan-M2\nHuman\nGPT-5.2-High\nQwen3-235B-Thinking\nBaichuan-M3\n33.9\n36.4\n39.9\n40.1\n43.2\n48.3\n75.8\nSafety Stratification\n0\n20\n40\n60\n80\n100\nQwen3-235B-Thinking\nDeepseek-V3.2-Thinking\nBaichuan-M2\nAntAngelMed\nHuman\nGPT-5.2-High\nBaichuan-M3\n66\n66.3\n67.5\n68.1\n74.6\n81.4\n84.5\nClarity Matters\n0\n20\n40\n60\n80\n100\nAntAngelMed\nDeepseek-V3.2-Thinking\nQwen3-235B-Thinking\nBaichuan-M2\nHuman\nGPT-5.2-High\nBaichuan-M3\n28.6\n34.4\n37.4\n40.8\n47.8\n54.5\n72.6\nAssociation & Inquiry\n0\n20\n40\n60\n80\n100\nBaichuan-M2\nHuman\nAntAngelMed\nDeepseek-V3.2-Thinking\nQwen3-235B-Thinking\nGPT-5.2-High\nBaichuan-M3\n35.3\n37.2\n38.8\n39.4\n41.4\n43.2\n59.9\nNormative Protocol\nFigure 5: Detailed breakdown of Inquiry Capabilities.\n4.1.4\nDynamic Consultation Efficiency\nFigure 6 plots model scores against the number of dialogue turns. To reduce noise from very rare\nlong conversations, we drop turn bins that contain fewer than 10% of the total samples. This makes\nthe trend across turns easier to interpret.\nConvergence in Basics, Divergence in Reasoning\nFigure 6b shows that most models quickly\nreach a similar level on symptom clarification (around 0.7‚Äì0.8). The gap becomes clear in Associ-\nation & Inquiry (Fig. 6c): Baichuan-M3 keeps improving as the dialogue goes on, while generalist\nmodels (e.g., Deepseek and Qwen) fall behind ‚Äî resulting in nearly a twofold advantage at longer\nhorizons.\n21\n"}, {"page": 22, "text": "Baichuan-M3 Technical Report\nRisk Sensitivity and Contextual Adherence\nFor Safety Stratification (Fig. 6a), Baichuan-M3 is\nmore responsive to risk signals, and its score rises to about 0.7 as evidence accumulates.\nBy\ncontrast, GPT-5.2-High stays around 0.5 even with longer dialogues, suggesting weaker acute-\nrisk recognition. For Normative Protocol (Fig. 6d), performance tends to improve in later turns,\nindicating that keeping track of context helps models follow structured clinical workflows.\n0\n5\n10\n15\n20\n25\n30\n35\nNumber of Turns\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAverage Score\nTriage\nGeneral Inquiry\nInpatient Inquiry\n(a) Safety Stratification\n0\n5\n10\n15\n20\n25\n30\n35\nNumber of Turns\n0.0\n0.2\n0.4\n0.6\n0.8\nAverage Score\nTriage\nGeneral Inquiry\nInpatient Inquiry\n(b) Clarity Matters\n0\n5\n10\n15\n20\n25\n30\n35\nNumber of Turns\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAverage Score\nTriage\nGeneral Inquiry\nInpatient Inquiry\n(c) Association & Inquiry\n0\n5\n10\n15\n20\n25\n30\n35\nNumber of Turns\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nAverage Score\nTriage\nGeneral Inquiry\nInpatient Inquiry\n(d) Normative Protocol\nFigure 6: Evolution of model performance across dialogue turns.\nIn summary, while general LLMs suffice for basic information gathering, specialized training is\nindispensable for the complex reasoning and safety assurance required in clinical settings.\n4.2\nHealthBench\nHealthBench serves as a rigorous benchmark to evaluate the clinical reasoning capabilities and\nsafety boundaries of LLMs. By assessing performance across varying difficulty levels and dimen-\nsions, it provides a comprehensive view of a model‚Äôs utility in real-world medical scenarios.\n22\n"}, {"page": 23, "text": "Baichuan-M3 Technical Report\n4.2.1\nHealthBench-Main\nAs illustrated in Fig. 7, Baichuan-M3 establishes a new state-of-the-art (SOTA) standard across\nall key metrics. On the comprehensive HealthBench Total, Baichuan-M3 achieves a score of 65.1,\nsurpassing the runner-up GPT-5.2-High (63.3) by a clear margin.\nNotably, in the challenging\nHealthBench Hard subset, Baichuan-M3 further extends its lead with a score of 44.4, significantly\noutperforming strong competitors such as GPT-5.2-High (42.0) and AntAngelMed (39.6). Fur-\nthermore, Baichuan-M3 demonstrates exceptional reliability by securing the lowest Hallucination\nRate of 3.5%, indicating a robust balance between deep clinical reasoning and factual accuracy\ncompared to other models.\nFigure 7: Overall performance comparison on HealthBench.\nIn terms of fine-grained capabilities compared with M2, M3 exhibits broad gains across most\nevaluation dimensions, resulting in a more robust and well-balanced medical service profile (see\nFig. 8). The improvements are especially pronounced in context seeking and context awareness.\nWe attribute these gains to transfer from enhanced Deep Clinical Consultation training: M3\nmore proactively elicits missing history and risk factors while recognizing contextual constraints\nthat shape clinical decisions.\nAs a result, it produces more complete and reliable treatment\nrecommendations and triage assessments. Overall, these results suggest that M3 generalizes the\n‚Äúinquiry‚Äìclarification‚Äìdecision‚Äù interaction paradigm learned in consultation tasks to a broader\nset of healthcare scenarios.\n23\n"}, {"page": 24, "text": "Baichuan-M3 Technical Report\ncontext seeking\ncommunication\nemergency referrals\nhedging\nglobal health\nhealth data tasks\ncomplex responses\n0.637\n0.558\n0.711\n0.686\n0.754\n0.746\n0.678\n0.635\n0.642\n0.571\n0.534\n0.476\n0.449\n0.414\ncompleteness\ncontext awareness\naccuracy\ninstruction following\ncommunication quality\n0.699\n0.672\n0.562\n0.480\n0.689\n0.649\n0.598\n0.593\n0.623\n0.619\nM2\nM3\nFigure 8: HealthBench fine-grained comparison: M2 vs M3 (themes & axes).\n4.2.2\nHealthBench-Hallu\nWhile benchmarks such as HealthBench have established standards for evaluating medical LLMs,\nensuring clinical safety remains paramount for healthcare AI deployment. Medical hallucina-\ntions, which involve the generation of superficially plausible yet factually incorrect information,\nrepresent a significant concern acknowledged by regulatory authorities and the research com-\nmunity [17, 50, 51]. This issue is particularly pronounced in complex medical reasoning tasks,\nwhere model outputs involve extended passages with high information density, encompassing\nspecialized knowledge such as precise pharmaceutical dosing and rare clinical presentations. The\ncombination of extended generation lengths and linguistic coherence creates conditions wherein\nfine-grained factual errors may evade detection, thereby presenting substantial clinical risks. We\ntherefore introduce HealthBench-Hallu, a fine-grained evaluation framework designed to assess\nthe factual integrity of model responses generated across HealthBench tasks. By decomposing\nthese outputs into discrete atomic claims and validating them against external evidence, this\nmetric provides a rigorous quantification of medical hallucinations.\nMetric Design\nHealthBench-Hallu focuses on healthcare factual misconceptions embedded\nwithin the model‚Äôs generated content, which primarily manifest as:\n‚Ä¢ Erroneous or misapplied medical knowledge: Stating incorrect clinical facts or applying\ncorrect knowledge to inappropriate contexts.\n‚Ä¢ Fabricated or distorted medical evidence: Inventing non-existent references, falsifying clin-\n24\n"}, {"page": 25, "text": "Baichuan-M3 Technical Report\nical data, or fabricating causal relationships.\nHealthBench-Hallu Evaluation\nHealthBench-HallureusestheFact-AwareVerificationPipeline(see\nSection 2.2.2), but runs it in a high-precision (rather than high-throughput) configuration: we up-\ngrade the claim extractor to GPT-5 to improve coverage of fine-grained atomic claims, and enforce\nreal-time multi-turn search verification instead of relying on cached evidence. Based on above\nevidence labels generated by this verification pipeline, we calculate the Weighted Hallucination\nRate (H):\nH =\nPN\ni=1 wi\n|Total Claims|\n(15)\nThe weights wi are assigned based on factual risk stratification: Refuted (1.0) represents severe\nfactual fallacies; Uncertain (0.5) represents statements with insufficient evidence or ambiguity\nrisks; and Supported (0.0) denotes correct statements. This metric directly reflects the credibility\nboundaries of the model when addressing complex medical problems.\nExperimental Results\nTable 2 demonstrates that the integration of Fact-Aware RL enables\nBaichuan-M3-235B to effectively suppress hallucinations while preserving medical reasoning\ncapabilities. The model maintains competitive task performance (HealthBench Score: 65.1) com-\nparable to the variant without reinforcement learning (66.2), while substantially reducing both\nrefuted response and uncertainty rates by approximately 50%. These results indicate that the\nadaptive weighting strategy effectively mitigates the typical trade-off between safety constraints\nand model utility, avoiding the reasoning degradation commonly induced by overly conservative\napproaches.\nTable 2: Trade-off between hallucination suppression and capability\nModel\nHealthBench Score\nRefuted Rate\nUncertain Rate\nGPT-5.2-High\n63.3\n2.37%\n2.78%\nBaichuan-M2-32B\n60.1\n5.73%\n5.43%\nM3 - w/o Fact Aware RL\n66.2\n4.68%\n3.64%\nBaichuan-M3-235B\n65.1\n2.45%\n2.07%\nHallucination Analysis through Knowledge Probes\nTo elucidate the underlying mechanism by\nwhich Fact-Aware RL reduces hallucination rates, we employ knowledge probes to analyze the\n25\n"}, {"page": 26, "text": "Baichuan-M3 Technical Report\nalignment between the model‚Äôs Internal Cognition (parametric truthfulness) and its External Output\n(generated claims), as illustrated in Fig. 9. This analysis allows us to decouple the sources of error\nby examining whether the model‚Äôs output faithfully reflects its internal parameters or deviates\ndue to generation instability.\n0\n20\n40\n60\n80\n100\nPercentage of Claims (%)\nBaichuan-M3\nM3 w/o  \n Fact Aware RL\nBaichuan-M2\nGPT-5.2\n10.1%\n11.2%\n13.5%\n15.2%\n88.3%\n87.1%\n84.2%\n83.5%\nKnowledge Consistency on Supported Claims (Facts)\n0\n20\n40\n60\n80\n100\nPercentage of Claims (%)\nBaichuan-M3\nM3 w/o  \n Fact Aware RL\nBaichuan-M2\nGPT-5.2\n26.4%\n30.4%\n35.4%\n9.2%\n28.7%\n29.9%\n28.0%\n32.6%\n44.9%\n39.7%\n36.6%\n58.2%\nKnowledge Consistency on Refuted Claims (Hallucinations)\nContradictory (Knowledge contradicts Claim)\nInconsistent (Knowledge partially matches)\nConsistent (Knowledge supports Claim)\nFigure 9: Knowledge boundary alignment analysis. We categorize the alignment between in-\nternal parameters and generated claims into three states: Consistent (output aligns with internal\ncognition), Inconsistent (partial mismatch), and Contradictory (output opposes internal cognition).\nNotably, in the context of hallucinations (Refuted Claims), a Consistent state indicates an ‚Äúhonest\nerror‚Äù derived from incorrect internal knowledge.\nThe results reveal a distinct bifurcation in the model‚Äôs alignment dynamics induced by Fact-Aware\nRL. For factually correct outputs (Supported Claims), the consistency between internal cognition\nand external output remains robust at 88.3%, indicating that the model‚Äôs factual truths are firmly\nanchored in its internal parameters. More critically, for erroneous outputs (Refuted Claims), the\nconsistency rate significantly increases to 44.9%. This upward shift signifies a marked reduction in\n‚Äúunfaithful hallucinations‚Äù‚Äîinstances where the model possesses correct internal cognition but\ngenerates contradictory false information. The data suggests that the remaining hallucinations are\npredominantly honest errors, where the external output faithfully reflects a misconception inherent\nin the model‚Äôs parameters.\nUltimately, these findings imply that the primary efficacy of Fact-Aware RL lies not in injecting\nnew knowledge, but in regulating the model‚Äôs generation strategy to strictly converge within its\nauthentic knowledge boundaries.\n26\n"}, {"page": 27, "text": "Baichuan-M3 Technical Report\n5\nInference Optimization\nTo facilitate the accessibility and efficient deployment of the Baichuan-M3 model in healthcare\napplications, two inference optimized strategies are implemented. First, to accelerate text gener-\nation, the Gated Eagle-3 speculative decoding approach was introduced, leading to a substantial\nimprovement in inference throughput. Second, advanced quantization methods were applied\nto significantly reduce the memory requirements of the model without compromising accuracy.\nThese inference optimizations lower the practical barriers to deployment and support broader\napplications of advanced medical large language models.\n5.1\nSpeculative Decoding\nSpeculative decoding is an inference-time acceleration technique for autoregressive generation. It\nuses a lightweight draft model to propose multiple candidate tokens, which are then verified in\nparallel by the target model. The algorithm accepts the longest verified prefix and discards the\nremaining candidates, allowing the target model to commit multiple tokens per verification step\nand thereby increasing decoding throughput.\nIn the Eagle-3 [52] framework, the draft model is additionally conditioned on hidden states from\nthe target model to leverage richer semantic information. However, the pronounced capacity gap\nbetween the target and draft models can induce a representation mismatch: high-dimensional and\ninformation-dense hidden states may overwhelm the lightweight draft model, reducing its ability\nto effectively exploit the auxiliary signal. This often leads to lower candidate acceptance rates and\nconsequently limits the achievable speedup.\nTo mitigate this issue, we incorporate a Gated-Attention [53] module into the Eagle-3 draft model\n(called Gated Eagle-3; see Fig. 10), providing a dynamic and learnable mechanism to regulate\nthe injected information. Concretely, the attention output is routed through a gating unit and\nmodulated via element-wise multiplication with a gate vector that is dynamically generated from\nthe current-layer input. This design enables fine-grained, dimension-wise control of information\nflow, emphasizing salient features while suppressing redundant or noisy components.\nAs detailed in Appendix A.4, Gated Eagle-3 achieves an average acceptance length improvement\nof 0.31 and improves average throughput by 12% over the Eagle-3 base. These results indicate that\nselectively regulating target-model information translates more reliably into practical decoding\nacceleration, providing useful insights for future improvements.\n27\n"}, {"page": 28, "text": "Baichuan-M3 Technical Report\nElement-wise multiply\nq proj\nq\nk\nv\nGQA\no proj\nœÉ\nNorm\nGated Attention\nGated \nAttention\nMLP\nNorm\nHidden state\nEmbedding\nk proj\nv proj\nGate proj\nEmbedding\nHead\nHead\nDecoder Layer\n√óN\nTarget Model\ngate\nGated Eagle-3 Draft\nDecoder Layer\nFigure 10: An illustration of Gated Eagle-3 draft model.\n5.2\nQuantization\nTo reduce GPU memory consumption for Baichuan-M3 in resource-constrained settings, we apply\nINT4 weight quantization. However, the sparsely activated nature of Mixture-of-Experts models\nposes challenges for standard quantization calibration [54]: calibration corpora typically activate\nonly a subset of experts, resulting in biased calibration. Consequently, frequently activated experts\nare quantized accurately, whereas rarely activated experts incur larger quantization errors, which\ncan lead to unstable and unpredictable accuracy degradation at inference time.\nTo address this issue, we propose a self-generated calibration scheme that promotes uniform expert\ncoverage. Specifically, we construct a multi-domain prompt set and use the BF16 model to generate\nhigh-quality responses for calibration.\nThis approach offers two key benefits: (i) the diverse\nprompts activate nearly all experts, providing sufficient samples for each expert and mitigating\ncalibration bias; and (ii) the self-generated responses encourage the quantized model to match the\noutput distribution of the BF16 model, thereby reducing distributional discrepancies. The training\nof quantization parameters was based on AutoRound [55] framework, and the quantization format\nadheres to the GPTQ [56] standard.\nEmpirically, the resulting INT4-quantized M3 model achieves near-lossless performance relative\n28\n"}, {"page": 29, "text": "Baichuan-M3 Technical Report\nto its BF16 counterpart on mainstream benchmarks. These results validate the effectiveness of\nthe proposed MoE-specific quantization calibration strategy and provide practical guidance for\ndeploying large-scale sparse models.\n6\nConclusion\nWe introduce Baichuan-M3, a medical-enhanced large language model that unifies clinical inquiry\nwith reliable medical decision-making. By explicitly modeling the clinical workflow, Baichuan-M3\nenables proactive information acquisition, coherent multi-step reasoning, and effective hallucina-\ntion suppression. Through a three-stage training paradigm combining task-specific reinforcement\nlearning and multi-teacher distillation, the model achieves strong performance on both factual\nand process-oriented benchmarks, including HealthBench, HealthBench-Hallu and the OSCE-\nstyle ScanBench. These results demonstrate that workflow-aligned optimization is an effective\napproach for advancing clinical-grade medical LLMs.\n7\nLimitation and Future Work\nBaichuan-M3 is currently limited to episodic, text-based clinical scenarios and does not fully cap-\nture longitudinal disease management, multimodal clinical signals, or ultra-long-horizon reason-\ning across patient trajectories. While hallucination control is substantially improved, rare high-risk\nerrors and limited explicit grounding in evidence-based sources remain open challenges. Future\nwork will focus on extending the model toward full-pathway clinical reasoning with multimodal\ninputs, long-context optimization, and tighter integration of evidence retrieval, safety constraints,\nand environment-based reinforcement learning.\n8\nContribution\nContributors are presented in alphabetical order according to their first names. An asterisk (*)\ndenotes those who are no longer part of the team.\n29\n"}, {"page": 30, "text": "Baichuan-M3 Technical Report\nCore Contributors\nChengfeng Dou, Fan Yang, Fei Li, Jiyuan Jia, Qiang Ju, Shuai Wang, Tianpeng Li, Xiangrong Zeng*,\nYƒ≥ie Zhou\nContributors\nHongda Zhang, Jinyang Tai, Linzhuang Sun, Peidong Guo, Yichuan Mo\nExperts and Advisors\nXiaochuan Wang, Hengfu Cui, Zhishou Zhang\nReferences\n[1]\nOpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774. url: https://arxiv.org/abs/2303.08774.\n[2]\nAaron Jaech, Adam Kalai, Adam Lerer, et al. ‚ÄúOpenAI o1 System Card‚Äù. In: CoRR abs/2412.16720 (2024). doi:\n10.48550/ARXIV.2412.16720. arXiv: 2412.16720. url: https://doi.org/10.48550/arXiv.2412.16720.\n[3]\nAiyuan Yang, Bin Xiao, Bingning Wang, et al. ‚ÄúBaichuan 2: Open Large-scale Language Models‚Äù. In: CoRR\nabs/2309.10305 (2023). doi: 10.48550/ARXIV.2309.10305. arXiv: 2309.10305. url:\nhttps://doi.org/10.48550/arXiv.2309.10305.\n[4]\nYadong Li, Jun Liu, Tao Zhang, et al. ‚ÄúBaichuan-Omni-1.5 Technical Report‚Äù. In: CoRR abs/2501.15368 (2025).\ndoi: 10.48550/ARXIV.2501.15368. arXiv: 2501.15368. url: https://doi.org/10.48550/arXiv.2501.15368.\n[5]\nBingning Wang, Haizhou Zhao, Huozhi Zhou, et al. ‚ÄúBaichuan-M1: Pushing the Medical Capability of Large\nLanguage Models‚Äù. In: CoRR abs/2502.12671 (2025). doi: 10.48550/ARXIV.2502.12671. arXiv: 2502.12671. url:\nhttps://doi.org/10.48550/arXiv.2502.12671.\n[6]\nChengfeng Dou, Chong Liu, Fan Yang, et al. ‚ÄúBaichuan-M2: Scaling Medical Capability with Large Verifier\nSystem‚Äù. In: CoRR abs/2509.02208 (2025). doi: 10.48550/ARXIV.2509.02208. arXiv: 2509.02208. url:\nhttps://doi.org/10.48550/arXiv.2509.02208.\n[7]\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, et al. ‚ÄúMedGemma Technical Report‚Äù. In: CoRR\nabs/2507.05201 (2025). doi: 10.48550/ARXIV.2507.05201. arXiv: 2507.05201. url:\nhttps://doi.org/10.48550/arXiv.2507.05201.\n[8]\nLASA Team, Weiwen Xu, Hou Pong Chan, et al. ‚ÄúLingshu: A Generalist Foundation Model for Unified\nMultimodal Medical Understanding and Reasoning‚Äù. In: CoRR abs/2506.07044 (2025). doi:\n10.48550/ARXIV.2506.07044. arXiv: 2506.07044. url: https://doi.org/10.48550/arXiv.2506.07044.\n30\n"}, {"page": 31, "text": "Baichuan-M3 Technical Report\n[9]\nSongtao Jiang, Yuan Wang, Sibo Song, et al. ‚ÄúHulu-Med: A Transparent Generalist Model towards Holistic\nMedical Vision-Language Understanding‚Äù. In: CoRR abs/2510.08668 (2025). doi: 10.48550/ARXIV.2510.08668.\narXiv: 2510.08668. url: https://doi.org/10.48550/arXiv.2510.08668.\n[10]\nBinbin Li, Tianxin Meng, Xiaoming Shi, Jie Zhai, and Tong Ruan. ‚ÄúMedDM: LLM-executable clinical guidance\ntree for clinical decision-making‚Äù. In: CoRR abs/2312.02441 (2023). doi: 10.48550/ARXIV.2312.02441. arXiv:\n2312.02441. url: https://doi.org/10.48550/arXiv.2312.02441.\n[11]\nShengxin Hong, Liang Xiao, Xin Zhang, and Jianxia Chen. ‚ÄúArgMed-Agents: Explainable Clinical Decision\nReasoning with LLM Disscusion via Argumentation Schemes‚Äù. In: IEEE International Conference on Bioinformatics\nand Biomedicine, BIBM 2024, Lisbon, Portugal, December 3-6, 2024. Ed. by Mario Cannataro, Huiru Jane Zheng,\nLin Gao, et al. IEEE, 2024, pp. 5486‚Äì5493. doi: 10.1109/BIBM62325.2024.10822109. url:\nhttps://doi.org/10.1109/BIBM62325.2024.10822109.\n[12]\nOpenAI. Introducing GPT-5.2. https://openai.com/index/introducing-gpt-5-2/. Accessed: 2026-01-29. Dec.\n2025.\n[13]\nOpenAI. Introducing ChatGPT Health. Jan. 2026. url:\nhttps://openai.com/index/introducing-chatgpt-health.\n[14]\nAnthropic. Advancing Claude in healthcare and the life sciences. Jan. 2026. url:\nhttps://www.anthropic.com/news/healthcare-life-sciences.\n[15]\nJia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Munan Ning, and Li Yuan. ‚ÄúLLM Lies: Hallucinations are not Bugs,\nbut Features as Adversarial Examples‚Äù. In: CoRR abs/2310.01469 (2023). doi: 10.48550/ARXIV.2310.01469.\narXiv: 2310.01469. url: https://doi.org/10.48550/arXiv.2310.01469.\n[16]\nRahul K. Arora, Jason Wei, Rebecca Soskin Hicks, et al. ‚ÄúHealthBench: Evaluating Large Language Models\nTowards Improved Human Health‚Äù. In: CoRR abs/2505.08775 (2025). doi: 10.48550/ARXIV.2505.08775. arXiv:\n2505.08775. url: https://doi.org/10.48550/arXiv.2505.08775.\n[17]\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. ‚ÄúMed-HALT: Medical Domain\nHallucination Test for Large Language Models‚Äù. In: Proceedings of the 27th Conference on Computational Natural\nLanguage Learning, CoNLL 2023, Singapore, December 6-7, 2023. Ed. by Jing Jiang, David Reitter, and\nShumin Deng. Association for Computational Linguistics, 2023, pp. 314‚Äì334. doi:\n10.18653/V1/2023.CONLL-1.21. url: https://doi.org/10.18653/v1/2023.conll-1.21.\n[18]\nTao Tu, Anil Palepu, Mike Schaekermann, et al. ‚ÄúTowards Conversational Diagnostic AI‚Äù. In: CoRR\nabs/2401.05654 (2024). doi: 10.48550/ARXIV.2401.05654. arXiv: 2401.05654. url:\nhttps://doi.org/10.48550/arXiv.2401.05654.\n[19]\nElahe Vedadi, David G. T. Barrett, Natalie Harris, et al. ‚ÄúTowards physician-centered oversight of conversational\ndiagnostic AI‚Äù. In: CoRR abs/2507.15743 (2025). doi: 10.48550/ARXIV.2507.15743. arXiv: 2507.15743. url:\nhttps://doi.org/10.48550/arXiv.2507.15743.\n[20]\nDaya Guo, Dejian Yang, Haowei Zhang, et al. ‚ÄúDeepSeek-R1 incentivizes reasoning in LLMs through\nreinforcement learning‚Äù. In: Nat. 645.8081 (2025), pp. 633‚Äì638. doi: 10.1038/S41586-025-09422-Z. url:\nhttps://doi.org/10.1038/s41586-025-09422-z.\n[21]\nMingan Lin, Fan Yang, Yanjun Shen, et al. ‚ÄúBaichuan Alignment Technical Report‚Äù. In: CoRR abs/2410.14940\n(2024). doi: 10.48550/ARXIV.2410.14940. arXiv: 2410.14940. url:\nhttps://doi.org/10.48550/arXiv.2410.14940.\n31\n"}, {"page": 32, "text": "Baichuan-M3 Technical Report\n[22]\nLiChun Cao and ZhiMin. ‚ÄúAn Overview of Deep Reinforcement Learning‚Äù. In: Proceedings of the 2019 4th\nInternational Conference on Automation, Control and Robotics Engineering, CACRE 2019, Shenzhen, China, July 19-21,\n2019. ACM, 2019, 17:1‚Äì17:9. doi: 10.1145/3351917.3351989. url:\nhttps://doi.org/10.1145/3351917.3351989.\n[23]\nZƒ≥un Yao, Yantao Liu, Yanxu Chen, et al. ‚ÄúAre Reasoning Models More Prone to Hallucination?‚Äù In: CoRR\nabs/2505.23646 (2025). doi: 10.48550/ARXIV.2505.23646. arXiv: 2505.23646. url:\nhttps://doi.org/10.48550/arXiv.2505.23646.\n[24]\nChenlong Yin, Zeyang Sha, Shiwen Cui, and Changhua Meng. ‚ÄúThe Reasoning Trap: How Enhancing LLM\nReasoning Amplifies Tool Hallucination‚Äù. In: CoRR abs/2510.22977 (2025). doi: 10.48550/ARXIV.2510.22977.\narXiv: 2510.22977. url: https://doi.org/10.48550/arXiv.2510.22977.\n[25]\nZenan Huang, Yihong Zhuang, Guoshan Lu, et al. ‚ÄúReinforcement Learning with Rubric Anchors‚Äù. In: CoRR\nabs/2508.12790 (2025). doi: 10.48550/ARXIV.2508.12790. arXiv: 2508.12790. url:\nhttps://doi.org/10.48550/arXiv.2508.12790.\n[26]\nAnisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. ‚ÄúRubrics as Rewards:\nReinforcement Learning Beyond Verifiable Domains‚Äù. In: CoRR abs/2507.17746 (2025). doi:\n10.48550/ARXIV.2507.17746. arXiv: 2507.17746. url: https://doi.org/10.48550/arXiv.2507.17746.\n[27]\nJiawei Gu, Xuhui Jiang, Zhichao Shi, et al. ‚ÄúA Survey on LLM-as-a-Judge‚Äù. In: CoRR abs/2411.15594 (2024). doi:\n10.48550/ARXIV.2411.15594. arXiv: 2411.15594. url: https://doi.org/10.48550/arXiv.2411.15594.\n[28]\nHaoyang Li, Yiming Li, Anxin Tian, et al. ‚ÄúA Survey on Large Language Model Acceleration based on KV Cache\nManagement‚Äù. In: Trans. Mach. Learn. Res. 2025 (2025). url: https://openreview.net/forum?id=z3JZzu9EA3.\n[29]\nSewon Min, Kalpesh Krishna, Xinxi Lyu, et al. ‚ÄúFActScore: Fine-grained Atomic Evaluation of Factual Precision\nin Long Form Text Generation‚Äù. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2023, Singapore, December 6-10, 2023. Ed. by Houda Bouamor, Juan Pino, and Kalika Bali.\nAssociation for Computational Linguistics, 2023, pp. 12076‚Äì12100. doi: 10.18653/V1/2023.EMNLP-MAIN.741.\nurl: https://doi.org/10.18653/v1/2023.emnlp-main.741.\n[30]\nJerry Wei, Chengrun Yang, Xinying Song, et al. ‚ÄúLong-form factuality in large language models‚Äù. In: Advances in\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS\n2024, Vancouver, BC, Canada, December 10 - 15, 2024. Ed. by Amir Globersons, Lester Mackey, Danielle Belgrave,\net al. 2024. url:\nhttp://papers.nips.cc/paper_files/paper/2024/hash/937ae0e83eb08d2cb8627fe1def8c751-Abstract-\nConference.html.\n[31]\nAaditya Singh, Adam Fry, Adam Perelman, et al. ‚ÄúOpenai gpt-5 system card‚Äù. In: CoRR (2025). arXiv:\n2601.03267.\n[32]\nSachin Ravi, Sebastian Musslick, Maia Hamin, Theodore L. Willke, and Jonathan D. Cohen. ‚ÄúNavigating the\nTrade-Off between Multi-Task Learning and Learning to Multitask in Deep Neural Networks‚Äù. In: CoRR\nabs/2007.10527 (2020). arXiv: 2007.10527. url: https://arxiv.org/abs/2007.10527.\n[33]\nYichuan Mo and Shilin Wang. ‚ÄúMulti-Task Learning Improves Synthetic Speech Detection‚Äù. In: IEEE\nInternational Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May\n2022. IEEE, 2022, pp. 6392‚Äì6396. doi: 10.1109/ICASSP43922.2022.9746059. url:\nhttps://doi.org/10.1109/ICASSP43922.2022.9746059.\n32\n"}, {"page": 33, "text": "Baichuan-M3 Technical Report\n[34]\nYisen Wang, Yichuan Mo, Hongjun Wang, Junyi Li, and Zhouchen Lin. ‚ÄúGeneralist++: A Meta-learning\nFramework for Mitigating Trade-off in Adversarial Training‚Äù. In: CoRR abs/2510.13361 (2025). doi:\n10.48550/ARXIV.2510.13361. arXiv: 2510.13361. url: https://doi.org/10.48550/arXiv.2510.13361.\n[35]\nLLM-Core Xiaomi. MiMo-V2-Flash Technical Report. 2025. url:\nhttps://github.com/XiaomiMiMo/MiMo-V2-Flash/paper.pdf.\n[36]\nKevin Lu and Thinking Machines Lab. ‚ÄúOn-Policy Distillation‚Äù. In: Thinking Machines Lab: Connectionism (2025).\nhttps://thinkingmachines.ai/blog/on-policy-distillation. doi: 10.64434/tml.20251026.\n[37]\nJie Liu, Wenxuan Wang, Zizhan Ma, et al. Medchain: Bridging the Gap Between LLM Agents and Clinical Practice\nwith Interactive Sequence. 2025. arXiv: 2412.01605 [cs.CL]. url: https://arxiv.org/abs/2412.01605.\n[38]\nKangenbei Liao, Qianlong Liu, Zhongyu Wei, et al. ‚ÄúTask-oriented Dialogue System for Automatic Disease\nDiagnosis via Hierarchical Reinforcement Learning‚Äù. In: CoRR abs/2004.14254 (2020). arXiv: 2004.14254. url:\nhttps://arxiv.org/abs/2004.14254.\n[39]\nChengfeng Dou, Ying Zhang, Zhi Jin, et al. ‚ÄúIntegrating Physician Diagnostic Logic into Large Language\nModels: Preference Learning from Process Feedback‚Äù. In: Findings of the Association for Computational Linguistics,\nACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024. Ed. by Lun-Wei Ku, Andre Martins, and\nVivek Srikumar. Vol. ACL 2024. Findings of ACL. Association for Computational Linguistics, 2024,\npp. 2453‚Äì2473. doi: 10.18653/V1/2024.FINDINGS-ACL.144. url:\nhttps://doi.org/10.18653/v1/2024.findings-acl.144.\n[40]\nShihui Yang, Chengfeng Dou, Peidong Guo, et al. ‚ÄúDCPO: Dynamic Clipping Policy Optimization‚Äù. In: CoRR\nabs/2509.02333 (2025). doi: 10.48550/ARXIV.2509.02333. arXiv: 2509.02333. url:\nhttps://doi.org/10.48550/arXiv.2509.02333.\n[41]\nRenrui Zhang, Chengzhuo Tong, Zhizheng Zhao, et al. ‚ÄúLet‚Äôs Verify and Reinforce Image Generation Step by\nStep‚Äù. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June\n11-15, 2025. Computer Vision Foundation / IEEE, 2025, pp. 28662‚Äì28672. doi: 10.1109/CVPR52734.2025.02669.\nurl: https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Lets_Verify_and_Reinforce_Image_\nGeneration_Step_by_Step_CVPR_2025_paper.html.\n[42]\nChujie Zheng, Shixuan Liu, Mingze Li, et al. ‚ÄúGroup Sequence Policy Optimization‚Äù. In: CoRR abs/2507.18071\n(2025). doi: 10.48550/ARXIV.2507.18071. arXiv: 2507.18071. url:\nhttps://doi.org/10.48550/arXiv.2507.18071.\n[43]\nYoshua Bengio, J√©r√¥me Louradour, Ronan Collobert, and Jason Weston. ‚ÄúCurriculum learning‚Äù. In: Proceedings\nof the 26th annual international conference on machine learning. 2009, pp. 41‚Äì48.\n[44]\nRulin Shao, Akari Asai, Shannon Zejiang Shen, et al. ‚ÄúDR Tulu: Reinforcement Learning with Evolving Rubrics\nfor Deep Research‚Äù. In: CoRR abs/2511.19399 (2025). doi: 10.48550/ARXIV.2511.19399. arXiv: 2511.19399. url:\nhttps://doi.org/10.48550/arXiv.2511.19399.\n[45]\nJunkai Zhang, Zihao Wang, Lin Gui, et al. ‚ÄúChasing the Tail: Effective Rubric-based Reward Modeling for Large\nLanguage Model Post-Training‚Äù. In: CoRR abs/2509.21500 (2025). doi: 10.48550/ARXIV.2509.21500. arXiv:\n2509.21500. url: https://doi.org/10.48550/arXiv.2509.21500.\n[46]\nXilun Chen, Ilia Kulikov, Vincent-Pierre Berges, et al. ‚ÄúLearning to Reason for Factuality‚Äù. In: CoRR\nabs/2508.05618 (2025). doi: 10.48550/ARXIV.2508.05618. arXiv: 2508.05618. url:\nhttps://doi.org/10.48550/arXiv.2508.05618.\n33\n"}, {"page": 34, "text": "Baichuan-M3 Technical Report\n[47]\nDeepSeek-AI and collaborators. ‚ÄúDeepSeek-V3.2: Pushing the Frontier of Open Large Language Models‚Äù. In:\narXiv preprint arXiv:2512.02556 (2025). DeepSeek-V3.2 thinking-enhanced reasoning model technical report;\naccessed 2026-01-29. url: https://arxiv.org/abs/2512.02556.\n[48]\nAn Yang, Anfeng Li, Baosong Yang, et al. Qwen3 Technical Report. 2025. doi: 10.48550/ARXIV.2505.09388. arXiv:\n2505.09388. url: https://doi.org/10.48550/arXiv.2505.09388.\n[49]\nAntAngelMed Team / Ant Group Health AI. AntAngelMed: Open-Source Medical Language Model.\nhttps://github.com/MedAIBase/AntAngelMed. Open-source medical reasoning model with MoE architecture\nand HealthBench-leading performance; accessed 2026-01-29. 2026.\n[50]\nYubin Kim, Hyewon Jeong, Shan Chen, et al. ‚ÄúMedical Hallucinations in Foundation Models and Their Impact\non Healthcare‚Äù. In: CoRR abs/2503.05777 (2025). doi: 10.48550/ARXIV.2503.05777. arXiv: 2503.05777. url:\nhttps://doi.org/10.48550/arXiv.2503.05777.\n[51]\nItay Manes, Naama Ronn, David Cohen, Ran Ilan Ber, Zehavi Horowitz-Kugler, and Gabriel Stanovsky. ‚ÄúK-QA:\nA Real-World Medical Q&A Benchmark‚Äù. In: Proceedings of the 23rd Workshop on Biomedical Natural Language\nProcessing, BioNLP@ACL 2024, Bangkok, Thailand, August 16, 2024. Ed. by Dina Demner-Fushman,\nSophia Ananiadou, Makoto Miwa, Kirk Roberts, and Junichi Tsujii. Association for Computational Linguistics,\n2024, pp. 277‚Äì294. doi: 10.18653/V1/2024.BIONLP-1.22. url:\nhttps://doi.org/10.18653/v1/2024.bionlp-1.22.\n[52]\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. ‚ÄúEAGLE-3: Scaling up Inference Acceleration of\nLarge Language Models via Training-Time Test‚Äù. In: CoRR abs/2503.01840 (2025). doi:\n10.48550/ARXIV.2503.01840. arXiv: 2503.01840. url: https://doi.org/10.48550/arXiv.2503.01840.\n[53]\nZihan Qiu, Zekun Wang, Bo Zheng, et al. ‚ÄúGated Attention for Large Language Models: Non-linearity, Sparsity,\nand Attention-Sink-Free‚Äù. In: CoRR abs/2505.06708 (2025). doi: 10.48550/ARXIV.2505.06708. arXiv:\n2505.06708. url: https://doi.org/10.48550/arXiv.2505.06708.\n[54]\nZihao Zheng, Xiuping Cui, Size Zheng, et al. ‚ÄúMoQa: Rethinking MoE Quantization with Multi-stage\nData-model Distribution Awareness‚Äù. In: CoRR abs/2503.21135 (2025). doi: 10.48550/ARXIV.2503.21135.\narXiv: 2503.21135. url: https://doi.org/10.48550/arXiv.2503.21135.\n[55]\nWenhua Cheng, Weiwei Zhang, Haihao Shen, et al. ‚ÄúOptimize Weight Rounding via Signed Gradient Descent\nfor the Quantization of LLMs‚Äù. In: Findings of the Association for Computational Linguistics: EMNLP 2024, Miami,\nFlorida, USA, November 12-16, 2024. Ed. by Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen. Vol. EMNLP\n2024. Findings of ACL. Association for Computational Linguistics, 2024, pp. 11332‚Äì11350. doi:\n10.18653/V1/2024.FINDINGS-EMNLP.662. url: https://doi.org/10.18653/v1/2024.findings-emnlp.662.\n[56]\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. ‚ÄúGPTQ: Accurate Post-Training Quantization\nfor Generative Pre-trained Transformers‚Äù. In: CoRR abs/2210.17323 (2022). doi: 10.48550/ARXIV.2210.17323.\narXiv: 2210.17323. url: https://doi.org/10.48550/arXiv.2210.17323.\n[57]\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, et al. ‚ÄúTraining Verifiers to Solve Math Word Problems‚Äù. In:\nCoRR abs/2110.14168 (2021). arXiv: 2110.14168. url: https://arxiv.org/abs/2110.14168.\n[58]\nMark Chen, Jerry Tworek, Heewoo Jun, et al. ‚ÄúEvaluating Large Language Models Trained on Code‚Äù. In: CoRR\nabs/2107.03374 (2021). arXiv: 2107.03374. url: https://arxiv.org/abs/2107.03374.\n34\n"}, {"page": 35, "text": "Baichuan-M3 Technical Report\n[59]\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, et al. ‚ÄúJudging LLM-as-a-Judge with MT-Bench and Chatbot\nArena‚Äù. In: Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information\nProcessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Ed. by Alice Oh,\nTristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine. 2023. url:\nhttp://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-\nDatasets_and_Benchmarks.html.\n[60]\nShenggui Li, Yikai Zhu, Chao Wang, et al. SpecForge: Train speculative decoding models effortlessly.\nhttps://github.com/sgl-project/specforge. 2025.\n[61]\nLianmin Zheng, Liangsheng Yin, Zhiqiang Xie, et al. ‚ÄúSGLang: Efficient Execution of Structured Language\nModel Programs‚Äù. In: Advances in Neural Information Processing Systems 38: Annual Conference on Neural\nInformation Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Ed. by\nAmir Globersons, Lester Mackey, Danielle Belgrave, et al. 2024. url:\nhttp://papers.nips.cc/paper_files/paper/2024/hash/724be4472168f31ba1c9ac630f15dec8-Abstract-\nConference.html.\n35\n"}, {"page": 36, "text": "Baichuan-M3 Technical Report\nA\nAppendix\nThis appendix presents supplementary experimental results and detailed analyses to further vali-\ndate our proposed methods. Specifically, we provide ablation studies on the SPAR algorithm and\nthe Clip-Forward-KL objective, along with extended evaluations of the Fact-Aware RL framework\nand the Gated Eegle-3 speculative decoding mechanism. For comprehensive details regarding\nthe specific prompts utilized for training and evaluation, please refer to the associated GitHub\nrepository.\nA.1\nAblation Study of SPAR\nRepeat Score\nLogical Score\nRubrics Score\nAvg Turns\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Score (0-1)\n0.99\n0.65\n0.47\n0.63\n0.55\n0.67\n0.90\n0.28\n0.69\n0.95\n0.51\n0.76\nbaseline\ngrpo\nglobal penalty\nbaichuan-m3\n0\n5\n10\n15\n20\n25\nAverage Turns\n10.4\n21.7\n20.9\n20.4\nFigure 11: Performance comparison between SPAR and baseline models in multi-turn medical\nconsultation tasks.\nTo systematically evaluate the efficacy of the SPAR algorithm in multi-turn medical consultation\ntasks, we conducted ablation studies using the following experimental configurations:\n1. Backbone: We use the Baichuan-M3 base model (before RL) as the foundational architecture.\n2. GRPO (Global Reward): Employs the GRPO algorithm with global rewards based solely on\nfinal consultation outcomes, excluding intermediate feedback.\n3. Global Penalty: Extends the GRPO baseline by incorporating a global repetition penalty to\nmitigate conversational redundancy.\n4. SPAR (Baichuan-M3): Implements the proposed step-level reward calculation algorithm to\nprovide granular guidance.\nMetrics: The evaluation framework comprises four metrics normalized to the [0, 1] interval: Repeat\n36\n"}, {"page": 37, "text": "Baichuan-M3 Technical Report\nScore (measuring non-redundancy), Logical Score, Rubrics Score, and Average Turns (Avg Turns).\nSpecifically, the first two metrics are derived by employing GPT-5 to evaluate dialogue logs on a\nthree-point scale (0, 1, 2), with the resulting scores subsequently normalized.\nResults: As shown in Fig. 11, training with the GRPO algorithm using only global rewards yields\na steady improvement in Rubrics Score, but it also increases redundant inquiries (as reflected\nby the decline in Repeat Score). Although adding a global repetition penalty effectively reduces\nredundancy, it causes a sharp drop in Logical Score, indicating severe logical fragmentation. These\nresults suggest that coarse-grained global penalties can substantially disrupt the model‚Äôs natural\nreasoning flow. In contrast, SPAR achieves a better balance: it markedly reduces repetition while\npreserving logical coherence. This dual optimization allows the model to extract a higher density\nof critical medical information within a limited number of consultation turns.\nA.2\nFact-Aware RL\nA.2.1\nEvaluation of Distilled Claim Extraction Models\nWhile offline claim extraction relies on GPT-5 as the reference extractor, deploying such a large\nteacher model for online reinforcement learning is computationally prohibitive. To address this,\nwe fine-tune smaller models via Supervised Fine-Tuning (SFT) to serve as efficient claim extractors\nduring online RL. We evaluate extraction fidelity using three metrics relative to GPT-5:\n‚Ä¢ Recall: Evaluates the coverage of the SFT model relative to the reference baseline (GPT-5).\n‚Ä¢ SFT Exclusivity Rate: The proportion of claims identified by the SFT model that are absent\nin the GPT-5 reference, reflecting potential over-generation or discovery of novel insights.\n‚Ä¢ GPT Exclusivity Rate: The proportion of claims identified by GPT-5 that remain uncaptured\nby the SFT model, indicating information loss.\nTable 3: Comparative experiments on claim extraction models across different scales.\nModel\nRecall (%)\nOur Exclusive Rate (%)\nGPT Exclusive Rate (%)\nClaim Count\nQwen3-8B\n30.45\n31.02\n69.55\n8.50\nQwen3-32B\n37.68\n33.29\n62.32\n12.81\nSFT-A3B-30B\n69.69\n46.89\n30.31\n47.19\nSFT-8B\n72.80\n45.60\n27.20\n46.66\nSFT-32B\n73.00\n47.15\n27.01\n46.78\n37\n"}, {"page": 38, "text": "Baichuan-M3 Technical Report\nAs shown in Table 3, SFT substantially improves extraction performance, with the 8B model\nachieving 72.80% recall compared to 30.45% for the untuned baseline.\nWhile the 32B variant\nprovides marginal performance gains, the significantly higher deployment cost is not justified for\nthe online RL pipeline. We adopt the SFT-8B model as the claim extractor, balancing extraction\nfidelity with practical deployment constraints.\nA.2.2\nAblation Study on Reward Components\nWe investigate the impact of different reward shaping strategies by comparing our method against\ntwo control groups starting from an intermediate SFT checkpoint: a w/o Fact Aware RL model\noptimizing only task rewards, and a Baseline utilizing static hallucination penalties in Eq. (9).\n0\n50\n100\n150\n200\n250\n300\nSteps\n0.64\n0.65\n0.66\n0.67\n0.68\nHealthBench Score\nw/o Fact Aware RL\nBaseline\nDenoise & Reweight\n0\n50\n100\n150\n200\n250\n300\nSteps\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nHallucination Rate\nw/o Fact Aware RL\nBaseline\nDenoise & Reweight\nFigure 12: Training dynamics of different optimization strategies. The plots contrast the impact of\neach objective on medical reasoning capability (Left) and factual reliability (Right).\nFigure 12 highlights distinct behavioral patterns driven by these objectives. The w/o Fact Aware RL\nmodel (Grey) achieves the highest HealthBench score (‚àº0.68) but suffers from significant hallu-\ncination drift (increasing to 0.08). This observation suggests that an unconstrained optimization\nobjective tends to bias the model toward generating expansive content, which may inadvertently\ncompromise factual stability. The Baseline (Blue) corrects this drift but overcompensates; its ag-\ngressive suppression of hallucinations results in a severe degradation of reasoning capability,\ncharacteristic of penalty-induced conservatism.\nOur Denoise & Reweight strategy in Eq. (14) effectively decouples safety alignment from capability\nloss. It achieves a hallucination reduction rate comparable to the Baseline (dropping to ‚àº0.035)\nwhile maintaining reasoning scores (‚àº0.665) close to the unconstrained model‚Äôs starting point.\n38\n"}, {"page": 39, "text": "Baichuan-M3 Technical Report\nThis stability verifies the efficacy of our dual-protection design, where marginal noise filtering and\ncompetence-based gating work in tandem. Consequently, the approach successfully guides the\nmodel toward factual correctness while preserving its core medical utility.\nA.3\nAblation Study on Clip-Forward-KL for Offline Expert Fusion\nWe analyze the role of Clip-Forward-KL in offline expert fusion. The student model is initialized\nfrom a medical inquiry expert and incorporates a healthcare expert through offline distillation. Both\ninquiry and healthcare data are distilled using either Clip-Forward-KL or standard Forward-KL,\nunder identical initialization, datasets, and training configurations.\nTable 4 reports results on ScanBench, HealthBench, and HealthBench-Hard. ScanBench primarily\nreflects the preservation of medical inquiry capability, while HealthBench and HealthBench-Hard\nassess the effectiveness of integrating broader healthcare expertise.\nTable 4: Ablation on Clip-Forward-KL for offline expert fusion.\nClip-Forward-KL improves\nHealthBench and HealthBench-Hard while maintaining comparable ScanBench performance un-\nder identical training configurations.\nMethod\nScanBench\nHealthBench\nHealthBench-Hard\nForward-KL\n73.7\n58.6\n33.2\nClip-Forward-KL\n73.5\n61.1\n38.5\nClip-Forward-KL preserves inquiry performance while substantially improving healthcare bench-\nmarks, indicating more effective fusion of new domain expertise. This suggests that standard\nForward-KL tends to over-amplify probabilities on sparse offline samples, which interferes with\nthe integration of new healthcare capabilities rather than the retention of existing inquiry ability.\nBy enforcing a one-sided lower-bound constraint on teacher-supported actions, Clip-Forward-KL\nenables a more conservative fusion and yields an initialization for on-policy optimization.\nA.4\nAblation Study on Gated Eagle-3 Speculative Decoding.\nTo ensure a rigorous and fair comparison, we evaluated the original Eagle-3 draft model against\nour proposed Gated Eagle-3 using an identical dataset of 35,000 training samples and the same\ntraining protocol.\nBoth models were assessed under uniform inference configurations across\na suite of representative benchmarks, including GSM8K [57], HumanEval [58], MT-Bench [59],\n39\n"}, {"page": 40, "text": "Baichuan-M3 Technical Report\nand HealthBench. All experiments were conducted on NVIDIA H20 GPUs, with model training\nperformed using the SpecForge [60] framework and inference executed via SGLang [61]. As shown\nin Table 5, Gated Eagle-3 achieves an average acceptance length improvement of 0.31 over the Eagle-\n3 base. Moreover, throughput comparisons conducted under a parallelism of 8 are presented in\nTable 6.\nTable 5: Comparison of average acceptance length between Eagle-3 Base and Gated Eagle-3.\nMethod\nGSM8K\nHumanEval\nMT-Bench\nHealthBench\nEagle-3 Base\n3.59\n3.58\n2.97\n2.41\nGated Eagle-3\n3.84\n4.03\n3.23\n2.70\n‚àÜ(Gain)\n+0.25\n+0.45\n+0.26\n+0.29\nTable 6: Comparison of throughput (tokens/s) between Eagle-3 Base and Gated Eagle-3.\nMethod\nGSM8K\nHumanEval\nMT-Bench\nHealthBench\nEagle-3 Base\n376.98\n576.19\n451.39\n356.94\nGated Eagle-3\n431.26\n646.17\n490.12\n400.52\n‚àÜ(Gain)\n+14.40%\n+12.15%\n+8.58%\n+12.21%\n40\n"}]}