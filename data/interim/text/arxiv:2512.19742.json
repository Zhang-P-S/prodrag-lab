{"doc_id": "arxiv:2512.19742", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.19742.pdf", "meta": {"doc_id": "arxiv:2512.19742", "source": "arxiv", "arxiv_id": "2512.19742", "title": "On-device Large Multi-modal Agent for Human Activity Recognition", "authors": ["Md Shakhrul Iman Siam", "Ishtiaque Ahmed Showmik", "Guanqun Song", "Ting Zhu"], "published": "2025-12-17T22:05:05Z", "updated": "2025-12-17T22:05:05Z", "summary": "Human Activity Recognition (HAR) has been an active area of research, with applications ranging from healthcare to smart environments. The recent advancements in Large Language Models (LLMs) have opened new possibilities to leverage their capabilities in HAR, enabling not just activity classification but also interpretability and human-like interaction. In this paper, we present a Large Multi-Modal Agent designed for HAR, which integrates the power of LLMs to enhance both performance and user engagement. The proposed framework not only delivers activity classification but also bridges the gap between technical outputs and user-friendly insights through its reasoning and question-answering capabilities. We conduct extensive evaluations using widely adopted HAR datasets, including HHAR, Shoaib, Motionsense to assess the performance of our framework. The results demonstrate that our model achieves high classification accuracy comparable to state-of-the-art methods while significantly improving interpretability through its reasoning and Q&A capabilities.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.19742v1", "url_pdf": "https://arxiv.org/pdf/2512.19742.pdf", "meta_path": "data/raw/arxiv/meta/2512.19742.json", "sha256": "02191b6348ece060f1f974bc21f91940780b512c4955769d6da263ac977b4a79", "status": "ok", "fetched_at": "2026-02-18T02:24:12.403323+00:00"}, "pages": [{"page": 1, "text": "On-device Large Multi-modal Agent for\nHuman Activity Recognition\nMd Shakhrul Iman Siam\nThe Ohio State University\nColumbus, Ohio, USA\nsiam.5@osu.edu\nIshtiaque Ahmed Showmik\nThe Ohio State University\nColumbus, Ohio, USA\nshowmik.1@osu.edu\nGuanqun Song\nThe Ohio State University\nColumbus, Ohio, USA\nsong.2107@osu.edu\nTing Zhu\nThe Ohio State University\nColumbus, Ohio, USA\nzhu.3445@osu.edu\nAbstract—Human Activity Recognition (HAR) has been an\nactive area of research, with applications ranging from healthcare\nto smart environments. The recent advancements in Large Lan-\nguage Models (LLMs) have opened new possibilities to leverage\ntheir capabilities in HAR, enabling not just activity classification\nbut also interpretability and human-like interaction. In this\npaper, we present a Large Multi-Modal Agent designed for\nHAR, which integrates the power of LLMs to enhance both\nperformance and user engagement. The proposed framework\nnot only delivers activity classification but also bridges the gap\nbetween technical outputs and user-friendly insights through\nits reasoning and question-answering capabilities. We conduct\nextensive evaluations using widely adopted HAR datasets, in-\ncluding HHAR, Shoaib, Motionsense to assess the performance\nof out framework. The results demonstrate that our model\nachieves high classification accuracy comparable to state-of-\nthe-art methods while significantly improving interpretability\nthrough its reasoning and Q&A capabilities.\nIndex Terms—Human Activity Recognition, Large Language\nModels\nI. INTRODUCTION\nHuman Activity Recognition (HAR) has emerged as a\ncrucial task in various applications, ranging from healthcare\nand fitness monitoring to smart home automation and in-\ndustrial safety. Traditional approaches to HAR [3], such as\nRandom Forest (RF), Long Short-Term Memory (LSTM), and\nRecurrent Neural Networks (RNN), have gained popularity\ndue to their ability to model temporal and spatial patterns\nin sensor data effectively [52]. However, these methods often\ncome with limitations, including being task-specific and facing\nsignificant challenges in scalability when applied to diverse\nor varying sensor environments [56], [59]. Addressing these\nchallenges is crucial for advancing HAR systems toward more\nrobust and generalized performance.\nRecent advancements in Large Language Models (LLMs)\nhave revolutionized fields like computer vision and natural\nlanguage processing (NLP), demonstrating their generaliza-\ntion and adaptability [31]. Their inherent ability to extract\nmeaningful patterns and reason contextually positions them\nas powerful tools for complex problem-solving. Despite their\ntransformative impact in these domains, the potential of LLMs\nremains relatively underexplored in the context of sensor data,\npresenting a research direction to explore in order to overcome\nthe limitations of traditional HAR methodologies. The flexibil-\nity and adaptability of LLMs present an opportunity to address\nthese challenges by integrating their reasoning and interpretive\ncapabilities with sensor-based systems.\nHowever, integrating Large Language Models with time-\nseries data like IMU reading is challenging because time-\nseries data lacks the inherent semantic structure and context\npresent in natural language or visual data, which LLMs are\nprimarily designed to process [23]. Unlike text or images,\ntime-series data consists of numerical sequences representing\nchanges over time, making it difficult for LLMs to inter-\npret patterns without additional pre-processing or contextual\nalignment. Moreover, the high dimensionality, varying lengths,\nand multi-channel nature of time-series data can overwhelm\nstandard LLM architectures, which are optimized for fixed-\nlength tokenized inputs.\nTo address the gap between sensory data and language\nmodels, several research efforts [21], [22], [32], [57] have\nattempted to integrate multimodal data into AI systems. How-\never, these approaches face significant limitations. One of the\nprimary challenges is aligning two fundamentally different\nmodalities—numerical time-series data from sensors and text-\nbased data designed for language models. While some studies\nhave proposed methods to bridge this gap, their solutions\noften fall short in terms of effectiveness. Moreover, many of\nthese approaches are not suitable for mobile or edge devices,\nas they rely on server-based architectures that demand high\ncomputational resources. This dependency not only limits\ntheir real-time applicability but also raises privacy concerns,\nas sensitive user data must be transmitted and processed\non external servers. These issues underscore the need for\nnovel, lightweight, and privacy-preserving solutions that can\neffectively align sensory and textual data for robust, real-world\napplications. In this paper, we propose a framework leveraging\nthe capabilities of LLMs to classify human activities from raw\nsensor data while providing reasoning for its decisions. The\nframework is designed to support both open-ended question-\nand-answer tasks and close-ended classification tasks. Further-\nmore, we generate instruction pairs to fine-tune a smaller,\nmobile-friendly LLM for real-time, on-device activity recogni-\ntion and reasoning, paving the way for scalable and accessible\nHAR solutions.\nTo summarize, our work makes the following core contri-\nbutions:\n• We developed a framework that can classify Human\narXiv:2512.19742v1  [cs.LG]  17 Dec 2025\n"}, {"page": 2, "text": "Based on my IMU data from my \nsmartwatch, can you identify the activity I \nwas doing between 4:00pm to 4:05pm?\nBased on the data provided, You were \nwalking. \nCan you analyze the activity?\nYour accelerometer data shows \nrepetitive oscillations, which indicates \nwalking movement pattern. There are \nalso significant presence of peaks \ncorrespond to arm swing in the direction \nof movement.\nHow many hours have I \nwalked in the past day?\nYou have walked a total of 47 minutes in \nthe past day\nDo you have any recommendation \nabout my fitness and daily activity?\nAim for at least 50 minutes of \nmoderate-intensity exercise daily. Over \nthe past week, you've been getting less \nsleep than recommended, so prioritize \n7–8 hours of quality sleep to help your \nbody recover and recharge.\nFig. 1: The proposed framework is capable of classifying\nHuman activity, providing reasoning, and performing QnA\ntasks.\nActivity as well as provide reasoning about its decision-\nmaking.\n• Our framework has the question-answering capabilities.\n• We fine-tune a computation-friendly smaller language\nmodel that can run on a mobile device.\nII. RELATED WORK\nA. Multimodal Sensing for Smart Health\nHuman Activity Recognition (HAR) models typically fol-\nlow the Activity Recognition Chain [4]. While initial research\nrelied on tree-based methods [18], [60] and wearable inertial\nsensors, the scope of sensing has expanded significantly to\nencompass diverse, non-intrusive modalities for smart health\napplications.\nPassive and device-free sensing approaches have been ex-\ntensively explored to capture physiological and behavioral\ndata without requiring active user intervention. For instance,\nDoppler radar has been utilized to recognize gestures [38]\nand monitor medication interactions [39] while preserving\nuser privacy. Similarly, acoustic-based systems have been\ndeveloped for passive fetal heart monitoring [62], and other\nspecialized sensing frameworks have been applied to blood\nglucose control [12] and pulmonary nodule detection [37].\nMore recently, multimodal systems fusing magnetic sensing\nwith WiFi infrastructure have been proposed to achieve precise\nbiometric tracking and metal detection [46]. Additionally,\nadvanced visual processing techniques, such as efficient se-\nmantic segmentation on edge devices [42], further demonstrate\nthe efficacy of combining diverse signals for robust activity\nanalysis.\nB. Efficient IoT Infrastructure for HAR\nDeploying HAR models in real-world scenarios requires a\nrobust and energy-efficient Internet of Things (IoT) infrastruc-\nture to ensure reliable data transmission from edge devices.\nSignificant research has focused on optimizing connectivity in\nheterogeneous network environments.\nTo enable low-latency data transfer between incompatible\nwireless standards, cross-technology communication (CTC)\ntechniques have been developed. Systems enabling concurrent\nhigh-throughput communication between WiFi and ZigBee\n[7], [29] allow for real-time sensing data coordination. Fur-\nthermore, spectral efficiency improvement methods [9], [54]\nand bi-directional communication frameworks [8] ensure that\nactivity data is delivered reliably even in crowded spectrums.\nAddressing energy constraints, ultra-low-power methods like\nbackscatter communication [10], [30] and OFDM-based pro-\ntocols [34] have gained prominence. Beyond terrestrial net-\nworks, research has extended to extreme edge environments,\noptimizing energy efficiency for LoRaWAN in LEO satellites\n[43] and managing thermal constraints in space-based com-\nputing [65], or even exploring global quantum communication\nnetworks [14].\nTo support the massive data processing required by these\nsystems, heterogeneous computing frameworks [25] and mul-\ntiprocessing strategies for data classification and map-reduce\ntasks [11], [41] have been introduced. Moreover, recent studies\nemphasize the environmental impact of IoT deployment, inves-\ntigating carbon neutrality [63] and the economic implications\nof hardware obsolescence [5], [17] to promote sustainable\nsmart health ecosystems.\nC. Security and Trustworthiness in Sensing\nAs HAR systems are increasingly integrated into critical\napplications, ensuring security across the entire stack—from\nthe physical layer to the operating system—is paramount.\nVulnerabilities in the physical layer have been identified\nacross various modalities. Risks in optical communication\nhave been revealed alongside potential defenses [35], and ma-\nchine learning-based frameworks have been proposed to secure\ncommunication in adversarial contexts [47], [48]. Additionally,\nthreats such as wireless jamming [6], [19] and invisible light\nattacks [55] highlight the need for robust defenses.\nBeyond physical attacks, system-level integrity is critical.\nResearch has analyzed security risks in OS-level virtualization\n[24] and the inner workings of operating system security [26].\nTo ensure data reliability, secure virtual file system imple-\nmentations [50] are essential. Furthermore, network-level pri-\nvacy challenges in microservices architectures [16], blockchain\nscalability [28], and location tracking in 5G networks [1] must\nbe addressed to protect user identity while maintaining system\nperformance [27], [51], [53], [61].\n"}, {"page": 3, "text": "D. Large Language Models for HAR\nAdvancements in Large Language Models (LLMs) have\nintroduced a transformative paradigm for interpreting sensor\ndata [45]. Zero-shot capabilities have been investigated for\nclassifying activities directly from raw IMU data [22]. Other\napproaches leverage evolutionary strategies for unsupervised\nlearning [13] or utilize multimodal agents to interpret human\nactivity queries [21]. To handle the discrepancy between\nnumerical sensor data and text, tokenization strategies have\nalso been proposed [32]. As these models grow in complexity,\ncomprehensive benchmarking remains essential for evaluating\ntheir effectiveness in edge environments [40].\nIII. DESIGN\nA. Dataset\nTo facilitate our experiments, we utilized the following\ndatasets:\n1) Shoaib Dataset: The Shoaib Dataset [44] is for human\nactivity recognition and sensor positioning studies. Data were\ncollected from four Samsung Galaxy S2 smartphones placed\non the arm, wrist, belt and pocket of the participants. Four\nmale volunteers aged 25–30 performed six activities: walk-\ning, running, sitting, standing, walking upstairs, and walk-\ning downstairs. Each activity was recorded for 3–5 minutes\nusing accelerometer, gyroscope and magnetometer sensors,\ncapturing 3 axial readings for each modality. It is a time-\nseries dataset comprising features as time stamp, acceleration,\nangular velocity, and magnetic field.\n2) Heterogeneity Dataset for Human Activity Recognition\n(HHAR): The Heterogeneity Dataset for Human Activity\nRecognition (HHAR) [49] addresses sensor heterogeneities by\nincorporating data collected from nine participants performing\nsix activities: biking, sitting, standing, walking, walking up-\nstairs, and walking downstairs. Sensor readings were recorded\nusing multiple smartphones, including Samsung Galaxy S3,\nSamsung Galaxy S3 Mini, and LG Nexus 4, as well as\nsmartwatches like LG Watch and Samsung Galaxy Gear.\n3) UCI Human Activity Recognition (HAR) Using Smart-\nphones Dataset: The UCI Human Activity Recognition (HAR)\nUsing Smartphones Dataset [2] simplifies this activity recog-\nnition through the provision of sensor data on 30 participants\nof ages between 19-48 years. Data preprocessing was done\nthrough the accelerometer and gyroscope of a smartphone,\nSamsung Galaxy S II, which was placed on the participant’s\nwaist. These participants were involved in the following activ-\nities that were to be identified: walking, walking up and down\nstaircases, sitting, standing, laying. These data were sampled at\na rate of 50 Hz, pre-processed with noise filters, and segmented\ninto fixed-size windows of 2.56 seconds. It includes raw time-\nseries data from each sensor as well as a processed feature set\nof 561 variables derived from time and frequency domains to\nenable comprehensive activity classification.\n4) MotionSense Dataset: The MotionSense Dataset [36]\nfeatures time-series for activity recognition. This was gath-\nered using an iPhone 6s kept in the front pocket from\n24 different participants representing varied demographics.\nSubsequent participants, performers, did all six activities:\nwalking, jogging, sitting, standing, walking upstairs, walking\ndownstairs, data captured from accelerometer and gyroscope\nsensors at a constant rate of 15 trials per participant. Sensor\nattributes contain a roll, pitch, user acceleration, and rotational\nrates-when properly segmented-represent meaningful data for\nactivity and attribute recognition. The dataset combines both\nlong trials of 2–3 minutes and shorter trials of 30 seconds to\n1 minute, offering variability in recording conditions.\n5) WISDM Dataset: The WISDM Dataset [58] provides\nactivity recognition data collected from 51 participants who\nperformed 18 activities, such as walking, jogging, sitting,\nstanding, and climbing stairs, for three minutes each. The\ndataset was captured using smartphones and smartwatches,\nincluding Nexus 5, Nexus 5X, Galaxy S6, and LG G Watch,\nat a sampling rate of 20 Hz. It includes raw accelerometer\nand gyroscope data with attributes such as subject ID, activ-\nity label, and triaxial sensor readings. Also, processed files\ninclude statistical features such as mean, standard deviation,\nand frequency-domain metrics, supporting diverse research in\nactivity recognition.\nDataset\nUsers\nClass\nDevice\nDevice Placement\nHHAR\n9\n6\nSmartphone, Smartwatch\nWaist and Arm\nMotionSense\n24\n6\nSmartphone\nFront Pocket\nShoaib\n10\n7\nSmartphone\nArm, waist, pocket\nUCI HAR\n30\n6\nSmartphone\nWaist\nWISDM\n51\n18\nSmartphones, Smartwatch\nTrouser Pocket\nTABLE I: Summary of the Datasets\nTable I shows the summary of the used datasets. Pie charts\nof Figure 2 indicate the activity distribution across five datasets\n: MotionSense, HHAR, UCI HAR, Shoaib and WISDM.\nMotionSense is dominated by static activities where Standing\nalone contributes 38.9% to the data while Upstairs and Down-\nstairs are underrepresented. HHAR has a more even distribu-\ntion among activities, with Biking the most dominant at 21.0%.\nThe UCI HAR also reflects a fairly balanced representation,\nLaying and Standing being the largest constituents with 19.1%\nand 18.7%, respectively. The Shoaib dataset has Walking as\nthe most dominant activity at 19.9%, and there is a good mix\nof dynamic and static behaviors. Finally, WISDM features 18\nactivities, and all are approximately equally distributed. This\nensures that multi-class classification will be diverse.\nB. Data Analysis\nTime-Series Signals: Sensor data from accelerometers, gy-\nroscopes, and magnetometers are presented in Fig. 6. In the\ncase of walking, accelerometer signals are periodic, highlight-\ning the rhythmicity of the activity, while gyroscope signals re-\nflect changes in angular velocity corresponding to limb motion.\nThe magnetometer signals are noisier, reflecting sensitivity\nto environmental magnetic fields. Similarly, when running,\n"}, {"page": 4, "text": "Fig. 2: Human activity labels in different datasets.\nFig. 3: Correlation Analysis.\naccelerometer signals generate higher amplitude variations\nbecause of increased intensity, while the gyroscope signals\nwill have more pronounced changes in angular velocity. These\npatterns underline the peculiarities of each activity in time.\nCorrelation\nAnalysis:\nThe\ncorrelation\nmatrix\nshown\nin\nFig.\n3\ngives\nthe\nrelations\namong\nthe\nfeatures\nAx, Ay, Az, Gx, Gy, Gz, Mx, My, Mz.\nThe\nhigh\nvalues\nof the coefficients between gyroscope axes-for example,\nGy and Gz have a correlation coefficient of 0.76-indicate\ncoupled angular motion. The low correlations across sensor\ntypes-for example, between accelerometer and gyroscope\nfeatures-indicate that the signals are complementary and will\nresult in higher performance by making use of diverse signal\ncharacteristics.\nPDF Distributions of Sensor Features: The probability den-\nsity functions of sensor features in Fig. 4 outline the statistical\ncharacteristics of the data. Accelerometer features such as\nAx, Ay, and Az are Gaussian-like distributions centered near\nzero, indicating stationary movement trends. Gyroscope fea-\ntures such as Gx, Gy, and Gz are tightly clustered around zero,\nreflecting limited angular motion during activities. Magne-\ntometer features such as Mx, My, and Mz have more dispersed\ndistributions, probably due to environmental variability.\nPrincipal Component Analysis (PCA): The PCA plots in\nFig. 5 represent the separability of activities in a reduced\nFig. 4: PDF Distributions of Sensor Features.\n(a) PCA for Accelerometer\n(b) PCA for Gyroscope\n(c) PCA for Magnetometer\nFig. 5: PCA analysis for Shoaib Dataset\ntwo-dimensional space for the features of the accelerometer,\ngyroscope and magnetometer sensor data. Activities such as\nwalking and running form separate clusters for accelerometer\ndata, while stationary activities such as sitting and stand-\ning overlap in this data representation. Gyroscope data may\npresent similar patterns of clustering with greater variability.\nThe combined features improve the separability, underlining\nthe merit of multi-sensor data for capturing activity-specific\npatterns.\n"}, {"page": 5, "text": "(a) Walking\n(b) Running\nFig. 6: Sensor data for Walking and Running activity\nC. Data Preprocessing\nFor preparing the data, a segmentation approach was used\nwith a sliding window for activity recognition. The designed\npreprocessing pipeline converts raw time-series sensor data\ninto structured inputs that are suitable for any machine\nlearning model. More precisely, there were nine features\naround accelerometer readings Ax, Ay, Az, gyroscope read-\nings Gx, Gy, Gz, and magnetometer readings Mx, My, Mz.\nThese were then segmented into fixed-sized windows of\nNW = 200 samples, corresponding to a fixed-length time\ninterval, with a step size of 20 samples to create overlapping\nwindows. Each segment was a multi-dimensional time-series\nof shape (NW × NF), where NF = 9.\nD. Zero-shot capability of LLMs\nWe explored Large Language Models’ zero-shot capability\nin human activity recognition tasks to see how the pre-trained\nLLM can interpret and analyze IMU data, without requiring\ntask-specific fine-tuning. We experimented with the LLama3-\n8B model, providing it with specific prompts and statistical\nfeatures extracted from IMU data to assess its capability in\nclassifying human activities and reasoning through its deci-\nsions step-by-step.\nFigure 7 demonstrates the zero-shot capability of the\nLLama-3-8B model in classifying human activity. Without\nprior training on this specific dataset or task, the model\nGround Truth:  walking\nFeatures: Statistical Features.\nObservation: \n1. The gyroscope data shows consistent small fluctuations along all axes.\n2. Accelerometer readings indicate cyclic pattern of vertical acceleration.\n3. The gyroscope values suggest slight rotations while walking.\n4. Acceleration values show consistent changes in velocity during each \nstep.\n5. Gyroscope data implies minimal angular changes in body orientation.\n6. Accelerometer data reflects regular patterns of movement in all \ndirections.\n7. Gyroscope readings remain relatively stable throughout the activity.\n8. Accelerometer data displays characteristic peaks corresponding to \nsteps taken.\n9. The combination of gyroscope and accelerometer data signifies a \nrhythmic gait.\n10. Overall, the sensor data indicates a typical walking pattern with subtle \nvariations.\nLLama-3-8B\nFig. 7: Zero-shot capability of LLama-3-8b model in HAR\naccurately identifies key patterns such as cyclic variations\nin acceleration, stable angular changes, and rhythmic gait\nindicative of walking.\nE. Sensor-text modality alighment\nAligning time-series sensor data with text presents a signif-\nicant challenge. Unlike modalities such as images or audio,\nsensor data lacks rich semantics, making it less interpretable\nfor large language models. Previous research on integrating\nLLMs with Human Activity Recognition (HAR) typically used\nraw sensor data as input, converting numerical sequences into\ntextual formats before feeding them to the model. While this\napproach allows LLMs to process sensor data, it comes with\nsignificant drawbacks, including high token requirements and\ncomputational overhead.\nMaster PDF\n[0.2558, -2.698, \n4.2688, -9.658]\nTokenizer\nConvert to text\nLLM\nRaw IMU data\nSampling\nFig. 8: Traditional approach of align numerical sensor data for\nLLM input.\nModels like LLama-3 8B, which have a maximum token\nlimit of 4096, struggle to handle long sequences of data with\nhigh sampling rates. This often leads to truncation, resulting\nin a loss of valuable information. To address this issue, we\nadopted a more efficient approach by extracting statistical\nfeatures from raw IMU data and using these condensed repre-\nsentations as input to the LLM. Figure 9 illustrates our feature\n"}, {"page": 6, "text": "extraction process from raw IMU data. We extracted both time-\ndomain and frequency-domain features. For the time-domain\nfeatures, we calculated statistical measures such as the mean,\nstandard deviation, and range for all three axes (X, Y, Z)\nof the accelerometer and gyroscope readings. These features\ncapture the overall variability, central tendency, and extent of\nthe motion data over time. For the frequency-domain features,\nwe employed techniques such as Fast Fourier Transform (FFT)\nto analyze the signal’s spectral properties. Features like mean\nfrequency, spectral entropy, and band power for high and low-\nfrequency ranges were derived. These metrics provide insights\ninto the periodicity and energy distribution of the motion\nsignals, which are critical for distinguishing between different\nactivities.\n    Mean value Accelerometer x axis: -5.4343\n    Mean value Accelerometer y axis: 0.2079\n    Mean value Accelerometer z axis: 8.4167\n    Standard deviation Accelerometer x axis: 2.259\n    Standard deviation Accelerometer y axis: 1.2089\n    Standard deviation Accelerometer z axis: 3.225\n    Value Range of Accelerometer x axis: 10.0317\n    Value Range of Accelerometer y axis: 6.4151\n    Value Range of Accelerometer z axis: 12.1369\n    Mean frequency Accelerometer x axis: 0.16614\n    Mean frequency Accelerometer y axis: 2.70019\n    Mean frequency Accelerometer z axis: 0.1821\n……….\n……….\n    Spectral Entropy Accelerometer x axis: 0.61647\n    Spectral Entropy Accelerometer y axis: 4.01644\n    Spectral Entropy Accelerometer z axis: 0.54824\nFig. 9: Statistical Feature Extraction from raw IMU data.\nF. Fine-Tuning\nFor fine-tuning, we utilized Parameter-Efficient Fine-Tuning\n(PEFT) techniques with Low-Rank Adaptation (LoRA) [20]\napplied to the LLama3-8B model. This approach allowed us\nto efficiently adapt the large-scale pre-trained model to our\nspecific Human Activity Recognition task without requiring\na full retraining of all parameters. The LoRA configuration\nincluded a rank r = 128, a LoRA alpha value of 32, and a\ndropout rate of 0.05. The learning rate was set to 2 × 10−4.\nInspired by LLaVA [33], we adopted an instruction-tuning\nformat for training, framing the HAR tasks as instruction-\nfollowing problems. The instruction tuning is completed on\na single A-100 GPU.\nIV. EVALUATION\nA. Experiment setup\n1) Baseline Models: For the activity recognition evalua-\ntion, we used four baseline models: Support Vector Machine,\nRandom Forest, Deep Neural Network, and Long-Short-Term\nMemory. SVM and RF are traditional machine learning models\nwhere SVM makes use of hyperplanes for classification and\nRF uses ensemble learning with the help of decision trees. On\nthe other hand, DNN and LSTM are deep learning models.\nDNN captures complex feature representations using a fully\nconnected layer, while LSTM is a kind of recurrent neural\nnetwork that was developed to capture sequential data in such\na way that its long-term dependencies are maintained. Hence,\nthese models provide an overall framework for the comparison\nbetween traditional and deep learning approaches in human\nactivity recognition.\n2) Evaluation Metrics: To evaluate the performance of\nthe models, we employed four metrics: Accuracy, Precision,\nRecall, and F1 Score.\nAccuracy: Accuracy is defined as the ratio of correctly pre-\ndicted instances to the total number of instances. It measures\nthe overall effectiveness of the model and is calculated as:\nAccuracy =\nTP + TN\nTP + TN + FP + FN\nwhere TP represents True Positives, TN represents True\nNegatives, FP represents False Positives, and FN represents\nFalse Negatives.\nPrecision: Precision quantifies the proportion of correctly\npredicted positive instances out of all instances predicted as\npositive. It evaluates the model’s ability to avoid FP and is\nexpressed as:\nPrecision =\nTP\nTP + FP\nRecall: Recall, also known as sensitivity or true positive\nrate, measures the proportion of actual positives correctly\nidentified by the model. It emphasizes the model’s ability to\ncapture TP and is calculated as:\nRecall =\nTP\nTP + FN\nF1 Score: The F1 score is the harmonic mean of Precision\nand Recall, providing a balanced measure that accounts for\nboth FP and FN. It is particularly useful for imbalanced\ndatasets and is given by:\nF1 Score = 2 × Precision × Recall\nPrecision + Recall\nThese metrics collectively offer a detailed evaluation of\nmodel performance, enabling a thorough comparison of clas-\nsification models across multiple datasets.\n"}, {"page": 7, "text": "ated in Master PDF Edit\nThe person is climbing a \nstairs\nCan you interpret the activity this \nperson is currently doing? Use proper \nreasoning and explain your thought \nprocess step-by-step.\nRetrieval Augmented \nGenerator\nLarge Language \nModel\nThe IMU data reveals distinct \nvertical acceleration spikes \nand higher foot elevation, \nindicating upward movement \ntypical of stair climbing. \nAdditionally, reduced \nhorizontal motion and periodic \nimpact forces from foot further \nconfirm this activity.\nClassification and Reasoning\nInstruction-tuning \ndataset\nPrompt\nHAR data\nRetrieval\nQuery\nRetrieval\nKnowledge\n(a) Creating Instruction Tuning Dataset using RAG\nd in Master PDF Edit\nPre-trained \nLLM\nFine-tuned LLM for \nmobile device\nInstruction-tuned Dataset\nClassification\nReasoning\nRecommendation\nQuestion \nanswering\nUser\nSupervised \nfine-tuning\nDownstream Tasks\n(b) Fine-tuning LLM for various downstream tasks\nFig. 10: Overview of the proposed approach.\n(a) MotionSense\n(b) Shoaib\n(c) UCI HAR\n(d) WISDM\nFig. 11: Performance analysis for four datasets.\nB. Experiment results\nFirst, we evaluate the performance of traditional HAR\nclassification techniques on each dataset.\nFig. 11 illustrates the performance comparison of Macro\nF1 scores for SVM, RF, LSTM, and DNN on Motion Sense,\nShoaib, UCI HAR, and WISDM datasets. Overall, Random\nForest produces the highest performance in most datasets,\nthus proving its strength in robustness and adaptability. The\nDNN and LSTM deep learning models present a very strong\nperformance, especially in those datasets that have a rich\ntemporal pattern, such as UCI HAR. However, they are limited\nin datasets with high variability, such as WISDM. SVM\nperforms reliably but lags behind RF and DNN, reflecting\nits restricted ability to capture complex relationships. These\nresults emphasize the importance of tailoring model selection\nto dataset characteristics to maximize performance.\nTable II presents the performance results for SVM, RF,\nLSTM, and DNN in classification tasks with four different\ndatasets. The comparison metrics used were Accuracy, Macro\nPrecision, Macro Recall, and Macro F1 Score. This compar-\nison revealed differences in both strengths and weaknesses\nwithin various combinations of model-dataset pairs.\nFor the Shoaib Arm Dataset, we got the accuracy of RF as\n0.9894 and Macro F1 Score of 0.9879. LSTM follows suit with\n"}, {"page": 8, "text": "Fig. 12: Classwise performance analysis for four datasets.\nDataset\nModel Name\nAccuracy\nMacro Precision\nMacro Recall\nMacro F1\nShoaib Arm\nSVM\n0.9352\n0.9365\n0.9213\n0.9270\nRF\n0.9894\n0.9892\n0.9867\n0.9879\nLSTM\n0.9695\n0.9664\n0.9645\n0.9645\nDNN\n0.9782\n0.9765\n0.9753\n0.9758\nUCI HAR\nSVM\n0.9190\n0.9148\n0.9169\n0.9144\nRF\n0.9747\n0.9747\n0.9692\n0.9716\nLSTM\n0.9765\n0.9716\n0.9730\n0.9719\nDNN\n0.9821\n0.9796\n0.9806\n0.9800\nWISDM\nSVM\n0.9895\n0.9754\n0.9589\n0.9656\nRF\n0.9965\n0.9982\n0.9833\n0.9904\nLSTM\n0.9668\n0.8317\n0.7628\n0.7807\nDNN\n0.9616\n0.9215\n0.8136\n0.7894\nMotion Sense\nSVM\n0.8603\n0.8390\n0.8116\n0.8203\nRF\n0.8796\n0.8590\n0.8410\n0.8466\nLSTM\n0.8508\n0.8181\n0.8081\n0.8110\nDNN\n0.8544\n0.8206\n0.8185\n0.8192\nTABLE II: Classification Metrics Summary\nan accuracy value of 0.9695 and DNN follows closely with\nan accuracy value of 0.9782, while SVM performs at 0.9352,\nranking lower among these models. In the case of the UCI\nHAR Dataset, DNN achieves the best overall performance\nwith an accuracy of 0.9821 and a Macro F1 Score of 0.9800.\nLSTM and RF also had good performances with an accuracy\nof 0.9765 and 0.9747, respectively. SVM is behind them\nwith an accuracy of 0.9190. The high scores of DNN and\nLSTM suggest that deep learning models effectively capture\nthe temporal patterns in this dataset.\nThere is a big variation in the performance of RF for the\nWISDM Dataset, with near-perfect results-0.9965 for accu-\nracy and 0.9904 for Macro F1 Score-always outperforming all\nthe others. It follows that SVM also runs smoothly on this\ndataset with 0.9895 accuracy and 0.9656 Macro F1 Score.\nThe deep learning models, however, show a remarkable fall,\nespecially LSTM, which produced 0.9668 for accuracy and\n0.7807 for Macro F1 values. This suggests that the dataset’s\nproperties may pose challenges for the sequential models.\nIn the Motion Sense Dataset, traditional machine learning\nmodels such as SVM and RF perform slightly better than\ndeep learning algorithms. RF yielded the highest accuracy\nvalue of 0.8796 with a Macro F1 Score of 0.8466, while\nSVM performed second with an accuracy value of 0.8603.\nLSTM and DNN perform similarly, their accuracy values\nbeing 0.8508 and 0.8544, respectively. The relatively low\nscores compared to other datasets hint at higher variability\nin data or imbalance in activity classes.\nRF\nconsistently\noutperforms\nother\nmodels\nacross\nall\ndatasets, demonstrating its robustness for activity recognition\ntasks. Deep learning models (DNN and LSTM) excel in\ndatasets with strong temporal patterns, such as UCI HAR, but\nshow limited performance on datasets like WISDM. SVM,\nwhile effective, generally underperforms compared to RF and\nDNN, indicating its limitations in capturing complex patterns.\nResults emphasize the selection of a model that best suits the\nnature of the dataset for optimal performance.\nFig. 12 shows the class-wise performance of four models,\nSVM, RF, LSTM, and DNN, across four datasets, namely Mo-\ntionSense, Shoaib, UCI HAR, and WISDM. Overall, RF tends\nto show pretty consistent and highest performance in most of\nthe activities across datasets, specifically doing exceptionally\nwell on structured and stationary activities like Standing and\nSitting for MotionSense, Shoaib, and UCI HAR datasets. DNN\nalso shows competitive accuracy, especially for datasets with\nobvious temporal patterns, such as UCI HAR. On the other\n"}, {"page": 9, "text": "hand, LSTM performs variably: it performs well on dynamic\nactivities such as Jogging but does poorly on diverse and\ncomplex activities in the WISDM dataset, such as Dribbling\nand Writing. SVM is generally reliable but usually behind RF\nand DNN, especially for harder classes. The analysis shows\nthat RF is relatively robust for activity classification tasks and\nthe selection of models should be based on data characteristics\nand activities.\n(a) Same-Modality\n(b) Cross-Modality\nFig. 13: Comparison of Same-Modality vs. Cross-Modality\nPerformance\nThese bar charts present some model performance metrics\nsuch as accuracy, precision, recall, and F1 score when trained\nand tested on the same dataset versus on different ones.\nAs expected, the same-modality performances are decidedly\nhigher across the board, with approximately 0.973 for accu-\nracy, precision, recall, and F1 score in that order. This means\nthat the model generalizes well for the same dataset since there\nare similar feature distributions and patterns. In contrast, the\nresults in the cross-modality setting show significant decreases\nin performance for all metrics: Accuracy and recall go down\nto about 0.0963, while Precision and F1 Score decrease further\nto 0.0442 and 0.0529, respectively. This performance gap\nunderlines the challenge of cross-modality activity recognition\nand highlights the importance of domain adaptation techniques\nto improve generalizability across data sets.\nDataset\nHHAR\nMotionSense\nShoaib\nTest Subject\nSeen\nUnseen\nSeen\nUnseen\nSeen\nUnseen\nRandom Forest\n0.97\n0.67\n0.84\n0.58\n0.98\n0.67\nSVM\n0.91\n0.47\n0.82\n0.42\n0.92\n0.56\nDNN\n0.98\n0.61\n0.81\n0.38\n0.97\n0.64\nLLama-3-8B\n-\n0.65\n-\n0.61\n-\n0.67\nLLama-3-8B (Fine-tuned)\n0.83\n0.75\n0.77\n0.65\n0.79\n0.71\nTABLE III: Classification Result\nTable III and Figure 14 provides a performance comparison\nof various models—Random Forest, Support Vector Machine\n(SVM), and Deep Neural Network (DNN)—against our pro-\nposed framework for a classification task across three datasets:\nHHAR, MotionSense, and Shoaib. The experiments were\nconducted under two distinct settings: ’seen’ and ’unseen’. In\nthe ’seen’ setting, the models were trained and evaluated on\nthe same data distribution, whereas in the ’unseen’ setting,\nHHAR\nMotionSense\nShoaib\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF-1 Score\n0.97\n0.84\n0.98\n0.91\n0.82\n0.92\n0.98\n0.81\n0.97\n0.83\n0.77\n0.79\nRandom Forest\nSVM\nDNN\nLLama-3-8b (Finetuned)\n(a) Classification result on seen test dataset\nHHAR\nMotionSense\nShoaib\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF-1 Score\n0.67\n0.58\n0.69\n0.47\n0.42\n0.56\n0.61\n0.38\n0.64\n0.65\n0.61\n0.67\n0.75\n0.65\n0.71\nRandom Forest\nSVM\nDNN\nLLama-3-8b\nLLama-3-8b (Finetuned)\n(b) Classification result on unseen test dataset\nFig. 14: Performance comparison.\nthe models encountered a different data distribution during\nevaluation, which was not part of their training data. From\nTable III, we observe that traditional approaches like RF and\nDNN perform better in the ’seen’ case, whereas the LLM-\nbased approach excels in the ’unseen’ case. This is likely be-\ncause traditional models are optimized for recognizing patterns\nin specific data distributions they were trained on, making\nthem highly effective when the test data closely resembles\nthe training IMU dataset. In contrast, LLM-based methods,\nleverage their inherent ability to generalize across diverse and\nunseen IMU data distributions.\nFigure 15 presents a radar chart comparing the class-\nwise performance of the Llama-3-8b model and its finetuned\ncounterpart on the HHAR dataset. Activities such as Walking\nand Standing demonstrate higher accuracy, while activities\nlike Climbing Upstairs and Climbing Downstairs tend to have\nlower accuracy. The higher accuracy for Walking and Standing\ncan be attributed to their distinctive and consistent patterns\nin sensor data, which are easier for the model to identify\nand classify. These activities often involve repetitive and pre-\ndictable movements, leading to less variability within the same\nclass. In contrast, activities like Climbing Stairs involve more\ncomplex and variable motion patterns, which can overlap with\nother classes, making them harder to distinguish accurately.\nThe finetuned Llama-3-8b model outperforms the base model\nacross most activity classes, with notable improvements in\nLaying, and Sitting.\n"}, {"page": 10, "text": "Laying\nStanding\nClimbing Downstairs\nClimbing upstairs\nSitting\nWalking\n20\n40\n60\n80\nLlama-3-8b\nLlama-3-8b (Finetuned)\nFig. 15: Classwise performance analysis on HHAR datasets.\nV. ISSUES ENCOUNTERED\nIn the process of integrating a large language model for the\nhuman activity recognition task, several issues were identified\nthat impacted both the development and deployment phases.\nA. Model Limitations\nWhile LLMs demonstrate robust natural language process-\ning capabilities, they often struggle to interpret raw IMU sen-\nsor data. Unlike textual data, IMU data requires sophisticated\npreprocessing to extract meaningful features, which LLMs are\nnot inherently designed to handle. A significant issue observed\nin LLMs, particularly smaller models such as LLama-3-8B and\nLLama-3-1B, is their tendency to hallucinate. These models\ncan generate made-up reasoning or predictions that deviate\nsignificantly from true activity patterns. Larger models like\nGPT-4 exhibit better performance compared to smaller, open-\nsource models. However, their substantial size and resource\nrequirements pose significant challenges for on-device imple-\nmentation.\nB. Modality Alignment\nLLMs excel in processing text-based data, leveraging their\npretraining on vast corpora of textual information to generate\noutputs. IMU data, on the other hand, is inherently time-series\nin nature and consists of numerical sequences captured over\ntime. Aligning such data with LLMs, which are primarily\ndesigned for textual input, introduces a key challenge. In this\nwork, we overcome this challenge by extracting statistical\nfeatures from the raw IMU data and transforming them into a\nstructured format compatible with text-based representation.\nVI. DISCUSSION AND FUTURE WORK\nIn this work, we conducted experiments to evaluate the\nperformance of large language models in human activity\nrecognition tasks. We explored the challenges associated with\nutilizing LLMs for HAR tasks and proposed a solution to\naddress these limitations. Our study includes an investigation\ninto the zero-shot capabilities of LLMs for HAR, examining\nhow LLMs can reason about their decisions and infer patterns\nfrom complex, multimodal data. Additionally, we developed\na fine-tuning pipeline to tailor LLMs for more specific HAR-\nrelated tasks.\nTo assess the effectiveness of LLMs, we compared their\nperformance with traditional machine learning (ML) and deep\nlearning (DL) approaches in both seen and unseen data set-\ntings. Our results demonstrate that traditional methods outper-\nform LLMs when trained on data from the same distribution,\nowing to their ability to specialize in specific patterns within\na controlled dataset. However, in scenarios involving unseen\ndata, LLMs show superior performance. This advantage arises\nbecause LLMs leverage their pretraining on diverse and ex-\ntensive datasets, enabling them to generalize better and make\ninformed assumptions about novel activities, even without\nexplicit prior training on similar examples.\nWe addressed the critical challenges of integrating sensor\ndata with large language models (LLMs) and proposed a\nmethod to bridge the gap between these modalities. Rec-\nognizing the limitations of manual feature extraction, future\nwork will focus on leveraging pre-trained encoders [64] to\nautomatically extract features from sensor data. By aligning\nthese encoder-driven features with LLMs, inspired by suc-\ncessful integration techniques in vision and audio domains\n[15], we aim to enhance the performance and scalability of\nour framework. This approach has the potential to streamline\nthe feature extraction process, improve accuracy, and enable\nmore seamless multimodal integration. Ultimately, these ad-\nvancements will further the applicability of LLMs in human\nactivity recognition and beyond, paving the way for robust,\nefficient, and interpretable AI-driven solutions.\nVII. CONCLUSION\nThis paper bridges the gap between IMU sensory data\nand large language models, presenting a novel approach that\nextends beyond traditional classification tasks to include re-\nport generation and reasoning capabilities. By addressing the\ninherent challenges of modality alignment between time-series\nIMU data and Language models, we proposed a framework\nthat extracts meaningful statistical features from raw sensor\ndata and integrates them with the reasoning power of LLMs.\nThis approach enables LLMs to not only classify activities\nbut also provide explainable insights and generate detailed\nanalyses based on their predictions. Our study demonstrates\nthe potential of LLMs to excel in zero-shot and unseen data\nscenarios, where their ability to generalize from extensive\npretraining proves advantageous. By developing a fine-tuning\npipeline, we enhanced the performance of LLMs for HAR-\nrelated tasks. The comparative analysis with traditional ML\nand DL methods highlights the complementary strengths of\nthese approaches—traditional models excel in distribution-\nspecific scenarios, while LLMs shine in environments requir-\ning generalization and adaptability. While the integration of\nLLMs in HAR shows promising potential, particularly in han-\ndling unseen data distributions, there is still a need to improve\n"}, {"page": 11, "text": "accuracy for broader adoption. Future work could focus on\nleveraging feature encoders to better extract and represent key\ncharacteristics from raw sensor data, enhancing the alignment\nbetween modalities and boosting overall performance in both\nseen and unseen scenarios.\nREFERENCES\n[1] Abshir Ali, Guanqun Song, and Ting Zhu. Security in 5g networks –\nhow 5g networks help mitigate location tracking vulnerability, 2023.\n[2] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge Luis\nReyes-Ortiz, et al. A public domain dataset for human activity recog-\nnition using smartphones. In Esann, volume 3, page 3, 2013.\n[3] Ferhat Attal, Samer Mohammed, Mariam Dedabrishvili, Faicel Cham-\nroukhi, Latifa Oukhellou, and Yacine Amirat. Physical Human Activity\nRecognition Using Wearable Sensors.\nSensors, 15(12):31314–31338,\nDec. 2015.\n[4] Andreas Bulling, Ulf Blanke, and Bernt Schiele. A tutorial on human\nactivity recognition using body-worn inertial sensors. ACM Computing\nSurveys (CSUR), 46(3):1–33, 2014.\n[5] Yun-Chieh Cheng, Yu-Tong Shen, Guanqun Song, and Ting Zhu.\nTechnological progress and obsolescence: Analyzing the environmental\neconomic impacts of macbook pro i/o devices, 2024.\n[6] Zicheng Chi, Yan Li, Xin Liu, Wei Wang, Yao Yao, Ting Zhu, and\nYanchao Zhang.\nCountering cross-technology jamming attack.\nIn\nProceedings of the 13th ACM Conference on Security and Privacy in\nWireless and Mobile Networks, WiSec ’20, page 99–110, New York,\nNY, USA, 2020. Association for Computing Machinery.\n[7] Zicheng Chi, Yan Li, Xin Liu, Yao Yao, Yanchao Zhang, and Ting\nZhu. Parallel inclusive communication for connecting heterogeneous iot\ndevices at the edge. In Proceedings of the 17th Conference on Embedded\nNetworked Sensor Systems, SenSys ’19, page 205–218, New York, NY,\nUSA, 2019. Association for Computing Machinery.\n[8] Zicheng Chi, Yan Li, Hongyu Sun, Zhichuan Huang, and Ting Zhu.\nSimultaneous bi-directional communications and data forwarding using\na single zigbee data stream. IEEE/ACM Transactions on Networking,\n29(2):821–833, 2021.\n[9] Zicheng Chi, Yan Li, Hongyu Sun, Yao Yao, and Ting Zhu. Concur-\nrent cross-technology communication among heterogeneous iot devices.\nIEEE/ACM Transactions on Networking, 27(3):932–947, 2019.\n[10] Zicheng Chi, Xin Liu, Wei Wang, Yao Yao, and Ting Zhu. Leveraging\nambient lte traffic for ubiquitous passive communication. In Proceedings\nof the Annual Conference of the ACM Special Interest Group on\nData Communication on the Applications, Technologies, Architectures,\nand Protocols for Computer Communication, SIGCOMM ’20, page\n172–185, New York, NY, USA, 2020. Association for Computing\nMachinery.\n[11] Anuja Dixit, Shreya Byreddy, Guanqun Song, and Ting Zhu.\nData\nclassification with multiprocessing, 2023.\n[12] Jialin Gao, Ping Yi, Zicheng Chi, and Ting Zhu.\nA smart medical\nsystem for dynamic closed-loop blood glucose-insulin control. Smart\nHealth, 1-2:18–33, 2017. Connected Health: Applications, Systems and\nEngineering Technologies (CHASE 2016).\n[13] Jiayuan Gao, Yingwei Zhang, Yiqiang Chen, Tengxiang Zhang, Boshi\nTang, and Xiaoyu Wang.\nUnsupervised human activity recognition\nvia large language models and iterative evolution.\nIn ICASSP 2024-\n2024 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 91–95. IEEE, 2024.\n[14] Yichen Gao, Guanqun Song, and Ting Zhu. Optimizing global quantum\ncommunication via satellite constellations, 2024.\n[15] Joshua P Gardner, Simon Durand, Daniel Stoller, and Rachel M Bittner.\nLlark: A multimodal instruction-following language model for music.\nIn Forty-first International Conference on Machine Learning, 2023.\n[16] Hemanth Gopal, Guanqun Song, and Ting Zhu. Security, privacy and\nchallenges in microservices architecture and cloud computing- survey,\n2022.\n[17] Patrick Gould, Guanqun Song, and Ting Zhu.\nEnvironmental and\neconomic impact of i/o device obsolescence, 2024.\n[18] Nils Y Hammerla, Shane Halloran, and Thomas Pl¨otz.\nDeep, con-\nvolutional, and recurrent models for human activity recognition using\nwearables. arXiv preprint arXiv:1604.08880, 2016.\n[19] Dianqi Han, Ang Li, Lili Zhang, Yan Zhang, Jiawei Li, Tao Li,\nTing Zhu, and Yanchao Zhang.\nDeep learning-guided jamming for\ncross-technology wireless networks: Attack and defense.\nIEEE/ACM\nTransactions on Networking, 29(5):1922–1932, 2021.\n[20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank\nadaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\n[21] Sheikh Asif Imran, Mohammad Nur Hossain Khan, Subrata Biswas, and\nBashima Islam. LLaSA: Large Multimodal Agent for Human Activity\nAnalysis Through Wearable Sensors. arXiv, June 2024.\n[22] Sijie Ji, Xinzhe Zheng, and Chenshu Wu. Hargpt: Are llms zero-shot\nhuman activity recognizers? arXiv preprint arXiv:2403.02727, 2024.\n[23] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang,\nXiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan,\net al.\nTime-llm: Time series forecasting by reprogramming large\nlanguage models. arXiv preprint arXiv:2310.01728, 2023.\n[24] Krishna Sai Ketha, Guanqun Song, and Ting Zhu. Analysis of security\nin os-level virtualization, 2025.\n[25] Dimple P. Khatri, Guanqun Song, and Ting Zhu.\nHeterogeneous\ncomputing systems, 2022.\n[26] Ashvini A Kulshrestha, Guanqun Song, and Ting Zhu.\nThe inner\nworkings of windows security, 2023.\n[27] Ang Li, Jiawei Li, Dianqi Han, Yan Zhang, Tao Li, Ting Zhu, and\nYanchao Zhang. PhyAuth: Physical-Layer message authentication for\nZigBee networks. In 32nd USENIX Security Symposium (USENIX Se-\ncurity 23), pages 1–18, Anaheim, CA, Aug. 2023. USENIX Association.\n[28] Angela Li, Guanqun Song, and Ting Zhu.\nA miniscule survey on\nblockchain scalability, 2022.\n[29] Yan Li, Zicheng Chi, Xin Liu, and Ting Zhu. Chiron: Concurrent high\nthroughput communication for iot devices. In Proceedings of the 16th\nAnnual International Conference on Mobile Systems, Applications, and\nServices, MobiSys ’18, page 204–216, New York, NY, USA, 2018.\nAssociation for Computing Machinery.\n[30] Yan Li, Zicheng Chi, Xin Liu, and Ting Zhu. Passive-zigbee: Enabling\nzigbee communication in iot networks with 1000x+ less power con-\nsumption. In Proceedings of the 16th ACM Conference on Embedded\nNetworked Sensor Systems, SenSys ’18, page 159–171, New York, NY,\nUSA, 2018. Association for Computing Machinery.\n[31] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,\nGuohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al.\nPersonal llm agents: Insights and survey about the capability, efficiency\nand security. arXiv preprint arXiv:2401.05459, 2024.\n[32] Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue, and Flora D. Salim.\nSensorLLM: Aligning Large Language Models with Motion Sensors for\nHuman Activity Recognition. arXiv, Oct. 2024.\n[33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual\ninstruction tuning. Advances in neural information processing systems,\n36, 2024.\n[34] Xin Liu, Zicheng Chi, Wei Wang, Yao Yao, Pei Hao, and Ting Zhu.\nHigh-granularity modulation for ofdm backscatter. IEEE/ACM Transac-\ntions on Networking, 32(1):338–351, 2024.\n[35] Xin Liu, Wei Wang, Guanqun Song, and Ting Zhu. LightThief: Your\noptical communication information is stolen behind the wall. In 32nd\nUSENIX Security Symposium (USENIX Security 23), pages 5325–5339,\nAnaheim, CA, Aug. 2023. USENIX Association.\n[36] Mohammad Malekzadeh, Richard G Clegg, Andrea Cavallaro, and\nHamed Haddadi. Mobile sensor data anonymization. In Proceedings\nof the international conference on internet of things design and imple-\nmentation, pages 49–58, 2019.\n[37] Yishuang Meng, Ping Yi, Xuejun Guo, Wen Gu, Xin Liu, Wei Wang,\nand Ting Zhu.\nDetection for pulmonary nodules using rgb channel\nsuperposition method in deep learning framework.\nIn 2018 Third\nInternational Conference on Security of Smart Cities, Industrial Control\nSystem and Communications (SSIC), pages 1–8, 2018.\n[38] Elishiah Miller, Zheng Li, Helena Mentis, Adrian Park, Ting Zhu,\nand Nilanjan Banerjee.\nRadsense: Enabling one hand and no hands\ninteraction for sterile manipulation of medical images using doppler\nradar. Smart Health, 15:100089, 2020.\n[39] Elishiah Miller, Zane MacFarlane, Seth Martin, Nilanjan Banerjee, and\nTing Zhu.\nRadar-based monitoring system for medication tampering\nusing data augmentation and multivariate time series classification.\nSmart Health, 23:100245, 2022.\n[40] Zeyu Ning, Hugues Nelson Iradukunda, Qingquan Zhang, and Ting Zhu.\nBenchmarking machine learning: How fast can your algorithms go?,\n2021.\n[41] Zefeng Qiu, Prashanth Umapathy, Qingquan Zhang, Guanqun Song,\nand Ting Zhu. Map-reduce for multiprocessing large data and multi-\nthreading for data scraping, 2023.\n[42] Farshad Safavi, Irfan Ali, Venkatesh Dasari, Guanqun Song, Ting Zhu,\n"}, {"page": 12, "text": "and Maryam Rahnemoonfar. Efficient semantic segmentation on edge\ndevices, 2023.\n[43] Muskan Shergill, Zach Thompson, Guanqun Song, and Ting Zhu.\nEnergy efficient lorawan in leo satellites, 2024.\n[44] Muhammad Shoaib, Stephan Bosch, Ozlem Durmaz Incel, Hans\nScholten, and Paul JM Havinga. Fusion of smartphone motion sensors\nfor physical activity recognition. Sensors, 14(6):10146–10176, 2014.\n[45] Shakhrul Iman Siam, Hyunho Ahn, Li Liu, Samiul Alam, Hui Shen,\nZhichao Cao, Ness Shroff, Bhaskar Krishnamachari, Mani Srivastava,\nand Mi Zhang. Artificial intelligence of things: A survey. ACM Trans.\nSen. Netw., Aug. 2024. Just Accepted.\n[46] Guanqun Song, Yan Li, and Ting Zhu. A metal sensing and biometric-\nbased tracking system. In Proceedings of the Tenth ACM/IEEE Sym-\nposium on Edge Computing, SEC ’25, New York, NY, USA, 2025.\nAssociation for Computing Machinery.\n[47] Guanqun Song and Ting Zhu. Ml-based secure low-power communica-\ntion in adversarial contexts, 2022.\n[48] Guanqun Song and Ting Zhu. Ml-based secure low-power communica-\ntion in adversarial contexts, 2022.\n[49] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\nMikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller\nJensen.\nSmart devices are different: Assessing and mitigatingmobile\nsensing heterogeneities for activity recognition. In Proceedings of the\n13th ACM conference on embedded networked sensor systems, pages\n127–140, 2015.\n[50] Qin Sun, Grace McKenzie, Guanqun Song, and Ting Zhu. Design and\nimplementation considerations for a virtual file system using an inode\ndata structure, 2023.\n[51] Yinrong Tao, Sheng Xiao, Bin Hao, Qingquan Zhang, Ting Zhu,\nand Zhuo Chen. Wire: Security bootstrapping for wireless device-to-\ndevice communication. In 2020 IEEE Wireless Communications and\nNetworking Conference (WCNC), pages 1–7, 2020.\n[52] Niall Twomey, Tom Diethe, Xenofon Fafoutis, Atis Elsts, Ryan Mc-\nConville, Peter Flach, and Ian Craddock. A comprehensive study of\nactivity recognition using accelerometers. Informatics, 5(2), 2018.\n[53] Wei Wang, Xin Liu, Zicheng Chi, Stuart Ray, and Ting Zhu.\nKey\nestablishment for secure asymmetric cross-technology communication.\nIn Proceedings of the 19th ACM Asia Conference on Computer and\nCommunications Security, ASIA CCS ’24, page 412–422, New York,\nNY, USA, 2024. Association for Computing Machinery.\n[54] Wei Wang, Tiantian Xie, Xin Liu, and Ting Zhu. Ect: Exploiting cross-\ntechnology concurrent transmission for reducing packet delivery delay in\niot networks. In IEEE INFOCOM 2018 - IEEE Conference on Computer\nCommunications, pages 369–377, 2018.\n[55] Wei Wang, Yao Yao, Xin Liu, Xiang Li, Pei Hao, and Ting Zhu. I can\nsee the light: Attacks on autonomous vehicles using invisible lights. In\nProceedings of the 2021 ACM SIGSAC Conference on Computer and\nCommunications Security, CCS ’21, page 1930–1944, New York, NY,\nUSA, 2021. Association for Computing Machinery.\n[56] Xiaohui Wei, Xiukun Wei, Xingwang Wang, Yundi Wang, and Yan\nNiu. Hrcache: Edge-end collaboration for mobile deep vision based on\nh.264 and approximated reuse. In 2022 IEEE International Performance,\nComputing, and Communications Conference (IPCCC), pages 380–388,\n2022.\n[57] Xiukun Wei and Xueru Zhang. Self-consuming generative models with\nadversarially curated data, 2025.\n[58] Gary Weiss. WISDM Smartphone and Smartwatch Activity and Bio-\nmetrics Dataset .\nUCI Machine Learning Repository, 2019.\nDOI:\nhttps://doi.org/10.24432/C5HK59.\n[59] Matthew Willetts, Sven Hollowell, Louis Aslett, Chris Holmes, and\nAiden Doherty.\nStatistical machine learning of sleep and physical\nactivity phenotypes from sensor data in 96,220 UK Biobank participants.\nSci. Rep., 8(7961):1–10, May 2018.\n[60] Jianbo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiaoli Li, and Shonali\nKrishnaswamy.\nDeep convolutional neural networks on multichannel\ntime series for human activity recognition. In Ijcai, volume 15, pages\n3995–4001. Buenos Aires, Argentina, 2015.\n[61] Yao Yao, Yan Li, and Ting Zhu.\nInterference-negligible privacy-\npreserved shield for rf sensing.\nIEEE Transactions on Mobile Com-\nputing, 23(5):3576–3588, 2024.\n[62] Yao Yao, Zeyu Ning, Qingquan Zhang, and Ting Zhu. Paris: Passive\nand continuous fetal heart monitoring system. Smart Health, 17:100087,\n2020.\n[63] Botao Yu, Guanqun Song, and Ting Zhu. Achieving carbon neutrality\nfor i/o devices, 2024.\n[64] Hang Yuan, Shing Chan, Andrew P Creagh, Catherine Tong, Aidan\nAcquah, David A Clifton, and Aiden Doherty. Self-supervised learning\nfor human activity recognition using 700,000 person-days of wearable\ndata. NPJ digital medicine, 7(1):91, 2024.\n[65] Zhehu Yuan, Jinyang Liu, Guanqun Song, and Ting Zhu. Heat: Satellite’s\nmeat is gpu’s poison, 2024.\n"}]}