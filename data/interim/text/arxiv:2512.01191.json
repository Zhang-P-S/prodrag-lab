{"doc_id": "arxiv:2512.01191", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.01191.pdf", "meta": {"doc_id": "arxiv:2512.01191", "source": "arxiv", "arxiv_id": "2512.01191", "title": "Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks", "authors": ["Krithik Vishwanath", "Mrigayu Ghosh", "Anton Alyakin", "Daniel Alexander Alber", "Yindalon Aphinyanaphongs", "Eric Karl Oermann"], "published": "2025-12-01T02:14:43Z", "updated": "2025-12-01T02:14:43Z", "summary": "Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.01191v1", "url_pdf": "https://arxiv.org/pdf/2512.01191.pdf", "meta_path": "data/raw/arxiv/meta/2512.01191.json", "sha256": "2e5ddd2239b6e9485bfe297be2bee78dc14b25bb1b2e10dbf89d08c981121216", "status": "ok", "fetched_at": "2026-02-18T02:25:43.094674+00:00"}, "pages": [{"page": 1, "text": "1 \nGeneralist Large Language Models Outperform  \nClinical Tools on Medical Benchmarks \n \nKrithik Vishwanath1-3, Mrigayu Ghosh4,5, Anton Alyakin, M.S.E.1,6,7 , \nDaniel Alexander Alber, B.S.1 , Yindalon Aphinyanaphongs, M.D., Ph.D.7-9,  \nEric Karl Oermann, M.D.1,7,10,11 \n \n1Department of Neurological Surgery, NYU Langone Health, New York, New York, USA \n2Department of Aerospace Engineering & Engineering Mechanics, The University of Texas at \nAustin, Austin, Texas, USA \n3Department of Mathematics, The University of Texas at Austin, Austin, Texas, USA \n4Department of Biomedical Engineering, The University of Texas at Austin, Austin, Texas, USA \n5Department of Molecular Biosciences, The University of Texas at Austin, Austin, Texas, USA \n6Department of Neurosurgery, Washington University School of Medicine in St. Louis, St. Louis, \nMissouri, USA \n7Global AI Frontier Lab, New York University, New York, New York, USA \n8Department of Population Health, NYU Langone Health, New York, New York, USA \n9Department of Medicine, NYU Langone Health, New York, New York, USA \n10Department of Radiology, NYU Langone Health, New York, New York, USA \n11Center for Data Science, New York University, New York, New York, USA\n \n"}, {"page": 2, "text": "2 \nCorrespondence: \nKrithik Vishwanath \nDepartment of Neurosurgery, \nNYU Langone Medical Center, \nNew York University, 550 First Ave, MS 3 205, \nNew York, NY10016, USA. \nEmail: krithik.vish@utexas.edu \n \nEric K. Oermann, MD \nDepartment of Neurosurgery, \nNYU Langone Medical Center, \nNew York University, 550 First Ave, MS 3 205, \nNew York, NY10016, USA. \nEmail: eric.oermann@nyulangone.org \n \n"}, {"page": 3, "text": "3 \nAbstract \nSpecialized clinical AI assistants are rapidly entering medical practice, often framed as safer or \nmore reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, \nthese clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical \nevidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. \nWe assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) \nagainst three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) \nusing a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench \n(clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with \nGPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in \ncompleteness, communication quality, context awareness, and systems-based safety reasoning. \nThese findings reveal that tools marketed for clinical decision support may often lag behind \nfrontier LLMs, underscoring the urgent need for transparent, independent evaluation before \ndeployment in patient-facing workflows.\n \n"}, {"page": 4, "text": "4 \nIntroduction \nLarge language models (LLMs) are increasingly used in clinical medicine.1â€“3 While major \nfrontier models are well-benchmarked1,2, specialized tools marketed for clinical use remain \nunder-studied. OpenEvidence, claimed to be used by 40% of U.S. physicians3 and to ace the \nUSMLE4, has only been evaluated either internally4 or qualitatively5. Similarly, the newly \nreleased UpToDate Expert AI, already used by clinicians at an estimated 70% of major enterprise \nhealth systems6, still awaits any sort of independent quantitative evaluation. There is a substantial \ngap between marketing claims and rigorous, peer-reviewed assessment of clinical AI \nperformance. This has created a situation in which individual clinicians and health systems may \nadopt AI systems without adequate evidence regarding their limitations, biases, comparative \nperformance, or failure modes. In settings where AI outputs influences diagnosis or management \ndecisions, this lack of transparency may introduce avoidable clinical risk and challenges \nevidence-based integration of AI into care. \n \nIt is commonly hypothesized that specialist tools may be superior to generalist LLMs through \ndomain-specific training7â€“9 or retrieval augmented generation (RAG)3,6,7. However, specialist \nsystems vary in their design, scope, and implementation. Frontier generalist models benefit from \nsubstantially larger training corpora and more advanced alignment, which may enable them to \nmatch or exceed specialist performance depending on the task. Independent evaluations capable \nof comparing these two categories of models on standardized, clinically relevant benchmarks are \ncritical to clarify whether specialization offers measurable advantage. To address this gap, we \nconducted the first quantitative assessment of specialized clinical AI tools (OpenEvidence, \nUpToDate Expert AI) and leading generalist models (GPT-5, Gemini 3 Pro Preview, and Claude \n"}, {"page": 5, "text": "5 \nSonnet 4.5) on a mini-benchmark comprising knowledge-intensive (MedQA1) and \nexpert-alignment (HealthBench2) tasks. \n \n \nMethods \nWe created a medical mini-benchmark by randomly sampling 500 USMLE-style questions from \nMedQA1 to assess factual knowledge, and 500 prompts from HealthBench2 to evaluate alignment \nwith expert judgement. The resulting 1,000 questions were used to assess GPT-5 (snapshot \ngpt-5-2025-08-07), Gemini 3 Pro Preview (accessed 11/20/25), and Sonnet 4.5 (snapshot \nclaude-sonnet-4-5-20250929) via their respective APIs, while OpenEvidence (accessed 09/25)  \nand UpToDate Expert AI (accessed 11/25) were manually queried via a browser interface. \nOpenEvidence and UpToDate were grouped as clinical tools, whereas the other three (GPT-5, \nGemini 3 Pro, and Sonnet 4.5) were considered to be generalist models. \n \nHealthBench responses were graded for the proportion of rubric points achieved across five axes: \naccuracy, completeness, communication quality, context awareness, and instruction following. \nHealthBench questions were restricted to prompts requiring a single, standalone response (i.e., \nno multi-turn interactions). Responses were also subset into seven themes: emergency referrals, \ncontext seeking, global health, health data tasks, expertise-tailored communication, responding \nunder uncertainty, and response depth. \n \n"}, {"page": 6, "text": "6 \nScoring for HealthBench and MedQA were completed by GPT-4.1 (gpt-4.1-2025-04-14) which \ndemonstrated reliability in grading medical Q&A2. Holmâ€“Bonferroni-adjusted statistical \nsignificance was established using exact McNemarâ€™s and Wilcoxonâ€™s tests. For grouped \ncomparisons of clinical tools to generalist models, statistical significance was established using \nWelchâ€™s t tests and Å idÃ¡k-adjusted two-way ANOVA. \n \nResults \nTo evaluate model performance on medical question answering, we first assessed accuracy on \nMedQA. The resultant MedQA accuracies were: GPT-5, 96.2% (95% CI, 94.1%â€“97.6%); \nGemini, 94.6% (92.3%â€“96.3%); Sonnet 4.5, 91.4% (88.6%â€“93.6%); OpenEvidence, 89.6% \n(86.6%â€“92.0%); and UpToDate, 88.4% (85.3%â€“90.9%); with GPT-5 outperforming all models \nexcept Gemini (McNemar P<1Ã—10-4 vs. OpenEvidence and vs. UpToDate, P=0.0008 vs. \nSonnet). Gemini outperformed OpenEvidence (P=0.003), UpToDate (P<1Ã—10-4), and Sonnet \n(P=0.026). Other differences were not significant (GPT-5 vs. Gemini, P=0.51; OpenEvidence vs. \nSonnet, P=0.52; UpToDate vs. Sonnet, P=0.21; OpenEvidence vs. UpToDate, P=0.52) (Figure \n1A). OpenEvidence, UpToDate Expert AI, and Sonnet 4.5 perform worse on Medical Knowledge \n/ Scientific Topic questions (P<0.008; Supplemental Figure 1). \n"}, {"page": 7, "text": "7 \n \nFigure 1. Performance of generative artificial intelligence (AI) tools on medical \nbenchmarks. A) MedQA accuracy by model. Statistical significance established via exact \nMcNemarâ€™s tests.  C) HealthBench average scores by model, assessed through a \nphysician-consensus rubric. Statistical significance established via pairwise Wilcoxonâ€™s tests. B, \nD) Comparison of the performance of Clinical (Expert AI, Open Evidence) tools to Generalist \n(GPT 5, Gemini 3 Pro, and Sonnet 4.5) models on MedQA and HealthBench, respectively. \nStatistical significance established via Welchâ€™s t-tests. Error bars represent 95% confidence \nintervals in A-D.  * P < 0.05, ** P < 0.01, *** P < 0.001, **** P < 0.0001. \n \nWhen comparing clinical tools to generalist models in a grouped fashion, generalist models \n(average accuracy of 94.1%) outperformed clinical tools (average accuracy of 89.0%), although \nthis difference was not significant (P=0.056; Figure 1B). Notably, when questions are classified \n"}, {"page": 8, "text": "8 \nby their competency category, clinical tools perform worse on Systems-based Practice \nQuestions, including Patient Safety (P<1Ã—10-4; Supplemental Figure 2). \n \nTo assess model agreement with expert clinicians, we evaluate performance on HealthBench. \nHealthBench mean consensus scores were: GPT-5, 97.0% (96.0â€“98.0%); Gemini, 90.5% \n(88.7â€“92.3%); Sonnet, 87.7% (85.5â€“89.9%); UpToDate, 75.2 (72.3%â€“78.1%); OpenEvidence, \n74.3% (71.4â€“77.2%). GPT-5 outperformed other models (all Wilcoxon P<1Ã—10-9), while \nOpenEvidence consistently underperformed (P<1Ã—10-10), except when compared to UpToDate \n(P=0.58) (Figure 1C). Furthermore, the generalist models outperformed the clinical tools by \nabout 1.23-fold (91.7% vs. 74.8%; P=0.023; Figure 1D). \n \nIn axis-level analysis, GPT-5 achieved the highest or tied-for-highest score across all five axes \n(Figure 2A). OpenEvidence consistently ranked lowest or tied for lowest with UpToDate, with \nall GPT-5 vs. OpenEvidence and GPT-5 vs. UpToDate comparisons being significant (McNemar \nPâ‰¤5.5Ã—10-4). In three out of five axes, OpenEvidence was inferior to all three generalist models \n(Pâ‰¤5.8Ã—10-4), and in two out of five axes, UpToDate was inferior to all three generalist models \n(Pâ‰¤2.7Ã—10-4). These trends correspond with the grouped clinical vs. generalist model analysis, \nwhich demonstrated superiority of generalist models in completeness (P<10-4), communication \nquality (P=0.002), and context awareness (P=0.012) (Figure 2C). \n \nIn a theme-level analysis, GPT-5 ranked first or tied for first in all categories, with perfect scores \nin four. OpenEvidence and UpToDate scored lowest or tied for lowest in all seven themes \n(Figure 2B). Again, all GPT-5 vs. OpenEvidence and GPT-5 vs. UpToDate comparisons were \n"}, {"page": 9, "text": "9 \nsignificant (Wilcoxon Pâ‰¤0.005). Grouped analysis revealed that there were significant \ndifferences in the scores of clinical tools and generalist models when responding to questions \nunder the cluster themes of emergency referrals (P=0.0002) and expertise-tailored \ncommunication (P=0.0392) (Figure 2D). \n \n \nFigure 2. Performance of generative AI tools on HealthBench by axis and by theme. A) \nProportion of prompts with axis met in HealthBench by model. Statistical significance \nestablished via pairwise exact McNemarâ€™s tests. B) Mean score in HealthBench by cluster theme \nby model. Statistical significance established via pairwise Wilcoxonâ€™s tests. In A-B, error bars \nrepresent 95% confidence intervals, and labeled letters indicate significant differences within \neach theme: models with the same letter are not significantly different whereas models with \ndifferent letters are significantly different at ð›¼=0.05. C-D) Comparison of the performance of \n"}, {"page": 10, "text": "10 \nclinical tools to generalist models on HealthBench axes and HealthBench cluster themes, \nrespectively. Statistical significance established via two-way ANOVA and Å idÃ¡k-corrected \npost-hoc analysis. In C-D, error bars represent SEM. * P < 0.05, ** P < 0.01, *** P < 0.001, \n**** P < 0.0001. \n \nDiscussion \nWe present an independent, quantitative comparison of popular clinical AI tools against frontier \ngeneralist LLMs. GPT-5 outperformed all models except for Gemini 3 Pro on the MedQA subset \nof our mini-benchmark, and outperformed all models on the HealthBench subset. Notably, \nOpenEvidence fell short of its reported 100% accuracy on USMLE-style questions4. Our results \nreveal a consistent pattern in which clinical AI tools lag behind frontier generalist models on \nnumerous dimensions of clinical importance, such as completeness, communication quality, and \ncontext awareness. Without knowing the underlying architecture of these systems, it is hard to \nknow why they may underperform generalist models. However research has shown that RAG, \nwhich both OpenEvidence and UpToDate Expert AI heavily rely on, can hurt model \nperformance if the wrong material is retrieved or if it is poorly integrated by the base model.7,10,11 \nAnother possibility is simply the strength of frontier LLMs, which excel at the knowledge \nretrieval and reasoning tasks that characterize many medical questions. \nThese findings have broader implications for the evolving landscape of clinical AI. As generative \nmodels become integrated into routine decision-making, discrepancies between advertised claims \nand real-world performance introduce avoidable clinical risk. Health systems are increasingly \ndeploying AI-driven assistants in documentation support, guideline lookup, triage, and \nambulatory care; settings in which even small reliability deficits can meaningfully affect patient \n"}, {"page": 11, "text": "11 \noutcomes. Importantly, much of todayâ€™s AI use is emerging bottom-up, as clinicians and patients \nexperiment with these models on their personal devices well before formal institutional adoption. \nThe consistently superior performance of generalist models suggests that scale, alignment, and \ncross-domain reasoning may outweigh domain-specific tuning as determinants of medical \ncompetency. If this trend continues, the perceived distinction between â€œclinical LLMsâ€ and \nâ€œgeneral-purpose LLMsâ€ may become increasingly artificial, with implications for procurement, \nreimbursement, and regulatory oversight. \nThis study has several limitations. We were constrained to modest sample sizes due to the need \nfor manual evaluation of OpenEvidence and UpToDate Expert AI. Moreover, standardized \nbenchmarks, while allowing quantitative comparison, have inherent shortcomings.10 It is possible \nthat recently trained models or tools have been trained on the benchmarks themselves, which \nwere incidentally swept up in their training data. Such concerns underscore the need for open \ndatasets, open weights, and greater transparency surrounding AI systems utilized within clinical \nmedicine. Nonetheless, controlled studies and independent benchmarks of AI systems in \nmedicine are essential to ensure the safe clinical use of these technologies. Perhaps most \nimportantly, however, is the pressing need for actual clinical trials looking at the impact of these \ntechnologies with patient centered outcomes as their endpoints. Future work should examine \nperformance across broader datasets, contexts, and safety-critical outcomes and the need for \nregulatory guidance regarding the evaluation of all LLMs in healthcare.\n \n"}, {"page": 12, "text": "12 \nAcknowledgements: We would like to acknowledge Nader Mherabi and Dafna Bar-Sagi, Ph.D., \nfor their continued support of medical AI research at NYU. We thank Michael Constantino, \nKevin Yie, and the NYU Langone High-Performance Computing (HPC) Team for supporting \ncomputing resources fundamental to our work. \n \nDisclosures: EKO has equity in Delvi, MarchAI, and Artisight, income from Merck & Co. and \nMirati Therapeutics, employment in Eikon Therapeutics, and consulting for Sofinnova Partners \nand Google. The other authors have no personal, financial, or institutional interest pertinent to \nthis article. \n \nAuthor Contributions: AA and EKO supervised the study. KV, AA, and EKO conceptualized \nand established the study design. KV designed and performed the LLM evaluations and scoring.  \nMG performed the statistical analysis. KV wrote the initial draft. KV, MG, and AA developed \nthe figures of the manuscript. KV, MG, AA, DAA, YA, and EKO revised and approved the \nmanuscript. \n \nFunding: EKO is supported by the National Cancer Instituteâ€™s Early Surgeon Scientist Program \n(3P30CA016087-41S1) and the W.M. Keck Foundation. This work was supported by the \nInstitute for Information & Communications Technology Promotion (IITP) grant funded by the \nKorea government (MSIT) (No. RS-2019-II190075 Artificial Intelligence Graduate School \nProgram (KAIST); No. RS-2024-00509279, Global AI Frontier Lab).\n \n"}, {"page": 13, "text": "13 \nReferences: \n1.â€‹\nJin, D. et al. What disease does this patient have? A large-scale open domain question answering \ndataset from medical exams. Appl. Sci. (Basel) 11, 6421 (2021). \n2.â€‹\nArora, R. K. et al. HealthBench: Evaluating large language models towards improved human health. \narXiv [cs.CL] (2025) doi:10.48550/ARXIV.2505.08775. \n3.â€‹\nOpenEvidence. OpenEvidence, the Fastest-Growing Application for Physicians in History, \nAnnounces 210 Million Round at 3.5 Billion Valuation. Cision PR Newswire \nhttps://www.prnewswire.com/news-releases/openevidence-the-fastest-growing-application-for-physi\ncians-in-history-announces-210-million-round-at-3-5-billion-valuation-302505806.html (2025). \n4.â€‹\nOpenEvidence. OpenEvidence Creates the First AI in History to Score a Perfect 100% on the United \nStates Medical Licensing Examination (USMLE). OpenEvidence \nhttps://www.openevidence.com/announcements/openevidence-creates-the-first-ai-in-history-to-score\n-a-perfect-100percent-on-the-united-states-medical-licensing-examination-usmle. \n5.â€‹\nHurt, R. T. et al. The use of an artificial intelligence platform OpenEvidence to augment clinical \ndecision-making for primary care physicians. J. Prim. Care Community Health 16, \n21501319251332215 (2025). \n6.â€‹\nHLTH 2025: Wolters Kluwer showcases UpToDate Expert AI and workflow innovations. \nWolterskluwer https://www.wolterskluwer.com/en/news/uptodate-expert-ai-workflow-hlth-2025 \n(2025). \n7.â€‹\nVishwanath, K., Stryker, J., Alyakin, A., Alber, D. A. & Oermann, E. K. MedMobile: A mobile-sized \nlanguage model with clinical capabilities. arXiv [cs.CL] (2025) doi:10.48550/arXiv.2410.09019. \n8.â€‹\nAlyakin, A. et al. CNS-Obsidian: A Neurosurgical Vision-Language Model Built From Scientific \nPublications. arXiv [cs.AI] (2025). \n9.â€‹\nJiang, L. Y. et al. Generalist foundation models are not clinical enough for hospital operations. arXiv \n[cs.CL] (2025) doi:10.48550/arXiv.2511.13703. \n"}, {"page": 14, "text": "14 \n10.â€‹ Vishwanath, K. et al. Medical large language models are easily distracted. arXiv [cs.CL] (2025). \n11.â€‹ Wu, E., Wu, K. & Zou, J. ClashEval: Quantifying the tug-of-war between an LLMâ€™s internal prior \nand external evidence. in Advances in Neural Information Processing Systems 37 33402â€“33422 \n(Neural Information Processing Systems Foundation, Inc. (NeurIPS), San Diego, California, USA, \n2024).\n \n"}, {"page": 15, "text": "15 \nSupplemental Figures: \n \nSupplemental Figure 1. Performance of each model on USMLE-style questions (MedQA), \nbroken down by A) human system category and B) USMLE competency category. Statistical \nsignificance established via pairwise exact McNemarâ€™s tests. Error bars represent 95% \nconfidence intervals, and labeled letters indicate significant differences within each system or \nwithin each competency: models with the same letter (or no letter) are not significantly different \nwhereas models with different letters are significantly different at ð›¼=0.05.\n \n"}, {"page": 16, "text": "16 \n \nSupplemental Figure 2. Performance of clinical versus generalist models on USMLE-style \nquestions (MedQA), broken down by A) human system category and B) USMLE competency \ncategory.  Statistical significance established via two-way ANOVA and Å idÃ¡k-corrected post-hoc \nanalysis. Error bars represent SEM. * P < 0.05, ** P < 0.01, *** P < 0.001, **** P < 0.0001. \n"}]}