{"doc_id": "arxiv:2602.03368", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.03368.pdf", "meta": {"doc_id": "arxiv:2602.03368", "source": "arxiv", "arxiv_id": "2602.03368", "title": "Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain", "authors": ["Liz Li", "Wei Zhu"], "published": "2026-02-03T10:37:42Z", "updated": "2026-02-12T02:31:13Z", "summary": "While retrieval augmented generation (RAG) has been swiftly adopted in industrial applications based on large language models (LLMs), there is no consensus on what are the best practices for building a RAG system in terms of what are the components, how to organize these components and how to implement each component for the industrial applications, especially in the medical domain. In this work, we first carefully analyze each component of the RAG system and propose practical alternatives for each component. Then, we conduct systematic evaluations on three types of tasks, revealing the best practices for improving the RAG system and how LLM-based RAG systems make trade-offs between performance and efficiency.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.03368v2", "url_pdf": "https://arxiv.org/pdf/2602.03368.pdf", "meta_path": "data/raw/arxiv/meta/2602.03368.json", "sha256": "f37c3299afad3a0f0c1a8ccf159db11566ceddde98d21bb26b2016cc1371c287", "status": "ok", "fetched_at": "2026-02-18T02:19:54.037426+00:00"}, "pages": [{"page": 1, "text": "Pursuing Best Industrial Practices for Retrieval-Augmented Generation in\nthe Medical Domain\nLiz Li1,, Wei Zhu2,∗\n1 DataSelect AI, Xuhui, Shanghai, China\n2University of Hong Kong, Hong Kong, HK, China\nAbstract\nWhile retrieval augmented generation (RAG)\nhas been swiftly adopted in industrial applica-\ntions based on large language models (LLMs),\nthere is no consensus on what are the best\npractices for building a RAG system in terms\nof what are the components, how to organize\nthese components and how to implement each\ncomponent for the industrial applications, es-\npecially in the medical domain. In this work,\nwe first carefully analyze each component of\nthe RAG system and propose practical alterna-\ntives for each component. Then, we conduct\nsystematic evaluations on three types of tasks,\nrevealing the best practices for improving the\nRAG system and how LLM-based RAG sys-\ntems make trade-offs between performance and\nefficiency.\n1\nIntroduction\nLarge Language Models (LLMs) have trans-\nformed the way people access information online,\nshifting from conventional web searches to direct\ninteractions with chatbots. Recently, there has been\na rapid development of commercial LLMs by in-\ndustry players, demonstrating state-of-the-art per-\nformance in question answering (QA) across both\ngeneral and medical domains (Achiam et al., 2023;\nAnil et al., 2023; Singhal et al., 2023a,b; Nori et al.,\n2023; Cui et al., 2023; Wang et al., 2024; Wen-\njing Yue and Wang, 2023; Zhang et al., 2023e;\nZhao et al., 2023; Xu et al., 2023; Ding et al., 2022;\nXin et al., 2024; Qin et al., 2023; Zhu et al., 2023;\nZhu et al., 2023a,b, 2021a; Li et al., 2023a; Zhu\net al., 2023c; Zhang et al., 2023b; Zhu et al., 2023e;\nGuo et al., 2021; Zhu et al., 2021b; Zheng et al.,\n2023; Sun et al., 2020; Zhang et al., 2023c,d; Wang\net al., 2023a; Zhu et al., 2019a; Zhu, 2021a; Zhang\net al., 2021; Wang et al., 2020; Li et al., 2025;\nLeong et al., 2025; Zhang et al., 2025; Yin et al.,\n∗Corresponding author. For any inquiries, please contact:\nmichaelwzhu91@gmail.com.\n2024; Zhu, 2026a,b). Despite these achievements,\na notable issue is that LLMs can sometimes pro-\nduce responses that, while plausible, are factually\ninaccurate, a problem referred to as hallucination\n(Ji et al., 2023; Rawte et al., 2023). Additionally,\nthe training data for these models may not include\nthe most up-to-date information, such as recent\nnews or the latest scientific and medical research.\nThese shortcomings present significant challenges\nand potential risks, particularly in critical areas\nlike finance, personal assistance, bio-medicine and\nhealthcare (Tian et al., 2024b; Hersh, 2024; Tian\net al., 2024b; Hersh, 2024; Zhu et al., 2024; Zhu\nand Tan, 2023; Liu et al., 2022; Xie et al., 2024;\nCui et al., 2023; Zheng et al., 2024; Zhu et al.,\n2023d; Gao et al., 2023a; Zuo et al., 2022; Zhang\net al., 2022; Sun et al., 2022; Zhu et al., 2021c;\nZhu, 2021b; Li et al., 2019; Zhu et al., 2019c,b;\nZhou et al., 2019; Zhang et al., 2025; Wang et al.,\n2025; Liu et al., 2025; Yi-Ge et al., 2024; Tian\net al., 2024a).\nTo tackle this issue, retrieval-augmented genera-\ntion (RAG) utilizes current and trustworthy docu-\nment collections to boost the capabilities of Large\nLanguage Models (LLMs), potentially overcom-\ning various challenges in the field (Lewis et al.,\n2020; Gao et al., 2023b; Zhao et al., 2024). By\nanchoring LLMs’ reasoning in these retrieved doc-\numents, RAG can also improve their explainabil-\nity and transparency. As illustrated in Figure 1, a\nstandard RAG system for the open-domain or med-\nical domain question answering typically involves\nseveral key steps: (a) query classification, which\nassesses whether a given input requires retrieval;\n(b) construction of the retrieval corpus, encompass-\ning processes like chunking and indexing; (c) the\nretrieval process, which identifies the most rele-\nvant information based on the search input; and (d)\nresponse generation, where prompting strategies\nare used to guide the LLMs. The complexity and\nchallenge lie in the variability of approaches for\narXiv:2602.03368v2  [cs.CL]  12 Feb 2026\n"}, {"page": 2, "text": "each step. For instance, when retrieving pertinent\ndocuments for an input, multiple techniques such\nas query rewriting (Ma et al., 2023) and pseudo-\nresponse generation (Gao et al., 2022) can be ap-\nplied to enhance the original query for more effec-\ntive searching. Thus, one central research question\nraises:\nRQ1. What is the best practice for building a RAG\nsystem, especially for the bio-medical tasks?\nThis study is designed to search for the best prac-\ntices of RAG systems through comprehensive ex-\nperimentation on both open-domain and medical\ndomain tasks. Given the impracticality of evalu-\nating every possible combination of methods, we\nemploy a three-step strategy to pinpoint the most\neffective RAG practices. Initially, we examine rep-\nresentative methods for each step or module within\nthe RAG process. Then, we optimize one module at\na time while keeping the others constant, iterating\nthrough all modules to establish an optimal config-\nuration for the RAG system. Lastly, we showcase\nthe experimental outcomes of this optimal RAG\nsetup and also present variations by altering one\nmodule at a time, thus generating a series of RAG\nconfigurations. Based on these results, we propose\nseveral strategies for deploying RAG that effec-\ntively balance performance and efficiency.\nIn summary, our contributions are two-fold:\n• We thoroughly investigate existing approaches\nfor different modules of the RAG system.\n• We have conducted extensive experiments to\ninvestigate many combinations for the RAG\nsettings and identify the optimal RAG prac-\ntices.\n2\nRelated work\n2.1\nRetrieval-augmented generation\nRetrieval-Augmented Generation (RAG) was\nproposed by Lewis et al. (2020) to enhance the gen-\neration performance on knowledge-intensive tasks\nby integrating retrieved relevant information. In the\nLLM era led by OpenAI’s ChatGPT and GPT-4,\nRAG not only mitigates the problem of halluci-\nnations as LLMs are grounded on given contexts\nbut can also provide up-to-date knowledge that the\nLLMs might not encode (Gao et al., 2023b; Zhao\net al., 2024). Many recent studies have been de-\nvoted to improving upon the vanilla RAG workflow\nby either designing novel retrieval and generation\nmechanisms (Borgeaud et al., 2022; Zhang et al.,\n2023a; Ram et al., 2023; Jiang et al., 2023), or\nincorporating pre-training and fine-tuning for im-\nproving LLMs’ capabilities in RAG (Zhang et al.,\n2024; Siriwardhana et al., 2023; Xue et al., 2024).\nIn specific domains like bio-medicine, current\nsystematic evaluations of LLMs typically focus on\nthe vanilla LLMs without RAG (Zhu et al., 2023;\nSinghal et al., 2023a,b; Nori et al., 2023; Chen et al.,\n2023; Saab et al., 2024). There has been a series\nof works on how RAG can help to improve LLMs’\ncapabilities in tasks like clinical decision-making,\nscientific literature analysis, and information ex-\ntraction (Frisoni et al., 2022; Naik et al., 2021;\nXiong et al., 2024; Lála et al., 2023; Jin et al., 2024;\nZakka et al., 2024; Jeong et al., 2024; Wang et al.,\n2023b). However, (a) comprehensive evaluation\nthat contains a variety of tasks is lacking, and (b)\nsystematic investigations on what are the best prac-\ntices to build a RAG system in these domains, such\nas the prompt strategies, are lacking, hindering its\nfurther industrial applications. Our work comple-\nments the existing literature by investigating best\npractices for the LLM-based RAG system.\n3\nRAG system\nIn this section, we detail the components of the\nRAG system, and the approaches available. Figure\n1 presents the RAG system with different methods\nfor each component.\n3.1\nQuery classification\nIn an RAG system, not all queries necessitate re-\ntrieval augmentation and can be effectively handled\nby the inherent capabilities of LLMs. Although\nRAG improves information accuracy and mitigates\nhallucinations, it also introduces increased latency\nand computational complexity. Thus, our approach\nbegins with classifying queries to ascertain whether\nretrieval is needed. Queries that require additional\ninformation go through the RAG modules, while\nthose that do not are directly processed and an-\nswered by the LLMs.\nTherefore, we propose the classifying task of\ndetermining if a query needs retrieval, and we train\na classifier to automate this decision-making pro-\ncess. To build the dataset for this task, we label the\nquery in the dataset as need RAG if its answer’s log-\nlikelihood function can increase when conditioned\non retrieved documents. Otherwise, the query will\nbe labeled not need RAG.\n"}, {"page": 3, "text": "Figure 1: The workflow of retrieval-augmented generation considered in this study. We investigate the contribution\nof each module and provide insights into optimal RAG practices through extensive experiments. The methods\nselected for the best practice are underlined.\n3.2\nChunking\nIt is essential to properly chunk the documents\ninto smaller segments to enhance retrieval preci-\nsion and avoid length issues in LLMs. In this study,\nwe use the sentence-level chunking strategy (Gao\net al., 2023b), which is to break documents into\nsentences and fill the sentences into chunks. Dur-\ning chunking, an important aspect to consider is the\nchunk size. Chunk size significantly impacts per-\nformance. Larger chunks provide more context but\ncould introduce irrelevant information and increase\nlatency. Smaller chunks improve retrieval recall\nand efficiency but may lack sufficient context. This\nstudy considers the chunk size Lc = 256 by default.\nMoreover, we must consider the following aspect\nof chunking, which determines how neighboring\nchunks are connected.\nChunking techniques\nWe consider the fol-\nlowing three chunking techniques: (a) Vanilla\nchunking, which chunks the documents into non-\noverlapping segments of length Lc. (b) Small2big\nchunking, which uses a small segment from vanilla\nchunking for retrieval, but a longer segment con-\ntaining the small one will be returned. We set the\nlength of the smaller chunk to Ls\nc = Lc/2, and\nthe larger chunk to Ll\nc = Lc. (c) Sliding-window\nchunking, which chunks the documents into over-\nlapping segments. The overlap length is set to\nLo = Lc/4.\nIntuitively, the latter two techniques improve\nretrieval quality over the vanilla chunking method\nby organizing chunks’ relationships.\n3.3\nDocument indexing\nAfter long documents are chunked into smaller\nsegments, we save them in a database with an index\nbuilt for efficient retrieval. Based on how the docu-\nments are indexed, the document indexing methods\nare: (a) Sparse indexing, which utilizes the tech-\nnique of inverted index. It first tokenizes the docu-\nment segments into words or tokens and then builds\nan inverted index where each key term points to a\nlist of documents that contain that term. (b) Dense\nindexing, which assumes the document segments\nare transformed to semantic vectors via an embed-\nding model. Approximate nearest neighbor (ANN)\nindex (Abbasifard et al., 2014) like HNSW or IVF\nis the possible method for indexing such a vector\ndatabase. (c) Hybrid indexing, which is to build\nboth indexes on the corpus.\nEmbedding models\nSince we consider dense\nindexing, a high-quality embedding model is cen-\ntral to the retrieval performance. In this work, we\nconsider a series of embedding models, both open-\ndomain and in-domain, with base sizes: (a) BGE-\nbase (Xiao et al., 2023). (b) GTE-base (Li et al.,\n2023b). (c) MedCPT (Jin et al., 2023), a medi-\ncal document representation model initialized from\nPubMedBERT (Gu et al., 2021), and further pre-\ntrained on the retrieval tasks curated with PubMed.\n3.4\nRetrieval\nGiven a user’s query, the retrieval module selects\nthe top k most relevant documents from a pre-built\ncorpus, based on the similarity between the query\nand the documents. The generation model subse-\n"}, {"page": 4, "text": "quently uses these selected documents to formulate\nan appropriate response to the query. How the cor-\npus is indexed directly determines the retrieval al-\ngorithm: (a) Best Matching 25 (BM25) (Robertson\net al., 2009) algorithm is used for sparse index-\ning. (b) ANN search is used for dense retrieval.\n(c) hybrid search is used if the corpus uses hybrid\nindexing. Sophistic toolkits are available to imple-\nment the first two. To implement a hybrid search,\none first uses the former two methods and then\ncombines the search results with a 3:1 weight ra-\ntio for similarity scores to rerank the two lists of\nretrieved documents.\nQuery augmentation strategy for search\nIn a\nstandard RAG system, the user query is input into\nthe search engine to retrieve relevant documents.\nHowever, the original queries often have poor ex-\npression and lack semantic information (Gao et al.,\n2022), negatively impacting the retrieval results.\nThus, we evaluate four approaches to search inputs:\n(a) Vanilla input, that is, directly utilizing the user\nquery as search input. (b) Query rewriting, that\nis, refining queries and transforming the queries to\nsub-questions so that they can better match relevant\ndocuments. (c) Pseudo-response generation, that\nis, the model first generates a response, which will\nbe concatenated to the user query.\n3.5\nResponse generation\nprompting strategy\nWhen the retrieval proce-\ndure returns a list of referential documents, an LLM\nwill concatenate these contents at the front of the\nprompt and generate the final response. Other than\nrelevant documents, the prompt contains instruc-\ntions that reflect the prompting strategies: (a) Di-\nrect answer (DA): Given the question, the prompt\nasks the LLM to output the answer directly. (b)\nChain-of-thought (COT) (Wei et al., 2022) explic-\nitly asks the LLM to think step by step and demon-\nstrate the intermediate outputs. (c) COT-Refine.\nBuilding on COT and Self-Refine(Madaan et al.,\n2024), this strategy assumes a response without\nRAG has been generated, and this prompt asks the\nmodel to reflect on this response, utilize the re-\ntrieved documents to make necessary corrections,\nand finally draft a new response.\n4\nExperiments\nWe systematically evaluate the RAG workflow\non a series of knowledge-intensive tasks, provid-\ning a multidimensional analysis of different com-\nponents in RAG and paving the way for optimal\npractices for implementing RAG.\n4.1\nExperimental settings\nEvaluation datasets\nWe evaluate the RAG sys-\ntems on three benchmarks, investigating how RAG\nhelps the LLMs in open-domain or in-domain\nquestion-answering tasks and information extrac-\ntion tasks: (a) MMLU (Hendrycks et al., 2020). (b)\nPubMedQA (Jin et al., 2019). (c) PromptNER,\na mixture of samples from multiple named en-\ntity recognition tasks, including CoNLL-03 (Sang\nand De Meulder, 2003), OntoNotes 5.01 and\nBioNLP2004 (Collier and Kim, 2004). Introduc-\ntions and dataset statistics are in Appendix A.\nNow we elaborate on the steps we take to con-\nstruct the dataset for query classification:\n(i) Query collection. We collects: (a) 10k samples\nfrom the dev set and test set of the original MMLU\ndatasets, with no overlapping with the test set for\nRAG evaluation. (b) 10k samples from the auto-\nmatic labeled set of PubMedQA. (c) 7.9k dev set\nfrom PromptNER. Thus, we have a collection of\n27.9k samples with query-response pairs.\n(ii) Query labeling. We conduct automatic labeling\nof the above dataset using the LlaMA-2 13B model\nand log-likelihood function. For a sample with\na query-response pair, we first calculates the log-\nlikelihood of the response text conditioned only\non the query, that is, l0 = LLL(response|query).\nHere, LLL() is the log-likelihood function cal-\nculated with the given LlaMA-2 13B backbone.\nWe search the corpus with hybrid search (as in\nSection 3.4), and retrieve k = 8 documents,\ndenoted as docs. Conditioned on the docs and\nquery, the response’s log-likelihood becomes l1 =\nLLL(response|query, docs). The query is labeled\n\"need RAG\" (or label 1) if l1 −l0 > 0, and \"not\nneed RAG\" (label 0) otherwise. Using the above\nprocedure, there are 17.9% queries with the \"need\nRAG\" label (denoted as label 1).\n(iii) Dataset split.\nThe automatically labeled\nqueries are split into a 24k:2k:1.9k train/dev/test\nsplit.\nEvaluation metrics\nFor the MMLU and Pub-\nMedQA tasks, we will directly consider the correct-\nness of the final answers. Thus, we report accuracy\n(denoted as acc).\nFor the PromptNER task, the output response\ntext will first be parsed and transformed to a json\n1https://catalog.ldc.upenn.edu/LDC2013T19\n"}, {"page": 5, "text": "Model backbone\nAcc\nF1\nBERT-base\n0.586\n0.341\nRoBERTa-base\n0.638\n0.382\nDeBERTa-base\n0.637\n0.354\nTable 1: The performance of query classification.\ninstance. If the response can not be parsed to json,\nthen we consider the prediction as a null list. We\nadopt the instance-level strict micro-F1 following\nZhu et al. (2023), that is, the model predicts an\nentity correctly if and only if it correctly predicts\nall its components.\nOther than the performance matrices on the eval-\nuation datasets, we also measure the efficiency of\nthe RAG systems by the average latency (in sec-\nonds (s)) for completing the response for a test\nsample.\nSettings for query classification\nWe use the\nquery classification dataset introduced above to\nbuild a query classifier. The pre-trained backbones\nwe consider are: (a) BERT-base (Devlin et al.,\n2019), (b) RoBERTa-base (Liu et al., 2019), and (c)\nDeBERTa-base (He et al., 2020). For fine-tuning\nthese models, we utilize the HuggingFace Trans-\nformers package (Wolf et al., 2019), with batch\nsize 64, learning rate 2e-5, warm-up steps 100,\nAdamW optimizer (Loshchilov and Hutter, 2019).\nThe other hyper-parameters are kept the same with\nTransformers. For every 200 optimization steps, we\nrun evaluations on the dev set and save the model\ncheckpoint. The checkpoint with the best dev accu-\nracy is used as the final fine-tuned model to make\npredictions on the test set. And this checkpoint is\nused as a part of the RAG system.\nRetrieval corpus\nThe retrieval corpus consists\nof the following sources: (a) Wikipedia (English)\ncorpus2, a large-scale open-source encyclopedia\ncontaining 6.5 million documents for world knowl-\nedge. (b) PubMed3 is the most widely used litera-\nture resource, containing 23.9 million biomedical\narticles’ titles and abstracts. (c) a proprietary cor-\npus containing 1.3 million books or documents\nfrom science, education, and medicine.\nWhen\nprocessing the retrieval corpus, we set chunk size\nLc = 256.\nRetrieval settings\nFor sparse indexing, we uti-\nlize the ElasticSearch toolkit4 with the BM25 algo-\n2https://en.wikipedia.org/wiki/Wikipedia:\nDatabase_download\n3https://pubmed.ncbi.nlm.nih.gov/\n4https://www.elastic.co/\nrithm for search. For dense indexing, we utilize the\nFaiss5 toolkit to index the document vectors and\nimplement efficient vector search. The top k = 8\ndocument segments are retrieved and concatenated\nto the input prompt (if using RAG for response\ngeneration) for each query.\nLLM backbone\nOur experiments uses the most\nrecent open-sourced LLM, LlaMA-2-chat 13B re-\nleased by Meta (Touvron et al., 2023). After receiv-\ning a prompt or instruction, all the predictions are\ngenerated using the model’s pretrained language\nmodeling head (LM head). For decoding during\ninference, we use beam search with beam size 3.\nStrategy to determine the optimal practice\nTo\nbegin with, we consider the following settings for\nthe RAG system: sliding-window chunking as the\nchunking strategy, semantic vector indexing for\nindexing the retrieval corpus, BGE-base as embed-\nding model, pseudo-response generation for query\naugmentation, COT for prompting. Following the\nframework depicted in Figure 1, we optimize indi-\nvidual modules step-by-step, and select the most\neffective option among the possible choices. This\niterative process continued until we could not im-\nprove the average task score.\nSettings for the RAG system\nWith the help\nof the above strategy, we have locked down the\noptimal practice for the RAG system: small2big\nchunking as the chunking strategy, hybrid index-\ning for indexing the retrieval corpus, BGE-base as\nthe embedding model, using query classification,\npseudo-response generation for query augmenta-\ntion, COT-Refine for prompting. We denote this\nsetting as BP-RAG. We also consider the following\nsettings to demonstrate the superiority of BP-RAG:\n(a) No RAG, which is not to use RAG at all. This\nsetting is presented as a sanity check. (b) RAG_1,\nwhich substitute small2big chunking in BP-RAG\nto the vanilla chunking. (c) RAG_2, which substi-\ntute small2big chunking in BP-RAG to the sliding-\nwindow chunking. (d) RAG_3, which substitute\nhybrid indexing in BP-RAG to the sparse indexing.\nAs a result, RAG_3 will use BM25 for search and\nnot use an embedding model. (e) RAG_4, which\nsubstitute hybrid indexing in BP-RAG to the dense\nindexing. (f) RAG_5, which substitute BGE-base\nin BP-RAG to MedCPT, an in-domain embedding\nmodel. It is interesting to see whether MedCPT\nstill performs well on open-domain retrieval. (g)\nRAG_6, which substitute BGE-base in BP-RAG to\n5https://github.com/facebookresearch/faiss\n"}, {"page": 6, "text": "Method\nRAG Setting\nMMLU\nPubMedQA\nPromptNER\nAvg score\nAvg latency\nBP-RAG\nBP-RAG\n59.7\n56.9\n25.8\n47.5\n14.3\nNo RAG\n-\n49.3\n43.4\n20.6\n37.8\n10.8\nRAG_1\n+ vanilla chunking\n59.3\n55.9\n25.2\n46.7\n14.1\nRAG_2\n+ sliding-window chunking\n59.7\n56.1\n25.4\n47.1\n14.2\nRAG_3\n+ sparse indexing\n53.1\n47.3\n22.5\n40.9\n14.2\nRAG_4\n+ dense indexing\n58.9\n55.7\n25.3\n46.6\n14.3\nRAG_5\n+ MedCPT\n55.6\n57.1\n24.1\n45.6\n14.3\nRAG_6\n+ GTE-base\n59.3\n56.2\n25.7\n47.1\n14.2\nRAG_7\n- query classification\n58.5\n55.8\n25.1\n46.5\n20.7\nRAG_8\n+ query rewriting\n57.4\n54.5\n23.8\n45.2\n11.4\nRAG_9\n+ vanilla query\n56.2\n51.6\n22.6\n43.5\n11.0\nRAG_10\n+ COT\n58.2\n55.8\n24.4\n46.1\n14.2\nRAG_11\n+ direct answering\n54.9\n51.7\n21.9\n42.8\n3.7\nTable 2: Results of different RAG settings. The average score is calculated by averaging the scores of all tasks,\nwhile the average latency is measured in seconds per query.\nGTE-base. (h) RAG_7, which does not employ the\nquery classification module. That is, it will retrieve\ndocuments for any given query. (i) RAG_8, which\nsubstitute pseudo-response generation in BP-RAG\nto query rewriting. (j) RAG_9, which substitute\npseudo-response generation in BP-RAG to vanilla\nquery. (k) RAG_10, which substitute COT-Refine\nin BP-RAG to COT. (l) RAG_11, which substitute\nCOT-Refine in BP-RAG to direct answering.\n4.2\nResults\nThe results of the RAG settings are presented\nin Table 2, while the results for the query classi-\nfication tasks are reported in Table 1. Based on\nthe experimental results presented in Table 2, the\nfollowing observations can be made:\n(a) The benefit of using RAG. Compared with\nnot using RAG and making responses directly, BP-\nRAG significantly improves the performance by\n25.6% on average. However, one can not ignore\nthat the latency of BP-RAG is 32.4% higher than\nNo RAG.\n(b) Query classification module. Table 1 reports\nthe query classification performance on the query\nclassification dataset we developed. DeBERTa-\nbase (He et al., 2020) outperforms BERT-base (De-\nvlin et al., 2019) and RoBERTa-base (Liu et al.,\n2019) by achieving an accuracy of 65.3%. Accord-\ning to Table 2, this module is beneficial for both\nthe effectiveness and efficiency of the RAG system,\nleading to an improvement in the average score\nfrom 46.5% to 47.5% and a reduction in latency\ntime from 20.7 to 14.3 seconds per query. The op-\nerations in this module do not significantly increase\nthe overall latency of the system since classifying\none query with DeBERTa-base can be done in less\nthan 20 ms.\n(c) Chunking strategy. Table 2 demonstrates\nthat the small2big strategy slightly outperforms the\nother two chunking strategies, demonstrating the\nbenefit of retaining contextual information in the\nretrieved documents.\n(d) Indexing Module. The experimental results\nshow that the hybrid indexing strategy attains the\nhighest scores.\nHybrid indexing combines the\nsearch results from sparse and dense indices, mak-\ning the retrieved documents more informative.\n(e) Embedding models. Among the three base-\nsized embedding models, the BGE-base model\nworks the best with the RAG. The MedCPT model\nis further pre-trained on the PubMed corpus, mak-\ning it especially beneficial for the PubMedQA task,\nbut it is unsuitable for the open-domain MMLU\nand PromptNER tasks.\n(f) Query augmentation module. In this mod-\nule, query rewriting and pseudo-response genera-\ntion significantly increase the latency. The former\nincreases the latency by 3.6%, while the latter in-\ncreases the latency by 30.1%. However, pseudo-\nresponse generation benefits the RAG system by\nhelping retrieve more relevant and informative doc-\numents.\n(g) Prompting module.\nDirect answering is\nthe most efficient in this module, but it signifi-\ncantly underperforms compared to its two com-\npetitors. From Table 2, COT-Refine outperforms\nCOT, demonstrating that explicitly asking the LLM\nto reflect on the past response helps improve its\nperformance.\nThe experimental results demonstrate that each\n"}, {"page": 7, "text": "module contributes uniquely to the overall perfor-\nmance of the RAG system. Note that boosting\nperformance requires the RAG system to increase\nlatency in some of the modules. Thus, in certain\ntime-sensitive applications, industrial practitioners\ncan select settings different from BP-RAG, making\nan informed trade-off between performance and\nefficiency.\n5\nConclusion\nThis study investigates the best practices for\nimplementing a retrieval-augmented generation\n(RAG) system. First, we identify potential solu-\ntions for each module within the RAG framework.\nSecond, we conduct extensive experiments to sys-\ntematically assess these solutions and recommend\nthe most effective approach for each module. Dur-\ning this process, we demonstrate how different so-\nlutions affect the RAG system’s performance and\nlatency. Our findings contribute to a deeper un-\nderstanding of RAG systems and also provide key\ninsights for industrial applications.\nLimitations\nDespite the fact that we provide extensive ex-\nperiments for medical RAG on a wide collection\nof tasks, the following limitations remain: (a) We\nfocus on open-sourced LLMs. Powerful language\nmodels like GPT-4o, Gemini (Reid et al., 2024),\nClaude-36, Grok7 are not evaluated due to resource\nlimitation. (b) There are literature in RAG that\nadopt more complicated workflow than our RAG\nsystem (in Figure 1), such as iterative retrieval\n(Zhang et al., 2023a; Jiang et al., 2023). These\nmethods require more LLM inference times. These\nmore advanced RAG strategies have not been eval-\nuated in our current version, but we will address\nthis aspect in our updated version.\nEthical statement\nOur work’s investigations on the best practices\nof Retrieval-Augmented Generation (RAG) sys-\ntems presents significant societal benefits alongside\ncritical considerations. By integrating Retrieval-\nAugmented Generation (RAG) with Large Lan-\nguage Models (LLMs), the RAG framework en-\nhances access to reliable medical information, sup-\nporting clinical decision-making and improving pa-\n6https://www.anthropic.com/news/\nclaude-3-family.\n7https://github.com/xai-org/grok-1\ntient outcomes through evidence-based responses.\nThe system’s transparency—enabled by source at-\ntribution to retrieved documents—helps build trust\nin AI-assisted medical applications while mitigat-\ning \"black box\" concerns. Our investigations and\nexperimental results provide useful messages to the\nRAG systems in the industry.\nHowever, these advancements also present po-\ntential risks requiring proactive mitigation. Over-\nreliance on AI systems could inadvertently erode\nhuman clinical judgment, necessitating balanced\nimplementation where RAG serves as an assistive\ntool rather than a decision-maker. Workforce impli-\ncations and job displacement risks call for parallel\ninvestments in healthcare worker retraining pro-\ngrams. We aim to advance LLM technologies that\nethically augment medical expertise while preserv-\ning human oversight.\nReferences\nMohammad Reza Abbasifard, Bijan Ghahremani, and\nHassan Naderi. 2014. A survey on nearest neighbor\nsearch methods. International Journal of Computer\nApplications, 95(25).\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206–2240. PMLR.\nQingyu Chen, Jingcheng Du, Yan Hu, Vipina Kuttichi\nKeloth, Xueqing Peng, Kalpana Raja, Rui Zhang,\nZhiyong Lu, and Hua Xu. 2023. Large language\nmodels in biomedical natural language processing:\nbenchmarks, baselines, and recommendations. arXiv\npreprint arXiv:2305.16326.\nNigel Collier and Jin-Dong Kim. 2004. Introduction to\nthe bio-entity recognition task at JNLPBA. In Pro-\nceedings of the International Joint Workshop on Nat-\nural Language Processing in Biomedicine and its Ap-\nplications (NLPBA/BioNLP), pages 73–78, Geneva,\nSwitzerland. COLING.\n"}, {"page": 8, "text": "Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,\nWei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and\nMaosong Sun. 2023. Ultrafeedback: Boosting lan-\nguage models with high-quality feedback. ArXiv,\nabs/2310.01377.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNing Ding, Yujia Qin, Guang Yang, Fu Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Haitao Zheng, Jianfei\nChen, Yang Liu, Jie Tang, Juan Li, and Maosong\nSun. 2022. Delta tuning: A comprehensive study of\nparameter efficient methods for pre-trained language\nmodels. ArXiv, abs/2203.06904.\nGiacomo Frisoni, Miki Mizutani, Gianluca Moro, and\nLorenzo Valgimigli. 2022. Bioreader: a retrieval-\nenhanced text-to-text transformer for biomedical lit-\nerature. In Proceedings of the 2022 conference on\nempirical methods in natural language processing,\npages 5770–5793.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2022. Precise zero-shot dense retrieval without rele-\nvance labels. arXiv preprint arXiv:2212.10496.\nXiangxiang Gao, Wei Zhu, Jiasheng Gao, and Congrui\nYin. 2023a. F-pabee: Flexible-patience-based early\nexiting for single-label and multi-label text classifica-\ntion tasks. In ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 1–5. IEEE.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023b. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nZhao Guo, Yuan Ni, Keqiang Wang, Wei Zhu, and\nGuotong Xie. 2021. Global attention decoder for\nChinese spelling error correction. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 1419–1428, Online. Association\nfor Computational Linguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu\nChen.\n2020.\nDeberta:\nDecoding-\nenhanced bert with disentangled attention. ArXiv,\nabs/2006.03654.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nWilliam Hersh. 2024. Search still matters: informa-\ntion retrieval in the era of generative ai. Journal of\nthe American Medical Informatics Association, page\nocae014.\nMinbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jae-\nwoo Kang. 2024.\nImproving medical reasoning\nthrough retrieval and self-reflection with retrieval-\naugmented large language models. arXiv preprint\narXiv:2401.15269.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. 2023.\nAc-\ntive retrieval augmented generation. arXiv preprint\narXiv:2305.06983.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\nQiao Jin, Won Kim, Qingyu Chen, Donald C Comeau,\nLana Yeganova, W John Wilbur, and Zhiyong Lu.\n2023. Medcpt: Contrastive pre-trained transformers\nwith large-scale pubmed search logs for zero-shot\nbiomedical information retrieval.\nBioinformatics,\n39(11):btad651.\nQiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu,\nDonald Wright, Thomas Huang, W John Wilbur,\nZhe He, Andrew Taylor, Qingyu Chen, et al. 2024.\nAgentmd: Empowering language agents for risk pre-\ndiction with large-scale clinical tool learning. arXiv\npreprint arXiv:2402.13225.\nJakub Lála, Odhran O’Donoghue, Aleksandar Shtedrit-\nski, Sam Cox, Samuel G Rodriques, and Andrew D\nWhite. 2023. Paperqa: Retrieval-augmented gener-\native agent for scientific research. arXiv preprint\narXiv:2312.07559.\nHui Yi Leong, Yuheng Li, Yuqing Wu, Wenwen Ouyang,\nWei Zhu, Jiechao Gao, and Wei Han. 2025. Amas:\nAdaptively determining communication topology for\nllm-based multi-agent system. In Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing: Industry Track, pages 2061–\n2070.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\n"}, {"page": 9, "text": "Xiaonan Li, Kai Lv, Hang Yan, Tianya Lin, Wei Zhu,\nYuan Ni, Guo Tong Xie, Xiaoling Wang, and Xipeng\nQiu. 2023a. Unified demonstration retriever for in-\ncontext learning. ArXiv, abs/2305.04320.\nXiepeng Li, Zhexi Zhang, Wei Zhu, Zheng Li, Yuan\nNi, Peng Gao, Junchi Yan, and Guotong Xie. 2019.\nPingan smart health and SJTU at COIN - shared task:\nutilizing pre-trained language models and common-\nsense knowledge in machine reading tasks. In Pro-\nceedings of the First Workshop on Commonsense\nInference in Natural Language Processing, pages\n93–98, Hong Kong, China. Association for Computa-\ntional Linguistics.\nYuheng Li, Jiechao Gao, Wei Han, Wenwen Ouyang,\nWei Zhu, and Hui Yi Leong. 2025. Ft-mdt: Extract-\ning decision trees from medical texts via a novel\nlow-rank adaptation method. In Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing: Industry Track, pages 65–76.\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,\nPengjun Xie, and Meishan Zhang. 2023b. Towards\ngeneral text embeddings with multi-stage contrastive\nlearning. arXiv preprint arXiv:2308.03281.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin Raffel.\n2022. Few-shot parameter-efficient fine-tuning is\nbetter and cheaper than in-context learning. ArXiv,\nabs/2205.05638.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZequan Liu, Yi Zhao, Ming Tan, Wei Zhu, and\nAaron Xuxiang Tian. 2025. Para: Parameter-efficient\nfine-tuning with prompt aware representation adjust-\nment. arXiv preprint arXiv:2502.01033.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting for retrieval-\naugmented large language models. arXiv preprint\narXiv:2305.14283.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2024. Self-refine: Iterative refinement with\nself-feedback. Advances in Neural Information Pro-\ncessing Systems, 36.\nAakanksha Naik, Sravanthi Parasa, Sergey Feldman,\nLucy Lu Wang, and Tom Hope. 2021. Literature-\naugmented clinical outcome prediction.\narXiv\npreprint arXiv:2111.08374.\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023. Capabili-\nties of gpt-4 on medical challenge problems. arXiv\npreprint arXiv:2303.13375.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 11:1316–1331.\nVipula Rawte, Amit Sheth, and Amitava Das. 2023. A\nsurvey of hallucination in large foundation models.\narXiv preprint arXiv:2309.05922.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-\nrat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-\nlocking multimodal understanding across millions of\ntokens of context. arXiv preprint arXiv:2403.05530.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nKhaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno,\nDavid Stutz, Ellery Wulczyn, Fan Zhang, Tim\nStrother, Chunjong Park, Elahe Vedadi, et al. 2024.\nCapabilities of gemini models in medicine. arXiv\npreprint arXiv:2404.18416.\nErik F Sang and Fien De Meulder. 2003. Introduction\nto the conll-2003 shared task: Language-independent\nnamed entity recognition. arXiv preprint cs/0306050.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\net al. 2023a. Large language models encode clinical\nknowledge. Nature, pages 1–9.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Le Hou, Kevin Clark, Stephen\nPfohl, Heather Cole-Lewis, Darlene Neal, et al.\n2023b. Towards expert-level medical question an-\nswering with large language models. arXiv preprint\narXiv:2305.09617.\nShamane Siriwardhana, Rivindu Weerasekera, Elliott\nWen, Tharindu Kaluarachchi, Rajib Rana, and\nSuranga Nanayakkara. 2023. Improving the domain\nadaptation of retrieval augmented generation (rag)\nmodels for open domain question answering. Trans-\nactions of the Association for Computational Linguis-\ntics, 11:1–17.\nHaixia Sun, Jin Xiao, Wei Zhu, Yilong He, Sheng\nZhang, Xiaowei Xu, Li Hou, Jiao Li, Yuan Ni, and\nGuotong Xie. 2020. Medical knowledge graph to\n"}, {"page": 10, "text": "enhance fraud, waste, and abuse detection on claim\ndata: Model development and performance evalua-\ntion. JMIR Med Inform, 8(7):e17653.\nTianxiang Sun, Xiangyang Liu, Wei Zhu, Zhichao Geng,\nLingling Wu, Yilong He, Yuan Ni, Guotong Xie, Xu-\nanjing Huang, and Xipeng Qiu. 2022.\nA simple\nhash-based early exiting approach for language un-\nderstanding and generation. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 2409–2421, Dublin, Ireland. Association for\nComputational Linguistics.\nAaron Tian, Yi Zhao, Congrui Yin, Wei Zhu, Xing Tian,\nand Yi Ge. 2024a. Fanlora: Fantastic loras and where\nto find them in large language model fine-tuning. In\nProceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing: Industry\nTrack, pages 515–528.\nShubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai,\nQingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu\nChen, Won Kim, Donald C Comeau, et al. 2024b.\nOpportunities and challenges for chatgpt and large\nlanguage models in biomedicine and health. Brief-\nings in Bioinformatics, 25(1):bbad493.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Cantón Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel M. Kloumann, A. V.\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aure-\nlien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv, abs/2307.09288.\nLi Wang, Wei Zhu, Sihang Jiang, Sheng Zhang, Ke-\nqiang Wang, Yuan Ni, Guo Tong Xie, and Yanghua\nXiao. 2020. Mining infrequent high-quality phrases\nfrom domain-specific corpora. Proceedings of the\n29th ACM International Conference on Information\n& Knowledge Management.\nPengfei Wang, Huanran Zheng, Silong Dai, Wenjing\nYue, Wei Zhu, and Xiaoling Wang. 2024. Ts-tcd:\nTriplet-level cross-modal distillation for time-series\nforecasting using large language models.\narXiv\npreprint arXiv:2409.14978.\nPengfei Wang, Huanran Zheng, Qi’ao Xu, Silong Dai,\nYiqiao Wang, Wenjing Yue, Wei Zhu, Tianwen Qian,\nand Liang Zhao. 2025. Ts-htfa: Advancing time-\nseries forecasting via hierarchical text-free alignment\nwith large language models. Symmetry, 17(3):401.\nXuwu Wang, Lihan Chen, Wei Zhu, Yuan Ni, Guo Tong\nXie, Deqing Yang, and Yanghua Xiao. 2023a. Multi-\ntask entity linking with supervision from a taxon-\nomy. Knowledge and Information Systems, 65:4335\n– 4358.\nYubo Wang, Xueguang Ma, and Wenhu Chen. 2023b.\nAugmenting black-box llms with medical textbooks\nfor clinical question answering.\narXiv preprint\narXiv:2309.02233.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and\nDenny Zhou. 2022. Chain of thought prompting\nelicits reasoning in large language models. ArXiv,\nabs/2201.11903.\nWei Zhu Wenjing Yue and Xiaoling Wang. 2023.\nTcmeb: Performance evaluation of large language\nmodels based on traditional chinese medicine\nbenchmarks.\nhttps://github.com/ywjawmw/\nShenNong-TCM-Evaluation-BenchMark.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighof. 2023. C-pack: Packaged resources to\nadvance general chinese embedding. arXiv preprint\narXiv:2309.07597.\nTianfang Xie, Tianjing Li, Wei Zhu, Wei Han, and\nYi Zhao. 2024.\nPedro: Parameter-efficient fine-\ntuning with prompt dependent representation modifi-\ncation. arXiv preprint arXiv:2409.17834.\nYi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiao-\nhong Liu, Yue Fan, Qing Li, and Yuntao Du. 2024.\nParameter-efficient fine-tuning for pre-trained vision\nmodels: A survey. ArXiv, abs/2402.02242.\nGuangzhi\nXiong,\nQiao\nJin,\nZhiyong\nLu,\nand\nAidong Zhang. 2024.\nBenchmarking retrieval-\naugmented generation for medicine. arXiv preprint\narXiv:2402.13178.\nLingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui\nTao, and Fu Lee Wang. 2023. Parameter-efficient\nfine-tuning methods for pretrained language mod-\nels:\nA critical review and assessment.\nArXiv,\nabs/2312.12148.\nJiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun\nChen, and Qian Lou. 2024. Badrag: Identifying vul-\nnerabilities in retrieval augmented generation of large\nlanguage models. arXiv preprint arXiv:2406.00083.\n"}, {"page": 11, "text": "Ellen Yi-Ge, Jiechao Gao, Wei Han, and Wei Zhu.\n2024.\nDrum: Learning demonstration retriever\nfor large multi-modal models.\narXiv preprint\narXiv:2412.07619.\nHuiming Yin, Kun Wang, Ruyu Yang, Yanfang Tan,\nQiang Li, Wei Zhu, and Suzi Sung. 2024. A ma-\nchine learning model for predicting acute exacerba-\ntion of in-home chronic obstructive pulmonary dis-\nease patients. Computer Methods and Programs in\nBiomedicine, 246:108005.\nCyril Zakka, Rohan Shad, Akash Chaurasia, Alex R\nDalal, Jennifer L Kim, Michael Moor, Robyn Fong,\nCurran Phillips, Kevin Alexander, Euan Ashley,\net al. 2024.\nAlmanac—retrieval-augmented lan-\nguage models for clinical medicine.\nNEJM AI,\n1(2):AIoa2300068.\nFengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin\nLiu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and\nWeizhu Chen. 2023a. Repocoder: Repository-level\ncode completion through iterative retrieval and gen-\neration. arXiv preprint arXiv:2303.12570.\nJingfang Zhang, Ming Tan, Pengyu Dai, and Wei-Guo\nZhu. 2023b.\nLeco: Improving early exiting via\nlearned exits and comparison-based exiting mech-\nanism. In Annual Meeting of the Association for\nComputational Linguistics.\nJuyuan Zhang, Jiechao Gao, Wenwen Ouyang, Wei Zhu,\nand Hui Yi Leong. 2025. Time-llama: Adapting\nlarge language models for time series modeling via\ndynamic low-rank adaptation. In Proceedings of the\n63rd Annual Meeting of the Association for Com-\nputational Linguistics (Volume 4: Student Research\nWorkshop), pages 1145–1157.\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng\nShen, Matei Zaharia, Ion Stoica, and Joseph E Gonza-\nlez. 2024. Raft: Adapting language model to domain\nspecific rag. arXiv preprint arXiv:2403.10131.\nXinpeng Zhang, Ming Tan, Jingfan Zhang, and Wei\nZhu. 2023c. Nag-ner: a unified non-autoregressive\ngeneration framework for various ner tasks. In An-\nnual Meeting of the Association for Computational\nLinguistics.\nYuming Zhang, Xiangxiang Gao, Wei Zhu, and Xiaol-\ning Wang. 2023d. Fastner: Speeding up inferences\nfor named entity recognition tasks. In International\nConference on Advanced Data Mining and Applica-\ntions.\nYuming Zhang, Peng Wang, Ming Tan, and Wei-Guo\nZhu. 2023e. Learned adapters are better than man-\nually designed adapters. In Annual Meeting of the\nAssociation for Computational Linguistics.\nZhen Zhang, Wei Zhu, Jinfan Zhang, Peng Wang, Rize\nJin, and Tae-Sun Chung. 2022. PCEE-BERT: Ac-\ncelerating BERT inference via patient and confident\nearly exiting. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022, pages 327–338,\nSeattle, United States. Association for Computational\nLinguistics.\nZhexi Zhang, Wei Zhu, Junchi Yan, Peng Gao, and\nGuowang Xie. 2021. Automatic student network\nsearch for knowledge distillation. 2020 25th Inter-\nnational Conference on Pattern Recognition (ICPR),\npages 2446–2453.\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren\nWang, Yunteng Geng, Fangcheng Fu, Ling Yang,\nWentao Zhang, and Bin Cui. 2024.\nRetrieval-\naugmented generation for ai-generated content: A\nsurvey. arXiv preprint arXiv:2402.19473.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nSurvey of Large Language Models. arXiv e-prints,\npage arXiv:2303.18223.\nHuanran Zheng, Wei Zhu, Pengfei Wang, and Xiaol-\ning Wang. 2023. Candidate soups: Fusing candi-\ndate results improves translation quality for non-\nautoregressive translation. ArXiv, abs/2301.11503.\nHuanran Zheng, Wei Zhu, and Xiaoling Wang. 2024.\nNat4at: Using non-autoregressive translation makes\nautoregressive translation faster and better. In Pro-\nceedings of the ACM on Web Conference 2024, pages\n4181–4192.\nXiaofeng Zhou, Yuan Ni, Guotong Xie, Wei Zhu, Cai\nChen, Tianhao Wang, and Zhigang Pan. 2019. Anal-\nysis of the health information needs of diabetics in\nchina. In MEDINFO 2019: Health and Wellbeing\ne-Networks for All, pages 487–491. IOS Press.\nWei Zhu. 2021a. Leebert: Learned early exit for bert\nwith cross-level optimization. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 2968–2980.\nWei Zhu. 2021b. Mvp-bert: Multi-vocab pre-training\nfor chinese bert. In Annual Meeting of the Associa-\ntion for Computational Linguistics.\nWei Zhu. 2026a.\nMrag:\nBenchmarking retrieval-\naugmented generation for bio-medicine.\narXiv\npreprint arXiv:2601.16503.\nWei Zhu. 2026b.\nMrag:\nBenchmarking retrieval-\naugmented generation for bio-medicine.\narXiv\npreprint arXiv:2601.21767.\nWei Zhu, Yilong He, Ling Chai, Yuanchun Fan, Yuan Ni,\nGuo Tong Xie, and Xiaoling Wang. 2021a. paht_nlp\n@ mediqa 2021: Multi-grained query focused multi-\nanswer summarization. In Workshop on Biomedical\nNatural Language Processing.\n"}, {"page": 12, "text": "Wei Zhu, Wenfeng Li, Xiaoling Wang, Wendi Ji, Yuan-\nbin Wu, Jin Chen, Liang Chen, and Buzhou Tang.\n2023a. Extracting decision trees from medical texts:\nAn overview of the text2dt track in chip2022. In\nHealth Information Processing. Evaluation Track Pa-\npers, pages 89–102, Singapore. Springer Nature Sin-\ngapore.\nWei Zhu, Wenfeng Li, Xiaoling Wang, Wendi Ji, Yuan-\nbin Wu, Jin Chen, Liang Chen, and Buzhou Tang.\n2023b. Extracting decision trees from medical texts:\nAn overview of the text2dt track in chip2022. In\nHealth Information Processing. Evaluation Track Pa-\npers, pages 89–102, Singapore. Springer Nature Sin-\ngapore.\nWei Zhu, Yuan Ni, Xiaoling Wang, and Guotong Xie.\n2021b. Discovering better model architectures for\nmedical query understanding. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies: Industry Papers, pages\n230–237, Online. Association for Computational Lin-\nguistics.\nWei Zhu, Yuan Ni, Guo Tong Xie, Xiaofeng Zhou,\nand Cai Chen. 2019a. The dr-kgqa system for au-\ntomatically answering medication related questions\nin chinese. 2019 IEEE International Conference on\nHealthcare Informatics (ICHI), pages 1–6.\nWei Zhu, Yuan Ni, Guotong Xie, Xiaofeng Zhou, and\nCai Chen. 2019b. The dr-kgqa system for automati-\ncally answering medication related questions in chi-\nnese. In 2019 IEEE International Conference on\nHealthcare Informatics (ICHI), pages 1–6. IEEE.\nWei Zhu and Ming Tan. 2023. SPT: Learning to se-\nlectively insert prompts for better prompt tuning.\nIn Proceedings of the 2023 Conference on Empir-\nical Methods in Natural Language Processing, pages\n11862–11878, Singapore. Association for Computa-\ntional Linguistics.\nWei Zhu, Aaron Xuxiang Tian, Congrui Yin, Yuan\nNi, Xiaoling Wang, and Guotong Xie. 2024. Iapt:\nInstruction-aware prompt tuning for large language\nmodels. arXiv preprint arXiv:2405.18203.\nWei Zhu, Peifeng Wang, Yuan Ni, Guo Tong Xie, and\nXiaoling Wang. 2023c. Badge: Speeding up bert\ninference after deployment via block-wise bypasses\nand divergence-based early exiting. In Annual Meet-\ning of the Association for Computational Linguistics.\nWei Zhu, Peng Wang, Xiaoling Wang, Yuan Ni, and\nGuotong Xie. 2023d. Acf: aligned contrastive fine-\ntuning for language and vision tasks. In ICASSP\n2023-2023 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n1–5. IEEE.\nWei Zhu, Xiaoling Wang, Mosha Chen, and Buzhou\nTang. 2023e. Overview of the promptcblue shared\ntask in chip2023. ArXiv, abs/2312.17522.\nWei Zhu, Xiaoling Wang, Yuan Ni, and Guotong Xie.\n2021c. GAML-BERT: Improving BERT early exit-\ning by gradient aligned mutual learning. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 3033–3044,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nWei Zhu, Xiaoling Wang, Huanran Zheng, Mosha Chen,\nand Buzhou Tang. 2023. PromptCBLUE: A Chinese\nPrompt Tuning Benchmark for the Medical Domain.\narXiv e-prints, page arXiv:2310.14151.\nWei Zhu, Xiaofeng Zhou, Keqiang Wang, Xun Luo,\nXiepeng Li, Yuan Ni, and Guotong Xie. 2019c. Panlp\nat mediqa 2019: Pre-trained language models, trans-\nfer learning and knowledge distillation. In Proceed-\nings of the 18th BioNLP Workshop and Shared Task,\npages 380–388.\nYuhui Zuo, Wei Zhu, and Guoyong GUET Cai. 2022.\nContinually detection, rapidly react: Unseen rumors\ndetection based on continual prompt-tuning.\nIn\nProceedings of the 29th International Conference\non Computational Linguistics, pages 3029–3041,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nA\nAppendix: Datasets\nThe datasets we experiment on are as follows:\n• MMLU\nThe Massive Multitask Lan-\nguage Understanding (MMLU) benchmark\n(Hendrycks et al., 2020) has been introduced\nto assess the knowledge gained by large lan-\nguage models during pretraining, specifically\nin zero-shot and few-shot scenarios. This ap-\nproach aims to make the evaluation process\nmore rigorous and akin to human assessment\nstandards. The MMLU covers 57 diverse top-\nics, spanning STEM, humanities, social sci-\nences, and many others, with questions rang-\ning from elementary school to advanced pro-\nfessional levels. It evaluates not only factual\nknowledge but also problem-solving skills,\ncovering a wide array of fields from conven-\ntional subjects like mathematics and history\nto more specialized domains such as law and\nethics. The extensive range and detailed cover-\nage of these subjects make the MMLU partic-\nularly effective for pinpointing areas where a\nmodel may struggle. Initially, the dataset com-\nprises a development set of 1,500 samples and\na test set of 14.1k samples. For our purposes,\nwe have selected 50 test samples from each\nof the 57 categories, creating a subset of 2.8k\ntest samples.\n"}, {"page": 13, "text": "• PubMedQA\nPubMedQA (Jin et al., 2019)\nis a biomedical research QA dataset. The\ndataset is released under the CC BY 4.0\n(Creative Commons Attribution 4.0 Inter-\nnational) license and released in https://\npubmedqa.github.io/. It has 1k manually an-\nnotated questions constructed from PubMed\nabstracts. To test the capability of RAG sys-\ntems to find related documents and answer\nthe question accordingly, we discard the rel-\nevant context for each question originally in-\ncluded in the dataset. The possible answer to\na PubMedQA question can be yes/no/maybe,\nreflecting the authenticity of the question state-\nment based on biomedical literature. This task\nhas a set of 1.0k manually labeled PubMedQA\nsamples, which will be considered the test set\nfor evaluating the RAG system. And the set of\n211k automatic labeled samples will be used\nto construct the dataset for query classifica-\ntion.\n• PromptNER\nThis dataset is a mixture of\nsamples from multiple named entity recogni-\ntion tasks:\n– CoNLL-03 (Sang and De Meulder,\n2003). The CoNLL 2003 Named Entity\nRecognition (NER) dataset, introduced\nas part of the Conference on Natural Lan-\nguage Learning (CoNLL) shared task in\n2003, is a cornerstone benchmark for\nassessing named entity recognition sys-\ntems. This dataset features a train/de-\nv/test split of 14,041:3,250:3,453, mak-\ning it a valuable resource for NLP re-\nsearchers and practitioners.\n– OntoNotes 5.08. A comprehensive re-\nsource in Natural Language Processing\n(NLP), OntoNotes 5.0 extends its prede-\ncessors with rich annotations covering\npart-of-speech tagging, syntactic parsing,\nsemantic role labeling, and named entity\nrecognition across multiple languages,\nincluding English, Chinese, Arabic, and\nmore. For NER, it categorizes entities\ninto predefined classes such as Person,\nLocation, Organization, and Misc (for\nmiscellaneous entities). The dataset’s di-\nversity, encompassing various text genres\nlike newswire, web content, and conver-\n8https://catalog.ldc.upenn.edu/LDC2013T19\nsational transcripts, makes it an excel-\nlent choice for training robust machine\nlearning models capable of handling a\nwide range of contexts. The English por-\ntion of the NER dataset is divided into a\n59,924/8,528/8,262 train/dev/test split.\n– BioNLP2004 (Collier and Kim, 2004).\nDesigned specifically for the bioinfor-\nmatics and biomedical domains, the\nBioNLP2004 NER dataset was launched\nas part of the BioNLP Shared Task 2004\nto evaluate systems that automatically\nidentify named entities within biomedi-\ncal texts. Comprising abstracts from the\nPubMed database, this dataset is metic-\nulously annotated with gene and pro-\ntein names, serving as a critical tool\nfor developing and testing NER mod-\nels in biological literature. It offers a\n16,619/1,927/3,856 train/dev/test distri-\nbution, facilitating the training and eval-\nuation phases of NER models in this spe-\ncialized field.\n"}]}