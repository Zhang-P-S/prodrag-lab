{"doc_id": "arxiv:2511.21006", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.21006.pdf", "meta": {"doc_id": "arxiv:2511.21006", "source": "arxiv", "arxiv_id": "2511.21006", "title": "TrackList: Tracing Back Query Linguistic Diversity for Head and Tail Knowledge in Open Large Language Models", "authors": ["Ioana Buhnila", "Aman Sinha", "Mathieu Constant"], "published": "2025-11-26T03:14:09Z", "updated": "2025-11-27T05:15:16Z", "summary": "Large Language Models (LLMs) have proven efficient in giving definition-type answers to user input queries. While for humans giving various types of answers, such as examples and paraphrases, is an easy task, LLMs struggle to provide correct answers for other than definition-type queries. In this study, we evaluated this drop in performance using TrackList, a fine-grained linguistic and statistical analysis pipeline to investigate the impact of the pre-training data on LLMs answers to diverse linguistic queries. We also introduce RefoMed-EN, an English dataset consisting of 6170 human-annotated medical terms alongside their corresponding definitions, denominations, exemplifications, explanations, or paraphrases. We studied whether the high frequency of a concept (head) or low frequency (tail) impacts the language model's performance. We evaluated the quality of the LLM's output using syntactic and semantic similarity metrics, statistical correlations and embeddings. Results showed that the LLM's task performance for definition type questions is the highest, while for the exemplification type it is the lowest. Additionally, we showed that for definition-type questions, large language models are prone to paraphrase more on popular and frequent knowledge and less on tail and technical knowledge, especially in the expert texts.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.21006v2", "url_pdf": "https://arxiv.org/pdf/2511.21006.pdf", "meta_path": "data/raw/arxiv/meta/2511.21006.json", "sha256": "b23505b05a52c8e33b23ade745dd759757bf26846cbfd9bd6f190d7361ba3dd1", "status": "ok", "fetched_at": "2026-02-18T02:26:12.294510+00:00"}, "pages": [{"page": 1, "text": "TrackList: Tracing Back Query Linguistic Diversity for Head and Tail\nKnowledge in Open Large Language Models\nIoana Buhnila1 2, Aman Sinha1 and Mathieu Constant1\n1ATILF, University of Lorraine - CNRS, France\n2Center for Data Science in Humanities, Chosun University, South Korea\nfirstname.lastname@univ-lorraine.fr\nAbstract\nLarge Language Models (LLMs) have proven efficient in giving definition-type answers to user input queries.\nWhile for humans giving various types of answers, such as examples and paraphrases, is an easy task, LLMs\nstruggle to provide correct answers for other than definition-type queries. In this study, we evaluated this drop in\nperformance using TrackList, a fine-grained linguistic and statistical analysis pipeline to investigate the impact of\nthe pre-training data on LLMs answers to diverse linguistic queries. We also introduce RefoMed-EN, an English\ndataset consisting of 6170 human-annotated medical terms alongside their corresponding definitions, denominations,\nexemplifications, explanations, or paraphrases. We studied whether the high frequency of a concept (head) or\nlow frequency (tail) impacts the language model’s performance. We evaluated the quality of the LLM’s output\nusing syntactic and semantic similarity metrics, statistical correlations and embeddings. Results showed that\nthe LLM’s task performance for definition type questions is the highest, while for the exemplification type it is\nthe lowest. Additionally, we showed that for definition-type questions, large language models are prone to para-\nphrase more on popular and frequent knowledge and less on tail and technical knowledge, especially in the expert texts.\nKeywords: query linguistic diversity, head and tail knowledge, open large language model\n1.\nIntroduction\nAs Large Language Models are becoming widely\nused in many writing tasks and are applied to\ndifferent fields, the need to explain and interpret\nthe generated responses has become crucial. In\norder to be trusted and used in sensitive fields\nsuch as medicine, finance or law, understand-\ning the real capabilities of LLMs is indispensable.\nMethods such as In-Context Learning (ICL) (Dong\net al., 2024), Chain-of-Thought (CoT) prompting\n(Wu et al., 2023), and Retrieval Augmented Gen-\neration systems (RAG) (Lewis et al., 2020) have\ncontributed to further interpret the LLM’s generated\nresponse, and help mitigate hallucinations (Akbar\net al., 2024; Huang et al., 2024; Asai et al., 2023).\nBlack box models like ChatGPT and other propri-\netary models showed limitations in terms of ex-\nplainability and reproducibility (Zhao et al., 2024a;\nLiesenfeld et al., 2023; Ravichandran et al., 2024).\nIn this work we explored non-proprietary and\nopen Large Language Models, like OLMo (Groen-\neveld et al., 2024) and Pythia (Biderman et al.,\n2023). We chose these language models (LMs)\nbecause they are fully open, thus fostering inter-\npretability and reproducibility. These LLMs were\nreleased together with model weights, inference\ncode, training/evaluation code, and pretraining cor-\npus, DOLMA (Soldaini et al., 2024) for OLMo, and\nThe Pile (Gao et al., 2020) for Pythia. We further in-\nvestigated OLMo and Pythia’s linguistic capabilities\nby conducting a fine-grained linguistic and statistic\nGive examples of\ncardiovascular disease\nA heart attack\nmupirocin and fusidic acid\n \n    freq.\n3303397\nGive examples\nof antibiotic creams\nThe skin is the largest\norgan in the body.\nHead term\nTail term\nLM answer\nLM answer\n     freq.\n11450\npeople with hypertension, \nincreased serum lipids or diabetes\n \n0.92\n \n0.78\nFigure 1: Large Language Models tend to gener-\nate more hallucinated definition-style outputs even\nwhen directly asked to answer an example-style\nquery for tail knowledge terms.\nanalysis of how a language model uses pretraining\ndata in the composition of the generated text. We\ntested these models in the most widespread real\nworld use-case: open question-answering, mean-\ning the model gives a free text answer to a user\nquestion (Shailendra et al., 2024). LMs are most\nefficient in question-answering tasks that cover pop-\nular information (head knowledge) usually from the\ngeneral domain (Mallen et al., 2023; Li et al., 2024a)\nand when giving definitions to concepts. However,\ntheir performance drops when tested on torso or tail\nknowledge (Sun et al., 2023; Kandpal et al., 2023)\nor when tackling complex questions (Daull et al.,\narXiv:2511.21006v2  [cs.CL]  27 Nov 2025\n"}, {"page": 2, "text": "2023). Our study analyzed the impact of query\nlinguistic diversity on QA performance, and demon-\nstrated that LLMs perform worse when asked to\ngive different types of answers, such as examples,\nto tail medical knowledge (as shown in Figure 1).\nWe further explored this research question by\napplying our experiments to the medical domain, as\nit contains a wide range of head and tail knowledge\nconcepts, such as cancer, asthma on the head side,\nand complex and rare medical concepts such as\nideatory apraxia or erythematous angina for rare\nmedical knowledge. Furthermore, we evaluated the\nLLM’s performance for different types of questions\nby exploring term requency in the pre-training data.\nOur research questions are the following: (RQ1)\nDoes the frequency of a concept in the pretraining\ndata influence a language model’s answer quality?\n(RQ2) How does the linguistic diversity and com-\nplexity of questions impact the language model abil-\nity to give accurate answers? (RQ3) To what extent\nare language models paraphrasing the data from\ntheir parametric memory for head and tail knowl-\nedge? We hypothesize that frequent (head) terms\nmay lead to distributional memorization, while less\nfrequent terms (tail) may result in lower downstream\ntask performance due to reliance on parametric\nknowledge (Wang et al., 2024).\nTo explore these hypothesis, we propose a fine-\ngrained analysis and evaluation pipeline, Track-\nList, to trace back query linguistic diversity of\nhead and tail knowledge in Open Large Language\nModels. Our contributions are the following:\n1. We built and share our fine-grained linguis-\ntic analysis pipeline, TrackList, to evalu-\nate how LLMs answer to five different types\nof questions according to pragmatic func-\ntions of human communication (definitions, ex-\nemplifications, explanations, denominations\nand paraphrases) using a non-contaminated\nbenchmark, RefoMed-EN. We traced the\nLLM’s answers back to their pretraining cor-\npora, DOLMA and The Pile, and showed that\nLLMs paraphrase more on head terms and are\nless semantically diverse when dealing with\ntail concepts. We will share the TrackList\ncode with the research community to ensure\nreproducibility and to foster new analysis and\nmetrics on LLMs’ linguistic abilities in diverse\nopen QA settings.\n2. We share the plug-and-play medical QA\ndataset, RefoMed-EN, an English dataset of\n6170 human annotated medical terms along-\nside with their context, discourse markers,\ncorresponding paraphrases, lexical and prag-\nmatic functions. The dataset will be available\nwith open license.\n3. We conducted a detailed analysis of the LLMs’\nperformance according to the linguistic com-\nplexity of the user question. We showed that\nLLMs have limited understanding of linguistic\ndifferences in queries, as they are more prone\nto giving definition-style answers to all queries,\neven when explicitly asked to give examples\nor a paraphrase for a concept.\n2.\nRelated Work\nIn the medical domain, LLM’s output must be cor-\nrect, therefore methods that combine ICL and RAG\nshowed promising results, such as improving the\naccuracy of GPT4-Turbo’s answers to oncology\nquestions from 62.5 to 83.3% (with ICL) and 79.2%\nof questions (with RAG) (Iivanainen et al., 2024).\nMoreover, QA frameworks like Self-BioRAG that an-\nswer biomedical questions by reflecting and retriev-\ning relevant documents showed a 7.2% improve-\nment on average over the state-of-the-art open\nSmall Language Models (SLM) of 7b or less (Jeong\net al., 2024).\nLarge medical QA datasets like MedQuAD\n(Ben Abacha and Demner-Fushman, 2019) and\nMEDIQA (Abacha et al., 2019) gather different\ntypes of medical data, such as treatment, symp-\ntoms, definition, susceptibility or prevention. These\ndatasets are built for medical experts, more than\nfor the general public interested in understanding\nmedical concepts (Nguyen et al., 2023). Moreover,\nthese datasets are shared in a XML format, not\nallowing easy implementation for LLMs prompts.\nFurthermore, the dataset is divided in more medical\nthan linguistic criteria.\nWhile evaluating LLMs answers to open ques-\ntions is difficult, recent studies showed that human\nannotation is still needed when the generated an-\nswer does not match a gold standard (Kamalloo\net al., 2023). In our work, we combine both human\nevaluation and automatic evaluation through simi-\nlarity metrics that we further develop in section 3.2.\nDetermining what is head and tail knowledge is a\nresearch question in itself, as recent studies inves-\ntigated this in detail (Mallen et al., 2023; Li et al.,\n2024b). We considered that the word frequency in\nthe pretraining data as the popularity metric for our\nmedical dataset.\nRecent studies investigated whether LLMs mem-\norize or generalize knowledge by exploring open\nlanguage models. Wang et al. (2024) introduce\nthe concept of distributional memorization, to mea-\nsure the correlation between the LLM output prob-\nabilities and the frequency of the pretraining data.\nThe authors evaluated the task performance with\n3 to 5 n-grams search into The Pile (Gao et al.,\n2020), the pretraining corpus of the language model\nPythia (Biderman et al., 2023). They found that\nLLMs generalize more in reasoning-intensive tasks,\n"}, {"page": 3, "text": "TERM / LM ANSWER \nCLS EMBEDDING\nSTATISTICAL EVALUATION\nLM / GOLD \nSEMANTIC EVALUATION\nLM QA \nDOCS / TERM FREQUENCY\nGive a definition for: \nCystic fibrosis is a\ngenetic disease that\naffects the lungs and\ndigestive system.\nNb of docs/term\nRefoMed Gold\nBERTscore\nPearson\ncorrelations \nterm / answer n-grams\nfrequency\nRefoMed\n1\n2\n3\n4\n5\ncystic fibrosis\ndiabetes\ncystic fibrosis 889800\ndiabetes 19011246\nFigure 2: The pipeline of our method represented in five steps. 1) The zero-shot inference QA task using\nthe medical concepts from the RefoMed-EN dataset. The dataset was divided in subdatasets according\nto the query type (detailed presentation in section 3.4). 2) We obtained the frequency in terms of number\nof documents for each RefoMed-EN concept. 3) We calculated the BERTScore between the LMs output\nand the RefoMed-EN gold standard. 4) These two values were used to compute Pearson correlations. 5)\nWe computer a probability metric between the CLS embedding score between the term and the n-grams\nof the output, and their frequency in the pre-training corpora.\nwhile they memorize more in simpler knowledge-\nintensive tasks (Wang et al., 2024). This finding is\ninteresting for our study, as we evaluate how LLMs\nanswer to medical knowledge questions. In this\nsense, we hypothesize that more frequent knowl-\nedge (head) is easier to be memorized by the lan-\nguage model. Concurrent work investigated the\nconcept of linguistic diversity in LLMs answers by\nevaluating their lexical, syntactic and semantic dis-\ntribution compared to human linguistic richness\n(Guo et al., 2024).\nIn our study, we investigated the linguistic com-\nplexity of LLM queries from a syntactic, semantic\nand pragmatic perspective, applied to a knowledge\nintensive task, medical QA. We present our full\nmethod below.\n3.\nMethodology\nWe illustrate our method in Figure 2. We conducted\nour experiments using four fully open language\nmodels, OLMo-1b, OLMo-7b, OLMo-7b-instruct,\nand Pythia-1b. To access the pre-training corpora,\nWe used the WIMBD tool’s API (Elazar et al., 2023)\nfor the DOLMA corpus (OLMo) and the infini-\ngram library (Liu et al., 2024) for The Pile dataset\n(Pythia). We counted the number of documents that\ncontain a certain medical concept, thus determining\nits frequency. We present our detailed rationale in\nthe section below.\n3.1.\nFormulation\nWe utilize a language model MLLM and assume\naccess to a corpora C with D documents which\nwas used to train MLLM. We represent the corpora\nC by a set of unique words {w1, w2, w3, .....w∞}\npresent in at least one document d ∈D. Each of\nthe document d ∈D can be similarly represented by\na subset of C as {dw1, dw2, dw3, .....dw∞} where\ndwi represents word wi present in document d. We\ndefine a document-frequency count operator over\ncorpora C and denote it by Cdf. It enables us to\ncount the number of documents present in corpora\nC which contains the provided term at least once1.\nWe further utilize query-answer (Q, A) pairs\nwhere a question denoted by q ∈Q can be of differ-\nent types as mentioned in table 1 and an reference\nanswer a ∈A which is borrowed from the RefoMed-\nEN dataset. In our experiment setup, we provide\nas input q ∈Q to the language model as follows\nˆa = MLLM(q)\n(1)\nIn the above equation, we denote the generated\nresponse from the MLLM as ˆa. During evaluation,\nwe consider different evaluation metrics (denoted\nby Ψ) to calculate the correctness (denoted by E)\nof the generated response ˆa with respect to the\ngold reference a ∈A as follows\nE = Ψ(ˆa, a)\n(2)\n1For example, Cdf(\"infection\" | C) gives 543435456,\nwhich implies that there are 543435456 documents in C\ncontaining the term infection at least once.\n"}, {"page": 4, "text": "Lower value of E, for example, in case of\nBERTscore, implies less relevance of generated\nanswer and vice versa.\n3.2.\nEvaluation Metrics\nAs the goal of our study is to advance the in-\nterpretability of open large language models, we\nchose simple and easy to interpret metrics that al-\nlow us to analyze the relationship between query\nterm frequency, gold answers, and generated LLM\nanswers.\nTracing back semantic similarity\nWe calculated\nthe BERTscore (Zhang et al., 2019) between the\ngenerated answers and the gold standard from the\nRefoMed-EN dataset.\nStatistical correlation\nWe computed the Pear-\nson correlation metrics to compare the BERTscore\ncalculated before with the term’s frequency in the\npretraining corpus.\nCLS cosine similarity\nWe computed the cosine\nsimilarity of sentence embeddings to compare the\nsemantic similarity between the medical term and\nthe n-grams of the generated answer.\n3.3.\nLinguistically Annotated Dataset\nWe constructed RefoMed-EN by automatically\ntranslating RefoMed2 (Buhnila et al., 2024) from\nits original version in French to English using a li-\ncensed DeepL Translator API 3. We chose to trans-\nlate this dataset because there is no benchmark\nannotated on question types following pragmatic\nand linguistic theories (to the best of our knowl-\nedge). The translation of a French dataset assures\nthat there is no benchmark contamination between\nthe pretraining corpora and the test dataset (Sainz\net al., 2023; Li et al., 2024c). We will share this new\nnon-contaminated benchmark with the NLP com-\nmunity on github. We present below the question\ntypes explored in this study.\n3.4.\nQuery Linguistic Diversity\nLinguistically, the concept of reformulation is de-\nfined as textual and discursive act performed with a\n2RefoMed is a French annotated dataset of 6170 an-\nnotated medical terms with their corresponding reformu-\nlations. The dataset is comprised of paraphrases and\nreformulations semi-automatically extracted from scien-\ntific and popularization medical texts and abstracts from\nthe ClassYN (Todirascu et al., 2012) and CLEAR (Grabar\nand Cardon, 2018) French medical corpora.\n3The dataset required extensive pre-processing as\nthere were formatting and translation inaccuracies from\nthe original annotation.\nprecise objective (Grabar and Eshkol, 2016). Refor-\nmulations have a well defined pragmatic role by ex-\npressing a content in a different semantic or lexical\nrepresentation, adapted to a specific audience and\ncommunicational need, like science popularization\nor education. The linguistic diversity is correlated to\nthe pragmatic usage of language in humans, such\nas asking for a definition, an explanation, reformu-\nlation or paraphrase of a concept, or to receive\nexamples of a certain entity (Grabar and Eshkol,\n2016).\nIn this work, we analyzed the role of reformula-\ntions in the case of medical knowledge populariza-\ntion for laypeople and patients Grabar and Eshkol\n(2016).\nWe focused on a knowledge intensive\nquestion-answering task taking into account the\nfive most common types of questions that require\nreformulation processes, as shown with examples\nin Table 1. We present the number of questions by\ntype from the RefoMed-EN dataset in Table 2.\n4.\nExperimental Setup\nThis section presents the experiments we con-\nducted to evaluate the generated text according\nto query types and term frequency. Experiments\nwere done on a P100 NVIDIA GPU, for an individ-\nual runtime of ≤15 hours including different steps\ninvolved.\n4.1.\nLinguistic Diversity and Frequency\nfor Task Performance\nWe explored the impact the query linguistic diver-\nsity has on the quality of the generated answer.\nWe evaluated the task performance of OLMo and\nPythia models in a zero-shot QA setting by con-\nducting several experiments:\n• We prompted the LLM to give a short answer\n(a) to the given question (q) as an expert in\nthe field: \"You are a medical expert.\nAnswer the following question in\na short sentence\".\nThe queries were\ndivided by question type, as shown in Table 2.\n• We evaluated the quality of the generated an-\nswers for each type of query by computing\nthe BERTscore between the generated answer\nand the gold standard, the human annotated\nparaphrases and definitions from RefoMed-EN.\n• We calculated the Pearson correlation be-\ntween the BERTscore and the frequency of\nthe medical concept in the pretraining corpora.\n"}, {"page": 5, "text": "Pragmatic\nfunc-\ntion\nDefinition\nQuery type\nExample\nDefinition\n(DEF)\na difficult or technical concept is\ndefined to ease comprehension\nGive a definition for /\nWhat is multiple sclero-\nsis\nMultiple sclerosis (MS) is a disease\nof the nervous system of immune ori-\ngin\nExemplification\n(EX)\nthe meaning of a concept is illus-\ntrated through examples of types\nand subtypes of entities\nGive examples of heart\nand vascular diseases\nHeart and vascular diseases include\nheart attacks, angina, stroke, sud-\nden cardiovascular death and the\nneed for heart surgery\nDenomination\n(DEN)\na concept is reformulated through\na semantically similar concept,\nwithout simplification\nGive another denomi-\nnation for motor neuron\ndisease\nPharmacotherapy for pain manage-\nment in amyotrophic lateral sclerosis\n(motor neuron disease)\nParaphrase\n(PARA)\nthe\nconcept\nis\nreformulated\nthrough a easy to understand\nsemantically similar synonym\nGive a paraphrase for\nhypotension\nHypotension, i.e.\nlow blood pres-\nsure, frequently occurs in newborns\nExplanation\n(EXP)\nthe concept is explained through\nits process or a part of it\nGive an explanation for\nautoimmune diseases\nThese are autoimmune diseases\nthat can be explained by the fact that\nthe organism produces an antibody\nagainst the person’s skin\nTable 1: Query types, definitions and examples according to pragmatic functions annotated on the\nRefoMed-EN corpus. These queries were used for prompting in our QA task.\nDEF\nEX\nDEN\nPARA\nEXP\nClassYN EX\n207\n305\n131\n30\n32\nCLEAR EX\n919\n379\n401\n73\n67\nRefM-EN-EX\n1126\n684\n532\n103\n99\nClassYN GP\n862\n343\n235\n285\n149\nCLEAR GP\n883\n470\n175\n124\n100\nRefM-EN-GP\n1745\n813\n410\n409\n249\nRefM-EN\n2871\n1497\n942\n512\n348\nTable 2: Distribution of question types across sub-\ncorpora in RefoMed-EN (RefM-EN). EX denotes\nexpert-oriented texts; GP targets the general public.\nQuestion types are ordered by frequency (left to\nright).\nDEF\nEX\nDEN\nPARA\nEXP\n0.2718\n0.0541\n0.1529\n0.1144\n0.1259\nTable 3: Pearson Correlation coefficient (ρ<0.05)\nbetween BERTscore(a, ˆa) and log(Cdf(q)) for\nOLMo-1b.\n4.2.\nCLS Embbedings and\nCo-occurrence Probability\nWe investigated the link between words embed-\ndings and frequency in the pre-training corpus by\nconducting the following experiments:\n• Firstly, we computed the cosine similarity\nscore between the embeddings of the sim-\nple or multi-word medical term to be explained\nor defined, and the embeddings of the answer\ngenerated by the language models. We used\nsentence transformer paraphrase-MiniLM-\nL6-v2. In order to do an exhaustive compari-\nson, we dissolve the reference answer (a) and\ngenerated answer (ˆa) into all possible n-grams.\nThen, we compute the pairwise cosine simi-\nlarity between 2, 3, 4 and 5 possible n-gram\npairs.\n• Secondly, we calculated the probability score\nbetween two document-frequency counts: 1)\nthe frequency of the query term (q) in the cor-\npora (C), and 2) the frequency of the term\ntogether with the generated answer of the lan-\nguage model in the corpus.\nWe used the\nWIMBD tool to found the frequencies in terms\nof number of documents from DOLMA, and\ninfini-gram library for The Pile.\nPcooccurence = Cdf(q|ˆa, C)\nCdf(q|C)\n(3)\nWe calculated different n-grams combinations,\nof 2 to 5 n-gram length. We kept the top-3 seman-\ntically meaningful values. We show the statistical\ncorrelation between these variables using graphical\nrepresentation.\n4.3.\nTracing Back Head and Tail\nKnowledge\nWe listed the most popular concepts (head) as\nthose appearing most frequently in the parametric\nmemory, and the least popular knowledge concepts\n(tail) as having the smallest number of corre-\nsponding documents in the parametric memory.\nWe traced back 100 concepts (1.62%), where\n50 were head concepts (0.8%) and 50 percent tail\n"}, {"page": 6, "text": "Term\nDocfreq\nExamples\ndisease\n75724477\nA disease is a medical condition that affects the body’s structure or function\ncancer\n50918098\nA disease that affects the cells\nanxiety\n35107524\nAnxiety is a feeling of fear and uneasiness\ntestosterone-inhibiting\n115\nThe answer is testosterone-inhibiting\nbiological tissue damage\n503\nGive examples of biological tissue damage\nlifelong neurological consequences\n29\nExplain the neurological consequences of the disease\nTable 4: Examples of LLM paraphrasing. Frequent terms are paraphrased, while rare terms yield outputs\nsimilar to or derived from pre-training data.\nconcepts, to the pretraining corpus. We discarded\nthe very long tail concepts from RefoMed-EN that\nhad zero frequency. However, it is important to note\nthat WIMBD’s search is exact match based (while\nignoring special characters and punctuation), and\nthat some zero frequency terms are very long and\ntechnical, such as \"disorder of bronchial ventilation\"\nor \"SMN1 gene-related proximal spinal muscular\natrophy\". To compare expert (EX) to general public\n(GP) datasets, we split this number evenly between\nRefoMed-EN-EX and RefoMed-EN-GP. We ana-\nlyzed word level frequencies in 100 documents for\neach term downloaded with the WIMBD tool.\nAs the linguistic diversity and the quality of the\npretraining data is extremely important in the task\nperformance evaluation, we conducted a close\nup analysis of corresponding documents in the\nDOLMA corpus. We analyzed the texts for a se-\nlected number of head and tail concepts.\n5.\nResults and Analysis\nStatistical correlations between medical con-\ncepts frequency in the parametric memory.\nTo\nanswer (RQ1), we computed the correlation co-\nefficient between BERTscore and query term fre-\nquency Cdf. We show the Pearson correlations\nscores coefficient between the BERTscore (a, ˆa)\nand log(Cdf (q)) on the full dataset of 6170 terms\nand gold paraphrases, in Table 3. The best scores\nwere obtained with OLMo-1b on the DEF query\ntype (0.27), showing there is a slightly moderate\ncorrelation between the semantic similarity of the\ngenerated answer compared to the gold standard\nanswer, and the frequency of the term in DOLMA.\nSecond best scores are on DEN (0.15), while PARA\nand EXP have lower values (0.11 and 0.12). The\nlowest correlations score were obtained on the EX\nquery type (0.05). We obtained similar results for\nthe bigger model, OLMo-7b, for DEF (0.23), DEN\n(0.15), and EXP (0.14). OLMo-7b has better scores\nfor EX (0.16) and PARA (0.14), which shows that\nthe model handles the complex linguistic task bet-\nter. We conducted a manual analysis of the quality\nof the generated answers for each query type, de-\nscribed in the next section that tackles (RQ2).\nWhen computed separately by type of corpus, ex-\npert medical texts (RefoMed-EN-EX) and general\npublic medical texts (RefoMed-EN-GP), the results\nare consistent: the scores are the highest for DEF\nand DEN. However, the scores on RefoMed-EN-\nEX are better than those on RefoMed-EN-GP for\nDEF, and even higher than the values on the full\ndataset (0.31 compared to 0.24 and 0.27). This\nresult indicates that in the expert texts, the task per-\nformance of the model on DEF is more correlated\nto the term frequency than in the general public (as\nRefoMed-EN-EX has a higher number of technical\nterms). On the contrary, for DEN, scores are higher\non RefoMed-EN-GP (0.19) than RefoMed-EN-EX\n(0.11), also surpassing the full dataset score (0.15).\nWe analyzed these results manually in the next\nsection.\nQA task performance evaluation on linguisti-\ncally diverse query types.\nTo investigate (RQ2),\nwe conducted a qualitative analysis of the LLM’s\nlinguistic understanding. Our hypothesis was that\nlanguage models show very high performance for\nthe definition type query, as this type of query\nis very frequent in knowledge intensive QA bench-\nmarks (Rebboud et al., 2024; Zhao et al., 2024b;\nFei et al., 2023). The results are consistent with our\nhypothesis (Table 3). On the flip side, we observed\nthat the LLM does not completely understand the\nconcept of paraphrase or explanation, as it\ngenerated definitions as answers instead.\nWe noticed that OLMo-1b obtained lower task\nperformance with the denomination and exemplifi-\ncation types of queries, as it tends to either repeat\nthe term (for denomination in particular) or give a\ndefinition style answer for both. The difficulties for\nthese two queries come from linguistic and domain\nspecific linguistic traits, such as:\nDenomination query - the LLM repeats the\nterm instead of giving another synonym for the med-\nical concept. Furthermore, the presence of highly\ntechnical long-tail terms (such as lymphocytic bac-\nterial meningitis; freq=2) or opaque abbreviations\n(e.g. FixM/F; freq=11) renders the task even more\ndifficult for the language model.\nExemplification query - the LLM does not\noutput examples (i.e instances, types, subtypes)\nwhen the query term is very technical, thus long-tail\n"}, {"page": 7, "text": "Criteria\nModel\nEX\nGP\nRefomed\nDiversity\nOLMo-1b\n-0.3904\n-0.5710\n-0.4403\nPythia-1b\n-0.0206\n-0.1202\n-0.0317\nScalability\nOLMo-7b\n-0.3656\n-0.3308\n-0.3501\nTable 5: Pearson correlation between CLS cosine\nsimilarity (query term, response-top3-ngrams) and\nthe cooccurence probability of frequency of the term\ntogether with its top3 n-grams. (Red color denotes\nρ>0.05)\nknowledge: (Give examples of severe motor dis-\norders →The answer is severe motor disorders).\nHowever, the model task performance increases for\nhead knowledge (1. Give examples of cardiovascu-\nlar disease →a heart attack; 2. Give examples of\npsychological factors →The answer is: Psycholog-\nical factors include: fear, anxiety, and depression).\nTracing back head and tail knowledge in the\npretraining corpus.\nWe looked into a total of\n5074 documents from the DOLMA corpus, where\n5000 contained the head words and 74 the tail for\nthe tail concepts. For each head term out of the 50,\nwe downloaded 100 documents where this term\nappeared at least once. For tail terms, we looked\nfor concepts that appeared in 1 to 3 documents in\nthe DOLMA pretraining corpus.\nThe long tail terms with very low frequency (>4)\nare very technical long multi-word terms, such\nas \"pseudo-rhizomelic polyarthritis\", \"lymphocytic\nbacterial meningitis\", \"CoA HMG reductase in-\nhibitors\". However, there are also very long multi-\nword terms in RefoMed-EN-GP, like \"work-related\nmusculoskeletal disorders of the upper limbs and\nneck\" or \"shiftworker sleep disorder\" with a term\nfrequency of 2 and 1 in the whole DOLMA corpus.\nRegarding the head terms, the most frequent\nterm from RefoMed-EN is \"life\", which appears in\n497M documents in the pretraining corpus, while\nthe second most frequent is \"control\" with 230M\ndocuments.\nOther head terms are \"conditions\"\n(130M), \"function\" (114M), \"skills\" (126M) and typ-\nical medical head concepts such as \"treatment\"\n(113M), \"pain\" (80M), \"disease\" (75M) and \"can-\ncer\" (50M). While analyzing the term frequency, we\nnoticed that terms such as \"control\", \"function\" and\n\"client\" appear in code texts, thus irrelevant for your\nmedical knowledge QA task.\nCLS n-gram embeddings on head and tail terms.\nWe explored (RQ3) in line with Wang et al.’s (2024)\nhypothesis: LLMs generalize more in reasoning-\nintensive tasks, and they memorize more in sim-\npler knowledge-intensive task (like in the case of\ntail knowledge). We conducted the CLS n-gram\nanalysis on the list of 100 head and tail concepts\nto verify this hypothesis. We calculated the CLS\nembedding BERTscore between the medical term\nand n-grams of the generated answer of different\nsizes (2, 3, 4 and 5 n-grams). We kept the top 3\nbest BERTscore for each term and we compared\nthem with the frequencies of the term together with\ntheses n-grams in the pretraining corpus. We show\nthe distribution of the results in Table 5.\nThe negative distribution scores (-0.44 on\nRefoMed-EN for OLMo-1b) indicate that the lan-\nguage model tends to create its own sentences\nand does not take full information package directly\nfrom DOLMA. In terms of scalability, OLMo-7b is\ninline with OLMo-1b (-0.35 on RefoMed-EN), while\nanother family of models, Pythia-1b, show very low\ndistribution scores. This suggests that the LLM\nprioritizes semantic paraphrasing (backed by the\nsemantic evidence in relation to the CLS similarity)\n(see examples in Table 4), and it is not reproducing\nthe same content from the pretraining corpus, as\nproven by the syntactic evidence related to the n-\ngram occurrence. OLMo-1b’s scores are better for\nhead terms and for the general public texts (-0.71),\nshowing that the model paraphrases more on pop-\nular and frequent knowledge and less on tail and\ntechnical knowledge, especially in the expert texts\n(+0.07), while OLMo-7b has similar values for both\ntypes of texts. This indicates that the LLM memo-\nrizes more on knowledge intensive questions, as\nshown in previous studies (Wang et al., 2024).\n6.\nDiscussion\nScaling up small language models does not nec-\nessarily improve performance.\nWe replicated\nour experiments on a bigger model, OLMo-7b. We\nnotice a similar trend for definition-type questions:\nboth OLMo-1b and OLMo-7b exhibit positive Pear-\nson correlation coefficient (p) between BERTscore\nand query term frequency of 0.16. Interestingly,\nwe noticed that OLMo-7b shows a weaker trend\ncompared to OLMo-1b model (p=0.27), this implies\nthat with increase in the size of language models\nthe ability to deal with tail knowledge does not nec-\nessarily increase. Regarding overall comparison\nof CLS n-grams scores on head and tails (see Ta-\nble 5), we further notice a similar trend between\nOLMo-1b and OLMo-7b models.\nHuman evaluation shows hallucinations are not\nscale related.\nWe conducted a manual analysis\nof 400 answers given my the four models for the\n100 head and tail terms dataset (Figure 3). We eval-\nuated task performance and diversity of models by\nlooking directly into the data for hallucinations. Our\nanalysis showed that Pythia-1b is more prone to\ngenerating hallucinatory texts (+22%) compared\nto OLMo-1b, as it was previously shown (Groen-\n"}, {"page": 8, "text": "Pythia1b\nOLMo1b\nOLMo7b\nOLMo7b-it\nModel\n0\n10\n20\n30\n40\n50\n60\nNumber of Hallucinations\nCategory\nTop25EX\nTop25GP\nBott25EX\nBott25GP\nFigure 3: Manual analysis of hallucinations on 100\nhead and tail terms. Best model is OLMo-1b, with\nthe lowest rate of hallucinations (33%).\neveld et al., 2024). As for bigger models, OLMo-7b\nis hallucinating more (+19%) than its Instruct ver-\nsion. The best model remains OLMo-1b (33% hal-\nlucinated answers), followed by OLMo-7b-Instruct\n(39%). Pythia-1b demonstrates an opposite trend\nfor definition-type questions (DEF) as compared to\nOLMo-1b, with a comparatively low Pearson cor-\nrelation coefficient of -0.103. However, OLMo-7b\nshows a similar positive trend for paraphrase-type\nquestions (PARA) with a coefficient of 0.14.\nMore linguistic diversity metrics are needed.\nBERTscore might not be the best fit for all\ntypes of queries we analyzed. For example, for\ndenomination-types queries where the LLM only re-\npeats the medical term in the query, the BERTscore\nwill be very high, but not relevant. In a concurrent\nwork to our, Guo et al. (2024) compared lexical,\nsyntactic and semantic distribution of LLMs texts\nto human gold answers. Lee and Lee (2023) pro-\nposed a pipeline to identify 220 popular handcrafted\nlinguistic features. However, the NLP community\nneeds to continue working on the best linguistic\nfeatures that count for each writing task.\n7.\nConclusion\nOur study introduced TrackList, a pipeline to ana-\nlyze, trace back and evaluate a language model’s\nanswer to diverse linguistic queries. We showed\nthat frequency of terms in the pretraining corpus\nimpacts performance: LLMs tend to give inaccu-\nrate answers for head terms and more accurate an-\nswers for tail terms. LLMs are prone to paraphrase\nmore on head, and thus popular, knowledge, and\nless on tail and technical knowledge, especially\nin the expert texts. Paraphrasing too much some-\ntimes leads to inaccurate answers for head terms\n(like life, condition, disease). Our study showed\nthat Pythia-1b hallucinates more (+22%) than the\nmodels from the OLMo family. Our linguistic anal-\nysis showed that language models tend to give\ndefinition-type answers to different queries, even\nwhen asked to give examples or paraphrases. This\ndemonstrates the limited linguistic knowledge of a\nsmall language model (1b-7b), still far from human\nknowledge.\nEthical Considerations\nThe dataset used for this experiments has open\nlicense (CC BY-NC 4.0) and can be used for re-\nsearch by the NLP community. The RefoMed-EN\ndataset contains no personal data or patient data.\nLimitations\nThis work was conducted only on English using only\nfully open language models, OLMo (Groeneveld\net al., 2024) and Pythia (Biderman et al., 2023).\nDue to computational power limitations, we con-\nducted our analysis on small language models (1b\nand 7b). Future studies could include the recently\nreleased OLMo2 family of models (OLMo et al.,\n2024) and testing new data tracing tools such as OL-\nMoTrace (Liu et al., 2025). Other open source LLMs\ngive access to their pretraining data, like BLOOM\n(Workshop et al., 2022) available, but the big size\nof the dataset makes it difficult to explore.\nOur study focused on small size language mod-\nels to investigate the inner working of LLMs without\ndomain knowledge finetuning, DPO finetuning or\nRAG systems. Our purpose was to analyze how the\npretraining corpus and the word frequency (head\nand tail) impacts the accuracy of the LLMs answer\nto different linguistic types of questions. This is\nmotivated by the fact that humans use language\nmodels as plug-and-play tools, without any finetun-\ning methods. We are aware that using the methods\nlisted above will improve vanilla LLM’s performance\nfor the QA task. Further research can include ex-\nploring these methods. Moreover, we are aware\nof this limitation for your semantic similarity evalu-\nation, and we further investigate medical metrics\nsuch as MEDCON (Yim et al., 2023), or fact check-\ning metrics and tools such as FACTSCORE (Min\net al., 2023), FIRE (Xie et al., 2024), or LOKI (Li\net al., 2025). LLM-as-a-Judge evaluation method\ncan also be explored and compared with existing\ntools and metrics.\n8.\nBibliographical References\nAsma Ben Abacha, Chaitanya Shivade, and Dina\nDemner-Fushman. 2019.\nOverview of the\nmediqa 2019 shared task on textual inference,\n"}, {"page": 9, "text": "question entailment and question answering. In\nProceedings of the 18th BioNLP Workshop and\nShared Task, pages 370–379.\nShayan Ali Akbar, Md Mosharaf Hossain, Tess\nWood, Si-Chi Chin, Erica Salinas, Victor Alvarez,\nand Erwin Cornejo. 2024. Hallumeasure: Fine-\ngrained hallucination measurement using chain-\nof-thought reasoning. In Proceedings of the 2024\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 15020–15037.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil,\nand Hannaneh Hajishirzi. 2023. Self-rag: Learn-\ning to retrieve, generate, and critique through\nself-reflection. arXiv preprint arXiv:2310.11511.\nAsma Ben Abacha and Dina Demner-Fushman.\n2019. A question-entailment approach to ques-\ntion answering. BMC bioinformatics, 20:1–23.\nStella Biderman, Hailey Schoelkopf, Quentin Gre-\ngory Anthony, Herbie Bradley, Kyle O’Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu\nPurohit, USVSN Sai Prashanth, Edward Raff,\net al. 2023. Pythia: A suite for analyzing large\nlanguage models across training and scaling. In\nInternational Conference on Machine Learning,\npages 2397–2430. PMLR.\nIoana Buhnila, Aman Sinha, and Matthieu Con-\nstant. 2024.\nRetrieve, generate, evaluate: A\ncase study for medical paraphrases generation\nwith small language models. In Proceedings of\nthe 1st Workshop on Towards Knowledgeable\nLanguage Models (KnowLLM 2024), pages 189–\n203.\nXavier Daull, Patrice Bellot, Emmanuel Bruno,\nVincent\nMartin,\nand\nElisabeth\nMurisasco.\n2023.\nComplex qa and language models\nhybrid architectures, survey.\narXiv preprint\narXiv:2302.09051.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng,\nJingyuan Ma, Rui Li, Heming Xia, Jingjing Xu,\nZhiyong Wu, Baobao Chang, et al. 2024. A sur-\nvey on in-context learning. In Proceedings of the\n2024 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1107–1128.\nYanai Elazar, Akshita Bhagia, Ian Magnusson, Ab-\nhilasha Ravichander, Dustin Schwenk, Alane\nSuhr, Pete Walsh, Dirk Groeneveld, Luca Sol-\ndaini, Sameer Singh, et al. 2023. What’s in my\nbig data? arXiv preprint arXiv:2310.20707.\nZhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe\nZhou, Zhuo Han, Songyang Zhang, Kai Chen,\nZongwen Shen, and Jidong Ge. 2023. Lawbench:\nBenchmarking legal knowledge of large lan-\nguage models. arXiv preprint arXiv:2309.16289.\nLeo Gao, Stella Biderman, Sid Black, Laurence\nGolding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima,\net al. 2020. The pile: An 800gb dataset of di-\nverse text for language modeling. arXiv preprint\narXiv:2101.00027.\nNatalia Grabar and Rémi Cardon. 2018. Clear-\nsimple corpus for medical french. In ATA.\nNatalia Grabar and Iris Eshkol. 2016. Why do we\nreformulate? automatic prediction of pragmatic\nfunctions. In HrTAL 2016.\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Ak-\nshita Bhagia, Rodney Kinney, Oyvind Tafjord,\nAnanya Harsh Jha, Hamish Ivison, Ian Magnus-\nson, Yizhong Wang, et al. 2024. Olmo: Accel-\nerating the science of language models. arXiv\npreprint arXiv:2402.00838.\nYanzhu Guo, Guokan Shang, and Chloé Clavel.\n2024.\nBenchmarking\nlinguistic\ndiversity\nof large language models.\narXiv preprint\narXiv:2412.10271.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong\nZhong, Zhangyin Feng, Haotian Wang, Qiang-\nlong Chen, Weihua Peng, Xiaocheng Feng, Bing\nQin, et al. 2024. A survey on hallucination in\nlarge language models: Principles, taxonomy,\nchallenges, and open questions. ACM Transac-\ntions on Information Systems.\nSanna Iivanainen, Jarkko Lagus, Henri Viertolahti,\nLauri Sippola, and Jussi Koivunen. 2024. Inves-\ntigating large language model (llm) performance\nusing in-context learning (icl) for interpretation of\nesmo and nccn guidelines for lung cancer.\nMinbyul Jeong, Jiwoong Sohn, Mujeen Sung,\nand Jaewoo Kang. 2024.\nImproving medical\nreasoning through retrieval and self-reflection\nwith retrieval-augmented large language models.\narXiv preprint arXiv:2401.15269.\nEhsan Kamalloo, Nouha Dziri, Charles Clarke, and\nDavood Rafiei. 2023. Evaluating open-domain\nquestion answering in the era of large language\nmodels. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 5591–5606.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2023. Large language\nmodels struggle to learn long-tail knowledge. In\nInternational Conference on Machine Learning,\npages 15696–15707. PMLR.\nBruce W Lee and Jason Lee. 2023. Lftk: Hand-\ncrafted features in computational linguistics. In\nProceedings of the 18th Workshop on Innovative\n"}, {"page": 10, "text": "Use of NLP for Building Educational Applications\n(BEA 2023), pages 1–19.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\nRocktäschel, et al. 2020. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks. Ad-\nvances in Neural Information Processing Sys-\ntems, 33:9459–9474.\nDongyang Li, Junbing Yan, Taolin Zhang, Chengyu\nWang, Xiaofeng He, Longtao Huang, Jun Huang,\net al. 2024a. On the role of long-tail knowledge in\nretrieval augmented large language models. In\nProceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Vol-\nume 2: Short Papers), pages 120–126.\nHaonan Li, Xudong Han, Hao Wang, Yuxia Wang,\nMinghan Wang, Rui Xing, Yilin Geng, Zenan\nZhai, Preslav Nakov, and Timothy Baldwin. 2025.\nLoki: An open-source tool for fact verification. In\nProceedings of the 31st International Conference\non Computational Linguistics: System Demon-\nstrations, pages 28–36.\nHuihan Li, Yuting Ning, Zeyi Liao, Siyuan Wang,\nXiang Li, Ximing Lu, Wenting Zhao, Faeze Brah-\nman, Yejin Choi, and Xiang Ren. 2024b.\nIn\nsearch of the long-tail: Systematic generation\nof long-tail inferential knowledge via logical rule\nguided search. In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing, pages 2348–2370.\nYucheng Li, Yunhao Guo, Frank Guerin, and\nChenghua Lin. 2024c. An open-source data con-\ntamination report for large language models. In\nFindings of the Association for Computational\nLinguistics: EMNLP 2024, pages 528–541.\nAndreas Liesenfeld, Alianda Lopez, and Mark\nDingemanse. 2023. Opening up chatgpt: Track-\ning openness, transparency, and accountability\nin instruction-tuned text generators. In Proceed-\nings of the 5th international conference on con-\nversational user interfaces, pages 1–6.\nJiacheng Liu, Taylor Blanton, Yanai Elazar, Sewon\nMin, YenSung Chen, Arnavi Chheda-Kothary,\nHuy Tran, Byron Bischoff, Eric Marsh, Michael\nSchmitz, et al. 2025. Olmotrace: Tracing lan-\nguage model outputs back to trillions of training\ntokens. arXiv preprint arXiv:2504.07096.\nJiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin\nChoi, and Hannaneh Hajishirzi. 2024. Infini-gram:\nScaling unbounded n-gram language models to\na trillion tokens. arXiv preprint arXiv:2401.17377.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric\nmemories. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 9802–\n9822.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke\nZettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of fac-\ntual precision in long form text generation. arXiv\npreprint arXiv:2305.14251.\nVincent Nguyen, Sarvnaz Karimi, Maciej Rybin-\nski, and Zhenchang Xing. 2023. Medredqa for\nmedical consumer question answering: Dataset,\ntasks, and neural baselines. In Proceedings of\nthe 13th International Joint Conference on Natu-\nral Language Processing and the 3rd Conference\nof the Asia-Pacific Chapter of the Association for\nComputational Linguistics (Volume 1: Long Pa-\npers), pages 629–648.\nTeam OLMo, Pete Walsh, Luca Soldaini, Dirk\nGroeneveld, Kyle Lo, Shane Arora, Akshita Bha-\ngia, Yuling Gu, Shengyi Huang, Matt Jordan,\net al. 2024. 2 olmo 2 furious. arXiv preprint\narXiv:2501.00656.\nAjay Madhavan Ravichandran, Julianna Grune,\nNils Feldhus, Aljoscha Burchardt, Roland Roller,\nand Sebastian Möller. 2024. Xai for better ex-\nploitation of text in medical decision support. In\nProceedings of the 23rd Workshop on Biomedical\nNatural Language Processing, pages 506–513.\nYoussra Rebboud, Pasquale Lisena, Lionel Tail-\nhardat, and Raphael Troncy. 2024. Benchmark-\ning llm-based ontology conceptualization: A pro-\nposal. In ISWC 2024, 23rd International Seman-\ntic Web Conference.\nOscar Sainz, Jon Campos, Iker García-Ferrero,\nJulen Etxaniz, Oier Lopez de Lacalle, and Eneko\nAgirre. 2023. Nlp evaluation in trouble: On the\nneed to measure llm data contamination for each\nbenchmark. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages\n10776–10787.\nPasi Shailendra, Rudra Chandra Ghosh, Rajdeep\nKumar, and Nitin Sharma. 2024. Survey of large\nlanguage models for answering questions across\nvarious fields. In 2024 10th International Con-\nference on Advanced Computing and Commu-\nnication Systems (ICACCS), volume 1, pages\n520–527. IEEE.\n"}, {"page": 11, "text": "Luca Soldaini, Rodney Kinney, Akshita Bhagia,\nDustin Schwenk, David Atkinson, Russell Au-\nthur, Ben Bogin, Khyathi Chandu, Jennifer Du-\nmas, Yanai Elazar, et al. 2024.\nDolma: An\nopen corpus of three trillion tokens for lan-\nguage model pretraining research. arXiv preprint\narXiv:2402.00159.\nKai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu,\nand Xin Luna Dong. 2023. Head-to-tail: How\nknowledgeable are large language models (llm)?\naka will llms replace knowledge graphs? arXiv\npreprint arXiv:2308.10168.\nAmalia Todirascu, Sebastian Padó, Jennifer Krisch,\nMax Kisselew, and Ulrich Heid. 2012. French\nand german corpora for audience-based text type\nclassification. In LREC, volume 2012, pages\n1591–1597.\nXinyi Wang, Antonis Antoniades, Yanai Elazar, Al-\nfonso Amayuelas, Alon Albalak, Kexun Zhang,\nand William Yang Wang. 2024. Generalization\nvs memorization: Tracing language models’ ca-\npabilities back to pretraining data. arXiv preprint\narXiv:2407.14985.\nBigScience Workshop, Teven Le Scao, Angela\nFan, Christopher Akiki, Ellie Pavlick, Suzana\nIlić, Daniel Hesslow, Roman Castagné, Alexan-\ndra Sasha Luccioni, François Yvon, et al.\n2022. Bloom: A 176b-parameter open-access\nmultilingual language model.\narXiv preprint\narXiv:2211.05100.\nSkyler Wu, Eric Meng Shen, Charumathi Badri-\nnath, Jiaqi Ma, and Himabindu Lakkaraju. 2023.\nAnalyzing chain-of-thought prompting in large\nlanguage models via gradient-based feature at-\ntributions. arXiv preprint arXiv:2307.13339.\nZhuohan Xie, Rui Xing, Yuxia Wang, Jiahui Geng,\nHasan Iqbal, Dhruv Sahnan, Iryna Gurevych,\nand Preslav Nakov. 2024. Fire: Fact-checking\nwith iterative retrieval and verification.\narXiv\npreprint arXiv:2411.00784.\nWen-wai Yim, Yujuan Fu, Asma Ben Abacha, Neal\nSnider, Thomas Lin, and Meliha Yetisgen. 2023.\nAci-bench: a novel ambient clinical intelligence\ndataset for benchmarking automatic visit note\ngeneration. Scientific Data, 10(1):586.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore:\nEvaluating text generation with bert.\narXiv\npreprint arXiv:1904.09675.\nHaiyan Zhao, Hanjie Chen, Fan Yang, Ninghao\nLiu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang,\nDawei Yin, and Mengnan Du. 2024a. Explain-\nability for large language models: A survey. ACM\nTransactions on Intelligent Systems and Technol-\nogy, 15(2):1–38.\nYilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang,\nChen Zhao, and Arman Cohan. 2024b. Knowl-\nedgefmath: Knowledge-intensive math reason-\ning in finance domains. In 62nd Annual Meeting\nof the Association for Computational Linguistics,\nACL 2024, pages 12841–12858. Association for\nComputational Linguistics (ACL).\n"}]}