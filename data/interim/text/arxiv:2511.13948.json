{"doc_id": "arxiv:2511.13948", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.13948.pdf", "meta": {"doc_id": "arxiv:2511.13948", "source": "arxiv", "arxiv_id": "2511.13948", "title": "EchoAgent: Guideline-Centric Reasoning Agent for Echocardiography Measurement and Interpretation", "authors": ["Matin Daghyani", "Lyuyang Wang", "Nima Hashemi", "Bassant Medhat", "Baraa Abdelsamad", "Eros Rojas Velez", "XiaoXiao Li", "Michael Y. C. Tsang", "Christina Luong", "Teresa S. M. Tsang", "Purang Abolmaesumi"], "published": "2025-11-17T22:06:12Z", "updated": "2025-11-17T22:06:12Z", "summary": "Purpose: Echocardiographic interpretation requires video-level reasoning and guideline-based measurement analysis, which current deep learning models for cardiac ultrasound do not support. We present EchoAgent, a framework that enables structured, interpretable automation for this domain. Methods: EchoAgent orchestrates specialized vision tools under Large Language Model (LLM) control to perform temporal localization, spatial measurement, and clinical interpretation. A key contribution is a measurement-feasibility prediction model that determines whether anatomical structures are reliably measurable in each frame, enabling autonomous tool selection. We curated a benchmark of diverse, clinically validated video-query pairs for evaluation. Results: EchoAgent achieves accurate, interpretable results despite added complexity of spatiotemporal video analysis. Outputs are grounded in visual evidence and clinical guidelines, supporting transparency and traceability. Conclusion: This work demonstrates the feasibility of agentic, guideline-aligned reasoning for echocardiographic video analysis, enabled by task-specific tools and full video-level automation. EchoAgent sets a new direction for trustworthy AI in cardiac ultrasound.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.13948v1", "url_pdf": "https://arxiv.org/pdf/2511.13948.pdf", "meta_path": "data/raw/arxiv/meta/2511.13948.json", "sha256": "74ef922f9eb1b06584114b074739cd968374c54c02854fbeeb0c6f9efa0b51ba", "status": "ok", "fetched_at": "2026-02-18T02:26:44.338446+00:00"}, "pages": [{"page": 1, "text": "EchoAgent: Guideline-Centric Reasoning Agent for\nEchocardiography Measurement and Interpretation\nMatin Daghyani1*†, Lyuyang Wang1*†, Nima Hashemi1,\nBassant Medhat1, Baraa Abdelsamad1, Eros Rojas Velez1,\nXiaoXiao Li1, Michael Y. C. Tsang2, Christina Luong1,2,\nTeresa S.M. Tsang1,2, Purang Abolmaesumi1\n1*University of British Columbia, Vancouver, British Columbia, Canada.\n2Vancouver General Hospital, Vancouver, British Columbia, Canada∗.\n*Corresponding author(s). E-mail(s): matin.daghyani@ece.ubc.ca;\nanne@ece.ubc.ca;\n†These authors contributed equally to this work.\nAbstract\nPurpose: Echocardiographic interpretation requires video-level reasoning and\nguideline-based measurement analysis, which current deep learning models for\ncardiac ultrasound do not support. We present EchoAgent, a framework that\nenables structured, interpretable automation for this domain. Methods: EchoA-\ngent orchestrates specialized vision tools under Large Language Model (LLM)\ncontrol to perform temporal localization, spatial measurement, and clinical inter-\npretation. A key contribution is a measurement-feasibility prediction model that\ndetermines whether anatomical structures are reliably measurable in each frame,\nenabling autonomous tool selection. We curated a benchmark of diverse, clini-\ncally validated video–query pairs for evaluation. Results: EchoAgent achieves\naccurate, interpretable results despite added complexity of spatiotemporal video\nanalysis. Outputs are grounded in visual evidence and clinical guidelines, sup-\nporting transparency and traceability. Conclusion: This work demonstrates\nthe feasibility of agentic, guideline-aligned reasoning for echocardiographic video\nanalysis, enabled by task-specific tools and full video-level automation. EchoA-\ngent sets a new direction for trustworthy AI in cardiac ultrasound. Our code will\nbe made publicly available at https://github.com/DeepRCL/EchoAgent.\nKeywords: Echocardiography, Agentic AI, Reasoning, Large Language Models\n∗T. S.M. Tsang and P. Abolmaesumi are joint senior authors.\n1\narXiv:2511.13948v1  [cs.CV]  17 Nov 2025\n"}, {"page": 2, "text": "1 Introduction\nEchocardiography is a central imaging modality in cardiovascular care, providing\nnon-invasive, real-time assessment of cardiac structure and function [1, 2]. Point-\nof-care-ultrasound (POCUS) extends this capability to the bedside, where focused\nexaminations by non-specialists address targeted clinical questions. In routine prac-\ntice, these focused scans capture only a fraction of the data acquired in comprehensive\nstudies (10–20 vs. > 100 cine clips), shifting the burden to clinicians to decide which\nmeasurements are feasible and diagnostically sufficient under time pressure. The result\nis high variability, knowledge-intensive decision making, and frequent uncertainty\nabout when to stop scanning, what to measure next, and how to integrate partial\nevidence into a safe conclusion. These are precisely the situations where structured\nreasoning, not only perception, becomes critical for usability.\nArtificial intelligence (AI) has shown promise in echocardiographic analysis, par-\nticularly through general-purpose foundation models such as EchoPrime [3] and\nEchoApex [4]. These models leverage large-scale vision–language or self-supervised\ntraining on millions of echocardiographic videos or images, enabling capabilities such\nas view classification, segmentation, and disease detection. Within POCUS, however,\nreal-world interpretation requires more than static predictions: the system must judge\nview adequacy, infer which quantitative measurements are feasible, and justify rec-\nommendations in language aligned with clinical standards. A unified framework that\nconnects quantitative analysis with guideline-based interpretation, and that exposes\nintermediate reasoning and visual evidence, could therefore improve both accuracy\nand trust in POCUS workflows.\nAgentic systems allow language language models to orchestrate specialized tools.\nMedRAX [5] demonstrated tool-based reasoning for chest radiographs, but its assump-\ntions do not hold for echocardiography. Unlike static images, echo requires integrated\nspatialtemporal reasoning and continuous quality control. An agent for echo must\nalign each step with clinical protocols while checking its reasoning against echocar-\ndiography guidelines and issuing safe-to-answer decisions. Such an agent closes the\nloop between perception and reasoning, reducing hallucinations and making bedside\nrecommendations auditable.\nTo address these limitations, we introduce EchoAgent, a guideline-centric agen-\ntic framework for unified measurement analysis and interpretation in echo with an\nemphasis on POCUS usability. EchoAgent operates directly on video input and inte-\ngrates domain specific tools under Large Language Model (LLM) orchestration. Its\nmain contributions are:\n1. Measurement-feasibility forecasting and planning to determine whether\nrequired anatomical structures are present and measurable, reducing cognitive load\nin focused POCUS exams.\n2. Interpretable, guideline-aligned reasoning, with each step grounded in visual\nevidence and verified against echocardiography standards; the agent exposes its\nintermediate rationale, highlights supporting echo cine-frame evidence, and issues\n“safe-to-answer” or escalation recommendations that clinicians can audit.\n2\n"}, {"page": 3, "text": "3. End-to-end video level analysis that accounts for temporal dynamics and vari-\nable image quality, essential for real-world POCUS where operator skill and patient\nfactors vary.\nBy elevating reasoning to deciding what is feasible, and why the answer is trustwor-\nthy, EchoAgent is designed to make POCUS more usable for non-specialists while\npreserving the rigor of guideline based echocardiography.\n2 Related Work\n2.1 Foundation Models for Echocardiography\nRecent work has explored large-scale echocardiography models trained using self-\nsupervised or contrastive objectives. Vision-only foundation models focus on represen-\ntation learning for downstream tasks but cannot reason over textual input or provide\ninterpretable outputs [4]. Vision–language models (VLMs) trained on video–report\npairs have demonstrated impressive zero-shot generalization for classification [3], but\ntheir predictions lack step-by-step justification or alignment with clinical guidelines.\nAs a result, outputs cannot be verified or audited, and such systems do not support\ninteractive visual question answering or structured measurement interpretation. VLM-\nbased approaches [6, 7] further require task-specific fine-tuning on image–text pairs,\nwhich are scarce in echocardiography, and limit evaluation to static frames rather\nthan full video sequences. Some recent work has introduced automated measurement\nprediction in echo videos [8], but these models do not perform multi-step reasoning\nor integrate interpretive standards. In contrast, our framework offers both numerical\nand interpretive outputs, grounded in visual evidence and expert-derived guidelines.\n2.2 Agentic AI in Medicine\nAgentic frameworks enable large language models to plan and invoke external tools\nfor clinical reasoning [5, 9–14]. Recent adaptations incorporate multi-agent and\nplanner–executor designs to improve retrieval-augmented reasoning and reduce hal-\nlucinations in medical tasks [15, 16]. However, these systems target static images or\nstructured data and do not address the spatiotemporal complexity of echocardiogra-\nphy. EchoAgent extends this paradigm by supporting full video-level automation with\ninterpretable, guideline-grounded outputs.\n3 EchoAgent\n3.1 Overview\nEchoAgent is a modular framework designed for end-to-end echocardiographic mea-\nsurement and interpretation through iterative reasoning. It comprises three primary\ncomponents: a set of domain-specific tools T for video analysis, measurement extrac-\ntion, and guideline lookup; an indexed guideline database D; and a large language\nmodel M that orchestrates the overall process. Given a video V and a user query\nQ, the agent incrementally determines its next step using prior observations. At each\n3\n"}, {"page": 4, "text": "step, it selects a tool from T , formulates its input parameters, and updates its internal\nhistory with the resulting output. This iterative process continues until the agent con-\ncludes that it has sufficient information to produce a guideline-consistent response. The\nfollowing subsections detail the agentic decision process and describe the specialized\ntools integrated into the framework.\n3.2 Agentic Design\nGiven an echocardiogram V and user query Q, the agent’s goal is to iteratively\ngather the information needed to answer the query in a guideline-consistent manner.\nIt integrates two information sources: (i) the video V , from which clinically valid mea-\nsurements, key frames, and feasibility assessments are extracted; and (ii) the guideline\ndatabase D, which provides structured clinical knowledge for interpreting results.\nInstead of a fixed pipeline, the language model orchestrator M operates adaptively\nin an iterative loop [9] of three phases: observation (incorporating new measurements\nor retrieved data), thought (identifying remaining uncertainties), and action (selecting\na tool from T and producing input parameters). Each action’s output is fed back into\nthe loop to guide subsequent reasoning.\nThe language model does not process V directly but interfaces with modular tools,\nsuch as a feasibility prediction model, measurement segmentation models, and a car-\ndiac phase estimator that return structured outputs. In parallel, it may employ a\nclinical context retrieval tool to access relevant information from D, such as threshold\nvalues, diagnostic criteria, or interpretive rules grounded in clinical guidelines.\nThe process continues until the agent determines it has sufficient information to\nanswer the query, invoking a FINISH action. The final output includes both the answer\nand a structured trace of reasoning steps, ensuring transparency and verifiability. The\ncomplete reasoning loop is formalized in Algorithm 1.\n3.3 Tools Integration\nTo support the LLM’s reasoning, EchoAgent integrates specialized tools that extract\nclinically meaningful information from echocardiographic videos and standard guide-\nlines. The vision tools include a cardiac phase detector for identifying end-diastolic\n(ED) and end-systolic (ES) frames, a measurement-feasibility model for assessing\nanatomical suitability, and segmentation models for standard B-mode linear mea-\nsurements. In addition, a guideline-retrieval tool enables the agent to query standard\nreferences to interpret user queries and evaluate the clinical relevance of predicted\nvalues.\nCardiac Phase Detection Tool: We trained this module in a self-supervised\nmanner [17] to learn frame-level representations that capture temporal and morpho-\nlogical variations across the cardiac cycle. Since echocardiographic measurements are\ninherently frame-specific, identifying the end-diastolic (ED) and end-systolic (ES)\nframes is crucial. These key frames define physiologically meaningful reference points\ncorresponding to maximal and minimal ventricular volumes, respectively. Accurate\ndetection of ED and ES frames enables consistent measurement of parameters such\n4\n"}, {"page": 5, "text": "LLM Orchestrator\nGuideline\nDatabase \nVideo\nObservation\nand Reasoning\nPhase Detection\nFeasibility\nPrediction\nMeasurements\nSegmentation\nGuideline\nRetrieval\nUser Query\nTools\nFinal Answer\nEchoAgent\nTool\nCalling\nFig. 1: Overview of the EchoAgent framework. EchoAgent is a guideline-centric\nreasoning system that integrates specialized visual tools under the orchestration of\nan LLM. Given a clinician’s natural-language query and an echo video, the LLM\norchestrator dynamically invokes tools for phase detection, feasibility prediction,\nmeasurement segmentation, and guideline retrieval to produce interpretable, guideline-\naligned answers. The system iteratively reasons over intermediate observations to\nensure that all measurements are visually grounded and clinically validated.\nas chamber dimensions, wall thickness, and strain, which are essential for reliable car-\ndiac function assessment. After completing the self-supervised training, we froze the\nencoder and appended a lightweight classifier to identify the ED and ES frames. Given\na video sequence x ∈RT ×H×W with T frames, the classifier takes the embeddings of\nall frames as input and outputs the indices corresponding to the ED and ES frames.\nFeasibility Prediction Tool: Although standard echocardiographic views are\ndefined for specific quantitative measurements [18], not all videos labeled as a given\nview are suitable for every measurement associated with that view. View classification\nalone does not guarantee measurement validity, as variability in zoom level, image qual-\nity, and anatomical visibility often makes certain measurements unreliable even when\nthe view label is correct. Moreover, in real-world usage, user queries may not always\nspecify what to measure, placing the burden on the agent to infer what is visually\nfeasible and clinically relevant. To address this, we introduce a feasibility prediction\nmodel that identifies which measurements are supported by the image content.\nGiven a frame from an echocardiogram x ∈RH×W , the model predicts a binary\nvector ˆy ∈{0, 1}m, where m is the number of linear measurements and ˆyj = 1 indi-\ncates measurement j is feasible. The model is trained on key frames (end-diastole or\nend-systole) annotated with which measurements were performed originally by sonog-\nraphers. We construct a binary label vector y per frame and train a ResNet-50 with a\nsigmoid output head using binary cross-entropy loss, framing the task as multi-label\nclassification. This tool allows the agent to infer what is measurable—even without\nan explicit user request—and avoid unreliable predictions, improving both robustness\nand autonomy in measurement selection.\nLinear Measurement Tool: We employ EchoNet-Measurements [8] as the core\ntool for computing echocardiographic linear measurements. After phase detection and\n5\n"}, {"page": 6, "text": "Algorithm 1 EchoAgent: Iterative Agentic Reasoning Loop\nRequire: Query Q, video V , tool set T , guideline database D, language model M\nEnsure: Final response R\n1: Initialize history H ←∅\n2: Set step counter i ←0, maximum steps K\n3: while i < K do\n4:\nOi ←M.observe(Q, H)\n5:\n(Ti, Pi) ←M.reason(Oi, T )\n6:\nif Ti = FINISH then\n7:\nR ←M.generateAnswer(H)\n8:\nreturn R\n9:\nelse if Ti = search guideline then\n10:\nYi ←executeTool(Ti, Pi, D)\n11:\nelse\n12:\nYi ←executeTool(Ti, Pi, V )\n13:\nend if\n14:\nH ←H ∪{(Ti, Pi, Yi)}\n15:\ni ←i + 1\n16: end while\n17: return M.generateAnswer(H)\nfeasibility assessment, the candidate frame and corresponding measurement name are\nprovided to the tool, which applies dedicated pre-trained weights for each measurement\ntype. The tool outputs both the measurement localization and quantitative value,\nwhich are retrieved by the agent for subsequent reasoning.\nClinical Context Retrieval Tool: LLMs are prone to hallucination, particularly\nin echocardiography, where limited data and guideline-based reasoning demand pre-\ncise numerical interpretation. To ensure reliable outputs, EchoAgent follows clinical\nstandards by using a retrieval tool that accesses guideline documents during reason-\ning. Unlike standard retrieval-augmented generation (RAG) [19], which performs a\nsingle initial retrieval, the agent dynamically retrieves relevant guidelines whenever\nadditional context is needed. The tool employs a Fossil-based dense index over curated\nguideline text [20] to return top-K relevant passages, providing clinically grounded\ncontext that informs the agent’s decisions.\n4 Evaluation\n4.1 Dataset\nThe dataset used in this study was sourced from the echocardiography database at\nVancouver General Hospital, with ethics approval. It includes 207,869 videos from\n56,218 exams across 13 standard views, and 305,229 annotated frames spanning 16\nmeasurement types. All 16 were used to train the measurement feasibility model.\nHowever, only seven standard linear measurements that overlap between our private\n6\n"}, {"page": 7, "text": "                                                                    EchoAgent\n   Reasoning: \n   To assess the right ventricle, I need to measure the RV basal diameter at end-diastole. \n   steps:\n   1. detect_phase(video) to get ED frame number.\n   2. predict_measurement_feasibility(video, ED frame) to see if \"rv_base\" feasible.\n   3. If feasible, take_measurement(video, ED frame, \"rv_base\") to get measurement    \n   value.\n   4. Get guideline reference. Use search_guidelines for \"rv_base\" or \"right ventricle \n   basal diameter\" to provide normative values.\n   Action_1: \n   Calling tool `detect_phase`\n   Reasoning: \n   ED detected at frame #1\n   Action_2: \n   Calling tool `predict_measurement_feasibility`\n   Reasoning: \n   Feasible: rv_base. Good. Now take measurement.\n   Action_3: \n   Calling tool `take_measurement`\n   Reasoning: \n   We have RV basal diameter 4.37 cm. Now we need guidelines. \n   Search guidelines for \"right ventricle basal diameter\".\n   Action_4: \n   Calling tool `search_guidelines` \n   Reasoning: \n   We need to provide normative reference for RV basal diameter. The guidelines   \n   mention \"RV basal diameter (RVD1)\" normal range is < 4.1 cm.\n   Final_Answer: \n   Measured value 4.37 cm is above normal, indicating RV dilation.\n                                                                   User Query\n                                           The given video is related to a female patient.                  \n                                          What are the findings about the right ventricle?\n                                                                  EchoAgent\n   Reasoning: \n   For LV mass, typically use ED measurements: IVS, LVPW, LVIDd. LV mass is   \n   computed using formula: LV mass (g) = 0.8 * [1.04 * ((IVS + LVIDd + LVPW)  - \n3\n   (LVIDd) )] + 0.6.\n3\n   steps:\n   1. detect_phase(video) to get ED frame.\n   2. predict_measurement_feasibility(video, ED frame) to see if IVS, LVPW, LVIDd \n   feasible.\n   3. For each measurement, call take_measurement(video, ED frame, \n   measurement_name) to get values.\n   4. Compute LV mass.\n   Action_1: \n   Calling tool `detect_phase`\n   Reasoning: \n   ED detected at frame #4\n   Action_2: \n   Calling tool `predict_measurement_feasibility`\n   Reasoning: \n   Feasible: IVS, LVPW and LVID. Good. Now take measurement.\n   Action_3: \n   Calling tool `take_measurement`\n   Reasoning: \n   We have IVS = 1.18 cm.\n   Final_Answer: \n   LV mass = 213.44 g.\n                                                                 User Query\n                                     Calculate the LV mass in this echo video.\n   Action_4: \n   Calling tool `take_measurement`\n   Reasoning: \n   We have LVPW = 1.10 cm.\n   Action_5: \n   Calling tool `take_measurement`\n   Reasoning: \n   We have LVIDd = 4.94 cm.\n   Reasoning: \n   Now we have IVS = 1.18 cm, LVPW = 1.10 cm, LVIDd = 4.94 cm. \n   Compute LV mass = 0.8 * [1.04 * ((IVS + LVIDd + LVPW)  - (LVIDd) )] + 0.6 = 213.44 \n3\n3\nFig. 2: EchoAgent Interaction Flow. Examples with visualizations of how EchoA-\ngent handles a user query through reasoning and interaction with different tools. The\nexample on the left is an easier case; the example on the right requires derived calcu-\nlations.\ndataset and the EchoNet-Measurements dataset were used to evaluate the full frame-\nwork. The data were split into training, validation, and test sets, with no patient or\nvideo overlap.\nFor end-to-end evaluation, we curated a benchmark of 60 representative test exam-\nples, each consisting of a video, a clinical question, and its corresponding answer,\ncovering diverse views, image qualities, and measurement types. Each was refined and\nverified by an expert sonographer, with answers grounded in clinical reports. The sono-\ngrapher also assigned difficulty levels: easy (single measurement), medium (multiple\nmeasurements), and difficult (requiring reasoning or derived calculations), enabling\nmore granular evaluation of the agent’s capabilities.\n7\n"}, {"page": 8, "text": "Table 1: Accuracy and failure-case analysis of LLMs on the benchmark (n=60), eval-\nuated on final answers. Accuracy is reported across difficulty levels. Failure cases\ninclude Tool Calling errors (hallucinated or invalid tool calls), and Final Conclu-\nsion errors (clinically invalid interpretations despite correct evidence). Tool-related\nmeasurement errors (e.g., incorrect values or phase detection) are excluded to isolate\nLLM-orchestration weaknesses.\nModel\nAccuracy\n#Failure Cases\nEasy\nMedium\nDifficult\nOverall\nTool Calling\nFinal Conclusion\nLLaMA 3.1 (8B) [23]\n0.33\n0.29\n0.00\n0.28\n17\n22\nQwen3Coder (30B) [22]\n0.44\n0.29\n0.38\n0.42\n2\n24\nGPT-OSS (20B) [21]\n0.64\n0.57\n0.50\n0.62\n2\n11\n4.2 Implementation Details\nEchoAgent employs GPT-OSS-20B [21] as its primary LLM backbone. This open-\nsource model, released by OpenAI, is fine-tuned for structured reasoning and tool use.\nThe framework is modular with respect to the LLM orchestrator. We also evaluate\nQwen3Coder [22] and LLaMA 3.1 [23], both optimized for tool-augmented inference.\nThe agent interacts with tools through structured JSON calls specifying function\nnames and arguments. All models use their official implementations and pretrained\nweights. Input videos are resized to 224 × 224 for feasibility and phase prediction,\nwhile measurement models operate at 480×640 to preserve spatial fidelity. Each query\nis limited to 15 reasoning iterations within the tool-use loop. All experiments were\nconducted on a single NVIDIA B200 GPU (180 GB VRAM).\n4.3 EchoAgent Evaluation\nWe evaluate the impact of different LLMs within our agentic framework by replacing\nthe underlying language model and measuring overall accuracy. Correctness is deter-\nmined by a GPT-4o-based judge. As shown in Table 1, GPT-OSS achieves the highest\noverall accuracy of 0.62, with consistently strong results across all difficulty levels.\nNotably, all models perform best on Easy queries, while struggling more with Medium\nand especially Difficult ones, indicating a growing challenge in handling more complex\nreasoning and decision-making steps within the agent workflow.\nTo better understand failure modes, we analyze the types of incorrect final answers\nacross LLMs (see Table 1). One failure type involves tool calls with hallucinated or\ninvalid arguments, reflecting poor control over structured actions. Another stems from\ndrawing clinically invalid conclusions despite having relevant evidence, revealing weak-\nnesses in reasoning. LLaMA 3.1 fails more often due to the former, while Qwen3Coder\nshows more of the latter. These contrasting patterns suggest that some models struggle\nwith tool reliability, while others falter in inference. GPT-OSS avoids both dominant\nfailure types, indicating a stronger integration of tool use and clinical reasoning within\nthe agentic loop.\n8\n"}, {"page": 9, "text": "Table 2: Tool-specific results on ED/ES frames of the test set.\n(a) Linear-measurement tool\nMeasurement\nMAE\n(cm)\nIVS\n0.13\nLVID\n0.31\nLVPW\n0.22\nLA\n0.29\nAorta\n0.28\nAortic root\n0.27\nRV base\n0.28\n(b) Feasibility-prediction tool\nPrec.\nRec.\nF1\nMicro\n0.84\n0.87\n0.86\nMacro\n0.85\n0.86\n0.85\n(c) Phase-detection tool\nFrame\nMAE (frames)\nED\n1.95 (3.05)\nES\n4.25 (6.63)\n4.4 Tools Evaluation\nLinear Measurement Tool. We evaluate pre-trained EchoNet-Measurements mod-\nels on our internal test set for the seven overlapping linear measurements shared with\ntheir original dataset. As shown in Table 2a, the models demonstrate strong agree-\nment with sonographer-provided ground truth, with low mean absolute error (MAE)\nvalues reported in centimeters. Notably, the reported metrics are comparable to those\nreported in the original paper, indicating that the models generalize well to our private\ndataset without additional fine-tuning.\nFeasibility Prediction Tool. We evaluate the performance of the feasibility predic-\ntion model on over 30,000 end-diastolic (ED) or end-systolic (ES) frames from the test\nset, each annotated with at least one measurement (Table 2b). The model achieves\nstrong frame-level prediction quality, with a micro F1-score of 0.86 and macro F1-score\nof 0.85, indicating consistent performance across both frequent and rare classes. This\nlevel of accuracy enables the agent to reliably identify frames in which the cardiac\nstructures are sufficiently visible and well-positioned for valid measurement extraction,\neffectively reducing spurious tool calls and improving downstream precision.\nPhase Detection Tool. We adopt the Mean Absolute Error (MAE) metric from [24]\nas the average frame-wise distance between each ground truth (GT) frame and the\nclosest predicted frame. For each video, we evaluate a single cardiac cycle by selecting\nthe most probable ED and ES predictions, consistent with the available annotations.\nAs shown in Table 2c, the model achieves an MAE of 1.95 (3.05) for ED and 4.25\n(6.63) for ES, with higher error in ES due to greater visual ambiguity near systole.\n4.5 Ablation Study\nTo evaluate the contribution of each component, we progressively add the feasibil-\nity detector and the clinical context retrieval tool. As shown in Table 3, when both\nare absent, performance drops notably (0.48 overall), indicating that the agent strug-\ngles to reason without structured guidance or measurement validation. Adding the\ncontext retrieval tool improves overall and medium-level accuracy, confirming that\n9\n"}, {"page": 10, "text": "Table 3: Ablation Study\nComponents\nAccuracy\nFeasibility Det.\nClinical Context Retrieval\nOverall\nEasy\nMedium\nDifficult\n✗\n✗\n0.48\n0.56\n0.29\n0.25\n✗\n✓\n0.52\n0.56\n0.43\n0.38\n✓\n✗\n0.53\n0.58\n0.29\n0.50\n✓\n✓\n0.62\n0.64\n0.57\n0.50\nguideline-derived knowledge reduces reasoning errors. Incorporating the feasibility\ndetector further boosts performance, particularly on difficult cases, by filtering unre-\nliable measurements. The full model achieves the highest accuracy across all levels,\ndemonstrating the complementary value of both components.\n5 Discussion\nEchoAgent demonstrates that guideline-centric tool orchestration enables accurate,\ninterpretable echocardiographic analysis with end-to-end reasoning consistent with\nagentic systems in other modalities, such as MedRAX for chest X-ray. By ground-\ning actions in retrieved guidelines and exposing intermediate steps, the framework\nenhances transparency and reduces unsupported conclusions in point-of-care work-\nflows. However, it remains vulnerable to error propagation, as final decision quality\ndepends on upstream tool accuracy. Moreover, the lack of explicit uncertainty mod-\neling limits calibrated confidence and robust deferral when evidence is insufficient.\nFuture work should incorporate uncertainty-aware aggregation, error detection and\nrecovery within the reasoning loop, and broader tool coverage to mitigate failures from\nmissing or imperfect measurements.\n6 Conclusion\nEchoAgent introduces a guideline-centric agent that integrates spatiotemporal video\nanalysis, measurement-feasibility prediction, and clinically grounded interpretation\nwithin a unified, LLM-orchestrated framework. On a curated benchmark, it achieves\naccurate, interpretable, and auditable performance, supported by strong tool accu-\nracy and reliable reasoning across difficulty levels, highlighting the effectiveness of\nstructured tool orchestration compared with end-to-end modeling. By exposing inter-\nmediate steps and guideline references, EchoAgent enhances transparency and ensures\nconsistency with established clinical standards. Future work will expand the toolset\n(e.g., Doppler, volumetric, and additional linear measurements) and extend reason-\ning from single-video analysis to multi-video, study-level synthesis for comprehensive,\ncontext-aware clinical reasoning.\n10\n"}, {"page": 11, "text": "References\n[1] Lang, R.M., Badano, L.P., Mor-Avi, V., et al.: Recommendations for cardiac\nchamber quantification by echocardiography in adults: an update from the amer-\nican society of echocardiography and the european association of cardiovascular\nimaging. European Heart Journal-Cardiovascular Imaging 16(3), 233–271 (2015)\n[2] Keller, M., Magunia, H., Rosenberger, P., Koeppen, M.: Echo as a tool to assess\ncardiac function in critical care—a review. Diagnostics 13(5), 839 (2023)\n[3] Vukadinovic, M., Tang, X., Yuan, N., Cheng, P., Li, D., Cheng, S., He, B.,\nOuyang, D.: Echoprime: A multi-video view-informed vision-language model for\ncomprehensive echocardiography interpretation. arXiv:2410.09704 (2024)\n[4] Amadou, A.A., Zhang, Y., Piat, S., Klein, P., Schmuecking, I., Passerini, T.,\nSharma, P.: Echoapex: A general-purpose vision foundation model for echocar-\ndiography. arXiv:2410.11092 (2024)\n[5] Fallahpour, A., Ma, J., Munim, A., Lyu, H., Wang, B.: MedRAX: Medical\nreasoning agent for chest X-ray. In: ICML, vol. 267, pp. 15661–15676 (2025)\n[6] Qin, Y., Gamage Nanayakkara, D.S., Li, X.: Multi-Agent Collaboration for Inte-\ngrating Echocardiography Expertise in Multi-Modal Large Language Models. In:\nMICCAI, vol. 15966, pp. 358–368 (2025)\n[7] She, C., Lu, R., Chen, L., Wang, W., Huang, Q.: Echovlm: Dynamic\nmixture-of-experts vision-language model for universal ultrasound intelligence.\narXiv:2509.14977 (2025)\n[8] Sahashi, Y., Ieki, H., Yuan, V., Christensen, M., Vukadinovic, M., Binder-\nRodriguez, C., Rhee, J., Zou, J.Y., He, B., Cheng, P., et al.: Artificial intelligence\nautomation of echocardiographic measurements. medRxiv (2025)\n[9] Yao, S., Zhao, J., Yu, D., et al.: React: Synergizing reasoning and acting in\nlanguage models. In: ICLR (2023)\n[10] Schick, T., Dwivedi-Yu, J., Nie, Y., Dess`ı, R., Raileanu, R., Lomeli, M., Hambro,\nE., Zettlemoyer, L., Cancedda, N., Scialom, T.: Toolformer: Language models can\nteach themselves to use tools. NEURIPS 36, 68539–68551 (2023)\n[11] Shen, Y., et al.: HuggingGPT: Solving AI tasks with ChatGPT and its friends in\nHuggingFace. arXiv:2303.17580 (2023)\n[12] Kim, Y., Park, C., Jeong, H., Chan, Y.S., Xu, X., McDuff, D., Lee, H., Ghassemi,\nM., Breazeal, C., Park, H.W.: Mdagents: An adaptive collaboration of LLMs for\nmedical decision-making. Advances in Neural Information Processing Systems 37,\n79410–79452 (2024)\n11\n"}, {"page": 12, "text": "[13] Fathi, N., Kumar, A., Arbel, T.: Aura: A multi-modal medical agent for under-\nstanding, reasoning and annotation. In: International Workshop on Agentic AI\nfor Medicine, pp. 105–114 (2025)\n[14] Bani-Harouni, D., Navab, N., Keicher, M.: Magda: Multi-agent guideline-driven\ndiagnostic assistance. In: International Workshop on Foundation Models for\nGeneral Medical AI, pp. 163–172 (2024)\n[15] Wind, S., Sopa, J., Truhn, D., et al.: Agentic large language models improve\nretrieval-based radiology question answering. arXiv:2508.00743 (2025)\n[16] Masayoshi, K., Hashimoto, M., Yokoyama, R., et al.: EHR-MCP: Real-world eval-\nuation of clinical information retrieval by large language models via model context\nprotocol. arXiv:2509.15957 (2025)\n[17] Dezaki, F.T., Luong, C., Ginsberg, T., Rohling, R., Gin, K., Abolmaesumi,\nP., Tsang, T.: Echo-syncnet: Self-supervised cardiac view synchronization in\nechocardiography. IEEE Trans. on Medical Imaging 40(8), 2092–2104 (2021)\n[18] Mitchell, C., Rahko, P.S., Blauwet, L.A., Canaday, B., Finstuen, J.A., Foster,\nM.C., Horton, K., Ogunyankin, K.O., Palma, R.A., Velazquez, E.J.: Guidelines\nfor performing a comprehensive transthoracic echocardiographic examination in\nadults: recommendations from the american society of echocardiography. Journal\nof the ASE 32(1), 1–64 (2019)\n[19] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K¨uttler,\nH., Lewis, M., Yih, W.-T., Rockt¨aschel, T., et al.: Retrieval-augmented generation\nfor knowledge-intensive NLP tasks. NEURIPS 33, 9459–9474 (2020)\n[20] Xiong, G., Jin, Q., Lu, Z., Zhang, A.: Benchmarking retrieval-augmented gener-\nation for medicine. In: Findings of the Association for Computational Linguistics\nACL 2024, pp. 6233–6251 (2024)\n[21] OpenAI: gpt-oss-120b & gpt-oss-20b Model Card (2025)\n[22] Team, Q.: Qwen3 Technical Report (2025)\n[23] Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur,\nA., Schelten, A., Yang, A., Fan, A., et al.: The Llama 3 herd of models. arXiv\ne-prints (2024)\n[24] Yang, Y., Yang, Q., Cui, K., Peng, C., D’Alberti, E., Hernandez-Cruz, N., Patey,\nO., Papageorghiou, A.T., Noble, J.A.: Latent Motion Profiling for Annotation-\nfree Cardiac Phase Detection in Adult and Fetal Echocardiography Videos. In:\nMICCAI, vol. 15973, pp. 316–325 (2025)\n12\n"}]}