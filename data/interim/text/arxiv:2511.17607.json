{"doc_id": "arxiv:2511.17607", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.17607.pdf", "meta": {"doc_id": "arxiv:2511.17607", "source": "arxiv", "arxiv_id": "2511.17607", "title": "Robustness of Structured Data Extraction from Perspectively Distorted Documents", "authors": ["Hyakka Nakada", "Yoshiyasu Tanaka"], "published": "2025-11-18T07:54:21Z", "updated": "2025-11-18T07:54:21Z", "summary": "Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.17607v1", "url_pdf": "https://arxiv.org/pdf/2511.17607.pdf", "meta_path": "data/raw/arxiv/meta/2511.17607.json", "sha256": "ab6e8bf4952a1570074ebda55f5f5b16adb18415d524dd8a4fb7bf5411b2e4ea", "status": "ok", "fetched_at": "2026-02-18T02:26:41.236943+00:00"}, "pages": [{"page": 1, "text": "1 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThis work has been submitted to the IEEE for possible publication. Copyright may be \ntransferred without notice, after which this version may no longer be accessible. \n"}, {"page": 2, "text": "2 \n \n \nRobustness of Structured Data Extraction from \nPerspectively Distorted Documents \n \nHyakka Nakada \nData Technology Lab \nRecruit Co.,Ltd. \nTokyo, Japan \nhyakka_nakada@r.recruit.co.jp \nYoshiyasu Tanaka \nTokyo Office \nYamadentanaka Co., Ltd. \nTokyo, Japan \ny-tanaka@yamadentanaka.com \n \n \nAbstractâ€”Optical Character Recognition (OCR) for data \nextraction from documents is essential to intelligent informatics, \nsuch as digitizing medical records and recognizing road signs. \nMulti-modal Large Language Models (LLMs) can solve this task \nand have shown remarkable performance. Recently, it has been \nnoticed that the accuracy of data extraction by multi-modal LLMs \ncan be affected when in-plane rotations are present in the \ndocuments. However, real-world document images are usually not \nonly in-plane rotated but also perspectively distorted. This study \ninvestigates the impacts of such perturbations on the data \nextraction accuracy for the state-of-the-art model, Gemini-1.5-pro. \nBecause perspective distortions have a high degree of freedom, \ndesigning experiments in the same manner as single-parametric \nrotations is difficult. We observed typical distortions of document \nimages and showed that most of them approximately follow an \nisosceles-trapezoidal transformation, which allows us to evaluate \ndistortions with a small number of parameters. We were able to \nreduce the number of independent parameters from eight to two, \ni.e. rotation angle and distortion ratio. Then, specific entities were \nextracted from synthetically generated sample documents with \nvarying these parameters. As the performance of LLMs, we \nevaluated not only a character-recognition accuracy but also a \nstructure-recognition accuracy. Whereas the former represents \nthe classical indicators for optical character recognition, the latter \nis related to the correctness of reading order. In particular, the \nstructure-recognition accuracy was found to be significantly \ndegraded by document distortion. In addition, we found that this \naccuracy can be improved by a simple rotational correction. This \ninsight will contribute to the practical use of multi-modal LLMs \nfor OCR tasks. \nKeywordsâ€”LLM; multi modal; OCR; entity extraction; \nperspective distortion \nI.  INTRODUCTION \nOptical Character Recognition (OCR) is a technology that \nrecognizes characters from document images [1]. It has many \napplications, including the digitization of medical records and \nrecognition of road signs. In recent years, the introduction of \ndeep learning has dramatically improved the accuracy of OCR \n[2]. In addition, powerful generative models are capable of \nunderstanding the context of a long document and providing \nanswers to a variety of questions. Particularly, efforts have been \nreported to extract entities by leveraging a Large Language \nModel (LLM) [3,4]. Based on texts and their coordinates \nextracted by OCR, LLMs infer target entities. Multi-modal \nLLMs have been also utilized to extract entities by directly \nreading document images without OCR [5-8]. In other words, \nrather than recognizing characters just as written, it is now \npossible to interpret the context and summarize the desired \nentities with LLMs. Particularly, multi-modal-based methods \nhave the advantage of reducing information loss because they \ncan extract entities from images in a single pass without utilizing \nintermediate output of OCR. This study is concerned with entity \nextraction methods using multi-modal LLMs.  \nScanners or smartphone cameras are usually used to capture \ndocument images. While a scanner produces a precise image that \nis faithful to the original document, smartphones generally \nproduce distorted images. Document skew (i.e. in-plane \nrotation) is a well-known kind of distortion. In addition, \nperspective distortion is a more general kind of distortion. The \nplane of a camera film or CMOS sensor is usually not parallel to \nthat of the document paper during hand-held shooting, which \nleads to perspective distortion. Even if they were parallel, in-\nplane rotation is inevitable. These perturbations are known to \ndegrade the OCR performance, which has been an issue in the \nfield of OCR analysis for several decades. In early studies, much \nattention was paid to the development of algorithms to detect \nand correct distortion in document images. The development of \nsuch algorithms paved the way for understanding the importance \nof correcting distortion for improving the OCR accuracy. \nSeveral previous studies have investigated the effect of \ndocument rotation on OCR performance. Hull et al. proposed to \nrealign rotation with connected component analysis so as to \nimprove OCR performance [9]. Li et al. used wavelet \ndecomposition and projection profile for detecting the rotation \nangle [10]. Recently, the application of deep learning has been \nproposed, leading to more sophisticated methods. Dobai et al. \nshowed that the correction based on a neural network \noutperformed conventional methods [11]. Many other methods \nhave been developed [12-15]. Several studies have been \nreported to resolve perspective distortion. Document boundary \n[16], text lines [17], character shape [18,19] can be used to \ncorrect distortion. In addition, Li et al. proposed the correction \nbased on a neural network [20].  \nWithout these corrections, accuracy degradation has recently \nbeen reported in entity extraction with multi-modal LLMs [21]. \nThey generated synthetically rotated documents with varying \nangles of in-plane rotation and inferred the target entities to \n"}, {"page": 3, "text": "3 \n \n \nevaluate the dependence of the rotation angle on the entity-\nextraction accuracy.  \nAs mentioned above, perspective distortion as well as \nrotation can occur in document images. More precisely, \nperspective distortion is the phenomenon that includes rotation. \nBecause rotation occurs only when the image sensor and the \nplane of a document paper are coincidentally parallel, \nperspective distortion is more frequent and general. Therefore, \nin this study, we focus on evaluating the accuracy degradation \nagainst such distortions. \nHowever, there are several challenges when handling \nperspective distortion. While rotation is described only by a \nsingle parameter (i.e. the rotation angle), perspective distortion \nis generally described by the homographic transformation [22]. \nThis transformation has eight independent parameters as shown \nlater. Thus, unlike the case of rotation in previous studies, grid-\nbased methods are not feasible for evaluating the effect of \nperturbation due to combinatorial explosion. In addition, \nbecause the homographic transformation includes rotation, it \nbecomes difficult to distinguish between rotational and other \ncontributions. Thus, the dependency of the degree of perspective \ndistortion on the entity-extraction accuracy is not easy to \ncomprehensively evaluate. \nIn this paper, we develop a new experimental design for \nperspective distortion with a reduced number of parameters. We \npropose to utilize specific but practical distortions that are likely \nto occur when document images are captured. In practice, the \nmanner of distortion in document images is biased due to \nconstraints of the document and camera locations, and the \nphotographer's intentions. Such restrictions can re-describe \ntypical distortions in document images with fewer parameters. \nWe evaluated the entity-extraction accuracy of the state-of-the-\nart multi-modal models, Gemini-1.5-pro [23] by synthesizing \ndocument images with these distortions. \nII. PREVIOUS WORK: IN-PLANE ROTATION \nRecently, multi-modal LLMs have been applied to the task \nof entity extraction from document images [5-8]. They showed \na great accuracy and have the potential to become standard for \nsuch a task instead of conventional OCR methods. A previous \nstudy applied multi-modal LLMs to entity extraction from \nrotated document images [21]. Specific entities were extracted \nfrom synthetically generated sample documents with varying \nangles of in-plane rotation to estimate the accuracy of entity \nextraction. As a result, they showed that document rotation \nsignificantly affects the performance of multi-modal LLMs. \nAccuracy degradation was observed when the rotation angle \nexceeded a specific threshold, several tens of degrees. Then, the \naccuracy recovered around the angle of 180Â°. The region below \nthe threshold is called Safe In-plane Rotation Angles (SIPRA) \n[21]. Thus, they showed the importance of addressing document \nrotation, through rotation correction or by developing more \nrobust model architectures and training approaches. \n \n \nFig. 1 Examples of shaded images. The shades of a \nsmartphone can be seen partially. \n \n \nFig. 2 Examples of receipt images. Red lines are showed as \nisosceles-trapezoidal guides for document areas. \n \nTable 1 Ratios (%) of document area shapes. Exception means \na complex shape other than quadrilateral, such as a shape \nstrongly bended or folded. \nRectangle \nIsosceles \ntrapezoid \nOther \nquadrilateral \nException \n35.5 \n60.5 \n1.5 \n2.5 \n \nIII. METHODOLOGY \nA. Generalization from Rotation to Perspective Distortion \nWhen document images are captured with a smartphone, the \nsubject is often set on a flat surface, such as a table. However, \nalthough it is desirable to capture images from directly above the \npaper, overhead lights tend to cast a shade on it, as shown in Fig. \n1. For enhanced visibility, photographers often take pictures by \nmoderately tilting their cameras. Therefore, we hypothesize that \ndocument areas are approximately isosceles-trapezoidally \ntransferred, as shown in Fig. 2. We visually checked 200 images \nin the open-source receipt database [24] and classification for \ntheir shapes is listed in Table 1. While less than 36% images \nwere found to be rectangles, more than 60%  were almost \nisosceles trapezoids. Here, by redefining an isosceles trapezoid \nto include a rectangle, about 96% of the document images could \nbe visually regarded as isosceles trapezoids. Although in-plane \nrotation can describe only the pattern of rectangles, isosceles-\ntrapezoidal transformation is more versatile transformation for \ndistortion in most document images. Therefore, in this study, we \nfocus on isosceles-trapezoidal transformations, and evaluated \nthe accuracy degradation against this type of distortion. \nB. Isosceles-trapezoidal Transformation \nA schematic picture of isosceles-trapezoidal transformation \nis shown in Fig. 3 (a). The shape before transformation is called \nthe original shape ğ‘‚ğ´ğµğ¶. The parameters describing an \nisosceles trapezoid are the length of the upper bottom, that of the \nlower bottom, and the height of the transformed shape ğ‘‚ğ´â€²ğµâ€²ğ¶â€². \n"}, {"page": 4, "text": "4 \n \n \nWe restrict ourselves to isosceles trapezoids that keep the area \nand height invariant to the original shape. Thus, \nâ„= â„!,\n(1) \n(ğ‘¥+ ğ‘Ÿğ‘¥) Ã— â„/2 = ğ‘¤!â„!\n(2) \n \nare obtained. Here, â„! is the height of the original shape, and â„ \nis that of the transformed shape. ğ‘¤! is the width of the original \nshape. ğ‘¥ denotes the length of the upper bottom and ğ‘Ÿ> 0 is the \nratio of the lower bottom length to the upper bottom length,  \nğ‘Ÿâ‰¡(Lower bottom length)\n(Upper bottom length) = ğ´\"ğµ\"\nğ‘‚ğ¶\"\n(3) \n \nğ‘¥ and â„ can be eliminated by ğ‘Ÿ according to Eqs. (1) and (2). \nThus, the independent variables can be reduced to only ğ‘Ÿ. \nHereafter, ğ‘Ÿ will be referred to as the distortion ratio, the value \nof which takes around 1 when the shape is almost a rectangle. \nWhen ğ‘Ÿâ†’âˆ, the length of the lower bottom approaches 0. On \nthe other hand, the length of the upper bottom approaches 0 for \nğ‘Ÿâ†’0. That is, the shape will be triangular in these limits. The \nvertices of the transformed shape are explicitly calculated: \nğ‘‚(0,0), ğ´â€²((1 âˆ’ğ‘Ÿ)ğ‘¤!/(1 + ğ‘Ÿ), â„!), ğµâ€²(ğ‘¤!, â„!), and ğ¶â€²(2ğ‘¤!/\n(1 + ğ‘Ÿ),0). \nIn addition, rotation is also combined with the above \ntransformation, as shown in Fig. 3 (b). Specifically, the \nrightmost vertex of the upper bottom edge is rotated by an angle \nğœƒ. Here, ğœƒ is defined as positive clockwise. The vertices are \nexplicitly transformed from ğ‘‚(0,0), ğ´(0, â„!), ğµ(ğ‘¤!, â„!), and \nğ¶(ğ‘¤!, 0) into \nğ‘‚(0,0), \nğ´\"\" P\n(1 âˆ’ğ‘Ÿ)ğ‘¤!\n1 + ğ‘Ÿ\ncos ğœƒâˆ’â„sin ğœƒ, (1 âˆ’ğ‘Ÿ)ğ‘¤!\n1 + ğ‘Ÿ\nsin ğœƒ+ â„cos ğœƒT, \nğµ\"\"(ğ‘¤! cos ğœƒâˆ’â„! sin ğœƒ, ğ‘¤! sin ğœƒ+ â„! cos ğœƒ), \nğ¶\"\" U 2ğ‘¤!\n1 + ğ‘Ÿcos ğœƒ, 2ğ‘¤!\n1 + ğ‘Ÿsin ğœƒV .\n(4) \n \n \n \n \n(a) \n(b) \nFig. 3 (a) Schematic picture of isosceles-trapezoidal \ntransformation. A Cartesian coordinate system is set along the \nsides ğ‘‚ğ´ and ğ‘‚ğ¶ of a rectangle ğ‘‚ğ´ğµğ¶, which represents an \noriginal document shape. The transferred shape is depicted as \nan isosceles trapezoid ğ‘‚ğ´â€™ğµâ€™ğ¶â€™. (b) Then, rotation by angle ğœƒ \nis performed with ğ‘‚ the rotation center. \n \nThe homographic transformation can map between two \nplanar projections of an image [22]. This is described by  \nğ‘ \"\nğ‘¥!\nğ‘¦!\n1\n& = ğ»)\nğ‘¥\nğ‘¦\n1\n* â‰¡\"\nâ„\"\"\nâ„\"#\nâ„\"$\nâ„#\"\nâ„##\nâ„#$\nâ„$\"\nâ„$#\nâ„$$\n&)\nğ‘¥\nğ‘¦\n1\n* .\n(5) \n \nWhile (ğ‘¥, ğ‘¦) is the coordinates in the original shape, (ğ‘¥â€², ğ‘¦â€²) \ndenotes that in the transformed shape. ğ» is a projection matrix, \nand â„##, â€¦ , â„$$, and ğ‘  are the parameters of the homographic \ntransformation. Equation (5) is rewritten in the following form \nğ‘¥\" = â„##ğ‘¥+ â„#%ğ‘¦+ â„#$\nâ„$#ğ‘¥+ â„$%ğ‘¦+ â„$$\n \nğ‘¦\" = â„%#ğ‘¥+ â„%%ğ‘¦+ â„%$\nâ„$#ğ‘¥+ â„$%ğ‘¦+ â„$$\n(6) \n \nby eliminating ğ‘ . Although there are apparently nine parameters, \nğ’‰ has arbitrariness regarding magnitude due to division. Thus, \nthe effective degrees of freedom are eight. Shifting from ğ‘‚ğ´ğµğ¶ \nto ğ‘‚ğ´â€²â€²ğµâ€²â€²ğ¶â€²â€²  (Eq. ( 4 )) uniquely determines the equivalent \nprojection matrix \nğ»=\nâ\nâœ\nâœ\nâœ\nâ›\ncos ğœƒ\n(1 âˆ’ğ‘Ÿ)ğ‘¤! cos ğœƒâˆ’(1 + ğ‘Ÿ)â„! sin ğœƒ\n2ğ‘Ÿâ„!\n0\nsin ğœƒ\n(1 âˆ’ğ‘Ÿ)ğ‘¤! sin ğœƒ+ (1 + ğ‘Ÿ)â„! cos ğœƒ\n2ğ‘Ÿâ„!\n0\n0\nâˆ’(1 âˆ’ğ‘Ÿ)(1 + ğ‘Ÿ)\n2ğ‘Ÿâ„!\n1 + ğ‘Ÿ\n2\nâ \nâŸ\nâŸ\nâŸ\nâ\n. (7)\n \n \nHere, there are only two parameters for the total transformation: \nthe rotation angle ğœƒ and the distortion ratio ğ‘Ÿ. In the limit of ğ‘Ÿâ†’\n1, that is without distortion, Eq. (7) reproduces to an in-plane \nrotation matrix. On the other hand, when ğœƒâ†’0, the isosceles-\ntrapezoidal transformation in Fig. 3 (a) is obtained. Our \nformalism can describe the multifaceted transformation between \nthese limits. \n \nFig. 4 Original image (red frame) is transferred by Eq. (7). For \nevery values of ğœƒ and ğ‘Ÿ, transferred documents are enumerated \nmatrix-like. Their several examples are also shown (yellow, \ngreen, and blue frames). \n!\n\"\n#\n$\n!!\n\"!\n#!\nâ„\nâ„\"\n&\"\n'\n(\n!\n\"!!\n#!!\n$!!\n%\n&\n'\n!\n\"\n2!\n2\".$\n2\"\n2%.$\n2%\n2&%.$\n2&\"\n2&\".$\n2&!\nâˆ’90\nâˆ’80\nâˆ’70\nâˆ’60\nâˆ’50\nâˆ’40\nâˆ’30\nâˆ’20\nâˆ’10\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n"}, {"page": 5, "text": "5 \n \n \n \n \nFig. 5 Prompt into Gemini-1.5-pro for entity extraction. \n \n \nFig. 6 Input document image (left) and LLM output (right). \nJSON data is outputted. By referring to keys, corresponding \nvalues are obtained. For the entity which an LLM cannot \nextract, the corresponding value is padded by null. \nIV. EXPERIMENT \nA. Data sets \nTen images with almost no distortion or rotation in the open-\nsource receipt database [24] were selected as original images and \nsubjected to our transformation. The pair of parameters ğœƒ, ğ‘Ÿ is \nset \nin \nthe \ngrid \nof ğœƒ= [âˆ’90, âˆ’80, â€¦ ,0, â€¦ ,90] , ğ‘Ÿ=\n[2&%, 2&#.(, â€¦ , 2!, â€¦ , 2%], as shown in Fig. 4. In other words, for \neach original shape, 19 Ã— 9 = 171  transformed images are \ngenerated according to Eq. ( 7). Thus, the total number of \nverification data is 1710. \nB. Multi-Modal LLM \nWe used the prompt shown in Fig. 5 to extract eight entities: \nvendor, date, list item, subtotal, tax, total, payment, and change. \nIt is a schema-type prompt for JSON. For simplicity, we gave \nthe following conditions. Items with no price or zero price are \nexcluded, and the price of the item should not be for a unit but \nfor the entire quantity. Vendor is case sensitive. In addition, we \ndefined that total is the sum of subtotal and tax. As a multi-\nmodal LLM, we use Gemini-1.5-Pro-002 with temperature 1.0, \nmax output tokens 8192, and Top-P 0.95. \nC. Evaluation \nAn example of LLM outputs responded by the prompt in Fig. \n5 is shown in Fig. 6. By referring to keys, we extract the \ncorresponding value of vendor, date, list item, subtotal, tax, total, \npayment, and change. Each score is estimated in the following. \nSubtotal, tax, total, payment, and change have numerical values. \nAs shown in Fig. 6, while the true value of subtotal is 71.40, the \npredicted value is 71.4 . Such a mismatch was frequently \nobserved. This is probably because the LLM eliminates \nunnecessary information for summarization. Thus, the scores for \nthese entities are set to 1 if a predicted value is numerically \nequal to a true value (e.g. 71.40 = 71.4), and 0 otherwise.  \n \n \nFig. 7 Average scores are estimated by averaging all of scores \nfor vendor, date, list item, subtotal, tax, total, payment, and \nchange. Darker areas mean the worse scores. \n \n \n[tasks] \nThis image is a receipt. \nA. Extract Vendor, Date, List items, Subtotal, Tax, Total, Payment \nand Change of the image. \n[conditions] \nA. Exclude items with no price or zero price. \nB. The price of the item is for the entire quantity. \nC. Be sure to include the quantity of the item.\nD. Total is the sum of Subtotal and tax. \nE. Vendor is case sensitive. \n[schema] \n{\n\"type\": \"object\", \n\"properties\": {\n\"Vendor\": {\n\"type\": \"string\", \n\"description\": \"Vendorâ€\n}, \n\"Date\": {\n\"type\": \"string\",\n\"description\": \"Date\" \n},\n\"List items\": { \n\"type\": \"array\",\n\"items\": { \n\"type\": \"object\", \n\"properties\": { \n\"Item\": { \n\"type\": \"string\", \n\"description\": \"Item Nameâ€\n}, \n\"Quantity\": { \n\"type\": \"number\", \n\"description\": \"number of Item\" \n}, \n\"Price\": { \n\"type\": \"number\", \n\"description\": \"Item price\" \n} \n} \n} \n}, \n\"Subtotal\": { \n\"type\": \"number\",\n\"description\": \"sum of List itemsâ€\n}, \n\"Tax\": { \n\"type\": \"number\", \n\"description\": \"subtotal taxâ€\n}, \n\"Total\": { \n\"type\": \"number\", \n\"description\": \"total price\" \n}, \n\"Payment\": {\n\"type\": \"number\", \n\"description\": \"payment\" \n}, \n\"Change\": { \n\"type\": \"number\", \n\"description\": \"changeâ€\n} \n} \n} \n[format instruction] \nDate format: YYYY-mm-dd Output in English. Use json not markdown.\n[tasks] \nThis image is a receipt. \nA. Extract Vendor, Date, List items, Subtotal, Tax, Total, Payment \nand Change of the image. \n[conditions] \nA. Exclude items with no price or zero price. \nB. The price of the item is for the entire quantity. \nC. Be sure to include the quantity of the item.\nD. Total is the sum of Subtotal and tax. \nE. Vendor is case sensitive. \n[schema] \n{\n\"type\": \"object\", \n\"properties\": {\n\"Vendor\": {\n\"type\": \"string\", \n\"description\": \"Vendorâ€\n}, \n\"Date\": {\n\"type\": \"string\",\n\"description\": \"Date\" \n},\n\"List items\": { \n\"type\": \"array\",\n\"items\": { \n\"type\": \"object\", \n\"properties\": { \n\"Item\": { \n\"type\": \"string\", \n\"description\": \"Item Nameâ€\n}, \n\"Quantity\": { \n\"type\": \"number\", \n\"description\": \"number of Item\" \n}, \n\"Price\": { \n\"type\": \"number\", \n\"description\": \"Item price\" \n} \n} \n} \n}, \n\"Subtotal\": { \n\"type\": \"number\",\n\"description\": \"sum of List itemsâ€\n}, \n\"Tax\": { \n\"type\": \"number\", \n\"description\": \"subtotal taxâ€\n}, \n\"Total\": { \n\"type\": \"number\", \n\"description\": \"total price\" \n}, \n\"Payment\": {\n\"type\": \"number\", \n\"description\": \"payment\" \n}, \n\"Change\": { \n\"type\": \"number\", \n\"description\": \"changeâ€\n} \n} \n} \n[format instruction] \nDate format: YYYY-mm-dd Output in English. Use json not markdown.\n{\n\"Vendor\": \"Friendly RedÊ¼s of Windham\",\n\"Date\": \"2018-07-21\",\n\"List items\": [\n{\n\"Item\": \"Cadillac Margarita\",\n\"Quantity\": 2,\n\"Price\": 28.0\n},\n{\n\"Item\": \"Maryland Crab Cakes\",\n\"Quantity\": 1,\n\"Price\": 14.0\n},\n{\n\"Item\": \"Chicken Caprese\",\n\"Quantity\": 1,\n\"Price\": 12.0\n},\n{\n\"Item\": \"Chix Parm Sandwich\",\n\"Quantity\": 1,\n\"Price\": 10.0\n},\n{\n\"Item\": \"Bud Light\",\n\"Quantity\": 2,\n\"Price\": 7.4\n}\n],\n\"Subtotal\": 71.4,\n\"Tax\": 6.43,\n\"Total\": 77.83,\n\"Payment\": null,\n\"Change\": null\n}\n!\n\"\n"}, {"page": 6, "text": "6 \n \n \n \n \nFig. 8 Document images rotated by typical angles. In lower side, full-texts extracted by a prompt of Fig. 12. In a middle angle, the \nquantity values are shifted from the original lines. \n \nFor vendor and date, their values are relatively lengthy strings. \nAs scores, we adopt a Jaro-Winkler distance between a true \nvalue and predicted value. Finally, because list item is sorted \ninformation, we estimate the score based on a reading order. In \nother words, from top to bottom, a true item and predicted item \nare compared. Individually, scores for quantity and price are \ncalculated in the same manner of entities with numerical values, \nsuch as subtotal. Scores for item name are calculated by the \nJaro-Winkler distance, like vendor. List item scores are \nestimated by averaging these scores. \nV. RESULTS \nA. Entity Extraction Accuracy \nThe average scores for all indicators are summarized in Fig. \n7. First, the results are explained with respect to rotation angle \nğœƒ. When the angle is ğœƒ= 0Â°, Â±90Â°, relatively high scores are \nachieved compared to other angles. The middle angles gave the \naccuracy degradation, which was consistent with the results of \nthe previous study [21]. In other words, the valleys of average \nscores are resolved at ğœƒ= Â±90Â°. Nevertheless, while the severe \ndecrease appears in the negative angles (i.e. counterclockwise \nrotated image), the change of the accuracy is relatively moderate \nin the positive. \nWith respect to distortion ratio ğ‘Ÿ, the behavior of the score \nis more complex. At ğœƒ= 0Â°, Â±90Â° , the entity-extraction \naccuracy was robust against the values of ğ‘Ÿ. On the other hand, \nthe middle region between such angles showed the degradation \nwith large or small ğ‘Ÿ. In this way, it was confirmed that \nperforming not only rotation but also perspective distortion \ngenerates a rich phenomenon. Because many real-world \ndocument images include the latter perturbation, this result has \ngreat applicability. If ğœƒ= 0Â°, Â±90Â°, i.e. rows or columns are \naligned as shown in Fig. 8, the degree of distortion appears to \nhave little effect on the accuracy. Therefore, for practical use, it \nis expected that highly accurate entity-extraction can be \nachieved simply by performing rotation correction without the \nneed for perspective-distortion correction, which is generally \nmore laborious than the rotation correction. \nB. Considerations for Detailed Scores \nFor further discussion, we observe the score for each entity. \nThe scores for vendor and date were estimated by the Jaro-\nWinkler distance between a true string and predicted string. \nThus, they are considered to reflect the power of character \nrecognition. As shown in Fig. 9, a slight asymmetry of the \nscores can be confirmed with respect to ğ‘Ÿ.  \nSuch an asymmetry may exist because the information of \nvendor and date is usually written on the upper side of receipts. \nIf ğ‘Ÿâ‰«1, i.e. the upper bottom is shrunk, the difficulty of \ncharacter recognition generally increases. For example, the \nyellow-framed image in Fig. 4 has small font sizes in vendor \nand date. Nevertheless, Fig. 9 (a) shows that almost all scores \nare more than 0.98, the accuracy of character recognition was \nfound to be largely unaffected except under excessive distortion. \n"}, {"page": 7, "text": "7 \n \n \nOn the other hand, the scores for subtotal, tax, total, \npayment, and change reflect the power of structure recognition, \nwhich is related to reading order. As mentioned above, the LLM \nhas a great ability of character recognition. Especially, we found \nthat number characters were almost perfectly recognized, as the \ndate score in Fig. 9 (b). Thus, mainly, misunderstanding the \norder relationship between a key and value is considered to \ncontribute to degradation of the scores of subtotal, tax, total, \npayment, and change.  \nIn addition, the list item score reflects the power of reading \norder by definition as mentioned in Chapter IV C. While \nsubtotal, tax, total, payment, and change have each one value, \nlist item is comprised of multiple keys and values. For example, \nas shown in Fig. 6, five pairs of a menu and price are required \nto extract from top to bottom. Here, â€œPatron Silverâ€ without \nprices should be discarded. Their scores were found to decrease \nsignificantly compared with those of vendor and date, as shown \nin Fig. 10. Note that the degradation of the change score is \nrelatively moderate. This is because change was not often \nwritten in the receipts and many of their true values were null. \nParticularly, the rotation angle has an impact on these \ndegradations. In Fig. 10 (a)-(e), the scores are slightly degraded \nin the positive angles and the severe decrease appears in the \nnegative. The score of (f) list item shows the severe decrease in \nboth regions. Two valleys exist around ğœƒ= âˆ’20Â°, 70Â° and the \nexample of extraction results is shown in Fig. 11. This tendency \nbecomes grater for large values of ğ‘Ÿ. Whereas some prices were \nmisassigned and items were partially lost at such angles, the \nother angles show successful extraction.  \nThe reason for this degradation can be considered as follows. \nFor example, in a rotated document as shown in the upper part \nof Fig. 8, the latitude of a key location was shifted far from that \nof the corresponding value's location. As a result, different \nvalues were sometimes assigned, leading to the low scores. By \nusing the prompt in Fig. 12 for extracting all the sentences, it \nwas found that the LLM did not accurately capture their row \nstructure, as shown in the red texts of Fig. 8. At ğœƒ= 0Â°, an item, \nquantity, and price were written in the same rows. On the other \nhand, at a middle angle, they were often observed to be scattered \nover \nseveral \nrows. \nSuch \ndestruction \nmay \nlead \nto \nmisunderstanding the order relationship between a key and \nvalue. In fact, according to the results of extraction shown in Fig. \n11, the prices of some items have been misassigned.  \nThe destruction resolves at ğœƒ= Â±90Â° and the high accuracy \nis achieved again. This recovery may be reasonable because \nLLMs are supposed to learn plentiful document data, which are \nusually documents vertically or horizontally scanned. In \naddition, the order relationship may have been successfully \nunderstood because the information is aligned not in a row, but \nin a column. \nFinally, we remark on the dependency of the distortion ratio \nğ‘Ÿ on the entity-extraction accuracy. Largely, this distortion has \na negative effect on the accuracy in the middle angles, implying \nthat structure recognition is degraded more than character \nrecognition. In addition, while the effect of magnifying the \nupper bottom is moderate, magnifying the lower bottom causes \nsignificant deterioration except for the change score. \nConsiderations for this asymmetry will be the subject of future \nwork. Again, the above degradation occurs mainly among the \nmiddle angles of rotation, little deterioration was observed near \nğœƒ= 0Â°, Â±90Â°. \n \n \nFig. 9 (a) Vendor scores and (b) date scores are enumerated. \nDarker areas mean the worse scores. \n \n   \n \nFig. 10 (a) Subtotal, (b) tax, (c) total, (d) payment, (e) change, \nand (f) list item scores are enumerated. Darker areas mean the \nworse scores. \n \n \n"}, {"page": 8, "text": "8 \n \n \n \n \nFig. 11 Under ğ‘Ÿ= 4, several rotated images are shown. In addition, their cropped images and the partial results of entity \nextraction are enumerated. \n \n \n \nFig. 12 Prompt into Gemini-1.5-pro for full-text extraction. \n \nVI. CONCLUSION \nWhen multi-modal LLMs are applied to data extraction \nfrom documents, the entity extraction accuracy has been \nreported to be affected by in-plane rotation of documents. \nHowever, real-world document images are usually not only in-\nplane rotated but also perspectively distorted. This study \ninvestigates their impacts on the entity extraction accuracy for \nthe state-of-the-art multi-modal LLMs, Gemini-1.5-pro. \nDesigning experiments in the same manner as single-parametric \nrotations is challenging because perspective distortions have a \nhigh degree of freedom. Thus, we eliminated the degree of \nfreedom for insignificant parameters by observing typical \ndistortions of document images, to find that most of them \napproximately follow an isosceles-trapezoidal transformation. \nThis type of distortion has only two parameters, i.e. rotation \nangle ğœƒ and distortion ratio ğ‘Ÿ. Then, specific entities were \nextracted from synthetically generated sample documents with \nvarying these parameters.  \nAs the performance of LLMs, we evaluated not only a \ncharacter-recognition accuracy but also a structure-recognition \naccuracy. Whereas the former represents the classical indicators \nfor optical character recognition, the latter relates with the \ncorrectness of reading order. Especially, the structure-\nrecognition accuracy was found to be significantly degraded due \nto document transformation. In addition, we found that only \ncorrecting rotation is sufficient to improve this accuracy. This \ninsight will contribute to the practical use of multi-modal LLMs \nfor OCR tasks. \nFuture works are discussed. Besides perspective distortions, \ndocument images generally contain noises, such as character \nblur. LLMs can be used to compensate for these character \ndefects based on the context of the document. Thus, we would \nlike to evaluate the robustness of LLM performance against \nsuch a noise. In addition, we proposed a new experimental \ndesign for perspective distortion in this study. Our method is \nexpected to augment high-quality training data for OCR because \nour transformation can automatically generate data that mimics \n. \nA. Be sure tax. \nB. Vendor is case sensitive. \n[schema] \n{\n[tasks] \nThis image is a receipt. \nA. Extract all text of the image. \n[format instruction] \nUse plain text.\n"}, {"page": 9, "text": "9 \n \n \nreal document images, as shown in Table 1. It can be applied to \ngeneral tasks of object detection [25] rather than OCR. \n \nACKNOWLEDGMENT \nThe authors wish to thank Masakazu Yakushiji, Takashi \nEgami, Rinka Fukuji, and Marika Kubota for their advice. We \nalso thank Ryo Takahashi and Viviane Takahashi and our \nanonymous reviewers for their comments. \n \nREFERENCES \n[1] \nS. Mori, H. Nishida, and H. Yamada, â€œOptical character recognition,â€ \nJohn Wiley & Sons, Inc., 1999. \n[2] \nN. Subramani, A. Matton, M. Greaves, and A. Lam, â€œA survey of deep \nlearning approaches for ocr and document understanding,â€ arXiv preprint \narXiv:2011.13534, 2020. \n[3] \nV. Perot, K. Kang, F. Luisier, G. Su, X. Sun, et al., â€œLMDX: Language \nmodel-based document information extraction and localization,â€ arXiv \npreprint arXiv:2309.10952, 2023. \n[4] \nF. Loukil, S. Cadereau, H. Verjus, M. Galfre, K. Salamatian, et al., \nâ€œLLM-centric pipeline for information extraction from invoices,â€ In \nProceedings of the 2nd International Conference on Foundation and \nLarge Language Models (FLLM), 2024, pp. 569-575. \n[5] \nJ. Ye, A. Hu, H. Xu, Q. Ye, M. Yan, et al., â€œmplug-docowl: Modularized \nmultimodal large language model for document understanding,â€ arXiv \npreprint arXiv:2307.02499, 2023. \n[6] \nD. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, et al., â€œDocllm: A \nlayout-aware generative language model for multimodal document \nunderstanding,â€ arXiv preprint arXiv:2401.00908, 2024. \n[7] \nY. Liu, B. Yang, Q. Liu, Z. Li, Z. Ma, et al., â€œTextmonkey: An ocr-free \nlarge multimodal model for understanding document,â€ arXiv preprint \narXiv:2403.04473, 2024. \n[8] \nY. Liu, Z. Li, M. Huang, B. Yang, W. Yu, et al., â€œOcrbench: on the hidden \nmystery of ocr in large multimodal models,â€ Science China Information \nSciences, vol. 67, no. 12, pp. 220102, December 2024. \n[9] \nJ. J. Hull, â€œDocument image skew detection: Survey and annotated \nbibliography,â€ Document Analysis Systems II, 1998, pp. 40-64. \n[10] S. Li, Q. Shen, and J. Sun, â€œSkew detection using wavelet decomposition \nand projection profile analysis,â€ Pattern Recognition Letters, vol. 28, no. \n5, pp. 555-562, April 2007. \n[11] L. Dobai and M. Teletin, â€œA document detection technique using \nconvolutional neural networks for optical character recognition systems,â€ \nIn Proceedings of the 27th European Symposium on Artificial Neural \nNetworks, 2019, pp. 547-552. \n[12] V. N. M. Aradhya, G. H. Kumar, and P. Shivakumara, â€œAn accurate and \nefficient skew estimation technique for south indian documents: a new \nboundary growing and nearest neighbor clustering based approach,â€ Int. \nJ. Robot. Autom., vol. 22, no. 4, pp. 272-280, September 2007. \n[13] A. A. Mascaro, G. D. Cavalcanti, and C. A. Mello, â€œFast and robust skew \nestimation of scanned documents through background area information,â€ \nPattern Recognition Letters, vol. 31, no. 11, pp. 1403-1411, August 2010. \n[14] M. Shafii, â€œOptical character recognition of printed persian/arabic \ndocuments,â€ Ph.D. thesis, 2014. \n[15] A. Al-Khatatneh, S. A. Pitchay, and M. Al-qudah, â€œA review of skew \ndetection techniques for document,â€ In Proceedings of the 17th UKSim-\nAMSS International Conference on Modelling and Simulation (UKSim), \n2015, pp. 316-321. \n[16] P. Clark and M. Mirmehdi, â€œLocation and recovery of text on oriented \nsurfaces,â€ In Proceedings of the SPIE Document Recognition and \nRetrieval VII, 1999, pp. 267-277. \n[17] P. Clark and M. Mirmehdi, â€œEstimating the orientation and recovery of \ntext planes in a single image,â€  In Proceedings of the 12th British Machine \nVision Conference (BMVC2001), 2001, pp. 421-430. \n[18] J. Liang, D. DeMenthon, and D. Doermann, â€œGeometric rectification of \ncamera-captured document images,â€ IEEE Transactions on Pattern \nAnalysis and Machine Intelligence, vol. 30, no. 4, pp. 591-605, April \n2008. \n[19] S. Lu, B. M. Chen, and C. Ko, â€œPerspective rectification of document \nimages using fuzzy set and morphological operations,â€ Image and Vision \nComputing, vol. 23, no. 5, pp. 541-553, May 2005. \n[20] X. Li, B. Zhang, P. V. Sander, and J. Liao, â€œBlind geometric distortion \ncorrection on images through deep learning,â€ In Proceedings of the \nIEEE/CVF Conference on Computer Vision and Pattern Recognition \n(CVPR), 2019, pp. 4850-4859. \n[21] A. Biswas and W. Talukdar, â€œRobustness of structured data extraction \nfrom in-plane rotated documents using multi-modal large language \nmodels (LLM),â€ Journal of Artificial Intelligence Research, vol. 4, no. 1, \npp. 176-195, March 2024. \n[22] Y. Luo, X. Wang, Y. Liao, Q. Fu, C. Shu, et al., â€œA review of homography \nestimation: Advances and challenges,â€ Electronics, vol. 12, no. 24, pp. \n4977, December 2023. \n[23] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, et al., â€œGemini 1.5: \nUnlocking multimodal understanding across millions of tokens of \ncontext,â€ arXiv preprint arXiv:2403.05530, 2024. \n[24] ExpressExpense, \nâ€œFree \nreceipt \nimages,â€ \nhttps://expressexpense.com/blog/free-receipt-images-ocr-machine-\nlearning-dataset/, 2020. \n[25] K. Wang, B. Fang, J. Qian, S. Yang, X. Zhou, and J. Zhou, â€œPerspective \ntransformation data augmentation for object detection,â€ IEEE Access, vol. \n8, pp. 4935-4943, December 2020. \n \n \n"}]}