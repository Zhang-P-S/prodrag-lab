{"doc_id": "arxiv:2511.13703", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.13703.pdf", "meta": {"doc_id": "arxiv:2511.13703", "source": "arxiv", "arxiv_id": "2511.13703", "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations", "authors": ["Lavender Y. Jiang", "Angelica Chen", "Xu Han", "Xujin Chris Liu", "Radhika Dua", "Kevin Eaton", "Frederick Wolff", "Robert Steele", "Jeff Zhang", "Anton Alyakin", "Qingkai Pan", "Yanbing Chen", "Karl L. Sangwon", "Daniel A. Alber", "Jaden Stryker", "Jin Vivian Lee", "Yindalon Aphinyanaphongs", "Kyunghyun Cho", "Eric Karl Oermann"], "published": "2025-11-17T18:52:22Z", "updated": "2025-11-17T18:52:22Z", "summary": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.13703v1", "url_pdf": "https://arxiv.org/pdf/2511.13703.pdf", "meta_path": "data/raw/arxiv/meta/2511.13703.json", "sha256": "cadfe91f5f4f7d2aaf7dd005880bcdbfd7404f1991950df39ff2fa134e70a3c8", "status": "ok", "fetched_at": "2026-02-18T02:26:46.420803+00:00"}, "pages": [{"page": 1, "text": "Generalist Foundation Models Are Not Clinical\nEnough for Hospital Operations\nLavender Y. Jiang1,2,3*†, Angelica Chen1†, Xu Han2, Xujin Chris Liu2,4,\nRadhika Dua1,2,3, Kevin Eaton5,6, Frederick Wolff2,7, Robert Steele2,8,\nJeff Zhang9,10, Anton Alyakin2,11, Qingkai Pan2, Yanbing Chen2,12,\nKarl L. Sangwon2,5, Daniel A. Alber2,5, Jaden Stryker2,\nJin Vivian Lee2,3,11, Yindalon Aphinyanaphongs6,9,10,\nKyunghyun Cho1,3,13, Eric Karl Oermann1,2,3,5,9,14*\n1Courant Institute School of Mathematics, Computing, and Data Science, New\nYork University, 60 5th Ave, New York, 10001, NY, USA.\n2Department of Neurosurgery, NYU Langone Health, 550 First Avenue, New\nYork, 10016, NY, USA.\n3Global AI Frontier Lab, New York University, 1 Metrotech Center, Fl. 22,\nBrooklyn, 11201, NY, USA.\n4Electrical and Computer Engineering, Tandon School of Engineering, 6\nMetroTech Center, Brooklyn, 11201, NY, USA.\n5Grossman School of Medicine, New York University, 550 First Avenue, New\nYork, 10016, NY, USA.\n6Department of Medicine, NYU Langone Health, 550 First Avenue, New York,\n10019, NY, USA.\n7Department of Computer Science, ETH Zurich, Universitätstrasse 6, City,\n8092, Zurich, Switzerland.\n8Department of Surgery, NYU Langone Health, 1 Park Avenue, New York,\n10016, NY, USA.\n9Division of Applied AI Technologies, NYU Langone Health, 227 East 30th\nStreet, New York, 10016, NY, USA.\n10Department of Population Health, NYU Langone Health, 450 First Avenue,\nNew York, 10019, NY, USA.\n11School of Medicine, Washington University of St. Louis, 660 S. Euclid Ave.,\nSt. Louis, 63110, MO, USA.\n12School of Global Public Health, New York University, 708 Broadway, New\nYork, 10003, NY, USA.\n1\narXiv:2511.13703v1  [cs.CL]  17 Nov 2025\n"}, {"page": 2, "text": "13Prescient Design, Genentech, 149 5th Ave. 3rd floor, New York, 10019, NY,\nUSA.\n14Department of Radiology, NYU Langone Health, 450 First Avenue, New\nYork City, 10019, NY, USA.\n*Corresponding author(s). E-mail(s): Lavender.Jiang@nyu.edu;\nEric.Oermann@nyulangone.org;\nContributing authors: Angelica.Chen@nyu.edu; Xu.Han@nyulangone.org;\nxl3942@nyu.edu; rd3571@nyu.edu ; Kevin.Eaton@nyulangone.org;\nwfrederick@ethz.ch; Robert.Steele@nyulangone.org;\nJeff.Zhang@nyulangone.org; Anton.Alyakin@nyulangone.org;\nivrinom@gmail.com; yc6785@nyu.edu; Karl.Sangwon@nyulangone.org;\nDaniel.Alber@nyulangone.org; Jaden.Stryker@nyulangone.org;\njin.v.lee@wustl.edu; yin.a@nyulangone.org; Kyunghyun.Cho@nyu.edu;\n†These authors contributed equally to this work.\nAbstract\nHospitals and healthcare systems rely on operational decisions that determine patient flow,\ncost, and quality of care. Despite strong performance on medical knowledge and conver-\nsational benchmarks, foundation models trained on general text may lack the specialized\nknowledge required for these operational decisions. We introduce Lang1, a family of mod-\nels (100M–7B parameters) pretrained on a specialized corpus blending 80 billion clinical\ntokens from NYU Langone Health’s electronic health records (EHRs) and 627 billion\ntokens from the internet. To rigorously evaluate Lang1 in real-world settings, we devel-\noped the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331\nEHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortal-\nity prediction, length of stay, comorbidity coding, and predicting insurance claims denial.\nIn zero-shot settings, both general-purpose and specialized models underperform on four\nof five tasks (36.6%–71.7% AUROC), with mortality prediction being an exception (up to\n94.2% AUROC). After finetuning, Lang1-1B outperforms finetuned generalist models up\nto 70× larger and zero-shot models up to 671× larger, improving AUROC by 3.64%–6.75%\nand 1.66%–23.66% respectively. We also observed cross-task scaling with joint finetun-\ning on multiple tasks leading to across the board improvement on other tasks. Lang1-1B\neffectively transfers to out-of-distribution settings, including other clinical tasks and an\nexternal health system. Our findings suggest that predictive capabilities for hospital oper-\nations require explicit supervised finetuning, and that this finetuning process is made\nmore efficient by in-domain pretraining on EHR. Our findings support the emerging view\nthat specialized LLMs can compete with generalist models in specialized tasks, and show\nthat effective healthcare systems AI requires the combination of in-domain pretraining,\nsupervised finetuning, and real-world evaluation beyond proxy benchmarks.\nKeywords: pretraining, finetuning, Electronic Health Records, hospital operations, clinical\nprediction tasks, domain-specific models\n2\n"}, {"page": 3, "text": "1 Main\nHealthcare systems face high-stakes operational decisions daily: which patients are at imminent\nrisk of decline, who can be safely discharged, how many beds will be available for new\nadmissions. These decisions directly impact resource allocation, care coordination, and patient\noutcomes [1–3]. As healthcare systems face increasing patient volume, there is a need for tools\nthat can analyze complex clinical data to inform these critical decisions. Foundation models,\nwith their powerful text comprehension capabilities and versatility in specialized domains\n[4–9], have emerged as a promising technology for optimizing hospital operations.\nHowever, deploying LLMs in clinical settings is fraught with challenges. While LLMs\nshow promise in various clinical tasks [10–25], there is disagreement on whether smaller\nspecialized models (\"specialists\") can outperform general-purpose models (\"generalists\") [26–\n28]. Many evaluations rely on proxy benchmarks that weakly reflect real-world clinical\nconstraints like data scarcity and temporal shifts [17, 29–35]. Data privacy concerns further\nlimit the pretraining data for the vast majority of the clinical LLMs [36] to a small set of pub-\nlic corpora, MIMIC [37] and PubMed [38], even though large-scale EHR datasets are known\nto improve out-of-domain generalization [28, 39–42].\nIn this work, we focus on hospital operation tasks that represent the daily challenge of\nhealthcare delivery and explore the tradeoffs between off-the-shelf generalists and specialized\nmodels trained on a health system’s internal patient notes. Our contributions are as follows:\n1. Lang1: a family of models specialized for hospital operations. We present Lang1, a\nsuite of decoder LLMs (100M, 1B, and 7B), pretrained from scratch on a mix of 80 billion\ntokens of EHR notes and 627 billion tokens of internet texts. After task-specific finetuning,\nLang1 outperforms both off-the-shelf generalist LLMs (such asDeepseek R1 671B) and\nthe parameter-efficient finetuned variant (LoRA finetuned Deepseek Distilled Llama\n70B), on the ReMedE benchmark. Instruction finetuned on one or more tasks, Lang1\ntransfers zero-shot to related tasks and to a different hospital, surpassing generalist models\nof similar scales.\n2. ReMedE: an operations-grounded evaluation suite. We construct an internal evaluation\nbenchmark consisting of five clinically important classification tasks from a multi-hospital\nacademic health system across 10 years, where each task contains 87,974 to 421,429\nreal patients. The benchmark has time-based splits to mimic deployment settings and\ndata-efficiency protocols to reflect real-world constraints.\n3. Engineering Principles for clinical utilities. We analyze the training dynamics and show\nthat pretraining on next token prediction of unlabeled clinical notes and web text contributes\nto emergent skills on comprehension tasks but is insufficient for excelling on ReMedE,\nwhich specifically requires supervised finetuning (SFT). However, SFT is made more\nefficient by in-domain pretraining, and larger models pretrained on more clinical data\nimprove temporal robustness.\nOverall, our results suggest that health systems with the capacity for in-house model devel-\nopment can gain clear advantages from smaller specialized models, providing a practical and\ndata-efficient pathway to robust operational prediction with minimal task-specific supervision.\n3\n"}, {"page": 4, "text": "“Sepsis”\na.\nData Collection\nb.\nPre-training\nc.\nFine-Tuning and Transfer Learning\ne.\nAblation Experiments\nInternet\nEHR\nDataset\nPre-trained \nLang1\nPatient is febrile and \nmay have [...]\nDeepSeek-R1\nLlama-3-70B\nGPT-4o\nMedQA\nLeaderboard Models\nFine-tuned \nLang1\nB. No\nMultiple-Choice based Fine-tuning tasks\nTest on Unseen Task\nClinical Notes\nWeb Texts\nQ. How long will pt \nstay at hospital?\nAdmission \nNotes\nQ. [Mortality] Given {admission note}, \nwill this patient die during admission?\nd.\nComparison to Generalist Language Models\nvs\nClinical Notes\nWeb Texts\nData Mix Variation\nPre-train \nModel Scale Variation\nData Scale Variation\nClinical \nFine-Tuning\n“A. 0 to 2 days”\nPredicted Probability: 0.4\nGround Truth: 0.6\nCompute\nLoss\nUpdate Weights\nGeneralist Language Models\nQ. Will pt be \nre-admitted?\nDischarge \nNotes\nA.\nYes\nB.\nNo\nEval Hospital Variation\nNYU\nBoston\nEval Task Type Variation\nNonclinical \nFine-tuned \nLang1\nPre-trained Trajectories \n…\nckpt 1k\nckpt 2k\nckpt 3k\nckpt + \nLow data FT\nckpt + \nmax data FT\nEval Time Variation\nPerformance Consistency \nAcross Real-Time\nFig. 1: Overview. (a) We mix unlabeled EHR notes and web texts as our pretrain corpus.\n(b) We pretrain using next token prediction. (c) Instruction finetuning in multiple choice\nformat enables cross-task transfer. (d) We compare Lang1 to off-the-shelf generalist models.\n(e) In order to derive design principles, we do ablations on data mix, model scale, pretrain\ntrajectories, data scale, eval task type, eval hospital, and eval time.\n1.1 Overview\nOur work consists of five stages: data collection, pretraining, finetuning, evaluation, and\nablations. As shown in Figure 1, we first collect unlabeled clinical notes from NYU Langone\nEHR and web texts from the internet and mix them to form the corpus (Figure 1a). We\npretrain Lang1 via next token prediction (Figure 1b). We instruction finetune Lang1 to predict\nlabels for real-world clinical tasks, enabling cross-task transfer (Figure 1c). We then compare\nfinetuned Lang1 with off-the-shelf generalist models (Figure 1d) and perform ablations of\nthe data mix, model scale, eval task type, pretraining trajectory, finetune data scale, eval data\ntime, and eval hospital to derive design principles for clinical utilities (Figure 1e).\nWe evaluate Lang1 using ReMedE, an internal benchmark of real-world, high-impact\nclinical tasks beyond diagnosis. Unlike recent benchmarks that focus on multi-turn diagnostic\ndialogue [15, 24, 29], which captures an important but narrow part of clinical decision making,\nReMedE is based on 668,331 EHR notes and emphasizes operational tasks that better represent\nthe day-to-day challenges of healthcare delivery [1–3]. These tasks support practical goals\nsuch as reducing costs, optimizing resource use, and improving continuity of care.\nReMedE includes five predictive tasks drawn from real-world hospital workflows: 30-\nday all-cause readmission, in-hospital mortality, binned length of stay, insurance denial, and\nimputation of binned Charlson Comorbidity Index (CCI, a measure of patient comorbidity\nburden). These tasks reflect critical decisions tied to patient outcomes, resource planning,\n4\n"}, {"page": 5, "text": "and healthcare operations (See Methods 5.2.2 for more details). To assess model robustness\nto temporal distribution shifts, each task is evaluated across three non-overlapping test splits\ndrawn from distinct time periods (Appendix A). ReMedE supports flexible few-shot evaluation\nacross a range of language model interfaces and task formats. It is easily extensible for new\ntasks and evaluation settings. We plan to release ReMedE as a secure evaluation service,\nallowing trusted researchers to submit models and receive standardized evaluation results\nwithout direct access to patient data. This design safeguards patient privacy while enabling\nfair and reproducible model comparison.\n2 Results\nReadmission\nIn\nHospital\nMortality\nInsurance\nDenial\nLOS\nCCI\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nROC AUC\nLang1-1B\nMedMobile-3.8B\nLlama 3.2 1B\nLlama 2 7B\nLlama 3.3 70B Chat\nDeepSeek R1 Distill Llama 70B \nDeepSeek R1\ngpt-4o-sampling-prob\nRandom-guess AUC\nBest per task (zero-shot)\n(a) Both generalists and specialists underperform zero-shot. Yellow triangles indicate the best zero-shot\nperformance per task. While the best mortality prediction AUROC is 94.2%, performance of other tasks\n(readmission, insurance denial, LOS, CCI) range from 36.6%–71.7% AUROC.\nReadmission\nIn\nHospital\nMortality\nInsurance\nDenial\nLOS\nCCI\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nROC AUC\nBest Zero-shot (max 671B)\nLlama 3.2 1B (finetuned)\nDSR Distill Llama 70B (LoRA finetuned)\nLang1-100M (finetuned)\nLang1-1B (finetuned)\nLang1-7B (finetuned)\nRandom-guess AUC\nBest per task (zero-shot)\nBest per task (finetuned)\n(b) Finetuned Lang1-1B (purple) outperform best zero-shot performance (magenta) by 1.66% to 23.66%\nAUROC and finetuned Llama 3.2 1B (light blue) and LoRA finetuned Llama 70B (deep blue) by 3.64%\nto 6.75% AUROC. Yellow stars indicate the best finetuned performance per task.\nFig. 2: Finetuned small specialists outperform strong generalists on ReMedE.\nLarge generalist models underperform on real-world clinical predictive tasks..\nWe\nevaluate both large generalist foundation models and medQA leaderboard models under zero-\nshot inference, and find that they underperform on ReMedE tasks. After finetuning, Lang1-1B\noutperform both two finetuned models (Llama 3.2 1B and LoRA Deepseek R1 Distill\nLlama 3.2 70B) by 3.64% to 6.75% AUROC. Lang1-1B also outperforms the best zero-shot\nperformance (including models up to DeepSeek R1 671B) by 1.66% to 23.66% AUROC.\n5\n"}, {"page": 6, "text": "Figure 2a shows that on specialized tasks such as insurance denial, both leaderboard models\n(Llama 2 7B—light blue, Llama 3.2 1B—blue, MedMobile—grey) and our own pretrained\nmodels (Lang1-1B—purple) underperform in the zero-shot setting. While mortality predic-\ntion has up to 94.2% AUROC, the other four tasks range between 36.6%-71.7% AUROC. This\nshows that even the strongest generalist models falter on these specialized operational tasks.\nFigure 2b compares the best zero-shot result per task against a few finetuned models,\nincluding Lang1 (100M, 1B and 7B), Llama 3.2 1B, and a parameter-efficient finetuned\nversion of Deepseek R1 Distill Llama 70B. Across all five tasks, Lang1-1B and Lang1-7B\n(purple bars) achieve higher AUROC than the best zero-shot model (magenta) and the other\nfinetuned models (light blue and dark blue). The improvements over the other finetuned models\nrange from 3.64%–6.75%. The improvements over the best zero-shot baseline range from\n1.66% (mortality) to 23.66% (insurance denial), with the largest gains observed at insurance\ndenial prediction, which is Deepseek R1’s worst performing task.\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nNumber of Pretraining Tokens\n1e11\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nAccuracy\nPubMedQA\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nNumber of Pretraining Tokens\n1e11\nSciQ\nLang1-1B\nLang1-7B\nrandom guess\n(a) Reading comprehension performance increases from pretraining.\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nROC AUC\nReadmission\nIn-Hospital Mortality\n0\n1\n2\n3\n4\nNumber of Pre-training Tokens\n1e11\nInsurance Denial\n0\n1\n2\n3\n4\nNumber of Pre-training Tokens\n1e11\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nROC AUC\nLength of Stay Prediction\n0\n1\n2\n3\n4\nNumber of Pre-training Tokens\n1e11\nComorbidity Prediction\nLang1-1B\nLang1-7B\nrandom guess\n(b) Clinical classification performance does not rapidly emerge from pretraining.\nFig. 3: Zero-shot clinical classification performance does not increase over the course of\npretraining, unlike reading comprehension. Error bars depict the 95% confidence interval.\nClinical performance does not emerge during pretraining, unlike reading comprehen-\nsion.\nWe tracked zero-shot performance of Lang1 (1B and 7B) throughout pretraining, as a\nfunction of the number of tokens seen. On comprehension tasks (Method 5.2.4 for data details),\naccuracy increased with additional pretraining data (Figure 3a), consistent with the intuition\nthat language models improve on text-based reasoning tasks as they are exposed to more data.\nIn contrast, zero-shot AUROC on clinical classification tasks (ReMedE) remained close to or\n6\n"}, {"page": 7, "text": "below random chance across the entire pretraining trajectory (Figure 3b). We hypothesize that\nthe mapping from clinical notes to outcomes does not emerge from learning next token pre-\ndiction on unlabeled texts alone, but must be learned through either task-specific finetuning\nor alternative pretraining objectives.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nTotal # of Tokens Seen (Pretraining + Finetuning)\n1e11\n0.4\n0.5\n0.6\n0.7\nReadmission ROC AUC\nRandom guess\nNo pretrain, no finetune\nNo pretrain, full finetune\nFull pretrain, full finetune\n0\n2\n4\n6\n# Finetune Tokens\n1e8\n(a) Finetuning is more token-efficient for performance gains, but pretraining still provides value. At any\nfixed token budget (a vertical slice on the x axis), using more finetuning tokens (darker colors) yields a\nhigher ROC AUC for Lang1-1B’s checkpoint trajectory. Nonetheless, a clear gap remains between fully\nfinetuning without pretraining (purple diamond) and the fully pretrained model (yellow star).\n1012\n1013\nTotal Number of Training Tokens (Pretrain + Finetune)\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\nReadmission Test ROC AUC (2024)\nModel\nLang1-1b (314.6B pretrain tokens)\nLang1-1b (419.4B pretrain tokens)\nLlama-2-7B (2T pretrain tokens)\nLlama-3.2-1B (9T pretrain tokens)\nExperiment\nBest Finetuning Run\nLow Finetune Data\n(b) Clinically pretrained models (purple Lang1-\n1b), when finetuned, outperform generalist models\nof similar size (blue Llama models), especially\nin low data regime (cross). Arrows track the same\npretrained model as it is finetuned on different\nnumbers of examples.\n10\n12\n14\n16\n18\n20\nPerplexity of Answer given Question ppl(A|Q)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nReadmission ROC AUC\nModel\nLang1-1b (314.6B tokens)\nLlama-3.2-1B (9T tokens)\nLlama-2-7B (2T tokens)\nExperiment\nFinetuned\nZero-shot\nExperiment\nFinetuned\nZero-shot\n(c) Lower model perplexity on ReMedE answer-\nquestion pairs is associated with better zero-\nshot (cross) and finetuned (circle) performance.\nLang1(purple) has lower perplexity despite fewer\ntotal pretrain tokens (though more clinical tokens).\nArrows track the same model’s performance.\nFig. 4: Finetuning is are more token-efficient than pretraining for performance gains\n(Figure 4a), but in-domain pretraining enables sample-efficient finetuning (Figure 4b). This\nadvantage is also associated with lower perplexity on in-domain tasks (Figure 4c).\nCompute is better spent on finetuning, but pretraining makes finetuning more efficient.\nFigure 4a examines the pretraining and finetuning trajectory of the Lang1-1B readmission\nmodel under a fixed total token budget (pretraining + finetuning, i.e., each vertical slice). Dur-\ning pretraining of Lang1-1B, checkpoints were saved after each one million training tokens.\n7\n"}, {"page": 8, "text": "Each pretrain checkpoint is finetuned using 100–362,259 discharge notes with readmission\nlabel (2.0M–742.0M tokens). The compute budget is the sum of pretraining token for that\ncheckpoint and the number of finetuning tokens used to finetune that particular checkpoint.\nWithin each slice, increasing the proportion of finetuning tokens (darker colors) consistently\nimproves performance. At the same time, pretraining still provides value: even with maximal\nfinetuning data, models initialized from pretraining (purple diamond) outperform the ran-\ndomly initialized one (yellow star) by 7.03% AUROC. A similar pattern appears in Figure 4b:\nwhen finetuning data are scarce, Lang1-1B (purple) outperforms generalist models of compa-\nrable scale (blue Llama-2-7B and Llama-3.2-1B) that were pretrained on more nonclinical\ntokens, demonstrating the efficiency gains of domain-specific pretraining. In fact, Figure 4c\nshows that Lang1-1B, despite being trained on fewer tokens, achieves lower perplexity on\nanswer–question pairs and stronger zero-shot and finetuned performance than Llama-2-7B\nand Llama-3.2-1B. Ablations show that larger models trained on more recent data have better\nperformance (Appendix D).\n \n \n \n \n \nTask-specific ROC AUCs\n             \n \n \n \n \n \n             \nFinetuning Dataset(s)\n48%\n56%\n55%\n54%\n43%\n77%\n84%\n64%\n77%\n57%\n70%\n96%\n64%\n80%\n58%\n63%\n47%\n77%\n65%\n58%\n69%\n78%\n63%\n90%\n61%\n50%\n30%\n52%\n51%\n87%\n76%\n95%\n76%\n90%\n88%\nNo Finetuning\n, \n, \n, \n, \n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nReadmission\nMortality\nLOS\nCCI\nInsurance\nDenial\nReadmission Mortality\nLOS\nCCI\nInsurance\nDenial\n(a) Finetuning Lang1-1B can often transfer across\ntasks. The heatmap shows the finetuned model’s\nperformance when finetuned on a subset of\nReMedE tasks (y axis) and evaluated on all five\nReMedE tasks (x axis).\nLang1-1B\nLlama-3.2-1B\nPretrained model\nzero-shot\nNYU\nReadmission\nMIMIC\nReadmission\nFinetune setting\n51.6%\n±1.5%\n45.5%\n±1.5%\n66.4%\n±1.4%\n60.8%\n±1.4%\n67.7%\n±1.4%\n58.3%\n±1.5%\nExternal Validation of Finetuning Lang1\nfor MIMIC III Readmission\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nROC AUC on MIMIC III Readmission\n(b) Finetuning on NYU Readmission (purple)\ntransfers well (darker yellow) to MIMIC III Read-\nmission (green). Overall performance is better\non finetuning Lang1-1B (purple) compared to\nLlama 3.2 1B (blue).\nFig. 5: Lang1 is able to transfer to unseen task (Figure 5a) and a different health system\n(Figure 5b).\nLang1 is able to transfer to unseen task and a different health system.\nThe heatmaps\nin Figure 5a show Lang1-1B finetuned on one or all ReMedE tasks (rows) and evaluated on\nall five tasks (columns). Overall, Lang1-1B achieves strong single-task (diagonal) and joint-\ntask (last row) performance, and is well calibrated (Appendix E). Many tasks transfer. For\nexample, finetuning on readmission (second row) boosts performance on the other four tasks.\nHowever, this transfer could be asymmetric, i.e., mortality helps LOS (third row, third column),\nbut LOS does not help mortality (fourth row, second column), which can be explained using\ndomain knowledge (Appendix F). Compared to Lang1-1B, Llama-3.2-1B has overall worse\nperformance and a different transfer pattern (Appendix C), suggesting that the pattern depends\non the pretrained model.\n8\n"}, {"page": 9, "text": "Figure 5b shows Lang1-1B and Llama-3.2-1B finetuned using different readmission data\n(MIMIC III or NYU) and test on MIMIC. Finetuning Lang1-1B shows better performance on\nboth data. For Lang1-1B, finetuning on MIMIC is slightly better than NYU by 1.2% AUROC.\nHowever, for Llama-3.2-1B, finetuning on NYU is surprisingly better than finetuning on\nMIMIC by 2.5% AUROC. We suspect this is because NYU+ Readmission has more labeled\npairs than MIMIC readmission, suggesting that nonclinical models may benefit more from a\nlarge number of slightly out-of-distribution examples compared to a smaller number of in-\ndistribution examples. Indeed, downsampling NYU dataset to the same size as MIMIC would\nlead to similar results for Llama-3.2-1B (Figure M11b). Appendix M extends this analysis to\nMIMIC mortality and LOS with similar observations.\n3 Discussion\nWe present what is, to our knowledge, the first comprehensive study of LLMs for hospital and\nhealth-system operations. We detail a full-cycle approach, in which we build, benchmark and\ntest the robustness of these models as part of the their operational deployment within the NYU\nLangone Health System. Our work has implications for the broader debate between generalist\nand specialist models, LLM generalizability, and pretraining-finetuning dynamics.\nWhy Operational Tasks Matter.\nMuch of the current excitement in medical AI centers\non diagnostic reasoning [12, 29, 35, 43–45]. These are valuable directions, but they do not\nfully capture the day-to-day challenges physicians face. Healthcare is as much operational as\nit is clinical. Physicians spend only 26% of their time in direct patient care, with much of the\nremainder devoted to documentation, insurance, and scheduling [1]. Operational outcomes\nsuch as readmission, insurance denial, and length of stay directly shape costs, capacity, and\ncontinuity of care. Predicting them is actionable, and improving them is measurable. By\nadvancing operational tasks alongside diagnostic ones, we can reduce costs, improve care\ndelivery, and make healthcare more accessible. Notably, we find that many of these tasks\nrequire specialized finetuning, and are not out-of-the-box accessible to generalist models,\nlikely reflecting their poor representation in web-scale datasets.\nOn The Need for Real-World Evaluation.\nOur results show the limitations of relying\non proxy benchmarks as evidence of readiness for clinical deployment. Several MedQA\nleaderboard models under-perform on ReMedE, suggesting that proxy benchmark success\ndoes not establish clinical utility. Evaluating models directly on real-world, task-specific\noutcomes is therefore essential, not only to identify models that genuinely improve hospital\noperations, but also to avoid overestimating the safety or value of systems based solely on\nproxy measures [17, 33, 34].\nThe Costs of Training.\nA common concern about specialized models is whether they can\nbe trained affordably outside of industrial labs. Our experience with Lang1 suggests that\nthe answer is yes. Training the 1B-parameter model on 314.5B tokens (150k steps) required\nroughly 30 days on 64 H100s, which at AWS p5.48xlarge pricing corresponds to a cost\nof about $180,000. While non insignificant, this figure is orders of magnitude lower than\nthe multimillion-dollar budgets required for frontier generalist models [5, 6, 46, 47], yet\n9\n"}, {"page": 10, "text": "delivers substantial performance gains on real-world hospital tasks. For a large health system,\nsuch costs are comparable to routine IT infrastructure upgrades and therefore financially and\noperationally feasible and profitable. This supports recent position papers [48, 49] that argue\nsmall, specialized models are the future of agentic AI because they can be more reliable,\neconomical, and aligned with domain-specific needs.\nIn-House Models.\nFrom an operational standpoint, training and deploying models in-house\ncan make more sense than relying exclusively on generalist models delivered via commercial\nAPIs. In this paradigm, hospitals not only reduce long-term costs and safeguard patient data,\nbut also retain the ability to adapt models as documentation practices, coding standards, and\npatient populations shift over time, a need highlighted by our temporal robustness results. In\ncontrast, the dependence on external APIs can introduce ongoing expenses, privacy risks, and\nlimited control over model behavior. Our findings suggest that even modestly sized models,\ntrained locally, can outperform larger off-the-shelf alternatives, reframing clinical AI from\n“renting intelligence\" to “building institutional assets\" that evolve with the health system itself.\nPretraining Is Not All You Need.\nWhile the reading comprehension capabilities emerge\ndirectly from large-scale pretraining, our finding provides evidence that high-stake objective\npredictions represent a different class of problem. Strong performance on ReMedE tasks\nrequire explicit finetuning and does not emerge from pretraining alone, even with domain-\nspecific data. This reliance on finetuning is shared by general-purpose chatbots, but the\nunderlying tasks are fundamentally different. Chatbot finetuning aligns emergent generalist\nskills to subjective, preference-based goals [50]. In contrast, ReMedE tasks require finetuning\nto build a new, non-emergent predictive skill against an objective, grouth-truth target. We\nhypothesize that this is because clinical predictions rely on complex relationships not well\ncaptured by a next-token-prediction objective. Our experiments show that these tasks demand\ndomain-specific supervision built on top of in-domain pretraining.\nWhy Transfer Matters.\nHealthcare tasks often suffer from limited labels due to the expertise\nrequired for annotation, and in some cases it is practically impossible to obtain large labeled\ndatasets (e.g., for rare conditions). Our experiments show that instruction finetuning enables\ntransfer across related tasks, where supervision on one outcome (e.g., readmission) improves\nperformance on others (e.g., mortality, length of stay). We also showed that finetuning Lang1\non NYU can transfer to a different hospital. This capability is especially valuable in clinical\nsettings because it reduces dependence on costly annotation pipelines and makes it possible\nto tackle tasks where labels are scarce or unattainable, effectively maximizing the value of\nindividual annotations.\n4 Conclusions\nWe introduced Lang1, a family of domain-specialized models trained on EHR and web\ntext. To evaluate Lang1, we developed ReMedE, an operations-grounded benchmark for\nevaluating language models across five operationally significant tasks. We find that large\ngeneralist models underperform Lang1-1B by 1.66% - 23.66% AUROC, despite their strong\nperformance on general and proxy medical benchmarks. Lang1-1B also outperformed other\n10\n"}, {"page": 11, "text": "finetuned models up to 70× larger by 3.64%–6.75%. These results demonstrate that success\non proxy benchmarks does not guarantee effectiveness in deployment-critical settings.\nOur analysis shows that clinical prediction capabilities do not arise from pretraining alone;\nexplicit finetuning is necessary, though in-domain pretraining improves data efficiency. Lang1\nalso demonstrates promising transfer across related tasks and health systems, suggesting that\ncarefully designed instruction finetuning can reduce dependency on expensive annotation.\nTraining costs for Lang1 are substantially smaller than frontier models and affordable for\nlarge health systems, making specialized models both effective and practical.\nFuture works include expanding ReMedE to additional institutions, more diverse patient\npopulations, and task types beyond classification outcomes. This is important for establishing\nshared standards for evaluating models in real operational contexts.\nOur findings challenge the assumption that ever-larger internet-trained models will gener-\nalize to all domains and instead point to a more hopeful direction for clinical AI. In healthcare,\nwhere patient safety is paramount, progress must be deliberate and grounded in real oper-\national outcomes. Benchmarks like ReMedE and specialized models such as Lang1 show\nthat smaller, domain-specific systems can be accurate, affordable, and adaptable, offering a\nscalable path forward that directly supports hospital operations and improves patient care.\n11\n"}, {"page": 12, "text": "5 Methods\n5.1 Data Collection\nData are extracted via structured query language (SQL) scripts to query the NYU Langone\nEHR, prototyped in an interactive web-based editor (Cloudera Hue), and exported as comma-\nseparated files (CSVs) to an on-prem high-performance computing (HPC) cluster.\nPreprocessing.\nRaw CSV notes (including pathology, radiology, and general hospital notes)\nare loaded with standard ASCII-encoding using Python Dask [51] for distributed processing.\nWe concatenate narrative fields, standardize punctuation, spacing and formatting via regular\nexpression substitutions, remove non-ASCII and malformed characters, remove errant whites-\npace and newlines, and filter out short notes (less than 10 words or with placeholder values\nsuch as <NA>).\n5.2 Datasets\n5.2.1 Pretraining Dataset\nWeb texts.\nWe use SlimPajama (627B tokens) [52], a large extensively deduplicated, multi-\ncorpora, open-source dataset for training LLMs. Its sources include Commoncrawl [53], C4\n[54], Github, Books [55, 56], Arxiv, Wikipedia and StackExchange.\nNYU Notes.\nThis dataset is previously created using unlabeled inpatient hospital notes\nsigned by medical professionals from the NYU Langone Health EHR1 for patient encounters\nstarting from January 2011 to May 2020. NYU Notes contains 387,144 patients, 7,247,694\nnotes, and 4,112,249,482 words in total. NYU Notes are used to train and evaluate NYUTron.\n[42]\nNYU Notes+.\nThis dataset builds on NYU Notes by including a wider range of note types\nand covering a longer time span, resulting in a total word count 14.5 times greater than that of\nNYU Notes. NYU Notes+ contains unlabeled hospital notes, pathology note and radiology\nnotes from NYU Langone Health EHR from 2003 to 2023. It comprises 11,689,342 patients,\n180,487,092 notes, and 59,917,646,788 words.\n5.2.2 Finetuning Datasets and ReMedE Test Set\nWe derive five task-specific labeled datasets by combining NYUTron [42] finetune datasets,\nwith the addition of 2024 temporal test set to approximate deployment robustness. The 2024\ntemporal tests set are used for ReMedE. See Appendix A for a visualization of the data\nsplit timeline and Appendix B for detailed dataset statistics. Appendix L shows that a small\npercentage of patient overlap does not over-estimate model performance on readmission. For\nboth zero-shot evaluation and finetuning (section 5.4), the dataset were converted to multiple\nchoice format (Appendix H).\n1This study is approved by the Institutional Review Board (IRB) at NYU Langone Health. The methods are carried out in accordance\nwith the IRB’s relevant guidelines and regulations.\n12\n"}, {"page": 13, "text": "NYU+ Readmission.\nReadmission occurs when a patient returns to the hospital shortly\nafter discharge. Predicting readmissions is critical for identifying patients who need longer stay\nor post-discharge support, and it also serves as a key hospital quality metric. This finetuning\ndataset contains discharge notes with 30-day all-cause readmission labels. The notes comprise\nof a subset of NYU+ Notes whose encounters end between January 2013 and November\n2021, and additional discharge notes from 2024 for the temporal test. Rehabilitation, dialysis\nand palliative care notes are excluded to focus on modeling acute readmission. A positive\nlabel is assigned if the patient is readmitted within 30 days of discharge, and a negative label\notherwise. We split the dataset into five sets. The first three sets are train, validation, and test\nset with a 8:1:1 ratio from 2013 to May 2021. The 2021 temporal test includes notes from\nJune to December 2021. The 2024 temporal test set includes notes from 2024. The positive\nclass ratio range from 10.81% to 11.29% across these five sets. The dataset contains 421,429\npatients, 604,326 notes and 607,877,177 words in total.\nNYU+ In-Hospital Mortality.\nIn-hospital mortality prediction identifies patients at highest\nrisk of death during their admission, enabling timely palliative care consultations and goals-of-\ncare discussions that align treatment with patient prognosis. This finetuning dataset contains\nHistory and physical (H&P) notes with in-hospital mortality labels. The notes comprise of\na subset of NYU+ Notes for encounters ending between January 2013 and November 2021,\nwith additional H&P notes from 2024 for the temporal test. A positive label is assigned is\nthe discharge disposition is “Expired\", and a negative label otherwise. We split the dataset\ninto five sets. The first three sets are train, validation, and test set with a 8:1:1 ratio from\n2013 to May 2021. The 2021 temporal test includes notes from June to December 2021. The\n2024 temporal test set includes notes from 2024. The positive class ratio range from 1.78%\nto 1.93% across these five sets. In total, the dataset contains 395,991 patients, 566,748 notes,\nand 608,603,182 words.\nNYU+ Length of Stay (LOS).\nLOS is the number of days a patient remains hospitalized.\nPredicting LOS is essential for bed management, staffing allocation, and discharge planning.\nThis finetuning dataset contains H&P notes with label for binned length of stay. The dataset\ncomprises of a subset of NYU+ Notes for encounters ending between January 2013 and\nNovember 2021, with additional H&P notes from 2024 for the temporal test. We assign the\nlabels based on quantile. We assigned label 0 for an LOS below the 25% quantile (0 to 2 days),\nlabel 1 for an LOS between the 25% and 50% quantile (3 days), label 2 for an LOS between\nthe 50% and 75% quantile (4 to 5 days) and label 3 for an LOS above the 75% quantile (> 5\ndays). We split the dataset into five sets. The first three sets were train, validation, and test set\nwith a 8:1:1 ratio from 2013 to May 2021. The 2021 temporal test includes notes from June\nto December 2021. The 2024 temporal test set includes notes from 2024. The majority class\nratio (0 to 2 days) range from 41.49% to 45.64% across these five sets. The minority class\nratio (more than 5 days) range from 23.92% to 26.34%. In total, the dataset contains 395,991\npatients, 566,748 notes and 608,603,182 words.\nNYU+ Insurance Denial.\nInsurance denials occur when payers reject claims for hospi-\ntal services. Predicting denials allows hospitals to proactively address documentation gaps,\nreducing administrative burden and preventing unexpected out-of-pocket costs for patients.\n13\n"}, {"page": 14, "text": "This finetuning dataset contains H&P notes with insurance denial label. The notes comprise\nof a subset of NYU+ Notes whose encounters ends between May 1, 2021 and April 30,\n2022, with additional H&P notes from Janurary 2024 for the temporal test. A positive label\nis assigned if claim status is “final, adverse determination\" (initial rejection by insurance and\nagain rejected following appeal) or “final, favorable determination\" (initial rejection by insur-\nance and approved following appeal), an negative label otherwise. We split the dataset into\nfive sets. The first three sets are train, validation, and test set with a 8:1:1 ratio from May\n2021-Feb 2022. The 2022 temporal test include notes from March-Apr 2022. The 2024 tem-\nporal test set includes notes from January 2024. The positive class ratio range from 12.01%\nto 13.90%. The dataset contains 87,974 patients, 97,837 notes, and 89,147,715 words.\nNYU+ Charlson Comorbidity Index (CCI).\nCCI is a standard score used to quantify a\npatient’s chronic illness based on their medical history [57]. It is useful for supporting accurate\nrisk stratification for patients with limited historical data. However, this heuristic score cannot\nbe computed when a patient’s medical history is unknown. This dataset addresses this problem\nby providing History & Physical (H&P) notes paired with their corresponding binned CCI\nscores. It allows models to be trained to impute the CCI score directly from unstructured\nclinical text. The target CCI is calculated using ICD code associated with the encounter in\nthe EHR following the scoring function in [58]. Encounters with no associated ICD codes are\nexcluded. The CCI is discretized into five classes: comorbidity index of 0 (<50th percentile),\ncomorbidity index of 1 −2 (50 −75% percentile), comorbidity index of 3 −4 (75-90th\npercentile), and comorbidity index of 5 −7 (90-99th percentile), and comorbidity index >7\n(>99th percentile). We split the dataset into five sets. The first three sets are train, validation,\nand test set with a 8:1:1 ratio from 2013 to May 2021. The 2021 temporal test includes notes\nfrom June to December 2021. The 2024 temporal test set includes notes from 2024. The\nmajority class (score 0) ratio range from 68.40% to 69.47%. The minority class (score more\nthan 7) ratio range from 0.059% to 0.17%. In total, the dataset has 306,741 patients, 443,915\nnotes, and 524,739,038 words.\n5.2.3 External Validation Datasets\nWe also create external validation datasets from MIMIC III [59], which is sourced from Beth\nIsrael hospital in Boston Massachusetts.\nMIMIC III Readmission.\nThe labeled dataset has 6% positive labels, with 52,725 examples\nand a 70% train, 15% validation and 15% test split. More dataset construction details are in\n[60]’s Appendix A.\nMIMIC III Mortality.\nThe labeled dataset has 10.55% positive labels, with 5658 examples\nand 80% train, 10% validation and 10% test split. The dataset is constructed from MIMIC-\nIII clinical notes. As no explicit \"Admission Note\" label exists, we identify notes by filtering\ndescriptions for \"admission\" while excluding \"readmission\" (≈19K notes). To prevent patient\nbias, we select a single note per hospital stay using a prioritization heuristic. The heuristic first\nprioritizes Physician Note > General Note > Nurse Note, then prioritizes \"resident/attending\"\n> \"resident\" > \"attending\" > \"fellow\" > \"h&p\". Finally the heuristic prefers longer notes. This\n14\n"}, {"page": 15, "text": "filters down to ≈6K notes. We further refine the dataset by removing notes written > 120\nhours after admission or associated with a negative length of stay. The final dataset contains\n5,658 unique admission notes.\nMIMIC III LOS.\nThe labeled dataset uses the same 5,658 admission notes as the mortality\ntask, with a mean LOS of 7.96 days. Similarly, the split is 80% train, 10% validation and 10%\ntest. The continuous LOS values are discretized into bins. We adapt the NYU+ LOS scheme\n(Methods 5.2.2) to handle MIMIC-III’s continuous range by treating the original integer bins\nas upper bounds. For example, the \"3 days\" bin is modified to capture the continuous range\n2 < LOS ≤3.\n5.2.4 Comprehension Datasets\nWe evaluate the performance of Lang1 checkpoints on comprehension datasets to analyze the\nemergence of its nonclinical abilities.\nSciQ [61].\nThe dataset contains 13.7K multiple choice science exam questions with con-\ntexts. An example question is Mesophiles grow best in moderate temperature,\ntypically between 25◦C and 40◦C (77◦F and 104◦F). Mesophiles are\noften found living in or on the bodies of humans or other animals.\nThe optimal growth temperature of many pathogenic mesophiles is 37◦C\n(98◦F), the normal human body temperature. Mesophilic organisms have\nimportant uses in food preparation, including cheese, yogurt, beer\nand wine. \\n Question: What type of organism is commonly used in\npreparation of foods such as cheese and yogurt? \\n Answer:\nPubmedQA [62].\nThe dataset contains 1k expert-annotated biomedical question answer-\ning dataset collected from PubMed abstracts. An example question is Abstract:\nComplex regional pain syndrome type I is treated symptomatically ...\nEarly cast-related complaints predicted the development of complex\nregional pain syndrome (relative risk, 5.35; 95% confidence interval,\n2.13 to 13.42)\\n Question: Can vitamin C prevent complex regional pain\nsyndrome in patients with wrist fractures?\\n Answer:\n5.3 Pretraining Lang1\nWe pretrain a family of Llama-style decoders (Lang1-100M, Lang1-1B, Lang1-7B) on a\nmixture of web texts and NYU Notes+ (section 5.2.1) using next token prediction (Figure 1 a,b).\nDetailed demographic statisitics is in Appendix G. Unless otherwise noted, Lang1 models\nare trained with equal sampling from both clinical and general sources, which is supported by\nour pretraining ablations (Appendix D). For tokenization, we use the Llama-2-7B tokenizer\n(SentencePiece, 32k vocabulary). The 100M-parameter model follows the Smol-Llama-\n101M architecture with a 1024 context length; the 1B model follows TinyLlama-1.1B with\na 2048 context length; and the 7B model follows Llama-2-7B with a 4096 context length\nWe pretrain on 8 to 64 nVidia 80GB H100s with NVLink. We used the LitGPT [63] library\nand Fully Sharded Data Parallel [64].\n15\n"}, {"page": 16, "text": "We run a few trials of manual hyperparameter search based on speed, performance and\ntraining stability. For all models we use AdamW optimizer with linear warmup (2000 steps),\nbeta1 of 0.9, beta2 of 0.95, eps of 1e-8 and cosine cycle decay down to a minimum learning\nrate of 4e-5. We use a seed of 3407, weight decay of 0.1, and gradient clipping of 1. In terms\nof FSDP sharding, we shard gradient and optimizer for models up to 1B, and full sharding for\n7B model. For effective batch size, we use 4096 for 100M model for controlled comparison\nwith NYUTron [42], and 1024 for 1B and 7B models.\nWe implement a monitoring pipeline that automatically triggered few-shot evaluations and\ngenerations at fixed intervals of pretraining steps. Slack Weights & Biases (W&B) alerts are\nconfigured to report loss spikes. Upon detection of anomalies in loss or output, we revert to\nthe most recent stable checkpoint. Validation loss is computed periodically on the held-out\n0.1% validation split and used to determine early stopping and model checkpoint selection.\n5.3.1 Pretrained Models\nWe pretrain the following variants of Lang1 models (Table 1). Ablations (Appendix D)\nshow that larger models trained on more clinical data has better performance, and mixing\nin web texts does not hurt much. When we refer to Lang1 without specifying data sources,\nwe mean the one trained with NYUNotes+ and WebTexts. For instance, Lang1-1B refers to\nlang1-1B-NYUNotes+,WebTexts.\nTable 1: Pretrained Model Specifications\nModel Name\nModel Size\nPretrain Data\nlang1-100m-NYUNotes\n100m\nNYUNotes\nlang1-100m-NYUNotes+\n100m\nNYUNotes+\nlang1-100m-NYUNotes+,WebTexts\n100m\nNYUNotes+,WebTexts\nlang1-1B-NYUNotes\n1B\nNYUNotes\nlang1-1B-NYUNotes+\n1B\nNYUNotes+\nlang1-1B-NYUNotes+,WebTexts\n1B\nNYUNotes+,WebTexts\nLang1-7B-NYUNotes+,WebTexts\n7B\nNYUNotes+,WebTexts\n5.4 Finetuning\nWe finetune Lang1 models (and their trajectories of checkpoints) and other pretrained models\n(Table 2) on ReMedE tasks using multiple choice format (Figure 1 panel c). The labeled\nclinical notes are converted to multiple choice format (Appendix H), and we train the model\nto predict the correct multiple choice option. For fair comparison with NYUTron, we right\ntruncate all clinical notes to a maximum of 512 tokens. All finetuning jobs are done one node\nof 8 nVidia 80GB H100s with NVLink.\nBefore each full finetuning, we run 5 hyperparameter search trials up to 100 steps using\nHydra and Optuna. We random search learning rate (based on [65]’s recommendation) in log\nscale over the closed interval 1e-6 to 1e-3. We used an AdamW optimizer with beta1 of 0.9,\nbeta2 of 0.999, eps of 1e-5. We used a weight decay of 0.02 and no gradient clipping. We\nused a cosine annealing scheduler with no warmup and a max steps of 5120. The best trial is\nselected based on both maximum validation AUROC and minimum validation loss.\n16\n"}, {"page": 17, "text": "Table 2: Additional Model Specifications\nModel Name\nModel Size\nPretrain Data\nllama-3.2-1b\n1B\nUnnamed public mix (9T tokens)\nllama-2-7b\n7B\nUnnamed public mix (2T tokens)\nDeepSeek-R1-Distill-Llama-70B\n70B\nUnnamed public mix (2T tokens) + reasoning data (800k samples)\nFor full finetuning, we used the best learning rate from hyperparameter search, and train\nfor maximum 5120 steps with early stopping based on Micro-AUROC and a patience of 300\nsteps. The probabilities for calculating AUROC is obtained via normalizing the logits of the\nmultiple choice options (e.g., “A”). We train all parameters except for DeepSeek-R1-Distill-\nLlama-70B, which has to be finetuned using low-rank adaptation finetuning [66] to meet the\nmemory constraint (Appendix I).\nFor multitask finetuning, we mix examples from each task evenly within each training\nbatch. We increase the total training steps scaled by the number of tasks.\n5.5 Evaluation\nPretraining evaluation.\nWe monitored the token-level cross entropy loss and perplexity for\nboth training and validation.\nZero-shot and few-shot evaluation.\nReMedE is built on LM Eval Harness [67]. We imple-\nmented the tasks as multiple choice questions and AUROC score as a metric. We implemented\na child class of LocalCompletionsAPI to connect on-prem models. For models whose log-\nits are not accessible (e.g., on-prem GPT-4o), we implemented custom sampling function to\napproximate (Appendix J) the probability via counting choices from 10 generations using a\ntemperature of 1.\nFinetuning evaluation.\nWe collected the logits of the multiple choice options, normalized it\nas probabilities, and calculated AUROC using sklearn’s implementation (same as ReMedE’s\nAUROC backend for consistency). For multiclass classification, we used One-Versus-Rest\n(OVR) AUROC.\nUncertainty.\nTo capture the uncertainty arose from the randomness of our test set, we\ncalculated 95% confidence intervals (CI = ± 1.96 x SD) by resampling each test set 1000\ntimes using the quantile bootstrap method from scipy.\nTemporal Shift.\nTo better mimic deployment conditions under temporal distribution shift,\nall AUROCs are reported on test data from 2024 – drawn from a period after the pretraining\ndata – unless otherwise noted. See Appendix A for a visualization of the test timeline.\nGeneralist\nModels.\nWe\ncompared\nagainst\ngeneralist\nfrontier\nmodels,\nincluding\nDeepSeek R1 (served via vLLM [68]), DeepSeek R1 Distilled Llama 70B (vLLM),\nLlama 3.3 70B Chat (vLLM), and on-premises GPT-4o (Azure-hosted service). We also\n17\n"}, {"page": 18, "text": "evaluated MedQA leaderboard models using in-memory inference, such as Llama 3.2 1B,\nLlama 2 7B, and MedMobile.\nAcknowledgements.\nE.K.O. is supported by the National Cancer Institute’s Early Surgeon\nScientist Program (3P30CA016087-41S1) and the W.M. Keck Foundation. L.Y.J. is supported\nby Apple AIML PhD fellowship. L.Y.J. and A.C. are supported by NSF Award 1922658.\nK.C., E.K.O., L.Y.J. and A.C. are supported by Institute for Information & communications\nTechnology Promotion (IITP) grant funded by the Korea government (MSIT) (No. RS-2019-\nII190075 Artificial Intelligence Graduate School Program (KAIST); No. RS-2024-00509279,\nGlobal AI Frontier Lab). We would like to acknowledge J. Golfinos, whose vision and support\nmade this project possible. We would like to acknowledge Michael Costantino, Ph.D., Ali\nSiavosh-Haghighi, Ph.D., Kevin Yie, M.S., Neelima Sharma, Tedum Sampson from the NYU\nLangone High Performance Computing (HPC) team. Without their tireless assistance in\nbuilding and maintaining our GPU cluster none of this research would have been possible.\nWe would also like to thank Dr. Dafna Bar-Sagi,Ph.D., and Nader Mherabi whose support\nfor this research has made everything possible. Thanks to He He, Ph.D., Eunsol Choi, Ph.D.,\nCarlos Fernandez-Granda, Ph.D., Julia Kempe, Ph.D., Vasant Dhar, Ph.D., Keunwoo Choi,\nPh.D., Jesse Swanson, Gavin Zihao Yang, William Merrill, Ph.D., Nicholas Lourie, Sophie\nHao, Ph.D., Vishakh Padmakumar, Ph.D., Michael Hu, Robert J Steele, Yueying Li, Yunzhen\nFeng, Guillermo Sapiro, Ph.D., Oussama Elaqchar, Kai Xu, Varun Yerram, Itay Itzhak, Jeff\nHammerbacher for their valuable discussions.\nDeclarations\nEthical approval.\nThis study was approved by the Institutional Review Board (IRB) at NYU\nLangone Health (study protocol s21-01189). The methods were carried out in accordance\nwith the IRB’s relevant guidelines and regulations.\nData Availability.\nThe clinical data used for the pretraining, finetuning, validation, and test\nsets were collected from the NYU Langone Health System EHR maintained by the NYULH\nDatacore team. Text data was stripped of rich text features and directly included in the dataset\n\"as-is\", and was augmented with structured features where noted. It consists of the production\nmedical records of NYU Langone and cannot be made publicly available. For the external\nvalidation task, the datasets were obtained from MIMIC III, and are publicly available from\ntheir website.\nCode Availability.\nThis work uses several open-source libraries including PyTorch, LitGPT,\nTransformers library, LM Eval Harness, and hydra. Our experimental framework involves the\nutilization of these libraries and in some cases modification of them. We will release code to\nreplicate the pretraining, finetuning, and testing of the models described in this paper at the\ntime of publication. We include detailed methods and implementation steps in the Methods\nand Supplementary Information to allow for independent replication.\nAuthor’s contribution.\nE.K.O. and K.C. supervised the project. L.Y.J and X.H. collected\npretrain, finetune and evaluation data (except NYU+ Insurance Denial and MIMIC-III). L.Y.J.\nand A.C. engineered the software for pretrain and finetune. A.C., X.H., L.Y.J. and J.Z. engi-\nneered the software for evaluation. L.Y.J., A.C., X.H., R.D., X.C.L. ran experiments. L.Y.J.,\nA.C., X.C.L., X.H., J.Z., R.D. and K.C. debugged and tested the software. A.C., L.Y.J., K.C.,\n18\n"}, {"page": 19, "text": "X.C.L., R.S., A.A., K.L.S, Y.C., and Q.P. created figures. A.C., L.Y.J., K.C., E.K.O. concep-\ntualized the training dynamics and transfer experiments. J.S. hosted deepseek and llama 70b\ninference server. K.E. collected NYU+ Insurance Denial data. Q.P. and L.Y.J. check pretrain\ndata quality and cleaned the data. F.W. processed the MIMIC-III Mortality and LOS data.\nA.C., L.Y.J, R.D., Y.C., D.A. and J.V.L. reviewed related literature. K.C., E.K.O., Y.A. pro-\nvided guidance and feedback throughout the project. L.Y.J., A.C., X.H., R.D., A.A., D.A.,\nJ.V.L., Q.P., Y.C., R.J.S., K.C., E.K.O. wrote the initial draft. All authors edited and revised\nthe manuscript.\nConflict of interest E.K.O. reports consulting with March AI, Sofinnova Inc., Google\nInc., income from Merck & Co., and Mirati Therapeutics, and equity in Artisight Inc. K.C. is\nemployed by Prescient Design, a subsidiary of Roche. A.C. is employed by Google Deepmind.\nQ.P. is employed by Faction Imaging Inc. J.S. is employed by March AI. There are no other\npotential conflicts of interest. The work presented herein was performed exclusively within\nthe NYU Langone Health System.\n19\n"}, {"page": 20, "text": "Appendix A\nData Timeline\nLang1’s pretrain data covers a wider time window than NYUTron[42].\nNYUTron’s\npretrain data is from 2013 to May 2021. Lang1’s pretrain data covers a longer span, from\n2003 to 2023. Overall, Lang1’s pretrain corpus is more than 10 times the size of NYUTron.\nLang1 uses the same finetuning dataset for training as NYUTron, but adds additional\ntemporal test.\nFor both NYUTron and Lang1 finetuning, we have a temporal test set to\nbetter mimic the deployment scenario, where the test set comes from the future of training\nset. For NYUTron, the temporal test set is between June to December of 2021. For Lang1,\nthe temporal test set is in 2024. All performance we report in main is performance on 2024\ntemporal test set, unless otherwise specified.\nNYUTron Pre-training\nLang1 Pre-training\n2003\nFinetune Temporal Test \n2024\nJan 2024 \nDec 2024 \nFinetune \nTrain/Val/Test\nMay 2021\nFinetune Temporal Test  \nJun-Dec 2021\nDec 2021\n2013\nFig. A1: Illustration of timeline of pretrain and finetune dataset for both Lang1 and NYUTron.\nLang1 covers a wider time window for pretraining and added additional temporal test set in\n2024 to capture temporal distribution shift.\nTemporal test is important and difficult.\nFigure A2 shows that both Lang1-1B (purple)\nand NYUTron (pink) perform worse as test data are sampled from a further future, illustrating\nthe importance of evaluating with temporal test to capture deployment scenario.\nJan 2013 -\nMay 2021\nJune 2021 -\nDec 2021\n2024\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nReadmission ROC AUC\nModel Performance Over Time\nLang1-1B\nNYUTron\nFig. A2: Models perform worse on temporal test and exhibit different level of degradation in\nface of temporal shift.\n20\n"}, {"page": 21, "text": "Appendix B\nReMedE Dataset Statistics\nThe following tables show the note counts (Table B1) and patient counts (Table B2) across\neach split, and class ratio for each task: readmission (Table B3), mortality (Table B4), length\nof stay (Table B5, comorbidity imputation (Table B6, and Insurance denial (Table B7).\nTable B1: Distinct note counts for each ReMedE task across five splits.\nTask\nTrain\nVal\nTest\nTemporal Test 2021\nTemporal Test 2024\nTotal\nReadmission\n362,259\n45,282\n45,283\n53,916\n97,586\n604,326\nCCI\n256,676\n32,085\n32,085\n42,137\n80,932\n443,915\nLength of Stay\n334,515\n41,814\n41,815\n51,018\n97,586\n566,748\nInsurance Denial\n41,842\n2,325\n2,325\n9,299\n42,046\n97,837\nMortality\n334,515\n41,814\n41,815\n51,018\n97,586\n566,748\nTable B2: Distinct patient counts for each ReMedE task across five splits.\nTask\nTrain\nVal\nTest\nTemporal Test 2021\nTemporal Test 2024\nTotal\nReadmission\n269,140\n42,692\n42,603\n46,003\n78,453\n421,429\nCCI\n188,298\n30,085\n30,098\n251,804\n64,873\n306,741\nLength of Stay\n248,486\n39,304\n39,331\n43,358\n78,453\n395,991\nInsurance Denial\n39,422\n2,319\n2,313\n9,037\n37,821\n87,974\nMortality\n248,674\n39,317\n39,357\n43,358\n78,453\n395,991\nTable B3: Readmission label ratios by split (Total notes =\n604,326; total words = 607,877,177).\nSplit\nNot Readmitted\nReadmitted within 30 days\nTrain\n0.891495\n0.108505\nVal\n0.891944\n0.108056\nTest\n0.893293\n0.106707\nTemporal Test (2021)\n0.888456\n0.111544\nNew Temporal (2024)\n0.887115\n0.112885\n21\n"}, {"page": 22, "text": "Table B4: In-hospital mortality label\nratios by split (Total notes = 566,748; total\nwords = 608,603,182).\nSplit\nSurvived\nDied\nTrain\n0.981161\n0.018839\nVal\n0.981250\n0.018750\nTest\n0.980916\n0.019084\nTemporal Test (2021)\n0.980693\n0.019307\nNew Temporal (2024)\n0.982241\n0.017759\nTable B5: Length of stay label ratios by split (Total notes =\n566,748; total words = 608,603,182).\nSplit\n0–2 days\n3 days\n4–5 days\n>5 days\nTrain\n0.417306\n0.176575\n0.165888\n0.240231\nVal\n0.415722\n0.180562\n0.164490\n0.239226\nTest\n0.414851\n0.176611\n0.167452\n0.241086\nTemporal Test (2021)\n0.418597\n0.153201\n0.164805\n0.263397\nNew Temporal (2024)\n0.456418\n0.144529\n0.159757\n0.239297\nTable B6: Charlson Comorbidity Index (CCI) label ratios by split (Total\nnotes = 443,915; total words = 524,739,038).\nSplit\nScore 0\nScore 1–2\nScore 3–4\nScore 5–7\nScore >7\nTrain\n0.694681\n0.224840\n0.054251\n0.025324\n0.000904\nVal\n0.689169\n0.229547\n0.055166\n0.025526\n0.000592\nTest\n0.693502\n0.226866\n0.053327\n0.025370\n0.000935\nTemporal Test (2021)\n0.685811\n0.228327\n0.059164\n0.025821\n0.000878\nNew Temporal (2024)\n0.684043\n0.217355\n0.067513\n0.029370\n0.001717\nTable B7: Insurance denial label ratios by\nsplit (Total notes = 97,837; total words =\n89,147,715).\nSplit\nApproved (0)\nDenied (1)\nTrain\n0.877850\n0.122150\nVal\n0.867097\n0.132903\nTest\n0.873978\n0.126022\nTemporal Test (2021)\n0.879880\n0.120120\nNew Temporal (2024)\n0.861033\n0.138967\n22\n"}, {"page": 23, "text": "Appendix C\nTransfer Pattern of Llama 3.2 1b v.s. Lang1 1B\n \n \n \n \n \nTask-specific ROC AUCs\n             \n \n \n \n \n \n             \nFinetuning Dataset(s)\n48%\n56%\n55%\n54%\n43%\n77%\n84%\n64%\n77%\n57%\n70%\n96%\n64%\n80%\n58%\n63%\n47%\n77%\n65%\n58%\n69%\n78%\n63%\n90%\n61%\n50%\n30%\n52%\n51%\n87%\n76%\n95%\n76%\n90%\n88%\nNo Finetuning\n, \n, \n, \n, \n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nReadmission\nMortality\nLOS\nCCI\nInsurance\nDenial\nReadmission Mortality\nLOS\nCCI\nInsurance\nDenial\n(a) Lang1-1B’s transfer heatmap.\n \n \n \n \n \nTask-specific ROC AUCs\n             \n \n \n \n \n \n             \nFinetuning Dataset(s)\n56%\n62%\n49%\n52%\n38%\n72%\n83%\n62%\n71%\n71%\n69%\n95%\n63%\n78%\n61%\n56%\n48%\n73%\n60%\n53%\n68%\n77%\n62%\n88%\n69%\n52%\n35%\n54%\n51%\n85%\n67%\n79%\n64%\n75%\n76%\nNo Finetuning\n, \n, \n, \n, \n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nReadmission\nMortality\nLOS\nCCI\nInsurance\nDenial\nReadmission Mortality\nLOS\nCCI\nInsurance\nDenial\n(b) Llama 3.2 1B’s transfer heatmap.\nFig. C3: Lang1-1B and Llama 3.2 1B transfers differently. The heatmap shows the two\ndifferent base models’ performance when finetuned on a subset of ReMedE tasks (y axis) and\nevaluated on all five ReMedE tasks (x axis). Overall, Lang1-1B has higher per-task and joint\nperformance, and shows a different transfer pattern than Llama 3.2 1B.\nLlama-3.2-1B has overall worse performance than Lang1-1B.\nCompared to Figure C3a,\nFigure C3b has worse single-task (diagonal) and joint-task (last row) performance, suggesting\nthat the specific transfer pattern could be highly model specific.\nBoth models exhibit some similar patterns.\n(i) finetuning on readmission (second row)\nboosts performance on the other four tasks, and (ii) transfer could be asymmetric, i.e., mortality\nhelps LOS (third row, third column) but LOS does not help mortality (fourth row, second\ncolumn), which can be explained using domain knowledge (Appendix F).\nLang1-1B and Llama-3.2-1B also have different patterns.\n(i) joint finetuning (last row)\nhelps Lang1-1B but hurts Llama-3.2-1B, and (ii) finetuning on insurance denial (fourth row)\nlowers Lang1-1B’s LOS performance (third column) while improving it for Llama-3.2-1B.\nThese results suggest that instruction finetuning enables cross-task transfer, though the\nspecific transfer patterns depend on model pretraining.\n23\n"}, {"page": 24, "text": "Appendix D\nPretraining ablations\nFigure D4 presents pretraining ablation results evaluated on 2024 readmission temporal test\nset. For pretraining, we control for the model architecture (encoder v.s. decoder), model size\n(100M v.s. 1B), and pretrain data (NYUNotes, NYUNotes+, NYUNotes+ and web texts). For\nfinetuning, we evaluated on three clinical predictive tasks: readmission prediction, insurance\ndenial prediction, and LOS prediction. These three tasks were chosen for their distinct transfer\npattern in Figure C3a.\nTraining larger models on more recent clinical data improves temporal robustness.\nFigure D4a shows the ablation results for readmission prediction. On the left, holding the\nmodel architecture fixed as a decoder, we vary pretraining data. Compared to models trained\nonly on EHR from 2013 to 2021 (DNYUNotes), adding more recent clinical data (DNYUNotes+,\nspanning 2003 to 2023) and further mixing in general-domain SlimPajama (DNYUNotes+,WebText)\nimproves performance for the 1B model but not the 100M model, suggesting that larger\nmodels are better able to leverage additional clinical data. This also justifies our choice of\nequally mixing EHR texts and web texts, since adding web texts does not significantly hurt\nthe downstream clinical task performance while instilling general-purpose knowledge. On the\nright, holding the pretraining data fixed to DNYUNotes, varying model architecture shows that\nscaling to 1B helps. We observe similar patterns for insurance denial prediction (Figure D4b)\nand LOS prediction (Figure D4c).\nDNYUNotes\nDNYUNotes+\nDNYUNotes+,WebText\nPretraining Dataset\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\nReadmission ROC AUC\nLang1-100m\nLang1-1B\nNYUTron-100m\n(encoder)\nLang1-100m\n(decoder)\nLang1-1B\n(decoder)\nModel Architecture\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\nReadmission ROC AUC\n(a) Readmission prediction\nDNYUNotes\nDNYUNotes+\nDNYUNotes+,WebText\nPretraining Dataset\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\nInsurance Denial ROC AUC\nLang1-100m\nLang1-1B\nNYUTron-100m\n(encoder)\nLang1-100m\n(decoder)\nLang1-1B\n(decoder)\nModel Architecture\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\nInsurance Denial ROC AUC\n(b) Insurance denial prediction\nDNYUNotes\nDNYUNotes+\nDNYUNotes+,WebText\nPretraining Dataset\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\nLOS (Length of Stay) ROC AUC\nLang1-100m\nLang1-1B\nNYUTron-100m\n(encoder)\nLang1-100m\n(decoder)\nLang1-1B\n(decoder)\nModel Architecture\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\nLOS (Length of Stay) ROC AUC\n(c) LOS prediction\nFig. D4: Pretraining ablations. Error bars indicating the 95% confidence interval. (a) Larger\nmodels trained on more clinical data has better performance, and mixing web texts does not\nhurt much. (b,c) Similar patterns are observed for insurance denial and LOS prediction.\n24\n"}, {"page": 25, "text": "Appendix E\nCalibration Plot\nCalibration curves are calculated using sklearn package [69] with n=15 bins. Expected\ncalibration error (ECE) is calculated with n=15 bins using the torchmetrics library [70].\n(a) Calibration plots for single-task models\n(b) Calibration plots for joint model\nFig. E5: Calibration plot shows that both single task finetuned Lang1 and joint finetuned\nLang1 are well calibrated.\n25\n"}, {"page": 26, "text": "Appendix F\nAsymmetry of Transfer between Mortality and\nLOS\nOur medical collaborators provided an explanation for the asymmetric transfer between Mor-\ntality and LOS (Length of Stay) we observe for both Lang1-1B and Llama-3.2-1B. If a\npatient died, they either stayed for a short time (very sick and died immediately) or a long time\n(doctors failed to save them after a long time). On the other hand, if a patient stayed for a long\ntime, they either survived, or they died after doctors’ attempts. This asymmetry in conditional\nprobability could help explain why the mortality transfer to LOS, but not vice versa.\nThis explanation is corroborated by analysis of the conditional probabilities. Figure F6a\nshows that patient who died are ore likely to stay for 0-2 days or >5 days compared to patient\nwho survived. F6b shows that patient who stay for a long time have similar mortality risks as\npatient who stayed for 0 days.\nDied\nSurvived\nIn-Hospital Mortality\n0.0\n0.1\n0.2\n0.3\n0.4\nProportion of Total\nDistribution of Length of Stay Given Mortality\nLOS bucket\n0-2\n3\n4-5\n>5\n(a) Probabilities of LOS given in-hospital mortal-\nity\n0\n10\n20\n30\n40\n50\n60\nLength of Stay\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nProbability of In-Hospital Mortality\nConditional Probability of In-Hospital Mortality Given Length of Stay\n(b) Probabilities of in-hospital mortality given\nLOS\nFig. F6: Analysis of conditional probabilities of LOS (length of stay) and in-hospital mortality\ncould help explain the assymetry of transfer.\n26\n"}, {"page": 27, "text": "Appendix G\nDetailed statistics of NYU Notes+\nWe analyzed the top five clinical departments, diagnosis and borough for pathology notes,\nradiology notes and hospital notes. See Figure G7.\n(a) Pathology Notes. Among pathology notes with specified departments, OB/GYN\n(obstetrics and gynecology) has the highest percentage (9.8%). The most common\nspecified diagnosis is gynecological exams (6%). Nearly half (48.6%) of the pathol-\nogy notes are from Manhattan borough.\n(b) Radiology Notes. Most of the radiology notes are from diagnostic radiology\n(82.2%), with screening mammograms being the most common specified diagnosis\n(5%). The two most common boroughs are Manhattan (41%) and Queens (32.6%).\n(c) Hospital Notes. Among notes with specified departments, most are from internal\nmedicine department (10.1%), with common diagnoses including sepsis (1.1%) and\nhypertension (1%). The majority of notes are from Brooklyn (27.9%) and Queens\n(24.7%) borough.\nFig. G7: Top five department, diagnosis and borough of NYU Notes+\n27\n"}, {"page": 28, "text": "Appendix H\nPrompts of ReMedE Tasks\nWe constructed prompts to create task-specific questions and answer options from the labeled\nfinetuning notes:\n• Readmission Question: Given the above discharge note of the patient, will the patient be\nreadmitted to the hospital within 30 days of discharge? \\n A. no \\n B. yes \\n Answer:\n• In-Hospital Mortality Question: Given the above admission note of the patient, will the\npatient die during the hospital admission? \\n A. no \\n B. yes \\n Answer:\n• Charlson Comorbidity Index Question: Given the above admission note of the patient,\nwhat’s the Charlson Comorbidity Index of the patient? \\n A. score 0 \\n B. score 1 to 2 \\n\nC. score 3 to 4 \\n D. score 4 to 7 \\n E. score more than 7 \\n Answer:\n• Insurance Denial Question: Given the above discharge note of the patient, will the insurance\nclaim of the patient be denied? \\n A. no \\n B. yes \\n Answer:\n• Length of Stay Question: Given the above admission note of the patient, how long will the\npatient stay at the hospital? \\n A. 0 to 2 days \\n B. 3 days \\n C. 4 to 5 days \\n D. more\nthan 5 days \\n Answer:\nAppendix I\nLoRa Finetuning for Llama-3-70b\nTo efficiently train Deepseek-R1-distill-Llama-3-70b on 1 node of 8 H100s, we used Low-Rank\nApproximation (LoRA) finetuning. LoRA reduces trainable parameters by inserting trainable\nrank decomposition matrices into transformer layers while freezing the pretrained weights.\nIn our configuration, we enabled LoRA adapters on the query and value projections of the\nattention mechanism, with rank r = 8. We set the LoRA scaling factor α = 16 and applied\na dropout rate of 0.05. Other components, such as the key, MLP, and projection layers, were\nleft frozen.\nAppendix J\nToken probability approximation for models\nwithout logprobs\nUnlike other generalist models, GPT-4o does not provide logprobs to prevent privacy attack.\nHowever, we need probabilities to reliably evaluate classification tasks. To approximate its\nprobabilities, we sample 10 generations from GPT using temperature of 1 (to not reweight the\nLM’s distribution), and count the number of occurrences for each multiple choice options.\nThen we normalize the counts to be probabilities for each option. For cost reasons, we limit\nthe number of examples to be 1000, except for CCI (comorbidity imputation).\nCCI is a special case because its label distribution is very skewed, so we greedily evaluate\non 10,000 samples instead. We binarize the greedy choice to be probability of 0 or 1.\n28\n"}, {"page": 29, "text": "Appendix K\nStratified Evaluation\nWe performed stratified evaluation on readmission to evaluation the performance variation\nacross different groups (age, first race, borough, ethnicity, sex, and whether the patients are\nchildren). Some groups are omitted because they have only one class due to small sample size. If\nwe look at means only, for age (Figure K8a), patients between 10 to 15 has the best performance,\nand patients between 85 and 90 has the worst performance. For race, Middle Eastern or North\nAfrican has the best performance, and Asian Indian has the worst performance. For borough,\nBrooklyn has the best performance and Queens has the worst performance. For ethnicity,\nSpanish Hispanic Origin is worst. For sex, female has better performance than male. Children’s\nperformance is better than adult and they also have a lower readmission rate.\n(a) Age.\n(b) First Race.\n(c) Borough.\n(d) Ethnicity.\n(e) Sex.\n(f) IsChild.\nFig. K8: Stratified performance analysis on 2024 readmission temporal test set.\n29\n"}, {"page": 30, "text": "Appendix L\nControl for Patient Overlap\nWe chose to construct the 2024 temporal test set without explicit patient split, because patients\ndo come back to the health system at deployment. While this choice is supported by our\nclinical collaborators, we did an ablation where the test set excludes seen patients. Keeping the\nfinetuned model fixed and varying the test data to either include or exclude seen patients, we see\nsimilar test performance on readmission prediction, in hospital mortality prediction, and\nlength of stay prediction. Figure L9a shows that on readmission prediction, removing patients\nseen from pretraining and finetuning slightly increases the performance from 76.5% (purple\nbar) to 77% (grey bar). Similarly, the performance increase is 0.39% for mortality (Figure L9b)\nand 0.70% for LOS (Figure L9c). While surprising, the slight increase could be attributed\nto repeated patients being both older and more likely to be a minority. For readmission\nprediction, repeated patients are on average 13 years older than non repeated patients (55\nv.s. 42 years). For mortality and LOS prediction, the age gap widens to 16 years (57 v.s. 41\nyears). In addition, repeated patients include a larger proportion of non-white patients (41%\nv.s. 36%). These findings show that our choice of temporal split does not over-estimate model\nperformance.\n0.760\n0.765\n0.770\n0.775\n0.780\n0.785\n2024 NYU Readmission ROC AUC\nControl for Patient Split\nFinetune Split\nTime Split Only (Reported)\nTime Split (Remove Finetune + Pretrain Patient)\n(a) Readmission\n0.950\n0.955\n0.960\n0.965\n0.970\n0.975\n0.980\n2024 NYU Mortality ROC AUC\nControl for Patient Split\nFinetune Split\nTime Split Only (Reported)\nTime Split (Remove Finetune Patient)\n(b) Mortality\n0.75\n0.76\n0.77\n0.78\n0.79\n0.80\n2024 NYU LOS ROC AUC\nControl for Patient Split\nFinetune Split\nTime Split Only (Reported)\nTime Split (Remove Finetune Patient)\n(c) LOS\nFig. L9: Lang1-1B’s finetuned performance on readmission, mortality and LOS 2024 test\nset, with or without patient split.\n30\n"}, {"page": 31, "text": "Appendix M\nExternal Validation: MIMIC v.s. NYU\nTo check how well Lang1 generalizes to a different health system, we compare finetuning\nLang1-1B and Llama-3.2-1B on three classification tasks (readmission, mortality, length of\nstay) from MIMIC III and NYU, and test on MIMIC III. MIMIC III [59] is a database of elec-\ntronic health records from ICU patients at the Beth Israel Hospital in Boston Massachusetts,\nwhereas NYU+ datasets are collected from New York City. See 5.2.3 for details about MIMIC\ndatasets and Methods 5.2.2 for NYU datasets. Figure M10 shows the heatmaps of MIMIC III\ntest performance for finetuning Lang1-1B and Llama-3.2-1B on NYU or MIMIC-III, with\nzero-shot performance as baselines. The x axis is the pretrained model (purple Lang1-1B and\nblue Llama-3.2-1B). The y axis is the finetuning setting (purple indicates finetuning on NYU\ndata; green indicates finetuning on MIMIC III data; and black indicates zero-shot). Darker\nyellow cells indicate higher test AUROC on MIMIC III.\nLang1-1B\nLlama-3.2-1B\nPretrained model\nzero-shot\nNYU\nReadmission\nMIMIC\nReadmission\nFinetune setting\n51.6%\n±1.5%\n45.5%\n±1.5%\n66.4%\n±1.4%\n60.8%\n±1.4%\n67.7%\n±1.4%\n58.3%\n±1.5%\nExternal Validation of Finetuning Lang1\nfor MIMIC III Readmission\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nROC AUC on MIMIC III Readmission\n(a) Readmission prediction.\nLang1-1B\nLlama-3.2-1B\nPretrained model\nzero-shot\nNYU\nMortality\nMIMIC\nMortality\nFinetune setting\n44.3%\n±3.8%\n42.7%\n±3.9%\n83.7%\n±2.8%\n83.1%\n±2.7%\n85.5%\n±2.9%\n77.3%\n±3.1%\nExternal Validation of Finetuning Lang1\nfor MIMIC III Mortality\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\nROC AUC on MIMIC III Mortality\n(b) In-hospital Mortality.\nLang1-1B\nLlama-3.2-1B\nPretrained model\nzero-shot\nNYU\nLOS\nMIMIC\nLOS\nFinetune setting\n50.1%\n±2.1%\n54.8%\n±2.0%\n65.2%\n±2.1%\n61.6%\n±2.1%\n64.8%\n±2.1%\n55.2%\n±2.1%\nExternal Validation of Finetuning Lang1\nfor MIMIC III LOS\n0.50\n0.52\n0.54\n0.56\n0.58\n0.60\n0.62\n0.64\nROC AUC on MIMIC III LOS\n(c) LOS.\nFig. M10: Finetuning on NYU transfers well to MIMIC III for readmission, mortality and\nLOS prediction. Overall performance is better on finetuning Lang1-1B compared to Llama\n3.2 1B.\n31\n"}, {"page": 32, "text": "Finetuning Lang1-1B on NYU Readmission transfers to MIMIC-III Readmission.\nThe\ndifference in mean AUROC between finetuning Lang1-1B on MIMIC III versus NYU range\nfrom 0.4%-1.8% AUROC and are roughly within standard error, showing that finetuning\nLang1-1B at NYU transfers well to MIMIC.\nLang1-1B achieves better performance than Llama-3.2-1B.\nThe difference in mean\nAUROC between finetuning Lang1-1B and Llama-3.2-1B on the same data range from 0.6%-\n9.6%, showing that the clinically pretrained Lang1-1B is a preferred base model for these\nthree clinical predictive tasks.\nFinetuning on NYU, which is slightly out-of-distribution, could outperform finetuning on\nMIMIC for Llama-3.2-1B.\nIt is reasonable to expect that in-distribution finetuning leads\nto best performance. While this is true for Lang1-1B (purple), it is not the case for Llama-3.2-\n1B (blue), which sees 2.5% - 6.4% AUROC increase from finetuning on NYU compared to\nfinetuning on MIMIC. We hypothesize that this is because NYU data has more labeled pairs,\nand that non clinically pretrained models would benefit more from a larger number of\nslightly out-of-distribution pairs. We test this hypothesis on readmission prediction, where\nNYU Readmission (36,2259 examples) is 8.6 times the size of MIMIC III readmission (42,180\nexamples). Figure M11a shows that test performance on MIMIC III is similar when Llama-\n3.2-1B is finetuned on NYU Readmission that is downsampled to be the same size as MIMIC\nIII (42,180). This pattern does not hold when the pretrained model is changed to the clinically\npretrained Lang1-1B: Figure M11b shows that finetuning on MIMIC-III readmission yields\nthe best test performance on MIMIC-III, despite fewer number of finetuning examples.\n42180\n362259\nNumber of Finetuning Examples for Llama-3.2-1B\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\nMIMIC III Readmission ROC AUC\nFinetune Data\nNYU\nMIMIC\n(a) Finetuning on full NYU dataset leads\nto best performance on Llama-3.2-1B, even\nthough NYU Readmission is not directly in-\ndistribution for MIMIC Readmission.\n42180\n362259\nNumber of Finetuning Examples for Lang1-1B\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\nMIMIC III Readmission ROC AUC\nFinetune Data\nNYU\nMIMIC\n(b) Finetuning on MIMIC III leads to best\nperformance for Lang1-1B, even though\nNYU Readmission has more labeled pairs.\nFig. M11: Clinical models such as Lang1-1B benefit more from in-domain data, whereas\ngeneralist models such as Llama-3.2-1B could benefit from more slightly OOD examples\ncompared to fewer in-distribution examples.\n32\n"}, {"page": 33, "text": "References\n[1] Sinsky, C., Colligan, L., Li, L., Prgomet, M., Reynolds, S., Goeders, L., Westbrook, J.,\nTutty, M., Blike, G.: Allocation of physician time in ambulatory practice: A time and\nmotion study in 4 specialties. Ann. Intern. Med. 165(11), 753–760 (2016)\n[2] Hingle, S.: Electronic health records: An unfulfilled promise and a call to action. Ann.\nIntern. Med. 165(11), 818–819 (2016)\n[3] Murphy, D.R., Meyer, A.N.D., Russo, E., Sittig, D.F., Wei, L., Singh, H.: The burden of\ninbox notifications in commercial electronic health records. JAMA Intern. Med. 176(4),\n559–560 (2016)\n[4] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., Cao, Y.: ReAct: Synergizing\nreasoning and acting in language models. arXiv [cs.CL] (2022) [cs.CL]\n[5] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas,\nD., Hendricks, L.A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driess-\nche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Vinyals, O., Rae,\nJ.W., Sifre, L.: An empirical analysis of compute-optimal large language model training.\nIn: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (eds.) Advances in Neural Information\nProcessing Systems (2022). https://openreview.net/forum?id=iBBcRUlOAPR\n[6] DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma,\nS., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z.F., Gou, Z., Shao, Z., Li, Z., Gao,\nZ., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G.,\nLi, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li,\nH., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J.L., Ni, J., Liang,\nJ., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang,\nL., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M.,\nLi, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q.,\nGe, R., Zhang, R., Pan, R., Wang, R., Chen, R.J., Jin, R.L., Chen, R., Lu, S., Zhou, S.,\nChen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S.S., Zhou, S., Wu, S., Ye, S.,\nYun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu,\nW., Zhang, W., Xiao, W.L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X.,\nLiu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X.Q., Jin, X., Shen, X.,\nChen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y.K., Wang,\nY.Q., Wei, Y.X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang,\nY., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou,\nY., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y.,\nZhu, Y.X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan,\nY., Ren, Z.Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan,\nZ., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z.,\nZhang, Z., Zhang, Z.: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning (2025). http://arxiv.org/abs/2501.12948\n33\n"}, {"page": 34, "text": "[7] Trinh, T.H., Wu, Y., Le, Q.V., He, H., Luong, T.: Solving olympiad geometry without\nhuman demonstrations. Nature 625(7995), 476–482 (2024)\n[8] Gottweis, J., Weng, W.-H., Daryin, A., Tu, T., Palepu, A., Sirkovic, P., Myaskovsky,\nA., Weissenberger, F., Rong, K., Tanno, R., Saab, K., Popovici, D., Blum, J., Zhang,\nF., Chou, K., Hassidim, A., Gokturk, B., Vahdat, A., Kohli, P., Matias, Y., Carroll, A.,\nKulkarni, K., Tomasev, N., Guan, Y., Dhillon, V., Vaishnav, E.D., Lee, B., Costa, T.R.D.,\nPenadÃľs, J.R., Peltz, G., Xu, Y., Pawlosky, A., Karthikesalingam, A., Natarajan, V.:\nTowards an AI co-scientist (2025). http://arxiv.org/abs/2502.18864\n[9] Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P.,\nRosenberg, D., Mann, G.: BloombergGPT: A Large Language Model for Finance (2023).\nhttp://arxiv.org/abs/2303.17564\n[10] Chen, J., Li, D., Chen, Q., Zhou, W., Liu, X.: Diaformer: Automatic diagnosis via\nsymptoms sequence generation. Proc. Conf. AAAI Artif. Intell. 36(4), 4432–4440 (2022)\n[11] Chen, W., Zhong, C., Peng, J., Wei, Z.: DxFormer: a decoupled automatic diagnostic\nsystem based on decoder-encoder transformer with dense symptom representations.\nBioinformatics 39(1), 744 (2023)\n[12] Panagoulias, D.P., Palamidas, F.A., Virvou, M., Tsihrintzis, G.A.: Evaluating the\npotential of LLMs and ChatGPT on medical diagnosis and treatment. In: 2023 14th\nInternational Conference on Information, Intelligence, Systems & Applications (IISA),\npp. 1–9. IEEE, ??? (2023)\n[13] Amin, K., Khosla, P., Doshi, R., Chheang, S., Forman, H.P.: Artificial intelligence to\nimprove patient understanding of radiology reports. Yale J. Biol. Med. 96(3), 407–417\n(2023)\n[14] Ellershaw, S., Tomlinson, C., Burton, O.E., Frost, T., Hanrahan, J.G., Khan, D.Z.,\nHorsfall, H.L., Little, M., Malgapo, E., Starup-Hansen, J., Ross, J., Noor, K., Vella-\nBaldacchino, M., Shah, A.D., Dobson, R.: Automated generation of hospital discharge\nsummaries using clinical guidelines and large language models. In: AAAI 2024 SSS on\nClinical FMs, pp. 1–8. Stanford University, Stanford, California, USA (2024)\n[15] Zhou, S., Xu, Z., Zhang, M., Xu, C., Guo, Y., Zhan, Z., Ding, S., Wang, J., Xu, K., Fang,\nY., Xia, L., Yeung, J., Zha, D., Melton, G.B., Lin, M., Zhang, R.: Large language models\nfor disease diagnosis: A scoping review. arXiv [cs.CL] (2024) [cs.CL]\n[16] Zhang, L., Liu, M., Wang, L., Zhang, Y., Xu, X., Pan, Z., Feng, Y., Zhao, J., Zhang, L.,\nYao, G., Chen, X., Xie, X.: Constructing a large language model to generate impressions\nfrom findings in radiology reports. Radiology 312(3), 240885 (2024)\n[17] He, Z., Bhasuran, B., Jin, Q., Tian, S., Hanna, K., Shavor, C., Arguello, L.G., Murray,\nP., Lu, Z.: Quality of answers of generative large language models versus peer users for\ninterpreting laboratory test results for lay patients: Evaluation study. J. Med. Internet\n34\n"}, {"page": 35, "text": "Res. 26(1), 56655 (2024)\n[18] Kweon, S., Kim, J., Kwak, H., Cha, D., Yoon, H., Kim, K., Yang, J., Won, S., Choi,\nE.: EHRNoteQA: An LLM benchmark for real-world clinical practice using discharge\nsummaries. Neural Inf Process Syst 37, 124575–124611 (2024)\n[19] Glicksberg, B.S., Timsina, P., Patel, D., Sawant, A., Vaid, A., Raut, G., Charney, A.W.,\nApakama, D., Carr, B.G., Freeman, R., Nadkarni, G.N., Klang, E.: Evaluating the accu-\nracy of a state-of-the-art large language model for prediction of admissions from the\nemergency room. J. Am. Med. Inform. Assoc. 31(9), 1921–1928 (2024)\n[20] Nazyrova, N., Chahed, S., Chausalet, T., Dwek, M.: Leveraging large language models\nfor medical text classification: a hospital readmission prediction case. In: 2024 14th\nInternational Conference on Pattern Recognition Systems (ICPRS), pp. 1–7. IEEE, ???\n(2024)\n[21] Ben Shoham, O., Rappoport, N.: Cpllm: Clinical prediction with large language mod-\nels. PLOS Digital Health 3(12), 0000680 (2024) https://doi.org/10.1371/journal.pdig.\n0000680 . Published December 6, 2024\n[22] Scarlat, A., Campion, F.X.: Predicting 30-day mortality and readmission using hos-\npital discharge summaries: A comparative analysis of machine learning models, large\nlanguage models, and physicians. medRxiv, 2025–032625324714 (2025)\n[23] Bhasuran, B., Jin, Q., Xie, Y., Yang, C., Hanna, K., Costa, J., Shavor, C., Han, W., Lu,\nZ., He, Z.: Preliminary analysis of the impact of lab results on large language model\ngenerated differential diagnoses. NPJ Digit. Med. 8(1), 166 (2025)\n[24] McDuff, D., Schaekermann, M., Tu, T., Palepu, A., Wang, A., Garrison, J., Singhal, K.,\nSharma, Y., Azizi, S., Kulkarni, K., Hou, L., Cheng, Y., Liu, Y., Mahdavi, S.S., Prakash,\nS., Pathak, A., Semturs, C., Patel, S., Webster, D.R., Dominowska, E., Gottweis, J., Barral,\nJ., Chou, K., Corrado, G.S., Matias, Y., Sunshine, J., Karthikesalingam, A., Natarajan, V.:\nTowards accurate differential diagnosis with large language models. Nature, 1–7 (2025)\n[25] Alyakin, A., Stryker, J., Alber, D.A., Sangwon, K.L., Lee, J.V., Duderstadt, B., Save, A.,\nKurland, D., Frome, S., Singh, S., Zhang, J., Yang, E., Park, K.Y., Orillac, C., Valliani,\nA.A., Neifert, S., Liu, A., Patel, A., Livia, C., Lau, D., Laufer, I., Rozman, P.A., Hidalgo,\nE.T., Riina, H., Feng, R., Hollon, T., Aphinyanaphongs, Y., Golfinos, J.G., Snyder, L.,\nLeuthardt, E., Kondziolka, D., Oermann, E.K.: CNS-Obsidian: A Neurosurgical Vision-\nLanguage Model Built From Scientific Publications (2025). https://arxiv.org/abs/2502.\n19546\n[26] Nori, H., Lee, Y.T., Zhang, S., Carignan, D., Edgar, R., Fusi, N., King, N., Larson, J.,\nLi, Y., Liu, W., Luo, R., McKinney, S.M., Ness, R.O., Poon, H., Qin, T., Usuyama, N.,\nWhite, C., Horvitz, E.: Can generalist foundation models outcompete special-purpose\ntuning? case study in medicine. ArXiv abs/2311.16452 (2023)\n35\n"}, {"page": 36, "text": "[27] Zhou, H.-Y., Adithan, S., Acosta, J.N., Topol, E.J., Rajpurkar, P.: A generalist learner for\nmultifaceted medical image interpretation. arXiv [cs.CV] (2024) [cs.CV]\n[28] Lehman, E., Hernandez, E., Mahajan, D., Wulff, J., Smith, M.J., Ziegler, Z., Nadler,\nD., Szolovits, P., Johnson, A., Alsentzer, E.: Do we still need clinical language models?\narXiv [cs.CL] (2023) [cs.CL]\n[29] Johri, S., Jeong, J., Tran, B.A., Schlessinger, D.I., Wongvibulsin, S., Barnes, L.A., Zhou,\nH.-Y., Cai, Z.R., Van Allen, E.M., Kim, D., Daneshjou, R., Rajpurkar, P.: An evaluation\nframework for clinical use of large language models in patient interaction tasks. Nature\nmedicine 31(1), 77–86 (2025)\n[30] Jiang, Y., Black, K.C., Geng, G., Park, D., Zou, J., Ng, A.Y., Chen, J.H.: MedAgentBench:\nA realistic virtual EHR environment to benchmark medical LLM agents. arXiv [cs.LG]\n(2025) [cs.LG]\n[31] Bean, A.M., Payne, R., Parsons, G., Kirk, H.R., Ciro, J., Mosquera, R., Monsalve, S.H.,\nEkanayaka, A.S., Tarassenko, L., Rocher, L., Mahdi, A.: Clinical knowledge in llms does\nnot translate to human interactions. ArXiv abs/2504.18919 (2025)\n[32] Vishwanath, K., Alyakin, A., Alber, D.A., Lee, J.V., Kondziolka, D., Oermann, E.K.:\nMedical large language models are easily distracted (2025). https://arxiv.org/abs/2504.\n01201\n[33] Yan, C., Fu, X., Xiong, Y., Wang, T., Hui, S.C., Wu, J., Liu, X.: Llm sensitivity evaluation\nframework for clinical diagnosis. ArXiv abs/2504.13475 (2025)\n[34] Hager, P., Jungmann, F., Holland, R., Bhagat, K., Hubrecht, I., Knauer, M., Vielhauer,\nJ., Makowski, M., Braren, R., Kaissis, G., Rueckert, D.: Evaluation and mitigation of\nthe limitations of large language models in clinical decision-making. Nat. Med. 30(9),\n2613–2622 (2024)\n[35] Arora, R.K., Wei, J., Hicks, R.S., Bowman, P., QuiÃśonero-Candela, J., Tsimpourlas, F.,\nSharman, M., Shah, M., Vallone, A., Beutel, A., Heidecke, J., Singhal, K.: HealthBench:\nEvaluating large language models towards improved human health. arXiv [cs.CL] (2025)\n[cs.CL]\n[36] Wornow, M., Xu, Y., Thapa, R., Patel, B., Steinberg, E., Fleming, S., Pfeffer, M.A., Fries,\nJ., Shah, N.H.: The shaky foundations of large language models and foundation models\nfor electronic health records. NPJ Digit. Med. 6(1), 135 (2023)\n[37] Johnson, A.E.W., Bulgarelli, L., Shen, L., Gayles, A., Shammout, A., Horng, S., Pollard,\nT.J., Hao, S., Moody, B., Gow, B., Lehman, L.-w.H., Celi, L.A., Mark, R.G.: Mimic-\niv, a freely accessible electronic health record dataset. Scientific Data 10(1) (2023)\nhttps://doi.org/10.1038/s41597-022-01899-x\n[38] pubmed.gov: Download PubMed Data. NCBI Literature Resources\n36\n"}, {"page": 37, "text": "[39] Sushil, M., Ludwig, D., Butte, A.J., Rudrapatna, V.A.: Developing a general-purpose\nclinical language inference model from a large corpus of clinical notes. arXiv [cs.CL]\n(2022) [cs.CL]\n[40] Yang, X., Chen, A., PourNejatian, N., Shin, H.C., Smith, K.E., Parisien, C., Compas,\nC., Martin, C., Costa, A.B., Flores, M.G., Zhang, Y., Magoc, T., Harle, C.A., Lipori, G.,\nMitchell, D.A., Hogan, W.R., Shenkman, E.A., Bian, J., Wu, Y.: A large language model\nfor electronic health records. NPJ Digit. Med. 5(1), 194 (2022)\n[41] Peng, C., Yang, X., Chen, A., Smith, K.E., PourNejatian, N., Costa, A.B., Martin, C.,\nFlores, M.G., Zhang, Y., Magoc, T., Lipori, G., Mitchell, D.A., Ospina, N.S., Ahmed,\nM.M., Hogan, W.R., Shenkman, E.A., Guo, Y., Bian, J., Wu, Y.: A study of generative\nlarge language model for medical research and healthcare. NPJ Digit. Med. 6(1), 210\n(2023)\n[42] Jiang, L.Y., Liu, X.C., Nejatian, N.P., Nasir-Moin, M., Wang, D., Abidin, A., Eaton, K.,\nRiina, H.A., Laufer, I., Punjabi, P., Miceli, M., Kim, N.C., Orillac, C., Schnurman, Z.,\nLivia, C., Weiss, H., Kurland, D., Neifert, S., Dastagirzada, Y., Kondziolka, D., Cheung,\nA.T.M., Yang, G., Cao, M., Flores, M., Costa, A.B., Aphinyanaphongs, Y., Cho, K.,\nOermann, E.K.: Health system-scale language models are all-purpose prediction engines.\nNature (2023)\n[43] Goh, E., Gallo, R., Hom, J., Strong, E., Weng, Y., Kerman, H., Cool, J.A., Kanjee, Z.,\nParsons, A.S., Ahuja, N., Horvitz, E., Yang, D., Milstein, A., Olson, A.P.J., Rodman,\nA., Chen, J.H.: Large language model influence on diagnostic reasoning: A randomized\nclinical trial: A randomized clinical trial. JAMA Netw. Open 7(10), 2440969 (2024)\n[44] Nori, H., Daswani, M., Kelly, C., Lundberg, S., Ribeiro, M.T., Wilson, M., Liu, X.,\nSounderajah, V., Carlson, J., Lungren, M.P., Gross, B., Hames, P., Suleyman, M., King,\nD., Horvitz, E.: Sequential diagnosis with language models. arXiv [cs.CL] (2025)\n[cs.CL]\n[45] Tu, T., Schaekermann, M., Palepu, A., Saab, K., Freyberg, J., Tanno, R., Wang, A.,\nLi, B., Amin, M., Cheng, Y., Vedadi, E., Tomasev, N., Azizi, S., Singhal, K., Hou, L.,\nWebson, A., Kulkarni, K., Mahdavi, S.S., Semturs, C., Gottweis, J., Barral, J., Chou, K.,\nCorrado, G.S., Matias, Y., Karthikesalingam, A., Natarajan, V.: Towards conversational\ndiagnostic artificial intelligence. Nature, 1–9 (2025)\n[46] Luccioni, A.S., Jernite, Y., Strubell, E.: Power hungry processing: Watts driving the cost\nof AI deployment? arXiv [cs.LG] (2023) [cs.LG]\n[47] Cottier, B., Rahman, R., Fattorini, L., Maslej, N., Besiroglu, T., Owen, D.: The rising\ncosts of training frontier AI models. arXiv [cs.CY] (2024) [cs.CY]\n[48] Feng, S., Ding, W., Liu, A., Wang, Z., Shi, W., Wang, Y., Shen, Z., Han, X., Lang,\nH., Lee, C.-Y., Pfister, T., Choi, Y., Tsvetkov, Y.: When one LLM drools, multi-LLM\ncollaboration rules. arXiv [cs.CL] (2025) [cs.CL]\n37\n"}, {"page": 38, "text": "[49] Belcak, P., Heinrich, G., Diao, S., Fu, Y., Dong, X., Muralidharan, S., Lin, Y.C.,\nMolchanov, P.: Small language models are the future of agentic AI. arXiv [cs.AI] (2025)\n[cs.AI]\n[50] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C.,\nAgarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\nM., Askell, A., Welinder, P., Christiano, P., Leike, J., Lowe, R.: Training language models\nto follow instructions with human feedback. arXiv [cs.CL] (2022) [cs.CL]\n[51] Rocklin, M.: Dask: Parallel computation with blocked algorithms and task scheduling.\nIn: Proceedings of the Python in Science Conference, pp. 126–132. SciPy, ??? (2015)\n[52] Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J.R., Hestness, J., Dey, N.: SlimPajama:\nA 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.\nnet/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama\n(2023). https://huggingface.co/datasets/cerebras/SlimPajama-627B\n[53] Common Crawl Foundation: Common Crawl Dataset. https://commoncrawl.org.\nAccessed: 2025-11-14 (2024)\n[54] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,\nW., Liu, P.J.: C4: Colossal Clean Crawled Corpus. https://www.tensorflow.org/datasets/\ncatalog/c4. Accessed: YYYY-MM-DD (2020)\n[55] Wenzek, G., Lacroix, T., Lavergne, T., et al.: BookCorpus2. https://github.com/\nfacebookresearch/cc_net. Included in The Pile and SlimPajama datasets. (2020)\n[56] Rae, J.W., Potapenko, A., Jayakumar, S.M., Lillicrap, T.: Compressing large-scale lan-\nguage models. In: International Conference on Machine Learning (ICML) (2020). PG-19\nlong-book subset from Project Gutenberg.\n[57] Charlson, M.E., Pompei, P., Ales, K.L., MacKenzie, C.R.: A new method of classifying\nprognostic comorbidity in longitudinal studies: Development and validation. Jour-\nnal of Chronic Diseases 40(5), 373–383 (1987) https://doi.org/10.1016/0021-9681(87)\n90171-8\n[58] Charlson\nComorbidity\nIndex\n(CCI).\nhttps://www.mdcalc.com/calc/3917/\ncharlson-comorbidity-index-cci. Accessed: 2025-10-12\n[59] Johnson, A., Pollard, T., Mark, R.: MIMIC-III clinical database. PhysioNet (2023)\n[60] Yang, G., Cao, M., Jiang, L.Y., Liu, X.C., Cheung, A.T.M., Weiss, H., Kurland, D.,\nCho, K., Oermann, E.K.: Language model classifier aligns better with physician word\nsensitivity than XGBoost on readmission prediction. arXiv [cs.CL] (2022) [cs.CL]\n[61] Welbl, J., Liu, N.F., Gardner, M.: Crowdsourcing multiple choice science questions.\narXiv [cs.HC] (2017) [cs.HC]\n38\n"}, {"page": 39, "text": "[62] Jin, Q., Dhingra, B., Liu, Z., Cohen, W.W., Lu, X.: PubMedQA: A dataset for biomedical\nresearch question answering. arXiv [cs.CL] (2019) [cs.CL]\n[63] Lightning AI: LitGPT. https://github.com/Lightning-AI/litgpt (2023)\n[64] Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H.,\nOtt, M., Shleifer, S., Desmaison, A., Balioglu, C., Damania, P., Nguyen, B., Chauhan,\nG., Hao, Y., Mathews, A., Li, S.: PyTorch FSDP: Experiences on scaling fully sharded\ndata parallel. arXiv [cs.DC] (2023) [cs.DC]\n[65] Bengio, Y.: Practical recommendations for gradient-based training of deep architectures.\narXiv [cs.LG] (2012) [cs.LG]\n[66] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LoRA:\nLow-rank adaptation of large language models. arXiv [cs.CL] (2021) [cs.CL]\n[67] Biderman, S., Schoelkopf, H., Sutawika, L., Gao, L., Tow, J., Abbasi, B., Aji, A.F.,\nAmmanamanchi, P.S., Black, S., Clive, J., DiPofi, A., Etxaniz, J., Fattori, B., Forde,\nJ.Z., Foster, C., Hsu, J., Jaiswal, M., Lee, W.Y., Li, H., Lovering, C., Muennighoff, N.,\nPavlick, E., Phang, J., Skowron, A., Tan, S., Tang, X., Wang, K.A., Winata, G.I., Yvon,\nF., Zou, A.: Lessons from the trenches on reproducible evaluation of language models.\narXiv [cs.CL] (2024) [cs.CL]\n[68] Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.H., Gonzalez, J.E., Zhang,\nH., Stoica, I.: Efficient memory management for large language model serving with\nPagedAttention. arXiv [cs.LG] (2023) [cs.LG]\n[69] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,\nM., MÃĳller, A., Nothman, J., Louppe, G., Prettenhofer, P., Weiss, R., Dubourg, V.,\nVanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, Ã.:\nScikit-learn: Machine learning in python. arXiv [cs.LG] (2012) [cs.LG]\n[70] Detlefsen, N., Borovec, J., Schock, J., Jha, A., Koker, T., Di Liello, L., Stancl, D., Quan,\nC., Grechkin, M., Falcon, W.: TorchMetrics - measuring reproducibility in PyTorch. J.\nOpen Source Softw. 7(70), 4101 (2022)\n39\n"}]}