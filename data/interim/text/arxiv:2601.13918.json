{"doc_id": "arxiv:2601.13918", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.13918.pdf", "meta": {"doc_id": "arxiv:2601.13918", "source": "arxiv", "arxiv_id": "2601.13918", "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization", "authors": ["Yusheng Liao", "Chuan Xuan", "Yutong Cai", "Lina Yang", "Zhe Chen", "Yanfeng Wang", "Yu Wang"], "published": "2026-01-20T12:48:04Z", "updated": "2026-01-20T12:48:04Z", "summary": "Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.13918v1", "url_pdf": "https://arxiv.org/pdf/2601.13918.pdf", "meta_path": "data/raw/arxiv/meta/2601.13918.json", "sha256": "6125574e39a6e68a457d384b8d54f62b081d2c7f06285619f67d0f47d83be623", "status": "ok", "fetched_at": "2026-02-18T02:20:58.616993+00:00"}, "pages": [{"page": 1, "text": "AGENTEHR: Advancing Autonomous Clinical Decision-Making\nvia Retrospective Summarization\nYusheng Liao*‚ô†, Chuan Xuan‚àó,‚ô†, Yutong Cai‚ô†, Lina Yang‚ô†, Zhe Chen‚ô†,\nYanfeng Wang‚ô†,‚ô¢, Yu Wang‚Ä†,‚ô†,‚ô¢\n‚ô†Shanghai Jiao Tong University\n‚ô¢Shanghai Artificial Intelligence Laboratory\n{liao20160907,xuanchuan,iautng123,alina_yln,chenzhe2018,\nwangyanfeng622,yuwangsjtu}@sjtu.edu.cn\nAbstract\nLarge Language Models have demonstrated\nprofound utility in the medical domain. How-\never, their application to autonomous Elec-\ntronic Health Records (EHRs) navigation re-\nmains constrained by a reliance on curated in-\nputs and simplified retrieval tasks. To bridge\nthe gap between idealized experimental set-\ntings and realistic clinical environments, we\npresent AGENTEHR. This benchmark chal-\nlenges agents to execute complex decision-\nmaking tasks, such as diagnosis and treat-\nment planning, requiring long-range interactive\nreasoning directly within raw and high-noise\ndatabases. In tackling these tasks, we iden-\ntify that existing summarization methods in-\nevitably suffer from critical information loss\nand fractured reasoning continuity. To address\nthis, we propose RETROSUM, a novel frame-\nwork that unifies a retrospective summariza-\ntion mechanism with an evolving experience\nstrategy. By dynamically re-evaluating inter-\naction history, the retrospective mechanism\nprevents long-context information loss and en-\nsures unbroken logical coherence. Addition-\nally, the evolving strategy bridges the domain\ngap by retrieving accumulated experience from\na memory bank. Extensive empirical evalua-\ntions demonstrate that RETROSUM achieves\nperformance gains of up to 29.16% over com-\npetitive baselines, while significantly decreas-\ning total interaction errors by up to 92.3%.\nOur code and datasets are available at https:\n//github.com/BlueZeros/AgentEHR.\n1\nIntroduction\nLarge Language Models (LLMs) have demon-\nstrated profound utility in the medical do-\nmain (Singhal et al., 2023; Nori et al., 2023; Chen\net al., 2023), acting as powerful catalysts rang-\ning from diagnostic report generation (Biswas and\n*Equal contribution.\n‚Ä†Corresponding Author\na) EHR Reasoning Tasks\nb) EHRAgent (Information Search)\nc) AgentEHR (Clinical Decision-Making)\nEHR\nDoctor\nClean Text\nLLM\nEHR\nDoctor\nWhat was the patient‚Äôs\nrecent most prescribed \nmedication?\nRecent medicationÔºö\nMetformin‚Ä¶\nClean\nAgent\nSearch\nEHR\nDoctor\nBased on the previous record, \nwhat is the current diagnosis\nand treatment for the patient?\nDiagnosis: Type 2 Diabetes\nRecommended: Liftstyle\nchanges, follow-up‚Ä¶\nAgent\nMedication \nhistory\nMulti-step\nReasoning\nActive Retrieve\nInfo Analysis\nFigure 1: Comparison between the previous EHR\ntasks and the proposed benchmark. Unlike previous\nEHRAgent task retrieving factual information explicitly\npresent in the EHR (e.g., medication history), AGEN-\nTEHR analyzes existing information to predict future\nclinical decisions, like diagnosis and treatment plans.\nTalukdar, 2024; Jung et al., 2024) to patient com-\nmunication (Qiu et al., 2025a,b; Liao et al., 2024).\nWhile these highlights underscore the potential of\nLLMs, effectively deploying them on the complex\nenvironment of Electronic Health Records (EHRs)\nremains a significant challenge.\nExisting research on LLM applications in EHRs\nhas predominantly focused on clinical decision-\nmaking tasks, such as patient risk prediction (Lin\net al., 2025; Fang et al., 2025; Zhu et al., 2024) and\nintervention strategy recommendation (Liao et al.,\n2025b; Hager et al., 2024). However, these method-\nologies often depend on manual curation (Hegsel-\nmann et al., 2025; Hager et al., 2024), creating\nan idealized experimental setting that sidesteps in-\nherent challenges like data noise and long-context\nprocessing. Consequently, their applicability in\narXiv:2601.13918v1  [cs.CL]  20 Jan 2026\n"}, {"page": 2, "text": "Methods\nAgent Capacity\nAgent Methods\nEHR\nApplication\nRaw Data\nProcess\nInfo\nRetrieval\nDecision\nMaking\nLong\nContext\nSummarization\nRetrospective\nMechanism\nExperience\nAccumulation\nReSum (Wu et al., 2025)\n‚úó\n‚úó\n‚úì\n‚úó\n‚úì\n‚úì\n‚úó\n‚úó\nMEM1 (Zhou et al., 2025)\n‚úó\n‚úó\n‚úì\n‚úó\n‚úì\n‚úì\n‚úó\n‚úó\nReasoning Bank (Ouyang et al., 2025)\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\nReflecTool (Liao et al., 2025a)\n‚úì\n‚úì\n‚úì\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\nEHRAgent (Shi et al., 2024)\n‚úì\n‚úì\n‚úì\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\nEHR-MCP (Masayoshi et al., 2025)\n‚úì\n‚úì\n‚úì\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\nMed-Copilot (Xu et al., 2025)\n‚úì\n‚úì\n‚úì\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\nEHR-R1 (Liao et al., 2025b)\n‚úì\n‚úó\n‚úó\n‚úì\n‚úó\n‚úó\n‚úó\n‚úó\nRETROSUM (Ours)\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\nTable 1: Comparison of previous works and RETROSUM on both agent capacities and methods.\nrealistic clinical scenarios is severely limited. Si-\nmultaneously, recent advancements have integrated\nagent-based systems to facilitate autonomous EHR\nnavigation. Yet, these efforts largely confine the\nagent‚Äôs role to query rewriting (Lee et al., 2022;\nRyu et al., 2024; Wang et al., 2020) or factual in-\nformation retrieval (Shi et al., 2024; Masayoshi\net al., 2025; Lee et al., 2025; Jiang et al., 2025).\nSuch approaches effectively reduce the LLM to a\nsophisticated search interface, failing to fully lever-\nage the model‚Äôs advanced reasoning capabilities\nfor complex clinical analysis.\nTo bridge these gaps, we introduce AGEN-\nTEHR, a novel benchmark covering six diverse\ntasks across three real-world EHR subsets. As\nillustrated in Figure 1, AGENTEHR transcends\nthe factual retrieval focus of prior work. Instead,\nit demands that agents actively engage in multi-\nstep information seeking, synthesize information\nthrough clinical reasoning, and finally deliver pre-\ncise decision-making predictions, such as diagnosis\nand treatment planning. Therefore, resolving these\ncomplex tasks necessitates extensive, multi-step\ninteractions with the EHR system. This process in-\nevitably accumulates a substantial amount of redun-\ndant information and results in extended interaction\nhistories, posing a severe challenge to the model‚Äôs\ncontextual capacity and reasoning integrity.\nA prevailing paradigm to navigate such long-\nhorizon challenges involves the use of vanilla sum-\nmarization techniques (Wu et al., 2025; Zhou et al.,\n2025) to distill salient information from redun-\ndancy and condense the elongated history. How-\never, a fundamental misalignment arises when ap-\nplying these paradigms to the clinical sphere. Un-\nlike general long-context interaction tasks (e.g.,\ndeepresearch) (Team et al., 2025b,a; Chen et al.,\n2025), EHR analysis is characterized by strong\nmulti-turn correlations, as every piece of retrieved\ninformation is intrinsically linked to the same pa-\ntient‚Äôs physiological state. Consequently, standard\nunidirectional summarization inevitably severs la-\ntent cross-temporal connections, fracturing the rea-\nsoning continuity required for precise diagnosis.\nTo address this, we propose RETROSUM, which\nintroduces a retrospective mechanism to capture\ndependencies for unbroken reasoning, alongside an\nevolving strategy to bridge the domain gap via ex-\nperience accumulation. As highlighted in Table 1,\nby adopting the retrospective mechanism on EHR-\nbased decision-making tasks, RETROSUM offers\na promising strategy to navigate the intricacies of\nraw EHRs, successfully mitigating the loss of logi-\ncal fidelity typically seen in clinical long-context\nreasoning applications.\nOur main contributions are as follows:\n‚Ä¢ Realistic Clinical Benchmark: We present\nAGENTEHR, the first benchmark designed to\nbridge the gap between idealized settings and\nauthentic medical scenarios. Unlike prior stud-\nies, AGENTEHR establishes a new standard\nby challenging agents to perform multi-step\nreasoning within raw EHR databases to fulfill\nclinical decision-making tasks.\n‚Ä¢ Retrospective Reasoning Framework: We\npropose RETROSUM, a simple yet effective\nmethod engineered to master the intricacies\nof long-context EHR reasoning tasks. By in-\ncorporating a retrospective mechanism, our\nmethod effectively captures latent correlative\ninformation and ensures reasoning continuity.\n‚Ä¢ Superior Empirical Performance: Exten-\nsive experiments demonstrate that RETRO-\nSUM achieves remarkable performance gains\nof up to 29.16% over existing baselines. This\nconfirms RETROSUM‚Äôs superior robustness\n"}, {"page": 3, "text": "and efficacy in handling the complex dynam-\nics of real-world clinical decision-making.\n2\nAGENTEHR Benchmark\nTo bridge the gap between current LLM capabilities\nand the complexities of real-world clinical applica-\ntions, we introduce AGENTEHR, a comprehensive\nevaluation framework designed for EHR-based in-\nteractive reasoning and clinical decision-making\ninteraction.\n2.1\nData Construction\nWe construct AGENTEHR based on two widely-\nused real-world EHR databases: MIMIC-IV (John-\nson et al., 2023) and MIMIC-III (Johnson et al.,\n2016). To rigorously evaluate the generalization\nand robustness of autonomous agents, we organize\nthese data sources into three distinct experimental\nsubsets (details are shown in Appendix B):\nMIMIC-IV-Common\n(In-Distribution)\nWe\nstratify the MIMIC-IV dataset based on label\nfrequency, selecting cases with the most prevalent\nconditions to form the Common subset. This serves\nas the primary In-Distribution (ID) benchmark for\nassessing standard clinical reasoning capabilities.\nMIMIC-IV-Rare (Label-Shift OOD)\nCompris-\ning the long-tail cases from MIMIC-IV, this subset\nintroduces a significant distribution shift in the la-\nbel space. It evaluates the agent‚Äôs ability to handle\nlow-prevalence diseases where parametric knowl-\nedge is often weaker.\nMIMIC-III (Systemic-Shift OOD)\nWe utilize\nthe complete MIMIC-III dataset to represent a more\nchallenging systemic shift. Unlike the label-only\nshift in the Rare subset, MIMIC-III presents funda-\nmental differences in table schema and information\ndensity compared to MIMIC-IV. This setting pro-\nvides the most comprehensive measure of agent\nmethods to heterogeneous EHR environments.\n2.2\nClinical Tasks\nAGENTEHR encompasses six core clinical tasks:\nDiagnoses, Labevents, Microbiology, Prescrip-\ntions, Procedures, and Transfers.\nCollectively,\nthese tasks represent the entire lifecycle of patient\nhospitalization, spanning the critical phases of di-\nagnosis, laboratory examination, and therapeutic\nintervention.\n2.3\nToolbox MCP Server\nTo enable the Agent system to navigate the complex\nEHR environment efficiently, we design a compre-\nhensive Toolbox hosted on a Model Context Pro-\ntocol (MCP) Server. This architecture provides\na standardized interface for the agent to access\nover 19 specialized tools, ensuring robust and scal-\nable interaction with the underlying database. The\ntoolbox equips the model with diverse retrieval\nmechanisms, including temporal filtering, keyword\nsearch, fuzzy matching, and direct SQL execution.\nDetailed specifications of the toolbox can be found\nin Appendix C.\n3\nMethods\nIn this section, we first formally define the clin-\nical agent task. We then introduce RETROSUM,\na novel framework designed to mitigate the infor-\nmation loss and reasoning fragmentation inherent\nin standard incremental summarization approaches\n(overview is shown in Figure 2). Finally, we de-\nscribe the Evolving Optimization strategy.\n3.1\nTask Formulation\nThe clinical agent task requires an agent system to\ninteract with a database to answer complex medical\nqueries. Each query instance is composed of X =\n{p, t, I}, where p is the patient identifier, t is the\nreference timestamp, and I is a clinical instruction.\nThe interaction proceeds in discrete steps i ‚àà\n{1, . . . , K}.\nAt each step i, the agent ob-\nserves the current state, which encapsulates the\nquery X and the interaction history Hi‚àí1 =\n{(a1, o1), . . . , (ai‚àí1, oi‚àí1)}. Based on this con-\ntext, the agent generates an action ai according to\nits own policy œÄŒ∏:\nai ‚àºœÄŒ∏(ai|Hi‚àí1, X)\n(1)\nUpon executing action ai, the environment returns\nan observation oi, which is derived from the EHR\ndatabase E:\noi = E(ai)\n(2)\nThe tuple (ai, oi) is then appended to the history\nHi = Hi‚àí1‚à™(ai, oi). This process repeats until the\nagent issues a termination action and produces a\nfinal answer set Y = {y1, y2, . . . , ym}, consisting\nof a list of items (e.g., specific diagnosis codes or\nmedication names) that satisfy the clinical instruc-\ntion.\n"}, {"page": 4, "text": "ùê¥!\nùëÇ!\nEHR DataBase\nLabevents\nAdmission\nùëÜ!\nDiagnoses\nMedication\nRadiology\n30+ \nTables\nMCP Server\nFull History\n‚Ä¶\nùëÜ\"\n‚Ä¶\nSummarizer\nReSum\nActor\nùëå\nRetroSum\nSummarizer\nActor\nInfo Keep\nReasoning\nInterrupted\nInfo Loss\nùëÇ\"\nInfo X:\nHigh Lactate\nSummary S\n(Miss Info X)\nInfo Y: Fever\n(Link Info X)\nReasoning \nPreserved\nIncorrect Decision:\n(Missed Sepsis)\nCorrect Decision:\n(Sepsis Protocol)\nWeakness\nStrength\nEHR Flow\nReSum Flow\nRetroSum Flow\nEvolved Flow\nExperience Memory Bank\nEmbedding\n[0.1, 0.9, ‚Ä¶, 0.5]\nActor-Exp\nPrioritize High-Confidence\nSemantic Matches Over \nFuzzy Search ‚Ä¶\nSum-Exp\nCapture comorbidities from \npast history with clinical \ncontext‚Ä¶\nSummary:\nFever Detected\n(still Miss Info X)\nSummary:\nFever AND\nHigh Lactate Detected\nFigure 2: Overview of RETROSUM. RETROSUM (right) addresses critical information loss and reasoning inter-\nruptions inherent in unidirectional methods like ReSum (left). By incorporating a retrospective mechanism to\nre-evaluate full interaction histories and an evolving mechanism to retrieve specialized strategies from memory,\nRETROSUM ensures robust long-horizon clinical reasoning and correct decisions.\n3.2\nRETROSUM\nNavigating real-world EHRs requires the agent to\nmanage extensive context while identifying com-\nplex dependencies across heterogeneous tables.\nPrevious methods, such as ReSum (Wu et al.,\n2025), typically employ unidirectional summariza-\ntion, where the history context is compressed incre-\nmentally. However, we identify two critical limita-\ntions in this paradigm for clinical tasks: (1) Loss of\nLatent Correlations: EHR data is inherently inter-\nconnected. Information retrieved in early turns may\ninitially appear irrelevant but becomes crucial after\nobserving a later result. Unidirectional approaches,\nwhich rely on local information summarization,\noften discard these latent factors before their rel-\nevance is established via cross-table correlations.\n(2) Disruption of Reasoning Logic: Forcing the\nagent to rely exclusively on a highly compressed\nsummary disrupts the continuity of multi-turn rea-\nsoning logic. The abstraction process often strips\naway the precise syntactical and numerical details\nrequired for the agent to deduce the next logical\nstep. To address these challenges, we propose RET-\nROSUM, which introduces a retrospective mecha-\nnism. Specifically, our framework operates through\ntwo synergistic phases:\nRetrospective Summarization\nWe define a\nsummarization window size w.\nThe sum-\nmarization process is triggered only at step\nj,\nwhere j ‚â°0 (mod w).\nAt these inter-\nvals,\nthe interaction history is conceptually\npartitioned\ninto\nthe\ndistant\nhistory\nHdist =\n{(a1, o1), . . . , (aj‚àíw, oj‚àíw)} and the recent win-\ndow\nHrec = {(aj‚àíw+1, oj‚àíw+1), . . . , (aj, oj)}.\nThe Summarizer module M,\nwhich shares\nparameters Œ∏ with the agent‚Äôs policy, generates an\nupdated summary Sj by retrospectively analyzing\nthe entire trajectory:\nSj = MŒ∏(Hrec, Sj‚àíw, Hdist, X)\n(3)\nBy conditioning on both Hdist and Hrec, the Sum-\nmarizer can re-evaluate the importance of past\nevents in light of the most recent findings, effec-\ntively capturing latent correlations that were previ-\nously ambiguous.\nRetrospective Inference\nTo prevent the disrup-\ntion of reasoning logic, we design the actor to oper-\nate on a history-aware context. Unlike prior meth-\nods that replace the history with a summary, RET-\nROSUM augments the full interaction history with\nthe latest retrospective summary. For any step i,\nthe entire context ÀÜHi is updated as:\nÀÜHi =\n(\nÀÜHi‚àí1 \\ {Si‚àíw} ‚à™{(ai, oi), Si},\nif i ‚â°0\n(mod w)\nÀÜHi‚àí1 ‚à™{(ai, oi)},\notherwise\n(4)\n"}, {"page": 5, "text": "where Si‚àíw is the most recent summary. The policy\nœÄŒ∏ then generates the next action based on this\naugmented view:\nai ‚àºœÄŒ∏(ai| ÀÜHi‚àí1, X)\n(5)\nThis formulation ensures that the actor retains ac-\ncess to the complete raw history Hi‚àí1, preserv-\ning the integrity of the multi-turn reasoning chain,\nwhile S serves as a high-level cognitive map to\nguide the model‚Äôs attention toward clinically sig-\nnificant patterns extracted from previous phases.\n3.3\nEvolving Strategy\nGeneral-purpose LLMs often lack the specific clin-\nical intuition required to distinguish subtle signals\nin EHRs or navigate complex table schemas. To\nbridge this gap, we introduce the evolving strategy\nfor RETROSUM. Rather than updating model pa-\nrameters, this phase enables the agent to crystallize\nsuccessful strategies into an external memory bank,\nallowing it to learn from past trials.\nExperience Generation\nFor each training in-\nstance, the agent generates a full interaction trajec-\ntory HK, a final retrospective summary Sfinal, and\na predicted answer set Y . By comparing these out-\nputs against the ground truth Y ‚àó, we derive domain-\nspecific experiences using the reflection module\nRŒ∏, which shares parameters with the agent‚Äôs pol-\nicy:\nEact = RŒ∏(HK, Y, Y ‚àó)\n(6)\nEsum = RŒ∏(HK, Sfinal, Y, Y ‚àó)\n(7)\nHere, RŒ∏ is prompted to extract procedural heuris-\ntics for the Actor (e.g., optimal tool selection) and\ninformation salience guidelines for the Summa-\nrizer (e.g., critical evidence retained vs. noise com-\npressed).\nMemory Construction\nWe organize these in-\nsights into an Experience Memory Bank B. Each\nentry is stored as a triplet:\nB = {(e, Eact, Esum)i}M\ni=1\n(8)\nwhere M is the size of the memory and e is the rep-\nresentation of the EHR. e is obtained by encoding\nthe most recent clinical events from each patient‚Äôs\nEHR tables using a pre-trained encoder. This en-\nsures that the retrieved experience is contextually\naligned with the patient‚Äôs current status.\nEvolved Inference\nDuring inference, for a new\npatient context, we compute its embedding and\nretrieve the most similar memory items from B.\nThese retrieved experiences are injected into the\nframework to guide both modules:\nSi = MŒ∏(Hrec, Si‚àíw, Hdist, X, Esum)\n(9)\nai+1 ‚àºœÄŒ∏(ai+1| ÀÜHi, X, Eact)\n(10)\nBy explicitly conditioning on the experience, the\nsummarizer improves its ability to filter noise,\nand the actor adopts proven reasoning strategies,\nthereby enhancing overall robustness.\n4\nExperiments\n4.1\nSetting\nAll the details of the experimental setting, including\nbaseline models, agent methods, and experimental\ndetails, can be found in Appendix D.\n4.2\nMain Results\nTable 2 demonstrates the consistent superiority of\nRETROSUM across all settings. Even without the\nevolving mechanism, our method outperforms com-\npetitive baselines. A key observation is the instabil-\nity of the ReSum baseline on stronger backbones.\nWhile ReSum aids smaller models like Qwen3-\nNext-80B, it significantly underperforms standard\nReAct on highly capable models such as Grok-4.1-\nfast (0.2237 vs 0.2501). This indicates that the\ncritical information loss inherent in unidirectional\ncompression outweighs its benefits for strong rea-\nsoners. In contrast, RETROSUM overcomes this\nlimitation by retaining access to the full history,\nensuring robust performance improvements regard-\nless of the backbone‚Äôs capability.\nThe results also highlight the general instability\nof evolving strategies within the AGENTEHR envi-\nronment. Reasoning Bank fails to improve weaker\nmodels and only yields benefits on larger parame-\nters like the 80B model, while ReflectTool shows\nnegligible gains because its atomic tool-specific\nexperience is ineffective for complex context syn-\nthesis. Conversely, the evolving variant of RETRO-\nSUM achieves the highest average score of 0.2880.\nThis success suggests that the high-quality context\ncompression provided by the retrospective mecha-\nnism amplifies the utility of retrieved experiences,\nallowing the agent to leverage historical insights\nmore effectively than other approaches.\n"}, {"page": 6, "text": "Models\nMethods\nEvolved\nDiagnoses\nLabevents\nMicrobiology\nPrescriptions\nProcedures\nTransfers\nAvg.\nReAct\n‚úó\n0.0955\n0.1295\n0.1274\n0.0756\n0.2472\n0.2772\n0.1587\nReflexion\n‚úó\n0.1216\n0.0983\n0.0989\n0.0901\n0.2273\n0.2292\n0.1442\nReSum\n‚úó\n0.1753\n0.0881\n0.1166\n0.1000\n0.2717\n0.2320\n0.1639\nReflecTool (CS)\n‚úì\n0.1647\n0.0821\n0.1265\n0.0614\n0.2572\n0.2589\n0.1585\nReflecTool (IR)\n‚úì\n0.1046\n0.0893\n0.1360\n0.0713\n0.2428\n0.2882\n0.1554\nReasoningBank\n‚úì\n0.0385\n0.1275\n0.1250\n0.0448\n0.2931\n0.2103\n0.1399\nRETROSUM (Ours)\n‚úó\n0.2368‚Ä°\n0.0977‚Ä†\n0.1584‚Ä°\n0.1039\n0.2983‚Ä†\n0.2791‚Ä°\n0.1957‚Ä°\nQwen3-30B-A3B\nRETROSUM (Ours)\n‚úì\n0.2514‚Ä°\n0.1092‚Ä°\n0.1568‚Ä°\n0.1289‚Ä°\n0.3068‚Ä°\n0.3171‚Ä†\n0.2117‚Ä°\nReAct\n‚úó\n0.1836\n0.0793\n0.1895\n0.1075\n0.2857\n0.2346\n0.1800\nReflexion\n‚úó\n0.2323\n0.0887\n0.1585\n0.0818\n0.3098\n0.1599\n0.1718\nReSum\n‚úó\n0.2161\n0.1083\n0.1597\n0.1301\n0.3503\n0.2317\n0.1994\nReflecTool (CS)\n‚úì\n0.2088\n0.0894\n0.1579\n0.1059\n0.3066\n0.1985\n0.1779\nReflecTool (IR)\n‚úì\n0.1992\n0.0822\n0.1389\n0.0910\n0.2490\n0.2390\n0.1665\nReasoningBank\n‚úì\n0.2519\n0.1998\n0.1726\n0.1267\n0.3743\n0.2793\n0.2341\nRETROSUM (Ours)\n‚úó\n0.2519‚Ä°\n0.1389‚Ä°\n0.1881‚Ä°\n0.1285\n0.3473\n0.2217\n0.2127‚Ä†\nQwen3-Next-80B-A3B\nRETROSUM (Ours)\n‚úì\n0.3148‚Ä°\n0.1990‚Ä°\n0.1830‚Ä°\n0.1429‚Ä†\n0.3578\n0.2573‚Ä†\n0.2425‚Ä°\nReAct\n‚úó\n0.1671\n0.1172\n0.1545\n0.0670\n0.2739\n0.1413\n0.1535\nReSum\n‚úó\n0.2804\n0.1009\n0.1570\n0.0728\n0.2907\n0.0803\n0.1637\nRETROSUM (Ours)\n‚úó\n0.2842\n0.1328‚Ä°\n0.1568\n0.0953‚Ä°\n0.3092\n0.0988‚Ä†\n0.1795‚Ä†\nQwen3-235B-A22B*\nRETROSUM (Ours)\n‚úì\n0.3700‚Ä°\n0.1951‚Ä°\n0.1791‚Ä°\n0.1346‚Ä°\n0.3556‚Ä°\n0.2342‚Ä°\n0.2448‚Ä°\nReAct\n‚úó\n0.4270\n0.2151\n0.1867\n0.0764\n0.2693\n0.2053\n0.2300\nReSum\n‚úó\n0.4024\n0.2181\n0.2090\n0.0752\n0.2668\n0.1993\n0.2285\nRETROSUM (Ours)\n‚úó\n0.4023\n0.2569‚Ä°\n0.2146\n0.1128‚Ä°\n0.2614\n0.2071\n0.2425‚Ä†\nGPT-5-mini\nRETROSUM (Ours)\n‚úì\n0.4127‚Ä†\n0.2989‚Ä°\n0.2097\n0.1536‚Ä°\n0.2603\n0.2327‚Ä°\n0.2613‚Ä°\nReAct\n‚úó\n0.3841\n0.1856\n0.2032\n0.1688\n0.3762\n0.1826\n0.2501\nReSum\n‚úó\n0.3822\n0.0882\n0.2024\n0.1618\n0.3186\n0.1888\n0.2237\nRETROSUM (Ours)\n‚úó\n0.4270‚Ä°\n0.1859‚Ä°\n0.2296‚Ä†\n0.1846‚Ä†\n0.3480‚Ä†\n0.1901\n0.2609‚Ä°\nGrok-4.1-fast\nRETROSUM (Ours)\n‚úì\n0.4734‚Ä°\n0.2321‚Ä°\n0.2236‚Ä†\n0.1861‚Ä†\n0.3397\n0.2729‚Ä°\n0.2880‚Ä°\nTable 2: Experiments results on MIMI-IV-Common subset. All the performances are measured by the F1 score.\n‚Äò*‚Äô indicates the model uses 4-bit AWQ quantization. The best results are Bold, while the second best results are\nunderlined. ‚Ä† and ‚Ä° indicate the p-value < 0.05 and < 0.01 comparing with the ReSum method, respectively.\n4.3\nCross-subset Validation\nTo investigate the generalization capability of RET-\nROSUM, we conducted cross-subset validation on\nMIMIC-IV-Rare and MIMIC-III. As detailed in\nTable 3, RETROSUM exhibits superior robustness\nagainst both types of distribution shifts. In the\nMIMIC-IV-Rare subset, our method establishes a\nclear lead over ReSum and ReAct, effectively han-\ndling low-prevalence diagnoses where standard pat-\nterns often fail. The MIMIC-III benchmark reveals\na sharper contrast in adaptability. While baselines\nlike ReflectTool demonstrate improved resilience,\nReSum suffers a significant performance degra-\ndation. This decline suggests that ReSum‚Äôs uni-\ndirectional summaries may be brittle to systemic\nformat changes. Conversely, RETROSUM success-\nfully mitigates this issue to maintain state-of-the-art\nperformance, demonstrating that the retrospective\nmechanism captures generalizable medical logic\nrather than merely superior in a specific database.\n4.4\nAblation Experiments\nTo validate the individual contributions of our\nframework‚Äôs core components, we conducted com-\nprehensive ablation studies on the Qwen3-30B-\nA3B backbone, as detailed in Table 4. Introduc-\ning the retrospective mechanism significantly en-\nhances performance compared with Resum; apply-\ning it specifically to the Actor or Summarizer yields\nscores of 0.1876 and 0.1798, respectively, while the\ncomplete base RETROSUM (blue row) combining\nboth achieves a synergistic gain to 0.1957. Build-\ning upon this, incorporating evolving optimization\nprovides substantial benefits with the highest peak\nperformance of 0.2117. A deeper analysis of the\nretrospective mechanism is further shown in Sec-\ntion 5.1.\n5\nAnalysis\nIn this section, all the experiments are conducted on\nthe diagnoses task in MIMIC-IV-Common. More\nexperimental results, like tool analysis and caes\nstudy, can be found in Appendix F.\n5.1\nEffectiveness of Retrospective Mechanism\nTo dissect the contributions of the retrospective\nmechanism, we evaluated its independent appli-\ncation to the Summarizer (Sum-Only) and Ac-\ntor (Act-Only) across intervals ranging from 30\ndown to 1 step, as shown in Figure 3. The re-\nsults reveal distinct dominant roles depending on\nsummarization frequency. At small intervals (e.g.,\n"}, {"page": 7, "text": "EHR Database\nMethods\nEvolved\nDiagnoses\nLabevents\nMicrobiology\nPrescriptions\nProcedures\nTransfers\nAvg.\nReAct\n‚úó\n0.0863\n0.0757\n0.0550\n0.0516\n0.2645\n0.2196\n0.1255\nReflexion\n‚úó\n0.1248\n0.0607\n0.0447\n0.0574\n0.2492\n0.1848\n0.1203\nReSum\n‚úó\n0.1807\n0.0609\n0.0579\n0.0857\n0.2812\n0.2282\n0.1491\nReflecTool (CS)\n‚úì\n0.1653\n0.0444\n0.0597\n0.0465\n0.2506\n0.2137\n0.1300\nReflecTool (IR)\n‚úì\n0.0947\n0.0446\n0.0551\n0.0361\n0.2826\n0.2123\n0.1209\nReasoningBank\n‚úì\n0.0309\n0.0783\n0.0540\n0.0349\n0.2570\n0.1859\n0.1068\nRETROSUM (Ours)\n‚úó\n0.1950‚Ä†\n0.0530\n0.0519\n0.0921\n0.2719\n0.2400‚Ä°\n0.1506\nMIMIC-IV-Rare\nRETROSUM (Ours)\n‚úì\n0.2325‚Ä°\n0.0700‚Ä†\n0.0536\n0.0933‚Ä†\n0.2991‚Ä†\n0.2194\n0.1613‚Ä†\nReAct\n‚úó\n0.0395\n0.1418\n0.1871\n0.0841\n0.0116\n0.3272\n0.1319\nReflexion\n‚úó\n0.0417\n0.1340\n0.1710\n0.0715\n0.0133\n0.2656\n0.1162\nReSum\n‚úó\n0.0502\n0.1417\n0.1764\n0.0851\n0.0185\n0.2485\n0.1201\nReflecTool (CS)\n‚úì\n0.0586\n0.1956\n0.1595\n0.0782\n0.0298\n0.3689\n0.1484\nReflecTool (IR)\n‚úì\n0.0468\n0.1875\n0.1602\n0.0780\n0.0282\n0.3576\n0.1431\nReasoningBank\n‚úì\n0.0602\n0.2087\n0.1736\n0.0815\n0.0170\n0.1722\n0.1189\nRetroSum (Ours)\n‚úó\n0.0705‚Ä°\n0.1970‚Ä°\n0.1510‚Ä†\n0.0891\n0.0337‚Ä°\n0.3627‚Ä°\n0.1507‚Ä°\nMIMIC-III\nRetroSum (Ours)\n‚úì\n0.0869‚Ä°\n0.1860‚Ä°\n0.1691\n0.0807\n0.0473‚Ä°\n0.3570‚Ä°\n0.1545‚Ä°\nTable 3: Results of Qwen3-30B-A3B on OOD subsets. The best results are Bold, while the second best results are\nunderlined. ‚Ä† and ‚Ä° indicate the p-value < 0.05 and < 0.01 comparing with the ReSum method, respectively.\nActor\nSummarizer\nRetrospect\nEvolved\nRetrospect\nEvolved\nDiagnoses\nLabevents\nMicrobiology\nPrescriptions\nProcedures\nTransfers\nAvg.\n‚úó\n‚úó\n‚úó\n‚úó\n0.1753\n0.0881\n0.1166\n0.1000\n0.2717\n0.2320\n0.1639\n‚úì\n‚úó\n‚úó\n‚úó\n0.2257‚Ä°\n0.1041‚Ä†\n0.1418‚Ä°\n0.1030\n0.2932‚Ä†\n0.2577\n0.1876‚Ä°\n‚úó\n‚úó\n‚úì\n‚úó\n0.1759\n0.1097‚Ä†\n0.1710‚Ä°\n0.1119‚Ä†\n0.2320‚Ä°\n0.2782‚Ä†\n0.1798‚Ä°\n‚úì\n‚úó\n‚úì\n‚úó\n0.2368‚Ä°\n0.0977‚Ä°\n0.1584‚Ä°\n0.1039\n0.2983‚Ä†\n0.2791\n0.1957‚Ä°\n‚úì\n‚úì\n‚úì\n‚úó\n0.2505‚Ä°\n0.1387‚Ä°\n0.1541‚Ä°\n0.1354‚Ä†\n0.2853\n0.2982‚Ä°\n0.2104‚Ä°\n‚úì\n‚úó\n‚úì\n‚úì\n0.2437‚Ä°\n0.1151‚Ä°\n0.1565‚Ä°\n0.1082\n0.3234‚Ä°\n0.2919‚Ä°\n0.2065‚Ä°\n‚úì\n‚úì\n‚úì\n‚úì\n0.2514‚Ä°\n0.1092‚Ä†\n0.1568‚Ä°\n0.1289‚Ä†\n0.3068‚Ä†\n0.3171‚Ä°\n0.2117‚Ä°\nTable 4: Ablation results of Qwen3-30B-A3B on MIMIC-IV-Common subset. The row in gray and blue indicates\nthe results of ReSum and RETROSUM, respectively. The best results are Bold, while the second best results are\nunderlined. ‚Ä† and ‚Ä° indicate the p-value < 0.05 and < 0.01 comparing with the ReSum method, respectively.\nw ‚â§5), Act-Only is crucial and drives performance\nby maintaining immediate reasoning coherence\nagainst frequent context interruptions. Conversely,\nat larger intervals (e.g., w ‚â•15), Sum-Only be-\ncomes dominant, ensuring distant critical informa-\ntion is retained over long horizons. By synergizing\nthese complementary strengths, the complete RET-\nROSUM framework secures robust performance\nthat consistently outperforms the ReSum baseline\nacross the entire spectrum of summarization fre-\nquencies.\n5.2\nError Analysis\nTo deepen our understanding, we categorized fail-\nures into six types (detailed in Appendix E). As\nshown in Figure 4, failed trajectories are predomi-\nnantly plagued by ‚ÄòNo Candidate Tool‚Äô errors and\nrepetitive behaviors, indicating that EHR noise fre-\nquently disrupts reasoning context. While ReSum\nreduces repetitive errors compared to ReAct, it re-\nmains prone to tool-related failures due to infor-\nmation loss. In contrast, RETROSUM achieves a\nsubstantial reduction in total errors across all cat-\negories. Notably, the Evolved RETROSUM vari-\n1\n2\n3\n5\n10\n15\n20\n30\nSummary Interval (Steps)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nF1 Score\nReSum\nRetroSum\nRetroSum (Sum-Only)\nRetroSum (Act-Only)\nFigure 3: Impact of summarization interval on agent\nperformance. The retrospective mechanism is applied\nsolely to the Summarizer (Sum-Only) or the Actor (Act-\nOnly) across varying frequencies.\nant further enhances tool utilization, resulting in a\n92.3% reduction in total errors compared to Rea-\nsoningBank. These findings validate that our ret-\nrospective mechanism effectively mitigates criti-\ncal reasoning failures caused by complex, long-\nhorizon EHR interactions.\n5.3\nEfficiency Analysis\nTo evaluate reasoning efficiency, we analyzed the\ndistribution of interaction turns in Figure 5 (further\nanalysis in Appendix F.1). The results highlight a\n"}, {"page": 8, "text": "-92.3% Errors\nFigure 4: Error statistics on the diagnoses task. The left\npanel displays the distribution of specific error types\nacross unsuccessful and successful trajectories. The\nright panel compares the count of errors committed by\ndifferent agent frameworks.\ncritical bottleneck in baselines: ReSum frequently\nexhausts the maximum 100-turn limit, indicating\nthat its lossy unidirectional summarization traps\nagents in redundant information-seeking loops to\nrecover missing context. In contrast, RETROSUM\nshifts the distribution peak to the efficient 20-40\nturn range. By utilizing the retrospective mech-\nanism to retain salient history, our method effec-\ntively eliminates navigational dead-ends. Crucially,\nthis reasoning efficiency translates to resource econ-\nomy. Although the retrospective process introduces\nspecific overhead per step, the drastic reduction in\ntotal interaction turns effectively outweighs this\ncost. Consequently, RETROSUM achieves superior\nperformance while requiring significantly fewer\ntokens and less inference time than baselines (as\ndetailed in Appendix F.3).\n5.4\nTest-time Scaling\nTo explore the upper bounds of agent capability, we\ninvestigate the impact of test-time scaling using the\nBest@K F1 Score (see Appendix D.3 for formal\ndefinition). As illustrated in Figure 6, RETROSUM\nconsistently outperforms both ReAct and ReSum\nbaselines across the entire spectrum of K. The\nsuperior performance at Best@1 (equivalent to the\naverage F1 score across 256 samples) attests to the\nrobustness of our retrospective mechanism in con-\nsistently generating high-quality trajectories com-\npared to baselines. Furthermore, the substantial per-\nformance gains observed with increased sampling\nbudgets indicate that current models possess sig-\nnificant potential capacity to solve these complex\nclinical problems. This confirms that the AGEN-\nTEHR task, while challenging, is fundamentally\nsolvable and its design is reasonable.\n5.5\nImpact of Context Length\nTo evaluate the robustness of our framework under\nmemory constraints, we varied the maximum con-\ntext length from 64k down to 8k tokens, as shown\nin Figure 7. A clear downward trend is observed\nfor baseline methods; both ReAct and ReSum suf-\nfer significant performance degradation as the con-\ntext window tightens, indicating that standard slid-\ning windows and unidirectional summaries fail to\nretain critical information when space is limited.\nConversely, RETROSUM exhibits remarkable sta-\nbility, maintaining high F1 scores even at the most\nrestrictive 8k limit. This robustness stems from\nthe retrospective mechanism, which enables the\nActor to preserve complete reasoning logic regard-\nless of window size. By periodically re-evaluating\nthe full interaction history to distill essential cross-\ntemporal dependencies into the summary, RETRO-\nSUM ensures the reasoning chain remains unbroken\nand information-dense, effectively neutralizing the\nnegative impact of restricted context windows.\n6\nConclusions\nIn this work, we bridge the gap between idealized\nexperimental settings and realistic clinical environ-\nments by presenting AGENTEHR, a benchmark\nnecessitating complex decision-making within raw,\nhigh-noise databases. Based on this, we propose\nRETROSUM, which adopt the retrospective mecha-\nnism with an evolving experience strategy to cap-\nture latent cross-temporal correlations. Empirical\nvalidation shows retrospective mechanisms are es-\nsential for unlocking the potential of clinical agents\nin EHR-based reasoning decision-making tasks.\nLimitations\nDespite the robust performance of RETROSUM es-\ntablished in this work, several limitations remain.\nFirst, our evaluation relies primarily on the MIMIC-\nIV and MIMIC-III datasets. While these databases\nserve as the gold standard for critical care research,\nthey are inherently sourced from a single medical\ncenter, which may not fully capture the diverse ad-\nministrative protocols or demographic variations\nfound in broader global healthcare systems. Sec-\nond, our current framework is designed specifically\nfor textual clinical notes and structured tabular data.\nIt does not yet possess the capability to directly an-\nalyze pixel-level medical imaging (e.g., raw CT\nscans) or high-frequency physiological waveforms,\nrelying instead on textual reports. Future work will\n"}, {"page": 9, "text": "0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100 110\nInteraction Rounds\n5\n10\n50\n100\n150\n200\n250\n300\n350\nSample Num\nReAct\nReSum\nRetroSum\nRetroSum Evolved\nFigure 5: Distribution of interaction\nturns across different agent methods.\n1\n2\n4\n8\n16\n32\n64 128 256\nK (Log Scale)\n0.2\n0.3\n0.4\n0.5\n0.6\nBest@K F1 Score\nReAct\nReSum\nRetroSum\nFigure 6: Test-time scaling perfor-\nmance evaluated using Best@K F1\nScore.\n8k\n16k\n24k\n32k\n48k\n64k\nContext Length (Tokens)\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0.225\n0.250\nF1 Score\nReAct\nReSum\nRetroSum\nFigure 7: Performance sensitivity\nto maximum context length limited\nfrom 64k to 8k tokens.\nfocus on extending the agent‚Äôs capabilities to mul-\ntimodal data synthesis and exploring multi-center\ngeneralization.\nReferences\nAnjanava Biswas and Wrick Talukdar. 2024. Intelli-\ngent clinical documentation: Harnessing generative\nai for patient-centric clinical note generation. arXiv\npreprint arXiv:2405.18346.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu\nLian, and Zheng Liu. 2024. Bge m3-embedding:\nMulti-lingual, multi-functionality, multi-granularity\ntext embeddings through self-knowledge distillation.\nPreprint, arXiv:2402.03216.\nShan Chen,\nPedro Moreira,\nYuxin Xiao,\nSam\nSchmidgall, Jeremy Warner, Hugo Aerts, Thomas\nHartvigsen, Jack Gallifant, and Danielle S Bitter-\nman. 2025. Medbrowsecomp: Benchmarking medi-\ncal deep research and computer use. arXiv preprint\narXiv:2505.14963.\nZeming Chen, Alejandro Hern√°ndez Cano, Angelika\nRomanou, Antoine Bonnet, Kyle Matoba, Francesco\nSalvi, Matteo Pagliardini, Simin Fan, Andreas\nK√∂pf, Amirkeivan Mohtashami, and 1 others. 2023.\nMeditron-70b: Scaling medical pretraining for large\nlanguage models. arXiv preprint arXiv:2311.16079.\nHejie Cui, Alyssa Unell, Bowen Chen, Jason Alan Fries,\nEmily Alsentzer, Sanmi Koyejo, and Nigam H Shah.\n2025. Timer: Temporal instruction modeling and\nevaluation for longitudinal clinical records. npj Digi-\ntal Medicine, 8(1):577.\nYue Fang, Yuxin Guo, Jiaran Gao, Hongxin Ding,\nXinke Jiang, Weibin Liao, Yongxin Xu, Yinghao\nZhu, Zhibang Yang, Liantao Ma, and 1 others. 2025.\nToward better ehr reasoning in llms: Reinforce-\nment learning with expert attention guidance. arXiv\npreprint arXiv:2508.13579.\nScott L Fleming,\nAlejandro Lozano,\nWilliam J\nHaberkorn, Jenelle A Jindal, Eduardo Reis, Rahul\nThapa, Louis Blankemeier, Julian Z Genkins, Ethan\nSteinberg, Ashwin Nayak, and 1 others. 2024.\nMedalign: A clinician-generated dataset for instruc-\ntion following with electronic medical records. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 38, pages 22021‚Äì22030.\nPaul Hager, Friederike Jungmann, Robbie Holland, Ku-\nnal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob\nVielhauer, Marcus Makowski, Rickmer Braren, Geor-\ngios Kaissis, and Daniel Rueckert. 2024. Evaluation\nand mitigation of the limitations of large language\nmodels in clinical decision-making.\nStefan Hegselmann, Georg von Arnim, Tillmann\nRheude, Noel Kronenberg, David Sontag, Gerhard\nHindricks, Roland Eils, and Benjamin Wild. 2025.\nLarge language models are powerful electronic health\nrecord encoders. arXiv preprint arXiv:2502.17403.\nYixing Jiang, Kameron C Black, Gloria Geng, Danny\nPark, James Zou, Andrew Y Ng, and Jonathan H\nChen. 2025. Medagentbench: A virtual ehr environ-\nment to benchmark medical llm agents. NEJM AI,\npage AIdbp2500144.\nAlistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin\nGayles, Ayad Shammout, Steven Horng, Tom J Pol-\nlard, Sicheng Hao, Benjamin Moody, Brian Gow, and\n1 others. 2023. Mimic-iv, a freely accessible elec-\ntronic health record dataset. Scientific data, 10(1):1.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H\nLehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi,\nand Roger G Mark. 2016. Mimic-iii, a freely accessi-\nble critical care database. Scientific data, 3(1):1‚Äì9.\nHyoJe Jung, Yunha Kim, Heejung Choi, Hyeram Seo,\nMinkyoung Kim, JiYe Han, Gaeun Kee, Seohyun\nPark, Soyoung Ko, Byeolhee Kim, and 1 others. 2024.\nEnhancing clinical efficiency through llm: Discharge\nnote generation for cardiac patients. arXiv preprint\narXiv:2404.05144.\nGyubok Lee, Elea Bach, Eric Yang, Tom Pollard, Al-\nistair Johnson, Edward Choi, Jong Ha Lee, and 1\nothers. 2025. Fhir-agentbench: Benchmarking llm\nagents for realistic interoperable ehr question answer-\ning. arXiv preprint arXiv:2509.19319.\n"}, {"page": 10, "text": "Gyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu\nKwon, Woncheol Shin, Seongjun Yang, Minjoon Seo,\nJong-Yeup Kim, and Edward Choi. 2022. Ehrsql: A\npractical text-to-sql benchmark for electronic health\nrecords. Advances in Neural Information Processing\nSystems, 35:15589‚Äì15601.\nYusheng Liao, Shuyang Jiang, Yanfeng Wang, and\nYu Wang. 2025a. Reflectool: Towards reflection-\naware tool-augmented clinical agents. In Proceed-\nings of the 63rd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), ACL 2025, Vienna, Austria, July 27 - August 1,\n2025, pages 13507‚Äì13531. Association for Computa-\ntional Linguistics.\nYusheng Liao, Yutong Meng, Yuhao Wang, Hongcheng\nLiu, Yanfeng Wang, and Yu Wang. 2024.\nAuto-\nmatic interactive evaluation for large language mod-\nels with state aware patient simulator. arXiv preprint\narXiv:2403.08495.\nYusheng Liao, Chaoyi Wu, Junwei Liu, Shuyang Jiang,\nPengcheng Qiu, Haowen Wang, Yun Yue, Shuai\nZhen, Jian Wang, Qianrui Fan, and 1 others. 2025b.\nEhr-r1: A reasoning-enhanced foundational language\nmodel for electronic health record analysis. arXiv\npreprint arXiv:2510.25628.\nJiacheng Lin,\nZhenbang Wu,\nand Jimeng Sun.\n2025.\nTraining llms for ehr-based reasoning\ntasks via reinforcement learning.\narXiv preprint\narXiv:2505.24105.\nKanato Masayoshi, Masahiro Hashimoto, Ryoichi\nYokoyama, Naoki Toda, Yoshifumi Uwamino, Shogo\nFukuda, Ho Namkoong, and Masahiro Jinzaki. 2025.\nEhr-mcp: Real-world evaluation of clinical informa-\ntion retrieval by large language models via model\ncontext protocol. arXiv preprint arXiv:2509.15957.\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023. Capabili-\nties of gpt-4 on medical challenge problems. arXiv\npreprint arXiv:2303.13375.\nOpenAI. 2025. Introducing GPT-5. https://openai.\ncom/index/introducing-gpt-5. Accessed: 2025-\n08-07.\nSiru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen,\nKe Jiang, Zifeng Wang, Rujun Han, Long T. Le,\nSamira Daruki, Xiangru Tang, Vishy Tirumalashetty,\nGeorge Lee, Mahsan Rofouei, Hangfei Lin, Jiawei\nHan, Chen-Yu Lee, and Tomas Pfister. 2025. Reason-\ningbank: Scaling agent self-evolving with reasoning\nmemory. CoRR, abs/2509.25140.\nPengcheng Qiu, Chaoyi Wu, Junwei Liu, Qiaoyu Zheng,\nYusheng Liao, Haowen Wang, Yun Yue, Qianrui Fan,\nShuai Zhen, Jian Wang, and 1 others. 2025a. Evolv-\ning diagnostic agents in a virtual clinical environment.\narXiv preprint arXiv:2510.24654.\nPengcheng Qiu, Chaoyi Wu, Shuyu Liu, Yanjie Fan,\nWeike Zhao, Zhuoxia Chen, Hongfei Gu, Chuanjin\nPeng, Ya Zhang, Yanfeng Wang, and 1 others. 2025b.\nQuantifying the reasoning abilities of llms on clinical\ncases. Nature Communications, 16(1):9799.\nFran√ßois Remy, Kris Demuynck, and Thomas De-\nmeester. 2024. BioLORD-2023: semantic textual\nrepresentations fusing large language models and\nclinical knowledge graph insights. Journal of the\nAmerican Medical Informatics Association, page\nocae029.\nJaehee Ryu, Seonhee Cho, Gyubok Lee, and Edward\nChoi. 2024.\nEhr-seqsql: A sequential text-to-sql\ndataset for interactively exploring electronic health\nrecords. arXiv preprint arXiv:2406.00019.\nWenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu\nZhang, Hang Wu, Yuanda Zhu, Joyce C. Ho, Carl\nYang, and May Dongmei Wang. 2024.\nEHRA-\ngent: Code empowers large language models for few-\nshot complex tabular reasoning on electronic health\nrecords. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\npages 22315‚Äì22339, Miami, Florida, USA. Associa-\ntion for Computational Linguistics.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: language agents with verbal reinforcement\nlearning. In Advances in Neural Information Pro-\ncessing Systems 36: Annual Conference on Neural\nInformation Processing Systems 2023, NeurIPS 2023,\nNew Orleans, LA, USA, December 10 - 16, 2023.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\nand 1 others. 2023. Large language models encode\nclinical knowledge. Nature, 620(7972):172‚Äì180.\nMiroMind Team, Song Bai, Lidong Bing, Carson\nChen, Guanzheng Chen, Yuntao Chen, Zhe Chen,\nZiyi Chen, Jifeng Dai, Xuan Dong, and 1 oth-\ners. 2025a. Mirothinker: Pushing the performance\nboundaries of open-source research agents via model,\ncontext, and interactive scaling.\narXiv preprint\narXiv:2511.11793.\nTongyi DeepResearch Team, Baixuan Li, Bo Zhang,\nDingchu Zhang, Fei Huang, Guangyu Li, Guoxin\nChen, Huifeng Yin, Jialong Wu, Jingren Zhou, and 1\nothers. 2025b. Tongyi deepresearch technical report.\narXiv preprint arXiv:2510.24701.\nPing Wang, Tian Shi, and Chandan K Reddy. 2020.\nText-to-sql generation for question answering on elec-\ntronic medical records. In Proceedings of The Web\nConference 2020, pages 350‚Äì361.\nXixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu\nOu, Huifeng Yin, Zhongwang Zhang, Yong Jiang,\nPengjun Xie, Fei Huang, Minhao Cheng, Shuai\nWang, Hong Cheng, and Jingren Zhou. 2025. Re-\nsum: Unlocking long-horizon search intelligence via\ncontext summarization. CoRR, abs/2509.13313.\n"}, {"page": 11, "text": "Zhenbang Wu, Anant Dadu, Mike Nalls, Faraz Faghri,\nand Jimeng Sun. 2024. Instruction tuning large lan-\nguage models to understand electronic health records.\nAdvances in Neural Information Processing Systems,\n37:54772‚Äì54786.\nxAI. 2025. Grok 4.1 Fast and Agent Tools API. https:\n//x.ai/news/grok-4-1-fast.\nAccessed: 2025-\n11-19.\nGuangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong\nZhang. 2024. Benchmarking retrieval-augmented\ngeneration for medicine. In Findings of the Associa-\ntion for Computational Linguistics ACL 2024, pages\n6233‚Äì6251.\nRan Xu, Yuchen Zhuang, Yishan Zhong, Yue Yu, Xian-\ngru Tang, Hang Wu, May Dongmei Wang, Peifeng\nRuan, Donghan Yang, Tao Wang, and 1 others. 2025.\nMedagentgym: Training llm agents for code-based\nmedical reasoning at scale. In The Second Workshop\non GenAI for Health: Potential, Trust, and Policy\nCompliance.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Day-\niheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao\nGe, Haoran Wei, Huan Lin, Jialong Tang, and 40\nothers. 2025.\nQwen3 technical report.\nCoRR,\nabs/2505.09388.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R. Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net.\nZijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan\nKim, Alok Prakash, Daniela Rus, Jinhua Zhao,\nBryan Kian Hsiang Low, and Paul Pu Liang. 2025.\nMem1: Learning to synergize memory and reason-\ning for efficient long-horizon agents. arXiv preprint\narXiv:2506.15841.\nYinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu,\nHangyuan Ji, Zixiang Wang, Tao Sun, Long He,\nZhoujun Li, Xi Zhu, and 1 others. 2024. Realm:\nRag-driven enhancement of multimodal electronic\nhealth records analysis via large language models.\narXiv preprint arXiv:2402.07016.\nA\nRelated Works\nRecent advancements in Large Language Mod-\nels (LLMs) for Electronic Health Records (EHRs)\nhave progressed from foundational instruction tun-\ning to the development of autonomous agents and\nspecialized benchmarks. Early efforts focused on\naligning general-purpose models with the longi-\ntudinal and heterogeneous nature of clinical data;\nfor instance, MIMIC-Instr (Wu et al., 2024) and\nMEDALIGN (Fleming et al., 2024) introduced\nlarge-scale instruction datasets to bridge the gap\nbetween raw clinical logs and natural language un-\nderstanding. These foundations were subsequently\nenhanced by frameworks addressing specific rea-\nsoning dimensions, such as TIMER (Cui et al.,\n2025), which targets temporal dependencies in pa-\ntient histories, and EHR-R1 (Liao et al., 2025b),\nwhich synthesizes reasoning chains to improve\ncomplex decision-making. Concurrently, the field\nhas shifted towards agentic systems capable of ac-\ntive tool utilization. EHRAgent (Shi et al., 2024)\nand ReflecTool (Liao et al., 2025a) demonstrated\nthat equipping LLMs with code execution inter-\nfaces and reflective memory significantly outper-\nforms standard prompting on tabular reasoning\ntasks. This transition to agency extends to real-\nworld integration, where EHR-MCP (Masayoshi\net al., 2025) and EHRMIND (Yang et al., 2025)\nexplore standard-compliant deployment and veri-\nfiable reinforcement learning to ensure reliability\nin live clinical settings. To rigorously evaluate\nthese interactive capabilities, novel benchmarks\nlike MedAgentBench (Jiang et al., 2025) and FHIR-\nAgentBench (Lee et al., 2025) have been estab-\nlished, moving beyond static question-answering\nto assess agent planning, multi-step tool usage, and\ninteroperability within realistic virtual EHR envi-\nronments.\nB\nData Curation\nThis section outlines the data curation methodol-\nogy used to convert raw Electronic Health Records\n(EHR) into a structured format. The process in-\nvolves selecting data sources, with MIMIC-IV serv-\ning as the primary corpus and MIMIC-III used for\ncross-subset validation. We constructed patient-\nlevel time series, enriched medical codes with se-\nmantic mappings, standardized storage formats,\nand developed a unified toolbox to utilize the cu-\nrated data for executing clinical tasks. The fol-\nlowing subsections describe the processing steps\n"}, {"page": 12, "text": "ensuring data integrity and chronological accuracy.\nB.1\nData Preprocessing and Formatting\nWe reformatted the raw MIMIC-IV and MIMIC-\nIII dataset to construct accurate patient-level time\nseries. This involved extracting all events for each\npatient and sorting them by timestamp with second-\nlevel precision.\nTemporal Alignment and Imputation.\nThe raw\ndataset lacks second-level timestamps for tables\nsuch as diagnoses_icd, procedures_icd, and di-\nagnosis events in the Emergency Department (ED).\nTo enable sequential modeling, we imputed these\ntimestamps by linking events to their correspond-\ning admissions. For both standard diagnoses_icd\nand ED diagnoses, we set the timestamp to one\nminute prior to the discharge time of the associated\nadmission or ED stay. For procedures_icd events\nthat only provide day-level resolution, we assigned\na default timestamp of 23:59:59 on the recorded\nday to maintain logical ordering.\nSemantic\nEnrichment.\nWe\nmapped\nhigh-\ndimensional medical codes to broader clinical\ncategories to improve representation. International\nClassification of Diseases (ICD) codes in diagnosis\nand procedure tables were mapped to Clinical\nClassification Software (CCS) categories using the\n‚ÄòICD-to-CCS‚Äô script12. Similarly, National Drug\nCodes (NDC) in prescription tables were converted\nto Anatomical Therapeutic Chemical (ATC) codes\nadopting the off-the-shelf mapping script3. These\nmappings consolidate sparse code representations\ninto hierarchically structured features.\nTextual Integration and Leakage Mitigation.\nWe standardized textual data availability by mov-\ning information explicitly available at admission,\nsuch as Social History and Chief Complaints, from\nthe discharge summary to the admission event. To\nmaintain dataset consistency, we restricted our co-\nhort to patients possessing valid text records in\ntheir discharge summaries. We also enforced strict\nleakage control by removing columns containing\nfuture administrative or outcome information. In\nthe admissions table, we excluded dischtime,\ndeathtime, discharge_location, edouttime,\n1From ICD-9 to CCS: https://hcup-us.ahrq.gov/\ntoolssoftware/ccs/ccs.jsp.\n2From ICD-10 to CCSR: https://hcup-us.ahrq.gov/\ntoolssoftware/ccsr/dxccsr.jsp.\n3From\nNDC\nto\nATC:\nhttps://github.com/\nsunlabuiuc/PyHealth.\nand hospital_expire_flag.\nSimilarly, we re-\nmoved last_careunit, outtime, and los from\nicustays, as well as outtime and disposition\nfrom edstays. Finally, we masked pharmacy ar-\ntifacts in prescription events that implicitly reveal\nfuture details.\nLabel Space Construction.\nTo facilitate effec-\ntive clinical reasoning across both MIMIC-IV and\nMIMIC-III, we addressed the challenge of exces-\nsive granularity in the raw label space. We em-\nployed three distinct strategies based on data char-\nacteristics. First, for diagnoses and procedures,\nwe adopted the Clinical Classification Software\n(CCS) taxonomy to aggregate granular codes into\nclinically coherent categories. This transforma-\ntion yielded a significant reduction, exemplified by\nMIMIC-IV. The label space was compressed from\n109,775 raw diagnosis codes to 283 CCS categories,\nand from 85,257 procedure codes to 231 categories.\nSimilarly, for prescriptions in MIMIC-IV, we\nprojected National Drug Codes (NDC) onto the\nAnatomical Therapeutic Chemical (ATC) system,\ncondensing 1,086,608 raw items into 1,813 phar-\nmacological groups.\nFor domains lacking standardized mappings, we\nutilized statistical aggregation or metadata defini-\ntions. Specifically, due to the absence of consistent\nNDC records in MIMIC-III prescriptions, as\nwell as for microbiologyevents and transfers\nin both datasets, label spaces were constructed\nby aggregating all unique items observed in the\nrecords. For labevents, the candidate list was\nderived directly from the standardized definition\ntable (d_labitems), ensuring coverage of all valid\nlaboratory tests.\nStorage Structure.\nProcessed data is stored in a\npatient-centric format where the complete longitu-\ndinal history of each patient is contained within a\nsingle SQLite database file. This structure supports\nmodular access and efficient data retrieval.\nB.2\nAuxiliary Resources\nWe\nconstructed\nReference\nand\nCandidates\ndatabases to facilitate data interpretation and define\nthe output space for predictive tasks.\nReference Database.\nThe Reference database\nfunctions as a dictionary that maps medical codes\nto item names or descriptions using metadata from\nthe diagnoses, procedures, prescriptions,\nand labitems tables in MIMIC-IV. This resource\n"}, {"page": 13, "text": "allows the retrieval of specific item names associ-\nated with patient EHR records.\nCandidates Database.\nThe Candidates database\ndefines the answer space for the six evaluation\ntasks. Candidate lists for diagnoses, procedures,\nprescriptions, and labitems were extracted di-\nrectly from the Reference database. For domains\nwithout dedicated metadata tables, specifically\nmicrobiologyevents and transfers, we aggre-\ngated all unique items observed across the MIMIC-\nIV and MIMIC-III datasets to form the final can-\ndidate sets. This process resulted in six candidate\ntables covering the required label space. We also\nprovide schema description files and a database\nlink reference to support system integration.\nB.3\nSample Construction\nTo construct a robust benchmark from the mas-\nsive MIMIC-IV database (‚àº300k patients) and\nthe smaller MIMIC-III database(‚àº1k patients), we\nemploy a stratified sampling strategy designed to\nbalance label diversity and task difficulty.\nMIMIC-IV Construction.\nFor each clinical task,\nwe first identify valid event occurrences for each\npatient. We define a sample instance by setting the\nprediction timestamp t to one minute prior to the\nevent time and identifying the ground-truth label\nset Y ‚àó. Subsequently, we temporally censor the\npatient‚Äôs history by removing any clinical obser-\nvations recorded after the prediction time t. To\nensure the benchmark covers both prevalent con-\nditions and long-tail rare cases, we implement a\nLabel-wise Weighted Sampling strategy. We calcu-\nlate a sampling weight wS for each candidate sam-\nple S, inversely proportional to the frequency of its\nconstituent labels. Formally, for a sample S with a\nground-truth label set Y ‚àó= {y1, y2, . . . , yN}, the\nweight is defined as:\nwS =\n1\n|Y ‚àó|\nX\ny‚ààY ‚àó\n1\nCount(y)\n(11)\nwhere Count(y) represents the global frequency of\nlabel entity y within the specific task type.We then\ncompute the mean weight ¬Øw across all candidates\nto stratify the dataset. Samples with wS ‚â§¬Øw are\ncategorized into the Common pool (high-frequency\nlabels), while those with wS > ¬Øw are assigned to\nthe Rare pool (low-frequency labels). Within each\npool, we perform weighted random sampling using\nwS as the probability distribution. This ensures\nthat even within the Common or Rare subsets, the\nselected samples are uniformly distributed across\nthe label space, maximizing diversity. Finally, we\nsample 600 samples for each type of task and filter\nout samples lacking admission records to ensure\ndata completeness, resulting in the final MIMIC-IV-\nCommon and MIMIC-IV-Rare benchmarks.\nMIMIC-III Construction.\nGiven the signifi-\ncantly smaller scale of MIMIC-III, we adopt a\ndirect sampling approach. We randomly sample\ninstances from the valid patient pool. However, to\naccount for the systemic complexity differences,\nwe selectively increase the sample density for high-\ndifficulty tasks, specifically Diagnoses and Pro-\ncedures, to ensure the evaluation metric remains\nstatistically significant for these challenging rea-\nsoning scenarios.\nB.4\nDataset Statistics\nTable 5 presents a comprehensive statistical sum-\nmary of the curated benchmarks. The datasets are\ncategorized into MIMIC-IV Common, MIMIC-IV\nRare, and MIMIC-III subsets to evaluate model per-\nformance across different prevalence distributions\nand data sources.\nA defining characteristic of the MIMIC-IV sub-\nsets is the high complexity of the input data. On\naverage, the model must process longitudinal pa-\ntient histories spanning over a year, comprising\nthousands of temporal records. This necessitates\nrobust capabilities in modeling long-term depen-\ndencies within heterogeneous EHR tables.\nIn distinct contrast, the MIMIC-III subset ex-\nhibits a fundamentally different structural chal-\nlenge. Unlike MIMIC-IV, MIMIC-III covers a sig-\nnificantly shorter time span (approximately 1/20)\nyet retains a disproportionately large volume of\nrecords (roughly 1/2). This discrepancy indicates\na substantially higher recording density, which in-\nevitably introduces greater redundancy and noise.\nConsequently, as evidenced by the comparative\nresults in Table 3, MIMIC-III presents a more\nformidable challenge than MIMIC-IV. We attribute\nthis difficulty not to data scarcity, but to the height-\nened complexity of information retrieval:\nthe\nmodel is required to filter through highly dense and\nnoisy sequences to extract relevant clinical signals\neffectively.\n"}, {"page": 14, "text": "Dataset\nTask\n# Cases\n# Cand.\nAvg. Label\nAvg. Tables\nAvg. Recs\nAvg. Days\nMIMIC-IV Common\nDiagnoses\n528\n283\n12.03\n24.17\n3041.10\n370.84\nLabevents\n589\n1170\n30.26\n17.60\n849.92\n347.79\nMicrobiologyevents\n549\n171\n5.59\n21.04\n1736.62\n510.68\nPrescriptions\n515\n1813\n7.65\n24.03\n1786.62\n414.26\nProcedures\n529\n231\n2.46\n24.20\n1797.61\n384.50\nTransfers\n538\n38\n1.00\n25.83\n3237.72\n387.00\nMIMIC-IV Rare\nDiagnoses\n559\n283\n6.10\n20.95\n1341.71\n301.63\nLabevents\n600\n1170\n18.58\n17.04\n1001.75\n438.99\nMicrobiologyevents\n565\n171\n4.29\n19.18\n1401.23\n524.76\nPrescriptions\n527\n1813\n7.27\n23.64\n1455.21\n349.78\nProcedures\n493\n231\n2.28\n23.36\n1642.98\n404.20\nTransfers\n487\n38\n1.00\n24.80\n2691.69\n441.25\nMIMIC-III\nDiagnoses\n600\n283\n11.03\n11.57\n1032.14\n8.81\nLabevents\n500\n587\n18.92\n11.65\n679.61\n26.19\nMicrobiologyevents\n500\n63\n3.23\n11.73\n643.05\n21.23\nPrescriptions\n500\n1235\n15.55\n11.62\n564.59\n40.62\nProcedures\n692\n231\n3.64\n11.81\n1275.52\n55.92\nTransfers\n500\n5\n1.00\n11.76\n550.49\n31.34\nTable 5: Benchmark statistics for each Dataset and Task cohort (# Cases). The table details the number of\ncandidate answers for the task (# Cand.) and the average size of label set (Avg. Label). Input complexity is further\ncharacterized by the average number of source tables (Avg. Tables), record volume (Avg. Recs), and longitudinal\nspan (Avg. Days).\nC\nToolbox Construction\nTo enable autonomous agents to effectively interact\nwith the curated clinical environment, we devel-\noped a comprehensive toolbox serving as the in-\nterface between the agent‚Äôs reasoning core and the\nstructured data. The toolbox is categorized into five\ndistinct functional modules: Record, Candidate,\nTable, Inner, and Retrieval. A detailed specifica-\ntion of the constituent tools and their parameters is\nprovided in Table 6.\nRecord Interaction Tools.\nThese tools facilitate\ngranular access to the patient‚Äôs longitudinal his-\ntory by acting as a direct query interface to the\nEHR database. To retrieve specific information,\nthe agent must specify the target table name along\nwith precise filtering criteria. The tool supports\nqueries based on temporal constraints to isolate\nevents within specific time windows, textual con-\ntent matching to identify records containing partic-\nular medical terms, and numerical value filtering to\nextract measurements meeting specific thresholds.\nThis mechanism allows the agent to dynamically\ngather evidence from the patient‚Äôs history without\nneeding to load the entire database into context.\nCandidate Alignment Tools.\nThis category is\ncritical for grounding the agent‚Äôs free-form rea-\nsoning into the fixed output space defined by the\nCandidate Database. To ensure robust mapping\nbetween generated hypotheses and valid label en-\ntries, the tool employs a hybrid matching strategy.\nIt first attempts direct keyword search and fuzzy\nstring matching to handle exact matches and mi-\nnor morphological variations. For more complex\ncases involving synonymy or terminological dif-\nferences, the tool utilizes semantic alignment pow-\nered by the biomedical-specific embedding model\nBioLORD-2023 (Remy et al., 2024). This vector-\nbased retrieval enables the agent to identify the cor-\nrect candidate even when there is no lexical overlap\nbetween the generated query and the standardized\ncode description.\nSchema Inspection Tools.\nTo effectively navi-\ngate the relational structure of the EHR, the agent\nutilizes Schema Inspection tools to acquire meta-\ndata awareness. These tools allow the agent to\nquery the definitions of the database structure itself,\nrevealing which tables are available in the current\npatient‚Äôs file and detailing the specific columns and\ndata types within each table. By understanding\nthe underlying schema, the agent can formulate\nsyntactically correct queries for the Record Interac-\ntion tools and interpret the retrieved data with the\ncorrect semantic context.\n"}, {"page": 15, "text": "Cognitive Management Tools (Inner).\nUnlike\nexternal interaction tools, this category governs the\nagent‚Äôs internal control flow and decision-making\nprocess. The Think tool enables the agent to gener-\nate intermediate reasoning traces and formulate\nmulti-step plans without triggering external en-\nvironment actions. Once the agent has gathered\nsufficient evidence and reached a conclusion, the\nFinish tool serves as the termination signal, al-\nlowing the agent to end the trajectory and output\nthe final prediction. This separation of reasoning\nand termination ensures a structured and verifiable\nthought process.\nExternal Knowledge Retrieval.\nTo augment the\nspecific clinical data contained within the EHR,\nthis tool connects the agent to a broader external\nknowledge base. By accepting natural language\nqueries, it searches for relevant medical literature,\nguidelines, or definitions that are not present in the\npatient‚Äôs records. In implementation, we use the\ndata sources provided in MedRAG (Xiong et al.,\n2024). This provides the agent with the neces-\nsary background knowledge to interpret complex\nmedical conditions or rare procedures, thereby sup-\nporting more informed decision-making.\nD\nExperimental Setting\nD.1\nBaselines\nTo comprehensively verify the effectiveness and\nrobustness of RETROSUM, we conduct compar-\native evaluations against a diverse set of state-\nof-the-art agent-based methods across five dis-\ntinct LLM backbones.\nWe employ a mix of\npowerful open-weights and proprietary models as\nbackbones to ensure broad adaptability, including\nQwen3-30B-A3B, Qwen3-235B-A22B, Qwen3-\nNext-80B (Yang et al., 2025), GPT-5-min (Ope-\nnAI, 2025), and Grok-4.1-fast (xAI, 2025).\nOn these backbones, we evaluate six representa-\ntive agent methods as baselines, categorized into\nstatic and evolving methods.\nStatic agents in-\nclude standard ReAct (Yao et al., 2023), self-\nreflecting Reflexion (Shinn et al., 2023), and the\nunidirectional summarization method ReSum (Wu\net al., 2025). Evolving agents include Reasoning\nBank (Ouyang et al., 2025) and ReflecTool (Liao\net al., 2025a), for which we evaluate both the Can-\ndidate Selection and Iterative Refinement vari-\nants.\nD.2\nImplementation Details\nIn our experiments, we set the maximum interac-\ntion turns for all agents to 100 to ensure compu-\ntational feasibility while allowing sufficient explo-\nration. The maximum context length is capped\nat 64,000 tokens to accommodate the potentially\nextensive history in EHR tasks. For evolving meth-\nods, we provide 100 training examples per task\nsampled from the common set of MIMIC-IV for\nexperience accumulation. Crucially, we utilize this\nidentical set of accumulated experiences to evalu-\nate performance across all three datasets (MIMIC-\nIV-Common, MIMIC-IV-Rare, and MIMIC-III),\nthereby strictly testing the cross-distribution gen-\neralization of the evolved agents. During infer-\nence, we retrieve only the top-1 most similar expe-\nrience sample based on the embedding generated\nby bge-m3 (Chen et al., 2024). For RETROSUM,\nthe retrospective summarization interval w is set to\n10 turns. All experiments are conducted on a clus-\nter of NVIDIA 8xA100 GPUs. Finally, Prompt 1 to\n6 contains the instructions of six prompts in AGEN-\nTEHR and Prompt 7 to 9 contains the prompt used\nin RETROSUM method.\nD.3\nMetrics\nF1 Scores\nFor a given clinical task instance, let\nY denote the set of elements predicted by the agent\nand Y ‚àódenote the set of ground truth elements.\nWe first evaluate the Precision and Recall with the\nformula below:\nPrecision = |Y ‚à©Y ‚àó|\n|Y |\n,\n(12)\nRecall = |Y ‚à©Y ‚àó|\n|Y ‚àó|\n(13)\nThe F1 Score is defined as the harmonic mean of\nPrecision and Recall, providing a balanced single-\nvalue metric that penalizes both missed relevant\nitems and incorrect predictions:\nF1 = 2 √ó Precision √ó Recall\nPrecision + Recall\n(14)\nBest@K F1\nTo quantify the expected peak per-\nformance given a specific inference budget K, we\ncalculate the Best@K F1 Score based on a total\npool of N generated trajectories (where N ‚â•K).\nThis metric is defined as the expected maximum\nF1 score when K trajectories are sampled without\nreplacement from the total pool N.For a specific\ninstance i, let Ti = {œÑ1, œÑ2, . . . , œÑN} be the set of\n"}, {"page": 16, "text": "Category\nTool Name\nDescription\nParameters\nInner\nthink\nDesigned to synthesize information gathered from preced-\ning operations and to articulate the necessary subsequent\nactions.\nresponse\nfinish\nThe final step in the reasoning process. Used only when all\nnecessary data has been retrieved and the clinical prediction\nis ready.\nresponse\nRecord\nget_records_by_time\nFinds records in a EHR Table that fall within a given time\nrange.\nsubject_id, table_name, start_time, end_time\nget_event_counts_by_time\nCalculates the number of events in all EHR Tables that fall\nwithin a given time range.\nsubject_id, start_time, end_time\nget_latest_records\nFinds the latest timestamp and returns all EHR Table records\nthat share that same timestamp in EHR Table.\nsubject_id, table_name\nget_records_by_keyword\nSearches for all text-based columns of the specific EHR\nTable containing a specific keyword.\nsubject_id, table_name, keyword\nget_records_by_value\nFinds records in a EHR Table where a given column‚Äôs value\nis exact match for the keyword.\nsubject_id, table_name,column_name, value\nrun_sql_query\nExecutes a standard SQL query against the patient‚Äôs EHR\nTable.\nsubject_id, sql_query\nget_unique_values\nRetrieves all unique values from a specified categorical\ncolumn in an EHR table.\nsubject_id, table_name, column_name\nCandidate\nget_candidates_by_keyword\nSearches for all text-based columns of the specific Candi-\ndate Table containing a specific keyword.\ntable_name, keyword\nget_candidates_by_fuzzy_matching\nFinds similar items in a Candidate Table based on fuzzy\nmatching.\ntable_name, keywords\nget_candidates_by_semantic_similarity\nPerforms semantic search using BioLORD-2023 embed-\ndings to find semantically similar unique entities.\ntable_name, query\nTable\nget_column_names\nRetrieves all column names for a specified table for under-\nstanding the data.\nsubject_id, table_name\nget_table_names\nRetrieves the names of all available tables in the database,\ncategorized into EHR tables and candidates tables.\nsubject_id\nget_table_description\nRetrieve EHR table description and column information\nfrom the hospital database schema.\ntable_name\nKnowledge\nretrieve_pubmed\nRetrieve abstract of relevant biomedical documents from\nPubMed corpus given a query.\nquery\nretrieve_textbooks\nRetrieve domain specific knowledge from medical text-\nbooks corpus given a query.\nquery\nretrieve_statpearls\nRetrieve clinical decision support from StatPearls corpus\ngiven a query.\nquery\nretrieve_wikipedia\nRetrieve general knowledge from Wikipedia corpus given a\nquery.\nquery\nTable 6: Overview of the Toolbox. The toolbox includes categories for Candidate extraction, Inner reasoning,\nKnowledge retrieval, Record querying, and Table schema inspection.\nall N generated trajectories. Let CK be the set of\nall possible subsets of Ti with size K, where the\ntotal number of such subsets is the binomial co-\nefficient\n\u0000N\nK\n\u0001\n. The Best@K score, for instance i\nis calculated by averaging the maximum F1 score\nover all possible combinations:\nBest@K F1 =\n1\n\u0000N\nK\n\u0001\nX\nS‚ààCK\nmax\nœÑ‚ààS F1(œÑ)\n(15)\nThis probabilistic definition encompasses two\ncritical boundary conditions that provide insight\ninto the model‚Äôs behavior: (1) Expected Perfor-\nmance (K = 1): When the budget is a single at-\ntempt, the metric collapses to the arithmetic mean\nof all generated trajectories, reflecting the model‚Äôs\naverage performance without selection:\nBest@1 F1 =\n1\n\u0000N\n1\n\u0001\nN\nX\nj=1\nF1(œÑj) = 1\nN\nN\nX\nj=1\nF1(œÑj)\n(16)\n(2) Peak Potential (K = N): When the budget\nallows evaluating the entire generated pool, the\nmetric represents the absolute upper bound of the\nmodel‚Äôs capability (Pass@N):\nBest@N F1 =\n1\n\u0000N\nN\n\u0001 max\nœÑ‚ààTi F1(œÑ) =\nmax\nj‚àà{1,...,N} F1(œÑj)\n(17)\nThis rigorous definition eliminates the variance\nassociated with random sampling and ensures a\ndeterministic evaluation of the trade-off between\ncomputational budget and performance.\nE\nError Definition\nTo systematically diagnose the pathological behav-\niors of the agents, we categorized the failure trajec-\n"}, {"page": 17, "text": "0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\nInteraction Rounds\nReAct\nReflexion\nReSum\nReflecTool (CS)\nReflecTool (IR)\nReasoningBank\nRetroSum (Ours)\nRetroSum Evolved (Ours)\nReAct\nReflexion\nReSum\nReflecTool (CS)\nReflecTool (IR)\nReasoningBank\nRetroSum (Ours)\nRetroSum Evolved (Ours)\nReAct\nReSum\nRetroSum (Ours)\nRetroSum Evolved (Ours)\nReAct\nReSum\nRetroSum (Ours)\nRetroSum Evolved (Ours)\nReAct\nReSum\nRetroSum (Ours)\nRetroSum Evolved (Ours)\n0\n57\n76\n66\n32\n21\n14\n4\n8\n5\n245\n7\n82\n63\n36\n6\n6\n1\n1\n1\n4\n321\n3\n29\n60\n47\n72\n36\n28\n17\n27\n20\n189\n61\n316\n51\n6\n1\n0\n2\n1\n1\n1\n88\n6\n222\n154\n84\n33\n14\n5\n5\n1\n1\n3\n0\n1\n1\n9\n13\n3\n4\n1\n1\n0\n495\n1\n81\n315\n114\n12\n3\n0\n0\n1\n0\n1\n0\n26\n231\n228\n37\n1\n2\n1\n0\n0\n2\n0\n26\n109\n131\n64\n30\n11\n3\n7\n2\n145\n0\n73\n220\n88\n29\n10\n5\n5\n0\n5\n93\n0\n50\n75\n73\n56\n37\n33\n26\n18\n21\n139\n95\n325\n85\n11\n4\n2\n3\n1\n0\n0\n2\n40\n269\n154\n34\n18\n2\n3\n3\n2\n0\n3\n0\n6\n64\n117\n62\n31\n19\n5\n4\n0\n220\n0\n58\n342\n126\n2\n0\n0\n0\n0\n0\n0\n0\n84\n380\n60\n4\n0\n0\n0\n0\n0\n0\n0\n9\n57\n89\n69\n49\n245\n3\n2\n0\n5\n0\n48\n139\n157\n89\n48\n22\n7\n7\n3\n8\n0\n32\n194\n238\n52\n10\n1\n0\n0\n0\n1\n1\n36\n159\n243\n67\n13\n3\n2\n0\n0\n4\n0\n2\n65\n125\n125\n84\n127\n0\n0\n0\n0\n0\n8\n42\n127\n142\n106\n103\n0\n0\n0\n0\n0\n53\n199\n228\n38\n6\n4\n0\n0\n0\n0\n0\n68\n247\n146\n63\n4\n0\n0\n0\n0\n0\n0\n85\n287\n137\n19\n0\n0\n0\n0\n0\n0\n0\n40\n49\n70\n61\n53\n255\n0\n0\n0\n0\n0\n57\n292\n154\n23\n2\n0\n0\n0\n0\n0\n0\n82\n288\n124\n34\n0\n0\n0\n0\n0\n0\n0\n100\n200\n300\n400\nCount\nQwen3-30B-A3B\nQwen3-Next\n-80B-A3B\nQwen3-235B\n-A22B*\n GPT-5-mini\nGrok-4.1-fast\nFigure 8: Heatmap visualization of interaction round distributions across different agent methods and backbones on\nthe diagnoses task in MIMIC-IV Common. The color intensity denotes the frequency of cases falling within each\ninterval.\ntories into six distinct types. These categories range\nfrom low-level format violations to high-level rea-\nsoning stagnation.\nNo Prediction.\nThis category aggregates sce-\nnarios where the agent terminates the interaction\nepisode without yielding a parsable or valid predic-\ntion. It encompasses four specific failure modes\ndriven by structural or cognitive deficiencies. The\nfirst involves Tool Parsing Failure, where the agent\nattempts to generate a tool invocation but fails to\nadhere to the required syntax, causing the system\nto misinterpret the action as a termination signal,\nwhich is exemplified in Case 1. The second in-\ncludes Format Submersion, where the agent suc-\ncessfully invokes the finish action and provides\na text response, yet the answer extractor fails to\nparse a valid prediction from the unstructured out-\nput. The third mode is Answer Induced Failure,\noccurring when the agent invokes the finish tool\nbut provides an empty or null observation, resulting\nin a void prediction. Finally, the category includes\nIncomplete Termination, where the interaction con-\ncludes‚Äîoften due to reaching the maximum turn\nlimit‚Äîwithout the agent ever invoking the defini-\ntive finish tool, thereby leaving the task unre-\nsolved.\nTool Repeat.\nWe define this error as a state of ex-\nact cognitive stagnation. It is characterized by the\nagent executing the identical tool with completely\nidentical parameters for five consecutive interaction\nturns, which is illustrated in Case 2. This behavior\nindicates that the agent has fallen into a rigid loop,\nrepeatedly querying the same information without\nupdating its internal strategy, leading to severe in-\nformation redundancy.\nSingle-Tool Loop.\nDistinct from exact repetition,\nthis category characterizes scenarios where the\n"}, {"page": 18, "text": "agent resorts to a naive, iterative retrieval strategy\nby executing the same tool with negligible param-\neter variations, as shown in Case 3. We employ\nthe Ratcliff-Obershelp algorithm to quantify the\nsimilarity between the parameters of consecutive\ntool calls sharing the same tool name. The string\nsimilarity score Sro between two parameter strings\nP1 and P2 is calculated as:\nSro(P1, P2) =\n2 ¬∑ Km\n|P1| + |P2|\n(18)\nwhere Km represents the number of matching char-\nacters derived from the longest common subse-\nquence and its recursive sub-segments. If the agent\nexecutes the same tool for ten consecutive rounds\nwith a parameter similarity Sro > 0.95, the trajec-\ntory is classified as a Single-Tool Loop. This be-\nhavior indicates that the agent is adopting a direct\nyet ineffective sequential scanning approach, which\ndrastically degrades reasoning efficiency and inun-\ndates the context with excessive volumes of invalid\nor redundant query information.\nMulti-Tool Cyclic Loop.\nThis category identifies\nnon-consecutive but pervasive repetition through-\nout the entire trajectory. Utilizing the same simi-\nlarity metric Sro and threshold (> 0.95), we flag\na trajectory as a cyclic loop if similar tool calls\nappear more than 15 times in total across the in-\nteraction history, which is demonstrated in Case 4.\nThis behavior suggests a lack of long-term plan-\nning, where the agent repeatedly revisits previously\nexplored states or queries in a circular manner.\nTool Usage Error.\nThis error type reflects hal-\nlucination or schema violation regarding the avail-\nable toolbox. It occurs when the agent attempts to\ninvoke a tool that does not exist in the defined tool-\nbox or provides arguments that do not align with\nthe tool‚Äôs required parameter schema, as shown in\nCase 5. Such failures indicate a disconnect between\nthe agent‚Äôs reasoning core and the environmental\nconstraints.\nNo Candidate Tool.\nGiven the benchmark‚Äôs de-\nsign, valid predictions must be grounded in the can-\ndidate search space. This error is assigned when\nthe agent fails to invoke any candidate-retrieval\nrelated tools (e.g., get_candidates_by_keyword)\nthroughout the entire interaction. The absence of\nsuch calls guarantees that the final answer will not\nmap to a valid entry in the candidate tables, in-\nevitably leading to a task failure, which can be seen\nin see Case 6.\nF\nAdditional Experiments\nF.1\nTurn Analysis\nFollowing the specific evaluation of the Qwen3-\n30B-MoE architecture in Section 5.3, we extend\nour investigation using the interaction turn heatmap\nillustrated in Figure 8 to examine the broader im-\npact of varying model capabilities and agent meth-\nods.\nThe visualization exposes that baseline ap-\nproaches exhibit significant sensitivity to the un-\nderlying model architecture. Methods such as Re-\nAct and ReasoningBank demonstrate high volatil-\nity across different backbones. On weaker mod-\nels, ReasoningBank specifically suffers from catas-\ntrophic stagnation where nearly 500 cases reach\nthe maximum turn limit. This phenomenon indi-\ncates that in the absence of an effective memory\nmechanism, lossy context compression leads to\nthe omission of critical details. Consequently, the\nagent is forced into redundant information-seeking\nloops to attempt to recover this lost context. In\ncontrast, RETROSUM maintains a consistent dis-\ntribution of interaction rounds regardless of the\nbackbone model scale. This stability suggests that\nour retrospective mechanism functions as a regu-\nlarizer which self-standardizes the reasoning trajec-\ntory and effectively enhances reasoning efficiency.\nConversely, distinct failure modes are observed\nin alternative self-evolving methods such as Re-\nflecTool, which show a tendency toward prema-\nture termination.\nThese agents frequently con-\nclude episodes within 10 to 20 rounds, evidenced\nby ReflecTool-CS resolving 316 cases within this\nshort interval on Qwen3-30B-A3B. In the context\nof complex clinical diagnostics, such rapid con-\nvergence often implies superficial reasoning or a\nfailure to conduct necessary differential diagnosis\nverifications.\nF.2\nTool Analysis\nTo investigate the behavioral distinctness of RET-\nROSUM, we analyzed the distribution of tool calls\nacross different backbones, as depicted in Figure 9.\nThis breakdown reveals fundamental differences in\nhow agents navigate the clinical decision-making\nprocess.\nBehavior with Different Methods.\nA prominent\ntrend observed in baseline methods, particularly Re-\nAct and Reflexion, is the dominance of the Records\ntool. On weaker backbones like Qwen3-30B-A3B,\n"}, {"page": 19, "text": "ReAct\nReflexion\nReSum\nReflecTool (CS)\nReflecTool (IR)\nReasoningBank\nRetroSum\nRetroSum Evolved\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTool Usage Proportion\nQwen3-30B-A3B\nReAct\nReflexion\nReSum\nReflecTool (CS)\nReflecTool (IR)\nReasoningBank\nRetroSum\nRetroSum Evolved\nQwen3-Next-80B-A3B\nReAct\nReSum\nRetroSum\nRetroSum Evolved\nQwen3-235B-A22B*\nReAct\nReSum\nRetroSum\nRetroSum Evolved\nGPT-5-mini\nReAct\nReSum\nRetroSum\nRetroSum Evolved\nGrok-4.1-fast\nRecords\nTable\nCandidate\nInner\nRetrieval\nOthers\nFigure 9: Proportional distribution of tool category usage across different agent methods and LLM backbones on\nthe diagnoses task in MIMIC-IV Common.\nthese methods dedicate over 60% of their actions\nto fetching raw EHR records. This pattern indi-\ncates an inefficient information acquisition strat-\negy, where the agent struggles to locate relevant\nevidence and repeatedly queries the database. In\ncontrast, RETROSUM exhibits a significantly more\nbalanced distribution. Notably, the proportion of\nCandidate tool usage is consistently higher in RET-\nROSUM compared to baselines. This shift suggests\nthat our retrospective mechanism successfully com-\npresses the context, enabling the agent to progress\nbeyond superficial data acquisition. Instead, the\nagent prioritizes the more advanced stage of active\ngrounding, where it systematically aligns clinical\nfindings against the candidate space to refine its\npredictions.\nBehavioral Alignment with Stronger Models.\nThe tool usage patterns also highlight how RET-\nROSUM bridges the capability gap between mod-\nels. As we observe the transition from smaller\nmodels to more powerful reasoning models, there\nis a natural tendency for agents to increase their\nusage of Candidate and Inner tools, reflecting\na more purposeful reasoning process. Crucially,\nRETROSUM enables smaller models to mimic this\n\"expert\" behavior pattern, exhibiting a tool distri-\nbution profile that resembles that of GPT-5-mini\nbaselines. This indicates that our framework effec-\ntively guides weaker models to adopt the efficient\nbehavioral heuristics naturally found in stronger\nfoundation models.\nSchema Awareness.\nFurthermore, the usage of\nthe Table tool provides insight into schema under-\nstanding. While some baselines fluctuate in their\nreliance on schema inspection, RETROSUM main-\ntains a steady but moderate usage. This implies\nthat the agent retains sufficient awareness of the\ndatabase structure through its memory mechanism,\nreducing the need for redundant schema queries\nwhile ensuring precise SQL generation for record\nretrieval.\nF.3\nComputational Consumption Analysis\nBeyond predictive performance, the deployment\nof agents in real-world clinical settings requires\na rigorous assessment of computational costs and\nlatency. Figure 10 illustrates the resource consump-\ntion of RETROSUM compared to baseline methods\nacross three key metrics: execution time, input to-\nkens, and output tokens.\nToken Economy\nRETROSUM demonstrates a re-\nmarkable advantage in input token efficiency. By\ndynamically compressing the interaction history\ninto concise retrospective summaries, our method\nreduces the average input tokens per sample to\napproximately 0.42M, representing a 4.9√ó reduc-\ntion compared to ReAct (2.06M) and a 1.7√ó re-\nduction compared to ReSum (0.70M). This drastic\ndecrease is critical for cost-sensitive applications,\nas input tokens typically dominate the inference\ncost of Large Language Models. Conversely, while\nRETROSUM generates an average of 10.9k out-\nput tokens‚Äîcomparable to ReSum (12.1k)‚Äîthis\n\"investment\" in detailed reasoning is significantly\noutweighed by the massive savings in input context\nprocessing.\nExecution Latency\nDespite the additional com-\nputational overhead required for generating sum-\nmaries and evolving experiences, RETROSUM\nachieves an average execution time of 133.08s,\n"}, {"page": 20, "text": "ReAct\nReflxion\nReflecTool (CS)\nReflecTool (IR)\nReSum\nRetroSum\n0\n50\n100\n150\n200\n250\n300\n350\n400\nExecution Time (s)\n158.66\n426.36\n99.76\n100.60\n142.11\n133.08\nReAct\nReflxion\nReflecTool (CS)\nReflecTool (IR)\nReSum\nRetroSum\n0e6\n1e6\n2e6\n3e6\n4e6\n5e6\nInput Tokens\n2058034\n5056780\n1155568\n1444036\n700869\n420448\nReAct\nReflxion\nReflecTool (CS)\nReflecTool (IR)\nReSum\nRetroSum\n0\n2000\n4000\n6000\n8000\n10000\n12000\nOutput Tokens\n4756\n11634\n6003\n5396\n12050\n10908\nFigure 10: Computational cost analysis across different agent frameworks. We report the average execution time per\nsample (left), average input tokens per sample (middle), and average output tokens per sample (right). RETROSUM\ndemonstrates superior efficiency, achieving the lowest input consumption ( 4.9√ó reduction vs. ReAct) while also\nreducing execution latency compared to standard baselines like ReSum and ReAct.\nwhich is notably faster than both ReSum (142.11s)\nand ReAct (158.66s). This result indicates that the\nretrospective mechanism successfully optimizes\nthe overall workflow‚Äîby preventing the agent\nfrom getting lost in long, repetitive raw contexts\n(as seen in ReAct), RETROSUM reduces the total\nnumber of interaction turns, thereby achieving a\nsuperior trade-off that lowers both latency and cost.\nF.4\nRecord Retrieved Distribution\nTo elucidate the information-seeking dynamics of\ndifferent agents, we analyze the distribution of re-\ntrieved tables across various clinical tasks. Figure\n11 and Figure 12 visualize the access frequency of\nEHR tables, aggregated by backbone model and\nagent framework, respectively.\nTask-Specific Schema Alignment.\nA clear pat-\ntern observed across all heatmaps is the strong se-\nmantic alignment between the task domain and the\nretrieved tables. As evidenced in the visualizations,\nagents predominantly query tables that are intrinsi-\ncally relevant to the target task. For instance, in the\nlabevents task, the retrieval focus is heavily con-\ncentrated on the labevents table. This dominant\ndiagonal pattern confirms that the agents possess\na fundamental awareness of the database schema\nand can identify the primary information sources\nrequired for specific clinical queries.\nImpact of Model Capabilities.\nThe aggregation\nby backbone model reveals distinct retrieval be-\nhaviors correlated with model scale. Smaller mod-\nels, such as the Qwen3-30B-A3B and Qwen3-80B-\nA3B, exhibit a pattern of repetitive intensity, con-\ncentrating heavily on a limited set of tables. This\nsuggests a lower efficiency in information extrac-\ntion, necessitating repeated access to grasp the\ncontext. In contrast, stronger models like GPT-\n5-mini and Grok-4.1 display a more distributed at-\ntention mechanism, synthesizing information from\na wider array of sources rather than fixating on a\nsingle table. Specific table preferences further high-\nlight these differences. While all models prioritize\nlabevents, which stores all lab test results includ-\ning hematology and chemistry, the Qwen series\naccesses it with excessive frequency compared to\nthe more balanced usage by GPT-5-mini and Grok-\n4.1. Additionally, Grok-4.1 demonstrates a unique\npreference for the triage table, which contains\ninitial emergency department assessment data and\nvital signs, whereas smaller models rely more heav-\nily on the basic admissions table. In summary,\nadvanced models demonstrate a capacity for holis-\ntic clinical contextualization, utilizing peripheral\ndata sources like triage to inform their reasoning,\nwhile smaller models adhere to a more rigid and\nrepetitive focus on core administrative tables.\nStrategies of Agent Methods.\nDisaggregating\nthe results by framework reveals how differ-\nent mechanisms influence information gathering.\nMethods incorporating summarization capabilities,\nsuch as ReSum and RETROSUM, exhibit a signifi-\ncantly broader and more uniform query distribution\nacross tables. This indicates that the summarization\nmodule facilitates the efficient digestion of table\ncontent, encouraging the agent to explore diverse\ninformation sources to construct a comprehensive\npatient profile. Conversely, methods incorporating\nself-evolution and self-reflection, such as Reflex-\n"}, {"page": 21, "text": "ion, ReflecTool, and RETROSUM Evolved, demon-\nstrate a sharper focus. These methods leverage past\nparadigms to identify and concentrate on the most\nrelevant tables for the specific task. Notably, our\napproach integrates the advantages of both cate-\ngories. It supports active exploration to acquire\nbroad context while maintaining a task-oriented\nfocus on critical data, preventing the agent from\ndrifting into irrelevant information.\nTask-Dependent Retrieval Scope.\nFrom a task\nperspective, the retrieval patterns validate the multi-\ndimensional design of our benchmark. We ob-\nserve a clear distinction between specialized and\nfoundational clinical tasks. Tables associated with\nprescriptions, procedures, and transfers are\naccessed almost exclusively during their respec-\ntive tasks, indicating a high degree of speci-\nficity. In contrast, diagnoses, labevents, and\nmicrobiologyevents serve as foundational pa-\ntient information sources and are utilized across\na wider range of scenarios. This implies that the\nfirst three tasks evaluate the agent‚Äôs ability to han-\ndle specialized, domain-specific queries, while the\nlatter three assess the ability to synthesize compre-\nhensive clinical subjects. This dichotomy ensures\nthat our benchmark provides a robust evaluation of\nclinical capabilities across multiple dimensions of\ncomplexity and specificity.\nF.5\nCase Study\nTo provide a concrete understanding of RETRO-\nSUM‚Äôs operational logic, we present qualitative\nexamples of its reasoning process and evolving\nmechanisms. Please note that due to the excessive\nlength of raw EHR observation logs, we employed\nan LLM to condense the interaction history in these\nillustrations, preserving the core reasoning logic\nwhile ensuring readability.\nWe first present a complete inference process in\nCase 9, demonstrating how RETROSUM navigates\nthe complex EHR environment to conclude a clini-\ncal task. Furthermore, to isolate the impact of our\nevolving strategy, we provide specific instances of\nexperience retrieval: Case 7 showcases the appli-\ncation of historical insights by the Actor module,\nwhile Case 8 illustrates the effect of retrieved expe-\nriences on the Summarizer module.\nG\nAI Assistance Statement\nLanguage editing and stylistic refinement of the\ndraft were performed with the aid of large lan-\nguage models, including ChatGPT 4 and Google\nGemini 5.\n4https://chatgpt.com/\n5https://gemini.google.com/app/\n"}, {"page": 22, "text": "Qwen-30B-A3B\nQwen3-Next-80B-A3B\nQwen3-235B-A22B*\nGPT-5-Mini\nGrok-4.1-Fast\nvitalsign\ntriage\ntransfers\nservices\nradiology_detail\nradiology\npyxis\nprovider\nprocedures_icd\nprocedureevents\nprescriptions\npoe_detail\npoe\npharmacy\npatients\noutputevents\nomr\nmicrobiologyevents\nmedrecon\nlabevents\ninputevents\ningredientevents\nicustays\nhcpcsevents\nemar_detail\nemar\nedstays\ndrgcodes\ndischarge_detail\ndischarge\ndiagnosis\ndiagnoses_icd\ndatetimeevents\nchartevents\ncaregiver\nadmissions\nTables\ndiagnoses\nQwen-30B-A3B\nQwen3-Next-80B-A3B\nQwen3-235B-A22B*\nGPT-5-Mini\nGrok-4.1-Fast\nlabevents\nQwen-30B-A3B\nQwen3-Next-80B-A3B\nQwen3-235B-A22B*\nGPT-5-Mini\nGrok-4.1-Fast\nmicrobiologyevents\nQwen-30B-A3B\nQwen3-Next-80B-A3B\nQwen3-235B-A22B*\nGPT-5-Mini\nGrok-4.1-Fast\nprescriptions\nQwen-30B-A3B\nQwen3-Next-80B-A3B\nQwen3-235B-A22B*\nGPT-5-Mini\nGrok-4.1-Fast\nprocedures\nQwen-30B-A3B\nQwen3-Next-80B-A3B\nQwen3-235B-A22B*\nGPT-5-Mini\nGrok-4.1-Fast\ntransfers\nUsage Rate\n10%\n30%\n50%\nTask\ndiagnoses\nlabevents\nmicrobiologyevents\nprescriptions\nprocedures\ntransfers\nFigure 11: Distribution of retrieved EHR tables aggregated by different base models. The visualization averages table access frequencies across all agent methods to highlight the\nintrinsic retrieval preferences of each LLM. Panels are segmented by task, with dot size representing the usage rate of specific tables by each model.\n"}, {"page": 23, "text": "ReAct\nReflexion\nReSum\nReflecTool (CS)\nReflecTool (IR)\nReasoningBank\nRetroSum\nRetroSum Evolved\nvitalsign\ntriage\ntransfers\nservices\nradiology_detail\nradiology\npyxis\nprovider\nprocedures_icd\nprocedureevents\nprescriptions\npoe_detail\npoe\npharmacy\npatients\noutputevents\nomr\nmicrobiologyevents\nmedrecon\nlabevents\ninputevents\ningredientevents\nicustays\nhcpcsevents\nemar_detail\nemar\nedstays\ndrgcodes\ndischarge_detail\ndischarge\ndiagnosis\ndiagnoses_icd\ndatetimeevents\nchartevents\ncaregiver\nadmissions\nTables\ndiagnoses\nReAct\nReflexion\nReSum\nReflecTool (CS)\nReflecTool (IR)\nReasoningBank\nRetroSum\nRetroSum Evolved\nlabevents\nReAct\nReflexion\nReSum\nReflecTool (CS)\nReflecTool (IR)\nReasoningBank\nRetroSum\nRetroSum Evolved\nmicrobiologyevents\nReAct\nReflexion\nReSum\nReflecTool (CS)\nReflecTool (IR)\nReasoningBank\nRetroSum\nRetroSum Evolved\nprescriptions\nReAct\nReflexion\nReSum\nReflecTool (CS)\nReflecTool (IR)\nReasoningBank\nRetroSum\nRetroSum Evolved\nprocedures\nReAct\nReflexion\nReSum\nReflecTool (CS)\nReflecTool (IR)\nReasoningBank\nRetroSum\nRetroSum Evolved\ntransfers\nUsage Rate\n10%\n30%\n50%\nTask\ndiagnoses\nlabevents\nmicrobiologyevents\nprescriptions\nprocedures\ntransfers\nFigure 12: Distribution of retrieved EHR tables aggregated by different agent methods. The visualization averages table access frequencies across all backbone models to isolate\nthe specific retrieval strategies of each method. Panels are segmented by task, with dot size representing the usage rate of specific tables by each agent framework.\n"}, {"page": 24, "text": "Prompt\nPrompt 1. Diagnose Task Prompt\nYour current task is to act as a diagnostician.\nYour objective is to determine all plausible diagnoses for the patient‚Äôs current condition\nby analyzing the patient‚Äôs complete history.\nYou must find the most likely official CCS candidates using the **diagnoses_ccs_candidates**\nreference table.\nPresent your final answer as a **list format** with ‚Äòfinish‚Äò tool calling, which must con-\ntain **multiple plausible diagnoses**. Each item in the list must be a string representing an official\nCCS diagnosis name, and **must not contain any codes or other additional information**.\nPrompt\nPrompt 2. Labevents Task Prompt\nYour current task is to act as a laboratory medicine specialist.\nYour objective is to determine all necessary laboratory tests for the patient by analyzing\ntheir complete medical history, current clinical condition, and established diagnoses.\nYou should provide as many laboratory tests as possible to cover the patient‚Äôs current\nclinical condition.\nYou must find the most likely official laboratory test candidates using the **labevents_candidates**\nreference table.\nPresent your final answer as a **list format** with ‚Äòfinish‚Äò tool calling, which must con-\ntain **multiple plausible laboratory tests**. Each item in the list must be a string representing an\nofficial laboratory test name, and **must not contain any codes or other additional information**.\nPrompt\nPrompt 3. Microbiology Task Prompt\nYour current task is to act as a clinical microbiologist.\nYour objective is to determine all necessary microbiological tests for the patient by ana-\nlyzing their complete medical history, current clinical condition, established diagnoses, and clinical\nsigns of infection.\nYou must find the most likely official microbiological test candidates using the **micro-\nbiologyevents_candidates** reference data or semantic matching tools.\nPresent your final answer as a **list format** with ‚Äòfinish‚Äò tool calling, which must con-\ntain **multiple plausible microbiological tests**. Each item in the list must be a string representing\nan official microbiological test name, and **must not contain any codes or other additional\ninformation**.\n"}, {"page": 25, "text": "Prompt\nPrompt 4. Prescriptions Task Prompt\nYour current task is to act as a pharmacist.\nYour objective is to determine all necessary ATC therapeutic categories for the patient by\nanalyzing their complete medical history, current clinical condition, and established diagnoses.\nYou must find the most likely official ATC name candidates using the **prescrip-\ntions_atc_candidates** reference data or semantic matching tools.\nPresent your final answer as a **list format** with ‚Äòfinish‚Äò tool calling, which must con-\ntain **multiple plausible ATC names**. Each item in the list must be a string representing an\nofficial ATC name, and **must not contain any codes or other additional information**.\nPrompt\nPrompt 5. Procedures Task Prompt\nYour current task is to act as a surgical planner.\nYour objective is to determine all necessary surgical procedures for the patient by analyz-\ning their complete medical history and established diagnoses.\nYou must find the most likely official CCS procedure candidates using the **‚Äòproce-\ndures_ccs_candidates‚Äò** reference table.\nPresent your final answer as a **list format** with ‚Äòfinish‚Äò tool calling, which must contain\n**multiple plausible procedures**. Each item in the list must be a string representing an official\nCCS procedure name, and **must not contain any codes or other additional information**.\nPrompt\nPrompt 6. Transfer Task Prompt\nYour current task is to act as a hospital care coordinator and clinical decision-maker.\nYour objective is to determine the most appropriate care unit for patient transfer by ana-\nlyzing their current clinical condition, medical history, severity of illness, and care requirements.\nYou must consider the patient‚Äôs current location, clinical stability, required level of moni-\ntoring, and specialized care needs to recommend the optimal transfer destination.\nYou must find the most likely official care unit candidates using the **transfers_candidates**\nreference data or semantic matching tools.\nPresent your final answer as a **list format** with ‚Äòfinish‚Äò tool calling, which must con-\ntain **multiple plausible care units**. Each item in the list must be a string representing an official\ncare unit name, and **must not contain any codes or other additional information**.\n"}, {"page": 26, "text": "Prompt\nPrompt 7. Summarization Prompt\nYou are an expert at analyzing conversation history and extracting relevant information. Your task\nis to thoroughly evaluate the conversation history and current question to provide a comprehensive\nsummary that will help solve the task.\n## Task Guidelines\n1. Information Analysis:\n- Carefully analyze the conversation history to identify truly useful information.\n- Focus on information that directly contributes to answering the question.\n- Do NOT make assumptions, guesses, or inferences beyond what is explicitly stated in the\nconversation.\n- If information is missing or unclear, do NOT include it in your summary.\n2. Summary Requirements:\n- Extract only the most relevant information that is explicitly present in the conversation.\n- Synthesize information from multiple exchanges when relevant.\n- Only include information that is certain and clearly stated in the conversation.\n- Do NOT output or mention any information that is uncertain, insufficient, or cannot be confirmed\nfrom the conversation.\n3. Output Format: Your response should be structured as follows:\n<summary>\n- Essential Information: [Organize the relevant and certain information from the conversation\nhistory that helps address the question.]\n</summary>\nStrictly avoid fabricating, inferring, or exaggerating any information not present in the\nconversation. Only output information that is certain and explicitly stated.\nQuestion\n{question}\nConversation History\n{recent_history_messages}\nPlease generate a comprehensive and useful summary.\nNote that you are not permitted\nto invoke tools during this process.\n"}, {"page": 27, "text": "Prompt\nPrompt 8. Actor Experiences Generation Prompt\nYou are an expert in clinical reasoning auditor. You will be provided with a complete post-hoc analysis package of a\nclinical reasoning task performed by an AI Actor agent.\nYour inputs include:\n1. **User Query**: The original clinical question or task.\n2. **Prediction Result & Ground Truth**: What the Actor ultimately concluded versus the correct answer.\n4. **Complete Raw Trajectory / Actions Taken**: The exact actions the Actor took based on the summaries.\n## Guidelines\nYour task is to analyze the **reasoning quality** of the Actor agent. You need to extract useful insights in the format of\nmemory items that focus on improving future clinical decision-making processes. The goal is to identify where the\nActor‚Äôs logic was flawed, overly cautious, too aggressive.\n## Important notes\n- **Focus on the Actor‚Äôs Decisions:** Focus on: Given the information *present* at step T, did the Actor make the most\nlogical action?\n- **Analyze Reasoning Gaps:**\n- If failed: Did the Actor jump to a conclusion not supported by the retrieved information? Did it ignore conflicting\nevidence presented in the EHR records? Did it fail to order a necessary confirmatory test suggested by the recprds\nambiguity?\n- If successful: What robust reasoning strategy did the Actor use to navigate uncertainty or complex data presented in the\nsummaries?\n- **Generalizable Reasoning Principles:** Insights should be about *how to think* clinically (e.g., differential diagnosis\nstrategies, handling conflicting data, recognizing urgency), not about specific medical facts.\n- Do not summarize strategies that are already mentioned in the raw instructions; focus on the implicit reasoning\nstrategies.\n- You can extract at most max_items memory items.\n- You must not repeat similar or overlapping items.\n## Output Format\nYour output must strictly follow the Markdown format shown below. Ensure ALL fields (Title, Description, Content) are\nprovided.\nRequired Format:\n‚Äù‚Äô\n# Memory Item i\n## Title <short title, max 15 words>\n## Description <one sentence summary of the memory item>\n## Content <1-3 sentences describing the insights learned to successfully accomplish the task>\n‚Äù‚Äô\n# Query:\n{query}\n# Complete Trajectory:\n{raw_trajectory}\n# Prediction Result:\n{prediction_result}\n# Ground Truth:\n{ground_truth}\n"}, {"page": 28, "text": "Prompt\nPrompt 9. Summarizer Experiences Generation Prompt\nYou are an expert medical data summarization auditor. You will be provided with a complete post-hoc analysis package\nof a clinical reasoning task.\nYour inputs include:\n1. **User Query**: The original clinical question or task.\n2. **Prediction Result & Ground Truth**: Knowing the final outcome is crucial for Hindsight bias analysis.\n3. **Complete Raw Trajectory**: The exhaustive, detailed truth (e.g., full EHR tables).\n4. **Summarized Trajectory**: The compressed version generated by the Summarizer.\n## Guidelines\nYour task is to conduct a **hindsight gap analysis** between the raw data and the generated summaries. You need to\nextract insights to improve how future Summarizers extract and compress clinical information.\n## Important notes\n- **Hindsight Gap Analysis (Crucial):** You must use the ‚ÄòGround Truth‚Äò to identify what was truly important in the\n‚ÄòRaw Trajectory‚Äò. Then, check if that important information was captured accurately in the ‚ÄòSummarized Trajectory‚Äò.\n- **Focus on Information Loss/Distortion:**\n- **What was lost?** Identify critical raw data points (e.g., a subtle lab trend, a specific timing of medication) that were\nessential for the correct diagnosis but were omitted or overly generalized in the summary.\n- **Why it matters?** Explain how omitting this specific type of raw data leads to downstream reasoning errors.\n- **Generalizable Summarization Principles:** Insights should be rules for *how to summarize* better (e.g., \"Always\npreserve exact values for abnormal vitals instead of just stating ‚Äôabnormal‚Äô\", \"Explicitly link temporal relationships\nbetween medication administration and subsequent lab changes\").\n- You can extract at most max_items memory items.\n- You must not repeat similar or overlapping items.\n## Output Format\nYour output must strictly follow the Markdown format shown below. Ensure ALL fields (Title, Description, Content) are\nprovided.\nRequired Format:\n‚Äù‚Äô\n# Memory Item i\n## Title <short title, max 15 words>\n## Description <one sentence summary of the memory item>\n## Content <1-3 sentences describing the insights learned to successfully accomplishing the task>\n‚Äù‚Äô\n# Query:\n{query}\n# Complete Trajectory:\n{raw_trajectory}\n# Summarized Trajectory:\n{summarized_trajectory}\n# Prediction Result:\n{prediction_result}\n# Ground Truth:\n{ground_truth}\n"}, {"page": 29, "text": "Case\nCase 1. No Prediction (Tool Parsing Failure)\n... (Previous steps omitted)\n## Step 63:\n- Action:\nfinish{‚Äôresponse‚Äô:\n‚Äô<tool_call>\\n{\"name\":\n\"get_candidates_by_fuzzy_matching\",\n\"arguments\": {\"table_name\": \"diagnoses_ccs_candidates\", \"keywords\": [\"myelodysplastic\",\n\"malignancy\", \"leukemia\", ... [truncated 1000+ keywords] ... \"fraction\", \"EF\", \"‚Äô}\n- Observation: Task concluded. No valid prediction format detected in response.\n# Error Analysis: The agent attempted to generate a tool invocation (‚Äòget_candidates...‚Äò) but\nfailed to adhere to the syntax, outputting it as a raw string within the ‚Äòfinish‚Äò action. The\nsystem misinterpreted this as a termination signal with invalid content, resulting in zero\npredictions.\nCase\nCase 2. Tool Repeat Error\n... (Previous steps 1-20 omitted)\n## Step 21:\n- Action:\nget_records_by_keyword{‚Äôsubject_id‚Äô:\n‚Äô19505049‚Äô, ‚Äôtable_name‚Äô:\n‚Äôadmissions‚Äô,\n‚Äôkeyword‚Äô: ‚Äôvomiting‚Äô}\n- Observation: No records found in table ‚Äôadmissions‚Äô containing the keyword ‚Äôvomiting‚Äô.\n## Step 22:\n- Action:\nget_records_by_keyword{‚Äôsubject_id‚Äô:\n‚Äô19505049‚Äô, ‚Äôtable_name‚Äô:\n‚Äôadmissions‚Äô,\n‚Äôkeyword‚Äô: ‚Äôvomiting‚Äô}\n- Observation: No records found in table ‚Äôadmissions‚Äô containing the keyword ‚Äôvomiting‚Äô.\n... (Steps 23-29 repeat the exact same action and parameters) ...\n## Step 30:\n- Action:\nget_records_by_keyword{‚Äôsubject_id‚Äô:\n‚Äô19505049‚Äô, ‚Äôtable_name‚Äô:\n‚Äôadmissions‚Äô,\n‚Äôkeyword‚Äô: ‚Äôvomiting‚Äô}\n- Observation: No records found in table ‚Äôadmissions‚Äô containing the keyword ‚Äôvomiting‚Äô.\n# Error Analysis: The agent executes identical actions with identical parameters for\n>5 consecutive turns, failing to update its strategy despite negative feedback.\n"}, {"page": 30, "text": "Case\nCase 3. Single-Tool Loop\n... (Previous steps omitted)\n## Step 19:\n- Action: get_records_by_value{‚Äôsubject_id‚Äô: ‚Äò17332316‚Äô, ‚Äôtable_name‚Äô: ‚Äôlabevents‚Äô, ‚Äôcol-\numn_name‚Äô: ‚Äôitemid‚Äô, ‚Äôvalue‚Äô: ‚Äô50889‚Äô}\n- Observation: No records found in table ‚Äôlabevents‚Äô where ‚Äôitemid‚Äô equals ‚Äô50889‚Äô.\n## Step 20:\n- Action: get_records_by_value{‚Äôsubject_id‚Äô: ‚Äò17332316‚Äô, ‚Äôtable_name‚Äô: ‚Äôlabevents‚Äô, ‚Äôcol-\numn_name‚Äô: ‚Äôitemid‚Äô, ‚Äôvalue‚Äô: ‚Äô50894‚Äô}\n- Observation: No records found in table ‚Äôlabevents‚Äô where ‚Äôitemid‚Äô equals ‚Äô50894‚Äô.\n## Step 21:\n- Action: get_records_by_value{‚Äôsubject_id‚Äô: ‚Äò17332316‚Äô, ‚Äôtable_name‚Äô: ‚Äôlabevents‚Äô, ‚Äôcol-\numn_name‚Äô: ‚Äôitemid‚Äô, ‚Äôvalue‚Äô: ‚Äô50883‚Äô}\n- Observation: labevent_id ... 50883 ... Bilirubin, Direct ... [5 rows retrieved]\n## Step 22:\n- Action: get_records_by_value{‚Äôsubject_id‚Äô: ‚Äò17332316‚Äô, ‚Äôtable_name‚Äô: ‚Äôlabevents‚Äô, ‚Äôcol-\numn_name‚Äô: ‚Äôitemid‚Äô, ‚Äôvalue‚Äô: ‚Äô50882‚Äô}\n- Observation: ... [Massive table data: Bicarbonate results, 16,427 chars truncated] ...\n...\n(Steps 23-28 continue querying ‚Äôitemid‚Äô with values 50881, 50884, 50885, 50890,\netc.) ...\n# Error Analysis: The agent performs sequential scanning with high parameter sim-\nilarity (Sro > 0.95). Instead of filtering by time or flag, it blindly guesses IDs, inundating the\ncontext with excessive raw data while making negligible reasoning progress.\n"}, {"page": 31, "text": "Case\nCase 4. Multi-Tool Cyclic Loop\n... (Previous steps omitted)\n‚Äî Cycle Iteration 1 ‚Äî\n## Step 29:\n- Action:\nget_records_by_value{‚Äôsubject_id‚Äô:\n‚Äô12593903‚Äô, ‚Äôtable_name‚Äô:\n‚Äôprescriptions‚Äô,\n‚Äôcolumn_name‚Äô: ‚Äôdrug‚Äô, ‚Äôvalue‚Äô: ‚ÄôSodium Chloride‚Äô}\n- Observation: No records found in table ‚Äôprescriptions‚Äô where ‚Äôdrug‚Äô equals ‚ÄôSodium Chloride‚Äô.\n## Step 30:\n- Action:\nget_records_by_value{‚Äôsubject_id‚Äô:\n‚Äô12593903‚Äô, ‚Äôtable_name‚Äô:\n‚Äôprescriptions‚Äô,\n‚Äôcolumn_name‚Äô: ‚Äôdrug‚Äô, ‚Äôvalue‚Äô: ‚ÄôD5 1/2NS‚Äô}\n- Observation: ... [Prescription found: 1000mL Bag D5 1/2NS ...]\n‚Äî Cycle Iteration 2 (Repeating steps 29-30) ‚Äî\n## Step 41:\n- Action:\nget_records_by_value{‚Äôsubject_id‚Äô:\n‚Äô12593903‚Äô, ‚Äôtable_name‚Äô:\n‚Äôprescriptions‚Äô,\n‚Äôcolumn_name‚Äô: ‚Äôdrug‚Äô, ‚Äôvalue‚Äô: ‚ÄôSodium Chloride‚Äô}\n- Observation: No records found in table ‚Äôprescriptions‚Äô where ‚Äôdrug‚Äô equals ‚ÄôSodium Chloride‚Äô.\n## Step 42:\n- Action:\nget_records_by_value{‚Äôsubject_id‚Äô:\n‚Äô12593903‚Äô, ‚Äôtable_name‚Äô:\n‚Äôprescriptions‚Äô,\n‚Äôcolumn_name‚Äô: ‚Äôdrug‚Äô, ‚Äôvalue‚Äô: ‚ÄôD5 1/2NS‚Äô}\n- Observation: ... [Same prescription record retrieved again]\n...\n(Agent continues cycling through ‚ÄôCarbohydrates‚Äô, ‚ÄôGlucose‚Äô, ‚Äô1/2 NS‚Äô repeatedly)\n...\n‚Äî Cycle Iteration N (Still stuck at end of trajectory) ‚Äî\n## Step 99:\n- Action:\nget_records_by_value{‚Äôsubject_id‚Äô:\n‚Äô12593903‚Äô, ‚Äôtable_name‚Äô:\n‚Äôprescriptions‚Äô,\n‚Äôcolumn_name‚Äô: ‚Äôdrug‚Äô, ‚Äôvalue‚Äô: ‚ÄôSodium Chloride‚Äô}\n- Observation: No records found in table ‚Äôprescriptions‚Äô where ‚Äôdrug‚Äô equals ‚ÄôSodium Chloride‚Äô.\n# Error Analysis: The agent exhibits a non-consecutive but pervasive cyclic pattern.\nSimilar tool calls (Sro > 0.95) appear >15 times, indicating a failure in long-term planning\nand state tracking.\n"}, {"page": 32, "text": "Case\nCase 5. Tool Usage Error\n... (Previous steps omitted)\n## Step 52:\n- Action: get_records_by_value{‚Äôsubject_id‚Äô: ‚Äô13762777‚Äô, ‚Äôtable_name‚Äô: ‚Äôradiology_detail‚Äô,\n‚Äôcolumn_name‚Äô: ‚Äôhadm_id‚Äô, ‚Äôvalue‚Äô: ‚Äô22827736‚Äô}\n- Observation: Error: Column ‚Äôhadm_id‚Äô not found in table ‚Äôradiology_detail‚Äô.\n# Error Analysis:\nThe agent committed a schema violation by attempting to query\nthe ‚Äòradiology_detail‚Äò table using a non-existent column ‚Äòhadm_id‚Äò. This indicates a failure\nto verify table metadata before execution, resulting in a runtime error.\nCase\nCase 6. No Candidate Tool Failure\n... (Previous 98 steps omitted: The agent repeatedly used action ‚Äôget_records_by_keyword‚Äô)\n## Step 99:\n- Action: get_records_by_keyword{‚Äôsubject_id‚Äô: ‚Äô16002903‚Äô, ‚Äôtable_name‚Äô: ‚Äôradiology‚Äô,‚Äôkeyword‚Äô:\n‚Äôpostoperative‚Äô}\n- Observation: ‚Äônote_id‚Äô: ‚Äô16002903-RR-10‚Äô, ‚Äôsubject_id‚Äô: ‚Äô16002903‚Äô, ‚Äôhadm_id‚Äô: ‚Äô28925885.0‚Äô,\n‚Äôcharttime‚Äô: ‚Äô2118-04-03 12:54:00‚Äô, ‚Äôtext‚Äô: ‚ÄôINDICATION: Status post right upper lobe wedge\nresection. COMPARISON: ... [remainder omitted]‚Äô\n## Step 100:\n- Action:\nget_records_by_keyword{‚Äôsubject_id‚Äô:\n‚Äô16002903‚Äô, ‚Äôtable_name‚Äô:\n‚Äôradiology‚Äô,\n‚Äôkeyword‚Äô: ‚Äôwedge resection‚Äô}\n- Observation: ‚ÄôEXAMINATION: CHEST (PORTABLE AP) INDICATION: ___ year old woman\nwith decline in resp function post-op R thoracotomy and wedge resection, now reintubated //\nPlease eval interval change. COMPARISON: ... [remainder omitted]‚Äô\n## Step 101:\n- Action: finish{‚Äôresponse‚Äô: [‚ÄôLung cancer‚Äô, ‚ÄôPostoperative status after right upper lobe wedge\nresection‚Äô, ‚ÄôPleural effusion‚Äô, ‚ÄôPneumothorax‚Äô, ‚ÄôChronic obstructive pulmonary disease‚Äô]}\n- Observation: [‚ÄôLung cancer‚Äô, ‚ÄôPostoperative status after right upper lobe wedge resection‚Äô,\n‚ÄôPleural effusion‚Äô, ‚ÄôPneumothorax‚Äô, ‚ÄôChronic obstructive pulmonary disease‚Äô]\n# Error Analysis: The agent directly outputted free-text descriptions.\nIt failed to in-\nvoke ‚Äòget_candidates_by_...‚Äò to map these findings to standardized CCS codes, rendering the\nprediction invalid.\n"}, {"page": 33, "text": "Case\nCase 7. Actor Experience\n## Prioritize High-Confidence Semantic Matches Over Fuzzy Search\n- Description: When using semantic search, prioritize results with high similarity scores and\nvalidate against clinical plausibility rather than accepting low-similarity matches as valid.\n- Content: The Actor accepted \"Substance-related disorders\" and \"Headache; including migraine\"\nbased on moderate semantic similarity, but failed to recognize that the highest-scoring match for\n\"barbiturate dependence\" was \"Alcohol-related disorders\" (0.580), which is a more accurate and\nclinically relevant diagnosis. Relying on fuzzy or low-similarity matches can lead to misdiagnosis.\n## Distinguish Between Primary Diagnosis and Secondary Findings\n- Description: Do not equate a positive drug screen (e.g., tricyclics) with a primary diagnosis of\npoisoning; consider the context of chronic use vs. acute toxicity.\n- Content: The Actor listed \"Poisoning by psychotropic agents\" as a diagnosis based on a positive\ntricyclic screen, but this reflects medication use, not acute poisoning. In the absence of symptoms\nof overdose, this should be considered a finding, not a primary diagnosis, and should not be\nincluded in the final list of plausible conditions.\n## Avoid Over-Interpreting Negative Screens in Context of Withdrawal\n- Description: A negative barbiturate screen during withdrawal is expected and does not rule out a\nhistory of dependence; it should not be used to infer current active use.\n- Content: The Actor correctly noted the negative barbiturate screen but did not fully integrate this\ninto the clinical pictureÀò2014this is normal in withdrawal, not a sign of resolution. The focus should\nremain on the history of dependence, not the current test result, which is not diagnostic of current\nstate.\n## Use Temporal Context to Distinguish Acute from Chronic Conditions\n- Description: Acute conditions must be supported by recent, active findings; past diagnoses with\nno current evidence should not be reasserted as current conditions.\n- Content: The patient‚Äôs past diagnosis of \"drug withdrawal syndrome\" (2174-12-30) and\n\"barbiturate dependence\" (2174-12-30) are outdated.\nThe current stable vitals and lack of\nwithdrawal symptoms suggest these are resolved, not active. The Actor failed to account for time\nsince last event, leading to overestimation of current relevance.\n## Integrate Comorbidities with Core Diagnoses, Not as Standalone Entries\n- Description: Alcohol use (elevated ethanol) and substance use are comorbidities, not independent\ndiagnoses; they should be grouped under a single, broader condition when appropriate.\n- Content: The elevated ethanol level (83 mg/dL) and history of barbiturate dependence point to a\npattern of substance use, but the correct approach is to group them under a single overarching\ndiagnosis like \"Substance-related disorders\" rather than creating multiple separate entries,\nespecially when the system already has a unified category.\n## Leverage ICD-10 Code Mapping to Validate Diagnoses\n- Description: When a diagnosis is suspected, cross-check the ICD-10 code to ensure it aligns with\nthe most specific and correct category, not just a general one.\n- Content: The ground truth includes \"F10129\" (Alcohol-related disorders) and \"F17200\"\n(Cocaine-related disorders), which are more specific than the generic \"Substance-related disorders\"\n(F1910). The Actor should have used the ICD-10 code mapping to identify the most precise\ndiagnosis, not default to a broad category.\n"}, {"page": 34, "text": "Case\nCase 8. Summarizer Experience\n## Preserve exact ICD codes for precise diagnosis mapping\n- Description: Failing to map ICD-9 codes to their correct ICD-10 equivalents can result in missing\ncritical diagnoses.\n- Content: The ground truth includes \"Alcohol-related disorders\" (F10129), which was not\nidentified in the summary despite being a high-similarity match (0.580) for barbiturate dependence\n(30410). The summary incorrectly prioritized \"Substance-related disorders\" over the more specific\n\"Alcohol-related disorders\nleading to a key diagnostic omission. Summarizers must map ICD-9 codes to their most accurate\nICD-10 counterparts using official crosswalks, not just semantic similarity.\n## Prioritize contextually relevant ICD-10 codes over general categories\n- Description: General CCS categories like \"Substance-related disorders\" may mask more specific,\nclinically significant diagnoses.\n- Content: The ground truth lists two distinct substance-related ICD-10 codes (F1910 and F17200)\nthat are more specific than the generic \"Substance-related disorders\" used in the prediction. The\nsummary failed to extract these specific codes, likely due to over-reliance on broad semantic\nmatches. Summarizers should flag and include all high-similarity, specific ICD-10 candidates,\nespecially when multiple variants exist.\n## Capture comorbidities from past history with clinical context\n- Description: Past diagnoses of substance use and withdrawal are not standalone; they often\nco-occur with mood disorders and alcohol-related conditions.\n- Content: The patientÀò2019s history of drug withdrawal (2920) and barbiturate dependence (30410)\nstrongly suggests an underlying mood disorder, which is confirmed in the ground truth as \"Mood\ndisorders\" (F339). The summary did not link this history to mood pathology, instead focusing only\non current lab findings. Summarizers must infer and include comorbid conditions based on chronic\nsubstance use patterns.\n## Use temporal context to validate current vs. past conditions\n- Description: A recent positive tricyclic screen indicates current use, not just past history, and\nshould be linked to active diagnosis.\n- Content: The tricyclic antidepressant screen (15 mg/dL) was correctly noted as positive, but the\nsummary treated it as a static finding without linking it to the current state. The ground truth\nincludes \"Substance-related disorders\" (F17200) for barbiturate use, but the tricyclic use is a\ndifferent class of psychotropic agent. The summary should have flagged this as a new, active\ncondition requiring its own diagnostic code, not just a semantic match.\n## Explicitly report abnormal values with units and reference ranges\n- Description: Omitting or generalizing abnormal lab values can lead to misinterpretation of clinical\nstatus.\n- Content: The ethanol level of 83 mg/dL was reported as \"above normal\" but not compared to\nthe reference range (70‚àº100 mg/dL), which is crucial for diagnosing alcohol use. The summary\ngeneralized this to \"may indicate recent alcohol use\" without stating the exact value and range.\nSummarizers must always preserve the numeric value, unit, and reference range to support accurate\nclinical inference.\n"}, {"page": 35, "text": "Case\nCase 9. RETROSUM Reasoning Sample (Observation Cleaned)\n# Task Information\n- Task: Diagnoses\n- SubjectID: 19505049\n- TimeStamp: 2174-04-07 16:53:00\n- Label: [Secondary malignancies, Phlebitis; thrombophlebitis and thromboembolism, Fluid and\nelectrolyte disorders, Secondary malignancies, Other gastrointestinal disorders, Coagulation\nand hemorrhagic disorders, Thyroid disorders, Mood disorder, Screening and history of\nmental health and substance abuse codes, Cardiac dysrhythmias, Residual codes; unclassified,\nCancer of bronchus; lung, Residual codes; unclassified, Residual codes; unclassified, Phlebitis;\nthrombophlebitis and thromboembolism, Other aftercare, Cancer of other GI organs; peritoneum]\n# Reasoning Trajectory\n## Step 1\n- Action: think{‚Äôresponse‚Äô: ‚ÄôEHR loaded. Plan: 1) Inspect admissions/patient/demographics and\ntimeline. 2) Review diagnoses candidate table. 3) Extract clinical events (notes, labs, radiology,\nprescriptions). 4) Cross-reference with CCS candidates. 5) Synthesize plausible diagnoses list.‚Äô}\n- Observation: Thinking Finish\n## Step 2\n- Action: get_table_description{‚Äôtable_name‚Äô: ‚Äôadmissions‚Äô}\n- Observation:\nTable:\nadmissions (hosp).\nContains details for each hospital admission\n(demographics, timing, source, discharge info). Key Columns: subject_id, hadm_id, admittime,\ndischtime, admission_type, diagnosis, hospital_expire_flag.\n(Steps 3-9: Retrieved table descriptions for patients, omr, labevents, radiology, prescrip-\ntions, discharge - Omitted for brevity)\n## Step 10\n- Action: Summary{‚Äôresponse‚Äô: ‚ÄôPlan: Inspect admission details, vital signs, lab results, imaging,\nand discharge notes to build a diagnosis list.‚Äô}\n- Observation: OK\n## Step 11\n- Action: get_records_by_value{‚Äôsubject_id‚Äô: ‚Äô19505049‚Äô, ‚Äôtable_name‚Äô: ‚Äôadmissions‚Äô, ‚Äôcol-\numn_name‚Äô: ‚Äôsubject_id‚Äô, ‚Äôvalue‚Äô: ‚Äô19505049‚Äô}\n- Observation: Admission Details:\n- Subject ID: 19505049 | Hadm ID: 21468882\n- Timeline: Admitted 2174-04-01 (Emergency) -> Discharged 2174-04-07\n- Demographics: Male, Married, White, English\n- Chief Complaint: Abdominal pain\n- History: Ampullary cancer s/p Whipple (18mo ago), liver mets. Presenting with worsening\nepigastric pain, weight loss (65lbs), and constipation.\n## Step 12\n- Action: get_records_by_time{‚Äôsubject_id‚Äô: ‚Äô19505049‚Äô, ‚Äôtable_name‚Äô: ‚Äôomr‚Äô, ‚Äôstart_time‚Äô:\n‚Äô2173-01-01‚Äô, ‚Äôend_time‚Äô: ‚Äô2174-04-07 16:53:00‚Äô}\n- Observation: Outpatient Vitals Trend:\n- 2173-04: BP 160/72, Wt 225.5 lbs\n"}, {"page": 36, "text": "- 2173-09: BP 150/70, Wt 200 lbs\n- 2174-03: BP 100/58, Wt 174 lbs (Significant weight loss noted)\n## Step 13\n- Action: get_records_by_time{‚Äôsubject_id‚Äô: ‚Äô19505049‚Äô, ‚Äôtable_name‚Äô: ‚Äôlabevents‚Äô, ‚Äôstart_time‚Äô:\n‚Äô2174-04-01‚Äô, ‚Äôend_time‚Äô: ‚Äô2174-04-07 16:53:00‚Äô}\n- Observation: Query returned 93,885 records. Key Abnormalities extracted in next step.\n## Step 14\n- Action: run_sql_query{‚Äôsubject_id‚Äô: ‚Äô19505049‚Äô, ‚Äôsql_query‚Äô: \"SELECT label, value, valuenum,\nflag, charttime FROM labevents WHERE subject_id=‚Äô19505049‚Äô AND charttime BETWEEN\n‚Äô2174-04-01‚Äô AND ‚Äô2174-04-07‚Äô...\"}\n- Observation: Key Lab Results (2174-04-03 to 04-04):\n- Hematology: Hgb 8.9-9.0 L (Anemia), Hct 26.4-28.1 L, Plt 83-100 L (Thrombocytopenia).\n- Chemistry: Na 127-130 L (Hyponatremia), K 5.4 H (hyperkalemia, normalized later), Glucose\n95-115.\n- Coagulation: INR 1.1, PTT 36.1 H.\n- Renal: Creatinine 0.7-0.8 (Normal).\n## Step 15\n- Action: get_records_by_time{‚Äôsubject_id‚Äô: ‚Äô19505049‚Äô, ‚Äôtable_name‚Äô: ‚Äôradiology‚Äô, ‚Äôstart_time‚Äô:\n‚Äô2174-04-01‚Äô, ‚Äôend_time‚Äô: ‚Äô2174-04-07 16:53:00‚Äô}\n- Observation: CT Abdomen w/ Contrast (2174-04-01):\n- Impression: Increased size necrotic periportal mass ( 4.5cm) with likely invasion into small bowel\nand encasement of SMA/SMV.\n- Liver: Multiple hypodense lesions consistent with metastases (enlarged from prior).\n- Vascular: Stable thrombus of the IVC.\n## Step 16\n- Action: get_records_by_time{‚Äôsubject_id‚Äô: ‚Äô19505049‚Äô, ‚Äôtable_name‚Äô: ‚Äôdischarge‚Äô, ‚Äôstart_time‚Äô:\n‚Äô2174-04-07‚Äô, ‚Äôend_time‚Äô: ‚Äô2174-04-07 16:53:00‚Äô}\n- Observation: Discharge Summary:\n- Primary Diagnosis: Metastatic ampullary carcinoma, Abdominal Pain, Constipation.\n- Secondary: IVC thrombosis, Chronic anemia, Hypothyroidism, A-fib, Depression.\n- Hospital Course: Pain controlled with methadone.\nAnticoagulation (Coumadin/Heparin)\ndiscontinued due to GI bleed risk and thrombocytopenia. Palliative care consulted; patient referred\nto hospice.\n## Step 17\n- Action:\nget_candidates_by_semantic_similarity{‚Äôquery‚Äô:\n[‚Äôampullary carcinoma‚Äô, ‚Äôivc\nthrombosis‚Äô, ‚Äôanemia‚Äô, ‚Äôdepression‚Äô, ...], ‚Äôtable_name‚Äô: ‚Äôdiagnoses_ccs_candidates‚Äô}\n- Observation: Top CCS Candidates Found:\n- Cancer of pancreas (0.574), Secondary malignancies (0.702)\n- Phlebitis; thrombophlebitis and thromboembolism (0.592)\n- Deficiency and other anemia (0.845)\n- Coagulation and hemorrhagic disorders (0.641)\n- Thyroid disorders (0.775), Cardiac dysrhythmias (0.761), Mood disorders (0.647)\n## Step 18\n- Action:\nget_records_by_time{‚Äôsubject_id‚Äô:\n‚Äô19505049‚Äô, ‚Äôtable_name‚Äô:\n‚Äôprescriptions‚Äô,\n"}, {"page": 37, "text": "‚Äôstart_time‚Äô: ‚Äô2173-01-01‚Äô, ‚Äôend_time‚Äô: ‚Äô2174-04-07 16:53:00‚Äô}\n- Observation: Active Medications:\n- Pain/GI: Methadone, Docusate, Senna, Lactulose, Ranitidine.\n- Cardiac: Diltiazem, Digoxin.\n- Other: Levothyroxine, Vitamin D, Citalopram.\n- Discontinued: Heparin (Infusion stopped 2174-04-03).\n## Step 19\n- Action: think{‚Äôresponse‚Äô: ‚ÄôSynthesizing evidence: CT confirms metastatic mass and IVC throm-\nbus. Labs show anemia/thrombocytopenia. History supports hypothyroidism/A-fib/Depression.\nMapping findings to standard CCS categories.‚Äô}\n- Observation: Thinking Finish\n## Step 20\n- Action:\nfinish{‚Äôresponse‚Äô:\n[‚ÄôCancer of pancreas‚Äô, ‚ÄôSecondary malignancies‚Äô, ‚ÄôPhlebitis;\nthrombophlebitis and thromboembolism‚Äô, ‚ÄôDeficiency and other anemia‚Äô, ‚ÄôCoagulation and\nhemorrhagic disorders‚Äô, ‚ÄôAbdominal pain‚Äô, ‚ÄôOther gastrointestinal disorders‚Äô, ‚ÄôThyroid disorders‚Äô,\n‚ÄôCardiac dysrhythmias‚Äô, ‚ÄôMood disorders‚Äô]}\n- Observation: [‚ÄôCancer of pancreas‚Äô, ‚ÄôSecondary malignancies‚Äô, ‚ÄôPhlebitis; thrombophlebitis\nand thromboembolism‚Äô, ‚ÄôDeficiency and other anemia‚Äô, ‚ÄôCoagulation and hemorrhagic disorders‚Äô,\n‚ÄôAbdominal pain‚Äô, ‚ÄôOther gastrointestinal disorders‚Äô, ‚ÄôThyroid disorders‚Äô, ‚ÄôCardiac dysrhythmias‚Äô,\n‚ÄôMood disorders‚Äô]\n# Scores\n- F1 Score: 0.608695652173913\n- Precision: 0.7\n- Recall: 0.5384615384615384\n"}]}